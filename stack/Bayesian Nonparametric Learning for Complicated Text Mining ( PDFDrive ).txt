Bayesian Nonparametric Learning
for Complicated Text Mining
Junyu Xuan
A thesis submitted for the Degree of
Doctor of Philosophy
Faculty of Engineering and Information Technology
University of Technology Sydney
October, 2016


CERTIFICATE OF
AUTHORSHIP/ORGINALITY
This thesis is the result of a research candidature conducted jointly with another
University as part of a collaborative Doctoral degree. I certify that the work
in this thesis has not previously been submitted for a degree nor has it been
submitted as part of the requirements for a degree except as fully acknowledged
within the text.
I also certify that this thesis has been written by me. Any help that I have
received in my research work and the preparation of the thesis itself has been
acknowledged. In addition, I certify that all information sources and literature
used are indicated in the thesis.
Signature of Candidate
iii


ACKNOWLEDGEMENTS
I would like to express my earnest thanks to my principal supervisor, Professor
Jie Lu, and my co-supervisor, Professor Guangquan Zhang. Their comprehensive
guidance has covered all aspects of my PhD study, including research method-
ology, research topic selection, experiments, academic writing skills and thesis
writing, even sentence structure and formulas. Their critical comments and sug-
gestions have strengthened my study signiﬁcantly. Their strict academic attitude
and respectful personalities have beneﬁted my PhD study and will be a great
treasure throughout my life. Without their excellent supervision and continuous
encouragement, this research could not have been ﬁnished on time. Thanks to
you both for your kind help! I would like to thank Dr. Richard Yi Da Xu for
his advice on this study. Without his help it would not have been possible for
me to plunge into this area and conduct this research.
I am grateful to all members of the Decision Systems and e-Service Intelligent
(DeSI) Lab in the Centre for Quantum Computation and Intelligent Systems
(QCIS) for their careful participation in my presentation and valuable comments
on my research. I would like to thank Ms. Sue Felix and Jem Moore for helping
me to proofread my publications and this thesis.
I wish to express my appreciation for the ﬁnancial support I received for
my study. Special thanks go to the China Scholarship Council (CSC) and the
University of Technology Sydney (UTS).
Last but not least, I would like also to thank my family members. Thanks to
v

my mother, father and sisters for their conscious encouragement and generous
support.
vi

ABSTRACT
Text mining has gained the ever-increasing attention of researchers in recent
years because text is one of the most natural and easy ways to express human
knowledge and opinions, and is therefore believed to have a variety of appli-
cation scenarios and a potentially high commercial value. It is commonly ac-
cepted that Bayesian models with ﬁnite-dimensional probability distributions as
building blocks, also known as parametric topic models, are eﬀective tools for
text mining. However, one problem in existing parametric topic models is that
the hidden topic number needs to be ﬁxed in advance. Determining an appro-
priate number is very diﬃcult, and sometimes unrealistic, for many real-world
applications and may lead to over-ﬁtting or under-ﬁtting issues. Bayesian non-
parametric learning is a key approach for learning the number of mixtures in a
mixture model (also called the model selection problem), and has emerged as
an elegant way to handle a ﬂexible number of topics. The core idea of Bayesian
nonparametric models is to use stochastic processes as building blocks, instead
of traditional ﬁxed-dimensional probability distributions. Even though Bayesian
nonparametric learning has gained considerable research attention and under-
gone rapid development, its ability to conduct complicated text mining tasks,
such as: document-word co-clustering, document network learning, multi-label
document learning, and so on, is still weak. Therefore, there is still a gap be-
tween the Bayesian nonparametric learning theory and complicated real-world
text mining tasks.
vii

To ﬁll this gap, this research aims to develop a set of Bayesian nonpara-
metric models to accomplish four selected complex text mining tasks.
First,
three Bayesian nonparametric sparse nonnegative matrix factorization model-
s, based on two innovative dependent Indian buﬀet processes, are proposed for
document-word co-clustering tasks. Second, a Dirichlet mixture probability mea-
sure strategy is proposed to link the topics from diﬀerent layers, and is used to
build a Bayesian nonparametric deep topic model for topic hierarchy learning.
Third, the thesis develops a Bayesian nonparametric relational topic model for
document network learning tasks by a subsampling Markov random ﬁeld. Last-
ly, the thesis develops Bayesian nonparametric cooperative hierarchical structure
models for multi-label document learning task based on two stochastic process
operations: inheritance and cooperation. The ﬁndings of this research not only
contribute to the development of Bayesian nonparametric learning theory, but
also provide a set of eﬀective tools for complicated text mining applications.
viii

TABLE OF CONTENT
CERTIFICATE OF AUTHORSHIP/ORGINALITY
iii
ACKNOWLEDGEMENTS
v
ABSTRACT
vii
TABLE OF CONTENT
ix
LIST OF FIGURES
xiii
LIST OF TABLES
xix
Chapter 1 Introduction
1
1.1
Background . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1
1.2
Research questions and objectives . . . . . . . . . . . . . . . . . .
5
1.3
Research contributions . . . . . . . . . . . . . . . . . . . . . . . .
9
1.4
Research signiﬁcance . . . . . . . . . . . . . . . . . . . . . . . . .
10
1.5
Thesis structure . . . . . . . . . . . . . . . . . . . . . . . . . . . .
11
1.6
Publications related to this thesis . . . . . . . . . . . . . . . . . .
14
Chapter 2 Literature Review
17
2.1
Deﬁnitions of Bayesian nonparametric learning . . . . . . . . . . .
17
2.2
Basic ingredients: stochastic processes
. . . . . . . . . . . . . . .
19
2.3
Manipulations of stochastic processes . . . . . . . . . . . . . . . .
23
2.3.1
Marginalization . . . . . . . . . . . . . . . . . . . . . . . .
23
2.3.2
Layering . . . . . . . . . . . . . . . . . . . . . . . . . . . .
25
2.3.3
Superposition . . . . . . . . . . . . . . . . . . . . . . . . .
27
2.3.4
Subsampling . . . . . . . . . . . . . . . . . . . . . . . . . .
28
2.3.5
Point-transition . . . . . . . . . . . . . . . . . . . . . . . .
30
2.3.6
Nesting
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
30
2.4
Supervised learning . . . . . . . . . . . . . . . . . . . . . . . . . .
31
2.5
Tree structure learning . . . . . . . . . . . . . . . . . . . . . . . .
32
2.6
Nonparametric extensions
. . . . . . . . . . . . . . . . . . . . . .
33
ix

TABLE OF CONTENT
2.7
Posterior inference
. . . . . . . . . . . . . . . . . . . . . . . . . .
36
2.7.1
Slice sampling . . . . . . . . . . . . . . . . . . . . . . . . .
36
2.7.2
Variational inference . . . . . . . . . . . . . . . . . . . . .
37
2.7.3
Scalability . . . . . . . . . . . . . . . . . . . . . . . . . . .
39
2.7.4
Others . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
40
2.8
Application scenarios . . . . . . . . . . . . . . . . . . . . . . . . .
40
2.8.1
Text mining . . . . . . . . . . . . . . . . . . . . . . . . . .
41
2.8.2
Natural language processing . . . . . . . . . . . . . . . . .
42
2.8.3
Computer vision
. . . . . . . . . . . . . . . . . . . . . . .
42
2.8.4
Biology
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
43
2.8.5
Music analysis . . . . . . . . . . . . . . . . . . . . . . . . .
44
2.9
Summary
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
45
Chapter 3 Bayesian Nonparametric Sparse Nonnegative Matrix
Factorization for Document-word Co-clustering
47
3.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
47
3.2
Preliminary knowledge . . . . . . . . . . . . . . . . . . . . . . . .
49
3.2.1
Sparse nonnegative matrix factorization
. . . . . . . . . .
49
3.2.2
Indian buﬀet process . . . . . . . . . . . . . . . . . . . . .
51
3.3
Doubly sparse nonparametric NMF . . . . . . . . . . . . . . . . .
51
3.3.1
Doubly sparse nonparametric NMF framework . . . . . . .
52
3.3.2
Implementation by GP-based dIBP . . . . . . . . . . . . .
54
3.3.3
Implementation by bivariate Beta distribution-based dIBP
56
3.3.4
Implementation by copula-based dIBP
. . . . . . . . . . .
59
3.3.5
Models discussion . . . . . . . . . . . . . . . . . . . . . . .
60
3.4
Model inference . . . . . . . . . . . . . . . . . . . . . . . . . . . .
62
3.4.1
Inference for the GP-based dIBP
. . . . . . . . . . . . . .
62
3.4.2
Update stick weights . . . . . . . . . . . . . . . . . . . . .
64
3.4.3
Update binary matrices
. . . . . . . . . . . . . . . . . . .
67
3.4.4
Update loading matrices . . . . . . . . . . . . . . . . . . .
68
3.4.5
Update model parameter . . . . . . . . . . . . . . . . . . .
68
3.5
Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
70
3.5.1
Evaluation on sparsity and nonparametric properties
. . .
70
3.5.2
Evaluation on correlation ﬂexibility . . . . . . . . . . . . .
74
3.5.3
Real-world task: document-word co-clustering . . . . . . .
76
3.6
Summary
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
83
Chapter 4 Bayesian Nonparametric Deep Topic Model for the
Topic Hierarchy Learning
85
4.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
85
4.2
Problem statement . . . . . . . . . . . . . . . . . . . . . . . . . .
87
x

TABLE OF CONTENT
4.3
Bayesian nonparametric deep topic model
. . . . . . . . . . . . .
88
4.4
Model inference . . . . . . . . . . . . . . . . . . . . . . . . . . . .
98
4.5
Experimental evaluation . . . . . . . . . . . . . . . . . . . . . . . 106
4.5.1
Synthetic data . . . . . . . . . . . . . . . . . . . . . . . . . 108
4.5.2
Real-world data . . . . . . . . . . . . . . . . . . . . . . . . 110
4.6
Summary
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 115
Chapter 5 Bayesian Nonparametric Relational Topic Model for
Document Network Learning
119
5.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 119
5.2
Preliminary knowledge . . . . . . . . . . . . . . . . . . . . . . . . 122
5.2.1
Relational topic model . . . . . . . . . . . . . . . . . . . . 122
5.2.2
Markov random ﬁeld . . . . . . . . . . . . . . . . . . . . . 123
5.3
Bayesian nonparametric relational topic model . . . . . . . . . . . 124
5.4
Model inference . . . . . . . . . . . . . . . . . . . . . . . . . . . . 132
5.4.1
Gibbs sampling . . . . . . . . . . . . . . . . . . . . . . . . 133
5.4.2
Slice sampling . . . . . . . . . . . . . . . . . . . . . . . . . 136
5.5
Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 139
5.5.1
Evaluation metrics . . . . . . . . . . . . . . . . . . . . . . 140
5.5.2
Experiments on synthetic data . . . . . . . . . . . . . . . . 141
5.5.3
Experiments on real-world data . . . . . . . . . . . . . . . 147
5.6
Summary
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 153
Chapter 6 Bayesian Nonparametric Cooperative Hierarchical Struc-
ture Models for Multi-label Document Learning
155
6.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 155
6.2
Cooperative hierarchical structure . . . . . . . . . . . . . . . . . . 157
6.3
Cooperative hierarchical Dirichlet processes
. . . . . . . . . . . . 161
6.3.1
Stick-breaking representation
. . . . . . . . . . . . . . . . 166
6.3.2
International restaurant process representation . . . . . . . 173
6.3.3
Model analysis
. . . . . . . . . . . . . . . . . . . . . . . . 180
6.4
Mixed Gamma-negative binomial processes . . . . . . . . . . . . . 184
6.4.1
Model description . . . . . . . . . . . . . . . . . . . . . . . 184
6.4.2
Model inference . . . . . . . . . . . . . . . . . . . . . . . . 189
6.4.3
Model analysis
. . . . . . . . . . . . . . . . . . . . . . . . 196
6.5
Experimental evaluation . . . . . . . . . . . . . . . . . . . . . . . 201
6.5.1
Author-topic model task . . . . . . . . . . . . . . . . . . . 201
6.5.2
Clinical free text labeling task . . . . . . . . . . . . . . . . 205
6.6
Summary
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 208
Chapter 7 Conclusion and Further Study
209
xi

TABLE OF CONTENT
7.1
Conclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 209
7.2
Further study . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 212
REFERENCES
215
ABBREVIATIONS
243
xii

LIST OF FIGURES
1.1
Thesis structure . . . . . . . . . . . . . . . . . . . . . . . . . . . .
12
2.1
The relations between stochastic processes. . . . . . . . . . . . . .
19
3.1
Graphical representations for (a) the original Indian Buﬀet Pro-
cess, (b) GP-based dependent Indian Buﬀet Processes and (c)
Bivariate Beta distribution-based or Copula-based dependent In-
dian Buﬀet Processes.
. . . . . . . . . . . . . . . . . . . . . . . .
53
3.2
FGM copula density surfaces with diﬀerent values of ρ = {−1, 0, 1}
and the marginal distributions are both Beta distributions: Beta(1, 1)
(means that α1 = α2 = 1 in Eq. (3.15)).
. . . . . . . . . . . . . .
59
3.3
Illustration of the meaning of learned correlation. The red/solid
curve denotes a focused distribution on words from a large neg-
ative correlation; the blue/dashed curve denotes a non-focused
distribution on words from a large positive correlation.
. . . . . .
61
3.4
Comparison of the sparsity on synthetic dataset between tradi-
tional NMF (NMF), traditional sparse NMF (sNMF), single IBP-
based sparse NMF (sIBP-NMF), and doubly IBP-based sparse
NMF (dIBP-NMF). Note that dIBP-NMF here is based on bi-
variate beta distribution.
. . . . . . . . . . . . . . . . . . . . . .
73
3.5
Comparison of the learned topic number distribution on synthetic
dataset between single IBP-based sparse NMF (sIBP-NMF) and
doubly IBP-based sparse NMF (dIBP-NMF). Note that dIBP-
NMF here is based on bivariate beta distribution.
. . . . . . . .
74
3.6
Results on synthetic data to show the ﬂexibility of the diﬀerent
models. The x-axis denotes the trial IDs (order is irrelevant).
. .
75
3.7
Document-word co-clustering (document side) comparisons on Cite-
seer dataset between GP-dIBP-NMF, BB-dIBP-NMF, C-dIBP-
NMF, sIBP-NMF, and SNMF. . . . . . . . . . . . . . . . . . . . .
77
3.8
Document-word co-clustering (word side) comparisons on Citeseer
dataset between GP-dIBP-NMF, BB-dIBP-NMF, C-dIBP-NMF,
sIBP-NMF, and SNMF.
. . . . . . . . . . . . . . . . . . . . . . .
78
xiii

LIST OF FIGURES
3.9
Document-word co-clustering (document side) comparisons on Co-
ra dataset between GP-dIBP-NMF, BB-dIBP-NMF, C-dIBP-NMF,
sIBP-NMF, and SNMF.
. . . . . . . . . . . . . . . . . . . . . . .
79
3.10 Document-word co-clustering (word side) comparisons on Cora
dataset between GP-dIBP-NMF, BB-dIBP-NMF, C-dIBP-NMF,
sIBP-NMF, and SNMF.
. . . . . . . . . . . . . . . . . . . . . . .
80
4.1
An example of Topic Hierarchy.
The orange eclipses denote 4
topics at the ﬁrst layer; the pink eclipses denote 7 topics at the
second layer; the cyan eclipses denote 9 topics at the third lay-
er. Data: abstracts of 11 papers from the special issue on Brain
Encoding in Pattern Recognition Journal. After removing some
stopwords, there are total 635 diﬀerent words and each abstract
contains about 100 words. α function: y = 5x and λ function is
y = 5x. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
89
4.2
Graphical model for the generation of the proposed Deep Topic
Model with three level Topic Hierarchy. The circles denotes ran-
dom measures or random variables and the solid box denotes the
repeat. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
91
4.3
Illustration of deep random partition and the relations between
topics on the simplex. . . . . . . . . . . . . . . . . . . . . . . . . .
91
4.4
The eﬀect of the parameter λ for the DMPM. Here, G is a mix-
ture of ﬁnite number (three) of Dirichlet distributions with equal
weights, and each subﬁgure is an illustration of the H(λ, G) under
diﬀerent value of λ.
. . . . . . . . . . . . . . . . . . . . . . . . .
93
4.5
Illustration of the base measures in Deep Topic Model . . . . . . .
99
4.6
Illustration of the adaptive truncation by Slice variables.
The
whole length of a black line is one. Each green box denotes a
stick weight, and the summation of the sticks on the same black
line is one. The red bar represents a slice variable which functions
as an adaptive truncation. At each iteration, the slices could move
to both directions. With the slice variables, there are only ﬁnite
number of stick weights and topis need to be updated.
. . . . . . 103
4.7
Illustration of the inﬂuence of the α function. The synthetic data
is: 100 documents and 200 diﬀerent words, and each document
has 20 diﬀerent words. The λ function is y = x. . . . . . . . . . . 108
4.8
Illustration of the inﬂuence of the λ function. The synthetic data
is: 100 documents and 200 diﬀerent words, and each document
has 20 diﬀerent words. The α function is y = x. . . . . . . . . . . 109
xiv

LIST OF FIGURES
4.9
An example of topic hierarchy. The orange eclipses denote 4 top-
ics at the ﬁrst layer; the pink eclipses denote 9 topics at the sec-
ond layer; the cyan eclipses denote 20 topics at the third layer.
Dataset: ICDM. The α function is y = 5x and the λ function is
also x = 5x. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 112
4.10 Results on Dataset PR . . . . . . . . . . . . . . . . . . . . . . . . 114
4.11 Results on Dataset ICDM
. . . . . . . . . . . . . . . . . . . . . . 115
4.12 Results on Dataset CORA . . . . . . . . . . . . . . . . . . . . . . 115
4.13 Results on Dataset Citeseer
. . . . . . . . . . . . . . . . . . . . . 116
4.14 Results on Dataset KOS . . . . . . . . . . . . . . . . . . . . . . . 116
4.15 Results on Dataset NIPS
. . . . . . . . . . . . . . . . . . . . . . 117
5.1
Graphical model of relational-topic-Model
. . . . . . . . . . . . . 122
5.2
Illustration of Gamma process assignments for a document net-
work. Each document is assigned a Gamma process which has in-
ﬁnite components (represented by the fences in the ﬁgure). Each
fence denotes a hidden topic, and some examples are given in the
ﬁgure. The length of the fences denote the weights of diﬀerent
topics in a document. . . . . . . . . . . . . . . . . . . . . . . . . . 127
5.3
Graphical representation for the Nonparametric Relational Top-
ic (NRT) Model by dependent thinned Gamma Processes and
Markov Random Field (MRF). The generative procedures for the
document 3 and 4 are omitted to make the ﬁgure more concise.
. 130
5.4
The boxplot of the learned topic numbers by truncated inference
method given diﬀerent truncation levels.
. . . . . . . . . . . . . . 142
5.5
Learned topic number distribution from NRT with synthetic dataset-
s under diﬀerent settings. Normally, the expectation of this distri-
bution will be regarded as the learn topic number of a document
network. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 143
5.6
The illustration of topics learning results. Three red/circle nodes
denote three benchmark topics that are also given at the top of
each subﬁgure; the blue/cross nodes denote learned topics from
NRT. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 143
5.7
Eﬀectiveness of SMRF in NRT. The ﬁrst subﬁgure is for the com-
parision between average similarity between topic interests of all
test linked document pairs from both NRT and NRT without SM-
RF; The second and third subﬁgures are for comparison on Link
and Document prediction. . . . . . . . . . . . . . . . . . . . . . . 144
5.8
Results of NRT and RTM under diﬀerent setting (K={20, 30, 40,
50, 60}) on a ﬁrst 5-fold of cora dataset.
. . . . . . . . . . . . . . 148
xv

LIST OF FIGURES
5.9
Results of NRT and RTM under diﬀerent settings (K={20, 30,
40, 50, 60}) on a second 5-fold of cora dataset. . . . . . . . . . . . 148
5.10 Results of NRT and RTM under diﬀerent settings (K={20, 30,
40, 50, 60}) on a third 5-fold of cora dataset. . . . . . . . . . . . . 149
5.11 Results of NRT and RTM under diﬀerent settings (K={20, 30,
40, 50, 60}) on a fourth 5-fold of cora dataset. . . . . . . . . . . . 149
5.12 Results of NRT and RTM under diﬀerent settings (K={20, 30,
40, 50, 60}) on a ﬁfth 5-fold of cora dataset. . . . . . . . . . . . . 150
5.13 Results of NRT and RTM under diﬀerent settings (K={2, 5, 10,
20, 30, 40, 50, 100}) on a ﬁrst 5-fold of citeseer dataset. . . . . . . 150
5.14 Results of NRT and RTM under diﬀerent settings (K={2, 5, 10,
20, 30, 40, 50, 100}) on a second 5-fold of citeseer dataset.
. . . . 151
5.15 Results of NRT and RTM under diﬀerent settings (K={2, 5, 10,
20, 30, 40, 50, 100}) on a third 5-fold of citeseer dataset. . . . . . 151
5.16 Results of NRT and RTM under diﬀerent settings (K={2, 5, 10,
20, 30, 40, 50, 100}) on a fourth 5-fold of citeseer dataset. . . . . . 152
5.17 Results of NRT and RTM under diﬀerent settings (K={2, 5, 10,
20, 30, 40, 50, 100}) on a ﬁfth 5-fold of citeseer dataset. . . . . . . 152
6.1
Two types of hierarchical structures . . . . . . . . . . . . . . . . . 157
6.2
The comparison between generated random measures from HDP
and CHDP.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 164
6.3
Comparing the Chinese restaurant franchise process (three-layer)
and international restaurant process. There are four restaurants
and three chefs in the ﬁgure.
In CRF, all the customers in a
restaurant can only be served by one chef, but the customers
in IRP could be served by diﬀerent chefs. The main diﬀerence
between HDP and CHDP is due to Cooperation.
. . . . . . . . . 175
6.4
The comparisons between empirical and expected factor number-
s of CHDP with two representations: Stick and IRP. Since the
factor number is parameterized by α0, αa and αd. . . . . . . . . . 182
6.5
Gamma-negative binomial process topic model. The left subﬁgure
is related to Eq. (6.38) and the right hand part is related to Eq.
(6.39). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 185
6.6
Gamma-Gamma-negative binomial process topic model (left one)
and mixed Gamma-negative binomial process topic model (right
one). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 187
6.7
The comparisons between expected and empirical factor numbers
of MGNBP under diﬀerent parameters: γ0, c0, ca and pd. Note
that the x-axes of c0 and ca are in negative (base-10) log space. . . 200
xvi

LIST OF FIGURES
6.8
The comparisons between the ATM (ﬁxed dimensional model)
with CHDP and MGNBP (nonparametric models) on the author
prediction.
‘AX’ denotes the ATM with X number of topics.
Each box expresses the statistics of results from the corresponding
model on 5-fold cross-validation. The larger the value of AP, the
better the performance. . . . . . . . . . . . . . . . . . . . . . . . . 202
6.9
The comparisons between the ATM (ﬁxed dimensional model)
with CHDP and MGNBP (nonparametric models) on the data
likelihood. Since CHDP and MGNBP do not need the preﬁxed
topic number, two lines are plotted in this ﬁgure.
. . . . . . . . . 202
6.10 The comparisons between LIFT, LEAD, CHDP and MGNBP on
the multi-label classiﬁcation.
Each subﬁgure demonstrates the
results on a corresponding metric.
. . . . . . . . . . . . . . . . . 206
xvii


LIST OF TABLES
2.1
Properties of stochastic processes. ‘Stick-breaking’ denotes if the
process has a stick-breaking constructive representation; ‘CRM’
denotes if the process is a completely random measure; ‘NRM’
denotes if the process is a normalized random measure; ‘Power-
law’ denotes if there is a version with power-law (cluster number
to data number) phenomenon. . . . . . . . . . . . . . . . . . . . .
24
3.1
Notations in this chapter . . . . . . . . . . . . . . . . . . . . . . .
52
3.2
Evaluation metrics for clustering . . . . . . . . . . . . . . . . . . .
76
4.1
The learned topic numbers at three layers from all datasets . . . . 113
5.1
Important notations in this chapter . . . . . . . . . . . . . . . . . 125
6.1
Important notations for this chapter
. . . . . . . . . . . . . . . . 159
xix


Chapter 1
Introduction
1.1
Background
Text mining, a variation on the ﬁeld of data mining (Han et al., 2011) that
tries to ﬁnd interesting patterns from large datasets, refers generally to the pro-
cess of extracting interesting and non-trivial information and knowledge from
unstructured text (Gupta and Lehal, 2009). Since it is believed that the ma-
jority of information, approximately 80%, is stored as text, text mining has not
only attracted scholars from the academic community but also industry prac-
titioners. The text mining has gained ever-increasing attention by researchers
in recent years, which makes it a very active research ﬁeld. Many research di-
rections have been explored in this ﬁeld, such as text clustering (Aggarwal and
Zhai, 2012), text classiﬁcation (Lodhi et al., 2002), text summarisation (Goyal
et al., 2013), information retrieval (William B. Frakes, 1992), and so on. Beyond
dealing with simple document-word structures, more intricate situations have
also been investigated, such as document network (e.g., paper citation networks
or email reply networks) and document labels (e.g., authors or emotion tags).
These more intricate situations bring tougher challenges to researchers because
1

Chapter 1
1.1. BACKGROUND
they signiﬁcantly extend the application scenarios of this ﬁeld. Given text is one
of the most easiest and most natural ways for humans to express their knowl-
edge and opinions, it is believed that text mining has a variety of application
scenarios and a high-value commercial potential. For example, 1) Customer rela-
tionship management, which aims to manage the contents of clientsąŕ messages
by automatically rerouting speciﬁc requests to the appropriate service, or supply
immediate answers to the most frequently asked questions (Gupta and Lehal,
2009); 2) Online media service text mining, which is being used by large media
companies, such as the Tribune Company, to clarify information and provide
readers with a better search experience, which in turn increases the companyąŕs
revenue1; 3) Academic analysis, where organisations and researchers might ob-
tain the status and trends in a certain research ﬁeld through analysing large
amounts of published academic papers (Miner, 2012).
Topic models (Hofmann, 1999; Blei et al., 2003), which are Bayesian models
with ﬁnite-dimensional probability distributions as building blocks, are common-
ly accepted as a very successful tool for text mining (Alghamdi and Alfalqi, 2015;
Sun et al., 2012). These models normally comprise two stages: 1) estimate or
compute the posterior distribution of the latent variables in a probabilistic model
from a collection of text; 2) for new documents, answer the question at hand
(e.g., classiﬁcation and retrieval) via the learned posterior distribution of the
latent variables (Blei, 2004). A set of signiﬁcant latent variables that form in-
teresting patterns within texts are termed as topics. Within diﬀerent document
collections, the subject of the topic will be diﬀerent. If the academic papers
of IEEE are taken as input documents, then a topic may be a research ﬁeld:
computer vision, text mining or image segmentation. If the emails in an organi-
sation are taken as input documents, then the topics may be the concerns of its
1https://en.wikipedia.org/wiki/Text_mining
2

1.1. BACKGROUND
Chapter 1
employees: salary increases, heavy workloads or the end-of-year party. If news
webpages are taken as input documents, then the topic may be a news story:
Malaysia Airlines Flight 370 Disappears, Ebola Strikes West Africa or ISIS De-
clares an Islamic Caliphate 2. These topics can be seen as a summarisation or
abstraction of the whole document collection. Discovering these hidden topics
could improve the services of IEEE, such as the ability to search, browse or visu-
alise academic papers; help an organisation understand and resolve the concerns
of its employees; assist news websites to organise the large amounts of news web-
pages and provide online news services. Except for the traditional text mining
tasks (e.g., text classiﬁcation (Ramage et al., 2009) and text stream (Wang and
McCallum, 2006)), there are existing some complicated text mining tasks, such
as: 1) document-word co-clustering; 2) topic hierarchy learning. An extension of
classical LDA, i.e., hierarchically labeled LDA (Perotte et al., 2011), is proposed
to resolve this task; 3) document network learning. A link variable is introduced
into LDA to model the document network (Chang and Blei, 2009; Chang et al.,
2010; Chen, Zhu, Xia and Zhang, 2015); 4) multi-label document learning. The
labels of documents could be author (Rosen-Zvi et al., 2004; Steyvers et al.,
2004; Rosen-Zvi et al., 2010) or emotional tags (Lin and He, 2009). Based on
the existing parametric topic models, these complicated text mining tasks could
be resolve in a degree.
A major issue in existing parametric topic models for text mining is that the
hidden topic number in the deﬁned priors needs to be ﬁxed in advance. This
number is normally chosen with domain knowledge. After ﬁxing the number of
topics, Dirichlet, multinomial, and other ﬁxed-dimensional distributions can be
adopted as the building blocks for (parametric) topic models. However, deter-
mining an appropriate number is very diﬃcult, and sometimes unrealistic, in
2http://www.theatlantic.com/international/archive/2014/12/the-10-biggest-international-
stories-of-2014/383935/
3

Chapter 1
1.1. BACKGROUND
many real-world applications. For example, limiting each document to a ﬁxed
exact number of topics is unrealistic for any given corpus. Furthermore, prede-
termining a number of topics may lead to overﬁtting when there are too many,
such that relatively speciﬁc topics will not generalise well to unseen observation-
s; Underﬁtting is the opposite case. When there are too few topics, unrelated
observations will be assigned to the same topic (Dai and Storkey, 2015). A num-
ber of other methods can be used to nominate the number of topics, including
cross-validation techniques (Griﬃths and Steyvers, 2004), but they are slow be-
cause the algorithm has to be restarted a number of times before determining
the optimal number of topics (Griﬃths and Steyvers, 2004; Dai and Storkey,
2015).
One elegant approach to resolve the above issue is Bayesian nonparametric
learning - a key approach for learning the number of mixtures in a mixture model,
also called the model selection problem (Gershman and Blei, 2012). This num-
ber is supposed to be inferred from the data, i.e., let the data speak. The idea
of Bayesian nonparametric learning is to use stochastic processes (i.e., Dirichlet
process (Ghosal, 2010), multinomial process, beta process (Tamara Broderick
and Pitman, 2012), poisson process (Iwata et al., 2013)) to replace traditional
ﬁxed-dimensional probability distributions. The merit of these stochastic pro-
cesses is that they have a theoretically inﬁnite number factors, and let the data
determine the number of used factors. Many probabilistic models with ﬁxed
dimensions have been extended into inﬁnite ones with the help of stochastic pro-
cesses. One example is the famous Gaussian mixture model, which was extended
into an inﬁnite Gaussian mixture model (Rasmussen, 1999) using a Dirichlet pro-
cess. Another famous example is the hidden Markov model (Rabiner, 1989; Yu,
2010) that has been extended into an inﬁnite hidden Markov model (Wulsin
et al., 2014) using hierarchical Dirichlet processes (Teh, 2006). The core of the
4

1.2. RESEARCH QUESTIONS AND OBJECTIVES
Chapter 1
Bayesian nonparametric learning is to build or infer the model for a particular
problem using stochastic processes as its building blocks.
Although the Bayesian nonparametric learning has experienced a boom in
recent years, there is still a huge gap between existing work and complicated
text mining tasks, such as: 1) document-word co-clustering; 2) topic hierar-
chy learning; 3) document network learning; 4) multi-label document learning.
Accomplishing all these complex tasks requires the further development new
technologies and theories. This thesis intends to ﬁll that gap by developing a set
of Bayesian nonparametric models for these text mining tasks.
1.2
Research questions and objectives
This research aims to develop a set of Bayesian nonparametric models for four
selected text mining tasks and will answer the following research questions:
QUESTION 1 How to co-cluster documents and words under Bayesian non-
parametric setting?
A text corpus is a form of dyadic data normally represented as a matrix with
rows and columns representing two entities (i.e., document and word). An im-
portant text mining task pertinent to a text corpus is to cluster each entity. For
example, document clustering could beneﬁt the eﬃciency of an organisation or
the browsing of a corpus; and word clustering could be used for automatic con-
struction of a statistical thesaurus or the enhancement of queries (Crouch, 1988).
However, clustering algorithms that cluster documents and words separately do
not perform well on such problems because the relationship between documents
and words is overlooked. By comparison, co-clustering (Dhillon, 2001; Bichot,
2010), where documents and words are clustered at the same time, can achieve
5

Chapter 1
1.2. RESEARCH QUESTIONS AND OBJECTIVES
better performance by investigating the relationship between documents and
words (Shan and Banerjee, 2008).
QUESTION 2 How to learn a topic hierarchy from documents under Bayesian
nonparametric setting?
The topics within a corpus normally have diﬀerent granularities. For example,
image segmentation is a sub-area of the research ﬁeld computer vision, therefore
computer vision is more general than image segmentation. Obtaining diﬀerent
levels of summarisation or abstraction in a document collection, called topic
hierarchy, is of practical beneﬁt to many real-world applications, including, but
not limited to, document retrieval, summarisation, browsing and visualisation.
Take information retrieval (Fersini et al., 2008) as an example. A user could
start with a more general description about information that she wants, and
could gradually choose more speciﬁc topics provided by the retrieval system
according to the learned topic hierarchy until the desired information is found.
QUESTION 3 How to learn from a document network under Bayesian non-
parametric setting?
Sometimes, there are links between the documents in a corpus.
A paper
citation network (Guo et al., 2014) is an example of a document network in
which the academic papers are linked by their citation relations; an email network
(Klimt and Yang, 2004) is a document network in which the emails are linked
by their reply relations; a webpage network (Park, 2003) is a document network
in which webpages are linked by their hyperlinks. Since these links also express
the nature of the documents, it is apparent that hidden topic discovery should
consider these links as well. Learning from the document network could support
relevant scientiﬁc papers or web pages recommendation.
6

1.2. RESEARCH QUESTIONS AND OBJECTIVES
Chapter 1
QUESTION 4 How to learn from the multi-label documents under Bayesian
nonparametric setting?
Multi-label learning (Gibaja and Ventura, 2015; Zhang and Zhou, 2014) is a
signiﬁcant learning paradigm in which each instance may be assigned with more
than one label. For instance, each academic paper may have more than one
author, and learning from this data could help to identify the academic inter-
ests of authors and recommend potential collaborators according their interests
(Rosen-Zvi et al., 2004; Xuan, Lu, Zhang, Xu and Luo, 2015); a patent may be
associated with several categories, and automatically assign large amount of new
patents to correct categories could save the costs in human resources and time
(Cong and Tong, 2008).
This research aims to achieve the following objectives, which are expected to
answer the above research questions:
OBJECTIVE 1 To develop a Bayesian Nonparametric Sparse Nonnegative
Matrix Factorization Model.
This objective corresponds to research Question 1. Nonnegative matrix fac-
torisation (NMF) aims to factorise a matrix into two optimised nonnegative fac-
tor matrices, appropriate for its intended application, which could beneﬁt many
tasks, including document-word co-clustering the weighting of a matrix papers
by its words). However, traditional NMF typically assumes the number of latent
factors (i.e., the dimensionality of the factor matrices) to be ﬁxed. This study
will develop a nonparametric sparse NMF model to remove this assumption and
learn the correlation between documents and words.
OBJECTIVE 2 To develop a Bayesian Nonparametric Deep Topic Model.
7

Chapter 1
1.2. RESEARCH QUESTIONS AND OBJECTIVES
This objective corresponds to research Question 2. Traditional topic models
only output a topic set without its hierarchical structure. Some extensions have
been made to learn the topic hierarchy based on topic models, such as tree labeled
LDA (Slutsky et al., 2013), hierarchically labeled LDA (Perotte et al., 2011),
Pachinko allocation (Li and McCallum, 2006; Mimno et al., 2007) and ﬁxed
structure LDA (Reisinger and Paşca, 2009). However, these extensions suﬀer
from one drawback: the number of topics need to be preﬁxed. This study will
develop a new Dirichlet mixture probability measurement strategy to facilitate
topic hierarchy learning without the need to predeﬁne the number of topics.
OBJECTIVE 3 To develop a Bayesian Nonparametric Relational Topic Mod-
el.
This objective corresponds to research Question 3. Similar studies focusing
on the hidden topics discovered from a document network using some relational
topic models (RTM) (Chang and Blei, 2009; Chang et al., 2010; Chen, Zhu, Xia
and Zhang, 2015) have already been successfully developed. Unlike tradition-
al topic models (Blei et al., 2003; Blei, 2012) that focus on mining the hidden
topics from a document corpus (without links between documents), RTMs can
force discovered topics to inherit their document network structure. One draw-
back of existing RTMs is that they are built with ﬁxed-dimensional probability
distributions, such as Dirichlet, multinomial, gamma and possion distributions,
which require their dimensions to be ﬁxed before use. This study will develop
a Bayesian nonparametric relational topic model based on stochastic processes
instead of probability distributions.
OBJECTIVE 4 To develop Bayesian Nonparametric Cooperative Hierarchical
Structure Models.
8

1.3. RESEARCH CONTRIBUTIONS
Chapter 1
This objective corresponds to research Question 4. Existing studies in Bayesian
nonparametric learning area mainly focus on non-cooperative hierarchical struc-
tures based on hierarchical Dirichlet processes (HDP) (Teh, 2006) or gamma-
negative binomial processes (Zhou and Carin, 2015) and their variations (Dai
and Storkey, 2015; Paisley et al., 2015; Canini and Griﬃths, 2011). To capture
the cooperative hierarchical Structure, this study will develop cooperative hier-
archical Dirichlet processes and mixed gamma-negative binomial processes based
on two designed process operations: inheritance and cooperation.
1.3
Research contributions
The main contributions of this study are concisely summarised as follows:
• Two new dependent Indian buﬀet processes (i.e., bivariate Beta distribution-
based dIBP and copula-based dIBP) with simpler model structures and
more direct correlation modeling capabilities have been developed as alter-
natives to the existing GP-based dIBP;
• Three dependent Indian buﬀet processes-based Bayesian nonparametric
doubly sparse nonnegative matrix factorisation models are proposed to
remove the assumption of the traditional NMF that the number of factors
needs to be ﬁxed in advance;
• A new Dirichlet mixture probability measure is designed to facilitate ﬂex-
ible topic hierarchy when learning from a document collection, while two
sampling inference algorithms, i.e., a truncated version and a slice version,
are proposed to facilitate the posterior inference for the proposed model;
• Cooperative hierarchical Dirichlet process and mixed gamma-negative bi-
nomial process are innovatively proposed based on two operations on ran-
9

Chapter 1
1.4. RESEARCH SIGNIFICANCE
dom measures: Inheritance and Cooperation, which could be used to model
the cooperative hierarchical structures that cannot be modelled by existing
Bayesian nonparametric models;
• two constructive representations (i.e., stick-breaking and the international
restaurant process) for cooperative hierarchical Dirichlet process are pro-
posed to facilitate the model inference, which rises to the challenge brought
about by inheritance and cooperation between random measures;
• The expectation of component numbers from cooperative hierarchical Dirich-
let processes and mixed Gamma-negative binomial processes are theoreti-
cally and empirically analysed to understand the behavior and sensitivity
of the processes under diﬀerent parameters.
1.4
Research signiﬁcance
The theoretical and practical signiﬁcance of this research is summarized as fol-
lows:
Theoretical signiﬁcance: The ﬁndings of this study contribute to the
Bayesian nonparametric learning community in the following two ways: enriching
the theoretical analysis methodology for Bayesian nonparametric learning; and
facilitating the application of Bayesian nonparametric learning to more com-
plex scenarios.
More speciﬁcally, as the core of the Bayesian nonparametric
learning is the manipulation of stochastic processes, the proposed ideas, such
as: dependent Indian buﬀet processes, Dirichlet mixture probability measure,
and cooperation between Dirichlet processes or gamma processes, could enrich
the approach for powerful and meaningful manipulations on stochastic processes
which lay the foundation for Bayesian nonparametric learning area. Meanwhile,
10

1.5. THESIS STRUCTURE
Chapter 1
the proposed ideas on the stochastic processes manipulation are particularly de-
signed for more complex scenario modeling, such as: modeling the correlation
between documents and words, modeling document networks, modeling multil-
abels of documents, and so on. The extended application scenarios could not only
improve the impact of this area, but also motivate its continuing development.
Practical signiﬁcance: The ﬁndings of this study contribute to the ben-
eﬁt of society given the important role text plays in modern life. The ﬁndings
help resolve the real-world problems of corporations and organisations and im-
prove quality of life for individuals. Corporations can manage their customer
relationships better, or improve their product designs by analysing feedback and
comments from their customers. Organisations can understand their employees
concerns by analysing internal email. Analysis of an individualąŕs search histo-
ry can help search engines improve user experience and satisfaction. There is
potential for many other such applications that could beneﬁt from this study.
1.5
Thesis structure
The structure of the thesis is shown in Figure 1.1 and the chapters are organized
as follows:
• In Chapter 2, this thesis reviews the current state-of-the-art of research on
Bayesian nonparametric learning with a particular focus on text mining,
including its basic concepts, commonly used mechanisms, existing models,
inference strategies, and application scenarios.
• In Chapter 3, this thesis develops Bayesian nonparametric sparse nonneg-
ative matrix factorisation models for document-word co-clustering. Three
strategies, which induce three implementations for the Bayesian nonpara-
metric sparse nonnegative matrix factorisation model, are proposed to cor-
11

Chapter 1
1.5. THESIS STRUCTURE
Figure 1.1: Thesis structure
12

1.5. THESIS STRUCTURE
Chapter 1
relate documents and words, and are theoretically and empirically com-
pared.
• In Chapter 4, this thesis develops a Bayesian nonparametric deep topic
model for topic hierarchy learning. Although the input data is same as the
model in Chapter 3, this model can learn a topic hierarchy rather than a
set of topics, as in Chapter 3. A Dirichlet mixture probability measure is
presented to link the topics located in diﬀerent layers.
• In Chapter 5, this thesis presents a Bayesian nonparametric relational top-
ic model for document network learning. As opposed to the above two
Chapters, this Chapter focuses on text corpora with document links. A s-
trategy is presented to incorporate the network structure into the Bayesian
nonparametric model design and inference. This model could be applied
to document link prediction.
• In Chapter 6, this thesis presents two Bayesian nonparametric coopera-
tive hierarchical structure models for multi-label document learning. This
Chapter focuses on text corpora with multiple labels. Two operations be-
tween the stochastic processes are formally deﬁned and used for cooperative
hierarchical structure modeling, including two implementations: coopera-
tive hierarchial Dirichlet processes and mixed gamma-negative binomial
processes. These models are characterised by the abilities in the super-
vised document learning.
• Finally, Chapter 7 summarises the ﬁndings of the thesis and points to
directions for future work.
13

Chapter 1
1.6. PUBLICATIONS RELATED TO THIS THESIS
1.6
Publications related to this thesis
Below is a list of the refereed international journal and conference papers during
my PhD research that have been published or currently under review:
Published:
Journals :
1. Junyu Xuan, Jie Lu, Guangquan Zhang, and Xiangfeng Luo, Topic
models for graph mining, IEEE Transactions on Cybernetices, Vol.45,
No.12, pp.2792-2803, 2014. (ERA rank: A)
2. Xiangfeng Luo, Junyu Xuan, Guangquan Zhang, and Jie Lu, Mea-
suring the semantic uncertainty of news events for evolution potential
estimation, ACM Transactions on Information Systems, Vol.34, No.4,
2016. (ERA rank: A)
3. Junyu Xuan, Xiangfeng Luo, Jie Lu, and Guangquan Zhang, Un-
certainty analysis for the keyword system of web events, IEEE Trans-
actions on Systems, Man and Cybernetics: Systems, Vol.46, No.6,
pp.829-842, 2016. (ERA rank: B)
Conferences :
4. Junyu Xuan, Jie Lu, Guangquan Zhang, Richard Yida Xu, and
Xiangfeng Luo, Inﬁnite author topic model through mixed Gamma
negative binomial processes, IEEE International Conference on Data
Mining (ICDM), Atlantic City, USA, 2015. (ERA rank: A)
5. Junyu Xuan, Jie Lu, Guangquan Zhang, and Xiangfeng Luo, Re-
lease ‘bag-of-words’ assumption of latent Dirichlet allocation, Interna-
tional Conference on Intelligent Systems and Knowledge Engineering
14

1.6. PUBLICATIONS RELATED TO THIS THESIS
Chapter 1
(ISKE), Shenzhen, China, pp.83-92, 2014. (ERA rank: B, Best Stu-
dent Paper Award)
6. Junyu Xuan, Jie Lu, Guangquan Zhang, and Xiangfeng Luo, Ex-
tension of similarity measures in VSM: from orthogonal coordinate
system to aﬃne coordinate system, The 2014 International Joint Con-
ference on Neural Networks (IJCNN), Beijing, China, pp.4084-4091,
2014. (ERA rank: A)
Under review:
1. Junyu Xuan, Jie Lu, Guangquan Zhang, Richard Yida Xu, and Xiangfeng
Luo, Nonparametric relational topic model through dependent Gamma pro-
cesses, IEEE Transactions on Knowledge and Data Engineering, under
review.
2. Junyu Xuan, Jie Lu, Guangquan Zhang, Richard Yida Xu, and Xi-
angfeng Luo, Dependent Indian buﬀet process-based sparse nonparametric
nonnegative matrix factorization, IEEE Transactions on Neural Network
and Learning Systems, under review.
3. Junyu Xuan, Jie Lu, Guangquan Zhang, Richard Yida Xu, and Xiangfeng
Luo, Cooperatively hierarchical Dirichlet processes, Artiﬁcial Intelligence,
under review.
4. Junyu Xuan, Jie Lu, Guangquan Zhang, Richard Yida Xu, and Xiangfeng
Luo, A Bayesian nonparametric model for multi-label learning, Machine
Learning, under review.
15


Chapter 2
Literature Review
This chapter reviews the existing researches on Bayesian nonparametric learning
(BNL) in detail. In Section 2.1, the deﬁnition and motivation of Bayesian non-
parametric learning are introduced. Section 2.2 presents the basic ingredients
of this area followed by their manipulations summarized in Section 2.3. Section
2.4 and 2.5 discuss the applications of Bayesian nonparametric learning on two
settings: supervised learning and tree structure learning. Section 2.6 collects the
nonparametric extensions of existing classical (parametric) models. Section 2.7
introduces the inference techniques used in this community. Section 2.8 describes
the domains where Bayesian nonparametric learning has been applied. Section
2.9 brieﬂy summarizes this chapter.
2.1
Deﬁnitions of Bayesian nonparametric learn-
ing
Many machine learning tasks involve building a model class deﬁned by a set of
parameters and then inferring an (globally or locally) optimized set of param-
eters. The model parameters control the goodness-of-ﬁt to the data and the
17

Chapter 2
2.1. DEFINITIONS OF BAYESIAN NONPARAMETRIC LEARNING
model complexity, so inappropriate model parameters may lead to underﬁtting
(i.e., bad goodness-of-ﬁt but good model complexity) or overﬁtting (i.e., good
goodness-of-ﬁt but bad model complexity) problems. To determine or learn these
parameters is termed model selection or model adaptation problem. Traditional
methods for the model selection are based on human labor or restated several
times to ﬁnd an optimal setting, which are quite time-consuming and not scal-
able to large unfamiliar data. Fortunately, Bayesian nonparametric learning has
emerged as an elegant way to resolve the above problem.
One deﬁnition of Bayesian nonparametric models comes from Encyclopedia
of Machine Learning (Orbanz and Teh, 2010) as
Deﬁnition 1 A Bayesian nonparametric model (BNP) is a Bayesian model on
an inﬁnite-dimensional parameter space. The parameter space is typically chosen
as the set of all possible solutions for a given learning problem.
The ability to build models on inﬁnite-dimensional parameter spaces is due to
stochastic processes as the alternative of probabilistic distributions. One major
contribution of Bayesian nonparametric learning to the machine learning com-
munity is to introduce stochastic processes for two-dimensional space partition
rather than intuitively time series events modeling. Therefore, an alternative
deﬁnition for Bayesian nonparametric learning is given as follow
Deﬁnition 2 Bayesian nonparametric learning (BNL) is to build and inference
the probabilistic models for speciﬁc data structures or tasks based on stochastic
processes and their manipulations.
From the above deﬁnition, it can be seen that the core of BNL is to manipu-
late stochastic processes according to the target task. Therefore, the stochastic
processes currently used in this area is reviewed with their properties in the next
section followed by developed manipulations on them.
18

2.2. BASIC INGREDIENTS: STOCHASTIC PROCESSES
Chapter 2
Figure 2.1: The relations between stochastic processes.
2.2
Basic ingredients: stochastic processes
In probability theory, a stochastic process is a collection of random variables.
According to diﬀerent properties of these random variables, there are a number
of diﬀerent stochastic processes deﬁned in the literature. In the following, only
the ones commonly adopted in BNL for the machine learning community are
reviewed. Some important properties are summarized in Table 2.1, and their
relations have been illustrated in Fig. 2.1.
— Poisson Process (PP). (Kingman, 1992) A poisson process Π ∼PP(G0) on
the product space Z+ ×Θ, where G0 is a base measure over Θ. For any subset
A ⊂Θ, Π(Ω) ∼Poisson(G0(Ω)) and {Π(Ωi)}N
i=1 are independent with each
19

Chapter 2
2.2. BASIC INGREDIENTS: STOCHASTIC PROCESSES
other for any disjoint subsets {Ωi}N
i=1. The Levy measure of PP is
ν(dθ, dλ) = δ1(dλ)G0(dθ)
(2.1)
where δ1(dλ) is a unit point mass at λ = 1. An important property of PP
is the equivalence with completely random measures (CRM) (i.e., any draw
from a PP is a completely random measure) (Kingman, 1967). This property
places it at the core of BNL. A number of stochastic processes in this area
have been proofed as special cases of CRM, and the manipulations of these
stochastic processes thank to the theorems of PP which will be discussed in
the following section.
— Dirichlet Process (DP). (Ferguson, 1973; Teh, 2010) DP is the pioneer and
foundation of the BNL. Its deﬁnition is as follow: A DP, which is speciﬁed by
a base measure H on a measurable space Θ and a concentration parameter α, is
a set of countably inﬁnite random variables that can be seen as a (probability)
measure on partitions from a random inﬁnite partition {Ω}∞
k=1 of Θ.
An
explicit representation for DP by stick-breaking process (Teh, 2010) is
G =

k
πkδθk
(2.2)
where 
k πk = 1. One property of DP is that: for any ﬁnite partition {Ω}K
k=1,
the variables (measures on these partitions) from DP satisfy a Dirichlet dis-
tribution parameterized by the measures from based measure H on relative
areas
(G(Ω1), G(Ω2), . . . , G(ΩK)) ∼Dir(αH(Ω1), αH(Ω2), . . . , αH(ΩK))
(2.3)
where G is a realization of DP(α, H) and Dir() denotes the Dirichlet distri-
20

2.2. BASIC INGREDIENTS: STOCHASTIC PROCESSES
Chapter 2
bution. Considering the property 
k πk = 1 of DP, it is normally used as the
mixing distribution in mixture models, which makes it the most active and
successful stochastic process in BNL.
— Beta Process (BP). (Hjort, 1990; Thibaux and Jordan, 2007) A beta process
B ∼BP(c, B0) is a positive Levy process whose Levy measure depends on two
parameters: c is a positive concentration parameter and B0 is a base measure
on Θ. If B0 is continuous, the Levy measure on Θ × [0, 1] of the BP is
ν(dθ, dp) = cp−1(1 −p)c−1dpB0(dθ)
(2.4)
A draw B is a set of points (θk, pk) ∈Θ × [0, 1] from a PP with base measure
ν and
B =

k
pkδθk
(2.5)
where δθk is a unit point mass at θk. A draw from BP is a list of (countablely
inﬁnite) probabilities (diﬀerent from DP, the summation does not need to be
one) which is commonly used as the prior for the rows of a matrix in factor
analysis (Tamara Broderick and Pitman, 2012). A three-parameterized version
is proposed in (Broderick et al., 2015).
— Bernoulli Process (BeP). (Kingman, 1992) Let B be a measure on Ω. Deﬁne a
Bernoulli process with hazard measure B, written X ∼BeP(B), as the Levy
process with Levy measure,
ν(dp, dω) = δ(dp)B(dω)
(2.6)
In summary, a Bernoulli process is similar to a PP, except that it gives weight
1 to points. BeP is normally used as the likelihood when BP is set as the
prior.
21

Chapter 2
2.2. BASIC INGREDIENTS: STOCHASTIC PROCESSES
— Gamma Process (GaP). (Roychowdhury and Kulis, 2015) A gamma process
Γ ∼GaP(c, Γ0) is a positive Levy process whose Levy measure depends on
two parameters: c is concentration parameter and G0 is the base measure.
The corresponding Levy measure on Θ × R+ of GaP is
ν(dθ, dr) = cr−1e−crdrG0(dθ)
(2.7)
A draw Γ is a set of points (θk, rk) ∈Θ × R+ from a PP with base measure ν
and
Γ =

k
rkδθk
(2.8)
Analogous to the relationship between Gamma and Dirichlet distributions, DP
could be seen as a normalized GaP, i.e., πk =
rk

j rj . For the computational
convenience, PP is usually selected as the likelihood when GaP is used as
prior.
— Negative-binomial Process (NBP). (Zhou and Carin, 2012; Zhou et al., 2012)
A NBP I ∼NBP(r, p) is parameterized by two parts: r is a shape measure
and p is the probability measure. It can also be represented as
X =

k
κkδθk, κk ∼NB(rk, pk)
(2.9)
NBP is an alternative as the likelihood for GaP prior. Comparing with PP,
NBP has better variance-to-mean ratio and overdispersion level than PP for
overdispersed count data (Zhou and Carin, 2015). Another prior for NBP is
the BP for p and it is proofed that NBP is conjugate with three-parameterized
BP (Broderick et al., 2015).
— Others. Except for the above processes, there exists other processes adopted
for BNL, such as Gumbel Process (GuP) (Maddison et al., 2014), Gaussian
22

2.3. MANIPULATIONS OF STOCHASTIC PROCESSES
Chapter 2
Process (GP) (Seeger, 2004), and Mondrian Process (MoP) (Roy and Teh,
2008; Balog and Teh, 2015). Since they are a little far from this study, they
are not given more details here.
2.3
Manipulations of stochastic processes
The aim of the BNL is to utilize the above stochastic processes to model the
data and then resolve the tasks. However, the modeling ability of any single
stochastic process is very limited, there are, therefore, a number of manipulations
on two or more stochastic processes developed in the literature account for more
complicated situations. This thesis summarises the existing manipulations in
the following.
2.3.1
Marginalization
According to de Finetti’s Theorem (Aldous, 1985), an exchangeable sequences
could be obtained through marginalizing Finetti mixing measure out as follows,
P(X1, X2, · · · , Xn) =

n

i=1
P(Xi|θ)P(dθ)
(2.10)
where P(·|θ) is named mixture measure and P(dθ) is named mixing measure.
For the BNL, the mixing measure is from a stochastic process, so marginalizing
out a stochastic process could obtain a new exchangeable measure. The two
examples of this marginalization manipulation are shown below.
— Chinese Restaurant Process (CRP). (Aldous, 1985) CRP is an example of
marginalization of DP through,
CRP(X) =

P(X|G)dG,
G ∼DP
(2.11)
23

Chapter 2
2.3. MANIPULATIONS OF STOCHASTIC PROCESSES
Table 2.1: Properties of stochastic processes. ‘Stick-breaking’ denotes if the process has a stick-breaking constructive
representation; ‘CRM’ denotes if the process is a completely random measure; ‘NRM’ denotes if the process is a
normalized random measure; ‘Power-law’ denotes if there is a version with power-law (cluster number to data
number) phenomenon.
Stochastic Process
Stick-breaking
CRM
NRM
Power-law
PP
√
DP
√
(Teh, 2010)
√
PYP
(Teh, 2006)
BP
√
(Tamara Broderick and Pitman, 2012)
√
BeP
√
GaP
√
(Roychowdhury and Kulis, 2015)
√
NBP
√
√
BNBP (with BP as based measure)
(Broderick et al., 2015)
HDP
√
(Wang et al., 2011)
√
HPYP
(Blunsom and Cohn, 2011)
IBP
√
(Teh, Görür and Ghahramani, 2007)
24

2.3. MANIPULATIONS OF STOCHASTIC PROCESSES
Chapter 2
where mixing measure G is from a DP. CRP could be seen as a random
partition of data points and its name comes from a metaphor for understanding
this process. It is normally used as a marginalized representation for DP.
— Indian Buﬀet Process (IBP). Another example is IBP which is a marginaliza-
tion of BP through,
IBP(X) =

P(X|B)dB,
B ∼BP
(2.12)
where mixing measure B is from a BP. The basic IBP is proposed in (Griﬃths
and Ghahramani, 2005, 2011). Its widespread popularity is due to its power
to generate a binary matrix with inﬁnite columns, especially suitable for the
factor analysis.
2.3.2
Layering
The base measure of a stochastic process could be a continuous or discrete mea-
sure while a realization from a stochastic process is also a random measure. It
is apparently possible to set a random measure from a stochastic process as the
base measure for another stochastic process, and this natural idea could enable
the statistical strength sharing between the random measures at the lower layer.
An illustrative example is document modeling. The upper layer random measure
could be understood as a topic pool and a lower measure for a document could
be seen as the topics in this document inherited from the topic pool. Existing
BNMs based on this manipulation are summarized as follow.
— Hierarchical Dirichlet Process (HDP). (Teh, 2006) HDP is built by piling a DP
above another DP, which deﬁnes a distribution over a set of random (prob-
ability) measures {Gd}D
d=1 and a global random probability measure G0 over
25

Chapter 2
2.3. MANIPULATIONS OF STOCHASTIC PROCESSES
Θ. The global measure G0 is distributed as a DP parameterized by a concen-
tration parameter α and a base (probability) measure H.
Gd ∼DP(αd, G0),
G0 ∼DP(α, H)
(2.13)
Meanwhile, each random measure, which Gd is conditionally independent with
others given G0, is also distributed as a DP with parameter αd and a base
probability measure G0. HDP is almost the ﬁrst and most successful model
using this manipulation so there are a lot of extensions developed based on this
model, including the supervised Dai and Storkey (2015), incremental Gao et al.
(2011), nested Paisley et al. (2015), adapted Zheng et al. (2014), tree Canini
and Griﬃths (2011), evolutionary Xu et al. (2008), and dynamic versions Ren
et al. (2008).
— Hierarchial Beta Process (HBP). (Thibaux and Jordan, 2007) Akin to HDP,
HBP is proposed so that the diﬀerent BPs could share a common base discrete
measure from a global BP through
Bd ∼BP(cd, B0),
B0 ∼BP(c, H)
(2.14)
Since BP is an eﬀective tool for the factor analysis, HBP could enable the factor
sharing during the factor analysis, so it has been successfully used for docu-
ment classiﬁcation, convolutional factor analysis (Chen et al., 2011), shared
subspace learning (Gupta et al., 2012), and image interpolation and denoising
(Zhou et al., 2011).
— Gamma-Negative Binomial Process (GNBP). Diﬀerent from the above, i.e.,
HDP and HBP, GNBP is not composed by single kind of processes (e.g., DP
and BP). GNBP (Zhou and Carin, 2015) is composed by two kinds of processes
26

2.3. MANIPULATIONS OF STOCHASTIC PROCESSES
Chapter 2
(i.e., GaP and NBP) as follow
Id ∼NBP(p, Γ0),
Γ0 ∼GaP(c, H)
(2.15)
More layers could be achieved by further assigning random measures to H
from another GaP.
— Beta-Negative Binomial Process (BNBP). Analogous to negative binomial dis-
tribution, there are two parameters for a BNP. While GNBP places a GaP to
one parameter, another layering manipulation BNBP (Broderick et al., 2015)
is to place a BP on another parameter as follow
Id ∼NBP(B0, r),
B0 ∼BP(c, H)
(2.16)
2.3.3
Superposition
Superposition is to combine two or more random measures together, like ‘plus’
operation. If the layering is seen as a vertical manipulation, the superposition
here could be seen as a horizontal manipulation.
— Superposition of PP. According to Superposition Theorem in (Kingman, 1992),
it is known that combining a set of independent Poisson processes yields a
new Poisson process whose mean measure is the sum of mean measures of the
individual ones as follows
Π1 ⊕· · · Πn ∼PP(μ1 + · · · + μn)
(2.17)
where Πn ∼PP(μn). Considering the relationship between PP and completely
random measures, this theorem is the foundation for the superposition of other
processes.
27

Chapter 2
2.3. MANIPULATIONS OF STOCHASTIC PROCESSES
— Superposition of DP. Inspired by the superposition of PP, the superposition
of DP is proposed in (Lin et al., 2010) as follows,
(c1, c2, · · · , cn) ∼Dir(μ1(Ω), μ2(Ω), · · · , μn(Ω))
c1G1 ⊕c2G2 ⊕· · · ⊕cnGn ∼DP(μ1 + μ2 + · · · + μn)
(2.18)
where {G} are a set of independent DPs.
— Superposition of normalized random measures (SNRM). DP is one case of
NRM. The more general superposition manipulation(Chen et al., 2012) is pro-
posed as follow: The superposition of n independent normalized random mea-
sures {μj}n
j=1 on Θ is as follow
μ1 ⊕· · · ⊕μn = c1μ1 + · · · + cnμn
(2.19)
where cn =
μn(Θ)

j μj(Θ). This more general deﬁnition extends this manipulation
to more situations.
2.3.4
Subsampling
Subsampling is to (randomly) select part of inﬁnite components in a random
measure from a stochastic process. When certain conditions are satisﬁed, the
selected components form a new random measure from an underlying process.
— Thinned PP. (Lin et al., 2010) Based on the Marking Theorem (Kingman,
1992), let Π ∼PP(μ) be a PP on the space Θ, and q : Θ →[0, 1] be a
measurable function. If independently drawing zθ ∈{0, 1} for each θ ∈Π
with P(zθ = 1) = q(θ), we can obtain a new PP as
Πt = {θ ∈Π : zθ = 1} ∼PP(qμ)
(2.20)
28

2.3. MANIPULATIONS OF STOCHASTIC PROCESSES
Chapter 2
where Πt still satisﬁes a PP on Θ with mean measure qμ.
— Thinned Completely Random Measures (TCRM). (Foti et al., 2013) Let Π =
∞
k=1 πkδθk be a CRM on Θ and q : Θ →[0, 1] be a measurable function.
For each point (θk, πk), deﬁne a bernoulli variable rk with P(rk = 1) = q(θk)
(independent with other {r}).
TCRM =
∞

k=1
rkπkδθk
(2.21)
TCRM is still a CRM with mean measure qμ. Note that PP is a special case
of CRM. TCRM generalizes this manipulation to all the CRM, such as BP
and NBP.
— Thinned DP. (Lin et al., 2010) Let G ∼DP(μ) be a DP on Θ and can be
represented by G = ∞
k=1 πkδθk and q : Θ →[0, 1] be a measurable function.
For each k, we independently draw rk through P(rk = 1) = q(θk),
Gt =

k:rk=1
π
′
kδθk ∼DP(qμ)
(2.22)
where π
′
k =
πk

j πj are the re-normalized coeﬃcients for the selected compo-
nents.
— Thinned Normalized Random Measures (TNRM). (Lin et al., 2010; Chen et al.,
2012, 2013) Given a NRM μ = ∞
k=1 πkδθk on Θ and a bernoulli variable
rk ∈[0, 1]. The TNRM is,
TNRM =

k:rk=1
π
′
kδθk
(2.23)
where π
′
k =
πk

j πj are the re-normalized coeﬃcients for the selected compo-
nents. Note that DP is a case of TNRM.
29

Chapter 2
2.3. MANIPULATIONS OF STOCHASTIC PROCESSES
2.3.5
Point-transition
Point-transition is to move the points of a random measure according to an
underlying probabilistic transition.
— Point-transition of PP. Based on Transition Theorem (Kingman, 1992), let
Π ∼PP(μ) be a PP on space Θ and T : Θ × FΘ →[0, 1] be a probabilistic
transition. A transformed measure (Lin et al., 2010) is
Πp = {T(θ) : θ ∈Π} = PP(Tμ)
(2.24)
— Point-transition of DP. (Lin et al., 2010) Let G = ∞
k=1 πkδθk ∼DP(μ) and
its point-transition is
Gp =
∞

k=1
πkδT(θk) ∼DP(Tμ)
(2.25)
— Point-transition of Normalized Random Measures (PNRM). (Chen et al., 2012)
Given a NRM μ = ∞
k=1 πkδθk on Θ, the point-transition of μ is to draw atoms
θ
′
k from a transformed base measure to yield a new NRM as
PNRM(μ) =

k
πkδθ′
k=T(θk)
(2.26)
where T is a transition kernel.
2.3.6
Nesting
With a partition of a space by a process in hand, nesting is to further partition
each area by another process. In other words, each component of a random
measure is attached an additional random measure.
30

2.4. SUPERVISED LEARNING
Chapter 2
— Nested Chinese Restaurant Process (nCRP) (Blei, Griﬃths and Jordan, 2010)
A root partition is from a CRP, and each component in this partition is further
attached a CRP. Keeping this procedure could form a tree with inﬁnite depth
and width.
2.4
Supervised learning
Sometimes data are with diﬀerent responses/covariants, such as the time tags,
geographic locations, GDPs of countries, network relations and so on. How to
model the relationship between data and their responses/covariants and applied
for future prediction tasks is called supervised learning. According to the method
to model above relationship, this thesis summarises the following two categories
of researches on supervised BNL.
— Generalized Linear Models (GLM)-based. The data are assigned to a number
of clusters and the responses (for classiﬁcation) of data are modeled by multi-
nomial logit models (also called ‘softmax’) (Shahbaba and Neal, 2009) or GLM
(Hannah et al., 2011). DP is used as prior for the weights {πk}∞
k=1 of clusters
and the parameters {θk}∞
k=1 for the multinomial logit model. Although there
is linear relation between data and responses in each cluster, multiple clus-
ters make it possible to capture non-linear relationship. This idea is further
extended for group data using HDP (Dai and Storkey, 2015; Zhang et al.,
2013).
— Covariant Space-based. The relationship between covariant space and data
is captured by dependent stochastic processes through: 1) setting the base
measure as a special stochastic process, such as single-variable stochastic pro-
cess (MacEachern, 1999) and multi-variable Gaussian process (Gelfand et al.,
31

Chapter 2
2.5. TREE STRUCTURE LEARNING
2005); 2) varying the stick-breaking procedure, such as linking stick weight-
s by stochastic process (Griﬃn and Steel, 2006) or kernel function (Dunson
and Park, 2008) and permutation stick breaks order (Griﬃn and Steel, 2006;
Chung and Dunson, 2009); 3) auxiliary poisson process on covariant space,
such as kernel beta process (Ren et al., 2011) and correlated normalized ran-
dom measure (Griﬃn et al., 2013); 4) revised seating mechanisms in CRP and
IBP, such as ddCRP (Blei and Frazier, 2011) and ddIBP (Gershman et al.,
2015). More discussions on this aspect can be found in the review (Foti and
Williamson, 2015).
2.5
Tree structure learning
Tree structure plays an important role in the machine learning due to its per-
vasive, so learning out a hidden tree from the data also becomes an important
branch of BNL. Comparing other eﬀorts on this task, BNL has the ability to
learn a tree without the bound of depth and width and to incorporate the new
coming data.
— nCRP-based. A tree is viewed as a nested sequence of partitions by nCRP. A
space is ﬁrst partitioned by a CRP and each area in this partition is further
partitioned into several areas.
In this way, a tree with inﬁnite depth and
branching could be generated. A datum (i.e., a document) is associated with a
path in the tree using DP or Flexible Martingale (Steinhardt and Ghahramani,
2012) prior by nCRP (Blei, Griﬃths and Jordan, 2010), and it could associated
with a subtree of the generated tree using HDP prior in nHDP (Paisley et al.,
2015) instead of a path.
— Stick-breaking-based. An iterative stick-breaking process is used to construct
a Polya Tree (PT) (R. Daniel Mauldin, 1992) in a nested fashion. A datum
32

2.6. NONPARAMETRIC EXTENSIONS
Chapter 2
is associated with a leaf node of the generated tree; Traditional stick-breaking
process is revised to generate breaks with tree structure and results to a Tree
Structured Stick Breaking (TSSB) (Ghahramani et al., 2010). A datum is
attached to a node in the generated tree.
— Diﬀusion-based.
Both Kingman’s coalescent (Teh, Daumé and Roy, 2007;
Kingman, 1982; Teh et al., 2011) and Dirichlet Diﬀusion Tree (DDT) (Neal,
2003a) deﬁne a prior for an inﬁnite (binary) tree. The idea is that the data
are generated by a diﬀusion procedure with several divergences during this
procedure. An additional time varying continuous stochastic processes (i.e.,
Markov process) is needed for the divergence control. A datum is placed at
the end of branches of diﬀusions. DDT is extended to more general structure:
multifurcating branches by Pitman Yor Diﬀusion Tree (PYDT) (Knowles and
Ghahramani, 2015) and to feature hierarchy by Beta Diﬀusion Tree (BDT)
(Heaukulani et al., 2014).
2.6
Nonparametric extensions
Considering the power of BNL on the model selection problem, a number of clas-
sical learning paradigms with this problem are extended to the nonparametric
versions, and these extensions have achieved very good performances on each
learning tasks. The idea is to build a Bayesian nonparametric prior through
stochastic processes with their manipulations as the model prior and the poste-
rior inference could select the ﬁnal one. This section names a few nonparametric
extensions on the classical models in machine learning area.
— Gaussian Mixture Model (GMM). GMM that is a convex combination of a (ﬁ-
nite) number of Gaussian distributions is originally proposed as the alternative
of single probability distribution for the density estimation task. Comparing
33

Chapter 2
2.6. NONPARAMETRIC EXTENSIONS
a single distribution, GMM could better ﬁt an unknown complex probabilistic
density. Inﬁnite GMM (Rasmussen, 1999) as the nonparametric extension of
GMM is proposed using DP, where the combining weights and the parameters
(i.e., mean and variance) of Gaussian distributions are given a DP prior. In
fact, this idea was ﬁrst developed in as DP mixture model (Ferguson, 1973).
— Latent Dirichlet Allocation (LDA). LDA (Blei et al., 2003) is a renowned
Bayesian probabilistic graphical model developed for document modeling. The
nonparametric extension of LDA is quit straightforward through replacing
the Dirichlet and multinomial distributions with DP and multinomial process
(Teh, 2006). Another extension LIDA (Archambeau et al., 2015) is to replace
Dirichlet distribution by a four-parameter IBP, which allows the number of
words to increase with the coming of new documents.
— Hidden Markov Model (HMM). HMM (Rabiner, 1989) is a classical and suc-
cessful model for the time-series data with Markov property. The nonpara-
metric extension of HMM mainly relies on HDP. It is composed by three
components: a starting state, a state transition matrix, and an observation
matrix. The dimension of sate transition matrix -the number of hidden states-
needs to be ﬁxed in advance in traditional HMM. HDP-HMM (Teh, 2006) is
the ﬁrst nonparametric extension by the help of HDP, where each row in state
transition matrix is seen as stick weights from DP and the observation from
a state is seen as the atoms/parameters from DP. Another three variations
are: Sticky HDP-HMM (Fox et al., 2011b) for increasing the probability of
stating at a state; HDP-HsMM (Johnson and Willsky, 2013) for semi-Markov
situations; and iFHMM (Gael et al., 2008) for factorial situations.
— Nonnegative Matrix/Tensor Factorization (NMF/NTF). The nonparametric
extension of NMF/NTF mainly relies on the machinery of IBP or BP. The
34

2.6. NONPARAMETRIC EXTENSIONS
Chapter 2
initial idea is to factor the data matrix or tensor into a binary (mask) matrix
and a factor matrix, and binary (mask) matrix is given a inﬁnite prior, such as
IBP (Griﬃths and Ghahramani, 2005; Wood and Griﬃths, 2006; Ding, Xiang,
Molloy, Li et al., 2010) and BP (Liang et al., 2013). Another category of ex-
tensions is to assign an inﬁnite prior (i.e., GaP) to the combination parameter
of likelihood function.
— Mixed Membership Stochastic Blockmodels (MMSB). MMSB (Airoldi et al.,
2008) is a famous model for the network/relational data. It deﬁnes a (ﬁnite)
number of blocks for the nodes in the network, and each node could belong to
these blocks with diﬀerent weights (i.e., mixed membership) and assumes that
the link between two nodes is determined by the memberships of nodes and
the relations between blocks. The (ﬁnite) number of blocks are extended to
be inﬁnite by HDP (Fan et al., 2015), where the mixed membership for each
node is with a sticky HDP-HMM (Fox et al., 2011b) fashion.
— Partially-Observable Reinforcement Learning (PORL). PORL is for decisions
making from incomplete information (i.e., domain knowledge) (Michael and
Jordan, 1994). According to the representation methods for the domain knowl-
edge, there are two categories for PORL: hidden variable-based and history-
based. For hidden variable-based repression, an eﬃcient tool for this task is
the Markov Decision Process (MDP) with ﬁnite number of hidden states. A
nonparametric extension (Doshi-Velez et al., 2015) for the MDP is based on
HDP-HMM, where hidden state transitions are modeled as the stick weights
from HDP but the base measure is composed by two parts: one is a function
for reward evaluation and the other is for observation generation. For history-
based representation, Deterministic Markov Models (DMM) (Mahmud, 2010)
are accepted as an eﬃcient tool and PYP (Doshi-Velez et al., 2015) is used as
35

Chapter 2
2.7. POSTERIOR INFERENCE
its nonparametric extension with similar fashion with HDP.
2.7
Posterior inference
After building appropriate Bayesian nonparametric models for the speciﬁc real-
world tasks, the next step is to infer the latent variables (more accurately, the
posterior joint distribution of latent variables) in the model with observed data
in hand. The most straightforward candidate for the Bayesian model posterior
inference is Markov chain Monte Carlo (MCMC) (e.g., Gibbs sampling) which
is widely adopted by the Bayesian models including Bayesian nonparametric
models. At the same time, there are some other alternatives actively existing in
the BNL literature, which are summarized as follow.
2.7.1
Slice sampling
While the (countable) inﬁnite nature enables BNL great modeling power, it also
presents a challenge for the model inference: the inﬁnite number of factors and
their weights make the posterior inference of the latent variables even harder.
One commonly work-around solution in BNL is to use a truncation method. The
truncation method (Fox, 2009; Willsky et al., 2009), which uses a relatively big
K† as the (potential) maximum number of topics, is widely accepted but brings
an approximation error. Another successful technique to resolve this problem
is: data/variable augmentation (Van Dyk and Meng, 2001; Tanner and Wong,
2010), also known as Slice Sampling (Damlen et al., 1999; Neal, 2003b). Next, the
existing Bayesian nonparametric models using slice sampling as model inference
method are summarised.
— Slice Sampling for DP. A realization from DP can be represented as G =
∞
k=1 πkδθk, where πk are (countable inﬁnite number of) stick weights. The
36

2.7. POSTERIOR INFERENCE
Chapter 2
slice sampling for DP (Kalli et al., 2011) is to introduce an auxiliary variable
ui ∼Unif(0, πki) which functions as an adaptive truncation for data i, and
then only need to sample π > ui for data i.
— Slice Sampling for BP. Instead of a ﬁxed range used in DP, a positive decay
function f : limk→∞f(πk) = 0 is used to control the support of the auxiliary
variable uk ∼Unif(0, f(πk)) (Broderick et al., 2015). Comparing the one
used for DP, the decay function is more ﬂexible and applicable to many other
models.
2.7.2
Variational inference
Although the methodology of Monte Carlo Markov chain (MCMC) could obtain
(in theory) the exact posterior distribution of latent variables, it is not eﬃcient
enough. One alternative for BNL is the variational inference, which uses a set
of (simpler and independent) variational distributions to approximate the real
posterior distribution. It transforms a posterior distribution inference problem
to a high-dimensional optimization problem. The work on variational inference
for BNL are summarized as follow.
— Ordinary Variational Inference. The variational inference for BNL is based
on the stick-breaking representation, and the latent variables in this represen-
tation include stick weights, atom parameters and data assignment indexes.
Since there are possible inﬁnite number of atoms from stochastic processes,
such as DP (Blei et al., 2006; Kurihara et al., 2006) and GaP (Roychowdhury
and Kulis, 2015), the truncation method is adopted here, and then there are
only a ﬁnite number of atoms which could be approximated by a ﬁnite number
of mean-ﬁeld (latent variable) distributions. Other similar works have been
applied for HDP (Blei et al., 2006), IBP (Doshi et al., 2009), and nCRP (Wang
37

Chapter 2
2.7. POSTERIOR INFERENCE
and Blei, 2009).
— Collapsed Variational Inference. As stated, there are normally three groups
of latent variables (i.e., stick weights, atom parameters and data assignment
indexes) in BNL. Sometimes stick weights could be marginalized out to make
the inferences of DP (Kurihara et al., 2007) and HDP (Teh, Kurihara and
Welling, 2007; Sato et al., 2012) more accurate and eﬃcient.
— Online/Stochastic Variational Inference.
Instead of coordinate-ascent opti-
mization which is not eﬃcient for large dataset because of full pass of the w-
hole data at each iteration, stochastic optimization (Wang et al., 2011; Bryant
and Sudderth, 2012) is used for the variational parameter update. At each
iteration, a number of data are sampled from the whole dataset and then the
variational parameters are updated. This optimization could improve the in-
ference eﬃciency because of the estimated noisy gradients of the variational
objective which are proofed as the natural gradients to the Kullback-Leibler
divergence objective. (Hoﬀman et al., 2013).
— Truncation-free Variational Inference. One problem of the above variational
inference methods is that the truncation is needed and brings additional ap-
proximation error, which is resolved by a locally collapsed method. The global
latent variables are marginalized out and the distribution of the local variables
could be sampled (Wang and Blei, 2012). This method could be also seen as
a combination of sampling and variational inference.
— Accelerated Variational Inference. Another idea to avoid truncation is Varia-
tional DP (Kurihara et al., 2006), where a tying assumption is given as: the
variational distributions of components larger than T are set as prior. T could
be adaptively increase as the decreasing of optimization objective. Anoth-
er kd-tree-based data organization is introduced to accelerate the speed of
38

2.7. POSTERIOR INFERENCE
Chapter 2
inference.
2.7.3
Scalability
Considering the exponentially increasing number of data in many areas, the a-
bility of handling this ‘big data’ also becomes a research direction for BNL. The
ides is to extend current inference algorithms particularly designed for single pro-
cessor/machine into parallel versions for multiple processors/machines. Existing
inferences algorithms for this problem are mainly categorized into the following
two parts based on their basis (i.e., MCMC and Variational).
— Parallel MCMC. A parallel MCMC is proposed for HDP (Smyth et al., 2009)
through an asynchronous method. The advantages are easy to incorporate new
data and processors and extremely fault-tolerant; the disadvantages is with
additional approximation. To overcome this disadvantage, another parallel
MCMC for DP, HDP, and PYP (Lovell et al., 2012; Williamson et al., 2013;
Dubey et al., 2014) is proposed based on the inverse-superposition of DPs and
each processor or machine handel a super-cluster (i.e., one of DPs). Data-based
super-clusters generation further improves the eﬃciency (Chang and Fisher III,
2013). Although above methods could use marginalized representations of DP
or HDP to avoid truncation, a slice sampling-based parallel MCMC (Ge et al.,
2015) is developed to explicitly sample the stick weights in DP and HDP.
— Parallel Variational Inference. The posterior distribution is decomposed for
the streaming and parallel inference, and a component identiﬁcation algorithm
is proposed to resolve the mismatch problem when merging diﬀerent variation-
al posteriors from diﬀerent processing nodes (Campbell et al., 2015). Instead
of obtaining the exact variational approximation of the real posterior distribu-
tion by combining subposteriors (from diﬀerent processing nodes) together, a
39

Chapter 2
2.8. APPLICATION SCENARIOS
Markov chain is designed to obtain the samples from the variational approx-
imation (Neiswanger et al., 2014), which could be seen as a combination of
variational and sampling.
2.7.4
Others
Some other inference methods applied for BNL are summarized as follow.
— Sequential Monte Carlo. Another inference method is sequential Monte Carlo
(also known as particle ﬁltering) which approximates the posterior distribution
by a large collection of samples (i.e., particles) which are propagated with
time and updated by sequential importance sampling. It has been applied
for time varying DP mixture (Caron et al., 2007), Beta-Binomial DP mixture
(MacEachern et al., 1999), general conjugate DP mixture (Fearnhead, 2004),
nonparametric Bayesian matrix factorization (Wood and Griﬃths, 2006).
— Power Expectation Propagation.
(Minka, 2004) It generalizes expectation
propagation and variational inference using a ﬂexible α-divergence. An ex-
ample is Nonparametric Bayesian Matrix Factorization (Ding, Xiang, Molloy,
Li et al., 2010).
2.8
Application scenarios
Although the initial study (Teh, 2006) of BNL in machine learning community
is applied for document modeling. As the development of this area, BNL has
attracted a lot of attention of researchers with diﬀerent backgrounds, and there-
fore many other applications, e.g., image processing and biostatistics, have been
considered by BNL. In fact, the increasing number of application scenarios is, in
40

2.8. APPLICATION SCENARIOS
Chapter 2
turn, prompting the further development of theories and techniques of this area.
A summary of the main ﬁelds of application of BNL is described here.
2.8.1
Text mining
The target of text mining is to understand document corpus. According to the
diﬀerence of target document corpus, applications on text mining are categorized
as follows,
— Single Corpus. This task aims to learn the shared knowledge across diﬀerent
documents in a corpus.
HDP (Teh, 2006) was designed to deal with this
problem by generating a global random measure and assigned each document
a random measure with global random measure as the base measure. However,
the word vocabulary needs to be ﬁxed in advance in HDP, which make it
incapable of incorporating new words when new documents are observed. This
constraint is further relaxed by Latent IBP Compound Dirichlet Allocation
(Archambeau et al., 2015) through a four-parameter IBP.
— Multiple Corpora. This task aims to learn the shared knowledge across diﬀer-
ent corpora. The ﬁrst attempt was conducted by three-level HDP (Teh, 2006)
which generated a global random measure and assigned each corpus a random
measure with global random measure as its base measure. Through this hier-
archical way, topics shared by diﬀerent corpora could be inferred. Except the
shared topics, the diﬀerence between topics at diﬀerent corpora is also learned
by Diﬀerential Topic Models (Chen, Buntine, Ding, Xie and Du, 2015) based
on PYP.
— Multiple Time-varying Corpora. This task aims to learn the shared knowledge
across time-varying corpora. Beyond the idea of HDP, an Evolutionary HDP
(Zhang et al., 2010) was proposed to deal with this task, which added another
41

Chapter 2
2.8. APPLICATION SCENARIOS
time-varying dependency between the random measures for diﬀerent corpora.
The base measure of a corpus is a convex combination of two measures: the
global one and the one at the former time stamp.
2.8.2
Natural language processing
The main applications of BNL in the ﬁeld of natural language processing are
detailed below.
— Word Segmentation. This task is to identify word boundaries in continuous
speech. CRP is used to capture the word sequence generation process (Gold-
water et al., 2009) with two options: if a novel lexical item arrives, generating
a phonemic form; if not, choosing an existing lexical form. This idea is further
applied HDP and achieves better performance than DP.
— Phrase Alignment. This task aims to ﬁnd frequent phrase pairs from bilingual
texts which could beneﬁt phrase-based translation systems. Since bilingual
texts do not come segmented and the number of aligned phrase pairs is un-
known, DP and HDP are adopted as the prior for the emergence of each aligned
phrase pair in a probabilistic model for this task (DeNero et al., 2008).
— Unsupervised Part-of-speech (PoS) Tagging. Pos tagging is to mark up the
words in a text by the corresponding part-of-speeches, which is the basis for
the text analysis. Inﬁnite HMM is adopted to model the word sequence by
the help HMM and the hidden states (i.e., PoS tags) are unbounded by the
help of HDP (Gael et al., 2009).
2.8.3
Computer vision
In the ﬁeld of computer vision (i.e., image processing and video processing), BNL
has been successfully applied to the following problems.
42

2.8. APPLICATION SCENARIOS
Chapter 2
— Image Interpolation. This task covers a lot of image processing, mainly includ-
ing image resize and remap. It tries to factor an image by a linear combination
of (ﬁnite number of) dictionary atoms (He et al., 2013) which are represented
by a matrix. BP and IBP (Zhou et al., 2009) are used as the prior for this
matrix to avoid to preﬁx the dictionary atom number with additional sparsity
property. A dependent HBP (Zhou et al., 2011) is proposed to incorporate
the patch positions (in an image) constraint.
— Motion Capture Segmentation. This task aims to identify the (ﬁnite number
of) hidden dynamic behaviors in multiple time series and each time series may
contain multiple dynamic behaviors. The mapping relationship between time
series and dynamic behaviors is expressed by a matrix which are given a BP
prior to make it inﬁnite (Fox et al., 2014).
— Background Subtraction. This task aims to identify the background from a
video stream. Background could be modeled by a probability density which
is better to be a multi-mode probability density for the dynamic background.
The appropriate number of modes in this distribution depends on the target
video stream. DP-based Gaussian Mixture Model (Haines and Xiang, 2014)
is proposed to allow mode number determined by the data.
2.8.4
Biology
As the massive amount of data available in Biology, i.e., clinical trials and
personalized medicines, BNL provides an eﬃcient paradigm in a fully model-
based probabilistic framework with highly ﬂexible and adaptable (Dunson, 2010).
This is extremely useful for biology data because of not only the highly multi-
dimensional property but also the lack of knowledge or conﬁdence on the para-
metric model establishing.
43

Chapter 2
2.8. APPLICATION SCENARIOS
— Brain MRI Tissue Classiﬁcation. Since MRI becomes an eﬀective and routine
diagnostic tool, the accurate automatic classiﬁcation of brain MRI could assist
the diagnose of doctors. Mixture model clustering algorithms have dominated
this task for a long time due to their relatively good performance. DP (da Sil-
va, 2007) and HDP (Jbabdi et al., 2009) are naturally used to extend these
algorithms to nonparametric setting and achieved even better performance.
— Positive Selection Detection. This task aims to detect the positive natural
selection from alignments of protein-coding DNA. Traditional methods assume
that the nonsynonymous/synonymous rate ratio at a site is considered as a
random variable satisfying a underlying distribution. DP is used as the prior
of the nonsynonymous/synonymous rate ratios of site clusters (Huelsenbeck
et al., 2006).
— Expressed Sequence Tag (EST) Analysis. It is a a fundamental tool for gene
identiﬁcation in organisms. This task aims to estimate how many new genes
can be detected in a future EST sample of given size and also to determine
the gene discovery rate. Such information is useful for establishing sequencing
eﬃciency in experimental design and for measuring the degree of redundancy
of an EST library (Lijoi et al., 2007; Dunson, 2010). PYP (Lijoi et al., 2007)
is used to estimate 1) the proportion of genes in the library; 2) the number
of new genes; 3) the probability of discovering new genes given additional
samples (Favaro et al., 2012).
2.8.5
Music analysis
An interesting application scenario for the BNL is the music analysis which aims
for music teaching, analysis of human perception of sounds, and design of music
search (Temperley, 2007; Ren et al., 2010). The music data is in the acoustic
44

2.9. SUMMARY
Chapter 2
waveforms.
Among a number of methodologies for music analysis, Bayesian
techniques have be found very eﬀective and they pave the way for the BNL for
the music analysis. The following applications are highlighted.
— Musical Similarity Computation. This task is to estimate the timbral sim-
ilarity between recorded songs, which could be further applied to music re-
trieval. HDP is used for this task through representing each song by a (stick
weight) posterior distribution. The similarity between two songs could be eval-
uated by the symmetrized KL divergence of two corresponding distributions.
Comparing the traditional single Guassian-based or GMM-based approach-
es, HDP-based approach achieved better performance (Hoﬀman et al., 2008).
Similar idea is also adopted by (Nakano et al., 2012), but a 2-dimensional tree
structure is learned to represent each song.
— Blind Source Separation. This task is to separate diﬀerent sources (i.e., in-
struments and persons) of sound from an audio. One challenge for this task is
that the number of sound sources is unknown in advance. A GP-based NMF
(Blei, Cook and Hoﬀman, 2010) is proposed to resolve this problem.
2.9
Summary
The BNL has become a hot topic in machine learning and an eﬃcient tool for the
model selection problem. This chapter has reviewed the current state-of-the-art
researches in this ﬁeld: the motivations and deﬁnitions, basic stochastic process-
es and their manipulations, the supervised learning and tree structure learning,
nonparametric extensions of classical models, the inference techniques for BNL,
and current application scenarios. The reviews have shown that BNL is experi-
encing its developing stage, so there is still a gap between current theories and
45

Chapter 2
2.9. SUMMARY
techniques with complicated real-world tasks, especially complicated text min-
ing tasks, such as document-word co-clustering and document network learning.
Therefore, this study aims to ﬁll this gap by contributing some new theories and
techniques to this ﬁeld, and applies these new theories and techniques on four
complicated text mining tasks.
46

Chapter 3
Bayesian Nonparametric Sparse
Nonnegative Matrix Factorization
for Document-word Co-clustering
3.1
Introduction
Nonnegative Matrix Factorization (NMF) is a renowned tool for unsupervised
learning which has been successfully applied in many research areas (Sandler
and Lindenbaum, 2011; He et al., 2011).
In text mining area, for example,
factorization on the document-word matrix can discover the hidden topics from
documents, and these topics can assist document clustering for organization or
browsing and word clustering for automatic construction of a statistical thesaurus
or enhancement of queries (Crouch, 1988).
When documents and words are
clustered together, it is called co-clustering task (Dhillon, 2001; Bichot, 2010).
Although NMF has experienced a boom, with signiﬁcant development in re-
cent years, the assumption that the dimensionality of factor matrices need to be
predeﬁned is blocking its usage in real-world applications. Therefore, it is more
47

Chapter 3
3.1. INTRODUCTION
reasonable and practical to automatically discover this number from the data. In
the Bayesian nonparametric learning context, IBP (Griﬃths and Ghahramani,
2005; Wood and Griﬃths, 2006; Ding, Xiang, Molloy, Li et al., 2010) can be
seen as the prior of a sparse matrix with an inﬁnite number of columns, which
has been adopted for the NMF through Y = A ∗XT = (V · Z) ∗XT where
Z is with a IBP prior.
Since only one factor matrix A is with sparsity and
nonparametric properties here, it is called Single Nonparametric Sparse NMF
(sIBP-NMF) in this chapter. Despite its success on some tasks (such as, factor
analysis), sIBP-NMF fails some broader tasks, such as co-clustering shown in
the Experiment section, which in fact is the merit of NMF comparing with ordi-
nary factor analysis. Therefore, the idea is to jointly assign two factor matrices
IBP priors (named doubly Nonparametric Sparse NMF in this chapter), thus
endowing both two factor matrices (A and X) with sparsity and nonparametric
properties.
In order to better and more ﬂexibly describe the correlations between corre-
sponding IBP columns than a closely related approach - GP-based dIBP (Williamson
et al., 2010), this study instead proposes a framework in which the correlation
function is used for the generation of the latent weights pairs μ(1)
j
and μ(2)
j
which
are responsible for the generation of Z(1)
j
and Z(2)
j
respectively. Compared with
(Williamson et al., 2010), this work allows both Z(1)
j
and Z(2)
j
to have much
greater ﬂexibility and variations in terms of their non-zero entries. To be spe-
ciﬁc, this chapter proposes two new dIBPs based on bivariate Beta distribution
and copula. Instead of correlating two IBPs at the binary matrices level, the two
proposed dIBPs correlate with the two IBPs at the very beginning (at the Beta
random variable level). This strategy results in the implementation of the dou-
bly sparse nonparametric NMF framework based on new dIBPs with a simple
model structure. Another advantage of the new dIBPs is that the data corre-
48

3.2. PRELIMINARY KNOWLEDGE
Chapter 3
lation is directly modeled by the parameters of the bivariate Beta distribution
and copula. This correlation is actually a measurement of the focus degree of
the hidden topics in the documents, which will be discussed with more details
in this chapter. Nevertheless, introducing bivariate Beta distribution and cop-
ula presents a challenge for the model inference. This chapter has given three
designed inference algorithms for three implementations of the nonparametric N-
MF framework: GP-based dIBP model, bivariate Beta distribution-based dIBP
model, and copula-based dIBP model. The experiments on the synthetic data
show the merits of this work comparing the traditional NMF, single IBP-based N-
MF, and GP-based NMF on the factorization eﬃciency, sparsity, and correlation
ﬂexibility. The experiments on the real-world datasets show that the proposed
models perform well on the document word co-clustering task without explicitly
predeﬁning the dimension number for the factor matrices, and the models based
on new dIBPs have better convergence rates than GP-based dIBP.
The rest of this chapter is organized as follows. Preliminary details of NMF
and IBP are brieﬂy introduced in Section 3.2. The dIBP-based doubly sparse
nonparametric NMF framework is proposed with three implementations in Sec-
tion 3.3, and Gibbs samplers are designed for the three models in Section 3.4.
Section 3.5 conducts a set of experiments on synthetic data and real-world tasks.
Lastly, Section 3.6 summarizes this chapter.
3.2
Preliminary knowledge
3.2.1
Sparse nonnegative matrix factorization
Sparse nonnegative matrix factorization (Ye et al., 2015) is favoured by re-
searchers in several areas due to its output: sparse representation of data. Sparse
representation discovers a limited number of components to represent data, which
49

Chapter 3
3.2. PRELIMINARY KNOWLEDGE
is an important research problem (Heiler and Schnörr, 2006). This sparse rep-
resentation is generally desirable because it can aid human understanding (e.g.,
with gene expression data), reduce computational costs, and obtain better gen-
eralization in learning algorithms. Sparsity is closely related to feature selection
and certain generalizations in machine learning algorithms (Kim and Park, 2008;
Yang et al., 2011).
Given a nonnegative matrix YM×N (extended to semi-nonnegative by (Ding,
Li and Jordan, 2010)), Nonnegative Matrix Factorization (NMF) aims to ﬁnd
two matrices AM×K and XN×K to minimize the following cost function
J =∥YM×N −AM×KXT
N×K∥
2
F + ∥AM×K∥1 + ∥XN×K∥1
(3.1)
where ∥· ∥F is the Frobenius norm, ∥· ∥1 is the ℓ1 norm and the elements of A
and X are with nonnegative constraint. The ℓ1 norm in the cost function is used
for the sparseness constraint.
NMF is widely used in many application scenarios. Take the document-word
co-clustering as an example. The input YM×N denotes the frequencies of N words
in M documents. The AM×K denotes the documents’ interests on K factors (i.e.,
topics), and XN×K denotes the words’ interests on K factors (i.e., topics). All
documents and words are projected into the same K-dimensional space by NMF.
Based on AM×K and XN×K, document-word co-clustering task can be ﬁnished.
One problem of NMF is to determine K. Normally, this variable is exper-
imentally adjusted within a range. Here, this study deﬁne the NMF without
predeﬁned K as Nonparametric NMF.
50

3.3. DOUBLY SPARSE NONPARAMETRIC NMF
Chapter 3
3.2.2
Indian buﬀet process
The Indian Buﬀet Process (IBP) (Griﬃths and Ghahramani, 2005, 2011) is de-
ﬁned as a prior for the binary matrices with an inﬁnite number of columns. The
graphical model is shown in Fig. 3.1(a). A stick-breaking construction for IBP
(Teh, Görür and Ghahramani, 2007) is proposed as
νj ∼Beta(α, 1), μk =
k

j=1
νj, zn,k ∼Bernoulli(μk)
(3.2)
where zn,k forms a matrix ZN×K, {νj} is a set of variables with a Beta distribu-
tion, α is the parameter of Beta distribution, and μk is the stick weight of column
k. The bigger μk is, the more ‘ones’ appear in the column k of the binary matrix
ZN×K. The ﬁnal number of K is determined by the data and the parameter α
of IBP.
Some important notations throughout this chapter are summarized in Table
3.1.
3.3
Doubly sparse nonparametric NMF
To set a prior for the inﬁnite matrices A and X in NMF and make sure the two
matrices have the same number of columns, this study applies the dependent
Indian Buﬀet processes as the prior for two binary mask matrices of A and X.
This section ﬁrst deﬁnes the Doubly Sparse Nonparametric NMF Framework
and then introduce its three implementations one by one. Finally, three models
are theoretically and empirically compared, and the meaning of the correlation
is also discussed.
51

Chapter 3
3.3. DOUBLY SPARSE NONPARAMETRIC NMF
Table 3.1: Notations in this chapter
Symbol
meaning in this chapter
M
the row number of Y
N
the column number of Y
K
the latent factor number
K†
the truncation level
Y
data matrix with size M × N
ym,n
the element of Y at m row and n column
A
factor matrix with size M × K
X
factor matrix with size N × K
am,k
the element of A at m row and k column
xn,k
the element of X at n row and k column
Z(1)
mask matrix for A with size M × K
Z(2)
mask matrix for X with size N × K
z(1)
m,k
the mask binary variable for element of A at m row and k column
z(2)
n,k
the mask binary variable for element of X at n row and k column
V (1)
loading matrix for A with size M × K
V (2)
loading matrix for X with size N × K
v(1)
m,k
the loading variable for element of A at m row and k column
v(2)
n,k
the loading variable for element of X at n row and k column
μ(1)
k
k-th stick weight of IBP for A
μ(2)
k
k-th stick weight of IBP for X
θ
model parameters for a bivariate Beta distribution or a copula
corr
correlation value
3.3.1
Doubly sparse nonparametric NMF framework
In the Doubly Sparse Nonparametric NMF Framework, the data matrix is mod-
eled as
Y = A ∗XT = (V (1) ⊙Z(1)) ∗(V (2) ⊙Z(2))T
(3.3)
where Z(1) and Z(2) are two binary matrices, V (1) and V (2) are loading matrices,
and ⊙denotes the Hadamard product. Next, Next, this study assigns these
52

3.3. DOUBLY SPARSE NONPARAMETRIC NMF
Chapter 3
α
μk
zn,k
(a) Indian Buﬀet Process
α
μk
Γ(1)
m,k
h(1)
m,k
z(1)
m,k
Γ(2)
n,k
h(2)
n,k
z(2)
n,k
gk
Σk
τ
V (1)
V (2)
X
A
Y
(b) Implementation by Gaussian Processes-based dependent Indian Buﬀet Processes
θ
μ(1)
k
μ(2)
k
z(1)
m,k
z(2)
n,k
τ
V (1)
V (2)
X
A
Y
(c) Implementations by Bivariate-based or Copula-based dependent Indian Buﬀet Processes
Figure 3.1: Graphical representations for (a) the original Indian Buﬀet Pro-
cess, (b) GP-based dependent Indian Buﬀet Processes and (c) Bivariate Beta
distribution-based or Copula-based dependent Indian Buﬀet Processes.
latent variables probability distributions as
am,k = v(1)
m,k · z(1)
m,k, v(1)
m,k ∼Gam(1, τ1)
xn,k = v(2)
n,k · z(2)
n,k, v(2)
n,k ∼Gam(1, τ2)
(3.4)
53

Chapter 3
3.3. DOUBLY SPARSE NONPARAMETRIC NMF
where Gam(·, ·) denotes the Gamma distribution and τ1 and τ2 are two param-
eters. It can be seen that am,k ≥0 and xn,k ≥0 are always satisﬁed under the
above probability distributions. With A and X in hand, the likelihood of the
model is deﬁned as
ym,n|am,k, xn,k ∼Exp(ym,n;

k
am,k · xn,k + ϵ)
(3.5)
where Exp(·) denotes the Exponential distribution and ϵ is a very small positive
number to make the parameter of Exponential distribution greater than zero.
The selection of the Exponential distribution is used to guarantee each element
of Y with support [0, +∞). The special parameters for the distributions are
designed to retain the desired expectations of these distributions. For example,
the expectation of distribution ym,n is 
k am,k · xn,k + ϵ.
Until now, the doubly sparse nonparametric NMF framework construction
is ﬁnished except for an appropriate prior for Z(1) and Z(2). In the following
subsections, three implementations of this framework are introduced using three
dIBPs.
3.3.2
Implementation by GP-based dIBP
The ﬁrst dIBP is proposed based on Gaussian Process (GP) (Williamson et al.,
2010).
In this GP-based dIBP, each stick weight μk is used to generated a
diﬀerent number of columns of binary matrices. For the NMF, this GP-based
dIBP is adopted as the prior for the matrices Z(1) and Z(2). The graphical model
is shown in Fig. 3.1(b), and the generative process is as
νj ∼Beta(α, 1), μk =
k

j=1
νj
(3.6)
54

3.3. DOUBLY SPARSE NONPARAMETRIC NMF
Chapter 3
where the {μk} are IBP sticks as in Eq. (3.2). This set of sticks is shared by two
binary matrices through
gk ∼GP(0, Σk)
h(1)
m,k ∼GP(gk, Γ(1)
m,k)
h(2)
n,k ∼GP(gk, Γ(2)
n,k)
Σk(t, t′) = σ2 exp(−(t −t′)2
s2
)
Γ(1) = Γ(2) = η2I
z(1)
m,k = δ{h(1)
m,k < F −1(μk|0, (Σk)(1,1) + (Γ(1)
m,k)(1,1))}
z(2)
n,k = δ{h(2)
n,k < F −1(μk|0, (Σk)(2,2) + (Γ(2)
n,k)(2,2))}
where GP(0, Σk) denotes a Gaussian process parameterized by a mean function
0 and a kernel function Σk. gk is a random draw from GP(0, Σk) which is in
turn set as the mean function of GP(gk, Γ(1)
m,k) and GP(gk, Γ(2)
n,k) where Γ(1)
m,k and
Γ(2)
n,k are identify matrices with a parameter η, so there is a hierarchy between
diﬀerent GPs where the correlation is captured. h(1)
m,k and h(2)
n,k are two random
draws from the corresponding GPs, respectively. F −1(·) is the inverse normal
cumulative distribution function, and δ{·} is an unit mass function. More details
can be found in (Williamson et al., 2010). Since there are only two binary matri-
ces, the GP is degenerated to a two-dimensional Gaussian distribution, and Σk is
equal to a 2 × 2 matrix. Note that there is no explicit correlation value between
Z(1) and Z(2) in this implementation. Although this model could be used to
implement a doubly sparse nonparametric NMF, it nonetheless is inadequate in
many matrix factorisation scenarios as the assumption that the total number of
non-zero entries are distributed identically does not hold universally. For exam-
ple, the number of non-zero factors of a Document column may be drastically
55

Chapter 3
3.3. DOUBLY SPARSE NONPARAMETRIC NMF
diﬀerent to the number of non-zero factors of a corresponding Word column in
a document-word matrix.
3.3.3
Implementation by bivariate Beta distribution-based
dIBP
There are diﬀerent ways to link two IBPs. The GP-based dIBP uses the same
set of sticks for diﬀerent binary matrices. Here, this chapter proposes another
method that links the initials of two IBPs, ν. In the original IBP, ν satisﬁes a
Beta distribution with parameter (α, 1) as in Eq. (3.2). Therefore, the innovative
idea is to ﬁnd a joint distribution (ν(1), ν(2)) with Beta distributions Beta(α1, 1)
and Beta(α2, 1) as marginal distributions. Following this idea, an intuitive can-
didate would be Dirichlet distribution.
However, there is a strictly negative
relation, ν(1) + ν(2) = 1, between the samples of the Dirichlet distribution, but
it hopes to preserve the freedom of two (ν(1), ν(2)).
Instead of the Dirichlet distribution, a bivariate Beta distribution (Olkin and
Liu, 2003) is adopted here, which probability density function is deﬁned as
p(r1, r2) = ra0−1
1
rb0−1
2
(1 −r1)b0+c0−1(1 −r2)a0+c0−1
B(a0, b0, c0)(1 −r1r2)a0+b0+c0
s.t.,
0 ≤r1, r2 ≤1,
a0, b0, c0 > 0
(3.7)
where a0, b0, c0 are three parameters of this distribution and B(a0, b0, c0) is the
normalization constant that is hard to be evaluated. One merit of this bivariate
Beta distribution is that two marginal distributions are
r1 ∼Beta(a0, c0),
r2
∼Beta(b0, c0)
(3.8)
which satisﬁes the requirement about Beta marginal distributions of ν(1) and ν(2).
56

3.3. DOUBLY SPARSE NONPARAMETRIC NMF
Chapter 3
Another merit is that this distribution could model positive correlation between
(r1, r2) with range [0, 1] adjusted by the three parameters (a0, b0, c0) compared
to the strictly negative correlation from the Dirichlet distribution. Here, set c0
of the bivariate Beta distribution to 1 because it must ensure that the marginal
distribution of each ν is a Beta distribution with parameter form (α, 1). This
condition is for the distribution of the generated binary matrices that satisfy the
IBPs. Considering Eq. (3.8), give c0 a ﬁxed value. Even with a ﬁxed value for
c0, the bivariate Beta distribution in Eq. (3.7) is still able to model diﬀerent
correlations of two variables. In fact, the correlation is a function of a0 and b0
when c0 is ﬁxed
corr =

a0b0(a0 + 2)(b0 + 2) ·

Γ(a + 2)Γ(b + 2)
(a + b + 1)Γ(a + b + 2)F −1

(3.9)
where
F =3F2(a0 + 1, b0 + 1, a0 + b0 + 1; a0 + b0 + 2, a0 + b0 + 2; 1)
(3.10)
is Hypergeometric function1 that could be evaluated given parameters.
The
derivative detail of this equation is as follows.
At ﬁrst, the expectations and variances for r1 and r2 of bivariate Beta distri-
bution in Eq. (3.7) with c0 = 1 are
E[r1] =
a0
a0 + 1, V[r1] =
a0
(a0 + 1)2(a0 + 2)
E[r2] =
b0
b0 + 1, V[r2] =
b0
(b0 + 1)2(b0 + 2)
(3.11)
1https://en.wikipedia.org/wiki/Generalized_hypergeometric_function
57

Chapter 3
3.3. DOUBLY SPARSE NONPARAMETRIC NMF
and
E[r1r2] =
a0b0Γ(a0 + 1)Γ(b0 + 1)
(a0 + b0 + 1)Γ(a0 + b0 + 2)
3F2(a0 + 1, b0 + 1, a0 + b0 + 1; a0 + b0 + 2, a0 + b0 + 2; 1)
(3.12)
Then,
corr(r1, r2) =
cov(r1, r2)

V[r1]V[r2]
= E[r1r2] −E[r1]E[r2]

V[r1]V[r2]
(3.13)
where corr(r1, r2) is the correlation between r1 and r2; cov is the covariance.
After substitution with above equations, Eq. (3.9) is obtained.
For example, when a0 = 2.5 and b0 = 4, the correlation between r1 and r2 is
0.995; when a0 = 0.05 and b0 = 0.1, the correlation between r1 and r2 is 0.080.
With the desired bivariate Beta distribution in hand, the BiBeta-based dIBP
could be built as
(ν(1)
k , ν(2)
k ) ∼biBeta(θ), θ : {a0, b0, c0 = 1} > 0
z(1)
n,k
i.i.d
∼
Bernoulli(μ(1)
k ), μ(1)
k
=
k

j=1
ν(1)
j
z(2)
n,k
i.i.d
∼
Bernoulli(μ(2)
k ), μ(2)
k
=
k

j=1
ν(2)
j
(3.14)
where biBeta(θ) denotes the bivariate Beta distribution in Eq.
(3.7) and θ
denotes the parameters of the distribution. The graphical model is shown in
Fig. 3.1(c).
Although the bivariate Beta distribution-based dIBP has extended the free-
dom of the relation between ν(1) and ν(2), the relation is restricted to positive
relations in bivariate Beta distribution.
Next, this study uses the copula to
capture more freedom of relations.
58

3.3. DOUBLY SPARSE NONPARAMETRIC NMF
Chapter 3
3.3.4
Implementation by copula-based dIBP
0
0.5
1
0
0.2
0.4
0.6
0.8
1
0
0.5
1
1.5
2
ρ=−1
0
0.5
1
0
0.2
0.4
0.6
0.8
1
0
0.5
1
1.5
2
ρ=1
0
0.5
1
0
0.5
1
0
0.5
1
1.5
2
ρ=0
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2
Figure 3.2: FGM copula density surfaces with diﬀerent values of ρ = {−1, 0, 1}
and the marginal distributions are both Beta distributions: Beta(1, 1) (means
that α1 = α2 = 1 in Eq. (3.15)).
Copula (Trivedi and Zimmer, 2007) is another alternative for linking two
variables with given marginal distributions, and is used to deﬁne a joint distri-
bution for variables with known marginal distributions in statistics. Here, this
study selects the Farlie-Gumbel-Morgenstern (FGM) Copula (Beare, 2010) as an
example.
The deﬁnition of the FGM Copula is
Cρ(o1, o2) = o1o2 + ρo1o2(1 −o1)(1 −o2)
cρ(o1, o2) = 1 + ρ(2o1 −1)(2o2 −1)
ρ ∈[−1, 1]
(3.15)
where Cρ(o1, o2) is the joint cumulative distribution function, cρ(o1, o2) is the
joint probability density function, ρ is the parameter of the FGM copula, and o1
and o2 are cumulative distribution function values of two marginal distributions
that are known in advance. For the problem, two variables are ν(1) and ν(2) and
their marginal distributions are Beta distributions with parameters (α1, 1) and
59

Chapter 3
3.3. DOUBLY SPARSE NONPARAMETRIC NMF
(α2, 1), so o1 and o2 are deﬁned as
o1 = F beta(ν(1)) ∼(ν(1))α1,
f beta(ν(1))
∼α1 · (ν(1))α1−1
o2 = F beta(ν(2)) ∼(ν(2))α2,
f beta(ν(2))
∼α2 · (ν(2))α2−1
(3.16)
where F beta(ν(1)) and f beta(ν(1)) represent the cumulative distribution function
and probabilistic density function of ν(1).
In Copula, the correlation of ν(1) and ν(2) is modeled or reﬂected by the value
of ρ. As shown in Fig. 3.2, the diﬀerent correlation (i.e., positive or negative)
can be captured by the diﬀerent value of ρ. The Spearman correlation can be
evaluated by corr = ρ
3. Since the support of ρ is [−1, 1], the correlation range
that can be modelled by the FGM Copula is [−1
3, 1
3]. In particular, there is no
correlation if ρ = 0.
Copula-based dIBP is deﬁned by replacing the bivariate Beta distribution in
Eq. (3.14) with joint distribution deﬁned by the FGM copula
p(ν(1)
k , ν(2)
k ) = cρ(θ),
θ : {ρ ∈[−1, 1], α1 > 0, α2 > 0}
(3.17)
The graphical model is the same as the BiBeta-based dIBP in Fig. 3.1(c) but
with diﬀerent parameters θ.
3.3.5
Models discussion
Comparing with GP-based dIBP, one advantage of bivariate Beta distribution-
based and copula-based dIBP is that less latent variables are involved. This is
easily observed through the graphical models in Fig. 3.1. More latent variables
will slow down the convergence of the model inference. Comparing the bivariate
Beta distribution, the advantage of Copula is that the cumulative distribution
function and probability density function of (ν(1)
k , ν(2)
k ) could be easily obtained,
60

3.3. DOUBLY SPARSE NONPARAMETRIC NMF
Chapter 3
Figure 3.3: Illustration of the meaning of learned correlation. The red/solid
curve denotes a focused distribution on words from a large negative correlation;
the blue/dashed curve denotes a non-focused distribution on words from a large
positive correlation.
but it is only known that the exact form of probability density function of the
bivariate Beta distribution. This will impact on the model inference, which will
be introduced later.
What does the correlation from bivariate beta distribution-based dIBP NMF
and Copula-based dIBP NMF reveal from the data? This study explains its
meaning using document-word matrix as an example here. After the factoriza-
tion, it can obtain two factor matrices AM×K and XN×K. The K hidden factors
are also named topics when considering the document datasets. It is known
that μ(1)
k
and μ(2)
k
accounts for the generations of Z(1)
k
and Z(2)
k
that are the k-th
column of Z(1)
M×K and Z(2)
N×K. If μ(1)
k
is larger, the number of ‘ones’ in Z(1)
k
is
greater; the same for μ(2)
k
and Z(2)
k . When there is a large negative correlation
between μ(1)
k
and μ(2)
k , the number of ‘ones’ in Z(2)
k
is smaller if the number of
61

Chapter 3
3.4. MODEL INFERENCE
‘ones’ in Z(1)
k
is larger. It means that the commonly used topics of documents
(the number of ‘ones’ in Z(1)
k
is large) tend to be focused topics which only fo-
cus on seldom words (the number of ‘ones’ in Z(2)
k
is small), as illustrated by
red/solid curve/distribution on words in Fig. 3.3. On the contrary, when there
is a large positive correlation between μ(1)
k
and μ(2)
k , the number of ‘ones’ in Z(2)
k
is larger if the number of ‘ones’ in Z(1)
k
is also larger. It means that the commonly
used topics of documents (the number of ‘ones’ in Z(1)
k
is large) tend to be non-
focused topics which are with many words (the number of ‘ones’ in Z(2)
k
is also
large), as illustrated by blue/dashed curve/distribution on words in Fig. 3.3.
Therefore, it is concluded that the correlation can be seen as the measurement
of the focus degree of commonly-used hidden factors/topics in the dataset. On
the contrary, the desired topics (i.e., focus or non-focus topics) can be learned
from the documents just by ﬁxing this correlation during the algorithms.
3.4
Model inference
With data matrix Y in hand, the objective of this section is to estimate the hid-
den variables by a properly designed MCMC (Gibbs) sampler for their posterior
distribution, p(μ, Z, V, θ|Y ). It is diﬃcult to perform posterior inference under
inﬁnite mixtures, thus a common work-around solution in Bayesian nonpara-
metric learning is to use a truncation method. The truncation method, which
uses a relatively big K† as the (potential) maximum number of factors, is widely
accepted.
3.4.1
Inference for the GP-based dIBP
It needs to highlight that GP-based dIBP is not the contribution of this study.
The contribution here is to link the GP-based dIBP with the nonnegative matrix
62

3.4. MODEL INFERENCE
Chapter 3
factorization likelihood. The inference of this model is given here for the self-
contained purpose. The details can be found in (Williamson et al., 2010). The
conditional distributions are
Sampling μ
p(μk| · · · ) ∝μα
K
μk
2

t=1
Nt

n
(γt
k)zt
n,k(1 −γt
k)1−zt
n,k
(3.18)
where
γt
k = F(F −1(μk|0, Σ(t,t)
k
+ η2) −gt
k|0, η2)
(3.19)
where F() is normal cumulative distribution function.
Sampling g
p(gk| · · · ) ∝N(gk|0, Σk) ·

t
Nt

n
N(ht
n,k|gt
k, η2)
(3.20)
where N() denotes normal distribution.
Sampling h
p(ht
n,k| · · · ) ∝N(gt
k, η2)
(3.21)
with support
⎧
⎨
⎩
ht
n,k ∈(−∞, μt
k]
if zt
n,k = 1
ht
n,k ∈[μt
k, +∞)
if zt
n,k = 0
(3.22)
where
μt
k = F −1(μk|0, Σ(t,t)
k
+ η2)
(3.23)
Sampling s
P(s| · · · ) ∝Gam(s; hs, 1)
K

k
N(gk|0, Σk)
(3.24)
63

Chapter 3
3.4. MODEL INFERENCE
Sampling Z
p(z(1)
m,k = 1| · · · ) ∝γ1
k

n
Exp(ym,n;

l
v(1)
m,lz(1)
m,lv(2)
n,lz(2)
n,l + ϵ)
p(z(1)
m,k = 0| · · · ) ∝(1 −γ1
k)

n
Exp(ym,n;

l
v(1)
m,lz(1)
m,lv(2)
n,lz(2)
n,l + ϵ)
(3.25)
and
p(z(2)
n,k = 1| · · · ) ∝γ2
k

m
Exp(ym,n;

l
v(1)
m,lz(1)
m,lv(2)
n,lz(2)
n,l + ϵ)
p(z(2)
n,k = 0| · · · ) ∝(1 −γ2
k)

m
Exp(ym,n;

l
v(1)
m,lz(1)
m,lv(2)
n,lz(2)
n,l + ϵ)
(3.26)
where γt
k is same as in Eq. (3.19).
3.4.2
Update stick weights
When updating μk, it needs to ﬁnd its conditional distribution given μk−1 and
μk+1, because the order of μ must be maintained to make the marginal distribu-
tions of Z satisfy two IBPs. Based on M-H sampler, the acceptance ratio of a
new sample for μk = [μ(1)
k , μ(2)
k ] is
min

1, p(Z|μ∗
k) · p(μ∗
k|μk+1, μk−1)
p(Z|μk) · p(μk|μk+1, μk−1) × q(μk)
q(μ∗
k)

(3.27)
where q(·) is a proposal distribution which will be explained later; μ∗
k is a
new sample drawn from proposal distribution q(·); p(Z|μk) is the likelihood
of μk to generate kth column of binary matrix Z as in Eq.
(3.14) and the
p(μ∗
k|μk+1, μk−1) is the conditional probability density function of μ∗
k within the
range of [μk+1, μk−1], which will be diﬀerent when diﬀerent strategies in Section
3.3 are used to link μ(1)
k
and μ(2)
k .
Next, derive the conditional probability density function of p(μk|μk+1, μk−1)
64

3.4. MODEL INFERENCE
Chapter 3
as follows: For the ﬁrst column,
μ(1)
1
= ν(1)
1 ,
μ(2)
1
= ν(2)
1
(3.28)
and
p(μ(1)
1 , μ(2)
1 ) = p(ν(1)
1 , ν(2)
1 )
(3.29)
where p(·, ·) is the joint probability density function given by Eq. (3.7) or Eq.
(3.15) from two proposed dIBPs. For the second column,
μ(1)
2
= ν(1)
2 μ(1)
1 ,
μ(2)
2
= ν(2)
2 μ(2)
1
(3.30)
and
p(μ(1)
2 , μ(2)
2 ) = p(ν(1)
2
· μ(1)
1 , ν(2)
2
· μ(2)
1 )
= p(ν(1)
2 , ν(2)
2 )
|J1|
=
p

μ(1)
2 /μ(1)
1 , μ(2)
2 /μ(2)
1

μ(1)
1
· μ(2)
1
where J1 is the Jacobian matrix as
J1 =
⎡
⎣μ(1)
1
0
0
μ(2)
1
⎤
⎦
(3.31)
For the k-th column,
μ(1)
k
= ν(1)
k μ(1)
k−1,
μ(2)
k
= ν(2)
k μ(2)
k−1
(3.32)
65

Chapter 3
3.4. MODEL INFERENCE
and
p(μ(1)
k , μ(2)
k ) = p(ν(1)
k
· μ(1)
k−1, ν(2)
k
· μ(2)
k−1)
= p(ν(1)
k , ν(2)
k )
|Jk−1|
=
p

μ(1)
k /μ(1)
k−1, μ(2)
k /μ(2)
k−1

μ(1)
k−1 · μ(2)
k−1
where Jk−1 is the Jacobian matrix as
Jk−1 =
⎡
⎣μ(1)
k−1
0
0
μ(2)
k−1
⎤
⎦
(3.33)
To summarize, the conditional density of μk is
p(μk|μk−1, μk+1) ∝
p

μ(1)
k /μ(1)
k−1, μ(2)
k /μ(2)
k−1

μ(1)
k−1 · μ(2)
k−1
·
p

μ(1)
k+1/μ(1)
k , μ(2)
k+1/μ(2)
k

μ(1)
k
· μ(2)
k
(3.34)
Considering the support of μk, the proposal distribution, q(·), is set as the
product of two independent truncated Beta distributions
q(μk) = Beta

μ(1)
k ; α1
K†, 1

· Beta

μ(2)
k ; α2
K†, 1

μ(1)
k
∼Beta
 α1
K†, 1

, μ(1)
k
∈[μ(1)
k+1, μ(1)
k−1]
μ(2)
k
∼Beta
 α2
K†, 1

, μ(2)
k
∈[μ(2)
k+1, μ(2)
k−1]
It is quite easy to sample the truncated Beta distribution because Beta distri-
bution is a standard distribution.
66

3.4. MODEL INFERENCE
Chapter 3
3.4.3
Update binary matrices
Two binary matrices, Z : {Z(1), Z(2)}, can be updated separately. Each element
in two matrices satisﬁes a Bernoulli distribution with the following conditional
probabilities
p(z(1)
m,k = 1) ∝μ(1)
k

n
Exp(ym,n;

l
am,l · xn,l + ϵ)
∝μ(1)
k

n
Exp(ym,n;

l
v(1)
m,lz(1)
m,lv(2)
n,lz(2)
n,l + ϵ)
(3.35)
and
p(z(1)
m,k = 0) ∝(1 −μ(1)
k )

n
Exp(ym,n;

l
am,l · xn,l + ϵ)
∝(1 −μ(1)
k )

n
Exp(ym,n;

l
v(1)
m,lz(1)
m,lv(2)
n,lz(2)
n,l + ϵ)
(3.36)
and
p(z(2)
n,k = 1) ∝μ(2)
k

m
Exp(ym,n;

l
am,l · xn,l + ϵ)
∝μ(2)
k

m
Exp(ym,n;

l
v(1)
m,lz(1)
m,lv(2)
n,lz(2)
n,l + ϵ)
(3.37)
and
p(z(2)
n,k = 0) ∝(1 −μ(2)
k )

m
Exp(ym,n;

l
am,l · xn,l + ϵ)
∝(1 −μ(2)
k )

m
Exp(ym,n;

l
v(1)
m,lz(1)
m,lv(2)
n,lz(2)
n,l + ϵ)
(3.38)
where ϵ is small positive number. Since elements of Z are with discrete distri-
bution, it is easy to obtain samples.
67

Chapter 3
3.4. MODEL INFERENCE
3.4.4
Update loading matrices
Since the prior for V is Gamma distribution and the likelihood is exponential
distribution, the conditional distribution for V is
p(v(1)
m,k|z(1)
m,k, am,k) ∝Gam(v(1)
m,k; α1)

n
Exp(ym,n;

k
am,k · xn,k + ϵ)
∝Gam(v(1)
m,k; α1)

n
Exp(ym,n;

k
v(1)
m,kz(1)
m,kv(2)
n,kz(2)
n,k + ϵ)
(3.39)
and
p(v(2)
n,k|z(2)
n,k, xn,k) ∝Gam(v(2)
n,k; α2)

m
Exp(ym,n;

l
am,l · xn,l + ϵ)
∝Gam(v(2)
n,k; α2)

m
Exp(ym,n;

l
v(1)
m,lz(1)
m,lv(2)
n,lz(2)
n,l + ϵ)
(3.40)
Since the existence of the ϵ, the posterior of V is not a Gamma distribution, so
M-H sampler could be adopted to get its samples.
3.4.5
Update model parameter
In Fig. 3.1(c), the graphical model has parameter θ. For the diﬀerent strategies
to link (ν(1), ν(2)), the parameters must be diﬀerent. Therefore, it needs to design
corresponding update methods for the proposed two strategies: bivariate Beta
distribution and copula.
68

3.4. MODEL INFERENCE
Chapter 3
Bivariate Beta Distribution
The parameters of bivariate Beta distribution, θ : {a0, b0}, are given two Gamma
priors. The conditional distributions are
p([a0 b0]| · · · ) ∝Gam([a0 b0]; hp)
K

k=1
p(μ(1)
k , μ(2)
k |a0, b0)
(3.41)
where hp is the hyper-parameter of the prior for a0 and b0 and K is the number
of active columns of Z. The ‘active’ column means that there is at least one
element with 1 in that column.
Copula
There are three parameters for each copula, θ : {ρ, α1, α2}. Their conditional
distributions are
p([α1 α2]| · · · ) ∝Gam([α1 α2]; hp)
K

k=1
c(μ(1)
k , μ(2)
k )
(3.42)
where c(·, ·) is the copula density in Eq. (3.15). ρ of the FGM copula is given a
uniform distribution on its support [−1, 1], and its posterior is
p(ρ| · · · ) ∝
K

k=1
c(μ(1)
k , μ(2)
k |ρ)
(3.43)
After introducing update methods for all the latent variables, the inference
(i.e., Gibbs sampler) for the three models are summarized in Algorithm 3.1 for the
GP-based dIBP NMF (GP-dIBP-NMF) model, Algorithm 3.2 for the bivariate
Beta distribution-based dIBP NMF (BB-dIBP-NMF) model, and Algorithm 3.3
for the Copula-based dIBP NMF (C-dIBP-NMF) model.
69

Chapter 3
3.5. EXPERIMENTS
Algorithm 3.1: Gibbs Sampler for GP-dIBP-NMF
Input: Y
Output: A, X
initialization;
while i ≤maxiter do
// latent variables of dIBP
Update μ by Eq. (3.18);
Update Z by Eq. (3.25) and (3.26);
Update g by Eq. (3.20);
Update h by Eq. (3.21);
Update s by Eq. (3.24);
// latent variables of NMF
Update V by Eq. (3.39) and (3.40);
i + +;
return A and X;
3.5
Experiments
In this section, a series experiments on the synthetic data is conducted to show
the merits of this work comparing the traditional NMF, single IBP-based NMF,
and GP-based NMF on the the sparsity and nonparametric properties (Section
3.5.1), and correlation ﬂexibility (Section 3.5.2). A real-world task: document-
word co-clustering is conducted to show the usefulness of this work comparing
other models (Section 3.5.3).
3.5.1
Evaluation on sparsity and nonparametric properties
Randomly generate a matrix Y20×30 using the following procedure: 1) give a
vector [0.5, 0.4, 0.3, 0.2, 0.1] as the parameters of ﬁve Bernoulli distributions; 2)
randomly generate the elements of i-th column (i.e., A(:, i) and X(:, i)) of both
two matrices A20×5 and X30×5 using the Bernoulli distribution with i-th value of
above vector; 3) generate Y as the product of A and X as Y = AXT. Since the
values of parameters of Bernoulli distributions are small, the generated factor
70

3.5. EXPERIMENTS
Chapter 3
Algorithm 3.2: Gibbs Sampler for BB-dIBP-NMF
Input: Y
Output: A, X
initialization;
while i ≤maxiter do
// latent variables of dIBP
Update μ by Eq. (3.27);
Update Z by Eq. (3.35), (3.36), (3.37) and (3.38);
Update a0 and b0 by Eq. (3.41);
// latent variables of NMF
Update V by Eq. (3.39) and (3.40);
i + +;
return A and X;
matrices A and X tend to be sparse. Here, the matrix Y is used as the input
data for diﬀerent algorithms (i.e., traditional sparse NMF (sNMF) in Eq. (3.1),
single nonparametric sparse NMF (sIBP-NMF), and the proposed dIBP-NMF),
and then the sparsity of learned factor matrices from diﬀerent algorithms are
evaluated and compared. In order to quantitatively compare the sparsity from
diﬀerent algorithms, the following metrics are designed
SA =

m

k
1(A(m, k) = 0)
SX =

n

k
1(X(n, k) = 0)
(3.44)
where 1(·) is an indicator function parameterized by a condition and equals
to 1 if condition is satisﬁed; 0, otherwise. Here, the sparsity of A and X is
evaluated separately considering the sIBP-NMF. Note that the 0.00000001 acts
as a relax of the sparsity in the implementation. In the experiments, randomly
generate a number 100 of Y20×30 using above procedure. The results are shown in
Fig. 3.4, which has shown and compared the sparsity of learned factor matrices
A and X from diﬀerent algorithms. Note that the results are average of 100
71

Chapter 3
3.5. EXPERIMENTS
Algorithm 3.3: Gibbs Sampler for C-dIBP-NMF
Input: Y
Output: A, X
initialization;
while i ≤maxiter do
// latent variables of dIBP
Update μ by Eq. (3.27);
Update Z by Eq. (3.35), (3.36), (3.37) and (3.38);
Update α1 and α2 by Eq. (3.42);
Update ρ by Eq. (3.43);
// latent variables of NMF
Update V by Eq. (3.39) and (3.40);
i + +;
return A and X;
trials.
The x-axis denotes the number of factors for NMF and sNMF since
they need it as an input but sIBP-NMF and dIBP-NMF do not.
It can be
seen that the sNMF obtains more sparsity comparing NMF by the help of the
sparse constraint. Apparently, the sparsity is increasing as the number of factors
increasing too. Since sIBP-NMF and dIBP-NMF do not need the factor number
as an input, the values of SA and SX from two algorithms are equal in the
ﬁgure. For each trial, a distribution on factor number can be obtained as two
examples shown in Fig. 3.5 for sIBP-NMF and dIBP-NMF with 5,000 Gibbs
samples. The peak of the distribution is seen as the ﬁnal learned factor number
from the algorithm. The statistics on learned factor numbers from 100 trails are
shown in the right subﬁgure in Fig. 3.5. The averages are around the real one
(i.e., 5 in the generative procedure), which denotes the relatively accurate on
the factor learning (i.e., nonparametric property). Not surprisingly, sIBP-NMF
has good sparsity of A but not the X (the value of SA is large even comparing
the one from sNMF but SX is small). Its sparsity on A is due to its IBP prior.
Since there are two IBP priors for A and X in dIBP-NMF, the resultant A and
X are both sparse. Therefore, a conclusion could be drawn that the proposed
72

3.5. EXPERIMENTS
Chapter 3
0
5
10
15
20
K
0
50
100
150
200
250
300
350
SA
A
NMF
sNMF
sIBP-NMF
dIBP-NMF
0
5
10
15
20
K
0
100
200
300
400
500
SX
X
NMF
sNMF
sIBP-NMF
dIBP-NMF
Figure 3.4: Comparison of the sparsity on synthetic dataset between traditional
NMF (NMF), traditional sparse NMF (sNMF), single IBP-based sparse NMF
(sIBP-NMF), and doubly IBP-based sparse NMF (dIBP-NMF). Note that dIBP-
NMF here is based on bivariate beta distribution.
73

Chapter 3
3.5. EXPERIMENTS
1
2
3
4
5
6
7
8
K
0
500
1000
1500
2000
Number
K distribution from sIBP-NMF
sIBP-NMF
dIBP-NMF
1
2
3
4
5
6
7
8
K
1
2
3
4
5
6
7
8
9
K
0
500
1000
1500
2000
Number
K distribution from dIBP-NMF
Figure 3.5: Comparison of the learned topic number distribution on synthetic
dataset between single IBP-based sparse NMF (sIBP-NMF) and doubly IBP-
based sparse NMF (dIBP-NMF). Note that dIBP-NMF here is based on bivariate
beta distribution.
dIBP-NMF could obtain two sparse factor matrices but sIBP-NMF could only
obtain one, which will impact on the ability to do co-clustering task that will be
demonstrated in Section 3.5.4.
3.5.2
Evaluation on correlation ﬂexibility
As claimed, the proposed model is with more ﬂexibility to allow the numbers
of non-zero entries of factor matrices more diﬀerent from each other comparing
GP-based dIBP. In order to show this ﬂexibility, a metric needs to be designed
ﬁrstly to measure this ﬂexibility by comparing the number of non-zero entries
of two factor matrices (A and X) from the models: the mean of the diﬀerences
74

3.5. EXPERIMENTS
Chapter 3
Trials ID (order-irrelevance)
1
2
3
4
5
6
7
8
9
10
Difference of non-zero entry number
0
1
2
3
4
5
6
7
8
9
GP-dIBP-NMF
BB-dIBP-NMF
C-dIBP-NMF
Figure 3.6: Results on synthetic data to show the ﬂexibility of the diﬀerent
models. The x-axis denotes the trial IDs (order is irrelevant).
between the corresponding columns of A and X as
1
K
K

k=1
|N (1)
k
−N (2)
k |
where K is the number of columns of both matrices, N (1)
k
is the number of non-
zero entries of k-th column of A, and N (2)
k
is the number of non-zero entries of
k-th column of X. Apparently, the larger this metric is, the more ﬂexibility a
model has. There are ten randomly generated matrices with same size: 20 × 30,
and run three models on ten matrices. The designed metric has been evaluated
by on the learned factor matrices of diﬀerent models. As shown in Fig. 3.6, it can
75

Chapter 3
3.5. EXPERIMENTS
Table 3.2: Evaluation metrics for clustering
Jaccard Coeﬃcient
JC =
a
a+b+c
Folkes & Mallows
FM =
 a
a+b ·
a
a+c
1/2
F1 measure
F1 =
2a2
2a2+ac+ab
be seen that the metrics on BB-dIBP-NMF and C-dIBP-NMF are larger than
GP-dIBP-NMF in most trails and are also with more ﬂuctuations and larger
non-zero entry number diﬀerences comparing with GP-dIBP-NMF. It concludes
that the proposed dIBPs are more ﬂexible than GP-based dIBP.
3.5.3
Real-world task: document-word co-clustering
In this subsection, the proposed algorithms is applied to a real-world task:
document-word co-clustering. The real-world datasets2 used for this task are:
• Cora Dataset The Cora dataset consists of 2708 scientiﬁc publications
classiﬁed into one of seven classes. The dictionary consists of 1433 unique
words.
• Citeseer Dataset The CiteSeer dataset consists of 3312 scientiﬁc pub-
lications.
The dictionary consists of 3703 unique words.
The labels of
these papers are set as their research areas, such as AI (Artiﬁcial Intelli-
gence), ML (Machine Learning), Agents, DB (Database), IR (Information
Retrieval) and HCI (Human-Computer Interaction).
The above datasets are already with benchmarks for the document clusters but
without benchmarks for the word clusters. The co-occurrence relations are used
between words to generate a distance matrix through which a number (10, here)
2http://linqs.cs.umd.edu/projects/projects/lbc/
76

3.5. EXPERIMENTS
Chapter 3
0
20
40
60
80
100
0.1
0.15
0.2
0.25
factor number
value
JC
0
20
40
60
80
100
0.2
0.25
0.3
0.35
0.4
0.45
factor number
value
FM
0
20
40
60
80
100
0.2
0.25
0.3
0.35
0.4
0.45
factor number
value
F1
SNMF
BB−dIBP−NMF
GP−dIBP−NMF
D−dIBP−NMF
sIBP−NMF
C−dIBP−NMF
GP−dIBP−NMF
BB−dIBP−NMF
SNMF
sIBP−NMF
SNMF
BB−dIBP−NMF
C−dIBP−NMF
GP−dIBP−NMF
sIBP−NMF
Figure 3.7: Document-word co-clustering (document side) comparisons on Cite-
seer dataset between GP-dIBP-NMF, BB-dIBP-NMF, C-dIBP-NMF, sIBP-
NMF, and SNMF.
77

Chapter 3
3.5. EXPERIMENTS
0
20
40
60
80
100
0.65
0.7
0.75
0.8
0.85
0.9
0.95
factor number
JC
SNMF
BB−dIBP−NMF
C−dIBP−NMF
GP−dIBP−NMF
sIBP−NMF
0
20
40
60
80
100
0.75
0.8
0.85
0.9
0.95
1
factor number
FM
SNMF
BB−dIBP−NMF
C−dIBP−NMF
GP−dIBP−NMF
sIBP−NMF
0
20
40
60
80
100
0.75
0.8
0.85
0.9
0.95
1
factor number
F1
SNMF
BB−dIBP−NMF
C−dIBP−NMF
GP−dIBP−NMF
sIBP−NMF
Figure 3.8: Document-word co-clustering (word side) comparisons on Citeseer
dataset between GP-dIBP-NMF, BB-dIBP-NMF, C-dIBP-NMF, sIBP-NMF,
and SNMF.
78

3.5. EXPERIMENTS
Chapter 3
0
20
40
60
80
100
0.1
0.12
0.14
0.16
0.18
0.2
factor number
JC
0
20
40
60
80
100
0.2
0.25
0.3
0.35
0.4
factor number
FM
SNMF
bIBP−NMF
cIBP−NMF
gIBP−NMF
sIBP−NMF
0
20
40
60
80
100
0.2
0.25
0.3
0.35
0.4
factor number
F1
SNMF
bIBP−NMF
cIBP−NMF
gIBP−NMF
sIBP−NMF
SNMF
bIBP−NMF
cIBP−NMF
gIBP−NMF
sIBP−NMF
Figure 3.9: Document-word co-clustering (document side) comparisons on Co-
ra dataset between GP-dIBP-NMF, BB-dIBP-NMF, C-dIBP-NMF, sIBP-NMF,
and SNMF.
79

Chapter 3
3.5. EXPERIMENTS
0
20
40
60
80
100
0.4
0.5
0.6
0.7
0.8
factor number
JC
SNMF
BB−dIBP−NMF
C−dIBP−NMF
GP−dIBP−NMF
sIBP−NMF
0
20
40
60
80
100
0.65
0.7
0.75
0.8
0.85
0.9
factor number
FM
SNMF
BB−dIBP−NMF
C−dIBP−NMF
sIBP−NMF
GP−dIBP−NMF
0
20
40
60
80
100
0.65
0.7
0.75
0.8
0.85
0.9
0.95
factor number
F1
SNMF
BB−dIBP−NMF
C−dIBP−NMF
GP−dIBP−NMF
sIBP−NMF
Figure 3.10: Document-word co-clustering (word side) comparisons on Cora
dataset between GP-dIBP-NMF, BB-dIBP-NMF, C-dIBP-NMF, sIBP-NMF,
and SNMF.
80

3.5. EXPERIMENTS
Chapter 3
of word clusters can be obtained by spectral clustering algorithm. These word
clusters are seen as benchmarks. The evaluation metrics for clustering are Jac-
card Coeﬃcient (JC), Folkes&Mallows (FM) and F1 measure (F1).
Given a
clustering result,
• a is the number of two points that are in the same cluster of both bench-
mark results and clustering results;
• b is the number of two points that are in the same cluster of benchmark
results but in diﬀerent clusters of clustering results;
• c is the number of two points that are not in the same cluster of the two
benchmark results but are in the same cluster of clustering results.
and three metrics are computed the equations in Table 3.2 (bigger means better).
The comparisons with other models on the document-word co-clustering task
are conducted, including sparse NMF (SNMF) in Eq. (3.1), single IBP-based
sparse NMF (sIBP-NMF) (Griﬃths and Ghahramani, 2005; Ding, Xiang, Molloy,
Li et al., 2010), and three proposed models (GP-dIBP-NMF, BB-dIBP-NMF,
and C-dIBP-NMF). When applied to document-word co-clustering, the output
A and X from each above model could be considered as the new representations
of documents and words on latent factors. Based on these new representations,
a clustering algorithm (K-means can be used and, in this section, K is set as the
number of benchmark clusters) to do documents and words clustering. Since a
common algorithm (i.e., K-means) is adopted for all models, the performances
of models will be only determined by the learned new data representation A
and X. Before the analysis on the clustering results, it needs to compare the
convergence rates of these models. GP-dIBP-NMF takes about 1100 iterations
to get stable after the burn-in stage (i.e., ﬁrst 1000 iterations) on cora dataset,
while C-dIBP-NMF and BB-dIBP-NMF take 200 and 500 iterations respectively.
81

Chapter 3
3.5. EXPERIMENTS
For citeseer dataset, GP-dIBP-NMF takes about 1600 iterations to get stable
after the burn-in stage (i.e., ﬁrst 1000 iterations), while C-dIBP-NMF and BB-
dIBP-NMF take 400 and 700 iterations respectively. The eﬃciency is due to
the relatively simple model structures of the proposed model comparing the GP-
based model. The clustering results on Citeseer are shown in Fig. 3.7 and Fig.
3.8. Since SNMF needs the factor number as the input, this parameter is adjusted
for this model (the x-axis of each subﬁgure). Except for SNMF, other models
are all nonparametric models which do not need a factor number as an input,
so the results of these models are also plotted as horizontal lines in the ﬁgure.
The learned factor numbers are 9 (from sIBP-NMF), 10 (from BB-dIBP-NMF),
12 (from C-dIBP-NMF) and 12 (from GP-dIBP-NMF). The results on Cora are
shown in Fig. 3.9 and Fig. 3.10. The learned factor numbers are 10 (from sIBP-
NMF), 17 (from BB-dIBP-NMF), 15 (from C-dIBP-NMF) and 22 (from GP-
dIBP-NMF). From these results, it can been seen that: 1) there are ﬂuctuations
on the performance of SNMF on documents and words clustering due to its
factor number input. 2) although sIBP-NMF has relatively good performance
on the documents clustering, the performance on the words clustering is really
bad, which may be due to its single side sparse and nonparametric control. 3)
Comparing sIBP-NMF, the proposed three models all have better performances
on both documents and words clustering.
To summarize, without the prior
knowledge of the number of factors, the proposed algorithms achieve relatively
good performance on the document clustering task. Of the three algorithms,
BB-dIBP-NMF achieves the best performance in all.
82

3.6. SUMMARY
Chapter 3
3.6
Summary
Nonnegative matrix factorization is advantageous for many machine learning
tasks (e.g., co-clustering), but the assumption that the dimension of the factors is
known in advance makes NMF impractical for many applications. To resolve this
issue, this chapter has proposed a nonparametric sparse NMF framework based
on dIBP to remove the assumption. First, a model is built by implementing this
framework using GP-based dIBP, which successfully removes the assumption
but suﬀers from larger model complexity and less ﬂexibility. Then, two new
dIBPs has been proposed through a bivariate Beta distribution and a copula.
Advantages of the models based on new dIBPs is that 1) they have simpler
model structures than models with GP-based dIBP; 2) the correlation between
data could be directly learned out which could be seen as a measurement of the
focus degree of hidden factors/topics. Lastly, three inference algorithms have
been designed for the proposed three models, respectively. The experiments on
the synthetic and real-world datasets demonstrates the capability of the proposed
models to perform NMF without predeﬁning the dimension number, and more
correlation ﬂexibility comparing GP-based dIBP.
83


Chapter 4
Bayesian Nonparametric Deep
Topic Model for the Topic
Hierarchy Learning
4.1
Introduction
Any document collection is with a number of hidden topics as learned in Chapter
3. If the documents are scientiﬁc publications, then a topic may be a research
ﬁeld: Computer Vision, Text Mining or Image Segmentation. These topics can
be seen as a summarisation or abstraction of the whole document collection.
Apparently, the topics are with diﬀerent granularity which is not considered in
Chapter 3.
For example, Image Segmentation is a sub-area of research ﬁeld
Computer Vision, so Computer Vision is more general than Image Segmenta-
tion. Obtaining diﬀerent levels of summarisation or abstraction of a document
collection (named topic hierarchy) is of practice for many real-world applications
including but not limited to document retrieval, summarisation, browsing and
visualization. This study only sticks to the document modeling task throughout
85

Chapter 4
4.1. INTRODUCTION
the rest of this chapter, although the usage of the topic hierarchy can be eas-
ily extend to other domains (i.e., image processing and genetic analysis)(Blei,
Griﬃths and Jordan, 2010).
Considering the signiﬁcance of topic hierarchy learning, it has attracted much
attention of the researchers and then many state-of-the-art methods or tech-
niques have been proposed to resolve this issue: hierarchical clustering algorithm-
based (Chuang and Chien, 2004), linear discriminant projection-based (Li et al.,
2003), fuzzy relations-based (joon Kim and goo Lee, 2003), bag-of-related-words-
based (Rossi and Rezende, 2011), word relations-based (Zhu et al., 2013), and
so on. These methods construct topic hierarchy through document or words
clustering. This clustering strategy will make a hard assignment of documents
to topics, and especially words-based clustering will make the diﬀerent topics
exclusive with each other. Except for these techniques, topic models (Blei et al.,
2003; Blei, 2012) have been proofed a very successful tool for the document anal-
ysis in practice. But the traditional topic models only output a topic set without
hierarchy structure. Some revisions have been made to learn the topic hierarchy
based on the topic models, such as Tree Labeled LDA (Slutsky et al., 2013),
Hierarchically Labeled LDA (Perotte et al., 2011), Pachinko Allocation (Li and
McCallum, 2006; Mimno et al., 2007) and ﬁxed structure LDA (Reisinger and
Paşca, 2009). However, these extensions suﬀer from one drawback: the number
of topics need to be preﬁxed.
This chapter proposes a nonparametric deep topic model to learn the topic
hierarchy from a document collection based on idea of hierarchically dependent
random probability measures. The similar idea has already been used in Hierar-
chial Dirichlet Processes (HDP) (Teh, 2006), but the topics at upper layer will
be ‘copied’ to the lower layer in HDP. This is not a desired phenomenon, and it
hopes the topics at diﬀerent layers will be diﬀerent topics with diﬀerent granu-
86

4.2. PROBLEM STATEMENT
Chapter 4
larity. Therefore, a new hierarchical dependence strategy between two random
probability measures has been designed through constructing a Dirichlet Mixture
Probability Measure based on the random measure at the upper layer. Since this
new designed measure is a continuous measure, the probability of ‘copy’ behavior
is equal to zero. At the same time, this new designed measure takes the upper
topics as the modes, so the topics at lower layer will be around the these modes
with big probability. Another advantage of this model is to allow the number of
topics in each layer could be automatically dynamic adjusted according to the
input documents because of the nonparametric model nature. However, most
of the current nonparametric models (i.e., nested Chinese Restaurant Process
(nCRP) (Blei, Griﬃths and Jordan, 2010) and nested Hierarchial Dirichlet Pro-
cesses (nHDP)) use truncation-based method as an approximation. In order to
keep this ﬂexibility, a slice sampling inference algorithm which can be seen as an
adaptive truncation method is designed to learn the topics and their hierarchical
relations with the given documents. Experiments on the real-world documen-
t collections show the eﬃciency of the proposed model on the topic hierarchy
learning.
The rest of this chapter is organized as follows: Section 4.2 gives the basic
deﬁnitions of some concepts and states the problem formally; Section 4.3 intro-
duces the proposed model in detail; Section 4.4 presents the inference algorithm
and parameter estimation; Section 4.5 evaluates the proposed model through
experiments; and Section 4.6 summarizes this chapter.
4.2
Problem statement
Given a document collection D, it would be helpful to ﬁnd the word patterns in D
to assist further document analysis, such as document clustering, classiﬁcation,
87

Chapter 4
4.3. BAYESIAN NONPARAMETRIC DEEP TOPIC MODEL
visualization. These word patterns are commonly named ‘Topics’.
Deﬁnition 3 A Topic of a corpus D is a normalized vector with length of the
vocabulary size.
Deﬁnition 4 A Topic Hierarchy of a corpus D is a hierarchical structure con-
sisted by diﬀerent layers with a number of topics as nodes in each layer, and there
are also links between nodes/topics in neighboring layers. It has the following
properties:
• the topics at each layer are a summarization of the corpus;
• the topics at upper layers are more general than the ones at lower layers;
• the number of topics at upper layers are less than the ones at lower layers;
• each topic has one and only one parent topic at the neighboring upper layer.
Without preﬁxing the number of layers, the limit of the top layer will only
contain one topic considering the properties of the Topic Hierarchy. An example
of Topic Hierarchy is shown in Fig. 4.1. The ﬁnal aim of this chapter is to
automatically learn this Topic Hierarchy from document collection.
4.3
Bayesian nonparametric deep topic model
This section proposes a new Bayesian nonparametric model to learn a topic
hierarchy from a corpus. The basic ingredient of the proposed model is Dirichlet
Process,
Deﬁnition 5 A Dirichlet Process (DP) (Teh, 2010), which is speciﬁed by a base
measure H on a measurable space Θ and a concentration parameter α, is a set of
countably inﬁnite random variables that can be seen as the measures on partitions
88

4.3. BAYESIAN NONPARAMETRIC DEEP TOPIC MODEL
Chapter 4
Figure 4.1: An example of Topic Hierarchy. The orange eclipses denote 4 topics
at the ﬁrst layer; the pink eclipses denote 7 topics at the second layer; the cyan
eclipses denote 9 topics at the third layer. Data: abstracts of 11 papers from the
special issue on Brain Encoding in Pattern Recognition Journal. After removing
some stopwords, there are total 635 diﬀerent words and each abstract contains
about 100 words. α function: y = 5x and λ function is y = 5x.
from a random inﬁnite partition {Ω}∞
k=1 of Θ. For a ﬁnite partition {Ω}K
k=1, the
variables (measures on these partitions) from DP satisfy a Dirichlet distribution
parameterized by the measures from based measure H on relative partitions
G(Ω1), . . . , G(ΩK) ∼Dir(αH(Ω1), . . . , αH(ΩK))
(4.1)
where G is a realization of DP(α, H) and Dir denotes the Dirichlet distribution.
89

Chapter 4
4.3. BAYESIAN NONPARAMETRIC DEEP TOPIC MODEL
Furthermore, for each realization G of a DP, it can be equally expressed as
G ∼DP(α, H) =⇒G =
∞

k=1
πkδθk,
θk ∼H
(4.2)
where πk (
k πk = 1) is the stick weights on the atom θk, δθk() is an unit
mass function, i.e., δθk(ˆθ) = 1 if ˆθ = θk; δθk(ˆθ) = 0, otherwise, and {πk} can
be explicitly constructed through a stick-breaking process(Sethuraman, 1994).
From Eq. 4.2, it can be found that G can also be seen as a purely discrete
measure on the measurable space Θ concentrating on countable inﬁnite points.
For the documents modeling task, the measurable space Θ is the N-simplex
where N is the total number of words in the documents. Θ can also be simply
seen as a set of all the topics. {θ}∞
k , which can be seen as a countable inﬁnite
points on the N-simplex, denote a countable inﬁnite number of topics. Simulta-
neously, {π}∞
k denote the weights on the corresponding topics. Therefore, when
assigned to a document, a Gd = ∞
k=1 πd,kδθk denotes all the discussed topics
and their corresponding weights in this document.
The discrete nature of the realization G from DP has been successfully uti-
lized by Hierarchial Dirichlet Process (Teh, 2006) to share the same set of topics
across diﬀerent documents as
G0 ∼DP(α, H),
Gd ∼DP(α, G0)
(4.3)
where G0 has deﬁned a countable inﬁnite set of topics and is considered as
the based measure of the DP at the document level. Through this way, each
Gd of a document can and only can inherit some or all of them. This topic
sharing property is very useful in some tasks, but it is not desirable for the task:
topic hierarchy learning. Considering the case that HDP is used to learn topic
hierarchy, the topics in the top level will be inherited and emerge in the following
90

4.3. BAYESIAN NONPARAMETRIC DEEP TOPIC MODEL
Chapter 4
H1
G1
G2
G3
Gd
θ
wd,n
D
N
HDP
Topic Hierarchy
Figure 4.2: Graphical model for the generation of the proposed Deep Topic
Model with three level Topic Hierarchy. The circles denotes random measures
or random variables and the solid box denotes the repeat.
levels (of course, with diﬀerent weights in each level). Apparently, it does not
meet the objective that is to learn diﬀerent granular topics in the topic hierarchy.
Figure 4.3: Illustration of deep random partition and the relations between topics
on the simplex.
This chapter proposes a new nonparametric model to learn the topic hierarchy
based on a new designed inherit strategy. In order to use the realization from DP
as the base measure of the following layer, a new measure based on a realization
from DP is designed,
Deﬁnition 6 A Dirichlet Mixture Probability Measure (DMPM) is a continu-
ous probability measure on a measurable space Θ which is constructed based on
91

Chapter 4
4.3. BAYESIAN NONPARAMETRIC DEEP TOPIC MODEL
a realization from DP as
H(λ, G) =
∞

k=1
πkDir(λθk + 1),
G =
∞

k=1
πkδθk
(4.4)
where G is a realization of DP(α, H) and Dir denotes the Dirichlet distribution.
Corollary 1 As the increasing of the value of λ, the H(λ, G) will be close to G.
lim
λ→+∞H(λ, G) = G
(4.5)
Proof 1 Start with the limitations of the expectations and variances of the com-
ponents of DMPM: Dirichlet distributions. The expectation of the Dirichlet dis-
tribution Dir(λθk + 1) is
λθk,v+1
V
v=1 λθk,v+1, where V is the dimension of this Dirichlet
distribution (i.e., the vocabulary size for the topic model). Since θk is a topic
from another Dirichlet distribution, it is with the property: V
v=1 θk,v = 1 and
0 ≤θk,v ≤1. Therefore, the expectation
λθk,v+1
V
v=1 λθk,v+1 is equal to λθk,v+1
λ+V . Further-
more, when λ goes to inﬁnity, the expectation goes to
lim
λ→+∞
λθk,v + 1
λ + V
= θk.
Similarly, the variance of the Dirichlet distribution Dir(λθk + 1) is
(λθk,v + 1)[V
v=1(λθk,v + 1) −(λθk,v + 1)]
[V
v=1(λθk,v + 1)]2([V
v=1(λθk,v + 1)] + 1)
When λ goes to inﬁnity, the variance goes to
lim
λ→+∞
(λθk,v + 1)[V
v=1(λθk,v + 1) −(λθk,v + 1)]
[V
v=1(λθk,v + 1)]2([V
v=1(λθk,v + 1)] + 1)
= 0.
The zero variance means that the mass of the distribution totally concentrates to
92

4.3. BAYESIAN NONPARAMETRIC DEEP TOPIC MODEL
Chapter 4
the expectation θk. So, the distribution Dir(λθk + 1) becomes δθk, and then the
Eq. 4.5 is proofed.
The DMPM has broken the discrete nature of the G. On the measurable
space Θ, G has only countable inﬁnite points with mass, but these points are
only modes for the H(λ, G). The λ is a mass concentration parameter. In other
words, λ controls mass diﬀusion from the modes. As Fig. 4.4 shown, the larger
λ leads to small mass concentration around modes.
(a) λ = 10
(b) λ = 30
(c) λ = 60
(d) λ = 120
Figure 4.4: The eﬀect of the parameter λ for the DMPM. Here, G is a mixture
of ﬁnite number (three) of Dirichlet distributions with equal weights, and each
subﬁgure is an illustration of the H(λ, G) under diﬀerent value of λ.
Note that Dirichlet Mixture Model (Ma et al., 2014) (the ﬁnite version of
the DMPM) has been investigated and applied to text modeling (Blei, 2004)
93

Chapter 4
4.3. BAYESIAN NONPARAMETRIC DEEP TOPIC MODEL
and image processing (Bouguila et al., 2004). The diﬀerence between DMM and
DMPM is that DMPM is composed by countable inﬁnite number of Dirichlet dis-
tributions but DMM is composed by a ﬁnite number. This nature is important
for the proposed nonparametric model, because it can model possible inﬁnite
number of topics. Given a document set, this model could get proper number
of topics at each layer. The Inﬁnite Mixture of Inﬁnite Gaussian Mixtures (Y-
erebakan et al., 2014) is similar with us, as they both use HDP as a framework,
and try to convert a discrete measure back to a continuous measure. However,
the proposed approach uses the entire measure G, whereas their work uses just
a single atom.
The complete Deep Topic Model (DTM) (here this aim is to learn a topic
hierarchy with three layers and extending to topic hierarchies with more layers
is easy) is deﬁned as follows. The ﬁrst layer is
β = [1, 1, . . . , 1]
H1 = Dir(β)
G1 = DP(α(1), H1)
G1 =
∞

k=1
π(1)
k
· δθ(1)
k
π(1)
k
∼GEM(α(1))
θ(1)
k
∼H1
where H1 is a measure on N-simplex and the total mass is uniformly-distributed
on this space under H1 which is shown in Fig. 4.5(a). This base measure is
used as a non-informative prior for the topics at the ﬁrst layer when there is no
data. G1 is a realization from a DP with H1 as base measure in which θ(1)
k
is a
94

4.3. BAYESIAN NONPARAMETRIC DEEP TOPIC MODEL
Chapter 4
topic/node at the ﬁrst level. The second layer is
H2(λ(2), G1) =
∞

k
π(1)
k
· Dir(λ(2)θ(1)
k
+ 1)
G2 = DP(α(2), H2)
G2 =
∞

i=1
π(2)
i
· δθ(2)
i
π(2)
i
∼GEM(α(2))
θ(2)
i
∼H2
where H2 is a Dirichlet mixture probability measure parameterized by λ(2) and
G1.
Diﬀerent from HDP, H2 (One example is shown in Fig. 4.5(b)) is used as
the base measure for the second layer DP rather than G1. Considering the the
continuous nature of the DMPM, the topics θ(2)
i
at this layer will be (with prob-
ability one) diﬀerent from the ones θk in the ﬁrst layer, and they are furthermore
also diﬀerent from each other. At the same time, these topics will surround
the topics (the modes of the H2(λ(2), G1)) at the upper layer. Each new topic
has its own variance (departure) from the modes. Therefore, these topics can
be seen as the speciﬁed topics in a domain deﬁned by the modes/topics. One
example is that the modes/topics in the ﬁrst layer are: text mining and com-
puter vision. The topics in the second layers may be document clustering, text
summarization, image segmentation, multi-object tracking. It can be seen that
the topics at the lower layer will be more ﬁne-grained comparing the ones in the
upper layers, which just satisﬁes desired property of the Topic Hierarchy. An-
other interesting thing is the relations between the topics at the lower layer and
the ones at the upper layer. Apparently, discovering their relations could help
us ﬁnd the inheriting relation between them. In the upper example, the topic
95

Chapter 4
4.3. BAYESIAN NONPARAMETRIC DEEP TOPIC MODEL
document clustering and text summarization can be seen as the speciﬁcation of
the topic text mining; the topic image segmentation and multi-object tracking
can be seen as the speciﬁcation of the topic computer vision. This relation is left
to the following Section.
The third layer is
H3(λ(3), G2) =
∞

i
π(2)
i
· Dir(λ(3)θ(2)
i
+ 1)
G3 = DP(α(3), H3)
G3 =
∞

j=1
π(3)
j
· δθ(3)
j
π(3)
j
∼GEM(α(3))
θ(3)
j
∼H3
The idea is similar with the second layer, and one example is shown in Fig.
4.5(c). It needs to highlight again that the topics in the third layer will be more
ﬁne-grained than the ones in the second layer. Until now, we have obtained a
prior for the three-layer topic hierarchy: G1 →G2 →G3.
In order to learn this topic hierarchy from the document set, it needs to deﬁne
the likelihood of the topic hierarchy prior. The requirements of the likelihood
are: 1) diﬀerent documents should share the same set of topics θ(3)
j
in the third
layer; 2) the words in the same document should share the same set of topis; 3)
the number of topics within one document should be not too much. Considering
these requirements, we cannot directly use the G3 as the prior for all the docu-
ments because it may make one document contain too many topics and diﬀerent
documents may not share any topics. Therefore, use the traditional HDP to link
96

4.3. BAYESIAN NONPARAMETRIC DEEP TOPIC MODEL
Chapter 4
the G3 to the documents. Each document is generated by
Gd = DP(αd, G3)
Gd =
∞

t=1
πd
t · δθd
t
πd
t ∼GEM(αd)
xd,t ∼π(3)
zd,n ∼πd
wd,n ∼θxd,zd,n
Each document only inherits a number of topis in the third layer θ(3)
j .
The
complete Deep Topic Model is graphically shown in Fig. 4.2.
In the DTM, λ() and α() are two increasing functions with layer index as
inputs. λ() function is used to control the Concentration of the learned Topic
Hierarchy. With the increase of the λ, the topics in the lower layer will tend
to have smaller variance to the modes of the DMPM comparing the ones in
the higher layer. The lower layer will tend to have more partitions comparing
to higher layer. α function is used to control the Width of the learned Topic
Hierarchy. With the increase of the α, the number of nodes/topics in the lower
layers will be larger than the upper layers.
When considering each layer alone in Topic Hierarchy, we can seen it as a
Chinese Restaurant Process (another representation of DP)1. Suppose the num-
ber of topics in the third layer is ﬁxed as T3. Considering the expectation of CRP,
the number of topics in the second layer will be proportional to T2 ∝log T3, and
furthermore the number in the ﬁrst layer will be proportional to T1 ∝log log T3.
Apparently, the number of topics in the lower layer will tend to be larger than
1http://en.wikipedia.org/wiki/Chinese_restaurant_process
97

Chapter 4
4.4. MODEL INFERENCE
the number in the upper layer, which just meets the desired property of Topic
Hierarchy. In the extreme case, there is inﬁnite number of layers in the topic
hierarchy, and then the number of topics at the ﬁrst layer will be equal to one.
4.4
Model inference
With the designed Deep Topic Model and a document set, the goal of model in-
ference is to estimate the latent variables in the model assuming the documents
are generated according to generative procedure of the model. The deﬁnition
of the model has actually deﬁned a joint distribution of hidden variables and
observations. Given the observations (i.e., documents for DTM), we are inter-
ested in the conditional distribution of the latent variables conditioned on the
observation. For the proposed model, the distribution is
p(π(1), π(2), π(3), πd, θ(1), θ(2), θ(3), xi, xj, xd,t, xd,n|D)
(4.6)
Normally, this conditional distribution is complicated, high-dimensional, and
intractable, so some approximation methods should be adopted to obtain its
quantiles. Here, a Markov Chain Monte Carlo (MCMC) algorithm is developed
to approximate the posterior distribution of latent variables, which has been
widely used in the probability graphical model area (Blei, 2004; Blei, Griﬃths
and Jordan, 2010; Teh, 2006). Through the properly designed MCMC algorithm,
a bunch of samples of the target posterior distribution of latent variables can
be obtained after a burn-in stage (a number of iterations). With the obtained
samples, the quantiles of the target distribution can be estimated.
However, the inﬁnite number of topics and their weights make the posterior
inference of the latent variables even harder. One common work-around solution
in nonparametric Bayesian learning is to use a truncation method. Truncation
98

4.4. MODEL INFERENCE
Chapter 4
(a) Base measure H1 of the ﬁrst layer
(b) Base measure H2 of the second layer
(c) Base measure H3 of the third layer
Figure 4.5: Illustration of the base measures in Deep Topic Model
99

Chapter 4
4.4. MODEL INFERENCE
method (Fox, 2009; Willsky et al., 2009) is widely accepted, which uses a rela-
tively big K as the (potential) maximum number of topics. But there will be an
approximation error brought from the truncation. Another successful technique
to resolve this problem is developed: Data/Variable Augmentation (Van Dyk
and Meng, 2001; Tanner and Wong, 2010), which is also known as Slice Sam-
pling (Neal, 2003b; Kalli et al., 2011). The idea of this technique is to introduce
additional variables to make the sampling of the posterior distribution more easy.
For the Bayesian nonparametric model, this technique is even more importan-
t, which makes the posterior distribution with inﬁnite number latent variables
inferable. The diﬃcult part is how to design the additional variables which is
really model-dependent.
Next, a slice sampler for DTM is designed based on the technique devel-
oped for DP mixture model (Neal, 2003b; Kalli et al., 2011). The conditional
distributions of the latent variables are given as follows,
Sampling u(4)
d,n. Since Gd contains countable inﬁnite number of candidate
topics to be assigned to the words in a document. u(4)
d,n is an introduced additional
variable which can make the possible assigned topics to word n in document d
within a ﬁnite scope, which is also known as adaptive truncation.
u(4)
d,n ∼U(0, πd
t=xd,n)
(4.7)
where t = xd,n is the former assignment and U(0, πd
t=xd,n) denotes a uniform
distribution with support on [0, πd
t=xd,n].
Sampling xd,n. It is an indicator variable to express the assigned topic to
word n in document d. The posterior conditional distribution is
p(xd,n = t| · · · ) ∝1(u(4)
d,n < πd
t ) · θ(3)
xd,xd,n,n
(4.8)
100

4.4. MODEL INFERENCE
Chapter 4
where 1() is an indicator function and equal to 1 if the condition is satisﬁed;
otherwise, 0. The adaptive truncation is reﬂected by the sampling of the xd,n.
Only the topics with larger weights than u(4)
d,n are considered as the assignment
candidates. Since the value of u(4)
d,n is larger than zero, this number must be
ﬁnite.
Sampling πd
t .
The update of the stick weights of topics in a document
should consider the upper adaptive truncation (i.e., the inequalities between
slice variables and the stick weights). In (Neal, 2003b; Kalli et al., 2011), it has
been proofed that the stick weights satisfy truncated Beta distributions. For
t ≤t∗,
νd
t ∼beta(νd
t ; 1, αd) · 1(Lt < νd
t < Ut)
(4.9)
where
t∗= max
n {xd,n}
Lt = max
xd,n=t

u(4)
d,n

l<t(1 −νd
l )

Ut = 1 −max
xd,n>t

u(4)
d,n
νd
xd,n

l<xd,n,l̸=t(1 −νd
l )

(4.10)
where t∗is the maximum number of used weights by the words in a document.
Note that we could only update these weights (ﬁnite number) leaving the others
(inﬁnite number) un-updated, because the posterior of other weights are same
with their priors.
Sampling u(3)
d,t. It is the slice variable for the stick weights in the third layer.
u(3)
d,t ∼U(0, π(3)
j=xd,t)
(4.11)
Sampling xd,t.
It is an indicator variable to express the assigned topic
in the third layer to the t-th topic in document d. The posterior conditional
101

Chapter 4
4.4. MODEL INFERENCE
distribution is
p(xd,t = j| · · · ) ∝1(ud,t < π(3)
j ) ·

n∈d,t
θ(3)
xd,t,n
(4.12)
Note that the likelihood for xd,t is associated with all the words on the t-the
topic.
Sampling π(3)
j . The update of the stick weights of topics in the third layer is
similar with the ones in a document. One thing needs to be clariﬁed is that the
maximum number of updated weights should be determined by all the documents
(i.e., all the inequalities for the words in all the documents should be considered).
For j ≤j∗,
ν(3)
j
∼beta(ν(3)
j ; 1, α(3)) · 1(Lj < ν(3)
j
< Uj)
(4.13)
where
j∗= max
d,t {xd,t}
Lj = max
xd,t=j

ud,t

l<xd,t(1 −ν(3)
l )

Uj = 1 −max
xd,t>j

ud,t
ν(3)
xd,t

l<xd,t,l̸=j(1 −ν(3)
l )

(4.14)
Sampling θ(3)
j . θ(3)
j
denotes a topic in the third layer, and its prior in the
model is
p(θ(3)
j ) =
∞

i
π(2)
i
· Dir(λ(3)θ(2)
i )
(4.15)
With the help of the indicator variable xj, the posterior is
p(θ(3)
j |xj = i, · · · ) ∝Dir(θ(3)
j ; λ(3)θ(2)
i )
(4.16)
It can be seen that the documents are also associated in the posterior of the
topics in the second layers. The reason is that it hopes that the topics in the
102

4.4. MODEL INFERENCE
Chapter 4
Figure 4.6: Illustration of the adaptive truncation by Slice variables. The whole length of a black line is one. Each
green box denotes a stick weight, and the summation of the sticks on the same black line is one. The red bar
represents a slice variable which functions as an adaptive truncation. At each iteration, the slices could move to
both directions. With the slice variables, there are only ﬁnite number of stick weights and topis need to be updated.
103

Chapter 4
4.4. MODEL INFERENCE
upper layer will not only be the prior for the topics in the lower layer but also an
summarization of the documents. For a topic in the second layer, the likelihood
is not only the linked topics in the third layer but also the data associated with
these topics.
Sampling u(2)
j . It is the slice variable for the stick weights in the second
layer.
u(2)
j
∼U(0, πi=xj)
(4.17)
Sampling xj. It is an indicator variable to express the assigned topic in the
second layer to j-th topic the third layer. The posterior conditional distribution
is
p(xj = i| · · · ) ∝1(uj < π(2)
i ) · Dir(θ(3)
j ; λ(3)θ(2)
i )
(4.18)
It is worth to mention that the indicator xj is not only for the ease of sampling
but also gives a way to learn the links between the topics in the neighboring
layers in the Topic Hierarchy.
As stated in Deﬁnition 4, each topic/node in
the topic hierarchy will have one and only one parent topic/node in the upper
and neighboring layer. How to determine which topic is the parent topic for
the nodes in a lower layer can be resolved through the designed indicators here.
The links between the topics within diﬀerent layers are also part of output of
the model, which can facilitate the exploration of a documents using diﬀerent
granular levels, such as information retrieval task (Fersini et al., 2008).
Sampling π(2)
i
. The update of the stick weights of topics in the second layer
is similar with the ones in a document. For i ≤i∗,
ν(2)
i
∼beta(νi; 1, α(2)) · 1(Li < ν(2)
i
< Ui)
(4.19)
104

4.4. MODEL INFERENCE
Chapter 4
where
i∗= max
j {xj}
Li = max
xj=i

u(2)
j

l<i(1 −ν(2)
l )

Ui = 1 −max
xj>i

u(2)
j
ν(2)
xj

l<xj,l̸=i(1 −ν(2)
l )

(4.20)
Sampling θ(2)
i
. θ(2)
i
denotes a topic in the third layer, and its prior in the
model is
p(θ(2)
i ) =
∞

k
π(1)
k
· Dir(λ(2)θ(1)
k )
(4.21)
With the help of the indicator variable xi, the posterior is
p(θ(2)
i |xi = k) ∝Dir(θ(2)
i ; λ(2)θ(1)
k ) ·

xj=i
Dir(θ(3)
j |λ(3)θ(2)
i )
(4.22)
The likelihood of θ(2)
i
also contains the words assigned to all the topics at the
third layer and linked to θ(2)
i .
Sampling u(1)
i . It is the slice variable for the stick weights in the ﬁrst layer.
u(1)
i
∼U(0, πk=xi)
(4.23)
Sampling xi. This indicator variable is to indicate the assigned topic in the
ﬁrst layer to i-th topic the second layer. The posterior conditional distribution
is
p(xi = k| · · · ) ∝1(ui < π(1)
k ) · Dir(θ(2)
i ; λ(2)θ(1)
k )
(4.24)
Sampling π(1)
k . For k ≤k∗,
ν(1)
k
∼beta(νk; 1, α(1)) · 1(Lk < ν(1)
k
< Uk)
(4.25)
105

Chapter 4
4.5. EXPERIMENTAL EVALUATION
where
k∗= max
i {xi}
Lk = max
xi=k

u(1)
i

l<i(1 −ν(2)
l )

Uk = 1 −max
xi>k

u(1)
i
ν(1)
xi

l<xi,l̸=k(1 −ν(1)
l )

(4.26)
Sampling θ(1)
k . θ(1)
k
denotes a topic in the ﬁrst layer, and its prior in the
model is
p(θ(1)
k ) = Dir(β)
(4.27)
The posterior is
p(θ(1)
k | · · · ) ∝Dir(θ(1)
k ; β) ·

xi=k
Dir(θ(2)
i |λ(2)θ(1)
k )
(4.28)
The likelihood for this topic needs to account for as a subtree of the topic hi-
erarchy. Based on these conditional distributions, the full inference scheme is
summarized in Algorithm 4.1. At the initialization stage, all the latent variables
are randomly initialized. In each Gibbs sampling iteration, it iteratively samples
each variable conditioned on the rest.
4.5
Experimental evaluation
This section presents the empirical evaluations of the proposed nonparametric
deep topic model with synthetic and real-world data. Through synthetic data,
the behavior of the model under diﬀerent parameters is evaluated, including α
function and λ function. Following that, the model has compared with other
state-of-the-art topic models, i.e., LDA (Blei et al., 2003) and HDP (Teh, 2006),
based on six real-world document datasets. Although it is trivial to extend to
106

4.5. EXPERIMENTAL EVALUATION
Chapter 4
Algorithm 4.1: Slice Sampler for DTM
Input: A corpus, D
Output: Topic Hierarchy
initialization;
while iter ≤maxiter do
for d = 1; d ≤D do
for n = 1; n ≤Nd do
Update u(4)
d,n by Eq. 4.7;
for n = 1; n ≤Nd do
Update xd,n by Eq. 4.8;
for t = 1; t ≤t∗do
Update πd
t by Eq. 4.9;
for t = 1; t ≤t∗do
Update u(3)
d,t by Eq. 4.11;
for d = 1; d ≤D do
for t = 1; t ≤t∗do
Update xd,t by Eq. 4.12;
for j = 1; j ≤j∗do
Update π(3)
j
by Eq. 4.13;
Update u(2)
j
by Eq. 4.17;
for j = 1; j ≤j∗do
Update xj by Eq. 4.18;
Update θ(3)
j
by Eq. 4.16;
for i = 1; i ≤i∗do
Update u(1)
i
by Eq. 4.23;
Update xi by Eq. 4.24;
Update θ(2)
i
by Eq. 4.22;
for k = 1; k ≤k∗do
Update π(1)
k
by Eq. 4.25;
Update θ(1)
k
by Eq. 4.27;
iter + +;
return {{θk}, {θi}, {θj}, {πk}, {πi}, {πj}, xi, xj};
107

Chapter 4
4.5. EXPERIMENTAL EVALUATION
Layer index
1
2
3
Number of topics
0
5
10
15
20
25
30
35
40
45
50
y=x
y=3x
y=6x
y=9x
y=exp(x)
y=x 2
y=x 3
Figure 4.7: Illustration of the inﬂuence of the α function. The synthetic data
is: 100 documents and 200 diﬀerent words, and each document has 20 diﬀerent
words. The λ function is y = x.
more layers, there are only three layers learned by the model in this section. It
needs to highlight that the number of topics in each layer is not ﬁxed in advance
and can be learnt from the data.
4.5.1
Synthetic data
As discussed in Section 4.3, the α function could control the number trending
of topics in layers, and the λ function could control the concentration degree
of topics in neighboring layers. In order to evaluate their inﬂuence, diﬀerent
function candidates for the α and λ function are compared. At ﬁrst, randomly
generate a document dataset with 100 documents and 20 words (vocabulary size).
With λ function is ﬁxed as y = x, then use the following function candidates for
108

4.5. EXPERIMENTAL EVALUATION
Chapter 4
between layer 1 and layer 2
between layer 2 and layer 3
Similarity
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
y=x
y=3x
y=5x
y=10x
y=20x
Figure 4.8: Illustration of the inﬂuence of the λ function. The synthetic data
is: 100 documents and 200 diﬀerent words, and each document has 20 diﬀerent
words. The α function is y = x.
α function: y = x, y = 3x, y = 6x, y = 9x, y = exp(x), y = x2, and y = x3. The
results are shown in Figure 4.7. As seen from this ﬁgure, the number of topics are
diﬀerent with diﬀerent α function, and the number of topics will increase with
the value of α function. These results could give a hint to set the ratios between
topic numbers on diﬀerent layers. Similarly, diﬀerent function candidates for the
λ function are also compared, including y = x, y = 3x, y = 5x, y = 10x, and
y = 20x, with α function ﬁxed as y = x. In order to quantitatively evaluate the
concentration degree, the following metric is proposed
S =
1
Nh,l

h∈H,l∈L,h→l
θh ⊙θl
(4.29)
109

Chapter 4
4.5. EXPERIMENTAL EVALUATION
where θh is a topic at the higher layer H, θl is a topic at the lower layer L, h is
the parent topic of l, Nh,l is the number of pairs with two layers, and ⊙denotes
inner product between two topics. This metric is to compute the similarity of
two topics within two layers with a relation. The large average similarities shows
the large concentration degree of topics. The ﬁnal results are show in Fig. 4.7.
There are two parts in the ﬁgure: one denotes the similarity between the topics
at the ﬁrst and second layers; the other denotes the similarity between the topics
at the second and third layers. As the increasing of the coeﬃcients of the linear
functions, the λ at each layer will be also increase and then the topics at the
lower layer will be more concentrated to the topics at the higher layer (the value
of similarity is larger).
4.5.2
Real-world data
The eﬃciency of the proposed model (DTM) to explain the training documents
and predict the testing documents is evaluated below. The selected comparative
models are LDA2 and HDP3. The real-world datasets used for evaluation are:
• PR Dataset The PR dataset consists of 11 abstracts from the special issue
on Brain Encoding in Pattern Recognition Journal 4. After removing some
stopwords, there are total 635 diﬀerent words and each abstract contains
about 100 words.
• ICDM Dataset The ICDM dataset consists of 143 abstracts of all papers
from ICDM2014 5 . After removing some stopwords, there are total 3664
diﬀerent words and each abstract contains about 112 words.
2The implementation of LDA is from: http://psiexp.ss.uci.edu/research/programs_data/toolbox.htm.
3The implementation of HDP is from the personal webpage of Prof.
Yee Whye Teh:
http://www.stats.ox.ac.uk/∼teh/.
4http://www.sciencedirect.com/science/journal/00313203/45/6
5http://ieeexplore.ieee.org/xpl/mostRecentIssue.jsp?punumber=7022262
110

4.5. EXPERIMENTAL EVALUATION
Chapter 4
• Cora Dataset6 The Cora dataset consists of 2708 scientiﬁc publications.
Each publication in the dataset is described by a 0/1-valued word vec-
tor indicating the absence/presence of the corresponding word from the
dictionary. The dictionary consists of 1433 unique words.
• Citeseer Dataset6 The CiteSeer dataset consists of 3312 scientiﬁc publi-
cations. Each publication in the dataset is also described by a 0/1-valued
word vector indicating the absence/presence of the corresponding word
from the dictionary. The dictionary consists of 3703 unique words.
• KOS Dataset7 The KOS dataset consists of 3430 documents with total
6906 words. The original source is dailykos.com.
• NIPS Dataset7 The NIPS dataset consists of 1500 documents with total
12419 words. The original source is books.nips.cc.
For each dataset, the whole documents are randomly separated into two parts:
training documents (around 80%) and test documents (around 20%). At ﬁrst,
run three models, i.e., DTM, LDA and HDP, on the training documents, and
then predict the test documents using the trained models respectively. Note that
there are two criteria: one is how well the model ﬁts the training documents; the
other is how well the model predicts the test documents. Considering that, the
following commonly adopted metrics are used to evaluate them. The ﬁrst one is
Lilkelihood =
Dtrain

d
log p(wd|model)
(4.30)
where Dtrain is the number of training documents and p(wd|model) is the prob-
ability of the document d given the model. If the Likelihood is larger, it denotes
6http://linqs.cs.umd.edu/projects/projects/lbc/
7https://archive.ics.uci.edu/ml/datasets/Bag+of+Words
111

Chapter 4
4.5. EXPERIMENTAL EVALUATION
Figure 4.9: An example of topic hierarchy. The orange eclipses denote 4 topics at the ﬁrst layer; the pink eclipses
denote 9 topics at the second layer; the cyan eclipses denote 20 topics at the third layer. Dataset: ICDM. The α
function is y = 5x and the λ function is also x = 5x.
112

4.5. EXPERIMENTAL EVALUATION
Chapter 4
that the model ﬁts the training documents better. For the test documents,
PredictL = exp

−
1
Dtest
log p(wd|model)

(4.31)
where Dtest is the number of test documents.
This metric is also known as
Perplexity used in other topic models. If the PredictL is smaller, it denotes that
the model can better predicts the test documents. Since LDA needs the number
of topics to be ﬁxed in advance, topic number in LDA is adjusted within [1, 200].
In all experiments, the models run 1000 iterations with 200 burn-in stages.
Table 4.1: The learned topic numbers at three layers from all datasets
Dataset
HDP
DTM L1
DTM L2
DTM L3
PR
34
2
4
10
ICDM
73
6
9
16
CITESEER
22
5
11
30
CORA
12
3
7
27
KOS
18
5
21
41
NIPS
9
10
14
40
The results on all the datasets are repsectively shown in Fig. 4.10, Fig. 4.11,
Fig. 4.12, Fig. 4.13, Fig. 4.14, and Fig. 4.15. Besides, another illustrative
example for the learned Topic Hierarchy based on ICDM dataset is shown in
Fig. 4.9. Each ﬁgure contains two subﬁguers: one shows the training document
Likelihood by Eq. 4.30; the other shows the test document PredictL by Eq.
4.31. Since the LDA needs to ﬁx the topic number but HDP and DTM do not
need, there is a curve for LDA where each point represents likelihood from LDA
with the input topic number but only a straight line for HDP (HDP does not
require the topic number as an input.) in the ﬁrst subﬁguer. For the proposed
DTM, there are three layer topics and so there are three straight lines for DTM
from three layer topics, because the topic number in each layer does also not
need to be ﬁxed for DTM. The learned topic number is listed in Table 4.1. From
113

Chapter 4
4.5. EXPERIMENTAL EVALUATION
these results, it can be seen that the performance of HDP is as good as LDA on
the training documents ﬁtting but without preﬁxing the topic number. For the
predicting test documents, HDP archives better results comparing LDA with
an exception that they have comparative results on CORA and CITESEER
datasets. Furthermore, it observes that the topics at lower layer have better
performance than the topics at higher layer on the training documents ﬁtting,
but the topics at higher layer have better performance than the topics at lower
layer on the test documents predicting. This is because that the topics in the
higher layer are more general than the topics at the lower layer, so they have the
ability to well generalize the training documents but the topics at lower layer
have the ability to speciﬁc the training documents with more details. The better
generalization ability help topics at the high layer achieves better performance on
the predicting the unseen documents; the better speciﬁcation ability help topics
at the lower layer achieves better performance on the explaining the training
documents.
Topic Mumber
0
50
100
150
200
Likelihood
-6500
-6000
-5500
-5000
-4500
-4000
LDA
HDP
DTM L1
DTM L2
DTM L3
(a) Likelihood of the training documents
Topic Number
0
50
100
150
200
PredictL
0.06
0.07
0.08
0.09
0.1
0.11
0.12
0.13
0.14
LDA
DTM L1
DTM L2
HDP
DTM L3
(b) PredictL of the test documents
Figure 4.10: Results on Dataset PR
114

4.6. SUMMARY
Chapter 4
Topic Mumber
0
50
100
150
200
Likelihood
×10 4
-10
-9.5
-9
-8.5
-8
-7.5
-7
-6.5
-6
LDA
DTM L3
DTM L2
DTM L1
HDP
(a) Likelihood of the training documents
Topic Number
0
50
100
150
200
PredictL
0.02
0.04
0.06
0.08
0.1
0.12
0.14
0.16
HDP
LDA
DTM L1
DTM L2
DTM L3
(b) PredictL of the test documents
Figure 4.11: Results on Dataset ICDM
Topic Number
0
50
100
150
200
Likelihood
×10 5
-2
-1.9
-1.8
-1.7
-1.6
-1.5
-1.4
-1.3
-1.2
LDA
HDP
DTM L3
DTM L2
DTM L1
(a) Likelihood of the training documents
Topic Number
0
50
100
150
200
PredictL
0.02
0.025
0.03
0.035
0.04
0.045
0.05
0.055
0.06
LDA
DTM L3
HDP
DTM L1
DTM L2
(b) PredictL of the test documents
Figure 4.12: Results on Dataset CORA
4.6
Summary
Topic models are a kind of eﬃcient tool for the document analysis through auto-
matically learning out a set of topics. Comparing a topic set, a topic hierarchy
115

Chapter 4
4.6. SUMMARY
Topic Number
0
50
100
150
200
Likelihood
×10 5
-4.4
-4.2
-4
-3.8
-3.6
-3.4
-3.2
-3
-2.8
-2.6
LDA
DTM L1
HDP
DTM L2
DTM L3
(a) Likelihood of the training documents
Topic Number
0
50
100
150
200
PredictL
0.02
0.025
0.03
0.035
0.04
0.045
0.05
0.055
LDA
DTM L3
HDP
DTM L2
DTM L1
(b) PredictL of the test documents
Figure 4.13: Results on Dataset Citeseer
Topic Mumber
0
50
100
150
200
Likelihood
×10 6
-2.6
-2.5
-2.4
-2.3
-2.2
-2.1
-2
-1.9
-1.8
-1.7
LDA
HDP
DTM L2
DTM L3
DTM L1
(a) Likelihood of the training documents
Topic Number
0
50
100
150
200
PredictL
0.02
0.04
0.06
0.08
0.1
0.12
0.14
0.16
DTM L2
LDA
DTM L3
HDP
DTM L1
(b) PredictL of the test documents
Figure 4.14: Results on Dataset KOS
is more helpful for the corpus understanding and browsing. This chapter has
proposed a nonparametric deep topic model to automatically learn out a topic
hierarchy from any document collection without the requirement to preﬁx the
number of topics at each layer. Inspired by the renowned Hierarchical Dirichlet
116

4.6. SUMMARY
Chapter 4
Topic Mumber
0
50
100
150
200
Likelihood
×10 6
-10
-9.5
-9
-8.5
-8
-7.5
-7
-6.5
-6
LDA
HDP
DTM L1
DTM L2
DTM L3
(a) Likelihood of the training documents
Topic Number
0
50
100
150
200
PredictL
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
LDA
DTM L3
HDP
DTM L1
DTM L2
(b) PredictL of the test documents
Figure 4.15: Results on Dataset NIPS
Processes, a new probability measure has been proposed, which has not only
inherited the mechanisms of linking the topics at diﬀerent layers but also broken
the topic sharing strategy of the original HDP. By the help of this new proba-
bility measure, a topic hierarchy can be learned out. Besides, this chapter has
also designed a slice sampling algorithm without the truncation approximation
that is commonly used by many nonparametric models. The experimental re-
sults show that the topics at higher layer are more general than the topics at
the lower layer, which have better ability to predict the new documents. On the
contrary, the topics at lower layer are more speciﬁc than the topics at the higher
layer, which have better ability to explain the training documents.
117


Chapter 5
Bayesian Nonparametric Relational
Topic Model for Document
Network Learning
5.1
Introduction
Understanding a corpus (without any additional information) in Chapter 3 and
4 is signiﬁcant for businesses, organizations and individuals for instance the aca-
demic papers of IEEE, the emails in an organization and the previously browsed
webpages of a person. One commonly accepted and successful way to understand
a corpus is to discover the hidden topics in the corpus (Blei et al., 2003; Blei,
2012). The revealed hidden topics could improve the services of IEEE, such as
the ability to search, browse or visualize academic papers ; help an organization
understand and resolve the concerns of its employees; assist internet browsers
to understand the interests of a person and then provide accurate personalized
services. Furthermore, there are normally links (additional information) between
the documents in a corpus. A paper citation network (Guo et al., 2014) is an
119

Chapter 5
5.1. INTRODUCTION
example of a document network in which the academic papers are linked by their
citation relations; an email network (Klimt and Yang, 2004) is a document net-
work in which the emails are linked by their reply relations; a webpage network
(Park, 2003) is a document network in which webpages are linked by their hyper-
links. Since these links also express the nature of the documents, it is apparent
that hidden topic discovery should consider these links as well. Therefore, this
Chapter will consider more complicated situation than Chapter 3 and 4.
Similar studies focusing on the hidden topics discovering from the documen-
t network using some Relational Topic Models (RTM) (Chang and Blei, 2009;
Chang et al., 2010; Chen, Zhu, Xia and Zhang, 2015) have already been success-
fully developed. Unlike the traditional topic models (Blei et al., 2003; Blei, 2012)
that focus on mining the hidden topics from a document corpus (without links
between documents), the RTM can make discovered topics inherit the document
network structure. The links between documents can be considered as constrains
of the hidden topics.
One drawback of existing RTMs is that they are built with ﬁxed-dimensional
probability distributions, such as Dirichlet, Multinomial, Gamma and Possion
distribution, which require their dimensions be ﬁxed before use.
Hence, the
number of hidden topics must be speciﬁed in advance, and is normally chosen
using domain knowledge. This is diﬃcult and unrealistic in many real-world
applications, so RTMs fail to ﬁnd the number of topics in a document network.
In order to overcome this drawback, this chapter proposes a Nonparametric
Relational Topic (NRT) model in this chapter, which removes the necessity of
ﬁxing the topic number. When aiming to build a NRT for a document network,
there are three challenges:
1. how to express the document interest on inﬁnite number of topics? Instead
of probability distributions, stochastic processes are adopted by the pro-
120

5.1. INTRODUCTION
Chapter 5
posed model to express the interest of a document on the ‘inﬁnite’ number
of topics. Stochastic process can be simply considered as ‘inﬁnite’ dimen-
sional distributions1.
2. How to make all the documents share the same set of topics? This is a
common feature found in many real-world applications, and many litera-
tures (Chang et al., 2010; Chen, Zhu, Xia and Zhang, 2015) have exploited
this property in their work. In order to achieve the above requirement,
this study has carefully designed the following mechanism: use a global
Gamma process to represent a set of base components and each documen-
t has its own Gamma process thinned from them. The thinned Gamma
processes help documents share the same set of topics. This is importan-
t because users are not interested in analyzing documents in a database
without sharing any common topics (Teh, 2006).
3. How to make two linked documents have similar topics?
This chapter
handles this challenge by controlling the subsampling probabilities of all
the documents on topics, and make the linked documents subsample the
similar topics. A subsampling Markov Random Field is proposed as the
model constraint.
Finally, two sampling algorithms are designed to learn the proposed model
under diﬀerent conditions.
Experiments with document networks show some
eﬃciency in learning what the hidden topics are and superior performance the
modeląŕs ability to learn the number of hidden topics. It is worth noting that,
although using document networks as examples throughout this chapter, this
work can be applied to other networks with node features.
1We only consider the pure-jump processes in this study. Some continuous processes cannot
be simply considered as the ‘inﬁnite’ dimensional distributions.
121

Chapter 5
5.2. PRELIMINARY KNOWLEDGE
τ
ci,j
ϑi
zi,n
wi,n
ϑj
zj,n
wj,n
θk
σ
< di, dj >
K
Figure 5.1: Graphical model of relational-topic-Model
The rest of this chapter is structured as follows. Section 5.2 introduces the
preliminary knowledge. The proposed model is presented in Section 5.3 and the
detailed derivations of its sampling inference have been illustrated in Section 5.4.
Section 5.5 presents experimental results both on the synthetic and real-world
data. Finally, Section 5.6 summarises this chapter.
5.2
Preliminary knowledge
This section brieﬂy introduces the related models which will be used in the rest
of sections.
5.2.1
Relational topic model
Since the ﬁnite relational topic models (Chang et al., 2010) are the comparative
model, it is given more details below. The corresponding graphical representation
122

5.2. PRELIMINARY KNOWLEDGE
Chapter 5
is shown in Fig. 5.1, and the generative process is as follows
ϑd
i.i.d
∼Dirichlet(τ)
θk
i.i.d
∼Dirichlet(σ)
zd,n ∼Discrete(ϑd)
wd,n ∼Discrete(θzd,n)
cdi,dj ∼GLM(zdi, zdj)
(5.1)
where ϑd is the topic distribution of a document d, θk is the word distribution of
topic k, zd,n is the topic index of n-th word in document d, wd,n is n-th observed
word in document d. All these variables are same with the original LDA (Blei
et al., 2003). The diﬀerent and signiﬁcant part is the variable c, which denotes
the observed document link. This model uses a Generalized Linear Model (GLM)
(McCullagh, 1984) to model the generation of the document links
p(cdi,dj = 1) ∼GLM(zdi, zdj)
(5.2)
One problem with this model is that the number of topics needs to be pre-deﬁned
and for some real-world applications, this is not trivial.
5.2.2
Markov random ﬁeld
Markov Random Field (MRF) (Li, 1995) is an undirected graphical model that
involves a set of random variables with links (probability dependencies). MRF
represents the joint distribution of all the random variables having Markov prop-
erty. Suppose there are N random variables {x}N
i=1 and their joint distribution
is
p({x}N
i=1) =

C∈clique(G)
ψ(C)
(5.3)
123

Chapter 5
5.3. BAYESIAN NONPARAMETRIC RELATIONAL TOPIC MODEL
where G is the graph structure from the dependencies of random variables in
MRF and C is a clique of G. ψ(C) is normally called energy function. The
factorization in the Eq. (5.3) is due to the Markov property. The ability of
MRF is to make the marginal distribution of each variable conditioned on its
neighbors.
5.3
Bayesian nonparametric relational topic mod-
el
In this section, the proposed Nonparametric Relational Topic (NRT) model for
the document network is presented in detail. When building a NRT, there are
three challenges: i) How to express the document interest on inﬁnite number of
topics? ii) How to make all the documents share the same set of topics? iii) How
to make two linked documents share similar topics? In the following, strategies
to handle the above three challenges are introduced one by one. Some frequently
used notations are summarised in Table 5.1.
Challenge 1 How to express the document interest on inﬁnite number of
topics?
When the topic number is preﬁxed, it is simply to draw a random variable
from a ﬁxed-dimensional probability distribution (such as Dirichlet distribution
and Logit-normal distribution) as the topic interest of a document in the tra-
ditional topic models. However, the probability distributions have to be aban-
doned for the model building when the number of topics cannot be reasonably
preﬁxed with enough prior knowledge. It makes traditional topic models built
by probability distributions not work.
Here, this chapter uses a draw from a Gamma process to express the interest
of a document on inﬁnite hidden topics.
A Gamma process GaP(α, H) is a
124

5.3. BAYESIAN NONPARAMETRIC RELATIONAL TOPIC MODEL
Chapter 5
Table 5.1: Important notations in this chapter
Symbol
meaning in this chapter
D
the number of documents
V
vocabulary size
K
the number of topics
d
document index
v
word index
k
topic index
Nd
number of words in document d
H
the base probability measure of a Gamma process
α
concentration parameter of a Gamma process
Γ
a random draw/realization of a Gamma process
πk
weight of the topic k
πd,k
weight of the topic k in document d
Θ
parameter (keyword distribution) space
θk
parameter (keyword distribution) of topic k
rk
binary (indicator) variable of topic k
rd,k
binary (indicator) variable of topic k in document d
qd,k
the subsampling probability of document d keeping topic k
C
a clique of document network
ψ(C)
energy function on clique C
nd,v
number of word v in document d
nd,v,k
number of word v assigned to topic k in document d
zd,v,m
topic index assigned to m-th word v in document d
ud,v,m
auxiliary slice variable assigned to m-th word v in document
d
stochastic process, where H is a base (shape) measure parameter on Θ and α is
the concentration (scale) parameter. Let Γ = {(πk, θk)}∞
k=1 be a random draw of
a Gamma process in the product space R+ × Θ where πk ∈R+ and θk ∈Θ, and
it can be represented as
Γ ∼GaP(α, H)
=
∞

k=1
πkδθk,
θk ∼H
(5.4)
125

Chapter 5
5.3. BAYESIAN NONPARAMETRIC RELATIONAL TOPIC MODEL
where δθk is an unit mass function (i.e., δθk(θ∗) = 1 if θk = θ∗and δθk(θ∗) = 0 if
θk ̸= θ∗); πk satisﬁes an improper Gamma distribution Gamma(0, α) and that
is why it is called Gamma process. Γ can also be seen as a complete random
measure. More information about Gamma process can be found in (Rao and Teh,
2009; Roychowdhury and Kulis, 2015). When using Γ to express the document
interest, the {θk)}∞
k=1 in Eq. (5.4) denotes the inﬁnite number of topics and
{πk)}∞
k=1 in Eq.
(5.4) denotes the weights of inﬁnite number of topics in a
document. Note that πk is within (0, +∞) not [0, 1], but {πk)}∞
k=1 can also be
seen as the weights of topics in a document. As illustrated in Fig. 5.2, the idea is
to assign each document a Gamma process. In this ﬁgure, each document is with
a ‘fence’ in which each bar has two properties: position that denotes the topic
and length that denotes the weight of the corresponding topic in this document.
Note that each document could set its fence positions at will. Due to the inﬁnity
of Γ, Challenge 1 can be handled for now.
Challenge 2 How to make all the documents share the same set of topics?
Considering the situation with inﬁnite number of topics, it hopes that there
are some topics that are shared by documents even with inﬁnite number of
candidate topics. Let us consider an extreme situation: each document in a
document network is with and only with its own topics that are diﬀerent from
others. Apparently, this situation is not desirable because the motivation of the
document modeling or topic models is to discover the shared knowledge (i.e.
topics) of a document corpus.
Considering the continuity of the parameter space Θ (the base line of the
fence in Fig. 5.2, equivalently), the probability that two documents are with
same topics is 0. In order to handel Challenge 2, a global Gamma process is
ﬁrstly generated as follow
Γ0 ∼GaP(α, H)
126

5.3. BAYESIAN NONPARAMETRIC RELATIONAL TOPIC MODEL
Chapter 5
Figure 5.2: Illustration of Gamma process assignments for a document network.
Each document is assigned a Gamma process which has inﬁnite components
(represented by the fences in the ﬁgure). Each fence denotes a hidden topic,
and some examples are given in the ﬁgure. The length of the fences denote the
weights of diﬀerent topics in a document.
which is equal to
Γ0 =
∞

k=1
πkδθk,
θk ∼H
where {πk, θk}∞
k=1 is the global set of topics. The idea is to consider {πk, θk}∞
k=1
as a global topic pool, and each document just selects its own topics from this
pool. In this way, the probability of sharing topics between diﬀerent documents
will not be 0. This chapter uses a thinned Gamma process to realize this idea.
Its deﬁnition is as follow,
Deﬁnition 7 (Thinned Gamma Process (Foti et al., 2013)) Suppose there
are countably inﬁnite points {(πk, θk)}∞
k=1 from a Gamma process Γ ∼GaP(α, H).
This chapter uses a thinned Gamma process to realize this idea. Its deﬁnition is
127

Chapter 5
5.3. BAYESIAN NONPARAMETRIC RELATIONAL TOPIC MODEL
as follow,
Γ′ =
∞

k=1
πkrkδθk
(5.5)
is still a Gamma process, which is proofed by (Foti et al., 2013). The {rk} can be
seen as the indicators for the reservation of the point of original/global Gamma
process, so Γ′ is called Thinned Gamma Process.
Each rk is assigned a Bernoulli prior p(rk = 1) = qk, where qk ∈[0, 1] is the
subsampling probability of keeping topic k. Apparently, diﬀerent realizations
of {rk} will lead to diﬀerent thinned Gamma processes. For each document, a
thinned Gamma process Γd is generated with Γ0 as the global process
Γd =
∞

k=1
πkrd,kδθk
(5.6)
where {rd,k}∞
k=1 is a set of indicators of document d on the corresponding com-
ponents. These {rd,k}∞
k=1 are independent identical distributed random variables
with Bernoulli distributions
rd,k ∼Bernoulli(qd,k)
(5.7)
where qd,k denotes the probability of the Gamma process Γd of document d with
component k. Until now, Challenge 2 is handled.
Challenge 3 How to make two linked documents have similar topics?
Two linked documents in a document network normally have similar topics.
For example, the linked two academic papers through a citation are normally
with some common researches, and two linked webpages through a hyperlink
normally report similar news. Therefore, it needs to control the sharing strategy
of documents on the inﬁnite topics in order to make two linked documents have
similar topics.
128

5.3. BAYESIAN NONPARAMETRIC RELATIONAL TOPIC MODEL
Chapter 5
Since each document has been assigned a thinned Gamma process, the idea
is to make thinned Gamma processes dependent with each other according to
the document network structure. Each thinned Gamma process is subsampled
from the global Gamma process Γ0 according to indicators {rk}. Therefore, the
dependence between the diﬀerent realizations of {rk} will also lead to dependence
of the thinned Gamma processes.
In order to obtain the dependent {rk} between documents, a Subsampling
Markov Random Field (MRF) is deﬁned to constrain the qd
k of all documents,
Deﬁnition 8 [Subsampling Markov Random Field] The subsampling probabili-
ties of all the documents on a component/topic in the global Gamma process have
the following constraint
p(qk) =

C∈℘(Network)
ψ(C)
=
1
Z(qk) exp
⎛
⎝−

<di,dj>∈C
∥qdi,k −qdj,k∥2
⎞
⎠
(5.8)
where qk = {qd,k}D
d=1; Network is the document network; ℘(Network) is the
clique set of Network; C is one clique; ψ(C) is the energy function of MRF;
< di, dj >∈C denotes there is link between di and dj and this link is within
clique C; and Z(qk) is the normalization part and also called partition function.
Note that the energy function exp

−
<di,dj>∈C ∥qdi,k −qdj,k∥2
in Deﬁni-
tion 8 is designed to constrain the distance between q of diﬀerent documents on
topic k. The more closely two documents are posited in the network, the more
close their q on topic k. Through this subsampling MRF constraint, the marginal
distribution of each subsampling probability qk will depend on the values of its
neighbors. Therefore, the qk of linked documents will be similar, which ensures
the Challenge 3 is handled.
129

Chapter 5
5.3. BAYESIAN NONPARAMETRIC RELATIONAL TOPIC MODEL
a0, c0
K
Network
q1,k
q2,k
q3,k
q4,k
qd,k
r1,k
r2,k
rd,k
Γ1
Γ2
Γd
Γ0
α
H
K
N1
N2
Nd
n1,v,k
n2,v,k
nd,v,k
β1,k
β2,k
βd,k
b0
Figure 5.3: Graphical representation for the Nonparametric Relational Topic
(NRT) Model by dependent thinned Gamma Processes and Markov Random
Field (MRF). The generative procedures for the document 3 and 4 are omitted
to make the ﬁgure more concise.
To sum up, the proposed Nonparametric Relational Topic (NRT) Model is
graphically illustrated in Fig. 5.3 and its generative procedure is
Γ0 ∼GaP(α, H)
130

5.3. BAYESIAN NONPARAMETRIC RELATIONAL TOPIC MODEL
Chapter 5
or
Γ0 =
∞

k=1
πkδθk
p(qd,k) ∝Beta(qd,k; a0, c0) · exp
⎛
⎜
⎜
⎜
⎝−

< d, di >∈C
C ∈℘(Network)
∥qd,k −qdi,k∥2
⎞
⎟
⎟
⎟
⎠
rd,k ∼Bernoulli(qd,k)
Γd =
∞

k=1
rd,kπkδθk
With the {Γd}D
d=1 for all the documents in hand, the generative procedure of the
documents is as follow
βd,k ∼Gamma(b0, 1)
nd,v,k|nd,v ∼Poisson(θk,vrd,kπkβd,k)
v ∈[1, V ]
nd,v =
∞

k=1
nd,v,k ∼Poisson(
∞

k=1
θk,vrd,kπkβd,k)
θk ∼H
where nd,v is the number of word v in document d (same word may appear several
times in a document), nd,v,k is the number of word v in document d assigned
to topic k, and βd,k is a parameter. Considering the relationship between the
Poisson distribution and the Multinomial distribution, the likelihood part is
equal to
zd,v,m ∼Discrete

θk,vrd,kπkβd,k

k θk,vrd,kπkβd,k

m ∈[1, nd,v]
131

Chapter 5
5.4. MODEL INFERENCE
nd,v,k =

m
δk(zd,v,m)
θ ∼H
This form is more convenient for the slice sampling design for the model which
will be explained in the following Section. a0, b0, c0, α are model parameters.
H is a base measure for the global Gamma process, and it is set as a Dirich-
let distribution parameterized by η. Note that the q are not only with Beta
distribution prior but also with a MRF constraint at the same time.
5.4
Model inference
The inference of the proposed (NRT) model is to compute the posterior distri-
bution of latent variables given data (i.e., document network)
p(K, π, q, r, θ, β|{nd,v}d∈[1,D],v∈[1,V ], Network)
It is apparently that this posterior distribution is a high-dimensional and multi-
variable distribution which analytical form is extremely hard to obtain. There-
fore, this chapter ﬁrst uses Gibbs sampling method to get samples of this pos-
terior distribution with a truncation (deﬁne a relatively large topic number),
which is a commonly-adopted strategy in the Bayesian nonparametric learning
area (Gershman and Blei, 2012) in Section 5.4.1. Furthermore, an exact sam-
pling method without the truncation requirement is also developed based on slice
sampling technique (Neal, 2003b) in Section 5.4.2.
132

5.4. MODEL INFERENCE
Chapter 5
5.4.1
Gibbs sampling
It is diﬃcult to perform posterior inference under inﬁnite mixtures, and a com-
mon work-around solution in Bayesian nonparametric learning is to use a trunca-
tion method. This method is widely accepted, which uses a relatively big K† as
the (potential) maximum number of topics. As required by the Gibbs sampling
framework, all the conditional distributions for the latent variables of the model
are list in the following.
Sampling qd,k. Since there are additional constraints for the variables q,
they do not have a closed-formed posterior distribution.
If rd,k = 1,
p(qd,k| · · · ) ∝qa0+1−1
d,k
(1 −qd,k)c0−1 · exp
⎛
⎜
⎜
⎜
⎝−

< d, di >∈C
C ∈℘(Network)
∥qd,k −qdi,k∥2
⎞
⎟
⎟
⎟
⎠
(5.9)
If rd,k = 0,
p(qd,k| · · · ) ∝qa0−1
d,k (1 −qd,k)c0+1−1 · exp
⎛
⎜
⎜
⎜
⎝−

< d, di >∈C
C ∈℘(Network)
∥qd,k −qdi,k∥2
⎞
⎟
⎟
⎟
⎠
(5.10)
Given this conditional distribution of qd,k, the eﬃcient A* sampling (Maddison
et al., 2014) that is developed recently can be used, because the conditional
distribution can be decomposed into two parts: qa0−1
d,k (1 −qd,k)c0+1−1 and expo-
nential part. The ﬁrst part is easily sampled using a beta distribution (proposal
distribution), and the second part is a bounded function.
Sampling rd,k.
1. ∀j, rd,j = 0 →rd,k = 1
133

Chapter 5
5.4. MODEL INFERENCE
2. ∃v, nd,v,k > 0 →rd,k = 1
3. ∀v, nd,v,k = 0
(a) if ∀v, ud,v,k = 0,
p(rd,k = 1) ∝qd,k

n
Poisson(0; θk,vπkβd,k)
(5.11)
(b) if ∀v, ud,v,k = 0,
p(1)(rd,k = 0) ∝(1 −qd,k)

n
Poisson(0; θk,vπkβd,k)
(5.12)
(c) if ∃v, ud,v,k > 0,
p(2)(rd,k = 0) ∝(1 −qd,k) ·
 
1 −

v
Poisson (0; θk,vπkβd,k)
!
(5.13)
Accordingly,
p(rd,k = 1| · · · ) ∝
p(rd,k = 1)
p(rd,k = 1) + p(1)(rd,k = 0) + p(2)(rd,k = 0)
(5.14)
Sampling βd,k. βd,k is a model parameter with a Gamma prior and due to
the conjugate between the Gamma and Poisson distribution,
p(βd,k| · · · ) ∝Gamma(nd,·,k + b0,
1
rd,kπk + 1)
(5.15)
where nd,·,k = 
v nd,v,k is the number of words assigned to topic k in document
d.
Sampling θk. In the proposed model, H is set as a probability (Dirichlet)
134

5.4. MODEL INFERENCE
Chapter 5
distribution parameterized by η, so the posterior is as follows
p(θk| · · · ) ∝Dirichlet(η + n·,1,k, . . . , η + n·,V,k)
(5.16)
where n·,v,k = 
d nd,v,k is the number of word v assigned to topic k in all the
documents.
Sampling nd,v,k. (truncated version) Here, it needs to sample the nd,v,1, ..., nd,v,K†
together due to the known nd,v = K†
k=1 nd,v,k according to Multinomial distri-
bution
p(nd,v,1, . . . , nd,v,K†| · · · ) ∝Mult(nd,v; ξd,v,1, . . . , ξd,v,K†)
(5.17)
where
ξd,v,k =
θk,vrd,kπkβd,k
K†
k
θk,vrd,kπkβd,k
.
(5.18)
Sampling πk. (truncated version) Although is from a Gamma process, it
can be seen with a Gamma distribution prior given a truncation level K†, so the
following posterior can be sampled
p(πk| · · · ) ∝Gamma(1/K† + n·,·,k,
1
β·,k + 1)
(5.19)
where n·,·,k = 
d

v nd,v,k is the total number of words assign to topic k and
β·,k = 
d βd,k. Note that the truncation version of the model is not equal to a
probability distribution-based model (Fox et al., 2011b). Under this truncation,
there is only limited of topics will be used by documents and most of others will
be unused. This truncation can be seen as an approximation of the NRT.
The whole sampling algorithm is summarized in Algorithm 5.1. It is interest-
ing that the qk of diﬀerent d are independent of each other given other variables.
So the update of qk of diﬀerent d can be implemented in a parallel fashion.
135

Chapter 5
5.4. MODEL INFERENCE
Algorithm 5.1: Truncated Version of Gibbs Sampling for NRT
Input: Network and nd,v
Output: K, {θk}K
k=1, {πd
k}K
k=1
initialization;
it = 1; while it ≤maxit do
for each topic k do
for each document d do
for each word v of document d do
Update nd,v,k by Eq. (5.17) ;
Update qd,k by Eq. (5.9) and (5.10) ;
Update rd,k by Eq. (5.14) ;
Update βd,k by Eq. (5.15) ;
Update θk by Eq. (6.65) ;
Update πk by Eq. (5.19);
it+;
5.4.2
Slice sampling
Although the truncated method are commonly accepted in the literature, main-
taining a large number of components and their parameters is time and space
consuming. An elegant idea (call slice sampling (Neal, 2003b)) to resolve this
problem is to introducing additional variables to adaptively truncate/select the
inﬁnite components.
Sampling nd,v,k (slice sampling version) In order to do slice sampling,
the following auxilary/slice variable is introduced
ud,v,m = Uniform(0, ζk), m ∈[1, nd,v]
(5.20)
where Uniform(0, ζk) is a Uniform distribution on [0, ζk] and ζk is a ﬁxed positive
decreasing sequence limk→∞ζk = 0. With the help of slice variable ud,v,m, sample
136

5.4. MODEL INFERENCE
Chapter 5
the zd,v,m within a ﬁnite scope as
p(zd,v,m = k| · · · ) ∝ξd,v,k · Π(ud,v,m ≤ζk)
ζk
nd,v,k =

m
δk(zd,v,m)
m ∈[1, nd,v]
(5.21)
where Π(ud,v,m ≤ζk) = 1 when ud,v,m ≤ζk is satisﬁed; Π(ud,v,m ≤ζk) = 0 when
ud,v,m ≤ζk is not satisﬁed. Note that the possible values of zd,v,m are limited by
Π(ud,v,m ≤ζk) because ζk is a ﬁxed positive decreasing sequence.
Sampling πk (slice sampling version) The construction of Gamma pro-
cess Γ0 ∼GaP(α, H) (Roychowdhury and Kulis, 2015) is
Γ0 =
∞

k=1
Eke−Tkδθk
(5.22)
where Ek and Tk are two additional auxiliary variables as
Ek ∼Exp( 1
α), Tk ∼Gamma(κk, 1
α), θk ∼H
(5.23)
where Exp( 1
α) denotes an Exponential distribution parameterized by
1
α.
Ac-
cording to (Wang and Carin, 2012; Roychowdhury and Kulis, 2015), all the
components/points/topics could be considered as draws from a number (I and I
could be inﬁnitely large) of Poisson processes, so each topic is assigned a Poisson
process index κk and the following property holds
∞

k=1
δκk(i) ∼Poisson(γ),
γ =

Θ
H
(5.24)
which means that the number of topics from each Poisson process satisﬁes a
137

Chapter 5
5.4. MODEL INFERENCE
Poisson distribution parameterized by γ that is the total mass of base measure
H of Gamma Process. Note that γ is equal to 1 if the H is set as a probability
measure. Finally, According to the construction in Eq. (5.22), the prior of πk is
πk = Eke−Tk ∼Exp( 1
α) · Gamma(κk, 1
α)
(5.25)
and the posterior is
πk = (Ek, Tk) ∼Poisson(data|Eke−Tk)Exp(Ek| 1
α)Gamma(Tk|κk, 1
α)
(5.26)
This posterior can be sampled by two Gamma distributions as
Ek|Tk ∼Gamma(Ek|n·,·,k + 1,
1
α−1 + β·,ke−Tk )
Tk|Ek ∼Poisson(n·,·,k|β·,ke−TkEk) · Gamma(Tk|κk, 1
α)
(5.27)
where n·,·,k = 
d

v nd,v,k and β·,k = 
d βd,k.
The conditional distribution for the indicator κk is
p(κk = i| · · · ) ∝p(Tk|κk = i) · p(κk = i|{κl}k−1
l=1 )
(5.28)
The second part on the right hand side of Eq. (5.28) is
p(κk = i| · · · )
=
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎩
0
if i < κk−1
1 −F(Ii−1|γ)
1 −F(Ii−1 −1|γ)
if i = κk−1
(F(Ii−1|γ) −F(Ii−1 −1|γ))
1 −F(Ii−1 −1|γ)
(1 −f(0|γ)) f(0|γ)h−1
if i = κk−1 + h
(5.29)
where h is an integer denotes the distance between κk with κk−1; Ii is the number
138

5.5. EXPERIMENTS
Chapter 5
Algorithm 5.2: Slice Version of Gibbs Sampling for NRT
Input: Network and nd,v
Output: K, {θk}K
k=1, {πd
k}K
k=1
initialization;
it = 1;
while it ≤maxit do
for each topic k do
for each document d do
for each word v of document d do
Sample slice variable nd,v,k by Eq. (5.20) ;
Update nd,v,k by Eq. (5.21) ;
Update qd,k by Eq. (5.9) or (5.10) ;
Update rd,k by Eq. (5.14) ;
Update βd,k by Eq. (5.15) ;
Update θk by Eq. (6.65) ;
Update πk by Eq. (5.27);
Update κk by Eq. (5.28);
it++;
of items in i-th Poisson process and Ii ∼Poisson(γ); F(·|γ) and f(·|γ) are
the cumulative distribution function and probability density function of Poisson
distribution parameterized by γ.
Note that the ud,v,m, κk, Ek and Tk are introduced additional variables. They
are not in the original model, and their appearances are only for the sampling
without the help of the truncation level. The whole slice sampling algorithm is
summarized in Algorithm 5.2.
5.5
Experiments
This section evaluates the eﬀectiveness of the proposed NRT model in learning
the hidden topics from document networks. First, two evaluation metrics are
designed for the quantiﬁcation of the eﬀectiveness and comparisons. Then, a
series of experiments on the synthetic datasets to demonstrate the model’s ability
139

Chapter 5
5.5. EXPERIMENTS
of topics and topic number learning and some properties. Finally, it shows the
usefulness of the proposed model using two real-world datasets.
5.5.1
Evaluation metrics
Since NRT builds on two parts of knowledge (i.e., the network structure and doc-
ument content) from a document network data, it is possible to make predictions
for one of them based on the other (Chen, Zhu, Xia and Zhang, 2015; Chang
and Blei, 2009). In order to quantitatively compare the NRT with other models,
two evaluation metrics are designed: link prediction and document prediction.
The link prediction is to predict the links between test documents with train-
ing documents using learned topics. The assumption is that there will be a link
between two documents if they have similar topics. For two given test documents
di and dj, the probability of a link between di and dj is
pl(cdi,dj = 1) ∝cos(ϖdi, ϖdj)
(5.30)
where cos(·, ·) is the cosine similarity measure between two vectors and ϖdi is
a vector and represents the interest of test document di on topics which can be
evaluated by
ϖdi,k =
V

v=1
ndi,v · θk,v
(5.31)
where ndi,v denotes the number of word v in document di, θk is the learned topic
from documents, and ϖdi is the normalized < ϖdi,1, · · · , ϖdi,K > where K is the
learned topic number. The above Eq. (5.30) accounts for one test link and sum
over the probabilities of all test links in the experiments.
The document prediction is to evaluate the likelihood of a test document
with known network structure with training documents using learned topics.
140

5.5. EXPERIMENTS
Chapter 5
The assumption is that the likelihood of a test document with similar topic
interests with its linked training documents is large. For a given test document
di with its linked training documents Ωd, the likelihood of this document can be
evaluated by
pd(di) ∝
V

v=1
ndi,v
K

k=1
θk,v · πdi,k
(5.32)
where πdi,k is the average of topic interests of neighbors training documents and
can be evaluated by
πdi,k =

d′∈Ωd πk · rd′,k
|Ωdi|
(5.33)
where Ωdi is the neighbor(i.e., training documents) set of test document di, |Ωdi|
is the number of documents in Ωdi, and {πk, rd,k} are from the NRT model. The
above Eq. (5.32) accounts for one test document and sum over the probabilities
of all test documents in the experiments.
Note that the probability of the test links and documents is used here rather
than the exact predicted number of links or documents because the proposed
model is kind of generative model same with other similar works (Chen, Zhu,
Xia and Zhang, 2015; Chang and Blei, 2009). So the probability evaluations are
more accurate and approximate for the proposed model and comparisons.
5.5.2
Experiments on synthetic data
Synthetic data is generated to explore the NRT’s ability to learn the hidden
topics and infer the number of hidden topics from the document network.
(1) Synthetic data generation
At ﬁrst, choose a set of ground truth numbers symbolised by K, D and V that
refer to the number of topics, documents and (diﬀerent) keywords, respectively.
Then, K global topics are generated through a V -dimensional Dirichlet distri-
141

Chapter 5
5.5. EXPERIMENTS
1
2
3
5
10
50
100
200
Truncation Level
2
4
6
8
10
12
Topic Number
K=3, D=10, V=50
Figure 5.4: The boxplot of the learned topic numbers by truncated inference
method given diﬀerent truncation levels.
bution parameterized by {α1, . . . , αV } where αi = 1 ∀i.
Next, generate the
document interests on these topics through a K-dimensional Dirichlet distri-
bution parameterized by β1, . . . , βK} ∀βi = 1. With topics and the document
interests on these topics in hand, generate each document d as follows: 1) Nd
is uniformly chosen to be a number between N
2 and N where N is set as the
maximum number of words in a document; 2) Repeat the following operations
Nd times: a topic index is drawn from the document’s topic interest and then
draw a word from the selected topic. Finally, obtain a D × V matrix with rows
as documents and columns as words, and each entry of this matrix nd,v denotes
the frequency a particular word v in a particular document d. The next step
142

5.5. EXPERIMENTS
Chapter 5
1
2
3
4
5
6
7
8
Topic Number
0
500
1000
1500
2000
2500
3000
Count
K=3, D=10, V=30
1
2
3
4
5
6
7
8
9
Topic Number
0
500
1000
1500
2000
2500
3000
Count
K=5, D=20, V=50
0
5
10
15
20
Topic Number
0
500
1000
1500
2000
2500
Count
K=12, D=80, V=100
0
10
20
30
Topic Number
0
500
1000
1500
2000
2500
Count
K=20, D=500, V=2000
Figure 5.5: Learned topic number distribution from NRT with synthetic datasets
under diﬀerent settings. Normally, the expectation of this distribution will be
regarded as the learn topic number of a document network.
(0.6, 0.2, 0.2)
(0.2, 0.6, 0.2)
(0.2, 0.2, 0.6)
Figure 5.6: The illustration of topics learning results. Three red/circle nodes
denote three benchmark topics that are also given at the top of each subﬁgure;
the blue/cross nodes denote learned topics from NRT.
143

Chapter 5
5.5. EXPERIMENTS
NRT
NRT-nSMRF
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
Similarity
NRT NRT-nSMRF
0
100
200
300
400
500
600
700
Link prediction
NRT NRT-nSMRF
6
6.5
7
7.5
8
8.5
9
9.5
Document prediction
Figure 5.7: Eﬀectiveness of SMRF in NRT. The ﬁrst subﬁgure is for the com-
parision between average similarity between topic interests of all test linked
document pairs from both NRT and NRT without SMRF; The second and third
subﬁgures are for comparison on Link and Document prediction.
is to generate the relations between documents. For each pair of documents,
compute the inner product between their topic interests. In order to sparsify
these relationships, only retain the ones where their inner products are greater
than 0.2.
(2) Inﬂuence of truncation level
There are two inference methods proposed in this chapter: one is truncation
version and the other is slice version. For the truncation version in Algorithm 5.1,
a truncation level needs to be given in advance. In order to show the inﬂuence of
this parameter, diﬀerent truncation levels (i.e., K† ∈{1, 2, 3, 5, 10, 50, 100, 200})
and a generated dataset using the procedure in Section 5.5.2.1 with the setting
144

5.5. EXPERIMENTS
Chapter 5
(i.e., K = 3, D = 10 and V = 50) are fed into Algorithm 5.1. For each run, it
takes 10,000 iterations with 2,000 burn-in samples. The ﬁnal 800 samples are
obtained by keeping each sample every 10 samples considering the dependency
between the consecutive samples from the Algorithm. The results are plotted in
Fig. 5.4 which shows not only the topic number means from eight truncation
levels but also the some basic statistics of 800 samples at each truncation level.
It can be seen that the topic number dose not exceed the truncation levels when
they are smaller than the real one (i.e., 3 for this dataset). When the truncation
level is larger than 3, there will be a ﬂuctuation of the learned topic numbers
but the learned topic number will still not exceed the truncation level, so the
ﬂuctuation is small when the truncation level is not very large (such as 5 in the
Fig. 5.4). As the increasing of the truncation level, the approximation of the
truncation version distribution is more accurate, so the learned topic number
will be closer to the real one and the variance is smaller.
(3) Topics learning
One ability of NRT model is to discover the hidden topics from a document
network. This subsection aims to show this ability. At ﬁrst, generate a synthetic
dataset using the revised procedure in Section 5.5.2.1 with a setting (i.e., K = 3,
D = 10, V = 3) and the topics are predeﬁned as benchmarks rather than
randomly sampled ones. The three topics are (0.6, 0.2, 0.2), (0.2, 0.6, 0.2), and
(0.2, 0.2, 0.6), which correspond to three points in the 3-dimensional simplex.
After running NRT model, keep 100 samples with 3 topics. In each sample, there
are three learned topics which are linked to the benchmark topics according to
the similarity measure, and choose the best linking status as the ﬁnal one for
each sample. The best linking status means the the total similarity between each
pair of topics reaches maximum. For example, there are three learned topics in
145

Chapter 5
5.5. EXPERIMENTS
a sample: (0.25, 0.5, 0.25), (0.5, 0.25, 0.25), and (0.25, 0.25, 0.5). It should link
the ﬁrst learned topic to (0.2, 0.6, 0.2), the second learned topic to (0.6, 0.2, 0.2),
and the third learned topic to (0.2, 0.2, 0.6). In Fig. 5.6, three red/circle nodes
denote three benchmark topics and the blue/cross ones are from samples. It can
be seen from this ﬁgure that the samples from NRT centers on the benchmark
topics with a certain variance, which shows the eﬀectiveness of NRT on the topics
learning.
(4) Topic number learning
Another ability of NRT model is to discover the hidden topics without the re-
quirement of the predeﬁned topic number. In order to show this ability, the
synthetic data generation procedure in Section 5.5.2.1 is used with diﬀerent set-
tings: K = 3, D = 10, V = 30; K = 5, D = 20, V = 50; K = 12, D = 80,
V = 100; K = 20, D = 500, V = 2000. For each setting, run the NRT model
with 10,000 iterations with ﬁrst 2,000 samples as burn-in stage. In Fig. 5.5, he
topic numbers in the remaining 8,000 samples from NRT model on four synthetic
datasets are plotted. From this ﬁgure, the conclusion can be drawn that NRT
has the ability to learn out the topic number from a document network to some
extent.
(5) Eﬀectiveness of SMRF
SMRF in Deﬁnition 8 is used to add the network structure into the model. In
order to evaluate the performance of this SMRF, this section compares NRT with
SMRF and NRT without SMRF using generated dataset by Section 5.5.2.1 with
setting: K = 10, D = 30, V = 200. Among all the documents, 23 documents are
considered as the training documents with 44 links, and 7 documents are reserved
as the test documents with 10 links.
After the mixing of sampling (10,000
146

5.5. EXPERIMENTS
Chapter 5
iterations with 2,000 burn-in samples), 100 samples with 80 as the interval are
kept. At ﬁrst, the average similarity between topic interests of all test linked
document pairs is evaluated.
The assumption is that the more similar topic
interests of two linked documents, the learned topics are more reasonable because
the test links are generated using through the topic interest similarity. The result
is plotted in the ﬁrst subﬁgure of Fig. 5.7, which shows that NRT with SMRF
constraint could recover the test links better. Next, they are compared using
the metrics proposed in Section 5.5.1. The results are shown in the second and
third subﬁgures of Fig. 5.7. It can be seen from these ﬁgures that SMRF helps
NRT obtain better performance on the link prediction task since it considers the
training document links, and slightly reduces the performance on the document
prediction task.
5.5.3
Experiments on real-world data
(1) Datasets and setup
The real-world document network datasets2 used in this study are:
• Cora Dataset It consists of 2708 scientiﬁc publications with their citation
relations.
The citation network consists of 5429 links.
The dictionary
consists of 1433 unique words.
• Citeseer Dataset The CiteSeer dataset consists of 3312 scientiﬁc publica-
tions. The citation network consists of 4732 links. The dictionary consists
of 3703 unique words (Sen et al., 2008).
For each dataset, 5-fold cross validation is used to evaluate the performance
of the proposed model comparing with Relational Topic Model (RTM)(Chang
2http://linqs.cs.umd.edu/projects/projects/lbc/
147

Chapter 5
5.5. EXPERIMENTS
0
500
1000
−2.4
−2.2
−2
−1.8
−1.6
−1.4
−1.2
x 10
5
Iteration
Log Likelihood
NRT
RTM20
RTM30
RTM40
RTM50
RTM60
20
30
40
50
0
10
20
30
40
50
60
70
80
90
100
Active Topic Number
Frequency
20
30
40
50
60
6500
7000
7500
8000
8500
9000
9500
10000
10500
11000
Topic Number of RTM
Link Prediction
20
30
40
50
60
50
100
150
200
250
300
350
400
450
500
550
Topic Number of RTM
Document Prediction
NRT
NRT
Figure 5.8: Results of NRT and RTM under diﬀerent setting (K={20, 30, 40,
50, 60}) on a ﬁrst 5-fold of cora dataset.
0
500
1000
−2.4
−2.2
−2
−1.8
−1.6
−1.4
−1.2
x 10
5
Iteration
Log Likelihood
NRT
RTM20
RTM30
RTM40
RTM50
RTM60
20
30
40
50
60
0
10
20
30
40
50
60
70
80
90
Active Topic Number
Frequency
20
30
40
50
60
7500
8000
8500
9000
9500
10000
10500
11000
11500
12000
Topic Number of RTM
Link Prediction
0
20
30
40
50
60
50
100
150
200
250
300
350
400
450
500
550
Topic Number of RTM
Document Prediction
NRT
NRT
Figure 5.9: Results of NRT and RTM under diﬀerent settings (K={20, 30, 40,
50, 60}) on a second 5-fold of cora dataset.
et al., 2010). The whole dataset is equally split into ﬁve parts. At each stage,
documents in one part are chosen for testing while the rest of the four parts
are used for training. The implementation of RTM is from A Fast And Scalable
Topic-Modeling Toolbox 3 for comparison.
(2) Results and discussions
The results are shown in Fig. 5.8, 5.9, 5.10, 5.11, 5.12, 5.13, 5.14, 5.15, 5.16
and 5.17 in which NRT with RTM under several settings have been compared.
3http://www.ics.uci.edu/ãsuncion/software/fast.htm#rtm
148

5.5. EXPERIMENTS
Chapter 5
0
500
1000
−2.4
−2.2
−2
−1.8
−1.6
−1.4
−1.2
x 10
5
Iteration
Log Likelihood
NRT
RTM20
RTM30
RTM40
RTM50
RTM60
20
30
40
50
60
0
10
20
30
40
50
60
70
80
90
100
Active Topic Number
Frequency
20
30
40
50
60
5500
6000
6500
7000
7500
8000
8500
9000
9500
Topic Number of RTM
Link Prediction
20
30
40
50
60
50
100
150
200
250
300
350
400
450
500
Topic Number of RTM
Document Prediction
NRT
NRT
Figure 5.10: Results of NRT and RTM under diﬀerent settings (K={20, 30, 40,
50, 60}) on a third 5-fold of cora dataset.
0
500
1000
−2.4
−2.2
−2
−1.8
−1.6
−1.4
−1.2
x 10
5
Iteration
Log Likelihood
NRT
RTM20
RTM30
RTM40
RTM50
RTM60
25
30
35
40
45
50
0
10
20
30
40
50
60
70
80
90
100
Active Topic Number
Frequency
20
30
40
50
60
5000
5500
6000
6500
7000
7500
8000
8500
Topic Number of RTM
Link Prediction
20
30
40
50
60
50
100
150
200
250
300
350
400
450
500
550
Topic Number of RTM
Document Prediction
NRT
NRT
Figure 5.11: Results of NRT and RTM under diﬀerent settings (K={20, 30, 40,
50, 60}) on a fourth 5-fold of cora dataset.
Since the learned topic number could be diﬀerent for each fold, the ﬁve fold of
datasets cannot be simply averaged, so the result on diﬀerent folds of a dataset
are all plotted. The ﬁve subﬁgures at the left hand side (i.e., Fig. 5.8, 5.9, 5.10,
5.11, and 5.12) are for cora dataset; the ﬁve subﬁgures at the right hand side
(i.e., Fig. 5.13, 5.14, 5.15, 5.16 and 5.17) are for citeseer dataset. For clarity,
denote RTM with K = num as “RTMnum”. For example, RTM20 means RTM
with K = 20. Since NRT does not need the predeﬁned topic number as an
input, it does not impacted by the K and is plotted as a line in ﬁgures with K
as x-axis. In each ﬁgure, there are four subﬁgures: the ﬁrst subﬁgure shows the
149

Chapter 5
5.5. EXPERIMENTS
0
500
1000
−2.4
−2.2
−2
−1.8
−1.6
−1.4
−1.2
x 10
5
Iteration
Log Likelihood
NRT
RTM20
RTM30
RTM40
RTM50
RTM60
20
30
40
50
0
10
20
30
40
50
60
70
80
90
100
Active Topic Number
Frequency
20
30
40
50
60
4500
5000
5500
6000
6500
7000
7500
Topic Number of RTM
Link Prediction
20
30
40
50
60
50
100
150
200
250
300
350
400
450
500
Topic Number of RTM
Document Prediction
NRT
NRT
Figure 5.12: Results of NRT and RTM under diﬀerent settings (K={20, 30, 40,
50, 60}) on a ﬁfth 5-fold of cora dataset.
0
500
1000
−6.5
−6
−5.5
−5
−4.5
−4
−3.5
−3
x 10
5
Iteration
Log Likelihood
IRT
RTM10
RTM100
RTM2
RTM20
RTM30
RTM40
RTM5
RTM50
30
40
50
60
70
0
10
20
30
40
50
60
70
80
90
100
Active Topic Number
Frequency
2
5
10 20 30 40 50 100
0.5
1
1.5
2
2.5
3
x 10
4
Topic number of RTM
Link Prediction
2
5
10 20 30 40 50 100
100
200
300
400
500
600
700
800
900
1000
Topic number of RTM
Document Prediction
NRT
NRT
Figure 5.13: Results of NRT and RTM under diﬀerent settings (K={2, 5, 10, 20,
30, 40, 50, 100}) on a ﬁrst 5-fold of citeseer dataset.
Log-likelihood along iterations; the second subﬁgure is the learned distribution
of active topic number; the third subﬁgure is the comparison of NRT and RTM
on link prediction task; the fourth subﬁgure is the comparison of NRT and RTM
on document prediction task.
Note that the slice version of NRT in Algorithm 5.2 is used as the implemen-
tation of NRT. The reason is that slice version is more eﬃcient than truncated
version because the slice version does not need to keep the (relatively) large
number of hidden topics in memory (the initial guess for the number of topics is
normally set as larger than the number of documents).
150

5.5. EXPERIMENTS
Chapter 5
0
500
1000
−6.5
−6
−5.5
−5
−4.5
−4
−3.5
−3
−2.5
x 10
5
Iteration
Log Likelihood
NRT
RTM10
RTM100
RTM2
RTM20
RTM30
RTM40
RTM5
RTM50
30
40
50
60
70
0
10
20
30
40
50
60
70
80
90
100
Active Topic Number
Frequency
2
5
10 20 30 40 50 100
1
1.5
2
2.5
3
3.5
4
4.5
x 10
4
Topic Number of RTM
Link Prediction
2
5
10 20 30 40 50 100
300
400
500
600
700
800
900
1000
Topic Number of RTM
Document Prediction
NRT
NRT
Figure 5.14: Results of NRT and RTM under diﬀerent settings (K={2, 5, 10, 20,
30, 40, 50, 100}) on a second 5-fold of citeseer dataset.
0
500
1000
−6.5
−6
−5.5
−5
−4.5
−4
−3.5
−3
x 10
5
Iteration
Log Likelihood
IRT
RTM10
RTM100
RTM2
RTM20
RTM30
RTM40
RTM5
RTM50
35
40
45
50
55
60
0
10
20
30
40
50
60
70
80
90
Active Topic Number
Frequency
2
5
10 20 30 40 50 100
0.6
0.8
1
1.2
1.4
1.6
1.8
2
2.2
2.4
x 10
4
Topic Number of RTM
Link Prediction
2
5
10 20 30 40 50 100
200
300
400
500
600
700
800
900
1000
Topic Number of RTM
Document Prediction
NRT
NRT
Figure 5.15: Results of NRT and RTM under diﬀerent settings (K={2, 5, 10, 20,
30, 40, 50, 100}) on a third 5-fold of citeseer dataset.
Note that the proposed algorithm mixed better than some of the K settings
in RTM and was generally compatible with the rest. As shown in ﬁrst part of
each subﬁgure, the likelihood by NRT model was generally larger than the RTM
under various settings. It means that the proposed model ﬁts or explains the
training data better than the RTM. As for the synthetic case, the distribution
of K is also plotted. This section compared the proposed method with RTM
in terms of link and document prediction. In terms of document prediction,
the proposed algorithm consistently outperformed RTM in every category. In
terms of link prediction, NRTąŕs performance was not universally better than
151

Chapter 5
5.5. EXPERIMENTS
0
500
1000
−6.5
−6
−5.5
−5
−4.5
−4
−3.5
−3
−2.5
x 10
5
Iteration
Log Likelihood
IRT
RTM10
RTM100
RTM2
RTM20
RTM30
RTM40
RTM5
RTM50
30
40
50
60
70
0
10
20
30
40
50
60
70
80
90
Active Topic Number
Frequency
2
5
10 20 30 40 50 100
0.6
0.8
1
1.2
1.4
1.6
1.8
2
2.2
2.4
x 10
4
Topic Number of RTM
Link Prediction
2
5
10 20 30 40 50 100
100
200
300
400
500
600
700
Topic Number of RTM
Document Prediction
NRT
NRT
Figure 5.16: Results of NRT and RTM under diﬀerent settings (K={2, 5, 10, 20,
30, 40, 50, 100}) on a fourth 5-fold of citeseer dataset.
0
500
1000
−6.5
−6
−5.5
−5
−4.5
−4
−3.5
−3
x 10
5
Iteration
Log Likelihood
IRT
RTM10
RTM100
RTM2
RTM20
RTM30
RTM40
RTM5
RTM50
35
40
45
50
55
60
0
10
20
30
40
50
60
70
80
90
100
Active Topic Number
Frequency
2
5
10 20 30 40 50 100
0.4
0.6
0.8
1
1.2
1.4
1.6
x 10
4
Topic Number of RTM
Link Prediction
2
5
10 20 30 40 50 100
100
150
200
250
300
350
400
450
500
550
600
Topic Number of RTM
Document Prediction
NRT
NRT
Figure 5.17: Results of NRT and RTM under diﬀerent settings (K={2, 5, 10, 20,
30, 40, 50, 100}) on a ﬁfth 5-fold of citeseer dataset.
RTM, where it noticed some less accurate results under some RTM settings. It
can be seen that there is a trend for the link prediction with respective to the
topic number in RTM. This trend comes from the evaluation Eq. (5.30). The
RTM with smaller topic number tends to have bigger provability of observed
links, which has also been observed in (Chen, Zhu, Xia and Zhang, 2015). At
an extreme situation (K = 1), the RTM reaches its best performance on link
prediction. The problem is how to choose the hidden topic number for RTM.
Take cora dataset as an example. The candidates of possible topic number are
at least within [1, 2708]. However, for the proposed NRT model, the active topic
152

5.6. SUMMARY
Chapter 5
number is automatically learned from the data (for cora dataset K around 42).
Without any prior domain knowledge, this topic number can achieve relatively
good results on the link prediction considering its large range [1, 2708]. In terms
of overall result, it shows that in the absence of an accurate domain knowledge
of K value, the NRT algorithm has allowed us achieving better and more robust
performance compared with the current state-of-the-art methods.
5.6
Summary
Despite of the success of existing relational topic models in discovering hidden
topics from document networks, they are based on the unrealistic assumption,
for many real-world applications, that the number of topics can be easily prede-
ﬁned. In order to relax this assumption, this chapter has presented a nonpara-
metric relational topic model. In the proposed model, the stochastic processes
are adopted to replace the ﬁxed-dimensional probability distributions used by
existing relational topic models which lead to the necessity of pre-deﬁning the
number of topics. At the same time, introducing stochastic processes leads to the
diﬃculty with model construction and inference, and this chapter has therefore
presented a thinned Gamma process-based model and also presented truncated
Gibbs and slice sampling algorithms for the proposed model. Experiments on
both the synthetic dataset and the real-world dataset have demonstrated the
proposed methodąŕs ability to inference the hidden topics and their number.
153


Chapter 6
Bayesian Nonparametric
Cooperative Hierarchical Structure
Models for Multi-label Document
Learning
6.1
Introduction
Diﬀerent from the network structure considered in Chapter 5, this Chapter con-
siders multi-label information of documents. A hierarchical structure has multi-
ple layers, and each layer contains a number of nodes that are linked to the nodes
in the higher and lower layers, as illustrated in Fig. 6.1. This structure is a very
common and pervasive structure that has been adopted in many diﬀerent areas.
One example of a hierarchical structure is found in text mining. Consider all the
papers in a scientiﬁc journal (e.g., Artiﬁcial Intelligence). An author-paper-word
(Rosen-Zvi et al., 2010) hierarchical structure emerges, given each author writes
and publishes a number of scientiﬁc papers in this journal, and each paper is
155

Chapter 6
6.1. INTRODUCTION
composed of several diﬀerent words. There are actually two types of hierarchical
structures according to the number of parents for each node: non-cooperative
hierarchical structures and cooperative hierarchical structures.
Current renowned Bayesian approaches for the hierarchical structure model-
ing are mainly based on topic models (Blei et al., 2003; Blei, 2012; Xuan, Lu,
Zhang and Luo, 2015) that were originally designed for modeling two-layer hier-
archical structure: document-word. Beyond the two-layer hierarchical structure,
many additional information of data are incorporated into the topic model, so
new hierarchical structures have also been successfully modelled, such as: author-
document-word (Rosen-Zvi et al., 2004; Steyvers et al., 2004; Rosen-Zvi et al.,
2010), emotion-document-word (Bao et al., 2012), entry-document-word (Kim
et al., 2012) and label-document-word (Zhu et al., 2012). One major issue in
existing (parametric) topic model-based hierarchical structure modeling is that
the hidden topic number in the deﬁned priors needs to be ﬁxed in advance. An
elegant approach to resolve the above issue is Bayesian nonparametric learning.
Although there are state-of-the-art works in existing Bayesian nonparametric
learning could capture non-cooperative hierarchical structures but they fail on
the more general one: cooperative hierarchical structures.
This chapter proposes two Bayesian nonparametric models for the coopera-
tive hierarchical structure learning: Cooperative Hierarchical Dirichlet Process
and Mixed Gamma-Negative Binomial Process. More speciﬁcally, two operations
on the random measures are deﬁned: Inheritance which is from the Hierarchi-
cal Dirichlet Process; Cooperation is an innovation proposed in this chapter
to account for the multiple parent nodes in the cooperative hierarchical struc-
ture. Although the proposed models elegantly captures cooperative hierarchical
structures, it does bring additional challenges to model inference. To resolve this
challenge, this chapter introduces a set of additional latent variables which makes
156

6.2. COOPERATIVE HIERARCHICAL STRUCTURE
Chapter 6
G10
G11
G20
G21
G22
G30
G31
G32
G33
(a) Type-I (Non-cooperative)
G10
G11
G20
G21
G22
G30
G31
G32
G33
(b) Type-II (Cooperative)
Figure 6.1: Two types of hierarchical structures
the model both easy to infer as well as scale to many layers. Finally, the expec-
tations of the topic number from two models are theoretically and empirically
analyzed. Experiments on two multi-label document learning tasks have shown
the eﬃciency of the proposed models on the cooperative hierarchical structure
modeling.
The rest of this chapter is organized as follows.
Cooperative hierarchical
structure is formally deﬁned in Section 6.2. The Cooperative Hierarchical Dirich-
let Process and Mixed Gamma-Negative Binomial Process are presented in Sec-
tion 6.3 and Section 6.4, respectively.
Section 6.5 conducts the comparative
experiments on two multi-label document learning tasks. Section 6.6 summa-
rizes this chapter.
6.2
Cooperative hierarchical structure
As discussed in Introduction, there are two types of hierarchical structures ac-
cording to the number of parents for each node, as illustrated in Fig. 6.1. In
Type-I hierarchical structure, as illustrated in Fig. 6.1(a), each node is with one
and only one parent node. The nodes in this structure could be seen as assigned
157

Chapter 6
6.2. COOPERATIVE HIERARCHICAL STRUCTURE
to diﬀerent groups and, in turn, assigned to the higher level groups. In Type-II
hierarchical structure, as illustrated in Fig. 6.1(b), each node could have more
than one parent node. This structure is named Cooperative Hierarchical Struc-
ture in this chapter. Apparently, Type-II is more general than Type-I because
Type-I could be seen as a special case of Type-II. Note that existing Bayesian
nonparametric models (such as, Hierarchical Dirichlet Process (Teh, 2006) and
its variances (Dai and Storkey, 2015; Paisley et al., 2015; Canini and Griﬃths,
2011)) are all particularly designed after Type-I hierarchical structures but fail
to model Type-II Hierarchical Structures. Consider the example of an author-
paper-word structure. When using the Type-I hierarchical structure to model
it, there must be an implication that each paper is only written by one author.
Despite of certain rationality in some situations, the constraint of Type-I hier-
archical structure is still too restricted to model many real-world phenomenons
(especially for multi-label document learning).
Next, the second type is formally deﬁned:
Deﬁnition 9 (Cooperative Hierarchical Structures) A cooperative hierar-
chical structure [CHS], as illustrated in Fig. 6.1(b), is composed of nodes assigned
to diﬀerent layers. Each node in the structure could be linked to multiple parent
nodes and child nodes.
A real-world example of the cooperative hierarchical structure is: author-
paper-word data. There are three-layer nodes in this data: nodes in ﬁrst layer
denote authors; nodes in second layer denote papers; nodes in third layer denote
words. If an author writes a paper, there will be a link between two corresponding
nodes; similarly, there will be a link between a paper and a word if this paper
contains this word. The authors here could be seen as labels associated with
documents, so authors could be replaced by any other kinds of labels. In this
158

6.2. COOPERATIVE HIERARCHICAL STRUCTURE
Chapter 6
chapter, author-paper-word is used as example for multi-label documents. Some
important notations of this chapter are summarized in Table 6.1.
Table 6.1: Important notations for this chapter
Symbols
Description
Θ
a measurable space
G
a random measure from DP
G0/G1
0
global random measure from DP at the ﬁrst layer
Ga/G2
a random measure from DP at the second layer
Gd/G3
a random measure from DP at the third layer
Gℓ
i
i-th random measure from DP at the ℓ-th layer
N ℓ
the number of random measures at ℓ-th layer
H
base measure of DP
γ
the parameter of H (when it is Dirichlet distribution)
Ω
a random partition
k
an index of partition/factor/topic/dish
K
the number of partitions/factors/topics/dishes
a
a chef/node at the second layer
A
number of chefs/nodes at the second layer
d
a restaurant
D
number of restaurants/nodes at the third layer
t
a table in a restaurant
Td
the table number in restaurant d
T a
d
the table number in restaurant d served by chef a
Ta,o
the number of tables served by menu option o of chef a
Tk
the number of tables served by dish k
o
a menu option on the personal menu
Oa
the number of menu options on the personal menu of chef a
Ok
the number of menu options with dish name k
V
the number of diﬀerent words in a corpus
θk
k-th partition/factor/topic/dish of DP (one point in Θ)
159

Chapter 6
6.2. COOPERATIVE HIERARCHICAL STRUCTURE
θa,o
assigned factor to menu option o of chef a
θd,t
assigned factor to table t in restaurant d
θd,n/θd,i
assigned factor to data/customer n/i in restaurant d
α
concentration parameter of general DP
α0
concentration parameter of global DP at ﬁrst layer
αa
concentration parameter of DPs at second layer
αd
concentration parameter of DPs at third layer
νk
k-th stick break from beta distribution Beta(1, α)
πk
the stick weight of k-th atom/factor from general DP
ν0,k
k-th stick break from beta distribution Beta(1, α0)
π0,k
the stick weight of k-th atom/factor from global DP at ﬁrst layer
νa,o
o-th stick break from beta distribution Beta(1, αa)
πa,k
the stick weight of k-th atom/factor from DP at second layer
νd,t
t-th stick break from beta distribution Beta(1, αd)
πd,k
the stick weight of k-th atom/factor from DP at third layer
za,o
the assigned index of factor/dish of a node at ﬁrst layer for menu option/factor
o of node a at second layer
zd,t
the assigned index of factor/menu option of a node at second layer for ta-
ble/factor t of node d at third layer
zd,n
the assigned index of factor/table of a node at third layer for a data n of d
ˆzd,n
the assigned index of node at second layer to the data n of d
Nd
the number of data/customers in restaurant d
Nd,t
the number of data/customers sitting at table t of restaurant d
N a
d,t
the number of data/customers sitting at table t of restaurant d served by chef
a
ϖd,a
the weight of link between node a and node d
AD
author-document mapping matrix
DN
document-word mapping matrix
Ad
number of authors of document d
Γ0
a global random measure from a Gamma process
r0,k
the global weight of topic k
160

6.3. COOPERATIVE HIERARCHICAL DIRICHLET PROCESSES
Chapter 6
Γd
a random measure from a Gamma process for document d
rd,k
the weight of topic k in document d (the interest of d on k)
Γa
a random measure from a Gamma process for author a
ra,k
the weight of topic k in author a (the interest of a on k)
Γd
a
the mixed measure of measures of all authors who write d
rd
a,k
the average weight of topic k in all author a who write document d
X
a random measure from a Negative binomial process
nk
number of words assigned to topic k
Xd
a random measure for document d from a negative binomial process
nd,k
number of words assigned to topic k in document d
na,k
number of words assigned to topic k and author a
na
d,k
number of words assigned to topic k and author a in document d
6.3
Cooperative hierarchical Dirichlet processes
The cooperative hierarchical Dirichlet process is built on the existing Bayesian
nonparametric prior: hierarchical Dirichlet process (HDP). This section reviews
its deﬁnitions and constructive representations that have been used to under-
stand and build the proposed CHDP in the following.
Hierarchial Dirichlet Processes (Teh, 2006) is built by piling a DP above
another DP through an elegant method which can make factors shared across
the hierarchical structure. Its deﬁnition is as follow
Deﬁnition 10 (Hierarchical Dirichlet Process) A Hierarchical Dirichlet Pro-
cess (HDP) (Teh, 2006) is a distribution over a set of random probability mea-
sures over Θ. The process deﬁnes a set of random probability measures {Gd}D
d=1
and a global random probability measure G0 as shown in Fig. 6.2(a). The global
measure G0 is distributed as a Dirichlet process parameterized by a concentration
161

Chapter 6
6.3. COOPERATIVE HIERARCHICAL DIRICHLET PROCESSES
parameter α and a base (probability) measure H
G0 ∼DP(α, H)
(6.1)
Meanwhile, each random measure Gd is conditionally independent with others
given G0, and is also distributed as a Dirichlet process with parameter αd and a
base probability measure G0
Gd ∼DP(αd, G0)
(6.2)
This deﬁnition actually deﬁnes an operation between two DPs which will
be discussed in more detail in the following. Note that extending the above
two-layer HDP to more layers is straightforward under this deﬁnition. It was
originally designed to model group data. For example, there are D documents
(i.e., groups) and each Gd could be adopted to model one document using the
mixture idea in (Rasmussen, 1999), such as
xi ∼F(θi),
θi ∼G
(6.3)
where xi is a data point generated according to a distribution parameterized by
a draw θi from G.
To capture the cooperative hierarchical structures, two operations between
random measures from DP are formally deﬁned as follows
Deﬁnition 11 (Inheritance) A probability measure G1 is the Inheritance from
another probability measure G2 from DP on space Θ by taking G2 as its base
measure
G1 ∼DP(α1, G2),
G2 ∼DP(α2, H)
(6.4)
where α1 and α2 are DP parameters. The discrete nature of G2 enables G1 to
162

6.3. COOPERATIVE HIERARCHICAL DIRICHLET PROCESSES
Chapter 6
inherit factors/atoms from G2.
Note that this operation is from HDP.
Deﬁnition 12 (Cooperation) A measure G is the Cooperation of two proba-
bility measures, i.e., G1 and G2, from DP on the same space Θ, if
G = G1 ⊕G2
(6.5)
where G is a new probability measure on the space Θ. For any given partition
{Ω}∞
k=1 on Θ, it has
G(Ωk) = G1(Ωk) ⊕G2(Ωk)
(6.6)
where ⊕denotes the convex combination. Extending the Cooperation of more
than two probability measures is straightforward.
Theorem 1 The Cooperation of ﬁnite number of probability measures is still a
probability measure.
Proof 2 A probability measure P is a real-valued function, deﬁned on the col-
lection of events, which should satisfy the following axioms: 1) P(Ωk) ≥0
for every event Ωk; 2) P(Θ) = 1; 3) If {Ωk} is countable and disjoint, then
P(#
k Ωk) = 
k P(Ωk).
Suppose there are a ﬁnite number of probability measures {Gi}I
i=1 and G is
their Cooperation with combination weights {ϖi}I
i=1 and I
i=1 ϖi = 1. For every
event Ωk, each Gi satisﬁes Gi(Ωk) ≥0, so G(Ωk) = 
i ϖiGi(Ωk) ≥0; For the
entire sample space Θ, each Gi satisﬁes Gi(Θ) = 1, so G(Θ) = 
i ϖiGi(Θ) = 1;
Since each measure Gi in Eq. (6.5) is with the same countable set, an injective
function exists from this countable set to the natural numbers. Therefore, the el-
ement pairwise superposition in Eq. (6.6) is well deﬁned. Similarly, G(#
k Ωk) =

i ϖiGi(#
k Ωk) = 
i ϖi

k Gi(Ωk) = 
k

i ϖiGi(Ωk) = 
k G(Ωk).
163

Chapter 6
6.3. COOPERATIVE HIERARCHICAL DIRICHLET PROCESSES
G1
0
G2
2
G2
1
G3
3
G3
2
G3
1
(a) (Three-layer) HDP
G1
0
G2
2
G2
1
G3
3
G3
2
G3
1
(b) CHDP
Figure 6.2: The comparison between generated random measures from HDP and
CHDP.
With both Inheritance and Cooperation in hand, the cooperative hierarchical
Dirichlet process is deﬁned as follow
Deﬁnition 13 (Cooperative Hierarchical Dirichlet Process) A coopera-
tive hierarchical Dirichlet process (CHDP) (Teh, 2006) is a distribution over a
set of random probability measures (over Θ) located at multiple layers as shown
in Fig. 6.2(b). The process deﬁnes:
• Each layer is with a number N ℓof random probability measures {Gℓ
i}i=1:Nℓ
where N 1 = 1 for the ﬁrst layer;
• At the ﬁrst layer ℓ= 1, a single global random probability measure G0
is deﬁned, which is distributed as a Dirichlet process parameterized by a
concentration parameter α and a base probability measure H
G0 ∼DP(α, H)
(6.7)
• At the following layer ℓ> 1, each probability measure Gℓ
i at layer ℓis the
Inheritance from the Cooperation of all probability measures at the upper
layer ℓ−1
Gℓ
i ∼DP(αℓ, Gℓ−1)
(6.8)
164

6.3. COOPERATIVE HIERARCHICAL DIRICHLET PROCESSES
Chapter 6
where
Gℓ−1 = Gℓ−1
1
⊕Gℓ−1
2
⊕· · · ⊕Gℓ−1
Nℓ−1
(6.9)
and αℓis the DP parameter at the layer ℓ.
In the Fig. 6.2(a) and 6.2(b), the generated sets of random measures from
both HDP and CHDP are compared. The similarity is that the random measures
in the second layer are used as the base measures of the DP to generate measures
in the third layer. The diﬀerence is the random measures at the second and third
layers of the HDP, shown in Fig. 6.2(a), have a one-to-many relationship, where
Fig. 6.2(b) (or CHDP) shows a many-to-many relationship.
For clarity, the node at the ﬁrst layer means the G1
0 of CHDP prior in Fig.
6.2(b) but not the (red) nodes of CHS in Fig. 6.1(b); a node at the second layer
means a G2 of CHDP prior in Fig. 6.2(b) which corresponds to a (red) node of
CHS in Fig. 6.1(b); a node at the third layer means the G3 of CHDP prior in
Fig. 6.2(b) which corresponds to an (orange) node of CHS in Fig. 6.1(b) in the
rest of paper.
It can be seen from the Deﬁnition 13 and Fig. 6.2 that HDP can be seen as a
special case of CHDP with each child node/probability measure having only one
parent node/probability measure. If the cooperative/Type-II hierarchical struc-
ture degenerates into Type-I hierarchical structure , the CHDP will degenerate
into a HDP as well.
This study aims to model cooperative hierarchical structures using CHDP-
based model. Based on the Fig. 6.2(b), the following discussion explains how
CHDP is used to model an author-paper-word structure. Suppose the papers
are from the area of Artiﬁcial Intelligence. G1
0 at the top layer deﬁnes a set of
topics within these papers which may be machine learning, constraint processing,
intelligent robotics, and so on. There are two random measures G2
1 and G2
2 at
the second layer for two authors, and G2
1 denotes the interest of ﬁrst author on
165

Chapter 6
6.3. COOPERATIVE HIERARCHICAL DIRICHLET PROCESSES
above topics. Two authors have cooperatively written three papers. G3
1, G3
2 and
G3
3, at the third layer, represent the ratios of topics in three papers respectively.
Combining the mixture in Eq. (6.3), the words in each paper could be generated
where F() is set as a multinomial distribution with a topic as the parameter.
Above procedure could also be seen as the generative process of the CHDP-based
topic model. With the observed data, it could inference all the random measures
in the Fig. 6.2(b). In the remainder of this paper, the above author-paper-word
structure and Fig. 6.2(b) are kept as the example to explain the inference of
CHDP, so the following do not distinguish: node a with author a and node d
with document d.
Next, two constructive representations for CHDP are introduced: stick-breaking
representation (explicit one) and international restaurant process representa-
tion (marginal one), and corresponding inference algorithms for CHDP based on
them.
6.3.1
Stick-breaking representation
Analogous to the stick-breaking representations of DP and HDP (Sethuraman,
1994), a stick-breaking representation for the proposed CHDP is introduced and
an inference algorithm for CHDP is developed based on this representation.
Description
Based on the stick-breaking process for HDP (Sethuraman, 1994; Wang et al.,
2011), the following stick-breaking representation for CHDP has been developed
G0 =
∞

k
π0,kδθk
π0,k = ν0,k
k−1

j=1
(1 −ν0,j)
ν0,k ∼Beta(1, α)
166

6.3. COOPERATIVE HIERARCHICAL DIRICHLET PROCESSES
Chapter 6
Ga =
∞

o
πa,oδθa,o
πa,o = νa,o
o−1

j=1
(1 −νa,j)
νa,o ∼Beta(1, αa)
Gd =
∞

t
πd,tδθd,t
πd,k = νd,t
t−1

j=1
(1 −νd,j)
νd,t ∼Beta(1, αd)
θk ∼H
θa,o = θza,o
za,o ∼π0
θd,t = θzd,t
zd,t ∼
$
ϖd,a1πa1, · · · , ϖd,aAπaA
%
where A is the number of nodes at the second layer and ϖd,a is the weight of
link between node a and node d.
Note that the Inheritance and Cooperation take place between the stick
weights π of the nodes. The diﬃcult part of this model is to infer πa considering
their Cooperation. Since it needs to sample zd,t from {ϖd,a1πa1, · · · , ϖd,aAπaA},
it is worth to note the following property of this set
Theorem 2 The set {ϖd,a1πa1, · · · , ϖd,aAπaA} is a countable inﬁnite set and the
summation of all elements is equal to one.
Proof 3 This set is the union of the stick weights (with an adjustment) from all
the nodes in the second layer linked with the node d. Since stick weights of a node
in the second layer are a countable set, the union of ﬁnite number of countable
sets is still a countable set.
Since the stick weights {πa,o} of each node at the second layer is from a
DP, their summation is deﬁnitely equal to one ∞
o=1 πa,o = 1.
Considering
the adjustment ϖd,a, the summation becomes ∞
o=1 ϖd,aπa,o = ϖd,a. Therefore,
the summation of ﬁnite number A of these stick weights is equal to 1 because

a ϖd,a = 1.
Note that there is no one-to-one mapping between πa,o with πa,k. In fact,
their relation is πa,k = 
o:za,o=k πa,o. Similar with πd,k and πd,t, their relation is
167

Chapter 6
6.3. COOPERATIVE HIERARCHICAL DIRICHLET PROCESSES
πd,k = 
t:zzd,t=k πd,t. Next, the inference method based on this representation is
given.
Inference
This section has designed a Markov Chain Monte Carlo (Andrieu et al., 2003) al-
gorithm to obtain samples of posterior distribution given the observation p(π0, πa,
πb, θ|data), however combining Inheritance and Cooperation presents a challenge,
and the inﬁnite number of factors and their weights make the posterior inference
of the latent variables even harder. One common work-around in nonparametric
Bayesian learning is to use a truncation method. The truncation method (Fox,
2009; Willsky et al., 2009), which uses a relatively big K† as the (potential) max-
imum number of topics, is widely accepted but brings an approximation error.
Another successful technique to resolve this problem is: data/variable augmen-
tation (Van Dyk and Meng, 2001; Tanner and Wong, 2010), also known as Slice
Sampling (Damlen et al., 1999; Neal, 2003b). Therefore, an auxiliary variable-
based inference algorithm has been developed to make inference feasible. Next,
the sampling of each variable is given in detail.
Sampling ud,n. ud,n is an introduced auxiliary variable that can make the
possible assigned factors to data n of node d within a ﬁnite scope, which can
also be known as adaptive truncation
ud,n ∼Unif(0, πd,zd,n)
(6.10)
where zd,n is the former assignment of n and Unif(0, πd,zd,n) denotes a uniform
distribution with support on [0, πd,zd,n].
Sampling πd,t. The update of the stick weights of node d should consider
the adaptive truncation ud,n (i.e., the inequalities between slice variables and the
168

6.3. COOPERATIVE HIERARCHICAL DIRICHLET PROCESSES
Chapter 6
stick weights). In (Neal, 2003b; Kalli et al., 2011), it has been proofed that the
stick breaks satisfy truncated Beta distributions. For t ≤t∗,
νd,t ∼beta(νd,t; 1, αd) · 1(Lt < νd,t < Ut)
(6.11)
where
t∗= max
n {zd,n}
Lt = max
n:zd,n=t
&
ud,n

l:l<t(1 −νd,l)
'
Ut = 1 −max
n:zd,n>t

ud,n
νd,zd,n

l:l<zd,n,l̸=t(1 −νd,l)

(6.12)
where 1() is an indicator function parameterized by a condition and equals 1 if
the condition is satisﬁed; 0, otherwise, and t∗is the maximum number of used
factors of node/document d by the words in this document.
Note that it is
possible to only update these breaks, a ﬁnite number, by leaving the others, an
inﬁnite number, not updated, because the posteriors of other weights are same
as their priors. Since there is a determinant relation between stick breaks and
stick weights, the update of breaks will update weights at the same time.
Sampling zd,n. zd,n is an indicator variable to express the assigned factor
to word n of document d. The posterior conditional distribution is
p(zd,n = t| · · · ) ∝1(ud,n < πd,t) · θza,zd,t,vd,n
(6.13)
where vd,n is the word name of n-th word in document d and θzzd,t,vd,n is the
likelihood of assignment. The adaptive truncation is reﬂected by the sampling
of the zd,n. Only the factors with larger weights than ud,n are considered as the
assignment candidates. Since the value of ud,n is larger than zero, this number
must be ﬁnite.
169

Chapter 6
6.3. COOPERATIVE HIERARCHICAL DIRICHLET PROCESSES
Sampling ud,t. ud,t also functions as an adaptive truncation, but it is a little
more complicated than ud,n because of the Cooperation between the measures in
the second layer. Therefore the following slice variable is introduced
ud,t ∼Unif(0, ϖd,aπzd,t)
(6.14)
where ϖd,a is the weight of link between node a and node d.
Sampling πa,o.
Based on the adaption truncation ud,t, update the stick
breaks of the nodes at the second layer as follows: for o ≤o∗,
νa,o ∼beta(νa,o; 1, αa) · 1(Lo < νa,o < Uo)
(6.15)
where
o∗= max
(d,t) {zd,t}
Lo =
max
(d,t):zd,t=(a,o)
&
ud,t
ϖd,a

l:l<o(1 −νa,l)
'
Uo = 1 −
max
(d,t):zd,t>o

ud,t
ϖd,aνa,zd,t

l:l<o,l̸=o(1 −νa,l)

where Lo needs to consider all nodes at the third layer linked with node a.
Sampling zd,t. zd,t is an indicator variable to express the assigned factor of
a node at the second layer to the t-th factor of node d. The posterior conditional
distribution is
p(zd,t = (a, o)| · · · ) ∝1(ud,t < ϖd,aπa,o) ·

n:zd,n=t
θzzd,t,vd,n
(6.16)
where zd,t is an index that means the t-th factor of node d is assigned to o-th
factor of node a.
Sampling ua,o. ua,o is the slice variable for the random measures at the
170

6.3. COOPERATIVE HIERARCHICAL DIRICHLET PROCESSES
Chapter 6
second layer which is similar to ud,n, and satisﬁes
ua,o ∼U(0, π0,za,o)
(6.17)
Sampling π0,k. The update of the stick breaks of the global measure G0.
For k ≤k∗,
ν0,k ∼beta(ν0,k; 1, α0) · 1(Lk < ν0,k < Uk)
(6.18)
where
k∗= max
(a,o) {za,o}
Lk =
max
(a,o):za,o=k

ua,o

l:l<za,o(1 −ν0,l)

Uk = 1 −
max
(a,o):za,o>k

ua,o
ν0,za,o

l:l<zb,t,l̸=k(1 −ν0,l)

Sampling za,o. This indicator variable is to indicate the assigned node to
o-th factor at the second layer. The posterior conditional distribution is
p(za,o = k| · · · ) ∝1(ua,o < π0,k) ·

(d,n):zd,n=t,zd,t=(a,o)
θk,vd,n
(6.19)
Sampling θk. θk denotes a global factor and its posterior distribution is
p(θk| · · · ) ∝Dir(θk; γ) ·

(d,n):zd,n=t,zd,t=o,za,o=k
θk,vd,n
(6.20)
where Dir(θk; γ) is the prior H and γ is the hyper-parameter.
Next, the reasonability of introducing the auxiliary variables is shown using
ud,t as an example because it is the most complicated and the others follow if it
is veriﬁed.
Theorem 3 Introducing the slice variable ud,t with the distribution in Eq. (6.14)
171

Chapter 6
6.3. COOPERATIVE HIERARCHICAL DIRICHLET PROCESSES
Algorithm 6.1: Slice Sampler for CHDP
initialization;
do
for d = 1; d ≤D do
for n = 1; n ≤Nd do
Update ud,n by Eq. (6.10);
Update πd,t by Eq. (6.11);
for n = 1; n ≤Nd do
Update zd,n by Eq. (6.13);
for t = 1; t ≤Td do
Update ud,t by Eq. (6.14);
for a = 1; a ≤A do
for o = 1; a ≤Oa do
Update πa,o by Eq. (6.15);
Update ua,o by Eq. (6.17);
for d = 1; d ≤D do
for t = 1; t ≤Td do
Update zd,t by Eq. (6.16);
for k = 1; k ≤K do
Update π0,k by Eq. (6.18);
for a = 1; a ≤A do
for o = 1; a ≤Oa do
Update za,o by Eq. (6.19);
for k = 1; k ≤K do
Update θk by Eq. (6.20);
while convergent;
return K, {θk}K
k=1, {πa}A
a=1, {πd}D
d=1;
will not change the posterior distribution of zd,t
p(zd,t = (a, o)| · · · ) = ϖd,aπa,o ·

n:zd,n=t
θzzd,t,vd,n
(6.21)
Proof 4 Here, check the marginal distribution of zd,t after integrating out the
172

6.3. COOPERATIVE HIERARCHICAL DIRICHLET PROCESSES
Chapter 6
newly introduced slice variables
p(zd,t| · · · ) =

ud,t
p(zd,t, ud,t| · · · )dud,t
=

ud,t
p(zd,t|ud,t, · · · ) · p(ud,t| · · · )dud,t
= ϖd,aπa,o ·

(d,n):zd,n=t
θzzd,t,vd,n
(6.22)
It is exactly same as Eq. (6.21).
The entire update procedure is summarized in Algorithm 6.1. The physical
meanings of these variables are explained in the next subsection with a metaphor.
6.3.2
International restaurant process representation
The international restaurant process (IRP) representation for CHDP is described,
then the designed inference method based on this representation is introduced.
Description
The marginal representation of CHDP with G0, {Ga}A
a=1, and {Gd}D
d=1 marginal-
ized out (named international restaurant process) is as follows
Gd
a = ϖd,a1Ga1 + ϖd,a2Ga2 + · · · + ϖd,AGA
θd,n|θd,1, · · · , θd,n−1, Gd
a ∼
Td

t=1
Nd,t

t Nd,t + αd
δθd,t +
αd

t Nd,t + αd
Gd
a
(6.23)
θd,t|θ1,1, · · · , θd,t−1, G0 ∼
Oa

o=1
Ta,o

o Ta,o + αa
δθa,o +
αa

o Ta,o + αa
G0
(6.24)
173

Chapter 6
6.3. COOPERATIVE HIERARCHICAL DIRICHLET PROCESSES
θa,o|θ1,1, · · · , θa,o−1, H ∼
K

k=1
Ok

k Ok + α0
δθk +
α0

k Ok + α0
H
(6.25)
where Nd,t denotes the number of θd,n associated with θd,t in d; Ta,o denotes the
number of θd,t associated with θa,o; and Ok denotes the number of θa,o associated
with θk.
Similar to the Chinese restaurant process of DP outlined Section 2.1.2 and
the Chinese restaurant franchise in HDP in Section 2.2.2, a metaphor eases
understanding of IRP. Since CHDP is based on a three-layer HDP, the following
paragraphs describe the metaphor for three-layer HDP ﬁrst, and then introduces
the one for CHDP. Note that the CRF in Section 2.2.2 is only a two-layer HDP.
As shown in Fig. 6.3(b), the metaphor for the three-layer HDP is as follows:
there is a global menu with diﬀerent dishes {θk}K
k=1 shared by all the chefs
{a} from diﬀerent countries (i.e., China, India, Italy, France). Each chef has a
personal menu with dish names as menu options {θa,o} (Note that menu options
are not eliminative - diﬀerent options could, in fact, be the same dish.) according
to her preference and ability. There are also several (national) restaurants {d}.
Each restaurant has employed one (and only one) chef, but a chef could work
in diﬀerent restaurants. For example, a French restaurant hired a French chef,
but this chef may work in other French restaurants. In each restaurant, there
are multiple tables Td, and each table is served with a dish cooked by the chef
of this restaurant. When a customer n walks into a restaurant d, she sits at an
occupied table with the probability
Nd,t

t Nd,t+αd or a new table with the probability
αd

t Nd,t+αd. If an occupied table is selected, she just eats the dish on this table;
if the table is new, the customer needs to order a dish for this table from the
personal menu of the chef. If option o on the menu is selected with the probability
Ta,o

o Ta,o+αa, she eats it; if she is not satisﬁed by all the current options on the menu
with the probability
αa

o Ta,o+αa, the chef has to add a new option on the menu
174

6.3. COOPERATIVE HIERARCHICAL DIRICHLET PROCESSES
Chapter 6
fish
steak
curry-rice
fish
steak
escargot
fish
steak
fish
steak
curry-rice
escargot
MENU
1. fish
2. steak
3. curry-rice
4. fish
5. escargot
6. curry-rice
MENU
1. escargot
2. steak
3. steak
4. fish
MENU
1. fish
2. steak
3. steak
4. fish
 
(a) International Restaurant Process (IRP)
fish
steak
curry-rice
escargot
fish
steak
fish
steak
curry-rice
escargot
MENU
1. escargot
2. steak
3. steak
4. fish
MENU
1. fish
2. steak
3. steak
4. fish
MENU
1. fish
2. steak
3. curry-rice
4. fish
5. escargot
6. curry-rice
MENU
1. fish
2. steak
3. steak
4. fish
 
(b) (Three-layer) Chinese Restaurant Franchise (CRF)
Figure 6.3: Comparing the Chinese restaurant franchise process (three-layer)
and international restaurant process. There are four restaurants and three chefs
in the ﬁgure. In CRF, all the customers in a restaurant can only be served by
one chef, but the customers in IRP could be served by diﬀerent chefs. The main
diﬀerence between HDP and CHDP is due to Cooperation.
175

Chapter 6
6.3. COOPERATIVE HIERARCHICAL DIRICHLET PROCESSES
from the global shared menu. If dish k on the global menu is selected with the
probability
Ok

k Ok+α0, she eats it; if all the dishes on the global menu still cannot
satisfy this customer with the probability
α0

k Ok+α0, the chefs have to add a new
dish to this global menu (while embarrassedly looking up a recipe book H).
As shown in Fig.
6.3, the metaphor for the CHDP is: there is a global
menu with diﬀerent dishes {θk}K
k=1 shared by all the chefs {a} from diﬀerent
countries (i.e., China, India, Italy, France).
Each chef has a personal menu
with dish names as menu options {θa,o} (again, diﬀerent options could be same
dish). There are also several (international) restaurants {d} served by these chefs.
Each restaurant could employ a number of chefs from diﬀerent countries, and a
chef could work in diﬀerent restaurants as well. For example, an international
restaurant may have a Chinese chef, a French chef, an Italian chef, and an Indian
chef (hence its name, international restaurant). In each restaurant, there are
multiple tables Td with a dish cooked by the chefs employed by this restaurant.
When a customer n walks into an international restaurant d, she sits at an
occupied table with the probability
Nd,t

t Nd,t+αd or a new table with the probability
αd

t Nd,t+αd. If an occupied table is selected, she just eats the dish on this table;
if the table is new, the customer needs to order a dish for this table from the
menus of all the chefs working in this restaurant. If option o on the menu of a
chef a is selected with the probability
Ta,o

o Ta,o+αa, she eats it; if she is not satisﬁed
by the current options on the menu with the probability
αa

o Ta,o+αa, she can ask
this chef a to add a new option on his menu from the global shared menu. If
dish k on the global menu is added to his menu with the probability
Ok

k Ok+α0,
she eats it; if all the dishes on the global menu cannot satisfy this customer with
the probability
α0

k Ok+α0, the chefs have to add a new dish to this global menu.
In this metaphor, the connection to the cooperative hierarchical structures
(e.g., author-paper-word) and CHDP is as follows: a dish θk corresponds to a
176

6.3. COOPERATIVE HIERARCHICAL DIRICHLET PROCESSES
Chapter 6
topic (a factor of global measure G0 at the ﬁrst layer in Fig. 6.2(b)); a chef a
corresponds to an author (a node a in the second layer in Fig. 6.2(b)) and an
menu option of chef a corresponds to a factor θa,o of the measure Ga for node
a; a restaurant d corresponds to a document (a node d in the third layer in Fig.
6.2(b)) and a table of restaurant d corresponds to a factor θd,t of the measure
Gd for node d; a customer n corresponds to a word.
Inference
Next, a Markov Chain Monte Carlo algorithm is designed to obtain samples of
posterior distribution p({θk}, {θa,o}, {θd,t}, K|data, · · · ) of CHDP based on IRP
representation. The ﬁnal aim is to learn the factors and the representations of
all the nodes on these factors, although combining Inheritance and Cooperation
still presents diﬃculties.
Sampling θd,n. Its prior is as in Eq. (6.23). Since Gb
a is diﬀerent from the
one in HDP and hard to marginalize out, an auxiliary variable ˆzd,n is introduced
to make it inferrable. ˆzd,n denotes the selected chef of customer n in restaurant
d. If it is known which chef this customer selects, it is just to assign a dish
to him by marginalizing the probability measure of the selected chef. However,
it must make sure that the introduced auxiliary variable will not change the
whole posterior distribution. Here, the distribution of the auxiliary variable ˆzd,n
is deﬁned as
p(ˆzd,n = a| · · · ) = N a
d,t + ϖd,aαd

t Nd,t + αd
(6.26)
where N a
d,t denotes the number of customers on table t served by chef a in
restaurant d. With the selected chef, θd,n can be sampled by
θd,n|ˆzd,n = a, Ga, · · · ∼
T a
d

t=1
Nd,t
N a
d,t + ϖd,aαd
δθd,t +
ϖd,aαd
N a
d,t + ϖd,aαd
Ga
(6.27)
177

Chapter 6
6.3. COOPERATIVE HIERARCHICAL DIRICHLET PROCESSES
where T a
d is the table number in restaurant d served by chef a and t ∈a denotes
table t served by chef a. If a new dish is needed, it needs to sample from Ga.
Theorem 4 Introducing the auxiliary variable ˆzd,n with the distribution in Eq.
(6.26) will not change the ﬁnal posterior distribution.
Proof 5 The marginal distribution of θd,n with ˆzd,n marginalized out is
p(θd,n) =

ˆzd,n
p(θd,n, ˆzd,n)p(ˆzd,n)
=

a
p(θd,n|ˆzd,n = a)p(ˆzd,n = a)
=

a
⎛
⎝
T a
d

t=1
Nd,t
N a
d,t + ϖd,aαd
δθd,t +
ϖd,aαd
N a
d,t + ϖd,aαd
Ga
⎞
⎠· N a
d,t + ϖd,aαb

t Nd,t + αd
=

a
⎛
⎝
T a
b

t=1
Nd,t

t Nd,t + αd
δθd,t +
ϖd,aαd

t Nd,t + αd
Ga
⎞
⎠
=
Td

t=1
Nd,t

t Nd,t + αd
δθd,t +
αd

t Nd,t + αd
(ϖd,a1Ga1 + ϖd,a2Ga2 + · · · )
=
Td

t=1
Nd,t

t Nd,t + αd
δθd,t +
αd

t Nd,t + αd
Gd
a
The result is same as in Eq. (6.23). So one conclusion is drawn that intro-
ducing an auxiliary variable will not impact on the posterior distribution of the
θd,n.
In the MCMC algorithm, the posterior distribution is approximated by its
samples, which is the product of the prior and likelihood. The above demon-
178

6.3. COOPERATIVE HIERARCHICAL DIRICHLET PROCESSES
Chapter 6
strates the prior, and the likelihood is derived by
L(θd,n) ∝
⎧
⎪
⎨
⎪
⎩
F(vd,n|θd,t)
if t is occupied
Oa
o=1
Ta,o

o Ta,o+αaF(vd,n|θa,o) +
αa

o Ta,o+αaLC(vd,n)
if t is new
(6.28)
where vd,n denotes the customer n in restaurant d and
LC(vd,n) =
K

k=1
Ok

k Ok + α0
F(vd,n|θk) +
α0

k Ok + α0
LK(vd,n)
LK(vd,n) =

θ
F(vd,n|θ)H(θ)dθ
and Ok is the number options with dish k; F is a multinomial distribution with
θ as a parameter; and H is a V -dimensional Dirichlet distribution parameterized
by γ. Due to the conjugate relationship between the multinomial and Dirichlet
distributions, there is a closed-form for LK.
The remaining posterior distributions simply follow the three-layer HDP. For
the self-contained reason, the likelihood parts of remaining variables are also
listed below.
Sampling θa
d,t. For assigning an menu option o to each table served by chef
a in restaurant d, the prior for θa
d,t is as the one in Eq. (6.24) and the likelihood
part is
L(θa
d,t) ∝
⎧
⎪
⎨
⎪
⎩
F(vd,t|θa,o)
if o is occupied
K
k=1
Ok

k Ok+α0F(vd,t|θk) +
α0

k Ok+α0LK(vd,t)
if o is new
(6.29)
where vd,t denotes all the customers sitting on the table t in restaurant d, Ok is
179

Chapter 6
6.3. COOPERATIVE HIERARCHICAL DIRICHLET PROCESSES
the number menu options with dish k
LK(vd,t) =

θ
F(vd,t|θ)H(θ)dθ.
Sampling θa,o For assigning a dish k to each menu option o of chef a, the
prior is as the one in Eq. (6.25) and the likelihood part is
L(ηa,o) ∝
⎧
⎪
⎨
⎪
⎩
F(va,o|θk)
if k is occupied
LK(va,o)
if k is new
(6.30)
where va,o denotes all the customers served by the o-th menu option of chef a
LK(xa,o) =

θ
F(va,o|θ)H(θ)dθ.
Sampling θk. θk denotes a global factor/topic and its posterior distribution
is
p(θk| · · · ) ∝Dir(θk; γ) · F(vk|θk)
(6.31)
where vk is total number of customers assigned to k.
The whole procedure for the inference of IRP is summarized in Algorithm
6.2.
6.3.3
Model analysis
The Bayesian nonparametric models (i.e., diﬀerent stochastic processes or their
designed combinations) actually provide a prior for the number of factors. Ap-
parently, the expected factor number from this prior is determined by the param-
eters of the designed nonparametric priors, so it is necessary to investigate the
relationships between the model parameters with the expectation of the factor
180

6.3. COOPERATIVE HIERARCHICAL DIRICHLET PROCESSES
Chapter 6
Algorithm 6.2: Gibbs Sampler for IRP
initialization;
do
for d = 1; d ≤D do
for n = 1; n ≤Nd do
Update ˆzd,n by Eq. (6.26);
Update θd,n by Eq. (6.23) and Eq. (6.28);
for t = 1; t ≤Td do
Update θa
d,t by Eq. (6.24) and Eq. (6.29);
for a = 1; a ≤A do
for o = 1; o ≤Oa do
Update θa,o by Eq. (6.25) and Eq. (6.30);
Update θ by Eq. (6.31);
while convergent;
return K, {θk}K
k=1, {{θd,t}Td
t=1}D
d=1}, {{θa,o}Oa
o=1}A
a=1 ;
number. For the proposed CHDP, the following result holds
Theorem 5 Given a (three-layer) cooperative hierarchical structure, the expect-
ed factor number from the CHDP is
E[K] =

a Oa

o=1
α0
o −1 + α0
(6.32)
where
Oa =
Ta

t=1
αa
t −1 + αa
,
Ta =
D

d
ϖd,aTd,
Td =
Nd

n=1
αd
n −1 + αd
(6.33)
and ϖd,a is the weight of link between node a at the second layer and node d at
the third layer; α0, αa and αd are three parameters of the CHDP.
Proof 6 CHDP is composed by three layer DPs. Start the factor number expec-
tation computation from the bottom. In the Section 6.3.2, the customers selecting
181

Chapter 6
6.3. COOPERATIVE HIERARCHICAL DIRICHLET PROCESSES
1
2
3
4
5
6
7
8
9
10
2
4
6
8
10
12
14
α0
Topic Number
IRP
Stick
Theory
(a) parameter α0
1
2
3
4
5
6
7
8
9
10
1
1.5
2
2.5
3
3.5
4
4.5
5
5.5
6
αa
Topic Number
IRP
Stick
Theory
(b) parameter αa
1
2
3
4
5
6
7
8
9
10
2
4
6
8
10
12
αd
Topic Number
IRP
Stick
Theory
(c) parameter αd
Figure 6.4: The comparisons between empirical and expected factor numbers
of CHDP with two representations: Stick and IRP. Since the factor number is
parameterized by α0, αa and αd.
tables in a restaurant follow a DP, and considering the expectation of DP (Fer-
guson, 1973), the expected number of tables in a restaurant d can be derived from
Td =
Nd

n=1
αd
n −1 + αd
(6.34)
where Nd is the number of customers in restaurant d.
A little diﬃcult part is the second layer DPs or the expected option numbers
of chefs. For the HDP, it is simply by counting all the tables in the restaurants
182

6.3. COOPERATIVE HIERARCHICAL DIRICHLET PROCESSES
Chapter 6
served by the same chef. The CHDP diﬀers from the HDP in that the expected
option number of a chef at the second layer is derived by counting all the tables
served by this chef in all the restaurants. Since there is a Cooperation between
the DPs at the second layer in CHDP, the tables in a restaurant are not served
by a single chef. As shown in Eq. (6.5), the Cooperation has the weights {ϖd,a}
and 
a ϖd,a = 1, so the expected number of tables in restaurant d from chefs a
is ϖd,aTd. The total expected number of the tables served by chef a is
Ta =
D

d
ϖd,aTd
(6.35)
where ϖd,a is the weight of link between node a at the second layer and node d
at the third layer. Ta can be seen as the observations of DP of chef a, so the
expected option number of a is
Oa =
Ta

t=1
αa
t −1 + αa
(6.36)
Lastly, all the menu options are, in turn, seen as the observations of the global
DP, so the expected factor number is
E[K] =

a Oa

o=1
α0
o −1 + α0
(6.37)
Theorem 5 is proven.
The above theoretical result is also empirically veriﬁed through experiments.
As discussed, the expected factor numbers from the Bayesian nonparametric
prior are parameterized by a set of variables. More speciﬁcally, the expected
factor number of CHDP (including two representations: stick-breaking and IRP)
is parameterized by α0, αa and αd. In the CHDP, a cooperative hierarchical
183

Chapter 6
6.4. MIXED GAMMA-NEGATIVE BINOMIAL PROCESSES
structure with size A = 10, D = 20 and V = 50 is ﬁrstly randomly generated.
The links between nodes at three layers are also randomly set.
The models
(i.e., stick-breaking in Algorithm 6.1 and IRP in Algorithm 6.2) was run on this
generated cooperative hierarchical structure with diﬀerent values of parameters
and ignoring the data likelihood, and then compared the learned empirical factor
number and the theoretical factor number. The results are shown in Fig. 6.4.
Three subﬁgures denote the three parameters of the model, respectively. From
these results, it can be seen that the theoretical factor number is very close
to the empirical factor number, so we have veriﬁed the result on the expected
factor number. Moreover, we have demonstrated the trend of the expected factor
number under the change of diﬀerent parameters (parameter sensitivity). From
these ﬁgures, it can be seen that the expected factor number is very sensitive to
the value of α0 but not αa and αd.
6.4
Mixed Gamma-negative binomial processes
This section ﬁrstly proposes a mixed Gamma-negative binomial processes model
(MGNBP) while author-paper-word is still kept as an example to explain why this
model could be used for multi-label document learning; then introduces a Gibbs
sampling strategy to inference the proposed model; The signiﬁcant property,
i.e., expectation of topic number, from the proposed model is theoretically and
empirically analyzed.
6.4.1
Model description
Since the proposed model is built on Gamma-negative binomial process, it is
ﬁrstly introduced in detail followed by the propose model. Normally, the negative
binomial process is used as the likelihood part in a Bayesian nonparametric
184

6.4. MIXED GAMMA-NEGATIVE BINOMIAL PROCESSES
Chapter 6
H
Γ
Xd
α
pd
H
Γ
Γd
Xd
α
pd
D
D
Figure 6.5: Gamma-negative binomial process topic model. The left subﬁgure is
related to Eq. (6.38) and the right hand part is related to Eq. (6.39).
model. Analogous to a negative binomial distribution x ∼NB(r, p) which has
two parameters: r > 0 and p ∈[0, 1], there are two kinds of priors for a negative
binomial process: one is Gamma process (Zhou and Carin, 2015) and the other
is the beta process (Broderick et al., 2015). In this chapter, the Gamma process
prior is used. A Gamma-negative binomial process-based topic model is proposed
in (Zhou and Carin, 2015) as shown in Fig. 6.5 and can be represented as
Γ0 ∼GaP(c0, H)
Xd ∼NBP(pd, Γ0)
(6.38)
where pd is a real-valued parameter within [0, 1] and the base measure of the
negative binomial process Γ0 is a random measure from a Gamma process. Xd
is for a document, and this hierarchial form makes the documents share a same
base measure Γ0. This Gamma-negative binomial process can be (in distribution)
185

Chapter 6
6.4. MIXED GAMMA-NEGATIVE BINOMIAL PROCESSES
equivalently augmented as Gamma-Gamma-Poisson process
Γ0 ∼GaP(c0, H)
Γd ∼GaP
1 −pd
pd
, Γ0

Xd ∼PP(Γd)
(6.39)
where PP(Γd) is a Poisson process with parameter Γd. This augmentation is
useful for the closed-form model inference algorithm design.
Consider the Gamma-negative binomial process topic model in Eqs. (6.38)
and (6.39): despite its successful, this model however is fundamentally the same
as the basic topic models, which are used for modeling the data of two layer
hierarchy: instance-feature (i.e., document-word). Multi-label learning requires
to model the data with three-layer hierarchy: label-instance-feature (i.e., author-
document-word). So an intuitive idea is to add another Gamma process layer
to capture the additional label (i.e, author) layer based on the Gamma-negative
binomial process topic model in Eq. (6.39) analogues to the hierarchical mech-
anism of Hieratical Dirichlet Process (Teh, 2006)
Γ0 ∼GaP(c0, H)
Γa ∼GaP(ca, Γ0)
Γd ∼GaP((1 −pd)/pd, Γd
a)
Xd ∼PP(Γd)
where Γa is the additional layer for the label (i.e, author). This model is called
a three-layer Gamma-negative binomial process topic model (3GNB), which is
graphically shown in the left subﬁgure of Fig. 6.6.
186

6.4. MIXED GAMMA-NEGATIVE BINOMIAL PROCESSES
Chapter 6
H
Γ0
Γa
Γd
Xd
c0
ca
pd
D
A
H
Γ0
c0
Γa1
Γa2 · · · ΓaA
ca
Γd
pd
Xd
D
A
Figure 6.6: Gamma-Gamma-negative binomial process topic model (left one)
and mixed Gamma-negative binomial process topic model (right one).
More speciﬁc, the global measure in the 3GNB model is
Γ0 =
∞

k=1
r0,kδθk
(6.40)
where r0,k is the global weight of topic θk. This global measure deﬁnes a set
of global topics {θk}∞
k=1 shared by all documents, and {r0,k}∞
k=1 indicates the
overall “interests” of documents on topics. The number of topics can potentially
be inﬁnite and therefore justiﬁes the inﬁnity in the summation. However, since
the data is limited, the learned topics will be also limited. Each author a is then
assigned a realization of Gamma process
Γa =
∞

k=1
ra,kδθk
(6.41)
187

Chapter 6
6.4. MIXED GAMMA-NEGATIVE BINOMIAL PROCESSES
where ra,k is the weight of k-th topic θk which is inherited from the global measure
Γ0. {ra,k}∞
k=1 can be viewed as the “interest” of author a on the topics {θk}∞
k=1.
Similar to the author, each document is also assigned a realization of Gamma
process
Γd =
∞

k=1
rd,kδθk
(6.42)
where {rd,k}∞
k=1 is the weight of “interests” of document d on the topics inherited
from the global measure Γ0 again. In the 3GNB model, the base measure for Γd
is from its author Γa. It can be seen as Inheritance. Finally, the likelihood is a
realization of Poisson process
Xd =
∞

k=1
nd,kδθk
(6.43)
where nd,k is the number of words in document d assigned to topic k.
When applying 3GNB to multi-label learning, there is a signiﬁcant issue that
each Γd could only have one parent Γa as its base measure which means that
each instance is with one and only one label (i.e., a document could only have
one author). In order to resolve this issue, the innovative idea is to combine all
the Gamma processes of every authors of a document together by
Γd
a = ϖd
a1Γa1 + ϖd
a2Γa2 + · · · + ϖd
aAdΓaAd
(6.44)
where Ad is the number of authors of document d; ϖd
a1 is the weight of label a1
on instance d; 
a ϖd
a = 1 (i.e., the contribution of author a1 to document d);
and Γd
a is the mixed prior for Γd. Note that the plus here is element-wise because
each Γa is with inﬁnite number of component. This element-wise plus operation
is reasonable because the components of each Γa is countable and they are all
with same discrete base measure Γ0. The mixed Gamma process Γd
a can be seen
188

6.4. MIXED GAMMA-NEGATIVE BINOMIAL PROCESSES
Chapter 6
as the Cooperation of all the authors of a document. To summarize, the proposed
Mixed Gamma-Negative Binomial Processes-based Topic Model (MGNBP) is as
follow
Γ0 ∼GaP(c0, H)
Γa ∼GaP(ca, Γ0)
Γd
a = ϖd
a1Γa1 + ϖd
a2Γa2 + · · · + ϖd
aAΓaA
Γd ∼GaP((1 −pd)/pd, Γd
a)
Xd ∼PP(Γd)
and its graphical representation is shown in right subﬁgure of Fig. 6.6.
6.4.2
Model inference
It is diﬃcult to perform posterior inference under inﬁnite mixtures, a common
work-around solution in Bayesian nonparametric learning is to use a truncation
method (Fox et al., 2011a; Blei, Griﬃths and Jordan, 2010). Truncation method
is widely accepted, which uses a relatively big K† as the (potential) maximum
number of topics. Under the truncation, the model can be expressed below as a
good approximation to the inﬁnite model
γ0 ∼Gamma(e0, 1/f0)
r0,k|γ0, c0 ∼Gamma(γ0/K†, 1/c0)
ra,k|r0, ca ∼Gamma(r0,k, 1/ca)
pd ∼beta(a0, b0)
rd
a,k = ϖd
a1ra1,k + ϖd
a2ra2,k + · · · ϖd
aAdraAd,K†
rd,k|ra, pd ∼Gamma(rd
a,k, pd/(1 −pd))
189

Chapter 6
6.4. MIXED GAMMA-NEGATIVE BINOMIAL PROCESSES
nd,k ∼Pois(rd,k)
Nd =
K†

k=1
nd,k
θ1:K† ∼1
γ0
H
zd,n ∼Multi(rd,1/

rd, · · · , rd,K†/

rd)
wd,n ∼θzd,n
where γ0 =
(
dH is the total mass of measure H and the parameters are given
the appropriate priors. Here, H is a V -dimensional Dirichlet distribution, and
each θ is a topic that is a V -dimensional vector.
The diﬃcult part of the inference for this model is the mixed part Γd
a or rd
a.
Since rd
a is the mixed value, it is hard to infer the posterior of ra through its
likelihood. In order to resolve this issue, the Additive Property of the negative
binomial distribution is ﬁrstly introduced.
Theorem 6 If Xi follows a negative binomial distribution with parameters ri
and p and if the various Xi are independent, then  Xi follows a negative bino-
mial distribution with parameters  ri and p.
In MGNBP model,
rd,k|{ra}, pd ∼Gamma(rd
a,k, pd/(1 −pd))
nd,k ∼Pois(rd,k)
(6.45)
which are (in distribution) equal to
nd,k ∼NB(rd
a,k, pd)
(6.46)
190

6.4. MIXED GAMMA-NEGATIVE BINOMIAL PROCESSES
Chapter 6
and according to Theorem 6, it is further (in distribution) equal to
na
d,k ∼NB

ϖd
a · ra,k, pd

nd,k =

a
na
d,k
(6.47)
where {na
d,k} are independent with each others.
nd,k the number of words assigned to topic k in document d have been split
into a number Ad of independent variables {na
d,k}. Here, na
d,k denotes the number
of words assigned to topic k from author a in document d. From Eq. (6.47),
it can be seen that if the likelihood part of the ra is known, the ra can be
updated using na
d. Introducing the auxiliary variables {na
d,k} helps us resolve the
diﬃcult inference problem brought by the mixed Gamma process. Note that the
independence between the elements of {na
d,k} is very important, which facilitates
us update each na
d,k independently.
According to the relationship between the negative binomial distribution and
the Gamma-Poisson distribution, for each na
d,k,
na
d,k ∼NB(ϖd
ara,k, pd) =⇒
ra
d,k ∼Gamma(ϖd
ara,k,
pd
1 −pd
), na
d,k ∼Pois(ra
d,k)
(6.48)
It needs to highlight that ra
d,k is diﬀerent from rd
a,k; rd
a,k is the mixed Gamma
process of multiple author Gamma processes Γa of Gamma process Γd of docu-
ment d and ra
d,k is the interest of document d on topic k inherited from author
a.
Due to the non-conjugacy of Gamma distribution and negative binomial dis-
tribution, it is diﬃcult to update ra with a Gamma prior. In order to make the
inference with only close-formed conditional distributions, the following result
on the negative binomial process is used
191

Chapter 6
6.4. MIXED GAMMA-NEGATIVE BINOMIAL PROCESSES
Theorem 7 (Zhou and Carin, 2015) If X follows a negative binomial distribu-
tion X ∼NB(r, p) with parameters r and p, then X can also be generated from
a compound Poisson distribution as
X =
l

t=1
ut, ut
i.i.d
∼Log(p), l ∼poiss (−rln(1 −p))
(6.49)
where Log() is a Logarithmic distribution. Furthermore, this Poisson-logarithmic
bivariate count distribution, p(X, l), can be expressed as
X ∼NB(r, p), l ∼CRT(X, r)
(6.50)
where CRT() denotes a Chinese Restaurant distribution, and its deﬁnition and
sampling can be found in (Zhou and Carin, 2015).
With Theorem 7, the Eq. (6.48) is also equal to
na
d,k ∼NB(ϖd
a · ra,k, pd) =⇒na
d,k ∼
la
d,k

1
log(pd), la
d,k ∼Pois(−ϖd
ara,kln(1 −pd))
=⇒la
d,k ∼CRT(na
d,k, ϖd
ara,k), na
d,k ∼NB(ϖd
ara,k, pd)
(6.51)
Finally, all na
d,k can be updated by
(na1
d,k1, · · · , naA
d,K) ∼Mult(nd, ϖd
a1ra1
d,k1
rd
, · · · ,
ϖd
aAdraA
d,K
rd
)
rd =

a

k
ϖd
a · ra
d,k
(6.52)
192

6.4. MIXED GAMMA-NEGATIVE BINOMIAL PROCESSES
Chapter 6
and for each word n in a document d, assign it to a topic k and author a by
p(zd,n = k, ˆzd,n = a) ∝ϖd
ara
d,k
rd
nd,k =

n
δ(zd,n = k)
na,k =

d

n
δ(zd,n = k & ˆzd,n = a)
(6.53)
where zd,n is the topic index assigned to word n in document d.
With these changes of variables, the original model is re-formulated as
γ0 ∼Gamma(e0, 1/f0)
r0,k|γ0, c0 ∼Gamma(γ0/K†, 1/c0)
pd ∼beta(ad,0, bd,0)
ra,k|r0, ca ∼Gamma(r0,k, 1/ca)
rd
a,k = ϖd
a1ra1,k + ϖd
a2ra2,k + · · · ϖd
aAdraAd,K†
rd,k|ra, pd ∼Gamma(rd
a,k, pd/(1 −pd))
ra
d,k ∼Gamma(ϖd
ara,k, pd/(1 −pd)),
a ∈Ad
za
d,n ∼Discrete(ϖd
ara
d,k
rd
, · · · )
nd,k =

n
δ(zd,n = k)
na,k =

d

n
δ(zd,n = k & ˆzd,n = a)
na
d,k =

n
δ(zd,n = k & ˆzd,n = a)
Nd =

n

a
za
d,n
where Ad is the set of associated authors of document d.
193

Chapter 6
6.4. MIXED GAMMA-NEGATIVE BINOMIAL PROCESSES
In the following, a Gibbs sampling algorithm is designed for the posterior
inference and all the conditional distributions are listed below.
Sampling z
p(zd,n = k, ˆzd,n = a| · · · ) ∝θk,n · ϖd
ara
d,k
(6.54)
Sampling ra
d
p(ra
d,k| · · · ) ∝Gamma(ϖd
ara,k + na
d,k, pd)
(6.55)
Sampling la
d
p(la
d,k| · · · ) ∝CRT

na
d,k, ϖd
ara,k

(6.56)
Sampling pd
rd
a,k = ϖd
a1ra1,k + ϖd
a2ra2,k + · · ·
p(pd| · · · ) ∝Beta
 
a0 +

k
nd,k, b0 +

k
rd
a,k
!
p(rd,k| · · · ) ∝Gamma(rd
a,k + nd,k, pd)
(6.57)
Sampling ra
p(ra,k| · · · ) ∝Gamma
 
r0,k +

d with a
la
d,k,
1
ca −
d with a ϖd
a · ln(1 −pd)
!
(6.58)
Sampling la
p(la,k| · · · ) ∝CRT
 
d with a
la
d,k, r0,k
!
(6.59)
194

6.4. MIXED GAMMA-NEGATIVE BINOMIAL PROCESSES
Chapter 6
Sampling r0,k
p(r0,k| · · · ) ∝Gamma
 
γ0/K† +

a
la,k,
1
c0 −
a ln(1 −pa)
!
(6.60)
where
pa =
−
d with a ϖd
aln(1 −pd)
ca −
d with a ϖd
aln(1 −pd)
(6.61)
Sampling l′
k
p(l′
k| · · · ) ∝CRT
 
a
la,k, γ0/K†
!
(6.62)
Sampling γ0
p(γ0| · · · ) ∝Gamma
 
e0 +

k
l′
k,
1
f0 −ln(1 −p′)
!
(6.63)
where
p′ =
−
a ln(1 −pa)
c0 −
a ln(1 −pa)
(6.64)
Sampling θk
p(θk| · · · ) ∝H(θk)

d
θzd,n=k,n
(6.65)
It can be seen from these conditional distributions that all of them are closed-
form which is very easy to update and implement.
The whole procedure is
summarized in Algorithm 6.3.
After obtaining the samples of the posterior
p(θ, ra, rd, r0, za
d,n, pd, γ0, na
d,k| · · · ) of latent variables and removing the burn-in
stage, the algorithm ﬁrstly identiﬁes the topic number with largest frequency as
the Kreal, and then ﬁnd the sample with largest likelihood and K = Kreal from
these samples. The output of Gibbs sampler are the latent variables θ, ra and
rd in this sample.
195

Chapter 6
6.4. MIXED GAMMA-NEGATIVE BINOMIAL PROCESSES
Algorithm 6.3: Gibbs Sampler for MGNBP
Input: D, A, N, AD, DN
Output: Kreal, {θ}, {ra}, {rd}
initialization;
while iter ≤maxiter do
for d = 1; d ≤D do
for n = 1; n ≤Nd do
Update zd,n and ˆzd,n by Eq. (6.54);
for a = 1; a ≤Ad do
Update ra
d,k by Eq. (6.55);
Update la
d,k by Eq. (6.56);
Update rd,k and pd by Eq. (6.57);
for a = 1; a ≤A do
Update ra,k by Eq. (6.58);
Update la,k by Eq. (6.59);
Update r0,k by Eq. (6.60);
Update l′
k by Eq. (6.62);
Update γ0 by Eq. (6.63);
Update θ by Eq. (6.65);
iter + +;
Identify Kreal;
Select the sample with largest likelihood and K = Kreal;
return {θ}, {ra}, {rd};
6.4.3
Model analysis
A distinguishing characteristic of Bayesian nonparametric model is that the num-
ber of the factors/topics to be learned is not speciﬁed a priori. Roughly speaking,
Bayesian nonparametric model could be simply seen as a prior for the this num-
ber. Conditioned on the observed data, the model could determine how many
factors/topics are needed. It would be interesting to investigate the prior ex-
pectation of the factors/topics number under the deﬁned model. The following
result is given
Theorem 8 Given D instances, A labels, and their mapping AD, the expected
196

6.4. MIXED GAMMA-NEGATIVE BINOMIAL PROCESSES
Chapter 6
factor number from the MGNBP is

r0
 
1 −

a
)
ca
ca −
d:AD[a,d]>0 ϖd
a ln(1 −pd)
*r0!
· γ0 · exp(−c0 · r0)
r0
· dr0
(6.66)
and when a truncation level K† is applied, the expected factor number is
K†
⎛
⎝1 −
)
c0
c0 −
a log
ca
ca−
d:AD[a,d]>0 ϖda ln(1−pd)
* γ0
K† ⎞
⎠
(6.67)
where γ0, c0, ca and pd are four parameters of the MGNBP.
Proof 7 It needs to ﬁrstly introduce the following theorem of a completely ran-
dom measure
Theorem 9 (Kingman, 1992) Campbell’s Theorem Let Π be a Poisson pro-
cess on Θ with mean measure μ, and let f : Θ →R be measurable. Then the
sum

=

Y ∈Π
f(Y )
(6.68)
is absolutely convergent with probability if and only if

Θ
min(|f(y)|, 1)μ(dy) < ∞
(6.69)
If this condition holds, the expectation
E[

] =

Θ
f(y)μ(dy)
(6.70)
exists if and only if the integral converges.
Since the proposed MGNBP is a completely random measure, the above the-
orem can be utilized to compute the expectation of sum of its variables. Deﬁne a
197

Chapter 6
6.4. MIXED GAMMA-NEGATIVE BINOMIAL PROCESSES
random variable
Xk = 1(
D

d=1

AD[a,d]=1
Ca
d,k > 0)
(6.71)
which equals to 1 if the factor k is used; 0, otherwise. The expected factor number
is E[
k Xk]. Then, according to the Theorem 9,
E[

k
Xk]
= E
)
E
)
k
Xk|r0,k
**
= E
)
E
)
E
)
k
Xk|{ra,k}
*
|r0,k
**
= E
)
E
)
k
E [Xk|{ra,k}] |r0,k
**
= E
⎡
⎣E
⎡
⎣
k
⎛
⎝1 −
D

d=1

a:AD[a,d]>0
(1 −pb)ϖd
ara,k
⎞
⎠|r0,k
⎤
⎦
⎤
⎦
= E
⎡
⎣
k
E
⎡
⎣
⎛
⎝1 −
D

d=1

a:AD[a,d]>0
(1 −pd)ϖd
ara,k
⎞
⎠|r0,k
⎤
⎦
⎤
⎦
= E
 
k

r1,k
· · ·

rA,k
⎛
⎝1 −
D

d=1

a:AD[a,d]>0
(1 −pd)ϖd
ara,k
⎞
⎠
·
 A

a=1
car0,k
Γ(r0,k)ra,k
r0,k−1 exp(−ca · ra,k)

dr1,k · · · drA,k

= E
 
k
 
1 −

a
)
ca
ca −
d:AD[a,d]>0 ϖd
a ln(1 −pd)
*r0,k! 
=

r0
 
1 −

a
)
ca
ca −
d:AD[a,d]>0 ϖd
a ln(1 −pd)
*r0!
· νGaP(r0) · dr0
=

r0
 
1 −

a
)
ca
ca −
d:AD[a,d]>0 ϖd
a ln(1 −pd)
*r0!
· γ0 · exp(−c0 · r0)
r0
· dr0
198

6.4. MIXED GAMMA-NEGATIVE BINOMIAL PROCESSES
Chapter 6
This integral cannot be easily solved. An approximate method for this integral
computation is Monte Carlo. If applying a truncation level K†, the expectation
is
E[
K†

k
Xk] =

r0,1···r0,K†
K†

k
 
1 −

a
)
ca
ca −
d:AD[a,d]>0 ϖd
a ln(1 −pd)
*r0,k!
·
K†

k=1
c0γ0/K†
Γ(γ0/K†)r0,k
γ0/K†−1 exp(−c0r0,k)dr0,1 · · · dr0,K†
=

k
⎧
⎨
⎩1 −
)
c0
c0 −
a log
ca
ca−
d:AD[a,d]>0 ϖda ln(1−pd)
* γ0
K† ⎫
⎬
⎭
=K†
⎛
⎝1 −
)
c0
c0 −
a log
ca
ca−
d:AD[a,d]>0 ϖda ln(1−pd)
* γ0
K† ⎞
⎠
The theorem is proofed.
The above theoretical result is also supported by simulation results, sum-
marized in Fig. 6.7. At ﬁrst, set A = 10, D = 20, and the mapping relations
between labels and instances are also randomly generated. Simulate the model
with the above setting and diﬀerent values of parameters, and then compare
the empirical factor number and the theoretical factor number from Theorem 8.
The default values of them are: γ0 = 1, c0 = 1, ca = 1 and pd = 0.5. When
investigating one parameter, the other three will be ﬁxed as the default values.
Four subﬁgures in Fig. 6.7 denote the changing of factor number as the changing
of four parameters of the model, respectively. In each subﬁgure, Expectation-MC
denotes the Monte Carlo approximation of the expectation of the factor number
from Eq. (6.66); Expectation-trun denotes the truncation-based approximation
of the expectation of the factor number from Eq. (6.67) (Note that the imple-
mentation of the MGNBP is based on the truncation K† = 1000); the x-axes of
c0 and ca are in negative (base-10) log space. From these results, it can be seen
199

Chapter 6
6.4. MIXED GAMMA-NEGATIVE BINOMIAL PROCESSES
1
2
3
4
5
6
7
8
9
10
γ0
0
5
10
15
20
25
Topic Number
MGNBP
Expecation-MC
Expecation-trun
(a) parameter γ0
0
1
2
3
4
5
6
7
8
9
10
-log(c 0)
0
5
10
15
20
25
30
Topic Number
MGNBP
Expecation-MC
Expecation-trun
(b) parameter c0
0
1
2
3
4
5
6
7
8
9
10
-log(c a)
2
2.5
3
3.5
4
4.5
5
5.5
6
Topic Number
MGNBP
Expecation-MC
Expecation-trun
(c) parameter ca
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
p b
5
10
15
20
25
30
Topic Number
MGNBP
Expecation-MC
Expecation-trun
(d) parameter pd
Figure 6.7: The comparisons between expected and empirical factor numbers of
MGNBP under diﬀerent parameters: γ0, c0, ca and pd. Note that the x-axes of
c0 and ca are in negative (base-10) log space.
that the theoretical factor number is very close to the empirical factor number,
so this veriﬁed the results on the expected factor number of the MGNBP. The
trends of the empirical and expected factor number with parameters γ0 and pd
are extremely close with each others. For the parameter ca, the trends are also
close; for the parameter c0, Expectation-MC is a little away from the others as
the increasing of the value of c0. These subﬁgures do not only verify the above
200

6.5. EXPERIMENTAL EVALUATION
Chapter 6
theoretical result, they also show the sensitivity of the model to the parameters.
6.5
Experimental evaluation
Two experiments in this section to demonstrate the proposed models on coop-
eratively hierarchical structure modeling using the real-world tasks. The public
datasets used for the experimental evaluation are:
• NIPS papers1 This dataset contains papers from the NIPS conferences
between 1987 and 1999. This dataset is a structure: author-paper-word. It
contains 1,740 papers with 2,037 authors, a total of 2,301,375 word tokens
and a vocabulary size of 13,649 unique words. More description can be
found in (Steyvers et al., 2004);
• Clinical free text labeling2. They are primary data about patients.
This dataset is a structure: label-instance-feature.
There are 45 labels
(like ICD-9-CM codes) and 645 (training) / 333 (testing) data with 1,449
features. More description can be found in (Pestian et al., 2007).
Note that three datasets for the three tasks all involve a cooperative hierar-
chical structure which cannot be modeled by existing Bayesian non- parametric
models, like HDP. Next, each task is introduced in more detail, including the
aim, comparative models, evaluation metrics, and the results analysis. the re-
sults analysis.
6.5.1
Author-topic model task
The ﬁrst task for the proposed models for cooperative hierarchical structures
is to model an author-document-word structure.
The aim of this task is to
1http://www.datalab.uci.edu/author-topic/NIPs.htm
2http //mulan.sourceforge.net/datasets-mlc.html
201

Chapter 6
6.5. EXPERIMENTAL EVALUATION
A10
A20
A30
A40
A50
A60
A70
A80
A90
A100
A110
A120
A130
A140
A150
A160
A170
A180
A190
A200
CHDP MGNBP
100
200
300
400
500
600
700
800
900
1000
1100
Models
AP
Figure 6.8: The comparisons between the ATM (ﬁxed dimensional model) with
CHDP and MGNBP (nonparametric models) on the author prediction. ‘AX’
denotes the ATM with X number of topics. Each box expresses the statistics of
results from the corresponding model on 5-fold cross-validation. The larger the
value of AP, the better the performance.
10
20
30
40
50
60
70
80
90
100
110
120
130
140
150
160
170
180
190
200
−7
−6.8
−6.6
−6.4
−6.2
−6
−5.8
−5.6
−5.4
x 10
6
Factor Number
Data Log Likelihood
ATM
CHDP
MGNBP
Figure 6.9: The comparisons between the ATM (ﬁxed dimensional model) with
CHDP and MGNBP (nonparametric models) on the data likelihood. Since CHD-
P and MGNBP do not need the preﬁxed topic number, two lines are plotted in
this ﬁgure.
discover the hidden topics from the NIPS papers and, simultaneously, the
authors’ interests in these topics. This task can be further applied to real-world
applications, such as 1) detecting the Most and Least Surprising Papers for an
Author, 2) an author-topic browser; 3) author disambiguation; and so on. The
202

6.5. EXPERIMENTAL EVALUATION
Chapter 6
selected comparative model for this task is author topic model (ATM) (Rosen-
Zvi et al., 2004; Steyvers et al., 2004; Rosen-Zvi et al., 2010), which is based
on the ﬁxed dimensional probability distributions. Therefore, the topic number
needs to be ﬁxed when using ATM for this task, but the proposed models in
this chapter do not suﬀer this problem. The setting for this task evaluation is as
follows: keep 20% documents as the test dataset and consider the remaining 80%
documents as the training dataset. After learning the proposed models on the
training dataset, predict the authors of test documents. Two evaluation metrics
were used for the qualitative comparisons.
Author-prediction accuracy:
AP =

d

a
< πd, πa >
(6.72)
where <, > denotes the cosine similarity between two vectors; πa represents the
interest of author a on the topics
πa,k =
Oa

o
πa,o1(za,o = k), k ∈[1, K]
(6.73)
and πd represents the interest of paper d on the topics
πd,k =
Nd

n
θk,vn,
k ∈[1, K]
(6.74)
where n denotes a word in paper d, vn is the word name of word n in paper d.
The larger the value of AP, the better the performance.
Data (log) likelihood:
L =
Dtraining

d
Nd

n
log
K

k
θk,vn
(6.75)
203

Chapter 6
6.5. EXPERIMENTAL EVALUATION
where Nd is the number of words in paper d and Dtraining is the number of training
papers in the dataset. Data (log) likelihood is an evaluation of the model ﬁtting
the training dataset. If the learned topics are better, data (log) likelihood should
be larger.
The results have been shown in Fig. 6.8. Since ATM needs the number of
topics to be ﬁxed in advance, the 20 candidates K ∈{i : i = j × 10, j = [1, 20]}
were evaluated. In Fig. 6.8, ‘AX’ denotes the ATM with X number of topics,
and CHDP is implemented by the stick-breaking representation in Section 6.3.1
and Algorithm 6.1. For NIPS papers dataset, 5-fold cross validation is con-
ducted: the entire dataset was separated into 5 parts and one of them was used
as the test data for each time. Each box in the Fig. 6.8 denotes the statistics
of results from the corresponding model. As shown in Fig. 6.8, the performance
of the ATM on the author prediction tended to worsen as the number of topics
increased. The proposed CHDP and MGNBP performed better than ATM with
diﬀerent number of topics in general. More speciﬁcally, the median of MGNBP is
slightly better than the one of CHDP, but CHDP is more reliable than MGNBP
(considering the minimum and the maximum values). Except the author pre-
diction, the data likelihood of diﬀerent models are also demonstrated in Fig.
6.9, which is an evaluation of the probability of the data under a given model,
P(data|model). It can be seen that, as opposed to the performance on author
prediction, the data likelihood increased with an increasing topic number. Since
the proposed CHDP and MGNBP do not need the preﬁxed topic number, there
are two lines representing their data likelihoods in the Figure. In this experi-
ment, the topic numbers between [10, 200] are evaluated. From the Fig. 6.9, the
data likelihood from ATM exceeded the one from MGNBP at around number
130, but the diﬀerence between their likelihoods beyond number 130 was not
very large. Within the scope of [10, 200], the data likelihood from CHDP was
204

6.5. EXPERIMENTAL EVALUATION
Chapter 6
always better than ATM, but it can be seen that the data likelihood from ATM
will exceed CHDP as the further increasing of the topic number. To summarize,
there is a, somewhat lax, contradiction between the ability to ﬁt the training
data and the ability to predict the test data. The CHDP and MGNBP balanced
two abilities well.
6.5.2
Clinical free text labeling task
The second task for the proposed models is clinical free text labeling. The aim of
this task is to automatically assign the labels to the data. Take the example of a
Clinical free text labeling dataset. Automatic and accurate label assignment
for the text can save an enormous amount of time and cost compared with to
manual labor.
The comparative models for this task are LEAD (Zhang and
Zhang, 2010) and LIFT (Zhang and Wu, 2015; Zhang, 2011)3, which are both
deterministic models based on support vector machine (SVM). Comparing with
them, the proposed models are both generative models which normally have
a better generalizing ability on unseen data. The evaluation metrics used for
the performance comparison on this task are: Oneerror, Coverage, Rankingloss,
Avgprecision, and their deﬁnitions can be found in (Madjarov et al., 2012). For
Avgprecision, the larger the value, the better the performance; For Oneerror,
Coverage and Rankingloss, the smaller the value, the better the performance.
These metrics are all ranking-based. They can rank all the labels in diﬀerent
multi-label classiﬁcation models for each data according the possibility of the
data with each label. The proposed models in this chapter can also rank the
labels for the test data according to their interests in the hidden topics by
R(l, xi) =< −→
vl , −→
vi >
(6.76)
3Implementations are both from: http://cse.seu.edu.cn/people/zhangml/Resources.htm
205

Chapter 6
6.5. EXPERIMENTAL EVALUATION
LIFT−L
LIFT−P
LIFT−R
CHDP
MGNBP
LEAD−L
LEAD−P
LEAD−R
0.1
0.2
0.3
0.4
0.5
0.6
0.7
Oneerror
(a) Oneerror (The smaller the value, the better the performance.)
LIFT−L
LIFT−P
LIFT−R
CHDP
MGNBP
LEAD−L
LEAD−P
LEAD−R
0
0.5
1
1.5
2
2.5
3
3.5
4
Coverage
(b) Coverage (The smaller the value, the better the performance.)
LIFT−L
LIFT−P
LIFT−R
CHDP
MGNBP
LEAD−L
LEAD−P
LEAD−R
0
0.025
0.05
0.075
0.1
0.125
0.15
Rankingloss
(c) Rankingloss (The smaller the value, the better the performance.)
LIFT−L
LIFT−P
LIFT−R
CHDP
MGNBP
LEAD−L
LEAD−P
LEAD−R
0.3
0.4
0.5
0.6
0.7
0.8
0.9
Avgprecision
(d) Avgprecision (The larger the value, the better the performance.)
Figure 6.10: The comparisons between LIFT, LEAD, CHDP and MGNBP on
the multi-label classiﬁcation. Each subﬁgure demonstrates the results on a cor-
responding metric.
206

6.5. EXPERIMENTAL EVALUATION
Chapter 6
where xi is i-th test data, l denotes a label, −→
vl is a K dimensional vector that
denotes the interest of label l on the K topics can be similarly evaluated by Eq.
(6.73), −→
vi is also a K dimensional vector that denotes the interest of data xi on
the K topics can be similarly evaluated by Eq. (6.74). The larger the value of
R(l, xi), the larger possibility of xi with label l. Therefore, labels for each data
can be ranked according to R(l, xi).
The results are shown in Fig. 6.10. Since the LEAD and LIFT are SVM-
based models, diﬀerent implementations with diﬀerent kernel functions have been
compared. ‘LIFT-L’ denotes LIFT with Linear kernel function; ‘LIFT-P’ denotes
LIFT with Polynomial kernel function (the degree is set as 3); ‘LIFT-R’ denotes
LIFT with Radial basis function kernel function. In the Fig. 6.10, CHDP and
MGNBP achieved best performances on Oneerror and Rankingloss, and they also
achieved a comparative performance on Avgprecision. For the Coverage, LEAD
achieved the best performances, while CHDP and MGNBP only surpassed the
worst LIFT-R. Four metrics each have their own preferences on the classiﬁca-
tion evaluation. Oneerror and Coverage prefer ‘variance’, and Rankingloss and
Avgprecision prefer ‘mean’, so the proposed models, i.e., CHDP and MGNBP,
performed better when evaluated against the ‘mean’, but the ‘variance’ was not
larger than LIFT and LEAD. This may be because that the proposed models are
based on MCMC, so each run is a sample from the real model posterior distri-
bution, which has a larger variance during the sampling, although the variance
has already been decreased by incorporating the data. To summarize, CHDP
and MGNBP had better performance than LIFT and LEAD in the clinical free
text labeling task, especially considering the generative model nature of CHDP
and MGNBP.
207

Chapter 6
6.6. SUMMARY
6.6
Summary
Hierarchical structures are a signiﬁcant data structure adopted by diﬀerent areas
for the data modeling, so it has attracted many attention of many research labo-
ratories, such as multi-label document learning. This chapter has presented two
Bayesian nonparametric models, i.e., CHDP and MGNBP, for more general hi-
erarchical structures: cooperative hierarchical structures. Two models are based
on two random measure operations which have been speciﬁcally designed for
modeling the cooperative hierarchical structures: Inheritance and Cooperation.
In order to resolve the issue brought about by Inheritance and Cooperation in
CHDP and MGNBP, Markov Chain Monte Carlo inference algorithms have been
designed through introducing properly designed additional auxiliary variables.
Experiments on two multi-label document learning tasks with diﬀerent public
datasets have shown the ability of the proposed models on modeling cooperative
hierarchical structure.
208

Chapter 7
Conclusion and Further Study
This chapter concludes the entire thesis and provides some further research di-
rections for this topic.
7.1
Conclusions
Bayesian nonparametric learning has attracted much attention in the area of
statistical machine learning due to its powerful modeling ability and demand
for less prerequisites over the past decades - especially for text mining. Even
though Bayesian nonparametric learning has gained considerable attention and
undergone rapid development, its ability to model or capture complex text da-
ta, such as document-word co-clustering, document networks, multi-label doc-
uments, and so on, is still lacking. Yet, text is one of the most natural and
easy ways to express human knowledge and opinion and holds an irreplaceable
position in modern life.
Thus, modeling and analysis of complex text is of
great signiﬁcance to corporations, organisations and individuals, and research
on Bayesian nonparametric learning for text mining has great theoretical and
practical relevance. The ﬁndings of this study are summarised as follows:
209

Chapter 7
7.1. CONCLUSIONS
1. The development of three Bayesian nonparametric sparse nonnegative matrix
factorisation models for document-word co-clustering. (To achieve OBJEC-
TIVE 1)
A Bayesian nonparametric NMF framework based on dependent Indian buﬀet
processes (dIBP) has been proposed to remove the assumption of NMF for co-
clustering tasks where the dimension of the factors is known in advance. First,
a model is built by implementing a framework using GP-based dIBP, which
successfully removes the assumption, but suﬀers from larger model complex-
ity and less ﬂexibility. Then, two new dIBPs have been proposed through
a bivariate Beta distribution and a copula. The advantages of the models
based on new dIBPs are: 1) They have simpler model structures than mod-
els with GP-based dIBP; 2) The correlation between data could be directly
learned and can be seen as a measurement of the focus degree of the hid-
den factors or topics. Lastly, three inference algorithms have been designed
for the proposed three models, respectively. Experiments on synthetic and
real-world datasets demonstrates the ability of the proposed models to perfor-
m NMF without predeﬁning the dimension number. It also more correlation
ﬂexibility when compared to GP-based dIBP. The bivariate Beta distribution-
based model and copula-based model both achieve comparative performance
on document-word co-clustering.
2. The development of a Bayesian nonparametric deep topic model for the topic
hierarchy learning.
(To achieve OBJECTIVE 2)
A Bayesian nonparametric deep topic model has been proposed to auto-
matically learn a topic hierarchy from any document collection. Based on
renowned hierarchical Dirichlet processes (HDP), a new probability measure
has been proposed, which not only inherits the mechanisms of linking the
210

7.1. CONCLUSIONS
Chapter 7
topics at diﬀerent layers but also breaks the topic sharing strategy of the o-
riginal HDP. With the help of this new probability measure, a topic hierarchy
can be learned. Additionally, a slice sampling method without truncation
approximation has been designed. Experimental results show that the topics
in higher layers are more general than the topics in lower layers, which have a
better ability to predict new documents. Conversely, the topics in the lower
layers are more speciﬁc than the topics in higher layers, which have a better
ability to explain the training documents.
3. The development of a Bayesian nonparametric relational topic model for doc-
ument network learning. (To achieve OBJECTIVE 3)
A Bayesian nonparametric relational topic model has been presented in which
stochastic processes are adopted to replace the ﬁxed-dimensional probabili-
ty distributions used by existing relational topic models and necessitates a
pre-deﬁned the number of topics. Introducing stochastic processes, however,
leads to diﬃculties with model construction and inference, and this study has
therefore presented a thinned gamma process-based model and has also pre-
sented truncated Gibbs and slice sampling algorithms for the proposed model.
Experiments on both synthetic and real-world datasets has demonstrated the
proposed methodąŕs ability to infer the hidden topics and their number.
4. The development of Bayesian nonparametric cooperative hierarchical structure
models for multi-label document learning. (To achieve OBJECTIVE 4)
Considering the problems of existing researches on hierarchical structures, i.e.,
that the number of hidden factors needs to be predeﬁned and only designed
for non-cooperative hierarchical structures, two measure operations have been
deﬁned to model the cooperative hierarchical structures: inheritance and co-
operation. Two diﬀerent models have been proposed, each of which exemplify
211

Chapter 7
7.2. FURTHER STUDY
the two measure operations. One is the cooperative hierarchical Dirichlet pro-
cess (CHDP) with a Dirichlet process as its basic ingredient. The other is a
mixed gamma-negative binomial process (MGNBP) with a gamma process as
its basic ingredient. In order to resolve the issues introduced by inheritance
and cooperation, three diﬀerent Markov chain Monte Carlo inference algo-
rithms have been designed through introducing properly designed auxiliary
variables. Experiments on various tasks with diﬀerent datasets have shown
the eﬃciency of the proposed models, and has demonstrated wide practical
value in a range of ﬁelds.
7.2
Further study
One possible future study for this work pertains to eﬃciency, because current
Gibbs sampling-based inference lacks eﬃciency when dealing with big data, al-
though it has the merit in that it is theoretically capable of reaching the exact
posterior distribution. This thesis identiﬁes the following directions as future
work:
• Variational Inference A mean-ﬁeld variational method, in which the varia-
tional distributions (normally fully-factorised) are used to approximate the
true posterior distribution by adjusting their distribution parameters (Hoﬀ-
man et al., 2013). This approach would transfer the sampling problem into an
optimisation problem, so the inference speed could be accelerated. Developing
a set of variational inference algorithms for the proposed models in this thesis
would therefore, be interesting for future work.
• Parallel Implementation Another strategy to accelerate inference speed uses
the help of a parallel machine (Gonzalez et al., 2011). Current sampling algo-
rithms would be carefully redesigned and assigned to multi-threads (multi-core
212

7.2. FURTHER STUDY
Chapter 7
parallel machines) or multi-machines (distributed parallel machines). Theo-
retical support could also be distributed along with the task assignment of the
original problem.
213


REFERENCES
Aggarwal, C. C. and Zhai, C. (2012), Mining Text Data, Springer US, Boston,
MA, chapter A survey of text clustering algorithms, pp. 77–128.
Airoldi, E. M., Blei, D. M., Fienberg, S. E. and Xing, E. P. (2008), ‘Mixed
membership stochastic blockmodels’, Journal of Machine Learning Research
9, 1981–2014.
Aldous, D. J. (1985), Exchangeability and Related Topics, Springer.
Alghamdi, R. and Alfalqi, K. (2015), ‘A survey of topic modeling in text min-
ing’, International Journal of Advanced Computer Science and Applications
6(1), 147–153.
Andrieu, C., de Freitas, N., Doucet, A. and Jordan, M. I. (2003), ‘An introduc-
tion to mcmc for machine learning’, Machine Learning 50(1), 5–43.
Archambeau, C., Lakshminarayanan, B. and Bouchard, G. (2015), ‘Latent ibp
compound dirichlet allocation’, IEEE Transactions on Pattern Analysis and
Machine Intelligence 37(2), 321–333.
Balog, M. and Teh, Y. W. (2015), ‘The mondrian process for machine learning’,
arXiv preprint arXiv:1507.05181 .
215

REFERENCES
Bao, S., Xu, S., Zhang, L., Yan, R., Su, Z., Han, D. and Yu, Y. (2012), ‘Mining
social emotions from aﬀective text’, IEEE Transactions on Knowledge and
Data Engineering 24(9), 1658–1670.
Beare,
B. K. (2010),
‘Copulas and temporal dependence’,
Econometrica
78(1), 395–410.
Bichot, C.-E. (2010), ‘Co-clustering documents and words by minimizing the
normalized cut objective function’, Journal of Mathematical Modelling and
Algorithms 9(2), 131–147.
Blei, D. M. (2004), Probabilistic Models of Text and Images, PhD thesis, Berke-
ley, CA, USA. AAI3183785.
Blei, D. M. (2012), ‘Probabilistic topic models’, Communications of the ACM
55(4), 77–84.
Blei, D. M., Cook, P. R. and Hoﬀman, M. (2010), Bayesian nonparametric ma-
trix factorization for recorded music, in ‘Proceedings of the 27th Internation-
al Conference on Machine Learning’, ICML ’10, JMLR.org, Haifa, Israel, p-
p. 439–446.
Blei, D. M. and Frazier, P. I. (2011), ‘Distance dependent chinese restaurant
processes’, The Journal of Machine Learning Research 12, 2461–2488.
Blei, D. M., Griﬃths, T. L. and Jordan, M. I. (2010), ‘The nested chinese restau-
rant process and bayesian nonparametric inference of topic hierarchies’, Jour-
nal of the ACM 57(2), 7.
Blei, D. M., Jordan, M. I. et al. (2006), ‘Variational inference for dirichlet process
mixtures’, Bayesian Analysis 1(1), 121–143.
216

REFERENCES
Blei, D. M., Ng, A. Y. and Jordan, M. I. (2003), ‘Latent dirichlet allocation’,
Journal of Machine Learning Research 3, 993–1022.
Blunsom, P. and Cohn, T. (2011), A hierarchical pitman-yor process hmm for
unsupervised part of speech induction, in ‘Proceedings of the 49th Annual
Meeting of the Association for Computational Linguistics: Human Language
Technologies-Volume 1’, ACL ’11, Association for Computational Linguistics,
Association for Computational Linguistics, Portland, Oregon, USA, pp. 865–
874.
Bouguila, N., Ziou, D. and Vaillancourt, J. (2004), ‘Unsupervised learning of a
ﬁnite mixture model based on the dirichlet distribution and its application’,
IEEE Transactions on Image Processing 13(11), 1533–1543.
Broderick, T., Mackey, L., Paisley, J. and Jordan, M. (2015), ‘Combinatorial
clustering and the beta negative binomial process’, IEEE Transactions on
Pattern Analysis and Machine Intelligence 37(2), 290–306.
Bryant, M. and Sudderth, E. B. (2012), Truly nonparametric online variational
inference for hierarchical dirichlet processes, in ‘26th Annual Conference on
Neural Information Processing Systems’, NIPS ’12, JMLR.org, Lake Tahoe,
Nevada, USA, pp. 2699–2707.
Campbell, T., Straub, J., III, J. W. F. and How, J. P. (2015), Streaming, dis-
tributed variational inference for bayesian nonparametrics, in ‘Annual Confer-
ence on Neural Information Processing Systems’, NIPS ’15, JMLR.org, Mon-
treal, Quebec, Canada, pp. 280–288.
Canini, K. R. and Griﬃths, T. L. (2011), A nonparametric bayesian model of
multi-level category learning, in ‘Proceedings of the Twenty-Fifth AAAI Con-
217

REFERENCES
ference on Artiﬁcial Intelligence’, AAAI ’11, AAAI Press, San Francisco, Cal-
ifornia, pp. 307–312.
Caron, F., Davy, M. and Doucet, A. (2007), Generalized polya urn for time-
varying dirichlet process mixtures, in ‘Proceedings of the Twenty-Third Con-
ference on Uncertainty in Artiﬁcial Intelligence’, UAI ’07, AUAI Press, pp. 33–
40.
Chang, J. and Blei, D. M. (2009), Relational topic models for document net-
works, in ‘Proceedings of the Twelfth International Conference on Artiﬁcial
Intelligence and Statistics’, AISTATS ’09, JMLR.org, Clearwater Beach, Flori-
da, USA, pp. 81–88.
Chang, J., Blei, D. M. et al. (2010), ‘Hierarchical relational models for document
networks’, The Annals of Applied Statistics 4(1), 124–150.
Chang, J. and Fisher III, J. W. (2013), Parallel sampling of dp mixture models
using sub-cluster splits, in ‘27th Annual Conference on Neural Information
Processing Systems’, NIPS ’13, Lake Tahoe, Nevada, USA, pp. 620–628.
Chen, B., Polatkan, G., Sapiro, G., Carin, L. and Dunson, D. B. (2011), The
hierarchical beta process for convolutional factor analysis and deep learning,
in ‘Proceedings of the 28th International Conference on Machine Learning’,
ICML ’11, Omnipress, Bellevue, Washington, USA, pp. 361–368.
Chen, C., Buntine, W., Ding, N., Xie, L. and Du, L. (2015), ‘Diﬀerential topic
models’, IEEE Transactions on Pattern Analysis and Machine Intelligence
37(2), 230–242.
Chen, C., Ding, N. and Buntine, W. L. (2012), Dependent hierarchical nor-
malized random measures for dynamic topic modeling, in ‘Proceedings of the
218

REFERENCES
29th International Conference on Machine Learning’, ICML ’12, JMLR.org,
Edinburgh, Scotland, UK.
Chen, C., Rao, V., Buntine, W. and Whye Teh, Y. (2013), Dependent normalized
random measures, in ‘Proceedings of the 30th International Conference on
Machine Learning’, ICML ’13, JMLR.org, Atlanta, GA, USA, pp. 969–977.
Chen, N., Zhu, J., Xia, F. and Zhang, B. (2015), ‘Discriminative relational topic
models’, IEEE Transactions on Pattern Analysis and Machine Intelligence
37(5), 973–986.
Chuang, S.-L. and Chien, L.-F. (2004), A practical web-based approach to gen-
erating topic hierarchy for text segments, in ‘Proceedings of the Thirteenth
ACM International Conference on Information and Knowledge Management’,
CIKM ’04, ACM, New York, NY, USA, pp. 127–136.
Chung, Y. and Dunson, D. B. (2009), ‘The local dirichlet process’, Annals of the
Institute of Statistical Mathematics 63(1), 59–80.
Cong, H. and Tong, L. H. (2008), ‘Grouping of triz inventive principles to
facilitate automatic patent classiﬁcation’, Expert Systems with Applications
34(1), 788 – 795.
Crouch, C. J. (1988), A cluster-based approach to thesaurus construction, in
‘Proceedings of the 11th Annual International ACM SIGIR Conference on
Research and Development in Information Retrieval’, SIGIR ’88, ACM, New
York, NY, USA, pp. 309–320.
da Silva, A. R. F. (2007), ‘A dirichlet process mixture model for brain mri tissue
classiﬁcation’, Medical Image Analysis 11(2), 169–182.
219

REFERENCES
Dai, A. and Storkey, A. (2015), ‘The supervised hierarchical dirichlet process’,
IEEE Transactions on Pattern Analysis and Machine Intelligence 37(2), 243–
255.
Damlen, P., Wakeﬁeld, J. and Walker, S. (1999), ‘Gibbs sampling for bayesian
non-conjugate and hierarchical models by using auxiliary variables’, Journal
of the Royal Statistical Society: Series B (Statistical Methodology) 61(2), 331–
344.
DeNero, J., Bouchard-Côté, A. and Klein, D. (2008), Sampling alignment struc-
ture under a bayesian translation model, in ‘Proceedings of the Conference on
Empirical Methods in Natural Language Processing’, EMNLP ’08, Association
for Computational Linguistics, Stroudsburg, PA, USA, pp. 314–323.
Dhillon, I. S. (2001), Co-clustering documents and words using bipartite spectral
graph partitioning, in ‘Proceedings of the Seventh ACM SIGKDD Internation-
al Conference on Knowledge Discovery and Data Mining’, KDD ’01, ACM,
New York, NY, USA, pp. 269–274.
Ding, C. H. Q., Li, T. and Jordan, M. I. (2010), ‘Convex and semi-nonnegative
matrix factorizations’, IEEE Transactions on Pattern Analysis and Machine
Intelligence 32(1), 45–55.
Ding, N., Xiang, R., Molloy, I., Li, N. et al. (2010), Nonparametric bayesian ma-
trix factorization by power-ep, in ‘Proceedings of the Thirteenth International
Conference on Artiﬁcial Intelligence and Statistics’, AISTATS ’10, JMLR.org,
pp. 169–176.
Doshi, F., Miller, K., Gael, J. V. and Teh, Y. W. (2009), Variational inference
for the indian buﬀet process, in ‘Proceedings of the Twelfth International
220

REFERENCES
Conference on Artiﬁcial Intelligence and Statistics’, AISTATS ’09, JMLR.org,
Clearwater Beach, Florida, USA, pp. 137–144.
Doshi-Velez, F., Pfau, D., Wood, F. and Roy, N. (2015), ‘Bayesian nonparametric
methods for partially-observable reinforcement learning’, IEEE Transactions
on Pattern Analysis and Machine Intelligence 37(2), 394–407.
Dubey, K., Williamson, S. and Xing, E. P. (2014), Parallel markov chain monte
carlo for pitman-yor mixture models, in ‘Proceedings of the Thirtieth Confer-
ence on Uncertainty in Artiﬁcial Intelligence’, UAI ’14, AUAI Press, Quebec
City, Quebec, Canada, pp. 142–151.
Dunson, D. B. (2010), Bayesian Nonparametrics, Cambridge University Press,
chapter Nonparametric Bayes applications to biostatistics, pp. 223–273.
Dunson, D. B. and Park, J.-H. (2008), ‘Kernel stick-breaking processes’,
Biometrika 95(2), 307–323.
Fan, X., Cao, L., Xu, D. and Yi, R. (2015), ‘Dynamic inﬁnite mixed-membership
stochastic blockmodel’, IEEE Transactions on Neural Networks and Learning
Systems 26(9), 2072–2085.
Favaro, S., Lijoi, A. and Prünster, I. (2012), ‘A new estimator of the discovery
probability’, Biometrics 68(4), 1188–1196.
Fearnhead, P. (2004), ‘Particle ﬁlters for mixture models with an unknown num-
ber of components’, Statistics and Computing 14(1), 11–21.
Ferguson, T. S. (1973), ‘A bayesian analysis of some nonparametric problems’,
The Annals of Statistics 1(2), 209–230.
Fersini, E., Messina, E. and Archetti, F. (2008), Granular modeling of web doc-
uments: Impact on information retrieval systems, in ‘Proceedings of the 10th
221

REFERENCES
ACM Workshop on Web Information and Data Management’, WIDM ’08,
ACM, New York, NY, USA, pp. 111–118.
Foti, N. J., Futoma, J. D., Rockmore, D. N. and Williamson, S. (2013), A unify-
ing representation for a class of dependent random measures, in ‘Proceedings
of the Sixteenth International Conference on Artiﬁcial Intelligence and Statis-
tics’, AISTATS ’13, JMLR.org, Scottsdale, AZ, USA, pp. 20–28.
Foti, N. and Williamson, S. (2015), ‘A survey of non-exchangeable priors for
bayesian nonparametric models’, IEEE Transactions on Pattern Analysis and
Machine Intelligence 37(2), 359–371.
Fox, E. B. (2009), Bayesian nonparametric learning of complex dynamical phe-
nomena, PhD thesis, Massachusetts Institute of Technology.
Fox, E. B., Hughes, M. C., Sudderth, E. B., Jordan, M. I. et al. (2014), ‘Joint
modeling of multiple time series via the beta process with application to mo-
tion capture segmentation’, The Annals of Applied Statistics 8(3), 1281–1313.
Fox, E., Sudderth, E., Jordan, M. and Willsky, A. (2011a), ‘Bayesian nonpara-
metric inference of wwitching dynamic linear models’, IEEE Transactions on
Signal Processing 59(4), 1569–1585.
Fox, E., Sudderth, E., Jordan, M. and Willsky, A. (2011b), ‘A sticky hdp-
hmm with application to speaker diarization’, Annals of Applied Statistics
5(2A), 1020–1056.
Gael, J. V., Teh, Y. W. and Ghahramani, Z. (2008), The inﬁnite factorial hidden
markov model, in ‘Proceedings of the Twenty-Second Annual Conference on
Neural Information Processing Systems’, NIPS ’08, Curran Associates, Inc.,
Vancouver, British Columbia, Canada, pp. 1697–1704.
222

REFERENCES
Gael, J. V., Vlachos, A. and Ghahramani, Z. (2009), The inﬁnite hmm for un-
supervised pos tagging, in ‘Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing’, EMNLP ’09, Association for Com-
putational Linguistics, Singapore, pp. 678–687.
Gao, Z., Song, Y., Liu, S., Wang, H., Wei, H., Chen, Y. and Cui, W. (2011),
Tracking and connecting topics via incremental hierarchical dirichlet processes,
in ‘The 11th IEEE International Conference on Data Mining’, ICDM ’11,
IEEE, Vancouver, Canada, pp. 1056–1061.
Ge, H., Chen, Y., Wan, M. and Ghahramani, Z. (2015), Distributed inference
for dirichlet process mixture models, in ‘Proceedings of the 32nd Interna-
tional Conference on Machine Learning’, ICML ’15, JMLR.org, Lille, France,
pp. 2276–2284.
Gelfand, A. E., Kottas, A. and MacEachern, S. N. (2005), ‘Bayesian nonparamet-
ric spatial modeling with dirichlet process mixing’, Journal of the American
Statistical Association 100(471), 1021–1035.
Gershman, S. J. and Blei, D. M. (2012), ‘A tutorial on bayesian nonparametric
models’, Journal of Mathematical Psychology 56(1), 1–12.
Gershman, S. J., Frazier, P. I. and Blei, D. M. (2015), ‘Distance dependent
inﬁnite latent feature models’, IEEE Transactions on Pattern Analysis and
Machine Intelligence 37(2), 334–345.
Ghahramani, Z., Jordan, M. I. and Adams, R. P. (2010), Tree-structured stick
breaking for hierarchical data, in ‘24th Annual Conference on Neural Infor-
mation Processing Systems’, NIPS ’10, Curran Associates, Inc., Vancouver,
British Columbia, Canada, pp. 19–27.
223

REFERENCES
Ghosal, S. (2010), Bayesian Nonparametrics, Vol. 2, Cambridge University Press,
chapter The Dirichlet process, related priors and posterior asymptotics, p-
p. 35–79.
Gibaja, E. and Ventura, S. (2015), ‘A tutorial on multilabel learning’, ACM
Computing Surveys 47(3), 52:1–52:38.
Goldwater, S., Griﬃths, T. L. and Johnson, M. (2009), ‘A bayesian framework
for word segmentation: Exploring the eﬀects of context’, Cognition 112(1), 21
– 54.
Gonzalez, J., Low, Y., Gretton, A. and Guestrin, C. (2011), Parallel gibbs
sampling: From colored ﬁelds to thin junction trees, in ‘Proceedings of the
Fourteenth International Conference on Artiﬁcial Intelligence and Statistics’,
AISTATS ’11, JMLR.org, Fort Lauderdale, USA, pp. 324–332.
Goyal, P., Behera, L. and McGinnity, T. M. (2013), ‘A context-based word in-
dexing model for document summarization’, IEEE Transactions on Knowledge
and Data Engineering 25(8), 1693–1705.
Griﬃn, J. E., Kolossiatis, M. and Steel, M. F. J. (2013), ‘Comparing distribution-
s by using dependent normalized random-measure mixtures’, Journal of the
Royal Statistical Society: Series B (Statistical Methodology) 75(3), 499–529.
Griﬃn, J. E. and Steel, M. J. (2006), ‘Order-based dependent dirichlet processes’,
Journal of the American Statistical Association 101(473), 179–194.
Griﬃths, T. L. and Ghahramani, Z. (2005), Inﬁnite latent feature models and
the indian buﬀet process, in ‘Neural Information Processing Systems’, NIPS
’05, MIT Press, Vancouver, British Columbia, Canada, pp. 475–482.
224

REFERENCES
Griﬃths, T. L. and Ghahramani, Z. (2011), ‘The indian buﬀet process: An in-
troduction and review’, The Journal of Machine Learning Research 12, 1185–
1224.
Griﬃths, T. L. and Steyvers, M. (2004), ‘Finding scientiﬁc topics’, Proceedings
of the National Academy of Sciences 101(suppl 1), 5228–5235.
Guo, Z., Zhang, Z., Zhu, S., Chi, Y. and Gong, Y. (2014), ‘A two-level topic mod-
el towards knowledge discovery from citation networks’, IEEE Transactions
on Knowledge and Data Engineering 26(4), 780–794.
Gupta, S. K., Phung, D. Q. and Venkatesh, S. (2012), A slice sampler for restrict-
ed hierarchical beta process with applications to shared subspace learning, in
‘Proceedings of the Twenty-Eighth Conference on Uncertainty in Artiﬁcial In-
telligence’, UAI ’12, AUAI Press, Catalina Island, CA, USA, pp. 316–325.
Gupta, V. and Lehal, G. S. (2009), ‘A survey of text mining techniques and
applications’, Journal of Emerging Technologies in Web Intelligence 1(1), 60–
76.
Haines, T. S. F. and Xiang, T. (2014), ‘Background subtraction with dirichlet
process mixture models’, IEEE Transactions on Pattern Analysis and Machine
Intelligence 36(4), 670–683.
Han, J., Kamber, M. and Pei, J. (2011), Data Mining: Concepts and Techniques,
Elsevier.
Hannah, L. A., Blei, D. M. and Powell, W. B. (2011), ‘Dirichlet process mixtures
of generalized linear models’, Journal of Machine Learning Research 12, 1923–
1953.
225

REFERENCES
He, L., Qi, H. and Zaretzki, R. (2013), Beta process joint dictionary learning
for coupled feature spaces with application to single image super-resolution,
in ‘Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition’, CVPR ’13, IEEE, Portland, OR, USA, pp. 345–352.
He, Z., Xie, S., Zdunek, R., Zhou, G. and Cichocki, A. (2011), ‘Symmetric
nonnegative matrix factorization: Algorithms and applications to probabilistic
clustering’, IEEE Transactions on Neural Networks 22(12), 2117–2131.
Heaukulani, C., Knowles, D. A. and Ghahramani, Z. (2014), Beta diﬀusion trees,
in ‘Proceedings of the 31th International Conference on Machine Learning’,
ICML ’14, JMLR.org, Beijing, China, pp. 1809–1817.
Heiler, M. and Schnörr, C. (2006), ‘Learning sparse representations by non-
negative matrix factorization and sequential cone programming’, The Journal
of Machine Learning Research 7, 1385–1407.
Hjort, N. L. (1990), ‘Nonparametric bayes estimators based on beta processes in
models for life history data’, The Annals of Statistics 18(3), 1259–1294.
Hoﬀman, M. D., Blei, D. M. and Cook, P. R. (2008), Content-based musical simi-
larity computation using the hierarchical dirichlet process, in ‘9th Internation-
al Conference on Music Information Retrieval’, ISMIR ’08, Drexel University,
Philadelphia, PA, USA, pp. 349–354.
Hoﬀman, M. D., Blei, D. M., Wang, C. and Paisley, J. (2013), ‘Stochastic varia-
tional inference’, The Journal of Machine Learning Research 14(1), 1303–1347.
Hofmann, T. (1999), Probabilistic latent semantic indexing, in ‘Proceedings of
the 22nd annual international ACM SIGIR conference on Research and devel-
opment in information retrieval’, SIGIR ’99, ACM, ACM, Berkeley, California,
USA, pp. 50–57.
226

REFERENCES
Huelsenbeck, J. P., Jain, S., Frost, S. W. and Pond, S. L. K. (2006), ‘A dirichlet
process model for detecting positive selection in protein-coding dna sequences’,
Proceedings of the National Academy of Sciences 103(16), 6263–6268.
Iwata, T., Shah, A. and Ghahramani, Z. (2013), Discovering latent inﬂuence in
online social activities via shared cascade poisson processes, in ‘Proceedings
of the 19th ACM SIGKDD International Conference on Knowledge Discovery
and Data Mining’, KDD ’13, ACM, Chicago, IL, USA, pp. 266–274.
Jbabdi, S., Woolrich, M. and Behrens, T. (2009), ‘Multiple-subjects connectivity-
based parcellation using hierarchical dirichlet process mixture models’, Neu-
roImage 44(2), 373 – 384.
Johnson, M. J. and Willsky, A. S. (2013), ‘Bayesian nonparametric hidden semi-
markov models’, Journal of Machine Learning Research 14(1), 673–701.
joon Kim, H. and goo Lee, S. (2003), ‘Building topic hierarchy based on fuzzy
relations’, Neurocomputing 51(0), 481 – 486.
Kalli, M., Griﬃn, J. E. and Walker, S. G. (2011), ‘Slice sampling mixture mod-
els’, Statistics and Computing 21(1), 93–105.
Kim, H., Sun, Y., Hockenmaier, J. and Han, J. (2012), Etm: Entity topic mod-
els for mining documents associated with entities, in ‘Proceedings of the 2012
IEEE 12th International Conference on Data Mining’, ICDM ’12, IEEE, Wash-
ington, DC, USA, pp. 349–358.
Kim, J. and Park, H. (2008), Sparse nonnegative matrix factorization for clus-
tering, Technical report.
Kingman, J. (1982), ‘The coalescent’, Stochastic Processes and their Applications
13(3), 235 – 248.
227

REFERENCES
Kingman, J. F. C. (1967), ‘Completely random measures.’, Paciﬁc Journal of
Mathematics 21(1), 59–78.
Kingman, J. F. C. (1992), Poisson Processes, Vol. 3, Oxford university press.
Klimt, B. and Yang, Y. (2004), Machine Learning: ECML 2004: 15th European
Conference on Machine Learning, Springer Berlin Heidelberg, Berlin, Heidel-
berg, chapter The Enron Corpus: A New Dataset for Email Classiﬁcation
Research, pp. 217–226.
Knowles, D. A. and Ghahramani, Z. (2015), ‘Pitman yor diﬀusion trees for
bayesian hierarchical clustering’, IEEE Transactions on Pattern Analysis and
Machine Intelligence 37(2), 271–289.
Kurihara, K., Welling, M. and Teh, Y. W. (2007), Collapsed variational dirichlet
process mixture models, in ‘Proceedings of the 20th International Joint Con-
ference on Artiﬁcial Intelligence’, Vol. 7 of IJCAI ’07, AAAI Press, Hyderabad,
India, pp. 2796–2801.
Kurihara, K., Welling, M. and Vlassis, N. A. (2006), Accelerated variational
dirichlet process mixtures, in ‘Proceedings of the Twentieth Annual Conference
on Neural Information Processing Systems’, NIPS ’06, MIT Press, Vancouver,
British Columbia, Canada, pp. 761–768.
Li, S. Z. (1995), Markov Random Field Modeling in Computer Vision, Springer-
Verlag New York, Inc.
Li, T., Zhu, S. and Ogihara, M. (2003), Topic hierarchy generation via linear dis-
criminant projection, in ‘Proceedings of the 26th Annual International ACM
SIGIR Conference on Research and Development in Informaion Retrieval’,
SIGIR ’03, ACM, Toronto, Canada, pp. 421–422.
228

REFERENCES
Li, W. and McCallum, A. (2006), Pachinko allocation: Dag-structured mixture
models of topic correlations, in ‘Proceedings of the 23rd International Confer-
ence on Machine Learning’, ICML ’06, ACM, Pittsburgh, Pennsylvania, USA,
pp. 577–584.
Liang, D., Hoﬀman, M. D. and Ellis, D. P. (2013), Beta process sparse nonnega-
tive matrix factorization for music., in ‘Proceedings of the 14th International
Society for Music Information Retrieval Conference’, ISMIR ’13, Curitiba,
Brazil, pp. 375–380.
Lijoi, A., Mena, R. H. and Prünster, I. (2007), ‘A bayesian nonparametric
method for prediction in est analysis’, BMC Bioinformatics 8(1), 1–10.
Lin, C. and He, Y. (2009), Joint sentiment/topic model for sentiment analysis,
in ‘Proceedings of the 18th ACM Conference on Information and Knowledge
Management’, CIKM ’09, ACM, ACM, Hong Kong, China, pp. 375–384.
Lin, D., Grimson, E. and III, J. W. F. (2010), Construction of dependent dirich-
let processes based on poisson processes, in ‘24th Annual Conference on Neural
Information Processing Systems’, NIPS ’10, Curran Associates, Inc., Vancou-
ver, British Columbia, Canada, pp. 1396–1404.
Lodhi, H., Saunders, C., Shawe-Taylor, J., Cristianini, N. and Watkins, C.
(2002), ‘Text classiﬁcation using string kernels’, The Journal of Machine
Learning Research 2, 419–444.
Lovell, D., Adams, R. P. and Mansingka, V. (2012), Parallel markov chain monte
carlo for dirichlet process mixtures, in ‘Workshop on Big Learning, 26th An-
nual Conference on Neural Information Processing Systems’, NIPS ’12, Lake
Tahoe, Nevada, USA.
229

REFERENCES
Ma, Z., Rana, P. K., Taghia, J., Flierl, M. and Leijon, A. (2014), ‘Bayesian esti-
mation of dirichlet mixture model with variational inference’, Pattern Recog-
nition 47(9), 3143 – 3157.
MacEachern, S. N. (1999), Dependent nonparametric processes, in ‘ASA pro-
ceedings of the section on Bayesian statistical science’, Alexandria, Virginia.
Virginia: American Statistical Association; 1999, American Statistical Asso-
ciation, pp. 50–55.
MacEachern, S. N., Clyde, M. and Liu, J. S. (1999), ‘Sequential importance
sampling for nonparametric bayes models: The next generation’, Canadian
Journal of Statistics 27(2), 251–267.
Maddison, C. J., Tarlow, D. and Minka, T. (2014), A* sampling, in ‘Annual
Conference on Neural Information Processing Systems’, NIPS ’14, JMLR.org,
Montreal, Quebec, Canada, pp. 3086–3094.
Madjarov, G., Kocev, D., Gjorgjevikj, D. and DÅ¿eroski, S. (2012), ‘An ex-
tensive experimental comparison of methods for multi-label learning’, Pattern
Recognition 45(9), 3084 – 3104. Best Papers of Iberian Conference on Pattern
Recognition and Image Analysis (IbPRIA’2011).
Mahmud, M. (2010), Constructing states for reinforcement learning, in ‘Pro-
ceedings of the 27th International Conference on Machine Learning’, ICML
’10, Omnipress, Haifa, Israel, pp. 727–734.
McCullagh, P. (1984), ‘Generalized linear models’, European Journal of Opera-
tional Research 16(3), 285–292.
Michael, T. and Jordan, I. (1994), Reinforcement learning algorithm for partially
observable markov decision problems, in ‘NIPS Conference’, NIPS ’94, MIT
Press, Denver, CO, USA, pp. 345–352.
230

REFERENCES
Mimno, D., Li, W. and McCallum, A. (2007), Mixtures of hierarchical topics with
pachinko allocation, in ‘Proceedings of the 24th International Conference on
Machine Learning’, ICML ’07, ACM, Corvalis, Oregon, USA, pp. 633–640.
Miner, G. (2012), Practical Text Mining and Statistical Analysis for Non-
structured Text Data Applications, Academic Press.
Minka, T. (2004), Power ep, Technical report, Technical report, Microsoft Re-
search, Cambridge.
Nakano, M., Ohishi, Y., Kameoka, H., Mukai, R. and Kashino, K. (2012),
Bayesian nonparametric music parser, in ‘2012 IEEE International Confer-
ence on Acoustics, Speech and Signal Processing’, ICASSP ’12, IEEE, Kyoto,
Japan, pp. 461–464.
Neal, R. M. (2003a), ‘Density modeling and clustering using dirichlet diﬀusion
trees’, Bayesian Statistics 7, 619–629.
Neal, R. M. (2003b), ‘Slice sampling’, Annals of Statistics 31(3), 705–767.
Neiswanger, W., Wang, C. and Xing, E. (2014), Embarrassingly parallel vari-
ational inference in nonconjugate models, in ‘Workshop on Advanced Varia-
tional Inference’, NIPS ’14, JMLR.org, Montreal, Quebec, Canada, pp. 1–18.
Olkin, I. and Liu, R. (2003), ‘A bivariate beta distribution’, Statistics & Proba-
bility Letters 62(4), 407–412.
Orbanz, P. and Teh, Y. W. (2010), Encyclopedia of Machine Learning, Springer
US, Boston, MA, chapter Bayesian nonparametric models, pp. 81–89.
Paisley, J., Wang, C., Blei, D. and Jordan, M. (2015), ‘Nested hierarchical dirich-
let processes’, IEEE Transactions on Pattern Analysis and Machine Intelli-
gence 37(2), 256–270.
231

REFERENCES
Park, H. W. (2003), ‘Hyperlink network analysis: A new method for the study
of social structure on the web’, Connections 25(1), 49–61.
Perotte, A. J., Wood, F., Elhadad, N. and Bartlett, N. (2011), Hierarchically
supervised latent dirichlet allocation, in ‘25th Annual Conference on Neural
Information Processing Systems’, NIPS ’11, JMLR.org, Granada, Spain, p-
p. 2609–2617.
Pestian, J. P., Brew, C., Matykiewicz, P., Hovermale, D., Johnson, N., Cohen,
K. B. and Duch, W. (2007), A shared task involving multi-label classiﬁcation of
clinical free text, in ‘Proceedings of the Workshop on BioNLP 2007: Biological,
Translational, and Clinical Language Processing’, BioNLP ’07, Association for
Computational Linguistics, Stroudsburg, PA, USA, pp. 97–104.
R. Daniel Mauldin, William D. Sudderth, S. C. W. (1992), ‘Polya trees and
random distributions’, The Annals of Statistics 20(3), 1203–1221.
Rabiner, L. R. (1989), ‘A tutorial on hidden markov models and selected appli-
cations in speech recognition’, Proceedings of the IEEE 77(2), 257–286.
Ramage, D., Hall, D., Nallapati, R. and Manning, C. D. (2009), Labeled lda: A
supervised topic model for credit attribution in multi-labeled corpora, in ‘Pro-
ceedings of the 2009 Conference on Empirical Methods in Natural Language
Processing: Volume 1-Volume 1’, EMNLP ’09, Association for Computational
Linguistics, Association for Computational Linguistics, Singapore, pp. 248–
256.
Rao, V. and Teh, Y. W. (2009), Spatial normalized gamma processes, in ‘23rd
Annual Conference on Neural Information Processing Systems’, NIPS ’09, Cur-
ran Associates, Inc., Vancouver, British Columbia, Canada, pp. 1554–1562.
232

REFERENCES
Rasmussen, C. E. (1999), The inﬁnite gaussian mixture model, in ‘Advances in
Neural Information Processing Systems’, NIPS ’99, The MIT Press, Denver,
CO, USA, pp. 554–560.
Reisinger, J. and Paşca, M. (2009), Latent variable models of concept-attribute
attachment, in ‘Proceedings of the Joint Conference of the 47th Annual Meet-
ing of the ACL and the 4th International Joint Conference on Natural Lan-
guage Processing of the AFNLP: Volume 2-Volume 2’, ACL/IJCNLP ’09, As-
sociation for Computational Linguistics, Association for Computational Lin-
guistics, Singapore, pp. 620–628.
Ren, L., Dunson, D. B. and Carin, L. (2008), The dynamic hierarchical dirich-
let process, in ‘Proceedings of the 25th International Conference on Machine
Learning’, ICML ’08, ACM, Helsinki, Finland, pp. 824–831.
Ren, L., Dunson, D., Lindroth, S. and Carin, L. (2010), ‘Dynamic nonparametric
bayesian models for analysis of music’, Journal of the American Statistical
Association 105(490), 458–472.
Ren, L., Wang, Y., Carin, L. and Dunson, D. B. (2011), The kernel beta process,
in ‘25th Annual Conference on Neural Information Processing Systems’, NIPS
’11, Granada, Spain, pp. 963–971.
Rosen-Zvi, M., Chemudugunta, C., Griﬃths, T., Smyth, P. and Steyvers, M.
(2010), ‘Learning author-topic models from text corpora’, ACM Transactions
on Information Systems 28(1), 4:1–4:38.
Rosen-Zvi, M., Griﬃths, T., Steyvers, M. and Smyth, P. (2004), The author-
topic model for authors and documents, in ‘Proceedings of the 20th Conference
on Uncertainty in Artiﬁcial Intelligence’, UAI ’04, AUAI Press, Arlington,
Virginia, United States, pp. 487–494.
233

REFERENCES
Rossi, R. G. and Rezende, S. O. (2011), Building a topic hierarchy using the bag-
of-related-words representation, in ‘Proceedings of the 11th ACM Symposium
on Document Engineering’, DocEng ’11, ACM, Mountain View, CA, USA,
pp. 195–204.
Roy, D. M. and Teh, Y. W. (2008), The mondrian process, in ‘Proceedings
of the Twenty-Second Annual Conference on Neural Information Processing
Systems’, NIPS ’08, Curran Associates, Inc., Vancouver, British Columbia,
Canada, pp. 1377–1384.
Roychowdhury, A. and Kulis, B. (2015), Gamma processes, stick-breaking, and
variational inference, in ‘Proceedings of the Eighteenth International Confer-
ence on Artiﬁcial Intelligence and Statistics’, AISTATS ’15, JMLR.org, San
Diego, California, USA, pp. 800–808.
Sandler, R. and Lindenbaum, M. (2011), ‘Nonnegative matrix factorization with
earth mover’s distance metric for image analysis’, IEEE Transactions on Pat-
tern Analysis and Machine Intelligence 33(8), 1590–1602.
Sato, I., Kurihara, K. and Nakagawa, H. (2012), Practical collapsed variational
bayes inference for hierarchical dirichlet process, in ‘Proceedings of the 18th
ACM SIGKDD international conference on Knowledge discovery and data
mining’, KDD ’12, ACM, ACM, Beijing, China, pp. 105–113.
Seeger, M. (2004), ‘Gaussian processes for machine learning’, International Jour-
nal of Neural Systems 14(02), 69–106.
Sen, P., Namata, G. M., Bilgic, M., Getoor, L., Gallagher, B. and Eliassi-Rad, T.
(2008), ‘Collective classiﬁcation in network data’, AI Magazine 29(3), 93–106.
Sethuraman, J. (1994), ‘A constructive deﬁnition of dirichlet priors’, Statistica
Sinica 4, 639–650.
234

REFERENCES
Shahbaba, B. and Neal, R. (2009), ‘Nonlinear models using dirichlet process
mixtures’, Journal of Machine Learning Research 10, 1829–1850.
Shan, H. and Banerjee, A. (2008), Bayesian co-clustering, in ‘The Eighth IEEE
International Conference on Data Mining’, ICDM ’08, IEEE, Pisa, Italy, p-
p. 530–539.
Slutsky, A., Hu, X. and An, Y. (2013), Tree labeled lda: A hierarchical model
for web summaries, in ‘IEEE International Conference on Big Data’, BigData
’13, IEEE, Santa Clara, CA, USA, pp. 134–140.
Smyth, P., Welling, M. and Asuncion, A. U. (2009), Asynchronous distributed
learning of topic models, in ‘23rd Annual Conference on Neural Information
Processing Systems’, NIPS ’09, Curran Associates, Inc., Vancouver, British
Columbia, Canada, pp. 81–88.
Steinhardt, J. and Ghahramani, Z. (2012), Flexible martingale priors for deep
hierarchies, in ‘Proceedings of the Fifteenth International Conference on Arti-
ﬁcial Intelligence and Statistics’, AISTATS ’12, JMLR.org, La Palma, Canary
Islands, pp. 1108–1116.
Steyvers, M., Smyth, P., Rosen-Zvi, M. and Griﬃths, T. (2004), Probabilistic
author-topic models for information discovery, in ‘Proceedings of the Tenth
ACM SIGKDD International Conference on Knowledge Discovery and Data
Mining’, KDD ’04, ACM, Seattle, WA, USA, pp. 306–315.
Sun, Y., Deng, H. and Han, J. (2012), Mining Text Data, Springer US, Boston,
MA, chapter Probabilistic models for text mining, pp. 259–295.
Tamara Broderick, M. I. J. and Pitman, J. (2012), ‘Beta processes, stick-breaking
and power laws’, Bayesian Analysis 7(2), 439–476.
235

REFERENCES
Tanner, M. A. and Wong, W. H. (2010), ‘From em to data augmentation: The
emergence of mcmc bayesian computation in the 1980s’, Statistical Science
25(4), 506–516.
Teh, Y., Daumé, H. and Roy, D. (2007), Bayesian agglomerative clustering with
coalescents, in ‘Proceedings of the Twenty-First Annual Conference on Neural
Information Processing Systems’, NIPS ’07, Curran Associates, Inc., Vancou-
ver, British Columbia, Canada, pp. 1473–1480.
Teh, Y. W. (2006), A hierarchical bayesian language model based on pitman-yor
processes, in ‘Proceedings of the 21st International Conference on Compu-
tational Linguistics and 44th Annual Meeting of the Association for Com-
putational Linguistics’, ACL ’06, Association for Computational Linguistics,
Association for Computational Linguistics, Sydney, NSW, Australia, pp. 985–
992.
Teh, Y. W. (2010), Dirichlet process, in C. Sammut and G. Webb, eds, ‘Ency-
clopedia of Machine Learning’, Springer US, pp. 280–287.
Teh, Y. W., Blundell, C. and Elliott, L. (2011), Modelling genetic variation-
s using fragmentation-coagulation processes, in ‘25th Annual Conference on
Neural Information Processing Systems’, NIPS ’11, JMLR.org, Granada, S-
pain, pp. 819–827.
Teh, Y. W., Görür, D. and Ghahramani, Z. (2007), Stick-breaking construction
for the indian buﬀet process, in ‘Proceedings of the Eleventh International
Conference on Artiﬁcial Intelligence and Statistics’, AISTATS ’07, JMLR.org,
San Juan, Puerto Rico, pp. 556–563.
Teh, Y. W., Kurihara, K. and Welling, M. (2007), Collapsed variational inference
for hdp, in ‘Proceedings of the Twenty-First Annual Conference on Neural In-
236

REFERENCES
formation Processing Systems’, NIPS ’07, Curran Associates, Inc., Vancouver,
British Columbia, Canada, pp. 1481–1488.
Temperley, D. (2007), Music and Probability, MIT Press.
Thibaux, R. and Jordan, M. I. (2007), Hierarchical beta processes and the in-
dian buﬀet process, in ‘Proceedings of the Eleventh International Conference
on Artiﬁcial Intelligence and Statistics’, AISTATS ’07, JMLR.org, San Juan,
Puerto Rico, pp. 564–571.
Trivedi, P. K. and Zimmer, D. M. (2007), Copula Modeling: An Introduction for
Practitioners, Now Publishers Inc.
Van Dyk, D. A. and Meng, X.-L. (2001), ‘The art of data augmentation’, Journal
of Computational and Graphical Statistics 10(1), 1–50.
Wang, C. and Blei, D. M. (2009), Variational inference for the nested chinese
restaurant process, in ‘23rd Annual Conference on Neural Information Process-
ing Systems’, NIPS ’09, Curran Associates, Inc., Vancouver, British Columbia,
Canada, pp. 1990–1998.
Wang, C. and Blei, D. M. (2012), Truncation-free online variational inference
for bayesian nonparametric models, in ‘26th Annual Conference on Neural
Information Processing Systems’, NIPS ’12, Lake Tahoe, Nevada, USA, p-
p. 413–421.
Wang, C., Paisley, J. W. and Blei, D. M. (2011), Online variational inference
for the hierarchical dirichlet process, in ‘Proceedings of the Fourteenth Inter-
national Conference on Artiﬁcial Intelligence and Statistics’, AISTATS ’11,
JMLR.org, Fort Lauderdale, USA, pp. 752–760.
237

REFERENCES
Wang, X. and McCallum, A. (2006), Topics over time: A non-markov continuous-
time model of topical trends, in ‘Proceedings of the 12th ACM SIGKDD In-
ternational Conference on Knowledge Discovery and Data Mining’, KDD ’06,
ACM, Philadelphia, PA, USA, pp. 424–433.
Wang, Y. and Carin, L. (2012), Levy measure decompositions for the beta and
gamma processes, in ‘Proceedings of the 29th International Conference on
Machine Learning’, ICML ’12, ACM, Edinburgh, Scotland, UK.
William B. Frakes, R. B.-Y. (1992), Information Retrieval: Data Structures and
Algorithms, Prentice-Hall, Inc., Upper Saddle River, NJ, USA.
Williamson, S., Dubey, A. and Xing, E. (2013), Parallel markov chain monte
carlo for nonparametric mixture models, in ‘Proceedings of the 30th Interna-
tional Conference on Machine Learning’, ICML ’13, JMLR.org, Atlanta, GA,
USA, pp. 98–106.
Williamson, S., Orbanz, P. and Ghahramani, Z. (2010), Dependent indian buﬀet
processes, in ‘Proceedings of the Thirteenth International Conference on Ar-
tiﬁcial Intelligence and Statistics’, AISTATS ’10, JMLR.org, Sardinia, Italy,
pp. 924–931.
Willsky, A. S., Sudderth, E. B., Jordan, M. I. and Fox, E. B. (2009), Nonpara-
metric bayesian learning of switching linear dynamical systems, in ‘23rd Annu-
al Conference on Neural Information Processing Systems’, NIPS ’09, Curran
Associates, Inc., Vancouver, British Columbia, Canada, pp. 457–464.
Wood, F. and Griﬃths, T. L. (2006), Particle ﬁltering for nonparametric bayesian
matrix factorization, in ‘Proceedings of the Twentieth Annual Conference on
Neural Information Processing Systems’, NIPS ’06, MIT Press, Vancouver,
British Columbia, Canada, pp. 1513–1520.
238

REFERENCES
Wulsin, D. F., Fox, E. B. and Litt, B. (2014), ‘Modeling the complex dynamics
and changing correlations of epileptic events’, Artiﬁcial Intelligence 216(0), 55
– 75.
Xu, T., Zhang, Z., Yu, P. and Long, B. (2008), Evolutionary clustering by hi-
erarchical dirichlet process with hidden markov state, in ‘The Eighth IEEE
International Conference on Data Mining’, ICDM ’08, IEEE, Pisa, Italy, p-
p. 658–667.
Xuan, J., Lu, J., Zhang, G. and Luo, X. (2015), ‘Topic model for graph mining’,
IEEE Transactions on Cybernetics 45(12), 2792–2803.
Xuan, J., Lu, J., Zhang, G., Xu, R. and Luo, X. (2015), Inﬁnite author topic
model based on mixed gamma-negative binomial process, in ‘The 15th IEEE
International Conference on Data Mining’, ICDM ’15, IEEE, Atlantic City,
New Jersey, USA, pp. 489–498.
Yang, Z., Zhou, G., Xie, S., Ding, S., Yang, J.-M. and Zhang, J. (2011), ‘Blind
spectral unmixing based on sparse nonnegative matrix factorization’, IEEE
Transactions on Image Processing 20(4), 1112–1125.
Ye, M., Qian, Y. and Zhou, J. (2015), ‘Multitask sparse nonnegative matrix
factorization for joint spectral-spatial hyperspectral imagery denoising’, IEEE
Transactions on Geoscience and Remote Sensing 53(5), 2621–2639.
Yerebakan, H. Z., Rajwa, B. and Dundar, M. (2014), The inﬁnite mixture of
inﬁnite gaussian mixtures, in ‘Annual Conference on Neural Information Pro-
cessing Systems’, NIPS ’14, Montreal, Quebec, Canada, pp. 28–36.
Yu,
S.-Z.
(2010),
‘Hidden
semi-markov
models’,
Artiﬁcial
Intelligence
174(2), 215 – 243. Special Review Issue.
239

REFERENCES
Zhang, C., Ek, C. H., Gratal, X., Pokorny, F. T. and Kjellstrom, H. (2013),
Supervised hierarchical dirichlet processes with variational inference, in ‘IEEE
International Conference on Computer Vision Workshops’, ICCVW ’13, IEEE,
Sydney, NSW, pp. 254–261.
Zhang, J., Song, Y., Zhang, C. and Liu, S. (2010), Evolutionary hierarchical
dirichlet processes for multiple correlated time-varying corpora, in ‘Proceed-
ings of the 16th ACM SIGKDD International Conference on Knowledge Dis-
covery and Data Mining’, KDD ’10, ACM, Washington, DC, USA, pp. 1079–
1088.
Zhang, M. (2011), Lift: Multi-label learning with label-speciﬁc features, in ‘Pro-
ceedings of the 22nd International Joint Conference on Artiﬁcial Intelligence’,
IJCAI ’11, AAAI Press, arcelona, Catalonia, Spain, pp. 1609–1614.
Zhang, M.-L. and Wu, L. (2015), ‘Lift: Multi-label learning with label-speciﬁc
features’, IEEE Transactions on Pattern Analysis and Machine Intelligence
37(1), 107–120.
Zhang, M.-L. and Zhang, K. (2010), Multi-label learning by exploiting label de-
pendency, in ‘Proceedings of the 16th ACM SIGKDD International Conference
on Knowledge Discovery and Data Mining’, KDD ’10, ACM, Washington, DC,
USA, pp. 999–1008.
Zhang, M.-L. and Zhou, Z.-H. (2014), ‘A review on multi-label learning algo-
rithms’, IEEE Transactions on Knowledge and Data Engineering 26(8), 1819–
1837.
Zheng, J., Liu, S. and Ni, L. (2014), Eﬀective mobile context pattern discovery
via adapted hierarchical dirichlet processes, in ‘IEEE 15th International Con-
240

REFERENCES
ference on Mobile Data Management’, Vol. 1 of MDM ’14, IEEE, Brisbane,
Australia, pp. 146–155.
Zhou, M. and Carin, L. (2012), Augment-and-conquer negative binomial process-
es, in ‘26th Annual Conference on Neural Information Processing Systems’,
NIPS ’12, Lake Tahoe, Nevada, USA, pp. 2546–2554.
Zhou, M. and Carin, L. (2015), ‘Negative binomial process count and mixture
modeling’, IEEE Transactions on Pattern Analysis and Machine Intelligence
37(2), 307–320.
Zhou, M., Chen, H., Ren, L., Sapiro, G., Carin, L. and Paisley, J. W. (2009),
Non-parametric bayesian dictionary learning for sparse image representations,
in ‘23rd Annual Conference on Neural Information Processing Systems’, NIPS
’09, Curran Associates, Inc., Vancouver, British Columbia, Canada, pp. 2295–
2303.
Zhou, M., Hannah, L., Dunson, D. B. and Carin, L. (2012), Beta-negative bi-
nomial process and poisson gactor snalysis, in ‘Proceedings of the Fifteenth
International Conference on Artiﬁcial Intelligence and Statistics’, AISTATS
’12, JMLR.org, La Palma, Canary Islands, pp. 1462–1471.
Zhou, M., Yang, H., Sapiro, G., Dunson, D. B. and Carin, L. (2011), Depen-
dent hierarchical beta process for image interpolation and denoising, in ‘Pro-
ceedings of the 22nd International Joint Conference on Artiﬁcial Intelligence’,
IJCAI ’11, Barcelona, Catalonia, Spain, pp. 883–891.
Zhu, J., Ahmed, A. and Xing, E. P. (2012), ‘Medlda: Maximum margin super-
vised topic models’, Journal of Machine Learning Research 13(1), 2237–2278.
Zhu, X., Ming, Z.-Y., Zhu, X. and Chua, T.-S. (2013), Topic hierarchy con-
struction for the organization of multi-source user generated contents, in ‘Pro-
241

REFERENCES
ceedings of the 36th International ACM SIGIR Conference on Research and
Development in Information Retrieval’, SIGIR ’13, ACM, Dubin, Ireland, p-
p. 233–242.
242

ABBREVIATIONS
ATM
Author topic model
BNL
Bayesian nonparametric learning
BNM
Bayesian nonparametric model
CHDP
Cooperative hierarchial Dirichlet process
CRF
Chinese restaurant franchise
CRM
Completely random measure
CRP
Chinese restaurant process
DMPM
Dirichlet mixture probability measure
DP
Dirichlet process
DTM
Deep topic model
dIBP
Dependent Indian buﬀet processes
FGM
Farlie-Gumbel-Morgenstern
GaP
Gamma process
GEM
Griﬃths-Engen-McCloskey random measure
GP
Gaussian process
HDP
Hierarchial Dirichlet process
IBP
Indian buﬀet process
LDA
Latent Dirichlet allocation
IRP
International restaurant process
MCMC
Markov chain Monte Carlo
243

ABBREVIATIONS
MGNBP
Mixed Gamma negative binomial process
MRF
Markov random ﬁeld
M-H
Metropolis-Hastings sampling
NBP
Negative binomial process
NMF
Nonnegative matrix factorisation
NTR
Nonparametric relational topic model
RTM
Relational topic model
SMRF
Subsampling Markov random ﬁeld
244

