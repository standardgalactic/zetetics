Attending to Topological Spaces:
The Cellular Transformer
Rubén Ballester
Departament de Matemàtiques i Informàtica
Universitat de Barcelona
08007 Barcelona, Spain
ruben.ballester@ub.edu
Pablo Hernández-García
Departamento de Matemáticas
Universidad de Salamanca
pablohg.eka@usal.es
Mathilde Papillon
Department of Electrical Engineering
University of California Santa Barbara
papillon@ucsb.edu
Claudio Battiloro
Department of Biostatistics
Harvard University
cbattiloro@hsph.harvard.edu
Nina Miolane
Department of Electrical Engineering
University of California Santa Barbara
ninamiolane@ucsb.edu
Tolga Birdal
Department of Computer Science
Imperial College London
t.birdal@imperial.ac.uk
Carles Casacuberta
Departament de Matemàtiques i Informàtica
Universitat de Barcelona
08007 Barcelona, Spain
carles.casacuberta@ub.edu
Sergio Escalera
Departament de Matemàtiques i Informàtica
Universitat de Barcelona
08007 Barcelona, Spain
sergio.escalera.guerrero@gmail.com
Mustafa Hajij
Departament of Data Science
University San Francisco
mhajij@usfca.edu
Abstract
Topological Deep Learning seeks to enhance the predictive performance of neural
network models by harnessing topological structures in input data. Topological
neural networks operate on spaces such as cell complexes and hypergraphs, that
can be seen as generalizations of graphs. In this work, we introduce the Cellular
Transformer (CT), a novel architecture that generalizes graph-based transformers
to cell complexes. First, we propose a new formulation of the usual self- and
cross-attention mechanisms, tailored to leverage incidence relations in cell com-
plexes, e.g., edge-face and node-edge relations. Additionally, we propose a set
of topological positional encodings specifically designed for cell complexes. By
transforming three graph datasets into cell complex datasets, our experiments reveal
that CT not only achieves state-of-the-art performance, but it does so without the
need for more complex enhancements such as virtual nodes, in-domain structural
encodings, or graph rewiring.
1
Introduction
Topological Deep Learning (TDL) [50, 81] is a fast growing field that leverages generalizations
of graphs, such as simplicial complexes, cell complexes, and hypergraphs (collectively known
arXiv:2405.14094v1  [cs.LG]  23 May 2024

as topological domains) to extract comprehensive global information from data [11, 18]. Neural
networks that are designed to process and learn from data supported on these topological domains
form the core of TDL [81]. By transcending traditional graph-based representations, which are limited
to binary relations in graph neural networks (GNNs), TDL exploits novel information contained in
higher-order relationships, i.e., interactions involving multiple entities simultaneously. This capability
provides new, unique opportunities to address innovative applications across diverse disciplines,
including social sciences [111], transportation [63], physics [10], epidemiology [28], as well as
scientific visualization and discovery.
In parallel to TDL, the transformer architecture [95] has brought about a paradigm shift in learning
on data with various modalities. Using multi-headed attention, transformers can capture long-
range dependencies and hierarchical patterns in data like text or video [30, 51]. Particularly, graph
transformers [89, 101] form a subset of transformers specifically designed to work with graph-based
data. These models suffer from similar expressivity limitations as GNNs compared to Topological
Neural Networks (TNNs), as they only represent pairwise relationships within the data.
Contributions. In this work, we propose to bridge the gap between TDL and transformers. We
introduce the Cellular Transformer (CT) to simultaneously harness the power of the expressive cell
complex representation and the attention mechanism. By augmenting the transformer with topological
awareness through cellular attention, CT is inherently capable of exploiting complex patterns in data
mapped to high order representations, showing competitive or improved performance compared to
graph and simplicial transformers and message passing architectures.
Our specific contibutions are summarized as follows.
1. We propose the CT framework, which generalizes the graph-based transformer to process
higher-order relations within cell complexes.
2. We introduce cell complex positional encodings and formulate self-attention and cross-attention
in topological terms. We demonstrate how to utilize these computational primitives to process
data supported on cell complexes in a transformer layer.
3. We benchmark the CT on three classical benchmark datasets, outperforming or achieving results
comparable to the state-of-the-art without the need for complex enhancements of the architecture
such as virtual nodes, involved in-domain structural or positional encodings or rewiring methods.
2
Related Work
Transformer models have seen significant advancements in various domains, including natural
language processing [65, 84, 95], computer vision [1, 30, 51], or graph learning.
Graph transformers. Graph transformers are a special subset of transformers designed to learn from
data supported on graphs. This evolving field encompasses three distinct strategies to harnessing the
power of transformers in graph contexts. The first approach involves integrating GNNs directly into
transformer architectures, either by stacking [89, 101], interweaving [72], or running in parallel [106].
A second method focuses on encoding the graph structure into positional embeddings, which are then
added to the input of the transformer model for spatial awareness. These embeddings can be computed
in a myriad of ways, such as from Laplacian eigenvectors [31] or SVD vectors of the adjacency
matrix [58]. Finally, the third approach hard codes adjacency information into the self-attention.
In [31, 78], only adjacent nodes are allowed to attend to each other, as all other attentional weights
are zeroed. [76] similarly manipulates self-attention via the kernel matrix instead of the adjacency
matrix. We refer the reader to [77] for more information. Our work adopts a combination of the
second and third methods in order to adapt the transformer to data supported on cell complexes.
Higher-order transformers. Transformer models which go beyond pairwise relations represent a
natural progression from graph-based transformers. The most prominent category of such higher order
transformers operates on hypergraphs. In many instances, they have also adopted the self-attention
mechanism [57, 67, 99, 107]. Beyond graphs and hypergraphs, transformers operating on topological
domains are scarce. To our knowledge, only two higher-order transformers operating on simplicial
complexes have been proposed [24, 110]. The first approach, however, does not consider higher order
features directly, but rather leverages higher order relations to improve features on nodes. The second
approach, although being a fairly general object to define higher-order structures, focuses primarily
2

on graph learning. It proposes two architectures: one operating on tuples of nodes (i.e., cliques)
within the graph, which may not naturally appear in the clique complex of the graph, and another
applying a general attention mechanism to all simplices in a lifted graph simultaneously, disregarding
the distinct nature of data across dimensions (e.g., properties of atoms in nodes vs. properties of
bonds in edges). The latter was only tested on nodes and edges, excluding higher-order elements. A
limitation of simplicial approaches is their representative power, as triangles and tetrahedra are scarce
in natural data domains.
Non-transformer topological neural networks. Besides transformers,
recent years have witnessed a growing interest in higher-order networks [11, 18]. In signal processing
and deep learning, various approaches, such as Hodge-theoretic methods, message-passing schemes,
and skip connections, have been developed for TNNs. The use of Hodge Laplacians for data analysis
has been investigated in [64, 71] and extended to a signal processing context, for example, in [88, 91,
92] for simplicial and cell complexes. Convolutional operators and message-passing algorithms for
TNNs have been developed. For hypergraphs, a convolutional operator has been proposed in [2, 36,
62] and has been further investigated in [3, 39, 62]. Message passing on simplicial and cell complexes
are proposed in [46, 49]. The expressive power of these networks is studied in [19]. Moreover,
message passing on network sheaves can be found in [5, 6, 9, 20, 52, 53]. A model able to infer a
latent regular cell complex from data has been introduced in [8]. Recently, message passing-free
architectures have been introduced for simplicial complexes [44, 73, 85].
Most attention-based models are designed primarily for graphs, with some recent exceptions that have
been introduced in topological domains [3, 40, 42, 66]. Attention on cell complexes was introduced
in [41] exploiting higher-order topological information through feature lifting and attention mechan-
isms over lower and upper neighborhoods, and a generalized attention mechanism on combinatorial
complexes was introduced in [48, 50]. However, neither [41] nor [50] consider query-key-value
attention and positional/structural encodings. The work in [41] works at the edge level and does not
leverage any interplay among cells of different ranks. Furthermore, the work in [50] does not account
for dense attention, and, being based on the general notion of combinatorial complex, it is not able to
readily encode and leverage specific topological features peculiar to cell complexes.
3
Cell Complexes
Cell complexes encompass various kinds of topological spaces used in network science, including
graphs, simplicial complexes, and cubical complexes. A precise definition can be found in [54], and
further information is provided in the Appendix.
We constrain ourselves with 2-dimensional regular cell complexes for simplicity, although our
constructions and discussion carry over to higher-dimensional cellular complexes similarly. Thus,
in this work, as in [88], a cell complex is a triplet X = (X0, X1, X2) of finite ordered sets, where
elements v ∈X0 are called nodes, vertices, or 0-cells, elements e ∈X1 are called edges or 1-cells,
and elements σ ∈X2 are called faces or 2-cells. Additionally, incidence relations represent each
edge as an ordered pair of vertices e = [v1, v2] incident to e, and each face as an ordered sequence
of edges σ = [e1, . . . , em(σ)] that form a closed path without self-intersections and constitute the
edges incident to σ. We assume that m(σ) ≥3 to ensure that the pair (X0, X1) is a (loopless, simple,
directed) graph. The subscript k of each set Xk is called its rank.
The incidence relations endow each edge and each face with an orientation. For an edge e = [v1, v2],
the oppositely oriented edge is denoted by −e = [v2, v1]. A collection of edges e1, . . . , em form a
closed path if there is a set of distinct vertices v1, . . . , vm such that ±ei = [vi, vi+1] for 1 ≤i ≤m
and vm+1 = v1. Incidence relations between cells of consecutive ranks are encoded into incidence
matrices. Thus, the first incidence matrix B1 has (i, j) entry equal to −1 if the j-th edge ej starts at
the i-th vertex vi, 1 if ej ends at vi, and 0 otherwise. The entries of the second incidence matrix B2
are the incidence numbers between faces and edges, where the incidence number of a face σ with an
edge e is the sign of ±e if it belongs to a closed path of edges incident to σ, and 0 otherwise. These
two matrices satisfy B1B2 = 0, as shown in [43, 54]. The non-signed incidence matrices Ik for
k = 1, 2, are obtained by replacing incidence numbers in Bk by their absolute values.
We refer to neighborhood matrices, including signed and non-signed incidence matrices, upper and
lower adjacency matrices, and Hodge Laplacians, as defined in detail in the Appendix.
3

3.1
Data on cell complexes: cochain spaces
Cochain spaces are used to process data supported over a cell complex X = (X0, X1, X2). For
k = 0, 1, 2, we denote by Ck(X, Rd) the R-vector space of functions Xk →Rd, where d ≥1. Here
d is called data dimension and elements of Ck(X, Rd) are called k-cochains or k-signals on X. For
short, we write Ck(X) instead of Ck(X, R) when d = 1; see fig. 1 for an example.
An annotated cell complex is a cell complex X together with a k-cochain Xk of dimension dk for
each rank k. We view Xk as a matrix in M(|Xk|, dk), that is, with |Xk| rows and dk columns, whose
i-th row is the image of the i-th element of Xk. In this work, all datasets consist of annotated cell
complexes sharing the same dimensions d0, d1 and d2.
X0
X1
X2
X0
X1
X2
Figure 1: Illustration of an annotated cell complex. Left: An annotated cell complex X consisting of
five vertices, five edges, and one 2-cell. Center: Xk is the collection of k-cells of X for k = 0, 1, 2.
Right: Rows depict values of a cochain Xk for each k, of dimensions d0 = 4, d1 = 3 and d2 = 2.
4
The Cellular Transformer
In this section, we present a general transformer architecture for cell complexes. First, we discuss
different approaches to perform attention on cells and define the cellular transformer layer. Then, we
propose different positional encoding methods for the cellular transformer, that identify cells by their
relative position in the cell complex or by their centrality according to random walks, as in [33].
4.1
Overview
A cellular transformer is a neural network which, given an annotated cell complex X, induces a
composition of functions CT = R ◦CTL ◦· · · ◦CT1 ◦P, named layers, where P is a preprocessing
layer, as described in section 4.4, R is a readout layer that converts cochains on top of cells into
an output prediction value, and CTl, for l = 1, . . . , L, are cellular transformer layers defined as
functions of the form
CTl : C0(X, Rdh
0 ) × · · · × Cn(X, Rdh
n) −→C0(X, Rdh
0 ) × · · · × Cn(X, Rdh
n),
(1)
where n = dim X and h indicates the dimension of hidden layers. In our experiments, we set
dh
0 = · · · = dh
n, whose value depends on the dataset and is specified in the Appendix.
Equation (1) does not provide an explicitly parametrization of the cellular transformer layer as it
only describes the function (co)domains. A parametrization of the cellular transformer layers can be
given using tensor diagrams together with the cellular attention formulae, described in section 4.2.
Transformer and preprocessing layers take as input an annotated cell complex and output the same
cell complex with different cochains. We denote input k-cochains on the CTl layer as Xk,l.
4.2
The cellular attention layer and cellular transformer architecture
We propose two cellular attention mechanisms for transformer layers. The first mechanism generalizes
self- and cross-attention and depends on the dimensions of the cells. We call it pairwise cellular
attention. The second mechanism performs the attention over all cells ignoring their dimension. We
call it general cellular attention. We discuss in section 5.1 the data regimes in which each attention
mechanism performs better than the other.
4.2.1
Pairwise cellular attention
Our first mechanism performs pairwise attention between cells of arbitrary ranks according to a tensor
diagram (see section 4.3), and then aggregates the outputs received for the same rank. Given source
4

and target ranks 0 ≤ks, kt ≤dim X and cochains Xkt, Xks, the single-head attention from ks to kt
is a map Cks(X, Rdh
s ) × Ckt(X, Rdh
t ) →Ckt(X, Rdh
t ) defined as
A•
ks→kt(Xkt, Xks) = softmax(XktQks→kt(XksKks→kt)T ⋆ϕ(Nks→kt))XksVks→kt,
(2)
where Qks→kt ∈M(dh
t , p), Kks→kt ∈M(dh
s, p), and Vks→kt ∈M(dh
s, dh
t ) are learnable query,
key, and value real matrices with p a fixed hyperparameter shared by all transformer layers. The
symbol • ∈{d, s, c} indicates whether we are performing dense, sparse, or a mixed type of attention,
performing dense attention for cells of the same rank and sparse attention otherwise, respectively.
The symbol ⋆is a sum or a Hadamard product for dense or sparse attention, respectively. N is a
neighborhood matrix, and ϕ is a function, possibly with learnable parameters. For our experiments,
we set ϕ(N) = θN where θ is a learnable parameter for dense attention and the identity matrix for
sparse attention. Attention formulae performs query, key, and value projections without bias for
simplicity. A bias term can be added to the projections, as in most transformer implementations.
Multi-head attention can also be performed by (1) splitting the cochains Xks and Xkt into multiple
cochains X1
ks, . . . , Xm
ks and X1
kt, . . . , Xm
kt of smaller dimension; (2) performing single-head attention
for each pair of cochains Xi
ks, Xi
kt; (3) concatenating the outputs of the single-head attention for the
different pairs into a full cochain of dimension dh
t .
For a specific rank kt, CT layers can produce multiple attention outputs from different rank sources
ks. In the CT layer, we adopt the standard prenorm design [102], where for each rank kt, the outputs
from the various rank sources ks are summed in the residual connection. The specific algorithm for
the CT layer is detailed in the Appendix. In our experiments, we set Nk→k to be the upper adjacency
matrix for k = 0 and the lower adjacency matrix if k > 0, and Nks→kt to be the non-signed incidence
matrix between dimensions ks and kt if ks > kt, and the transpose matrix otherwise.
4.2.2
General cellular attention
The second mechanism performs attention with all the cells at the same time, disregarding their
rank, as proposed in [110]. In the general attention mechanism, cells share the same key and query
matrices, but have different value matrices for each rank. The single-head attention formula is
A•
g(X) = softmax
 XQ(XK)T ⋆ϕ(N)
  Concat

(X0V 0)T , . . . , (Xdim X V dim X )T T ,
using the same notations as in the previous subsection. In this case, the prenorm transformer layer
is performed as usual. The algorithm corresponding to the general attention CT layer is detailed in
the Appendix. As in section 4.2.1, multi-head attention can be performed by splitting the original
cochains into smaller cochains, applying the general single-head attention to pairs of the smaller
cochains, and concatenating again into a single, big cochain. In our experiments, we let N be the
following combination of the previous Nks→kt matrices, where n = dim X:
N =


N0→0
N1→0
. . .
0
NT
1→0
N1→1
. . .
0
...
...
...
...
0
0
. . .
Nn→n

.
(3)
4.3
Tensor diagrams for cellular transformers
Cellular transformers involve interactions between cochains of different ranks. Tensor diagrams [50]
provide a graphical abstraction that illustrates the flow of information of one CT layer. A tensor
diagram portrays a CT Layer through the use of a directed graph. Nodes of a tensor diagram represent
cochain spaces for different ranks 0 ≤k ≤n, where n is the maximum allowed rank of cell
complexes processed by the CT layer. If the input cell complex X is of lower dimension than n, the
attention on ranks k > dim X are ignored. In turn, edges represent either the pairwise attentions
performed in the CT layer together with the bias matrices Nks→kt, or simply the matrices used to
build the matrix N from smaller matrices Nks→kt as in eq. (3), for the general attention. A missing
arrow from cochains of rank ks to cochains of rank kt implies a zero in the block of N corresponding
to the matrix Nks→kt. An illustration of the tensor diagram used in our experiments is given in fig. 2.
4.4
Positional encodings on cellular complexes
Transformers do not leverage the input structure explicitly by default [95]. Positional encodings
help to overcome this problem by injecting positional and structural information about the input
5

Aup
0
A down
1
A down
2
IT
1
I1
IT
2
I2
C0
C0
C1
C1
C2
C2
Aup
0
IT
1
Adown
1
IT
2
I1
Adown
2
I2
Figure 2: Tensor diagram illustrating the flow of signals between cochains defined on 0-, 1-, and
2-cells. For pairwise attention (section 4.2.1), the neighborhood matrices indicate the bias N in the
attention formula (2). For general attention (section 4.2.2), neighborhood matrices indicates how to
build the bias matrix N by composition of smaller bias matrices Nks→kt between dimensions.
tokens. For sequences, the first positional encoding used sine and cosine functions depending on the
position of the token in the sequence. For graphs, several positional encodings have been studied such
as the eigenvectors of the graph Laplacian (LapPE) [32] and Random Walk Positional Encodings
(RWPe) [33], where the latter were also adapted for simplicial complex transformers [93, 109].
Let 0 ≤k ≤dim X, where X is a cell complex. A cellular k-positional encoding of Xk is a k-cochain
Ek that captures some structural information about Xk within X (positional encoding may also be
defined on the entire complex X). Given cochains Xk and positional encodings Ek, the input for the
first transformer layer is defined as a function Pk : Ck(X, Rdk) × Ck(X, Rdpe) →Ck(X, Rdh
k) with
X1
k = Pk(Xk, Ek), where Pk combines the signals and the positional encodings. Usual functions are
SumPE(Xk, Ek) = Xkθin,k + bin,k + Ekθin,pe + bin,pe
ConcatPE(Xk, Ek) = Concat(Xk, Ek)θin,pe + bin,pe,
(4)
where θ•, b• ∈are learnable parameters. For this paper, we use Pk = ConcatPE.
Next we discuss three novel positional encodings on cell complexes 4.4.1: Barycentric Subdivision
4.4.2, Random Walk, and Topological Slepians 4.4.3.
4.4.1
BSPe: Barycentric Subdivision Positional Encoding
A popular positional encoding for graph transformers is given by graph Laplacian eigenvectors
(LapPE) [32]. LapPE assigns to each vertex vi a vector LapPE(vi) = (e1
i , . . . , ek
i ), where {ej
i | j =
1, . . . , k} are eigenvectors of the k smallest eigenvalues of the normalized graph Laplacian for a
graph G = (V, E), counting multiplicities, where k is a hyperparameter.
We denote the naive extension from LapPE for cell complexes using the unnormalized Hodge
Laplacian matrix, instead of the graph Laplacian one, as HodgeLapPE. We use the unnormalized
version because normalizing the Hodge Laplacian for dimensions greater than zero is not an easy
task [93]. HodgeLapPE are, however, not a good choice for high-order positional encodings a priori
due to both a lack of normalization and the ambiguous information contained in Hodge Laplacians
for nonzero rank. Details on LapPE for graphs and their HodgeLapPE extension are in the Appendix.
To overcome the previous drawbacks, we propose to extend LapPE by taking the original Laplacian
positional encodings of the 1-skeleton of the barycentric subdivisions of the cell complexes. The
barycentric subdivision of a cell complex X, denoted by ∆(X), is the order complex of its face poset
[98], i.e., the abstract simplicial complex whose set of vertices is the set of cells of X and whose
simplices are the totally ordered flags of cells of X. Barycentric subdivisions yield triangulations
of cell complexes that preserve their topological properties [25]. The 1-skeleton of the barycentric
subdivision of X is a graph G = (V, E) where V is the set of cells of X and where two vertices σ1
and σ2 are connected if one is a face of the other. The positional encoding of a cell σ is the Laplacian
positional encoding of σ seen as a vertex in G.
This positional encoding respects the same theoretical advantages of the LapPE while assigning
relative positions to all the cells at the same time, and thus relative positions take into account all the
cells and not only the cells of a specific dimension. We say that the positional encodings satisfying
6

v1
v2
v3
v0
e0
e2
e3
e4
e1
bc
bc
bc
bc
σ
v0
v1
v2
v3
e0
e1
e2
e3
e4
bc
bc
bc
bc
bc
bc
bc
bc
bc
bc
σ
v0
v1
v2
v3
e0
e1
e2
e3
e4
bc
bc
bc
bc
bc
bc
bc
bc
bc
bc
σ
Figure 3: Left: A cell complex X. Center: Barycentric subdivision of X. Right: 1-skeleton of the
barycentric subdivision. Each original cell of X is represented by a node in the 1-skeleton.
this property are called global, in contrast to local positional encodings, where encodings are assigned
independently for each dimension. We denote this positional encoding as BSPe.
4.4.2
RWPe: Random Walk Positional Encoding
RWPe take another different way of positioning vertices on a graph based on random walks. Given
a vertex vi ∈V , the RWPe of vi is given by the vector RWPe(vi) =
 RWii, . . . , RWk
ii

where
RW = AD−1 is the random walk operator of a graph based on edge connectivity. In this case, each
vertex is assigned the probabilities of landing again on itself on the random walks from one to k steps.
In this case, RWPe(vi) is unique and does not need sign or eigenvector selection invariance. For
RWPe we have again difficulties defining meaningful random walks on general cell complexes. This
problem is first explored in [93] and then in [109] for simplicial complexes, making a special focus
on edge random walks. There, transition matrices are defined to perform random walks in simplices
of an arbitrary rank and are not affected by the orientation of the simplicial complex.
As a first, more naive approach, we propose to extend RWPe to cell complexes by taking the positional
encodings given by the original RWPe for the 1-skeleton of the barycentric subdivision introduced
in section 4.4.1. We denote these global positional encodings as RWBSPe. We also propose a
more sophisticated, local approach, denoted RWPe, extending the random walks from [93] to cell
complexes. The full development of the random walk matrix can be found in the Appendix. From the
random walk matrix, positional encodings are taken as in RWPe for each rank of the cell complex.
4.4.3
TopoSlepiansPE: Topological Slepians Positional Encoding
The objective of positional encoding is to assign a relative position to tokens informed by the
underlying domain, which for us are cells within a cell complex. Topological Slepians, as introduced
in [7], represent a novel category of signals specifically defined over cell complexes. These signals
are characterized by their maximal concentration within the topological domain (the cells) and their
perfect localization in the corresponding dual domain (the frequencies [90]). This means that the
non-zero coordinates of the different topological Slepians are concentrated only on some of the
cells of the complex, with a specific spectral content. Topological Slepians have been shown to be
particularly efficient for signal representation and sparse coding tasks [7], making them a valuable
tool to devise positional encoding.
In this work, we compute Slepians as in Section 4 of [7]. Given an ordered multiset of rank k Slepians
Sk = {si
k | i = 1, . . . , dpe}, we let the positional encoding of a k-cell σ be E(σ)i = si
k,σ, the
coordinate corresponding to σ of the si
k Slepian (in practice, we can obtain |Sk| = dpe by adding zero
vectors). We denote this positional encoding as TopoSlepiansPE. Although it comes with guaranteed
localization properties, it is a local positional encoding, and suffers from permutation variance and
eigenvector sign variance, that the transformers need to learn.
5
Experiments
Due to the lack of cell complex datasets, as argued in [80], we test our cellular transformers in
three different graph datasets: ZINC [61], ogbg-molhiv [56], and the graph classification
benchmark (GCB) [15] in its hard version, that we lift to cell complexes. We lift each graph G to
a cell complex by filling its cycles with 2-cells using the TopoX library [47]. Cycle filling is not
the optimal way of adding cells to our molecules datasets, and may detriment the performance of
7

Table 1: Models and scores compared for the datasets GCB, ZINC, and ogbg-molhiv. Average and
standard deviation for accuracy (↑) are shown for the GCB test dataset; MAE (↓) is reported for the
ZINC test dataset; AUC-ROC (↑) is reported for the ogbg-molhiv test dataset. GCB models are
described in the Appendix. Results for GCB were extracted from [13], and other results were extracted
from [110]. The first seven model rows represent message passing architectures. For the GCB dataset,
the other six models are classic machine learning algorithms. For the ZINC and OGB datasets, the
second set of rows correspond to graph transformer models, and the third set of rows belong to the
simplicial transformer models of [110]. The abbreviation v.n. means virtual node.
GCB
ZINC
ogbg-molhiv
Model
Accuracy (↑)
Model
MAE (↓) Model
AUC-ROC (↑)
Graclus [29]
0.690 ± 0.015 GCN [68]
0.367
GCN+v.n.
0.7599
NDP [17]
0.726 ± 0.009 GAT [96]
0.384
GIN+v.n. [103]
0.7707
DiffPool [105]
0.699 ± 0.019 GatedGCN [22]
0.282
DGN [12]
0.7970
Top-K [38]
0.427 ± 0.152 PNA [26]
0.188
PNA
0.7905
SAGPool [70]
0.377 ± 0.145
GSN [21]
0.8039
MinCutPool [16]
0.738 ± 0.019 CIN [19]
0.079
CIN
0.8094
ESC + RBF-SVM [74] 0.625 ± 0.046 GIN-AK+ [108]
0.080
GIN-AK+
0.7961
ESC + L1-SVM [74]
0.722 ± 0.010 Graphormer [104]
0.122
ESC + L2-SVM [74]
0.693 ± 0.016 SAN [69]
0.139
SAN
0.7785
Hist Kernel [75]
0.720 ± 0.000 EGT [59]
0.108
Jaccard Kernel [75]
0.630 ± 0.000 GPS [86]
0.070
GPS
0.7880
Edit Kernel [75]
0.600 ± 0.000 ASSN
0:1
0.080
Stratedit Kernel [75]
0.600 ± 0.000 ASSN+VS
0:1
[110]
0.073
ASSN+VS
0:1
0.7981
C (ours)
0.752 ± 0.010 C (ours)
0.080
C (ours)
0.7946
Table 2: Table containing results for the best attention mechanism and positional encoding combin-
ations on the datasets GCB, ZINC, and ogbg-molhiv. Columns indicate combinations of attention
mechanisms and positional encodings. Abbreviations for the positional encodings are B for BSPe,
H for HodgeLapPE, RB for RWBSPe, T for TopoSlepiansPE, and R for RWPe. Best result, second
best result, and third best result for each dataset are highlighted in boldface green, blue, and orange,
respectively. For GCB, the third best result is obtained by the zero positional encoding.
Dataset
As
g
B
H
RB
T
R
GCB (Accuracy ↑)
0.7516
0.7432
0.7442
0.7432
0.7442
ZINC (MAE ↓)
0.0840
0.0824
0.1296
0.1303
0.1051
ogbg-molhiv (AUC-ROC ↑)
0.7681
0.7032
0.7082
0.7192
0.7343
Dataset
Ac
ks→kt
B
H
RB
T
R
GCB (Accuracy ↑)
0.7347
0.7337
0.7442
0.7421
0.7379
ZINC (MAE ↓)
0.0833
0.0831
0.0802
0.1202
0.0833
ogbg-molhiv (AUC-ROC ↑)
0.7784
0.7658
0.7321
0.7565
0.7338
Dataset
As
ks→kt
B
H
RB
T
R
GCB (Accuracy ↑)
0.7400
0.7421
0.7389
0.7505
0.7505
ZINC (MAE ↓)
0.0934
0.0852
0.1210
0.1452
0.0973
ogbg-molhiv (AUC-ROC ↑)
0.7946
0.7586
0.7058
0.7111
0.7288
the transformers. Yet, we will show that it still allows our proposed transformer to achieve results
comparable to the state-of-the-art, and thus represents a first step towards encouraging the community
to develop cell complex datasets.
For each dataset, we use its official train, validation and test splits, and we try all the possible
combinations of the attention mechanisms presented in section 4.2 with the positional encodings
presented in section 4.4 plus a positional encoding called zero that assigns a zero vector of fixed
length to each cell, simulating absence of positional encodings. For GCB, we run the experiments with
8

five different random seeds and report average accuracy and standard deviation. Due to computational
constraints, we only run the experiments once for the other two datasets ZINC and ogbg-molhiv,
reporting only the obtained score. For the state-of-the-art methods, we report the average and standard
deviation achieved for these datasets in [15, 110]. Details on the architectures are in the Appendix.
From state-of-the-art methods in [110], we only report the graph and simplicial architectures, and
skip the transformers based on clique-lifting, as these cliques do not appear naturally on the graph as
high-order cells, and our purpose is developing general cell complex transformers for cell domains.
In their experiments, though, the models, while applicable for arbitrary ranks, were tested using
only vertex and edge data, excluding higher-order features. The tensor diagram for the self- and
cross-attention and general architectures in each layer can be found in fig. 2. Results with our best
performing combinations of attention mechanism and positional encodings are in table 1. A summary
of the best attention and positional combinations for CT is reported in table 2. Complete experimental
results, training details, and hardware resources used for the experiments are in the Appendix.
5.1
Discussion
Overall, we outperform all previous state-of-the-art methods tested in the GCB dataset and we obtain
comparable results to some of the most effective architectures in the other two.
For ZINC, we surpass most of the message passing architectures with the exception of CIN [19]
and GIN-AK [108], for which the differences in the performance between them and our models are
relatively small compared to the biggest gap in performance for the dataset. For ogbg-molhiv, mes-
sage passing architectures consistently obtain comparable or better results than graph transformers,
except for the GCN [68] and GIN [103] architectures. In the case of graph transformers, the best
performing architecture for ZINC is GPS [86], which we surpass in ogbg-molhiv. Following GPS,
the second best transformer in ZINC is the simplicial transformer of [110] equipped with virtual
simplices, a similar technique to virtual nodes in graph architectures [60, 94]. Without equipping
virtual simplices, though, the MAE values for our best model in ZINC and for the simplicial trans-
formers are equal. Both architectures outperform all other non-GPS traditional graph transformers in
this dataset, suggesting that high-order interactions are relevant even in the case of graph datasets.
For ogbg-molhiv, our best model outperforms graph transformer architectures, and obtains slightly
worse performance than the simplicial transformer for vertices and edges of [110] equipped with
virtual simplices. Results without virtual simplices were not reported for ogbg-molhiv.
The results corroborate that leveraging high-order information about the dataset in transformer
architectures is capable of outperforming or obtaining comparable SOTA results without the need for
advanced techniques such as graph rewiring, virtual nodes, or learnable bias matrices in the attention
mechanism.
General vs pairwise attention. We observe that pairwise attention mechanisms obtained the best
results for the molecule datasets (ZINC and ogbg-molhiv) and the general attention mechanism
obtained the best result for the GCB dataset. We observe that in the GCB dataset, vertices, edges, and
2-cells contain homogeneous information about clustering properties of the input graphs, where edges
and 2-cells use concatenations of vertex features associated to the cell. On the other hand, ZINC
and ogb-molhiv contain atomic information for the nodes and 2-cells and bond information for the
edges, making the cochains heterogeneous at different ranks. The results suggest that
1. general attention, using common query and key projections for all cells at the same time, is more
suitable for problems where features in all ranks are homogeneous, i.e., of the same nature;
2. pairwise attention, that uses specific query and key projections for pairwise ranks, is more
suitable for problems where features are heterogeneous, because it allows to each rank to attend
to specific properties of each rank in an isolated way.
Global vs local positional encodings. We classified our positional encodings into two groups
depending on whether they were obtained for all ranks simultaneously (global p.e.) or for each rank
isolatedly (local p.e.). We observe that global positional encodings obtained the best results in the
three datasets. However, for GCB and ZINC, the second and third best results used local positional
encodings. For GCB, the second place was shared between TopoSlepiansPE and RWPe, while the
third place was achieved with the zero positional encoding. For the ZINC dataset, the second and third
9

places were obtained by HodgeLapPE. Interestingly, neither TopoSlepiansPE nor RWPe obtained
good results for the molecule datasets overall.
6
Conclusion
In this work, we introduced the Cellular Transformer (CT), a novel transformer architecture for cell
complexes that leverages high-order relationships inherent in topological spaces, together with new
positional encodings for it. Our experimental results demonstrated that CT achieves state-of-the-art
performance or comparable results without the need for additional enhancements such as virtual
nodes or learnable bias matrices. This work motivates further exploration in topological deep learning,
particularly in the development of cell complex datasets and the improvement of transformer layers
and positional encodings, such as it has happened for graph transformers. Future work will focus on
extending the CT framework to more efficient and effective attention mechanisms, exploring more
sophisticated positional encodings, and applying the framework to a broader range of real-world
datasets and techniques where transformers are being used, such as generative or multi-modal models.
6.1
Limitations
Cellular transformers suffer from the same computational limitations as traditional and graph trans-
formers [95], with attention having quadratic complexity on the number of cells. This means that
cellular transformers are currently limited to cell complexes with a low number of cells. However, as
in other transformers, computational limitations may be overcome with linearized attention mechan-
isms [23]. We leave the study of efficient cellular transformers as future work. Another drawback
that comes with general topological deep learning is that, for harnessing the power of cell complexes
in graph datasets, the selection of a good lifting procedure converting graphs into complexes is
fundamental. Although there is no general answer as to which lifting algorithm to select in each case,
we expect to see proposals in the second edition of the Topological Deep Learning Challenge [14].
References
[1]
A. Arnab, M. Dehghani, G. Heigold, C. Sun, M. Luˇci´c, and C. Schmid. ‘Vivit: A video vision
transformer’. In: Proceedings of the IEEE/CVF international conference on computer vision.
2021, pp. 6836–6846.
[2]
D. Arya and M. Worring. ‘Exploiting relational information in social networks using geo-
metric deep learning on hypergraphs’. In: Proceedings of the 2018 ACM on International
Conference on Multimedia Retrieval. 2018, pp. 117–125.
[3]
S. Bai, F. Zhang, and P. H. Torr. ‘Hypergraph convolution and hypergraph attention’. In:
Pattern Recognition 110 (2021), p. 107637.
[4]
S. Barbarossa and S. Sardellitti. ‘Topological Signal Processing Over Simplicial Complexes’.
In: IEEE Transactions on Signal Processing 68 (2020), pp. 2992–3007. ISSN: 1941-0476.
DOI: 10.1109/tsp.2020.2981920. URL: http://dx.doi.org/10.1109/TSP.2020.
2981920.
[5]
F. Barbero, C. Bodnar, H. S. de Ocáriz Borde, M. Bronstein, P. Veliˇckovi´c, and P. Liò. Sheaf
Neural Networks with Connection Laplacians. 2022. arXiv: 2206.08702 [cs.LG].
[6]
C. Battiloro, Z. Wang, H. Riess, P. D. Lorenzo, and A. Ribeiro. ‘Tangent Bundle Filters and
Neural Networks: From Manifolds to Cellular Sheaves and Back’. In: ICASSP 2023 - 2023
IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). 2023,
pp. 1–5.
[7]
C. Battiloro, P. Di Lorenzo, and S. Barbarossa. ‘Topological Slepians: Maximally Localized
Representations of Signals Over Simplicial Complexes’. In: ICASSP 2023 - 2023 IEEE
International Conference on Acoustics, Speech and Signal Processing (ICASSP). 2023, pp. 1–
5. DOI: 10.1109/ICASSP49357.2023.10095803.
[8]
C. Battiloro, I. Spinelli, L. Telyatnikov, M. M. Bronstein, S. Scardapane, and P. D. Lorenzo.
‘From Latent Graph to Latent Topology Inference: Differentiable Cell Complex Module’.
In: The Twelfth International Conference on Learning Representations. 2024. URL: https:
//openreview.net/forum?id=0JsRZEGZ7L.
10

[9]
C. Battiloro, Z. Wang, H. Riess, P. Di Lorenzo, and A. Ribeiro. ‘Tangent bundle convolutional
learning: from manifolds to cellular sheaves and back’. In: IEEE Transactions on Signal
Processing (2024).
[10]
F. Battiston et al. ‘The physics of higher-order interactions in complex systems’. In: Nature
Physics 17.10 (2021), pp. 1093–1098.
[11]
F. Battiston, G. Cencetti, I. Iacopini, V. Latora, M. Lucas, A. Patania, J.-G. Young, and G.
Petri. ‘Networks beyond pairwise interactions: structure and dynamics’. In: Physics Reports
874 (2020), pp. 1–92.
[12]
D. Beaini, S. Passaro, V. L’etourneau, W. L. Hamilton, G. Corso, and P. Lio’. ‘Directional
Graph Networks’. In: International Conference on Machine Learning. 2020.
[13]
Benchmark dataset for graph classification GitHub repository. https://github.com/
FilippoMB/Benchmark_dataset_for_graph_classification. Accessed: 2024-04-
14.
[14]
G. Bernárdez, L. Telyatnikov, M. Montagna, M. Papillon, M. Ferriol-Galmés, F. Baccini,
N. Miolane, M. Hajij, T. Papamarkou, G. Alzamzmi, T. Doster, T. Emerson, H. Kvinge, and
B. Rieck. ICML Topological Deep Learning Challenge 2024: Beyond the Graph Domain.
https://pyt-team.github.io/packs/challenge.html. Accessed: 2024-05-15. 2024.
[15]
F. M. Bianchi, C. Gallicchio, and A. Micheli. ‘Pyramidal Reservoir Graph Neural Network’.
In: vol. 470. Elsevier, 2022, pp. 389–404.
[16]
F. M. Bianchi, D. Grattarola, and C. Alippi. ‘Spectral clustering with graph neural networks
for graph pooling’. In: International Conference on Machine Learning. PMLR. 2020, pp. 874–
883.
[17]
F. M. Bianchi, D. Grattarola, L. Livi, and C. Alippi. ‘Hierarchical Representation Learning in
Graph Neural Networks With Node Decimation Pooling’. In: IEEE Transactions on Neural
Networks and Learning Systems 33.5 (2022), pp. 2195–2207. DOI: 10.1109/TNNLS.2020.
3044146.
[18]
C. Bick, E. Gross, H. A. Harrington, and M. T. Schaub. ‘What are higher-order networks?’
In: arXiv:2104.11329 (2021).
[19]
C. Bodnar, F. Frasca, N. Otter, Y. G. Wang, P. Lio’, G. Montúfar, and M. M. Bronstein.
‘Weisfeiler and Lehman Go Cellular: CW Networks’. In: Neural Information Processing
Systems. 2021.
[20]
C. Bodnar, F. D. Giovanni, B. P. Chamberlain, P. Lio, and M. M. Bronstein. ‘Neural Sheaf
Diffusion: A Topological Perspective on Heterophily and Oversmoothing in GNNs’. In:
Advances in Neural Information Processing Systems. 2022.
[21]
G. Bouritsas, F. Frasca, S. Zafeiriou, and M. M. Bronstein. ‘Improving Graph Neural Network
Expressivity via Subgraph Isomorphism Counting’. In: IEEE Transactions on Pattern Analysis
and Machine Intelligence 45.1 (2023), pp. 657–668. DOI: 10.1109/TPAMI.2022.3154319.
[22]
X. Bresson and T. Laurent. ‘Residual Gated Graph ConvNets’. In: ArXiv abs/1711.07553
(2017).
[23]
K. Choromanski, V. Likhosherstov, D. Dohan, X. Song, A. Gane, T. Sarlos, P. Hawkins, J.
Davis, A. Mohiuddin, L. Kaiser, D. Belanger, L. Colwell, and A. Weller. Rethinking Attention
with Performers. 2022. arXiv: 2009.14794 [cs.LG].
[24]
J. Clift, D. Doryn, D. Murfet, and J. Wallbridge. ‘Logic and the 2-Simplicial Transformer’. In:
International Conference on Learning Representations. 2020. URL: https://openreview.
net/forum?id=rkecJ6VFvr.
[25]
G. E. COOKE and R. L. PINNEY. Homology of Cell Complexes. Princeton University Press,
1967. ISBN: 9780691623139. URL: http://www.jstor.org/stable/j.ctt183pvgt
(visited on 16/05/2024).
[26]
G. Corso, L. Cavalleri, D. Beaini, P. Lio’, and P. Velickovic. ‘Principal Neighbourhood
Aggregation for Graph Nets’. In: ArXiv abs/2004.05718 (2020).
[27]
M. Defferrard, X. Bresson, and P. Vandergheynst. ‘Convolutional Neural Networks on Graphs
with Fast Localized Spectral Filtering’. In: Advances in Neural Information Processing
Systems. Ed. by D. Lee, M. Sugiyama, U. Luxburg, I. Guyon, and R. Garnett. Vol. 29. Curran
Associates, Inc., 2016. URL: https://proceedings.neurips.cc/paper_files/
paper/2016/file/04df4d434d481c5bb723be1b6df1ee65-Paper.pdf.
11

[28]
S. Deng, S. Wang, H. Rangwala, L. Wang, and Y. Ning. ‘Cola-gnn: Cross-location attention
based graph neural networks for long-term ili prediction’. In: Proceedings of the 29th ACM
International Conference on Information & Knowledge Management. 2020, pp. 245–254.
[29]
I. S. Dhillon, Y. Guan, and B. Kulis. ‘Weighted Graph Cuts without Eigenvectors A Multilevel
Approach’. In: IEEE Transactions on Pattern Analysis and Machine Intelligence 29.11 (2007),
pp. 1944–1957. DOI: 10.1109/TPAMI.2007.1115.
[30]
A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M.
Dehghani, M. Minderer, G. Heigold, S. Gelly, et al. ‘An image is worth 16x16 words:
Transformers for image recognition at scale’. In: arXiv preprint arXiv:2010.11929 (2020).
[31]
V. P. Dwivedi and X. Bresson. ‘A Generalization of Transformer Networks to Graphs’. In:
AAAI Workshop on Deep Learning on Graphs: Methods and Applications (2021).
[32]
V. P. Dwivedi, C. K. Joshi, A. T. Luu, T. Laurent, Y. Bengio, and X. Bresson. ‘Benchmarking
Graph Neural Networks’. In: Journal of Machine Learning Research 24.43 (2023), pp. 1–48.
URL: http://jmlr.org/papers/v24/22-0567.html.
[33]
V. P. Dwivedi, A. T. Luu, T. Laurent, Y. Bengio, and X. Bresson. ‘Graph Neural Networks with
Learnable Structural and Positional Representations’. In: International Conference on Learn-
ing Representations. 2022. URL: https://openreview.net/forum?id=wTTjnvGphYj.
[34]
E. Estrada and G. J. Ross. ‘Centralities in simplicial complexes. Applications to protein
interaction networks’. In: Journal of Theoretical Biology 438 (2018), pp. 46–60. ISSN:
0022-5193. DOI: https://doi.org/10.1016/j.jtbi.2017.11.003. URL: https:
//www.sciencedirect.com/science/article/pii/S0022519317305040.
[35]
W. Falcon and The PyTorch Lightning team. PyTorch Lightning. Version 1.4. Mar. 2019. DOI:
10.5281/zenodo.3828935. URL: https://github.com/Lightning-AI/lightning.
[36]
Y. Feng, H. You, Z. Zhang, R. Ji, and Y. Gao. ‘Hypergraph Neural Networks’. In: Proc. AAAI
33.01 (2019), pp. 3558–3565.
[37]
M. Fey and J. E. Lenssen. ‘Fast Graph Representation Learning with PyTorch Geometric’. In:
ICLR Workshop on Representation Learning on Graphs and Manifolds. 2019.
[38]
H. Gao and S. Ji. ‘Graph U-Nets’. In: Proceedings of the 36th International Conference on
Machine Learning. Ed. by K. Chaudhuri and R. Salakhutdinov. Vol. 97. Proceedings of Ma-
chine Learning Research. PMLR, Sept. 2019, pp. 2083–2092. URL: https://proceedings.
mlr.press/v97/gao19a.html.
[39]
Y. Gao, Z. Zhang, H. Lin, X. Zhao, S. Du, and C. Zou. ‘Hypergraph learning: Methods and
practices’. In: IEEE Transactions on Pattern Analysis and Machine Intelligence (2020).
[40]
L. Giusti, C. Battiloro, P. Di Lorenzo, S. Sardellitti, and S. Barbarossa. ‘Simplicial Attention
Networks’. In: arXiv preprint arXiv:2203.07485 (2022).
[41]
L. Giusti, C. Battiloro, L. Testa, P. Di Lorenzo, S. Sardellitti, and S. Barbarossa. ‘Cell attention
networks’. In: 2023 International Joint Conference on Neural Networks (IJCNN). IEEE. 2023,
pp. 1–8.
[42]
C. W. J. Goh, C. Bodnar, and P. Lio. ‘Simplicial attention networks’. In: arXiv preprint
arXiv:2204.09455 (2022).
[43]
L. J. Grady and J. R. Polimeni. Discrete calculus: Applied analysis on graphs for computa-
tional science. Vol. 3. Springer, 2010.
[44]
S. Gurugubelli and S. P. Chepuri. ‘SaNN: Simple Yet Powerful Simplicial-aware Neural
Networks’. In: The Twelfth International Conference on Learning Representations. 2024.
URL: https://openreview.net/forum?id=eUgS9Ig8JG.
[45]
A. Hagberg, P. J. Swart, and D. A. Schult. ‘Exploring network structure, dynamics, and
function using NetworkX’. In: (Jan. 2008). URL: https://www.osti.gov/biblio/
960616.
[46]
M. Hajij, K. Istvan, and G. Zamzmi. ‘Cell Complex Neural Networks’. In: NeurIPS Workshop
TDA and Beyond (2020).
[47]
M. Hajij, M. Papillon, F. Frantzen, J. Agerberg, I. AlJabea, R. Ballester, C. Battiloro, G.
Bernárdez, T. Birdal, A. Brent, et al. ‘TopoX: a suite of Python packages for machine learning
on topological domains’. In: arXiv preprint arXiv:2402.02441 (2024).
12

[48]
M. Hajij, G. Zamzmi, T. Papamarkou, A. Guzman-Saenz, T. Birdal, and M. T. Schaub.
‘Combinatorial complexes: bridging the gap between cell complexes and hypergraphs’. In:
2023 57th Asilomar Conference on Signals, Systems, and Computers. IEEE. 2023, pp. 799–
803.
[49]
M. Hajij, G. Zamzmi, T. Papamarkou, V. Maroulas, and X. Cai. ‘Simplicial Complex Repres-
entation Learning’. In: Machine Learning on Graphs (MLoG) Workshop at ACM International
WSD Conference (2022).
[50]
M. Hajij, G. Zamzmi, T. Papamarkou, N. Miolane, A. Guzmán-Sáenz, K. N. Ramamurthy,
T. Birdal, T. K. Dey, S. Mukherjee, S. N. Samaga, et al. ‘Topological Deep Learning: Going
Beyond Graph Data’. In: ().
[51]
K. Han, Y. Wang, H. Chen, X. Chen, J. Guo, Z. Liu, Y. Tang, A. Xiao, C. Xu, Y. Xu, et al.
‘A survey on vision transformer’. In: IEEE transactions on pattern analysis and machine
intelligence 45.1 (2022), pp. 87–110.
[52]
J. Hansen and R. Ghrist. ‘Toward a spectral theory of cellular sheaves’. In: Journal of Applied
and Computational Topology (2019).
[53]
J. Hansen and T. Gebhart. Sheaf Neural Networks. 2020. arXiv: 2012.06333 [cs.LG].
[54]
A. Hatcher. Algebraic Topology. Cambridge University Press, 2005.
[55]
D. Hernández Serrano, J. Hernández-Serrano, and D. Sánchez Gómez. ‘Simplicial degree in
complex networks. Applications of topological data analysis to network science’. In: Chaos
Solitons Fractals 137 (2020), pp. 109839, 21. ISSN: 0960-0779. DOI: 10.1016/j.chaos.
2020.109839. URL: https://doi.org/10.1016/j.chaos.2020.109839.
[56]
W. Hu, M. Fey, M. Zitnik, Y. Dong, H. Ren, B. Liu, M. Catasta, and J. Leskovec.
‘Open Graph Benchmark: Datasets for Machine Learning on Graphs’. In: Advances in
Neural Information Processing Systems. Ed. by H. Larochelle, M. Ranzato, R. Had-
sell, M. Balcan, and H. Lin. Vol. 33. Curran Associates, Inc., 2020, pp. 22118–22133.
URL: https : / / proceedings . neurips . cc / paper _ files / paper / 2020 / file /
fb60d411a5c5b72b2e7d3527cfc84fd0-Paper.pdf.
[57]
Z. Hu, J. Wang, S. Chen, and X. Du. ‘A Semi-supervised Framework with Efficient Feature
Extraction and Network Alignment for User Identity Linkage’. In: Database Systems for
Advanced Applications: 26th International Conference, DASFAA 2021, Taipei, Taiwan, April
11–14, 2021, Proceedings, Part II 26. Springer. 2021, pp. 675–691.
[58]
M. S. Hussain, M. J. Zaki, and D. Subramanian. ‘Edge-augmented graph transformers: Global
self-attention is enough for graphs’. In: arXiv preprint arXiv:2108.03348 (2021).
[59]
M. S. Hussain, M. J. Zaki, and D. Subramanian. ‘Global Self-Attention as a Replacement
for Graph Convolution’. In: Proceedings of the 28th ACM SIGKDD Conference on Know-
ledge Discovery and Data Mining (2021). URL: https://api.semanticscholar.org/
CorpusID:249375304.
[60]
E. Hwang, V. Thost, S. S. Dasgupta, and T. Ma. ‘An Analysis of Virtual Nodes in Graph
Neural Networks for Link Prediction (Extended Abstract)’. In: The First Learning on Graphs
Conference. 2022. URL: https://openreview.net/forum?id=dI6KBKNRp7.
[61]
J. J. Irwin, T. Sterling, M. M. Mysinger, E. S. Bolstad, and R. G. Coleman. ‘ZINC: A Free
Tool to Discover Chemistry for Biology’. In: Journal of Chemical Information and Modeling
52.7 (2012). PMID: 22587354, pp. 1757–1768. DOI: 10.1021/ci3001277. eprint: https:
//doi.org/10.1021/ci3001277. URL: https://doi.org/10.1021/ci3001277.
[62]
J. Jiang, Y. Wei, Y. Feng, J. Cao, and Y. Gao. ‘Dynamic Hypergraph Neural Networks.’ In:
IJCAI. 2019, pp. 2635–2641.
[63]
W. Jiang and J. Luo. ‘Graph neural network for traffic forecasting: A survey’. In: arXiv
preprint arXiv:2101.11174 (2021).
[64]
X. Jiang, L.-H. Lim, Y. Yao, and Y. Ye. ‘Statistical ranking and combinatorial Hodge theory’.
In: Mathematical Programming 127.1 (2011), pp. 203–244.
[65]
J. D. M.-W. C. Kenton and L. K. Toutanova. ‘Bert: Pre-training of deep bidirectional trans-
formers for language understanding’. In: Proceedings of naacL-HLT. Vol. 1. 2019, p. 2.
[66]
E.-S. Kim, W. Y. Kang, K.-W. On, Y.-J. Heo, and B.-T. Zhang. ‘Hypergraph attention networks
for multimodal learning’. In: Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition. 2020, pp. 14581–14590.
13

[67]
J. Kim, S. Oh, and S. Hong. ‘Transformers generalize deepsets and can be extended to
graphs & hypergraphs’. In: Advances in Neural Information Processing Systems 34 (2021),
pp. 28016–28028.
[68]
T. N. Kipf and M. Welling. ‘Semi-Supervised Classification with Graph Convolutional
Networks’. In: (2017). URL: https://openreview.net/forum?id=SJU4ayYgl.
[69]
D. Kreuzer, D. Beaini, W. L. Hamilton, V. L’etourneau, and P. Tossou. ‘Rethinking Graph
Transformers with Spectral Attention’. In: ArXiv abs/2106.03893 (2021).
[70]
J. Lee, I. Lee, and J. Kang. ‘Self-Attention Graph Pooling’. In: Proceedings of the 36th
International Conference on Machine Learning. Ed. by K. Chaudhuri and R. Salakhutdinov.
Vol. 97. Proceedings of Machine Learning Research. PMLR, Sept. 2019, pp. 3734–3743.
URL: https://proceedings.mlr.press/v97/lee19c.html.
[71]
L.-H. Lim. ‘Hodge Laplacians on graphs’. In: Siam Review 62.3 (2020), pp. 685–715.
[72]
K. Lin, L. Wang, and Z. Liu. ‘Mesh graphormer’. In: Proceedings of the IEEE/CVF interna-
tional conference on computer vision. 2021, pp. 12939–12948.
[73]
K. Maggs, C. Hacker, and B. Rieck. ‘Simplicial Representation Learning with Neural $k$-
Forms’. In: The Twelfth International Conference on Learning Representations. 2024. URL:
https://openreview.net/forum?id=Djw0XhjHZb.
[74]
A. Martino, A. Giuliani, and A. Rizzi. ‘(Hyper)Graph Embedding and Classification via
Simplicial Complexes’. In: Algorithms 12.11 (2019). ISSN: 1999-4893. DOI: 10.3390/
a12110223. URL: https://www.mdpi.com/1999-4893/12/11/223.
[75]
A. Martino and A. Rizzi. ‘(Hyper)graph Kernels over Simplicial Complexes’. In: Entropy
22.10 (2020). ISSN: 1099-4300. DOI: 10.3390/e22101155. URL: https://www.mdpi.
com/1099-4300/22/10/1155.
[76]
G. Mialon, D. Chen, M. Selosse, and J. Mairal. GraphiT: Encoding Graph Structure in
Transformers. 2021. arXiv: 2106.05667 [cs.LG].
[77]
E. Min, R. Chen, Y. Bian, T. Xu, K. Zhao, W. Huang, P. Zhao, J. Huang, S. Ananiadou, and
Y. Rong. ‘Transformer for graphs: An overview from architecture perspective’. In: arXiv
preprint arXiv:2202.08455 (2022).
[78]
E. Min, Y. Rong, T. Xu, Y. Bian, P. Zhao, J. Huang, D. Luo, K. Lin, and S. Ananiadou.
‘Masked Transformer for Neighhourhood-aware Click-Through Rate Prediction’. In: CoRR
abs/2201.13311 (2022). URL: https://arxiv.org/abs/2201.13311.
[79]
N. Miolane, M. Hajij, E. Paine, T. Papamarkou, J. Hoppe, J. Meissner, F. Frantzen, M.
Papillon, and USFCA-MSDS. pyt-team/TopoNetX: TopoNetX 0.0.2. Version 0.0.2. May 2023.
DOI: 10.5281/zenodo.7958504. URL: https://doi.org/10.5281/zenodo.7958504.
[80]
T. Papamarkou, T. Birdal, M. Bronstein, G. Carlsson, J. Curry, Y. Gao, M. Hajij, R. Kwitt,
P. Liò, P. D. Lorenzo, V. Maroulas, N. Miolane, F. Nasrin, K. N. Ramamurthy, B. Rieck,
S. Scardapane, M. T. Schaub, P. Veliˇckovi´c, B. Wang, Y. Wang, G.-W. Wei, and G. Zamzmi.
Position Paper: Challenges and Opportunities in Topological Deep Learning. 2024. arXiv:
2402.08871 [cs.LG].
[81]
M. Papillon, S. Sanborn, M. Hajij, and N. Miolane. ‘Architectures of topological deep
learning: A survey on topological neural networks’. In: arXiv preprint arXiv:2304.10031
(2023).
[82]
J. Park, Y. Hwang, M. Kim, M. K. Chung, G. Wu, and W. H. Kim. ‘Convolving Directed Graph
Edges via Hodge Laplacian for Brain Network Analysis’. In: Medical Image Computing
and Computer Assisted Intervention – MICCAI 2023. Ed. by H. Greenspan, A. Madabhushi,
P. Mousavi, S. Salcudean, J. Duncan, T. Syeda-Mahmood, and R. Taylor. Cham: Springer
Nature Switzerland, 2023, pp. 789–799. ISBN: 978-3-031-43904-9.
[83]
A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N.
Gimelshein, L. Antiga, A. Desmaison, A. Köpf, E. Yang, Z. DeVito, M. Raison, A. Tejani, S.
Chilamkurthy, B. Steiner, L. Fang, J. Bai, and S. Chintala. ‘PyTorch: an imperative style, high-
performance deep learning library’. In: Proceedings of the 33rd International Conference on
Neural Information Processing Systems. Red Hook, NY, USA: Curran Associates Inc., 2019.
[84]
A. Radford, K. Narasimhan, T. Salimans, I. Sutskever, et al. ‘Improving language understand-
ing by generative pre-training’. In: (2018).
[85]
K. N. Ramamurthy, A. Guzmán-Sáenz, and M. Hajij. ‘Topo-mlp: A simplicial network
without message passing’. In: ICASSP 2023-2023 IEEE International Conference on Acous-
tics, Speech and Signal Processing (ICASSP). IEEE. 2023, pp. 1–5.
14

[86]
L. Rampasek, M. Galkin, V. P. Dwivedi, A. T. Luu, G. Wolf, and D. Beaini. ‘Recipe for a
General, Powerful, Scalable Graph Transformer’. In: ArXiv abs/2205.12454 (2022).
[87]
O. Rioul and M. Vetterli. ‘Wavelets and signal processing’. In: IEEE Signal Proc. Mag. 8.4
(1991), pp. 14–38. DOI: 10.1109/79.91217.
[88]
T. M. Roddenberry, M. T. Schaub, and M. Hajij. ‘Signal processing on cell complexes’. In:
Proc. IEEE ICASSP (2022).
[89]
Y. Rong, Y. Bian, T. Xu, W. Xie, Y. Wei, W. Huang, and J. Huang. ‘Self-supervised graph
transformer on large-scale molecular data’. In: Advances in Neural Information Processing
Systems 33 (2020), pp. 12559–12571.
[90]
S. Sardellitti and S. Barbarossa. ‘Topological Signal Representation and Processing over Cell
Complexes’. In: arXiv preprint arXiv:2201.08993 (2022).
[91]
S. Sardellitti, S. Barbarossa, and L. Testa. ‘Topological Signal Processing over Cell Com-
plexes’. In: Proceeding IEEE Asilomar Conference. Signals, Systems and Computers (2021).
[92]
M. T. Schaub, Y. Zhu, J.-B. Seby, T. M. Roddenberry, and S. Segarra. ‘Signal processing on
higher-order networks: Livin’on the edge... and beyond’. In: Signal Processing 187 (2021),
p. 108149.
[93]
M. T. Schaub, A. R. Benson, P. Horn, G. Lippner, and A. Jadbabaie. ‘Random Walks
on Simplicial Complexes and the Normalized Hodge 1-Laplacian’. In: SIAM Review 62.2
(2020), pp. 353–391. DOI: 10.1137/18M1201019. eprint: https://doi.org/10.1137/
18M1201019. URL: https://doi.org/10.1137/18M1201019.
[94]
H. Shirzad, A. Velingker, B. Venkatachalam, D. J. Sutherland, and A. K. Sinop. ‘Exphormer:
Sparse transformers for graphs’. In: International Conference on Machine Learning. PMLR.
2023, pp. 31613–31632.
[95]
A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and
I. Polosukhin. ‘Attention is all you need’. In: Advances in neural information processing
systems 30 (2017).
[96]
P. Velickovic, G. Cucurull, A. Casanova, A. Romero, P. Lio’, and Y. Bengio. ‘Graph Attention
Networks’. In: ArXiv abs/1710.10903 (2017).
[97]
P. Virtanen, R. Gommers, T. E. Oliphant, M. Haberland, T. Reddy, D. Cournapeau, E.
Burovski, P. Peterson, W. Weckesser, J. Bright, S. J. van der Walt, M. Brett, J. Wilson, K. J.
Millman, N. Mayorov, A. R. J. Nelson, E. Jones, R. Kern, E. Larson, C. J. Carey, ˙I. Polat,
Y. Feng, E. W. Moore, J. VanderPlas, D. Laxalde, J. Perktold, R. Cimrman, I. Henriksen,
E. A. Quintero, C. R. Harris, A. M. Archibald, A. H. Ribeiro, F. Pedregosa, P. van Mulbregt,
and SciPy 1.0 Contributors. ‘SciPy 1.0: Fundamental Algorithms for Scientific Computing in
Python’. In: Nature Methods 17 (2020), pp. 261–272. DOI: 10.1038/s41592-019-0686-2.
[98]
M. L. Wachs. ‘Poset topology: tools and applications’. In: arXiv preprint math/0602226
(2006). arXiv: math/0602226 [math.CO].
[99]
J. Wang, K. Ding, L. Hong, H. Liu, and J. Caverlee. ‘Next-item recommendation with
sequential hypergraphs’. In: Proceedings of the 43rd international ACM SIGIR conference on
research and development in information retrieval. 2020, pp. 1101–1110.
[100]
M. Wang, D. Zheng, Z. Ye, Q. Gan, M. Li, X. Song, J. Zhou, C. Ma, L. Yu, Y. Gai, T. Xiao,
T. He, G. Karypis, J. Li, and Z. Zhang. ‘Deep Graph Library: A Graph-Centric, Highly-
Performant Package for Graph Neural Networks’. In: arXiv preprint arXiv:1909.01315
(2019).
[101]
Z. Wu, P. Jain, M. Wright, A. Mirhoseini, J. E. Gonzalez, and I. Stoica. ‘Representing
long-range context for graph neural networks with global attention’. In: Advances in Neural
Information Processing Systems 34 (2021), pp. 13266–13279.
[102]
R. Xiong, Y. Yang, D. He, K. Zheng, S. Zheng, C. Xing, H. Zhang, Y. Lan, L. Wang, and
T. Liu. ‘On Layer Normalization in the Transformer Architecture’. In: Proceedings of the
37th International Conference on Machine Learning. Ed. by H. D. III and A. Singh. Vol. 119.
Proceedings of Machine Learning Research. PMLR, 13–18 Jul 2020, pp. 10524–10533. URL:
https://proceedings.mlr.press/v119/xiong20b.html.
[103]
K. Xu, W. Hu, J. Leskovec, and S. Jegelka. ‘How powerful are graph neural networks?’ In:
arXiv preprint arXiv:1810.00826 (2018).
[104]
C. Ying, T. Cai, S. Luo, S. Zheng, G. Ke, D. He, Y. Shen, and T.-Y. Liu. ‘Do Transformers
Really Perform Bad for Graph Representation?’ In: Neural Information Processing Systems.
2021.
15

[105]
Z. Ying, J. You, C. Morris, X. Ren, W. Hamilton, and J. Leskovec. ‘Hierarchical
Graph Representation Learning with Differentiable Pooling’. In: Advances in Neural In-
formation Processing Systems. Ed. by S. Bengio, H. Wallach, H. Larochelle, K. Grau-
man, N. Cesa-Bianchi, and R. Garnett. Vol. 31. Curran Associates, Inc., 2018. URL:
https : / / proceedings . neurips . cc / paper _ files / paper / 2018 / file /
e77dbaf6759253c7c6d0efc5690369c7-Paper.pdf.
[106]
J. Zhang, H. Zhang, C. Xia, and L. Sun. ‘Graph-bert: Only attention is needed for learning
graph representations’. In: arXiv preprint arXiv:2001.05140 (2020).
[107]
R. Zhang, Y. Zou, and J. Ma. ‘Hyper-SAGNN: a self-attention based graph neural network
for hypergraphs’. In: International Conference on Learning Representations (ICLR). 2020.
[108]
L. Zhao, W. Jin, L. Akoglu, and N. Shah. ‘From Stars to Subgraphs: Uplifting Any GNN
with Local Structure Awareness’. In: ArXiv abs/2110.03753 (2021).
[109]
C. Zhou, X. Wang, and M. Zhang. ‘Facilitating Graph Neural Networks with Random Walk
on Simplicial Complexes’. In: Thirty-seventh Conference on Neural Information Processing
Systems. 2023. URL: https://openreview.net/forum?id=H57w5EOj6O.
[110]
C. Zhou, R. Yu, and Y. Wang. On the Theoretical Expressive Power and the Design Space of
Higher-Order Graph Transformers. 2024. arXiv: 2404.03380 [cs.LG].
[111]
J. Zhou, G. Cui, S. Hu, Z. Zhang, C. Yang, Z. Liu, L. Wang, C. Li, and M. Sun. ‘Graph neural
networks: A review of methods and applications’. In: AI Open 1 (2020), pp. 57–81.
A
Architecture details and experiments
Pairwise attention transformer layer. Following the usual prenorm design [102], the output of
the cellular transformer layer for a specific rank kt is denoted Xkt,l+1 and computed in six steps, as
follows:
X1
kt,l = LayerNormkt(Xkt,l),
X1
ks,l = LayerNormks(Xks,l) for each ks in the tensor diagram,
X2
ks→kt,l = A•
ks→kt(X1
kt,l, X1
ks,l) for each ks in the tensor diagram,
X3
ks→kt,l = Dropout(X2
ks→kt,l) for each ks in the tensor diagram,
X4
kt,l = Xkt,l +
X
ks
X3
ks→kt,l
X5
kt,l = LayerNorm(X4
kt,l),
X6
kt,l = Dropout(FFN2(Dropout(ReLU(FFN1(X5
kt,l))))),
Xkt,l+1 = X4
kt,l + X6
kt,l,
The LayerNorm is unique for each dimension d and each layer l.
General attention transformer layer. Similarly to the pairwise attention transformer layer, the
general attention transformer layer introduced in section 4.2.2 performs the following steps:
X1
l = LayerNorm(Xl),
X2
l = A•
g(X1
l ),
X3
l = Dropout(X2
l ),
X4
l = Xl + X3
l ,
X5
l = LayerNorm(X4
l ),
X6
l = Dropout(FFN2(Dropout(ReLU(FFN1(X5
l ))))),
Xl+1 = X4
l + X6
l .
Training details. All experiments use a Cosine Annealing scheduler with linear warmup, an
AdamW optimizer with ϵ = 1−8, (µ1, µ2) = (0.9, 0.999) and variable peak learning rate, and a
gradient clipping norm of 5. All our transformer architectures, after the transformer layers, use a fully-
connected readout whose dropout and number of hidden layers is fixed for each set of experiments,
16

followed by a global add pool layer over all the vertex signals to perform prediction or regression.
The fully-connected block begins with a number of neurons equivalent to the hidden dimension of the
transformers and concludes with a number of neurons corresponding to the network’s output number.
Throughout the block, each hidden layer has half as many neurons as its predecessor.
Architecture details of the cellular transformer. Table 3 presents the hyperparameters of the
cellular transformer for the three datasets GCB, ogbg-molhiv, and ZINC.
Table 3: Cellular transformer parameters for the experiments of the three datasets. The attention type
and the positional encodings vary depending on the experiment configuration.
GCB
ogbg-molhiv
ZINC
#Layers
12
12
12
Hidden dimension (dh)
80
768
96
FFN inner-layer dimension
80
768
96
# Attention heads (m)
8
32
8
Hidden dimension of each head
10
24
12
Attention dropout
0.1
0.1
0.1
Embedding dropout
0.0
0.0
0.0
Readout MLP dropout
0.1
0.0
0.0
Max epochs
350
200
10000
Peak learning rate
3e−4
2e−4
2e−4
Batch size
256
1024
256
Warmup epochs
35
20
1000
Weight decay
0.01
1e−5
0.01
# hidden layers readout MLP
1
3
2
Signals on cells. All graphs in the three datasets contain at least discrete signals for the vertices. For
the GCB dataset, we associate to each edge a signal corresponding to concatenating the signals of its
endpoints. For the ogbg-molhiv and ZINC datasets, the edges contain signals, so we do not change
them. As a first step in the transformer architecture, we learn an embedding for the discrete features.
For the edge features in GCB, each vertex feature is embedded individually. For the three datasets,
signals on the 2-cells are given by sum of the embedded signals of their vertices.
GCB architectures. The first six models in table 1 are all graph neural networks with different graph
pooling layers and common architecture composition given by MP(32)-Pool-MP(32)-Pool-MP(32)-
GlobalPool-Dense(Softmax), where MP(32) is a Chebyshev convolutional layer [27] with 32 hidden
units, Pool is a pooling message passing layer, GlobalPool is a global pool layer used as readout, and
Dense(Softmax) is a dense layer with softmax activation. Skip connections were used. The other
state-of-the-art models consist of models proposed in [74, 75].
B
Mathematical details and examples
Cell complexes. A definition of cell complexes in the context of algebraic topology can be found
in [54]. In brief, a cell complex is a topological space X that can be decomposed as a union of
disjoint subspaces called cells, where each cell σ is homeomorphic to Rk for some integer k ≥0,
called the rank of σ. Additionally, for every cell σ, the difference σ \ σ is a union of finitely many
cells of lower rank, where σ denotes the closure of σ. The dimension of a finite cell complex is the
maximum of the ranks of its cells. The set of cells of rank k in a cell complex X is denoted by Xk.
The n-skeleton of X is the cell complex spanned by X0, . . . , Xn, for 0 ≤n ≤dim X.
A characteristic map for a cell σ of rank k is a map from the Euclidean unit closed ball of dimension
k into σ whose restriction to the open ball is a homeomorphism. A cell complex is called regular
if each cell σ admits a characteristic map which is itself a homeomorphism from the closed ball
to σ. For example, a decomposition of a circle as the union of a 0-cell and a 1-cell is not regular.
Geometric realizations of abstract simplicial complexes are regular cell complexes. Cell complexes
generalize simplicial complexes as their cells are not constrained to be simplices.
17

Boundary and coboundary operators. For each rank k, the incidence matrix Bk of a cell complex
X, as defined in section 3, is the matrix of the boundary operator Ck(X) →Ck−1(X), where Ck(X)
is the R-vector space spanned by the set Xk of k-cells of X. The transpose BT
k is the matrix of the
coboundary operator Ck−1(X) →Ck(X) on the dual vector spaces. Thus the matrix BT
k encodes
reverse incidence relations from (k −1)-cells to k-cells.
Neighborhood matrices. We jointly call neighborhood matrices the (signed or non-signed) incidence
matrices, Laplacians, and adjacency matrices. Laplacians are extensively used in graph learning
[32, 82] and signal processing [4, 91]. For a cell complex X of arbitrary dimension, the k-th Hodge
Laplacian is defined in terms of incidence matrices as Lk = BT
k Bk + Bk+1BT
k+1, where Bk = 0
if k = 0 or k > dim X. The upper and lower Laplacians are, respectively, Lup
k = Bk+1BT
k+1 and
Ldown
k
= BT
k Bk. Therefore, for a 2-dimensional cell complex,
L0 = B1BT
1 ,
L1 = BT
1 B1 + B2BT
2 ,
L2 = BT
2 B2.
The lower Laplacian BT
k Bk can be interpreted as the matrix of the composite of the boundary
operator Ck(X) →Ck−1(X) with the adjoint of the cobundary operator Ck−1(X) →Ck(X) through
the isomorphism Ck(X) ∼= R|Xk| determined by the given order of the set Xk of k-cells of X. The
upper Laplacian is the composite of the adjoint of the k-coboundary with the (k + 1)-boundary.
Two distinct k-cells are upper adjacent if there exists at least one (k + 1)-cell incident to both. We
denote upper adjacency by σi ∼U σj. Similarly, two distinct k-cells are lower adjacent if they share
at least one common incident (k −1)-cell. We denote lower adjacency by σi ∼L σj [34]. The upper
and lower adjacency relations are stored using adjacency matrices Aup
k and Adown
k
. These are square
matrices of size |Xk| = dim Ck(X), which are related to non-signed incidence matrices as follows:
Aup
k is obtained from Ik+1IT
k+1 by replacing its diagonal entries with zeros, and Adown
k
is obtained
from IT
k Ik analogously.
Details on LapPE for graphs and their HodgeLapPE extension. A popular positional encoding
for graph transformers is graph Laplacian eigenvectors (LapPE) [32]. Let 0 ≤˜λ1 ≤· · · ≤˜λ˜p ≤2
be the distinct eigenvalues of the normalized graph Laplacian ˜L0 for a graph G = (V, E) with
V = {v1, . . . , vn}. As ˜L0 is a real symmetric matrix,
˜λi =
min
∥x∥=1
x∈⟨˜Vi−1⟩⊥
xT ˜L0x =
min
∥x∥=1
x∈⟨˜Vi−1⟩⊥
∥BT
1 (D+)1/2x∥2 =
min
∥x∥=1
x∈⟨˜Vi−1⟩⊥
X
(vi,vj)∈E
 
xi
p
d(vi)
−
xj
p
d(vj)
!2
,
(5)
Figure 4: BSPe positional en-
coding of length three for a
cell complex with two 2-cells.
To generate a colour from the
positional encoding, we nor-
malize each coordinate of the
positional encodings to the
[0, 1] range, generating nor-
malized RGB colours. Note
that close cells are assigned
similar colours.
where d(v) is the degree of v and ˜Vi−1 is a set of linearly independent
eigenvectors of ˜λ1, . . . , ˜λi−1. The minimum attained for any unit-
norm eigenvector of ˜λi is unique up to a sign for non-degenerate
eigenvalues. Eigenvectors corresponding to small eigenvalues give
a gradient on the graph. The close vertices (with respect to the
adjacency) have close eigenvector coordinates, thus, represent a
relative position encoding of the vertices of the graph. Thus, LapPE
assigns to each vertex vi a vector LapPE(vi) = (e1
i , . . . , ek
i ), where
{ej
i | j = 1, . . . , k} are eigenvectors of the k smallest eigenvalues
counting multiplicities, where k is a hyperparameter. To avoid
the sign ambiguity for normalized eigenvectors of non-degenerate
eigenvalues, positional encodings are multiplied by random signs
at each iteration during training, with the objective of making the
neural network invariant to this ambiguity.
HodgeLapPE is the extension of LapPE to cells of arbitrary rank
using the unnormalized Hodge Laplacian. However, without nor-
malizing the Hodge Laplacian, we can obtain eigenvalues that are
not small enough to make the previous Rayleigh quotient (5) mean-
ingful even in the case of simple simplicial complexes. With the
unnormalized version we also do not obtain straightforward formulae that require close cells to have
close coordinates in the eigenvector. The following examples illustrate the previous drawbacks.
18

Example B.1 (Large eigenvalues extending LapPE to simplicial complexes using the unnormalized
Hodge Laplacian). Take a triangle graph G with vertices v1, v2, v3. The eigenvalues of the unnor-
malized Hodge Laplacian L1 given the orientation induced by the ordering of the vertices are 0 and 3.
For λ = 3, an eigenvector of unit length is (1/
√
2) (1, 0, 1), that assigns the same coordinates to
two of the edges and a different coordinate to the other one, although the three edges are clearly
neighborhoods and must have similar representations, making eigenvectors useless in this case.
Example B.2 (Rayleigh quotient of the Hodge Laplacian does not produce a gradient of arbitrary
dimensional cells). If we want to produce positional encodings for k-cells using the k-th Hodge
Laplacian, we obtain
λi =
min
∥x∥=1
x∈⟨Si−1⟩⊥
xT Lkx =
min
∥x∥=1
x∈⟨Si−1⟩⊥
∥Bkx∥2 + ∥BT
k+1x∥2
=
min
∥x∥=1
x∈⟨Si−1⟩⊥
(1 −1(k = 0))
X
γ∈Si−1
 X
σj∈Si
γ<σj
(s(γ, σj)xj)
!2
+ (1 −1(d = dim(X)))
X
γ∈Si+1
 X
σj∈Si
σj<γ
(s(σj, γ)xj)
!2
,
(6)
where γ < σ means that γ is a proper face of σ and s(γ, σ) is the value of γ in the boundary of σ.
In this case, taking the previous triangle graph G with three vertices, the eigenvalue λ = 0 has a
unit eigenvector (1/
√
3) (−1, −1, 1), that assigns to two of the edges the same coordinates and to
the third edge the opposite coordinate, though being the three of them neighbors since they are all
adjacent. We refer to [4] for more information about Equation (6) in the context of Topological Signal
Processing.
C
Positional encoding details
In this section, we extend the details about the positional encodings built in section 4.4.
C.1
Random walks on cell complexes
Let X be a regular cell complex. We describe a random walk on the set of k-cells of X. To this end,
we first recall that the number of upper and lower adjacent k-cells of a given cell σ ∈Xk are named
respectively the (0, k + 1)-upper and (0, k −1)-lower degree of σ [55],
deg0,k+1
U
(σ) = #{σ′ ∈Xk : σ ∼U σ′};
deg0,k−1
L
(σ) = #{σ′ ∈Xk : σ ∼L σ′}.
On the one hand, for each k ≥0, we define a random upper k-walk based on upper adjacencies
of the k-cells of X. At each step, we move from a k-cell σi to any upper adjacent k-cell σj with
probability proportional to the number of (k + 1)-cells in common. To describe this process, we
consider a weighted undirected graph Gup
k , whose vertices are the k-cells of X and the weight of each
edge (σi, σj) is the number of (k + 1)-cells whose closure contains both cells (if a k-cell is not upper
adjacent to any k-cell, we draw a loop on the corresponding vertex with weight equal to 1). Thus, the
upper random k-walk is described by the left stochastic matrix RWup
k = wAup
k (Dup
k )−1, where wAup
k
and Dup
k denote the weighted adjacency and diagonal weighted degree matrices of the graph Gup
k .
On the other hand, for each k > 0, we define a random lower k-walk through lower adjacencies
of the k-cells of X. In this case, we move from a k-cell σi to any lower adjacent k-cell σj with
probability proportional to the number of (k −1)-faces in common. As in the previous case, the
random lower walk can be described as a random walk on a weighted graph Gdown
k
, whose vertices
are the k-cells of X and the weight of an edge (σi, σj) is set as the number of (k −1)-cells that both
cells have in common (as before, if a k-cell is not lower adjacent to any other k-cell, then we draw
a loop on it with weight equal to 1). The lower random k-walk is described by the left stochastic
matrix RWdown
k
= wAdown
k
(Ddown
k
)−1, where wAdown
k
and Ddown
k
denote the corresponding weighted
adjacency and diagonal weighted degree matrices of the graph Gdown
k
. The matrices wAup
k and wAdown
k
19

(a) RWBSPe random walk possible transitions from
the upper-left edge.
(b) RWPe random walk possible transitions from the
upper-left edge.
Figure 5: Differences between RWBSPe and RWPe random walks. RWBSPe random walks can
jump from a cell to all its incident and coincident cells, while RWPe random walks can jump from a
cell to all its upper and lower adjacent cells.
correspond respectively to the upper and lower adjacency matrices Aup
k and Adown
k
with the diagonal
entries in null rows replaced with 1.
We can combine both processes to obtain a random walk in which information flows through upper
and lower adjacencies, in line with [93]. The idea is as follows: if we are in a k-cell σ with upper and
lower adjacent k-cells, we take a step with equal probability via either upper or lower connections.
If σ has upper adjacent k-cells but not lower ones, we move following the random upper k-walk
process, and vice versa. Lastly, if σ has neither upper nor lower connections, then we do not move.
The left stochastic matrix that describes the random k-walk is defined for σi, σj ∈Xk by
(RWk)σiσj =









1
2(RWup
k )σiσj + 1
2(RWdown
k
)σiσj
if deg0,k+1
U
(σj) ̸= 0 and deg0,k−1
L
(σj) ̸= 0
(RWup
k )σiσj
if deg0,k+1
U
(σj) ̸= 0 and deg0,k−1
L
(σj) = 0
(RWdown
k
)σiσj
if deg0,k+1
U
(σj) = 0 and deg0,k−1
L
(σj) ̸= 0
1(i = j)
if deg0,k+1
U
(σj) = deg(0,k−1)
L
(σj) = 0.
An example of the differences between transitions from an edge in the random walks described in
this section and the barycentric subdivision random walks of RWBSPe are described in fig. 5.
C.2
Topological Slepians
Let X be an oriented regular two-dimensional cell complex, with Hodge Laplacians L1 and L2. Hodge
Laplacians admit a Hodge decomposition [71], such that the k-cochain space can be decomposed as
Ck(X, R) = im
 BT
k

⊕im
 Bk+1

⊕ker
 Lk

,
(7)
where L denotes direct sum of vector spaces, and ker(−) and im(−) are the kernel and image spaces
of a matrix, respectively. The k-cochains can be represented by means of eigenvector bases of the
corresponding Hodge Laplacian. Using the decomposition Lk = UkΛkUT
k , the k-th Cellular Fourier
Transform (k-CWFT) is the projection of a k-cochain onto the eigenvectors of Lk [90]:
bXk = UT
k Xk.
(8)
We refer to the eigenvalue set Bk of the k-CWFT as the frequency domain. An immediate consequence
of the Hodge decomposition in (7) is that the eigenvectors belonging to im(Ld
k) are orthogonal to
those belonging to im(Lu
k), for all k = 1, . . . , K −1. Therefore, the eigenvectors of Lk are given by
the union of the eigenvectors of Lu
k, the eigenvectors of Ld
k, and the kernel of Lk. We now introduce
two localization operators acting onto a k-cell concentration set (thus, onto the topological domain),
say Sk ⊂X k, and onto a spectral concentration set (thus, onto the frequency domain), say Fk ⊂Bk,
respectively. In particular, we define a cell-limiting operator onto the k-cell set Sk as
CSk = diag(1S) ∈R|Xk|×|Xk|,
(9)
where 1Sk ∈R|Xk| is a vector having ones in the index positions specified in Sk, and zero otherwise;
and diag(z) denotes a diagonal matrix having z on the diagonal. A k-cochain Xk is perfectly
localized onto the set Sk if CSkXk = Xk. Similarly, the frequency limiting operator is defined as
BFk = U diag(1Fk) UT ∈R|Xk|×|Xk|,
(10)
20

that can be interpreted as a band-pass filter over the frequency set Fk. A k-cochain is perfectly
localized over the bandwidth Fk if BFkXk = Xk. The matrices in (9) and (10) are proper projection
operators.
At this point, k-topological Slepians are defined as orthonormal vectors that are maximally concen-
trated over the k-cell set Sk, and perfectly localized onto the bandwidth Fk:
si
k = arg max
si
k
∥CSksi
k∥2
2
subject to ∥si
k∥= 1,
BFksi
k = si
k,
(11)
⟨si
k, sj
k⟩= 0,
j = 1, . . . , i −1, if i > 1,
for i = 1, . . . , |Xk|. As shown in [7], the solution of problem (11) is given by the eigenvectors of the
matrix operator BFCFBFk, i.e.,
BFkCSkBFksi
k = λi
ksi
k.
(12)
It is then clear that the maximum number of k-topological Slepians per each pair of concentration
sets {Sk, Fk} is given by rank{BFkCSkBFk}. For this reason and to have a more exhaustive
representation of structural and topological properties of the complex [7], we choose a sequence
of M concentration sets {Sk,i, Fk,i}M
i=1 and concatenate the corresponding Slepians. From (7), the
frequency domain can be partitioned into two separate sets: (i) the set of eigenvalues of Ldown
k
, say
Fd
k, and (ii) the set of eigenvalues of Lup
k , say Fu
k . For the same reason, we can define two distinct
sequences of (not necessarily disjoint) k-cell concentration sets: (i) Kd
k sets based on lower adjacency
(encoded by Ldown
k
), that we refer to as lower sets; (ii) Ku
k sets based on upper adjacency (encoded
by Lup
k ), that we refer to as upper sets. It is then natural to associate the frequency concentration
set Fu
k to each upper k-cell concentration set, and the frequency concentration set Fd
k to each lower
k-cell concentration set. Finally, suppose that the kernel of the Laplacian Lk is not empty. In
that case, topological Slepians can be combined with the harmonic eigenvectors of Lk, i.e. the
eigenvectors associated with the zero eigenvalues, resulting in a mixed positional encoding strategy.
The number of topological Slepians can then be controlled either by tuning Kd
k and Ku
k , or by taking
just the top Slepians per each pair of concentration sets. In this work, we choose the upper and
lower k-cell concentration sets as the adjacency and coadjacency of each k-cell including the k-cell
itself, respectively, obtaining |Xk| pairs of concentration sets for each rank k. In this way, we are
also sure that the obtained set of topological Slepians comes with theoretical guarantees, i.e., it is an
(A, B)-frame [7, 87] In general, the choice of the localization sets is an interesting directions, that
could be easily combined with prior knowledge about the complex and the data at hand.
D
Numerical results
The set of results of our experiments for all the combinations of attention mechanisms and positional
encodings is reported in table 4.
E
Implementation and hardware resources
Implementation was performed mainly using the TopoNetX [79] library for cell complex represent-
ation and manipulation, PyTorch [83] for the deep learning pipelines, PyTorch Geometric [37]
for feature pooling and dataset loading, Deep Graph Library [100] and Scipy [97] for sparse
tensor algebraic operations and sparse tensor representation and manipulation, NetworkX [45] for
graph manipulation, and PyTorch Lightning [35] as a top layer for experimentation in PyTorch.
The most critical pieces of software implemented in this project have been the DataLoader and
the collate function to batch cell complexes. The DataLoader is implemented in the class
TopologicalTransformerDataLoader. The collate function is implemented in the function
collate, both inside the file src/datasets/cell_dataloader.py. The collate function cre-
ates a cell complex batch by performing the disjoint union of cell complexes. As an input, the
collate function receives an object with the signals for each cell, the neighborhood matrices used in
the transformer architecture as bias N in a sparse format, and other data needed by the experiments
such as the label of the dataset and the positional encodings. The neighborhood matrices are batched
21

Table 4: Results of experiments with cellular transformers in the three datasets GCB, ogbg-molhiv,
and ZINC. Reported scores are test accuracy (↑), test AUC-ROC (↑), and test MAE (↓), respectively.
Best result, second best result, and third best result are highlighted in boldface green, blue, and
orange, respectively.
Datasets →
GCB
ogbg-molhiv
ZINC
Attention type
Positional encodings
Accuracy (↑)
AUC-ROC (↑)
MAE (↓)
As
g
BSPe
0.7516 ± 0.0102
0.7681
0.0840
As
ks→kt
BSPe
0.7400 ± 0.0123
0.7946
0.0934
Ad
ks→kt
RWBSPe
0.6295 ± 0.0263
0.7136
0.3451
As
g
zeros
0.7453 ± 0.0086
0.7362
0.1239
Ac
ks→kt
HodgeLapPE
0.7337 ± 0.0103
0.7658
0.0831
Ad
ks→kt
HodgeLapPE
0.6179 ± 0.0286
0.7670
0.4009
As
ks→kt
zeros
0.7453 ± 0.0123
0.7008
0.1416
As
ks→kt
HodgeLapPE
0.7421 ± 0.0191
0.7586
0.0852
Ad
g
BSPe
0.5989 ± 0.0219
0.7177
0.3983
As
g
RWBSPe
0.7442 ± 0.0140
0.7082
0.1296
Ac
ks→kt
zeros
0.7484 ± 0.0061
0.7435
0.1090
Ad
ks→kt
BSPe
0.6095 ± 0.0234
0.7328
0.4096
Ad
ks→kt
zeros
0.6400 ± 0.0196
0.5952
0.4348
Ac
ks→kt
TopoSlepiansPE
0.7421 ± 0.0094
0.7565
0.1202
Ac
ks→kt
RWPe
0.7379 ± 0.0102
0.7338
0.0833
Ad
g
RWBSPe
0.6200 ± 0.0423
0.6949
0.3586
Ad
g
TopoSlepiansPE
0.6179 ± 0.0276
0.6954
0.5175
Ac
ks→kt
BSPe
0.7347 ± 0.0136
0.7784
0.0833
Ac
ks→kt
RWBSPe
0.7442 ± 0.0108
0.7321
0.0802
As
g
HodgeLapPE
0.7432 ± 0.0112
0.7032
0.0824
As
g
RWPe
0.7442 ± 0.0136
0.7343
0.1051
Ad
g
zeros
0.6105 ± 0.0221
0.6679
0.4526
Ad
ks→kt
RWPe
0.6463 ± 0.0406
0.6944
0.3843
As
ks→kt
RWBSPe
0.7389 ± 0.0144
0.7058
0.1210
Ad
ks→kt
TopoSlepiansPE
0.6463 ± 0.0214
0.6690
0.4527
Ad
g
HodgeLapPE
0.5811 ± 0.0250
0.7243
0.3981
As
ks→kt
TopoSlepiansPE
0.7505 ± 0.0054
0.7111
0.1452
As
g
TopoSlepiansPE
0.7432 ± 0.0107
0.7192
0.1303
As
ks→kt
RWPe
0.7505 ± 0.0054
0.7288
0.0973
Ad
g
RWPe
0.6368 ± 0.0282
0.6836
0.3770
into a new sparse block matrix, taking into account that different cell complexes may have different
dimensions and thus not all the cell complexes have the same neighborhood matrices. Currently, the
collate function supports adjacency and boundary matrices, although the function can be extended
easily. Signals, positional encodings, and labels are simply concatenated. To keep track of which
signals and positional encodings correspond to each of the individual cell complex, we also return,
for each dimension, a vector of size equal to total number of cells of that dimension in the disjoint
union which indicates to which cell complex belong each signal or positional encoding.
The experiments were executed on a server with an AMD EPYC 7452 (128) @ 2.350GHz CPU,
503GiB of RAM memory, x4 PNY Nvidia RTX 6000 Ada Generation 48GB GPUs, and Ubuntu
22.04.4 LTS with the 6.5.0-28-generic Linux kernel. Each experiment was executed on a separated
GPU device, using 12 workers per experiment.
22

F
Licenses
The GCB dataset is distributed under a MIT license. ogbg-molhiv is distributed under a MIT
license. The ZINC dataset is free to use and download and its license can be found at https:
//wiki.docking.org/index.php?title=UCSF_ZINC_License.
23

