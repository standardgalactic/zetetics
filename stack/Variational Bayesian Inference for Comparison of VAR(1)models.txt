Variational Bayesian Inference for
Comparison of VAR(1) models
Adrian James Houghton
Thesis submitted for the degree of
Doctor of Philosophy
School of Mathematics and Statistics
University of Newcastle upon Tyne
Newcastle upon Tyne
United Kingdom
January 2009

Abstract
Suppose that we wish to determine which models in a candidate set are most likely to
have given rise to a set of observed data. Then, it is well-established that, from a Bayesian
viewpoint, evaluation of the marginal likelihood for each candidate is a crucial step to
this end. For the purposes of model comparison, this will enable subsequent computation
of both Bayes’ factors and posterior model probabilities. Given its evident signiﬁcance
in this area, it is thus regrettable that analytic calculation of the marginal likelihood is
often not possible. To tackle this problem, one recent addition to the literature is the
variational Bayesian approach.
In this thesis, it is seen that variational Bayes provides eﬃcient, accurate approximations
to both the marginal likelihood and the parameter posterior distribution, conditioned on
each model. In particular, the theory is applied to ranking sparse, vector autoregressive
graphical models of order 1 in both the zero and non-zero mean case. That is, our primary
aim is to estimate the unknown sparsity structure of the autoregressive matrix in the
process. Moreover, approximate, marginal posterior information about the coeﬃcients of
this matrix is also of interest. To enable rapid exploration of higher-dimensional graphical
spaces, a Metropolis-Hastings algorithm is presented so that a random walk can be made
between neighbouring graphs. The scheme is then tested on both simulated and real
datasets of varying dimension.

Acknowledgements
Initially, I would like to express my gratitude to my supervisor, Darren Wilkinson, for his
exceptional guidance, expertise and endless patience in the midst of some rather repetitive
questioning during my time at Newcastle. Without his inﬂuence, this thesis would look
somewhat diﬀerent. Thanks must also go to my advisor, Richard Boys, for providing
additional, insightful input.
Without the immense love, support and encouragement of my mum and dad, I would
never have got to where I am today. For that, I will always be greatly appreciative. I also
extend these words to the rest of my family.
High praise is reserved for all my friends, in particular those within the Department of
Statistics, who have always helped me in times of crisis, provided great encouragement
and cheered me up with humour of ﬁne quality.
Finally, I would like to acknowledge the ﬁnancial support of the Engineering and Physical
Sciences Research Council.

Contents
1
Introduction
1
1.1
A Bayesian perspective . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2
1.1.1
Bayes’ Factors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
6
1.2
Approximation of the marginal likelihood . . . . . . . . . . . . . . . . . . .
14
1.2.1
Laplace’s approximation . . . . . . . . . . . . . . . . . . . . . . . .
15
1.2.2
Bayesian Information Criterion (BIC) . . . . . . . . . . . . . . . . .
16
1.3
Further criteria . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
18
1.3.1
Akaike’s Information Criterion (AIC) . . . . . . . . . . . . . . . . .
19
1.3.2
Deviance Information Criterion (DIC)
. . . . . . . . . . . . . . . .
20
1.4
Outline of thesis and literature review . . . . . . . . . . . . . . . . . . . . .
22
2
Variational Bayes
25
2.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
25
2.2
Markov chain Monte Carlo . . . . . . . . . . . . . . . . . . . . . . . . . . .
26

CONTENTS
2.2.1
The Gibbs sampler . . . . . . . . . . . . . . . . . . . . . . . . . . .
26
2.2.2
Metropolis-Hastings algorithm . . . . . . . . . . . . . . . . . . . . .
27
2.3
Expectation-Maximisation (EM) algorithm . . . . . . . . . . . . . . . . . .
30
2.4
Variational Bayesian methods . . . . . . . . . . . . . . . . . . . . . . . . .
31
2.4.1
Kullback-Leibler divergence
. . . . . . . . . . . . . . . . . . . . . .
32
2.4.2
Deﬁnition of L(q) . . . . . . . . . . . . . . . . . . . . . . . . . . . .
33
2.4.3
Approximating the marginal likelihood . . . . . . . . . . . . . . . .
34
2.4.4
Computation of L(q) . . . . . . . . . . . . . . . . . . . . . . . . . .
35
2.5
A univariate example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
37
2.5.1
Free form variational method
. . . . . . . . . . . . . . . . . . . . .
39
2.5.2
Fixed form variational method . . . . . . . . . . . . . . . . . . . . .
44
2.5.3
The Gibbs sampler . . . . . . . . . . . . . . . . . . . . . . . . . . .
49
2.5.4
EM algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
50
2.5.5
A numerical example . . . . . . . . . . . . . . . . . . . . . . . . . .
52
2.6
A multivariate example . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
54
2.7
Summary
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
61
3
Model comparison of VAR(1) models
62
3.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
62
3.2
VAR(1) graphical models . . . . . . . . . . . . . . . . . . . . . . . . . . . .
63

CONTENTS
3.2.1
Example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
66
3.3
Scoring zero mean VAR(1) graphical models . . . . . . . . . . . . . . . . .
68
3.3.1
Priors
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
71
3.3.2
Free form method . . . . . . . . . . . . . . . . . . . . . . . . . . . .
73
3.3.3
Fixed form method . . . . . . . . . . . . . . . . . . . . . . . . . . .
79
3.4
Other issues . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
90
3.4.1
Problems with computation . . . . . . . . . . . . . . . . . . . . . .
90
3.4.2
Speciﬁcation of priors . . . . . . . . . . . . . . . . . . . . . . . . . .
92
3.5
Toy example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
95
4
Searching the graphical space
99
4.1
Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
99
4.2
Hill-climbing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 100
4.2.1
Example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 102
4.3
Random walks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 104
4.3.1
Implementation and analysis . . . . . . . . . . . . . . . . . . . . . . 114
4.3.2
Examples
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 120
4.3.3
Prior sensitivity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 130
4.3.4
Small sample size . . . . . . . . . . . . . . . . . . . . . . . . . . . . 135
4.3.5
A further example
. . . . . . . . . . . . . . . . . . . . . . . . . . . 137
i

CONTENTS
4.3.6
Application to ERP data . . . . . . . . . . . . . . . . . . . . . . . . 140
4.3.7
Application to microarray data
. . . . . . . . . . . . . . . . . . . . 152
4.4
Summary
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 155
5
Generalisation to non-zero mean VAR(1) models
157
5.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 157
5.2
Scoring non-zero mean VAR(1) models . . . . . . . . . . . . . . . . . . . . 158
5.2.1
Priors
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 160
5.2.2
Free form method . . . . . . . . . . . . . . . . . . . . . . . . . . . . 160
5.2.3
Fixed form method . . . . . . . . . . . . . . . . . . . . . . . . . . . 174
5.3
Toy example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 181
5.4
Taking a random walk . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 184
5.4.1
Examples
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 185
5.4.2
Application to ERP data . . . . . . . . . . . . . . . . . . . . . . . . 196
5.4.3
Application to microarray data
. . . . . . . . . . . . . . . . . . . . 202
6
Conclusions and further work
209
6.1
Conclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 209
6.2
Further work
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 211
A Probability distributions
213
ii

CONTENTS
A.1 Gaussian distribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 213
A.2 Inverse gamma distribution
. . . . . . . . . . . . . . . . . . . . . . . . . . 214
A.3 Multivariate Gaussian distribution
. . . . . . . . . . . . . . . . . . . . . . 214
A.4 Inverse Wishart distribution . . . . . . . . . . . . . . . . . . . . . . . . . . 215
B Graphical Models
216
B.1 Conditional Independence
. . . . . . . . . . . . . . . . . . . . . . . . . . . 216
B.2 Graph theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 217
B.3 Undirected graphical models . . . . . . . . . . . . . . . . . . . . . . . . . . 218
B.3.1
Example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 220
B.4 Bayesian networks
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 221
B.4.1
Example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 222
C Generalised inverses
224
iii

List of Tables
3.1
Lower bounds and posterior means for each zero mean VAR(1) model . . .
98
4.1
Comparing the accuracy of ˆΠ for each choice of c
. . . . . . . . . . . . . . 132
4.2
Comparing the accuracy of ˆΠ for each choice of p
. . . . . . . . . . . . . . 134
4.3
Comparing the accuracy of ˆΠ for each choice of N . . . . . . . . . . . . . . 136
4.4
Genes examined in the microarray experiment . . . . . . . . . . . . . . . . 153
5.1
Lower bounds and posterior means for each non-zero mean VAR(1) model
183
iv

List of Figures
2.1
(a) and (b): Approximate marginal posterior distributions for m and v
respectively, using variational Bayes, the EM algorithm and the Gibbs
sampler; (c) and (d): Corresponding trace plots for m and v produced
by the Gibbs sampler . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
54
2.2
Contour plots for: (a) Variational posterior, (b) True posterior . . . . . . .
55
3.1
Time series graph for a VAR(1) model
. . . . . . . . . . . . . . . . . . . .
67
3.2
Causality graph for the VAR(1) process . . . . . . . . . . . . . . . . . . . .
67
3.3
A-graph for the true zero mean VAR(1) model . . . . . . . . . . . . . . . .
95
4.1
Convergence of hill-climbing algorithm for diﬀerent, initial graphs . . . . . 103
4.2
A simple graph on which to make a random walk
. . . . . . . . . . . . . . 105
4.3
Plots for the analysis of the MCMC output in Example 1 . . . . . . . . . . 121
4.4
Plots showing estimated, marginal posterior distributions for aij, i, j =
1, 2, 3, in Example 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 124
4.5
Plots for the analysis of the MCMC output in Example 2 . . . . . . . . . . 125
v

LIST OF FIGURES
4.6
Plots showing estimated, marginal posterior distributions for aij, i, j =
1, 2, 3, in Example 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 127
4.7
Plots for the analysis of the MCMC output in Example 3 . . . . . . . . . . 128
4.8
Plots showing estimated, marginal posterior distributions for aij, i, j =
1, 2, 3, in Example 3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 129
4.9
Plots of ˆΠ for diﬀerent speciﬁcations of c: (a) c = 10, 000, (b) c = 10, (c)
c = 0.5, (d) c = 0.1, (e) c = 0.01, (f) c = 0.001 . . . . . . . . . . . . . . . . 132
4.10 Plots of ˆΠ for diﬀerent speciﬁcations of p: (a) p = 0.95, (b) p = 0.855, (c)
p = 0.5, (d) p = 0.3, (e) p = 0.1 . . . . . . . . . . . . . . . . . . . . . . . . 134
4.11 Plots of ˆΠ for diﬀerent speciﬁcations of N: (a) N = 100, (b) N = 80, (c)
N = 50, (d) N = 20, (e) N = 10 . . . . . . . . . . . . . . . . . . . . . . . . 136
4.12 Plots for the analysis of the MCMC output in Example 4.3.5 . . . . . . . . 138
4.13 Plots showing estimated, marginal posterior distributions for aij, i, j =
1, 2, 3 in Example 4.3.5
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 141
4.14 Animal ERP data for one subject . . . . . . . . . . . . . . . . . . . . . . . 144
4.15 Distractor ERP data for one subject
. . . . . . . . . . . . . . . . . . . . . 145
4.16 Plots for the analysis of the MCMC output for the animal ERP data
. . . 147
4.17 Plots for the analysis of the MCMC output for the distractor ERP data . . 148
4.18 Plots showing estimated, marginal posterior distributions for aij, i, j =
1, 2, 3, for the animal ERP data . . . . . . . . . . . . . . . . . . . . . . . . 151
4.19 Plots showing estimated, marginal posterior distributions for aij, i, j =
1, 2, 3, for the distractor ERP data . . . . . . . . . . . . . . . . . . . . . . 152
vi

LIST OF FIGURES
4.20 Plots for the analysis of the MCMC output for the microarray data . . . . 153
4.21 Plots showing estimated, marginal posterior distributions for aij, i, j =
1, 2, 3, for the microarray data
. . . . . . . . . . . . . . . . . . . . . . . . 155
5.1
Plots for the analysis of the MCMC output in Example 1 (non-zero mean) 186
5.2
Plots showing estimated, marginal posterior distributions for aij, i, j =
1, 2, 3, in Example 1 (non-zero mean) . . . . . . . . . . . . . . . . . . . . . 188
5.3
Plots showing estimated, marginal posterior distributions for μj, j = 1, . . . , 6,
in Example 1
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 189
5.4
Plots for the analysis of the MCMC output in Example 2 (non-zero mean) 190
5.5
Plots showing estimated, marginal posterior distributions for aij, i, j =
1, 2, 3, in Example 2 (non-zero mean) . . . . . . . . . . . . . . . . . . . . . 192
5.6
Plots showing estimated, marginal posterior distributions for μj, j = 1, . . . , 6,
in Example 2
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 193
5.7
Plots for the analysis of the MCMC output in Example 3 (non-zero mean) 194
5.8
Plots showing estimated, marginal posterior distributions for aij, i, j =
1, 2, 3, in Example 3 (non-zero mean) . . . . . . . . . . . . . . . . . . . . . 195
5.9
Plots showing estimated, marginal posterior distributions for μj, j = 1, . . . , 6,
in Example 3
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 196
5.10 Plots for the analysis of the MCMC output for the animal ERP data (non-
zero mean) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 197
5.11 Plots for the analysis of the MCMC output for the distractor ERP data
(non-zero mean) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 198
vii

LIST OF FIGURES
5.12 Plots showing estimated, marginal posterior distributions for aij, i, j =
1, 2, 3, for the animal ERP data (non-zero mean) . . . . . . . . . . . . . . 200
5.13 Plots showing estimated, marginal posterior distributions for aij, i, j =
1, 2, 3, for the distractor ERP data (non-zero mean)
. . . . . . . . . . . . 201
5.14 Plots showing estimated, marginal posterior distributions for μj, j = 1, . . . , 6,
for the animal ERP data . . . . . . . . . . . . . . . . . . . . . . . . . . . . 203
5.15 Plots showing estimated, marginal posterior distributions for μj, j = 1, . . . , 6,
for the distractor ERP data
. . . . . . . . . . . . . . . . . . . . . . . . . . 204
5.16 Plots for the analysis of the MCMC output for the microarray data (non-
zero mean) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 205
5.17 Plots showing estimated, marginal posterior distributions for aij, i, j =
1, 2, 3, for the microarray data (non-zero mean) . . . . . . . . . . . . . . . 207
5.18 Plots showing estimated, marginal posterior distributions for μj, j = 1, . . . , 6,
for the microarray data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 208
B.1 4-node graph, corresponding to the choice of K
. . . . . . . . . . . . . . . 220
B.2 6-node DAG . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 222
viii

Chapter 1
Introduction
Suppose that we possess an observed dataset, which has been generated by an incom-
pletely understood underlying process. Then, an important statistical problem is to ﬁnd
a model that explains the inherent trends in the data well. In this case, such a model
can subsequently be utilised to make reasonably accurate, future predictions. In real life
situations, it is customarily the case that there will be a huge number of complicated
factors that will aﬀect the generation of the data. Thus, a standard philosophy to follow
is that a model is merely an approximation to the mechanism giving rise to the data.
Assume we now have a collection of possible models in competition, referred to as a
candidate set. Then, the model selection task is to choose the ‘best’ model in the set, given
the data. That is, we ideally require the model that forms the most suitable representation
of the reality. Unfortunately, the procedure is non-trivial. It is valid to ask at this stage
what constitutes such a selection. For instance, a suﬃciently complex model (with many
parameters) will be able to provide a good ﬁt, i.e. the underlying trends will be well
reﬂected. Else, the model is said to underﬁt the data. So, the ﬁt in a simple model can
be improved by adding extra parameters and will be equivalent to before if these new
parameters are set to zero.
1

1.1. A Bayesian perspective
However, as Beal (2003) indicates, model ﬁt alone is an unsatisfactory criterion for choos-
ing between models. In any model, by its deﬁnition of being an approximation, it will be
practically infeasible to capture exactly each factor that has given rise to the data. Hence,
we refer to these factors as noise. A suﬃciently complex model, with its exceptional ﬂex-
ibility, can be made to produce an exact ﬁt. However, this is not because the trends are
being accurately approximated, but instead the noise is being absorbed into the model.
That is, an excessive number of parameters will resultantly ﬁt the noise in the data. So,
although such a model may be the best ﬁtting in a candidate set given a dataset, it will
provide inadequate predictions of future observations, generated by the same truth, as
the noise will vary in these new observations. In this case, the model is said to overﬁt the
data.
To summarise, by choosing the most complex model in a candidate set, we are not precisely
approximating the intangible reality. Instead, there is a necessary trade-oﬀto be made
between the ﬁt of a model to a particular dataset and its complexity, in terms of how well
it predicts new observations. These issues are at the forefront for any technique used to
select a model given observed data. In this chapter, some of these established methods
are presented. In particular, we focus primarily on how a Bayesian tackles the model
comparison dilemma.
1.1
A Bayesian perspective
Let M = {M1, . . . , MR} be a set of R candidate models, where each model is a probability
distribution. Given the observation of data D, we want to compare the credibility of
these candidates. To eﬀect this, the fundamentals of the Bayesian approach to model
comparison are now examined, illustrated by, inter alia, Kass and Raftery (1995) and
Chipman et al. (2001). We ﬁrst require some initial deﬁnitions. If θi = (θi1, . . . , θid)T is
a set of unknown parameters speciﬁc to model Mi, then let p(D | θi, Mi) be the probability
2

1.1. A Bayesian perspective
density function of D given the value of θi (also referred to as the likelihood function for
θi).
A Bayesian framework dictates the introduction of priors on all unknowns. Thus, in this
case, let p(θi | Mi) be the prior distribution over the parameters of each model. Moreover,
we suppose that p(Mi) is the prior probability assigned to each model itself. Upon the
observation of data D, we are able to update our prior beliefs about the probability of
each model. Thus, by Bayes’ Theorem, the posterior probability of model Mi is given by
p(Mi | D) = p(D | Mi) p(Mi)
p(D)
,
(1.1)
where the probability of the data, a normalising constant, is equivalent to
p(D) =

i
p(D | Mi) p(Mi).
Moreover, the term p(D | Mi) is referred to as the marginal likelihood of data D given
model Mi, such that
p(D | Mi) =

p(D | θi, Mi) p(θi | Mi) dθi.
(1.2)
It is so named since we marginalise, or integrate, over the model parameter space.
We realise that the model posterior, p(Mi | D), is a valuable tool to possess when choosing
between models. If our task were to pick the most plausible model, we can easily choose
that which maximises the value of the posterior probability. So, we can interpret p(Mi | D)
as the probability that the model Mi is the mechanism that generated the data initially.
In other words, this posterior expresses our beliefs, hence quantiﬁes our uncertainty,
about each model after the observation of data. Furthermore, we can derive a posterior
3

1.1. A Bayesian perspective
distribution for the parameters, speciﬁc to each model. This is expressed as
p(θi | D, Mi) = p(D | θi, Mi) p(θi | Mi)
p(D | Mi)
.
(1.3)
Upon examination of (1.1) and (1.3), it is noted that computation of the marginal like-
lihood enables calculation of not only the posterior over models, but also the posterior
over parameters. We shall make a further comment on this relationship in due course.
We now turn to the question of specifying a prior over the set of models, namely p(Mi);
the same procedure for p(θi | Mi) is examined in Section 1.1.1. In both cases however,
as noted by Chipman et al. (2001), there are two approaches to consider. On the one
hand, we could adopt subjective priors, representing our own personal knowledge or beliefs
about the unknowns before data is observed. Although a nice proposal, this framework is
most idealistic, especially if there are many candidate models in our set, each with high-
dimensional parameters θi, and we must somehow quantify our information as probability
distributions.
Therefore, a pragmatic Bayesian approach is adopted here. In this case, priors are con-
structed whereby little or even no prior knowledge is available, hence not aﬀecting the
construction of the posterior. Such priors are described as being broad, ﬂat, diﬀuse or
vague (Gelman et al., 1995). So, as regards speciﬁcation of a model prior, a straightfor-
ward procedure is to make all models equally plausible, hence representing prior ignorance.
Thus, if there are R candidates in our model set M, our prior could be
p(Mi) = 1
R.
(1.4)
The above prior follows a (discrete) uniform distribution, whereby each model has been
awarded the same prior probability. An interesting point to notice is that, upon using
this prior, (1.1) will simplify such that p(Mi | D) ∝p(D | Mi) as the model prior cancels.
4

1.1. A Bayesian perspective
Hence, on this basis, the model posterior is computed up to a multiplicative constant, and
thus the marginal likelihood can be viewed as the evidence for each model. By deﬁnition,
it is the average probability of the data for a given model, with respect to the prior
distribution.
As discussed previously in this chapter, it is critical to ﬁnd a technique to compare models
fairly so that more complex models are penalised suﬃciently. The marginal likelihood is
able to eﬀect this since, by its deﬁnition, it naturally integrates out parameters. Thus, it
embodies the principle of Occam’s razor, which states, in general, that a simpler model
for the data is preferred over a more complex alternative.
MacKay (1995b) and Beal (2003) discuss this aspect of Bayesian model comparison. Sup-
pose that we have two competing models, Mk and Ml, the former being a simple model
and the latter a more complex oﬀering. Consider the space of all datasets of size N. As
Ml will possess additional parameters due to its relative, extra complexity, it will be able
to model a wider range of datasets than its simpler counterpart, Mk.
For every dataset, the corresponding marginal likelihood for each model can be computed
to assess which is the most plausible. Yet, the marginal likelihood over datasets must
integrate to 1. Consequently, Ml, although capable of modelling a plethora of datasets,
can assign only small marginal likelihoods to each. On the contrary, Mk can award a
higher marginal likelihood value to the limited number of datasets that it can model.
Thus, if it is possible for a particular dataset D to be modelled by both Mk and Ml,
then p(D | Mk) > p(D | Ml) and the simpler structure is hence favoured. Initially, this
phenomenon was displayed diagrammatically in Section 1.3 of MacKay (1995b).
Unfortunately, despite its importance in model comparison, calculation of the marginal
likelihood via (1.2) is diﬃcult since the integral could be intractable (e.g. if θi is high-
dimensional as we integrate over the model parameters). Analytic computation of this
quantity is rare. Therefore, a good approximation to this quantity is desirable and tech-
5

1.1. A Bayesian perspective
niques that enable this will be presented later in this chapter. At the present time however,
we now illustrate its signiﬁcance in model comparison.
1.1.1
Bayes’ Factors
Kass and Raftery (1995) elucidated a simple, but elegant criterion for comparing models
in the Bayesian framework, called a Bayes’ factor. Assume that we have two models, say
Mk and Ml, and we want to discover which is the most plausible, given data D. As above,
we specify prior probabilities over the models, namely p(Mk) and p(Ml). For subsequent
interpretation purposes, we choose to work on the odds scale.
In general, recall that, if the probability of an event occurring is p, then the odds, o, in
favour of such an event is given by
o =
p
1 −p.
(1.5)
This is customarily written as 1 : o. Hence currently, the prior odds in favour of Mk are
p(Mk)
1 −p(Mk),
where, moreover, the denominator is equal to p(Ml). Then, by use of (1.1), it is evident
that the ratio of posteriors, or posterior odds of Mk, is equivalent to
p(Mk | D)
p(Ml | D) = p(D | Mk)
p(D | Ml)
p(Mk)
p(Ml) ,
(1.6)
where, of course, p(Ml | D) = 1−p(Mk | D). Then, the Bayes’ factor for model Mk against
model Ml is deﬁned as
Bkl = p(D | Mk)
p(D | Ml) .
(1.7)
6

1.1. A Bayesian perspective
As hinted at previously, calculation of Bkl is dependent on the two marginal likelihoods.
Moreover, the Bayes’ factor is seen to be the ratio of the posterior odds of Mk to its prior
odds, and can be interpreted as the evidence provided by the data in favour of Mk com-
pared to Ml. Thus, for instance, if Bkl = 1, we are indiﬀerent between the two models. If
Bkl > 1, then model Mk is preferred, otherwise model Ml. A more deﬁnitive interpreta-
tion is provided by Kass and Raftery. Here, the authors oﬀer a guideline whereby values
of Bkl > 100 indicate decisive evidence in favour of Mk. Conversely, if approximately
1 < Bkl < 3, then the preference for Mk over Ml is small.
There is no doubt that Bayes’ factors are straightforward and easy to interpret, by quanti-
fying our preference for one model over another. However, they are certainly not infallible.
Initially, we mention two general criticisms. Kass and Raftery adopt the stance that eval-
uation of the Bayes’ factor is made to determine which of the two models is correct. As the
authors themselves identity, many would dispute this claim from a couple of viewpoints.
Firstly, as discussed above, a model is only an approximation to the truth. Moreover, by
examining only two models, we may ultimately choose a poor model, only since it repre-
sents the data better than an even worse model. A second criticism is that computation
of the Bayes’ factor can be arduous due to its reliance on the marginal likelihood, a point
already illuminated. This is exacerbated if the size of the candidate set is large.
The ﬁnal problem that is raised is the most speciﬁc and perhaps that which has caused
most debate in the literature. It is concerned with prior speciﬁcation of the parameters.
This is elucidated, for instance, by O’Hagan (1995) and we now follow this author’s
explanation. Of course, to calculate a Bayes’ factor, p(θi | Mi) must be speciﬁed (c.f.
(1.2)). Suppose we want to represent prior ignorance (also referred to as vague prior
knowledge) for our parameters. For instance, to express each value of θi as equally likely
a priori, we could employ a (continuous) uniform distribution (c.f. (1.4)). Yet, if the
parameter space is inﬁnite, this distribution is no longer deﬁned since it possesses only
7

1.1. A Bayesian perspective
ﬁnite support. Hence, we could use an improper uniform prior given as
p(θi | Mi) ∝1.
(1.8)
Clearly, no particular value of θi is favoured since the prior mass is spread equally across
all values. However, the distribution is now improper since

p(θi | Mi)dθi (representing
the total probability mass) diverges and is not equal to one. Thus, any improper prior will
contain no normalising constant. A further example of an improper, vague prior is the
Jeﬀreys’ prior. Such a choice is characterised by its invariance under reparameterisation,
i.e. the vagueness of a prior on θi is maintained upon transformation of this parameter
vector. So, in this case, prior ignorance is represented by the distribution p(θi | Mi) ∝
|I(θi | Mi)|
1
2, where I(θi | Mi) is Fisher’s information matrix. For more details, see, for
instance, Gelman et al. (1995). We note that the use of improper priors to represent
prior ignorance is common and often provide an easy Bayesian update from prior to
posterior distribution. They are particularly useful when there is diﬃculty in attempting
to quantify one’s prior uncertainty in a distribution. Yet, their use is the only way that
an improper posterior may be produced.
Now, generalise (1.8) such that
p(θi | Mi) ∝fi(θi | Mi),
(1.9)
where fi is any known function whose integral does not converge. For instance, this could
be the Jeﬀreys’ prior. Thus, it follows that p(θi | Mi) = cifi(θi | Mi) for some unspeciﬁed
constant ci. Of course, this normalising constant does not exist due to the divergence of
the integral. Then, the parameter posterior is
p(θi | D, Mi) =
p(D | θi, Mi) fi(θi | Mi)

p(D | θi, Mi) fi(θi | Mi) dθi
.
(1.10)
8

1.1. A Bayesian perspective
This distribution is well-deﬁned, assuming the integral for the marginal likelihood is con-
vergent, since the constants ci have cancelled. Yet, if the models Mk and Ml are given
improper priors similar to (1.9), then the corresponding Bayes’ factor is, by deﬁnition,
equivalent to
Bkl = ck
cl

p(D | θk, Mk) fk(θk | Mk) dθk

p(D | θl, Ml) fl(θl | Ml) dθl
.
(1.11)
Unfortunately, the constants now do not cancel and so the Bayes’ factor contains a ratio of
two unknown constants. This dilemma has caused much consternation amongst Bayesian
statisticians.
If improper priors are to be persisted with to represent ignorance, then
solutions have been sought in the literature. Two of the most common are now presented.
Fractional Bayes’ factors
To remove the dependence on ck
cl from the usual Bayes’ factor in the case of improper
priors, O’Hagan (1991, 1995) suggested a new variant. Suppose again we wish to compare
the models Mk and Ml. Initially, partition the data such that D = (D1, D2). The portions,
D1 and D2, are now employed for two separate purposes: D1, known as a training sample,
is used to learn the parameters θk and θl, and D2 to compare Mk and Ml in a Bayes’
factor.
Thus, it is simple to form parameter posterior distributions through D1, p(θi | D1, Mi) for
i = k, l, using (1.3). Then, the Bayesian paradigm is implemented in a sequential way so
that these parameter posteriors become prior distributions in the wake of the new data
D2, hence resulting in a Bayes’ factor calculation. So, O’Hagan (1995) deﬁnes the partial
Bayes’ factor for model Mk against model Ml using data D2, conditional on D1, as
Bkl(D2 | D1) = p(D2 | D1, Mk)
p(D2 | D1, Ml)
(1.12)
9

1.1. A Bayesian perspective
=

p(D2 | θk, D1, Mk) p(θk | D1, Mk) dθk

p(D2 | θl, D1, Ml) p(θl | D1, Ml) dθl
.
(1.13)
Here, the probability density for D2, namely p(D2 | θi, D1, Mi) for i = k, l, is dependent
on the parameters and the dataset D1, itself previously utilised to learn the parame-
ters.
Even if the initial priors, p(θk | Mk) and p(θl | Ml), are chosen as improper, the
sequential updating of posterior to prior implies that the new ‘priors’, p(θk | D1, Mk) and
p(θl | D1, Ml), are proper and any unspeciﬁed constants have cancelled using (1.10). The
partial Bayes’ factor is so-called as comparison of the models requires only a portion of
the data, hence diﬀering from the full Bayes’ factor, and is well-deﬁned.
This partial Bayes’ factor can now subsequently be used to construct a full Bayes’ factor,
incorporating all data D. In this case, the marginal likelihood of D2 under Mi, conditioned
on D1, is simply
p(D2 | D1, Mi) = p(D1, D2 | Mi)
p(D1 | Mi)
=

p(D | θi, Mi) p(θi | Mi) dθi

p(D1 | θi, Mi) p(θi | Mi) dθi
.
(1.14)
Then, it is evident that
p(D | Mk)
p(D | Ml) = p(D1 | Mk)
p(D1 | Ml)
p(D2 | D1, Mk)
p(D2 | D1, Ml)
and so, by deﬁnition of Bayes’ factors,
Bkl(D) = Bkl(D1)Bkl(D2 | D1).
(1.15)
10

1.1. A Bayesian perspective
By assigning the prior distribution (1.9) for the parameters speciﬁc to each model, it
follows from (1.11) that the term
ck
cl is common in the deﬁnition of both the Bayes’
factors, Bkl(D) and Bkl(D1). Hence, this ratio of unspeciﬁed constants cancels from both
sides of (1.15). Thus, Bkl(D) is now well-deﬁned, as intended. Theoretically, the partial
Bayes’ factor would appear to possess a solid foundation. Yet, in practice, although no-
longer dependent on any unspeciﬁed constants, it remains reliant on choosing a training
sample of size m from a total of N observations, so that the parameters may be learnt
(there are
N
m

ways to do this). To avert the selection of such a dataset D1, O’Hagan
makes an asymptotic approximation to the partial Bayes’ factor.
If we deﬁne b = m
N and then let both m and N become large, then an approximation is
obtained such that
p(D1 | θi, Mi) ≈[p(D | θi, Mi)]b ,
where D1 and D are datasets with m and N observations respectively. Thus, by con-
sideration of (1.14), an alternative marginal likelihood for D under model Mi is given
as
pb(D | Mi) =

p(D | θi, Mi) p(θi | Mi) dθi

[p(D | θi, Mi)]b p(θi | Mi) dθi
.
(1.16)
Hence ﬁnally, motivated by (1.12), the fractional Bayes’ factor, denoted as Bb
kl(D), is
equivalent to
Bb
kl(D) = pb(D | Mk)
pb(D | Ml) .
(1.17)
It is apparent that, if we choose a prior over the parameters that is improper, any un-
speciﬁed constants will now cancel in (1.16), and hence the fractional Bayes’ factor will
be well-deﬁned. Yet, one outstanding issue still remains. Although there is no need to
speciﬁcally choose a training dataset D1, we must however specify the proportion, b, of
D1. This is the main problem with fractional Bayes’ factors and is discussed further in
11

1.1. A Bayesian perspective
O’Hagan (1995). On face value, it appears that the method has replaced one problem
(the unspeciﬁed ratio ck
cl ) with another (how to select a value for b).
Posterior Bayes’ Factors
An alternative framework in the context of using Bayes’ factors with improper priors is
developed by Aitkin (1991). Again, the author is able to construct a methodology, which
removes the dependence of any unspeciﬁed constants in the comparison of the models
Mk and Ml. Firstly, reconsider (1.2), representing the marginal likelihood of the data D,
given model Mi. As noted previously, an alternative perspective shows that this equation
can also be viewed as the prior mean of the density function.
Consequently, Aitkin suggests that when comparing models, to avert the dilemma caused
by arbitrary constants, we can average the density, p(D | θi, Mi), with respect to the
parameter posterior distribution, p(θi | D, Mi), instead of the corresponding prior. This
seems reasonable since, via (1.10), this posterior is well-deﬁned. Thus, the posterior mean
of the likelihood is deﬁned as
ppost(D | Mi) =

p(D | θi, Mi) p(θi | D, Mi) dθi.
(1.18)
So, a posterior Bayes’ factor for model Mk against model Ml is then deﬁned as
Bpost
kl
= ppost(D | Mk)
ppost(D | Ml) .
(1.19)
Notably, the posterior Bayes’ factor is extremely similar in form to the partial Bayes’
factor in (1.13), the diﬀerence being the latter is dependent on the partition of the data
for the purposes of both parameter learning and model comparison. The derivation of
both has required a sequential use of Bayes’ theorem whereby the parameter posterior
12

1.1. A Bayesian perspective
has subsequently been applied as a well-deﬁned prior distribution for model comparison.
In fact, by substituting in for the parameter posterior, (1.18) can be rewritten as
ppost(D | Mi) =

[p(D | θi, Mi)]2 p(θi | Mi) dθi

p(D | θi, Mi) p(θi | Mi) dθi
.
(1.20)
When studying (1.16) and (1.20), now notice the similarity between the fractional and
posterior Bayes’ factors. Thus, akin to before, any outstanding, unspeciﬁed constants
will cancel from (1.20) and so leave a well-deﬁned Bayes’ factor. A consistent criticism of
posterior Bayes’ factors is the use of the data ‘twice’ for learning parameters and model
comparison, which, as illustrated above, is the signiﬁcant diﬀerence between partial and
posterior Bayes’ factor methodology. Such a practice lacks any logical foundation. More-
over, it has been shown by Lindley (1991) that the method can be viewed as incoherent
via a counter-example.
To summarise this section, the use of Bayes’ factors for the purpose of model comparison
can become problematic when improper priors are used to illustrate prior ignorance. In
response, O’Hagan (1995) and Aitkin (1991) have independently constructed solutions
to remove the ratio of unspeciﬁed constants, as seen in (1.11). A further technique is
developed in Berger and Pericchi (1996), producing a so-called intrinsic Bayes’ factor.
Using a similar, initial foundation to O’Hagan, the authors reason that, to avoid specifying
a training sample D1, partial Bayes’ factors should be computed for all training samples
and the result then averaged.
A sensible question to ask at this stage is whether it is even necessary to use improper
priors to represent prior ignorance. A clear, simple alternative is to specify a proper prior
distribution (so integrates to 1), which is not concentrated around any one particular
value. In other words, we require a prior with a reasonable variance. If both p(θk | Mk)
and p(θl | Ml) are speciﬁed as proper, then calculation of the Bayes’ factor is theoretically
13

1.2. Approximation of the marginal likelihood
possible and no dependence on arbitrary constants, seen in (1.11), exists.
Unfortunately, the use of proper, diﬀuse priors in these circumstances is dangerous since
the Bayes’ factor may be highly dependent on the arbitrary choice of such a prior vari-
ance, and hence inappropriate conclusions may be reached. This is referred to as Lindley’s
paradox and is discussed in more detail in Chapter 3. The fractional and posterior Bayes’
factors do not suﬀer from this paradox in quite the same way as, even if proper priors
were speciﬁed in each case, both methods have their foundations in using the parameter
posterior as a prior for marginal likelihood computation. Thus, speciﬁcation of a reason-
able prior variance will not inﬂuence the conclusion of the Bayes’ factor in these cases.
Yet, each procedure will be inﬂuenced by the choice of b and the repetitive use of the data
respectively.
The work of O’Hagan and Aitkin is motivated due to the diﬃculties created with improper
priors. Yet, we must question whether much can be gained by the use of the comparison
techniques that the authors advocate. In solving one problem, it appears that further
issues have been created. Therefore, has much been learnt as regards how to practice
Bayesian model comparison?
It is evident however that the speciﬁcation of either a
proper or improper prior is a thorny issue when assessing the value of a set of competing
models, and a solution is hence required. As mentioned by Aitkin (1991), one possibility
would be to carefully apply an informative, proper prior and analyse the sensitivity of
results to such a choice. This technique is performed later in this thesis.
1.2
Approximation of the marginal likelihood
The importance of the marginal likelihood in Bayesian model comparison is clear. How-
ever, as commented previously, the integral (1.2) is often intractable and so an approxi-
mation is necessary. In this section, two of the more popular, analytic techniques for this
14

1.2. Approximation of the marginal likelihood
are considered. Of course, we must stress that such an approximation is also vital in the
computation of the normalised parameter posterior distribution, as seen by (1.3).
1.2.1
Laplace’s approximation
For the derivation of this method, we follow that given by Beal (2003). Initially, consider
the integrand in the deﬁnition of the marginal likelihood. By taking logarithms of this
expression, we can deﬁne
h(θi) = log [p(D | θi, Mi) p(θi | Mi)] .
(1.21)
This expression is now expanded using a second-order multivariate Taylor series about its
maximum a posteriori (MAP) estimate, denoted by ˜θi. Clearly, this is the point where
the posterior density is maximised, i.e. the mode of the posterior distribution. Hence, we
achieve
h(θi) = h(˜θi) + (θi −˜θi)T h′(˜θi) + 1
2!

θi −˜θi
T
h′′(˜θi)

θi −˜θi

+ . . .
≈h(˜θi) + 1
2

θi −˜θi
T
Hh(˜θi)

θi −˜θi

,
(1.22)
where ′ represents diﬀerentiation with respect to θi. Moreover, Hh(˜θi) is the Hessian
matrix of second partial derivatives for the function h, evaluated at ˜θi. Now, notice that
log p(θi | D, Mi) ∝h(θi) and, consequently by (1.3), [log p(θi | D, Mi)]′ = h′(θi). Thus,
h′(˜θi) = 0 as ˜θi is a maximum of h(θi), that is, the MAP estimate. Via (1.21) and (1.22),
it follows that the log marginal likelihood is given by
log p(D | Mi) = log

exp {h(θi)} dθi
= log
	
exp

h(˜θi)
 
exp
1
2

θi −˜θi
T
Hh(˜θi)

θi −˜θi

dθi

15

1.2. Approximation of the marginal likelihood
≈h(˜θi) + log

(2π)di/2 
|W −1|
1/2
,
(1.23)
where di is the dimension of θi and W = −Hh(˜θi). In other words, we have approxi-
mated exp {h(θi)} = p(D | θi, Mi) p(θi | Mi) via a multivariate normal distribution (see
Appendix A) with mean vector ˜θi, the MAP estimate, and covariance matrix W −1, and
then subsequently integrated. Finally, by substituting (1.21) into (1.23) and taking ex-
ponentials, the Laplace approximation is given by
p(D | Mi)Lap = p(D | ˜θi, Mi) p(˜θi | Mi) (2π)di/2|W|−1/2.
(1.24)
This approximation is based on the fact that, for a large dataset, the parameter posterior
distribution can be approximately normally distributed (Gelman et al., 1995). Hence,
using Laplace seems reasonable if the posterior is unimodal and almost symmetric. Fur-
ther, it is an enticing option due to the ease of computing the MAP estimate. Yet, on
the contrary, we may expect an inaccurate approximation to the marginal likelihood, and
hence posterior, if the sample size is small. Moreover, notice that the Hessian matrix is of
dimension di × di. So, such a method may suﬀer from a computational perspective if θi
is high-dimensional. Finally, as Beal (2003) also mentions, this method may not capture
the position of the posterior probability mass well since the MAP estimate maximises the
posterior density. So, we will obtain a more eﬀective approximation if p(θi | D, Mi) is
tightly peaked about its mode, where all the mass is situated.
1.2.2
Bayesian Information Criterion (BIC)
A further procedure applied to approximate the marginal likelihood is the Bayesian Infor-
mation Criterion (Schwarz, 1978), also termed Schwarz’s Information Criterion (SIC).
This criterion is viewed purely as a means to compare candidate models, and not to
16

1.2. Approximation of the marginal likelihood
construct an approximate, parameter posterior distribution. As we shall see, it contains
terms to evaluate both the ﬁt and complexity of any particular model, as discussed in the
introduction to this chapter.
The criterion can be derived directly from the Laplace approximation as Ghahramani
(2004) demonstrates. Note initially that the Hessian matrix of h, evaluated at the MAP
estimate, is equivalent to
Hh(˜θi) =

log p(D | θi, Mi) + log p(θi | Mi)
′′
θi=˜θi
=

N

t=1
log p(xt | θi, Mi) + log p(θi | Mi)
′′
θi=˜θi
(1.25)
where we possess a dataset D = {x1, . . ., xN}. So, it is evident that the Hessian matrix
is dependent on N. Consequently, by taking logarithms of (1.24), and then rejecting all
terms that are independent of the sample size N, we obtain
log p(D | Mi)Lap = log p(D | ˜θi, Mi) −1
2 log |W|.
(1.26)
Here, log p(D | ˜θi, Mi) will be a sum of N terms. Consequently, it is realised that the
Hessian matrix is of order O(N) since, for each entry of Hh(˜θi), N summations must again
be made. Then, by deﬁnition of O notation, we can specify W ≈NW0 for suﬃciently
large N, where W0 is a ﬁxed constant matrix. Thus, it follows immediately that, as W is
of dimension di × di,
1
2 log |W| ≈di
2 log N + 1
2 log |W0|,
(1.27)
where |NW0| = Ndi|W0| (Harville, 1997). Now, the term
1
2 log |W0| is also ﬁxed with
respect to N. By dropping this and substituting (1.27) into (1.26), the BIC, as presented
17

1.3. Further criteria
by Schwarz (1978), is deﬁned to be
log p(D | Mi)BIC = log p(D | ˜θi, Mi) −di
2 log N.
(1.28)
In (1.28), it is customary that the log-likelihood, log p(D | θi, Mi), is evaluated not at the
MAP estimate ˜θi, but instead at ˆθi, the maximum likelihood estimate (MLE). This is the
value of θi for which the likelihood is maximised.
Due to its reliance on calculation of the MLE, this criterion is easy to handle. Moreover,
notice that, although working in a Bayesian context, the BIC is deﬁned such that no
speciﬁcation of the prior p(θi | D, Mi) is required, assuming that the log-likelihood is
evaluated at the MLE. Depending on one’s perspective, this may be a positive attribute
if it is awkward to elicit one’s parameter prior knowledge. However, the converse may be
true if an informative prior is required. In addition, as the derivation of the BIC given
here is reliant on the Laplace approximation, the criterion may suﬀer if the sample size
is insuﬃciently large.
We realise that the two terms in the BIC expression each serve a purpose. If we interpret
the MLE as the value of the parameters for model Mi that makes the data most plausible,
log p(D | ˆθi, Mi) illustrates how well Mi ﬁts the data, a term that is ideally maximised.
On the other hand, di
2 log N acts to penalise more complex models, determined by the
number of parameters, di, that each possesses. So, for a candidate set of models, the
optimum model choice is that which has the highest value of (1.28).
1.3
Further criteria
The BIC is a classical technique to evaluate the evidence for a set of models. In fact,
other such criteria exist and, in this section, we consider brieﬂy two of the more signiﬁcant
18

1.3. Further criteria
options. We shall see that, as previously, each is dependent on assessing model ﬁt and
model complexity.
1.3.1
Akaike’s Information Criterion (AIC)
Akaike (1974) realised that we need a way to measure the misﬁt between a model and a
truth to judge whether the former is a decent approximation to the latter on the basis of
a dataset. From an alternative perspective, we determine the information lost in making
such an approximation whereby a good model will minimise this quantity. To quantify
this, the Kullback-Leibler (KL) divergence (Kullback and Leibler, 1951) from the truth
to the model is employed, deﬁned as
KL(f | p) =

f(D) log

f(D)
p(D | θi, Mi)

dD
= Ef(D){log f(D)} −Ef(D){log p(D | θi, Mi)},
(1.29)
where log is the natural logarithm. In addition, the expectations above are taken with
respect to f(D), the true density of D, speciﬁed without parameters. Clearly, in (1.29),
the term Ef(D){log f(D)} is a constant across models. Hence, minimising KL(f | p) is
equivalent to ﬁnding the model that maximises J = Ef(D){log p(D | θi, Mi)}, referred to
as the relative KL divergence.
Unfortunately, calculation of J is not possible per se as it is dependent upon knowledge
of the truth f. Paradoxically, an understanding of this reality would render the deriva-
tion of such a criterion unnecessary. Thus, Akaike introduced a fabricated dataset X,
independent of D, but arising from the same distribution. It was then shown that the
expected value of J with respect to f(X) could be estimated, where θi is replaced by the
corresponding MLE ˆθi(X), dependent on model Mi and constructed using the dataset X
(if it were available). In fact, a biased estimator of Ef(X){J} is given by the maximised
19

1.3. Further criteria
log-likelihood function, namely log p(D | ˆθi(D), Mi). Moreover, it was further established
that the bias of this estimate is asymptotically (for a large dataset) equivalent to di, the
dimension of the parameter vector. For additional details on this, see, for instance, Burn-
ham and Anderson (2004) or Stoica and Sel´en (2004). Upon removing the dependence of
the MLE upon D, we see that maximising the unbiased estimator, log p(D | ˆθi, Mi) −di,
for the expectation of J, is equivalent to minimising the following, known as Akaike’s
information criterion:
AIC = −2 log p(D | ˆθi, Mi) + 2di.
(1.30)
The ‘best’ model is deemed to be that which has the smallest AIC value and is interpreted
as the model ‘closest’ to the actual truth. According to Burnham and Anderson (2004),
the multiplication here by −2 is for ‘historical reasons’. In fact, the BIC, given by (1.28), is
also presented similarly, implying that the resulting expression should now be minimised.
Clearly, the AIC and BIC have the same goodness of ﬁt term. Yet, the model complexity
term is more stringent in the BIC case (if N ≥8, then di log N > 2di), hence providing an
obvious preference for simpler models. However, this could be detrimental when a simpler
model is chosen over a more complex one, even if the former is a poor speciﬁcation. On
the other hand, AIC could be susceptible to overﬁtting the data by showing an aﬃnity
for too complex models. Finally, as it is based on asymptotic maximum likelihood theory,
the performance of the AIC in datasets of small size may be questionable.
1.3.2
Deviance Information Criterion (DIC)
The ﬁnal model comparison criterion that is examined was pioneered by Spiegelhalter
et al. (2002). The initial foundation for this technique is provided by the classical de-
viance, which is equivalent to the diﬀerence in the log-likelihoods between a model and
20

1.3. Further criteria
the unknown truth that generated the data. In fact, the deviance D∗is deﬁned as
D∗(θi | Mi) = −2 log p(D | θi, Mi) + 2 log f(D)
(1.31)
where, again, f(D) is the true density of the data. However, this term is independent of
the model Mi. Correspondingly, it is constant, and hence irrelevant, for the purposes of
model comparison. Spiegelhalter et al. examine a Bayesian treatment for the problem at
hand and thus focus their attention on the posterior distribution of the deviance.
Thus, the posterior mean of D∗(θi | Mi) could be utilised as a Bayesian measure of model
ﬁt, denoted as
¯D∗=

D∗(θi | Mi) p(θi | D, Mi) dθi
= Eθi | D, Mi{D∗}.
(1.32)
Due to the deﬁnition of the deviance, those models that provide a good ﬁt will possess a
small value of ¯D∗. This will occur when the number of parameters is increased so we now
require a measure of model complexity to counterbalance this. So, Spiegelhalter et al.
denote such a quantity as pD, taking the form
pD = Eθi | D, Mi{D∗} −D∗
Eθi | D, Mi{θi} | Mi

= ¯D∗−D∗(¯θi | Mi).
Thus, pD is equivalent to the diﬀerence between the posterior mean of the deviance
and the deviance evaluated at the posterior mean of the parameters.
Recalling that
D∗(θi | Mi) = −2 log p(D | θi, Mi), our terms for both measure of ﬁt and the penalty for
model complexity can now be summed (akin to the AIC and BIC) to form the Deviance
21

1.4. Outline of thesis and literature review
Information Criterion:
DIC = ¯D∗+ pD
= D∗(¯θi | Mi) + 2pD,
(1.33)
the latter by rearranging the expression for pD. In the way of both the AIC and BIC, the
high-ranking models are those that minimise the DIC and, hence, an optimal model can
be chosen. By writing the AIC in terms of the deviance such that AIC = D∗(ˆθi | Mi)+2di,
Spiegelhalter et al. show that the DIC is a Bayesian generalisation of the AIC.
In the discussion to this paper, some salient points were raised. For instance, Robert
and Titterington (2002) noticed that the authors’ had used the data once, to construct
a posterior distribution for θi, and then a second time, to take the posterior mean of
the deviance.
This is the same criticism as seen for Aitkin’s posterior Bayes’ factor
whereby the dataset is applied to both learning the parameters and for model comparison.
Moreover, Brooks (2002) questioned why it was possible that pD could in fact be negative,
leaving it open to interpretation in such a case.
1.4
Outline of thesis and literature review
In this chapter, a variety of procedures have been analysed so that the evidence for each
model in a candidate set can be evaluated. Moreover, the potential hazards associated
with each method have also been discussed. In Chapter 2, a relatively recent addition to
the Bayesian model comparison literature is introduced, referred to as variational Bayes.
This method is advantageous since we inherently derive separate approximations to both
the posterior distribution and the marginal likelihood, suitable for future inference and
ranking models respectively.
Its theoretical foundation is reliant upon the Kullback-
Leibler divergence, previously seen in this chapter to derive the AIC. To conclude this
22

1.4. Outline of thesis and literature review
chapter, its performance in posterior approximation is compared to two other, standard
techniques.
For the remainder of the thesis, variational Bayes is applied speciﬁcally to comparing
sparse vector autoregressive (VAR) models of order 1. In Chapter 3, by modelling using
sparsity, a candidate set of zero mean VAR(1) graphical models (speciﬁcally dynamic
Bayesian networks) is established, each of which relates to the autoregressive matrix in
the VAR process.
We proceed to form a lower bound on the marginal likelihood to
compare the evidence for such models. A valid question to inquire at this stage would
be how to handle the problem if the candidate set of graphical models is large. This is
the focus of Chapter 4 and it is answered by constructing a Metropolis-Hastings type
algorithm to search quickly and eﬃciently for high-scoring models in the graphical space.
The ideas of Chapter 3 are then mimicked in Chapter 5 by the study of non-zero mean
VAR(1) models. Examples involving both simulated and real data are then utilised to
elucidate the theory of these two chapters. A summary, illustrating the main points of
the thesis, is presented in Chapter 6.
The most comprehensive review of the variational approximation is provided by Beal
(2003). In this thesis, by considering any model with both parameters and hidden vari-
ables, the author develops a variational Bayesian EM algorithm, allowing alternate up-
dating of approximate posteriors for these two sets of unknowns. The algorithm is applied
to a variety of statistical models, in particular, hidden Markov models, mixtures of factor
analysers and linear dynamical systems, using both simulated and real datasets in each
case. The current work extends that of Beal by providing the variational treatment to
both zero and non-zero mean VAR models (of course, deﬁned without hidden variables).
However, as opposed to determining an optimum model order for a VAR(p) process, we
wish to evaluate the evidence for a set of sparse graphical models of order 1, given a
dataset. This is aided by the use of MCMC methods in high-dimensional spaces.
We realise that it is essential to use an approximation technique such as variational Bayes
23

1.4. Outline of thesis and literature review
in the context of VAR(1) model comparison since, even for the model that is saturated, it
is not possible to derive the marginal likelihood analytically. This fact is shown explicitly
in Chapter 3. Furthermore, this approach is able to enforce naturally the speciﬁc spar-
sity constraints placed upon the approximate posterior distribution of the autoregressive
matrix for each candidate. It is also important to note that learning a dynamic Bayesian
network from data is a problem that has received much coverage in the statistical lit-
erature. To rank candidate structures, Friedman et al. (1998) suggested application of
the BIC or the so-called Bayesian Dirichlet equivalence (BDe) score, originally developed
in the static case by Heckerman et al. (1995). Alternatively, Husmeier (2003) used a
MCMC search algorithm to locate the most plausible models in the space, similar to that
which is presented in Chapter 4. However, to enable analytic computation of the marginal
likelihood, the author was required to discretise the data, leading to a considerable loss
of information.
As opposed to the structural search algorithm considered in this thesis, an alternative
method would be to put sparsity priors on the coeﬃcients of the autoregressive matrix.
This idea is used by, for instance, Lucas et al. (2006) in the circumstance of regression
modelling for microarray data. The aim here is for many entries of this matrix to be
estimated close to (or even equal to) zero following a variational Bayesian analysis. Thus,
if the value of such elements lies below a speciﬁed threshold point, no edge is placed on
the graph between the corresponding nodes. Although this approach is more eﬃcient,
ascertaining possible inﬂuences between nodes in this way can be inaccurate and so is not
pursued here.
24

Chapter 2
Variational Bayes
2.1
Introduction
The focus in the previous chapter was to explore techniques in which the marginal like-
lihood could be approximated, for the primary purpose of model comparison. Our em-
phasis now turns to the approximation of the posterior distribution over parameters. In
this chapter, the dependence of our distributions on each model Mi will be predominantly
removed. So, for completeness, our beliefs about the parameter vector θ = (θ1, . . . , θd)T
upon observing data D are quantiﬁed by the distribution
p(θ | D) =
p(D | θ) p(θ)

p(D | θ) p(θ) dθ
.
(2.1)
Of course, the integral in the denominator of this expression could be intractable. So, a
straightforward, direct solution to this would be to apply the analytic Laplace approxi-
mation. In this chapter, some alternative approaches are examined. For instance, the use
of Markov chain Monte Carlo methods enables samples to be drawn from p(θ | D), with
which, inter alia, understanding can be garnered about the marginal posterior of each
25

2.2. Markov chain Monte Carlo
component of θ. Furthermore, an expectation-maximisation type algorithm can allow
approximation of the afore-mentioned marginal posteriors via MAP estimation. Finally,
special detail is devoted to a relatively, recent technique, known as variational Bayes.
Each method will be treated theoretically and then compared via example.
2.2
Markov chain Monte Carlo
The use of Markov chain Monte Carlo (MCMC) methodology to understand a posterior
distribution has become highly popular in the Bayesian community. Instead of using an
analytic technique such as Laplace to approximate (2.1), a Markov chain is simulated
whose samples will be draws from the posterior, upon convergence of the chain. When a
chain converges to its stationary distribution, it will possess this distribution for all time
henceforth. Thus, to simulate from the posterior, we construct a Markov chain whose
stationary distribution is the posterior distribution.
The summary statistics and the
distribution of these posterior samples will approximate the corresponding characteristics
of the true posterior.
Here, we present two of the most fundamental MCMC methods: the Gibbs sampler and
the Metropolis-Hastings algorithm. In each case, we consider the general case and hence
suppose that π(θ) is the density of interest, where we allow the possibility that each θj
(the j-th component of θ for j = 1, . . .d) could be multi-dimensional. When simulating
from a posterior, we let π(θ) = p(θ | D).
2.2.1
The Gibbs sampler
First documented by Geman and Geman (1984), this method relies on sampling from the
full conditional distributions for each component of θ, such that a sample from π(θ) may
26

2.2. Markov chain Monte Carlo
be obtained. The full conditionals are denoted as
π(θj | θ1, . . . , θj−1, θj+1, . . ., θd) = π(θj | θ−j),
(2.2)
for j = 1, . . ., d, and are assumed to be in closed form so that we may sample from them.
The algorithm for the Gibbs sampler is as given below.
Algorithm 1
1. Initialise the iteration counter to k = 1. The chain itself is initialised
at a starting value θ(0) = (θ(0)
1 , . . . , θ(0)
d )T.
2. By successive simulation from the full conditionals, a new value θ(k) is obtained from
the previous θ(k−1):
θ(k)
1
∼π(θ1 | θ(k−1)
2
, . . ., θ(k−1)
d
)
θ(k)
2
∼π(θ2 | θ(k)
1 , θ(k−1)
3
, . . ., θ(k−1)
d
)
...
θ(k)
d
∼π(θd | θ(k)
1 , . . . , θ(k)
d−1).
3. Change the counter from k to k + 1 and return to step 2.
Upon convergence of the Markov chain, the simulated iterates will be draws from π(θ).
Moreover, at this stage, the values of a particular component will be draws from the
corresponding marginal posterior distribution for that component.
2.2.2
Metropolis-Hastings algorithm
A complementary methodology is the Metropolis-Hastings algorithm (Hastings, 1970),
which allows simulation from the density of interest when this is known only up to a
constant of proportionality. We now introduce an arbitrary proposal distribution, q(θ, φ),
27

2.2. Markov chain Monte Carlo
the notation of which here speciﬁes the probability of a move from θ to φ. The reverse
move is implied by q(φ, θ).
This distribution should be easy to simulate from.
The
algorithm is as follows:
Algorithm 2
1. Initialise both the iteration counter to k = 1 and the chain itself to
starting value θ(0).
2. Generate a proposed value φ from the distribution q(θ(k−1), φ).
3. Compute the acceptance probability α(θ(k−1), φ) of the proposed move, where
α(θ, φ) = min

1, π(φ)q(φ, θ)
π(θ)q(θ, φ)

.
(2.3)
4. Put θ(k) = φ with probability α(θ(k−1), φ), otherwise put θ(k) = θ(k−1).
5. Change the counter from k to k + 1 and return to step 2.
In essence, at each iteration, a new value is generated from the proposal distribution, which
may be accepted (indicating that the chain moves) or rejected (hence the chain stays put).
The movement of the chain is dependent on the acceptance probability α. By drawing
u ∼U(0, 1), the proposal φ is accepted if u < α(θ(k−1), φ) at iteration k. Once the
chain reaches convergence, all simulated values will be draws from π(θ), irrespective of the
choice of proposal distribution. The method is of particular use in the Bayesian paradigm,
as since π(·) is only involved in α(θ, φ) via a ratio, the proportionality constant p(D),
required to compute the posterior distribution and itself computationally problematic,
will cancel out. Although not utilised in this chapter, the Metropolis-Hastings algorithm
will be a crucial tool at our disposal later in this thesis.
When running an MCMC scheme, the period that elapses prior to convergence of the
chain, i.e. before the stationary distribution has been reached, is referred to as the burn-
in period. Therefore, if we want to generate samples from π(θ), we discard those values
28

2.2. Markov chain Monte Carlo
simulated during burn-in. In fact, a good method to establish the length of the burn-in
period required is to plot all values using a trace plot, a time series plot displaying the
values of a component of θ against the number of iterations. Moreover, without burn-in,
such a plot can be used as a crude test for convergence by revealing how well a chain
is said to mix. A well mixing chain will move freely about a constant mean level with
constant variance, exploring the parameter space. Conversely, a poorly mixing chain will
not traverse quickly through the space, indicated on the plot by long, ‘ﬂat’ regions, as a
consequence of numerous, proposed moves being rejected.
The use of MCMC methods in Bayesian statistical inference enables posterior approxima-
tion in large multivariate problems or where the posterior itself is of non-standard form.
Highly accurate results can be obtained if we draw a large number of samples. However,
a huge amount of computational time may be required to achieve this. As Lappalainen
and Miskin (2000) indicate, this is in contrast, for instance, to the Laplace approximation,
which will produce less accurate results, but in shorter time. Moreover, uncertainty will
remain as to whether the chain has reached its stationary distribution. An additional
discussion is provided to this latter issue in Chapter 4.
It is brieﬂy worth mentioning that, although this chapter is primarily concerned with ap-
proximating the parameter posterior, sampling from this distribution by MCMC methods
enables a further approximation to the marginal likelihood. A simple estimate was given
by Newton and Raftery (1994) as
p(D | Mi) ≈

1
B
B

k=1
p(D | θ(k)
i , Mi)−1
−1
,
where θ(k)
i
= (θ(k)
i1 , . . ., θ(k)
id )T, the parameters speciﬁc to Mi, and we obtain B draws from
the posterior. For further details, the reader is referred to the afore-mentioned paper.
29

2.3. Expectation-Maximisation (EM) algorithm
2.3
Expectation-Maximisation (EM) algorithm
We now turn our attention to a method that will approximate analytically each marginal
posterior distribution. As above, let θ = (θ1, . . . , θd)T be a parameter vector of length
d. Consider the density p(θ1 | D) for a dataset D. Then, Gelman et al. (1995) have
illustrated that the EM algorithm (developed by Dempster et al. (1977)) can be applied
as an iterative procedure to ﬁnd a mode (MAP estimate) of this marginal posterior density.
This is of particular use in circumstances where knowledge of this marginal is limited, and
hence cannot be maximised directly to ﬁnd such an estimate. We denote the resulting
estimate by ˜θ1. Further suppose that we ﬁnd corresponding marginal posterior modes,
˜θ2, . . . , ˜θj−1, ˜θj+1, . . . , ˜θd. Then, by deriving the full conditional posteriors, p(θj | θ−j, D)
for each j = 1, . . .d, the marginal posterior for θj can be approximated via p(θj | θ1 =
˜θ1, . . . , θj−1 = ˜θj−1, θj+1 = ˜θj+1, . . . , θd = ˜θd, D). Hence, the algorithm has increased
value, as opposed to just providing a parameter point estimate of the marginal posterior
where the density is highest.
The algorithm itself is a two-stage iterative process, consisting of an E-step (expectation)
and M-step (maximisation).
To ﬁnd a marginal posterior mode for p(θ1 | D), we can
follow the procedure below, as presented by Gelman et al..
Algorithm 3
1. Initialise the iteration counter to k = 1. Make an initial MAP esti-
mate of p(θ1 | D), say θ(0)
1 .
2. At iteration k, perform the following two stages:
(a) E-step: Determine the log joint posterior density, log p(θ1, . . . , θd | D). Then,
take its expectation with respect to the conditional posterior distribution of
θ2, . . . , θd given the previous estimate θ(k−1)
1
, with density denoted by
p(θ2, . . . , θd | θ(k−1)
1
, D). In other words, derive the expectation
30

2.4. Variational Bayesian methods
Ek−1 {log p(θ1, . . . , θd | D)}
=

· · ·

log p(θ1, . . . , θd | D) p(θ2, . . . , θd | θ(k−1)
1
, D) dθ2 . . . dθd.
(b) M-step: Determine θ(k)
1 , the new value of θ1 that maximises
Ek−1 {log p(θ1, . . . , θd | D)}.
3. Change the counter from k to k + 1 and return to step 2.
The algorithm alternates between the E-step and M-step until the estimate has converged
after, say, K iterations. At this point, we deﬁne ˜θ1 := θ(K)
1
. As Gelman et al. note,
the algorithm works since, at each iteration, Ek−1 {log p(θ1, . . . , θd | D)} is maximised,
producing an estimate θ(k)
1
that monotonically increases the log marginal posterior density,
i.e. log p(θ(k)
1 | D) > log p(θ(k−1)
1
| D). By running until convergence such that θ(K)
1
=
θ(K−1)
1
, a mode of the marginal posterior is hence found.
It was indicated earlier that, to approximate a marginal posterior for θj, the algorithm
must be repeated to ﬁnd the additional modes ˜θ2, . . ., ˜θj−1, ˜θj+1, . . . , ˜θd, although this
task could become somewhat laborious.
A ﬁnal point to make is that, if p(θ1 | D) is
multimodal, we may not automatically arrive at the global maximum of p(θ1 | D). To
negotiate this problem, the algorithm should be initialised at a variety of points in the
parameter space, and then let ˜θ1 be the mode such that log p(˜θ1 | D) is maximal.
2.4
Variational Bayesian methods
We now present a ﬁnal, alternative technique for approximating a posterior distribution,
known as variational Bayes (also referred to by MacKay (1995a) as ensemble learning).
In recent years, the literature has become quite rich in this area - see, for instance,
31

2.4. Variational Bayesian methods
Lappalainen and Miskin (2000), Winn (2003), Beal (2003) or Penny et al. (2006). The
outline of the method is as follows. For a set of observed data D and a parameter vector
θ, this approach forms a parametric approximation of the true posterior, p(θ | D). The
approximating distribution is known as a variational distribution (or an ensemble), and
is denoted subsequently by q(θ | D). We must ensure that this distribution is as ‘close’ as
possible to the true posterior. To eﬀect this, a dissimilarity measure can be employed to
gauge the misﬁt between the two distributions. As mentioned in Section 1.3.1 to derive
the AIC, one standard choice of measure is the Kullback-Leibler divergence.
2.4.1
Kullback-Leibler divergence
Given the true and approximating posterior densities p(θ | D) and q(θ | D), recall from
Chapter 1 that the KL divergence from q to p is deﬁned as
KL(q | p) =

q(θ | D) log q(θ | D)
p(θ | D) dθ.
(2.4)
It merely measures the extent to which the two densities agree. Of course, as previously
discussed, to provide a good approximation, we choose the distribution q such that the KL
divergence between q and p is minimised. An important property of the KL divergence is
that it is always non-negative, a result known as the Gibbs’ inequality (Penny et al., 2006),
i.e. KL(q | p) ≥0 with equality if and only if q = p. We also note that, although the KL
divergence is gauging the distance between q and p, it is not per se a true ‘distance’ metric
since it is not symmetric, i.e. KL(q | p) ̸= KL(p | q). Therefore, it is relevant whether we
minimise the misﬁt between q and p or vice versa. A further mention of this is made
below.
32

2.4. Variational Bayesian methods
2.4.2
Deﬁnition of L(q)
As it currently stands, evaluation of (2.4) is not possible since it requires knowledge of
p(θ | D), which we have assumed to be intractable. However, the true posterior can be
simply rewritten as p(θ | D) = p(θ, D)
p(D) . By substituting in, we hence obtain
KL(q | p) =

q(θ | D) log q(θ | D)p(D)
p(θ, D)
dθ
=

q(θ | D) log q(θ | D)
p(θ, D) dθ +

q(θ | D) log p(D) dθ
=

q(θ | D) log q(θ | D)
p(θ, D) dθ + log p(D).
(2.5)
Yet, alternatively, if the same substitution is used to simplify the reverse KL-divergence
KL(p | q), the following is reached:
KL(p | q) =

p(θ | D) log
p(θ, D)
q(θ | D)p(D) dθ
=

p(θ | D) log p(θ, D)
q(θ | D) dθ −log p(D).
Hence, calculating KL(p | q) instead of KL(q | p) provides no beneﬁt since we would now
be required to evaluate the expectation of log p(θ, D)
q(θ | D) under the true posterior p(θ | D),
which is only known up to a constant. Consequently, as mentioned previously, there is
crucial signiﬁcance attached to how we measure the misﬁt between the two distributions.
Therefore, we now return to (2.5). Clearly here, the term log p(D) is a constant, indepen-
dent of q(θ | D). Thus, to minimise the KL divergence, we need only minimise the ﬁrst
term in (2.5). A quantity, referred to only as L(q) for the present time, is deﬁned to be
the negative of this ﬁrst term such that
L(q) =

q(θ | D) log p(θ, D)
q(θ | D) dθ.
(2.6)
33

2.4. Variational Bayesian methods
In general, the integral in (2.6) is able to be evaluated, as will be discussed shortly. Hence,
by taking the negative, we wish to maximise the value of L(q), which, correspondingly,
minimises the KL divergence between the true and approximating posterior. In performing
this, an optimal variational distribution will then have been derived.
2.4.3
Approximating the marginal likelihood
In this chapter hitherto, the variational approach has been examined from the perspec-
tive of approximating a posterior distribution. However, such methods are yet further
attractive since they can play a signiﬁcant role in the area of model comparison. Suppose
we have an available set of candidate models, M = {M1, . . . , MR}. By conditioning our
distributions upon the model Mi, (2.5) can now be speciﬁed as
KL(qi | p) = log p(D | Mi) −LMi(qi),
(2.7)
where each LMi(qi) is speciﬁc to every Mi, qi = q(θi | D, Mi) and θi = (θi1, . . . , θid)T.
Moreover, by the Gibbs’ inequality of the KL divergence, we also have that
LMi(qi) ≤log p(D | Mi).
(2.8)
Equations (2.7) and (2.8) are of critical importance in the variational Bayesian approach.
For each model Mi, (2.8) reveals that LMi(qi) provides a lower bound on the logarithm of
the marginal likelihood, with the diﬀerence between the two being the KL divergence, as
indicated by (2.7). Henceforth, L(q), whether dependent on a model or not, is referred
to as the lower bound (or variational score). Moreover, to form (2.8), we now see the
rationale behind deﬁning the lower bound to be the negative of the ﬁrst term in (2.5).
The crux of the method is that, by maximising LMi(qi), we hence minimise the KL
34

2.4. Variational Bayesian methods
divergence by (2.7), and so ensure that the variational distribution is a good approximation
to the true posterior. Furthermore, this implies correspondingly that the bound (2.8) will
be made as tight as possible, and thus LMi(qi) will be a good approximation to the log
marginal likelihood over models. The process of enforcing the accuracy of the bound to
the true value is referred to as bound optimisation. Now, reconsider (1.1) as in Winn
(2003). If we again suppose a uniform prior across models such that p(Mi) = 1
R, then the
afore-mentioned variational theory implies that, approximately, the posterior density for
Mi is such that
p(Mi | D) ∝exp {LMi(qi)} ,
upon optimising qi. So, we can utilise the lower bound to compare and rank a set of
models. Notice that, if the KL divergence is zero, the lower bound will equal the log
marginal likelihood, and the approximate posterior will hence be equivalent to the true
posterior. Henceforth in this chapter, the dependence of our distributions on Mi is no
longer assumed.
2.4.4
Computation of L(q)
Thus far, we have seen that the variational Bayesian framework is often employed to
ﬁnd an optimal approximation to the true posterior distribution, but moreover, the lower
bound L(q) can be utilised as a variational model selection criterion. However, we have
not discussed how to calculate L(q), deﬁned by (2.6). Thus, to ensure this integral is
tractable, the variational approximation is required to be of a simpler form than the true
posterior, else nothing has been gained. One way to ensure this is to assume q(θ | D)
factorises over parameters such that, if again θ = (θ1, . . . , θd)T, then
q(θ | D) =
d

j=1
q(θj | D).
(2.9)
35

2.4. Variational Bayesian methods
This implies that the set of parameters, {θj}, have now been constrained to be inde-
pendent approximately a posteriori. In other words, the approximating distribution now
possesses a simpler dependency structure than the true posterior. Hence, approximation
(2.9) can be substituted into (2.6). Moreover, we suppose that the joint density of data
and parameters can also be split into a product of likelihood and prior terms such that,
if D = (x1, . . ., xN), then
p(θ, D) = p(D | θ)p(θ) =
N

t=1
p(xt | θ)
d

j=1
p(θj).
The prior distributions are thus forced to be independent. Consequently, by taking loga-
rithms as stated in (2.6), the lower bound L(q) can be written as
L(q) =

q(θ | D)
 N

t=1
log p(xt | θ) +
d

j=1
log
p(θj)
q(θj | D)

dθ.
(2.10)
At this stage, we can proceed using two separate procedures, known as the free form and
ﬁxed form variational methods (Lappalainen and Miskin, 2000). Both techniques rely
on the independence of the variational distributions, as seen in (2.9). Yet, in the free
form approach, no distributional form for the variational posteriors is assumed. Here, by
writing L(q) as a functional of q(θj | D) for all j, the lower bound is maximised by taking
a functional derivative with respect to each variational distribution. Thus, we can derive
the required distributional forms. For instance, by expressing (2.10) as a functional of
q(θj | D) and optimising, it is shown by Miskin (2000) and Winn (2003) that, in general,
q(θj | D) ∝exp

Eq(θ\j | D) (log p(θ, D))

(2.11)
= p(θj) exp

Eq(θ\j | D)
 N

t=1
log p(xt | θ)

,
where the notation θ\j refers to all components of θ except θj.
The above formula
36

2.5. A univariate example
can be used to simplify calculations in this thesis. Moreover, in this method, equations
for the parameters of the variationals (referred to as variational parameters) are found
simultaneously. Then ultimately, the explicit expression for L(q) is derived by use of (2.6).
On the other hand, by contemplating the situation from a ﬁxed form perspective, a ﬁxed
and speciﬁed parametric form is assumed for each variational distribution. Such a selection
is made to ensure that the joint variational distribution is similar to the true posterior,
albeit an approximation as seen by (2.9). Thus, the lower bound L(q) is calculated initially
by evaluation of the necessary integrals. Then, this bound is maximised with respect to the
variational parameter set, hence minimising the KL-divergence and deriving expressions
for the parameters.
It is customarily the case that, independent of which method is used, the algebraic ex-
pressions for the variational parameters will be dependent on each other. Hence, to ﬁnd
the optimal values of the parameters that maximise L(q), each equation must be iterated
to convergence. Moreover, we realise that the value of L(q), due to its maximisation, will
monotonically increase (or remain unchanged) at each iteration. As this quantity is also
bounded above, the algorithm is guaranteed to converge. In fact, as Beal (2003) com-
ments, a local maximum of the lower bound will eventually be reached. In the following
example, both methods described here will be elucidated.
2.5
A univariate example
Suppose that we have a set of observed, one-dimensional data D = (x1, . . ., xN) such that
we model xt ∼N (m, v) for all t = 1, . . . , N. Assuming the xt to be independent, we
write p(D | m, v) = N
t=1 p(xt | m, v). We wish to infer m and v. Moreover, deﬁne prior
distributions over m and v so that
p(m) = N (m | μm, σm)
(2.12)
37

2.5. A univariate example
p(v) = IG(v | a, b),
(2.13)
where an inverse gamma prior is deﬁned over v. The choice of priors stems from the
fact that these are the typical semi-conjugate choices for a Gaussian distribution with
unknown mean and variance. That is the case when, although the usual Bayesian update
is non-conjugate using independent priors, the two full conditional posterior distributions
for both mean and variance are of standard form and follow the same distributional form
as the respective priors. This will become clear when constructing a Gibbs sampler for
this example later in the chapter. Allowing our prior beliefs about m and v to separate
into independent speciﬁcations implies that knowledge of one of the parameters does not
inform us about the distribution of the other.
It is evident that, for this model, analytic analysis of the posterior distribution is in-
tractable. This follows since
p(m, v | D) ∝
 N

t=1
p(xt | m, v)

p(m) p(v)
= v−N
2 exp

−v−1
2
N

t=1
(xt −m)2

× exp

−σ−1
m
2 (m −μm)2

× v−(a+1) exp

−bv−1
= v−(a+ N
2 +1) exp

−v−1
2
N

t=1
(xt −m)2 −σ−1
m
2 (m −μm)2 −bv−1

.
(2.14)
Clearly, this density will not factorise for m and v and hence these parameters are not
independent a posteriori. To combat this problem, three techniques are now employed to
learn an approximate, marginal posterior for both m and v, given the data: variational
Bayesian methods, Gibbs sampling and the EM algorithm. Initially, both the free form
and ﬁxed form variational procedures are applied to elucidate the theory and reveal how
the methods can coincide. Throughout this example, the results of Appendix A are of
38

2.5. A univariate example
particular importance.
2.5.1
Free form variational method
In this instance, optimal variationals, initially for m and later for v, are found without
assuming any distributional form. These are denoted by q(m | D) and q(v | D) respectively.
In fact, we only allow independence between these distributions such that q(m, v | D) =
q(m | D) q(v | D). This assumption will crucially simplify the following computation. Of
course, it is also an approximation to the truth, which, as recognised above, does not
factorise.
To commence, we write L(q) as a functional of both q(m | D) and q(v | D). Therefore, in
this example, the lower bound is deﬁned as
L(q) =

q(m, v | D) log
	p(D, m, v)
q(m, v | D)

dm dv.
(2.15)
This expression is straightforward to manipulate since the prior and variational distribu-
tions are independent. Hence, the lower bound is also equivalent to
L(q) =

q(m | D)q(v | D)
 N

t=1
log p(xt | m, v)

dm dv +

q(m | D) log p(m) dm
+

q(v | D) log p(v) dv −

q(m | D) log q(m | D) dm
−

q(v | D) log q(v | D) dv.
(2.16)
Here, parameters have been integrated out when necessary, a process aided by the fac-
torisation of q(m, v | D). Resultantly, by recombining integrals, the lower bound can now
be expressed as a functional of both variational distributions. As a functional of q(m | D),
39

2.5. A univariate example
we obtain
L(q) =

q(m | D)
 
q(v | D)
 N

t=1
log p(xt | m, v)

dv
+ log p(m) −log q(m | D)

dm + const.
(2.17)
Moreover, expressing in terms of q(v | D) provides
L(q) =

q(v | D)
 
q(m | D)
 N

t=1
log p(xt | m, v)

dm
+ log p(v) −log q(v | D)

dv + const.
(2.18)
Each functional contains a constant term that is independent of q(m | D) and q(v | D)
correspondingly. Recall that, in this method, we take a functional derivative of L(q) with
respect to both variational distributions. Consequently, at that stage, these constants will
disappear.
By examining both equations, we can derive the distributional forms for both q(m | D)
and q(v | D) respectively. The integrals are tackled in order. So, we can substitute in for
both log p(xt | m, v) and log p(m) in (2.17) such that
L(q) =

q(m | D)

N

t=1

q(v | D)

−1
2 log 2πv −v−1
2 (xt −m)2

dv
−1
2 log 2πσm −σ−1
m
2 (m −μm)2 −log q(m | D)

dm + const.
Again here, there are terms, independent of m. After taking a functional derivative of
the lower bound, such terms will clearly play no part in determining the form of q(m | D).
40

2.5. A univariate example
Henceforth, they are dropped and included in a new constant term. Thus, we acquire
L(q) =

q(m | D)

−1
2
N

t=1
(xt −m)2

q(v | D)v−1 dv
−σ−1
m
2 (m −μm)2 −log q(m | D)

dm + const.′
(2.19)
Of course here,

q(v | D)v−1 dv = Eq(v | D) {v−1}. This expectation can be deﬁned when
the variational distribution for v has been derived. Subsequently, L(q), written as a func-
tional of q(m | D), will no longer depend on v as this parameter will have been integrated
out. This is important since we assumed independence between the variational distribu-
tions. At this time, notice that since q(m | D) is a density function, it must integrate to 1.
That is, we look for the variational distribution that optimises the lower bound, subject
to the constraint that it is normalised. To enforce this, Lappalainen and Miskin (2000)
use a Lagrange multiplier. Thus, correspondingly, a new functional, ˜L(q), is formed such
that
˜L(q) = L(q) + νm

q(m | D) dm −1

,
(2.20)
and νm is the required Lagrangian. Via (2.19), we diﬀerentiate ˜L(q) with respect to the
distribution q(m | D). Taking the functional derivative and equating to zero results in
∂˜L(q)
∂q(m | D) = −Eq(v | D) {v−1}
2
N

t=1
(xt −m)2 −σ−1
m
2 (m −μm)2 −log q(m | D) −1 + νm = 0.
Rearranging in terms of q(m | D) and dropping constant terms, we arrive at
q(m | D) ∝exp

−1
2

m2 
NEq(v | D)

v−1
+ σ−1
m

−2m

Eq(v | D)

v−1
N

t=1
xt + σ−1
m μm
 
41

2.5. A univariate example
∝exp
⎧
⎨
⎩−1
2

NEq(v | D)

v−1
+ σ−1
m


m −Eq(v | D) {v−1}  N
t=1 xt + σ−1
m μm
NEq(v | D) {v−1} + σ−1
m
2⎫
⎬
⎭
via completing the square. It is thus apparent that the variational distribution for m is a
Gaussian distribution such that
q(m | D) = N (m | μm
′, σm
′).
(2.21)
Moreover, the variational parameters, μm′ and σm′, are deﬁned as
μm
′ = Eq(v | D) {v−1}  N
t=1 xt + σ−1
m μm
NEq(v | D) {v−1} + σ−1
m
(2.22)
σm
′ =
1
NEq(v | D) {v−1} + σ−1
m
.
(2.23)
This process is now repeated to ﬁnd the optimum form for q(v | D). Hence, reconsider
(2.18). On this occasion, by substituting in for log p(xt | m, v) and log p(v), the lower
bound is expressed as
L(q) =

q(v | D)

N

t=1

q(m | D)

−1
2 log 2πv −v−1
2 (xt −m)2

dm
+ a log b −log Γ(a) −(a + 1) log v −bv−1 −log q(v | D)

dv + const.
As we strive to ﬁnd the variational distribution for v, those terms that are independent
of this parameter can again be dropped as before. Consequently, we obtain
L(q) =

q(v | D)

−N
2 log v −v−1
2
N

t=1

q(m | D)(xt −m)2 dm
−(a + 1) log v −bv−1 −log q(v | D)

dv + const.′
(2.24)
42

2.5. A univariate example
with the new constant term speciﬁed. In addition, with q(m | D) now known, this expres-
sion can be simpliﬁed yet further. Therefore, by (A.2),
N

t=1

q(m | D)(xt −m)2 dm =
N

t=1
Eq(m | D)

(xt −m)2
=
N

t=1

Eq(m | D) {xt −m}
2 +
N

t=1
Varq(m | D) {xt −m}
=
N

t=1
(xt −μm
′)2 + Nσm
′.
(2.25)
So, we have an expression for L(q) that is now independent of m. We seek the variational
q(v | D) that maximises the lower bound, with respect to the density function integrating
to 1. Hence again, the Lagrangian νv is applied to construct ˜L(q) such that
˜L(q) = L(q) + νv

q(v | D) dv −1

.
It is now possible to optimise ˜L(q) with respect to q(v | D). Thus, by diﬀerentiating and
setting to zero, we achieve
∂˜L(q)
∂q(v | D) = −

a + N
2 + 1

log v −v−1

b + 1
2
N

t=1
(xt −μm
′)2 + N
2 σm
′

−log q(v | D) −1 + νv = 0.
A simple rearrangement here provides
q(v | D) ∝v−(a+ N
2 +1) exp

−v−1

b + 1
2
N

t=1
(xt −μm
′)2 + N
2 σm
′

.
Hence, we have ﬁnally that the approximate posterior q(v | D) is an inverse gamma dis-
43

2.5. A univariate example
tribution given by
q(v | D) = IG(v | a ′ , b ′),
(2.26)
where a ′ and b ′ have been found to be equivalent to
a ′ = a + N
2
(2.27)
b ′ = b + 1
2
N

t=1
(xt −μm
′)2 + N
2 σm
′.
(2.28)
Consequently, we can now compute Eq(v | D){v−1}, upon which the equations for μm′ and
σm′ depend. It is evident that Eq(v | D){v−1} = a ′
b ′ via (A.5). To summarise, we have a set
of variational parameters, {μm′, σm′, a ′, b ′}. When inspecting the corresponding alge-
braic expressions, it follows that these equations, and hence the variational distributions
for m and v, are dependent upon each other, as commented upon in Section 2.4.4. There-
fore, we solve these equations iteratively. That is, we update the parameter values by
continuous use of (2.22), (2.23), (2.27) and (2.28) until convergence. The resulting distri-
butions are then optimal in terms of minimising KL divergence, given the approximation
(2.9). In the free form method, this procedure of updating each variational distribution
with respect to all others in the approximation will be seen in further examples in this
thesis.
2.5.2
Fixed form variational method
The second variational procedure is now applied to learn an approximate posterior for
both m and v, given a dataset. Consequently, ﬁxed distributional forms for both varia-
tional distributions must be chosen. On this occasion, it is straightforward to make such
selections since we can use the distributions derived by the free form approach. Hence,
44

2.5. A univariate example
we have
q(m | D) = N (m | μm
′, σm
′)
q(v | D) = IG(v | a ′ , b ′),
and these distributions are required to be independent as before, thus simplifying com-
putation.
We can return to (2.16) and compute L(q) initially, using the now known
variationals. At this point, we realise an overlap between the free and ﬁxed variational
approaches. The free form method concludes by deriving L(q) to obtain an estimate of
the log evidence, provided this calculation is deemed necessary. However initially, in this
ﬁxed form case, the procedure is carried out in identical fashion since q(m | D) and q(v | D)
have been ﬁxed to follow the same distributions as previously suggested by the free form
approach.
The sum of integrals in (2.16) are now tackled in order. Thus ﬁrstly, we acquire

q(m | D)q(v | D)
 N

t=1
log p(xt | m, v)

dm dv
=
N

t=1
	
q(m | D)q(v | D)

−1
2 log 2πv −v−1
2 (xt −m)2

dm dv

= −N
2 log 2π −N
2

q(v | D) log v dv
−1
2
N

t=1

q(v | D)v−1 dv

q(m | D)(xt −m)2 dm
= −N
2 log 2π −N
2 [log b ′ −ψ(a ′)] −a ′
2b ′
N

t=1
(xt −μm
′)2 −Na ′
2b ′ σm
′.
In the last line, (2.25), (A.5) and (A.6) have been applied. Further, by the deﬁnition of
the prior and variational distribution for m,
45

2.5. A univariate example

q(m | D) log p(m) dm
=

q(m | D)

−1
2 log 2πσm −σ−1
m
2 (m −μm)2

dm
= −1
2 log 2πσm −σ−1
m
2

Eq(m | D) {m −μm}
2 + Varq(m | D) {m −μm}

= −1
2 log 2πσm −σ−1
m
2

(μm
′ −μm)2 + σm
′
.
In a similar fashion for v, we obtain

q(v | D) log p(v) dv
=

q(v | D)

a log b −log Γ(a) −(a + 1) log v −bv−1
dv
= a log b −log Γ(a) −(a + 1) [log b ′ −ψ(a ′)] −ba ′
b ′ .
Moreover, it is apparent that

q(m | D) log q(m | D) dm
=

q(m | D)

−1
2 log 2πσm
′ −(σm′)−1
2
(m −μm
′)2

dm
= −1
2 log 2πσm
′ −(σm′)−1
2

Eq(m | D) {m −μm
′}
2 + Varq(m | D) {m −μm
′}

= −1
2 log 2πσm
′ −1
2.
Finally, in a comparable way to

q(v | D) log p(v) dv, it follows that

q(v | D) log q(v | D) dv
=

q(v | D)

a ′ log b ′ −log Γ(a ′) −(a ′ + 1) log v −b ′v−1
dv
46

2.5. A univariate example
= −log Γ(a ′) −log b ′ + (a ′ + 1)ψ(a ′) −a ′.
By collecting all these integrals together, we can substitute into (2.16), and hence obtain
an expression for L(q). This gives the lower bound to be
L(q) = −N
2 log 2π −N
2 [log b ′ −ψ(a ′)] −a ′
2b ′
N

t=1
(xt −μm
′)2 −Na ′
2b ′ σm
′
−1
2 log 2πσm −σ−1
m
2

(μm
′ −μm)2 + σm
′
+ a log b −log Γ(a)
−(a + 1) [log b ′ −ψ(a ′)] −ba ′
b ′ + 1
2 log 2πσm
′ + 1
2 + log Γ(a ′)
+ log b ′ −(a ′ + 1)ψ(a ′) + a ′.
(2.29)
Consequently, we can ﬁnd the maximum of the lower bound by setting its gradient to zero.
That is, we eﬀect partial diﬀerentiation of L(q) with respect to the variational parameter
set {μm′, σm′, a ′, b ′}. Thus, we examine initially maximisation with respect to μm′. This
then yields
∂L(q)
∂μm′ =
∂
∂μm′

−σ−1
m
2 (μm
′ −μm)2 −a ′
2b ′
N

t=1
(xt −μm
′)2

= −σ−1
m (μm
′ −μm) + a ′
b ′
 N

t=1
xt −Nμm
′

.
By setting to zero and then rearranging, we ﬁnd that the update equation for μm′ is
identical to (2.22) as required, recalling the deﬁnition of Eq(v | D){v−1}. Diﬀerentiating
with respect to σm′ leads to
∂L(q)
∂σm′ =
∂
∂σm′

−Na ′
2b ′ σm
′ −σ−1
m
2 σm
′ + 1
2 log 2πσm
′

= −Na ′
2b ′ −σ−1
m
2
+ (σm′)−1
2
.
47

2.5. A univariate example
On this occasion, equating to zero and solving for σm′ provides (2.23). Eﬀecting the same
procedure in terms of a ′ implies
∂L(q)
∂a ′
=
∂
∂a ′

N
2 ψ(a ′) −a ′
2b ′
N

t=1
(xt −μm
′)2 −Na ′
2b ′ σm
′ + (a + 1)ψ(a ′)
−ba ′
b ′ + log Γ(a ′) −(a ′ + 1)ψ(a ′) + a ′

= ψ1(a ′)
	
a + N
2 −a ′

−1
b ′

b + 1
2
N

t=1
(xt −μm
′)2 + N
2 σm
′

+ 1,
(2.30)
where the trigamma function (Johnson et al., 1992), denoted by ψ1(z) for some z ∈R, is
deﬁned to be
ψ1(z) = d2
dz2 log Γ(z) = d
dzψ(z).
Finally, the partial diﬀerentiation of L(q) with respect to b ′ oﬀers
∂L(q)
∂b ′
=
∂
∂b ′

−N
2 log b ′ −a ′
2b ′
N

t=1
(xt −μm
′)2 −Na ′
2b ′ σm
′
−(a + 1) log b ′ −ba ′
b ′ + log b ′

=
a ′
(b ′)2

b + 1
2
N

t=1
(xt −μm
′)2 + N
2 σm
′

−1
b ′
	
a + N
2

.
(2.31)
By inspecting (2.30) and (2.31), it is apparent that these expressions are zeroed by speci-
ﬁcations (2.27) and (2.28) for a ′ and b ′. Therefore, when choosing the variational distri-
butional forms in the ﬁxed form method to be those suggested by the free form method,
the two variational approaches, as expected, have coincided. However, as Miskin (2000)
indicates, one can make incorrect choices for the approximating posteriors in the ﬁxed
form algorithm, hence aﬀecting subsequent results.
48

2.5. A univariate example
2.5.3
The Gibbs sampler
Here, we are only required to ﬁnd the two full conditional posterior distributions for m
and v, namely p(m | v, D) and p(v | m, D). From (2.14), it is evident that
p(m | v, D) ∝exp

−1
2

v−1
N

t=1
(xt −m)2 + σ−1
m (m −μm)2

∝exp
⎧
⎨
⎩−1
2

Nv−1 + σ−1
m


m −v−1  N
t=1 xt + σ−1
m μm
Nv−1 + σ−1
m
2⎫
⎬
⎭,
and hence
p(m | v, D) = N

v−1  N
t=1 xt + σ−1
m μm
Nv−1 + σ−1
m
,
1
Nv−1 + σ−1
m

.
(2.32)
Furthermore,
p(v | m, D) ∝v−(a+ N
2 +1) exp

−v−1

b + 1
2
N

t=1
(xt −m)2

,
whereby
p(v | m, D) = IG

a + N
2 , b + 1
2
N

t=1
(xt −m)2

.
(2.33)
The semi-conjugacy of the problem is now realised, i.e. the full conditionals for m and
v are of standard form, and follow the same distributions as the corresponding priors.
Once initialised anywhere such that the posterior has support, the sampler then produces
alternate simulations from the full conditionals and a bivariate Markov chain is hence
deﬁned. Upon convergence, the corresponding samples will be draws from the density of
interest, p(m, v | D). Moreover, the values for each component are simulations from the
corresponding marginal posterior distribution.
49

2.5. A univariate example
2.5.4
EM algorithm
Two separate EM algorithms are now constructed to ﬁnd the modes ˜m and ˜v of the respec-
tive marginal posterior densities, p(m | D) and p(v | D). Hence, via the full conditionals
(2.32) and (2.33), the unknown marginals can be approximated by p(m | v = ˜v, D) and
p(v | m = ˜m, D).
From (2.14), it is clear that the logarithm of the joint posterior density is
log p(m, v | D) = −

a + N
2 + 1

log v−v−1
2
N

t=1
(xt−m)2−σ−1
m
2 (m−μm)2−bv−1. (2.34)
Suppose that we are currently at iteration k. Initially, we use Algorithm 3 to derive an
expression for m(k). So, in the E-step, we take the expectation of (2.34) with respect to
p(v | m(k−1), D), where m(k−1) is the marginal posterior mode at the previous iteration.
Denoting Em(k−1){·} to be the expectation with respect to p(v | m(k−1), D), the following
is yielded:
Em(k−1) {log p(m, v | D)} = −

a + N
2 + 1

Em(k−1) {log v} −1
2 Em(k−1)

v−1
N

t=1
(xt −m)2
−σ−1
m
2 (m −μm)2 −b Em(k−1)

v−1
.
(2.35)
Here, evaluation of Em(k−1) {log v} is not required since, being independent of m, the cor-
responding term in (2.35) will disappear under diﬀerentiation in the M-step. Resultantly,
we need only compute Em(k−1) {v−1}, which, by (A.5) and our previous derivation of the
full conditional for v, is equivalent to
Em(k−1)

v−1
=
a + N
2
b + 1
2
 N
t=1 [xt −m(k−1)]2.
(2.36)
We can now proceed to the M-step. By diﬀerentiating (2.35) with respect to m, we hence
50

2.5. A univariate example
achieve
∂
∂mEm(k−1) {log p(m, v | D)} = Em(k−1)

v−1
N

t=1
(xt −m) −σ−1
m (m −μm).
By equating to zero and solving for m, the current marginal posterior mode estimate for
p(m | D) is
m(k) = Em(k−1) {v−1}  N
t=1 xt + σ−1
m μm
NEm(k−1) {v−1} + σ−1
m
,
(2.37)
substituting in (2.36). The same procedure is performed to determine v(k). As a conse-
quence, we now calculate the expectation of (2.34) with respect to p(m | v(k−1), D) in the
E-step. Hence, we obtain
Ev(k−1) {log p(m, v | D)} = −

a + N
2 + 1

log v −v−1
2
N

t=1
Ev(k−1)

(xt −m)2
−σ−1
m
2 Ev(k−1)

(m −μm)2
−bv−1.
(2.38)
We realise that, similar to before, computation of Ev(k−1) {(m −μm)2} is not necessary.
Therefore, we have
Ev(k−1)

(xt −m)2
= (Ev(k−1) {xt −m})2 + Varv(k−1) {m}
=

xt −
$
v(k−1)%−1  N
t=1 xt + σ−1
m μm
N [v(k−1)]−1 + σ−1
m
2
+
1
N [v(k−1)]−1 + σ−1
m
,
(2.39)
due to (2.32). In the M-step, maximising (2.38) with respect to v implies
∂
∂vEv(k−1) {log p(m, v | D)} = −

a + N
2 + 1

v−1
+ v−2
2
N

t=1
Ev(k−1)

(xt −m)2
+ bv−2.
51

2.5. A univariate example
So, this equation is zeroed when
v(k) = b + 1
2
 N
t=1 Ev(k−1) {(xt −m)2}
a + N
2 + 1
,
(2.40)
substituting in (2.39). By iterating equations (2.37) and (2.40) separately K times un-
til convergence, we will obtain ˜m = m(K) and ˜v = v(K), the modes for p(m | D) and
p(v | D) respectively. Thus, an approximation to these two marginal posteriors is given
by p(m | v = ˜v, D) and p(v | m = ˜m, D).
It is worth brieﬂy mentioning that the above derivation can also be used to obtain the
expressions for the variational parameters seen in Section 2.5.1. By application of (2.11),
it is apparent that both (2.22) and (2.23) can be read oﬀfrom equation (2.35) without
any additional work, similarly (2.27) and (2.28) from (2.38).
2.5.5
A numerical example
We illustrate the theory above with a simple, numerical example.
Suppose that our
dataset consists of N = 20 samples, simulated from a univariate Gaussian distribution
with mean m = 2 and variance v = 1. In addition, the priors for m and v were given the
following speciﬁcations:
p(m) = N (m | 0, 10, 000)
p(v) = IG(v | 1, 0.001).
Thus, both priors are deemed to be diﬀuse as each has been assigned a huge variance.
In fact, as the variance of an IG(a, b) distribution is deﬁned only for a > 2, the above
distribution for v has inﬁnite variance. So importantly, we do not favour any particular
value of the parameters a priori. A more thorough discussion of vague, inverse gamma
52

2.5. A univariate example
prior speciﬁcation, in particular, is oﬀered in Chapter 3.
The variational Bayes approach, EM algorithm and Gibbs sampler were then run for this
example. In the variational case, equations (2.22) and (2.23), for μm′ and σm′ respectively,
are both dependent upon a ′ and b ′. So, an arbitrary, initial choice of a ′ = b ′ = 1 was
made for the algorithm to commence. Similarly, the EM algorithm and Gibbs sampler
were both initialised such that m(0) and v(0) were points simulated from the respective
prior distributions. The sampler was run for 10, 000 iterations, the ﬁrst 1000 of which
were discarded as burn-in. Convergence of the variational Bayes algorithm was extremely
rapid, taking no more than 4 iterations.
Figure 2.1 shows the plots of the approximate marginal posteriors for the two parameters,
illustrating the three methods. To recap, the variational posteriors are the distributions
(2.21) and (2.26) with variational parameters whose update equations have been run until
convergence. The marginals via the EM algorithm are the full conditionals (2.32) and
(2.33), dependent upon the posterior modes ˜v and ˜m respectively. Finally, kernel density
estimates are plotted for the draws obtained via the Gibbs sampler. Inspection of plots
(a) and (b) in Figure 2.1 clearly illustrate the similarity of the distributions produced by
the three approaches. Moreover, each marginal is centred at values very close to the true
values of the parameters. This is impressive since a dataset of only small size was used
to infer m and v. Hence in this case, the variational Bayes method appears to produce
results, considered equally as good as two other rival approximations.
Further evidence for the worth of the variational approach is oﬀered in Figure 2.2. Here,
contour lines are plotted for both the joint variational distribution, q(m, v | D), and the
true posterior (2.14), known up to a multiplicative constant. The ﬁgure clearly shows
that the contours for these distributions are centred in almost the equivalent position
and, moreover, are similar in shape. However, due to the independence assumption that
q(m, v | D) = q(m | D)q(v | D), the variational approximation is not quite able to fully
capture correlations between m and v, seen in the truth. Yet, it does correctly show that
53

2.6. A multivariate example
0
1
2
3
4
0.0
0.5
1.0
1.5
2.0
m
Density
Var
EM
Gibbs
0
1
2
3
4
0.0
0.5
1.0
1.5
v
Density
Var
EM
Gibbs
Iteration
m
0
2000
4000
6000
8000
1.5
2.0
2.5
3.0
Iteration
v
0
2000
4000
6000
8000
0.5
1.0
1.5
2.0
2.5
3.0
3.5
(a)
(b)
(c)
(d)
Figure 2.1: (a) and (b): Approximate marginal posterior distributions for m and v re-
spectively, using variational Bayes, the EM algorithm and the Gibbs sampler; (c) and (d):
Corresponding trace plots for m and v produced by the Gibbs sampler
the posterior density is not symmetric about the mode value of v.
2.6
A multivariate example
In the previous section, inter alia, a variational Bayesian approach was used to infer
approximate distributions for the unknown mean and variance of a univariate, Gaussian
sample. In fact, the above numerical example has shown the method to produce fast and
accurate results. These ideas are now extended to the corresponding multivariate case.
This will motivate subsequent chapters in this thesis, whereby variational Bayes is applied
to vector autoregressive models of order 1.
54

2.6. A multivariate example
m
v
1
2
3
0.5
1
1.5
m
v
1
2
3
0.5
1
1.5
(a)
(b)
Figure 2.2: Contour plots for: (a) Variational posterior, (b) True posterior
Consequently, we now possess a dataset D = (x1, . . . , xN), where each xt is an indepen-
dent, d-dimensional random vector such that xt ∼N (m, V ) for all t = 1, . . . , N. Here,
m is a mean vector and V a d × d covariance matrix. The speciﬁcation of priors for m
and V is now
p(m) = N (m | μm, Σm)
(2.41)
p(V ) = IW(V | A, r),
(2.42)
where the prior for V follows an inverse Wishart distribution with parameters A, a d × d
matrix, and a scalar r. Further details of this distribution are provided in Appendix A.
These independent prior distributions merely generalise the univariate, semi-conjugate
choices, seen in Section 2.5, to higher dimensions.
To emphasise the need to approximate the joint posterior in this case, it follows that
p(m, V | D) ∝
 N

t=1
p(xt | m, V )

p(m) p(V )
55

2.6. A multivariate example
= |V |−(r+d+N+1)/2 exp

−1
2

N

t=1
(xt −m)TV −1(xt −m)
+ (m −μm)TΣ−1
m (m −μm) + Tr
$
V −1A
%

.
Of course, this is akin to (2.14) and shows that the above density will not factorise, hence
no distributional form can be found for the marginal posteriors of m and V . Thus, we can
use variational Bayesian techniques to infer respective variational distributions, q(m | D)
and q(V | D). Again, we form an approximation such that the joint variational posterior
factorises into the corresponding variational marginals.
We proceed by mimicking the free form method of Section 2.5.1, hence assuming no
variational distributional form. Consequently, the lower bound is now given by
L(q) =

q(m, V | D) log p(D, m, V )
q(m, V | D) dm dV.
(2.43)
Writing L(q) as a functional of both q(m | D) and q(V | D) is elementary via studying the
analogous univariate expressions, (2.17) and (2.18). We again notice the importance here
of independence between each prior and variational distribution. Hence, as a functional
of q(m | D), the following is acquired:
L(q) =

q(m | D)
 
q(V | D)
 N

t=1
log p(xt | m, V )

dV
+ log p(m) −log q(m | D)

dm + const.
(2.44)
The corresponding expression in terms of q(V | D) is
56

2.6. A multivariate example
L(q) =

q(V | D)
 
q(m | D)
 N

t=1
log p(xt | m, V )

dm
+ log p(V ) −log q(V | D)

dV + const.
(2.45)
The distributional form for q(m | D) is derived initially. By substituting in the appropriate
terms, (2.44) can be rewritten as
L(q) =

q(m | D)

N

t=1

q(V | D)

−d
2 log 2π −1
2 log |V |
−1
2(xt −m)TV −1(xt −m)

dV −d
2 log 2π −1
2 log |Σm|
−1
2(m −μm)TΣ−1
m (m −μm) −log q(m | D)

dm + const.
By dropping all terms independent of m, we then obtain
L(q) =

q(m | D)

−1
2
N

t=1
(xt −m)T Eq(V | D)

V −1
(xt −m) dV
−1
2(m −μm)TΣ−1
m (m −μm) −log q(m | D)

dm + const.′
The term Eq(V | D) {V −1} can be computed upon determining the variational posterior
for V . The functional ˜L(q) is now formed using the Lagrangian νm in the way akin to
(2.20), hence ensuring that q(m | D) is normalised. We now seek the optimal q(m | D)
that maximises ˜L(q). Hence, by diﬀerentiating, we obtain
∂˜L(q)
∂q(m | D) = −1
2
N

t=1
(xt −m)T Eq(V | D)

V −1
(xt −m)
−1
2(m −μm)TΣ−1
m (m −μm) −log q(m | D) −1 + νm = 0.
57

2.6. A multivariate example
Rearranging this expression then implies
q(m | D) ∝exp

−1
2

mT 
NEq(V | D)

V −1
+ Σ−1
m

m
−mT

Eq(V | D)

V −1
N

t=1
xt + Σ−1
m μm

−
 N

t=1
xT
t Eq(V | D)

V −1
+ μT
mΣ−1
m

m

∝exp

−1
2

m −

NEq(V | D)

V −1
+ Σ−1
m
−1

Eq(V | D)

V −1
N

t=1
xt + Σ−1
m μm
T
×
$
NEq(V | D)

V −1
+ Σ−1
m
%
×

m −

NEq(V | D)

V −1
+ Σ−1
m
−1

Eq(V | D)

V −1
N

t=1
xt + Σ−1
m μm
 
.
Hence, it follows that the variational posterior for m is a multivariate Gaussian distribu-
tion such that
q(m | D) = N (m | μm
′, Σm
′),
(2.46)
with update equations for the variational parameters speciﬁed as
μm
′ =

NEq(V | D)

V −1
+ Σ−1
m
−1

Eq(V | D)

V −1
N

t=1
xt + Σ−1
m μm

(2.47)
Σm
′ =

NEq(V | D)

V −1
+ Σ−1
m
−1 .
(2.48)
The identical course is now taken for q(V | D). By substituting in for the prior on V and
likelihood, (2.45) is now given by
L(q) =

q(V | D)

N

t=1

q(m | D)

−d
2 log 2π −1
2 log |V |
−1
2(xt −m)TV −1(xt −m)

dm −log k + r
2 log |A|
−r + d + 1
2
log |V | −1
2Tr
$
V −1A
%
−log q(V | D)

dV + const.
58

2.6. A multivariate example
where k is deﬁned by (A.10). Dropping all terms independent of V provides
L(q) =

q(V | D)

−N
2 log |V | −1
2
N

t=1
Eq(m | D)

(xt −m)TV −1(xt −m)

−r + d + 1
2
log |V | −1
2Tr
$
V −1A
%
−log q(V | D)

dV + const.′
(2.49)
By knowledge of q(m | D), we can also compute
Eq(m | D)

(xt −m)TV −1(xt −m)

= Eq(m | D) {xt −m}T V −1Eq(m | D) {xt −m}
+ Tr
$
V −1Varq(m | D) {xt −m}
%
= (xt −μm
′)TV −1(xt −μm
′) + Tr
$
V −1Σm
′%
. (2.50)
Here, we have utilised the identity to ﬁnd the expectation of a quadratic form, i.e.
E

wTPw

= E {w}T P E {w} + Tr [P Var {w}] ,
(2.51)
where w is a random vector and P is a compatible, ﬁxed matrix (Rice, 1995).
By forming ˜L(q) with the Lagrangian νV , we diﬀerentiate with respect to q(V | D):
∂˜L(q)
∂q(V | D) = −N
2 log |V | −1
2
N

t=1
(xt −μm
′)TV −1(xt −μm
′) −N
2 Tr
$
V −1Σm
′%
−r + d + 1
2
log |V | −1
2Tr
$
V −1A
%
−log q(V | D) −1 + νV = 0.
Notice that (xt −μm
′)TV −1(xt −μm
′) = Tr
$
V −1(xt −μm
′)(xt −μm
′)T%
since
Tr [PQR] = Tr [RPQ] = Tr [QRP]
59

2.6. A multivariate example
for compatible matrices P, Q, R (Harville, 1997).
Thus, deﬁning S ′ = 1
N
 N
t=1(xt −μm
′)(xt −μm
′)T, it hence follows that
q(V | D) ∝|V |−(r+N+d+1)/2 exp

−1
2Tr
$
V −1 (A + NΣm
′ + NS ′)
%
.
Therefore, the variational for V is distributed as
q(V | D) = IW(V | A ′, r ′),
(2.52)
with A ′ and r ′ expressed as
A ′ = A + NΣm
′ + NS ′
(2.53)
r ′ = r + N.
(2.54)
Hence, we can now express Eq(V | D) {V −1} = r ′(A ′)−1 via (A.11) and this result is sub-
stituted into both (2.47) and (2.48). Finally, update equations (2.47), (2.48), (2.53) and
(2.54) are iterated until converged values of μm
′, Σm
′, A ′ and r ′ are found, hence deﬁning
the variational distributions for m and V .
In the task of deriving variational posteriors for the unknown mean and variance of a
Gaussian sample, it is evident that the univariate case in Section 2.5 has been naturally
extended in this section to higher dimensions. Clearly, the choice of multivariate normal
and inverse Wishart priors for m and V respectively simply generalises the semi-conjugate
speciﬁcations seen previously. This is further true in terms of the variational distributions
ultimately derived in both circumstances. In the following chapter, similar variational
multivariate theory will be required to score sparse vector autoregressive models.
60

2.7. Summary
2.7
Summary
The purpose of this chapter was to introduce the theory behind variational Bayes, founded
upon minimising the KL divergence between the approximating, variational distribution
and the true posterior.
Hence, as Kullback-Leibler is a global measure, an analytic,
global approximation to this distribution will be provided that is optimal over the whole
parameter space. This is in contrast to Laplace’s approximation that only makes a local
(Gaussian) approximation to the posterior at the MAP estimate. Furthermore, we noted
that L(q), a bound on the log marginal likelihood for each model, can be utilised as a
variational model comparison criterion. This feature is exploited in the remainder of this
thesis.
The main problem with the variational Bayesian approach is that, for computational
reasons, the true posterior is assumed to factorise. This implies that we cannot determine
any a posteriori dependencies between parameters. However, in contrast, we have seen
that variational Bayes is a fast and computationally eﬃcient procedure. In addition, the
example of Section 2.5.5 has moreover revealed its accuracy, relative to two competing
alternatives: the Gibbs sampler and the EM algorithm.
The option of using either a free form or ﬁxed form variational method would also appear
attractive. As has been mentioned, care must be employed when choosing a distributional
form for the variationals in the latter case to ensure a reliable approximation. Yet, by
not making such an assumption, the free form procedure will always ensure the best,
possible variationals are selected. On the other hand, the ﬁxed form method is often
straightforward to apply for more complicated models where the free form approach is
intractable, i.e. in situations where integration over the model parameters cannot be
performed. Although that is not the case in this thesis, both of these procedures will have
an important role to play in Chapters 3 and 5.
61

Chapter 3
Model comparison of VAR(1) models
3.1
Introduction
By deﬁnition, a time series is a set of data values that are measured at equally spaced,
successive time points. For instance, a simple example would be to monitor the average
price of houses in a particular region each month. By modelling such data accurately, we
hope to be able to predict future events in the series. A popular way to eﬀect this would be
to use an autoregressive (AR) model, deﬁned so that there exists a linear dependence on
previous data values. Moreover, if the data is of dimension d, then the time series is now
multivariate (i.e. there are d time series), and can be modelled via a vector autoregressive
(VAR) process, possessing either a zero or non-zero mean.
The Bayesian treatment of VAR models has traditionally focussed on learning the opti-
mum model order and the model parameters, given a set of time series data. Such analysis
is analytically intractable. To tackle this, the variational Bayesian algorithm has been
often applied as an approximation. For instance, in the context of choosing the model
order p, its use has been seen when modelling via a zero mean, univariate autoregressive
model, with noise given by both a Gaussian distribution (Penny and Roberts, 2000) and
62

3.2. VAR(1) graphical models
a mixture of Gaussians (Roberts and Penny, 2002) and, moreover, a zero mean VAR(p)
model (Penny and Roberts, 2002). In addition, the method has allowed approximation
of the parameter posterior distributions found in dynamic linear models, leading to iden-
tiﬁcation and subsequent graphical display of potential interactions between genes (Beal
et al., 2005).
In contrast, in the rest of this thesis, an analogous treatment is provided to the particular
situation of VAR models with ﬁxed model order 1, but sparse matrix of VAR coeﬃcients,
denoted as A. Hence, it can be shown that the VAR process can be represented graphically.
Moreover, the use of sparsity as a modelling tool implies that we are able to develop
a model comparison problem.
This is due to the construction of a candidate set of
potential ‘A-graphs’, corresponding to sparse ‘A-matrices’. Our task therefore is, given
data modelled using a VAR(1) process, to ﬁnd the models that appear the most likely
from the set. That is, we want to estimate the unknown sparsity structure of A, the
autoregressive matrix. In this chapter, the zero mean case is considered, in Chapter 5,
non-zero mean models. Of course, we already know that the variational framework is
particularly attractive for this purpose as we can use LMi(qi), a tractable lower bound on
the logarithm of the marginal likelihood, inherent within the algorithm, to rank candidate
models.
3.2
VAR(1) graphical models
We commence by studying the family of VAR(p) models, in particular the VAR(1) process,
and showing how to model using sparsity. The zero mean VAR(p) process of dimension
d is expressed as
yt =
p

i=1
yt−iA(i) + et.
(3.1)
So, as Penny and Roberts (2002) indicate, the new, t-th value of the multivariate time
63

3.2. VAR(1) graphical models
series, yt, is explained via a linear combination of the p previous data values of the series.
Here, yt = (yt1, yt2, . . . , ytd), a (1 × d) vector, each A(i) is a d × d matrix of coeﬃcients
and et = (et1, et2, . . . , etd) is a (1 × d) noise-vector, distributed as
et ∼N (0, Γ).
Moreover, with et independent to eu for t ̸= u (i.e. Cov(et, eu) = 0) and with zero mean,
such vectors are deﬁned to be Gaussian white noise. We now focus in particular on the
following zero mean VAR(1) model:
yt = yt−1A + et,
(3.2)
where et is distributed as above.
The covariance matrix is now deﬁned as Γ = σ2Id
for unknown parameter σ2 and d × d identity matrix Id. This speciﬁcation implies that
all oﬀ-diagonal covariances are zero, i.e. Cov(eij, ekl) ̸= 0 if and only if (i, j) = (k, l)
for i, k = 1, . . .N and j, l = 1, . . . d, assuming N samples are collected.
Moreover,
Var(eij) = σ2. We deﬁne Γ in this way since, in a Bayesian analysis, it allows a simple,
univariate prior speciﬁcation on σ2 as opposed to needing a more complicated speciﬁcation
on a matrix, such as an inverse Wishart prior (c.f. Appendix A).
For a detailed analysis of VAR(p) models, refer to L¨utkepohl (2005). We now draw special
attention to the matrix A of VAR(1) coeﬃcients. For the purposes of what follows, it
is assumed that A is a sparse matrix. Hence, by deﬁnition, it will consist of many, in
particular oﬀ-diagonal, elements constrained to be zero, with only a few, unspeciﬁed non-
zero entries. A matrix of this ilk allows us to take advantage of the substantial number
of zeroes that it possesses. For instance, it can be related to a graphical structure. We
realise that, in a diﬀerent context, the pattern of zeroes in the concentration matrix of
an arbitrary multivariate Gaussian distribution provides the conditional independence
64

3.2. VAR(1) graphical models
structure, which subsequently characterises an existent graphical (Gaussian) model. This
is detailed in Appendix B.
Similarly, the sparse matrix A in a VAR(1) process, containing a clear zero structure,
can be represented graphically also. To do this, we must model (3.2) using a dynamic
graphical structure (Ghahramani, 1997; Friedman, Murphy, and Russell, 1998; Mihajlovic
and Petkovic, 2001). Of course, the same procedure can be applied in the non-zero mean
case. We realise that a Bayesian network (a graphical model with directed edges) is used
to describe the conditional dependencies between a ﬁxed set of random variables in a
static situation (see Appendix B).
Conversely, a dynamic Bayesian network is a special case of the afore-mentioned static
graphical model, speciﬁcally orientated towards modelling time series. Each time point,
at which the values of a set of random variables are observed, is often referred to as a time
slice. Within a network, directed edges connect nodes from one slice to the next, denoting
the dependencies of the corresponding variables. Such edges are sometimes called inter-
edges. A convention is adopted whereby inter-edges point in the direction of time, hence
illustrating that one variable can cause another, only if the latter is in the future.
Moreover, dynamic Bayesian networks can also contain edges within each slice, known as
intra-edges. In this case, the conditional dependencies between variables in a single time
slice are represented by a static Bayesian network. In other words, a dynamic Bayesian
network can be viewed as merely a collection of static Bayesian networks, linked by inter-
edges. Each dynamic network would contain not only the identical graphical structure for
every time slice, but, moreover, the identical dependencies between slices. Thus, notice
that the term ‘dynamic’ does not refer to the network changing over time slices, but
instead to the dynamic process being modelled.
VAR models can be represented as continuous-state dynamic Bayesian networks since
each node is a continuous random variable. In what follows, we consider no intra-edges
65

3.2. VAR(1) graphical models
in the graphical model. However, such edges, given as undirected connections, can be
used to specify the zero structure in the corresponding concentration matrix of the noise
vector et (Eichler, 2001).
In our case, the VAR model has order equal to 1.
Thus,
consider a dynamic Bayesian network between times t −1 and t, where each component
of yt−1 = (yt−1,1, yt−1,2, . . . , yt−1,d) and yt is a node. We use inter-edges to connect nodes
in these two successive time slices together and this pattern is repeated over all slices.
The network is correspondingly said to have order 1. By aggregating the nodes yti for
each i = 1, . . . , d across all time points t (in particular, from t −1 to t) into a single
node, say yi, in the time series graph, we can hence form a causality graph (Dahlhaus and
Eichler, 2003). Thus, each node represents one component of the whole time series. If a
component is dependent upon its own past, we allow this to be expressed by a directed
self-loop.
It has previously been documented that in such a time series graph with p = 1, for
a, b = 1, . . ., d, an edge exists between nodes yt−1,a and ytb if and only if the element
aba of the autoregressive matrix is non-zero (Eichler, 2001; Murphy, 2002; Dahlhaus and
Eichler, 2003). We note that such a result generalises to a VAR(p) model. Hence, there
is a clear link between the causality graph and A in this circumstance as the former is
deﬁned through the sparsity structure of the latter. Due to this relationship, the causality
graph is resultantly referred to as an A-graph throughout the remainder of this thesis.
The subsequent example elucidates the situation.
3.2.1
Example
Suppose d = 2. Consider Figure 3.1, showing a time series graph for a VAR(1) pro-
cess. As mentioned erstwhile for such a model, inter-edges are used to deﬁne a structure
between successive time slices, here shown repeated.
By letting yi = (yi1, yi2) where
i = t −2, t −1, t, the nodes on the graph are clearly speciﬁed by representing these ran-
66

3.2. VAR(1) graphical models
dom variables.
t −2
t −1
t
1
2
Figure 3.1: Time series graph for a VAR(1) model
By concentrating only on one pair of successive slices, we can hence produce a causality
graph across all time slices for this process, with nodes y1 and y2, as given below.
y1
y2
Figure 3.2: Causality graph for the VAR(1) process
Notice the use of self-loops for both nodes here. Using the above result of correspondence
between the dynamic Bayesian network, hence causality graph, and sparse A-matrix, we
have the speciﬁcation in this circumstance that A =
⎛
⎝∗
0
∗
∗
⎞
⎠, whereby ∗represents a
free, non-zero element. Thus, the graph of Figure 3.2 is termed an A-graph.
67

3.3. Scoring zero mean VAR(1) graphical models
3.3
Scoring zero mean VAR(1) graphical models
It is clear that the variational Bayesian method is of high relevance in terms of scoring
models. We now apply this to the particular case of zero mean VAR(1) models. Hence,
using the theory of Section 3.2, we construct a candidate set of graphical models, say
M = {M1, M2, . . .}. Each graphical model Mi relates to an A-graph, Gi, which, in turn,
corresponds to a sparse A-matrix, denoted by A(i). Recall that there exists an edge be-
tween two nodes of a given Gi if and only if the correct corresponding element of A(i) is
non-zero. We can quantify the evidence for each prospective graphical model with the
corresponding marginal likelihood, denoted as p({yt} | Mi). However, as previously dis-
cussed, we can approximate this quantity using a variational Bayesian framework and, in
particular, the tractable lower bound LMi(qi). Henceforth, we assume not only depen-
dence of the lower bound, but also conditioning in our distributions upon the graphical
model Mi, although not stated explicitly.
The subsequent set-up follows that of Penny and Roberts (2002). Assume there exists
t = 1, . . . , N independent samples of the time series. Therefore, to take account of and
store these samples, we rewrite (3.2) using matrix notation, and hence form a multivariate
linear model. Firstly, deﬁne xt = [yt−1] for all t = 1, . . . , N. Then, we form matrices Y ,
X and E, all of which have dimension N × d, such that the t-th row of each matrix is
respectively given by yt, xt and et. Consequently, using the deﬁnitions of these vectors,
we obtain a matrix equation such that
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
y11
. . .
y1d
...
...
...
yt1
. . .
ytd
...
...
...
yN1
. . .
yNd
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
=
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
x11
. . .
x1d
...
...
...
xt1
. . .
xtd
...
...
...
xN1
. . .
xNd
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
⎛
⎜
⎜
⎜
⎝
a11
. . .
a1d
...
...
...
ad1
. . .
add
⎞
⎟
⎟
⎟
⎠+
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
e11
. . .
e1d
...
...
...
et1
. . .
etd
...
...
...
eN1
. . .
eNd
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
.
68

3.3. Scoring zero mean VAR(1) graphical models
So, we can succinctly denote this as
Y = XA + E.
(3.3)
At the end-points, we take x1 = (0, 0, . . ., 0) and xN = yN−1. xN is speciﬁed directly
through the deﬁnition of xt. Correspondingly, we set x1 = y0 to equal the mean of the
stationary distribution. Of course, (3.2) is such that E(yt) = 0 for all t, i.e. all yt possess
this mean, regardless of t.
Next, by using the afore-mentioned matrix notation, we can now resultantly compute
the probability of the data, using ideas from L¨utkepohl (2005) and Box and Tiao (1992).
Assume a given data set D = {X, Y }. By using the vec operator, we now rewrite (3.3)
according to
vec(Y ) = vec(XA + E)
= vec(XA) + vec(E)
= (Id ⊗X)vec(A) + vec(E)
=⇒y = (Id ⊗X)a + e,
(3.4)
where y, e are both dN × 1 vectors and a is a d2 × 1 vector. That is, for example, y is
formed by stacking the columns of Y one under the other, similarly for e and a.
Here, we have used a core property of the vec operator: vec(P +Q) = vec(P)+vec(Q), for
compatible matrices P and Q (Petersen and Pedersen, 2007). In addition, we deﬁne ⊗to
be a Kronecker product (Henderson and Searle, 1981). Furthermore, suppose speciﬁcally
that P and Q are matrices of dimensions m × p and p × r respectively. Then, notice the
result from Henderson and Searle (1979) that
vec(PQ) = (Ir ⊗P)vec(Q) = (QT ⊗P)vec(Ip) = (QT ⊗Im)vec(P).
(3.5)
69

3.3. Scoring zero mean VAR(1) graphical models
Recall that et ∼N (0, σ2Id). We now determine the mean vector and covariance ma-
trix of e.
Clearly, E(e) = 0.
Moreover, to derive the covariance, we deﬁne e(s) =
(e1s, e2s, . . . , eNs)T, the error vector of the component s = 1, . . . , d for each of the N data
samples, i.e. the s-th column of E. Thus,
Var(e) = Var
⎛
⎜
⎜
⎜
⎜
⎝
e(1)
...
e(d)
⎞
⎟
⎟
⎟
⎟
⎠
=
⎛
⎜
⎜
⎜
⎝
Var

e(1)

. . .
Cov

e(1), e(d)

...
...
...
Cov

e(d), e(1)

. . .
Var

e(d)

⎞
⎟
⎟
⎟
⎠.
Assume henceforth that i, k = 1, . . . N and j, l, r, s = 1, . . .d. We know that Var(eij) =
σ2, for all i, j.
Thus, for each s, it follows that Var(e(s)) = σ2IN, an N × N ma-
trix. Moreover, from before, Cov(eij, ekl) = 0 if and only if (i, j) ̸= (k, l). Therefore,
Cov

e(r), e(s)

= 0 for all r ̸= s.
As E possesses d columns, consequently Var(e) = Id ⊗σ2IN by deﬁnition of the Kronecker
product. Therefore, e ∼N (0, Id⊗σ2IN). In other words, the probability density function
for e is denoted by
p(e | σ2) = (2π)−dN
2 ,,Id ⊗σ2IN
,,−1
2 exp

−1
2eT(Id ⊗σ2IN)−1e

.
(3.6)
Ultimately, to ﬁnd the likelihood of the data, we rearrange (3.4) in terms of e and sub-
stitute into the exponent of the above. By concentrating solely on this exponent for the
time being, this provides
exp

−1
2eT(Id ⊗σ−2IN)e

= exp

−1
2 [y −(Id ⊗X)a]T (Id ⊗σ−2IN) [y −(Id ⊗X)a]

= exp

−1
2 [vec(Y ) −vec(XA)]T (Id ⊗σ−2IN) [vec(Y ) −vec(XA)]

70

3.3. Scoring zero mean VAR(1) graphical models
= exp

−1
2 [vec(Y −XA)]T (Id ⊗σ−2IN) [vec(Y −XA)]

= exp

−1
2Tr
$
(Y −XA)Tσ−2IN(Y −XA)Id
%
= exp

−1
2Tr
$
σ−2g(A)
%
,
(3.7)
where, to ease notation, we let g(A) = (Y −XA)T(Y −XA), a d × d matrix. In addition,
notice, for compatible matrices P, Q and R, the use of the identity Tr(P TQPR) =
[vec(P)]T(RT ⊗Q)vec(P) (Henderson and Searle, 1979). Furthermore, when surveying
(3.6), we realise that
,,Id ⊗σ2IN
,,−1
2 =
,,σ2(Id ⊗IN)
,,−1
2 =
$
(σ2)dN|Id|N|IN|d %−1
2 = (σ2)−dN
2 .
Here, we use the identity that if P is m × m and Q is r × r, then |P ⊗Q| = |P|r |Q|m
(Muirhead, 1982). Finally then, the probability of the data is given by the expression
p(D |A, σ2) = (2πσ2)−dN
2 exp

−1
2Tr
$
σ−2g(A)
%
.
(3.8)
3.3.1
Priors
To perform in a Bayesian framework, we specify prior distributions over the parameter
set θ = {A, σ2}. In fact here, a prior is assigned over a = vec(A) where a is a d2-vector.
Note that the use of the vec operator to convert a matrix to a vector is a simple way to
deﬁne any distribution over a matrix. Thus, we have
p(a) = N (a | 0, C∗)
(3.9)
p(σ2) = IG(σ2 | α, β).
(3.10)
71

3.3. Scoring zero mean VAR(1) graphical models
As was noted in Chapter 2, the above, independent priors seem reasonable as they are the
semi-conjugate speciﬁcations for a normally distributed random sample with both mean
and variance unknown.
At this stage, attention is drawn to C∗. We wish to evaluate L(q) for diﬀerent choices
of sparsity of A, corresponding to diﬀerent graphical structures. Furthermore, we must
carry such a sparsity choice through the whole problem, implied by the A-matrices. As
a result, a prior distribution is chosen on vec A that imposes the sparsity structure, and
which appropriately distinguishes diﬀerent priors. So, we construct a matrix C = (cij)
such that, for each choice of A = (aij) and ∀i, j,
cij =
⎧
⎨
⎩
c
if aij ̸= 0
0
if aij = 0
,
(3.11)
for some ﬁxed constant c. Accordingly, deﬁne C∗= diag {vec(C)}, a natural choice for
the covariance matrix of size d2 × d2. Hence, the sparsity structure is maintained by
constraining C to be of the same form as A, and thus the prior distribution will vary,
dependent upon the sparsity structure for each A-matrix. So, we have eﬀectively speciﬁed
a prior only on the non-zero components of a. Whenever an element of sparse matrix A
is equal to zero (i.e. not present in the problem), the corresponding variance element of
C∗is thus constrained to zero. Moreover, the constant c represents the prior variance of
those elements of a that are present. Each non-zero element is given the equivalent prior
variance since we have no extra prior knowledge about the value of one these elements
over another.
This construction, for constraining certain prior variance elements to zero, is simple and el-
egant to apply. In addition, realise that C∗is usually not of full rank, i.e. its columns/rows
do not form a linearly independent set. This is unless A is a dense matrix, i.e. strictly
no zero elements, in which case the diagonal elements of C∗are all non-zero. When C∗
72

3.3. Scoring zero mean VAR(1) graphical models
is rank deﬁcient, this complicates subsequent analysis in terms of matrix inversion and
computing the logarithms of determinants. We shall make further mention of this, and
of maintaining sparsity structure, later.
3.3.2
Free form method
Recall that, in the variational approach, we intend to approximate each true posterior
by a variational distribution. In this case, two approximate posteriors are considered,
namely q(a | D) and q(σ2 | D). We continue initially by using the free form method to ﬁnd
the variational posteriors for σ2 and then a. However, we shall also make use of the ﬁxed
form method, as will be seen in due course.
So, by a free form perspective, recollect from Chapter 2 that one and only one assumption
is made, which aids the subsequent calculation: that these variational distributions are
independent, i.e. q(a, σ2 | D) = q(a | D) q(σ2 | D). However, the true posterior p(a, σ2 | D)
does not factorise in this way, and hence this assumption is an approximation. This is
clear as follows:
p(a, σ2 | D) ∝p(D | A, σ2) p(a) p(σ2)
= (σ2)−dN
2 exp

−1
2Tr
$
σ−2g(A)
%
× exp

−1
2aTC∗−1a

× (σ2)−(α+1) exp

−β(σ2)−1
= (σ2)−(α+ dN
2 +1) exp

−(σ2)−1
2
Tr [g(A)] −1
2aTC∗−1a −β(σ2)−1

.
Notice that g(·) is deﬁned only in terms of matrix A, not vector a, hence also the likelihood,
(3.8). However, Tr [g(A)] can be written in terms of a, as will be seen later. Nevertheless,
the term (σ2)−1
2
Tr [g(A)] implies that this posterior density will not factorise, and hence a
and σ2 are not a posteriori independent.
73

3.3. Scoring zero mean VAR(1) graphical models
Consequently, the lower bound in this context is given as
L(q) =

q(a, σ2 | D) log
	p(D | A, σ2) p(a, σ2)
q(a, σ2 | D)

da dσ2.
(3.12)
Due to the independence of both prior and approximating posterior distributions, this
can now be rewritten as a sum of integrals:
L(q) =

q(a | D)q(σ2 | D) log p(D | A, σ2) da dσ2 +

q(a | D) log p(a) da
+

q(σ2 | D) log p(σ2) dσ2 −

q(a | D) log q(a | D) da
−

q(σ2 | D) log q(σ2 | D) dσ2,
(3.13)
integrating out parameters where appropriate. Thus, by writing L(q) as a functional of
q(a | D), we derive
L(q) =

q(a | D)
	
q(σ2 | D) log p(D | A, σ2) dσ2 + log p(a) −log q(a | D)

da + const.
(3.14)
As a functional of q(σ2 | D), the lower bound is:
L(q) =

q(σ2 | D)
	
q(a | D) log p(D | A, σ2) da + log p(σ2) −log q(σ2 | D)

dσ2+const.
(3.15)
We inspect both of these equations in turn. Firstly, derive the variational distribution,
q(σ2 | D). By substituting in for both log p(D | A, σ2) and log p(σ2) in (3.15), we acquire
L(q) =

q(σ2 | D)
	 
q(a | D)

−dN
2 log 2πσ2 −1
2Tr
$
(σ2)−1g(A)
%
da
+ α log β −log Γ(α) −(α + 1) log σ2 −β(σ2)−1 −log q(σ2 | D)

dσ2 + const.
74

3.3. Scoring zero mean VAR(1) graphical models
By dropping terms independent of σ2, a new constant term is formed, and resultantly, we
obtain
L(q) =

q(σ2 | D)
	
−dN
2 log σ2 −(σ2)−1
2
Eq(a | D) {Tr [g(A)]}
−(α + 1) log σ2 −β(σ2)−1 −log q(σ2 | D)

dσ2 + const.′
(3.16)
Our attention is now focussed on the term Eq(a | D) {Tr [g(A)]}. As was commented upon
previously, Tr [g(A)] can be denoted in terms of a and, since here we take the expectation
with respect to the variational distribution of vec(A), this would be beneﬁcial. Therefore,
Tr [g(A)] = Tr
$
(Y −XA)T(Y −XA)
%
= [vec(Y −XA)]T [vec(Y −XA)]
= [vec(Y ) −vec(XA)]T [vec(Y ) −vec(XA)]
= [y −(Id ⊗X) a]T [y −(Id ⊗X) a]
(3.17)
=: h(a),
where y, a are as given previously, and the function h is deﬁned to ease notation. More-
over, we have used the following identity:
Tr(P TQ) = [vec(P)]Tvec(Q)
(3.18)
for compatible matrices P, Q (Henderson and Searle, 1979). We now take the expectation
of (3.17). Multiplying out the brackets and use of (2.51) consequently provides
Eq(a | D) {Tr [g(A)]} = Eq(a | D)

yTy −aT(Id ⊗XT)y −yT(Id ⊗X)a + aT(Id ⊗XTX)a

= yTy −Eq(a | D)

aT
(Id ⊗XT)y −yT(Id ⊗X)Eq(a | D) {a}
+ Eq(a | D)

aT(Id ⊗XTX)a

75

3.3. Scoring zero mean VAR(1) graphical models
= yTy −ρT(Id ⊗XT)y −yT(Id ⊗X)ρ + ρT(Id ⊗XTX)ρ
+ Tr
$
(Id ⊗XTX)τ
%
= h(ρ) + Tr
$
(Id ⊗XTX)τ
%
,
(3.19)
where we deﬁne ρ = Eq(a | D) {a} and τ = Varq(a | D) {a}. When deriving the variational
posterior for a, algebraic forms for ρ and τ will be found. Notice that
(P ⊗Q)(R ⊗S) = PR ⊗QS
(3.20)
for matrices P compatible with R, Q with S (Harville, 1997).
We can hence substitute (3.19) back into (3.16) to give an expression for L(q), a functional
of q(σ2 | D), which no longer depends upon a = vec(A). Now, in accordance with the
technique of Chapter 2, a Lagrangian νσ2 can be used to ensure that the distribution
q(σ2 | D) is normalised. Hence, the new functional ˜L(q) is formed, given by
˜L(q) = L(q) + νσ2

q(σ2 | D) dσ2 −1

.
(3.21)
Thus, we determine the maximum of ˜L(q) by computing the functional derivative with
respect to q(σ2 | D) and setting to zero. This gives
∂˜L(q)
∂q(σ2 | D) = −dN
2 log σ2 −(σ2)−1
2

h(ρ) + Tr
$
(Id ⊗XTX)τ
%
−(α + 1) log σ2 −β(σ2)−1 −log q(σ2 | D) −1 + νσ2 = 0.
By dropping constant terms and manipulating, we obtain
q(σ2 | D) ∝(σ2)−(α+ dN
2 +1) exp

−(σ2)−1

β + 1
2

h(ρ) + Tr
$
(Id ⊗XTX)τ
%
.
76

3.3. Scoring zero mean VAR(1) graphical models
Hence, it is immediately apparent that
q(σ2 | D) = IG(σ2 | γ, δ),
(3.22)
with variational parameters expressed as
γ = α + dN
2
(3.23)
δ = β + 1
2

h(ρ) + Tr
$
(Id ⊗XTX)τ
%
,
(3.24)
where the function h(·) is deﬁned in (3.17) and τ = Varq(a | D) {a}.
Notice that it is
also possible for (3.23) and (3.24) to be derived from the equations (2.53) and (2.54) in
Chapter 2.
Now, return to (3.14) and follow the identical procedure to ﬁnd q(a | D). Substituting in
for log p(D | A, σ2) and log p(a) implies that
L(q) =

q(a | D)
	 
q(σ2 | D)

−dN
2 log 2πσ2 −1
2Tr
$
(σ2)−1g(A)
%
dσ2
−d2
2 log 2π −1
2 log |C∗| −1
2aTC∗−1a −log q(a | D)

da + const.
This time, those terms that are independent of a will disappear under a functional deriva-
tive with respect to q(a | D). Therefore, we arrive at
L(q) =

q(a | D)
	
−1
2Tr [g(A)] Eq(σ2 | D)

(σ2)−1
−1
2aTC∗−1a −log q(a | D)

da + const.′
(3.25)
Furthermore, we already know that Eq(σ2 | D) {(σ2)−1} =
γ
δ . By forming ˜L(q) with the
Lagrange multiplier νa to enforce normality, we maximise this functional, now with respect
77

3.3. Scoring zero mean VAR(1) graphical models
to q(a | D). So, we acquire
∂˜L(q)
∂q(a | D) = −γ
2δTr [g(A)] −1
2aTC∗−1a −log q(a | D) −1 + νa = 0.
In a similar fashion to before, this can be rewritten as
q(a | D) ∝exp

−γ
2δTr [g(A)] −1
2aTC∗−1a

.
(3.26)
At this stage, we can express Tr [g(A)] more usefully in terms of a by using (3.17), as this
is the parameter for which we require the variational distribution. This hence removes
the dependence of (3.26) on A. In other words, we have
q(a | D) ∝exp

−1
2
	γ
δ yTy −γ
δ aT(Id ⊗XT) y −γ
δ yT(Id ⊗X)a
+ γ
δ aT(Id ⊗XTX)a −aTC∗−1a

∝exp

−1
2
	 
a −γ
δ
γ
δ (Id ⊗XTX) + C∗−1−1
(Id ⊗XT)y
T
×
γ
δ (Id ⊗XTX) + C∗−1
×

a −γ
δ
γ
δ (Id ⊗XTX) + C∗−1−1
(Id ⊗XT)y
 
,
via completing the square. Consequently, the variational posterior of a is distributed such
that
q(a | D) = N (a | ρ, τ),
(3.27)
and algebraic equations for ρ and τ have been derived such that
ρ = γ
δ
γ
δ (Id ⊗XTX) + C∗−1−1
(Id ⊗XT)y
(3.28)
τ =
γ
δ (Id ⊗XTX) + C∗−1−1
.
(3.29)
78

3.3. Scoring zero mean VAR(1) graphical models
In a way akin to the variational distribution on σ2, we realise that (3.28) and (3.29) are
applications of equations (2.47) and (2.48). At this time, the expression for δ is now fully
deﬁned. Optimal solutions for {γ, δ, ρ, τ} can be found by iteratively updating these
parameter values until convergence, using equations (3.23), (3.24), (3.28) and (3.29).
However, here we realise a problem. The above variational posterior for a is ﬁne when
the matrix A is dense. Yet, we also need to examine the circumstance when A is sparse.
Therefore, we must maximise the functional ˜L(q) with respect to some of the elements
of a being zero, dependent on the sparsity structure of each A-matrix. Recall that, when
specifying the prior p(a), a matrix C was created to have the identical zero structure
of a given A. Consequently, some of the prior variance elements of C∗were necessarily
constrained to zero.
Now similarly, for these same entries of A, we are to enforce the corresponding variational
posterior mean and variance elements of ρ and τ respectively to be zero. Thus, for each
graphical structure, ρ will be of the same form in terms of its dimension and sparsity
structure as a, likewise τ with the diagonal matrix C∗. To apply this constraint, a clean
and direct solution is now oﬀered.
3.3.3
Fixed form method
In the previous section, we have derived the variational distribution for a, and hence the
variational parameters, ρ and τ, in the dense case. However, dealing with a prescribed
sparsity structure using a free form approach is diﬃcult.
Fortunately, to handle this
problem, it turns out to be relatively straightforward to adopt the ﬁxed form variational
procedure.
Thus, now suppose that we assume ﬁxed parametric forms for the variational distributions
of both a and σ2.
To eﬀect this, as suggested in Chapter 2, we can simply use the
79

3.3. Scoring zero mean VAR(1) graphical models
parametric families suggested by the free form method, i.e.
q(a | D) = N (a | ρ, τ)
q(σ2 | D) = IG(σ2 | γ, δ).
Then, as mentioned previously, the lower bound is derived initially using the known
variational distributions. Ordinarily thereafter, we would optimise L(q) with respect to
the variational parameter set. However, update equations for γ and δ, namely (3.23) and
(3.24), have erstwhile been computed via the free form approach, and q(σ2 | D) does not
depend upon the sparsity of A. Thus, we need only to consider maximising with respect
to ρ and τ. At this point, the sparsity constraint can be enforced.
Consider once again (3.13). With assumed knowledge of the variational posteriors, these
integrals can now be computed in turn. Therefore ﬁrstly, by (3.8), we obtain

q(a | D)q(σ2 | D) log p(D | A, σ2) da dσ2
=

q(a | D)q(σ2 | D)
	
−dN
2 log 2πσ2 −1
2Tr
$
(σ2)−1g(A)
%
da dσ2
= −dN
2 log 2π −dN
2

q(σ2 | D) log σ2 dσ2
−1
2

q(a | D)Tr [g(A)] da

q(σ2 | D)(σ2)−1 dσ2
= −dN
2 log 2π −dN
2 [log δ −ψ(γ)] −γ
2δ

h(ρ) + Tr
$
(Id ⊗XTX)τ
%
.
Furthermore, it is realised that the ﬁnal line requires the use of (3.19) and (A.6). Moreover,
by (2.51),

q(a | D) log p(a) da
=

q(a | D)
	
−d2
2 log 2π −1
2 log |C∗| −1
2aTC∗−1a

da
80

3.3. Scoring zero mean VAR(1) graphical models
= −d2
2 log 2π −1
2 log |C∗| −1
2Eq(a | D) {a}T C∗−1Eq(a | D) {a}
−1
2Tr

C∗−1Varq(a | D) {a}

= −d2
2 log 2π −1
2 log |C∗| −1
2ρTC∗−1ρ −1
2Tr

C∗−1τ

.
Similarly, using previous results,

q(σ2 | D) log p(σ2) dσ2
=

q(σ2 | D)
$
α log β −log Γ(α) −(α + 1) log σ2 −β(σ2)−1%
dσ2
= α log β −log Γ(α) −(α + 1)[log δ −ψ(γ)] −βγ
δ .
In addition, by knowledge of the variational for a, we have that

q(a | D) log q(a | D) da
=

q(a | D)
	
−d2
2 log 2π −1
2 log |τ| −1
2(a −ρ)Tτ −1(a −ρ)

da
= −d2
2 log 2π −1
2 log |τ| −1
2 Eq(a | D)

[a −ρ]T
τ −1Eq(a | D) {a −ρ}
−1
2Tr
$
τ −1Varq(a | D) {a −ρ}
%
= −d2
2 log 2π −1
2 log |τ| −d2
2 .
Finally, again by (A.6), it is clear that

q(σ2 | D) log q(σ2 | D) dσ2
=

q(σ2 | D)
$
γ log δ −log Γ(γ) −(γ + 1) log σ2 −δ(σ2)−1%
dσ2
= −log Γ(γ) −log δ + (γ + 1)ψ(γ) −γ.
81

3.3. Scoring zero mean VAR(1) graphical models
Therefore, by simplifying all integral computations from (3.13), the lower bound is found
to be
L(q) = −dN
2 log 2π −dN
2 log δ + dN
2 ψ(γ) −γ
2δ

h(ρ) + Tr
$
(Id ⊗XTX)τ
%
−1
2 log |C∗| −1
2ρTC∗−1ρ −1
2Tr

C∗−1τ

+ α log β −log Γ(α) −α log δ
+ αψ(γ) −βγ
δ + 1
2 log |τ| + d2
2 + log Γ(γ) −γψ(γ) + γ.
(3.30)
Resultantly, we have calculated L(q), the quantity required to approximate the log marginal
likelihood for each graphical model Mi, using the ﬁxed form method.
Having derived the lower bound, to optimise, the partial diﬀerentiation of L(q) with
respect to ρ and τ is now examined. Of course, if L(q) is maximised with respect to γ
and δ, then, by setting to zero and manipulating, we acquire the same iterative equations
for these variational parameters, namely (3.23) and (3.24), as in the free form method.
However, recall that the sparsity structure only needs to be enforced for the variational
distribution q(a | D). Thus, ﬁrst by maximising with respect to ρ, it is established that
∂L(q)
∂ρ
= ∂
∂ρ

−γ
2δh(ρ) −1
2ρTC∗−1ρ

= ∂
∂ρ

−γ
2δ
$
yTy −ρT(Id ⊗XT)y −yT(Id ⊗X)ρ + ρT(Id ⊗XTX)ρ
%
−1
2ρTC∗−1ρ

= ∂
∂ρ

ρT
	
−γ
2δ (Id ⊗XTX) −1
2C∗−1
ρ + γ
δ yT(Id ⊗X)ρ

,
where ρT(Id ⊗XT)y =
$
yT(Id ⊗X)ρ
%T = yT(Id ⊗X)ρ, as we transpose a scalar quantity.
Maximising this expression per se is fairly straightforward, requiring some standard matrix
calculus. However, this is a logical point at which we can introduce the constraint of some
of the elements of ρ being zero, dependent on the sparsity of each A-matrix, as explained
82

3.3. Scoring zero mean VAR(1) graphical models
earlier. Thus, we observe the beneﬁt of using the ﬁxed form method in this context.
Accordingly, the problem reduces to a quadratic programming (QP) problem. Classically,
this features the minimisation of a quadratic function, subject to a set of linear constraints.
For more details on this topic, see Fletcher (2000). In this case, the constraint is able to
be handled more easily and, in fact, the problem is speciﬁed as:
max
ρ
ρTHρ + cTρ
(3.31)
subject to:
some elements of ρ constrained to be zero,
where
H = −γ
2δ (Id ⊗XTX) −1
2C∗−1
cT = γ
δ yT(Id ⊗X).
Suppose that a given A-matrix contains η free, non-zero elements with a prescribed spar-
sity structure. This structure is moreover inherent within ρ and thus, to solve the QP
problem, we subsequently maximise with respect to the non-zero elements of ρ. To eﬀect
this, we initially must permute rows and columns of (3.31) so that the ﬁrst η elements of
ρ are now these non-zeroes. The corresponding η-vector is deﬁned to be ρ1. Henceforth,
we practise in terms of block matrices.
Thus, after permuting rows and columns, deﬁne ρperm =
⎡
⎣ρ1
0
⎤
⎦. Moreover, let Hperm =
⎡
⎣H11
H12
H21
H22
⎤
⎦, whereby H11 is of dimension η × η, H12 is η × (d2 −η), H21 (d2 −η) × η
and H22 is a (d2 −η) × (d2 −η) matrix. In other words, H11 is the submatrix obtained
by deleting the i-th row and column of H, corresponding to a zero element in the i-th
83

3.3. Scoring zero mean VAR(1) graphical models
position of ρ for all i. Likewise, c T
perm =
⎡
⎣c1
c2
⎤
⎦
T
, where c1, a vector of dimension η, is the
analogous subvector of c, and c2 is of size d2 −η.
Hence, by substituting into (3.31), the problem reduces to
max
ρ1
⎡
⎣ρ1
0
⎤
⎦
T ⎡
⎣H11
H12
H21
H22
⎤
⎦
⎡
⎣ρ1
0
⎤
⎦+
⎡
⎣c1
c2
⎤
⎦
T ⎡
⎣ρ1
0
⎤
⎦
= max
ρ1
ρT
1 H11ρ1 + cT
1 ρ1.
Optimising this expression with respect to the non-zero vector, ρ1, is now elementary
since
∂
∂ρ1

ρT
1 H11ρ1 + cT
1 ρ1

= (H11 + H
T
11 )ρ1 + c1
= 2H11ρ1 + c1
=

−γ
δ (Id ⊗XTX) −C∗−1
11 ρ1 +
γ
δ (Id ⊗XT) y

1 ,
using the deﬁnitions of H and cT, in addition to the continuing subscript notation. We
apply standard results for the matrix calculus required here (Petersen and Pedersen,
2007). Observe that as H is symmetric, then by only removing rows and corresponding
columns, H11 is also a symmetric matrix. Ultimately, setting to zero and solving for ρ1
provides
ρ1 = γ
δ
γ
δ (Id ⊗XTX) + C∗−1
11
−1 $
(Id ⊗XT) y
%
1 .
(3.32)
Finally, it is recognised that once ρ1 has been found, then ρ is reformed by re-introducing
the sparsity structure, in accordance with the speciﬁc A-matrix. We realise that this
reconstruction is needed as, for instance, (3.24) is strictly reliant upon the full vector ρ.
84

3.3. Scoring zero mean VAR(1) graphical models
In summary, when A is dense, we need only utilise (3.28) to ﬁnd ρ. However, when A
is sparse, ρ1 must be computed initially using (3.32), before re-constructing ρ. This is
easily performed by choosing the correct block of H (i.e. H11) and c (i.e. c1) every time.
Thus, notice the obvious analogy between the dense and the sparse case from (3.28) and
(3.32).
So, we have constrained, according to the speciﬁc sparsity structure of each A-matrix,
some elements of the variational posterior mean vector, ρ, to be zero. The same operation
can now be performed to constrain to zero the corresponding elements of τ. Customarily,
when diﬀerentiating (3.30) with respect to τ, we would need to ﬁnd
∂L(q)
∂τ
= ∂
∂τ

−γ
2δTr
$
(Id ⊗XTX)τ
%
−1
2Tr

C∗−1τ

+ 1
2 log |τ|

.
Clearly, in this case, we cannot enforce the sparsity constraint by using simple quadratic
programming as was seen with ρ. Consequently, an alternative approach is to attempt
the problem again in component form, and ﬁnd an expression for the elements of τ that
correspond to those elements of C∗, which have non-zero, prior variance. Obviously, by
the sparsity structure, if an element of C∗is constrained to have zero prior variance, then
the corresponding element of τ will have zero variational posterior variance.
The prior over σ2 is maintained to be of the same form as before since σ2 is unaﬀected
by sparsity. Yet, as C∗is a diagonal matrix, we can use a well-known property of the
multivariate Gaussian distribution to denote the prior over a as a product of independent,
univariate Gaussians, i.e.
p(a) =

(p,q)∈I
N (apq | 0, C∗
(p,q)),
(3.33)
where I is the set of those elements of a (corresponding to A) for which apq ̸= 0. We then
can use a ﬁxed form method to proceed. Previously, we assumed that the joint variational
85

3.3. Scoring zero mean VAR(1) graphical models
posterior, q(a, σ2 | D) could be factorised, an approximation to the true posterior. Now,
a further approximation is made at the component level to p(a, σ2 | D), namely that,
moreover, the variational distribution for a can be factorised into a product of univariate
Gaussian distributions, i.e. we let
q(a | D) =

(p,q)∈I
N (apq | ρ(p,q), τ(p,q)),
(3.34)
where ρ(p,q) is the variational posterior mean that corresponds to element apq of a, similarly
τ(p,q). We now continue in parallel with the ﬁxed form method in the multivariate case
by evaluating (3.13) at the component level.
Initially, the likelihood (3.8) is rewritten in component form. Realising, for an m × r
matrix P, that by Harville (1997),
Tr
$
P TP
%
=
m

j=1
r

k=1
p2
jk,
(3.35)
consequently we acquire, by deﬁnition of g(A) and matrix multiplication,
p(D | {aij}, σ2) = (2πσ2)−dN
2 exp

−(σ2)−1
2
Tr
$
(Y −XA)T(Y −XA)
%
= (2πσ2)−dN
2 exp

−(σ2)−1
2
N

j=1
d

k=1

[Y −XA]jk
2

= (2πσ2)−dN
2 exp
⎧
⎨
⎩−(σ2)−1
2
N

j=1
d

k=1

yjk −
d

i=1
xjiaik
2⎫
⎬
⎭.
Now, examine the ﬁrst integral of (3.13):

q(a | D)q(σ2 | D) log p(D | {aij}, σ2) da dσ2
86

3.3. Scoring zero mean VAR(1) graphical models
=

q(a | D)q(σ2 | D)
⎡
⎣−dN
2 log 2πσ2 −(σ2)−1
2
N

j=1
d

k=1

yjk −
d

i=1
xjiaik
2⎤
⎦da dσ2
= −dN
2 log 2π −dN
2 [log δ −ψ(γ)] −γ
2δ
N

j=1
d

k=1

q(a | D)

yjk −
d

i=1
xjiaik
2
da,
(3.36)
using (A.5) and (A.6). The ﬁnal integral in (3.36) is equivalent to
Eq(a | D)
⎧
⎨
⎩

yjk −
d

i=1
xjiaik
2⎫
⎬
⎭
= y2
jk −2yjkEq(a | D)

d

i=1
xjiaik

+ Eq(a | D)
⎧
⎨
⎩

d

i=1
xjiaik
2⎫
⎬
⎭
= y2
jk −2yjk
d

i=1
xjiEq(a | D){aik} +
d

i=1
Varq(a | D){xjiaik} +

d

i=1
xjiEq(a | D){aik}
2
= y2
jk −2yjk
d

i=1
xjiρ(i,k) +
d

i=1
x2
jiτ(i,k) +

d

i=1
xjiρ(i,k)
2
,
noting that, as τ is diagonal, the covariance between elements of a with respect to q(a | D)
is always zero. Furthermore, we obtain, by (3.33), that

q(a | D) log p(a) da
=

(p,q)∈I

q(a | D)

−1
2 log 2π −1
2 log C∗
(p,q) −1
2
a2
pq
C∗
(p,q)

da
= −

(p,q)∈I

1
2 log 2π + 1
2 log C∗
(p,q) +
1
2C∗
(p,q)

Varq(a | D){apq} +
$
Eq(a | D){apq}
%2
= −

(p,q)∈I

1
2 log 2π + 1
2 log C∗
(p,q) + 1
2
τ(p,q)
C∗
(p,q)
+ 1
2
ρ2
(p,q)
C∗
(p,q)

.
87

3.3. Scoring zero mean VAR(1) graphical models
Moreover, by (3.34),

q(a | D) log q(a | D) da
=

(p,q)∈I

q(a | D)
	
−1
2 log 2π −1
2 log τ(p,q) −1
2
(apq −ρ(p,q))2
τ(p,q)

da
= −

(p,q)∈I
	1
2 log 2π + 1
2 log τ(p,q)
+
1
2τ(p,q)
$
Eq(a | D){apq −ρ(p,q)}
%2 + Varq(a | D){apq −ρ(p,q)}
 
= −

(p,q)∈I
	1
2 log 2π + 1
2 log τ(p,q) + 1
2

.
The two other integrals of (3.13) are independent of a, and so are calculated as previous.
Therefore, in component form, the lower bound is equivalent to
L(q) = −dN
2 log 2π −dN
2 [log δ −ψ(γ)] −γ
2δ
N

j=1
d

k=1
y2
jk + γ
δ
d

i=1
N

j=1
d

k=1
yjkxjiρ(i,k)
−γ
2δ
d

i=1
N

j=1
d

k=1
x2
jiτ(i,k) −γ
2δ
N

j=1
d

k=1

d

i=1
xjiρ(i,k)
2
−1
2

(p,q)∈I
log C∗
(p,q)
−1
2

(p,q)∈I
τ(p,q)
C∗
(p,q)
−1
2

(p,q)∈I
ρ2
(p,q)
C∗
(p,q)
+ α log β −log Γ(α) −α log δ + αψ(γ)
−βγ
δ + 1
2

(p,q)∈I
log τ(p,q) +

(p,q)∈I
1
2 + log Γ(γ) −γψ(γ) + γ.
(3.37)
Finally, we optimise L(q) by diﬀerentiating with respect to the component τ(p,q) to obtain
∂L(q)
∂τ(p,q)
=
∂
∂τ(p,q)
⎧
⎨
⎩−γ
2δ
d

i=1
N

j=1
d

k=1
x2
jiτ(i,k) −1
2

(p,q)∈I
τ(p,q)
C∗
(p,q)
+ 1
2

(p,q)∈I
log τ(p,q)
⎫
⎬
⎭
= −γ
2δ
N

j=1
x2
jp −
1
2C∗
(p,q)
+
1
2τ(p,q)
.
88

3.3. Scoring zero mean VAR(1) graphical models
By equating to zero and solving for τ(p,q), we have that each non-zero diagonal element of
τ is given by
τ(p,q) =

1
C∗
(p,q)
+ γ
δ
N

j=1
x2
jp
−1
.
(3.38)
Consequently, we can express the diagonal elements of τ such that
τ(p,q) =
⎧
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎩

1
C∗
(p,q)
+ γ
δ
N

j=1
x2
jp
−1
if apq ̸= 0
0
if apq = 0
.
Clearly, by diﬀerentiating (3.37) with respect to γ and δ, we reach the same update
equations as before, but in component form. A genuine question to ask at this stage
would be why not use this method to ﬁnd an expression for any ρ(p,q), corresponding to
a non-zero element apq, rather than the quadratic programming method, as examined
earlier. In this case, maximising with respect to ρ(p,q), we acquire
∂L(q)
∂ρ(p,q)
=
∂
∂ρ(p,q)
γ
δ
d

i=1
N

j=1
d

k=1
yjkxjiρ(i,k) −γ
2δ
N

j=1
d

k=1

d

i=1
xjiρ(i,k)
2
−1
2

(p,q)∈I
ρ2
(p,q)
C∗
(p,q)

= γ
δ
N

j=1
yjqxjp −γ
2δ
N

j=1
d

t=1
xjpxjtρ(t,q) −ρ(p,q)
C∗
(p,q)
,
where we note that  N
j=1
 d
k=1
 d
i=1 xjiρ(i,k)
2
=  N
j=1
 d
k=1
 d
t=1
 d
i=1 xjixjtρ(i,k)ρ(t,k).
However, by equating to zero and solving for ρ(p,q), we realise that the expression is not
independent of  d
t=1 ρ(t,q). Hence, the quadratic programming method to constrain ρ
according to sparsity is preferred.
To summarise, we have derived the distributional form for both variational posteriors,
namely q(a | D) and q(σ2 | D), together with update equations for the set of variational
89

3.4. Other issues
parameters, initially γ and δ, then ρ (also ρ1) and τ (using τ(p,q)). These update equa-
tions are run until convergence, hence ﬁnding the parameter values for our variational
distributions. At each iteration for every graph, we can evaluate the lower bound (3.30)
until convergence, thereupon giving a good approximation of the log marginal likelihood.
This provides the evidence needed to rank the graphical structures available from the
candidate set of models.
3.4
Other issues
3.4.1
Problems with computation
We comment brieﬂy upon the computational issues of the matrices C∗and τ. Notice that
in (3.30), the expression for L(q), we must compute log |C∗|, log |τ| and C∗−1. Yet, these
afore-mentioned matrices by construction have all oﬀ-diagonal entries equal to zero and,
unless A is dense, contain some zero elements on the leading diagonal. Thus, when A is
sparse, the determinant of C∗and τ will typically be zero and, hence, the logarithm of
the determinant is undeﬁned. Moreover by deﬁnition, both matrices will be consequently
singular, implying a problem in calculating the inverse of C∗.
However, these dilemmas can be overcome.
To understand this, suppose that X is a
random vector that follows a Nr(m, V ) distribution, where the subscript r is used to
emphasise the dimension of X. If V is singular (with rank k < r), then the standard
multivariate normal density function does not exist on Rr. However, it does exist on a
k-dimensional subspace of Rr where the distribution has support. In addition, the density
of X on this subspace is deﬁned by Rao and Mitra (1972) as
p(x | m, V ) =
(2π)−k/2
(λ1 · · ·λk)1/2 exp

−1
2(x −m)TV −(x −m)

,
(3.39)
90

3.4. Other issues
where λ1, . . . , λk are the non-zero eigenvalues of V , and V −is any generalised inverse
of V (see Appendix C). Thus, we refer to (3.39) as the density of a singular Nr(m, V )
distribution of rank k.
Of course, there is a clear correspondence between (3.39) and its non-singular counterpart,
(A.7). That is, the singular density may be computed on the subspace by alternative
calculation of the determinant and inverse of V in the density of full rank. We use this
relationship to justify the following analysis. If a given A-matrix has η non-zero elements
as before, then by construction, C∗, and consequently τ, will have rank η < d2 (recall
that the rank of any diagonal matrix is equivalent to the number of non-zero diagonal
elements that it possesses). So, analogous to (3.39) where V was again a singular, positive
semideﬁnite matrix, the determinant of C∗(and hence τ) can also be calculated as the
product of its non-zero eigenvalues (Neudecker, 1995), namely its η, non-zero diagonal
entries. If c∗
11, . . ., c∗
ηη are these elements, then resultantly, by taking logarithms, we easily
have
log |C∗| =
η

i=1
log c∗
ii.
(3.40)
The case is similar for τ.
Additionally, if it were non-singular, the inverse of C∗would trivially be the matrix
with the diagonal elements of C∗replaced by their reciprocals. Yet, when A is sparse,
a generalised inverse can be utilised for this procedure, as seen in (3.39). Moreover, a
generalised inverse of a diagonal matrix is formed by reciprocating only the non-zero,
diagonal entries (Harville, 1997). In fact, this is the Moore-Penrose inverse of the matrix,
denoted in this case by C∗+ — additional details are again provided in Appendix C.
Consequently, in (3.30), the lower bound expression is altered so that C∗−1 is replaced by
C∗+.
Finally, the lower bound contains terms that remain constant across diﬀerent models.
91

3.4. Other issues
Thus, we can rewrite (3.30) as
L(q) ∝−dN
2 log δ −γ
2δ

h(ρ) + Tr
$
(Id ⊗XTX)τ
%
−1
2 log |C∗|
−1
2ρTC∗+ρ −1
2Tr

C∗+τ

−α log δ −βγ
δ + 1
2 log |τ|,
(3.41)
and hence use the above expression to provide evidence for the competing, candidate
models.
3.4.2
Speciﬁcation of priors
We discuss speciﬁcation of the parameter values for both prior distributions included in
the model. Such issues were succinctly touched upon in Section 2.5.5. In the case of p(σ2),
it is customary to want the prior to have little inﬂuence over the resulting approximate
posterior distribution. Thus, we aim to use a vague prior. A popular choice is to apply the
relatively ﬂat, proper prior IG(ϵ, ϵ) for low values of ϵ and, in particular, when ϵ = 0.001
(Spiegelhalter et al., 1995). By Appendix A, any IG(a, b) distribution is deﬁned only
when a, b > 0, and so this speciﬁcation is thus considered to be a ‘just’ proper prior. As
mentioned previously, the IG(0.001, 0.001) distribution is described as vague since it has
very large (in this case, inﬁnite) variance.
However, it has been seen, in the context of hierarchical models, that the resultant pos-
terior distribution can be highly sensitive to the choice of ϵ, when the variance parameter
in question is estimated to be small a posteriori (Gelman, 2006). In this case, the author
showed that, in particular for one dataset, the prior was perversely not at all vague. We
realise that this prior is highly peaked for small σ2, and so may show a preference for
lower values of σ2 in the Bayesian update. Perhaps a more attractive choice would be to
use a IG(1, ϵ) density for ϵ →0. When ϵ = 0.001 say, this prior has a maximum around
σ2 = 0 as before. Yet, this peak is now extremely sharp. As such, the density reaches
92

3.4. Other issues
negligible values quicker than before, implying that the prior is then ﬂatter.
Customarily, for the choice of a Gaussian prior, we again represent prior ignorance by
choosing the distribution to have large variance. However, when choosing between models,
this policy may be susceptible to Lindley’s paradox. Introduced initially in Chapter 1, we
now explain the paradox, following that of Shafer (1982). Suppose a random quantity X
follows a Gaussian distribution with unknown mean μ and known variance ω2. On the
basis of observing a dataset D of size n with sample mean ¯x, we want to evaluate the
evidence for two models, which may have given rise to the data. These are:
Mk : X ∼N (μ0, ω2)
Ml : X ∼N (μ, ω2).
According to Ml, we place a diﬀuse prior over μ, centred at μ0, e.g. μ ∼N (μ0, d2) for
large d. We then can compute the Bayes’ factor for model Mk against Ml, as illustrated
in Chapter 1:
Bkl = p(D | Mk)
p(D | Ml) .
Yet, as Shafer illustrates, by allowing the prior variance d2 to become suﬃciently large,
the Bayes’ factor, in turn, will become signiﬁcantly greater than 1, and hence, Mk will
be favoured always ahead of Ml.
Here, we realise that model Ml can be written as
X ∼N (μ0, d2 + ω2). Thus, with increasing d, Ml becomes more complex and Bkl works
in favour of the simpler Mk. The paradox arises since a standard, hypothesis test, in par-
ticular Z = ¯x −μ0
ω/√n , may show strong evidence against model Mk, whilst simultaneously,
the Bayesian assessment can display exactly the reverse conclusion. In other words, even
if the sample mean of the data is signiﬁcantly diﬀerent from μ0, then, by making the
prior as ﬂat as necessary, this will override the evidence, provided by the data, and hence
suggest that μ = μ0. This conﬂicts with the interpretation of the Bayes’ factor as being
93

3.4. Other issues
the odds of the two models, implied solely by the data. For further discussion, see, for
instance, Robert (1993), Berger and Sellke (1987) and Aitkin (1991).
The converse of the paradox is the scenario when the prior variance is allowed to ap-
proach zero. Then, the prior, now highly informative for μ, will be tightly-peaked around
μ0, implying that those values of μ, in the vicinity of μ0, will be given very high prior
probability. If the sample mean ¯x is reasonably close to the value of μ0, then these same
values will also possess high likelihood. Deﬁne L(μ | D, Ml) = p(D | μ, Ml) as the like-
lihood function, speciﬁc to Ml. Hence, the marginal likelihood for this model, namely
p(D | Ml) =

L(μ | D, Ml) p(μ | Ml) dμ, denoting the average of the likelihood with re-
spect to the prior, will increase, hence reducing the Bayes’ factor to below 1, and thus
favouring Ml.
In the above example of Lindley’s paradox, we can consider Mk to be the simpler model
and Ml the more complex model. In eﬀect, we choose a model either with mean equal
to μ0 or with unknown mean μ. This situation corresponds to the current, zero mean
VAR(1) case if we take μ0 = 0. Then, we choose each element of the d × d matrix A to
be either a zero or a free entry, whereby the latter follows a prior distribution of the form
given by (3.33). That is, in each case, we again examine a simpler or more complex model
respectively. When specifying the prior, it seems reasonable to take the mean as zero,
hence the prior distribution is centred around the simpler model for each component.
By carefully specifying a value not too small for the prior variance on each non-zero
element of a, we will be able to penalise more complex A-matrices in the candidate set,
i.e.
those with more unspeciﬁed, non-zero entries.
Although such models with more
parameters will be better at ﬁtting the data, by penalising, it will enable us to choose the
model with optimum structure. On the other hand, the use of a diﬀuse prior on a may
lead to favouring an A-matrix of simpler structure. Hence, there is a justiﬁcation to use
a deliberately chosen informative prior so that the problem is not susceptible to Lindley’s
paradox. Moreover, the speciﬁcation of c from (3.11) must be a compromise between
94

3.5. Toy example
always favouring a simpler A-matrix (large c) and a more complex matrix (small c). A
full prior sensitivity analysis, examining these issues, is provided in the next chapter.
3.5
Toy example
To illustrate the above procedure, we consider a simple toy example, based upon an
arbitrary, zero mean VAR(1) model. In previous work, we have speciﬁed distributions
over a = vec A and found that Eq(a | D)(a) = ρ. In the following example, we unstack
the d2-vector ρ to form a d × d matrix called ˆA. This procedure is illustrated further in
Section 5.2.2.
An easy case is supposed whereby d = 2. A dataset of size N = 250 was simulated from
the zero mean VAR(1) model (3.2) with speciﬁcations such that A =
⎛
⎝0
0.7
0.3
0
⎞
⎠and
σ2 = 0.1. We choose A with care to ensure that all its eigenvalues have modulus less
than 1. In this case, the VAR process is said to be stable and, hence, the dataset does
not explode for increasing N (L¨utkepohl, 2005). As mentioned previously, we again let
x1 = (0, 0) and x250 = y249. This choice of A thus deﬁnes the A-graph below.
y1
y2
Figure 3.3: A-graph for the true zero mean VAR(1) model
When d = 2, there are 24 = 16 directed A-graphs on two vertices. However, the null
model, represented by a completely sparse graph, is ignored. In such a case, C∗and
τ would both be zero matrices, implying that taking the logarithm of their determi-
nants, as required in the computation of LMi(qi), would be undeﬁned (c.f. (3.40)). As a
consequence, we construct a candidate set of 15 graphs, referred to in terms of their cor-
95

3.5. Toy example
responding A-matrices. Thus, a zero element in a given matrix implies no edge between
the corresponding vertices on the graph, as explained earlier. So, given the data, we aim
to select the optimum model from the set. The prior distributions over a = vec(A) and
σ2 were speciﬁed to be
p(a) = N (a | 0, C∗) where cij ∈{0, 0.5}
p(σ2) = IG(σ2 | 1, 0.001).
Clearly, the prior on σ2 remains the same throughout whereas, for that on a, the covariance
matrix C∗changes according to the sparsity structure of each A-matrix.
Recall that we wish to ﬁnd parameter values for variational distributions given by
q(a | D) = N (a | ρ, τ)
q(σ2 | D) = IG(σ2 | γ, δ).
Consequently, for each candidate model, the lower bound, LMi(qi), was evaluated at each
iteration, and update equations for the variational parameters were run until convergence.
For this medium-sized dataset, this took merely 4 iterations in each case, identical to what
was seen in the example of Section 2.5.5. As the update equations for ρ1 and τ(p,q) (hence
deﬁning ρ and τ) depend upon γ and δ, the algorithm was initialised arbitrarily with
γ = δ = 1.
The results of the example are shown in Table 3.1.
Initially, we report back the log
marginal likelihood estimation by LMi(qi). As we would hoped, the model which relates
to the true choice of A, in terms of sparsity structure, was chosen. The more complex
models with A-matrices that contained at least the two, true free elements were also well-
favoured. Recall that the prior parameters for a were chosen to avoid Lindley’s paradox.
The speciﬁcation made appears to be a valid one since neither the simpler, nor more
96

3.5. Toy example
complex models are favoured, ahead of the truth.
Now, we examine the estimates of the true A and σ2, namely ˆA and Eq(σ2 | D){σ2} respec-
tively. Notice that all the non-zero elements of each ˆA are very similar to each other, but
not mathematically exact. This stems from the variational algorithm and, in particular,
the deﬁnition of ρ1. For each candidate model, H11 and c1 will vary in dimension and
value of elements, dependent on the sparsity structure, hence creating such a diﬀerence.
Moreover, C∗will also change from model to model. With a reasonably-sized dataset
chosen, each ˆA-matrix is similar to the original choice of A. This is since, with N = 250,
the likelihood term dominates the prior distribution. That is, with a prior chosen so that
no favouritism exists for either simpler or more complex models, most of the information
about a was passed through to the variational distribution q(a | D), in this approximate
Bayesian update, via the data.
The corresponding estimates for σ2, found using (A.4), are all reasonably accurate to the
true value. However, there is less concordance between these values than the ˆA-matrices.
It is apparent that any candidate that possessed a sparsity structure similar to the truth
produced better estimates of σ2 than the remaining models. We are already aware that
γ is constant across models (c.f. (3.23)). Thus, if the wrong model is chosen, the value
of δ has become more inaccurate when compared with that for the true model, hence
Eq(σ2 | D) {σ2} also. Suppose we attempt to ﬁt a model with the wrong sparsity pattern to
the given set of data. Then, the noise variance, which determines the extent to which the
data ﬂuctuates about the mean of the process (in this case, zero), must be readjusted to
cope with this model misspeciﬁcation. That is, the model error, created by attempting
to model the data incorrectly, is ‘pushed’ exclusively into the estimate of σ2.
97

3.5. Toy example
Speciﬁcation
Posterior means
LMi(qi)
A-matrix
ˆA-matrix
Eq(σ2 | D){σ2}
∗
0
0
0

−0.066
0
0
0

0.135
−1136.791
0
0
0
∗

0
0
0
−0.041

0.135
−1137.165
0
∗
0
0

0
0.674
0
0

0.109
−1084.561

0
0
∗
0


0
0
0.341
0

0.126
−1119.862
∗
∗
0
0

−0.066
0.674
0
0

0.109
−1086.926
∗
0
∗
0

−0.074
0
0.342
0

0.126
−1122.113
0
∗
0
∗

0
0.676
0
−0.052

0.110
−1087.169

0
0
∗
∗


0
0
0.341
−0.041

0.126
−1122.622

∗
0
0
∗


−0.066
0
0
−0.041

0.135
−1139.534
0
∗
∗
0


0
0.675
0.341
0

0.100
−1065.786
∗
∗
0
∗

−0.066
0.676
0
−0.052

0.109
−1089.534

∗
0
∗
∗


−0.074
0
0.342
−0.041

0.126
−1124.873

∗
∗
∗
0


−0.074
0.675
0.343
0

0.100
−1067.992
0
∗
∗
∗


0
0.676
0.341
−0.052

0.100
−1068.393
∗
∗
∗
∗

−0.074
0.676
0.343
−0.052

0.100
−1070.599
Table 3.1: Lower bounds and posterior means for each zero mean VAR(1) model
98

Chapter 4
Searching the graphical space
4.1
Motivation
In the previous chapter, we constructed a candidate set of individual VAR(1) models, each
dependent on the sparsity structure of an A-matrix and represented by an A-graph. Given
a set of observed data, these sparse models were able to be scored by evaluating, in each
case, LMi(qi), a lower bound approximation to the logarithm of the marginal likelihood,
derived using the variational Bayesian method. Thus, we were able to select the most
plausible models from the set. At present, this approach is applicable, but, conversely,
rather limited.
This is because we can only compute the lower bound, or variational score, for a set
of predetermined graphs individually.
As a consequence, it becomes a computational
impossibility to consider all the candidate models within the graphical space in this way,
when the dimension of the VAR(1) models, d, increases. In fact, explicitly, the number of
possible candidate models, each represented by a graph on d nodes, is 2d2 −1, excluding
the null model. Obviously, for large d, the task of computing a lower bound for each
candidate individually is somewhat arduous! It would hence be more beneﬁcial if the
99

4.2. Hill-climbing
whole process was fully automated, and thus we possessed an eﬃcient way to traverse
through the graphical space quickly to ﬁnd high scoring graphs.
In this chapter, we consider such an automated system. In particular, two such methods
are developed. Initially, a customary hill-climbing algorithm is contemplated. In this
circumstance, we are able to manoeuvre through the graphical space by comparing the
values of two lower bounds, and accepting the A-graph that eﬀects the higher L(q), with
probability 1. The alternative is to make a random walk across the space so that moves to
neighbouring graphs are reliant upon Markov chain Monte Carlo (MCMC) techniques and,
in particular, the Metropolis-Hastings algorithm (previously documented in Chapter 2).
Thus, we accept a move to a new graph on the basis of an acceptance probability, α.
However, due to exclusion of the null graph from our candidate set, care must be shown
when specifying α. We shall consider each approach in turn.
4.2
Hill-climbing
As the above introduction suggests, the hill-climbing algorithm (Russell and Norvig, 2003)
is the simpler of the two approaches. In general, this a straightforward search method
used in a large state space that, at each iteration, will move to a neighbour of a given
current state, whenever the new state is of increased value. In this case, the algorithm is
said to ‘climb’ in an uphill direction until it reaches a local maximum, i.e. a point where
no neighbour has higher value. The algorithm is known as greedy as it always chooses the
best available state at each iteration without thinking any further ahead.
Consider the algorithm from the perspective of scoring sparse VAR(1) models. Thus, at
some iteration, say that we have accepted a model from the graphical space, represented
by an A-graph, together with an associated lower bound value. Then, at the next iteration,
we propose a new model, by randomly choosing a graph within the neighbourhood of the
100

4.2. Hill-climbing
current, accepted graph, i.e. by the addition or deletion of a single edge. We then evaluate
the lower bound for the proposal, and compare the value to that of the accepted graph. If
the variational score between the two models improves (i.e. increases), then the proposed
graph, with its corresponding lower bound, is accepted categorically, otherwise we reject
and return to the graph at the previous iteration. We continue until all neighbouring
graphs have lower scores, at which point the lower bound of the accepted graph is a local
maximum. This procedure can be represented by a formal hill-climbing algorithm as given
below.
Algorithm 4
1. Initialise the iteration counter to k = 1. For an initial graphical
model M0, relating to a directed A-graph on d vertices, G0 (itself corresponding to
matrix A(0)), run update equations for the variational parameters until convergence,
and hence evaluate the converged lower bound, LM0(q0).
2. At iteration k, propose a modiﬁed graphical model, Mφ, to the current model, Mk−1,
such that we have exactly one of the following:
(a) a new edge is randomly added to the current graph, Gk−1.
(b) a randomly selected edge is deleted from this graph.
That is, randomly and independently, simulate two integers from the sequence 1, . . . , d,
namely i, j. Examine the corresponding entry of the matrix A(k−1). If a(k−1)
ji
= 0
(the value of aji at iteration k −1), add the corresponding directed edge from i to
j to the existing Gk−1, and let a(φ)
ji
= ∗(an unspeciﬁed, non-zero). Otherwise, if
non-zero, delete this edge and let a(φ)
ji = 0.
3. Evaluate the variational score, LMφ(qφ), for the proposed model Mφ.
4. Set LMk(qk) = LMφ(qφ), hence Mk = Mφ, i.e. accept the new variational score and
model Mφ, if LMφ(qφ) > LMk−1(qk−1). Otherwise, set LMk(qk) = LMk−1(qk−1) and
thus Mk = Mk−1.
101

4.2. Hill-climbing
5. Change the counter from k to k + 1 and return to step 2.
So, this algorithm attempts to locate a locally optimum graph by searching throughout
the graphical space, accepting and rejecting moves as appropriate.
This procedure is
illustrated by a simple example.
4.2.1
Example
Suppose d = 10. A dataset of size N = 250 was simulated from the VAR(1) model (3.2)
with speciﬁcations A = diag(0.6), a 10 × 10 diagonal matrix of coeﬃcients, and σ2 = 0.1.
This implies that the true choice of A can be represented by an A-graph such that each
of the 10 nodes has a directed self-loop, and no other edges exist. We realise now that
the dimension of the graphical space is 2102 −1 ≈1.268 × 1030.
The prior distributions over a and σ2 were again given by
p(a) = N (a | 0, C∗) where cij ∈{0, 0.5}
p(σ2) = IG(σ2 | 1, 0.001).
Algorithm 4 was then implemented for this example by choosing three distinct, ini-
tial models, denoted as M0, 1, M0, 2, M0, 3, each represented by a corresponding A-graph,
namely G0, 1, G0, 2, G0, 3. Then, G0, 1 was speciﬁed to be a graph with only one edge, a
self-loop on node y1, whereas G0, 2 was given as the complete graph. Finally, G0, 3 had
directed edges in both directions between nodes yi and yi+1 for all i = 1, . . ., 9. This lat-
ter graph corresponds to an A-matrix with non-zero elements down the ﬁrst sub-diagonal
and ﬁrst super-diagonal (the diagonals immediately below and above the main diagonal
respectively) and zeroes elsewhere.
Then, in each case, the algorithm was run for 10, 000 iterations. For the three starting
102

4.2. Hill-climbing
graphs, G0, 1, G0, 2 and G0, 3, a local maximum of LMi(qi) was located after 1041, 540 and
1041 iterations, requiring 39, 98 and 58 accepted moves respectively to reach this value.
The local maximum found by each model was, in fact, the variational score associated with
the graph from which the data was simulated. Given the dimension of the graphical space,
we cannot be certain that we have reached a global maximum as there may be other graphs
that are erroneously preferred to the truth. However, since the same optimum has been
reached from three diﬀerent starting points, there is a good chance that this maximum
cannot be improved upon, and is indeed global. The convergence patterns of the three
models is shown in Figure 4.1 below.
Time
L(q)
0
200
400
600
800
1000
1200
−8400
−8200
−8000
−7800
−7600
−7400
G0, 1
G0, 2
G0, 3
Figure 4.1: Convergence of hill-climbing algorithm for diﬀerent, initial graphs
The plot shows that the climb taken by G0, 2 to reach the maximum was much smoother
and quicker than that of G0, 1 and G0, 3, whose paths were actually quite akin to each
other. Clearly, the complete graph was already well-favoured by the data. For G0, 2, it
is no surprise however that the number of accepted moves needed to reach convergence
was greater than for both G0, 1 and G0, 3 as it was the most distinct initial graph from the
truth. We notice that both G0, 1 and G0, 3, due to the similarity of their routes, became
103

4.3. Random walks
stuck at the same local maximum, after around 400 iterations, before reaching the ﬁnal
lower bound value. This is indicated by the ﬂatness of their convergence at this stage.
The hill-climbing algorithm is an eﬃcient search tool that can be used in large state spaces
and, as the above example illustrates, can perform well in reasonable time. It is clear that
as the state space decreases in size, the number of iterations required to reach a local
maximum will also lessen. However, hill-climbing does possess some intrinsic problems.
For instance, we can never be certain of ﬁnding a global maximum. We can easily get
stuck at a local maximum, where all neighbouring states are of lesser value. However, this
point could be signiﬁcantly worse than the global maximum, and even other local maxima
in the space. Moreover, the algorithm can reach a ﬂat part of the state space known as
a plateau. In this case, all neighbours will be of the same quality, no uphill moves can
be made, and hence the algorithm is again trapped. So, the success of the algorithm is
dependent upon the shape of the state space. To tackle these issues, several diﬀerent
forms of hill-climbing have been constructed. For details, see, for example, Russell and
Norvig (2003).
4.3
Random walks
As we have seen in the previous section, application of the hill-climbing algorithm consists
of comparing the variational scores for a current and proposed model at each iteration.
However, unfortunately, no information is provided about the model posterior distribu-
tion. Of course, this is a key concept because it encapsulates our post-data beliefs about
each model. So, an alternative method, which will allow exploration of this distribu-
tion, would be to make a random walk across the graphical space.
As mentioned in
Section 4.1, the acceptance of a proposed move to a new graph is determined by the
Metropolis-Hastings algorithm. It is already evident that the greediness of hill-climbing
is, in fact, its downfall, i.e. we can never see beyond exactly any one move. So, a further
104

4.3. Random walks
advantage of random walks is now that we may be willing to accept ‘downhill’ moves
which, although result in a decrease of variational score, may lead to a greater increase
at a subsequent iteration, and hence escape local maxima. Other such authors to search
the graphical space in this way include Giudici and Green (1999) and Jones et al. (2005).
A random walk on a single graph involves randomly selecting, and hence moving to, an
adjacent node to the current node on the graph, with equal probability for each of these
neighbours, hence forming a sequence of selected nodes. That is, the next node is chosen
from the (discrete) uniform distribution. We can illustrate this by considering Figure 4.2.
Here, we let each undirected edge represent a two-way directed edge between a pair of
nodes. Suppose we are at node 1. Then, as this node possesses two neighbours (nodes 2
and 4), the probability of moving to either neighbour is 1
2. However, if we are at node 2,
the corresponding probability would be 1
3 and so on.
1
2
3
4
Figure 4.2: A simple graph on which to make a random walk
In general, notice that the probability of moving to a new node at time t+1 is dependent
only upon the node at which we reside at time t, and none of the previous history of the
random walk. Thus, the sequence of visited nodes forms a Markov chain with transition
105

4.3. Random walks
matrix probabilities, pij, indicating a move from node i to node j, such that
pij =
⎧
⎨
⎩
1
di
if (i, j) ∈E
0
otherwise
,
as noted by H¨aggstr¨om (2002). Here, di is the number of neighbours of node i and E is
the set of edges. For a comprehensive review of random walks, see, for instance, Lov´asz
(1993).
Here however, we examine not movement between adjacent nodes on one graph, but
instead between adjacent graphs in the space.
So, in terms of models, for a current
model, say Mj, represented by an A-graph, Gj, a proposed, neighbouring model, Ml,
is considered, whose graph Gl diﬀers from that corresponding to Mj by the addition or
deletion of a uniformly selected edge. We allow acceptance of the proposed graphical
model by using the Metropolis-Hastings algorithm.
For further details, the reader is
referred back to Chapter 2.
By considering a move from a current to a proposed model, an acceptance probability
(c.f. (2.3)) can be speciﬁed such that
α(Mj, Ml) = min

1, p(Ml | D) q(Ml, Mj)
p(Mj | D) q(Mj, Ml)

.
(4.1)
Our distribution of interest is now given as the posterior over models, and q(· , ·) is again
the proposal distribution, entering α(· , ·) via a ratio, namely
q(Ml, Mj)
q(Mj, Ml).
The denominator speciﬁes the probability of the move from current model Mj to proposed
model Ml whereas, on the numerator, that of the reverse move. However, as was seen in
106

4.3. Random walks
Chapter 1, we have no knowledge of the model posterior as, by Bayes’ Theorem, this is
dependent upon the marginal likelihood, p(D | Mi), for each model Mi. Despite this, we
have already used the variational algorithm to bound, and hence approximate, p(D | Mi)
whereby LMi(qi) ≤log p(D | Mi). Thus, the ratio of model posteriors in the acceptance
probability (4.1) can be rewritten as
p(Ml | D)
p(Mj | D) = p(D | Ml) p(Ml)
p(D | Mj) p(Mj) ≈exp{LMl(ql)} p(Ml)
exp{LMj(qj)} p(Mj).
(4.2)
As was mentioned previously in Chapter 2, the constant of proportionality, p(D), is
eliminated in the acceptance probability by the ratio of posteriors. An important point
to raise here is that we can only be sure of sampling from an approximation to the true
model posterior. Of course, by minimising the KL divergence between the variational and
true posterior, we suppose that the lower bound is close to the log marginal likelihood
(and hence the samples are of suﬃcient quality). However, this is only an assumption and
does not illustrate formally the accuracy of the bound.
To combat this problem, Miskin (2000) and Beal (2003) discuss the use of importance
sampling from the varaiational approximation to estimate log p(D | Mi). That is, the loga-
rithm of integral (1.2) is approximated by taking importance samples from the variational
distribution q(θi | D, Mi). This idea seems sensible since the variational should be repre-
sentative of the true posterior by free form optimisation. However, Beal (2003) indicates
several drawbacks with this approach, most notably that importance sampling performs
poorly in high dimensions and can even fail in one dimension. In any case, we would
expect the diﬀerence between the log marginal likelihood and lower bound to be similar
for any model Mi. As a consequence, any inaccuracy in the approximation will cancel
from the ratio of bounds in (4.2), and so such a comparison simulation is not employed
here.
In returning to (4.2), a form for the model priors must also be speciﬁed. However, further
107

4.3. Random walks
discussion is required before making such a choice. We reconsider (3.9) and (3.10). Al-
though not stated explicitly, these prior distributions are conditioned on each model Mi in
the candidate set, similarly the variational distributions (3.22) and (3.27). Yet moreover,
it is recalled that, in the previous chapter, sparsity modelling was used to induce many
zeroes in the parameter matrix A. That is, across Metropolis-Hastings iterations, we ex-
pect models to be chosen with few existent edges, implying that the number of causations
between nodes will be sparse. By modelling in this manner, a prior over the coeﬃcients aij
of the autoregressive matrix A, where i, j = 1, . . ., d, across models is induced automat-
ically, which dictates that each aij may either be zero or non-zero. Such a speciﬁcation
is termed a sparsity prior (also termed by Lucas et al. (2006) as a ‘point-mass mixture’
prior), taking the form
p(aij) = pδ0(aij) + (1 −p)N (aij | 0, c).
(4.3)
Here, c is deﬁned from (3.11), and p = P(aij = 0) = 1 −P(aij ̸= 0) is the ‘in-out’, prior
probability that a coeﬃcient is zero. Moreover, δ0(·) is the Dirac delta function, which,
for any z ∈R, possesses the properties
 +∞
−∞
δ0(z) dz = 1
δ0(z) =
⎧
⎨
⎩
0
if z ̸= 0
∞
if z = 0
.
That is, δ0(z) has a peak of inﬁnite height at z = 0, and vanishes elsewhere on the real
line such that it integrates to unity. Thus, (4.3) dictates that aij is a point mass at zero
a priori with probability p, whereas a Gaussian distribution is followed with probability
1 −p. So, this prior, not conditional on each model, mixes a probability mass at aij = 0
with a distribution over non-zero values of aij. By updating (4.3), approximate marginal
posterior information can be provided about each aij, and this is discussed in the next
108

4.3. Random walks
section.
Presently however, we examine how this sparsity prior aﬀects the speciﬁcation of the
model prior distributions. Since p is the common, prior probability that no edge exists
between any pair of nodes, a form for p(Mi) is determined, assuming that all edges are
a priori independent. If a model Mi has η non-zero elements in its associated A-matrix
(i.e. there are η edges on the graph), then, as A contains d2 elements, a prior distribution
can be speciﬁed such that
p(Mi) = pd2−η(1 −p)η.
(4.4)
For example, suppose that a model is represented by the matrix A =
⎛
⎝0
0
∗
0
⎞
⎠. Then, the
prior for this model is equivalent to P(a11, a12, a22 = 0) × P(a21 ̸= 0) = p3(1 −p).
From (4.4), we notice that all models will be equally likely a priori if p = 0.5. In such a
case, no preference is given to any particular model. However, if we choose p > 0.5, this is
no-longer true since more complex models will be penalised, and sparse models favoured.
Correspondingly, in the sparsity prior (4.3), this would then imply that all nodes have
lower prior probability of association with another node. For the subsequent examples in
this chapter, varying choices of p will be made to gauge the eﬀect produced on the models
accepted across iterations.
Ultimately, we return to (4.2), and specify the ratio of model priors in this expression by
inspecting two distinct circumstances. Suppose that there are η edges on the graph Gj,
corresponding to the current model Mj. Then ﬁrstly, assume that the proposed model
Ml is deﬁned such that a uniformly chosen edge is added to the graph. Thus, the model
prior ratio provides
p(Ml)
p(Mj) = pd2−η−1(1 −p)η+1
pd2−η(1 −p)η
= 1 −p
p
.
109

4.3. Random walks
So, in this case, the acceptance probability is given by
α(Mj, Ml) = min

1,
	1 −p
p
× exp{LMl(ql)} q(Ml, Mj)
exp{LMj(qj)} q(Mj, Ml)

.
(4.5)
Secondly, let Ml now be speciﬁed whereby an edge is randomly deleted from the current
graph. Hence, on this occasion, the afore-mentioned ratio yields
p(Ml)
p(Mj) = pd2−η+1(1 −p)η−1
pd2−η(1 −p)η
=
p
1 −p.
Of course, the proposed model is subsequently accepted on the basis of
α(Mj, Ml) = min

1,
	
p
1 −p × exp{LMl(ql)} q(Ml, Mj)
exp{LMj(qj)} q(Mj, Ml)

.
(4.6)
Notice that (4.5) and (4.6) are equivalent, only when p = 0.5, as mentioned previously.
We also need to deﬁne a form for the proposal distribution, q(· , ·). In general, care must
be shown when making such a choice. If the proposal results in candidates being regularly
rejected, the chain will move infrequently, and so mixing will be poor. Similarly, if too
many candidates are accepted, achieved by proposing only small moves, exploration of
the graphical space will again take a long time. A good proposal will avert both these
extremes.
If we examine the random walk on a single graph again, such proposal probabilities are
easily speciﬁed. For instance, reconsider Figure 4.2. If we were proposing the move from
node 1 to node 2, then using the same notation for proposal densities as above, the ratio
of proposals in the Metropolis-Hastings acceptance probability would be
q(2, 1)
q(1, 2) =
1
3
1
2
= 2
3,
110

4.3. Random walks
as node 1 and node 2 have two and three neighbours respectively. The case of random
walks amongst graphs is analogous to this, where we now use a (discrete) uniform proposal
distribution to choose a new graph from the set of neighbouring graphs. For a graph on
d nodes, there are typically d2 neighbours. This is clear since each graph corresponds to
a d × d sparse matrix A, and hence there are d2 possible ways to change exactly one of
the elements in the matrix to form a new graph. However, we must proceed cautiously
here since we ignore the null graph, represented by the zero matrix, as pointed out in
Section 3.5. Thus, for graphs with just 1 existent edge, there are only d2 −1 neighbouring
graphs.
Consequently, we look at three scenarios for the ratio of proposal densities, each to be
considered in turn. Recall that we modify the existent model in two ways: either adding or
deleting an edge from the current graph. A ‘delete move’ from a model Mj, corresponding
to a graph Gj with two edges, to a speciﬁed model Ml, whose graph Gl possesses a single
edge, has ratio of proposals such that
q(Ml, Mj)
q(Mj, Ml) =
1
d2−1
1
d2
=
d2
d2 −1.
(4.7)
Here, the probability of a move from model Mj to Ml, given by the denominator, follows
since Gj has d2 neighbouring graphs.
Moreover, the probability of the reverse move,
provided by the numerator, is clear as Gl has only d2 −1 neighbours. In contrast, an
‘add move’ from Mj, whose associated graph has one edge, to a given Ml, represented
graphically with two edges, has ratio of proposals given by
q(Ml, Mj)
q(Mj, Ml) =
1
d2
1
d2−1
= d2 −1
d2
.
(4.8)
Any other move has the ratio
q(Ml, Mj)
q(Mj, Ml) =
1
d2
1
d2
= 1,
(4.9)
111

4.3. Random walks
as now we can traverse to any of the d2 neighbouring graphs. A minor issue here is how
to proceed if the ﬁnal edge is selected for deletion, and this is explained in Algorithm 5
below.
Finally, we realise that, in practice, the exponential of the variational lower bound is
customarily very small in size. Hence, it makes computationally better sense to work
on the log scale.
In total, four separate versions are obtained for the log acceptance
probability log α(·, ·). If an edge is added to the current graph, the ratio of proposal
densities, (4.8) and (4.9), are substituted directly into (4.5), representing respectively
the cases when the current graph has one edge, and otherwise. In an analogous way,
(4.7) and (4.9) are inserted accordingly into (4.6), when an edge is deleted. In all cases,
logarithms are taken of both parts of the acceptance probability. So, the Metropolis-
Hastings algorithm can be provided thus. In what follows, we suppose that each current
graph has λ edges, whereas every proposed graph has ζ edges, i.e. λ and ζ represent the
number of non-zeroes in the corresponding A-matrix.
Algorithm 5
1. Initialise the iteration counter to k = 1. For an initial graphical
model M0, relating to a directed A-graph on d vertices, G0 (itself corresponding to
matrix A(0)), run update equations for the variational parameters until convergence,
and hence evaluate the converged lower bound, LM0(q0).
2. At iteration k, propose a modiﬁed graphical model, Mφ, to the current model, Mk−1,
such that we have exactly one of the following:
(a) a new edge is randomly added to the current graph, Gk−1.
(b) a randomly selected edge is deleted from this graph.
That is, randomly and independently, simulate two integers from the sequence 1, . . . , d,
namely i, j. Examine the corresponding entry of the matrix A(k−1). If a(k−1)
ji
= 0,
add the corresponding directed edge from i to j to the existing Gk−1, and let a(φ)
ji = ∗
112

4.3. Random walks
(an unspeciﬁed, non-zero). Otherwise, if non-zero, delete this edge and let a(φ)
ji = 0.
If the last edge is chosen for deletion, additional pairs of integers are simulated until
an edge is found that can be added. During this time, the algorithm remains at
iteration k.
3. Evaluate the variational score, LMφ(qφ), for the proposed model Mφ.
4. Calculate the log acceptance probability log α(Mk−1, Mφ) of the proposed move, where:
(a) an edge is added to the graph Gk−1.
• if λ = 1,
log α(Mk−1, Mφ) = min{0, log(1 −p) −log p + LMφ(qφ) −LMk−1(qk−1)
+ log(d2 −1) −log d2}.
• otherwise,
log α(Mk−1, Mφ) = min{0, log(1 −p) −log p + LMφ(qφ) −LMk−1(qk−1)}.
(b) an edge is deleted from the graph Gk−1.
• if ζ = 1,
log α(Mk−1, Mφ) = min{0, log p −log(1 −p) + LMφ(qφ) −LMk−1(qk−1)
+ log d2 −log(d2 −1)}.
• otherwise,
log α(Mk−1, Mφ) = min{0, log p −log(1 −p) + LMφ(qφ) −LMk−1(qk−1)}.
5. Put LMk(qk) = LMφ(qφ), hence Mk = Mφ, i.e. accept the new variational score and
graphical model Mφ, with log probability log α(Mk−1, Mφ). Otherwise, put LMk(qk) =
113

4.3. Random walks
LMk−1(qk−1) and thus Mk = Mk−1.
6. Change the counter from k to k + 1 and return to step 2.
To clarify, at each iteration, a new model is simulated from the proposal distribution,
represented in the algorithm by a corresponding variational score. The score (and hence
model) can be either accepted or rejected upon comparison to the lower bound of the
current model, determined by the acceptance probability.
Notice that, from a com-
putational perspective, a proposed move is accepted if log u < log α(Mk−1, Mφ) where
u ∼U(0, 1). It is also realised that, from the deﬁnitions of the log acceptance probability,
log

1−p
p

= log

p
1−p

= 0 when p = 0.5. Thus, the acceptance of models in the scheme
will be independent of p in this circumstance.
We hence construct a Markov chain of accepted variational scores, whose values, upon
convergence and exponentiating, will be draws from the distribution proportional to
exp{LMi(qi)} p(Mi), an approximation to the model posterior. Here, LMi(qi) is the dis-
tribution of lower bounds across all models, where i = 1, . . ., R. By comparing both
Algorithms 4 and 5, it is clear that this version of the Metropolis-Hastings algorithm is
merely an extension of the simpler hill-climbing algorithm of earlier. Presently however,
a proposed move is dependent upon an acceptance probability, which, as discussed previ-
ously, enables the graphical space to be explored with greater eﬀect than would be seen
with hill-climbing.
4.3.1
Implementation and analysis
In due course, we will use Algorithm 5 to make a random walk across the graphical space
in several examples. Before this, we examine the MCMC theory required to produce and
analyse the subsequent results. Initially, recall from Chapter 2 that trace plots can be
used to assess not only the duration of the burn-in period, but also the mixing properties
114

4.3. Random walks
of a Markov chain as a way to analyse possible convergence. Note that, in this case prior
to convergence, both the accepted lower bound values and associated candidate models
are eliminated.
A further plot to utilise when examining for convergence of a chain is that of the autocor-
relation function (ACF). We realise that the values generated by using the Metropolis-
Hastings sampler, upon convergence, are not independent since, by deﬁnition of a Markov
chain, each simulated value is dependent on the previous value. For instance here, the
current model, which may have been accepted at many iterations previous, is used to
generate a proposed model, by the addition or deletion of a uniformly selected edge from
the representative graph. Hence, this dependence implies that there will be correlation
between the corresponding variational scores for these models.
To quantify this correlation, the ACF at lag h measures the correlation between the whole
chain of lower bound values and the same chain, time-shifted by h iterations. Suppose
that, after burn-in and upon convergence, the chain is of length n. Then, for example, at
lag 10, we study the correlation between the lower bounds of the chain at the iteration
sets {1, 2, . . . , n −10} and {11, 12, . . . , n}. If LMk(qk) is the lower bound at the k-th
iteration, then the lag h ACF is estimated by
ˆrh =
 n−h
k=1(LMk(qk) −¯L(q))(LMk+h(qk+h) −¯L(q))
 n
k=1(LMk(qk) −¯L(q))2
,
(4.10)
where ¯L(q) = 1
n
 n
k=1 LMk(qk).
A high value of the ACF indicates poor mixing as indicated by no rapid movements on
the trace plot, and hence a lack of convergence. On the contrary, lower autocorrelations
correspond to little dependence between chain values. Therefore, new values of the chain
will not remain in the same area of the graphical space as those before, leading to good
coverage of the space, and hence a well mixing chain. Thus, the values are seen to be
‘independent’ when there is approximately zero autocorrelation at each lag. The number
115

4.3. Random walks
of independent values represented is called the eﬀective sample size of the chain.
A
standard way to reduce autocorrelation is by only retaining every t-th value of the chain
after burn-in, a method referred to as thinning. It is important that t is chosen not to be
too large, since, although this would further reduce autocorrelation, we require a chain of
suﬃcient length to conduct analysis.
In our discussion of testing for the convergence of a Markov chain, we have hitherto
inspected graphical methods, which are reliable, but lack formality. To this end, several
such convergence diagnostics have been developed, two of which are now examined and
a third illustrated in Section 4.3.5. Such diagnostics can be located within the R package
called CODA (Plummer et al., 2006).
The Raftery-Lewis test (Raftery and Lewis, 1992) is formulated around estimating a
quantile Q of the distribution of interest to within an accuracy of ±r with probability s.
Recall, in general, that the value xp of a distribution F whereby F(xp) = p, for 0 < p < 1,
is the p-th quantile of this distribution.
Having speciﬁed Q, r, s, the test breaks the
Markov chain into a new sequence such that we obtain a ‘1’ if LMk(qk) ≤LQ(q) (the
Q-th quantile of the sample distribution of lower bounds) and a ‘0’ otherwise, for all k.
This binary sequence generates a two-state Markov chain. Transition probabilities can be
estimated from the sample by counting the number of times that state a moves to state
b where a, b = 0, 1, and normalising so that the row sums in the transition matrix equal
one.
Thus, the test subsequently estimates the length of the burn-in period, M, the thinning
interval t and the number of additional iterations, N, required to achieve the level of
speciﬁed accuracy. Moreover, also determined is Nmin, the number of iterations required
had the chain been fully independent. From this, the convergence diagnostic, I, known
as the dependence factor is derived such that I =
N
Nmin. This measures the increase in the
number of iterations needed to achieve convergence as a result of the correlation within
the chain. As a rule of thumb, if I > 5, then the chain suﬀers from strong autocorrelation,
116

4.3. Random walks
indicating convergence problems.
An alternative diagnostic for convergence of a Markov chain is given by Heidelberger and
Welch (1983). Initially, we test the null hypothesis that the values of the chain come from
a stationary distribution. If the null is accepted, then no burn-in is needed; if rejected,
the ﬁrst 10% of the chain is removed and we repeat the test. If the test fails again, we
remove the next 10% from the sequence, and continue on until either the null hypothesis
is accepted, whereby the burn-in is considered to be the discarded part of the chain, or
less than 50% of the chain is left. In the latter case, the ‘stationarity test’ is deemed to
have failed, and hence the requirement for a longer MCMC run.
If the stationarity test is passed, then we conduct a half-width test on the remaining
part of the chain by constructing a conﬁdence interval for the mean of the distribution of
interest. Subsequently, we ﬁnd the ratio between half the width of the interval and the
sample mean. If the ratio is less than a speciﬁed value, ϵ, then this test is also passed.
A failure implies that a larger sample is required from which the mean can be estimated
with the necessary accuracy.
Thus far, the MCMC output has been pivotal to our analysis. However, we have additional
interest in the graphical structures of the models accepted across iterations, information
not provided by the variational scores on their own. To proceed, we simply measure the
cumulative eﬀect of such models. Thus, we create a d ×d ‘counting’ matrix, ˆΠ, initialised
as the zero matrix, which records each edge between any pair of nodes for the accepted
graph at every iteration of the thinned chain with burn-in discarded. For example, at
iteration k, suppose that there exists an edge between nodes i and j, and this same edge
had already been counted a times in the previous k −1 iterations. Then, we say that
ˆΠ(k)
ij = a + 1. This process is repeated for other edges between nodes before progressing
to iteration k + 1, and so on.
At the end of the MCMC run, the matrix can be inspected to establish which edges have
117

4.3. Random walks
been accepted most often. It would be beneﬁcial if this task were able to be performed
visually. Fortunately, we can utilise the standard R function, image. This function creates
a grid, representing each element of the matrix ˆΠ, and each rectangle on the grid is
assigned a colour. In the examples forthcoming, a spectrum of colours is applied, ranging
from red to white. The lighter the colour, the more often that edge has been accepted
between two particular nodes.
In other words, if any rectangle is red, that edge has
occurred infrequently.
When simulating data ourselves, we can introduce a true adjacency matrix, Π, deﬁned
such that Πij = 1 if aij ̸= 0, otherwise zero. Having normalised ˆΠ by the length of the
chain n, we wish to employ a formal technique so that we may compare the empirical
proportions that any edge exists on the graph to the (0, 1)–matrix of true probabilities.
That is, to measure the discrepancy between the truth and the normalised estimate, we
can compute the residual sum of squares, denoted by S. Hence, in this instance, we have
S =
d

i=1
d

j=1
1
n
ˆΠij −Πij
2
.
(4.11)
As mentioned in Section 4.3, the coeﬃcients aij of the sparse matrix A across models
are a further quantity of interest. Formerly, given a dataset, our principle objective has
been to estimate the unknown sparsity structure of A, and discover which nodes on the
A-graph have an inﬂuence over others. That is, we aspired to determine the pattern of
zeroes in the truth. However, the focus now switches to learning the likely values of each
aij on the basis of an MCMC sampler. Thus, upon specifying the sparsity prior (4.3) over
these coeﬃcients, we would like to revise our beliefs by inferring both P(aij = 0 | D) and
p(aij | aij ̸= 0, D). The former is the posterior probability over models that a particular
aij = 0, whereas the latter is the marginal posterior density of aij, given that it is non-zero
in value.
For each coeﬃcient, it is possible to estimate P(aij = 0 | D) by counting the number of
118

4.3. Random walks
times that aij = 0 for the models accepted across Metropolis-Hastings iterations of the
thinned, converged chain, and dividing by the length of the run n. It is critical to realise
that this estimate is dependent, not only upon the simulation of a new model via the
proposal distribution, but also the use of the variational algorithm to determine which
model is accepted at each iteration. At this stage, we recollect that the two lower bounds
for the proposed and current model are entered into the acceptance probability. Hence,
our approximation is denoted by Pvar(aij = 0 | D).
We now proceed to infer the marginal p(aij | aij ̸= 0, D). It is clear that the variational
posterior for aij, which corresponds to the model, Mk, accepted at iteration k of the
sampler, is conditioned upon it. So, to estimate this true posterior density of aij without
conditioning, we average these variational densities across iterations whenever aij ̸= 0.
This technique is known as Bayesian model averaging — for a brief overview, see Kass
and Raftery (1995). Now, deﬁne naij̸=0 to be the length of the chain when aij ̸= 0. Thus,
using (3.34) and previous notation, we calculate
pvar(aij | aij ̸= 0, D) =
1
naij̸=0

a(k)
ij ̸=0
q(aij | D, Mk)
=
1
naij̸=0

a(k)
ij ̸=0
N

aij | ρ(k)
(i,j), τ (k)
(i,j)

.
(4.12)
Here, recall that a(k)
ij is the value of aij at iteration k, similarly ρ(k)
(i,j), τ (k)
(i,j). Computation-
ally, we can evaluate the densities at the same set of points, and then average the values
obtained. Of course, the error associated with this estimate, achieved by simulation, is
again a consequence of the use of the variational approximation.
We now require a way to summarise the above, approximate marginal posterior informa-
tion for a set of coeﬃcients of the matrix A. This can be performed graphically, as seen
in Scott and Berger (2006). For each aij, Pvar(aij = 0 | D) is denoted by the height of a
black, vertical bar with a circle atop, placed at zero and corresponding to the probability
119

4.3. Random walks
scale on the right-hand side of every graph. Moreover, the density pvar(aij | aij ̸= 0, D) is
also plotted, measured by the scale on the opposite side, and indicating the value of aij,
given that it is non-zero.
4.3.2
Examples
Algorithm 5 was coded in C, and then applied to three simulated data-sets from the
VAR(1) model (3.2), each of size N = 250 with dimension d = 10 and σ2 = 0.1. Only
the speciﬁcations of A and p = P(aij = 0) were considered for alteration in each example.
Here, the true A-graphs were chosen to be highly symmetric. Of course, the simulation
could be extended to test the algorithm on randomly generated, less structured graphs.
The prior distributions were again chosen to be
p(a) = N (a | 0, C∗) where cij ∈{0, 0.5}
p(σ2) = IG(σ2 | 1, 0.001).
The Metropolis-Hastings scheme was run for 10, 000, 000 iterations in each case, and
the output transferred to R for subsequent analysis. It was initialised from graph G0,
containing one self-loop on the node y1. Of course, this corresponds to the matrix A(0)
where a(0)
11 = ∗, otherwise zero. Using trace plots, the burn-in period was taken to be
the ﬁrst 100, 000 iterations. The remainder was thinned by maintaining every 1000-th
iteration, leaving a total of 9900 iterations for each example on which to conduct analysis.
A histogram of 30 bins was employed throughout.
Moreover, the Raftery-Lewis test was initialised with Q = 0.025, r = 0.005, s = 0.95, i.e.
estimate the 2.5% quantile of the cumulative distribution function to within an accuracy
of ±0.005 with probability 0.95. These are the default speciﬁcations for the function,
as quoted in Raftery and Lewis (1992).
On the other hand, the Heidelberger-Welch
120

4.3. Random walks
diagnostic was speciﬁed to ﬁnd a 95% conﬁdence interval for the mean, and the half-
width ratio to be less than ϵ = 0.1. Moreover, the stationarity test was passed if the
p-value calculated was greater than 0.05.
Example 1
Initially, A and p were speciﬁed such that A = diag(0.8) and p = 0.5, i.e. all models were
favoured equally a priori. Figure 4.3 shows the trace plot, ACF plot and histogram of
the lower bound values, and the image plot of ˆΠ.
0
2000
4000
6000
8000
10000
−7340
−7330
−7320
−7310
Trace plot of L(q)
Iteration
L(q)
0
20
40
60
80
100
0.0
0.2
0.4
0.6
0.8
1.0
Lag
ACF
ACF plot for L(q)
Histogram of L(q)
L(q)
Frequency
−7340
−7330
−7320
−7310
−7300
0
200
400
600
0
1
2
3
4
5
6
7
8
9
10
0
1
2
3
4
5
6
7
8
9
10
Image plot for Pihat
Row
Col
Figure 4.3: Plots for the analysis of the MCMC output in Example 1
The trace plot shows rapid movement throughout the graphical space, and hence that
it is mixing well.
Moreover, the ACF plot drops immediately to approximately zero
autocorrelation, thus indicating the independence of the values of the chain.
This is
121

4.3. Random walks
illustrated further by utilising the effectiveSize function in the CODA library. In this
case, the eﬀective sample size, i.e. the equivalent independent sample size, is calculated
to be 9900, which is the length of the entire chain. Therefore, we can view the output
as an independent chain. Given the length of the MCMC run, the stringent thinning has
evidently worked. It is stressed that the correlation at lag 0 is always equal to 1, as this
is the chain cross-correlated with itself without any time shift. This is clear from (4.10)
by setting h = 0.
The above analysis provides good evidence of convergence of the chain to the station-
ary distribution. For the purposes of formality, we now apply the two afore-mentioned
convergence diagnostics to the set of lower bounds. So, using CODA, the output from the
Raftery-Lewis test was as follows.
Iterations = 1:9900
Thinning interval = 1
Number of chains = 1
Sample size per chain = 9900
Quantile (q) = 0.025
Accuracy (r) = +/- 0.005
Probability (s) = 0.95
Burn-in
Total Lower bound
Dependence
(M)
(N)
(Nmin)
factor (I)
Lq
2
3780
3746
1.01
The results suggest that only the ﬁrst 2 iterations should be taken as additional burn-in,
and a further 3780 iterations are necessary to attain the desired level of accuracy. To this
end, resultantly, the 9900 iterations actually applied are more than suﬃcient. Finally, the
value of the dependence factor is close to 1, indicating the independence inherent within
the chain and evidence for convergence.
The Heidelberger-Welch diagnostic produced
122

4.3. Random walks
Stationarity start
p-value
test
iteration
Lq
passed
1
0.119
Halfwidth Mean
Halfwidth
test
Lq
passed
-7315 0.117
On the output for the stationarity test, we see that the test was passed without the
need to discard any of the chain, hence the start iteration is given as 1.
Thus, the
null hypothesis that the chain has converged is accepted with a p-value greater than the
threshold value of 0.05. Moreover, the half-width test above yields the sample mean of the
lower bounds, and the size of half of the constructed conﬁdence interval. With both tests
passed, once again, the chain of variational scores seems to have converged. It is noted
that diagnostics were also considered for several components of ρ and τ, recorded at each
iteration and corresponding to the accepted model. In each case, these were consistent
with the convergence results for LMk(qk), and so are not shown here. We suggest that
studying LMk(qk) is deemed suﬃcient to test for convergence.
In addition, we realise that the image plot of ˆΠ in Figure 4.3 is as expected, with self-loops
regularly recognised for all nodes (white rectangles along the main diagonal). With the
true adjacency matrix speciﬁed as Π = diag(1), this accuracy is ampliﬁed by calculating
the residual sum of squares to be S = 0.684. So, when choosing p = 0.5, the true sparsity
structure has been identiﬁed to a highly acceptable level. Finally, graphical summaries
for both Pvar(aij = 0 | D) and pvar(aij | aij ̸= 0, D) are presented in Figure 4.4 for several
coeﬃcients of A.
As the true A had non-zero entries only along the diagonal, we would expect the value of
Pvar(aij = 0 | D), the approximate posterior probability of a point mass at zero, to be high
for those oﬀ-diagonal coeﬃcients. This is certainly apparent from the above plots. More-
over, each diagonal coeﬃcient possesses the corresponding probability to be approximately
zero. It is also noticeable that these same entries possess a density pvar(aij | aij ̸= 0, D)
123

4.3. Random walks
0.0
0.5
1.0
Value
0
2
4
6
8
10
Density
0
0.2
0.4
0.6
0.8
1
Probability
0.0
0.5
1.0
Value
0
2
4
6
8
10
Density
0
0.2
0.4
0.6
0.8
1
Probability
0.0
0.5
1.0
Value
0
2
4
6
8
10
Density
0
0.2
0.4
0.6
0.8
1
Probability
0.0
0.5
1.0
Value
0
2
4
6
8
10
Density
0
0.2
0.6
1
Probability
0.0
0.5
1.0
Value
0
2
4
6
8
10
Density
0
0.2
0.4
0.6
0.8
1
Probability
0.0
0.5
1.0
Value
0
2.2
4.4
6.6
8.8
11
Density
0
0.2
0.4
0.6
0.8
1
Probability
0.0
0.5
1.0
Value
0
2
4
6
8
10
Density
0
0.2
0.4
0.6
0.8
1
Probability
0.0
0.5
1.0
Value
0
2
4
6
8
10
Density
0
0.2
0.4
0.6
0.8
1
Probability
0.0
0.5
1.0
Value
0
2
4
6
8
10
Density
0
0.2
0.4
0.6
0.8
1
Probability
a11
a12
a13
a21
a22
a23
a31
a32
a33
Figure 4.4: Plots showing estimated, marginal posterior distributions for aij, i, j = 1, 2, 3,
in Example 1
with mode approximately equal to 0.8. Similarly, the related density for the oﬀ-diagonal
components is peaked around zero. So, in each case, the truth is being well represented.
Thus, the dataset of size N = 250 has overridden the prior on a at each iteration in the
variational Bayesian update, hence providing accurate, akin estimates of aij, when not
constrained to zero (c.f. Table 3.1 in Section 3.5). At the same time, the variance from
prior to variational distribution has decreased, and so there is greater certainty about
these values.
Therefore, as this will be the case for each component, these marginal
density plots possess a similar shape.
124

4.3. Random walks
Example 2
On this occasion, we again chose p = 0.5, but now A as the tridiagonal matrix such that
A = tridiag(0.2, 0.4, 0.2), using the notation of Saad (2003). That is, A has 0.4s down the
main diagonal and 0.2s down the ﬁrst sub-diagonal and ﬁrst super-diagonal, with zeroes
elsewhere. The graphical analysis of the MCMC output for this data-set is displayed in
Figure 4.5.
0
2000
4000
6000
8000
10000
−7380
−7370
−7360
−7350
−7340
Trace plot of L(q)
Iteration
L(q)
0
20
40
60
80
100
0.0
0.2
0.4
0.6
0.8
1.0
Lag
ACF
ACF plot for L(q)
Histogram of L(q)
L(q)
Frequency
−7380
−7370
−7360
−7350
−7340
0
200
400
600
0
1
2
3
4
5
6
7
8
9
10
0
1
2
3
4
5
6
7
8
9
10
Image plot for Pihat
Row
Col
Figure 4.5: Plots for the analysis of the MCMC output in Example 2
The trace and ACF plots reveal quick mixing and an independent chain respectively,
implying convergence. This is emphasised by effectiveSize being calculated as 9900,
similar to above. For completeness, we apply the two convergence diagnostics for the
lower bounds. The Raftery-Lewis output shows
125

4.3. Random walks
Iterations = 1:9900
Thinning interval = 1
Number of chains = 1
Sample size per chain = 9900
Quantile (q) = 0.025
Accuracy (r) = +/- 0.005
Probability (s) = 0.95
Burn-in
Total Lower bound
Dependence
(M)
(N)
(Nmin)
factor (I)
Lq
2
3812
3746
1.02
whereas the Heidelberger-Welch diagnostic provides
Stationarity start
p-value
test
iteration
Lq
passed
1
0.270
Halfwidth Mean
Halfwidth
test
Lq
passed
-7355 0.116
Both tests supply good evidence that the stationary distribution of the chain has been
acquired. As before, a similar conclusion is reached when applying the diagnostics to
components of ρ and τ. With the true value of Π taken as Π = tridiag(1, 1, 1), it was
found that S = 1.746. Thus, ˆΠ remains highly accurate, as is displayed by the image
plot whereby the non-zero elements of A have been identiﬁed, despite the A-graph that
simulated the data containing more edges than previous.
Upon examination of Figure 4.6, we see that the output is encouraging. For instance,
both Pvar(a13 = 0 | D) and Pvar(a31 = 0 | D) are very high in value. The same probability
for all other coeﬃcients is negligible, whereas the plots pvar(aij | aij ̸= 0, D) for these
elements are peaked, close to the true value in each case. When compared with Example
1, the algorithm here had to determine more non-zero signals in the truth at each iteration,
indicating a reason as to why the density estimates were slightly less accurate than before.
126

4.3. Random walks
0.0
0.5
1.0
Value
0
1.5
3
4.5
6
Density
0
0.2
0.4
0.6
0.8
Probability
0.0
0.5
1.0
Value
0
1.5
3
4.5
6
Density
0
0.2
0.4
0.6
0.8
Probability
0.0
0.5
1.0
Value
0
1.4
2.8
4.2
5.6
7
Density
0
0.2
0.4
0.6
0.8
1
Probability
0.0
0.5
1.0
Value
0
1.6
3.2
4.8
6.4
8
Density
0
0.2
0.4
0.6
0.8
1
Probability
0.0
0.5
1.0
Value
0
1.6
3.2
4.8
6.4
8
Density
0
0.2
0.4
0.6
0.8
1
Probability
0.0
0.5
1.0
Value
0
1.6
3.2
4.8
6.4
8
Density
0
0.2
0.4
0.6
0.8
1
Probability
0.0
0.5
1.0
Value
0
1.5
3
4.5
6
7.5
Density
0
0.2
0.4
0.6
0.8
1
Probability
0.0
0.5
1.0
Value
0
1.5
3
4.5
6
7.5
Density
0
0.2
0.4
0.6
0.8
1
Probability
0.0
0.5
1.0
Value
0
1.5
3
4.5
6
7.5
Density
0
0.2
0.4
0.6
0.8
1
Probability
a11
a12
a13
a21
a22
a23
a31
a32
a33
Figure 4.6: Plots showing estimated, marginal posterior distributions for aij, i, j = 1, 2, 3,
in Example 2
Example 3
For the ﬁnal example, A was the sparse matrix given as A = tridiag(0.4, 0, 0.4). However,
we let p = 0.9, a larger speciﬁcation than used before and one which should favour
the acceptance of sparse models across iterations. The results are shown graphically in
Figure 4.7.
As with the two previous examples, the effectiveSize was computed in CODA as 9900
and, together with the trace and ACF plots, reveals probable convergence. This is further
emphasised by the Raftery-Lewis test on the variational scores, the results of which are
given below.
127

4.3. Random walks
0
2000
4000
6000
8000
10000
−7330
−7325
−7320
−7315
Trace plot of L(q)
Iteration
L(q)
0
20
40
60
80
100
0.0
0.2
0.4
0.6
0.8
1.0
Lag
ACF
ACF plot for L(q)
Histogram of L(q)
L(q)
Frequency
−7330
−7325
−7320
−7315
0
1000
2000
3000
4000
5000
0
1
2
3
4
5
6
7
8
9
10
0
1
2
3
4
5
6
7
8
9
10
Image plot for Pihat
Row
Col
Figure 4.7: Plots for the analysis of the MCMC output in Example 3
Iterations = 1:9900
Thinning interval = 1
Number of chains = 1
Sample size per chain = 9900
Quantile (q) = 0.025
Accuracy (r) = +/- 0.005
Probability (s) = 0.95
Burn-in
Total Lower bound
Dependence
(M)
(N)
(Nmin)
factor (I)
Lq
2
3843
3746
1.03
Moreover, the Heidelberger-Welch diagnostic exhibits additional conﬁrmation:
Stationarity start
p-value
test
iteration
Lq
passed
1
0.303
128

4.3. Random walks
Halfwidth Mean
Halfwidth
test
Lq
passed
-7317 0.0393
The histogram in Figure 4.7 is most intriguing. Here, very few moves are being accepted
(illustrated by an acceptance rate of just 1.4%), and any movement that is made is to
high-ranking models in the neighbourhood of the truth. By specifying p = 0.9, the plot of
ˆΠ shows only the true edges being selected consistently, and no ‘wrong’ links identiﬁed.
This contrasts slightly to the previous examples. After normalising, the proximity of ˆΠ
to the true adjacency matrix is evident since now S = 0.014.
0.0
0.5
1.0
Value
0
1.4
4.2
7
Density
0
0.2
0.6
1
Probability
0.0
0.5
1.0
Value
0
1.5
3
4.5
6
7.5
Density
0
0.2
0.4
0.6
0.8
1
Probability
0.0
0.5
1.0
Value
0
1.4
4.2
7
Density
0
0.2
0.6
1
Probability
0.0
0.5
1.0
Value
0
1.7
3.4
5.1
6.8
8.5
Density
0
0.2
0.4
0.6
0.8
1
Probability
0.0
0.5
1.0
Value
0
1.7
3.4
5.1
6.8
8.5
Density
0
0.2
0.4
0.6
0.8
1
Probability
0.0
0.5
1.0
Value
0
1.7
3.4
5.1
6.8
8.5
Density
0
0.2
0.4
0.6
0.8
1
Probability
0.0
0.5
1.0
Value
0
1.7
3.4
5.1
6.8
8.5
Density
0
0.2
0.4
0.6
0.8
1
Probability
0.0
0.5
1.0
Value
0
1.7
3.4
5.1
6.8
8.5
Density
0
0.2
0.4
0.6
0.8
1
Probability
0.0
0.5
1.0
Value
0
1.7
3.4
5.1
6.8
8.5
Density
0
0.2
0.4
0.6
0.8
1
Probability
a11
a12
a13
a21
a22
a23
a31
a32
a33
Figure 4.8: Plots showing estimated, marginal posterior distributions for aij, i, j = 1, 2, 3,
in Example 3
Ultimately, the trends in the variational marginal posteriors for a set of coeﬃcients of
A, given above in Figure 4.8, mimic what has been seen in the previous examples. In
129

4.3. Random walks
other words, those entries in the truth, speciﬁed as zero, are again predicted to have a
point mass at zero with very high probability. Moreover, for all true non-zero elements,
pvar(aij | aij ̸= 0, D) is centred around the original speciﬁcation in each case.
4.3.3
Prior sensitivity
The above examples show initially that the Markov chain is reaching its stationary dis-
tribution.
We have tested this by using a combination of trace and ACF plots, the
effectiveSize function in CODA and two diﬀerent convergence diagnostics. Moreover,
we have emphasised that the true sparsity structure is being recognised across iterations,
as seen by the image plot and statistic S, although this is aﬀected by the prior speciﬁca-
tion of p. Finally, approximate marginal posteriors for sets of coeﬃcients aij have been
seen to be accurate, compared to the true speciﬁcation of A.
Henceforth, a prior sensitivity analysis is conducted. Initially, we alter the prior parame-
ters for both a and σ2, whilst using a ﬁxed data-set. This is important, in particular for
the informative prior on a, to gauge if such a choice averts Lindley’s paradox (see Sec-
tion 3.4.2). Subsequently, sensitivity to the choice of p is also analysed. On this occasion,
the dimension is increased such that d = 20. A data-set was simulated with respect to
N = 250, A = tridiag(0.2, 0.4, 0.2) and σ2 = 0.1. Six diﬀerent speciﬁcations of non-zero
cij = c and ﬁve varying choices of α, β were examined (cf. (3.9) and (3.10)). The in-
verse gamma speciﬁcations were amended between α = 1, β = 0.001; α = 1, β = 0.01;
α = 1, β = 0.1; α = 1, β = 1 and α = 10, β = 10. Currently, p was ﬁxed at the true
proportion of zeroes in the A-matrix, namely p = 0.855. Hence, the results obtained will
not be aﬀected by this choice.
Algorithm 5 was initialised as in the previous section, and run for 10,000,000 iterations,
separately for each of the 30 combinations of c and α, β. Again, in each case, the ﬁrst
100,000 were treated as burn-in and the remainder thinned by 1000. Image plots were then
130

4.3. Random walks
produced for all possibilities. It was found that, for every choice of c, the varying speciﬁca-
tions of α, β made often no diﬀerence to each image plot. So, at each Metropolis-Hastings
iteration, a suﬃcient quantity of data was available to estimate the noise parameter, σ2,
extremely well, regardless of the prior speciﬁcation. Therefore, with the noise in the data
identiﬁed, we will be able to establish the correct signals in the truth throughout the
scheme, leading to image plots that are similar to the truth, and to each other.
Consequently, Figure 4.9 contains the plots for altering c where α = 1, β = 0.001 are
now ﬁxed, a ﬂat prior, as mentioned previously. Moreover, Table 4.1 displays the values
of S, computed for each speciﬁcation of c. Here, the impact of Lindley’s paradox can be
ascertained. To produce frame (a), the most diﬀuse prior distribution was used, i.e. the
variance for each non-zero component, aij, was large. In addition, by recalling that every
aij is speciﬁed as either a zero or a free entry, this prior was not concentrated around the
simpler of these two models in each case. So, an intuitive rationale may be that the signal
in the data will be recognised, and hence models, similar and at least as complex as the
truth, would be accepted during the MCMC run.
However, although the sparsity structure is being predicted here to some extent, the para-
dox is visible. That is, models, even simpler than the truth, are being accepted, and hence
favoured (shown in frame (a) by the white rectangles darkening or even disappearing) since
the non-zero elements of A are not being detected in the data. This is illustrated further
by the inaccuracy of S for this image plot. In the Metropolis-Hastings sampler, very few
proposed models are being accepted, hence leading to the lack of complexity shown here.
Indeed, across iterations, the overall acceptance rate of proposals was merely 2%. We
realise that a comparable, but less severe scenario is displayed in frame (b).
In contrast, consider frame (f), which resulted from specifying a highly informative prior.
Correct edges are now only selected infrequently, and more uncertainty has arisen about
the truth. When examining the matrix ˆΠ itself, all incorrect edges are chosen more often
than in the other cases, although to an insuﬃcient level so as to register on the image
131

4.3. Random walks
0
2
4
6
8
10
12
14
16
18
20
0
4
8
12
16
20
Row
Col
0
2
4
6
8
10
12
14
16
18
20
0
4
8
12
16
20
Row
Col
0
2
4
6
8
10
12
14
16
18
20
0
4
8
12
16
20
Row
Col
0
2
4
6
8
10
12
14
16
18
20
0
4
8
12
16
20
Row
Col
0
2
4
6
8
10
12
14
16
18
20
0
4
8
12
16
20
Row
Col
0
2
4
6
8
10
12
14
16
18
20
0
4
8
12
16
20
Row
Col
(a)
(b)
(c)
(d)
(e)
(f)
Figure 4.9: Plots of ˆΠ for diﬀerent speciﬁcations of c: (a) c = 10, 000, (b) c = 10, (c)
c = 0.5, (d) c = 0.1, (e) c = 0.01, (f) c = 0.001
Speciﬁcation of c
S
10, 000
21.244
10
10.018
0.5
6.439
0.1
6.513
0.01
7.078
0.001
11.731
Table 4.1: Comparing the accuracy of ˆΠ for each choice of c
132

4.3. Random walks
plot. Resultantly, the S-value dictates that accuracy has been lost. So, we intimate a
slight tendency to approve denser models, the converse of the paradox, noted by the more
routine acceptance of proposals (23% acceptance rate).
The three foremost speciﬁcations of c appear to be c = 0.5, 0.1, 0.01. When studying
frame (e), ‘non-existent’ edges are occasionally identiﬁed, and hence there is a preference
for models with more complexity. Yet, there is little diﬀerence between frames (c) and
(d), whilst the values of S, when c = 0.5 and c = 0.1, are almost identical. Thus, it is
evident that, in each case, the truth is well represented. We notice that elements a16, 2
and a19, 12 are incorrectly recognised as non-zeroes in both of these plots, although more
recurrently when c = 0.1. Therefore, it is suggested that c = 0.5 is a sensible choice for
this example to compromise between continuous acceptance of simpler or more complex
models.
As a consequence of this sensitivity analysis, the prior parameter speciﬁcations of α =
1, β = 0.001 and c = 0.5 are henceforth maintained for further studies. We now wish
to establish the extent to which results are aﬀected by varying the choice of p.
For
this purpose, we retained d = 20, and utilised the same dataset as simulated above.
Moreover, for each choice of p, the MCMC sampler was also run in an identical fashion.
The speciﬁcations given for p were namely p = 0.95, 0.855, 0.5, 0.3, 0.1. Image plots and
their accuracy to the truth are displayed in Figure 4.10 and Table 4.2 respectively.
When A = tridiag(0.2, 0.4, 0.2), recall that specifying p = 0.855 will induce the correct
level of sparsity for models accepted during the scheme. Thus, it follows that frame (b)
is the best portrayal of the truth. If p is assigned above this level, we would then expect
those models considered too sparse to be in favour. A slight indication of this is revealed
in frame (a), and explains why the value of S has now become more discrepant.
In
addition, the acceptance rate of proposals here is only 3%. Similarly, when p < 0.855,
more dense models are preferred. As p approaches zero, this bias is more conspicuous,
and the acceptance rate rises dramatically; for instance, when p = 0.1, the rate is 58%.
133

4.3. Random walks
0
2
4
6
8
10
12
14
16
18
20
0
4
8
12
16
20
Row
Col
0
2
4
6
8
10
12
14
16
18
20
0
4
8
12
16
20
Row
Col
0
2
4
6
8
10
12
14
16
18
20
0
4
8
12
16
20
Row
Col
0
2
4
6
8
10
12
14
16
18
20
0
4
8
12
16
20
Row
Col
0
2
4
6
8
10
12
14
16
18
20
0
4
8
12
16
20
Row
Col
(a)
(b)
(c)
(d)
(e)
Figure 4.10: Plots of ˆΠ for diﬀerent speciﬁcations of p: (a) p = 0.95, (b) p = 0.855, (c)
p = 0.5, (d) p = 0.3, (e) p = 0.1
Speciﬁcation of p
S
0.95
9.164
0.855
6.439
0.5
8.312
0.3
16.123
0.1
61.563
Table 4.2: Comparing the accuracy of ˆΠ for each choice of p
134

4.3. Random walks
Hence, in frame (e), although most true links are routinely discovered, it is unsurprising
that a plethora of false edges are now common, implying an inﬂated S value. We conclude
that altering the speciﬁcation of p can have an extreme inﬂuence upon subsequent results.
4.3.4
Small sample size
We explore another study, namely to establish how well the true A is represented over
a Metropolis-Hastings run as the sample size N is changed. The choices for d, A and
σ2 are retained from the above section, whereas the prior speciﬁcations are set with
α = 1, β = 0.001, c = 0.5 and p = 0.855. Five separate datasets were simulated for
diﬀering values of N, in particular, N = 100, 80, 50, 20, 10. This was performed in such
a way that the matrix Y , of dimension N × d, would form the ﬁrst N rows of the new
Y for the next highest selection of N. This ensured consistency between the data-sets.
Algorithm 5 was run for each N, whereby initialising graph, length of MCMC run, burn-in
period and thinning ratio were maintained as above.
The outcome of the investigation is summarised in Figure 4.11 and Table 4.3. Thus, it is
apparent that, as N decreases, the algorithm struggles to locate the truth and, as such,
the accuracy of ˆΠ deteriorates. This is to be expected since, in this case, the signal in the
data will be weak. In the ﬁgure, for the higher choices of N, frames (a), (b) and (c) do
show that some correct edges are being continually recognised. However, once N = 20,
the signal disappears completely, and no obvious pattern emerges on the image plots. As
a consequence, values of S increase signiﬁcantly.
One ﬁnal analysis was administered by altering the value of the noise variance, σ2. For
this purpose, we let N = 250, and all other speciﬁcations stated above remained the
same.
The choices of σ2 considered were σ2 = 0.1, 0.5, 1, 5, 10.
Upon running the
MCMC algorithm and constructing image plots in each case, negligible diﬀerence was
seen between Figure 4.9(c) (when σ2 = 0.1) and all other plots. Hence, all S-values were
135

4.3. Random walks
0
2
4
6
8
10
12
14
16
18
20
0
4
8
12
16
20
Row
Col
0
2
4
6
8
10
12
14
16
18
20
0
4
8
12
16
20
Row
Col
0
2
4
6
8
10
12
14
16
18
20
0
4
8
12
16
20
Row
Col
0
2
4
6
8
10
12
14
16
18
20
0
4
8
12
16
20
Row
Col
0
2
4
6
8
10
12
14
16
18
20
0
4
8
12
16
20
Row
Col
(a)
(b)
(c)
(d)
(e)
Figure 4.11: Plots of ˆΠ for diﬀerent speciﬁcations of N: (a) N = 100, (b) N = 80, (c)
N = 50, (d) N = 20, (e) N = 10
Speciﬁcation of N
S
100
19.437
80
23.477
50
35.035
20
47.113
10
51.729
Table 4.3: Comparing the accuracy of ˆΠ for each choice of N
136

4.3. Random walks
akin. So, analogous to varying the prior speciﬁcation on σ2, it follows that adequate
observations are present here to estimate the noise in the data. Thus, whilst the true
noise is changed, the correct sparsity structure can be predicted accurately.
4.3.5
A further example
Hitherto, the Metropolis-Hastings algorithm has been applied to ﬁnd high scoring models
in graphical spaces of small to moderate dimension. Here, a more challenging example is
inspected whereby a dataset is simulated of size N = 250, with noise variance speciﬁed
as σ2 = 0.1 and A = tridiag(0.2, 0.4, 0.2) as erstwhile, but now dimension d = 100. The
parameters for the prior distributions are maintained as α = 1, β = 0.001 and c = 0.5,
but we now let p = 0.9702, the proportion of zeroes in the true A.
It has been customary thus far to run the MCMC sampler for 10, 000, 000 iterations.
However, on this occasion, a problem may be encountered since the amount of compu-
tational time to complete the run can become too great. A simple solution to this is to
make several, shorter, parallel runs of the scheme, each independent and initialised from
a varying starting value (known as a seed). An advantage of this technique is that, by
comparing MCMC output, it is simple to identify any salient diﬀerences between several
seemingly converged chains, if stationarity has yet to be reached.
Thus, ﬁve Markov chains were simulated from diﬀerent, user-speciﬁed seeds in C, each of
length 2, 000, 000 iterations, and initialised from the same choice of G0 as before. Each
chain had the ﬁrst 100, 000 iterations dropped as burn-in, and was then thinned by 1000.
Hence, in total, 9500 iterations remained for analysis purposes. Figure 4.12 reveals the
plots of the output from the algorithm. Notice that the histogram displays the pooled
lower bound values for all ﬁve chains. Here, CODA has been applied to produce an overall
trace plot, whereby the traces of the ﬁve, individual chains are overlaid on top of each
other. Convergence can be realised when all chains possess similar behaviour, and hence
137

4.3. Random walks
are independent of each initial choice of seed.
By examining the plot, the mean and
variance of each chain are similar since the chains all overlap. Thus, there is reasonable
evidence for convergence.
0
500
1000
1500
−102500
−102450
−102400
Trace plot of L(q)
Iterations
L(q)
0
20
40
60
80
100
0.0
0.2
0.4
0.6
0.8
ACF plot for L(q)
Lag
ACF
Histogram of L(q)
L(q)
Frequency
−102500
−102450
−102400
0
200
400
600
800
0
10
20
30
40
50
60
70
80
90
100
0
10
30
50
70
90
Image plot for Pihat
Row
Col
Figure 4.12: Plots for the analysis of the MCMC output in Example 4.3.5
The ACF plot shows the average autocorrelation across all ﬁve chains for a set of lags,
calculated simply by application of the function autocorr.diag. Here, the ACF declines
steadily, reaching zero at around lag 35, whereby the values in the chain are recognised
as being approximately independent.
In hindsight, the chain could be thinned by a
higher factor than 1000 to reduce this initial dependence between consecutive values.
Nevertheless, the trace and ACF plots still show acceptably quick mixing, and thus good
graphical space coverage.
At this stage, it would be beneﬁcial to amplify this view formally by the use of a conver-
138

4.3. Random walks
gence diagnostic. However, both tests discussed previously can be utilised when only one
chain is simulated. Fortunately, in the case of parallel chains, Gelman and Rubin (1992)
formulated such a diagnostic. For a set of m > 1 multiple chains, each containing n iter-
ations with the ﬁrst n
2 then discarded as burn-in, the statistic is based upon comparing
the variance within each chain, and the variance between chains. Thus, to estimate the
variance, κ2, of the stationary distribution, we can compute both W, the mean of the m
within-chain variances, and ˆκ2, the variance of the mn values from all chains combined.
A variance ratio (often referred to as the potential scale reduction factor), denoted as ˆR, is
then computed, dependent upon these two estimates. Consequently, if ˆR is approximately
equal to 1, the variances within and between chains are coinciding, and so there is evidence
that all chains have converged to the stationary distribution, indicated on a trace plot by
overlapping. That is, each run is viewed as being independent of its initial seed choice.
In practice, Gelman et al. (1995) suggest a value of ˆR ≈1.2 should be suﬃcient for this
purpose, otherwise further iterations will be required to improve the estimates, W and
ˆκ2.
As with previous diagnostics, CODA can be used to apply the Gelman and Rubin test. So,
in the current example, the corresponding output was
Iterations = 1:1900
Thinning interval = 1
Number of chains = 5
Sample size per chain = 1900
Potential scale reduction factor:
Point est. 97.5% quantile
[1,]
1.02
1.04
Reported here are the estimated value and 97.5% quantile of ˆR, the latter, an upper
limit, derived from its approximate sampling distribution — see Gelman and Rubin for
additional details. We realise that the ﬁrst n = 950 iterations of each chain are dropped
as burn-in automatically, and so the above values are computed using the lower bounds
139

4.3. Random walks
in the second half of the chains. As ˆR and its upper limit are both close to 1, we can
conclude that 950 iterations were enough to enable convergence, and samples 950 −1900
from all chains are assumed to be draws from the stationary distribution.
We now return to Figure 4.12, and examine the plot of ˆΠ. This was produced by summing
the counting matrices from each of the ﬁve chains, where recall that the total number of
iterations was n = 9500. Upon visual inspection, most true edges are recognised regularly,
whilst, due to the speciﬁcation of p, false links are seldom in favour. In this case, S is
computed as 82.611, a higher value than has been noted for previous examples. Yet, this
is hardly surprising when we realise that the size of the graphical space is now 210,000 −1.
If the size of the dataset were increased, we would expect S to be reduced, since the
algorithm should be able to locate the true sparsity structure with a much stronger signal
now in the data.
Finally, consider Figure 4.13, providing approximate, marginal posterior summaries for a
set of aij. In accordance with the initial speciﬁcation, the estimated posterior probabilities
that a13 and a31 are zero are very high.
Moreover, the plots of pvar(aij | aij ̸= 0, D)
have predicted the true coeﬃcient values with impressive accuracy.
Here, we realise
that it is important to specify both parts of the approximate posterior distribution, as
noted by Scott and Berger (2006).
When studying both a21 and a32, it is seen that
Pvar(aij = 0 | D) ≈0.4 in each case. Yet, the density portion is concentrated around non-
zero values, as we would expect. Again, our uncertainty about these coeﬃcients would
have been reduced if a larger dataset had been simulated.
4.3.6
Application to ERP data
A fresh example of our Metropolis-Hastings algorithm is now presented, where it is now
applied to real time series data, as opposed to the simulated datasets used previously.
This data has been analysed formerly by Delorme et al. (2004), and is freely available at
140

4.3. Random walks
Value
0
1.4
2.8
4.2
5.6
0
0.5
1
Density
0
0.2
0.4
0.6
0.8
Probability
Value
0
1.4
4.2
7
0
0.5
1
Density
0
0.2
0.6
1
Probability
Value
0
1.4
2.8
4.2
5.6
7
0
0.5
1
Density
0
0.2
0.4
0.6
0.8
1
Probability
Value
0
1.6
3.2
4.8
6.4
0
0.5
1
Density
0
0.2
0.4
0.6
0.8
Probability
Value
0
1.4
4.2
7
0
0.5
1
Density
0
0.2
0.6
1
Probability
Value
0
1.6
3.2
4.8
6.4
0
0.5
1
Density
0
0.2
0.4
0.6
0.8
Probability
Value
0
1.4
4.2
7
0
0.5
1
Density
0
0.2
0.6
1
Probability
Value
0
1.5
3
4.5
6
7.5
0
0.5
1
Density
0
0.2
0.4
0.6
0.8
1
Probability
Value
0
1.4
2.8
4.2
5.6
7
0
0.5
1
Density
0
0.2
0.4
0.6
0.8
1
Probability
a11
a12
a13
a21
a22
a23
a31
a32
a33
Figure 4.13: Plots showing estimated, marginal posterior distributions for aij, i, j =
1, 2, 3 in Example 4.3.5
141

4.3. Random walks
http://sccn.ucsd.edu/∼arno/fam2data/publicly available EEG data.html.
The authors
of the paper also provide details about the experiment that gave rise to the data, and this
is now summarised.
Several human subjects participated in an animal categorisation task, involving 100 diﬀer-
ent photographs being shown at random to each volunteer for 20ms, with a small, random
time between each display. Of the images exhibited, half were target photographs, featur-
ing an animal, and the remaining 50 were non-targets, each known as a distractor. The
subjects had to respond within 1s, by releasing a button whenever the picture was an
animal, or keep the button depressed if a distractor was identiﬁed.
During the experiment, for each participant, the electrical activity produced by the brain
(measured in microvolts, denoted μV ) was recorded via d = 32 electrodes placed on the
scalp, hence giving rise to a set of electro-encephalographic (EEG) data. In every case,
this was split into two diﬀerent datasets, representing the display and correct identiﬁcation
of either a target or non-target image. Moreover, the animal and distractor datasets were
further separated into a series of time periods or epochs, each lasting for approximately
3s, in which time one image was displayed. In what follows, N = 250 corresponding,
independent time points were examined in each epoch, a number suﬃcient for decent
analysis. To reduce the amount of data further to a more, manageable level, the EEG
measurements were averaged at each time point across epochs, thus forming a set of event-
related potential (ERP) data. This procedure is also performed in practice, as it makes
the brain response to a particular stimulus more visible graphically.
For our analysis, we want to compare the fast, cerebral processing involved, by taking
ERP measurements, when correctly identifying either a target or non-target image. By
modelling the two sets of 32-electrode data by a zero mean VAR(1) process with unknown
A and σ2, our primary focus is to infer the sparsity structure of A. Moreover, an A-graph
can be constructed, with each node an electrode, determining the neural dynamics of
the information processing.
That is, our interest is to discover which electrodes were
142

4.3. Random walks
signiﬁcant by activating further responses elsewhere, and whether this was consistent
between viewing an animal or a distractor photograph.
We consider the animal and distractor ERP data for one particular subject in the study,
shown respectively in Figures 4.14 and 4.15.
Here, ERP value (in μV ) is shown on
the vertical axis, time (in ms) the horizontal axis. Each ﬁgure displays the position of
every electrode on the scalp and, moreover, ERP measurements at each electrode. In all
cases, the stimulus was administered at time 0ms, hence the scale on the time axis. It
is noticeable that these two ﬁgures reveal similar ERPs at corresponding electrodes. It
will be seen in due course whether the same processing pathways are also involved upon
observation of the two distinct stimuli.
In each case, a data matrix Y of dimension 250 × 32 was formed such that each row
was the measurement for a single time point across all electrodes. Moreover, as we have
assumed that the mean of the process is zero, the data can be centred by subtracting the
sample mean vector of the ERP values at each electrode from every time point. This is a
standard practice to estimate the mean, and is performed to ensure that results will not
be aﬀected if the true mean is signiﬁcantly diﬀerent from zero. Prior speciﬁcations for a
and σ2 were maintained from the previous section.
A valid question to ask at this stage is what might be a sensible choice for p. In general,
we know that each A-matrix in a candidate set has dimension d × d, and so contains d2
elements. Typically, by deﬁnition, the number of non-zeroes in any sparse matrix of such
size will be of order O(d). Suppose that we set p = 1 −1
d. Then, using (4.4), the number
of non-zeroes present will follow a binomial distribution with parameters d2, the number
of trials, and 1 −p = 1
d, the success probability. Moreover, it follows from Johnson et al.
(1992) that this distribution has expected value equal to d, which is clearly of the required
order. Hence, p = 1 −1
d is a general speciﬁcation that induces the correct level of sparsity
for every A-matrix a priori.
143

4.3. Random walks
Figure 4.14: Animal ERP data for one subject
1 
2 
3  
4  
5  
6  
7  
8  
9  
10  
11  
12  
13  
14  
15  
16  
17  
18  
19 
20 
21 
22 
23 
24 
25 
26  
27   
28
29
30 
31 
32  
−52.2
+52.2
−1000
1996
144

4.3. Random walks
Figure 4.15: Distractor ERP data for one subject
1 
2 
3  
4  
5  
6  
7  
8  
9  
10  
11  
12  
13  
14  
15  
16  
17  
18  
19 
20 
21 
22 
23 
24 
25 
26  
27   
28
29
30 
31 
32  
−85
+85
−1000
1996
145

4.3. Random walks
So, in the current scenario, we let p = 31
32. Then, the MCMC scheme was run for both
datasets for 10, 000, 000 iterations, with the same burn-in period, thinning interval and
choice of G0 as used previously. The output for the animal and distractor data is shown
in Figures 4.16 and 4.17 respectively. In each case, the trace plot indicates that the chain
is mixing reasonably well, although slower than has been seen in previous examples. This
is a consequence of fewer proposed moves being accepted. Moreover, the ACF plots reveal
that the autocorrelation only reduces to approximately zero by lag 100. Of course, it is
now beneﬁcial to apply our convergence diagnostics. For the animal data, using CODA, the
test of Raftery-Lewis yielded
Iterations = 1:9900
Thinning interval = 1
Number of chains = 1
Sample size per chain = 9900
Quantile (q) = 0.025
Accuracy (r) = +/- 0.005
Probability (s) = 0.95
Burn-in
Total Lower bound
Dependence
(M)
(N)
(Nmin)
factor (I)
Lq
6
7800
3746
2.08
Moreover, Heidelberger-Welch produced the output
Stationarity start
p-value
test
iteration
Lq
passed
1
0.558
Halfwidth Mean
Halfwidth
test
Lq
passed
-31934 0.798
Likewise, the Raftery-Lewis diagnostic for the chain that arose from the distractor data
supplied the following:
Iterations = 1:9900
Thinning interval = 1
Number of chains = 1
146

4.3. Random walks
0
2000
4000
6000
8000
10000
−31960
−31940
−31920
Trace plot of L(q)
Iteration
L(q)
0
20
40
60
80
100
0.2
0.4
0.6
0.8
1.0
Lag
ACF
ACF plot for L(q)
Histogram of L(q)
L(q)
Frequency
−31970
−31960
−31950
−31940
−31930
−31920
−31910
0
200
400
600
800
1000
0
5
10
15
20
25
30
0
5
10
15
20
25
30
Image plot for Pihat
Row
Col
Figure 4.16: Plots for the analysis of the MCMC output for the animal ERP data
147

4.3. Random walks
0
2000
4000
6000
8000
10000
−31720
−31700
−31680
Trace plot of L(q)
Iteration
L(q)
0
20
40
60
80
100
0.2
0.4
0.6
0.8
1.0
Lag
ACF
ACF plot for L(q)
Histogram of L(q)
L(q)
Frequency
−31730
−31720
−31710
−31700
−31690
−31680
−31670
0
200
400
600
800
0
5
10
15
20
25
30
0
5
10
15
20
25
30
Image plot for Pihat
Row
Col
Figure 4.17: Plots for the analysis of the MCMC output for the distractor ERP data
148

4.3. Random walks
Sample size per chain = 9900
Quantile (q) = 0.025
Accuracy (r) = +/- 0.005
Probability (s) = 0.95
Burn-in
Total Lower bound
Dependence
(M)
(N)
(Nmin)
factor (I)
Lq
10
10996
3746
2.94
Meanwhile, the results of Heidelberger-Welch in this case were
Stationarity start
p-value
test
iteration
Lq
passed
1
0.143
Halfwidth Mean
Halfwidth
test
Lq
passed
-31692 1.23
So, these diagnostics still oﬀer good evidence of convergence to the respective stationary
distributions in each case. We note however that, due to the slower movement of these
chains than before, a longer MCMC run must be used to produce better coverage of
the graphical space, and hence more independent chain values. Along with additional
thinning, this would improve the trace and ACF plots in Figures 4.16 and 4.17.
We now compare the two image plots for the datasets. Here, the axes on these plots
correspond to the respective, numbered electrodes, as pictured on Figures 4.14 and 4.15.
It is clear that these graphs are quite similar. For instance, when examining the main
diagonals, it follows that many nodes have analogous self-loops, and moreover, several
common links are discovered elsewhere. An intriguing feature seen in both cases is however
that not all nodes possess a self-loop. From Chapter 3, this implies that, on the time series
graph, an electrode i at time t −1 will not cause a reaction in the same electrode at the
next time point t, but instead, stimulate other electrodes on the scalp.
In both cases, it is interesting that many electrodes regularly activate a response in elec-
149

4.3. Random walks
trodes 24 and 29 that are, independently, in reasonable proximity to these nodes. In fact,
the converse is true of electrodes 2 and 14 that act as inﬂuencing nodes. Unsurprisingly,
diﬀerences can be spotted. For the distractor data, numerous edges are widespread be-
tween medium and high-numbered electrodes in both directions (as displayed above and
below the main diagonal). Additionally, in the animal case, electrode 4 is stimulated
less frequently whereas electrode 20 has reduced aﬀect over other electrodes. However,
in general, we conclude that, upon display of either a target or non-target image, the
cerebral processing of the stimulus is reasonably consistent.
Finally, we can contrast the approximate posterior distributions for aij, where again i, j =
1, 2, 3, for the animal data (Figure 4.18) and distractor data (Figure 4.19). In both cases,
we can suggest conﬁdently that a13, a21, a23 and a31 are all zero in the true speciﬁcation
of A, since Pvar(aij = 0 | D) is extremely high for these coeﬃcients. The most captivating
plot is that for a32. At ﬁrst glance, we realise that Pvar(a32 = 0 | D) ≈1 for the two
datasets. Yet, despite this, pvar(a32 | a32 ̸= 0, D) takes values that are somewhat distinct
from zero. A simple explanation for this is that very few non-zero values of a32 were
discovered during the MCMC run (as we are extremely certain that this coeﬃcient is
zero). Hence, little weight is given to this density, and so a slight perturbation in the
variational approximation will produce this imprecision.
All other coeﬃcients plotted
would appear to be non-zero signals, and the mode values of the corresponding non-zero
densities between datasets are most alike.
It is also noticeable that, upon close inspection, some of these densities for each dataset
consist of a mixture of components, and thus are multimodal. We have established pre-
viously that any node yj causes a ﬁxed yi on an A-graph at each MCMC iteration if the
element aij, for j = 1, . . ., d, is non-zero. That is, our attention is on the i-th row of A.
Then, the models accepted throughout the sampler will reveal that each yi can be aﬀected
by several nodes to varying degrees; the cumulative eﬀect of this is shown in each image
plot.
150

4.3. Random walks
In fact, during the scheme, many diﬀerent combinations of edges from nodes yj can
inﬂuence yi for numerous j, hence aﬀecting the non-zero value of a particular aik. So,
an intuitive explanation for these multimodal plots is that each peak is a consequence
of one such combination.
Moreover, the highest peak corresponds to the most likely
combination, i.e. that which occurs most regularly in the MCMC run. Similarly, the
next highest peak will arise from the second most plausible combination, and so on.
This feature is intriguing since it was not observed in any previous examples that used
simulated data. To understand this, we realise that the ERP datasets possess a more
complicated structure whereby, judging from each image plot in Figures 4.16 and 4.17,
many edges exist between diﬀerent nodes.
Value
0
9
18
27
36
45
0
0.4
0.8
1.2
Density
0
0.2
0.4
0.6
0.8
1
Probability
Value
0
4
8
12
16
20
−0.2
0.2
0.6
1
Density
0
0.2
0.4
0.6
0.8
1
Probability
Value
0
9
18
27
36
45
−0.4
0
0.4
0.8
Density
0
0.2
0.6
1
Probability
Value
0
9
18
27
36
45
−0.4
0
0.4
0.8
Density
0
0.2
0.4
0.6
0.8
1
Probability
Value
0
5
10
15
20
25
0
0.4
0.8
1.2
Density
0
0.2
0.6
1
Probability
Value
0
11
22
33
44
55
−0.4
0
0.4
0.8
Density
0
0.2
0.4
0.6
0.8
1
Probability
Value
0
4
8
12
16
20
−0.4
0
0.4
0.8
Density
0
0.2
0.6
1
Probability
Value
0
2.8
5.6
8.4
14
−0.6
−0.2
0.2
0.6
Density
0
0.2
0.4
0.6
0.8
1
Probability
Value
0
4
8
12
16
20
0
0.4
0.8
1.2
Density
0
0.2
0.6
1
Probability
a11
a12
a13
a21
a22
a23
a31
a32
a33
Figure 4.18: Plots showing estimated, marginal posterior distributions for aij, i, j =
1, 2, 3, for the animal ERP data
151

4.3. Random walks
Value
0
8
16
24
32
0
0.4
0.8
1.2
Density
0
0.2
0.4
0.6
0.8
Probability
Value
0
4
8
12
16
20
−0.2
0.2
0.6
1
Density
0
0.2
0.6
1
Probability
Value
0
8
16
24
32
40
−0.4
0
0.4
0.8
Density
0
0.2
0.4
0.6
0.8
1
Probability
Value
0
8
16
24
32
40
−0.4
0
0.4
0.8
Density
0
0.2
0.6
1
Probability
Value
0
6
12
18
24
30
0
0.4
0.8
1.2
Density
0
0.2
0.4
0.6
0.8
1
Probability
Value
0
9
18
27
36
45
−0.4
0
0.4
0.8
Density
0
0.2
0.6
1
Probability
Value
0
4
8
12
16
20
−0.4
0
0.4
0.8
Density
0
0.2
0.6
1
Probability
Value
0
1.7
3.4
5.1
6.8
8.5
−0.6
−0.2
0.2
0.6
Density
0
0.2
0.4
0.6
0.8
1
Probability
Value
0
4
8
12
16
20
0
0.4
0.8
1.2
Density
0
0.2
0.6
1
Probability
a11
a12
a13
a21
a22
a23
a31
a32
a33
Figure 4.19: Plots showing estimated, marginal posterior distributions for aij, i, j =
1, 2, 3, for the distractor ERP data
4.3.7
Application to microarray data
Finally, we wish to determine a graphical structure for a set of microarray time series
data. A study of gene expression was conducted in the gram-positive bacterium Bacillus
subtilis whereby d = 9 genes are believed to aﬀect the organism’s decision on whether to
sporulate. Hence, the levels of mRNA are measured for each gene at N = 40 time points.
For the purposes of understanding the subsequent image plot, a number is assigned to
every gene, as shown in Table 4.4. As before, the data was centred by subtracting the
sample mean at every time point, and modelled with a zero mean VAR(1) process with
unknown A and σ2. The optimum sparsity level is now given as p = 8
9, whilst all other
speciﬁcations are preserved.
152

4.3. Random walks
Number
1
2
3
4
5
6
7
8
9
Gene
spo0A
sda
kinA
lexA
dnaA
spoIIAA
clpP
spo0F
spo0B
Table 4.4: Genes examined in the microarray experiment
Figure 4.20 reveals the results of the MCMC run for this dataset.
0
2000
4000
6000
8000
10000
−890
−885
−880
−875
Trace plot of L(q)
Iteration
L(q)
0
20
40
60
80
100
0.0
0.2
0.4
0.6
0.8
1.0
Lag
ACF
ACF plot for L(q)
Histogram of L(q)
L(q)
Frequency
−895
−890
−885
−880
−875
0
500
1000
1500
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
Image plot for Pihat
Row
Col
Figure 4.20: Plots for the analysis of the MCMC output for the microarray data
On this occasion, excellent evidence for convergence is displayed by both the trace and
ACF plots. This is ampliﬁed by the output of the usual diagnostics. For completeness,
Raftery-Lewis returned
Iterations = 1:9900
Thinning interval = 1
Number of chains = 1
Sample size per chain = 9900
153

4.3. Random walks
Quantile (q) = 0.025
Accuracy (r) = +/- 0.005
Probability (s) = 0.95
Burn-in
Total Lower bound
Dependence
(M)
(N)
(Nmin)
factor (I)
Lq
1
3748
3746
1
whereas Heidelberger-Welch issued
Stationarity start
p-value
test
iteration
Lq
passed
1
0.215
Halfwidth Mean
Halfwidth
test
Lq
passed
-878
0.05
To interpret the image plot, we refer back to Table 4.4.
It follows that many genes
stimulate a response in the same gene at consecutive time points, although this is not
the case for gene kinA. Moreover, lexA seems to be rather insigniﬁcant in the decision
making process. Two clear links are spotted between diﬀerent genes in the study, namely
the inﬂuence of spoIIAA over spo0B and the reaction caused by kinA in spo0F.
Variational posterior summaries for a set of aij are provided in Figure 4.21. It appears
that only a11 and a22 are true signals since the estimate of P(aij = 0 | D) for all other
coeﬃcients is approximately equal to 1. Yet, despite this, these latter entries often possess
a plot of pvar(aij | aij ̸= 0, D) that is peaked away from zero. This is merely a consequence
of the size of the dataset — if N was increased, we would anticipate, from previous
experience, greater precision in these densities. It is also observed that the likely values
of a11 and a22 are in close proximity.
154

4.4. Summary
Value
0
0.6
1.2
1.8
2.4
3
0
0.4
0.8
1.2
Density
0
0.2
0.4
0.6
0.8
1
Probability
Value
0
0.55
1.65
2.75
−0.4
0
0.4
0.8
Density
0
0.2
0.4
0.6
0.8
1
Probability
Value
0
0.24
0.72
1.2
−0.4
0
0.4
0.8
1.2
Density
0
0.2
0.4
0.6
0.8
1
Probability
Value
0
0.65
1.95
3.25
−0.5
−0.1
0.3
0.7
Density
0
0.2
0.6
1
Probability
Value
0
0.7
1.4
2.1
2.8
3.5
0
0.4
0.8
1.2
Density
0
0.2
0.4
0.6
0.8
1
Probability
Value
0
0.55
1.65
2.75
−0.8
−0.4
0
0.4
Density
0
0.2
0.4
0.6
0.8
1
Probability
Value
0
0.95
2.85
4.75
−0.6
−0.2
0.2
0.6
Density
0
0.2
0.4
0.6
0.8
1
Probability
Value
0
0.9
1.8
2.7
3.6
4.5
−0.4
0
0.4
0.8
Density
0
0.2
0.4
0.6
0.8
1
Probability
Value
0
0.55
1.65
2.75
−0.5
−0.1
0.3
0.7
Density
0
0.2
0.6
1
Probability
a11
a12
a13
a21
a22
a23
a31
a32
a33
Figure 4.21: Plots showing estimated, marginal posterior distributions for aij, i, j =
1, 2, 3, for the microarray data
4.4
Summary
In this chapter, our main focus was to develop a procedure by which graphical spaces
of increasing dimension could be searched, rapidly and eﬀectively, to locate high ranking
models. Moreover, we wished to explore the posterior distribution on model space. A
Metropolis-Hastings algorithm was constructed whereby, at each iteration, a new model
was proposed uniformly, and then accepted on the basis of an acceptance probability.
This probability, α(·, ·), was itself dependent on the lower bound approximation to the
logarithm of the marginal likelihood for each model, derived by variational Bayes.
A
matrix ˆΠ was constructed to count the number of occasions that an edge between any pair
of nodes was included in the accepted model at each iteration. The algorithm was applied
155

4.4. Summary
to a variety of simulated examples where, inter alia, the prior probability p = P(aij = 0)
was altered. Throughout, the true sparsity structure of A was identiﬁed with impressive
accuracy.
It was also possible to determine an approximate, marginal posterior distribution for each
aij across models. That is, we were able to display graphically the probability that a
particular aij was zero, and the probable magnitude of aij, given that it was non-zero.
For the simulated data, it was feasible to ascertain which aij were actually signals, and,
in that case, estimate their true values precisely. Finally, the MCMC scheme was applied
to two real cases, namely a set of ERP and microarray data. For the former, we con-
cluded that there are many similarities in processing the visual information when correctly
recognising either an animal or distractor photograph, a decision made in equivalent brain
areas. Moreover, in the latter, a gene network was identiﬁed, regarding the decision of a
bacterium to sporulate. In the next chapter, the ideas presented here, and in Chapter 3,
are extended, so that the variational Bayesian treatment is provided to VAR(1) models
that are no-longer assumed to have zero mean.
156

Chapter 5
Generalisation to non-zero mean
VAR(1) models
5.1
Introduction
In Chapter 3, a candidate set of VAR(1) graphical models was constructed, each reliant
upon the diﬀering sparsity structure of the autoregressive matrix A. The evidence for
each model, given by the corresponding marginal likelihood, was approximated using the
variational Bayesian algorithm. Hence, it was possible to determine the more plausible
models in the set.
Recall that, in general, a VAR(1) model of dimension d is speciﬁed as
yt = yt−1A + et,
with the noise vector et ∼N (0, Γ). However, alternatively, the model can be rewritten
157

5.2. Scoring non-zero mean VAR(1) models
in mean-adjusted form (L¨utkepohl, 2005) such that
yt −μ = (yt−1 −μ)A + et,
(5.1)
whereby μ = (μ1, μ2, . . . , μd) = E(yt) for all t, a (1×d) vector, and the noise is distributed
as before. All other dimensions are as previous. (5.1) is now classiﬁed as the non-zero
mean VAR(1) model.
Of course, as L¨utkepohl informs us, this generalisation can be
extended further to the VAR(p) process.
Therefore, using again the graphical representation of the sparse matrix A, we can conduct
a similar procedure to that of Chapter 3 by applying the variational algorithm to derive
a lower bound, LMi(qi), for each graphical model Mi, an approximation to the logarithm
of the corresponding marginal likelihood for each model, p({yt} | Mi).
Consequently,
by ranking models, we seek primarily to learn the sparsity structure of A.
However,
analysis is now complicated since, by model (5.1), the parameter set is now deﬁned as
θ = {A, σ2, μ}. Here, as erstwhile, we let the covariance matrix of the noise vector be
Γ = σ2Id for ease of prior speciﬁcation. Thus, by the deﬁnition of the lower bound (2.6),
we extend previous work by now placing a further prior on μ, and subsequently deducing
an additional variational distribution for this parameter. Henceforth, conditioning and
dependence on Mi is assumed throughout, although not expressed explicitly.
5.2
Scoring non-zero mean VAR(1) models
Suppose that t = 1, . . ., N independent samples of the time series have been collected.
Initially, by rewriting (5.1) as a matrix equation, we derive an expression for the data
likelihood. Deﬁne Y , X and E, with the same dimension, as in Section 3.3, whereby
xt = [yt−1] for all t. Moreover, let M be a N × d matrix, with each row of M given by
158

5.2. Scoring non-zero mean VAR(1) models
the vector μ. Thus, we designate (5.1) in matrix form as
Y −M = [X −M]A + E.
(5.2)
At the end-points, we again allow xN = yN−1.
Moreover, we assume the process is
initialised such that x1 = y0 = μ. As E(yt) = μ for all t, this is, by deﬁnition, the
stationary mean. Recall that this value exists if all eigenvalues of A have modulus less
than 1, hence the VAR(1) process is referred to as stable.
Now, decompose (5.2) into vector form such that
vec(Y −M) = vec([X −M]A + E)
=⇒vec(Y ) −vec(M) = vec(XA) −vec(MA) + vec(E)
=⇒y −m = (Id ⊗X)a −(Id ⊗M)a + e,
(5.3)
using the same results as in Section 3.3. Moreover, let vec(M) = m, a dN ×1 vector, and
allow all other deﬁnitions as before. Determination of the probability density function for
e is exactly as before since, again, et ∼N (0, σ2Id). Consequently, e ∼N (0, Id ⊗σ2IN).
Hence, we can rearrange (5.3) in terms of e, and substitute into the probability density
function of e, (3.6). The exponent of this expression is then
exp

−1
2eT(Id ⊗σ−2IN)e

= exp

−1
2 [y −m −(Id ⊗X)a + (Id ⊗M)a]T (Id ⊗σ−2IN)
× [y −m −(Id ⊗X)a + (Id ⊗M)a]

= exp

−1
2 [vec(Y −M −[X −M]A)]T (Id ⊗σ−2IN) [vec(Y −M −[X −M]A)]

= exp

−1
2Tr
$
σ−2g(A, M)
%
,
(5.4)
159

5.2. Scoring non-zero mean VAR(1) models
where g(A, M) = (Y −M −[X −M]A)T(Y −M −[X −M]A), a d × d matrix and using
corresponding results. Resultantly, given a data-set D = {X, Y }, the probability of the
data is such that
p(D |A, σ2, μ) = (2πσ2)−dN
2 exp

−1
2Tr
$
σ−2g(A, M)
%
.
(5.5)
5.2.1
Priors
As mentioned in Section 5.1, we must denote priors over, not only a = vec(A) and σ2 as
before, but also μ. Therefore, the speciﬁcations are
p(a) = N (a | 0, C∗)
(5.6)
p(σ2) = IG(σ2 | α, β)
(5.7)
p(μ) = N (μ | b, Δ).
(5.8)
There is no motivation to change the priors on both a and σ2, and so these are maintained
from Section 3.3.1. Again, we deﬁne C∗= diag {vec(C)}, where C is indicated by equation
(3.11). We realise that C∗can be rank deﬁcient, hence creating problems in later analysis.
Thus, the sparsity structure on A is retained in the prior by constraining C. The prior
on μ was chosen since the multivariate normal distribution is the typical conjugate choice
for the unknown mean vector of a normal random sample. Of course, these speciﬁcations
are assumed to be independent such that p(a, σ2, μ) = p(a)p(σ2)p(μ).
5.2.2
Free form method
When considering non-zero mean VAR(1) models, the variational distributions, which
approximate each true posterior, are here deﬁned to be q(a | D), q(σ2 | D) and q(μ | D).
160

5.2. Scoring non-zero mean VAR(1) models
We continue by applying the same procedure as in Chapter 3.
That is, a free form
variational method is implemented initially to derive the variationals for σ2, μ and a
respectively in the dense case. Furthermore, a ﬁxed form technique is then employed
to calculate the lower bound and, more importantly, to constrain as a result of a given
sparsity structure.
From a free form approach, we only assume independence between the variational distri-
butions, i.e. q(a, σ2, μ | D) = q(a | D) q(σ2 | D) q(μ| D). This is an approximation since
a, σ2 and μ are not a posteriori independent. This was seen previously for zero mean
VAR(1) models, and hence still holds in the more advanced situation here.
By deﬁnition, the lower bound is speciﬁed as
Lμ(q) =

q(a, σ2, μ | D) log
	p(D | A, σ2, μ) p(a, σ2, μ)
q(a, σ2, μ | D)

da dσ2 dμ,
(5.9)
where the subscript μ on L(q) represents the non-zero mean model. As seen formerly, this
equation can be rewritten as a sum of integrals using the independence of the distributions
involved, and simpliﬁed further by integrating out parameters when necessary. Hence, by
recombining integrals, we can denote Lμ(q) as a functional of q(a | D), q(σ2 | D) and
q(μ | D). These are given respectively by equations (5.10), (5.11) and (5.12) below.
Lμ(q) =

q(a | D)
	 
q(σ2 | D) q(μ | D) log p(D | A, σ2, μ) dσ2 dμ
+ log p(a) −log q(a | D)

da + const.
(5.10)
Lμ(q) =

q(σ2 | D)
	 
q(a | D) q(μ | D) log p(D | A, σ2, μ) da dμ
+ log p(σ2) −log q(σ2 | D)

dσ2 + const.
(5.11)
Lμ(q) =

q(μ | D)
	 
q(a | D) q(σ2 | D) log p(D | A, σ2, μ) da dσ2
+ log p(μ) −log q(μ | D)

dμ + const.
(5.12)
161

5.2. Scoring non-zero mean VAR(1) models
We shall tackle each of these integrals. Examine ﬁrst (5.11). In this equation, we can
substitute for both log p(D | A, σ2, μ) and log p(σ2). Upon comparison, we notice the
similarity between (3.15) and (5.11), the functionals of q(σ2 | D) in both the zero and
non-zero mean cases. Moreover, as the prior on σ2 is identical and the data likelihood of
similar form to the zero mean circumstance, by dropping terms independent of σ2 which
disappear upon diﬀerentiation with respect to q(σ2 | D), we will arrive at an expression
that is akin to (3.16). Thus, we acquire
Lμ(q) =

q(σ2 | D)
	
−dN
2 log σ2 −(σ2)−1
2

q(a | D) q(μ| D) {Tr [g(A, M)]} da dμ
−(α + 1) log σ2 −β(σ2)−1 −log q(σ2 | D)

dσ2 + const.′
(5.13)
Now notice, by the deﬁnition of g(A, M) and the derivation of (5.3), that,
Tr [g(A, M)] = [vec(Y −M −[X −M]A)]T [vec(Y −M −[X −M]A)]
= [y −m −(Id ⊗X)a + (Id ⊗M)a]T [y −m −(Id ⊗X)a + (Id ⊗M)a]
(5.14)
=: h(a, M),
again using identity (3.18), and deﬁning the function h to ease the algebra. Hence, we
take the expectation of this expression with respect to the variational distributions for
both a and μ. Thus, we compute
Eq(a | D)

Eq(μ | D) {Tr [g(A, M)]}

= yTy −[vec(Ω)]T y −ρT(Id ⊗XT)y + ρT(Id ⊗ΩT)y −yTvec(Ω)
+ Eq(μ| D){mT}Eq(μ| D){m} + Tr[Varq(μ| D){m}] + ρT(Id ⊗XT)vec(Ω)
−ρTEq(μ | D)

vec(MTM)

−yT(Id ⊗X)ρ + [vec(Ω)]T (Id ⊗X)ρ
+ Eq(a | D){aT}(Id ⊗XTX)Eq(a | D){a} + Tr[(Id ⊗XTX)Varq(a | D){a}]
162

5.2. Scoring non-zero mean VAR(1) models
−Eq(a | D){aT}(Id ⊗ΩTX)Eq(a | D){a} −Tr[(Id ⊗ΩTX)Varq(a | D){a}] + yT(Id ⊗Ω)ρ
−Eq(μ | D)

$
vec(MTM)
%T
ρ −Eq(a | D){aT}(Id ⊗XTΩ)Eq(a | D){a}
−Tr[(Id ⊗XTΩ)Varq(a | D){a}] + Eq(a | D)

aT 
Id ⊗Eq(μ| D){MTM}

a

.
(5.15)
Here, we deﬁne ρ = Eq(a | D) {a} as before. In addition, let Eq(μ| D){μ} = ω and, moreover,
Eq(μ| D){M} = Ω. That is, by construction of M, Ω is the N×d matrix with each row equal
to the d-vector ω. Furthermore, Eq(μ| D){m} = vec(Eq(μ| D){M}) = vec(Ω). To derive
(5.15), the identities (3.20) and (2.51), used at the corresponding stage in Chapter 3, have
been applied, and moreover (3.5) (by setting P = MT, Q = M). We also realise that
Eq(μ| D){Id⊗M} = Id⊗Eq(μ| D){M} since Id⊗M is merely the block diagonal matrix where
each block is equivalent to M. Taking expectations here is simpliﬁed by the variational a
posteriori independence between a and μ.
We now account for the other terms in (5.15).
As erstwhile, let τ = Varq(a | D) {a}.
Moreover, deﬁne Varq(μ| D){μ} = Λ and Varq(μ| D){m} = Ξ. In the above expression,
we must ﬁnd Tr[Ξ]. However, as m = (μ1, μ1, . . . , μ1, μ2, μ2, . . . , μ2, . . . , μd, μd, . . . μd)T,
with each component repeated N times by deﬁnition, it follows that
Tr[Ξ] = N(Varq(μ| D){μ1} + Varq(μ| D){μ2} + · · · + Varq(μ| D){μd})
= N Tr[Varq(μ| D){μ}]
= N Tr[Λ].
(5.16)
We now endeavour to ﬁnd Eq(μ| D)

vec(MTM)

= vec

Eq(μ | D)

MTM

. It helps to
inspect this problem in component form. So, let M = (mij) such that MT = (mji).
Then, by deﬁnition of matrix multiplication,
[MTM]ij =
N

k=1
mkimkj =
N

k=1
μiμj = Nμiμj,
163

5.2. Scoring non-zero mean VAR(1) models
since each element of the j-th column of M is μj. The expectation of this expression with
respect to q(μ | D) is now taken. Thus,
Eq(μ| D){[MTM]ij} = N Eq(μ | D){μiμj}
= N Eq(μ | D){μi}Eq(μ| D){μj} + NCovq(μ| D){μi, μj}
=
N

k=1
Eq(μ| D){mki}Eq(μ| D){mkj} + N
$
Varq(μ| D){μ}
%
ij
=
$
Eq(μ| D){MT}Eq(μ | D){M}
%
ij + N
$
Varq(μ| D){μ}
%
ij ,
using the deﬁnition of covariance. Consequently, by removing subscripts and returning to
matrix form, we obtain the identity
Eq(μ| D){MTM} = ΩTΩ + NΛ,
(5.17)
and hence Eq(μ | D)

vec(MTM)

= vec(ΩTΩ) + Nvec(Λ). As a result, the ﬁnal term of
(5.15) is now equal to
Eq(a | D)

aT 
Id ⊗Eq(μ | D){MTM}

a

= Eq(a | D)

aT 
Id ⊗[ΩTΩ + NΛ]

Eq(a | D) {a} + Tr[

Id ⊗[ΩTΩ + NΛ]

Varq(a | D){a}]
= ρT 
Id ⊗[ΩTΩ + NΛ]

ρ + Tr[

Id ⊗[ΩTΩ + NΛ]

τ].
By (5.14), the expression (5.15) can be simpliﬁed somewhat. In fact, we reach
Eq(a | D)

Eq(μ| D) {Tr [g(A, M)]}

= h(ρ, Ω) + N Tr[Λ] −NρTvec(Λ) + Tr[(Id ⊗XTX)τ] −Tr[(Id ⊗ΩTX)τ]
−N[vec(Λ)]Tρ −Tr[(Id ⊗XTΩ)τ] + NρT(Id ⊗Λ)ρ
+ Tr[(Id ⊗ΩTΩ)τ] + N Tr[(Id ⊗Λ)τ]
164

5.2. Scoring non-zero mean VAR(1) models
= h(ρ, Ω) + N Tr[Λ] + NρT [(Id ⊗Λ)ρ −2vec(Λ)]
+ Tr
$
Id ⊗
$
(X −Ω)T(X −Ω) + NΛ
%
τ
%
=: h(ρ, Ω) + j(ρ, τ, Ω, Λ),
(5.18)
introducing a further function j to simplify notation. Now, substitute (5.18) into (5.13)
so that the latter is now independent of a and μ. As in Section 3.3.2, we use the Lagrange
multiplier νσ2 to construct the functional ˜Lμ(q) (c.f. (3.21)). Maximising this functional
with respect to q(σ2 | D) then provides
∂˜Lμ(q)
∂q(σ2 | D) = −dN
2 log σ2 −(σ2)−1
2
[h(ρ, Ω) + j(ρ, τ, Ω, Λ)]
−(α + 1) log σ2 −β(σ2)−1 −log q(σ2 | D) −1 + νσ2 = 0.
When comparing this expression with the zero mean case, it is clear that
q(σ2 | D) ∝(σ2)−(α+ dN
2 +1) exp

−(σ2)−1

β + 1
2 [h(ρ, Ω) + j(ρ, τ, Ω, Λ)]

.
Therefore, using the same notation as before, we can easily see that, as in Chapter 3,
q(σ2 | D) = IG(σ2 | γ, δ),
(5.19)
but now with variational parameters speciﬁed as
γ = α + dN
2
(5.20)
δ = β + 1
2 [h(ρ, Ω) + j(ρ, τ, Ω, Λ)] ,
(5.21)
165

5.2. Scoring non-zero mean VAR(1) models
whereby
h(ρ, Ω) = [y −vec(Ω) −(Id ⊗X)ρ + (Id ⊗Ω)ρ]T
× [y −vec(Ω) −(Id ⊗X)ρ + (Id ⊗Ω)ρ]
(5.22)
j(ρ, τ, Ω, Λ) = N Tr[Λ] + NρT [(Id ⊗Λ)ρ −2vec(Λ)]
+ Tr
$
Id ⊗
$
(X −Ω)T(X −Ω) + NΛ
%
τ
%
.
(5.23)
It is worth mentioning that the expression for γ is the same as that in the zero mean case.
That for δ, however, is more complicated; yet, if the mean is equated to zero, then (5.21)
is equivalent to (3.24). We now reconsider (5.12), and ﬁnd the variational distribution for
μ. By substituting the likelihood and prior for μ, given by (5.8), this functional becomes
Lμ(q) =

q(μ | D)
	 
q(a | D) q(σ2 | D)

−dN
2 log 2πσ2
−1
2Tr
$
(σ2)−1g(A, M)
% 
da dσ2 −d
2 log 2π −1
2 log |Δ|
−1
2(μ −b)Δ−1(μ −b)T −log q(μ | D)

dμ + const.
Here, we show care when specifying the prior density since μ is a row vector. By dropping
terms independent of μ, we then arrive at
Lμ(q) =

q(μ | D)
	
−1
2

q(σ2 | D)(σ2)−1 dσ2

q(a | D)Tr [g(A, M)] da
−1
2(μ −b)Δ−1(μ −b)T −log q(μ | D)

dμ + const.′
(5.24)
This expression can be simpliﬁed by the substitution of result (A.5). We now concentrate
on computing the expectation of Tr [g(A, M)] with respect to the variational distribution
q(a | D). To this end, Tr [g(A, M)] is expanded in a slightly diﬀerent way to that seen
166

5.2. Scoring non-zero mean VAR(1) models
previously. That is, we can rewrite (5.14) as
Tr [g(A, M)] =
$
y −m −(Id ⊗X)a + (AT ⊗IN)m
%T
×
$
y −m −(Id ⊗X)a + (AT ⊗IN)m
%
,
using the necessary identities of (3.5). This will aid greatly in later algebra simpliﬁcation.
Taking the afore-mentioned expectation of the above provides
Eq(a | D){Tr [g(A, M)]}
= yTy −mTy −ρT(Id ⊗XT)y + mT( ˆA ⊗IN)y −yTm + mTm + ρT(Id ⊗XT)m
−mT( ˆA ⊗IN)m −yT(Id ⊗X)ρ + mT(Id ⊗X)ρ
+ Eq(a | D){aT}(Id ⊗XTX)Eq(a | D){a} + Tr[(Id ⊗XTX)Varq(a | D){a}]
−Eq(a | D){mTvec(XAAT)} + yT( ˆAT ⊗IN)m −mT( ˆAT ⊗IN)m
−Eq(a | D)

$
vec(XAAT)
%T m

+ mT 
Eq(a | D){AAT} ⊗IN

m.
(5.25)
In the above, we have made use of (3.20) and (2.51). Further, it is realised that
vec(PQR) = (RT ⊗P)vec(Q)
(5.26)
for compatible matrices P, Q, R (Henderson and Searle, 1979). Moreover, we have deﬁned
Eq(a | D){A} = ˆA. As mentioned in Section 3.5, the d×d matrix ˆA is created by unstacking
the d2-vector ρ. So, if ρ = (ρ1, ρ2, . . . , ρd2)T, then
ˆA =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
ρ1
ρd+1
· · ·
ρd(d−1)+1
ρ2
ρd+2
· · ·
ρd(d−1)+2
...
...
...
...
ρd
ρ2d
· · ·
ρd2
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
.
167

5.2. Scoring non-zero mean VAR(1) models
To enable this, we introduce the notation vec−1 to be the inverse vec operator, and hence
write ˆA = vec−1
d (ρ), where the subscript d represents the number of rows in the new
matrix. Thus, the operator is deﬁned such that this subscript must be a divisor of the
length of the vector, and can produce non-square matrices. Notice that, if P is any matrix
with r rows, it follows that vec−1
r [vec(P)] = P.
In (5.25), we now must ﬁnd Eq(a | D){AAT}. In particular, it is apparent that
Eq(a | D){mTvec(XAAT)} = mTvec(XEq(a | D){AAT}).
Again via component form, the
expectation of the (i, j)-th element of AAT, using the deﬁnition of matrix multiplication
and covariance, is seen to be
Eq(a | D){[AAT]ij} = Eq(a | D)

d

k=1
aikajk

=
d

k=1
Eq(a | D){aikajk}
=
d

k=1

Eq(a | D){aik}Eq(a | D){ajk} + Covq(a | D){aik, ajk}

=
$
Eq(a | D){A}Eq(a | D){AT}
%
ij +
d

k=1
$
Varq(a | D){Ak}
%
ij ,
(5.27)
where we deﬁne Ak to be the k-th column vector of A. We can simplify this expres-
sion yet further by examining Varq(a | D){Ak}.
Suppose that the k-th column of Id is
denoted by ik, i.e. Id = (i1 | i2 | . . .| id). Then, it follows that Ak = Aik. We realise that
Varq(a | D) {vec(A)} = τ by the deﬁnition of a. Consequently,
Varq(a | D){Ak} = Varq(a | D){Aik}
= Varq(a | D){vec(Aik)}
= Varq(a | D){(iT
k ⊗Id)vec(A)}
= (iT
k ⊗Id)Varq(a | D){vec(A)}(iT
k ⊗Id)T
= (iT
k ⊗Id)τ(ik ⊗Id)
168

5.2. Scoring non-zero mean VAR(1) models
by (3.5), and since Var{Pw} = P Var{w}P T for any random vector w and ﬁxed matrix
P of suitable dimension. Moreover, for any vector r, it is clear that vec(r) = r. Hence,
substituting back into (5.27) and returning to matrix form provides
Eq(a | D){AAT} = ˆA ˆAT +
d

k=1
(iT
k ⊗Id)τ(ik ⊗Id)
= ˆA ˆAT + (1T
d ⊗Id)τ(1d ⊗Id),
(5.28)
where 1d is deﬁned to be the vector of 1s of length d. Here, we notice the result
d

i=1
d

j=1
(iT
i ⊗Id)τ(ij ⊗Id) = (1T
d ⊗Id)τ(1d ⊗Id).
However, as τ is diagonal, then (5.28) holds. All outstanding terms in (5.25) have now
been accounted for, and this expression in turn can be substituted into (5.24). Again, we
establish ˜Lμ(q) = Lμ(q) + νμ(

q(μ | D) dμ −1), and diﬀerentiate now with respect to
q(μ | D). Accordingly, this leads to
∂˜Lμ(q)
∂q(μ | D) = −γ
2δEq(a | D){Tr [g(A, M)]} −1
2(μ −b)Δ−1(μ −b)T
−log q(μ | D) −1 + νμ = 0.
Upon rearranging and dropping all terms that are independent of μ, in particular those
within Eq(a | D){Tr [g(A, M)]}, we arrive at
q(μ | D) ∝exp

−γ
2δ
	
mT

IdN −( ˆA ⊗IN) −( ˆAT ⊗IN) + ( ˆA ˆAT ⊗IN)
+

(1T
d ⊗Id)τ(1d ⊗Id)

⊗IN
 
m
−mT

y −vec(Y ˆAT) −vec(X ˆA) + vec(X ˆA ˆAT) + vec

X(1T
d ⊗Id)τ(1d ⊗Id)

169

5.2. Scoring non-zero mean VAR(1) models
−

yT −

vec(Y ˆAT)
T
−

vec(X ˆA)
T
+

vec(X ˆA ˆAT)
T
+

vec

X(1T
d ⊗Id)τ(1d ⊗Id)
T 
m

−1
2(μ −b)Δ−1(μ −b)T

,
using (3.5) and that (P + Q) ⊗R = (P ⊗R) + (Q ⊗R). This expression can be further
simpliﬁed:
q(μ | D) ∝exp

−γ
2δ
	
mT
 
Id −ˆA
 
Id −ˆA
T
+ (1T
d ⊗Id)τ(1d ⊗Id)

⊗IN

m
−mT

vec

Y −X ˆA
 
Id −ˆA
T
+ X(1T
d ⊗Id)τ(1d ⊗Id)
 
−
	
vec

Y −X ˆA
 
Id −ˆA
T
+ X(1T
d ⊗Id)τ(1d ⊗Id)
 T
m

−1
2(μ −b)Δ−1(μ −b)T

,
(5.29)
whereby y = vec(Y ) and Id ⊗IN = IdN (Harville, 1997).
Here, there is an evident
problem.
We require a variational distribution for μ whereas the expression above is
given predominantly in terms of m = vec(M). Recall that M is a matrix with each row
equivalent to μ. So, we need to manipulate such terms, in particular, those of the form
mTr and mTCm for a given dN-vector r and dN × dN matrix C.
With regards to the former, notice that, by deﬁnition, m = μT ⊗1N, remembering that
μ is a row vector. Thus, by introducing a N × d matrix R such that R = vec−1
N (r), it
follows that
mTr = rTm = mTvec(R)
= (μT ⊗1N)Tvec(R)
= (μ ⊗1N
T)vec(R)
= vec(1N
TR μT)
= 1N
TR μT,
(5.30)
170

5.2. Scoring non-zero mean VAR(1) models
where (5.26) has been applied. On the other hand, we also realise that
mTCm = (μT ⊗1N)T C (μT ⊗1N)
= (μ ⊗1N
T) C (μT ⊗1N)
= (μ ⊗1)(Id ⊗1N
T) C (Id ⊗1N)(μT ⊗1)
= μ(Id ⊗1N
T) C (Id ⊗1N)μT,
(5.31)
noting (3.20) and that P ⊗z = zP for any matrix P and scalar z. Hence, we can utilise
(5.30) and (5.31) to ensure that each term of (5.29) is written in terms of μ. To eﬀect
this, we must be aware that 1N T1N = N. As a result, (5.29) can be expressed in the form
q(μ | D) ∝exp

−1
2
$
μΥμT −ΘμT −μΘT%
(5.32)
∝exp

−1
2
$
(μ −ΘΥ−1)Υ(μ −ΘΥ−1)T%
,
where we have
Υ = Nγ
δ

Id −ˆA
 
Id −ˆA
T
+ (1T
d ⊗Id)τ(1d ⊗Id)

+ Δ−1
(5.33)
Θ = γ
δ 1N
T

Y −X ˆA
 
Id −ˆA
T
+ X(1T
d ⊗Id)τ(1d ⊗Id)

+ bΔ−1.
(5.34)
So ultimately, the variational distribution for μ is such that
q(μ | D) = N (μ | ω, Λ)
(5.35)
whereby
ω = ΘΥ−1
(5.36)
Λ = Υ−1.
(5.37)
171

5.2. Scoring non-zero mean VAR(1) models
Application of the free form method is concluded by seeking a form for q(a | D). By sub-
stitution of terms into (5.10) and dropping those independent of a, hence, in accordance
with (3.25), we acquire
Lμ(q) =

q(a | D)
	
−γ
2δ

q(μ | D)Tr [g(A, M)] dμ
−1
2aTC∗−1a −log q(a | D)

da + const.′,
(5.38)
moreover via (A.5). If Tr [g(A, M)] is expanded with respect to (5.14), then, in comparison
with (5.15), taking expectations implies
Eq(μ| D){Tr [g(A, M)]}
= yTy −[vec(Ω)]T y −aT(Id ⊗XT)y + aT(Id ⊗ΩT)y −yTvec(Ω) + [vec(Ω)]T [vec(Ω)]
+ N Tr[Λ] + aT(Id ⊗XT)vec(Ω) −aTvec(ΩTΩ) −NaTvec(Λ) −yT(Id ⊗X)a
+ [vec(Ω)]T (Id ⊗X)a + aT(Id ⊗XTX)a −aT(Id ⊗ΩTX)a + yT(Id ⊗Ω)a
−
$
vec(ΩTΩ)
%T a −N [vec(Λ)]T a −aT(Id ⊗XTΩ)a + aT 
Id ⊗{ΩTΩ + NΛ}

a,
in particular, as a consequence of (5.16) and (5.17). We now feed this expression into
(5.38), and subsequently form ˜Lμ(q) as before, using the Lagrange multiplier νa. Opti-
mising this functional with respect to q(a | D) provides
∂˜Lμ(q)
∂q(a | D) = −γ
2δEq(μ| D){Tr [g(A, M)]} −1
2aTC∗−1a −log q(a | D) −1 + νa = 0.
By rearranging and dropping all terms that do not depend on a, we obtain
q(a | D) ∝exp

−1
2
	
aTγ
δ

(Id ⊗XTX) −(Id ⊗ΩTX) −(Id ⊗XTΩ) + (Id ⊗ΩTΩ)
+ N(Id ⊗Λ)

+ C∗−1
a
172

5.2. Scoring non-zero mean VAR(1) models
−γ
δ aT
(Id ⊗XT)y −(Id ⊗ΩT)y −(Id ⊗XT)vec(Ω) + vec(ΩTΩ) + Nvec(Λ)

−γ
δ

yT(Id ⊗X) −yT(Id ⊗Ω) −[vec(Ω)]T (Id ⊗X) +
$
vec(ΩTΩ)
%T + N [vec(Λ)]T 
a

.
Since vec(ΩTΩ) = (Id ⊗ΩT)vec(Ω), this equation can be easily rewritten as
q(a | D) ∝exp

−1
2
	
aT
γ
δ

Id ⊗
$
(X −Ω)T(X −Ω) + NΛ
% 
+ C∗−1
a
−aT
γ
δ

 
Id ⊗[X −Ω]T
(y −vec(Ω)) + Nvec(Λ)

−
γ
δ

[y −vec(Ω)]T (Id ⊗[X −Ω]) + N [vec(Λ)]T 
a

.
It is apparent that this is now in the form as given by (5.32), and consequently
q(a | D) = N (a | ρ, τ)
(5.39)
as in the zero-mean case, but now with ρ and τ equivalent to
ρ =
	γ
δ

Id ⊗
$
(X −Ω)T(X −Ω) + NΛ
% 
+ C∗−1−1
×
	γ
δ

 
Id ⊗[X −Ω]T
(y −vec(Ω)) + Nvec(Λ)

(5.40)
τ =
	γ
δ

Id ⊗
$
(X −Ω)T(X −Ω) + NΛ
% 
+ C∗−1−1
.
(5.41)
Thus, the expressions for the variational parameters {γ, δ, ω, Λ, ρ, τ} are (5.20), (5.21),
(5.36), (5.37), (5.40) and (5.41) respectively. Once again, these are update equations,
which must be solved iteratively.
173

5.2. Scoring non-zero mean VAR(1) models
5.2.3
Fixed form method
The variational distributions for both σ2, μ and a, when this vector is dense, have now
been accounted for. However, as in Chapter 3, we must now derive the approximation
for a in the sparse case, i.e. we constrain elements of ρ and τ to zero relative to the
sparsity structure of a given A-matrix. Consequently, the ﬁxed form procedure is again
most beneﬁcial.
Therefore, we assume ﬁxed forms for each variational, as given by (5.19), (5.35) and (5.39)
in the free form approach. Recall that an inherent component of this method is to derive
the lower bound initially. So, using again the independence of both prior and variational
distributions, (5.9) can be split up into a sum of integrals:
Lμ(q) =

q(a | D)q(σ2 | D)q(μ| D) log p(D | A, σ2, μ) da dσ2 dμ +

q(a | D) log p(a) da
+

q(σ2 | D) log p(σ2) dσ2 +

q(μ | D) log p(μ) dμ −

q(a | D) log q(a | D) da
−

q(σ2 | D) log q(σ2 | D) dσ2 −

q(μ | D) log q(μ | D) dμ.
(5.42)
Each integral is subsequently tackled in turn. Thus, initially, it follows that

q(a | D)q(σ2 | D)q(μ | D) log p(D | A, σ2, μ) da dσ2 dμ
=

q(a | D)q(σ2 | D)q(μ | D)
	
−dN
2 log 2πσ2 −1
2Tr
$
(σ2)−1g(A, M)
%
da dσ2 dμ
= −dN
2 log 2π −dN
2

q(σ2 | D) log σ2 dσ2
−1
2

q(a | D)q(μ| D)Tr [g(A, M)] da dμ

q(σ2 | D)(σ2)−1 dσ2
= −dN
2 log 2π −dN
2 [log δ −ψ(γ)] −γ
2δ [h(ρ, Ω) + j(ρ, τ, Ω, Λ)] ,
(5.43)
via equations (A.5), (A.6) and (5.18). In addition, using (2.51) and the variational distri-
174

5.2. Scoring non-zero mean VAR(1) models
bution for μ,

q(μ | D) log p(μ) dμ
=

q(μ | D)
	
−d
2 log 2π −1
2 log |Δ| −1
2 (μ −b) Δ−1 (μ −b)T

dμ
= −d
2 log 2π −1
2 log |Δ| −1
2Eq(μ| D) {μ −b} Δ−1Eq(μ| D)

[μ −b]T
−1
2Tr

Δ−1Varq(μ| D)

[μ −b]T
= −d
2 log 2π −1
2 log |Δ| −1
2 (ω −b) Δ−1 (ω −b)T −1
2Tr
$
Δ−1Λ
%
.
Furthermore, using the above computation,

q(μ | D) log q(μ | D) dμ
= −d
2 log 2π −1
2 log |Λ| −1
2Eq(μ| D) {μ −ω} Λ−1Eq(μ | D)

[μ −ω]T
−1
2Tr

Λ−1Varq(μ| D)

[μ −ω]T
= −d
2 log 2π −1
2 log |Λ| −d
2,
as Tr [Λ−1Λ] = Tr [Id] = d. At this moment, we notice that, as the prior and variational
distributions for both a and σ2 are identical for both the zero and non-zero mean cases
(albeit with diﬀering update equations for the variational parameters), all other integrals
in (5.42) have been previously evaluated in Section 3.3.3. Thus, to recap, we have

q(a | D) log p(a) da = −d2
2 log 2π −1
2 log |C∗| −1
2ρTC∗−1ρ −1
2Tr

C∗−1τ

.

q(σ2 | D) log p(σ2) dσ2 = α log β −log Γ(α) −(α + 1)[log δ −ψ(γ)] −βγ
δ .

q(a | D) log q(a | D) da = −d2
2 log 2π −1
2 log |τ| −d2
2 .

q(σ2 | D) log q(σ2 | D) dσ2 = −log Γ(γ) −log δ + (γ + 1)ψ(γ) −γ.
175

5.2. Scoring non-zero mean VAR(1) models
Ultimately, by substituting back into (5.42), the lower bound is given by
Lμ(q) = −dN
2 log 2π −dN
2 log δ + dN
2 ψ(γ) −γ
2δ [h(ρ, Ω) + j(ρ, τ, Ω, Λ)] −1
2 log |C∗|
−1
2ρTC∗−1ρ −1
2Tr

C∗−1τ

+ α log β −log Γ(α) −α log δ + αψ(γ)
−βγ
δ −1
2 log |Δ| −1
2 (ω −b) Δ−1 (ω −b)T −1
2Tr
$
Δ−1Λ
%
+ 1
2 log |τ| + d2
2 + log Γ(γ) −γψ(γ) + γ + 1
2 log |Λ| + d
2.
(5.44)
As before, we realise that (3.40) can be applied to compute the logarithm of the deter-
minant for any singular matrix in (5.44), in particular, for C∗and τ. If we knew μ = 0,
then, of course, this expression would simplify down to the lower bound in the zero mean
case, given by (3.30). We now consider the maximisation of Lμ(q) with respect to ρ and
τ, and hence subsequently, enforce the sparsity constraint. By diﬀerentiating with respect
to ρ, we have
∂Lμ(q)
∂ρ
= ∂
∂ρ

−γ
2δ [h(ρ, Ω) + j(ρ, τ, Ω, Λ)] −1
2ρTC∗−1ρ

= ∂
∂ρ

−γ
2δ [y −vec(Ω) −(Id ⊗[X −Ω]) ρ]T [y −vec(Ω) −(Id ⊗[X −Ω]) ρ]
−γ
2δ

NρT [(Id ⊗Λ)ρ −2vec(Λ)]

−1
2ρTC∗−1ρ

= ∂
∂ρ

ρTHρ + cTρ

,
whereby
H = −γ
2δ

Id ⊗
$
(X −Ω)T(X −Ω) + NΛ
% 
−1
2C∗−1
(5.45)
cT = γ
δ

(y −vec(Ω))T (Id ⊗[X −Ω]) + N [vec(Λ)]T 
,
(5.46)
and [(Id ⊗[X −Ω]) ρ]T [y −vec(Ω)] = [y −vec(Ω)]T [(Id ⊗[X −Ω]) ρ].
Therefore, we
have the quadratic programming problem (3.31) that was faced in Section 3.3.3, with
176

5.2. Scoring non-zero mean VAR(1) models
diﬀerent speciﬁcations of H and cT. By again deﬁning ρ1 as the non-zero elements of a
given A, we must thus maximise ρT
1 H11ρ1 + cT
1 ρ1 with respect to ρ1 where H11 and c1
are deﬁned as previous. Resultantly, upon optimisation and solving for ρ1, it is evident
that
ρ1 =
 γ
δ

Id ⊗
$
(X −Ω)T(X −Ω) + NΛ
% 
+ C∗−1
11
−1
×
γ
δ

 
Id ⊗[X −Ω]T
(y −vec(Ω)) + Nvec(Λ)

1.
(5.47)
This deﬁnition of ρ1 is again used to reconstruct ρ, according to the prescribed sparsity
structure. Recall that the subscript notation refers to choosing the correct submatrix H11
and subvector c1, following row and column permutation.
When maximising Lμ(q) with respect to τ, as in the zero mean case, it is apparent
that the sparsity constraint cannot be enforced using quadratic programming. Thus, we
derive an expression for those elements of τ with non-zero variational posterior variance
in component form as before. The prior distributions for σ2 and μ, parameters unaﬀected
by sparsity, remain speciﬁed by equations (5.7) and (5.8), whereas that for a is denoted
by (3.33), a product of univariate Gaussians. Using a ﬁxed form variational procedure,
the variational distribution for a is again given, in component form, by (3.34).
We thus strive to re-calculate (5.42) at the component level. By using the identity (3.35),
the probability of the data can be rewritten as
p(D | {aij}, σ2, μ)
= (2πσ2)−dN
2 exp

−(σ2)−1
2
Tr
$
(Y −M −[X −M]A)T(Y −M −[X −M]A)
%
= (2πσ2)−dN
2 exp

−(σ2)−1
2
N

j=1
d

k=1

[Y −M −[X −M]A]jk
2

= (2πσ2)−dN
2 exp
⎧
⎨
⎩−(σ2)−1
2
N

j=1
d

k=1

yjk −mjk −
d

i=1
xjiaik +
d

i=1
mjiaik
2⎫
⎬
⎭.
177

5.2. Scoring non-zero mean VAR(1) models
The deﬁnition of matrix multiplication is again noted. Hence, the ﬁrst integral of (5.42)
is computed as

q(a | D)q(σ2 | D)q(μ | D) log p(D | {aij}, σ2, μ) da dσ2 dμ
=

q(a | D)q(σ2 | D)q(μ | D)

−dN
2 log 2πσ2
−(σ2)−1
2
N

j=1
d

k=1

yjk −mjk −
d

i=1
xjiaik +
d

i=1
mjiaik
2 
da dσ2 dμ
= −dN
2 log 2π −dN
2 [log δ −ψ(γ)]
−γ
2δ
N

j=1
d

k=1

q(a | D)q(μ| D)

yjk −mjk −
d

i=1
xjiaik +
d

i=1
mjiaik
2
da dμ,
via (5.43). Moreover, the double integral in the above expression can be calculated as
Eq(a | D)
⎧
⎨
⎩Eq(μ| D)
⎧
⎨
⎩

yjk −mjk −
d

i=1
xjiaik +
d

i=1
mjiaik
2⎫
⎬
⎭
⎫
⎬
⎭
= y2
jk +
$
Eq(μ | D){mjk}
%2 + Varq(μ| D){mjk} +

d

i=1
xjiEq(a | D){aik}
2
+
d

i=1
x2
jiVarq(a | D){aik} + Eq(μ | D)
⎧
⎨
⎩

d

i=1
mjiEq(a | D){aik}
2⎫
⎬
⎭
+ Eq(μ| D)

d

i=1
m2
jiVarq(a | D){aik}

−2yjkΩjk −2yjk
d

i=1
xjiρ(i,k)
+ 2yjk
d

i=1
Ωji ρ(i,k) + 2Ωjk
d

i=1
xji ρ(i,k) −2Eq(μ| D)

mjk
d

i=1
mji ρ(i,k)

−2Eq(a | D)

d

i=1
xjiaik
d

u=1
Ωjuauk

,
(5.48)
using (2.51) and previous deﬁnitions of variational parameters. Recall that ρ(i,k) and τ(i,k)
are the variational posterior mean and variance corresponding to element aik respectively.
178

5.2. Scoring non-zero mean VAR(1) models
Notice that, in the ﬁnal line, the indices i and u are used, merely to distinguish the two
summations. Since we only require to maximise Lμ(q) with respect to τ, a little extra
work can be saved by computing only those outstanding terms in (5.48) that will depend
upon τ. Clearly,  d
i=1 x2
jiVarq(a | D){aik} =  d
i=1 x2
jiτ(i,k). Moreover,
Eq(μ| D)

d

i=1
m2
jiVarq(a | D){aik}

=
d

i=1
τ(i,k)
$
Eq(μ | D){mji}
%2 + Varq(μ| D){mji}

=
d

i=1
τ(i,k)

Ω2
ji + Λii

.
Here, we realise that Varq(μ| D){mji} = Varq(μ| D){μi} = Λii, by construction of
M = (mij). Furthermore,
Eq(a | D)

d

i=1
xjiaik
d

u=1
Ωjuauk

= Eq(a | D)

d

i=1
xjiaik

Eq(a | D)

d

u=1
Ωjuauk

+
d

i=1
d

u=1
xjiΩjuCovq(a | D){aik, auk}
=
d

i=1
xjiρ(i,k)
d

u=1
Ωjuρ(u,k) +
d

i=1
xjiΩjiτ(i,k).
Notice that Covq(a | D){aik, auk} ̸= 0 only if i = u as τ is diagonal. The above computations
can then be substituted back into (5.48). Consequently, by dropping all terms independent
of τ, we obtain

q(a | D)q(σ2 | D)q(μ | D) log p(D | {aij}, σ2, μ) da dσ2 dμ
∝−γ
2δ
N

j=1
d

k=1
d

i=1
τ(i,k)
$
(xji −Ωji)2 + Λii
%
.
Calculating the additional integrals in (5.42) is straightforward since the prior and vari-
179

5.2. Scoring non-zero mean VAR(1) models
ational posterior for a are, in eﬀect, identical to those in the zero mean case. Hence, to
recap the results from Section 3.3.3,

q(a | D) log p(a) da = −

(p,q)∈I

1
2 log 2π + 1
2 log C∗
(p,q) + 1
2
τ(p,q)
C∗
(p,q)
+ 1
2
ρ2
(p,q)
C∗
(p,q)


q(a | D) log q(a | D) da = −

(p,q)∈I
	1
2 log 2π + 1
2 log τ(p,q) + 1
2

,
where (p, q) ∈I if and only if element apq ̸= 0. All other terms in (5.42) are independent
of τ. Thus, in component form and as a function of τ, the lower bound is now such that
Lμ(q) ∝−γ
2δ
N

j=1
d

k=1
d

i=1
τ(i,k)
$
(xji −Ωji)2 + Λii
%
−1
2

(p,q)∈I

τ(p,q)
C∗
(p,q)
−log τ(p,q)

. (5.49)
Maximising (5.49) with respect to the element τ(p,q) thus provides
∂Lμ(q)
∂τ(p,q)
= −γ
2δ
 N

j=1
$
(xjp −Ωjp)2%
+ NΛpp

−1
2

1
C∗
(p,q)
−
1
τ(p,q)

.
Upon equating to zero, this equation can be quickly solved for the non-zero τ(p,q). Hence,
the diagonal elements of τ are declared as
τ(p,q) =
⎧
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎩

1
C∗
(p,q)
+ γ
δ
N

j=1
(xjp −Ωjp)2 + Nγ
δ Λpp
−1
if apq ̸= 0
0
if apq = 0
.
To conclude, update equations have been derived for the variational parameters of σ2,
namely γ and δ, and, moreover, for those of μ, that is ω and Λ. Furthermore, we can use
ρ1 and τ(p,q) to construct ρ and τ, the parameters of a. By running until convergence,
parameter values for q(σ2 | D), q(μ | D) and q(a | D) are acquired. Of course, the converged
value of Lμ(q) will provide evidence for each model. As before, we can rewrite the lower
180

5.3. Toy example
bound (5.44) so that constant terms across models are disregarded:
Lμ(q) ∝−dN
2 log δ −γ
2δ [h(ρ, Ω) + j(ρ, τ, Ω, Λ)] −1
2 log |C∗| −1
2ρTC∗+ρ
−1
2Tr

C∗+τ

−α log δ −βγ
δ −1
2 (ω −b) Δ−1 (ω −b)T
−1
2Tr
$
Δ−1Λ
%
+ 1
2 log |τ| + 1
2 log |Λ|.
(5.50)
Here, C∗is now inverted using the Moore-Penrose inverse, C∗+, as explained in Sec-
tion 3.4.1.
5.3
Toy example
The methodology discussed thus far in this chapter is elucidated via a straightforward
example. Here, we examine an arbitrary non-zero mean VAR(1) model. In fact, the true
model is chosen with speciﬁcations identical to those given in the corresponding example
for the zero mean case in Section 3.5. Moreover here, the mean is speciﬁed as μ = (1, 1),
a row vector. Consequently, a dataset was generated from the model (5.1), where A is
represented graphically by Figure 3.3. We let x1 = μ and x250 = y249. Again, a candidate
set of 15 A-graphs, ignoring the null graph, is constructed, each of which is scored using
Lμ(q).
The choice of prior parameter values is made as before for both a and σ2. That is, we
avoid Lindley’s paradox, namely that a simpler model will be favoured as the prior is
made to be more diﬀuse, by choosing an informative prior of the form N (0, C∗) on a,
where cij ∈{0, 0.5}. Moreover, a vague IG(1, 0.001) prior is speciﬁed on σ2. For further
details, refer back to Section 3.4.2. Furthermore, a new prior is required for μ. This is
denoted as
p(μ) = N (μ | 0, 10, 000Id).
181

5.3. Toy example
In this speciﬁcation, it seems sensible to centre the distribution at the zero mean case.
Each element of the vector μ is then assigned prior variance equal to 10, 000. Hence,
prior ignorance is represented since the prior is not concentrated around any particular
value. By running all update equations until convergence, we sought to ﬁnd values for
the variational parameters for each variational distribution, given by (5.19), (5.35) and
(5.39). As erstwhile, convergence of update equations and lower bound values took 4
iterations. The expressions for ω and Λ were run ﬁrst so that, to start the algorithm,
initial, arbitrary choices were made such that γ = δ = 1 and ρ(i,j) = τ(i,j) = 1, whenever
aij ̸= 0.
The results obtained in this case are revealed in Table 5.1. Consider ﬁrstly the values of
the lower bound, Lμ, Mi(qi). As in the zero mean case, the true A was deemed to be the
most plausible model in the candidate set. Moreover, those models, containing at least
the two, correct free elements, were again ranked highly. Hence, the more complex models
were penalised suﬃciently by the choice of informative prior variance on a. Candidates
with neither of the true non-zero elements of A unsurprisingly fared poorly, thus indicating
a very weak signal in the data for the two, true zero elements being non-zero.
In addition, the posterior means of A, σ2 and μ are now inspected and compared to the
truth. With a dataset of size N = 250, ˆA and Eq(μ | D){μ} are reasonably close to the
truth for each candidate A-matrix. In fact, in both cases, the estimates are extremely
akin to each other. In particular, if we misspecify the model, the estimates for μ, the
point about which the data ﬂuctuates, are unaﬀected. However, those for σ2 tend to show
more discrepancy, a scenario also seen in the zero mean case. If a candidate model was
speciﬁed with at least the correct free elements seen in the truth, the afore-mentioned
estimates were extremely accurate. Yet, if the wrong model was chosen, i.e. an incorrect
sparsity structure of A, the resulting error provided inaccuracy in Eq(σ2 | D)(σ2).
Finally, we compare Tables 3.1 and 5.1, the zero and non-zero mean models respectively.
It is seen in these tables that the estimates of σ2 for each candidate are almost identical.
182

5.3. Toy example
Speciﬁcation
Posterior means
Lμ, Mi(qi)
A-matrix
ˆA-matrix
Eq(σ2 | D){σ2}
Eq(μ | D){μ}
∗
0
0
0

−0.069
0
0
0

0.134
(0.973, 0.951)
−1142.389
0
0
0
∗

0
0
0
−0.055

0.134
(0.973, 0.951)
−1142.610
0
∗
0
0

0
0.667
0
0

0.109
(0.973, 0.951)
−1091.392

0
0
∗
0


0
0
0.338
0

0.125
(0.974, 0.951)
−1126.006
∗
∗
0
0

−0.070
0.662
0
0

0.109
(0.973, 0.951)
−1093.702
∗
0
∗
0

−0.072
0
0.338
0

0.125
(0.973, 0.951)
−1128.282
0
∗
0
∗

0
0.668
0
−0.060

0.109
(0.973, 0.951)
−1093.840

0
0
∗
∗


0
0
0.338
−0.055

0.125
(0.973, 0.951)
−1128.561

∗
0
0
∗


−0.069
0
0
−0.055

0.134
(0.973, 0.951)
−1144.945
0
∗
∗
0


0
0.669
0.340
0

0.100
(0.974, 0.952)
−1073.091
∗
∗
0
∗

−0.070
0.663
0
−0.060

0.109
(0.973, 0.951)
−1096.150

∗
0
∗
∗


−0.072
0
0.338
−0.055

0.125
(0.973, 0.951)
−1130.837

∗
∗
∗
0


−0.073
0.664
0.341
0

0.100
(0.974, 0.952)
−1075.335
0
∗
∗
∗


0
0.670
0.340
−0.060

0.100
(0.974, 0.952)
−1075.536
∗
∗
∗
∗

−0.073
0.665
0.340
−0.060

0.100
(0.974, 0.952)
−1077.778
Table 5.1: Lower bounds and posterior means for each non-zero mean VAR(1) model
183

5.4. Taking a random walk
Here, there is a correspondence between the two cases whereby, if we select an A-matrix
with an erroneous sparsity pattern, the variational posterior mean of the noise variance
suﬀers, regardless of our beliefs about μ. The most notable diﬀerence stems from the
values of LMi(qi) and Lμ, Mi(qi). Although the correct model is selected in each case, the
lower bound values are higher when μ = 0. When the mean is unknown, the resulting
uncertainty in the problem implies that the approximate evidence for each model, denoted
by Lμ, Mi(qi), is reduced. This may also account for the slight discrepancy between the
corresponding ˆA-matrices. These estimates are very similar, although marginally more
inaccurate to the truth in the non-zero mean case.
5.4
Taking a random walk
Hitherto in this chapter, variational Bayesian methods have been utilised to derive an ap-
proximation, Lμ, Mi(qi), to the logarithm of the marginal likelihood, p(D | Mi). Hence, we
were able to score non-zero mean VAR(1) models, in particular for graphs with a small
number of nodes. However, we can apply the methods of Chapter 4 to ﬁnd the most
plausible models in graphical spaces of higher dimension. In particular, the variational al-
gorithm, presented in this chapter, can again be embedded within the Metropolis-Hastings
scheme, given by Algorithm 5, so that a random walk can be made across the space. The
principles behind the MCMC algorithm remain the same — a new model is proposed
by the addition or deletion of a randomly selected edge from the current model, and is
accepted on the basis of a log acceptance probability.
For analysis, trace and ACF plots can be used to test for the convergence of the chain, as
well as the more formal diagnostics previously described. Furthermore, image plots of the
counting matrix ˆΠ are produced, which will be dependent on the choice of p = P(aij = 0).
When using simulated data, ˆΠ can then be normalised, and hence compared to the true
adjacency matrix Π by computing the residual sum of squares, S (c.f. (4.11)).
184

5.4. Taking a random walk
Moreover, approximate posterior summaries can be produced for the coeﬃcients aij of
the matrix A. We realise that of additional inferential interest here are the components
of the mean vector μ across models. As the prior speciﬁcation for μ, given by (5.8), is an
equivalent choice for every model, its conditioning on Mi (although not stated explicitly)
can be dropped. Thus, we wish to update the prior
p(μj) = N (μj | bj, Δjj),
and subsequently infer the marginal posterior p(μj | D), where j = 1, . . . , d. As before,
Bayesian model averaging can be applied to estimate this true density, i.e. for a con-
verged chain of length n, we average all variational densities for μj (c.f. (5.35)) that are
associated with the models accepted across the scheme. So, akin to (4.12) and using the
corresponding notation, we aim to compute
pvar(μj | D) = 1
n
n

k=1
N

μj | ω(k)
j , Λ(k)
jj

.
(5.51)
5.4.1
Examples
In the following, the same speciﬁcations were maintained from Section 4.3.2, i.e. d = 10,
N = 250 and σ2 = 0.1. The prior on a was such that cij ∈{0, 0.5} and for that on σ2,
α = 1, β = 0.001. Moreover, a N (0, 10, 000Id) prior was allowed for μ as in Section 5.3.
By now simulating data from the non-zero mean VAR(1) model (5.1), only A, p and also
now μ were changed between examples. The MCMC algorithm was initialised from the
graph with only one self-loop on node y1, and run in C for 10, 000, 000 iterations, of which
the ﬁrst 100, 000 were discarded as burn-in and the remainder thinned by 1000.
185

5.4. Taking a random walk
Example 1
We allow direct comparison between this and the corresponding ﬁrst example in Sec-
tion 4.3.2 by specifying A = diag(0.8) and p = 0.5, but, furthermore, μ = (1, . . . , 1), a
10-vector. The output of the scheme is displayed graphically in Figure 5.1.
0
2000
4000
6000
8000
10000
−7360
−7350
−7340
−7330
Trace plot of L(q)
Iteration
L(q)
0
20
40
60
80
100
0.0
0.2
0.4
0.6
0.8
1.0
Lag
ACF
ACF plot for L(q)
Histogram of L(q)
L(q)
Frequency
−7360
−7350
−7340
−7330
0
100
300
500
700
0
1
2
3
4
5
6
7
8
9
10
0
1
2
3
4
5
6
7
8
9
10
Image plot for Pihat
Row
Col
Figure 5.1: Plots for the analysis of the MCMC output in Example 1 (non-zero mean)
The trace and ACF plots are extremely similar to those in the corresponding zero mean
example, indicating a well-mixing and independent chain. The only signiﬁcant diﬀerent
is that the mean value of the lower bound, about which the values of the chain ﬂuctu-
ate, is greater in this case due to the additional uncertainty about μ. Moreover, the
effectiveSize of the chain is equal to 9900, indicating that the chain is fully indepen-
dent. When applied to the variational scores, the Raftery-Lewis test yielded the following:
186

5.4. Taking a random walk
Iterations = 1:9900
Thinning interval = 1
Number of chains = 1
Sample size per chain = 9900
Quantile (q) = 0.025
Accuracy (r) = +/- 0.005
Probability (s) = 0.95
Burn-in
Total Lower bound
Dependence
(M)
(N)
(Nmin)
factor (I)
Lq
2
3794
3746
1.01
The Heidelberger-Welch diagnostic reached a similar conclusion:
Stationarity start
p-value
test
iteration
Lq
passed
1
0.0708
Halfwidth Mean
Halfwidth
test
Lq
passed
-7339 0.114
Each diagnostic has produced overwhelming evidence in favour of the chain having con-
verged. In addition, when employing these tests for components of ρ, τ, ω and Λ, stored
at each iteration, the results produced were concurrent with those above.
Moreover, it is evident from Figures 4.3 and 5.1 that the image plots of ˆΠ in both the
zero and non-zero mean cases are well-matched and, after normalising, will be close to
the truth Π.
In this case, the residual sum of squares is computed as S = 0.757, a
value only marginally bigger than that in the zero mean case. This implies that there is
suﬃcient data available here to learn the unknown mean, and hence the zero and non-zero
mean cases subsequently become most alike. So, the new variational algorithm, derived
in this chapter, is able to accurately predict the sparsity structure of the true A from the
simulated data.
Finally, Figures 5.2 and 5.3 display approximate posterior information for numerous co-
eﬃcients of both A and μ.
187

5.4. Taking a random walk
0.0
0.5
1.0
Value
0
2
4
6
8
10
Density
0
0.2
0.4
0.6
0.8
1
Probability
0.0
0.5
1.0
Value
0
2
4
6
8
10
Density
0
0.2
0.4
0.6
0.8
1
Probability
0.0
0.5
1.0
Value
0
2
4
6
8
10
Density
0
0.2
0.6
1
Probability
0.0
0.5
1.0
Value
0
2.2
4.4
6.6
8.8
11
Density
0
0.2
0.4
0.6
0.8
1
Probability
0.0
0.5
1.0
Value
0
2
4
6
8
10
Density
0
0.2
0.4
0.6
0.8
1
Probability
0.0
0.5
1.0
Value
0
2.2
4.4
6.6
8.8
11
Density
0
0.2
0.4
0.6
0.8
1
Probability
0.0
0.5
1.0
Value
0
2
4
6
8
10
Density
0
0.2
0.4
0.6
0.8
1
Probability
0.0
0.5
1.0
Value
0
2
4
6
8
10
Density
0
0.2
0.4
0.6
0.8
1
Probability
0.0
0.5
1.0
Value
0
2
4
6
8
10
Density
0
0.2
0.4
0.6
0.8
1
Probability
a11
a12
a13
a21
a22
a23
a31
a32
a33
Figure 5.2: Plots showing estimated, marginal posterior distributions for aij, i, j = 1, 2, 3,
in Example 1 (non-zero mean)
188

5.4. Taking a random walk
Value
Density
0
0.8
1.6
2.4
3.2
4
0.4
0.8
1.2
1.6
Value
Density
0
0.8
1.6
2.4
3.2
4
0.4
0.8
1.2
1.6
Value
Density
0
0.9
1.8
2.7
3.6
0.4
0.8
1.2
1.6
Value
Density
0
1
2
3
4
0.4
0.8
1.2
1.6
Value
Density
0
0.7
1.4
2.1
2.8
0.4
0.8
1.2
1.6
Value
Density
0
0.8
1.6
2.4
3.2
4
0.4
0.8
1.2
1.6
μ1
μ2
μ3
μ4
μ5
μ6
Figure 5.3: Plots showing estimated, marginal posterior distributions for μj, j = 1, . . . , 6,
in Example 1
As in the zero mean case, the approximate posterior probability that aij = 0 is large for
all oﬀ-diagonal elements. Moreover, the density plots, given that aij ̸= 0, are peaked at
around the true speciﬁcation for the diagonal entries. We realise that Figures 4.4 and
5.2 are almost identical. As mentioned above, this is a consequence of the mean being
estimated accurately, a fact borne out by Figure 5.3. Here, as expected, we see that the
approximate posterior mode of each μj is close to 1.
Example 2
We now choose A = tridiag(0.2, 0.4, 0.2), as in the corresponding zero mean case. How-
ever, on this occasion, the prior probability p is assigned such that p = 0.9. In addition,
189

5.4. Taking a random walk
the mean is speciﬁed as μ = (2, . . ., 2). Figure 5.4 reveals the results of the algorithm.
0
2000
4000
6000
8000
10000
−7405
−7395
−7385
−7375
Trace plot of L(q)
Iteration
L(q)
0
20
40
60
80
100
0.0
0.2
0.4
0.6
0.8
1.0
Lag
ACF
ACF plot for L(q)
Histogram of L(q)
L(q)
Frequency
−7405
−7400
−7395
−7390
−7385
−7380
−7375
0
200
400
600
800
0
1
2
3
4
5
6
7
8
9
10
0
1
2
3
4
5
6
7
8
9
10
Image plot for Pihat
Row
Col
Figure 5.4: Plots for the analysis of the MCMC output in Example 2 (non-zero mean)
From the plots, it is seen that the chain is moving freely and quickly in the graphi-
cal space, as well as the autocorrelation dropping to zero immediately. Moreover, the
effectiveSize of the chain, once again computed as 9900, intimates full independence
of the values.
Both convergence diagnostics produce favourable results, whereby the
Raftery-Lewis output is
Iterations = 1:9900
Thinning interval = 1
Number of chains = 1
Sample size per chain = 9900
Quantile (q) = 0.025
Accuracy (r) = +/- 0.005
Probability (s) = 0.95
190

5.4. Taking a random walk
Burn-in
Total Lower bound
Dependence
(M)
(N)
(Nmin)
factor (I)
Lq
2
3768
3746
1.01
and the Heidelberger-Welch test gives the results
Stationarity start
p-value
test
iteration
Lq
passed
1
0.657
Halfwidth Mean
Halfwidth
test
Lq
passed
-7383 0.0875
Again, upon application of these diagnostics to components of ρ, τ, ω and Λ, the same
outcome was provided. Henceforth, only the chain of lower bound is thus analysed.
Noticeable diﬀerences are apparent upon comparison of the image plots in the two cases
due to the change in speciﬁcation of p. In this case, p = 0.9 was chosen to be higher
than the ‘true’ value (computed as p = 0.72).
Consequently, a slight preference has
been given to the acceptance of models considered too sparse, as displayed in Figure 5.4.
That is, many true edges are identiﬁed from the data with less regularity than seen in
Figure 4.5, whilst the link from y5 to y4 is no-longer recognised. This trait is reﬂected
by the calculation of S = 2.619, a value relatively less accurate than in the zero mean
circumstance, where p = 0.5.
By studying Figure 5.5, it follows that the true speciﬁcations of aij are being well repre-
sented in these graphical summaries. Moreover, it is clear that there is much similarity
between these plots and those in the zero mean case, shown in Figure 4.6. This is despite
the choice of p = P(aij = 0) being increased here. Although this implies a bias for the
selection of more sparse models, the variational algorithm is still able to predict accu-
rately those values of aij that are not constrained to zero in all models accepted across
the scheme. In addition, it is obvious from Figure 5.6 that all plots of pvar(μj | D) are
peaked near to the true value. At each iteration, accurate estimates of all μj have been
191

5.4. Taking a random walk
0.0
0.5
1.0
Value
0
1.4
2.8
4.2
5.6
7
Density
0
0.2
0.4
0.6
0.8
1
Probability
0.0
0.5
1.0
Value
0
1.4
2.8
4.2
5.6
7
Density
0
0.2
0.4
0.6
0.8
1
Probability
0.0
0.5
1.0
Value
0
1.4
2.8
4.2
5.6
7
Density
0
0.2
0.4
0.6
0.8
1
Probability
0.0
0.5
1.0
Value
0
1.6
3.2
4.8
6.4
8
Density
0
0.2
0.4
0.6
0.8
1
Probability
0.0
0.5
1.0
Value
0
1.5
3
4.5
6
7.5
Density
0
0.2
0.4
0.6
0.8
1
Probability
0.0
0.5
1.0
Value
0
1.6
3.2
4.8
6.4
8
Density
0
0.2
0.4
0.6
0.8
1
Probability
0.0
0.5
1.0
Value
0
1.4
2.8
4.2
5.6
7
Density
0
0.2
0.4
0.6
0.8
1
Probability
0.0
0.5
1.0
Value
0
1.5
3
4.5
6
7.5
Density
0
0.2
0.4
0.6
0.8
1
Probability
0.0
0.5
1.0
Value
0
1.4
2.8
4.2
5.6
Density
0
0.2
0.4
0.6
0.8
Probability
a11
a12
a13
a21
a22
a23
a31
a32
a33
Figure 5.5: Plots showing estimated, marginal posterior distributions for aij, i, j = 1, 2, 3,
in Example 2 (non-zero mean)
192

5.4. Taking a random walk
Value
Density
0
2
4
6
8
10
1.4
1.8
2.2
2.6
Value
Density
0
1.8
3.6
5.4
7.2
9
1.4
1.8
2.2
2.6
Value
Density
0
2
4
6
8
1.4
1.8
2.2
2.6
Value
Density
0
2
4
6
8
10
1.4
1.8
2.2
2.6
Value
Density
0
2
4
6
8
10
1.4
1.8
2.2
2.6
Value
Density
0
2
4
6
8
1.4
1.8
2.2
2.6
μ1
μ2
μ3
μ4
μ5
μ6
Figure 5.6: Plots showing estimated, marginal posterior distributions for μj, j = 1, . . . , 6,
in Example 2
provided, and uncertainty about every component reduced. Hence, as in the previous
example, the density plots are alike in shape.
Example 3
The third example in Section 4.3.2 was also repeated with A = tridiag(0.4, 0, 0.4) and
p = 0.9 as before, but now μ = (3, . . ., 3). The output was analysed and is displayed
graphically below.
When calculating convergence diagnostics, the Raftery-Lewis test yielded
193

5.4. Taking a random walk
0
2000
4000
6000
8000
10000
−7365
−7360
−7355
Trace plot of L(q)
Iteration
L(q)
0
20
40
60
80
100
0.0
0.2
0.4
0.6
0.8
1.0
Lag
ACF
ACF plot for L(q)
Histogram of L(q)
L(q)
Frequency
−7365
−7360
−7355
0
1000
2000
3000
4000
5000
0
1
2
3
4
5
6
7
8
9
10
0
1
2
3
4
5
6
7
8
9
10
Image plot for Pihat
Row
Col
Figure 5.7: Plots for the analysis of the MCMC output in Example 3 (non-zero mean)
Iterations = 1:9900
Thinning interval = 1
Number of chains = 1
Sample size per chain = 9900
Quantile (q) = 0.025
Accuracy (r) = +/- 0.005
Probability (s) = 0.95
Burn-in
Total Lower bound
Dependence
(M)
(N)
(Nmin)
factor (I)
Lq
2
3856
3746
1.03
Moreover, application of Heidelberger and Welch resulted in
Stationarity start
p-value
test
iteration
Lq
passed
1
0.113
194

5.4. Taking a random walk
Halfwidth Mean
Halfwidth
test
Lq
passed
-7355 0.0386
Therefore, the plots and diagnostics are all concurrent with chain convergence and in-
dependence of values (effectiveSize = 9900). Notice that the histograms produced
in Figures 4.7 and 5.7 are almost identical due to the larger speciﬁcation of p in each
case. Similarly, the image plots also overlap signiﬁcantly, revealing an obvious tendency
to select models that are not dense. For completeness, we note that S = 0.015 here. To
conclude, we again realise that the approximate marginal posterior summaries for both
aij (Figure 5.8) and μj (Figure 5.9) are a strong reﬂection of the truth.
0.0
0.5
1.0
Value
0
1.4
4.2
7
Density
0
0.2
0.6
1
Probability
0.0
0.5
1.0
Value
0
1.4
4.2
7
Density
0
0.2
0.6
1
Probability
0.0
0.5
1.0
Value
0
1.4
4.2
7
Density
0
0.2
0.6
1
Probability
0.0
0.5
1.0
Value
0
1.7
3.4
5.1
6.8
8.5
Density
0
0.2
0.4
0.6
0.8
1
Probability
0.0
0.5
1.0
Value
0
1.7
3.4
5.1
6.8
8.5
Density
0
0.2
0.4
0.6
0.8
1
Probability
0.0
0.5
1.0
Value
0
1.7
3.4
5.1
6.8
8.5
Density
0
0.2
0.4
0.6
0.8
1
Probability
0.0
0.5
1.0
Value
0
1.7
3.4
5.1
6.8
8.5
Density
0
0.2
0.4
0.6
0.8
1
Probability
0.0
0.5
1.0
Value
0
1.7
3.4
5.1
6.8
8.5
Density
0
0.2
0.4
0.6
0.8
1
Probability
0.0
0.5
1.0
Value
0
1.7
3.4
5.1
6.8
8.5
Density
0
0.2
0.4
0.6
0.8
1
Probability
a11
a12
a13
a21
a22
a23
a31
a32
a33
Figure 5.8: Plots showing estimated, marginal posterior distributions for aij, i, j = 1, 2, 3,
in Example 3 (non-zero mean)
195

5.4. Taking a random walk
Value
Density
0
2.6
5.2
7.8
10.4
13
2.4
2.8
3.2
3.6
Value
Density
0
2
4
6
8
10
2.4
2.8
3.2
3.6
Value
Density
0
2
4
6
8
2.4
2.8
3.2
3.6
Value
Density
0
2
4
6
8
10
2.4
2.8
3.2
3.6
Value
Density
0
2.2
4.4
6.6
8.8
11
2.4
2.8
3.2
3.6
Value
Density
0
2
4
6
8
10
2.4
2.8
3.2
3.6
μ1
μ2
μ3
μ4
μ5
μ6
Figure 5.9: Plots showing estimated, marginal posterior distributions for μj, j = 1, . . . , 6,
in Example 3
5.4.2
Application to ERP data
We now reconsider the ERP data, introduced previously in Section 4.3.6. It is recalled
that an animal and distractor dataset, each of size N = 250, were obtained by monitoring
the cerebral activity produced at d = 32 electrodes for a particular volunteer in the study
(shown in Figures 4.14 and 4.15). On this occasion, each dataset was ﬁtted to a non-zero
mean VAR(1) model. The sparsity structure of A and the likely values of the coeﬃcients
aij, μj are of inferential interest.
Here, the sample mean of ERP values was not subtracted from the data since the true
mean is itself estimated during the algorithm. Thus, the Metropolis-Hastings scheme was
run twice in an identical fashion to that described in Section 5.4.1. The prior distributions
196

5.4. Taking a random walk
for a, σ2 and μ were also chosen as here, whereas we ﬁxed p = 31
32 in accordance with the
zero mean case. Figures 5.10 and 5.11 display the graphical summaries of the sampler for
the animal and distractor datasets respectively.
Trace plot of L(q)
Iteration
L(q)
0
2000
4000
6000
8000
10000
−32950
−32930
−32910
0
20
40
60
80
100
0.0
0.2
0.4
0.6
0.8
1.0
Lag
ACF
ACF plot for L(q)
Histogram of L(q)
L(q)
Frequency
−32960
−32950
−32940
−32930
−32920
−32910
0
200
400
600
800
1200
0
5
10
15
20
25
30
0
5
10
15
20
25
30
Image plot for Pihat
Row
Col
Figure 5.10: Plots for the analysis of the MCMC output for the animal ERP data (non-
zero mean)
Formal diagnostics can now be administered to test for convergence of the chains. For
the animal dataset, Raftery-Lewis oﬀered the results
Iterations = 1:9900
Thinning interval = 1
Number of chains = 1
Sample size per chain = 9900
Quantile (q) = 0.025
Accuracy (r) = +/- 0.005
Probability (s) = 0.95
197

5.4. Taking a random walk
Trace plot of L(q)
Iteration
L(q)
0
2000
4000
6000
8000
10000
−32510
−32490
−32470
0
20
40
60
80
100
0.2
0.4
0.6
0.8
1.0
Lag
ACF
ACF plot for L(q)
Histogram of L(q)
L(q)
Frequency
−32510
−32500
−32490
−32480
−32470
−32460
0
200
400
600
800
1000
0
5
10
15
20
25
30
0
5
10
15
20
25
30
Image plot for Pihat
Row
Col
Figure 5.11: Plots for the analysis of the MCMC output for the distractor ERP data
(non-zero mean)
Burn-in
Total Lower bound
Dependence
(M)
(N)
(Nmin)
factor (I)
Lq
7
8315
3746
2.22
whereas the Heidelberger-Welch diagnostic revealed
Stationarity start
p-value
test
iteration
Lq
passed
1
0.484
Halfwidth Mean
Halfwidth
test
Lq
passed
-32926 0.757
Moreover, in the distractor case, the output of the Raftery-Lewis test was
198

5.4. Taking a random walk
Iterations = 1:9900
Thinning interval = 1
Number of chains = 1
Sample size per chain = 9900
Quantile (q) = 0.025
Accuracy (r) = +/- 0.005
Probability (s) = 0.95
Burn-in
Total Lower bound
Dependence
(M)
(N)
(Nmin)
factor (I)
Lq
8
10428
3746
2.78
Additionally, Heidelberger-Welch returned
Stationarity start
p-value
test
iteration
Lq
passed
1
0.246
Halfwidth Mean
Halfwidth
test
Lq
passed
-32482 1.04
So, as was noted in the zero mean case due to the length of the run, the graphical output
and diagnostics suggest that these chains have converged, but without rapid exploration
of the space, and thus with fewer independent values.
Upon comparison of the two image plots for each dataset, all of the conclusions seen when
μ = 0 can again be reached. For further discussion, the reader is referred back to the
corresponding stage in Section 4.3.6. So, when the mean is non-zero, we can again surmise
that the decision needed to categorise both animal and distractor images is made along
similar neural pathways. Moreover, the analogous animal and distractor image plots are
closely related, independent of the value of μ. This is particularly true along the main
diagonals, but many edges between diﬀerent electrodes are also regularly recognised.
Figures 5.12 and 5.13 provide variational posterior summaries for a set of aij in the animal
and distractor cases respectively.
199

5.4. Taking a random walk
Value
0
8
16
24
32
0
0.4
0.8
1.2
Density
0
0.2
0.4
0.6
0.8
Probability
Value
0
6
12
18
24
−0.2
0.2
0.6
1
Density
0
0.2
0.4
0.6
0.8
Probability
Value
0
8
16
24
32
40
−0.4
0
0.4
0.8
Density
0
0.2
0.6
1
Probability
Value
0
4
8
12
16
20
−0.4
0
0.4
0.8
Density
0
0.2
0.6
1
Probability
Value
0
6
12
18
24
0
0.4
0.8
1.2
Density
0
0.2
0.4
0.6
0.8
Probability
Value
0
10
20
30
40
50
−0.4
0
0.4
0.8
Density
0
0.2
0.4
0.6
0.8
1
Probability
Value
0
4
8
12
16
20
−0.4
0
0.4
0.8
Density
0
0.2
0.4
0.6
0.8
1
Probability
Value
0
4
8
12
16
20
−0.6
−0.2
0.2
0.6
Density
0
0.2
0.6
1
Probability
Value
0
5
10
15
20
0
0.4
0.8
1.2
Density
0
0.2
0.4
0.6
0.8
Probability
a11
a12
a13
a21
a22
a23
a31
a32
a33
Figure 5.12: Plots showing estimated, marginal posterior distributions for aij, i, j =
1, 2, 3, for the animal ERP data (non-zero mean)
200

5.4. Taking a random walk
Value
0
4
8
12
16
20
0
0.4
0.8
1.2
Density
0
0.2
0.4
0.6
0.8
1
Probability
Value
0
4
8
12
16
−0.2
0.2
0.6
1
Density
0
0.2
0.4
0.6
0.8
Probability
Value
0
8
16
24
32
40
−0.4
0
0.4
0.8
Density
0
0.2
0.4
0.6
0.8
1
Probability
Value
0
9
18
27
36
45
−0.4
0
0.4
0.8
Density
0
0.2
0.4
0.6
0.8
1
Probability
Value
0
6
12
18
24
30
0
0.4
0.8
1.2
Density
0
0.2
0.4
0.6
0.8
1
Probability
Value
0
9
18
27
36
45
−0.4
0
0.4
0.8
Density
0
0.2
0.4
0.6
0.8
1
Probability
Value
0
3
6
9
12
15
−0.4
0
0.4
0.8
Density
0
0.2
0.6
1
Probability
Value
0
4.5
9
13.5
22.5
−0.6
−0.2
0.2
0.6
Density
0
0.2
0.4
0.6
0.8
1
Probability
Value
0
5
10
15
20
25
0
0.4
0.8
1.2
Density
0
0.2
0.6
1
Probability
a11
a12
a13
a21
a22
a23
a31
a32
a33
Figure 5.13: Plots showing estimated, marginal posterior distributions for aij, i, j =
1, 2, 3, for the distractor ERP data (non-zero mean)
201

5.4. Taking a random walk
It follows that a13, a21, a23 and a31 appear all to be zero in these ﬁgures as before. In
fact, the respective densities for these coeﬃcients are similar both to each other and the
corresponding plots in the zero mean case, with the possible exception of pvar(a21 | a21 ̸=
0, D). It is additionally evident that a32 = 0; here, we note that, unlike in Figures 4.18 and
4.19, the densities for this coeﬃcient are tightly peaked around zero. All of the remaining
elements considered are suggested to be non-zero, with the likely values of each akin for
the two datasets. Moreover, it is realised that some of the densities are multimodal, as
discussed erstwhile.
For completeness, plots of pvar(μj | D) are provided for the two datasets in Figures 5.14
and 5.15. Upon comparison to each other, the densities are peaked around quite distinct
values for all coeﬃcients, apart from that for μ5 and especially μ6. Thus, we can suggest
that the mean level of electrical activity varies regularly at corresponding electrodes for
the two datasets. In such cases, the response is greater upon recognition of an animal
despite the use of comparable circuits in each case to process the information.
A valid question to ask at this stage is whether there is much gain in applying the more
complex non-zero mean approach as opposed to comparing zero mean VAR(1) models with
centralised input data (i.e. by subtracting the sample mean). For instance, in the current
scenario, results are similar between the two approaches. We have learnt additionally
about the likely values of components of μ here, but this required a large quantity of
theoretical and computational work. Yet, in poor datasets, diﬀerences may exist if we
assume either zero or non-zero mean models. In fact, evidence of this is provided in the
ﬁnal example below.
5.4.3
Application to microarray data
To conclude, our Metropolis-Hastings algorithm is run again for the microarray data,
introduced in the previous chapter, and now modelled via a non-zero mean VAR(1) process
202

5.4. Taking a random walk
Value
Density
0
0.35
0.7
1.05
1.4
1.75
1.1
1.8
2.5
3.2
3.9
Value
Density
0
0.55
1.1
1.65
2.2
2.75
1.9
2.6
3.3
4
4.7
Value
Density
0
0.25
0.5
0.75
1
1.25
0.6
1.3
2
2.7
3.4
Value
Density
0
0.2
0.4
0.6
0.8
1
3.8
4.5
5.2
5.9
6.6
Value
Density
0
0.19
0.38
0.57
0.76
0.95
1.2
1.9
2.6
3.3
4
Value
Density
0
0.2
0.4
0.6
0.8
1
4.3
5
5.7
6.4
7.1
μ1
μ2
μ3
μ4
μ5
μ6
Figure 5.14: Plots showing estimated, marginal posterior distributions for μj, j = 1, . . . , 6,
for the animal ERP data
203

5.4. Taking a random walk
Value
Density
0
0.45
0.9
1.35
1.8
2.25
−0.2
0.5
1.2
1.9
2.6
Value
Density
0
0.35
0.7
1.05
1.4
1.75
0.4
1.1
1.8
2.5
3.2
Value
Density
0
0.25
0.5
0.75
1
1.25
−0.1
0.6
1.3
2
2.7
Value
Density
0
0.55
1.1
1.65
2.2
2.75
2.6
3.3
4
4.7
5.4
Value
Density
0
0.22
0.44
0.66
0.88
1.1
1.4
2.1
2.8
3.5
4.2
Value
Density
0
0.25
0.5
0.75
1
1.25
4.3
5
5.7
6.4
7.1
μ1
μ2
μ3
μ4
μ5
μ6
Figure 5.15: Plots showing estimated, marginal posterior distributions for μj, j = 1, . . . , 6,
for the distractor ERP data
204

5.4. Taking a random walk
with unknown A, σ2 and μ. Recall that, for this dataset, we have d = 9 and N = 40.
Moreover, we let p = 8
9 and retain the remaining speciﬁcations from before. The genes
considered in the study are displayed in Table 4.4.
0
2000
4000
6000
8000
10000
−920
−910
−900
Trace plot of L(q)
Iteration
L(q)
0
20
40
60
80
100
0.0
0.2
0.4
0.6
0.8
1.0
Lag
ACF
ACF plot for L(q)
Histogram of L(q)
L(q)
Frequency
−920
−915
−910
−905
−900
−895
0
500
1000
1500
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
Image plot for Pihat
Row
Col
Figure 5.16: Plots for the analysis of the MCMC output for the microarray data (non-zero
mean)
The output of the scheme, displayed in Figure 5.16, is now analysed. It is clear from the
trace and ACF plots that the chain is moving rapidly through the space and contains
many independent values. Furthermore, the Raftery-Lewis test yielded
Iterations = 1:9900
Thinning interval = 1
Number of chains = 1
Sample size per chain = 9900
Quantile (q) = 0.025
Accuracy (r) = +/- 0.005
Probability (s) = 0.95
205

5.4. Taking a random walk
Burn-in
Total Lower bound
Dependence
(M)
(N)
(Nmin)
factor (I)
Lq
2
3812
3746
1.02
whilst Heidelberger-Welch produced
Stationarity start
p-value
test
iteration
Lq
passed
1
0.528
Halfwidth Mean
Halfwidth
test
Lq
passed
-905
0.0525
Thus, it can be suggested conﬁdently that the stationary distribution of the chain has
been reached. When examining the image plot, we see that there are similarities with
that in the zero mean case (c.f. Figure 4.20). For instance, kinA causes a reaction in
a distinct gene, namely spo0F, as opposed to inﬂuencing itself at the next time point.
However, on this occasion, a new link is determined from spoOF to clpP whereas the
aﬀect of spoIIAA over spo0B is scarcely recognised. In fact, there may exist other such
edges between diﬀerent genes, although these associations seem rather weak.
Finally, the approximate posterior information for the coeﬃcients aij and μj is revealed in
Figures 5.17 and 5.18 respectively. As in the zero mean case, the only non-zero coeﬃcients
of A shown are a11 and a22, although the possible value of a22 appears to be marginally
smaller than before. On the other hand, most of the densities pvar(μj | D) have negative
modal values. Again, for a larger dataset, we would expect these densities to be more
tightly peaked, and hence the same links would be suggested in the image plots for when
the mean was both zero or otherwise.
206

5.4. Taking a random walk
Value
0
0.6
1.2
1.8
2.4
3
0
0.4
0.8
1.2
Density
0
0.2
0.4
0.6
0.8
1
Probability
Value
0
0.55
1.65
2.75
−0.4
0
0.4
0.8
Density
0
0.2
0.6
1
Probability
Value
0
0.35
1.05
1.75
−0.4
0
0.4
0.8
1.2
Density
0
0.2
0.4
0.6
0.8
1
Probability
Value
0
0.8
1.6
2.4
3.2
4
−0.7
−0.3
0.1
0.5
Density
0
0.2
0.4
0.6
0.8
1
Probability
Value
0
0.7
1.4
2.1
2.8
3.5
0
0.4
0.8
1.2
Density
0
0.2
0.4
0.6
0.8
1
Probability
Value
0
0.45
1.35
2.25
−0.8
−0.4
0
0.4
Density
0
0.2
0.4
0.6
0.8
1
Probability
Value
0
0.95
2.85
4.75
−0.6
−0.2
0.2
0.6
Density
0
0.2
0.4
0.6
0.8
1
Probability
Value
0
0.6
1.8
3
−0.4
0
0.4
0.8
Density
0
0.2
0.6
1
Probability
Value
0
0.85
2.55
4.25
−0.4
0
0.4
0.8
Density
0
0.2
0.4
0.6
0.8
1
Probability
a11
a12
a13
a21
a22
a23
a31
a32
a33
Figure 5.17: Plots showing estimated, marginal posterior distributions for aij, i, j =
1, 2, 3, for the microarray data (non-zero mean)
207

5.4. Taking a random walk
Value
Density
0
0.3
0.6
0.9
1.2
1.5
−1.4
−0.9
−0.4
0.1
0.6
Value
Density
0
0.4
0.8
1.2
1.6
−2.6
−2.1
−1.6
−1.1
−0.6
Value
Density
0
0.4
0.8
1.2
1.6
2
−3.2
−2.7
−2.2
−1.7
−1.2
Value
Density
0
0.8
1.6
2.4
3.2
4
−1
−0.5
0
0.5
1
Value
Density
0
0.24
0.48
0.72
0.96
1.2
−0.7
−0.2
0.3
0.8
1.3
Value
Density
0
0.3
0.6
0.9
1.2
1.5
−2.1
−1.6
−1.1
−0.6
−0.1
μ1
μ2
μ3
μ4
μ5
μ6
Figure 5.18: Plots showing estimated, marginal posterior distributions for μj, j = 1, . . . , 6,
for the microarray data
208

Chapter 6
Conclusions and further work
6.1
Conclusions
In this thesis, the predominant focus has been to illustrate how variational Bayesian
methods can be applied so that sparse VAR(1) graphical models may be scored. As noted,
this approximation has been utilised previously by Penny and Roberts (2002) in zero mean
VAR(p) models for the purpose of model-order selection, itself a signiﬁcant, inferential
problem. Here however, our wish was to estimate the unknown sparsity structure of the
autoregressive matrix A in both zero and non-zero mean processes, as seen respectively
in Chapters 3 and 5.
To rank models, we realise that an inherent feature of variational Bayes methodology is
that a lower bound is formed on the logarithm of the marginal likelihood, an essential
statistic for Bayesian model comparison. At the same time, an attractive beneﬁt of the
approach is that a global approximation can also be made to each parameter posterior
by minimising the KL divergence between the true and variational distribution.
The
optimality of every estimate is ensured by iterating update equations that are derived
for the corresponding set of variational parameters until convergence. It was shown via
209

6.1. Conclusions
example in Chapter 2 that such an approximation competes favourably with that of the
EM algorithm and Gibbs’ sampling.
An additional advantage here is that variational distributions may be determined by either
a free form or ﬁxed form approach. Of course, this proved to be of importance in both
Chapters 3 and 5. If A was dense, application of the free form method was possible to
ﬁnd q(a | D). However, in the sparse case, the most natural and straightforward way to
proceed was to assume ﬁxed forms for the variational posteriors, namely those suggested
by the free form approach. A toy example based upon simulated data was considered in
both the zero and non-zero mean cases, and positive results were produced. In particular,
the model possessing the true sparsity structure was ranked highest in each case and,
when unknown, the mean of the process was accurately estimated.
In Chapter 4, an MCMC algorithm was constructed to traverse quickly graphical spaces
of higher dimensions. Any move to a neighbouring graph was proposed by the addition
or deletion of a single edge from the current graph, and accepted in accordance with
a Metropolis-Hastings acceptance probability. Throughout the scheme, a Markov chain
of accepted lower bounds was formed, enabling exploration of an approximation to the
model posterior distribution. For analysis purposes, image plots could be produced to
display which edges were accepted most frequently during the run. Moreover, the probable
values of the coeﬃcients aij could be determined by estimating both P(aij = 0 | D) and
the marginal density p(aij | aij ̸= 0, D). Similarly, in Chapter 5, an approximate posterior
summary could also be provided for each coeﬃcient of the mean vector.
The algorithm was tested on several datasets of varying dimension, simulated from both
zero and non-zero mean VAR(1) models. In this case, the results produced throughout
accurately predicted the true speciﬁcations. Moreover, two sets of real time series data
were also considered. Initially, for the 32-electrode ERP datasets, we concluded that the
networks required to process the information of target and non-target photographs were
similar, independently of whether the mean was equal to zero or otherwise. Then, for the
210

6.2. Further work
microarray data, it was possible to discover which genes were inﬂuential in determining
whether an organism should sporulate. So, in summary, upon modelling a real dataset by
a zero or non-zero mean VAR(1) process, we can use our algorithm to locate high scoring
models with computational eﬃciency in graphical spaces of potentially huge dimension.
6.2
Further work
We consider brieﬂy how the methodology that is comprised within this thesis could be
extended. Recall that, in Chapter 3, the VAR(1) model was speciﬁed such that the noise
vector et was distributed with covariance matrix Γ = σ2Id. Thus, a simple direction to
take would be to implement the variational Bayesian approach for ranking sparse VAR(1)
models when Γ was no-longer constrained. In this case, the most natural way to proceed
would be to place an inverse Wishart prior on Γ. Alternatively, we could examine the
scenario when the noise is modelled as a mixture of Gaussian distributions, as opposed
to the standard single Gaussian. This has been tackled previously to identify the optimal
model order by Roberts and Penny (2002).
However, an additional area of research that is perhaps most clearly motivated here is
to compare sparse VAR(p) models. Now, our task would be to determine the sparsity
structure of all p autoregressive matrices in the process, where each A(i) is of dimension
d × d. By deﬁning xt = [yt−1, yt−2, . . . , yt−p] where t = 1 . . . , N, we could follow Penny
and Roberts (2002) and rewrite (3.1) as
yt = xtW + et.
Here, W is a pd × d matrix, formed by stacking the A(i)-matrices. Thus, by specifying
a prior on vec(W) that imposes the correct sparsity structure for each model, the vari-
ational algorithm could proceed as before. In particular, it would be interesting to see
211

6.2. Further work
how eﬀectively our Metropolis-Hastings algorithm could handle moving through graphical
spaces of such extreme dimension.
Of course, we are not restricted to model time series data using just VAR processes. Hence
ﬁnally, it is noted that the variational Bayes treatment could be given to such alternatives.
For instance, one possibility is the VARMA(p, q) (vector autoregressive moving average)
process, deﬁned as
yt =
p

i=1
yt−iA(i) + et +
q

j=1
et−jφ(j),
where again et ∼N (0, Γ). Thus, for i = 1, . . ., p and j = 1, . . . , q, our parameter set
would be {A(i), φ(j), Γ}, where each φ(j) has dimension d × d. For further information
on this and other related models, the reader is referred to L¨utkepohl (2005).
212

Appendix A
Probability distributions
In this appendix, some standard, continuous probability distributions are documented.
In each case, the probability density function is deﬁned, together with any salient expec-
tations, taken with respect to this density.
A.1
Gaussian distribution
The Gaussian (normal) distribution with mean m and variance v > 0 is denoted as
p(x | m, v) = N (x | m, v)
=
1
√
2πv exp

−(x −m)2
2v

.
(A.1)
An important result is that
E

X2
= m2 + v.
(A.2)
213

A.2. Inverse gamma distribution
A.2
Inverse gamma distribution
With support wherever x > 0, the density of the inverse gamma distribution is
p(x | a, b) = IG(x | a, b)
=
ba
Γ(a)x−(a+1) exp

−bx−1
,
(A.3)
with parameters a, b > 0. We realise three pertinent identities for this distribution.
E {X} =
b
a −1
for a > 1
(A.4)
E

X−1
= a
b
(A.5)
E {log X} = log b −ψ(a),
(A.6)
by both Beal (2003) and Nicolas (2002).
Here, for z ∈R, we deﬁne ψ(z) to be the
digamma function (Johnson et al., 1992), i.e. the logarithmic derivative of the gamma
function, given by
ψ(z) = d
dz log Γ(z) = Γ
′(z)
Γ(z) .
A.3
Multivariate Gaussian distribution
The univariate Gaussian can be generalised to d dimensions with density
p(x | m, V ) = N (x | m, V )
= (2π)−d/2 |V |−1/2 exp

−1
2(x −m)TV −1(x −m)

,
(A.7)
where m = (m1, . . ., md) is the mean vector and V is the symmetric, positive-deﬁnite,
214

A.4. Inverse Wishart distribution
d × d covariance matrix. Akin to the univariate case, we have
E

XXT
= mmT + V.
(A.8)
A.4
Inverse Wishart distribution
A multivariate generalisation of the inverse gamma distribution is the inverse Wishart,
with density function for d × d matrix X given by O’Hagan et al. (1994) as
p(X | B, r) = IW(X | B, r)
= k−1|B|r/2|X|−(r+d+1)/2 exp

−Tr
$
X−1B
%
/2

,
(A.9)
where the normalising constant is
k = 2rd/2πd(d−1)/4
d

i=1
Γ {(r + 1 −i)/2} .
(A.10)
The parameters of this distribution are B, a symmetric, positive deﬁnite, d × d matrix
and a scalar r > d. The afore-mentioned authors also indicate that
E {X} =
B
r −d −1
for r > d + 1
E

X−1
= rB−1.
(A.11)
215

Appendix B
Graphical Models
The main focus of this appendix is to introduce the concept of a graphical model. This
will lead us to examine brieﬂy both graphical Gaussian models and Bayesian networks.
Initially, we deﬁne conditional independence, the key notion that characterises a graphical
model.
B.1
Conditional Independence
Suppose we have two random variables, X1 and X2, possessing a joint probability density
function pX1, X2. Then, these random variables are independent, written X1 ⊥⊥X2, if
pX1, X2(x1, x2) = pX1(x1)pX2(x2). An equivalent formulation is pX1 | X2(x1 | x2) = pX1(x1),
i.e.
the conditional density of X1, given X2 = x2, is not a function of x2, and that
pX2 | X1(x2 | x1) = pX2(x2).
Now, introduce a third variable, X3. We say X1 and X2 are conditionally independent
given X3, written X1 ⊥⊥X2 | X3, if X1 and X2 are independent in their conditional
distribution given X3 = x3, for any value of x3.
In other words, given knowledge of
X3, subsequent understanding of X2 will not provide any new information about X1.
216

B.2. Graph theory
Conditional independence can be characterised in terms of density functions as follows:
X1 ⊥⊥X2 | X3 ⇐⇒pX1, X2 | X3(x1, x2 | x3) = pX1 | X3(x1 | x3)pX2 | X3(x2 | x3)
(B.1)
⇐⇒pX1 | X2, X3(x1 | x2, x3) = pX1 | X3(x1 | x3)
(B.2)
⇐⇒pX2 | X1, X3(x2 | x1, x3) = pX2 | X3(x2 | x3).
(B.3)
B.2
Graph theory
Some standard notation and terminology for graphs is recalled. For further discussion,
the reader is referred to Cowell et al. (1999).
By deﬁnition, a graph is a pair G = (V, E), whereby V is a ﬁnite set of nodes (or vertices)
and E a set of edges of ordered pairs of nodes. If, for two nodes a and b, (a, b) ∈E and
(b, a) ∈E, the edge between them is described as undirected, written a ∼b (represented
on a graph by a line between the two nodes). Thus, a and b are described as neighbours.
On the contrary, if (a, b) ∈E, but (b, a) /∈E, the edge is called directed, written a →b
(represented on a graph by an arrow from a to b). In this case, a is termed as a parent
of b, and b one of the children of a. We denote pa(b) to be the set of parents of the node
b, similarly ch(a) the set of children of a. The boundary, bd(a), of a ∈V is the set of
parents and neighbours of this node. Moreover, the closure, cl(a), is the set a ∪bd(a).
If a graph possesses only directed edges, it is referred to as a directed graph, similarly an
undirected graph. If an edge exists between every pair of nodes, the graph is complete.
A sequence of distinct nodes a = a0, . . . , an = b, such that aj−1 ∼aj for all j = 1, . . ., n,
forms a path from a to b of length n. If the path is such that a = b, i.e. the end-points
coincide, it is referred to as an n-cycle. A path from a to b, given by the same set of nodes
as above, is described as directed if it contains at least one directed edge aj−1 →aj for
any j. In this case, a is an ancestor of b and b one of the descendants of a. Denote an(b)
217

B.3. Undirected graphical models
to be the set of ancestors of b, similarly de(a) the set of descendants of a. The deﬁnition
of a directed n-cycle follows immediately. A graph without any cycles is called acyclic.
Finally, suppose that A, B and C are subsets of V. If all paths from A to B intersect C,
then C is deemed to separate A and B. The theory presented here is important for what
ensues in this appendix.
B.3
Undirected graphical models
Let G = (V, E) be an undirected graph and X = (X1, . . . , Xp)T a p-dimensional random
vector. If the graph has p nodes, then a random variable Xa is associated to each node for
all a ∈V where, of course, V = {1, . . ., p}. In general, note that, on any graph, a circle
is used to represent a continuous random variable, a dot for a discrete variable. Here,
we are concerned with the former case. Now, suppose a subset A ⊆V . We thus denote
XA = (Xa : a ∈A) to be a collection of random variables.
Furthermore, introduce P, a probability distribution for X. If A ⊆V , then let PA denote
the marginal distribution for XA. Thus, an important deﬁnition is realised.
Deﬁnition 1 Assume that A, B, C are disjoint subsets of V . If XA ⊥⊥XB | XC whenever
C separates A and B in the graph G, the distribution P is said to be Markov with respect
to G.
This is known as the global Markov property. We stress that, on the graph, if two nodes
are conditionally independent, no edge exists between them. It is worth mentioning that
other such Markov properties exist over graphs.
Deﬁnition 2 If Xa ⊥⊥XV \cl(a) | Xbd(a) for any a ∈V , a distribution P obeys the local
Markov property with respect to a graph G.
218

B.3. Undirected graphical models
Moreover, if Xa ⊥⊥Xb | XV \{a, b} for any pair (a, b) /∈E, then, relative to a graph, the
pairwise Markov property is satisﬁed. These properties are important since they show
that any conditional independencies that can be determined from the graph also hold in
the corresponding probability distribution. Further analysis of these Markov properties
is provided by Lauritzen (1996). Finally, an undirected graphical model (also termed a
Markov network) for X is a joint probability distribution for X, that is Markov (obeys
the global Markov property) with respect to an undirected graph G.
If the distribution is multivariate Gaussian, say N (x | μ, Σ), then a graphical Gaussian
model is so deﬁned. We now examine the conditional independencies between random
variables, inherent in such a model. Thus, let K = Σ−1 be the concentration (preci-
sion) matrix for such a multivariate Gaussian. Speed and Kiiveri (1986) illustrate that
K determines the conditional independence structure of a graphical Gaussian model as
follows.
Proposition 1 Let a, b ∈{1, . . ., p} be distinct nodes on an undirected graph G, giv-
ing rise to a graphical Gaussian model, parameterised by mean vector μ and covari-
ance matrix Σ.
Deﬁning the corresponding concentration matrix as K = (kab), then
Xa ⊥⊥Xb | XV \{a, b} (pairwise Markov property) if and only if kab = 0.
So, for a given K, a graph can be associated and its independencies identiﬁed. Moreover,
a given graph determines a sparse matrix K. Of course, as the graph is undirected and
with K symmetric, if kab = 0, then kba = 0, implying Xb ⊥⊥Xa | XV \{a, b}. Thus, in this
case, no edge would exist between nodes a and b. To clarify, consider this simple example.
219

B.3. Undirected graphical models
B.3.1
Example
Suppose that the random vector X = (X1, X2, X3, X4)T is modelled via a multivariate
Gaussian distribution, with concentration matrix K speciﬁed as
K =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
∗
∗
∗
∗
∗
∗
0
0
∗
0
∗
0
∗
0
0
∗
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
,
where ∗refers to an unspeciﬁed, non-zero element. Then, a graphical Gaussian model is
deﬁned, with respect to the graph below.
X1
X2
X3
X4
Figure B.1: 4-node graph, corresponding to the choice of K
Using Proposition 1, the subsequent conditional independencies are apparent: X2 ⊥⊥
X3 | {X1, X4}, X2 ⊥⊥X4 | {X1, X3} and X3 ⊥⊥X4 | {X1, X2}.
Similarly, the reverse
independencies, such as X3 ⊥⊥X2 | {X1, X4}, also hold.
220

B.4. Bayesian networks
B.4
Bayesian networks
Let G = (V, E) now be a directed graph and suppose that we have again the random
vector X = (X1, . . . , Xp)T. Here, a directed edge implies a causal dependence between
a pair of nodes. So if Xa →Xb for a, b ∈{1, . . . , p}, then we say that Xa causes (or
inﬂuences) Xb. Moreover, whenever G contains no directed cycles, then it is referred to
as a directed acyclic graph (DAG). On any DAG, a (non-unique) ordering of the nodes
can be found such that Xa →Xb only when a < b, i.e. every node follows its parents in
the ordering.
Introducing a distribution P for X, we now present an analogue to Deﬁnition 2 for the
directed case.
Deﬁnition 3 Let nd(Xa) represent the set of non-descendants of Xa for a ∈V . Then, if
Xa ⊥⊥nd(Xa) | pa(Xa), the distribution P obeys the directed local Markov property with
respect to a directed acyclic graph G.
Although not treated here, there are also directed counterparts to the (undirected) global
and pairwise Markov properties. In contrast to the undirected case, the directed local and
global Markov properties are equivalent over a DAG (Lauritzen 1996, pg. 33, 51). Thus,
if either of these two properties is satisﬁed, P is termed a directed Markov distribution
(c.f. Deﬁnition 1). Finally, a Bayesian network (also termed a belief network) for X is
a joint probability distribution for X, that is directed Markov (obeys the directed global
Markov property) with respect to G, a DAG.
Essentially, a Bayesian network is merely a directed, acyclic graphical model, containing
an ordering of the nodes. We note that this ordering is consistent with the DAG, but
is otherwise arbitrary. Of course, any conditional independencies between variables can
be simply read oﬀthe graph. Moreover, the probability distribution for X can be fac-
torised according to the DAG (Cowell, 1998). The condition Xa ⊥⊥nd(Xa) | pa(Xa),
221

B.4. Bayesian networks
determining the directed local Markov property, can be re-expressed, in general, as
Xa ⊥⊥X1, . . . , Xa−1 | pa(Xa). This is because Xb /∈de(Xa) if b < a. So, in terms of
densities and dropping subscripts on p, we have
p(xa | x1, . . . , xa−1) = p(xa | pa(xa)),
by (B.2). Hence, the full distribution can be factorised with density
p(x1, . . . , xp) = p(x1)p(x2 | x1)p(x3 | x1, x2) × · · · × p(xp | x1, . . ., xp−1)
=
p

a=1
p(xa | x1, . . . , xa−1)
=
p

a=1
p(xa | pa(xa)).
(B.4)
In other words, the joint density, represented by the graph, consists of a product of
marginal densities for each node, conditioned on the parents of that node. It is evident
that this ﬁnal factorisation is independent of the (arbitrary) choice of ordering.
B.4.1
Example
As a straightforward illustration, Figure B.2 shows a simple, directed acyclic graph, which
deﬁnes a Bayesian network for X = (X1, . . . , X6)T, possessing a joint density p(x1, . . . , x6).
X1
X3
X5
X2
X4
X6
Figure B.2: 6-node DAG
222

B.4. Bayesian networks
Then, by (B.4), it is evident from the graph that
p(x1, . . . , x6) = p(x1)p(x2 | x1)p(x3 | x2, x1)p(x4)p(x5 | x3)p(x6 | x3, x4).
Hence, for instance, it follows that X6 ⊥⊥X1 | {X3, X4}, X5 ⊥⊥X1 | X3, etc.
223

Appendix C
Generalised inverses
Every non-singular, square matrix P possesses a unique inverse, denoted by P −1, whereby
PP −1 = P −1P = I.
(C.1)
The inverse matrix itself has many properties, for instance, (P −1)−1 = P, (P T)−1 =
(P −1)T and (aP)−1 = a−1P −1 for all non-zero a ∈R etc. However, as we have seen in
both Chapters 3 and 5, it can be the case that we want to ﬁnd an inverse matrix when P
is singular or even not square. To eﬀect this, we search for a generalised inverse (termed
by some authors as a pseudoinverse), with similar properties to the standard inverse of a
square, non-singular matrix.
Initially, we have the following deﬁnition.
Deﬁnition 4 A generalised inverse of a m×p matrix P is any p×m matrix G such that
PGP = P.
(C.2)
224

Appendix C. Generalised inverses
Let P −denote an arbitrary generalised inverse of such a matrix P. Hence, PP −P =
P.
In the speciﬁc case of P being square and non-singular, then this matrix has a
unique generalised inverse, the standard inverse P −1. Clearly, when G = P −1, the above
generalised inverse condition is satisﬁed. Moreover, if G is a generalised inverse of P,
then, by deﬁnition, G = P −1PGPP −1 = P −1PP −1 = P −1 (Harville, 1997).
Some of the properties of the standard inverse can correspond to an arbitrary generalised
inverse, proven by direct substitution into (C.2). For instance, if P is m × p, then one
choice of (P T)−is (P −)T. In this case, P T(P −)TP T = (PP −P)T = P T, hence (C.2) is
satisﬁed. In addition, a−1P −is a generalised inverse of aP where a ∈R and is non-zero.
However, it is not necessarily true that one choice of (P −)−is P.
The generalised inverse as deﬁned above exists for any matrix, but is not unique. In fact,
for a m×p matrix P of rank r, there are an inﬁnite number of generalised inverses (Harville,
1997). So, an alternative, unique generalised inverse has been considered, initially by
Moore (1920) and then independently by Penrose (1955), which now must hold for several
constraints.
Deﬁnition 5 The Moore-Penrose inverse of any m × p matrix P is the unique p × m
matrix G that holds for the following conditions:
PGP
=
P
(C.3)
GPG
=
G
(C.4)
(PG)T
=
PG
(C.5)
(GP)T
=
GP.
(C.6)
Let P + denote the Moore-Penrose inverse (often referred to as the generalised inverse) of
such a matrix P. We realise that other generalised inverses exist that meet (C.3) and a
combination of the properties (C.4)–(C.6). See Ben-Israel and Greville (1974) or Harville
225

Appendix C. Generalised inverses
(1997) for more details.
When P is square and non-singular, then, similar to the case as that of any generalised
inverse, P + = P −1. We realise this since P has a unique generalised inverse, as mentioned
earlier, and that G = P −1 holds for conditions (C.3)–(C.6).
The Moore-Penrose inverse possesses some properties that are in common with both an
arbitrary generalised inverse, P −, and the standard inverse, P −1. For instance, analogous
to previous, (P T)+ = (P +)T and (aP)+ = a−1P + for all non-zero a ∈R. However, unlike
P −, we now have (P +)+ = P. Such results are proven by direct substitution into (C.3)–
(C.6) (Harville, 1997). Other such properties of the Moore-Penrose inverse do not hold
for any generalised inverse. For a complete list, see Rao (1966).
A simple way to compute P + is to use matrix decomposition. Here, we examine one of
the more popular methods, used in this context by Rao (1962).
Deﬁnition 6 The singular value decomposition of any m×p matrix P of rank r is deﬁned
to be
P = U
⎡
⎣S
0
0
0
⎤
⎦V T,
(C.7)
where U and V are m×m and p×p orthogonal matrices respectively (i.e. UTU = UUT =
Im, similarly for V ) and S = diag(σ1, σ2, . . . , σr), an r × r matrix with strictly positive
diagonal elements.
In this deﬁnition, the σi, i = 1, . . . , r are the singular values of P and are unique. Note
that P is of rank r since it has r non-zero singular values.
Harville (1997) shows that the Moore-Penrose inverse of P with this singular value de-
composition is given by
P + = V
⎡
⎣S
0
0
0
⎤
⎦
+
UT,
(C.8)
226

Appendix C. Generalised inverses
where
⎡
⎣S
0
0
0
⎤
⎦
+
=
⎡
⎣S+
0
0
0
⎤
⎦.
At this stage, it can be realised that for any D = diag(d1, d2, . . . , dr), an r × r diagonal
matrix, then D+ = diag(d+
1 , d+
2 , . . . , d+
r ), whereby, for all i,
d+
i =
⎧
⎨
⎩
d−1
i
if di ̸= 0
0
if di = 0
.
(C.9)
Thus, S+ = diag(σ−1
1 , σ−1
2 , . . . , σ−1
r ).
The proof is quite straightforward. We note that, as S is diagonal and the product of two
diagonal matrices is merely the product of each pair of diagonal entries, S+, as deﬁned
above, holds for the conditions (C.3)–(C.6), and hence is the Moore-Penrose inverse of
S. Moreover, by substituting (C.7) and (C.8) directly into conditions (C.3)–(C.6), then
it is easy to see that (C.8) is the Moore-Penrose inverse of (C.7), as U and V are both
orthogonal.
227

Bibliography
Aitkin, M. (1991). Posterior Bayes factors (with discussion). Journal of the Royal Statis-
tical Society, Series B 53, 111–142.
Akaike, H. (1974). A new look at statistical model identiﬁcation. IEEE Transactions on
Automatic Control 19, 716–723.
Beal, M. (2003). Variational algorithms for approximate Bayesian inference. Ph. D. thesis,
Gatsby Computational Neuroscience Unit, University College London.
Beal, M., F. Falciani, Z. Ghahramani, C. Rangel, and D. Wild (2005).
A Bayesian
approach to reconstructing genetic regulatory networks with hidden factors. Bioin-
formatics 21(3), 349–356.
Ben-Israel, A. and T. Greville (1974). Generalized Inverses: Theory and Applications.
New York, Wiley.
Berger, J. and L. Pericchi (1996). The Intrinsic Bayes factor for model selection and
prediction. Journal of the American Statistical Association 91(433), 109–122.
Berger, J. and T. Sellke (1987). Testing a point null hypothesis: the irreconcilability
of p-values and evidence. Journal of the American Statistical Association 82(397),
112–122.
Box, G. and G. Tiao (1992). Bayesian Inference in Statistical Analysis. New York, Wiley.
228

BIBLIOGRAPHY
Brooks, S. (2002). Discussion on Bayesian measures of model complexity and ﬁt (by D.J.
Spiegelhalter et al). Journal of the Royal Statistical Society, Series B 64(4), 616–618.
Burnham, K. and D. Anderson (2004). Multimodel inference: Understanding AIC and
BIC in model selection. Sociological Methods and Research 33(2), 261–304.
Chipman, H., E. George, and R. McCulloch (2001). The practical implementation of
Bayesian model selection. In P. Lahiri (Ed.), Model Selection, Volume 38, pp. 67–
116. IMS, Beachwood, OH.
Cowell, R. (1998). Introduction to inference in Bayesian networks. In M. Jordan (Ed.),
Learning in Graphical Models, pp. 9–26. Kluwer.
Cowell, R., A. Dawid, S. Lauritzen, and D. Spiegelhalter (1999). Probabilistic Networks
and Expert Systems. Springer-Verlag, New York.
Dahlhaus, R. and M. Eichler (2003). Causality and graphical models in time series anal-
ysis. In P. Green, N. Hjort, and S. Richardson (Eds.), Highly Structured Stochastic
Systems, pp. 115–137. Oxford University Press.
Delorme, A., G. Rousselet, M.-M. Mac´e, and M. Fabre-Thorpe (2004). Interaction of
top-down and bottom-up processing in the fast visual analysis of natural scenes.
Cognitive Brain Research 19(2), 103–113.
Dempster, A., N. Laird, and D. Rubin (1977). Maximum likelihood from incomplete data
via the EM algorithm. Journal of the Royal Statistical Society, Series B 39(1), 1–38.
Eichler, M. (2001). Markov properties for graphical time series models. Technical report,
Department of Statistics, University of Heidelberg.
Fletcher, R. (2000). Practical Methods of Optimization (2nd ed.). Wiley.
Friedman, N., K. Murphy, and S. Russell (1998). Learning the structure of dynamic prob-
abilistic networks. In Fourteenth Conference on Uncertainty in Artiﬁcial Intelligence,
pp. 139–147.
229

BIBLIOGRAPHY
Gelman, A. (2006). Prior distributions for variance parameters in hierarchical models.
Bayesian Analysis 1(3), 515–533.
Gelman, A., J. Carlin, H. Stern, and D. Rubin (1995). Bayesian Data Analysis. Chapman
and Hall.
Gelman, A. and D. Rubin (1992).
Inference from iterative simulation using multiple
sequences. Statistical Science 7(4), 457–511.
Geman, S. and D. Geman (1984).
Stochastic relaxation, Gibbs distributions and the
Bayesian restoration of images. IEEE Transactions on Pattern Analysis and Machine
Intelligence 6, 721–741.
Ghahramani, Z. (1997). Learning Dynamic Bayesian Networks. In C. Giles and M. Gori
(Eds.), Adaptive Processing of Temporal Information, Lecture Notes in Artiﬁcial In-
telligence, pp. 168–197. Berlin, Springer-Verlag.
Ghahramani, Z. (2004).
Unsupervised Learning.
In O. Bousquet, U. von Luxburg,
and G. Raetsch (Eds.), Advanced Lectures in Machine Learning, pp. 72–112. Berlin,
Springer-Verlag.
Giudici, P. and P. Green (1999). Decomposable graphical Gaussian model determination.
Biometrika 86(4), 785–801.
H¨aggstr¨om, O. (2002). Finite Markov Chains and Algorithmic Applications. Cambridge
University Press.
Harville, D. (1997). Matrix Algebra from a Statistician’s Perspective. New York, Springer.
Hastings, W. (1970).
Monte Carlo sampling methods using Markov chains and their
applications. Biometrika 57(1), 97–109.
Heckerman, D., D. Geiger, and D. Chickering (1995). Learning Bayesian networks: The
combination of knowledge and statistical data. Machine Learning 20, 197–243.
230

BIBLIOGRAPHY
Heidelberger, P. and P. Welch (1983). Simulation run length control in the presence of
an initial transient. Operations Research 31, 1109–1144.
Henderson, H. and S. Searle (1979). Vec and vech operators for matrices, with some uses
in Jacobian and multivariate statistics. The Canadian Journal of Statistics 7(1),
65–81.
Henderson, H. and S. Searle (1981). The vec-permutation matrix, the vec operator and
Kronecker products: a review. Linear and multilinear algebra 9, 271–288.
Husmeier, D. (2003). Sensitivity and speciﬁcity of inferring genetic regulatory interac-
tions from microarray experiments with dynamic Bayesian networks. Bioinformat-
ics 19(17), 2271–2282.
Johnson, N., S. Kotz, and A. Kemp (1992). Univariate Discrete Distributions (2nd ed.).
New York, Wiley.
Jones, B., C. Carvalho, A. Dobra, C. Hans, C. Carter, and M. West (2005). Experi-
ments in stochastic computation for high-dimensional graphical models. Statistical
Science 20(4), 388–400.
Kass, R. and A. Raftery (1995).
Bayes factors.
Journal of the American Statistical
Association 90(430), 773–795.
Kullback, S. and R. Leibler (1951).
On information and suﬃciency.
The Annals of
Mathematical Statistics 22(1), 79–86.
Lappalainen, H. and J. Miskin (2000). Ensemble learning. In M. Girolami (Ed.), Advances
in Independent Component Analysis, pp. 76–92. Springer-Verlag.
Lauritzen, S. (1996). Graphical Models. Oxford University Press.
Lindley, D. (1991). Discussion on Posterior Bayes factors (by M. Aitkin). Journal of the
Royal Statistical Society, Series B 53, 130–131.
231

BIBLIOGRAPHY
Lov´asz, L. (1993). Random walks on graphs: A survey. Combinatorics, Paul Erd¨os is
Eighty 2, 353–397.
Lucas, J., C. Carvalho, Q. Wang, A. Bild, J. Nevins, and M. West (2006). Sparse statistical
modelling in gene expression genomics. In K. Do, P. Mueller, and M. Vannucci (Eds.),
Bayesian Inference for Gene Expression and Proteomics, pp. 155–176. Cambridge
University Press.
L¨utkepohl, H. (2005). New Introduction to Multiple Time Series. Springer-Verlag.
MacKay, D. (1995a). Developments in probabilistic modelling with neural networks –
ensemble learning. In Neural Networks: Artiﬁcial Intelligence and Industrial Appli-
cations. Proceedings of the 3rd Annual Symposium on Neural Networks, Nijmegen,
Netherlands, pp. 191–198. Springer.
MacKay, D. (1995b). Probable networks and plausible predictions - a review of practical
Bayesian methods for supervised neural networks. Network: Computation in Neural
Systems 6, 469–505.
Mihajlovic, V. and M. Petkovic (2001). Dynamic Bayesian Networks: A State of the Art.
Technical report, Computer Science Department, University of Twente.
Miskin, J. (2000). Ensemble learning for independent component analysis. Ph. D. thesis,
University of Cambridge.
Moore, E. (1920). On the reciprocal of the general algebraic matrix. Bulletin of the
American Mathematical Society 26, 394–395.
Muirhead, R. (1982). Aspects of Multivariate Statistical Theory. New York, Wiley.
Murphy, K. (2002). Dynamic Bayesian Networks: Representation, Inference and Learn-
ing. Ph. D. thesis, UC Berkeley.
Neudecker, H. (1995). Mathematical properties of the variance of the multinomial distri-
bution. Journal of Mathematical Analysis and Applications 189, 757–762.
232

BIBLIOGRAPHY
Newton, M. and A. Raftery (1994). Approximate Bayesian inference with the weighted
likelihood bootstrap (with discussion). Journal of the Royal Statistical Society, Series
B 56(1), 3–48.
Nicolas, J. (2002). Introduction to second kind statistics: application of log-moments and
log-cumulants to SAR image law analysis. Traitement du signal 19(3), 139–167.
O’Hagan, A. (1991). Discussion on Posterior Bayes factors (by M. Aitkin). Journal of the
Royal Statistical Society, Series B 53, 136.
O’Hagan, A. (1995). Fractional Bayes factors for model comparison. Journal of the Royal
Statistical Society, Series B 57(1), 99–138.
O’Hagan, A., A. Stuart, J. Ord, and M. Kendall (1994). Kendall’s Advanced Theory of
Statistics: Bayesian Inference, Volume 2B. Edward Arnold.
Penny, W., S. Kiebel, and K. Friston (2006). Variational Bayes. In K. Friston, J. Ash-
burner, S. Kiebel, T. Nichols, and W. Penny (Eds.), Statistical Parametric Mapping:
The analysis of functional brain images. Elsevier, London.
Penny, W. and S. Roberts (2000). Bayesian methods for autoregressive models. In IEEE
Workshop on Neural Networks for Signal Processing, Sydney, Australia.
Penny, W. and S. Roberts (2002, February). Bayesian multivariate autoregressive models
with structured priors. In IEE Proceedings - Vision, Image, and Signal Processing,
Volume 149, pp. 33–41.
Penrose, R. (1955). A generalised inverse for matrices. Proceedings of the Cambridge
Philosophical Society 51, 406–413.
Petersen,
K.
and
M.
Pedersen
(2007,
September).
The
Matrix
Cookbook.
http://matrixcookbook.com.
Plummer, M., N. Best, K. Cowles, and K. Vines (2006, March). CODA: Convergence
diagnosis and output analysis for MCMC. R News 6(1), 7–11.
233

BIBLIOGRAPHY
Raftery, A. and S. Lewis (1992). How many iterations in the Gibbs sampler? In Bayesian
Statistics 4, pp. 763–773. Oxford University Press.
Rao, C. (1962). A note on a generalized inverse of a matrix with applications to problems
in mathematical statistics. Journal of the Royal Statistical Society, Series B 24(1),
152–158.
Rao, C. (1966). Generalized inverse for matrices and its applications in mathematical
statistics. In F. David (Ed.), Festschrift for J. Neyman: Research Papers in Statistics,
pp. 263–279. London, Wiley.
Rao, C. and S. Mitra (1972). Generalized inverse of a matrix and its applications. In
Proceedings of the Berkeley Symposium on Mathematical Statistics and Probability,
Volume 1, pp. 601–620. University of California Press.
Rice, J. (1995). Mathematical Statistics and Data Analysis. Duxbury Press.
Robert, C. (1993). A note on Jeﬀreys-Lindley paradox. Statistica Sinica 3, 601–608.
Robert, C. and D. Titterington (2002). Discussion on Bayesian measures of model com-
plexity and ﬁt (by D.J. Spiegelhalter et al). Journal of the Royal Statistical Society,
Series B 64(4), 621–622.
Roberts, S. and W. Penny (2002). Variational Bayes for generalized autoregressive models.
IEEE Transactions on Signal Processing 50(9), 2245–2257.
Russell, S. and P. Norvig (2003). Artiﬁcial Intelligence: A Modern Approach (2nd ed.).
Prentice Hall.
Saad, Y. (2003). Iterative Methods for Sparse Linear Systems (2nd ed.). SIAM.
Schwarz, G. (1978). Estimating the dimension of a model. The Annals of Statistics 6(2),
461–464.
234

BIBLIOGRAPHY
Scott, J. and J. Berger (2006). An exploration of aspects of Bayesian multiple testing.
Journal of Statistical Planning and Inference 136, 2144–2162.
Shafer, G. (1982).
Lindley’s paradox.
Journal of the American Statistical Associa-
tion 77(378), 325–334.
Speed, T. and H. Kiiveri (1986). Gaussian Markov distributions over ﬁnite graphs. The
Annals of Statistics 14(1), 138–150.
Spiegelhalter, D., N. Best, B. Carlin, and A. van der Linde (2002). Bayesian measures of
model complexity and ﬁt. Journal of the Royal Statistical Society, Series B 64(4),
583–639.
Spiegelhalter, D., A. Thomas, N. Best, and W. Gilks (1995). BUGS: Bayesian inference
using Gibbs sampling, Version 0.50.
Technical report, Medical Research Council
Biostatistics Unit, Institute of Public Health, Cambridge University.
Stoica, P. and Y. Sel´en (2004). Model-order selection: A review of information criterion
rules. IEEE Signal Processing Magazine 21(4), 36–47.
Winn, J. (2003). Variational message passing and its applications. Ph. D. thesis, Depart-
ment of Physics, University of Cambridge.
235

