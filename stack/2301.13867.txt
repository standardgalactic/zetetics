Mathematical Capabilities of ChatGPT
Simon Frieder∗1,4, Luca Pinchetti1, Ryan-Rhys Griﬃths3, Tommaso Salvatori2, Thomas
Lukasiewicz1,2, Philipp Christian Petersen4,5, Alexis Chevalier6, and Julius Berner4
1Department of Computer Science, University of Oxford, Oxford, UK
2Institute of Logic and Computation, TU Wien, Vienna, Austria
3Department of Physics, University of Cambridge, Cambridge, UK
4Faculty of Mathematics, University of Vienna, Vienna, Austria
5Research Network Data Science, University of Vienna, Vienna, Austria
6School of Mathematics, Institute for Advanced Study, Princeton, US
February 1, 2023
Abstract
We investigate the mathematical capabilities of ChatGPT by testing it on publicly available datasets, as
well as hand-crafted ones, and measuring its performance against other models trained on a mathematical
corpus, such as Minerva. We also test whether ChatGPT can be a useful assistant to professional
mathematicians by emulating various use cases that come up in the daily professional activities of
mathematicians (question answering, theorem searching). In contrast to formal mathematics, where
large databases of formal proofs are available (e.g., the Lean Mathematical Library), current datasets of
natural-language mathematics, used to benchmark language models, only cover elementary mathematics.
We address this issue by introducing a new dataset: GHOSTS. It is the ﬁrst natural-language dataset made
and curated by working researchers in mathematics that (1) aims to cover graduate-level mathematics
and (2) provides a holistic overview of the mathematical capabilities of language models. We benchmark
ChatGPT on GHOSTS and evaluate performance against ﬁne-grained criteria.
We make this new
dataset publicly available1 to assist a community-driven comparison of ChatGPT with (future) large
language models in terms of advanced mathematical comprehension. We conclude that contrary to many
positive reports in the media (a potential case of selection bias), ChatGPT’s mathematical abilities are
signiﬁcantly below those of an average mathematics graduate student. Our results show that ChatGPT
often understands the question but fails to provide correct solutions. Hence, if your goal is to use it to
pass a university exam, you would be better oﬀcopying from your average peer!
1
Introduction
Since its introduction, ChatGPT has rapidly become a widely known question-and-answer dialogue system.
It has been mentioned in traditional media across the globe [33, 28, 22] and across all major internet
platforms [40, 43]. According to Twitter data, it is by far the most talked about language model to date; cf.
Figure 1.
The performance of ChatGPT has been analyzed in a large number of exam-related use cases, with varying
degrees of scientiﬁc rigor, ranging from detailed studies to anecdotal evidence. Use cases include passing the
United States Medical Licensing Examination [17], scoring highly on the Psychology Today Verbal-Linguistic
Intelligence IQ Test [34], and answering (and generating) Operations Management exam questions that were
∗Corresponding author: simon.frieder@wolfson.ox.ac.uk. The subsequent author list is ordered randomly.
1github.com/friederrr/science-GHOSTS
1
arXiv:2301.13867v1  [cs.LG]  31 Jan 2023

2018
2019
2020
2021
2022
Year
0k
5k
10k
15k
20k
25k
30k
Number of Mentions
BERT
LaMDA
Codex
GPT-3
InstructGPT
ChatGPT
Figure 1: Twitter count data relating the counts of a selection of notable large language models from the
beginning of the release date of GPT-3. The x-axis is a log-scaled. ChatGPT counts by far dominate those
of all other language models. Vertical year-ticks denote the end of the mentioned year.
deemed to be within the scope of a typical MBA curriculum [41], all with a performance that elicited positive
surprise of the authors. Due to this and other reasons, it is widely believed that large language models
(LLMs) will impact a large number of areas and will be used as assistants by many professionals.
In this article, we will focus on performing a detailed analysis of the mathematical capabilities of ChatGPT.
This includes, but is not limited to, answering exam-style mathematical questions and investigating how
ChatGPT behaves in a number of mathematical contexts. Our analysis includes testing how many of the
skills ChatGPT can emulate that are necessary to do professional mathematics. Examples of such skills are
the ability to answer computational questions (“What is the value of
R π
2
0 arccos(
cos x
1+2 cos x) dx?”), the ability
to complete mathematical proofs that have gaps or missing steps, the ability to solve questions that are
more focused on deep insights and original solutions, such as those of mathematical Olympiads, and the
ability to survey the literature and think across domains (“which other theorems do we need to prove a given
theorem?”).
To do this, we have designed a thorough testing methodology to evaluate the outputs of ChatGPT, including
error codes that represent various possible failure modes of ChatGPT (see Section 3). We score ChatGPT’s
responses, we report on the results using this methodology, and we compare ChatGPT to state-of-the-art
models trained for mathematical comprehension.
Moreover, we have created new datasets of prompts that are aimed at testing speciﬁc aspects of ChatGPT
related to mathematical comprehension.
We evaluate ChatGPT by comparing it to random samples
from existing datasets that were devised to test models that were speciﬁcally trained for mathematical
comprehension [18, 16]. A number of the datasets are speciﬁcally designed so that the questions could not be
answered if ChatGPT were memorizing the results. All of those datasets are created by the authors.
In summary, the contributions of this article are threefold:
• First, insight for mathematical use is provided. We show for which types of questions and which
domains of mathematics, ChatGPT may be useful and how it could be integrated into the workﬂow of
a mathematician.
• Second, the failure modes of ChatGPT are identiﬁed, as well as the limits of its capabilities. This
can aid future eﬀorts to develop LLMs that perform better in mathematics. Our analysis is akin to
2

a mathematical model card, where the mathematical strengths and weaknesses are summarized (see
Section 4).
• Third, we provide benchmarks for testing the mathematical capabilities of future LLMs so that they can
be compared to ChatGPT across a range of aspects regarding advanced mathematical comprehension.
This is achieved by introducing new natural-language math datasets. Two of these benchmarks are
derived from the most advanced datasets regarding mathematical queries for language models that exist
today. Additionally, we devise four more datasets on which we benchmark ChatGPTs performance. We
release the collection of these datasets publicly on GitHub2, and we encourage community participation
by allowing GitHub pull requests in order to grow the datasets beyond their current sizes.
2
Related Work
As a large language model, ChatGPT can be universally employed to perform mathematical reasoning
and therefore has to compare with technologies that in this space are sometimes decades old. Performing
mathematical reasoning in an automated way has a long history and can be traced back to 1959 [37], the most
focus being devoted to proving theorems [11]. Presently, there is a realization that the classical approaches,
using a symbolic encoding of mathematics, have reached a plateau [14].
There is now a growing body of literature on learning mathematical relationships directly in a supervised-
learning manner [2, 10, 15] or by using LLMs to perform mathematical reasoning directly on mathematics
encoded in natural language [20]. Sometimes, the distinction is blurred because Transformers can also
be used in a supervised-learning setting and have been employed successfully in learning mathematical
relationships [18, 6].
Most recently published large language models, such as PaLM [8], released in 2022, are tested only on
elementary-level mathematical reasoning datasets, such as the GSM8K dataset [9]. We speculate this is
because the obtained results already suggest that the models struggle on much simpler datasets than ours,
such as the MathQA [1] dataset or the GSM8K dataset [9], respectively. For example, the version of PaLM
with 540 billion parameters with chain-of-thought prompting and access to an external calculator solves
only 58% on the GSM8K dataset [8, Table 10]. This model nonetheless outperforms GPT-3 [5] on the same
dataset, which only solves at best 54%; this performance is consistent with the performance of older models.
Variations of BERT [30] have been shown to only solve between 28% and 37% of the problems when ﬁne-tuned
and tested on the AQuA-RAT dataset [21], which is the direct predecessor of MathQA. In some cases, such as
the LaMDA model [42] or BLOOM [19], both released also in 2022 by Google, an evaluation of mathematical
reasoning capability is missing entirely.
Among the mentioned LLMs, Minerva [20], based on PaLM, stands out, being trained in equal parts on websites
that contain MathJax elements and arXiv preprints (and on general natural language data on which PaLM
was trained), achieving a score of roughly 50% on a signiﬁcantly harder dataset, the MATH (Mathematics
Aptitude Test of Heuristics) dataset [16] that was sourced from various mathematical competitions.
One distinguishing feature of the MATH dataset [16] is that its problems admit 1) a unique answer (no
open-ended questions) 2) and the answer can be condensed within a few characters (a number, for example).
This is beneﬁcial in terms of automatic evaluation of a model on such a dataset since one can simply ask
for the ﬁnal answer, ignoring the step-by-step solution (moreover, one can train models, as [16] do, to ﬁt
this style of inquiry and output either the ﬁnal solution only, or the step-by-step derivation leading to the
solution).
Among the supervised approaches, we mention [18], where a Transformer architecture was used to generate
symbolic solutions to integrating functions and ﬁnding closed-form solutions to ﬁrst-order and second-order
diﬀerential equations, which outperformed classical solvers, such as Mathematica, MATLAB, and Maple
2github.com/friederrr/science-GHOSTS
3

by at least 14% on a test set of integration problems. On the task of solving diﬀerential equations, the
Transformer-based approach still exceeds the classical approach, but by a smaller margin (at least 4% in
the case of ﬁrst-order diﬀerential equations and with more varied results for second-order equations). An
up-to-date survey on mathematical datasets and performance of various LLMs can be found in [23].
For ChatGPT, most investigations related to mathematical reasoning consist to date of anecdotal evidence
concerning its performance and its failure modes; see, e.g., [43, 32, 24, 40]. Unfortunately, a clear methodology
is missing, as most of the results are scattered on various internet platforms and are not easily reproducible.
To the best of our knowledge, the only mathematical investigation was undertaken in [4], which mainly
investigated ChatGPT’s capability to compute irrational numbers to high accuracy.
On the other hand, we like to mention the case of formalized mathematics, where large databases that encode
advanced mathematical concepts exist, e.g. the Lean Mathematical Library [25]. Some of the ideas that we
have used in this article, such as prompting with missing proofs, are echoed in [31] for formal mathematics.
Yet, for the purpose of doing mathematics with large language models, these formal datasets cannot be
leveraged since no straightforward way exists to convert them to natural language (in addition to various
issues, such as bias, that might occur in the context of an automatic conversion).
3
Datasets
3.1
Dataset creation
We assess the mathematical reasoning capabilities of ChatGPT by creating a collection of multiple datasets
of prompts, totaling 728 prompts, for which ChatGPT’s output was manually rated by experts. Then, we
record and rate each of the outputs provided by the model. The combined eﬀort of devising mathematically
insightful prompts, some of which are at graduate-level mathematics, and carefully rating the the output of
ChatGPT amount to several hundreds of person-hours.
We divide our entire collection of prompts into six subdatasets3, called
• Grad-Text
• Holes-in-Proofs
• Olympiad-Problem-Solving
• Symbolic-Integration
• MATH
• Search-Engine-Aspects
We summarize those in Table 1. The letters that are set in boldface make up the GHOSTS acronym.
Two of the subdatasets, the MATH subdataset and the Symbolic-Integration subdataset, use prompts taken
from existing datasets, [16] and [18], respectively. This was done in order to be able to compare how ChatGPT
performs against existing state-of-the-art models, one based on an LLM, Minvera [20], and one based on a
supervised-learning approach [18]. Nonetheless, signiﬁcant, additional annotation eﬀort was involved since in
both cases the authors, as experts in the ﬁeld, rated the output. Furthermore, in the second case, a conversion
from Polish notation was necessary.
The other subdatasets were hand-crafted by the authors. We note that it is neither possible to outsource the
creation of these datasets to a crowdsourcing service, such as Amazon Mechanical Turk, nor is it possible to
generate these datasets automatically from code because advanced mathematical insight is required for the
creation of each prompt (though based on our work, it might be possible to extend the dataset by creating
variations of our questions in a purely programmatic manner; see Section 5). Furthermore, unlike in the case
3In the GitHub repository, each subdataset corresponds to a folder, which in turn can consist of multiple ﬁles.
4

Dataset name
Comprised of the ﬁle(s)
Tags
Grad-Text
W. Rudin, Functional Analysis (ch. 1)
M3 Q4
W. Rudin, Functional Analysis (ch. 2)
M3 Q4
J. Munkres, Topology (ch. 1)
M3 Q4
J. Munkres, Topology (ch. 2)
M3 Q4
R. Durret, Probability Theory
M3 Q4
Holes-in-Proofs
Proofs Collection A
M3 Q2 Q5
Proofs Collection B Prealgebra
M1 Q5
Proofs Collection B Precalculus
M1 Q5
Olympiad-Problem-Solving
Olympiad Problem Solving
M4 Q4 D2
Symbolic-Integration
Symbolic Integration
M2 Q3 D1
MATH
MATH Algebra
M1 M2 M3 Q3 Q4
MATH Counting and Probability
M1 M2 M3 Q3 Q4
MATH Prealgebra
M1 Q3 Q4
MATH Precalculus
M1 Q3 Q4
Search-Engine-Aspects
Deﬁnition Retrieval
M3 Q1 Q2 D3
Reverse Deﬁnition Retrieval
M3 Q2 D3
Named Theorem Proof Completion
M3 Q1 Q2 D3
Table 1: A summary of all datasets, together with their associated tags. The tags Mi, Qi, and Di relate to
the level of Mathematical diﬃculty, the Question type, and the Out-of-Distribution type from Section 3.3,
respectively.
of the MATH dataset by [16] (see Section 2), the output of ChatGPT cannot be automatically evaluated and
a professional opinion on its output needs to be given. This raises the diﬃculty of the creation of more data,
since graduate-level mathematics (and in some cases PhD-level mathematics) is required. The mathematical
skill level of the authors matches this requirement.
Our dataset goes beyond all the mentioned mathematical datasets for LLMs in Section 2 in terms of
mathematical sophistication and in terms of the diﬀerent aspects of mathematical reasoning that are being
tested. It also surpasses datasets mentioned in the survey [23] along these dimensions. Futhermore, unlike for
the MATH dataset by [16], a large number of our questions are not of the type of having an answer that
can be condensed into a few tokens (e.g., when the answer is a mathematical proof), such as a number or a
function, and therefore evaluation cannot be performed automatically.
3.2
Format
The format of each of our subdatasets that make up our GHOSTS dataset follows the same convention
and each subdataset consists of (potentially multiple) JSON-formatted ﬁles with entries such as in the
example below. Our format is similar to, e.g., the AQuA-RAT (Algebra Question Answering with Rationales)
dataset [21]. A single datapoint in a JSON ﬁle has the following form:
{
"prompt" :
"Can you quote a famous
mathematical
theorem ?" ,
" output " :
" Euclid ’ s
Elements ,
Proposition
47:
In
right −angled
t r i a n g l e s ,
the
square
on the
s i d e
opposite
the
r i g h t
angle
i s
equal
to
the sum of
the
sqares
on the
other
two
s i d e s . " ,
" r a t i n g " :
" 5" ,
" errorcodes " :
[ "" ] ,
" warningcodes " :
[ "w1" ] ,
"comment" :
"This
i s
a c t u a l l y
Pythagoras ’
theorem ,
which was not
mentioned ,
hence
only
a warning
f l a g
i s
r a i s e d . " ,
" r e f "
:
""
5

"msc" :
"51M04" ,
" confidence " :
" high " ,
"timestamp"
:
"2023−01−30"
}
An example where ChatGPT was less successful is the following:
{
"prompt" :
"Suppose X i s
a
vector
space .
Prove
that
$0x = 0 = a0$
i f
$x
in X$ and
$a$
i s
a
s c a l a r . " ,
" output " :
"To prove
that
$0x = 0$
f o r
any $x
in X$ ,
we use
the
f a c t
that
$0$
i s
the
a d ditive
i d e n t i t y
of
the
vector
space $X$ ,
meaning
that
f o r
any $x
in X$ ,
$0 + x = x$ .
Thus ,
i f
we multiply
$0$ by any
s c a l a r
$a$ ,
we have $a (0 + x) =
ax = a0 + ax = a0$ ,
which shows
that
$a0 = 0$ .\ n\ nSimilarly ,
to
prove
that
$a0
= 0$
f o r
any
s c a l a r
$a$ ,
we use
the
f a c t
that
$0$
i s
the
m u l t i p l i c a t i v e
i d e n t i t y
of
the
s c a l a r
f i e l d ,
meaning
that
f o r
any
s c a l a r
$a$ ,
$a
cdot 1 = a$ .
Thus ,
i f
we multiply
$0$ by any
s c a l a r
$a$ ,
we have $a
cdot 0 = a cdot 1 cdot
0 = a cdot 0 = a0$ ,
which shows
that
$a0 = 0$ . " ,
" r a t i n g " :
" 3" ,
" errorcodes " :
[ "e5_2" , "e5_4" ] ,
" warningcodes " :
[ ] ,
"comment" :
" part
of
the
r e s u l t
i s
shown ,
but
the
argument
f o r
the
second
h a l f
i s
nonsense .
Also makes wrong
statements . " ,
"msc" :
"46A03" ,
" r e f " :
"Rudin−Functional
Analysis −Second−Ed . Part1−ex1/b−page38 " ,
" confidence " :
" high " ,
"timestamp"
:
"2023−01−18"
}
We explain each of these ﬁelds; unless explicitly mentioned below:
• prompt denotes the input that we supply to ChatGPT via its web interface. We always use a new
session for each prompt to avoid biasing ChatGPT by using previous prompts.
• output denotes the raw output that ChatGTP supplies us with;
• rating is a number from 1 to 5 that shows how many points ChatGPT has scored, 5 being a perfect
answer and 1 being the lowest. A detailed explanation regarding the rating policy that we followed is
contained in the appendix, Section A.
• errorcodes highlight a list of error types that illustrate the failure mode of ChatGPT in a more ﬁne-
grained way. Not all types of errors apply to all (sub)datasets: For example, an error code for a missing
proof step would not be applicable on a dataset that tests whether ChatGPT can multiply numbers or
ﬁnd prime divisors. This ﬁeld can be empty if no error code applies. The detailed explanation of the
error codes (and the warning codes; see below) that was provided to the annotators is contained in the
appendix, Section B.
• warningcodes highlight any problematic aspects of ChatGPT; for example, ChatGPT might be rambling
and providing the user with unrelated information or use a poor (but correct) way of solving problems.
This ﬁeld can be empty if no warning code applies.
• comment denotes any noteworthy commentary that an assessor of ChatGPTs may make. This can be
related to giving a more detailed explanation of output (or its failure modes), providing reasoning
behind awarding a certain error code, generally providing context, etc. For some subdatasets (see
Section 3.3), this ﬁeld was used to indicate the diﬃculty level of the prompt, as well as an oﬃcial
solution, if available. This ﬁeld is not required to always have a value.
6

• msc denotes the math subject classiﬁcation4 (MSC) that pertains to the output, not the prompt that one
gives to ChatGPT. This is because the prompt, unlike the output, might not really have a classiﬁcation,
for example, if ChatGPT is asked what the most important theorem in all of math is5.
• ref indicates a reference to where the prompt was originally taken from (for some subdatasets, such as
Holes-in-Proofs, we have changed proofs from various books or math.stackexchange.com; the original
source was recorded in this ﬁeld). This ﬁeld can be empty if the question was formulated by the authors
and no authoritative source was plausible.
• confidence indicates how conﬁdent we have perceived ChatGPT to be when presenting us with its
output. We allow values of “high”, “medium”, and “low”.
• timestamp denotes when the prompt was entered into ChatGPT.
Each subdataset is made up of multiple such data points. In cases where we used prompt engineering (the
Olympiad-Problem-Solving dataset), and asked variations of a single question, we enclosed multiple such data
points in an array within the JSON ﬁle.
The ﬁelds within a single data point interact in nontrivial ways: If a rating of 5 is given, then it is expected
that no error code is present—though there may be warning codes that are used. The error codes and warning
codes are loosely in the spirit of a compiler throwing errors and warnings if it is given incorrect or sloppy
code—although we have a role reversal, where the human is now the compiler and the machine produced the
code. In this sense, for some prompts, we have used multiple error and/or warning codes, which is why these
ﬁelds are arrays of strings. We use these codes to collect statistics on the behavior of ChatGPT; see Section 4.
The usage of MSC codes can be useful for mathematicians who want to integrate ChatGPT in their daily
workﬂow, as it allows them to know in which areas the model performs better and can hence be trusted more.
Our dataset is very diverse, as it has a total of 41 MSC codes. The top short version of these codes (ﬁrst
two digits) is 26 (“Real functions”, 125 occurrences) followed by 05 (“Combinatorics”, 124 occurrences) and
60 (“Probability theory and stochastic processes”, 100 occurrences). An exhaustive survey of ChatGPT’s
performance across every MSC code would necessitate a large, community-driven eﬀort to set up a very large
database. Because of the high cost of rating each output, which requires specialized skills, this is something
that no individual research group could reasonably do—but we hope that our approach is a starting point for
such an eﬀort.
For end-users of ChatGPT, it is desirable to avoid having a long-winded dialogue to arrive at a solution.
Therefore, we require that ChatGPT gives us the correct solution by providing it only the input without any
subsequent clariﬁcation. All chats with ChatGPT are thus “cold”. But we do allow the possibility of prompt
engineering of the input, where more information is added beyond the core prompt content.
3.3
The subdatasets
For most of our subdatasets, we have used LATEX to encode mathematical input. Our experiments have
shown that ChatGPT can process LATEX-encoded mathematics well. For example, on the Holes-in-Proofs
dataset, except for one case, the output of the prompts by ChatGPT was valid LATEX source code and could
be rendered.
The Grad-Text subdataset consists of a collection of books that are used widely in universities to teach upper
undergraduate or ﬁrst-year graduate courses in a degree in mathematics. We have used as prompt and rated
the output of most of the exercises from the ﬁrst and second chapters of these books (except for the book
[12] of which we only used exercises from the (quite long) ﬁrst chapter).
The Olympiad-Problem-Solving subdataset consists of a selection of exercises from the book Problem-Solving
Strategies, [13], that is often used when preparing for mathematical competitions. We selected and graded
4A complete list of MSC codes can be accessed under the URL zbmath.org/static/msc2020.pdf.
5The answer is Pythagoras’ theorem, according to ChatGPT.
7

ChatGPT output on one hundred exercises drawn from all chapters.
The Holes-in-Proofs subdataset consists of a number of proofs drawn from math.stackexchange.com, a
collection of books [3, 35], and the MATH dataset [16] where intentionally parts of the proofs where deleted
and ChatGPT was prompted to ﬁll the gaps: This was done either by (1) using a MISSING token, (2) ﬁnishing
the proof early and prompting ChatGPT to complete it, or (3) explicitly asking for certain conditions or
results.
The MATH subdataset consists of a random sample of prompts from the MATH dataset [16]. The MATH
dataset by [16] comes with a level of diﬃculty for each problem. We focused our random samples on two
domains, Algebra and Probability Theory, but sampled the same number of problems at each level of diﬃculty.
The Symbolic-Integration subdataset consists of random samples of integrals that were in the test set of [18].
We converted these from Polish notation to LATEX before prompting ChatGPT. The assessment was done by
comparing it with a classical symbolic integration system, Mathematica.
The Search-Engine-Aspects subdataset consists of problems that were not sampled from a particular source
and generated by a human expert in the ﬁeld. In the ﬁle Named Theorem Proof Completion we focused on
prompting ChatGPT to provide proof outlines of various theorems that are suﬃciently well-known within
Functional Analysis to have names. In the Deﬁnition Retrieval ﬁle, we prompted ChatGPT to state correctly
various deﬁnitions centered around Functional Analysis and Topology. In contrast, in the Reverse Deﬁnition
Retrieval ﬁle, we veriﬁed whether ChatGPT was able to deduce the name of a mathematical object by
describing its properties.
Our subdatasets can be categorized along multiple dimensions, which we state below:
Mathematical diﬃculty (ascending)
1. elementary arithmetic problems, as found in the MATH dataset [16] at lower levels of diﬃculty;
2. symbolic problems (integration of functions) that can be also solved via a supervised-learning, data-driven
approach to mathematics [18];
3. (under)graduate-level exercises from well-known textbooks [35, 36, 26, 12, 3] as well as questions from
math.stackexchange.com, spanning diverse domains of mathematics;
4. exercises that are in the style of mathematical olympiad problems, such as those taken from Engel’s
Problem-Solving Strategies book [13];
Question type
1. Review questions, which ask to state or name correctly certain mathematical facts (the Deﬁnition
Retrieval ﬁle of the Search-Engine-Aspects subdataset);
2. Overview-type review questions, which cut through an entire ﬁeld of mathematics (the Named Proof
Completion and Reverse Deﬁnition Retrieval parts of the Search-Engine-Aspects subdataset, as well the
Holes-in-Proofs subdataset);
3. Computational questions (the Symbolic Integration subdataset as well as various prompts from the
MATH dataset);
4. Proof-based questions, which ask for a theorem proof or for a puzzle solution (The Olympiad-Problem-
Solving subdataset, the Grad-Text subdataset);
5. Proof-completion questions, which ask for a proof that has missing gaps, or is incomplete, to be
completed (the Holes-in-Proofs subdataset as well as various prompts from the MATH dataset)
8

Types of high out-of-distribution likelihood
1. Nontrivial problem encoding: The data points from the Symbolic Integration subdataset come from [18]
and are publicly available6. Since the online training set uses Polish notation, it is very unlikely that
ChatGPT has seen these exact prompts before;
2. Succinct solution: The solutions for the Olympiad-Problem-Solving subdataset are included in the book
by Engel [13]. But the solutions are extremely concise, and simply repeating them would not show an
immediate understanding of the problem;
3. Spoken dialogue: The Search-Engine-Aspects subdataset is unlikely to be well represented in the data
on which ChatGPT has been trained since its prompts resemble word fragments that might appear in a
mathematical dialogue (e.g., an oral mathematical exam), rather than in a textbook;
One could, in theory, start to investigate every possible combination of these attributes (e.g., for elementary
arithmetic problems, in a non-trivial encoding, one could generate data to cover every possible question type
listed above). This would lead to 60 (sub)datasets, which, because of the manual, skilled curation eﬀort,
is by far too much for a single research group. Hence, we will allow pull requests in order to encourage
the community to contribute and grow these datasets, so that they can be used as a useful benchmark for
other LLMs. We have nonetheless striven in this work to cover each of these aspects individually, in some
subdataset, as can be seen in Table 1. Investigating every possible combination of attributes with a separate
(sub)dataset is not feasible.
Existing datasets do not cover all possible combinations of possibilities across all these dimensions. Devising
further datasets to do so is not feasible. Instead, we will devise a speciﬁc, well-crafted dataset to cover a
number of illustrative cases, which we describe below, so that we cover all these dimensions individually. We
collect statistics for ChatGPT’s output, including output length, the stability of the answer under prompt
engineering, as well as a personal rating of how close we perceived ChatGPT to be to the correct answer.
Because input to ChatGPT is purely textual, certain types of questions that have a strong geometrical
ﬂavor and might be stated and solved in non-text-based fashion (e.g., questions involving diagrams or small
algorithms, as occasionally occur in [13]) have been excluded.
4
Results
Will ChatGPT get you through a university math class? No, you would be better oﬀcopying from your
average peer! In this section, we analyze why by going through the common mistakes performed by ChatGPT
one dataset at a time.
Grad-Text
ChatGPT performed best on simple set-theory and logic questions (the ﬁrst chapter from the
book Topology by J. Munkres ([26])), which is reﬂected in its rating; see Figure 2. On the rest of the books,
it performed substantially worse. We note that it never failed to understand a query; the lowest grade it
received is a 2. Because of the conﬁdence (high) with which it outputs the answer, the use of ChatGPT is
particularly deceiving in this use-case, since it may be intensively used by students studying these subjects.
Olympiad-Problem-Solving
On this subdataset, ChatGPT performed poorly. Extra points were awarded
when the answer started to show promise, giving a score of 3, but most scores are 2 because the answer
does no show promise. No rating of 5 was awarded, and only two ratings of 4 were achieved. ChatGPT
had a tendency to try and solve many questions using induction arguments. While this is not obviously
false, this was very far from the solutions given in the book. ChatGPT’s inductive proofs were easily seen
to contain mistakes. ChatGPT often struggled to understand unusual puzzles and strange situations. For
example, on the questions based on changing the colour of the squares on a chess board, the solution oﬀered
6github.com/facebookresearch/SymbolicMathematics
9

1
2
3
4
5
Average Rating
Olympiad Problem Solving
W. Rudin, Functional Analysis (ch. 2)
Symbolic Integration
J. Munkres, Topology (ch. 2)
R. Durret, Probability  Theory
W. Rudin, Functional Analysis (ch. 1)
MATH Algebra
Named Theorem Proof Completion
MATH Counting and Probability
MATH Precalculus
Proofs Collection B Precalculus
Proofs Collection A
J. Munkres, Topology (ch. 1)
Definition Retrieval
MATH Prealgebra
Proofs Collection B Prealgebra
Reverse Definition Retrieval
Dataset
Figure 2: Average rating for each ﬁle in each subdataset (same colors indicate the ﬁles come from the same
subdataset). Since the maximal ranking is 5, and the minimal ranking where the question was at least
understood is 2, a passing grade (50% of points) would be 3.5, as indicated by the dotted line. The error bars
represent 95% conﬁdence intervals.
by ChatGPT obviously did not cover an 8 × 8 chessboard. Sometimes, ChatGPT would try to solve the
problem by changing only 5 squares, far from the 32 required. Similarly, ChatGPT struggled to respect
the unusual constraints given in the questions. This gave rise to lots of e6 errors. The solutions oﬀered by
ChatGPT obviously did not respect the rules set out in the question and show that ChatGPT struggles
to integrate surprising information into its answers. In some cases where the problem seemed to require
complicated mathematics but was actually solvable by elementary techniques, ChatGPT did not spot this but
instead referred to the general theory of, e.g., diophantine equations. ChatGPT would often say, e.g., that the
question could be solved with these means but that this was hard, so the conﬁdence score was downgraded to
medium or low; this was the only dataset we gave such conﬁdence score. From a mathematical point of view,
these questions were also by far the hardest, as they can also pose diﬃculty to professional mathematicians.
Holes-in-Proofs
ChatGPT correctly recognized most well-known results or concepts (e.g., ﬁlling the
mean-value theorem, given a proof where a reference to this theorem was missing, and ChatGPT needed to
ﬁll it in). In only three cases within Proofs Collection A, the question was not understood, which resulted in
the lowest possible rating of 1. We noted that ChatGPT was very strong at recognizing the context. The
notation of the output has always been the one given in the prompt. The ability of ChatGPT to execute
algebraic manipulations is surprisingly inconsistent. On many occasions, ChatGPT executes complicated
symbolic tasks with ease, and on many occasions, ChatGPT fails on basic arithmetic or basic rearranging.
The mistakes do not seem to correlate with the complexity of the algebraic expression. When ChatGPT
makes an algebraic mistake, it carries over this mistake reliably to the rest of the computation. We did not
spot any cases where the intermediate workings were wrong, but the ﬁnal solution was correct.
MATH
On the questions related to Algebra and Probability theory, ChatGPT got the reasoning often
correctly. However, the most common type of error was e4: ChatGPT may struggle when confronted with
standard operations, such as inverting fractions, least common multiples, changing the sign of numbers when
moving them from one side of the equal sign to the other. Often, in these questions, a correct solution requires
10

performing multiple operations in sequence. In most cases, at least one operation was wrong, preventing the
model to get a rating of 5 on the output.
Symbolic-Integration
ChatGPT was dominated by systems that were trained speciﬁcally to solve inte-
gration problems [18]. In a number of instances, ChatGPT got the structure of terms right (for example,
the number of summands in the output, as well as where factors had to be placed before summands), but it
failed at concrete computations. Even very simple examples were not correct. For example, the antiderivative
of x 7→x2/2 is evaluated to x 7→x3/3 + C, where C is a constant of integration (the correct answer is
x 7→x3/6 + C). For a number of prompts, ChatGPT claims there is no closed form solution for the integral
with complete conﬁdence when in fact there is a solution (see selection of worst-3 examples from Section F.
Named Theorem Proof Completion
On this part of the Search-Engine-Aspects subdataset, ChatGPT
knew almost all the theorems that it was asked at a basic level but made mistakes when stating them. When
it came to listing other results required for the proofs, ChatGPT typically requested way more than the
necessary theory—occasionally even results that only follow from the theorem which was asked for (error
code e5_5).
Deﬁnition Retrieval
On this part of the Search-Engine-Aspects subdataset, ChatGPT had a quite good
performance: it recited most deﬁnitions correctly. It sometimes got confused when being asked about
distributions in the sense of elements of the dual space of test functions. ChatGPT strongly favors the notion
of distributions in the stochastic sense. Similarly, for the adjective “closed”, where it chose to pick the context
of algebra (instead of topology) and interpreted it to mean “algebraically closed”.
Reverse Deﬁnition Retrieval
On this part of the Search-Engine-Aspects subdataset, ChatGPT had the
strongest performance, being able to recover most deﬁnitions from their descriptions, with an average rating
of 4.3. This indicates the usefulness of ChatGPT as a general-purpose mathematical search engine. This
subdataset is also the simplest from a mathematical point of view since no logical thinking is required, but
only a name needs to be found.
4.1
Overall Performance
If we take a rating of 3.5 to be the threshold between success and failure, then Figure 2 shows that for most
(in particular, harder) problems, ChatGPT will not pass. In particular, on problems that are within the style
of mathematical olympiads, ChatGPT performs badly. Moreover, Figure 2 shows that the achieved ratings
correspond closely to the ranking of mathematical diﬃculty of the exercises that a mathematician would
assign. We analyze the results for diﬀerent mathematical ﬁelds in Figure 3.
The prompt length has no clear eﬀect on the rating; see Figure 4. This eliminates prompt length as a
confounding variable, and in combination with the ﬁndings of Figure 2 strongly indicates that ChatGPT’s
rating is indeed reﬂective of mathematical diﬃculty.
Comparing ChatGPT to the performance obtained by [18], who obtained a percentage of correctly solved
integral that was close to 100% on a collection of 500 test equations [18, Table 3], our random sample of their
dataset (after translation from Polish notation and conversion to LATEX) achieves an average rating of 2.6,
where a rating of 2 is dominating 60% of the time followed by a rating of 4 almost 20% of the time. These
scores trail behind the performance that the model trained in [18] achieved.
The situation is similar when comparing ChatGPT to Minerva [20]. Their best model achieved an accuracy
of 50% on the MATH dataset [16, Table 3]. On our random sample of the MATH dataset, ChatGPT achieves
a perfect score only in 26% of cases.
One interesting ﬁnding is related to prompt engineering. On 20% of the questions from Olympiad-Problem-
Solving, the question was asked a second time, where ChatGPT was additionally instructed to proceed
11

1.0
1.5
2.0
2.5
3.0
3.5
4.0
4.5
5.0
Average Rating
00
26
28
54
33
46
97
51
40
05
15
42
60
03
11
47
MSC code
Figure 3: Average rating over mathematical ﬁelds (only those MSC codes that have at least 6 occurrences are
shown). Note that the ranking is not indicative of the complexity of the ﬁelds since we do not use equally
complicated exercises for all ﬁelds. The error bars represent 95% conﬁdence intervals.
step-by-step. This type of engineering is recommended by OpenAI in their cookbook, to improve reliability7.
This had an interesting eﬀect: Less severe errors e2 went down, though more severe errors (e5-type errors)
stayed about the same, as did the average rating, see Figure 5.
ChatGPT is usually very conﬁdent, unlike other GPT-like models, such as Codex, that we have experimented
with. Consider in this sense the following statement (of statements):
Here
are
two
statements :
"Let
(X, A, μ)
be a measure
space .
Show that
the
signed
measures λ : A -> R form a Banach
space . " ;
"Let $ (X,A,\mu) $ be a measure
space .
Show that
the
signed
measures $\lambda :A \ rightarrow R$ form a Banach
space ."
Are
they
equivalent ?
The response by ChatGPT is not phrased in order to show any nuance in terms of conﬁdence (this is typical,
even if ChatGPT is wrong):
Yes ,
the two
statements
are
equivalent .
They both
express
the same mathematical
idea ,
but
use
d i f f e r e n t
notation
and
formatting
conventions .
The
f i r s t
statement
uses
parenthess
to
enclose
the
elements
of
the
measure
space
and
r e g u l a r
l e t t e r s
to
represent
the
functions ,
while
the
second
statement
uses
subscripted
parentheses
and
uses
the LaTeX formatting
to
represent
the
elements
of
the
measure
space
and the
function .
The response by Codex [7], another model that we brieﬂy tested (but whose scope would have exceeded that
of a single conference article) gives a cautions response and, unlike ChatGPT, is capable of voicing doubt:
I
think
they
are
equivalent ,
but
I am not
sure .
7github.com/openai/openai-cookbook/blob/main/techniques_to_improve_reliability.md
12

1.0
1.5
2.0
2.5
3.0
3.5
4.0
4.5
5.0
Average Rating
20-57
58-85
86-119
120-161
162-202
203-276
277-423
424-1362
Prompt Length
1.0
1.5
2.0
2.5
3.0
3.5
4.0
4.5
5.0
Average Rating
19-187
188-355
356-523
524-691
692-858
859-1026
1027-1194
1195-1362
Prompt Length
Figure 4: Prompt length (for two binnings) has no signiﬁcant eﬀect on the rating. Over all prompts, average
is 3.2, with a standard deviation of 1.23.
5
Conclusion
We have examined the behavior of ChatGPT across various datasets that test multiple aspects of mathematical
skill. Contrary to the media sensation that ChatGPT has caused (see the Twitter counts from Figure 1),
ChatGPT is not yet ready to deliver high-quality proofs or calculations consistently. At the same time, the
quality of the answer can be positively surprising. In Section F in the appendix, we collect the best and the
worst results for a number of selected datasets. The best responses can be seen to justify the media sensation.
It seems fair to say that ChatGPT is inconsistently bad at advanced mathematics: While its ratings drop
with the mathematical diﬃculty of a prompt, it does give insightful proofs in a few cases.
However, ChatGPT falls short of achieving the same performance as models that are speciﬁcally trained for
one single task. These models, in contrast, lack the ﬂexibility of ChatGPT, which is a universal tool suitable
for any area of mathematics.
Nonetheless, its ability to search for mathematical objects, given information about them, is where ChatGPT
shines. It received its highest scores on the Reverse Deﬁnition Retrieval ﬁles from the Search-Engine-Aspects
subdataset.
Because of prohibitive annotation eﬀort, our dataset is not large enough to be used to ﬁne-tune LLMs in
13

50
40
30
20
10
0
Figure 5: Eﬀect of prompt engineering on the rating (left-most bars). Prompt engineering seems to only
reduce error codes with smaller indexes (e2, e3, e4) that are not rooted in faulty logic. On the other hand,
error codes related to logical mistakes (e5) even increase. Moreover, prompt engineering does not aﬀect the
average rating at all.
order to increase their mathematical ability; though we believe it is suﬃciently comprehensive to allow an
evaluation of existing LLMs. We also note as a recommendation for future LLM design, that incorporating
some form automatic evaluation capabilities, as done by [16] is essential for lowering the cost of rating the
output.
We hope that the dataset that we release with this publication will motivate other professional mathematicians
to contribute in order to establish a thorough benchmark for assessing the mathematical abilities of LLMs.
We will allow pull requests on our GitHub repository and encourage public participation. We encourage other
researchers to mine our dataset beyond the descriptive statistics we have computed, in order to gain a deeper
understanding of the behavior of ChatGPT (and other LLMs) on mathematics.
References
[1] Aida Amini, Saadia Gabriel, Shanchuan Lin, and Rik Koncel-Kedziorski et al. MathQA: Towards
interpretable math word problem solving with operation-based formalisms. In Proceedings of the 2019
Conference of the North American Chapter of the Association for Computational Linguistics: Human
Language Technologies, Volume 1 (Long and Short Papers), pages 2357–2367, Minneapolis, Minnesota,
June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1245.
[2] Malik Amir, Yang-Hui He, Kyu-Hwan Lee, Thomas Oliver, and Eldar Sultanow. Machine Learning
Class Numbers of Real Quadratic Fields. arXiv preprint arXiv:2209.09283, 2022.
[3] Sheldon Axler. Linear algebra done right. Springer, 2015.
[4] Amos Azaria. ChatGPT Usage and Limitations, Retrieved 2023-01-15. URL https://hal.science/hal-
03913837.
[5] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, and Jared D Kaplan et al. Language
models are few-shot learners. In Advances in Neural Information Processing Systems, volume 33, pages
1877–1901. Curran Associates, Inc., 2020.
[6] Francois Charton, Amaury Hayat, and Guillaume Lample. Learning advanced mathematical computations
from examples. In International Conference on Learning Representations, 2021.
[7] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, and Henrique Ponde de Oliveira Pinto et al.
Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021.
14

[8] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, and Gaurav Mishra et al. Palm:
Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.
[9] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, and Heewoo Jun et al. Training
veriﬁers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.
[10] Alex Davies, Petar Veličković, Lars Buesing, Sam Blackwell, and Daniel Zheng et al.
Advancing
mathematics by guiding human intuition with AI. Nature, 600(7887):70–74, 2021.
[11] Jörg Denzinger, Matthias Fuchs, Christoph Goller, and Stephan Schulz. Learning from previous proof
experience: A survey. Technical report, TU München, 1999.
[12] R. Durrett. Probability: Theory and Examples. Cambridge Series in Statistical and Probabilistic
Mathematics. Cambridge University Press, 2019.
[13] Arthur Engel. Problem-Solving Strategies. Springer, New York, 1998.
[14] John Harrison, Josef Urban, and Freek Wiedijk. History of interactive theorem proving. In Computational
Logic, volume 9, pages 135–214, 2014.
[15] Yang-Hui He. Machine-learning the string landscape. Physics Letters B, 774:564–568, 2017. doi:
https://doi.org/10.1016/j.physletb.2017.10.024.
[16] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, and Steven Basart et al. Measuring
mathematical problem solving with the MATH dataset. arXiv preprint arXiv:2103.03874, 2021.
[17] Tiﬀany H. Kung, Morgan Cheatham, Arielle Medenilla, Czarina Sillos, and Lorie De Leon et al.
Performance of ChatGPT on USMLE: Potential for AI-Assisted Medical Education Using Large Language
Models. medRxiv, 2022. doi: 10.1101/2022.12.19.22283643.
[18] Guillaume Lample and François Charton. Deep learning for symbolic mathematics. arXiv preprint
arXiv:1912.01412, 2019.
[19] Teven Le Scao and Angela Fan et al. Bloom: A 176b-parameter open-access multilingual language model.
arXiv preprint arXiv:2211.05100, 2022.
[20] Aitor Lewkowycz, Anders Johan Andreassen, David Dohan, Ethan Dyer, and Henryk Michalewski et el.
Solving quantitative reasoning problems with language models. In Advances in Neural Information
Processing Systems, 2022.
[21] Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom. Program induction by rationale generation:
Learning to solve and explain algebraic word problems. In Proceedings of the 55th Annual Meeting of the
Association for Computational Linguistics (Volume 1: Long Papers), pages 158–167, Vancouver, Canada,
July 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1015.
[22] Sascha Lobo.
Das Ende von Google,
wie wir es kannten,
Retrieved 2023-01-10.
URL
https://www.spiegel.de/netzwelt/netzpolitik/bessere-treffer-durch-chatgpt-das-ende-
von-google-wie-wir-es-kannten-kolumne-a-77820af6-51d7-4c03-b822-cf93094fd709.
[23] Pan Lu, Liang Qiu, Wenhao Yu, Sean Welleck, and Kai-Wei Chang. A survey of deep learning for
mathematical reasoning. arXiv preprint arXiv:2212.10535, 2022.
[24] Tranquil Sea Of Math. Does ChatGPT code LaTeX and write proofs?, Retrieved 2023-01-12. URL
https://www.youtube.com/watch?v=ge2N7VI_8P0.
[25] The mathlib Community. The lean mathematical library. In Proceedings of the 9th ACM SIGPLAN
International Conference on Certiﬁed Programs and Proofs. ACM, jan 2020. doi: 10.1145/3372885.
3373824.
15

[26] James R. Munkres. Topology. Prentice-Hall, Upper Saddle River, NJ, 2000.
[27] Natalie.
ChatGPT – Release Notes, Retrieved 2023-01-31.
URL https://help.openai.com/en/
articles/6825453-chatgpt-release-notes.
[28] John Naughton. The ChatGPT bot is causing panic now – but it’ll soon be as mundane a tool as Excel,
Retrieved 2023-01-14. URL https://www.theguardian.com/commentisfree/2023/jan/07/chatgpt-
bot-excel-ai-chatbot-tec.
[29] Long Ouyang, JeﬀWu, Xu Jiang, Diogo Almeida, and Carroll L. Wainwright et al. Training language
models to follow instructions with human feedback. arXiv preprint arXiv:2203.02155, 2022.
[30] Piotr Piękos, Mateusz Malinowski, and Henryk Michalewski. Measuring and improving BERT’s mathe-
matical abilities by predicting the order of reasoning. In Proceedings of the 59th Annual Meeting of the
Association for Computational Linguistics and the 11th International Joint Conference on Natural Lan-
guage Processing (Volume 2: Short Papers), pages 383–394. Association for Computational Linguistics,
August 2021. doi: 10.18653/v1/2021.acl-short.49.
[31] Markus N. Rabe, Dennis Lee, Kshitij Bansal, and Christian Szegedy. Language modeling for formal
mathematics. arXiv preprint arXiv:2006.04757, 2020.
[32] Richard Van Noorden @richvn@mastodon.social [@Richvn]. Huh. ChatGPT conﬁdently gives the right
kind of reasoning to solve this math problem, but whiﬀs on the algebra in the middle and gets the
answer wrong. [Tweet]. Twitter, Retrieved 2023-01-09. URL https://twitter.com/Richvn/status/
1598714487711756288.
[33] Kevon Roose. The Brilliance and Weirdness of ChatGPT, Retrieved 2023-01-24. URL https://www.
nytimes.com/2022/12/05/technology/chatgpt-ai-twitter.html.
[34] David Rozado.
What is the IQ of ChatGPT?, Retrieved 2023-01-09. URL https://davidrozado.
substack.com/p/what-is-the-iq-of-chatgpt.
[35] W. Rudin. Principles of Mathematical Analysis. International series in pure and applied mathematics.
McGraw-Hill, 1976.
[36] Walter Rudin. Functional analysis. McgGraw-Hill, Inc, New York, 1991.
[37] A. L. Samuel. Some studies in machine learning using the game of checkers. IBM Journal of Research
and Development, 3(3):210–229, 1959. doi: 10.1147/rd.33.0210.
[38] Sarah Wiegreﬀe (sigmoid.social/@sarah) [@sarahwiegreﬀe]. If text-davinci-001 is a rough approximate to
the model reported in the NeurIPS 2020 paper, and text-davinci-002 is InstructGPT in the 2022 preprint,
then what is just "davinci"? Trying to reproduce results from a time before this naming existed. [Tweet].
Twitter, 2023-01-15. URL https://twitter.com/BlancheMinerva/status/1584788418751406080.
[39] John Schulman, Barret Zoph, Christina Kim, Jacob Hilton, and Jacob Menick et al. ChatGPT: Optimizing
Language Models for Dialogue, Retrieved 2023-01-24. URL https://openai.com/blog/chatgpt.
[40] teddy [@teddynpc]. I made ChatGPT take a full SAT test. Here’s how it did: [Tweet]. Twitter, 2023-01-13.
URL https://twitter.com/teddynpc/status/1598767389390573569.
[41] Christian Terwiesch. Would Chat GPT3 Get a Wharton MBA? A Prediction Based on Its Performance
in the Operations Management Course, Retrieved 2023-01-04. URL https://mackinstitute.wharton.
upenn.edu/wp-content/uploads/2023/01/Christian-Terwiesch-Chat-GTP.pdf.
[42] Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, and Apoorv Kulshreshtha et al. Lamda:
Language models for dialog applications. arXiv preprint arXiv:2201.08239, 2022.
16

[43] Timothy Gowers [@wtgowers]. It’s amusing when ChatGPT makes ridiculous mathematical mistakes.
But of course, it’s more interesting to ﬁnd out what it can do well. Here’s one example that wasn’t bad:
I gave it a very rough outline of a proof and asked it to ﬁll in the details. [Tweet]. Twitter, 2023-01-13.
URL https://twitter.com/wtgowers/status/1611750773607604224.
[44] Carroll Wainwright and Ryan Lowe.
InstructGPT: Training Language Models to Follow Instruc-
tions with Human Feedback , Retrieved 2023-01-09. URL https://github.com/openai/following-
instructions-human-feedback.
17

Appendix
A
Rating policy
Below is the policy that was followed by each assessor of ChatGPT’s output regarding the rating score:
• 0 →ChatGPT cannot process the query (due to consistently occurring timeouts or errors);
• 1 →failure to understand the query (e.g., the user asks it something about number theory and it
responds with information about diﬀerential equations);
• 2 →query was understood but the answer was entirely wrong (e.g., the user asks what the prime
divisors of 111 are8 and it responds with 8 and 6);
• 3 →query was understood but the answer was only partially correct (e.g., the user asks it what the
prime divisors of 111 are and it responds with 3 and 6);
• 4 →query was understood and the answer was mostly correct (e.g, the user asks it what the prime
divisors of 222 are9 and it responds with 3 and 37);
• 5 →query was understood and answer was completely correct.
B
Error and warning code policy
Below is the policy that was followed by each assessor of ChatGPT’s output regarding the error codes and
warning codes:
Error codes:
• e1 →missing examples (e.g., the user asks it what the prime divisors of 111 are and it responds with
3, missing 37);
• e2 →a few wrong statements (e.g., the user asks it what the prime divisors of 30030 are10 and it
responds with 2, 3, 5, 7, 13);
• e3 →a lot of wrong statements (e.g., the user asks it what the prime divisors of 30030 are and it
responds with 2, 5, 8, 12, 13, 15);
• e4 →wrong computations (an additional error ﬂag to disambiguate between statements that are of
computational nature or not);
• e5 →denotes wrong logic or wrong ﬂow of arguments, which we further subdivide into speciﬁc ﬂags, as
we prohibit the use of e5 on its own, as it would be uninformative:
– e5_1 →ChatGPT claims that to complete a proof, statements need to be shown that are unrelated
to the claim;
– e5_2 →a proof step is missing;
– e5_3 →an edge case has not been considered ;
– e5_4 →an inference step is not supported (e.g., ChatGPT claims that from A follows B, but this
claim is not true);
– e5_5 →circular logical argument (using the hypothesis to prove the hypothesis);
8They are 37 and 3.
9They are 2, 37 and 3.
10They are 2, 3, 5, 7, 11.
18

• e6 →the general set-up is understood but the legal operations are not respected (e.g., we are given a
puzzle where we’re only allowed to add even integers but it changes the rules and motivates the solution
by allowing addition of odd integers).
Warning codes:
• w1 →ChatGPT is withholding essential information related to the prompt (e.g., the user asked it
something about the integral
R ∞
−∞e−x2 dx and it answers correctly but doesn’t tell the user that the
integral was actually a famous, named integral, the Gaussian integral)
• w2 →ChatGPT is rambling (after answering (correctly or incorrectly) ChatGPT tells the user much
more than the user wanted to know)
• w3 →ChatGPT is hallucinating (after answering (correctly or incorrectly) ChatGPT tells the user
unrelated stuﬀ)
• w4 →weirdness (ChatGPT is being weird, for example, by using a weird proof structure (where
applicable), using strange mathematical formulations, or by adopting a strange tone of the conversation
or making opinionated statements)
• w5 →it takes a number of tries to get ChatGPT to answer a prompt (because occasional timeouts or
errors occur that are related to this particular prompt)
• w6 →ChatGPT changes the notation from the prompt without being instructed to do so (e.g., the
prompt contains a vector space X, but ChatGPT calls it F)
C
ChatGPT version
We focus on the 9th-January-2023 version of ChatGPT [27], as made available through web access at chat.
openai.com/chat. This version was online for the majority of the writing process of this article. Since the
30th of January 2023, a new version has been online which we will comment on at the end of this section.
Focusing on one version is necessary because precise details of the model architecture and, in particular,
ChatGPT’s training methodology have not been released in the introductory statement [39] by its creator,
OpenAI. ChatGPT is the latest model of the GPT lineage [27], being based on InstructGPT, which in turn
is based on a trained GPT-3 [5], and ﬁne-tuned using reinforcement learning with human feedback [29]. We
note that even for models that predate ChatGPT, such as InstructGPT, where research articles and model
cards [44] have been released, full reproducibility is not possible since the code and exact datasets have not
been released. Furthermore, it was conﬁrmed by OpenAI employees that a slight mismatch exists between the
trained model that is accessible via OpenAI web interface and the model referred to in the oﬃcial paper [38].
This indicates how essential it is to document carefully which model our analysis pertains to; in our dataset,
we have included time stamps for each prompt in order to be able to track any changes in ChatGPT’s version
that have occurred.
In contrast to the version that was studied in this manuscript, the latest version from 30th-January-2023
is claimed to have “improved factuality and mathematical capabilities.”
[27]. We compared the outputs
computed by the latest version to the old version on two of our subdatasets and could not ﬁnd any substantial
diﬀerences in the resulting average rating: On the W. Rudin Functional Analysis (ch. 1) ﬁle of the Grad-Text
subdataset (29 prompts) the average rating of the 9th January version was 2.96 and on the 30th January
version it was 2.90. Additionally, on the 20 prompts of the Proofs Collection A ﬁle in the Holes-in-Proofs
subdataset, where missing equations or conditions need to be ﬁlled in, the average rating slightly dropped
from 3.5 to 3.3. It is planned to add a full comparison between the 09th and 30th January versions in the
near future.
19

D
Further ﬁgures of ChatGPT’s performance
In this section, we collect further ﬁgures, which are not directly related to our discussion in the main body of
the article, but which provide other views on the data and other descriptive statistics.
0.0
0.2
0.4
0.6
0.8
1.0
Error codes percentage
Symbolic Integration
Reverse Definition Retrieval
Definition Retrieval
MATH Precalculus
Proofs Collection B Precalculus
Proofs Collection B Prealgebra
MATH Prealgebra
Named Theorem Proof Completion
MATH Algebra
MATH Counting and Probability
J. Munkres, Topology (ch. 1)
Olympiad Problem Solving
Proofs Collection A
J. Munkres, Topology (ch. 2)
W. Rudin, Functional Analysis (ch. 2)
R. Durret, Probability  Theory
W. Rudin, Functional Analysis (ch. 1)
Dataset
e1
e2
e3
e4
e5
e6
Figure 6: Error types per dataset sorted by relative frequency of e5 (the most high-level error type).
e1
e2
e3
e4
e5_1
e5_2
e5_3
e5_4
e5_5
e6
w1
w2
w3
w4
w5
w6
Proofs Collection A
Proofs Collection B Prealgebra
Proofs Collection B Precalculus
J. Munkres, Topology (ch. 1)
J. Munkres, Topology (ch. 2)
R. Durret, Probability  Theory
W. Rudin, Functional Analysis (ch. 1)
W. Rudin, Functional Analysis (ch. 2)
MATH Algebra
MATH Counting and Probability
MATH Prealgebra
MATH Precalculus
Olympiad Problem Solving
Definition Retrieval
Named Theorem Proof Completion
Reverse Definition Retrieval
Symbolic Integration
Dataset
0.1
0.2
0.3
0.4
0.5
0.6
0.7
Figure 7: Relative frequencies of error codes by subdatasets and ﬁles.
20

1.0
1.5
2.0
2.5
3.0
3.5
4.0
4.5
5.0
Average Rating
47B
05C
00A
26A
54A
54C
26D
97G
60F
28A
46A
46F
33B
97H
60E
26B
51A
97F
97K
34A
40A
46B
42B
42A
60B
15A
03B
42C
11A
46E
05A
46C
47A
54D
60A
03E
35D
12D
MSC code
Figure 8: Rating by MSC codes (MSC code longer version).
E
Copyright
Some of the subdatasets contain prompts that may be protected under copyright (e.g., exercises from
textbooks). In these cases, we have used the SHA256 hash function, and the publicly released datasets will
only contain the hashed value of the prompt that we used, together with a detailed reference that points to
the exact text page and theorem or exercise that was used. The non-hashed version of these datasets is kept
privately.
21

F
Best-3 and worst-3 across selected subdatasets and ﬁles
We list below the best and worst answers of Chat-GPT over a selection of subdatasets. For readability,
the prompts and answers are lightly modiﬁed so that the LATEX-based formulas are correctly displayed and
whitespace is removed.
F.1
Grad-Text
Examples from Grad-Text subdataset, comprising the books [36, 26, 12].
Best-3
"prompt" :
"Let µ be a measure ,
∥f∥∞= inf{M : µ({x : |f(x)| > M}) = 0} and ∥g∥1 = R
|g|dµ .
Prove
that R
|fg|dµ ≤∥g∥1∥f∥∞. " ,
" output " :
"Let M = ∥f∥∞and ϵ > 0 .
Then ,
by the
d e f i n i t i o n
of
∥f∥∞, we have
µ({x : |f(x)| > M + ϵ}) = 0 .
Therefore ,
Z
|fg|dµ =
Z
|f|≤M+ϵ
|fg|dµ +
Z
|f|>M+ϵ
|fg|dµ ≤(M + ϵ)
Z
|g|dµ +
Z
|f|>M+ϵ
|f||g|dµ.
The second
term on the
r i g h t
s i d e
i s
zero
because µ(x : |f(x)| > M + ϵ) = 0 .
So we
have R
|fg|dµ ≤(M + ϵ)R
|g|dµ = (M + ϵ)∥g∥1 .
As ϵ can be
a r b i t r a r i l y
small ,
we can
l e t
i t
go
to
zero
and we get R
|fg|dµ ≤M∥g∥1 = ∥f∥∞∥g∥1 .
Hence proved ."
"prompt" :
" I f
{Tα}
i s
a
family
of
t o p o l o g i e s
on X , show that T
α Tα
i s
a
topology
on X . " ,
" output " :
"To show
that T
α Tα
i s
a
topology
on X , we need
to show that
i t
s a t i s f i e s
the
three
axioms
of
a
topology :
∅and X
are
both
in T
α Tα
s i n c e
∅
and X
are
in
every
topology Tα .
I f
{Ui}i∈I
i s
a
c o l l e c t i o n
of
s e t s
in T
α Tα ,
then S
i∈I Ui
i s
a l s o
in T
α Tα .
Since
each Tα
i s
a
topology , S
i∈I Ui
i s
in
every
Tα, and so
i t
i s
a l s o
in T
α Tα .
I f
{Ui}i∈I
i s
a
c o l l e c t i o n
of
s e t s
in T
α Tα ,
then
T
i∈I Ui
i s
a l s o
in T
α Tα .
Since
each Tα
i s
a
topology , T
i∈I Ui
i s
in
every Tα , and
so
i t
i s
a l s o
in T
α Tα .
Therefore , T
α Tα
i s
a
topology
on X ."
"prompt" :
"Let X be a
vector
space
and A and B
subsets
of
X .
I f
A and B
are
balanced ,
so
i s
A + B . " ,
" output " :
"A subset S
of
a
vector
space X
i s
c a l l e d
balanced
i f
f o r
every
vector
x in X and
every
s c a l a r
λ with |λ| ≤1 , λx
i s
in S
i f
x
i s
in S . To prove
that
A + B
i s
balanced ,
we must show
that
f o r
every
vector x = a + b in A + B , and
every
s c a l a r
λ with |λ| ≤1 , λx = λ(a + b) = λa + λb
i s
in A + B
i f
x
i s
in A + B .
Since
A and B
are
balanced ,
we know that
f o r
every a in A and
every
s c a l a r
λ with
|λ| ≤1 , λa
i s
in A
i f
a
i s
in A .
Similarly ,
f o r
every b in B and
every
s c a l a r
λ
with |λ| ≤1 , λb
i s
in B
i f
b
i s
in B .
So ,
i f
x = a + b
i s
in A + B ,
then a
i s
in A
and b
i s
in B .
Therefore ,
λa
i s
in A and λb
i s
in B , and thus λx = λa + λb
i s
in
A + B .
Therefore ,
A + B
i s
balanced ."
Worst-3
"prompt" :
"Find a
function
f : R →R that
i s
continuous
at
p r e c i s e l y
one
point . " ,
" output " :
"One example
of
such a
function
i s
f(x) =

x2
if x ̸= 0
1
if x = 0.
This
function
i s
continuous
at x = 0 but
not
at
any
other
point ."
22

"prompt" :
"Let X be a
t o p o l o g i c a l
vector
space .
All
s e t s
mentioned
below
are
understood
to
be
the
subsets
of
X .
Prove
the
f o l l o w i n g
statement :
I f
A
i s
compact and B
i s
closed ,
then A + B
i s
closed . " ,
" output " :
"A t o p o l o g i c a l
vector
space
i s
a
vector
space
with a
topology
such
that
the
vector
addition
and
s c a l a r
m u l t i p l i c a t i o n
are
continuous . To prove
that
A + B
i s
closed ,
we must show
that
the
complement
of
A + B
i s
open .
Let
y ∈X \ (A + B) .
Then there
e x i s t s
some x1 ∈A and x2 ∈B such
that y = x1 + x2 .
Since
A i s
compact and B
i s
closed ,
there
e x i s t
neighborhoods U1
of
x1 and U2
of
x2
such
that U1
i s
r e l a t i v e l y
compact and X \ U2
i s
open . Now consider
the
s e t
V = U1 + U2 = {x1 + x2 : x1 ∈U1, x2 ∈U2} .
This
s e t
i s
an open
neighborhood
of
y and
i t
i s
contained
in X \ (A + B) ,
which shows
that X \ (A + B)
i s
open and hence A + B
i s
closed ."
"prompt" :
"Let X be a
t o p o l o g i c a l
vector
space .
All
s e t s
mentioned
below
are
understood
to
be
the
subsets
of
X .
Prove
the
f o l l o w i n g
statement :
The sum of
two
closed
s e t s may
f a i l
to
be
closed . " ,
" output " :
"A counterexample
can be used
to
prove
that
the sum of
two
closed
s e t s
may
f a i l
to
be
closed .
For example ,
l e t
X = R and
l e t
A = [0, 1] and B = (1, 2] .
Both
A and B
are
closed
s e t s
in R ,
but A + B = [0, 2]
i s
not a
closed
s e t
in R."
F.2
Holes-in-Proofs (Proofs Collection A)
Examples from the Holes-in-Proofs subdataset, Proofs Collection A ﬁle, based on the books [3, 35] and
questions from math.stackexchange.com
Best-3
"prompt" :
"We want to show
that
any open
subset
of
R
i s
a
countable
union
of
d i s j o i n t
open
i n t e r v a l s .
Let U
be a non−empty open
subset
of
R .
For x, y ∈U
d e f i n e
x ∼y
i f f
[ MISSING ] .
It ’ s
e a s i l y
checked
that ∼
i s
an
equivalence
r e l a t i o n
on U
whose
equivalence
c l a s s e s
are
pairwise
d i s j o i n t
open
i n t e r v a l s
in R .
(The term \ enquote { i n t e r v a l }
here
i n c l u d e s
unbounded
i n t e r v a l s ,
i . e . ,
rays . )
Let I
be
the
se t
of
∼−c l a s s e s .
Clearly
U = S
I∈I I .
For
each I ∈I
choose
a
r a t i o n a l
qI ∈I ;
the map I →Q : I 7→qI
i s
i n j e c t i v e ,
so I
i s
countable .
Can you
f i l l
in
the
missing
condition ?" ,
" output " :
"The missing
condition
i s
" there
e x i s t s
an open
i n t e r v a l
containing
x
and y that
i s
contained
in U ."
"
"prompt" :
"The p−norm on Rn
i s
given by ∥x∥p =  Pn
k=1 |xk|p1/p . We want to show show
that ∥x∥p ≤C∥x∥q
f o r
a r b i t r a r y
1 ≤p < q . We compute
∥x∥p =
 
n
X
i=1
|xi|p
!1/p
≤


 
n
X
i=1
|xi|q
! p
q
n1−p
q


1/p
=
 
n
X
i=1
|xi|q
! 1
q
n
1
p −1
q = n1/p−1/q∥x∥q.
Which
r e s u l t
has
been
used
in
the
i n e q u a l i t y
above ?" ,
" output " :
"The
i n e q u a l i t y
used
in
the
above
c a l c u l a t i o n
i s Hölder ’ s
i n e q u a l i t y .
I t
s t a t e s
that
f o r
any p, q ≥1 such
that
1
p + 1
q = 1 , and
f o r
any
vectors
x ∈Rn , y ∈Rn ,
 
n
X
i=1
|xiyi|
!
≤
 
n
X
i=1
|xi|p
!1/p  
n
X
i=1
|yi|q
!1/q
."
23

"prompt" :
"Let f
be a
real −valued
function
continuous
on [a, b] and
d i f f e r e n t i a b l e
on (a, b) .
Suppose
that limx→a f ′(x)
e x i s t s .
Then ,
we want to
prove
that f
i s
d i f f e r e n t i a b l e
at a and f ′(a) = limx→a f ′(x) .
Using
the
d e f i n i t i o n
of
the
derivative ,
we need
to show
that
lim
h→0+
f(a + h) −f(a)
h
e x i s t s
and
i s
equal
to
lim
x→a+ f ′(x) .
Note
that
f o r
h > 0 the Mean Value Theorem
provides
a
point ch with a < ch < a + h such
that
[ MISSING ] .
Finally ,
note
that
ch →a+ as h →0+ .
Can you
f i l l
in
the
missing
equation ?" ,
" output " :
"The missing
equation
from
the Mean Value Theorem
i s :
f ′(ch) = f(a + h) −f(a)
h
."
Worst-3
"prompt" :
"We want to
compute
∞
X
n=1
2n
3n+1 .
Let ’ s
f i r s t
find
a
general
formula
f o r
the
f o l l o w i n g sum :
Sm = Pm
n=1 nrn. Notice
that
Sm −rSm = −mrm+1 +
m
X
n=1
rn = −mrm+1 + r −rm+1
1 −r
= mrm+2 −(m + 1)rm+1 + r
1 −r
.
Hence
Sm = mrm+2 −(m + 1)rm+1 + r
(1 −r)2
.
Can you
f i n i s h
the
c a l c u l a t i o n ?" ,
" output " :
"Yes ,
to
f i n i s h
the
c a l c u l a t i o n
we can
s u b s t i t u t e
the
values
of
m = ∞,
r = 2/3 into
the
formula we found
f o r
Sm :
S∞=
∞  2
3
∞+2 −(∞+ 1)  2
3
∞+1 + 2
3
 1 −2
3
2
This
s i m p l i f i e s
to
S∞=
2
9
1
9
= 2, "
"prompt" :
"Suppose V
i s
an
inner
product
space
and T ∈L(V ) . We want to
prove
that
| det(T)| = det
√
T ∗T.
By the
Polar
Decomposition
there
i s
an
isometry S ∈L(V ) such
that
T = S
√
T ∗T.
Can you
f i n i s h
the
proof ?" ,
" output " :
"Yes ,
once we have
that T = S
√
T ∗T , we can
take
the
determinant
of
both
s i d e s
to
get
det(T) = det(S) det(
√
T ∗T).
Since S
i s
an isometry ,
i t
preserves
inner
products ,
so det(S) = ±1 .
Therefore ,
| det(T)| = | det(S)|| det(
√
T ∗T)| = | det(
√
T ∗T)|.
Since
the
determinant
of
a matrix
i s
always non−negative ,
we have
that
| det(T)| = det(
√
T ∗T)"
24

"prompt" :
"F
i s
a
f i n i t e
f i e l d
of
order q . We want to
compute
the
s i z e
of
GLn(F) .
In
order
f o r
an n × n matrix
to
be
i n v e r t i b l e ,
we need
the
rows
to
be
l i n e a r l y
independent .
As you note ,
we have qn −1 ch oi ce s
f o r
the
f i r s t
row ;
now ,
there
are q
vectors
in
the
span
of
the
f i r s t
row ,
so we have qn −q
ch oi ce s
f o r
the
second row . Now,
l e t
v1, v2 be
the
f i r s t
two rows .
Then the
s e t
of
vectors
in
the
span
of
v1, v2
i s
of
the
form {c1v1 + c2v2|c1, c2 ∈F} .
This
s e t
i s
of
s i z e
q2 ,
as
we have q
choices
f o r
c1 and q
ch oi ces
f o r
c2 .
Thus ,
we have qn −q2
ch oi ces
f o r
the
third
row .
Continuing
t h i s
gi v es
the
de sir ed
formula .
Can you
s t a t e
the
formula ?" ,
" output " :
"The
s i z e
of
GLn(F)
i s
given by the
formula
(qn −1)(qn−1 −q)(qn−2 −q2) · · · (q2 −q)(q −1)."
F.3
Holes-in-Proofs (Proofs Collection B Prealgebra and Precalculus)
Examples from the Holes-in-Proofs subdataset, Proofs Collection B Prealgebra and Proofs Collection B
Precalculus ﬁles, based on [16].
Best-3
"prompt" :
"Find
the
s m a l l e s t
p o s i t i v e
i n t e g e r
that
i s
g r e a t e r
than 1 and
r e l a t i v e l y
prime
to
the
product
of
the
f i r s t
20
p o s i t i v e
i n t e g e r s .
Reminder :
two numbers
are
r e l a t i v e l y
prime
i f
t h e i r
g r e a t e s t common d i v i s o r
i s
1.
Solution : <MISSING> the
s m a l l e s t
p o s s i b l e
i n t e g e r
i s
23 .
F i l l
in
the
missing
steps . " ,
" output " :
"The
f i r s t
20
p o s i t i v e
i n t e g e r s
are
1 ,
2 ,
3 ,
4 ,
5 ,
6 ,
7 ,
8 ,
9 ,
10 ,
11 ,
12 ,
13 ,
14 ,
15 ,
16 ,
17 ,
18 ,
19 , and
20. The product
of
these
numbers
i s
2432902008176640000. To find
the
s m a l l e s t
p o s i t i v e
i n t e g e r
g r e a t e r
than 1
that
i s
r e l a t i v e l y
prime
to
t h i s
product ,
we can
s t a r t
at 2 and
incrementally
check
each number
u n t i l
we find
one
that
has a
g r e a t e s t common d i v i s o r
(GCD)
of
1
with
the
product . We can
check
2 ,
3 ,
4 ,
. . . ,
22 , and
see
that
the GCD of
each
of
these
numbers with
the
product
i s
not
1.
However ,
when we check
23 , we find
that
the GCD of
23 and 2432902008176640000
i s
1.
Therefore ,
23
i s
the
s m a l l e s t
p o s i t i v e
i n t e g e r
that
i s
g r e a t e r
than 1 and
r e l a t i v e l y
prime
to
the
product
of
the
f i r s t
20
p o s i t i v e
i n t e g e r s . " ,
"prompt" :
"Find
the number
of
s o l u t i o n s
to cos x
4 = cos x in
the
i n t e r v a l
0 < x < 24π.
Solution :
From the
equation cos x
4 = cos x, cos x −cos x
4 = 0. From the sum−to−product
formula ,
we can
write
t h i s
as
−2 sin 5x
8 sin 3x
8 = 0.
<MISSING> F i l l
in
the
missing
steps . " ,
" output " :
" Since
the
product
of
s i n e s
i s
equal
to
zero ,
e i t h e r
sin 5x
8 = 0 or sin 3x
8 = 0.
Solving
f o r
each
case :
sin 5x
8 = 0 =⇒
5x
8 = nπ, where n
i s
an
i n t e g e r .
This
g i v es
us
the
s o l u t i o n s
x = 8n
5 π. sin 3x
8 = 0 =⇒
3x
8 = nπ, where n
i s
an
i n t e g e r .
This
g i v es
us
the
s o l u t i o n s
x = 8n
3 π. Now we need
to
find
the
s o l u t i o n s
that
are
in
the
i n t e r v a l
0 < x < 24π. x = 8n
5 π
give s
s o l u t i o n s
x = 16
5 π, 24
5 π, 32
5 π, 40
5 π, 48
5 π which
are
in
the
given
i n t e r v a l .
x = 8n
3 π
gives
s o l u t i o n s
x = 8
3π, 16
3 π, 24
3 π which
are
in
the
given
i n t e r v a l .
Therefore ,
there
are
5
s o l u t i o n s
f o r
x = 8n
5 π and 3
s o l u t i o n s
f o r
x = 8n
3 π in
the
given
i n t e r v a l .
The
t o t a l
number
of
s o l u t i o n s
i s
5+3=8."
"prompt" :
"Let a, b, c be
three
vectors
such
that ∥a∥=
1
√
2, ∥b∥=
1
√
3, ∥c∥=
1
√
6,
∥a + b + c∥= 1, and
a = k(b × c)
25

f o r
some
s c a l a r
k. Find
the
angle
between b and c, in
degrees .
Solution :
Since
b × c
i s
orthogonal
to
both b and c, a · b = 0 and a · c = 0. Since ∥a + b + c∥= 1 ,
(a + b + c) · (a + b + c) = 1.
<MISSING> F i l l
in
the
missing
steps . " ,
" output " :
"Expanding
the
dot
product on the
l e f t
s i d e
g i v es :
a · a + a · b + a · c + b · a + b · b + b · c + c · a + c · b + c · c = 1. Since a · b = a · c = b · a = c · a = 0, t h i s
s i m p l i f i e s
to :
|a|2 + |b|2 + |c|2 + 2b · c = 1. We know that |a|2 = 1
2, |b|2 = 1
3, |c|2 = 1
6, and
s u b s t i t u t i n g
these
values
in
gi v es :
1
2 + 1
3 + 1
6 + 2b · c = 1 Solving
f o r
b · c g iv e s :
b · c = 1
6 −1
6 = 0 Which means the
vectors
are
orthogonal ,
hence
the
angle
between
them
i s
90
degrees ."
Worst-3
"prompt" :
"The
l i n e
described
by
 2
−1
3
!
+ t
 k
2
1
!
i s
perpendicular
to
the
l i n e
described
by
 2
−1
1
!
+ u
 2
1
2
!
and
passes
through
the
point (4, a, b). Find a + b + k. Solution :
The
d i r e c t i o n
vector
of
the
f i r s t
l i n e
i s
 k
2
1
!
, and the
d i r e c t i o n
vector
of
the
second
l i n e
i s
 2
1
2
!
.
<MISSING> Then t = −1, so a = −3 and b = 2, so a + b + k = −3 .
F i l l
in
the
missing
steps . " ,
" output " :
"The
d i r e c t i o n
vector
of
the
f i r s t
l i n e
i s
 k
2
1
!
, and the
d i r e c t i o n
vector
of
the
second
l i n e
i s
 2
1
2
!
. To find
the
point
of
i n t e r s e c t i o n ,
we can
s e t
the two
parametric
equations
equal
to
each
other :
 2
−1
3
!
+ t
 k
2
1
!
=
 2
−1
1
!
+ u
 2
1
2
!
Solving
f o r
t , u we get
t=−1,u=−3. So the
point
of
i n t e r s e c t i o n
i s
 2
−1
3
!
−
 k
2
1
!
= (4, a, b). Given
that ,
we get
a = −3 and b = 2 ,
then a + b + k = −3 . " ,
"prompt" :
"How many p o s i t i v e
i n t e g e r s
l e s s
than 100 and
d i v i s i b l e
by 3
are
a l s o
d i v i s i b l e
by 4?
Solution :
I f
we
s t a r t
counting
from
1 ,
every
third
consecutive
i n t e g e r
i s
d i v i s i b l e
by 3 and
every
fourth
consecutive
i n t e g e r
i s
d i v i s i b l e
by
4. <MISSING> so
there
are
8
multiples
of
12 between 1 and
100.
F i l l
in
the
missing
steps . " ,
" output " :
"The missing
steps
are : We need
to
find
the
multiples
of
12 ,
which
are
d i v i s i b l e
by both 3 and
4. To find
the
f i r s t
multiple
of
12
l e s s
than
100 , we divide
100 by 12 and round down to
the
nearest
integer ,
which
i s
26

8. To find
the
l a s t
multiple
of
12
l e s s
than
100 , we divide
12 by 100 and
round up to
the
nearest
integer ,
which
i s
8. The number
of
multiples
of
12
between 1 and 100
i s
the
d i f f e r e n c e
between
the
l a s t
and
f i r s t
multiple ,
which
i s
8 −8 = 0.
So
there
are
no
p o s i t i v e
i n t e g e r s
l e s s
than 100
that
are
d i v i s i b l e
by both 3 and
4." ,
"prompt" :
"The complex numbers 5 + 10i, 7 + 2i, and 11 + 3i are
three
v e r t i c e s
of
a
r e c t a n g l e .
Find
the
fourth
vertex .
Solution :
Let a = 5 + 10i, b = 7 + 2i, and c = 11 + 3i.
Let d be
the
fourth
vertex .
Note
that
c −b = 4 + i
and
a −b = −2 + 8i = 2i(4 + i) = 2i(c −b).
Thus ,
the
angle
between
the
segment
j o i n i n g
a and b and the
segment
j o i n i n g
b
and c
i s
90◦. <MISSING> F i l l
in
the
missing
steps . " ,
" output " :
" Since
the
angle
between
the
segment
j o i n i n g
a and b and the
segment
j o i n i n g
b and c
i s
90◦,
i t
f o l l o w s
that
the
segment
j o i n i n g
c and d
i s
perpendicular
to
the
segment
j o i n i n g
a and b .
Since c −b = 4 + i , we have
c −d = −(b −d) = −(−2 + 8i) = 2 −8i .
Therefore ,
d = c −(2 −8i) = 11 + 3i −(2 −8i) = 9 −5i .
So
the
fourth
vertex
i s
(9, −5i)."
F.4
Olympiad-Problem-Solving
Examples from the Olympiad-Problem-Solving subdataset based on the book [13].
Best-3
"prompt" :
"n numbers
are
written
on a
blackboard .
In
one
step ,
you may erase
any
two
of
the numbers ,
say a and b ,
and write ,
instead
( a+b) /4.
Repeating
t h i s
step n−1 times ,
there
i s
one number
l e f t .
Prove
that ,
i n i t i a l l y ,
i f
there
were
n ones on the
board ,
at
the end ,
a number which
i s
not
l e s s
than 1/n
w i l l
remain . " ,
" output " :
"This
can be proven by
induction .
Base
case :
n = 2.
I f
there
are
two
ones on the
board ,
a f t e r
one
step ,
the
remaining number
i s
(1+1)/4 = 1/2 ,
which
i s
not
l e s s
than
1/2.
Inductive
case :
Assume that
the
statement
i s
true
f o r
n = k . Now consider n = k+1.
I f
there
are k+1 ones on the
board ,
we can
erase
any two
of
them and
write
t h e i r
average ,
which
i s
not
l e s s
than 1/k+1 by
the
induction
hypothesis .
Therefore ,
we can
conclude
that
the
statement
holds
f o r
a l l
n ."
"prompt" :
"The
i n t e g e r s
1 ,
. . . ,
n are
arranged
in
any
order .
In
one
step
you may
switch
any two
neighboring
i n t e g e r s .
Prove
that
you can
never
reach
the
i n i t i a l
order
a f t e r
an odd number
of
steps . " ,
" output " :
"This
problem can be proven
using
the
concept
of
parity .
S p e c i f i c a l l y ,
the
parity
of
a permutation
i s
the number
of
i n v e r s i o n s
( i . e .
p a i r s
of
i n t e g e r s
( i ,
j )
such
that
i < j
but
the
i t h
i n t e g e r
appears
a f t e r
the
jth
i n t e g e r
in
the
permutation ) modulo
2.
I t
can be shown that
the
parity
of
a
permutation
remains
unchanged
a f t e r
any
s i n g l e
swap
of
neighboring
i n t e g e r s .
Since
the
i n i t i a l
ordering
has
zero
i n v e r s i o n s
( i . e .
i t
has
even
parity ) ,
i t
f o l l o w s
that
an odd number
of
swaps
w i l l
always
y i e l d
a permutation
with odd
parity ,
and thus
i t
cannot
be
the same as
the
i n i t i a l
ordering ."
27

"prompt" :
"Let ’ s
answer
t h i s
question
step
by
step .
Derive
d i v i s i b i l i t y
c r i t e r i a
f o r
9 and
11." ,
" output " :
" D i v i s i b i l i t y
C r i t e r i a
f o r
9:
I f
the sum of
d i g i t s
of
a number
i s
d i v i s i b l e
by 9 ,
then
the number
i s
a l s o
d i v i s i b l e
by
9.
I f
the
d i f f e r e n c e
between
the sum of
d i g i t s
in
the odd
p o s i t i o n s
and even
p o s i t i o n s
i s
d i v i s i b l e
by 9 ,
then
the number
i s
a l s o
d i v i s i b l e
by
9.
D i v i s i b i l i t y
C r i t e r i a
f o r
11:
I f
the
d i f f e r e n c e
between
the sum of
d i g i t s
in
the odd
p o s i t i o n s
and even
p o s i t i o n s
i s
d i v i s i b l e
by 11 ,
then
the number
i s
a l s o
d i v i s i b l e
by
11.
I f
the
a l t e r n a t i n g sum of
d i g i t s
( f i r s t
minus
second
plus
third
minus
fourth ,
etc . )
i s
d i v i s i b l e
by 11 ,
then
the number
i s
a l s o
d i v i s i b l e
by
11."
Worst-3
"prompt" :
"Assume an 8 × 8 chessboard
with
the
usual
c o l o r i n g .
You may repaint
a l l
squares
( a )
of
a row or
column
(b)
of
a 2 × 2 square .
The goal
i s
to
a t t a i n
j u s t
one
black
square .
Can you
reach
the
goal ?" ,
" output " :
"Yes ,
i t
i s
p o s s i b l e
to
a t t a i n
j u s t
one
black
square
on an 8 × 8
chessboard .
One way to do
t h i s
i s
to
repaint
a l l
the
squares
of
the
top−l e f t
2 × 2 square ,
which
w i l l
leave
one
black
square
in
the bottom−r i g h t
corner
of
the
chessboard .
Another way to do
t h i s
i s
to
repaint
a l l
the
squares
of
the
f i r s t
column ,
which
w i l l
leave
one
black
square
in
the bottom−r i g h t
corner
of
the
chessboard .
"prompt" :
" Start
with
the
p o s i t i v e
i n t e g e r s
1, . . . , 4n −1 .
In
one move you may r e p l a c e
any two
i n t e g e r s
by
t h e i r
d i f f e r e n c e .
Prove
that
an even
i n t e g e r
w i l l
be
l e f t
a f t e r
4n−2 steps . " ,
" output " :
"We can
prove
t h i s
by mathematical
induction .
Base
case : When n = 1 , we
have
the
i n t e g e r s
1 ,
2 ,
3 ,
4. We can
perform
the
f o l l o w i n g
moves :
(1 ,
2) →1 ,
3 ,
4;
(3 ,
4) →1 ,
3;
(1 ,
3) →2 , an even
i n t e g e r .
Inductive
case :
Assume that
the
statement
i s
true
f o r
n = k . We w i l l
show that
i t
i s
a l s o
true
f o r
n =
k+1. We have
the
i n t e g e r s
1 ,
2 ,
3 ,
. . . ,
4(k+1)−1. We can
perform
the
f o l l o w i n g
moves :
(1 ,
2) →1 ,
3 ,
4 ,
. . . ,
4(k+1)−1;
(3 ,
4) →1 ,
3 ,
. . . ,
4(k+1)−3;
. . . ;
(4k−1, 4k) →1 ,
3 ,
. . . ,
4(k+1)−5. Now we have 4k−2 i n t e g e r s
l e f t ,
1 ,
3 ,
. . . ,
4(k+1)−5, which
are
4k−2 consecutive
odd
i n t e g e r s . By the
induction
hypothesis ,
a f t e r
4k−2 steps ,
we
w i l l
be
l e f t
with an even
i n t e g e r .
Therefore ,
by mathematical
induction ,
we can
prove
that
an even
i n t e g e r
w i l l
be
l e f t
a f t e r
4n−2 steps ."
"prompt" :
"Let ’ s
answer
t h i s
question
step
by
step . .
Assume an 8 × 8 chessboard
with
the
usual
c o l o r i n g .
You may repaint
a l l
squares
( a )
of
a row or
column
(b)
of
a 2 × 2 square .
The goal
i s
to
a t t a i n
j u s t
one
black
square .
Can you
reach
the
goal ?" ,
" output " :
"Yes ,
i t
i s
p o s s i b l e
to
reach
the
goal
of
having
j u s t
one
black
square
on an 8 × 8 chessboard .
One way to do
t h i s
i s
to
repaint
a l l
of
the
squares
in
the
f i r s t
row and
f i r s t
column
black ,
and then
repaint
the
top−l e f t
square
of
the 2 × 2 square
white .
This
r e s u l t s
in
one
black
square
and
a l l
other
squares
white ."
F.5
Symbolic-Integration
Examples from our Symbolic-Integration subdataset based on [18].
28

Best-3
"prompt" :
"What
i s
the
i n t e g r a l
of
4x^7 + s i n (1 + x) " ,
" output " :
"The
i n t e g r a l
of
4x^7
i s
(4/8) x^8 + C,
where C i s
the
constant
of
i n t e g r a t i o n .
The
i n t e g r a l
of
s i n (1 + x)
i s −cos (1 + x) + C.
So the
i n t e g r a l
of
4x^7 + sin (1 + x)
i s
(4/8) x^8 −cos (1 + x) + C."
"prompt" :
"What
i s
the
i n t e g r a l
of
2048 + 32x^3" ,
" output " :
"The
a n t i d e r i v a t i v e
or
i n d e f i n i t e
i n t e g r a l
of
2048 + 32x^3 with
r esp ect
to x
i s R
(2048 + 32x^3)dx = 2048x + (32/4) x^4 + C where C i s
the
constant
of
i n t e g r a t i o n ."
"prompt" :
"What
i s
the
i n t e g r a l
of
12 + 6 cosh (x) " ,
" output " :
"The
i n t e g r a l
of
12 + 6 cosh (x)
i s
12x + 6
sinh (x) + C,
where C i s
the
constant
of
i n t e g r a t i o n ."
Worst-3
"prompt" :
"What
i s
the
i n t e g r a l
of
(4 + x) [4 + 3x ] [ e^{x∗(4+x) ^2}]" ,
" output " :
"The
i n t e g r a l
of
(4 + x) [4 + 3x ] [ e^{x∗(4+x) ^2}]
i s
not
e x p r e s s i b l e
in
terms
of
elementary
functions ."
"prompt" :
"What
i s
the
i n t e g r a l
of
xe^{−3x −cosh (x) }" ,
" output " :
"The
i n t e g r a l
of
xe^{−3x −cosh (x) } with
re spe ct
to x does
not
have a
closed −form
s o l u t i o n .
I t
must be
evaluated
numerically
or
approximated
using
numerical
i n t e g r a t i o n
methods such
as
Simpson ’ s
r u l e
or
the
t r a p e z o i d a l
r u l e ."
"prompt" :
"What
i s
the
i n t e g r a l
of
1/(2 + ln (3/2) ) " ,
" output " :
"The
i n t e g r a l
of
1/(2 + ln (3/2) )
cannot
be
expressed
in
terms
of
elementary
functions
and must be approximated
numerically
or
solved
using
s p e c i a l
functions ."
29

