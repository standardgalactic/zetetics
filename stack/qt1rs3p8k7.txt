UC Irvine
UC Irvine Electronic Theses and Dissertations
Title
Geometric Bayes
Permalink
https://escholarship.org/uc/item/1rs3p8k7
Author
Holbrook, Andrew
Publication Date
2018
 
Peer reviewed|Thesis/dissertation
eScholarship.org
Powered by the California Digital Library
University of California

UNIVERSITY OF CALIFORNIA,
IRVINE
Geometric Bayes
DISSERTATION
submitted in partial satisfaction of the requirements
for the degree of
DOCTOR OF PHILOSOPHY
in Statistics
by
Andrew J. Holbrook
Dissertation Committee:
Associate Professor Babak Shahbaba, Chair
Professor Daniel L. Gillen
Professor Hernando C. Ombao
2018

Chapter 2 c⃝2017 Taylor & Francis
Chapter 3 c⃝2018 Elsevier
Chapter 6 c⃝2017 Wiley
All other materials c⃝2018 Andrew J. Holbrook

DEDICATION
To my parents
ii

TABLE OF CONTENTS
Page
LIST OF FIGURES
vi
LIST OF TABLES
ix
LIST OF ALGORITHMS
x
ACKNOWLEDGMENTS
xi
CURRICULUM VITAE
xii
ABSTRACT OF THE DISSERTATION
xiv
1
Introduction
1
1.1
Geometry in statistics
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1
1.2
Riemannian geometry . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4
1.2.1
Smooth manifolds . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5
1.2.2
Riemannian manifolds
. . . . . . . . . . . . . . . . . . . . . . . . . .
7
1.2.3
Geodesics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
10
1.3
Summary of contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . .
12
2
Bayesian inference on general Riemannian manifolds
15
2.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
17
2.2
Motivation: learning the spectral density matrix . . . . . . . . . . . . . . . .
20
2.3
The space of positive deﬁnite matrices
. . . . . . . . . . . . . . . . . . . . .
23
2.4
Bayesian inference on general Riemannian manifolds using the geodesic La-
grangian Monte Carlo
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
26
2.4.1
Geodesic Lagrangian Monte Carlo . . . . . . . . . . . . . . . . . . . .
28
2.5
gLMC on the manifold of PD matrices . . . . . . . . . . . . . . . . . . . . .
31
2.6
Some priors on covariance matrices . . . . . . . . . . . . . . . . . . . . . . .
34
2.7
Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
37
2.7.1
Empirical validation
. . . . . . . . . . . . . . . . . . . . . . . . . . .
37
2.7.2
Learning the spectral density
. . . . . . . . . . . . . . . . . . . . . .
42
2.8
Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
45
iii

3
Diﬀerentiating the pseudo determinant
47
3.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
49
3.2
The canonical derivative . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
50
3.2.1
The matrix diﬀerential . . . . . . . . . . . . . . . . . . . . . . . . . .
60
3.3
An example from statistics . . . . . . . . . . . . . . . . . . . . . . . . . . . .
62
4
Simplifying the geodesic Monte Carlo
65
4.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
67
4.2
gMC on embedded manifolds
. . . . . . . . . . . . . . . . . . . . . . . . . .
68
4.3
An alternative derivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
70
4.4
Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
73
5
Fisher geometry and Bayesian nonparametric density estimation
75
5.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
77
5.2
The nonparametric Fisher geometry . . . . . . . . . . . . . . . . . . . . . . .
80
5.2.1
The parametric Fisher geometry . . . . . . . . . . . . . . . . . . . . .
80
5.2.2
Beyond parametric models . . . . . . . . . . . . . . . . . . . . . . . .
81
5.3
The chi-square process density prior . . . . . . . . . . . . . . . . . . . . . . .
87
5.3.1
The Karhunen-Lo`eve representation . . . . . . . . . . . . . . . . . . .
89
5.3.2
The model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
91
5.4
Inference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
92
5.4.1
Inference in the limit . . . . . . . . . . . . . . . . . . . . . . . . . . .
94
5.5
Empirical results
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
97
5.5.1
Simulated experiments . . . . . . . . . . . . . . . . . . . . . . . . . .
99
5.5.2
Experiments with real-world data . . . . . . . . . . . . . . . . . . . .
100
5.6
Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
102
6
Application: Bayesian neural decoding
104
6.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
106
6.2
Scientiﬁc background and experimental setup
. . . . . . . . . . . . . . . . .
109
6.3
Bayesian linear dimensionality reduction and extensions . . . . . . . . . . . .
111
6.3.1
Exponential family PCA . . . . . . . . . . . . . . . . . . . . . . . . .
112
6.3.2
Wavelet transform and wavelet PCA
. . . . . . . . . . . . . . . . . .
113
6.3.3
Supervised exponential PCA . . . . . . . . . . . . . . . . . . . . . . .
114
6.3.4
Modeling the loading matrix . . . . . . . . . . . . . . . . . . . . . . .
116
6.4
The supervised dual-dimensionality reduction model . . . . . . . . . . . . . .
119
6.4.1
LFP module . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
119
6.4.2
Spike train module . . . . . . . . . . . . . . . . . . . . . . . . . . . .
120
6.4.3
Sequential classiﬁcation module . . . . . . . . . . . . . . . . . . . . .
120
6.5
Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
121
6.5.1
Prediction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
121
6.5.2
Variable selection and scientiﬁc inference . . . . . . . . . . . . . . . .
124
6.6
Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
128
iv

7
Future directions: Bayesian inference on inﬁnite manifolds
130
7.1
Dimensionality reduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
132
7.2
Learning the quantum wave . . . . . . . . . . . . . . . . . . . . . . . . . . .
133
7.3
Computing on inﬁnite manifolds . . . . . . . . . . . . . . . . . . . . . . . . .
135
Bibliography
136
A Estimating prediction error for complex samples
149
A.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
151
A.2 Prediction error estimation for simple random samples
. . . . . . . . . . . .
154
A.2.1
The covariance-inﬂated estimator . . . . . . . . . . . . . . . . . . . .
155
A.2.2
Akaike’s information criterion . . . . . . . . . . . . . . . . . . . . . .
157
A.2.3
Estimating the covariance penalty . . . . . . . . . . . . . . . . . . . .
159
A.3 Prediction error estimation for complex samples . . . . . . . . . . . . . . . .
161
A.3.1
Design-based AIC . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
162
A.3.2
The Horvitz–Thompson–Efron estimator . . . . . . . . . . . . . . . .
167
A.3.3
Estimating the covariance penalty . . . . . . . . . . . . . . . . . . . .
172
A.4 Simulation study
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
173
A.5 Prediction of renal function using data from the National Health and Nutrition
Examination Survey
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
177
A.6 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
179
B Bayesian inference on general Riemannian manifolds
181
B.1 Real and complex matrix derivatives
. . . . . . . . . . . . . . . . . . . . . .
181
B.1.1
The complex reference prior . . . . . . . . . . . . . . . . . . . . . . .
183
C Simplifying the geodesic Monte Carlo
185
C.1 Projection matrix for the Stiefel manifold . . . . . . . . . . . . . . . . . . . .
185
D Fisher geometry and Bayesian nonparametric density estimation
187
D.1 Initializing the Markov chain: Newton’s method on the sphere . . . . . . . .
187
D.2 Relationship to the Cox process . . . . . . . . . . . . . . . . . . . . . . . . .
189
v

LIST OF FIGURES
Page
1.1
A Riemannian manifold. For every point q on the d-dimensional manifold Q,
there is at least one open set U ⊆Q containing q and a diﬀeomorphism φ
from U to an open set in Rd. The tangent space to Q at q may be constructed
as the span of the coordinate-wise directional derivatives ∂φ−1/∂xi. At any
point q, the Riemannian metric gq is a smooth map from TqQ × TqQ to the
real line. This map provides a notion of magnitude that is independent of
manifold parameterization (see Figure 1.2). . . . . . . . . . . . . . . . . . . .
6
1.2
Three parameterizations of the unit sphere. The top sphere is parameterized
by angles φ ∈[−π, π] and θ ∈(0, 2π]; the left sphere is embedded in 3-
dimensional Euclidean space; and the right sphere is parameterized by R2
via stereographic projection. Since it is invariant under re-parameterization,
the Hausdorﬀmeasure is a consistent background measure for Riemannian
manifolds. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
9
1.3
Geodesic ﬂow on S2. Given initial position-velocity pair (q, v), the geodesic
ﬂow advances along the path minimizing the energy functional given in For-
mula 1.2. On the sphere, the geodesic paths are the great circles. . . . . . . .
11
2.1
Median condition number by dimension and prior speciﬁcation. Box plots
describe distributions of 100 median condition numbers for each dimension
and prior. Each point is the median from 200 posterior samples based on
independent data and using gLMC. The reference prior is designed to yield
smaller condition numbers than Jeﬀreys prior and hence better asymptotics
[182]. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
35
2.2
These ﬁgures provide empirical validation for the well-posedness of gLMC for
PD matrices. On the top-left is a quantile-quantile plot comparing the gLMC
(for Hermitian PD matrices) posterior sample with that of the closed-form
posterior for the complex Gaussian inverse-Wishart model.
Both real and
imaginary elements are included, and points are jittered for visibility. On
the top-right are posterior samples of ‘global’ matrix summaries pertaining
both to gLMC (for symmetric PD matrices), the closed-form ‘exact’ solution,
and the ‘indirect’ log-Cholesky parameterization. These summaries are the
eﬀective variance and the eﬀective dependence, built oﬀthe covariance matrix
and the correlation matrix, respectively. On the bottom are posterior density
plots of the same matrix summaries.
. . . . . . . . . . . . . . . . . . . . . .
38
vi

2.3
Two 4-dimensional VAR1 time series and credible intervals for their 6 corre-
sponding coherences measured at 20-40 Hz. The top row belongs to a block
VAR1 process characterized by two independent 2-dimensional VAR1 time
series; the bottom row belongs to a full VAR1 process. The left column shows
the ﬁrst 100 samples of both time series, each of which totals 5,000 samples
in length. The right column shows credible intervals from posteriors obtained
using the inverse-Wishart and reference priors. . . . . . . . . . . . . . . . . .
39
2.4
A 4-dimensional LFP signal with credible intervals for 6 coherences measured
at 20-40 Hz (left) and 40-160 Hz (right). First 200 samples are shown for
ease of visualization; the multi-dimensional time series totals 4,000 samples in
length. Coherence proﬁles are remarkably similar between the two frequency
bands considered. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
40
5.1
Each plot shows 100 posterior draws from the χ2-process density sampler.
1,000 data samples were drawn from a diﬀerent beta distribution for each
plot. The generating pdf is given in red, and the red hash marks describe the
actual data produced. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
98
5.2
The contours (black) of the posterior median from 1,000 draws of the χ2-
process density sampler. Each posterior is conditioned on 1,000 data points
(red). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
99
5.3
Coal mining disasters data. The left ﬁgure shows 100 posterior draws from
the χ2-process density model (gray) over 191 vertical lines (red) marking the
precise date of each disaster. The right ﬁgure shows the pointwise median
(black) for the same sample as well as pointwise quantile bands (blue). Note
how the undulations exhibited by individual draws does not appear in the
quantile bands.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
100
5.4
Hutchings’ bramble canes data. The ﬁrst ﬁgure depicts the 823 bramble canes
(red), a heatmap of the pointwise posterior mean (black is low, white is high),
and a single contour at density 0.3 (blue) including all but a few points. The
second ﬁgure shows 823 draws from the χ2-process density posterior predictive
distribution, obtained using a rejection sampling scheme. . . . . . . . . . . .
101
6.1
Trial-wise spike counts for 14 of the 52 neurons recorded over the course of the
session. Counts range from 0 to 24, with a median of 1 and a third quartile
of 2. Individual neurons vary greatly in spike proﬁle: the top neuron hardly
ﬁres, while the third from the top averages 5 spikes a trial. In-sequence trials
are colored blue, out-of-sequence trials are colored orange. The ﬁfth neuron
from the bottom appears to ﬁre more during out-of-sequence trials, and the
third neuron from the bottom never ﬁres during an out-of-sequence trial. . .
109
vii

6.2
Two examples of LFP time series, each color pertains to one of twelve channels.
The left and right plots come from trials 16 and 17, respectively. In trial 16, the
correct odor was presented, and in trial 17 an incorrect odor was presented. In
both plots, the time series seem to belong to two separate groups by channel:
one of lower amplitude and one of higher amplitude. Both amplitude groups
show interesting low frequency patterns. Indeed, the in-sequence signals are
far from stationary. It is for these reasons a wavelet transform is used: it
accounts for both changes in scale and shifts in time.
. . . . . . . . . . . . .
110
6.3
95% credible intervals for the elements of association vector U S ΛS βS (Equa-
tion (6.5.2.1)) corresponding to each of the 52 neurons modeled. Orange are
point-wise statistically signiﬁcant; blue are not. The fact that few neurons
are registered signiﬁcant and that one or two have higher orders of magni-
tude suggests that low- dimensional representations should be suﬃcient for
prediction. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
124
6.4
95% credible intervals for the elements of association vector U L ΛL βL (Equa-
tion (6.5.2.1)) corresponding to each of the 64 wavelet coeﬃcients modeled.
Orange are point-wise statistically signiﬁcant; blue are not. Vertical lines di-
vide coeﬃcients into scale groups moving from coarse to ﬁne from left to right.
Within scales, coeﬃcients move from early time series to late from left to right.125
6.5
The posterior trace plots of the absolute value of logistic regression parame-
ters associated with the two dimensional latent representations zS
i , zL
i of the
spike and LFP data. Non-zero distributions suggest signiﬁcant association
between low-dimensional representations and outcome, but to know the di-
rection of this association one must view the low-dimensional representations
themselves. See Figure (6.6). . . . . . . . . . . . . . . . . . . . . . . . . . . .
126
6.6
One hundred draws from posterior distribution of low dimensional representa-
tions zS
i of the spike data. The horizontal axis is the ﬁrst principal component,
the vertical is the second. Blue signiﬁes in-sequence, and orange signiﬁes out-
of-sequence trials. Figure divided into plots stratiﬁed by odor presented.
. .
127
7.1
Left, 100 posterior draws from my manifold nonparametric density model
(gray) over 191 vertical lines (red) marking the precise date of each disaster
(Coal mining disasters data [92]). Right, 12 local ﬁeld potentials recorded in
rat hippocampus (Section 7.1). This functional data is ripe for dimensionality
reduction via the inﬁnite Stiefel manifold.
. . . . . . . . . . . . . . . . . . .
132
viii

LIST OF TABLES
Page
2.1
Priors for Σ and their densities up to proportionality. The ﬁrst two priors
are proper, i.e. comprise well-deﬁned probability distributions, the last three
are not. Σ is symmetric and Hermitian PD in the left and right columns,
respectively. Note how the Wishart, inverse-Wishart, and Jeﬀreys priors share
similar patterns moving from real to complex numbers. . . . . . . . . . . . .
36
6.1
Predictive ﬁt: 10-fold CV results
. . . . . . . . . . . . . . . . . . . . . . . .
121
ix

LIST OF ALGORITHMS
Page
1
Geodesic Lagrangian Monte Carlo . . . . . . . . . . . . . . . . . . . . . . . .
29
2
gLMC for symmetric PD matrices . . . . . . . . . . . . . . . . . . . . . . . .
31
3
gLMC for Hermitian PD matrices . . . . . . . . . . . . . . . . . . . . . . . .
35
4
Embedding geodesic Monte Carlo with non-trivial mass matrix . . . . . . . .
73
5
A single iteration of Newton’s method on the sphere . . . . . . . . . . . . . .
189
x

ACKNOWLEDGMENTS
I would like to thank my wife, Shirley, for making me a happy man indeed. You are my best
friend and my perfect complement.
I thank my parents for their constant encouragement and support.
I thank my brother
and sister for putting up with me and making me who I am. I thank Professor Shahbaba
for giving me freedom to explore my interests and encouraging me to be creative. I thank
Professor Gillen for providing me the opportunity to work at UCI MIND and facilitating my
scientiﬁc collaborations. I thank Professor Ombao for believing in me and bringing me into
the department. I thank Alex Mramor for being my teacher and my friend. I thank Brian
Vegetabile for getting me through ﬁrst year. I thank Eric Nalisnick for being my ML friend.
We will write that paper some day.
I would like to thank Dr. Alexander Vandenberg-Rodes for edifying conversation and timely
collaborations. Thank you, Alexander, for all of the help. I thank Dr. Corinne S. Craw-
ford for teaching me the beauty of Cicero. I thank Professors Shiwei Lan, Jeﬀrey Streets,
Norbert Fortin, Michael Yassa, Thomas Lumley, and Nick Tustison for collaborations that
beneﬁted me entirely. In particular, Chapter 2 of this dissertation was a collaboration with
Dr. Vandenberg-Rodes and Professors Lan and Shahbaba; Chapter 5 was a collaborative
eﬀort with Professors Lan, Shahbaba, and Streets; and Chapter 6 was a collaboration with
Dr. Vandenberg-Rodes, and Professors Shahbaba and Fortin. The appendix includes the
paper Estimating prediction error for complex samples, which was written in collaboration
with Professors Thomas Lumley and Daniel Gillen. This paper is not included within the
dissertation proper as it falls considerably outside the geometric scope.
My funding for this dissertation came from the NIH grant [T32 AG000096], the UC Irvine
Graduate Dean’s Dissertation Fellowship award, and the NSF grant [DMS-1622490].
I thank the publishers of my work who have given me permission to incorporate that work
into my dissertation.
Speciﬁcally, Chapter 2 of this dissertation was adapted from the
paper Geodesic Lagrangian Monte Carlo over the space of positive deﬁnite matrices: with
application to Bayesian spectral density estimation https://doi.org/10.1080/00949655.
2017.1416470, published in the Journal of Statistical Computation and Simulation ( c⃝2018
Taylor & Francis), Chapter 3, Diﬀerentiating the pseudo determinant https://doi.org/10.
1016/j.laa.2018.03.018, was published in the journal Linear Algebra and its Applications
( c⃝2018 Elsevier), and Chapter 6 was adapted from the paper A Bayesian supervised dual-
dimensionality reduction model for simultaneous decoding of LFP and spike train signals
https://doi.org/10.1002/sta4.137, published in the journal Stat ( c⃝2017 Wiley).
xi

CURRICULUM VITAE
Andrew J. Holbrook
EDUCATION
Doctor of Philosophy in Statistics
2018
University of California, Irvine
Irvine, CA
Master of Science in Statistics
2015
University of California, Irvine
Irvine, CA
Bachelor of Arts in German and Classical Languages
2009
University of California, Berkeley
Berkeley, CA
HONORS
Carl W. Cotman Young Investigator Award
2018
UC Irvine Institute for Memory Impairments and Neurological Disorders
Irvine, CA
UC Irvine Graduate Dean’s Dissertation Fellowship Award
2017-2018
University of California, Irvine
Irvine, CA
UCI MIND Aging Fellowship
2015-2017
UC Irvine Institute for Memory Impairments and Neurological Disorders
Irvine, CA
Robert L. Newcomb Memorial Endowed Graduate Student Award
2014
Department of Statistics, University of California, Irvine
Irvine, CA
RESEARCH EXPERIENCE
Graduate Research Assistant
2015–2017
UC Irvine Alzheimers Disease Research Center
Irvine, CA
Graduate Research Assistant
2014
UC Irvine Center for Statistical Consulting
Irvine, CA
Trainee
2013
Summer Institute for Training in Biostatistics
Raleigh, NC
xii

REFEREED JOURNAL PUBLICATIONS
Diﬀerentiating the pseudo determinant
2018
Linear Algebra and its Applications
Geodesic Lagrangian Monte Carlo over the space of
positive deﬁnite matrices: with application to Bayesian
spectral density estimation
2018
Journal of Statistical Computation and Simulation
A Bayesian supervised dual-dimensionality reduction
model for simultaneous decoding of LFP and spike train
signals
2017
Stat
Attitudes toward potential participant registries
2017
Journal of Alzheimer’s Disease
xiii

ABSTRACT OF THE DISSERTATION
Geometric Bayes
By
Andrew J. Holbrook
Doctor of Philosophy in Statistics
University of California, Irvine, 2018
Associate Professor Babak Shahbaba, Chair
This dissertation is an investigation into the intersections between diﬀerential geometry and
Bayesian analysis. The former is the mathematical discipline that underlies our understand-
ing of the spatial structure of the universe; the latter is the uniﬁed framework for statistical
inference built upon the language of probability and the elegant Bayes’ theorem. Here, the
two disciplines are combined with the hope that a synergy might emerge and facilitate the
useful application of Bayesian inference to real-world science. In particular, dynamic and
high-dimensional neural data provides a challenging litmus test for the methods developed
herein.
A major component of this work is the development and application of probabilistic models
deﬁned over smooth manifolds: dependencies between time series are modeled using the
manifold of Hermitian positive deﬁnite matrices; probability density functions are modeled
using the inﬁnite sphere; and high-dimensional data are modeled using the Stiefel manifold
of orthonormal matrices. Whereas formulating a manifold-based model is not diﬃcult—in a
certain sense, the geometry occurs a priori in each of the cases considered—the non-trivial
geometry presents computational challenges for model-based inference. Hence, this thesis
contributes two new algorithms for Bayesian inference on Riemannian manifolds. The ﬁrst is
an algorithm for inference over general Riemannian manifolds and is applied to inference on
xiv

Hermitian positive deﬁnite matrices. The second is an algorithm for inference over manifolds
that are embedded in Euclidean space and is applied to inference on the sphere and Stiefel
manifolds.
This dissertation is ordered as follows. In Chapter 1, the general setting is introduced along
with the rudiments of Riemannian geometry. In Chapter 2, the geodesic Lagrangian Monte
Carlo algorithm is presented and used for Bayesian inference over the space of Hermitian
positive deﬁnite matrices to learn the spectral densities of multivariate time series arising
from local ﬁeld potentials in a rodent brain. In Chapter 4, an alternative, conceptually
simpler version of the geodesic Monte Carlo is developed, but the new algorithm requires
diﬀerentiating the pseudo determinant, the derivative of which is derived in Chapter 3.
In Chapter 5, the geometry of the inﬁnite-dimensional sphere is leveraged for Bayesian
nonparametric density estimation. In Chapter 6, high-dimensional spike trains and local
ﬁeld potentials in a rodent brain are used to predict environmental stimuli. This Bayesian
‘neural decoding’ is facilitated by both geometric and non-geometric models. Chapter 7
charts the frontiers of Bayesian inference on inﬁnite manifolds.
xv

Chapter 1
Introduction
1.1
Geometry in statistics
It has somehow become the case that humans are capable of spatial reasoning. Our spa-
tial faculties aid us as we explore and manipulate our environment. These faculties help
us organize and dichotomize the phenomena we encounter and, somehow, deduce complex
relationships between these objects. Human induction, however it is accomplished, aids us as
we project power over nature and ourselves. Given this very primal usefulness, it is not sur-
prising that geometry—the mathematical formalization of our spatial intuition—has played
an important role in the development of statistics, the mathematical science of inference,
whatever that may be.
Statisticians often assume that the observed data lie in a certain space. A classic example
is the use of Euclidean geometry in linear regression [151]. Here, the association between
an outcome variable and explanatory variables is modelled by some line-of-best-ﬁt. When
this line is found by minimizing the squared-error between the observed outcomes and ﬁtted
1

values, the solution is equivalent to projecting the vector of outcomes onto the linear subspace
spanned by the covariate vectors.
Indeed, although data may be high-dimensional and complex in nature, it is often reason-
able to assume that data points concentrate on some low-dimensional space: Euclidean
geometry is also used in linear dimensionality reduction (e.g., factor analysis, principal com-
ponent analysis [140, 103], canonical correlation analysis [98], etc.). Here, the observed,
high-dimensional data is assumed to have low-dimensional structure that is projected onto
higher dimensions by some linear transformation. More on linear dimensionality reduction
and its Bayesian realizations may be found in Chapter 6.
But there is no reason a priori to assume that the data belongs to a linear subspace of the
observed support. Manifold learning [56, 172, 14] is a large and active body of research
based on what is called the ‘manifold hypothesis’ [133, 58], the idea that high-dimensional
data concentrates on low-dimensional, smooth subspaces of the data space called manifolds.
Spheres and tori are manifolds, but so are amoebae and gently rolled-up pieces of paper.
Thus, manifold learning requires less stringent assumptions than linear dimensionality re-
duction and is therefore more challenging. Spectral methods based on the weighted graph
Laplacian are prime examples of this approach [13]. Although the methodology presented
in this dissertation leverages manifold (i.e., non-Euclidean) structure, it does not belong to
the broader class of manifold learning for reasons made explicit in the following paragraphs.
One may distinguish between the data space and the model space.
The previous para-
graphs feature methods (linear dimensionality reduction, manifold learning) that make as-
sumptions about the geometry of the data space (although the example of linear regression
should suggest how this dichotomy might fall apart). On the other hand, likelihood based
methodologies—be they frequentist or Bayesian—provide geometry further inroads to statis-
tics. The Fisher information [60, 126] is the expected value of the negative log-likelihood
Hessian with respect to the data generating distribution speciﬁed by the likelihood itself.
2

The resulting positive deﬁnite matrix has uses in many areas of statistics, but it may also
be thought of as an inner product that induces a non-Euclidean geometry on the param-
eter space and thus the model space. That is, although the parameter space may be the
Euclidean upper-half plane (as is the case for the univariate Gaussian distribution), the ge-
ometry induced by the Fisher information matrix is no longer Euclidean. In fact, for the
Gaussian distribution, the Fisher geometry is equivalent to the hyperbolic half plane model.
The study of the Fisher geometry is called information geometry [4] and has been studied
extensively. Chapter 5 of this dissertation discusses the Fisher geometry in depth and relates
this geometry to the problem of nonparametric density estimation.
But this dissertation takes a diﬀerent approach to model geometry. Whereas information
geometry begins with Euclidean models and renders them non-Euclidean, I consider models
for which the parameter space is a priori a non-Euclidean manifold. Covariance matrix mod-
eling is performed over the manifold of positive deﬁnite matrices, nonparametric probability
density functions are naturally identiﬁed with points on the inﬁnite sphere, and linear sub-
spaces (over which data vary) are modeled on the Stiefel manifold of orthonormal matrices.
I argue that, in each of these cases, the natural model geometry provides fresh perspective
on the modeling task.
The geometric approach brings computational challenges, so two new algorithms are de-
veloped in this dissertation. The ﬁrst, geodesic Lagrangian Monte Carlo (Chapter 2), is
a Hamiltonian Monte Carlo algorithm for computing posterior distributions deﬁned on in-
trinsically deﬁned Riemannian manifolds (see below). In Chapter 4, a new derivation for
geodesic Monte Carlo on embedded manifolds is presented. This new algorithm extends
its predecessor and is conceptually simpler. Embedding geodesic Monte Carlo is applied to
density estimation (Chapter 5) and dimensionality reduction (Chapter 6).
The chief contents of this dissertation are as follows. In Chapter 2, Bayesian inference is
performed over the Riemannian manifold of positive deﬁnite matrices using geodesic La-
3

grangian Monte Carlo. The methodology is used for Bayesian spectral density estimation
and is applied to the analysis of neural time series. In Chapter 3, the derivative of the pseudo
determinant is derived and used to solve the maximum likelihood problem for the degen-
erate normal distribution. This derivative is also used in Chapter 4. In this chapter, the
embedding geodesic Monte Carlo algorithm of [25] is re-derived and extended. In Chapter 5,
a model is proposed for Bayesian density estimation using the inﬁnite-dimensional sphere as
model space. Finally, in Chapter 6, Bayesian models for linear dimensionality reduction are
developed for which the model space is the Stiefel manifold, i.e. the space of orthonormal
matrices of a given dimension. These models are used to decode local ﬁeld potentials and
high-dimensional neural spike train data generated in a rat hippocampus.
The proposed model geometric approach intersects with the data geometric and information
geometric approaches discussed above.
In Chapter 6, the orthonormal matrices selected
by Bayesian inference actually correspond to low-dimensional linear subspaces of the data
space. This chapter thus serves to undercut the data space, model space dichotomy. Both
Chapters 2 and 5 touch on the Fisher geometry: the Riemannian structure over the manifold
of positive deﬁnite matrices used in Chapter 2 is equivalent to the geometry induced by the
Fisher metric on the same manifold; in Chapter 5, the geometry of the inﬁnite dimensional
sphere is shown to be equivalent to a nonparametric Fisher geometry.
Next, a very brief introduction to Riemannian geometry is given with the hopes that it might
prove useful in the following chapters.
1.2
Riemannian geometry
This section contains a cursory introduction to smooth manifolds and Riemannian geom-
etry. For thorough introductions to smooth manifolds, see [114] and [159]. For a focused
4

introduction to Riemannian manifolds, see [115]. For advanced treatments of Riemannian
geometry, see [84] and [45].
1.2.1
Smooth manifolds
A d-dimensional manifold Q is a collection of points satisfying, among other things, that for
each point q ∈Q, there exists an open set U ⊆Q and a smooth, invertible map φ : U →O ⊆
Rd, the inverse of which is also smooth. Taking directional derivatives of φ−1 with respect
to the vectors
∂
∂xi, i = 1, . . . , d in Rd, one may construct a tangent space to Q at q, denoted
TqQ, as the linear span of vectors ∂φ−1
∂xi . In the same way that a tangent line to a curve is a
copy of R joined to the curve at a single point, the space TqQ is a copy of Rd joining Q at
point q. A point v ∈TqQ is called a tangent vector. Figure 1.1 illustrates the general setup.
Example 1.1. Rd is a d-dimensional manifold. At any point x ∈Rd, the tangent space
TxRd ∼= Rd, i.e., the tangent space is just another copy of Euclidean space.
Example 1.2. Chapter 2 features the d2-dimensional manifold of d × d Hermitian positive
deﬁnite matrices S+
d (C). This manifold is a subspace of the vector space Sd(C) of Hermitian
matrices.
Any matrix V ∈Sd(C) satisﬁes V H = V , where (·)H denotes the conjugate
transpose. As a result, the eigenvalues of any Hermitian matrix are real-valued. S+
d (C)
is the set of Hermitian matrices with positive eigenvalues. The tangent space TΣS+
d (C) to
the space of Hermitian positive deﬁnite matrices at any point Σ ∈S+
d (C) is the space of
Hermitian matrices Sd(C).
Example 1.3. The collection of d × k matrices with orthonormal columns, denoted Od,k, is
known as the real Stiefel manifold. This manifold appears in Chapter 6 of this dissertation.
It is a compact manifold of dimension dk −k(k −1)/2. At any point U ∈Od,k, the tangent
5

q
TqQ ∼= span{ ∂φ−1
∂xi }
Q
φ(q)
xd
x2, . . . , xd−1
x1
φ(U)
U
φ
φ−1
R
0
×
gq
Figure 1.1: A Riemannian manifold. For every point q on the d-dimensional manifold Q,
there is at least one open set U ⊆Q containing q and a diﬀeomorphism φ from U to an open
set in Rd. The tangent space to Q at q may be constructed as the span of the coordinate-
wise directional derivatives ∂φ−1/∂xi. At any point q, the Riemannian metric gq is a smooth
map from TqQ × TqQ to the real line. This map provides a notion of magnitude that is
independent of manifold parameterization (see Figure 1.2).
6

space TUOd,k consists of all d × k matrices V satisfying
V TU + U TV = 0 .
Example 1.4. A special case of the Stiefel manifold appears in Chapter 5. By setting k = 1,
we get the d−1-dimensional sphere Sd−1 ∼= Od,1. In this case, the tangent space is the vector
space of d-dimensional vectors v satisfying uTv = 0 for u ∈Sd−1.
1.2.2
Riemannian manifolds
Whereas the distance between two points x1, x2 ∈Rd is easily calculated
d(x1, x2) =
p
(x1 −x2)T(x1 −x2) ,
the distance between two points on a non-Euclidean manifold is not so easily obtainable. To
obtain an intrinsic notion of distance, one ﬁrst requires a Riemannian metric. A Riemannian
manifold is a smooth manifold equipped with a Riemannian metric. Let TQ = F
q TqQ
denote the tangent bundle of the manifold Q, where F denotes the disjoint union of sets.
Then a Riemannian metric g is a smooth function over the product space TQ × TQ. For
any tangent space TqQ, a Riemannian metric takes a pair of vectors as input and returns a
real number:
gq(·, ·) : TqQ × TqQ →R .
A Riemannian metric is also linear in both vector inputs, as well as symmetric. Finally, it
is positive deﬁnite, i.e.,
gq(v, v) ≥0 ,
and
gq(v, v) = 0 ⇐⇒v = 0
7

for any q ∈Q and v ∈TqQ. Any smooth manifold supports uncountably many Riemannian
metrics, but the following examples are metrics that are used (explicitly or implicitly) in this
dissertation.
Example 1.5. For Rd, the standard inner product g(x1, x2) = xT
1 x2 is a Riemannian metric.
More generally, for the vector space of d×k matrices, the standard, Euclidean inner product
is g(V1, V2) = tr(V T
1 V2). It is easy to check that this is a Riemannian metric as well.
Example 1.6. For the manifold of Hermitian positive deﬁnite matrices S+(C), the so-called
‘canonical metric’ evaluated at two Hermitian matrices V1, V2 ∈TΣS+(C) returns
gΣ(V1, V2) = tr
 V1Σ−1V2Σ−1
.
The reader may check that gΣ(·, ·) satisﬁes the requirements of a Riemannian metric.
Example 1.7. When viewed as as a manifold embedded in the Euclidean space of d × k
matrices, the Stiefel manifold inherits the Euclidean inner product
gU(V1, V2) = tr(V T
1 V2) .
(1.1)
As noted above, gU is just one of an inﬁnite collection of candidate metrics. Another option
is the ‘canonical metric’ [51]
gC
U (V1, V2) = tr
 V T
1 (I −1
2UU T)V2

,
but this metric is not used in the following.
In coordinates (x1, . . . , xd), a Riemannian metric is a quadratic form gq(v1, v2) = vT
1 G(q)v2
for a positive deﬁnite matrix G(q) called the metric tensor with elements given by
G(q)ij = gq( ∂
∂xi, ∂
∂xj ) .
8

N
ˆP
S
P
P
φ
✓
x
y
z
P
Figure 1.2: Three parameterizations of the unit sphere. The top sphere is parameterized by
angles φ ∈[−π, π] and θ ∈(0, 2π]; the left sphere is embedded in 3-dimensional Euclidean
space; and the right sphere is parameterized by R2 via stereographic projection. Since it
is invariant under re-parameterization, the Hausdorﬀmeasure is a consistent background
measure for Riemannian manifolds.
In Chapters 2 and 4, the metric tensor appears as the covariance matrix of the Gaussian
momentum variable of Riemannian HMC and geodesic Lagrangian Monte Carlo.
The metric tensor may be used to construct a base measure on non-Euclidean manifolds that
is invariant under reparameterization. This is the Hausdorﬀmeasure Hd and may be thought
of as a generalization of the Lebesgue measure λd on d-dimensional Euclidean spaces:
Hd(dq) =
p
|G(q)|λd(dq) .
The Hausdorﬀmeasure is useful for deﬁning probabilities on Riemannian manifolds be-
cause there are often many diﬀerent parameterizations to choose from (see Figure 1.2). The
Hausdorﬀmeasure is useful in geodesic Lagrangian Monte Carlo (on intrinsically deﬁned
9

Riemannian manifolds), it is shown in Chapter 4 that it is unnecessary for geodesic Monte
Carlo on manifolds embedded in Euclidean space.
1.2.3
Geodesics
Riemannian metrics are used to construct a notion of distance between points. This con-
struction is based on a special kind of curve called a geodesic. A curve in Q is deﬁned as a
smooth map γ : [a, b] →Q. Given a Riemannian metric g, the length of a curve γ is given
by
L(γ) =
Z b
a
q
gγ(t)(˙γ(t), ˙γ(t)) dt .
Here, ˙γ(t) is the derivative of the curve with respect to t evaluated at time t and belongs
to the tangent space Tγ(t)Q. The Riemannian distance between two points is the inﬁmum
over all piecewise smooth curves connecting the two points. A geodesic is a locally distance
minimizing curve, i.e., it is obtained by minimizing the following energy functional:
E(γ) = 1
2
Z b
a
gγ(t)(˙γ(t), ˙γ(t)) dt .
(1.2)
In Chapter 5 (Lemma 5.3), this fact is used to derive the geodesic equations on the inﬁnite
sphere. Given a point q ∈Q and a vector v ∈TqQ, there exists a unique geodesic γ : [a, b] →
Q satisfying γ(0) = q and ˙γ(0) = v. For some particularly nice manifolds, the formulas are
known for the unique geodesic ﬂow given starting position q0 and velocity v0.
Example 1.8. When the space of Hermitian positive deﬁnite matrices S+
d (C) is equipped
with the Riemannian metric from Example 1.6, the geodesics (arising from the Levi-Civita
connection [115]) are available in closed-form. For Hermitian positive deﬁnite starting point
10

N
q
v
TqS2
Figure 1.3: Geodesic ﬂow on S2. Given initial position-velocity pair (q, v), the geodesic ﬂow
advances along the path minimizing the energy functional given in Formula 1.2. On the
sphere, the geodesic paths are the great circles.
Σ(0) and Hermitian initial velocity V (0), the geodesic ﬂow results in
Σ(t) = expΣ tV (0) = Σ(0)1/2 exp

tΣ(0)−1/2V (0)Σ(0)−1/2
Σ(0)1/2
at time t. V (t) is obtained by diﬀerentiating the above formula with respect to t. For more
on the derivation of this formula, see Chapter 2, Section 3.
Example 1.9. When the d −1-sphere is equipped with the Euclidean metric, the resulting
geodesic ﬂow (see Figure 1.3) is given by


q(t)
v(t)

=


1
0
0
∥v(0)∥2




cos(∥v(0)∥2h) + sin(∥v(0)∥2h)
−sin(∥v(0)∥2h) + cos(∥v(0)∥2h)




1
0
0
∥v(0)∥−1
2




q(0)
v(0)

,
where ∥· ∥2 denotes the Euclidean norm. This formula is used in the inference algorithm
used in Chapter 5.
11

Geodesics appear in a class of eﬃcient MCMC algorithms for Bayesian inference on manifolds
where they are used to traverse the non-Euclidean state space [25]. I extend this class of
algorithms in Chapters 2 and 4. A simple reason for the use of geodesics is that they restrict
movement to the manifold of interest and are reversible.
A subtler reason for their use
is that they are symplectic transformations, i.e. they preserve the symplectic structure on
the phase space. For more on symplectic integration methods see [118, 78]. In the context
of Metropolis-Hastings, a particularly helpful fact about symplectic transformations is that
they are volume preserving.
1.3
Summary of contributions
This dissertation chieﬂy contributes to Bayesian computing and modeling on Riemannian
manifolds.
A new MCMC algorithm for general, intrinsically deﬁned Riemannian mani-
folds is invented, and an existing MCMC algorithm for embedded Riemannian manifolds is
simpliﬁed and expanded. Results in linear algebra are obtained to facilitate the latter devel-
opments (incidentally, a new maximum likelihood equation for the covariance of a degenerate
Gaussian distribution is derived). Resulting from these advancements are contributions to
practical Bayesian modeling of functional and multimodal brain data. Finally, Bayesian non-
parametric density estimation is related to the Fisher geometry and reframed as a problem of
Bayesian inference on the inﬁnite dimensional sphere. Major contributions are listed below.
The contributions of Chapter 2 are:
• geodesic Lagrangian Monte Carlo, an unprecedented methodology for Bayesian infer-
ence on general Riemannian manifolds, is proposed;
• the general methodology is illustrated on the spaces of symmetric and Hermitian pos-
itive deﬁnite matrices;
12

• the algorithm is applied to Bayesian spectral density estimation using a number of
diﬀerent prior speciﬁcations;
• the geometric approach allows for priors to be deﬁned directly on positive deﬁnite
matrices and thus circumvents diﬃcult-to-interpret parameterizations.
It is often more natural to consider distributions deﬁned on Riemannian manifolds that have
been embedded in Euclidean space. Together, Chapters 3 and 4 develop a general method-
ology for Bayesian inference on such embedded manifolds. Chapter 3 lays the groundwork
for Chapter 4 and contains the following contributions:
• the canonical derivative of the pseudo determinant is carefully deﬁned so as to account
for the function’s discontinuous nature;
• the form of this canonical derivative is derived using multiple approaches;
• the canonical derivative of the pseudo determinant is used to obtain new maximum
likelihood equations for the low-rank covariance matrix of the degenerate Gaussian
distribution.
Based on the results of Chapter 3, Chapter 4 contains the following contributions:
• clarifying simpliﬁcations are made to the existing embedding geodesic Monte Carlo
algorithm so that diﬃcult concepts from Riemannian geometry are no longer required;
• the existing algorithm is shown to be a special case of a larger class of algorithms with
non-trivial mass matrices.
The embedding geodesic Monte Carlo algorithm of Chapter 4 is used in Chapters 5 and 6.
Chapter 5 contains the following contributions:
13

• a nonparametric generalization of the Fisher geometry is reviewed and its relationship
to the inﬁnite-dimensional (L2) sphere, the space of square-root density functions, is
shown;
• the geodesics on the L2 sphere are derived and used to formalize the relationship
between Riemannian HMC and embedding geodesic Monte Carlo on the inﬁnite-
dimensional sphere;
• focusing on Bayesian nonparametric density estimation, the practical beneﬁts to mod-
eling the square-root density function are demonstrated; the resulting χ2-process den-
sity prior performs well for a variety of problems and is eﬃciently computed using
embedding geodesic Monte Carlo on the sphere.
In Chapter 6, a Bayesian hierarchical model for neural decoding is proposed. This chapter’s
contributions are:
• the supervised dual-dimensionality reduction model is presented;
• the model’s loading matrices are deﬁned using Euclidean and non-Euclidean parame-
terizations;
• the model is applied to neural decoding of high dimensional spike count and local ﬁeld
potential data obtained during an experiment with rodent subject;
• the model is used for feature selection, visualization, prediction, and scientiﬁc inference.
14

Chapter 2
Bayesian inference on general
Riemannian manifolds
15

Chapter Summary
We present geodesic Lagrangian Monte Carlo, an extension of Hamiltonian Monte Carlo
for sampling from posterior distributions deﬁned on general Riemannian manifolds. We
apply this new algorithm to Bayesian inference on symmetric or Hermitian positive
deﬁnite matrices. To do so, we exploit the Riemannian structure induced by Cartan’s
canonical metric. The geodesics that correspond to this metric are available in closed-
form and—within the context of Lagrangian Monte Carlo—provide a principled way to
travel around the space of positive deﬁnite matrices. Our method improves Bayesian
inference on such matrices by allowing for a broad range of priors, so we are not limited
to conjugate priors only. In the context of spectral density estimation, we use the (non-
conjugate) complex reference prior as an example modeling option made available by
the algorithm. Results based on simulated and real-world multivariate time series are
presented in this context, and future directions are outlined.
16

2.1
Introduction
In this chapter1, we introduce geodesic Lagrangian Monte Carlo (gLMC), a methodology
for Bayesian inference on a broad class of Riemannian manifolds. We illustrate this general
methodology using the space of positive deﬁnite (PD) matrices as a concrete example. The
resulting algorithms allow for direct inference on the space of PD matrices and are thus the
ﬁrst of their kind. As a result, gLMC facilitates better prior elicitation of covariance matrices
by negating the need for conjugate priors and avoiding diﬃcult-to-interpret transformations
on variables of interest.
In statistics, positive deﬁnite matrices primarily appear as covariance matrices parameteriz-
ing the multivariate Gaussian model. This model is the workhorse of modern statistics and
machine learning: linear regression, probabilistic principal components analysis, Gaussian
Markov random ﬁelds, spectral density estimation, and Gaussian process models all rely on
the multivariate Gaussian distribution. The d-dimensional Gaussian distribution is com-
pletely speciﬁed by a mean vector µ and a covariance matrix Σ in S+
d , the space of d-by-d
PD matrices. By imposing diﬀerent structures on the covariance matrix, one can create
diﬀerent models. In some cases, it is possible to parameterize the covariance matrices in
terms of a small number of parameters. However, learning of the unstructured covariance
matrices, usually involved in inference on a large number of parameters, has remained as an
issue. The conjugate Gaussian inverse-Wishart model has known deﬁciencies [168]. Outside
of non-linear parameterizations of the Cholesky decomposition or matrix logarithm, there
1This chapter is adapted from the paper Geodesic Lagrangian Monte Carlo over the space of positive
deﬁnite matrices: with application to Bayesian spectral density estimation [93], written by A Holbrook,
Shiwei Lan, Alexander Vandenberg-Rodes, and Babak Shahbaba. The paper was previously published in
the Journal of Statistical Computation and Simulation ( c⃝2017 Taylor & Francis).
AH was supported
by NIH grant [T32 AG000096].
SL was supported by the Defense Advanced Research Projects Agency
(DARPA) funded program Enabling Quantiﬁcation of Uncertainty in Physical Systems (EQUiPS), contract
W911NF-15-2-0121. AV and BS were supported by National Institutes of Health [grant R01-AI107034] and
National Science Foundation [grant DMS-1622490]. The authors thanked Hernando Ombao and Norbert
Fortin for their helpful discussions.
17

has not yet been a way to perform Bayesian inference directly on the space of PD matrices
with ﬂexible prior speciﬁcations using unstructured covariance matrices.
In this most general context the diﬃculty is in sampling from a posterior distribution on an
abstract, high-dimensional manifold with boundary. It has not been clear how to propose
moves from point to point within (and without leaving) this space.
Our method takes
advantage of the intrinsic, Riemannian geometry on the space of PD matrices. This space
is incomplete under the Euclidean metric: following a straight trajectory will often result in
matrices that are no longer positive deﬁnite. The space is, however, geodesically complete
under the canonical metric: no matter how far the sampler travels along any geodesic,
it never leaves the space of PD matrices. Intuitively, we redeﬁne ‘straight line’ in a way
that precludes leaving the set of interest. Moreover, the metric-induced geodesics provide a
natural way to traverse the space of PD matrices, and these geodesics ﬁt nicely with recent
advances in Hamiltonian Monte Carlo on manifolds [66, 25, 112].
To this end, we use geodesic Lagrangian Monte Carlo (gLMC), which belongs to a growing
class of Hamiltonian Monte Carlo algorithms. Hamiltonian Monte Carlo (HMC) provides
an intelligent, partially deterministic method for moving around the parameter space while
leaving the target distribution invariant. New Markov states are generated by numerically
integrating a Hamiltonian system while Metropolis-Hastings steps account for the numerical
error [137]. Riemannian manifold Hamiltonian Monte Carlo (RMHMC) adapts the proposal
path by incorporating second-order information in the form of a Riemannian metric tensor
[66]. Lagrangian Monte Carlo (LMC) builds on RMHMC by using a random velocity in
place of RMHMC’s random momentum.
LMC’s explicit integrator is no longer volume
preserving; it therefore requires Jacobian corrections for each accept-reject step [111]. The
embedding geodesic Monte Carlo (egMC) [25] is able to take the geometry of the parameter
space into account while avoiding implicit integration by splitting the Hamiltonian [153]
into a Euclidean and a geodesic component. Unfortunately, egMC is not applicable when a
18

manifold’s Riemannian embedding is unknown. gLMC, on the other hand, eﬃciently uses
the same split Hamiltonian formulation as egMC but does not require an explicit Riemannian
embedding [See 112, for example]. This last fact makes gLMC an ideal candidate for Bayesian
inference on the space of PD matrices.
gLMC allows us to treat the entire covariance matrix as one would treat any other model
parameters. We are no longer restricted to use a conjugate prior distribution or to specify a
low-rank structure. We illustrate applications of gLMC for PD matrices using both simulated
and real-world data. First, we show that gLMC provides the same empirical results as the
closed-form solution for the conjugate Gaussian inverse-Wishart model. After this, we focus
on applying gLMC for Hermitian PD matrices to multivariate spectral density estimation
and compare the results obtained from two diﬀerent prior speciﬁcations: the inverse-Wishart
prior and the complex reference prior. Then, we obtain credible intervals for the squared
coherences (see Section 2.2) of simulated vector auto-regressive time series for which the
spectral density matrix is known. Finally, we apply gLMC to learn the spectral density
matrix associated with multivariate local ﬁeld potentials (LFPs).
The contributions of this chapter are as follows:
• gLMC, an MCMC methodology for Bayesian inference on general Riemannian mani-
folds, is proposed;
• to illustrate the general methodology, we provide a detailed description of gLMC on
the spaces of symmetric and Hermitian PD matrices;
• for classical statisticians, the chapter serves as a brief introduction to spectral density
estimation and its Bayesian approach;
• the proposed algorithms are applied to Bayesian inference on (real and complex) covari-
ance matrices based on simulated and real-world data and using a number of diﬀerent
prior speciﬁcations.
19

It should be noted that the proposed method is useful for generating samples from the
posterior distribution of interest and not just a point estimate. The proposed method is
for full inference of a posterior distribution deﬁned directly over the space of PD matrices
without limiting ourselves to conjugate priors, as such, is the ﬁrst of its kind.
That said, it is sometimes suﬃcient for the scientist to obtain a point estimate of the co-
variance or spectral density matrix. In this context, regularization of the estimate is often
advantageous. Regularization approaches may be interpreted as Bayesian and their corre-
sponding point estimates are interpreted as maximum a posteriori (MAP) estimates. See
[144] for a statistically minded survey of covariance estimation and regularization, and see
[1] for a state-of-science approach to point estimation in signal processing applications.
The rest of the chapter is outlined thus: in Section 2.2, we provide motivation for our
approach in the form of a brief introduction to spectral density estimation for multivariate
time series; in Section 2.3, we deﬁne PD matrices and show how the space of PD (symmetric
or Hermitian) matrices comprises a Riemannian manifold; in Section 2.4, we present the
gLMC methodology for Bayesian inference on general Riemannian manifolds; in Section 2.5,
we detail gLMC on the spaces of symmetric and Hermitian PD matrices; in Section 2.6,
we introduce the reader to common proper and improper priors for covariance matrices; in
Section 2.7, we present empirical results based on simulated and real-world data.
2.2
Motivation: learning the spectral density matrix
Given a stationary multivariate time series y(t) = (y1(t), . . . , yd(t))T ∈Rd, t = 1, . . . , T, one
often wants to characterize the dependencies between vector elements through time. There
are multiple ways to deﬁne such dependencies, and these deﬁnitions feature either the time
series directly or the Fourier-transformed series in the frequency domain. In the time domain,
20

one characterization of dependence is provided by the lagged variance-covariance matrix Γℓ.
In terms of lag ℓ, this is written
Γℓ= Cov
 y(t), y(t −ℓ)

= E
 y(t) −µ
 y(t −ℓ) −µ
T
.
(2.1)
Note that Γℓand µ are invariant over time by stationarity. If the scientist has a reason to
suspect that a certain lag ℓis important a priori, then Γℓcan be a useful measure. On the
other hand, it is often more scientiﬁcally tractable to think in terms of frequencies rather
than lags. In neuroscience, for example, one might hypothesize that two brain regions have
‘correlated’ activity during the performance of a speciﬁc task, but this co-activity may be too
complex to describe in terms of a simple lagged relationship. The spectral density approach
lends itself naturally to this kind of question. For a full discussion, see [176].
The power spectral density matrix is the Fourier transform of Γℓ:
Σ(ω) =
∞
X
ℓ=−∞
Γℓexp(−i2πωℓ) .
(2.2)
Σ(ω) is a Hermitian PD matrix. A Hermitian matrix M is a complex valued matrix satisfying
M H = M
T = M, where (·) denotes taking the complex conjugate. A Hermitian matrix M
is deﬁned to be PD if zHMz > 0, ∀z ∈Cd \ {0}.
A diagonal element Σii(ω) is called the auto-spectrum of yi(t) at frequency ω, and an oﬀ-
diagonal element Σij(ω), i ̸= j is the cross-spectrum of yi(t) and yj(t) at frequency ω. The
squared coherence is given by
ρ2
ij(ω) =
|Σij(ω)|2
Σii(ω) Σjj(ω) ,
(2.3)
where | · | denotes the complex modulus.
There are a number of ways to estimate the
spectral density matrix and, hence, the matrix of squared coherences. In this chapter, we
21

use the Whittle likelihood approximation [178]. We model the discrete Fourier transformed
time series Y (ωk) ∈Cd as following a (circularly-symmetric) complex multivariate Gaussian
distribution:
Y (ωk)
ind
∼CNd(0, Σ(ωk)) ,
(2.4)
where, for ωk = k
T and k = −(T
2 −1), . . . , T
2 ,
Y (ωk) =
1
√
T
T
X
t=1
y(t) exp(−i2πωkt) .
(2.5)
Three assumptions are made here. First, we assume that the Y (ωk)s are exactly Gaussian:
this is true when the y(t) follow any Gaussian process. Moreover, if y(t) follow a linear
process, then the Y (ωk) are asymptotically Gaussian as T goes to inﬁnity [23]. Second, we
assume that for ωk ̸= ωk′, Y (ωk) and Y (ωk′) are independent, whereas [23] show that they
are asymptotically uncorrelated. Third, we assume that Σ(·) is approximately piecewise
constant across frequency bands, and take all Y (ωk) to be approximately i.i.d. within a
small enough frequency band. For example, if we are interested in the alpha band of neural
oscillations ranging from 7.5 to 12.5 Hz, then we model
Y (ωk)
iid
∼CNd(0, Σα) ,
(2.6)
where Σα denotes the spectral density matrix shared by the entire band.
For a recent
use of the approximately piecewise constant assumption, see [64], where the spectrum is
represented as a sum of unique AR(2) spectra, with each of the AR(2) capturing distinct
frequency bands.
Thus, having obtained samples Y (ω) from a ﬁxed frequency band, we will use gLMC over
Hermitian PD matrices to perform inference on Σ. The posterior samples of Σ automatically
22

provide samples for the distributions of the squared coherences, which can in turn elucidate
dependencies between the univariate time series. Before discussing gLMC for PD matrices,
we establish necessary facts regarding the space of positive deﬁnite matrices.
2.3
The space of positive deﬁnite matrices
Let Sd(C) denote the space of d × d Hermitian matrices2, and S+
d (C) denote its subspace of
PD matrices. The space of Hermitian PD matrices, S+
d (C), may be written as a quotient
space GL(d, C)/U(d) of the complex general linear group GL(d, C) and the unitary group
U(d). The general linear group is the smooth manifold for which every point is a matrix with
non-zero determinant. The unitary group is the space of all complex matrices U satisfying
U HU = UU H = I. This quotient space representation is rooted in the fact that every PD
matrix may be written as the product Σ = GGH = GUU HGH for a unique G ∈GL(d, C)
and any arbitrary unitary matrix U ∈U(d). For the convenience of exposition, we drop
the dependence on C of symbols in the following of this section. Related references are
[141, 129, 85].
S+
d is a homogeneous space with respect to the general linear group: this means that the
group acts transitively on the S+
d . Here the group action is given by conjugation:
G∗Σ = GΣGH .
(2.7)
For any Σ1, Σ2 ∈S+
d , it simply takes the composition Σ1/2∗
2
◦Σ−1/2∗
1
to transform Σ1 into Σ2:
Σ1/2∗
2
◦Σ−1/2∗
1
Σ1 = Σ1/2∗
2
 Σ−1/2
1
Σ1Σ−1/2
1

= Σ1/2
2 I Σ1/2
2
= Σ2.
(2.8)
2In this section we focus on the space of Hermitian positive deﬁnite matrices, since the class of symmetric
matrices belongs to the broader class of Hermitian matrices. If the reader is primarily interested in the
smaller class, then she is free to substitute R for C, transpose (·)T for conjugate transpose (·)H, and the
orthogonal group O(d) for the unitary group U(d).
23

The space of Hermitian matrices, Sd, happens to be the tangent space to the space of
Hermitian PD matrices at the identity, denoted as TIdS+
d , that is, TIdS+
d = Sd. The action
Σ1/2∗: V 7→Σ1/2V Σ1/2
(2.9)
translates vector V ∈TIdS+
d to its corresponding vector in TΣS+
d , the tangent space to the
space of PD matrices at point Σ.
´Elie Cartan constructed a natural Riemannian metric g(·, ·) on the tangent bundle TS+
d that
is invariant under group action (2.7). For two vectors V1, V2 ∈TIdS+
d , the metric is given by
gI(V1, V2) = tr(V1V2) .
(2.10)
In this way the space of PD matrices is isometric to Euclidean space (equipped with the
Frobenius norm) at the identity. Next deﬁne the metric at any arbitrary point Σ to be
gΣ(V1, V2) = tr(Σ−1V1Σ−1V2)
(2.11)
It is easy to check that gI(V1, V2) = gΣ(Σ1/2∗V1, Σ1/2∗V2) and so Σ1/2∗is a Riemannian
isometry on S+
d .
Two geometric quantities are required for our purposes: the Riemannian metric tensor and
its corresponding geodesic ﬂow, speciﬁed by a starting point and an initial velocity vector.
The computational details involving the metric tensor are presented in Section 2.5. Here
we present the closed form solution for the geodesic ﬂow as found in [141]. S+
d is an aﬃne
symmetric space [106].
As such the geodesics under the invariant metric are generated
by the one-parameter subgroups of the acting Lie group [141, 85]. These one-parameter
subgroups are given by the group exponential map which, at the identity, is given by the
matrix exponential exp tG. In order to calculate the unique geodesic curve with starting
24

position Σ(0) and initial velocity V (0), all one needs is to translate the velocity to the
identity, compute the matrix exponential, and translate it back to the point of interest. In
sum, the geodesic is given by
Σ(t) = expΣ tV (0) = Σ(0)1/2 exp

tΣ(0)−1/2V (0)Σ(0)−1/2
Σ(0)1/2 .
(2.12)
The corresponding ﬂow on the tangent bundle will also be useful. This is obtained by taking
the derivative with respect to t:
V (t) = ˙Σ(t) = d
dt expΣ tV (0)
(2.13)
= V (0)Σ(0)−1/2 exp

tΣ(0)−1/2V (0)Σ(0)−1/2
Σ(0)1/2 .
For a Lie group, the exponential map (on which the above formula is based) is a local
diﬀeomorphism between the tangent space at a point on the manifold and the manifold
itself. Given a tangent vector V at Σ, expΣ V is a point on the manifold. Incidentally, for
the spaces of PD matrices, this diﬀeomorphism is global. The inverse of the exponential
map is the logarithmic map. Whereas the exponential map on the manifold takes Hermitian
matrices (Sd) to Hermitian PD matrices (S+
d ), the logarithmic map takes Hermitian PD
matrices (S+
d ) to Hermitian matrices (Sd).
Together these are most of the geometric quantities required for gLMC over PD matrices.
The next section presents Hamiltonian Monte Carlo, its geometric extension RMHMC, and
its Lagrangian manifestations.
25

2.4
Bayesian inference on general Riemannian mani-
folds using the geodesic Lagrangian Monte Carlo
Given data y1, . . . , yN ∈Rn, one may specify a generative model by a likelihood function,
p(y|q). In the following we allow q ∈Mm to be an m-dimensional vector on a manifold that
parameterizes the likelihood. Endowing q with a prior distribution p(q) renders the posterior
distribution
π(q) = p(q|y) =
p(y|q)p(q)
R
p(y|q)p(q) dq .
(2.14)
The integral is often referred to as the evidence and may be interpreted as the probability
of observing data y given the model. In most interesting models the evidence integral is
intractable and high dimensional models do not lend themselves to easy numerical integra-
tion. Non-quadrature sampling techniques such as importance sampling or even random
walk MCMC also suﬀer in high dimensions. HMC is an eﬀective sampling tool for higher
dimensional models over continuous parameter spaces [49, 137]. Here we discuss HMC and
its geometric variants (see Section 2.1) in detail.
In HMC, a Hamiltonian system is constructed that consists of the parameter vector q and an
auxiliary vector p of the same dimension. The negative-log transform turns the probability
density functions into a potential energy function U(q) = −log π(q) and corresponding
kinetic function K(p). Thus q and p become the position and momentum of Hamiltonian
function
H(q, p) = U(q) + K(p) .
(2.15)
26

By Euler’s method or extensions, the system is numerically advanced according to Hamilton’s
equations:
dq
dt
=
∂H
∂p
(2.16)
dp
dt
=
−∂H
∂q .
Riemannian manifold HMC uses a slightly more complicated Hamiltonian to sample from
posterior π(q):
H(q, p)
=
−log π(q) + 1
2 log |G(q)| + 1
2pTG(q)−1p .
(2.17)
Here, G(q) is the Fisher information matrix at point q (in Euclidean space) and may be
interpreted as a Riemannian metric tensor induced by the curvature of the log-probability.
Exponentiating and negating H(q, p) reveals p to follow a Gaussian distribution centered
at origin with metric tensor G(q) for covariance. The corresponding system of ﬁrst-order
diﬀerential equations is given by
dq
dt
=
G(q)−1p,
(2.18)
dp
dt
=
∇q

log π(q) −1
2 log |G(q)| −1
2pTG(q)−1p

.
The Hamiltonian is not separable in p and q. To get numerical solutions, one may split it into
a potential term H[1], featuring q alone, and a kinetic term, H[2], featuring both variables
27

[153, 25]. The two systems are then simulated in turn. The ﬁrst term is given by
H[1](q, p) = −log π(q) + 1
2 log |G(q)|
(2.19)
and starting at (q(0), p(0)) the associated system has solutions
q(t) = q(0)
and
p(t) = p(0) + t∇q
 log π(q) −1
2 log |G(q)|

|q=q(0) .
(2.20)
The second component is the quadratic form
H[2](q, p) = 1
2pTG(q)−1p
(2.21)
The solutions to the system associated with H[2] are given by the geodesic ﬂow under the
Levi-Civita connection with respect to metric G and with momentum p(t) = G(q(t)) ˙q(t).
There is, however, no a priori reason to restrict G(q) to be the Fisher information as is done
in the [66]. In fact, by allowing G(q) to take on other forms, one may perform HMC on a
number of manifold parameterized models.
2.4.1
Geodesic Lagrangian Monte Carlo
[25] show how to extend the RMHMC framework to manifolds that admit a known Rie-
mannian isometric embedding into Euclidean space. The algorithm is especially eﬃcient
when there exists a closed form linear projection of vectors in the ambient space onto the
tangent space at any point. Although this embedding will always exist [134], it is rarely
known. When equipped with the canonical metric, the space of PD matrices does not admit
a known isometric embedding. Moreover, we are unaware of a closed-form projection onto
28

Algorithm 1 Geodesic Lagrangian Monte Carlo
Let q = q(k) be the kth state of the Markov chain. The next sample is generated according
to the following procedure.
(a) Generate proposal state q∗:
1: v ∼N(0, G−1(q))
2: e ←−log π(q) −1
2 log |G(q)| + 1
2vTG(q)v
3: q∗←q
4: for τ = 1, . . . , T do
5:
v ←v + ϵ
2G(q∗)−1∇q
 log π(q∗) + 1
2 log |G(q∗)|

6:
Progress (q∗, v) along the geodesic ﬂow for time ϵ.
7:
v ←v + ϵ
2G(q∗)−1∇q
 log π(q∗) + 1
2 log |G(q∗)|

8: end for
9: e∗←−log π(q∗) −1
2 log |G(q∗)| + 1
2vTG(q∗)v
(b) Accept proposal with probability min{1, exp(e)/ exp(e∗)}:
1: u ∼U(0, 1)
2: if u < exp(e −e∗) then
3:
q ←q∗
4: end if
(c) Assign value q to q(k+1), the (k + 1)th state of the Markov chain.
the manifold’s tangent space at a given point. We therefore opt for an intrinsic approach
instead.
In the prior section, we stated that the solution to Hamilton’s equations associated with the
kinetic term H[2] is given by the geodesic ﬂow with respect to the Levi-Civita connection.
This ﬂow is easily written in terms of the exponential map with respect to a velocity vector
(as opposed to the momentum covector). Given an arbitrary covector p ∈T ∗
q M, one may
obtain the corresponding vector v ∈TqM by the one-to-one transformation v = G−1(q)p.
Hence whereas RMHMC augments the system with p ∼N(0, G(q)), Lagrangian Monte Carlo
makes use of v = G−1(q)p ∼N(0, G−1(q)). The energy function is then given by
E(q, v)
∝
−log π(q) −1
2 log |G(q)| + 1
2vTG(q)v .
(2.22)
29

The probabilistic interpretation of the energy remains the same as in the case of RMHMC:
the energy is the negative logarithm of the probability density functions of two independent
random variables, one of which is the variable of interest, the other of which is the augmenting
Gaussian variable. On the other hand, the physical interpretation is diﬀerent. We use the
term ‘energy’ in order to accommodate the two physical interpretations available for (2.22):
E(q, v) may be thought of either as a Hamiltonian or as a Lagrangian energy. In practice,
which formulation is used is dictated by the geometric information available. The Lagrangian
formulation provides eﬃcient update equations when no closed-form geodesics are available.
In this case, the Lagrangian (energy) is deﬁned as the kinetic term T less the potential term
V as follows
V (q, v) = log π(q) + 1
2 log |G(q)|,
and
T(q, v) = 1
2vTG(q)v .
(2.23)
But when closed-form geodesics are available, it is useful to follow [25] and split the (now
considered) Hamiltonian into two terms as in (2.19) and (2.21). Within this regime, H[1] =
−V and H[2] = T. In analogy with (2.20) and starting at (q(0), v(0)), the system deﬁned by
potential V has solution
q(t) = q(0),
v(t) = v(0) + t G(q)−1∇qV (q, v)|q=q(0) ,
(2.24)
and the system deﬁned by kinetic term T has the unique geodesic path speciﬁed by starting
position q(0) and initial velocity v(0) as a solution. The inverse metric tensor G−1(q) is
used to ‘raise the index’, i.e. transform the covector ∇qV (q, v) into a vector on the tangent
space at q. Thus it plays a similar function to the orthogonal projection in [25]. We call this
formulation geodesic Lagrangian Monte Carlo (gLMC) and detail its steps in Algorithm 1,
30

where the term ‘Lagrangian’ is used to emphasize the fact that we use velocities in place of
momenta. Note [112, 110] implemented the similar idea on the manifold of a d-dimensional
sphere. To implement geodesic Lagrangian Monte Carlo, one must be able to compute the
inverse metric tensor G−1(q) and the geodesic path given starting values. When the space of
PD matrices is equipped with the canonical metric, G−1(q) is given in closed-form and the
geodesic path is easily computable.
2.5
gLMC on the manifold of PD matrices
Algorithm 2 gLMC for symmetric PD matrices
Let Σ = Σ(k) be the kth state of the Markov chain. The next sample is generated according
to the following procedure.
(a) Generate proposal state Σ∗:
1: vech(V ) ∼N(0, G−1(Σ))
2: e ←−log π(Σ) −d+1
2 log |Σ| + 1
2vech(V )TG(Σ)vech(V )
3: Σ∗←Σ
4: for τ = 1, . . . , T do
5:
vech(V ) ←vech(V ) + ϵ
2G−1(Σ∗)vech

∇Σ
 log π(Σ∗) + d+1
2 log |Σ∗|

6:
Progress (Σ∗, V ) along the geodesic ﬂow for time ϵ.
7:
vech(V ) ←vech(V ) + ϵ
2G−1(Σ∗)vech

∇Σ
 log π(Σ∗) + d+1
2 log |Σ∗|

8: end for
9: e∗←−log π(Σ∗) −d+1
2 log |Σ∗| + 1
2vech(V )TG(Σ∗)vech(V )
(b) Accept proposal with probability min{1, exp(e)/ exp(e∗)}:
1: u ∼U(0, 1)
2: if u < exp(e −e∗) then
3:
Σ ←Σ∗
4: end if
(c) Assign value Σ to Σ(k+1), the (k + 1)th state of the Markov chain.
To perform gLMC on the space of PD matrices, S+
d , we equip the manifold with the canonical
metric. In order to signify that we are no longer dealing with gLMC in its full generality,
we adopt the notation of Section 2.3. PD matrix Σ replaces q, and symmetric or Hermitian
matrix V replaces v. All other notations remain the same. As stated in the previous section,
31

we require the inverse metric tensor G−1(Σ). To compute this quantity, we need a couple
more tools provided by [129]. Let vech(·) take symmetric (Hermitian) d × d matrices to
vectors of length
d
2(d + 1) by stacking diagonal and subdiagonal matrix elements in the
following way:
vech(V ) = (V11, V21, . . . , Vd1, V22, . . . , Vd2, . . . , Vdd).
(2.25)
Let vec(·) take symmetric (Hermitian) d × d matrices to vectors of length d2 by stacking all
matrix elements:
vec(V ) = (V11, V21, . . . , Vd1, V12, . . . , Vd2, . . . , V1d, . . . , Vdd).
(2.26)
Let Dd be the unique d2 × d
2(d + 1) matrix satisfying
vec(V ) = Ddvech(V ) .
(2.27)
Denote D+
d as the Moore-Penrose inverse of Dd satisfying
vech(V ) = D+
d vec(V ) ,
(2.28)
32

with D+
d given by
D+
d = (DT
d Dd)−1DT
d .
(2.29)
Then [129] show that the metric tensor and inverse metric tensor are given by the d
2(d+1)×
d
2(d + 1) dimensional matrices
G(Σ) = DT
d (Σ−1 ⊗Σ−1)Dd
and
G−1(Σ) = D+
d (Σ ⊗Σ)D+T
d
.
(2.30)
Finally, the determinant of G(Σ) can be expressed in terms of Σ alone:
|G(Σ)| ∝|Σ|d+1 .
(2.31)
The metric tensor features in the energy function for gLMC for both symmetric and Hermi-
tian PD matrices. For symmetric PD matrices, the energy is given by
E(Σ, V ) ∝−log π(Σ) −1
2 log |G(Σ)| + 1
2vech(V )TG(Σ)vech(V )
(2.32)
∝−log π(Σ) −d + 1
2
log |Σ| + 1
2vech(V )TG(Σ)vech(V ) ,
but the energy for Hermitian PD matrices is slightly diﬀerent. In this case, both Σ and V
are complex valued, and vech(V ) follows a multivariate complex Gaussian distribution with
covariance G−1(Σ). Therefore, the gLMC energy for Hermitian PD matrices is given by
E(Σ, V ) ∝−log π(Σ) −log |G(Σ)| + vech(V )HG(Σ)vech(V )
(2.33)
∝−log π(Σ) −(d + 1) log |Σ| + vech(V )HG(Σ)vech(V )
33

where (·)H signiﬁes the conjugate transpose. Notice that the log-determinant and quadradic
terms are not multiplied by the factor 1/2. This accords with the density function of a
complex Gaussian random variable. See Appendix B.1 for more details.
The metric tensor (2.30) and the geodesic equations (2.12) and (2.13) are the only geometric
quantities required for gLMC on PD matrices. The k-th iteration of the symmetric PD
algorithm is shown in Algorithm 2. The k-th iteration of the Hermitian PD algorithm is
shown in Algorithm 3. First, one generates a Gaussian initial velocity on TΣ(k)S+
d (Step 1).
Then, the energy function is evaluated and stored (Step 2). Next, the system is numerically
advanced using the split Hamiltonian scheme. Following Equation (2.24), the velocity vector
V is updated one half-step with the gradient of H[1] (Step 4). For Step 5, both Σ and V
are updated with respect to H[2], i.e. they are transported along the geodesic ﬂow given by
Equations (2.12) and (2.13):

Σ(0), V (0)

7→

Σ(ϵ), V (ϵ)

.
(2.34)
Again, the velocity vector V is updated one half-step with the gradient of H[1] (Step 6).
Finally, the energy is evaluated at the new Markov state (Step 9), and a Metropolis accept-
reject step is implemented (Steps 10-12). It is important to note that, besides being over
diﬀerent algebraic ﬁelds, the symmetric and Hermitian instantiations only diﬀer in their
respective energies. The general implementation is the same. See Appendix B.1 for a short
discussion on gradients.
2.6
Some priors on covariance matrices
We provide a short introduction to some well known priors for covariance matrices. This
list is in no way exhaustive but is meant to hint at the choices available to practitioners. Of
34

Algorithm 3 gLMC for Hermitian PD matrices
Let Σ = Σ(k) be the kth state of the Markov chain. The next sample is generated according
to the following procedure.
(a) Generate proposal state Σ∗:
1: vech(V ) ∼CN(0, G−1(Σ))
2: e ←−log π(Σ) −(d + 1) log |Σ| + vech(V )HG(Σ)vech(V )
3: Σ∗←Σ
4: for τ = 1, . . . , T do
5:
vech(V ) ←vech(V ) + ϵ
2G−1(Σ∗)vech

∇Σ
 log π(Σ∗) + (d + 1) log |Σ∗|

6:
Progress (Σ∗, V ) along the geodesic ﬂow for time ϵ.
7:
vech(V ) ←vech(V ) + ϵ
2G−1(Σ∗)vech

∇Σ
 log π(Σ∗) + (d + 1) log |Σ∗|

8: end for
9: e∗←−log π(Σ∗) −(d + 1) log |Σ∗| + vech(V )HG(Σ∗)vech(V )
(b) Accept proposal with probability min{1, exp(e)/ exp(e∗)}:
1: u ∼U(0, 1)
2: if u < exp(e −e∗) then
3:
Σ ←Σ∗
4: end if
(c) Assign value Σ to Σ(k+1), the (k + 1)th state of the Markov chain.
0
2
4
6
3
5
10
Dimension
Condition number
Prior
Flat
Jeffreys
Reference
Figure 2.1: Median condition number by dimension and prior speciﬁcation. Box plots de-
scribe distributions of 100 median condition numbers for each dimension and prior. Each
point is the median from 200 posterior samples based on independent data and using gLMC.
The reference prior is designed to yield smaller condition numbers than Jeﬀreys prior and
hence better asymptotics [182].
35

Prior
Real
Complex
Wishart
|Σ|(n−d−1)/2 exp
 −tr{Ψ−1Σ}/2

|Σ|n−d exp
 −tr{Ψ−1Σ}

inverse-Wishart
|Σ|−(n+d+1)/2 exp
 −tr{ΨΣ−1}/2

|Σ|−(n+d) exp
 −tr{ΨΣ−1}

uniform
1
1
Jeﬀreys
|Σ|−(d+1)/2
|Σ|−d
reference
 |Σ| Q
i<j(di −dj)
−1
 |Σ| Q
i<j(di −dj)2−1
Table 2.1: Priors for Σ and their densities up to proportionality. The ﬁrst two priors are
proper, i.e.
comprise well-deﬁned probability distributions, the last three are not.
Σ is
symmetric and Hermitian PD in the left and right columns, respectively. Note how the
Wishart, inverse-Wishart, and Jeﬀreys priors share similar patterns moving from real to
complex numbers.
the priors we present, three are improper (are not well deﬁned probability distributions) and
two are proper. The real and complex versions of all ﬁve priors are shown in Table 1.
The Wishart and inverse-Wishart distributions are the most well known for PD matri-
ces. These two distributions are popular not because they are particularly good models
but because they make Bayesian inference easy for covariance matrices. The Wishart and
inverse-Wishart distributions are conjugate priors for the precision and covariance matrices,
respectively. This means that they provide closed-form posteriors given the data and thus
obviate Monte Carlo methods. Of course, the Wishart distribution can be used as a prior
for covariances (as opposed to precision matrices), but it usually is not since conjugacy is
then lost. The inverse-Wishart distribution is the distribution of the inverse of a Wishart
random variable. Shown in Table 1, both distributions are parameterized by symmetric or
Hermitian PD matrices Ψ and scalar ν which is greater than d−1 for real and d for complex
Σ.
The improper priors are the ﬂat, the Jeﬀreys, and the reference priors. The MLE may be
interpreted as the MAP estimate given the ﬂat prior. The Jeﬀreys prior is the square-root
determinant of the Fisher information and is parameterization invariant. When Σ is real,
the Jeﬀreys prior is the reciprocal of the density of the Hausdorﬀmeasure with respect to
the Lebesgue measure [25]; it can therefore be interpreted as the ﬂat prior with respect to
36

the Hausdorﬀmeasure. The reference prior is designed to prevent estimates from being
ill-conditioned: as may be seen in Figure 1, it favors eigenvalues that are close together.
This corresponds to better frequentist estimation properties [144]. For an introduction to
the complex Wishart and inverse-Wishart distributions, see [154]. For the complex Jeﬀreys
and reference priors, see [163] and [164], respectively.
Again, these priors are not intended to form a comprehensive list but give an idea of the
kinds of choices that statisticians might make when choosing a prior for a covariance. For
more recent developments in this area, see [150] and [57].
2.7
Results
This section features empirical validation of the gLMC algorithm as well as an application
to learning the spectral density matrix for vector time series. For empirical validation, we
present quantile-quantile and trace plots comparing the gLMC sample to the closed-form
solution made available by the conjugate prior.
We then use gLMC for Hermitian PD
matrices to learn the spectral density matrices of both simulated and LFP time series. We
use the posteriors thus obtained to get credible intervals on the squared coherences for the
vector time series.
2.7.1
Empirical validation
Before applying gLMC to spectral density estimation, we demonstrate validity by compar-
ing samples from empirical posterior distributions of the Gaussian inverse-Wishart model
obtained by gLMC and the closed-form solution. Note that our objective is to show that our
proposed method provides valid results, similar to those obtained based on conjugate priors,
while creating a ﬂexible framework for eliciting and specifying prior distributions directly
37

0.0
0.4
0.8
−0.4
0.0
0.4
0.8
Exact posterior
gLMC (Hermitian PD)
Matrix
elements
(1,1)
(1,2)
(1,3)
(2,1)
(2,2)
(2,3)
(3,1)
(3,2)
(3,3)
Local comparison
0.0
0.2
0.4
0.6
0
250
500
750
1000
Iteration
Value
Method/
summary
Exact/ED
Exact/EV
gLMC/ED
gLMC/EV
Indirect/ED
Indirect/EV
Global comparison
0
3
6
9
0.35
0.40
0.45
0.50
0.55
0.60
Effective variance
Posterior density
0
5
10
15
20
0.00
0.05
0.10
Effective dependence
Method
Exact
gLMC
Indirect
0.0
0.4
0.8
−0.4
0.0
0.4
0.8
Exact posterior
gLMC (Hermitian PD)
Matrix
elements
(1,1)
(1,2)
(1,3)
(2,1)
(2,2)
(2,3)
(3,1)
(3,2)
(3,3)
Local comparison
0.0
0.2
0.4
0.6
0
250
500
750
1000
Iteration
Value
Method/
summary
Exact/ED
Exact/EV
gLMC/ED
gLMC/EV
Indirect/ED
Indirect/EV
Global comparison
0
3
6
9
0.35
0.40
0.45
0.50
0.55
0.60
Effective variance
Posterior density
0
5
10
15
20
0.00
0.05
0.10
Effective dependence
Method
Exact
gLMC
Indirect
Figure 2.2: These ﬁgures provide empirical validation for the well-posedness of gLMC for PD
matrices. On the top-left is a quantile-quantile plot comparing the gLMC (for Hermitian PD
matrices) posterior sample with that of the closed-form posterior for the complex Gaussian
inverse-Wishart model.
Both real and imaginary elements are included, and points are
jittered for visibility. On the top-right are posterior samples of ‘global’ matrix summaries
pertaining both to gLMC (for symmetric PD matrices), the closed-form ‘exact’ solution, and
the ‘indirect’ log-Cholesky parameterization. These summaries are the eﬀective variance
and the eﬀective dependence, built oﬀthe covariance matrix and the correlation matrix,
respectively. On the bottom are posterior density plots of the same matrix summaries.
38

−4
−2
0
2
4
0
25
50
75
100
Block VAR1 values
Time
series
1
2
3
4
Simulated series
G
G
G
G
G
G
G
G
G
G
G
G
0.00
0.25
0.50
0.75
1.00
1:2 1:3 1:4 2:3 2:4 3:4
Coherence values
Prior
Inverse−wishart
Reference
95% Credible intervals
−4
−2
0
2
0
25
50
75
100
Time
Full VAR1 values
Time
series
1
2
3
4
G
G
G
G
G
G
G
G
G
G
G
G
0.00
0.25
0.50
0.75
1.00
1:2 1:3 1:4 2:3 2:4 3:4
Series pairs
Coherence values
Prior
Inverse−wishart
Reference
−4
−2
0
2
4
0
25
50
75
100
Block VAR1 values
Time
series
1
2
3
4
Simulated series
G
G
G
G
G
G
G
G
G
G
G
G
0.00
0.25
0.50
0.75
1.00
1:2 1:3 1:4 2:3 2:4 3:4
Coherence values
Prior
Inverse−wishart
Reference
95% Credible intervals
−4
−2
0
2
0
25
50
75
100
Time
Full VAR1 values
Time
series
1
2
3
4
G
G
G
G
G
G
G
G
G
G
G
G
0.00
0.25
0.50
0.75
1.00
1:2 1:3 1:4 2:3 2:4 3:4
Series pairs
Coherence values
Prior
Inverse−wishart
Reference
Figure 2.3: Two 4-dimensional VAR1 time series and credible intervals for their 6 corre-
sponding coherences measured at 20-40 Hz. The top row belongs to a block VAR1 process
characterized by two independent 2-dimensional VAR1 time series; the bottom row belongs
to a full VAR1 process. The left column shows the ﬁrst 100 samples of both time series,
each of which totals 5,000 samples in length. The right column shows credible intervals from
posteriors obtained using the inverse-Wishart and reference priors.
39

−0.50
−0.25
0.00
0.25
0
50
100
150
200
Milliseconds (ms)
Rescaled voltage
Channel
1
2
3
4
LFP signals
0.00
0.25
0.50
0.75
1.00
1:2
1:3
1:4
2:3
2:4
3:4
Series pairs
Coherence values
95% CI: 20−40 Hz
0.00
0.25
0.50
0.75
1.00
1:2
1:3
1:4
2:3
2:4
3:4
Series pairs
95% CI: 40−160 Hz
Prior
Inverse−wishart
Reference
−0.50
−0.25
0.00
0.25
0
50
100
150
200
Milliseconds (ms)
Rescaled voltage
Channel
1
2
3
4
LFP signals
0.00
0.25
0.50
0.75
1.00
1:2
1:3
1:4
2:3
2:4
3:4
Series pairs
Coherence values
95% CI: 20−40 Hz
0.00
0.25
0.50
0.75
1.00
1:2
1:3
1:4
2:3
2:4
3:4
Series pairs
95% CI: 40−160 Hz
Prior
Inverse−wishart
Reference
Figure 2.4: A 4-dimensional LFP signal with credible intervals for 6 coherences measured at
20-40 Hz (left) and 40-160 Hz (right). First 200 samples are shown for ease of visualization;
the multi-dimensional time series totals 4,000 samples in length.
Coherence proﬁles are
remarkably similar between the two frequency bands considered.
40

over the space of PD matrices. To this end, we compare element-wise distributions with
quantile-quantile plots and whole-matrix distributions with two global matrix summaries.
The comparisons based on 10,000 samples by the diﬀerent sampling methods over 3-by-3 PD
matrices are illustrated in Figure 2.2. The ﬁrst 200 samples are discarded, and, for better
visualization, every tenth sample is kept. For the global matrix summaries, we also include
samples from an indirect approach, the log-Cholesky parameterization of the PD matrix
(currently used in Stan software implementations [26]). Starting with a lower-triangular
matrix L, one obtains a PD matrix by exponentiating the diagonal elements of L to get eL
and then evaluating Σ = eLeLT. Given a distribution over PD matrices, one may obtain a
distribution over lower-triangular matrices using the inverses of these transforms and their
corresponding Jacobians.
On the top-left panel of Figure 2.2, a quantile-quantile (Q-Q) plot is used to compare the
gLMC Hermitian PD posterior sample to the closed-form posterior. The Q-Q plot is the
gold standard for comparing two scalar distributions using empirical samples because full
quantile agreement corresponds to equality of cumulative distribution functions. Points are
jittered for easy visualization, and each color speciﬁes a diﬀerent matrix element. Note that
some colors appear twice: these double appearances correspond to real and imaginary matrix
elements. For example, pink appears at zero as well as the upper right of the plot: this color
corresponds to a diagonal matrix element since, on account of the matrix being Hermitian,
its imaginary part is ﬁxed at zero. Most importantly, all matrix elements ﬁt tightly around
the line y = x, suggesting a perfect match in quantiles between empirical distributions.
On the top-right panel of Figure 2.2, we present samples obtained from two whole-matrix
summaries using gLMC, the ‘exact’ closed-form posterior, and the ‘indirect’ log-Cholesky
parameterization. These summaries are the eﬀective variance (EV) and the eﬀective depen-
41

dence (ED):
EV(Σ) = |Σ|1/d ,
and
ED(Σ) = 1 −|corr(Σ)|1/d .
(2.35)
The EV is the geometric mean of the eigenvalues of the matrix Σ. It provides a dimension
free summary of the total variance encoded in the matrix. The ED gets its name because
the determinant of a correlation matrix is inversely related to the magnitude of the indi-
vidual correlations that make up the oﬀ-diagonals. In addition to seeing that element-wise
distributions match, one would also like to know that their joint distributions correspond.
The EV and ED are good summaries of global matrix features and here provide empirical
evidence for the validity of gLMC for PD matrices. The three methods have similar posterior
distributions of EV and ED (Figure 2.2, bottom panels).
2.7.2
Learning the spectral density
An important beneﬁt of gLMC is that it enables practitioners to specify prior distributions
other than the inverse-Wishart on PD matrices based on needs dictated by the problems at
hand. gLMC improves modeling ﬂexibility. We use the problem of Bayesian spectral density
estimation to demonstrate the possibility and advantage of using non-conjugate priors. The
spectral density matrix Σ(ω) and its coherence matrix R(ω) are deﬁned in Section 2.2. In the
context of stationary, multivariate time series, the coherences that make up the oﬀ-diagonals
of R(ω) provide a lag-free measure of dependence between univariate time series at a given
frequency ω. Hence, these coherences are among the more interpratable parameters of the
spectral density matrix.
42

We compare posterior inference for these coherences between two models with diﬀerent
priors: the ﬁrst model uses the complex inverse-Wishart prior; the second uses the complex
reference prior [164]. The reference prior is an improper prior that has been proposed as an
alternative to Jeﬀrey’s prior for its superior eigenvalue shrinkage (which improves asymptotic
eﬃciency of estimators). We use the reference prior to emphasize the ﬂexibility allowed by
gLMC but not as a modeling suggestion. [164] provide a Gibbs sampling routine based on
the eigen-decomposition of the covariance matrix. The reference prior’s form is provided in
Appendix B.1.1.
We apply gLMC to learning the spectral density matrix for three distinct 4-dimensional
time series. The ﬁrst is a simulated ﬁrst-order vector-autoregressive (VAR1) time series with
block structure consisting of two independent, 2-dimensional VAR1 time series. The second
time series is also VAR1 but with dependencies allowed between all four of the scalar time
series of which it is composed. The third time series comes from local ﬁeld potentials (LFP)
recorded in the CA1 region of a rat hippocampus [3].
Figure 2.3 shows the ﬁrst 100 samples from both VAR1 time series along with 95% posterior
intervals for the complex moduli of the coherences. The time series are simulated with the
form:
y(t) = Φ y(t −1) + ϵt,
y(1) = ϵ1,
ϵt ∼N4(0, I), t = 1, . . . , 15000 ,
(2.36)
where the eigenvalues of transition matrix Φ are bounded with absolute value less than 1
to induce stationarity. The ﬁrst 10,000 data points are discarded to allow time for mixing.
The ﬁrst row of Figure 2.3 belongs to the block VAR1: Φ is a randomly constructed, block-
diagonal matrix, so the ﬁrst two scalar time series are independent from the second two. The
second row of Figure 2.3 belongs the the second VAR1, all the scalar time series of which
are dependent on all the others. Here Φ is also a randomly constructed matrix but is not
43

block-diagonal. The intervals corresponding to the inverse-Wishart prior model are given in
orange. The intervals corresponding to the reference prior model are given in blue. The true
coherences are represented by black points and are obtained using the following closed-form
formula for the spectral density of a VAR1 process [176]:
Σ(ω) =
 I −Φe−2πiω−1Q
 I −Φe−2πiω−H.
(2.37)
Here Q is the covariance matrix of the additive noise ϵt, and (·)−H denotes the inverse
conjugate transpose. For the block VAR1 example, both models capture the true, non-null
coherences (i.e. those given on the far left and the far right), but neither captures the null
coherences. This is more than satisfactory, since coherences equal to zero imply the identity
for a covariance matrix. By looking closely, one can see that the ﬁrst and second time series
(orange and green) are indeed strongly dependent on each other, as interval ‘1:2’ suggests.
For the full VAR1 example, both models capture ﬁve out of six true coherences, but the
reference prior model gets closer to the truth than the inverse-Wishart model does.
We use the same tools to detect coherences between LFP signals simultaneously recorded
from the CA1 region of a rat hippocampus prior to a memory experiment [3]. Two of the LFP
signals are recorded on one end of the CA1 axis, and the other two LFP signals are recorded
at the opposite end. Figure 2.4 shows the ﬁrst 200 of 4,000 samples (recorded at 1,000 Hz)
and 95% credible intervals for the coherences at two diﬀerent frequency bands: 20-40 Hz and
40-160 Hz. The spatial discrepancy is reﬂected in the posterior distributions of the individual
coherences. Both bands show similar coherence patterns, where spatial location appears to
dictate strength of coherence: the leftmost and rightmost pairs are closer to each other in
space, while the center pairs are farther from each other. This reﬂects what is apparent in the
top of Figure (2.4), where the ﬁrst and second time series (orange and green) are dependent,
and the third and fourth time series (blue and purple) are dependent. These correspond to
the intervals labeled ‘1:2’ and ‘3:4’, respectively. The credible intervals are smaller for the
44

20-40 Hz band because that band has only 1/6 the data of the 40-160 Hz band. Between
prior models, the intervals diﬀer more for the 40-160 Hz band. This is counter-intuitive since
the inﬂuence of the prior distribution is often assumed to diminish with the size of the data
set. One question is whether this surprising result is related to the reference prior’s being the
prior that is ‘maximally dominated by the data’ [16]. These diﬀerences—diﬀerences between
posterior distributions for the two prior models—communicate that other prior distributions
might provide tangible diﬀerences between results in spectral analysis and that it would be
useful to understand which prior distributions are appropriate in which contexts.
2.8
Discussion
We presented geodesic Lagrangian Monte Carlo an MCMC methodology for Bayesian in-
ference on general Riemannian manifolds. We outlined its relationship to other geometric
extensions of HMC and showed how to apply gLMC to both symmetric and Hermitian PD
matrices. We demonstrated empirical validity using both element-wise and whole-matrix
comparisons against the conjugate inverse-Wishart model. Finally, we applied gLMC on
Hermitian PD matrices to Bayesian spectral density estimation. The algorithm proved ef-
fective for detecting true coherences of simulated time series, as well as recovering spatial
discrepancies between real-world LFP signals.
We see three branches of inquiry stemming from this work: the ﬁrst is algorithmic; the
second, theoretical; the third, methodological. First, what variations of HMC might help
extend gLMC over PD matrices into higher dimensions? There are multiple such extensions
that are orthogonal to gLMC. Examples are windowed HMC, geometric extensions to the
NUTs algorithm, shortcut MCMC, and look-ahead HMC [137, 158, 20]. Auto-tuning will
prove useful: even within the same dimension, diﬀerent samples will dictate diﬀerent num-
bers of leapfrog steps and step-sizes. From the theoretical standpoint, the canonical metric
45

on the space of PD matrices is closely related to the Fisher information metric on covari-
ance matrices: how should one characterize this intersection between information geometry
and Riemannian symmetric spaces, and how might this relationship inform Bayes estimator
properties or future variations on gLMC? Methodologically, much work needs to be done in
prior elicitation for Bayesian spectral density estimation. Which priors on Hermitian PD
matrices should be used for which problems, what are the costs and beneﬁts, and are there
priors over symmetric PD matrices that need to be complexiﬁed (cf. [150, 57])? A clear
delineation will be useful for practitioners in Bayesian time series research.
46

Chapter 3
Diﬀerentiating the pseudo
determinant
47

Chapter Summary
In the following, the groundwork is laid for Chapter 4’s simpliﬁcation of the geodesic
Monte Carlo, the equations of which require diﬀerentiating the pseudo determinant.
Here, a class of derivatives is deﬁned for the pseudo determinant Det(A) of a Hermitian
matrix A. This class is shown to be non-empty and to have a unique, canonical member
∇Det(A) = Det(A)A+, where A+ is the Moore-Penrose pseudo inverse. The classic
identity for the gradient of the determinant is thus reproduced. Examples are provided,
including the maximum likelihood problem for the rank-deﬁcient covariance matrix of
the degenerate multivariate Gaussian distribution.
48

3.1
Introduction
We derive the class of derivatives of the pseudo determinant with respect to Hermitian
matrices1, placing an emphasis on understanding the forms taken by this class and their
relationship to established results in linear algebra. In particular, care must be taken to
address the discontinuous nature of the pseudo derivative. The contributions in this chapter
are primarily of a linear algebraic nature but are well motivated in ﬁelds of application.
The pseudo determinant arises in graph theory within Kirchoﬀ’s matrix tree theorem [105]
and in statistics, in the deﬁnition of the degenerate Gaussian distribution. The degenerate
Gaussian has been useful in image segmentation [181], communications [27], and as the
asymptotic distribution for multinomial samples [177]. Despite these appearances, knowledge
of how to diﬀerentiate the distribution’s density function is conspicuously absent from the
literature, and—since diﬀerentiation is often essential for maximization—the lack of this
knowledge is a plausible barrier to the distribution’s wider use.
Speciﬁcally, to obtain the maximum likelihood (ML) estimator for the singular covariance
matrix of the degenerate Gaussian, one must be able to calculate the derivative of the log
likelihood and hence the pseudo determinant of the covariance. Although [5] ﬁrmly estab-
lishes the subject of ML estimation for multivariate Gaussians, the authors never directly
address singular covariance estimation. This problem is explored in Section 3. In Section 2,
the pseudo determinant is introduced, and its derivative with respect to Hermitian matrices
is derived.
1This chapter was adapted from the paper Diﬀerentiating the pseudo determinant, published in the
journal Linear Algebra and its Applications ( c⃝2018 Elsevier). The author was sponsored by the UC Irvine
Graduate Deans Dissertation Fellowship award and acknowledged Oliver Knill for his helpful discussion and
advice.
49

3.2
The canonical derivative
We begin by introducing the pseudo determinant both as a product of eigenvalues and as a
limiting form.
Deﬁnition 3.1. The pseudo determinant Det of a square matrix A is deﬁned as the product
of its non-zero eigenvalues. If a matrix has no non-zero eigenvalues, then we say Det(0) = 1.
See [105] for an equivalent deﬁnition of the pseudo determinant in terms of the characteristic
polynomial. In deriving its derivative, it will be useful to write the pseudo determinant as a
limit.
Proposition 3.1. If A is an n × n matrix of rank k, then Det(A) is the limit
Det(A) = lim
δ→0
det(A + δI)
δn−k
(3.1)
for det(·) the regular determinant.
Whereas this result is known [128], we were unable to ﬁnd its proof, so it is given here in
the spirit of completeness.
Proof. We use the identity
det(X + ZY Z∗) = det(Y −1 + Z∗X−1Z) det(Y ) det(X) .
(3.2)
50

Replacing X with k In and letting A = UΛU ∗= ZY Z∗, we have
lim
δ→0
det(A + δI)
δn−k
= lim
δ→0
kn
kn−r det(Λ−1 + 1
kIr) det(Λ)
(3.3)
= Det(A) lim
k→0 kr det(Λ−1 + 1
kIr)
= Det(A) lim
k→0 det(kΛ−1 + Ir)
= Det(A) .
Next, we deﬁne the Moore-Penrose pseudo inverse [67], an important object involved in the
derivative of the pseudo determinant.
Deﬁnition 3.2. The pseudo inverse A+ of a matrix A is also deﬁned in terms of a limit:
A+ = lim
δ→0 A∗(AA∗+ δI)−1 = lim
δ→0(A∗A + δI)−1A∗.
(3.4)
A+ exists in general and is unique. It may also be deﬁned as the matrix satisfying all the
following criteria:
1. AA+A = A
2. A+AA+ = A+
3. (AA+)∗= AA+
4. (A+A)∗= A+A
For Hermitian matrices, the pseudo inverse is obtained by inverting the matrix eigenvalues.
51

As is the case for the pseudo inverse [67], the pseudo determinant is discontinuous. For an
example, consider the two matrices
A =



1
0
0
0


,
and
Bj =



0
0
0
j


.
(3.5)
Note that Det(A) = 1 and Det(A + Bj) = j, but that
lim
j→0 Det(A + Bj) = 0 ̸= 1 = Det(lim
j→0 A + Bj) .
(3.6)
As one might gather from this example, the pseudo determinant is discontinuous between sets
of matrices of diﬀering ranks. This discontinuity will eﬀect the way we deﬁne the derivative
of the pseudo determinant. We now turn to deriving this derivative.
For matrix A in the space of n × n matrices Mn×n, the matrix derivative of a function
h : Mn×n →R is given by the matrix ∇h(A) satisfying
∇Bh(A) = tr
 B∇h(A)

= lim
τ→0
h(A + τB) −h(A)
τ
(3.7)
for any matrix B ∈Mn×n, where ∇Bh(A) is the directional derivative. We use the direc-
tional derivative to deﬁne the derivative of the pseudo determinant, but, on account of the
discontinuity of the pseudo determinant, we must restrict the directions B in which the di-
rectional derivative is deﬁned. For this reason, we may deﬁne the derivative at a point only
in certain directions and must modify the common deﬁnition of the directional derivative.
Deﬁnition 3.3. (Deﬁnition 1) For a matrix A in the space of Hermitian n×n, rank k matrices
M k
n×n, the directional derivative ∇B Det(A) of the pseudo determinant Det : Mn×n →R is
deﬁned in directions B ∈M k
n×n that share the same kernel as A, i.e. for which Ker(A) =
52

Ker(B). Then the derivative ∇Det(A) is given by any matrix satisfying
∇B Det(A) = tr
 B∇Det(A)

= lim
τ→0
Det(A + τB) −Det(A)
τ
.
(3.8)
Note that, according to this deﬁnition, ∇Det(A) is not unique, since it can take on diﬀerent
values along the kernel of B. This non-uniqueness can also be seen using the following class
equations for the class of derivatives ∇Det(A) of the pseudo determinant at a matrix A.
Deﬁnition 3.4. (Deﬁnition 2) A derivative of the pseudo determinant at a point A ∈M k
n×n
is any non-zero matrix ∇Det(A) ∈M k
n×n satisfying
A ∇Det(A) = A A+ Det(A)
(3.9)
∇Det(A) A = A+A Det(A) .
(3.10)
We demonstrate that this is a natural deﬁnition using the facts that A(A2)+ = A+ and
(A2)+A = A+ for any Hermitian A and assuming one may interchange limits:
A1/2∇Det(A) = A1/2∇lim
δ→0
det(A + δI)
δn−k
(3.11)
= A1/2 lim
δ→0
1
δn−k ∇det(A + δI)
= Det(A) lim
δ→0 A1/2(A + δI)−1
= Det(A) (A1/2)+
= Det(A) A1/2A+ .
Multiplying both sides by A1/2 and rearranging gives the ﬁrst class equation. The derivation
of the second equation is symmetric. We illustrate the preceding deﬁnitions—and that they
do not deﬁne unique derivatives—with a few examples.
53

Example 3.1. Consider the 2 × 2 matrix
A =



a
0
0
0


.
(3.12)
It is clear that Det(A) = a and A+ is obtained by taking the reciprocal of the ﬁrst element
of A. The above result renders
A∇Det(A) = a AA+ =



a
0
0
0


= a A+A = A∇Det(A) .
(3.13)
Note that multiple matrices solve this equation. Two examples are the identity and the
matrix A/a.
Example 3.2. Now consider the 2 × 2 matrix pair
A =



1
1
1
1


,
A+ =



.25
.25
.25
.25


.
(3.14)
One can check that Det(A) = 2. Then it follows from the result that
A∇Det(A) = 2 AA+ = 2 1
2A = A = · · · = ∇Det(A)A .
(3.15)
Again, multiple matrices satisfy Equation (3.15): take for example



1
0
0
1



and



.5
.5
.5
.5


.
(3.16)
It turns out that the matrix A in the class equations of Deﬁnition (3.4) may be replaced
by any Hermitian B such that Ker(B) = Ker(A). This is easily shown using the fact that
BA+A = B = BAA+ = B = A+AB = B = AA+B for any such B.
54

Proposition 3.2. The derivative of the pseudo determinant is any matrix ∇Det(A) satis-
fying the equations
B ∇Det(A) = B A+ Det(A)
(3.17)
∇Det(A) B = A+B Det(A) ,
(3.18)
for any matrix B for which Ker(B) = Ker(A).
This result may be combined with the directional derivative based deﬁnition of ∇Det(A).
Proposition 3.3. The derivative of the pseudo determinant is any matrix ∇Det(A) satis-
fying the equations
tr
 B∇Det(A)

= Det(A) tr(BA+).
(3.19)
for any matrix B for which Ker(B) = Ker(A).
In practice, one may obtain the canonical element ∇Det(A) of class ∇Det(A) directly from
a corollary to the following Pythagorian theorem.
Theorem 3.1. (Knill 2014 [105]) For Hermitian A of rank k,
Det2(A) = Det(A2) =
X
P
det2(AP)
(3.20)
where P indexes all k × k minors of A satisfying det(AP) ̸= 0.
As a corollary, the canonical gradient ∇Det is directly obtainable.
Corollary 3.1. For Hermitian A of rank k, one has
∇Det(A) =
1
Det(A)
X
P
det2(AP)A−1
P =
P
P det2(AP)A−1
P
qP
P det2(AP)
:= ∇Det(A) .
(3.21)
55

This ∇Det(A) satisﬁes the class equations as well as Equation (3.19). Before proving this
claim, we illustrate by revisiting our examples.
Example 3.3. We again consider matrix
A =



a
0
0
0


.
(3.22)
This time we use Formula (3.21). Here, the rank k minors are simply the elements of A.
Since only the ﬁrst element is non-zero, we have
∇Det(A) = det2(A11) A−1
11
Det(A)
= a2
a



a−1
0
0
0


=



1
0
0
0


.
(3.23)
This, of course, agrees with the original example.
Example 3.4. Again, consider the matrix
A =



1
1
1
1


.
(3.24)
The gradient of the pseudo determinant may be found using Formula (3.21):
∇Det(A) = 1
2
X
ij
det2(Aij)A−1
ij = 1
2A .
(3.25)
The reader may check that
A ∇Det(A) = 1
2A2 = A = · · · = ∇Det(A)A ,
(3.26)
as expected from Equation (3.15).
56

The above examples suggest that ∇Det(A) should satisfy the class equations in general. To
show this, we ﬁrst cite a result.
Theorem 3.2. (Berg 1986 [15]) The pseudo inverse of a Hermitian, rank k matrix A takes
the following form:
A+ =
P
P det2(AP)A−1
P
Det2(A)
=
P
P det2(AP)A−1
P
P
P det2(AP)
.
(3.27)
Theorem 3.3.
∇Det(A) = Det(A)A+
(3.28)
Thus ∇Det(A) satisﬁes the class equations and belongs to the equivalence class ∇Det(A).
Moreover, ∇Det(A) is the unique member of the equivalence class that has the same kernel
as A. In this sense, it may be considered the canonical gradient of the pseudo determinant.
Proof. That ∇Det(A) = Det(A)A+ is a simple result of Corollary 3.1 and Theorem 3.2. As
a result, it immediately satisﬁes the two propositions as well.
We now consider the uniqueness claim. In general, A : Ker(A)⊥→Im(A) is an isomor-
phism, and A : Im(A) →Ker(A)⊥is its inverse. Since A is Hermitian, Ker(A) ⊕Im(A) =
Cn, and so A : Im(A) →Im(A), A+ : Im(A) →Im(A) is the isomorphism pair. Clearly
Ker(A) = Ker(A+), and so Ker
 ∇Det(A)

= Ker(A).
We proceed by contradiction.
Suppose that there exists another matrix B ̸= ∇Det(A)
satisfying Ker(A) = Ker(B) and
AB = A A+ Det(A)
(3.29)
BA = A+A Det(A) .
57

Since B ̸= A, there exists at least one element y ∈Cn such that By ̸= ∇Det(A)y. Since
Cn = Im(A)⊕Ker(A), we may consider y in each subspace separately. If y ∈Ker(A), then
By = 0 = ∇Det(A)y. Therefore y must be an element of Im(A). Then,
(B −∇Det(A))y = (B −∇Det(A))(AA+)y
(3.30)
= (BA −∇Det(A)A)A+y
= (A+A Det(A) −A+A Det(A))A+y
= 0 .
Then By = ∇Det(A)y, thus establishing a contradiction.
We round out this section with a few examples demonstrating applications of Formula (3.28).
Example 3.5. Let A be the constant, n×n matrix satisfying Aij = 1, ∀i, j = 1, . . . , n. Then
it is true that
Det(A) = n ,
and
A+ = 1
n2A .
(3.31)
Hence
∇Det(A) = Det(A)A+ = 1
nA .
(3.32)
Example 3.6. Let A = 0 be the n × n zero matrix for arbitrary integer n. The reader can
check that A+ = A = 0 by observing the four criteria in the deﬁnition of the pseudo inverse.
Recall also that Det(0) = 1 for any square matrix with no non-zero eigenvalues. It follows
that
∇Det(A) = Det(A)A+ = A = 0 .
(3.33)
58

This basic result is more appealing using the shorthand ∇Det(0) = 0.
Example 3.7. Consider the projection-dilation matrix
A =



a2
ab
ab
b2



(3.34)
that maps a point v ∈R2 onto the line through the origin containing the unit vector u =
(a, b)T/
p
(a2 + b2) while scaling by a2 + b2. The reader may check that
Det(A) = a2 + b2 ,
and
A+ =
1
(a2 + b2)2A .
(3.35)
We thus obtain the intriguing result
∇Det(A) =
1
a2 + b2A =
1
a2 + b2



a2
ab
ab
b2


=
1
(a, b)(a, b)T (a, b)T(a, b) ,
(3.36)
where the last form is meant to make clear that the result is the projection onto the subspace
spanned by (a, b)T.
The previous example touches on graph theory if we let (a, b) = (√c, −√c).
Example 3.8. Let L denote the Laplacian L = D −A of a weighted graph, where A is
the weighted adjacency matrix having zeros down the diagonal and oﬀ-diagonal elements
Aij equal to the value associated with the edge connecting nodes i and j. The matrix D is
diagonal and has elements satisfying Dij = P
i Aij = P
j Aij.
In the special case of a connected, two node graph with edge value c, the Laplacian is
L =



c
0
0
c


−



0
c
c
0


= c ·



1
−1
−1
1


.
(3.37)
59

Noting that L is a projection-dilation matrix (see prior example), we get
Det(L) = √c
2 + (−√c)2 = 2c ,
and
L+ = 1
4c2L ,
(3.38)
and thus, by Formula (3.28),
∇Det(L) = 2c
4c2L = 1
2cL = 1
2



1
−1
−1
1


.
(3.39)
The last term is half the Laplacian associated to the simple, unweighted graph obtained by
removing the weight c. Hence, ∇Det(L) takes graph connectivity into account but not scale.
3.2.1
The matrix diﬀerential
When obtaining matrix derivatives, it is often easiest to calculate the matrix diﬀerential dA
and then relate back to the gradient using the formula [123]
dh(A) = tr
 (dA) G

⇐⇒∇h(A) = G .
(3.40)
Combining this identity with directional derivative Formula (3.7), we see that Ker(dA) must
equal Ker(A) for the special case of the derivative of the pseudo determinant. It follows
that the matrix diﬀerential of the pseudo determinant is
d Det(A) = Det(A) tr
 A+(dA)

,
(3.41)
where we are implicitly selecting for the canonical gradient ∇Det(A) in order to satisfy
Ker(dA) = Ker(A). Equation (3.41) may also be derived directly using the spectral de-
composition A = UΛU ∗= Pk
j=1 λj uju∗
j for rank k, Hermitian A. The diﬀerential of an
eigenvalue of a Hermitian matrix A may be written in terms of the matrix diﬀerential itself
60

[123]:
dλ = tr
 uu∗(dA)

.
(3.42)
Theorem 3.4. The matrix diﬀerential of the pseudo determinant of Hermitian A ∈M k
n×n
is
d Det(A) = Det(A) tr
 A+(dA)

.
(3.43)
Proof. The result is proven directly using Formula (3.42).
d Det(A) = d
k
Y
j=1
λj
(3.44)
=
k
X
j=1
dλj
Y
i̸=j
λi
=
k
X
j=1
tr
 uju∗
j (dA)
 Y
i̸=j
λi
=
k
X
j=1
tr
  1
λj
uju∗
j (dA)

k
Y
i=1
λi
= Det(A)
k
X
j=1
tr
  1
λj
uju∗
j (dA)

= Det(A) tr
 k
X
j=1
1
λj
uju∗
j (dA)

= Det(A) tr
 A+(dA)

The reader should note that Theorem 3.4 could also be used to derive the canonical gradient
∇Det(A) via Formula (3.40).
61

3.3
An example from statistics
We now derive the maximum likelihood estimator (MLE) for the singular covariance of the
degenerate multivariate Gaussian distribution.
Thus, this section may be considered an
extension of the results found in [5]. The MLE may be incorporated into more advanced
statistical algorithms such as expectation maximization for image segmentation [181]. The
formulas derived in the following are also potentially useful in a Hamiltonian Monte Carlo
algorithm for Bayesian inference over reduced-rank covariance matrices (cf. [93]).
Let x1, . . . , xN follow a degenerate Gaussian distribution with mean µ and singular covariance
Σ. The probability density function of such a random variable xi is given by
f(xi; µ, Σ) = Det(2πΣ)−1/2 exp
 −1
2(xi −µ)TΣ+(xi −µ)

.
(3.45)
Assuming that µ is known, the log-likelihood ℓ(Σ) of Σ is proportional to
−N log
 Det(Σ)

−
N
X
i=1
(xi −µ)TΣ+(xi −µ) = −N log
 Det(Σ)

−tr
 Σ+R

,
(3.46)
where R is the matrix of residuals.
To obtain the MLE ˆΣ, we obtain the gradient of ℓ(Σ) and set it to zero, just as in the case
of a full-rank covariance matrix. To calculate the second term in the log-likelihood, we need
the formula for the matrix diﬀerential of the pseudo inverse [67]:
dΣ+ = −Σ+(dΣ)Σ+ + Σ+Σ+(dΣ)(I −ΣΣ+) + (I −Σ+Σ)(dΣ)Σ+Σ+ .
(3.47)
62

It follows that
dℓ(Σ) = −N tr
 Σ+(dΣ)

+ tr
 Σ+(dΣ)Σ+R

(3.48)
−tr
 Σ+Σ+(dΣ)(I −ΣΣ+)R

−tr
 (I −Σ+Σ)(dΣ)Σ+Σ+R

= −N tr
 Σ+(dΣ)

+ tr
 Σ+RΣ+(dΣ)

−tr
 (I −ΣΣ+)RΣ+Σ+(dΣ)

−tr
 Σ+Σ+R(I −Σ+Σ)(dΣ)

.
Setting dℓ(ˆΣ) = 0 and applying Formula (3.40), we get
N ˆΣ+ = ˆΣ+RˆΣ+ −(I −ˆΣˆΣ+)RˆΣ+ ˆΣ+ −ˆΣ+ ˆΣ+R(I −ˆΣ+ ˆΣ) ,
(3.49)
and multiplying both sides by ˆΣ on the right and the left gives
N ˆΣ = ˆΣˆΣ+RˆΣ+ ˆΣ −ˆΣ(I −ˆΣˆΣ+)RˆΣ+ ˆΣ+ ˆΣ −ˆΣˆΣ+ ˆΣ+R(I −ˆΣ+ ˆΣ)ˆΣ
(3.50)
= ˆΣˆΣ+RˆΣ+ ˆΣ .
This last line follows because the matrices ΣΣ+ and Σ+Σ are projections onto the range of
Σ and Σ+, and therefore (I −Σ+Σ) and (I −ΣΣ+) annihilate Σ. For the same reason, if we
are willing to assume that Ker(R) = Ker(Σ), this last equation is solved by
ˆΣ = 1
N
ˆΣˆΣ+RˆΣ+ ˆΣ = 1
N R .
(3.51)
Thus only with that key assumption are we able to reproduce the classical result for full rank
Σ. If we are not willing to make this assumption, i.e. if we have prior belief that, or have
set up our model in such a way that, the range of Σ is a predetermined subspace, then the
above equation may be written
ˆΣ = 1
N
ˆΣˆΣ+RˆΣ+ ˆΣ = ˆΣ = 1
N ΣΣ+RΣ+Σ .
(3.52)
63

Then ˆΣ is precisely the projection of the residual matrix R/N onto the range of Σ.
64

Chapter 4
Simplifying the geodesic Monte Carlo
65

Chapter Summary
Geodesic Monte Carlo (gMC) comprises a powerful class of algorithms for Bayesian
inference on non-Euclidean manifolds. The original gMC algorithm was cleverly de-
rived in terms of its progenitor, the Riemannian manifold Hamiltonian Monte Carlo
(RMHMC). Here, it is shown that an alternative, conceptually simpler derivation is
available which clariﬁes the algorithm when applied to manifolds embedded in Eu-
clidean space.
66

4.1
Introduction
Bayesian inference is hard. Bayesian inference on non-Euclidean manifolds is harder. Prior
to the publication of [25], a statistician required great ingenuity to compute the posterior
distribution for any model with non-Euclidean parameter space, and the algorithmic details
might change signiﬁcantly depending on the prior, the likelihood, and the constraints implied
by the non-Euclidean geometry. A good example of this approach is found in [88], where
the posterior distribution over the Stiefel manifold of orthonormal matrices is computed by
way of column-at-a-time Gibbs updates that rely on model speciﬁcations.
It is preferable, rather, that the same algorithm work for many diﬀerent kinds of models.
This is one of the strengths of Hamiltonian Monte Carlo [49] and its Riemannian extension,
RMHMC [66], which augments the posterior distribution π(q) by the random Gaussian
momentum p ∼N
 0, G(q)

, where G(q) is the metric tensor pertaining to the Riemannian
manifold over which the model is deﬁned. RMHMC simulates from the posterior distribution
by simulating the augmented canonical distribution with Hamiltonian
H(q, p) = U(q) + K(q, p) ∝−log π(q) + 1
2 log |G(q)| + 1
2pTG(q)−1p ,
(4.1)
i.e., U(q) is the negative log-posterior and K(q, p) is the negative logarithm of the probability
density function of Gaussian momentum p. Since the kinetic energy is not separable in q and
p, the system is not integrable using Euler’s method, so, in most cases, implicit integration
methods are required [66]. However, [25] point out that, for certain manifolds with known
geodesics, it is beneﬁcial to split the Hamiltonian into two parts and simulate the two systems
iteratively. Here, the ﬁrst Hamiltonian H[1] = −log π(q) + 1
2 log |G(q)| renders the equations
˙q = 0
˙p = ∇q
 log π(q) −1
2 log |G(q)|

,
67

and, crucially, the second Hamiltonian H[2] = 1
2pTG(q)−1p renders the geodesic dynamics for
the Riemannian metric’s Levi-Civita connection. Thus, the entire system may be simulated
by iterating between (1) perturbing the momentum and (2) advancing along the manifold
geodesics.
4.2
gMC on embedded manifolds
[25] extends the RMHMC formalism to posterior inference on manifolds embedded in Eu-
clidean space. In the following, this extension is referred to as the embedding geodesic Monte
Carlo (egMC). To maintain the RMHMC formalism, the authors begin by considering the
inference problem on the intrinsic manifold, where the Hausdorﬀmeasure
Hd(dq) =
p
|G(q)| λd(dq) ,
and not the Lebesgue measure λd(dq), is the base measure with respect to which the posterior
distribution is deﬁned1. Here, the RMHMC Hamiltonian (4.1) may be written
H(q, p) = −log πH(q) + 1
2pTG(q)−1p ,
for
log πH(q) = log π(q) −1
2 log |G(q)|
the log-posterior with respect to the Hausdorﬀbase measure.
Now, a clever change of
variables occurs using an isometric embedding as a tool.
An isometric embedding of a
1Whereas the ensuing derivation is extremely clever, it is unfortunate that it relies on an intrinsic concep-
tion of the inference problem, which, we will argue, causes confusion when the object of interest is a priori
deﬁned on the embedded manifold.
68

manifold Q into Euclidean space is a map x : Q →Rd satisfying
Gij(q) =
d
X
l=1
∂xl
∂qi (q)∂xl
∂qj (q) ,
or
G(q) = Jx(q)TJx(q)
for Jx(q) the Jacobian of the map x evaluated at q ∈Q. [25] use the isometric embedding to
make gMC practical on certain manifolds. This is accomplished by the change of variables
(q, p) 7→
 x(q), Mp

, with
M = Jx(q)
 Jx(q)TJx(q)
−1 = Jx(q)G(q)−1 .
If v = Mp, then the Hamiltonian H(q, p) becomes ([25], Equation (9))
H(x, v) = −log πH(x) + 1
2vTΠqv
(4.2)
= −log πH(x) + 1
2vTv
for Πq the projection matrix of the tangent space of the embedded manifold (at point q)
conceived of as a subspace of the ambient Euclidean space. The authors point out that “the
target density πH(x) is still deﬁned with respect to the Hausdorﬀmeasure of the manifold,
and so no additional log-Jacobian term is introduced,” and invite the reader to
[n]ote that by working entirely in the embedded space, we completely avoid the
coordinate system q and the related problems where no single global coordinate
system exists. The Riemannian metric G only appears in the Jacobian determinant
term of the density: in certain examples, this can also be removed, for example by
specifying the prior distribution as uniform with respect to the Hausdorﬀmeasure...
Indeed, this latter situation occurs for compact manifolds such as the Stiefel manifold and
its special cases, the sphere and the orthogonal group.
69

But how should one approach the more common scenario where the prior is deﬁned a priori
with respect to the Lebesgue measure of the embedded manifold? On the sphere, for example,
such priors include the uniform and Bingham-Von Mises-Fisher distributions.
Here, one
suspects that the log-Jacobian term should never be necessary, and this turns out to be the
case.
4.3
An alternative derivation
Let π(x) denote a target posterior density deﬁned directly on the embedded manifold. For
the unit sphere, this means that xTx = 1; for the Stiefel manifold of d × s orthonormal
matrices, this means that xTx = Is, for Is the identity matrix of the given dimension s. Let
Πx be the the orthogonal projection onto the tangent space of the embedded manifold at
point x. For example, for the sphere, this projection is given by
Πx = I −xxT ;
for the Stiefel manifold, the matrix is (see Appendix C.1)
Πx = Ids −1
2(Is2 ⊗x)(P + Ids)(Is2 ⊗xT) ,
for ⊗the Kronecker product and P the ds × ds permutation matrix for which P vec(x) =
vec(xT) for any matrix x. For simplicity, we take the sphere as our prime example and leave
the Stiefel manifold case for the appendix.
Let momentum p follow a degenerate Gaussian distribution on the tangent space to the
sphere at x, i.e. p ∼N(0, ΠxM Πx), where M is some positive semi-deﬁnite matrix. Then
70

at any point x, the density of p is proportional to
Det−1/2(ΠxM Πx) exp

−1
2pT(ΠxM Πx)+p

,
where Det(A) is the pseudo determinant and A+ is the pseudo inverse of matrix A. Then
the Hamiltonian is given by
H(x, p) = −log π(x) + 1
2 log Det(ΠxM Πx) + 1
2pT(ΠxM Πx)+p ,
for any pair x and p. Similarly to the original gMC algorithm, we split H(x, p) into two
Hamiltonians
H[1](x, p) = −log π(x) + 1
2 log Det(ΠxM Πx)
and
H[2](x, p) = 1
2pT(ΠxM Πx)+p .
Using some matrix calculus and the fact that ∇Det(A) = Det(A) A+ ([90], see Chapter 3),
the ﬁrst system gives the equations
˙x = 0
˙p = ∇x log π(x) −(ΠxM Πx)+ΠxMx .
71

Since the gradient ∇x log π(x) does not necessarily belong to the tangent space, we perform
the change of variables v = (ΠxM Πx)+p. The equations now read
˙x = 0
(4.3)
˙v = (ΠxM Πx)+ ∇x log π(x) −(ΠxM Πx)+ΠxMx

.
Velocity v stays on the tangent space at x because (ΠxM Πx)+ = Πx(ΠxM Πx)+Πx in general.
The second system may also be written in terms of v:
H[2](x, p) = 1
2pT(ΠxM Πx)+p
= 1
2pT(ΠxM Πx)+(ΠxM Πx)(ΠxM Πx)+p
= 1
2vT(ΠxM Πx)v
= 1
2˜vT ˜v := H[2](x, v) ,
where ˜v = (ΠxMΠx)1/2v. The system corresponding to H[2] is solved by the geodesic with
initial conditions (x, ˜v). Thus the system corresponding to H may be integrated by iteratively
integrating according to (4.3) and the spherical geodesics. The accept/reject step is easy
since the Hamiltonian is given explicity.
The formulas greatly simplify when M is the identity matrix. Since the pseudo determinant
is equal to unity on projection matrices, the Hamiltonian reduces to
H(x, v) = −log π(x) + 1
2vTv .
This is the same as Formula (4.2), but with π(x) replacing πH(x), the posterior with respect
to the Hausdorﬀmeasure. Hence, by working completely on the embedded manifold, we
are able to derive a Hamiltonian that does not depend on any notion of intrinsic geometry
72

whatsoever and thus avoids the log-Jacobian calculation of the embedding. The resulting
details are given by Algorithm 4.
Algorithm 4 Embedding geodesic Monte Carlo with non-trivial mass matrix
Let x = x(k) be the kth state of the Markov chain. The next sample is generated according
to the following procedure.
(a) Generate proposal state x∗:
1: v ∼N
 0, (ΠxM Πx)+
2: e ←−log π(x) + 1
2 log Det(ΠxM Πx) + 1
2vT(ΠxM Πx)v
3: x∗←x
4: for τ = 1, . . . , T do
5:
v ←v + ϵ
2(Πx∗M Πx∗)+ ∇x∗log π(x∗) −(Πx∗M Πx∗)+Πx∗Mx∗
6:
˜v ←(Πx∗MΠx∗)1/2v
7:
Progress (x∗, ˜v) along the geodesic ﬂow for time ϵ.
8:
v ←(Πx∗MΠx∗)−1/2˜v
9:
v ←v + ϵ
2(Πx∗M Πx∗)+ ∇x∗log π(x∗) −(Πx∗M Πx∗)+Πx∗Mx∗
10: end for
11: e∗←−log π(x∗) + 1
2 log Det(Πx∗M Πx∗) + 1
2vT(Πx∗M Πx∗)v
(b) Accept proposal with probability min{1, exp(e)/ exp(e∗)}:
1: u ∼U(0, 1)
2: if u < exp(e −e∗) then
3:
x ←x∗
4: end if
(c) Assign value x to x(k+1), the (k + 1)th state of the Markov chain.
4.4
Discussion
We have proposed an alternative derivation to the geodesic Monte Carlo for embedded
manifolds [25]. This derivation is conceptually simpler, as it does not rely on a notion of
intrinsic manifold geometry and thus clariﬁes the algorithm. Speciﬁcally, it becomes clear
that the inclusion of the log-Jacobian of the embedding in the Hamiltonian is unnecessary
in any case where the target distribution is deﬁned with respect to embedding coordinates.
The resulting class of algorithms are symplectic and completely explicit (do not require
implicit integration). The algorithm also allows for non-trivial mass matrix as an added
73

beneﬁt. Finally, the exposition hints how Metropolis adjustments may be incorporated into
geometric Langevin algorithms such as [117].
74

Chapter 5
Fisher geometry and Bayesian
nonparametric density estimation
75

Chapter Summary
It is well known that the Fisher information induces a Riemannian geometry on para-
metric families of probability density functions. Following recent work, we consider
the nonparametric generalization of the Fisher geometry. The resulting nonparametric
Fisher geometry is shown to be equivalent to a familiar, albeit inﬁnite-dimensional, ge-
ometric object—the sphere. By shifting focus away from density functions and toward
square-root density functions, one may calculate theoretical quantities of interest with
ease. More importantly, the sphere of square-root densities is much more computation-
ally tractable. This insight leads to a novel Bayesian nonparametric density estimation
model. We construct the χ2-process density prior by modeling the square-root density
with a restricted Gaussian process prior. Inference over square-root densities is fast,
and the model retains the ﬂexibility characteristic of Bayesian nonparametric mod-
els. Finally, we formalize the relationship between embedding geodesic Monte Carlo
(egMC) on the inﬁnite-dimensional sphere and standard Riemannian HMC.
76

5.1
Introduction
The Fisher information—and the geometry it induces—has been one of the unequivocal suc-
cess stories of geometry in statistics. Building on recent work, we extend the Fisher geometry
beyond parametric statistical models and show that the resulting geometry is equivalent to
that of the inﬁnite-dimensional sphere. The purpose of this chapter is to bring attention to
this new perspective and to demonstrate its theoretical and methodological consequences.
As an application, we introduce the χ2-process density prior, a ﬂexible nonparametric model
for Bayesian density estimation that admits fast computation while requiring minimal as-
sumptions.
The Fisher information matrix is canonical in statistics: it is rooted in information theory
[68]; it appears in Jeﬀrey’s prior of Bayesian analysis [101]; and it plays a central role
in Bayesian and Frequentist asymptotics [113].
Fisher advocated the importance of the
information matrix in maximum likelihood estimation [60]. Fisher’s student, Rao, was the
ﬁrst to place the information matrix in a diﬀerential geometric context [147]. Since then, the
diﬀerential geometric implications for parametric statistical models have been the subject
of extensive inquiry [4]. Recently, a number of researchers have drawn connections between
the Fisher geometry and the geometry of the inﬁnite sphere [160, 30, 100, 109, 161, 142].
Much of this work has been in the area of shape analysis and has focused on using the
Fisher geometry to measure distance between probability densities. Bayesian uses for the
nonparametric Fisher geometry were featured in [30], where Bayesian variational inference
was accomplished by minimizing the Fisher distance, and in [109], where the nonparametric
Fisher geometry was used for sensitivity analysis of Bayesian models. Here, we focus on
fully Bayesian nonparametric inference, including the generation of posterior samples using
Hamiltonian Monte Carlo (HMC). In contrast to recent research, the geodesics associated
with the nonparametric Fisher geometry are used to eﬃciently explore the MCMC state
space and not to measure or minimize the distance between density functions.
77

This chapter, and other recent research in the Fisher geometry, builds on the sub-ﬁeld
of square-root density estimation. [143] used a wavelet basis to estimate the square-root
density by eﬀectively ﬁtting the curve and then normalizing a sparse collection of wavelet
coeﬃcients, and [130] introduced a Bayesian follow-up to this work. Recently, [96] used
Riemannian geometry to ﬁt a square-root density model, but did not make any connections to
the Fisher geometry. More recently [142] performed square-root density estimation for object
recognition using minimum description length as ﬁtting criterion and used the nonparametric
Fisher geometry to obtain a closed-form expression of this criterion.
In this chapter, we focus on the application of the nonparametric Fisher geometry to Bayesian
inference for probability densities. While the density function is the object of interest, we
instead model the square-root density function, that is, the function the square of which
integrates to unity. We take a Bayesian nonparametric approach and endow the square-root
density with a Gaussian process (GP) prior [179, 9] multiplied by a Dirac measure limiting
its support to the inﬁnite-dimensional sphere. In order to maintain this restriction, it is
useful to use the Karhunen-Lo`eve (K-L) expansion [173] of the GP prior as opposed to its
kernel representation. Every GP with bounded second moment may be represented in terms
of the eigenfunction expansion of its covariance operator, but this (the K-L) expansion is
only explicitly known for a few classes of GPs [173]. Still, the K-L expansion has seen much
recent success in the realm of Bayesian inverse problems [40, 37] and has been featured
in inﬁnite-dimensional HMC and inﬁnite manifold HMC (∞-mHMC) [18]. The proposed
application of the K-L expansion to model the square-root density is unprecedented and
oﬀers a probabilistic interpretation to the use of basis expansions for density estimation.
Due to the orthonormality of the eigenfunction basis, the restriction to the (uncountably)
inﬁnite-dimensional sphere translates to a restriction to the (countably) inﬁnite-dimensional
sphere for the eigenvalues of the GP. Then, following the precedent set in [18], the K-L
expansion is truncated and the object of inference is reduced to the posterior distribution of
78

a ﬁnite number of K-L coeﬃcients restricted to a ﬁnite sphere. This computation is quick and
easy using egMC on the sphere ([25, 91], Chapter 4). Thanks to the basis representation,
computational complexity scales linearly with the number of data points, as opposed the
cubic rate of the GP density sampler [132]. Moreover, we show that—in the square-root
density estimation context—egMC on the sphere corresponds to Riemannian HMC in the
inﬁnite-dimensional limit.
Squaring the GP square-root density prior gives a χ2-process [cf. 145] density prior. We
illustrate the use of this prior for a number of problems.
The model is ﬂexible and its
posterior draws provide plausible realizations of the uncertainty inherent in the density
estimation problem. Besides a recent application to Bayesian quadrature [74], we are unaware
of statistical applications for the χ2-process and are therefore pleased to present its novel
application to Bayesian density estimation.
The contributions of this chapter are as follows:
• we review a nonparametric generalization of the Fisher geometry and show its relationship
to the inﬁnite-dimensional (L2) sphere, the space of square-root density functions;
• we derive the geodesics on the L2 sphere and use these geodesics to formalize the relation-
ship between Riemannian HMC and egMC on the inﬁnite-dimensional sphere;
• focusing on Bayesian nonparametric density estimation, we demonstrate the practical
beneﬁts to modeling the square-root density function. The resulting χ2-process density
prior performs well for a variety of problems and is eﬃciently computed using egMC.
The rest of the chapter is organized in the following way.
In Section 2 we review the
parametric Fisher geometry, present a nonparametric extension of the Fisher geometry, and
derive key results by relating this geometry to the inﬁnite-dimensional sphere. Section 3
presents the χ2-process density prior along with some necessary tools, such as the Karhunen-
79

Lo`eve expansion. In Section 4, we discuss eﬃcient Bayesian inference for the model and
relate Riemannian HMC to egMC on the inﬁnite-dimensional sphere. Empirical results are
presented in Section 5.
Finally, in Section 6 we discuss model limitations and possible
extensions.
5.2
The nonparametric Fisher geometry
5.2.1
The parametric Fisher geometry
Given data x in domain D, it is often useful to specify a probabilistic model S = {pθ =
p(x, θ) | θ = [θ1, . . . , θp]}, where θ is a vector parameterizing the model and taking values in
the continuous parameter space Θ. Then at any point θ ∈Θ, the Fisher information is the
expectation of the negative log-likelihood Hessian:
I(θ) = −Ex
∂2ℓ(θ)
∂θ∂θT

= −
Z
D
∂2ℓ(θ)
∂θ∂θT p(x|θ) µ(dx) ,
(5.1)
where ℓ(θ) = log p(x|θ). In the language of optimization, the Fisher information encodes
second-order functional information about ℓ(θ). This fact explains the use of the Fisher
information as a gradient preconditioning matrix in both (the Frequentist) Fisher scoring
[121] and (the Bayesian) Riemannian HMC [66]. The Fisher information may also be written
as the expected outer product of the score vector ∂log p(x|θ)/∂θ:
I(θ) = Ex
 ∂ℓ(θ)
∂θ
 ∂ℓ(θ)
∂θ
T
=
Z
D
 ∂ℓ(θ)
∂θ
 ∂ℓ(θ)
∂θ
T p(x|θ) µ(dx) .
(5.2)
The Fisher information is symmetric positive deﬁnite at any point θ ∈Θ. Taking note of
this fact, [147] interpreted the Fisher information matrix as a Riemannian metric tensor (see
Section 1.2.2), i.e. a smoothly varying, symmetric positive deﬁnite matrix deﬁned over the
80

parameter space Θ. In this way, the Fisher information matrix induces a Riemannian metric
gθ(·, ·) over Θ satisfying
gθ(ℓi, ℓj) = Iij(θ) ,
and
gθ(ψ, φ) =
X
i,j
ψiφjIij(θ)
(5.3)
for ℓi = ∂ℓ(θ)/∂θi, ψ = Pp
k=1 ψkℓk and φ = Pp
k=1 φkℓk. Hence, the Fisher information may
be thought of as inducing a non-trival geometry on the otherwise Euclidean parameter space
Θ. There has been much inquiry into the nature of the parametric Fisher geometry. Efron
used the Fisher geometry to prove the second-order eﬃciency of the MLE for exponential
family models [52], and [4] has constructed a body of work around the Fisher geometry
and its dual connections. More recently, [66] successfully used the Fisher geometry to guide
the Hamiltonian ﬂow of their Riemannian HMC. In this chapter, we take another tact by
generalizing the notion of the Fisher geometry to nonparametric models.
5.2.2
Beyond parametric models
We consider probability distributions over smooth manifolds D, of which D ∼= Rd is a special
case. Having ﬁxed a background measure µ, let
P :=

p : D →R | p ≥0,
Z
D
p(x) µ(dx) = 1

(5.4)
be the space of probability density functions over D. That is, P is the set of Radon-Nikodym
derivatives of probability measures that are absolutely continuous with respect to µ. The
following construction is agnostic to whether µ is the Lebesgue measure over D = Rd or the
Hausdorﬀmeasure over a general Riemannian manifold D = M.
We deal with the space P and do not ﬁx a parametric model. Instead we give P the structure
of an inﬁnite dimensional (formal) Riemannian manifold. First, we think of it as a smooth
81

manifold. Observe that for a given p ∈P, the tangent space can be identiﬁed with
TpP :=

φ ∈C∞(D) |
Z
D
φ(x) µ(dx) = 0

.
(5.5)
This identiﬁcation arises when one diﬀerentiates the unit measure condition on probability
density functions. That is, for a smooth curve pt : (−ϵ, ϵ) →P satisfying dpt/dt|t=0 = φ, we
have
0 = d
dt
Z
D
pt(x) µ(dx)

t=0 =
Z
D
dpt
dt (x) µ(dx) =
Z
D
φ(x) µ(dx) .
(5.6)
Now that we have a smooth manifold and an associated tangent space, we may deﬁne a
Riemannian metric, i.e. a smoothly varying, symmetric, non-degenerate, bilinear function
g(·, ·)p : TpP × TpP →R. Riemannian metrics are useful for developing a notion of distance
on a manifold that does not depend on any embedding in Euclidean space. One may deﬁne
uncountably many metrics on a general manifold, but we are interested in a generalization
of the parametric Fisher information metric.
Deﬁnition 5.1. ([160, 161]) Given D, the nonparametric Fisher information metric on
P(D)1 is
gF(φ, ψ)p :=
Z
D
φ(x)ψ(x)
p(x)
µ(dx).
(5.7)
This metric is a consistent generalization of the parametric Fisher information metric. To
see this, consider the parametric model p(x|θ), with θ as a vector. Then each element θi of
1From the deﬁnition, the nonparametric Fisher metric can take on inﬁnite values. It is possible to avoid
this by limiting the space of interest to strictly-positive density functions or by bounding the metric at an
arbitrarily large value. It is also possible to modify the deﬁnition of the tangent space to enforce tangent
functions to equal 0 when their respective densities do.
82

θ deﬁnes a curve Θi →P, where Θi is a slice of Θ, and
Iij(θ) =
Z
D
ℓiℓj p(x|θ)µ(dx) =
Z
D
pi(x|θ)
p(x|θ)
pj(x|θ)
p(x|θ) p(x|θ)µ(dx) =
Z
D
pi(x|θ)pj(x|θ)
p(x|θ)
µ(dx) .
(5.8)
Here, we have adopted the shorthand pi(x|θ) = ∂p(x|θ)/∂θi. Expressed in a more invariant
fashion, interpreting a model as a map θ : Θ →P, one has that the parametric Fisher metric
is induced by the nonparameteric Fisher metric, i.e.
θ∗gF = gθ.
(5.9)
In what follows we make a nontrivial change of variables suggested by this geometric picture
which provides various theoretical and computational simpliﬁcations. In particular, for var-
ious reasons the manifold P equipped with Riemannian metric (5.7) is not particularly easy
to deal with. In order to calculate geometric quantities of interest (e.g. geodesics, distances),
we shift focus to the L2 unit sphere, i.e. the space of square-root density functions
Q :=

q : D →R |
Z
D
q(x)2 µ(dx) = 1

.
(5.10)
This space, which is identiﬁed with P by a simple transformation indicated below, provides a
much simpler backdrop for calculations. This inﬁnite-dimensional L2 sphere is a surprisingly
familiar object. Its tangent spaces and geodesics are formally the exact same as those of the
ﬁnite dimensional sphere Sn−1, the only diﬀerence being the replacement of the Euclidean
inner product with the integral inner product of L2:
⟨f, h⟩L2 =
Z
D
f(x)h(x) µ(dx) .
(5.11)
83

Remarkably, this simpler space is isometric to the space of density functions equipped with
the nonparameteric Fisher metric deﬁned above.
Proposition 5.1. The map S : (P, gF) →(Q, ⟨·, ·⟩L2) deﬁned by S(p) := 2√p is a Rieman-
nian isometry.
Proof. We must show that ⟨S∗ψ, S∗φ⟩L2 = gF(ψ, φ)p, where S∗is the pushforward (or Jaco-
bian) of S:
S∗= dS
dp (p) = d(2√p)
dp
= 1
√p .
(5.12)
By direct computation,
⟨S∗ψ, S∗φ⟩L2 =
Z
D
(S∗ψ)(x) (S∗φ)(x) µ(dx) =
Z
D
ψ(x)
p
p(x)
φ(x)
p
p(x)
µ(dx)
(5.13)
=
Z
D
ψ(x)φ(x)
p(x)
µ(dx) = gF(ψ, φ)p .
In the remainder of this section we present a few basic results regarding the nonparametric
Fisher geometry, working with the L2 sphere model and transferring results to the traditional
Fisher geometry. We note that investigations of the nonparameteric Fisher information have
independently appeared in [160, 30, 100, 109, 161, 142]. We reproduce some fundamental
aspects of this geometry relevant to Theorem 1 (Section 5.4.1) for convenience. To begin we
observe how to describe the tangent space to Q.
Proposition 5.2. Given q ∈Q, one has that
TqQ :=

f : D →R |
Z
D
q(x)f(x) µ(dx) = 0

.
(5.14)
84

Proof. If qt : (−ϵ, ϵ) →Q denotes a path in Q satisfying dqt/dt|t=0 = f, then the unit
integration constraint on p = q2 means
0 = d
dt
Z
D
qt(x)2µ(dx)

t=0 = 2
Z
D
q0(x)dq
dt (x)

t=0 µ(dx) = 2
Z
D
q0(x)f(x)µ(dx) .
(5.15)
We next solve one version of the geodesic problem on P. In particular we consider an initial
point and velocity and solve for continuing the geodesic in that direction. We will exploit
the isometry between P and Q and solve ﬁrst in Q.
Proposition 5.3. Given q0 ∈Q and f ∈TqQ a unit vector, the geodesic with initial
condition q0 and velocity f exists on (−∞, ∞) and takes the form
qt = q0 cos t + f sin t.
(5.16)
Proof. First we derive the geodesic equation in Q. One conceptual method for obtaining
this, exploiting the spherical structure of Q, is to ﬁrst observe that if qt is a path in Q and
at ∈Tq(t)Q is a tangent vector along the curve, the Fisher geometry induces a covariant
derivative along the path via
D
∂ta = ˙a −q
Z
D
˙aq,
(5.17)
which is manifestly the time derivative of the family at projected to the tangent space at qt,
as expected. For a curve qt to be a geodesic, it should have zero acceleration, i.e.
0 = D
∂t ˙q = ¨q −q
Z
D
¨qq.
(5.18)
85

However, using that
R
D qt(x)2 µ(dx) = 1 for all q and diﬀerentiating twice in t, one sees that
this is equivalent to
¨q + q
Z
D
˙q2 = 0,
(5.19)
which we now take as the geodesic equation in Q. Another method for deriving this equation
is to solve for which curves are critical points for the length functional with ﬁxed endpoints.
Now, to solve this equation in our setting, ﬁrst let us observe that since f ∈Tq0Q, by Lemma
5.2 we have
Z
D
q0f = 0.
(5.20)
Using this and the fact that f is a unit vector we compute
d
dt
Z
D
˙q2 = 2
Z
D
¨q ˙q
(5.21)
= 2
Z
D
[−q0 cos t −f sin t] [−q0 sin t + f cos t]
= 2
Z
D

q2
0 −f 2
cos t sin t
= 0.
Thus
R
M ˙q2 =
R
D f 2 = 1. We then simply observe the ODE
¨q = −q,
(5.22)
and it is clear that q satisﬁes (5.19), and so the lemma follows.
We now translates this result into a corresponding one for geodesics in P.
86

Proposition 5.4. Given p0 ∈P and f ∈TpP a unit vector, the geodesic with initial
condition p0 and initial velocity f exists on (−∞, ∞), and takes the form
pt =
√p0 cos t +
f
2√p0
sin t
2
.
(5.23)
Proof. We use Lemma 5.3 and reinterpret the geodesic equation in terms of square-roots. In
this formalism the initial condition is q0 = √p0 and the initial velocity is
d
dtq = d
dt
√p =
f
2√p0
= f
2q0
.
These basic propositions show the advantage of working in Q, yielding a conceptual deriva-
tion of the geodesic equation.
As we will see below, not only is the L2 sphere Q more
theoretically tractable, it also turns out to be more computationally tractable. In the fol-
lowing sections, we take advantage of these two kinds of tractability to construct a Bayesian
nonparametric model on Q and use it for an application in density estimation.
5.3
The chi-square process density prior
In this section, we transition from the theoretical to the applied aspects of the nonparametric
Fisher geometry. We ﬁnd that the square-root representation q = √p is of use practically as
well as theoretically. Here we focus on its natural application for density estimation.
A good density estimate places more mass where there is more data but takes the ﬁnite
nature—and the uncertainty that comes with it—of that data into account. Bayesian non-
parametric density estimation eﬀects this balance: non-parametric models give ﬂexibility,
while the Bayesian prior contributes regularization. These methods model the data gen-
87

erating distribution as a random function, itself drawn from a speciﬁed stochastic process.
Dirichlet processes mixture models (DPMMs) convolve the Dirichlet process with a smooth
distribution, in eﬀect constructing an inﬁnite mixture model [6]. More recently, [132] pro-
posed a new method, called Gaussian Process Density Sampler (GPDS), oﬀering a similar
amount of ﬂexibility as the DPMM but having an arguably simpler framework. Nonethe-
less, inference for DPMMs requires an advanced Gibbs sampling routine [136], and inference
for the GPDS requires exchange sampling to handle the unit-integral restriction on the GP
model [132]. In contrast, the model we propose here can be computed using egMC. Further,
we take a diﬀerent approach from other Bayesian nonparametric density models by modeling
the square-root density function instead. In the previous section, theoretical results for the
nonparametric Fisher geometry were easier to obtain by ﬁrst obtaining the corresponding
results on the L2 sphere and then translating the results to the Fisher geometry. This theme
continues in application, where we show that Bayesian density estimation can be much easier
when one shifts focus to the sphere of square-root densities. We place a GP prior on the
square-root of the probability density function. This amounts to a χ2-process prior on the
density function itself.
Suppose we want to attribute a smooth density function to observed data x1, . . . , xn on ﬁnite
domain D ⊂Rd and recall the deﬁnitions (from Section 2) of the space of density functions
and the space of square-root density functions:
P :=

p : D →R | p ≥0,
Z
D
p(x) µ(dx) = 1

and
Q :=

q : D →R |
Z
D
q(x)2 µ(dx) = 1

,
(5.24)
respectively. We want to ﬁnd a suitable element p(·) ∈P(D), the space of functions over
domain D. Although this space contains the functions of interest, we opt to deal with the
space Q of square-root densities instead. As stated in the prior section, Q is the unit sphere
in the inﬁnite-dimensional Hilbert space L2(D). We model the square-root density with a
88

GP prior (or a Gaussian measure in L2) multiplied by the Dirac measure restricting the
function to the unit sphere:
q ∼GP × δq(Q) .
(5.25)
It turns out that it is much easier to enforce the constraint given by Dirac measure δq(Q)
than it is to enforce the corresponding constraint δp(P) (as is done for the GPDS). To do so,
however, we do not represent the GP prior using its kernel representation as is commonly
done in the literature [148]. We opt instead to represent q in terms of the eigenvalues and
orthonormal eigenfunctions of its covariance operator.
5.3.1
The Karhunen-Lo`eve representation
In order to tractably enforce the constraint δq(Q) in (5.25), it is helpful to write q as a
function (or linear sum of functions) for which we know the values of both
Z
D
q(x)µ(dx)
and
Z
D
q(x)2µ(dx) .
(5.26)
This condition is satisﬁed by representing random function q as a linear combination of
orthonormal basis functions. The K-L representation [173] provides a canonical way of doing
so and thus links our fully probabilistic approach to other square-root density methods that
rely on a basis [143, 130, 96]. Let u(·) ∼GP(0, K(·)) be a mean zero Gaussian process over
domain D with covariance operator K(·). Then u admits a K-L expansion of the form
u(·) =
∞
X
i=1
ui φi(·),
ui
ind
∼N(0, λ2
i ),
(5.27)
89

where the λis and the φis are respectively the eigenvalues and eigenfunctions of operator K.
That is to say, they satisfy
K(φi)(x′) =
Z
k(x, x′)φi(x)µ(dx) = λiφi(x′)
(5.28)
where k(·, ·) is the usual covariance kernel. The eigenvalues are decreasing and their sum-of-
squares is ﬁnite: λi+1 < λi, P∞
i=1 λ2
i < ∞. Finally, the eigenfunctions form an orthonormal
basis of L2:
Z
φi(x)φj(x)µ(dx) = 0,
and
Z
φ2
i (x)µ(dx) = 1 .
(5.29)
In this chapter, we model q as belonging to the Mat´ern class of GPs. For the Mat´ern class,
a closed-form orthonormal basis may be obtained from the eigenfunctions of the Laplacian
[31, 18]. The covariance operator is given by
K = σ2(α −∆)−s ,
(5.30)
where α and σ2 are positively constrained scale parameters, s is a smoothness parameter,
and ∆is the Laplacian Pd
i=1 ∂2
i . The eigenvalues and eigenfunctions corresponding to this
covariance operator depend on the area and dimensionality of domain D and are presented
in Section 5.5 below. It should be noted that the decision to use the Mat´ern class is entirely
dictated by ease of computation and does not preclude other classes of GP from being used
in future applications.
90

5.3.2
The model
The proposed density model is Bayesian nonparametric, i.e. we place a prior distribution on a
set of functions and eschew a restrictive parametric form. Given data x = (x1, · · · , xN) ∈D,
we obtain a posterior distribution, which is itself a distribution over the same set of functions
and is absolutely continuous with respect to the speciﬁed prior distribution. As stated above,
the prior π(q) on square-root density q ∈Q is a GP multiplied by the Dirac measure on the
L2 sphere. Following (5.27), the prior for q and the likelihood of the data x given q are given
by
π(q) ∝δq(Q)
∞
Y
i=1
exp
 −q2
i /(2λ2
i )

,
and
π(x|q) =
N
Y
n=1
q2(xn) ,
(5.31)
since q is the square-root density. This prior can also be interpreted as arising from an
inﬁnite-dimensional Bingham distribution on the coeﬃcients [48]. The posterior distribution
on q is then given by
π(q|x) =
π(x|q) π(q)
R
Q π(x|q) π(q) dq ∝π(q)
N
Y
n=1
q2(xn) .
(5.32)
Suppressing the Dirac measure, the log-posterior given data x1:N may be written in terms
of the K-L expansion (5.27) of q:
log π(q|x) ∝
N
X
n=1
log q(xn)2 −1
2
∞
X
i=1
q2
i /λ2
i
(5.33)
= 2
N
X
n=1
log |q(xn)| −1
2
∞
X
i=1
q2
i /λ2
i
= 2
N
X
n=1
log |
∞
X
i=1
qiφi(xn)| −1
2
∞
X
i=1
q2
i /λ2
i .
91

By modelling the square-root density q with a GP prior, we model the density function p
with a χ2-process prior. Modeling the density p as a χ2-process, we automatically enforce
the non-negativity requirement for probability density functions. On the other hand, χ2-
processes are not restricted to have unit integrals. We therefore rely on geodesic Monte
Carlo to restrict proposals to the L2 sphere. This is discussed in the following section.
5.4
Inference
Inference for the χ2-process density model is relatively straightforward and amenable to
geodesic Monte Carlo. In Section 5.4.1, we show that, in this context, inﬁnite-dimensional
spherical egMC is equivalent to Riemannian HMC using the parametric Fisher information.
In practice, we follow [18] and truncate2 the K-L expansion of the GP square-root density
prior for an integer I using truncation operator TI:
TI
 q(x)

= TI

∞
X
i=0
qi φi(x)

=
I
X
i=0
qi φi(x) .
(5.34)
Due to the orthonormality of the basis φi, the unit integral constraint on TI(q)2 translates
directly to a spherical constraint on the random coeﬃcients qI = (q0, · · · , qI). That is,
1 =
Z
D
TI
 q(x)
2µ(dx) =
Z
D

I
X
i=0
qi φi(x)
2
µ(dx) =
I
X
i=0
q2
i
Z
φi(x)2µ(dx) =
I
X
i=0
q2
i
(5.35)
where the penultimate equality is given by the orthogonality of the basis elements and the
last equality is on account of the basis elements being normal.
Thus, inference can be
2We note that one may conceivably place a prior on the truncation index I and thus avoid having to
choose the number of eigenfunctions. This would provide for an interesting extension of the model presented
here, but would necessitate new MCMC techniques that enable the change of model dimensionality (e.g.
reversible jump MCMC [70]) while maintaining manifold constraints. Hence, we leave this for future work.
92

performed over the coeﬃcients qI by using egMC (see Chapter 4) on the sphere SI. The
method augments the state space with an auxiliary velocity variable v (satisfying vTqI = 0)
and simulate from a Hamiltonian system by splitting [153] the Hamiltonian of interest (H)
into two Hamiltonians (H[1] + H[2]):
H(qI, v) = −log π(qI) + 1
2vTv
(5.36)
H[1](qI, v) = −log π(qI)
H[2](qI, v) = 1
2vTv .
Here π is the posterior distribution and vTv = vTΠqv, for Πq the matrix projecting onto the
tangent space of the sphere at q. See Chapter 4 for details. Simulating from H[1] involves a
small perturbation of the velocity by the gradient of H[1] with respect to qI; simulating H[2]
involves moving along the sphere’s geodesics in the direction v. This last fact is relevant to
the discussion of the following section.
egMC requires the gradient of the log-posterior with respect to the coeﬃcients. Elementwise,
this is given by
∂
∂qj
log π(qI|x) = 2
N
X
n=1
∂
∂qi
log |
I
X
i=1
qiφi(xn)| −1
2
∂
∂qj
I
X
i=1
q2
i /λ2
i
(5.37)
= 2
N
X
n=1
φj(xn)
PI
i=1 qi φi(xn)
−qj/λ2
j .
The Markov chain may be initialized using Newton’s method on the sphere (see Appendix
D.1).
Since the values of the eigenfunctions at the observations may be precomputed, the main
computational burden is in the summations involved in the evaluation of the log-posterior
and its gradient. Since in practice I ≪N, these computations are O(N), where N is the
93

number of data points.
This is orders faster than the O(N 3) computations required to
perform inference for the GPDS [132].
5.4.1
Inference in the limit
egMC uses geodesic ﬂows on the ﬁnite dimensional sphere to propose new Markov chain
states.
Since these ﬂows are formally equivalent to the geodesic ﬂows on the L2 sphere
(see Section 5.2) and since the natural geometry on L2 is equivalent to the nonparametric
Fisher geometry, it is worth asking whether these inference schemes are adapted to the
nonparametric Fisher geometry in a similar way to Riemannian HMC’s adaptation to the
parametric Fisher geometry3.
Indeed this is the case, and it is a simple consequence of Proposition 5.1 and the isometric
relationship between square-integrable functions and square-summable sequences induced by
any orthonormal basis {φi}∞
i=1 with completion L2. Denote the space of square-summable
sequences and its sphere
ℓ2 =
(
q = {qi}∞
i=1
 ⟨q, q⟩ℓ2 =
∞
X
i=1
q2
i < ∞
)
,
S∞=
(
q ∈ℓ2 ⟨q, q⟩ℓ2 =
∞
X
i=1
q2
i = 1
)
.
(5.38)
Then it follows from the orthonormality of {φi}∞
i=1 that (L2, ⟨·, ·⟩L2) ∼= (ℓ2, ⟨·, ·⟩ℓ2), since for
any arbitrary function q = q(·) ∈L2,
⟨q, q⟩L2 =
Z
q(x)2µ(dx) =
Z  ∞
X
i=1
qiφi(x)
2µ(dx) =
∞
X
i=1
q2
i = ⟨q, q⟩ℓ2 .
(5.39)
3By Riemannian HMC, we mean Riemannian HMC where the Riemannian metric is the ﬁnite Fisher
metric, as this is the most common usage. We note that it is theoretically possible to use other metrics
[25, 93].
94

It is an immediate result that the respective spheres are also isometric, i.e. (Q, ⟨·, ·⟩L2) ∼=
(S∞, ⟨·, ·⟩ℓ2), and hence, by Proposition 5.1, the following result holds.
Lemma 5.1. Given an orthonormal basis for L2, the space of density functions equipped
with the Fisher metric is isometric to the sphere S∞with its natural Euclidean metric, i.e.
(P, gF(·, ·)) ∼= (S∞, ⟨·, ·⟩ℓ2).
Our goal is to show that egMC on the sphere is adapted to the nonparametric Fisher geometry
in the inﬁnite-dimensional limit. Given that the geodesic paths followed by egMC converge
to geodesics on S∞, Lemma 5.1 will imply that these paths correspond to geodesics on
(P, gF(·, ·)).
Lemma 5.2. Geodesic ﬂows on the ﬁnite sphere SI−1 converge to geodesic ﬂows on the
inﬁnite-dimensional sphere S∞as I →∞.
Proof. For any point q ∈S∞, let qI ∈SI−1 be vector obtained by applying the truncation
operator to q and then normalizing:
qI =
TI(q)
∥TI(q)∥=
(q1, . . . , qI)T
p
(q1, . . . , qI)(q1, . . . , qI)T .
(5.40)
Similarly, for any vector in the tangent space to S∞
v ∈TqS∞=
(
v ∈ℓ2 ⟨v, q⟩ℓ2 =
∞
X
i=1
qivi = 0
)
(5.41)
let vI ∈TqISI−1 be the I-dimensional vector obtained by truncating v, projecting onto the
tangent space TqISI−1, and scaling such that ∥v∥ℓ2 = ∥vI∥(where ∥· ∥is the Euclidean
norm):
˜vI = TI(v) −qI⟨qI, TI(v)⟩ℓ2 ,
and
vI = ˜vI ∥v∥ℓ2
∥˜vI∥.
(5.42)
95

It follows from the deﬁnition of truncation (Equation (5.34)) that qI →q and vI →v with
respect to ⟨·, ·⟩ℓ2 as I →∞.
Next, let t 7→(q(t), v(t)) be the geodesic ﬂow on S∞with initial position q0 = q(0) and
initial velocity v0 = v(0) ∈Tq0S∞. Let t 7→(qI(t), vI(t)) be the analogous ﬂow on the
tangent bundle TSI−1, where qI
0 and vI
0 are obtained from q0 and v0 following Formulas
(5.41) and (5.42), respectively. Denote the distance between ﬂows at time t
f(t) = ∥qt −qI
t ∥2
ℓ2 + ∥˙qt −˙qI
t ∥2
ℓ2 .
(5.43)
Our goal is to show that
lim
I→∞
Z T
0
f(t) dt = 0 ,
(5.44)
for any ﬁnite T, and hence that geodesic ﬂows on the ﬁnite sphere converge to those on S∞.
Begin by bounding ˙f(t) by a constant times f(t):
d
dtf(t) = 2
 ⟨qt −qI
t , ˙qt −˙qI
t ⟩ℓ2 + ⟨˙qt −˙qI
t , ¨qt −¨qI
t ⟩ℓ2
(5.45)
= 2
 ⟨qt −qI
t , ˙qt −˙qI
t ⟩ℓ2 + ⟨˙qt −˙qI
t , −qt∥˙qt∥2
ℓ2 + qI
t ∥˙qI
t ∥2⟩ℓ2
.
Here, the second line follows from the geodesic formula. Noting that ∥˙qt∥2
ℓ2 = ∥˙q0∥2
ℓ2, ∥˙qI
t ∥2 =
∥˙qI
0∥2, and that (by Equation (5.42)) ∥˙qI
0∥2 = ∥˙q0∥2
ℓ2, we get
d
dtf(t) = 2
 ⟨qt −qI
t , ˙qt −˙qI
t ⟩ℓ2 −⟨˙qt −˙qI
t , qt −qI
t ⟩ℓ2∥˙q0∥2
ℓ2

(5.46)
= 2
 1 −∥˙q0∥2
ℓ2

⟨qt −qI
t , ˙qt −˙qI
t ⟩ℓ2 .
96

We obtain our bounds by noting that
0 ≤∥qt −qI
t ∥2
ℓ2 −2⟨qt −qI
t , ˙qt −˙qI
t ⟩ℓ2 + ∥˙qt −˙qI
t ∥2
ℓ2
(5.47)
= f(t) −2⟨qt −qI
t , ˙qt −˙qI
t ⟩ℓ2
(5.48)
= f(t) −
˙f(t)
1 −∥˙q0∥2
ℓ2
,
and hence that
d
dtf(t) ≤
 1 −∥˙q0∥2
ℓ2

f(t) .
(5.49)
Integrating gives
f(t) ≤f(0) et (1−∥˙q0∥2
ℓ2) .
(5.50)
Since, by deﬁnition, f(0) →0 as I →∞, we have
Z T
0
f(t) dt ≤f(0)
Z T
0
et (1−∥˙q0∥2
ℓ2) dt
(5.51)
= c f(0) −→0 .
Thus we have proven the convergence of geodesic ﬂows on the ﬁnite sphere to those on
S∞.
5.5
Empirical results
Here we apply the χ2-process density model to both simulated and real-world data. As
stated in Section 5.3.1, the eigen-pairs corresponding to the GP with covariance operator
(5.30) depend on both the dimension and the area of D. When D is the one-dimensional
97

|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
||
|
|
|
|
|
|
|
|
|
|
|
|
||
|
|
|
|
|
|
|
|
|
|
|
|
| |
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
||
|
|
|
|
|
|
|
|
||
|
|
|
|
|
|
|
|
|
|
|
||
|
||
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
| |
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
||
|
|
|
|
|
|
|
||
|
|
|
|
|
|
||
|
| |
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|| |
|
|
|
|
|
|
|
|
||
|
|
|
|
|
|
||
|
|
|
|
|
|
|
|
||
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
||
|
|
|
|
|
|
||
|
|
||
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
| |
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
||
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
| |
|
|
|
|
|
|
| |
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
| |
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
||
|
|
|
|
|
|
|
|
|
|
|
|
|
||
|
|
|
|
|
||
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
||
|
|
|
|
|
|
| ||
|
|
| |
| |
|
|
|
|
|
|
||
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
||
|
| |
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
||
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
||
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
| |
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
||
| |
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
| |
|
|
|
|
|
|
||
|
|
|
|
|
||
|
|
|
|
|
| |
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
||
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
||
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
| |
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
| |
|
|
|
|
|
||
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
| |
|
|
|
| |
|
|
|
|
|
|
|
|
|
||
|
||
|
| |
|
|
||
|
|
|
|
||
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
||
||
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
||
|
|
|
|
|
|
|
|
|
||
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
||
|
|
0
1
2
3
0.00
0.25
0.50
0.75
1.00
Density values
|
|
|
||
|
|
|
|
|
|
|
|
|
| |
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
||
|
|
|
|
|
||
|
|
|
|
|
|
|
|
|
|
|
|
||
|
|||
|
||
|
|
|
|
|
|
||
| |
|
|
|
|
|
|
|
| |
|
|
|
|
|
|
|
|||
|
|
|
|
|
|
|| |
||
||
||
|
|
|
|
|
|
|
||
|
|
|
|
|
|
|
|
|
|
|
|
|| ||
| |
| |
|
|
|
|
|
| |
|
|
|
|
|
| |
|
|
|
|
|
|
|
| |
|
||
|
||
|
|
|
| |
||
|
|
|
|
|
|
|
||
|
|
|
|
|
||
|
|
||
|
|
|
|
|
|
|
|
|
|
| |
|
|
|
|
|
|
||
|
|
|
|
|
|
|
|
|
||
|
|
|
|
|
||
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
||
|
|
|
|
|
|
|
|
|
|
|
|||
|
|
|
|
||
|
|
|
|
|
|
|
| |
||
|
|
|
|
|
||
|
|
|
|
|
| | |
||
|
|
||
|
|
|
|
|
|
|
|
|
||
|
|
|
||
|
|
| |
|
|
|
|
|
|
|
|
|
|
|
|
|||
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
||
|
|
|
|
|
||
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
||
|
||
|
|
|
|
||
| |
|
||
|
||
|
| ||
|
|
|
|
|
|
| |
|||
||
|
|
||
|
||
|
|
|
|
|
|
|
|
|
|
| |
| |
|
|
|||
||
|
|
|
||
|
||
|
|
|
| |
|
| ||
|
|
|
|
|
|
|
|
|
| |
|
|
|||
|
|
|
||
||
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|||
|
|
|
|
|
|
| |
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
| |
|
|
|
|
|
|
|
||
|
||
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
| | |
|
|
|||
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|| ||
|
| |
|
|
|
|
|
|
|
|
|
|
|
||
|
|
|| |
|
|
|
|| |
||
|
|
|
|
|
|
|
||
|
||
|
|
|
|
|
|
|
|
|
|
|| |
|
|
|
|
|
|
|
|
||
||
|
|
|
|
||
|
|
|
|
|
|
|
|
|
|
||
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
||
|
|
||
|
|
|
|
|
|
|
|
|
|
|||
|
|
|
||
|
|
|
| |
| |
|
|
|
|
| |
| |
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
| |
|
|
|
|
|
|
| |
| |
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|||
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|||
|
|
|
|
|
|
|
|
|
|||
|
|
|
|
|
|
|
|
|
|
|
|
|
||
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
||
||
|
|
|
|
|
|
|
|
|
|
|
||
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
||
||
|
|
||
|
|
|
|
| |
|
|
|
|
|
||
|
|
|
|
|
|
|
|
|
|
|
|
| |
| ||
|
|
||
|
|
|
|| |
|
|
|
| |
|
|
|
|
|
|
|
||
|
|
|
|
| |
|
|
0
1
2
3
0.00
0.25
0.50
0.75
1.00
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
| | |
|
|
|
|
| | |
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
| |
|
|
|
| |
|
|
|
|
|
|
|
|
|
|
|
|
|
|
||
||
|
|
|
|
|
| |
|
|
|
| |
|
|
|
|
|
|
|
|
|
|
||
|
|
|
|
||
|
|
|
|
|
|
|
|
|
|
||
|
|
|
|
|
||
|
|
|
|
|
||
|
|
|
|
|
|
|
|
| |
|
|
|
|
|
|
|
|
|
|
|
|
|
|
| ||
|
|
||
||
|
|
|
|
|
|
|
|
|
|
|
|
|
|
||
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
| |
|
|
|
|
|
|
|
|
|
|
|
|
||
|
||
|
|
||
|
|
|
|
||
|
||
|
|
|
|
||
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
||
|
|
|
|
|
|
|
|
|
|
|
|
|
|
| |
|||
|
|
|
|
|
|
|
|
|
|
|
|
|
| |
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
||
|| |
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
||
|
|
|
|
|
|
|
|
|
|
| |
|
|
|
|
|
|
|
|
| |
|
|
|
|
|
|
|
|
|
|||
|
|
|
|
| |
|
|
|
|
|
|
|
|||
|
|
|
|
|
|
|
|
|
|
|
|
||
|
|
|
|
|
|
|
|
|
||
|
|
|
||
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
||
| |
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
||
|
|
|
|
|
|
||
|
|
|
|
|
|
|
| |
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
| |
|
|
|
|
|
| |
|
|
||
|
|
|
|
|
||
|
||
||
|
||
||
||
|
|
|
|
|
|
|
|
| |
|
|
|
|
|
|
| |
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|| |
|
|
||
|
|
|
|
| |
|
|
|
|
|
|
|
|
|
|
|
|
||
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
||
|
|
|
|
|
|
|
| |
| |
|
|
| |
|
|
|
|
|
|
|
|
| |
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
||
|
||
|
|
|
|
||
|
|
|
|
|
|
|
|
|
|
||
|
|
|
|
|
|
|
|
|
||
|
|
|
||
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
||
|
|
|
|
|
|
|
|
||
|
|
|
|
|
|
|
|
|||
|
|
|
||
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
||
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
||
|
|
|| |
|
|
|
|
|
|
|
||
|
|
|
|
| |
|
|
|
|
|
||
|
|
|
|
|
|
||
|
|
|
|
|
|
|
|
|
|
| |
|
| |
|
|
| |
|
|
|
| |
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
||
| |
||
|
||
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
||
|
|
|
|
|
|
|
|
|
|
|
||
|
| |
|
|
|
|
|
|
|
0
1
2
3
0.00
0.25
0.50
0.75
1.00
Support
Density values
|
|
|
|
||
| |
|
|
|
|
|
|
|
|
|
| |
|
|
|
|
|
|
|
|
|
||
|
|
| | |
|
|
|
|| |
||
|
|
|
|
|
|
|
|
|
|
|
|
|
|
| |
|
|
|
|
|
|
|
|
|
|
|
| | |
|
|
|
|
|
|
|
|
|
|
|
| |
|
|
|
|
| |
|
|
|
|
|
| |
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
||
|
|
|
|
|
|
||
|
|
|
|
| | |
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|| |
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
||
|
|
||
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|||
|
|
|
|
||
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|||
|
|
|
|
|
|
|
|
|
|
|
| |
|
|
|
|
|
|
|
|
|
| |
|
|
|
|
|
|
|
|
|
|
|
|
| |
|
|
|
|
|
|
|
| |
|
|
|
|
|
||
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
| |
|
|
|
| |
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
| |
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
||
|
| |
|
|
|
|
|
|
|
|
|
||
|
|
|
|
|
|
|
|
|
|
|
|
|
| |
|
|
|
|
|
|
|
||
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
||
|
|
||
|
|
|
|
|
|
|
|
|
| |
|
|
|
|
|
|
|
|
| |
|
|
| |
|
|
|
| |
|
|
|
|
|
|
|
|
|
|
|
||
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|||
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
||
|
|| ||
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
| |
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
| |
|
|
|
|
|
|
|
|
|
|
|
|
|
|
||
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
| || |
|
|
|
|
|
|
|
|
|
|
|
|
| |
|
|
|
|
|
|
|
|
|
|
|
||
|
|
|
|
|
|
|
|
||
|
|
|
|
|
|
|
|
|
|
| |
||
|
|
|
|
|
|
|
||
|
|
|
|
|
|
|
|
|| |
|
|
|
|
|
||
|
|
|
|
|
|
|
||
|
|
|
|
|
|
|
|
|
||
|
| |
|
| |
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
||
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
||
|
|
|
|
|
|
|
|
|
|
|
|
|
||
|
|
| |
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
| |
|
|
| |
|
|
|
|
|
|
|
||
|
|
|
|
|
|
| |
|
|
|
| |
|
|
|
|
|
|
|
|
| |
|
|
|
|
||
|
|
|
|
|
|
|
|
|
| |
|
|
|
|
|
||
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
| |
|
|
|
|
| |
|
|
|
|
| |
| |
|
|
|
|
|
|
|
|
|
|
|
|
||
|
| |
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
| |
|
||
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
| |
|
|
|
|
|
|
|
|
|
|
|
0
1
2
3
0.00
0.25
0.50
0.75
1.00
Support
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
||
|
|
|
|
|
|
|
|
|
|
|
|
||
|
|
|
|
|
|
|
|
|
|
|
|
| |
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
||
|
|
|
|
|
|
|
|
||
|
|
|
|
|
|
|
|
|
|
|
||
|
||
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
| |
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
||
|
|
|
|
|
|
|
||
|
|
|
|
|
|
||
|
| |
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|| |
|
|
|
|
|
|
|
|
||
|
|
|
|
|
|
||
|
|
|
|
|
|
|
|
||
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
||
|
|
|
|
|
|
||
|
|
||
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
| |
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
||
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
| |
|
|
|
|
|
|
| |
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
| |
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
||
|
|
|
|
|
|
|
|
|
|
|
|
|
||
|
|
|
|
|
||
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
||
|
|
|
|
|
|
| ||
|
|
| |
| |
|
|
|
|
|
|
||
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
||
|
| |
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
||
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
||
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
| |
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
||
| |
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
| |
|
|
|
|
|
|
||
|
|
|
|
|
||
|
|
|
|
|
| |
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
||
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
||
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
| |
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
| |
|
|
|
|
|
||
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
| |
|
|
|
| |
|
|
|
|
|
|
|
|
|
||
|
||
|
| |
|
|
||
|
|
|
|
||
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
||
||
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
||
|
|
|
|
|
|
|
|
|
||
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
||
|
|
0
1
2
3
0.00
0.25
0.50
0.75
1.00
Density values
|
|
|
||
|
|
|
|
|
|
|
|
|
| |
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
||
|
|
|
|
|
||
|
|
|
|
|
|
|
|
|
|
|
|
||
|
|||
|
||
|
|
|
|
|
|
||
| |
|
|
|
|
|
|
|
| |
|
|
|
|
|
|
|
|||
|
|
|
|
|
|
|| |
||
||
||
|
|
|
|
|
|
|
||
|
|
|
|
|
|
|
|
|
|
|
|
|| ||
| |
| |
|
|
|
|
|
| |
|
|
|
|
|
| |
|
|
|
|
|
|
|
| |
|
||
|
||
|
|
|
| |
||
|
|
|
|
|
|
|
||
|
|
|
|
|
||
|
|
||
|
|
|
|
|
|
|
|
|
|
| |
|
|
|
|
|
|
||
|
|
|
|
|
|
|
|
|
||
|
|
|
|
|
||
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
||
|
|
|
|
|
|
|
|
|
|
|
|||
|
|
|
|
||
|
|
|
|
|
|
|
| |
||
|
|
|
|
|
||
|
|
|
|
|
| | |
||
|
|
||
|
|
|
|
|
|
|
|
|
||
|
|
|
||
|
|
| |
|
|
|
|
|
|
|
|
|
|
|
|
|||
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
||
|
|
|
|
|
||
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
||
|
||
|
|
|
|
||
| |
|
||
|
||
|
| ||
|
|
|
|
|
|
| |
|||
||
|
|
||
|
||
|
|
|
|
|
|
|
|
|
|
| |
| |
|
|
|||
||
|
|
|
||
|
||
|
|
|
| |
|
| ||
|
|
|
|
|
|
|
|
|
| |
|
|
|||
|
|
|
||
||
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|||
|
|
|
|
|
|
| |
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
| |
|
|
|
|
|
|
|
||
|
||
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
| | |
|
|
|||
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|| ||
|
| |
|
|
|
|
|
|
|
|
|
|
|
||
|
|
|| |
|
|
|
|| |
||
|
|
|
|
|
|
|
||
|
||
|
|
|
|
|
|
|
|
|
|
|| |
|
|
|
|
|
|
|
|
||
||
|
|
|
|
||
|
|
|
|
|
|
|
|
|
|
||
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
||
|
|
||
|
|
|
|
|
|
|
|
|
|
|||
|
|
|
||
|
|
|
| |
| |
|
|
|
|
| |
| |
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
| |
|
|
|
|
|
|
| |
| |
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|||
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|||
|
|
|
|
|
|
|
|
|
|||
|
|
|
|
|
|
|
|
|
|
|
|
|
||
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
||
||
|
|
|
|
|
|
|
|
|
|
|
||
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
||
||
|
|
||
|
|
|
|
| |
|
|
|
|
|
||
|
|
|
|
|
|
|
|
|
|
|
|
| |
| ||
|
|
||
|
|
|
|| |
|
|
|
| |
|
|
|
|
|
|
|
||
|
|
|
|
| |
|
|
0
1
2
3
0.00
0.25
0.50
0.75
1.00
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
| | |
|
|
|
|
| | |
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
| |
|
|
|
| |
|
|
|
|
|
|
|
|
|
|
|
|
|
|
||
||
|
|
|
|
|
| |
|
|
|
| |
|
|
|
|
|
|
|
|
|
|
||
|
|
|
|
||
|
|
|
|
|
|
|
|
|
|
||
|
|
|
|
|
||
|
|
|
|
|
||
|
|
|
|
|
|
|
|
| |
|
|
|
|
|
|
|
|
|
|
|
|
|
|
| ||
|
|
||
||
|
|
|
|
|
|
|
|
|
|
|
|
|
|
||
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
| |
|
|
|
|
|
|
|
|
|
|
|
|
||
|
||
|
|
||
|
|
|
|
||
|
||
|
|
|
|
||
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
||
|
|
|
|
|
|
|
|
|
|
|
|
|
|
| |
|||
|
|
|
|
|
|
|
|
|
|
|
|
|
| |
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
||
|| |
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
||
|
|
|
|
|
|
|
|
|
|
| |
|
|
|
|
|
|
|
|
| |
|
|
|
|
|
|
|
|
|
|||
|
|
|
|
| |
|
|
|
|
|
|
|
|||
|
|
|
|
|
|
|
|
|
|
|
|
||
|
|
|
|
|
|
|
|
|
||
|
|
|
||
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
||
| |
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
||
|
|
|
|
|
|
||
|
|
|
|
|
|
|
| |
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
| |
|
|
|
|
|
| |
|
|
||
|
|
|
|
|
||
|
||
||
|
||
||
||
|
|
|
|
|
|
|
|
| |
|
|
|
|
|
|
| |
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|| |
|
|
||
|
|
|
|
| |
|
|
|
|
|
|
|
|
|
|
|
|
||
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
||
|
|
|
|
|
|
|
| |
| |
|
|
| |
|
|
|
|
|
|
|
|
| |
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
||
|
||
|
|
|
|
||
|
|
|
|
|
|
|
|
|
|
||
|
|
|
|
|
|
|
|
|
||
|
|
|
||
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
||
|
|
|
|
|
|
|
|
||
|
|
|
|
|
|
|
|
|||
|
|
|
||
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
||
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
||
|
|
|| |
|
|
|
|
|
|
|
||
|
|
|
|
| |
|
|
|
|
|
||
|
|
|
|
|
|
||
|
|
|
|
|
|
|
|
|
|
| |
|
| |
|
|
| |
|
|
|
| |
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
||
| |
||
|
||
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
||
|
|
|
|
|
|
|
|
|
|
|
||
|
| |
|
|
|
|
|
|
|
0
1
2
3
0.00
0.25
0.50
0.75
1.00
Support
Density values
|
|
|
|
||
| |
|
|
|
|
|
|
|
|
|
| |
|
|
|
|
|
|
|
|
|
||
|
|
| | |
|
|
|
|| |
||
|
|
|
|
|
|
|
|
|
|
|
|
|
|
| |
|
|
|
|
|
|
|
|
|
|
|
| | |
|
|
|
|
|
|
|
|
|
|
|
| |
|
|
|
|
| |
|
|
|
|
|
| |
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
||
|
|
|
|
|
|
||
|
|
|
|
| | |
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|| |
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
||
|
|
||
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|||
|
|
|
|
||
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|||
|
|
|
|
|
|
|
|
|
|
|
| |
|
|
|
|
|
|
|
|
|
| |
|
|
|
|
|
|
|
|
|
|
|
|
| |
|
|
|
|
|
|
|
| |
|
|
|
|
|
||
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
| |
|
|
|
| |
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
| |
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
||
|
| |
|
|
|
|
|
|
|
|
|
||
|
|
|
|
|
|
|
|
|
|
|
|
|
| |
|
|
|
|
|
|
|
||
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
||
|
|
||
|
|
|
|
|
|
|
|
|
| |
|
|
|
|
|
|
|
|
| |
|
|
| |
|
|
|
| |
|
|
|
|
|
|
|
|
|
|
|
||
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|||
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
||
|
|| ||
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
| |
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
| |
|
|
|
|
|
|
|
|
|
|
|
|
|
|
||
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
| || |
|
|
|
|
|
|
|
|
|
|
|
|
| |
|
|
|
|
|
|
|
|
|
|
|
||
|
|
|
|
|
|
|
|
||
|
|
|
|
|
|
|
|
|
|
| |
||
|
|
|
|
|
|
|
||
|
|
|
|
|
|
|
|
|| |
|
|
|
|
|
||
|
|
|
|
|
|
|
||
|
|
|
|
|
|
|
|
|
||
|
| |
|
| |
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
||
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
||
|
|
|
|
|
|
|
|
|
|
|
|
|
||
|
|
| |
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
| |
|
|
| |
|
|
|
|
|
|
|
||
|
|
|
|
|
|
| |
|
|
|
| |
|
|
|
|
|
|
|
|
| |
|
|
|
|
||
|
|
|
|
|
|
|
|
|
| |
|
|
|
|
|
||
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
| |
|
|
|
|
| |
|
|
|
|
| |
| |
|
|
|
|
|
|
|
|
|
|
|
|
||
|
| |
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
| |
|
||
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
| |
|
|
|
|
|
|
|
|
|
|
|
0
1
2
3
0.00
0.25
0.50
0.75
1.00
Support
Figure 5.1: Each plot shows 100 posterior draws from the χ2-process density sampler. 1,000
data samples were drawn from a diﬀerent beta distribution for each plot. The generating
pdf is given in red, and the red hash marks describe the actual data produced.
unit interval, the eigen-pairs are given by
λ2
i = σ2(α + π2i2)−s ,
and
φi(x) =
√
2 cos(π i x) ,
(5.52)
for i ≥0. For D the two-dimensional unit square D = [0, 1] × [0, 1], the eigen-pairs are given
by
λ2
i = σ2 α + π2(i2
1 + i2
2)
−s ,
and
φi(x) = 2 cos(π i1 x1) cos(π i2 x2) ,
(5.53)
98

G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
0.00
0.25
0.50
0.75
0.0
0.2
0.4
0.6
0.8
Dimension 2
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
0.00
0.25
0.50
0.75
1.00
0.00
0.25
0.50
0.75
1.00
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
0.00
0.25
0.50
0.75
1.00
0.00
0.25
0.50
0.75
1.00
Dimension 1
Dimension 2
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
0.00
0.25
0.50
0.75
1.00
0.00
0.25
0.50
0.75
1.00
Dimension 1
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
0.00
0.25
0.50
0.75
0.0
0.2
0.4
0.6
0.8
Dimension 2
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
0.00
0.25
0.50
0.75
1.00
0.00
0.25
0.50
0.75
1.00
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
0.00
0.25
0.50
0.75
1.00
0.00
0.25
0.50
0.75
1.00
Dimension 1
Dimension 2
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
0.00
0.25
0.50
0.75
1.00
0.00
0.25
0.50
0.75
1.00
Dimension 1
Figure 5.2: The contours (black) of the posterior median from 1,000 draws of the χ2-process
density sampler. Each posterior is conditioned on 1,000 data points (red).
for i1, i2 ≥0. See [18] for a similar approach. In the following experiments, all Markov chains
are initialized using Newton’s method on the sphere (see Appendix D.1).
5.5.1
Simulated experiments
Figure 5.1 depicts 1,000 data points (red hash marks) drawn from four diﬀerent beta dis-
tributions (density red) along with 100 MCMC draws from the posterior distribution based
on the χ2-process density model. From left to right and top to bottom, the beta distribu-
tion parameters are (1, 1), (5, 2), (.5, .5), and (2, 2). Note that while the individual posterior
draws adhere closely to the sampled data, the variability in the posterior draws accounts for
uncertainty and gives good coverage to the true density. The hyperparameter settings for the
top-left plot is given by (σ, α, s) = (.5, 1, 1), and (σ, α, s) = (.5, .5, .8) is the hyperparameter
99

|||||||||||||| ||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||| ||| |||||||||||||| || ||| || |||||||||||||||||||||||| ||||| |
| ||
0
1
2
3
1850
1875
1900
1925
1950
Year
Density values
|||||||||||||| |||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||| ||| |||||||||||||| || ||| |||||||||||||||||||||||||| ||||| |
| ||
0
1
2
3
1850
1875
1900
1925
1950
Year
|||||||||||||| ||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||| ||| |||||||||||||| || ||| || |||||||||||||||||||||||| ||||| |
| ||
0
1
2
3
1850
1875
1900
1925
1950
Year
Density values
|||||||||||||| |||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||| ||| |||||||||||||| || ||| |||||||||||||||||||||||||| ||||| |
| ||
0
1
2
3
1850
1875
1900
1925
1950
Year
Figure 5.3: Coal mining disasters data. The left ﬁgure shows 100 posterior draws from the
χ2-process density model (gray) over 191 vertical lines (red) marking the precise date of each
disaster. The right ﬁgure shows the pointwise median (black) for the same sample as well
as pointwise quantile bands (blue). Note how the undulations exhibited by individual draws
does not appear in the quantile bands.
setting for the rest. I = 30 for each example. 10,000 thinned MCMC iterations were used
to make each ﬁgure.
Figure 5.2 depicts 1,000 data points (red) drawn from four diﬀerent distributions on the unit
square along with the contours of the pointwise median of 1,000 posterior draws from the
χ2-process density model. The data in the ﬁrst three plots was generated using truncated
Gaussians and mixtures of truncated Gaussians. The data for the last plot was generated by
Gaussian noise added to the uniform distribution on the circle. The model adapts easily to
multimodal and patterned data samples. For all examples, the hyperparameters were ﬁxed
to (σ, α, s) = (.9, .1, 1.1). 0 ≤i1, i2 ≤5 for each example.
5.5.2
Experiments with real-world data
Figure 5.3 features the British coal mine disaster data set, in which the dates of 191 disasters
are recorded between the years of 1851 and 1967. In both plots, the dates are given in red.
Two comparisons are implied by the ﬁgure. The ﬁrst is a comparison between the variability
100

G
GGG
G G
GG
GGG
GG
GG
G
G
G
G
G
G
G
G
G
GG
G
G
GG
G
GG
G
G
G
G
GGG
G
G
G
G
G
GG
GG
G
G
G
G
G
GG
G
G
G
G
G
GG
GGGG
G
G
GG
G
G
G
G
G
G
G
G
G
GGG
G
G
GG
G
G
G
G
G
GG
G
G
G
GGGG
G
G
GG
G
GGGG
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
GGG
G G
G
GG
GG
G
G
G G
G
G
G
G
G
G
G
G
G
G
GG
G
GG
G
G
G
G
G
G
GGG
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
GG
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GGG
GG
G
G
G
G
G
GG
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G G
GG
G
G
G
G
GG
G
G
G G
G
GG
G
G
G
G
G
G
G
G
G
G GG
G
G
G
GGG
G
G
G
G
G
GGGG
G
G
G
G
G
G
G
GGG
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
GGG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
GG
G
G
G
GG
G
G
G
G
G
GGGG
G
G
G
G
G
G
G
G
G
G
G GG
G
G
G
G
G
G
G
G
G
G
GG
GG
GG
GG
G
GGG
G
G
G
G
G
G
G
G
GGG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
GG
G
GG
G
G
G
GG
G
GG
G
G
G
G
G
G
G
G
G
G
GGGG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
GG
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
GG
G
G
G
GG
G
G
G
G
G
G
GGG
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
GGG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
GG
G
G
G
G GG
G
GG
G
GG
G
G
G
G
G G
G
G
G
G
G
G
GG GG
GG
G G
G
GGG
GGG
G
GG
G
G
G
G
G
G
G
GG
GG
G
G
G
GGG
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
GG
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
0.00
0.25
0.50
0.75
1.00
0.00
0.25
0.50
0.75
1.00
Dimension 1
Dimension 2
Pointwise posterior mean
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
0.00
0.25
0.50
0.75
1.00
0.00
0.25
0.50
0.75
1.00
Dimension 1
Posterior predictive sample
G
GGG
G G
GG
GGG
GG
GG
G
G
G
G
G
G
G
G
G
GG
G
G
GG
G
GG
G
G
G
G
GGG
G
G
G
G
G
GG
GG
G
G
G
G
G
GG
G
G
G
G
G
GG
GGGG
G
G
GG
G
G
G
G
G
G
G
G
G
GGG
G
G
GG
G
G
G
G
G
GG
G
G
G
GGGG
G
G
GG
G
GGGG
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
GGG
G G
G
GG
GG
G
G
G G
G
G
G
G
G
G
G
G
G
G
GG
G
GG
G
G
G
G
G
G
GGG
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
GG
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GGG
GG
G
G
G
G
G
GG
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G G
GG
G
G
G
G
GG
G
G
G G
G
GG
G
G
G
G
G
G
G
G
G
G GG
G
G
G
GGG
G
G
G
G
G
GGGG
G
G
G
G
G
G
G
GGG
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
GGG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
GG
G
G
G
GG
G
G
G
G
G
GGGG
G
G
G
G
G
G
G
G
G
G
G GG
G
G
G
G
G
G
G
G
G
G
GG
GG
GG
GG
G
GGG
G
G
G
G
G
G
G
G
GGG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
GG
G
GG
G
G
G
GG
G
GG
G
G
G
G
G
G
G
G
G
G
GGGG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
GG
G G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
GG
G
G
G
GG
G
G
G
G
G
G
GGG
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
GGG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
GG
G
G
G
G GG
G
GG
G
GG
G
G
G
G
G G
G
G
G
G
G
G
GG GG
GG
G G
G
GGG
GGG
G
GG
G
G
G
G
G
G
G
GG
GG
G
G
G
GGG
G
G
G
G
G
G
G
G G
G
G
G
G
G
G
GG
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
0.00
0.25
0.50
0.75
1.00
0.00
0.25
0.50
0.75
1.00
Dimension 1
Dimension 2
Pointwise posterior mean
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
0.00
0.25
0.50
0.75
1.00
0.00
0.25
0.50
0.75
1.00
Dimension 1
Posterior predictive sample
Figure 5.4: Hutchings’ bramble canes data. The ﬁrst ﬁgure depicts the 823 bramble canes
(red), a heatmap of the pointwise posterior mean (black is low, white is high), and a single
contour at density 0.3 (blue) including all but a few points. The second ﬁgure shows 823
draws from the χ2-process density posterior predictive distribution, obtained using a rejection
sampling scheme.
of 100 posterior draws based on 191 data points (left plot) with the variability in 100 posterior
draws based on 1,000 data points, as in Figure 5.1. One sees much less variability in the
latter. The other comparison is between the close ﬁt exhibited in the posterior draws of the
left plot compared to the smooth ﬁt shown by the pointwise quantiles (median, black; .25,
blue; .75, blue). As we can see, our method is valid for modeling densities without periodic
tendencies, despite the speciﬁc form of the basis. Both plots are based on 10,000 thinned
MCMC iterations, with hyperparameter settings (σ, α, s) = (.5, .5, .8) with I = 30.
Figure 5.4 features Hutchings’ bramble canes data (red) [99, 42], consisting of the locations
of 823 bramble canes in a square plot. The left ﬁgure contains a heatmap of the pointwise
posterior mean of the χ2-process density model, where black pertains to low density and
white pertains to high density. Finally, a single contour (blue) at density level 0.3 divides
the majority of points from areas of extremely low density. The hyperparameters were set to
(σ, α, s) = (2, .01, 1.1) with 0 ≤i1, i2 ≤5, and the posterior sample featured 10,000 MCMC
iterations. The right ﬁgure features 823 draws from the posterior predictive distribution
of the χ2 process density model. Each draw from the posterior predictive distribution was
101

obtained by randomly selecting one posterior draw from the χ2 process density model. Since
this single posterior sample is itself a density function, one can then sample from its corre-
sponding distribution using a rejection sampling scheme. There is a remarkable similarity
between the posterior predictive sample (right, black) and the bramble canes data (left, red):
despite a few diﬀerences, both low and high density regions are faithfully recovered.
5.6
Discussion
The Fisher geometry is central to many areas of classical and parametric statistics. On
the other hand, nonparametric methods—both Frequentist and Bayesian—is a vital area of
statistical research with many realizations and applications. We presented a nonparametric
extension to the parametric Fisher geometry and showed that this generalization is consistent
with its parametric predecessor. To do so, the set of probability density functions over a given
domain was deﬁned to be an inﬁnite-dimensional smooth manifold where each point is itself
a density function. This manifold becomes a Riemannian manifold when equipped with the
nonparametric Fisher information metric and is then identiﬁed with the inﬁnite-dimensional
sphere, a well understood geometric object for which results are readily obtainable. Indeed,
the beneﬁts of shifting focus to the inﬁnite-dimensional sphere do not stop at theory. Due
to the relationship between the nonparametric Fisher geometry and the inﬁnite sphere, it
proves convenient to deﬁne nonparametric models directly on this sphere.
We demonstrated one application of this approach in the form of Bayesian nonparametric
density estimation. The resulting χ2-process density model is ﬂexible and computationally
eﬃcient: it is amenable to HMC and, in comparison to the cubic scaling of GP competitors,
scales linearly in the number of data points. Of course, there is nothing a priori restricting
the prior to be Gaussian [50], and an important next step is placing a prior on the number
of basis functions to use, as is done in [37].
102

Moreover, egMC uses geodesics to propose new states on the sphere, and these geodesic ﬂows
are formally equivalent to those derived on the L2 sphere. Thus, the empirical eﬀectiveness
of egMC in this context suggests that the proposals somehow adapt to the nonparametric
Fisher geometry. We hope the χ2-process density model will serve to motivate extensions of
HMC to Hilbert manifolds, of which the L2 sphere is one example. It is now known how to
perform HMC on ﬁnite dimensional, Riemannian manifolds [112, 25, 93] as well as Hilbert
spaces [19]. We hope that the model presented here will motivate the extension of HMC
technology to a large class of Hilbert manifolds, including the inﬁnite-dimensional sphere.
The theoretical and methodological results presented in this chapter are merely ﬁrst steps
in exploiting the simple geometry implied by the nonparametric Fisher metric. Whereas
density estimation is perhaps the most obvious application, it is also one of the fundamental
problems in statistics and thus has connections to many other areas of statistics and machine
learning. On the other hand, methodologies such as functional regression and classiﬁcation
[146] can beneﬁt from the use of random functions deﬁned on the sphere, which objects
we constructed and performed inference on. Additionally, the nonparametric methodology
proposed in this chapter was Bayesian, but the spherical representation of the nonparametric
Fisher geometry has clear connections to Frequentist nonparametrics by way of the geometry
of the bootstrap [55].
103

Chapter 6
Application: Bayesian neural
decoding
104

Chapter Summary
Neuroscientists are increasingly collecting multimodal data during experiments and
observational studies. Diﬀerent data modalities—such as EEG, fMRI, LFP, and spike
trains—oﬀer diﬀerent views of the complex systems contributing to neural phenom-
ena.
Here, we focus on joint modeling of LFP and spike train data, and present
a novel Bayesian method for neural decoding to infer behavioral and experimental
conditions. This model performs supervised dual-dimensionality reduction: it learns
low-dimensional representations of two diﬀerent sources of information that not only
explain variation in the input data itself, but also predict extra-neuronal outcomes.
Despite being one probabilistic unit, the model consists of multiple modules: exponen-
tial PCA and wavelet PCA are used for dimensionality reduction in the spike train and
LFP modules, respectively; these modules simultaneously interface with a Bayesian
binary regression module. We demonstrate how this model may be used for prediction,
parametric inference, and identiﬁcation of inﬂuential predictors. In prediction, the hi-
erarchical model outperforms other models trained on LFP alone, spike train alone,
and combined LFP and spike train data. We compare two methods for modeling the
loading matrix and ﬁnd them to perform similarly. Finally, model parameters and their
posterior distributions yield scientiﬁc insights.
105

6.1
Introduction
Both in behavioral research and in clinical medicine, the relationship between neuronal sig-
nals and extra-neuronal phenomena is often of interest1. This is true whether a scientist
wishes to test a hypothesis on memory function or whether a physician wants to detect
a vulnerability toward stroke. Neural decoding is the science of deducing extra-neuronal
phenomena from the activity of neurons or groups of neurons [24]. This activity can be reg-
istered in a number of modalities, including functional magnetic resonance imaging (fMRI),
electroencephalogram (EEG), local ﬁeld potential (LFP), and spike trains. In general, neural
decoding becomes more diﬃcult as the number of neurons considered increases and as more
distinct data modalities are integrated. On the other hand, the beneﬁt of increasing neuron
count—and from incorporating diﬀerent signal types—is large. A priori, large groups of neu-
rons have more capacity to encode complex behavior than does a single neuron. Since signals
captured by diﬀerent data modalities are obtained by measuring diﬀerent neuro-biological
quantities, incorporating multiple data modalities into a single statistical analysis oﬀers an
increased perspective that may help detect latent patterns otherwise missed.
Our contribution is a Bayesian hierarchical model that performs supervised dual-dimensionality
reduction (sDDR) of both LFP and spike count data. Given a collection of binary outcomes
yi that may be plausibly linked to or reﬂected in neuronal activity, we assume that the prob-
ability p(yi = 1) is functionally linked to low-dimensional representations of both kinds of
neuronal data:
g{p(yi = 1)} = β + βT
S zS
i + βT
LzL
i .
(6.1)
1This chapter was adapted from the paper A Bayesian supervised dual-dimensionality reduction model
for simultaneous decoding of LFP and spike train signals [94], published in the journal Stat ( c⃝2017 Wiley)
and authored by A Holbrook, Alexander Vandenberg-Rodes, Norbert Fortin, and Babak Shahbaba. AH was
supported by NIH grant [T32 AG000096]. The work was also partially supported by NIH grant R01-AI107034
and NSF grant DMS-1622490. The authors thanked Hernando Ombao for helpful discussion.
106

Here, zS
i and zL
i are low-dimensional representations of the spike counts and LFP data, re-
spectively. In the following, we explain the precise nature of these low-dimensional quantities
and specify exactly how they may be obtained. Importantly, the model components are not
found sequentially or by a multi-stage approach: the low-dimensional signals, zS
i and zL
i , are
learned simultaneously with the coeﬃcients of the predictive model (6.1). The model is thus
able to relate low-dimensional representations of this input data to extra-neuronal outcomes
and measure uncertainty while doing so. We demonstrate how this model may be used for
neural decoding and the identiﬁcation of inﬂuential predictors.
Our Bayesian hierarchical model captures variability in the input data in a way that is pre-
dictive of experimental conditions. We represent both the spike train and LFP data as being
generated from exponential family distributions with low-dimensional covariance structure.
This is done using probabilistic extensions of principal components analysis (PCA) [140, 103],
a non-probabilistic linear dimensionality reduction technique in which the eigenvalue de-
composition of the empirical covariance matrix is considered. Generative versions include
probabilistic PCA (PPCA) and factor analysis [167, 102]. Both of these methods model high
dimensional data as generated by a multivariate Gaussian distribution with covariance the
sum of a low-rank matrix and a diagonal matrix (restricted to a multiple of the identity un-
der PPCA). Exponential family PCA (ePCA) [35] is a generalized linear models extension of
PCA, and we use this to model the non-negative integer constrained vectors of spike counts.
A probabilistic wavelet PCA (wPCA) is used to model the LFP signals. See [8] and [59] for
non-probabilistic implementations.
Besides performing dimensionality reduction, the sDDR model also discriminates between
extra-neuronal conditions. It uses a variation of supervised PCA [183], which is an alternative
to principal components regression (PCR) and partial least squares (PLS) [180, 77]. Applying
logistic regression to a PCA derived, low-dimensional representation of data x for classifying
outcome variables y is known as principle components (logistic) regression (PCR). PCR is
107

rarely a recommended method, for the simple reason that the directions (eigenvectors) that
best explain variability in x have little reason a priori to explain the variation in the outcomes
y [104]. Partial least squares has both probabilistic and non-probabilistic variations, and also
has been extended to partial least squares-discriminant analysis (PLS-DA), a deterministic
method that handles discrete outcomes y [10]. PLS-DA favors the directions that maximize
discrimination for y, but without taking into account the within-class covariances. In their
probabilistic versions, PLS is easily extended to handle discrete data using the exponential
family, and supervised PCA can be viewed as a special case of PLS [131] (Section 6.3.3).
The sDDR model is applied to data from an experiment testing the capacity for non-spatial
sequential memory in rats. During this experiment, the rats are presented with ‘correct’
and ‘incorrect’ sequences of odors and must classify them accordingly. Meanwhile, spike
trains are recorded from over ﬁfty neurons (Figure (6.1)), and LFP signals are recorded from
12 channels (Figure (6.2)). Due to the LFP signals’ non-stationary functional nature, the
model performs wPCA on the LFP data. Since the spike counts are vectors of non-negative
integers, we use ePCA with the log-link function on the spike train data. Latent variables
from these two generalized PCA models feed into a logistic regression with sequential status
(correct/incorrect) response. The model renders accurate predictions and interpretable pa-
rameters. If regression coeﬃcients are larger for latent variables associated with one of the
data modalities than they are for the other, there is evidence that that modality holds more
information regarding the outcome (sequence status). We show that a certain functional
on a few model parameters renders a useful metric for how predictive individual neurons
and wavelet coeﬃcients are of sequence status. This leads to easy variable selection and
identiﬁcation of inﬂuential predictors.
108

GG
GGGG
GGG
GGGG
GG
G
G
GGG
G
GGG
GG
GGG
G
GGGG
GGGGG
GGGG
GG
GG
G
GGG
GGGGGG
G
GGG
GGG
G
G
GG
GGGGGG
GGGGGGGG
GGGG
GG
GGG
GG
GGGGG
GGGGG
GG
GGG
GGG
GG
G
GGG
GGGGGG
GGGGGG
GG
GGGGGGGG
GGGG
GGGGGGGGGGGGGG
G
GG
GG
GGGG
GGGG
GG
GGGG
GGGGGGGGGGGGGGGG
GG
GG
GGG
GGGGGGGGGGGG
GG
GGGG
G
GGG
G
GGGG
GGGG
G
G
G
G
G
GGGGGG
GG
GGG
G
G
G
G
G
G
G
GGG
GG
GG
GG
GG
G
G
GGG
G
G
GGG
GGGG
GG
G
G
G
GGGGGG
GG
GG
G
G
GGGGGG
G
GG
G
G
G
GGG
GGG
G
G
G
G
G
G
G
G
GG
G
G
GGGG
G
G
G
GG
GG
GGG
G
G
GGG
G
G
G
G
GGG
G
GG
G
GG
G
GG
G
GG
G
GG
GGG
GGGG
GG
G
GG
G
GGGG
G
GGG
G
G
G
GG
G
GG
GGGG
G
G
G
G
G
G
G
G
G
GGG
G
GGG
GGGGGG
G
G
GG
G
G
G
G
G
G
GGG
G
G
GG
G
G
G
G
G
GGG
G
GG
GG
G
G
G
G
G
G
G
GG
G
G
GGG
G
G
G
GG
G
G
G
G
GGGG
G
GGG
G
G
G
GG
G
G
G
GG
G
G
G
GG
G
GG
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
GG
G
GG
G
G
G
GG
G
G
G
GG
G
G
G
G
G
G
G
GG
G
G
G
G
G
GG
G
G
G
G
G
G
G
GGG
G
G
G
G
G
G
G
G
GG
G
GG
G
GGG
GG
G
G
G
G
GG
G
G
GG
G
G
G
GG
G
GG
GG
G
G
GG
G
G
G
GG
G
G
G
GG
G
G
G
G
G
GGG
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
GG
G
G
G
GGG
G
G
GG
G
GGGGGG
GGGGGGG
GGGGG
G
GGGG
GGGGG
GG
G
GG
GGGG
G
GG
G
GGGGGGG
GG
GGGG
GGGGGGGGGGG
GGG
G
GG
GG
GGG
GG
GGGGGG
G
GGGG
G
G
GGGGGGGGGG
GG
G
GGGGG
GG
GG
GG
G
GG
GG
GGGG
GG
GGG
G
GG
GGGG
GGGGGGGG
GGGGGG
GG
GG
GGG
GG
GG
GGGGGGGGGGGG
GGGGGGGGG
GGGGGG
GGGGG
GG
GGGGG
GGGGG
GGGG
G
GGGG
GGG
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
GG
GG
G
G
G
G
G
G
G
G
G
G
G
G
GG
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
GGGGG
GGGGGG
GG
GGGG
GGGG
GGGGGGGGGGGGGGG
GG
GGG
G
GGG
G
G
G
GGGGG
G
GGGGGGGG
GGGGGGGGGGGGG
GGG
GG
GGG
GGG
G
G
GGGG
GG
GGGGGG
GG
GGGGGG
GG
G
GG
GG
GGG
GGGGGG
G
GGG
GG
GGG
G
GG
G
G
GG
G
GGGGG
G
G
GG
GG
GGGGG
GG
G
GGGG
G
G
GG
GGG
GG
GGG
G
G
GGGGG
G
G
GG
GGGG
GG
G
GGG
GGG
GGG
GG
GGG
GGG
G
G
G
G
GGGGGG
G
GG
G
GGGGG
GGGGG
G
GGGG
GG
GGGG
G
GGGG
GG
GGG
GGGGG
GGGGG
GGGGGGGG
GGGGG
GG
GGGGG
GGG
GGGGGGGGGGGGGGG
GGGGG
GGGGGG
GG
GGG
GGGG
G
GGGGGGGG
GGGG
G
G
GGGGG
GGGG
GGG
GGGG
G
G
GGGGGGG
GGGG
GGG
GGGGGGGGGG
GGGGGGGGGGGGGG
G
GGGGG
GGGGGG
G
GGGG
GGGGG
G
GG
GGG
G
G
GGGGG
GG
G
G
G
G
GG
GGGG
GGG
GGGGG
G
GGGG
G
GG
GG
GG
G
G
G
GGGGGGG
G
G
GGGG
GGG
GG
GG
G
G
GGG
G
G
G
G
G
G
GGG
GGGGGG
GG
G
G
GGGGGGGG
G
G
GGGG
G
GG
GG
GG
G
G
GG
GG
G
G
GGG
G
G
G
G
G
G
GGGG
GG
GG
G
G
G
G
GG
GG
G
G
GG
G
GG
GGG
G
GG
G
G
GGG
G
GG
G
GG
G
G
GG
GGG
G
G
GGGG
G
G
G
GGG
G
GG
G
GGGGG
G
G
GG
G
G
G
G
G
GG
GG
G
GG
G
GG
G
G
G
G
G
G
GG
G
GGG
GG
GG
GGGGG
GGGGGGGGGGG
G
GGGGGGGG
GGGGGGGGG
G
GGGGGGGG
GGGGGGGGGGGGGGGGGGGG
GGGG
GGG
GGG
G
G
GGGGGGGG
GGGGGGGGGGGGGGG
GGGGG
GGG
G
GGGGGGGG
GG
GGG
GGGGGGGGGGGGGGGGGG
G
GGGGGGGGGGGGGG
GG
GGGGGGG
GGGGGGGGGGGGG
GGGGG
G
GG
GGGGGGGG
GG
GGGGGGG
GG
G
G
GGGG
GG
GGGGGGG
G
GGGGGGGGG
GGGGGGGGG
GGGGGGGGG
GGGGGGGGGGGGGGGGGGGGGG
GGGGGGGGGG
GGGGGGGGGGGGGGGGGGGGGGGGG
GGGGGGGGGGGGGGG
GGGGGGGGGGGGGGGGG
GGGGGGGGGGGGGG
GGGGG
GGGGGGGGGGGGGGGGGGGGGGGG
GGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGG
GGGGGGGGGGGGGGGGGGG
GGGG
G
G
G
GG
G
G
G
G
GG
G
GG
G
G
G
GG
G
GGG
G
G
G
G
G
G
G
G
GG
G
G
G
G
GG
G
G
G
G
G
GG
G
GG
G
G
G
G
G
G
G
GGG
G
GG
G
G
G
GG
G
G
GG
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
GGG
G
G
G
G
G
G
G
G
GG
G
G
G
G
GG
GGGGGGGGGG
GGGG
GGGG
GGGGGGG
GG
GG
GGG
GGGGGGGGG
GGG
GG
GGGGGG
GGGGGGGGGG
GG
GG
GG
GGG
G
GGGGGGG
GGGGGGGG
GGGG
GGGGGG
G
GGG
GGGGGG
GGGGGGGGG
GG
GGGGG
GGGGG
GGGGGGGGGGG
GGGGGG
GG
GGGGGGGG
G
GGGGGGGGGGGG
GGGGGGGGGGG
GGGGGGGGG
GG
GGGG
GGG
GGGGGGGGGGGGGG
GGGGG
GG
G
GGG
GGG
GG
G
GG
G
GGG
G
GG
GG
GGGG
G
G
G
G
G
GG
GG
G
G
G
G
G
G
G
G
G
G
G
GG
GGGGG
GG
G
G
G
G
G
G
GGG
GG
G
G
GG
GG
G
GG
G
GGG
GGG
G
G
GGGG
GGGGGGGG
G
G
G
GGGGGG
GGGG
GG
GGG
GGG
GG
GGG
G
G
GGGG
GGGG
GGG
GG
GG
GGG
GG
G
GGGGG
GGG
GGGG
GG
GG
G
GG
GGG
G
GGG
G
GGG
G
GGG
GGGG
G
G
GGGG
GGGG
GGGGG
GG
G
GGGGGG
GGGGGGG
GG
G
G
G
G
GG
GG
G
GG
G
G
G
G
G
GG
G
G
GG
GG
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
GG
GG
G
G
G
G
G
G
G
G
GG
G
G
G
G
G
GGG
G
G
G
G
G
GG
G
G
GG
G
G
GG
G
G
G
G
G
G
G
GG
G
G
G
GG
G
GG
G
G
G
G
G
G
G
G
GGGG
GG
G
G
G
G
G
G
G
G
GG
G
G
G
GGG
G
G
G
G
G
GG
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
GG
G
G
G
G
GG
G
GG
G
G
GG
G
G
G
G
G
G
GG
G
G
G
G
G
GG
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
0
50
100
150
200
Trial
Neurons
Spike count
G
G
G
G
G
0
5
10
15
20
In−sequence
G
G
No
Yes
Figure 6.1: Trial-wise spike counts for 14 of the 52 neurons recorded over the course of the
session. Counts range from 0 to 24, with a median of 1 and a third quartile of 2. Individual
neurons vary greatly in spike proﬁle: the top neuron hardly ﬁres, while the third from the top
averages 5 spikes a trial. In-sequence trials are colored blue, out-of-sequence trials are colored
orange. The ﬁfth neuron from the bottom appears to ﬁre more during out-of-sequence trials,
and the third neuron from the bottom never ﬁres during an out-of-sequence trial.
6.2
Scientiﬁc background and experimental setup
The hippocampus is central to the memory for sequences of events, but the basic neuronal
mechanisms underlying sequential memory capacity are not well understood. There is sig-
niﬁcant evidence for the ability of hippocampal neurons to encode sequences of locations
[156, 127, 47, 62, 75]. Direct evidence, however, for a neural role in the memory of sequential
relationships among nonspatial events remains absent. To address this issue, [3] recorded
neural activity in the CA1 region of the hippocampus as rats performed a sequential mem-
ory task. The task involved the presentation of repeated sequences of odors at a single port
and tested the rats’ ability to identify each odor as ‘in-sequence’ or ‘out-of-sequence’. LFP
signals and multi-dimensional spike trains were recorded in the hippocampi of 6 rats, who
had previously been trained on a particular ‘correct’ sequence (A, B, C, D, E) of odors. Each
109

−1.5
−1.0
−0.5
0.0
0.5
1.0
1.5
0
100
200
300
400
500
Milliseconds (ms)
LFP: rescaled voltage
In−sequence trial
−1.5
−1.0
−0.5
0.0
0.5
1.0
1.5
0
100
200
300
400
500
Milliseconds (ms)
Out−of−sequence trial
−1.5
−1.0
−0.5
0.0
0.5
1.0
1.5
0
100
200
300
400
500
Milliseconds (ms)
LFP: rescaled voltage
In−sequence trial
−1.5
−1.0
−0.5
0.0
0.5
1.0
1.5
0
100
200
300
400
500
Milliseconds (ms)
Out−of−sequence trial
Figure 6.2: Two examples of LFP time series, each color pertains to one of twelve channels.
The left and right plots come from trials 16 and 17, respectively. In trial 16, the correct
odor was presented, and in trial 17 an incorrect odor was presented. In both plots, the time
series seem to belong to two separate groups by channel: one of lower amplitude and one of
higher amplitude. Both amplitude groups show interesting low frequency patterns. Indeed,
the in-sequence signals are far from stationary. It is for these reasons a wavelet transform is
used: it accounts for both changes in scale and shifts in time.
trial involved the rat smelling one of the ﬁve odors through a port. The rat signaled whether
the odor was in-sequence or out-of-sequence by choosing to withdraw its nose from the port
either after or before one second, respectively.
Roughly 88% of the trials were in sequence. This chapter only considers data from a single
session featuring rat ‘Super Chris’. The session consisted of 218 trials lasting anywhere from
0.48 to 1.74 seconds each. For each trial the data features spike counts from 52 neurons,
LFP signals from 12 channels, and a binary indicator for whether the odor presented was
in-sequence (1) or out-of-sequence (0). In order to minimize diﬀerences in motor neuron
activity across trials, the spike counts for each trial are the total number of spikes in the 0.5
second interval immediately preceding port withdrawal. We are interested in a supervised
learning problem: can we decode the rat’s response from the neuronal data alone? If so, is
there evidence for one data modality having more predictive capacity than the other? Are the
110

two data modalities complementary with respect to outcome? We assert that our Bayesian
sDDR model can help answer these questions.
6.3
Bayesian linear dimensionality reduction and ex-
tensions
The most prevalent linear dimensionality reduction methods fall into the factor analysis
framework. Such models specify the N observed continuous data points x1, . . . , xN ∈Rd as
xi = Fzi + µ + ϵi,
(6.2)
where zi ∈Rk (k < d) are the latent factors, F is the d × k factor loading matrix, and ϵi
are iid Nd(0, Ψ), with Ψ a diagonal covariance matrix. Typically, the parameters F, µ, Ψ are
optimized over, either by EM or using closed form expressions available when Ψ is restricted
to be a multiple of the identity [167]. If we place N(0, Ik) priors on zi, this latent factor is
easily integrated out, leaving the sampling distribution written as
xi ∼N(µ, FF T + Ψ).
(6.3)
This formulation assumes that the data lies close to an aﬃne subspace spanned by the column
vectors of F. There are, however, a continuum of subspaces that approximately span the
data. Picking a single subspace can dramatically understate variation in the data and lead
to over-ﬁtting. One approach is to instead use the Bayesian framework to obtain a posterior
distribution over model parameters F, µ, and Ψ given x:
π(F, µ, Ψ|x) =
f(x|F, µ, Ψ) π(F, µ, Ψ)
R
f(x|F, µ, Ψ) π(F, µ, Ψ) dF dµ dΨ ,
(6.4)
111

where f(x|F, µ, Ψ) is the Gaussian density function of x given model parameters, and
π(F, µ, Ψ) is the prior distribution. Obtaining the posterior distribution means integrat-
ing over high dimensional, high density regions inhabited by F and helps avoid over-ﬁtting
in a natural way. But even with much simpler models, the integral over model parameters
is almost always intractable. The common solution is to sample from the posterior distri-
bution using Markov Chain Monte Carlo (MCMC) methods. Unfortunately, using MCMC
for large matrices such as F is not straightforward, and the way F is modeled will delineate
which sampling tools are available. Two diﬀerent models for the loading matrix and their
respective MCMC methods are discussed in Section 6.3.4 below.
6.3.1
Exponential family PCA
A signiﬁcant limitation of standard PCA appears when one tries to apply it to binary, count,
or categorical data. Taking a cue from generalized linear models, [35] models each data point
xj,i as being generated by an exponential family distribution:
f(x|θ) = h(x) exp
 x θ −b(θ)

.
(6.5)
Here the natural parameter θ is related to the mean ξ = E(x| θ) through the canonical link
function: θ = g(ξ), where g−1(θ) = b′(θ). Dimension reduction is then applied to the natural
parameter θ, which for many distributions of interest (e.g. Bernoulli or Poisson) can take
any value on the real line, unlike mean ξ.
Since one kind of data we deal with is from spike trains, we focus on non-negative, integer
valued output and the log link. With xi ∈({0} ∪Z+)d, we have the canonical log link
112

function g(ξ) = log ξ and
xj,i ∼Poisson(ξj,i),
with
(6.6)
ξj,i = g−1
 k
X
ℓ=1
Fj,ℓzi,ℓ+ µj
!
,
(6.7)
where the xj,i are conditionally independent given the parameters (F, z, µ). In shorthand,
this scheme may be written
xi ∼Pois⊗
 exp{UΛ zi + µ}

.
(6.8)
Here ⊗indicates the element-wise factorization of the multivariate conditional density func-
tion into univariate Poisson distributions. Parameters z, and µ play the same role they do in
(6.2) and can be given the same priors. Priors and constraints for F are discussed in Section
6.3.4 below. Note that the form of (6.4) in no way restricts f to being a Gaussian density:
MCMC is performed in exponential family PCA in the exact same way it is in run-of-the-mill
Bayesian PCA.
6.3.2
Wavelet transform and wavelet PCA
A wavelet (little wave), is a function approximately localized in both time and frequency.
Thus, expressing a signal in terms of wavelets, instead of sinusoids as with the Fourier
transform, can allow for more compact representations of non-stationary behavior. A wavelet
basis, generated by mother and father wavelets ψ and φ, has a particular dyadic structure
113

describing the multi-resolution analysis [41]. Given a signal f(·) on the unit interval, the
wavelet transform returns the coeﬃcients in the expansion
f(t) = s0φ(t) +
∞
X
j=0
2j−1
X
k=0
dj,kψj,k(t),
(6.9)
where s0 is the coarsest smoothing coeﬃcient, dj,k is the detail coeﬃcient at scale j and
location k, and ψj,k(t) = 2j/2ψ(2jt −k) is the rescaled and shifted mother wavelet. Given
2J equally spaced observations of f, the outer sum in (6.9) is truncated at scale J. In the
classiﬁcation setting, the resulting collection of 2J wavelet coeﬃcients can be treated as fea-
tures, though dimension reduction is still necessary. In the Bayesian context, [170] train
a probit model that includes a spike and slab prior for variable selection on the wavelet
coeﬃcient, while [175] instead use shrinkage priors. However, [39] noted signiﬁcant correla-
tion between neighboring wavelet detail coeﬃcients dj,k in real-world signals, and proposed
a hidden Markov tree (HMT) model for the dependencies both across locations k and scales
j. Classiﬁcation is then based on the learned HMT parameters. For dimension reduction
one can apply PCA to a modiﬁed set of wavelet features [76]. Because the wavelet transform
is an orthogonal transformation, applying PCA to the raw wavelet coeﬃcients would have
the same eﬀect as applying PCA to the data itself. In our setting we observe 512 samples
at 1000 Hz, and apply the wavelet transform using the Daubechies extremal phase wavelet
with ten vanishing moments. As only the low frequencies are thought to be informative, up
to about 50Hz, we throw away the three ﬁnest scales leaving us with 64 coeﬃcients.
6.3.3
Supervised exponential PCA
Suppose now that paired data (x, y) is collected. Often the goal is to ﬁt a joint model for
the data, such that future x data can be used to predict future y – the supervised learning
114

problem. Two classical linear dimensionality reduction methods for paired data are linear
discriminant analysis (LDA) [61] and canonical correlation analysis (CCA) [98]: the former
method is applied when y is a discrete label, the latter when y takes on real multivariate
values. LDA is a directed method: a function on x is used to predict label y. CCA is
undirected: x and y are modeled as exchangeable (ignoring diﬀerences in dimensionality).
Their variability is explained by one latent variable each, one shared latent variable, and
additive noise [131]. Partial least squares (PLS) [180, 77] is half way between CCA and
supervised PCA, in that it is directed, but not fully.
Ignoring additive noise, the PLS
scheme features y fully explained by a latent variable that also underlies x. In this sense, it
is directed. On the other hand, variability in x is explained by an additional latent variable
that does not interface with y. In this way, it is undirected.
Supervised PCA [17, 183], on the other hand, is fully directed: univariate y is regressed over
the latents underlying x. Expanding on the notation of (6.2), we write
xi = F zi + µ + ϵx
i ,
(6.10)
yi = β zi + β0 + ϵy
i .
Since we are interested in predicting sequence status (in-sequence/out-of-sequence), and since
the spike counts are non-negative integers, we need a generalized linear model extension of
supervised PCA. Consider the following supervised ePCA model:
xi ∼p(x| θj), an exponential family vector
(6.11)
θi = g−1
x (Fzi + µ)
(6.12)
yi ∼p(y| ηi), an exponential family random variable
(6.13)
ηi = g−1
y (βTzi + β0)
(6.14)
115

We keep the prior speciﬁcation for F, Z, µ in (6.12) as before – only the β coeﬃcients are
new. Later, this model will be applied in a number of variations: to count data coming from
neural spike trains, where we specify xj as Poisson with canonical link function gx(ξ) = log ξ;
to wavelet coeﬃcient data where xj is normally distributed and given the identity link;
and to both modalities combined by creating a hierarchical extension (see Section 6.4). In
all three cases, the binary behavioral response, y, is modeled using the logit link function
gy(p) = log
p
1−p. An example of this type of model is supervised logistic PCA, which was
applied to genomic data by [183]. The parameters there were learned by maximum likelihood
estimation.
6.3.4
Modeling the loading matrix
Historically, the loading matrix has been modeled a number of diﬀerent ways. Most of these
models involve some sort of constraint on the loading matrix or a factorization of the matrix
into two constrained matrices: F = U Λ. Here, U is a d × k matrix, the columns of which
represent the low-dimensional directions of variation in x. Λ is a positive-diagonal k × k
matrix parameterizing the scales of variation in those principal directions. For example, the
maximum likelihood loading matrix from PPCA factorizes into U—the ﬁrst k columns of the
orthogonal matrix obtained by symmetrizing the empirical covariance matrix—times a scale
matrix [167]. In [35], Λ = Ik and mean µ are set to zero, but U is an unconstrained d × k
matrix with all parameters learned via (penalized) maximum likelihood. Two competing
methods for Bayesian PCA model the loading matrix diﬀerently: the ﬁrst gives the columns
of the loading matrix F independent spherical Gaussian priors [21]. Their treatment cor-
responds to making the elements of U i.i.d. standard normal and giving ﬂat priors to the
positively constrained elements of Λ. The second method models U as being an element of
the Stiefel manifold (i.e., an orthonormal matrix) and gives it uniform prior with respect to
the Haar measure [95, 89, 29]. The decision to model U as a matrix with standard normal
116

entries, or as an orthonormal matrix, does not change the general form of any of the models
presented here. That said, how this decision eﬀects model ﬁtness is of particular interest.
This question is addressed in Section 6.5.2.
6.3.4.1
The spherical Gaussian loading matrix
Modeling the loading matrix as having spherical Gaussian columns was proposed in the
original Bayesian PCA paper, [21]. If F1, . . . , Fk are the columns of F in equation (6.2), this
prior may be written
Fi
ind
∼Nk(0, λ2
i I),
i = 1, . . . , k .
(6.15)
If one speciﬁes k × k matrix Λ to be diagonal with elements λj, j = 1, . . . , k, the model may
also be written
xi = UΛzi + µ + ϵi ,
Uij
iid
∼N(0, 1),
(6.16)
where all elements of U have standard normal distributions. One may give λ2
i a diﬀuse,
positively constrained prior (The author opts for ﬂat priors in [21]).
Because (6.16) is
unchanged when applying a permutation to the columns of U and the entries of Λzi, we
further specify λ1 < · · · < λk, to make the loading matrix UΛ identiﬁable. Hybrid Monte
Carlo [137] is an eﬀective computational inference method for this model.
6.3.4.2
The orthonormal loading matrix
Under the assumption of dimension reduction (d > k), the singular value decomposition
(SVD) F = UΛV T can be modiﬁed so that U and V are, respectively, d × k and k × k
matrices with orthonormal columns, while Λ is a k × k diagonal matrix with non-negative
117

entries (the singular values) in decreasing order. Assuming the singular values are all distinct,
F is uniquely speciﬁed by U, Λ, and V .
Now, recalling that z ∼N(0, Ik) implies that
V Tz ∼N(0, Ik), one may ignore the superﬂuous rotation by V T and reparameterize (6.2)
into the exact same form as (6.16):
xi = UΛzi + µ + ϵi ,
U ∼UniH(Od,k) .
(6.17)
The collection of d × k matrices with orthogonal columns, denoted by Od,k, is known as the
real Stiefel manifold, which is a (compact) Riemannian manifold of dimension dk −1
2k(k +
1). In this chapter, a uniform prior distribution with respect to the Haar measure H of
Od,k is speciﬁed, but prior information can be incorporated with a matrix Bingham-von
Mises-Fisher distributional prior if one so chooses [88]. Posterior inference on model (6.17)
requires embedding geodesic Monte Carlo, and extension of HMC that eﬀectively constrains
parameters to the relevant manifold [25].
Similar models were considered in [89] and [29], which model the SVD of the data, instead
of the SVD of the factor matrix. In particular, [89] assumes that Z ∈ON,k, where ZT =
[z1 · · · zN]. The conditional distributions of each orthonormal column P(Uj|U−j, Λ, Z, Y ) and
P(Zj|Z−j, Λ, U, Y ) are shown to follow a von Mises-Fisher distribution, making the model
amenable to Gibbs sampling. Relaxing this assumption to Z1, . . . , ZN ∼N(0, Ik), as we
do with (6.17), is not that diﬀerent, at least a-priori. In most situations we have N ≫k,
(even if the data dimension d ≈N) and the high-dimensional independent Gaussian random
variables are orthogonal in prior expectation.
Both equations (6.16) and (6.17) are easily extended to their ePCA and supervised ePCA
analogs by substituting F = U Λ. In the following section, the presentation of the hierarchical
model is agnostic to the loading matrix model used.
118

6.4
The supervised dual-dimensionality reduction model
Let yi be the in-sequence indicator: yi = 1 if the odor presented for trial i is in-sequence
and yi = 0 otherwise. Let xS
i be the vector of spike counts, and xL
i be the LFP time series
associated with trial i. For each trial, the joint model takes in a vector of spike counts
and a vector of wavelet coeﬃcients, ˜xL
i , derived from the LFP time series of that trial. The
purpose of the wavelet transformation is to eﬃciently discretize the data (making it amenable
to PCA), while maintaining local, temporal information that would not be preserved by the
Fourier transform.
In turn, both LFP and spike train modules interface with the sequence classiﬁcation module.
The latent, low dimensional signals, zX
i and zL
i , are featured as latent ‘data’ in the Bayesian
logistic regression and are learned at the same time as the logistic regression coeﬃcients.
6.4.1
LFP module
The LFP data is modeled using wPCA on the vectors of wavelet coeﬃcients, ˜xL
i . Following
Section 6.3.4, the loading matrix for the wavelet PCA model may either be modeled as
standard Gaussian or as uniform orthonormal and is denoted U L. The wavelet PCA model
reads
˜xL
i = U LΛL zL
i + µL + ϵL
i ,
ϵL
i ∼Nd(0, σ2
L I),
µL ∼Nd(0, τ 2
L I),
zL
i ∼Nk(0, I),
σ2
L, τ 2
L, λL
j ∼Cauchy+(0, 5),
j = 1, . . . , k,
λL
j > λL
j′,
j > j′.
(6.18)
Although truncated Cauchy priors are speciﬁed on the scale parameters, most diﬀuse priors
will do. Ordering the scale parameters λj maintains identiﬁability of the loading matrix and
119

helps the MCMC sampler by reducing multi-modality of the density function. As stated
above, the latent zL
i are also featured in the sequential classiﬁcation module of Section 6.4.3
below.
6.4.2
Spike train module
The high dimensional spike trains are modeled using ePCA with the log link. Carrying over
notation from Section 6.3.1, Pois⊗(·) takes in vector arguments and indicates a vector of
conditionally independent Poisson random variables with means the elements of the vector
input. The ePCA spike train model is written thus:
xS
i ∼Pois⊗
 exp{U SΛS zS
i + µS}

,
µS ∼Nd(0, τ 2
S I),
zS
i ∼Nk(0, I)
τ 2
S, λS
j ∼Cauchy+(0, 5),
j = 1, . . . , k,
λS
j > λS
j′,
j > j′.
(6.19)
The low dimensional latent variables zS
i are also featured in the the sequential classiﬁcation
module as input ’data’ for the Bayesian logistic regression.
6.4.3
Sequential classiﬁcation module
We model each sequential status, yi, using Bayesian logistic regression: each yi is distributed
as a Bernoulli random variable with mean given by the inverse logit of the inputs from the
120

Method
0-1 Error
c
lpd
LFP
Spikes
Joint
LFP
Spikes
Joint
sDDR, Gaussian
0.110
0.064
0.060
-57.98
-33.92
-32.25
sDDR, Stiefel
0.106
0.069
0.064
-56.16
-34.48
-32.34
Logistic lasso
0.106
0.092
0.087
-70.81
-53.78
-49.13
Random forest
0.106
0.096
0.106
-66.56
-48.51
-55.13
PLS-DA
0.106
0.073
0.096
—
—
—
Table 6.1: Predictive ﬁt: 10-fold CV results
LFP and spike train modules multiplied by regression coeﬃcients.:
yi ∼Bernoulli
 logit−1(β + βT
S zS
i + βT
LzL
i )

,
(6.20)
β ∼N(0, 102),
βS, βL ∼Nk(0, 102 I) .
The logit function maps probabilities to their respective log-odds p 7→log p/(1 −p); its
inverse maps numbers from R to the interval (0, 1). For this chapter it has been assumed
that k = kS = kL, i.e., that the spike trains and LFP coeﬃcients are given the same latent
dimensionality. Of course, the practitioner is in no way restricted to this assumption.
6.5
Results
6.5.1
Prediction
The sDDR model outperforms other common methods with respect to prediction. To show
this, 10-fold cross-validation is used. We ﬁt two versions of the sDDR model and a number
of competing prediction methods to each training set and evaluate ‘0-1’ loss (predicting
sequential status) and log pointwise predictive densities on each of the ten folds. One instance
of the sDDR model uses an unconstrained Gaussian loading matrix; the other constrains the
121

matrix U to the Stiefel manifold. Cross-validation is done three times for each of three
possible data inputs: LFP alone, spike trains alone, and LFP and spikes combined.
The computed log pointwise predictive density ( c
lpd) is an estimate of the log pointwise
predictive density (lpd), which is itself a measure of the model’s predictive ﬁt to future data
[171]. If having observed training data, y, one uses MCMC to obtain a posterior sample
θ1, . . . , θS, then given hold-out data, yi, . . . , yn, the lpd and c
lpd are given by
lpd =
n
X
i=1
log p(yi|y) =
n
X
i=1
log
Z
p(yi|θ)p(θ|y) dθ
(6.21)
≈
n
X
i=1
log
 1
S
S
X
s=1
p(yi|θs)

= c
lpd
Results comparing both sDDR models to random forest [22], PLS-DA [10], and logistic
regression with lasso [166] are presented in Table (6.1). The left half of Table (6.1) compares
methods with respect to ‘0-1’ loss, and the right half compares with respect to lpd. Within
these halves, the columns show results for LFP input, spike train input, and combined input,
moving from left to right. lpd results are presented for all methods except PLS-DA, for which
lpd in undeﬁned.
None of the methods performed well when presented with LFP data alone. Most achieve
roughly 90% accuracy by predicting in-sequence 100% of the time. The Gaussian sDDR
model actually performs worse here with respect to ‘0-1’ loss: it predicts out-of-sequence
once but for a trial that is actually in-sequence! Despite this, the sDDR model performs
best with respect to lpd, registering an c
lpd of -57.98 for the Gaussian model and a -56.16
for the Stiefel model. Both the poor performance with respect to ‘0-1’ loss and the strong
performance with respect to lpd reinforce the idea that the sDDR model accomplishes more
than the trivial ﬁt to this rare event data. For the spike input models, the sDDR model
performs better with respect to both criteria: it scores a 0.064 error rate for the Gaussian
122

model and a 0.069 for the Stiefel model, followed by PLS-DA at 0.073; it also achieves the
best c
lpd at -33.92 and -34.48, followed by the random forest at -48.51. It is reassuring that
the sDDR model seems to increase its lead over other methods when the input data has
higher predictive capacity, as seems to be the case for the spike data. On the combined
data, the sDDR and logistic lasso models continue to improve, unlike the random forest and
PLS-DA models. The improvement in performance over the spike-only models is small, as
one might expect from the weak performance of the LFP only models. As with the spike-
only models, the hierarchical model performs best: it gets an error of 0.060 for the Gaussian
model and 0.064 for the Stiefel model, compared to the logistic lasso’s 0.087, and c
lpds of
-32.25 and -32.34 compared to the logistic lasso’s -49.13.
Both sDDR variants outperform the competition, but their similar performances are worth
commenting on. One of the chief motivations for using the Stiefel constrained loading ma-
trix is that it avoids the over-parameterization (discussed in Section 6.3.4.2 above) of the
model caused by the rotational invariance of the standard normal latent variables. Another
group of methods that accomplish the same thing, though with a very diﬀerent approach,
fall under the category of independent component analysis (ICA). ICA models avoid this
over-parameterization by modeling the latents as non-Gaussian and therefore no longer ro-
tationally invariant [131]. Indeed, by also including the latents in a logistic regression, the
sDDR can be thought of as inadvertently aﬀecting an ICA-like model. To see this, consider
the logistic regression module as a prior on the latents that makes them non-Gaussian a
priori, as well as making them predictive of the outcome y. This insight could explain the
similar performance of the two loading matrix models with respect to predictive ﬁt: thanks
to the logistic regression module, they are approximately the same model.
123

G
G
G
G
G G
G
G
G
G G G G G G
G
G G G G G
G
G G
G
G G G G
G
G
G G
G
G G
G G G G
G
G G
G
G G G G G G
G
G
−10
0
10
1
5
9
13
17
21
25
29
33
37
41
45
49
Neurons
Association (UΛβ)j
Signif.
G
G
Yes
No
Influential neurons
Figure 6.3: 95% credible intervals for the elements of association vector U S ΛS βS (Equation
(6.5.2.1)) corresponding to each of the 52 neurons modeled. Orange are point-wise statisti-
cally signiﬁcant; blue are not. The fact that few neurons are registered signiﬁcant and that
one or two have higher orders of magnitude suggests that low- dimensional representations
should be suﬃcient for prediction.
6.5.2
Variable selection and scientiﬁc inference
6.5.2.1
Identifying inﬂuential predictors
In addition to prediction, the hierarchical model can assist in variable selection and mean-
ingful inference. This is done through direct interpretation of the model parameters. Recall
that both the LFP and spike modules explain the variability of the input variable, xi, by
some low-dimensional variable, zi. In the case of the spike counts, the relationship is medi-
ated by the log-link function, but it is still useful to consider the log-intensity U Λ zi ≈log xi.
Consider then the covariance of this log-intensity or of the wavelet coeﬃcients, ˜xi, with the
log-odds of an odor being in-sequence. This covariance is directly assessable for each neuron
and each wavelet coeﬃcient in the model. In the case of the wavelet coeﬃcients, this may
be written
Cov
 ˜xi, log P(yi = 1)
P(yi = 0)

= Cov(U Λ zi, βTzi) = U Λ Cov(zi, zi) β = U Λ β .
(6.22)
124

G
G
G
G
G G G
G
G G G G G G G G
G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G
−75
−50
−25
0
25
1
5
9
13
17
21
25
29
33
37
41
45
49
53
57
61
Coefficients
Association (UΛβ)j
Signif.
G
G
Yes
No
Influential wavelet coefficients
Figure 6.4: 95% credible intervals for the elements of association vector U L ΛL βL (Equation
(6.5.2.1)) corresponding to each of the 64 wavelet coeﬃcients modeled. Orange are point-
wise statistically signiﬁcant; blue are not. Vertical lines divide coeﬃcients into scale groups
moving from coarse to ﬁne from left to right. Within scales, coeﬃcients move from early
time series to late from left to right.
Thus U Λ β is a vector whose jth element is a measure of the association of each wavelet
coeﬃcient (or neuron) with the outcome. MCMC gives the posterior distribution over this
vector. Figures (6.3) and (6.4) present 95% HPD intervals for the elements of this association
vector for all neurons and wavelet coeﬃcients.
For both ﬁgures, orange intervals imply
point-wise statistical signiﬁcance, and blue imply a lack thereof. In Figure (6.3), only a few
neurons are shown to be inﬂuential. In particular, neurons one and three are associated with
an odor being in-sequence, while neuron seven is strongly associated with an odor being
out-of-sequence.
Figure (6.4) shows a clear pattern in terms of the scales and translations associated to the
various wavelet coeﬃcients.
The plot is divided into scale blocks by vertical lines.
The
leftmost block features the summation and diﬀerence coeﬃcients associated with the half-
scale, i.e., that pertaining to the time series being divided into halves. The next block is
on the quartic scale and so on. In other words, the scale becomes ﬁner and ﬁner moving
from block to block towards the right.
Note that the low frequencies seem to have the
125

0.0
2.5
5.0
7.5
10.0
0
250
500
750
1000
MCMC iteration
Posterior values
Spikes
0.0
2.5
5.0
7.5
10.0
0
250
500
750
1000
MCMC iteration
LFP
Component
1
2
Figure 6.5: The posterior trace plots of the absolute value of logistic regression parame-
ters associated with the two dimensional latent representations zS
i , zL
i of the spike and LFP
data. Non-zero distributions suggest signiﬁcant association between low-dimensional repre-
sentations and outcome, but to know the direction of this association one must view the
low-dimensional representations themselves. See Figure (6.6).
largest association with the outcome: the leftmost block is entirely signiﬁcant, the next is
half signiﬁcant, the third is a quarter. Apparently, the ﬁnal block is too ﬁne to be predictive
of outcome. Within blocks, another pattern emerges. The rightmost elements of each block
(save the last block) are predictive of outcome. These elements pertain to the rightmost
translations in time. In other words, these are the wavelet coeﬃcients that correspond to
the tail of the time series within their respective scales. This suggests that the neural activity
occurring right before the rat removes its nose from the port is most informative about the
rat’s response.
6.5.2.2
Logistic regression coeﬃcients
The posterior distributions of the regression parameters β and γ reﬂect the prediction results.
Figure (6.5) presents the absolute values of 1,000 draws from the posterior distributions of
the logistic coeﬃcients associated with both latent factors from both the LFP and the spike
126

−2
0
2
−2
−1
0
1
2
Component 2
All odors
−2
0
2
−2
−1
0
1
2
Odor A
−2
0
2
−2
−1
0
1
2
Odor B
−2
0
2
−2
−1
0
1
2
Component 1
Component 2
Odor C
−2
0
2
−2
−1
0
1
2
Component 1
Odor D
−2
0
2
−2
−1
0
1
2
Component 1
Odor E
Sequence
Out
In
Figure 6.6: One hundred draws from posterior distribution of low dimensional representations
zS
i of the spike data. The horizontal axis is the ﬁrst principal component, the vertical is the
second. Blue signiﬁes in-sequence, and orange signiﬁes out-of-sequence trials. Figure divided
into plots stratiﬁed by odor presented.
module. For both LFP and spike modules the second principal component is the signiﬁcant
predictor of sequential status, but the magnitude of the second coeﬃcient corresponding to
the spike module is larger than that of that corresponding to the LFP data. This result
agrees with the idea that the spike data contains more predictive content than does the LFP
data.
The regression coeﬃcients relate low-dimensional patterns in spike counts and LFPs to
whether the model believes a sequence is correctly ordered.
Their distributions support
the hypothesis that the rat hippocampus is a place where sequential learning is performed.
Here learning is meant to suggest a global phenomenon, one involving relationships between
individual neurons and groups thereof. Figure (6.5) aﬃrms the hypothesis in a speciﬁc sense:
127

if a coeﬃcient has a signiﬁcantly non-zero posterior distribution, then intensity of the relevant
latent variable corresponds to the increased or decreased odds of sequential correctness.
6.5.2.3
Low dimensional visualization
The posterior distributions of the low dimensional representations zi give insight into the
predictive content available in the data modalities. Figure (6.6) shows contour plots derived
from posterior samples of the low dimensional representations zS
i of the spike data, colored
by sequential status and separated by odor presented. Combining all odors together, one
can see that there are distinct in-sequence and out-of-sequence clusters, but that the out-
of-sequence cluster has two distinct sub-clusters: one overlaps with the in-sequence cluster,
and one is completely separate from the in-sequence cluster. The root of this bimodality can
be seen in the odor speciﬁc plots. It is clear that the ability of the rat to detect sequential
status varies by the odor presented. Odor A has the clearest distinction in sequential status,
odor B has the next biggest distinction, and the other three odors all have poor separation.
This result suggests that it is easier for the rat to remember the correct order early on in the
sequence. It seems reasonable that the rat should be able to remember the ﬁrst two odors
with which the sequence starts. It is also reasonable that the the ability to detect out-of-
sequence odors should degrade with progress into the sequence. As one might guess from the
poor predictive performance of the LFP only model, the low dimensional representations of
the LFP data show no such separation.
6.6
Discussion
We have developed a Bayesian framework for supervised dual-dimensionality reduction and
used it to perform neural decoding where multiple data modalities were available. In a one
128

step process, low-dimensional representations of both LFP and spike train data were found
and fed into a logistic regression model. The hierarchical model achieved better predictive
ﬁt than multiple competitors, as measured by ‘0-1’ loss and log pointwise predictive density.
Results were obtained for two diﬀerent versions of the sDDR, one with a Gaussian loading
matrix model, the other with a Stiefel constrained loading matrix model.
Bayesian inference allowed for the ﬂexible incorporation of the two data modalities in a way
that automatically accounted for their predictive capacities. In terms of prediction, the sDDR
model was at its best with the combined data. The spike train only model outperformed the
LFP only model with respect to both predictive measures. The posterior distributions of
model parameters such as logistic regression coeﬃcients and low dimensional representations
also reinforced the idea that the spike counts had greater predictive content than the LFP
data. That said, the model’s ability to exploit the LFP data was completely dependent on
the form in which the LFP data was presented to the model, i.e. as a vector of wavelet coef-
ﬁcients taken from averaging multiple channels. Transforming time series data into wavelet
coeﬃcients involves the throwing away of information (as does averaging over channels), so it
is plausible that incorporating the LFP data in another form might provide superior results.
Whereas both LFP and spike modules featured extensions of Bayesian PCA, it might be pru-
dent to model the LFP data using functional logistic regression. Preliminary results using
Gaussian process regression coeﬃcients have been promising. Whether to perform functional
logistic regression on the time series data or on the periodogram is an open question, and
its answer may be application dependent. A supervised functional PCA approach, obtained
by combining PCA and functional logistic regression, might prove ideal. Possibilities aside,
the sDDR model provides a ﬁrm basis for future multimodal neural decoding eﬀorts.
129

Chapter 7
Future directions: Bayesian inference
on inﬁnite manifolds
One of the most immediate extensions of the methodology presented in this dissertation is the
extension from ﬁnite dimensional manifolds to Hilbert manifolds. Many problems in statistics
and applied mathematics require a solution that belongs to a set of constrained functions.
Although these functional constraints (e.g. non-negativity, orthogonality, normality, etc.)
often complicate problems, structure and symmetries on the solution space can be leveraged
to simplify statistical models and speed-up computations. On the one hand, manifolds are
a large class of spaces that are useful in many of the mathematical sciences [65, 84]. On the
other hand, functions are the objects of interest, but spaces of functions are (in most cases)
inﬁnite-dimensional. Future research may demonstrate that manifold structure can provide
order to a statistical problem, even when the solution space is an inﬁnite-dimensional space
of functions.
Bayesian inference on functional manifolds necessitates new computational and modelling
techniques that rely upon the foundations of diﬀerential geometry, inverse-problems, and
130

Bayesian nonparametrics. Such techniques would allow for scalable applications of Bayesian
methods to multiple complex spatial and temporal problems.
In Chapter 5, I used a reinterpretation of information geometry [4] to inspire a new approach
to Bayesian nonparametric density estimation [92]. Whereas density estimation has tradi-
tionally focused on functions that are non-negative and integrate to unity, I focused on the
square-root of this function and thus identiﬁed the problem with Bayesian inference on the
L2 sphere
Q :=

q : D →R |
Z
D
q(x)2 dx = 1

,
(7.1)
for given domain D. I modeled the square-root density with a GP prior [148, 9] (or a Gaussian
measure in L2) multiplied by the Dirac measure restricting the function to the unit sphere:
q ∼GP × δq(Q) .
(7.2)
Computationally, it is much easier to enforce the constraint to the L2 sphere—given by Dirac
measure δq(Q)—than it is to constrain a function to integrate to 1. This is accomplished by
using the Karhunen-Lo`eve decomposition [173] of the random function q(·) and truncating for
suﬃciently high frequencies. Note that this basis truncation may also be learned adaptively
in a Bayesian fashion [37]. The left of Figure 7.1 shows samples drawn from the posterior
distribution of density function p(·) = q(·)2 using geodesic Monte Carlo on the ﬁnite sphere
[25].
Along with the development of additional Bayesian manifold nonparametric models (below),
there is a need for two major theoretical developments: (i) the development of Bayesian
inference algorithms that are well-deﬁned and eﬃcient in the inﬁnite-dimensional limit (Sec-
tion 7.3); and (ii) the development of statistical consistency results of estimators in the spirit
of [155] but taking the geometry of the model space into account.
131

|||||||||||||| |||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||| ||| |||||||||||||| || ||| || |||||||||||||||||||||||| ||||| |
| ||
0
1
2
3
1850
1875
1900
1925
1950
Year
Density values
Random density functions
−0.5
0.0
0.5
1.0
0
100
200
300
400
500
Milliseconds (ms)
Rescaled voltage
Local field potentials
|||||||||||||| |||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||| ||| |||||||||||||| || ||| || |||||||||||||||||||||||| ||||| |
| ||
0
1
2
3
1850
1875
1900
1925
1950
Year
Density values
Random density functions
−0.5
0.0
0.5
1.0
0
100
200
300
400
500
Milliseconds (ms)
Rescaled voltage
Local field potentials
Figure 7.1: Left, 100 posterior draws from my manifold nonparametric density model (gray)
over 191 vertical lines (red) marking the precise date of each disaster (Coal mining disasters
data [92]). Right, 12 local ﬁeld potentials recorded in rat hippocampus (Section 7.1). This
functional data is ripe for dimensionality reduction via the inﬁnite Stiefel manifold.
7.1
Dimensionality reduction
The central idea from Chapter 5—i.e. restricting a Gaussian measure to a manifold em-
bedded in Hilbert space—can be used for a completely diﬀerent application: functional
dimensionality reduction of brain signals. Local ﬁeld potentials (LFP) (Figure 7.1, right)
are a measure of electric current recorded from multiple neurons at a time [46]. In Chapter
6, I used a geometric Bayesian hierarchical model to decompose LFP data into latent signals
and accurately predict experimental outcomes. The challenge is to decompose the 12 func-
tions in Figure 7.1 into orthonormal functions of a lesser number that somehow contain the
majority of the information contained in the original signals. This task is often referred to
as functional PCA and is an eﬃcient way to perform scientiﬁc analysis of high-dimensional
data [79].
In Chapter 6, I performed a wavelet decomposition before applying Bayesian
methodology, but it is possible to remove the ﬁrst step and work entirely in the Bayesian
paradigm.
Let y(t) = (y1(t), . . . yp(t))T be a p-dimensional vector process parameterized by time t. Let
Hd = ⊗d
i=1Hi be a product of d copies of L2 Hilbert space with temporal domain T , and let
132

Sd,∞be the d-dimensional Stiefel manifold embedded in Hd, i.e.
Sd,∞:=

(q1, . . . , qd)T ∈Hd|
Z
T
qi(t) qj(t) dt = δj
i

,
(7.3)
where δj
i is the Kronecker delta. Then, given observations y(t) recorded at times t1, . . . , tT,
Bayesian nonparametric functional PCA may be performed using the following generative
model:
y(t) = L q(t) + ϵt ,
q ∼⊗d
i=1GPi × δq(Sd,∞) ,
ϵt
iid
∼N(0, σ) ,
(7.4)
where L is a p-by-d loading matrix, and q is a vector of d Gaussian processes restricted to
the Stiefel manifold Sd,∞. In addition to the vector process q, L may also be learned using
geodesic Monte Carlo methods, as is shown in Chapter 6.
Just as in the density estimation example, computations are performed by passing from the
inﬁnite to the ﬁnite Stiefel manifold using the Karhunen-Lo`eve expansion of the individual
Gaussian processes [173]. Again, theoretical study (cf. [155]) of consistency for approxima-
tion based estimates will be required.
7.2
Learning the quantum wave
There is also the possibility of future work in non-traditional areas of statistical application,
particularly uncertainty quantiﬁcation for quantum systems. It is ironic that quantum sys-
tems are inherently probabilistic, but uncertainty for quantum Monte Carlo (QMC)[80, 28,
81] based estimates is rarely modeled. The behavior of a quantum system in space D, is com-
pletely described by the wave function Ψ(x, t) that solves the (non-relativistic) Schr¨odinger
133

equation [149, 71]:
iℏ∂
∂tΨ(x, t) =
−ℏ2
2µ ∇2 + V (x, t)

Ψ(x, t) ,
(7.5)
where x ∈D denotes position, t denotes time, and Ψ satisﬁes
1 =
Z
X
|Ψ(x, t)|2dx .
(7.6)
That is, Ψt(·) = Ψ(·, t) is the square-root of the density function describing the probability
of the particle being in position x at time t. Thus, Ψt plays the exact same role as function
q(·) in Equation (7.2), with the additional constraint that for times s and t, Ψs and Ψt are
related by the deterministic Schr¨odinger equation. The dynamics implied by the Schr¨odinger
equation also enforce that Ψ be complex valued. In the notation of Equations (7.2) and (7.4),
I propose the following Bayesian manifold nonparametric model:
Ψt ∼GP × δΨ(CP∞) ,
(7.7)
where CP∞is the inﬁnite-dimensional projective Hilbert space [72] (and each ‘ray’ is iden-
tiﬁed with its member that integrates to unity). The details are surprisingly similar to the
manifold nonparametric density estimation of Chapter 6, and extensions to multi-particle
fermionic systems can be made using the Slater determinant [157]. Through manifold non-
parametrics, this project would serve to introduce quantum physics to Bayesian inverse
problems and vice-versa. The existence of less ﬂexible Bayesian methods [119, 107, 69] for
quantum data integration combined with interest within the statistics community [7] sug-
gests that the proposed research would have substantial impact and help extend the scientiﬁc
domain of statistics.
134

7.3
Computing on inﬁnite manifolds
HMC has facilitated eﬃcient computing for state spaces that were formerly diﬃcult to
perform Bayesian inference on: embedding geodesic Monte Carlo [25] (Chapters 4-6) and
geodesic Lagrangian Monte Carlo (Chapter 2) eﬃciently perform Bayesian inference on many
ﬁnite dimensional manifolds. In parallel to the development of HMC algorithms for ﬁnite
manifolds, the HMC class has been extended to compute probability measures on Hilbert
spaces [19, 37]. This inﬁnite-dimensional extension has been helpful for the growth of the
ﬁeld of Bayesian inverse problems.
As of today, HMC has not been extended to manifolds of functions. One reason is that such
an extension has been mostly unmotivated outside the density estimation model of Chapter
5. Work is needed that builds on ﬁnite manifold HMC algorithms so that it provably and
eﬃciently scales in the inﬁnite-dimensional limit.
135

Bibliography
[1] Y. I. Abramovich and O. Besson. On the expected likelihood approach for assessment
of regularization covariance matrix. IEEE Signal Processing Letters, 22(6):777–781,
2015.
[2] H. Akaike. Information theory and an extension of the maximum likelihood principle.
In Selected Papers of Hirotugu Akaike, pages 199–213. Springer, 1998.
[3] T. A. Allen, D. M. Salz, S. McKenzie, and N. J. Fortin. Nonspatial sequence coding
in CA1 neurons. The Journal of Neuroscience, 36(5):1547–1563, 2016.
[4] S.-i. Amari and H. Nagaoka. Methods of information geometry, volume 191. American
Mathematical Soc., 2007.
[5] T. W. Anderson and I. Olkin. Maximum-likelihood estimation of the parameters of
a multivariate normal distribution. Linear algebra and its applications, 70:147–171,
1985.
[6] C. E. Antoniak. Mixtures of dirichlet processes with applications to Bayesian nonpara-
metric problems. The annals of statistics, pages 1152–1174, 1974.
[7] L. Artiles, R. Gill, et al. An invitation to quantum tomography. Journal of the Royal
Statistical Society: Series B (Statistical Methodology), 67(1):109–134, 2005.
[8] B. R. Bakshi.
Multiscale PCA with application to multivariate statistical process
monitoring. AIChE journal, 44(7):1596–1610, 1998.
[9] S. Banerjee, B. P. Carlin, and A. E. Gelfand. Hierarchical modeling and analysis for
spatial data. Crc Press, 2014.
[10] M. Barker and W. Rayens. Partial least squares for discrimination. Journal of chemo-
metrics, 17(3):166–173, 2003.
[11] M. A. Beaumont, W. Zhang, and D. J. Balding. Approximate Bayesian computation
in population genetics. Genetics, 162(4):2025–2035, 2002.
[12] R. S. Behara, A. Agarwal, P. Pulumati, R. Jain, and V. Rao. Predictive modeling for
wellness and chronic conditions. In Bioinformatics and Bioengineering (BIBE), 2014
IEEE International Conference on, pages 394–398. IEEE, 2014.
136

[13] M. Belkin and P. Niyogi. Laplacian eigenmaps for dimensionality reduction and data
representation. Neural computation, 15(6):1373–1396, 2003.
[14] M. Belkin, P. Niyogi, and V. Sindhwani. Manifold regularization: A geometric frame-
work for learning from labeled and unlabeled examples. Journal of machine learning
research, 7(Nov):2399–2434, 2006.
[15] L. Berg. Three results in connection with inverse matrices. Linear Algebra and its
Applications, 84:63–77, 1986.
[16] J. O. Berger, J. M. Bernardo, and D. Sun. The formal deﬁnition of reference priors.
The Annals of Statistics, pages 905–938, 2009.
[17] J. Bernardo, M. Bayarri, J. Berger, A. Dawid, D. Heckerman, A. Smith, and M. West.
Bayesian factor regression models in the ‘large p, small n’ paradigm. Bayesian statis-
tics, 7:733–742, 2003.
[18] A. Beskos, M. Girolami, S. Lan, P. E. Farrell, and A. M. Stuart. Geometric MCMC for
inﬁnite-dimensional inverse problems. Journal of Computational Physics, 335:327–351,
2017.
[19] A. Beskos, F. J. Pinski, J. M. Sanz-Serna, and A. M. Stuart. Hybrid Monte Carlo on
Hilbert spaces. Stochastic Processes and their Applications, 121(10):2201–2230, 2011.
[20] M. Betancourt. Generalizing the no-u-turn sampler to Riemannian manifolds. arXiv
preprint arXiv:1304.1920, 2013.
[21] C. M. Bishop. Bayesian PCA. Advances in Neural Information Processing Systems,
11:382–388, 1999.
[22] L. Breiman. Random forests. Machine learning, 45(1):5–32, 2001.
[23] P. J. Brockwell and R. A. Davis. Time series: theory and methods. Springer Science
& Business Media, 2013.
[24] E. N. Brown, L. M. Frank, D. Tang, M. C. Quirk, and M. A. Wilson. A statisti-
cal paradigm for neural spike train decoding applied to position prediction from en-
semble ﬁring patterns of rat hippocampal place cells. The Journal of Neuroscience,
18(18):7411–7425, 1998.
[25] S. Byrne and M. Girolami. Geodesic Monte Carlo on embedded manifolds. Scandina-
vian Journal of Statistics, 40(4):825–845, 2013.
[26] B. Carpenter, A. Gelman, M. Hoﬀman, D. Lee, B. Goodrich, M. Betancourt, M. A.
Brubaker, J. Guo, P. Li, and A. Riddell. Stan: A probabilistic programming language.
Journal of Statistical Software, 20:1–37, 2016.
[27] M. H. Castaneda and J. A. Nossek. Estimation of rank deﬁcient covariance matrices
with kronecker structure. In Acoustics, Speech and Signal Processing (ICASSP), 2014
IEEE International Conference on, pages 394–398. IEEE, 2014.
137

[28] D. Ceperley, G. V. Chester, and M. H. Kalos. Monte Carlo simulation of a many-
fermion study. Phys. Rev. B, 16:3081–3099, Oct 1977.
[29] J. C. Chan, R. Leon-Gonzales, and R. W. Strachan. Invariant inference and eﬃcient
computation in the static factor model. 2013.
[30] T. Chen, J. Streets, and B. Shahbaba. A geometric view of posterior approximation.
arXiv preprint arXiv:1510.00861, 2015.
[31] K. L. Chung.
Lectures from Markov processes to Brownian motion, volume 249.
Springer Science & Business Media, 2013.
[32] G. Claeskens and N. L. Hjort. Model Selection and Model Averaging, volume 330.
Cambridge University Press Cambridge, 2008.
[33] S. Cl´emen¸con, P. Bertail, and G. Papa. Learning from survey training samples: Rate
bounds for Horvitz–Thompson risk minimizers.
In Asian Conference on Machine
Learning, pages 142–157, 2016.
[34] A. A. Cohen, V. Morissette-Thomas, L. Ferrucci, and L. P. Fried. Deep biomarkers of
aging are population-dependent. Aging (Albany NY), 8(9):2253, 2016.
[35] M. Collins, S. Dasgupta, and R. E. Schapire. A generalization of principal compo-
nents analysis to the exponential family. In Advances in neural information processing
systems, pages 617–624, 2001.
[36] J. Coresh, B. C. Astor, G. McQuillan, J. Kusek, T. Greene, F. Van Lente, and A. S.
Levey.
Calibration and random variation of the serum creatinine assay as critical
elements of using equations to estimate glomerular ﬁltration rate. American Journal
of Kidney Diseases, 39(5):920–929, 2002.
[37] S. L. Cotter, G. O. Roberts, A. M. Stuart, D. White, et al. MCMC methods for func-
tions: modifying old algorithms to make them faster. Statistical Science, 28(3):424–446,
2013.
[38] D. R. Cox. Some statistical methods connected with series of events. Journal of the
Royal Statistical Society. Series B (Methodological), pages 129–164, 1955.
[39] M. S. Crouse, R. D. Nowak, and R. G. Baraniuk. Wavelet-based statistical signal
processing using hidden markov models.
IEEE Transactions on signal processing,
46(4):886–902, 1998.
[40] M. Dashti and A. M. Stuart.
The Bayesian approach to inverse problems.
arXiv
preprint arXiv:1302.6989, 2013.
[41] I. Daubechies et al. Ten lectures on wavelets, volume 61. SIAM, 1992.
[42] P. J. Diggle and R. K. Milne. Bivariate cox processes: some models for bivariate spatial
point patterns. Journal of the Royal Statistical Society. Series B (Methodological),
pages 11–21, 1983.
138

[43] J. F. Dipnall, J. A. Pasco, M. Berk, L. J. Williams, S. Dodd, F. N. Jacka, and D. Meyer.
Fusing data mining, machine learning and traditional statistics to detect biomarkers
associated with depression. PloS One, 11(2):e0148195, 2016.
[44] J. F. Dipnall, J. A. Pasco, M. Berk, L. J. Williams, S. Dodd, F. N. Jacka, and D. Meyer.
Into the bowels of depression: Unravelling medical symptoms associated with depres-
sion by applying machine-learning techniques to a community based population sample.
PloS One, 11(12):e0167055, 2016.
[45] M. P. do Carmo Valero. Riemannian geometry. 1992.
[46] J. P. Donoghue, J. N. Sanes, N. G. Hatsopoulos, and G. Ga´al. Neural discharge and
local ﬁeld potential oscillations in primate motor cortex during voluntary movements.
Journal of neurophysiology, 79(1):159–173, 1998.
[47] G. Dragoi and G. Buzs´aki. Temporal encoding of place sequences by hippocampal cell
assemblies. Neuron, 50(1):145–157, 2006.
[48] I. L. Dryden et al. Statistical analysis on high-dimensional spheres and shape spaces.
The Annals of Statistics, 33(4):1643–1665, 2005.
[49] S. Duane, A. D. Kennedy, B. J. Pendleton, and D. Roweth. Hybrid Monte Carlo.
Physics letters B, 195(2):216–222, 1987.
[50] M. M. Dunlop, M. A. Iglesias, and A. M. Stuart.
Hierarchical Bayesian level set
inversion. Statistics and Computing, 27(6):1555–1584, 2017.
[51] A. Edelman, T. A. Arias, and S. T. Smith. The geometry of algorithms with orthogo-
nality constraints. SIAM journal on Matrix Analysis and Applications, 20(2):303–353,
1998.
[52] B. Efron. The geometry of exponential families. The Annals of Statistics, 6(2):362–376,
1978.
[53] B. Efron. How biased is the apparent error rate of a prediction rule? Journal of the
American Statistical Association, 81(394):461–470, 1986.
[54] B. Efron. The estimation of prediction error: covariance penalties and cross-validation.
Journal of the American Statistical Association, 99(467):619–632, 2004.
[55] B. Efron and R. J. Tibshirani. An introduction to the bootstrap. CRC press, 1994.
[56] A. Elgammal and C.-S. Lee. Inferring 3d body pose from silhouettes using activity
manifold learning. In Computer Vision and Pattern Recognition, 2004. CVPR 2004.
Proceedings of the 2004 IEEE Computer Society Conference on, volume 2, pages II–II.
IEEE, 2004.
[57] F. Fazayeli and A. Banerjee. The matrix generalized inverse Gaussian distribution:
Properties and applications. arXiv preprint arXiv:1604.03463, 2016.
139

[58] C. Feﬀerman, S. Mitter, and H. Narayanan. Testing the manifold hypothesis. Journal
of the American Mathematical Society, 29(4):983–1049, 2016.
[59] G.-C. Feng, P. C. Yuen, and D.-Q. Dai. Human face recognition using PCA on wavelet
subband. Journal of Electronic Imaging, 9(2):226–233, 2000.
[60] R. A. Fisher. Theory of statistical estimation. In Mathematical Proceedings of the
Cambridge Philosophical Society, volume 22, pages 700–725. Cambridge Univ Press,
1925.
[61] R. A. Fisher. The use of multiple measurements in taxonomic problems. Annals of
eugenics, 7(2):179–188, 1936.
[62] D. J. Foster and M. A. Wilson. Reverse replay of behavioural sequences in hippocampal
place cells during the awake state. Nature, 440(7084):680–683, 2006.
[63] W. A. Fuller. Sampling Statistics, volume 560. John Wiley & Sons, 2011.
[64] X. Gao, B. Shahbaba, N. Fortin, and H. Ombao.
Evolutionary state-space model
and its application to time-frequency analysis of local ﬁeld potentials. arXiv preprint
arXiv:1610.07271, 2016.
[65] R. Gilmore. Lie groups, physics, and geometry: an introduction for physicists, engi-
neers and chemists. Cambridge University Press, 2008.
[66] M. Girolami and B. Calderhead. Riemann manifold langevin and Hamiltonian Monte
Carlo methods. Journal of the Royal Statistical Society: Series B (Statistical Method-
ology), 73(2):123–214, 2011.
[67] G. H. Golub and V. Pereyra. The diﬀerentiation of pseudo-inverses and nonlinear
least squares problems whose variables separate. SIAM Journal on numerical analysis,
10(2):413–432, 1973.
[68] C. Gourieroux and A. Monfort. Statistics and econometric models, volume 1. Cam-
bridge University Press, 1995.
[69] C. Granade, J. Combes, and D. Cory. Practical Bayesian tomography. New Journal
of Physics, 18(3):033024, 2016.
[70] P. J. Green. Reversible jump markov chain Monte Carlo computation and Bayesian
model determination. Biometrika, 82(4):711–732, 1995.
[71] D. J. Griﬃths. Introduction to quantum mechanics. Cambridge University Press, 2016.
[72] A. Grigorenko. Geometry of projective Hilbert space. Physical Review A, 46(11):7292,
1992.
[73] H. Guetterman, L. Auvil, N. Russell, M. Welge, M. Berry, L. Gatzke, C. Bushell, and
H. Holscher. Utilizing machine learning approaches to understand the interrelationship
of diet, the human gastrointestinal microbiome, and health. The FASEB Journal, 30(1
Supplement):406–3, 2016.
140

[74] T. Gunter, M. A. Osborne, R. Garnett, P. Hennig, and S. J. Roberts. Sampling for
inference in probabilistic models with fast Bayesian quadrature. In Advances in neural
information processing systems, pages 2789–2797, 2014.
[75] A. S. Gupta, M. A. van der Meer, D. S. Touretzky, and A. D. Redish. Segmentation of
spatial experience by hippocampal theta sequences. Nature neuroscience, 15(7):1032–
1039, 2012.
[76] M. R. Gupta and N. P. Jacobson. Wavelet principal component analysis and its appli-
cation to hyperspectral images. In 2006 International Conference on Image Processing,
pages 1585–1588. IEEE, 2006.
[77] M. G. Gustafsson. A probabilistic derivation of the partial least-squares algorithm.
Journal of chemical information and computer sciences, 41(2):288–294, 2001.
[78] E. Hairer, C. Lubich, and G. Wanner. Geometric numerical integration: structure-
preserving algorithms for ordinary diﬀerential equations, volume 31. Springer Science
& Business Media, 2006.
[79] P. Hall and M. Hosseini-Nasab.
On properties of functional principal components
analysis. Journal of the Royal Statistical Society: Series B (Statistical Methodology),
68(1):109–126, 2006.
[80] B. L. Hammond, W. A. Lester, and P. J. Reynolds. Monte Carlo methods in ab initio
quantum chemistry, volume 1. World Scientiﬁc, 1994.
[81] A. Harju, B. Barbiellini, S. Siljam¨aki, R. M. Nieminen, and G. Ortiz.
Stochastic
gradient approximation: An eﬃcient method to optimize many-body wave functions.
Phys. Rev. Lett., 79:1173–1177, Aug 1997.
[82] T. Hastie, R. Tibshirani, and J. Friedman.
The Elements of Statistical Learning.
Springer Series in Statistics, New York, 2001.
[83] H. He and E. A. Garcia.
Learning from imbalanced data.
IEEE Transactions on
knowledge and data engineering, 21(9):1263–1284, 2009.
[84] S. Helgason. Diﬀerential geometry and symmetric spaces, volume 12. Academic press,
1962.
[85] S. Helgason.
Diﬀerential geometry, Lie groups, and symmetric spaces, volume 80.
Academic press, 1979.
[86] A. Heredia-Langner, K. H. Jarman, B. G. Amidan, and J. G. Pounds. Genetic al-
gorithms and classiﬁcation trees in feature discovery: diabetes and the NHANES
database. In Proceedings of the International Conference on Data Mining (DMIN),
page 1. The Steering Committee of The World Congress in Computer Science, Com-
puter Engineering and Applied Computing (WorldComp), 2013.
141

[87] A. Hjorungnes and D. Gesbert. Complex-valued matrix diﬀerentiation: Techniques
and key results. IEEE Transactions on Signal Processing, 55(6):2740–2746, 2007.
[88] P. D. Hoﬀ. Simulation of the matrix Bingham–von Mises–Fisher distribution, with ap-
plications to multivariate and relational data. Journal of Computational and Graphical
Statistics, 18(2):438–456, 2009.
[89] P. D. Hoﬀ. Model averaging and dimension selection for the singular value decompo-
sition. Journal of the American Statistical Association, 2012.
[90] A. Holbrook. Diﬀerentiating the pseudo determinant. Linear Algebra and its Applica-
tions, 548:293–304, 2018.
[91] A. Holbrook. Note on the geodesic monte carlo. arXiv preprint arXiv:1805.05289,
2018.
[92] A. Holbrook, S. Lan, J. Streets, and B. Shahbaba.
The nonparametric Fisher
information geometry and the chi-square process density prior.
arXiv preprint
arXiv:1707.03117, 2017.
[93] A. Holbrook, S. Lan, A. Vandenberg-Rodes, and B. Shahbaba. Geodesic lagrangian
Monte Carlo over the space of positive deﬁnite matrices: with application to Bayesian
spectral density estimation.
Journal of Statistical Computation and Simulation,
88(5):982–1002, 2018.
[94] A. Holbrook, A. Vandenberg-Rodes, N. Fortin, and B. Shahbaba. A Bayesian super-
vised dual-dimensionality reduction model for simultaneous decoding of LFP and spike
train signals. Stat, 6(1):53–67, 2017.
[95] A. Holbrook, A. Vandenberg-Rodes, and B. Shahbaba. Bayesian inference on matrix
manifolds for linear dimensionality reduction. arXiv preprint arXiv:1606.04478, 2016.
[96] X. Hong and J. Gao. A fast algorithm to estimate the square root of probability density
function. In Research and Development in Intelligent Systems XXXIII: Incorporating
Applications and Innovations in Intelligent Systems XXIV 33, pages 165–176. Springer,
2016.
[97] D. G. Horvitz and D. J. Thompson. A generalization of sampling without replacement
from a ﬁnite universe. Journal of the American statistical Association, 47(260):663–
685, 1952.
[98] H. Hotelling. Relations between two sets of variates. Biometrika, 28(3/4):321–377,
1936.
[99] M. J. Hutchings. Standing crop and pattern in pure stands of mercurialis perennis and
rubus fruticosus in mixed deciduous woodland. Oikos, pages 351–357, 1978.
[100] M. Itoh and H. Satoh. Geometry of Fisher information metric and the barycenter map.
Entropy, 17(4):1814–1849, 2015.
142

[101] H. Jeﬀreys. An invariant form for the prior probability in estimation problems. In
Proceedings of the Royal Society of London a: mathematical, physical and engineering
sciences, volume 186, pages 453–461. The Royal Society, 1946.
[102] R. A. Johnson, D. W. Wichern, et al. Applied multivariate statistical analysis, volume 4.
Prentice hall Englewood Cliﬀs, NJ, 1992.
[103] I. Jolliﬀe. Principal component analysis. Wiley Online Library, 2002.
[104] I. T. Jolliﬀe. A note on the use of principal components in regression. Applied Statistics,
pages 300–303, 1982.
[105] O. Knill. Cauchy–binet for pseudo-determinants. Linear Algebra and its Applications,
459:522–547, 2014.
[106] S. S. Koh. On aﬃne symmetric spaces. Transactions of the American Mathematical
Society, 119(2):291–309, 1965.
[107] K. Kravtsov, S. Straupe, I. Radchenko, N. Houlsby, F. Husz´ar, and S. Kulik. Experi-
mental adaptive Bayesian tomography. Physical Review A, 87(6):062122, 2013.
[108] A. Kumar Gopal. Chronological age estimation of an individual using machine learning
algorithms. 2010.
[109] S. Kurtek and K. Bharath. Bayesian sensitivity analysis with the Fisher–Rao metric.
Biometrika, 102(3):601–616, 2015.
[110] S. Lan and B. Shahbaba. Sampling Constrained Probability Distributions Using Spher-
ical Augmentation, pages 25–71. Springer International Publishing, Cham, 2016.
[111] S. Lan, V. Stathopoulos, B. Shahbaba, and M. Girolami.
Markov chain Monte
Carlo from lagrangian dynamics. Journal of Computational and Graphical Statistics,
24(2):357–378, 2015.
[112] S. Lan, B. Zhou, and B. Shahbaba. Spherical Hamiltonian Monte Carlo for constrained
target distributions. In JMLR workshop and conference proceedings, volume 32, page
629. NIH Public Access, 2014.
[113] L. Le Cam. Asymptotic methods in statistical decision theory. Springer Science &
Business Media, 2012.
[114] J. Lee. Introduction to smooth manifolds, volume 218. Springer Science & Business
Media, 2012.
[115] J. M. Lee. Riemannian manifolds: an introduction to curvature, volume 176. Springer
Science & Business Media, 2006.
[116] J. w. Lee and C. Giraud-Carrier. Results on mining NHANES data: A case study in
evidence-based medicine. Computers in biology and medicine, 43(5):493–503, 2013.
143

[117] B. Leimkuhler and C. Matthews. Eﬃcient molecular dynamics using geodesic integra-
tion and solvent–solute splitting. In Proc. R. Soc. A, volume 472, page 20160138. The
Royal Society, 2016.
[118] B. Leimkuhler and S. Reich. Simulating Hamiltonian dynamics, volume 14. Cambridge
university press, 2004.
[119] J. Lemm, J. Uhlig, and A. Weiguny. Bayesian approach to inverse quantum statistics.
Physical review letters, 84(10):2068, 2000.
[120] A. Levy, J. Bosch, J. Lewis, T. Green, N. Rogers, D. Roth, and the Modiﬁcation of
Diet in Renal Disease Study Group. A more accurate method to estimate glomeru-
lar ﬁltration rate from serum creatinine: A new prediction equation. Ann Int Med,
130:461–470, 1999.
[121] N. Longford. A fast scoring algorithm for maximum likelihood estimation in unbalanced
mixed models with nested random eﬀects. ETS Research Report Series, 1987(1), 1987.
[122] T. Lumley and A. Scott. AIC and BIC for modeling with complex survey data. Journal
of Survey Statistics and Methodology, 3(1):1–18, 2015.
[123] J. R. Magnus and H. Neudecker.
Matrix diﬀerential calculus with applications in
statistics and econometrics. Wiley series in probability and mathematical statistics,
1988.
[124] J. R. Magnus, H. Neudecker, et al. Matrix diﬀerential calculus with applications in
statistics and econometrics. 1995.
[125] C. L. Mallows. Some comments on Cp. Technometrics, 15(4):661–675, 1973.
[126] P. McCullagh and J. A. Nelder. Generalized Linear Models. Number 7 in Monograph
on Statistics and Applied Probability. Chapman & Hall,, 1989.
[127] M. R. Mehta, M. C. Quirk, and M. A. Wilson. Experience-dependent asymmetric
shape of hippocampal receptive ﬁelds. Neuron, 25(3):707–715, 2000.
[128] T. P. Minka. Inferring a Gaussian distribution. 1998.
[129] M. Moakher and M. Z´era¨ı. The Riemannian geometry of the space of positive-deﬁnite
matrices and its application to the regularization of positive-deﬁnite matrix-valued
data. Journal of Mathematical Imaging and Vision, 40(2):171–187, 2011.
[130] P. M¨uller and B. Vidakovic. Bayesian inference with wavelets: Density estimation.
Journal of Computational and Graphical Statistics, 7(4):456–468, 1998.
[131] K. P. Murphy. Machine learning: a probabilistic perspective. MIT press, 2012.
[132] I. Murray, D. MacKay, and R. P. Adams. The Gaussian process density sampler. In
Advances in Neural Information Processing Systems, pages 9–16, 2009.
144

[133] H. Narayanan and S. Mitter. Sample complexity of testing the manifold hypothesis.
In Advances in Neural Information Processing Systems, pages 1786–1794, 2010.
[134] J. Nash. The imbedding problem for Riemannian manifolds. Annals of mathematics,
pages 20–63, 1956.
[135] National Center For Health Statistics. Third National Health and Nutrition Exami-
nation Survey (Documentation (catalogue number 76200)), 1996.
[136] R. M. Neal. Markov chain sampling methods for dirichlet process mixture models.
Journal of computational and graphical statistics, 9(2):249–265, 2000.
[137] R. M. Neal et al. MCMC using Hamiltonian dynamics. Handbook of Markov Chain
Monte Carlo, 2:113–162, 2011.
[138] O. Ogunyemi and D. Kermah. Machine learning approaches for detecting diabetic
retinopathy from clinical and public health records.
In AMIA Annual Symposium
Proceedings, volume 2015, page 983. American Medical Informatics Association, 2015.
[139] M. Parida. Improving a diabetes type 2 risk calculator: A machine learning approach.
2010.
[140] K. Pearson.
Liii. on lines and planes of closest ﬁt to systems of points in space.
The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science,
2(11):559–572, 1901.
[141] X. Pennec, P. Fillard, and N. Ayache. A Riemannian framework for tensor computing.
International Journal of Computer Vision, 66(1):41–66, 2006.
[142] A. M. Peter, A. Rangarajan, and M. Moyou.
The geometry of orthogonal-series,
square-root density estimators: Applications in computer vision and model selection.
In Computational Information Geometry, pages 175–215. Springer, 2017.
[143] A. Pinheiro and B. Vidakovic. Estimating the square root of a density via compactly
supported wavelets. Computational Statistics & Data Analysis, 25(4):399–415, 1997.
[144] M. Pourahmadi.
Covariance estimation: The glm and regularization perspectives.
Statistical Science, pages 369–387, 2011.
[145] C.-E. Rabier and A. Genz. The supremum of chi-square processes. Methodology and
Computing in Applied Probability, 16(3):715–729, 2014.
[146] J. O. Ramsay. Functional data analysis. Wiley Online Library, 2006.
[147] C. R. Rao. Information and accuracy attainable in the estimation of statistical param-
eters. Bull. Calcutta Math. Soc, 37(3):81–91, 1945.
[148] C. E. Rasmussen. Gaussian processes for machine learning. 2006.
145

[149] E. Schr¨odinger. An undulatory theory of the mechanics of atoms and molecules. Phys-
ical Review, 28(6):1049, 1926.
[150] A. Schwartzman. Lognormal distributions and geometric averages of symmetric posi-
tive deﬁnite matrices. International Statistical Review, 2015.
[151] G. A. Seber and A. J. Lee. Linear regression analysis, volume 329. John Wiley &
Sons, 2012.
[152] H. Seng. Predicting age using biomarkers and physiological measurements. Journal
Biomedical Information, pages 1–5, 2009.
[153] B. Shahbaba, S. Lan, W. O. Johnson, and R. M. Neal. Split Hamiltonian Monte Carlo.
Statistics and Computing, 24(3):339–349, 2014.
[154] P. Shaman. The inverted complex wishart distribution and its application to spectral
estimation. Journal of Multivariate Analysis, 10(1):51–59, 1980.
[155] W. Shen and S. Ghosal. Adaptive Bayesian procedures using random series priors.
Scandinavian Journal of Statistics, 42(4):1194–1213, 2015.
[156] W. E. Skaggs and B. L. McNaughton.
Replay of neuronal ﬁring sequences in rat
hippocampus during sleep following spatial experience. Science, 271(5257):1870, 1996.
[157] J. C. Slater. Wave functions in a periodic potential. Physical Review, 51(10):846, 1937.
[158] J. Sohl-Dickstein, M. Mudigonda, and M. R. DeWeese.
Hamiltonian Monte Carlo
without detailed balance. arXiv preprint arXiv:1409.5191, 2014.
[159] M. Spivak. Diﬀerential geometry, volume 1–5. Publish or Perish, Berkeley, 2, 1975.
[160] A. Srivastava, I. Jermyn, and S. Joshi. Riemannian analysis of probability density
functions with applications in vision. In Computer Vision and Pattern Recognition,
2007. CVPR’07. IEEE Conference on, pages 1–8. IEEE, 2007.
[161] A. Srivastava and E. P. Klassen. Functional and shape data analysis. Springer, 2016.
[162] C. M. Stein. Estimation of the mean of a multivariate normal distribution. The Annals
of Statistics, pages 1135–1151, 1981.
[163] L. Svensson and M. Lundberg. On posterior distributions for signals in Gaussian noise
with unknown covariance matrix. IEEE Transactions on Signal Processing, 53(9):3554–
3571, 2005.
[164] L. Svensson and M. L. Nordenvaad. The reference prior for complex covariance matrices
with eﬃcient implementation strategies.
IEEE Transactions on Signal Processing,
58(1):53–66, 2010.
[165] K. Takeuchi. Distribution of information statistics and criteria for adequacy of models.
Math. Sci, 153:12–18, 1976.
146

[166] R. Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal
Statistical Society. Series B (Methodological), pages 267–288, 1996.
[167] M. E. Tipping and C. M. Bishop. Probabilistic principal component analysis. Journal
of the Royal Statistical Society: Series B (Statistical Methodology), 61(3):611–622,
1999.
[168] T. Tokuda, B. Goodrich, I. Van Mechelen, A. Gelman, and F. Tuerlinckx. Visualizing
distributions of covariance matrices. Dept. Statist., Columbia Univ., New York, NY,
USA, Tech. Rep, 2011.
[169] United States Bureau of the Census. Current Population Survey: Design and Method-
ology, volume 63. US Department of Commerce, Bureau of the Census, 2000.
[170] M. Vannucci, N. Sha, and P. J. Brown. NIR and mass spectra classiﬁcation: Bayesian
methods for wavelet-based feature selection. Chemometrics and Intelligent Laboratory
Systems, 77(1):139–148, 2005.
[171] A. Vehtari, A. Gelman, and J. Gabry.
Practical Bayesian model evaluation using
leave-one-out cross-validation and waic. arXiv preprint arXiv:1507.04544, 2016.
[172] J. Wang, Z. Zhang, and H. Zha. Adaptive manifold learning. In Advances in neural
information processing systems, pages 1473–1480, 2005.
[173] L. Wang. Karhunen-Loeve expansions and their applications. PhD thesis, London
School of Economics and Political Science (United Kingdom), 2008.
[174] W. Wang, D. Rothschild, S. Goel, and A. Gelman. Forecasting elections with non-
representative polls. International Journal of Forecasting, 31(3):980–991, 2015.
[175] X. Wang, S. Ray, and B. K. Mallick. Bayesian curve classiﬁcation using wavelets.
Journal of the American Statistical Association, 102(479):962–973, 2007.
[176] H. L. Wang Y. and O. H. Statistical Analysis of Electroencephalograms, pages 523–565.
CRC Press, 2016.
[177] L. Wasserman. All of statistics: a concise course in statistical inference. Springer
Science & Business Media, 2013.
[178] P. Whittle.
The analysis of multiple stationary time series.
Journal of the Royal
Statistical Society. Series B (Methodological), pages 125–139, 1953.
[179] C. K. Williams and C. E. Rasmussen. Gaussian processes for regression. Advances in
neural information processing systems, pages 514–520, 1996.
[180] H. Wold. Partial least squares. Encyclopedia of statistical sciences, 1985.
[181] A. Y. Yang, J. Wright, Y. Ma, and S. S. Sastry. Unsupervised segmentation of nat-
ural images via lossy data compression. Computer Vision and Image Understanding,
110(2):212–225, 2008.
147

[182] R. Yang and J. O. Berger. Estimation of a covariance matrix using the reference prior.
The Annals of Statistics, pages 1195–1211, 1994.
[183] S. Yu, K. Yu, V. Tresp, H.-P. Kriegel, and M. Wu. Supervised probabilistic principal
component analysis. In Proceedings of the 12th ACM SIGKDD international conference
on Knowledge discovery and data mining, pages 464–473. ACM, 2006.
[184] W. Yu, T. Liu, R. Valdez, M. Gwinn, and M. J. Khoury.
Application of support
vector machine modeling for prediction of common diseases: the case of diabetes and
pre-diabetes. BMC medical informatics and decision making, 10(1):16, 2010.
[185] Y. Zhang, J. Huang, and P. Wang. A prediction model for the peripheral arterial
disease using NHANES data. Medicine, 95(16), 2016.
[186] Z. Zhang, C. Gillespie, B. Bowman, and Q. Yang. Prediction of atherosclerotic car-
diovascular disease mortality in a nationally representative cohort using a set of risk
factors from pooled cohort risk equations. PloS One, 12(4):e0175822, 2017.
148

Appendix A
Estimating prediction error for
complex samples
149

Chapter Summary
With a growing interest in using non-representative samples to train prediction models
for numerous outcomes it is necessary to account for the sampling design that gave rise
to the data in order to assess the generalized predictive utility of a proposed prediction
rule. After learning a prediction rule based on a non-uniform sample, it is of interest
to estimate the rule’s error rate when applied to unobserved members of the popula-
tion. [53] proposed a general class of covariance-inﬂated prediction error estimators
that assume the available training data is representative of the target population for
which the prediction rule is to be applied. We extend Efron’s estimator to the complex
sample context by incorporating Horvitz-Thompson sampling weights and show that
it is consistent for the true generalization error rate when applied to the underlying
superpopulation. The resulting Horvitz-Thompson-Efron (HTE) estimator is equiva-
lent to dAIC, a recent extension of AIC to survey sampling data, but is more widely
applicable. The proposed methodology is assessed with simulations and is applied to
models predicting renal function obtained from the large-scale NHANES survey.
150

A.1
Introduction
The goal of building prediction models using empirical samples has become ubiquitous
throughout all areas of business and science1. With the exponential rise in statistical and
machine learning methods for training ﬂexible prediction models, increasing interest has
been devoted to assessing the out-of-sample performance of candidate models when they are
applied to the population of interest. From a decision theoretic perspective, analytic assess-
ments of the performance of a prediction rule commonly focus on the expected loss associated
with the rule, where the expectation is taken with respect to the underlying distribution of a
new, independently sampled response conditional upon the observed support of the sampled
predictors giving rise to the rule (cf. [82]). It is widely recognized that computation of the
loss function solely based on the training sample is optimistically biased for this expectation.
Perhaps the most commonly used analytic estimate of out-of-sample prediction error is
Akaike’s Information Criteria (AIC) [2], which can be viewed from a likelihood-based perspec-
tive, where the predictive loss function is taken to be the negative log-likelihood (deviance)
function giving rise to the response. In a linear model, AIC is equivalent to Mallows’s Cp
[53], which is naturally derived as a covariance-inﬂated estimator of expected squared-error
loss, where the optimism of the mean squared error based upon the training sample is esti-
mated by the covariance between each observed response, y, and it’s ﬁtted value based upon
prediction rule m(·) and predictors x, ˆµ = m(x). A more general treatment of covariance-
inﬂated estimators for arbitrary prediction rules and loss functions has been considered by
Efron [53, 54]. In this case, it was proposed that Cov(y, ˆµ) be estimated using a parametric
bootstrap providing an implementable assessment of prediction error.
Non-uniform random sampling designs are commonly employed throughout multiple empir-
ical sciences as they aﬀord researchers greater eﬃciency in estimating parameters speciﬁc
1This chapter was written by A Holbrook, Thomas Lumley, and Daniel Gillen. AH was supported by
NIH grant T32 AG000096. DLG was supported by NIA AG016573 and NIA R01AG053555.
151

to less prevalent sub-populations. Classic examples of complex sampling designs include
those implemented by the United States Census Bureau [169] and the National Health and
Nutrition Examination Study (NHANES) [135]. In each case, speciﬁc sub-populations are
over-sampled by design and sampling weights are used to correct population prevalence es-
timates and draw inference for estimands at the population level. Adopting the terminology
of [83], the resulting data is extrinsically and relatively imbalanced: extrinsic because the
sampling mechanism is inducing the imbalance; relative because class proportions in the
data diﬀer from population proportions.
There is a vast literature on inferential techniques for association estimation based upon
data derived from complex sampling designs, where the bulk of work focuses on inversely
weighted probability of sampling estimators for population estimands and consistent esti-
mation of their corresponding standard errors (cf. [97, 63]). This framework provides a
natural extension for the assessment of prediction error. Lumley and Scott [122] recently
proposed an extension of AIC to non-uniformly sampled data. They termed the proposed
extension dAIC, or design-based AIC, as it extends AIC by incorporating Horvitz–Thompson
weighting based upon known sampling probabilities. While dAIC provides a useful tool for
prediction model assessment in the context of a biased sampling design, its utility is limited
to likelihood or quasi-likelihood based prediction rules and assumes that the log-likelihood
is the most scientiﬁcally relevant loss function.
Despite numerous methodologic advances focused on the assessment of prediction error rules
and the growing interest in developing and applying prediction models, there is a paucity
of prediction error estimators for data arising from complex sampling designs. This dearth
is not for a lack of need. Recently, [174] proposed statistical modeling methodologies for
forecasting based on non-representative samples but did not propose any sort of model
comparison criterion that would take sample complexity into account. In the applied realm,
prediction models trained on health survey data are common and of interest within the
152

medical and health-care communities: [184] trains support vector machine models using the
NHANES survey to predict diabetes (and boasts 140 citations); [185] uses NHANES to train
a predictive model for peripheral arterial disease; and [186] does the same for predicting
atherosclerotic cardiovascular disease mortality. There are many more such examples. A
cursory search returns a large number of similar papers in which models are developed to
predict health outcomes based on the NHANES survey [108, 139, 152, 73, 12, 34, 86, 116,
44, 138, 43], and this is an inexhaustive sample from a single survey.
To address this deﬁciency, in the current manuscript we extend Efron’s covariance-inﬂated
prediction error estimator to incorporate non-simple random sample data. The resulting
estimator provides a uniﬁed framework for prediction model assessment that can be used
for arbitrary loss functions and can be applied to regression based predictive models as well
as algorithmically deduced prediction rules such as random forests and k-nearest neighbor
approaches. We establish consistency for the true error rate relative to the super-population
and further show equivalence to dAIC as a special case in the context of generalized linear
regression models (GLM) [126]. We point out that there has been one other study of gen-
eralization error for survey samples [33]. This study established error bounds for prediction
rules derived from complex data, but did not propose a prediction error estimator and diﬀers
from the methodology proposed here.
The remainder of the chapter is organized as follows: in Section A.2, we revisit the prediction
error estimation problem for the case of a simple random sample, Efron’s covariance-inﬂated
estimator, its relationship to AIC, and estimation of the covariance penalty term. In Section
A.3, we consider the impact of a complex sampling design on the estimation of out-of-
sample prediction error. In the same section, we describe dAIC in more detail and provide
an analytic example based on linear regression. We then introduce the proposed extension
of the covariance-inﬂated estimator to complex samples, prove its consistency, and establish
the equivalence between our estimator and dAIC in the setting of generalized linear models.
153

In Section 4, we explore the performance of the proposed method via empirical simulation
studies based on four broad scenarios. In Section 5, we illustrate the method using data from
NHANES to predict renal function, as estimated by glomerular ﬁltration rate, in the general
US population. Section 6 concludes with a discussion of the proposed approach, suggestions
for future research and further applications.
A.2
Prediction error estimation for simple random sam-
ples
We begin with the classic problem of estimating the prediction error rate where data, (y, X),
is obtained via a simple random sample with y denoting the vector of outcomes of interest
obtained on multiple sampling units and X denoting a matrix of explanatory variables on
sampling units.
Assume that an unknown data generating mechanism deﬁned by g has
produced y, from which we estimate the expectation µ = Eg(y) with ˆµ = m(y).
The
within-sample error is given by
err = 1
n
n
X
i=1
Q
 yi, ˆµi

,
(A.1)
where n is the length of y, the number of observations and Q(·, ·) denotes a speciﬁed loss
function. Given the within-sample error, a common goal is to estimate
Err = 1
n
n
X
i=1
E0 Q
 y0
i , ˆµi

(A.2)
for ﬁxed ˆµi. Here E0 denotes the expectation over an unobserved random variable y0
i drawn
independently from mechanism g but conditioning on observed support xi. Note that al-
154

though y0
i shares the same covariates xi as observation yi, the true data generating mechanism
g may or may not be a function of observed covariates.
In the following sections, we review an analytic, covariance based prediction error estimator
and show AIC to be equivalent to this estimator when deviance loss is speciﬁed for a GLM. In
this context, the covariance-inﬂation based penalty is easily approximated, but this term is
more diﬃcult to obtain when another loss function is speciﬁed or when algorithmic prediction
rules are used. We therefore ﬁnish this section by reviewing Efron’s parametric bootstrap
based method for estimating the covariance-inﬂation penalty before introducing our complex
sample prediction error estimation framework.
A.2.1
The covariance-inﬂated estimator
We assume that the loss function Q(·, ·) belongs to the q-class of loss functions [53] . A
member of the q-class of loss functions is constructed from some concave function q(·).
Given this function, the error for outcome yi and prediction ˆµi is given by
Q(yi, ˆµi) = q(ˆµi) + ˙q(ˆµi) (yi −ˆµi) −q(yi) .
(A.3)
This is not a limiting assumption. For example, the deviance functions of exponential family
distributions belong to this class. Suppose yi follows an exponential family distribution
gµi(yi) = exp
 λi yi −ψ(λi)

.
(A.4)
Here, λi is the natural parameter and ψ enforces density function integration constraints.
Then the choice
q(yi) = 2
 ψ(λ(yi) −yi λ(yi)

(A.5)
155

renders the deviance [126] as in-sample error:
err = 1
n
n
X
i=1
Q
 yi, ˆµi

= 2
n
 log gy(y) −log gˆµ(y)

.
(A.6)
Note that since only the second term in the deviance depends on estimate ˆµ, one may also
consider concave function (A.5) as inducing the negative log-likelihood loss. Now, since the
in-sample error is (most often) an underestimate of Err, we deﬁne the
Optimism
Oi = Oi(g, y) = Erri −erri ,
(A.7)
and the
Expected optimism
Ωi(g) = Eg Oi(g, y) .
(A.8)
If one is able to to obtain a consistent estimate of the optimism pertaining to a prediction rule,
then the true generalization error may be estimated by adding the estimated optimism to the
in-sample error err. Within the q-class of loss functions, the optimism can be analytically
estimated using the term
ˆλi = −˙q(ˆµi)/2 .
(A.9)
When Q(y, ˆµ) is the deviance for an exponential family distribution, ˆλi is the estimated
natural parameter for the ith observation [53, 54]. Indeed, the following result identiﬁes the
expected optimism with the covariance between y and ˆλ.
Theorem A.1. Efron’s optimism theorem [53, 54] For error measure Q(y, ˆµ), we have
Eg(Erri) = Eg(erri + Ωi) ,
(A.10)
156

where
Ωi = 2 covg(yi, ˆλi) .
(A.11)
It is a corollary [53] that when ˆµ is the MLE of µ for a correctly speciﬁed GLM, and when
prediction error is given by the deviance, then
1
n
n
X
i=1
Ωi = Eg
 Err −err) ≈2 p/n .
(A.12)
This approximation is obtained through the Taylor expansion of the link function, and is
exact for the Gaussian case.
Based on the theorem, it follows that if one knows Ωi or is able to estimate it, then one can
estimate the true prediction error rate Err. Section A.2.2 reviews a large class of analytic
prediction error rate estimators that implicitly rely on Formula (A.11). In Section A.2.3, we
discuss other limited situations in which the optimism is explicitly known and how it may
be estimated otherwise.
A.2.2
Akaike’s information criterion
AIC is a model selection criterion based on the minimization of the Kullback–Liebler (KL)
divergence between a proposed model f(y|θ) and the density function g(y) of true data
generating mechanism g. Prediction error is not explicitly invoked in AIC’s derivation, but
for simple random samples it has been shown [53] that the AIC model selection rule is
equivalent to choosing the model with lowest estimated generalization error Err when the
deviance loss is speciﬁed (see below).
157

In order to facilitate connections with our proposed estimator we brieﬂy review the devel-
opment of AIC, following [32]. The KL divergence between f(y|θ) and g(y) is given by
KL(fθ||g) = Eg

log g(y)
f(y|θ)

= Eg
 log g(y)

−Eg
 log f(y|θ)

.
(A.13)
Letting ℓn(θ) be the log-likelihood given the n-vector y, the strong law of large numbers
assures that
ℓn(θ)/n
a.s.
−→Eg
 log f(y|θ)

.
(A.14)
Assuming regularity conditions, it follows that the maximum likelihood estimator ˆθ con-
verges to θ0, the minimizer of the KL divergence. Thus, the maximum likelihood estimator
minimizes both the KL divergence and the deviance loss. Since ˆθ is a random variable,
Rn = Eg
 log f(y|ˆθ)

(A.15)
is also a random variable, where the expectation is taken over the data, leaving ˆθ ﬁxed. Now,
let Qn = EgRn, where the expectation is with respect to ˆθ under g. If the proposed model is
correctly speciﬁed, i.e. if g has density f(y|θ) for some θ, then
Eg
 ℓn(ˆθ)/n −Qn

≈p/n ,
(A.16)
where p is the length of vector θ. From this approximation arises the well-known formula
AIC(ˆθ) = −2ℓn(ˆθ) + 2p .
(A.17)
Given a set of candidate models, one chooses the model with smallest AIC. Hence, AIC
favors the model with largest penalized log-likelihood where the penalization is proportional
to the number of model parameters, i.e. the dimension of θ. Note the similarity between
158

Equations (A.12) and (A.17). Following Equation (A.12), AIC is equivalent to selecting the
model with the lowest covariance-inﬂated estimate of Err when deviance loss is speciﬁed for
a GLM:
d
Err = 2
n
 log gy(y) −log gˆµ(y)

+ 2
n
n
X
i=1
covg(yi, ˆλi)
(A.18)
≈2
n

log gy(y) −
 log gˆµ(y) −p

= 2
n

log gy(y) −
 log f(y|ˆθ) −p

= 2
n log gy(y) + 1
nAIC(ˆθ) .
The change of notation in the penultimate line stems from the fact that Formula (A.17)
depends on the assumption that the model is correctly speciﬁed, i.e. g(y) = f(y|θ) for some
θ, and hence that gˆµ(y) = f(y|ˆθ), where ˆθ emphasizes the MLE of the canonical parameter
and ˆµ emphasizes the ﬁtted values obtained using the MLE. Again, when the model is
correctly-speciﬁed Gaussian, the correspondence is exact. See [53] for details.
A.2.3
Estimating the covariance penalty
For the majority of models, the inﬂation term covg(yi, ˆλi) is unknown. [54] provides two cases
where this term is actually known. The ﬁrst is the homoskedastic model under squared-error
loss:
y ∼(µ, σ2I) ;
Q(yi, ˆµi) = (yi −ˆµi)2 .
(A.19)
Mallows [125] showed that for linear estimator ˆµ = My
d
Erri = erri + 2σ2Mii
(A.20)
159

is an unbiased estimator of Erri. Here, σ2Mii is none other than the covariance between yi
and ˆyi. Hence, Mallows’s Cp is a special case and predecessor of Efron’s covariance-inﬂation
method.
The second case where the covariance term is known was put forward by Stein [162]. Still
using squared-error loss, let ˆµ = m(y) be a diﬀerentiable, not necessarily linear, prediction
rule. Then, further assuming Gaussian errors
y ∼N(µ, σ2I) ,
(A.21)
the covariance is given by
covg(yi, ˆµi) = σ2Eg
∂ˆµi
∂yi
.
(A.22)
Since this derivative is usually obtainable, a simple, unbiased estimator of the generalization
error is also obtainable.
Besides these two basic cases, estimators for the covariance-inﬂation term must be found
another way. As mentioned in the previous sections, the covariance penalty may be analyt-
ically estimated for GLMs under the deviance or KL divergence losses. Again, despite their
having diﬀerent derivations, we use these loss functions interchangeably since they amount
to the negative log-likelihood in practice.
For more complex models, [54] suggests the parametric bootstrap and ﬁnds that it agrees with
the above methods for their respective models but does not require that the prediction rule
be diﬀerentiable or indeed continuous. Brieﬂy, if ˆµ is the vector of within-sample predictions,
then y∗
i is sampled from the parametric distribution of choice centered at ˆµi. For binary yi,
160

sample
y∗b
i ∼Bern(ˆµi) ,
b = 1, . . . , B .
(A.23)
Then, ˆλ∗b
i
= −˙q(ˆµ∗b
i )/2 is a function of the bootstrap prediction ˆµ∗b
i , and the estimated
covariance-inﬂation for the ith individual is
c
covi =
B
X
b=1
ˆλ∗b
i (y∗b
i −y∗·
i ) ,
(A.24)
where y∗.
i is the bootstrap mean for individual i. Assuming the distribution of model er-
rors is correctly speciﬁed, this process provides a consistent estimator for covi and nowhere
requires that the prediction rule itself be probabilistic or generated from a GLM. The para-
metric bootstrap therefore facilitates the extension of the covariance-inﬂation prediction error
methodology to algorithmic prediction rules.
In the following, we consider the extension of the covariance-inﬂated prediction error esti-
mator to non-uniform sampling regimes.
A.3
Prediction error estimation for complex samples
While the covariance-inﬂated prediction error estimation procedure is useful for prediction
rules derived from simple random samples, it is no longer accurate in a complex sample
context. Hence, there is a need for a modiﬁed prediction error estimator that is applicable
to models based on, say, large-scale health surveys or political polling. We now consider
how to incorporate knowledge about the complex sampling design giving rise to data for
accurate estimation of Err. Due to the connection between the covariance-inﬂated estimator
and AIC, we begin by reviewing dAIC, an extension of AIC to complex sampling designs
161

recently proposed by [122]. After this, we extend Efron’s optimism theorem to complex
samples thereby allowing for analytic estimation of Err under a generic loss function.
In the following, we make use of the superpopulation framework [97] for ﬁnite population
analysis. That is, we assume that the ﬁnite population y1 . . . , yN is generated independently
(not necessarily identically) by the same mechanism g, and that the data is then obtained
via a (not necessarily uniform) sampling distribution, denoted π, where πi = Pr(yi ∈s),
for sample s. Note that in prior sections we used g to denote the distribution producing
the data, but we now use the same symbol to denote the distribution producing the ﬁnite
population.
A.3.1
Design-based AIC
Lumley and Scott [122] proposed an extension to AIC under a non-uniform sampling regime
by adopting the above superpopulation framework. As in Section A.2.2, we do not know
the true distribution g, and we seek to minimize the KL divergence between a plausible
conditional distribution fθ(y|x) and g(y) for observed covariate vector x. As in the uniform
sampling case, this corresponds to maximizing the log-likelihood ℓ(θ) of the assumed model.
For complex samples, this may be estimated using Horvitz–Thompson [97] weights:
ˆℓ(θ) = 1
N
n
X
i=1
wi ℓi(θ) ,
(A.25)
where n is the size of s, N is the size of the ﬁnite population, wi ∝1/πi, and Pn
i=1 wi = N.
Now, suppose that θ∗and ˆθ are obtained by solving the score equation and pseudo-score
equation, respectively:
U(θ) = ∂ℓ(θ)
∂θ
= 0 ,
and
ˆU(θ) = ∂ˆℓ(θ)
∂θ
= 0 .
(A.26)
162

In the context of AIC, we are interested in estimating Eg(ℓ(ˆθ)), the expected value of the
log-likelihood log fθ(y|x) evaluated at ˆθ with respect to the true superpopulation distribution
g. Then it is shown in [122] that
Eg
 ˆℓ(ˆθ)

= Eg
 ℓ(ˆθ)

+ 1
ntr(∆) + op(n−1)
(A.27)
where ∆= I(θ∗)V (θ∗), V (θ∗) is the asymptotic covariance of √nˆθ, and
I(θ) = Eπ,g
  ˆ
J (θ)

= −Eg
∂2ℓ(θ)
∂θ∂θT

,
(A.28)
for
ˆ
J (θ) = −1
N
n
X
i=1
wi
∂2ℓi(θ)
∂θ∂θT .
(A.29)
Note that I is just the Fisher information corresponding to distribution g and that ˆ
J (ˆθ)
reduces to the observed Fisher information when the sample is collected uniformly. Equation
(A.27) results in a design-based formulation of AIC for complex data, dAIC:
dAIC = −2ˆℓ(ˆθ) + 2 tr
 ˆ
J (ˆθ)ˆV (ˆθ)
	
,
(A.30)
where ˆV (ˆθ) is the sandwich estimator for V (θ∗):
ˆV (ˆθ) = ˆ
J (ˆθ)−1 ˆVU(ˆθ) ˆ
J (ˆθ)−1
(A.31)
for ˆVU(ˆθ) a consistent estimator of cov
 √n ˆU(θ)

. Thus dAIC may be rewritten as
dAIC = −2ˆℓ(ˆθ) + 2 tr
 ˆ
J (ˆθ)−1 ˆVU(ˆθ)
	
.
(A.32)
163

When the weights are uniform, dAIC reduces to a robust version of AIC called Takeuchi’s
information criterion [165]. If, in addition, the model is correctly speciﬁed, dAIC reduces to
AIC [122].
A.3.1.1
Example: linear regression model based on weighted independent sam-
ple
Let fθ(y|x) be the homoskedastic linear regression model with Gaussian errors and regression
coeﬃcients θ. Then
ˆθ = (XTΠ−1X)−1XTΠ−1Y ,
(A.33)
and
ˆ
J (ˆθ) = (XTΠ−1X)/(N ˆσ2) .
(A.34)
where X is the n × p observed design matrix, and Π is an n × n diagonal matrix of sampling
weights. Next, the estimated covariance matrix of ˆθ is given by
ˆV (ˆθ) = (N ˆσ2)(XTΠ−1X)−1XTΠ−1Diag
 (Y −X ˆθ)(Y −X ˆθ)T
Π−1X
(N ˆσ2)2
(XTΠ−1X)−1(N ˆσ2)
(A.35)
= (XTΠ−1X)−1XTΠ−1 ˆΣO Π−1X(XTΠ−1X)−1 .
164

for ˆΣO = Diag
 (Y −X ˆθ)(Y −X ˆθ)T
. The design-eﬀect corrected penalty term is then given
by
tr
 ˆ
J (ˆθ)ˆV (ˆθ)
	
= tr

(XTΠ−1X) × (XTΠ−1X)−1XTΠ−1 ˆΣO Π−1X(XTΠ−1X)−1	
/(N ˆσ2)
(A.36)
= tr

XTΠ−1 ˆΣO Π−1X(XTΠ−1X)−1	
/(N ˆσ2) .
It follows that the dAIC for the classical linear regression case is given by
1
N
n
X
i=1
(yi −ˆµi)2
πi ˆσ2
+
2
N ˆσ2 tr

XTΠ−1 ˆΣO Π−1X(XTΠ−1X)−1	
.
(A.37)
A.3.1.2
The meat of the sandwich
The sandwich estimator for V (θ∗) is provided in Equation (A.31). The meat of this sandwich
is the estimated asymptotic covariance ˆVU(ˆθ) of the score function ˆU(ˆθ). In its most general
form, we have
ˆVU(ˆθ) = ˆU(ˆθ) ˆU(ˆθ)T .
(A.38)
Among other things, the previous example shows that for linear regression in the non-
uniform, unstratiﬁed sampling case, ˆVU(ˆθ) takes the form
XTΠ−1 ˆΣO Π−1X
(N ˆσ2)2
,
(A.39)
where ˆΣO is the diagonal matrix of pointwise residuals. In general, for exponential family
GLMs for non-uniform, unstratiﬁed samples
ˆVU(ˆθ) = 1
N 2XTΠ−1 ˆΣO Π−1X ,
(A.40)
165

for ˆΣO the diagonal matrix of observed residuals. The NHANES data considered in Section
A.4 is obtained from a stratiﬁed sample with intra-stratum/inter-PSU correlations as well
as intra-PSU correlations. In such a sample with strata h = 1, . . . , H, ˆVU(ˆθ) takes the form
ˆVU(ˆθ) = 1
N 2XTΠ−1bdiag
 ˆΣ1
O, . . . , ˆΣH
O

Π−1X ,
(A.41)
assuming observations are ordered according to stratum membership. Here bdiag indicates
a block-diagonal structure, and ˆΣh
O itself has a block structure corresponding to individual
PSUs:
ˆΣh
O =






ˆΣh
1
ˆΣh
12
. . .
...
...
ˆΣh
1nh
ˆΣh
nh






.
Here, blocks along the diagonal are given by
ˆΣh
j = (Y h
j −ˆµh
j )(Y h
j −ˆµh
j )T =
X
i,i′∈j
(yh
i −ˆµh
i )(yh
i′ −ˆµh
i′) ,
(A.42)
where i, i′ denote individuals and j denotes the PSU. Oﬀ-diagonal blocks take the form
ˆΣh
jj′ = (Y h
j −ˆ¯µh
j )(Y h
j′ −ˆ¯µh
j′)T =
X
i∈j, i′∈j′
(yh
i −ˆ¯µh
j )(yh
i′ −ˆ¯µh
j′)
(A.43)
where ˆ¯µh
j is the average predicted value for PSU j, and ˆ¯µh
j is this average multiplied by
an nj-vector of ones. The upshot is that in the following (the proof of Theorem 3.1, in
particular), we can ignore the particular sampling structure and simply write
ˆVU(ˆθ) = 1
N 2XTΠ−1 ˆΣOΠ−1X ,
(A.44)
letting ˆΣO take whichever form appropriate for whichever sampling structure.
166

A.3.2
The Horvitz–Thompson–Efron estimator
AIC and its complex sample extension dAIC are useful for comparing models and estimat-
ing Err when the models feature a likelihood and when the loss function is taken to be
the deviance or negative log-likelihood. For certain prediction methodologies, such as sup-
port vector machines, k-nearest neighbors, and adaptive boosting, it may not be reasonable
to assume probabilistic structure. Furthermore, likelihood free methods are now common
in Bayesian inference in the form of approximate Bayesian computation [11].
Moreover,
scientiﬁc context may necessitate the need to consider a loss function diﬀerent from the
log-likelihood. Finally, dAIC is built on the estimating equations framework, which requires
continuous parameter spaces and twice diﬀerentiable functions. The former requirement is
not always satisﬁed by algorithmic prediction models.
If we know covg(ˆλi, yi)—or if we can obtain a consistent estimate of it—then Theorem A.1
provides an analytic, consistent estimator of Erri in the case of uniform sampling. However,
if the individual yis are collected according to a non-uniform sampling scheme with known
sampling probabilities, it is still possible to obtain consistent estimates of generalization error
by incorporating Horvitz-Thompson (HT) sampling weights into the error estimate. In order
to address this issue, we must distinguish between diﬀerent kinds of generalization error for
the ﬁnite population framework. We now use Err to denote the ﬁnite population prediction
error rate:
Err = 1
N
N
X
i=1
Q(yi, ˆµi) .
(A.45)
The superpopulation prediction error rate is the expected value of the ﬁnite population error
rate Eg(Err). Next, deﬁne the Horvitz–Thompson–Efron (HTE) estimator of the predictive
167

error rate
d
Err = 1
N
n
X
i=1
1
πi
 erri + 2 covg(ˆλi, yi)

,
(A.46)
where Pn
i=1 1/πi = N. The following corollary follows easily from Theorem 2.1.
Corollary A.1. The HTE estimator
ˆ
Err is unbiased for the superpopulation generalization
error, i.e.
E( ˆ
Err) = Eg(Err) .
(A.47)
Proof. Let Eπ denote expectation with respect to sampling mechanism. Then,
E(d
Err) = Eπ,g
 1
N
n
X
i=1
1
πi
 erri + 2 covg(ˆλi, yi)

(A.48)
= Eπ,g
 1
N
N
X
i=1
1
πi
Ii∈s
 erri + 2 covg(ˆλi, yi)

= Eg
 1
N
N
X
i=1
1
πi
Eπ|g(Ii∈s)
 erri + 2 covg(ˆλi, yi)

= Eg
 1
N
N
X
i=1
 erri + 2 covg(ˆλi, yi)

= 1
N
N
X
i=1
Eg
 erri + 2 covg(ˆλi, yi)

= 1
N
N
X
i=1
Eg(Erri)
= Eg(Err) .
Thus, the HT extension (HTE) of the covariance-inﬂated estimator gives an unbiased esti-
mator for the superpopulation prediction error irrespective of sample design. As a simple
168

application of the law of large numbers, we know that Err
a.s.
→Eg(Err), and hence in the
limit as n, N →∞we have that
ˆ
Err is consistent for ﬁnite population generalization error
Err. For more details on ﬁnite population, superpopulation asymptotics, see [63] Section
1.3.
In the following theorem we establish the canonical result that, under non-uniform sampling,
dAIC is a special case of the HTE estimator for standard generalized linear models (GLMs).
Theorem A.2. The dAIC and HTE penalty terms correspond exactly, provided that: (1) a
generalized linear model with canonical link is speciﬁed; and (2) the weighted deviance loss is
used and the model is ﬁt by minimizing this loss function (which corresponds to maximizing
the weighted log-likelihood).
Proof. As in Section 2.1, let λ and µ denote the natural and mean parameters of the expo-
nential family model
gµ(y) = exp
 λ y −ψ(λ)

.
(A.49)
Given observations (yi, xi), i = 1, . . . , N, we adopt the GLM framework and assume that,
for each observation, the canonical parameter is given by a linear combination of covariates:
λi = xT
i θ. We show that the dAIC penalty is equal to the HTE penalty, i.e., that
tr
 ˆV (ˆθ) ˆ
J (ˆθ)

= 1
N
n
X
i=1
1
πi
c
cov(ˆλi, yi) ,
(A.50)
when the estimate c
cov is obtained using the analytic estimate (below) and not obtained from
the parametric bootstrap (although similar estimates are obtained in practice). We ﬁrst use
the following two facts about exponential family distributions to obtain the forms of ˆ
J (ˆθ)
169

and ˆVU(ˆθ):
∂ψ
∂λ = µ
and
∂µ
∂λ = ∂2ψ
∂λ2 = covµ(y) .
(A.51)
ˆ
J (ˆθ) is deﬁned as the HT weighted, observed Fisher information:
ˆ
J (ˆθ) = −1
N
n
X
i=1
1
πi
∂2ℓi(θ)
∂θ∂θT
(A.52)
= 1
N XT(Π−1∂2ψ
∂λ2
ˆλ) X
= 1
N XT(Π−1 ˆΣM) X .
Here ˆΣM = Σ(ˆµ) is a diagonal matrix with elements given by the model-based covariances
Σ(ˆµ)ii = covˆµi(yi). Next, we recall from Section A.3.1.2, that ˆVU(ˆθ) takes the form
1
N 2XTΠ−1 ˆΣO Π−1X ,
(A.53)
for ˆΣO a matrix of observed residuals with speciﬁc form depending on sample characteristics.
Next we need a formula for ˆθ.
Suppose that ˆθ is obtained by maximizing the weighted
log-likelihood. Then ˆθ takes the form of the WLS solution
ˆθ = (XT(ˆΣMΠ−1)X)−1XT(ˆΣMΠ−1)z ,
(A.54)
where z is the linearization of the canonical link applied to y called the ‘adjusted dependent
variable’ or the ‘working residual’:
z = ˆλ + (y −ˆµ)∂λ
∂µ

ˆµ .
(A.55)
170

Combining the formulas gives
tr
 ˆV (ˆθ) ˆ
J (ˆθ)

= tr
 ˆVU(ˆθ) ˆ
J (ˆθ)−1
(A.56)
= 1
N tr

XT(Π−1 ˆΣOΠ−1)X
 XT(ˆΣMΠ−1)X
−1
= 1
N tr

Π−1 X
 XT(ˆΣMΠ−1)X
−1XTΠ−1 ˆΣO

= 1
N tr

Π−1 X
 XT(ˆΣMΠ−1)X
−1XTΠ−1 ˆΣM ˆΣ−1
M ˆΣO

= 1
N tr

Π−1 X
 XT(ˆΣMΠ−1)X
−1XTΠ−1 ˆΣM
∂λ
∂µ|ˆµ ˆΣO

= 1
N tr

Π−1 X
 XT(ˆΣMΠ−1)X
−1XTΠ−1 ˆΣM c
cov(z, y)

= 1
N tr

Π−1 c
cov(X ˆθ, y)

= 1
N tr

Π−1 c
cov(ˆλ, y)

= 1
N
n
X
i=1
1
πi
c
cov(ˆλi, yi) ,
completing the proof.
A.3.2.1
Linear regression example revisited
We revisit the scenario of Section 3.1.1 where fθ(y|x) is the homoskedastic linear regression
model with Gaussian errors based on a weighted, non-stratiﬁed sample. Let
erri = Q(yi, ˆµi) = (yi −ˆµi)2
(A.57)
be the loss function, then the inﬂation term for the HTE estimator is given by
1
N
n
X
i=1
2
πi
c
covg(ˆµi, yi) = 2
N
n
X
i=1
1
πi
 X(XTΠ−1X)−1XTΠ−1 ˆΣO

ii
(A.58)
= 2
N tr

Π−1X(XTΠ−1X)−1XTΠ−1 ˆΣO
	
.
171

By the cyclic property of the trace, one has
ˆ
Err = 1
N
n
X
i=1
1
πi
(yi −ˆµi)2 + 2
N tr

XTΠ−1 ˆΣO Π−1X(XTΠ−1X)−1	
(A.59)
= dAIC · ˆσ2 .
Thus equivalence between the covariance-inﬂated prediction error estimator and dAIC clearly
holds in this context. This is expected as a special case of Theorem 3.1.
A.3.3
Estimating the covariance penalty
In Section A.2.3, it was stated that the covariance between the linear predictor ˆλi and the
outcome yi is rarely known outside of a few basic examples and approximations. As a result,
[53] suggested the use of the parametric bootstrap to produce empirical covariances between
simulated outcomes and their resulting ﬁtted values. The exact way in which the parametric
bootstrap is employed will vary from situation to situation, since model and data speciﬁcs
inﬂuence the way the bootstrap data should be simulated.
An example of the interplay between data and model speciﬁcs and their inﬂuence on the
parametric bootstrap is presented in Section A.5 below, where we use NHANES to train
two diﬀerent models to predict abnormal GFR. The ﬁrst model is a stratiﬁed quasi-binomial
GLM, where ‘quasi’ denotes a shared intra-PSU dispersion parameter φ satisfying:
var(yi) = pi(1 −pi)φ
and
φ = (1 + (nj −1)ρ) ,
(A.60)
for nj the size of PSU j and ρ the within-PSU correlation shared across all PSUs. If ˆρ
diﬀers from zero, then ˆφ should be multiplied by the naive parametric bootstrap estimated
172

covariance
B
X
b=1
ˆλ∗b
i (y∗b
i −y∗·
i ) ,
(A.61)
making the correct estimate
c
covi = ˆφ
B
X
b=1
ˆλ∗b
i (y∗b
i −y∗·
i ) .
(A.62)
That said, this point is moot in a number of ways. First, when using a GLM (as is the
case in this example), it is less computationally intensive to use dAIC, the fully analytic
special case of the HTE. Second, in a study such as NHANES with PSUs on the order of
600, within-PSU correlations tend to be small (in this chapter, |ˆρ| < 5×10−4. Third, survey
structure is often approximated for end-users. Fourth, if the purpose is model comparison,
multiplying by a scalar eﬀects each ﬁt equally. Nonetheless, Table 3 presents dAIC (using
sandwich estimator) and HTE (using parametric bootstrap) side-by-side for the to show that
results are similar as is to be expected.
A.4
Simulation study
In this section we illustrate the properties of the HTE estimator via monte carlo simulation
under four potential non-uniform sampling designs and consider performance under a linear
regression and logistic regression model ﬁt. The simulated experiments encompass four sim-
pliﬁed scenarios in which the relationship between model covariates, sampling probabilities,
and model noise are allowed to diﬀer. In most scenarios, the HTE estimator is shown to
provide a useful estimate for the generalization error. We also show that the HTE estimator
fails to be useful when sampling probabilities and models errors are strongly correlated.
173

Scenario
XN
yN
π
1
Xi
N
iid
∼N(0, 1)
yi
N
ind
∼N(Xi
N, 1)
πi ∝log(i)
2
Xi
N
iid
∼N(0, 1)
yi
N
ind
∼N(Xi
N, |Xi
N|)
πi ∝log(i)
Xi
N
iid
∼N(0, 1)
yi
N
ind
∼Bern Φ(Xi
N)
πi ∝log(i)
3
Xi
N
iid
∼N(0, 1)
yi
N
ind
∼N(Xi
N, log(i))
πi ∝log(i)
4a
Xi
N
iid
∼N(0, 1)
yi
N
ind
∼N(Xi
N, |Xi
N|)
πi ∝|Xi
N|
Xi
N
iid
∼N(0, 1)
yi
N
ind
∼Bern Φ(Xi
N)
πi ∝|Xi
N|
4b
Xi
N
iid
∼N(0, 1)
yi
N
ind
∼N(Xi
N, |Xi
N|)
πi ∝|Xi
N|−1
Xi
N
iid
∼N(0, 1)
yi
N
ind
∼Bern Φ(Xi
N)
πi ∝|Xi
N|−1
Table A.1: Data generation schemes.
For scenario 1, the distribution of response yi
N is
independent of the sampling probability πi. In scenario 2, the predictor inﬂuences the mean
and variance of yi
N, but is independent from the sampling probability. For scenario 3, the
distributions of yi
N and πi both depend on the index i. For scenario 4, the distributions of
yi
N and πi both depend on Xi
N. Since the variance of yi
N and the sampling probability both
grow with Xi
N, one might expect optimism to be negative. This turns out to be the case
(see Table 2).
Err-err
2ˆΩ
Response
Scenario
meanmedian
95% Interval
meanmedian
95% Interval
Gaussian
1
0.00380.0044
(−0.0869, 0.0913)
0.00400.0040
( 0.0032, 0.0049)
2
0.00160.0068
(−0.1917, 0.1680)
0.00400.0040
(−0.0185, 0.0258)
3
0.39230.5235
(−9.7924, 10.1136)
0.45120.4488
( 0.3604, 0.5546)
4a
0.02280.0678
(−0.6740, 0.4456)
0.00860.0079
(−0.0035, 0.0235)
4b
−0.0068−0.0077
(−0.1640, 0.1556)
0.05190.0361
( 0.0143, 0.1893)
Bernoulli
2
0.00070.0007
(−0.0269, 0.0278)
0.00070.0007
( 0.0003, 0.0012)
4a
0.01470.0153
(−0.0405, 0.0645)
0.01010.0067
( 0.0016, 0.0381)
4b
−0.0014−0.0008
(−0.0496, 0.0443)
0.00040.0003
(−0.0001, 0.0010)
Table A.2: Population optimism versus HTE estimates.
For both the ﬁnite population
based optimism ‘Err-err’ and the HTE estimated optimism 2ˆΩ, means, medians, and em-
pirical intervals based on 10,000 independent simulations are shown. Based on their mutual
consistency for the superpopulation optimism, one expects the empirical means of the ﬁnite
population optimisms to be close to the HTE estimates. Similar to other prediction error
estimation methods, the HTE conditions on the observed support of X, and is inaccurate
for Scenario 4 (see text).
174

Corollary A.1 shows that the HTE estimator is consistent for the superpopulation generaliza-
tion error. In this section, we provide an empirical illustration by simulating data under four
diﬀerent variable relationship structures. The general modeling contexts are simple linear
regression and logistic regression based on a non-uniform data sample. Let yN be the ﬁnite
population response, and XN be the ﬁnite population predictor of interest. Let π denote the
non-uniform sampling mechanism:
(yn, Xn) ∼π(yN, XN) ,
(A.63)
where n denotes the size of the collected sample.
In all four scenarios, the ﬁnite population is ﬁrst generated and then subsampled. The exact
distributions of the data are given in Table A.1. In scenario 1, the non-uniform sampling
mechanism is independent of Xi
N and yi
N. For scenario 2, both the mean and variance are
functions of the predictor, but data generation is independent of the sampling mechanism.
In scenario 3, the model errors and the sampling probabilities are a function of the same
random variable zi; in simulations zi was degenerate at the logarithm of index i. In scenario
4, the model errors and the sampling probabilities are both dependent on the absolute value
of Xi
N.
For each scenario, 10,000 simulations are run. Within each simulation, a ﬁnite population
of size N = 100, 000 is generated from which a sample of size n = 1, 000 is obtained. Then,
weighted least squares regression or weighted logistic regression (using inverse probabilities
as weights) are performed. In-sample error, out-of-sample error, and the HTE estimator are
recorded. Thus, the simulations provide a 10,000 large empirical sample of the diﬀerence
between out-of-sample and in-sample errors and the HTE estimator. Results are shown in
Table 2.
175

In the ﬁrst column of Table 2, the mean, median, and 0.025 and 0.975 quantiles of the
simulated true optimism are presented.
Note that in each scenario the optimism varies
greatly between each of the 10,000 simulation iterations. The variance of the HTE estimates
is much smaller in comparison, and, in most cases, their means and medians are extremely
close to the means and medians of the empirical optimisms. The HTE estimator is not
directly consistent for the population optimism but is consistent for the superpopulation
optimism.
We therefore expect that the means and medians of Table 2 should become
arbitrarily close as the number of simulation iterations gets large.
Here we address the performance of the HTE estimator for Scenario 4.
It is important
to note that—in covariance-inﬂated prediction error estimation, AIC, the bootstrap, and
cross-validation—the estimate is not just based on the observed data, but explicitly for the
error rate of future observations over the exact same support as the observed data. For
the covariance-inﬂated prediction error methodology, it is easy to forget that the accuracy
of the error estimate deteriorates as the distance (given by some metric on the data-space)
between observations and future observations grows. This is another way of saying that
error is a function of—among other things—the amount by which we use our error estimate
inappropriately, i.e. as an estimate of something it is not an explicit estimator of. This fact
explains why sampling probabilities that are strongly correlated with model errors would
cause ostensibly inaccurate results (as in Table 2). Strong correlations between the sampling
mechanism and model errors are problematic for the proposed methodology (and for dAIC)
insofar as we choose to use it to generalize to future observations with drastically diﬀerent
support from that of the data observed. Nonetheless, Scenario 4 is simulated with unusually
perverse dependencies between sampling probabilities and model errors and diﬀers from most
large survey contexts.
176

Model
Weighted deviance
p
dAIC ˆp
HTE ˆp
Age
9017.07
2
3.652
3.442 (2.550, 4.470)
+ BMI
8929.49
3
5.472
5.180 (3.998, 6.463)
+ Gender
8748.63
4
7.163
6.385 (5.339, 7.870)
+ Race/ethnicity
8695.96
5
7.121
7.158 (6.036, 8.782)
Table A.3: Comparing dAIC and HTE for prediction of GFR (< 60) with diﬀerent covariates
included.
Results come from a logistic regression model using deviance loss.
For each
method, ˆp is the estimated eﬀective number of parameters. The HTE term is calculated
using parametric bootstrap and presented as median from 100 simulations and 95% empirical
interval. Penalties increase with the number of covariates but are too small to inﬂuence
generalization error estimates because of the large sample size. Importantly, dAIC and HTE
give similar results and have penalties that are larger that the usual AIC penalty.
Predictive model
err
ˆΩ
ˆ
Err
10-NN
0.1081
83.7 × 10−4
0.1248
20-NN
0.1125
43.3 × 10−4
0.1212
30-NN
0.1138
30.9 × 10−4
0.1200
40-NN
0.1160
22.8 × 10−4
0.1206
Table A.4: Estimated error for k-nearest neighbors (kNN) classiﬁcation. Again, GFR (< 60)
is being predicted, but errors are based on 0-1 loss. As the number of voting neighbors
increases, in-sample error increases, and the covariance penalty decreases. Generalization
error appears to hit its lowest point at around 30-NN
A.5
Prediction of renal function using data from the
National Health and Nutrition Examination Sur-
vey
We consider data from the Third National Health and Nutrition Examination Survey (NHANES
III) with the illustrative goal of constructing a model for predicting abnormal renal function
as proxied by a estimated glomerular ﬁltration rate (GFR) of less than 60 millilitres per
minute per 1.73m2. Brieﬂy, NHANES III was one of several periodic surveys conducted by
the National Center for Health Statistics (NCHS). The survey was conducted during 1988 to
1994, and was designed to provide national estimates of health and nutritional status in the
civilian non-institutionalized United States population aged 2 months and older. Children
177

ages two months to ﬁve years, persons 60 years and older, Mexican-American persons, and
non-Hispanic black persons were sampled at rates substantially higher than their propor-
tions in the general population [135]. To estimate renal function, we use the Modiﬁcation of
Diet in Renal Disease (MDRD) equation for GFR based on serum levels and demographic
covariates [120]. Speciﬁcally, GFR was estimated as
GFRMDRD = 170×SCr−0.999×ageyrs−0.176×BUN−0.170×Salb0.318×1.180black×0.762female,
where SCr denotes serum creatinine, BUN denotes blood urea nitrogen, Salb denotes serum
albumin, and black and female denote indicators of non-Hispanic black race and sex, re-
spectively. [36] have previously reported that the assay used for measuring serum creatinine
in the NHANES study resulted in creatinine levels systematically higher than those used
to obtain the MDRD prediction model. As a consequence, they suggest creatinine values
from NHANES III be recalibrated to account for an average overestimate of 0.23 mg/dL.
All analyses presented here have performed the recommended recalibration.
Grade 3 chronic kidney disease (CKD) is deﬁned as a GFR less than 60 millilitres per
minute per 1.73m2 and is associated with increased morbidity and risk of end stage renal
disease. As such we consider building a model for predicting grade 3 CKD. To illustrate
the generalizability of the proposed HTE estimator, we approach this binary prediction task
from two separate vantage points and using two diﬀerent loss functions. First, we use logistic
regression to demonstrate the empirical equivalence between dAIC and the HTE approach;
here, deviance loss is used. Second, we compare a number of prediction models with respect
to in-sample and estimated out-of-sample errors; here, 0-1 loss is used. Using the NHANES
data, we ﬁrst demonstrate that dAIC and HTE give similar results for a binary prediction
task with deviance loss. Second, using 0-1 loss, we calculate the in-sample error and HTE
estimator for four common prediction methods (for which dAIC is not available) and show
how the HTE estimate might inﬂuence prediction model preference.
178

Deviance loss is used for the dAIC/HTE comparison, so ˆλi is given by the log-odds (see
Section 2.2.2). Table 3 shows the results from the dAIC/HTE comparison based on a logistic
regression model with diﬀerent covariate combinations. p/n, the number of covariates divided
by sample size, is used as a reference that accords with a scaled traditional AIC, where
the scaling is to meant for easy comparison to prediction error. dAIC is similarly scaled.
In general, all optimism estimates grow with the model size, but all generalization error
estimates get smaller with model size. Indeed, the optimism estimates are kept small by the
large size of the data sample. We note that dAIC and HTE are close.
We also compare the in-sample error and HTE estimated errors for k-nearest neighbors
(kNN) models with diﬀerent values of k. Here we adopt 0-1 loss, so ˆλi is given by -1 for
pi < .5 and 1 for pi ≥.5 [54]. Results are shown in Table 4. In general, the larger k is, the
smoother the decision rule. In terms of the bias-variance trade-oﬀ, this amounts to more bias
and less variance. Indeed, as k increases from 10 to 60, in-sample prediction error increases,
but the covariance-inﬂation decreases. The HTE estimator appears to achieve an optimum
somewhere around k = 30.
A.6
Discussion
Motivated by the increasing importance of algorithmic prediction methodologies and the need
to eﬀectively make predictions in the public health and medical sectors, we have presented a
prediction error estimation methodology with the hope that it will provide for the rigorous
comparison of competing predictive rules obtained from complex survey data. We have shown
that our Horvitz–Thompson–Efron (HTE) estimator is accurate and particularly robust for
algorithmic prediction methods. Moreover, we have proved that the HTE generalizes dAIC
(an AIC variant for complex samples) in the exact same way that Efron’s covariance-inﬂated
estimator generalizes AIC. This fact was empirically demonstrated via simulation and also
179

by considering the prediction of chronic kidney disease using data from NHANES III, a large
public health survey with prescribed sampling weights.
There is a trend in medicine toward increasingly personalized treatment. Such treatment
is essentially a prediction task and, as such, is subject to the bias-variance trade-oﬀ. To
help avoid over-ﬁtting when training the necessary predictive models, it will be necessary
to use large swaths of public health data, the majority of which is derived from complex
sampling procedures. We therefore expect that our proposed methodology and its exten-
sions will become increasingly important for model scoring in the context of personalized
medicine. Causal inference from observational data is in some ways the opposite challenge
of personalized medicine, although the two are closely tied together. Moreover, methods in
observational causal inference often make use of the Horvitz–Thompson reweighting proce-
dure. We are particularly interested in the question of whether the proposed HTE estimator
may be extended using reweighting procedures commonly use in causal inference and whether
this methodology might be useful for eﬀective personalized medicine. This remains an area
of current research.
There are a number of immediate extensions to the methodology proposed here. Whereas the
HTE estimator is based on the closed-form model optimism, algorithmic and non-analytic
prediction error estimators are more common in the machine learning literature. We therefore
anticipate the extension of both cross-validation and bootstrap prediction error estimation
to the complex sample domain. Indeed, cross-validation for complex samples has already
been roughly outlined in [122], but a full study of the method’s properties is needed. Some
prediction methods are too computationally intensive for cross-validation or the bootstrap.
For this reason there is a need for the development of further generalization error bounds
(e.g. VC dimension) in addition to those already established in [33]. Statistical learning
theory has traditionally been focused on i.i.d. samples, so there is likely a signiﬁcant portion
of literature that can be transferred to the complex sample context.
180

Appendix B
Bayesian inference on general
Riemannian manifolds
B.1
Real and complex matrix derivatives
The derivative of a univariate, real valued function with respect to a matrix is most cleanly
calculated using the matrix diﬀerential. This is true whether f : Mp(R) 7→R or f : Mp(C) 7→
R, i.e. whether f is a function over real p × p matrices or complex p × p matrices. As an
example, we consider the multivariate Gaussian distribution with mean 0 and covariance Σ.
First, let f be the probability density function over real valued Gaussian random vectors
yn ∈Rd, n = 1, . . . , N. Let Y be the d×N concatenation of these N i.i.d. random variables.
Then the log density is given by
log f(Y N, Σ) ∝−N
2 log |Σ| −1
2
N
X
n=1
yT
nΣ−1yn
(B.1)
= −N
2 log |Σ| −1
2tr{Σ−1Y Y T} .
181

We apply the matrix diﬀerential to (B.1) using two general formulas:
d log |Σ| = tr{Σ−1 d Σ},
and
d Σ−1 = −Σ−1 (d Σ) Σ−1 ,
(B.2)
rendering
d log f(Y, Σ) = −N
2 tr{Σ−1 d Σ} + 1
2tr

Σ−1(d Σ)Σ−1Y Y T	
(B.3)
= −N
2 tr{(d Σ) Σ−1} + 1
2tr

(d Σ) Σ−1Y Y TΣ−1	
.
Finally, we relate the matrix diﬀerential to the gradient with the fact that, for an arbitrary
function g,
d g(Σ) = tr{(d Σ)A}
⇐⇒
∇Σ g(Σ) = A .
(B.4)
This gives the ﬁnal form of the gradient of the log density function with respect to covariance
Σ:
∇Σ log f(Y, Σ) = −N
2 Σ−1 + 1
2Σ−1Y Y TΣ−1 .
(B.5)
For more on the matrix diﬀerential, see [124]. The complex matrix diﬀerential is treated
in [87] and has a similar form real valued functions. The log density of the multivariate
complex Gaussian with mean 0 is given by
log f(Y, Σ) ∝−N log |Σ| −
N
X
n=1
yH
n Σ−1yn
(B.6)
= −N log |Σ| −tr{Σ−1Y Y H} ,
182

where (·)H denotes the conjugate transpose. Note that the log density is scaled by a factor
of two compared to the real case. The resulting gradient is
∇Σ log f(Y, Σ) = −NΣ−1 + Σ−1Y Y HΣ−1 .
(B.7)
B.1.1
The complex reference prior
Gradients of prior probabilities are calculated in a similar way. We demonstrate for the
complex reference prior. Let λi, i = 1, . . . , d be the decreasing eigenvalues of Hermitian PD
matrix Σ. Then the complex reference prior has the following form:
p(Σ) ∝
dΣ
|Σ| Q
k<j(λk −λj)2 .
(B.8)
To use the above approach for deriving the matrix derivatives, we need to be able to write
the diﬀerential dλi in terms of the matrix diﬀerential dΣ. [124] provides the formula when
all eigenvalues are distinct:
dλi = tr

d
X
j=1
V −1
ij Σj−1dΣ

,
(B.9)
where V is the Vandermonde matrix:
V T =

1
λ1
λ2
1
· · ·
λn−2
1
λn−1
1
1
λ2
λ2
2
· · ·
λn−2
2
λn−1
2
...
...
...
...
...
...
1
λn
λ2
n
· · ·
λn−2
n
λn−1
n

.
(B.10)
183

We now calculate the gradient of the log of the complex reference prior:
d log p(Σ) = −d log |Σ| −2
X
k<j
d log(λk −λj)
(B.11)
= −tr(Σ−1dΣ) −2
X
k<j
dλk −dλj
λk −λj
= −tr(Σ−1dΣ) −2
X
k<j
tr

d
X
i=1
 V −1
ki −V −1
ji

Σi−1dΣ

/(λk −λj) .
Combining this with Equations (B.2) and (B.4) renders matrix gradient
∇Σ log p(Σ) ∝−Σ−1 −2
X
k<j

d
X
i=1
 V −1
ki −V −1
ji

Σi−1
/(λk −λj) .
(B.12)
184

Appendix C
Simplifying the geodesic Monte Carlo
C.1
Projection matrix for the Stiefel manifold
When modeling an element x ∈O(d, s) of the Stiefel manifold, for d × s momentum matrix
we write the degenerate Gaussian distribution
Det−1/2(ΠxM Πx) exp

−1
2 vec(p)T(ΠxM Πx)+ vec(p)

,
Πx and M are ds × ds matrices.
To get the form for Πx, we note that the orthogonal
projection of a matrix v onto the tangent space at x is
Πx(v) = v −1
2x(vTx + xTv) .
185

Applying the vec operator gives
vec(Πx(v)) = vec(v) −1
2 vec
 x(vTx + xTv)

= vec(v) −1
2(Is2 ⊗x) vec(vTx + xTv)
= vec(v) −1
2(Is2 ⊗x) vec(vTx) + vec(xTv)
= vec(v) −1
2(Is2 ⊗x)P vec(xT) + xTv)
= vec(v) −1
2(Is2 ⊗x)(P + Ids) vec(xTv)
= vec(v) −1
2(Is2 ⊗x)(P + Ids)(Is2 ⊗xT) vec(v)
=
 Ids −1
2(Is2 ⊗x)(P + Ids)(Is2 ⊗xT)

vec(v)
= Πx v
Hence
Πx = Ids −1
2(Is2 ⊗x)(P + Ids)(Is2 ⊗xT) .
186

Appendix D
Fisher geometry and Bayesian
nonparametric density estimation
D.1
Initializing the Markov chain: Newton’s method
on the sphere
Starting with a Riemannian manifold Q isometrically embedded in Euclidean space, we
consider function F : Q →R.
Deﬁnition D.1. Given point q0 ∈Q and initial velocity ˙q0 ∈Tq0Q, we follow [51] and deﬁne
the Hessian of function F along ˙q0 as the matrix satisfying
Hess F( ˙q0, ˙q0) = d2
dt2

t=0 F(q(t)) .
(D.1)
Proposition D.1. Hess F on the sphere is given by
Hess F = Fqq −F T
q q0 I ,
(D.2)
187

where Fq and Fqq are the Jacobian and usual Hessian matrices.
Proof. We need the formula for the geodesic on the sphere given q0 and ˙q0. Letting α be the
Euclidean norm of ˙q0, the geodesic is given by:
q(t) = q0 cos(αt) + ˙q0
α sin(αt) .
(D.3)
It is easy to verify that
¨q(t) = −α2 q(t) .
(D.4)
Next the derivatives are given by:
d
dtF(q(t)) = ∂F
∂q (y(t)) ˙q(t) ,
(D.5)
and
d2
dt2F(q(t)) = ˙q(t)T ∂2F
∂q2 ˙q(t) + ∂F
∂q
T
¨q(t) .
(D.6)
Combining (D.4) with (D.6) gives:
d2
dt2F(q(t)) = ˙q(t)T ∂2F
∂q2 ˙q(t) −α2 ∂F
∂q
T
q(t)
(D.7)
= ˙q(t)T
Fqq −F T
q q(t) I

˙q(t) .
Evaluating at t = 0 gives the result.
Hess F is the Hessian matrix at point q0 in direction ˙q0. Newton’s method on the sphere is
achieved by Algorithm 5.
188

Algorithm 5 A single iteration of Newton’s method on the sphere
1: Given point q on sphere:
2: Calculate Fq
3: Calculate Hess F = Fqq −F T
q q0 I
4: Calculate W = (I −qqT) Hess−1F (I −qqT)
5: V ←−W Fq
6: Progress along geodesic (D.3) with initial velocity V for time 1.
7: q ←q(1)
D.2
Relationship to the Cox process
The χ2-process density prior may be used to model the intensity function of a Cox process
[38]. The Cox process is a point process over a given domain such that each realization at
point t is drawn from a Poisson distribution with intensity µ(s), where intensity function
µ(·) is itself a random process over the same given domain. Cox processes are useful for the
analysis of spatial and time series data. Given µ(·), the likelihood of such data {sn}N
n=1 is
given by
p
 {sn}N
n=1|µ(·)

= exp

−
Z
D
µ(s) ds

×
N
Y
n=1
µ(sn) .
(D.8)
Bayesian inference on µ(·) requires the calculation of two integrals, that over the parameter
space and that from Equation (D.8). We make the latter integral trivial by modeling the
intensity function as the product of a density function and a positively constrained random
variable:
µ(s) = M × p(s) = M × q(s)2 .
(D.9)
189

In this case, the likelihood may be written
p
 {sn}N
n=1|µ(·)

= exp

−
Z
D
Mq(s)2 ds

×
N
Y
n=1
Mq(sn)2
(D.10)
= exp(−M) M N
N
Y
n=1
q(sn)2 .
Since the likelihood factors in M and q(·), it follows that the two random variables will
be independent in posterior distribution if they are speciﬁed to be independent in prior
distribution. Indeed, M may even be given a conjugate prior: it is easy to see that
M ∼Γ(a, b) ,
implies
M|N ∼Γ(a + N, b + 1) .
(D.11)
Sampling from the joint posterior of µ(·) is as simple as independently sampling M from
its posterior and q2(·) from the χ2-process density sampler and then multiplying the two
together. Such a model should be used with care. As a function of the data, the posterior
distribution of M solely depends on N, which is itself a single realization from a Poisson dis-
tribution. Thus, our χ2-process density prior–Cox process formulation is useful in situations
where ample prior information on M is available.
190

