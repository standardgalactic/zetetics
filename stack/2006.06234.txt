Revisiting the Continuity of Rotation
Representations in Neural Networks
Sitao Xiang
University of Southern California
sitaoxia@usc.edu
Hao Li
Pinscreen, Inc.
hao@hao-li.com
Abstract
In this paper, we provide some careful analysis of certain pathological behavior
of Euler angles and unit quaternions encountered in previous works related to
rotation representation in neural networks. In particular, we show that for certain
problems, these two representations will provably produce completely wrong
results for some inputs, and that this behavior is inherent in the topological property
of the problem itself and is not caused by unsuitable network architectures or
training procedures. We further show that previously proposed embeddings of
SO(3) into higher dimensional Euclidean spaces aimed at ﬁxing this behavior are
not universally effective, due to possible symmetry in the input causing changes to
the topology of the input space. We propose an ensemble trick as an alternative
solution.
1
Introduction
Quaternions and Euler angles have traditionally been used to represent 3D rotations in computer
graphics and vision. This tradition is preserved in more recent works where neural networks are
employed for inferring or synthesizing rotations, for a wide range of applications such as pose
estimation from images, e.g. [10], and skeleton motion synthesis, e.g. [9]. However, difﬁculties
has been encountered, in that the network seems unable to avoid rotation estimation errors in excess
of 100◦in certain cases, as reported by [10]. Attempts has been made to explain this, including
arguments that Euler angle and quaternion representations are not embeddings and in a certain sense
discontinuous [11], and from symmetry present in the data [10, 6]. One proposed solution is to use
embeddings of SO(3) into R5 or R6 [11]. However, we feel that these arguments are mostly based
on intuition and empirical results from experiments, while the nature of the problem is topological
which is one aspect that has not been examined in depth. In this paper we aim to give a more precise
characterization of this problem, theoretically prove the existence of high errors, analysis the effect
of symmetries, and propose a solution to this problem. In particular:
• We prove that a neural network converting rotation matrices to quaternions and Euler angles
must produce an error of 180◦for some input.
• We prove that symmetries in the input cause embeddings to also produce high errors, and
calculate error bounds for each kind of symmetry.
• We propose the self-selected ensemble, a method that works well with many different
rotation representations, even in the presense of input symmetry.
We further verify our theoretical claims with experiments.
Preprint. Under review.
arXiv:2006.06234v2  [math.OC]  12 Jun 2020

2
Theoretical Results
2.1
Guaranteed Occurrence of High Errors
We ﬁrst consider a toy problem: given a 3-d rotation represented by a rotation matrix, we want to
convert it to other rotation representations with neural networks. We will see that under the very
weak assumption that our neural network computes a continuous function, it is provable that given
any such network that converts rotation matrices to quaternions or Eular angles, there always exists
inputs on witch the network produces outputs with high error.
When treating quaternions as Euclidean vectors, we identify q = a + bi + cj + dk with (a, b, c, d).
We denote the vector dot product between p and q with a dot as p · q and quaternion multiplication
with juxtaposition as pq. The quaternion conjugate of q is q and the norm of q which is the same for
quaternions and vectors is ||q||.
It has been noticed that any function that converts 3D rotation matrices to their corresponding quater-
nion exhibits some “discontinuities” and that this is related to the fact that SO(3) does not embed
in R4. This has been argued by giving a speciﬁc conversion function f and ﬁnding discontinuities.
Most often, given a rotation matrix M, if tr(M) > −1 we have
f(M) = ( t
2, 1
2t(M32 −M23), 1
2t(M13 −M31), 1
2t(M21 −M12))
(1)
where t =
p
1 + tr(M). Since quaternions q and −q give the same rotation, any conversion from
rotation matrix to quaternion needs to break ties. The conversion given above breaks ties towards
the ﬁrst coordinate being positive. When it equals zero there needs to be additional rules that are
not relevant here. Then discontinuities can be found by taking limits on the “decision boundary”:
consider r(t) : [0, 1] →SO(3) deﬁned by
r(t) =
"cos 2πt
−sin 2πt
0
sin 2πt
cos 2πt
0
0
0
1
#
(2)
That is, r(t) is the rotation around z-axis by angle 2πt. Then f(r(t)) = (cos πt, 0, 0, sin πt) when
r ∈[0, 1
2) and f(r(t)) = (−cos πt, 0, 0, −sin πt) when r ∈( 1
2, 1]. So, we have
lim
t→1
2
−f(r(t)) = (0, 0, 0, 1) ̸= (0, 0, 0, −1) = lim
t→1
2
+ f(r(t))
(3)
Thus f is not continuous at r( 1
2). Since neural networks typically compute continuous functions,
such a function cannot be computed by a neural network.
However, we feel that this argument fails to address this problem satisfactorily. Firstly, it pertains to
a speciﬁc conversion rule. The ties can be broken towards any hemisphere, for which there are an
inﬁnite number of choices. In fact the image of f needs not be a hemisphere. In addition, there is
no reason to mandate a speciﬁc conversion function for the neural network to ﬁt. We need to prove
that even with the freedom of learning its own tie-breaking rules, the neural network cannot learn a
correct conversion from rotation matrices to quaternions.
Another shortcoming is that this argument does not give error bounds. Since neural networks can only
approximate the conversion anyways, can we get a continuous conversion function if we allow some
errors and if so, how large does the margin have to be? Experiments in [11] hint at an unavoidable
maximum error of 180◦, which is the largest possible distance between two 3D rotations. We want to
prove that. Now we introduce our ﬁrst theorem. Let RQ : S3 →SO(3) be the standard conversion
from a quaternion to the rotation it represents:
RQ(w, x, y, z) =


1 −2y2 −2z2
2(xy −zw)
2(xz + yw)
2(xy + zw)
1 −2x2 −2z2
2(yz −xw)
2(xz −yw)
2(yz + xw)
1 −2x2 −2y2


(4)
and let d(R1, R2) denote the distance between two rotations R1 and R2, measured as an angle. To
reduce cumbersome notations we overload d so that when a quaternion q appear as an argument in d
we mean RQ(q). We have d(p, q) = 2 cos−1 |p · q| (see appendix A).
Theorem 1. For any continuous function f : SO(3) →S3, there exists a rotation R ∈SO(3) such
that d(R, f(R)) = π.
2

Proof. Let r(t) : [0, 1] →SO(3) be deﬁned as in equation 2. r(t) is continuous in t. Consider r as a
path in SO(3). r(0) = r(1) = I3×3, so r is a loop.
S3 is a covering space of SO(3) with RQ being the covering map. RQ(1, 0, 0, 0) = I3×3 = r(0).
By the lifting property of covering spaces,1 r lifts to a unique path in S3 starting from (1, 0, 0, 0).
That is, there exists a unique continuous function h : [0, 1] →S3 such that RQ(h(t)) = r(t) for all
0 ≤t ≤1 and h(0) = (1, 0, 0, 0). It is easy to see that h∗(t) = (cos πt, 0, 0, sin πt) is that path.
Let v(t) = h∗(t) · f(r(t)), then v(0) = (1, 0, 0, 0) · f(I3×3) and v(1) = (−1, 0, 0, 0) · f(I3×3), so
v(0) = −v(1). v is continuous in t. By the intermediate value theorem, there exists t0 ∈[0, 1] such
that v(t0) = 0. So, d(h∗(t0), f(r(t0))) = 2 cos−1 |v(t0)| = 2 cos−1 0 = π.
Now we have found R0 = r(t0) such that the rotations R0 = RQ(h∗(t0)) and RQ(f(R0)) =
f(r(t0))) differ by a rotation of π.
Note that there exists continuous functions mapping a set of Euler angles to a quaternion representing
the same rotation. For example, for extrinsic x-y-z Euler angles (α, β, γ), we can get a quaternion of
this rotation by multiplying three elemental rotations:
Qxyz(α, β, γ) = (cos γ
2 + k sin γ
2 )(cos β
2 + j sin β
2 )(cos α
2 + i sin α
2 )
(5)
So, as a corollary, we conclude that a continuous function from 3D rotation matrices to Euler
angles likewise must produce large error at some point, for otherwise by composing it with Qxyz
we get a continuous function violating theorem 1. Let Rxyz(α, β, γ) : R3 →SO(3) be the
standard conversion from extrinsic x-y-z Euler angles to rotation matrices given by Rxyz(α, β, γ) =
RQ(Qxyz(α, β, γ)).
Corollary 2. For any continuous function f : SO(3) →R3, there exists a rotation R ∈SO(3) such
that d(R, Rxyz(f(R))) = π.
Obviously the same conclusion holds for any possible sequence of Euler angles, intrinsic or extrinsic.
It is easy to see that the same type of argument applies to the simpler case of functions from SO(2)
to R that tries to compute the angle of the rotation, by using the top-left 2 × 2 of r(t), setting
h∗(t) = 2πt and v(t) = h∗(t) −r(t) and ﬁnding t0 such that v(t0) = π.
Theorem 3. For any continuous function f : SO(2) →R, there exists a rotation R ∈SO(2) such
that the rotation of angle f(R) differs from R by a rotation of angle π.
Given that the quaternion representation of 3D rotations is due to the exceptional isomorphism
Spin(3) ∼= SU(2), there is no obvious generalization to n-dimensional rotations. We do however
discuss analogous results for 4D rotations in appendix D, due to Spin(4) ∼= SU(2) × SU(2).
2.2
The Self-selecting Ensemble
Neural networks are typically continuous (and differentiable) so that gradient based methods can be
used for training. Nevertheless, we often employ discontinuous operations at test time, e.g. quantizing
a probability distribution into class label in classiﬁcation networks. Unfortunately this does not work
for a regression problem with a continuous output space. However, by employing a simple ensemble
trick, it is possible to transform this discontinuity of regression into a discontinuity of classiﬁcation.
Consider again the problem of recovering the rotation angle from a 2D rotation matrix. Given
M =
 a −b
b
a

with a2 + b2 = 1, what we want is exactly atan2(b, a). There are different ways
for choosing the principal value for this multi-valued function, e.g. in [0, 2π) or in (−π, π]. Let us
distinguish between these two by calling the ﬁrst one atan2▷and the second one atan2◁. They have
discontinuities at rotation angles of 2kπ and (2k + 1)π (k ∈Z), respectively.
Now we can construct two functions that are continuous and computes atan2▷and atan2◁respec-
tively except when near the discontinuity, where they give incorrect values in order to make them
continuous. If we make sure that these two “wrong regions” do not overlap, then for any input matrix
at least one of the two functions will give a correct rotation angle.
1See standard algebraic topology texts, e.g. page 60 of [2]
3

Theorem 4. There exists continuous functions f1, f2 : SO(2) →R such that for any rotation
R ∈SO(2), at least one of f1(R) and f2(R) gives the correct rotation angle of R.
Proof. We give an example of f1 and f2 as follows:
f1

a
−b
b
a

=
2π −2 · atan2▷(b, a)
(a < −1
2)
atan2◁(b, a)
(a ≥−1
2)
(6)
f2

a
−b
b
a

=
atan2▷(b, a)
(a ≤1
2)
π −2 · atan2◁(b, a)
(a > 1
2)
It can be checked that these functions are continuous and that their wrong regions do not overlap.
Since these functions are continuous, they can be approximated by neural networks. On top of these,
we can add a classiﬁer that predicts which function would give the correct output for each input.
During training time, these functions and the classiﬁer can be trained jointly: the error of the whole
ensemble is the sum of the error of each individual functions, weighted by the probability assigned
by the classiﬁer. Now the discontinuity only happens at test time, when we select the output of the
function with highest assigned probability. We call this method the self-selecting ensemble.
Can a similar approach work for the conversion from 3D rotation matrices to quaternions? It turns
out that two or even three functions are not enough:
Theorem 5. For any three continuous functions f1, f2, f3 : SO(3) →S3, there exists a rotation
R ∈SO(3) such that d(R, fi(R)) = π for all i ∈{1, 2, 3}.
Proof. Consider functions vi : S3 →R deﬁned by vi(q) = q · fi(RQ(q)), for i = 1, 2, 3. For any
q ∈S3, Since RQ(−q) = RQ(q), vi(−q) = −q · fi(RQ(−q)) = −q · fi(RQ(q)) = −vi(q).
Let V : S3 →R3 deﬁned by V (q) = (v1(q), v2(q), v3(q)), then V (−q) = −V (q) for any
q ∈S3. By the Borsuk–Ulam theorem,2 there exists q0 ∈S3 such that V (−q0) = V (q0), then
−V (q0) = V (−q0) = V (q0), so V (q0) = 0, which means v1(q0) = v2(q0) = v3(q0) = 0. So
for R0 = RQ(q0) ∈SO(3), d(R0, fi(R0)) = π for i = 1, 2, 3.
Theorem 5, with a seemingly simpler proof, implies theorem 1. But the proof of Borsuk–Ulam
theorem is not simple, and it applies to hyperspheres only while the techniques in theorem 1 can be
useful for other spaces as well, as we will show.
Allowing a fourth function, however, can give us a successful ensemble:
Theorem 6. There exists continuous functions f1, f2, f3, f4 : SO(3) →S3 such that for any rotation
R ∈SO(3), RQ(fi(R)) = R for some i ∈{1, 2, 3, 4}.
The proof is by construction. See appendix B.
Is the same true for Euler angles? Similar to corollary 2 we can conclude that an ensemble of 3
functions will not work. But to ﬁnd an ensemble of 4 functions that does work, we have to additionally
deal with gimbal lock. We ﬁrst show that there is no correct continuous conversion from rotation
matrices to Euler angles when the domain contains a gimbal locked position in its interior.
Theorem 7. Let U be any neighborhood of Rxyz(0, π
2 , 0) in SO(3). There exists no continuous
function f : U →R3 such that Rxyz(f(R)) = R for every R ∈U.
See appendix B for proof.
The same conclusion can be drawn near any Rxyz(α, β, γ) where β = ± π
2 . Since gimbal locked
positions has to be correctly handled, this problem cannot be solved by simply adding more functions
that all have the same gimbal locked positions. Notice that if the order of elemental rotations is x-z-y,
then the gimbal locked positions will be different. Analogous to Qxyz we deﬁne Qxzy as follows:
Qxzy(α, β, γ) = (cos γ
2 + j sin γ
2 )(cos β
2 + k sin β
2 )(cos α
2 + i sin α
2 )
(7)
2See e.g. page 174 of [2]
4

and also Rxzy(α, β, γ) = RQ(Qxzy(α, β, γ)). We show the existence of a mixed Euler angle
ensemble that gives the correct conversion:
Theorem 8. There exists continuous functions f1, f2, f3, f4 : SO(3) →R3 such that for any rotation
R ∈SO(3), at least one of the following is equal to R: Rxyz(f1(R)), Rxyz(f2(R)), Rxzy(f3(R))
and Rxzy(f4(R)).
The proof is by construction. See appendix B.
In section 3.1, we show with experiments that a neural network can successfully learn ensembles
of four quaternion representations or ensembles of four mixed-type Euler angle representations that
gives small error for rotation matrix conversion over the entire SO(3).
2.3
Input Symmetry and Effective Input Topology
The discussion above is of theoretical interest, but would be of little practical relevance if the discon-
tinuity of quaternion and Euler angle representations can be solved by simply using a representation
that is continuous, such as the embedding of SO(3) into R5 proposed in [11]. We show however, that
due to the combined effect of a symmetry in the input and a symmetry of the neural network function,
such embeddings can become ineffective.
As in [11], we consider the problem of estimating the rotation of a target point cloud relative to a
reference point cloud. Since by giving both the target and the reference and considering all possible
point cloud at once we will be faced with a extremely complicated input space, for our theoretical
analysis we focus on a very simple case: the reference point cloud is ﬁxed, so that the network is only
given the target point cloud which is a rotated version of the ﬁxed reference point cloud.
At a ﬁrst glimpse the topological structure of this problem is exactly the same as converting a 3D
rotation matrix into other representations, since there is a homeomorphism between the input space
and SO(3). However, the neural network might see a different picture. In a point cloud, there is no
assumption of any relationship between different points, and it is considered desirable for the neural
network to be invariant under a permutation of input points, which is a design principle of some
popular neural network architectures for point cloud processing, e.g. PointNet [5].
This behavior causes unexpected consequences. If the input point cloud itself possesses nontrivial
rotational symmetry, then different rotations on this point cloud might result in the same set of points,
differing only in order. Since the neural network is oblivious to the order of input points, these point
clouds generated by different rotations are effectively the same input to the network.
Let X ⊂R3 be a 3D point cloud and R ∈SO(3) be a rotation. Let RX = {Rx | x ∈X}. Then
the symmetry group of X, Sym(X), is deﬁned by Sym(X) = {R ∈SO(3) | RX = X}. It is a
subgroup of SO(3). Take X as the reference point cloud. Then if two rotations R1, R2 ∈SO(3)
generates the same target point cloud, then R1X = R2X, so X = R−1
1 R2X, so R−1
1 R2 ∈Sym(X),
that is, R1 and R2 belong to the same left coset of Sym(X) in SO(3). The reverse is also true.
So for the network, two inputs are equivalent if and only if the rotations that generate them belong to
the same left coset of Sym(X). The left cosets of Sym(X) in SO(3) forms a homogeneous space,
denoted SO(3)/Sym(X).3 When X is ﬁnite, except for the degenerate case where all points of X
lie on a line, Sym(X) must be ﬁnite. If G is a ﬁnite subgroup of SO(3), then SO(3) is a covering
space of SO(3)/G, with covering map pG : SO(3) →SO(3)/G, R 7→RG.
We can show that when Sym(X) is nontrivial, a network that is invariant under input point permuta-
tion cannot always recover the correct rotation matrix. Here by “correct” we mean the rotation given
by the network need not be the same as the one used for generating the input point cloud, but must
generate the same point cloud up to permutation. p−1
G [R] denotes the preimage of R under pG.
Theorem 9. Let G be a nontrivial ﬁnite subgroup of SO(3), then there does not exist continuous
function f : SO(3)/G →SO(3) such that for every R ∈SO(3)/G, pG(f(R)) = R.
Proof. Assume that there exists such a function f. Choose any R ∈SO(3)/G. Let R0 = f(R).
Then R0 ∈p−1
G [R]. Since G is a nontrivial group, SO(3) is a nontrivial covering space of SO(3)/G.
3A quotient group is denoted the same way but Sym(X) needs not be a normal subgroup of SO(3) in general
so here we do not mean a quotient group.
5

So |p−1
G [R]| > 1. Select any R1 ̸= R0 from p−1
G [R]. SO(3) is path-connected, so there is a path
in SO(3) from R0 to R1, that is, there exists continuous function r : [0, 1] →SO(3) such that
r(0) = R0 and r(1) = R1. Fix such an r and let h = pG ◦r. Then h is a path in SO(3)/G and
h(0) = h(1) = R.
By the lifting property of covering spaces, h lifts to a unique path in SO(3) starting from R0, which
is just r. That is, r is the unique continuous function such that h = pG ◦r and r(0) = R0. We
also have pG ◦(f ◦h) = (pG ◦f) ◦h = 1 ◦h = h and f(h(0)) = f(R) = R0. Since r is the
unique continuous function such that h = pG ◦r and r(0) = R0, we must have r = f ◦h. But
r(1) = R1 ̸= R0 = f(h(1)), which is a contradiction.
This is essentially the same proof as in theorem 1 but without error bounds. Such bounds can be
established, but the techniques are much more complicated. We discuss about this in appendix B.
Similar to corollary 2, for an embedding g : SO(3) →Rn, since we can continuously map from the
image of g back to SO(3), there exists no continuous function f that ﬁnds such an embedding of the
correct rotation from the input point cloud, for otherwise g−1 ◦f gives a continuous function that
computes a correct rotation matrix.
Corollary 10. Let G be a nontrivial ﬁnite subgroup of SO(3) and g : SO(3) →Rn be an embedding,
then there does not exist continuous function f : SO(3)/G →g[SO(3)] such that for every R ∈
SO(3)/G, pG(g−1(f(R))) = R.
In particular, This means using the 6D or 5D rotation representations proposed in [11], which are
embeddings of SO(3) in R6 or R5, does not resolve the problem. However, the self-selecting
ensemble can solve this problem. We state the following without a formal proof:
Proposition 11. Let G be a ﬁnite subgroup of SO(3), then there exists continuous functions
f1, f2, f3, f4 : SO(3)/G →SO(3) such that for every R ∈SO(3)/G, pG(fi(R)) = R for
some i ∈{1, 2, 3, 4}.
The idea is that to each fi is assigned a contractible subset of SO(3)/G on which they give the
“correct” output. The values on the rest of SO(3)/G are such that fi is continuous on SO(3)/G.
Then the fi’s satisfy the requirement if these contractible subsets collectively cover SO(3)/G. We
discuss more about this in appendix B.
In section 3.2, we test rotation estimation for point clouds using different representations, on one
point cloud with trivial symmetry and one with nontrivial rotational symmetry. We show that for the
nontrivial case, an ensemble is necessary for the 5D and 6D embeddings as well as quaternions.
3
Experiments
3.1
Converting Rotation Matrices
We test the accuracy of converting a 3D rotation matrix into various representations with neural
networks, including ensembles. We use an MLP with 5 hidden layers of size 128 each. The size of
the output layer varies according to the representation. For ensembles, each individual function as
well as the classiﬁer share all their computations except for the output layer, so the overhead of an
ensemble over a single network is tiny.
For a single network, the loss function is simply the rotation distance between the input and the
output. For an ensemble of n functions, let each individual function in the ensemble be fi and the
classiﬁer be g. Here we take the “raw” output vector of the classiﬁer, without converting it into a
distribution with e.g. softmax. The loss of the whole ensemble on one input rotation R is deﬁned as
L(R) =
n
X
i=1
max{0, g(i)(R)} · d(R, fi(R)) + max{0, 1 −
n
X
i=1
g(i)(R)} · Lmax
(8)
where Lmax = π is the maximum possible distance between two 3D rotations. At test time, fk(R) is
selected as the output of the ensemble where k = argmaxi g(i)(R).
We train each network using Adam with learning rate 10−4 and batch size 256 for 500, 000 iterations.
For training and testing, we sample uniformly from SO(3) (see appendix A.4 for some notes). We
sample 100 million random rotations for testing.
6

Figure 1 shows the semi-log plot of errors of each representation by percentile. Mean and maximum
errors are given in table 1. The maximum error is also marked in the graph for clarity. Here we
compare a single quaternion, ensemble of four quaternions, a single set of Euler angles, mixed Euler
angle ensemble with two sets each of x-y-z and x-z-y Euler angles, the 5D embedding and the 6D
embedding. Comparison of ensembles of different sizes can be found in appendix C. In particular we
will show that ensembles of three networks do not work.
The result does not actually show a maximum error of 180◦for a single quaternion or a single set of
Euler angles. This is because while such high errors are guaranteed to exist, they are nevertheless
very rare as in general the set of such inputs has measure zero and will almost never be encountered
by uniform random sampling. We can see that while using a single quaternion or a single set of Euler
angles we inevitably hit the maximum possible error, ensembles of four quaternions or mixed Euler
angles can give fairly accurate conversion from rotation matrices to quaternions or Euler angles. In
fact, the quaternion ensemble is only marginally worse than the 6D embedding and noticeably better
than the 5D embedding.
3.2
Estimating the Rotation of a Point Cloud
We test the accuracy of estimating the rotation of a point cloud with various rotation representation,
on point clouds with and without symmetries. There are many different possible symmetries. Here
we test one of the possibilities as an example. For each of symmetry/non-symmetry we only use
one ﬁxed point cloud, for both training and testing, with the rotation being the only variation in the
input. To understand why we use such an unconventional setting, along with all the details of network
architecture, loss function, training and construction of the point cloud data, please refer to appendix
C. For now it sufﬁces to know that our network is invariant under input point permutation, and that
our experiment comes in two parts: in part one, the point could does not have rotational symmetry; in
part two, the point cloud has the rotational symmetry D2 (in Schoenﬂies notation) which means it is
invariant under a rotation of 180◦around the x axis, the y axis and the z axis.
The result of part 1 is shown in ﬁgure 2 and table 2. A single quaternion, ensemble of 4 quaternions,
and the 5D and 6D embeddings are compared. The result is consistent with that of rotation matrix
conversion.
The result of part 2 is shown in ﬁgure 3 and table 3. It can be clearly seen that when the input
possessess nontrivial rotational symmetry, the 5D and 6D embeddings can no longer correctly
estimate the rotation of the input in all cases. In contrast, the ensemble of four quaternions continues
to perform well. We added the ensemble of four 5D embeddings and the ensemble of four 6D
embeddings to the comparison, and the results are similar to the ensemble of four quaternions. We
can see that the difference between ensembles and single networks is qualitative while the difference
between different kinds of representations is quantitative and comparatively rather minor.
From the numerical result one might guess that with a single network and input with D2 symmetry,
the lower bound of maximum error of rotation estimation is 120◦. We will derive this in appendix B.
4
Conclusion
In this paper, we analyzed the discontinuity problem of quaternion and Euler angle representation of
3D rotations in neural networks from a topological perspective and showed that a maximum error
of 180◦must occur. We further explored the effect of symmetry in the input on the ability of the
network to ﬁnd the correct rotation representation, and found that symmetries in the input can cause
continuous rotation representations to become ineffective, with provable lower bounds of maximum
error. We proposed the self-selecting ensemble to solve this discontinuity problem and showed that it
works well with different rotation representations, even in the presence of symmetry in the input. We
veriﬁed our theory with experiments on two simple example problems, the conversion from rotation
matrices to other representations and the estimation of the rotation of a point cloud. An extension of
our results to 4D rotations were also discussed.
The application of our theoretical analysis and ensemble method to real-world problems involving
rotation representation can be a direction for future researches. In addition, the potential usefulness
of the self-selecting ensemble for solving a broader range of regression problems with different input
and output topology or with other discontinuities in general can be further explored.
7

0%
20%
40%
60%
80%
100%
0.001 ∘
0.01 ∘
0.1 ∘
1 ∘
10 ∘
100 ∘
180 ∘
Quaternion
Quaternion,∘ensemble∘of∘4
Euler∘angle
Euler∘angle,∘mixed∘ensemble
5D∘embedding
6D∘embedding
Figure 1: Error of rotation matrix conversion by per-
centile
Table 1: Error statistics
• Type
Mean(◦)
Max(◦)
• Quat.
0.3323 179.9995
• Quat. ×4
0.0226
0.1059
• Euler
0.7368 179.9981
• Euler mix
0.2278
3.6693
• 5D
0.0838
0.4328
• 6D
0.0225
0.0901
0%
20%
40%
60%
80%
100%
0.1 ∘
1 ∘
10 ∘
100 ∘
180 ∘
Quaternion
Quaternion,∘ensemble∘of∘4
5D∘embedding
6D∘embedding
Figure 2: Error of rotation estimation by percentile, of
a point cloud with trivial symmetry
Table 2: Error statistics
• Type
Mean(◦)
Max(◦)
• Quat.
8.3963 179.9938
• Quat. ×4
1.1487
5.8517
• 5D
1.2550
13.5052
• 6D
1.0197
3.7235
0%
20%
40%
60%
80%
100%
0.1 ∘
1 ∘
10 ∘
100 ∘
180 ∘
Quaternion
Quaternion,∘ensemble∘of∘4
5D∘embedding
5D∘embedding,∘ensemble∘of∘4
6D∘embedding
6D∘embedding,∘ensemble∘of∘4
Figure 3: Error of rotation estimation by percentile, of
a point cloud with D2 symmetry
Table 3: Error statistics
• Type
Mean(◦)
Max(◦)
• Quat.
4.5983 116.2830
• Quat. ×4
0.8465
4.8397
• 5D
6.5245 116.5406
• 5D ×4
0.7958
4.5727
• 6D
4.5320 115.6072
• 6D ×4
0.7420
3.0548
8

Broader Impact
This work is mainly concerned with a theoretical problem and does not present any foreseeable
societal consequence.
References
[1] Gene H. Golub and Charles F. Van Loan. Matrix Computations. Johns Hopkins University
Press, 4th edition, 2013.
[2] Allen Hatcher. Algebraic Topology. Cambridge University Press, 2002. Electronic version
available at https://pi.math.cornell.edu/~hatcher/AT/ATpage.html.
[3] Johan Ernest Mebius. A matrix-based proof of the quaternion representation theorem for
four-dimensional rotations. arXiv preprint: math/0501249, 2005.
[4] R. E. Miles. On random rotations in R3. Biometrika, 52(3/4):636–639, 1965.
[5] Charles R. Qi, Hao Su, Kaichun Mo, and Leonidas J. Guibas. Pointnet: Deep learning on point
sets for 3d classiﬁcation and segmentation. In Proceedings of the IEEE conference on computer
vision and pattern recognition, pages 652–660, 2017.
[6] Ashutosh Saxena, Justin Driemeyer, and Andrew Y. Ng. Learning 3-d object orientation from
images. In 2009 IEEE International Conference on Robotics and Automation, pages 794–800.
IEEE, 2009.
[7] Floris Takens. The minimal number of critical points of a function on a compact manifold and
the lusternik-schnirelman category. Inventiones mathematicae, 6(3):197–244, 1968.
[8] L. van Elfrinkhof. Eene eigenschap van de orthogonale substitutie van de vierde orde. In
Handelingen van het zesde Nederlandsch Natuuren Geneeskundig Congres, pages 237–240,
1897.
[9] Ruben Villegas, Jimei Yang, Duygu Ceylan, and Honglak Lee. Neural kinematic networks for
unsupervised motion retargetting. In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pages 8639–8648, 2018.
[10] Yu Xiang, Tanner Schmidt, Venkatraman Narayanan, and Dieter Fox. Posecnn: A convolutional
neural network for 6d object pose estimation in cluttered scenes. In Robotics: Science and
Systems (RSS), 2018.
[11] Yi Zhou, Connelly Barnes, Jingwan Lu, Jimei Yang, and Hao Li. On the continuity of rotation
representations in neural networks. In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pages 5745–5753, 2019.
9

A
Mathematical Notes
A.1
Distance in SO(3)
The “difference” between two rotations R1 and R2 can be described by what it takes to change
R1 into R2, which can be done by multiplying R2R−1
1
on the left. R2R−1
1
is also a rotation. In 3
dimensions, the rotation angle of R2R−1
1
can be used to measure the distance between R1 and R2.
Rotation matrices are orthogonal matrices. An important property of orthogonal matrices is that all
their eigenvalues have length 1. Complex eigenvalues of real matrices always appear in conjugate
pairs. For an orthogonal matrix, this means their eigenvalues must be 1, −1 or pairs of complex
numbers of the form cos θ ± i sin θ.
For 3D rotation matrices, their three eigenvalues are exactly 1 and cos θ±i sin θ where θ is the rotation
angle. The trace of a matrix equals the sum of its eigenvalues. So, for a 3D rotation matrix with
rotation angle θ, tr(R) = 1+(cos θ+i sin θ)+(cos θ−i sin θ) = 1+2 cos θ, so θ = cos−1 tr(R)−1
2
.
Let d(R1, R2) denote the distance between R1 and R2, then d(R1, R2) = cos−1 tr(R2R−1
1
)−1
2
.
For a unit quaternion q = w + ix + jy + kz, we have
d(I, RQ(q)) = cos−1 tr(RQ(q)) −1
2
= cos−1 (1 −2y2 −2z2) + (1 −2x2 −2z2) + (1 −2x2 −2y2) −1
2
= cos−1 2 −4(x2 + y2 + z2)
2
= cos−1(2w2 −1)
= 2 cos−1 |w|
(9)
That is, d(I, RQ(q)) = 2 cos−1 |Re(q)| where Re(q) is the scalar part of q. We consider distances
between rotations represented by quaternions often, so to avoid having to write RQ every time we let
d also take quaternions as arguments directly, in which case by q we mean RQ(q). We have
d(p, q) = d(RQ(p), RQ(q))
= d(I, RQ(q)RQ(p)−1)
= d(I, RQ(qp))
= cos−1(2Re(qp)2 −1)
= cos−1(2(p · q)2 −1)
= 2 cos−1 |p · q|
(10)
Note however that d deﬁned as such is a metric in SO(3) but not a metric in S3, as it does not
satisfy the triangle inequality. Instead, we denote the usual metric on S3, the geodesic distance,
by dQ: dQ(p, q) = cos−1(p · q). We have d(p, q) = min{2dQ(p, q), 2π −2dQ(p, q)}, or,
d(p, q) = min{2dQ(p, q), 2dQ(p, −q)}.
A.2
Distance in SO(3)/G
In theorem 9 we left open the problem of establishing error bounds. To ﬁnd such bounds we must ﬁrst
deﬁne distance between a rotation and an element of SO(3)/G. Let S ∈SO(3) and R ∈SO(3)/G.
Deﬁne
dG(S, R) =
min
R∈p−1
G [R]
d(S, R)
(11)
10

That is, the distance from a rotation S to an element R of SO(3)/G is its distance to the nearest
preimage of R in SO(3). This can also be extended to have quaternions as the ﬁrst argument:
dG(s, R) = dG(RQ(s), R)
=
min
R∈p−1
G [R]
d(RQ(s), R)
=
min
R∈p−1
G [R]
d(s, R)
=
min
r∈R−1
Q [p−1
G [R]]
d(s, r)
=
min
r∈R−1
Q [p−1
G [R]]
min{2dQ(s, r), 2dQ(s, −r)}
=
min
r∈R−1
Q [p−1
G [R]]
2dQ(s, r)
(12)
On the last line, the inner min can be absorbed into the outer min because for any r ∈R−1
Q [p−1
G [R]]
we also have −r ∈R−1
Q [p−1
G [R]]. We then further extend the deﬁnition to allow quaternions as the
second argument:
dG(s, r) = dG(s, pG(RQ(r))) =
min
r′∈R−1
Q [p−1
G [pG(RQ(r))]]
2dQ(s, r′)
(13)
Note that R−1
Q [p−1
G [pG(RQ(r))]] = {rq|q ∈R−1
Q (G)}. R−1
Q (G), the preimage of G under the
covering map RQ : S3 →SO(3), is called a binary polyhedral group. Like speciﬁc ﬁnite subgroups
of SO(3), the speciﬁc binary polyhedral groups have their names and notations, but generically let us
denote them by bG. So we have
dG(s, r) = min
r′∈r b
G
2dQ(s, r′)
(14)
A.3
Distance in SO(4)
To derive analogous results with error bounds for 4D rotations, we need to deﬁne the distance between
two 4D rotations. In contrast to 3D rotations, there is no single rotation angle and rotation axis in
general. Instead, there exists a pair of orthogonal planes that are invariant under the rotation. The
rotation matrix has eigenvalues cos θ±i sin θ and cos φ±i sin φ, and the restriction of the 4D rotation
on each of these two planes is a 2D rotation, one with rotation angle θ and the other with rotation
angle φ.
Note that unlike in 3D where a rotation of angle −θ is also a rotation of angle θ around the opposite
axis, in 4D if we negate the sign of rotation in one of the pair of invariant planes by ﬂipping the
orientation of the other invariant plane, then the sign of rotation on the other invariant plane will
also be negated. So in general we cannot necessarily have θ and φ both positive without changing
the handedness of the coordinate system. Without loss of generality we assume that |θ| ≥|φ| and
θ ∈[0, π].
For two 4D rotations R1 and R2, we compute θ and φ from R2R−1
1
and take |θ| + |φ| as the distance
between R1 and R2. Let us denote this by d4(R1, R2).
Since we are interested in the quaternion representation of 4D rotations, we want to compute the
distance between two rotations from their quaternion representations, without having to compute
the eigenvalues of a non-symmetric matrix. We ﬁrst introduce the quaternion representation of 4D
rotations. The formula is due to van Elfrinkhof [8]. A proof in English can be found in [3].
Let A be a 4 × 4 matrix. Deﬁne the associate matrix M as
M = 1
4


A11+A22+A33+A44
A21−A12−A43+A34
A31+A42−A13−A24
A41−A32+A23−A14
A21−A12+A43−A34
−A11−A22+A33+A44
A41−A32−A23+A14
−A31−A42−A13−A24
A31−A42−A13+A24
−A41−A32−A23−A14
−A11+A22−A33+A44
A21+A12−A43−A34
A41+A32−A23−A14
A31−A42+A13−A24
−A21−A12−A43−A34
−A11+A22+A33−A44


(15)
M has rank 1 and Frobenius norm 1 if and only if A is a rotation matrix. In such case there exist
real numbers a, b, c, d, e, f, g and h such that M = [a b c d]T [e f g h] and a2 + b2 + c2 + d2 =
11

e2 + f 2 + g2 + h2 = 1. The solution is unique up to negating all these numbers. Then, the rotation
matrix A can be decomposed as A = ALAR where
AL =


a
−b
−c
−d
b
a
−d
c
c
d
a
−b
d
−c
b
a


AR =


e
−f
−g
−h
f
e
h
−g
g
−h
e
f
h
g
−f
e


(16)
The two matrices commute, and it can be checked that
ALAR[w xi yj zk]T = (a + bi + cj + dk)(w + xi + yj + zk)(e + fi + gj + hk)
(17)
So, given a pair of two unit quaternions qL = (a + bi + cj + dk) and qR = (e + fi + gj + hk), they
represent the 4D rotation
RQQ(qL, qR) =


a
−b
−c
−d
b
a
−d
c
c
d
a
−b
d
−c
b
a




e
−f
−g
−h
f
e
h
−g
g
−h
e
f
h
g
−f
e


(18)
and if p is a point (w, x, y, z) and p is its quaternion form w + xi + yj + zk, then the quaternion
form of RQQ(qL, qR)p is qLpqR. (qL, qR) and (−qL, −qR) represent the same rotation, and for
a given rotation matrix, (qL, qR) can be uniquely determined up to negation.
We then proceed to ﬁnd the relationship between quaternion distance dQ and 4D rotation distance
d4. The real Schur decomposition theorem4 states that for any real matrix A ∈Rn×n, there exists an
orthogonal matrix Q ∈Rn×n such that
QT AQ =


R11
R12
· · ·
R1m
0
R22
· · ·
R2m
...
...
...
...
0
0
· · ·
Rmm


(19)
where each Rii is either a 1 × 1 matrix or a 2 × 2 matrix having complex conjugate eigenvalues.
Apply this to the case where A is a 4D rotation matrix, and treat a pair of eigenvalue 1 or a pair of
eigenvalue −1 as a complex conjugate pair, then there exists orthogonal matrix Q ∈R4×4 such that
QT AQ =

R11
R12
0
R22

(20)
where R11 and R22 are 2 × 2 matrices having complex conjugate eigenvalues. Since A is or-
thogonal, QT AQ is also orthogonal. det(QT AQ) = det(A) = 1 so QT AQ is a rotation. From
(QT AQ)T (QT AQ) = I we can get R12 = 0. Assume that Q has determinant 1 (otherwise it has
determinant −1, then we can negate the last row of Q so that it has determinant 1) so that it is also a
rotation. QT AQ has the same eigenvalues as A, so we must have
QT AQ =


cos θ
−sin θ
0
0
sin θ
cos θ
0
0
0
0
cos φ
−sin φ
0
0
sin φ
cos φ


(21)
where θ and φ are the two rotation angles of A. Let S = QT AQ. Its associate matrix is
MS = 1
4


2 cos θ + 2 cos φ
2 sin θ −2 sin φ
0
0
2 sin θ + 2 sin φ
−2 cos θ + 2 cos φ
0
0
0
0
0
0
0
0
0
0


=


cos θ+φ
2
cos θ−φ
2
cos θ+φ
2
sin θ−φ
2
0
0
sin θ+φ
2
cos θ−φ
2
sin θ+φ
2
sin θ−φ
2
0
0
0
0
0
0
0
0
0
0


(22)
4See e.g. page 377 of [1]
12

From this we can then ﬁnd the quaternion representation of S:
S = RQQ(±(sL, sR)),
sL = cos θ + φ
2
+ i sin θ + φ
2
,
sR = cos θ −φ
2
+ i sin θ −φ
2
(23)
Since Q is also a rotation matrix, it can be represented as a pair of quaternions as well. Assume that
Q = RQQ(qL, qR).
Now we can ﬁnd the quaternion representation of A. Given a point p, the quaternion form of
Ap = QSQT p is qLsLqLpqRsRqR, which tells us that A = RQQ(qLsLqL, qRsRqR). If one
quaternion representation of A is (uL, uR), then (uL, uR) = ±(qLsLqL, qRsRqR).
For any unit quaternions p and q, Re(pqp) = (pq) · p = p · (pq) = Re(ppq) = Re(q), so either
Re(uL) = Re(qLsLqL) = Re(sL) = cos θ + φ
2
Re(uR) = Re(qRsRqR) = Re(sR) = cos θ −φ
2
(24)
or
Re(uL) = Re(−qLsLqL) = −Re(sL) = −cos θ + φ
2
Re(uR) = Re(−qRsRqR) = −Re(sR) = −cos θ −φ
2
(25)
In the ﬁrst case, dQ(1, uL) = θ+φ
2
and dQ(1, uR) = θ−φ
2 . In the second case, dQ(1, uL) = π−θ+φ
2
and dQ(1, uR) = π −θ−φ
2 . We assumed that θ ∈[0, π], so we can combine both cases:
|θ| = min{dQ(1, uL) + dQ(1, uR), 2π −dQ(1, uL) −dQ(1, uR)}
|φ| = |dQ(1, uL) −dQ(1, uR)|
(26)
So, in terms of quaternions, d4 is deﬁned as
d4((pL, pR), (qL, qR))
= min{dQ(pL, qL) + dQ(pR, qR), 2π −dQ(pL, qL) −dQ(pR, qR)}
+ |dQ(pL, qL) −dQ(pR, qR)|
(27)
A.4
Sampling Random Rotations
We would like to ensure that random 3D rotations are sampled properly for training and testing. [4]
gives a discussion of what it means for a distribution of 3D rotations to be “uniform”. We note here
that uniformly sampling α ∈(−π, π], β ∈[−π
2 , π
2 ] and γ ∈(−π, π] does not result in Rxyz(α, β, γ)
being uniformly distributed in SO(3).
Uniformly sampling a rotation axis from S2 and an angle from [0, π] does not give a uniformly
distributed random rotation in SO(3) either. This seemingly correct method actually heavily favors
rotations with small angles.
There is in fact a easy and correct way to sample a uniform random rotation in SO(3). The covering
map RQ : S3 →SO(3) is a local isometry, so if we uniformly sample a unit quaternion q from S3,
then RQ(q) is a uniform random rotation in SO(3). To uniformly sample a unit quaternion, sample a
4D vector from a standard normal distribution and normalize it to have unit length.
B
Additional Theoretical Results
In this section we give proofs omitted in the main text, as well as some additional remarks.
Theorem 6. There exists continuous functions f1, f2, f3, f4 : SO(3) →S3 such that for any rotation
R ∈SO(3), RQ(fi(R)) = R for some i ∈{1, 2, 3, 4}.
Proof. We give an example of such a set of functions. For clarity, we deﬁne them as functions of
quaternions. If we ensure that fi(q) = fi(−q) for all q ∈S3 and i ∈{1, 2, 3, 4}, they will be
13

well-deﬁned functions of rotations. N is the normalization function N(q) =
q
||q|| deﬁned for all
q ̸= 0. Let
f1(a, b, c, d) =







(a, b, c, d)
(a ≥1
2)
N(1 −a, 2ab, 2ac, 2ad)
(0 ≤a < 1
2)
N(1 + a, 2ab, 2ac, 2ad)
(−1
2 ≤a < 0)
(−a, −b, −c, −d)
(a < −1
2)
(28)
f2(a, b, c, d) =







(a, b, c, d)
(b ≥1
2)
N(2ab, 1 −b, 2bc, 2bd)
(0 ≤b < 1
2)
N(2ab, 1 + b, 2bc, 2bd)
(−1
2 ≤b < 0)
(−a, −b, −c, −d)
(b < −1
2)
f3(a, b, c, d) =







(a, b, c, d)
(c ≥1
2)
N(2ac, 2bc, 1 −c, 2cd)
(0 ≤c < 1
2)
N(2ac, 2bc, 1 + c, 2cd)
(−1
2 ≤c < 0)
(−a, −b, −c, −d)
(c < −1
2)
f4(a, b, c, d) =







(a, b, c, d)
(d ≥1
2)
N(2ad, 2bd, 2cd, 1 −d)
(0 ≤d < 1
2)
N(2ad, 2bd, 2cd, 1 + d)
(−1
2 ≤d < 0)
(−a, −b, −c, −d)
(d < −1
2)
Check that these functions are indeed continuous at case boundaries and that fi(−q) = fi(q). For a
unit quaternion (a, b, c, d), a2 + b2 + c2 + d2 = 1, so max{a2, b2, c2, d2} ≥1
4. If a2, b2, c2 or d2 is
at least 1
4, then f1, f2, f3 or f4 respectively will give the correct output.
Theorem 7. Let U be any neighborhood of Rxyz(0, π
2 , 0) in SO(3). There exists no continuous
function f : U →R3 such that Rxyz(f(R)) = R for every R ∈U.
Proof. Assume that such a function f exists. It can be checked that Qxyz(θ, φ, θ) · Qxyz(0, π
2 , 0) =
cos( π
4 −φ
2 ), so for 0 ≤φ ≤π, d(Rxyz(θ, φ, θ), Rxyz(0, π
2 , 0)) = π
2 −φ. So there exists ϵ > 0 such
that Rxyz(θ, φ, θ) ∈U for all θ ∈R and π
2 −ϵ < φ < π
2 .
If Rxyz(α, β, γ) = Rxyz(α′, β′, γ′), but β ̸= kπ + π
2 for any k ∈Z, we must have (α′, β′, γ′) =
(α + 2mπ, β + 2nπ, γ + 2kπ) or (α′, β′, γ′) = (α + (2m + 1)π, −β + (2n + 1)π, γ + (2k + 1)π)
for some m, n, k ∈Z. In any case, let
g(θ, φ) = f (1)(Rxyz(θ, φ, θ)) −θ
π
(29)
where f (1) is the ﬁrst coordinate of f. Then for θ ∈R and π
2 −ϵ < φ <
π
2 , g is continuous
and integer-valued, so it must be constant. But since Rxyz(0, φ, 0) = Rxyz(2π, φ, 2π), we have
g(0, φ) −g(2π, φ) = 2, which is a contradiction.
Theorem 8. There exists continuous functions f1, f2, f3, f4 : SO(3) →R3 such that for any rotation
R ∈SO(3), at least one of the following is equal to R: Rxyz(f1(R)), Rxyz(f2(R)), Rxzy(f3(R))
and Rxzy(f4(R)).
Proof. Similar to theorem 4 we distinguish between two versions of atan2 with different choices of
principal values. Let atan2◁be the version with principal value in (−π, π] and atan2▷be the version
with principal value in [0, 2π).
Let T be a parametrized function, deﬁned as follows:
T (a1, a2, a3, a4)(x) =















0
(x ≤a1)
x−a1
a2−a1
(a1 < x ≤a2)
1
(a2 < x ≤a3)
a4−x
a4−a3
(a3 < x ≤a4)
0
(a4 < x)
(30)
14

That is, T (a1, a2, a3, a4) is a piecewise linear function deﬁned by connecting the points (a1, 0),
(a2, 1), (a3, 1), (a4, 0) in [a1, a4] and constantly 0 beyond that range. Deﬁne these instances of T :
Tα◁= T (−2π
3 , −π
2 , π
2 , 2π
3 )
(31)
Tα▷= T (π
3 , π
2 , 3π
2 , 5π
3 )
Tβ = T (−π
3 , −π
4 , π
4 , π
3 )
Tγ◁= T (−5π
6 , −3π
4 , 3π
4 , 5π
6 )
Tγ▷= T (π
6 , π
4 , 7π
4 , 11π
6 )
Let M ∈SO(3) be a rotation matrix. Deﬁne these functions:
α1(M) =
atan2◁(M32, M33)
(M31 ̸= ±1)
π
(M31 = ±1)
(32)
α2(M) =
atan2▷(M32, M33)
(M31 ̸= ±1)
0
(M31 = ±1)
β1(M) = −sin−1 M31
γ1(M) =
atan2◁(M21, M11)
(M31 ̸= ±1)
π
(M31 = ±1)
t1(M) = Tα◁(α1(M)) · Tβ(β1(M)) · Tγ◁(γ1(M))
t2(M) = Tα▷(α2(M)) · Tβ(β1(M)) · Tγ◁(γ1(M))
f1(M) = (α1(M), β1(M), γ1(M)) · t1(M)
f2(M) = (α2(M), β1(M), γ1(M)) · t2(M)
α3(M) =
atan2◁(−M23, M22)
(M21 ̸= ±1)
π
(M21 = ±1)
α4(M) =
atan2▷(−M23, M22)
(M21 ̸= ±1)
0
(M21 = ±1)
β3(M) = sin−1 M21
γ3(M) =
atan2▷(M31, M11)
(M21 ̸= ±1)
0
(M21 = ±1)
t3(M) = Tα◁(α3(M)) · Tβ(β3(M)) · Tγ▷(γ3(M))
t4(M) = Tα▷(α4(M)) · Tβ(β3(M)) · Tγ▷(γ3(M))
f3(M) = (α3(M), β3(M), γ3(M)) · t3(M)
f4(M) = (α4(M), β3(M), γ3(M)) · t4(M)
We prove that f1, f2, f3 and f4 deﬁned as such meet the requirements in the theorem. Consider the
continuity of f1. We divide SO(3) into overlapping regions according to the range of α, β and γ in
their Euler angle representation. Let
M = Rxyz(α, β, γ) =
"cβcγ
−cαsγ + sαsβcγ
sαsγ + cαsβcγ
cβsγ
cαcγ + sαsβsγ
−sαcγ + cαsβsγ
−sβ
sαcβ
cαcβ
#
(33)
Where sθ = sin θ and cθ = cos θ.
Case 1: α ∈(−π, π), β ∈(−π
2 , π
2 ), γ ∈(−π, π). M31 ̸= ±1 and the discontinuities of
atan2◁(M32, M33) and atan2◁(M21, M11) are avoided, so α1, β1 and γ1 are continuous. T is
always continuous, so t1 and thus f1 are continuous.
Case 2: α ∈(−4π
3 , −2π
3 ) ∪( 2π
3 , 4π
3 ). Tα◁(α1(M)) = 0, so f1(M) = (0, 0, 0) is constant and
continuous.
15

Table 4: Deciding which function would be correct for M
Let m = max{M11, −M11, M21, −M21, M31, −M31}
m = −M21, M11, M21
m = −M31, −M11, M31
β1(M) ∈[−π
4 , π
4 ]
β3(M) ∈[−π
4 , π
4 ]
γ1(M) ∈[−3π
4 , −π
4 ], [−π
4 , π
4 ], [ π
4 , 3π
4 ] resp.
γ3(M) ∈[ π
4 , 3π
4 ], [ 3π
4 , 5π
4 ], [ 5π
4 , 7π
4 ] resp.
M33 ≥0
M33 ≤0
M22 ≥0
M22 ≤0
α1(M) ∈[−π
2 , π
2 ]
α2(M) ∈[ π
2 , 3π
2 ]
α3(M) ∈[−π
2 , π
2 ]
α4(M) ∈[ π
2 , 3π
2 ]
Rxyz(f1(M)) = M
Rxyz(f2(M)) = M
Rxzy(f3(M)) = M Rxzy(f4(M)) = M
Case 3: β ∈(−2π
3 , −π
3 ) ∪( π
3 , 2π
3 ). Tβ(β1(M)) = 0, so f1(M) = (0, 0, 0) is constant and
continuous.
Case 4: γ ∈(−7π
6 , −5π
6 ) ∪( 5π
6 , 7π
6 ). Tγ◁(γ1(M)) = 0, so f1(M) = (0, 0, 0) is constant and
continuous.
These four cases collectively cover the entire SO(3), so f1 is continuous in SO(3). Additionally,
when α ∈[−π
2 , π
2 ], β ∈[−π
4 , π
4 ] and γ ∈[−3π
4 , 3π
4 ], α1(M) = α, β1(M) = β, γ1(M) = γ and
Tα◁(α1(M)) = Tβ(β1(M)) = Tγ◁(γ1(M)) = 1, so f1(M) = (α, β, γ).
Similarly, we can prove that f2, f3 and f4 are continuous, and each have a speciﬁc region in SO(3)
where they give the correct Euler angles. Several things in the proof above need to be changed for
each function accordingly:
For f2 and f4, the range of α in case 1 is (0, 2π), the range of α in case 4 is (−π
3 , π
3 ) ∪( 5π
3 , 7π
3 ) and
the range of α in the correct range is [ π
2 , 3π
2 ].
For f3 and f4, divide SO(3) by angle range of Rxzy instead of Rxyz:
M = Rxzy(α, β, γ) =
"cβcγ
−cαsβcγ −sαsγ
sαsβcγ −cαsγ
sβ
cαcβ
−sαcβ
cβsγ
−cαsβsγ + sαcγ
sαsβsγ + cαcγ
#
(34)
the range of γ in case 1 is (0, 2π), the range of γ in case 4 is (−π
6 , π
6 ) ∪( 11π
6 , 13π
6 ), and the range of
γ in the correct range is [ π
4 , 7π
4 ].
Now we need to prove that any rotation in SO(3) falls within the correct range of at least one of f1,
f2, f3 and f4. For M ∈SO(3), let m = max{M11, −M11, M21, −M21, M31, −M31}.
If m = M11, then 2M 2
31 ≤M 2
11 + M 2
31 ≤M 2
11 + M 2
21 + M 2
31 = 1, so −
√
2
2
≤M31 ≤
√
2
2 , so
−π
4 ≤β1(M) ≤π
4 . −M11 ≤M21 ≤M11, so −π
4 ≤γ1(M) ≤π
4 .
If additionally M33 ≥0, then −π
2 ≤α1(M) ≤π
2 so M is in the correct range of f1. Otherwise
π
2 ≤α2(M) ≤3π
2 so M is in the correct range of f2.
Other cases can be analyzed similarly. We summarize the result in table 4. If M satisﬁes the
conditions for multiple functions, then it falls in the correct range of each of them.
In conclusion, f1, f2, f3 and f4 are continuous functions from SO(3) to R3 and for any rotation
R ∈SO(3), at least one of Rxyz(f1(R)), Rxyz(f2(R)), Rxzy(f3(R)) and Rxzy(f4(R)) is equal to
R.
Theorem 9. Let G be a nontrivial ﬁnite subgroup of SO(3), then there does not exist continuous
function f : SO(3)/G →SO(3) such that for every R ∈SO(3)/G, pG(f(R)) = R.
Remark. In addition to showing that these functions do not exist, we also want to know if for a
continuous function there is a provable maximum error that must be achieved on some input. Such
error bounds will be speciﬁc to each possible choice of G, so we ﬁrst list these possibilities. There
are two inﬁnite series of ﬁnite subgroups of SO(3), plus three isolated ones. We list them, with their
Schoenﬂies notations:
16

• Cn, the cyclic groups, the rotational symmetry group of a right pyramid with a regular
n-sided base.
• Dn, the dihedral groups, the rotational symmetry group of a right prism with a regular
n-sided base.
• T, the chiral tetrahedral group, the rotational symmetry group of a regular tetrahedron.
• O, the chiral octahedral group, the rotational symmetry group of a cube or a regular
octahedron.
• I, the chiral icosahedral group, the rotational symmetry group of a regular dodecahedron or
icosahedron.
These are all different except for C2 = D1. Our derivation is valid for the cases where the group
contains an element of order 2 (a 180◦rotation), that is, all cases except for Cn where n is odd.
Assume that G is one of the applicable groups. Let f be any continuous function from SO(3)/G to
SO(3). Now pick one of the two preimages of f(pG(I3×3)) under RQ as s. Deﬁne bf : S3 →S3 by
the following method:
For any r ∈S3, let h be a path from 1 to r, that is, h is a continuous function from [0, 1] to
S3 such that h(0) = 1 and h(1) = r. Then f ◦pG ◦RQ ◦h is a path in SO(3) starting from
f(pG(I3×3)) = RQ(s). By the lifting property of covering spaces, f ◦pG ◦RQ ◦h lifts to a unique
path in S3 starting from s. Let g be that path. We prove that the value of g(1) is independent of the
choice of h:
for any two paths h1 and h2 from 1 to r, suppose that f ◦pG ◦RQ ◦h1 lifts to g1 from s and
f ◦pG ◦RQ ◦h2 lifts to g2 from s. Since S3 is simply connected, there exists a homotopy of
paths from h1 to h2. That is, there exists a continuous function H : [0, 1] × [0, 1] →S3 such that
H(t, 0) = h1(t) and H(t, 1) = h2(t) for all 0 ≤t ≤1 and H(0, u) = 1 and H(1, u) = r for all
0 ≤u ≤1. Now let N = f ◦pG ◦RQ ◦H, then N : [0, 1] × [0, 1] →SO(3) is a homotopy of paths
in SO(3), N(·, 0) = f ◦pG ◦RQ ◦h1 and N(·, 1) = f ◦pG ◦RQ ◦h2.
RQ ◦g1 = N(·, 0). By the lifting property of covering spaces, N lifts to a unique homotopy b
N
in S3 such that b
N(·, 0) = g1. Since RQ( b
N(0, u)) = RQ(s), for any u we must have b
N(0, u) = s
or b
N(0, u) = −s.
b
N(0, u) is also continuous in u, so it must be constant. So we must have
b
N(0, 1) = b
N(0, 0) = s. Likewise we must have b
N(1, 1) = b
N(1, 0).
b
N(·, 1) is a lift of N(·, 1) starting from b
N(0, 1) = s. Since g2 is the unique such lift, we have
b
N(·, 1) = g2. So, g2(1) = b
N(1, 1) = b
N(1, 0) = g1(1).
Now we have proven that the value of g(1) is independent of the choice of h. So we can deﬁne
bf(r) = g(1). bf(r) is a preimage of f(pG(RQ(r))). By the construction of bf, we can see that if h is
a path from p to q, then the lift of f ◦pG ◦RQ ◦h starting from bf(p) will end at bf(q).
Let bG = R−1
Q [G] be the binary polyhedral group corresponding to G. Since G contains a 180◦
rotation, bG must contain a pure vector. Let q0 = xi + yj + zk ∈bG be such a quaternion. Deﬁne
q(t) = cos π
2 t + sin π
2 t · q0. Now for any p ∈S3, let r◁(t) = pq(t) and r▷(t) = pq0q(t), then
r◁(t) is a path from p to pq0, r▷(t) is a path from pq0 to pq2
0 = −p and r▷(t) = r◁(t)q0
Note that pG(RQ(p)) = pG(RQ(pq)) if q ∈bG. So, pG ◦RQ ◦r◁and pG ◦RQ ◦r▷are the same
path in SO(3)/G and they are a loop. Then, v = f ◦pG ◦RQ ◦r◁= f ◦pG ◦RQ ◦r▷is a loop in
SO(3). Consider the lift of v in S3. Let u = bf(p), then the lift of v in S3 must start and end at either
u or −u. If the lifted path starting from u also ends at u, then the lifted path starting from −u must
end at −u, for otherwise the reverse of v lifts to two paths path from u to two different destinations
u and −u which is impossible. Likewise, if the lifted path starting from u ends at −u, then the lifted
path starting from −u must end at u.
Now join r◁and r▷as a long path r: r(t) = q(2t). r is a path from p to −p. The ﬁrst half of r
coincides with r◁and the second half of r coincides with r▷. Consider the lift of f ◦pG ◦RQ ◦r. It
can be obtained by joining two lifts of v, lets call bv◁and bv▷. If bv◁goes from u to −u, then bv▷goes
17

from −u to u. If bv◁loops from u back to u, then bv◁does the same. In any case, the whole lifted
path is a loop from u back to u.
The start of this path is u = bf(p). By the construction of bf, the end point equals bf(−p). So for any
p ∈S3, bf(p) = bf(−p).
Recall the deﬁnition of distance from a rotation S to an element R of SO(3)/G. We have
dG(f(pG(RQ(p))), pG(RQ(p))) = dG(p, bf(p)) = min
q∈b
G
2dQ(p, bf(p)q)
(35)
Let us now select 3 quaternions q1, q2 and q3 from bG. The method of selection will be considered
later. Let li(p) = p · bf(p) −p · ( bf(p)qi), then
li(−p) = (−p) · bf(−p) −(−p)(· bf(−p)qi)
= −p · bf(p) + p · ( bf(p)qi)
= −li(p)
(36)
Let L(p) = (l1(p), l2(p), l3(p)). Similar to theorem 5, by the Borsuk–Ulam theorem, there exists
p0 such that L(p0) = 0. Then we have
p0· bf(p0)−p0·( bf(p0)q1) = p0· bf(p0)−p0·( bf(p0)q2) = p0· bf(p0)−p0·( bf(p0)q3) = 0 (37)
a · (bc) = Re(a(bc)) = Re((ab)c) = (ba) · c for any quaternions a, b and c, so equivalently
( bf(p0)p0) · 1 = ( bf(p0)p0) · q1 = ( bf(p0)p0) · q2 = ( bf(p0)p0) · q3
(38)
In non-degenerate cases, this will give us three independent linear equations in the four components
of bf(p0)p0. Together with || bf(p0)p0|| = 1, bf(p0)p0 can be uniquely determined up to negation.
From this, we can compute
dG(p0, bf(p0)) = min
q∈b
G
2dQ(p0, bf(p0)q)
= min
q∈b
G
2 cos−1(p0 · ( bf(p0)q))
= min
q∈b
G
2 cos−1(( bf(p0)p0) · q)
(39)
which gives us a lower bound for maxp dG(p, bf(p)), i.e. the lower bound for the maximum error
that must occur. It turns out that the highest lower bound so obtained is exactly the largest possible
value of dG(p, q) and so must be the best possible lower bound. The choice of qi’s that achieves this
is such that bf(p0)p0 as solved from equation 37 achieves the largest value.
We list all choices of G, along with the elements of bG, one of many best choices of qi’s and the
corresponding bf(p0)p0 and dG(p0, bf(p0)) in table 5. In the table, Perm() means all permutations
of the number sequence, excluding duplicate sequences, EvenPerm() is the same but with only even
permutations. Cn is a degenerate case where one of the qi is redundant and the solution is not unique,
but they all give the same bound. Our derivation does not work for Cn when n is odd, but we believe
that the conclusion is the same as for Cn where n is even, namely, that the lower bound of highest
error is π.
As a sanity check, now take the formula for Dn and plug in n = 2, we get
min
f
max
p
dD2(p, bf(p)) = cos−1(−sin2 π
4 ) = cos−1(−1
2) = 2π
3
(40)
which is indeed what the result in the second part of section 3.2 suggested.
Proposition 11. Let G be a ﬁnite subgroup of SO(3), then there exists continuous functions
f1, f2, f3, f4 : SO(3)/G →SO(3) such that for every R ∈SO(3)/G, pG(fi(R)) = R for
some i ∈{1, 2, 3, 4}.
18

Table 5: Table for computing error bounds of f : SO(3)/G →SO(3)
G
|G|
Elements of bG
Choice of qi’s
bf(p0)p0
dG(p0, bf(p0))
Cn
n
(cos kπ
n , 0, 0, sin kπ
n ), 0 ≤k < 2n
(−1, 0, 0, 0)
(cos π
n, 0, 0, sin π
n)
(0, cos θ, sin θ, 0), 0≤θ<2π
π
Dn
2n
(cos kπ
n , 0, 0, sin kπ
n ), 0 ≤k < 2n
(0, cos kπ
n , sin kπ
n , 0), 0 ≤k < 2n
(cos π
n, 0, 0, sin π
n)
(0, 1, 0, 0)
(0, cos π
n, sin π
n, 0)
√
2
2 (cos π
2n, cos π
2n, sin π
2n, sin π
2n)
cos−1(−sin2
π
2n)
T
12
Perm(±1, 0, 0, 0)
(± 1
2, ± 1
2, ± 1
2, ± 1
2)
( 1
2, 1
2, 1
2, 1
2)
( 1
2, 1
2, 1
2, −1
2)
( 1
2, 1
2, −1
2, 1
2)
(
√
2
2 ,
√
2
2 , 0, 0)
π
2
O
24
Perm(±1, 0, 0, 0)
(± 1
2, ± 1
2, ± 1
2, ± 1
2)
Perm(±
√
2
2 , ±
√
2
2 , 0, 0)
(
√
2
2 ,
√
2
2 , 0, 0)
(
√
2
2 , 0,
√
2
2 , 0)
( 1
2, 1
2, 1
2, 1
2)
( 2+
√
2
4
,
√
2
4 ,
√
2
4 , 2−
√
2
4
)
cos−1 2
√
2−1
4
I
60
Perm(±1, 0, 0, 0)
(± 1
2, ± 1
2, ± 1
2, ± 1
2)
EvenPerm(0, ±
√
5+1
4
, ± 1
2, ±
√
5−1
4
)
(
√
5+1
4
, 1
2, 0,
√
5−1
4
)
(
√
5+1
4
,
√
5−1
4
, 1
2, 0)
(
√
5+1
4
, 0,
√
5−1
4
, 1
2)
√
10−
√
2
8
(
√
5 + 2, 1, 1, 1)
cos−1 3
√
5−1
8
Remark. We have seen in the proof of both theorem 1 and theorem 9 that the key technique is to ﬁnd
a loop in the base space that lifts to a non-loop in the covering space. The existence of such a loop is
because the base space is not simply connected. If we instead only require f : SO(3)/G →SO(3)
to satisfy pG(f(R)) = R on a contractible open subset U of SO(3)/G then such f could exist: a
contractible open subset is always evenly covered, which means the preimage of U under pG is the
disjoint union of open subsets of SO(3) each of which is homeomorphic to U under pG. We can
select any one of these subsets V and deﬁne f to be the inverse of pG : V →U on U, and assign
values of f on SO(3)/G \ U so that f is continuous on SO(3)/G.
The way to achieve this is exempliﬁed in the proof of theorem 8: expand U a bit to leave some margin.
Beyond the margin, let f be a constant function so that global structures of SO(3)/G will not affect
the continuity of f outside this local patch. In theorem 8, this is done by setting f to (0, 0, 0) near
gimbal locked positions and discontinuities of atan2. In the margin, let f continuously change from
being correct to being constant. In theorem 8 this is done by blending (αi(M), βi(M), γi(M)) and
(0, 0, 0) using ti(M).
Now if we can cover SO(3)/G with ﬁnitely many such open contractible subsets, then we can
construct an f for each of them and these f’s will make a successful ensemble. For a topological
space X, the smallest number k such that there exists an open cover {Ui|1 ≤i ≤k} of X with each
Ui contractible is called the Lusternik–Schnirelmann category of X, denoted cat(X). It is proved in
[7] that for a smooth compact manifold X, cat(X) ≤dim(X) + 1. SO(3)/G is a 3-dimensional
smooth compact manifold, so it can be covered by four contractible open subsets.
19

C
Details of Experiments
C.1
Initialization of Last Layer
In all networks, in both experiments and including both the conversion functions and the classiﬁers,
the weight of the last layer is initialized to zero. For the classiﬁers, the bias of the last layer is
initialized to 1
n for all output neurons where n is the size of the ensemble. For the conversion
functions, the bias of the last layer is initialized to the representation of a randomly sampled rotation.
For the quaternion, 5D and 6D representations, the rotation is uniformly sampled from SO(3). For
the Euler angle, we want to avoid gimbal lock in the initial bias, so instead of uniform sampling from
SO(3), we uniformly sample α and γ from [−π, π] and β from [−π
4 , π
4 ].
C.2
Penalizing Bad Representations
We introduce a penalty that helps prevent bad representations. Although our theory considers output
spaces of various forms, in practice the output of the neural network is in a Euclidean space and has
to be normalized in some way to produce a valid rotation representation, for example, for quaternion
the output must be normalized to have length 1. This can be problematic if the unnormalized output
is close to zero, since zero is a singularity and a small change in the unnormalized output near zero
can result in a big change in the normalized output. So to improve the stability, we bound the output
of the network away from singularities by adding a penalty on representations too far away from a
valid rotation representation.
For quaternions, the penalty is Lp(p) = (ln ||p||)2. For a 6D representation x = (x1, x2, . . . , x6),
let
u = (x1, x2, x3)
v = (x4, x5, x6)
Lp(x) = (ln ||u||)2 + (ln ||v||)2 + (u · v)2
(41)
For a 5D representation, ﬁrst convert it to 6D by stereographic projection then apply the penalty for a
6D representation. The conversion from Euler angles to rotations is smooth, so no penalty is applied.
The 3D matrix conversion problem is simple enough that this penalty is found to be unnecessary.
For the point cloud rotation estimation and 4D rotation matrix conversion that we will introduce in
section D the penalty is added to the loss function with an appropriate weight.
C.3
Additional Results of Rotation Matrix Conversion
Here we compare quaternion and Euler angle ensembles of different sizes to show that an ensemble
of four is necessary. Figure 4 and table 6 compares the conversion error with a single quaternion
and quaternion ensembles of size 2, 3 and 4. We can see that by introducing a second and a third
network into the ensemble the conversion is reduced overall, but this does not prevent the occurrence
of error close to 180◦. In contrast, introducing a fourth network causes qualitative changes in that the
maximum error is lowered drastically to only about 0.1◦.
Figure 5 and table 7 compares the conversion error with a single set of Euler angles, uniform x-y-z
Euler angle ensembles of size 2, 3 and 4, and the mixed Euler angle ensemble with two set each of
x-y-z and x-z-y Euler angles. Qualitatively we can observe the same behavior in that ensembles of
size up to three give a maximum error of 180◦while the maximum error for an ensemble of four
is much lower. The mixed ensemble further improves upon the uniform ensemble of four but this
improvement is only quantitative.
Although qualitatively similar, overall Euler angles compare poorly to quaternions. We think that this
is due to RQ being a local isometry. In a sense, the same amount of change anywhere in the input
space will also cause the same amount of change in the output space, which is a favorable condition
for the network to ﬁt such a function. The same is true for the 6D embedding. Indeed, as seen in
ﬁgure 1, the error curves of the 6D embedding and the ensemble of four quaternions almost match
perfectly. In contrast, the Euler angle does not have this property. Near the gimbal locked positions,
small changes in the input can cause huge changes in the output. The 5D embedding does not have
this property either, as the stereographic projection does not preserve distance.
20

0%
20%
40%
60%
80%
100%
0.001 ∘
0.01 ∘
0.1 ∘
1 ∘
10 ∘
100 ∘
180 ∘
Quaternion
Quaternion,∘ensemble∘of∘2
Quaternion,∘ensemble∘of∘3
Quaternion,∘ensemble∘of∘4
Figure 4: Error of rotation matrix conversion by per-
centile, using different quaternion ensembles
Table 6: Error statistics
• Type
Mean(◦)
Max(◦)
• Quat.
0.3323 179.9995
• Quat. ×2
0.0443 179.7149
• Quat. ×3
0.0243 178.6680
• Quat. ×4
0.0226
0.1059
0%
20%
40%
60%
80%
100%
0.001 ∘
0.01 ∘
0.1 ∘
1 ∘
10 ∘
100 ∘
180 ∘
Euler∘angle
Euler∘angle,∘ensemble∘of∘2
Euler∘angle,∘ensemble∘of∘3
Euler∘angle,∘ensemble∘of∘4
Euler∘angle,∘mixed∘ensemble
Figure 5: Error of rotation matrix conversion by per-
centile, using different Euler angle ensembles
Table 7: Error statistics
• Type
Mean(◦)
Max(◦)
• Euler
0.7368 179.9991
• Euler ×2
0.4898 179.9981
• Euler ×3
0.3578 179.9979
• Euler ×4
0.2990
8.5356
• Euler mix
0.2278
3.6693
C.4
Setup of Point Cloud Rotation Estimation
Our network is a modiﬁed PointNet [5]. In the full PointNet there are two feature transforms on local
features before max-pooling is applied to obtain global features. The transforms takes the form of a
matrix multiplication, and the matrices are computed from a T-Net, which is a simpliﬁed PointNet,
thus resulting in a nested structure. We consider this to be too complex for our simple problem, but
we also think that this causes some global information to be combined into local features before the
max pooling, which might be important. So instead of a complete removal of these nested networks,
we replace it with a simpler way for incorporating global information: We perform max pooling,
multiply the pooled feature with a learned matrix and apply ReLU, then concatenate the resulting
feature vector to the local feature of each point. This doubles the number of features at each point, so
the size of the input layer of the subsequent MLP in PointNet is modiﬁed accordingly.
We stress however that our discussion is focused on the topological relationship between the input
space and the output space, rather than concrete aspects of implementation, including the choice
of network architecture. As long as the network is invariant under input point permutation and has
appropriate capacity for our example problem, the same qualitative result should be expected.
As mentioned in section 3.2, in each part we only use a single point cloud, for both training and
testing. Normally to demonstrate the effectiveness of a machine learning algorithm, the model is
21

trained on a training dataset of suitable size and tested on a separate test dataset, and our setup seems
rather unconventional. But note that our purpose is not to demonstrate the effectiveness of a machine
learning algorithm. Rather, we aim to examine a property of a machine learning problem itself. We
want to show that the unavoidable large error is a result of the topological structure of the problem.
If the conventional setting is used and a large maximum arises in the test, the source of error can
be hard to explain. It might be because the network failed to generalize. It might be because the
network does not have enough capacity to give low error on every sample from the dataset. It might
be because the topological property of the rotation representation guarantees that large errors must
occur. The last one is what we want to show. The best way to prove that a large error is indeed an
inherent property of the representation is to avoid introducing any possible error due to the ﬁrst two
reasons at all. In fact, for a moderately difﬁcult problem, it is almost never possible to train a neural
network that generalizes perfectly. Since the existence or nonexistence of large error is the main
differentiator, and since even a single instance of generalization failure can result in networks using
different representations giving the same largest error, it is almost ensured that we will not be able to
draw any useful conclusions if a conventional training/test split is used.
Consider a negative example. In the same point cloud rotation estimation experiment in [11], a large
dataset of thousands of point clouds was used, with a training/test split. Theoretically, since their
5D and 6D representations are continuous and their dataset of point clouds of aeroplanes can be
safely assumed to possess no rotational symmetry, a sufﬁciently good network should always give
small error, while for the quaternion and Euler angle representation large errors are guaranteed to
occur. But as shown in ﬁgure 5f in [11], their experiment failed to reﬂect this important qualitative
difference between continuous and non-continuous representations since every representation gave
a largest error close to 180◦, which presumably was due to generalization error. We need to avoid
this. Thus, we adopted the setting that only the error on the training set should be considered, and the
training set should comprise only one point cloud, with its rotation being the only variable quantity.
In short, certain rotation representations are bad precisely because they are guaranteed to produce a
large error, even on a training dataset with only one sample.
We then introduce how we constructed our point cloud data with the desired symmetry. The basis of
our point cloud is a down-sampled version of the Stanford bunny. We ﬁrst normalize the model by
scaling and translation so that the bounding sphere of its axis-aligned bounding box is the unit sphere.
Then a 3D model consisting of four Stanford bunnies arranged to have D2 symmetry is constructed
by taking four copies of the base model, transformed by the following four matrices respectively:
"1
0
0
0.5
0
1
0
0.5
0
0
1
0.5
# "1
0
0
0.5
0
−1
0
−0.5
0
0
−1
−0.5
# "−1
0
0
−0.5
0
1
0
0.5
0
0
−1
−0.5
# "−1
0
0
−0.5
0
−1
0
−0.5
0
0
1
0.5
#
(42)
Our point cloud with D2 symmetry is the set of vertices of the resulting 3D model.
The bunny itself has no rotational symmetry. But we desired a point cloud with no symmetry that is
otherwise similar to the point cloud with D2 symmetry, so we took four copies the base model and
translated them by (0.5, 0.5, 0.5), (0.5, −0.5, −0.5), (−0.5, 0.5, −0.5) and (−0.5, −0.5, 0.5) and
took the vertices to form our point cloud with no symmetry.
The resulting 3D models are shown in ﬁgure 6. Since point clouds of scanned symmetric real-world
objects typically do not have perfect symmetry, we simulate this effect by adding Gaussian noise to
point coordinates. The displacement of each point is independent, and generated independently on
the ﬂy for each training sample.
We train each network using Adam with learning rate 10−4 and batch size 8 for 500, 000 iterations.
For training and testing, we sample uniformly from SO(3). We sample 3 million random rotations for
testing. To improve stability, the Gaussian noise is introduced gradually. In the ﬁrst 100k iterations,
We do not add any noise. Then for the next 100k iterations, the standard deviation of the Gaussian
noise increases linearly from 0 to 0.01. Then for the rest of the training it stays at 0.01. The
representation penalty is imposed with a weight of 1 for the ﬁrst 300k iterations. Then for the next
100k iterations the weight linearly decreases to 0. For the rest of the training the penalty is removed.
The loss function is the same as equation 8 but with d() replaced with dG(). Lmax may also be altered
to take the maximum value of dG() but the difference is minor as long as it is no smaller than the
actual maximum possible error so for simplicity we keep it at π.
22

(a) With no symmetry
(b) With D2 symmetry
Figure 6: 3D models of quadruple Stanford bunnies with different symmetries
D
Extension to 4D Rotations
In this section we extend some of our results to the quaternion pair representation of 4D rotations.
D.1
Theoretical Results
We ﬁrst prove the error bound of conversion from 4D rotation matrices to quaternion pairs. We jump
straight to an ensemble of three functions, since it implies the same result for a single function.
Theorem 12. For any three continuous functions f1, f2, f3 : SO(4) →S3 × S3, there exists a
rotation R ∈SO(4) such that d4(R, fi(R)) ≥π for all i ∈{1, 2, 3}.
Proof. Deﬁne the dot product between pairs of quaternions as (p, q) · (r, s) = p · q + r · s. Consider
functions vi : S3 →R deﬁned by vi(q) = (q, q) · fi(RQQ(q, q)), for i = 1, 2, 3. For any q ∈S3,
Since RQQ(−q, −q) = RQQ(q, q),
vi(−q) = (−q, −q) · fi(RQQ(−q, −q)) = −(q, q) · fi(RQQ(q, q)) = −vi(q)
(43)
Let V : S3 →R3 deﬁned by V (q) = (v1(q), v2(q), v3(q)), then V (−q) = −V (q) for any
q ∈S3. By the Borsuk–Ulam theorem, there exists q0 ∈S3 such that V (−q0) = V (q0), then
−V (q0) = V (−q0) = V (q0), so V (q0) = 0, which means v1(q0) = v2(q0) = v3(q0) = 0.
Assume that fi(RQQ(q0, q0)) = (ri, si). Then (q0, q0) · (ri, si) = 0 so q0 · ri = −q0 · si. So,
dQ(q0, ri) = cos−1(q0 · ri)
= cos−1(−q0 · si)
= π −cos−1(q0 · si)
= π −dQ(q0, si)
(44)
So,
d4((q0, q0), (ri, si))
= min{dQ(q0, ri) + dQ(q0, si), 2π −dQ(q0, ri) −dQ(q0, si)}
+ |dQ(q0, ri) −dQ(q0, si)|
≥min{π, 2π −π}
=π
(45)
So, RQQ(q0, q0) is a rotation such that d4(RQQ(q0, q0), fi(RQQ(q0, q0))) ≥π for all i ∈
{1, 2, 3}.
23

In 3D, our lower bound of maximum error, π, is also the maximum value of d(p, q), so it must be
optimal. Here the maximum value of d4((p, q), (r, s)) is 2π but we only proved a lower bound of π.
We show however that in this case the lower bound is also optimal.
Theorem 13. There exists a continuous function f : SO(4) →S3 × S3 such that for any rotation
R ∈SO(4), d4(R, f(R)) ≤π.
Proof. We give an example of such a function. For clarity we deﬁne f on the quaternion representa-
tion. If we ensure that f(p, q) = f(−p, −q) for all p, q ∈S3, f will be a well deﬁned function of
rotations.
Let f(p, q) = (1, pq), then for any p, q ∈S3, f(−p, −q) = (1, (−p)(−q)) = (1, pq) = f(p, q),
and
d4((p, q), f(p, q))
=d4((p, q), (1, pq))
= min{dQ(p, 1) + dQ(q, pq), 2π −dQ(p, 1) −dQ(q, pq))} + |dQ(p, 1) −dQ(q, pq))|
≤π + | cos−1(Re(p)) −cos−1(Re(p))|
=π
(46)
Similar to the 3D case, we can construct a successful ensemble of four functions. In fact, the
construction for 4D directly uses the construction for 3D.
Theorem 14. There exists continuous functions g1, g2, g3, g4 : SO(4) →S3 × S3 such that for any
rotation R ∈SO(4), RQQ(gi(R)) = R for some i ∈{1, 2, 3, 4}.
Proof. We give an example of such a set of functions. For clarity, we deﬁne them as functions of
quaternions. If we ensure that gi(p, q) = gi(−p, −q) for all p, q ∈S3 and i ∈{1, 2, 3, 4}, they
will be well-deﬁned functions of rotations. Use the deﬁnition of fi from the proof of theorem 6. Let
gi(p, q) = (fi(p), fi(p)pq)
(47)
then
gi(−p, −q) = (fi(−p), fi(−p)(−p)(−q)) = (fi(p), fi(p)pq)) = gi(p, q)
(48)
By theorem 6, for all p ∈S3 there exists k ∈{1, 2, 3, 4} such that RQ(p) = RQ(fk(p)), which
means fk(p) = p or p = −p. Now for any p, q ∈S3 ﬁnd the k such that RQ(p) = RQ(fk(p)). If
fk(p) = p, then gk(p, q) = (p, ppq) = (p, q). If fk(p) = −p, then gk(p, q) = (−p, −ppq) =
(−p, −q). In either case, RQQ(p, q) = RQQ(gk(p, q)).
D.2
Experiments
We test the accuracy of converting 4D rotation matrices to pairs of quaternions using ensembles of
different sizes. The size of the MLP used for this experiment is increased to 8 hidden layers of size
256 each. training time, batch size and learning rate remain he same as section 3.1. The result is
shown in ﬁgure 7 and table 7.
We found that, in terms of eliminating large errors, the conversion of 4D rotation matrices is
considerably harder than 3D rotation matrices. In particular, the existence or nonexistence of a
large maximum error is the main differentiator between ensembles of three or fewer networks and
ensembles with four or more networks and we aim to show this difference, but stochastic gradient
descent aims at reducing average loss with does not necessarily result in a low maximum error. Thus
we modiﬁed the training procedure to put more emphasis on inputs with large error: in each iteration,
all input samples in the training batch were sorted by their error. Half of the current batch with the
largest errors were retained in the training batch of the next iteration, while the other half of the next
training batch was randomly sampled.
Even with this modiﬁcation, we were not able to reliably train an ensemble of four networks that
result in a low maximum error. So we additionally tested with an ensemble of ﬁve networks, which
when combined with this training strategy proved much easier to train than an ensemble of four. We
24

think that this is likely due to the “correct region” of some or all networks in the ensemble of four
being noncontractible. Note that there is a difference between the construction in the proof of theorem
14 compared to those in theorems 4, 6 and 8: in the earlier theorems, the correct region of each given
function is contractible, while in theorem 14, the correct region of each function is homeomorphic to
D3 × S3 (Dn is the closed disk) which is homotopic to S3 and thus noncontractible. The classiﬁer
may have difﬁculty dividing SO(4) into noncontractible regions without strong supervision.
Indeed, it is not possible to cover SO(4) with 4 contractible open subsets, as it is known that
cat(SO(4)) = 5. Possibly as a result of this, we can see that we succeeded with an ensemble of ﬁve.
Nevertheless, we show that a successful ensemble of four networks do exist by forcing it to learn the
functions we constructed in theorem 14. We modify the loss function as follows: let Li(R) be the
loss of network i on input matrix R, deﬁne
L1(RQQ(p, q)) =
d4((p, q), f1(RQQ(p, q)))
(|Re(p)| ≥1
2)
2π
(|Re(p)| < 1
2)
(49)
L2(RQQ(p, q)) =
d4((p, q), f2(RQQ(p, q)))
(|Re(pi)| ≥1
2)
2π
(|Re(pi)| < 1
2)
L3(RQQ(p, q)) =
d4((p, q), f3(RQQ(p, q)))
(|Re(pj)| ≥1
2)
2π
(|Re(pj)| < 1
2)
L4(RQQ(p, q)) =
d4((p, q), f4(RQQ(p, q)))
(|Re(pk)| ≥1
2)
2π
(|Re(pk)| < 1
2)
That is, the Li takes the maximum value 2π regardless of the output if the input does not lie in the
correct region of fi as deﬁned in the proof of theorem 14. The loss function of the whole ensemble is
derived from these as in equation 8. We train the networks with this modiﬁed loss function for the
ﬁrst 300k iterations, then revert to the normal loss function for another 200k iterations. The result
is shown as “Theorem 14”. We can see that this hand-designed ensemble of four achieves lower
maximum error than the ensemble of ﬁve but higher average error than the ensemble of three, which
shows again that lower average error and lower maximum error might be conﬂicting goals and if
eliminating large errors is considered important then we may need more suitable training methods.
Another point of note is that all autonomous ensembles up to size 4 gave a maximum error close to
360◦. We would like to verify theorem 13, and again we are faced with the problem with lowering
the maximum error. This time, we deliberately let the network learn the “wrong” function we
constructed in the proof of theorem 13. That is, the loss function of the network used for training
is L(RQQ(p, q)) = d4((p, q), (1, pq)) while for testing the error is measured as normal. The
result is shown as “Theorem 13”. While the average error is awfully large (in theory it equals
π
2 + 2
π ≈126.4756◦, the average angle of a uniform random 3D rotation), the maximum error is
close to 180◦.
25

0%
20%
40%
60%
80%
100%
0.1 ∘
1 ∘
10 ∘
100 ∘
360 ∘
Single
Ensemble∘of∘2
Ensemble∘of∘3
Ensemble∘of∘4
Ensemble∘of∘5
Theorem∘14
Theorem∘13
Figure 7: Error of 4D rotation matrix conversion by
percentile, using different quaternion pair ensembles
Table 8: Error statistics
• Type
Mean(◦)
Max(◦)
• Single
6.7637 344.7241
• ×2
0.7298 350.5893
• ×3
0.6702 357.1087
• ×4
0.5760 356.0749
• ×5
0.5563
8.7145
• Thm. 14
0.6877
4.6233
• Thm. 13 126.5174 180.1556
26

