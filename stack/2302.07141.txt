Event-based Backpropagation for Analog Neuromorphic Hardware
Christian Pehle * 1 Luca Blessing * 1 Elias Arnold 1 Eric M¨uller 1 2 Johannes Schemmel 1
Abstract
Brain-inspired or neuromorphic computing aims
to incorporate lessons from studying biological-
nervous systems in the design of practical com-
puter architectures. While existing approaches
have successfully implemented aspects of those
computational principles, such as sparse spike-
based computation, event-based scalable learning
has remained an elusive goal in large-scale sys-
tems. Reaching this goal is important because
only then the potential energy-efﬁciency advan-
tages of neuromorphic systems relative to other
hardware architectures can be realized during
learning. We present our progress implement-
ing such an event-based algorithm for learning —
the EventProp Algorithm— using the example of
the BrainScaleS-2 analog neuromorphic hardware.
Previous gradient-based approaches to learning
used “surrogate gradients” and dense sampling of
system observables or were limited by assump-
tions on the underlying dynamics and loss func-
tions. In contrast, our approach does only need
spike time observations from the system while
being able to incorporate other system observ-
ables, such as membrane voltage measurements,
in a principled way. This leads to a one-order-of-
magnitude improvement in the information efﬁ-
ciency of the gradient estimate, which would di-
rectly translate to corresponding energy efﬁciency
improvements in an optimized hardware imple-
mentation. We present the theoretical framework
for estimating gradients and results verifying the
correctness of the gradient estimation, as well as
experimental results on a low-dimensional clas-
siﬁcation task using the BrainScaleS-2 system.
Building on this work has the potential to enable
scalable gradient estimation in large-scale neuro-
*Equal contribution
1Kirchhoff-Institute for Physics, Hei-
delberg
University,
Heidelberg,
Germany
2European
In-
stitute for Neuromorphic Computing,
Heidelberg Univer-
sity, Heidelberg, Germany.
Correspondence to:
Christian
Pehle <christian.pehle@kip.uni-heidelberg.de>, Luca Blessing
<luca.blessing@kip.uni-heidelberg.de>.
morphic hardware, including those using novel
nano-devices, as a continuous measurement of
the system state would be prohibitive and energy-
inefﬁcient in such instances. It also suggests the
feasibility of a full on-device implementation of
the algorithm that would enable scalable, energy-
efﬁcient, event-based learning in large-scale ana-
log neuromorphic hardware.
Neuromorphic or brain-inspired hardware aims to incorpo-
rate lessons and metaphors derived from biological nervous
systems into the design of practical computer architectures.
Many complementary approaches exist, ranging from digi-
tal implementations to architectures involving novel nano-
devices. They have in common that they conceptualize com-
putation as occurring in a network of processing elements
—“neurons” exchanging messages “spikes”— with varying
amounts of ﬂexibility in the realizable network topology and
conﬁgurability of the processing elements. The question
then becomes how to endow such a network with function.
While there are, in principle, many approaches, some taking
inspiration from local learning rules postulated in compu-
tational neuroscience a by now well-established approach
is to use machine-learning techniques and, in particular,
gradient-based optimization methods.
In-the-loop gradient-based training on digital neuromor-
phic hardware was demonstrated on the TrueNorth sys-
tem (Merolla et al., 2014; Esser et al., 2016). To side-step
the discontinuous state transition in the neuron model, a
pseudo-derivative was introduced, which then resulted in
a differentiable computation graph. The limited hardware
weight precision was addressed by tracking a ﬂoating point
precision weight in software, which was updated by gra-
dient information during a backward pass computed on a
conventional computer.
While in digital neuromorphic hardware neuron models are
numerically implemented, with typically ﬁxed discretized
time-step, analog neuromorphic hardware implements phys-
ical neuron models, which inherently operate continuously
in time. Gradient-based hardware in-the-loop learning has
also been demonstrated to be a suitable learning scheme for
analog neuromorphic hardware (Arnold et al., 2022; Cramer
et al., 2022; G¨oltz et al., 2021; Schmitt et al., 2017).
arXiv:2302.07141v1  [q-bio.NC]  13 Feb 2023

Event-based Backpropagation for Analog Neuromorphic Hardware
However, these algorithms either make use of dense obser-
vations of neuron dynamics (Cramer et al., 2022), rely on
a rate limit of the observed spike activity (Schmitt et al.,
2017), or are limited by assumptions on time constants,
spiking behavior, and network topology (G¨oltz et al., 2021).
Here, we introduce a gradient-estimation algorithm, based
on the EventProp algorithm (Wunderlich & Pehle, 2021),
that only needs spike time observations of the Neuromorphic
Hardware and is suitable for arbitrary network topologies,
dynamics and loss functions, eliminating all those limita-
tions.
More speciﬁcally we make the following contributions:
1. We show that spike observations are sufﬁcient for es-
timating gradients in analog neuromorphic hardware
emulating spiking neurons. By training a feed-forward
network of 120 leaky-integrate and ﬁre (LIF) neurons
on a low dimensional classiﬁcation task, we demon-
strate for the ﬁrst time that the EventProp algorithm
can be used as an in-the-loop (ITL) training algorithm
for analog neuromorphic hardware.
2. By comparing with a surrogate gradient implementa-
tion both in simulation and on hardware using ITL
training, we show that both algorithms solve the task
with equivalent performance. The proposed algorithm
is more information efﬁcient, resulting in a ten-fold
reduction in required data.
3. We demonstrate that the gradient estimation is robust
to the variability of the underlying neuromorphic sub-
strate. More speciﬁcally, we show for a test case, that
the mean gradient estimate obtained using spike time
hardware measurements by our algorithm agrees with
the analytical solution.
Taken together we believe this indicates that the proposed
approach holds promise as an efﬁcient and scalable solution
to gradient estimation in analog neuromorphic hardware.
1. Neuromorphic Hardware
A wide variety of neuromorphic or brain-inspired hardware
architectures have been proposed, representing many com-
plementary approaches (Thakur et al., 2018). While digital
systems rely on simulation using numerical calculations,
physical models use analog or physical properties of a sub-
strate for some aspect of the implemented “neuro-inspired”
computation to gain an advantage in terms of power efﬁ-
ciency, speed, or density relative to digital computer archi-
tectures. BSS-2 is a research platform (Figure 1) based on
a mixed-signal neuromorphic system using conventional
CMOS technology. Its analog network core emulates 512
Figure 1. Illustration of the ITL gradient optimisation approach for
analog neuromorphic hardware. During the forward pass a spiking
neural network is physically emulated on the BSS-2 neuromorphic
chip. Hardware parameters phw setting up the digital synaptic
weights of the network are programmed and then spikes sin are
sent into the chip. The resulting spikes produced by the network
sout, and optionally a subset of the membrane voltage traces v,
are recorded. During the backward pass the recorded hardware
membrane voltages v, input spikes sin and output spikes sout are
used to estimate parameter gradients for the hardware system,
based on a software model implemented in PyTorch. Top left:
A BSS-2 neuromorphic chip (4 mm × 8 mm) bonded to a carrier
board, see Pehle et al. (2022) for detailed information.
spiking adaptive exponential integrate-and-ﬁre (AdEx) neu-
ron circuits in continuous time, typically accelerated by 103
compared to biological time scales. Each circuit is conﬁg-
urable to exhibit LIF or leaky integrator (LI) dynamics and
connects to a column of 256 6 bit synapses. The connec-
tivity is restricted to a deﬁnite sign per synapse row (the
synapses are organized in two 256 × 256 arrays). To im-
plement signed synapses (in violation of Dale’s law) two
synapse rows are required. Larger synaptic input counts
or multi-compartmental neuron morphologies are realized
by connecting circuits into “logical” neurons. For further
details, we refer to (Pehle et al., 2022; Billaudelle et al.,
2022). On-chip digital components handle spike event com-
munication and provide memory-mapped access to, e.g.,
neuron-individual parameters, synaptic weights, and other
settings. Besides allowing the implementation of ﬂexible
local learning rules or general control tasks, two embed-
ded single instruction, multiple data processors can access
on-chip observables such as membrane voltages via analog-
to-digital converters (ADCs) as well as off-chip memory.
A ﬁeld-programmable gate array (FPGA) orchestrates ex-
periments and turns the neuromorphic hardware into a
network-attached spiking neural network (SNN) acceler-

Event-based Backpropagation for Analog Neuromorphic Hardware
ator. Dynamic random-access memory (DRAM) on the
FPGA printed circuit board serves as a real-time capable
playback buffer, e.g., for input stimuli or timed conﬁgura-
tion commands, and stores streamed-out chip data.
2. Advantages of Event-based training
In order to implement the surrogate gradient ITL learning
scheme (Cramer et al., 2022), the membrane voltage of the
neuron circuits is digitized and stored with a temporal reso-
lution of approximately 2 µs. This imposes a signiﬁcant bur-
den both in terms of energy consumption and memory use,
making a more information-efﬁcient gradient estimation
algorithm desirable. Moreover, to compute the surrogate
gradient the membrane recording is interpolated in software
and a dense membrane trace is constructed for all neurons.
This leads to a computational overhead, which affects train-
ing speed. In our experiments, we observe a speed-up in the
EventProp-based training, which would further increase if
the loss function was only dependent on spike times.
The external memory bandwidth and capacity can also be
a limiting factor. In the particular hardware system under
consideration, up to 512 neurons can be used with each
membrane sample providing an 8 bit value per neuron and
an spike event being represented by a 8 bit label and a 16 bit
timestamp. Digitizing the membrane voltage state with a fre-
quency of 500 kHz, results in data transfer rates of 2 Gbit/s.
The external trace memory has a capacity of 512 MiB, there-
fore the surrogate gradient approach accommodates at most
4 s of experiment runtime. Biologically plausible ﬁring
rates of neurons are around 10 Hz, resulting in an average
expected spike rate of 10 kHz on BSS-2. This translates to
approximately 0.12 Gbit/s required bandwidth and therefore
one order of magnitude improvement in memory efﬁciency
of the proposed algorithm relative to the surrogate gradi-
ent approach (Cramer et al., 2022) on BSS-2. For smaller
networks, or lower hardware utilization, the encoding over-
head of the membrane samples increases. Similar estimates
would apply to other analog or digital neuromorphic hard-
ware, but the details depend on the memory representation
of spike and voltage data.
We can estimate the information efﬁciency of the proposed
method compared to surrogate-gradient-based training in
terms of the number of observed spike events ne, the num-
ber of bits used to represent a spike event be, the number
of voltage samples that are recorded nv to compute the sur-
rogate gradients and the number of bits per voltage sample
bv. The surrogate gradient method needs both voltage trace
data and spike observations, so it requires nvbv + nebe bits
of information. In contrast the gradient estimation algo-
rithm used in this study relies on nebe bits of information.
t
tpre
0
tpre
1
tpost
0
tpost
1
x
I
V
λV
λI
Figure 2. Forward and adjoint (backward) dynamics of a LIF neu-
ron receiving two input spikes at tpre
1
and tpre
2
. The membrane
potential experiences two jumps, giving the post-synaptic spike
times tpost
1
and tpost
2
. In the computation of the adjoint dynamics,
which happens backwards in time, the adjoint variable λV expe-
riences jumps at the two post-synaptic spike times. The gradient
contributions are then computed by sampling the adjoint state at
the respective pre-synaptic spike times.
Therefore the overall gain is given by
1 + nvbv
nebe
.
(1)
Since the number of required voltage samples is fundamen-
tally determined by the time constants of the membrane
voltage dynamics, whereas the number of observed spike
events is typically orders of magnitudes lower, this leads to
the conclusion that the method is generically more informa-
tion efﬁcient. This estimate applies to the particular kind of
neuron model under consideration. Similar estimates apply
to other processing elements coupled by events, as long as
they are solely coupled by messages passed between them.
3. Hardware Training Results on the
Yin-Yang Dataset
We choose the Yin-Yang dataset (Kriener et al., 2022), a
two-dimensional classiﬁcation task with three classes, to
demonstrate the learning algorithm on BrainScaleS-2. A
point (a, b) in the dataset lies in the interior of the box
[0, 1] × [0, 1] and belongs to one of three classes depending
on which part of the “Yin-Yang” it belongs to. In Table 1 and
Fig. 3 we report our results. Hyperparameters and training
details are given in Section 6.4 and Table 3.
We compare our surrogate-gradient and EventProp imple-

Event-based Backpropagation for Analog Neuromorphic Hardware
0
1
x
0
1
y
A
0
1
x
0
1
y
0
1
x
0
1
0
1
x
0
1
0.00
0.25
0.50
0.75
1.00
1.25
1.50
max
t
(V O)
B
tearly
tlate
T
x
y
1 −x
1 −y
bias
119
:
0
V O [a.u.]
C
Input
120 LIF
Output LI
0
50
100
150
200
250
300
epochs
10−1
test error
(a)
(b)
(c)
(d)
D
Figure 3. A Example points of Yin-Yang dataset (Kriener et al., 2022). The dataset is represented by two-dimensional points classiﬁed
into one of the three classes yin (blue), yang (orange) or dot (green). B The maximum membrane values of the three output LI-neurons
are used for classiﬁcation and are shown for the test set after training. C Samples from the dataset are symmetrized and encoded into a
time window [tearly, tlate]. Additionally, a bias spike is added. For classiﬁcation, those input spikes are projected onto a hidden layer of 120
LIF neurons. The hidden spikes are received by an output layer of 3 LI neurons. The activity and observables of the network are shown
for the yellow sample in A. Spikes and membrane voltages use different timestamp clocks; we only approximately aligned the two time
domains. D The test error for hardware-ITL training with EventProp on the Yin-Yang dataset is depicted and compared to other results as
given in Table 1.
mentation both in simulation and on hardware in-the-loop
training. We ﬁnd that in simulation both the surrogate gra-
dient and EventProp implementation achieve comparable
performance. Using the hardware-in-the loop approach to
estimate gradients we achieve on average 1.5% higher ac-
curacy with the EventProp gradient estimation method and
the highest observed hardware performance on this task
so far, compared to previously reported results using the
Fast-and-Deep gradient estimation algorithm (G¨oltz et al.,
2021).
We use Eq. (1) to estimate the gain in information efﬁciency
for this particular experiment on the Yin-Yang dataset. Sam-
pling the membrane voltages of 120 hidden neurons with
500 kHz over 38 µs results in a total of nv = 2280 voltage
samples per input datapoint. In the last epoch of the training
with EventProp, we measure an average of ne = 146 ± 13
spikes in the hidden layer per input datapoint of the train-
ing set. This leads to a factor of 6.2 ± 0.5 improvement in
memory efﬁciency using EventProp compared to training
with surrogate gradients.
4. Hardware Gradient Estimation
Since previous work (G¨oltz et al., 2021) had demonstrated
that gradient-estimation using an analytical formula for the
spike time of LIF neurons could successfully be applied to
hardware in-the-loop training, we ask whether the gradient
estimate computed using our method would match the ana-
lytic estimate. We ﬁnd that for a simple experiment setup
with one LIF neuron receiving one input spike with weight
w at time t = 0, the mean of the estimated gradient of the
loss function L = tpost, where tpost is the spike time of the
LIF neuron, agrees well with the analytical prediction (see
Fig. 4).
5. Discussion
We demonstrated a gradient estimation algorithm for analog
neuromorphic hardware requiring only spike observations
and making no assumptions on the network topology or loss
function. As such, this has the potential to enable scalable
gradient estimation in large-scale neuromorphic hardware
since continuous measurement of the system state would
be prohibitively expensive in this case. In particular, a sim-

Event-based Backpropagation for Analog Neuromorphic Hardware
Type
Grad. Estimator
Loss
Acc. [%]
ANN
Backprop. (Kriener et al., 2022)
CE
97.6 ±1.5
Backprop. (Kriener et al., 2022)
CE
85.5 ±5.8
(frozen lower weights)
SNN
analytical (G¨oltz et al., 2021)
sim.
TTFS
95.9 ±0.7
analytical (G¨oltz et al., 2021)
hw
◀(b)
TTFS
95.0 ±0.9
EventProp (Wunderlich & Pehle, 2021)
sim.
◀(d)
TTFS
98.1 ±0.2
SNN
Surr. Gradient
sim.
Max
97.6 ±0.4
Surr. Gradient
hw
Max
94.6 ±0.7
EventProp
sim.
◀(c)
Max
97.9 ±0.6
EventProp
hw
◀(a)
Max
96.1 ±0.8
Table 1. Test accuracies on the Yin-Yang task for ANNs and SNNs with different gradient estimators and loss deﬁnitions. For the SNNs
numerical integration and hardware emulation are compared. The results marked by ‘CE’ use a cross-entropy loss, the ‘TTFS’ results are
using a loss based on time-to-ﬁrst-spike decoding and the ‘Max’ results are based on ‘maximum membrane value over time’. Kriener et al.
(2022) use an ANN with 30 hidden neurons. Wunderlich & Pehle (2021) use a SNN with 200 hidden neurons, the results by G¨oltz et al.
(2021) and our own results use 120 hidden neurons.
L = tpost
0
w
0.2
0.4
0.6
tpost
0
[τs]
3.0
3.2
3.4
3.6
3.8
4.0
4.2
w
−1.0
−0.5
0.0
∂tpost
0
/∂w
50 neurons
average
analytic
Figure 4. Experiment setup (top), spike time (middle) acquired
from runs on BrainScaleS-2 and estimated gradient (bottom) for
a loss function L = tpost as a function of the weight w. Results
for n = 50 hardware neuron circuits are shown in light grey, the
average in dashed black. The analytical result (blue line, see Fig. 5)
agrees well with the hardware average.
ilar approach to the one pursued here would also apply to
neuromorphic systems for which continuous sampling of
the system state would be infeasible even in principle, such
as ones based on novel nano-devices or photonic neuro-
morphic systems. While the original implementation of
EventProp (Wunderlich & Pehle, 2021) relied on a custom
event-based solver, we use here a PyTorch implementation
and time-discretized forward- and adjoint dynamics. Such
a time-discretized implementation would also be suitable
for digital neuromorphic hardware. Future work will take
advantage of an event-based solver to demonstrate tasks that
require precise spike timing or computation on sparse data.
Although the results reported here are encouraging, further
work is needed in several areas:
• We would like to demonstrate the algorithm on fur-
ther tasks, particularly ones that are not feasible using
surrogate-gradient-based in-the-loop training due to
hardware trace-memory limitations.
• We would like to demonstrate the scalability of the
algorithm by applying it on larger-scale networks, and
on longer time scales.
• We would like to demonstrate the algorithm on other
neuron conﬁgurations supported by the chip.
Going beyond the immediate applications of this work, we
would like to use hardware observables to learn the dynam-
ics, instead of assuming a particular model. Demonstrating
the scalability of the algorithm beyond the modest size of the
network considered here requires both a multi-chip experi-
ment setup, which is currently being built, and potentially
network models that account for delays. We have evaluated
the EventProp algorithm itself on larger-scale convolutional
feed-forward architectures with ∼105 neurons and ∼106
parameters and found no performance disadvantage over sur-
rogate gradients. However, we were fundamentally limited
by the small number of integration timesteps and memory
constraints.
In digital neuromorphic hardware, it is relatively straight-
forward to simulate the implemented neuron dynamics with
commodity hardware. Thus, this work can be viewed mainly

Event-based Backpropagation for Analog Neuromorphic Hardware
as a demonstration that the temporal discretization and quan-
tization of observables are not an obstacle to the implemen-
tation of the EventProp gradient estimation algorithm.
Our results also suggests that a fully on-device implementa-
tion of the algorithm would be possible, where the required
spike time information and voltage slopes at spike times
could be stored locally at each neuron circuit. Both the
forward and adjoint dynamics can be realised within the
analog computation paradigm and in the current form with
the very similar analog circuits. Following this direction
would enable scalable, energy-efﬁcient, event-based learn-
ing in large-scale analog neuromorphic hardware.
6. Methods
6.1. Adjoint Sensitivity Analysis with Jumps
The BrainScaleS-2 system can emulate the AdEx neuron
model (Gerstner & Brette, 2009) faithfully (Billaudelle et al.,
2022). To illustrate how to determine paramter gradients
we consider here only an adaptive leaky-integrate and ﬁre
neuron. Its equations specify a hybrid-ordinary differential
equation and we use adjoint-sensitivity analysis with jumps
to derive exact gradients for this model, following the ap-
proach of Wunderlich & Pehle (2021). We are considering a
loss function that can depend on the spike times and voltage
traces
L = lp(tp) +
Z T
0
l(V )dt
(2)
more general loss functions can also be considered and
indeed the max-time-loss does not follow this form. For
a derivation of how the max-over-time loss can be incor-
porated see Wunderlich & Pehle (2021). The dynamical
equations of the model are given by
C ˙V = −gL(V −EL) −w + RI
(3)
τw ˙w = a(V −EL) −w
(4)
τs ˙I = −I.
(5)
Here C is the membrane capacitance, gL the leak conduc-
tance, EL the leak potential, w the adaptation variable, R
the input resistance and τw, τs are the adaptation and synap-
tic time constants respectively. After a change of variables
v = V −EL, we can write these equations in matrix form
˙x =


˙v
˙w
˙I

= Ax =


−τ −1
m
−1/C
R/C
−aτ −1
w
−τ −1
w
0
0
0
−τ −1
s




v
w
I

.
(6)
When v reaches a threshold ϑ, then the neuron is subject to
the following transition in its state:


v+
w+
I+

=


0
0
0
0
1
0
0
0
1




v−
w−
I−

+


vr
b
0

.
(7)
If we consider a network of n neurons with state vector
x, connected by a synaptic weight matrix, then if neuron i
reaches its threshold the resulting transition can be written
as
x+ = P T
i TPix−+ (⊮−P T
i Pi)x−+ pi,
(8)
with Pi the projection of the 3n dimensional state space to
the 3 dimensional state-space of neuron i,
T =


0
0
0
0
1
0
0
0
1


(9)
and pi given by the translation induced by synaptic trans-
mission, the reset of the membrane potential and shift of the
adaptation constant.
Based on this description it is easy to compute the associ-
ated adjoint sensitivity equations with jumps, as well as the
corresponding parameter gradients. The adjoint equations
are given by,
λ′ =


−gL/C
−a/τw
0
−1/C
−1/τw
0
R/C
0
−1/τs

λ,
(10)
and their jumps are computed according to
(λ−)T = (λ+)T

[f + −∂x−θf −] ∂x−j
∂x−j ˙x−+ ∂x−θ

.
(11)
Here we have
f + = A(P T
i TPix−+ (⊮−P T
i Pi)x−+ pi),
(12)
f −= Ax−,
(13)
∂x−θ = P T
i TPi + (⊮−P T
i Pi),
(14)
∂x−j = ∂x−(aT x−+ w) = aT ,
(15)
where A was deﬁned in Eq. (6) and aT = (1, 0, 0). This
simpliﬁes the equation to
(λ−)T = (λ+)T

Api
aT
aT ˙x−+ P T
i TPi + (⊮−P T
i Pi)

.
(16)
There exists an equally simple expression for the gradient
contributions to the synaptic weights wji (Wunderlich &
Pehle, 2021)
dL
dwji
= −τs
X
spikes from i
(λI)j.
(17)

Event-based Backpropagation for Analog Neuromorphic Hardware
The formulas derived here simplify to the ones obtained
in (Wunderlich & Pehle, 2021), if the adaptation variable
w is omitted. It is also possible to use the same overall ap-
proach to derive explicit adjoint equations for the full AdEx
neuron model, including the case of an absolute refractory
period or synaptic transmission delays. One consideration
is that since the full AdEx dynamics is non-linear, the full
membrane voltage trace enters the adjoint dynamics. This
introduces an additional complication in the potential appli-
cability of the method.
6.2. Software Framework
Our software stack translates the high-level SNN experi-
ment description to a data ﬂow graph representation, places
and routes neurons and synapses on the hardware substrate,
and compiles stimulus inputs, recording settings, and other
runtime dynamics into an experiment program representing
an equivalent experiment conﬁguration on BrainScaleS-2,
see M¨uller et al. (2022a) for a detailed description. The
analog substrate on BrainScaleS-2 is subject to device vari-
ations, or ﬁxed-pattern noise, that can be compensated for
by calibration. At the same time, the calibration routines
consider user-deﬁned model parameters to provide an equiv-
alent parametrization of the emulation. A complete calibra-
tion data set provides per-circuit operation point settings.
The training employed in this paper only affects the digital
weight parameters and is implemented as an incremental
reconﬁguration providing quick hardware-ITL updates. The
BrainScaleS-2 hardware substrate only supports ﬁxed-sign
6 bit synapses. We allocate two hardware synapses per soft-
ware weight wsw to support efﬁcient signed weight matrix
updates. Each software weight wsw is linearly scaled into
a hardware-compatible range and rounded to the nearest
integer value. The batched input spikes are injected into
BrainScaleS-2 and the SNN is emulated for T = 38 µs per
batch entry. During emulation, spike events and neuron
membrane traces can be recorded to the FPGA DRAM. The
host computer reads back and post-processes the recorded
data. For experiments relying on columnar ADC membrane
measurements, the membrane samples are expressed on an
equidistant time grid by linear interpolation in order to be
represented by a torch::Tensor with a ﬁxed time grid
and thus aligned to the PyTorch API. Additionally, the val-
ues are offset and scaled into a desired range. The spike
recordings are mapped to a boolean tensor on the same time
grid.
This hardware-ITL operation mode on BrainScaleS-2 is
supported by hxtorch.snn (Spilger et al., 2023), a
PyTorch-based (Paszke et al., 2017) library that automates
and abstracts away hardware-speciﬁc procedures and pro-
vides data conversions from and to PyTorch.
6.3. Numerical Gradient Estimate
We discretize the forward and adjoint dynamics using the ex-
plicit Euler integration scheme to implement the EventProp
Algorithm, which is derived in continuous time (Wunder-
lich & Pehle, 2021). The dynamics of the LIF neurons
with exponential-shaped, current-based synapses are either
computed in simulation only or can be injected from ob-
servations when training with hardware in the loop. This,
together with computing the adjoint trajectories, is handled
in a custom torch.autograd.Function. The com-
plete dataﬂow, together with another function ensuring the
correct backpropagation to the synaptic weights and the pre-
vious layer is displayed in Figure 6. The gradient estimation
for a layer of LIF neurons are described in Algorithm 1.
For the backward computation we need the spike times
and the time derivatives of the membrane ˙V , which are
determined only by the synaptic currents I at spike times.
When training with hardware in the loop, synaptic currents
are not accessible and need to be estimated in order to be
able to calculate the jumps of the adjoint variables. We
estimate the synaptic currents by numerically integrating
Equation (5) while assuming ideal dynamics on hardware
and use the boolean tensors, to which the spike recordings
were mapped, to apply the transitions as in Equation (8).
To compare our gradient estimate from simulation to the
analytical gradient, we consider the experiment setup of a
LIF neuron receiving a single spike with weight w as in
Figure 4. For the special case of τsyn = τmem, τrefrac = 0 the
explicit formula from G¨oltz et al. (2021) is used.
For spike time dependent losses, as for the gradient
comparisons in Figure 4 and Figure 5, we need to ex-
tract spike times from dense boolean tensors.
To be
able to optimize on such losses, we wrote a custom
torch.autograd.Function, see Listing 1. The gra-
dient with respect to the time of a spike is backpropagated
at the index, at which the spike was located in the dense
boolean input tensor.
There are two sources of errors introduced by numerical
implementation: Since we only evaluate spike times on
a ﬁxed time grid, the spike time changes in jumps as the
synaptic weight increases. The second error source is the
numerical error introduced by the integration method itself.
The chosen integration scheme is only ﬁrst-order accurate.
As seen in Figure 5, the jumps in the spike times have
a larger impact on the mismatch in the gradient estimate,
relative to the analytical formula.
For decreasing integration time steps dt the numerical gra-
dient estimate converges to the analytically known gradient.
Therefore, we consider forward euler integration sufﬁcient
for our experiments.

Event-based Backpropagation for Analog Neuromorphic Hardware
0.3
0.4
0.5
0.6
tpost
0
[τs]
3.2
3.6
4.0
4.4
4.8
w
−0.4
−0.2
∂tpost
0
/∂w
analytic
dt = 0.1τs
dt = 0.05τs
dt = 0.01τs
3.6
4.0
4.4
w
−0.30
−0.25
−0.20
−0.15
−0.10
−0.05
∂tpost
0
/∂w
EventProp
SuperSpike
sur. grad.
analytic
Figure 5. Spike time (top) and estimated gradient (bottom) for
a loss function L = tpost as a function of the weight w. As
the numerical integration timestep dt decreases the numerically
estimated gradient (dashed lines) converges to analytically known
gradient (solid-black line), cf. G¨oltz et al. (2021). Jumps in the
gradient and spike time are due to the ﬁnite resolution dt of the
time-grid, note that the gradient is changing although the spike
time remains constant.
Algorithm 1 Discrete EventProp gradient estimation for a
single layer of LIF neurons
input
pre-synaptic spikes zpre,
parameters W , τm, τs, dt, T,
optional: hardware data (Vhw, zhw)
{Forward pass}
if no hardware data then
(Vsw, Isw, zsw) by forward euler integration of forward
dynamics and jumps from 0 to T, step size dt
else
Estimate Ihw from W and zpre
end if
{Backward pass}
(λI, λV ) by forward euler integration of adjoint dynam-
ics and jumps from T to 0, step size dt
output
dL
dW = −τsλ⊤
I zpre,
dL
dzpre = (λV −λI)W
6.4. Training Details
The four values (a, b, 1−a, 1−b) for each sample of the Yin-
Yang dataset are translated into spike times using a latency
encoding inside the time interval [tearly, tlate]. Additionally
a bias spike is added at tbias, which serves as a ﬁfth input
spike and is the same for every data point. Those times are
then mapped as events onto a boolean tensor on the desired
time grid.
The feed-forward network consists of a hidden layer with
120 LIF neurons and an output layer of 3 LI neurons, one
for each class. The loss we use is composed of two terms
L = L1 + L2. The ﬁrst and main term is a max-over-time
loss
L1 = −
1
Nbatch
Nbatch
X
n=1
log
exp
 maxt V O
n,yn(t)

PC
c=1 exp
 maxt V O
n,c(t)
, (18)
with time t, the voltages of the output layer neurons V O, the
target y, the batch size Nbatch and the number of classes C.
Additionally, to prevent the amplitudes from being to high,
we add a regularization term
L2 = α ·
1
Nbatch C
Nbatch
X
n=1
C
X
c=1

max
t
V O
n (t)
2
,
(19)
where α is a scaling factor to adjust the inﬂuence of this
amplitude regularization.
When using BrainScaleS-2 ITL training, we repeat the input
per sample ﬁve times, ultimately giving us 25 input streams
per data point. The hidden weights are initialized as an
nhidden × 5 matrix and are then also repeated 5 times along
the input dimension, resulting in a nhidden × 25 weight ma-
trix. This gives us an equivalent of an increase in synaptic
efﬁcacy without changing the target model parameters and
underlying calibration data set.
Comparing the gradients of the output layer weights to those
of the hidden layer weights, the gradients differ by multiple
orders of magnitude. To counteract this, we scale the the
weight gradients computed by the EventProp algorithm with
a factor 1/τs, which was necessary to obtain the results on
the Yin-Yang dataset.
Author Contributions
CP: Conceptualization, formal analysis, writing — original
draft, writing — reviewing & editing; LB: Validation, for-
mal analysis, investigation, visualization, writing — original
draft, writing — reviewing & editing; EA: Methodology,
software, resources, writing — original draft, writing — re-
viewing & editing; EM: Methodology, software, resources,
writing — original draft, writing — reviewing & editing,
supervision; JS: Supervision, funding acquisition, writing
— reviewing & editing.

Event-based Backpropagation for Analog Neuromorphic Hardware
Acknowledgements
The authors wish to thank P. Spilger and C. Mauch for soft-
ware enhancements and platform operation, S. Billaudelle,
J. G¨oltz, and J. Weis for fruitful discussions and hardware
parameterization knowledge, and all present and former
members of the Electronic Vision(s) research group con-
tributing to the BrainScaleS-2 neuromorphic platform.
Funding
This work has received funding from the EC Horizon 2020
Framework Programme under grant agreements 785907
(HBP SGA2) and 945539 (HBP SGA3), the Deutsche
Forschungsgemeinschaft (DFG, German Research Foun-
dation) under Germany’s Excellence Strategy EXC 2181/1-
390900948 (the Heidelberg STRUCTURES Excellence
Cluster), the German Federal Ministry of Education and
Research under grant number 16ES1127 as part of the Pi-
lotinnovationswettbewerb ‘Energieefﬁzientes KI-System’,
the Helmholtz Association Initiative and Networking Fund
[Advanced Computing Architectures (ACA)] under Project
SO-092, as well as from the Manfred St¨ark Foundation,
and the Lautenschl¨ager-Forschungspreis 2018 for Karlheinz
Meier.
References
Nice workshop 2021:
A tiny spiking neural net-
work on dynap-se1 board simulator, 2021.
URL
https://code.ini.uzh.ch/yigit/NICE-
workshop-2021.
Intel:
Announcement
of
loihi-2
and
new
soft-
ware
framework.
https://www.intel.com/
content/www/us/en/newsroom/news/
intel-unveils-neuromorphic-loihi-
2-lava-software.html,
2021.
https:
//github.com/lava-nc/.
Amir, A., Datta, P., Risk, W. P., Cassidy, A. S., Kusnitz, J. A.,
Esser, S. K., Andreopoulos, A., Wong, T. M., Flickner,
M., Alvarez-Icaza, R., McQuinn, E., Shaw, B., Pass, N.,
and Modha, D. S. Cognitive computing programming
paradigm: A corelet language for composing networks
of neurosynaptic cores. In The 2013 International Joint
Conference on Neural Networks (IJCNN), pp. 1–10. IEEE,
2013.
Arnold,
E.,
B¨ocherer,
G.,
M¨uller,
E.,
Spilger,
P.,
Schemmel,
J.,
Calabr`o,
S.,
and Kuschnerov,
M.
Spiking neural network equalization on neuromor-
phic hardware for IM/DD optical communication.
In European Conference on Optical Communication
(ECOC) 2022, pp. Th1C.5. Optica Publishing Group,
June 2022.
URL https://opg.optica.org/
abstract.cfm?URI=ECEOC-2022-Th1C.5.
Benjamin, B. V., Gao, P., McQuinn, E., Choudhary, S.,
Chandrasekaran, A. R., Bussat, J.-M., Alvarez-Icaza, R.,
Arthur, J. V., Merolla, P. A., and Boahen, K. Neurogrid:
A mixed-analog-digital multichip system for large-scale
neural simulations. Proceedings of the IEEE, 102(5):
699–716, 2014.
Billaudelle, S., Weis, J., Dauer, P., and Schemmel, J. An
accurate and ﬂexible analog emulation of AdEx neuron
dynamics in silicon. arXiv preprint, 2022. doi: 10.48550/
arXiv.2209.09280.
Cramer, B., Billaudelle, S., Kanya, S., Leibfried, A., Gr¨ubl,
A., Karasenko, V., Pehle, C., Schreiber, K., Stradmann, Y.,
Weis, J., et al. Surrogate gradients for analog neuromor-
phic computing. Proceedings of the National Academy
of Sciences, 119(4), 2022.
Davies, M., Srinivasa, N., Lin, T.-H., Chinya, G., Cao, Y.,
Choday, S. H., Dimou, G., Joshi, P., Imam, N., Jain, S.,
et al. Loihi: A neuromorphic manycore processor with
on-chip learning. IEEE Micro, 38(1):82–99, 2018. doi:
10.1109/MM.2018.112130359.
DeWolf, T., Jaworski, P., and Eliasmith, C. Nengo and low-
power ai hardware for robust, embedded neurorobotics.
Frontiers in Neurorobotics, 14, 2020. ISSN 1662-5218.
doi: 10.3389/fnbot.2020.568359.
Esser, S. K., Merolla, P. A., Arthur, J. V., Cassidy, A. S.,
Appuswamy, R., Andreopoulos, A., Berg, D. J., McK-
instry, J. L., Melano, T., Barch, D. R., di Nolfo, C.,
Datta, P., Amir, A., Taba, B., Flickner, M. D., and Modha,
D. S. Convolutional networks for fast, energy-efﬁcient
neuromorphic computing. Proceedings of the National
Academy of Sciences, 113(41):11441–11446, 2016. ISSN
0027-8424. doi: 10.1073/pnas.1604850113.
Frenkel, C., Lefebvre, M., Legat, J.-D., and Bol, D. A 0.086-
mm212.7-pj/sop 64k-synapse 256-neuron online-learning
digital spiking neuromorphic processor in 28-nm cmos.
IEEE transactions on biomedical circuits and systems, 13
(1):145–158, 2018.
Furber, S. B., Lester, D. R., Plana, L. A., Garside, J. D.,
Painkras, E., Temple, S., and Brown, A. D. Overview of
the SpiNNaker system architecture. IEEE Transactions
on Computers, 99(PrePrints), 2012. ISSN 0018-9340.
doi: 10.1109/TC.2012.142.
Galluppi, F., Davies, S., Rast, A., Sharp, T., Plana, L. A.,
and Furber, S. A hierachical conﬁguration system for a
massively parallel neural hardware platform. In Proceed-
ings of the 9th conference on Computing Frontiers, pp.
183–192, 2012.

Event-based Backpropagation for Analog Neuromorphic Hardware
Galluppi, F., Lagorce, X., Stromatias, E., Pfeiffer, M., Plana,
L. A., Furber, S. B., and Benosman, R. B. A framework
for plasticity implementation on the spinnaker neural
architecture. Frontiers in Neuroscience, 8(429), 2015.
ISSN 1662-453X. doi: 10.3389/fnins.2014.00429.
Gerstner, W. and Brette, R.
Adaptive exponential
integrate-and-ﬁre model.
Scholarpedia, 4(6):8427,
2009.
doi:
10.4249/scholarpedia.8427.
URL
http://www.scholarpedia.org/article/
Adaptive exponential integrate-and-
fire model.
G¨oltz, J., Kriener, L., Baumbach, A., Billaudelle, S., Bre-
itwieser, O., Cramer, B., Dold, D., Kungl, ´A. F., Senn, W.,
Schemmel, J., Meier, K., and Petrovici, M. A. Fast and
energy-efﬁcient neuromorphic deep learning with ﬁrst-
spike times. Nature Machine Intelligence, 3(9):823–835,
2021. doi: 10.1038/s42256-021-00388-x.
Ji, Y., Zhang, Y., Li, S., Chi, P., Jiang, C., Qu, P., Xie, Y., and
Chen, W. Neutrams: Neural network transformation and
co-design under neuromorphic hardware constraints. In
2016 49th Annual IEEE/ACM International Symposium
on Microarchitecture (MICRO), pp. 1–13. IEEE, 2016.
Kriener, L., G¨oltz, J., and Petrovici, M. A. The yin-yang
dataset. In Neuro-Inspired Computational Elements Con-
ference, NICE 2022, pp. 107–111, New York, NY, USA,
2022. Association for Computing Machinery.
ISBN
9781450395595. doi: 10.1145/3517343.3517380.
Lin, C.-K., Wild, A., Chinya, G. N., Cao, Y., Davies, M.,
Lavery, D. M., and Wang, H. Programming spiking neural
networks on intel’s loihi. Computer, 51(3):52–61, 2018.
Mayr, C., Hoeppner, S., and Furber, S. Spinnaker 2: A
10 million core processor system for brain simulation
and machine learning. arXiv preprint arXiv:1911.02385,
2019.
Merolla, P. A., Arthur, J. V., Alvarez-Icaza, R., Cassidy,
A. S., Sawada, J., Akopyan, F., Jackson, B. L., Imam, N.,
Guo, C., Nakamura, Y., et al. A million spiking-neuron
integrated circuit with a scalable communication network
and interface. Science, 345(6197):668–673, 2014. doi:
10.1126/science.1254642.
Moradi, S., Qiao, N., Stefanini, F., and Indiveri, G. A scal-
able multicore architecture with heterogeneous memory
structures for dynamic neuromorphic asynchronous pro-
cessors (DYNAPs). IEEE Trans. Biomed. Circuits Syst.,
12(1):106–122, 2018.
M¨uller, E., Arnold, E., Breitwieser, O., Czierlinski, M., Em-
mel, A., Kaiser, J., Mauch, C., Schmitt, S., Spilger, P.,
Stock, R., Stradmann, Y., Weis, J., Baumbach, A., Bil-
laudelle, S., Cramer, B., Ebert, F., G¨oltz, J., Ilmberger, J.,
Karasenko, V., Kleider, M., Leibfried, A., Pehle, C., and
Schemmel, J. A scalable approach to modeling on acceler-
ated neuromorphic hardware. Front. Neurosci., 16, 2022a.
ISSN 1662-453X. doi: 10.3389/fnins.2022.884128.
M¨uller, E., Schmitt, S., Mauch, C., Billaudelle, S., Gr¨ubl,
A., G¨uttler, M., Husmann, D., Ilmberger, J., Jeltsch, S.,
Kaiser, J., Kl¨ahn, J., Kleider, M., Koke, C., Montes, J.,
M¨uller, P., Partzsch, J., Passenberg, F., Schmidt, H., Vog-
ginger, B., Weidner, J., Mayr, C., and Schemmel, J. The
operating system of the neuromorphic BrainScaleS-1 sys-
tem. Neurocomputing, 501:790–810, 2022b. ISSN 0925-
2312. doi: 10.1016/j.neucom.2022.05.081.
Orchard, G., Frady, E. P., Rubin, D. B. D., Sanborn, S.,
Shrestha, S. B., Sommer, F. T., and Davies, M. Efﬁcient
neuromorphic signal processing with Loihi 2. In 2021
IEEE Workshop on Signal Processing Systems (SiPS), pp.
254–259. IEEE, 2021.
Paszke, A., Gross, S., Chintala, S., Chanan, G., Yang, E.,
DeVito, Z., Lin, Z., Desmaison, A., Antiga, L., and Lerer,
A. Automatic differentiation in pytorch. 2017.
Pehle, C., Billaudelle, S., Cramer, B., Kaiser, J., Schreiber,
K., Stradmann, Y., Weis, J., Leibfried, A., M¨uller, E.,
and Schemmel, J. The BrainScaleS-2 accelerated neu-
romorphic system with hybrid plasticity.
Frontiers
in Neuroscience, 16, 2022.
ISSN 1662-453X.
doi:
10.3389/fnins.2022.795876.
Pei, J., Deng, L., Song, S., Zhao, M., Zhang, Y., Wu, S.,
Wang, G., Zou, Z., Wu, Z., He, W., Chen, F., Deng, N.,
Wu, S., Wang, Y., Wu, Y., Yang, Z., Ma, C., Li, G.,
Han, W., Li, H., Wu, H., Zhao, R., Xie, Y., and Shi, L.
Towards artiﬁcial general intelligence with hybrid tianjic
chip architecture. Nature, 572(7767):106–111, August
2019.
Qiao, N., Mostafa, H., Corradi, F., Osswald, M., Stefanini,
F., Sumislawska, D., and Indiveri, G. A re-conﬁgurable
on-line learning spiking neuromorphic processor com-
prising 256 neurons and 128k synapses.
Front. Neu-
rosci., 9(141), 2015. ISSN 1662-453X. doi: 10.3389/
fnins.2015.00141.
Rhodes, O., Bogdan, P. A., Brenninkmeijer, C., David-
son, S., Fellows, D., Gait, A., Lester, D. R., Mikaitis,
M., Plana, L. A., Rowley, A. G. D., Stokes, A. B.,
and Furber, S. B. spynnaker: A software package for
running pynn simulations on spinnaker.
Frontiers in
Neuroscience, 12:816, 2018. ISSN 1662-453X. doi:
10.3389/fnins.2018.00816.
Rostami, A., Vogginger, B., Yan, Y., and Mayr, C. G. E-prop
on SpiNNaker 2: Exploring online learning in spiking
RNNs on neuromorphic hardware. Front. Neurosci., 16,
2022. doi: 10.3389/fnins.2022.1018006.

Event-based Backpropagation for Analog Neuromorphic Hardware
Rowley, A. G. D., Brenninkmeijer, C., Davidson, S., Fel-
lows, D., Gait, A., Lester, D. R., Plana, L. A., Rhodes,
O., Stokes, A. B., and Furber, S. B. Spinntools: The
execution engine for the spinnaker platform. Frontiers
in Neuroscience, 13:231, 2019. ISSN 1662-453X. doi:
10.3389/fnins.2019.00231.
Rueckauer, B., Bybee, C., Goettsche, R., Singh, Y., Mishra,
J., and Wild, A. NxTF: An API and compiler for deep
spiking neural networks on Intel Loihi. arXiv preprint,
January 2021.
Schemmel, J., Br¨uderle, D., Gr¨ubl, A., Hock, M., Meier, K.,
and Millner, S. A wafer-scale neuromorphic hardware
system for large-scale neural modeling. In Proceedings of
the 2010 IEEE International Symposium on Circuits and
Systems (ISCAS), pp. 1947–1950, 2010. doi: 10.1109/
ISCAS.2010.5536970.
Schmitt, S., Kl¨ahn, J., Bellec, G., Gr¨ubl, A., G¨uttler, M.,
Hartel, A., Hartmann, S., Husmann, D., Husmann, K.,
Jeltsch, S., Kleider, M., Koke, C., Kononov, A., Mauch,
C., M¨uller, E., M¨uller, P., Partzsch, J., Petrovici, M. A.,
Vogginger, B., Schiefer, S., Scholze, S., Thanasoulis, V.,
Schemmel, J., Legenstein, R., Maass, W., Mayr, C., and
Meier, K. Neuromorphic hardware in the loop: Training
a deep spiking network on the BrainScaleS wafer-scale
system. Proceedings of the 2017 IEEE International Joint
Conference on Neural Networks (IJCNN), pp. 2227–2234,
2017. doi: 10.1109/IJCNN.2017.7966125.
Spilger, P., M¨uller, E., Emmel, A., Leibfried, A., Mauch,
C., Pehle, C., Weis, J., Breitwieser, O., Billaudelle,
S., Schmitt, S., Wunderlich, T. C., Stradmann, Y., and
Schemmel, J.
hxtorch: PyTorch for BrainScaleS-2
— perceptrons on analog neuromorphic hardware. In
IoT Streams for Data-Driven Predictive Maintenance
and IoT, Edge, and Mobile for Embedded Machine
Learning, pp. 189–200, Cham, 2020. Springer Inter-
national Publishing.
ISBN 978-3-030-66770-2.
doi:
10.1007/978-3-030-66770-2 14.
Spilger, P., Arnold, E., Blessing, L., Mauch, C., Pehle, C.,
M¨uller, E., and Schemmel, J. hxtorch.snn: Machine-
learning-inspired spiking neural network modeling on
BrainScaleS-2. Accepted, 2023.
Stefanini, F., Neftci, E. O., Sheik, S., and Indiveri, G.
Pyncs:
A microkernel for high-level deﬁnition and
conﬁguration of neuromorphic electronic systems.
Frontiers in Neuroinformatics, 8:73, 2014.
ISSN
1662-5196.
doi:
10.3389/fninf.2014.00073.
URL
https://www.frontiersin.org/article/
10.3389/fninf.2014.00073.
Thakur, C. S., Molin, J. L., Cauwenberghs, G., Indiveri, G.,
Kumar, K., Qiao, N., Schemmel, J., Wang, R., Chicca, E.,
Olson Hasler, J., Seo, J.-s., Yu, S., Cao, Y., van Schaik, A.,
and Etienne-Cummings, R. Large-scale neuromorphic
spiking array processors: A quest to mimic the brain.
Front. Neurosci., 12:891, 2018. ISSN 1662-453X. doi:
10.3389/fnins.2018.00891.
Voelker, A. R., Benjamin, B. V., Stewart, T. C., Boahen,
K., and Eliasmith, C. Extending the neural engineer-
ing framework for nonideal silicon synapses. In 2017
IEEE International Symposium on Circuits and Systems
(ISCAS), pp. 1–4. IEEE, 2017.
Wunderlich, T. C. and Pehle, C. Event-based backpropa-
gation can compute exact gradients for spiking neural
networks. Scientiﬁc Reports, 11(1):1–17, 2021. doi:
10.1038/s41598-021-91786-z.
Yan, Y., Kappel, D., Neum¨arker, F., Partzsch, J., Vogginger,
B., H¨oppner, S., Furber, S., Maass, W., Legenstein, R.,
and Mayr, C. Efﬁcient reward-based structural plastic-
ity on a spinnaker 2 prototype. IEEE Transactions on
Biomedical Circuits and Systems, 13(3):579–591, 2019.
doi: 10.1109/TBCAS.2019.2906401.

Event-based Backpropagation for Analog Neuromorphic Hardware
A. Neuromorphic Architectures
Arch
Hardware Model
Modeling Paradigm
SpiNNaker 1
soft digital
bio. SNN; distributed programming
(Furber et al., 2012)
(Rhodes et al., 2018; Rowley et al., 2019; Galluppi et al., 2015; 2012)
SpiNNaker 2
soft digital
bio. SNN; distributed programming; ANN
(Mayr et al., 2019)
(Rostami et al., 2022; Yan et al., 2019)
Loihi 1
ﬂexible digital
bio. & ML-friendly SNN; ANN
(Davies et al., 2018)
(Rueckauer et al., 2021; DeWolf et al., 2020; Lin et al., 2018)
Loihi 2
ﬂexible digital
bio. & ML-friendly SNN; ANN
(Orchard et al., 2021)
(loi, 2021)
TrueNorth
hard digital
bio. SNN; ANN
(Merolla et al., 2014)
(Amir et al., 2013)
ODIN
hard digital
bio. SNN
(Frenkel et al., 2018)
(Frenkel et al., 2018)
Tianjic
hard digital
bio. SNN; ANN
(Pei et al., 2019)
(Ji et al., 2016)
BrainScaleS-1
physical
bio. SNN
(Schemmel et al., 2010)
(M¨uller et al., 2022b)
BrainScaleS-2
physical
bio. and ML-friendly SNN; ANN
(Pehle et al., 2022; Billaudelle et al., 2022)
(M¨uller et al., 2022a; Spilger et al., 2020; 2023)
ROLLS
physical
bio. SNN
(Qiao et al., 2015)
(Stefanini et al., 2014)
DynapSE
physical
SNN
(Moradi et al., 2018)
(dyn, 2021)
Neurogrid
physical
SNN
(Benjamin et al., 2014)
(Voelker et al., 2017)
Table 2. Overview of neuromorphic chip architectures: At one end of the spectrum, programmable standard processors offer modeling
ﬂexibility that can come close to software simulations while maintaining higher efﬁciency. On the other end of the spectrum, analog
circuits typically offer high energy efﬁciency or speed, but this is typically based on a trade-off in modeling ﬂexibility or precision. Both
digital and analog implementations offer many different optimization options. Some systems focus on biological SNN operation, while
others support machine-learning-inspired training or classical ANN operation.

Event-based Backpropagation for Analog Neuromorphic Hardware
B. Spike time decoder
Listing 1 Custom torch.autograd.Function to convert boolean tensors holding spikes into spike times in a
backpropagation-compatible fashion. The argument spike count adjusts how many of each neurons spikes are retrieved.
If a neuron does spike fewer times than speciﬁed by spike count, the spike times are set to ﬂoating-point inﬁnity. The
backpropagation happens by injecting the gradient with respect to a spike time at the corresponding position, at which the
spike occurred in the boolean input tensor, with a negative sign. All other entries remain to be zero.
class ToSpikeTimes(torch.autograd.Function):
def forward(
ctx,
spike_input: torch.Tensor,
spike_count: torch.Tensor,
dt: float) -> torch.Tensor:
batch_size, time_size, out_size = spike_input.shape
indexed_spike_input =
spike_input * torch.arange(1, time_size + 1)[None, :, None] - 1.0
indexed_spike_input[indexed_spike_input == -1.0] = float("inf")
if spike_count < time_size:
spike_indices = torch.sort(
indexed_spike_input, dim=1).values[:, :spike_count]
else:
spike_indices = torch.sort(
indexed_spike_input, dim=1).values[:, :time_size]
ctx.save_for_backward(spike_indices, spike_input)
ctx.shape = spike_input.shape
return spike_indices * dt
def backward(
ctx,
grad_output: torch.Tensor) -> Tuple[torch.Tensor, None, None]:
(spike_indices, spike_input) = ctx.saved_tensors
batch_size, spike_count, out_size = spike_indices.shape
noninf_spike_indices = spike_indices.flatten() != float("inf")
grad_input = torch.zeros_like(spike_input, dtype=torch.float)
grad_input_indices = (
torch.arange(batch_size).repeat_interleave(out_size).repeat(
spike_count)[noninf_spike_indices],
spike_indices.flatten()[noninf_spike_indices].type(torch.long),
torch.arange(out_size).repeat(batch_size).repeat(
spike_count)[noninf_spike_indices],
)
grad_input[grad_input_indices] = \
- 1.0 * grad_output.flatten()[noninf_spike_indices]
return grad_input, None, None

Event-based Backpropagation for Analog Neuromorphic Hardware
C. Training Parameters
Parameter
EventProp
SuperSpike
τmem
6 µs
6 µs
τsyn
6 µs
6 µs
size input
5
5
size hidden layer
120
120
size output layer
3
3
weight init [mean, stdev]
hidden
[0.2, 0.2]
[0.001, 0.15]
output
[0.01, 0.1]
[0.0, 0.1]
dt
0.5 µs
0.5 µs
tbias
2 µs
2 µs
tearly
2 µs
2 µs
tlate
26 µs
26 µs
tsim
38 µs
38 µs
training epochs
300
300
batch size
50
100
optimizer
Adam
Adam
Adam parameter β
(0.9, 0.999)
(0.9, 0.999)
Adam parameter ϵ
10−8
10−8
learning rate
0.0005
0.001
lr-scheduler
StepLR
StepLR
lr-schedule step size
50
50
lr-scheduler γ
0.5
0.5
readout reg.
0.0004
0.0004
Table 3. Parameters of neuron dynamics, network and training used to produce the hardware-in-the-loop results on the Yin-Yang dataset
using EventProp and SuperSpike surrogate gradients as shown in Figure 3 and Table 1.

Event-based Backpropagation for Analog Neuromorphic Hardware
Parameter
EventProp
SuperSpike
size input
5
5
size hidden layer
120
120
size output layer
3
3
weight init [mean, stdev]
hidden
[1.0, 0.4]
[1.0, 0.4]
output
[0.01, 0.1]
[0.01, 0.1]
dt [τs]
0.01
0.01
tbias [τs]
0
0
tearly [τs]
0
0
tlate [τs]
4
4
tsim [τs]
6
6
training epochs
200
200
batch size
25
50
optimizer
Adam
Adam
Adam parameter β
(0.9, 0.999)
(0.9, 0.999)
Adam parameter ϵ
10−8
10−8
learning rate
0.0005
0.0005
lr-scheduler
StepLR
StepLR
lr-schedule step size
50
50
lr-scheduler γ
0.5
0.5
readout reg.
0.0
0.0
Table 4. Parameters of neuron dynamics, network and training used to produce the software-only results on the Yin-Yang dataset using
EventProp and SuperSpike surrogate gradients as listed Table 1. We chose τs = τm and times are given in units of τs.

Event-based Backpropagation for Analog Neuromorphic Hardware
D. EventProp Dataﬂow
EventPropSynapse
forward
w
backward
grad w
EventPropNeuron
forward
backward
spikes
projection
spikes
current
spikes
membrane
grad spikes
grad membrane
grad projection
grad spikes
hw data
Figure 6. Dataﬂow occurring during hardware-in-the-loop training. EventPropSynapse returns the product of the input spikes and
the weight w stacked on top of an empty tensor with same shape. EventPropNeuron either returns the hardware observations or,
if those are not present, computes the forward trajectories in simulation. In the backward pass, the adjoint dynamics are computed
using the stored spikes from the forward-call and a stacked tensor consisting of τsλI and (λI −λV) is returned. Note that returning
these stacked tensors is only possible due to the output of EventPropSynapse already being a stacked tensor with same shape. In
backward direction, EventPropSynapse backpropagates τsλ⊤
I zpre, the gradient with respect to the weight according to the EventProp
algorithm (Wunderlich & Pehle, 2021), and (λI −λV)w, the gradient with respect to the pre-synaptic spikes.

