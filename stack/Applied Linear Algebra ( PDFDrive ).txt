Undergraduate Texts in Mathematics
Peter J. Olver · Chehrzad Shakiban
Applied 
Linear 
Algebra
 Second Edition 

Undergraduate Texts in Mathematics

Undergraduate Texts in Mathematics are generally aimed at third- and fourth-year undergraduate
mathematics students at North American universities. These texts strive to provide students and teachers
with new perspectives and novel approaches. The books include motivation that guides the reader to
an appreciation of interrelations among different aspects of the subject. They feature examples that 
illustrate key concepts as well as exercises that strengthen understanding.
More information about this series at http://www.springer.com/series/666
Undergraduate Texts in Mathematics
Series Editors:
Sheldon Axler
San Francisco State University, San Francisco, CA, USA
Kenneth Ribet
University of California, Berkeley, CA, USA
Advisory Board:
Colin Adams, Williams College
David A. Cox, Amherst College
L. Craig Evans, University of California, Berkeley
Pamela Gorkin, Bucknell University
Roger E. Howe, Yale University
Michael Orrison, Harvey Mudd College
Lisette G. de Pillis, Harvey Mudd College
Jill Pipher, Brown University
Fadil Santosa, University of Minnesota

Peter J. Olver • Chehrzad Shakiban 
Applied Linear Algebra 
Second Edition 

Peter J. Olver 
School of Mathematics 
University of Minnesota 
Minneapolis, MN
Chehrzad Shakiban 
Department of Mathematics 
University of St. Thomas 
St. Paul, MN
 ISSN 0172-6056 
ISSN 2197-5604 (electronic) 
Undergraduate Texts in Mathematics 
ISBN 978-3-319-91040-6
 ISBN 978-3-319-910 41-3 (eB ook)
https://doi.org/10.1007/978-3-319-91041-3 
 
Library of Congress Control Number: 2018941541
 
Mathematics Subject Classification (2010): 15-01, 15AXX, 65FXX, 05C50, 34A30, 62H25, 65D05, 65D07, 65D18 
 
This work is subject to copyright. All rights are reserved by the Publisher, whether the whole or part of the material is 
concerned, specifically the rights of translation, reprinting, reuse of illustrations, recitation, broadcasting, reproduction on 
microfilms or in any other physical way, and transmission or information storage and retrieval, electronic adaptation, 
computer software, or by similar or dissimilar methodology now known or hereafter developed. 
The use of general descriptive names, registered names, trademarks, service marks, etc. in this publication does not imply, 
even in the absence of a specific statement, that such names are exempt from the relevant protective laws and regulations and 
therefore free for general use. 
The publisher, the authors and the editors are safe to assume that the advice and information in this book are believed to be 
true and accurate at the date of publication. Neither the publisher nor the authors or the editors give a warranty, express or 
implied, with respect to the material contained herein or for any errors or omissions that may have been made. The publisher 
remains neutral with regard to jurisdictional claims in published maps and institutional affiliations. 
 
Printed on acid-free paper 
 
This Springer imprint is published by the registered company Springer International Publishing AG part of Springer Nature. 
The registered company address is: Gewerbestrasse 11, 6330 Cham, Switzerland 
1st edition: © 2006 Pearson Education, Inc., Pearson Prentice Hall, Pearson Education, Inc., Upper Saddle River, NJ 07458 
2nd edition: © Springer International Publishing AG, part of Springer Nature 2018 
USA 
USA 

To our children and grandchildren.
You are the light of our life.

Preface
Applied mathematics rests on two central pillars: calculus and linear algebra. While cal-
culus has its roots in the universal laws of Newtonian physics, linear algebra arises from a
much more mundane issue: the need to solve simple systems of linear algebraic equations.
Despite its humble origins, linear algebra ends up playing a comparably profound role in
both applied and theoretical mathematics, as well as in all of science and engineering,
including computer science, data analysis and machine learning, imaging and signal pro-
cessing, probability and statistics, economics, numerical analysis, mathematical biology,
and many other disciplines. Nowadays, a proper grounding in both calculus and linear al-
gebra is an essential prerequisite for a successful career in science, technology, engineering,
statistics, data science, and, of course, mathematics.
Since Newton, and, to an even greater extent following Einstein, modern science has
been confronted with the inherent nonlinearity of the macroscopic universe. But most of
our insight and progress is based on linear approximations. Moreover, at the atomic level,
quantum mechanics remains an inherently linear theory.
(The complete reconciliation
of linear quantum theory with the nonlinear relativistic universe remains the holy grail
of modern physics.)
Only with the advent of large-scale computers have we been able
to begin to investigate the full complexity of natural phenomena.
But computers rely
on numerical algorithms, and these in turn require manipulating and solving systems of
algebraic equations. Now, rather than just a handful of equations, we may be confronted
by gigantic systems containing thousands (or even millions) of unknowns. Without the
discipline of linear algebra to formulate systematic, eﬃcient solution algorithms, as well
as the consequent insight into how to proceed when the numerical solution is insuﬃciently
accurate, we would be unable to make progress in the linear regime, let alone make sense
of the truly nonlinear physical universe.
Linear algebra can thus be viewed as the mathematical apparatus needed to solve po-
tentially huge linear systems, to understand their underlying structure, and to apply what
is learned in other contexts. The term “linear” is the key, and, in fact, it refers not just
to linear algebraic equations, but also to linear diﬀerential equations, both ordinary and
partial, linear boundary value problems, linear integral equations, linear iterative systems,
linear control systems, and so on. It is a profound truth that, while outwardly diﬀerent,
all linear systems are remarkably similar at their core. Basic mathematical principles such
as linear superposition, the interplay between homogeneous and inhomogeneous systems,
the Fredholm alternative characterizing solvability, orthogonality, positive deﬁniteness and
minimization principles, eigenvalues and singular values, and linear iteration, to name but
a few, reoccur in surprisingly many ostensibly unrelated contexts.
In the late nineteenth and early twentieth centuries, mathematicians came to the real-
ization that all of these disparate techniques could be subsumed in the ediﬁce now known
as linear algebra. Understanding, and, more importantly, exploiting the apparent simi-
larities between, say, algebraic equations and diﬀerential equations, requires us to become
more sophisticated — that is, more abstract — in our mode of thinking. The abstraction
vii

viii
Preface
process distills the essence of the problem away from all its distracting particularities, and,
seen in this light, all linear systems rest on a common mathematical framework. Don’t be
afraid! Abstraction is not new in your mathematical education. In elementary algebra,
you already learned to deal with variables, which are the abstraction of numbers. Later,
the abstract concept of a function formalized particular relations between variables, say
distance, velocity, and time, or mass, acceleration, and force. In linear algebra, the abstrac-
tion is raised to yet a further level, in that one views apparently diﬀerent types of objects
(vectors, matrices, functions, . . . ) and systems (algebraic, diﬀerential, integral, . . . ) in a
common conceptual framework. (And this is by no means the end of the mathematical
abstraction process; modern category theory, [37], abstractly unites diﬀerent conceptual
frameworks.)
In applied mathematics, we do not introduce abstraction for its intrinsic beauty. Our
ultimate purpose is to develop eﬀective methods and algorithms for applications in science,
engineering, computing, statistics, data science, etc. For us, abstraction is driven by the
need for understanding and insight, and is justiﬁed only if it aids in the solution to real
world problems and the development of analytical and computational tools. Whereas to the
beginning student the initial concepts may seem designed merely to bewilder and confuse,
one must reserve judgment until genuine applications appear. Patience and perseverance
are vital. Once we have acquired some familiarity with basic linear algebra, signiﬁcant,
interesting applications will be readily forthcoming. In this text, we encounter graph theory
and networks, mechanical structures, electrical circuits, quantum mechanics, the geometry
underlying computer graphics and animation, signal and image processing, interpolation
and approximation, dynamical systems modeled by linear diﬀerential equations, vibrations,
resonance, and damping, probability and stochastic processes, statistics, data analysis,
splines and modern font design, and a range of powerful numerical solution algorithms, to
name a few. Further applications of the material you learn here will appear throughout
your mathematical and scientiﬁc career.
This textbook has two interrelated pedagogical goals.
The ﬁrst is to explain basic
techniques that are used in modern, real-world problems. But we have not written a mere
mathematical cookbook — a collection of linear algebraic recipes and algorithms.
We
believe that it is important for the applied mathematician, as well as the scientist and
engineer, not just to learn mathematical techniques and how to apply them in a variety
of settings, but, even more importantly, to understand why they work and how they are
derived from ﬁrst principles. In our approach, applications go hand in hand with theory,
each reinforcing and inspiring the other. To this end, we try to lead the reader through the
reasoning that leads to the important results. We do not shy away from stating theorems
and writing out proofs, particularly when they lead to insight into the methods and their
range of applicability.
We hope to spark that eureka moment, when you realize “Yes,
of course!
I could have come up with that if I’d only sat down and thought it out.”
Most concepts in linear algebra are not all that diﬃcult at their core, and, by grasping
their essence, not only will you know how to apply them in routine contexts, you will
understand what may be required to adapt to unusual or recalcitrant problems. And, the
further you go on in your studies or work, the more you realize that very few real-world
problems ﬁt neatly into the idealized framework outlined in a textbook. So it is (applied)
mathematical reasoning and not mere linear algebraic technique that is the core and raison
d’ˆetre of this text!
Applied mathematics can be broadly divided into three mutually reinforcing compo-
nents. The ﬁrst is modeling — how one derives the governing equations from physical

Preface
ix
principles. The second is solution techniques and algorithms — methods for solving the
model equations. The third, perhaps least appreciated but in many ways most important,
are the frameworks that incorporate disparate analytical methods into a few broad themes.
The key paradigms of applied linear algebra to be covered in this text include
• Gaussian Elimination and factorization of matrices;
• linearity and linear superposition;
• span, linear independence, basis, and dimension;
• inner products, norms, and inequalities;
• compatibility of linear systems via the Fredholm alternative;
• positive deﬁniteness and minimization principles;
• orthonormality and the Gram–Schmidt process;
• least squares solutions, interpolation, and approximation;
• linear functions and linear and aﬃne transformations;
• eigenvalues and eigenvectors/eigenfunctions;
• singular values and principal component analysis;
• linear iteration, including Markov processes and numerical solution schemes;
• linear systems of ordinary diﬀerential equations, stability, and matrix exponentials;
• vibrations, quasi-periodicity, damping, and resonance; .
These are all interconnected parts of a very general applied mathematical ediﬁce of remark-
able power and practicality. Understanding such broad themes of applied mathematics is
our overarching objective. Indeed, this book began life as a part of a much larger work,
whose goal is to similarly cover the full range of modern applied mathematics, both lin-
ear and nonlinear, at an advanced undergraduate level. The second installment is now in
print, as the ﬁrst author’s text on partial diﬀerential equations, [61], which forms a nat-
ural extension of the linear analytical methods and theoretical framework developed here,
now in the context of the equilibria and dynamics of continuous media, Fourier analysis,
and so on. Our inspirational source was and continues to be the visionary texts of Gilbert
Strang, [79, 80]. Based on students’ reactions, our goal has been to present a more linearly
ordered and less ambitious development of the subject, while retaining the excitement and
interconnectedness of theory and applications that is evident in Strang’s works.
Syllabi and Prerequisites
This text is designed for three potential audiences:
• A beginning, in-depth course covering the fundamentals of linear algebra and its appli-
cations for highly motivated and mathematically mature students.
• A second undergraduate course in linear algebra, with an emphasis on those methods
and concepts that are important in applications.
• A beginning graduate-level course in linear mathematics for students in engineering,
physical science, computer science, numerical analysuis, statistics, and even math-
ematical biology, ﬁnance, economics, social sciences, and elsewhere, as well as
master’s students in applied mathematics.
Although most students reading this book will have already encountered some basic
linear algebra — matrices, vectors, systems of linear equations, basic solution techniques,
etc. — the text makes no such assumptions. Indeed, the ﬁrst chapter starts at the very
beginning by introducing linear algebraic systems, matrices, and vectors, followed by very

x
Preface
basic Gaussian Elimination.
We do assume that the reader has taken a standard two
year calculus sequence. One-variable calculus — derivatives and integrals — will be used
without comment; multivariable calculus will appear only ﬂeetingly and in an inessential
way. The ability to handle scalar, constant coeﬃcient linear ordinary diﬀerential equations
is also assumed, although we do brieﬂy review elementary solution techniques in Chapter 7.
Proofs by induction will be used on occasion.
But the most essential prerequisite is a
certain degree of mathematical maturity and willingness to handle the increased level of
abstraction that lies at the heart of contemporary linear algebra.
Survey of Topics
In addition to introducing the fundamentals of matrices, vectors, and Gaussian Elimination
from the beginning, the initial chapter delves into perhaps less familiar territory, such as
the (permuted) LU and LDV decompositions, and the practical numerical issues underly-
ing the solution algorithms, thereby highlighting the computational eﬃciency of Gaussian
Elimination coupled with Back Substitution versus methods based on the inverse matrix
or determinants, as well as the use of pivoting to mitigate possibly disastrous eﬀects of
numerical round-oﬀerrors.
Because the goal is to learn practical algorithms employed
in contemporary applications, matrix inverses and determinants are de-emphasized —
indeed, the most eﬃcient way to compute a determinant is via Gaussian Elimination,
which remains the key algorithm throughout the initial chapters.
Chapter 2 is the heart of linear algebra, and a successful course rests on the students’
ability to assimilate the absolutely essential concepts of vector space, subspace, span, linear
independence, basis, and dimension. While these ideas may well have been encountered
in an introductory ordinary diﬀerential equation course, it is rare, in our experience, that
students at this level are at all comfortable with them. The underlying mathematics is not
particularly diﬃcult, but enabling the student to come to grips with a new level of abstrac-
tion remains the most challenging aspect of the course. To this end, we have included a
wide range of illustrative examples. Students should start by making sure they understand
how a concept applies to vectors in Euclidean space Rn before pressing on to less famil-
iar territory. While one could design a course that completely avoids inﬁnite-dimensional
function spaces, we maintain that, at this level, they should be integrated into the subject
right from the start. Indeed, linear analysis and applied mathematics, including Fourier
methods, boundary value problems, partial diﬀerential equations, numerical solution tech-
niques, signal processing, control theory, modern physics, especially quantum mechanics,
and many, many other ﬁelds, both pure and applied, all rely on basic vector space con-
structions, and so learning to deal with the full range of examples is the secret to future
success. Section 2.5 then introduces the fundamental subspaces associated with a matrix
— kernel (null space), image (column space), coimage (row space), and cokernel (left null
space) — leading to what is known as the Fundamental Theorem of Linear Algebra which
highlights the remarkable interplay between a matrix and its transpose. The role of these
spaces in the characterization of solutions to linear systems, e.g., the basic superposition
principles, is emphasized. The ﬁnal Section 2.6 covers a nice application to graph theory,
in preparation for later developments.
Chapter 3 discusses general inner products and norms, using the familiar dot product
and Euclidean distance as motivational examples.
Again, we develop both the ﬁnite-
dimensional and function space cases in tandem. The fundamental Cauchy–Schwarz in-
equality is easily derived in this abstract framework, and the more familiar triangle in-

Preface
xi
equality, for norms derived from inner products, is a simple consequence. This leads to
the deﬁnition of a general norm and the induced matrix norm, of fundamental importance
in iteration, analysis, and numerical methods. The classiﬁcation of inner products on Eu-
clidean space leads to the important class of positive deﬁnite matrices. Gram matrices,
constructed out of inner products of elements of inner product spaces, are a particularly
fruitful source of positive deﬁnite and semi-deﬁnite matrices, and reappear throughout the
text. Tests for positive deﬁniteness rely on Gaussian Elimination and the connections be-
tween the LDLT factorization of symmetric matrices and the process of completing the
square in a quadratic form. We have deferred treating complex vector spaces until the
ﬁnal section of this chapter — only the deﬁnition of an inner product is not an evident
adaptation of its real counterpart.
Chapter 4 exploits the many advantages of orthogonality. The use of orthogonal and
orthonormal bases creates a dramatic speed-up in basic computational algorithms. Orthog-
onal matrices, constructed out of orthogonal bases, play a major role, both in geometry
and graphics, where they represent rigid rotations and reﬂections, as well as in notable
numerical algorithms. The orthogonality of the fundamental matrix subspaces leads to a
linear algebraic version of the Fredholm alternative for compatibility of linear systems. We
develop several versions of the basic Gram–Schmidt process for converting an arbitrary
basis into an orthogonal basis, used in particular to construct orthogonal polynomials and
functions. When implemented on bases of Rn, the algorithm becomes the celebrated QR
factorization of a nonsingular matrix. The ﬁnal section surveys an important application to
contemporary signal and image processing: the discrete Fourier representation of a sampled
signal, culminating in the justly famous Fast Fourier Transform.
Chapter 5 is devoted to solving the most basic multivariable minimization problem:
a quadratic function of several variables. The solution is reduced, by a purely algebraic
computation, to a linear system, and then solved in practice by, for example, Gaussian
Elimination.
Applications include ﬁnding the closest element of a subspace to a given
point, which is reinterpreted as the orthogonal projection of the element onto the subspace,
and results in the least squares solution to an incompatible linear system. Interpolation
of data points by polynomials, trigonometric function, splines, etc., and least squares ap-
proximation of discrete data and continuous functions are thereby handled in a common
conceptual framework.
Chapter 6 covers some striking applications of the preceding developments in mechanics
and electrical circuits. We introduce a general mathematical structure that governs a wide
range of equilibrium problems.
To illustrate, we start with simple mass–spring chains,
followed by electrical networks, and ﬁnish by analyzing the equilibrium conﬁgurations and
the stability properties of general structures. Extensions to continuous mechanical and
electrical systems governed by boundary value problems for ordinary and partial diﬀerential
equations can be found in the companion text [61].
Chapter 7 delves into the general abstract foundations of linear algebra, and includes
signiﬁcant applications to geometry.
Matrices are now viewed as a particular instance
of linear functions between vector spaces, which also include linear diﬀerential operators,
linear integral operators, quantum mechanical operators, and so on. Basic facts about linear
systems, such as linear superposition and the connections between the homogeneous and
inhomogeneous systems, which were already established in the algebraic context, are shown
to be of completely general applicability. Linear functions and slightly more general aﬃne
functions on Euclidean space represent basic geometrical transformations — rotations,
shears, translations, screw motions, etc. — and so play an essential role in modern computer

xii
Preface
graphics, movies, animation, gaming, design, elasticity, crystallography, symmetry, etc.
Further, the elementary transpose operation on matrices is viewed as a particular case
of the adjoint operation on linear functions between inner product spaces, leading to a
general theory of positive deﬁniteness that characterizes solvable quadratic minimization
problems, with far-reaching consequences for modern functional analysis, partial diﬀerential
equations, and the calculus of variations, all fundamental in physics and mechanics.
Chapters 8–10 are concerned with eigenvalues and their many applications, includ-
ing data analysis, numerical methods, and linear dynamical systems, both continuous
and discrete. After motivating the fundamental deﬁnition of eigenvalue and eigenvector
through the quest to solve linear systems of ordinary diﬀerential equations, the remainder
of Chapter 8 develops the basic theory and a range of applications, including eigenvector
bases, diagonalization, the Schur decomposition, and the Jordan canonical form. Practical
computational schemes for determining eigenvalues and eigenvectors are postponed until
Chapter 9. The ﬁnal two sections cover the singular value decomposition and principal
component analysis, of fundamental importance in modern statistical analysis and data
science.
Chapter 9 employs eigenvalues to analyze discrete dynamics, as governed by linear iter-
ative systems. The formulation of their stability properties leads us to deﬁne the spectral
radius and further develop matrix norms.
Section 9.3 contains applications to Markov
chains arising in probabilistic and stochastic processes. We then discuss practical alter-
natives to Gaussian Elimination for solving linear systems, including the iterative Jacobi,
Gauss–Seidel, and Successive Over–Relaxation (SOR) schemes, as well as methods for com-
puting eigenvalues and eigenvectors including the Power Method and its variants, and the
striking QR algorithm, including a new proof of its convergence. Section 9.6 introduces
more recent semi-direct iterative methods based on Krylov subspaces that are increasingly
employed to solve the large sparse linear systems arising in the numerical solution of partial
diﬀerential equations and elsewhere: Arnoldi and Lanczos methods, Conjugate Gradients
(CG), the Full Orthogonalization Method (FOM), and the Generalized Minimal Residual
Method (GMRES). The chapter concludes with a short introduction to wavelets, a power-
ful modern alternative to classical Fourier analysis, now used extensively throughout signal
processing and imaging science.
The ﬁnal Chapter 10 applies eigenvalues to linear dynamical systems modeled by systems
of ordinary diﬀerential equations. After developing basic solution techniques, the focus
shifts to understanding the qualitative properties of solutions and particularly the role
of eigenvalues in the stability of equilibria. The two-dimensional case is discussed in full
detail, culminating in a complete classiﬁcation of the possible phase portraits and stability
properties. Matrix exponentials are introduced as an alternative route to solving ﬁrst order
homogeneous systems, and are also applied to solve the inhomogeneous version, as well as
to geometry, symmetry, and group theory. Our ﬁnal topic is second order linear systems,
which model dynamical motions and vibrations in mechanical structures and electrical
circuits. In the absence of frictional damping and instabilities, solutions are quasiperiodic
combinations of the normal modes. We ﬁnish by brieﬂy discussing the eﬀects of damping
and of periodic forcing, including its potentially catastrophic role in resonance.
Course Outlines
Our book includes far more material than can be comfortably covered in a single semester;
a full year’s course would be able to do it justice. If you do not have this luxury, several

Preface
xiii
possible semester and quarter courses can be extracted from the wealth of material and
applications.
First, the core of basic linear algebra that all students should know includes the following
topics, which are indexed by the section numbers where they appear:
• Matrices, vectors, Gaussian Elimination, matrix factorizations, Forward and
Back Substitution, inverses, determinants: 1.1–1.6, 1.8–1.9.
• Vector spaces, subspaces, linear independence, bases, dimension: 2.1–2.5.
• Inner products and their associated norms: 3.1–3.3.
• Orthogonal vectors, bases, matrices, and projections: 4.1–4.4.
• Positive deﬁnite matrices and minimization of quadratic functions: 3.4–3.5, 5.2
• Linear functions and linear and aﬃne transformations: 7.1–7.3.
• Eigenvalues and eigenvectors: 8.2–8.3.
• Linear iterative systems: 9.1–9.2.
With these in hand, a variety of thematic threads can be extracted, including:
• Minimization, least squares, data ﬁtting and interpolation: 4.5, 5.3–5.5.
• Dynamical systems: 8.4, 8.6 (Jordan canonical form), 10.1–10.4.
• Engineering applications: Chapter 6, 10.1–10.2, 10.5–10.6.
• Data analysis: 5.3–5.5, 8.5, 8.7–8.8.
• Numerical methods: 8.6 (Schur decomposition), 8.7, 9.1–9.2, 9.4–9.6.
• Signal processing: 3.6, 5.6, 9.7.
• Probabilistic and statistical applications: 8.7–8.8, 9.3.
• Theoretical foundations of linear algebra: Chapter 7.
For a ﬁrst semester or quarter course, we recommend covering as much of the core
as possible, and, if time permits, at least one of the threads, our own preference being
the material on structures and circuits.
One option for streamlining the syllabus is to
concentrate on ﬁnite-dimensional vector spaces, bypassing the function space material,
although this would deprive the students of important insight into the full scope of linear
algebra.
For a second course in linear algebra, the students are typically familiar with elemen-
tary matrix methods, including the basics of matrix arithmetic, Gaussian Elimination,
determinants, inverses, dot product and Euclidean norm, eigenvalues, and, often, ﬁrst or-
der systems of ordinary diﬀerential equations. Thus, much of Chapter 1 can be reviewed
quickly. On the other hand, the more abstract fundamentals, including vector spaces, span,
linear independence, basis, and dimension are, in our experience, still not fully mastered,
and one should expect to spend a signiﬁcant fraction of the early part of the course covering
these essential topics from Chapter 2 in full detail. Beyond the core material, there should
be time for a couple of the indicated threads depending on the audience and interest of the
instructor.
Similar considerations hold for a beginning graduate level course for scientists and engi-
neers. Here, the emphasis should be on applications required by the students, particularly
numerical methods and data analysis, and function spaces should be ﬁrmly built into the
class from the outset. As always, the students’ mastery of the ﬁrst ﬁve sections of Chapter 2
remains of paramount importance.

xiv
Preface
Comments on Individual Chapters
Chapter 1: On the assumption that the students have already seen matrices, vectors,
Gaussian Elimination, inverses, and determinants, most of this material will be review and
should be covered at a fairly rapid pace. On the other hand, the LU decomposition and the
emphasis on solution techniques centered on Forward and Back Substitution, in contrast to
impractical schemes involving matrix inverses and determinants, might be new. Sections
1.7, on the practical/numerical aspects of Gaussian Elimination, is optional.
Chapter 2: The crux of the course. A key decision is whether to incorporate inﬁnite-
dimensional vector spaces, as is recommended and done in the text, or to have an abbre-
viated syllabus that covers only ﬁnite-dimensional spaces, or, even more restrictively, only
Rn and subspaces thereof. The last section, on graph theory, can be skipped unless you
plan on covering Chapter 6 and (parts of) the ﬁnal sections of Chapters 9 and 10.
Chapter 3: Inner products and positive deﬁnite matrices are essential, but, under time
constraints, one can delay Section 3.3, on more general norms, as they begin to matter
only in the later stages of Chapters 8 and 9. Section 3.6, on complex vector spaces, can
be deferred until the discussions of complex eigenvalues, complex linear systems, and real
and complex solutions to linear iterative and diﬀerential equations; on the other hand, it
is required in Section 5.6, on discrete Fourier analysis.
Chapter 4: The basics of orthogonality, as covered in Sections 4.1–4.4, should be an
essential part of the students’ training, although one can certainly omit the ﬁnal subsection
in Sections 4.2 and 4.3. The ﬁnal section, on orthogonal polynomials, is optional.
Chapter 5: We recommend covering the solution of quadratic minimization problems
and at least the basics of least squares. The applications — approximation of data, interpo-
lation and approximation by polynomials, trigonometric functions, more general functions,
and splines, etc., are all optional, as is the ﬁnal section on discrete Fourier methods and
the Fast Fourier Transform.
Chapter 6 provides a welcome relief from the theory for the more applied students in the
class, and is one of our favorite parts to teach. While it may well be skipped, the material
is particularly appealing for a class with engineering students. One could specialize to just
the material on mass/spring chains and structures, or, alternatively, on electrical circuits
with the connections to spectral graph theory, based on Section 2.6, and further developed
in Section 8.7.
Chapter 7: The ﬁrst third of this chapter, on linear functions, linear and aﬃne trans-
formations, and geometry, is part of the core. This remainder of the chapter recasts many
of the linear algebraic techniques already encountered in the context of matrices and vec-
tors in Euclidean space in a more general abstract framework, and could be skimmed over
or entirely omitted if time is an issue, with the relevant constructions introduced in the
context of more concrete developments, as needed.
Chapter 8: Eigenvalues are absolutely essential. The motivational material based on
solving systems of diﬀerential equations in Section 8.1 can be skipped over. Sections 8.2
and 8.3 are the heart of the matter.
Of the remaining sections, the material on sym-
metric matrices should have the highest priority, leading to singular values and principal
component analysis and a variety of numerical methods.

Preface
xv
Chapter 9: If time permits, the ﬁrst two sections are well worth covering. For a numeri-
cally oriented class, Sections 9.4–9.6 would be a priority, whereas Section 9.3 studies Markov
processes — an appealing probabilistic/stochastic application. The chapter concludes with
an optional introduction to wavelets, which is somewhat oﬀ-topic, but nevertheless serves
to combine orthogonality and iterative methods in a compelling and important modern
application.
Chapter 10 is devoted to linear systems of ordinary diﬀerential equations, their solutions,
and their stability properties. The basic techniques will be a repeat to students who have
already taken an introductory linear algebra and ordinary diﬀerential equations course, but
the more advanced material will be new and of interest.
Changes from the First Edition
For the Second Edition, we have revised and edited the entire manuscript, correcting all
known errors and typos, and, we hope, not introducing any new ones! Some of the existing
material has been rearranged. The most signiﬁcant change is having moved the chapter on
orthogonality to before the minimization and least squares chapter, since orthogonal vec-
tors, bases, and subspaces, as well as the Gram–Schmidt process and orthogonal projection
play an absolutely fundamental role in much of the later material. In this way, it is easier
to skip over Chapter 5 with minimal loss of continuity. Matrix norms now appear much
earlier in Section 3.3, since they are employed in several other locations. The second major
reordering is to switch the chapters on iteration and dynamics, in that the former is more
attuned to linear algebra, while the latter is oriented towards analysis. In the same vein,
space constraints compelled us to delete the last chapter of the ﬁrst edition, which was on
boundary value problems. Although this material serves to emphasize the importance of
the abstract linear algebraic techniques developed throughout the text, now extended to
inﬁnite-dimensional function spaces, the material contained therein can now all be found
in the ﬁrst author’s Springer Undergraduate Text in Mathematics, Introduction to Partial
Diﬀerential Equations, [61], with the exception of the subsection on splines, which now
appears at the end of Section 5.5.
There are several signiﬁcant additions:
• In recognition of their increasingly essential role in modern data analysis and statis-
tics, Section 8.7, on singular values, has been expanded, continuing into the new
Section 8.8, on Principal Component Analysis, which includes a brief introduction
to basic statistical data analysis.
• We have added a new Section 9.6, on Krylov subspace methods, which are increasingly
employed to devise eﬀective and eﬃcient numerical solution schemes for sparse linear
systems and eigenvalue calculations.
• Section 8.4 introduces and characterizes invariant subspaces, in recognition of their
importance to dynamical systems, both ﬁnite- and inﬁnite-dimensional, as well as
linear iterative systems, and linear control systems. (Much as we would have liked
also to add material on linear control theory, space constraints ultimately interfered.)
• We included some basics of spectral graph theory, of importance in contemporary
theoretical computer science, data analysis, networks, imaging, etc., starting in Sec-
tion 2.6 and continuing to the graph Laplacian, introduced, in the context of elec-
trical networks, in Section 6.2, along with its spectrum — eigenvalues and singular
values — in Section 8.7.

xvi
Preface
• We decided to include a short Section 9.7, on wavelets. While this perhaps ﬁts more
naturally with Section 5.6, on discrete Fourier analysis, the convergence proofs rely
on the solution to an iterative linear system and hence on preceding developments
in Chapter 9.
• A number of new exercises have been added, in the new sections and also scattered
throughout the text.
Following the advice of friends, colleagues, and reviewers, we have also revised some
of the less standard terminology used in the ﬁrst edition to bring it closer to the more
commonly accepted practices. Thus “range” is now “image” and “target space” is now
“codomain”. The terms “special lower/upper triangular matrix” are now “lower/upper
unitriangular matrix”, thus drawing attention to their unipotence. On the other hand, the
term “regular” for a square matrix admitting an LU factorization has been kept, since
there is really no suitable alternative appearing in the literature. Finally, we decided to
retain our term “complete” for a matrix that admits a complex eigenvector basis, in lieu of
“diagonalizable” (which depends upon whether one deals in the real or complex domain),
“semi-simple”, or “perfect”. This choice permits us to refer to a “complete eigenvalue”,
independent of the underlying status of the matrix.
Exercises and Software
Exercises appear at the end of almost every subsection, and come in a medley of ﬂavors.
Each exercise set starts with some straightforward computational problems to test students’
comprehension and reinforce the new techniques and ideas. Ability to solve these basic
problems should be thought of as a minimal requirement for learning the material. More
advanced and theoretical exercises tend to appear later on in the set. Some are routine,
but others are challenging computational problems, computer-based exercises and projects,
details of proofs that were not given in the text, additional practical and theoretical results
of interest, further developments in the subject, etc. Some will challenge even the most
advanced student.
As a guide, some of the exercises are marked with special signs:
♦
indicates an exercise that is used at some point in the text, or is important for further
development of the subject.
♥
indicates a project — usually an exercise with multiple interdependent parts.
♠indicates an exercise that requires (or at least strongly recommends) use of a computer.
The student could either be asked to write their own computer code in, say, Matlab,
Mathematica, Maple, etc., or make use of pre-existing software packages.
♣= ♠+ ♥
indicates a computer project.
Advice to instructors: Don’t be afraid to assign only a couple of parts of a multi-part
exercise. We have found the True/False exercises to be a particularly useful indicator of
a student’s level of understanding. Emphasize to the students that a full answer is not
merely a T or F, but must include a detailed explanation of the reason, e.g., a proof, or a
counterexample, or a reference to a result in the text, etc.

Preface
xvii
Conventions and Notations
Note: A full symbol and notation index can be found at the end of the book.
Equations are numbered consecutively within chapters, so that, for example, (3.12)
refers to the 12th equation in Chapter 3. Theorems, Lemmas, Propositions, Deﬁnitions,
and Examples are also numbered consecutively within each chapter, using a common index.
Thus, in Chapter 1, Lemma 1.2 follows Deﬁnition 1.1, and precedes Theorem 1.3 and
Example 1.4. We ﬁnd this numbering system to be the most conducive for navigating
through the book.
References to books, papers, etc., are listed alphabetically at the end of the text, and
are referred to by number. Thus, [61] indicates the 61st listed reference, which happens to
be the ﬁrst author’s partial diﬀerential equations text.
Q.E.D. is placed at the end of a proof, being the abbreviation of the classical Latin phrase
quod erat demonstrandum, which can be translated as “what was to be demonstrated”.
R, C, Z, Q denote, respectively, the real numbers, the complex numbers, the integers,
and the rational numbers. We use e ≈2.71828182845904 . . . to denote the base of the
natural logarithm, π = 3.14159265358979 . . . for the area of a circle of unit radius, and i
to denote the imaginary unit, i.e., one of the two square roots of −1, the other being −i .
The absolute value of a real number x is denoted by | x |; more generally, | z | denotes the
modulus of the complex number z.
We consistently use boldface lowercase letters, e.g., v, x, a, to denote vectors (almost
always column vectors), whose entries are the corresponding non-bold subscripted letter:
v1, xi, an, etc. Matrices are denoted by ordinary capital letters, e.g., A, C, K, M — but
not all such letters refer to matrices; for instance, V often refers to a vector space, L to
a linear function, etc. The entries of a matrix, say A, are indicated by the corresponding
subscripted lowercase letters, aij being the entry in its ith row and jth column.
We use the standard notations
n

i=1
ai = a1 + a2 + · · · + an,
n

i=1
ai = a1a2 · · · an,
for the sum and product of the quantities a1, . . . , an.
We use max and min to denote
maximum and minimum, respectively, of a closed subset of R.
Modular arithmetic is
indicated by j = k mod n, for j, k, n ∈Z with n > 0, to mean j −k is divisible by n.
We use S = { f | C } to denote a set, where f is a formula for the members of the
set and C is a list of conditions, which may be empty, in which case it is omitted. For
example, { x | 0 ≤x ≤1 } means the closed unit interval from 0 to 1, also denoted [0, 1],
while { ax2 + bx + c | a, b, c ∈R } is the set of real quadratic polynomials, and {0} is the
set consisting only of the number 0. We write x ∈S to indicate that x is an element of the
set S, while y ̸∈S says that y is not an element. The cardinality, or number of elements,
in the set A, which may be inﬁnite, is denoted by #A. The union and intersection of the
sets A, B are respectively denoted by A ∪B and A ∩B. The subset notation A ⊂B
includes the possibility that the sets might be equal, although for emphasis we sometimes
write A ⊆B, while A ⊊B speciﬁcally implies that A ̸= B. We can also write A ⊂B as
B ⊃A. We use B \ A = { x | x ∈B, x ̸∈A } to denote the set-theoretic diﬀerence, meaning
all elements of B that do not belong to A.

xviii
Preface
An arrow →is used in two senses: ﬁrst, to indicate convergence of a sequence: xn →x⋆
as n →∞; second, to indicate a function, so f: X →Y means that f deﬁnes a function
from the domain set X to the codomain set Y , written y = f(x) ∈Y for x ∈X. We use
≡to emphasize when two functions agree everywhere, so f(x) ≡1 means that f is the
constant function, equal to 1 at all values of x. Composition of functions is denoted f ◦g.
Angles are always measured in radians (although occasionally degrees will be mentioned
in descriptive sentences). All trigonometric functions, cos, sin, tan, sec, etc., are evaluated
on radians. (Make sure your calculator is locked in radian mode!)
As usual, we denote the natural exponential function by ex. We always use log x for
its inverse — the natural (base e) logarithm (never the ugly modern version ln x), while
loga x = log x/ log a is used for logarithms with base a.
We follow the reference tome [59] (whose mathematical editor is the ﬁrst author’s father)
and use ph z for the phase of a complex number. We prefer this to the more common term
“argument”, which is also used to refer to the argument of a function f(z), while “phase”
is completely unambiguous and hence to be preferred.
We will employ a variety of standard notations for derivatives. In the case of ordinary
derivatives, the most basic is the Leibnizian notation du
dx for the derivative of u with
respect to x; an alternative is the Lagrangian prime notation u′. Higher order derivatives
are similar, with u′′ denoting d2u
dx2 , while u(n) denotes the nth order derivative dnu
dxn . If the
function depends on time, t, instead of space, x, then we use the Newtonian dot notation,
u = du
dt ,
u = d2u
dt2 . We use the full Leibniz notation ∂u
∂x , ∂u
∂t , ∂2u
∂x2 , ∂2u
∂x ∂t , for partial
derivatives of functions of several variables. All functions are assumed to be suﬃciently
smooth that any indicated derivatives exist and mixed partial derivatives are equal, cf. [2].
Deﬁnite integrals are denoted by
 b
a
f(x) dx, while

f(x) dx is the corresponding
indeﬁnite integral or anti-derivative. In general, limits are denoted by lim
x →y , while
lim
x →y+
and
lim
x →y−are used to denote the two one-sided limits in R.

Preface
xix
History and Biography
Mathematics is both a historical and a social activity, and many of the algorithms, theo-
rems, and formulas are named after famous (and, on occasion, not-so-famous) mathemati-
cians, scientists, engineers, etc. — usually, but not necessarily, the one(s) who ﬁrst came up
with the idea. We try to indicate ﬁrst names, approximate dates, and geographic locations
of most of the named contributors. Readers who are interested in additional historical de-
tails, complete biographies, and, when available, portraits or photos, are urged to consult
the wonderful University of St. Andrews MacTutor History of Mathematics archive:
http://www-history.mcs.st-and.ac.uk
Some Final Remarks
To the student:
You are about to learn modern applied linear algebra. We hope you
enjoy the experience and proﬁt from it in your future studies and career. (Indeed, we
recommended holding onto this book to use for future reference.) Please send us your
comments, suggestions for improvement, along with any errors you might spot. Did you
ﬁnd our explanations helpful or confusing? Were enough examples included in the text?
Were the exercises of suﬃcient variety and at an appropriate level to enable you to learn
the material?
To the instructor: Thank you for adopting our text! We hope you enjoy teaching from
it as much as we enjoyed writing it. Whatever your experience, we want to hear from you.
Let us know which parts you liked and which you didn’t. Which sections worked and which
were less successful. Which parts your students enjoyed, which parts they struggled with,
and which parts they disliked. How can we improve it?
Like every author, we sincerely hope that we have written an error-free text. Indeed, all
known errors in the ﬁrst edition have been corrected here. On the other hand, judging from
experience, we know that, no matter how many times you proofread, mistakes still manage
to sneak through. So we ask your indulgence to correct the few (we hope) that remain.
Even better, email us with your questions, typos, mathematical errors and obscurities,
comments, suggestions, etc.
The second edition’s dedicated web site
http://www.math.umn.edu/∼olver/ala2.html
will contain a list of known errors, commentary, feedback, and resources, as well as a
number of illustrative Matlab programs that we’ve used when teaching the course. Links
to the Selected Solutions Manual will also be posted there.

xx
Preface
Acknowledgments
First, let us express our profound gratitude to Gil Strang for his continued encouragement
from the very beginning of this undertaking. Readers familiar with his groundbreaking
texts and remarkable insight can readily ﬁnd his inﬂuence throughout our book.
We
thank Pavel Belik, Tim Garoni, Donald Kahn, Markus Keel, Cristina Santa Marta, Nil-
ima Nigam, Greg Pierce, Fadil Santosa, Wayne Schmaedeke, Jackie Shen, Peter Shook,
Thomas Scoﬁeld, and Richard Varga, as well as our classes and students, particularly Ta-
iala Carvalho, Colleen Duﬀy, and Ryan Lloyd, and last, but certainly not least, our late
father/father-in-law Frank W.J. Olver and son Sheehan Olver, for proofreading, correc-
tions, remarks, and useful suggestions that helped us create the ﬁrst edition. We acknowl-
edge Mikhail Shvartsman’s contributions to the arduous task of writing out the solutions
manual.
We also acknowledge the helpful feedback from the reviewers of the original
manuscript: Augustin Banyaga, Robert Cramer, James Curry, Jerome Dancis, Bruno
Harris, Norman Johnson, Cerry Klein, Doron Lubinsky, Juan Manfredi, Fabio Augusto
Milner, Tzuong-Tsieng Moh, Paul S. Muhly, Juan Carlos ´Alvarez Paiva, John F. Rossi,
Brian Shader, Shagi-Di Shih, Tamas Wiandt, and two anonymous reviewers.
We thank many readers and students for their strongly encouraging remarks, that cumu-
latively helped inspire us to contemplate making this new edition. We would particularly
like to thank Nihat Bayhan, Joe Benson, James Broomﬁeld, Juan Cockburn, Richard Cook,
Stephen DeSalvo, Anne Dougherty, Ken Driessel, Kathleen Fuller, Mary Halloran, Stu-
art Hastings, David Hiebeler, Jeﬀrey Humpherys, Roberta Jaskolski, Tian-Jun Li, James
Meiss, Willard Miller, Jr., Sean Rostami, Arnd Scheel, Timo Sch¨urg, David Tieri, Peter
Webb, Timothy Welle, and an anonymous reviewer for their comments on, suggestions for,
and corrections to the three printings of the ﬁrst edition that have led to this improved
second edition.
We particularly want to thank Linda Ness for extensive help with the
sections on SVD and PCA, including suggestions for some of the exercises. We also thank
David Kramer for his meticulous proofreading of the text.
And of course, we owe an immense debt to Loretta Bartolini and Achi Dosanjh at
Springer, ﬁrst for encouraging us to take on a second edition, and then for their willingness
to work with us to produce the book you now have in hand — especially Loretta’s unwa-
vering support, patience, and advice during the preparation of the manuscript, including
encouraging us to adopt and helping perfect the full-color layout, which we hope you enjoy.
Peter J. Olver
University of Minnesota
olver@umn.edu
Cheri Shakiban
University of St. Thomas
cshakiban@stthomas.edu
Minnesota, March 2018

Table of Contents
Preface
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
vii
Chapter 1.
Linear Algebraic Systems . . . . . . . . . . . . . . .
1
1.1. Solution of Linear Systems
. . . . . . . . . . . . . . . . . . . .
1
1.2. Matrices and Vectors . . . . . . . . . . . . . . . . . . . . . . .
3
Matrix Arithmetic
. . . . . . . . . . . . . . . . . . . . . .
5
1.3. Gaussian Elimination — Regular Case
. . . . . . . . . . . . . . . 12
Elementary Matrices
. . . . . . . . . . . . . . . . . . . . . 16
The LU Factorization . . . . . . . . . . . . . . . . . . . . . 18
Forward and Back Substitution . . . . . . . . . . . . . . . . . 20
1.4. Pivoting and Permutations
. . . . . . . . . . . . . . . . . . . . 22
Permutations and Permutation Matrices . . . . . . . . . . . . . 25
The Permuted LU Factorization
. . . . . . . . . . . . . . . . 27
1.5. Matrix Inverses
. . . . . . . . . . . . . . . . . . . . . . . . . 31
Gauss–Jordan Elimination . . . . . . . . . . . . . . . . . . . 35
Solving Linear Systems with the Inverse . . . . . . . . . . . . . 40
The LDV Factorization . . . . . . . . . . . . . . . . . . . . 41
1.6. Transposes and Symmetric Matrices
. . . . . . . . . . . . . . . . 43
Factorization of Symmetric Matrices . . . . . . . . . . . . . . . 45
1.7. Practical Linear Algebra
. . . . . . . . . . . . . . . . . . . . . 48
Tridiagonal Matrices
. . . . . . . . . . . . . . . . . . . . . 52
Pivoting Strategies
. . . . . . . . . . . . . . . . . . . . . . 55
1.8. General Linear Systems . . . . . . . . . . . . . . . . . . . . . . 59
Homogeneous Systems . . . . . . . . . . . . . . . . . . . . . 67
1.9. Determinants
. . . . . . . . . . . . . . . . . . . . . . . . . . 69
Chapter 2.
Vector Spaces and Bases
. . . . . . . . . . . . . . . 75
2.1. Real Vector Spaces . . . . . . . . . . . . . . . . . . . . . . . . 76
2.2. Subspaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . 81
2.3. Span and Linear Independence
. . . . . . . . . . . . . . . . . . 87
Linear Independence and Dependence . . . . . . . . . . . . . . 92
2.4. Basis and Dimension . . . . . . . . . . . . . . . . . . . . . . . 98
2.5. The Fundamental Matrix Subspaces
. . . . . . . . . . . . . . .
105
Kernel and Image . . . . . . . . . . . . . . . . . . . . . .
105
The Superposition Principle
. . . . . . . . . . . . . . . . .
110
Adjoint Systems, Cokernel, and Coimage . . . . . . . . . . . .
112
The Fundamental Theorem of Linear Algebra
. . . . . . . . .
114
2.6. Graphs and Digraphs . . . . . . . . . . . . . . . . . . . . . .
120
xxi

xxii
Table of Contents
Chapter 3.
Inner Products and Norms
. . . . . . . . . . . . .
129
3.1. Inner Products
. . . . . . . . . . . . . . . . . . . . . . . .
129
Inner Products on Function Spaces
. . . . . . . . . . . . . .
133
3.2. Inequalities
. . . . . . . . . . . . . . . . . . . . . . . . . .
137
The Cauchy–Schwarz Inequality
. . . . . . . . . . . . . . .
137
Orthogonal Vectors
. . . . . . . . . . . . . . . . . . . . .
140
The Triangle Inequality
. . . . . . . . . . . . . . . . . . .
142
3.3. Norms
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
144
Unit Vectors
. . . . . . . . . . . . . . . . . . . . . . . .
148
Equivalence of Norms
. . . . . . . . . . . . . . . . . . . .
150
Matrix Norms
. . . . . . . . . . . . . . . . . . . . . . .
153
3.4. Positive Deﬁnite Matrices . . . . . . . . . . . . . . . . . . . .
156
Gram Matrices . . . . . . . . . . . . . . . . . . . . . . .
161
3.5. Completing the Square . . . . . . . . . . . . . . . . . . . . .
166
The Cholesky Factorization
. . . . . . . . . . . . . . . . .
171
3.6. Complex Vector Spaces . . . . . . . . . . . . . . . . . . . . .
172
Complex Numbers
. . . . . . . . . . . . . . . . . . . . .
173
Complex Vector Spaces and Inner Products
. . . . . . . . . .
177
Chapter 4.
Orthogonality . . . . . . . . . . . . . . . . . . .
183
4.1. Orthogonal and Orthonormal Bases
. . . . . . . . . . . . . . .
184
Computations in Orthogonal Bases
. . . . . . . . . . . . . .
188
4.2. The Gram–Schmidt Process . . . . . . . . . . . . . . . . . . .
192
Modiﬁcations of the Gram–Schmidt Process
. . . . . . . . . .
197
4.3. Orthogonal Matrices . . . . . . . . . . . . . . . . . . . . . .
200
The QR Factorization . . . . . . . . . . . . . . . . . . . .
205
Ill-Conditioned Systems and Householder’s Method
. . . . . . .
208
4.4. Orthogonal Projections and Orthogonal Subspaces . . . . . . . . .
212
Orthogonal Projection . . . . . . . . . . . . . . . . . . . .
213
Orthogonal Subspaces . . . . . . . . . . . . . . . . . . . .
216
Orthogonality of the Fundamental Matrix Subspaces
and the Fredholm Alternative
. . .
221
4.5. Orthogonal Polynomials
. . . . . . . . . . . . . . . . . . . .
226
The Legendre Polynomials . . . . . . . . . . . . . . . . . .
227
Other Systems of Orthogonal Polynomials
. . . . . . . . . . .
231
Chapter 5.
Minimization and Least Squares . . . . . . . . . . .
235
5.1. Minimization Problems . . . . . . . . . . . . . . . . . . . . .
235
Equilibrium Mechanics
. . . . . . . . . . . . . . . . . . .
236
Solution of Equations
. . . . . . . . . . . . . . . . . . . .
236
The Closest Point . . . . . . . . . . . . . . . . . . . . . .
238
5.2. Minimization of Quadratic Functions . . . . . . . . . . . . . . .
239
5.3. The Closest Point
. . . . . . . . . . . . . . . . . . . . . . .
245
5.4. Least Squares . . . . . . . . . . . . . . . . . . . . . . . . .
250

Table of Contents
xxiii
5.5. Data Fitting and Interpolation
. . . . . . . . . . . . . . . . .
254
Polynomial Approximation and Interpolation . . . . . . . . . .
259
Approximation and Interpolation by General Functions
. . . . .
271
Least Squares Approximation in Function Spaces
. . . . . . . .
274
Orthogonal Polynomials and Least Squares . . . . . . . . . . .
277
Splines
. . . . . . . . . . . . . . . . . . . . . . . . . .
279
Compression and Denoising
. . . . . . . . . . . . . . . . .
293
The Fast Fourier Transform
. . . . . . . . . . . . . . . . .
295
Chapter 6.
Equilibrium . . . . . . . . . . . . . . . . . . . .
301
6.1. Springs and Masses
. . . . . . . . . . . . . . . . . . . . . .
301
Positive Deﬁniteness and the Minimization Principle
. . . . . .
309
6.2. Electrical Networks
. . . . . . . . . . . . . . . . . . . . . .
311
Batteries, Power, and the Electrical–Mechanical Correspondence
.
317
6.3. Structures
. . . . . . . . . . . . . . . . . . . . . . . . . .
322
Chapter 7.
Linearity . . . . . . . . . . . . . . . . . . . . .
341
7.1. Linear Functions . . . . . . . . . . . . . . . . . . . . . . . .
342
Linear Operators
. . . . . . . . . . . . . . . . . . . . . .
347
The Space of Linear Functions
. . . . . . . . . . . . . . . .
349
Dual Spaces
. . . . . . . . . . . . . . . . . . . . . . . .
350
Composition
. . . . . . . . . . . . . . . . . . . . . . . .
352
Inverses
. . . . . . . . . . . . . . . . . . . . . . . . . .
355
7.2. Linear Transformations . . . . . . . . . . . . . . . . . . . . .
358
Change of Basis
. . . . . . . . . . . . . . . . . . . . . .
365
7.3. Aﬃne Transformations and Isometries
. . . . . . . . . . . . . .
370
Isometry . . . . . . . . . . . . . . . . . . . . . . . . . .
372
7.4. Linear Systems
. . . . . . . . . . . . . . . . . . . . . . . .
376
The Superposition Principle
. . . . . . . . . . . . . . . . .
378
Inhomogeneous Systems . . . . . . . . . . . . . . . . . . .
383
Superposition Principles for Inhomogeneous Systems
. . . . . .
388
Complex Solutions to Real Systems . . . . . . . . . . . . . .
390
7.5. Adjoints, Positive Deﬁnite Operators, and Minimization Principles . .
395
Self-Adjoint and Positive Deﬁnite Linear Functions
. . . . . . .
398
Minimization . . . . . . . . . . . . . . . . . . . . . . . .
400
Chapter 8.
Eigenvalues and Singular Values . . . . . . . . . . .
403
8.1. Linear Dynamical Systems
. . . . . . . . . . . . . . . . . . .
404
Scalar Ordinary Diﬀerential Equations . . . . . . . . . . . . .
404
First Order Dynamical Systems . . . . . . . . . . . . . . . .
407
8.2. Eigenvalues and Eigenvectors
. . . . . . . . . . . . . . . . . .
408
Basic Properties of Eigenvalues . . . . . . . . . . . . . . . .
415
The Gerschgorin Circle Theorem
. . . . . . . . . . . . . . .
420
5.6. Discrete Fourier Analysis and the Fast Fourier Transform
. . . . .
285

xxiv
Table of Contents
8.3. Eigenvector Bases
. . . . . . . . . . . . . . . . . . . . . . .
423
Diagonalization . . . . . . . . . . . . . . . . . . . . . . .
426
8.4. Invariant Subspaces
. . . . . . . . . . . . . . . . . . . . . .
429
8.5. Eigenvalues of Symmetric Matrices . . . . . . . . . . . . . . . .
431
The Spectral Theorem . . . . . . . . . . . . . . . . . . . .
437
Optimization Principles for Eigenvalues of Symmetric Matrices . .
440
8.6. Incomplete Matrices
. . . . . . . . . . . . . . . . . . . . . .
444
The Schur Decomposition
. . . . . . . . . . . . . . . . . .
444
The Jordan Canonical Form
. . . . . . . . . . . . . . . . .
447
8.7. Singular Values
. . . . . . . . . . . . . . . . . . . . . . . .
454
The Pseudoinverse
. . . . . . . . . . . . . . . . . . . . .
457
The Euclidean Matrix Norm
. . . . . . . . . . . . . . . . .
459
Condition Number and Rank . . . . . . . . . . . . . . . . .
460
Spectral Graph Theory
. . . . . . . . . . . . . . . . . . .
462
8.8. Principal Component Analysis . . . . . . . . . . . . . . . . . .
467
Variance and Covariance . . . . . . . . . . . . . . . . . . .
467
The Principal Components . . . . . . . . . . . . . . . . . .
471
Chapter 9.
Iteration
. . . . . . . . . . . . . . . . . . . . .
475
9.1. Linear Iterative Systems
. . . . . . . . . . . . . . . . . . . .
476
Scalar Systems
. . . . . . . . . . . . . . . . . . . . . . .
476
Powers of Matrices
. . . . . . . . . . . . . . . . . . . . .
479
Diagonalization and Iteration . . . . . . . . . . . . . . . . .
484
9.2. Stability
. . . . . . . . . . . . . . . . . . . . . . . . . . .
488
Spectral Radius . . . . . . . . . . . . . . . . . . . . . . .
489
Fixed Points
. . . . . . . . . . . . . . . . . . . . . . . .
493
Matrix Norms and Convergence . . . . . . . . . . . . . . . .
495
9.3. Markov Processes
. . . . . . . . . . . . . . . . . . . . . . .
499
9.4. Iterative Solution of Linear Algebraic Systems . . . . . . . . . . .
506
The Jacobi Method . . . . . . . . . . . . . . . . . . . . .
508
The Gauss–Seidel Method
. . . . . . . . . . . . . . . . . .
512
Successive Over-Relaxation . . . . . . . . . . . . . . . . . .
517
9.5. Numerical Computation of Eigenvalues . . . . . . . . . . . . . .
522
The Power Method
. . . . . . . . . . . . . . . . . . . . .
522
The QR Algorithm
. . . . . . . . . . . . . . . . . . . . .
526
Tridiagonalization . . . . . . . . . . . . . . . . . . . . . .
532
9.6. Krylov Subspace Methods . . . . . . . . . . . . . . . . . . . .
536
Krylov Subspaces . . . . . . . . . . . . . . . . . . . . . .
536
Arnoldi Iteration
. . . . . . . . . . . . . . . . . . . . . .
537
The Full Orthogonalization Method . . . . . . . . . . . . . .
540
The Conjugate Gradient Method
. . . . . . . . . . . . . . .
542
The Generalized Minimal Residual Method . . . . . . . . . . .
546

Table of Contents
xxv
9.7. Wavelets
. . . . . . . . . . . . . . . . . . . . . . . . . . .
549
The Haar Wavelets
. . . . . . . . . . . . . . . . . . . . .
549
Modern Wavelets . . . . . . . . . . . . . . . . . . . . . .
555
Solving the Dilation Equation
. . . . . . . . . . . . . . . .
559
Chapter 10.
Dynamics
. . . . . . . . . . . . . . . . . . . .
565
10.1. Basic Solution Techniques
. . . . . . . . . . . . . . . . . . .
565
The Phase Plane
. . . . . . . . . . . . . . . . . . . . . .
567
Existence and Uniqueness
. . . . . . . . . . . . . . . . . .
570
Complete Systems
. . . . . . . . . . . . . . . . . . . . .
572
The General Case . . . . . . . . . . . . . . . . . . . . . .
575
10.2. Stability of Linear Systems . . . . . . . . . . . . . . . . . . .
579
10.3. Two-Dimensional Systems
. . . . . . . . . . . . . . . . . . .
585
Distinct Real Eigenvalues
. . . . . . . . . . . . . . . . . .
586
Complex Conjugate Eigenvalues
. . . . . . . . . . . . . . .
587
Incomplete Double Real Eigenvalue
. . . . . . . . . . . . . .
588
Complete Double Real Eigenvalue . . . . . . . . . . . . . . .
588
10.4. Matrix Exponentials
. . . . . . . . . . . . . . . . . . . . .
592
Applications in Geometry
. . . . . . . . . . . . . . . . . .
599
Invariant Subspaces and Linear Dynamical Systems . . . . . . .
603
Inhomogeneous Linear Systems . . . . . . . . . . . . . . . .
605
10.5. Dynamics of Structures
. . . . . . . . . . . . . . . . . . . .
608
Stable Structures . . . . . . . . . . . . . . . . . . . . . .
610
Unstable Structures . . . . . . . . . . . . . . . . . . . . .
615
Systems with Diﬀering Masses
. . . . . . . . . . . . . . . .
618
Friction and Damping . . . . . . . . . . . . . . . . . . . .
620
10.6. Forcing and Resonance
. . . . . . . . . . . . . . . . . . . .
623
Electrical Circuits . . . . . . . . . . . . . . . . . . . . . .
628
Forcing and Resonance in Systems
. . . . . . . . . . . . . .
630
References . . . . . . . . . . . . . . . . . . . . . . . . . .
633
Symbol Index
. . . . . . . . . . . . . . . . . . . . . . . .
637
Subject Index
. . . . . . . . . . . . . . . . . . . . . . . .
643

Chapter 1
Linear Algebraic Systems
Linear algebra is the core of modern applied mathematics. Its humble origins are to be
found in the need to solve “elementary” systems of linear algebraic equations. But its
ultimate scope is vast, impinging on all of mathematics, both pure and applied, as well
as numerical analysis, statistics, data science, physics, engineering, mathematical biology,
ﬁnancial mathematics, and every other discipline in which mathematical methods are re-
quired. A thorough grounding in the methods and theory of linear algebra is an essential
prerequisite for understanding and harnessing the power of mathematics throughout its
multifaceted applications.
In the ﬁrst chapter, our focus will be on the most basic method for solving linear
algebraic systems, known as Gaussian Elimination in honor of one of the all-time mathe-
matical greats, the early nineteenth-century German mathematician Carl Friedrich Gauss,
although the method appears in Chinese mathematical texts from around 150 CE, if not
earlier, and was also known to Isaac Newton. Gaussian Elimination is quite elementary,
but remains one of the most important algorithms in applied (as well as theoretical) math-
ematics. Our initial focus will be on the most important class of systems: those involving
the same number of equations as unknowns — although we will eventually develop tech-
niques for handling completely general linear systems. While the former typically have
a unique solution, general linear systems may have either no solutions or inﬁnitely many
solutions. Since physical models require existence and uniqueness of their solution, the sys-
tems arising in applications often (but not always) involve the same number of equations
as unknowns. Nevertheless, the ability to conﬁdently handle all types of linear systems
is a basic prerequisite for further progress in the subject. In contemporary applications,
particularly those arising in numerical solutions of diﬀerential equations, in signal and im-
age processing, and in contemporary data analysis, the governing linear systems can be
huge, sometimes involving millions of equations in millions of unknowns, challenging even
the most powerful supercomputer. So, a systematic and careful development of solution
techniques is essential. Section 1.7 discusses some of the practical issues and limitations in
computer implementations of the Gaussian Elimination method for large systems arising
in applications.
Modern linear algebra relies on the basic concepts of scalar, vector, and matrix, and
so we must quickly review the fundamentals of matrix arithmetic. Gaussian Elimination
can be proﬁtably reinterpreted as a certain matrix factorization, known as the (permuted)
LU decomposition, which provides valuable insight into the solution algorithms. Matrix
inverses and determinants are also discussed in brief, primarily for their theoretical prop-
erties. As we shall see, formulas relying on the inverse or the determinant are extremely
ineﬃcient, and so, except in low-dimensional or highly structured environments, are to
be avoided in almost all practical computations. In the theater of applied linear algebra,
Gaussian Elimination and matrix factorization are the stars, while inverses and determi-
nants are relegated to the supporting cast.
1.1 Solution of Linear Systems
Gaussian Elimination is a simple, systematic algorithm to solve systems of linear equations.
It is the workhorse of linear algebra, and, as such, of absolutely fundamental importance
© Springer International Publishing AG, part of Springer Nature 2018 
https://doi.org/10.1007/978-3-319-91041-3_1 
1
P. J. Olver, C. Shakiban, Applied Linear Algebra, Undergraduate Texts in Mathematics, 

2
1 Linear Algebraic Systems
in applied mathematics. In this section, we review the method in the most important case,
in which there is the same number of equations as unknowns. The general situation will
be deferred until Section 1.8.
To illustrate, consider an elementary system of three linear equations
x + 2y + z = 2,
2x + 6y + z = 7,
x + y + 4z = 3,
(1.1)
in three unknowns x, y, z. Linearity† refers to the fact that the unknowns only appear to
the ﬁrst power, and there are no product terms like xy or xy z. The basic solution method
is to systematically employ the following fundamental operation:
Linear System Operation #1:
Add a multiple of one equation to another equation.
Before continuing, you might try to convince yourself that this operation doesn’t change
the solutions to the system. Our goal is to judiciously apply the operation and so be led to
a much simpler linear system that is easy to solve, and, moreover, has the same solutions
as the original. Any linear system that is derived from the original system by successive
application of such operations will be called an equivalent system. By the preceding remark,
equivalent linear systems have the same solutions.
The systematic feature is that we successively eliminate the variables in our equations
in order of appearance. We begin by eliminating the ﬁrst variable, x, from the second
equation. To this end, we subtract twice the ﬁrst equation from the second, leading to the
equivalent system
x + 2y + z = 2,
2y −z = 3,
x + y + 4z = 3.
(1.2)
Next, we eliminate x from the third equation by subtracting the ﬁrst equation from it:
x + 2y + z = 2,
2y −z = 3,
−y + 3z = 1.
(1.3)
The equivalent system (1.3) is already simpler than the original (1.1). Notice that the
second and third equations do not involve x (by design) and so constitute a system of two
linear equations for two unknowns. Moreover, once we have solved this subsystem for y
and z, we can substitute the answer into the ﬁrst equation, and we need only solve a single
linear equation for x.
We continue on in this fashion, the next phase being the elimination of the second
variable, y, from the third equation by adding 1
2 the second equation to it. The result is
x + 2y + z = 2,
2y −z = 3,
5
2 z = 5
2,
(1.4)
which is the simple system we are after. It is in what is called triangular form, which means
that, while the ﬁrst equation involves all three variables, the second equation involves only
the second and third variables, and the last equation involves only the last variable.
†
The “oﬃcial” deﬁnition of linearity will be deferred until Chapter 7.

1.2 Matrices and Vectors
3
Any triangular system can be straightforwardly solved by the method of Back Substi-
tution. As the name suggests, we work backwards, solving the last equation ﬁrst, which
requires that z = 1. We substitute this result back into the penultimate equation, which
becomes 2y −1 = 3, with solution y = 2. We ﬁnally substitute these two values for y and
z into the ﬁrst equation, which becomes x + 5 = 2, and so the solution to the triangular
system (1.4) is
x = −3,
y = 2,
z = 1.
(1.5)
Moreover, since we used only our basic linear system operation to pass from (1.1) to the
triangular system (1.4), this is also the solution to the original system of linear equations,
as you can check. We note that the system (1.1) has a unique — meaning one and only
one — solution, namely (1.5).
And that, barring a few minor complications that can crop up from time to time, is all
that there is to the method of Gaussian Elimination! It is extraordinarily simple, but its
importance cannot be overemphasized. Before exploring the relevant issues, it will help to
reformulate our method in a more convenient matrix notation.
Exercises
1.1.1. Solve the following systems of linear equations by reducing to triangular form and then
using Back Substitution.
(a)
x −y = 7,
x + 2y = 3;
(b)
6u + v = 5,
3u −2v = 5;
(c)
p + q −r = 0,
2p −q + 3r = 3,
−p −q = 6;
(d)
2u −v + 2w = 2,
−u −v + 3w = 1,
3u −2w = 1;
(e)
5x1 + 3x2 −x3 = 9,
3x1 + 2x2 −x3 = 5,
x1 + x2 + x3 = −1;
(f )
x + z −2w = −3,
2x −y + 2z −w = −5,
−6y −4z + 2w = 2,
x + 3y + 2z −w = 1;
(g)
3x1 + x2 = 1,
x1 + 3x2 + x3 = 1,
x2 + 3x3 + x4 = 1,
x3 + 3x4 = 1.
1.1.2. How should the coeﬃcients a, b, and c be chosen so that the system ax + by + cz = 3,
ax −y + cz = 1, x + by −cz = 2, has the solution x = 1, y = 2 and z = −1?
♥1.1.3. The system 2x = −6, −4x + 3y = 3, x + 4y −z = 7, is in lower triangular form.
(a) Formulate a method of Forward Substitution to solve it. (b) What happens if you
reduce the system to (upper) triangular form using the algorithm in this section?
(c) Devise an algorithm that uses our linear system operation to reduce a system to lower
triangular form and then solve it by Forward Substitution.
(d) Check your algorithm by
applying it to one or two of the systems in Exercise 1.1.1. Are you able to solve them in all
cases?
1.2 Matrices and Vectors
A matrix is a rectangular array of numbers. Thus,

1
0
3
−2
4
1

,
⎛
⎜
⎜
⎜
⎝
π
0
e
1
2
−1
.83
√
5
−4
7
⎞
⎟
⎟
⎟
⎠,
( .2
−1.6
.32 ),

0
0

,

1
3
−2
5

,

4
1 Linear Algebraic Systems
are all examples of matrices. We use the notation
A =
⎛
⎜
⎜
⎝
a11
a12
. . .
a1n
a21
a22
. . .
a2n
...
...
...
...
am1
am2
. . .
amn
⎞
⎟
⎟
⎠
(1.6)
for a general matrix of size m×n (read “m by n”), where m denotes the number of rows in
A and n denotes the number of columns. Thus, the preceding examples of matrices have
respective sizes 2 × 3, 4 × 2, 1 × 3, 2 × 1, and 2 × 2. A matrix is square if m = n, i.e., it
has the same number of rows as columns. A column vector is an m × 1 matrix, while a row
vector is a 1 × n matrix. As we shall see, column vectors are by far the more important
of the two, and the term “vector” without qualiﬁcation will always mean “column vector”.
A 1 × 1 matrix, which has but a single entry, is both a row and a column vector.
The number that lies in the ith row and the jth column of A is called the (i, j) entry
of A, and is denoted by aij. The row index always appears ﬁrst and the column index
second.† Two matrices are equal, A = B, if and only if they have the same size, say m×n,
and all their entries are the same: aij = bij for i = 1, . . . , m and j = 1, . . ., n.
A general linear system of m equations in n unknowns will take the form
a11 x1 + a12 x2 + · · · + a1n xn = b1,
a21 x1 + a22 x2 + · · · + a2n xn = b2,
...
...
...
am1 x1 + am2 x2 + · · · + amn xn = bm.
(1.7)
As such, it is composed of three basic ingredients: the m × n coeﬃcient matrix A, with
entries aij as in (1.6), the column vector x =
⎛
⎜
⎜
⎜
⎝
x1
x2
...
xn
⎞
⎟
⎟
⎟
⎠containing the unknowns, and
the column vector b =
⎛
⎜
⎜
⎜
⎝
b1
b2
...
bm
⎞
⎟
⎟
⎟
⎠containing right-hand sides.
In our previous example,
x + 2y + z = 2,
2x + 6y + z = 7,
x + y + 4z = 3,
the coeﬃcient matrix A =
⎛
⎝
1
2
1
2
6
1
1
1
4
⎞
⎠can be ﬁlled in, entry by entry,
from the coeﬃcients of the variables appearing in the equations; if a variable does not
appear in an equation, the corresponding matrix entry is 0. The vector x =
⎛
⎝
x
y
z
⎞
⎠lists
the variables, while the entries of b =
⎛
⎝
2
7
3
⎞
⎠are the right-hand sides of the equations.
†
In tensor analysis, [1], a sub- and super-script notation is adopted, with ai
j denoting the (i, j)
entry of the matrix A. This has certain advantages, but, to avoid possible confusion with powers,
we shall stick with the simpler subscript notation throughout this text.

1.2 Matrices and Vectors
5
Remark.
We will consistently use bold face lower case letters to denote vectors, and
ordinary capital letters to denote general matrices.
Exercises
1.2.1. Let A =
⎛
⎜
⎝
−2
0
1
3
−1
2
7
−5
6
−6
−3
4
⎞
⎟
⎠. (a) What is the size of A? (b) What is its (2, 3) entry?
(c) (3, 1) entry? (d) 1st row? (e) 2nd column?
1.2.2. Write down examples of (a) a 3 × 3 matrix; (b) a 2 × 3 matrix; (c) a matrix with 3 rows
and 4 columns; (d) a row vector with 4 entries; (e) a column vector with 3 entries;
(f ) a matrix that is both a row vector and a column vector.
1.2.3. For which values of x, y, z, w are the matrices

x + y
x −z
y + w
x + 2w
	
and

1
0
2
1
	
equal?
1.2.4. For each of the systems in Exercise 1.1.1, write down the coeﬃcient matrix A and the
vectors x and b.
1.2.5. Write out and solve the linear systems corresponding to the indicated matrix, vector of
unknowns, and right-hand side.
(a) A =

1
−1
2
3
	
, x =

x
y
	
, b =

−1
−3
	
;
(b) A =
⎛
⎜
⎝
1
0
1
1
1
0
0
1
1
⎞
⎟
⎠, x =
⎛
⎜
⎝
u
v
w
⎞
⎟
⎠, b =
⎛
⎜
⎝
−1
−1
2
⎞
⎟
⎠;
(c) A =
⎛
⎜
⎝
3
0
−1
−2
−1
0
1
1
−3
⎞
⎟
⎠,
x =
⎛
⎜
⎝
x1
x2
x3
⎞
⎟
⎠, b =
⎛
⎜
⎝
1
0
1
⎞
⎟
⎠;
(d) A =
⎛
⎜
⎜
⎜
⎝
1
1
−1
−1
−1
0
1
2
1
−1
1
0
0
2
−1
1
⎞
⎟
⎟
⎟
⎠, x =
⎛
⎜
⎜
⎜
⎝
x
y
z
w
⎞
⎟
⎟
⎟
⎠, b =
⎛
⎜
⎜
⎜
⎝
0
4
1
5
⎞
⎟
⎟
⎟
⎠.
Matrix Arithmetic
Matrix arithmetic involves three basic operations: matrix addition, scalar multiplication,
and matrix multiplication. First we deﬁne addition of matrices. You are allowed to add
two matrices only if they are of the same size, and matrix addition is performed entry by
entry. For example,

1
2
−1
0

+

3
−5
2
1

=

4
−3
1
1

.
Therefore, if A and B are m × n matrices, their sum C = A+ B is the m × n matrix whose
entries are given by cij = aij + bij for i = 1, . . . , m and j = 1, . . ., n. When deﬁned, matrix
addition is commutative, A + B = B + A, and associative, A + (B + C) = (A + B) + C,
just like ordinary addition.
A scalar is a fancy name for an ordinary number — the term merely distinguishes it
from a vector or a matrix. For the time being, we will restrict our attention to real scalars
and matrices with real entries, but eventually complex scalars and complex matrices must
be dealt with. We will consistently identify a scalar c ∈R with the 1 × 1 matrix (c) in
which it is the sole entry, and so will omit the redundant parentheses in the latter case.
Scalar multiplication takes a scalar c and an m × n matrix A and computes the m × n

6
1 Linear Algebraic Systems
matrix B = c A by multiplying each entry of A by c. For example,
3

1
2
−1
0

=

3
6
−3
0

.
In general, bij = c aij for i = 1, . . . , m and j = 1, . . . , n.
Basic properties of scalar
multiplication are summarized at the end of this section.
Finally, we deﬁne matrix multiplication. First, the product of a row vector a and a
column vector x having the same number of entries is the scalar or 1 × 1 matrix deﬁned
by the following rule:
a x = ( a1 a2 . . . an )
⎛
⎜
⎜
⎜
⎝
x1
x2
...
xn
⎞
⎟
⎟
⎟
⎠= a1 x1 + a2 x2 + · · · + an xn =
n

k=1
ak xk.
(1.8)
More generally, if A is an m × n matrix and B is an n × p matrix, so that the number of
columns in A equals the number of rows in B, then the matrix product C = AB is deﬁned
as the m × p matrix whose (i, j) entry equals the vector product of the ith row of A and
the jth column of B. Therefore,
cij =
n

k=1
aik bkj.
(1.9)
Note that our restriction on the sizes of A and B guarantees that the relevant row and
column vectors will have the same number of entries, and so their product is deﬁned.
For example, the product of the coeﬃcient matrix A and vector of unknowns x for our
original system (1.1) is given by
A x =
⎛
⎝
1
2
1
2
6
1
1
1
4
⎞
⎠
⎛
⎝
x
y
z
⎞
⎠=
⎛
⎝
x + 2y + z
2x + 6y + z
x + y + 4z
⎞
⎠.
The result is a column vector whose entries reproduce the left-hand sides of the original
linear system! As a result, we can rewrite the system
A x = b
(1.10)
as an equality between two column vectors. This result is general; a linear system (1.7)
consisting of m equations in n unknowns can be written in the matrix form (1.10), where A
is the m×n coeﬃcient matrix (1.6), x is the n×1 column vector of unknowns, and b is the
m × 1 column vector containing the right-hand sides. This is one of the principal reasons
for the non-evident deﬁnition of matrix multiplication. Component-wise multiplication of
matrix entries turns out to be almost completely useless in applications.
Now, the bad news. Matrix multiplication is not commutative — that is, BA is not
necessarily equal to AB. For example, BA may not be deﬁned even when AB is. Even if
both are deﬁned, they may be diﬀerent sized matrices. For example the product s = r c
of a row vector r, a 1 × n matrix, and a column vector c, an n × 1 matrix with the same
number of entries, is a 1 × 1 matrix, or scalar, whereas the reversed product C = c r is an
n × n matrix. For instance,
( 1 2 )

3
0

= 3,
whereas

3
0

( 1 2 ) =

3
6
0
0

.

1.2 Matrices and Vectors
7
In computing the latter product, don’t forget that we multiply the rows of the ﬁrst matrix
by the columns of the second, each of which has but a single entry. Moreover, even if
the matrix products AB and B A have the same size, which requires both A and B to be
square matrices, we may still have AB ̸= B A. For example,

1
2
3
4
 
0
1
−1
2

=

−2
5
−4
11

̸=

3
4
5
6

=

0
1
−1
2
 
1
2
3
4

.
On the other hand, matrix multiplication is associative, so A(B C) = (AB)C whenever
A has size m × n, B has size n × p, and C has size p × q; the result is a matrix of
size m × q. The proof of associativity is a tedious computation based on the deﬁnition of
matrix multiplication that, for brevity, we omit.† Consequently, the one diﬀerence between
matrix algebra and ordinary algebra is that you need to be careful not to change the order
of multiplicative factors without proper justiﬁcation.
Since matrix multiplication acts by multiplying rows by columns, one can compute the
columns in a matrix product AB by multiplying the matrix A and the individual columns
of B. For example, the two columns of the matrix product

1
−1
2
2
0
−2
⎛
⎝
3
4
0
2
−1
1
⎞
⎠=

1
4
8
6

are obtained by multiplying the ﬁrst matrix with the individual columns of the second:

1
−1
2
2
0
−2
⎛
⎝
3
0
−1
⎞
⎠=

1
8

,

1
−1
2
2
0
−2
⎛
⎝
4
2
1
⎞
⎠=

4
6

.
In general, if we use bk to denote the kth column of B, then
AB = A

b1
b2
. . .
bp

=

Ab1
Ab2
. . .
Abp

,
(1.11)
indicating that the kth column of their matrix product is Abk.
There are two important special matrices. The ﬁrst is the zero matrix, all of whose
entries are 0. We use Om×n to denote the m × n zero matrix, often written as just O if the
size is clear from the context. The zero matrix is the additive unit, so A + O = A = O + A
when O has the same size as A. In particular, we will use a bold face 0 to denote a column
vector with all zero entries, i.e., O1×n.
The role of the multiplicative unit is played by the square identity matrix
I = In =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎝
1
0
0
· · ·
0
0
0
1
0
· · ·
0
0
0
0
1
· · ·
0
0
...
...
...
...
...
...
0
0
0
· · ·
1
0
0
0
0
· · ·
0
1
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎠
of size n × n. The entries along the main diagonal — which runs from top left to bottom
right — are equal to 1, while the oﬀ-diagonal entries are all 0. As you can check, if A is
†
A much simpler — but more abstract proof can be found in Exercise 7.1.45.

8
1 Linear Algebraic Systems
Basic Matrix Arithmetic
Matrix Addition:
Commutativity
A + B = B + A
Associativity
(A + B) + C = A + (B + C)
Zero Matrix
A + O = A = O + A
Additive Inverse
A + (−A) = O,
−A = (−1)A
Scalar Multiplication:
Associativity
c(dA) = (cd)A
Distributivity
c (A + B) = (cA) + (cB)
(c + d)A = (cA) + (dA)
Unit Scalar
1 A = A
Zero Scalar
0 A = O
Matrix Multiplication:
Associativity
(AB)C = A(B C)
Distributivity
A(B + C) = AB + AC,
(A + B)C = AC + B C,
Compatibility
c (AB) = (cA)B = A(cB)
Identity Matrix
A I = A = I A
Zero Matrix
A O = O,
O A = O
any m×n matrix, then Im A = A = A In . We will sometimes write the preceding equation
as just I A = A = A I , since each matrix product is well-deﬁned for exactly one size of
identity matrix.
The identity matrix is a particular example of a diagonal matrix. In general, a square
matrix A is diagonal if all its oﬀ-diagonal entries are zero: aij = 0 for all i ̸= j. We will
sometimes write D = diag (c1, . . . , cn) for the n × n diagonal matrix with diagonal entries
dii = ci. Thus, diag (1, 3, 0) refers to the diagonal matrix
⎛
⎝
1
0
0
0
3
0
0
0
0
⎞
⎠, while the 4 × 4
identity matrix can be written as
I 4 = diag (1, 1, 1, 1) =
⎛
⎜
⎝
1
0
0
0
0
1
0
0
0
0
1
0
0
0
0
1
⎞
⎟
⎠.
Let us conclude this section by summarizing the basic properties of matrix arithmetic.
In the accompanying table, A, B, C are matrices; c, d are scalars; O is a zero matrix; and
I is an identity matrix. All matrices are assumed to have the correct sizes so that the
indicated operations are deﬁned.
Exercises
1.2.6.(a) Write down the 5 × 5 identity and zero matrices.
(b) Write down their sum and
their product. Does the order of multiplication matter?

1.2 Matrices and Vectors
9
1.2.7. Consider the matrices A =
⎛
⎜
⎝
1
−1
3
−1
4
−2
3
0
6
⎞
⎟
⎠, B =

−6
0
3
4
2
−1
	
, C =
⎛
⎜
⎝
2
3
−3
−4
1
2
⎞
⎟
⎠.
Compute the indicated combinations where possible. (a) 3A −B, (b) AB, (c) B A,
(d) (A+B)C, (e) A+B C, (f ) A+2C B, (g) B C B−I , (h) A2−3A+ I , (i) (B−I )(C+ I ).
1.2.8. Which of the following pairs of matrices commute under matrix multiplication?
(a)

1
2
−2
1
	
,

2
3
5
0
	
, (b)
⎛
⎜
⎝
3
−1
0
2
1
4
⎞
⎟
⎠,

4
2
−2
5
2
4
	
, (c)
⎛
⎜
⎝
3
0
−1
−2
−1
2
2
0
0
⎞
⎟
⎠,
⎛
⎜
⎝
2
0
−1
1
1
−1
2
0
−1
⎞
⎟
⎠.
1.2.9. List the diagonal entries of A =
⎛
⎜
⎜
⎜
⎝
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
⎞
⎟
⎟
⎟
⎠.
1.2.10. Write out the following diagonal matrices: (a) diag (1, 0, −1), (b) diag (2, −2, 3, −3).
1.2.11. True or false: (a) The sum of two diagonal matrices of the same size is a diagonal
matrix.
(b) The product is also diagonal.
♥1.2.12.(a) Show that if D =

a
0
0
b
	
is a 2 × 2 diagonal matrix with a ̸= b, then the only
matrices that commute (under matrix multiplication) with D are other 2 × 2 diagonal
matrices. (b) What if a = b? (c) Find all matrices that commute with D =
⎛
⎜
⎝
a
0
0
0
b
0
0
0
c
⎞
⎟
⎠,
where a, b, c are all diﬀerent. (d) Answer the same question for the case when a ̸= b = c.
(e) Prove that a matrix A commutes with an n × n diagonal matrix D with all distinct
diagonal entries if and only if A is a diagonal matrix.
1.2.13. Show that the matrix products AB and B A have the same size if and only if A and B
are square matrices of the same size.
1.2.14. Find all matrices B that commute (under matrix multiplication) with A =

1
2
0
1
	
.
1.2.15.(a) Show that, if A, B are commuting square matrices, then (A + B)2 = A2 + 2AB + B2.
(b) Find a pair of 2 × 2 matrices A, B such that (A + B)2 ̸= A2 + 2AB + B2.
1.2.16. Show that if the matrices A and B commute, then they necessarily are both square and
the same size.
1.2.17. Let A be an m × n matrix. What are the permissible sizes for the zero matrices
appearing in the identities A O = O and O A = O?
1.2.18. Let A be an m × n matrix and let c be a scalar. Show that if cA = O, then either c = 0
or A = O.
1.2.19. True or false: If AB = O then either A = O or B = O.
1.2.20. True or false: If A, B are square matrices of the same size, then
A2 −B2 = (A + B)(A −B).
1.2.21. Prove that Av = 0 for every vector v (with the appropriate number of entries) if and
only if A = O is the zero matrix. Hint: If you are stuck, ﬁrst try to ﬁnd a proof when A is
a small matrix, e.g., of size 2 × 2.
1.2.22.(a) Under what conditions is the square A2 of a matrix deﬁned? (b) Show that A and
A2 commute. (c) How many matrix multiplications are needed to compute An?
1.2.23. Find a nonzero matrix A ̸= O such that A2 = O.
♦1.2.24. Let A have a row all of whose entries are zero. (a) Explain why the product AB also
has a zero row.
(b) Find an example where B A does not have a zero row.

10
1 Linear Algebraic Systems
1.2.25.(a) Find all solutions X =

x
y
z
w
	
to the matrix equation AX = I when
A =

2
1
3
1
	
. (b) Find all solutions to X A = I . Are they the same?
1.2.26.(a) Find all solutions X =

x
y
z
w
	
to the matrix equation AX = B when
A =

0
1
−1
3
	
and B =

1
2
−1
1
	
. (b) Find all solutions to X A = B. Are they the same?
1.2.27.(a) Find all solutions X =

x
y
z
w
	
to the matrix equation AX = X B when
A =

1
2
−1
0
	
and B =

0
1
3
0
	
. (b) Can you ﬁnd a pair of nonzero matrices A ̸= B such
that the matrix equation AX = X B has a nonzero solution X ̸= O?
1.2.28. Let A be a matrix and c a scalar. Find all solutions to the matrix equation c A = I .
♦1.2.29. Let e be the 1 × m row vector all of whose entries are equal to 1. (a) Show that if
A is an m × n matrix, then the ith entry of the product v = eA is the jth column sum
of A, meaning the sum of all the entries in its jth row.
(b) Let W denote the m × m
matrix whose diagonal entries are equal to 1 −m
m
and whose oﬀ-diagonal entries are all
equal to 1
m . Prove that the column sums of B = W A are all zero. (c) Check both results
when A =
⎛
⎜
⎝
1
2
−1
2
1
3
−4
5
−1
⎞
⎟
⎠.
Remark. If the rows of A represent experimental data
values, then the entries of 1
m eA represent the means or averages of the data values, while
B = W A corresponds to data that has been normalized to have mean 0; see Section 8.8.
♥1.2.30. The commutator of two matrices A, B, is deﬁned to be the matrix
C = [ A, B ] = AB −B A.
(1.12)
(a) Explain why [ A, B ] is deﬁned if and only if A and B are square matrices of the
same size. (b) Show that A and B commute under matrix multiplication if and only if
[ A, B ] = O. (c) Compute the commutator of the following matrices:
(i)

1
0
1
−1
	
,

2
1
−2
0
	
; (ii)

−1
3
3
−1
	
,

1
7
7
1
	
; (iii)
⎛
⎜
⎝
0
−1
0
1
0
0
0
0
1
⎞
⎟
⎠,
⎛
⎜
⎝
1
0
0
0
0
−1
0
1
0
⎞
⎟
⎠;
(d) Prove that the commutator is (i) Bilinear: [ cA + dB, C ] = c [ A, C ] + d [ B, C ]
and [ A, cB + dC ] = c [ A, B ] + d [ A, C ] for any scalars c, d; (ii) Skew-symmetric:
[ A, B ] = −[ B, A ]; (iii) satisﬁes the the Jacobi identity:
[ [ A, B ], C ] + [ [ C, A ], B ] + [ [ B, C ], A ] = O,
for any square matrices A, B, C of the same size.
Remark. The commutator plays a very important role in geometry, symmetry, and
quantum mechanics. See Section 10.4 as well as [54, 60, 93] for further developments.
♦1.2.31. The trace of a n × n matrix A ∈Mn×n is deﬁned to be the sum of its diagonal entries:
tr A = a11 + a22 + · · ·+ ann. (a) Compute the trace of (i)

1
−1
2
3
	
, (ii)
⎛
⎜
⎝
1
3
2
−1
0
1
−4
3
−1
⎞
⎟
⎠.
(b) Prove that tr(A + B) = tr A + tr B. (c) Prove that tr(AB) = tr(B A). (d) Prove that
the commutator matrix C = AB −B A has zero trace: tr C = 0. (e) Is part (c) valid if A
has size m × n and B has size n × m? (f ) Prove that tr(AB C) = tr(CAB) = tr(B CA).
On the other hand, ﬁnd an example where tr(AB C) ̸= tr(ACB).

1.2 Matrices and Vectors
11
♦1.2.32. Prove that matrix multiplication is associative: A(B C) = (AB)C when deﬁned.
♦1.2.33. Justify the following alternative formula for multiplying a matrix A and a column
vector x:
A x = x1 c1 + x2 c2 + · · · + xn cn,
(1.13)
where c1, . . . , cn are the columns of A and x1, . . . , xn the entries of x.
♥1.2.34. The basic deﬁnition of matrix multiplication AB tells us to multiply rows of A by
columns of B. Remarkably, if you suitably interpret the operation, you can also compute
AB by multiplying columns of A by rows of B! Suppose A is an m×n matrix with columns
c1, . . . , cn. Suppose B is an n × p matrix with rows r1, . . . , rn. Then we claim that
AB = c1 r1 + c2 r2 + · · · + cn rn,
(1.14)
where each summand is a matrix of size m × p. (a) Verify that the particular case

1
2
3
4
	 
0
−1
2
3
	
=

1
3
	
( 0
−1 ) +

2
4
	
( 2
3 ) =

0
−1
0
−3
	
+

4
6
8
12
	
=

4
5
8
9
	
agrees with the usual method for computing the matrix product.
(b) Use this method to
compute the matrix products (i)

−2
1
3
2
	 
1
−2
1
0
	
, (ii)

1
−2
0
−3
−1
2
	⎛
⎜
⎝
2
5
−3
0
1
−1
⎞
⎟
⎠,
(iii)
⎛
⎜
⎝
3
−1
1
−1
2
1
1
1
−5
⎞
⎟
⎠
⎛
⎜
⎝
2
3
0
3
−1
4
0
4
1
⎞
⎟
⎠, and verify that you get the same answer as that
obtained by the traditional method. (c) Explain why (1.13) is a special case of (1.14).
(d) Prove that (1.14) gives the correct formula for the matrix product.
♥1.2.35. Matrix polynomials. Let p(x) = cn xn + cn−1 xn−1 + · · · + c1 x + c0 be a polynomial
function. If A is a square matrix, we deﬁne the corresponding matrix polynomial p(A) =
cn An + cn−1 An−1 + · · · + c1 A + c0 I ; the constant term becomes a scalar multiple of the
identity matrix. For instance, if p(x) = x2−2x+3, then p(A) = A2−2A+3 I . (a) Write out
the matrix polynomials p(A), q(A) when p(x) = x3 −3x + 2, q(x) = 2x2 + 1. (b) Evaluate
p(A) and q(A) when A =

1
2
−1
−1
	
. (c) Show that the matrix product p(A)q(A) is the
matrix polynomial corresponding to the product polynomial r(x) = p(x) q(x). (d) True or
false: If B = p(A) and C = q(A), then B C = C B. Check your answer in the particular
case of part (b).
♥1.2.36. A block matrix has the form M =

A
B
C
D
	
in which A, B, C, D are matrices with
respective sizes i × k, i × l, j × k, j × l. (a) What is the size of M?
(b) Write out the
block matrix M when A =

1
3
	
, B =

1
−1
0
1
	
, C =
⎛
⎜
⎝
1
−2
1
⎞
⎟
⎠, D =
⎛
⎜
⎝
1
3
2
0
1
−1
⎞
⎟
⎠.
(c) Show that if N =

P
Q
R
S
	
is a block matrix whose blocks have the same size as those
of M, then M + N =

A + P
B + Q
C + R
D + S
	
, i.e., matrix addition can be done in blocks.
(d) Show that if P =

X
Y
Z
W
	
has blocks of a compatible size, the matrix product is
M P =

AX + B Z
AY + B W
C X + DZ
C Y + DW
	
. Explain what “compatible” means.
(e) Write down
a compatible block matrix P for the matrix M in part (b). Then validate the block matrix
product identity of part (d) for your chosen matrices.

12
1 Linear Algebraic Systems
♥1.2.37. The matrix S is said to be a square root of the matrix A if S2 = A. (a) Show that
S =

1
1
3
−1
	
is a square root of the matrix A =

4
0
0
4
	
. Can you ﬁnd another square
root of A? (b) Explain why only square matrices can have a square root. (c) Find all real
square roots of the 2 × 2 identity matrix I =

1
0
0
1
	
. (d) Does −I =

−1
0
0
−1
	
have a
real square root?
1.3 Gaussian Elimination — Regular Case
With the basic matrix arithmetic operations in hand, let us now return to our primary
task. The goal is to develop a systematic method for solving linear systems of equations.
While we could continue to work directly with the equations, matrices provide a convenient
alternative that begins by merely shortening the amount of writing, but ultimately leads
to profound insight into the structure of linear systems and their solutions.
We begin by replacing the system (1.7) by its matrix constituents. It is convenient to
ignore the vector of unknowns, and form the augmented matrix
M =

A | b

=
⎛
⎜
⎜
⎝
a11
a12
. . .
a1n
a21
a22
. . .
a2n
...
...
...
...
am1
am2
. . .
amn

b1
b2
...
bm
⎞
⎟
⎟
⎠,
(1.15)
which is an m × (n + 1) matrix obtained by tacking the right-hand side vector onto the
original coeﬃcient matrix. The extra vertical line is included just to remind us that the
last column of this matrix plays a special role. For example, the augmented matrix for the
system (1.1), i.e.,
x + 2y + z = 2,
2x + 6y + z = 7,
x + y + 4z = 3,
is
M =
⎛
⎝
1
2
1
2
6
1
1
1
4

2
7
3
⎞
⎠.
(1.16)
Note that one can immediately recover the equations in the original linear system from
the augmented matrix. Since operations on equations also aﬀect their right-hand sides,
keeping track of everything is most easily done through the augmented matrix.
For the time being, we will concentrate our eﬀorts on linear systems that have the same
number, n, of equations as unknowns. The associated coeﬃcient matrix A is square, of
size n × n. The corresponding augmented matrix M =

A | b

then has size n × (n + 1).
The matrix operation that assumes the role of Linear System Operation #1 is:
Elementary Row Operation #1:
Add a scalar multiple of one row of the augmented matrix to another row.
For example, if we add −2 times the ﬁrst row of the augmented matrix (1.16) to the second
row, the result is the row vector
−2 ( 1 2
1 2 ) + ( 2
6 1 7 ) = ( 0 2
−1 3 ).
The result can be recognized as the second row of the modiﬁed augmented matrix
⎛
⎝
1
2
1
0
2
−1
1
1
4

2
3
3
⎞
⎠
(1.17)

1.3 Gaussian Elimination — Regular Case
13
that corresponds to the ﬁrst equivalent system (1.2). When elementary row operation #1
is performed, it is critical that the result replaces the row being added to — not the row
being multiplied by the scalar. Notice that the elimination of a variable in an equation —
in this case, the ﬁrst variable in the second equation — amounts to making its entry in the
coeﬃcient matrix equal to zero.
We shall call the (1, 1) entry of the coeﬃcient matrix the ﬁrst pivot.
The precise
deﬁnition of pivot will become clear as we continue; the one key requirement is that a
pivot must always be nonzero. Eliminating the ﬁrst variable x from the second and third
equations amounts to making all the matrix entries in the column below the pivot equal to
zero. We have already done this with the (2, 1) entry in (1.17). To make the (3, 1) entry
equal to zero, we subtract (that is, add −1 times) the ﬁrst row from the last row. The
resulting augmented matrix is
⎛
⎝
1
2
1
0
2
−1
0
−1
3

2
3
1
⎞
⎠,
which corresponds to the system (1.3). The second pivot is the (2, 2) entry of this matrix,
which is 2, and is the coeﬃcient of the second variable in the second equation. Again, the
pivot must be nonzero. We use the elementary row operation of adding 1
2 of the second
row to the third row to make the entry below the second pivot equal to 0; the result is the
augmented matrix
N =
⎛
⎝
1
2
1
0
2
−1
0
0
5
2

2
3
5
2
⎞
⎠
that corresponds to the triangular system (1.4). We write the ﬁnal augmented matrix as
N =

U | c

,
where
U =
⎛
⎝
1
2
1
0
2
−1
0
0
5
2
⎞
⎠,
c =
⎛
⎝
2
3
5
2
⎞
⎠.
The corresponding linear system has vector form
U x = c.
(1.18)
Its coeﬃcient matrix U is upper triangular, which means that all its entries below the
main diagonal are zero: uij = 0 whenever i > j. The three nonzero entries on its diagonal,
1, 2, 5
2, including the last one in the (3, 3) slot, are the three pivots. Once the system has
been reduced to triangular form (1.18), we can easily solve it by Back Substitution.
The preceding algorithm for solving a linear system of n equations in n unknowns is
known as regular Gaussian Elimination. A square matrix A will be called regular † if the
algorithm successfully reduces it to upper triangular form U with all non-zero pivots on the
diagonal. In other words, for regular matrices, as the algorithm proceeds, each successive
pivot appearing on the diagonal must be nonzero; otherwise, the matrix is not regular.
We then use the pivot row to make all the entries lying in the column below the pivot
equal to zero through elementary row operations. The solution is found by applying Back
Substitution to the resulting triangular system.
†
Strangely, there is no commonly accepted term to describe this kind of matrix. For lack of a
better alternative, we propose to use the adjective “regular” in the sequel.

14
1 Linear Algebraic Systems
Gaussian Elimination — Regular Case
start
for j = 1 to n
if mjj = 0, stop; print “A is not regular”
else for i = j + 1 to n
set lij = mij/mjj
add −lij times row j of M to row i of M
next i
next j
end
Let us state this algorithm in the form of a program, written in a general “pseudocode”
that can be easily translated into any speciﬁc language, e.g., C++, Fortran, Java,
Maple, Mathematica, Matlab. In accordance with the usual programming conven-
tion, the same letter M = (mij) will be used to denote the current augmented matrix at
each stage in the computation, keeping in mind that its entries will change as the algorithm
progresses. We initialize M =

A | b

. The ﬁnal output of the program, assuming A is
regular, is the augmented matrix M =

U | c

, where U is the upper triangular matrix
whose diagonal entries are the pivots, while c is the resulting vector of right-hand sides in
the triangular system U x = c.
For completeness, let us include the pseudocode program for Back Substitution. The
input to this program is the upper triangular matrix U and the right-hand side vector c that
results from the Gaussian Elimination pseudocode program, which produces M =

U | c

.
The output of the Back Substitution program is the solution vector x to the triangular
system U x = c, which is the same as the solution to the original linear system A x = b.
Back Substitution
start
set xn = cn/unn
for i = n −1 to 1 with increment −1
set xi = 1
uii
⎛
⎝ci −
i+1

j =1
uijxj
⎞
⎠
next j
end
Exercises
1.3.1. Solve the following linear systems by Gaussian Elimination. (a)

1
−1
1
2
	 
x
y
	
=

7
3
	
,

1.3 Gaussian Elimination — Regular Case
15
(b)

6
1
3
−2
	 
u
v
	
=

5
5
	
,
(c)
⎛
⎜
⎝
2
1
2
−1
3
3
4
−3
0
⎞
⎟
⎠
⎛
⎜
⎝
u
v
w
⎞
⎟
⎠=
⎛
⎜
⎝
3
−2
7
⎞
⎟
⎠,
(d)
⎛
⎜
⎝
5
3
−1
3
2
−1
1
1
2
⎞
⎟
⎠
⎛
⎜
⎝
x1
x2
x3
⎞
⎟
⎠=
⎛
⎜
⎝
9
5
−1
⎞
⎟
⎠,
(e)
⎛
⎜
⎝
1
1
−1
2
−1
3
−1
−1
3
⎞
⎟
⎠
⎛
⎜
⎝
p
q
r
⎞
⎟
⎠=
⎛
⎜
⎝
0
3
5
⎞
⎟
⎠,
(f )
⎛
⎜
⎜
⎜
⎝
−1
1
1
0
2
−1
0
1
1
0
2
3
0
1
−1
−2
⎞
⎟
⎟
⎟
⎠
⎛
⎜
⎜
⎜
⎝
a
b
c
d
⎞
⎟
⎟
⎟
⎠=
⎛
⎜
⎜
⎜
⎝
1
0
1
0
⎞
⎟
⎟
⎟
⎠,
(g)
⎛
⎜
⎜
⎜
⎝
2
−3
1
1
1
−1
−2
−1
3
−2
1
2
1
3
2
1
⎞
⎟
⎟
⎟
⎠
⎛
⎜
⎜
⎜
⎝
x
y
z
w
⎞
⎟
⎟
⎟
⎠=
⎛
⎜
⎜
⎜
⎝
−1
0
5
3
⎞
⎟
⎟
⎟
⎠.
1.3.2. Write out the augmented matrix for the following linear systems. Then solve the system
by ﬁrst applying elementary row operations of type #1 to place the augmented matrix in
upper triangular form, followed by Back Substitution.
(a)
x1 + 7x2 = 4,
−2x1 −9x2 = 2.
(b) 3z −5w = −1,
2z + w = 8.
(c)
x −2y + z = 0,
2y −8z = 8,
−4x + 5y + 9z = −9.
(d)
p + 4q −2r = 1,
−2p −3r = −7,
3p −2q + 2r = −1.
(e)
x1 −2x3 = −1,
x2 −x4 = 2,
−3x2 + 2x3 = 0,
−4x1 + 7x4 = −5.
(f )
−x + 3y −z + w = −2,
x −y + 3z −w = 0,
y −z + 4w = 7,
4x −y + z = 5.
1.3.3. For each of the following augmented matrices write out the corresponding linear system
of equations. Solve the system by applying Gaussian Elimination to the augmented matrix.
(a)

3
2
−4
−3





2
−1
	
,
(b)
⎛
⎜
⎝
1
2
0
−1
2
1
−2
0
−3







−3
−6
1
⎞
⎟
⎠,
(c)
⎛
⎜
⎜
⎜
⎝
2
−1
0
0
−1
2
−1
0
0
−1
2
−1
0
0
−1
2









0
1
1
0
⎞
⎟
⎟
⎟
⎠.
1.3.4. Which of the following matrices are regular?
(a)

2
1
1
4
	
,
(b)

0
−1
3
−2
	
,
(c)
⎛
⎜
⎝
3
−2
1
−1
4
−3
3
−2
5
⎞
⎟
⎠,
(d)
⎛
⎜
⎝
1
−2
3
−2
4
−1
3
−1
2
⎞
⎟
⎠,
(e)
⎛
⎜
⎜
⎜
⎝
1
3
−3
0
−1
0
−1
2
3
3
−6
1
2
3
−3
5
⎞
⎟
⎟
⎟
⎠.
1.3.5. The techniques that are developed for solving linear systems are also applicable to
systems with complex coeﬃcients, whose solutions may also be complex. Use Gaussian
Elimination to solve the following complex linear systems.
(a) −i x1 + (1 + i )x2 = −1,
(1 −i )x1 + x2 = −3 i .
(b)
i x + (1 −i )z = 2 i ,
2 i y + (1 + i )z = 2,
−x + 2 i y + i z = 1 −2 i .
(c)
(1 −i )x + 2y = i ,
−i x + (1 + i )y = −1.
(d)
(1 + i )x + i y + (2 + 2 i )z = 0,
(1 −i )x + 2y + i z = 0,
(3 −3 i )x + i y + (3 −11 i )z = 6.
1.3.6.(a) Write down an example of a system of 5 linear equations in 5 unknowns with regular
diagonal coeﬃcient matrix. (b) Solve your system. (c) Explain why solving a system
whose coeﬃcient matrix is diagonal is very easy.
1.3.7. Find the equation of the parabola y = ax2 + bx + c that goes through the points
(1, 6), (2, 4), and (3, 0).
♦1.3.8. A linear system is called homogeneous if all the right-hand sides are zero, and so takes
the matrix form A x = 0. Explain why the solution to a homogeneous system with regular
coeﬃcient matrix is x = 0.

16
1 Linear Algebraic Systems
1.3.9. Under what conditions do two 2 × 2 upper triangular matrices commute?
1.3.10. A matrix is called lower triangular if all entries above the diagonal are zero. Show that
a matrix is both lower and upper triangular if and only if it is a diagonal matrix.
♦1.3.11. A square matrix is called strictly lower (upper) triangular if all entries on or above
(below) the main diagonal are 0. (a) Prove that every square matrix can be uniquely
written as a sum A = L + D + U, with L strictly lower triangular, D diagonal, and U
strictly upper triangular.
(b) Decompose A =
⎛
⎜
⎝
3
1
−1
1
−4
2
−2
0
5
⎞
⎟
⎠in this manner.
♦1.3.12. A square matrix N is called nilpotent if Nk = O for some k ≥1.
(a) Show that N =
⎛
⎜
⎝
0
1
2
0
0
1
0
0
0
⎞
⎟
⎠is nilpotent. (b) Show that every strictly upper triangular
matrix, as deﬁned in Exercise 1.3.11, is nilpotent. (c) Find a nilpotent matrix which is
neither lower nor upper triangular.
♦1.3.13. A square matrix W is called unipotent if N = W −I is nilpotent, as in Exercise 1.3.12,
so (W −I )k = O for some k ≥1. (a) Show that every lower or upper triangular matrix is
unipotent if and only if it is unitriangular, meaning its diagonal entries are all equal to 1.
(b) Find a unipotent matrix which is neither lower nor upper triangular.
1.3.14. A square matrix P is called idempotent if P 2 = P. (a) Find all 2 × 2 idempotent upper
triangular matrices. (b) Find all 2 × 2 idempotent matrices.
Elementary Matrices
A key observation is that elementary row operations can, in fact, be realized by matrix
multiplication. To this end, we introduce the ﬁrst type of “elementary matrix”. (Later we
will meet two other types of elementary matrix, corresponding to the other two kinds of
elementary row operation.)
Deﬁnition 1.1. The elementary matrix associated with an elementary row operation for
m-rowed matrices is the m × m matrix obtained by applying the row operation to the
m × m identity matrix Im .
For example, applying the elementary row operation that adds −2 times the ﬁrst row to
the second row of the 3 × 3 identity matrix I =
⎛
⎝
1
0
0
0
1
0
0
0
1
⎞
⎠results in the corresponding
elementary matrix E1 =
⎛
⎝
1
0
0
−2
1
0
0
0
1
⎞
⎠. We claim that, if A is any 3–rowed matrix, then
multiplying E1 A has the same eﬀect as the given elementary row operation. For example,
⎛
⎝
1
0
0
−2
1
0
0
0
1
⎞
⎠
⎛
⎝
1
2
1
2
6
1
1
1
4
⎞
⎠=
⎛
⎝
1
2
1
0
2
−1
1
1
4
⎞
⎠,
which you may recognize as the ﬁrst elementary row operation we used to solve our

1.3 Gaussian Elimination — Regular Case
17
illustrative example. If we set
E1 =
⎛
⎝
1
0
0
−2
1
0
0
0
1
⎞
⎠,
E2 =
⎛
⎝
1
0
0
0
1
0
−1
0
1
⎞
⎠,
E3 =
⎛
⎝
1
0
0
0
1
0
0
1
2
1
⎞
⎠,
(1.19)
then multiplication by E1 will subtract twice the ﬁrst row from the second row, multipli-
cation by E2 will subtract the ﬁrst row from the third row, and multiplication by E3 will
add 1
2 the second row to the third row — precisely the row operations used to place our
original system in triangular form. Therefore, performing them in the correct order, we
conclude that when
A =
⎛
⎝
1
2
1
2
6
1
1
1
4
⎞
⎠,
then
E3 E2 E1 A = U =
⎛
⎝
1
2
1
0
2
−1
0
0
5
2
⎞
⎠.
(1.20)
The reader is urged to check this by directly multiplying the indicated matrices. Keep in
mind that the associative property of matrix multiplication allows us to compute the above
matrix product in any convenient order:
E3 E2 E1 A = E3 (E2 (E1 A)) = ((E3 E2)E1)A = (E3 (E2 E1))A = (E3 E2)(E1 A) = · · · ,
making sure that the overall left to right order of the matrices is maintained, since the
matrix products are usually not commutative.
In general, then, an m × m elementary matrix E of the ﬁrst type will have all 1’s on the
diagonal, one nonzero entry c in some oﬀ-diagonal position (i, j), with i ̸= j, and all other
entries equal to zero. If A is any m × n matrix, then the matrix product E A is equal to
the matrix obtained from A by the elementary row operation adding c times row j to row
i. (Note that the order of i and j is reversed.)
To undo the operation of adding c times row j to row i, we must perform the inverse
row operation that subtracts c (or, equivalently, adds −c) times row j from row i. The
corresponding inverse elementary matrix again has 1’s along the diagonal and −c in the
(i, j) slot. Let us denote the inverses of the particular elementary matrices (1.19) by Li,
so that, according to our general rule,
L1 =
⎛
⎝
1
0
0
2
1
0
0
0
1
⎞
⎠,
L2 =
⎛
⎝
1
0
0
0
1
0
1
0
1
⎞
⎠,
L3 =
⎛
⎝
1
0
0
0
1
0
0
−1
2
1
⎞
⎠.
(1.21)
Note that the products
L1 E1 = L2 E2 = L3 E3 = I
(1.22)
yield the 3 × 3 identity matrix, reﬂecting the fact that the matrices represent mutually
inverse row operations. (A more thorough discussion of matrix inverses will be postponed
until Section 1.5.)
The product of the latter three elementary matrices (1.21) is equal to
L = L1 L2 L3 =
⎛
⎝
1
0
0
2
1
0
1
−1
2
1
⎞
⎠.
(1.23)
The matrix L is called a lower unitriangular matrix, where “lower triangular” means that
all the entries above the main diagonal are 0, while “uni-”, which is short for “unipotent”

18
1 Linear Algebraic Systems
as deﬁned in Exercise 1.3.13, imposes the requirement that all the entries on the diag-
onal are equal to 1. Observe that the entries of L below the diagonal are the same as
the corresponding nonzero entries in the Li. This is a general fact that holds when the
lower triangular elementary matrices are multiplied in the correct order. More generally,
the following elementary consequence of the laws of matrix multiplication will be used
extensively.
Lemma 1.2. If L and L are lower triangular matrices of the same size, so is their product
L L. If they are both lower unitriangular, so is their product. Similarly, if U, U are upper
(uni)triangular matrices, so is their product U U.
The LU Factorization
We have almost arrived at our ﬁrst important result. Let us compute the product of the
matrices L and U in (1.20), (1.23). Using associativity of matrix multiplication, equa-
tions (1.22), and the basic property of the identity matrix I , we conclude that
LU = (L1L2L3)(E3E2E1A) = L1L2(L3E3)E2E1A = L1L2 I E2E1A
= L1(L2E2)E1A = L1 I E1A = (L1E1)A = I A = A.
In other words, we have factored the coeﬃcient matrix A = LU into a product of a lower
unitriangular matrix L and an upper triangular matrix U with the nonzero pivots on its
main diagonal. By similar reasoning, the same holds true for any regular square matrix.
Theorem 1.3. A matrix A is regular if and only if it can be factored
A = L U,
(1.24)
where L is a lower unitriangular matrix, having all 1’s on the diagonal, and U is upper
triangular with nonzero diagonal entries, which are the pivots of A.
The nonzero oﬀ-
diagonal entries lij for i > j appearing in L prescribe the elementary row operations that
bring A into upper triangular form; namely, one subtracts lij times row j from row i at
the appropriate step of the Gaussian Elimination process.
In practice, to ﬁnd the LU factorization of a square matrix A, one applies the regular
Gaussian Elimination algorithm to reduce A to its upper triangular form U. The entries
of L can be ﬁlled in during the course of the calculation with the negatives of the multiples
used in the elementary row operations.
If the algorithm fails to be completed, which
happens whenever zero appears in any diagonal pivot position, then the original matrix is
not regular, and does not have an LU factorization.
Example 1.4.
Let us compute the LU factorization of the matrix A =
⎛
⎝
2
1
1
4
5
2
2
−2
0
⎞
⎠.
Applying the Gaussian Elimination algorithm, we begin by adding −2 times the ﬁrst row
to the second row, and then adding −1 times the ﬁrst row to the third. The result is the
matrix
⎛
⎝
2
1
1
0
3
0
0
−3
−1
⎞
⎠. The next step adds the second row to the third row, leading to the
upper triangular matrix U =
⎛
⎝
2
1
1
0
3
0
0
0
−1
⎞
⎠, whose diagonal entries are the pivots. The

1.3 Gaussian Elimination — Regular Case
19
corresponding lower triangular matrix is L =
⎛
⎝
1
0
0
2
1
0
1
−1
1
⎞
⎠; its entries lying below the
main diagonal are the negatives of the multiples we used during the elimination procedure.
For instance, the (2, 1) entry indicates that we added −2 times the ﬁrst row to the second
row, and so on. The reader might wish to verify the resulting factorization
⎛
⎝
2
1
1
4
5
2
2
−2
0
⎞
⎠= A = LU =
⎛
⎝
1
0
0
2
1
0
1
−1
1
⎞
⎠
⎛
⎝
2
1
1
0
3
0
0
0
−1
⎞
⎠.
Exercises
1.3.15. What elementary row operations do the following matrices represent? What size
matrices do they apply to?
(a)

1
−2
0
1
	
, (b)

1
0
7
1
	
, (c)
⎛
⎜
⎝
1
0
0
0
1
−5
0
0
1
⎞
⎟
⎠, (d)
⎛
⎜
⎝
1
0
0
0
1
0
1
2
0
1
⎞
⎟
⎠, (e)
⎛
⎜
⎜
⎜
⎝
1
0
0
0
0
1
0
−3
0
0
1
0
0
0
0
1
⎞
⎟
⎟
⎟
⎠.
1.3.16. Write down the elementary matrix corresponding to the following row operations on
4 × 4 matrices: (a) Add the third row to the fourth row. (b) Subtract the fourth row
from the third row.
(c) Add 3 times the last row to the ﬁrst row. (d) Subtract twice the
second row from the fourth row.
1.3.17. Compute the product L3 L2 L1 of the elementary matrices (1.21). Compare your
answer with (1.23).
1.3.18. Determine the product E3 E2 E1 of the elementary matrices in (1.19). Is this the same
as the product E1 E2 E3? Which is easier to predict?
1.3.19.(a) Explain, using their interpretation as elementary row operations, why elementary
matrices do not generally commute: E E ̸= E E.
(b) Which pairs of the elementary
matrices listed in (1.19) commute? (c) Can you formulate a general rule that tells in
advance whether two given elementary matrices commute?
1.3.20. Determine which of the following 3 × 3 matrices is (i) upper triangular, (ii) upper
unitriangular, (iii) lower triangular, and/or (iv) lower unitriangular:
(a)
⎛
⎜
⎝
1
2
0
0
3
2
0
0
−2
⎞
⎟
⎠(b)
⎛
⎜
⎝
1
0
0
0
1
0
0
0
1
⎞
⎟
⎠(c)
⎛
⎜
⎝
1
0
0
2
0
0
0
3
3
⎞
⎟
⎠(d)
⎛
⎜
⎝
1
0
0
0
1
0
1
−4
1
⎞
⎟
⎠(e)
⎛
⎜
⎝
0
0
0
0
3
1
0
1
0
⎞
⎟
⎠.
1.3.21. Find the LU factorization of the following matrices:
(a)

1
3
−1
0
	
,
(b)

1
3
3
1
	
,
(c)
⎛
⎜
⎝
−1
1
−1
1
1
1
−1
1
2
⎞
⎟
⎠,
(d)
⎛
⎜
⎝
2
0
3
1
3
1
0
1
1
⎞
⎟
⎠,
(e)
⎛
⎜
⎝
−1
0
0
2
−3
0
1
3
2
⎞
⎟
⎠,
(f )
⎛
⎜
⎝
1
0
−1
2
3
2
−3
1
0
⎞
⎟
⎠,
(g)
⎛
⎜
⎜
⎜
⎝
1
0
−1
0
0
2
−1
−1
−1
3
0
2
0
−1
2
1
⎞
⎟
⎟
⎟
⎠,
(h)
⎛
⎜
⎜
⎜
⎝
1
1
−2
3
−1
2
3
0
−2
1
1
−2
3
0
1
5
⎞
⎟
⎟
⎟
⎠,
(i)
⎛
⎜
⎜
⎜
⎝
2
1
3
1
1
4
0
1
3
0
2
2
1
1
2
2
⎞
⎟
⎟
⎟
⎠.
1.3.22. Given the factorization A =
⎛
⎜
⎝
2
−1
0
−6
4
−1
4
−6
7
⎞
⎟
⎠=
⎛
⎜
⎝
1
0
0
−3
1
0
2
−4
1
⎞
⎟
⎠
⎛
⎜
⎝
2
−1
0
0
1
−1
0
0
3
⎞
⎟
⎠,
explain, without computing, which elementary row operations are used to reduce A to
upper triangular form. Be careful to state the order in which they should be applied. Then
check the correctness of your answer by performing the elimination.

20
1 Linear Algebraic Systems
1.3.23.(a) Write down a 4 × 4 lower unitriangular matrix whose entries below the diagonal
are distinct nonzero numbers. (b) Explain which elementary row operation each entry
corresponds to. (c) Indicate the order in which the elementary row operations should be
performed by labeling the entries 1, 2, 3, . . . .
♦1.3.24. Let t1, t2, . . . be distinct real numbers. Find the LU factorization of the following
Vandermonde matrices: (a)

1
1
t1
t2
	
, (b)
⎛
⎜
⎝
1
1
1
t1
t2
t3
t2
1
t2
2
t2
3
⎞
⎟
⎠, (c)
⎛
⎜
⎜
⎜
⎜
⎝
1
1
1
1
t1
t2
t3
t4
t2
1
t2
2
t2
3
t2
4
t3
1
t3
2
t3
3
t3
4
⎞
⎟
⎟
⎟
⎟
⎠.
Can you spot a pattern? Test your conjecture with the 5 × 5 Vandermonde matrix.
1.3.25. Write down the explicit requirements on its entries aij for a square matrix A to be
(a) diagonal, (b) upper triangular, (c) upper unitriangular, (d) lower triangular,
(e) lower unitriangular.
♦1.3.26.(a) Explain why the product of two lower triangular matrices is lower triangular.
(b) What can you say concerning the diagonal entries of the product of two lower
triangular matrices? (c) Explain why the product of two lower unitriangular matrices is
also lower unitriangular.
1.3.27. True or false: If A has a zero entry on its main diagonal, it is not regular.
1.3.28. In general, how many elementary row operations does one need to perform in order to
reduce a regular n × n matrix to upper triangular form?
1.3.29. Prove that if A is a regular 2 × 2 matrix, then its LU factorization is unique. In other
words, if A = LU = L U where L, L are lower unitriangular and U, U are upper triangular,
then L = L and U = U. (The general case appears in Proposition 1.30.)
♦1.3.30. Prove directly that the matrix A =

0
1
1
0
	
does not have an LU factorization.
♦1.3.31. Suppose A is regular. (a) Show that the matrix obtained by multiplying each column
of A by the sign of its pivot is also regular and, moreover, has all positive pivots.
(b) Show that the matrix obtained by multiplying each row of A by the sign of its pivot is
also regular and has all positive pivots.
(c) Check these results in the particular case A =
⎛
⎜
⎝
−2
2
1
1
0
1
4
2
3
⎞
⎟
⎠.
Forward and Back Substitution
Knowing the LU factorization of a regular matrix A enables us to solve any associated
linear system A x = b in two easy stages:
(1) First, solve the lower triangular system
L c = b
(1.25)
for the vector c by Forward Substitution. This is the same as Back Substitution, except
one solves the equations for the variables in the direct order — from ﬁrst to last. Explicitly,
c1 = b1,
ci = bi −
i−1

j =1
lij cj,
for
i = 2, 3, . . ., n,
(1.26)
noting that the previously computed values of c1, . . . , ci−1 are used to determine ci.
(2) Second, solve the resulting upper triangular system
U x = c
(1.27)

1.3 Gaussian Elimination — Regular Case
21
by Back Substitution. The values of the unknowns
xn = cn
unn
,
xi = 1
uii
⎛
⎝ci −
n

j =i+1
uij xj
⎞
⎠,
for
i = n −1, . . . , 2, 1,
(1.28)
are successively computed, but now in reverse order. It is worth pointing out that the
requirement that each pivot be nonzero, uii ̸= 0, is essential here, as otherwise we would
not be able to solve for the corresponding variable xi.
Note that the combined algorithm does indeed solve the original system, since if
U x = c
and
L c = b,
then
A x = L U x = L c = b.
Example 1.5.
With the LU decomposition
⎛
⎝
2
1
1
4
5
2
2
−2
0
⎞
⎠=
⎛
⎝
1
0
0
2
1
0
1
−1
1
⎞
⎠
⎛
⎝
2
1
1
0
3
0
0
0
−1
⎞
⎠
found in Example 1.4, we can readily solve any linear system with the given coeﬃcient
matrix by Forward and Back Substitution. For instance, to ﬁnd the solution to
⎛
⎝
2
1
1
4
5
2
2
−2
0
⎞
⎠
⎛
⎝
x
y
z
⎞
⎠=
⎛
⎝
1
2
2
⎞
⎠,
we ﬁrst solve the lower triangular system
⎛
⎝
1
0
0
2
1
0
1
−1
1
⎞
⎠
⎛
⎝
a
b
c
⎞
⎠=
⎛
⎝
1
2
2
⎞
⎠,
or, explicitly,
a
= 1,
2a + b
= 2,
a −b + c = 2.
The ﬁrst equation says a = 1; substituting into the second, we ﬁnd b = 0; the ﬁnal equation
yields c = 1. We then use Back Substitution to solve the upper triangular system
⎛
⎝
2
1
1
0
3
0
0
0
−1
⎞
⎠
⎛
⎝
x
y
z
⎞
⎠=
⎛
⎝
a
b
c
⎞
⎠=
⎛
⎝
1
0
1
⎞
⎠,
which is
2x + y + z = 1,
3y
= 0,
−z = 1.
We ﬁnd z = −1, then y = 0, and then x = 1, which is indeed the solution.
Thus, once we have found the LU factorization of the coeﬃcient matrix A, the Forward
and Back Substitution processes quickly produce the solution to any system A x = b.
Moreover, they can be straightforwardly programmed on a computer. In practice, to solve
a system from scratch, it is just a matter of taste whether you work directly with the
augmented matrix, or ﬁrst determine the LU factorization of the coeﬃcient matrix, and
then apply Forward and Back Substitution to compute the solution.
Exercises
1.3.32. Given the LU factorizations you calculated in Exercise 1.3.21, solve the associated
linear systems A x = b, where b is the column vector with all entries equal to 1.

22
1 Linear Algebraic Systems
1.3.33. In each of the following problems, ﬁnd the A = LU factorization of the coeﬃcient
matrix, and then use Forward and Back Substitution to solve the corresponding linear
systems A x = bj for each of the indicated right-hand sides:
(a) A =

−1
3
3
2
	
, b1 =

1
−1
	
, b2 =

2
5
	
, b3 =

0
3
	
.
(b) A =
⎛
⎜
⎝
−1
1
−1
1
1
1
−1
1
2
⎞
⎟
⎠, b1 =
⎛
⎜
⎝
1
−1
1
⎞
⎟
⎠, b2 =
⎛
⎜
⎝
−3
0
2
⎞
⎟
⎠.
(c) A =
⎛
⎜
⎝
9
−2
−1
−6
1
1
2
−1
0
⎞
⎟
⎠, b1 =
⎛
⎜
⎝
2
−1
0
⎞
⎟
⎠, b2 =
⎛
⎜
⎝
1
2
5
⎞
⎟
⎠.
(d) A =
⎛
⎜
⎝
2.0
.3
.4
.3
4.0
.5
.4
.5
6.0
⎞
⎟
⎠, b1 =
⎛
⎜
⎝
1
0
0
⎞
⎟
⎠, b2 =
⎛
⎜
⎝
0
1
0
⎞
⎟
⎠, b3 =
⎛
⎜
⎝
0
0
1
⎞
⎟
⎠.
(e) A =
⎛
⎜
⎜
⎜
⎝
1
0
−1
0
0
2
3
−1
−1
3
2
2
0
−1
2
1
⎞
⎟
⎟
⎟
⎠, b1 =
⎛
⎜
⎜
⎜
⎝
1
0
−1
1
⎞
⎟
⎟
⎟
⎠, b2 =
⎛
⎜
⎜
⎜
⎝
0
−1
0
1
⎞
⎟
⎟
⎟
⎠.
(f ) A =
⎛
⎜
⎜
⎜
⎝
1
−2
0
2
4
1
−1
−1
−8
−1
2
1
−4
−1
1
2
⎞
⎟
⎟
⎟
⎠, b1 =
⎛
⎜
⎜
⎜
⎝
1
0
0
0
⎞
⎟
⎟
⎟
⎠, b2 =
⎛
⎜
⎜
⎜
⎝
3
0
−1
2
⎞
⎟
⎟
⎟
⎠, b3 =
⎛
⎜
⎜
⎜
⎝
2
3
−2
1
⎞
⎟
⎟
⎟
⎠.
1.4 Pivoting and Permutations
The method of Gaussian Elimination presented so far applies only to regular matrices.
But not every square matrix is regular; a simple class of examples is matrices whose upper
left, i.e., (1, 1), entry is zero, and so cannot serve as the ﬁrst pivot. More generally, the
algorithm cannot proceed whenever a zero entry appears in the current pivot position on
the diagonal. What then to do? The answer requires revisiting the source of the method.
Consider, as a speciﬁc example, the linear system
2y + z = 2,
2x + 6y + z = 7,
x + y + 4z = 3.
(1.29)
The augmented coeﬃcient matrix is
⎛
⎝
0
2
1
2
6
1
1
1
4

2
7
3
⎞
⎠.
In this case, the (1, 1) entry is 0, and so is not a legitimate pivot. The problem, of course,
is that the ﬁrst variable x does not appear in the ﬁrst equation, and so we cannot use it
to eliminate x in the other two equations. But this “problem” is actually a bonus — we
already have an equation with only two variables in it, and so we need to eliminate x from
only one of the other two equations. To be systematic, we rewrite the system in a diﬀerent
order,
2x + 6y + z = 7,
2y + z = 2,
x + y + 4z = 3,

1.4 Pivoting and Permutations
23
by interchanging the ﬁrst two equations. In other words, we employ
Linear System Operation #2:
Interchange two equations.
Clearly, this operation does not change the solution and so produces an equivalent linear
system. In our case, the augmented coeﬃcient matrix,
⎛
⎝
2
6
1
0
2
1
1
1
4

7
2
3
⎞
⎠,
can be obtained from the original by performing the second type of row operation:
Elementary Row Operation #2:
Interchange two rows of the matrix.
The new nonzero upper left entry, 2, can now serve as the ﬁrst pivot, and we may
continue to apply elementary row operations of type #1 to reduce our matrix to upper
triangular form. For this particular example, we eliminate the remaining nonzero entry in
the ﬁrst column by subtracting 1
2 the ﬁrst row from the last:
⎛
⎝
2
6
1
0
2
1
0
−2
7
2

7
2
−1
2
⎞
⎠.
The (2, 2) entry serves as the next pivot. To eliminate the nonzero entry below it, we add
the second to the third row:
⎛
⎝
2
6
1
0
2
1
0
0
9
2

7
2
3
2
⎞
⎠.
We have now placed the system in upper triangular form, with the three pivots 2, 2, and
9
2 along the diagonal. Back Substitution produces the solution x = 5
6, y = 5
6, z = 1
3.
The row interchange that is required when a zero shows up in the diagonal pivot position
is known as pivoting. Later, in Section 1.7, we will discuss practical reasons for pivoting
even when a diagonal entry is nonzero. Let us distinguish the class of matrices that can be
reduced to upper triangular form by Gaussian Elimination with pivoting. These matrices
will prove to be of fundamental importance throughout linear algebra.
Deﬁnition 1.6. A square matrix is called nonsingular if it can be reduced to upper tri-
angular form with all non-zero elements on the diagonal — the pivots — by elementary
row operations of types 1 and 2.
In contrast, a singular square matrix cannot be reduced to such upper triangular form
by such row operations, because at some stage in the elimination procedure the diagonal
entry and all the entries below it are zero. Every regular matrix is nonsingular, but, as
we just saw, not every nonsingular matrix is regular. Uniqueness of solutions is the key
deﬁning characteristic of nonsingularity.
Theorem 1.7. A linear system A x = b has a unique solution for every choice of right-
hand side b if and only if its coeﬃcient matrix A is square and nonsingular.
We are able to prove the “if” part of this theorem, since nonsingularity implies reduction
to an equivalent upper triangular form that has the same solutions as the original system.

24
1 Linear Algebraic Systems
The unique solution to the system is then found by Back Substitution. The “only if” part
will be proved in Section 1.8.
The revised version of the Gaussian Elimination algorithm, valid for all nonsingular co-
eﬃcient matrices, is implemented by the accompanying pseudocode program. The starting
point is the augmented matrix M =

A | b

representing the linear system A x = b.
After successful termination of the program, the result is an augmented matrix in upper
triangular form M =

U | c

representing the equivalent linear system U x = c. One then
uses Back Substitution to determine the solution x to the linear system.
Gaussian Elimination — Nonsingular Case
start
for j = 1 to n
if mkj = 0 for all k ≥j, stop; print “A is singular”
if mjj = 0 but mkj ̸= 0 for some k > j, switch rows k and j
for i = j + 1 to n
set lij = mij/mjj
add −lij times row j to row i of M
next i
next j
end
Remark. When performing the algorithm using exact arithmetic, when pivoting is re-
quired it does not matter which row k one chooses to switch with row j, as long as it
lies below and the (k, j) entry is nonzero. When dealing with matters involving numerical
precision and round oﬀerrors, there are some practical rules of thumb to be followed to
maintain accuracy in the intervening computations. These will be discussed in Section 1.7.
Exercises
1.4.1. Determine whether the following matrices are singular or nonsingular:
(a)

0
1
1
2
	
, (b)

−1
2
4
−8
	
, (c)
⎛
⎜
⎝
0
1
2
−1
1
3
2
−2
0
⎞
⎟
⎠, (d)
⎛
⎜
⎝
1
1
3
2
2
2
3
−1
1
⎞
⎟
⎠, (e)
⎛
⎜
⎝
1
2
3
4
5
6
7
8
9
⎞
⎟
⎠,
(f )
⎛
⎜
⎜
⎜
⎝
−1
1
0
−3
2
−2
4
0
1
−2
2
−1
0
1
0
1
⎞
⎟
⎟
⎟
⎠, (g)
⎛
⎜
⎜
⎜
⎝
0
−1
0
1
1
0
−1
0
0
2
0
−2
2
0
2
0
⎞
⎟
⎟
⎟
⎠, (h)
⎛
⎜
⎜
⎜
⎝
1
−2
0
2
4
1
−1
−1
−8
−1
2
1
−4
−1
1
2
⎞
⎟
⎟
⎟
⎠.
1.4.2. Classify the following matrices as (i) regular, (ii) nonsingular, and/or (iii) singular:
(a)

2
1
1
4
	
, (b)
⎛
⎜
⎝
3
−2
1
−1
4
4
2
2
5
⎞
⎟
⎠, (c)
⎛
⎜
⎝
1
−2
3
−2
4
−1
3
−1
2
⎞
⎟
⎠, (d)
⎛
⎜
⎜
⎜
⎝
1
3
−3
0
−1
0
−1
2
3
−2
6
1
2
−1
3
5
⎞
⎟
⎟
⎟
⎠.
1.4.3. Find the equation z = ax + by + c for the plane passing through the three points
p1 = (0, 2, −1), p2 = (−2, 4, 3), p3 = (2, −1, −3).

1.4 Pivoting and Permutations
25
1.4.4. Show that a 2 × 2 matrix A =

a
b
c
d
	
is (a) nonsingular if and only if ad −bc ̸= 0,
(b) regular if and only if ad −bc ̸= 0 and a ̸= 0.
1.4.5. Solve the following systems of equations by Gaussian Elimination:
(a)
x1 −2x2 + 2x3 = 15,
x1 −2x2 + x3 = 10,
2x1 −x2 −2x3 = −10.
(b)
2x1 −x2 = 1,
−4x1 + 2x2 −3x3 = −8,
x1 −3x2 + x3 = 5.
(c)
x2 −x3 = 4,
−2x1 −5x2 = 2,
x1 + x3 = −8.
(d) x −y + z −w = 0, −2x + 2y −z + w = 2,
−4x + 4y + 3z = 5, x −3y + w = 4.
(e)
−3x2 + 2x3 = 0, x3 −x4 = 2,
x1 −2x3 = −1, −4x1 + 7x4 = −5.
1.4.6. True or false: A singular matrix cannot be regular.
1.4.7. True or false: A square matrix that has a column with all 0 entries is singular. What
can you say about a linear system that has such a coeﬃcient matrix?
♦1.4.8. Explain why the solution to the homogeneous system A x = 0 with nonsingular
coeﬃcient matrix is x = 0.
1.4.9. Write out the details of the proof of the “if” part of Theorem 1.7: if A is nonsingular,
then the linear system A x = b has a unique solution for every b.
Permutations and Permutation Matrices
As with the ﬁrst type of elementary row operation, row interchanges can be accomplished
by multiplication by a second type of elementary matrix, which is found by applying the
row operation to the identity matrix of the appropriate size. For instance, interchanging
rows 1 and 2 of the 3 × 3 identity matrix produces the elementary interchange matrix
P =
⎛
⎝
0
1
0
1
0
0
0
0
1
⎞
⎠. The result P A of multiplying any 3-rowed matrix A on the left by P is
the same as interchanging the ﬁrst two rows of A. For instance,
⎛
⎝
0
1
0
1
0
0
0
0
1
⎞
⎠
⎛
⎝
1
2
3
4
5
6
7
8
9
⎞
⎠=
⎛
⎝
4
5
6
1
2
3
7
8
9
⎞
⎠.
Multiple row interchanges are accomplished by combining such elementary interchange
matrices.
Each such combination of row interchanges uniquely corresponds to what is
called a permutation matrix.
Deﬁnition 1.8. A permutation matrix is a matrix obtained from the identity matrix by
any combination of row interchanges.
In particular, applying a row interchange to a permutation matrix produces another
permutation matrix. The following result is easily established.
Lemma 1.9. A matrix P is a permutation matrix if and only if each row of P contains
all 0 entries except for a single 1, and, in addition, each column of P also contains all 0
entries except for a single 1.
In general, if, in the permutation matrix P, a 1 appears in position (i, j), then multi-
plication by P will move the jth row of A into the ith row of the product P A.

26
1 Linear Algebraic Systems
Example 1.10.
There are six diﬀerent 3 × 3 permutation matrices, namely
⎛
⎝
1
0
0
0
1
0
0
0
1
⎞
⎠,
⎛
⎝
0
1
0
0
0
1
1
0
0
⎞
⎠,
⎛
⎝
0
0
1
1
0
0
0
1
0
⎞
⎠,
⎛
⎝
0
1
0
1
0
0
0
0
1
⎞
⎠,
⎛
⎝
0
0
1
0
1
0
1
0
0
⎞
⎠,
⎛
⎝
1
0
0
0
0
1
0
1
0
⎞
⎠.
(1.30)
These have the following eﬀects: if A is a matrix with row vectors r1, r2, r3, then multipli-
cation on the left by each of the six permutation matrices produces, respectively,
⎛
⎝
r1
r2
r3
⎞
⎠,
⎛
⎝
r2
r3
r1
⎞
⎠,
⎛
⎝
r3
r1
r2
⎞
⎠,
⎛
⎝
r2
r1
r3
⎞
⎠,
⎛
⎝
r3
r2
r1
⎞
⎠,
⎛
⎝
r1
r3
r2
⎞
⎠.
(1.31)
Thus, the ﬁrst permutation matrix, which is the identity, does nothing — the identity
permutation. The fourth, ﬁfth, sixth represent row interchanges. The second and third are
non-elementary permutations, and can be realized by a pair of successive row interchanges.
In general, any rearrangement of a ﬁnite ordered collection of objects is called a per-
mutation. Thus, the 6 permutation matrices (1.30) produce the 6 possible permutations
(1.31) of the rows of a 3 × 3 matrix. In general, if a permutation π rearranges the integers
(1, . . ., n) to form (π(1), . . ., π(n)), then the corresponding permutation matrix P = Pπ
that maps row ri to row rπ(i) will have 1’s in positions (i, π(i)) for i = 1, . . . , n and zeros
everywhere else. For example, the second permutation matrix in (1.30) corresponds to the
permutation with π(1) = 2, π(2) = 3, π(3) = 1. Keep in mind that π(1), . . . , π(n) is merely
a rearrangement of the integers 1, . . . , n, so that 1 ≤π(i) ≤n and π(i) ̸= π(j) when i ̸= j.
An elementary combinatorial argument proves that there is a total of
n! = n (n −1) (n −2) · · · 3 · 2 · 1
(1.32)
diﬀerent permutations of (1, . . ., n), and hence the same number of permutation matrices
of size n × n. Moreover, the product P = P1 P2 of any two permutation matrices is also a
permutation matrix, and corresponds to the composition of the two permutations, meaning
one permutes according to P2 and then permutes the result according to P1. An important
point is that multiplication of permutation matrices is noncommutative — the order in
which one permutes makes a diﬀerence. Switching the ﬁrst and second rows, and then
switching the second and third rows, does not have the same eﬀect as ﬁrst switching the
second and third rows and then switching the ﬁrst and second rows!
Exercises
1.4.10. Write down the elementary 4 × 4 permutation matrix (a) P1 that permutes the second
and fourth rows, and (b) P2 that permutes the ﬁrst and fourth rows. (c) Do P1 and P2
commute? (d) Explain what the matrix products P1 P2 and P2 P1 do to a 4 × 4 matrix.
1.4.11. Write down the permutation matrix P such that
(a) P
⎛
⎜
⎝
u
v
w
⎞
⎟
⎠=
⎛
⎜
⎝
v
w
u
⎞
⎟
⎠,
(b) P
⎛
⎜
⎜
⎜
⎝
a
b
c
d
⎞
⎟
⎟
⎟
⎠=
⎛
⎜
⎜
⎜
⎝
d
c
a
b
⎞
⎟
⎟
⎟
⎠,
(c) P
⎛
⎜
⎜
⎜
⎝
a
b
c
d
⎞
⎟
⎟
⎟
⎠=
⎛
⎜
⎜
⎜
⎝
b
a
d
c
⎞
⎟
⎟
⎟
⎠,
(d) P
⎛
⎜
⎜
⎜
⎜
⎜
⎝
x1
x2
x3
x4
x5
⎞
⎟
⎟
⎟
⎟
⎟
⎠
=
⎛
⎜
⎜
⎜
⎜
⎜
⎝
x4
x1
x3
x2
x5
⎞
⎟
⎟
⎟
⎟
⎟
⎠
.
1.4.12. Construct a multiplication table that shows all possible products of the 3 × 3
permutation matrices (1.30). List all pairs that commute.

1.4 Pivoting and Permutations
27
1.4.13. Write down all 4 × 4 permutation matrices that (a) ﬁx the third row of a 4 × 4 matrix
A; (b) take the third row to the fourth row; (c) interchange the second and third rows.
1.4.14. True or false: (a) Every elementary permutation matrix satisﬁes P 2 = I . (b) Every
permutation matrix satisﬁes P 2 = I . (c) A matrix that satisﬁes P 2 = I is necessarily a
permutation matrix.
1.4.15.(a) Let P and Q be n × n permutation matrices and v ∈Rn a vector. Under what
conditions does the equation P v = Qv imply that P = Q? (b) Answer the same question
when P A = QA, where A is an n × k matrix.
1.4.16. Let P be the 3 × 3 permutation matrix such that the product P A permutes the ﬁrst
and third rows of the 3 × 3 matrix A. (a) Write down P. (b) True or false: The product
AP is obtained by permuting the ﬁrst and third columns of A.
(c) Does the same conclusion hold for every permutation matrix: is the eﬀect of P A on the
rows of a square matrix A the same as the eﬀect of AP on the columns of A?
♥1.4.17. A common notation for a permutation π of the integers {1, . . . , m} is as a 2 × m
matrix

1
2
3
. . .
m
π(1)
π(2)
π(3)
. . .
π(m)
	
, indicating that π takes i to π(i). (a) Show
that such a permutation corresponds to the permutation matrix with 1’s in positions
(π(j), j) for j = 1, . . . , m.
(b) Write down the permutation matrices corresponding to
the following permutations: (i)

1
2
3
2
1
3
	
, (ii)

1
2
3
4
4
2
3
1
	
, (iii)

1
2
3
4
1
4
2
3
	
,
(iv)

1
2
3
4
5
5
4
3
2
1
	
. Which are elementary matrices? (c) Write down, using the
preceding notation, the permutations corresponding to the following permutation matrices:
(i)
⎛
⎜
⎝
0
0
1
1
0
0
0
1
0
⎞
⎟
⎠,
(ii)
⎛
⎜
⎜
⎜
⎝
0
0
1
0
0
0
0
1
1
0
0
0
0
1
0
0
⎞
⎟
⎟
⎟
⎠, (iii)
⎛
⎜
⎜
⎜
⎝
0
1
0
0
0
0
1
0
0
0
0
1
1
0
0
0
⎞
⎟
⎟
⎟
⎠, (iv)
⎛
⎜
⎜
⎜
⎜
⎜
⎝
0
0
0
1
0
1
0
0
0
0
0
0
1
0
0
0
0
0
0
1
0
1
0
0
0
⎞
⎟
⎟
⎟
⎟
⎟
⎠
.
♦1.4.18. Justify the statement that there are n! diﬀerent n × n permutation matrices.
1.4.19. Consider the following combination of elementary row operations of type #1: (i) Add
row i to row j. (ii) Subtract row j from row i. (iii) Add row i to row j again. Prove that
the net eﬀect is to interchange −1 times row i with row j. Thus, we can almost produce
an elementary row operation of type #2 by a combination of elementary row operations
of type #1. Lest you be tempted to try, Exercise 1.9.16 proves that one cannot produce a
bona ﬁde row interchange by a combination of elementary row operations of type #1.
1.4.20. What is the eﬀect of permuting the columns of its coeﬃcient matrix on a linear system?
The Permuted LU Factorization
As we now know, every nonsingular matrix A can be reduced to upper triangular form
by elementary row operations of types #1 and #2. The row interchanges merely reorder
the equations. If one performs all of the required row interchanges in advance, then the
elimination algorithm can proceed without requiring any further pivoting. Thus, the matrix
obtained by permuting the rows of A in the prescribed manner is regular. In other words,
if A is a nonsingular matrix, then there is a permutation matrix P such that the product
P A is regular, and hence admits an LU factorization. As a result, we deduce the general
permuted LU factorization
P A = L U,
(1.33)

28
1 Linear Algebraic Systems
where P is a permutation matrix, L is lower unitriangular, and U is upper triangular with
the pivots on the diagonal. For instance, in the preceding example, we permuted the ﬁrst
and second rows, and hence equation (1.33) has the explicit form
⎛
⎝
0
1
0
1
0
0
0
0
1
⎞
⎠
⎛
⎝
0
2
1
2
6
1
1
1
4
⎞
⎠=
⎛
⎝
1
0
0
0
1
0
1
2
−1
1
⎞
⎠
⎛
⎝
2
6
1
0
2
1
0
0
9
2
⎞
⎠.
We have now established the following generalization of Theorem 1.3.
Theorem 1.11. Let A be an n × n matrix. Then the following conditions are equivalent:
(i) A is nonsingular.
(ii) A has n nonzero pivots.
(iii) A admits a permuted LU factorization: P A = LU.
A practical method to construct a permuted LU factorization of a given matrix A would
proceed as follows. First set up P = L = I as n × n identity matrices. The matrix P
will keep track of the permutations performed during the Gaussian Elimination process,
while the entries of L below the diagonal are gradually replaced by the negatives of the
multiples used in the corresponding row operations of type #1. Each time two rows of A are
interchanged, the same two rows of P will be interchanged. Moreover, any pair of entries
that both lie below the diagonal in these same two rows of L must also be interchanged,
while entries lying on and above its diagonal need to stay in their place. At a successful
conclusion to the procedure, A will have been converted into the upper triangular matrix
U, while L and P will assume their ﬁnal form. Here is an illustrative example.
Example 1.12.
Our goal is to produce a permuted LU factorization of the matrix
A =
⎛
⎜
⎝
1
2
−1
0
2
4
−2
−1
−3
−5
6
1
−1
2
8
−2
⎞
⎟
⎠.
To begin the procedure, we apply row operations of type #1 to eliminate the entries below
the ﬁrst pivot. The updated matrices† are
A =
⎛
⎜
⎝
1
2
−1
0
0
0
0
−1
0
1
3
1
0
4
7
−2
⎞
⎟
⎠,
L =
⎛
⎜
⎝
1
0
0
0
2
1
0
0
−3
0
1
0
−1
0
0
1
⎞
⎟
⎠,
P =
⎛
⎜
⎝
1
0
0
0
0
1
0
0
0
0
1
0
0
0
0
1
⎞
⎟
⎠,
where L keeps track of the row operations, and we initialize P to be the identity matrix.
The (2, 2) entry of the new A is zero, and so we interchange its second and third rows,
leading to
A =
⎛
⎜
⎝
1
2
−1
0
0
1
3
1
0
0
0
−1
0
4
7
−2
⎞
⎟
⎠,
L =
⎛
⎜
⎝
1
0
0
0
−3
1
0
0
2
0
1
0
−1
0
0
1
⎞
⎟
⎠,
P =
⎛
⎜
⎝
1
0
0
0
0
0
1
0
0
1
0
0
0
0
0
1
⎞
⎟
⎠.
†
Here, we are adopting computer programming conventions, where updates of a matrix are all
given the same name.

1.4 Pivoting and Permutations
29
We interchanged the same two rows of P, while in L we only interchanged the already
computed entries in its second and third rows that lie in its ﬁrst column below the diagonal.
We then eliminate the nonzero entry lying below the (2, 2) pivot, leading to
A =
⎛
⎜
⎝
1
2
−1
0
0
1
3
1
0
0
0
−1
0
0
−5
−6
⎞
⎟
⎠,
L =
⎛
⎜
⎝
1
0
0
0
−3
1
0
0
2
0
1
0
−1
4
0
1
⎞
⎟
⎠,
P =
⎛
⎜
⎝
1
0
0
0
0
0
1
0
0
1
0
0
0
0
0
1
⎞
⎟
⎠.
A ﬁnal row interchange places the matrix in upper triangular form:
U = A =
⎛
⎜
⎝
1
2
−1
0
0
1
3
1
0
0
−5
−6
0
0
0
−1
⎞
⎟
⎠,
L =
⎛
⎜
⎝
1
0
0
0
−3
1
0
0
−1
4
1
0
2
0
0
1
⎞
⎟
⎠,
P =
⎛
⎜
⎝
1
0
0
0
0
0
1
0
0
0
0
1
0
1
0
0
⎞
⎟
⎠.
Again, we performed the same row interchange on P, while interchanging only the third
and fourth row entries of L that lie below the diagonal. You can verify that
P A =
⎛
⎜
⎝
1
2
−1
0
−3
−5
6
1
−1
2
8
−2
2
4
−2
−1
⎞
⎟
⎠=
⎛
⎜
⎝
1
0
0
0
−3
1
0
0
−1
4
1
0
2
0
0
1
⎞
⎟
⎠
⎛
⎜
⎝
1
2
−1
0
0
1
3
1
0
0
−5
−6
0
0
0
−1
⎞
⎟
⎠= LU,
(1.34)
as promised. Thus, by rearranging the equations in the order ﬁrst, third, fourth, second,
as prescribed by P, we obtain an equivalent linear system whose coeﬃcient matrix P A is
regular, in accordance with Theorem 1.11.
Once the permuted LU factorization is established, the solution to the original system
A x = b is obtained by applying the same Forward and Back Substitution algorithm
presented above. Explicitly, we ﬁrst multiply the system A x = b by the permutation
matrix, leading to
P A x = P b = b,
(1.35)
whose right-hand side b has been obtained by permuting the entries of b in the same
fashion as the rows of A. We then solve the two triangular systems
L c = b
and
U x = c
(1.36)
by, respectively, Forward and Back Substitution, as before.
Example 1.12 (continued).
Suppose we wish to solve the linear system
⎛
⎜
⎝
1
2
−1
0
2
4
−2
−1
−3
−5
6
1
−1
2
8
−2
⎞
⎟
⎠
⎛
⎜
⎝
x
y
z
w
⎞
⎟
⎠=
⎛
⎜
⎝
1
−1
3
0
⎞
⎟
⎠.
In view of the P A = LU factorization established in (1.34), we need only solve the two
auxiliary lower and upper triangular systems (1.36). The lower triangular system is
⎛
⎜
⎝
1
0
0
0
−3
1
0
0
−1
4
1
0
2
0
0
1
⎞
⎟
⎠
⎛
⎜
⎝
a
b
c
d
⎞
⎟
⎠=
⎛
⎜
⎝
1
3
0
−1
⎞
⎟
⎠;

30
1 Linear Algebraic Systems
whose right-hand side was obtained by applying the permutation matrix P to the right-
hand side of the original system. Its solution, namely a = 1, b = 6, c = −23, d = −3, is
obtained through Forward Substitution. The resulting upper triangular system is
⎛
⎜
⎝
1
2
−1
0
0
1
3
1
0
0
−5
−6
0
0
0
−1
⎞
⎟
⎠
⎛
⎜
⎝
x
y
z
w
⎞
⎟
⎠=
⎛
⎜
⎝
1
6
−23
−3
⎞
⎟
⎠.
Its solution, w = 3, z = 1, y = 0, x = 2, which is also the solution to the original system,
is easily obtained by Back Substitution.
Exercises
1.4.21. For each of the listed matrices A and vectors b, ﬁnd a permuted LU factorization of
the matrix, and use your factorization to solve the system A x = b.
(a)

0
1
2
−1
	
,

3
2
	
,
(b)
⎛
⎜
⎝
0
0
−4
1
2
3
0
1
7
⎞
⎟
⎠,
⎛
⎜
⎝
1
2
−1
⎞
⎟
⎠, (c)
⎛
⎜
⎝
0
1
−3
0
2
3
1
0
2
⎞
⎟
⎠,
⎛
⎜
⎝
1
2
−1
⎞
⎟
⎠, (d)
⎛
⎜
⎜
⎜
⎝
1
2
−1
0
3
6
2
−1
1
1
−7
2
1
−1
2
1
⎞
⎟
⎟
⎟
⎠,
⎛
⎜
⎜
⎜
⎝
1
0
0
3
⎞
⎟
⎟
⎟
⎠,
(e)
⎛
⎜
⎜
⎜
⎝
0
1
0
0
2
3
1
0
1
4
−1
2
7
−1
2
3
⎞
⎟
⎟
⎟
⎠,
⎛
⎜
⎜
⎜
⎝
−1
−4
0
5
⎞
⎟
⎟
⎟
⎠, (f )
⎛
⎜
⎜
⎜
⎜
⎜
⎝
0
0
2
3
4
0
1
−7
2
3
1
4
1
1
1
0
0
1
0
2
0
0
1
7
3
⎞
⎟
⎟
⎟
⎟
⎟
⎠
,
⎛
⎜
⎜
⎜
⎜
⎜
⎝
−3
−2
0
0
−7
⎞
⎟
⎟
⎟
⎟
⎟
⎠
.
1.4.22. For each of the following linear systems ﬁnd a permuted LU factorization of the
coeﬃcient matrix and then use it to solve the system by Forward and Back Substitution.
(a)
4x1 −4x2 + 2x3 = 1,
−3x1 + 3x2 + x3 = 3,
−3x1 + x2 −2x3 = −5.
(b)
y −z + w = 0,
y + z = 1,
x −y + z −3w = 2,
x + 2y −z + w = 4.
(c)
x −y + 2z + w = 0,
−x + y −3z = 1,
x −y + 4z −3w = 2,
x + 2y −z + w = 4.
♦1.4.23.(a) Explain why
⎛
⎜
⎝
0
1
0
1
0
0
0
0
1
⎞
⎟
⎠
⎛
⎜
⎝
0
1
3
2
−1
1
2
−2
0
⎞
⎟
⎠=
⎛
⎜
⎝
1
0
0
0
1
0
1
−1
1
⎞
⎟
⎠
⎛
⎜
⎝
2
−1
1
0
1
3
0
0
2
⎞
⎟
⎠,
⎛
⎜
⎝
0
0
1
0
1
0
1
0
0
⎞
⎟
⎠
⎛
⎜
⎝
0
1
3
2
−1
1
2
−2
0
⎞
⎟
⎠=
⎛
⎜
⎝
1
0
0
1
1
0
0
1
1
⎞
⎟
⎠
⎛
⎜
⎝
2
−2
0
0
1
1
0
0
2
⎞
⎟
⎠,
⎛
⎜
⎝
0
0
1
1
0
0
0
1
0
⎞
⎟
⎠
⎛
⎜
⎝
0
1
3
2
−1
1
2
−2
0
⎞
⎟
⎠=
⎛
⎜
⎝
1
0
0
0
1
0
1
1
1
⎞
⎟
⎠
⎛
⎜
⎝
2
−2
0
0
1
3
0
0
−2
⎞
⎟
⎠,
are all legitimate permuted LU factorizations of the same matrix. List the elementary row
operations that are being used in each case.
(b) Use each of the factorizations to solve the linear system
⎛
⎜
⎝
0
1
3
2
−1
1
2
−2
0
⎞
⎟
⎠
⎛
⎜
⎝
x
y
z
⎞
⎟
⎠=
⎛
⎜
⎝
−5
−1
0
⎞
⎟
⎠.
Do you always obtain the same result? Explain why or why not.
1.4.24.(a) Find three diﬀerent permuted LU factorizations of the matrix A =
⎛
⎜
⎝
0
1
2
1
0
−1
1
1
3
⎞
⎟
⎠.
(b) How many diﬀerent permuted LU factorizations does A have?

1.5 Matrix Inverses
31
1.4.25. What is the maximal number of permuted LU factorizations a regular 3 × 3 matrix can
have? Give an example of such a matrix.
1.4.26. True or false: The pivots of a nonsingular matrix are uniquely deﬁned.
♠1.4.27.(a) Write a pseudocode program implementing the algorithm for ﬁnding the permuted
LU factorization of a matrix.
(b) Program your algorithm and test it on the examples in
Exercise 1.4.21.
1.5 Matrix Inverses
The inverse of a matrix is analogous to the reciprocal a−1 = 1/a of a nonzero scalar
a ̸= 0. We already encountered the inverses of matrices corresponding to elementary row
operations. In this section, we will study inverses of general square matrices. We begin
with the formal deﬁnition.
Deﬁnition 1.13. Let A be a square matrix of size n× n. An n× n matrix X is called the
inverse of A if it satisﬁes
XA = I = AX,
(1.37)
where I = I n is the n× n identity matrix. The inverse of A is commonly denoted by A−1.
Remark. Noncommutativity of matrix multiplication requires that we impose both con-
ditions in (1.37) in order to properly deﬁne an inverse to the matrix A. The ﬁrst condition,
X A = I , says that X is a left inverse, while the second, A X = I , requires that X also
be a right inverse. Rectangular matrices might have either a left inverse or a right inverse,
but, as we shall see, only square matrices have both, and so only square matrices can have
full-ﬂedged inverses. However, not every square matrix has an inverse. Indeed, not every
scalar has an inverse: 0−1 = 1/0 is not deﬁned, since the equation 0x = 1 has no solution.
Example 1.14.
Since
⎛
⎝
1
2
−1
−3
1
2
−2
2
1
⎞
⎠
⎛
⎝
3
4
−5
1
1
−1
4
6
−7
⎞
⎠=
⎛
⎝
1
0
0
0
1
0
0
0
1
⎞
⎠=
⎛
⎝
3
4
−5
1
1
−1
4
6
−7
⎞
⎠
⎛
⎝
1
2
−1
−3
1
2
−2
2
1
⎞
⎠,
we conclude that when A =
⎛
⎝
1
2
−1
−3
1
2
−2
2
1
⎞
⎠, then A−1 =
⎛
⎝
3
4
−5
1
1
−1
4
6
−7
⎞
⎠. Observe that
there is no obvious way to anticipate the entries of A−1 from the entries of A.
Example 1.15.
Let us compute the inverse X =

x
y
z
w

, when it exists, of a general
2 × 2 matrix A =

a
b
c
d

. The right inverse condition
A X =

a x + b z
a y + b w
c x + d z
c y + d w

=

1
0
0
1

= I
holds if and only if x, y, z, w satisfy the linear system
a x + b z = 1,
c x + d z = 0,
a y + b w = 0,
c y + d w = 1.

32
1 Linear Algebraic Systems
Solving by Gaussian Elimination (or directly), we ﬁnd
x =
d
ad −bc ,
y = −
b
ad −bc ,
z = −
c
ad −bc ,
w =
a
ad −bc ,
provided the common denominator ad −bc ̸= 0 does not vanish. Therefore, the matrix
X =
1
ad −bc

d
−b
−c
a

forms a right inverse to A. However, a short computation shows that it also deﬁnes a left
inverse:
X A =

x a + y c
x b + y d
z a + w c
z b + w d

=

1
0
0
1

= I ,
and hence X = A−1 is the inverse of A.
The denominator appearing in the preceding formulas has a special name; it is called
the determinant of the 2 × 2 matrix A, and denoted by
det

a
b
c
d

= ad −bc.
(1.38)
Thus, the determinant of a 2 × 2 matrix is the product of the diagonal entries minus
the product of the oﬀ-diagonal entries. (Determinants of larger square matrices will be
discussed in Section 1.9.) Thus, the 2 × 2 matrix A is invertible, with
A−1 =
1
ad −bc

d
−b
−c
a

,
(1.39)
if and only if det A ̸= 0. For example, if A =

1
3
−2
−4

, then det A = 2 ̸= 0. We
conclude that A has an inverse, which, by (1.39), is A−1 = 1
2

−4
−3
2
1

=

−2
−3
2
1
1
2

.
Example 1.16.
We already learned how to ﬁnd the inverse of an elementary matrix of
type #1: we just negate the one nonzero oﬀ-diagonal entry. For example, if
E =
⎛
⎝
1
0
0
0
1
0
2
0
1
⎞
⎠,
then
E−1 =
⎛
⎝
1
0
0
0
1
0
−2
0
1
⎞
⎠.
This is because the inverse of the elementary row operation that adds twice the ﬁrst row
to the third row is the operation of subtracting twice the ﬁrst row from the third row.
Example 1.17.
Let P =
⎛
⎝
0
1
0
1
0
0
0
0
1
⎞
⎠denote the elementary matrix that has the eﬀect
of interchanging rows 1 and 2 of a 3–rowed matrix. Then P 2 = I , since performing the
interchange twice returns us to where we began. This implies that P −1 = P is its own
inverse. Indeed, the same result holds for all elementary permutation matrices that corre-
spond to row operations of type #2. However, it is not true for more general permutation
matrices.
The following fundamental result will be established later in this chapter.
Theorem 1.18. A square matrix has an inverse if and only if it is nonsingular.

1.5 Matrix Inverses
33
Consequently, an n × n matrix will have an inverse if and only if it can be reduced to
upper triangular form, with n nonzero pivots on the diagonal, by a combination of elemen-
tary row operations. Indeed, “invertible” is often used as a synonym for “nonsingular”. All
other matrices are singular and do not have an inverse as deﬁned above. Before attempting
to prove Theorem 1.18, we need ﬁrst to become familiar with some elementary properties
of matrix inverses.
Lemma 1.19. The inverse of a square matrix, if it exists, is unique.
Proof : Suppose both X and Y satisfy (1.37), so
XA = I = A X
and
Y A = I = A Y.
Then, by associativity,
X = X I = X(A Y ) = (XA)Y = I Y = Y.
Q.E.D.
Inverting a matrix twice brings us back to where we started.
Lemma 1.20. If A is an invertible matrix, then A−1 is also invertible and (A−1)−1 = A.
Proof : The matrix inverse equations A−1 A = I = A A−1 are suﬃcient to prove that A is
the inverse of A−1.
Q.E.D.
Lemma 1.21. If A and B are invertible matrices of the same size, then their product,
AB, is invertible, and
(AB)−1 = B−1A−1.
(1.40)
Note that the order of the factors is reversed under inversion.
Proof : Let X = B−1A−1. Then, by associativity,
X (AB) = B−1A−1AB = B−1 I B = B−1B = I ,
(AB)X = AB B−1A−1 = A I A−1 = AA−1 = I .
Thus X is both a left and a right inverse for the product matrix AB.
Q.E.D.
Example 1.22.
One
veriﬁes,
directly,
that
the
inverse
of
A
=

1
2
0
1

is
A−1 =

1
−2
0
1

, while the inverse of B =

0
1
−1
0

is B−1 =

0
−1
1
0

.
There-
fore, the inverse of their product C = AB =

1
2
0
1
 
0
1
−1
0

=

−2
1
−1
0

is given by
C−1 = B−1A−1 =

0
−1
1
0
 
1
−2
0
1

=

0
−1
1
−2

.
We can straightforwardly generalize the preceding result. The inverse of a k-fold product
of invertible matrices is the product of their inverses, in the reverse order:
(A1A2 · · · Ak−1Ak)−1 = A−1
k A−1
k−1 · · · A−1
2 A−1
1 .
(1.41)
Warning. In general, (A + B)−1 ̸= A−1 + B−1. Indeed, this equation is not even true for
scalars (1 × 1 matrices)!

34
1 Linear Algebraic Systems
Exercises
1.5.1. Verify by direct multiplication that the following matrices are inverses, i.e., both
conditions in (1.37) hold: (a) A =

2
3
−1
−1
	
, A−1 =

−1
−3
1
2
	
; (b) A =
⎛
⎜
⎝
2
1
1
3
2
1
2
1
2
⎞
⎟
⎠,
A−1 =
⎛
⎜
⎝
3
−1
−1
−4
2
1
−1
0
1
⎞
⎟
⎠; (c) A =
⎛
⎜
⎝
−1
3
2
2
2
−1
−2
1
3
⎞
⎟
⎠, A−1 =
⎛
⎜
⎜
⎝
−1
1
1
4
7
−1
7
−3
7
−6
7
5
7
8
7
⎞
⎟
⎟
⎠.
1.5.2. Let A =
⎛
⎜
⎝
1
2
0
0
1
3
1
−1
−8
⎞
⎟
⎠. Find the right inverse of A by setting up and solving the linear
system AX = I . Verify that the resulting matrix X is also a left inverse.
1.5.3. Write down the inverse of each of the following elementary matrices: (a)

0
1
1
0
	
,
(b)

1
0
5
1
	
, (c)

1
−2
0
1
	
, (d)
⎛
⎜
⎝
1
0
0
0
1
−3
0
0
1
⎞
⎟
⎠, (e)
⎛
⎜
⎜
⎜
⎝
1
0
0
0
0
1
0
0
0
6
1
0
0
0
0
1
⎞
⎟
⎟
⎟
⎠, (f )
⎛
⎜
⎜
⎜
⎝
0
0
0
1
0
1
0
0
0
0
1
0
1
0
0
0
⎞
⎟
⎟
⎟
⎠.
1.5.4. Show that the inverse of L =
⎛
⎜
⎝
1
0
0
a
1
0
b
0
1
⎞
⎟
⎠is L−1 =
⎛
⎜
⎝
1
0
0
−a
1
0
−b
0
1
⎞
⎟
⎠. However, the inverse
of M =
⎛
⎜
⎝
1
0
0
a
1
0
b
c
1
⎞
⎟
⎠is not
⎛
⎜
⎝
1
0
0
−a
1
0
−b
−c
1
⎞
⎟
⎠. What is M−1?
1.5.5. Explain why a matrix with a row of all zeros does not have an inverse.
1.5.6.(a) Write down the inverse of the matrices A =

1
1
2
1
	
and B =

1
−1
1
2
	
. (b) Write
down the product matrix C = AB and its inverse C−1 using the inverse product formula.
1.5.7.(a) Find the inverse of the rotation matrix Rθ =

cos θ
−sin θ
sin θ
cos θ
	
, where θ ∈R.
(b) Use your result to solve the system x = a cos θ −b sin θ, y = a sin θ + b cos θ, for a and b
in terms of x and y.
(c) Prove that, for all a ∈R and 0 < θ < π, the matrix Rθ −a I has
an inverse.
1.5.8.(a) Write down the inverses of each of the 3 × 3 permutation matrices (1.30). (b) Which
ones are their own inverses, P −1 = P? (c) Can you ﬁnd a non-elementary permutation
matrix P that is its own inverse: P −1 = P?
1.5.9. Find the inverse of the following permutation matrices:
(a)
⎛
⎜
⎜
⎜
⎝
0
0
0
1
0
0
1
0
0
1
0
0
1
0
0
0
⎞
⎟
⎟
⎟
⎠, (b)
⎛
⎜
⎜
⎜
⎝
0
1
0
0
0
0
1
0
0
0
0
1
1
0
0
0
⎞
⎟
⎟
⎟
⎠, (c)
⎛
⎜
⎜
⎜
⎝
1
0
0
0
0
0
0
1
0
1
0
0
0
0
1
0
⎞
⎟
⎟
⎟
⎠, (d)
⎛
⎜
⎜
⎜
⎜
⎜
⎝
1
0
0
0
0
0
0
1
0
0
0
0
0
0
1
0
1
0
0
0
0
0
0
1
0
⎞
⎟
⎟
⎟
⎟
⎟
⎠
.
1.5.10. Explain how to write down the inverse permutation using the notation of Exercise
1.4.17. Apply your method to the examples in Exercise 1.5.9, and check the result by
verifying that it produces the inverse permutation matrix.
1.5.11. Find all real 2 × 2 matrices that are their own inverses: A−1 = A.

1.5 Matrix Inverses
35
1.5.12. Show that if a square matrix A satisﬁes A2 −3A + I = O, then A−1 = 3 I −A.
1.5.13. Prove that if c ̸= 0 is any nonzero scalar and A is an invertible matrix, then the scalar
product matrix cA is invertible, and (cA)−1 = 1
c A−1.
1.5.14. Show that A =
⎛
⎜
⎜
⎜
⎜
⎜
⎝
0
a
0
0
0
b
0
c
0
0
0
d
0
e
0
0
0
f
0
g
0
0
0
h
0
⎞
⎟
⎟
⎟
⎟
⎟
⎠
is not invertible for any value of the entries.
1.5.15. Show that if A is a nonsingular matrix, so is everyy power An.
1.5.16. Prove that a diagonal matrix D = diag (d1, . . . , dn) is invertible if and only if all its
diagonal entries are nonzero, in which case D−1 = diag (1/d1, . . . , 1/dn).
1.5.17. Prove that if U is a nonsingular upper triangular matrix, then the diagonal entries of
U−1 are the reciprocals of the diagonal entries of U.
♦1.5.18.(a) Let U be a m × n matrix and V an n × m matrix, such that the m × m matrix
I m + U V is invertible. Prove that I n + V U is also invertible, and is given by
( I n + V U)−1 = I n −V ( I m + U V )−1 U.
(b) The Sherman–Morrison–Woodbury formula generalizes this identity to
(A + V B U)−1 = A−1 −A−1V (B−1 + UA−1V )−1 UA−1.
(1.42)
Explain what assumptions must be made on the matrices A, B, U, V for (1.42) to be valid.
♦1.5.19. Two matrices A and B are said to be similar, written A ∼B, if there exists an
invertible matrix S such that B = S−1AS. Prove: (a) A ∼A. (b) If A ∼B, then B ∼A.
(c) If A ∼B and B ∼C, then A ∼C.
♥1.5.20.(a) A block matrix D =

A
O
O
B
	
is called block diagonal if A and B are square
matrices, not necessarily of the same size, while the O’s are zero matrices of the
appropriate sizes. Prove that D has an inverse if and only if both A and B do, and
D−1 =

A−1
O
O
B−1
	
. (b) Find the inverse of
⎛
⎜
⎝
1
2
0
2
1
0
0
0
3
⎞
⎟
⎠and
⎛
⎜
⎜
⎜
⎝
1
−1
0
0
2
−1
0
0
0
0
1
3
0
0
2
5
⎞
⎟
⎟
⎟
⎠by
using this method.
1.5.21.(a) Show that B =

1
1
0
−1
−1
1
	
is a left inverse of A =
⎛
⎜
⎝
1
−1
0
1
1
1
⎞
⎟
⎠.
(b) Show that
A does not have a right inverse.
(c) Can you ﬁnd any other left inverses of A?
1.5.22. Prove that the rectangular matrix A =

1
2
−1
1
2
0
	
has a right inverse, but no left
inverse.
1.5.23.(a) Are there any nonzero real scalars that satisfy (a + b)−1 = a−1 + b−1?
(b) Are there any nonsingular real 2 × 2 matrices that satisfy (A + B)−1 = A−1 + B−1?
Gauss–Jordan Elimination
The principal algorithm used to compute the inverse of a nonsingular matrix is known as
Gauss–Jordan Elimination, in honor of Gauss and Wilhelm Jordan, a nineteenth-century
German engineer. A key fact is that, given that A is square, we need to solve only the
right inverse equation
AX = I
(1.43)

36
1 Linear Algebraic Systems
in order to compute X = A−1. The left inverse equation in (1.37), namely XA = I ,
will then follow as an automatic consequence. In other words, for square matrices, a right
inverse is automatically a left inverse, and conversely! A proof will appear below.
The reader may well ask, then, why use both left and right inverse conditions in the
original deﬁnition?
There are several good reasons.
First of all, a non-square matrix
may satisfy one of the two conditions — having either a left inverse or a right inverse
— but can never satisfy both. Moreover, even when we restrict our attention to square
matrices, starting with only one of the conditions makes the logical development of the
subject considerably more diﬃcult, and not really worth the extra eﬀort. Once we have
established the basic properties of the inverse of a square matrix, we can then safely discard
the superﬂuous left inverse condition. Finally, when we generalize the notion of an inverse
to linear operators in Chapter 7, then, in contrast to the case of square matrices, we cannot
dispense with either of the conditions.
Let us write out the individual columns of the right inverse equation (1.43). The jth
column of the n × n identity matrix I is the vector ej that has a 1 in the jth slot and 0’s
elsewhere, so
e1 =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎝
1
0
0
...
0
0
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎠
,
e2 =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎝
0
1
0
...
0
0
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎠
,
. . .
en =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎝
0
0
0
...
0
1
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎠
.
(1.44)
According to (1.11), the jth column of the matrix product AX is equal to A xj, where
xj denotes the jth column of the inverse matrix X. Therefore, the single matrix equation
(1.43) is equivalent to n linear systems
A x1 = e1,
A x2 = e2,
. . .
A xn = en,
(1.45)
all having the same coeﬃcient matrix.
As such, to solve them we should form the n
augmented matrices M1 =

A | e1

, . . . , Mn =

A | en

, and then apply our Gaussian
Elimination algorithm to each. But this would be a waste of eﬀort. Since the coeﬃcient
matrix is the same, we will end up performing identical row operations on each augmented
matrix. Clearly, it will be more eﬃcient to combine them into one large augmented matrix
M =

A | e1 . . . en

=

A | I

, of size n × (2n), in which the right-hand sides e1, . . . , en
of our systems are placed into n diﬀerent columns, which we then recognize as reassembling
the columns of an n×n identity matrix. We may then simultaneously apply our elementary
row operations to reduce, if possible, the large augmented matrix so that its ﬁrst n columns
are in upper triangular form.
Example 1.23.
For example, to ﬁnd the inverse of the matrix A =
⎛
⎝
0
2
1
2
6
1
1
1
4
⎞
⎠, we
form the large augmented matrix
⎛
⎝
0
2
1
2
6
1
1
1
4

1
0
0
0
1
0
0
0
1
⎞
⎠.
Applying the same sequence of elementary row operations as in Section 1.4, we ﬁrst inter-
change the rows
⎛
⎝
2
6
1
0
2
1
1
1
4

0
1
0
1
0
0
0
0
1
⎞
⎠,

1.5 Matrix Inverses
37
and then eliminate the nonzero entries below the ﬁrst pivot,
⎛
⎝
2
6
1
0
2
1
0
−2
7
2

0
1
0
1
0
0
0
−1
2
1
⎞
⎠.
Next we eliminate the entry below the second pivot:
⎛
⎝
2
6
1
0
2
1
0
0
9
2

0
1
0
1
0
0
1
−1
2
1
⎞
⎠.
At this stage, we have reduced our augmented matrix to the form

U | C

, where U is
upper triangular. This is equivalent to reducing the original n linear systems A xi = ei to
n upper triangular systems U xi = ci. We can therefore perform n back substitutions to
produce the solutions xi, which would form the individual columns of the inverse matrix
X = (x1 . . . xn). In the more common version of the Gauss–Jordan scheme, one instead
continues to employ elementary row operations to fully reduce the augmented matrix. The
goal is to produce an augmented matrix

I | X

in which the left-hand n × n matrix has
become the identity, while the right-hand matrix is the desired solution X = A−1. Indeed,

I | X

represents the n trivial linear systems I x = xi whose solutions x = xi are the
columns of the inverse matrix X.
Now, the identity matrix has 0’s below the diagonal, just like U. It also has 1’s along
the diagonal, whereas U has the pivots (which are all nonzero) along the diagonal. Thus,
the next phase in the reduction process is to make all the diagonal entries of U equal to 1.
To proceed, we need to introduce the last, and least, of our linear systems operations.
Linear System Operation #3:
Multiply an equation by a nonzero constant.
This operation clearly does not aﬀect the solution, and so yields an equivalent linear system.
The corresponding elementary row operation is:
Elementary Row Operation #3:
Multiply a row of the matrix by a nonzero scalar.
Dividing the rows of the upper triangular augmented matrix

U | C

by the diagonal
pivots of U will produce a matrix of the form

V | B

, where V is upper unitriangular,
meaning it has all 1’s along the diagonal. In our particular example, the result of these
three elementary row operations of type #3 is
⎛
⎜
⎝
1
3
1
2
0
1
1
2
0
0
1

0
1
2
0
1
2
0
0
2
9
−1
9
2
9
⎞
⎟
⎠,
where we multiplied the ﬁrst and second rows by 1
2 and the third row by 2
9.
We are now over halfway towards our goal.
We need only make the entries above
the diagonal of the left-hand matrix equal to zero. This can be done by elementary row
operations of type #1, but now we work backwards. First, we eliminate the nonzero entries
in the third column lying above the (3, 3) entry by subtracting one half the third row from
the second and also from the ﬁrst:
⎛
⎜
⎝
1
3
0
0
1
0
0
0
1

−1
9
5
9
−1
9
7
18
1
18
−1
9
2
9
−1
9
2
9
⎞
⎟
⎠.

38
1 Linear Algebraic Systems
Finally, we subtract 3 times the second row from the ﬁrst to eliminate the remaining
nonzero oﬀ-diagonal entry, thereby completing the Gauss–Jordan procedure:
⎛
⎜
⎝
1
0
0
0
1
0
0
0
1

−23
18
7
18
2
9
7
18
1
18
−1
9
2
9
−1
9
2
9
⎞
⎟
⎠.
The left-hand matrix is the identity, and therefore the ﬁnal right-hand matrix is our desired
inverse:
A−1 =
⎛
⎜
⎝
−23
18
7
18
2
9
7
18
1
18
−1
9
2
9
−1
9
2
9
⎞
⎟
⎠.
(1.46)
The reader may wish to verify that the ﬁnal result does satisfy both inverse conditions
A A−1 = I = A−1A.
We are now able to complete the proofs of the basic results on inverse matrices. First,
we need to determine the elementary matrix corresponding to an elementary row operation
of type #3. Again, this is obtained by performing the row operation in question on the
identity matrix. Thus, the elementary matrix that multiplies row i by the nonzero scalar
c is the diagonal matrix having c in the ith diagonal position, and 1’s elsewhere along the
diagonal. The inverse elementary matrix is the diagonal matrix with 1/c in the ith diagonal
position and 1’s elsewhere on the main diagonal; it corresponds to the inverse operation
that divides row i by c. For example, the elementary matrix that multiplies the second
row of a 3-rowed matrix by 5 is E =
⎛
⎝
1
0
0
0
5
0
0
0
1
⎞
⎠; its inverse is E−1 =
⎛
⎝
1
0
0
0
1
5
0
0
0
1
⎞
⎠.
In summary:
Lemma 1.24.
Every elementary matrix is nonsingular, and its inverse is also an
elementary matrix of the same type.
The Gauss–Jordan method tells us how to reduce any nonsingular square matrix A to
the identity matrix by a sequence of elementary row operations. Let E1, E2, . . . , EN be
the corresponding elementary matrices. The elimination procedure that reduces A to I
amounts to multiplying A by a succession of elementary matrices:
EN EN−1 · · · E2 E1 A = I .
(1.47)
We claim that the product matrix
X = EN EN−1 · · · E2 E1
(1.48)
is the inverse of A. Indeed, formula (1.47) says that XA = I , and so X is a left inverse.
Furthermore, each elementary matrix has an inverse, and so by (1.41), X itself is invertible,
with
X−1 = E−1
1
E−1
2
· · · E−1
N−1 E−1
N .
(1.49)
Therefore, multiplying formula (1.47), namely X A = I , on the left by X−1 leads to A =
X−1. Lemma 1.20 implies X = A−1, as claimed, completing the proof of Theorem 1.18.
Finally, equating A = X−1 to the product (1.49), and invoking Lemma 1.24, we have
established the following result.

1.5 Matrix Inverses
39
Proposition 1.25. Every nonsingular matrix can be written as the product of elementary
matrices.
Example 1.26.
The 2 × 2 matrix A =

0
−1
1
3

is converted into the identity matrix
by ﬁrst interchanging its rows,

1
3
0
−1

, then scaling the second row by −1,

1
3
0
1

,
and, ﬁnally, subtracting 3 times the second row from the ﬁrst to obtain

1
0
0
1

= I . The
corresponding elementary matrices are
E1 =

0
1
1
0

,
E2 =

1
0
0
−1

,
E3 =

1
−3
0
1

.
Therefore, by (1.48),
A−1 = E3 E2 E1 =

1
−3
0
1
 
1
0
0
−1
 
0
1
1
0

=

3
1
−1
0

,
while
A = E−1
1
E−1
2
E−1
3
=

0
1
1
0
 
1
0
0
−1
 
1
3
0
1

=

0
−1
1
3

.
As an application, let us prove that the inverse of a nonsingular triangular matrix is
also triangular. Speciﬁcally:
Proposition 1.27. If L is a lower triangular matrix with all nonzero entries on the main
diagonal, then L is nonsingular and its inverse L−1 is also lower triangular. In particular,
if L is lower unitriangular, so is L−1. A similar result holds for upper triangular matrices.
Proof : It suﬃces to note that if L has all nonzero diagonal entries, one can reduce L to the
identity by elementary row operations of types #1 and #3, whose associated elementary
matrices are all lower triangular.
Lemma 1.2 implies that the product (1.48) is then
also lower triangular. If L is unitriangular, then all the pivots are equal to 1. Thus, no
elementary row operations of type #3 are required, and so L can be reduced to the identity
matrix by elementary row operations of type #1 alone. Therefore, its inverse is a product
of lower unitriangular matrices, and hence is itself lower unitriangular. A similar argument
applies in the upper triangular case.
Q.E.D.
Exercises
1.5.24.(a) Write down the elementary matrix that multiplies the third row of a 4 × 4 matrix
by 7.
(b) Write down its inverse.
1.5.25. Find the inverse of each of the following matrices, if possible, by applying the Gauss–
Jordan Method.
(a)

1
−2
3
−3
	
,
(b)

1
3
3
1
	
,
(c)
⎛
⎝
3
5
−4
5
4
5
3
5
⎞
⎠,
(d)
⎛
⎜
⎝
1
2
3
4
5
6
7
8
9
⎞
⎟
⎠,
(e)
⎛
⎜
⎝
1
0
−2
3
−1
0
−2
1
−3
⎞
⎟
⎠,
(f )
⎛
⎜
⎝
1
2
3
3
5
5
2
1
2
⎞
⎟
⎠,
(g)
⎛
⎜
⎝
2
1
2
4
2
3
0
−1
1
⎞
⎟
⎠,
(h)
⎛
⎜
⎜
⎜
⎝
2
1
0
1
0
0
1
3
1
0
0
−1
0
0
−2
−5
⎞
⎟
⎟
⎟
⎠, (i)
⎛
⎜
⎜
⎜
⎝
1
−2
1
1
2
−3
3
0
3
−7
2
4
0
2
1
1
⎞
⎟
⎟
⎟
⎠.

40
1 Linear Algebraic Systems
1.5.26. Write each of the matrices in Exercise 1.5.25 as a product of elementary matrices.
1.5.27. Express A =
⎛
⎜
⎜
⎜
⎝
√
3
2
−1
2
1
2
√
3
2
⎞
⎟
⎟
⎟
⎠as a product of elementary matrices.
1.5.28. Use the Gauss–Jordan Method to ﬁnd the inverse of the following complex matrices:
(a)

i
1
1
i
	
,
(b)

1
1 −i
1 + i
1
	
,
(c)
⎛
⎜
⎝
0
1
−i
i
0
−1
−1
i
1
⎞
⎟
⎠,
(d)
⎛
⎜
⎝
1
0
i
i
−1
1 + i
−3 i
1 −i
1 + i
⎞
⎟
⎠.
1.5.29. Can two nonsingular linear systems have the same solution and yet not be equivalent?
♥1.5.30.(a) Suppose A is obtained from A by applying an elementary row operation. Let
C = AB, where B is any matrix of the appropriate size. Explain why C = A B can be
obtained by applying the same elementary row operation to C.
(b) Illustrate by adding
−2 times the ﬁrst row to the third row of A =
⎛
⎜
⎝
1
2
−1
2
−3
2
0
1
−4
⎞
⎟
⎠and then multiplying the
result on the right by B =
⎛
⎜
⎝
1
−2
3
0
−1
1
⎞
⎟
⎠. Check that the resulting matrix is the same as ﬁrst
multiplying AB and then applying the same row operation to the product matrix.
Solving Linear Systems with the Inverse
The primary motivation for introducing the matrix inverse is that it provides a compact
formula for the solution to any linear system with an invertible coeﬃcient matrix.
Theorem 1.28. If the matrix A is nonsingular, then x = A−1 b is the unique solution to
the linear system A x = b.
Proof : We merely multiply the system by A−1, which yields x = A−1A x = A−1b. More-
over, A x = AA−1b = b, proving that x = A−1b is indeed the solution.
Q.E.D.
For example, let us return to the linear system (1.29). Since we computed the inverse
of its coeﬃcient matrix in (1.46), a “direct” way to solve the system is to multiply the
right-hand side by the inverse matrix:
⎛
⎝
x
y
z
⎞
⎠=
⎛
⎜
⎝
−23
18
7
18
2
9
7
18
1
18
−1
9
2
9
−1
9
2
9
⎞
⎟
⎠
⎛
⎜
⎝
2
7
3
⎞
⎟
⎠=
⎛
⎜
⎝
5
6
5
6
1
3
⎞
⎟
⎠,
reproducing our earlier solution.
However, while æsthetically appealing, the solution method based on the inverse matrix
is hopelessly ineﬃcient as compared to direct Gaussian Elimination, and, despite what you
may have been told, should not be used in practical computations. (A complete justiﬁcation
of this dictum will be provided in Section 1.7.) On the other hand, the inverse does play
a useful role in theoretical developments, as well as providing insight into the design of
practical algorithms. But the principal message of applied linear algebra is that LU de-
composition and Gaussian Elimination are fundamental; matrix inverses are to be avoided
in all but the most elementary computations.
Remark.
The reader may have learned a version of the Gauss–Jordan algorithm for
solving a single linear system that replaces the Back Substitution step by a complete

1.5 Matrix Inverses
41
reduction of the coeﬃcient matrix to the identity. In other words, to solve A x = b, we
start with the augmented matrix M =

A | b

and use all three types of elementary
row operations to produce (assuming nonsingularity) the fully reduced form

I | d

,
representing the trivially soluble, equivalent system x = d, which is the solution to the
original system. However, Back Substitution is more eﬃcient, and it remains the method
of choice in practical computations.
Exercises
1.5.31. Solve the following systems of linear equations by computing the inverses of their
coeﬃcient matrices.
(a) x + 2y = 1,
x −2y = −2.
(b) 3u −2v = 2,
u + 5v = 12.
(c)
x −y + 3z = 3,
x −2y + 3z = −2,
x −2y + z = 2.
(d)
y + 5z = 3,
x −y + 3z = −1,
−2x + 3y = 5.
(e)
x + 4y −z = 3,
2x + 7y −2z = 5,
−x −5y + 2z = −7.
(f )
x + y = 4,
2x + 3y −w = 11,
−y −z + w = −7,
z −w = 6.
(g)
x −2y + z + 2u = −2,
x −y + z −u = 3,
2x −y + z + u = 3,
−x + 3y −2z −u = 2.
1.5.32. For each of the nonsingular matrices in Exercise 1.5.25, use your computed inverse to
solve the associated linear system A x = b, where b is the column vector of the appropriate
size that has all 1’s as its entries.
The LDV Factorization
The second phase of the Gauss–Jordan process leads to a slightly more detailed version of
the LU factorization. Let D denote the diagonal matrix having the same diagonal entries
as U; in other words, D contains the pivots on its diagonal and zeros everywhere else. Let
V be the upper unitriangular matrix obtained from U by dividing each row by its pivot,
so that V has all 1’s on the diagonal. We already encountered V during the course of
the Gauss–Jordan procedure. It is easily seen that U = DV , which implies the following
result.
Theorem 1.29. A matrix A is regular if and only if it admits a factorization
A = LDV,
(1.50)
where L is a lower unitriangular matrix, D is a diagonal matrix with nonzero diagonal
entries, and V is an upper unitriangular matrix.
For the matrix appearing in Example 1.4, we have U = DV , where
U =
⎛
⎝
2
1
1
0
3
0
0
0
−1
⎞
⎠,
D =
⎛
⎝
2
0
0
0
3
0
0
0
−1
⎞
⎠,
V =
⎛
⎝
1
1
2
1
2
0
1
0
0
0
1
⎞
⎠.
This leads to the factorization
A =
⎛
⎝
2
1
1
4
5
2
2
−2
0
⎞
⎠=
⎛
⎝
1
0
0
2
1
0
1
−1
1
⎞
⎠
⎛
⎝
2
0
0
0
3
0
0
0
−1
⎞
⎠
⎛
⎝
1
1
2
1
2
0
1
0
0
0
1
⎞
⎠= LDV.

42
1 Linear Algebraic Systems
Proposition 1.30. If A = LU is regular, then the factors L and U are uniquely deter-
mined. The same holds for the A = LDV factorization.
Proof : Suppose LU = L U. Since the diagonal entries of all four matrices are non-zero,
Proposition 1.27 implies that they are invertible. Therefore,
L−1L = L−1L U U −1 = L−1L U U −1 = U U −1.
(1.51)
The left-hand side of the matrix equation (1.51) is the product of two lower unitriangular
matrices, and so, by Lemma 1.2, is itself lower unitriangular. The right-hand side is the
product of two upper triangular matrices, and hence is upper triangular. But the only way
a lower unitriangular matrix can equal an upper triangular matrix is if they both equal
the diagonal identity matrix. Therefore, L−1L = I = U U −1, and so L = L and U = U,
proving the ﬁrst result. The LDV version is an immediate consequence.
Q.E.D.
As you may have guessed, the more general cases requiring one or more row interchanges
lead to a permuted LDV factorization in the following form.
Theorem 1.31. A matrix A is nonsingular if and only if there is a permutation matrix P
such that
P A = LDV,
(1.52)
where L is a lower unitriangular matrix, D is a diagonal matrix with nonzero diagonal
entries, and V is a upper unitriangular matrix.
Uniqueness does not hold for the more general permuted factorizations (1.33), (1.52),
since there may be several permutation matrices that place a matrix in regular form; an
explicit example can be found in Exercise 1.4.23. Moreover, in contrast to regular Gaussian
Elimination, here the pivots, i.e., the diagonal entries of U, are no longer uniquely deﬁned,
but depend on the particular combination of row interchanges employed during the course
of the computation.
Exercises
1.5.33. Produce the LDV or a permuted LDV factorization of the following matrices:
(a)

1
2
−3
1
	
,
(b)

0
4
−7
2
	
,
(c)
⎛
⎜
⎝
2
1
2
2
4
−1
0
−2
1
⎞
⎟
⎠,
(d)
⎛
⎜
⎝
1
1
5
1
1
−2
2
−1
3
⎞
⎟
⎠,
(e)
⎛
⎜
⎝
2
−3
2
1
−1
1
1
−1
2
⎞
⎟
⎠,
(f )
⎛
⎜
⎜
⎜
⎝
1
−1
1
2
1
−4
1
5
1
2
−1
−1
3
1
1
6
⎞
⎟
⎟
⎟
⎠,
(g)
⎛
⎜
⎜
⎜
⎝
1
0
2
−3
2
−2
0
1
1
−2
−2
−1
0
1
1
2
⎞
⎟
⎟
⎟
⎠.
1.5.34. Using the LDV factorization for the matrices you found in parts (a–g) of Exercise
1.5.33, solve the corresponding linear systems A x = b, for the indicated vector b.
(a)

1
2
	
, (b)

−1
−2
	
, (c)
⎛
⎜
⎝
1
−3
2
⎞
⎟
⎠, (d)
⎛
⎜
⎝
−1
4
−1
⎞
⎟
⎠, (e)
⎛
⎜
⎝
−1
−2
5
⎞
⎟
⎠, (f )
⎛
⎜
⎜
⎜
⎝
2
−9
3
4
⎞
⎟
⎟
⎟
⎠, (g)
⎛
⎜
⎜
⎜
⎝
6
−4
0
−3
⎞
⎟
⎟
⎟
⎠.

1.6 Transposes and Symmetric Matrices
43
1.6 Transposes and Symmetric Matrices
Another basic operation on matrices is to interchange their rows and columns. If A is an
m × n matrix, then its transpose, denoted by AT , is the n × m matrix whose (i, j) entry
equals the (j, i) entry of A; thus
B = AT
means that
bij = aji.
For example, if
A =

1
2
3
4
5
6

,
then
AT =
⎛
⎝
1
4
2
5
3
6
⎞
⎠.
Observe that the rows of A become the columns of AT and vice versa. In particular, the
transpose of a row vector is a column vector, while the transpose of a column vector is a
row vector; if v =
⎛
⎝
1
2
3
⎞
⎠, then vT = ( 1 2 3 ). The transpose of a scalar, considered as a
1 × 1 matrix, is itself: cT = c.
Remark.
Most vectors appearing in applied mathematics are column vectors.
To
conserve vertical space in this text, we will often use the transpose notation, e.g.,
v = ( v1, v2, v3 )T , as a compact way of writing the column vector v =
⎛
⎝
v1
v2
v3
⎞
⎠.
In the square case, transposition can be viewed as “reﬂecting” the matrix entries across
the main diagonal. For example,
⎛
⎝
1
2
−1
3
0
5
−2
−4
8
⎞
⎠
T
=
⎛
⎝
1
3
−2
2
0
−4
−1
5
8
⎞
⎠.
In particular, the transpose of a lower triangular matrix is upper triangular and vice-versa.
Transposing twice returns you to where you started:
(AT )T = A.
(1.53)
Unlike inversion, transposition is compatible with matrix addition and scalar multiplica-
tion:
(A + B)T = AT + BT ,
(cA)T = cAT.
(1.54)
Transposition is also compatible with matrix multiplication, but with a twist. Like the
inverse, the transpose reverses the order of multiplication:
(AB)T = BT AT .
(1.55)
Indeed, if A has size m × n and B has size n × p, so they can be multiplied, then AT has
size n × m and BT has size p × n, and so, in general, one has no choice but to multiply
BT AT in that order. Formula (1.55) is a straightforward consequence of the basic laws of
matrix multiplication. More generally,
(A1A2 · · · Ak−1Ak)T = AT
k AT
k−1 · · · AT
2 AT
1 .

44
1 Linear Algebraic Systems
An important special case is the product of a row vector vT and a column vector w with
the same number of entries. In this case,
vT w = (vT w)T = wT v,
(1.56)
because their product is a scalar and so, as noted above, equals its own transpose.
Lemma 1.32. If A is a nonsingular matrix, so is AT , and its inverse is denoted by
A−T = (AT )−1 = (A−1)T .
(1.57)
Thus, transposing a matrix and then inverting yields the same result as ﬁrst inverting and
then transposing.
Proof : Let X = (A−1)T . Then, according to (1.55),
X AT = (A−1)T AT = (AA−1)T = I T = I .
The proof that AT X = I is similar, and so we conclude that X = (AT )−1.
Q.E.D.
Exercises
1.6.1. Write down the transpose of the following matrices: (a)

1
5
	
, (b)

1
1
0
2
	
,
(c)

1
2
2
1
	
,
(d)

1
2
−1
2
0
2
	
,
(e) ( 1
2
−3 ), (f )
⎛
⎜
⎝
1
2
3
4
5
6
⎞
⎟
⎠,
(g)
⎛
⎜
⎝
1
2
−1
0
3
2
1
1
5
⎞
⎟
⎠.
1.6.2. Let A =

3
−1
−1
1
2
1
	
, B =
⎛
⎜
⎝
−1
2
2
0
−3
4
⎞
⎟
⎠. Compute AT and BT . Then compute (AB)T
and (B A)T without ﬁrst computing AB or B A.
1.6.3. Show that (AB)T = AT BT if and only if A and B are square commuting matrices.
♦1.6.4. Prove formula (1.55).
1.6.5. Find a formula for the transposed product (AB C)T in terms of AT , BT and CT .
1.6.6. True or false: Every square matrix A commutes with its transpose AT .
♦1.6.7. A square matrix is called normal if it commutes with its transpose: AT A = AAT .
Find all normal 2 × 2 matrices.
1.6.8.(a) Prove that the inverse transpose operation (1.57) respects matrix multiplication:
(AB)−T = A−T B−T .
(b) Verify this identity for A =

1
−1
1
0
	
, B =

2
1
1
1
	
.
1.6.9. Prove that if A is an invertible matrix, then AAT and AT A are also invertible.
1.6.10. If v, w are column vectors with the same number of entries, does v wT = w vT ?
1.6.11. Is there a matrix analogue of formula (1.56), namely AT B = BT A?
♦1.6.12.(a) Let A be an m × n matrix. Let ej denote the 1 × n column vector with a single 1
in the jth entry, as in (1.44). Explain why the product Aej equals the jth column of A.
(b) Similarly, let ei be the 1 × m column vector with a single 1 in the ith entry. Explain
why the triple product eT
i Aej = aij equals the (i, j) entry of the matrix A.

1.6 Transposes and Symmetric Matrices
45
♦1.6.13. Let A and B be m × n matrices. (a) Suppose that vT Aw = vT B w for all vectors
v, w. Prove that A = B.
(b) Give an example of two matrices such that vT Av = vT B v
for all vectors v, but A ̸= B.
♦1.6.14.(a) Explain why the inverse of a permutation matrix equals its transpose: P −1 = P T .
(b) If A−1 = AT , is A necessarily a permutation matrix?
♦1.6.15. Let A be a square matrix and P a permutation matrix of the same size. (a) Explain
why the product AP T has the eﬀect of applying the permutation deﬁned by P to the
columns of A. (b) Explain the eﬀect of multiplying P AP T . Hint: Try this on some 3 × 3
examples ﬁrst.
♥1.6.16. Let v, w be n × 1 column vectors. (a) Prove that in most cases the inverse of the n × n
matrix A = I −vwT has the form A−1 = I −c vwT for some scalar c. Find all v, w for
which such a result is valid.
(b) Illustrate the method when v =

1
3
	
and w =

−1
2
	
.
(c) What happens when the method fails?
Factorization of Symmetric Matrices
A particularly important class of square matrices consists of those that are unchanged by
the transpose operation.
Deﬁnition 1.33. A matrix is called symmetric if it equals its own transpose: A = AT .
Thus, A is symmetric if and only if it is square and its entries satisfy aji = aij for all
i, j. In other words, entries lying in “mirror image” positions relative to the main diagonal
must be equal. For example, the most general symmetric 3 × 3 matrix has the form
A =
⎛
⎝
a
b
c
b
d
e
c
e
f
⎞
⎠.
Note that all diagonal matrices, including the identity, are symmetric. A lower or upper
triangular matrix is symmetric if and only if it is, in fact, a diagonal matrix.
The LDV factorization of a nonsingular matrix takes a particularly simple form if
the matrix also happens to be symmetric. This result will form the foundation of some
signiﬁcant later developments.
Theorem 1.34. A symmetric matrix A is regular if and only if it can be factored as
A = LDLT ,
(1.58)
where L is a lower unitriangular matrix and D is a diagonal matrix with nonzero diagonal
entries.
Proof : We already know, according to Theorem 1.29, that we can factor
A = LDV.
(1.59)
We take the transpose of both sides of this equation:
AT = (LDV )T = V T DT LT = V TDLT ,
(1.60)

46
1 Linear Algebraic Systems
since diagonal matrices are automatically symmetric: DT = D. Note that V T is lower
unitriangular, and LT is upper unitriangular. Therefore (1.60) is the LDV factorization
of AT .
In particular, if A is symmetric, then
LDV = A = AT = V T DLT .
Uniqueness of the LDV factorization implies that
L = V T
and
V = LT
(which are two versions of the same equation). Replacing V by LT in (1.59) establishes
the factorization (1.58).
Q.E.D.
Remark. If A = LDLT , then A is necessarily symmetric. Indeed,
AT = (L D LT )T = (LT )T DT LT = L D LT = A.
However, not every symmetric matrix has an LDLT factorization. A simple example is
the irregular but nonsingular 2 × 2 matrix

0
1
1
0

.
Example 1.35.
The problem is to ﬁnd the LDLT factorization of the particular sym-
metric matrix A =
⎛
⎝
1
2
1
2
6
1
1
1
4
⎞
⎠. This requires performing the usual Gaussian Elimination
algorithm. Subtracting twice the ﬁrst row from the second and also the ﬁrst row from the
third produces the matrix
⎛
⎝
1
2
1
0
2
−1
0
−1
3
⎞
⎠. We then add one half of the second row of the
latter matrix to its third row, resulting in the upper triangular form
U =
⎛
⎝
1
2
1
0
2
−1
0
0
5
2
⎞
⎠=
⎛
⎝
1
0
0
0
2
0
0
0
5
2
⎞
⎠
⎛
⎝
1
2
1
0
1
−1
2
0
0
1
⎞
⎠= DV,
which we further factor by dividing each row of U by its pivot. On the other hand, the lower
unitriangular matrix associated with the preceding row operations is L =
⎛
⎝
1
0
0
2
1
0
1
−1
2
1
⎞
⎠,
which, as guaranteed by Theorem 1.34, is the transpose of V = LT . Therefore, the desired
A = LU = LDLT factorizations of this particular symmetric matrix are
⎛
⎝
1
2
1
2
6
1
1
1
4
⎞
⎠=
⎛
⎝
1
0
0
2
1
0
1
−1
2
1
⎞
⎠
⎛
⎝
1
2
1
0
2
−1
0
0
5
2
⎞
⎠=
⎛
⎝
1
0
0
2
1
0
1
−1
2
1
⎞
⎠
⎛
⎝
1
0
0
0
2
0
0
0
5
2
⎞
⎠
⎛
⎝
1
2
1
0
1
−1
2
0
0
1
⎞
⎠.
Example 1.36.
Let us look at a general 2 × 2 symmetric matrix A =

a
b
b
c

.
Regularity requires that the ﬁrst pivot be a ̸= 0. A single row operation will place A
in upper triangular form U =

a
b
0
ac −b2
a

, and so A is regular provided ac −b2 ̸= 0

1.6 Transposes and Symmetric Matrices
47
also. The associated lower triangular matrix is L =

1
0
b
a
1

. Thus, A = LU, as you can
check. Finally, D =
 a
0
0
ac −b2
a

is just the diagonal part of U, and hence U = DLT ,
so that the LDLT factorization is explicitly given by

a
b
b
c

=

1
0
b
a
1
  a
0
0
ac −b2
a
 
1
b
a
0
1

.
(1.61)
Exercises
1.6.17. Find all values of a, b, and c for which the following matrices are symmetric:
(a)

3
a
2a −1
a −2
	
,
(b)
⎛
⎜
⎝
1
a
2
−1
b
c
b
3
0
⎞
⎟
⎠,
(c)
⎛
⎜
⎝
3
a + 2b −2c
−4
6
7
b −c
−a + b + c
4
b + 3c
⎞
⎟
⎠.
1.6.18. List all symmetric (a) 3 × 3 permutation matrices, (b) 4 × 4 permutation matrices.
1.6.19. True or false: If A is symmetric, then A2 is symmetric.
♦1.6.20. True or false: If A is a nonsingular symmetric matrix, then A−1 is also symmetric.
♦1.6.21. True or false: If A and B are symmetric n × n matrices, so is AB.
1.6.22.(a) Show that every diagonal matrix is symmetric. (b) Show that an upper (lower)
triangular matrix is symmetric if and only if it is diagonal.
1.6.23. Let A be a symmetric matrix. (a) Show that An is symmetric for every nonnegative
integer n. (b) Show that 2A2 −3A + I is symmetric. (c) Show that every matrix
polynomial p(A) of A, cf. Exercise 1.2.35, is a symmetric matrix.
1.6.24. Show that if A is any matrix, then K = AT A and L = AAT are both well-deﬁned,
symmetric matrices.
1.6.25. Find the LDLT factorization of the following symmetric matrices:
(a)

1
1
1
4
	
,
(b)

−2
3
3
−1
	
,
(c)
⎛
⎜
⎝
1
−1
−1
−1
3
2
−1
2
0
⎞
⎟
⎠,
(d)
⎛
⎜
⎜
⎜
⎝
1
−1
0
3
−1
2
2
0
0
2
−1
0
3
0
0
1
⎞
⎟
⎟
⎟
⎠.
1.6.26. Find the LDLT factorization of the matrices
M2 =

2
1
1
2
	
,
M3 =
⎛
⎜
⎝
2
1
0
1
2
1
0
1
2
⎞
⎟
⎠,
and
M4 =
⎛
⎜
⎜
⎜
⎝
2
1
0
0
1
2
1
0
0
1
2
1
0
0
1
2
⎞
⎟
⎟
⎟
⎠.
♦1.6.27. Prove that the 3 × 3 matrix A =
⎛
⎜
⎝
1
2
1
2
4
−1
1
−1
3
⎞
⎟
⎠cannot be factored as A = LDLT .
♥1.6.28. Skew-symmetric matrices: An n × n matrix J is called skew-symmetric if JT = −J.
(a) Show that every diagonal entry of a skew-symmetric matrix is zero. (b) Write down
an example of a nonsingular skew-symmetric matrix. (c) Can you ﬁnd a regular skew-
symmetric matrix? (d) Show that if J is a nonsingular skew-symmetric matrix, then J−1 is
also skew-symmetric. Verify this fact for the matrix you wrote down in part (b). (e) Show
that if J and K are skew-symmetric, then so are JT , J + K, and J −K. What about J K?
(f ) Prove that if J is a skew-symmetric matrix, then vT J v = 0 for all vectors v ∈Rn.

48
1 Linear Algebraic Systems
1.6.29.(a) Prove that every square matrix can be expressed as the sum, A = S + J, of a
symmetric matrix S = ST and a skew-symmetric matrix J = −JT .
(b) Write

1
2
3
4
	
and
⎛
⎜
⎝
1
2
3
4
5
6
7
8
9
⎞
⎟
⎠as the sum of symmetric and skew-symmetric matrices.
♦1.6.30. Suppose A = LU is a regular matrix. Write down the LU factorization of AT . Prove
that AT is also regular, and its pivots are the same as the pivots of A.
1.7 Practical Linear Algebra
For pedagogical and practical reasons, the examples and exercises we have chosen to illus-
trate the algorithms are all based on relatively small matrices. When dealing with matrices
of moderate size, the diﬀerences between the various approaches to solving linear systems
(Gauss, Gauss–Jordan, matrix inverse, and so on) are relatively unimportant, particularly
if one has a decent computer or even hand calculator to do the tedious parts. However,
real-world applied mathematics deals with much larger linear systems, and the design of
eﬃcient algorithms is a must. For example, numerical solution schemes for ordinary diﬀer-
ential equations will typically lead to matrices with thousands of entries, while numerical
schemes for partial diﬀerential equations arising in ﬂuid and solid mechanics, weather pre-
diction, image and video processing, quantum mechanics, molecular dynamics, chemical
processes, etc., will often require dealing with matrices with more than a million entries.
It is not hard for such systems to tax even the most sophisticated supercomputer. Thus, it
is essential that we understand the computational details of competing methods in order
to compare their eﬃciency, and thereby gain some experience with the issues underlying
the design of high performance numerical algorithms.
The most basic question is this: how many arithmetic operations† — in numerical
applications these are almost always performed in ﬂoating point with various precision
levels — are required to complete an algorithm? The number will directly inﬂuence the
time spent running the algorithm on a computer. We shall keep track of additions and
multiplications separately, since the latter typically take longer to process.‡ But we shall
not distinguish between addition and subtraction, nor between multiplication and division,
since these typically have the same complexity. We shall also assume that the matrices
and vectors we deal with are generic, with few, if any, zero entries. Modiﬁcations of the
basic algorithms for sparse matrices, meaning those that have lots of zero entries, are an
important topic of research, since these include many of the large matrices that appear
in applications to diﬀerential equations. We refer the interested reader to more advanced
treatments of numerical linear algebra, such as [21, 40, 66, 89], for such developments.
First, when multiplying an n × n matrix A and an n × 1 column vector b, each entry
of the product Ab requires n multiplications of the form aij bj and n −1 additions to sum
the resulting products. Since there are n entries, this means a total of n2 multiplications
†
For simplicity, we will count only the basic arithmetic operations. But it is worth noting
that other issues, such as the number of storage and retrieval operations, may also play a role in
estimating the computational complexity of a numerical algorithm.
‡
At least, in traditional computer architectures. New algorithms and new methods for per-
forming basic arithmetic operations on a computer, particularly in high precision arithmetic, make
this discussion trickier. For simplicity, we will stay with the “classical” version here.

1.7 Practical Linear Algebra
49
and n(n −1) = n2 −n additions. Thus, for a matrix of size n = 100, one needs about
10,000 distinct multiplications and a similar number of additions. If n = 1,000,000 = 106,
then n2 = 1012, which is phenomenally large, and the total time required to perform the
computation becomes a signiﬁcant issue†.
Let us next look at the (regular) Gaussian Elimination algorithm, referring back to
our pseudocode program for the notational details. First, we count how many arithmetic
operations are based on the jth pivot mjj. For each of the n −j rows lying below it, we
must perform one division to compute the factor lij = mij/mjj used in the elementary
row operation. The entries in the column below the pivot will be set to zero automatically,
and so we need only compute the updated entries lying strictly below and to the right of
the pivot. There are (n −j)2 such entries in the coeﬃcient matrix and an additional n −j
entries in the last column of the augmented matrix. Let us concentrate on the former for
the moment. For each of these, we replace mik by mik −lij mjk, and so must perform one
multiplication and one addition. For the jth pivot, there is a total of (n −j)(n −j + 1)
multiplications — including the initial n −j divisions needed to produce the lij — and
(n −j)2 additions needed to update the coeﬃcient matrix. Therefore, to reduce a regular
n × n matrix to upper triangular form requires a total‡ of
n

j =1
(n −j)(n −j + 1) = n3 −n
3
multiplications, and
n

j =1
(n −j)2 = 2n3 −3n2 + n
6
additions.
(1.62)
Thus, when n is large, both involve approximately 1
3 n3 operations.
We should also be keeping track of the number of operations on the right-hand side of
the system. No pivots appear there, and so there are
n

j =1
(n −j) = n2 −n
2
(1.63)
multiplications and the same number of additions required to produce the right-hand side
in the resulting triangular system U x = c. For large n, this count is considerably smaller
than the coeﬃcient matrix totals (1.62). We note that the Forward Substitution equations
(1.26) require precisely the same number of arithmetic operations to solve Lc = b for the
right-hand side of the upper triangular system. Indeed, the jth equation
cj = bj −
j−1

k=1
ljk ck
requires j −1 multiplications and the same number of additions, giving a total of
n

j =1
(j −1) = n2 −n
2
operations of each type. Therefore, to reduce a linear system to upper triangular form,
it makes no diﬀerence in computational eﬃciency whether one works directly with the
†
See Exercise 1.7.8 for more sophisticated computational algorithms that can be employed to
(slightly) speed up multiplication of large matrices.
‡
In Exercise 1.7.4, the reader is asked to prove these summation formulaes by induction.

50
1 Linear Algebraic Systems
augmented matrix or employs Forward Substitution after the LU factorization of the co-
eﬃcient matrix has been established.
The Back Substitution phase of the algorithm can be similarly analyzed. To ﬁnd the
value of
xj =
1
ujj
⎛
⎝cj −
n

k=j+1
ujk xk
⎞
⎠
once we have computed xj+1, . . . , xn, requires n−j + 1 multiplications/divisions and n−j
additions. Therefore, Back Substitution requires
n

j =1
(n −j + 1) = n2 + n
2
multiplications, along with
n

j =1
(n −j) = n2 −n
2
additions.
(1.64)
For n large, both of these are approximately equal to 1
2 n2. Comparing the counts, we
conclude that the bulk of the computational eﬀort goes into the reduction of the coeﬃcient
matrix to upper triangular form.
Combining the two counts (1.63–64), we discover that, once we have computed the
A = LU decomposition of the coeﬃcient matrix, the Forward and Back Substitution
process requires n2 multiplications and n2 −n additions to solve a linear system A x = b.
This is exactly the same as the number of multiplications and additions needed to compute
the product A−1b. Thus, even if we happen to know the inverse of A, it is still just as
eﬃcient to use Forward and Back Substitution to compute the solution!
On the other hand, the computation of A−1 is decidedly more ineﬃcient. There are two
possible strategies. First, we can solve the n linear systems (1.45), namely
A x = ei,
i = 1, . . ., n,
(1.65)
for the individual columns of A−1. This requires ﬁrst computing the LU decomposition,
which uses about 1
3 n3 multiplications and a similar number of additions, followed by apply-
ing Forward and Back Substitution to each of the systems, using n·n2 = n3 multiplications
and n(n2 −n) ≈n3 additions, for a grand total of about 4
3 n3 operations of each type in
order to compute A−1. Gauss–Jordan Elimination fares no better (in fact, slightly worse),
also requiring about the same number, 4
3 n3, of each type of arithmetic operation. Both
algorithms can be made more eﬃcient by exploiting the fact that there are lots of zeros
on the right-hand sides of the systems (1.65). Designing the algorithm to avoid adding
or subtracting a preordained 0, or multiplying or dividing by a preordained ±1, reduces
the total number of operations required to compute A−1 to exactly n3 multiplications and
n(n−1)2 ≈n3 additions. (Details are relegated to the exercises.) And don’t forget that we
still need to multiply A−1b to solve the original system. As a result, solving a linear system
with the inverse matrix requires approximately three times as many arithmetic operations,
and so would take three times as long to complete, as the more elementary Gaussian Elim-
ination and Back Substitution algorithm. This justiﬁes our earlier contention that matrix
inversion is ineﬃcient, and, except in very special situations, should never be used for
solving linear systems in practice.

1.7 Practical Linear Algebra
51
Exercises
1.7.1. Solve the following linear systems by (i) Gaussian Elimination with Back Substitution;
(ii) the Gauss–Jordan algorithm to convert the augmented matrix to the fully reduced
form

I | x

with solution x; (iii) computing the inverse of the coeﬃcient matrix,
and then multiplying it by the right-hand side. Keep track of the number of arithmetic
operations you need to perform to complete each computation, and discuss their relative
eﬃciency.
(a) x −2y = 4
3x + y = −7,
(b)
2x −4y + 6z = 6,
3x −3y + 4z = −1,
−4x + 3y −4z = 5,
(c)
x −3y
= 1,
3x −7y + 5z = −1,
−2x + 6y −5z = 0.
1.7.2.(a) Let A be an n × n matrix. Which is faster to compute, A2 or A−1? Justify
your answer.
(b) What about A3 versus A−1? (c) How many operations are needed
to compute Ak? Hint: When k > 3, you can get away with less than k −1 matrix
multiplications!
1.7.3. Which is faster: Back Substitution or multiplying a matrix by a vector? How much faster?
♦1.7.4. Use induction to prove the summation formulas (1.62), (1.63) and (1.64).
♥1.7.5. Let A be a general n × n matrix. Determine the exact number of arithmetic operations
needed to compute A−1 using (a) Gaussian Elimination to factor P A = LU and then
Forward and Back Substitution to solve the n linear systems (1.65); (b) the Gauss–
Jordan method. Make sure your totals do not count adding or subtracting a known 0, or
multiplying or dividing by a known ±1.
1.7.6. Count the number of arithmetic operations needed to solve a system the “old-fashioned”
way, by using elementary row operations of all three types, in the same order as the Gauss–
Jordan scheme, to fully reduce the augmented matrix M =

A | b

to the form

I | d

,
with x = d being the solution.
1.7.7. An alternative solution strategy, also called Gauss–Jordan in some texts, is, once a pivot
is in position, to use elementary row operations of type #1 to eliminate all entries both
above and below it, thereby reducing the augmented matrix to diagonal form

D | c

where D = diag (d1, . . . , dn) is a diagonal matrix containing the pivots. The solutions
xi = ci/di are then obtained by simple division. Is this strategy more eﬃcient, less eﬃcient,
or the same as Gaussian Elimination with Back Substitution? Justify your answer with an
exact operations count.
♥1.7.8. Here, we describe a remarkable algorithm for matrix multiplication discovered by
Strassen, [82]. Let A =

A1
A2
A3
A4
	
, B =

B1
B2
B3
B4
	
, and C =

C1
C2
C3
C4
	
= AB
be block matrices of size n = 2m, where all blocks are of size m × m. (a) Let D1 =
(A1 + A4)(B1 + B4), D2 = (A1 −A3)(B1 + B2), D3 = (A2 −A4)(B3 + B4),
D4 = (A1 + A2)B4, D5 = (A3 + A4)B1, D6 = A4(B1 −B3), D7 = A1(B2 −B4). Show
that C1 = D1 + D3 −D4 −D6, C2 = D4 + D7, C3 = D5 −D6, C4 = D1 −D2 −D5 + D7.
(b) How many arithmetic operations are required when A and B are 2 × 2 matrices? How
does this compare with the usual method of multiplying 2 × 2 matrices?
(c) In the general case, suppose we use standard matrix multiplication for the matrix
products in D1, . . . , D7. Prove that Strassen’s Method is faster than the direct algorithm
for computing AB by a factor of ≈7
8. (d) When A and B have size n × n with n = 2r,
we can recursively apply Strassen’s Method to multiply the 2r−1 × 2r−1 blocks Ai, Bi.
Prove that the resulting algorithm requires a total of 7r = nlog2 7 = n2.80735 multiplications

52
1 Linear Algebraic Systems
and 6(7r−1 −4r−1) ≤7r = nlog2 7 additions/subtractions, versus n3 multiplications and
n3 −n2 ≈n3 additions for the ordinary matrix multiplication algorithm. How much faster
is Strassen’s Method when n = 210? 225? 2100? (e) How might you proceed if the size of
the matrices does not happen to be a power of 2? Further developments of these ideas can
be found in [11, 40].
Tridiagonal Matrices
Of course, in special cases, the actual arithmetic operation count might be considerably
reduced, particularly if A is a sparse matrix with many zero entries. A number of specialized
techniques have been designed to handle sparse linear systems. A particularly important
class consists of the tridiagonal matrices
A =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎝
q1
r1
p1
q2
r2
p2
q3
r3
...
...
...
pn−2
qn−1
rn−1
pn−1
qn
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎠
(1.66)
with all entries zero except for those on the main diagonal, namely aii = qi, the subdi-
agonal, meaning the n −1 entries ai+1,i = pi immediately below the main diagonal, and
the superdiagonal, meaning the entries ai,i+1 = ri immediately above the main diagonal.
(Blanks are used to indicate 0 entries.) Such matrices arise in the numerical solution of
ordinary diﬀerential equations and the spline ﬁtting of curves for interpolation and com-
puter graphics. If A = LU is regular, it turns out that the factors are lower and upper
bidiagonal matrices, of the form
L =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎝
1
l1
1
l2
1
...
...
ln−2
1
ln−1
1
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎠
,
U =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎝
d1
u1
d2
u2
d3
u3
...
...
dn−1
un−1
dn
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎠
. (1.67)
Multiplying out LU and equating the result to A leads to the equations
d1 = q1,
u1 = r1,
l1 d1 = p1,
l1 u1 + d2 = q2,
u2 = r2,
l2 d2 = p2,
...
...
...
lj−1 uj−1 + dj = qj,
uj = rj,
lj dj = pj,
...
...
...
ln−2 un−2 + dn−1 = qn−1,
un−1 = rn−1,
ln−1 dn−1 = pn−1,
ln−1 un−1 + dn = qn.
(1.68)
These elementary algebraic equations can be successively solved for the entries of L and U
in the following order: d1, u1, l1, d2, u2, l2, d3, u3 . . . . The original matrix A is regular pro-
vided none of the diagonal entries d1, d2, . . . are zero, which allows the recursive procedure
to successfully proceed to termination.

1.7 Practical Linear Algebra
53
Once the LU factors are in place, we can apply Forward and Back Substitution to solve
the tridiagonal linear system A x = b. We ﬁrst solve the lower triangular system Lc = b
by Forward Substitution, which leads to the recursive equations
c1 = b1,
c2 = b2 −l1 c1,
. . .
cn = bn −ln−1 cn−1.
(1.69)
We then solve the upper triangular system U x = c by Back Substitution, again recursively:
xn = cn
dn
,
xn−1 = cn−1 −un−1 xn
dn−1
,
. . .
x1 = c1 −u1 x2
d1
.
(1.70)
As you can check, there are a total of 5n −4 multiplications/divisions and 3n −3 addi-
tions/subtractions required to solve a general tridiagonal system of n linear equations —
a striking improvement over the general case.
Example 1.37.
Consider the n × n tridiagonal matrix
A =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
4
1
1
4
1
1
4
1
1
4
1
... ... ...
1
4
1
1
4
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
in which the diagonal entries are all qi = 4, while the entries immediately above and below
the main diagonal are all pi = ri = 1. According to (1.68), the tridiagonal factorization
(1.67) has u1 = u2 = · · · = un−1 = 1, while
d1 = 4,
lj = 1/dj,
dj+1 = 4 −lj,
j = 1, 2, . . ., n −1.
The computed values are
j
1
2
3
4
5
6
7
dj
4.0
3.75
3.733333
3.732143
3.732057
3.732051
3.732051
lj
.25
.266666
.267857
.267942
.267948
.267949
.267949
These converge rapidly to
dj
−→
2 +
√
3 = 3.732050 . . . ,
lj
−→
2 −
√
3 = .267949 . . . ,
which makes the factorization for large n almost trivial. The numbers 2±
√
3 are the roots
of the quadratic equation x2 −4x + 1 = 0, and are characterized as the ﬁxed points of the
nonlinear iterative system dj+1 = 4 −1/dj.

54
1 Linear Algebraic Systems
Exercises
1.7.9. For each of the following tridiagonal systems ﬁnd the LU factorization of the coeﬃcient
matrix, and then solve the system.
(a)
⎛
⎜
⎝
1
2
0
−1
−1
1
0
−2
3
⎞
⎟
⎠x =
⎛
⎜
⎝
4
−1
−6
⎞
⎟
⎠,
(b)
⎛
⎜
⎜
⎜
⎝
1
−1
0
0
−1
2
1
0
0
−1
4
1
0
0
−5
6
⎞
⎟
⎟
⎟
⎠x =
⎛
⎜
⎜
⎜
⎝
1
0
6
7
⎞
⎟
⎟
⎟
⎠,
(c)
⎛
⎜
⎜
⎜
⎝
1
2
0
0
−1
−3
0
0
0
−1
4
−1
0
0
−1
−1
⎞
⎟
⎟
⎟
⎠x =
⎛
⎜
⎜
⎜
⎝
0
−2
−3
1
⎞
⎟
⎟
⎟
⎠.
1.7.10. True or false: (a) The product of two tridiagonal matrices is tridiagonal.
(b) The inverse of a tridiagonal matrix is tridiagonal.
1.7.11.(a) Find the LU factorization of the n × n tridiagonal matrix An with all 2’s along the
diagonal and all −1’s along the sub- and super-diagonals for n = 3, 4, and 5.
(b) Use
your factorizations to solve the system An x = b, where b = (1, 1, 1, . . . , 1)T .
(c) Can
you write down the LU factorization of An for general n? Do the entries in the factors
approach a limit as n gets larger and larger? (d) Can you ﬁnd the solution to the system
An x = b = (1, 1, 1, . . . , 1)T for general n?
♠1.7.12. Answer Exercise 1.7.11 if the super-diagonal entries of An are changed to +1.
♠1.7.13. Find the LU factorizations of
⎛
⎜
⎝
4
1
1
1
4
1
1
1
4
⎞
⎟
⎠,
⎛
⎜
⎜
⎜
⎝
4
1
0
1
1
4
1
0
0
1
4
1
1
0
1
4
⎞
⎟
⎟
⎟
⎠,
⎛
⎜
⎜
⎜
⎜
⎜
⎝
4
1
0
0
1
1
4
1
0
0
0
1
4
1
0
0
0
1
4
1
1
0
0
1
4
⎞
⎟
⎟
⎟
⎟
⎟
⎠
.
Do you see a pattern? Try the 6 × 6 version. The following exercise should now be clear.
♥1.7.14. A tricirculant matrix C =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
q1
r1
p1
p2
q2
r2
p3
q3
r3
...
...
...
pn−1
qn−1
rn−1
rn
pn
qn
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
is tridiagonal except
for its (1, n) and (n, 1) entries. Tricirculant matrices arise in the numerical solution of
periodic boundary value problems and in spline interpolation.
(a) Prove that if C = LU is regular, its factors have the form
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
1
l1
1
l2
1
l3
1
...
...
ln−2
1
m1
m2
m3
. . .
mn−2
ln−1
1
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
,
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
d1
u1
v1
d2
u2
v2
d3
u3
v3
...
...
...
dn−2
un−2
vn−2
dn−1
un−1
dn
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
.
(b) Compute the LU factorization of the n × n tricirculant matrix
Cn =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
1
−1
−1
−1
2
−1
−1
3
−1
...
...
...
−1
n −1
−1
−1
−1
1
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
for n = 3, 5, and 6. What goes wrong when n = 4?

1.7 Practical Linear Algebra
55
♥1.7.15. A matrix A is said to have bandwidth k if all entries that are more than k slots away
from the main diagonal are zero: aij = 0 whenever | i −j | > k. (a) Show that a tridiagonal
matrix has band width 1. (b) Write down an example of a 6 × 6 matrix of band width 2
and one of band width 3. (c) Prove that the L and U factors of a regular banded matrix
have the same band width. (d) Find the LU factorization of the matrices you wrote down
in part (b). (e) Use the factorization to solve the system A x = b, where b is the column
vector with all entries equal to 1. (f ) How many arithmetic operations are needed to solve
A x = b if A is banded? (g) Prove or give a counterexample: the inverse of a banded
matrix is banded.
Pivoting Strategies
Let us now investigate the practical side of pivoting. As we know, in the irregular situations
when a zero shows up in a diagonal pivot position, a row interchange is required to proceed
with the elimination algorithm. But even when a nonzero pivot element is in place, there
may be good numerical reasons for exchanging rows in order to install a more desirable
element in the pivot position. Here is a simple example:
.01 x + 1.6 y = 32.1,
x + .6 y = 22.
(1.71)
The exact solution to the system is easily found:
x = 10,
y = 20.
Suppose we are working with a very primitive calculator that only retains 3 digits of
accuracy.
(Of course, this is not a very realistic situation, but the example could be
suitably modiﬁed to produce similar diﬃculties no matter how many digits of accuracy our
computer is capable of retaining.) The augmented matrix is

.01
1.6
1
.6

32.1
22

.
Choosing the (1, 1) entry as our pivot, and subtracting 100 times the ﬁrst row from the
second produces the upper triangular form

.01
1.6
0
−159.4

32.1
−3188

.
Since our calculator has only three–place accuracy, it will round the entries in the second
row, producing the augmented coeﬃcient matrix

.01
1.6
0
−159.0

32.1
−3190

.
The solution by Back Substitution gives
y = 3190/159 = 20.0628 . . . ≃20.1,
and then
x = 100 (32.1 −1.6 y) = 100 (32.1 −32.16) ≃100 (32.1 −32.2) = −10.
The relatively small error in y has produced a very large error in x — not even its sign is
correct!
The problem is that the ﬁrst pivot, .01, is much smaller than the other element, 1, that
appears in the column below it. Interchanging the two rows before performing the row

56
1 Linear Algebraic Systems
Gaussian Elimination With Partial Pivoting
start
for i = 1 to n
set r(i) = i
next i
for j = 1 to n
if mr(i),j = 0 for all i ≥j, stop; print “A is singular”
choose i > j such that mr(i),j is maximal
interchange r(i) ←→r(j)
for i = j + 1 to n
set lr(i)j = mr(i)j/mr(j)j
for k = j + 1 to n + 1
set mr(i)k = mr(i)k −lr(i)jmr(j)k
next k
next i
next j
end
operation would resolve the diﬃculty — even with such an inaccurate calculator! After
the interchange, we have

1
.6
.01
1.6

22
32.1

,
which results in the rounded-oﬀupper triangular form

1
.6
0
1.594

22
31.88

≃

1
.6
0
1.59

22
31.9

.
The solution by Back Substitution now gives a respectable answer:
y = 31.9/1.59 = 20.0628 . . . ≃20.1,
x = 22 −.6 y = 22 −12.06 ≃22 −12.1 = 9.9.
The general strategy, known as Partial Pivoting, says that at each stage, we should
use the largest (in absolute value) legitimate (i.e., in the pivot column on or below the
diagonal) element as the pivot, even if the diagonal element is nonzero. Partial Pivoting
can help suppress the undesirable eﬀects of round-oﬀerrors during the computation.
In a computer implementation of pivoting, there is no need to waste processor time
physically exchanging the row entries in memory. Rather, one introduces a separate array
of pointers that serve to indicate which original row is currently in which permuted position.
More concretely, one initializes n row pointers r(1) = 1, . . . , r(n) = n.
Interchanging
row i and row j of the coeﬃcient or augmented matrix is then accomplished by merely
interchanging r(i) and r(j). Thus, to access a matrix element that is currently in row i of
the augmented matrix, one merely retrieves the element that is in row r(i) in the computer’s
memory.
An explicit implementation of this strategy is provided in the accompanying
pseudocode program.

1.7 Practical Linear Algebra
57
Partial pivoting will solve most problems, although there can still be diﬃculties. For
instance, it does not accurately solve the system
10 x + 1600 y = 32100,
x + .6 y = 22,
obtained by multiplying the ﬁrst equation in (1.71) by 1000. The tip-oﬀis that, while the
entries in the column containing the pivot are smaller, those in its row are much larger. The
solution to this diﬃculty is Full Pivoting, in which one also performs column interchanges
— preferably with a column pointer — to move the largest legitimate element into the
pivot position. In practice, a column interchange amounts to reordering the variables in
the system, which, as long as one keeps proper track of the order, also doesn’t change the
solutions. Thus, switching the order of x, y leads to the augmented matrix

1600
10
.6
1

32100
22

,
in which the ﬁrst column now refers to y and the second to x. Now Gaussian Elimination
will produce a reasonably accurate solution to the system.
Finally, there are some matrices that are hard to handle even with sophisticated pivoting
strategies.
Such ill-conditioned matrices are typically characterized by being “almost”
singular. A famous example of an ill-conditioned matrix is the n × n Hilbert matrix
Hn =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
1
1
2
1
3
1
4
. . .
1
n
1
2
1
3
1
4
1
5
. . .
1
n + 1
1
3
1
4
1
5
1
6
. . .
1
n + 2
1
4
1
5
1
6
1
7
. . .
1
n + 3
...
...
...
...
...
...
1
n
1
n + 1
1
n + 2
1
n + 3
. . .
1
2n −1
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
.
(1.72)
Later, in Proposition 3.40, we will prove that Hn is nonsingular for all n. However, the solu-
tion of a linear system whose coeﬃcient matrix is a Hilbert matrix Hn, even for moderately
large n, is a very challenging problem, even using high precision computer arithmetic†. This
is because the larger n is, the closer Hn is, in a sense, to being singular. A full discussion
of the so-called condition number of a matrix can be found in Section 8.7.
The reader is urged to try the following computer experiment. Fix a moderately large
value of n, say 20. Choose a column vector x with n entries chosen at random. Compute
b = Hn x directly. Then try to solve the system Hn x = b by Gaussian Elimination, and
compare the result with the original vector x. If you obtain an accurate solution with
n = 20, try n = 50 or 100. This will give you a good indicator of the degree of arithmetic
precision used by your computer hardware, and the accuracy of the numerical solution
algorithm(s) in your software.
†
In computer algebra systems such as Maple and Mathematica, one can use exact rational
arithmetic to perform the computations. Then the important issues are time and computational
eﬃciency.
Incidentally, there is an explicit formula for the inverse of a Hilbert matrix, which
appears in Exercise 1.7.23.

58
1 Linear Algebraic Systems
Exercises
1.7.16.(a) Find the exact solution to the linear system

.1
2.7
1.0
.5
	 
x
y
	
=

10.
−6.0
	
. (b) Solve
the system using Gaussian Elimination with 2-digit rounding. (c) Solve the system using
Partial Pivoting and 2-digit rounding. (d) Compare your answers and discuss.
1.7.17.(a) Find the exact solution to the linear system x −5y −z = 1,
1
6 x −5
6 y + z = 0,
2x −y = 3. (b) Solve the system using Gaussian Elimination with 4-digit rounding.
(c) Solve the system using Partial Pivoting and 4-digit rounding. Compare your answers.
1.7.18. Answer Exercise 1.7.17 for the system
x + 4y −3z = −3,
25x + 97y −35z = 39,
35x −22y + 33z = −15.
1.7.19. Employ 2 digit arithmetic with rounding to compute an approximate solution of the
linear system 0.2x + 2y −3z = 6, 5x + 43y + 27z = 58, 3x + 23y −42z = −87,
using the following methods: (a) Regular Gaussian Elimination with Back Substitution;
(b) Gaussian Elimination with Partial Pivoting; (c) Gaussian Elimination with Full
Pivoting.
(d) Compare your answers and discuss their accuracy.
1.7.20. Solve the following systems by hand, using pointers instead of physically interchanging
the rows:
(a)
⎛
⎜
⎝
0
1
−2
1
−1
1
3
1
0
⎞
⎟
⎠
⎛
⎜
⎝
x
y
z
⎞
⎟
⎠=
⎛
⎜
⎝
1
2
1
⎞
⎟
⎠,
(b)
⎛
⎜
⎜
⎜
⎝
0
−1
0
−1
0
0
−2
1
1
0
2
0
−1
0
0
3
⎞
⎟
⎟
⎟
⎠
⎛
⎜
⎜
⎜
⎝
x
y
z
w
⎞
⎟
⎟
⎟
⎠=
⎛
⎜
⎜
⎜
⎝
1
0
0
1
⎞
⎟
⎟
⎟
⎠,
(c)
⎛
⎜
⎜
⎜
⎝
3
−1
2
−1
6
−2
4
3
3
1
0
−2
−1
3
−2
0
⎞
⎟
⎟
⎟
⎠
⎛
⎜
⎜
⎜
⎝
x
y
z
w
⎞
⎟
⎟
⎟
⎠=
⎛
⎜
⎜
⎜
⎝
1
2
1
1
⎞
⎟
⎟
⎟
⎠,
(d)
⎛
⎜
⎜
⎜
⎝
0
−1
5
−1
1
−2
0
1
2
−3
−3
−1
2
0
1
−1
⎞
⎟
⎟
⎟
⎠
⎛
⎜
⎜
⎜
⎝
x
y
z
w
⎞
⎟
⎟
⎟
⎠=
⎛
⎜
⎜
⎜
⎝
1
−2
3
0
⎞
⎟
⎟
⎟
⎠.
1.7.21. Solve the following systems using Partial Pivoting and pointers:
(a)

1
5
2
−3
	 
x
y
	
=

3
−2
	
,
(b)
⎛
⎜
⎝
1
2
−1
4
−2
1
3
5
−1
⎞
⎟
⎠
⎛
⎜
⎝
x
y
z
⎞
⎟
⎠=
⎛
⎜
⎝
1
3
1
⎞
⎟
⎠,
(c)
⎛
⎜
⎜
⎜
⎝
1
−3
6
−1
2
−5
0
1
−1
−6
4
−2
3
0
2
−1
⎞
⎟
⎟
⎟
⎠
⎛
⎜
⎜
⎜
⎝
x
y
z
w
⎞
⎟
⎟
⎟
⎠=
⎛
⎜
⎜
⎜
⎝
1
−2
0
1
⎞
⎟
⎟
⎟
⎠,
(d)
⎛
⎜
⎝
.01
4
2
2
−802
3
7
.03
250
⎞
⎟
⎠
⎛
⎜
⎝
x
y
z
⎞
⎟
⎠=
⎛
⎜
⎝
1
2
122
⎞
⎟
⎠.
1.7.22. Use Full Pivoting with pointers to solve the systems in Exercise 1.7.21.
♠1.7.23. Let Hn be the n × n Hilbert matrix (1.72), and Kn = H−1
n
its inverse. It can be
proved, [40; p. 513], that the (i, j) entry of Kn is
(−1)i+j(i + j −1)
⎛
⎝n + i −1
n −j
⎞
⎠
⎛
⎝n + j −1
n −i
⎞
⎠
⎛
⎝i + j −2
i −1
⎞
⎠
2
,
where
⎛
⎝n
k
⎞
⎠=
n!
k! (n −k)! is the standard binomial coeﬃcient. (Warning. Proving this
formula is a nontrivial combinatorial challenge.) (a) Write down the inverse of the Hilbert
matrices H3, H4, H5 using the formula or the Gauss–Jordan Method with exact rational
arithmetic. Check your results by multiplying the matrix by its inverse.
(b) Recompute the inverses on your computer using ﬂoating point arithmetic and compare
with the exact answers.
(c) Try using ﬂoating point arithmetic to ﬁnd K10 and K20. Test
the answer by multiplying the Hilbert matrix by its computed inverse.
♠1.7.24.(a) Write out a pseudo-code algorithm, using both row and column pointers, for
Gaussian Elimination with Full Pivoting.
(b) Implement your code on a computer, and
try it on the systems in Exercise 1.7.21.

1.8 General Linear Systems
59
1.8 General Linear Systems
So far, we have treated only linear systems involving the same number of equations as
unknowns, and then only those with nonsingular coeﬃcient matrices. These are precisely
the systems that always have a unique solution. We now turn to the problem of solving a
general linear system of m equations in n unknowns. The cases not treated as yet are non-
square systems, with m ̸= n, as well as square systems with singular coeﬃcient matrices.
The basic idea underlying the Gaussian Elimination algorithm for nonsingular systems can
be straightforwardly adapted to these cases, too. One systematically applies the same two
types of elementary row operation to reduce the coeﬃcient matrix to a simpliﬁed form that
generalizes the upper triangular form we aimed for in the nonsingular situation.
Deﬁnition 1.38. An m × n matrix U is said to be in row echelon form if it has the
following “staircase” structure:
U =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
⃝∗
∗
. . .
∗
∗
. . .
∗
∗
. . .
. . .
∗
∗
∗
. . .
∗
0
0
. . .
0
⃝∗
. . .
∗
∗
. . .
. . .
∗
∗
∗
. . .
∗
0
0
. . .
0
0
. . .
0
⃝∗
. . .
. . .
∗
∗
∗
. . .
∗
...
...
...
...
...
...
...
...
...
...
...
...
...
0
0
. . .
0
0
. . .
0
0
. . .
. . .
0
⃝∗
∗
. . .
∗
0
0
. . .
0
0
. . .
0
0
. . .
. . .
0
0
0
. . .
0
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
0
0
. . .
0
0
. . .
0
0
. . .
. . .
0
0
0
. . .
0
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
The entries indicated by ⃝∗are the pivots, and must be nonzero. The ﬁrst r rows of U each
contain exactly one pivot, but not all columns are required to include a pivot entry. The
entries below the “staircase”, indicated by the solid line, are all zero, while the non-pivot
entries above the staircase, indicated by stars, can be anything. The last m −r rows are
identically zero, and do not contain any pivots. There may, in exceptional situations, be
one or more all zero initial columns. Here is an explicit example of a matrix in row echelon
form:
⎛
⎜
⎝
3
1
0
4
5
−7
0
−1
−2
1
8
0
0
0
0
0
2
−4
0
0
0
0
0
0
⎞
⎟
⎠.
The three pivots are the ﬁrst nonzero entries in the three nonzero rows, namely, 3, −1, 2.
Slightly more generally, U may have several initial columns consisting of all zeros. An
example is the row echelon matrix
⎛
⎜
⎝
0
0
3
5
−2
0
0
0
0
0
5
3
0
0
0
0
0
−7
0
0
0
0
0
0
⎞
⎟
⎠,
which also has three pivots. The latter matrix corresponds to a linear system in which the
ﬁrst two variables do not appear in any of the equations. Thus, such row echelon forms
almost never appear in applications.

60
1 Linear Algebraic Systems
Proposition 1.39. Every matrix can be reduced to row echelon form by a sequence of
elementary row operations of types #1 and #2.
In matrix language, Proposition 1.39 implies that if A is any m × n matrix, then there
exists an m × m permutation matrix P and an m × m lower unitriangular matrix L such
that
P A = LU,
(1.73)
where U is an m × n row echelon matrix. The factorization (1.73) is not unique. Observe
that P and L are square matrices of the same size, while A and U are rectangular, also of
the same size. As with a square matrix, the entries of L below the diagonal correspond to
the row operations of type #1, while P keeps track of row interchanges. As before, one
can keep track of row interchanges with a row pointer.
A constructive proof of this result is based on the general Gaussian Elimination algo-
rithm, which proceeds as follows. Starting on the left of the matrix, one searches for the
ﬁrst column that is not identically zero. Any of the nonzero entries in that column may
serve as the pivot. Partial pivoting indicates that it is probably best to choose the largest
one, although this is not essential for the algorithm to proceed. One places the chosen
pivot in the ﬁrst row of the matrix via a row interchange, if necessary. The entries below
the pivot are made equal to zero by the appropriate elementary row operations of type #1.
One then proceeds iteratively, performing the same reduction algorithm on the submatrix
consisting of all entries strictly to the right and below the pivot. The algorithm terminates
when either there is a nonzero pivot in the last row, or all of the rows lying below the last
pivot are identically zero, and so no more pivots can be found.
Example 1.40.
The easiest way to learn the general Gaussian Elimination algorithm is
to follow through an illustrative example. Consider the linear system
x + 3y + 2z −u
= a,
2x + 6y + z + 4u + 3v = b,
−x −3y −3z + 3u + v = c,
3x + 9y + 8z −7u + 2v = d,
(1.74)
of 4 equations in 5 unknowns, where a, b, c, d are given numbers†. The coeﬃcient matrix is
A =
⎛
⎜
⎝
1
3
2
−1
0
2
6
1
4
3
−1
−3
−3
3
1
3
9
8
−7
2
⎞
⎟
⎠.
(1.75)
To solve the system, we introduce the augmented matrix
⎛
⎜
⎝
1
3
2
−1
0
2
6
1
4
3
−1
−3
−3
3
1
3
9
8
−7
2

a
b
c
d
⎞
⎟
⎠,
obtained by appending the right-hand side of the system. The upper left entry is nonzero,
and so can serve as the ﬁrst pivot. We eliminate the entries below it by elementary row
†
It will be convenient to work with the right-hand side in general form, although the reader
may prefer, at least initially, to assign numerical values to a, b, c, d.

1.8 General Linear Systems
61
operations, resulting in
⎛
⎜
⎝
1
3
2
−1
0
0
0
−3
6
3
0
0
−1
2
1
0
0
2
−4
2

a
b −2a
c + a
d −3a
⎞
⎟
⎠.
Now, the second column contains no suitable nonzero entry to serve as the second pivot.
(The top entry already lies in a row containing a pivot, and so cannot be used.) Therefore,
we move on to the third column, choosing the (2, 3) entry, −3, as our second pivot. Again,
we eliminate the entries below it, leading to
⎛
⎜
⎜
⎝
1
3
2
−1
0
0
0
−3
6
3
0
0
0
0
0
0
0
0
0
4

a
b −2a
c −1
3 b + 5
3 a
d + 2
3 b −13
3 a
⎞
⎟
⎟
⎠.
The fourth column has no pivot candidates, and so the ﬁnal pivot is the 4 in the ﬁfth
column. We interchange the last two rows in order to place the coeﬃcient matrix in row
echelon form:
⎛
⎜
⎜
⎝
1
3
2
−1
0
0
0
−3
6
3
0
0
0
0
4
0
0
0
0
0

a
b −2a
d + 2
3 b −13
3 a
c −1
3 b + 5
3 a
⎞
⎟
⎟
⎠.
(1.76)
There are three pivots, 1, −3, and 4, sitting in positions (1, 1), (2, 3), and (3, 5). Note
the staircase form, with the pivots on the steps and everything below the staircase being
zero. Recalling the row operations used to construct the solution (and keeping in mind
that the row interchange that appears at the end also aﬀects the entries of L), we ﬁnd the
factorization (1.73) takes the explicit form
⎛
⎜
⎝
1
0
0
0
0
1
0
0
0
0
0
1
0
0
1
0
⎞
⎟
⎠
⎛
⎜
⎝
1
3
2
−1
0
2
6
1
4
3
−1
−3
−3
3
1
3
9
8
−7
2
⎞
⎟
⎠=
⎛
⎜
⎜
⎝
1
0
0
0
2
1
0
0
3
−2
3
1
0
−1
1
3
0
1
⎞
⎟
⎟
⎠
⎛
⎜
⎝
1
3
2
−1
0
0
0
−3
6
3
0
0
0
0
4
0
0
0
0
0
⎞
⎟
⎠.
We shall return to ﬁnd the solution to our linear system after a brief theoretical interlude.
Warning. In the augmented matrix, pivots can never appear in the last column, repre-
senting the right-hand side of the system. Thus, even if c −1
3 b + 5
3 a ̸= 0, that entry does
not qualify as a pivot.
We now introduce the most important numerical quantity associated with a matrix.
Deﬁnition 1.41. The rank of a matrix is the number of pivots.
For instance, the rank of the matrix (1.75) equals 3, since its reduced row echelon form,
i.e., the ﬁrst ﬁve columns of (1.76), has three pivots. Since there is at most one pivot per
row and one pivot per column, the rank of an m × n matrix is bounded by both m and n,
and so
0 ≤r = rank A ≤min{m, n}.
(1.77)
The only m × n matrix of rank 0 is the zero matrix O — which is the only matrix without
any pivots.

62
1 Linear Algebraic Systems
Proposition 1.42. A square matrix of size n × n is nonsingular if and only if its rank is
equal to n.
Indeed, the only way an n × n matrix can end up having n pivots is if its reduced row
echelon form is upper triangular with nonzero diagonal entries. But a matrix that reduces
to such triangular form is, by deﬁnition, nonsingular.
Interestingly, the rank of a matrix does not depend on which elementary row operations
are performed along the way to row echelon form. Indeed, performing a diﬀerent sequence of
row operations — say using Partial Pivoting versus no pivoting — can produce a completely
diﬀerent reduced form. The remarkable result is that all such row echelon forms end up
having exactly the same number of pivots, and this number is the rank of the matrix. A
formal proof of this fact will appear in Chapter 2; see Theorem 2.49.
Once the coeﬃcient matrix has been reduced to row echelon form

U | c

, the solution
to the equivalent linear system U x = c proceeds as follows. The ﬁrst step is to see whether
there are any equations that do not have a solution.
Suppose one of the rows in the
echelon form U is identically zero, but the corresponding entry in the last column c of
the augmented matrix is nonzero. What linear equation would this represent? Well, the
coeﬃcients of all the variables are zero, and so the equation is of the form
0 = ci,
(1.78)
where i is the row’s index.
If ci ̸= 0, then the equation cannot be satisﬁed — it is
inconsistent. The reduced system does not have a solution. Since the reduced system was
obtained by elementary row operations, the original linear system is incompatible, meaning
it also has no solutions. Note: It takes only one inconsistency to render the entire system
incompatible. On the other hand, if ci = 0, so the entire row in the augmented matrix is
zero, then (1.78) is merely 0 = 0, and is trivially satisﬁed. Such all-zero rows do not aﬀect
the solvability of the system.
In our example, the last row in the echelon form (1.76) is all zero, and hence the
last entry in the ﬁnal column must also vanish in order that the system be compatible.
Therefore, the linear system (1.74) will have a solution if and only if the right-hand sides
a, b, c, d satisfy the linear constraint
5
3 a −1
3 b + c = 0.
(1.79)
In general, if the system is incompatible, there is nothing else to do. Otherwise, every
all zero row in the row echelon form of the coeﬃcient matrix also has a zero entry in the
last column of the augmented matrix; the system is compatible and admits one or more
solutions. (If there are no all-zero rows in the coeﬃcient matrix, meaning that every row
contains a pivot, then the system is automatically compatible.) To ﬁnd the solution(s), we
split the variables in the system into two classes.
Deﬁnition 1.43. In a linear system U x = c in row echelon form, the variables cor-
responding to columns containing a pivot are called basic variables, while the variables
corresponding to the columns without a pivot are called free variables.
The solution to the system then proceeds by an adaptation of the Back Substitution
procedure. Working in reverse order, each nonzero equation is solved for the basic variable
associated with its pivot. The result is substituted into the preceding equations before
they in turn are solved.
The solution then speciﬁes all the basic variables as certain
combinations of the remaining free variables. As their name indicates, the free variables, if

1.8 General Linear Systems
63
any, are allowed to take on any values whatsoever, and so serve to parameterize the general
solution to the system.
Example 1.44.
Let us illustrate the solution procedure with our particular system
(1.74). The values a = 0, b = 3, c = 1, d = 1, satisfy the consistency constraint (1.79), and
the corresponding reduced augmented matrix (1.76) is
⎛
⎜
⎝
1
3
2
−1
0
0
0
−3
6
3
0
0
0
0
4
0
0
0
0
0

0
3
3
0
⎞
⎟
⎠.
The pivots are found in columns 1, 3, 5, and so the corresponding variables, x, z, v, are
basic; the other variables, y, u, corresponding to the non-pivot columns 2, 4, are free. Our
task is to solve the reduced system
x + 3y + 2z −u
= 0,
−3z + 6u + 3v = 3,
4v = 3,
0 = 0,
for the basic variables x, z, v in terms of the free variables y, u. As before, this is done in the
reverse order, by successively substituting the resulting values in the preceding equation.
The result is the general solution
v = 3
4,
z = −1 + 2u + v = −1
4 + 2u,
x = −3y −2z + u = 1
2 −3y −3u.
The free variables y, u remain completely arbitrary; any assigned values will produce a
solution to the original system.
For instance, if y = −1, u = π, then x =
7
2 −3π,
z = −1
4 + 2π, v = 3
4. But keep in mind that this is merely one of an inﬁnite number
of valid solutions.
In general, if the m×n coeﬃcient matrix of a system of m linear equations in n unknowns
has rank r, there are m−r all-zero rows in the row echelon form, and these m−r equations
must have zero right-hand side in order that the system be compatible and have a solution.
Moreover, there is a total ofr basic variables and n −r free variables, and so the general
solution depends upon n −r parameters.
Summarizing the preceding discussion, we have learned that there are only three possible
outcomes for the solution to a system of linear equations.
Theorem 1.45. A system A x = b of m linear equations in n unknowns has either
(i) exactly one solution, (ii) inﬁnitely many solutions, or (iii) no solution.
Case (iii) occurs if the system is incompatible, producing a zero row in the echelon form
that has a nonzero right-hand side. Case (ii) occurs if the system is compatible and there
are one or more free variables, and so the rank of the coeﬃcient matrix is strictly less than
the number of columns: r < n. Case (i) occurs for nonsingular square coeﬃcient matrices,
and, more generally, for compatible systems for which r = n, implying there are no free
variables. Since r ≤m, this case can arise only if the coeﬃcient matrix has at least as
many rows as columns, i.e., the linear system has at least as many equations as unknowns.
A linear system can never have a ﬁnite number — other than 0 or 1 — of solutions. As
a consequence, any linear system that admits two or more solutions automatically has
inﬁnitely many!

64
1 Linear Algebraic Systems
No Solution
Unique Solution
Inﬁnitely Many Solutions
Figure 1.1.
Intersecting Planes.
Warning. This property requires linearity, and is not valid for nonlinear systems. For
instance, the real quadratic equation x2 + x −2 = 0 has exactly two real solutions: x = 1
and x = −2.
Example 1.46.
Consider the linear system
y + 4z = a,
3x −y + 2z = b,
x + y + 6z = c,
consisting of three equations in three unknowns. The augmented coeﬃcient matrix is
⎛
⎝
0
1
4
3
−1
2
1
1
6

a
b
c
⎞
⎠.
Interchanging the ﬁrst two rows, and then eliminating the elements below the ﬁrst pivot
leads to
⎛
⎝
3
−1
2
0
1
4
0
4
3
16
3

b
a
c −1
3 b
⎞
⎠.
The second pivot is in the (2, 2) position, but after eliminating the entry below it, we ﬁnd
the row echelon form to be
⎛
⎝
3
−1
2
0
1
4
0
0
0

b
a
c −1
3 b −4
3 a
⎞
⎠.
Since there is a row of all zeros, the original coeﬃcient matrix is singular, and its rank is
only 2.
The consistency condition follows from this last row in the reduced echelon form, which
requires
4
3 a + 1
3 b −c = 0.
If this is not satisﬁed, the system has no solutions; otherwise, it has inﬁnitely many. The
free variable is z, since there is no pivot in the third column. The general solution is
y = a −4z,
x = 1
3 b + 1
3 y −2
3 z = 1
3 a + 1
3 b −2z,
where z is arbitrary.

1.8 General Linear Systems
65
Geometrically, Theorem 1.45 is telling us about the possible conﬁgurations of linear
subsets (lines, planes, etc.) of an n-dimensional space. For example, a single linear equation
ax + by + cz = d, with (a, b, c) ̸= 0, deﬁnes a plane P in three-dimensional space. The
solutions to a system of three linear equations in three unknowns belong to all three planes;
that is, they lie in their intersection P1 ∩P2 ∩P3. Generically, three planes intersect in
a single common point; this is case (i) of the theorem, which occurs if and only if the
coeﬃcient matrix is nonsingular. The case of inﬁnitely many solutions occurs when the
three planes intersect in a common line, or, even more degenerately, when they all coincide.
On the other hand, parallel planes, or planes intersecting in parallel lines, have no common
point of intersection, and this occurs when the system is incompatible and has no solutions.
There are no other possibilities: the total number of points in the intersection is either 0,
1, or ∞. Some sample geometric conﬁgurations appear in Figure 1.1.
Exercises
1.8.1. Which of the following systems has (i) a unique solution?
(ii) inﬁnitely many
solutions? (iii) no solution? In each case, ﬁnd all solutions:
(a)
x −2y = 1,
3x + 2y = −3.
(b) 2x + y + 3z = 1,
x + 4y −2z = −3.
(c)
x + y −2z = −3,
2x −y + 3z = 7,
x −2y + 5z = 1.
(d)
x −2y + z = 6,
2x + y −3z = −3,
x −3y + 3z = 10.
(e)
x −2y + 2z −w = 3,
3x + y + 6z + 11w = 16,
2x −y + 4z + w = 9.
(f )
3x −2y + z = 4,
x + 3y −4z = −3,
2x −3y + 5z = 7,
x −8y + 9z = 10.
(g)
x + 2y + 17z −5w = 50,
9x −16y + 10z −8w = 24,
2x −5y −4z = −13,
6x −12y + z −4w = −1.
1.8.2. Determine if the following systems are compatible and, if so, ﬁnd the general solution:
(a)
6x1 + 3x2 = 12,
4x1 + 2x2 = 9.
(b)
8x1 + 12x2 = 16,
6x1 + 9x2 = 13. (c)
x1 + 2x2 = 1,
2x1 + 5x2 = 2,
3x1 + 6x2 = 3.
(d)
2x1 −6x2 + 4x3 = 2,
−x1 + 3x2 −2x3 = −1.
(e)
2x1 + 2x2 + 3x3 = 1,
x2 + 2x3 = 3,
4x1 + 5x2 + 7x3 = 15.
(f )
x1 + x2 + x3 + 9x4 = 8,
x2 + 2x3 + 8x4 = 7,
−3x1 + x3 −7x4 = 9.
(g)
x1 + 2x2 + 3x3 + 4x4 = 1,
2x1 + 4x2 + 6x3 + 5x4 = 0,
3x1 + 4x2 + x3 + x4 = 0,
4x1 + 6x2 + 4x3 −x4 = 0.
1.8.3. Graph the following planes and determine whether they have a common intersection:
x + y + z = 1,
x + y = 1,
x + z = 1.
1.8.4. Let A =
⎛
⎜
⎝
a
0
b
a
2
a
b
2
a







2
b
a
⎞
⎟
⎠be the augmented matrix for a linear system. For which
values of a and b does the system have (i) a unique solution? (ii) inﬁnitely many
solutions? (iii) no solution?
1.8.5. Determine the general (complex) solution to the following systems:
(a) 2x + (1 + i )y −2 i z = 2 i ,
(1 −i )x + y −2 i z = 0.
(b)
x + 2 i y + (2 −4 i )z = 5 + 5 i ,
(−1 + i )x + 2y + (4 + 2 i )z = 0,
(1 −i )x + (1 + 4 i )y −5 i z = 10 + 5 i .

66
1 Linear Algebraic Systems
(c)
x1 + i x2 + x3 = 1 + 4 i ,
−x1 + x2 −i x3 = −1,
i x1 −x2 −x3 = −1 −2 i .
(d)
(2 + i )x + i y + (2 + 2 i )z + (1 + 12 i )w = 0,
(1 −i )x + y + (2 −i )z + (8 + 2 i )w = 0,
(3 + 2 i )x + i y + (3 + 3 i )z + 19 i w = 0.
1.8.6. For which values of b and c does the system x1 + x2 + bx3 = 1,
bx1 + 3x2 −x3 = −2,
3x1 + 4x2 + x3 = c, have (a) no solution?
(b) exactly one solution?
(c) inﬁnitely many
solutions?
1.8.7. Determine the rank of the following matrices:
(a)

1
1
1
−2
	
,
(b)

2
1
3
−2
−1
−3
	
,
(c)
⎛
⎜
⎝
1
−1
1
1
−1
2
−1
1
0
⎞
⎟
⎠,
(d)
⎛
⎜
⎝
2
−1
0
2
−1
1
1
1
−1
⎞
⎟
⎠,
(e)
⎛
⎜
⎝
3
0
−2
⎞
⎟
⎠,
(f ) ( 0
−1
2
5 ),
(g)
⎛
⎜
⎜
⎜
⎝
0
−3
4
−1
1
2
−1
−5
⎞
⎟
⎟
⎟
⎠,
(h)
⎛
⎜
⎜
⎜
⎜
⎜
⎝
1
−1
2
1
2
1
−1
0
1
2
−3
−1
4
−1
3
2
0
3
−5
−2
⎞
⎟
⎟
⎟
⎟
⎟
⎠
,
(i)
⎛
⎜
⎝
0
0
0
3
1
1
2
−3
1
−2
2
4
−2
1
−2
⎞
⎟
⎠.
1.8.8. Write out a P A = LU factorization for each of the matrices in Exercise 1.8.7.
1.8.9. Construct a system of three linear equations in three unknowns that has
(a) one and only one solution; (b) more than one solution; (c) no solution.
1.8.10. Find a coeﬃcient matrix A such that the associated linear system A x = b has
(a) inﬁnitely many solutions for every b; (b) 0 or ∞solutions, depending on b;
(c) 0 or 1 solution depending on b; (d) exactly 1 solution for all b.
1.8.11. Give an example of a nonlinear system of two equations in two unknowns that has
(a) no solution; (b) exactly two solutions; (c) exactly three solutions; (d) inﬁnitely
many solutions.
1.8.12. What does it mean if a linear system has a coeﬃcient matrix with a column of all 0’s?
1.8.13. True or false: One can ﬁnd an m × n matrix of rank r for every 0 ≤r ≤min {m, n}.
1.8.14. True or false: Every m × n matrix has (a) exactly m pivots; (b) at least one pivot.
♥1.8.15.(a) Prove that the product A = v wT of a nonzero m × 1 column vector v and a nonzero
1 × n row vector wT is an m × n matrix of rank r = 1. (b) Compute the following rank one
products: (i)

1
3
	
( −1
2 ),
(ii)
⎛
⎜
⎝
4
0
−2
⎞
⎟
⎠( −2
1 ),
(iii)

2
−3
	
( 1
3
−1 ).
(c) Prove that every rank one matrix can be written in the form A = v wT .
♦1.8.16.(a) Let A be an m × n matrix and let M =

A | b

be the augmented matrix for the
linear system A x = b. Show that either (i) rank A = rank M, or (ii) rank A = rank M −1.
(b) Prove that the system is compatible if and only if case (i) holds.
1.8.17. Find the rank of the matrix
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎝
a
ar
. . .
arn−1
arn
arn+1
. . .
ar2n−1
...
...
...
...
ar(n−1)n
ar(n−1)n+1
. . .
arn2−1
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎠
when a, r ̸= 0.
1.8.18. Find the rank of the n × n matrix
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
1
2
3
. . .
n
n + 1
n + 2
n + 3
. . .
2n
2n + 1
2n + 2
2n + 3
. . .
3n
...
...
...
...
...
n2 −n + 1
n2 −n + 2
. . .
. . .
n2
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
.

1.8 General Linear Systems
67
1.8.19. Find two matrices A, B such that rank AB ̸= rank B A.
♦1.8.20. Let A be an m×n matrix of rank r. (a) Suppose C = (A B) is an m×k matrix, k > n,
whose ﬁrst n columns are the same as the columns of A. Prove that rank C ≥rank A. Give
an example with rank C = rank A; with rank C > rank A. (b) Let E =

A
D
	
be a j × n
matrix, j > m, whose ﬁrst m rows are the same as those of A. Prove that rank E ≥rank A.
Give an example with rank E = rank A; with rank E > rank A.
♦1.8.21. Let A be a singular square matrix. Prove that there exist elementary matrices E1, . . . , EN
such that A = E1 E2 · · · EN Z, where Z is a matrix with at least one all-zero row.
Homogeneous Systems
A linear system with all 0’s on the right-hand side is called homogeneous. Conversely, if
at least one of the right-hand sides is nonzero, the system is called inhomogeneous.
In matrix notation, a homogeneous system takes the form
A x = 0,
(1.80)
where the zero vector 0 indicates that every entry on the right-hand side is zero. Homo-
geneous systems are always compatible, since x = 0 is a solution, known as the trivial
solution. If a homogeneous system has a nontrivial solution x ̸= 0, then Theorem 1.45
assures us that it must have inﬁnitely many solutions. This will occur if and only if the
reduced system has one or more free variables.
Theorem 1.47. A homogeneous linear system A x = 0 of m equations in n unknowns
has a nontrivial solution x ̸= 0 if and only if the rank of A is r < n. If m < n, the system
always has a nontrivial solution. If m = n, the system has a nontrivial solution if and only
if A is singular.
Thus, homogeneous systems with fewer equations than unknowns always have inﬁnitely
many solutions. Indeed, the coeﬃcient matrix of such a system has more columns than
rows, and so at least one column cannot contain a pivot, meaning that there is at least one
free variable in the general solution formula.
Example 1.48.
Consider the homogeneous linear system
2x1 + x2 + 5x4 = 0,
4x1 + 2x2 −x3 + 8x4 = 0,
−2x1 −x2 + 3x3 −4x4 = 0,
with coeﬃcient matrix
A =
⎛
⎝
2
1
0
5
4
2
−1
8
−2
−1
3
−4
⎞
⎠.
Since there are only three equations in four unknowns, we already know that the system
has inﬁnitely many solutions, including the trivial solution x1 = x2 = x3 = x4 = 0.
When solving a homogeneous system, the ﬁnal column of the augmented matrix consists
of all zeros. As such, it will never be altered by row operations, and so it is a waste of
eﬀort to carry it along during the process. We therefore perform the Gaussian Elimination

68
1 Linear Algebraic Systems
algorithm directly on the coeﬃcient matrix A. Working with the (1, 1) entry as the ﬁrst
pivot, we ﬁrst obtain
⎛
⎝
2
1
0
5
0
0
−1
−2
0
0
3
1
⎞
⎠.
The (2, 3) entry is the second pivot, and we apply one ﬁnal row operation to place the
matrix in row echelon form
⎛
⎝
2
1
0
5
0
0
−1
−2
0
0
0
−5
⎞
⎠.
This corresponds to the reduced homogeneous system
2x1 + x2 + 5x4 = 0,
−x3 −2x4 = 0,
−5x4 = 0.
Since there are three pivots in the ﬁnal row echelon form, the rank of the coeﬃcient matrix
A is 3. There is one free variable, namely x2. Using Back Substitution, we easily obtain
the general solution
x1 = −1
2 t,
x2 = t,
x3 = x4 = 0,
which depends upon a single free parameter t = x2.
Example 1.49.
Consider the homogeneous linear system
2x −y + 3z = 0,
−4x + 2y −6z = 0,
2x −y + z = 0,
6x −3y + 3z = 0,
with coeﬃcient matrix
A =
⎛
⎜
⎝
2
−1
3
−4
2
−6
2
−1
1
6
−3
3
⎞
⎟
⎠.
The system admits the trivial solution x = y = z = 0, but in this case we need to complete
the elimination algorithm before we know for sure whether there are other solutions. After
the ﬁrst stage in the reduction process, the coeﬃcient matrix becomes
⎛
⎜
⎝
2
−1
3
0
0
0
0
0
−2
0
0
−6
⎞
⎟
⎠.
To continue, we need to interchange the second and third rows to place a nonzero entry in
the ﬁnal pivot position; after that, the reduction to the row echelon form
⎛
⎜
⎝
2
−1
3
0
0
−2
0
0
0
0
0
0
⎞
⎟
⎠
is immediate. Thus, the system reduces to the equations
2x −y + 3z = 0,
−2z = 0,
0 = 0,
0 = 0.
The third and fourth equations are trivially compatible, as they must be in the homo-
geneous case. The rank of the coeﬃcient matrix is equal to two, which is less than the
number of columns, and so, even though the system has more equations than unknowns,
it has inﬁnitely many solutions. These can be written in terms of the free variable y; the
general solution is x = 1
2 y, z = 0, where y is arbitrary.

1.9 Determinants
69
Exercises
1.8.22. Solve the following homogeneous linear systems.
(a)
x + y −2z = 0,
−x + 4y −3z = 0.
(b)
2x + 3y −z = 0,
−4x + 3y −5z = 0,
x −3y + 3z = 0.
(c)
−x + y −4z = 0,
−2x + 2y −6z = 0,
x + 3y + 3z = 0.
(d) x + 2y −2z + w = 0,
−3x + z −2w = 0.
(e)
−x + 3y −2z + w = 0,
−2x + 5y + z −2w = 0,
3x −8y + z −4w = 0.
(f )
−y + z = 0,
2x −3w = 0,
x + y −2w = 0,
y −3z + w = 0.
1.8.23. Find all solutions to the homogeneous system A x = 0 for the coeﬃcient matrix
(a)

3
−1
−9
3
	
,
(b)

2
−1
4
3
1
2
	
,
(c)

1
−2
3
−3
2
1
4
0
	
,
(d)
⎛
⎜
⎝
1
2
3
4
5
6
7
8
9
⎞
⎟
⎠,
(e)
⎛
⎜
⎝
0
2
−1
−2
0
3
1
3
0
⎞
⎟
⎠,
(f )
⎛
⎜
⎜
⎜
⎝
1
−2
1
−1
2
−1
1
0
⎞
⎟
⎟
⎟
⎠,
(g)
⎛
⎜
⎜
⎜
⎝
1
2
0
−1
−3
2
4
7
2
−1
1
6
⎞
⎟
⎟
⎟
⎠,
(h)
⎛
⎜
⎜
⎜
⎝
0
0
3
−3
1
−1
0
3
2
−2
1
5
−1
1
1
−4
⎞
⎟
⎟
⎟
⎠.
1.8.24. Let U be an upper triangular matrix. Show that the homogeneous system U x = 0
admits a nontrivial solution if and only if U has at least one 0 on its diagonal.
1.8.25. Find the solution to the homogeneous system 2x1 + x2 −2x3 = 0, 2x1 −x2 −2x3 = 0.
Then solve the inhomogeneous version where the right-hand sides are changed to a, b,
respectively. What do you observe?
1.8.26. Answer Exercise 1.8.25 for the system 2x1 +x2 + x3 −x4 = 0, 2x1 −2x2 −x3 +3x4 = 0.
1.8.27. Find all values of k for which the following homogeneous systems of linear equations
have a non-trivial solution:
(a)
x + ky = 0,
kx + 4y = 0,
(b)
x1 + kx2 + 4x3 = 0,
kx1 + x2 + 2x3 = 0,
2x1 + kx2 + 8x3 = 0.
(c)
x + ky + 2z = 0,
3x −ky −2z = 0,
(k + 1)x −2y −4z = 0,
kx + 3y + 6z = 0.
1.9 Determinants
You may be surprised that, so far, we have not mentioned determinants — a topic that
typically assumes a central role in many treatments of basic linear algebra. Determinants
can be useful in low-dimensional and highly structured problems, and have many fascinat-
ing properties. They also prominently feature in theoretical developments of the subject.
But, like matrix inverses, they are almost completely irrelevant when it comes to large
scale applications and practical computations. Indeed, for most matrices, the best way to
compute a determinant is (surprise) Gaussian Elimination! Consequently, from a computa-
tional standpoint, the determinant adds no new information concerning the linear system
and its solutions. However, for completeness and in preparation for certain later develop-
ments (particularly computing eigenvalues of small matrices), you should be familiar with
the basic facts and properties of determinants, as summarized in this ﬁnal section.

70
1 Linear Algebraic Systems
The determinant of a square matrix† A is a scalar, written det A, that will distinguish
between singular and nonsingular matrices. We already encountered in (1.38) the determi-
nant of a 2 × 2 matrix‡: det

a
b
c
d

= ad −bc. The key fact is that the determinant is
nonzero if and only if the matrix has an inverse, or, equivalently, is nonsingular. Our goal
is to ﬁnd an analogous quantity for general square matrices.
There are many diﬀerent ways to deﬁne determinants. The diﬃculty is that the ac-
tual formula is very unwieldy — see (1.87) below — and not well motivated. We prefer
an axiomatic approach that explains how our three elementary row operations aﬀect the
determinant.
Theorem 1.50.
Associated with every square matrix, there exists a uniquely deﬁned
scalar quantity, known as its determinant, that obeys the following axioms:
(i) Adding a multiple of one row to another does not change the determinant.
(ii) Interchanging two rows changes the sign of the determinant.
(iii) Multiplying a row by any scalar (including zero) multiplies the determinant by the
same scalar.
(iv) The determinant of an upper triangular matrix U is equal to the product of its
diagonal entries: det U = u11u22 · · · unn.
In particular, axiom (iv) implies that the determinant of the identity matrix is
det I = 1.
(1.81)
Checking that all four of these axioms hold in the 2 × 2 case is an elementary exercise.
The proof of Theorem 1.50 is based on the following results. Suppose, in particular, we
multiply a row of the matrix A by the zero scalar. The resulting matrix has a row of all
zeros, and, by axiom (iii), has zero determinant. Since any matrix with a zero row can be
obtained in this fashion, we conclude:
Lemma 1.51. Any matrix with one or more all-zero rows has zero determinant.
Using these properties, one is able to compute the determinant of any square matrix
by Gaussian Elimination, which is, in fact, the fastest and most practical computational
method in all but the simplest situations.
Theorem 1.52. If A = LU is a regular matrix, then
det A = det U = u11u22 · · · unn
(1.82)
equals the product of the pivots. More generally, if A is nonsingular, and requires k row
interchanges to arrive at its permuted factorization P A = LU, then
det A = det P det U = (−1)k u11u22 · · · unn.
(1.83)
Finally, A is singular if and only if
det A = 0.
(1.84)
†
Non-square matrices do not have determinants.
‡
Some authors use vertical lines to indicate the determinant:





a
b
c
d





 = det

a
b
c
d
	
.

1.9 Determinants
71
Proof : In the regular case, we need only elementary row operations of type #1 to reduce
A to upper triangular form U, and axiom (i) says these do not change the determinant.
Therefore, det A = det U, the formula for the latter being given by axiom (iv).
The
nonsingular case follows in a similar fashion. By axiom (ii), each row interchange changes
the sign of the determinant, and so det A equals det U if there has been an even number of
interchanges, but equals −det U if there has been an odd number. For the same reason,
the determinant of the permutation matrix P equals +1 if there has been an even number
of row interchanges, and −1 for an odd number. Finally, if A is singular, then we can
reduce it to a matrix with at least one row of zeros by elementary row operations of types
#1 and #2. Lemma 1.51 implies that the resulting matrix has zero determinant, and so
det A = 0, also.
Q.E.D.
Remark.
If we then apply Gauss–Jordan elimination to reduce the upper triangular
matrix U to the identity matrix I , and use axiom (ii) when each row is divided by its
pivot, we ﬁnd that axiom (iv) follows from the simpler formula (1.81), which could thus
replace it in Theorem 1.50.
Example 1.53.
Let us compute the determinant of the 4 × 4 matrix
A =
⎛
⎜
⎝
1
0
−1
2
2
1
−3
4
0
2
−2
3
1
1
−4
−2
⎞
⎟
⎠.
We perform our usual Gaussian Elimination algorithm, successively leading to the matrices
A −→
⎛
⎜
⎝
1
0
−1
2
0
1
−1
0
0
2
−2
3
0
1
−3
−4
⎞
⎟
⎠−→
⎛
⎜
⎝
1
0
−1
2
0
1
−1
0
0
0
0
3
0
0
−2
−4
⎞
⎟
⎠−→
⎛
⎜
⎝
1
0
−1
2
0
1
−1
0
0
0
−2
−4
0
0
0
3
⎞
⎟
⎠,
where we used a single row interchange to obtain the ﬁnal upper triangular form. Owing
to the row interchange, the determinant of the original matrix is −1 times the product of
the pivots:
det A = −1 · (1 · 1 · (−2) · 3) = 6.
In particular, this tells us that A is nonsingular. But, of course, this was already evident,
since we successfully reduced the matrix to upper triangular form with 4 nonzero pivots.
There is a variety of other approaches to evaluating determinants. However, except
for very small (2 × 2 or 3 × 3) matrices or other special situations, the most eﬃcient
algorithm for computing the determinant of a matrix is to apply Gaussian Elimination,
with pivoting if necessary, and then invoke the relevant formula from Theorem 1.52. In
particular, the determinantal criterion (1.84) for singular matrices, while of theoretical
interest, is unnecessary in practice, since we will have already detected whether the matrix
is singular during the course of the elimination procedure by observing that it has fewer
than the full number of pivots.
Let us ﬁnish by stating a few of the basic properties of determinants. Proofs are outlined
in the exercises.

72
1 Linear Algebraic Systems
Proposition 1.54. The determinant of the product of two square matrices of the same
size is the product of their determinants:
det(AB) = det A det B.
(1.85)
Therefore, even though matrix multiplication is not commutative, and so AB ̸= B A in
general, both matrix products have the same determinant:
det(AB) = det A det B = det B det A = det(B A),
because ordinary (scalar) multiplication is commutative. In particular, setting B = A−1
and using axiom (iv), we ﬁnd that the determinant of the inverse matrix is the reciprocal
of the matrix’s determinant.
Proposition 1.55. If A is a nonsingular matrix, then
det A−1 =
1
det A.
(1.86)
Finally, for later reference, we end with the general formula for the determinant of an
n × n matrix A with entries aij:
det A =

π
(sign π) aπ(1),1 aπ(2),2 · · · aπ(n),n.
(1.87)
The sum is over all possible permutations π of the rows of A. The sign of the permutation,
written sign π, equals the determinant of the corresponding permutation matrix P, so
sign π = det P = +1 if the permutation is composed of an even number of row interchanges
and −1 if composed of an odd number. For example, the six terms in the well-known
formula
det
⎛
⎝
a11
a12
a13
a21
a22
a23
a31
a32
a33
⎞
⎠=
a11 a22 a33 + a31 a12 a23 + a21a 32 a13 −
−a11 a32 a23 −a21 a12 a33 −a31 a22 a13
(1.88)
for a 3 × 3 determinant correspond to the six possible permutations (1.31) of a 3-rowed
matrix. A proof that the formula (1.87) satisﬁes the deﬁning properties of the determinant
listed in Theorem 1.50 is tedious, but not hard. The reader might wish to try out the 3×3
case to be convinced that it works.
The explicit formula (1.87) proves that the determinant function is well-deﬁned, and
formally completes the proof of Theorem 1.50. One consequence of this formula is that the
determinant is unaﬀected by the transpose operation.
Proposition 1.56. Transposing a matrix does not change its determinant:
det AT = det A.
(1.89)
Remark.
Proposition 1.56 has the interesting consequence that one can equally well
use “elementary column operations” to compute determinants. We will not develop this
approach in any detail here, since it does not help us to solve linear equations.
However, the explicit determinant formula (1.87) is not used in practice. Since there are
n! diﬀerent permutations of the n rows, the determinantal sum (1.87) contains n! distinct
terms, which, as soon as n is of moderate size, renders it completely useless for practical
computations. For instance, the determinant of a 10 × 10 matrix contains 10! = 3,628,800

1.9 Determinants
73
terms, while a 100 × 100 determinant would require summing 9.3326 × 10157 terms, each of
which is a product of 100 matrix entries! The most eﬃcient way to compute determinants
is still our mainstay — Gaussian Elimination, coupled with the fact that the determinant
is ± the product of the pivots! On this note, we conclude our brief introduction.
Exercises
1.9.1. Use Gaussian Elimination to ﬁnd the determinant of the following matrices:
(a)

2
−1
−4
3
	
, (b)
⎛
⎜
⎝
0
1
−2
−1
0
3
2
−3
0
⎞
⎟
⎠, (c)
⎛
⎜
⎝
1
2
3
2
5
8
3
8
10
⎞
⎟
⎠, (d)
⎛
⎜
⎝
0
1
−1
−2
1
3
2
7
−8
⎞
⎟
⎠,
(e)
⎛
⎜
⎜
⎜
⎝
5
−1
0
2
0
3
−1
5
0
0
−4
2
0
0
0
3
⎞
⎟
⎟
⎟
⎠, (f )
⎛
⎜
⎜
⎜
⎝
1
−2
1
4
2
−4
0
0
3
−4
2
5
0
2
−4
−9
⎞
⎟
⎟
⎟
⎠, (g)
⎛
⎜
⎜
⎜
⎜
⎜
⎝
1
−2
1
4
−5
1
1
−2
3
−3
2
−1
−1
2
2
5
−1
0
5
5
2
2
0
4
−1
⎞
⎟
⎟
⎟
⎟
⎟
⎠
.
1.9.2. Verify the determinant product formula (1.85) when
A =
⎛
⎜
⎝
1
−1
3
2
−1
1
4
−2
0
⎞
⎟
⎠,
B =
⎛
⎜
⎝
0
1
−1
1
−3
−2
2
0
1
⎞
⎟
⎠.
1.9.3.(a) Give an example of a non-diagonal 2 × 2 matrix for which A2 = I . (b) In general, if
A2 = I , show that det A = ±1. (c) If A2 = A, what can you say about det A?
1.9.4. True or false: If true, explain why. If false, give an explicit counterexample.
(a) If det A ̸= 0 then A−1 exists. (b) det(2A) = 2 det A. (c) det(A + B) = det A + det B.
(d) det A−T =
1
det A. (e) det(AB−1) = det A
det B .
(f ) det[(A + B)(A −B)] = det(A2 −B2).
(g) If A is an n × n matrix with det A = 0, then rank A < n.
(h) If det A = 1 and AB = O, then B = O.
1.9.5. Prove that the similar matrices B = S−1 AS have the same determinant: det A = det B.
1.9.6. Prove that if A is a n × n matrix and c is a scalar, then det(c A) = cn det A.
1.9.7. Prove that the determinant of a lower triangular matrix is the product of its diagonal
entries.
1.9.8.(a) Show that if A has size n × n, then det(−A) = (−1)n det A. (b) Prove that, for
n odd, any n × n skew-symmetric matrix A = −AT is singular. (c) Find a nonsingular
skew-symmetric matrix.
♦1.9.9. Prove directly that the 2 × 2 determinant formula (1.38) satisﬁes the four determinant
axioms listed in Theorem 1.50.
♦1.9.10. In this exercise, we prove the determinantal product formula (1.85). (a) Prove that
if E is any elementary matrix (of the appropriate size), then det(E B) = det E det B.
(b) Use induction to prove that if A = E1 E2 · · · EN is a product of elementary matrices,
then det(AB) = det A det B. Explain why this proves the product formula whenever A is
a nonsingular matrix. (c) Prove that if Z is a matrix with a zero row, then Z B also has a
zero row, and so det(Z B) = 0 = det Z det B. (d) Use Exercise 1.8.21 to complete the proof
of the product formula.
1.9.11. Prove (1.86).
♦1.9.12. Prove (1.89). Hint: Use Exercise 1.6.30 in the regular case. Then extend to the
nonsingular case. Finally, explain why the result also holds for singular matrices.

74
1 Linear Algebraic Systems
1.9.13. Write out the formula for a 4 × 4 determinant. It should contain 24 = 4! terms.
♦1.9.14. Show that (1.87) satisﬁes all four determinant axioms, and hence is the correct formula
for a determinant.
♦1.9.15. Prove that axiom (iv) in Theorem 1.50 can be proved as a consequence of the ﬁrst
three axioms and the property det I = 1.
♦1.9.16. Prove that one cannot produce an elementary row operation of type #2 by a
combination of elementary row operations of type #1.
♥1.9.17. Show that (a) if A =

a
b
c
d
	
is regular, then its pivots are a and det A
a
;
(b) if A =
⎛
⎜
⎝
a
b
e
c
d
f
g
h
j
⎞
⎟
⎠is regular, then its pivots are a, ad −bc
a
, and
det A
ad −bc.
(c) Can you generalize this observation to regular n × n matrices?
♥1.9.18. In this exercise, we justify the use of “elementary column operations” to compute
determinants. Prove that (a) adding a scalar multiple of one column to another does not
change the determinant; (b) multiplying a column by a scalar multiplies the determinant
by the same scalar; (c) interchanging two columns changes the sign of the determinant.
(d) Explain how to use elementary column operations to reduce a matrix to lower
triangular form and thereby compute its determinant.
♦1.9.19. Find the determinant of the Vandermonde matrices listed in Exercise 1.3.24. Can you
guess the general n × n formula?
♥1.9.20. Cramer’s Rule.
(a) Show that the nonsingular system ax + by = p, cx + dy = q has
the solution given by the determinantal ratios
x = 1
Δ det

p
b
q
d
	
,
y = 1
Δ det

a
p
c
q
	
,
where
Δ = det

a
b
c
d
	
.
(1.90)
(b) Use Cramer’s Rule (1.90) to solve the systems
(i)
x + 3y = 13,
4x + 2y = 0,
(ii)
x −2y = 4,
3x + 6y = −2.
(c) Prove that the solution to
ax + by + cz = p,
dx + ey + f z = q,
gx + hy + j z = r,
with Δ = det
⎛
⎜
⎝
a
b
c
d
e
f
g
h
j
⎞
⎟
⎠̸= 0 is
x = 1
Δ det
⎛
⎜
⎝
p
b
c
q
e
f
r
h
j
⎞
⎟
⎠,
y = 1
Δ det
⎛
⎜
⎝
a
p
c
d
q
f
g
r
j
⎞
⎟
⎠,
z = 1
Δ det
⎛
⎜
⎝
a
b
p
d
e
q
g
h
r
⎞
⎟
⎠.
(1.91)
(d) Use Cramer’s Rule (1.91) to solve
(i)
x + 4y = 3,
4x + 2y + z = 2,
−x + y −z = 0,
(ii)
3x + 2y −z = 1,
x −3y + 2z = 2,
2x −y + z = 3.
(e) Can you see the pattern that will generalize to n equations in n unknowns?
Remark. Although elegant, Cramer’s rule is not a very practical solution method.
♦1.9.21.(a) Show that if D =

A
O
O
B
	
is a block diagonal matrix, where A and B are square
matrices, then det D = det A det B.
(b) Prove that the same holds for a block upper
triangular matrix det

A
C
O
B
	
= det A det B.
(c) Use this method to compute the
determinant of the following matrices:
(i)
⎛
⎜
⎝
3
2
−2
0
4
−5
0
3
7
⎞
⎟
⎠, (ii)
⎛
⎜
⎜
⎜
⎝
1
2
−2
5
−3
1
0
−5
0
0
1
3
0
0
2
−2
⎞
⎟
⎟
⎟
⎠, (iii)
⎛
⎜
⎜
⎜
⎝
1
2
0
4
−3
1
4
−1
0
3
1
8
0
0
0
−3
⎞
⎟
⎟
⎟
⎠, (iv)
⎛
⎜
⎜
⎜
⎝
5
−1
0
0
2
5
0
0
2
4
4
−2
3
−2
9
−5
⎞
⎟
⎟
⎟
⎠.

Chapter 2
Vector Spaces and Bases
Vector spaces and their ancillary structures provide the common language of linear algebra,
and, as such, are an essential prerequisite for understanding contemporary applied (and
pure) mathematics. The key concepts of vector space, subspace, linear independence, span,
and basis will appear, not only in linear systems of algebraic equations and the geometry
of n-dimensional Euclidean space, but also in the analysis of linear diﬀerential equations,
linear boundary value problems, Fourier analysis, signal processing, numerical methods,
and many, many other ﬁelds. Therefore, in order to master modern linear algebra and its
applications, the ﬁrst order of business is to acquire a ﬁrm understanding of fundamental
vector space constructions.
One of the grand themes of mathematics is the recognition that many seemingly unre-
lated entities are, in fact, diﬀerent manifestations of the same underlying abstract structure.
This serves to unify and simplify the disparate special situations, at the expense of intro-
ducing an extra level of abstraction. Indeed, the history of mathematics, as well as your
entire mathematical educational career, can be viewed as an evolution towards ever greater
abstraction resulting in ever greater power for solving problems. Here, the abstract no-
tion of a vector space serves to unify spaces of ordinary vectors, spaces of functions, such
as polynomials, exponentials, and trigonometric functions, as well as spaces of matrices,
spaces of linear operators, and so on, all in a common conceptual framework. Moreover,
proofs that might appear to be complicated in a particular context often turn out to be
relatively transparent when recast in the more inclusive vector space language. The price
that one pays for the increased level of abstraction is that, while the underlying math-
ematics is not all that complicated, novices typically take a long time to assimilate the
underlying concepts. In our opinion, the best way to approach the subject is to think in
terms of concrete examples. First, make sure you understand what is being said in the case
of ordinary Euclidean space. Once this is grasped, the next important case to consider is
an elementary function space, e.g., the space of continuous scalar functions. With the two
most important cases ﬁrmly in hand, the leap to the general abstract formulation should
not be too painful. Patience is essential; ultimately, the only way to truly understand an
abstract concept like a vector space is by working with it in real-life applications! And
always keep in mind that the eﬀort expended here will be amply rewarded later on.
Following an introduction to vector spaces and subspaces, we develop the fundamental
notions of span and linear independence, ﬁrst in the context of ordinary vectors, but then
in more generality, with an emphasis on function spaces. These are then combined into
the all-important deﬁnition of a basis of a vector space, leading to a linear algebraic char-
acterization of its dimension. Here is where the distinction between ﬁnite-dimensional and
inﬁnite-dimensional vector spaces ﬁrst becomes apparent, although the full ramiﬁcations
of this dichotomy will take time to unfold. We will then study the four fundamental sub-
spaces associated with a matrix — its image, kernel, coimage, and cokernel — and explain
how they help us understand the structure and the solutions of linear algebraic systems.
Of particular signiﬁcance is the linear superposition principle that enables us to combine
© Springer International Publishing AG, part of Springer Nature 2018 
https://doi.org/10.1007/978-3-319-91041-3_2 
75
P. J. Olver, C. Shakiban, Applied Linear Algebra, Undergraduate Texts in Mathematics, 

76
2 Vector Spaces and Bases
solutions to linear systems. Superposition is the hallmark of linearity, and will apply not
only to linear algebraic equations, but also to linear ordinary diﬀerential equations, linear
boundary value problems, linear partial diﬀerential equations, linear integral equations,
linear control systems, etc. The ﬁnal section in this chapter develops some interesting ap-
plications in graph theory that serve to illustrate the fundamental matrix subspaces; these
results will be developed further in our study of electrical circuits.
2.1 Real Vector Spaces
A vector space is the abstract reformulation of the quintessential properties of n-dimen-
sional† Euclidean space Rn, which is deﬁned as the set of all real (column) vectors with
n entries. The basic laws of vector addition and scalar multiplication in Rn serve as the
template for the following general deﬁnition.
Deﬁnition 2.1. A vector space is a set V equipped with two operations:
(i) Addition: adding any pair of vectors v, w ∈V produces another vector v + w ∈V ;
(ii) Scalar Multiplication: multiplying a vector v ∈V by a scalar c ∈R produces a
vector cv ∈V .
These are subject to the following axioms, valid for all u, v, w ∈V and all scalars c, d ∈R:
(a) Commutativity of Addition: v + w = w + v.
(b) Associativity of Addition: u + (v + w) = (u + v) + w.
(c) Additive Identity: There is a zero element 0 ∈V satisfying v + 0 = v = 0 + v.
(d) Additive Inverse: For each v ∈V there is an element −v ∈V such that
v + (−v) = 0 = (−v) + v.
(e) Distributivity: (c + d)v = (cv) + (dv), and c(v + w) = (cv) + (cw).
(f ) Associativity of Scalar Multiplication: c(dv) = (cd)v.
(g) Unit for Scalar Multiplication: the scalar 1 ∈R satisﬁes 1v = v.
Remark. For most of this text, we will deal with real vector spaces, in which the scalars
are ordinary real numbers, as indicated in the deﬁnition. Complex vector spaces, where
complex scalars are allowed, will be introduced in Section 3.6. Vector spaces over other
ﬁelds are studied in abstract algebra, [38].
In the beginning, we will refer to the individual elements of a vector space as “vectors”,
even though, as we shall see, they might also be functions, or matrices, or even more general
objects. Unless we are dealing with certain speciﬁc examples such as a space of functions
or matrices, we will use bold face, lower case Latin letters v, w, . . . to denote the elements
of our vector space. We will usually use a bold face 0 to denote the unique‡ zero element
of our vector space, while ordinary 0 denotes the real number zero.
The following identities are elementary consequences of the vector space axioms:
(h)
0v = 0;
(i)
(−1)v = −v;
(j)
c 0 = 0;
(k)
If cv = 0, then either c = 0 or v = 0.
†
The precise deﬁnition of dimension will appear later, in Theorem 2.29.
‡
See Exercise 2.1.12.

2.1 Real Vector Spaces
77
v + w
v
w
v
w
Vector Addition
v
cv
Scalar Multiplication
Figure 2.1.
Vector Space Operations in Rn.
Let us, for example, prove (h). Let z = 0v. Then, by the distributive property,
z + z = 0v + 0v = (0 + 0)v = 0v = z.
Adding −z to both sides of this equation, and making use of axioms (b), (d), and then
(c), we conclude that
z = z + 0 = z + (z + (−z)) = (z + z) + (−z) = z + (−z) = 0,
which completes the proof. Veriﬁcation of the other three properties is left as an exercise
for the reader.
Let us now introduce the most important examples of (real) vector spaces.
Example 2.2.
As noted above, the prototypical example of a real vector space is the
Euclidean space Rn, consisting of all n-tuples of real numbers v = ( v1, v2, . . . , vn )T , which
we consistently write as column vectors.
Vector addition and scalar multiplication are
deﬁned in the usual manner:
v + w =
⎛
⎜
⎜
⎝
v1 + w1
v2 + w2
...
vn + wn
⎞
⎟
⎟
⎠,
cv =
⎛
⎜
⎜
⎜
⎝
cv1
cv2
...
cvn
⎞
⎟
⎟
⎟
⎠,
whenever
v =
⎛
⎜
⎜
⎜
⎝
v1
v2
...
vn
⎞
⎟
⎟
⎟
⎠,
w =
⎛
⎜
⎜
⎜
⎝
w1
w2
...
wn
⎞
⎟
⎟
⎟
⎠,
c ∈R.
The zero vector is 0 = ( 0, . . . , 0 )T . The two vector space operations are illustrated in
Figure 2.1. The fact that vectors in Rn satisfy all of the vector space axioms is an immediate
consequence of the laws of vector addition and scalar multiplication.
Example 2.3.
Let Mm×n denote the space of all real matrices of size m × n. Then
Mm×n forms a vector space under the laws of matrix addition and scalar multiplication.
The zero element is the zero matrix O. (We are ignoring matrix multiplication, which is not
a vector space operation.) Again, the vector space axioms are immediate consequences of
the basic laws of matrix arithmetic. The preceding example of the vector space Rn = Mn×1
is a particular case in which the matrices have only one column.
Example 2.4.
Consider the space
P(n) =

p(x) = an xn + an−1 xn−1 + · · · + a1 x + a0

(2.1)

78
2 Vector Spaces and Bases
f(x) = 1
3 cos 14x
Scalar Multiplication:
2f(x) = 2
3 cos 14x
g(x) = x2
Addition:
f(x) + g(x) = 1
3 cos 14x + x2
Figure 2.2.
Vector Space Operations in Function Space.
consisting of all real polynomials of degree ≤n. Addition of polynomials is deﬁned in the
usual manner; for example,
(x2 −3x) + (2x2 −5x + 4) = 3x2 −8x + 4.
Note that the sum p(x) + q(x) of two polynomials of degree ≤n also has degree ≤n. The
zero element of P(n) is the zero polynomial. We can multiply polynomials by scalars — real
constants — in the usual fashion; for example if p(x) = x2 −2x, then 3p(x) = 3x2 −6x.
The proof that P(n) satisﬁes the vector space axioms is an easy consequence of the basic
laws of polynomial algebra.
Warning. It is not true that the sum of two polynomials of degree n also has degree n.
For example (x2 + 1) + (−x2 + x) = x + 1 has degree 1 even though the two summands
have degree 2. This means that the set of polynomials of degree = n is not a vector space.
Warning. You might be tempted to identify a scalar with a constant polynomial, but one
should really regard these as two completely diﬀerent objects — one is a number, while the
other is a constant function. To add to the confusion, one typically uses the same notation
for these two objects; for instance, 0 could mean either the real number 0 or the constant
function taking the value 0 everywhere, which is the zero element, 0, of this vector space.
The reader needs to exercise due care when interpreting each occurrence.
For much of analysis, including diﬀerential equations, Fourier theory, numerical meth-
ods, etc., the most important vector spaces consist of functions that have certain prescribed
properties. The simplest such example is the following.

2.1 Real Vector Spaces
79
Example 2.5.
Let I ⊂R be an interval†. Consider the function space F = F(I) whose
elements are all real-valued functions f(x) deﬁned for all x ∈I. The claim is that the
function space F has the structure of a vector space. Addition of functions in F is deﬁned
in the usual manner: (f +g)(x) = f(x)+g(x) for all x ∈I. Multiplication by scalars c ∈R
is the same as multiplication by constants, (c f)(x) = c f(x). The zero element is the zero
function — the constant function that is identically 0 for all x ∈I. The proof of the vector
space axioms is straightforward. Observe that we are ignoring all additional operations
that aﬀect functions such as multiplication, division, inversion, composition, etc.; these are
irrelevant as far as the vector space structure of F goes.
Example 2.6.
The preceding examples are all, in fact, special cases of an even more
general construction. A clue is to note that the last example of a function space does not
make any use of the fact that the domain of the functions is a real interval. Indeed, the
same construction produces a function space F(I) corresponding to any subset I ⊂R.
Even more generally, let S be any set. Let F = F(S) denote the space of all real-valued
functions f: S →R.
Then we claim that V is a vector space under the operations of
function addition and scalar multiplication. More precisely, given functions f and g, we
deﬁne their sum to be the function h = f + g such that h(x) = f(x) + g(x) for all x ∈S.
Similarly, given a function f and a real scalar c ∈R, we deﬁne the scalar multiple g = cf
to be the function such that g(x) = cf(x) for all x ∈S. The proof of the vector space
axioms is straightforward, and the reader should be able to ﬁll in the necessary details.
In particular, if S ⊂R is an interval, then F(S) coincides with the space of scalar
functions described in the preceding example. If S ⊂Rn is a subset of Euclidean space,
then the elements of F(S) are real-valued functions f(x1, . . . , xn) depending upon the n
variables corresponding to the coordinates of points x = (x1, . . . , xn) ∈S in the domain.
In this fashion, the set of real-valued functions deﬁned on any domain in Rn forms a vector
space.
Another useful example is to let S = {x1, . . . , xn} ⊂R be a ﬁnite set of real numbers. A
real-valued function f: S →R is deﬁned by its values f(x1), f(x2), . . . , f(xn) at the speciﬁed
points. In applications, these objects serve to indicate the sample values of a scalar function
f(x) ∈F(R) taken at the sample points x1, . . . , xn. For example, if f(x) = x2 and the
sample points are x1 = 0,
x2 = 1,
x3 = 2,
x4 = 3, then the corresponding sample
values are f(x1) = 0, f(x2) = 1, f(x3) = 4, f(x4) = 9.
When measuring a physical
quantity — velocity, temperature, pressure, etc. — one typically records only a ﬁnite set
of sample values. The intermediate, non-recorded values between the sample points are
then reconstructed through some form of interpolation, a topic that we shall visit in depth
in Chapters 4 and 5.
Interestingly, the sample values f(xi) can be identiﬁed with the entries fi of a vector
f = ( f1, f2, . . . , fn )T = ( f(x1), f(x2), . . . , f(xn) )T ∈Rn,
†
An interval is a subset I ⊂R that contains all the real numbers between a, b ∈R, where a < b,
and can be
• closed, meaning that it includes its endpoints: I = [a, b] = { x | a ≤x ≤b };
• open, which does not include either endpoint: I = (a, b) = { x | a < x < b }; or
• half-open, which includes one but not the other endpoint, so I = [a, b) = { x | a ≤x < b }
or I = (a, b] = { x | a < x ≤b }.
An open endpoint is allowed to be inﬁnite; in particular, (−∞, ∞) = R is another way of writing
the entire real line.

80
2 Vector Spaces and Bases
f(x1)
f(x2)
f(x3)
f(x4)
x1
x2
x3
x4
Figure 2.3.
Sampled Function.
known as the sample vector. Every sampled function f: S →R corresponds to a unique
vector f ∈Rn and vice versa. (But keep in mind that diﬀerent scalar functions f(x) ∈F(R)
may have the same sample values.) Addition of sample functions corresponds to addition
of their sample vectors, as does scalar multiplication. Thus, the vector space of sample
functions F(S) = F{x1, . . . , xn} is the same as the vector space Rn! The identiﬁcation
of sampled functions as vectors is of fundamental importance in modern signal processing
and data analysis, as we will see below.
Example 2.7.
The above construction admits yet a further generalization. We continue
to let S be an arbitrary set. Let V be a vector space. The claim is that the space F(S, V )
consisting of all V -valued functions f : S →V is a vector space. In other words, we replace
the particular vector space R in the preceding example by a general vector space V , and
the same conclusion holds. The operations of function addition and scalar multiplication
are deﬁned in the evident manner: (f +g)(x) = f(x)+g(x) and (c f)(x) = c f(x) for x ∈S,
where we are using the vector addition and scalar multiplication operations on V to induce
corresponding operations on V -valued functions. The proof that F(S, V ) satisﬁes all of the
vector space axioms proceeds as before.
The most important example of such a function space arises when S ⊂Rn is a do-
main in Euclidean space and V = Rm is itself a Euclidean space.
In this case, the
elements of F(S, Rm) consist of vector-valued functions f: S →Rm, so that f(x) =
( f1(x1, . . . , xn), . . . , fm(x1, . . . , xn) )T is a column vector consisting of m functions of n
variables, all deﬁned on a common domain S.
The general construction implies that
addition and scalar multiplication of vector-valued functions is done componentwise; for
example
2

x2
ex −4

−

cos x
x

=

2x2 −cos x
2ex −x −8

.
Of particular importance are the vector ﬁelds arising in physics, including gravitational
force ﬁelds, electromagnetic ﬁelds, ﬂuid velocity ﬁelds, and many others.
Exercises
2.1.1. Show that the set of complex numbers x + i y forms a real vector space under the
operations of addition (x + i y) + (u + i v) = (x + u) + i (y + v) and scalar multiplication
c(x + i y) = cx + i cy. (But complex multiplication is not a real vector space operation.)

2.2 Subspaces
81
2.1.2. Show that the positive quadrant Q = { (x, y) | x, y > 0 } ⊂R2 forms a vector space
if we deﬁne addition by (x1, y1) + (x2, y2) = (x1 x2, y1 y2) and scalar multiplication by
c (x, y) = (xc, yc).
♦2.1.3. Let S be any set. Carefully justify the validity of all the vector space axioms for the
space F(S) consisting of all real-valued functions f: S →R.
2.1.4. Let S = {0, 1, 2, 3}. (a) Find the sample vectors corresponding to the functions 1,
cos π x, cos 2π x, cos 3π x. (b) Is a function uniquely determined by its sample values?
2.1.5. Find two diﬀerent functions f(x) and g(x) that have the same sample vectors f, g at the
sample points x1 = 0, x2 = 1, x3 = −1.
2.1.6.(a) Let x1 = 0, x2 = 1. Find the unique linear function f(x) = ax + b that has the
sample vector f = ( 3, −1 )T . (b) Let x1 = 0, x2 = 1, x3 = −1. Find the unique quadratic
function f(x) = ax2 + bx + c with sample vector f = ( 1, −2, 0 )T .
2.1.7. Let F(R2, R2) denote the vector space consisting of all functions f: R2 →R2.
(a) Which of the following functions f(x, y) are elements? (i) x2 + y2, (ii)

x −y
xy
	
,
(iii)

ex
cos y
	
, (iv)

1
3
	
, (v)

x
y
−y
x
	
, (vi)
⎛
⎜
⎝
x
y
x + y
⎞
⎟
⎠. (b) Sum all of the elements
of F(R2, R2) you identiﬁed in part (a). Then multiply your sum by the scalar −5.
(c) Carefully describe the zero element of the vector space F(R2, R2).
♦2.1.8. A planar vector ﬁeld is a function that assigns a vector v(x, y) =

v1(x, y)
v2(x, y)
	
to each
point

x
y
	
∈R2. Explain why the set of all planar vector ﬁelds forms a vector space.
♥2.1.9. Let h, k > 0 be ﬁxed. Let S = { (ih, j k) | 1 ≤i ≤m, 1 ≤j ≤n } be
points in a rectangular planar grid. Show that the function space F(S) can
be identiﬁed with the vector space of m × n matrices Mm×n.
2.1.10. The space R∞is deﬁned as the set of all inﬁnite real sequences a = (a1, a2, a3, . . . ),
where ai ∈R. Deﬁne addition and scalar multiplication in such a way as to make R∞into
a vector space. Explain why all the vector space axioms are valid.
2.1.11. Prove the basic vector space properties (i), (j), (k) following Deﬁnition 2.1.
♦2.1.12. Prove that a vector space has only one zero element 0.
♦2.1.13. Suppose that V and W are vector spaces. The Cartesian product space, denoted by
V × W, is deﬁned as the set of all ordered pairs (v, w), where v ∈V, w ∈W, with vector
addition (v, w) + (v, w) = (v + v, w + w) and scalar multiplication c(v, w) = (cv, cw).
(a) Prove that V × W is a vector space. (b) Explain why R × R is the same as R2.
(c) More generally, explain why Rm × Rn is the same as Rm+n.
2.1.14. Use Exercise 2.1.13 to show that the space of pairs (f(x), a), where f is a continuous
scalar function and a is a real number, is a vector space. What is the zero element? Be
precise! Write out the laws of vector addition and scalar multiplication.
2.2 Subspaces
In the preceding section, we were introduced to the most basic vector spaces that arise in
this text. Almost all of the vector spaces used in applications appear as subsets of these
prototypical examples.

82
2 Vector Spaces and Bases
Deﬁnition 2.8. A subspace of a vector space V is a subset W ⊂V that is a vector space
in its own right — under the same operations of vector addition and scalar multiplication
and the same zero element.
In particular, a subspace W must contain the zero element of V . Proving that a given
subset of a vector space is a subspace is particularly easy: we need only check its closure
under addition and scalar multiplication.
Proposition 2.9. A nonempty subset W ⊂V of a vector space is a subspace if and only if
(a) for every v, w ∈W, the sum v + w ∈W, and
(b) for every v ∈W and every c ∈R, the scalar product cv ∈W.
Proof : The proof is immediate. For example, let us check commutativity. The subspace
elements v, w ∈W can be regarded as elements of V , in which case v+w = w+v because
V is a vector space. But the closure condition implies that the common sum also belongs
to W, and so the commutativity axiom also holds for elements of W. Establishing the
validity of the other axioms is equally easy.
Q.E.D.
It is sometimes convenient to combine the two closure conditions. Thus, to prove that
W ⊂V is a subspace, it suﬃces to check that cv + dw ∈W for all v, w ∈W and c, d ∈R.
Example 2.10.
Let us list some examples of subspaces of the three-dimensional Eu-
clidean space R3.
(a) The trivial subspace W = {0}. Demonstrating closure is easy: since there is only
one element 0 in W, we just need to check that 0 + 0 = 0 ∈W and c0 = 0 ∈W for every
scalar c.
(b) The entire space W = R3. Here closure is immediate because R3 is a vector space
in its own right.
(c) The set of all vectors of the form ( x, y, 0 )T , i.e., the xy coordinate plane. To prove
closure, we check that all sums ( x, y, 0 )T + ( x, y, 0 )T = ( x + x, y + y, 0 )T and scalar
multiples c ( x, y, 0 )T = ( cx, cy, 0 )T of vectors in the xy-plane remain in the plane.
(d) The set of solutions ( x, y, z )T to the homogeneous linear equation
3x + 2y −z = 0.
(2.2)
Indeed, if x = ( x, y, z )T is a solution, then so is every scalar multiple cx = ( cx, cy, cz )T
since
3(cx) + 2(cy) −(cz) = c(3x + 2y −z) = 0.
Moreover, if x = ( x, y, z )T is a second solution, the sum x + x = ( x + x, y + y, z + z )T
is also a solution, since
3(x + x) + 2(y + y ) −(z + z ) = (3x + 2y −z) + (3 x + 2 y −z ) = 0.
The solution space is, in fact, the two-dimensional plane passing through the origin with
normal vector ( 3, 2, −1 )T .
(e) The set of all vectors lying in the plane spanned by the vectors v1 = ( 2, −3, 0 )T
and v2 = ( 1, 0, 3 )T . In other words, we consider all vectors of the form
v = av1 + bv2 = a
⎛
⎝
2
−3
0
⎞
⎠+ b
⎛
⎝
1
0
3
⎞
⎠=
⎛
⎝
2a + b
−3a
3b
⎞
⎠,

2.2 Subspaces
83
where a, b ∈R are arbitrary scalars. If v = av1 + bv2 and w = a v1 + b v2 are any two
vectors in the span, then so is
cv + dw = c(av1 + bv2) + d(a v1 + b v2) = (ac + a d)v1 + (bc + b d)v2 = a v1 + b v2,
where a = ac+ad, b = bc+b d. This demonstrates that the span is a subspace of R3. The
reader might already have noticed that this subspace is the same plane deﬁned by (2.2).
Example 2.11.
The following subsets of R3 are not subspaces.
(a) The set P of all vectors of the form ( x, y, 1 )T , i.e., the plane parallel to the xy
coordinate plane passing through ( 0, 0, 1 )T . Indeed, ( 0, 0, 0 )T ̸∈P, which is the most basic
requirement for a subspace. In fact, neither of the closure axioms hold for this subset.
(b) The nonnegative orthant O+ = {x ≥0, y ≥0, z ≥0}. Although 0 ∈O+, and
the sum of two vectors in O+ also belongs to O+, multiplying by negative scalars takes us
outside the orthant, violating closure under scalar multiplication.
(c) The unit sphere S1 = { x2 + y2 + z2 = 1 }. Again, 0 ̸∈S1. More generally, curved
surfaces, such as the paraboloid P = { z = x2 + y2 }, are not subspaces. Although 0 ∈P,
most scalar multiples of elements of P do not belong to P. For example, ( 1, 1, 2 )T ∈P,
but 2 ( 1, 1, 2 )T = ( 2, 2, 4 )T ̸∈P.
In fact, there are only four fundamentally diﬀerent types of subspaces W ⊂R3 of
three-dimensional Euclidean space:
(i)
the entire three-dimensional space W = R3,
(ii)
a plane passing through the origin,
(iii)
a line passing through the origin,
(iv)
a point — the trivial subspace W = {0}.
We can establish this observation by the following argument. If W = {0} contains only
the zero vector, then we are in case (iv). Otherwise, W ⊂R3 contains a nonzero vector
0 ̸= v1 ∈W. But since W must contain all scalar multiples cv1 of this element, it includes
the entire line in the direction of v1. If W contains another vector v2 that does not lie
in the line through v1, then it must contain the entire plane {cv1 + dv2} spanned by
v1, v2. Finally, if there is a third vector v3 not contained in this plane, then we claim
that W = R3. This ﬁnal fact will be an immediate consequence of general results in this
chapter, although the interested reader might try to prove it directly before proceeding.
Example 2.12.
Let I ⊂R be an interval, and let F(I) be the space of real-valued
functions f: I →R. Let us look at some of the most important examples of subspaces of
F(I). In each case, we need only verify the closure conditions to verify that the given subset
is indeed a subspace. In particular, the zero function belongs to each of the subspaces.
(a) The space P(n) of polynomials of degree ≤n, which we already encountered.
(b) The space P(∞) =

n≥0 P(n) consisting of all polynomials. Closure means that
the sum of any two polynomials is a polynomial, as is any scalar (constant) multiple of a
polynomial.
(c) The space C0(I) of all continuous functions.
Closure of this subspace relies on
knowing that if f(x) and g(x) are continuous, then both f(x) + g(x) and cf(x), for any
c ∈R, are also continuous — two basic results from calculus, [2, 78].

84
2 Vector Spaces and Bases
(d) More restrictively, we can consider the subspace Cn(I) consisting of all functions
f(x) that have n continuous derivatives f ′(x), f ′′(x), . . . , f (n)(x) on† I. Again, we need to
know that if f(x) and g(x) have n continuous derivatives, then so does cf(x) + dg(x) for
all c, d ∈R.
(e) The space C∞(I) =

n≥0 Cn(I) of inﬁnitely diﬀerentiable or smooth functions is
also a subspace. This can be proved directly, or it follows from the general fact that the
intersection of subspaces is a subspace, cf. Exercise 2.2.23.
(f ) The space A(I) of analytic functions on the interval I. Recall that a function f(x)
is called analytic at a point a if it is smooth, and, moreover, its Taylor series
f(a) + f ′(a) (x −a) + 1
2 f ′′(a) (x −a)2 + · · ·
=
∞

n=0
f (n)(a)
n!
(x −a)n
(2.3)
converges to f(x) for all x suﬃciently close to a. (The series is not required to converge on
the entire interval I.) Not every smooth function is analytic, and so A(I) ⊊C∞(I). An
explicit example of a smooth but non-analytic function can be found in Exercise 2.2.30.
(g) The set of all mean zero functions. The mean or average of an integrable function
deﬁned on a closed interval I = [a, b] is the real number
f =
1
b −a
 b
a
f(x) dx.
(2.4)
In particular, f has mean zero if and only if
 b
a
f(x) dx = 0. Since f + g = f + g, and
cf = c f, sums and scalar multiples of mean zero functions also have mean zero, proving
closure.
(h) Fix x0 ∈I. The set of all functions f(x) that vanish at the point, f(x0) = 0, is
a subspace. Indeed, if f(x0) = 0 and g(x0) = 0, then, clearly (cf + dg)(x0) = cf(x0) +
dg(x0) = 0 for all c, d ∈R, proving closure. This example can evidently be generalized to
functions that vanish at several points, or even on an entire subset S ⊂I.
(i) The set of all solutions u = f(x) to the homogeneous linear diﬀerential equation
u′′ + 2u′ −3u = 0.
Indeed, if f(x) and g(x) are solutions, then so is f(x) + g(x) and cf(x) for all c ∈R. Note
that we do not need to actually solve the equation to verify these claims! They follow
directly from linearity:
(f + g)′′ + 2(f + g)′ −3(f + g) = (f ′′ + 2f ′ −3f) + (g′′ + 2g′ −3g) = 0,
(cf)′′ + 2(cf)′ −3(cf) = c(f ′′ + 2f ′ −3f) = 0.
Remark. In the last three examples, 0 is essential for the indicated set of functions to be
a subspace. The set of functions such that f(x0) = 1, say, is not a subspace. The set of
functions with a given nonzero mean, say f = 3, is also not a subspace. Nor is the set of
solutions to an inhomogeneous ordinary diﬀerential equation, say u′′ + 2u′ −3u = x −3.
None of these subsets contain the zero function, nor do they satisfy the closure conditions.
†
We use one-sided derivatives at any endpoint belonging to the interval.

2.2 Subspaces
85
Exercises
2.2.1.(a) Prove that the set of all vectors ( x, y, z )T such that x −y + 4z = 0 forms a subspace
of R3.
(b) Explain why the set of all vectors that satisfy x −y + 4z = 1 does not form a
subspace.
2.2.2. Which of the following are subspaces of R3? Justify your answers! (a) The set of all
vectors ( x, y, z )T satisfying x + y + z + 1 = 0. (b) The set of vectors of the form ( t, −t, 0 )T
for t ∈R. (c) The set of vectors of the form ( r −s, r + 2s, −s )T for r, s ∈R. (d) The
set of vectors whose ﬁrst component equals 0. (e) The set of vectors whose last component
equals 1. (f ) The set of all vectors ( x, y, z )T with x ≥y ≥z. (g) The set of all solutions
to the equation z = x−y. (h) The set of all solutions to z = xy. (i) The set of all solutions
to x2 + y2 + z2 = 0. (j) The set of all solutions to the system xy = y z = xz.
2.2.3. Graph the following subsets of R3 and use this to explain which are subspaces:
(a) The line ( t, −t, 3t )T for t ∈R. (b) The helix ( cos t, sin t, t )T . (c) The surface
x−2y +3z = 0. (d) The unit ball x2 +y2 +z2 < 1. (e) The cylinder (y +2)2 +(z −1)2 = 5.
(f ) The intersection of the cylinders (x −1)2 + y2 = 1 and (x + 1)2 + y2 = 1.
2.2.4. Show that if W ⊂R3 is a subspace containing the vectors ( 1, 2, −1 )T , ( 2, 0, 1 )T ,
( 0, −1, 3 )T , then W = R3.
2.2.5. True or false: An interval is a vector space.
2.2.6.(a) Can you construct an example of a subset S ⊂R2 with the property that cv ∈S
for all c ∈R, v ∈S, and yet S is not a subspace?
(b) What about an example in which
v + w ∈S for every v, w ∈S, and yet S is not a subspace?
2.2.7. Determine which of the following sets of vectors x = ( x1, x2, . . . , xn )T are subspaces of
Rn: (a) all equal entries x1 = · · · = xn; (b) all positive entries: xi ≥0; (c) ﬁrst and last
entries equal to zero: x1 = xn = 0; (d) entries add up to zero: x1 + · · · + xn = 0; (e) ﬁrst
and last entries diﬀer by one: x1 −xn = 1.
2.2.8. Prove that the set of all solutions x of the linear system A x = b forms a subspace if and
only if the system is homogeneous.
2.2.9. A square matrix is called strictly lower triangular if all entries on or above the main
diagonal are 0. Prove that the space of strictly lower triangular matrices is a subspace of
the vector space of all n × n matrices.
2.2.10. Which of the following are subspaces of the vector space of n × n matrices Mn×n?
The set of all (a) regular matrices; (b) nonsingular matrices; (c) singular matrices;
(d) lower triangular matrices; (e) lower unitriangular matrices; (f ) diagonal matrices;
(g) symmetric matrices; (h) skew-symmetric matrices.
♦2.2.11. The trace of an n × n matrix A ∈Mn×n is deﬁned to be the sum of its diagonal
entries: tr A = a11 + a22 + · · · + ann. Prove that the set of trace zero matrices, tr A = 0, is
a subspace of Mn×n.
2.2.12.(a) Is the set of n × n matrices with det A = 1 a subspace of Mn×n?
(b) What about the matrices with det A = 0?
2.2.13. Let V = C0(R) be the vector space consisting of all continuous functions f: R →R.
Explain why the set of all functions such that f(1) = 0 is a subspace, but the set of
functions such that f(0) = 1 is not. For which values of a, b does the set of functions such
that f(a) = b form a subspace?

86
2 Vector Spaces and Bases
2.2.14. Which of the following are vector spaces? Justify your answer! (a)
The set of all row
vectors of the form ( a, 3a ). (b) The set of all vectors of the form ( a, a + 1 ) . (c) The set
of all continuous functions for which f(−1) = 0. (d) The set of all periodic functions of
period 1, i.e., f(x + 1) = f(x). (e) The set of all non-negative functions: f(x) ≥0.
(f ) The set of all even polynomials: p(x) = p(−x). (g) The set of all polynomials p(x) that
have x −1 as a factor. (h) The set of all quadratic forms q(x, y) = ax2 + bxy + cy2.
2.2.15. Determine which of the following conditions describe subspaces of the vector space C1
consisting of all continuously diﬀerentiable scalar functions f(x).
(a) f(2) = f(3), (b) f′(2) = f(3), (c) f′(x) + f(x) = 0, (d) f(2 −x) = f(x),
(e) f(x + 2) = f(x) + 2, (f ) f(−x) = ex f(x). (g) f(x) = a + b | x | for some a, b ∈R,
2.2.16. Let V = C0[a, b] be the vector space consisting of all functions f(t) that are deﬁned
and continuous on the interval 0 ≤t ≤1. Which of the following conditions deﬁne
subspaces of V ? Explain your answer. (a) f(0) = 0, (b) f(0) = 2 f(1), (c) f(0) f(1) = 1,
(d) f(0) = 0 or f(1) = 0, (e) f(1 −t) = −t f(t), (f ) f(1 −t) = 1 −f(t),
(g) f
 1
2

=
 1
0 f(t) dt, (h)
 1
0 (t −1) f(t) dt = 0, (i)
 t
0 f(s) sin s ds = sin t.
2.2.17. Prove that the set of solutions to the second order ordinary diﬀerential equation
u′′ = xu is a vector space.
2.2.18. Show that the set of solutions to u′′ = x + u does not form a vector space.
2.2.19.(a) Prove that C1([a, b], R2), which is the space of continuously diﬀerentiable
parameterized plane curves f: [a, b] →R2, is a vector space.
(b) Is the subset consisting of all curves that go through the origin a subspace?
2.2.20. A planar vector ﬁeld v(x, y) = ( u(x, y), v(x, y) )T is called irrotational if it has zero
divergence: ∇· v = ∂u
∂x + ∂v
∂y ≡0. Prove that the set of all irrotational vector ﬁelds is a
subspace of the space of all planar vector ﬁelds.
2.2.21. Let C ⊂R∞denote the set of all convergent sequences of real numbers, where R∞was
deﬁned in Exercise 2.2.21. Is C a subspace?
♦2.2.22. Show that if W and Z are subspaces of V , then (a) their intersection W ∩Z is a
subspace of V , (b) their sum W + Z = { w + z | w ∈W, z ∈Z } is also a subspace, but
(c) their union W ∪Z is not a subspace of V , unless W ⊂Z or Z ⊂W.
♦2.2.23. Let V be a vector space. Prove that the intersection

Wi of any collection (ﬁnite or
inﬁnite) of subspaces Wi ⊂V is a subspace.
♥2.2.24. Let W ⊂V be a subspace. A subspace Z ⊂V is called a complementary subspace to W
if (i) W ∩Z = {0}, and (ii) W + Z = V , i.e., every v ∈V can be written as v = w + z for
w ∈W and z ∈Z. (a) Show that the x- and y-axes are complementary subspaces of R2.
(b) Show that the lines x = y and x = 3y are complementary subspaces of R2.
(c) Show
that the line ( a, 2a, 3a )T and the plane x + 2y + 3z = 0 are complementary subspaces of
R3.
(d) Prove that if v = w + z, then w ∈W and z ∈Z are uniquely determined.
2.2.25.(a) Show that V0 = { (v, 0) | v ∈V } and W0 = { (0, w) | w ∈W } are complementary
subspaces, as in Exercise 2.2.24, of the Cartesian product space V × W, as deﬁned in
Exercise 2.1.13.
(b) Prove that the diagonal D = {(v, v)} and the anti-diagonal
A = {(v, −v)} are complementary subspaces of V × V .
2.2.26. Show that the set of skew-symmetric n × n matrices forms a complementary subspace
to the set of symmetric n × n matrices. Explain why this implies that every square matrix
can be uniquely written as the sum of a symmetric and a skew-symmetric matrix.

2.3 Span and Linear Independence
87
2.2.27.(a) Show that the set of even functions, f(−x) = f(x), is a subspace of the vector space
of all functions F(R).
(b) Show that the set of odd functions, g(−x) = −g(x), forms a
complementary subspace, as deﬁned in Exercise 2.2.24. (c) Explain why every function can
be uniquely written as the sum of an even function and an odd function.
♥2.2.28. Let V be a vector space. A subset of the form A = { w + a | w ∈W }, where W ⊂V is
a subspace and a ∈V is a ﬁxed vector, is known as an aﬃne subspace of V . (a) Show that
an aﬃne subspace A ⊂V is a genuine subspace if and only if a ∈W.
(b) Draw the aﬃne
subspaces A ⊂R2 when (i) W is the x-axis and a = ( 2, 1 )T , (ii) W is the line y = 3
2 x
and a = ( 1, 1 )T , (iii) W is the line { ( t, −t )T | t ∈R }, and a = ( 2, −2 )T . (c) Prove that
every aﬃne subspace A ⊂R2 is either a point, a line, or all of R2. (d) Show that the plane
x −2y + 3z = 1 is an aﬃne subspace of R3. (e) Show that the set of all polynomials such
that p(0) = 1 is an aﬃne subspace of P(n).
♥2.2.29. Quotient spaces: Let V be a vector space and W ⊂V a subspace. We say that
two vectors u, v ∈V are equivalent modulo W if u −v ∈W. (a) Show that this
deﬁnes an equivalence relation, written u ∼W v on V , i.e., (i) v ∼W v for every v;
(ii) if u ∼W v, then v ∼W u; and (iii) if, in addition, v ∼W z, then u ∼W z. (b) The
equivalence class of a vector u ∈V is deﬁned as the set of all equivalent vectors,
written [u]W = { v ∈V | v ∼W u }. Show that [0]W = W. (c) Let V = R2 and
W = { ( x, y )T | x = 2y }. Sketch a picture of several equivalence classes as subsets of
R2. (d) Show that each equivalence class [u]W for u ∈V is an aﬃne subspace of V , as in
Exercise 2.2.28. (e) Prove that the set of equivalence classes, called the quotient space and
denoted by V/W = { [u] | u ∈V }, forms a vector space under the operations of addition,
[u]W + [v]W = [u + v]W , and scalar multiplication, c [u]W = [cu]W . What is the zero
element? Thus, you ﬁrst need to prove that these operations are well deﬁned, and then
demonstrate the vector space axioms.
♦2.2.30. Deﬁne f(x) =
⎧
⎨
⎩
e−1/x,
x > 0,
0,
x ≤0.
(a) Prove that all derivatives of f vanish at the origin: f(n)(0) = 0 for n = 0, 1, 2, . . . .
(b) Prove that f(x) is not analytic by showing that its Taylor series at a = 0 does not
converge to f(x) when x > 0.
2.2.31. Let f(x) =
1
1 + x2 . (a) Find the Taylor series of f at a = 0. (b) Prove that the Taylor
series converges for | x | < 1, but diverges for | x | ≥1. (c) Prove that f(x) is analytic at x = 0.
2.3 Span and Linear Independence
The deﬁnition of the span of a collection of elements of a vector space generalizes, in a
natural fashion, the geometric notion of two vectors spanning a plane in R3. As such, it
describes the ﬁrst of two universal methods for constructing subspaces of vector spaces.
Deﬁnition 2.13. Let v1, . . . , vk be elements of a vector space V . A sum of the form
c1v1 + c2v2 + · · · + ckvk =
k

i=1
civi,
(2.5)
where the coeﬃcients c1, c2, . . . , ck are any scalars, is known as a linear combination of the
elements v1, . . . , vk. Their span is the subset W = span {v1, . . ., vk} ⊂V consisting of all
possible linear combinations with scalars c1, . . . , ck ∈R.

88
2 Vector Spaces and Bases
v1
v2
v1
v2
Figure 2.4.
Plane and Line Spanned by Two Vectors.
For instance, 3v1 + v2 −2v3, 8v1 −1
3 v3 = 8v1 + 0v2 −1
3 v3, v2 = 0v1 + 1v2 +
0v3, and 0 = 0v1 + 0v2 + 0v3 are four diﬀerent linear combinations of the three vector
space elements v1, v2, v3 ∈V .
The key observation is that the span always forms a subspace.
Proposition 2.14. The span W = span {v1, . . . , vk} of any ﬁnite collection of vector
space elements v1, . . . , vk ∈V is a subspace of the underlying vector space V .
Proof : We need to show that if
v = c1v1 + · · · + ckvk
and
v = c1v1 + · · · + ckvk
are any two linear combinations, then their sum is also a linear combination, since
v + v = (c1 + c1)v1 + · · · + (ck + ck)vk = c1v1 + · · · + ckvk,
where ci = ci + ci. Similarly, for any scalar multiple,
a v = (ac1)v1 + · · · + (ack)vk = c∗
1 v1 + · · · + c∗
kvk,
where c∗
i = aci, which completes the proof.
Q.E.D.
Example 2.15.
Examples of subspaces spanned by vectors in R3:
(i) If v1 ̸= 0 is any non-zero vector in R3, then its span is the line { c v1 | c ∈R }
consisting of all vectors parallel to v1. If v1 = 0, then its span just contains the
origin.
(ii) If v1 and v2 are any two vectors in R3, then their span is the set of all vectors of the
form c1v1 + c2v2. Typically, such a span prescribes a plane passing through the
origin. However, if v1 and v2 are parallel, then their span is just a line. The most
degenerate case occurs when v1 = v2 = 0, where the span is just a point — the
origin.
(iii) If we are given three non-coplanar vectors v1, v2, v3, then their span is all of R3, as
we shall prove below. However, if they all lie in a plane, then their span is the
plane — unless they are all parallel, in which case their span is a line — or, in the
completely degenerate situation v1 = v2 = v3 = 0, a single point.
Thus, every subspace of R3 can be realized as the span of some set of vectors. One can
consider subspaces spanned by four or more vectors in R3, but these continue to be limited
to being either a point (the origin), a line, a plane, or the entire three-dimensional space.

2.3 Span and Linear Independence
89
A crucial question is to determine when a given vector belongs to the span of a
prescribed collection.
Example 2.16.
Let W ⊂R3 be the plane spanned by the vectors v1 = ( 1, −2, 1 )T and
v2 = ( 2, −3, 1 )T . Question: Is the vector v = ( 0, 1, −1 )T an element of W? To answer,
we need to see whether we can ﬁnd scalars c1, c2 such that
v = c1 v1 + c2 v2;
that is,
⎛
⎝
0
1
−1
⎞
⎠= c1
⎛
⎝
1
−2
1
⎞
⎠+ c2
⎛
⎝
2
−3
1
⎞
⎠=
⎛
⎝
c1 + 2c2
−2c1 −3c2
c1 + c2
⎞
⎠.
Thus, c1, c2 must satisfy the linear algebraic system
c1 + 2c2 = 0,
−2c1 −3c2 = 1,
c1 + c2 = −1.
Applying Gaussian Elimination, we ﬁnd the solution c1 = −2, c2 = 1, and so v = −2v1+v2
does belong to the span. On the other hand, v = ( 1, 0, 0 )T does not belong to W. Indeed,
there are no scalars c1, c2 such that v = c1 v1 + c2 v2, because the corresponding linear
system is incompatible.
Warning. It is entirely possible for diﬀerent sets of vectors to span the same subspace.
For instance, e1 = ( 1, 0, 0 )T and e2 = ( 0, 1, 0 )T span the xy-plane in R3, as do the three
coplanar vectors v1 = ( 1, −1, 0 )T , v2 = ( −1, 2, 0 )T , v3 = ( 2, 1, 0 )T .
Example 2.17.
Let V = F(R) denote the space of all scalar functions f(x).
(a) The span of the three monomials f1(x) = 1, f2(x) = x, and f3(x) = x2 is the set
of all functions of the form
f(x) = c1 f1(x) + c2 f2(x) + c3 f3(x) = c1 + c2 x + c3 x2,
where c1, c2, c3 are arbitrary scalars (constants). In other words, span {1, x, x2} = P(2)
is the subspace of all quadratic (degree ≤2) polynomials. In a similar fashion, the space
P(n) of polynomials of degree ≤n is spanned by the monomials 1, x, x2, . . . , xn.
(b) The next example plays a key role in many applications. Let 0 ̸= ω ∈R. Consider
the two basic trigonometric functions f1(x) = cos ωx, f2(x) = sin ωx of frequency ω, and
hence period 2π/ω. Their span consists of all functions of the form
f(x) = c1 f1(x) + c2 f2(x) = c1 cos ωx + c2 sin ωx.
(2.6)
For example, the function cos(ωx + 2) lies in the span because, by the addition formula
for the cosine,
cos(ωx + 2) = (cos 2) cos ωx −(sin 2) sin ωx
is a linear combination of cos ωx and sin ωx, with respective coeﬃcients cos 2, sin 2. Indeed,
we can express a general function in the span in the alternative phase-amplitude form
f(x) = c1 cos ωx + c2 sin ωx = r cos(ωx −δ),
(2.7)
in which r ≥0 is known as the amplitude and 0 ≤δ < 2π the phase shift.
Indeed,
expanding the right-hand side, we obtain
r cos(ωx −δ) = (r cos δ) cos ωx + (r sin δ) sin ωx,
and hence c1 = r cos δ, c2 = r sin δ.

90
2 Vector Spaces and Bases
Figure 2.5.
Graph of 3 cos(2x −1).
Thus, (r, δ) are the polar coordinates of the point c = (c1, c2) ∈R2 prescribed by the coef-
ﬁcients. We conclude that every linear combination of sin ωx and cos ωx can be rewritten
as a single cosine containing an extra phase shift. Figure 2.5 shows the particular function
3 cos(2x −1), which has amplitude r = 3, frequency ω = 2, and phase shift δ = 1. The
ﬁrst peak appears at x = δ/ω = 1
2.
(c) The space T (2) of quadratic trigonometric polynomials is spanned by the functions
1,
cos x,
sin x,
cos2 x,
cos x sin x,
sin2 x.
Its general element is a linear combination
q(x) = c0 + c1 cos x + c2 sin x + c3 cos2 x + c4 cos x sin x + c5 sin2 x,
(2.8)
where c0, . . . , c5 are arbitrary constants. A more useful spanning set for the same subspace
consists of the trigonometric functions
1,
cos x,
sin x,
cos 2x,
sin 2x.
(2.9)
Indeed, by the double-angle formulas, both
cos 2x = cos2 x −sin2 x,
sin 2x = 2 sin x cosx,
have the form of a quadratic trigonometric polynomial (2.8), and hence both belong to
T (2). On the other hand, we can write
cos2 x = 1
2 cos 2x + 1
2,
cos x sin x = 1
2 sin 2x,
sin2 x = −1
2 cos 2x + 1
2,
in terms of the functions (2.9). Therefore, the original linear combination (2.8) can be
written in the alternative form
q(x) =

c0 + 1
2 c3 + 1
2 c5

+ c1 cos x + c2 sin x +
 1
2 c3 −1
2 c5

cos 2x + 1
2 c4 sin 2x
= c0 + c1 cos x + c2 sin x + c3 cos 2x + c4 sin 2x,
(2.10)
and so the functions (2.9) do indeed span T (2). It is worth noting that we ﬁrst character-
ized T (2) as the span of 6 functions, whereas the second characterization required only 5
functions. It turns out that 5 is the minimal number of functions needed to span T (2), but
the proof of this fact will be deferred until Chapter 4.
(d) The homogeneous linear ordinary diﬀerential equation
u′′ + 2u′ −3u = 0
(2.11)
considered in part (i) of Example 2.12 has two solutions: f1(x) = ex and f2(x) = e−3x.
(Now may be a good time for you to review the basic techniques for solving linear, constant

2.3 Span and Linear Independence
91
coeﬃcient ordinary diﬀerential equations, cf. [7, 22]; see also Chapter 7.)
Its general
solution is, in fact, a linear combination
u = c1 f1(x) + c2 f2(x) = c1 ex + c2 e−3x,
where c1, c2 are arbitrary scalars. Thus, the vector space of solutions to (2.11) is described
as the span of these two basic solutions.
The fact that there are no other solutions is
not obvious, but relies on the basic uniqueness theorem for ordinary diﬀerential equations;
further details can be found in Theorem 7.34.
Remark. One can also deﬁne the span of an inﬁnite collection of elements of a vector space.
To avoid convergence issues, one should consider only ﬁnite linear combinations (2.5). For
example, the span of the monomials 1, x, x2, x3, . . . is the subspace P(∞) of all polynomials
— not the space of analytic functions or convergent Taylor series. Similarly, the span of
the functions 1, cosx, sin x, cos 2x, sin 2x, cos 3x, sin 3x, . . . is the space T (∞) containing all
trigonometric polynomials, of fundamental importance in the theory of Fourier series, [61].
Exercises
2.3.1. Show that
⎛
⎜
⎝
−1
2
3
⎞
⎟
⎠belongs to the subspace of R3 spanned by
⎛
⎜
⎝
2
−1
2
⎞
⎟
⎠,
⎛
⎜
⎝
5
−4
1
⎞
⎟
⎠by writing
it as a linear combination of the spanning vectors.
2.3.2. Show that
⎛
⎜
⎜
⎜
⎝
−3
7
6
1
⎞
⎟
⎟
⎟
⎠is in the subspace of R4 spanned by
⎛
⎜
⎜
⎜
⎝
1
−3
−2
0
⎞
⎟
⎟
⎟
⎠,
⎛
⎜
⎜
⎜
⎝
−2
6
3
4
⎞
⎟
⎟
⎟
⎠and
⎛
⎜
⎜
⎜
⎝
−2
4
6
−7
⎞
⎟
⎟
⎟
⎠.
2.3.3.(a) Determine whether
⎛
⎜
⎝
1
−2
−3
⎞
⎟
⎠is in the span of
⎛
⎜
⎝
1
1
0
⎞
⎟
⎠and
⎛
⎜
⎝
0
1
1
⎞
⎟
⎠.
(b) Is
⎛
⎜
⎝
1
−2
−1
⎞
⎟
⎠in the
span of
⎛
⎜
⎝
1
2
2
⎞
⎟
⎠,
⎛
⎜
⎝
1
−2
0
⎞
⎟
⎠,
⎛
⎜
⎝
0
3
4
⎞
⎟
⎠?
(c) Is
⎛
⎜
⎜
⎜
⎝
3
0
−1
−2
⎞
⎟
⎟
⎟
⎠in the span of
⎛
⎜
⎜
⎜
⎝
1
2
0
1
⎞
⎟
⎟
⎟
⎠,
⎛
⎜
⎜
⎜
⎝
0
−1
3
0
⎞
⎟
⎟
⎟
⎠,
⎛
⎜
⎜
⎜
⎝
2
0
1
−1
⎞
⎟
⎟
⎟
⎠?
2.3.4. Which of the following sets of vectors span all of R2? (a)

1
−1
	
; (b)

2
−1
	
,

1
3
	
;
(c)

2
−1
	
,

−1
2
	
; (d)

6
−9
	
,

−4
6
	
; (e)

1
−1
	
,

2
−1
	
,

3
−1
	
; (f )

0
0
	
,

1
−1
	
,

2
−2
	
.
2.3.5.(a) Graph the subspace of R3 spanned by the vector v1 = ( 3, 0, 1 )T .
(b) Graph the subspace spanned by the vectors v1 = ( 3, −2, −1 )T , v2 = ( −2, 0, −1 )T .
(c) Graph the span of v1 = ( 1, 0, −1 )T , v2 = ( 0, −1, 1 )T , v3 = ( 1, −1, 0 )T .
2.3.6. Let U be the subspace of R3 spanned by u1 = ( 1, 2, 3 )T , u2 = ( 2, −1, 0 )T . Let V be
the subspace spanned by v1 = ( 5, 0, 3 )T , v2 = ( 3, 1, 3 )T . Is V a subspace of U? Are U
and V the same?
2.3.7.(a) Let S be the subspace of M2×2 consisting of all symmetric 2 × 2 matrices. Show that
S is spanned by the matrices

1
0
0
0
	
,

0
0
0
1
	
, and

0
1
1
0
	
.
(b) Find a spanning set of
the space of symmetric 3 × 3 matrices.

92
2 Vector Spaces and Bases
2.3.8.(a) Determine whether the polynomials x2 + 1, x2 −1, x2 + x + 1 span P(2).
(b) Do x3 −1, x2 + 1, x −1, 1 span P(3)? (c) What about x3, x2 + 1, x2 −x, x + 1?
2.3.9. Determine whether any of the following functions lies in the subspace spanned by 1, x,
sin x, sin2 x: (a) 3 −5x, (b) x2 + sin2 x, (c) sin x −2 cos x, (d) cos2 x, (e) x sin x, (f ) ex.
2.3.10. Write the following trigonometric functions in phase–amplitude form:
(a) sin 3x,
(b) cos x −sin x,
(c) 3 cos 2x + 4 sin 2x,
(d) cos x sin x.
2.3.11.(a) Prove that the set of solutions to the homogeneous ordinary diﬀerential equation
u′′ −4u′ + 3u = 0 is a vector space.
(b) Write the solution space as the span of a ﬁnite
number of functions.
(c) What is the minimal number of functions needed to span the
solution space?
2.3.12. Explain why the functions 1, cos x, sin x span the solution space to the third order
ordinary diﬀerential equation u′′′ + u′ = 0.
2.3.13. Find a ﬁnite set of real functions that spans the solution space to the following
homogeneous ordinary diﬀerential equations: (a) u′ −2u = 0, (b) u′′ + 4u = 0,
(c) u′′ −3u′ = 0, (d) u′′ + u′ + u = 0, (e) u′′′ −5u′′ = 0, (f ) u(4) + u = 0.
2.3.14. Consider the boundary value problem u′′ + 4u = 0, 0 ≤x ≤π, u(0) = 0, u(π) = 0.
(a) Prove, without solving, that the set of solutions forms a vector space.
(b) Write this space as the span of one or more functions. Hint: First solve the diﬀerential
equation; then ﬁnd out which solutions satisfy the boundary conditions.
2.3.15. Which of the following functions lie in the span of the vector-valued functions
f1(x) =

1
x
	
,
f2(x) =

x
1
	
,
f3(x) =

x
2x
	
?
(a)

2
1
	
,
(b)

1 −2x
1 −x
	
,
(c)

1 −2x
−1 −x
	
,
(d)

1 + x2
1 −x2
	
,
(e)

2 −x
0
	
.
2.3.16. True or false: The zero vector belongs to the span of any collection of vectors.
2.3.17. Prove or give a counter-example: if z is a linear combination of u, v, w, then w is a
linear combination of u, v, z.
♦2.3.18. Suppose v1, . . . , vm span V . Let vm+1, . . . , vn ∈V be any other elements. Prove that
the combined collection v1, . . . , vn also spans V .
♦2.3.19.(a) Show that if v is a linear combination of v1, . . . , vm, and each vj is a linear
combination of w1, . . . , wn, then v is a linear combination of w1, . . . , wn.
(b) Suppose v1, . . . , vm span V . Let w1, . . . , wm ∈V be any other elements. Suppose that
each vi can be written as a linear combination of w1, . . . , wm. Prove that w1, . . . , wm also
span V .
♦2.3.20. The span of an inﬁnite collection v1, v2, v3, . . . ∈V of vector space elements is deﬁned
as the set of all ﬁnite linear combinations
n

i=1
ci vi, where n < ∞is ﬁnite but arbitrary.
(a) Prove that the span deﬁnes a subspace of the vector space V .
(b) What is the span of the monomials 1, x, x2, x3, . . . ?
Linear Independence and Dependence
Most of the time, all of the vectors used to form a span are essential. For example, we
cannot use fewer than two vectors to span a plane in R3, since the span of a single vector is
at most a line. However, in degenerate situations, some of the spanning elements may be

2.3 Span and Linear Independence
93
redundant. For instance, if the two vectors are parallel, then their span is a line, but only
one of the vectors is really needed to prescribe the line. Similarly, the subspace spanned by
the polynomials p1(x) = x −2, p2(x) = 3x + 4, p3(x) = −x + 1, is the vector space P(1)
consisting of all linear polynomials. But only two of the polynomials are really required to
span P(1). (The reason will become clear soon, but you may wish to see whether you can
demonstrate this on your own.) The elimination of such superﬂuous spanning elements is
encapsulated in the following important deﬁnition.
Deﬁnition 2.18. The vector space elements v1, . . . , vk ∈V are called linearly dependent
if there exist scalars c1, . . . , ck, not all zero, such that
c1v1 + · · · + ckvk = 0.
(2.12)
Elements that are not linearly dependent are called linearly independent.
The restriction that not all the ci’s are zero is essential: if c1 = · · · = ck = 0, then the
linear combination (2.12) is automatically zero. Thus, to check linear independence, one
needs to show that the only linear combination that produces the zero vector (2.12) is this
trivial one. In other words, c1 = · · · = ck = 0 is the one and only solution to the vector
equation (2.12).
Example 2.19.
Some examples of linear independence and dependence:
(a) The vectors
v1 =
⎛
⎝
1
2
−1
⎞
⎠,
v2 =
⎛
⎝
0
3
1
⎞
⎠,
v3 =
⎛
⎝
−1
4
3
⎞
⎠,
are linearly dependent, because
v1 −2v2 + v3 = 0.
On the other hand, the ﬁrst two vectors v1, v2 are linearly independent.
To see this,
suppose that
c1v1 + c2v2 =
⎛
⎝
c1
2c1 + 3c2
−c1 + c2
⎞
⎠=
⎛
⎝
0
0
0
⎞
⎠.
For this to happen, c1, c2 must satisfy the homogeneous linear system
c1 = 0,
2c1 + 3c2 = 0,
−c1 + c2 = 0,
which, as you can check, has only the trivial solution c1 = c2 = 0.
(b) In general, any collection v1, . . . , vk that includes the zero vector, say v1 = 0, is
automatically linearly dependent, since 1 0 + 0 v2 + · · · + 0 vk = 0 is a nontrivial linear
combination that adds up to 0.
(c) Two vectors v, w ∈V are linearly dependent if and only if they are parallel, meaning
that one is a scalar multiple of the other. Indeed, if v = aw, then v −aw = 0 is a
nontrivial linear combination summing to zero. Conversely, if cv + dw = 0 and c ̸= 0,
then v = −(d/c)w, while if c = 0 but d ̸= 0, then w = 0.
(d) The polynomials
p1(x) = x −2,
p2(x) = x2 −5x + 4,
p3(x) = 3x2 −4x,
p4(x) = x2 −1,
are linearly dependent, since
p1(x) + p2(x) −p3(x) + 2p4(x) ≡0

94
2 Vector Spaces and Bases
is a nontrivial linear combination that vanishes identically. On the other hand, the ﬁrst
three polynomials,
p1(x) = x −2,
p2(x) = x2 −5x + 4,
p3(x) = 3x2 −4x,
are linearly independent. Indeed, if the linear combination
c1 p1(x) + c2 p2(x) + c3 p3(x) = (c2 + 3c3)x2 + (c1 −5c2 −4c3)x −2c1 + 4c2 ≡0
is the zero polynomial, then its coeﬃcients must vanish, and hence c1, c2, c3 are required
to solve the homogeneous linear system
c2 + 3c3 = 0,
c1 −5c2 −4c3 = 0,
−2c1 + 4c2 = 0.
But this has only the trivial solution c1 = c2 = c3 = 0, and so linear independence follows.
Remark. In the last example, we are using the basic fact that a polynomial is identically
zero,
p(x) = a0 + a1 x + a2 x2 + · · · + an xn ≡0
for all
x,
if and only if its coeﬃcients all vanish: a0 = a1 = · · · = an = 0. This is equivalent to the
“obvious” fact that the basic monomial functions 1, x, x2, . . . , xn are linearly independent.
Exercise 2.3.36 asks for a bona ﬁde proof.
Example 2.20.
The trigonometric functions
1,
cos x,
sin x,
cos2 x,
cos x sin x,
sin2 x,
which were used to deﬁne the vector space T (2) of quadratic trigonometric polynomials,
are, in fact, linearly dependent. This is a consequence of the basic trigonometric identity
cos2 x + sin2 x ≡1,
which can be rewritten as a nontrivial linear combination
1 + 0 cos x + 0 sin x + (−1) cos2 x + 0 cos x sin x + (−1) sin2 x ≡0
that equals the zero function. On the other hand, the alternative spanning set
1,
cos x,
sin x,
cos 2x,
sin 2x
is linearly independent, since the only identically zero linear combination,
c0 + c1 cos x + c2 sin x + c3 cos 2x + c4 sin 2x ≡0,
turns out to be the trivial one c0 = · · · = c4 = 0. However, the latter fact is not as obvious,
and requires a bit of work to prove directly; see Exercise 2.3.37. An easier proof, based on
orthogonality, will appear in Chapter 4.
Let us now focus our attention on the linear independence or dependence of a set
of vectors v1, . . . , vk ∈Rn in Euclidean space. We begin by forming the n × k matrix
A = ( v1 . . . vk ) whose columns are the given vectors. (The fact that we use column
vectors is essential here.) Our analysis is based on the very useful formula
Ac = c1 v1 + · · · + ck vk,
where
c =
⎛
⎜
⎜
⎝
c1
c2
...
ck
⎞
⎟
⎟
⎠,
(2.13)

2.3 Span and Linear Independence
95
that expresses any linear combination in terms of matrix multiplication. For example,
⎛
⎝
1
3
0
−1
2
1
4
−1
−2
⎞
⎠
⎛
⎝
c1
c2
c3
⎞
⎠=
⎛
⎝
c1 + 3c2
−c1 + 2c2 + c3
4c1 −c2 −2c3
⎞
⎠= c1
⎛
⎝
1
−1
4
⎞
⎠+ c2
⎛
⎝
3
2
−1
⎞
⎠+ c3
⎛
⎝
0
1
−2
⎞
⎠.
Formula (2.13) follows directly from the rules of matrix multiplication; see also Exercise
1.2.34(c).
It enables us to reformulate the notions of linear independence and span of
vectors in Rn in terms of linear algebraic systems. The key result is the following:
Theorem 2.21. Let v1, . . . , vk ∈Rn and let A = ( v1 . . . vk ) be the corresponding n × k
matrix whose columns are the given vectors.
(a) The vectors v1, . . . , vk ∈Rn are linearly dependent if and only if there is a non-zero
solution c ̸= 0 to the homogeneous linear system A c = 0.
(b) The vectors are linearly independent if and only if the only solution to the homoge-
neous system A c = 0 is the trivial one, c = 0.
(c) A vector b lies in the span of v1, . . . , vk if and only if the linear system A c = b is
compatible, i.e., has at least one solution.
Proof : We prove the ﬁrst statement, leaving the other two as exercises for the reader. The
condition that v1, . . ., vk be linearly dependent is that there exists a nonzero vector
c = ( c1, c2, . . ., ck )T ̸= 0
such that
Ac = c1 v1 + · · · + ck vk = 0.
Therefore, linear dependence requires the existence of a nontrivial solution to the homoge-
neous linear system Ac = 0.
Q.E.D.
Example 2.22.
Let us determine whether the vectors
v1 =
⎛
⎝
1
2
−1
⎞
⎠,
v2 =
⎛
⎝
3
0
4
⎞
⎠,
v3 =
⎛
⎝
1
−4
6
⎞
⎠,
v4 =
⎛
⎝
4
2
3
⎞
⎠
(2.14)
are linearly independent or linearly dependent. We combine them as column vectors into
a single matrix
A =
⎛
⎝
1
3
1
4
2
0
−4
2
−1
4
6
3
⎞
⎠.
According to Theorem 2.21, we need to ﬁgure out whether there are any nontrivial solutions
to the homogeneous equation A c = 0; this can be done by reducing A to row echelon form
U =
⎛
⎝
1
3
1
4
0
−6
−6
−6
0
0
0
0
⎞
⎠.
(2.15)
The general solution to the homogeneous system A c = 0 is c = ( 2c3 −c4, −c3 −c4, c3, c4 )T ,
where c3, c4 — the free variables — are arbitrary. Any nonzero choice of c3, c4 will produce
a nontrivial linear combination
(2c3 −c4)v1 + (−c3 −c4)v2 + c3 v3 + c4 v4 = 0
that adds up to the zero vector. We conclude that the vectors (2.14) are linearly dependent.

96
2 Vector Spaces and Bases
In fact, in this particular case, we didn’t even need to complete the row reduction if we
only need to check linear (in)dependence. According to Theorem 1.47, any coeﬃcient ma-
trix with more columns than rows automatically has a nontrivial solution to the associated
homogeneous system. This implies the following result:
Lemma 2.23. Any collection of k > n vectors in Rn is linearly dependent.
Warning. The converse to this lemma is not true.
For example, v1 = ( 1, 2, 3 )T and
v2 = ( −2, −4, −6 )T are two linearly dependent vectors in R3, since 2v1 + v2 = 0. For a
collection of n or fewer vectors in Rn, one needs to analyze the homogeneous linear system.
Lemma 2.23 is a particular case of the following general characterization of linearly
independent vectors.
Proposition 2.24. A set of k vectors in Rn is linearly independent if and only if the
corresponding n × k matrix A has rank k. In particular, this requires k ≤n.
Or, to state the result another way, the vectors are linearly independent if and only
if the homogeneous linear system A c = 0 has no free variables. Proposition 2.24 is an
immediate corollary of Theorems 2.21 and 1.47.
Example 2.22 (continued).
Let us now see which vectors b ∈R3 lie in the span of
the vectors (2.14). According to Theorem 2.21, this will be the case if and only if the linear
system A c = b has a solution. Since the resulting row echelon form (2.15) has a row of
all zeros, there will be a compatibility condition on the entries of b, and hence not every
vector lies in the span. To ﬁnd the precise condition, we augment the coeﬃcient matrix,
and apply the same row operations, leading to the reduced augmented matrix
⎛
⎜
⎝
1
3
1
4
0
−6
−6
−6
0
0
0
0

b1
b2 −2b1
b3 + 7
6 b2 −4
3 b1
⎞
⎟
⎠.
Therefore, b = ( b1, b2, b3 )T lies in the span if and only if −4
3 b1 + 7
6 b2 + b3 = 0. Thus,
these four vectors span only a plane in R3.
The same method demonstrates that a collection of vectors will span all of Rn if and only
if the row echelon form of the associated matrix contains no all-zero rows, or, equivalently,
the rank is equal to n, the number of rows in the matrix.
Proposition 2.25. A collection of k vectors spans Rn if and only if their n × k matrix
has rank n. In particular, this requires k ≥n.
Warning. Not every collection of n or more vectors in Rn will span all of Rn. A coun-
terexample was already provided by the vectors (2.14).

2.3 Span and Linear Independence
97
Exercises
2.3.21. Determine whether the given vectors are linearly independent or linearly dependent:
(a)

1
2
	
,

2
1
	
, (b)

1
3
	
,

−2
−6
	
, (c)

2
1
	
,

−1
3
	
,

5
2
	
, (d)
⎛
⎜
⎝
1
3
−2
⎞
⎟
⎠,
⎛
⎜
⎝
0
2
−1
⎞
⎟
⎠,
(e)
⎛
⎜
⎝
0
1
1
⎞
⎟
⎠,
⎛
⎜
⎝
1
−1
0
⎞
⎟
⎠,
⎛
⎜
⎝
3
−1
2
⎞
⎟
⎠, (f )
⎛
⎜
⎝
2
1
3
⎞
⎟
⎠,
⎛
⎜
⎝
1
−2
1
⎞
⎟
⎠,
⎛
⎜
⎝
2
−3
0
⎞
⎟
⎠,
⎛
⎜
⎝
0
−1
4
⎞
⎟
⎠,
(g)
⎛
⎜
⎜
⎜
⎝
4
2
0
−6
⎞
⎟
⎟
⎟
⎠,
⎛
⎜
⎜
⎜
⎝
−6
−3
0
9
⎞
⎟
⎟
⎟
⎠, (h)
⎛
⎜
⎜
⎜
⎝
2
1
−1
3
⎞
⎟
⎟
⎟
⎠,
⎛
⎜
⎜
⎜
⎝
−1
3
1
0
⎞
⎟
⎟
⎟
⎠,
⎛
⎜
⎜
⎜
⎝
5
1
2
−3
⎞
⎟
⎟
⎟
⎠, (i)
⎛
⎜
⎜
⎜
⎝
1
0
1
0
⎞
⎟
⎟
⎟
⎠,
⎛
⎜
⎜
⎜
⎝
1
0
0
1
⎞
⎟
⎟
⎟
⎠,
⎛
⎜
⎜
⎜
⎝
2
2
1
0
⎞
⎟
⎟
⎟
⎠,
⎛
⎜
⎜
⎜
⎝
1
2
3
4
⎞
⎟
⎟
⎟
⎠,
2.3.22.(a) Show that the vectors
⎛
⎜
⎜
⎜
⎝
1
0
2
1
⎞
⎟
⎟
⎟
⎠,
⎛
⎜
⎜
⎜
⎝
−2
3
−1
1
⎞
⎟
⎟
⎟
⎠,
⎛
⎜
⎜
⎜
⎝
2
−2
1
−1
⎞
⎟
⎟
⎟
⎠are linearly independent. (b) Which
of the following vectors are in their span? (i)
⎛
⎜
⎜
⎜
⎝
1
1
2
1
⎞
⎟
⎟
⎟
⎠, (ii)
⎛
⎜
⎜
⎜
⎝
1
0
0
0
⎞
⎟
⎟
⎟
⎠, (iii)
⎛
⎜
⎜
⎜
⎝
0
1
0
0
⎞
⎟
⎟
⎟
⎠, (iv)
⎛
⎜
⎜
⎜
⎝
0
0
0
0
⎞
⎟
⎟
⎟
⎠.
(c) Suppose b = ( a, b, c, d )T lies in their span. What conditions must a, b, c, d satisfy?
2.3.23.(a) Show that the vectors
⎛
⎜
⎜
⎜
⎝
1
1
1
0
⎞
⎟
⎟
⎟
⎠,
⎛
⎜
⎜
⎜
⎝
1
1
−1
0
⎞
⎟
⎟
⎟
⎠,
⎛
⎜
⎜
⎜
⎝
1
−1
0
1
⎞
⎟
⎟
⎟
⎠,
⎛
⎜
⎜
⎜
⎝
1
−1
0
−1
⎞
⎟
⎟
⎟
⎠are linearly independent.
(b) Show that they also span R4. (c) Write ( 1, 0, 0, 1 )T as a linear combination of them.
2.3.24. Determine whether the given row vectors are linearly independent or linearly dependent:
(a) ( 2, 1 ) , ( −1, 3 ) , ( 5, 2 ),
(b) ( 1, 2, −1 ) , ( 2, 4, −2 ),
(c) ( 1, 2, 3 ) , ( 1, 4, 8 ) , ( 1, 5, 7 ),
(d) ( 1, 1, 0 ) , ( 1, 0, 3 ) , ( 2, 2, 1 ) , ( 1, 3, 4 ),
(e) ( 1, 2, 0, 3 ) , ( −3, −1, 2, −2 ) , ( 3, −4, −4, 5 ) ,
(f ) ( 2, 1, −1, 3 ) , ( −1, 3, 1, 0 ) , ( 5, 1, 2, −3 ).
2.3.25. True or false: The six 3 × 3 permutation matrices (1.30) are linearly independent.
2.3.26. True or false: A set of vectors is linearly dependent if the zero vector belongs to their span.
2.3.27. Does a single vector ever deﬁne a linearly dependent set?
2.3.28. Let x and y be linearly independent elements of a vector space V . Show that
u = ax + by, and v = cx + dy are linearly independent if and only if ad −bc ̸= 0. Is the
entire collection x, y, u, v linearly independent?
2.3.29. Prove or give a counterexample to the following statement: If v1, . . . , vk are elements of
a vector space V that do not span V , then v1, . . . , vk are linearly independent.
♦2.3.30. Prove parts (b) and (c) of Theorem 2.21.
♦2.3.31.(a) Prove that if v1, . . . , vm are linearly independent, then every subset, e.g., v1, . . . , vk
with k < m, is also linearly independent.
(b) Does the same hold true for linearly
dependent vectors? Prove or give a counterexample.
2.3.32.(a) Determine whether the polynomials f1(x) = x2 −3, f2(x) = 2 −x, f3(x) = (x −1)2,
are linearly independent or linearly dependent.
(b) Do they span the vector space of all quadratic polynomials?

98
2 Vector Spaces and Bases
2.3.33. Determine whether the given functions are linearly independent or linearly dependent:
(a) 2 −x2, 3x, x2 + x −2, (b) 3x −1, x(2x + 1), x(x −1); (c) ex, ex+1; (d) sin x,
sin(x + 1); (e) ex, ex+1, ex+2; (f ) sin x, sin(x + 1), sin(x + 2); (g) ex, xex, x2 ex;
(h) ex, e2x, e3x; (i) x + y, x −y + 1, x + 3y + 2 — these are functions of two variables.
2.3.34. Show that the functions f(x) = x and g(x) = | x | are linearly independent when
considered as functions on all of R, but are linearly dependent when considered as functions
deﬁned only on R+ = {x > 0}.
♥2.3.35.(a) Prove that the polynomials pi(x) =
n

j =0
aij xj for i = 1, . . . , k are linearly
independent if and only if the k × (n + 1) matrix A whose entries are their coeﬃcients
aij, 1 ≤i ≤k, 0 ≤j ≤n, has rank k. (b) Formulate a similar matrix condition for
testing whether another polynomial q(x) lies in their span.
(c) Use (a) to determine
whether p1(x) = x3 −1, p2(x) = x3 −2x + 4, p3(x) = x4 −4x, p4(x) = x2 + 1,
p5(x) = −x4 + 4x3 + 2x + 1 are linearly independent or linearly dependent.
(d) Does the
polynomial q(x) = x3 lie in their span? If so ﬁnd a linear combination that adds up to q(x).
♦2.3.36. The Fundamental Theorem of Algebra, [26], states that a non-zero polynomial of
degree n has at most n distinct real roots, that is, real numbers x such that p(x) = 0. Use
this fact to prove linear independence of the monomial functions 1, x, x2, . . . , xn.
Remark. An elementary proof of the latter fact can be found in Exercise 5.5.38.
♥2.3.37.(a) Let x1, x2, . . . , xn be a set of distinct sample points. Prove that the functions
f1(x), . . . , fk(x) are linearly independent if their sample vectors f1, . . . , fk are linearly
independent vectors in Rn. (b) Give an example of linearly independent functions that have
linearly dependent sample vectors. (c) Use this method to prove that the functions 1, cos x,
sin x, cos 2x, sin 2x, are linearly independent. Hint: You need at least 5 sample points.
2.3.38. Suppose f1(t), . . . , fk(t) are vector-valued functions from R to Rn. (a) Prove that if
f1(t0), . . . , fk(t0) are linearly independent vectors in Rn at one point t0, then f1(t), . . . , fk(t)
are linearly independent functions.
(b) Show that f1(t) =

1
t
	
and f2(t) =

2t −1
2t2 −t
	
are
linearly independent functions, even though at each t0, the vectors f1(t0), f2(t0) are linearly
dependent. Therefore, the converse to the result in part (a) is not valid.
♥2.3.39. The Wronskian of a pair of diﬀerentiable functions f(x), g(x) is the scalar function
W[f(x), g(x)] = det
⎛
⎝f(x)
g(x)
f′(x)
g′(x)
⎞
⎠= f(x) g′(x) −f′(x) g(x).
(2.16)
(a) Prove that if f, g are linearly dependent, then W[f(x), g(x)] ≡0. Hence, if
W[f(x), g(x)] ̸≡0, then f, g are linearly independent. (b) Let f(x) = x3, g(x) = | x |3.
Prove that f, g ∈C2 are twice continuously diﬀerentiable and linearly independent, but
W[f(x), g(x)] ≡0. Thus, the Wronskian is not a fool-proof test for linear independence.
Remark. It can be proved, [7], that if f, g both satisfy a second order linear ordinary
diﬀerential equation, then f, g are linearly dependent if and only if W[f(x), g(x)] ≡0.
2.4 Basis and Dimension
In order to span a vector space or subspace, we must employ a suﬃcient number of distinct
elements. On the other hand, including too many elements in the spanning set will violate
linear independence, and cause redundancies. The optimal spanning sets are those that are

2.4 Basis and Dimension
99
also linearly independent. By combining the properties of span and linear independence,
we arrive at the all-important concept of a “basis”.
Deﬁnition 2.26. A basis of a vector space V is a ﬁnite collection of elements v1, . . . , vn ∈
V that (a) spans V , and (b) is linearly independent.
Bases are absolutely fundamental in all areas of linear algebra and linear analysis, includ-
ing matrix algebra, Euclidean geometry, statistical analysis, solutions to linear diﬀerential
equations — both ordinary and partial — linear boundary value problems, Fourier analysis,
signal and image processing, data compression, control systems, and many others.
Example 2.27.
The standard basis of Rn consists of the n vectors
e1 =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎝
1
0
0
...
0
0
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎠
,
e2 =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎝
0
1
0
...
0
0
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎠
,
. . .
en =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎝
0
0
0
...
0
1
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎠
,
(2.17)
so that ei is the vector with 1 in the ith slot and 0’s elsewhere. We already encountered
these vectors — they are the columns of the n × n identity matrix. They clearly span Rn,
since we can write any vector
x =
⎛
⎜
⎜
⎜
⎝
x1
x2
...
xn
⎞
⎟
⎟
⎟
⎠= x1 e1 + x2 e2 + · · · + xn en
(2.18)
as a linear combination, whose coeﬃcients are its entries. Moreover, the only linear combi-
nation that yields the zero vector x = 0 is the trivial one x1 = · · · = xn = 0, which shows
that e1, . . . , en are linearly independent.
In the three-dimensional case R3, a common physical notation for the standard basis is
i = e1 =
⎛
⎝
1
0
0
⎞
⎠,
j = e2 =
⎛
⎝
0
1
0
⎞
⎠,
k = e3 =
⎛
⎝
0
0
1
⎞
⎠.
(2.19)
This is but one of many possible bases for R3. Indeed, any three non-coplanar vectors can
be used to form a basis. This is a consequence of the following general characterization of
bases in Euclidean space as the columns of a nonsingular matrix.
Theorem 2.28. Every basis of Rn consists of exactly n vectors. Furthermore, a set of
n vectors v1, . . . , vn ∈Rn is a basis if and only if the n × n matrix A = ( v1 . . . vn ) is
nonsingular: rank A = n.
Proof : This is a direct consequence of Theorem 2.21. Linear independence requires that
the only solution to the homogeneous system A c = 0 be the trivial one c = 0. On the
other hand, a vector b ∈Rn will lie in the span of v1, . . . , vn if and only if the linear system
A c = b has a solution. For v1, . . . , vn to span all of Rn, this must hold for all possible
right-hand sides b. Theorem 1.7 tells us that both results require that A be nonsingular,
i.e., have maximal rank n.
Q.E.D.

100
2 Vector Spaces and Bases
Thus, every basis of n-dimensional Euclidean space Rn contains the same number of
vectors, namely n. This is a general fact, that motivates a linear algebraic characterization
of dimension.
Theorem 2.29. Suppose the vector space V has a basis v1, . . . , vn for some n ∈N. Then
every other basis of V has the same number, n, of elements in it. This number is called
the dimension of V , and written dim V = n.
The proof of Theorem 2.29 rests on the following lemma.
Lemma 2.30. Suppose v1, . . . , vn span a vector space V . Then every set of k > n ele-
ments w1, . . . , wk ∈V is linearly dependent.
Proof : Let us write each element
wj =
n

i=1
aij vi,
j = 1, . . . , k,
as a linear combination of the spanning set. Then
c1 w1 + · · · + ck wk =
n

i=1
k

j =1
aij cjvi.
This linear combination will be zero whenever c = ( c1, c2, . . . , ck )T solves the homogeneous
linear system
k

j =1
aij cj = 0,
i = 1, . . . , n,
consisting of n equations in k > n unknowns. Theorem 1.47 guarantees that every ho-
mogeneous system with more unknowns than equations always has a non-trivial solution
c ̸= 0, and this immediately implies that w1, . . . , wk are linearly dependent.
Q.E.D.
Proof of Theorem 2.29:
Suppose we have two bases containing a diﬀerent number of
elements. By deﬁnition, the smaller basis spans the vector space. But then Lemma 2.30
tell us that the elements in the larger purported basis must be linearly dependent, which
contradicts our initial assumption that the latter is a basis.
Q.E.D.
As a direct consequence, we can now give a precise meaning to the optimality of bases.
Theorem 2.31. Suppose V is an n-dimensional vector space. Then
(a) Every set of more than n elements of V is linearly dependent.
(b) No set of fewer than n elements spans V .
(c) A set of n elements forms a basis if and only if it spans V .
(d) A set of n elements forms a basis if and only if it is linearly independent.
In other words, once we know the dimension of a vector space, to check that a collection
having the correct number of elements forms a basis, we only need establish one of the
two deﬁning properties: span or linear independence. Thus, n elements that span an n-
dimensional vector space are automatically linearly independent and hence form a basis;
conversely, n linearly independent elements of an n-dimensional vector space automatically
span the space and so form a basis.
Example 2.32.
The standard basis of the space P(n) of polynomials of degree ≤n is
given by the n + 1 monomials 1, x, x2, . . . , xn. We conclude that the vector space P(n)

2.4 Basis and Dimension
101
has dimension n + 1. Any other basis of P(n) must contain precisely n + 1 polynomials.
But, not every collection of n + 1 polynomials in P(n) is a basis — they must be linearly
independent. We conclude that no set of n or fewer polynomials can span P(n), while any
collection of n + 2 or more polynomials of degree ≤n is automatically linearly dependent.
By deﬁnition, every vector space of dimension 1 ≤n < ∞has a basis. If a vector space
V has no basis, it is either the trivial vector space V = {0}, which by convention has
dimension 0, or its dimension is inﬁnite. An inﬁnite-dimensional vector space contains an
inﬁnite collection of linearly independent elements, and hence no (ﬁnite) basis. Examples
of inﬁnite-dimensional vector spaces include most spaces of functions, such as the spaces of
continuous, diﬀerentiable, or mean zero functions, as well as the space of all polynomials,
and the space of solutions to a linear homogeneous partial diﬀerential equation. (On the
other hand, the solution space for a homogeneous linear ordinary diﬀerential equation
turns out to be a ﬁnite-dimensional vector space.) There is a well-developed concept of a
“complete basis” of certain inﬁnite-dimensional function spaces, [67, 68], but this requires
more delicate analytical considerations that lie beyond our present abilities. Thus, in this
book, the term “basis” always means a ﬁnite collection of vectors in a ﬁnite-dimensional
vector space.
Proposition 2.33. If v1, . . . , vm span the vector space V , then dim V ≤m.
Thus, every vector space spanned by a ﬁnite number of elements is necessarily ﬁnite-
dimensional, and so, if non-zero, admits a basis. Indeed, one can ﬁnd the basis by succes-
sively looking at the members of a collection of spanning vectors, and retaining those that
cannot be expressed as linear combinations of their predecessors in the list. Therefore,
n = dim V is the maximal number of linearly independent vectors in the set v1, . . . , vm.
The details of the proof are left to the reader; see Exercise 2.4.22.
Lemma 2.34. The elements v1, . . . , vn form a basis of V if and only if every x ∈V can
be written uniquely as a linear combination of the basis elements:
x = c1 v1 + · · · + cn vn =
n

i=1
ci vi.
(2.20)
Proof : The fact that a basis spans V implies that every x ∈V can be written as some
linear combination of the basis elements. Suppose we can write an element
x = c1 v1 + · · · + cn vn = c1 v1 + · · · + cn vn
(2.21)
as two diﬀerent combinations. Subtracting one from the other, we obtain
(c1 −c1)v1 + · · · + (cn −cn)vn = 0.
The left-hand side is a linear combination of the basis elements, and hence vanishes if and
only if all its coeﬃcients ci −ci = 0, meaning that the two linear combinations (2.21) are
one and the same.
Q.E.D.
One sometimes refers to the coeﬃcients (c1, . . . , cn) in (2.20) as the coordinates of the
vector x with respect to the given basis. For the standard basis (2.17) of Rn, the coordinates
of a vector x = ( x1, x2, . . . , xn )T are its entries, i.e., its usual Cartesian coordinates,
cf. (2.18).

102
2 Vector Spaces and Bases
Example 2.35.
A Wavelet Basis. The vectors
v1 =
⎛
⎜
⎝
1
1
1
1
⎞
⎟
⎠,
v2 =
⎛
⎜
⎝
1
1
−1
−1
⎞
⎟
⎠,
v3 =
⎛
⎜
⎝
1
−1
0
0
⎞
⎟
⎠,
v4 =
⎛
⎜
⎝
0
0
1
−1
⎞
⎟
⎠,
(2.22)
form a basis of R4. This is veriﬁed by performing Gaussian Elimination on the correspond-
ing 4 × 4 matrix
A =
⎛
⎜
⎝
1
1
1
0
1
1
−1
0
1
−1
0
1
1
−1
0
−1
⎞
⎟
⎠
to check that it is nonsingular. This is a very simple example of a wavelet basis. Wavelets
play an increasingly central role in modern signal and digital image processing; see Sec-
tion 9.7 and [18, 88].
How do we ﬁnd the coordinates of a vector, say x = ( 4, −2, 1, 5 )T , relative to the
wavelet basis? We need to ﬁnd the coeﬃcients c1, c2, c3, c4 such that
x = c1 v1 + c2 v2 + c3 v3 + c4 v4.
We use (2.13) to rewrite this equation in matrix form x = A c, where c = ( c1, c2, c3, c4 )T .
Solving the resulting linear system by Gaussian Elimination produces
c1 = 2,
c2 = −1,
c3 = 3,
c4 = −2,
which are the coordinates of
x =
⎛
⎜
⎝
4
−2
1
5
⎞
⎟
⎠= 2v1 −v2 + 3v3 −2v4 = 2
⎛
⎜
⎝
1
1
1
1
⎞
⎟
⎠−
⎛
⎜
⎝
1
1
−1
−1
⎞
⎟
⎠+ 3
⎛
⎜
⎝
1
−1
0
0
⎞
⎟
⎠−2
⎛
⎜
⎝
0
0
1
−1
⎞
⎟
⎠.
in the wavelet basis. See Section 9.7 for the general theory of wavelet bases.
In general, to ﬁnd the coordinates of a vector x with respect to a new basis of Rn
requires the solution of a linear system of equations, namely
A c = x
for
c = A−1 x.
(2.23)
The columns of A = ( v1 v2 . . . vn ) are the basis vectors, x = ( x1, x2, . . . , xn )T
are
the Cartesian coordinates of x, with respect to the standard basis e1, . . . , en, while c =
( c1, c2, . . . , cn )T contains its coordinates with respect to the new basis v1, . . ., vn.
In
practice, one ﬁnds the coordinates c by Gaussian Elimination, not matrix inversion.
Why would one want to change bases? The answer is simpliﬁcation and speed — many
computations and formulas become much easier, and hence faster, to perform in a basis
that is adapted to the problem at hand. In signal processing, wavelet bases are particularly
appropriate for denoising, compression, and eﬃcient storage of signals, including audio,
still images, videos, medical and geophysical images, and so on. These processes would be
quite time-consuming — if not impossible in complicated situations like video and three-
dimensional image processing — to accomplish in the standard basis. Additional examples
will appear throughout the text.

2.4 Basis and Dimension
103
Exercises
2.4.1. Determine which of the following sets of vectors are bases of R2:
(a)

1
−3
	
,

−2
5
	
;
(b)

1
−1
	
,

−1
1
	
; (c)

1
2
	
,

2
1
	
; (d)

3
5
	
,

0
0
	
; (e)

2
0
	
,

−1
2
	
,

0
−1
	
.
2.4.2. Determine which of the following are bases of R3:
(a)
⎛
⎜
⎝
2
1
5
⎞
⎟
⎠,
⎛
⎜
⎝
1
5
2
⎞
⎟
⎠; (b)
⎛
⎜
⎝
0
1
−5
⎞
⎟
⎠,
⎛
⎜
⎝
−1
3
0
⎞
⎟
⎠,
⎛
⎜
⎝
1
3
0
⎞
⎟
⎠; (c)
⎛
⎜
⎝
0
4
−1
⎞
⎟
⎠,
⎛
⎜
⎝
−1
0
1
⎞
⎟
⎠,
⎛
⎜
⎝
1
−8
1
⎞
⎟
⎠; (d)
⎛
⎜
⎝
2
0
−2
⎞
⎟
⎠,
⎛
⎜
⎝
−1
2
−1
⎞
⎟
⎠,
⎛
⎜
⎝
0
−1
0
⎞
⎟
⎠,
⎛
⎜
⎝
−1
2
1
⎞
⎟
⎠.
2.4.3. Let v1 =
⎛
⎜
⎝
1
0
2
⎞
⎟
⎠, v2 =
⎛
⎜
⎝
3
−1
1
⎞
⎟
⎠, v3 =
⎛
⎜
⎝
2
−1
−1
⎞
⎟
⎠, v4 =
⎛
⎜
⎝
4
−1
3
⎞
⎟
⎠. (a) Do v1, v2, v3, v4 span
R3? Why or why not?
(b) Are v1, v2, v3, v4 linearly independent? Why or why not?
(c) Do v1, v2, v3, v4 form a basis for R3? Why or why not? If not, is it possible to choose
some subset that is a basis?
(d) What is the dimension of the span of v1, v2, v3, v4?
Justify your answer.
2.4.4. Answer Exercise 2.4.3 when v1 =
⎛
⎜
⎝
1
−1
2
⎞
⎟
⎠, v2 =
⎛
⎜
⎝
2
−2
5
⎞
⎟
⎠, v3 =
⎛
⎜
⎝
0
−2
1
⎞
⎟
⎠, v4 =
⎛
⎜
⎝
1
3
−1
⎞
⎟
⎠.
2.4.5. Find a basis for (a) the plane given by the equation z −2y = 0 in R3; (b) the plane
given by the equation 4x + 3y −z = 0 in R3; (c) the hyperplane x + 2y + z −w = 0 in R4.
2.4.6.(a) Show that
⎛
⎜
⎝
4
0
1
⎞
⎟
⎠,
⎛
⎜
⎝
2
1
0
⎞
⎟
⎠, and
⎛
⎜
⎝
2
−1
1
⎞
⎟
⎠,
⎛
⎜
⎝
0
2
−1
⎞
⎟
⎠are two diﬀerent bases for the plane
x −2y −4z = 0. (b) Show how to write both elements of the second basis as linear
combinations of the ﬁrst. (c) Can you ﬁnd a third basis?
♥2.4.7. A basis v1, . . . , vn of Rn is called right-handed if the n × n matrix A = ( v1 v2 . . . vn )
whose columns are the basis vectors has positive determinant: det A > 0. If det A < 0,
the basis is called left-handed. (a) Which of the following form right-handed bases of R3?
(i)
⎛
⎜
⎝
1
0
1
⎞
⎟
⎠,
⎛
⎜
⎝
−1
1
1
⎞
⎟
⎠,
⎛
⎜
⎝
−1
1
0
⎞
⎟
⎠,
(ii)
⎛
⎜
⎝
2
1
1
⎞
⎟
⎠,
⎛
⎜
⎝
1
2
1
⎞
⎟
⎠,
⎛
⎜
⎝
1
1
2
⎞
⎟
⎠,
(iii)
⎛
⎜
⎝
−1
2
3
⎞
⎟
⎠,
⎛
⎜
⎝
1
−2
−2
⎞
⎟
⎠,
⎛
⎜
⎝
1
−2
2
⎞
⎟
⎠,
(iv)
⎛
⎜
⎝
3
2
1
⎞
⎟
⎠,
⎛
⎜
⎝
1
2
3
⎞
⎟
⎠,
⎛
⎜
⎝
2
1
3
⎞
⎟
⎠. (b) Show that if v1, v2, v3 is a left-handed basis of R3, then v2,
v1, v3 and −v1, v2, v3 are both right-handed bases. (c) What sort of basis has det A = 0?
2.4.8. Find a basis for and the dimension of the following subspaces: (a) The space of solutions
to the linear system A x = 0, where A =

1
2
−1
1
3
0
2
−1
	
. (b) The set of all quadratic
polynomials p(x) = ax2 + bx + c that satisfy p(1) = 0. (c) The space of all solutions to the
homogeneous ordinary diﬀerential equation u′′′ −u′′ + 4u′ −4u = 0.
2.4.9.(a) Prove that 1 + t2, t + t2, 1 + 2t + t2 is a basis for the space of quadratic polynomials
P(2). (b) Find the coordinates of p(t) = 1 + 4t + 7t2 in this basis.

104
2 Vector Spaces and Bases
2.4.10. Find a basis for and the dimension of the span of
(a)
⎛
⎜
⎝
3
1
−1
⎞
⎟
⎠,
⎛
⎜
⎝
−6
−2
2
⎞
⎟
⎠,
(b)
⎛
⎜
⎝
2
0
1
⎞
⎟
⎠,
⎛
⎜
⎝
0
−1
3
⎞
⎟
⎠,
⎛
⎜
⎝
2
1
−2
⎞
⎟
⎠,
(c)
⎛
⎜
⎜
⎜
⎝
1
0
−1
2
⎞
⎟
⎟
⎟
⎠,
⎛
⎜
⎜
⎜
⎝
0
1
1
3
⎞
⎟
⎟
⎟
⎠,
⎛
⎜
⎜
⎜
⎝
2
−1
−3
1
⎞
⎟
⎟
⎟
⎠,
⎛
⎜
⎜
⎜
⎝
1
−2
1
1
⎞
⎟
⎟
⎟
⎠.
2.4.11.(a) Show that 1, 1 −t, (1 −t)2, (1 −t)3 is a basis for P(3).
(b) Write p(t) = 1 + t3 in terms of the basis elements.
2.4.12. Let P(4) denote the vector space consisting of all polynomials p(x) of degree ≤4.
(a) Are x3 −3x + 1, x4 −6x + 3, x4 −2x3 + 1 linearly independent elements of P(4)?
(b) What is the dimension of the subspace of P(4) they span?
2.4.13. Let S =

0, 1
4, 1
2, 3
4

. (a) Show that the sample vectors corresponding to the functions
1, cos π x, cos 2π x, and cos 3π x form a basis for the vector space of all sample functions on
S.
(b) Write the sampled version of the function f(x) = x in terms of this basis.
2.4.14.(a) Prove that the vector space of all 2 × 2 matrices is a four-dimensional vector space
by exhibiting a basis. (b) Generalize your result and prove that the vector space Mm×n
consisting of all m × n matrices has dimension mn.
2.4.15. Determine all values of the scalar k for which the following four matrices form a basis
for M2×2: A1 =

1
−1
0
0
	
, A2 =

k
−3
1
0
	
, A3 =

1
0
−k
2
	
, A4 =

0
k
−1
−2
	
.
2.4.16. Prove that the space of diagonal n × n matrices is an n-dimensional vector space.
2.4.17.(a) Find a basis for and the dimension of the space of upper triangular 2 × 2 matrices.
(b) Can you generalize your result to upper triangular n × n matrices?
2.4.18.(a) What is the dimension of the vector space of 2 × 2 symmetric matrices? Of skew-
symmetric matrices?
(b) Generalize to the 3 × 3 case. (c) What about n × n matrices?
♥2.4.19. A matrix is said to be a semi-magic square if its row sums and column sums (i.e., the
sum of entries in an individual row or column) all add up to the same number. An example
is
⎛
⎜
⎝
8
1
6
3
5
7
4
9
2
⎞
⎟
⎠, whose row and column sums are all equal to 15. (a) Explain why the set
of all semi-magic squares is a subspace of the vector space of 3 × 3 matrices. (b) Prove
that the 3 × 3 permutation matrices (1.30) span the space of semi-magic squares. What is
its dimension? (c) A magic square also has the diagonal and anti-diagonal (running from
top right to bottom left) add up to the common row and column sum; the preceding 3 × 3
example is magic. Does the set of 3 × 3 magic squares form a vector space? If so, what is
its dimension? (d) Write down a formula for all 3 × 3 magic squares.
♦2.4.20.(a) Prove that if v1, . . . , vm forms a basis for V ⊊Rn, then m < n. (b) Under the
hypothesis of part (a), prove that there exist vectors vm+1, . . . , vn ∈Rn \ V such that the
complete collection v1, . . . , vn forms a basis for Rn. (c) Illustrate by constructing bases of
R3 that include (i) the basis

1, 1, 1
2
T of the line x = y = 2z; (ii) the basis ( 1, 0, −1 )T ,
( 0, 1, −2 )T of the plane x + 2y + z = 0.
♦2.4.21. Suppose that v1, . . . , vn form a basis for Rn. Let A be a nonsingular matrix. Prove
that Av1, . . . , Avn also form a basis for Rn. What is this basis if you start with the
standard basis: vi = ei?
♦2.4.22. Show that if v1, . . . , vn span V ̸= {0}, then one can choose a subset vi1, . . . , vim that
forms a basis of V . Thus, dim V = m ≤n. Under what conditions is dim V = n?
♦2.4.23. Prove that if v1, . . . , vn are a basis of V , then every subset thereof, e.g., vi1, . . . , vik, is
linearly independent.

2.5 The Fundamental Matrix Subspaces
105
♦2.4.24. Show, by example, how the uniqueness result in Lemma 2.34 fails if one has a linearly
dependent set of vectors.
♦2.4.25. Let W ⊂V be a subspace. (a) Prove that dim W ≤dim V .
(b) Prove that if dim W = dim V = n < ∞, then W = V . Equivalently, if W ⊊V is a
proper subspace of a ﬁnite-dimensional vector space, then dim W < dim V .
(c) Give an example in which the result is false if dim V = ∞.
♦2.4.26. Let W, Z ⊂V be complementary subspaces in a ﬁnite-dimensional vector space V , as in
Exercise 2.2.24. (a) Prove that if w1, . . . , wj form a basis for W and z1, . . . , zk a basis for
Z, then w1, . . . , wj, z1, . . . , zk form a basis for V .
(b) Prove that dim W + dim Z = dim V .
♦2.4.27. Let V be a ﬁnite-dimensional vector space and W ⊂V a subspace. Prove that the
quotient space, as deﬁned in Exercise 2.2.29, has dimension dim(V/W) = dim V −dim W.
♦2.4.28. Let f1(x), . . . , fn(x) be scalar functions. Suppose that every set of sample points
x1, . . . , xm ∈R, for all ﬁnite m ≥1, leads to linearly dependent sample vectors
f1, . . . , fn ∈Rm. Prove that f1(x), . . . , fn(x) are linearly dependent functions.
Hint: Given sample points x1, . . . , xm, let Vx1,...,xm ⊂Rn be the subspace consisting of all
vectors c = ( c1, c2, . . . , cn )T such that c1 f1 + · · · + cn fn = 0. First, show that one can
select sample points x1, x2, x3, . . . such that Rn ⊋Vx1 ⊋Vx1,x2 ⊋· · · . Then, apply Exercise
2.4.25 to conclude that Vx1,...,xn = {0}.
2.5 The Fundamental Matrix Subspaces
Let us now return to the general study of linear systems of equations, which we write in
our usual matrix form
A x = b.
(2.24)
As before, A is an m × n matrix, where m is the number of equations, so b ∈Rm, and
n is the number of unknowns, i.e., the entries of x ∈Rn. We already know how to solve
the system, at least when the coeﬃcient matrix is not too large: just apply a variant of
Gaussian Elimination. Our goal now is to better understand the solution(s) and thereby
prepare ourselves for more sophisticated problems and solution techniques.
Kernel and Image
There are four important vector subspaces associated with any matrix. The ﬁrst two are
deﬁned as follows.
Deﬁnition 2.36. The image of an m × n matrix A is the subspace img A ⊂Rm spanned
by its columns. The kernel of A is the subspace ker A ⊂Rn consisting of all vectors that
are annihilated by A, so
ker A = { z ∈Rn | A z = 0 } ⊂Rn.
(2.25)
The image is also known as the column space or the range† of the matrix. By deﬁnition,
†
The latter term can be confusing, since some authors call all of Rm the range of the (function
deﬁned by the) matrix, hence our preference to use image here, and, later, codomain to refer to
the space Rn. On the other hand, the space Rm will be called the domain of the (function deﬁned
by the) matrix.

106
2 Vector Spaces and Bases
a vector b ∈Rm belongs to img A if can be written as a linear combination,
b = x1 v1 + · · · + xn vn,
of the columns of A = ( v1 v2 . . . vn ). By our basic matrix multiplication formula (2.13),
the right-hand side of this equation equals the product A x of the matrix A with the column
vector x = ( x1, x2, . . . , xn )T , and hence b = A x for some x ∈Rn. Thus,
img A = { A x | x ∈Rn } ⊂Rm,
(2.26)
and so a vector b lies in the image of A if and only if the linear system A x = b has a
solution. The compatibility conditions for linear systems can thereby be re-interpreted as
the requirements for a vector to lie in the image of the coeﬃcient matrix.
A common alternative name for the kernel is the null space. The kernel or null space of
A is the set of solutions z to the homogeneous system A z = 0. The proof that ker A is a
subspace requires us to verify the usual closure conditions: suppose that z, w ∈ker A, so
that A z = 0 = Aw. Then, by the compatibility of scalar and matrix multiplication, for
any scalars c, d,
A(c z + d w) = cA z + dAw = 0,
which implies that c z + d w ∈ker A.
Closure of ker A can be re-expressed as the fol-
lowing important superposition principle for solutions to a homogeneous system of linear
equations.
Theorem 2.37. If z1, . . . , zk are individual solutions to the same homogeneous linear
system A z = 0, then so is every linear combination c1 z1 + · · · + ck zk.
Warning. The set of solutions to an inhomogeneous linear system A x = b with b ̸= 0 is
not a subspace. Linear combinations of solutions are not, in general, solutions to the same
inhomogeneous system.
Superposition is the reason why linear systems are so much easier to solve, since one
needs to ﬁnd only relatively few solutions in order to construct the general solution as a
linear combination. In Chapter 7 we shall see that superposition applies to completely
general linear systems, including linear diﬀerential equations, both ordinary and partial;
linear boundary value problems; linear integral equations; linear control systems; etc.
Example 2.38.
Let us compute the kernel of the matrix
A =
⎛
⎝
1
−2
0
3
2
−3
−1
−4
3
−5
−1
−1
⎞
⎠.
Our task is to solve the homogeneous system A x = 0, so we need only perform the
elementary row operations on A itself. The resulting row echelon form
U =
⎛
⎝
1
−2
0
3
0
1
−1
−10
0
0
0
0
⎞
⎠
corresponds to the equations x −2y + 3w = 0, y −z −10w = 0. The free variables are
z, w, and the general solution is
x =
⎛
⎜
⎝
x
y
z
w
⎞
⎟
⎠=
⎛
⎜
⎝
2z + 17w
z + 10w
z
w
⎞
⎟
⎠= z
⎛
⎜
⎝
2
1
1
0
⎞
⎟
⎠+ w
⎛
⎜
⎝
17
10
0
1
⎞
⎟
⎠.

2.5 The Fundamental Matrix Subspaces
107
The result describes the most general vector in ker A, which is thus the two-dimensional
subspace of R4 spanned by the linearly independent vectors ( 2, 1, 1, 0 )T, ( 17, 10, 0, 1 )T .
This example is indicative of a general method for ﬁnding a basis for ker A, to be developed
in more detail below.
Once we know the kernel of the coeﬃcient matrix A, i.e., the space of solutions to the
homogeneous system A z = 0, we are able to completely characterize the solutions to the
inhomogeneous linear system (2.24).
Theorem 2.39. The linear system A x = b has a solution x⋆if and only if b lies in the
image of A. If this occurs, then x is a solution to the linear system if and only if
x = x⋆+ z,
(2.27)
where z ∈ker A is an element of the kernel of the coeﬃcient matrix.
Proof : We already demonstrated the ﬁrst part of the theorem. If A x = b = A x⋆are any
two solutions, then their diﬀerence z = x −x⋆satisﬁes
A z = A(x −x⋆) = A x −Ax⋆= b −b = 0,
and hence z is in the kernel of A. Therefore, x and x⋆are related by formula (2.27), which
proves the second part of the theorem.
Q.E.D.
Therefore, to construct the most general solution to an inhomogeneous system, we need
only know one particular solution x⋆, along with the general solution z ∈ker A to the
corresponding homogeneous system. This construction should remind the reader of the
method for solving inhomogeneous linear ordinary diﬀerential equations.
Indeed, both
linear algebraic systems and linear ordinary diﬀerential equations are but two particular
instances in the general theory of linear systems, to be developed in Chapter 7.
Example 2.40.
Consider the system A x = b, where
A =
⎛
⎝
1
0
−1
−1
1
−1
1
−2
3
⎞
⎠,
x =
⎛
⎝
x1
x2
x3
⎞
⎠,
b =
⎛
⎝
b1
b2
b3
⎞
⎠,
where the right-hand side of the system will remain unspeciﬁed for the moment. Applying
our usual Gaussian Elimination procedure to the augmented matrix
⎛
⎝
1
0
−1
−1
1
−1
1
−2
3

b1
b2
b3
⎞
⎠leads to the row echelon form
⎛
⎝
1
0
−1
0
1
−2
0
0
0

b1
b2
b3 + 2b2 + b1
⎞
⎠.
Therefore, the system has a solution if and only if the compatibility condition
b1 + 2b2 + b3 = 0
(2.28)
holds. This equation serves to characterize the vectors b that belong to the image of the
matrix A, which is therefore a plane in R3.
To characterize the kernel of A, we take b = 0, and solve the homogeneous system

108
2 Vector Spaces and Bases
A z = 0. The row echelon form corresponds to the reduced system
z1 −z3 = 0,
z2 −2z3 = 0.
The free variable is z3, and the equations are solved to give
z1 = c,
z2 = 2c,
z3 = c,
where c is an arbitrary scalar. Thus, the general solution to the homogeneous system is
z = ( c, 2c, c )T = c ( 1, 2, 1 )T , and so the kernel is the line in the direction of the
vector ( 1, 2, 1 )T .
If we take b = ( 3, −2, 1 )T — which satisﬁes (2.28) and hence lies in the image of A —
then the general solution to the inhomogeneous system A x = b is
x1 = 3 + c,
x2 = 1 + 2c,
x3 = c,
where c is arbitrary. We can write the solution in the form (2.27), namely
x =
⎛
⎝
3 + c
1 + 2c
c
⎞
⎠=
⎛
⎝
3
1
0
⎞
⎠+ c
⎛
⎝
1
2
1
⎞
⎠= x⋆+ z,
(2.29)
where, as in (2.27), x⋆= ( 3, 1, 0 )T
plays the role of the particular solution, while
z = c ( 1, 2, 1 )T is the general element of the kernel.
Finally, we remark that the particular solution is not uniquely deﬁned — any individual
solution to the system will serve the purpose. Thus, in this example, we could choose, for
instance, x⋆⋆= ( −2, −9, −5 )T instead, corresponding to c = −5 in the preceding formula
(2.29). The general solution can be expressed in the alternative form
x = x⋆⋆+ z =
⎛
⎝
−2
−9
−5
⎞
⎠+ c
⎛
⎝
1
2
1
⎞
⎠,
where
z = c
⎛
⎝
1
2
1
⎞
⎠∈ker A,
which agrees with (2.29) when we identify c = c + 5.
We can characterize the situations in which the linear system has a unique solution in
any of the following equivalent ways.
Proposition 2.41. If A is an m × n matrix, then the following conditions are equivalent:
(i) ker A = {0}, i.e., the homogeneous system A x = 0 has the unique solution x = 0.
(ii) rank A = n.
(iii) The linear system A x = b has no free variables.
(iv) The system A x = b has a unique solution for each b ∈img A.
Thus, while existence of a solution may depend upon the particularities of the right-
hand side b, uniqueness is universal: if for any one b, e.g., b = 0, the system admits a
unique solution, then all b ∈img A also admit unique solutions. Specializing even further
to square matrices, we can now characterize invertible matrices by looking at either their
kernels or their images.
Proposition 2.42. If A is a square n × n matrix, then the following four conditions are
equivalent: (i) A is nonsingular; (ii) rank A = n; (iii) ker A = {0}; (iv) img A = Rn.

2.5 The Fundamental Matrix Subspaces
109
Exercises
2.5.1. Characterize the image and kernel of the following matrices:
(a)

8
−4
−6
3
	
, (b)

1
−1
2
−2
2
−4
	
, (c)
⎛
⎜
⎝
1
2
3
−2
4
1
4
0
5
⎞
⎟
⎠, (d)
⎛
⎜
⎜
⎜
⎝
1
−1
0
1
−1
0
1
−1
1
−2
1
1
1
2
−3
1
⎞
⎟
⎟
⎟
⎠.
2.5.2. For the following matrices, write the kernel as the span of a ﬁnite number of vectors.
Is the kernel a point, line, plane, or all of R3?
(a) ( 2
−1
5 ),
(b)

1
2
−1
3
−2
0
	
,
(c)

2
6
−4
−1
−3
2
	
, (d)
⎛
⎜
⎝
1
2
5
0
4
8
1
−6
−11
⎞
⎟
⎠, (e)
⎛
⎜
⎝
2
−1
1
−1
1
−2
3
−1
1
⎞
⎟
⎠, (f )
⎛
⎜
⎜
⎜
⎝
1
−2
3
−3
6
−9
−2
4
−6
3
0
−1
⎞
⎟
⎟
⎟
⎠.
2.5.3.(a) Find the kernel and image of the coeﬃcient matrix for the system x −3y + 2z = a,
2x −6y + 2w = b, z −3w = c. (b) Write down compatibility conditions on a, b, c for a
solution to exist.
2.5.4. Suppose x⋆=
⎛
⎜
⎝
1
2
3
⎞
⎟
⎠is a particular solution to the equation
⎛
⎜
⎝
1
−1
0
−1
0
1
0
1
−1
⎞
⎟
⎠x = b.
(a) What is b?
(b) Find the general solution.
2.5.5. Prove that the average of all the entries in each row of A is 0 if and only if
( 1, 1, . . . , 1 )T ∈ker A.
2.5.6. True or false: If A is a square matrix, then ker A ∩img A = {0}.
2.5.7. Write the general solution to the following linear systems in the form (2.27). Clearly
identify the particular solution x⋆and the element z of the kernel.
(a) x −y + 3z = 1,
(b)

1
−2
0
2
3
1
	⎛
⎜
⎝
x
y
z
⎞
⎟
⎠=

3
−1
	
,
(c)
⎛
⎜
⎝
1
−1
0
2
0
−4
2
−1
−2
⎞
⎟
⎠
⎛
⎜
⎝
x
y
z
⎞
⎟
⎠=
⎛
⎜
⎝
−1
−6
−4
⎞
⎟
⎠,
(d)
⎛
⎜
⎝
2
−1
1
4
−1
2
0
1
3
⎞
⎟
⎠
⎛
⎜
⎝
x
y
z
⎞
⎟
⎠=
⎛
⎜
⎝
0
1
−1
⎞
⎟
⎠,
(e)
⎛
⎜
⎜
⎜
⎝
1
−2
2
−4
−3
6
−1
2
⎞
⎟
⎟
⎟
⎠

u
v
	
=
⎛
⎜
⎜
⎜
⎝
−1
−2
3
1
⎞
⎟
⎟
⎟
⎠,
(f )
⎛
⎜
⎝
1
−3
2
0
−1
5
1
1
2
−8
1
−1
⎞
⎟
⎠
⎛
⎜
⎜
⎜
⎝
p
q
r
s
⎞
⎟
⎟
⎟
⎠=
⎛
⎜
⎝
4
−3
7
⎞
⎟
⎠,
(g)
⎛
⎜
⎜
⎜
⎝
0
−1
2
−1
1
−3
0
1
−2
5
2
−3
1
1
−8
5
⎞
⎟
⎟
⎟
⎠
⎛
⎜
⎜
⎜
⎝
x
y
z
w
⎞
⎟
⎟
⎟
⎠=
⎛
⎜
⎜
⎜
⎝
−2
−3
4
5
⎞
⎟
⎟
⎟
⎠.
2.5.8. Given a, r ̸= 0, characterize the kernel and the image of the matrix
⎛
⎜
⎜
⎜
⎜
⎜
⎝
a
ar
. . .
arn−1
arn
arn+1
. . .
ar2n−1
...
...
...
...
ar(n−1)n
ar(n−1)n+1
. . .
arn2−1
⎞
⎟
⎟
⎟
⎟
⎟
⎠
.
Hint: See Exercise 1.8.17.
♦2.5.9. Let the square matrix P be idempotent, meaning that P 2 = P. (a) Prove that
w ∈img P if and only if P w = w. (b) Show that img P and ker P are complementary
subspaces, as deﬁned in Exercise 2.2.24, so every v ∈Rn can be uniquely written as
v = w + z where w ∈img P, z ∈ker P.
♦2.5.10. Let A be an m × n matrix. Suppose that C =

A
B
	
is an (m + k) × n matrix whose
ﬁrst m rows are the same as those of A. Prove that ker C ⊆ker A. Thus, appending more
rows cannot increase the size of a matrix’s kernel. Give an example in which ker C ̸= ker A.

110
2 Vector Spaces and Bases
♦2.5.11. Let A be an m × n matrix. Suppose that C = ( A B ) is an m × (n + k) matrix whose
ﬁrst n columns are the same as those of A. Prove that img C ⊇img A. Thus, appending
more columns cannot decrease the size of a matrix’s image. Give an example in which
img C ̸= img A.
The Superposition Principle
The principle of superposition lies at the heart of linearity. For homogeneous systems,
superposition allows one to generate new solutions by combining known solutions. For
inhomogeneous systems, superposition combines the solutions corresponding to diﬀerent
inhomogeneities.
Suppose we know particular solutions x⋆
1 and x⋆
2 to two inhomogeneous linear systems
A x = b1,
A x = b2,
that have the same coeﬃcient matrix A. Consider the system
A x = c1 b1 + c2 b2,
whose right-hand side is a linear combination, or superposition, of the previous two. Then
a particular solution to the combined system is given by the same superposition of the
previous solutions:
x⋆= c1 x⋆
1 + c2 x⋆
2.
The proof is easy:
A x⋆= A(c1 x⋆
1 + c2 x⋆
2) = c1 A x⋆
1 + c2 A x⋆
2 = c1 b1 + c2 b2.
In physical applications, the inhomogeneities b1, b2 typically represent external forces,
and the solutions x⋆
1, x⋆
2 represent the respective responses of the physical apparatus. The
linear superposition principle says that if we know how the system responds to the indi-
vidual forces, we immediately know its response to any combination thereof. The precise
details of the system are irrelevant — all that is required is its linearity.
Example 2.43.
For example, the system

4
1
1
4
 
x1
x2

=

f1
f2

models the mechanical response of a pair of masses connected by springs, subject to external
forcing. The solution x = ( x1, x2 )T represents the displacements of the masses, while the
entries of the right-hand side f = ( f1, f2 )T are the applied forces. (Details can be found
in Chapter 6.) We can directly determine the response of the system x⋆
1 =
 4
15, −1
15
T to
a unit force e1 = ( 1, 0 )T on the ﬁrst mass, and the response x⋆
2 =

−1
15, 4
15
T to a unit
force e2 = ( 0, 1 )T on the second mass. Superposition gives the response of the system to
a general force, since we can write
f =

f1
f2

= f1 e1 + f2 e2 = f1

1
0

+ f2

0
1

,
and hence
x = f1 x⋆
1 + f2 x⋆
2 = f1

4
15
−1
15

+ f2

−
1
15
4
15

=

4
15 f1 −1
15 f2
−1
15 f1 + 4
15 f2

.

2.5 The Fundamental Matrix Subspaces
111
The preceding construction is easily extended to several inhomogeneities, and the result
is the general Superposition Principle for inhomogeneous linear systems.
Theorem 2.44. Suppose that x⋆
1, . . . , x⋆
k are particular solutions to each of the inhomo-
geneous linear systems
A x = b1,
A x = b2,
. . .
A x = bk,
(2.30)
all having the same coeﬃcient matrix, and where b1, . . . , bk ∈img A. Then, for any choice
of scalars c1, . . . , ck, a particular solution to the combined system
A x = c1b1 + · · · + ckbk
(2.31)
is the corresponding superposition
x⋆= c1 x⋆
1 + · · · + ck x⋆
k
(2.32)
of individual solutions. The general solution to (2.31) is
x = x⋆+ z = c1 x⋆
1 + · · · + ck x⋆
k + z,
(2.33)
where z ∈ker A is the general solution to the homogeneous system A z = 0.
For instance, if we know particular solutions x⋆
1, . . . , x⋆
m to
A x = ei,
for each
i = 1, . . ., m,
(2.34)
where e1, . . . , em are the standard basis vectors of Rm, then we can reconstruct a particular
solution x⋆to the general linear system A x = b by ﬁrst writing
b = b1 e1 + · · · + bm em
as a linear combination of the basis vectors, and then using superposition to form
x⋆= b1 x⋆
1 + · · · + bm x⋆
m.
(2.35)
However, for linear algebraic systems, the practical value of this insight is rather limited.
Indeed, in the case that A is square and nonsingular, the superposition formula (2.35) is
merely a reformulation of the method of computing the inverse of the matrix. Indeed, the
vectors x⋆
1, . . . , x⋆
m that satisfy (2.34) are just the columns of A−1 (why?), while (2.35) is
precisely the solution formula x⋆= A−1b that we abandoned in practical computations,
in favor of the more eﬃcient Gaussian Elimination process. Nevertheless, this idea turns
out to have important implications in more general situations, such as linear diﬀerential
equations and boundary value problems.
Exercises
2.5.12. Find the solution x⋆
1 to the system

1
2
−3
−4
	 
x
y
	
=

1
0
	
, and the solution x⋆
2 to

1
2
−3
−4
	 
x
y
	
=

0
1
	
. Express the solution to

1
2
−3
−4
	 
x
y
	
=

1
4
	
as a linear
combination of x⋆
1 and x⋆
2.

112
2 Vector Spaces and Bases
2.5.13. Let A =
⎛
⎜
⎝
1
2
−1
2
5
−1
1
3
2
⎞
⎟
⎠. Given that x⋆
1 =
⎛
⎜
⎝
5
−1
2
⎞
⎟
⎠solves A x = b1 =
⎛
⎜
⎝
1
3
6
⎞
⎟
⎠and
x⋆
2 =
⎛
⎜
⎝
−11
5
−1
⎞
⎟
⎠solves A x = b2 =
⎛
⎜
⎝
0
4
2
⎞
⎟
⎠, ﬁnd a solution to A x = 2b1 + b2 =
⎛
⎜
⎝
2
10
14
⎞
⎟
⎠.
2.5.14.(a) Show that x⋆
1 =
⎛
⎜
⎝
1
1
0
⎞
⎟
⎠and x⋆
2 =
⎛
⎜
⎝
−3
3
−2
⎞
⎟
⎠are particular solutions to the system
⎛
⎜
⎝
2
−1
−5
1
−4
−6
3
2
−4
⎞
⎟
⎠x =
⎛
⎜
⎝
1
−3
5
⎞
⎟
⎠.
(b) Find the general solution.
2.5.15. A physical apparatus moves 2 meters under a force of 4 newtons. Assuming linearity,
how far will it move under a force of 10 newtons?
2.5.16. Applying a unit external force in the horizontal direction moves a mass 3 units to the
right, while applying a unit force in the vertical direction moves it up 2 units. Assuming
linearity, where will the mass move under the applied force f = ( 2, −3 )T ?
2.5.17. Suppose x⋆
1 and x⋆
2 are both solutions to A x = b. List all linear combinations of x⋆
1
and x⋆
2 that solve the system.
♦2.5.18. Let A be a nonsingular m × m matrix. (a) Explain in detail why the solutions
x⋆
1, . . . , x⋆
m to the systems (2.34) are the columns of the matrix inverse A−1.
(b) Illustrate your argument in the case A =
⎛
⎜
⎝
0
1
2
−1
1
3
1
0
1
⎞
⎟
⎠.
2.5.19. True or false: If x⋆
1 solves A x = c, and x⋆
2 solves B x = d, then x⋆= x⋆
1 + x⋆
2 solves
(A + B)x = c + d.
♦2.5.20. Under what conditions on the coeﬃcient matrix A will the systems in (2.34) all have a
solution?
Adjoint Systems, Cokernel, and Coimage
A linear system of m equations in n unknowns is based on an m × n coeﬃcient matrix A.
The transposed matrix AT will be of size n × m, and forms the coeﬃcient matrix of an
associated linear system, consisting of n equations in m unknowns.
Deﬁnition 2.45. The adjoint† to a linear system A x = b of m equations in n unknowns
is the linear system
AT y = f
(2.36)
consisting of n equations in m unknowns y ∈Rm with right-hand side f ∈Rn.
Example 2.46.
Consider the linear system
x1 −3x2 −7x3 + 9x4 = b1,
x2 + 5x3 −3x4 = b2,
x1 −2x2 −2x3 + 6x4 = b3,
(2.37)
†
Warning. Some texts misuse the term “adjoint” to describe the adjugate or cofactor matrix,
[80]. The constructions are completely unrelated, and the adjugate will play no role in this book.

2.5 The Fundamental Matrix Subspaces
113
of three equations in four unknowns. Its coeﬃcient matrix
A =
⎛
⎝
1
−3
−7
9
0
1
5
−3
1
−2
−2
6
⎞
⎠
has transpose
AT =
⎛
⎜
⎝
1
0
1
−3
1
−2
−7
5
−2
9
−3
6
⎞
⎟
⎠.
Thus, the adjoint system to (2.37) is the following system of four equations in three un-
knowns:
y1 +
y3 = f1,
−3y1 + y2 −2y3 = f2,
−7y1 + 5y2 −2y3 = f3,
9y1 −3y2 + 6y3 = f4.
(2.38)
On the surface, there appears to be no direct connection between the solutions to a
linear system and its adjoint. Nevertheless, as we shall soon see (and then in even greater
depth in Sections 4.4 and 8.7), the two are linked in a number of remarkable, but subtle
ways. As a ﬁrst step in this direction, we use the adjoint system to deﬁne the remaining
two fundamental subspaces associated with a coeﬃcient matrix A.
Deﬁnition 2.47. The coimage of an m × n matrix A is the image of its transpose,
coimg A = img AT =

AT y
 y ∈Rm 
⊂Rn.
(2.39)
The cokernel of A is the kernel of its transpose,
cokerA = ker AT =

w ∈Rm  AT w = 0

⊂Rm,
(2.40)
that is, the set of solutions to the homogeneous adjoint system.
The coimage coincides with the subspace of Rn spanned by the rows† of A, and is thus
often referred to as the row space. As a direct consequence of Theorem 2.39, the adjoint
system AT y = f has a solution if and only if f ∈img AT = coimg A. The cokernel is also
sometimes called the left null space of A, since it can be identiﬁed with the set of all row
vectors r satisfying rA = 0T , where 0T is the row vector with m zero entries. Indeed,
we can identify r = wT and so, taking the transpose of the preceding equation, deduce
AT w = (wT A)T = (rA)T = 0, and so w = rT ∈coker A.
Example 2.48.
To solve the linear system (2.37) just presented, we perform Gaussian
Elimination on the augmented matrix
⎛
⎝
1
−3
−7
9
0
1
5
−3
1
−2
−2
6

b1
b2
b3
⎞
⎠, reducing it to the row
echelon form
⎛
⎝
1
−3
−7
9
0
1
5
−3
0
0
0
0

b1
b2
b3 −b2 −b1
⎞
⎠. Thus, the system has a solution if and
only if
−b1 −b2 + b3 = 0,
†
Or, more precisely, the column vectors obtained by transposing the rows.

114
2 Vector Spaces and Bases
which is required in order that b ∈img A. For such vectors, the general solution is
x =
⎛
⎜
⎝
b1 + 3b2 −8x3
b2 −5x3 + 3x4
x3
x4
⎞
⎟
⎠=
⎛
⎜
⎝
b1 + 3b2
b2
0
0
⎞
⎟
⎠+ x3
⎛
⎜
⎝
−8
−5
1
0
⎞
⎟
⎠+ x4
⎛
⎜
⎝
0
3
0
1
⎞
⎟
⎠.
In the second expression, the ﬁrst vector represents a particular solution, while the two
remaining terms constitute the general element of ker A.
The solution to the adjoint system (2.38) is also obtained by Gaussian Elimination,
starting with its augmented matrix
⎛
⎜
⎝
1
0
1
−3
1
−2
−7
5
−2
9
−3
6

f1
f2
f3
f4
⎞
⎟
⎠. The resulting row echelon
form is
⎛
⎜
⎝
1
0
1
0
1
1
0
0
0
0
0
0

f1
f2 + 3f1
f3 −5f2 −8f1
f4 + 3f2
⎞
⎟
⎠. Thus, there are two consistency constraints re-
quired for a solution to the adjoint system:
−8f1 −5f2 + f3 = 0,
3f2 + f4 = 0.
These are the conditions required for the right-hand side to belong to the coimage:
f ∈img AT = coimg A.
If these conditions are satisﬁed, the adjoint system has the
following general solution depending on the single free variable y3:
y =
⎛
⎝
f1 −y3
3f1 + f2 −y3
y3
⎞
⎠=
⎛
⎝
f1
3f1 + f2
0
⎞
⎠+ y3
⎛
⎝
−1
−1
1
⎞
⎠.
In the latter formula, the ﬁrst term represents a particular solution, while the second is
the general element of the cokernel ker AT = cokerA.
The Fundamental Theorem of Linear Algebra
The four fundamental subspaces associated with an m × n matrix A, then, are its image,
coimage, kernel, and cokernel. The image and cokernel are subspaces of Rm, while the
kernel and coimage are subspaces of Rn. The Fundamental Theorem of Linear Algebra†
states that their dimensions are determined by the rank (and size) of the matrix.
Theorem 2.49. Let A be an m × n matrix, and let r be its rank. Then
dim coimg A = dim img A = rank A = rank AT = r,
dim ker A = n −r,
dim cokerA = m −r.
(2.41)
Thus, the rank of a matrix, i.e., the number of pivots, indicates the number of linearly
independent columns, which, remarkably, is always the same as the number of linearly
independent rows. A matrix and its transpose are guaranteed to have the same rank, i.e.,
†
Not to be confused with the Fundamental Theorem of Algebra, which states that every
(nonconstant) polynomial has a complex root; see [26].

2.5 The Fundamental Matrix Subspaces
115
the same number of pivots, despite the fact that their row echelon forms are quite diﬀerent,
and are almost never transposes of each other. Theorem 2.49 also establishes our earlier
contention that the rank of a matrix is an intrinsic quantity, since it equals the common
dimension of its image and coimage, and so does not depend on which speciﬁc elementary
row operations are employed during the reduction process, nor on the ﬁnal row echelon
form.
Let us turn to the proof of the Fundamental Theorem 2.49. Since the dimension of a
subspace is prescribed by the number of vectors in any basis, we need to relate bases of
the fundamental subspaces to the rank of the matrix. Before trying to digest the general
argument, it is better ﬁrst to understand how to construct the required bases in a particular
example. Consider the matrix
A =
⎛
⎝
2
−1
1
2
−8
4
−6
−4
4
−2
3
2
⎞
⎠.
Its row echelon form
U =
⎛
⎝
2
−1
1
2
0
0
−2
4
0
0
0
0
⎞
⎠
(2.42)
is obtained in the usual manner. There are two pivots, and thus the rank of A is r = 2.
Kernel: The general solution to the homogeneous system A x = 0 can be expressed as
a linear combination of n −r linearly independent vectors, whose coeﬃcients are the free
variables for the system corresponding to the n −r columns without pivots. In fact, these
vectors form a basis for the kernel, which thus has dimension n −r.
In our example, the pivots are in columns 1 and 3, and so the free variables are x2, x4.
Applying Back Substitution to the reduced homogeneous system U x = 0, we obtain the
general solution
x =
⎛
⎜
⎜
⎜
⎝
1
2 x2 −2x4
x2
2x4
x4
⎞
⎟
⎟
⎟
⎠= x2
⎛
⎜
⎜
⎜
⎝
1
2
1
0
0
⎞
⎟
⎟
⎟
⎠+ x4
⎛
⎜
⎜
⎜
⎝
−2
0
2
1
⎞
⎟
⎟
⎟
⎠
(2.43)
written as a linear combination of the vectors
z1 =
 1
2, 1, 0, 0
T ,
z2 = ( −2, 0, 2, 1 )T .
We claim that z1, z2 form a basis of ker A. By construction, they span the kernel, and linear
independence follows easily, since the only way in which the linear combination (2.43) could
vanish is if both free variables vanish: x2 = x4 = 0.
Coimage: The coimage is the subspace of Rn spanned by the rows† of A. As we prove
below, applying an elementary row operation to a matrix does not alter its coimage. Since
the row echelon form U is obtained from A by a sequence of elementary row operations, we
conclude that coimg A = coimg U. Moreover, the row echelon structure implies that the
r nonzero rows of U are necessarily linearly independent, and hence form a basis of both
coimg U and coimg A, which therefore have dimension r = rank A. In our example, then,
a basis for coimg A consists of the vectors
s1 = ( 2, −1, 1, 2 )T,
s2 = ( 0, 0, −2, 4 )T ,
†
Or, more correctly, the transposes of the rows, since the elements of Rn are supposed to be
column vectors.

116
2 Vector Spaces and Bases
coming from the nonzero rows of U. The reader can easily check their linear independence,
as well as the fact that every row of A lies in their span.
Image: There are two methods for computing a basis of the image, or column space.
The ﬁrst proves that it has dimension equal to the rank. This has the important, and
remarkable consequence that the space spanned by the rows of a matrix and the space
spanned by its columns always have the same dimension, even though they are usually
diﬀerent subspaces of diﬀerent vector spaces.
Now, the row echelon structure implies that the columns of U that contain the pivots
form a basis for its image, i.e., img U. In our example, these are its ﬁrst and third columns,
and you can check that they are linearly independent and span the full column space. But
the image of A is not the same as the image of U, and so, unlike the coimage, we cannot
directly use a basis for img U as a basis for img A. However, the linear dependencies among
the columns of A and U are the same, and this implies that the r columns of A that end
up containing the pivots will form a basis for img A. In our example (2.42), the pivots lie
in the ﬁrst and third columns of U, and hence the ﬁrst and third columns of A; namely,
v1 =
⎛
⎝
2
−8
4
⎞
⎠,
v3 =
⎛
⎝
1
−6
3
⎞
⎠,
form a basis for img A. This means that every column of A can be written uniquely as a
linear combination of its ﬁrst and third columns. Again, skeptics may wish to check this.
An alternative method to ﬁnd a basis for the image is to recall that img A = coimg AT ,
and hence we can employ the previous algorithm to compute coimg AT . In our example,
applying Gaussian Elimination to
AT =
⎛
⎜
⎝
2
−8
4
−1
4
−2
1
−6
3
2
−4
2
⎞
⎟
⎠
leads to the row echelon form
U =
⎛
⎜
⎝
2
−8
4
0
−2
1
0
0
0
0
0
0
⎞
⎟
⎠.
(2.44)
Note that the row echelon form of AT is not the transpose of the row echelon form of A.
However, they do have the same number of pivots, since, as we now know, both A and AT
have the same rank, namely 2. The two nonzero rows of U (again transposed to be column
vectors) form a basis for coimg AT , and therefore
y1 =
⎛
⎝
2
−8
4
⎞
⎠,
y2 =
⎛
⎝
0
−2
1
⎞
⎠,
forms an alternative basis for img A.
Cokernel: Finally, to determine a basis for the cokernel, we apply the algorithm for
ﬁnding a basis for ker AT = coker A. Since the ranks of A and AT coincide, there are
now m −r free variables, which is the same as the dimension of ker AT . In our particular
example, using the reduced form (2.44), the only free variable is y3, and the general solution
to the homogeneous adjoint system AT y = 0 is
y =
⎛
⎜
⎝
0
1
2 y3
y3
⎞
⎟
⎠= y3
⎛
⎜
⎝
0
1
2
1
⎞
⎟
⎠.

2.5 The Fundamental Matrix Subspaces
117
We conclude that coker A is one-dimensional, with basis

0, 1
2, 1
T .
Summarizing, given an m × n matrix A with row echelon form U, to ﬁnd a basis for
• img A: choose the r columns of A in which the pivots appear in U;
• ker A: write the general solution to A x = 0 as a linear combination of the n −r basis
vectors whose coeﬃcients are the free variables;
• coimg A: choose the r nonzero rows of U;
• cokerA:
write the general solution to the adjoint system AT y = 0 as a linear
combination of the m −r basis vectors whose coeﬃcients are the free vari-
ables. (An alternative method — one that does not require solving the adjoint
system — can be found on page 223.)
Let us conclude this section by justifying these constructions for general matrices, and
thereby complete the proof of the Fundamental Theorem 2.49.
Kernel: If A has rank r, then the general element of the kernel, i.e., solution to the
homogeneous system A x = 0, can be written as a linear combination of n −r vectors
whose coeﬃcients are the free variables, and hence these vectors span ker A. Moreover,
the only combination that yields the zero solution x = 0 is when all the free variables are
zero, since any nonzero value for a free variable, say xi ̸= 0, gives a solution x ̸= 0 whose
ith entry (at least) is nonzero. Thus, the only linear combination of the n −r kernel basis
vectors that sums to 0 is the trivial one, which implies their linear independence.
Coimage: We need to prove that elementary row operations do not change the coimage.
To see this for row operations of the ﬁrst type, suppose, for instance, that A is obtained
by adding b times the ﬁrst row of A to the second row. If r1, r2, r3, . . . , rm are the rows of
A, then the rows of A are r1,r2 = r2 + br1, r3, . . . , rm. If
v = c1 r1 + c2 r2 + c3 r3 + · · · + cm rm
is any vector belonging to coimg A, then
v = c1 r1 + c2r2 + c3 r3 + · · · + cm rm,
where
c1 = c1 −bc2,
is also a linear combination of the rows of the new matrix, and hence lies in coimg A.
The converse is also valid — v ∈coimg A implies v ∈coimg A — and we conclude that
elementary row operations of type #1 do not change coimg A. The proofs for the other
two types of elementary row operations are even easier, and are left to the reader.
The basis for coimg A will be the ﬁrst r nonzero pivot rows s1, . . . , sr of U. Since the
other rows, if any, are all 0, the pivot rows clearly span coimg U = coimg A. To prove their
linear independence, suppose
c1s1 + · · · + crsr = 0.
(2.45)
Let u1k ̸= 0 be the ﬁrst pivot. Since all entries of U lying below the pivot are zero, the
kth entry of (2.45) is c1 u1k = 0, which implies that c1 = 0. Next, suppose u2l ̸= 0 is the
second pivot. Again, using the row echelon structure of U, the lth entry of (2.45) is found
to be c1 u1l + c2 u2l = 0, and so c2 = 0, since we already know c1 = 0. Continuing in this
manner, we deduce that only the trivial linear combination c1 = · · · = cr = 0 will satisfy
(2.45), proving linear independence. Thus, s1, . . . , sr form a basis for coimg U = coimg A,
which therefore has dimension r = rank A.
Image: In general, a vector b ∈img A if and only if it can be written as a linear
combination of the columns: b = A x. But, as we know, the general solution to the linear

118
2 Vector Spaces and Bases
system A x = b is expressed in terms of the free and basic variables; in particular, we are
allowed to set all the free variables to zero, and so end up writing b in terms of the basic
variables alone. This eﬀectively expresses b as a linear combination of the pivot columns
of A only, which proves that they span img A. To prove their linear independence, suppose
some linear combination of the pivot columns adds up to 0. Interpreting the coeﬃcients
as basic variables, this would correspond to a vector x, all of whose free variables are
zero, satisfying A x = 0. But our solution to this homogeneous system expresses the basic
variables as combinations of the free variables, which, if the latter are all zero, are also zero
when the right-hand sides all vanish. This shows that, under these assumptions, x = 0,
and hence the pivot columns are linearly independent.
Cokernel: By the preceding arguments, rank A = rank AT = r, and hence the general
element of cokerA = ker AT can be written as a linear combination of m −r basis vectors
whose coeﬃcients are the free variables in the homogeneous adjoint system AT y = 0.
Linear independence of the basis elements follows as in the case of the kernel.
Exercises
2.5.21. For each of the following matrices ﬁnd bases for the (i) image, (ii) coimage,
(iii) kernel, and (iv) cokernel.
(a)

1
−3
2
−6
	
,
(b)
⎛
⎜
⎝
0
0
−8
1
2
−1
2
4
6
⎞
⎟
⎠,
(c)
⎛
⎜
⎝
1
1
2
1
1
0
−1
3
2
3
7
0
⎞
⎟
⎠,
(d)
⎛
⎜
⎜
⎜
⎜
⎜
⎝
1
−3
2
2
1
0
3
−6
0
−2
2
−3
−2
4
0
3
−3
−6
6
3
1
0
−4
2
3
⎞
⎟
⎟
⎟
⎟
⎟
⎠
.
2.5.22. Find a set of columns of the matrix
⎛
⎜
⎝
−1
2
0
−3
5
2
−4
1
1
−4
−3
6
2
0
8
⎞
⎟
⎠that form a basis for its
image. Then express each column as a linear combination of the basis columns.
2.5.23. For each of the following matrices A: (a) Determine the rank and the dimensions of the
four fundamental subspaces.
(b) Find bases for both the kernel and cokernel.
(c) Find
explicit conditions on vectors b that guarantee that the system A x = b has a solution.
(d) Write down a speciﬁc nonzero vector b that satisﬁes your conditions, and then ﬁnd all
possible solutions x.
(i)

1
2
−2
−4
	
, (ii)

3
−1
−2
−6
2
4
	
, (iii)
⎛
⎜
⎝
1
5
−2
3
2
7
⎞
⎟
⎠, (iv)
⎛
⎜
⎝
2
−5
−1
1
−6
−4
3
−4
2
⎞
⎟
⎠,
(v)
⎛
⎜
⎜
⎜
⎝
2
5
7
6
13
19
3
8
11
1
2
3
⎞
⎟
⎟
⎟
⎠, (vi)
⎛
⎜
⎜
⎜
⎝
1
2
3
4
3
2
4
1
1
−2
2
7
3
6
5
−2
⎞
⎟
⎟
⎟
⎠, (vii)
⎛
⎜
⎜
⎜
⎝
2
4
0
−6
0
1
2
3
15
0
3
6
−1
15
5
−3
−6
2
21
−6
⎞
⎟
⎟
⎟
⎠.
2.5.24. Find the dimension of and a basis for the subspace spanned by the following sets of
vectors. Hint: First identify the subspace with the image of a certain matrix.
(a)
⎛
⎜
⎝
1
2
−1
⎞
⎟
⎠,
⎛
⎜
⎝
2
2
0
⎞
⎟
⎠,
(b)
⎛
⎜
⎝
1
1
−1
⎞
⎟
⎠,
⎛
⎜
⎝
2
2
−2
⎞
⎟
⎠,
⎛
⎜
⎝
−3
−3
3
⎞
⎟
⎠,
(c)
⎛
⎜
⎜
⎜
⎝
1
0
1
0
⎞
⎟
⎟
⎟
⎠,
⎛
⎜
⎜
⎜
⎝
1
0
0
1
⎞
⎟
⎟
⎟
⎠,
⎛
⎜
⎜
⎜
⎝
2
2
1
0
⎞
⎟
⎟
⎟
⎠,
⎛
⎜
⎜
⎜
⎝
1
2
3
−3
⎞
⎟
⎟
⎟
⎠,
(d)
⎛
⎜
⎜
⎜
⎝
1
0
−3
2
⎞
⎟
⎟
⎟
⎠,
⎛
⎜
⎜
⎜
⎝
0
1
2
−3
⎞
⎟
⎟
⎟
⎠,
⎛
⎜
⎜
⎜
⎝
−3
−4
1
6
⎞
⎟
⎟
⎟
⎠,
⎛
⎜
⎜
⎜
⎝
1
−3
−8
7
⎞
⎟
⎟
⎟
⎠,
⎛
⎜
⎜
⎜
⎝
2
1
−6
9
⎞
⎟
⎟
⎟
⎠, (e)
⎛
⎜
⎜
⎜
⎜
⎜
⎝
1
1
−1
1
1
⎞
⎟
⎟
⎟
⎟
⎟
⎠
,
⎛
⎜
⎜
⎜
⎜
⎜
⎝
2
−1
2
2
1
⎞
⎟
⎟
⎟
⎟
⎟
⎠
,
⎛
⎜
⎜
⎜
⎜
⎜
⎝
3
0
1
3
2
⎞
⎟
⎟
⎟
⎟
⎟
⎠
,
⎛
⎜
⎜
⎜
⎜
⎜
⎝
0
−3
4
0
−1
⎞
⎟
⎟
⎟
⎟
⎟
⎠
,
⎛
⎜
⎜
⎜
⎜
⎜
⎝
1
3
−1
2
1
⎞
⎟
⎟
⎟
⎟
⎟
⎠
,
⎛
⎜
⎜
⎜
⎜
⎜
⎝
1
0
3
2
0
⎞
⎟
⎟
⎟
⎟
⎟
⎠
.

2.5 The Fundamental Matrix Subspaces
119
2.5.25. Show that the set of all vectors v = ( a −3b, a + 2c + 4d, b + 3c −d, c −d )T , where
a, b, c, d are real numbers, forms a subspace of R4, and ﬁnd its dimension.
2.5.26. Find a basis of the solution space of the following homogeneous linear systems.
(a) x1 −2x3 = 0,
x2 + x4 = 0.
(b) 2x1 + x2 −3x3 + x4 = 0,
2x1 −x2 −x3 −x4 = 0.
(c)
x1 −x2 −2x3 + 4x4 = 0,
2x1 + x2 −x4 = 0,
−2x1 + 2x3 −2x4 = 0.
2.5.27. Find bases for the image and coimage of
⎛
⎜
⎝
1
−3
0
2
−6
4
−3
9
1
⎞
⎟
⎠. Make sure they have the
same number of elements. Then write each row and column as a linear combination of the
appropriate basis vectors.
2.5.28. Find bases for the image of
⎛
⎜
⎜
⎜
⎝
1
2
−1
0
3
−3
2
−4
6
1
5
−4
⎞
⎟
⎟
⎟
⎠using both of the indicated methods.
Demonstrate that they are indeed both bases for the same subspace by showing how to
write each basis in terms of the other.
2.5.29. Show that v1 = ( 1, 2, 0, −1 )T , v2 = ( −3, 1, 1, −1 )T , v3 = ( 2, 0, −4, 3 )T and
w1 = ( 3, 2, −4, 2 )T , w2 = ( 2, 3, −7, 4 )T , w3 = ( 0, 3, −3, 1 )T are two bases for the same
three-dimensional subspace V ⊂R4.
2.5.30.(a) Prove that if A is a symmetric matrix, then ker A = coker A and img A = coimg A.
(b) Use this observation to produce bases for the four fundamental subspaces associated
with A =
⎛
⎜
⎝
1
2
0
2
6
2
0
2
2
⎞
⎟
⎠.
(c) Is the converse to part (a) true?
2.5.31.(a) Write down a matrix of rank r whose ﬁrst r rows do not form a basis for its row
space. (b) Can you ﬁnd an example that can be reduced to row echelon form without any
row interchanges?
2.5.32. Let A be a 4 × 4 matrix and let U be its row echelon form. (a) Suppose columns 1, 2,
4 of U form a basis for its image. Do columns 1, 2, 4 of A form a basis for its image? If so,
explain why; if not, construct a counterexample. (b) Suppose rows 1, 2, 3 of U form a basis
for its coimage. Do rows 1, 2, 3 of A form a basis for its coimage? If so, explain why; if not,
construct a counterexample. (c) Suppose you ﬁnd a basis for ker U. Is it also a basis for
ker A? (d) Suppose you ﬁnd a basis for coker U. Is it also a basis for coker A?
2.5.33. Can you devise a nonzero matrix whose row echelon form is the same as the row
echelon form of its transpose?
♦2.5.34. Explain why the elementary row operations of types #2 and #3 do not change the
coimage of a matrix.
2.5.35. Let A be an m × n matrix. Prove that img A = Rm if and only if rank A = m.
2.5.36. Prove or give a counterexample: If U is the row echelon form of A, then img U = img A.
♦2.5.37.(a) Devise an alternative method for ﬁnding a basis of the coimage of a matrix.
Hint: Look at the two methods for ﬁnding a basis for the image. (b) Use your method
to ﬁnd a basis for the coimage of
⎛
⎜
⎝
1
3
−5
2
2
−1
1
−4
4
5
−9
2
⎞
⎟
⎠. Is it the same basis as found
by the method in the text?
♦2.5.38. Prove that ker A ⊆ker A2. More generally, prove ker A ⊆ker B A for every compatible
matrix B.

120
2 Vector Spaces and Bases
♦2.5.39. Prove that img A ⊇img A2. More generally, prove img A ⊇img (AB) for every
compatible matrix B.
2.5.40. Suppose A is an m × n matrix, and B and C are nonsingular matrices of sizes m × m
and n × n, respectively. Prove that rank A = rank B A = rank AC = rank B AC.
2.5.41. True or false: If ker A = ker B, then rank A = rank B.
♦2.5.42. Let A and B be matrices of respective sizes m × n and n × p.
(a) Prove that dim ker(AB) ≤dim ker A + dim ker B.
(b) Prove the Sylvester Inequalities rank A+rank B−n ≤rank(AB) ≤min

rank A, rank B

.
♦2.5.43. Suppose A is a nonsingular n × n matrix. (a) Prove that every n × (n + k) matrix of
the form ( A B ), where B has size n × k, has rank n. (b) Prove that every (n + k) × n
matrix of the form

A
C
	
, where C has size k × n, has rank n.
♦2.5.44. Let A be an m × n matrix of rank r. Suppose v1, . . . , vn are a basis for Rn such that
vr+1, . . . , vn form a basis for ker A. Prove that w1 = Av1, . . . , wr = Avr form a basis
for img A.
♦2.5.45.(a) Suppose A, B are m × n matrices such that ker A = ker B. Prove that there is a
nonsingular m × m matrix M such that M A = B. Hint: Use Exercise 2.5.44. (b) Use this
to conclude that if A x = b and B x = c have the same solutions then they are equivalent
linear systems, i.e., one can be obtained from the other by a sequence of elementary row
operations.
♦2.5.46.(a) Let A be an m × n matrix and let V be a subspace of Rn. Show that W = AV =
{ Av | v ∈V } forms a subspace of img A. (b) If dim V = k, show that dim W ≤min{k, r},
where r = rank A. Give an example in which dim(AV ) < dim V . Hint: Use Exercise 2.4.25.
♦2.5.47.(a) Show that an m × n matrix has a left inverse if and only if it has rank n.
Hint: Use Exercise 2.5.46. (b) Show that it has a right inverse if and only if it has rank m.
(c) Conclude that only nonsingular square matrices have both left and right inverses.
2.6 Graphs and Digraphs
We now present an intriguing application of linear algebra to graph theory. A graph consists
of a ﬁnite number of points, called vertices, and ﬁnitely many lines or curves connecting
them, called edges. Each edge connects exactly two vertices, which are its endpoints. To
avoid technicalities, we will always assume that the graph is simple, which means that
every edge connects two distinct vertices, so no edge forms a loop that connects a vertex
to itself, and, moreover, two distinct vertices are connected by at most one edge. Some
examples of graphs appear in Figure 2.6; the vertices are the black dots and the edges are
the lines connecting them.
Graphs arise in a multitude of applications. A particular case that will be considered in
depth is electrical networks, where the edges represent wires, and the vertices represent the
nodes where the wires are connected. Another example is the framework for a building —
the edges represent the beams, and the vertices the joints where the beams are connected.
In each case, the graph encodes the topology — meaning interconnectedness — of the
system, but not its geometry — lengths of edges, angles, etc.
In a planar representation of a graph, the edges are allowed to cross over each other
at non-nodal points without meeting — think of a network where the (insulated) wires lie

2.6 Graphs and Digraphs
121
Figure 2.6.
Three Diﬀerent Graphs.
Figure 2.7.
Three Versions of the Same Graph.
on top of each other, but do not interconnect. Thus, the ﬁrst graph in Figure 2.6 has 5
vertices and 8 edges; the second has 4 vertices and 6 edges — the two central edges do not
meet; the ﬁnal graph has 5 vertices and 10 edges.
Two graphs are considered to be the same if there is a one-to-one correspondence be-
tween their edges and their vertices, so that matched edges connect matched vertices. In
an electrical network, moving the nodes and wires around without cutting or rejoining will
have no eﬀect on the underlying graph. Consequently, there are many ways to draw a given
graph; three representations of one and the same graph appear in Figure 2.7.
A path in a graph is an ordered list of distinct edges e1, . . . , ek connecting (not necessarily
distinct) vertices v1, . . . , vk+1 so that edge ei connects vertex vi to vi+1. For instance, in
the graph in Figure 2.8, one path starts at vertex 1, then goes in order along the edges
labeled as 1, 4, 3, 2, successively passing through the vertices 1, 2, 4, 1, 3. Observe that while
an edge cannot be repeated in a path, a vertex may be. A graph is connected if you can
get from any vertex to any other vertex by a path, which is the most important case for
applications. We note that every graph can be decomposed into a disconnected collection
of connected subgraphs.
A circuit is a path that ends up where it began, i.e., vk+1 = v1. For example, the circuit
in Figure 2.8 consisting of edges 1, 4, 5, 2 starts at vertex 1, then goes to vertices 2, 4, 3 in
order, and ﬁnally returns to vertex 1. In a closed circuit, the choice of starting vertex is
not important, and we identify circuits that go around the edges in the same order. Thus,
for example, the edges 4, 5, 2, 1 represent the same circuit as above.

122
2 Vector Spaces and Bases
1
2
3
4
1
2
3
4
5
Figure 2.8.
A Simple Graph.
Figure 2.9.
Digraphs.
In electrical circuits, one is interested in measuring currents and voltage drops along the
wires in the network represented by the graph. Both of these quantities have a direction,
and therefore we need to specify an orientation on each edge in order to quantify how the
current moves along the wire. The orientation will be ﬁxed by specifying the vertex the
edge “starts” at, and the vertex it “ends” at. Once we assign a direction to an edge, a
current along that wire will be positive if it moves in the same direction, i.e., goes from
the starting vertex to the ending one, and negative if it moves in the opposite direction.
The direction of the edge does not dictate the direction of the current — it just ﬁxes what
directions positive and negative values of current represent. A graph with directed edges
is known as a directed graph, or digraph for short. The edge directions are represented by
arrows; examples of digraphs can be seen in Figure 2.9. Again, the underlying graph is
always assumed to be simple. For example, at any instant in time, the internet can be
viewed as a gigantic digraph, in which each vertex represents a web page, and each edge
represents an existing link from one page to another.
Consider a digraph D consisting of n vertices connected by m edges. The incidence
matrix associated with D is an m × n matrix A whose rows are indexed by the edges and
whose columns are indexed by the vertices. If edge k starts at vertex i and ends at vertex
j, then row k of the incidence matrix will have +1 in its (k, i) entry and −1 in its (k, j)
entry; all other entries in the row are zero. Thus, our convention is that +1 represents the

2.6 Graphs and Digraphs
123
1
2
3
4
1
2
3
4
5
Figure 2.10.
A Simple Digraph.
outgoing vertex at which the edge starts and −1 the incoming vertex at which it ends.
A simple example is the digraph in Figure 2.10, which consists of ﬁve edges joined at
four diﬀerent vertices. Its 5 × 4 incidence matrix is
A =
⎛
⎜
⎜
⎜
⎝
1
−1
0
0
1
0
−1
0
1
0
0
−1
0
1
0
−1
0
0
1
−1
⎞
⎟
⎟
⎟
⎠.
(2.46)
Thus the ﬁrst row of A tells us that the ﬁrst edge starts at vertex 1 and ends at vertex
2. Similarly, row 2 says that the second edge goes from vertex 1 to vertex 3, and so on.
Clearly, one can completely reconstruct any digraph from its incidence matrix.
Example 2.50.
The matrix
A =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
1
−1
0
0
0
−1
0
1
0
0
0
−1
1
0
0
0
1
0
−1
0
0
0
−1
1
0
0
0
1
0
−1
0
0
0
1
−1
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
.
(2.47)
qualiﬁes as an incidence matrix of a simple graph because each row contains a single +1,
a single −1, and the other entries are 0; moreover, to ensure simplicity, no two rows are
identical or −1 times each other. Let us construct the digraph corresponding to A. Since
A has ﬁve columns, there are ﬁve vertices in the digraph, which we label by the numbers
1, 2, 3, 4, 5. Since it has seven rows, there are 7 edges. The ﬁrst row has its +1 in column
1 and its −1 in column 2, and so the ﬁrst edge goes from vertex 1 to vertex 2. Similarly,
the second edge corresponds to the second row of A and so goes from vertex 3 to vertex 1.
The third row of A indicates an edge from vertex 3 to vertex 2; and so on. In this manner,
we construct the digraph drawn in Figure 2.11.
The incidence matrix serves to encode important geometric information about the di-
graph it represents. In particular, its kernel and cokernel have topological signiﬁcance.

124
2 Vector Spaces and Bases
1
2
3
4
5
1
2
3
4
5
6
7
Figure 2.11.
Another Digraph.
For example, the kernel of the incidence matrix (2.47) is spanned by the single vector
z = ( 1, 1, 1, 1, 1 )T, and represents the fact that the sum of the entries in any given row of
A is zero. This observation holds in general for connected digraphs.
Proposition 2.51. If A is the incidence matrix for a connected digraph, then ker A is
one-dimensional, with basis z = ( 1, 1, . . ., 1 )T .
Proof : If edge k connects vertex i to vertex j, then the kth equation in Az = 0 is zi−zj = 0,
or, equivalently, zi = zj. The same equality holds, by a simple induction, if the vertices i
and j are connected by a path. Therefore, if D is connected, then all the entries of z are
equal, and the result follows.
Q.E.D.
Remark. In general, dim ker A equals the number of connected components in the digraph
D. See Exercise 2.6.12.
Applying the Fundamental Theorem 2.49, we immediately deduce the following:
Corollary 2.52. If A is the incidence matrix for a connected digraph with n vertices, then
rank A = n −1.
Next, let us look at the cokernel of an incidence matrix. Consider the particular example
(2.46) corresponding to the digraph in Figure 2.10. We need to compute the kernel of the
transposed incidence matrix
AT =
⎛
⎜
⎝
1
1
1
0
0
−1
0
0
1
0
0
−1
0
0
1
0
0
−1
−1
−1
⎞
⎟
⎠.
(2.48)
Solving the homogeneous system AT y = 0 by Gaussian Elimination, we discover that
cokerA = ker AT is spanned by the two vectors
y1 = ( 1, 0, −1, 1, 0 )T ,
y2 = ( 0, 1, −1, 0, 1 )T .
Each of these vectors represents a circuit in the digraph. Keep in mind that their entries
are indexed by the edges, so a nonzero entry indicates the direction to traverse the corre-
sponding edge. For example, y1 corresponds to the circuit that starts out along edge 1,

2.6 Graphs and Digraphs
125
then goes along edge 4 and ﬁnishes by going along edge 3 in the reverse direction, which is
indicated by the minus sign in its third entry. Similarly, y2 represents the circuit consisting
of edge 2, followed by edge 5, and then edge 3, backwards. The fact that y1 and y2 are
linearly independent vectors says that the two circuits are “independent”.
The general element of coker A is a linear combination c1 y1 + c2 y2. Certain values of
the constants lead to other types of circuits; for example, −y1 represents the same circuit
as y1, but traversed in the opposite direction. Another example is
y1 −y2 = ( 1, −1, 0, 1, −1 )T ,
which represents the square circuit going around the outside of the digraph along edges
1, 4, 5, 2, the ﬁfth and second edges taken in the reverse direction. We can view this circuit
as a combination of the two triangular circuits; when we add them together, the middle
edge 3 is traversed once in each direction, which eﬀectively “cancels” its contribution. (A
similar cancellation occurs in the calculus of line integrals, [2, 78].) Other combinations
represent “virtual” circuits; for instance, one can “interpret” 2y1−1
2 y2 as two times around
the ﬁrst triangular circuit plus one-half of the other triangular circuit, taken in the reverse
direction — whatever that might mean.
Let us summarize the preceding discussion.
Theorem 2.53. Each circuit in a digraph D is represented by a vector in the cokernel of
its incidence matrix A, whose entries are +1 if the edge is traversed in the correct direction,
−1 if in the opposite direction, and 0 if the edge is not in the circuit. The dimension of
the cokernel of A equals the number of independent circuits in D.
Remark. A full proof that the cokernel of the incidence matrix of a general digraph has
a basis consisting entirely of independent circuits requires a more in depth analysis of the
properties of graphs than we can provide in this abbreviated treatment. Full details can
be found in [6; §II.3].
The preceding two theorems have an important and remarkable consequence. Suppose
D is a connected digraph with m edges and n vertices and A its m × n incidence matrix.
Corollary 2.52 implies that A has rank r = n −1 = n −dim ker A. On the other hand,
Theorem 2.53 tells us that l = dim coker A equals the number of independent circuits in
D. The Fundamental Theorem 2.49 says that r = m −l. Equating these two formulas for
the rank, we obtain r = n−1 = m−l, or n+l = m+1. This celebrated result is known as
Euler’s formula for graphs, ﬁrst discovered by the extraordinarily proliﬁc and inﬂuential
eighteenth-century Swiss mathematician Leonhard Euler†.
Theorem 2.54. If G is a connected graph, then
# vertices + # independent circuits = # edges + 1.
(2.49)
Remark. If the graph is planar, meaning that it can be graphed in the plane without
any edges crossing over each other, then the number of independent circuits is equal to the
number of “holes”, i.e., the number of distinct polygonal regions bounded by the edges of
the graph. For example, the pentagonal digraph in Figure 2.11 bounds three triangles, and
so has three independent circuits.
†
Pronounced “Oiler”. Euler spent most of his career in Russia and Germany.

126
2 Vector Spaces and Bases
Figure 2.12.
A Cubical Digraph.
Example 2.55.
Consider the graph corresponding to the edges of a cube, as illustrated
in Figure 2.12, where the second ﬁgure represents the same graph squashed down onto a
plane. The graph has 8 vertices and 12 edges. Euler’s formula (3.92) tells us that there
are 5 independent circuits. These correspond to the interior square and four trapezoids in
the planar version of the digraph, and hence to circuits around 5 of the 6 faces of the cube.
The “missing” face does indeed deﬁne a circuit, but it can be represented as the sum of
the other ﬁve circuits, and so is not independent. In Exercise 2.6.6, the reader is asked to
write out the incidence matrix for the cubical digraph and explicitly identify the basis of
its kernel with the circuits.
Further development of the many remarkable connections between graph theory and
linear algebra will be developed in the later chapters.
The applications to very large
graphs, e.g., with millions or billions of vertices, is playing an increasingly important role
in modern computer science and data analysis.
One example is the dominant internet
search engine run by Google, which is based on viewing the entire internet as a gigantic
(time-dependent) digraph. The vertices are the web pages, and a directed edge represents
a link from one web page to another. (The resulting digraph is not simple according to our
deﬁnition, since web pages can link in both directions.) Ranking web pages by importance
during a search relies on analyzing the internet digraph; see Section 9.3 for further details.
Exercises
2.6.1.(a) Draw the graph corresponding to the 6×7 incidence matrix whose nonzero (i, j) entries
equal 1 if j = i and −1 if j = i + 1, for i = 1 to 6. (b) Find a basis for its
kernel and cokernel. (c) How many circuits are in the digraph?
2.6.2. Draw the digraph represented by the following incidence matrices:
(a)
⎛
⎜
⎜
⎜
⎝
−1
0
1
0
1
0
0
−1
0
−1
1
0
0
1
0
−1
⎞
⎟
⎟
⎟
⎠,
(b)
⎛
⎜
⎜
⎜
⎝
1
0
−1
0
0
1
0
−1
−1
1
0
0
0
0
1
−1
⎞
⎟
⎟
⎟
⎠,
(c)
⎛
⎜
⎜
⎜
⎝
0
1
0
0
−1
−1
0
1
0
0
0
0
0
−1
1
0
−1
1
0
0
⎞
⎟
⎟
⎟
⎠,
(d)
⎛
⎜
⎜
⎜
⎜
⎜
⎝
−1
0
1
0
0
0
−1
0
1
0
1
−1
0
0
0
0
0
0
−1
1
0
0
−1
0
1
⎞
⎟
⎟
⎟
⎟
⎟
⎠
,
(e)
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
0
1
−1
0
0
0
0
−1
0
0
1
0
0
0
0
0
0
−1
1
0
0
0
−1
0
0
0
0
1
0
0
−1
0
0
1
0
0
0
0
0
0
1
−1
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
.

2.6 Graphs and Digraphs
127
2.6.3. Write out the incidence matrix of the following digraphs.
(a)
(b)
(c)
(d)
(e)
(f )
2.6.4. For each of the digraphs in Exercise 2.6.3, see whether you can predict a collection of
independent circuits. Verify your prediction by constructing a suitable basis of the cokernel
of the incidence matrix and identifying each basis vector with a circuit.
♥2.6.5.(a) Write down the incidence matrix A for the indicated digraph.
(b) What is the rank of A? (c) Determine the dimensions of its four
fundamental subspaces.
(d) Find a basis for its kernel and cokernel.
(e) Determine explicit conditions on vectors b that guarantee that the system
A x = b has a solution.
(f ) Write down a speciﬁc nonzero vector b that
satisﬁes your conditions, and then ﬁnd all possible solutions.
♦2.6.6.(a) Write out the incidence matrix for the cubical digraph and identify the basis of its
cokernel with the circuits.
(b) Find three circuits that do not correspond to any of your
basis elements, and express them as a linear combination of the basis circuit vectors.
♥2.6.7. Write out the incidence matrix for the other Platonic solids: (a) tetrahedron,
(b) octahedron, (c) dodecahedron, and (d) icosahedron. (You will need to choose an
orientation for the edges.) Show that, in each case, the number of independent circuits
equals the number of faces minus 1.
♦2.6.8. Prove that a graph with n vertices and n edges must have at least one circuit.
♥2.6.9. A connected graph is called a tree if it has no circuits. (a) Find the incidence matrix for
each of the following directed trees:
(i)
(ii)
(iii)
(iv)
(b) Draw all distinct trees with 4 vertices. Assign a direction to the edges, and write down
the corresponding incidence matrices.
(c) Prove that a connected graph on n vertices is a
tree if and only if it has precisely n −1 edges.
♥2.6.10. A complete graph Gn on n vertices has one edge joining every distinct pair of vertices.
(a) Draw G3, G4 and G5. (b) Choose an orientation for each edge and write out the
resulting incidence matrix of each digraph. (c) How many edges does Gn have? (d) How
many independent circuits?
♥2.6.11. The complete bipartite digraph Gm,n is based on two disjoint sets of, respectively, m
and n vertices. Each vertex in the ﬁrst set is connected to each vertex in the second set
by a single edge. (a) Draw G2,3, G2,4, and G3,3. (b) Write the incidence matrix of each
digraph. (c) How many edges does Gm,n have? (d) How many independent circuits?

128
2 Vector Spaces and Bases
♥2.6.12.(a) Construct the incidence matrix A for the disconnected digraph
D in the ﬁgure. (b) Verify that dim ker A = 3, which is the same
as the number of connected components, meaning the maximal
connected subgraphs in D.
(c) Can you assign an interpretation
to your basis for ker A?
(d) Try proving the general statement
that dim ker A equals the number of connected components in the
digraph D.
2.6.13. How does altering the direction of the edges of a digraph aﬀect its incidence
matrix? The cokernel of its incidence matrix? Can you realize this operation by
matrix multiplication?
♥2.6.14.(a) Explain why two digraphs are equivalent under relabeling of vertices and
edges if and only if their incidence matrices satisfy P AQ = B, where P, Q are
permutation matrices. (b) Decide which of the following incidence matrices produce
the equivalent digraphs:
(i)
⎛
⎜
⎜
⎜
⎝
1
0
−1
0
0
1
0
−1
−1
1
0
0
0
0
1
−1
⎞
⎟
⎟
⎟
⎠,
(ii)
⎛
⎜
⎜
⎜
⎝
0
−1
1
0
−1
0
1
0
1
0
0
−1
0
−1
0
1
⎞
⎟
⎟
⎟
⎠,
(iii)
⎛
⎜
⎜
⎜
⎝
1
0
0
−1
0
1
0
−1
1
0
−1
0
0
0
−1
1
⎞
⎟
⎟
⎟
⎠,
(iv)
⎛
⎜
⎜
⎜
⎝
1
−1
0
0
1
0
−1
0
0
−1
0
1
0
0
−1
1
⎞
⎟
⎟
⎟
⎠,
(v)
⎛
⎜
⎜
⎜
⎝
1
0
0
−1
0
0
−1
1
0
−1
1
0
1
−1
0
0
⎞
⎟
⎟
⎟
⎠,
(vi)
⎛
⎜
⎜
⎜
⎝
1
−1
0
0
0
−1
1
0
0
−1
0
1
−1
0
1
0
⎞
⎟
⎟
⎟
⎠.
(c) How are the cokernels of equivalent incidence matrices related?
2.6.15. True or false: If A and B are incidence matrices of the same size and
coker A = coker B, then the corresponding digraphs are equivalent.
♦2.6.16.(a) Explain why the incidence matrix for a disconnected graph can be written in block
diagonal matrix form A =

B
O
O
C
	
under an appropriate labeling of the vertices.
(b) Show how to label the vertices of the digraph in Exercise 2.6.3e so that its incidence
matrix is in block form.

Chapter 3
Inner Products and Norms
The geometry of Euclidean space is founded on the familiar properties of length and angle.
The abstract concept of a norm on a vector space formalizes the geometrical notion of the
length of a vector. In Euclidean geometry, the angle between two vectors is speciﬁed by
their dot product, which is itself formalized by the abstract concept of an inner product.
Inner products and norms lie at the heart of linear (and nonlinear) analysis, in both
ﬁnite-dimensional vector spaces and inﬁnite-dimensional function spaces. A vector space
equipped with an inner product and its associated norm is known as an inner product
space. It is impossible to overemphasize their importance for theoretical developments,
practical applications, and the design of numerical solution algorithms.
Mathematical analysis relies on the exploitation of inequalities. The most fundamental
is the Cauchy–Schwarz inequality, which is valid in every inner product space. The more
familiar triangle inequality for the associated norm is then derived as a simple consequence.
Not every norm comes from an inner product, and, in such cases, the triangle inequality
becomes part of the general deﬁnition. Both inequalities retain their validity in both ﬁnite-
dimensional and inﬁnite-dimensional vector spaces.
Indeed, their abstract formulation
exposes the key ideas behind the proof, avoiding all distracting particularities appearing
in the explicit formulas.
The characterization of general inner products on Euclidean space will lead us to the
noteworthy class of positive deﬁnite matrices. Positive deﬁnite matrices appear in a wide
variety of applications, including minimization, least squares, data analysis and statistics,
as well as, for example, mechanical systems, electrical circuits, and the diﬀerential equa-
tions describing both static and dynamical processes. The test for positive deﬁniteness
relies on Gaussian Elimination, and we can reinterpret the resulting matrix factorization
as the algebraic process of completing the square for the associated quadratic form. In
applications, positive deﬁnite matrices most often arise as Gram matrices, whose entries
are formed by taking inner products between selected elements of an inner product space.
So far, we have focussed our attention on real vector spaces. Complex numbers, vectors,
and functions also arise in numerous applications, and so, in the ﬁnal section, we take the
opportunity to formally introduce complex vector spaces. Most of the theory proceeds in
direct analogy with the real version, but the notions of inner product and norm on complex
vector spaces require some thought. Applications of complex vector spaces and their inner
products are of particular signiﬁcance in Fourier analysis, signal processing, and partial
diﬀerential equations, [61], and they play an absolutely essential role in modern quantum
mechanics, [54].
3.1 Inner Products
The most basic example of an inner product is the familiar dot product
v · w = v1 w1 + v2 w2 + · · · + vn wn =
n

i=1
vi wi,
(3.1)
© Springer International Publishing AG, part of Springer Nature 2018 
https://doi.org/10.1007/978-3-319-91041-3_3 
129
P. J. Olver, C. Shakiban, Applied Linear Algebra, Undergraduate Texts in Mathematics, 

130
3 Inner Products and Norms
v1
v2
∥v∥
v1
v2
v3
∥v∥
Figure 3.1.
The Euclidean Norm in R2 and R3.
between (column) vectors v = ( v1, v2, . . . , vn )T , w = ( w1, w2, . . . , wn )T, both lying in the
Euclidean space Rn. A key observation is that the dot product (3.1) is equal to the matrix
product
v · w = vT w = ( v1
v2
. . . vn )
⎛
⎜
⎜
⎜
⎝
w1
w2
...
wn
⎞
⎟
⎟
⎟
⎠
(3.2)
between the row vector vT and the column vector w.
The dot product is the cornerstone of Euclidean geometry. The key fact is that the dot
product of a vector with itself,
v · v = v2
1 + v2
2 + · · · + v2
n,
is the sum of the squares of its entries, and hence, by the classical Pythagorean Theorem,
equals the square of its length; see Figure 3.1. Consequently, the Euclidean norm or length
of a vector is found by taking the square root:
∥v∥= √v · v =

v2
1 + v2
2 + · · · + v2
n .
(3.3)
Note that every nonzero vector, v ̸= 0, has positive Euclidean norm, ∥v∥> 0, while only
the zero vector has zero norm: ∥v∥= 0 if and only if v = 0. The elementary properties
of dot product and Euclidean norm serve to inspire the abstract deﬁnition of more general
inner products.
Deﬁnition 3.1. An inner product on the real vector space V is a pairing that takes two
vectors v, w ∈V and produces a real number ⟨v , w ⟩∈R. The inner product is required
to satisfy the following three axioms for all u, v, w ∈V , and scalars c, d ∈R.
(i) Bilinearity:
⟨c u + d v , w ⟩= c ⟨u , w ⟩+ d ⟨v , w ⟩,
⟨u , c v + d w ⟩= c ⟨u , v ⟩+ d ⟨u , w ⟩.
(3.4)
(ii) Symmetry:
⟨v , w ⟩= ⟨w , v ⟩.
(3.5)
(iii) Positivity:
⟨v , v ⟩> 0
whenever
v ̸= 0,
while
⟨0 , 0 ⟩= 0.
(3.6)
A vector space equipped with an inner product is called an inner product space. As we
shall see, a vector space can admit many diﬀerent inner products. Veriﬁcation of the inner

3.1 Inner Products
131
product axioms for the Euclidean dot product is straightforward, and left as an exercise
for the reader.
Given an inner product, the associated norm of a vector v ∈V is deﬁned as the positive
square root of the inner product of the vector with itself:
∥v∥=

⟨v , v ⟩.
(3.7)
The positivity axiom implies that ∥v∥≥0 is real and non-negative, and equals 0 if and
only if v = 0 is the zero vector.
Example 3.2.
While certainly the most common inner product on R2, the dot product
⟨v , w ⟩= v · w = v1 w1 + v2 w2
is by no means the only possibility. A simple example is provided by the weighted inner
product
⟨v , w ⟩= 2v1 w1 + 5v2 w2,
v =

v1
v2

,
w =

w1
w2

.
(3.8)
Let us verify that this formula does indeed deﬁne an inner product. The symmetry axiom
(3.5) is immediate. Moreover,
⟨c u + d v , w ⟩= 2(cu1 + dv1)w1 + 5(cu2 + dv2)w2
= c(2u1w1 + 5u2 w2) + d(2v1 w1 + 5v2 w2) = c ⟨u , w ⟩+ d ⟨v , w ⟩,
which veriﬁes the ﬁrst bilinearity condition; the second follows by a very similar computa-
tion. (Or, one can use the symmetry axiom to deduce the second bilinearity identity from
the ﬁrst; see Exercise 3.1.9.) Moreover, ⟨0 , 0 ⟩= 0, while
⟨v , v ⟩= 2v2
1 + 5v2
2 > 0
whenever
v ̸= 0,
since at least one of the summands is strictly positive. This establishes (3.8) as a legitimate
inner product on R2. The associated weighted norm ∥v∥
=

2v2
1 + 5v2
2
deﬁnes an
alternative, “non-Pythagorean” notion of length of vectors and distance between points in
the plane.
A less evident example of an inner product on R2 is provided by the expression
⟨v , w ⟩= v1 w1 −v1 w2 −v2 w1 + 4v2 w2.
(3.9)
Bilinearity is veriﬁed in the same manner as before, and symmetry is immediate. Positivity
is ensured by noticing that the expression
⟨v , v ⟩= v2
1 −2v1 v2 + 4v2
2 = (v1 −v2)2 + 3v2
2 ≥0
is always non-negative, and, moreover, is equal to zero if and only if v1 −v2 = 0, v2 = 0,
i.e., only when v1 = v2 = 0 and so v = 0. We conclude that (3.9) deﬁnes yet another inner
product on R2, with associated norm
∥v∥=

⟨v , v ⟩
=

v2
1 −2v1 v2 + 4v2
2 .
The second example (3.8) is a particular case of a general class of inner products.
Example 3.3.
Let c1, . . . , cn > 0 be a set of positive numbers.
The corresponding
weighted inner product and weighted norm on Rn are deﬁned by
⟨v , w ⟩=
n

i=1
ci vi wi,
∥v∥=

⟨v , v ⟩=




n

i=1
ci v2
i .
(3.10)

132
3 Inner Products and Norms
The numbers ci > 0 are the weights. Observe that the larger the weight ci, the more the
ith coordinate of v contributes to the norm. Weighted norms are particularly relevant in
statistics and data ﬁtting, [43, 87], when one wants to emphasize the importance of certain
measurements and de-emphasize others; this is done by assigning appropriate weights to
the diﬀerent components of the data vector v. Section 5.4, on least squares approximation
methods, will contain further details.
Exercises
3.1.1. Prove that the formula ⟨v , w ⟩= v1 w1 −v1 w2 −v2 w1 + bv2 w2 deﬁnes an inner product
on R2 if and only if b > 1.
3.1.2. Which of the following formulas for ⟨v , w ⟩deﬁne inner products on R2?
(a) 2v1 w1 + 3v2 w2, (b) v1 w2 + v2 w1, (c) (v1 + v2)(w1 + w2), (d) v2
1 w2
1 + v2
2 w2
2,
(e)

v2
1 + v2
2

w2
1 + w2
2 , (f ) 2v1 w1 + (v1 −v2)(w1 −w2),
(g) 4v1 w1 −2v1 w2 −2v2 w1 + 4v2 w2.
3.1.3. Show that ⟨v , w ⟩= v1 w1+v1 w2+ v2 w1+v2 w2 does not deﬁne an inner product on R2.
3.1.4. Prove that each of the following formulas for ⟨v , w ⟩deﬁnes an inner product on R3.
Verify all the inner product axioms in careful detail:
(a) v1 w1 + 2v2 w2 + 3v3 w3,
(b) 4v1 w1 + 2v1 w2 + 2v2 w1 + 4v2 w2 + v3 w3,
(c) 2v1 w1 −2v1 w2 −2v2 w1 + 3v2 w2 −v2 w3 −v3 w2 + 2v3 w3.
3.1.5. The unit circle for an inner product on R2 is deﬁned as the set of all vectors of unit
length: ∥v∥= 1. Graph the unit circles for (a) the Euclidean inner product, (b) the
weighted inner product (3.8), (c) the non-standard inner product (3.9).
(d) Prove that
cases (b) and (c) are, in fact, both ellipses.
♦3.1.6.(a) Explain why the formula for the Euclidean norm in R2 follows from the Pythagorean
Theorem.
(b) How do you use the Pythagorean Theorem to justify the formula for the
Euclidean norm in R3? Hint: Look at Figure 3.1.
♦3.1.7. Prove that the norm on an inner product space satisﬁes ∥cv∥= | c | ∥v∥for every scalar
c and vector v.
3.1.8. Prove that ⟨av + bw , cv + dw ⟩= ac∥v∥2 + (ad + bc)⟨v , w ⟩+ bd∥w∥2.
♦3.1.9. Prove that the second bilinearity formula (3.4) is a consequence of the ﬁrst and the other
two inner product axioms.
♦3.1.10. Let V be an inner product space. (a) Prove that ⟨x , v ⟩= 0 for all v ∈V if and
only if x = 0.
(b) Prove that ⟨x , v ⟩= ⟨y , v ⟩for all v ∈V if and only if x = y.
(c) Let v1, . . . , vn be a basis for V . Prove that ⟨x , vi ⟩= ⟨y , vi ⟩, i = 1, . . . , n,
if and only if x = y.
♦3.1.11. Prove that x ∈Rn solves the linear system A x = b if and only if
xT AT v = bT v
for all
v ∈Rm.
The latter is known as the weak formulation of the linear system, and its generalizations are
of great importance in the study of diﬀerential equations and numerical analysis, [61].
♦3.1.12.(a) Prove the identity
⟨u , v ⟩= 1
4

∥u + v∥2 −∥u −v∥2 
,
(3.11)
which allows one to reconstruct an inner product from its norm. (b) Use (3.11) to ﬁnd the
inner product on R2 corresponding to the norm ∥v∥=

v2
1 −3v1 v2 + 5v2
2 .

3.1 Inner Products
133
3.1.13.(a) Show that, for all vectors x and y in an inner product space,
∥x + y∥2 + ∥x −y∥2 = 2

∥x∥2 + ∥y∥2 
.
(b) Interpret this result pictorially for vectors in R2 under the Euclidean norm.
3.1.14. Suppose u, v satisfy ∥u∥= 3, ∥u + v∥= 4, and ∥u −v∥= 6. What must ∥v∥equal?
Does your answer depend upon which norm is being used?
3.1.15. Let A be any n × n matrix. Prove that the dot product identity v · (Aw) = (AT v) · w
is valid for all vectors v, w ∈Rn.
♦3.1.16. Prove that A = AT is a symmetric n × n matrix if and only if (Av) · w = v · (Aw) for
all v, w ∈Rn.
3.1.17. Prove that ⟨A , B ⟩= tr(AT B) deﬁnes an inner product on the vector space Mn×n of
real n × n matrices.
3.1.18. Suppose ⟨v , w ⟩deﬁnes an inner product on a vector space V . Explain why it also
deﬁnes an inner product on every subspace W ⊂V .
3.1.19. Prove that if ⟨v , w ⟩and ⟨⟨v , w ⟩⟩are two inner products on the same vector space V ,
then their sum ⟨⟨⟨v , w ⟩⟩⟩= ⟨v , w ⟩+ ⟨⟨v , w ⟩⟩deﬁnes an inner product on V .
♦3.1.20. Let V and W be inner product spaces with respective inner products ⟨v , v ⟩and
⟨⟨w , w ⟩⟩. Show that ⟨⟨⟨(v, w) , (v, w) ⟩⟩⟩= ⟨v , v ⟩+ ⟨⟨w , w ⟩⟩for v, v ∈V, w, w ∈W,
deﬁnes an inner product on their Cartesian product V × W.
Inner Products on Function Spaces
Inner products and norms on function spaces lie at the foundation of modern analysis
and its applications, particularly Fourier analysis, boundary value problems, ordinary and
partial diﬀerential equations, and numerical analysis. Let us introduce the most important
examples.
Example 3.4.
Let [a, b] ⊂R be a bounded closed interval. Consider the vector space
C0[a, b] consisting of all continuous scalar functions f deﬁned on the interval [a, b]. The
integral of the product of two continuous functions,
⟨f , g ⟩=
 b
a
f(x) g(x) dx,
(3.12)
deﬁnes an inner product on the vector space C0[a, b], as we shall prove below. The asso-
ciated norm is, according to the basic deﬁnition (3.7),
∥f ∥=
 b
a
f(x)2 dx ,
(3.13)
and is known as the L2 norm of the function f over the interval [a, b].
The L2 inner
product and norm of functions can be viewed as the inﬁnite-dimensional function space
versions of the dot product and Euclidean norm of vectors in Rn. The reason for the name
L2 will become clearer later on.
For example, if we take [a, b] =

0 , 1
2 π

, then the L2 inner product between f(x) = sin x
and g(x) = cos x is equal to
⟨sin x , cos x ⟩=
 π/2
0
sin x cos x dx = 1
2 sin2 x

π/2
x=0
= 1
2 .

134
3 Inner Products and Norms
Similarly, the norm of the function sin x is
∥sin x∥=
 π/2
0
(sin x)2 dx =
 π
4 .
One must always be careful when evaluating function norms. For example, the constant
function c(x) ≡1 has norm
∥1∥=
 π/2
0
12 dx =
 π
2 ,
not 1 as you might have expected. We also note that the value of the norm depends upon
which interval the integral is taken over. For instance, on the longer interval [0, π ],
∥1∥=
 π
0
12 dx = √π .
Thus, when dealing with the L2 inner product or norm, one must always be careful to
specify the function space, or, equivalently, the interval on which it is being evaluated.
Let us prove that formula (3.12) does, indeed, deﬁne an inner product. First, we need
to check that ⟨f , g ⟩is well deﬁned. This follows because the product f(x)g(x) of two
continuous functions is also continuous, and hence its integral over a bounded interval is
deﬁned and ﬁnite. The symmetry requirement is immediate:
⟨f , g ⟩=
 b
a
f(x) g(x) dx =
 b
a
g(x) f(x) dx = ⟨g , f ⟩,
because multiplication of functions is commutative. The ﬁrst bilinearity axiom
⟨c f + d g , h ⟩= c ⟨f , h ⟩+ d ⟨g , h ⟩
amounts to the following elementary integral identity
 b
a

c f(x) + d g(x)

h(x) dx = c
 b
a
f(x) h(x) dx + d
 b
a
g(x) h(x) dx,
valid for arbitrary continuous functions f, g, h and scalars (constants) c, d. The second
bilinearity axiom is proved similarly; alternatively, one can use symmetry to deduce it
from the ﬁrst as in Exercise 3.1.9. Finally, positivity requires that
∥f ∥2 = ⟨f , f ⟩=
 b
a
f(x)2 dx ≥0.
This is clear because f(x)2 ≥0, and the integral of a nonnegative function is nonnegative.
Moreover, since the function f(x)2 is continuous and nonnegative, its integral will vanish,
 b
a
f(x)2 dx = 0, if and only if f(x) ≡0 is the zero function, cf. Exercise 3.1.29. This
completes the proof that (3.12) deﬁnes a bona ﬁde inner product on the space C0[a, b].
Remark. The L2 inner product formula can also be applied to more general functions, but
we have restricted our attention to continuous functions in order to avoid certain technical
complications. The most general function space admitting this inner product is known

3.1 Inner Products
135
as Hilbert space, which forms the basis of much of modern analysis, function theory, and
Fourier analysis, as well as providing the theoretical setting for all of quantum mechanics,
[54].
Unfortunately, we cannot provide the mathematical details of the Hilbert space
construction, since it requires that you be familiar with measure theory and the Lebesgue
integral. See [61] for a basic introduction and [19, 68, 77] for the fully rigorous theory.
Warning. One needs to be extremely careful when trying to extend the L2 inner product
to other spaces of functions. Indeed, there are nonzero discontinuous functions with zero
“L2 norm”. For example, the function
f(x) =
 1,
x = 0,
0,
otherwise,
satisﬁes
∥f ∥2 =
 1
−1
f(x)2 dx = 0,
(3.14)
because every function that is zero except at ﬁnitely many (or even countably many) points
has zero integral.
The L2 inner product is but one of a vast number of possible inner products on function
spaces. For example, one can also deﬁne weighted inner products on the space C0[a, b].
The weighting along the interval is speciﬁed by a (continuous) positive scalar function
w(x) > 0. The corresponding weighted inner product and norm are
⟨f , g ⟩=
 b
a
f(x) g(x) w(x) dx,
∥f ∥=
 b
a
f(x)2 w(x) dx .
(3.15)
The veriﬁcation of the inner product axioms in this case is left as an exercise for the reader.
As in the ﬁnite-dimensional version, weighted inner products are often used in statistics
and data analysis, [20, 43, 87].
Exercises
3.1.21. For each of the given pairs of functions in C0[0, 1], ﬁnd their L2 inner product
⟨f , g ⟩and their L2 norms ∥f ∥, ∥g∥: (a) f(x) = 1, g(x) = x; (b) f(x) = cos 2πx,
g(x) = sin 2πx; (c) f(x) = x, g(x) = ex; (d) f(x) = (x + 1)2, g(x) =
1
x + 1 .
3.1.22. Let f(x) = x, g(x) = 1 + x2. Compute ⟨f , g ⟩, ∥f ∥, and ∥g∥for (a) the L2 inner
product ⟨f , g ⟩=
 1
0 f(x)g(x) dx; (b) the L2 inner product ⟨f , g ⟩=
 1
−1 f(x)g(x) dx;
(c) the weighted inner product ⟨f , g ⟩=
 1
0 f(x)g(x)x dx.
3.1.23. Which of the following formulas for ⟨f , g ⟩deﬁne inner products on the space
C0[−1, 1]?
(a)
 1
−1 f(x)g(x) e−x dx,
(b)
 1
−1 f(x)g(x) x dx,
(c)
 1
−1 f(x)g(x) (x + 2) dx,
(d)
 1
−1 f(x)g(x) x2 dx.
3.1.24. Prove that ⟨f , g ⟩=
 1
0 f(x)g(x) dx does not deﬁne an inner product on the vector
space C0[−1, 1]. Explain why this does not contradict the fact that it deﬁnes an inner
product on the vector space C0[0, 1]. Does it deﬁne an inner product on the subspace
P(n) ⊂C0[−1, 1] consisting of all polynomial functions?

136
3 Inner Products and Norms
3.1.25. Does either of the following deﬁne an inner product on C0[0, 1]?
(a) ⟨f , g ⟩= f(0)g(0) + f(1)g(1),
(b) ⟨f , g ⟩= f(0)g(0) + f(1)g(1) +
 1
0 f(x)g(x) dx.
3.1.26. Let f(x) be a function, and ∥f ∥its L2 norm on [a, b]. Is ∥f2 ∥= ∥f ∥2? If yes, prove
the statement. If no, give a counterexample.
♦3.1.27. Prove that ⟨f , g ⟩=
 b
a

f(x)g(x) + f′(x)g′(x)

dx deﬁnes an inner product on the
space C1[a, b] of continuously diﬀerentiable functions on the interval [a, b]. Write out the
corresponding norm, known as the Sobolev H1 norm; it and its generalizations play an
extremely important role in advanced mathematical analysis, [49].
3.1.28. Let V = C1[−1, 1] denote the vector space of continuously diﬀerentiable functions for
−1 ≤x ≤1. (a) Does the expression ⟨f , g ⟩=
 1
−1 f′(x) g′(x) dx deﬁne an inner product on
V ? (b) Answer the same question for the subspace W = { f ∈V | f(0) = 0 } consisting of
all continuously diﬀerentiable functions that vanish at 0.
♦3.1.29.(a) Let h(x) ≥0 be a continuous, non-negative function deﬁned on an interval [a, b].
Prove that
 b
a h(x) dx = 0 if and only if h(x) ≡0. Hint: Use the fact that
 d
c h(x) dx > 0 if
h(x) > 0 for c ≤x ≤d. (b) Give an example that shows that this result is not valid if h is
allowed to be discontinuous.
♦3.1.30.(a) Prove the inner product axioms for the weighted inner product (3.15), assuming
w(x) > 0 for all a ≤x ≤b. (b) Explain why it does not deﬁne an inner product if w is
continuous and w(x0) < 0 for some x0 ∈[a, b]. (c) If w(x) ≥0 for a ≤x ≤b, does (3.15)
deﬁne an inner product? Hint: Your answer may depend upon w(x).
♥3.1.31. Let Ω ⊂R2 be a closed bounded subset. Let C0(Ω) denote the vector space consisting
of all continuous, bounded real-valued functions f(x, y) deﬁned for (x, y) ∈Ω. (a) Prove
that if f(x, y) ≥0 is continuous and

Ω f(x, y) dx dy = 0, then f(x, y) ≡0. Hint: Mimic
Exercise 3.1.29. (b) Use this result to prove that
⟨f , g ⟩=

Ω f(x, y)g(x, y) dx dy
(3.16)
deﬁnes an inner product on C0(Ω), called the L2 inner product on the domain Ω. What is
the corresponding norm?
3.1.32. Compute the L2 inner product (3.16) and norms of the functions f(x, y) ≡1 and
g(x, y) = x2 + y2, when (a) Ω = {0 ≤x ≤1, 0 ≤y ≤1} is the unit square;
(b) Ω = {x2 + y2 ≤1} is the unit disk. Hint: Use polar coordinates.
♥3.1.33. Let V be the vector space consisting of all continuous, vector-valued functions
f(x) = ( f1(x), f2(x) )T deﬁned on the interval 0 ≤x ≤1.
(a) Prove that ⟨⟨f , g ⟩⟩=
 1
0

f1(x)g1(x) + f2(x)g2(x)

dx deﬁnes an inner product on V .
(b) Prove, more generally, that if ⟨v , w ⟩is any inner product on R2, then
⟨⟨f , g ⟩⟩=
 b
a ⟨f(x) , g(x) ⟩dx deﬁnes an inner product on V . (Part (a) corresponds to the
dot product.)
(c) Use part (b) to prove that
⟨⟨f , g ⟩⟩=
 b
a

f1(x)g1(x) −f1(x)g2(x) −f2(x)g1(x) + 3f2(x)g2(x)

dx
deﬁnes an inner product on V .

3.2 Inequalities
137
θ
v
w
Figure 3.2.
Angle Between Two Vectors.
3.2 Inequalities
There are two absolutely basic inequalities that are valid for any inner product space.
The ﬁrst is inspired by the geometric interpretation of the dot product on Euclidean space
in terms of the angle between vectors. It is named after two of the founders of modern
analysis, the nineteenth-century mathematicians Augustin Cauchy, of France, and Herman
Schwarz, of Germany, who established it in the case of the L2 inner product on function
space.†
The more familiar triangle inequality, that the length of any side of a triangle
is bounded by the sum of the lengths of the other two sides, is, in fact, an immediate
consequence of the Cauchy–Schwarz inequality, and hence also valid for any norm based
on an inner product.
We will present these two inequalities in their most general, abstract form, since this
brings their essence into the limelight. Specializing to diﬀerent inner products and norms
on both ﬁnite-dimensional and inﬁnite-dimensional vector spaces leads to a wide variety of
striking and useful inequalities.
The Cauchy–Schwarz Inequality
In Euclidean geometry, the dot product between two vectors v, w ∈Rn can be geometri-
cally characterized by the equation
v · w = ∥v∥∥w∥cos θ,
(3.17)
where θ =<) (v, w) measures the angle between the two vectors, as illustrated in Figure 3.2.
Since | cos θ | ≤1, the absolute value of the dot product is bounded by the product of the
lengths of the vectors:
| v · w | ≤∥v∥∥w∥.
This is the simplest form of the general Cauchy–Schwarz inequality. We present a direct
algebraic proof that does not rely on the geometrical notions of length and angle and thus
demonstrates its universal validity for any inner product.
Theorem 3.5. Every inner product satisﬁes the Cauchy–Schwarz inequality
| ⟨v , w ⟩| ≤∥v∥∥w∥,
for all
v, w ∈V.
(3.18)
Here, ∥v∥is the associated norm, while | · | denotes the absolute value of a real number.
Equality holds in (3.18) if and only if v and w are parallel vectors.
†
Russians also give credit for its discovery to their compatriot Viktor Bunyakovsky, and, indeed,
some authors append his name to the inequality.

138
3 Inner Products and Norms
Proof : The case when w = 0 is trivial, since both sides of (3.18) are equal to 0. Thus, we
concentrate on the case when w ̸= 0. Let t ∈R be an arbitrary scalar. Using the three
inner product axioms, we have
0 ≤∥v + t w∥2 = ⟨v + t w , v + t w ⟩= ⟨v , v ⟩+ 2t ⟨v , w ⟩+ t2 ⟨w , w ⟩
= ∥v∥2 + 2t ⟨v , w ⟩+ t2 ∥w∥2,
(3.19)
with equality holding if and only if v = −t w — which requires v and w to be parallel
vectors. We ﬁx v and w, and consider the right-hand side of (3.19) as a quadratic function
of the scalar variable t:
0 ≤p(t) = at2 + 2bt + c,
where
a = ∥w∥2,
b = ⟨v , w ⟩,
c = ∥v∥2.
To get the maximum mileage out of the fact that p(t) ≥0, let us look at where it assumes
its minimum, which occurs when its derivative is zero:
p′(t) = 2at + 2b = 0,
and so
t = −b
a = −⟨v , w ⟩
∥w∥2 .
Substituting this particular value of t into (3.19), we obtain
0 ≤∥v∥2 −2 ⟨v , w ⟩2
∥w∥2
+ ⟨v , w ⟩2
∥w∥2
= ∥v∥2 −⟨v , w ⟩2
∥w∥2
.
Rearranging this last inequality, we conclude that
⟨v , w ⟩2
∥w∥2
≤∥v∥2,
or
⟨v , w ⟩2 ≤∥v∥2 ∥w∥2.
Also, as noted above, equality holds if and only if v and w are parallel. Equality also holds
when w = 0, which is of course parallel to every vector v. Taking the (positive) square
root of both sides of the ﬁnal inequality completes the proof of (3.18).
Q.E.D.
Given any inner product, we can use the quotient
cos θ =
⟨v , w ⟩
∥v∥∥w∥
(3.20)
to deﬁne the “angle” θ =<) (v, w) between the vector space elements v, w ∈V .
The
Cauchy–Schwarz inequality tells us that the ratio lies between −1 and +1, and hence the
angle θ is well deﬁned modulo 2π, and, in fact, unique if we restrict it to lie in the range
0 ≤θ ≤π.
For example, the vectors v = ( 1, 0, 1 )T , w = ( 0, 1, 1 )T have dot product v · w = 1 and
norms ∥v∥= ∥w∥=
√
2. Hence the Euclidean angle between them is given by
cos θ =
1
√
2 ·
√
2 = 1
2 ,
and so
θ =<) (v, w) = 1
3 π = 1.0471 . . . .
On the other hand, if we adopt the weighted inner product ⟨v , w ⟩= v1 w1+2v2w2+3v3w3,
then v · w = 3, ∥v∥= 2, ∥w∥=
√
5, and hence their “weighted” angle becomes
cos θ =
3
2
√
5 = .67082 . . . ,
with
θ =<) (v, w) = .83548 . . . .
Thus, the measurement of angle (and length) depends on the choice of an underlying inner
product.

3.2 Inequalities
139
Similarly, under the L2 inner product on the interval [0, 1], the “angle” θ between the
polynomials p(x) = x and q(x) = x2 is given by
cos θ =
⟨x , x2 ⟩
∥x∥∥x2∥=
 1
0
x3 dx
 1
0
x2 dx
 1
0
x4 dx
=
1
4

1
3

1
5
=

15
16 ,
so that θ =<) (p, q) = .25268 . . . radians.
Warning. You should not try to give this notion of angle between functions more sig-
niﬁcance than the formal deﬁnition warrants — it does not correspond to any “angular”
properties of their graphs. Also, the value depends on the choice of inner product and
the interval upon which it is being computed. For example, if we change to the L2 inner
product on the interval [−1, 1], then ⟨x , x2 ⟩=
 1
−1
x3 dx = 0. Hence, (3.20) becomes
cos θ = 0, so the “angle” between x and x2 is now θ =<) (p, q) = 1
2 π.
Exercises
3.2.1. Verify the Cauchy–Schwarz inequality for each of the following pairs of vectors v, w,
using the standard dot product, and then determine the angle between them:
(a) ( 1, 2 )T , ( −1, 2 )T ,
(b) ( 1, −1, 0 )T , ( −1, 0, 1 )T ,
(c) ( 1, −1, 0 )T , ( 2, 2, 2 )T ,
(d) ( 1, −1, 1, 0 )T , ( −2, 0, −1, 1 )T ,
(e) ( 2, 1, −2, −1 )T , ( 0, −1, 2, −1 )T .
3.2.2.(a) Find the Euclidean angle between the vectors ( 1, 1, 1, 1 )T and ( 1, 1, 1, −1 )T in R4.
(b) List the possible angles between ( 1, 1, 1, 1 )T and ( a1, a2, a3, a4 )T , where each ai is
either 1 or −1.
3.2.3. Prove that the points (0, 0, 0), (1, 1, 0), (1, 0, 1), (0, 1, 1) form the vertices of a regular
tetrahedron, meaning that all sides have the same length. What is the common Euclidean
angle between the edges? What is the angle between any two rays going from the
center
 1
2, 1
2, 1
2

to the vertices?
Remark. Methane molecules assume this geometric
conﬁguration, and the angle inﬂuences their chemistry.
3.2.4. Verify the Cauchy–Schwarz inequality for the vectors v = ( 1, 2 )T , w = ( 1, −3 )T , using
(a) the dot product; (b) the weighted inner product ⟨v , w ⟩= v1 w1 + 2v2 w2;
(c) the inner product (3.9).
3.2.5. Verify the Cauchy–Schwarz inequality for the vectors v = ( 3, −1, 2 )T , w = ( 1, −1, 1 )T ,
using (a) the dot product; (b) the weighted inner product ⟨v , w ⟩= v1 w1+2v2 w2+3v3 w3;
(c) the inner product ⟨v , w ⟩= vT
⎛
⎜
⎝
2
−1
0
−1
2
−1
0
−1
2
⎞
⎟
⎠w.
♦3.2.6. Show that one can determine the angle θ between v and w via the formula
cos θ = ∥v + w∥2 −∥v −w∥2
4 ∥v∥∥w∥
. Draw a picture illustrating what is being measured.
♦3.2.7. The Law of Cosines: Prove that the formula
∥v −w∥2 = ∥v∥2 + ∥w∥2 −2 ∥v∥∥w∥cos θ,
(3.21)
where θ is the angle between v and w, is valid in every inner product space.

140
3 Inner Products and Norms
3.2.8. Use the Cauchy–Schwarz inequality to prove (a cos θ + b sin θ)2 ≤a2 + b2 for any θ, a, b.
3.2.9. Prove that (a1 + a2 + · · · + an)2 ≤n (a2
1 + a2
2 + · · · + a2
n) for any real numbers
a1, . . . , an. When does equality hold?
♥3.2.10. The cross product of two vectors in R2 is deﬁned as the scalar
v × w = v1 w2 −v2 w1
for
v = ( v1, v2 )T ,
w = ( w1, w2 )T .
(3.22)
(a) Does the cross product deﬁne an inner product on R2? Carefully explain which axioms
are valid and which are not. (b) Prove that v × w = ∥v∥∥w∥sin θ, where θ denotes the
angle from v to w as in Figure 3.2. (c) Prove that v × w = 0 if and only if v and w are
parallel vectors. (d) Show that | v × w | equals the area of the parallelogram deﬁned by v
and w.
♦3.2.11. Explain why the inequality ⟨v , w ⟩≤∥v∥∥w∥, obtained by omitting the absolute
value sign on the left-hand side of Cauchy–Schwarz, is valid.
3.2.12. Verify the Cauchy–Schwarz inequality for the functions f(x) = x and g(x) = ex with
respect to (a) the L2 inner product on the interval [0, 1], (b) the L2 inner product on
[−1, 1], (c) the weighted inner product ⟨f , g ⟩=
 1
0 f(x)g(x)e−x dx.
3.2.13. Using the L2 inner product on the interval [0 , π ], ﬁnd the angle between the functions
(a) 1 and cos x;
(b) 1 and sin x;
(c) cos x and sin x.
3.2.14. Verify the Cauchy–Schwarz inequality for the two particular functions appearing in
Exercise 3.1.32 using the L2 inner product on (a) the unit square; (b) the unit disk.
Orthogonal Vectors
In Euclidean geometry, a particularly noteworthy conﬁguration occurs when two vectors are
perpendicular. Perpendicular vectors meet at a right angle, θ = 1
2 π or 3
2 π, with cos θ = 0.
The angle formula (3.17) implies that the vectors v, w are perpendicular if and only if their
dot product vanishes: v · w = 0. Perpendicularity is of interest in general inner product
spaces, but, for historical reasons, has been given a more suggestive name.
Deﬁnition 3.6. Two elements v, w ∈V of an inner product space V are called orthogonal
if their inner product vanishes: ⟨v , w ⟩= 0.
In particular, the zero element is orthogonal to all other vectors: ⟨0 , v ⟩= 0 for all
v ∈V . Orthogonality is a remarkably powerful tool that appears throughout the manifold
applications of linear algebra, and often serves to dramatically simplify many computations.
We will devote all of Chapter 4 to a detailed exploration of its manifold implications.
Example 3.7.
The vectors v = ( 1, 2 )T and w = ( 6, −3 )T are orthogonal with respect
to the Euclidean dot product in R2, since v · w = 1 · 6 + 2 · (−3) = 0. We deduce that
they meet at a right angle. However, these vectors are not orthogonal with respect to the
weighted inner product (3.8):
⟨v , w ⟩=
 
1
2

,

6
−3
 !
= 2 · 1 · 6 + 5 · 2 · (−3) = −18 ̸= 0.
Thus, the property of orthogonality, like angles in general, depends upon which inner
product is being used.

3.2 Inequalities
141
Example 3.8.
The polynomials p(x) = x and q(x) = x2 −1
2 are orthogonal with respect
to the inner product ⟨p , q ⟩=
 1
0
p(x) q(x) dx on the interval [0, 1], since
"
x , x2 −1
2
#
=
 1
0
x

x2 −1
2

dx =
 1
0

x3 −1
2 x

dx = 0.
They fail to be orthogonal on most other intervals. For example, on the interval [0, 2],
"
x , x2 −1
2
#
=
 2
0
x

x2 −1
2

dx =
 2
0

x3 −1
2 x

dx = 3.
Warning. There is no obvious connection between the orthogonality of two functions and
the geometry of their graphs.
Exercises
Note: Unless stated otherwise, the inner product is the standard dot product on Rn.
3.2.15.(a) Find a such that ( 2, a, −3 )T is orthogonal to ( −1, 3, −2 )T .
(b) Is there any value
of a for which ( 2, a, −3 )T is parallel to ( −1, 3, −2 )T ?
3.2.16. Find all vectors in R3 that are orthogonal to both ( 1, 2, 3 )T and ( −2, 0, 1 )T .
3.2.17. Answer Exercises 3.2.15 and 3.2.16 using the weighted inner product
⟨v , w ⟩= 3v1 w1 + 2v2 w2 + v3 w3.
3.2.18. Find all vectors in R4 that are orthogonal to both ( 1, 2, 3, 4 )T and ( 5, 6, 7, 8 )T .
3.2.19. Determine a basis for the subspace W ⊂R4 consisting of all vectors which are
orthogonal to the vector ( 1, 2, −1, 3 )T .
3.2.20. Find three vectors u, v and w in R3 such that u and v are orthogonal, u and w are
orthogonal, but v and w are not orthogonal. Are your vectors linearly independent or
linearly dependent? Can you ﬁnd vectors of the opposite dependency satisfying the same
conditions? Why or why not?
3.2.21. For what values of a, b are the vectors ( 1, 1, a )T and ( b, −1, 1 )T orthogonal
(a) with respect to the dot product?
(b) with respect to the weighted inner product of Exercise 3.2.17?
3.2.22. When is a vector orthogonal to itself?
♦3.2.23. Prove that the only element w in an inner product space V that is orthogonal to every
vector, so ⟨w , v ⟩= 0 for all v ∈V , is the zero vector: w = 0.
3.2.24. A vector with ∥v∥= 1 is known as a unit vector. Prove that if v, w are both unit
vectors, then v + w and v −w are orthogonal. Are they also unit vectors?
♦3.2.25. Let V be an inner product space and v ∈V . Prove that the set of all vectors w ∈V
that are orthogonal to v is a subspace of V .
3.2.26.(a) Show that the polynomials p1(x) = 1, p2(x) = x −1
2, p3(x) = x2 −x + 1
6
are mutually orthogonal with respect to the L2 inner product on the interval [0, 1].
(b) Show that the functions sin nπ x, n = 1, 2, 3, . . . , are mutually orthogonal with respect
to the same inner product.

142
3 Inner Products and Norms
∥v + w∥
∥v∥
∥w∥
Figure 3.3.
Triangle Inequality.
3.2.27. Find a non-zero quadratic polynomial that is orthogonal to both p1(x) = 1 and
p2(x) = x under the L2 inner product on the interval [−1, 1].
3.2.28. Find all quadratic polynomials that are orthogonal to the function ex with respect to
the L2 inner product on the interval [0, 1].
3.2.29. Determine all pairs among the functions 1, x, cos πx, sin πx, ex, that are orthogonal
with respect to the L2 inner product on [−1, 1].
3.2.30. Find two non-zero functions that are orthogonal with respect to the weighted inner
product ⟨f , g ⟩=
 1
0 f(x) g(x) x dx.
The Triangle Inequality
The familiar triangle inequality states that the length of one side of a triangle is at most
equal to the sum of the lengths of the other two sides.
Referring to Figure 3.3, if the
ﬁrst two sides are represented by vectors v and w, then the third corresponds to their
sum v + w. The triangle inequality turns out to be an elementary consequence of the
Cauchy–Schwarz inequality (3.18), and hence is valid in every inner product space.
Theorem 3.9. The norm associated with an inner product satisﬁes the triangle inequality
∥v + w∥≤∥v∥+ ∥w∥
for all
v, w ∈V.
(3.23)
Equality holds if and only if v and w are parallel vectors.
Proof : We compute
∥v + w∥2 = ⟨v + w , v + w ⟩= ∥v∥2 + 2 ⟨v , w ⟩+ ∥w∥2
≤∥v∥2 + 2 ∥v∥∥w∥+ ∥w∥2 =

∥v∥+ ∥w∥
2,
where the middle inequality follows from Cauchy–Schwarz, cf. Exercise 3.2.11.
Taking
square roots of both sides and using the fact that the resulting expressions are both positive
completes the proof.
Q.E.D.
Example 3.10.
The vectors v =
⎛
⎝
1
2
−1
⎞
⎠and w =
⎛
⎝
2
0
3
⎞
⎠sum to v +w =
⎛
⎝
3
2
2
⎞
⎠. Their
Euclidean norms are ∥v∥=
√
6 and ∥w∥=
√
13, while ∥v + w∥=
√
17. The triangle
inequality (3.23) in this case says
√
17 ≤
√
6 +
√
13, which is true.

3.2 Inequalities
143
Example 3.11.
Consider the functions f(x) = x −1 and g(x) = x2 + 1. Using the L2
norm on the interval [0, 1], we ﬁnd that
∥f ∥=
 1
0
(x −1)2 dx =

1
3 ,
∥g ∥=
 1
0
(x2 + 1)2 dx =

28
15 ,
∥f + g∥=
 1
0
(x2 + x)2 dx =

31
30 .
The triangle inequality requires

31
30 ≤

1
3 +

28
15 , which is valid.
The Cauchy–Schwarz and triangle inequalities look much more impressive when written
out in full detail. For the Euclidean dot product (3.1), they are

n

i=1
vi wi
 ≤




n

i=1
v2
i




n

i=1
w2
i ,




n

i=1
(vi + wi)2 ≤




n

i=1
v2
i
+




n

i=1
w2
i .
(3.24)
Theorems 3.5 and 3.9 imply that these inequalities are valid for arbitrary real numbers
v1, . . . , vn, w1, . . . , wn. For the L2 inner product (3.13) on function space, they produce the
following splendid integral inequalities:

 b
a
f(x) g(x) dx
 ≤
 b
a
f(x)2 dx
 b
a
g(x)2 dx ,
 b
a

f(x) + g(x)
2 dx ≤
 b
a
f(x)2 dx +
 b
a
g(x)2 dx ,
(3.25)
which hold for arbitrary continuous (and, in fact, rather general) functions. The ﬁrst of
these is the original Cauchy–Schwarz inequality, whose proof appeared to be quite deep
when it ﬁrst appeared.
Only after the abstract notion of an inner product space was
properly formalized did its innate simplicity and generality become evident.
Exercises
3.2.31. Use the dot product on R3 to answer the following: (a) Find the angle between the
vectors ( 1, 2, 3 )T and ( 1, −1, 2 )T . (b) Verify the Cauchy–Schwarz and triangle inequalities
for these two particular vectors. (c) Find all vectors that are orthogonal to both of these
vectors.
3.2.32. Verify the triangle inequality for each pair of vectors in Exercise 3.2.1.
3.2.33. Verify the triangle inequality for the vectors and inner products in Exercise 3.2.4.
3.2.34. Verify the triangle inequality for the functions in Exercise 3.2.12 for the indicated inner
products.

144
3 Inner Products and Norms
3.2.35. Verify the triangle inequality for the two particular functions appearing in Exercise
3.1.32 with respect to the L2 inner product on (a) the unit square; (b) the unit disk.
3.2.36. Use the L2 inner product ⟨f , g ⟩=
 1
−1 f(x) g(x) dx to answer the following:
(a) Find the “angle” between the functions 1 and x. Are they orthogonal? (b) Verify the
Cauchy–Schwarz and triangle inequalities for these two functions. (c) Find all quadratic
polynomials p(x) = a + bx + cx2 that are orthogonal to both of these functions.
3.2.37.(a) Write down the explicit formulae for the Cauchy–Schwarz and triangle inequalities
based on the weighted inner product ⟨f , g ⟩=
 1
0 f(x) g(x) ex dx.
(b) Verify that the
inequalities hold when f(x) = 1, g(x) = ex by direct computation. (c) What is the “angle”
between these two functions in this inner product?
3.2.38. Answer Exercise 3.2.37 for the Sobolev H1 inner product
⟨f , g ⟩=
 1
0

f(x)g(x) + f′(x)g′(x)

dx,
cf. Exercise 3.1.27.
3.2.39. Prove that ∥v −w∥≥



 ∥v∥−∥w∥



. Interpret this result pictorially.
3.2.40. True or false: ∥w∥≤∥v∥+ ∥v + w∥for all v, w ∈V .
♥3.2.41.(a) Prove that the space R∞consisting of all inﬁnite sequences x = (x1, x2, x3, . . . )
of real numbers xi ∈R is a vector space. (b) Prove that the set of all sequences x such
that
∞

k=1
x2
k < ∞is a subspace, commonly denoted by ℓ2 ⊂R∞. (c) Write down two
examples of sequences x belonging to ℓ2 and two that do not belong to ℓ2. (d) True or
false: If x ∈ℓ2, then xk →0 and k →∞. (e) True or false: If xk →0 as k →∞, then
x ∈ℓ2. (f ) Given α ∈R, let x be the sequence with xk = αk. For which values of α is
x ∈ℓ2?
(g) Answer part (f ) when xk = kα. (h) Prove that ⟨x , y ⟩=
∞

k=1
xk yk deﬁnes an
inner product on the vector space ℓ2. What is the corresponding norm? (i) Write out the
Cauchy–Schwarz and triangle inequalities for the inner product space ℓ2.
3.3 Norms
Every inner product gives rise to a norm that can be used to measure the magnitude or
length of the elements of the underlying vector space. However, not every norm that is
used in analysis and applications arises from an inner product. To deﬁne a general norm
on a vector space, we will extract those properties that do not directly rely on the inner
product structure.
Deﬁnition 3.12. A norm on a vector space V assigns a non-negative real number ∥v∥
to each vector v ∈V , subject to the following axioms, valid for every v, w ∈V and c ∈R:
(i) Positivity:
∥v∥≥0, with ∥v∥= 0 if and only if v = 0.
(ii) Homogeneity:
∥c v∥= | c | ∥v∥.
(iii) Triangle inequality:
∥v + w∥≤∥v∥+ ∥w∥.
As we now know, every inner product gives rise to a norm. Indeed, positivity of the
norm is one of the inner product axioms. The homogeneity property follows since
∥c v∥=

⟨c v , c v ⟩=

c2 ⟨v , v ⟩= | c |

⟨v , v ⟩= | c | ∥v∥.

3.3 Norms
145
Finally, the triangle inequality for an inner product norm was established in Theorem 3.9.
Let us introduce some of the principal examples of norms that do not come from inner
products.
First, let V = Rn. The 1 norm of a vector v = ( v1, v2, . . . , vn )T is deﬁned as the sum
of the absolute values of its entries:
∥v∥1 = | v1 | + | v2 | + · · · + | vn |.
(3.26)
The max or ∞norm is equal to its maximal entry (in absolute value):
∥v∥∞= max { | v1 |, | v2 |, . . . , | vn | }.
(3.27)
Veriﬁcation of the positivity and homogeneity properties for these two norms is straight-
forward; the triangle inequality is a direct consequence of the elementary inequality
| a + b | ≤| a | + | b |,
a, b ∈R,
for absolute values.
The Euclidean norm, 1 norm, and ∞norm on Rn are just three representatives of the
general p norm
∥v∥p =
p




n

i=1
| vi |p .
(3.28)
This quantity deﬁnes a norm for all 1 ≤p < ∞. The ∞norm is a limiting case of (3.28) as
p →∞. Note that the Euclidean norm (3.3) is the 2 norm, and is often designated as such;
it is the only p norm which comes from an inner product. The positivity and homogeneity
properties of the p norm are not hard to establish. The triangle inequality, however, is not
trivial; in detail, it reads
p




n

i=1
| vi + wi |p ≤
p




n

i=1
| vi |p +
p




n

i=1
| wi |p ,
(3.29)
and is known as Minkowski’s inequality. A complete proof can be found in [50].
There are analogous norms on the space C0[a, b] of continuous functions on an interval
[a, b]. Basically, one replaces the previous sums by integrals. Thus, the Lp norm is deﬁned
as
∥f ∥p =
p
 b
a
| f(x) |
p dx .
(3.30)
In particular, the L1 norm is given by integrating the absolute value of the function:
∥f ∥1 =
 b
a
| f(x) | dx.
(3.31)
The L2 norm (3.13) appears as a special case, p = 2, and, again, is the only one arising
from an inner product. The limiting L∞norm is deﬁned by the maximum
∥f ∥∞= max { | f(x) | : a ≤x ≤b } .
(3.32)
Positivity of the Lp norms again relies on the fact that the only continuous non-negative
function with zero integral is the zero function. Homogeneity is easily established. On the

146
3 Inner Products and Norms
∥v −w∥
v
w
Figure 3.4.
Distance Between Vectors.
other hand, the proof of the general triangle, or Minkowski, inequality for p ̸= 1, 2, ∞is
again not trivial, [19, 68].
Example 3.13.
Consider the polynomial p(x) = 3x2 −2 on the interval −1 ≤x ≤1.
Its L2 norm is
∥p∥2 =
 1
−1
(3x2 −2)2 dx =

18
5
= 1.8973 . . . .
Its L∞norm is
∥p∥∞= max

| 3x2 −2 | : −1 ≤x ≤1

= 2,
with the maximum occurring at x = 0. Finally, its L1 norm is
∥p∥1 =
 1
−1
| 3x2 −2 | dx
=
 −√
2/3
−1
(3x2 −2) dx +
 √
2/3
−√
2/3
(2 −3x2) dx +
 1
√
2/3
(3x2 −2) dx
=
$
4
3

2
3 −1
%
+ 8
3

2
3 +
$
4
3

2
3 −1
%
= 16
3

2
3 −2 = 2.3546 . . . .
Every norm deﬁnes a distance between vector space elements, namely
d(v, w) = ∥v −w∥.
(3.33)
For the standard dot product norm, we recover the usual notion of distance between points
in Euclidean space. Other types of norms produce alternative (and sometimes quite useful)
notions of distance that are, nevertheless, subject to all the familiar properties:
(a) Symmetry: d(v, w) = d(w, v);
(b) Positivity: d(v, w) = 0 if and only if v = w;
(c) Triangle inequality: d(v, w) ≤d(v, z) + d(z, w).
Just as the distance between vectors measures how close they are to each other —
keeping in mind that this measure of proximity depends on the underlying choice of norm
— so the distance between functions in a normed function space tells something about
how close they are to each other, which is related, albeit subtly, to how close their graphs
are. Thus, the norm serves to deﬁne the topology of the underlying vector space, which
determines notions of open and closed sets, convergence, and so on, [19, 68].

3.3 Norms
147
Exercises
3.3.1. Compute the 1, 2, 3, and ∞norms of the vectors

1
0
	
,

0
1
	
. Verify the triangle
inequality in each case.
3.3.2. Answer Exercise 3.3.1 for (a)

2
−1
	
,

1
−2
	
, (b)
⎛
⎜
⎝
1
0
−1
⎞
⎟
⎠,
⎛
⎜
⎝
−1
1
0
⎞
⎟
⎠, (c)
⎛
⎜
⎝
1
−2
−1
⎞
⎟
⎠,
⎛
⎜
⎝
2
−1
−3
⎞
⎟
⎠.
3.3.3. Which two of the vectors u = ( −2, 2, 1 )T , v = ( 1, 4, 1 )T , w = ( 0, 0, −1 )T are closest
to each other in distance for (a) the Euclidean norm?
(b) the ∞norm?
(c) the 1 norm?
3.3.4.(a) Compute the L∞norm on [0, 1] of the functions f(x) = 1
3 −x and g(x) = x −x2.
(b) Verify the triangle inequality for these two particular functions.
3.3.5. Answer Exercise 3.3.4 using the L1 norm.
3.3.6. Which two of the functions f(x) = 1, g(x) = x, h(x) = sin πx are closest to each other
on the interval [0, 1] under (a) the L1 norm?
(b) the L2 norm?
(c) the L∞norm?
3.3.7. Consider the functions f(x) = 1 and g(x) = x −3
4 as elements of the vector space
C0[0, 1]. For each of the following norms, compute ∥f ∥, ∥g∥, ∥f + g∥, and verify the
triangle inequality: (a) the L1 norm; (b) the L2 norm; (c) the L3 norm; (d) the L∞norm.
3.3.8. Answer Exercise 3.3.7 when f(x) = ex and g(x) = e−x.
3.3.9. Carefully prove that ∥( x, y )T ∥= | x | + 2 | x −y | deﬁnes a norm on R2.
3.3.10. Prove that the following formulas deﬁne norms on R2: (a) ∥v∥=

2v2
1 + 3v2
2 ,
(b) ∥v∥=

2v2
1 −v1 v2 + 2v2
2 , (c) ∥v∥= 2| v1 | + | v2 |, (d) ∥v∥= max

2| v1 |, | v2 |

,
(e) ∥v∥= max

| v1 −v2 |, | v1 + v2 |

, (f ) ∥v∥= | v1 −v2 | + | v1 + v2 |.
3.3.11. Which of the following formulas deﬁne norms on R3?
(a) ∥v∥=

2v2
1 + v2
2 + 3v2
3 ,
(b) ∥v∥=

v2
1 + 2v1 v2 + v2
2 + v2
3 , (c) ∥v∥= max

| v1 |, | v2 |, | v3 |

,
(d) ∥v∥= | v1 −v2 | + | v2 −v3 | + | v3 −v1 |, (e) ∥v∥= | v1 | + max

| v2 |, | v3 |

.
3.3.12. Prove that two parallel vectors v and w have the same norm if and only if v = ±w.
3.3.13. True or false: If ∥v + w∥= ∥v∥+ ∥w∥, then v, w are parallel vectors.
3.3.14. Prove that the ∞norm on R2 does not come from an inner product. Hint: Look at
Exercise 3.1.13.
3.3.15. Can formula (3.11) be used to deﬁne an inner product for (a) the 1 norm ∥v∥1 on R2?
(b) the ∞norm ∥v∥∞on R2?
♦3.3.16. Prove that limp →∞∥v∥p = ∥v∥∞for all v ∈R2.
♦3.3.17. Justify the triangle inequality for (a) the L1 norm (3.31); (b) the L∞norm (3.32).
♦3.3.18. Let w(x) > 0 for a ≤x ≤b be a weight function. (a) Prove that
∥f ∥1,w =
 b
a | f(x) | w(x) dx deﬁnes a norm on C0[a, b], called the weighted L1 norm.
(b) Do the same for the weighted L∞norm ∥f ∥∞,w = max

| f(x) | w(x) : a ≤x ≤b

.

148
3 Inner Products and Norms
3.3.19. Let ∥·∥1 and ∥·∥2 be two diﬀerent norms on a vector space V . (a) Prove that
∥v∥= max

∥v∥1, ∥v∥2

deﬁnes a norm on V . (b) Does ∥v∥= min

∥v∥1, ∥v∥2

deﬁne
a norm? (c) Does the arithmetic mean ∥v∥= 1
2

∥v∥1 + ∥v∥2

deﬁne a norm?
(d) Does the geometric mean ∥v∥=

∥v∥1 ∥v∥2 deﬁne a norm?
Unit Vectors
Let V be a normed vector space. The elements u ∈V that have unit norm, ∥u∥= 1, play
a special role, and are known as unit vectors (or functions or elements). The following easy
lemma shows how to construct a unit vector pointing in the same direction as any given
nonzero vector.
Lemma 3.14. If v ̸= 0 is any nonzero vector, then the vector u = v/∥v∥obtained by
dividing v by its norm is a unit vector parallel to v.
Proof : We compute, making use of the homogeneity property of the norm and the fact
that ∥v∥is a scalar,
∥u∥=
&&&&
v
∥v∥
&&&& = ∥v∥
∥v∥= 1.
Q.E.D.
Example 3.15.
The vector v = ( 1, −2 )T has length ∥v∥2 =
√
5 with respect to the
standard Euclidean norm. Therefore, the unit vector pointing in the same direction is
u =
v
∥v∥2
=
1
√
5

1
−2

=

1
√
5
−2
√
5

.
On the other hand, for the 1 norm, ∥v∥1 = 3, and so
u =
v
∥v∥1
= 1
3

1
−2

=

1
3
−2
3

is the unit vector parallel to v in the 1 norm. Finally, ∥v∥∞= 2, and hence the corre-
sponding unit vector for the ∞norm is
u =
v
∥v∥∞
= 1
2

1
−2

=

1
2
−1

.
Thus, the notion of unit vector will depend upon which norm is being used.
Example 3.16.
Similarly, on the interval [0, 1], the quadratic polynomial p(x) = x2 −1
2
has L2 norm
∥p∥2 =
 1
0

x2 −1
2
2 dx =
 1
0

x4 −x2 + 1
4

dx =

7
60 .
Therefore, u(x) = p(x)
∥p∥=

60
7 x2 −

15
7 is a “unit polynomial”, ∥u∥2 = 1, which is
“parallel” to (or, more precisely, a scalar multiple of) the polynomial p. On the other
hand, for the L∞norm,
∥p∥∞= max
  x2 −1
2
  0 ≤x ≤1

= 1
2 ,

3.3 Norms
149
Figure 3.5.
Unit Balls and Spheres for 1, 2, and ∞Norms in R2.
and hence, in this case, u(x) = 2p(x) = 2x2 −1 is the corresponding unit polynomial.
The unit sphere for the given norm is deﬁned as the set of all unit vectors
S1 =

∥u∥= 1

,
while
Sr =

∥u∥= r

(3.34)
is the sphere of radius r ≥0. Thus, the unit sphere for the Euclidean norm on Rn is the
usual round sphere
S1 =

∥x∥2 = x2
1 + x2
2 + · · · + x2
n = 1

.
The unit sphere for the ∞norm is the surface of a unit cube:
S1 =

x ∈Rn

| xi | ≤1, i = 1, . . . , n,
and either
x1 = ±1 or x2 = ±1 or
. . .
or xn = ±1
'
.
For the 1 norm,
S1 = { x ∈Rn | | x1 | + | x2 | + · · · + | xn | = 1 }
is the unit diamond in two dimensions, unit octahedron in three dimensions, and unit cross
polytope in general. See Figure 3.5 for the two-dimensional pictures.
In all cases, the unit ball B1 =

∥u∥≤1

consists of all vectors of norm less than or
equal to 1, and has the unit sphere as its boundary. If V is a ﬁnite-dimensional normed
vector space, then the unit ball B1 is a compact subset, meaning that it is closed and
bounded.
This basic topological fact, which is not true in inﬁnite-dimensional normed
spaces, underscores the distinction between ﬁnite-dimensional vector analysis and the vastly
more complicated inﬁnite-dimensional realm.
Exercises
3.3.20. Find a unit vector in the same direction as v = ( 1, 2, −3 )T for (a) the Euclidean norm,
(b) the weighted norm ∥v∥2 = 2v2
1 + v2
2 + 1
3 v2
3, (c) the 1 norm, (d) the ∞norm, (e) the
norm based on the inner product 2v1 w1 −v1 w2 −v2 w1 + 2v2 w2 −v2 w3 −v3 w2 + 2v3 w3.
3.3.21. Show that, for every choice of given angles θ , φ, and ψ, the following are unit vectors
in the Euclidean norm: (a) ( cos θ cos φ, cos θ sin φ, sin θ )T . (b)
1
√
2 ( cos θ, sin θ, cos φ, sin φ )T .
(c) ( cos θ cos φ cos ψ, cos θ cos φ sin ψ, cos θ sin φ, sin θ )T .
3.3.22. How many unit vectors are parallel to a given vector v ̸= 0? (a) 1, (b) 2, (c) 3,
(d) ∞, (e) depends on the norm. Explain your answer.
3.3.23. Plot the unit circle (sphere) for (a) the weighted norm ∥v∥=

v2
1 + 4v2
2 ;
(b) the norm based on the inner product (3.9); (c) the norm of Exercise 3.3.9.

150
3 Inner Products and Norms
3.3.24. Draw the unit circle for each norm in Exercise 3.3.10.
3.3.25. Sketch the unit sphere S1 ⊂R3 for (a) the L1 norm, (b) the L∞norm, (c) the
weighted norm ∥v∥2 = 2v2
1 + v2
2 + 3v2
3, (d) ∥v∥= max

| v1 + v2 |, | v1 + v3 |, | v2 + v3 |

.
3.3.26. Let v ̸= 0 be any nonzero vector in a normed vector space V . Show how to construct a
new norm on V that changes v into a unit vector.
3.3.27. True or false: Two norms on a vector space have the same unit sphere if and only if
they are the same norm.
3.3.28. Find the unit function that is a constant multiple of the function f(x) = x −1
3 with
respect to the (a) L1 norm on [0, 1]; (b) L2 norm on [0, 1]; (c) L∞norm on [0, 1]; (d) L1
norm on [−1, 1]; (e) L2 norm on [−1, 1]; (f ) L∞norm on [−1, 1].
3.3.29. For which norms is the constant function f(x) ≡1 a unit function?
(a) L1 norm on [0, 1]; (b) L2 norm on [0, 1]; (c) L∞norm on [0, 1];
(d) L1 norm on [−1, 1]; (e) L2 norm on [−1, 1]; (f ) L∞norm on [−1, 1];
(g) L1 norm on R; (h) L2 norm on R; (i) L∞norm on R.
♦3.3.30. A subset S ⊂Rn is called convex if, for all x, y ∈S, the line segment joining x to y
is also in S, i.e., tx + (1 −t)y ∈S for all 0 ≤t ≤1. Prove that the unit ball is a convex
subset of a normed vector space. Is the unit sphere convex?
Equivalence of Norms
While there are many diﬀerent types of norms, in a ﬁnite-dimensional vector space they
are all more or less equivalent. “Equivalence” does not mean that they assume the same
values, but rather that they are, in a certain sense, always close to one another, and so,
for many analytical purposes, may be used interchangeably. As a consequence, we may be
able to simplify the analysis of a problem by choosing a suitably adapted norm; examples
can be found in Chapter 9.
Theorem 3.17. Let ∥·∥1 and ∥·∥2 be any two norms on Rn. Then there exist positive
constants 0 < c⋆≤C⋆such that
c⋆∥v∥1 ≤∥v∥2 ≤C⋆∥v∥1
for every
v ∈Rn.
(3.35)
Proof : We just sketch the basic idea, leaving the details to a more rigorous real analysis
course, cf. [19; §7.6]. We begin by noting that a norm deﬁnes a continuous real-valued
function f(v) = ∥v∥on Rn. (Continuity is, in fact, a consequence of the triangle inequal-
ity.) Let S1 =

∥u∥1 = 1

denote the unit sphere of the ﬁrst norm. Every continuous
function deﬁned on a compact set achieves both a maximum and a minimum value. Thus,
restricting the second norm function to the unit sphere S1 of the ﬁrst norm, we can set
c⋆= min { ∥u∥2 | u ∈S1 } ,
C⋆= max { ∥u∥2 | u ∈S1 } .
(3.36)
Moreover, 0 < c⋆≤C⋆< ∞, with equality holding if and only if the norms are the same.
The minimum and maximum (3.36) will serve as the constants in the desired inequalities
(3.35). Indeed, by deﬁnition,
c⋆≤∥u∥2 ≤C⋆
when
∥u∥1 = 1,
(3.37)
which proves that (3.35) is valid for all unit vectors v = u ∈S1. To prove the inequalities in
general, assume v ̸= 0. (The case v = 0 is trivial.) Lemma 3.14 says that u = v/∥v∥1 ∈S1

3.3 Norms
151
Figure 3.6.
Equivalence of the ∞and 2 Norms.
is a unit vector in the ﬁrst norm: ∥u∥1 = 1. Moreover, by the homogeneity property of the
norm, ∥u∥2 = ∥v∥2/∥v∥1. Substituting into (3.37) and clearing denominators completes
the proof of (3.35).
Q.E.D.
Example 3.18.
Consider the Euclidean norm ∥·∥2 and the max norm ∥·∥∞on Rn.
According to (3.36), the bounding constants are found by minimizing and maximizing
∥u∥∞= max{ | u1 |, . . . , | un | } over all unit vectors ∥u∥2 = 1 on the (round) unit sphere.
The maximal value is achieved at the poles ± ek, with ∥± ek ∥∞= C⋆= 1. The minimal
value is attained at the points
$
±
1
√n , . . . , ±
1
√n
%
, whereby c⋆=
1
√n . Therefore,
1
√n ∥v∥2 ≤∥v∥∞≤∥v∥2.
(3.38)
We can interpret these inequalities as follows. Suppose v is a vector lying on the unit sphere
in the Euclidean norm, so ∥v∥2 = 1. Then (3.38) tells us that its ∞norm is bounded from
above and below by
1
√n ≤∥v∥∞≤1. Therefore, the Euclidean unit sphere sits inside the
∞norm unit sphere and outside the ∞norm sphere of radius
1
√n . Figure 3.6 illustrates
the two-dimensional situation: the unit circle is inside the unit square, and contains the
square of size
1
√
2 .
One signiﬁcant consequence of the equivalence of norms is that, in Rn, convergence
is independent of the norm. The following are all equivalent to the standard notion of
convergence of a sequence u(1), u(2), u(3), . . . of vectors in Rn:
(a) the vectors converge: u(k) −→u⋆:
(b) the individual coordinates all converge: u(k)
i
−→u⋆
i for i = 1, . . ., n.
(c) the diﬀerence in norms goes to zero: ∥u(k) −u⋆∥−→0.
The last version, known as convergence in norm, does not depend on which norm is chosen.
Indeed, the inequality (3.35) implies that if one norm goes to zero, so does any other
norm. A consequence is that all norms on Rn induce the same topology — convergence
of sequences, notions of open and closed sets, and so on. None of this is true in inﬁnite-
dimensional function space!
A rigorous development of the underlying topological and
analytical properties of compactness, continuity, and convergence is beyond the scope of
this course. The motivated student is encouraged to consult a text in real analysis, e.g.,
[19, 68], to ﬁnd the relevant deﬁnitions, theorems, and proofs.

152
3 Inner Products and Norms
Example 3.19.
Consider the inﬁnite-dimensional vector space C0[0, 1] consisting of all
continuous functions on the interval [0, 1]. The functions
fn(x) =
(
1 −nx,
0 ≤x ≤1
n,
0,
1
n ≤x ≤1,
have identical L∞norms
∥fn∥∞= sup { | fn(x) | | 0 ≤x ≤1 } = 1.
On the other hand, their L2 norm
∥fn∥2 =
  1
0
fn(x)2 dx =
  1/n
0
(1 −nx)2 dx =
1
√
3n
goes to zero as n →∞. This example shows that there is no constant C⋆such that
∥f ∥∞≤C⋆∥f ∥2
for all f ∈C0[0, 1]. Thus, the L∞and L2 norms on C0[0, 1] are not equivalent — there
exist functions that have unit L∞norm, but arbitrarily small L2 norm. Similar comparative
results can be established for the other function space norms. Analysis and topology on
function space is intimately linked to the underlying choice of norm.
Exercises
3.3.31. Check the validity of the inequalities (3.38) for the particular vectors
(a) ( 1, −1 )T ,
(b) ( 1, 2, 3 )T ,
(c) ( 1, 1, 1, 1 )T ,
(d) ( 1, −1, −2, −1, 1 )T .
3.3.32. Find all v ∈R2 such that
(a) ∥v∥1 = ∥v∥∞, (b) ∥v∥1 = ∥v∥2 , (c) ∥v∥2 = ∥v∥∞, (d) ∥v∥∞=
1
√
2 ∥v∥2.
3.3.33. How would you quantify the following statement: The norm of a vector is small if and
only if all its entries are small.
3.3.34. Can you ﬁnd an elementary proof of the inequalities ∥v∥∞≤∥v∥2 ≤√n ∥v∥∞for
v ∈Rn directly from the formulas for the norms?
3.3.35. (i) Show the equivalence of the Euclidean norm and the 1 norm on Rn by proving
∥v∥2 ≤∥v∥1 ≤√n ∥v∥2. (ii) Verify that the vectors in Exercise 3.3.31 satisfy both
inequalities. (iii) For which vectors v ∈Rn is (a) ∥v∥2 = ∥v∥1? (b) ∥v∥1 = √n ∥v∥2?
3.3.36. (i) Establish the equivalence inequalities (3.35) between the 1 and ∞norms.
(ii) Verify them for the vectors in Exercise 3.3.31.
(iii) For which vectors v ∈Rn are your inequalities equality?
3.3.37. Let ∥·∥2 denote the usual Euclidean norm on Rn. Determine the constants in the norm
equivalence inequalities c⋆∥v∥≤∥v∥2 ≤C⋆∥v∥for the following norms: (a) the weighted
norm ∥v∥=

2v2
1 + 3v2
2 , (b) the norm ∥v∥= max

| v1 + v2 |, | v1 −v2 |

.
3.3.38. Let ∥·∥be a norm on Rn. Prove that there is a constant C > 0 such that the entries of
every v = ( v1, v2, . . . , vn )T ∈Rn are all bounded, in absolute value, by | vi | ≤C ∥v∥.
3.3.39. Prove that if [a, b] is a bounded interval and f ∈C0[a, b], then ∥f ∥2 ≤
√
b −a ∥f ∥∞.

3.3 Norms
153
♥3.3.40. In this exercise, the indicated function norms are taken over all of R.
(a) Let fn(x) =
 1,
−n ≤x ≤n,
0,
otherwise.
Prove that ∥fn ∥∞= 1, but ∥fn ∥2 →∞as n →∞.
(b) Explain why there is no constant C such that ∥f ∥2 ≤C ∥f ∥∞for all functions f.
(c) Let fn(x) =
⎧
⎪
⎪
⎨
⎪
⎪
⎩
n
2 ,
−1
n ≤x ≤1
n,
0,
otherwise.
Prove that ∥fn ∥2 = 1, but ∥fn ∥∞→∞
as n →∞. Conclude that there is no constant C such that ∥f ∥∞≤C ∥f ∥2.
(d) Construct similar examples that disprove the related inequalities
(i) ∥f ∥∞≤C ∥f ∥1,
(ii) ∥f ∥1 ≤C ∥f ∥2,
(iii) ∥f ∥2 ≤C ∥f ∥1.
♥3.3.41.(a) Prove that the L∞and L2 norms on the vector space C0[−1, 1] are not equivalent.
Hint: Look at Exercise 3.3.40 for ideas.
(b) Can you establish a bound in either direction,
i.e., ∥f ∥∞≤C ∥f ∥2 or ∥f ∥2 ≤C ∥f ∥∞for all f ∈C0[−1, 1] for some positive constants
C, C?
(c) Are the L1 and L∞norms equivalent?
♦3.3.42. What does it mean if the constants deﬁned in (3.36) are equal: c⋆= C⋆?
3.3.43. Suppose ⟨v , w ⟩1 and ⟨v , w ⟩2 are two inner products on the same vector space V . For
which α, β ∈R is the linear combination ⟨v , w ⟩= α ⟨v , w ⟩1 + β ⟨v , w ⟩2 a legitimate
inner product? Hint: The case α, β ≥0 is easy. However, some negative values are also
permitted, and your task is to decide which.
♦3.3.44. Suppose ∥·∥1, ∥·∥2 are two norms on Rn. Prove that the corresponding matrix norms
satisfy c⋆∥A∥1 ≤∥A∥2 ≤C⋆∥A∥1 for any n × n matrix A for some positive constants
0 < c⋆≤C⋆.
Matrix Norms
Each norm on Rn will naturally induce a norm on the vector space Mn×n of all n × n
matrices. Roughly speaking, the matrix norm tells us how much a linear transformation
stretches vectors relative to the given norm. Matrix norms will play an important role
in Chapters 8 and 9, particularly in our analysis of linear iterative systems and iterative
numerical methods for solving both linear and nonlinear systems.
We work exclusively with real n×n matrices in this section, although the results straight-
forwardly extend to complex matrices. We begin by ﬁxing a norm ∥·∥on Rn. The norm
may or may not come from an inner product — this is irrelevant as far as the construction
goes.
Theorem 3.20. If ∥·∥is any norm on Rn, then the quantity
∥A∥= max { ∥Au∥| ∥u∥= 1 }
(3.39)
deﬁnes the norm of an n×n matrix A ∈Mn×n, called the associated natural matrix norm.
Proof : First note that ∥A∥< ∞, since the maximum is taken on a closed and bounded
subset, namely the unit sphere S1 = {∥u∥= 1} for the given norm. To show that (3.39)
deﬁnes a norm, we need to verify the three basic axioms of Deﬁnition 3.12.
Non-negativity, ∥A∥≥0, is immediate. Suppose ∥A∥= 0. This means that, for every
unit vector, ∥Au∥= 0, and hence Au = 0 whenever ∥u∥= 1. If 0 ̸= v ∈Rn is any
nonzero vector, then u = v/r, where r = ∥v∥, is a unit vector, so
Av = A(ru) = rAu = 0.
(3.40)

154
3 Inner Products and Norms
Therefore, Av = 0 for every v ∈Rn, which implies that A = O is the zero matrix. This
serves to prove the positivity property: ∥A∥= 0 if and only if A = O.
As for homogeneity, if c ∈R is any scalar, then
∥cA∥= max { ∥cAu∥} = max { | c | ∥Au∥} = | c | max { ∥Au∥} = | c | ∥A∥.
Finally, to prove the triangle inequality, we use the fact that the maximum of the sum of
quantities is bounded by the sum of their individual maxima. Therefore, since the norm
on Rn satisﬁes the triangle inequality,
∥A + B ∥= max { ∥Au + B u∥} ≤max { ∥Au∥+ ∥B u∥}
≤max { ∥Au∥} + max { ∥B u∥} = ∥A∥+ ∥B ∥.
Q.E.D.
The property that distinguishes a matrix norm from a generic norm on the space of
matrices is the fact that it also obeys a very useful product inequality.
Theorem 3.21. A natural matrix norm satisﬁes
∥Av∥≤∥A∥∥v∥,
for all
A ∈Mn×n,
v ∈Rn.
(3.41)
Furthermore,
∥AB∥≤∥A∥∥B ∥,
for all
A, B ∈Mn×n.
(3.42)
Proof : Note ﬁrst that, by deﬁnition ∥Au∥≤∥A∥for all unit vectors ∥u∥= 1. Then,
letting v = r u where u is a unit vector and r = ∥v∥, we have
∥Av∥= ∥A(r u)∥= r ∥Au∥≤r ∥A∥= ∥v∥∥A∥,
proving the ﬁrst inequality. To prove the second, we apply the ﬁrst, replacing v by B u:
∥A B ∥= max { ∥A B u∥} = max { ∥A (B u)∥}
≤max { ∥A∥∥B u∥} = ∥A∥max { ∥B u∥} = ∥A∥∥B ∥.
Q.E.D.
Remark. In general, a norm on the vector space of n×n matrices is called a matrix norm
if it also satisﬁes the multiplicative inequality (3.42). Most, but not all, matrix norms used
in applications come from norms on the underlying vector space.
The multiplicative inequality (3.42) implies, in particular, that ∥A2 ∥≤∥A∥2; equality
is not necessarily valid. More generally:
Proposition 3.22. If A is a square matrix, then ∥Ak ∥≤∥A∥k.
Let us determine the explicit formula for the matrix norm induced by the ∞norm
∥v∥∞= max {| v1 |, . . . , | vn |}.
The corresponding formula for the 1 norm is left as Exercise 3.3.48. The formula for the
Euclidean matrix norm (2 norm) will be deferred until Theorem 8.71.

3.3 Norms
155
Deﬁnition 3.23. The ith absolute row sum of a matrix A is the sum of the absolute values
of the entries in the ith row:
si = | ai1 | + · · · + | ain | =
n

j =1
| aij |.
(3.43)
Proposition 3.24. The ∞matrix norm of a matrix A is equal to its maximal absolute
row sum:
∥A∥∞= max{s1, . . . , sn} = max
⎧
⎨
⎩
n

j =1
| aij |

1 ≤i ≤n
⎫
⎬
⎭.
(3.44)
Proof : Let s = max{s1, . . . , sn} denote the right-hand side of (3.44). Given any v ∈Rn,
we compute the ∞norm of the image vector Av:
∥Av∥∞= max
⎧
⎨
⎩

n

j =1
aijvj

⎫
⎬
⎭≤max
⎧
⎨
⎩
n

j =1
| aijvj |
⎫
⎬
⎭
≤max
⎧
⎨
⎩
n

j =1
| aij |
⎫
⎬
⎭max

| vj |

= s ∥v∥∞.
In particular, by specializing to a unit vector, ∥v∥∞= 1, we deduce that ∥A∥∞≤s.
On the other hand, suppose the maximal absolute row sum occurs at row i, so
si =
n

j =1
| aij | = s.
(3.45)
Let u ∈Rn be the speciﬁc vector that has the following entries: uj = +1 if aij ≥0, while
uj = −1 if aij < 0. Then ∥u∥∞= 1. Moreover, since aij uj = | aij |, the ith entry of Au is
equal to the ith absolute row sum (3.45). This implies that ∥A∥∞≥∥Au∥∞≥s. Q.E.D.
Example 3.25.
Consider the symmetric matrix A =

1
2
−1
3
−1
3
1
4

. Its two absolute
row sums are
 1
2
 +
 −1
3
 = 5
6 ,
 −1
3
 +
 1
4
 =
7
12 , so ∥A∥∞= max
 5
6, 7
12

= 5
6.
Exercises
3.3.45. Compute the ∞matrix norm of the following matrices.
(a)
⎛
⎝
1
2
1
4
1
3
1
6
⎞
⎠,
(b)
⎛
⎝
5
3
4
3
−7
6
−5
6
⎞
⎠,
(c)
⎛
⎜
⎝
0
.1
.8
−.1
0
.1
−.8
−.1
0
⎞
⎟
⎠,
(d)
⎛
⎜
⎜
⎜
⎝
1
3
0
0
−1
3
0
1
3
0
2
3
1
3
⎞
⎟
⎟
⎟
⎠.
3.3.46. Find a matrix A such that ∥A2 ∥∞̸= ∥A∥2
∞.
3.3.47. True or false: If B = S−1AS are similar matrices, then ∥B ∥∞= ∥A∥∞.
♦3.3.48. (i) Find an explicit formula for the 1 matrix norm ∥A∥1.
(ii) Compute the 1 matrix norm of the matrices in Exercise 3.3.45.

156
3 Inner Products and Norms
3.3.49. Prove directly from the axioms of Deﬁnition 3.12 that (3.44) deﬁnes a norm on the
space of n × n matrices.
3.3.50. Let A =

1
1
1
−2
	
. Compute the natural matrix norm ∥A∥for (a) the weighted ∞
norm ∥v∥= max{2| v1 |, 3| v2 |}; (b) the weighted 1 norm ∥v∥= 2| v1 | + 3| v2 |.
♥3.3.51. The Frobenius norm of an n × n matrix A is deﬁned as ∥A∥F =





n

i,j =1
a2
ij .
Prove that this deﬁnes a matrix norm by checking the three norm axioms plus the
multiplicative inequality (3.42).
3.3.52. Explain why ∥A∥= max | aij | deﬁnes a norm on the space of n × n matrices. Show by
example that this is not a matrix norm, i.e., (3.42) is not necessarily valid.
3.4 Positive Deﬁnite Matrices
Let us now return to the study of inner products and ﬁx our attention on the ﬁnite-
dimensional situation. Our immediate goal is to determine the most general inner prod-
uct that can be placed on the ﬁnite-dimensional vector space Rn. The answer will lead
us to the important class of positive deﬁnite matrices, which appear in a wide range of
applications, including minimization problems, mechanics, electrical circuits, diﬀerential
equations, statistics, and numerical methods. Moreover, their inﬁnite-dimensional coun-
terparts, positive deﬁnite linear operators, govern most boundary value problems arising
in continuum physics and engineering.
Suppose we are given an inner product ⟨x , y ⟩between vectors x = ( x1 x2 . . . xn )T
and y = ( y1 y2 . . . yn )T in Rn. Our goal is to determine its explicit formula. We begin
by writing the vectors in terms of the standard basis vectors (2.17):
x = x1 e1 + · · · + xn en =
n

i=1
xi ei,
y = y1 e1 + · · · + yn en =
n

j =1
yj ej.
(3.46)
To evaluate their inner product, we will appeal to the three basic axioms. We ﬁrst employ
bilinearity to expand
⟨x , y ⟩=
/
n

i=1
xi ei ,
n

j =1
yj ej
0
=
n

i,j =1
xi yj⟨ei , ej ⟩.
Therefore,
⟨x , y ⟩=
n

i,j =1
kij xi yj = xT K y,
(3.47)
where K denotes the n × n matrix of inner products of the basis vectors, with entries
kij = ⟨ei , ej ⟩,
i, j = 1, . . ., n.
(3.48)
We conclude that any inner product must be expressed in the general bilinear form (3.47).
The two remaining inner product axioms will impose certain constraints on the inner
product matrix K. Symmetry implies that
kij = ⟨ei , ej ⟩= ⟨ej , ei ⟩= kji,
i, j = 1, . . . , n.

3.4 Positive Deﬁnite Matrices
157
Consequently, the inner product matrix K must be symmetric:
K = KT .
Conversely, symmetry of K ensures symmetry of the bilinear form:
⟨x , y ⟩= xT K y = (xT K y)T = yT KT x = yT K x = ⟨y , x ⟩,
where the second equality follows from the fact that the quantity xTK y is a scalar, and
hence equals its transpose.
The ﬁnal condition for an inner product is positivity, which requires that
∥x∥2 = ⟨x , x ⟩= xT K x =
n

i,j =1
kij xi xj ≥0
for all
x ∈Rn,
(3.49)
with equality if and only if x = 0. The precise meaning of this positivity condition on the
matrix K is not so immediately evident, and so will be encapsulated in a deﬁnition.
Deﬁnition 3.26. An n×n matrix K is called positive deﬁnite if it is symmetric, KT = K,
and satisﬁes the positivity condition
xT K x > 0
for all
0 ̸= x ∈Rn.
(3.50)
We will sometimes write K > 0 to mean that K is a positive deﬁnite matrix.
Warning. The condition K > 0 does not mean that all the entries of K are positive. There
are many positive deﬁnite matrices that have some negative entries; see Example 3.28
below.
Conversely, many symmetric matrices with all positive entries are not positive
deﬁnite!
Remark. Although some authors allow non-symmetric matrices to be designated as pos-
itive deﬁnite, we will say that a matrix is positive deﬁnite only when it is symmetric.
But, to underscore our convention and remind the casual reader, we will often include the
superﬂuous adjective “symmetric” when speaking of positive deﬁnite matrices.
Our preliminary analysis has resulted in the following general characterization of inner
products on a ﬁnite-dimensional vector space.
Theorem 3.27. Every inner product on Rn is given by
⟨x , y ⟩= xT K y
for
x, y ∈Rn,
(3.51)
where K is a symmetric, positive deﬁnite n × n matrix.
Given a symmetric matrix K, the homogeneous quadratic polynomial
q(x) = xTK x =
n

i,j =1
kij xi xj,
(3.52)
is known as a quadratic form† on Rn. The quadratic form is called positive deﬁnite if
q(x) > 0
for all
0 ̸= x ∈Rn.
(3.53)
So the quadratic form (3.52) is positive deﬁnite if and only if its coeﬃcient matrix K is.
†
Exercise 3.4.15 shows that the coeﬃcient matrix K in any quadratic form can be taken to be
symmetric without any loss of generality.

158
3 Inner Products and Norms
Example 3.28.
Even though the symmetric matrix K =

4
−2
−2
3

has two negative
entries, it is, nevertheless, a positive deﬁnite matrix. Indeed, the corresponding quadratic
form
q(x) = xTK x = 4x2
1 −4x1 x2 + 3x2
2 = (2x1 −x2)2 + 2x2
2 ≥0
is a sum of two non-negative quantities. Moreover, q(x) = 0 if and only if both 2x1−x2 = 0
and x2 = 0, which implies x1 = 0 also. This proves q(x) > 0 for all x ̸= 0, and hence K is
indeed a positive deﬁnite matrix. The corresponding inner product on R2 is
⟨x , y ⟩= ( x1 x2 )

4
−2
−2
3
 
y1
y2

= 4x1 y1 −2x1 y2 −2x2 y1 + 3x2 y2.
On the other hand, despite the fact that K =

1
2
2
1

has all positive entries, it is not
a positive deﬁnite matrix. Indeed, writing out
q(x) = xT K x = x2
1 + 4x1 x2 + x2
2,
we ﬁnd, for instance, that q(1, −1) = −2 < 0, violating positivity.
These two simple
examples should be enough to convince the reader that the problem of determining whether
a given symmetric matrix is positive deﬁnite is not completely elementary.
Example 3.29.
By deﬁnition, a general symmetric 2×2 matrix K =

a
b
b
c

is positive
deﬁnite if and only if the associated quadratic form satisﬁes
q(x) = ax2
1 + 2bx1 x2 + cx2
2 > 0
for all
x ̸= 0.
(3.54)
Analytic geometry tells us that this is the case if and only if
a > 0,
a c −b2 > 0,
(3.55)
i.e., the quadratic form has positive leading coeﬃcient and positive determinant (or negative
discriminant). A direct proof of this well-known fact will appear shortly.
With a little practice, it is not diﬃcult to read oﬀthe coeﬃcient matrix K from the
explicit formula for the quadratic form (3.52).
Example 3.30.
Consider the quadratic form
q(x, y, z) = x2 + 4xy + 6y2 −2xz + 9z2
depending upon three variables. The corresponding coeﬃcient matrix is
K =
⎛
⎝
1
2
−1
2
6
0
−1
0
9
⎞
⎠
whereby
q(x, y, z) = ( x y
z )
⎛
⎝
1
2
−1
2
6
0
−1
0
9
⎞
⎠
⎛
⎝
x
y
z
⎞
⎠.
Note that the squared terms in q contribute directly to the diagonal entries of K, while
the mixed terms are split in half to give the symmetric oﬀ-diagonal entries. As a challenge,
the reader might wish to try proving that this particular matrix is positive deﬁnite by
establishing positivity of the quadratic form: q(x, y, z) > 0 for all nonzero ( x, y, z )T ∈R3.
Later, we will devise a simple, systematic test for positive deﬁniteness.
Slightly more generally, a quadratic form and its associated symmetric coeﬃcient matrix
are called positive semi-deﬁnite if
q(x) = xT K x ≥0
for all
x ∈Rn,
(3.56)

3.4 Positive Deﬁnite Matrices
159
in which case we write K ≥0. A positive semi-deﬁnite matrix may have null directions,
meaning non-zero vectors z such that q(z) = zT K z = 0. Clearly, every nonzero vector
z ∈ker K that lies in the coeﬃcient matrix’s kernel deﬁnes a null direction, but there
may be others. A positive deﬁnite matrix is not allowed to have null directions, and so
ker K = {0}. Recalling Proposition 2.42, we deduce that all positive deﬁnite matrices are
nonsingular. The converse, however, is not valid; many symmetric, nonsingular matrices
fail to be positive deﬁnite.
Proposition 3.31. If a matrix is positive deﬁnite, then it is nonsingular.
Example 3.32.
The matrix K =

1
−1
−1
1

is positive semi-deﬁnite, but not positive
deﬁnite. Indeed, the associated quadratic form
q(x) = xT K x = x2
1 −2x1 x2 + x2
2 = (x1 −x2)2 ≥0
is a perfect square, and so clearly non-negative. However, the elements of ker K, namely
the scalar multiples of the vector ( 1, 1 )T , deﬁne null directions: q(c, c) = 0.
In a similar fashion, a quadratic form q(x) = xT K x and its associated symmetric matrix
K are called negative semi-deﬁnite if q(x) ≤0 for all x and negative deﬁnite if q(x) < 0
for all x ̸= 0. A quadratic form is called indeﬁnite if it is neither positive nor negative
semi-deﬁnite, equivalently, if there exist points x+ where q(x+) > 0 and points x−where
q(x−) < 0. Details can be found in the exercises.
Only positive deﬁnite matrices deﬁne inner products.
However, indeﬁnite matrices
play a fundamental role in Einstein’s theory of special relativity, [55]. In particular, the
quadratic form associated with the matrix
K =
⎛
⎜
⎝
c2
0
0
0
0
1
0
0
0
0
1
0
0
0
0
1
⎞
⎟
⎠
namely
q(x) = xT K x = c2t2 −x2 −y2 −z2
where
x =
⎛
⎜
⎝
t
x
y
z
⎞
⎟
⎠,
(3.57)
with c representing the speed of light, is the so-called Minkowski “metric” on relativistic
space-time R4. The null directions form the light cone; see Exercise 3.4.20.
Exercises
3.4.1. Which of the following 2 × 2 matrices are positive deﬁnite?
(a)

1
0
0
2
	
, (b)

0
1
2
0
	
, (c)

1
2
2
1
	
, (d)

5
3
3
−2
	
, (e)

1
−1
−1
3
	
, (f )

1
1
−1
2
	
.
In the positive deﬁnite cases, write down the formula for the associated inner product.
3.4.2. Let K =

1
2
2
3
	
. Prove that the associated quadratic form q(x) = xT K x is indeﬁnite
by ﬁnding a point x+ where q(x+) > 0 and a point x−where q(x−) < 0.
♦3.4.3.(a) Prove that a diagonal matrix D = diag (c1, c2, . . . , cn) is positive deﬁnite if and only
if all its diagonal entries are positive: ci > 0.
(b) Write down and identify the associated
inner product.
3.4.4. Write out the Cauchy–Schwarz and triangle inequalities for the inner product deﬁned in
Example 3.28.

160
3 Inner Products and Norms
♦3.4.5.(a) Show that every diagonal entry of a positive deﬁnite matrix must be positive.
(b) Write down a symmetric matrix with all positive diagonal entries that is not positive
deﬁnite. (c) Find a nonzero matrix with one or more zero diagonal entries that is positive
semi-deﬁnite.
3.4.6. Prove that if K is any positive deﬁnite matrix, then every positive scalar multiple cK,
c > 0, is also positive deﬁnite.
♦3.4.7.(a) Show that if K and L are positive deﬁnite matrices, so is K + L. (b) Give an
example of two matrices that are not positive deﬁnite whose sum is positive deﬁnite.
3.4.8. Find two positive deﬁnite matrices K and L whose product K L is not positive deﬁnite.
3.4.9. Write down a nonsingular symmetric matrix that is not positive or negative deﬁnite.
♦3.4.10. Let K be a nonsingular symmetric matrix. (a) Show that xT K−1x = yT K y, where
K y = x. (b) Prove that if K is positive deﬁnite, then so is K−1.
♦3.4.11. Prove that an n × n symmetric matrix K is positive deﬁnite if and only if, for every
0 ̸= v ∈Rn, the vectors v and Kv meet at an acute Euclidean angle: | θ | < 1
2 π.
♦3.4.12. Prove that the inner product associated with a positive deﬁnite quadratic form q(x) is
given by the polarization formula ⟨x , y ⟩= 1
2

q(x + y) −q(x) −q(y)

.
3.4.13.(a) Is it possible for a quadratic form to be positive, q(x+) > 0, at only one point
x+ ∈Rn? (b) Under what conditions is q(x0) = 0 at only one point?
♦3.4.14.(a) Let K and L be symmetric n × n matrices. Prove that xT K x = xT Lx for all
x ∈Rn if and only if K = L. (b) Find an example of two non-symmetric matrices K ̸= L
such that xT K x = xT Lx for all x ∈Rn.
♦3.4.15. Suppose q(x) = xT A x =
n

i,j =1
aij xi xj is a general quadratic form on Rn, whose
coeﬃcient matrix A is not necessarily symmetric. Prove that q(x) = xT K x, where
K = 1
2 (A + AT ) is a symmetric matrix. Therefore, we do not lose any generality by
restricting our discussion to quadratic forms that are constructed from symmetric matrices.
3.4.16.(a) Show that a symmetric matrix N is negative deﬁnite if and only if K = −N is
positive deﬁnite. (b) Write down two explicit criteria that tell whether a 2 × 2 matrix
N =

a
b
b
c
	
is negative deﬁnite.
(c) Use your criteria to check whether
(i)

−1
1
1
−2
	
,
(ii)

−4
−5
−5
−6
	
,
(iii)

−3
−1
−1
2
	
are negative deﬁnite.
3.4.17. Show that x =

1
1
	
is a null direction for K =

1
−2
−2
3
	
, but x ̸∈ker K.
3.4.18. Explain why an indeﬁnite quadratic form necessarily has a non-zero null direction.
3.4.19. Let K = KT . True or false: (a) If K admits a null direction, then ker K ̸= {0}.
(b) If K has no null directions, then K is either positive or negative deﬁnite.
♦3.4.20. In special relativity, light rays in Minkowski space-time Rn travel along the light cone
which, by deﬁnition, consists of all null directions associated with an indeﬁnite quadratic
form q(x) = xT K x. Find and sketch a picture of the light cone when the coeﬃcient matrix
K is (a)

1
0
0
−1
	
, (b)

1
2
2
3
	
, (c)
⎛
⎜
⎝
1
0
0
0
−1
0
0
0
−1
⎞
⎟
⎠.
Remark. In the physical
universe, space-time is n = 4-dimensional, and K is given in (3.57), [55].

3.4 Positive Deﬁnite Matrices
161
♦3.4.21. A function f(x) on Rn is called homogeneous of degree k if f(c x) = ckf(x) for all
scalars c.
(a) Given a ∈Rn, show that the linear form ℓ(x) = a · x = a1 x1 + · · · + an xn is
homogeneous of degree 1.
(b) Show that the quadratic form
q(x) = xT K x =
n

i,j =1
kij xi xj is homogeneous of degree 2.
(c) Find a homogeneous function of degree 2 on R2 that is not a quadratic form.
Gram Matrices
Symmetric matrices whose entries are given by inner products of elements of an inner
product space will appear throughout this text. They are named after the nineteenth-
century Danish mathematician Jørgen Gram — not the metric mass unit!
Deﬁnition 3.33. Let V be an inner product space, and let v1, . . . , vn ∈V . The associated
Gram matrix
K =
⎛
⎜
⎜
⎜
⎜
⎝
⟨v1 , v1 ⟩
⟨v1 , v2 ⟩
. . .
⟨v1 , vn ⟩
⟨v2 , v1 ⟩
⟨v2 , v2 ⟩
. . .
⟨v2 , vn ⟩
...
...
...
...
⟨vn , v1 ⟩
⟨vn , v2 ⟩
. . .
⟨vn , vn ⟩
⎞
⎟
⎟
⎟
⎟
⎠
(3.58)
is the n × n matrix whose entries are the inner products between the selected vector space
elements.
Symmetry of the inner product implies symmetry of the Gram matrix:
kij = ⟨vi , vj ⟩= ⟨vj , vi ⟩= kji,
and hence
KT = K.
(3.59)
In fact, the most direct method for producing positive deﬁnite and semi-deﬁnite matrices
is through the Gram matrix construction.
Theorem 3.34. All Gram matrices are positive semi-deﬁnite. The Gram matrix (3.58) is
positive deﬁnite if and only if v1, . . . , vn are linearly independent.
Proof : To prove positive (semi-)deﬁniteness of K, we need to examine the associated quad-
ratic form
q(x) = xT K x =
n

i,j =1
kij xi xj.
Substituting the values (3.59) for the matrix entries, we obtain
q(x) =
n

i,j =1
⟨vi , vj ⟩xi xj.
Bilinearity of the inner product on V implies that we can assemble this summation into a
single inner product
q(x) =
/
n

i=1
xi vi ,
n

j =1
xj vj
0
= ⟨v , v ⟩= ∥v∥2 ≥0,
where
v =
n

i=1
xi vi
lies in the subspace of V spanned by the given vectors. This immediately proves that K is
positive semi-deﬁnite.

162
3 Inner Products and Norms
Moreover, q(x) = ∥v∥2 > 0 as long as v ̸= 0. If v1, . . . , vn are linearly independent,
then
v = x1 v1 + · · · + xn vn = 0
if and only if
x1 = · · · = xn = 0,
and hence q(x) = 0 if and only if x = 0. This implies that q(x) and hence K are positive
deﬁnite.
Q.E.D.
Example 3.35.
Consider the vectors v1 =
⎛
⎝
1
2
−1
⎞
⎠, v2 =
⎛
⎝
3
0
6
⎞
⎠.
For the standard
Euclidean dot product on R3, the Gram matrix is
K =

v1 · v1
v1 · v2
v2 · v1
v2 · v2

=

6
−3
−3
45

.
Since v1, v2 are linearly independent, K > 0. Positive deﬁniteness implies that
q(x1, x2) = 6x2
1 −6x1 x2 + 45x2
2 > 0
for all
(x1, x2) ̸= 0.
Indeed, this can be checked directly, by using the criteria in (3.55).
On the other hand, for the weighted inner product
⟨v , w ⟩= 3v1 w1 + 2v2 w2 + 5v3 w3,
(3.60)
the corresponding Gram matrix is
K =

⟨v1 , v1 ⟩
⟨v1 , v2 ⟩
⟨v2 , v1 ⟩
⟨v2 , v2 ⟩

=

16
−21
−21
207

.
(3.61)
Since v1, v2 are still linearly independent (which, of course, does not depend upon which
inner product is used), the matrix K is also positive deﬁnite.
In the case of the Euclidean dot product, the construction of the Gram matrix K can
be directly implemented as follows. Given column vectors v1, . . . , vn ∈Rm, let us form
the m × n matrix A = ( v1 v2 . . . vn ). In view of the identiﬁcation (3.2) between the dot
product and multiplication of row and column vectors, the (i, j) entry of K is given as the
product
kij = vi · vj = vT
i vj
of the ith row of the transpose AT and the jth column of A. In other words, the Gram
matrix can be evaluated as a matrix product:
K = ATA.
(3.62)
For the preceding Example 3.35,
A =
⎛
⎝
1
3
2
0
−1
6
⎞
⎠,
and so
K = ATA =

1
2
−1
3
0
6
⎛
⎝
1
3
2
0
−1
6
⎞
⎠=

6
−3
−3
45

.
Theorem 3.34 implies that the Gram matrix (3.62) is positive deﬁnite if and only if the
columns of A are linearly independent vectors. This implies the following result.
Proposition 3.36. Given an m × n matrix A, the following are equivalent:
(a) The n × n Gram matrix K = ATA is positive deﬁnite.
(b) A has linearly independent columns.
(c) rank A = n ≤m.
(d) ker A = {0}.

3.4 Positive Deﬁnite Matrices
163
Changing the underlying inner product will, of course, change the Gram matrix. As
noted in Theorem 3.27, every inner product on Rm has the form
⟨v , w ⟩= vT C w
for
v, w ∈Rm,
(3.63)
where C > 0 is a symmetric, positive deﬁnite m × m matrix. Therefore, given n vectors
v1, . . . , vn ∈Rm, the entries of the Gram matrix with respect to this inner product are
kij = ⟨vi , vj ⟩= vT
i C vj.
If, as above, we assemble the column vectors into an m × n matrix A = ( v1 v2 . . . vn ),
then the Gram matrix entry kij is obtained by multiplying the ith row of AT by the jth
column of the product matrix C A. Therefore, the Gram matrix based on the alternative
inner product (3.63) is given by
K = AT C A.
(3.64)
Theorem 3.34 immediately implies that K is positive deﬁnite — provided that the matrix
A has rank n.
Theorem 3.37. Suppose A is an m × n matrix with linearly independent columns. Sup-
pose C is any positive deﬁnite m × m matrix. Then the Gram matrix K = AT C A is a
positive deﬁnite n × n matrix.
The Gram matrices constructed in (3.64) arise in a wide variety of applications, including
least squares approximation theory (cf. Chapter 5), and mechanical structures and electrical
circuits (cf. Chapters 6 and 10). In the majority of applications, C = diag (c1, . . . , cm) is a
diagonal positive deﬁnite matrix, which requires it to have strictly positive diagonal entries
ci > 0. This choice corresponds to a weighted inner product (3.10) on Rm.
Example 3.38.
Returning to the situation of Example 3.35, the weighted inner product
(3.60) corresponds to the diagonal positive deﬁnite matrix C =
⎛
⎝
3
0
0
0
2
0
0
0
5
⎞
⎠. Therefore,
the weighted Gram matrix (3.64) based on the vectors v1 =
⎛
⎝
1
2
−1
⎞
⎠, v2 =
⎛
⎝
3
0
6
⎞
⎠, is
K = AT C A =

1
2
−1
3
0
6
 ⎛
⎝
3
0
0
0
2
0
0
0
5
⎞
⎠
⎛
⎝
1
3
2
0
−1
6
⎞
⎠=

16
−21
−21
207

,
reproducing (3.61).
The Gram matrix construction is not restricted to ﬁnite-dimensional vector spaces, but
also applies to inner products on function space. Here is a particularly important example.
Example 3.39.
Consider the vector space C0[0, 1] consisting of continuous functions
on the interval 0 ≤x ≤1, equipped with the L2 inner product ⟨f , g ⟩=
 1
0
f(x) g(x) dx.
Let us construct the Gram matrix corresponding to the simple monomial functions 1, x, x2.

164
3 Inner Products and Norms
We compute the required inner products
⟨1 , 1 ⟩= ∥1∥2 =
 1
0
dx = 1,
⟨1 , x ⟩=
 1
0
x dx = 1
2 ,
⟨x , x ⟩= ∥x∥2 =
 1
0
x2 dx = 1
3,
⟨1 , x2 ⟩=
 1
0
x2 dx = 1
3 ,
⟨x2 , x2 ⟩= ∥x2 ∥2 =
 1
0
x4 dx = 1
5,
⟨x , x2 ⟩=
 1
0
x3 dx = 1
4 .
Therefore, the Gram matrix is
K =
⎛
⎜
⎝
⟨1 , 1 ⟩
⟨1 , x ⟩
⟨1 , x2 ⟩
⟨x , 1 ⟩
⟨x , x ⟩
⟨x , x2 ⟩
⟨x2 , 1 ⟩
⟨x2 , x ⟩
⟨x2 , x2 ⟩
⎞
⎟
⎠=
⎛
⎜
⎝
1
1
2
1
3
1
2
1
3
1
4
1
3
1
4
1
5
⎞
⎟
⎠.
As we know, the monomial functions 1, x, x2 are linearly independent, and so Theorem 3.34
immediately implies that the matrix K is positive deﬁnite.
The alert reader may recognize this particular Gram matrix as the 3 × 3 Hilbert matrix
that we encountered in (1.72). More generally, the Gram matrix corresponding to the
monomials 1, x, x2, . . . , xn has entries
kij = ⟨xi−1 , xj−1 ⟩=
 1
0
xi+j−2 dx =
1
i + j −1 ,
i, j = 1, . . ., n + 1,
and is thus the (n + 1) × (n + 1) Hilbert matrix (1.72): K = Hn+1. As a consequence of
Theorem 3.34 and Proposition 3.31 (and also Exercise 2.3.36), we have proved the following
non-trivial result.
Proposition 3.40. The n × n Hilbert matrix Hn is positive deﬁnite. Consequently, Hn
is a nonsingular matrix.
Example 3.41.
Let us construct the Gram matrix corresponding to the trigonometric
functions 1, cos x, sin x, with respect to the inner product ⟨f , g ⟩=
 π
−π
f(x) g(x) dx on
the interval [−π, π ]. We compute the inner products
⟨1 , 1 ⟩= ∥1∥2 =
 π
−π
dx = 2π,
⟨1 , cosx ⟩=
 π
−π
cos x dx = 0,
⟨cos x , cos x ⟩= ∥cosx∥2 =
 π
−π
cos2 x dx = π,
⟨1 , sin x ⟩=
 π
−π
sin x dx = 0,
⟨sin x , sin x ⟩= ∥sin x∥2 =
 π
−π
sin2 x dx = π,
⟨cos x , sin x ⟩=
 π
−π
cos x sin x dx = 0.
Therefore, the Gram matrix is a simple diagonal matrix: K =
⎛
⎝
2π
0
0
0
π
0
0
0
π
⎞
⎠. Positive
deﬁniteness of K is immediately evident.
If the columns of A are linearly dependent, then the associated Gram matrix is only
positive semi-deﬁnite. In this case, the Gram matrix will have nontrivial null directions v,
so that 0 ̸= v ∈ker K = ker A.

3.4 Positive Deﬁnite Matrices
165
Proposition 3.42. Let K = AT C A be the n×n Gram matrix constructed from an m×n
matrix A and a positive deﬁnite m × m matrix C > 0. Then ker K = ker A, and hence
rank K = rank A.
Proof : Clearly, if A x = 0, then K x = AT C A x = 0, and so ker A ⊂ker K. Conversely, if
K x = 0, then
0 = xT K x = xT AT C A x = yT C y,
where
y = A x.
Since C > 0, this implies y = 0, and hence x ∈ker A. Finally, by Theorem 2.49, rank K =
n −dim ker K = n −dim ker A = rank A.
Q.E.D.
Exercises
3.4.22.(a) Find the Gram matrix corresponding to each of the following sets of vectors using
the Euclidean dot product on Rn.
(i)

−1
3
	
,

0
2
	
, (ii)

1
2
	
,

−2
3
	
,

−1
−1
	
,
(iii)
⎛
⎜
⎝
2
1
−1
⎞
⎟
⎠,
⎛
⎜
⎝
−3
0
2
⎞
⎟
⎠, (iv)
⎛
⎜
⎝
1
1
0
⎞
⎟
⎠,
⎛
⎜
⎝
1
0
1
⎞
⎟
⎠,
⎛
⎜
⎝
0
1
1
⎞
⎟
⎠, (v)
⎛
⎜
⎝
1
−2
2
⎞
⎟
⎠,
⎛
⎜
⎝
2
−1
1
⎞
⎟
⎠,
⎛
⎜
⎝
−1
−1
1
⎞
⎟
⎠,
(vi)
⎛
⎜
⎜
⎜
⎝
1
0
−1
0
⎞
⎟
⎟
⎟
⎠,
⎛
⎜
⎜
⎜
⎝
−1
1
0
1
⎞
⎟
⎟
⎟
⎠, (vii)
⎛
⎜
⎜
⎜
⎝
1
2
3
4
⎞
⎟
⎟
⎟
⎠,
⎛
⎜
⎜
⎜
⎝
−2
1
−4
3
⎞
⎟
⎟
⎟
⎠,
⎛
⎜
⎜
⎜
⎝
−1
3
−1
−2
⎞
⎟
⎟
⎟
⎠, (viii)
⎛
⎜
⎜
⎜
⎝
1
0
0
1
⎞
⎟
⎟
⎟
⎠,
⎛
⎜
⎜
⎜
⎝
−2
1
0
0
⎞
⎟
⎟
⎟
⎠,
⎛
⎜
⎜
⎜
⎝
−1
0
−1
0
⎞
⎟
⎟
⎟
⎠,
⎛
⎜
⎜
⎜
⎝
0
2
−3
0
⎞
⎟
⎟
⎟
⎠.
(b) Which are positive deﬁnite? (c) If the matrix is positive semi-deﬁnite, ﬁnd all its null
directions.
3.4.23. Recompute the Gram matrices for cases (iii)–(v) in the previous exercise using the
weighted inner product ⟨x , y ⟩= x1 y1 + 2 x2 y2 + 3x3 y3. Does this change its positive
deﬁniteness?
3.4.24. Recompute the Gram matrices for cases (vi)–(viii) in Exercise 3.4.22 for the weighted
inner product ⟨x , y ⟩= x1 y1 + 1
2 x2 y2 + 1
3 x3 y3 + 1
4 x4 y4.
3.4.25. Find the Gram matrix K for the functions 1, ex, e2x using the L2 inner product on
[0, 1]. Is K positive deﬁnite?
3.4.26. Answer Exercise 3.4.25 using the weighted inner product ⟨f , g ⟩=
 1
0 f(x) g(x) e−x dx.
3.4.27. Find the Gram matrix K for the monomials 1, x, x2, x3 using the L2 inner product on
[−1, 1]. Is K positive deﬁnite?
3.4.28. Answer Exercise 3.4.27 using the weighted inner product ⟨f , g ⟩=
 1
−1 f(x) g(x) (1 + x) dx.
3.4.29. Let K be a 2 × 2 Gram matrix. Explain why the positive deﬁniteness criterion (3.55) is
equivalent to the Cauchy–Schwarz inequality.
♦3.4.30.(a) Prove that if K is a positive deﬁnite matrix, then K2 is also positive deﬁnite.
(b) More generally, if S = ST is symmetric and nonsingular, then S2 is positive deﬁnite.
3.4.31. Let A be an m × n matrix. (a) Explain why the product L = AAT is a Gram matrix.
(b) Show that, even though they may be of diﬀerent sizes, both Gram matrices K = ATA
and L = AAT have the same rank.
(c) Under what conditions are both K and L positive
deﬁnite?

166
3 Inner Products and Norms
♦3.4.32. Let K = AT C A, where C > 0. Prove that
(a) ker K = coker K = ker A;
(b) img K = coimg K = coimg A.
♦3.4.33. Prove that every positive deﬁnite matrix K can be written as a Gram matrix.
3.4.34. Suppose K is the Gram matrix computed from v1, . . . , vn ∈V relative to a given inner
product. Let 
K be the Gram matrix for the same elements, but computed relative to a
diﬀerent inner product. Show that K > 0 if and only if 
K > 0.
♦3.4.35. Let K1 = AT
1 C1 A1 and K2 = AT
2 C2 A2 be any two n × n Gram matrices. Let
K = K1 + K2. (a) Show that if K1, K2 > 0 then K > 0. (b) Give an example in which K1
and K2 are not positive deﬁnite, but K > 0. (c) Show that K is also a Gram matrix, by
ﬁnding a matrix A such that K = AT C A. Hint: A will have size (m1 + m2) × n, where m1
and m2 are the numbers of rows in A1, A2, respectively.
3.4.36. Show that 0 ̸= z is a null direction for the quadratic form q(x) = xT K x based on the
Gram matrix K = AT C A if and only if z ∈ker K.
3.5 Completing the Square
Gram matrices furnish us with an almost inexhaustible supply of positive deﬁnite matrices.
However, we still do not know how to test whether a given symmetric matrix is positive
deﬁnite. As we shall soon see, the secret already appears in the particular computations
in Examples 3.2 and 3.28.
You may recall the algebraic technique known as “completing the square”, ﬁrst arising
in the derivation of the formula for the solution to the quadratic equation
q(x) = a x2 + 2b x + c = 0,
(3.65)
and, later, helping to facilitate the integration of various types of rational and algebraic
functions. The idea is to combine the ﬁrst two terms in (3.65) as a perfect square, and
thereby rewrite the quadratic function in the form
q(x) = a

x + b
a
2
+ ac −b2
a
= 0.
(3.66)
As a consequence,

x + b
a
2
= b2 −ac
a2
.
The familiar quadratic formula
x = −b ±
√
b2 −ac
a
follows by taking the square root of both sides and then solving for x. The intermediate
step (3.66), where we eliminate the linear term, is known as completing the square.
We can perform the same kind of manipulation on a homogeneous quadratic form
q(x1, x2) = ax2
1 + 2bx1 x2 + cx2
2.
(3.67)
In this case, provided a ̸= 0, completing the square amounts to writing
q(x1, x2) = ax2
1 + 2bx1x2 + cx2
2 = a

x1 + b
a x2
2
+ ac −b2
a
x2
2 = ay2
1 + ac −b2
a
y2
2.
(3.68)

3.5 Completing the Square
167
The net result is to re-express q(x1, x2) as a simpler sum of squares of the new variables
y1 = x1 + b
a x2,
y2 = x2.
(3.69)
It is not hard to see that the ﬁnal expression in (3.68) is positive deﬁnite, as a function of
y1, y2, if and only if both coeﬃcients are positive:
a > 0,
ac −b2
a
> 0.
(3.70)
Therefore, q(x1, x2) ≥0, with equality if and only if y1 = y2 = 0, or, equivalently, x1 =
x2 = 0. This conclusively proves that conditions (3.70) are necessary and suﬃcient for the
quadratic form (3.67) to be positive deﬁnite.
Our goal is to adapt this simple idea to analyze the positivity of quadratic forms de-
pending on more than two variables. To this end, let us rewrite the quadratic form identity
(3.68) in matrix form. The original quadratic form (3.67) is
q(x) = xT K x,
where
K =

a
b
b
c

,
x =

x1
x2

.
(3.71)
Similarly, the right-hand side of (3.68) can be written as
q (y) = yT D y,
where
D =
 a
0
0
ac −b2
a

,
y =

y1
y2

.
(3.72)
Anticipating the ﬁnal result, the equations (3.69) connecting x and y can themselves be
written in matrix form as
y = LT x
or

y1
y2

=

x1 + b
a x2
x2

,
where
LT =

1
b
a
0
1

.
Substituting into (3.72), we obtain
yT D y = (LT x)TD (LT x) = xT L D LT x = xT K x,
where
K = LDLT .
(3.73)
The result is the same factorization (1.61) of the coeﬃcient matrix that we previously
obtained via Gaussian Elimination. We are thus led to the realization that completing the
square is the same as the LDLT factorization of a symmetric matrix!
Recall the deﬁnition of a regular matrix as one that can be reduced to upper triangular
form without any row interchanges. Theorem 1.34 says that the regular symmetric matrices
are precisely those that admit an LDLT factorization. The identity (3.73) is therefore valid
for all regular n × n symmetric matrices, and shows how to write the associated quadratic
form as a sum of squares:
q(x) = xTK x = yT Dy = d1 y2
1 + · · · + dn y2
n,
where
y = LT x.
(3.74)
The coeﬃcients di are the diagonal entries of D, which are the pivots of K. Furthermore,
the diagonal quadratic form is positive deﬁnite, yT Dy > 0 for all y ̸= 0, if and only if
all the pivots are positive, di > 0. Invertibility of LT tells us that y = 0 if and only
if x = 0, and hence, positivity of the pivots is equivalent to positive deﬁniteness of the
original quadratic form: q(x) > 0 for all x ̸= 0. We have thus almost proved the main
result that completely characterizes positive deﬁnite matrices.
Theorem 3.43. A symmetric matrix is positive deﬁnite if and only if it is regular and has
all positive pivots.

168
3 Inner Products and Norms
Equivalently, a square matrix K is positive deﬁnite if and only if it can be factored
K = LDLT , where L is lower unitriangular and D is diagonal with all positive diagonal
entries.
Example 3.44.
Consider the symmetric matrix K =
⎛
⎝
1
2
−1
2
6
0
−1
0
9
⎞
⎠. Gaussian Elim-
ination produces the factors
L =
⎛
⎝
1
0
0
2
1
0
−1
1
1
⎞
⎠,
D =
⎛
⎝
1
0
0
0
2
0
0
0
6
⎞
⎠,
LT =
⎛
⎝
1
2
−1
0
1
1
0
0
1
⎞
⎠,
in its factorization K = LDLT . Since the pivots — the diagonal entries 1, 2, and 6 in D
— are all positive, Theorem 3.43 implies that K is positive deﬁnite, which means that the
associated quadratic form satisﬁes
q(x) = x2
1 + 4x1 x2 −2x1 x3 + 6x2
2 + 9x2
3 > 0,
for all
x = ( x1, x2, x3 )T ̸= 0.
Indeed, the LDLT factorization implies that q(x) can be explicitly written as a sum of
squares:
q(x) = x2
1 + 4x1 x2 −2x1 x3 + 6x2
2 + 9x2
3 = y2
1 + 2y2
2 + 6y2
3,
(3.75)
where
y1 = x1 + 2x2 −x3,
y2 = x2 + x3,
y3 = x3,
are the entries of y = LT x. Positivity of the coeﬃcients of the y2
i (which are the pivots)
implies that q(x) is positive deﬁnite.
Example 3.45.
Let’s test whether the matrix K =
⎛
⎝
1
2
3
2
3
7
3
7
8
⎞
⎠is positive deﬁnite.
When we perform Gaussian Elimination, the second pivot turns out to be −1, which
immediately implies that K is not positive deﬁnite — even though all its entries are positive.
(The third pivot is 3, but this does not aﬀect the conclusion; all it takes is one non-positive
pivot to disqualify a matrix from being positive deﬁnite. Also, row interchanges aren’t of
any help, since we are not allowed to perform them when checking for positive deﬁniteness.)
This means that the associated quadratic form
q(x) = x2
1 + 4x1 x2 + 6x1 x3 + 3x2
2 + 14x2x3 + 8x2
3
assumes negative values at some points. For instance, q(−2, 1, 0) = −1.
A direct method for completing the square in a quadratic form goes as follows: The ﬁrst
step is to put all the terms involving x1 in a suitable square, at the expense of introducing
extra terms involving only the other variables. For instance, in the case of the quadratic
form in (3.75), the terms involving x1 can be written as
x2
1 + 4x1 x2 −2x1 x3 = (x1 + 2x2 −x3)2 −4x2
2 + 4x2 x3 −x2
3.
Therefore,
q(x) = (x1 + 2x2 −x3)2 + 2x2
2 + 4x2 x3 + 8x2
3 = (x1 + 2x2 −x3)2 + q(x2, x3),
where
q(x2, x3) = 2x2
2 + 4x2 x3 + 8x2
3

3.5 Completing the Square
169
is a quadratic form that involves only x2, x3. We then repeat the process, combining all
the terms involving x2 in the remaining quadratic form into a square, writing
q(x2, x3) = 2(x2 + x3)2 + 6x2
3.
This gives the ﬁnal form
q(x) = (x1 + 2x2 −x3)2 + 2(x2 + x3)2 + 6x2
3,
which reproduces (3.75).
In general, as long as k11 ̸= 0, we can write
q(x) = xT K x = k11 x2
1 + 2k12 x1 x2 + · · · + 2k1n x1 xn + k22 x2
2 + · · · + knn x2
n
= k11

x1 + k12
k11
x2 + · · · + k1n
k11
xn
2
+ q(x2, . . . , xn)
= k11 (x1 + l21 x2 + · · · + ln1 xn)2 + q(x2, . . . , xn),
(3.76)
where
l21 = k21
k11
= k12
k11
,
. . .
ln1 = kn1
k11
= k1n
k11
are precisely the multiples appearing in the matrix L obtained from applying Gaussian
Elimination to K, while
q(x2, . . . , xn) =
n

i,j =2
kij xi xj
is a quadratic form involving one less variable. The entries of its symmetric coeﬃcient
matrix K are
kij = kji = kij −lj1 k1i = kij −
k1j k1i
k11
,
i, j = 2, . . . n,
which are exactly the same as the entries appearing below and to the right of the ﬁrst
pivot after applying the the ﬁrst phase of the Gaussian Elimination process to K.
In
particular, the second pivot of K is the diagonal entry k22. We continue by applying the
same procedure to the reduced quadratic form q(x2, . . . , xn) and repeating until only the
ﬁnal variable remains. Completing the square at each stage reproduces the corresponding
phase of the Gaussian Elimination process. The ﬁnal result is our formula (3.74) rewriting
the original quadratic form as a sum of squares whose coeﬃcients are the pivots.
With this in hand, we can now complete the proof of Theorem 3.43. First, if the upper
left entry k11, namely the ﬁrst pivot, is not strictly positive, then K cannot be positive
deﬁnite, because q(e1) = eT
1 K e1 = k11 ≤0. Otherwise, suppose k11 > 0, and so we can
write q(x) in the form (3.76). We claim that q(x) is positive deﬁnite if and only if the
reduced quadratic form q(x2, . . ., xn) is positive deﬁnite. Indeed, if q is positive deﬁnite
and k11 > 0, then q(x) is the sum of two positive quantities, which simultaneously vanish
if and only if x1 = x2 = · · · = xn = 0. On the other hand, suppose q(x⋆
2, . . . , x⋆
n) ≤0 for
some x⋆
2, . . . , x⋆
n, not all zero. Setting x⋆
1 = −l21 x⋆
2 −· · · −ln1 x⋆
n makes the initial square
term in (3.76) equal to 0, so
q(x⋆
1, x⋆
2, . . . , x⋆
n) = q(x⋆
2, . . . , x⋆
n) ≤0,
proving the claim. In particular, positive deﬁniteness of q requires that the second pivot
satisfy k22 > 0.
We then continue the reduction procedure outlined in the preceding

170
3 Inner Products and Norms
paragraph; if a non-positive entry appears in the diagonal pivot position at any stage, the
original quadratic form and matrix cannot be positive deﬁnite. On the other hand, ﬁnding
all positive pivots (without using any row interchanges) will, in the absence of numerical
errors, ensure positive deﬁniteness.
Q.E.D.
Exercises
3.5.1. Are the following matrices are positive deﬁnite?
(a)

4
−2
−2
4
	
, (b)

1
1
1
1
	
,
(c)
⎛
⎜
⎝
1
1
2
1
2
1
2
1
1
⎞
⎟
⎠, (d)
⎛
⎜
⎝
1
1
1
1
2
−2
1
−2
4
⎞
⎟
⎠, (e)
⎛
⎜
⎜
⎜
⎝
2
1
1
1
1
2
1
1
1
1
2
1
1
1
1
2
⎞
⎟
⎟
⎟
⎠, (f )
⎛
⎜
⎜
⎜
⎝
−1
1
1
1
1
−1
1
1
1
1
−1
1
1
1
1
−1
⎞
⎟
⎟
⎟
⎠.
3.5.2. Find an LDLT factorization of the following symmetric matrices. Which are positive
deﬁnite?
(a)

1
2
2
3
	
, (b)

5
−1
−1
3
	
, (c)
⎛
⎜
⎝
3
−1
3
−1
5
1
3
1
5
⎞
⎟
⎠, (d)
⎛
⎜
⎝
−2
1
−1
1
−2
1
−1
1
−2
⎞
⎟
⎠,
(e)
⎛
⎜
⎝
2
1
−2
1
1
−3
−2
−3
11
⎞
⎟
⎠, (f )
⎛
⎜
⎜
⎜
⎝
1
1
1
0
1
2
0
1
1
0
1
1
0
1
1
2
⎞
⎟
⎟
⎟
⎠, (g)
⎛
⎜
⎜
⎜
⎝
3
2
1
0
2
3
0
1
1
0
3
2
0
1
2
3
⎞
⎟
⎟
⎟
⎠, (h)
⎛
⎜
⎜
⎜
⎝
2
1
−2
0
1
1
−3
2
−2
−3
10
−1
0
2
−1
7
⎞
⎟
⎟
⎟
⎠.
3.5.3.(a) For which values of c is the matrix A =
⎛
⎜
⎝
1
1
0
1
c
1
0
1
1
⎞
⎟
⎠positive deﬁnite? (b) For the
particular value c = 3, carry out elimination to ﬁnd the factorization A = LDLT .
(c) Use
your result from part (b) to rewrite the quadratic form q(x, y, z) = x2+2xy+3y2+2y z+z2
as a sum of squares. (d) Explain how your result is related to the positive deﬁniteness of A.
3.5.4. Write the quadratic form q(x) = x2
1 +x1 x2 +2x2
2 −x1 x3 +3x2
3 in the form q(x) = xT K x
for some symmetric matrix K. Is q(x) positive deﬁnite?
3.5.5. Write the following quadratic forms on R2 as a sum of squares. Which are positive
deﬁnite?
(a) x2 + 8xy + y2,
(b) x2 −4xy + 7y2,
(c) x2 −2xy −y2,
(d) x2 + 6xy.
3.5.6. Prove that the following quadratic forms on R3 are positive deﬁnite by writing each as a
sum of squares:
(a) x2 + 4xz + 3y2 + 5z2,
(b) x2 + 3xy + 3y2 −2xz + 8z2,
(c) 2x2
1 + x1 x2 −2x1 x3 + 2x2
2 −2x2 x3 + 2x2
3.
3.5.7. Write the following quadratic forms in matrix notation and determine if they are positive
deﬁnite:
(a) x2 + 4xz + 2y2 + 8y z + 12z2,
(b) 3x2 −2y2 −8xy + xz + z2,
(c) x2 + 2xy + 2y2 −4xz −6y z + 6z2,
(d) 3x2
1 −x2
2 + 5x2
3 + 4x1x2 −7x1x3 + 9x2 x3,
(e) x2
1 + 4x1 x2 −2x1 x3 + 5x2
2 −2x2 x4 + 6x2
3 −x3 x4 + 4x2
4.
3.5.8. For what values of a, b, and c is the quadratic form x2 + axy + y2 + bxz + cy z + z2
positive deﬁnite?
3.5.9. True or false: Every planar quadratic form q(x, y) = ax2 + 2bxy + cy2 can be written as
a sum of squares.
3.5.10.(a) Prove that a positive deﬁnite matrix has positive determinant: det K > 0.
(b) Show that a positive deﬁnite matrix has positive trace: tr K > 0. (c) Show that every
2 × 2 symmetric matrix with positive determinant and positive trace is positive deﬁnite.
(d) Find a symmetric 3 × 3 matrix with positive determinant and positive trace that is not
positive deﬁnite.

3.5 Completing the Square
171
3.5.11.(a) Prove that if K1, K2 are positive deﬁnite n × n matrices, then K =

K1
O
O
K2
	
is a positive deﬁnite 2n × 2n matrix.
(b) Is the converse true?
3.5.12. Let ∥·∥be any norm on Rn. (a) Show that q(x) is a positive deﬁnite quadratic form
if and only if q(u) > 0 for all unit vectors, ∥u∥= 1.
(b) Prove that if S = ST is any
symmetric matrix, then K = S + c I > 0 is positive deﬁnite if c is suﬃciently large.
3.5.13. Prove that every symmetric matrix S = K + N can be written as the sum of a positive
deﬁnite matrix K and a negative deﬁnite matrix N. Hint: Use Exercise 3.5.12(b).
♦3.5.14.(a) Prove that every regular symmetric matrix can be decomposed as a linear combination
K = d1 l1 lT
1 + d2 l2 lT
2 + · · · + dn ln lT
n
(3.77)
of symmetric rank 1 matrices, as in Exercise 1.8.15, where l1, . . . , ln are the columns of the
lower unitriangular matrix L and d1, . . . , dn are the pivots, i.e., the diagonal entries of D.
Hint: See Exercise 1.2.34.
(b) Decompose

4
−1
−1
1
	
and
⎛
⎜
⎝
1
2
1
2
6
1
1
1
4
⎞
⎟
⎠in this manner.
♥3.5.15. There is an alternative criterion for positive deﬁniteness based on subdeterminants of
the matrix. The 2 × 2 version already appears in (3.70). (a) Prove that a 3 × 3 matrix
K =
⎛
⎜
⎝
a
b
c
b
d
e
c
e
f
⎞
⎟
⎠is positive deﬁnite if and only if a > 0, ad −b2 > 0, and det K > 0.
(b) Prove the general version: an n × n matrix K > 0 is positive deﬁnite if and only if its
upper left square k × k submatrices have positive determinant for all k = 1, . . . , n.
Hint: See Exercise 1.9.17.
♦3.5.16. Let K be a symmetric matrix. Prove that if a non-positive diagonal entry appears
anywhere (not necessarily in the pivot position) in the matrix during Regular Gaussian
Elimination, then K is not positive deﬁnite.
♦3.5.17. Formulate a determinantal criterion similar to that in Exercise 3.5.15 for negative
deﬁnite matrices. Write out the 2 × 2 and 3 × 3 cases explicitly.
3.5.18. True or false: A negative deﬁnite matrix must have negative trace and negative
determinant.
The Cholesky Factorization
The identity (3.73) shows us how to write an arbitrary regular quadratic form q(x) as
a linear combination of squares. We can push this result slightly further in the positive
deﬁnite case. Since each pivot di is positive, we can write the diagonal quadratic form
(3.74) as a sum of pure squares:
d1 y2
1 + · · · + dn y2
n =
 
d1 y1
2 + · · · +
 
dn yn
2 = z2
1 + · · · + z2
n,
where zi =

di yi. In matrix form, we are writing
q (y) = yT Dy = zTz = ∥z∥2,
where
z = S y,
with
S = diag
$ 
d1 , . . . ,

dn
%
.
Since D = S2, the matrix S can be thought of as a “square root” of the diagonal matrix
D. Substituting back into (1.58), we deduce the Cholesky factorization
K = LDLT = LS ST LT = M M T,
where
M = LS,
(3.78)
of a positive deﬁnite matrix, ﬁrst proposed by the early twentieth-century French geogra-
pher Andr´e-Louis Cholesky for solving problems in geodetic surveying. Note that M is a

172
3 Inner Products and Norms
lower triangular matrix with all positive diagonal entries, namely the square roots of the
pivots: mii =

di . Applying the Cholesky factorization to the corresponding quadratic
form produces
q(x) = xT K x = xT M M Tx = zTz = ∥z∥2,
where
z = M Tx.
(3.79)
We can interpret (3.79) as a change of variables from x to z that converts an arbitrary
inner product norm, as deﬁned by the square root of the positive deﬁnite quadratic form
q(x), into the standard Euclidean norm ∥z∥.
Example 3.46.
For the matrix K =
⎛
⎝
1
2
−1
2
6
0
−1
0
9
⎞
⎠considered in Example 3.44, the
Cholesky formula (3.78) gives K = M M T, where
M = LS =
⎛
⎝
1
0
0
2
1
0
−1
1
1
⎞
⎠
⎛
⎝
1
0
0
0
√
2
0
0
0
√
6
⎞
⎠=
⎛
⎝
1
0
0
2
√
2
0
−1
√
2
√
6
⎞
⎠.
The associated quadratic function can then be written as a sum of pure squares:
q(x) = x2
1 + 4x1 x2 −2x1 x3 + 6x2
2 + 9x2
3 = z2
1 + z2
2 + z2
3,
where
z = M Tx,
or, explicitly,
z1 = x1 + 2x2 −x3,
z2 =
√
2 x2 +
√
2 x3,
z3 =
√
6 x3.
Exercises
3.5.19. Find the Cholesky factorizations of the following matrices:
(a)

3
−2
−2
2
	
,
(b)

4
−12
−12
45
	
,
(c)
⎛
⎜
⎝
1
1
1
1
2
−2
1
−2
14
⎞
⎟
⎠,
(d)
⎛
⎜
⎝
2
1
1
1
2
1
1
1
2
⎞
⎟
⎠,
(e)
⎛
⎜
⎜
⎜
⎝
2
1
0
0
1
2
1
0
0
1
2
1
0
0
1
2
⎞
⎟
⎟
⎟
⎠.
3.5.20. Which of the matrices in Exercise 3.5.1 have a Cholesky factorization? For those that
do, write out the factorization.
3.5.21. Write the following positive deﬁnite quadratic forms as a sum of pure squares, as
in (3.79):
(a) 16x2
1 + 25x2
2,
(b) x2
1 −2x1 x2 + 4x2
2, (c) 5x2
1 + 4x1 x2 + 3x2
2,
(d) 3x2
1 −2x1 x2 −2x1 x3 + 2x2
2 + 6x2
3, (e) x2
1 + x1 x2 + x2
2 + x2 x3 + x2
3,
(f ) 4x2
1 −2x1 x2 −4x1 x3 + 1
2 x2
2 −x2 x3 + 6x2
3,
(g) 3x2
1 + 2x1 x2 + 3x2
2 + 2x2 x3 + 3x2
3 + 2x3 x4 + 3x2
4.
3.6 Complex Vector Spaces
Although physical applications ultimately require real answers, complex numbers and com-
plex vector spaces play an extremely useful, if not essential, role in the intervening analysis.
Particularly in the description of periodic phenomena, complex numbers and complex ex-
ponentials help to simplify complicated trigonometric formulas. Complex variable methods

3.6 Complex Vector Spaces
173
r
θ
z
z
x
y
Figure 3.7.
Complex Numbers.
are ubiquitous in electrical engineering, Fourier analysis, potential theory, ﬂuid mechanics,
electromagnetism, and many other applied ﬁelds, [49, 79]. In quantum mechanics, the ba-
sic physical quantities are complex-valued wave functions, [54]. Moreover, the Schr¨odinger
equation, which governs quantum dynamics, is an inherently complex partial diﬀerential
equation.
In this section, we survey the principal properties of complex numbers and complex
vector spaces.
Most of the constructions are straightforward adaptations of their real
counterparts, and so will not be dwelled on at length. The one exception is the complex
version of an inner product, which does introduce some novelties not found in its simpler
real sibling.
Complex Numbers
Recall that a complex number is an expression of the form z = x + i y, where x, y ∈R are
real and† i = √−1. The set of all complex numbers (scalars) is denoted by C. We call
x = Re z the real part of z and y = Im z the imaginary part of z = x + i y. (Note: The
imaginary part is the real number y, not i y.) A real number x is merely a complex number
with zero imaginary part, Im z = 0, and so we may regard R ⊂C. Complex addition and
multiplication are based on simple adaptations of the rules of real arithmetic to include
the identity i 2 = −1, and so
(x + i y) + (u + i v) = (x + u) + i (y + v),
(x + i y) (u + i v) = (xu −y v) + i (xv + y u).
(3.80)
Complex numbers enjoy all the usual laws of real addition and multiplication, including
commutativity: z w = wz.
We can identify a complex number x + i y with a vector ( x, y )T ∈R2 in the real
plane. For this reason, C is sometimes referred to as the complex plane. Complex addition
(3.80) corresponds to vector addition, but complex multiplication does not have a readily
identiﬁable vector counterpart.
Another useful operation on complex numbers is that of complex conjugation.
Deﬁnition 3.47. The complex conjugate of z = x+ i y is z = x−i y, whereby Re z = Re z,
while Im z = −Im z.
†
To avoid confusion with the symbol for current, electrical engineers prefer to use j to indicate
the imaginary unit.

174
3 Inner Products and Norms
Geometrically, the complex conjugate of z is obtained by reﬂecting the corresponding
vector through the real axis, as illustrated in Figure 3.7. In particular z = z if and only if
z is real. Note that
Re z = z + z
2
,
Im z = z −z
2 i
.
(3.81)
Complex conjugation is compatible with complex arithmetic:
z + w = z + w,
z w = z w.
In particular, the product of a complex number and its conjugate,
z z = (x + i y) (x −i y) = x2 + y2,
(3.82)
is real and non-negative. Its square root is known as the modulus or norm of the complex
number z = x + i y, and written
| z | =

x2 + y2 .
(3.83)
Note that | z | ≥0, with | z | = 0 if and only if z = 0. The modulus | z | generalizes the
absolute value of a real number, and coincides with the standard Euclidean norm in the
xy–plane, which implies the validity of the triangle inequality
| z + w | ≤| z | + | w |.
(3.84)
Equation (3.82) can be rewritten in terms of the modulus as
z z = | z |2.
(3.85)
Rearranging the factors, we deduce the formula for the reciprocal of a nonzero complex
number:
1
z =
z
| z |2 ,
z ̸= 0,
or, equivalently,
1
x + i y = x −i y
x2 + y2 .
(3.86)
The general formula for complex division,
w
z = w z
| z |2
or
u + i v
x + i y = (xu + y v) + i (xv −y u)
x2 + y2
,
(3.87)
is an immediate consequence.
The modulus of a complex number,
r = | z | =

x2 + y2 ,
is one component of its polar coordinate representation
x = r cos θ,
y = r sin θ
or
z = r(cos θ + i sin θ).
(3.88)
The polar angle, which measures the angle that the line connecting z to the origin makes
with the horizontal axis, is known as the phase, and written
θ = ph z.
(3.89)
As such, the phase is deﬁned only up to an integer multiple of 2π. The more common term
for the angle is the argument, written arg z = ph z. However, we prefer to use “phase”
throughout this text, in part to avoid confusion with the argument z of a function f(z).

3.6 Complex Vector Spaces
175
We note that the modulus and phase of a product of complex numbers can be readily
computed:
| z w | = | z | | w |,
ph (z w) = ph z + ph w.
(3.90)
Complex conjugation preserves the modulus, but reverses the sign of the phase:
| z | = | z |,
ph z = −ph z.
(3.91)
One of the most profound formulas in all of mathematics is Euler’s formula
e i θ = cos θ + i sin θ,
(3.92)
relating the complex exponential with the real sine and cosine functions. It has a variety of
mathematical justiﬁcations; see Exercise 3.6.23 for one that is based on comparing power
series. Euler’s formula can be used to compactly rewrite the polar form (3.88) of a complex
number as
z = r e i θ
where
r = | z |,
θ = ph z.
(3.93)
The complex conjugation identity
e−i θ = cos(−θ) + i sin(−θ) = cos θ −i sin θ = e i θ
permits us to express the basic trigonometric functions in terms of complex exponentials:
cos θ = e i θ + e−i θ
2
,
sin θ = e i θ −e−i θ
2 i
.
(3.94)
These formulas are very useful when working with trigonometric identities and integrals.
The exponential of a general complex number is easily derived from the Euler formula
and the standard properties of the exponential function — which carry over unaltered to
the complex domain; thus,
ez = ex+ i y = ex e i y = ex cos y + i ex sin y.
(3.95)
Note that e2π i = 1, and hence the exponential function is periodic,
ez+2π i = ez,
(3.96)
with imaginary period 2π i — indicative of the periodicity of the trigonometric functions
in Euler’s formula.
Exercises
3.6.1. Write down a single equation that relates the ﬁve most important numbers in
mathematics, which are 0, 1, e, π, and i .
3.6.2. For any integer k, prove that ekπ i = (−1)k.
3.6.3. Is the formula 1z = 1 valid for all complex values of z?
3.6.4. What is wrong with the calculation e2aπ i = (e2π i )a = 1a = 1?
3.6.5.(a) Write i in phase–modulus form. (b) Use this expression to ﬁnd
√
i , i.e., a complex
number z such that z2 = i . Can you ﬁnd a second square root?
(c) Find explicit
formulas for the three third roots and four fourth roots of i .
3.6.6. In Figure 3.7, where would you place the point 1/z?

176
3 Inner Products and Norms
3.6.7.(a) If z moves counterclockwise around a circle of radius r in the complex plane, around
which circle and in which direction does w = 1/z move?
(b) What about w = z?
(c) What if the circle is not centered at the origin?
♦3.6.8. Show that −| z | ≤Re z ≤| z | and −| z | ≤Im z ≤| z |.
♦3.6.9. Prove that if ϕ is real, then Re (e i ϕ z) ≤| z |, with equality if and only if ϕ = −ph z.
3.6.10. Prove the identities in (3.90) and (3.91).
3.6.11. Prove ph(z/w) = ph z −ph w = ph(z w) is equal to the angle between the vectors
representing z and w.
3.6.12. The phase of a complex number z = x + i y is often written as ph z = tan−1(y/x).
Explain why this formula is ambiguous, and does not uniquely deﬁne ph z.
3.6.13. Show that if we identify the complex numbers z, w with vectors in the plane, then their
Euclidean dot product is equal to Re (z w).
3.6.14.(a) Prove that the complex numbers z and w correspond to orthogonal vectors in R2 if
and only if Re z w = 0.
(b) Prove that z and i z are always orthogonal.
3.6.15. Prove that ez+w = ez ew. Conclude that emz = (ez)m whenever m is an integer.
3.6.16.(a) Use the formula e2 i θ = (e i θ)2 to deduce the well-known trigonometric identities
for cos 2θ and sin 2θ.
(b) Derive the corresponding identities for cos 3θ and sin 3θ.
(c) Write down the explicit identities for cos mθ and sin mθ as polynomials in cos θ and
sin θ. Hint: Apply the Binomial Formula to (e i θ)m.
♦3.6.17. Use complex exponentials to prove the identity cos θ −cos ϕ = 2 cos θ −ϕ
2
cos θ + ϕ
2
.
3.6.18. Prove that if z = x + i y, then | ez | = ex,
ph ez = y.
3.6.19. The formulas cos z = e i z + e−i z
2
and sin z = e i z −e−i z
2 i
serve to deﬁne the basic
complex trigonometric functions. Write out the formulas for their real and imaginary parts
in terms of z = x + i y, and show that cos z and sin z reduce to their usual real forms when
z = x is real. What do they become when z = i y is purely imaginary?
3.6.20. The complex hyperbolic functions are deﬁned as cosh z = ez + e−z
2
, sinh z = ez −e−z
2
.
(a) Write out the formulas for their real and imaginary parts in terms of z = x + i y.
(b) Prove that cos i z = cosh z and sin i z = i sinh z.
♥3.6.21. Generalizing Example 2.17c, by a trigonometric polynomial of degree ≤n, we mean
a function T(x) =
 
0≤j+k≤n cjk (cos θ)j (sin θ)k in the powers of the sine and cosine
functions up to degree n. (a) Use formula (3.94) to prove that every trigonometric
polynomial of degree ≤n can be written as a complex linear combination of the 2n + 1
complex exponentials e−n i θ, . . . e−i θ, e0 i θ = 1, e i θ, e2 i θ, . . . en i θ. (b) Prove that
every trigonometric polynomial of degree ≤n can be written as a real linear combination of
the trigonometric functions 1, cos θ, sin θ, cos 2θ, sin 2θ,
. . .
cos nθ, sin nθ.
(c) Write out the following trigonometric polynomials in both of the preceding forms:
(i) cos2 θ, (ii) cos θ sin θ, (iii) cos3 θ, (iv) sin4 θ, (v) cos2 θ sin2 θ.
♦3.6.22. Write out the real and imaginary parts of the power function xc with complex exponent
c = a + i b ∈C.
♦3.6.23. Write the power series expansions for e i x. Prove that the real terms give the power
series for cos x, while the imaginary terms give that of sin x. Use this identiﬁcation to
justify Euler’s formula (3.92).

3.6 Complex Vector Spaces
177
♦3.6.24. The derivative of a complex-valued function f(x) = u(x) + i v(x), depending on a
real variable x, is given by f′(x) = u′(x) + i v′(x). (a) Prove that if λ = μ + i ν is any
complex scalar, then d
dx eλx = λ eλx. (b) Prove, conversely,
 b
a eλx dx = 1
λ

eλb −eλa 
provided λ ̸= 0.
3.6.25. Use the complex trigonometric formulas (3.94) and Exercise 3.6.24 to evaluate the
following trigonometric integrals:
(a)

cos2 x dx, (b)

sin2 x dx, (c)

cos x sin x dx,
(d)

cos 3x sin 5x dx. How did you calculate them in ﬁrst-year calculus? If you’re not
convinced this method is easier, try the more complicated integrals
(e)

cos4 x dx, (f )

sin4 x dx, (g)

cos2 x sin2 x dx, (h)

cos 3x sin 5x cos 7x dx.
Complex Vector Spaces and Inner Products
A complex vector space is deﬁned in exactly the same manner as its real counterpart, as
in Deﬁnition 2.1, the only diﬀerence being that we replace real scalars by complex scalars.
The most basic example is the n-dimensional complex vector space Cn consisting of all
column vectors z = ( z1, z2, . . . , zn )T that have n complex entries z1, . . . , zn ∈C. Vector
addition and scalar multiplication are deﬁned in the obvious manner, and veriﬁcation of
each of the vector space axioms is immediate.
We can write any complex vector z = x + i y ∈Cn as a linear combination of two real
vectors x = Re z and y = Im z ∈Rn called its real and imaginary parts. Its complex
conjugate z = x−i y is obtained by taking the complex conjugates of its individual entries.
Thus, for example, if
z =
⎛
⎝
1 + 2 i
−3
5 i
⎞
⎠=
⎛
⎝
1
−3
0
⎞
⎠+ i
⎛
⎝
2
0
5
⎞
⎠,
then
Re z =
⎛
⎝
1
−3
0
⎞
⎠,
Im z =
⎛
⎝
2
0
5
⎞
⎠,
and so its complex conjugate is
z =
⎛
⎝
1 −2 i
−3
−5 i
⎞
⎠=
⎛
⎝
1
−3
0
⎞
⎠−i
⎛
⎝
2
0
5
⎞
⎠.
In particular, z ∈Rn ⊂Cn is a real vector if and only if z = z.
Most of the vector space concepts we developed in the real domain, including span, linear
independence, basis, and dimension, can be straightforwardly extended to the complex
regime.
The one exception is the concept of an inner product, which requires a little
thought. In analysis, the primary applications of inner products and norms rely on the
associated inequalities: Cauchy–Schwarz and triangle. But there is no natural ordering of
the complex numbers, and so one cannot assign a meaning to a complex inequality like
z < w. Inequalities make sense only in the real domain, and so the norm of a complex
vector should still be a positive and real. With this in mind, the na¨ıve idea of simply
summing the squares of the entries of a complex vector will not deﬁne a norm on Cn,
since the result will typically be complex. Moreover, some nonzero complex vectors, e.g.,
( 1, i )T , would then have zero “norm”.
The correct deﬁnition is modeled on the formula
| z | =
√
z z ,
which deﬁnes the modulus of a complex scalar z ∈C. If, in analogy with the real deﬁnition
(3.7), the quantity inside the square root should represent the inner product of z with

178
3 Inner Products and Norms
itself, then we should deﬁne the “dot product” between two complex numbers to be
z · w = z w,
so that
z · z = z z = | z |2.
Writing out the formula when z = x + i y and w = u + i v, we obtain
z · w = z w = (x + i y) (u −i v) = (xu + y v) + i (y u −xv).
(3.97)
Thus, the dot product of two complex numbers is, in general, complex. The real part of
z · w is, in fact, the Euclidean dot product between the corresponding vectors in R2, while
its imaginary part is, interestingly, their scalar cross product, cf. (3.22).
The vector version of this construction is named after the nineteenth-century French
mathematician Charles Hermite, and called the Hermitian dot product on Cn. It has the
explicit formula
z · w = zT w = z1 w1 + z2 w2 + · · · + zn wn,
for
z =
⎛
⎜
⎜
⎜
⎝
z1
z2
...
zn
⎞
⎟
⎟
⎟
⎠,
w =
⎛
⎜
⎜
⎜
⎝
w1
w2
...
wn
⎞
⎟
⎟
⎟
⎠.
(3.98)
Pay attention to the fact that we must apply complex conjugation to all the entries of the
second vector. For example, if
z =

1 + i
3 + 2 i

,
w =

1 + 2 i
i

,
then
z·w = (1+ i )(1−2 i )+(3+2 i )(−i ) = 5−4 i .
On the other hand,
w · z = (1 + 2 i )(1 −i ) + i (3 −2 i ) = 5 + 4 i ,
and we conclude that the Hermitian dot product is not symmetric. Indeed, reversing the
order of the vectors conjugates their dot product:
w · z = z · w.
This is an unexpected complication, but it does have the desired eﬀect that the induced
norm, namely
0 ≤∥z∥= √z · z =
√
zT z =

| z1 |2 + · · · + | zn |2 ,
(3.99)
is strictly positive for all 0 ̸= z ∈Cn. For example, if
z =
⎛
⎝
1 + 3 i
−2 i
−5
⎞
⎠,
then
∥z∥=

| 1 + 3 i |2 + | −2 i |2 + | −5 |2 =
√
39 .
The Hermitian dot product is well behaved under complex vector addition:
(z + z) · w = z · w + z · w,
z · (w + w) = z · w + z · w.
However, while complex scalar multiples can be extracted from the ﬁrst vector without
alteration, when they multiply the second vector, they emerge as complex conjugates:
(c z) · w = c (z · w),
z · (c w) = c (z · w),
c ∈C.
Thus, the Hermitian dot product is not bilinear in the strict sense, but satisﬁes something
that, for lack of a better name, is known as sesquilinearity.

3.6 Complex Vector Spaces
179
The general deﬁnition of an inner product on a complex vector space is modeled on the
preceding properties of the Hermitian dot product.
Deﬁnition 3.48. An inner product on the complex vector space V is a pairing that takes
two vectors v, w ∈V and produces a complex number ⟨v , w ⟩∈C, subject to the following
requirements, for u, v, w ∈V , and c, d ∈C:
(i) Sesquilinearity:
⟨c u + d v , w ⟩= c ⟨u , w ⟩+ d ⟨v , w ⟩,
⟨u , c v + d w ⟩= c ⟨u , v ⟩+ d ⟨u , w ⟩.
(3.100)
(ii) Conjugate Symmetry:
⟨v , w ⟩= ⟨w , v ⟩.
(3.101)
(iii) Positivity:
∥v∥2 = ⟨v , v ⟩≥0,
and
⟨v , v ⟩= 0
if and only if
v = 0.
(3.102)
Thus, when dealing with a complex inner product space, one must pay careful attention
to the complex conjugate that appears when the second argument in the inner product
is multiplied by a complex scalar, as well as the complex conjugate that appears when
the order of the two arguments is reversed. But, once this initial complication has been
properly taken into account, the further properties of the inner product carry over directly
from the real domain. Exercise 3.6.45 contains the formula for a general inner product on
the complex vector space Cn.
Theorem 3.49. The Cauchy–Schwarz inequality,
| ⟨v , w ⟩| ≤∥v∥∥w∥,
(3.103)
with | · | now denoting the complex modulus, and the triangle inequality
∥v + w∥≤∥v∥+ ∥w∥
(3.104)
are both valid on an arbitrary complex inner product space.
The proof of (3.103–104) is modeled on the real case, and the details are left to the
reader.
Example 3.50.
The vectors v = ( 1 + i , 2 i , −3 )T , w = ( 2 −i , 1, 2 + 2 i )T , satisfy
∥v∥=
√
2 + 4 + 9 =
√
15,
∥w∥=
√
5 + 1 + 8 =
√
14,
v · w = (1 + i )(2 + i ) + 2 i + (−3)(2 −2 i ) = −5 + 11 i .
Thus, the Cauchy–Schwarz inequality reads
| ⟨v , w ⟩| = | −5 + 11 i | =
√
146 ≤
√
210 =
√
15
√
14 = ∥v∥∥w∥.
Similarly, the triangle inequality tells us that
∥v + w∥= ∥( 3, 1 + 2 i , −1 + 2 i )T ∥=
√
9 + 5 + 5 =
√
19 ≤
√
15 +
√
14 = ∥v∥+ ∥w∥.
Example 3.51.
Let C0[−π, π ] denote the complex vector space consisting of all
complex-valued continuous functions f(x) = u(x)+ i v(x) depending upon the real variable

180
3 Inner Products and Norms
−π ≤x ≤π. The Hermitian L2 inner product on C0[−π, π ] is deﬁned as
⟨f , g ⟩=
 π
−π
f(x) g(x) dx ,
(3.105)
i.e., the integral of f times the complex conjugate of g, with corresponding norm
∥f ∥=
 π
−π
| f(x) |2 dx =
 π
−π

u(x)2 + v(x)2 
dx .
(3.106)
The reader can verify that (3.105) satisﬁes the Hermitian inner product axioms.
In particular, if k, l are integers, then the inner product of the complex exponential
functions e i kx and e i lx is
⟨e i kx , e i lx ⟩=
 π
−π
e i kxe−i lx dx =
 π
−π
e i (k−l)x dx =
⎧
⎪
⎪
⎨
⎪
⎪
⎩
2π,
k = l,
e i (k−l)x
i (k −l)

π
x=−π
= 0,
k ̸= l.
We conclude that when k ̸= l, the complex exponentials e i kx and e i lx are orthogonal,
since their inner product is zero. The complex formulation of Fourier analysis, [61, 77], is
founded on this key example.
Exercises
3.6.26. Determine whether the indicated sets of complex vectors are linearly independent or
dependent.
(a)

i
1
	
,

1
i
	
,
(b)

1 + i
1
	
,

2
1 −i
	
,
(c)

1 + 3 i
2 −i
	
,

2 −3 i
1 −i
	
,
(d)

−2 + i
i
	
,

4 −3 i
1
	
,

2 i
1 −5 i
	
,
(e)
⎛
⎜
⎝
1 + 2 i
2
0
⎞
⎟
⎠,
⎛
⎜
⎝
2
0
1 −i
⎞
⎟
⎠,
(f )
⎛
⎜
⎝
1
3 i
2 −i
⎞
⎟
⎠,
⎛
⎜
⎝
1 + 2 i
−3
0
⎞
⎟
⎠,
⎛
⎜
⎝
1 −i
−i
1
⎞
⎟
⎠,
(g)
⎛
⎜
⎝
1 + i
2 −i
1
⎞
⎟
⎠,
⎛
⎜
⎝
1 −i
−3 i
1 −2 i
⎞
⎟
⎠,
⎛
⎜
⎝
−1 + i
2 + 3 i
1 + 2 i
⎞
⎟
⎠.
3.6.27. True or false: The set of complex vectors of the form

z
z
	
for z ∈C is a subspace of C2.
3.6.28.(a) Determine whether the vectors v1 =
⎛
⎜
⎝
1
i
0
⎞
⎟
⎠, v2 =
⎛
⎜
⎝
0
1 + i
2
⎞
⎟
⎠, v3 =
⎛
⎜
⎝
−1 + i
1 + i
−1
⎞
⎟
⎠,
are linearly independent or linearly dependent. (b) Do they form a basis of C3?
(c) Compute the Hermitian norm of each vector. (d) Compute the Hermitian dot
products between all diﬀerent pairs. Which vectors are orthogonal?
3.6.29. Find the dimension of and a basis for the following subspaces of C3: (a) The set of all
complex multiples of ( 1, i , 1 −i )T . (b) The plane z1 + i z2 + (1 −i )z3 = 0. (c) The image
of the matrix A =

1
i
2 −i
2 + i
1 + 3 i
−1 −i
	
. (d) The kernel of the same matrix. (e) The
set of vectors that are orthogonal to ( 1 −i , 2 i , 1 + i )T .
3.6.30. Find bases for the four fundamental subspaces associated with the complex matrices
(a)

i
2
−1
2 i
	
,
(b)

2
−1 + i
1 −2 i
−4
3 −i
1 + i
	
,
(c)
⎛
⎜
⎝
i
−1
2 −i
−1 + 2 i
−2 −i
3
i
−1
1 + i
⎞
⎟
⎠.

3.6 Complex Vector Spaces
181
3.6.31. Prove that v = x + i y and v = x −i y are linearly independent complex vectors if and
only if their real and imaginary parts x and y are linearly independent real vectors.
3.6.32. Prove that the space of complex m × n matrices is a complex vector space. What is its
dimension?
3.6.33. Determine which of the following are subspaces of the vector space consisting of all
complex 2 × 2 matrices. (a) All matrices with real diagonals. (b) All matrices for which
the sum of the diagonal entries is zero. (c) All singular complex matrices. (d) All matrices
whose determinant is real. (e) All matrices of the form

a
b
a
b
	
, where a, b ∈C.
3.6.34. True or false: The set of all complex-valued functions u(x) = v(x) + i w(x) with
u(0) = i is a subspace of the vector space of complex-valued functions.
3.6.35. Let V denote the complex vector space spanned by the functions 1, e i x and e−i x,
where x is a real variable. Which of the following functions belong to V ?
(a) sin x,
(b) cos x −2 i sin x,
(c) cosh x,
(d) sin2 1
2 x,
(e) cos2 x?
3.6.36. Prove that the following deﬁne Hermitian inner products on C2:
(a) ⟨v , w ⟩= v1 w1 + 2v2w2,
(b) ⟨v , w ⟩= v1 w1 + i v1 w2 −i v2 w1 + 2v2w2.
3.6.37. Which of the following deﬁne inner products on C2?
(a) ⟨v , w ⟩= v1 w1 + 2 i v2w2,
(b) ⟨v , w ⟩= v1 w1 + 2v2 w2,
(c) ⟨v , w ⟩= v1 w2 + v2w1,
(d) ⟨v , w ⟩=
2v1 w1+v1 w2+v2 w1+2v2w2,
(e) ⟨v , w ⟩= 2v1 w1+(1+ i )v1 w2+(1−i )v2 w1+3v2 w2.
♦3.6.38. Let A = AT be a real symmetric n × n matrix. Show that (Av) · w = v · (Aw) for all
v, w ∈Cn.
3.6.39. Let z = x + i y ∈Cn.
(a) Prove that, for the Hermitian dot product, ∥z∥2 = ∥x∥2 + ∥y∥2.
(b) Does this formula remain valid under a more general Hermitian inner product on Cn?
♦3.6.40. Let V be a complex inner product space. Prove that, for all z, w ∈V ,
(a) ∥z + w∥2 = ∥z∥2 + 2 Re ⟨z , w ⟩+ ∥w∥2;
(b) ⟨z , w ⟩= 1
4

∥z + w∥2 −∥z −w∥2 + i ∥z + i w∥2 −i ∥z −i w∥2 
.
♦3.6.41.(a) How would you deﬁne the angle between two elements of a complex inner product
space? (b) What is the angle between ( −1, 2 −i , −1 + 2 i )T and ( −2 −i , −i , 1 −i )T
relative to the Hermitian dot product?
3.6.42. Let 0 ̸= v ∈Cn. Which scalar multiples c v have the same Hermitian norm as v?
♦3.6.43. Prove the Cauchy–Schwarz inequality (3.103) and the triangle inequality (3.104) for a
general complex inner product. Hint: Use Exercises 3.6.8, 3.6.40(a).
♦3.6.44. The Hermitian adjoint of a complex m × n matrix A is the complex conjugate of its
transpose, written A† = AT = AT .
For example, if A =

1 + i
2 i
−3
2 −5 i
	
, then A† =

1 −i
−3
−2 i
2 + 5 i
	
. Prove that
(a) (A†)† = A, (b) (z A + wB)† = z A† + wB† for z, w ∈C, (c) (AB)† = B†A†.
♦3.6.45. A complex matrix H is called Hermitian if it equals its Hermitian adjoint, H† = H,
as deﬁned in the preceding exercise.
(a) Prove that the diagonal entries of a Hermitian
matrix are real. (b) Prove that (H z) · w = z · (H w) for z, w ∈Cn. (c) Prove that every
Hermitian inner product on Cn has the form ⟨z , w ⟩= zT H w, where H is an n×n positive
deﬁnite Hermitian matrix. (d) How would you verify positive deﬁniteness of a complex
matrix?

182
3 Inner Products and Norms
3.6.46. Multiple choice: Let V be a complex normed vector space. How many unit vectors are
parallel to a given vector 0 ̸= v ∈V ? (a) none; (b) 1; (c) 2; (d) 3; (e) ∞; (f ) depends
upon the vector; (g) depends on the norm. Explain your answer.
♦3.6.47. Let v1, . . . , vn be elements of a complex inner product space. Let K denote the
corresponding n × n Gram matrix, deﬁned in the usual manner.
(a) Prove that K is a Hermitian matrix, as deﬁned in Exercise 3.6.45.
(b) Prove that K is positive semi-deﬁnite, meaning zT K z ≥0 for all z ∈Cn.
(c) Prove that K is positive deﬁnite if and only if v1, . . . , vn are linearly independent.
3.6.48. For each of the following pairs of complex-valued functions,
(i) compute their L2 norm and Hermitian inner product on the interval [0, 1], and then
(ii) check the validity of the Cauchy–Schwarz and triangle inequalities.
(a) 1, e i πx;
(b) x + i , x −i ;
(c) i x2, (1 −2 i )x + 3 i .
3.6.49. Formulate conditions on a weight function w(x) that guarantee that the weighted
integral ⟨f , g ⟩=
 b
a f(x) g(x) w(x) dx deﬁnes an inner product on the space of continuous
complex-valued functions on [a, b].
3.6.50.(a) Formulate a general deﬁnition of a norm on a complex vector space.
(b) How would you deﬁne analogues of the L1, L2 and L∞norms on Cn?

Chapter 4
Orthogonality
Orthogonality is the mathematical formalization of the geometrical property of perpendic-
ularity, as adapted to general inner product spaces. In linear algebra, bases consisting of
mutually orthogonal elements play an essential role in theoretical developments, in a broad
range of applications, and in the design of practical numerical algorithms. Computations
become dramatically simpler and less prone to numerical instabilities when performed in
orthogonal coordinate systems. Indeed, many large-scale modern applications would be
impractical, if not completely infeasible, were it not for the dramatic simplifying power of
orthogonality.
The duly famous Gram–Schmidt process will convert an arbitrary basis of an inner
product space into an orthogonal basis. In Euclidean space, the Gram–Schmidt process can
be reinterpreted as a new kind of matrix factorization, in which a nonsingular matrix A =
QR is written as the product of an orthogonal matrix Q and an upper triangular matrix R.
The QR factorization and its generalizations are used in statistical data analysis as well as
the design of numerical algorithms for computing eigenvalues and eigenvectors. In function
space, the Gram–Schmidt algorithm is employed to construct orthogonal polynomials and
other useful systems of orthogonal functions.
Orthogonality is motivated by geometry, and orthogonal matrices, meaning those whose
columns form an orthonormal system, are of fundamental importance in the mathemat-
ics of symmetry, in image processing, and in computer graphics, animation, and cinema,
[5, 12, 72, 73]. The orthogonal projection of a point onto a subspace turns out to be the
closest point or least squares minimizer, as we discuss in Chapter 5. Yet another important
fact is that the four fundamental subspaces of a matrix that were introduced in Chapter 2
come in mutually orthogonal pairs. This observation leads directly to a new characteri-
zation of the compatibility conditions for linear algebraic systems known as the Fredholm
alternative, whose extensions are used in the analysis of linear boundary value problems,
diﬀerential equations, and integral equations, [16, 61]. The orthogonality of eigenvector
and eigenfunction bases for symmetric matrices and self-adjoint operators provides the key
to understanding the dynamics of discrete and continuous mechanical, thermodynamical,
electrical, and quantum mechanical systems.
One of the most fertile applications of orthogonal bases is in signal processing. Fourier
analysis decomposes a signal into its simple periodic components — sines and cosines
— which form an orthogonal system of functions, [61, 77]. Modern digital media, such as
CD’s, DVD’s and MP3’s, are based on discrete data obtained by sampling a physical signal.
The Discrete Fourier Transform (DFT) uses orthogonality to decompose the sampled signal
vector into a linear combination of sampled trigonometric functions (or, more accurately,
complex exponentials). Basic data compression and noise removal algorithms are applied to
the discrete Fourier coeﬃcients, acting on the observation that noise tends to accumulate
in the high-frequency Fourier modes.
More sophisticated signal and image processing
techniques, including smoothing and compression algorithms, are based on orthogonal
wavelet bases, which are discussed in Section 9.7.
© Springer International Publishing AG, part of Springer Nature 2018 
https://doi.org/10.1007/978-3-319-91041-3_4 
183
P. J. Olver, C. Shakiban, Applied Linear Algebra, Undergraduate Texts in Mathematics, 

184
4 Orthogonality
Figure 4.1.
Orthonormal Bases in R2 and R3.
4.1 Orthogonal and Orthonormal Bases
Let V be a real† inner product space.
Recall that two elements v, w ∈V are called
orthogonal if their inner product vanishes: ⟨v , w ⟩= 0. In the case of vectors in Euclidean
space, orthogonality under the dot product means that they meet at a right angle.
A particularly important conﬁguration arises when V admits a basis consisting of mu-
tually orthogonal elements.
Deﬁnition 4.1. A basis u1, . . . , un of an n-dimensional inner product space V is called
orthogonal if ⟨ui , uj ⟩= 0 for all i ̸= j. The basis is called orthonormal if, in addition,
each vector has unit length: ∥ui∥= 1, for all i = 1, . . ., n.
For the Euclidean space Rn equipped with the standard dot product, the simplest
example of an orthonormal basis is the standard basis
e1 =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎝
1
0
0
...
0
0
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎠
,
e2 =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎝
0
1
0
...
0
0
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎠
,
. . .
en =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎝
0
0
0
...
0
1
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎠
.
Orthogonality follows because ei · ej = 0, for i ̸= j, while ∥ei ∥= 1 implies normality.
Since a basis cannot contain the zero vector, there is an easy way to convert an orthogo-
nal basis to an orthonormal basis. Namely, we replace each basis vector with a unit vector
pointing in the same direction, as in Lemma 3.14.
Lemma 4.2. If v1, . . . , vn is an orthogonal basis of a vector space V , then the normalized
vectors ui = vi/∥vi ∥, i = 1, . . ., n, form an orthonormal basis.
†
The methods can be adapted more or less straightforwardly to the complex realm. The main
complication, as noted in Section 3.6, is that we need to be careful with the order of vectors
appearing in the conjugate symmetric complex inner products. In this chapter, we will be careful
to write the inner product formulas in the proper order so that they retain their validity in complex
vector spaces.

4.1 Orthogonal and Orthonormal Bases
185
Example 4.3.
The vectors
v1 =
⎛
⎝
1
2
−1
⎞
⎠,
v2 =
⎛
⎝
0
1
2
⎞
⎠,
v3 =
⎛
⎝
5
−2
1
⎞
⎠,
are easily seen to form a basis of R3. Moreover, they are mutually perpendicular, v1 ·v2 =
v1 · v3 = v2 · v3 = 0, and so form an orthogonal basis with respect to the standard dot
product on R3. When we divide each orthogonal basis vector by its length, the result is
the orthonormal basis
u1 = 1
√
6
⎛
⎝
1
2
−1
⎞
⎠=
⎛
⎜
⎜
⎝
1
√
6
2
√
6
−1
√
6
⎞
⎟
⎟
⎠,
u2 = 1
√
5
⎛
⎝
0
1
2
⎞
⎠=
⎛
⎜
⎜
⎝
0
1
√
5
2
√
5
⎞
⎟
⎟
⎠,
u3 =
1
√
30
⎛
⎝
5
−2
1
⎞
⎠=
⎛
⎜
⎜
⎝
5
√
30
−
2
√
30
1
√
30
⎞
⎟
⎟
⎠,
satisfying u1 · u2 = u1 · u3 = u2 · u3 = 0 and ∥u1 ∥= ∥u2∥= ∥u3 ∥= 1. The appearance
of square roots in the elements of an orthonormal basis is fairly typical.
A useful observation is that every orthogonal collection of nonzero vectors is automati-
cally linearly independent.
Proposition 4.4. Let v1, . . . , vk ∈V be nonzero, mutually orthogonal elements, so vi ̸= 0
and ⟨vi , vj ⟩= 0 for all i ̸= j. Then v1, . . . , vk are linearly independent.
Proof : Suppose
c1 v1 + · · · + ck vk = 0.
Let us take the inner product of this equation with any vi. Using linearity of the inner
product and orthogonality, we compute
0 = ⟨c1 v1 + · · · + ck vk , vi ⟩= c1 ⟨v1 , vi ⟩+ · · · + ck ⟨vk , vi ⟩= ci ⟨vi , vi ⟩= ci ∥vi ∥2.
Therefore, given that vi ̸= 0, we conclude that ci = 0. Since this holds for all i = 1, . . . , k,
the linear independence of v1, . . . , vk follows.
Q.E.D.
As a direct corollary, we infer that every collection of nonzero orthogonal vectors forms
a basis for its span.
Theorem 4.5. Suppose v1, . . . , vn ∈V are nonzero, mutually orthogonal elements of
an inner product space V .
Then v1, . . . , vn form an orthogonal basis for their span
W = span {v1, . . . , vn} ⊂V , which is therefore a subspace of dimension n = dim W.
In particular, if dim V = n, then v1, . . . , vn form a orthogonal basis for V .
Orthogonality is also of profound signiﬁcance for function spaces. Here is a relatively
simple example.
Example 4.6.
Consider the vector space P(2) consisting of all quadratic polynomials
p(x) = α + β x + γ x2, equipped with the L2 inner product and norm
⟨p , q ⟩=
 1
0
p(x) q(x) dx,
∥p∥=

⟨p , p ⟩=
 1
0
p(x)2 dx .

186
4 Orthogonality
The standard monomials 1, x, x2 do not form an orthogonal basis. Indeed,
⟨1 , x ⟩= 1
2 ,
⟨1 , x2 ⟩= 1
3 ,
⟨x , x2 ⟩= 1
4 .
One orthogonal basis of P(2) is provided by following polynomials:
p1(x) = 1,
p2(x) = x −1
2 ,
p3(x) = x2 −x + 1
6 .
(4.1)
Indeed, one easily veriﬁes that ⟨p1 , p2 ⟩= ⟨p1 , p3 ⟩= ⟨p2 , p3 ⟩= 0, while
∥p1 ∥= 1,
∥p2 ∥=
1
√
12
=
1
2
√
3
,
∥p3 ∥=
1
√
180
=
1
6
√
5
.
The corresponding orthonormal basis is found by dividing each orthogonal basis element
by its norm:
u1(x) = 1,
u2(x) =
√
3 ( 2x −1 ) ,
u3(x) =
√
5

6x2 −6x + 1

.
In Section 4.5 below, we will learn how to systematically construct such orthogonal systems
of polynomials.
Exercises
4.1.1. Let R2 have the standard dot product. Classify the following pairs of vectors as
(i) basis, (ii) orthogonal basis, and/or (iii) orthonormal basis:
(a) v1 =

−1
2
	
, v2 =

2
1
	
; (b) v1 =
⎛
⎜
⎝
1
√
2
1
√
2
⎞
⎟
⎠, v2 =
⎛
⎜
⎝
−
1
√
2
1
√
2
⎞
⎟
⎠; (c) v1 =

−1
−1
	
, v2 =

2
2
	
;
(d) v1 =

2
3
	
, v2 =

1
−6
	
; (e) v1 =

−1
0
	
, v2 =

0
3
	
; (f ) v1 =
⎛
⎝
3
5
4
5
⎞
⎠, v2 =
⎛
⎝−4
5
3
5
⎞
⎠.
4.1.2. Let R3 have the standard dot product. Classify the following sets of vectors as
(i) basis, (ii) orthogonal basis, and/or (iii) orthonormal basis:
(a)
⎛
⎜
⎝
1
1
0
⎞
⎟
⎠,
⎛
⎜
⎝
1
−1
1
⎞
⎟
⎠,
⎛
⎜
⎝
0
1
1
⎞
⎟
⎠; (b)
⎛
⎜
⎜
⎜
⎝
−4
13
3
5
−48
65
⎞
⎟
⎟
⎟
⎠,
⎛
⎜
⎜
⎝
12
13
0
−5
13
⎞
⎟
⎟
⎠,
⎛
⎜
⎜
⎜
⎝
3
13
4
5
36
65
⎞
⎟
⎟
⎟
⎠; (c)
⎛
⎜
⎜
⎜
⎜
⎝
0
1
√
2
−
1
√
2
⎞
⎟
⎟
⎟
⎟
⎠,
⎛
⎜
⎜
⎜
⎜
⎝
−
1
√
2
0
1
√
2
⎞
⎟
⎟
⎟
⎟
⎠,
⎛
⎜
⎜
⎜
⎜
⎝
1
√
2
−
1
√
2
0
⎞
⎟
⎟
⎟
⎟
⎠.
4.1.3. Repeat Exercise 4.1.1, but use the weighted inner product ⟨v , w ⟩= v1 w1 + 1
9 v2 w2
instead of the dot product.
4.1.4. Show that the standard basis vectors e1, e2, e3 form an orthogonal basis with respect to
the weighted inner product ⟨v , w ⟩= v1 w1 + 2v2 w2 + 3v3 w3 on R3. Find an orthonormal
basis for this inner product space.
4.1.5. Find all values of a such that the vectors

a
1
	
,

−a
1
	
form an orthogonal basis of
R2 under (a) the dot product; (b) the weighted inner product ⟨v , w ⟩= 3v1 w1 + 2v2 w2;
(c) the inner product prescribed by the positive deﬁnite matrix K =

2
−1
−1
3
	
.
4.1.6. Find all possible values of a and b in the inner product ⟨v , w ⟩= a v1 w1 + b v2 w2 that
make the vectors ( 1, 2 )T , ( −1, 1 )T , an orthogonal basis in R2.
4.1.7. Answer Exercise 4.1.6 for the vectors (a) ( 2, 3 )T , ( −2, 2 )T ; (b) ( 1, 4 )T , ( 2, 1 )T .

4.1 Orthogonal and Orthonormal Bases
187
4.1.8. Find an inner product such that the vectors ( −1, 2 )T and ( 1, 2 )T form an orthonormal
basis of R2.
4.1.9. True or false: If v1, v2, v3 are a basis for R3, then they form an orthogonal basis under
some appropriately weighted inner product ⟨v , w ⟩= a v1 w1 + b v2 w2 + c v3 w3.
♥4.1.10. The cross product between two vectors in R3 is the vector deﬁned by the formula
v × w =
⎛
⎜
⎝
v2w3 −v3w2
v3w1 −v1w3
v1w2 −v2w1
⎞
⎟
⎠,
where
v =
⎛
⎜
⎝
v1
v2
v3
⎞
⎟
⎠,
w =
⎛
⎜
⎝
w1
w2
w3
⎞
⎟
⎠.
(4.2)
(a) Show that u = v × w is orthogonal, under the dot product, to both v and w.
(b) Show that v × w = 0 if and only if v and w are parallel. (c) Prove that if v, w ∈R3
are orthogonal nonzero vectors, then u = v × w, v, w form an orthogonal basis of R3.
(d) True or false: If v, w ∈R3 are orthogonal unit vectors, then v, w and u = v × w form
an orthonormal basis of R3.
♦4.1.11. Prove that every orthonormal basis of R2 under the standard dot product has the form
u1 =

cos θ
sin θ
	
and u2 = ±

−sin θ
cos θ
	
for some 0 ≤θ < 2π and some choice of ± sign.
♦4.1.12. Given angles θ, ϕ, ψ, prove that the vectors u1 =
⎛
⎜
⎝
cos ψ cos ϕ −cos θ sin ϕ sin ψ
−sin ψ cos ϕ −cos θ sin ϕ cos ψ
sin θ sin ϕ
⎞
⎟
⎠,
u2 =
⎛
⎜
⎝
cos ψ sin ϕ + cos θ cos ϕ sin ψ
−sin ψ sin ϕ + cos θ cos ϕ cos ψ
−sin θ cos ϕ
⎞
⎟
⎠, u3 =
⎛
⎜
⎝
sin θ sin ϕ
sin θ cos ϕ
cos θ
⎞
⎟
⎠, form an orthonormal basis
of R3 under the standard dot product. Remark. It can be proved, [31; p. 147], that every
orthonormal basis of R3 has the form u1, u2, ±u3 for some choice of angles θ, ϕ, ψ.
♥4.1.13.(a) Show that v1, . . . , vn form an orthonormal basis of Rn for the inner product
⟨v , w ⟩= vT K w for K > 0 if and only if AT K A = I , where A = ( v1 v2 . . . vn ).
(b) Prove that every basis of Rn is an orthonormal basis with respect to some inner
product. Is the inner product uniquely determined? (c) Find the inner product on R2 that
makes v1 = ( 1, 1 )T , v2 = ( 2, 3 )T into an orthonormal basis. (d) Find the inner product
on R3 that makes v1 = ( 1, 1, 1 )T , v2 = ( 1, 1, 2 )T , v3 = ( 1, 2, 3 )T an orthonormal basis.
4.1.14. Describe all orthonormal bases of R2 for the inner products
(a) ⟨v , w ⟩= vT

1
0
0
2
	
w;
(b) ⟨v , w ⟩= vT

1
−1
−1
2
	
w.
4.1.15. Let v and w be elements of an inner product space. Prove that
∥v + w∥2 = ∥v∥2 + ∥w∥2 if and only if v, w are orthogonal. Explain why this formula can
be viewed as the generalization of the Pythagorean Theorem.
4.1.16. Prove that if v1, v2 form a basis of an inner product space V and ∥v1 ∥= ∥v2 ∥, then
v1 + v2 and v1 −v2 form an orthogonal basis of V .
4.1.17. Suppose v1, . . . , vk are nonzero mutually orthogonal elements of an inner product space
V . Write down their Gram matrix. Why is it nonsingular?
4.1.18. Let V = P(1) be the vector space consisting of linear polynomials p(t) = at + b.
(a) Carefully explain why ⟨p , q ⟩=
 1
0 t p(t) q(t) dt deﬁnes an inner product on V .
(b) Find all polynomials p(t) = a t + b ∈V that are orthogonal to p1(t) = 1 based on
this inner product.
(c) Use part (b) to construct an orthonormal basis of V for this inner
product. (d) Find an orthonormal basis of the space P(2) of quadratic polynomials for the
same inner product. Hint: First ﬁnd a quadratic polynomial that is orthogonal to the basis
you constructed in part (c).

188
4 Orthogonality
4.1.19. Explain why the functions cos x, sin x form an orthogonal basis for the space of
solutions to the diﬀerential equation y′′ + y = 0 under the L2 inner product on [−π, π ].
4.1.20. Do the functions ex/2, e−x/2 form an orthogonal basis for the space of solutions to the
diﬀerential equation 4y′′ −y = 0 under the L2 inner product on [0, 1]? If not, can you ﬁnd
an orthogonal basis of the solution space?
Computations in Orthogonal Bases
What are the advantages of orthogonal and orthonormal bases? Once one has a basis of
a vector space, a key issue is how to express other elements as linear combinations of the
basis elements — that is, to ﬁnd their coordinates in the prescribed basis. In general, this
is not so easy, since it requires solving a system of linear equations, as described in (2.23).
In high-dimensional situations arising in applications, computing the solution may require
a considerable, if not infeasible, amount of time and eﬀort.
However, if the basis is orthogonal, or, even better, orthonormal, then the change of basis
computation requires almost no work. This is the crucial insight underlying the eﬃcacy of
both discrete and continuous Fourier analysis in signal, image, and video processing, least
squares approximations, the statistical analysis of large data sets, and a multitude of other
applications, both classical and modern.
Theorem 4.7. Let u1, . . . , un be an orthonormal basis for an inner product space V .
Then one can write any element v ∈V as a linear combination
v = c1 u1 + · · · + cn un,
(4.3)
in which its coordinates
ci = ⟨v , ui ⟩,
i = 1, . . . , n,
(4.4)
are explicitly given as inner products. Moreover, its norm is given by the Pythagorean
formula
∥v∥=

c2
1 + · · · + c2
n =




n

i=1
⟨v , ui ⟩2 ,
(4.5)
namely, the square root of the sum of the squares of its orthonormal basis coordinates.
Proof : Let us compute the inner product of the element (4.3) with one of the basis vectors.
Using the orthonormality conditions
⟨ui , uj ⟩=
 0
i ̸= j,
1
i = j,
(4.6)
and bilinearity of the inner product, we obtain
⟨v , ui ⟩=
/
n

j =1
cj uj , ui
0
=
n

j =1
cj ⟨uj , ui ⟩= ci ∥ui ∥2 = ci.
To prove formula (4.5), we similarly expand
∥v∥2 = ⟨v , v ⟩=
/
n

j =1
ci ui ,
n

j =1
cj uj
0
=
n

i,j =1
ci cj ⟨ui , uj ⟩=
n

i=1
c2
i ,
again making use of the orthonormality of the basis elements.
Q.E.D.

4.1 Orthogonal and Orthonormal Bases
189
It is worth emphasizing that the Pythagorean-type formula (4.5) is valid for all inner
products.
Example 4.8.
Let us rewrite the vector v = ( 1, 1, 1 )T in terms of the orthonormal
basis
u1 =
⎛
⎜
⎜
⎝
1
√
6
2
√
6
−1
√
6
⎞
⎟
⎟
⎠,
u2 =
⎛
⎜
⎜
⎝
0
1
√
5
2
√
5
⎞
⎟
⎟
⎠,
u3 =
⎛
⎜
⎜
⎝
5
√
30
−
2
√
30
1
√
30
⎞
⎟
⎟
⎠,
constructed in Example 4.3. Computing the dot products
v · u1 =
2
√
6
,
v · u2 =
3
√
5
,
v · u3 =
4
√
30
,
we immediately conclude that
v =
2
√
6 u1 + 3
√
5 u2 +
4
√
30 u3.
Needless to say, a direct computation based on solving the associated linear system, as in
Chapter 2, is more tedious.
While passage from an orthogonal basis to its orthonormal version is elementary — one
simply divides each basis element by its norm — we shall often ﬁnd it more convenient to
work directly with the unnormalized version. The next result provides the corresponding
formula expressing a vector in terms of an orthogonal, but not necessarily orthonormal
basis. The proof proceeds exactly as in the orthonormal case, and details are left to the
reader.
Theorem 4.9. If v1, . . . , vn form an orthogonal basis, then the corresponding coordinates
of a vector
v = a1 v1 + · · · + an vn
are given by
ai = ⟨v , vi ⟩
∥vi ∥2 .
(4.7)
In this case, its norm can be computed using the formula
∥v∥2 =
n

i=1
a2
i ∥vi ∥2 =
n

i=1
⟨v , vi ⟩
∥vi∥
2
.
(4.8)
Equation (4.7), along with its orthonormal simpliﬁcation (4.4), is one of the most useful
formulas we shall establish, and applications will appear repeatedly throughout this text
and beyond.
Example 4.10.
The wavelet basis
v1 =
⎛
⎜
⎝
1
1
1
1
⎞
⎟
⎠,
v2 =
⎛
⎜
⎝
1
1
−1
−1
⎞
⎟
⎠,
v3 =
⎛
⎜
⎝
1
−1
0
0
⎞
⎟
⎠,
v4 =
⎛
⎜
⎝
0
0
1
−1
⎞
⎟
⎠,
(4.9)
introduced in Example 2.35 is, in fact, an orthogonal basis of R4. The norms are
∥v1 ∥= 2,
∥v2 ∥= 2,
∥v3 ∥=
√
2,
∥v4 ∥=
√
2.

190
4 Orthogonality
Therefore, using (4.7), we can readily express any vector as a linear combination of the
wavelet basis vectors. For example,
v =
⎛
⎜
⎝
4
−2
1
5
⎞
⎟
⎠= 2 v1 −v2 + 3 v3 −2 v4,
where the wavelet coordinates are computed directly by
⟨v , v1 ⟩
∥v1 ∥2 = 8
4 = 2,
⟨v , v2 ⟩
∥v2 ∥2 = −4
4
= −1,
⟨v , v3 ⟩
∥v3 ∥2 = 6
2 = 3,
⟨v , v4 ⟩
∥v4 ∥2 = −4
2
= −2.
This is clearly quicker than solving the linear system, as we did earlier in Example 2.35.
Finally, we note that
46 = ∥v∥2 = 22∥v1 ∥2 + (−1)2∥v2 ∥2 + 32∥v3 ∥2 + (−2)2 ∥v4 ∥2 = 4 · 4 + 1 · 4 + 9 · 2 + 4 · 2,
in conformity with (4.8).
Example 4.11.
The same formulas are equally valid for orthogonal bases in function
spaces. For example, to express a quadratic polynomial
p(x) = c1 p1(x) + c2 p2(x) + c3 p3(x) = c1 + c2

x −1
2

+ c3

x2 −x + 1
6

in terms of the orthogonal basis (4.1), we merely compute the L2 inner product integrals
c1 = ⟨p , p1 ⟩
∥p1 ∥2 =
 1
0
p(x) dx,
c2 = ⟨p , p2 ⟩
∥p2 ∥2 = 12
 1
0
p(x)

x −1
2

dx,
c3 = ⟨p , p3 ⟩
∥p3∥2 = 180
 1
0
p(x)

x2 −x + 1
6

dx.
Thus, for example, the coeﬃcients for p(x) = x2 + x + 1 are
c1 =
 1
0
(x2 + x + 1) dx = 11
6 ,
c2 = 12
 1
0
(x2 + x + 1)

x −1
2

dx = 2,
c3 = 180
 1
0
(x2 + x + 1)

x2 −x + 1
6

dx = 1,
and so
p(x) = x2 + x + 1 = 11
6 + 2

x −1
2

+

x2 −x + 1
6

.
Example 4.12.
Perhaps the most important example of an orthogonal basis is provided
by the basic trigonometric functions. Let T (n) denote the vector space consisting of all
trigonometric polynomials
T(x) =

0≤j+k≤n
ajk (sin x)j (cos x)k
(4.10)
of degree ≤n. The individual monomials (sin x)j (cos x)k span T (n), but, as we saw in
Example 2.20, they do not form a basis, owing to identities stemming from the basic

4.1 Orthogonal and Orthonormal Bases
191
trigonometric formula cos2 x + sin2 x = 1. Exercise 3.6.21 introduced a more convenient
spanning set consisting of the 2n + 1 functions
1,
cos x,
sin x,
cos 2x,
sin 2x,
. . .
cos nx,
sin nx.
(4.11)
Let us prove that these functions form an orthogonal basis of T (n) with respect to the L2
inner product and norm:
⟨f , g ⟩=
 π
−π
f(x) g(x) dx,
∥f ∥2 =
 π
−π
f(x)2 dx.
(4.12)
The elementary integration formulas
 π
−π
cos kx cos lx dx =
⎧
⎨
⎩
0,
k ̸= l,
2π,
k = l = 0,
π,
k = l ̸= 0,
 π
−π
sin kx sin lx dx =
(
0,
k ̸= l,
π,
k = l ̸= 0,
 π
−π
cos kx sin lx dx = 0,
(4.13)
which are valid for all nonnegative integers k, l ≥0, imply the orthogonality relations
⟨cos kx , cos lx ⟩= ⟨sin kx , sin lx ⟩= 0,
k ̸= l,
⟨cos kx , sin lx ⟩= 0,
∥coskx∥= ∥sin kx∥= √π ,
k ̸= 0,
∥1∥=
√
2π .
(4.14)
Theorem 4.5 now assures us that the functions (4.11) form a basis for T (n). One conse-
quence is that dim T (n) = 2n + 1 — a fact that is not so easy to establish directly.
Orthogonality of the trigonometric functions (4.11) means that we can compute the
coeﬃcients a0, . . . , an, b1, . . . , bn of any trigonometric polynomial
p(x) = a0 +
n

k=1

ak cos k x + bk sin k x

(4.15)
by an explicit integration formula. Namely,
a0 = ⟨f , 1 ⟩
∥1∥2 = 1
2π
 π
−π
f(x) dx,
ak = ⟨f , cos kx ⟩
∥coskx∥2 = 1
π
 π
−π
f(x) cos kx dx,
bk = ⟨f , sin kx ⟩
∥sin kx∥2 = 1
π
 π
−π
f(x) sin kx dx,
k ≥1.
(4.16)
These fundamental formulas play an essential role in the theory and applications of Fourier
series, [61, 79, 77].
Exercises
♥4.1.21.(a) Prove that the vectors v1 = ( 1, 1, 1 )T , v2 = ( 1, 1, −2 )T , v3 = ( −1, 1, 0 )T , form
an orthogonal basis of R3 with the dot product. (b) Use orthogonality to write the vector
v = ( 1, 2, 3 )T as a linear combination of v1, v2, v3. (c) Verify the formula (4.8) for ∥v∥.
(d) Construct an orthonormal basis, using the given vectors. (e) Write v as a linear
combination of the orthonormal basis, and verify (4.5).
4.1.22.(a) Prove that v1 =
 3
5, 0, 4
5
T , v2 =

−4
13, 12
13, 3
13
T , v3 =

−48
65, −5
13, 36
65
T ,
form an orthonormal basis for R3 for the usual dot product. (b) Find the coordinates of
v = ( 1, 1, 1 )T relative to this basis. (c) Verify formula (4.5) in this particular case.

192
4 Orthogonality
4.1.23. Let R2 have the inner product deﬁned by the positive deﬁnite matrix K =

2
−1
−1
3
	
.
(a) Show that v1 = ( 1, 1 )T , v2 = ( −2, 1 )T form an orthogonal basis. (b) Write the
vector v = ( 3, 2 )T as a linear combination of v1, v2 using the orthogonality formula (4.7).
(c) Verify the formula (4.8) for ∥v∥. (d) Find an orthonormal basis u1, u2 for this inner
product. (e) Write v as a linear combination of the orthonormal basis, and verify (4.5).
♦4.1.24.(a) Let u1, . . . , un be an orthonormal basis of a ﬁnite-dimensional inner product space V .
Let v = c1u1 + · · · + cnun and w = d1u1 + · · · + dnun be any two elements of V .
Prove that ⟨v , w ⟩= c1d1 + · · · + cndn.
(b) Write down the corresponding inner product formula for an orthogonal basis.
4.1.25. Find an example that demonstrates why equation (4.5) is not valid for a non-
orthonormal basis.
4.1.26. Use orthogonality to write the polynomials 1, x and x2 as linear combinations of the
orthogonal basis (4.1).
4.1.27.(a) Prove that the polynomials P0(t) = 1, P1(t) = t, P2(t) = t2 −1
3 , P3(t) = t3 −3
5 t,
form an orthogonal basis for the vector space P(3) of cubic polynomials for the L2 inner
product ⟨f , g ⟩=
 1
−1 f(t) g(t) dt.
(b) Find an orthonormal basis of P(3). (c) Write t3 as
a linear combination of P0, P1, P2, P3 using the orthogonal basis formula (4.7).
4.1.28.(a) Prove that the polynomials P0(t) = 1, P1(t) = t −2
3, P2(t) = t2 −6
5 t + 3
10, form an
orthogonal basis for P(2) with respect to the weighted inner product
⟨f , g ⟩=
 1
0 f(t) g(t) t dt.
(b) Find the corresponding orthonormal basis.
(c) Write t2 as a linear combination of P0, P1, P2 using the orthogonal basis formula (4.7).
4.1.29. Write the following trigonometric polynomials in terms of the basis functions (4.11):
(a) cos2 x,
(b) cos x sin x,
(c) sin3 x,
(d) cos2 x sin3 x,
(e) cos4 x.
Hint: You can use complex exponentials to simplify the inner product integrals.
4.1.30. Write down an orthonormal basis of the space of trigonometric polynomials T (n) with
respect to the L2 inner product ⟨f , g ⟩=
 π
−π f(x) g(x) dx.
♦4.1.31. Show that the 2n + 1 complex exponentials e i kx for k = −n, −n + 1, . . . , −1, 0, 1, . . . , n,
form an orthonormal basis for the space of complex-valued trigonometric polynomials under
the Hermitian inner product ⟨f , g ⟩= 1
2π
 π
−π f(x) g(x) dx.
♦4.1.32. Prove the trigonometric integral identities (4.13). Hint: You can either use a
trigonometric summation identity, or, if you can’t remember the right one, use Euler’s
formula (3.94) to rewrite sine and cosine as combinations of complex exponentials.
♦4.1.33. Fill in the complete details of the proof of Theorem 4.9.
4.2 The Gram–Schmidt Process
Once we become convinced of the utility of orthogonal and orthonormal bases, a natural
question arises: How can we construct them? A practical algorithm was ﬁrst discovered
by the French mathematician Pierre–Simon Laplace in the eighteenth century. Today the
algorithm is known as the Gram–Schmidt process, after its rediscovery by Gram, whom
we already met in Chapter 3, and the twentieth-century German mathematician Erhard

4.2 The Gram–Schmidt Process
193
Schmidt.
The Gram–Schmidt process is one of the premier algorithms of applied and
computational linear algebra.
Let W denote a ﬁnite-dimensional inner product space. (To begin with, you might wish
to think of W as a subspace of Rm, equipped with the standard Euclidean dot product,
although the algorithm will be formulated in complete generality.) We assume that we
already know some basis w1, . . . , wn of W, where n = dim W. Our goal is to use this
information to construct an orthogonal basis v1, . . . , vn.
We will construct the orthogonal basis elements one by one. Since initially we are not
worrying about normality, there are no conditions on the ﬁrst orthogonal basis element v1,
and so there is no harm in choosing
v1 = w1.
Note that v1 ̸= 0, since w1 appears in the original basis. Starting with w2, the second
basis vector v2 must be orthogonal to the ﬁrst: ⟨v2 , v1 ⟩= 0. Let us try to arrange this
by subtracting a suitable multiple of v1, and set
v2 = w2 −c v1,
where c is a scalar to be determined. The orthogonality condition
0 = ⟨v2 , v1 ⟩= ⟨w2 , v1 ⟩−c ⟨v1 , v1 ⟩= ⟨w2 , v1 ⟩−c ∥v1∥2
requires that c = ⟨w2 , v1 ⟩/∥v1 ∥2 , and therefore
v2 = w2 −⟨w2 , v1 ⟩
∥v1 ∥2
v1.
(4.17)
Linear independence of v1 = w1 and w2 ensures that v2 ̸= 0. (Check!)
Next, we construct
v3 = w3 −c1 v1 −c2 v2
by subtracting suitable multiples of the ﬁrst two orthogonal basis elements from w3. We
want v3 to be orthogonal to both v1 and v2. Since we already arranged that ⟨v1 , v2 ⟩= 0,
this requires
0 = ⟨v3 , v1 ⟩= ⟨w3 , v1 ⟩−c1 ⟨v1 , v1 ⟩,
0 = ⟨v3 , v2 ⟩= ⟨w3 , v2 ⟩−c2 ⟨v2 , v2 ⟩,
and hence
c1 = ⟨w3 , v1 ⟩
∥v1 ∥2
,
c2 = ⟨w3 , v2 ⟩
∥v2 ∥2
.
Therefore, the next orthogonal basis vector is given by the formula
v3 = w3 −⟨w3 , v1 ⟩
∥v1 ∥2
v1 −⟨w3 , v2 ⟩
∥v2 ∥2
v2.
Since v1 and v2 are linear combinations of w1 and w2, we must have v3 ̸= 0, since
otherwise, this would imply that w1, w2, w3 are linearly dependent, and hence could not
come from a basis.
Continuing in the same manner, suppose we have already constructed the mutually or-
thogonal vectors v1, . . . , vk−1 as linear combinations of w1, . . . , wk−1. The next orthogonal
basis element vk will be obtained from wk by subtracting oﬀa suitable linear combination
of the previous orthogonal basis elements:
vk = wk −c1 v1 −· · · −ck−1 vk−1.

194
4 Orthogonality
Since v1, . . . , vk−1 are already orthogonal, the orthogonality constraint
0 = ⟨vk , vj ⟩= ⟨wk , vj ⟩−cj ⟨vj , vj ⟩
requires
cj =
⟨wk , vj ⟩
∥vj ∥2
for
j = 1, . . . , k −1.
(4.18)
In this fashion, we establish the general Gram–Schmidt formula
vk = wk −
k−1

j =1
⟨wk , vj ⟩
∥vj ∥2
vj,
k = 1, . . . , n.
(4.19)
The iterative Gram–Schmidt process (4.19), where we start with v1 = w1 and successively
construct v2, . . . , vn, deﬁnes an explicit, recursive procedure for constructing the desired
orthogonal basis vectors.
If we are actually after an orthonormal basis u1, . . . , un, we
merely normalize the resulting orthogonal basis vectors, setting uk = vk/∥vk ∥for each
k = 1, . . . , n.
Example 4.13.
The vectors
w1 =
⎛
⎝
1
1
−1
⎞
⎠,
w2 =
⎛
⎝
1
0
2
⎞
⎠,
w3 =
⎛
⎝
2
−2
3
⎞
⎠,
(4.20)
are readily seen to form a basis† of R3. To construct an orthogonal basis (with respect to
the standard dot product) using the Gram–Schmidt process, we begin by setting
v1 = w1 =
⎛
⎝
1
1
−1
⎞
⎠.
The next basis vector is
v2 = w2 −w2 · v1
∥v1 ∥2 v1 =
⎛
⎝
1
0
2
⎞
⎠−−1
3
⎛
⎝
1
1
−1
⎞
⎠=
⎛
⎜
⎝
4
3
1
3
5
3
⎞
⎟
⎠.
The last orthogonal basis vector is
v3 = w3 −w3 · v1
∥v1∥2 v1 −w3 · v2
∥v2 ∥2 v2 =
⎛
⎜
⎝
2
−2
3
⎞
⎟
⎠−−3
3
⎛
⎜
⎝
1
1
−1
⎞
⎟
⎠−7
14
3
⎛
⎜
⎝
4
3
1
3
5
3
⎞
⎟
⎠=
⎛
⎜
⎝
1
−3
2
−1
2
⎞
⎟
⎠.
The reader can easily validate the orthogonality of v1, v2, v3.
An orthonormal basis is obtained by dividing each vector by its length. Since
∥v1 ∥=
√
3 ,
∥v2 ∥=

14
3 ,
∥v3 ∥=

7
2 .
†
This will, in fact, be a consequence of the successful completion of the Gram–Schmidt process
and does not need to be checked in advance. If the given vectors were not linearly independent, then
eventually one of the Gram–Schmidt vectors would vanish, vk = 0, and the iterative algorithm
would break down.

4.2 The Gram–Schmidt Process
195
we produce the corresponding orthonormal basis vectors
u1 =
⎛
⎜
⎜
⎝
1
√
3
1
√
3
−1
√
3
⎞
⎟
⎟
⎠,
u2 =
⎛
⎜
⎜
⎝
4
√
42
1
√
42
5
√
42
⎞
⎟
⎟
⎠,
u3 =
⎛
⎜
⎜
⎝
2
√
14
−
3
√
14
−
1
√
14
⎞
⎟
⎟
⎠.
(4.21)
Example 4.14.
Here is a typical problem: ﬁnd an orthonormal basis, with respect to
the dot product, for the subspace W ⊂R4 consisting of all vectors that are orthogonal to
the given vector a = ( 1, 2, −1, −3 )T . The ﬁrst task is to ﬁnd a basis for the subspace.
Now, a vector x = ( x1, x2, x3, x4 )T is orthogonal to a if and only if
x · a = x1 + 2x2 −x3 −3x4 = 0.
Solving this homogeneous linear system by the usual method, we observe that the free
variables are x2, x3, x4, and so a (non-orthogonal) basis for the subspace is
w1 =
⎛
⎜
⎝
−2
1
0
0
⎞
⎟
⎠,
w2 =
⎛
⎜
⎝
1
0
1
0
⎞
⎟
⎠,
w3 =
⎛
⎜
⎝
3
0
0
1
⎞
⎟
⎠.
To obtain an orthogonal basis, we apply the Gram–Schmidt process. First,
v1 = w1 =
⎛
⎜
⎝
−2
1
0
0
⎞
⎟
⎠.
The next element is
v2 = w2 −w2 · v1
∥v1 ∥2 v1 =
⎛
⎜
⎝
1
0
1
0
⎞
⎟
⎠−−2
5
⎛
⎜
⎝
−2
1
0
0
⎞
⎟
⎠=
⎛
⎜
⎜
⎝
1
5
2
5
1
0
⎞
⎟
⎟
⎠.
The last element of our orthogonal basis is
v3 = w3 −w3 · v1
∥v1 ∥2 v1 −w3 · v2
∥v2 ∥2 v2 =
⎛
⎜
⎜
⎜
⎝
3
0
0
1
⎞
⎟
⎟
⎟
⎠−−6
5
⎛
⎜
⎜
⎜
⎝
−2
1
0
0
⎞
⎟
⎟
⎟
⎠−
3
5
6
5
⎛
⎜
⎜
⎜
⎝
1
5
2
5
1
0
⎞
⎟
⎟
⎟
⎠=
⎛
⎜
⎜
⎜
⎝
1
2
1
−1
2
1
⎞
⎟
⎟
⎟
⎠.
An orthonormal basis can then be obtained by dividing each vi by its length:
u1 =
⎛
⎜
⎜
⎜
⎜
⎝
−
2
√
5
1
√
5
0
0
⎞
⎟
⎟
⎟
⎟
⎠
,
u2 =
⎛
⎜
⎜
⎜
⎜
⎝
1
√
30
2
√
30
5
√
30
0
⎞
⎟
⎟
⎟
⎟
⎠
,
u3 =
⎛
⎜
⎜
⎜
⎜
⎜
⎝
1
√
10
2
√
10
−
1
√
10
2
√
10
⎞
⎟
⎟
⎟
⎟
⎟
⎠
.
(4.22)
Remark. The orthonormal basis produced by the Gram–Schmidt process depends on the
order of the vectors in the original basis. Diﬀerent orderings produce diﬀerent orthonormal
bases.

196
4 Orthogonality
The Gram–Schmidt process has one ﬁnal important consequence. According to The-
orem 2.29, every ﬁnite-dimensional vector space — except {0} — admits a basis. Given
an inner product, the Gram–Schmidt process enables one to construct an orthogonal and
even orthonormal basis of the space.
Therefore, we have, in fact, implemented a con-
structive proof of the existence of orthogonal and orthonormal bases of an arbitrary ﬁnite-
dimensional inner product space.
Theorem 4.15. Every non-zero ﬁnite-dimensional inner product space has an ortho-
normal basis.
In fact, if its dimension is > 1, then the inner product space has inﬁnitely many
orthonormal bases.
Exercises
Note: For Exercises #1–7 use the Euclidean dot product on Rn.
4.2.1. Use the Gram–Schmidt process to determine an orthonormal basis for R3 starting with
the following sets of vectors:
(a)
⎛
⎜
⎝
1
0
1
⎞
⎟
⎠,
⎛
⎜
⎝
1
1
1
⎞
⎟
⎠,
⎛
⎜
⎝
−1
2
1
⎞
⎟
⎠;
(b)
⎛
⎜
⎝
1
1
0
⎞
⎟
⎠,
⎛
⎜
⎝
0
1
−1
⎞
⎟
⎠,
⎛
⎜
⎝
1
0
−1
⎞
⎟
⎠;
(c)
⎛
⎜
⎝
1
2
3
⎞
⎟
⎠,
⎛
⎜
⎝
4
5
0
⎞
⎟
⎠,
⎛
⎜
⎝
2
3
−1
⎞
⎟
⎠.
4.2.2. Use the Gram–Schmidt process to construct an orthonormal basis for R4 starting with
the following sets of vectors: (a) ( 1, 0, 1, 0 )T , ( 0, 1, 0, −1 )T , ( 1, 0, 0, 1 )T , ( 1, 1, 1, 1 )T ;
(b) ( 1, 0, 0, 1 )T , ( 4, 1, 0, 0 )T , ( 1, 0, 2, 1 )T , ( 0, 2, 0, 1 )T .
4.2.3. Try the Gram–Schmidt procedure on the vectors
⎛
⎜
⎜
⎜
⎝
1
−1
0
1
⎞
⎟
⎟
⎟
⎠,
⎛
⎜
⎜
⎜
⎝
0
−1
1
2
⎞
⎟
⎟
⎟
⎠,
⎛
⎜
⎜
⎜
⎝
2
−1
−1
0
⎞
⎟
⎟
⎟
⎠,
⎛
⎜
⎜
⎜
⎝
2
2
−2
1
⎞
⎟
⎟
⎟
⎠.
What happens? Can you explain why you are unable to complete the algorithm?
4.2.4. Use the Gram–Schmidt process to construct an orthonormal basis for the following
subspaces of R3:
(a) the plane spanned by ( 0, 2, 1 )T , ( 1, −2, −1 )T ; (b) the plane deﬁned
by the equation 2x −y + 3z = 0; (c) the set of all vectors orthogonal to ( 1, −1, −2 )T .
4.2.5. Find an orthogonal basis of the subspace spanned by the vectors w1 = ( 1, −1, −1, 1, 1 )T ,
w2 = ( 2, 1, 4, −4, 2 )T , and w3 = ( 5, −4, −3, 7, 1 )T .
4.2.6. Find an orthonormal basis for the following subspaces of R4: (a) the span of the vectors
⎛
⎜
⎜
⎜
⎝
1
1
−1
0
⎞
⎟
⎟
⎟
⎠,
⎛
⎜
⎜
⎜
⎝
−1
0
1
1
⎞
⎟
⎟
⎟
⎠,
⎛
⎜
⎜
⎜
⎝
2
−1
2
1
⎞
⎟
⎟
⎟
⎠; (b) the kernel of the matrix

2
1
0
−1
3
2
−1
−1
	
; (c) the coimage
of the preceding matrix; (d) the image of the matrix
⎛
⎜
⎜
⎜
⎝
1
−2
2
2
−4
1
0
0
−1
−2
4
5
⎞
⎟
⎟
⎟
⎠; (e) the cokernel
of the preceding matrix; (f ) the set of all vectors orthogonal to ( 1, 1, −1, −1 )T .
4.2.7. Find orthonormal bases for the four fundamental subspaces associated with the following
matrices:
(a)

1
−1
−3
3
	
,
(b)
⎛
⎜
⎝
−1
0
2
1
1
−1
0
1
1
⎞
⎟
⎠,
(c)
⎛
⎜
⎝
1
0
1
0
1
1
1
1
−1
2
0
1
⎞
⎟
⎠,
(d)
⎛
⎜
⎜
⎜
⎝
1
2
1
0
−2
1
−1
0
−2
1
−2
3
⎞
⎟
⎟
⎟
⎠.

4.2 The Gram–Schmidt Process
197
4.2.8. Construct an orthonormal basis of R2 for the nonstandard inner products
(a) ⟨x , y ⟩= xT

3
0
0
5
	
y, (b) ⟨x , y ⟩= xT

4
−1
−1
1
	
y, (c) ⟨x , y ⟩= xT

2
1
1
3
	
y.
4.2.9. Construct an orthonormal basis for R3 with respect to the inner products deﬁned by the
following positive deﬁnite matrices:
(a)
⎛
⎜
⎝
4
−2
0
−2
3
−1
0
−1
2
⎞
⎟
⎠, (b)
⎛
⎜
⎝
3
−1
1
−1
4
−2
1
−2
4
⎞
⎟
⎠.
4.2.10. Redo Exercise 4.2.1 using
(i) the weighted inner product ⟨v , w ⟩= 3v1 w1 + 2v2 w2 + v3 w3;
(ii) the inner product induced by the positive deﬁnite matrix K =
⎛
⎜
⎝
2
−1
0
−1
2
−1
0
−1
2
⎞
⎟
⎠.
♦4.2.11.(a) How many orthonormal bases does R have? (b) What about R2? (c) Does your
answer change if you use a diﬀerent inner product? Justify your answers.
4.2.12. True or false: Reordering the original basis before starting the Gram–Schmidt process
leads to the same orthogonal basis.
♦4.2.13. Suppose that W ⊊Rn is a proper subspace, and u1, . . . , um forms an orthonormal
basis of W. Prove that there exist vectors um+1, . . . , un ∈Rn \ W such that the complete
collection u1, . . . , un forms an orthonormal basis for Rn. Hint: Begin with Exercise 2.4.20.
♦4.2.14. Verify that the Gram–Schmidt formula (4.19) also produce an orthogonal basis of a
complex vector space under a Hermitian inner product.
4.2.15.(a) Apply the complex Gram–Schmidt algorithm from Exercise 4.2.14 to produce an
orthonormal basis starting with the vectors ( 1 + i , 1 −i )T , ( 1 −2 i , 5 i )T ∈C2.
(b) Do the same for ( 1 + i , 1 −i , 2 −i )T , ( 1 + 2 i , −2 i , 2 −i )T , ( 1, 1 −2 i , i )T ∈C3.
4.2.16. Use the complex Gram–Schmidt algorithm from Exercise 4.2.14 to construct
orthonormal bases for (a) the subspace spanned by ( 1 −i , 1, 0 )T , ( 0, 3 −i , 2 i )T ;
(b) the set of solutions to (2 −i )x −2 i y + (1 −2 i )z = 0;
(c) the subspace spanned by ( −i , 1, −1, i )T , ( 0, 2 i , 1 −i , −1 + i )T , ( 1, i , −i , 1 −2 i )T .
Modiﬁcations of the Gram–Schmidt Process
With the basic Gram–Schmidt algorithm now in hand, it is worth looking at a couple of
reformulations that have both practical and theoretical advantages. The ﬁrst can be used
to construct the orthonormal basis vectors u1, . . . , un directly from the basis w1, . . . , wn.
We begin by replacing each orthogonal basis vector in the basic Gram–Schmidt for-
mula (4.19) by its normalized version uj = vj/∥vj ∥. The original basis vectors can be
expressed in terms of the orthonormal basis via a “triangular” system
w1 = r11 u1,
w2 = r12 u1 + r22 u2,
w3 = r13 u1 + r23 u2 + r33 u3,
...
...
...
...
wn = r1n u1 + r2n u2 + · · · + rnn un.
(4.23)
The coeﬃcients rij can, in fact, be computed directly from these formulas. Indeed, taking
the inner product of the equation for wj with the orthonormal basis vector ui for i ≤j,

198
4 Orthogonality
we obtain, in view of the orthonormality constraints (4.6),
⟨wj , ui ⟩= ⟨r1j u1 + · · · + rjj uj , ui ⟩= r1j ⟨u1 , ui ⟩+ · · · + rjj ⟨un , ui ⟩= rij,
and hence
rij = ⟨wj , ui ⟩.
(4.24)
On the other hand, according to (4.5),
∥wj ∥2 = ∥r1j u1 + · · · + rjj uj ∥2 = r2
1j + · · · + r2
j−1,j + r2
jj.
(4.25)
The pair of equations (4.24–25) can be rearranged to devise a recursive procedure to com-
pute the orthonormal basis. We begin by setting r11 = ∥w1 ∥and so u1 = w1/r11. At
each subsequent stage j ≥2, we assume that we have already constructed u1, . . . , uj−1.
We then compute
rij = ⟨wj , ui ⟩,
for each
i = 1, . . ., j −1.
(4.26)
We obtain the next orthonormal basis vector uj by computing
rjj =

∥wj ∥2 −r2
1j −· · · −r2
j−1,j ,
uj =
wj −r1j u1 −· · · −rj−1,j uj−1
rjj
. (4.27)
Running through the formulas (4.26–27) for j = 1, . . . , n leads to the same orthonormal
basis u1, . . . , un produced by the previous version of the Gram–Schmidt procedure.
Example 4.16.
Let us apply the revised algorithm to the vectors
w1 =
⎛
⎝
1
1
−1
⎞
⎠,
w2 =
⎛
⎝
1
0
2
⎞
⎠,
w3 =
⎛
⎝
2
−2
3
⎞
⎠,
of Example 4.13. To begin, we set
r11 = ∥w1 ∥=
√
3 ,
u1 = w1
r11
=
⎛
⎜
⎜
⎝
1
√
3
1
√
3
−
1
√
3
⎞
⎟
⎟
⎠.
The next step is to compute
r12 = ⟨w2 , u1 ⟩= −1
√
3 , r22 =

∥w2 ∥2 −r2
12 =

14
3 , u2 = w2 −r12u1
r22
=
⎛
⎜
⎜
⎝
4
√
42
1
√
42
5
√
42
⎞
⎟
⎟
⎠.
The ﬁnal step yields
r13 = ⟨w3 , u1 ⟩= −
√
3 ,
r23 = ⟨w3 , u2 ⟩=

21
2 ,
r33 =

∥w3 ∥2 −r2
13 −r2
23 =

7
2 ,
u3 = w3 −r13 u1 −r23 u2
r33
=
⎛
⎜
⎜
⎝
2
√
14
−
3
√
14
−
1
√
14
⎞
⎟
⎟
⎠.
As advertised, the result is the same orthonormal basis vectors that we previously found
in Example 4.13.
For hand computations, the original version (4.19) of the Gram–Schmidt process is
slightly easier — even if one does ultimately want an orthonormal basis — since it avoids

4.2 The Gram–Schmidt Process
199
the square roots that are ubiquitous in the orthonormal version (4.26–27). On the other
hand, for numerical implementation on a computer, the orthonormal version is a bit faster,
since it involves fewer arithmetic operations.
However, in practical, large-scale computations, both versions of the Gram–Schmidt
process suﬀer from a serious ﬂaw. They are subject to numerical instabilities, and so accu-
mulating round-oﬀerrors may seriously corrupt the computations, leading to inaccurate,
non-orthogonal vectors. Fortunately, there is a simple rearrangement of the calculation
that ameliorates this diﬃculty and leads to the numerically robust algorithm that is most
often used in practice, [21, 40, 66]. The idea is to treat the vectors simultaneously rather
than sequentially, making full use of the orthonormal basis vectors as they arise. More
speciﬁcally, the algorithm begins as before — we take u1 = w1/∥w1 ∥. We then subtract
oﬀthe appropriate multiples of u1 from all of the remaining basis vectors so as to arrange
their orthogonality to u1. This is accomplished by setting
w(2)
k
= wk −⟨wk , u1 ⟩u1
for
k = 2, . . ., n.
The second orthonormal basis vector u2 = w(2)
2 /∥w(2)
2 ∥is then obtained by normalizing.
We next modify the remaining w(2)
3 , . . . , w(2)
n
to produce vectors
w(3)
k
= w(2)
k
−⟨w(2)
k
, u2 ⟩u2,
k = 3, . . . , n,
that are orthogonal to both u1 and u2. Then u3 = w(3)
3 /∥w(3)
3 ∥is the next orthonormal
basis element, and the process continues. The full algorithm starts with the initial basis
vectors wj = w(1)
j , j = 1, . . . , n, and then recursively computes
uj =
w(j)
j
∥w(j)
j
∥
,
w(j+1)
k
= w(j)
k
−⟨w(j)
k
, uj ⟩uj,
j = 1, . . ., n,
k = j + 1, . . . , n.
(4.28)
(In the ﬁnal phase, when j = n, the second formula is no longer needed.) The result is a
numerically stable computation of the same orthonormal basis vectors u1, . . . , un.
Example 4.17.
Let us apply the stable Gram–Schmidt process to the basis vectors
w(1)
1
= w1 =
⎛
⎝
2
2
−1
⎞
⎠,
w(1)
2
= w2 =
⎛
⎝
0
4
−1
⎞
⎠,
w(1)
3
= w3 =
⎛
⎝
1
2
−3
⎞
⎠.
The ﬁrst orthonormal basis vector is u1 =
w(1)
1
∥w(1)
1
∥
=
⎛
⎜
⎝
2
3
2
3
−1
3
⎞
⎟
⎠. Next, we compute
w(2)
2
= w(1)
2
−⟨w(1)
2
, u1 ⟩u1 =
⎛
⎝
−2
2
0
⎞
⎠,
w(2)
3
= w(1)
3
−⟨w(1)
3
, u1 ⟩u1 =
⎛
⎝
−1
0
−2
⎞
⎠.
The second orthonormal basis vector is u2 =
w(2)
2
∥w(2)
2 ∥
=
⎛
⎜
⎜
⎝
−
1
√
2
1
√
2
0
⎞
⎟
⎟
⎠. Finally,
w(3)
3
= w(2)
3
−⟨w(2)
3
, u2 ⟩u2 =
⎛
⎜
⎝
−1
2
−1
2
−2
⎞
⎟
⎠,
u3 =
w(3)
3
∥w(3)
3 ∥
=
⎛
⎜
⎝
−
√
2
6
−
√
2
6
−2
√
2
3
⎞
⎟
⎠.
The resulting vectors u1, u2, u3 form the desired orthonormal basis.

200
4 Orthogonality
Exercises
4.2.17. Use the modiﬁed Gram-Schmidt process (4.26–27) to produce orthonormal bases for the
spaces spanned by the following vectors:
(a)
⎛
⎜
⎝
−1
1
2
⎞
⎟
⎠,
⎛
⎜
⎝
−1
−1
1
⎞
⎟
⎠,
⎛
⎜
⎝
0
1
3
⎞
⎟
⎠, (b)
⎛
⎜
⎝
0
1
1
⎞
⎟
⎠,
⎛
⎜
⎝
1
0
1
⎞
⎟
⎠,
⎛
⎜
⎝
2
1
0
⎞
⎟
⎠,
(c)
⎛
⎜
⎜
⎜
⎝
1
1
−1
0
⎞
⎟
⎟
⎟
⎠,
⎛
⎜
⎜
⎜
⎝
−1
0
1
1
⎞
⎟
⎟
⎟
⎠,
⎛
⎜
⎜
⎜
⎝
2
−1
2
1
⎞
⎟
⎟
⎟
⎠, (d)
⎛
⎜
⎜
⎜
⎜
⎜
⎝
2
1
3
1
0
⎞
⎟
⎟
⎟
⎟
⎟
⎠
,
⎛
⎜
⎜
⎜
⎜
⎜
⎝
0
−1
2
−1
1
⎞
⎟
⎟
⎟
⎟
⎟
⎠
,
⎛
⎜
⎜
⎜
⎜
⎜
⎝
1
2
−1
0
1
⎞
⎟
⎟
⎟
⎟
⎟
⎠
, (e)
⎛
⎜
⎜
⎜
⎜
⎜
⎝
0
1
0
1
0
⎞
⎟
⎟
⎟
⎟
⎟
⎠
,
⎛
⎜
⎜
⎜
⎜
⎜
⎝
1
0
1
1
0
⎞
⎟
⎟
⎟
⎟
⎟
⎠
,
⎛
⎜
⎜
⎜
⎜
⎜
⎝
1
−1
0
1
−1
⎞
⎟
⎟
⎟
⎟
⎟
⎠
,
⎛
⎜
⎜
⎜
⎜
⎜
⎝
1
0
−1
0
1
⎞
⎟
⎟
⎟
⎟
⎟
⎠
.
4.2.18. Repeat Exercise 4.2.17 using the numerically stable algorithm (4.28) and check that you
get the same result. Which of the two algorithms was easier for you to implement?
4.2.19. Redo each of the exercises in the preceding subsection by implementing the numerically
stable Gram–Schmidt process (4.28) instead, and verify that you end up with the same
orthonormal basis.
♦4.2.20. Prove that (4.28) does indeed produce an orthonormal basis. Explain why the result is
the same orthonormal basis as the ordinary Gram–Schmidt method.
4.2.21. Let w(j)
j
be the vectors in the stable Gram–Schmidt algorithm (4.28). Prove that the
coeﬃcients in (4.23) are given by rii = ∥w(i)
i
∥, and rij = ⟨w(i)
j
, ui ⟩for i < j.
4.3 Orthogonal Matrices
Matrices whose columns form an orthonormal basis of Rn relative to the standard Euclidean
dot product play a distinguished role. Such “orthogonal matrices” appear in a wide range of
applications in geometry, physics, quantum mechanics, crystallography, partial diﬀerential
equations, [61], symmetry theory, [60], and special functions, [59]. Rotational motions
of bodies in three-dimensional space are described by orthogonal matrices, and hence they
lie at the foundations of rigid body mechanics, [31], including satellites, airplanes, drones,
and underwater vehicles, as well as three-dimensional computer graphics and animation for
video games and movies, [5]. Furthermore, orthogonal matrices are an essential ingredient
in one of the most important methods of numerical linear algebra: the QR algorithm for
computing eigenvalues of matrices, to be presented in Section 9.5.
Deﬁnition 4.18. A square matrix Q is called orthogonal if it satisﬁes
QT Q = QQT = I .
(4.29)
The orthogonality condition implies that one can easily invert an orthogonal matrix:
Q−1 = QT .
(4.30)
In fact, the two conditions are equivalent, and hence a matrix is orthogonal if and only
if its inverse is equal to its transpose. In particular, the identity matrix I is orthogonal.
Also note that if Q is orthogonal, so is QT . The second important characterization of
orthogonal matrices relates them directly to orthonormal bases.

4.3 Orthogonal Matrices
201
Proposition 4.19. A matrix Q is orthogonal if and only if its columns form an orthonor-
mal basis with respect to the Euclidean dot product on Rn.
Proof : Let u1, . . . , un be the columns of Q. Then uT
1 , . . . , uT
n are the rows of the trans-
posed matrix QT . The (i, j) entry of the product QTQ is given as the product of the ith row
of QT and the jth column of Q.
Thus, the orthogonality requirement (4.29) implies
ui · uj = uT
i uj =
 1,
i = j,
0,
i ̸= j,
which are precisely the conditions (4.6) for u1, . . . , un
to form an orthonormal basis.
Q.E.D.
In particular, the columns of the identity matrix produce the standard basis e1, . . . , en
of Rn. Also, the rows of an orthogonal matrix Q also produce an (in general diﬀerent)
orthonormal basis.
Warning. Technically, we should be referring to an “orthonormal” matrix, not an “orthog-
onal” matrix. But the terminology is so standard throughout mathematics and physics that
we have no choice but to adopt it here. There is no commonly accepted name for a matrix
whose columns form an orthogonal but not orthonormal basis.
Example 4.20.
A 2 × 2 matrix Q =

a
b
c
d

is orthogonal if and only if its columns
u1 =

a
c

, u2 =

b
d

, form an orthonormal basis of R2. Equivalently, the requirement
QT Q =

a
c
b
d
 
a
b
c
d

=

a2 + c2
ab + cd
ab + cd
b2 + d2

=

1
0
0
1

,
implies that its entries must satisfy the algebraic equations
a2 + c2 = 1,
ab + cd = 0,
b2 + d2 = 1.
The ﬁrst and last equations say that the points ( a, c )T and ( b, d )T lie on the unit circle
in R2, and so
a = cos θ,
c = sin θ,
b = cos ψ,
d = sin ψ,
for some choice of angles θ, ψ. The remaining orthogonality condition is
0 = ab + cd = cos θ cos ψ + sin θ sin ψ = cos(θ −ψ),
which implies that θ and ψ diﬀer by a right angle: ψ = θ ± 1
2 π. The ± sign leads to two
cases:
b = −sin θ,
d = cos θ,
or
b = sin θ,
d = −cosθ.
As a result, every 2 × 2 orthogonal matrix has one of two possible forms

cos θ
−sin θ
sin θ
cos θ

or

cos θ
sin θ
sin θ
−cosθ

,
where
0 ≤θ < 2π.
(4.31)
The corresponding orthonormal bases are illustrated in Figure 4.2. The former is a right-
handed basis, as deﬁned in Exercise 2.4.7, and can be obtained from the standard basis
e1, e2 by a rotation through angle θ, while the latter has the opposite, reﬂected orientation.

202
4 Orthogonality
u1
u2
θ
u1
u2
θ
Figure 4.2.
Orthonormal Bases in R2.
Example 4.21.
A 3×3 orthogonal matrix Q = ( u1 u2 u3 ) is prescribed by 3 mutually
perpendicular vectors of unit length in R3. For instance, the orthonormal basis constructed
in (4.21) corresponds to the orthogonal matrix Q =
⎛
⎜
⎜
⎝
1
√
3
4
√
42
2
√
14
1
√
3
1
√
42
−
3
√
14
−
1
√
3
5
√
42
−
1
√
14
⎞
⎟
⎟
⎠. A complete
list of 3 × 3 orthogonal matrices can be found in Exercises 4.3.4 and 4.3.5.
Lemma 4.22. An orthogonal matrix Q has determinant det Q = ±1.
Proof : Taking the determinant of (4.29), and using the determinantal formulas (1.85),
(1.89), shows that
1 = det I = det(QTQ) = det QT det Q = (det Q)2,
which immediately proves the lemma.
Q.E.D.
An orthogonal matrix is called proper or special if it has determinant +1. Geometrically,
the columns of a proper orthogonal matrix form a right-handed basis of Rn, as deﬁned in
Exercise 2.4.7. An improper orthogonal matrix, with determinant −1, corresponds to a
left handed basis that lives in a mirror-image world.
Proposition 4.23. The product of two orthogonal matrices is also orthogonal.
Proof : If
QT
1 Q1 = I = QT
2 Q2,
then
(Q1 Q2)T(Q1 Q2) = QT
2 QT
1 Q1 Q2 = QT
2 Q2 = I ,
and so the product matrix Q1 Q2 is also orthogonal.
Q.E.D.
This multiplicative property combined with the fact that the inverse of an orthogonal
matrix is also orthogonal says that the set of all orthogonal matrices forms a group†. The
†
The precise mathematical deﬁnition of a group can be found in Exercise 4.3.24. Although
they will not play a signiﬁcant role in this text, groups underlie the mathematical formalization of
symmetry and, as such, form one of the most fundamental concepts in advanced mathematics and
its applications, particularly quantum mechanics and modern theoretical physics, [54]. Indeed,
according to the mathematician Felix Klein, cf. [92], all geometry is based on group theory.

4.3 Orthogonal Matrices
203
orthogonal group lies at the foundation of everyday Euclidean geometry, as well as rigid
body mechanics, atomic structure and chemistry, computer graphics and animation, and
many other areas.
Exercises
4.3.1.
Determine which of the following matrices are (i) orthogonal; (ii) proper orthogonal.
(a)

1
1
−1
1
	
,
(b)
⎛
⎝
12
13
5
13
−5
13
12
13
⎞
⎠,
(c)
⎛
⎜
⎝
0
1
0
−1
0
0
0
0
−1
⎞
⎟
⎠,
(d)
⎛
⎜
⎜
⎜
⎝
−1
3
2
3
2
3
2
3
−1
3
2
3
2
3
2
3
−1
3
⎞
⎟
⎟
⎟
⎠,
(e)
⎛
⎜
⎜
⎜
⎝
1
2
1
3
1
4
1
3
1
4
1
5
1
4
1
5
1
6
⎞
⎟
⎟
⎟
⎠,
(f )
⎛
⎜
⎜
⎜
⎝
3
5
0
4
5
−4
13
12
13
3
13
−48
65
−5
13
36
65
⎞
⎟
⎟
⎟
⎠,
(g)
⎛
⎜
⎜
⎜
⎝
2
3
−
√
2
6
√
2
2
−2
3
√
2
6
√
2
2
1
3
2
√
2
3
0
⎞
⎟
⎟
⎟
⎠.
4.3.2.(a) Show that R =
⎛
⎜
⎝
1
0
0
0
0
1
0
1
0
⎞
⎟
⎠, a reﬂection matrix, and Q =
⎛
⎜
⎝
cos θ
sin θ
0
−sin θ
cos θ
0
0
0
1
⎞
⎟
⎠,
representing a rotation by the angle θ around the z-axis, are both orthogonal. (b) Verify
that the products RQ and QR are also orthogonal. (c) Which of the preceding matrices,
R, Q, RQ, QR, are proper orthogonal?
4.3.3. True or false: (a) If Q is an improper 2 × 2 orthogonal matrix, then Q2 = I .
(b) If Q is an improper 3 × 3 orthogonal matrix, then Q2 = I .
♥4.3.4.(a) Prove that, for all θ, ϕ, ψ,
Q =
⎛
⎜
⎝
cos ϕ cos ψ −cos θ sin ϕ sin ψ
sin ϕ cos ψ + cos θ cos ϕ sin ψ
sin θ sin ψ
−cos ϕ sin ψ −cos θ sin ϕ cos ψ
−sin ϕ sin ψ + cos θ cos ϕ cos ψ
sin θ cos ψ
sin θ sin ϕ
−sin θ cos ϕ
cos θ
⎞
⎟
⎠
is a proper orthogonal matrix.
(b) Write down a formula for Q−1.
Remark. It can be shown that every proper orthogonal matrix can be parameterized
in this manner; θ, ϕ, ψ are known as the Euler angles, and play an important role in
applications in mechanics and geometry, [31; p. 147].
♥4.3.5.(a) Show that if y2
1 + y2
2 + y2
3 + y2
4 = 1, then the matrix
Q =
⎛
⎜
⎜
⎝
y2
1 + y2
2 −y2
3 −y2
4
2(y2 y3 + y1 y4)
2(y2 y4 −y1 y3)
2(y2 y3 −y1 y4)
y2
1 −y2
2 + y2
3 −y2
4
2(y3 y4 + y1 y2)
2(y2 y4 + y1 y3)
2(y3 y4 −y1 y2)
y2
1 −y2
2 −y2
3 + y2
4
⎞
⎟
⎟
⎠
is a proper orthogonal matrix. The numbers y1, y2, y3, y4 are known as Cayley–Klein
parameters.
(b) Write down a formula for Q−1.
(c) Prove the formulas
y1 = cos ϕ + ψ
2
cos θ
2 , y2 = cos ϕ −ψ
2
sin θ
2 , y3 = sin ϕ −ψ
2
sin θ
2 , y4 = sin ϕ + ψ
2
cos θ
2 ,
relating the Cayley–Klein parameters and the Euler angles of Exercise 4.3.4, cf. [31; §§4–5].
♦4.3.6.(a) Prove that the transpose of an orthogonal matrix is also orthogonal. (b) Explain
why the rows of an n × n orthogonal matrix also form an orthonormal basis of Rn.
4.3.7. Prove that the inverse of an orthogonal matrix is orthogonal.
4.3.8. Show that if Q is a proper orthogonal matrix, and R is obtained from Q by
interchanging two rows, then R is an improper orthogonal matrix.

204
4 Orthogonality
4.3.9. Show that the product of two proper orthogonal matrices is also proper orthogonal.
What can you say about the product of two improper orthogonal matrices? What about an
improper times a proper orthogonal matrix?
4.3.10. True or false: (a) A matrix whose columns form an orthogonal basis of Rn is an
orthogonal matrix. (b) A matrix whose rows form an orthonormal basis of Rn is an
orthogonal matrix. (c) An orthogonal matrix is symmetric if and only if it is a diagonal
matrix.
4.3.11. Write down all diagonal n × n orthogonal matrices.
♦4.3.12. Prove that an upper triangular matrix U is orthogonal if and only if U is a diagonal
matrix. What are its diagonal entries?
4.3.13.(a) Show that the elementary row operation matrix corresponding to the interchange of
two rows is an improper orthogonal matrix. (b) Are there any other orthogonal elementary
matrices?
4.3.14. True or false: Applying an elementary row operation to an orthogonal matrix produces
an orthogonal matrix.
4.3.15.(a) Prove that every permutation matrix is orthogonal.
(b) How many permutation
matrices of a given size are proper orthogonal?
♦4.3.16.(a) Prove that if Q is an orthogonal matrix, then ∥Qx∥= ∥x∥for every vector x ∈Rn,
where ∥·∥denotes the standard Euclidean norm. (b) Prove the converse: if ∥Qx∥= ∥x∥
for all x ∈Rn, then Q is an orthogonal matrix.
♦4.3.17. Show that if AT = −A is any skew-symmetric matrix, then its Cayley Transform
Q = ( I −A)−1( I + A) is an orthogonal matrix. Can you prove that I −A is always
invertible?
4.3.18. Suppose S is an n × n matrix whose columns form an orthogonal, but not orthonormal,
basis of Rn. (a) Find a formula for S−1 mimicking the formula Q−1 = QT for an
orthogonal matrix.
(b) Use your formula to determine the inverse of the wavelet matrix
W whose columns form the orthogonal wavelet basis (4.9) of R4.
♦4.3.19. Let v1, . . . , vn and w1, . . . , wn be two sets of linearly independent vectors in Rn. Show
that all their dot products are the same, so vi · vj = wi · wj for all i, j = 1, . . . , n, if and
only if there is an orthogonal matrix Q such that wi = Qvi for all i = 1, . . . , n.
4.3.20. Suppose u1, . . . , uk form an orthonormal set of vectors in Rn with k < n. Let
Q = ( u1 u2 . . . uk ) denote the n × k matrix whose columns are the orthonormal vectors.
(a) Prove that QT Q = I k. (b) Is QQT = I n?
♦4.3.21. Let u1, . . . , un and u1, . . . , un be orthonormal bases of an inner product space V .
Prove that ui =
n

j =1
qijuj for i = 1, . . . , n, where Q = (qij) is an orthogonal matrix.
4.3.22. Let A be an m × n matrix whose columns are nonzero, mutually orthogonal vectors in
Rm. (a) Explain why m ≥n.
(b) Prove that ATA is a diagonal matrix. What are the
diagonal entries?
(c) Is AAT diagonal?
♦4.3.23. Let K > 0 be a positive deﬁnite n × n matrix. Prove that an n × n matrix S satisﬁes
ST K S = I if and only if the columns of S form an orthonormal basis of Rn with respect to
the inner product ⟨v , w ⟩= vT K w.
♥4.3.24. Groups: A set of n × n matrices G ⊂Mn×n is said to form a group if
(1) whenever A, B ∈G, so is the product AB ∈G, and
(2) whenever A ∈G, then A is nonsingular, and A−1 ∈G.

4.3 Orthogonal Matrices
205
(a) Show that I ∈G. (b) Prove that the following sets of n × n matrices form a group:
(i) all nonsingular matrices; (ii) all nonsingular upper triangular matrices; (iii) all
matrices of determinant 1; (iv) all orthogonal matrices; (v) all proper orthogonal matrices;
(vi) all permutation matrices; (vii) all 2 × 2 matrices with integer entries and determinant
equal to 1. (c) Explain why the set of all nonsingular 2 × 2 matrices with integer entries
does not form a group. (d) Does the set of positive deﬁnite matrices form a group?
♥4.3.25. Unitary matrices: A complex, square matrix U is called unitary if it satisﬁes U† U = I ,
where U† = UT denotes the Hermitian adjoint in which one ﬁrst transposes and then
takes complex conjugates of all entries. (a) Show that U is a unitary matrix if and only if
U−1 = U†. (b) Show that the following matrices are unitary and compute their inverses:
(i)
⎛
⎜
⎝
1
√
2
i
√
2
i
√
2
1
√
2
⎞
⎟
⎠, (ii)
⎛
⎜
⎜
⎜
⎜
⎝
1
√
3
1
√
3
1
√
3
1
√
3
−
1
2
√
3 + i
2
−
1
2
√
3 −i
2
1
√
3
−
1
2
√
3 −i
2
−
1
2
√
3 + i
2
⎞
⎟
⎟
⎟
⎟
⎠, (iii)
⎛
⎜
⎜
⎜
⎜
⎜
⎝
1
2
1
2
1
2
1
2
1
2
i
2
−1
2
−i
2
1
2
−1
2
1
2
−1
2
1
2
−i
2
−1
2
i
2
⎞
⎟
⎟
⎟
⎟
⎟
⎠
.
(c) Are the following matrices unitary?
(i)

2
1 + 2 i
1 −2 i
3
	
,
(ii) 1
5

−1 + 2 i
−4 −2 i
2 −4 i
−2 −i
	
,
(iii)
⎛
⎝
12
13
5
13
5
13
−12
13
⎞
⎠.
(d) Show that U is a unitary matrix if and only if its columns form an orthonormal basis of
Cn with respect to the Hermitian dot product. (e) Prove that the set of unitary matrices
forms a group, as deﬁned in Exercise 4.3.24.
The QR Factorization
The Gram–Schmidt procedure for orthonormalizing bases of Rn can be reinterpreted as
a matrix factorization. This is more subtle than the LU factorization that resulted from
Gaussian Elimination, but is of comparable signiﬁcance, and is used in a broad range of
applications in mathematics, statistics, physics, engineering, and numerical analysis.
Let w1, . . . , wn be a basis of Rn, and let u1, . . . , un be the corresponding orthonormal
basis that results from any one of the three implementations of the Gram–Schmidt process.
We assemble both sets of column vectors to form nonsingular n × n matrices
A = ( w1
w2
. . .
wn ),
Q = ( u1
u2
. . . un ).
Since the ui form an orthonormal basis, Q is an orthogonal matrix. In view of the matrix
multiplication formula (2.13), the Gram–Schmidt equations (4.23) can be recast into an
equivalent matrix form:
A = QR,
where
R =
⎛
⎜
⎜
⎝
r11
r12
. . .
r1n
0
r22
. . .
r2n
...
...
...
...
0
0
. . .
rnn
⎞
⎟
⎟
⎠
(4.32)
is an upper triangular matrix whose entries are the coeﬃcients in (4.26–27). Since the
Gram–Schmidt process works on any basis, the only requirement on the matrix A is that
its columns form a basis of Rn, and hence A can be any nonsingular matrix. We have
therefore established the celebrated QR factorization of nonsingular matrices.
Theorem 4.24. Every nonsingular matrix can be factored, A = QR, into the product of
an orthogonal matrix Q and an upper triangular matrix R. The factorization is unique if
R is positive upper triangular, meaning that all its diagonal entries of are positive.

206
4 Orthogonality
QR Factorization of a Matrix A
start
for j = 1 to n
set rjj =

a2
1j + · · · + a2
nj
if rjj = 0, stop; print “A has linearly dependent columns”
else for i = 1 to n
set aij = aij/rjj
next i
for k = j + 1 to n
set rjk = a1j a1k + · · · + anj ank
for i = 1 to n
set aik = aik −aij rjk
next i
next k
next j
end
The proof of uniqueness is relegated to Exercise 4.3.30.
Example 4.25.
The columns of the matrix A =
⎛
⎝
1
1
2
1
0
−2
−1
2
3
⎞
⎠are the same basis
vectors considered in Example 4.16. The orthonormal basis (4.21) constructed using the
Gram–Schmidt algorithm leads to the orthogonal and upper triangular matrices
Q =
⎛
⎜
⎜
⎝
1
√
3
4
√
42
2
√
14
1
√
3
1
√
42
−
3
√
14
−
1
√
3
5
√
42
−
1
√
14
⎞
⎟
⎟
⎠,
R =
⎛
⎜
⎜
⎝
√
3
−1
√
3
−
√
3
0
√
14
√
3
√
21
√
2
0
0
√
7
√
2
⎞
⎟
⎟
⎠.
(4.33)
The reader may wish to verify that, indeed, A = QR.
While any of the three implementations of the Gram–Schmidt algorithm will produce
the QR factorization of a given matrix A = ( w1 w2 . . . wn ), the stable version, as encoded
in equations (4.28), is the one to use in practical computations, since it is the least likely to
fail due to numerical artifacts produced by round-oﬀerrors. The accompanying pseudocode
program reformulates the algorithm purely in terms of the matrix entries aij of A. During
the course of the algorithm, the entries of the matrix A are successively overwritten; the
ﬁnal result is the orthogonal matrix Q appearing in place of A. The entries rij of R must
be stored separately.
Example 4.26.
Let us factor the matrix A =
⎛
⎜
⎝
2
1
0
0
1
2
1
0
0
1
2
1
0
0
1
2
⎞
⎟
⎠using the numerically
stable QR algorithm. As in the program, we work directly on the matrix A, gradually

4.3 Orthogonal Matrices
207
changing it into orthogonal form.
In the ﬁrst loop, we set r11 =
√
5 to be the norm
of the ﬁrst column vector of A.
We then normalize the ﬁrst column by dividing by
r11; the resulting matrix is
⎛
⎜
⎜
⎜
⎜
⎝
2
√
5
1
0
0
1
√
5
2
1
0
0
1
2
1
0
0
1
2
⎞
⎟
⎟
⎟
⎟
⎠
.
The next entries r12 =
4
√
5, r13 =
1
√
5,
r14 = 0, are obtained by taking the dot products of the ﬁrst column with the other three
columns.
For j = 1, 2, 3, we subtract r1j times the ﬁrst column from the jth column;
the result
⎛
⎜
⎜
⎜
⎜
⎝
2
√
5
−3
5
−2
5
0
1
√
5
6
5
4
5
0
0
1
2
1
0
0
1
2
⎞
⎟
⎟
⎟
⎟
⎠
is a matrix whose ﬁrst column is normalized to have
unit length, and whose second, third and fourth columns are orthogonal to it.
In the
next loop, we normalize the second column by dividing by its norm r22 =

14
5 , and so
obtain the matrix
⎛
⎜
⎜
⎜
⎜
⎝
2
√
5
−
3
√
70
−2
5
0
1
√
5
6
√
70
4
5
0
0
5
√
70
2
1
0
0
1
2
⎞
⎟
⎟
⎟
⎟
⎠
.
We then take dot products of the second
column with the remaining two columns to produce r23 =
16
√
70, r24 =
5
√
70. Subtract-
ing these multiples of the second column from the third and fourth columns, we obtain
⎛
⎜
⎜
⎜
⎜
⎝
2
√
5
−
3
√
70
2
7
3
14
1
√
5
6
√
70
−4
7
−3
7
0
5
√
70
6
7
9
14
0
0
1
2
⎞
⎟
⎟
⎟
⎟
⎠
, which now has its ﬁrst two columns orthonormalized, and or-
thogonal to the last two columns. We then normalize the third column by dividing by
r33 =

15
7 , yielding
⎛
⎜
⎜
⎜
⎜
⎜
⎝
2
√
5
−
3
√
70
2
√
105
3
14
1
√
5
6
√
70
−
4
√
105
−3
7
0
5
√
70
6
√
105
9
14
0
0
7
√
105
2
⎞
⎟
⎟
⎟
⎟
⎟
⎠
. Finally, we subtract r34 =
20
√
105 times
the third column from the fourth column. Dividing the resulting fourth column by its norm
r44 =

5
6 results in the ﬁnal formulas,
Q =
⎛
⎜
⎜
⎜
⎜
⎜
⎝
2
√
5
−
3
√
70
2
√
105
−
1
√
30
1
√
5
6
√
70
−
4
√
105
2
√
30
0
5
√
70
6
√
105
−
3
√
30
0
0
7
√
105
4
√
30
⎞
⎟
⎟
⎟
⎟
⎟
⎠
,
R =
⎛
⎜
⎜
⎜
⎜
⎜
⎝
√
5
4
√
5
1
√
5
0
0
√
14
√
5
16
√
70
5
√
70
0
0
√
15
√
7
20
√
105
0
0
0
√
5
√
6
⎞
⎟
⎟
⎟
⎟
⎟
⎠
,
for the A = QR factorization.

208
4 Orthogonality
Ill-Conditioned Systems and Householder’s Method
The QR factorization can be employed as an alternative to Gaussian Elimination to solve
linear systems. Indeed, the system
A x = b
becomes
QR x = b,
and hence
R x = QT b,
(4.34)
because Q−1 = QT is an orthogonal matrix. Since R is upper triangular, the latter system
can be solved for x by Back Substitution. The resulting algorithm, while more expensive
to compute, oﬀers some numerical advantages over traditional Gaussian Elimination, since
it is less prone to inaccuracies resulting from ill-conditioning.
Example 4.27.
Let us apply the A = QR factorization
⎛
⎝
1
1
2
1
0
−2
−1
2
3
⎞
⎠=
⎛
⎜
⎜
⎝
1
√
3
4
√
42
2
√
14
1
√
3
1
√
42
−
3
√
14
−
1
√
3
5
√
42
−
1
√
14
⎞
⎟
⎟
⎠
⎛
⎜
⎜
⎝
√
3
−1
√
3
−
√
3
0
√
14
√
3
√
21
√
2
0
0
√
7
√
2
⎞
⎟
⎟
⎠
that we found in Example 4.25 to solve the linear system A x = ( 0, −4, 5 )T . We ﬁrst
compute
QT b =
⎛
⎜
⎜
⎝
1
√
3
1
√
3
−1
√
3
4
√
42
1
√
42
5
√
42
2
√
14
−
3
√
14
−
1
√
14
⎞
⎟
⎟
⎠
⎛
⎜
⎝
0
−4
5
⎞
⎟
⎠=
⎛
⎜
⎜
⎝
−3
√
3
√
21
√
2
√
7
√
2
⎞
⎟
⎟
⎠.
We then solve the upper triangular system
R x =
⎛
⎜
⎜
⎝
√
3
−1
√
3
−
√
3
0
√
14
√
3
√
21
√
2
0
0
√
7
√
2
⎞
⎟
⎟
⎠
⎛
⎜
⎝
x
y
z
⎞
⎟
⎠=
⎛
⎜
⎜
⎝
−3
√
3
√
21
√
2
√
7
√
2
⎞
⎟
⎟
⎠
by Back Substitution, leading to the solution x = ( −2, 0, 1 )T .
In computing the QR factorization of a mildly ill-conditioned matrix, one should employ
the stable version (4.28) of the Gram–Schmidt process. However, yet more recalcitrant
matrices require a completely diﬀerent approach to the factorization, as formulated by the
mid-twentieth-century American mathematician Alston Householder. His idea was to use a
sequence of certain simple orthogonal matrices to gradually convert the matrix into upper
triangular form.
Consider the Householder or elementary reﬂection matrix
H = I −2u uT,
(4.35)
in which u is a unit vector (in the Euclidean norm). Geometrically, the matrix H represents
a reﬂection of vectors through the subspace
u⊥= { v | v · u = 0 }
(4.36)
consisting of all vectors orthogonal to u, as illustrated in Figure 4.3. It is a symmetric
orthogonal matrix, and so
HT = H,
H2 = I ,
H−1 = H.
(4.37)

4.3 Orthogonal Matrices
209
v
u
H v
u⊥
Figure 4.3.
Elementary Reﬂection Matrix.
The proof is straightforward: symmetry is immediate, while
HHT = H2 = ( I −2u uT) ( I −2u uT) = I −4u uT + 4u (uTu) uT = I ,
since, by assumption, uTu = ∥u∥2 = 1. Thus, by suitably forming the unit vector u, we
can construct a Householder matrix that interchanges any two vectors of the same length.
Lemma 4.28. Let v, w ∈Rn with ∥v∥= ∥w∥. Set u = (v −w)/∥v −w∥. Let H =
I −2u uT be the corresponding elementary reﬂection matrix. Then H v = w and H w = v.
Proof : Keeping in mind that v and w have the same Euclidean norm, we compute
H v = ( I −2u uT) v = v −2 (v −w)(v −w)T v
∥v −w∥2
= v −2
∥v∥2 −w · v
2 ∥v∥2 −2 v · w (v −w) = v −(v −w) = w.
The proof of the second equation is similar.
Q.E.D.
In the ﬁrst phase of Householder’s method, we introduce the elementary reﬂection matrix
that maps the ﬁrst column v1 of the matrix A to a multiple of the ﬁrst standard basis
vector, namely w1 = ∥v1 ∥e1, noting that ∥v1 ∥= ∥w1 ∥. Assuming v1 ̸= c e1, we deﬁne
the ﬁrst unit vector and corresponding elementary reﬂection matrix as
u1 =
v1 −∥v1 ∥e1
∥v1 −∥v1 ∥e1∥,
H1 = I −2u1uT
1 .
On the other hand, if v1 = c e1 is already in the desired form, then we set u1 = 0 and
H1 = I . Since, by the lemma, H1v1 = w1, when we multiply A on the left by H1, we
obtain a matrix
A2 = H1 A =
⎛
⎜
⎜
⎜
⎜
⎝
r11
a12
a13
. . .
a1n
0
a22
a23
. . .
a2n
0
a32
a33
. . .
a3n
...
...
...
...
...
0
an2
an3
. . .
ann
⎞
⎟
⎟
⎟
⎟
⎠
whose ﬁrst column is in the desired upper triangular form.
In the next phase, we construct a second elementary reﬂection matrix to make all the
entries below the diagonal in the second column of A2 zero, keeping in mind that, at the

210
4 Orthogonality
same time, we should not mess up the ﬁrst column. The latter requirement tells us that
the vector used for the reﬂection should have a zero in its ﬁrst entry. The correct choice is
to set
v2 = ( 0,a22,a32, . . . ,an2 )T ,
u2 =
v2 −∥v2 ∥e2
∥v2 −∥v2 ∥e2 ∥,
H2 = I −2u2 uT
2 .
As before, if v2 = c e2, then u2 = 0 and H2 = I . The net eﬀect is
A3 = H2 A2 =
⎛
⎜
⎜
⎜
⎜
⎝
r11
r12
a13
. . .
a1n
0
r22
a23
. . .
a2n
0
0
a33
. . .
a3n
...
...
...
...
...
0
0
an3
. . .
ann
⎞
⎟
⎟
⎟
⎟
⎠
,
and now the ﬁrst two columns are in upper triangular form.
The process continues; at the kth stage, we are dealing with a matrix Ak whose ﬁrst
k −1 columns coincide with the ﬁrst k columns of the eventual upper triangular matrix R.
Let vk denote the vector obtained from the kth column of Ak by setting its initial k −1
entries equal to 0. We deﬁne the kth Householder vector and corresponding elementary
reﬂection matrix by
wk = vk −∥vk ∥ek,
uk =
(
wk/∥wk ∥,
if
wk ̸= 0,
0,
if
wk = 0,
Hk = I −2uk uT
k ,
Ak+1 = Hk Ak.
(4.38)
The process is completed after n −1 steps, and the ﬁnal result is
R = Hn−1An−1 = Hn−1Hn−2 · · · H1A = QT A,
where
Q = H1H2 · · · Hn−1
is an orthogonal matrix, since it is the product of orthogonal matrices, cf. Proposition 4.23.
In this manner, we have reproduced a† QR factorization of
A = QR = H1H2 · · · Hn−1R.
(4.39)
Example 4.29.
Let us implement Householder’s Method on the particular matrix
A =
⎛
⎝
1
1
2
1
0
−2
−1
2
3
⎞
⎠
considered earlier in Example 4.25. The ﬁrst Householder vector
v1 =
⎛
⎝
1
1
−1
⎞
⎠−
√
3
⎛
⎝
1
0
0
⎞
⎠=
⎛
⎝
−.7321
1
−1
⎞
⎠
leads to the elementary reﬂection matrix
H1 =
⎛
⎝
.5774
.5774
−.5774
.5774
.2113
.7887
−.5774
.7887
.2113
⎞
⎠, whereby A2 = H1A =
⎛
⎝
1.7321
−.5774
−1.7321
0
2.1547
3.0981
0
−.1547
−2.0981
⎞
⎠.
†
The upper triangular matrix R may not have positive diagonal entries; if desired, this can be
easily ﬁxed by changing the signs of the appropriate columns of Q.

4.3 Orthogonal Matrices
211
To construct the second and ﬁnal Householder matrix, we start with the second column of
A2 and then set the ﬁrst entry to 0; the resulting Householder vector is
v2 =
⎛
⎝
0
2.1547
−.1547
⎞
⎠−2.1603
⎛
⎝
0
1
0
⎞
⎠=
⎛
⎝
0
−.0055
−.1547
⎞
⎠.
Therefore,
H2 =
⎛
⎝
1
0
0
0
.9974
−.0716
0
−.0716
−.9974
⎞
⎠,
and so
R = H2A2 =
⎛
⎝
1.7321
−.5774
−1.7321
0
2.1603
3.2404
0
0
1.8708
⎞
⎠
is the upper triangular matrix in the QR decomposition of A. The orthogonal matrix Q
is obtained by multiplying the reﬂection matrices:
Q = H1H2 =
⎛
⎝
.5774
.6172
.5345
.5774
.1543
−.8018
−.5774
.7715
−.2673
⎞
⎠,
which numerically reconﬁrms the previous factorization (4.33).
Remark. If the purpose of the QR factorization is to solve a linear system via (4.34), it
is not necessary to explicitly multiply out the Householder matrices to form Q; we merely
need to store the corresponding unit Householder vectors u1, . . . , un−1. The solution to
A x = QR x = b
can be found by solving
R x = Hn−1Hn−2 · · · H1b
(4.40)
by Back Substitution. This is the method of choice for moderately ill-conditioned systems.
Severe ill-conditioning will defeat even this ingenious approach, and accurately solving such
systems can be an extreme challenge.
Exercises
4.3.26. Write down the QR matrix factorization corresponding to the vectors in Example 4.17.
4.3.27. Find the QR factorization of the following matrices: (a)

1
−3
2
1
	
,
(b)

4
3
3
2
	
,
(c)
⎛
⎜
⎝
2
1
−1
0
1
3
−1
−1
1
⎞
⎟
⎠,
(d)
⎛
⎜
⎝
0
1
2
−1
1
1
−1
1
3
⎞
⎟
⎠,
(e)
⎛
⎜
⎝
0
0
2
0
4
1
−1
0
1
⎞
⎟
⎠,
(f )
⎛
⎜
⎜
⎜
⎝
1
1
1
1
1
2
1
0
1
1
2
1
1
0
1
1
⎞
⎟
⎟
⎟
⎠.
4.3.28. For each of the following linear systems, ﬁnd the QR factorization of the coeﬃcient
matrix, and then use your factorization to solve the system: (i)

1
2
−1
3
	 
x
y
	
=

−1
2
	
,
(ii)
⎛
⎜
⎝
2
1
−1
1
0
2
2
−1
3
⎞
⎟
⎠
⎛
⎜
⎝
x
y
z
⎞
⎟
⎠=
⎛
⎜
⎝
2
−1
0
⎞
⎟
⎠,
(iii)
⎛
⎜
⎝
1
1
0
−1
0
1
0
−1
1
⎞
⎟
⎠
⎛
⎜
⎝
x
y
z
⎞
⎟
⎠=
⎛
⎜
⎝
0
1
0
⎞
⎟
⎠.
♠4.3.29. Use the numerically stable version of the Gram–Schmidt process to ﬁnd the QR
factorizations of the 3 × 3, 4 × 4 and 5 × 5 versions of the tridiagonal matrix that has 4’s
along the diagonal and 1’s on the sub- and super-diagonals, as in Example 1.37.
♦4.3.30. Prove that the QR factorization of a matrix is unique if all the diagonal entries of R
are assumed to be positive. Hint: Use Exercise 4.3.12.

212
4 Orthogonality
♥4.3.31.(a) How many arithmetic operations are required to compute the QR factorization of an
n × n matrix?
(b) How many additional operations are needed to utilize the factorization
to solve a linear system A x = b via (4.34)?
(c) Compare the amount of computational
eﬀort with standard Gaussian Elimination.
♥4.3.32. Suppose A is an m × n matrix with rank A = n. (a) Show that applying the Gram–
Schmidt algorithm to the columns of A produces an orthonormal basis for img A. (b) Prove
that this is equivalent to the matrix factorization A = QR, where Q is an m × n matrix
with orthonormal columns, while R is a nonsingular n × n upper triangular matrix.
(c) Show that the QR program in the text also works for rectangular, m × n, matrices as
stated, the only modiﬁcation being that the row indices i run from 1 to m. (d) Apply this
method to factor
(i)
⎛
⎜
⎝
1
−1
2
3
0
2
⎞
⎟
⎠,
(ii)
⎛
⎜
⎝
−3
2
1
−1
4
1
⎞
⎟
⎠,
(iii)
⎛
⎜
⎜
⎜
⎝
−1
1
1
−2
−1
−3
0
5
⎞
⎟
⎟
⎟
⎠,
(iv)
⎛
⎜
⎜
⎜
⎝
0
1
2
−3
1
−1
−1
0
−2
1
1
−2
⎞
⎟
⎟
⎟
⎠.
(e) Explain what happens if rank A < n.
♥4.3.33.(a) According to Exercise 4.2.14, the Gram–Schmidt process can also be applied to
produce orthonormal bases of complex vector spaces. In the case of Cn, explain how this is
equivalent to the factorization of a nonsingular complex matrix A = U R into the product of
a unitary matrix U (see Exercise 4.3.25) and a nonsingular upper triangular matrix R.
(b) Factor the following complex matrices into unitary times upper triangular:
(i)

i
1
−1
2 i
	
, (ii)

1 + i
2 −i
1 −i
−i
	
, (iii)
⎛
⎜
⎝
i
1
0
1
i
1
0
1
i
⎞
⎟
⎠, (iv)
⎛
⎜
⎝
i
1
−i
1 −i
0
1 + i
−1
2 + 3 i
1
⎞
⎟
⎠.
(c) What can you say about uniqueness of the factorization?
4.3.34.(a) Write down the Householder matrices corresponding to the following unit vectors:
(i) ( 1, 0 )T , (ii)
 3
5, 4
5
T , (iii) ( 0, 1, 0 )T , (iv)
!
1
√
2, 0, −1
√
2
"T
. (b) Find all vectors
ﬁxed by a Householder matrix, i.e., H v = v — ﬁrst for the matrices in part (a), and then
in general.
(c) Is a Householder matrix a proper or improper orthogonal matrix?
4.3.35. Use Householder’s Method to solve Exercises 4.3.27 and 4.3.29.
♣4.3.36. Let Hn = Qn Rn be the QR factorization of the n × n Hilbert matrix (1.72). (a) Find
Qn and Rn for n = 2, 3, 4. (b) Use a computer to ﬁnd Qn and Rn for n = 10 and 20.
(c) Let x⋆∈Rn denote the vector whose ith entry is x⋆
i = (−1)i i/(i + 1). For the values of
n in parts (a) and (b), compute y⋆= Hnx⋆. Then solve the system Hnx = y⋆(i) directly
using Gaussian Elimination; (ii) using the QR factorization based on(4.34); (iii) using
Householder’s Method. Compare the results to the correct solution x⋆and discuss the pros
and cons of each method.
4.3.37. Write out a pseudocode program to implement Householder’s Method. The input
should be an n × n matrix A and the output should be the Householder unit vectors
u1, . . . , un−1 and the upper triangular matrix R. Test your code on one of the examples in
Exercises 4.3.26–28.
4.4 Orthogonal Projections and Orthogonal Subspaces
Orthogonality is important, not just for individual vectors, but also for subspaces. In this
section, we develop two concepts. First, we investigate the orthogonal projection of a vector
onto a subspace, an operation that plays a key role in least squares minimization and data

4.4 Orthogonal Projections and Orthogonal Subspaces
213
z
v
w
W
Figure 4.4.
The Orthogonal Projection of a Vector onto a Subspace.
ﬁtting, as we shall discuss in Chapter 5. Second, we develop the concept of orthogonality
for a pair of subspaces, culminating with a proof of the orthogonality of the fundamental
subspaces associated with an m × n matrix that at last reveals the striking geometry that
underlies linear systems of equations and matrix multiplication.
Orthogonal Projection
Throughout this section, W ⊂V will be a ﬁnite-dimensional subspace of a real inner
product space.
The inner product space V is allowed to be inﬁnite-dimensional.
But,
to facilitate your geometric intuition, you may initially want to view W as a subspace of
Euclidean space V = Rm equipped with the ordinary dot product.
Deﬁnition 4.30. A vector z ∈V is said to be orthogonal to the subspace W ⊂V if it is
orthogonal to every vector in W, so ⟨z , w ⟩= 0 for all w ∈W.
Given a basis w1, . . . , wn of the subspace W, we note that z is orthogonal to W if
and only if it is orthogonal to every basis vector: ⟨z , wi ⟩= 0 for i = 1, . . ., n. Indeed,
any other vector in W has the form w = c1 w1 + · · · + cn wn, and hence, by linearity,
⟨z , w ⟩= c1 ⟨z , w1 ⟩+ · · · + cn ⟨z , wn ⟩= 0, as required.
Deﬁnition 4.31. The orthogonal projection of v onto the subspace W is the element
w ∈W that makes the diﬀerence z = v −w orthogonal to W.
The geometric conﬁguration underlying orthogonal projection is sketched in Figure 4.4.
As we shall see, the orthogonal projection is unique. Note that v = w + z is the sum of
its orthogonal projection w ∈V and the perpendicular vector z ⊥W.
The explicit construction is greatly simpliﬁed by taking an orthonormal basis of the
subspace, which, if necessary, can be arranged by applying the Gram–Schmidt process
to a known basis.
(The direct construction of the orthogonal projection in terms of a
non-orthogonal basis appears in Exercise 4.4.10.)
Theorem 4.32. Let u1, . . . , un be an orthonormal basis for the subspace W ⊂V . Then
the orthogonal projection of v ∈V onto w ∈W is given by
w = c1 u1 + · · · + cn un
where
ci = ⟨v , ui ⟩,
i = 1, . . ., n.
(4.41)

214
4 Orthogonality
Proof : First, since u1, . . . , un form a basis of the subspace, the orthogonal projection
element must be some linear combination thereof: w = c1 u1 + · · · + cnun. Deﬁnition 4.31
requires that the diﬀerence z = v −w be orthogonal to W, and, as noted above, it suﬃces
to check orthogonality to the basis vectors. By our orthonormality assumption,
0 = ⟨z , ui ⟩= ⟨v −w , ui ⟩= ⟨v −c1 u1 −· · · −cn un , ui ⟩
= ⟨v , ui ⟩−c1 ⟨u1 , ui ⟩−· · · −cn ⟨un , ui ⟩= ⟨v , ui ⟩−ci.
The coeﬃcients ci = ⟨v , ui ⟩of the orthogonal projection w are thus uniquely prescribed
by the orthogonality requirement, which thereby proves its uniqueness.
Q.E.D.
More generally, if we employ an orthogonal basis v1, . . . , vn for the subspace W, then
the same argument demonstrates that the orthogonal projection of v onto W is given by
w = a1v1 + · · · + anvn,
where
ai = ⟨v , vi ⟩
∥vi ∥2 ,
i = 1, . . . , n.
(4.42)
We could equally well replace the orthogonal basis by the orthonormal basis obtained by
dividing each vector by its length: ui = vi/∥vi ∥. The reader should be able to prove that
the two formulas (4.41, 42) for the orthogonal projection yield the same vector w.
Example 4.33.
Consider the plane W ⊂R3 spanned by the orthogonal vectors
v1 =
⎛
⎝
1
−2
1
⎞
⎠,
v2 =
⎛
⎝
1
1
1
⎞
⎠.
According to formula (4.42), the orthogonal projection of v = ( 1, 0, 0 )T onto W is
w = ⟨v , v1 ⟩
∥v1 ∥2 v1 + ⟨v , v2 ⟩
∥v2 ∥2 v2 = 1
6
⎛
⎝
1
−2
1
⎞
⎠+ 1
3
⎛
⎝
1
1
1
⎞
⎠=
⎛
⎝
1
2
0
1
2
⎞
⎠.
Alternatively, we can replace v1, v2 by the orthonormal basis
u1 =
v1
∥v1 ∥=
⎛
⎜
⎜
⎝
1
√
6
−2
√
6
1
√
6
⎞
⎟
⎟
⎠,
u2 =
v2
∥v2 ∥=
⎛
⎜
⎜
⎝
1
√
3
1
√
3
1
√
3
⎞
⎟
⎟
⎠.
Then, using the orthonormal version (4.41),
w = ⟨v , u1 ⟩u1 + ⟨v , u2 ⟩u2 =
1
√
6
⎛
⎜
⎜
⎝
1
√
6
−2
√
6
1
√
6
⎞
⎟
⎟
⎠+ 1
√
3
⎛
⎜
⎜
⎝
1
√
3
1
√
3
1
√
3
⎞
⎟
⎟
⎠=
⎛
⎜
⎝
1
2
0
1
2
⎞
⎟
⎠.
The answer is, of course, the same. As the reader may notice, while the theoretical formula
is simpler when written in an orthonormal basis, for hand computations the orthogonal
basis version avoids having to deal with square roots. (Of course, when the numerical
computation is performed on a computer, this is not a signiﬁcant issue.)
An intriguing observation is that the coeﬃcients in the orthogonal projection formulas
(4.41–42) coincide with the formulas (4.4, 7) for writing a vector in terms of an orthonormal

4.4 Orthogonal Projections and Orthogonal Subspaces
215
or orthogonal basis. Indeed, if v were an element of W, then it would coincide with its
orthogonal projection, w = v. (Why?) As a result, the orthogonal projection formula
include the orthogonal basis formula as a special case.
It is also worth noting that the same formulae occur in the Gram–Schmidt algorithm,
cf. (4.19). This observation leads to a useful geometric interpretation of the Gram–Schmidt
construction. For each k = 1, . . . , n, let
Wk = span {w1, . . . , wk} = span {v1, . . . , vk} = span {u1, . . . , uk}
(4.43)
denote the k-dimensional subspace spanned by the ﬁrst k basis elements, which is the
same as that spanned by their orthogonalized and orthonormalized counterparts. In view
of (4.41), the basic Gram–Schmidt formula (4.19) can be re-expressed in the form vk =
wk−pk, where pk is the orthogonal projection of wk onto the subspace Wk−1. The resulting
vector vk is, by construction, orthogonal to the subspace, and hence orthogonal to all of
the previous basis elements, which serves to rejustify the Gram–Schmidt construction.
Exercises
Note: Use the dot product and Euclidean norm unless otherwise speciﬁed.
4.4.1. Determine which of the vectors v1 =
⎛
⎜
⎝
1
1
0
⎞
⎟
⎠, v2 =
⎛
⎜
⎝
−2
2
2
⎞
⎟
⎠, v3 =
⎛
⎜
⎝
2
−1
−3
⎞
⎟
⎠, v4 =
⎛
⎜
⎝
−1
3
4
⎞
⎟
⎠, is
orthogonal to (a) the line spanned by
⎛
⎜
⎝
1
3
−2
⎞
⎟
⎠; (b) the plane spanned by
⎛
⎜
⎝
1
−1
1
⎞
⎟
⎠,
⎛
⎜
⎝
2
1
1
⎞
⎟
⎠;
(c) the plane deﬁned by x −y −z = 0; (d) the kernel of the matrix

1
−1
−1
3
−2
−4
	
;
(e) the image of the matrix
⎛
⎜
⎝
−3
1
3
−1
−1
0
⎞
⎟
⎠; (f ) the cokernel of the matrix
⎛
⎜
⎝
−1
0
3
2
1
−2
3
1
−5
⎞
⎟
⎠.
4.4.2. Find the orthogonal projection of the vector v = ( 1, 1, 1 )T onto the following subspaces,
using the indicated orthonormal/orthogonal bases: (a) the line in the direction
!
−1
√
3,
1
√
3,
1
√
3
"T
; (b) the line spanned by ( 2, −1, 3 )T ; (c) the plane spanned by
( 1, 1, 0 )T , ( −2, 2, 1 )T ; (d) the plane spanned by

−3
5, 4
5, 0
T ,
 4
13, 3
13, −12
13
T .
4.4.3. Find the orthogonal projection of v = ( 1, 2, −1, 2 )T onto the following subspaces:
(a) the span of
⎛
⎜
⎜
⎜
⎝
1
−1
2
1
⎞
⎟
⎟
⎟
⎠,
⎛
⎜
⎜
⎜
⎝
2
1
0
−1
⎞
⎟
⎟
⎟
⎠; (b) the image of the matrix
⎛
⎜
⎜
⎜
⎝
1
2
−1
1
0
3
−1
1
⎞
⎟
⎟
⎟
⎠; (c) the kernel
of the matrix

1
−1
0
1
−2
1
1
0
	
; (d) the subspace orthogonal to a = ( 1, −1, 0, 1 )T .
Warning. Make sure you have an orthogonal basis before applying formula (4.42)!
4.4.4. Find the orthogonal projection of the vector
⎛
⎜
⎝
1
2
3
⎞
⎟
⎠onto the image of
⎛
⎜
⎝
3
2
2
−2
1
−2
⎞
⎟
⎠.
4.4.5. Find the orthogonal projection of the vector v = ( 1, 3, −1 )T onto the plane spanned
by ( −1, 2, 1 )T , ( 2, 1, −3 )T by ﬁrst using the Gram–Schmidt process to construct an
orthogonal basis.

216
4 Orthogonality
4.4.6. Find the orthogonal projection of v = ( 1, 2, −1, 2 )T onto the span of ( 1, −1, 2, 5 )T and
( 2, 1, 0, −1 )T using the weighted inner product ⟨v , w ⟩= 4v1 w1 + 3v2 w2 + 2v3 w3 + v4 w4.
4.4.7. Redo Exercise 4.4.2 using
(i) the weighted inner product ⟨v , w ⟩= 2v1 w1 + 2v2 w2 + v3 w3;
(ii) the inner product induced by the positive deﬁnite matrix K =
⎛
⎜
⎝
2
−1
0
−1
2
−1
0
−1
2
⎞
⎟
⎠.
4.4.8.(a) Prove that the set of all vectors orthogonal to a given subspace V ⊂Rm forms
a subspace. (b) Find a basis for the set of all vectors in R4 that are orthogonal to the
subspace spanned by ( 1, 2, 0, −1 )T , ( 2, 0, 3, 1 )T .
♥4.4.9. Let u1, . . . , uk be an orthonormal basis for the subspace W ⊂Rm. Let
A = ( u1 u2 . . . uk ) be the m × k matrix whose columns are the orthonormal basis vectors,
and deﬁne P = AAT to be the corresponding projection matrix. (a) Given v ∈Rn, prove
that its orthogonal projection w ∈W is given by matrix multiplication: w = P v.
(b) Prove that P = P T is symmetric. (c) Prove that P is idempotent: P 2 = P. Give
a geometrical explanation of this fact. (d) Prove that rank P = k. (e) Write out the
projection matrix corresponding to the subspaces spanned by
(i)
⎛
⎜
⎝
1
√
2
1
√
2
⎞
⎟
⎠, (ii)
⎛
⎜
⎜
⎜
⎝
2
3
−2
3
1
3
⎞
⎟
⎟
⎟
⎠, (iii)
⎛
⎜
⎜
⎜
⎜
⎝
1
√
6
−2
√
6
1
√
6
⎞
⎟
⎟
⎟
⎟
⎠,
⎛
⎜
⎜
⎜
⎜
⎝
1
√
3
1
√
3
1
√
3
⎞
⎟
⎟
⎟
⎟
⎠, (iv)
⎛
⎜
⎜
⎜
⎜
⎜
⎝
1
2
1
2
1
2
−1
2
⎞
⎟
⎟
⎟
⎟
⎟
⎠
,
⎛
⎜
⎜
⎜
⎜
⎜
⎝
1
2
−1
2
1
2
1
2
⎞
⎟
⎟
⎟
⎟
⎟
⎠
,
⎛
⎜
⎜
⎜
⎜
⎜
⎝
1
2
1
2
−1
2
1
2
⎞
⎟
⎟
⎟
⎟
⎟
⎠
.
♥4.4.10. Let w1, . . . , wn be an arbitrary basis of the subspace W ⊂Rm. Let A = (w1, . . . , wn)
be the m × n matrix whose columns are the basis vectors, so that W = img A and
rank A = n. (a) Prove that the corresponding projection matrix P = A(ATA)−1AT
is idempotent: P 2 = P. (b) Prove that P is symmetric. (c) Prove that img P = W.
(d) (e) Prove that the orthogonal projection of v ∈Rn onto w ∈W is obtained by
multiplying by the projection matrix: w = P v. (f ) Show that if A is nonsingular, then
P = I . How do you interpret this in light of part (e)? (g) Explain why Exercise 4.4.9 is
a special case of this result. (h) Show that if A = QR is the factorization of A given in
Exercise 4.3.32, then P = QQT . Why is P ̸= I ?
4.4.11. Use the projection matrix method of Exercise 4.4.10 to ﬁnd the orthogonal projection
of v = ( 1, 0, 0, 0 )T onto the image of the following matrices:
(a)
⎛
⎜
⎜
⎜
⎝
5
−5
−7
1
⎞
⎟
⎟
⎟
⎠,
(b)
⎛
⎜
⎜
⎜
⎝
1
0
−1
2
0
−1
1
2
⎞
⎟
⎟
⎟
⎠,
(c)
⎛
⎜
⎜
⎜
⎝
2
−1
−3
1
1
−2
1
2
⎞
⎟
⎟
⎟
⎠,
(d)
⎛
⎜
⎜
⎜
⎝
0
1
−1
0
−1
2
1
1
1
−2
−1
0
⎞
⎟
⎟
⎟
⎠.
Orthogonal Subspaces
We now extend the notion of orthogonality from individual elements to entire subspaces
of an inner product space V .
Deﬁnition 4.34. Two subspaces W, Z ⊂V are called orthogonal if every vector in W is
orthogonal to every vector in Z.
In other words, W and Z are orthogonal subspaces if and only if ⟨w , z ⟩= 0 for every
w ∈W and z ∈Z. In practice, one only needs to check orthogonality of basis elements,

4.4 Orthogonal Projections and Orthogonal Subspaces
217
Figure 4.5.
Orthogonal Complement to a Line.
or, more generally, spanning sets.
Lemma 4.35. If w1, . . . , wk span W and z1, . . . , zl span Z, then W and Z are orthogonal
subspaces if and only if ⟨wi , zj ⟩= 0 for all i = 1, . . . , k and j = 1, . . . , l.
The proof of this lemma is left to the reader; see Exercise 4.4.26.
Example 4.36.
Let V = R3 have the ordinary dot product. Then the plane W ⊂R3
deﬁned by the equation 2x −y + 3z = 0 is orthogonal to the line Z spanned by its normal
vector n = ( 2, −1, 3 )T . Indeed, every w = ( x, y, z )T ∈W satisﬁes the orthogonality
condition w · n = 2x −y + 3z = 0, which is simply the equation for the plane.
Example 4.37.
Let W be the span of w1 = ( 1, −2, 0, 1 )T , w2 = ( 3, −5, 2, 1 )T , and
let Z be the span of the vectors z1 = ( 3, 2, 0, 1 )T , z2 = ( 1, 0, −1, −1 )T .
We ﬁnd that
w1 · z1 = w1 · z2 = w2 · z1 = w2 · z2 = 0, and so W and Z are orthogonal two-dimensional
subspaces of R4 under the Euclidean dot product.
Deﬁnition 4.38. The orthogonal complement of a subspace W ⊂V , denoted† W ⊥, is
deﬁned as the set of all vectors that are orthogonal to W:
W ⊥= { v ∈V | ⟨v , w ⟩= 0 for all w ∈W } .
(4.44)
If W is the one-dimensional subspace (line) spanned by a single vector w ̸= 0 then we
also denote W ⊥by w⊥, as in (4.36). One easily checks that the orthogonal complement
W ⊥is also a subspace.
Moreover, W ∩W ⊥= {0}.
(Why?)
Keep in mind that the
orthogonal complement will depend upon which inner product is being used.
Example 4.39.
Let W = { ( t, 2t, 3t )T | t ∈R } be the line (one-dimensional subspace)
in the direction of the vector w1 = ( 1, 2, 3 )T ∈R3. Under the dot product, its orthogonal
†
And usually pronounced “W perp”

218
4 Orthogonality
z
v
w
W
W ⊥
Figure 4.6.
Orthogonal Decomposition of a Vector.
complement W ⊥= w⊥
1 is the plane passing through the origin having normal vector w1,
as sketched in Figure 4.5. In other words, z = ( x, y, z )T ∈W ⊥if and only if
z · w1 = x + 2y + 3z = 0.
(4.45)
Thus, W ⊥is characterized as the solution space of the homogeneous linear equation (4.45),
or, equivalently, the kernel of the 1 × 3 matrix A = wT
1 = ( 1 2
3 ). We can write the
general solution in the form
z =
⎛
⎝
−2y −3z
y
z
⎞
⎠= y
⎛
⎝
−2
1
0
⎞
⎠+ z
⎛
⎝
−3
0
1
⎞
⎠= y z1 + z z2,
where y, z are the free variables. The indicated vectors z1 = ( −2, 1, 0 )T , z2 = ( −3, 0, 1 )T ,
form a (non-orthogonal) basis for the orthogonal complement W ⊥.
Proposition 4.40. Suppose that W ⊂V is a ﬁnite-dimensional subspace of an inner
product space. Then every vector v ∈V can be uniquely decomposed into v = w + z,
where w ∈W and z ∈W ⊥.
Proof : We let w ∈W be the orthogonal projection of v onto W. Then z = v −w is,
by deﬁnition, orthogonal to W and hence belongs to W ⊥. Note that z can be viewed
as the orthogonal projection of v onto the complementary subspace W ⊥(provided it is
ﬁnite-dimensional). If we are given two such decompositions, v = w + z = w + z, then
w −w = z −z. The left-hand side of this equation lies in W, while the right-hand side
belongs to W ⊥. But, as we already noted, the only vector that belongs to both W and
W ⊥is the zero vector. Thus, w −w = 0 = z −z, so w = w and z = z, which proves
uniqueness.
Q.E.D.
As a direct consequence of Exercise 2.4.26, in a ﬁnite-dimensional inner product space,
a subspace and its orthogonal complement have complementary dimensions:
Proposition 4.41. If W ⊂V is a subspace with dim W = n and dim V = m, then
dim W ⊥= m −n.
Example 4.42.
Return to the situation described in Example 4.39. Let us decompose
the vector v = ( 1, 0, 0 )T ∈R3 into a sum v = w + z of a vector w lying on the line W

4.4 Orthogonal Projections and Orthogonal Subspaces
219
and a vector z belonging to its orthogonal plane W ⊥, deﬁned by (4.45). Each is obtained
by an orthogonal projection onto the subspace in question, but we only need to compute
one of the two directly, since the second can be obtained by subtracting the ﬁrst from v.
Orthogonal projection onto a one-dimensional subspace is easy, since every basis is,
trivially, an orthogonal basis. Thus, the projection of v onto the line spanned by
w1 = ( 1, 2, 3 )T
is
w = ⟨v , w1 ⟩
∥w1 ∥2 w1 =
 1
14, 2
14, 3
14
T .
The component in W ⊥is then obtained by subtraction:
z = v −w =
 13
14, −2
14, −3
14
T .
Alternatively, one can obtain z directly by orthogonal projection onto the plane W ⊥. But
you need to be careful: the basis found in Example 4.39 is not orthogonal, and so you will
need to either ﬁrst convert to an orthogonal basis and then use the orthogonal projection
formula (4.42), or apply the more direct result in Exercise 4.4.10.
Example 4.43.
Let W ⊂R4 be the two-dimensional subspace spanned by the orthog-
onal vectors w1 = ( 1, 1, 0, 1 )T and w2 = ( 1, 1, 1, −2 )T . Its orthogonal complement W ⊥
(with respect to the Euclidean dot product) is the set of all vectors v = ( x, y, z, w )T that
satisfy the linear system
v · w1 = x + y + w = 0,
v · w2 = x + y + z −2w = 0.
Applying the usual algorithm — the free variables are y and w — we ﬁnd that the solution
space is spanned by
z1 = ( −1, 1, 0, 0 )T ,
z2 = ( −1, 0, 3, 1 )T ,
which form a non-orthogonal basis for W ⊥. An orthogonal basis
y1 = z1 = ( −1, 1, 0, 0 )T ,
y2 = z2 −1
2 z1 =

−1
2, −1
2, 3, 1
T ,
for W ⊥is obtained by a single Gram–Schmidt step.
To decompose the vector v =
( 1, 0, 0, 0 )T = w + z, say, we compute the two orthogonal projections:
w = 1
3 w1 + 1
7 w2 =
 10
21, 10
21, 1
7, 1
21
T ∈W,
z = v −w = −1
2 y1 −1
21 y2 =
 11
21, −10
21, −1
7, −1
21
T ∈W ⊥.
Proposition 4.44. If W is a ﬁnite-dimensional subspace of an inner product space, then
(W ⊥)⊥= W.
This result is a corollary of the orthogonal decomposition derived in Proposition 4.40.
Warning. Propositions 4.40 and 4.44 are not necessarily true for inﬁnite-dimensional sub-
spaces. If dim W = ∞, one can assert only that W ⊆(W ⊥)⊥. For example, it can be
shown, [19; Exercise 10.2.D], that on every bounded interval [a, b] the orthogonal com-
plement of the subspace of all polynomials P(∞) ⊂C0[a, b] with respect to the L2 inner
product is trivial: (P(∞))⊥= {0}. This means that the only continuous function that
satisﬁes
⟨xn , f(x) ⟩=
 b
a
xnf(x) dx = 0
for all
n = 0, 1, 2, . . .

220
4 Orthogonality
is the zero function f(x) ≡0. But the orthogonal complement of {0} is the entire space,
and so ((P(∞))⊥)⊥= C0[a, b] ̸= P(∞).
The diﬀerence is that, in inﬁnite-dimensional function space, a proper subspace W ⊊V
can be dense†, whereas in ﬁnite dimensions, every proper subspace is a “thin” subset that
occupies only an inﬁnitesimal fraction of the entire vector space. However, this seeming
paradox is, interestingly, the reason behind the success of numerical approximation schemes
in function space, such as the ﬁnite element method, [81].
Exercises
Note: In Exercises 4.4.12–15, use the dot product.
4.4.12. Find the orthogonal complement W ⊥of the subspaces W ⊂R3 spanned by the
indicated vectors. What is the dimension of W ⊥in each case?
(a)
⎛
⎜
⎝
3
−1
1
⎞
⎟
⎠, (b)
⎛
⎜
⎝
1
2
3
⎞
⎟
⎠,
⎛
⎜
⎝
2
0
1
⎞
⎟
⎠, (c)
⎛
⎜
⎝
1
2
3
⎞
⎟
⎠,
⎛
⎜
⎝
2
4
6
⎞
⎟
⎠, (d)
⎛
⎜
⎝
0
1
−1
⎞
⎟
⎠,
⎛
⎜
⎝
−2
3
1
⎞
⎟
⎠,
⎛
⎜
⎝
−1
2
0
⎞
⎟
⎠, (e)
⎛
⎜
⎝
1
1
0
⎞
⎟
⎠,
⎛
⎜
⎝
1
0
1
⎞
⎟
⎠,
⎛
⎜
⎝
0
1
1
⎞
⎟
⎠.
4.4.13. Find a basis for the orthogonal complement of the following subspaces of R3:
(a) the
plane 3x + 4y −5z = 0; (b) the line in the direction ( −2, 1, 3 )T ; (c) the image of the
matrix
⎛
⎜
⎝
1
2
−1
3
−2
0
2
1
−1
2
1
4
⎞
⎟
⎠; (d) the cokernel of the same matrix.
4.4.14. Find a basis for the orthogonal complement of the following subspaces of R4:
(a) the
set of solutions to −x + 3y −2z + w = 0; (b) the subspace spanned by ( 1, 2, −1, 3 )T ,
( −2, 0, 1, −2 )T , ( −1, 2, 0, 1 )T ; (c) the kernel of the matrix in Exercise 4.4.13c; (d) the
coimage of the same matrix.
4.4.15. Decompose each of the following vectors with respect to the indicated subspace as
v = w + z, where w ∈W, z ∈W ⊥.
(a) v =

1
2
	
, W = span
 
−3
1
	 #
;
(b) v =
⎛
⎜
⎝
1
2
−1
⎞
⎟
⎠, W = span
⎧
⎪
⎨
⎪
⎩
⎛
⎜
⎝
−3
2
1
⎞
⎟
⎠,
⎛
⎜
⎝
−1
0
5
⎞
⎟
⎠
⎫
⎪
⎬
⎪
⎭; (c) v =
⎛
⎜
⎝
1
0
0
⎞
⎟
⎠, W = ker

1
2
−1
2
0
2
	
;
(d) v =
⎛
⎜
⎝
1
0
0
⎞
⎟
⎠, W = img
⎛
⎜
⎝
1
0
1
−2
−1
0
1
3
−5
⎞
⎟
⎠; (e) v =
⎛
⎜
⎜
⎜
⎝
1
0
0
1
⎞
⎟
⎟
⎟
⎠, W = ker

1
0
0
2
−2
−1
1
−3
	
.
4.4.16. Redo Exercise 4.4.12 using the weighted inner product ⟨v , w ⟩= v1 w1+2v2 w2+3v3 w3
instead of the dot product.
4.4.17. Redo Example 4.43 with the dot product replaced by the weighted inner product
⟨v , w ⟩= v1 w1 + 2v2 w2 + 3v3 w3 + 4v4 w4.
♦4.4.18. Prove that the orthogonal complement W ⊥of a subspace W ⊂V is itself a subspace.
†
In general, a subset W ⊂V of a normed vector space is dense if, for every v ∈V , and every
ε > 0, one can ﬁnd w ∈W with ∥v −w∥< ε. The Weierstrass Approximation Theorem, [19;
Theorem 10.2.2], tells us that the polynomials form a dense subspace of the space of continuous
functions, and underlies the proof of the result mentioned in the preceding paragraph.

4.4 Orthogonal Projections and Orthogonal Subspaces
221
4.4.19. Let V = P(4) denote the space of quartic polynomials, with the L2 inner product
⟨p , q ⟩=
 1
−1 p(x) q(x) dx. Let W = P(2) be the subspace of quadratic polynomials.
(a) Write down the conditions that a polynomial p ∈P(4) must satisfy in order to belong
to the orthogonal complement W ⊥. (b) Find a basis for and the dimension of W ⊥.
(c) Find an orthogonal basis for W ⊥.
4.4.20. Let W ⊂V . Prove that (a) W ∩W ⊥= {0}, (b) W ⊆(W ⊥)⊥.
4.4.21. Let V be an inner product space. Prove that (a) V ⊥= {0}, (b) {0}⊥= V .
4.4.22. Prove that if W1 ⊂W2 are ﬁnite-dimensional subspaces of an inner product space,
then W ⊥
1 ⊃W ⊥
2 .
4.4.23.(a) Show that if W, Z ⊂Rn are complementary subspaces, then W ⊥and Z⊥are also
complementary subspaces. (b) Sketch a picture illustrating this result when W and Z are
lines in R2.
4.4.24. Prove that if W, Z are subspaces of an inner product space, then (W +Z)⊥= W ⊥∩Z⊥.
(See Exercise 2.2.22(b) for the deﬁnition of the sum of two subspaces.)
♦4.4.25. Fill in the details of the proof of Proposition 4.44.
♦4.4.26. Prove Lemma 4.35.
♦4.4.27. Let W ⊂V with dim V = n. Suppose w1, . . . , wm is an orthogonal basis for W and
wm+1, . . . , wn is an orthogonal basis for W ⊥. (a) Prove that the combination w1, . . . , wn
forms an orthogonal basis of V . (b) Show that if v = c1 w1 + · · · + cn wn is any vector in
V , then its orthogonal decomposition v = w + z is given by w = c1 w1 + · · · + cm wm ∈W
and z = cm+1 wm+1 + · · · + cn wn ∈W ⊥.
♥4.4.28. Consider the subspace W = { u(a) = 0 = u(b) } of the vector space C0[a, b] with the
usual L2 inner product. (a) Show that W has a complementary subspace of dimension 2.
(b) Prove that there does not exist an orthogonal complement of W.
Thus, an inﬁnite-
dimensional subspace may not admit an orthogonal complement!
Orthogonality of the Fundamental Matrix Subspaces
and the Fredholm Alternative
In Chapter 2, we introduced the four fundamental subspaces associated with an m × n
matrix A. According to the Fundamental Theorem 2.49, the ﬁrst two, the kernel (null
space) and the coimage (row space), are subspaces of Rn having complementary dimensions.
The second two, the cokernel (left null space) and the image (column space), are subspaces
of Rm, also of complementary dimensions. In fact, more than this is true — the paired
subspaces are orthogonal complements with respect to the standard Euclidean dot product!
Theorem 4.45. Let A be a real m×n matrix. Then its kernel and coimage are orthogonal
complements as subspaces of Rn under the dot product, while its cokernel and image are
orthogonal complements in Rm, also under the dot product:
ker A = (coimg A)⊥⊂Rn,
coker A = (img A)⊥⊂Rm.
(4.46)
Proof : A vector x ∈Rn lies in ker A if and only if A x = 0. According to the rules of
matrix multiplication, the ith entry of A x equals the vector product of the ith row rT
i of

222
4 Orthogonality
ker A
coimg A
A
img A
cokerA
Figure 4.7.
The Fundamental Matrix Subspaces.
A and x. But this product vanishes, rT
i x = ri · x = 0, if and only if x is orthogonal
to ri. Therefore, x ∈ker A if and only if x is orthogonal to all the rows of A. Since the
rows span coimg A, this is equivalent to x lying in its orthogonal complement (coimg A)⊥,
which proves the ﬁrst statement. Orthogonality of the image and cokernel follows by the
same argument applied to the transposed matrix AT .
Q.E.D.
Combining Theorems 2.49 and 4.45, we deduce the following important characterization
of compatible linear systems.
Theorem 4.46. A linear system A x = b has a solution if and only if b is orthogonal to
the cokernel of A.
Indeed, the system has a solution if and only if the right-hand side belongs to the image
of the coeﬃcient matrix, b ∈img A, which, by (4.46), requires that b be orthogonal to its
cokernel. Thus, the compatibility conditions for the linear system A x = b can be written
in the form
y · b = 0
for every y satisfying
AT y = 0.
(4.47)
In practice, one only needs to check orthogonality of b with respect to a basis y1, . . . , ym−r
of the cokernel, leading to a system of m −r compatibility constraints
yi · b = 0,
i = 1, . . ., m −r.
(4.48)
Here r = rank A denotes the rank of the coeﬃcient matrix, and so m−r is also the number
of all zero rows in the row echelon form of A. Hence, (4.48) contains precisely the same
number of constraints as would be derived using Gaussian Elimination.
Theorem 4.46 is known as the Fredholm alternative, named after the Swedish mathe-
matician Ivar Fredholm. His primary motivation was to solve linear integral equations, but
his compatibility criterion was recognized to be a general property of linear systems, includ-
ing linear algebraic systems, linear diﬀerential equations, linear boundary value problems,
and so on.
Example 4.47.
In Example 2.40, we analyzed the linear system A x = b with coeﬃcient
matrix A =
⎛
⎝
1
0
−1
0
1
−2
1
−2
3
⎞
⎠. Using direct Gaussian Elimination, we were led to a single
compatibility condition, namely −b1 + 2b2 + b3 = 0, required for the system to have a
solution. We now understand the meaning behind this equation: it is telling us that the
right-hand side b must be orthogonal to the cokernel of A. The cokernel is determined by

4.4 Orthogonal Projections and Orthogonal Subspaces
223
solving the homogeneous adjoint system AT y = 0, and is the line spanned by the vector
y1 = (−1, 2, 1)T. Thus, the compatibility condition requires that b be orthogonal to y1,
in accordance with the Fredholm alternative (4.48).
Example 4.48.
Let us determine the compatibility conditions for the linear system
x1 −x2 + 3x3 = b1,
−x1 + 2x2 −4x3 = b2,
2x1 + 3x2 + x3 = b3,
x1 + 2x3 = b4,
by computing the cokernel of its coeﬃcient matrix
A =
⎛
⎜
⎝
1
−1
3
−1
2
−4
2
3
1
1
0
2
⎞
⎟
⎠.
We need to solve the homogeneous adjoint system AT y = 0, namely
y1 −y2 + 2y3 + y4 = 0,
−y1 + 2y2 + 3y3 = 0,
3y1 −4y2 + y3 + 2y4 = 0.
Applying Gaussian Elimination, we deduce that the general solution
y = y3 ( −7, −5, 1, 0 )T + y4 ( −2, −1, 0, 1 )T
is a linear combination (whose coeﬃcients are the free variables) of the two basis vectors
for cokerA. Thus, the Fredholm compatibility conditions (4.48) are obtained by taking
their dot products with the right-hand side of the original system:
−7b1 −5b2 + b3 = 0,
−2b1 −b2 + b4 = 0.
The reader can check that these are indeed the same compatibility conditions that result
from a direct Gaussian Elimination on the augmented matrix

A | b

.
Remark. Conversely, rather than solving the homogeneous adjoint system, we can use
Gaussian Elimination on the augmented matrix

A | b

to determine the m −r basis
vectors y1, . . . , ym−r for cokerA. They are formed from the coeﬃcients of b1, . . . , bm in
the m −r consistency conditions yi · b = 0 for i = 1, . . . , m −r, arising from the all zero
rows in the reduced row echelon form.
We are now very close to a full understanding of the fascinating geometry that lurks
behind the simple algebraic operation of multiplying a vector x ∈Rn by an m × n matrix,
resulting in a vector b = A x ∈Rm. Since the kernel and coimage of A are orthogonal
complements in the domain space Rn, Proposition 4.41 tells us that we can uniquely
decompose x = w + z, where w ∈coimg A, while z ∈ker A. Since A z = 0, we have
b = A x = A(w + z) = Aw.
Therefore, we can regard multiplication by A as a combination of two operations:
(i) The ﬁrst is an orthogonal projection onto the coimage of A taking x to w.
(ii) The second maps a vector in coimg A ⊂Rn to a vector in img A ⊂Rm, taking the
orthogonal projection w to the image vector b = Aw = A x.
Moreover, if A has rank r, then both img A and coimg A are r-dimensional subspaces,
albeit of diﬀerent vector spaces. Each vector b ∈img A corresponds to a unique vector
w ∈coimg A. Indeed, if w, w ∈coimg A satisfy b = Aw = A w, then A(w −w) = 0, and
hence w −w ∈ker A. But, since the kernel and the coimage are orthogonal complements,

224
4 Orthogonality
the only vector that belongs to both is the zero vector, and hence w = w. In this manner,
we have proved the ﬁrst part of the following result; the second is left as Exercise 4.4.38.
Theorem 4.49. Multiplication by an m × n matrix A of rank r deﬁnes a one-to-one
correspondence between the r-dimensional subspaces coimg A ⊂Rn and img A ⊂Rm.
Moreover, if v1, . . . , vr forms a basis of coimg A then their images Av1, . . . , Avr form a
basis for img A.
In summary, the linear system A x = b has a solution if and only if b ∈img A, or,
equivalently, is orthogonal to every vector y ∈cokerA. If the compatibility conditions
hold, then the system has a unique solution w ∈coimg A that, by the deﬁnition of the
coimage, is a linear combination of the rows of A. The general solution to the system is
x = w + z, where w is the particular solution belonging to the coimage, while z ∈ker A is
an arbitrary element of the kernel.
Theorem 4.50. A compatible linear system A x = b with b ∈img A = (cokerA)⊥has a
unique solution w ∈coimg A satisfying Aw = b. The general solution is x = w + z, where
z ∈ker A. The particular solution w ∈coimg A is distinguished by the fact that it has the
smallest Euclidean norm of all possible solutions: ∥w∥≤∥x∥whenever A x = b.
Proof : We have already established all but the last statement. Since the coimage and
kernel are orthogonal subspaces, the norm of a general solution x = w + z is
∥x∥2 = ∥w + z∥2 = ∥w∥2 + 2 w · z + ∥z∥2 = ∥w∥2 + ∥z∥2 ≥∥w∥2,
with equality if and only if z = 0.
Q.E.D.
In practice, to determine the unique minimum-norm solution to a compatible linear
system, we invoke the orthogonality of the coimage and kernel of the coeﬃcient matrix.
Thus, if z1, . . . , zn−r form a basis for ker A, then the minimum-norm solution x = w ∈
coimg A is obtained by solving the enlarged system
A x = b,
zT
1 x = 0,
. . .
zT
n−r x = 0.
(4.49)
The associated (m + n −r) × n coeﬃcient matrix is simply obtained by appending the
(transposed) kernel vectors to the original matrix A. The resulting matrix is guaranteed
to have maximum rank n, and so, assuming b ∈img A, the enlarged system has a unique
solution, which is the minimum-norm solution to the original system A x = b.
Example 4.51.
Consider the linear system
⎛
⎜
⎝
1
−1
2
−2
0
1
−2
1
1
3
−5
2
5
−1
9
−6
⎞
⎟
⎠
⎛
⎜
⎝
x
y
z
w
⎞
⎟
⎠=
⎛
⎜
⎝
−1
1
4
6
⎞
⎟
⎠.
(4.50)
Applying the usual Gaussian Elimination algorithm, we discover that the coeﬃcient matrix
has rank 3, and its kernel is spanned by the single vector z1 = ( 1, −1, 0, 1 )T . The system
itself is compatible; indeed, the right-hand side is orthogonal to the basis cokernel vector
( 2, 24, −7, 1 )T , and so satisﬁes the Fredholm condition (4.48). The general solution to the
linear system is x = ( t, 3 −t, 1, t )T , where t = w is the free variable.

4.4 Orthogonal Projections and Orthogonal Subspaces
225
To ﬁnd the solution of minimum Euclidean norm, we can apply the algorithm described
in the previous paragraph.† Thus, we supplement the original system by the constraint
( 1
−1
0
1 )
⎛
⎜
⎝
x
y
z
w
⎞
⎟
⎠= x −y + w = 0
(4.51)
that the solution be orthogonal to the kernel basis vector. Solving the combined linear
system (4.50–51) leads to the unique solution x = w = ( 1, 2, 1, 1 )T , obtained by setting
the free variable t equal to 1. Let us check that its norm is indeed the smallest among all
solutions to the original system:
∥w∥=
√
7 ≤∥x∥= ∥( t, 3 −t, 1, t )T ∥=

3t2 −6t + 10 ,
where the quadratic function inside the square root achieves its minimum value of
√
7 at
t = 1. It is further distinguished as the only solution that can be expressed as a linear
combination of the rows of the coeﬃcient matrix:
wT = ( 1, 2, 1, 1 )
= −4 ( 1, −1, 2, −2 ) −17 ( 0, 1, −2, 1 ) + 5 ( 1, 3, −5, 2 ),
meaning that w lies in the coimage of the coeﬃcient matrix.
Exercises
4.4.29. For each of the following matrices A, (i) ﬁnd a basis for each of the four fundamental
subspaces; (ii) verify that the image and cokernel are orthogonal complements; (iii) verify
that the coimage and kernel are orthogonal complements:
(a)

1
−2
2
−4
	
, (b)
⎛
⎜
⎝
5
0
1
2
0
2
⎞
⎟
⎠, (c)
⎛
⎜
⎝
0
1
2
−1
0
−3
−2
3
0
⎞
⎟
⎠, (d)
⎛
⎜
⎝
1
2
0
1
−1
1
3
1
0
3
3
2
⎞
⎟
⎠,
(e)
⎛
⎜
⎝
3
1
4
2
7
1
1
2
0
3
5
2
7
3
12
⎞
⎟
⎠, (f )
⎛
⎜
⎜
⎜
⎝
1
3
0
−2
−2
1
2
3
−3
5
4
4
1
−4
−2
−1
⎞
⎟
⎟
⎟
⎠, (g)
⎛
⎜
⎜
⎜
⎜
⎜
⎝
−1
2
2
−1
2
−4
−5
2
−3
6
2
−3
1
−2
−3
1
−2
4
−5
−2
⎞
⎟
⎟
⎟
⎟
⎟
⎠
.
4.4.30. For each of the following matrices, use Gaussian elimination on the augmented matrix

A | b

to determine a basis for its cokernel:
(a)

9
−6
6
−4
	
,
(b)
⎛
⎜
⎝
1
3
2
6
−3
−9
⎞
⎟
⎠,
(c)
⎛
⎜
⎝
1
1
3
−1
1
−2
−1
3
6
⎞
⎟
⎠,
(d)
⎛
⎜
⎜
⎜
⎝
1
−2
−2
0
−1
3
2
−5
−1
−2
2
10
⎞
⎟
⎟
⎟
⎠.
4.4.31. Let A =
⎛
⎜
⎝
1
−2
2
−1
−2
4
−3
5
−1
2
0
7
⎞
⎟
⎠. (a) Find a basis for coimg A. (b) Use Theorem 4.49
to ﬁnd a basis of img A. (c) Write each column of A as a linear combination of the basis
vectors you found in part (b).
†
An alternative is to orthogonally project the general solution onto the coimage. The result is
the same.

226
4 Orthogonality
4.4.32. Write down the compatibility conditions on the following systems of linear equations by
ﬁrst computing a basis for the cokernel of the coeﬃcient matrix. (a) 2x + y = a,
x + 4y = b, −3x + 2y = c; (b) x + 2y + 3z = a, −x + 5y −2z = b, 2x −3y + 5z = c;
(c) x1 + 2x2 + 3x3 = b1, x2 + 2x3 = b2, 3x1 + 5x2 + 7x3 = b3, −2x1 + x2 + 4x3 = b4;
(d) x−3y+2z+w = a, 4x−2y+2z+3w = b, 5x−5y+4z+4w = c, 2x+4y−2z+w = d.
4.4.33. For each of the following m × n matrices, decompose the ﬁrst standard basis vector
e1 = w + z ∈Rn, where w ∈coimg A and z ∈ker A. Verify your answer by expressing w as
a linear combination of the rows of A.
(a)

1
−2
1
2
−3
2
	
, (b)
⎛
⎜
⎝
1
1
2
−1
0
−1
−2
−1
−3
⎞
⎟
⎠, (c)
⎛
⎜
⎝
1
−1
0
3
2
1
3
3
1
2
3
0
⎞
⎟
⎠, (d)

−1
1
1
−1
2
−3
2
−1
−2
0
	
.
4.4.34. For each of the following linear systems, (i) verify compatibility using the Fredholm
alternative, (ii) ﬁnd the general solution, and (iii) ﬁnd the solution of minimum
Euclidean norm:
(a)
2x −4y = −6,
−x + 2y = 3,
(b)
2x + 3y = −1,
3x + 7y = 1,
−3x + 2y = 8,
(c) 6x −3y + 9z = 12,
2x −y + 3z = 4,
(d)
x + 3y + 5z = 3,
−x + 4y + 9z = 11,
2x + 3y + 4z = 0,
(e)
x1 −3x2 + 7x3 = −8,
2x1 + x2 = 5,
4x1 −3x2 + 10x3 = −5,
−2x1 + 2x2 −6x3 = 4.
(f )
x −y + 2z + 3w = 5,
3x −3y + 5z + 7w = 13,
−2x + 2y + z + 4w = 0.
4.4.35. Show that if A = AT is a symmetric matrix, then A x = b has a solution if and only if
b is orthogonal to ker A.
♦4.4.36. Suppose v1, . . . , vn span a subspace V ⊂Rm. Prove that w is orthogonal to V if and
only if w ∈coker A, where A = ( v1 v2 . . . vn ) is the matrix with the indicated columns.
4.4.37. Let A =
⎛
⎜
⎜
⎜
⎝
1
−1
0
2
2
−2
0
4
−1
1
1
−1
0
0
2
2
⎞
⎟
⎟
⎟
⎠. (a) Find an orthogonal basis for coimg A.
(b) Find an
orthogonal basis for ker A.
(c) If you combine your bases from parts (a) and (b), do you
get an orthogonal basis of R4? Why or why not?
♦4.4.38. Prove that if v1, . . . , vr are a basis of coimg A, then their images Av1, . . . , Avr are a
basis for img A.
4.4.39. True or false: The standard algorithm for ﬁnding a basis for ker A will always produce
an orthogonal basis.
♦4.4.40. Is Theorem 4.45 true as stated for complex matrices? If not, can you formulate a
similar theorem that is true? What is the Fredholm alternative for complex matrices?
4.5 Orthogonal Polynomials
Orthogonal and orthonormal bases play, if anything, an even more essential role in func-
tion spaces. Unlike the Euclidean space Rn, most of the obvious bases of a typical (ﬁnite-
dimensional) function space are not orthogonal with respect to any natural inner product.
Thus, the computation of an orthonormal basis of functions is a critical step towards sim-
pliﬁcation of the analysis. The Gram–Schmidt algorithm, in any of the above formulations,
can be successfully applied to construct suitably orthogonal functions. The most impor-

4.5 Orthogonal Polynomials
227
tant examples are the classical orthogonal polynomials that arise in approximation and
interpolation theory. Other orthogonal systems of functions play starring roles in Fourier
analysis and its generalizations, including wavelets, in quantum mechanics, in the solution
of partial diﬀerential equations by separation of variables, and a host of further applications
in mathematics, physics, engineering, numerical analysis, etc., [43, 54, 62, 61, 77, 79, 88].
The Legendre Polynomials
We shall construct an orthonormal basis for the vector space P(n) of polynomials of degree
≤n. For deﬁniteness, the construction will be based on the L2 inner product
⟨p , q ⟩=
 1
−1
p(t) q(t) dt
(4.52)
on the interval [−1, 1]. The underlying method will work on any other bounded interval,
as well as for weighted inner products, but (4.52) is of particular importance. We shall
apply the Gram–Schmidt orthogonalization process to the elementary, but non-orthogonal
monomial basis 1, t, t2, . . . tn. Because
⟨tk , tl ⟩=
 1
−1
tk+l dt =
⎧
⎨
⎩
2
k + l + 1,
k + l even,
0,
k + l odd,
(4.53)
odd-degree monomials are orthogonal to those of even degree, but that is all. We will use
q0(t), q1(t), . . . , qn(t) to denote the resulting orthogonal polynomials. We begin by setting
q0(t) = 1,
with
∥q0∥2 =
 1
−1
q0(t)2 dt = 2.
According to formula (4.17), the next orthogonal basis polynomial is
q1(t) = t −⟨t , q0 ⟩
∥q0∥2 q0(t) = t,
with
∥q1∥2 = 2
3 .
In general, the Gram–Schmidt formula (4.19) says we should deﬁne
qk(t) = tk −
k−1

j =0
⟨tk , qj ⟩
∥qj∥2
qj(t)
for
k = 1, 2, . . . .
We can thus recursively compute the next few orthogonal polynomials:
q2(t) = t2 −1
3,
∥q2∥2 =
8
45 ,
q3(t) = t3 −3
5 t,
∥q3∥2 =
8
175 ,
q4(t) = t4 −6
7 t2 + 3
35 ,
∥q4∥2 =
128
11025 ,
q5(t) = t5 −10
9 t3 + 5
21 t
∥q6∥2 =
128
43659 ,
(4.54)
and so on. The reader can verify that they satisfy the orthogonality conditions
⟨qi , qj ⟩=
 1
−1
qi(t) qj(t) dt = 0,
i ̸= j.
The resulting polynomials q0, q1, q2, . . . are known as the monic† Legendre polynomials, in
honor of the eighteenth-century French mathematician Adrien-Marie Legendre, who ﬁrst
†
A polynomial is called monic if its leading coeﬃcient is equal to 1.

228
4 Orthogonality
used them for studying Newtonian gravitation. Since the ﬁrst n Legendre polynomials,
namely q0, . . . , qn−1 span the subspace P(n−1) of polynomials of degree ≤n −1, the next
one, qn, can be characterized as the unique monic polynomial that is orthogonal to every
polynomial of degree ≤n −1:
⟨tk , qn ⟩= 0,
k = 0, . . ., n −1.
(4.55)
Since the monic Legendre polynomials form a basis for the space of polynomials, we can
uniquely rewrite any polynomial of degree n as a linear combination:
p(t) = c0 q0(t) + c1 q1(t) + · · · + cn qn(t).
(4.56)
In view of the general orthogonality formula (4.7), the coeﬃcients are simply given by inner
products
ck = ⟨p , qk ⟩
∥qk∥2 =
1
∥qk∥2
 1
−1
p(t) qk(t) dt,
k = 0, . . . , n.
(4.57)
For example,
t4 = q4(t) + 6
7 q2(t) + 1
5 q0(t) =

t4 −6
7 t2 + 3
35

+ 6
7

t2 −1
3

+ 1
5,
where the coeﬃcients can be obtained either directly or via (4.57):
c4 = 11025
128
 1
−1
t4 q4(t) dt = 1,
c3 = 175
8
 1
−1
t4 q3(t) dt = 0,
and so on.
The classical Legendre polynomials, [59], are certain scalar multiples, namely
Pk(t) =
(2k)!
2k (k!)2 qk(t),
k = 0, 1, 2, . . .,
(4.58)
and so also deﬁne a system of orthogonal polynomials. The multiple is ﬁxed by the re-
quirement that
Pk(1) = 1,
(4.59)
which is not so important here, but does play a role in other applications. The ﬁrst few
classical Legendre polynomials are
P0(t) = 1,
∥P0 ∥2 = 2,
P1(t) = t,
∥P1 ∥2 = 2
3 ,
P2(t) = 3
2 t2 −1
2 ,
∥P2 ∥2 = 2
5,
P3(t) = 5
2 t3 −3
2 t,
∥P3 ∥2 = 2
7,
P4(t) = 35
8 t4 −15
4 t2 + 3
8 ,
∥P4 ∥2 = 2
9,
P5(t) = 63
8 t5 −35
4 t3 + 15
8 t,
∥P5 ∥2 =
2
11,
(4.60)
and are graphed in Figure 4.8. There is, in fact, an explicit formula for the Legendre poly-
nomials, due to the early nineteenth-century mathematician, banker, and social reformer
Olinde Rodrigues.
Theorem 4.52. The Rodrigues formula for the classical Legendre polynomials is
Pk(t) =
1
2k k!
dk
dtk (t2 −1)k,
∥Pk ∥=

2
2k + 1 ,
k = 0, 1, 2, . . . .
(4.61)

4.5 Orthogonal Polynomials
229
Figure 4.8.
The Legendre Polynomials P0(t), . . . , P5(t).
Thus, for example,
P4(t) =
1
16 · 4!
d4
dt4 (t2 −1)4 =
1
384
d4
dt4 (t2 −1)4 = 35
8 t4 −15
4 t2 + 3
8 .
Proof of Theorem 4.52:
Let
Rj,k(t) = dj
dtj (t2 −1)k,
(4.62)
which is evidently a polynomial of degree 2k−j. In particular, the Rodrigues formula (4.61)
claims that Pk(t) is a multiple of Rk,k(t). Note that
d
dt Rj,k(t) = Rj+1,k(t).
(4.63)
Moreover,
Rj,k(1) = 0 = Rj,k(−1)
whenever
j < k,
(4.64)
since, by the product rule, diﬀerentiating (t2 −1)k a total of j < k times still leaves at
least one factor of t2 −1 in each summand, which therefore vanishes at t = ±1. In order
to complete the proof of the ﬁrst formula, let us establish the following result:
Lemma 4.53. If j ≤k, then the polynomial Rj,k(t) is orthogonal to all polynomials of
degree ≤j −1.
Proof : In other words,
⟨ti , Rj,k ⟩=
 1
−1
ti Rj,k(t) dt = 0,
for all
0 ≤i < j ≤k.
(4.65)

230
4 Orthogonality
Since j > 0, we use (4.63) to write Rj,k(t) = R ′
j−1,k(t). Integrating by parts,
⟨ti , Rj,k ⟩=
 1
−1
ti R ′
j−1,k(t) dt
= i ti Rj−1,k(t)

1
t=−1 −i
 1
−1
ti−1 Rj−1,k(t) dt = −i ⟨ti−1 , Rj−1,k ⟩,
where the boundary terms vanish owing to (4.64).
In particular, setting i = 0 proves
⟨1 , Rj,k ⟩= 0 for all j > 0. We then repeat the process, and, eventually, for any j > i,
⟨ti , Rj,k ⟩= −i ⟨ti−1 , Rj−1,k ⟩
= i(i −1) ⟨ti−2 , Rj−2,k ⟩= · · · = (−1)i i(i −1) · · · 3 · 2 ⟨1 , Rj−i,k ⟩= 0,
completing the proof.
Q.E.D.
In particular, Rk,k(t) is a polynomial of degree k that is orthogonal to every polynomial
of degree ≤k −1. By our earlier remarks, this implies that it must be a constant multiple,
Rk,k(t) = ck Pk(t),
of the kth Legendre polynomial. To determine ck, we need only compare the leading terms:
Rk,k(t)= dk
dtk (t2−1)k = dk
dtk (t2k+ · · · )= (2k)!
k!
tk+ · · · , while Pk(t)= (2k)!
2k (k!)2 t2k+ · · · .
We conclude that ck = 2k k!, which proves the ﬁrst formula in (4.61). The proof of the
formula for ∥Pk ∥can be found in Exercise 4.5.9.
Q.E.D.
The Legendre polynomials play an important role in many aspects of applied math-
ematics, including numerical analysis, least squares approximation of functions, and the
solution of partial diﬀerential equations, [61].
Exercises
4.5.1. Write the following polynomials as linear combinations of monic Legendre polynomials.
Use orthogonality to compute the coeﬃcients:
(a) t3, (b) t4 + t2, (c) 7t4 + 2t3 −t.
4.5.2.(a) Find the monic Legendre polynomial of degree 5 using the Gram–Schmidt process.
Check your answer using the Rodrigues formula. (b) Use orthogonality to write t5 as a
linear combination of Legendre polynomials. (c) Repeat the exercise for degree 6.
♦4.5.3.(a) Explain why qn is the unique monic polynomial that satisﬁes (4.55). (b) Use this
characterization to directly construct q5(t).
4.5.4. Prove that the even (odd) degree Legendre polynomials are even (odd) functions of t.
4.5.5. Prove that if p(t) = p(−t) is an even polynomial, then all the odd-order coeﬃcients
c2j+1 = 0 in its Legendre expansion (4.56) vanish.
4.5.6. Write out an explicit Rodrigues-type formula for the monic Legendre polynomial qk(t)
and its norm.
4.5.7. Write out an explicit Rodrigues-type formula for an orthonormal basis Q0(t), . . . , Qn(t)
for the space of polynomials of degree ≤n under the inner product (4.52).
♦4.5.8. Use the Rodrigues formula to prove (4.59). Hint: Write (t2 −1)k = (t −1)k (t + 1)k.

4.5 Orthogonal Polynomials
231
♥4.5.9. A proof of the formula in (4.61) for the norms of the Legendre polynomials is based
on the following steps. (a) First, prove that ∥Rk,k ∥2 = (−1)k (2k)!
 1
−1 (t2 −1)k dt by a
repeated integration by parts. (b) Second, prove that
 1
−1 (t2 −1)k dt = (−1)k 22k+1 (k!)2
(2k + 1)!
by using the change of variables t = cos θ in the integral. The resulting trigonometric
integral can be done by another repeated integration by parts. (c) Finally, use the
Rodrigues formula to complete the proof.
♥4.5.10.(a) Find the roots, Pn(t) = 0, of the Legendre polynomials P2, P3 and P4. (b) Prove
that for 0 ≤j ≤k, the polynomial Rj,k(t) deﬁned in (4.62) has roots of order k −j at
t = ±1, and j additional simple roots lying between −1 and 1. Hint: Use induction on j
and Rolle’s Theorem from calculus, [2, 78].
(c) Conclude that all k roots of the Legendre
polynomial Pk(t) are real and simple, and that they lie in the interval −1 < t < 1.
Other Systems of Orthogonal Polynomials
The standard Legendre polynomials form an orthogonal system with respect to the L2
inner product on the interval [−1, 1]. Dealing with any other interval, or, more generally,
a weighted inner product, leads to a diﬀerent, suitably adapted collection of orthogonal
polynomials. In all cases, applying the Gram–Schmidt process to the standard monomials
1, t, t2, t3, . . . will produce the desired orthogonal system.
Example 4.54.
In this example, we construct orthogonal polynomials for the weighted
inner product†
⟨f , g ⟩=
 ∞
0
f(t) g(t) e−t dt
(4.66)
on the interval [0, ∞). A straightforward integration by parts proves that
 ∞
0
tk e−t dt = k!,
and hence
⟨ti , tj ⟩= (i + j)!,
∥ti ∥2 = (2i)!.
(4.67)
We apply the Gram–Schmidt process to construct a system of orthogonal polynomials for
this inner product. The ﬁrst few are
q0(t) = 1,
∥q0 ∥2 = 1,
q1(t) = t −⟨t , q0 ⟩
∥q0 ∥2 q0(t) = t −1,
∥q1 ∥2 = 1,
q2(t) = t2 −⟨t2 , q0 ⟩
∥q0 ∥2
q0(t) −⟨t2 , q1 ⟩
∥q1 ∥2
q1(t) = t2 −4t + 2,
∥q2 ∥2 = 4,
q3(t) = t3 −9t2 + 18t −6,
∥q3 ∥2 = 36.
(4.68)
The resulting orthogonal polynomials are known as the (monic) Laguerre polynomials,
named after the nineteenth-century French mathematician Edmond Laguerre, [59].
†
The functions f, g must not grow too rapidly as t →∞in order that the inner product be
deﬁned. For example, polynomial growth, meaning | f(t) |, | g(t) | ≤C tN for t ≫0 and some
C > 0, 0 ≤N < ∞, suﬃces.

232
4 Orthogonality
In some cases, a change of variables may be used to relate systems of orthogonal poly-
nomials and thereby circumvent the Gram–Schmidt computation. Suppose, for instance,
that our goal is to construct an orthogonal system of polynomials for the L2 inner product
⟨⟨f , g ⟩⟩=
 b
a
f(t) g(t) dt
on the interval [a, b]. The key remark is that we can map the interval [−1, 1] to [a, b] by
a simple change of variables of the form s = α + β t. Speciﬁcally,
s = 2t −b −a
b −a
will change
a ≤t ≤b
to
−1 ≤s ≤1.
(4.69)
It therefore changes functions F(s), G(s), deﬁned for −1 ≤s ≤1, into functions
f(t) = F
 2t −b −a
b −a

,
g(t) = G
 2t −b −a
b −a

,
(4.70)
deﬁned for a ≤t ≤b. Moreover, when integrating, we have ds =
2
b −a dt, and so the inner
products are related by
⟨f , g ⟩=
 b
a
f(t) g(t) dt =
 b
a
F
 2t −b −a
b −a

G
 2t −b −a
b −a

dt
=
 1
−1
F(s) G(s) b −a
2
ds = b −a
2
⟨F , G ⟩,
(4.71)
where the ﬁnal L2 inner product is over the interval [−1, 1]. In particular, the change of
variables maintains orthogonality, while rescaling the norms; explicitly,
⟨f , g ⟩= 0
if and only if
⟨F , G ⟩= 0,
while
∥f ∥=

b −a
2
∥F ∥.
(4.72)
Moreover, if F(s) is a polynomial of degree n in s, then f(t) is a polynomial of degree n in
t and conversely. Let us apply these observations to the Legendre polynomials:
Proposition 4.55. The transformed Legendre polynomials
P k(t) = Pk
 2t −b −a
b −a

,
∥P k ∥=

b −a
2k + 1 ,
k = 0, 1, 2, . . . ,
(4.73)
form an orthogonal system of polynomials with respect to the L2 inner product on the
interval [a, b].
Example 4.56.
Consider the L2 inner product ⟨⟨f , g ⟩⟩=
2 1
0 f(t) g(t) dt.
The map
s = 2t −1 will change 0 ≤t ≤1 to −1 ≤s ≤1. According to Proposition 4.55, this
change of variables will convert the Legendre polynomials Pk(s) into an orthogonal system
of polynomials on [0, 1], namely
P k(t) = Pk(2t −1),
with corresponding L2 norms
∥P k ∥=

1
2k + 1 .
The ﬁrst few are
P 0(t) = 1,
P 1(t) = 2t −1,
P 2(t) = 6t2 −6t + 1,
P 3(t) = 20t3 −30t2 + 12t −1,
P 4(t) = 70t4 −140t3 + 90t2 −20t + 1,
P 5(t) = 252t5 −630t4 + 560t3 −210t2 + 30t −1.
(4.74)

4.5 Orthogonal Polynomials
233
Alternatively, one can derive these formulas through a direct application of the Gram–
Schmidt process.
Exercises
4.5.11. Construct polynomials P0, P1, P2, and P3 of degree 0, 1, 2, and 3, respectively, that are
orthogonal with respect to the inner products (a) ⟨f , g ⟩=
 2
1 f(t) g(t) dt, (b) ⟨f , g ⟩=
 1
0 f(t)g(t)t dt, (c) ⟨f , g ⟩=
 1
−1 f(t)g(t)t2 dt, (d) ⟨f , g ⟩=
 ∞
−∞f(t)g(t)e−| t | dt..
4.5.12. Find the ﬁrst four orthogonal polynomials on the interval [0, 1] for the weighted L2
inner product with weight w(t) = t2.
4.5.13. Write down an orthogonal basis for vector space P(5) of quintic polynomials under the
inner product ⟨f , g ⟩=
 2
−2 f(t) g(t) dt.
4.5.14. Use the Gram–Schmidt process based on the L2 inner product on [0, 1] to construct a
system of orthogonal polynomials of degree ≤4. Verify that your polynomials are multiples
of the modiﬁed Legendre polynomials found in Example 4.56.
4.5.15. Find the ﬁrst four orthogonal polynomials under the Sobolev H1 inner product
⟨f , g ⟩=
 1
−1

f(t)g(t) + f′(t)g′(t)

dt; cf. Exercise 3.1.27.
♦4.5.16. Prove the formula for ∥P k ∥in (4.73) .
4.5.17. Find the monic Laguerre polynomials of degrees 4 and 5 and their norms.
♦4.5.18. Prove the integration formula (4.67).
♦4.5.19.(a) The physicists’ Hermite polynomials are orthogonal with respect to the inner
product ⟨f , g ⟩=
 ∞
−∞f(t) g(t) e−t2
dt. Find the ﬁrst ﬁve monic Hermite polynomials.
Hint:
 ∞
−∞e−t2
dt = √π . (b) The probabilists prefer to use the inner product
⟨f , g ⟩=
 ∞
−∞f(t) g(t) e−t2/2 dt. Find the ﬁrst ﬁve of their monic Hermite polynomials.
(c) Can you ﬁnd a change of variables that transforms the physicists’ versions to the
probabilists’ versions?
♥4.5.20. The Chebyshev polynomials: (a) Prove that Tn(t) = cos(n arccos t), n = 0, 1, 2, . . . ,
form a system of orthogonal polynomials under the weighted inner product
⟨f , g ⟩=
 1
−1
f(t) g(t) dt
√
1 −t2
.
(4.75)
(b) What is ∥Tn ∥? (c) Write out the formulas for T0(t), . . . , T6(t) and plot their graphs.
4.5.21. Does the Gram–Schmidt process for the inner product (4.75) lead to the Chebyshev
polynomials Tn(t) deﬁned in the preceding exercise? Explain why or why not.
4.5.22. Find two functions that form an orthogonal basis for the space of the solutions to the
diﬀerential equation y′′ −3y′ + 2y = 0 under the L2 inner product on [0, 1].
4.5.23. Find an orthogonal basis for the space of solutions to the diﬀerential equation
y′′′ −y′′ + y′ −y = 0 for the L2 inner product on [−π, π ].

234
4 Orthogonality
♥4.5.24. In this exercise, we investigate the eﬀect of more general changes of variables on
orthogonal polynomials. (a) Prove that t = 2s2 −1 deﬁnes a one-to-one map from the
interval 0 ≤s ≤1 to the interval −1 ≤t ≤1.
(b) Let pk(t) denote the monic Legendre
polynomials, which are orthogonal on −1 ≤t ≤1. Show that qk(s) = pk(2s2 −1)
deﬁnes a polynomial. Write out the cases k = 0, 1, 2, 3 explicitly. (c) Are the polynomials
qk(s) orthogonal under the L2 inner product on [0, 1]? If not, do they retain any sort of
orthogonality property? Hint: What happens to the L2 inner product on [−1, 1] under the
change of variables?
4.5.25.(a) Show that the change of variables s = e−t maps the Laguerre inner product (4.66)
to the standard L2 inner product on [0, 1]. However, explain why this does not allow you
to change Legendre polynomials into Laguerre polynomials.
(b) Describe the functions
resulting from applying the change of variables to the modiﬁed Legendre polynomials (4.74)
and their orthogonality properties. (c) Describe the functions that result from applying
the inverse change of variables to the Laguerre polynomials (4.68) and their orthogonality
properties.
4.5.26. Explain how to adapt the numerically stable Gram–Schmidt method in (4.28) to
construct a system of orthogonal polynomials. Test your algorithm on one of the preceding
exercises.

Chapter 5
Minimization and Least Squares
Because Nature seems to strive for eﬃciency, many systems arising in physical applications
are founded on a minimization principle. In a mechanical system, the stable equilibrium
conﬁgurations minimize the potential energy. In an electrical circuit, the current adjusts
itself to minimize the power. In optics and relativity, light rays follow the paths of minimal
distance — the geodesics on the curved space-time manifold. Solutions to most of the
boundary value problems arising in applications to continuum mechanics are also char-
acterized by a minimization principle, which is then employed to design ﬁnite element
numerical approximations to their solutions, [61, 81].
Optimization — ﬁnding minima
or maxima — is ubiquitous throughout mathematical modeling, physics, engineering, eco-
nomics, and data science, including the calculus of variations, diﬀerential geometry, control
theory, design and manufacturing, linear programming, machine learning, and beyond.
This chapter introduces and solves the most basic mathematical minimization problem:
a quadratic polynomial function depending on several variables. (Minimization of more
complicated functions is of comparable signiﬁcance, but relies on the nonlinear methods
of multivariable calculus, and thus lies outside our scope.) Assuming that the quadratic
coeﬃcient matrix is positive deﬁnite, the minimizer can be found by solving an associated
linear algebraic system. Orthogonality also plays an important role in minimization prob-
lems. Indeed, the orthogonal projection of a point onto a subspace turns out to be the
closest point or least squares minimizer. Moreover, when written in terms of an orthogonal
or orthonormal basis for the subspace, the orthogonal projection has an elegant explicit
formula that also oﬀers numerical advantages over the direct approach to least squares
minimization.
The most common way of ﬁtting a function to prescribed data points is to minimize the
least squares error, which serves to quantify the overall deviation between the data and the
sampled function values. Our presentation includes an introduction to the interpolation of
data points by functions, with a particular emphasis on polynomials and splines. The ﬁnal
Section 5.6 is devoted to the basics of discrete Fourier analysis — the interpolation of data
by trigonometric functions — culminating in the remarkable Fast Fourier Transform, a key
algorithm in modern signal processing and numerical analysis. Additional applications of
these tools in equilibrium mechanics and electrical circuits will form the focus of Chapter 6.
5.1 Minimization Problems
Let us begin by introducing three important minimization problems — the ﬁrst arising in
physics, the second in analysis, and the third in geometry.
© Springer International Publishing AG, part of Springer Nature 2018 
https://doi.org/10.1007/978-3-319-91041-3_5 
235
P. J. Olver, C. Shakiban, Applied Linear Algebra, Undergraduate Texts in Mathematics, 

236
5 Minimization and Least Squares
Figure 5.1.
Minimizing a Quadratic Function.
Equilibrium Mechanics
A fundamental principle of mechanics is that systems in equilibrium minimize potential
energy. For example, a ball in a bowl will roll downhill unless it is sitting at the bottom,
where its potential energy due to gravity is at a (local) minimum. In the simplest class of
examples, the energy is a quadratic function, e.g.,
f(x, y) = 3x2 −2xy + 4y2 + x −2y + 1,
(5.1)
and one seeks the point x = x⋆, y = y⋆, (if one exists) at which f(x⋆, y⋆) achieves its
overall minimal value.
Similarly, a pendulum will swing back and forth unless it rests at the bottom of its arc,
where potential energy is minimized. Actually, the pendulum has a second equilibrium
position at the top of the arc, as in Figure 5.2, but this is rarely observed, since it is an
unstable equilibrium, meaning that any tiny movement will knock it oﬀbalance. There-
fore, a better way of stating the principle is that stable equilibria are where the mechanical
system (locally) minimizes potential energy. For a ball rolling on a curved surface, the
local minima — the bottoms of valleys — are the stable equilibria, while the local maxima
— the tops of hills — are unstable. Minimization principles serve to characterize the equi-
librium conﬁgurations of a wide range of physical systems, including masses and springs,
structures, electrical circuits, and even continuum models of solid mechanics and elasticity,
ﬂuid mechanics, relativity, electromagnetism, thermodynamics, and so on.
Solution of Equations
Suppose we wish to solve a system of equations
f1(x) = 0,
f2(x) = 0,
. . .
fm(x) = 0,
(5.2)
where x = (x1, . . . , xn) ∈Rn. This system can be converted into a minimization problem
in the following seemingly silly manner. Deﬁne
p(x) =

f1(x)
2 + · · · +

fm(x)
2 = ∥f(x)∥2,
(5.3)
where f(x) = ( f1(x), . . . , fm(x) )T and ∥·∥denotes the Euclidean norm on Rm. Clearly,
p(x) ≥0 for all x. Moreover, p(x⋆) = 0 if and only if each summand is zero, and hence

5.1 Minimization Problems
237
Stable
Unstable
Figure 5.2.
Equilibria of a Pendulum.
x = x⋆is a solution to (5.2).
Therefore, the minimum value of p(x) is zero, and the
minimum is achieved if and only if we are at a solution to the original system of equations.
For us, the most important case is that of a linear system
A x = b
(5.4)
consisting of m equations in n unknowns. In this case, the solutions may be obtained by
minimizing the function
p(x) = ∥A x −b∥2,
(5.5)
where ∥·∥denotes the Euclidean norm on Rm. Clearly p(x) has a minimum value of 0,
which is achieved if and only if x is a solution to the linear system (5.4). Of course, it
is not clear that we have gained much, since we already know how to solve A x = b by
Gaussian Elimination. However, this artiﬁce turns out to have profound consequences.
Suppose that the linear system (5.4) does not have a solution, i.e., b does not lie in
the image of the matrix A. This situation is very typical when there are more equations
than unknowns. Such problems arise in data ﬁtting, when the measured data points are all
supposed to lie on a straight line, say, but rarely do so exactly, due to experimental error.
Although we know there is no exact solution to the system, we might still try to ﬁnd an
approximate solution — a vector x⋆that comes as close to solving the system as possible.
One way to measure closeness is by looking at the magnitude of the error as measured by
the residual vector r = b−A x, i.e., the diﬀerence between the right- and left-hand sides of
the system. The smaller its norm ∥r∥= ∥A x −b∥, the better the attempted solution. For
the Euclidean norm, the vector x⋆that minimizes the squared residual norm function (5.5)
is known as the least squares solution to the linear system, because ∥r∥2 = r2
1 + · · · +r2
n is
the sum of the squares of the individual error components. As before, if the linear system
(5.4) happens to have an actual solution, with A x⋆= b, then x⋆qualiﬁes as the least
squares solution too, since in this case, ∥A x⋆−b∥= 0 achieves its absolute minimum. So
least squares solutions include traditional solutions as special cases.
Unlike an exact solution, the least squares minimizer depends on the choice of inner
product governing the norm; thus a suitable weighted norm can be introduced to emphasize
or de-emphasize the various errors. While not the only possible approach, least squares
is certainly the easiest to analyze and solve, and, hence, is often the method of choice for

238
5 Minimization and Least Squares
b
v
V
Figure 5.3.
The Closest Point.
ﬁtting functions to experimental data and performing statistical analysis. It is essential
that the norm arise from an inner product; minimizing the error based on other kinds of
norms is a much more diﬃcult, nonlinear problem, although one that has recently become
of immense practical interest in the newly emergent ﬁeld of compressed sensing, [28].
The Closest Point
The following minimization problem arises in elementary geometry, although its practical
implications cut a much wider swath. Given a point b ∈Rm and a subset V ⊂Rm, ﬁnd
the point v⋆∈V that is closest to b. In other words, we seek to minimize the Euclidean
distance d(v, b) = ∥v −b∥over all possible v ∈V .
The simplest situation occurs when V is a subspace of Rm. In this case, the closest
point problem can, in fact, be reformulated as a least squares minimization problem. Let
v1, . . . , vn be a basis for V . The general element v ∈V is a linear combination of the
basis vectors. Applying our handy matrix multiplication formula (2.13), we can write the
subspace elements in the form
v = x1 v1 + · · · + xn vn = A x,
where A = ( v1 v2 . . . vn ) is the m × n matrix formed by the (column) basis vectors
and x = ( x1, x2, . . . , xn )T are the coordinates of v relative to the chosen basis. In this
manner, we can identify V with the image of A, i.e., the subspace spanned by its columns.
Consequently, the closest point in V to b is found by minimizing ∥v −b∥2 = ∥A x −b∥2
over all possible x ∈Rn. But this is exactly the same as the least squares function (5.5)!
Thus, if x⋆is the least squares solution to the system A x = b, then v⋆= A x⋆is the closest
point to b belonging to V = img A. In this way, we have established a profound and fertile
connection between least squares solutions to linear systems and the geometrical problem
of minimizing distances to subspaces. And, as we shall see, the closest point v ∈V turns
out to be the orthogonal projection of b onto the subspace.
All three of the preceding minimization problems are solved by the same underlying
mathematical construction, which will now be described in detail.
Remark. In this book, we will concentrate on minimization problems.
Maximizing a
function p(x) is the same as minimizing its negative −p(x), and so can be easily handled
by the same techniques.

5.2 Minimization of Quadratic Functions
239
Exercises
Note: Unless otherwise indicated, “distance” refers to the Euclidean norm.
5.1.1. Find the least squares solution to the pair of equations 3x = 1, 2x = −1.
5.1.2. Find the minimizer of the function f(x, y) = (3x −2y + 1)2 + (2x + y + 2)2.
5.1.3. Find the closest point or points to b = ( −1, 2 )T that lie on (a) the x-axis, (b) the
y-axis, (c) the line y = x, (d) the line x + y = 0, (e) the line 2x + y = 0.
5.1.4. Solve Exercise 5.1.3 when distance is measured in (i) the ∞norm, (ii) the 1 norm.
5.1.5. Given b ∈R2, is the closest point on a line L unique when distance is measured in
(a) the Euclidean norm? (b) the 1 norm? (c) the ∞norm?
♥5.1.6. Let L ⊂R2 be a line through the origin, and let b ∈R2 be any point.
(a) Find a geometrical construction of the closest point v ∈L to b when distance is
measured in the standard Euclidean norm.
(b) Use your construction to prove that there is one and only one closest point.
(c) Show that if 0 ̸= a ∈L, then the distance equals

∥a∥2 ∥b∥2 −(a · b)2
∥a∥
= | a × b |
∥a∥
,
using the two-dimensional cross product (3.22).
5.1.7. Suppose a and b are unit vectors in R2. Show that the distance from a to the line
through b is the same as the distance from b to the line through a. Use a picture to
explain why this holds. How is the distance related to the angle between the two vectors?
5.1.8.(a) Prove that the distance from the point ( x0, y0 )T to the line ax + by = 0 is
| ax0 + by0 |
√
a2 + b2
. (b) What is the minimum distance to the line ax + by + c = 0?
♥5.1.9.(a) Generalize Exercise 5.1.8 to ﬁnd the distance between a point ( x0, y0, z0 )T and the
plane ax + by + cz + d = 0 in R3. (b) Use your formula to compute the distance between
( 1, 1, 1 )T and the plane 3x −2y + z = 1.
5.1.10.(a) Explain in detail why the minimizer of ∥v −b∥coincides with the minimizer of
∥v −b∥2. (b) Find all scalar functions F(x) for which the minimizer of F

∥v −b∥

is
the same as the minimizer of ∥v −b∥.
5.1.11.(a) Explain why the problem of maximizing the distance from a point to a subspace
does not have a solution.
(b) Can you formulate a situation in which maximizing distance
to a point leads to a problem with a solution?
5.2 Minimization of Quadratic Functions
The simplest algebraic equations are linear systems. As such, one must thoroughly un-
derstand them before venturing into the far more complicated nonlinear realm. For mini-
mization problems, the starting point is the quadratic function. (Linear functions do not
have minima — think of the function f(x) = αx + β, whose graph is a straight line†.) In
this section, we shall see how the problem of minimizing a general quadratic function of n
†
Technically, this function is linear only when β = 0; otherwise it is known as an “aﬃne
function”. See Chapter 7 for details.

240
5 Minimization and Least Squares
a > 0
a < 0
a = 0
Figure 5.4.
Parabolas.
variables can be solved by linear algebra techniques.
Let us begin by reviewing the very simplest example — minimizing a scalar quadratic
polynomial
p(x) = ax2 + 2bx + c
(5.6)
over all possible values of x ∈R. If a > 0, then the graph of p is a parabola opening
upwards, and so there exists a unique minimum value.
If a < 0, the parabola points
downwards, and there is no minimum (although there is a maximum). If a = 0, the graph
is a straight line, and there is neither minimum nor maximum over all x ∈R — except in
the trivial case b = 0 also, and the function p(x) = c is constant, with every x qualifying as a
minimum (and a maximum). The three nontrivial possibilities are illustrated in Figure 5.4.
In the case a > 0, the minimum can be found by calculus. The critical points of a
function, which are candidates for minima (and maxima), are found by setting its derivative
to zero. In this case, diﬀerentiating, and solving
p′(x) = 2ax + 2b = 0,
we conclude that the only possible minimum value occurs at
x⋆= −b
a ,
where
p(x⋆) = c −b2
a .
(5.7)
Of course, one must check that this critical point is indeed a minimum, and not a maximum
or inﬂection point. The second derivative test will show that p′′(x⋆) = 2a > 0, and so x⋆
is at least a local minimum.
A more instructive approach to this problem — and one that requires only elementary
algebra — is to “complete the square”. As in (3.66), we rewrite
p(x) = a

x + b
a
2
+ ac −b2
a
.
(5.8)
If a > 0, then the ﬁrst term is always ≥0, and, moreover, attains its minimum value 0
only at x⋆= −b/a. The second term is constant, and so is unaﬀected by the value of x.
Thus, the global minimum of p(x) is at x⋆= −b/a. Moreover, its minimal value equals
the constant term, p(x⋆) = c −b2/a, thereby reconﬁrming and strengthening the calculus
result in (5.7).
Now that we have the one-variable case ﬁrmly in hand, let us turn our attention to the
more substantial problem of minimizing quadratic functions of several variables. Thus, we
seek to minimize a (real) quadratic polynomial
p(x) = p(x1, . . . , xn) =
n

i,j =1
kij xi xj −2
n

i=1
fi xi + c,
(5.9)

5.2 Minimization of Quadratic Functions
241
depending on n variables x = ( x1, x2, . . . , xn )T ∈Rn. The coeﬃcients kij, fi and c are
all assumed to be real.
Moreover, we can assume, without loss of generality, that the
coeﬃcients of the quadratic terms are symmetric: kij = kji. (See Exercise 3.4.15 for a
justiﬁcation.) Note that p(x) is more general than a quadratic form (3.52) in that it also
contains linear and constant terms. We seek a global minimum, and so the variables x
are allowed to vary over all of Rn. (Minimizing a quadratic function over a proper subset
x ∈S ⊊Rn is a more challenging problem, and will not be discussed here.)
Let us begin by rewriting the quadratic function (5.9) in a more compact matrix nota-
tion:
p(x) = xT K x −2 xTf + c,
x ∈Rn,
(5.10)
in which K = (kij) is a symmetric n × n matrix, f ∈Rn is a constant vector, and c is a
constant scalar.
Example 5.1.
Consider the quadratic function
p(x1, x2) = 4x2
1 −2x1 x2 + 3x2
2 + 3x1 −2x2 + 1
depending on two real variables x1, x2. It can be written in the matrix form (5.10) as
p(x1, x2) = ( x1 x2 )

4
−1
−1
3

x1
x2

−2 ( x1 x2 )

−3
2
1

+ 1,
(5.11)
whereby
K =

4
−1
−1
3

,
x =

x1
x2

,
f =

−3
2
1

,
c = 1.
(5.12)
Pay attention to the symmetry of K = KT , whereby its corresponding oﬀ-diagonal entries,
here both −1, are each one-half the coeﬃcient of the corresponding quadratic monomial,
in this case −2x1 x2. Also note the overall factor of −2 in front of the linear terms, which
is included for later convenience.
We ﬁrst note that in the simple scalar case (5.6), we needed to impose the condition
that the quadratic coeﬃcient a be positive in order to obtain a (unique) minimum. The
corresponding condition for the multivariable case is that the quadratic coeﬃcient matrix
K be positive deﬁnite. This key assumption enables us to establish a general minimization
criterion.
Theorem 5.2. If K is a positive deﬁnite (and hence symmetric) matrix, then the quad-
ratic function (5.10) has a unique minimizer, which is the solution to the linear system
K x = f,
namely
x⋆= K−1f.
(5.13)
The minimum value of p(x) is equal to any of the following expressions:
p(x⋆) = p(K−1f) = c −f TK−1f = c −f T x⋆= c −(x⋆)T K x⋆.
(5.14)
Proof : First recall that, by Proposition 3.31, positive deﬁniteness implies that K is a
nonsingular matrix, and hence the linear system (5.13) has a unique solution x⋆= K−1f.
Then, for all x ∈Rn, since f = K x⋆, it follows that
p(x) = xT K x −2 xTf + c = xT K x −2 xTK x⋆+ c
= (x −x⋆)T K(x −x⋆) +

c −(x⋆)T K x⋆
,
(5.15)

242
5 Minimization and Least Squares
where we used the symmetry of K = KT to identify the scalar terms
xT K x⋆= (xTK x⋆)T = (x⋆)T KT x = (x⋆)T K x.
The ﬁrst term in the ﬁnal expression in (5.15) has the form yT K y, where y = x−x⋆. Since
we assumed that K is positive deﬁnite, we know that yT K y > 0 for all y ̸= 0. Thus, the
ﬁrst term achieves its minimum value, namely 0, if and only if 0 = y = x −x⋆. Since x⋆is
ﬁxed, the second, bracketed, term does not depend on x, and hence the minimizer of p(x)
coincides with the minimizer of the ﬁrst term, namely x = x⋆. Moreover, the minimum
value of p(x) is equal to the constant term: p(x⋆) = c −(x⋆)T K x⋆.
The alternative
expressions in (5.14) follow from simple substitutions.
Q.E.D.
Example 5.1 (continued).
Let us minimize the quadratic function appearing in (5.11)
above. According to Theorem 5.2, to ﬁnd the minimum we must solve the linear system
K x = f, which, in this case, is

4
−1
−1
3

x1
x2

=

−3
2
1

.
(5.16)
When applying the usual Gaussian Elimination algorithm, only one row operation is re-
quired to place the coeﬃcient matrix in upper triangular form:

4
−1
−1
3

−3
2
1

−→

4
−1
0
11
4

−3
2
5
8

.
The coeﬃcient matrix is regular, since no row interchanges were required, and its two
pivots, namely 4 and 11
4 , are both positive. Thus, by Theorem 3.43, K > 0, and hence
p(x1, x2) really does have a minimum, obtained by applying Back Substitution to the
reduced system:
x⋆=

x⋆
1
x⋆
2

=

−7
22
5
22

≈

−.31818
.22727

.
(5.17)
The quickest way to compute the minimal value is to use the second formula in (5.14):
p(x⋆) = p

−7
22, 5
22

= 1 −

−3
2, 1


−7
22
5
22

= 13
44 ≈.29546
It is instructive to compare the algebraic solution method with the minimization proce-
dure you learned in multi-variable calculus, cf. [2, 78]. The critical points of p(x1, x2) are
found by setting both partial derivatives equal to zero:
∂p
∂x1
= 8x1 −2x2 + 3 = 0,
∂p
∂x2
= −2x1 + 6x2 −2 = 0.
If we divide by an overall factor of 2, these are precisely the same linear equations we
already constructed in (5.16). Thus, not surprisingly, the calculus approach leads to the
same minimizer (5.17). To check whether x⋆is a (local) minimum, we need to apply the
second derivative test. In the case of a function of several variables, this requires analyzing
the Hessian matrix, which is the symmetric matrix of second order partial derivatives
H =
⎛
⎜
⎜
⎝
∂2p
∂x2
1
∂2p
∂x1∂x2
∂2p
∂x1∂x2
∂2p
∂x2
2
⎞
⎟
⎟
⎠=

8
−2
−2
6

= 2K,

5.2 Minimization of Quadratic Functions
243
which is exactly twice the quadratic coeﬃcient matrix (5.12). If the Hessian matrix is
positive deﬁnite — which we already know in this case — then the critical point is indeed
a (local) minimum.
Thus, the calculus and algebraic approaches to this minimization problem lead, as they
must, to identical results. However, the algebraic method is more powerful, because it
immediately produces the unique, global minimum, whereas, barring additional work, cal-
culus can guarantee only that the critical point is a local minimum. Moreover, the proof of
the calculus local minimization criterion — that the Hessian matrix be positive deﬁnite at
the critical point — relies, in fact, on the algebraic solution to the quadratic minimization
problem! In summary: minimization of quadratic functions is a problem in linear alge-
bra, while minimizing more complicated functions requires the full force of multivariable
calculus.
The most eﬃcient method for producing a minimum of a quadratic function p(x) on
Rn, then, is to ﬁrst write out the symmetric coeﬃcient matrix K and the vector f as in
(5.10). Solving the system K x = f will produce the minimizer x⋆provided K > 0 — which
should be checked during the course of the procedure using the criteria of Theorem 3.43,
that is, making sure that no row interchanges are used and all the pivots are positive.
Example 5.3.
Let us minimize the quadratic function
p(x, y, z) = x2 + 2xy + xz + 2y2 + y z + 2z2 + 6y −7z + 5.
This has the matrix form (5.10) with
K =
⎛
⎜
⎝
1
1
1
2
1
2
1
2
1
2
1
2
2
⎞
⎟
⎠,
x =
⎛
⎜
⎝
x
y
z
⎞
⎟
⎠,
f =
⎛
⎜
⎝
0
−3
7
2
⎞
⎟
⎠,
c = 5,
and the minimum is found by solving the linear system, K x = f. Gaussian Elimination
produces the LDLT factorization
K =
⎛
⎜
⎝
1
1
1
2
1
2
1
2
1
2
1
2
2
⎞
⎟
⎠=
⎛
⎜
⎝
1
0
0
1
1
0
1
2
0
1
⎞
⎟
⎠
⎛
⎜
⎝
1
0
0
0
1
0
0
0
7
4
⎞
⎟
⎠
⎛
⎜
⎝
1
1
1
2
0
1
0
0
0
1
⎞
⎟
⎠.
The pivots, i.e., the diagonal entries of D, are all positive, and hence K is positive deﬁnite.
Theorem 5.2 then guarantees that p(x, y, z) has a unique minimizer, which is found by
solving the linear system K x = f. The solution is then quickly obtained by forward and
back substitution:
x⋆= 2,
y⋆= −3,
z⋆= 2,
with
p(x⋆, y⋆, z⋆) = p(2, −3, 2) = −11,
and we conclude that p(x, y, z) > p(2, −3, 2) = −11 for all (x, y, z) ̸= (2, −3, 2).
Theorem 5.2 solves the general quadratic minimization problem when the quadratic
coeﬃcient matrix is positive deﬁnite.
If K is not positive deﬁnite, then the quadratic
function (5.10) does not have a minimum, apart from one exceptional situation.

244
5 Minimization and Least Squares
Theorem 5.4. If the matrix K is positive deﬁnite, then the quadratic function (5.10) has
a unique global minimizer x⋆satisfying K x⋆= f. If K is only positive semi-deﬁnite, and
f ∈img K, then every solution to the linear system K x⋆= f is a global minimum of p(x),
but the minimum is not unique, since p(x⋆+ z) = p(x⋆) whenever z ∈ker K. In all other
cases, p(x) has no global minimum.
Proof : The ﬁrst part is merely a restatement of Theorem 5.2. The second part is proved
by a similar computation, and is left to the reader.
If K is not positive semi-deﬁnite,
then one can ﬁnd a vector y such that a = yT K y < 0. If we set x = ty, then p(x) =
p(ty) = at2 + 2bt + c, with b = yT f. Since a < 0, by choosing | t | ≫0 suﬃciently large,
we can arrange that p(ty) ≪0 is an arbitrarily large negative quantity, and so p has no
(ﬁnite) minimum value. The one remaining case — when K is positive semi-deﬁnite, but
f ̸∈img K — is the subject of Exercise 5.2.14.
Q.E.D.
Exercises
5.2.1. Find the minimum value of the function f(x, y, z) = x2+2xy+3y2+2y z+z2−2x+3z+2.
How do you know that your answer is really the global minimum?
5.2.2. For the potential energy function in (5.1), where is the equilibrium position of the ball?
5.2.3. For each of the following quadratic functions, determine whether there is a minimum. If
so, ﬁnd the minimizer and the minimum value for the function.
(a) x2 −2xy +4y2 +x−1, (b) 3x2 +3xy +3y2 −2x−2y +4, (c) x2 + 5xy + 3y2 + 2x −y,
(d) x2 + y2 + y z + z2 + x + y −z, (e) x2 + xy −y2 −y z + z2 −3,
(f ) x2 + 5xz + y2 −2y z + z2 + 2x −z −3, (g) x2 + xy + y2 + y z + z2 + z w + w2 −2x −w.
5.2.4.(a) For which numbers b (allowing both positive and negative numbers) is the matrix
A =

1
b
b
4
	
positive deﬁnite?
(b) Find the factorization A = LDLT when b is in the
range for positive deﬁniteness.
(c) Find the minimum value (depending on b; it might be
ﬁnite or it might be −∞) of the function p(x, y) = x2 + 2bxy + 4y2 −2y.
5.2.5. For each matrix K, vector f, and scalar c, write out the quadratic function p(x) given by
(5.10). Then either ﬁnd the minimizer x⋆and minimum value p(x⋆), or explain why there
is none. (a) K =

4
−12
−12
45
	
, f =

−1
2
2
	
, c = 3;
(b) K =

3
2
2
1
	
, f =

4
1
	
,
c = 0;
(c) K =
⎛
⎜
⎝
3
−1
1
−1
2
−1
1
−1
3
⎞
⎟
⎠, f =
⎛
⎜
⎝
1
0
−2
⎞
⎟
⎠, c = −3; (d) K =
⎛
⎜
⎝
1
1
1
1
2
−1
1
−1
1
⎞
⎟
⎠,
f =
⎛
⎜
⎝
−3
−1
2
⎞
⎟
⎠, c = 1; (e) K =
⎛
⎜
⎜
⎜
⎝
1
1
0
0
1
2
1
0
0
1
3
1
0
0
1
4
⎞
⎟
⎟
⎟
⎠, f =
⎛
⎜
⎜
⎜
⎝
−1
2
−3
4
⎞
⎟
⎟
⎟
⎠, c = 0.
5.2.6. Find the minimum value of the quadratic function
p(x1, . . . , xn) = 4
n

i=1
x2
i −2
n−1

i=1
xi xi+1 +
n

i=1
xi
for
n = 2, 3, 4.
5.2.7. Find the maximum value of the quadratic functions
(a) −x2 + 3xy −5y2 −x + 1,
(b) −2x2 + 6xy −3y2 + 4x −3y.

5.3 The Closest Point
245
5.2.8. Suppose K1 and K2 are positive deﬁnite n × n matrices. Suppose that, for i = 1, 2, the
minimizer of pi(x) = xT Kix −2xT fi + ci, is x⋆
i . Is the minimizer of p(x) = p1(x) + p2(x)
given by x⋆= x⋆
1 + x⋆
2? Prove or give a counterexample.
♦5.2.9. Let K > 0. Prove that a quadratic function p(x) = xT K x −2 xT f without constant
term has non-positive minimum value: p(x⋆) ≤0. When is the minimum value zero?
5.2.10. Let q(x) = xT A x be a quadratic form. Prove that the minimum value of q(x) is
either 0 or −∞.
5.2.11. Under what conditions does the aﬃne function p(x) = xT f + c have a minimum?
♦5.2.12. Under what conditions does a quadratic function p(x) = xT K x −2 xT f + c have a ﬁnite
global maximum? Explain how to ﬁnd the maximizer and maximum value.
5.2.13. True or false: The minimal-norm solution to A x = b is obtained by setting all the free
variables to zero.
♦5.2.14. Prove that if K is a positive semi-deﬁnite matrix, and f ̸∈img K, then the quadratic
function p(x) = xT K x −2xT f + c has no minimum value.
Hint: Try looking at vectors x ∈ker K.
5.2.15. Why can’t you minimize a complex-valued quadratic function?
5.3 The Closest Point
We are now ready to solve the geometric problem of ﬁnding the element in a prescribed
subspace that lies closest to a given point. For simplicity, we work mostly with subspaces
of Rm, equipped with the Euclidean norm and inner product, but the method extends
straightforwardly to arbitrary ﬁnite-dimensional subspaces of any inner product space.
However, it does not apply to more general norms not associated with inner products,
such as the 1 norm, the ∞norm and, in fact, the p norms whenever p ̸= 2. In such cases,
ﬁnding the closest point problem is a nonlinear minimization problem whose solution
requires more sophisticated analytical techniques; see, for example, [28, 66, 79].
Problem. Let Rm be equipped with an inner product ⟨v , w ⟩and associated norm ∥v∥,
and let W ⊂Rm be a subspace. Given b ∈Rm, the goal is to ﬁnd the point w⋆∈W that
minimizes ∥w −b∥over all possible w ∈W. The minimal distance d⋆= ∥w⋆−b∥to the
closest point is designated as the distance from the point b to the subspace W.
Of course, if b ∈W lies in the subspace, then the answer is easy: the closest point in
W is w⋆= b itself, and the distance from b to the subspace is zero. Thus, the problem
becomes interesting only when b ̸∈W.
In solving the closest point problem, the goal is to minimize the squared distance
∥w −b∥2 = ⟨w −b , w −b ⟩= ∥w∥2 −2 ⟨w , b ⟩+ ∥b∥2
(5.18)
over all possible w belonging to the subspace W ⊂Rm. Let us assume that we know a
basis w1, . . . , wn of W, with n = dim W. Then the most general vector in W is a linear
combination
w = x1 w1 + · · · + xn wn
(5.19)
of the basis vectors. We substitute the formula (5.19) for w into the squared distance
function (5.18). As we shall see, the resulting expression is a quadratic function of the
coeﬃcients x = ( x1, x2, . . . , xn )T , and so the minimum is provided by Theorem 5.2.

246
5 Minimization and Least Squares
First, the quadratic terms come from expanding
∥w∥2 = ⟨x1 w1 + · · · + xn wn , x1 w1 + · · · + xn wn ⟩=
n

i,j =1
xi xj ⟨wi , wj ⟩.
(5.20)
Therefore,
∥w∥2 =
n

i,j =1
kij xi xj = xT Kx,
where K is the symmetric n × n Gram matrix whose (i, j) entry is the inner product
kij = ⟨wi , wj ⟩
(5.21)
between the basis vectors of our subspace; see Deﬁnition 3.33. Similarly,
⟨w , b ⟩= ⟨x1 w1 + · · · + xn wn , b ⟩=
n

i=1
xi ⟨wi , b ⟩,
and so
⟨w , b ⟩=
n

i=1
xi fi = xT f,
where f ∈Rn is the vector whose ith entry is the inner product
fi = ⟨wi , b ⟩
(5.22)
between the point and the subspace’s basis elements. Substituting back, we conclude that
the squared distance function (5.18) reduces to the quadratic function
p(x) = xTKx −2xTf + c =
n

i,j =1
kij xi xj −2
n

i=1
fi xi + c,
(5.23)
in which K and f are given in (5.21–22), while c = ∥b∥2.
Since we assumed that the basis vectors w1, . . . , wn are linearly independent, Proposi-
tion 3.36 assures us that their associated Gram matrix is positive deﬁnite. Therefore, we
may directly apply our basic Minimization Theorem 5.2 to solve the closest point problem.
Theorem 5.5. Let w1, . . ., wn form a basis for the subspace W ⊂Rm. Given b ∈Rm,
the closest point w⋆= x⋆
1 w1 + · · · + x⋆
n wn ∈W is unique and prescribed by the solution
x⋆= K−1f to the linear system
K x = f,
(5.24)
where the entries of K and f are given in (5.21–22). The (minimum) distance between the
point and the subspace is
d⋆= ∥w⋆−b∥=

∥b∥2 −f Tx⋆.
(5.25)
When the standard dot product and Euclidean norm on Rm are used to measure dis-
tance, the entries of the Gram matrix K and the vector f are given by
kij = wi · wj = wT
i wj,
fi = wi · b = wT
i b.
As in (3.62), each set of equations can be combined into a single matrix equation.
If
A = ( w1 w2 . . . wn ) denotes the m × n matrix formed by the basis vectors, then
K = ATA,
f = AT b,
c = bT b = ∥b∥2.
(5.26)

5.3 The Closest Point
247
A direct derivation of these equations is instructive. Since, by formula (2.13),
w = x1 w1 + · · · + xn wn = A x,
we have
∥w −b∥2 = ∥A x −b∥2 = (A x −b)T (A x −b) = (xT AT −bT ) (A x −b)
= xT AT A x −2 xTAT b + bT b = xT Kx −2xTf + c,
thereby justifying (5.26). Thus, Theorem 5.5 implies that the closest point w⋆= Ax⋆∈W
to b in the Euclidean norm is obtained by solving what are known as the normal equations
(ATA)x = AT b,
(5.27)
for
x⋆= (ATA)−1AT b,
giving
w⋆= Ax⋆= A(ATA)−1AT b.
(5.28)
If, instead of the Euclidean inner product, we adopt a weighted inner product ⟨v , w ⟩=
vT C w on Rm prescribed by a positive deﬁnite m × m matrix C > 0, then the same
computations produce
K = AT CA,
f = AT C b,
c = bT C b = ∥b∥2.
(5.29)
The resulting formula for the weighted Gram matrix K was previously derived in (3.64).
In this case, the closest point w⋆∈W in the weighted norm is obtained by solving the
weighted normal equations
AT CAx = AT C b,
(5.30)
so that
x⋆= (AT C A)−1 AT C b,
w⋆= Ax⋆= A(AT C A)−1AT C b.
(5.31)
Example 5.6.
Let W ⊂R3 be the plane spanned by w1 =
⎛
⎝
1
2
−1
⎞
⎠, w2 =
⎛
⎝
2
−3
−1
⎞
⎠.
Our goal is to ﬁnd the point w⋆∈W closest to b =
⎛
⎝
1
0
0
⎞
⎠, where distance is mea-
sured in the usual Euclidean norm.
We combine the basis vectors to form the matrix
A =
⎛
⎝
1
2
2
−3
−1
−1
⎞
⎠. According to (5.26), the positive deﬁnite Gram matrix and associated
vector are
K = ATA =

6
−3
−3
14

,
f = AT b =

1
2

.
(Alternatively, these can be computed directly by taking inner products, as in (5.21–22).)
We solve the linear system
K x = f
for
x⋆= K−1 f =

4
15
1
5

.
Theorem 5.5 implies that the closest point is
w⋆= A x⋆= x⋆
1 w1 + x⋆
2 w2 =
⎛
⎜
⎝
2
3
−1
15
−7
15
⎞
⎟
⎠≈
⎛
⎝
.6667
−.0667
−.4667
⎞
⎠.

248
5 Minimization and Least Squares
The distance from the point b to the plane is d⋆= ∥w⋆−b∥=
1
√
3 ≈.5774.
Suppose, on the other hand, that distance in R3 is measured in the weighted norm
∥v∥
=
v2
1 +
1
2 v2
2 +
1
3 v2
3
corresponding to the positive deﬁnite diagonal matrix
C = diag

1, 1
2, 1
3

. In this case, we form the weighted Gram matrix and vector (5.29):
K = AT CA =

1
2
−1
2
−3
−1
 ⎛
⎝
1
0
0
0
1
2
0
0
0
1
3
⎞
⎠
⎛
⎝
1
2
2
−3
−1
−1
⎞
⎠=

10
3
−2
3
−2
3
53
6

,
f = AT C b =

1
2
−1
2
−3
−1
 ⎛
⎝
1
0
0
0
1
2
0
0
0
1
3
⎞
⎠
⎛
⎝
1
0
0
⎞
⎠=

1
2

,
and so
x⋆= K−1f ≈

.3506
.2529

,
w⋆= A x⋆≈
⎛
⎝
.8563
−.0575
−.6034
⎞
⎠.
Now the distance between the point and the subspace is measured in the weighted norm:
d⋆= ∥w⋆−b∥≈.3790.
Remark. The solution to the closest point problem given in Theorem 5.5 applies, as stated,
to the more general case in which W ⊂V is a ﬁnite-dimensional subspace of a general inner
product space V . The underlying inner product space V can even be inﬁnite-dimensional,
as, for example, in least squares approximations in function space.
Now, consider what happens if we know an orthonormal basis u1, . . . , un of the subspace
W. Since, by deﬁnition, ⟨ui , uj ⟩= 0 for i ̸= j, while ⟨ui , ui ⟩= ∥ui ∥2 = 1, the associated
Gram matrix is the identity matrix: K = I . Thus, in this situation, the system (5.24)
reduces to simply x = f, with solution x⋆
i = fi = ⟨ui , b ⟩, and the closest point is given by
w⋆= x⋆
1 u1 + · · · + x⋆
n un
where
x⋆
i = ⟨b , ui ⟩,
i = 1, . . ., n.
(5.32)
We have already seen this formula!
According to Theorem 4.32, w⋆is the orthogonal
projection of b onto the subspace W. Thus, if we are supplied with an orthonormal basis
of our subspace, we can easily compute the closest point using the orthogonal projection
formula (5.32). If the basis is orthogonal, one can either normalize it or directly apply the
equivalent orthogonal projection formula (4.42).
In this manner, we have established the key connection identifying the closest point
in the subspace to a given vector with the orthogonal projection of that vector onto the
subspace.
Theorem 5.7. Let W ⊂V be a ﬁnite-dimensional subspace of an inner product space.
Given a point b ∈V , the closest point w⋆∈W coincides with the orthogonal projection
of b onto W.
Example 5.8.
Let R4 be equipped with the ordinary Euclidean norm. Consider the
three-dimensional subspace W ⊂R4 spanned by the orthogonal vectors v1 = ( 1, −1, 2, 0 )T ,
v2 = ( 0, 2, 1, −2 )T , v3 = ( 1, 1, 0, 1 )T . Given b = ( 1, 2, 2, 1 )T , our task is to ﬁnd the clos-
est point w⋆∈W. Since the spanning vectors are orthogonal (but not orthonormal), we
can use the orthogonal projection formula (4.42) to ﬁnd w⋆= x1 v1 + x2 v2 + x3 v3, with
x1 = ⟨b , v1 ⟩
∥v1 ∥2 = 3
6 = 1
2 ,
x2 = ⟨b , v2 ⟩
∥v2 ∥2 = 4
9 ,
x3 = ⟨b , v3 ⟩
∥v3 ∥2 = 4
3 .

5.3 The Closest Point
249
Thus, the closest point to b in the given subspace is
w⋆= 1
2 v1 + 4
9 v2 + 4
3 v3 =
 11
6 , 31
18, 13
9 , 4
9
T .
We further note that, in accordance with the orthogonal projection property, the vector
z = b −w⋆=

−5
6, 5
18, 5
9, 5
9
T
is orthogonal to v1, v2, v3 and hence to the entire subspace.
Even when we only know a non-orthogonal basis for the subspace, it may still be a good
strategy to ﬁrst apply the Gram–Schmidt process in order to replace it by an orthonormal or
orthogonal basis, and then apply the relevant orthogonal projection formula to calculate the
closest point. Not only does this simplify the ﬁnal computation, it can often ameliorate the
numerical inaccuracies associated with ill-conditioning that can aﬄict the direct solution
to the system (5.24). The following example illustrates this alternative procedure.
Example 5.9.
Let us return to the problem, solved in Example 5.6, of ﬁnding the
closest point in the plane W spanned by w1 = ( 1, 2, −1 )T , w2 = ( 2, −3, −1 )T to the
point b = ( 1, 0, 0 )T . We proceed by ﬁrst using the Gram–Schmidt process to compute an
orthogonal basis
v1 = w1 =
⎛
⎝
1
2
−1
⎞
⎠,
v2 = w2 −w2 · v1
∥v1 ∥2 v1 =
⎛
⎜
⎝
5
2
−2
−3
2
⎞
⎟
⎠,
for our subspace.
As a result, we can use the orthogonal projection formula (4.42) to
produce the closest point
w⋆= b · v1
∥v1 ∥2 v1 + b · v2
∥v2 ∥2 v2 =
⎛
⎜
⎝
2
3
−1
15
−7
15
⎞
⎟
⎠,
reconﬁrming our earlier result.
Exercises
Note: Unless otherwise indicated, “distance” refers to the Euclidean norm.
5.3.1. Find the closest point in the plane spanned by ( 1, 2, −1 )T , ( 0, −1, 3 )T to the point
( 1, 1, 1 )T . What is the distance between the point and the plane?
5.3.2. Redo Exercise 5.3.1 using
(a) the weighted inner product ⟨v , w ⟩= 2v1 w1 + 4v2 w2 + 3v3 w3;
(b) the inner product
⟨v , w ⟩= vT C w based on the positive deﬁnite matrix C =
⎛
⎜
⎝
2
−1
0
−1
2
−1
0
−1
2
⎞
⎟
⎠.
5.3.3. Find the point in the plane x + 2y −z = 0 that is closest to ( 0, 0, 1 )T .
5.3.4. Let b = ( 3, 1, 2, 1 )T . Find the closest point and the distance from b to the following
subspaces: (a) the line in the direction ( 1, 1, 1, 1 )T ; (b) the plane spanned by ( 1, 1, 0, 0 )T
and ( 0, 0, 1, 1 )T ; (c) the hyperplane spanned by ( 1, 0, 0, 0 )T , ( 0, 1, 0, 0 )T , ( 0, 0, 1, 0 )T ;
(d) the hyperplane deﬁned by the equation x + y + z + w = 0.

250
5 Minimization and Least Squares
5.3.5. Find the closest point and the distance from b = ( 1, 1, 2, −2 )T to the subspace spanned
by ( 1, 2, −1, 0 )T , ( 0, 1, −2, −1 )T , ( 1, 0, 3, 2 )T .
5.3.6. Redo Exercises 5.3.4 and 5.3.5 using
(i) the weighted inner product ⟨v , w ⟩= 1
2 v1 w1 + v2 w2 + 1
2 v3 w3 + v4 w4;
(ii) the inner product based on the positive deﬁnite matrix C =
⎛
⎜
⎜
⎜
⎝
4
−1
1
0
−1
4
−1
1
1
−1
4
−1
0
1
−1
4
⎞
⎟
⎟
⎟
⎠.
5.3.7. Find the vector w⋆∈span

( 0, 0, 1, 1 ) , ( 2, 1, 1, 1 )

that minimizes ∥w −( 0, 3, 1, 2 )∥.
5.3.8.(a) Find the distance from the point b = ( 1, 2, −1 )T to the plane x −2y + z = 0.
(b) Find the distance to the plane x −2y + z = 3.
Hint: Move the point and the plane so that the plane goes through the origin.
♦5.3.9.(a) Given a conﬁguration of n points a1, . . . , an in the plane, explain how to ﬁnd the
point x ∈R2 that minimizes the total squared distance
n

i=1
∥x −ai ∥2. (b) Apply your
method when (i) a1 = ( 1, 3 ) , a2 = ( −2, 5 ); (ii) a1 = ( 0, 0 ) , a2 = ( 0, 1 ) , a3 = ( 1, 0 ) ;
(iii) a1 = ( 0, 0 ) , a2 = ( 0, 2 ) , a3 = ( 1, 2 ) , a4 = ( −2, −1 ).
5.3.10. Answer Exercise 5.3.9 when distance is measured in (a) the weighted norm
∥x∥=

2x2
1 + 3x2
2 ;
(b) the norm based on the positive deﬁnite matrix

3
−1
−1
2
	
.
5.3.11. Explain why the quantity inside the square root in (5.25) is always non-negative.
5.3.12. Find the closest point to the vector b = ( 1, 0, 2 )T belonging the two-dimensional
subspace spanned by the orthogonal vectors v1 = ( 1, −1, 1 )T , v2 = ( −1, 1, 2 )T .
5.3.13. Let b = ( 0, 3, 1, 2 )T . Find the vector w⋆∈span {( 0, 0, 1, 1 )T , ( 2, 1, 1, −1 )T } such
that ∥w⋆−b∥is minimized.
5.3.14. Find the closest point to b = ( 1, 2, −1, 3 )T in the subspace W = span

( 1, 0, 2, 1 )T ,
( 1, 1, 0, −1 )T , ( 2, 0, 1, −1 )T 
by ﬁrst constructing an orthogonal basis of W and then
applying the orthogonal projection formula (4.42).
5.3.15. Repeat Exercise 5.3.14 using the weighted norm ∥v∥= v2
1 + 2v2
2 + v2
3 + 3v2
4.
♦5.3.16. Justify the formulas in (5.29).
5.4 Least Squares
As we ﬁrst observed in Section 5.1, the solution to the closest point problem also solves
the basic least squares minimization problem. Let us ﬁrst oﬃcially deﬁne the notion of a
(classical) least squares solution to a linear system.
Deﬁnition 5.10. A least squares solution to a linear system of equations
A x = b
(5.33)
is a vector x⋆∈Rn that minimizes the squared Euclidean norm ∥A x −b∥2.
If the system (5.33) actually has a solution, then it is automatically the least squares

5.4 Least Squares
251
solution. The concept of least squares solution is new only when the system does not have
a solution, i.e., b does not lie in the image of A. We also want the least squares solution
to be unique.
As with an ordinary solution, this happens if and only if ker A = {0},
or, equivalently, the columns of A are linearly independent, or, equivalently, rank A = n.
Indeed, if z ∈ker A, then x = x + z also satisﬁes
∥Ax −b∥2 = ∥A(x + z) −b∥2 = ∥A x −b∥2,
and hence is also a minimum. Thus, uniqueness requires z = 0.
As before, to make the connection with the closest point problem, we identify the
subspace W = img A ⊂Rm as the image or column space of the matrix A. If the columns
of A are linearly independent, then they form a basis for the image W.
Since every
element of the image can be written as w = A x, minimizing ∥A x −b∥2 is the same as
minimizing the distance ∥w −b∥between the point and the subspace. The solution x⋆to
the quadratic minimization problem produces the closest point w⋆= A x⋆in W = img A,
which is thus found using Theorem 5.5. In the Euclidean case, we therefore ﬁnd the least
squares solution by solving the normal equations given in (5.27).
Theorem 5.11. Assume that ker A = {0}. Then the least squares solution to the linear
system A x = b under the Euclidean norm is the unique solution x⋆to the normal equations
(ATA)x = AT b,
namely
x⋆= (ATA)−1AT b.
(5.34)
The least squares error is
∥Ax⋆−b∥2 = ∥b∥2 −f Tx⋆= ∥b∥2 −bTA(ATA)−1AT b .
(5.35)
Note that the normal equations (5.27) can be simply obtained by multiplying the original
system A x = b on both sides by AT . In particular, if A is square and invertible, then
(ATA)−1 = A−1(AT )−1, and so the least squares solution formula (5.34) reduces to x =
A−1b, while the two terms under the square root in the error formula (5.35) cancel out,
producing zero error. In the rectangular case — when inversion of A itself is not allowed
— (5.34) gives a new formula for the solution to the linear system A x = b whenever
b ∈img A. See also the discussion concerning the pseudoinverse of a matrix in Section 8.7
for an alternative approach.
Example 5.12.
Consider the linear system
x1 + 2x2
= 1,
3x1 −x2 + x3 = 0,
−x1 + 2x2 + x3 = −1,
x1 −x2 −2x3 = 2,
2x1 + x2 −x3 = 2,
consisting of 5 equations in 3 unknowns. The coeﬃcient matrix and right-hand side are
A =
⎛
⎜
⎜
⎜
⎝
1
2
0
3
−1
1
−1
2
1
1
−1
−2
2
1
−1
⎞
⎟
⎟
⎟
⎠,
b =
⎛
⎜
⎜
⎜
⎝
1
0
−1
2
2
⎞
⎟
⎟
⎟
⎠.

252
5 Minimization and Least Squares
A direct application of Gaussian Elimination shows that b ̸∈img A, and so the system is
incompatible — it has no solution. Of course, to apply the least squares method, we are
not required to check this in advance. If the system has a solution, it is the least squares
solution too, and the least squares method will ﬁnd it.
Let us ﬁnd the least squares solution based on the Euclidean norm, so that C = I in
Theorem 5.13. According to (5.26),
K = ATA =
⎛
⎝
16
−2
−2
−2
11
2
−2
2
7
⎞
⎠,
f = AT b =
⎛
⎝
8
0
−7
⎞
⎠.
Solving the 3 × 3 system of normal equations K x = f by Gaussian Elimination, we ﬁnd
x⋆= K−1f ≈( .4119, .2482, −.9532 )T
to be the least squares solution to the system. The least squares error is
∥b −A x⋆∥2 ≈∥( −.0917, .0342, .1313, .0701, .0252 )T ∥2 ≈.03236,
which is reasonably small — indicating that the system is, roughly speaking, not too
incompatible.
An alternative strategy is to begin by orthonormalizing the columns of A using Gram–
Schmidt. We can then apply the orthogonal projection formula (4.41) to construct the
same least squares solution. Details of the latter computation are left to the reader.
One can extend the basic least squares method by introducing a suitable weighted
norm in the measurement of the error.
Let C > 0 be a positive deﬁnite matrix that
governs the weighted norm ∥v∥2 = vT C v. In most applications, C = diag (c1, . . . , cm)
is a diagonal matrix whose entries are the assigned weights of the individual coordinates,
but the method works equally well for general norms deﬁned by positive deﬁnite matrices.
The oﬀ-diagonal entries of C can be used to weight cross-correlations between data values,
although this extra freedom is rarely used in practice. The weighted least squares solution
is thus obtained by solving the corresponding weighted normal equations (5.30), as follows.
Theorem 5.13. Suppose A is an m×n matrix such that ker A = {0}, and suppose C > 0
is any positive deﬁnite m × m matrix specifying the weighted norm ∥v∥2 = vT C v. Then
the least squares solution to the linear system A x = b that minimizes the weighted squared
error ∥A x −b∥2 is the unique solution x⋆to the weighted normal equations
AT CAx⋆= AT C b,
so that
x⋆= (AT C A)−1 AT C b.
(5.36)
The weighted least squares error is
∥Ax⋆−b∥2 = ∥b∥2 −f Tx⋆= ∥b∥2 −bTCA(ATA)−1AT C b .
(5.37)
Exercises
Note: Unless otherwise indicated, use the Euclidean norm to measure the least squares error.
5.4.1. Find the least squares solution to the linear system A x = b when
(a) A =
⎛
⎜
⎝
1
2
1
⎞
⎟
⎠, b =
⎛
⎜
⎝
1
1
0
⎞
⎟
⎠, (b) A =
⎛
⎜
⎝
1
0
2
−1
3
5
⎞
⎟
⎠, b =
⎛
⎜
⎝
1
3
7
⎞
⎟
⎠, (c) A =
⎛
⎜
⎝
2
1
−1
1
−2
0
3
−1
1
⎞
⎟
⎠, b =
⎛
⎜
⎝
1
0
1
⎞
⎟
⎠.

5.4 Least Squares
253
5.4.2. Find the least squares solutions to the following linear systems:
(a) x + 2y = 1, 3x −y = 0, −x + 2y = 3,
(b) 4x −2y = 1, 2x + 3y = −4, x −2y = −1, 2x + 2y = 2,
(c) 2u + v −2w = 1, 3u −2w = 0, u −v + 3w = 2,
(d) x −z = −1, 2x −y + 3z = 1, y −3z = 0, −5x + 2y + z = 3,
(e) x1 + x2 = 2, x2 + x4 = 1, x1 + x3 = 0, x3 −x4 = 1, x1 −x4 = 2.
5.4.3. Let A =
⎛
⎜
⎝
3
−3
1
2
4
1
1
2
1
⎞
⎟
⎠, b =
⎛
⎜
⎝
6
5
4
⎞
⎟
⎠.
Prove, using Gaussian Elimination, that the linear
system A x = b has a unique solution. Show that the least squares solution (5.34) is the
same. Explain why this is necessarily the case.
5.4.4. Find the least squares solution to the linear system A x = b when
(a) A =
⎛
⎜
⎜
⎜
⎝
2
3
4
−2
1
5
2
0
⎞
⎟
⎟
⎟
⎠, b =
⎛
⎜
⎜
⎜
⎝
2
−1
1
3
⎞
⎟
⎟
⎟
⎠,
(b) A =
⎛
⎜
⎜
⎜
⎝
2
1
4
1
−2
1
1
0
−3
5
2
−2
⎞
⎟
⎟
⎟
⎠, b =
⎛
⎜
⎜
⎜
⎝
0
0
1
0
⎞
⎟
⎟
⎟
⎠.
5.4.5. Given A =
⎛
⎜
⎜
⎜
⎝
1
2
−1
0
−2
3
1
5
−1
−3
1
1
⎞
⎟
⎟
⎟
⎠and b =
⎛
⎜
⎜
⎜
⎝
0
5
6
8
⎞
⎟
⎟
⎟
⎠, ﬁnd the least squares solution to the system
A x = b. What is the error? Interpret your result.
5.4.6. Find the least squares solution to the linear systems in Exercise 5.4.1 under the weighted
norm ∥x∥2 = x2
1 + 2x2
2 + 3x2
3.
♦5.4.7. Let A be an m × n matrix with ker A = {0}. Suppose that we use the Gram–Schmidt
algorithm to factor A = QR as in Exercise 4.3.32. Prove that the least squares solution to
the linear system A x = b is found by solving the triangular system Rx = QT b by Back
Substitution.
5.4.8. Apply the method in Exercise 5.4.7 to ﬁnd the least squares solutions to the systems in
Exercise 5.4.2.
♦5.4.9.(a) Find a formula for the least squares error (5.35) in terms of an orthonormal basis of
the subspace. (b) Generalize your formula to the case of an orthogonal basis.
5.4.10. Find the least squares solutions to the following linear systems. Hint: Check
orthogonality of the columns of the coeﬃcient matrix.
(a)
⎛
⎜
⎝
1
−1
2
2
3
−1
⎞
⎟
⎠

x
y
	
=
⎛
⎜
⎝
1
0
−1
⎞
⎟
⎠,
(b)
⎛
⎜
⎜
⎜
⎝
3
−1
0
2
−2
1
1
5
⎞
⎟
⎟
⎟
⎠

x
y
	
=
⎛
⎜
⎜
⎜
⎝
2
1
−1
1
⎞
⎟
⎟
⎟
⎠,
(c)
⎛
⎜
⎜
⎜
⎜
⎜
⎝
1
−1
−1
1
3
2
−2
1
0
1
0
−1
0
7
−1
⎞
⎟
⎟
⎟
⎟
⎟
⎠
⎛
⎜
⎝
x
y
z
⎞
⎟
⎠=
⎛
⎜
⎜
⎜
⎜
⎜
⎝
−1
0
1
−1
0
⎞
⎟
⎟
⎟
⎟
⎟
⎠
.
♦5.4.11. Suppose we are interested in solving a linear system A x = b by the method of least
squares when the coeﬃcient matrix A has linearly dependent columns. Let K x = f,
where K = AT CA, f = AT C b, be the corresponding normal equations. (a) Prove
that f ∈img K, and so the normal equations have a solution. Hint: Use Exercise 3.4.32.
(b) Prove that every solution to the normal equations minimizes the least squares error,
and hence qualiﬁes as a least squares solution to the original system. (c) Explain why the
least squares solution is not unique.
♦5.4.12. Which is the more eﬃcient algorithm: direct least squares based on solving the normal
equations by Gaussian Elimination, or using Gram–Schmidt orthonormalization and then
solving the resulting triangular system by Back Substitution as in Exercise 5.4.7? Justify
your answer.

254
5 Minimization and Least Squares
5.4.13. A group of students knows that the least squares solution to A x = b can be identiﬁed
with the closest point on the subspace img A spanned by the columns of the coeﬃcient
matrix. Therefore, they try to ﬁnd the solution by ﬁrst orthonormalizing the columns using
Gram–Schmidt, and then ﬁnding the least squares coeﬃcients by the orthonormal basis
formula (4.41). To their surprise, they does not get the same solution! Can you explain the
source of their diﬃculty? How can you use their solution to obtain the proper least squares
solution x? Check your algorithm with the system that we treated in Example 5.12.
5.5 Data Fitting and Interpolation
One of the most important applications of the least squares minimization process is to
the ﬁtting of data points. Suppose we are running an experiment in which we measure a
certain time-dependent physical quantity. At time ti we make the measurement yi, and
thereby obtain a set of, say, m data points
(t1, y1),
(t2, y2),
. . .
(tm, ym).
(5.38)
Suppose our theory indicates that all the data points are supposed to lie on a single line
y = α + β t,
(5.39)
whose precise form — meaning its coeﬃcients α, β — is to be determined. For example,
a police car is interested in clocking the speed of a vehicle by using measurements of its
relative distance at several times. Assuming that the vehicle is traveling at constant speed,
its position at time t will have the linear form (5.39), with β, the velocity, and α, the initial
position, to be determined. The amount by which β exceeds the speed limit will determine
whether the police decide to give chase. Experimental error will almost inevitably make
this measurement impossible to achieve exactly, and so the problem is to ﬁnd the straight
line (5.39) that “best ﬁts” the measured data and then use its slope to estimate the vehicle’s
velocity.
At the time t = ti, the error between the measured value yi and the sample value
predicted by the function (5.39) is
ei = yi −(α + β ti),
i = 1, . . ., m.
We can write this system of equations in the compact vectorial form
e = y −A x,
where
e =
⎛
⎜
⎜
⎜
⎝
e1
e2
...
em
⎞
⎟
⎟
⎟
⎠,
y =
⎛
⎜
⎜
⎜
⎝
y1
y2
...
ym
⎞
⎟
⎟
⎟
⎠,
while
A =
⎛
⎜
⎜
⎝
1
t1
1
t2
...
...
1
tm
⎞
⎟
⎟
⎠,
x =

α
β

.
(5.40)
We call e ∈Rm the error vector and y ∈Rm the data vector. The m × 2 matrix A is
prescribed by the sample times. The coeﬃcients α, β of our desired function (5.39) are the
unknowns, forming the entries of the column vector x ∈R2.
If we could ﬁt the data exactly, so yi = α + β ti for all i, then each error would vanish,
ei = 0, and we could solve the linear system A x = y for the coeﬃcients α, β. In the
language of linear algebra, the data points all lie on a straight line if and only if y ∈img A.

5.5 Data Fitting and Interpolation
255
Figure 5.5.
Least Squares Approximation of Data by a Straight Line.
If the data points are not collinear, then we seek the straight line that minimizes the total
squared error:
Squared Error = ∥e∥2 = e2
1 + · · · + e2
m,
which coincides with the squared Euclidean norm of the error vector. Pictorially, referring
to Figure 5.5, the errors are the vertical distances from the points to the line, and we
are seeking to minimize the sum of the squares of the individual errors†, hence the term
least squares. In other words, we are looking for the coeﬃcient vector x = ( α, β )T that
minimizes the Euclidean norm of the error vector
∥e∥= ∥A x −y∥.
(5.41)
Thus, we have a manifestation of the problem of characterizing the least squares solution
to the linear system A x = y.
Theorem 5.11 prescribes the solution to this least squares minimization problem. We
form the normal equations
(ATA)x = AT y,
with solution
x⋆= (ATA)−1AT y.
(5.42)
Invertibility of the Gram matrix K = ATA relies on the assumption that the matrix A has
linearly independent columns. For the particular matrix in (5.40), linear independence of
its two columns requires that not all the ti’s be equal, i.e., we must measure the data at
at least two distinct times. Note that this restriction does not preclude measuring some of
the data at the same time, e.g., by repeating the experiment. However, choosing all the
ti’s to be the same is a silly data ﬁtting problem. (Why?)
†
This choice of minimization may strike the reader as a little odd. Why not just minimize the
sum of the absolute value of the errors, i.e., the 1 norm ∥e∥1 = | e1 |+· · ·+| en | of the error vector,
or minimize the maximal error, i.e., the ∞norm ∥e∥∞= max{| e1 |, . . . , | en |}? The answer is
that, although each of these alternative minimization criteria is interesting and potentially useful,
they all lead to nonlinear minimization problems, and so are much harder to solve! The least
squares minimization problem can be solved by linear algebra, and so, purely on the grounds of
simplicity, is the method of choice in most applications. Moreover, as always, one needs to fully
understand the linear problem before diving into more treacherous nonlinear waters. Or, even
better, why minimize the vertical distance to the line? The shortest distance from each data point
to the line, as measured along the perpendicular and explicitly computed in Exercise 5.1.8, might
strike you as a better measure of error. To solve the latter problem, see Section 8.8 on Principal
Component Analysis, particularly Exercise 8.8.11.

256
5 Minimization and Least Squares
Under this assumption, we then compute
ATA =

1
1
. . .
1
t1
t2
. . .
tm

⎛
⎜
⎜
⎝
1
t1
1
t2
...
...
1
tm
⎞
⎟
⎟
⎠=

m
3 ti
3 ti
3 (ti)2

= m

1
t
t
t2

,
AT y =

1
1
. . .
1
t1
t2
. . .
tm

⎛
⎜
⎜
⎝
y1
y2
...
ym
⎞
⎟
⎟
⎠=
 3 yi
3 ti yi

= m

y
t y

,
(5.43)
where the overbars, namely
t = 1
m
m

i=1
ti,
y = 1
m
m

i=1
yi,
t2 = 1
m
m

i=1
t2
i ,
t y = 1
m
m

i=1
ti yi,
(5.44)
denote the average sample values of the indicated variables.
Warning. The average of a product is not equal to the product of the averages!
In
particular, t2 ̸= (t )2, t y ̸= t y.
Substituting (5.43) into the normal equations (5.42), and canceling the common factor
of m, we ﬁnd that we have only to solve the pair of linear equations
α + β t = y,
α t + β t2 = t y,
for the coeﬃcients:
α = y −β t,
β = t y −t y
t2 −( t )2 =
3 (ti −t ) yi
3 (ti −t )2 .
(5.45)
Therefore, the best (in the least squares sense) straight line that ﬁts the given data is
y = β (t −t ) + y,
(5.46)
where the line’s slope β is given in (5.45).
More generally, one may wish to assign diﬀerent weights to the measurement errors.
Suppose some of the data are known to be more reliable or more signiﬁcant than others.
For example, measurements at an earlier time may be more accurate, or more critical to
the data ﬁtting problem, than later measurements. In that situation, we should penalize
any errors in the earlier measurements and downplay errors in the later data.
In general, this requires the introduction of a positive weight ci > 0 associated with
each data point (ti, yi); the larger the weight, the more vital the error. For a straight line
approximation y = α + β t, the weighted squared error is deﬁned as
Weighted Squared Error =
m

i=1
ci e2
i = eT C e = ∥e∥2,
where C = diag (c1, . . . , cm) > 0 is the positive deﬁnite diagonal weight matrix, while ∥e∥
denotes the associated weighted norm of the error vector e = y −A x. One then applies
the weighted normal equations (5.30) to eﬀect the solution to the problem.

5.5 Data Fitting and Interpolation
257
Example 5.14.
Suppose the data points are given by the table
ti
0
1
3
6
yi
2
3
7
12
To ﬁnd the least squares line, we construct
A =
⎛
⎜
⎝
1
0
1
1
1
3
1
6
⎞
⎟
⎠,
AT =

1
1
1
1
0
1
3
6

,
y =
⎛
⎜
⎝
2
3
7
12
⎞
⎟
⎠.
Therefore
ATA =

4
10
10
46

,
AT y =

24
96

.
The normal equations (5.42) reduce to
4α + 10β = 24,
10α + 46β = 96,
so
α = 12
7 ,
β = 12
7 .
Therefore, the best least squares ﬁt to the data is the straight line
y = 12
7 + 12
7 t ≈1.71429 + 1.71429 t.
Alternatively, one can compute this formula directly from (5.45–46).
Now, suppose we assign diﬀerent weights to the preceding data points, e.g., c1 = 3,
c2 = 2, c3 =
1
2, c4 =
1
4. Thus, errors in the ﬁrst two data values are assigned higher
signiﬁcance than those in the latter two. To ﬁnd the weighted least squares line that best
ﬁts the data, we compute
AT C A =

1
1
1
1
0
1
3
6

⎛
⎜
⎝
3
0
0
0
0
2
0
0
0
0
1
2
0
0
0
0
1
4
⎞
⎟
⎠
⎛
⎜
⎝
1
0
1
1
1
3
1
6
⎞
⎟
⎠=

23
4
5
5
31
2

,
AT C y =

1
1
1
1
0
1
3
6

⎛
⎜
⎝
3
0
0
0
0
2
0
0
0
0
1
2
0
0
0
0
1
4
⎞
⎟
⎠
⎛
⎜
⎝
2
3
7
12
⎞
⎟
⎠=

37
2
69
2

.
Thus, the weighted normal equations (5.30) reduce to
23
4 α + 5β = 37
2 ,
5α + 31
2 β = 69
2 ,
so
α = 1.7817,
β = 1.6511.
Therefore, the least squares ﬁt to the data under the given weights is
y = 1.7817 + 1.6511t.
Example 5.15.
Suppose we are given a sample of an unknown radioactive isotope. At
several times ti, we measure the amount mi of radioactive material remaining in the sample.
The problem is to determine the initial amount of material along with the isotope’s half-
life. If the measurements were exact, we would have m(t) = m0eβ t, where m0 = m(0) is
the initial mass, and β < 0 the decay rate. The half-life is given by t⋆= β−1 log 2; see
Example 8.1 for additional details.

258
5 Minimization and Least Squares
As it stands, this is not a linear least squares problem. But it can be easily converted
to the proper form by taking logarithms:
y(t) = log m(t) = log m0 + β t = α + β t
where
α = log m0.
We can thus do a linear least squares ﬁt on the logarithms yi = log mi of the radioactive
mass data at the measurement times ti to determine the best values for α and β.
Exercises
5.5.1. Find the straight line y = α + β t that best ﬁts the following data in the least squares
sense: (a)
ti
−2
0
1
3
yi
0
1
2
5
(b)
ti
1
2
3
4
5
yi
1
0
−2
−3
−3
(c)
ti
−2
−1
0
1
2
yi
−5
−3
−2
0
3
5.5.2. The proprietor of an internet travel company compiled the following data relating the
annual proﬁt of the ﬁrm to its annual advertising expenditure (both measured in thousands
of dollars):
Annual advertising
expenditure
12
14
17
21
26
30
Annual proﬁt
60
70
90
100
100
120
(a) Determine the equation of the least squares line. (b) Plot the data and the least
squares line. (c) Estimate the proﬁt when the annual advertising budget is $50,000.
(d) What about a $100,000 budget?
5.5.3. The median price (in thousands of dollars) of existing homes in a certain metropolitan
area from 1989 to 1999 was:
year
1989
1990
1991
1992
1993
1994
1995
1996
1997
1998
1999
price
86.4
89.8
92.8
96.0
99.6
103.1
106.3
109.5
113.3
120.0
129.5
(a) Find an equation of the least squares line for these data.
(b) Estimate the median
price of a house in the year 2005, and the year 2010, assuming that the trend continues.
5.5.4. A 20-pound turkey that is at the room temperature of 72◦is placed in the oven at 1:00
pm. The temperature of the turkey is observed in 20 minute intervals to be 79◦, 88◦, and
96◦. A turkey is cooked when its temperature reaches 165◦. How much longer do you need
to wait until the turkey is done?
♥5.5.5. The amount of waste (in millions of tons a day) generated in a certain city from 1960 to
1995 was
year
1960
1965
1970
1975
1980
1985
1990
1995
amount
86
99.8
115.8
125
132.6
143.1
156.3
169.5
(a) Find the equation for the least squares line that best ﬁts these data.
(b) Use the result to estimate the amount of waste in the year 2000, and in the year 2005.
(c) Redo your calculations using an exponential growth model y = ceαt.
(d) Which model do you think most accurately reﬂects the data? Why?

5.5 Data Fitting and Interpolation
259
5.5.6. The amount of radium-224 in a sample was measured at the indicated times.
time in days
0
1
2
3
4
5
6
7
mg
100
82.7
68.3
56.5
46.7
38.6
31.9
26.4
(a) Estimate how much radium will be left after 10 days.
(b) If the sample is considered to be safe when the amount of radium is less than .01 mg,
estimate how long the sample needs to be stored before it can be safely disposed of.
5.5.7. The following table gives the population of the United States for the years 1900-2000.
year
1900
1920
1940
1960
1980
2000
population −inmillions
76
106
132
181
227
282
(a) Use an exponential growth model of the form y = ceat to predict the population
in 2020, 2050, and 3000. (b) The actual population for the year 2020 has recently been
estimated to be 334 million. How does this aﬀect your predictions for 2050 and 3000?
5.5.8. Find the best linear least squares ﬁt of the following data using the indicated weights:
(a)
ti
1
2
3
4
yi
.2
.4
.7
1.2
ci
1
2
3
4
(b)
ti
0
1
3
6
yi
2
3
7
12
ci
4
3
2
1
(c)
ti
−2
−1
0
1
2
yi
−5
−3
−2
0
3
ci
2
1
.5
1
2
(d)
ti
1
2
3
4
5
yi
2
1.3
1.1
.8
.2
ci
5
4
3
2
1
5.5.9. For the data points
x
1
1
2
2
3
3
y
1
2
1
2
2
4
z
3
6
11
−2
0
3
(a) determine the best plane z = a + bx + cy that best ﬁts the data in the least squares
sense; (b) how would you answer the question in part (a) if the plane were constrained to
go through the point x = 2, y = 2, z = 0?
5.5.10. For the data points in Exercise 5.5.9, determine the plane z = α + β x + γ y that ﬁts the
data in the least squares sense when the errors are weighted according to the reciprocal of
the distance of the point (xi, yi, zi) from the origin.
♦5.5.11. Show, by constructing explicit examples, that t2 ̸= (t )2 and t y ̸= t y. Can you ﬁnd any
data for which either equality is valid?
♦5.5.12. Given points t1, . . . , tm, prove t2 −( t )2 = 1
m
m

i=1
(ti −t )2, thereby justifying (5.45).
Polynomial Approximation and Interpolation
The basic least squares philosophy has a variety of diﬀerent extensions, all interesting and
all useful. First, we can replace the straight line (5.39) by a parabola deﬁned by a quadratic
function
y = α + β t + γ t2.
(5.47)
For example, Newton’s theory of gravitation says that (in the absence of air resistance) a
falling object obeys the parabolic law (5.47), where α = h0 is the initial height, β = v0 is
the initial velocity, and γ = −1
2 g is minus one half the gravitational constant. Suppose
we observe a falling body on a new planet, and measure its height yi at times ti. Then we

260
5 Minimization and Least Squares
Linear
Quadratic
Cubic
Figure 5.6.
Interpolating Polynomials.
can approximate its initial height, initial velocity, and gravitational acceleration by ﬁnding
the parabola (5.47) that best ﬁts the data. Again, we characterize the least squares ﬁt by
minimizing the sum of the squares of the individual errors ei = yi −y(ti).
The method can evidently be extended to a completely general polynomial function
y(t) = α0 + α1 t + · · · + αn tn
(5.48)
of degree n. The total squared error between the data and the sample values of the function
is equal to
∥e∥2 =
m

i=1

yi −y(ti)
2 = ∥y −A x∥2,
(5.49)
where
A =
⎛
⎜
⎜
⎜
⎜
⎝
1
t1
t2
1
. . .
tn
1
1
t2
t2
2
. . .
tn
2
...
...
...
...
...
1
tm
t2
m
. . .
tn
m
⎞
⎟
⎟
⎟
⎟
⎠
,
x =
⎛
⎜
⎜
⎜
⎜
⎝
α0
α1
α2
...
αn
⎞
⎟
⎟
⎟
⎟
⎠
,
y =
⎛
⎜
⎜
⎜
⎝
y1
y2
...
ym
⎞
⎟
⎟
⎟
⎠.
(5.50)
The m × (n + 1) coeﬃcient matrix is known as a Vandermonde matrix, named after the
eighteenth-century French mathematician, scientist, and musician/musicologist Alexandre–
Th´eophile Vandermonde — despite the fact that it appears nowhere in his four mathemat-
ical papers! In particular, if m = n + 1, then A is square, and so, assuming invertibility,
we can solve A x = y exactly. In other words, there is no error, and the solution is an
interpolating polynomial, meaning that it ﬁts the data exactly. A proof of the following
result can be found at the end of this section.
Lemma 5.16. If t1, . . . , tn+1 ∈R are distinct, so ti ̸= tj for i ̸= j, then the (n+1)×(n+1)
Vandermonde interpolation matrix (5.50) is nonsingular.
This result implies the basic existence theorem for interpolating polynomials.
Theorem 5.17. Let t1, . . . , tn+1 be distinct sample points. Then, for any prescribed data
y1, . . . , yn+1, there exists a unique interpolating polynomial y(t) of degree ≤n that has
the prescribed sample values: y(ti) = yi for all i = 1, . . ., n + 1.
Thus, in particular, two points will determine a unique interpolating line, three points
a unique interpolating parabola, four points an interpolating cubic, and so on, as sketched
in Figure 5.6.
The basic ideas of interpolation and least squares ﬁtting of data can be applied to
approximate complicated mathematical functions by much simpler polynomials. Such ap-
proximation schemes are used in all numerical computations. Your computer or calculator

5.5 Data Fitting and Interpolation
261
is only able to add, subtract, multiply, and divide. Thus, when you ask it to compute
√
t
or et or cos t or any other non-rational function, the program must rely on an approxima-
tion scheme based on polynomials†. In the “dark ages” before electronic computers‡, one
would consult precomputed tables of values of the function at particular data points. If
one needed a value at a non-tabulated point, then some form of polynomial interpolation
would be used to approximate the intermediate value.
Example 5.18.
Suppose that we would like to compute reasonably accurate values for
the exponential function et for values of t lying in the interval 0 ≤t ≤1 by approximating
it by a quadratic polynomial
p(t) = α + β t + γ t2.
(5.51)
If we choose 3 points, say t1 = 0, t2 = .5, t3 = 1, then there is a unique quadratic
polynomial (5.51) that interpolates et at the data points, i.e., p(ti) = eti for i = 1, 2, 3. In
this case, the coeﬃcient matrix (5.50), namely A =
⎛
⎝
1
0
0
1
.5
.25
1
1
1
⎞
⎠, is nonsingular, so we
can exactly solve the interpolation equations
A x = y,
where
y =
⎛
⎜
⎝
et1
et2
et3
⎞
⎟
⎠=
⎛
⎜
⎝
1.
1.64872
2.71828
⎞
⎟
⎠
is the data vector, which we assume we already know. The solution
x =
⎛
⎜
⎝
α
β
γ
⎞
⎟
⎠=
⎛
⎜
⎝
1.
.876603
.841679
⎞
⎟
⎠
yields the interpolating polynomial
p(t) = 1 + .876603 t + .841679 t2.
(5.52)
It is the unique quadratic polynomial that agrees with et at the three speciﬁed data points.
See Figure 5.7 for a comparison of the graphs; the ﬁrst graph shows et, the second p(t),
and the third lays the two graphs on top of each other.
Even with such a primitive
interpolation scheme, the two functions are quite close. The maximum error, or L∞norm,
of the diﬀerence is
∥et −p(t)∥∞= max

| et −p(t) |
 0 ≤t ≤1

≈.01442,
with the largest deviation occurring at t ≈.796.
There is, in fact, an explicit formula for the interpolating polynomial that is named after
the inﬂuential eighteenth century Italian–French mathematician Joseph-Louis Lagrange. It
relies on the basic superposition principle for solving inhomogeneous systems that we found
in Theorem 2.44. Speciﬁcally, suppose we know the solutions x1, . . . , xn+1 to the particular
interpolation systems
A xk = ek,
k = 1, . . ., n + 1,
(5.53)
†
Actually, since division also is possible, one could also allow interpolation and approximation
by rational functions, a subject known as Pad´e approximation theory, [3].
‡
Back then, the word “computer” referred to a human who computed, mostly female and
including the ﬁrst author’s mother, Grace Olver.

262
5 Minimization and Least Squares
Figure 5.7.
Quadratic Interpolating Polynomial for et.
where e1, . . . , en+1 are the standard basis vectors of Rn+1. Then the solution to
A x = y = y1 e1 + · · · + yn+1 en+1
is given by the superposition formula
x = y1 x1 + · · · + yn+1 xn+1.
The particular interpolation equation (5.53) corresponds to the interpolation data y = ek,
meaning that yk = 1, while yi = 0 at all points ti with i ̸= k.
If we can ﬁnd the
n + 1 particular interpolating polynomials that realize this very special data, we can use
superposition to construct the general interpolating polynomial.
Theorem 5.19. Given distinct sample points t1, . . . , tn+1, the kth Lagrange interpolating
polynomial is given by
Lk(t) =
(t −t1) · · · (t −tk−1)(t −tk+1) · · · (t −tn+1)
(tk −t1) · · · (tk −tk−1)(tk −tk+1) · · · (tk −tn+1) ,
k = 1, . . ., n + 1.
(5.54)
It is the unique polynomial of degree n that satisﬁes
Lk(ti) =
(
1,
i = k,
0,
i ̸= k,
i, k = 1, . . . , n + 1.
(5.55)
Proof : The uniqueness of the Lagrange interpolating polynomial is an immediate conse-
quence of Theorem 5.17. To show that (5.54) is the correct formula, we note that when
t = ti for any i ̸= k, the factor (t −ti) in the numerator of Lk(t) vanishes, while the
denominator is not zero, since the points are distinct: ti ̸= tk for i ̸= k. On the other hand,
when t = tk, the numerator and denominator are equal, and so Lk(tk) = 1.
Q.E.D.
Theorem 5.20. If t1, . . . , tn+1 are distinct, then the polynomial of degree ≤n that inter-
polates the associated data y1, . . . , yn+1 is
p(t) = y1L1(t) + · · · + yn+1 Ln+1(t).
(5.56)
Proof : We merely compute
p(tk) = y1L1(tk) + · · · + yk Lk(tk) + · · · + yn+1 Ln+1(tk) = yk,
where, according to (5.55), every summand except the kth is zero.
Q.E.D.

5.5 Data Fitting and Interpolation
263
L1(t)
L2(t)
L3(t)
Figure 5.8.
Lagrange Interpolating Polynomials for the Points 0, .5, 1.
Example 5.21.
For example, the three quadratic Lagrange interpolating polynomials
for the values t1 = 0, t2 = 1
2, t3 = 1, used to interpolate et in Example 5.18 are
L1(t) =

t −1
2

(t −1)

0 −1
2

(0 −1)
= 2t2 −3t + 1,
L2(t) =
(t −0)(t −1)
 1
2 −0
 1
2 −1
 = −4t2 + 4t,
L3(t) =
(t −0)

t −1
2

(1 −0)

1 −1
2
 = 2t2 −t.
(5.57)
Thus, we can rewrite the quadratic interpolant (5.52) to et as
y(t) = L1(t) + e1/2 L2(t) + e L3(t)
= (2t2 −3t + 1) + 1.64872 (−4t2 + 4t) + 2.71828 (2t2 −t).
We stress that this is the same interpolating polynomial — we have merely rewritten it in
the alternative Lagrange form.
You might expect that the higher the degree, the more accurate the interpolating poly-
nomial.
This expectation turns out, unfortunately, not to be uniformly valid.
While
low-degree interpolating polynomials are usually reasonable approximants to functions,
not only are high-degree interpolants more expensive to compute, but they can be rather
badly behaved, particularly near the ends of the interval. For example, Figure 5.9 displays
the degree 2, 4, and 10 interpolating polynomials for the function 1/(1 + t2) on the interval
−3 ≤t ≤3 using equally spaced data points. Note the rather poor approximation of the
function near the ends of the interval. Higher degree interpolants fare even worse, although
the bad behavior becomes more and more concentrated near the endpoints. (Interestingly,
this behavior is a consequence of the positions of the complex singularities of the function
being interpolated, [62].) As a consequence, high-degree polynomial interpolation tends
not to be used in practical applications. Better alternatives rely on least squares approx-
imants by low-degree polynomials, to be described next, and interpolation by piecewise
cubic splines, to be discussed at the end of this section.
If we have m > n + 1 data points, then, usually, there is no degree n polynomial that
ﬁts all the data, and so we must switch over to a least squares approximation. The ﬁrst

264
5 Minimization and Least Squares
Figure 5.9.
Degree 2, 4 and 10 Interpolating Polynomials for 1/(1 + t2).
requirement is that the associated m × (n+ 1) interpolation matrix (5.50) have rank n+ 1;
this follows from Lemma 5.16 (coupled with Exercise 2.5.43), provided that at least n + 1
of the values t1, . . . , tm are distinct. Thus, given data at m ≥n + 1 distinct sample points
t1, . . . , tm, we can uniquely determine the best least squares polynomial of degree n that
best ﬁts the data by solving the normal equations (5.42).
Example 5.22.
Let us return to the problem of approximating the exponential function
et. If we use more than three data points, but still require a quadratic polynomial, then
we can no longer interpolate exactly, and must devise a least squares approximant. For
instance, using ﬁve equally spaced sample points
t1 = 0,
t2 = .25,
t3 = .5,
t4 = .75,
t5 = 1,
the coeﬃcient matrix and sampled data vector (5.50) are
A =
⎛
⎜
⎜
⎜
⎜
⎝
1
t1
t2
1
1
t2
t2
2
1
t3
t2
3
1
t4
t2
4
1
t5
t2
5
⎞
⎟
⎟
⎟
⎟
⎠
=
⎛
⎜
⎜
⎜
⎝
1
0
0
1
.25
.0625
1
.5
.25
1
.75
.5625
1
1
1
⎞
⎟
⎟
⎟
⎠,
y =
⎛
⎜
⎜
⎜
⎝
et1
et2
et3
et4
et5
⎞
⎟
⎟
⎟
⎠=
⎛
⎜
⎜
⎜
⎝
1.
1.28403
1.64872
2.11700
2.71828
⎞
⎟
⎟
⎟
⎠.
The solution to the normal equations (5.27), with
K = ATA =
⎛
⎝
5.
2.5
1.875
2.5
1.875
1.5625
1.875
1.5625
1.38281
⎞
⎠,
f = AT y =
⎛
⎝
8.76803
5.45140
4.40153
⎞
⎠,
is
x = K−1f = ( 1.00514, .864277, .843538 )T .
This leads to the quadratic least squares approximant
p2(t) = 1.00514 + .864277 t + .843538 t2.
On the other hand, the quartic interpolating polynomial
p4(t) = 1 + .998803 t + .509787 t2 + .140276 t3 + .069416 t4
is found directly from the data values as above. The quadratic polynomial has a maximal
error of ≈.011 over the interval [0, 1] — slightly better than our previous quadratic
interpolant — while the quartic has a signiﬁcantly smaller maximal error: ≈.0000527.

5.5 Data Fitting and Interpolation
265
Figure 5.10.
Quadratic Approximant and Quartic Interpolant for et.
(In this case, high-degree interpolants are not ill behaved.) See Figure 5.10 for a comparison
of the graphs, and Example 5.24 below for further discussion.
As noted above, the required calculations can be signiﬁcantly simpliﬁed by the intro-
duction of an orthogonal basis of the least squares subspace. Let us see how this works in
the present situation. Given sample points t1, . . . , tm, let
tk =

tk
1, tk
2, . . . , tk
m
T ,
k = 0, 1, 2, . . . ,
be the vector obtained by sampling the monomial tk. More generally, sampling a polyno-
mial, i.e., a linear combination of monomials
y = p(t) = α0 + α1 t + · · · + αn tn
(5.58)
results in the selfsame linear combination
p = ( p(t1), . . . , p(tn) )T = α0 t0 + α1 t1 + · · · + αn tn
(5.59)
of monomial sample vectors. Thus, all sampled polynomial vectors belong to the subspace
W = span

t0, . . . , tn

⊂Rm spanned by the monomial sample vectors.
Let y = ( y1, y2, . . . , ym )T be data that has been measured at the sample points. The
polynomial least squares approximation is, by deﬁnition, the polynomial y = p(t) whose
sample vector p is the closest point to y lying in the subspace W, which, according to
Theorem 5.7, is the same as the orthogonal projection of the data vector y onto W.
But the monomial sample vectors t0, . . . , tn are not orthogonal, and so a direct approach
requires solving the normal equations (5.42) for the least squares coeﬃcients α0, . . . , αn.
An better strategy is to ﬁrst apply the Gram–Schmidt process to construct an orthogonal
basis for the subspace W, from which the least squares coeﬃcients are then found by our
orthogonal projection formula (4.41). Let us adopt the rescaled version
⟨v , w ⟩= 1
m
m

i=1
vi wi = v w
(5.60)
of the standard dot product† on Rm. If v, w represent the sample vectors corresponding
to the functions v(t), w(t), then their inner product ⟨v , w ⟩is equal to the average value
†
For weighted least squares, we would use an appropriately weighted inner product.

266
5 Minimization and Least Squares
of the product function v(t) w(t) on the m sample points. In particular, the inner product
between our “monomial” basis vectors corresponding to sampling tk and tl is
⟨tk , tl ⟩= 1
m
m

i=1
tk
i tl
i = 1
m
m

i=1
tk+l
i
= tk+l,
(5.61)
which is the averaged sample value of the monomial tk+l.
To keep the formulas reasonably simple, let us further assume† that the sample points
are evenly spaced and symmetric about 0. The ﬁrst requirement is that ti −ti−1 = h be
independent of i, while the second means that if ti is a sample point, then so is −ti. An
example is provided by the seven sample points −3, −2, −1, 0, 1, 2, 3. As a consequence of
these two assumptions, the averaged sample values of the odd powers of t vanish: t2i+1 = 0.
Hence, by (5.61), the sample vectors tk and tl are orthogonal whenever k + l is odd.
Applying the Gram–Schmidt algorithm to t0, t1, t2, . . . produces the orthogonal basis
vectors q0, q1, q2, . . . . Each
qk = ( qk(t1), . . . , qk(tm) )T = ck0 t0 + ck1 t1 + · · · + ckk tk
(5.62)
can be interpreted as the sample vector for a certain degree k interpolating polynomial
qk(t) = ck0 + ck1 t + · · · + ckk tk.
Under these assumptions, the ﬁrst few of these polynomials, along with their corresponding
orthogonal sample vectors, are as follows:
q0(t) = 1,
q0 = t0,
∥q0∥2 = 1,
q1(t) = t,
q1 = t1,
∥q1∥2 = t2,
q2(t) = t2 −t2 ,
q2 = t2 −t2 t0,
∥q2∥2 = t4 −

t2 2,
q3(t) = t3 −t4
t2 t ,
q3 = t3 −t4
t2 t1,
∥q3∥2 = t6 −

t4 2
t2
.
(5.63)
With these in hand, the least squares approximating polynomial of degree n to the given
data vector y is given by a linear combination
p(t) = a0 q0(t) + a1 q1(t) + a2 q2(t) + · · · + an qn(t).
(5.64)
The coeﬃcients can now be obtained directly through the orthogonality formula (4.42),
and so
ak = ⟨qk , y ⟩
∥qk∥2
= qk y
q2
k
.
(5.65)
Thus, once we have set up the orthogonal basis, we no longer need to solve any linear
system to construct the least squares approximation.
An additional advantage of the orthogonal basis is that, in contrast to the direct method,
the formulas (5.65) for the least squares coeﬃcients do not depend on the degree of the ap-
proximating polynomial. As a result, one can readily increase the degree, and, in favorable
†
The method works without this restriction, but the formulas become more unwieldy. See
Exercise 5.5.31 for details.

5.5 Data Fitting and Interpolation
267
Linear
Quadratic
Cubic
Figure 5.11.
Least Squares Data Approximations.
situations, the accuracy of the approximant without having to recompute any of the lower-
degree terms. For instance, if a quadratic polynomial p2(t) = a0 + a1 q1(t) + a2 q2(t) is
insuﬃciently accurate, the cubic least squares approximant p3(t) = p2(t) + a3 q3(t) can be
constructed without having to recompute the quadratic coeﬃcients a0, a1, a2. This doesn’t
work when using the non-orthogonal monomials, all of whose coeﬃcients will be aﬀected
by increasing the degree of the approximating polynomial.
Example 5.23.
Consider the following tabulated sample values:
ti
−3
−2
−1
0
1
2
3
yi
−1.4
−1.3
−.6
.1
.9
1.8
2.9
To compute polynomial least squares ﬁts of degrees 1, 2 and 3, we begin by computing the
polynomials (5.63), which for the given sample points ti are
q0(t) = 1,
q1(t) = t,
q2(t) = t2 −4,
q3(t) = t3 −7t ,
∥q0∥2 = 1,
∥q1∥2 = 4,
∥q2∥2 = 12,
∥q3∥2 = 216
7 .
Thus, to four decimal places, the coeﬃcients (5.64) for the least squares approximation are
a0 = ⟨q0 , y ⟩= .3429,
a1 = 1
4 ⟨q1 , y ⟩= .7357,
a2 =
1
12 ⟨q2 , y ⟩= .0738,
a3 =
7
216 ⟨q3 , y ⟩= −.0083.
To obtain the best linear approximation, we use
p1(t) = a0 q0(t) + a1 q1(t) = .3429 + .7357 t,
with a least squares error of .7081. Similarly, the quadratic and cubic least squares ap-
proximations are
p2(t) = .3429 + .7357 t + .0738 (t2 −4),
p3(t) = .3429 + .7357 t + .0738 (t2 −4) −.0083 (t3 −7t),
with respective least squares errors .2093 and .1697 at the sample points. Observe that,
as noted above, the lower order coeﬃcients do not change as we increase the degree of the
approximating polynomial. A plot of the ﬁrst three approximations appears in Figure 5.11.

268
5 Minimization and Least Squares
The small cubic term does not signiﬁcantly increase the accuracy of the approximation,
and so this data probably comes from sampling a quadratic function.
Proof of Lemma 5.16:
We will establish the rather striking LU factorization of the trans-
posed Vandermonde matrix V = AT , which will immediately prove that, when t1, . . . , tn+1
are distinct, both V and A are nonsingular matrices. The 4 × 4 case is instructive for
understanding the general pattern. Applying regular Gaussian Elimination, we obtain the
explicit LU factorization
⎛
⎜
⎜
⎜
⎝
1
1
1
1
t1
t2
t3
t4
t2
1
t2
2
t2
3
t2
4
t3
1
t3
2
t3
3
t3
4
⎞
⎟
⎟
⎟
⎠=
⎛
⎜
⎜
⎜
⎝
1
0
0
0
t1
1
0
0
t2
1
t1 + t2
1
0
t3
1
t2
1 + t1 t2 + t2
2
t1 + t2 + t3
1
⎞
⎟
⎟
⎟
⎠
⎛
⎜
⎜
⎜
⎝
1
1
1
1
0
t2 −t1
t3 −t1
t4 −t1
0
0
(t3 −t1)(t3 −t2)
(t4 −t1)(t4 −t2)
0
0
0
(t4 −t1)(t4 −t2)(t4 −t3)
⎞
⎟
⎟
⎟
⎠.
Observe that the pivots, i.e., the diagonal entries of U, are all nonzero if the points
t1, t2, t3, t4 are distinct. The reader may be able to spot the pattern in the above formula
and thus guess the general case. Indeed, the individual entries of the matrices appearing
in the factorization
V = LU
(5.66)
of the (n + 1) × (n + 1) Vandermonde matrix are
vij = ti−1
j
,
i, j = 1, . . ., n + 1,
(5.67)
ℓij =

1≤k1≤···≤ki−j≤j
tk1 tk2 · · · tki−j,
1 ≤j < i ≤n + 1,
ℓii = 1,
i = 1, . . . , n + 1,
ℓij = 0,
1 ≤i < j ≤n + 1,
uij =
i
k=1
(tj −tk),
1 < i ≤j ≤n + 1,
u1j = 1,
j = 1, . . ., n + 1,
uij = 0,
1 ≤j < i ≤n + 1.
Full details of the proof that (5.67) holds can be found in [30, 63]. (Surprisingly, as far
as we know, these are the ﬁrst places this factorization appears in the literature.) The
entries of L lying below the diagonal are known as the complete monomial polynomials
since ℓij is obtained by summing, with unit coeﬃcients, all monomials of degree i −j in
the j variables t1, . . . , tj. The entries of U appearing on or above the diagonal are known
as the Newton diﬀerence polynomials. In particular, if t1, . . . , tn are distinct, so ti ̸= tj for
i ̸= j, then all entries of U lying on or above the diagonal are nonzero. In this case, V has
all nonzero pivots, namely, the diagonal entries of U, and is a regular, hence nonsingular
matrix.
Q.E.D.
Exercises
5.5.13. Find and graph the polynomial of minimal degree that passes through the following
points:
(a) (3, −1), (6, 5); (b) (−2, 4), (0, 6), (1, 10); (c) (−2, 3), (0, −1), (1, −3);
(d) (−1, 2), (0, −1), (1, 0), (2, −1); (e) (−2, 17), (−1, −3), (0, −3), (1, −1), (2, 9).

5.5 Data Fitting and Interpolation
269
5.5.14. For the following data values, construct the interpolating polynomial in Lagrange form:
(a)
ti
−3
2
yi
1
5
(b)
ti
0
1
3
yi
1
.5
.25
(c)
ti
−1
0
1
yi
1
2
−1
(d)
ti
0
1
2
3
yi
0
1
4
9
(e)
ti
−2
−1
0
1
2
yi
−1
−2
2
1
3
5.5.15. Given
ti
1
2
3
yi
3
6
11
(a) ﬁnd the straight line y = α + β t that best ﬁts the
data in the least squares sense; (b) ﬁnd the parabola y = α + β t + γ t2 that best ﬁts the
data. Interpret the error.
5.5.16. Re-solve Exercise 5.5.15 using the respective weights 2, 1, .5 at the three data points.
5.5.17. The table
time in sec
0
10
20
30
meters
4500
4300
3930
3000
measures the altitude
of a falling parachutist before her chute has opened. Predict how many seconds she can
wait before reaching the minimum altitude of 1500 meters.
5.5.18. A missile is launched in your direction. Using a range ﬁnder, you measure its altitude
at the times:
time in sec
0
10
20
30
40
50
altitude in meters
200
650
970
1200
1375
1130
How long until you have to run?
5.5.19. A student runs an experiment six times in an attempt to obtain an equation relating
two physical quantities x and y. For x = 1, 2, 4, 6, 8, 10 units, the experiments result in
corresponding y values of 3, 3, 4, 6, 7, 8 units. Find and graph the following: (a) the least
squares line; (b) the least squares quadratic polynomial; (c) the interpolating polynomial.
(d) Which do you think is the most likely theoretical model for this data?
5.5.20.(a) Write down the Taylor polynomials of degrees 2 and 4 at t = 0 for the function
f(t) = et.
(b) Compare their accuracy with the interpolating and least squares
polynomials in Examples 5.18 and 5.22.
♥5.5.21. Given the values of sin t at t = 0◦, 30◦, 45◦, 60◦, ﬁnd the following approximations:
(a) the least squares linear polynomial; (b) the least squares quadratic polynomial; (c) the
quadratic Taylor polynomial at t = 0; (d) the interpolating polynomial; (e) the cubic
Taylor polynomial at t = 0; (f ) Graph each approximation and discuss its accuracy.
5.5.22. Find the quartic (degree 4) polynomial that exactly interpolates the function tan t at
the ﬁve data points t0 = 0, t1 = .25, t2 = .5, t3 = .75, t4 = 1. Compare the graphs of the
two functions over 0 ≤t ≤1
2 π.
5.5.23.(a) Find the least squares linear polynomial approximating
√
t on [0, 1], choosing six
diﬀerent exact data values.
(b) How much more accurate is the least squares quadratic
polynomial based on the same data?
5.5.24. A table of logarithms contains the following entries:
t
1.0
2.0
3.0
4.0
log10 t
0
.3010
.4771
.6021
Approximate log10 e by constructing an interpolating polynomial of (a) degree two using
the entries at x = 1.0, 2.0, and 3.0, (b) degree three using all the entries.

270
5 Minimization and Least Squares
♦5.5.25. Let q(t) denote the quadratic interpolating polynomial that goes through the data
points (t0, y0), (t1, y1), (t2, y2). (a) Under what conditions does q(t) have a minimum? A
maximum? (b) Show that the minimizing/maximizing value is at t⋆= m0 s1 −m1 s0
s1 −s0
,
where s0 = y1 −y0
t1 −t0
, s1 = y2 −y1
t2 −t1
, m0 = t0 + t1
2
, m1 = t1 + t2
2
. (c) What is q(t⋆)?
5.5.26. Use the orthogonal sample vectors (5.63) to ﬁnd the best polynomial least squares ﬁts
of degree 1, 2 and 3 for the following sets of data:
(a)
ti
−2
−1
0
1
2
yi
7
11
13
18
21
(b)
ti
−3
−2
−1
0
1
2
3
yi
−2.7
−2.1
−.5
.5
1.2
2.4
3.2
(c)
ti
−3
−2
−1
0
1
2
3
yi
60
80
90
100
120
120
130
♦5.5.27.(a) Verify the orthogonality of the sample polynomial vectors in (5.63). (b) Construct
the next orthogonal sample polynomial q4(t) and the norm of its sample vector.
(c) Use your result to compute the quartic least squares approximation for the data
in Example 5.23.
5.5.28. Use the result of Exercise 5.5.27 to ﬁnd the best approximating polynomial of degree 4
to the data in Exercise 5.5.26.
5.5.29. Justify the fact that the orthogonal sample vector qk in (5.62) is a linear combination
of only the ﬁrst k monomial sample vectors.
♥5.5.30. The formulas (5.63) apply only when the sample times are symmetric around 0. When
the sample points t1, . . . , tn are equally spaced, so ti+1 −ti = h for all i = 1, . . . , n −1,
then there is a simple trick to convert the least squares problem into a symmetric form.
(a) Show that the translated sample points si = ti −t, where t = 1
n
 n
i=1 ti is the average,
are symmetric around 0.
(b) Suppose q(s) is the least squares polynomial for the data
points (si, yi). Prove that p(t) = q(t −t ) is the least squares polynomial for the original
data (ti, yi). (c) Apply this method to ﬁnd the least squares polynomials of degrees 1 and
2 for the following data:
ti
1
2
3
4
5
6
yi
−8
−6
−4
−1
1
3
♦5.5.31. Construct the ﬁrst three orthogonal basis elements for sample points t1, . . . , tm that are
in general position.
♠5.5.32. Use n + 1 equally spaced data points to interpolate f(t) = 1/(1 + t2) on an interval
−a ≤t ≤a for a = 1, 1.5, 2, 2.5, 3 and n = 2, 4, 10, 20. Do all intervals exhibit the pathology
illustrated in Figure 5.9? If not, how large can a be before the interpolants have poor
approximation properties? What happens when the number of interpolation points is taken
to be n = 50?
♠5.5.33. Repeat Exercise 5.5.32 for the hyperbolic secant function f(t) = sech t = 1/ cosh t.
5.5.34. Given A as in (5.50) with m < n + 1, how would you characterize those polynomials
p(t) whose coeﬃcient vector x lies in ker A?
5.5.35.(a) Give an example of an interpolating polynomial through n + 1 points that has degree
< n. (b) Can you explain, without referring to the explicit formulas, why all the Lagrange
interpolating polynomials based on n + 1 points must have degree equal to n?

5.5 Data Fitting and Interpolation
271
5.5.36. Let x1, . . . , xn be distinct real numbers. Prove that the n × n matrix K with entries
kij =
1 −(xixj)n
1 −xixj
is positive deﬁnite.
♦5.5.37. Prove the determinant formula det A =
'
1≤i<j≤n+1 (ti −tj) for the (n + 1) × (n + 1)
Vandermonde matrix deﬁned in (5.50).
♦5.5.38.(a) Prove that a polynomial p(x) = a0 + a1 x + a2 x2 + · · · + an xn of degree ≤n vanishes
at n + 1 distinct points, so p(x1) = p(x2) = · · · = p(xn+1) = 0, if and only if p(x) ≡0 is the
zero polynomial. (b) Prove that the monomials 1, x, x2, . . . , xn are linearly independent.
(c) Explain why p(x) ≡0 if and only if all its coeﬃcients a0 = a1 = · · · = an = 0.
Hint: Use Lemma 5.16 and Exercise 2.3.37.
♥5.5.39. Numerical diﬀerentiation: The most common numerical methods for approximating
the derivatives of a function are based on interpolation. To approximate the kth derivative
f(k)(x0) at a point x0, one replaces the function f(x) by an interpolating polynomial pn(x)
of degree n ≥k based on the nearby points x0, . . . , xn (the point x0 is almost always
included as an interpolation point), leading to the approximation f(k)(x0) ≈p(k)
n (x0).
Use this method to construct numerical approximations to (a) f′(x) using a quadratic
interpolating polynomial based on x −h, x, x + h. (b) f′′(x) with the same quadratic
polynomial. (c) f′(x) using a quadratic interpolating polynomial based on x, x + h, x + 2h.
(d) f′(x), f′′(x), f′′′(x) and f(iv)(x) using a quartic interpolating polynomial based on
x −2h, x −h, x, x + h, x + 2h. (e) Test your methods by approximating the derivatives of
ex and tan x at x = 0 with step sizes h =
1
10,
1
100,
1
1000,
1
10000. Discuss the accuracies you
observe. Can the step size be arbitrarily small?
(f ) Why do you need n ≥k?
♥5.5.40. Numerical integration: Most numerical methods for evaluating a deﬁnite integral
 b
a f(x) dx are based on interpolation. One chooses n + 1 interpolation points a ≤x0 <
x1 < · · · < xn ≤b and replaces the integrand by its interpolating polynomial pn(x) of
degree n, leading to the approximation
 b
a f(x) dx ≈
 b
a pn(x) dx, where the polynomial
integral can be done explicitly. Write down the following popular integration rules:
(a) Trapezoid Rule: x0 = a, x1 = b. (b) Simpson’s Rule: x0 = a, x1 = 1
2(a + b), x2 = b.
(c) Simpson’s 3
8 Rule: x0 = a, x1 = 1
3(a + b), x2 = 2
3(a + b), x3 = b.
(d) Midpoint Rule: x0 = 1
2(a + b). (e) Open Rule: x0 = 1
3(a + b), x1 = 2
3(a + b).
(f ) Test your methods for accuracy on the following integrals:
(i)
 1
0 ex dx,
(ii)
 π
0 sin x dx,,
(iii)
 e
1 log x dx,
(iv)
 π/2
0

x3 + 1 dx.
Note: For more details on numerical diﬀerentiation and integration, you are encouraged to
consult a basic numerical analysis text, e.g., [8].
Approximation and Interpolation by General Functions
There is nothing special about polynomial functions in the preceding approximation and
interpolation schemes. For example, suppose we are interested in determining the best
trigonometric approximation
y = α1 cos t + α2 sin t
to a given set of data. Again, the least squares error takes the same form as in (5.49),

272
5 Minimization and Least Squares
namely ∥e∥2 = ∥y −A x∥2, where
A =
⎛
⎜
⎜
⎝
cos t1
sin t1
cos t2
sin t2
...
...
cos tm
sin tm
⎞
⎟
⎟
⎠,
x =

α1
α2

,
y =
⎛
⎜
⎜
⎜
⎝
y1
y2
...
ym
⎞
⎟
⎟
⎟
⎠.
Thus, the columns of A are the sampled values of the functions cos t, sin t. The key re-
quirement is that the unspeciﬁed parameters — in this case α1, α2 — occur linearly in the
approximating function. Thus, the most general case is to approximate the data (5.38) by
a linear combination
y(t) = α1 h1(t) + α2 h2(t) + · · · + αn hn(t)
of prescribed functions h1(x), . . . , hn(x). The total squared error is, as always, given by
Squared Error =
m

i=1

yi −y(ti)
2 = ∥y −A x∥2,
where the sample matrix A, the vector of unknown coeﬃcients x, and the data vector y
are
A =
⎛
⎜
⎜
⎜
⎜
⎝
h1(t1)
h2(t1)
. . .
hn(t1)
h1(t2)
h2(t2)
. . .
hn(t2)
...
...
...
...
h1(tm)
h2(tm)
. . .
hn(tm)
⎞
⎟
⎟
⎟
⎟
⎠
,
x =
⎛
⎜
⎜
⎜
⎝
α1
α2
...
αn
⎞
⎟
⎟
⎟
⎠,
y =
⎛
⎜
⎜
⎜
⎝
y1
y2
...
ym
⎞
⎟
⎟
⎟
⎠.
(5.68)
If A is square and nonsingular, then we can ﬁnd an interpolating function of the prescribed
form by solving the linear system
A x = y.
(5.69)
A particularly important case is provided by the 2n + 1 trigonometric functions
1,
cos x,
sin x,
cos 2x,
sin 2x,
. . .
cos nx,
sin nx.
Interpolation on 2n + 1 equally spaced data points on the interval [0, 2π ] leads to the
Discrete Fourier Transform, used in signal processing, data transmission, and compression,
and to be the focus of Section 5.6.
If there are more than n data points, then we cannot, in general, interpolate exactly,
and must content ourselves with a least squares approximation that minimizes the error at
the sample points as best it can. The least squares solution to the interpolation equations
(5.69) is found by solving the associated normal equations K x = f, where the (i, j) entry
of K = ATA is m times the average sample value of the product of hi(t) and hj(t), namely
kij = m hi(t) hj(t) =
m

ℓ=1
hi(tℓ) hj(tℓ),
(5.70)
whereas the ith entry of f = AT y is
fi = m hi(t) y =
m

ℓ=1
hi(tℓ) yℓ.
(5.71)

5.5 Data Fitting and Interpolation
273
The one issue is whether the columns of the sample matrix A are linearly independent,
which is a more subtle issue than the polynomial case covered by Lemma 5.16. Linear
independence of the sampled function vectors is, in general, more restrictive than merely
requiring the functions themselves to be linearly independent; see Exercise 2.3.37 for details.
If the parameters do not occur linearly in the functional formula, then we cannot use
linear algebra to eﬀect a least squares approximation. For example, one cannot use least
squares to determine the frequency ω, the amplitude r, and the phase shift δ of the general
trigonometric approximation
y = c1 cos ωt + c2 sin ωt = r cos(ωt + δ)
that minimizes the least squares error at the sample points. Approximating data by such
a function constitutes a nonlinear optimization problem.
Exercises
5.5.41. Given the values
ti
0
.5
1
yi
1
.5
.25
construct the trigonometric function of
the form g(t) = a cos π t+b sin π t that best approximates the data in the least squares sense.
5.5.42. Find the hyperbolic function g(t) = a cosh t + b sinh t that best approximates the data in
Exercise 5.5.41.
5.5.43.(a) Find the exponential function of the form g(t) = a et + b e2t that best approximates
t2 in the least squares sense based on the sample points 0, 1, 2, 3, 4.
(b) What is the
least squares error?
(c) Compare the graphs on the interval [0, 4] — where is the
approximation the worst? (d) How much better can you do by including a constant term
in g(t) = a et + b e2t + c?
5.5.44.(a) Find the best trigonometric approximation of the form g(t) = r cos(t + δ) to t2 using
5 and 9 equally spaced sample points on [0, π ].
(b) Can you answer the question for g(t) = r1 cos(t + δ1) + r2 cos(2t + δ2)?
♥5.5.45. A trigonometric polynomial of degree n is a function of the form
p(t) = a0 + a1 cos t + b1 sin t + a2 cos 2t + b2 sin 2t + · · · + an cos nt + bn sin nt,
where a0, a1, b1, . . . , an, bn are the coeﬃcients. Find the trigonometric polynomial of degree
n that is the least squares approximation to the function f(t) = 1/(1 + t2) on the interval
[−π, π ] based on the k equally spaced data points tj = −π + 2π j
k
, j = 0, . . . , k −1,
(omitting the right-hand endpoint), when (a) n = 1, k = 4, (b) n = 2, k = 8,
(c) n = 2, k = 16, (d) n = 3, k = 16. Compare the graphs of the trigonometric
approximant and the function, and discuss. (e)
Why do we not include the right-hand
endpoint tk = π?
♥5.5.46. The sinc functions are deﬁned as S0(x) = sin(πx/h)
πx/h
, while Sj(x) = S0(x −j h)
whenever h > 0 and j is an integer. We will interpolate a function f(x) at the mesh
points xj = j h, j = 0, . . . , n, by a linear combination of sinc functions: S(x) =
c0 S0(x) + · · · + cn Sn(x). What are the coeﬃcients cj? Graph and discuss the accuracy
of the sinc interpolant for the functions x2 and 1
2 −



 x −1
2



 on the interval [0, 1] using
h = .25, .1, and .025.

274
5 Minimization and Least Squares
Least Squares Approximation in Function Spaces
So far, while we have used least squares minimization to interpolate and approximate
known, complicated functions by simpler polynomials, we have only worried about the
errors committed at a discrete, preassigned set of sample points. A more uniform approach
would be to take into account the errors at all points in the interval of interest. This can
be accomplished by replacing the discrete, ﬁnite-dimensional vector space norm on sample
vectors by a continuous, inﬁnite-dimensional function space norm in order to specify the
least squares error that must be minimized over the entire interval.
More precisely, we let V = C0[a, b] denote the space of continuous functions on the
bounded interval [a, b] with L2 inner product and norm
⟨f , g ⟩=
 b
a
f(t) g(t) dt,
∥f ∥=
  b
a
f(t)2 dt .
(5.72)
Let P(n) ⊂C0[a, b] denote the subspace consisting of all polynomials of degree ≤n. For
simplicity, we employ the standard monomial basis 1, t, t2, . . . , tn. We will be approximating
a general function f(t) ∈C0[a, b] by a polynomial
p(t) = α0 + α1 t + · · · + αn tn ∈P(n)
(5.73)
of degree at most n. The error function e(t) = f(t)−p(t) measures the discrepancy between
the function and its approximating polynomial at each t. Instead of summing the squares
of the errors at a ﬁnite set of sample points, we go to a continuous limit that sums or,
rather, integrates the squared errors of all points in the interval. Thus, the approximating
polynomial will be characterized as the one that minimizes the total L2 squared error:
Squared Error = ∥e∥2 = ∥p −f ∥2 =
 b
a
[ p(t) −f(t) ]2 dt.
(5.74)
To solve the problem of minimizing the squared error, we begin by substituting (5.73)
into (5.74) and expanding, as in (5.20):
∥p −f ∥2 =
&&&&&
n

i=0
αi ti −f(t)
&&&&&
2
=
/
n

i=0
αi ti −f(t) ,
n

i=0
αi ti −f(t)
0
=
n

i,j =0
αi αj ⟨ti , tj ⟩−2
n

i=0
αi ⟨ti , f(t) ⟩+ ∥f(t)∥2.
As a result, we are required to minimize a quadratic function of the standard form
xT K x −2 xTf + c,
(5.75)
where x = ( α0, α1, . . . , αn )T is the vector containing the unknown coeﬃcients in the
minimizing polynomial (5.73), while†
kij = ⟨ti , tj ⟩=
 b
a
ti+j dt,
fi = ⟨ti , f ⟩=
 b
a
tif(t) dt,
(5.76)
†
Here, the indices i, j labeling the entries of the (n + 1) × (n + 1) matrix K and vectors
x, f ∈Rn+1 range from 0 to n instead of 1 to n + 1.

5.5 Data Fitting and Interpolation
275
et
p⋆(t)
Comparison
Figure 5.12.
Quadratic Least Squares Approximation of et.
are the Gram matrix K consisting of inner products between basis monomials along with
the vector f of inner products between the monomials and the given function. The coeﬃ-
cients of the least squares minimizing polynomial are thus found by solving the associated
normal equations K x = f.
Example 5.24.
Let us return to the problem of approximating the exponential func-
tion f(t) = et by a quadratic polynomial on the interval 0 ≤t ≤1, but now with the
least squares error being measured by the L2 norm. Thus, we consider the subspace P(2)
consisting of all quadratic polynomials
p(t) = α + β t + γ t2.
Using the monomial basis 1, t, t2, the coeﬃcient matrix is the Gram matrix K consisting
of the inner products
⟨ti , tj ⟩=
 1
0
ti+j dt =
1
i + j + 1
between basis monomials, while the right-hand side is the vector of inner products
⟨ti , et ⟩=
 1
0
ti et dt.
The solution to the normal system
⎛
⎜
⎝
1
1
2
1
3
1
2
1
3
1
4
1
3
1
4
1
5
⎞
⎟
⎠
⎛
⎜
⎝
α
β
γ
⎞
⎟
⎠=
⎛
⎜
⎝
e −1
1
e −2
⎞
⎟
⎠.
is computed to be
α = 39e −105 ≃1.012991,
β = −216e + 588 ≃.851125,
γ = 210e −570 ≃.839184,
leading to the least squares quadratic approximant
p⋆(t) = 1.012991 + .851125 t + .839184 t2,
(5.77)
plotted in Figure 5.12. The least squares error is
∥et −p⋆(t)∥2 ≃.000027835.

276
5 Minimization and Least Squares
The maximal error over the interval is measured by the L∞norm of the diﬀerence:
∥et −p⋆(t)∥∞= max

| et −p⋆(t) |
 0 ≤t ≤1

≃.014981815,
with the maximum occurring at t = 1.
Thus, the simple quadratic polynomial (5.77)
will give a reasonable approximation to the ﬁrst two decimal places in et on the entire
interval [0, 1].
A more accurate approximation can be constructed by taking a higher
degree polynomial, or by decreasing the length of the interval.
Remark. Although the least squares polynomial (5.77) minimizes the L2 norm of the
error, it does slightly worse with the L∞norm than the previous sample-based minimizer
(5.52). The problem of ﬁnding the quadratic polynomial that minimizes the L∞norm is
more diﬃcult, and must be solved by nonlinear minimization techniques.
Remark. As noted in Example 3.39, the Gram matrix for the simple monomial basis is
the n × n Hilbert matrix (1.72). The ill-conditioned nature of the Hilbert matrix and the
consequential diﬃculty in accurately solving the normal equations complicate the practi-
cal numerical implementation of high-degree least squares polynomial approximations. A
better approach, based on an orthogonal polynomial basis, will be discussed next.
Exercises
5.5.47. Approximate the function f(t) =
3√
t using the least squares method based on the L2
norm on the interval [0, 1] by (a) a straight line; (b) a parabola; (c) a cubic polynomial.
5.5.48. Approximate the function f(t) = 1
8 (2t −1)3 + 1
4 by a quadratic polynomial on the
interval [−1, 1] using the least squares method based on the L2 norm. Compare the graphs.
Where is the error the largest?
5.5.49. For the function f(t) = sin t determine the approximating linear and quadratic
polynomials that minimize the least squares error based on the L2 norm on

0 , 1
2 π

.
5.5.50. Find the quartic (degree 4) polynomial that best approximates the function et on the
interval [0, 1] by minimizing the L2 error (5.72).
♥5.5.51.(a) Find the quadratic interpolant to f(x) = x5 on the interval [0, 1] based on equally
spaced data points. (b) Find the quadratic least squares approximation based on the data
points 0, .25, .5, .75, 1.
(c) Find the quadratic least squares approximation with respect to
the L2 norm.
(d) Discuss the strengths and weaknesses of each approximation.
5.5.52. Let f(x) = x. Find the trigonometric function of the form g(x) = a + b cos x + c sin x
that minimizes the L2 error ∥g −f ∥=
 π
−π [g(x) −f(x)]2 dx .
♦5.5.53. Let g1(t), . . . , gn(t) be prescribed, linearly independent functions. Explain how to best
approximate a function f(t) by a linear combination c1 g1(t) + · · · + cn gn(t) when the
least squares error is measured in a weighted L2 norm ∥f ∥2
w =
 b
a f(t)2 w(t) dt with weight
function w(t) > 0.
5.5.54.(a) Find the quadratic least squares approximation to f(t) = t5 on the interval [0, 1]
with weights (i) w(t) = 1, (ii) w(t) = t, (iii) w(t) = e−t. (b) Compare the errors — which
gives the best result over the entire interval?

5.5 Data Fitting and Interpolation
277
♥5.5.55. Let fa(x) =

a
1 + a4 x2 . Prove that (a) ∥fa ∥2 =

π
a , where ∥·∥2 denotes the L2
norm on (−∞, ∞).
(b) ∥fa ∥∞= √a , where ∥·∥∞denotes the L∞norm on (−∞, ∞).
(c) Use this example to explain why having a small least squares error does not necessarily
mean that the functions are everywhere close.
5.5.56. Find the plane z = α + β x + γ y that best approximates the following functions on the
square S = {0 ≤x ≤1, 0 ≤y ≤1} using the L2 norm ∥f ∥2 =
 
S | f(x, y) |2 dx dy to
measure the least squares error:
(a) x2 + y2, (b) x3 −y3, (c) sin π x sin π y.
5.5.57. Find the radial polynomial p(x, y) = a + br + cr2, where r2 = x2 + y2, that best
approximates the function f(x, y) = x using the L2 norm on the unit disk D = {r ≤1} to
measure the least squares error.
Orthogonal Polynomials and Least Squares
In a similar fashion, the orthogonality of Legendre polynomials and their relatives serves
to simplify the construction of least squares approximants in function space. Suppose, for
instance, that our goal is to approximate the exponential function et by a polynomial on
the interval −1 ≤t ≤1, where the least squares error is measured using the standard L2
norm. We will write the best least squares approximant as a linear combination of the
Legendre polynomials,
p(t) = a0 P0(t) + a1 P1(t) + · · · + an Pn(t) = a0 + a1 t + a2
 3
2 t2 −1
2

+ · · · .
(5.78)
By orthogonality, the least squares coeﬃcients can be immediately computed by the inner
product formula (4.42), so, by the Rodrigues formula (4.61),
ak = ⟨et , Pk ⟩
∥Pk ∥2
= 2k + 1
2
 1
−1
et Pk(t) dt.
(5.79)
For example, the quadratic approximation is given by the ﬁrst three terms in (5.78), whose
coeﬃcients are
a0 = 1
2
 1
−1
et dt = 1
2

e −1
e

≃1.175201,
a1 = 3
2
 1
−1
tet dt = 3
e ≃1.103638,
a2 = 5
2
 1
−1
 3
2 t2 −1
2

et dt = 5
2

e −7
e

≃.357814.
Graphs appear in Figure 5.13; the ﬁrst shows et, the second its quadratic approximant
et ≈1.175201 + 1.103638 t + .357814
 3
2 t2 −1
2

,
(5.80)
and the third compares the two by laying them on top of each other.
As in the discrete case, there are two major advantages of the orthogonal Legendre poly-
nomials over the direct approach presented in Example 5.24. First, we do not need to solve
any linear systems of equations since the required coeﬃcients (5.79) are found by direct in-
tegration. Indeed, the coeﬃcient matrix for polynomial least squares approximation based
on the monomial basis is some variant of the notoriously ill-conditioned Hilbert matrix,
(1.72), and the computation of an accurate solution can be tricky. Our precomputation
of an orthogonal system of polynomials has successfully circumvented the ill-conditioned
normal system.

278
5 Minimization and Least Squares
Figure 5.13.
Quadratic Least Squares Approximation to et.
The second advantage is that the coeﬃcients ak do not depend on the degree of the
approximating polynomial. Thus, if the quadratic approximation (5.80) is insuﬃciently
accurate, we merely append the cubic correction a3 P3(t) whose coeﬃcient is given by
a3 = 7
2
 1
−1
 5
2 t3 −3
2 t

et dt = 7
2

37e −5
e

≃.070456,
Unlike the earlier method, there is no need to recompute the coeﬃcients a0, a1, a2, and
hence the cubic least squares approximant is
et ≈1.175201 + 1.103638 t + .357814
 3
2 t2 −1
2

+ .070456
 5
2 t3 −3
2 t

.
(5.81)
And, if we desire yet further accuracy, we need only compute the next one or two coeﬃ-
cients.
To exploit orthogonality, each interval and norm requires the construction of a corre-
sponding system of orthogonal polynomials.
Let us reconsider Example 5.24, in which
we used the method of least squares to approximate et based on the L2 norm on [0, 1].
Here, the ordinary Legendre polynomials are no longer orthogonal, and so we must use the
rescaled Legendre polynomials (4.74) instead. Thus, the quadratic least squares approxi-
mant can be written as
p(t) = a0 + a1 P 1(t) + a2 P 2(t) = 1.718282 + .845155 (2t −1) + .139864 (6t2 −6t + 1)
= 1.012991 + .851125 t + .839184 t2,
where the coeﬃcients
ak =
⟨et , P k ⟩
∥P k ∥2
= (2k + 1)
 1
0
P k(t) et dt
are found by direct integration:
a0 =
 1
0
et dt = e −1 ≃1.718282,
a1 = 3
 1
0
(2t −1)et dt = 3(3 −e) ≃.845155,
a2 = 5
 1
0
(6t2 −6t + 1)et dt = 5(7e −19) ≃.139864.
It is worth emphasizing that this is the same approximating polynomial we found ear-
lier in (5.77).
The use of an orthogonal system of polynomials merely streamlines the
computation.

5.5 Data Fitting and Interpolation
279
Exercises
5.5.58. Use the Legendre polynomials to ﬁnd the best (a) quadratic, and (b) cubic
approximation to t4, based on the L2 norm on [−1, 1].
5.5.59. Repeat Exercise 5.5.58 using the L2 norm on [0, 1].
5.5.60. Find the best cubic approximation to f(t) = et based on the L2 norm on [0, 1].
5.5.61. Find the (a) linear, (b) quadratic, and (c) cubic polynomials q(t) that minimize the
following integral:
 1
0 [q(t) −t3 ]2 dt. What is the minimum value in each case?
5.5.62. Find the best quadratic and cubic approximations for sin t for the L2 norm on [0, π ] by
using an orthogonal basis. Graph your results and estimate the maximal error.
♠5.5.63. Answer Exercise 5.5.60 when f(t) = sin t. Use a computer to numerically evaluate the
integrals.
♠5.5.64. Find the degree 6 least squares polynomial approximation to et on the interval [−1, 1]
under the L2 norm.
5.5.65.(a) Use the polynomials and weighted norm from Exercise 4.5.12 to ﬁnd the quadratic
least squares approximation to f(t) = 1/t. In what sense is your quadratic approximation
“best”? (b) Now ﬁnd the best approximating cubic polynomial. (c) Compare the graphs
of the quadratic and cubic approximants with the original function and discuss what you
observe.
♠5.5.66. Use the Laguerre polynomials (4.68) to ﬁnd the quadratic and cubic polynomial least
squares approximation to f(t) = tan−1 t relative to the weighted inner product (4.66). Use
a computer to evaluate the coeﬃcients. Graph your result and discuss what you observe.
Splines
In pre-CAD (computer aided design) draftsmanship, a spline was a long, thin, ﬂexible
strip of wood or metal that was used to draw a smooth curve through prescribed points.
The points were marked by small pegs, and the spline rested on the pegs. The mathematical
theory of splines was ﬁrst developed in the 1940s by the Romanian–American mathematician
Isaac Schoenberg as an attractive alternative to polynomial interpolation and approximation.
Splines have since become ubiquitous in numerical analysis, in geometric modeling, in
design and manufacturing, in computer graphics and animation, and in many other
applications.
We suppose that the spline coincides with the graph of a function y = u(x). The pegs
are ﬁxed at the prescribed data points (x0, y0), . . . , (xn, yn), and this requires u(x) to satisfy
the interpolation conditions
u(xj) = yj,
j = 0, . . ., n.
(5.82)
The mesh points x0 < x1 < x2 < · · · < xn are distinct and labeled in increasing order.
The spline is modeled as an elastic beam, and so satisﬁes the homogeneous beam equation
u′′′′ = 0, cf. [61, 79]. Therefore,
u(x) = aj + bj (x −xj) + cj (x −xj)2 + dj (x −xj)3,
xj ≤x ≤xj+1,
j = 0, . . ., n −1,
(5.83)

280
5 Minimization and Least Squares
is a piecewise cubic function — meaning that, between successive mesh points, it is a cubic
polynomial, but not necessarily the same cubic on each subinterval. The fact that we write
the formula (5.83) in terms of x −xj is merely for computational convenience.
Our problem is to determine the coeﬃcients
aj,
bj,
cj,
dj,
j = 0, . . . , n −1.
Since there are n subintervals, there is a total of 4n coeﬃcients, and so we require 4n
equations to uniquely prescribe them. First, we need the spline to satisfy the interpolation
conditions (5.82). Since it is deﬁned by a diﬀerent formula on each side of the mesh point,
this results in a total of 2n conditions:
u(x+
j ) = aj = yj,
u(x−
j+1) = aj + bj hj + cj h2
j + dj h3
j = yj+1,
j = 0, . . . , n −1,
(5.84)
where we abbreviate the length of the jth subinterval by
hj = xj+1 −xj.
The next step is to require that the spline be as smooth as possible. The interpolation
conditions (5.84) guarantee that u(x) is continuous. The condition that u(x) ∈C1 be
continuously diﬀerentiable requires that u′(x) be continuous at the interior mesh points
x1, . . . , xn−1, which imposes the n −1 additional conditions
bj + 2cj hj + 3dj h2
j = u′(x−
j+1) = u′(x+
j+1) = bj+1,
j = 0, . . ., n −2.
(5.85)
To make u ∈C2, we impose n −1 further conditions
2cj + 6dj hj = u′′(x−
j+1) = u′′(x+
j+1) = 2cj+1,
j = 0, . . ., n −2,
(5.86)
to ensure that u′′ is continuous at the mesh points.
We have now imposed a total of
4n −2 conditions, namely (5.84–86), on the 4n coeﬃcients. The two missing constraints
will come from boundary conditions at the two endpoints, namely x0 and xn. There are
three common types:
(i) Natural boundary conditions: u′′(x0) = u′′(xn) = 0, whereby
c0 = 0,
cn−1 + 3dn−1 hn−1 = 0.
(5.87)
Physically, this models a simply supported spline that rests freely on the ﬁrst and last pegs.
(ii) Clamped boundary conditions: u′(x0) = α, u′(xn) = β, where α, β, which could be
0, are speciﬁed by the user. This requires
b0 = α,
bn−1 + 2cn−1 hn−1 + 3dn−1 h2
n−1 = β.
(5.88)
This corresponds to clamping the spline at prescribed angles at each end.
(iii) Periodic boundary conditions: u′(x0) = u′(xn), u′′(x0) = u′′(xn), so that
b0 = bn−1 + 2cn−1 hn−1 + 3dn−1 h2
n−1,
c0 = cn−1 + 3dn−1 hn−1.
(5.89)
The periodic case is used to draw smooth closed curves; see below.
Theorem 5.25. Given mesh points a = x0 < x1 < · · · < xn = b, and corresponding data
values y0, y1, . . . , yn, along with one of the three kinds of boundary conditions (5.87), (5.88),
or (5.89), then there exists a unique piecewise cubic spline function u(x) ∈C2[a, b] that
interpolates the data, u(x0) = y0, . . . , u(xn) = yn, and satisﬁes the boundary conditions.

5.5 Data Fitting and Interpolation
281
Proof : We ﬁrst discuss the natural case. The clamped case is left as an exercise for the
reader, while the slightly harder periodic case will be treated at the end of the section.
The ﬁrst set of equations in (5.84) says that
aj = yj,
j = 0, . . ., n −1.
(5.90)
Next, (5.86–87) imply that
dj =
cj+1 −cj
3hj
.
(5.91)
This equation also holds for j = n −1, provided that we make the convention that†
cn = 0.
We now substitute (5.90–91) into the second set of equations in (5.84), and then solve the
resulting equation for
bj =
yj+1 −yj
hj
−
(2cj + cj+1)hj
3
.
(5.92)
Substituting this result and (5.91) back into (5.85), and simplifying, we obtain
hj cj + 2(hj + hj+1)cj+1 + hj+1 cj+2 = 3
4
yj+2 −yj+1
hj+1
−yj+1 −yj
hj
5
= zj+1,
(5.93)
where we introduce zj+1 as a shorthand for the quantity on the right-hand side.
In the case of natural boundary conditions, we have
c0 = 0,
cn = 0,
and so (5.93) constitutes a tridiagonal linear system
A c = z,
(5.94)
for the unknown coeﬃcients c =

c1, c2, . . . , cn−1
T , with coeﬃcient matrix
A =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎝
2(h0 + h1)
h1
h1
2(h1 + h2)
h2
h2
2(h2 + h3)
h3
...
...
...
hn−3
2(hn−3 + hn−2)
hn−2
hn−2
2(hn−2 + hn−1)
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎠
(5.95)
and right-hand side z =

z1, z2, . . . , zn−1
T. Once (5.95) has been solved, we will then use
(5.90–92) to reconstruct the other spline coeﬃcients aj, bj, dj.
The key observation is that the coeﬃcient matrix A is strictly diagonally dominant,
meaning that each diagonal entry is strictly greater than the sum of the other entries in
its row:
2(hj−1 + hj) > hj−1 + hj.
Theorem 8.19 below implies that A is nonsingular, and hence the tridiagonal linear system
has a unique solution c. This suﬃces to prove the theorem in the case of natural boundary
conditions.
Q.E.D.
†
This is merely for convenience; there is no cn used in the formula for the spline.

282
5 Minimization and Least Squares
Figure 5.14.
A Cubic Spline.
To actually solve the linear system (5.94), we can apply our tridiagonal solution algorithm
(1.68). Let us specialize to the most important case, in which the mesh points are equally
spaced in the interval [a, b], so that
xj = a + j h,
where
h = hj = b −a
n
,
j = 0, . . ., n −1.
In this case, the coeﬃcient matrix A = hB is equal to h times the tridiagonal matrix
B =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎝
4
1
1
4
1
1
4
1
1
4
1
1
4
1
... ... ...
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎠
that ﬁrst appeared in Example 1.37. Its LU factorization takes on a rather simple form,
since most of the entries of L and U are essentially the same, modulo rounding error. This
makes the implementation of the Forward and Back Substitution procedures almost trivial.
Figure 5.14 shows a particular example — a natural spline passing through the data
points (0, 0), (1, 2), (2, −1), (3, 1), (4, 0). The human eye is unable to discern the discontinuities
in its third derivatives, and so the graph appears completely smooth, even though it is, in
fact, only C2.
In the periodic case, we set
an+k = an,
bn+k = bn,
cn+k = cn,
dn+k = dn,
zn+k = zn.
With this convention, the basic equations (5.90–93) are the same.
In this case, the
coeﬃcient matrix for the linear system
A c = z,
with
c =

c0, c1, . . . , cn−1
T ,
z =

z0, z1, . . . , zn−1
T ,
is of tricirculant form:
A =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎝
2(hn−1 + h0)
h0
hn−1
h0
2(h0 + h1)
h1
h1
2(h1 + h2)
h2
...
...
...
hn−3
2(hn−3 + hn−2)
hn−2
hn−1
hn−2
2(hn−2 + hn−1)
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎠
. (5.96)

5.5 Data Fitting and Interpolation
283
Figure 5.15.
Three Sample Spline Letters.
Again A is strictly diagonally dominant, and so there is a unique solution c, from which one
reconstructs the spline, proving Theorem 5.25 in the periodic case. The LU factorization
of tricirculant matrices was discussed in Exercise 1.7.14.
One immediate application of splines is curve ﬁtting in computer aided design and
graphics. The basic problem is to draw a smooth parameterized curve u(t) = ( u(t), v(t) )T
that passes through a set of prescribed data points xk = ( xk, yk )T in the plane. We have
the freedom to choose the parameter value t = tk when the curve passes through the kth
point; the simplest and most common choice is to set tk = k.
We then construct the
functions x = u(t) and y = v(t) as cubic splines interpolating the x and y coordinates of
the data points, so u(tk) = xk, v(tk) = yk. For smooth closed curves, we require that both
splines be periodic; for curves with ends, either natural or clamped boundary conditions
are used.
Most computer graphics packages include one or more implementations of parameterized
spline curves.
The same idea also underlies modern font design for laser printing and
typography (including the fonts used in this book). The great advantage of spline fonts
over their bitmapped counterparts is that they can be readily scaled. Some sample letter
shapes parameterized by periodic splines passing through the indicated data points are
plotted in Figure 5.15. Better ﬁts can be easily obtained by increasing the number of data
points. Various extensions of the basic spline algorithms to space curves and surfaces are
an essential component of modern computer graphics, design, and animation, [25, 74].
Exercises
5.5.67. Find and graph the natural cubic spline interpolant for the following data:
(a)
x
−1
0
1
y
−2
1
−1
(b)
x
0
1
2
3
y
1
2
0
1
(c)
x
1
2
4
y
3
0
2
(d)
x
−2
−1
0
1
2
y
5
2
3
−1
1
5.5.68. Repeat Exercise 5.5.67 when the spline has homogeneous clamped boundary conditions.

284
5 Minimization and Least Squares
5.5.69. Find and graph the periodic cubic spline that interpolates the following data:
(a)
x
0
1
2
3
y
1
0
0
1
(b)
x
0
1
2
3
y
1
2
0
1
(c)
x
0
1
2
3
4
y
1
0
0
0
1
(d)
x
−2
−1
0
1
2
y
1
2
−2
−1
1
♠5.5.70.(a) Given the known values of sin x at x = 0◦, 30◦, 45◦, 60◦, construct the natural cubic
spline interpolant. (b) Compare the accuracy of the spline with the least squares and
interpolating polynomials you found in Exercise 5.5.21.
♣5.5.71.(a) Using the exact values for √x at x = 0, 1
4, 9
16, 1, construct the natural cubic
spline interpolant. (b) What is the maximal error of the spline on the interval [0, 1]?
(c) Compare the error with that of the interpolating cubic polynomial you found in
Exercise 5.5.23. Which is the better approximation? (d) Answer part (d) using the cubic
least squares approximant based on the L2 norm on [0, 1].
♠5.5.72. According to Figure 5.9, the interpolating polynomials for the function 1/(1 + x2) on
the interval [−3, 3] based on equally spaced mesh points are very inaccurate near the ends
of the interval. Does the natural spline interpolant based on the same 3, 5, and 11 data
points exhibit the same inaccuracy?
♣5.5.73.(a) Draw outlines of the block capital letters I, C, S, and Y on a sheet of graph paper.
Fix several points on the graphs and measure their x and y coordinates.
(b) Use periodic
cubic splines x = u(t), y = v(t) to interpolate the coordinates of the data points using
equally spaced nodes for the parameter values tk. Graph the resulting spline letters, and
discuss how the method could be used in font design. To get nicer results, you may wish to
experiment with diﬀerent numbers and locations for the points.
♣5.5.74. Repeat Exercise 5.5.73, using the Lagrange interpolating polynomials instead of
splines to parameterize the curves. Compare the two methods and discuss advantages and
disadvantages.
♥5.5.75. Let x0 < x1 < · · · < xn. For each j = 0, . . . , n, the jth cardinal spline Cj(x) is deﬁned
to be the natural cubic spline interpolating the Lagrange data
y0 = 0,
y1 = 0,
. . .
yj−1 = 0,
yj = 1,
yj+1 = 0,
. . .
yn = 0.
(a) Construct and graph the natural cardinal splines corresponding to the nodes x0 = 0,
x1 = 1, x2 = 2, and x3 = 3.
(b) Prove that the natural spline that interpolates the data
y0, . . . , yn can be uniquely written as a linear combination u(x) = y0 C0(x) + y1 C1(x) +
· · · + yn Cn(x) of the cardinal splines. (c) Explain why the space of natural splines on
n + 1 nodes is a vector space of dimension n + 1. (d) Discuss brieﬂy what modiﬁcations are
required to adapt this method to periodic and to clamped splines.
♥5.5.76. A bell-shaped or B-spline u = β(x) interpolates the data
β(−2) = 0,
β(−1) = 1,
β(0) = 4,
β(1) = 1,
β(2) = 0.
(a) Find the explicit formula for the natural B-spline and plot its graph. (b) Show that
β(x) also satisﬁes the homogeneous clamped boundary conditions u′(−2) = u′(2) = 0.
(c) Show that β(x) also satisﬁes the periodic boundary conditions. Thus, for this particular
interpolation problem, the natural, clamped, and periodic splines happen to coincide.
(d) Show that β⋆(x) =
 β(x),
−2 ≤x ≤2
0,
otherwise,
deﬁnes a C2 spline on every interval [−k, k].
♥5.5.77. Let β(x) denote the B-spline function of Exercise 5.5.76. Assuming n ≥4, let Pn
denote the vector space of periodic cubic splines based on the integer nodes xj = j
for j = 0, . . . , n. (a) Prove that the B-splines Bj(x) = β

(x −j −m) mod n + m

,

5.6 Discrete Fourier Analysis and the Fast Fourier Transform
285
j = 0, . . . , n −1, where m denotes the integer part of n/2, form a basis for Pn.
(b) Graph
the basis periodic B-splines in the case n = 5. (c) Let u(x) denote the periodic spline
interpolant for the data values y0, . . . , yn−1. Explain how to write u(x) = α0 B0(x) +
· · · + αn−1 Bn−1(x) in terms of the B-splines by solving a linear system for the coeﬃcients
α0, . . . , αn−1.
(d) Write the periodic spline with y0 = y5 = 0, y1 = 2, y2 = 1, y3 = −1,
y4 = −2, as a linear combination of the periodic basis B-splines B0(x), . . . , B4(x). Plot the
resulting periodic spline function.
5.6 Discrete Fourier Analysis and the Fast Fourier Transform
In modern digital media — audio, still images, video, etc. — continuous signals are sampled
at discrete time intervals before being processed. Fourier analysis decomposes the sampled
signal into its fundamental periodic constituents — sines and cosines, or, more conveniently,
complex exponentials.
The crucial fact, upon which all of modern signal processing is
based, is that the sampled complex exponentials form an orthogonal basis. This section
introduces the Discrete Fourier Transform, and concludes with an introduction to the justly
famous Fast Fourier Transform, an eﬃcient algorithm for computing the discrete Fourier
representation and reconstructing the signal from its Fourier coeﬃcients.
We will concentrate on the one-dimensional version here.
Let f(x) be a function
representing the signal, deﬁned on an interval a ≤x ≤b. Our computer can store its
measured values only at a ﬁnite number of sample points a ≤x0 < x1 < · · · < xn ≤b.
In the simplest, and by far the most common, case, the sample points are equally spaced,
and so
xj = a + j h,
j = 0, . . . , n,
where
h = b −a
n
indicates the sample rate. In signal processing applications, x represents time instead of
space, and the xj are the times at which we sample the signal f(x). Sample rates can be
very high, e.g., every 10–20 milliseconds in current speech recognition systems.
For simplicity, we adopt the “standard” interval of 0 ≤x ≤2π, and the n equally
spaced sample points†
x0 = 0,
x1 = 2π
n ,
x2 = 4π
n ,
. . .
xj = 2j π
n
,
. . .
xn−1 = 2(n −1)π
n
.
(5.97)
(Signals deﬁned on other intervals can be handled by simply rescaling the interval to have
length 2π.)
Sampling a (complex-valued) signal or function f(x) produces the sample
vector
f =

f0, f1, . . . , fn−1
T =

f(x0), f(x1), . . . , f(xn−1)
T ,
where
fj = f(xj) = f
2j π
n

,
j = 0, . . ., n −1.
(5.98)
Sampling cannot distinguish between functions that have the same values at all of the
sample points — from the sampler’s point of view they are identical. For example, the
periodic complex exponential function
f(x) = e i nx = cos nx + i sin nx
†
We will ﬁnd it convenient to omit the ﬁnal sample point xn = 2π from consideration.

286
5 Minimization and Least Squares
Figure 5.16.
Sampling e−i x and e7 i x on n = 8 sample points.
has sampled values
fj = f
2j π
n

= exp

i n 2j π
n

= e2j π i = 1
for all
j = 0, . . . , n −1,
and hence is indistinguishable from the constant function c(x) ≡1 — both lead to the
same sample vector ( 1, 1, . . ., 1 )T . This has the important implication that sampling at n
equally spaced sample points cannot detect periodic signals of frequency n. More generally,
the two complex exponential signals
e i (k+n)x
and
e i kx
are also indistinguishable when sampled. Consequently, we need only use the ﬁrst n periodic
complex exponential functions
f0(x) = 1,
f1(x) = e i x,
f2(x) = e2 i x,
. . .
fn−1(x) = e(n−1) i x,
(5.99)
in order to represent any 2π periodic sampled signal. In particular, exponentials e−i kx of
“negative” frequency can all be converted into positive versions, namely e i (n−k)x, by the
same sampling argument. For example,
e−i x = cos x −i sin x
and
e(n−1) i x = cos(n −1) x + i sin(n −1) x
have identical values on the sample points (5.97). However, oﬀof the sample points, they
are quite diﬀerent; the former is slowly varying, while the latter represents a high-frequency
oscillation. In Figure 5.16, we compare e−i x and e7 i x when there are n = 8 sample values,
indicated by the dots on the graphs. The top row compares the real parts, cos x and cos 7x,
while the bottom row compares the imaginary parts, sin x and −sin 7x. Note that both
functions have the same pattern of sample values, even though their overall behavior is
strikingly diﬀerent.
This eﬀect is commonly referred to as aliasing.† If you view a moving particle under a
stroboscopic light that ﬂashes only eight times, you would be unable to determine which
†
In computer graphics, the term “aliasing” is used in a much broader sense that covers a variety
of artifacts introduced by discretization — particularly, the jagged appearance of lines and smooth
curves on a digital monitor.

5.6 Discrete Fourier Analysis and the Fast Fourier Transform
287
of the two graphs the particle was following. Aliasing is the cause of a well-known artifact
in movies: spoked wheels can appear to be rotating backwards when our brain interprets
the discretization of the high-frequency forward motion imposed by the frames of the ﬁlm
as an equivalently discretized low-frequency motion in reverse. Aliasing also has important
implications for the design of music CD’s. We must sample an audio signal at a suﬃciently
high rate that all audible frequencies can be adequately represented.
In fact, human
appreciation of music also relies on inaudible high-frequency tones, and so a much higher
sample rate is actually used in commercial CD design.
But the sample rate that was
selected remains controversial; hi ﬁaﬁcionados complain that it was not set high enough
to fully reproduce the musical quality of an analog LP record!
The discrete Fourier representation decomposes a sampled function f(x) into a linear
combination of complex exponentials. Since we cannot distinguish sampled exponentials
of frequency higher than n, we only need consider a ﬁnite linear combination
f(x) ∼p(x) = c0 + c1 e i x + c2 e2 i x + · · · + cn−1 e(n−1) i x =
n−1

k=0
ck e i kx
(5.100)
of the ﬁrst n exponentials (5.99). The symbol ∼in (5.100) means that the function f(x)
and the sum p(x) agree on the sample points:
f(xj) = p(xj),
j = 0, . . ., n −1.
(5.101)
Therefore, p(x) can be viewed as a (complex-valued) interpolating trigonometric polynomial
of degree ≤n −1 for the sample data fj = f(xj).
Remark. If f(x) is real, then p(x) is also real on the sample points, but may very well
be complex-valued in between. To avoid this unsatisfying state of aﬀairs, we will usually
discard its imaginary component, and regard the real part of p(x) as “the” interpolating
trigonometric polynomial. On the other hand, sticking with a purely real construction
unnecessarily complicates the underlying mathematical analysis, and so we will retain the
complex exponential form (5.100) of the discrete Fourier sum.
Since we are working in the ﬁnite-dimensional complex vector space Cn throughout,
we can reformulate the discrete Fourier series in vectorial form.
Sampling the basic
exponentials (5.99) produces the complex vectors
ωk =

e i kx0, e i kx1, e i kx2, . . . , e i kxn−1 T
=
$
1, e2kπ i /n, e4kπ i /n, . . . , e2(n−1)kπ i /n %T
,
k = 0, . . . , n −1.
(5.102)
The interpolation conditions (5.101) can be recast in the equivalent vector form
f = c0 ω0 + c1 ω1 + · · · + cn−1 ωn−1.
(5.103)
In other words, to compute the discrete Fourier coeﬃcients c0, . . . , cn−1 of f, all we need
to do is rewrite its sample vector f as a linear combination of the sampled exponential
vectors ω0, . . . , ωn−1.
Now, the absolutely crucial property is the orthonormality of the basis elements
ω0, . . . , ωn−1.
Were it not for the power of orthogonality, Fourier analysis might have
remained a mere mathematical curiosity, rather than today’s indispensable tool.

288
5 Minimization and Least Squares
ζ0
5 = 1
ζ5
ζ2
5
ζ3
5
ζ4
5
Figure 5.17.
The Fifth Roots of Unity.
Proposition 5.26. The sampled exponential vectors ω0, . . . , ωn−1 form an orthonormal
basis of Cn with respect to the inner product
⟨f , g ⟩= 1
n
n−1

j =0
fj gj = 1
n
n−1

j =0
f(xj) g(xj) ,
f, g ∈Cn.
(5.104)
Remark. The inner product (5.104) is a rescaled version of the standard Hermitian dot
product (3.98) between complex vectors. We can interpret the inner product between the
sample vectors f, g as the average of the sampled values of the product signal f(x) g(x).
Proof : The crux of the matter relies on properties of the remarkable complex numbers
ζn = e2π i /n = cos 2π
n + i sin 2π
n ,
where
n = 1, 2, 3, . . . .
(5.105)
Particular cases include
ζ2 = −1,
ζ3 = −1
2 +
√
3
2 i ,
ζ4 = i ,
and
ζ8 =
√
2
2 +
√
2
2 i .
(5.106)
The nth power of ζn is
ζn
n =
$
e2π i /n%
n = e2π i = 1,
and hence ζn is one of the complex nth roots of unity: ζn =
n√
1. There are, in fact, n
distinct complex nth roots of 1, including 1 itself, namely the powers of ζn:
ζk
n = e2 k π i /n = cos 2 k π
n
+ i sin 2 k π
n
,
k = 0, . . ., n −1.
(5.107)
Since it generates all the others, ζn is known as a primitive nth root of unity. Geometrically,
the nth roots (5.107) are the vertices of a regular unit n-gon inscribed in the unit circle
| z | = 1; see Figure 5.17 for the case n = 5, where the roots form the vertices of a regular
pentagon. The primitive root ζn is the ﬁrst vertex we encounter as we go around the n-gon
in a counterclockwise direction, starting at 1. Continuing around, the other roots appear
in their natural order ζ2
n, ζ3
n, . . . , ζn−1
n
, cycling back to ζn
n = 1. The complex conjugate of
ζn is the “last” nth root:
e−2π i /n = ζn = 1
ζn
= ζn−1
n
= e2(n−1)π i /n.
(5.108)

5.6 Discrete Fourier Analysis and the Fast Fourier Transform
289
The complex numbers (5.107) are a complete set of roots of the polynomial zn −1,
which can therefore be completely factored:
zn −1 = (z −1)(z −ζn)(z −ζ2
n) · · · (z −ζn−1
n
).
On the other hand, elementary algebra provides us with the real factorization
zn −1 = (z −1)(1 + z + z2 + · · · + zn−1).
Comparing the two, we conclude that
1 + z + z2 + · · · + zn−1 = (z −ζn)(z −ζ2
n) · · ·(z −ζn−1
n
).
Substituting z = ζk
n into both sides of this identity, we deduce the useful formula
1 + ζk
n + ζ2k
n + · · · + ζ(n−1)k
n
=
 n,
k = 0,
0,
0 < k < n.
(5.109)
Since ζn+k
n
= ζk
n, this formula can easily be extended to general integers k; the sum is equal
to n if n evenly divides k and is 0 otherwise.
Now, let us apply what we’ve learned to prove Proposition 5.26. First, in view of (5.107),
the sampled exponential vectors (5.102) can all be written in terms of the nth roots of unity:
ωk =

1, ζk
n, ζ2k
n , ζ3k
n , . . . , ζ(n−1)k
n
T ,
k = 0, . . ., n −1.
(5.110)
Therefore, applying (5.108, 109), we conclude that
⟨ωk , ωl ⟩= 1
n
n−1

j =0
ζj k
n
ζjl
n = 1
n
n−1

j =0
ζj(k−l)
n
=
(
1,
k = l,
0,
k ̸= l,
0 ≤k, l < n,
which establishes orthonormality of the sampled exponential vectors.
Q.E.D.
Orthonormality of the basis vectors implies that we can immediately compute the
Fourier coeﬃcients in the discrete Fourier sum (5.100) by taking inner products:
ck = ⟨f , ωk ⟩= 1
n
n−1

j =0
fj e i kxj = 1
n
n−1

j =0
fj e−i kxj = 1
n
n−1

j =0
ζ−j k
n
fj.
(5.111)
In other words, the discrete Fourier coeﬃcient ck is obtained by averaging the sampled
values of the product function f(x) e−i kx.
The passage from a signal to its Fourier
coeﬃcients is known as the Discrete Fourier Transform or DFT for short. The reverse
procedure of reconstructing a signal from its discrete Fourier coeﬃcients via the sum (5.100)
(or (5.103)) is known as the Inverse Discrete Fourier Transform or IDFT.
Example 5.27.
If n = 4, then ζ4 = i . The corresponding sampled exponential vectors
ω0 =
⎛
⎜
⎝
1
1
1
1
⎞
⎟
⎠,
ω1 =
⎛
⎜
⎝
1
i
−1
−i
⎞
⎟
⎠,
ω2 =
⎛
⎜
⎝
1
−1
1
−1
⎞
⎟
⎠,
ω3 =
⎛
⎜
⎝
1
−i
−1
i
⎞
⎟
⎠,
form an orthonormal basis of C4 with respect to the averaged Hermitian dot product
⟨v , w ⟩= 1
4

v0 w0 + v1 w1 + v2 w2 + v3 w3

,
where
v =
⎛
⎜
⎝
v0
v1
v2
v3
⎞
⎟
⎠,
w =
⎛
⎜
⎝
w0
w1
w2
w3
⎞
⎟
⎠.

290
5 Minimization and Least Squares
Figure 5.18.
The Discrete Fourier Representation of 2π x −x2.
Given the sampled function values
f0 = f(0),
f1 = f
 1
2 π

,
f2 = f(π),
f3 = f
 3
2 π

,
we construct the discrete Fourier representation
f = c0 ω0 + c1 ω1 + c2 ω2 + c3 ω3,
(5.112)
where
c0 = ⟨f , ω0 ⟩= 1
4(f0 + f1 + f2 + f3),
c1 = ⟨f , ω1 ⟩= 1
4(f0 −i f1 −f2 + i f3),
c2 = ⟨f , ω2 ⟩= 1
4(f0 −f1 + f2 −f3),
c3 = ⟨f , ω3 ⟩= 1
4(f0 + i f1 −f2 −i f3).
We interpret this decomposition as the complex exponential interpolant
f(x) ∼p(x) = c0 + c1 e i x + c2 e2 i x + c3 e3 i x
that agrees with f(x) on the 4 sample points.
For instance, if
f(x) = 2π x −x2,
then
f0 = 0,
f1 = 7.4022,
f2 = 9.8696,
f3 = 7.4022,
and hence
c0 = 6.1685,
c1 = −2.4674,
c2 = −1.2337,
c3 = −2.4674.
Therefore, the interpolating trigonometric polynomial is given by the real part of
p4(x) = 6.1685 −2.4674 e i x −1.2337 e2 i x −2.4674 e3 i x,
(5.113)
namely,
Re p4(x) = 6.1685 −2.4674 cos x −1.2337 cos 2x −2.4674 cos 3x.
(5.114)
In Figure 5.18, we compare the function, with the interpolation points indicated, and its
discrete Fourier representations (5.114) for both n = 4 in the ﬁrst row, and n = 16 points
in the second. The resulting graphs point out a signiﬁcant diﬃculty with the Discrete
Fourier Transform as developed so far. While the trigonometric polynomials do indeed
correctly match the sampled function values, their pronounced oscillatory behavior makes
them completely unsuitable for interpolation away from the sample points.

5.6 Discrete Fourier Analysis and the Fast Fourier Transform
291
Figure 5.19.
The Low–Frequency Discrete Fourier Representation of x2 −2π x.
However, this diﬃculty can be rectiﬁed by being a little more clever. The problem is
that we have not been paying suﬃcient attention to the frequencies that are represented
in the Fourier sum. Indeed, the graphs in Figure 5.18 might remind you of our earlier
observation that, due to aliasing, low- and high-frequency exponentials can have the same
sample data, but diﬀer wildly in between the sample points. While the ﬁrst half of the
summands in (5.100) represent relatively low frequencies, the second half do not, and can be
replaced by equivalent lower-frequency, and hence less-oscillatory, exponentials. Namely,
if 0 < k ≤1
2 n, then e−i kx and e i (n−k)x have the same sample values, but the former is of
lower frequency than the latter. Thus, for interpolatory purposes, we should replace the
second half of the summands in the Fourier sum (5.100) by their low-frequency alternatives.
If n = 2m + 1 is odd, then we take
p2m+1(x) = c−m e−i mx + · · · +c−1 e−i x +c0 +c1 e i x + · · · +cm e i mx =
m

k=−m
ck e i kx
(5.115)
as the equivalent low-frequency interpolant.
If n = 2m is even — which is the most
common case occurring in applications — then
p2m(x) = c−m e−i mx+ · · · +c−1 e−i x+c0+c1 e i x+ · · · +cm−1 e i (m−1)x =
m−1

k=−m
ck e i kx
(5.116)
will be our choice. (It is a matter of personal taste whether to use e−i mx or e i mx to
represent the highest-frequency term.) In both cases, the Fourier coeﬃcients with negative
indices are the same as their high-frequency alternatives:
c−k = cn−k = ⟨f , ωn−k ⟩= ⟨f , ω−k ⟩,
(5.117)
where ω−k = ωn−k is the sample vector for e−i kx ∼e i (n−k)x.
Returning to the previous example, for interpolating purposes, we should replace (5.113)
by the equivalent low-frequency interpolant
p4(x) = −1.2337 e−2 i x −2.4674 e−i x + 6.1685 −2.4674 e ix,
(5.118)
with real part
Re p4(x) = 6.1685 −4.9348 cos x −1.2337 cos 2x.

292
5 Minimization and Least Squares
Graphs of the n = 4 and 16 low-frequency trigonometric interpolants can be seen in
Figure 5.19.
Thus, by utilizing only the lowest-frequency exponentials, we successfully
suppress the aliasing artifacts, resulting in a quite reasonable trigonometric interpolant to
the given function on the entire interval.
Remark.
The low-frequency version also serves to unravel the reality of the Fourier
representation of a real function f(x).
Since ω−k = ωk, formula (5.117) implies that
c−k = ck, and so the common frequency terms
c−k e−i kx + ck e i kx = ak cos kx + bk sin kx
add up to a real trigonometric function. Therefore, the odd n interpolant (5.115) is a real
trigonometric polynomial, whereas in the even version (5.116) only the highest-frequency
term c−m e−i mx produces a complex term — which is, in fact, 0 on the sample points.
Exercises
5.6.1. Find (i) the discrete Fourier coeﬃcients, and (ii) the low-frequency trigonometric inter-
polant, for the following functions using the indicated number of sample points:
(a) sin x,
n = 4,
(b) | x −π |, n = 6,
(c) f(x) =
 1,
x ≤2,
0,
x > 2,
n = 6,
(d) sign(x −π), n = 8.
5.6.2. Find (i) the sample values, and (ii) the trigonometric interpolant corresponding to the
following discrete Fourier coeﬃcients:
(a) c−1 = c1 = 1, c0 = 0,
(b) c−2 = c0 = c2 = 1, c−1 = c1 = −1,
(c) c−2 = c0 = c1 = 2, c−1 = c2 = 0,
(d) c0 = c2 = c4 = 1, c1 = c3 = c5 = −1.
♠5.6.3. Let f(x) = x. Compute its discrete Fourier coeﬃcients based on n = 4, 8 and
16 interpolation points. Then, plot f(x) along with the resulting (real) trigonometric
interpolants and discuss their accuracy.
♠5.6.4. Answer Exercise 5.6.3 for the functions
(a) x2,
(b) (x −π)2,
(c) sin x,
(d) cos 1
2 x,
(e)
 1,
1
2 π ≤x ≤3
2 π,
0,
otherwise,
(f )
 x,
0 ≤x ≤π,
2π −x,
π ≤x ≤2π.
5.6.5.(a) Draw a picture of the complex plane with the complex solutions to z6 = 1 marked.
(b) What is the exact formula (no trigonometric functions allowed) for the primitive sixth
root of unity ζ6?
(c) Verify explicitly that 1 + ζ6 + ζ2
6 + ζ3
6 + ζ4
6 + ζ5
6 = 0.
(d) Give a
geometrical explanation of this identity.
♦5.6.6.(a) Explain in detail why the nth roots of 1 lie on the vertices of a regular n-gon. What
is the angle between two consecutive sides?
(b) Explain why this is also true for the nth roots of every non-zero complex number z ̸= 0.
Sketch a picture of the hexagon corresponding to
6√z for a given z ̸= 0.
♦5.6.7. In general, an nth root of unity ζ is called primitive if all the nth roots of unity
are obtained by raising it to successive powers: 1, ζ, ζ2, ζ3, . . . . (a) Find all primitive
(i) fourth, (ii) ﬁfth, (iii) ninth roots of unity. (b) Can you characterize all the primitive
nth roots of unity?
5.6.8.(a) In Example 5.27, the n = 4 discrete Fourier coeﬃcients of the function
f(x) = 2π x −x2 were found to be real. Is this true when n = 16? For general n?
(b) What property of a function f(x) will guarantee that its Fourier coeﬃcients are real?

5.6 Discrete Fourier Analysis and the Fast Fourier Transform
293
The Original Signal
The Noisy Signal
The Denoised Signal
Comparison of the Two
Figure 5.20.
Denoising a Signal.
♥5.6.9. Let c = (c0, c1, . . . , cn−1)T ∈Cn be the vector of discrete Fourier coeﬃcients
corresponding to the sample vector f = (f0, f1, . . . , fn−1)T . (a) Explain why the sampled
signal f = Fn c can be reconstructed by multiplying its Fourier coeﬃcient vector by an n×n
matrix Fn. Write down F2, F3, F4, and F8. What is the general formula for the entries of
Fn? (b) Prove that, in general, F −1
n
= 1
n F †
n = 1
n Fn
T , where † denotes the Hermitian
transpose deﬁned in Exercise 4.3.25.
(c) Prove that Un =
1
√n Fn is a unitary matrix, i.e.,
U−1
n
= U†
n.
Compression and Denoising
In a typical experimental signal, noise primarily aﬀects the high-frequency modes, while
the authentic features tend to appear in the low frequencies. Think of the hiss and static
you hear on an AM radio station or a low-quality audio recording. Thus, a very simple, but
eﬀective, method for denoising a corrupted signal is to decompose it into its Fourier modes,
as in (5.100), and then discard the high-frequency constituents. A similar idea underlies the
Dolby recording system used on most movie soundtracks: during the recording process, the
high-frequency modes are artiﬁcially boosted, so that scaling them back when the movie
is shown in the theater has the eﬀect of eliminating much of the extraneous noise. The
one design issue is the speciﬁcation of a cut-oﬀbetween low and high frequency, that is,
between signal and noise. This choice will depend upon the properties of the measured
signal, and is left to the discretion of the signal processor.
A correct implementation of the denoising procedure is facilitated by using the unaliased
forms (5.115, 116) of the trigonometric interpolant, in which the low-frequency summands
appear only when | k | is small. In this version, to eliminate high-frequency components,

294
5 Minimization and Least Squares
The Original Signal
Moderate Compression
High Compression
Figure 5.21.
Compressing a Signal.
we replace the full summation by
ql(x) =
l

k=−l
ck e i kx,
(5.119)
where l < 1
2 (n + 1) speciﬁes the selected cut-oﬀfrequency between signal and noise. The
2l + 1 ≪n low-frequency Fourier modes retained in (5.119) will, in favorable situations,
capture the essential features of the original signal while simultaneously eliminating the
high-frequency noise.
In Figure 5.20 we display a sample signal followed by the same signal corrupted by adding
in random noise. We use n = 29 = 512 sample points in the discrete Fourier representation,
and to remove the noise, we retain only the 2l + 1 = 11 lowest-frequency modes.
In
other words, instead of all n = 512 Fourier coeﬃcients c−256, . . . , c−1, c0, c1, . . . , c255, we
compute only the 11 lowest-order ones c−5, . . . , c5. Orthogonality is the key that allows us
to do this! Summing up just those 11 exponentials produces the denoised signal q(x) =
c−5 e−5 i x + · · · + c5 e5 i x. To compare, we plot both the original signal and the denoised
version on the same graph. In this case, the maximal deviation is less than .15 over the
entire interval [0, 2π ].
The same idea underlies many data compression algorithms for audio recordings, digital
images, and, particularly, video. The goal is eﬃcient storage and/or transmission of the
signal. As before, we expect all the important features to be contained in the low-frequency
constituents, and so discarding the high-frequency terms will, in favorable situations,
not lead to any noticeable degradation of the signal or image.
Thus, to compress a
signal (and, simultaneously, remove high-frequency noise), we retain only its low-frequency
discrete Fourier coeﬃcients. The signal is reconstructed by summing the associated discrete
Fourier representation (5.119). A mathematical justiﬁcation of Fourier-based compression
algorithms relies on the fact that the Fourier coeﬃcients of smooth functions tend rapidly
to zero — the smoother the function, the faster the decay rate; see [61] for details. Thus,
the small high-frequency Fourier coeﬃcients will be of negligible importance.
In Figure 5.21, the same signal is compressed by retaining, respectively, 2l + 1 = 21
and 2l + 1 = 7 Fourier coeﬃcients only instead of all n = 512 that would be required for
complete accuracy. For the case of moderate compression, the maximal deviation between
the signal and the compressed version is less than 1.5 × 10−4 over the entire interval, while
even the highly compressed version deviates by at most .05 from the original signal. Of
course, the lack of any ﬁne-scale features in this particular signal means that a very high
compression can be achieved — the more complicated or detailed the original signal, the
more Fourier modes need to be retained for accurate reproduction.

5.6 Discrete Fourier Analysis and the Fast Fourier Transform
295
Exercises
♠5.6.10. Construct the discrete Fourier coeﬃcients for f(x) =
⎧
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎩
−x,
0 ≤x ≤1
3 π,
x −2
3 π,
1
3 π ≤x ≤4
3 π,
−x + 2π,
4
3 π ≤x ≤2π.
based on n = 128 sample points.
Then graph the reconstructed function when using
the data compression algorithm that retains only the 11 and 21 lowest-frequency modes.
Discuss what you observe.
♠5.6.11. Answer Exercise 5.6.10 when f(x) = (a) x; (b) x2(2π −x)2; (c)
 sin x, 0 ≤x ≤π,
0,
π ≤x ≤2π.
♣5.6.12. Let ql(x) denote the trigonometric polynomial (5.119) obtained by summing the ﬁrst
2l + 1 discrete Fourier modes. Suppose the criterion for compression of a signal f(x) is
that ∥f −ql ∥∞= max{ | f(x) −ql(x) | | 0 ≤x ≤2π } < ε. For the particular function in
Exercise 5.6.10, how large do you need to choose k when ε = .1? ε = .01? ε = .001?
♣5.6.13. Let f(x) = x(2π −x) be sampled on n = 128 equally spaced points between 0 and
2π. Use a random number generator with −1 ≤rj ≤1 to add noise by replacing each
sample value fj = f(xj) by gj = fj + ε rj. Investigate, for diﬀerent values of ε, how many
discrete Fourier modes are required to reconstruct a reasonable denoised approximation to
the original signal.
♠5.6.14. The signal in Figure 5.20 was obtained from the explicit formula
f(x) = −1
5
 x(2π −x)
10
	5
(x + 1.5)(x + 2.5)(x −4) + 1.7.
Noise was added by using a random number generator. Experiment with diﬀerent
intensities of noise and diﬀerent numbers of sample points and discuss what you observe.
♣5.6.15. If we use the original form (5.100) of the discrete Fourier representation, we might be
tempted to denoise/compress the signal by retaining only the ﬁrst 0 ≤k ≤l terms in the
sum. Test this method on the signal in Exercise 5.6.10 and discuss what you observe.
5.6.16. True or false: If f(x) is real, the compressed/denoised signal (5.119) is a real
trigonometric polynomial.
The Fast Fourier Transform
While one may admire an algorithm for its intrinsic beauty, in the real world, the bottom
line is always eﬃciency of implementation: the less total computation, the faster the
processing, and hence the more extensive the range of applications. Orthogonality is the
ﬁrst and most important feature of many practical linear algebra algorithms, and is the
critical feature of Fourier analysis. Still, even the power of orthogonality reaches its limits
when it comes to dealing with truly large-scale problems such as three-dimensional medical
imaging or video processing. In the early 1960’s, James Cooley and John Tukey, [15],
discovered† a much more eﬃcient approach to the Discrete Fourier Transform, exploiting
the rather special structure of the sampled exponential vectors. The resulting algorithm is
†
In fact, the key ideas can be found in Gauss’s hand computations in the early 1800’s, but his
insight was not fully appreciated until modern computers arrived on the scene.

296
5 Minimization and Least Squares
known as the Fast Fourier Transform, often abbreviated FFT, and its discovery launched
the modern revolution in digital signal and data processing, [9, 10].
In general, computing all the discrete Fourier coeﬃcients (5.111) of an n times sampled
signal requires a total of n2 complex multiplications and n2 −n complex additions. Note
also that each complex addition
z + w = (x + i y) + (u + i v) = (x + u) + i (y + v)
(5.120)
generally requires two real additions, while each complex multiplication
z w = (x + i y)(u + i v) = (xu −y v) + i (xv + y u)
(5.121)
requires 4 real multiplications and 2 real additions, or, by employing the alternative formula
xv + y u = (x + y)(u + v) −xu −y v
(5.122)
for the imaginary part, 3 real multiplications and 5 real additions. (The choice of formula
(5.121) or (5.122) will depend upon the processor’s relative speeds of multiplication and
addition.) Similarly, given the Fourier coeﬃcients c0, . . . , cn−1, reconstruction of the sampled
signal via (5.100) requires n2 −n complex multiplications and n2 −n complex additions.
As a result, both computations become quite labor-intensive for large n. Extending these
ideas to multi-dimensional data only exacerbates the problem. The Fast Fourier Transform
provides a shortcut around this computational bottleneck and thereby signniﬁcantly extends
the range of discrete Fourier analysis.
In order to explain the method without undue complication, we return to the original,
aliased form of the discrete Fourier representation (5.100). (Once one understands how
the FFT works, one can easily adapt the algorithm to the low-frequency version (5.116).)
The seminal observation is that if the number of sample points n = 2m is even, then the
primitive mth root of unity ζm =
m√
1 equals the square of the primitive nth root: ζm = ζ2
n.
We use this fact to split the summation (5.111) for the order n discrete Fourier coeﬃcients
into two parts, collecting the even and the odd powers of ζk
n:
ck = 1
n

f0 + f1 ζ−k
n
+ f2 ζ −2k
n
+ · · · + fn−1 ζ −(n−1)k
n

= 1
n

f0 + f2 ζ −2k
n
+ f4 ζ −4k
n
+ · · · + f2m−2 ζ −(2m−2)k
n

+
+ ζ−k
n
1
n

f1 + f3 ζ −2k
n
+ f5 ζ −4k
n
+ · · · + f2m−1 ζ −(2m−2)k
n

= 1
2
 1
m

f0 + f2 ζ −k
m
+ f4 ζ −2k
m
+ · · · + f2m−2 ζ −(m−1)k
m
 '
+
+ ζ −k
n
2
 1
m

f1 + f3 ζ −k
m
+ f5 ζ −2k
m
+ · · · + f2m−1 ζ −(m−1)k
m
 '
.
(5.123)
Now, observe that the expressions in braces are the order m Fourier coeﬃcients for the
sample data
f e =

f0, f2, f4, . . . , f2m−2
T =

f(x0), f(x2), f(x4), . . . , f(x2m−2)
T ,
f o =

f1, f3, f5, . . . , f2m−1
T =

f(x1), f(x3), f(x5), . . . , f(x2m−1)
T .
(5.124)
Note that f e is obtained by sampling f(x) on the even sample points x2j, while f o is
obtained by sampling the same function f(x), but now at the odd sample points x2j+1. In

5.6 Discrete Fourier Analysis and the Fast Fourier Transform
297
other words, we are splitting the original sampled signal into two “half-sampled” signals
obtained by sampling on every other point. The even and odd Fourier coeﬃcients are
ce
k = 1
m

f0 + f2 ζ −k
m
+ f4 ζ −2k
m
+ · · · + f2m−2 ζ −(m−1)k
m

,
co
k = 1
m

f1 + f3 ζ −k
m
+ f5 ζ −2k
m
+ · · · + f2m−1 ζ −(m−1)k
m

,
k = 0, . . ., m −1.
(5.125)
Since they contain just m data values, both the even and odd samples require only m
distinct Fourier coeﬃcients, and we adopt the identiﬁcation
ce
k+m = ce
k,
co
k+m = co
k,
k = 0, . . . , m −1.
(5.126)
Therefore, the order n = 2m discrete Fourier coeﬃcients (5.123) can be constructed from
a pair of order m discrete Fourier coeﬃcients via
ck = 1
2

ce
k + ζ−k
n
co
k

,
k = 0, . . . , n −1.
(5.127)
Now if m = 2l is also even, then we can play the same game on the order m Fourier
coeﬃcients (5.125), reconstructing each of them from a pair of order l discrete Fourier
coeﬃcients — obtained by sampling the signal at every fourth point. If n = 2r is a power
of 2, then this game can be played all the way back to the start, beginning with the trivial
order 1 discrete Fourier representation, which just samples the function at a single point.
The result is the desired algorithm. After some rearrangement of the basic steps, we arrive
at the Fast Fourier Transform, which we now present in its ﬁnal form.
We begin with a sampled signal on n = 2r sample points. To eﬃciently program the
Fast Fourier Transform, it helps to write out each index 0 ≤j < 2r in its binary (as
opposed to decimal) representation
j = jr−1 jr−2 . . . j2 j1 j0,
where
jν = 0 or 1;
(5.128)
the notation is shorthand for its r digit binary expansion
j = j0 + 2j1 + 4j2 + 8j3 + · · · + 2r−1 jr−1.
We then deﬁne the bit reversal map
ρ(jr−1 jr−2 . . . j2 j1 j0) = j0 j1 j2 . . . jr−2 jr−1.
(5.129)
For instance, if r = 5, and j = 13, with 5 digit binary representation 01101, then ρ(j) = 22
has the reversed binary representation 10110. Note especially that the bit reversal map
ρ = ρr depends upon the original choice of r = log2 n.
Secondly, for each 0 ≤k < r, deﬁne the maps
αk(j) = jr−1 . . . jk+1 0 jk−1 . . . j0,
βk(j) = jr−1 . . . jk+1 1 jk−1 . . . j0 = αk(j) + 2k,
for
j = jr−1 jr−2 . . . j1 j0.
(5.130)
In other words, αk(j) sets the kth binary digit of j to 0, while βk(j) sets it to 1. In the
preceding example, α2(13) = 9, with binary form 01001, while β2(13) = 13 with binary
form 01101. The bit operations (5.129, 130) are especially easy to implement on modern
binary computers.
Given a sampled signal f0, . . . , fn−1, its discrete Fourier coeﬃcients c0, . . . , cn−1 are
computed by the following iterative algorithm:
c(0)
j
= fρ(j),
c(k+1)
j
= 1
2

c(k)
αk(j) + ζ−j
2k+1 c(k)
βk(j)

,
j = 0, . . . , n −1,
k = 0, . . ., r −1,
(5.131)

298
5 Minimization and Least Squares
in which ζ2k+1 is the primitive 2k+1 root of unity.
The ﬁnal output of the iterative
procedure, namely
cj = c(r)
j ,
j = 0, . . . , n −1,
(5.132)
are the discrete Fourier coeﬃcients of our signal. The preprocessing step of the algorithm,
where we deﬁne c(0)
j , produces a more convenient rearrangement of the sample values. The
subsequent steps successively combine the Fourier coeﬃcients of the appropriate even and
odd sampled subsignals, reproducing (5.123) in a diﬀerent notation. The following example
should help make the overall process clearer.
Example 5.28.
Consider the case r = 3, and so our signal has n = 23 = 8 sampled
values f0, f1, . . . , f7. We begin the process by rearranging the sample values
c(0)
0
= f0, c(0)
1
= f4, c(0)
2
= f2, c(0)
3
= f6, c(0)
4
= f1, c(0)
5
= f5, c(0)
6
= f3, c(0)
7
= f7,
in the order speciﬁed by the bit reversal map ρ. For instance, ρ(3) = 6, or, in binary
notation, ρ(011) = 110.
The ﬁrst stage of the iteration is based on ζ2 = −1. Equation (5.131) gives
c(1)
0
= 1
2(c(0)
0
+ c(0)
1 ),
c(1)
1
= 1
2(c(0)
0
−c(0)
1 ),
c(1)
2
= 1
2(c(0)
2
+ c(0)
3 ),
c(1)
3
= 1
2(c(0)
2
−c(0)
3 ),
c(1)
4
= 1
2(c(0)
4
+ c(0)
5 ),
c(1)
5
= 1
2(c(0)
4
−c(0)
5 ),
c(1)
6
= 1
2(c(0)
6
+ c(0)
7 ),
c(1)
7
= 1
2(c(0)
6
−c(0)
7 ),
where we combine successive pairs of the rearranged sample values. The second stage of
the iteration has k = 1 with ζ4 = i . We obtain
c(2)
0
= 1
2(c(1)
0
+ c(1)
2 ),
c(2)
1
= 1
2(c(1)
1
−i c(1)
3 ),
c(2)
2
= 1
2(c(1)
0
−c(1)
2 ),
c(2)
3
= 1
2(c(1)
1
+ i c(1)
3 ),
c(2)
4
= 1
2(c(1)
4
+ c(1)
6 ),
c(2)
5
= 1
2(c(1)
5
−i c(1)
7 ),
c(2)
6
= 1
2(c(1)
4
−c(1)
6 ),
c(2)
7
= 1
2(c(1)
5
+ i c(1)
7 ).
Note that the indices of the combined pairs of coeﬃcients diﬀer by 2. In the last step,
where k = 2 and ζ8 =
√
2
2 (1 + i ), we combine coeﬃcients whose indices diﬀer by 4 = 22;
the ﬁnal output
c0 = c(3)
0
= 1
2(c(2)
0
+ c(2)
4 ),
c4 = c(3)
4
= 1
2(c(2)
0
−c(2)
4 ),
c1 = c(3)
1
= 1
2

c(2)
1
+
√
2
2 (1 −i ) c(2)
5

,
c5 = c(3)
5
= 1
2

c(2)
1
−
√
2
2 (1 −i ) c(2)
5

,
c2 = c(3)
2
= 1
2

c(2)
2
−i c(2)
6

,
c6 = c(3)
6
= 1
2

c(2)
2
+ i c(2)
6

,
c3 = c(3)
3
= 1
2

c(2)
3
−
√
2
2 (1 + i ) c(2)
7

,
c7 = c(3)
7
= 1
2

c(2)
3
+
√
2
2 (1 + i ) c(2)
7

,
is the complete set of discrete Fourier coeﬃcients.
Let us count the number of arithmetic operations required in the Fast Fourier Transform
algorithm. At each stage in the computation, we must perform n = 2r complex additions/
subtractions and the same number of complex multiplications. (Actually, the number of
multiplications is slightly smaller, since multiplications by ±1 and ± i are extremely simple.
However, this does not signiﬁcantly alter the ﬁnal operations count.) There are r = log2 n
stages, and so we require a total of r n = n log2 n complex additions/subtractions and the
same number of multiplications. Now, when n is large, n log2 n is signiﬁcantly smaller than
n2, which is the number of operations required for the direct algorithm. For instance, if
n = 210 = 1,024, then n2 = 1,048,576, while n log2 n = 10,240 — a net savings of 99%. As a
result, many large-scale computations that would be intractable using the direct approach
are immediately brought into the realm of feasibility. This is the reason why all modern

5.6 Discrete Fourier Analysis and the Fast Fourier Transform
299
implementations of the Discrete Fourier Transform are based on the FFT algorithm and
its variants.
The reconstruction of the signal from the discrete Fourier coeﬃcients c0, . . . , cn−1 is
speeded up in exactly the same manner. The only diﬀerences are that we replace ζ−1
n
= ζn
by ζn, and drop the factors of 1
2, since there is no need to divide by n in the ﬁnal result
(5.100). Therefore, we apply the slightly modiﬁed iterative procedure
f (0)
j
= cρ(j),
f (k+1)
j
= f (k)
αk(j) + ζj
2k+1 f (k)
βk(j),
j = 0, . . ., n −1,
k = 0, . . . , r −1,
(5.133)
and ﬁnish with
f(xj) = fj = f (r)
j
,
j = 0, . . . , n −1.
(5.134)
Example 5.29.
The reconstruction formulas in the case of n = 8 = 23 Fourier coeﬃcients
c0, . . . , c7, which were computed in Example 5.28, can be implemented as follows. First,
we rearrange the Fourier coeﬃcients in bit reversed order:
f (0)
0
= c0, f (0)
1
= c4, f (0)
2
= c2, f (0)
3
= c6, f (0)
4
= c1, f (0)
5
= c5, f (0)
6
= c3, f (0)
7
= c7.
Then we begin combining them in successive pairs:
f (1)
0
= f (0)
0
+ f (0)
1 ,
f (1)
1
= f (0)
0
−f (0)
1 ,
f (1)
2
= f (0)
2
+ f (0)
3 ,
f (1)
3
= f (0)
2
−f (0)
3 ,
f (1)
4
= f (0)
4
+ f (0)
5 ,
f (1)
5
= f (0)
4
−f (0)
5 ,
f (1)
6
= f (0)
6
+ f (0)
7 ,
f (1)
7
= f (0)
6
−f (0)
7 .
Next,
f (2)
0
= f (1)
0
+ f (1)
2 ,
f (2)
1
= f (1)
1
+ i f (1)
3 ,
f (2)
2
= f (1)
0
−f (1)
2 ,
f (2)
3
= f (1)
1
−i f (1)
3 ,
f (2)
4
= f (1)
4
+ f (1)
6 ,
f (2)
5
= f (1)
5
+ i f (1)
7 ,
f (2)
6
= f (1)
4
−f (1)
6 ,
f (2)
7
= f (1)
5
−i f (1)
7 .
Finally, the sampled signal values are
f(x0) = f (3)
0
= f (2)
0
+ f (2)
4 ,
f(x4) = f (3)
4
= f (2)
0
−f (2)
4 ,
f(x1) = f (3)
1
= f (2)
1
+
√
2
2 (1 + i ) f (2)
5 ,
f(x5) = f (3)
5
= f (2)
1
−
√
2
2 (1 + i ) f (2)
5 ,
f(x2) = f (3)
2
= f (2)
2
+ i f (2)
6 ,
f(x6) = f (3)
6
= f (2)
2
−i f (2)
6 ,
f(x3) = f (3)
3
= f (2)
3
−
√
2
2 (1 −i ) f (2)
7 ,
f(x7) = f (3)
7
= f (2)
3
+
√
2
2 (1 −i ) f (2)
7 .
Exercises
♠5.6.17. Use the Fast Fourier Transform to ﬁnd the discrete Fourier coeﬃcients for the the
following functions using the indicated number of sample points. Carefully indicate each
step in your analysis.
(a) x
π , n = 4;
(b) sin x, n = 8;
(c) | x −π |, n = 8;
(d) sign(x −π), n = 16.
♠5.6.18. Use the Inverse Fast Fourier Transform to reassemble the sampled function data
corresponding to the following discrete Fourier coeﬃcients. Carefully indicate each step in
your analysis.
(a) c0 = c2 = 1, c1 = c3 = −1,
(b) c0 = c1 = c4 = 2, c2 = c6 = 0, c3 = c5 = c7 = −1.

300
5 Minimization and Least Squares
♥5.6.19. In this exercise, we show how the Fast Fourier Transform is equivalent to a certain
matrix factorization. Let c = ( c0, c1, . . . , c7 )T be the vector of Fourier coeﬃcients,
and let f(k) = (f(k)
0
, f(k)
1
, . . . , f(k)
7
)T for k = 0, 1, 2, 3, be the vectors containing
the coeﬃcients deﬁned in the reconstruction algorithm Example 5.29. (a) Show that
f(0) = M0c, f(1) = M1f(0), f(2) = M2f(1), f = f(3) = M3f(2), where M0, M1, M2, M3
are 8 × 8 matrices. Write down their explicit forms. (b) Explain why the matrix product
F8 = M3M2M1M0 reproduces the Fourier matrix derived in Exercise 5.6.9. Check the
factorization directly. (c) Write down the corresponding matrix factorization for the direct
algorithm of Example 5.28.

Chapter 6
Equilibrium
In this chapter, we will apply what we have learned so far to the analysis of equilibrium
conﬁgurations and stability of mechanical structures and electrical networks. Both phys-
ical problems ﬁt into a common, and surprisingly general, mathematical framework. The
physical laws of equilibrium mechanics and circuits lead to linear algebraic systems whose
coeﬃcient matrix is of positive (semi-)deﬁnite Gram form. The positive deﬁnite cases corre-
spond to stable structures and networks, which can support any applied forcing or external
current, producing a unique, stable equilibrium solution that can be characterized by an
energy minimization principle. On the other hand, systems with semi-deﬁnite coeﬃcient
matrices model unstable structures and networks that are unable to remain in equilibrium
except under very special conﬁgurations of external forces. In the case of mechanical struc-
tures, the instabilities are of two types: rigid motions, in which the structure moves while
maintaining its overall geometrical shape, and mechanisms, in which it spontaneously de-
forms in the absence of any applied force. The same linear algebra framework, but now
reformulated for inﬁnite-dimensional function space, also characterizes the boundary value
problems for both ordinary and partial diﬀerential equation that model the equilibria of
continuous media, including bars, beams, solid bodies, and many other systems arising
throughout physics and engineering, [61, 79].
The starting point is a linear chain of masses interconnected by springs and constrained
to move only in the longitudinal direction. Our general mathematical framework is already
manifest in this rather simple mechanical system. In the second section, we discuss simple
electrical networks consisting of resistors, current sources and/or batteries, interconnected
by a network of wires. Here, the resulting Gram matrix is known as the graph Laplacian,
which plays an increasingly important role in modern data analysis and network theory.
Finally, we treat small (so as to remain in a linear modeling regime) displacements of
two- and three-dimensional structures constructed out of elastic bars.
In all cases, we
consider only the equilibrium solutions. Dynamical (time-varying) processes for each of
these physical systems are governed by linear systems of ordinary diﬀerential equations, to
be formulated and analyzed in Chapter 10.
6.1 Springs and Masses
A mass–spring chain consists of n masses m1, m2, . . . mn arranged in a straight line. Each
mass is connected to its immediate neighbor(s) by springs. Moreover, the chain may be
connected at one or both ends to a ﬁxed support by a spring — or may even be completely
free, e.g., ﬂoating in outer space. For speciﬁcity, let us ﬁrst look at the case when both
ends of the chain are attached to unmoving supports, as illustrated in Figure 6.1
We assume that the masses are arranged in a vertical line, and order them from top to
bottom. For simplicity, we will only allow the masses to move in the vertical direction, that
is, we restrict to a one-dimensional motion. (Section 6.3 deals with the more complicated
two- and three-dimensional situations.)
© Springer International Publishing AG, part of Springer Nature 2018 
https://doi.org/10.1007/978-3-319-91041-3_6 
301
P. J. Olver, C. Shakiban, Applied Linear Algebra, Undergraduate Texts in Mathematics, 

302
6 Equilibrium
m1
m2
m3
Figure 6.1.
A Mass–Spring Chain with Fixed Ends.
If we subject some or all of the masses to an external force, e.g., gravity, then the system
will move† to a new equilibrium position. The resulting position of the ith mass is measured
by its displacement ui from its original position, which, since we are only allowing vertical
motion, is a scalar quantity. Referring to Figure 6.1, we use the convention that ui > 0
if the mass has moved downwards, and ui < 0 if it has moved upwards. Our goal is to
determine the new equilibrium conﬁguration of the chain under the prescribed forcing, that
is, to set up and solve a system of equations for the displacements u1, . . . , un.
As sketched in Figure 6.2, let ej denote the elongation of the jth spring, which connects
mass mj−1 to mass mj. By “elongation”, we mean how far the spring has been stretched,
so that ej > 0 if the spring is longer than its reference length, while ej < 0 if the spring
has been compressed. The elongations of the internal springs can be determined directly
from the displacements of the masses at each end according to the geometric formula
ej = uj −uj−1,
j = 2, . . . , n,
(6.1)
while, for the top and bottom springs,
e1 = u1,
en+1 = −un,
(6.2)
since the supports are not allowed to move. We write the elongation equations (6.1–2) in
matrix form
e = Au,
(6.3)
where e =
⎛
⎜
⎜
⎜
⎝
e1
e2
...
en+1
⎞
⎟
⎟
⎟
⎠is the elongation vector, u =
⎛
⎜
⎜
⎜
⎝
u1
u2
...
un
⎞
⎟
⎟
⎟
⎠is the displacement vector, and
†
The diﬀerential equations governing its dynamical behavior during the motion will be the
subject of Chapter 10. Damping or frictional eﬀects will cause the system to eventually settle
down into a stable equilibrium conﬁguration, if such exists.

6.1 Springs and Masses
303
mj
mj−1
uj
uj−1
ej
Figure 6.2.
Elongation of a Spring.
the coeﬃcient matrix
A =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
1
−1
1
−1
1
−1
1
...
...
−1
1
−1
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
(6.4)
has size (n + 1) × n, with only its non-zero entries being indicated.
We refer to A as
the reduced incidence matrix † for the mass–spring chain. The incidence matrix eﬀectively
encodes the underlying geometry of the system, including the ﬁxed “boundary conditions”
at the top and the bottom.
The next step is to relate the elongation ej experienced by the jth spring to its internal
force yj. This is the basic constitutive assumption, which relates geometry to kinematics.
In the present case, we suppose that the springs are not stretched (or compressed) particu-
larly far. Under this assumption, Hooke’s Law, named in honor of the seventeenth-century
English scientist and inventor Robert Hooke, states that the internal force is directly pro-
portional to the elongation — the more you stretch a spring, the more it tries to pull you
back. Thus,
yj = cj ej,
(6.5)
where the constant of proportionality cj > 0 measures the spring’s stiﬀness. Hard springs
have large stiﬀness and so takes a large force to stretch, whereas soft springs have a small,
but still positive, stiﬀness. We will also write the constitutive equations (6.5) in matrix
form
y = C e,
(6.6)
†
The connection with the incidence matrix of a graph, as introduced in Section 2.6, will become
evident in the following Section 6.2.

304
6 Equilibrium
mi
yi
yi+1
Figure 6.3.
Force Balance.
where
y =
⎛
⎜
⎜
⎜
⎝
y1
y2
...
yn+1
⎞
⎟
⎟
⎟
⎠,
C =
⎛
⎜
⎜
⎝
c1
c2
...
cn+1
⎞
⎟
⎟
⎠
are the internal force vector and the matrix of spring stiﬀnesses. Note particularly that C
is a diagonal matrix, and, more importantly, positive deﬁnite, C > 0, since all its diagonal
entries are strictly positive.
Finally, the forces must balance if the system is to remain in equilibrium.
In this
simpliﬁed model, the external forces act only on the masses, and not on the springs. Let
fi denote the external force on the ith mass mi. We also measure force in the downward
direction, so fi > 0 means that the force is pulling the ith mass downward. (In particular,
gravity would induce a positive force on each mass.) If the ith spring is stretched, it will
exert an upward force on mi, while if the (i + 1)st spring is stretched, it will pull mi
downward. Therefore, the balance of forces on mi requires that
fi = yi −yi+1.
(6.7)
The vectorial form of the force balance law is
f = AT y,
(6.8)
where f = (f1, . . . , fn)T . The remarkable fact is that the force balance coeﬃcient matrix
AT =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎝
1
−1
1
−1
1
−1
1
−1
...
...
1
−1
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎠
(6.9)
is the transpose of the reduced incidence matrix (6.4) for the chain.
This connection
between geometry and force balance turns out to be of almost universal applicability, and

6.1 Springs and Masses
305
is the reason underlying the positivity of the ﬁnal coeﬃcient matrix in the resulting system
of equilibrium equations.
Summarizing, the basic geometrical and physical properties of our mechanical system
lead us to the full system of equilibrium equations (6.3, 6, 8) relating its displacements u,
elongations e, internal forces y, and external forces f:
e = Au,
y = C e,
f = AT y.
(6.10)
These equations imply f = AT y = AT C e = AT C Au, and hence can be combined into a
single linear system
Ku = f,
where
K = AT C A
(6.11)
is called the stiﬀness matrix associated with the entire mass–spring chain. In the particular
case under consideration,
K =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
c1 + c2
−c2
−c2
c2 + c3
−c3
−c3
c3 + c4
−c4
−c4
c4 + c5
−c5
...
...
...
−cn−1
cn−1 + cn
−cn
−cn
cn + cn+1
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
(6.12)
has a very simple symmetric, tridiagonal form. As such, we can use the tridiagonal solution
algorithm of Section 1.7 to rapidly solve the linear system (6.11) for the displacements of
the masses. Once we have solved (6.11) for the displacements u we can then compute the
resulting elongations e and internal forces y by substituting into the original system (6.10).
Example 6.1.
Let us consider the particular case of n = 3 masses connected by identical
springs with unit spring constant. Thus, c1 = c2 = c3 = c4 = 1, and C = diag (1, 1, 1, 1) = I
is the 4 × 4 identity matrix. The 3 × 3 stiﬀness matrix is then
K = ATA =
⎛
⎝
1
−1
0
0
0
1
−1
0
0
0
1
−1
⎞
⎠
⎛
⎜
⎝
1
0
0
−1
1
0
0
−1
1
0
0
−1
⎞
⎟
⎠=
⎛
⎝
2
−1
0
−1
2
−1
0
−1
2
⎞
⎠.
A straightforward Gaussian Elimination produces the K = LDLT factorization
⎛
⎝
2
−1
0
−1
2
−1
0
−1
2
⎞
⎠=
⎛
⎝
1
0
0
−1
2
1
0
0
−2
3
1
⎞
⎠
⎛
⎝
2
0
0
0
3
2
0
0
0
4
3
⎞
⎠
⎛
⎝
1
−1
2
0
0
1
−2
3
0
0
1
⎞
⎠.
(6.13)
With this in hand, we can solve the basic equilibrium equations Ku = f by the usual
Forward and Back Substitution algorithm.
Remark. Even though we construct K = AT C A and then factor it as K = LDLT , there
is no direct algorithm to get from A and C to L and D, which, typically, are matrices of
diﬀerent sizes.
Suppose, for example, we pull the middle mass downwards with a unit force, so f2 = 1
while f1 = f3 = 0. Then f = ( 0, 1, 0 )T , and the solution to the equilibrium equations

306
6 Equilibrium
(6.11) is u =
 1
2, 1, 1
2
T, whose entries prescribe the mass displacements. Observe that
all three masses have moved down, with the middle mass moving twice as far as the other
two.
The corresponding spring elongations and internal forces are obtained by matrix
multiplication
y = e = Au =
⎛
⎜
⎜
⎜
⎝
1
2
1
2
−1
2
−1
2
⎞
⎟
⎟
⎟
⎠,
since C = I . Thus, the top two springs are elongated, while the bottom two are compressed,
all by an equal amount.
Similarly, if all the masses are equal, m1 = m2 = m3 = m, then the solution under a
constant downwards gravitational force f = ( mg, mg, mg )T is
u = K−1
⎛
⎜
⎝
mg
mg
mg
⎞
⎟
⎠=
⎛
⎜
⎝
3
2 mg
2 mg
3
2 mg
⎞
⎟
⎠,
and
y = e = Au =
⎛
⎜
⎜
⎜
⎝
3
2 mg
1
2 mg
−1
2 mg
−3
2 mg
⎞
⎟
⎟
⎟
⎠.
Now, the middle mass has only moved 33% farther than the others, whereas the top
and bottom springs are experiencing three times as much elongation/compression as the
middle two springs.
An important observation is that we cannot determine the internal forces y or elon-
gations e directly from the force balance law (6.8), because the transposed matrix AT is
not square, and so the system f = AT y does not have a unique solution. We must ﬁrst
compute the displacements u by solving the full equilibrium equations (6.11), and then
use the resulting displacements to reconstruct the elongations and internal forces. Such
systems are referred to as statically indeterminate.
The behavior of the system will depend on both the forcing and the boundary conditions.
Suppose, by way of contrast, that we ﬁx only the top of the chain to a support, and leave
the bottom mass hanging freely, as in Figure 6.4. The geometric relation between the
displacements and the elongations has the same form (6.3) as before, but the reduced
incidence matrix is slightly altered:
A =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎝
1
−1
1
−1
1
−1
1
...
...
−1
1
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎠
.
(6.14)
This matrix has size n×n and is obtained from the preceding example (6.4) by eliminating
the last row corresponding to the missing bottom spring. The constitutive equations are
still governed by Hooke’s law y = C e, as in (6.6), with C = diag (c1, . . . , cn) the n × n
diagonal matrix of spring stiﬀnesses. Finally, the force balance equations are also found
to have the same general form f = AT y as in (6.8), but with the transpose of the revised
incidence matrix (6.14). In conclusion, the equilibrium equations Ku = f have an identical

6.1 Springs and Masses
307
m1
m2
m3
Figure 6.4.
A Mass–Spring Chain with One Free End.
form (6.11), based on the revised stiﬀness matrix
K = AT CA =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
c1 + c2
−c2
−c2
c2 + c3
−c3
−c3
c3 + c4
−c4
−c4
c4 + c5
−c5
...
...
...
−cn−1
cn−1 + cn
−cn
−cn
cn
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
(6.15)
Only the bottom right entry diﬀers from the ﬁxed end matrix (6.12).
This system is called statically determinate, because the incidence matrix A is square
and nonsingular, and so it is possible to solve the force balance law (6.8) directly for the
internal forces y = A−T f without having to solve the full equilibrium equations for the
displacements u before computing the internal forces y = C Au.
Example 6.2.
For a three mass chain with one free end and equal unit spring constants
c1 = c2 = c3 = 1, the stiﬀness matrix is
K = ATA =
⎛
⎝
1
−1
0
0
1
−1
0
0
1
⎞
⎠
⎛
⎝
1
0
0
−1
1
0
0
−1
1
⎞
⎠=
⎛
⎝
2
−1
0
−1
2
−1
0
−1
1
⎞
⎠.
Pulling the middle mass downwards with a unit force, whereby f = ( 0, 1, 0 )T , results in
the displacements
u = K−1f =
⎛
⎝
1
2
2
⎞
⎠,
so that
y = e = Au =
⎛
⎝
1
1
0
⎞
⎠.
In this conﬁguration, the bottom two masses have moved by the same amount, which is
twice as far as the top mass. Because we are pulling only on the middle mass, the bottom
spring hangs free and experiences no elongation, whereas the top two springs are stretched
by the same amount.
Similarly, for a chain of equal masses subject to a constant downwards gravitational

308
6 Equilibrium
force f = ( mg, mg, mg )T , the equilibrium position is
u = K−1
⎛
⎝
mg
mg
mg
⎞
⎠=
⎛
⎝
3mg
5mg
6mg
⎞
⎠,
and
y = e = Au =
⎛
⎝
3mg
2mg
mg
⎞
⎠.
Note how much farther the masses have moved now that the restraining inﬂuence of the
bottom support has been removed. The top spring is experiencing the most elongation,
and is thus the most likely to break, because it must support all three masses.
Exercises
6.1.1. A mass–spring chain consists of two masses connected to two ﬁxed supports. The spring
constants are c1 = c3 = 1 and c2 = 2. (a) Find the stiﬀness matrix K. (b) Solve the
equilibrium equations Ku = f when f = ( 4, 3 )T . (c) Which mass moved the farthest?
(d) Which spring has been stretched the most? Compressed the most?
6.1.2. Solve Exercise 6.1.1 when the ﬁrst and second springs are interchanged, c1 = 2,
c2 = c3 = 1. Which of your conclusions changed?
6.1.3. Redo Exercises 6.1.1–2 when the bottom support and spring are removed.
6.1.4. A mass–spring chain consists of four masses suspended between two ﬁxed supports.
The spring stiﬀnesses are c1 = 1, c2 = 1
2, c3 = 2
3, c4 = 1
2, c5 = 1. (a) Determine the
equilibrium positions of the masses and the elongations of the springs when the external
force is f = ( 0, 1, 1, 0 )T . Is your solution unique? (b) Suppose we ﬁx only the top support.
Solve the problem with the same data and compare your results.
6.1.5.(a) Show that, in a mass–spring chain with two ﬁxed ends, under any external force, the
average elongation of the springs is zero:
1
n + 1(e1 + · · · + en+1) = 0. (b) What can you say
about the average elongation of the springs in a chain with one ﬁxed end?
♦6.1.6. Suppose we subject the ith mass (and no others) in a chain to a unit force, and then
measure the resulting displacement of the jth mass. Prove that this is the same as the
displacement of the ith mass when the chain is subject to a unit force on the jth mass.
Hint: See Exercise 1.6.20.
♣6.1.7. Find the displacements u1, u2, . . . , u100 of 100 masses connected in a row by identical
springs, with spring constant c = 1. Consider the following three types of force functions:
(a) Constant force: f1 = · · · = f100 = .01; (b) Linear force: fi = .0002 i; (c) Quadratic
force: fi = 6 · 10−6 i (100 −i). Also consider two diﬀerent boundary conditions at the
bottom: (i) spring 101 connects the last mass to a support; (ii) mass 100 hangs free at the
end of the line of springs. Graph the displacements and elongations in all six cases. Discuss
your results; in particular, comment on whether they agree with your physical intuition.
6.1.8.(a) Suppose you are given three springs with respective stiﬀnesses c = 1, c′ = 2, c′′ = 3.
In what order should you connect them to three masses and a top support so that the
bottom mass goes down the farthest under a uniform gravitational force?
(b) Answer Exercise 6.1.8 when the springs connect two masses to top and bottom supports.
♣6.1.9. Generalizing Exercise 6.1.8, suppose you are given n diﬀerent springs. (a) In which
order should you connect them to n masses and a top support so that the bottom mass
goes down the farthest under a uniform gravitational force? Does your answer depend upon
the relative sizes of the spring constants?
(b) Answer the same question when the springs
connect n −1 masses to both top and bottom supports.

6.1 Springs and Masses
309
6.1.10. Find the LDLT factorization of an n × n tridiagonal matrix whose diagonal entries are
all equal to 2 and whose sub- and super-diagonal entries are all equal to −1. Hint: Start
with the 3 × 3 case (6.13), and then analyze a slightly larger one to spot the pattern.
♥6.1.11. In a statically indeterminate situation, the equations AT y = f do not have a unique
solution for the internal forces y in terms of the external forces f. (a) Prove that,
nevertheless, if C = I , the internal forces are the unique solution of minimal Euclidean
norm, as given by Theorem 4.50.
(b) Use this method to directly ﬁnd the internal force
for the system in Example 6.1. Make sure that your values agree with those in the example.
Positive Deﬁniteness and the Minimization Principle
You may have already observed that the stiﬀness matrix K = AT C A of a mass–spring
chain has the form of a Gram matrix, cf. (3.64), for the weighted inner product ⟨v , w ⟩=
vT C w induced by the diagonal matrix of spring stiﬀnesses. Moreover, since A has linearly
independent columns (which should be checked), and C is positive deﬁnite, Theorem 3.37
tells us that the stiﬀness matrix is positive deﬁnite: K > 0. In particular, Theorem 3.43
guarantees that K is nonsingular, and hence the linear system (6.11) has a unique solution
u = K−1f.
We can therefore conclude that the mass–spring chain assumes a unique
equilibrium position under an arbitrary external force. However, one must keep in mind
that this is a mathematical result and may not hold in all physical situations. Indeed, we
should anticipate that a very large force will take us outside the regime covered by the
linear Hooke’s law relation (6.5), and render our simple mathematical model physically
irrelevant.
According to Theorem 5.2, when the coeﬃcient matrix of a linear system is positive
deﬁnite, the equilibrium solution can be characterized by a minimization principle. For
mass–spring chains, the quadratic function to be minimized has a physical interpreta-
tion: it is the potential energy of the system. Nature is parsimonious with energy, so a
physical system seeks out an energy-minimizing equilibrium conﬁguration. Energy min-
imization principles are of almost universal validity, and can be advantageously used for
the construction of mathematical models, as well as their solutions, both analytical and
numerical.
The energy function to be minimized can be determined directly from physical prin-
ciples. For a mass–spring chain, the potential energy of the ith mass equals the product
of the applied force and the displacement: −fi ui. The minus sign is the result of our
convention that a positive displacement ui > 0 means that the mass has moved down,
and hence decreased its potential energy. Thus, the total potential energy due to external
forcing on all the masses in the chain is
−
n

i=1
fi ui = −uTf.
Next, we calculate the internal energy of the system. In a single spring elongated by an
amount e, the work done by the internal forces y = ce is stored as potential energy, and
so is calculated by integrating the force over the elongated distance:
 e
0
y de =
 e
0
c e de = 1
2 c e2.

310
6 Equilibrium
Totaling the contributions from each spring, we ﬁnd the internal spring energy to be
1
2
n

i=1
ci e2
i = 1
2 eT C e = 1
2 uT AT CAu = 1
2 uTKu,
where we used the incidence equation e = Au relating elongation and displacement. There-
fore, the total potential energy is
p(u) = 1
2 uTKu −uTf.
(6.16)
Since K > 0, Theorem 5.2 implies that this quadratic function has a unique minimizer
that satisﬁes the equilibrium equation Ku = f.
Example 6.3.
For the three mass chain with two ﬁxed ends described in Example 6.1,
the potential energy function (6.16) has the explicit form
p(u) = 1
2 ( u1
u2
u3 )
⎛
⎝
2
−1
0
−1
2
−1
0
−1
2
⎞
⎠
⎛
⎝
u1
u2
u3
⎞
⎠−( u1
u2
u3 )
⎛
⎝
f1
f2
f3
⎞
⎠
= u2
1 −u1 u2 + u2
2 −u2 u3 + u2
3 −f1 u1 −f2 u2 −f3 u3,
where f = ( f1, f2, f3 )T is the external forcing. The minimizer of this particular quadratic
function gives the equilibrium displacements u = ( u1, u2, u3 )T of the three masses.
Exercises
6.1.12. Prove directly that the stiﬀness matrices in Examples 6.1 and 6.2 are positive deﬁnite.
6.1.13. Write down the potential energy for the following mass–spring chains with identical
unit springs when subject to a uniform gravitational force: (a) three identical masses
connected to only a top support. (b) four identical masses connected to top and bottom
supports. (c) four identical masses connected only to a top support.
6.1.14.(a) Find the total potential energy of the equilibrium conﬁguration of the mass–spring
chain in Exercise 6.1.1. (b) Test the minimum principle by substituting three other possible
displacements of the masses and checking that they all have larger potential energy.
6.1.15. Answer Exercise 6.1.14 for the mass–spring chain in Exercise 6.1.4.
6.1.16. Describe the mass–spring chains that gives rise to the following potential energy
functions, and ﬁnd their equilibrium conﬁguration: (a) 3u2
1 −4u1 u2 + 3u2
2 + u1 −3u2,
(b) 5u2
1 −6u1 u2 + 3u2
2 + 2u2, (c) 2u2
1 −3u1 u2 + 4u2
2 −5u2 u3 + 5
2 u2
3 −u1 −u2 + u3,
(d) 2u2
1 −u1 u2 + u2
2 −u2 u3 + u2
3 −u3 u4 + 2u2
4 + u1 −2u3.
6.1.17. Explain why the columns of the reduced incidence matrices (6.4) and (6.14) are linearly
independent.
6.1.18. Suppose that when subject to a nonzero external force f ̸= 0, a mass–spring chain has
equilibrium position u⋆. Prove that the potential energy is strictly negative at equilibrium:
p(u⋆) < 0.
♥6.1.19. Return to the situation investigated in Exercise 6.1.8. How should you arrange the
springs in order to minimize the potential energy in the resulting mass–spring chain?
6.1.20. True or false: The potential energy function uniquely determines the mass–spring
chain.

6.2 Electrical Networks
311
u1
u2
u3
u4
R1
R2
R3
R4
R5
Figure 6.5.
A Simple Electrical Network.
6.2 Electrical Networks
By an electrical network, we mean a collection of (insulated) wires that are joined together
at their ends. The junctions connecting the ends of one or more wires are called nodes.
Mathematically, we can view any such electrical network as a graph, the wires being the
edges and the nodes the vertices. As before, to avoid technicalities, we will assume that
the underlying graph is simple, meaning that there are no loops and at most one edge
connecting any two vertices. To begin with, we further assume that there are no electrical
devices (batteries, inductors, capacitors, etc.) in the network, and so the only impediments
to the current ﬂowing through the network are the resistances in the wires. As we shall
see, resistance (or rather its reciprocal) plays a very similar role to that of spring stiﬀness.
Thus, the network corresponds to a weighted graph in which the weight of an edge is the
number representing the resistance of the corresponding wire. We shall feed a current into
the network at one or more of the nodes, and would like to determine how the induced
current ﬂows through the wires. The basic equations governing the equilibrium voltages and
currents in such a network follow from the three fundamental laws of electricity, named
after the pioneering nineteenth-century German physicists Gustav Kirchhoﬀand Georg
Ohm, two of the founders of electric circuit theory, [58].
Voltage is deﬁned as the electromotive force that moves electrons through a wire. An
individual wire’s voltage is determined by the diﬀerence in the voltage potentials at its two
ends — just as the gravitational force on a mass is induced by a diﬀerence in gravitational
potential. To quantify voltage, we need to ﬁx an orientation for the wire. A positive voltage
will mean that the electrons move in the chosen direction, while a negative voltage causes
them to move in reverse. The original choice of orientation is arbitrary, but once assigned
will pin down the sign conventions to be used by voltages, currents, etc. To this end, we
draw a digraph to represent the network, whose edges represent wires and whose vertices
represent nodes. Each edge is assigned an orientation that indicates the wire’s starting and
ending nodes. A simple example consisting of ﬁve wires joined at four diﬀerent nodes can
be seen in Figure 6.5. The arrows indicate the selected directions for the wires, the wavy
lines are the standard electrical symbols for resistance, while the resistances provide the
edge weights in the resulting weighted digraph.
In an electrical network, each node will have a voltage potential, denoted by ui. If wire
k starts at node i and ends at node j under its assigned orientation, then its voltage vk

312
6 Equilibrium
equals the diﬀerence between the voltage potentials at its ends:
vk = ui −uj.
(6.17)
Note that vk > 0 if ui > uj, indicating that the electrons ﬂow from the starting node i
to the ending node j. In our particular illustrative example, the ﬁve wires have respective
voltages
v1 = u1 −u2,
v2 = u1 −u3,
v3 = u1 −u4,
v4 = u2 −u4,
v5 = u3 −u4.
Let us rewrite this linear system of equations in vector form
v = Au,
(6.18)
where
A =
⎛
⎜
⎜
⎜
⎝
1
−1
0
0
1
0
−1
0
1
0
0
−1
0
1
0
−1
0
0
1
−1
⎞
⎟
⎟
⎟
⎠.
(6.19)
The alert reader will recognize the incidence matrix (2.46) for the digraph deﬁned by the
network. This is true in general — the voltages along the wires of an electrical network are
related to the potentials at the nodes by a linear system of the form (6.18), in which A is
the incidence matrix of the network digraph. The rows of the incidence matrix are indexed
by the wires, and the columns by the nodes. Each row of the matrix A has a single +1
in the column indexed by the starting node of the associated wire, and a single −1 in the
column of the ending node.
Kirchhoﬀ’s Voltage Law states that the sum of the voltages around each closed circuit
in the network is zero. For example, in the network under consideration, summing the
voltages around the left-hand triangular circuit gives
v1 + v4 −v3 = (u1 −u2) + (u2 −u4) −(u1 −u4) = 0.
Note that v3 appears with a minus sign, since we must traverse wire 3 in the opposite
direction to its assigned orientation when going around the circuit in the counterclockwise
direction.
The voltage law is a direct consequence of (6.18).
Indeed, as discussed in
Section 2.6, the circuits can be identiﬁed with vectors ℓ∈cokerA = ker AT in the cokernel
of the incidence matrix, and so
ℓ· v = ℓT v = ℓTAu = 0.
(6.20)
Therefore, orthogonality of the voltage vector v to the circuit vector ℓis the mathematical
formalization of Kirchhoﬀ’s Voltage Law.
Given a prescribed set of voltages v along the wires, can one ﬁnd corresponding voltage
potentials u at the nodes?
To answer this question, we need to solve v = Au, which
requires v ∈img A. According to the Fredholm Alternative Theorem 4.46, the necessary
and suﬃcient condition for this to hold is that v be orthogonal to coker A. Theorem 2.53
says that the cokernel of an incidence matrix is spanned by the circuit vectors, and so v is
a possible set of voltages if and only if v is orthogonal to all the circuit vectors ℓ∈cokerA,
i.e., the Voltage Law is necessary and suﬃcient for the given voltages to be physically
realizable in the network.
Kirchhoﬀ’s Law is related to the topology of the network — how the diﬀerent wires are
connected together. Ohm’s Law is a constitutive relation, indicating what the wires are

6.2 Electrical Networks
313
made of. The resistance along a wire (including any added resistors) prescribes the relation
between voltage and current or the rate of ﬂow of electric charge. The law reads
vk = Rk yk,
(6.21)
where vk is the voltage, Rk is the resistance, and yk (often denoted by Ik in the engineering
literature) denotes the current along wire k.
Thus, for a ﬁxed voltage, the larger the
resistance of the wire, the smaller the current that ﬂows through it.
The direction of
the current is also prescribed by our choice of orientation of the wire, so that yk > 0 if
the current is ﬂowing from the starting to the ending node. We combine the individual
equations (6.21) into a single vector equation
v = R y,
(6.22)
where the resistance matrix R = diag (R1, . . . , Rn) > 0 is diagonal and positive deﬁnite.
We shall, in analogy with (6.6), replace (6.22) by the inverse relationship
y = C v,
(6.23)
where C = R−1 is the conductance matrix, again diagonal, positive deﬁnite, whose entries
are the conductances ck = 1/Rk of the wires. For the particular network in Figure 6.5,
C =
⎛
⎜
⎜
⎜
⎝
c1
0
0
0
0
0
c2
0
0
0
0
0
c3
0
0
0
0
0
c4
0
0
0
0
0
c5
⎞
⎟
⎟
⎟
⎠=
⎛
⎜
⎜
⎜
⎝
1/R1
0
0
0
0
0
1/R2
0
0
0
0
0
1/R3
0
0
0
0
0
1/R4
0
0
0
0
0
1/R5
⎞
⎟
⎟
⎟
⎠= R−1.
(6.24)
Finally, we stipulate that electric current is not allowed to accumulate at any node, i.e.,
every electron that arrives at a node must leave along one of the wires. Let yk, yl, . . . , ym
denote the currents along all the wires k, l, . . ., m that meet at node i in the network, and
fi an external current source, if any, applied at node i. Kirchhoﬀ’s Current Law requires
that the net current leaving the node along the wires equals the external current coming
into the node, and so
± yk ± yl ± · · · ± ym = fi.
(6.25)
Each ± sign is determined by the orientation of the wire, with + if node i is its starting
node and −if it is its ending node.
In our particular example, suppose that we send a 1 amp current source into the ﬁrst
node. Then Kirchhoﬀ’s Current Law requires
y1 + y2 + y3 = 1,
−y1 + y4 = 0,
−y2 + y5 = 0,
−y3 −y4 −y5 = 0,
the four equations corresponding to the four nodes in our network. The vector form of this
linear system is
AT y = f,
(6.26)
where y = ( y1, y2, y3, y4, y5 )T are the currents along the ﬁve wires, and f = ( 1, 0, 0, 0 )T
represents the current sources at the four nodes. The coeﬃcient matrix
AT =
⎛
⎜
⎝
1
1
1
0
0
−1
0
0
1
0
0
−1
0
0
1
0
0
−1
−1
−1
⎞
⎟
⎠
(6.27)

314
6 Equilibrium
is the transpose of the incidence matrix (6.19). As in the mass–spring chain, this is a
remarkable general fact, which follows directly from Kirchhoﬀ’s two laws. The coeﬃcient
matrix for the Current Law is the transpose of the incidence matrix for the Voltage Law.
Let us assemble the full system of equilibrium equations (6.18, 23, 26):
v = Au,
y = C v,
f = AT y.
(6.28)
Remarkably, we arrive at a system of linear relations that has an identical form to the
mass–spring chain system (6.10), albeit with diﬀerent physical quantities and diﬀerent
coeﬃcient matrices. As before, they combine into a single linear system
Ku = f,
where
K = AT C A
(6.29)
is known as the resistivity matrix associated with the network. In our particular example,
combining (6.19, 24, 27) produces the resistivity matrix
K = AT C A =
⎛
⎜
⎝
c1 + c2 + c3
−c1
−c2
−c3
−c1
c1 + c4
0
−c4
−c2
0
c2 + c5
−c5
−c3
−c4
−c5
c3 + c4 + c5
⎞
⎟
⎠,
(6.30)
whose entries depend on the conductances of the ﬁve wires in the network.
Remark.
There is a simple pattern to the resistivity matrix, evident in (6.30).
The
diagonal entries kii equal the sum of the conductances of all the wires having node i at one
end. The non-zero oﬀ-diagonal entries kij, i ̸= j, equal −ck, the conductance of the wire†
joining node i to node j, while kij = 0 if there is no wire joining the two nodes.
Consider the case in which all the wires in our network have equal unit resistance, and
so ck = 1/Rk = 1 for k = 1, . . ., 5. Then the resistivity matrix is
K =
⎛
⎜
⎝
3
−1
−1
−1
−1
2
0
−1
−1
0
2
−1
−1
−1
−1
3
⎞
⎟
⎠.
(6.31)
However, when trying to solve the linear system (6.29), we run into an immediate diﬃculty:
there is no solution! The matrix (6.31) is not positive deﬁnite — it is a singular matrix.
Moreover, the particular current source vector f = ( 1, 0, 0, 0 )T does not lie in the image
of K. Something is clearly amiss.
Before getting discouraged, let us sit back and use a little physical intuition. We are
trying to put a 1 amp current into the network at node 1. Where can the electrons go?
The answer is nowhere — they are all trapped in the network and, as they accumulate,
something drastic will happen — sparks will ﬂy! This is clearly an unstable situation, and
so the fact that the equilibrium equations do not have a solution is trying to tell us that
the physical system cannot remain in a steady state. The physics rescues the mathematics,
or, vice versa, the mathematics elucidates the underlying physical processes.
In order to achieve equilibrium in an electrical network, we must remove as much current
as we put in. Thus, if we feed a 1 amp current into node 1, then we must extract a total of
†
This assumes that there is only one wire joining the two nodes.

6.2 Electrical Networks
315
1 amp’s worth of current from the other nodes. In other words, the sum of all the external
current sources must vanish:
f1 + f2 + · · · + fn = 0,
and so there is no net current being fed into the network. Suppose we also extract a 1 amp
current from node 4; then the modiﬁed current source vector f = ( 1, 0, 0, −1 )T indeed lies
in the image of K, as you can check, and the equilibrium system (6.29) has a solution.
This is all well and good, but we are not out of the woods yet. As we know, if a linear
system has a singular coeﬃcient matrix, then either it has no solutions — the case we
already rejected — or it has inﬁnitely many solutions — the case we are considering now.
In the particular network under consideration, the general solution to the linear system
⎛
⎜
⎝
3
−1
−1
−1
−1
2
0
−1
−1
0
2
−1
−1
−1
−1
3
⎞
⎟
⎠
⎛
⎜
⎝
u1
u2
u3
u4
⎞
⎟
⎠=
⎛
⎜
⎝
1
0
0
−1
⎞
⎟
⎠
is found by Gaussian Elimination:
u =
⎛
⎜
⎜
⎜
⎝
1
2 + t
1
4 + t
1
4 + t
t
⎞
⎟
⎟
⎟
⎠=
⎛
⎜
⎜
⎜
⎝
1
2
1
4
1
4
0
⎞
⎟
⎟
⎟
⎠+ t
⎛
⎜
⎜
⎜
⎝
1
1
1
1
⎞
⎟
⎟
⎟
⎠,
(6.32)
where t = u4 is the free variable. The resulting nodal voltage potentials
u1 = 1
2 + t,
u2 = 1
4 + t,
u3 = 1
4 + t,
u4 = t,
depend on a free parameter t.
The ambiguity arises because voltage potential is a mathematical abstraction that can-
not be measured directly; only relative potential diﬀerences have physical import.
To
resolve the inherent ambiguity, we need to assign a baseline value for the voltage poten-
tials. In terrestrial electricity, the Earth is assumed to have zero potential. Specifying a
particular node to have zero potential is physically equivalent to grounding that node. For
our example, suppose we ground node 4 by setting u4 = 0. This ﬁxes the free variable t = 0
in our solution (6.32), and so uniquely speciﬁes all the other voltage potentials: u1 = 1
2,
u2 = 1
4, u3 = 1
4, u4 = 0.
On the other hand, even without speciﬁcation of a baseline potential level, the cor-
responding physical voltages and currents along the wires are uniquely speciﬁed. In our
example, computing y = v = Au gives
y1 = v1 = 1
4 ,
y2 = v2 = 1
4 ,
y3 = v3 = 1
2 ,
y4 = v4 = 1
4 ,
y5 = v5 = 1
4 ,
independent of the value of t in (6.32). Thus, the nonuniqueness of the voltage potential
solution u is an inessential feature. All physical quantities that we can measure — currents
and voltages — are uniquely speciﬁed by the solution to the equilibrium system.
Remark. Although they have no real physical meaning, we cannot dispense with the
nonmeasurable (and nonunique) voltage potentials u. Most networks are statically inde-
terminate, since their incidence matrices are rectangular and hence not invertible, so the
linear system AT y = f cannot be solved directly for the currents in terms of the voltage

316
6 Equilibrium
sources since the system does not have a unique solution. Only by ﬁrst solving the full
equilibrium system (6.29) for the potentials, and then using the relation y = CAu between
the potentials and the currents, can we determine their actual values.
Let us analyze what is going on in the context of our general mathematical framework.
Proposition 3.36 says that the resistivity matrix K = AT CA is a positive semi-deﬁnite
Gram matrix, which is positive deﬁnite (and hence nonsingular) if and only if A has
linearly independent columns, or, equivalently, ker A = {0}. But Proposition 2.51 says
that the incidence matrix A of a directed graph never has a trivial kernel. Therefore, the
resistivity matrix K is only positive semi-deﬁnite, and hence singular. If the network is
connected, then ker A = ker K = cokerK is one-dimensional, spanned by the vector z =
( 1, 1, 1, . . ., 1 )T . According to the Fredholm Alternative Theorem 4.46, the fundamental
network equation Ku = f has a solution if and only if f is orthogonal to coker K, and so
the current source vector must satisfy
z · f = f1 + f2 + · · · + fn = 0,
(6.33)
as we already observed. Therefore, the linear algebra reconﬁrms our physical intuition: a
connected network admits an equilibrium conﬁguration, obtained by solving (6.29), if and
only if the nodal current sources add up to zero, i.e., there is no net inﬂux of current into
the network.
Grounding one of the nodes is equivalent to nullifying the value of its voltage potential:
ui = 0. This variable is now ﬁxed, and can be safely eliminated from our system. To
accomplish this, we let A⋆denote the m × (n −1) matrix obtained by deleting the ith
column from A. For example, grounding node 4 in our sample network, so u4 = 0, allows
us to erase the fourth column of the incidence matrix (6.19), leading to the reduced incidence
matrix
A⋆=
⎛
⎜
⎜
⎜
⎝
1
−1
0
1
0
−1
1
0
0
0
1
0
0
0
1
⎞
⎟
⎟
⎟
⎠.
(6.34)
The key observation is that A⋆has trivial kernel, ker A⋆= {0}, and therefore the reduced
network resistivity matrix
K⋆= (A⋆)T C A⋆=
⎛
⎝
c1 + c2 + c3
−c1
−c2
−c1
c1 + c4
0
−c2
0
c2 + c5
⎞
⎠
(6.35)
is positive deﬁnite. Note that we can obtain K⋆directly from K in (6.30) by deleting both
its fourth row and fourth column. Let f ⋆= ( 1, 0, 0 )T denote the reduced current source
vector obtained by deleting the fourth entry from f. Then the reduced linear system is
K⋆u⋆= f ⋆,
(6.36)
where u⋆= ( u1, u2, u3 )T is the reduced voltage potential vector. Positive deﬁniteness
of K⋆implies that (6.36) has a unique solution u⋆, from which we can reconstruct the
voltages v = A⋆u⋆and currents y = C v = CA⋆u⋆along the wires. In our example, if all
the wires have unit resistance, then the reduced system (6.36) is
⎛
⎝
3
−1
−1
−1
2
0
−1
0
2
⎞
⎠
⎛
⎝
u1
u2
u3
⎞
⎠=
⎛
⎝
1
0
0
⎞
⎠,

6.2 Electrical Networks
317
and has unique solution u⋆=
 1
2, 1
4, 1
4
T. The voltage potentials are
u1 = 1
2,
u2 = 1
4,
u3 = 1
4,
u4 = 0,
and correspond to the earlier solution (6.32) when t = 0. The corresponding voltages and
currents along the wires are the same as before.
Remark. When C = I , the matrix K = AT A constructed from the incidence matrix of a
directed graph is known in the mathematical literature as the graph Laplacian associated
with the graph. The graph Laplacian matrix can be easily constructed directly: its rows
and columns are indexed by the vertices. The diagonal entry kii equals the degree of the
ith vertex, meaning the number of edges that have vertex i as one of their endpoints. The
oﬀ-diagonal entries kij are equal to −1 if there is an edge connecting vertices i and j and
0 otherwise. This is often written as K = D −J, where D is the diagonal degree matrix,
whose diagonal entries are the degrees of the nodes, and J is the symmetric adjacency
matrix, which contains a 1 in every oﬀ-diagonal entry corresponding to two adjacent nodes,
that is two nodes connected by a single edge; all other entries are 0. Observe that the
graph Laplacian is independent of the direction assigned to the edges; it depends only
on the underlying graph. The term “Laplacian” is used because this matrix represents
the discrete analogue of the Laplacian diﬀerential operator, described in Examples 7.36
and 7.52 below.
In particular, if the graph comes from an n-dimensional square grid,
the corresponding graph Laplacian coincides with the standard ﬁnite diﬀerence numerical
discretization of the Laplacian diﬀerential operator.
Batteries, Power, and the Electrical–Mechanical Correspondence
So far, we have considered only the eﬀect of current sources at the nodes. Suppose now
that the network contains one or more batteries. Each battery serves as a voltage source
along a wire, and we let bk denote the voltage of a battery connected to wire k. The sign
of bk indicates the relative orientation of the battery’s terminals with respect to the wire,
with bk > 0 if the current produced by the battery runs in the same direction as our chosen
orientation of the wire. The battery’s voltage is included in the voltage balance equation
(6.17):
vk = ui −uj + bk.
The corresponding vector equation (6.18) becomes
v = Au + b,
(6.37)
where b = ( b1, b2, . . . , bm )T is the battery vector, whose entries are indexed by the wires.
(If there is no battery on wire k, the corresponding entry is bk = 0.) The remaining two
equations are as before, so y = C v are the currents in the wires, and, in the absence of
external current sources, Kirchhoﬀ’s Current Law implies AT y = 0. Using the modiﬁed
formula (6.37) for the voltages, these combine into the following equilibrium system:
Ku = AT C Au = −AT C b.
(6.38)
Remark. Interestingly, the voltage potentials satisfy the weighted normal equations (5.36)
that characterize the least squares solution to the system Au = −b for the weighted norm
∥v∥2 = vT C v
(6.39)
determined by the network’s conductance matrix C. It is a striking fact that Nature solves
a least squares problem in order to make the weighted norm of the voltages v as small as
possible. A similar remark holds for the mass–spring chains considered above.

318
6 Equilibrium
Batteries have exactly the same eﬀect on the voltage potentials as if we imposed the
current source vector
f = −ATC b.
(6.40)
Namely, placing a battery of voltage bk on wire k is exactly the same as introducing
additional current sources of −ck bk at the starting node and ck bk at the ending node.
Note that the induced current vector f ∈coimg A = img K (see Exercise 3.4.32) continues
to satisfy the network constraint (6.33). Conversely, a system of allowed current sources
f ∈img K has the same eﬀect as any collection of batteries b that satisﬁes (6.40).
In the absence of external current sources, a network with batteries always admits
a solution for the voltage potentials and currents.
Although the currents are uniquely
determined, the voltage potentials are not. As before, to eliminate the ambiguity, we can
ground one of the nodes and use the reduced incidence matrix A⋆and reduced current
source vector f ⋆obtained by eliminating the column, respectively entry, corresponding to
the grounded node. The details are left to the interested reader.
Example 6.4.
Consider an electrical network running along the sides of a cube, where
each wire contains a 2 ohm resistor and there is a 9 volt battery source on one wire. The
problem is to determine how much current ﬂows through the wire directly opposite the
battery. Orienting the wires and numbering them as indicated in Figure 6.6, the incidence
matrix is
A =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
1
−1
0
0
0
0
0
0
1
0
−1
0
0
0
0
0
1
0
0
−1
0
0
0
0
0
1
0
0
−1
0
0
0
0
1
0
0
0
−1
0
0
0
0
1
0
−1
0
0
0
0
0
1
0
0
0
−1
0
0
0
0
1
0
−1
0
0
0
0
0
1
0
0
−1
0
0
0
0
0
1
0
0
−1
0
0
0
0
0
1
0
−1
0
0
0
0
0
0
1
−1
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
.
We connect the battery along wire 1 and measure the resulting current along wire 12. To
avoid the ambiguity in the voltage potentials, we ground the last node and erase the ﬁnal
column from A to obtain the reduced incidence matrix A⋆. Since the resistance matrix R
has all 2’s along the diagonal, the conductance matrix is C = 1
2 I . Therefore, the network
resistivity matrix is one-half the cubical graph Laplacian:
K⋆= (A⋆)T CA⋆= 1
2 (A⋆)T A⋆= 1
2
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
3
−1
−1
−1
0
0
0
−1
3
0
0
−1
−1
0
−1
0
3
0
−1
0
−1
−1
0
0
3
0
−1
−1
0
−1
−1
0
3
0
0
0
−1
0
−1
0
3
0
0
0
−1
−1
0
0
3
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
.
Alternatively, it can be found by eliminating the ﬁnal row and column, representing the
grounded node, from the graph Laplacian matrix constructed by the above recipe. The
reduced current source vector
b = ( 9, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 )T

6.2 Electrical Networks
319
u1
u2
u3
u4
u5
u6
u7
u8
R1
R2
R3
R4
R5
R6
R7
R8
R9
R10
R11
R12
Figure 6.6.
Cubical Electrical Network.
corresponding to the battery situated on the ﬁrst wire is
f ⋆= −(A⋆)TC b =

−9
2, 9
2, 0, 0, 0, 0, 0
T .
Solving the resulting linear system K⋆u⋆= f ⋆by Gaussian Elimination yields the voltage
potentials
u⋆=

−3,
9
4, −9
8, −9
8,
3
8,
3
8, −3
4
T .
Thus, the induced currents along the sides of the cube are
y = C v = 1
2 (A⋆u⋆+ b) =
 15
8 , −15
16, −15
16,
15
16,
15
16, −3
4, −3
16, −3
4, −3
16,
3
16,
3
16, −3
8
T .
In particular, the current on the wire that is opposite the battery is y12 = −3
8, ﬂowing
in the opposite direction to its orientation. The largest current ﬂows through the battery
wire, while wires 7, 9, 10, 11 transmit the least.
As with a mass–spring chain, the voltage potentials in such a resistive electrical network
can be characterized by a minimization principle. The power in a single conducting wire
is deﬁned as the product of its current yj and voltage vj,
Pj = yj vj = Rj y2
j = cj v2
j ,
(6.41)
where Rj is the resistance, cj = 1/Rj the conductance, and we are using Ohm’s Law
(6.21) to relate voltage and current. Physically, the power quantiﬁes the rate at which
electrical energy is converted into heat by the wire’s resistance. Summing over all wires in
the system, the internal power† of the network
Pint =

j
Pj =

j
cj v2
j = ∥v∥2
is identiﬁed as the square of the weighted norm (6.39).
†
So far, we have not considered the eﬀect of batteries or current sources on the network.

320
6 Equilibrium
The Electrical–Mechanical Correspondence
Structures
Variables
Networks
Displacements
u
Voltage potentials
Prestressed bars/springs
b
Batteries
Elongations†
v = Au + b
Voltages
Spring stiﬀnesses
C
Conductivities
Internal Forces
y = C v
Currents
External forcing
f = AT y
Current sources
Stiﬀness matrix
K = AT C A
Resistivity matrix
Potential energy
p(u) = 1
2uT Ku −uTf
1
2 × Power
Consider a network that contains batteries, but no external current sources. Summing
over all the wires in the network, the total power due to internal and external sources can
be identiﬁed as the product of the current and voltage vectors:
P = y1 v1 + · · · + ym vm = yT v = vT C v = (Au + b)T C (Au + b)
= uTAT C Au + 2 uTAT C b + bT C b,
and is thus a quadratic function of the voltage potentials, which we rewrite in our usual
form‡
1
2 P = p(u) = 1
2 uT Ku −uT f + c,
(6.42)
where K = AT C A is the network resistivity matrix, while f = −AT C b are the equiva-
lent current sources at the nodes (6.40) that correspond to the batteries. The last term
c =
1
2 bT C b is one-half the internal power of the batteries, and is not aﬀected by the
currents/voltages in the wires. In deriving (6.42), we have ignored external current sources
at the nodes. By the preceding discussion, external current sources can be viewed as an
equivalent collection of batteries, and so contribute to the linear terms uT f in the power,
which will then represent the combined eﬀect of all batteries and external current sources.
In general, the resistivity matrix K is only positive semi-deﬁnite, and so the quadratic
power function (6.42) does not, in general, possess a minimizer. As argued above, to ensure
equilibrium, we need to ground one or more of the nodes. The resulting reduced power
function
p⋆(u⋆) = 1
2 (u⋆)T K⋆u⋆−(u⋆)T f ⋆,
(6.43)
has a positive deﬁnite coeﬃcient matrix: K⋆> 0. Its unique minimizer is the voltage
potential u⋆that solves the reduced linear system (6.36). We conclude that the electrical
network adjusts itself so as to minimize the power or total energy loss throughout the
network. As in mechanics, Nature solves a minimization problem in an eﬀort to conserve
energy.
†
Here, we use v instead of e to represent elongation.
‡
For alternating currents, the power is reduced by a factor of 1
2, so p(u) equals the power.

6.2 Electrical Networks
321
We have now discovered the remarkable correspondence between the equilibrium equa-
tions for electrical networks (6.10) and those of mass–spring chains (6.28). This Electrical–
Mechanical Correspondence is summarized in the above table. In the following section, we
will see that the analogy extends to more general mechanical structures.
Exercises
6.2.1. Draw the electrical networks corresponding to the following incidence matrices.
(a)
⎛
⎜
⎜
⎜
⎝
1
0
−1
0
0
1
0
−1
−1
1
0
0
0
0
1
−1
⎞
⎟
⎟
⎟
⎠,
(b)
⎛
⎜
⎜
⎜
⎝
0
0
1
−1
1
0
0
−1
0
−1
1
0
1
0
−1
0
⎞
⎟
⎟
⎟
⎠,
(c)
⎛
⎜
⎜
⎜
⎝
0
1
0
0
−1
−1
0
1
0
0
0
0
0
−1
1
0
−1
1
0
0
⎞
⎟
⎟
⎟
⎠,
(d)
⎛
⎜
⎜
⎜
⎜
⎜
⎝
−1
0
1
0
0
0
−1
0
1
0
1
−1
0
0
0
0
0
0
−1
1
0
0
−1
0
1
⎞
⎟
⎟
⎟
⎟
⎟
⎠
,
(e)
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
0
−1
0
1
0
0
0
0
0
−1
0
1
0
0
1
0
−1
0
0
0
0
1
−1
0
0
0
0
0
0
0
0
−1
0
1
0
0
0
0
0
−1
0
1
0
0
−1
0
0
1
0
0
0
0
0
0
1
−1
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
.
1
2
3
4
1
2
3
4
5
6.2.2. Suppose that all wires in the illustrated network have unit resistivity.
(a) Write down the incidence matrix A.
(b) Write down the equilibrium
system for the network when node 4 is grounded and there is a current
source of magnitude 3 at node 1.
(c) Solve the system for the voltage
potentials at the ungrounded nodes.
(d) If you connect a light bulb
to the network, which wire should you connect it to so that it shines
the brightest?
6.2.3. What happens in the network in Figure 6.5 if we ground both nodes 3 and 4? Set up
and solve the system and compare the currents for the two cases.
1
2
3
4
5
1
2
3
4
5
6
7
6.2.4.(a) Write down the incidence matrix A for the illustrated
electrical network.
(b) Suppose all the wires contain unit
resistors, except for R4 = 2. Let there be a unit current source
at node 1, and assume node 5 is grounded. Find the voltage
potentials at the nodes and the currents through the wires.
(c) Which wire would shock you the most?
6.2.5. Answer Exercise 6.2.4 if, instead of the current source, you
put a 1.5 volt battery on wire 1.
♠6.2.6. Consider an electrical network running along the sides of a tetrahedron.
Suppose that each wire contains a 3 ohm resistor and there is a 10 volt
battery source on one wire. Determine how much current ﬂows through
the wire directly opposite the battery.
♠6.2.7. Now suppose that each wire in the tetrahedral network in Exercise
6.2.6 contains a 1 ohm resistor and there are two 5 volt battery sources
located on two non-adjacent wires. Determine how much current ﬂows
through the wires in the network.
♠6.2.8.(a) How do the currents change if the resistances in the wires in the cubical network in
Example 6.4 are all equal to 1 ohm?
(b) What if wire k has resistance Rk = k ohms?
♣6.2.9. Suppose you are given six resistors with respective resistances 1, 2, 3, 4, 5, and 6. How
should you connect them in a tetrahedral network (one resistor per wire) so that a light
bulb on the wire opposite the battery burns the brightest?

322
6 Equilibrium
♣6.2.10. The nodes in an electrical network lie on the vertices
 i
n, j
n

for −n ≤i, j ≤n in a
square grid centered at the origin; the wires run along the grid lines. The boundary nodes,
when x or y = ±1, are all grounded. A unit current source is introduced at the origin.
(a) Compute the potentials at the nodes and currents along the wires for n = 2, 3, 4.
(b) Investigate and compare the solutions for large n, i.e., as the grid size becomes small.
Do you detect any form of limiting behavior?
6.2.11. Show that, in a network with all unit resistors, the currents y can be characterized as
the unique solution to the Kirchhoﬀequations AT y = f of minimum Euclidean norm.
6.2.12. True or false: (a) The nodal voltage potentials in a network with batteries b are the
same as in the same network with the current sources f = −AT C b.
(b) Are the currents
the same?
6.2.13.(a) Assuming all wires have unit resistance, ﬁnd the voltage potentials at all the
nodes and the currents along the wires of the following trees when the bottom node is
grounded and a unit current source is introduced at the top node.
(i)
(ii)
(iii)
(iv)
(v)
(b) Can you make any general predictions about electrical currents in trees?
6.2.14. A node in a tree is called terminating if it has only one edge. Repeat the preceding
exercise when all terminating nodes except for the top one are grounded.
6.2.15. Suppose the graph of an electrical network is a tree, as in Exercise 2.6.9. Show that if
one of the nodes in the tree is grounded, the system is statically determinate.
6.2.16. Suppose two wires in a network join the same pair of nodes. Explain why their eﬀect
on the rest of the network is the same as a single wire whose conductance c = c1 + c2 is the
sum of the individual conductances. How are the resistances related?
6.2.17.(a) Write down the equilibrium equations for a network that contains both batteries
and current sources.
(b) Formulate a general superposition principle for such situations.
(c) Write down a formula for the power in the network.
♦6.2.18. Prove that the voltage potential at node i due to a unit current source at node j is the
same as the voltage potential at node j due to a unit current source at node i. Can you
give a physical explanation of this reciprocity relation?
6.2.19. What is the analogue of condition (6.33) for a disconnected graph?
6.3 Structures
A structure (sometimes called a truss) is a mathematical idealization of a framework for
a building. Think of a radio tower or a skyscraper when just the I-beams are connected
— before the walls, ﬂoors, ceilings, roof, and ornamentation are added. An ideal structure
is constructed of elastic bars connected at joints. By a bar, we mean a straight, rigid rod
that can be (slightly) elongated, but not bent. (Beams, which are allowed to bend, are
more complicated and are modeled by boundary value problems for ordinary and partial
diﬀerential equations, [61, 79]. See also our discussion of splines in Section 5.5.) When
a bar is stretched, it obeys Hooke’s law — at least in the linear regime we are modeling

6.3 Structures
323
ai
aj
bi
bj
ε ui
ε uj
L
L + e
Figure 6.7.
Displacement of a Bar.
— and so, for all practical purposes, behaves like a spring with a very large stiﬀness. As
a result, a structure can be regarded as a two- or three-dimensional generalization of a
mass–spring chain.
The joints will allow the bar to rotate in any direction. Of course, this is an idealization;
in a building, the rivets and bolts will (presumably) prevent rotation to a signiﬁcant degree.
However, under moderate stress — for example, if the wind is blowing on our skyscraper,
the bolts can be expected only to keep the structure connected, and the resulting motions
will induce stresses on the joints that must be taken into account when designing the
structure. Of course, under extreme stress, the structure will fall apart — a disaster that
its designers must avoid.
The purpose of this section is to derive conditions that will
guarantee that a structure is rigidly stable under moderate forcing, or, alternatively, help
us to understand the processes that might lead to its collapse.
The ﬁrst order of business is to understand how an individual bar reacts to motion. We
have already encountered the basic idea in our treatment of springs. The key complication
here is that the ends of the bar are not restricted to a single direction of motion, but can
move in either two- or three-dimensional space. We use d to denote the dimension of the
underlying space. In the d = 1-dimensional case, the structure reduces to a mass–spring
chain that we analyzed in Section 6.1. Here we concentrate on structures in d = 2 and 3
dimensions.
Consider an unstressed bar with one end at position a1 ∈Rd and the other end at
position a2 ∈Rd. In d = 2 dimensions, we write ai = ( ai, bi )T , while in d = 3-dimensional
space, ai = ( ai, bi, ci )T . The length of the bar is L = ∥a1 −a2 ∥, where we use the standard
Euclidean norm to measure distance on Rd throughout this section.
Suppose we move the ends of the bar a little, sending ai to bi = ai + ε ui and, simul-
taneously, aj to bj = aj + ε uj, moving the blue bar in Figure 6.7 to the displaced orange
bar. The vectors ui, uj ∈Rd indicate the respective directions of displacement of the two
ends, and we use ε to represent the relative magnitude of the displacement. How much has
this motion stretched the bar? Since we are assuming that the bar can’t bend, the length
of the displaced bar is
L + e = ∥bi −bj ∥= ∥(ai + ε ui) −(aj + ε uj)∥= ∥(ai −aj) + ε (ui −uj)∥
=

∥ai −aj ∥2 + 2 ε (ai −aj) · (ui −uj) + ε2 ∥ui −uj ∥2 .
(6.44)

324
6 Equilibrium
Figure 6.8.
Tangent Line Approximation.
The diﬀerence between the new length and the original length, namely
e =

∥ai −aj ∥2 + 2 ε (ai −aj) · (ui −uj) + ε2 ∥ui −uj ∥2 −∥ai −aj ∥,
(6.45)
is, by deﬁnition, the bar’s elongation.
If the underlying dimension d is 2 or more, the elongation (6.45) is a nonlinear function
of the displacement vectors ui, uj. Thus, an exact, geometrical treatment of structures
in equilibrium requires dealing with complicated nonlinear systems of equations. In some
situations, e.g., the design of robotic mechanisms, [57, 75], analysis of the nonlinear system
is crucial, but this lies beyond the scope of this text. However, in many practical situations,
the displacements are fairly small, so | ε | ≪1. For example, when a building moves, the
lengths of bars are in meters, but the displacements are, barring catastrophes, typically in
centimeters if not millimeters. In such situations, we can replace the geometrically exact
elongation by a much simpler linear approximation.
As you learned in calculus, the most basic linear approximation to a nonlinear function
g(ε) near ε = 0 is given by its tangent line or linear Taylor polynomial
g(ε) ≈g(0) + g′(0) ε,
| ε | ≪1,
(6.46)
as sketched in Figure 6.8. In the case of small displacements of a bar, the elongation (6.45)
is a square root function of the particular form
g(ε) =

a2 + 2 ε b + ε2 c2 −a,
where
a = ∥ai −aj ∥,
b = (ai −aj) · (ui −uj),
c = ∥ui −uj ∥,
are independent of ε. Since g(0) = 0 and g′(0) = b/a, the linear approximation (6.46) is

a2 + 2 ε b + ε2 c2 −a ≈ε b
a
for
| ε | ≪1.
In this manner, we arrive at the linear approximation to the bar’s elongation
e ≈ε (ai −aj) · (ui −uj)
∥ai −aj ∥
= n · (ε ui −ε uj),
where
n =
ai −aj
∥ai −aj ∥
is the unit vector, ∥n∥= 1, that points in the direction of the bar from node j to node i.
The factor ε was merely a mathematical device used to derive the linear approximation.
It can now be safely discarded, so that the displacement of the ith node is now ui instead of
ε ui, and we assume ∥ui ∥is small. If bar k connects node i to node j, then its (approximate)

6.3 Structures
325
ai
aj
nk
−nk
Figure 6.9.
Unit Vectors for a Bar.
elongation is equal to
ek = nk · (ui −uj) = nk · ui −nk · uj,
where
nk =
ai −aj
∥ai −aj ∥.
(6.47)
The elongation ek is the sum of two terms: the ﬁrst, nk · ui, is the component of the
displacement vector for node i in the direction of the unit vector nk that points along the
bar towards node i, whereas the second, −nk · uj, is the component of the displacement
vector for node j in the direction of the unit vector −nk that points in the opposite direction
along the bar toward node j; see Figure 6.9. Their sum equals the total elongation of the
bar.
We assemble all the linear equations (6.47) relating nodal displacements to bar elonga-
tions in matrix form
e = Au.
(6.48)
Here e =
⎛
⎜
⎜
⎜
⎝
e1
e2
...
em
⎞
⎟
⎟
⎟
⎠∈Rm is the vector of elongations, while u =
⎛
⎜
⎜
⎜
⎝
u1
u2
...
un
⎞
⎟
⎟
⎟
⎠∈Rdn is the vector
of displacements. Each ui ∈Rd is itself a column vector with d entries, and so u has a
total of dn entries. For example, in the planar case d = 2, we have ui =

xi
yi

, since each
node’s displacement has both an x and y component, and so
u =
⎛
⎜
⎜
⎜
⎝
u1
u2
...
un
⎞
⎟
⎟
⎟
⎠=
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
x1
y1
x2
y2
...
xn
yn
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
∈R2n.
In three dimensions, d = 3, we have ui = ( xi, yi, zi )T , and so each node will contribute
three components to the displacement vector
u = ( x1, y1, z1, x2, y2, z2, . . . , xn, yn, zn )T ∈R3n.
The incidence matrix A connecting the displacements and elongations will be of size
m × (dn). The kth row of A will have (at most) 2d nonzero entries. The entries in the d

326
6 Equilibrium







Figure 6.10.
Three Bar Planar Structure.
slots corresponding to node i will be the components of the (transposed) unit bar vector nT
k
pointing towards node i, as given in (6.47), while the entries in the d slots corresponding to
node j will be the components of its negative −nT
k , which is the unit bar vector pointing
towards node j. All other entries are 0. The general mathematical formulation is best
appreciated by working through an explicit example.
Example 6.5.
Consider the planar structure pictured in Figure 6.10. The four nodes
are at positions
a1 = (0, 0)T,
a2 = (1, 1)T,
a3 = (3, 1)T,
a4 = (4, 0)T,
so the two side bars are at 45◦angles and the center bar is horizontal. Implementing our
construction, the associated incidence matrix is
A =
⎛
⎜
⎝
−
1
√
2
−
1
√
2
0
0
0
0

1
√
2
1
√
2
−1
0
0
0

0
0
1
0
−
1
√
2
1
√
2

0
0
0
0
1
√
2
−
1
√
2
⎞
⎟
⎠.
(6.49)
The three rows of A refer to the three bars in our structure. The columns come in pairs,
as indicated by the vertical lines in the matrix: the ﬁrst two columns refer to the x and
y displacements of the ﬁrst node; the third and fourth columns refer to the second node;
and so on. The ﬁrst two entries of the ﬁrst row of A indicate the unit vector
n1 =
a1 −a2
∥a1 −a2 ∥=
$
−
1
√
2, −
1
√
2
%T
that points along the ﬁrst bar towards the ﬁrst node, while the third and fourth entries
have the opposite signs, and form the unit vector
−n1 =
a2 −a1
∥a2 −a1 ∥=
$
1
√
2,
1
√
2
%T
along the same bar that points in the opposite direction — towards the second node. The
remaining entries are zero because the ﬁrst bar connects only the ﬁrst two nodes. Similarly,
the unit vector along the second bar pointing towards node 2 is
n2 =
a2 −a3
∥a2 −a3 ∥= ( −1, 0 )T ,
and this gives the third and fourth entries of the second row of A; the ﬁfth and sixth entries
are their negatives, corresponding to the unit vector −n2 pointing towards node 3. The
last row is constructed from the unit vectors along bar #3 in the same fashion.
Remark. Interestingly, the incidence matrix for a structure depends only on the directions
of the bars and not their lengths. This is analogous to the fact that the incidence matrix

6.3 Structures
327
for an electrical network depends only on the connectivity properties of the wires and not
on their overall lengths. One can regard the incidence matrix for a structure as a kind of
d-dimensional generalization of the incidence matrix for a directed graph.
The next phase of our procedure is to introduce the constitutive relations for the bars
that determine their internal forces or stresses. As we remarked at the beginning of the
section, each bar is viewed as a hard spring, subject to a linear Hooke’s law equation
yk = ck ek
(6.50)
that relates its elongation ek to its internal force yk. The bar stiﬀness ck > 0 is a positive
scalar, and so yk > 0 if the bar is in tension, while yk < 0 if the bar is compressed. We
write (6.50) in matrix form
y = C e,
(6.51)
where C = diag (c1, . . . , cm) > 0 is a diagonal, positive deﬁnite matrix.
Finally, we need to balance the forces at each node in order to achieve equilibrium.
If bar k terminates at node i, then it exerts a force −yk nk on the node, where nk is
the unit vector pointing towards the node in the direction of the bar, as in (6.47). The
minus sign comes from physics: if the bar is under tension, so yk > 0, then it is trying to
contract back to its unstressed state, and so will pull the node towards it — in the opposite
direction to nk — while a bar in compression will push the node away. In addition, we
may have an externally applied force vector, denoted by f i, on node i, which might be
some combination of gravity, weights, mechanical forces, and so on. (In this admittedly
simpliﬁed model, external forces act only on the nodes and not directly on the bars.) Force
balance at equilibrium requires that all the nodal forces, external and internal, cancel; thus,
f i +

k
(−yk nk) = 0,
or

k
yk nk = f i,
where the sum is over all the bars that are attached to node i. The matrix form of the
force balance equations is (and this should no longer come as a surprise)
f = AT y,
(6.52)
where AT is the transpose of the incidence matrix, and f =
⎛
⎜
⎜
⎜
⎝
f1
f2
...
fn
⎞
⎟
⎟
⎟
⎠∈Rdn is the vector
containing all external forces on the nodes. Putting everything together, (6.48, 51, 52),
e = Au,
y = C e,
f = AT y,
we are once again led to a by now familiar linear system of equations:
Ku = f,
where
K = AT C A
(6.53)
is the stiﬀness matrix for our structure.
The stiﬀness matrix K is a positive (semi-)deﬁnite Gram matrix (3.64) associated with
the weighted inner product on the space of elongations prescribed by the diagonal matrix
C. As we know, K will be positive deﬁnite if and only if the kernel of the incidence matrix is
trivial: ker A = {0}. However, the preceding example does not enjoy this property, because
we have not tied down (or “grounded”) our structure. In essence, we are considering a
structure ﬂoating in outer space, which is free to move around in any direction. Each rigid

328
6 Equilibrium
a1
a2
a3
Figure 6.11.
A Triangular Structure.
motion† of the structure will correspond to an element of the kernel of its incidence matrix,
and thereby preclude positive deﬁniteness of its stiﬀness matrix.
Example 6.6.
Consider a planar space station in the shape of a unit equilateral triangle,
as in Figure 6.11. Placing the nodes at positions
a1 =
$
1
2,
√
3
2
%T
,
a2 = ( 1, 0 )T ,
a3 = ( 0, 0 )T ,
we use the preceding algorithm to construct the incidence matrix
A =
⎛
⎜
⎝
−1
2
√
3
2
1
2
√
3
2
0
0

1
2
−
√
3
2
0
0
1
0

0
0
−1
2
−
√
3
2
−1
0
⎞
⎟
⎠,
whose rows are indexed by the bars, and whose columns are indexed in pairs by the three
nodes. The kernel of A is three-dimensional, with basis
z1 =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
1
0
1
0
1
0
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
,
z2 =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
0
1
0
1
0
1
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
,
z3 =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
−
√
3
2
1
2
0
1
0
0
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
.
(6.54)
We claim that these three displacement vectors represent three diﬀerent planar rigid mo-
tions: the ﬁrst two correspond to translations, and the third to a rotation.
The translations are easy to discern.
Translating the space station in a horizontal
direction means that we move all three nodes the same amount, and so the displacements
are u1 = u2 = u3 = a for some vector a. In particular, a rigid unit horizontal translation
has a = e1 = ( 1, 0 )T , and corresponds to the ﬁrst kernel basis vector. Similarly, a unit
vertical translation of all three nodes corresponds to a = e2 = ( 0, 1 )T , and corresponds
to the second kernel basis vector. Every other translation is a linear combination of these
two. Translations do not alter the lengths of any of the bars, and so do not induce any
stress in the structure.
†
See Section 7.2 for an extended discussion of rigid motions.

6.3 Structures
329
ε
ε
Figure 6.12.
Rotating a Space Station.
The rotations are a little more subtle, owing to the linear approximation that we used to
compute the elongations. Referring to Figure 6.12, we see that rotating the space station
through a small angle ε around the node a3 = ( 0, 0 )T will move the other two nodes to
positions
b1 =

1
2 cos ε −
√
3
2 sin ε
1
2 sin ε +
√
3
2 cos ε

,
b2 =

cos ε
sin ε

,
b3 =

0
0

.
(6.55)
However, the corresponding displacements
u1 = b1 −a1 =

1
2 (cos ε −1) −
√
3
2 sin ε
1
2 sin ε +
√
3
2 (cos ε −1)

,
u2 = b2 −a2 =

cos ε −1
sin ε

,
u3 = b3 −a3 =

0
0

,
(6.56)
do not combine into a vector that belongs to ker A. The problem is that, under a rotation,
the nodes move along circles, while the kernel displacements u = ε z ∈ker A correspond
to straight line motion! In order to maintain consistency, we must adopt a similar linear
approximation of the nonlinear circular motion of the nodes. Thus, we replace the nonlinear
displacements uj(ε) in (6.56) by their linear tangent approximations† ε u′
j(0), so
u1 ≈ε

−
√
3
2
1
2

,
u2 ≈ε

0
1

,
u3 =

0
0

.
The resulting displacements do combine to produce the displacement vector
u = ε
$
−
√
3
2 ,
1
2, 0, 1, 0, 0
%T
= ε z3
that moves the space station in the direction of the third kernel basis vector. Thus, as
claimed, z3 represents the linear approximation to a rigid rotation around the ﬁrst node.
Remarkably, the rotations around the other two nodes, although distinct nonlinear
motions, can be linearly approximated by particular combinations of the three kernel basis
†
Note that uj(0) = 0.

330
6 Equilibrium
elements z1, z2, z3, and so already appear in our description of ker A. For example, the
displacement vector
u = ε
$ √
3
2 z1 + 1
2 z2 −z3
%
= ε
$
0, 0,
√
3
2 , −1
2,
√
3
2 ,
1
2
%T
(6.57)
represents the linear approximation to a rigid rotation around the ﬁrst node. We conclude
that the three-dimensional kernel of the incidence matrix represents the sum total of all
possible rigid motions of the space station, or, more correctly, their linear approximations.
Which types of forces will maintain the space station in equilibrium? This will happen
if and only if we can solve the force balance equations AT y = f for the internal forces
y. The Fredholm Alternative Theorem 4.46 implies that this system has a solution if and
only if f is orthogonal to coker AT = ker A. Therefore, f = ( f1, g1, f2, g2, f3, g3 )T must be
orthogonal to the kernel basis vectors (6.54), and so must satisfy the three linear constraints
z1 · f = f1 + f2 + f3 = 0,
z2 · f = g1 + g2 + g3 = 0,
z3 · f = −
√
3
2 f1 + 1
2 g1 + g2 = 0.
(6.58)
The ﬁrst constraint requires that there be no net horizontal force on the space station.
The second requires no net vertical force. The last constraint requires that the moment
of the forces around the third node vanishes. The vanishing of the force moments around
each of the other two nodes is a consequence of these three conditions, since the associated
kernel vectors can be expressed as linear combinations of the three basis elements. The
corresponding physical requirements are clear. If there is a net horizontal or vertical force,
the space station will rigidly translate in that direction; if there is a non-zero force moment,
the station will rigidly rotate. In any event, unless the force balance constraints (6.58) are
satisﬁed, the space station cannot remain in equilibrium. A freely ﬂoating space station is
an unstable structure that can easily be set into motion with a tiny external force.
Since there are three independent rigid motions, we must impose three constraints on
the structure in order to fully stabilize it under general external forcing. “Grounding” one
of the nodes, i.e., preventing it from moving by attaching it to a ﬁxed support, will serve
to eliminate the two translational instabilities. For example, setting u3 = 0 has the eﬀect
of ﬁxing the third node of the space station to a support. With this speciﬁcation, we can
eliminate the variables associated with that node, and thereby delete the corresponding
columns of the incidence matrix — leaving the reduced incidence matrix
A⋆=
⎛
⎜
⎝
1
2
√
3
2
0
0
−1
2
√
3
2
1
2
−
√
3
2
0
0
1
0
⎞
⎟
⎠.
The kernel of A⋆is one-dimensional, spanned by the single vector z⋆
3 =
$ √
3
2 ,
1
2, 0, 1
%T
,
which corresponds to (the linear approximation of) the rotations around the ﬁxed node. To
prevent the structure from rotating, we can also ﬁx the second node, by further requiring
u2 = 0. This serves to also eliminate the third and fourth columns of the original incidence
matrix. The resulting “doubly reduced” incidence matrix
A⋆⋆=
⎛
⎜
⎝
1
2
√
3
2
−1
2
√
3
2
0
0
⎞
⎟
⎠

6.3 Structures
331







Figure 6.13.
Three Bar Structure with Fixed Supports.
has trivial kernel: ker A⋆⋆= {0}. Therefore, the corresponding reduced stiﬀness matrix
K⋆⋆= (A⋆⋆)T A⋆⋆=

1
2
−1
2
0
√
3
2
√
3
2
0
⎛
⎜
⎝
1
2
√
3
2
−1
2
√
3
2
0
0
⎞
⎟
⎠=

1
2
0
0
3
2

is positive deﬁnite. A planar triangle with two ﬁxed nodes is a stable structure, which can
now support an arbitrary external forcing on the remaining free node. (Forces on the ﬁxed
nodes have no eﬀect, since they are no longer allowed to move.)
In general, a planar structure without any ﬁxed nodes will have at least a three-
dimensional kernel, corresponding to the rigid motions of translations and (linear approxi-
mations to) rotations. To stabilize the structure, one must ﬁx two (non-coincident) nodes.
A three-dimensional structure that is not tied to any ﬁxed supports will admit 6 inde-
pendent rigid motions in its kernel. Three of these correspond to rigid translations in the
three coordinate directions, while the other three correspond to linear approximations to
the rigid rotations around the three coordinate axes. To eliminate the rigid motion insta-
bilities of the structure, we need to ﬁx three non-collinear nodes. Indeed, ﬁxing one node
will eliminate translations; ﬁxing two nodes will still leave the rotations around the axis
through the ﬁxed nodes. Details can be found in the exercises.
Even after a suﬃcient number of nodes have been attached to ﬁxed supports so as to
eliminate all possible rigid motions, there may still remain nonzero vectors in the kernel
of the reduced incidence matrix of the structure. These indicate additional instabilities
that allow the shape of the structure to deform without any applied force. Such non-rigid
motions are known as mechanisms of the structure. Since a mechanism moves the nodes
without elongating any of the bars, it does not induce any internal forces. A structure that
admits a mechanism is unstable — even tiny external forces may provoke a large motion.
Example 6.7.
Consider the three-bar structure of Example 6.5, but now with its two
ends attached to supports, as pictured in Figure 6.13. Since we are ﬁxing nodes 1 and 4,
we set u1 = u4 = 0. Hence, we should remove the ﬁrst and last column pairs from the
incidence matrix (6.49), leading to the reduced incidence matrix
A⋆=
⎛
⎜
⎝
1
√
2
1
√
2
−1
0
0
0

0
0
1
0
−
1
√
2
1
√
2
⎞
⎟
⎠.
The structure no longer admits any rigid motions.
However, the kernel of A⋆is one-
dimensional, spanned by reduced displacement vector z⋆= ( 1, −1, 1, 1 )T , which corre-
sponds to the unstable mechanism that displaces the second node in the direction u2 =

332
6 Equilibrium
Figure 6.14.
Unstable Mechanism of the Three Bar Structure.
( 1, −1 )T and the third node in the direction u3 = ( 1, 1 )T . Geometrically, then, z⋆rep-
resents the displacement whereby node 2 moves down and to the right at a 45◦angle,
while node 3 moves simultaneously up and to the right at a 45◦angle; the result of the
mechanism is sketched in Figure 6.14. This mechanism does not alter the lengths of the
three bars (at least in our linear approximation regime) and so requires no net force to be
set into motion.
As with the rigid motions of the space station, an external forcing vector f ⋆will maintain
equilibrium only when it lies in the coimage of A⋆, and hence, by the Fredholm Alternative,
must be orthogonal to all the mechanisms in ker A⋆. Thus, the nodal forces f 2 = ( f2, g2 )T
and f 3 = ( f3, g3 )T must satisfy the balance law
z⋆· f ⋆= f2 −g2 + f3 + g3 = 0.
If this fails, the equilibrium equation has no solution, and the structure will be set into
motion. For example, a uniform horizontal force f2 = f3 = 1, g2 = g3 = 0, will induce
the mechanism, whereas a uniform vertical force, f2 = f3 = 0, g2 = g3 = 1, will maintain
equilibrium. In the latter case, the equilibrium equations
K⋆u⋆= f ⋆,
where
K⋆= (A⋆)T A⋆=
⎛
⎜
⎜
⎜
⎝
3
2
1
2
−1
0
1
2
1
2
0
0
−1
0
3
2
−1
2
0
0
−1
2
1
2
⎞
⎟
⎟
⎟
⎠,
have an indeterminate solution
u⋆= ( −3, 5, −2, 0 )T + t ( 1, −1, 1, 1 )T ,
since we can add in any element of ker K⋆= ker A⋆.
In other words, the equilibrium
position is not unique, since the structure can still be displaced in the direction of the
unstable mechanism while maintaining the overall force balance. On the other hand, the
elongations and internal forces
y = e = A⋆u⋆=
 √
2 , 1,
√
2
T ,
are well deﬁned, indicating that, under our stabilizing uniform vertical (upwards) force, all
three bars are elongated, with the two diagonals experiencing 41.4% more elongation than
the horizontal bar.
Remark. Just like the rigid rotations, the mechanisms described here are linear approx-
imations to the actual nonlinear motions. In a physical structure, the vertices will move

6.3 Structures
333
Figure 6.15.
Nonlinear Mechanism of the Three Bar Structure.
along curves whose tangents at the initial conﬁguration are the directions indicated by the
mechanism vector. In the linear approximation illustrated in Figure 6.14, the lengths of
the bars will change slightly. In the true nonlinear mechanism, illustrated in Figure 6.15,
the nodes must move along circles so as to rigidly preserve the lengths of all three bars. In
certain cases, a structure can admit a linear mechanism, but one that cannot be physically
realized due to the nonlinear constraints imposed by the geometrical conﬁgurations of the
bars. Nevertheless, such a structure is at best borderline stable, and should not be used in
any real-world constructions.
We can always stabilize a structure by ﬁrst ﬁxing nodes to eliminate rigid motions, and
then adding in a suﬃcient number of extra bars to prevent mechanisms. In the preceding
example, suppose we attach an additional bar connecting nodes 2 and 4, leading to the
reinforced structure in Figure 6.16. The revised incidence matrix is
A =
⎛
⎜
⎜
⎜
⎜
⎝
−1
√
2
−
1
√
2
0
0
0
0
0
0

1
√
2
1
√
2
−1
0
0
0
−
3
√
10
1
√
10

0
0
1
0
−1
√
2
1
√
2
0
0

0
0
0
0
1
√
2
−1
√
2
3
√
10
−
1
√
10
⎞
⎟
⎟
⎟
⎟
⎠
,
and is obtained from (6.49) by appending another row representing the added bar. When
nodes 1 and 4 are ﬁxed, the reduced incidence matrix
A⋆=
⎛
⎜
⎜
⎜
⎜
⎝
1
√
2
1
√
2
0
0
−1
0
1
0
0
0
−
1
√
2
1
√
2
−
3
√
10
1
√
10
0
0
⎞
⎟
⎟
⎟
⎟
⎠
has trivial kernel, ker A⋆= {0}, and hence the reinforced structure is stable. It admits no
mechanisms, and can support any conﬁguration of forces (within reason — mathematically
the structure will support an arbitrarily large external force, but very large forces will take
us outside the linear regime described by the model, and the structure may be crushed).
This particular case is statically determinate owing to the fact that the incidence matrix
is square and nonsingular, which implies that one can solve the force balance equations
(6.52) directly for the internal forces. For instance, a uniform downwards vertical force
f2 = f3 = 0, g2 = g3 = −1, e.g., gravity, will produce the internal forces
y1 = −
√
2,
y2 = −1,
y3 = −
√
2,
y4 = 0,
indicating that bars 1, 2 and 3 are compressed, while, interestingly, the reinforcing bar 4
remains unchanged in length and hence experiences no internal force. Assuming that the

334
6 Equilibrium








Figure 6.16.
Reinforced Planar Structure.


















Figure 6.17.
Doubly Reinforced Planar Structure.
bars are all of the same material, and taking the elastic constant to be 1, so C = I , then
the reduced stiﬀness matrix is
K⋆= (A⋆)T A⋆=
⎛
⎜
⎜
⎜
⎝
12
5
1
5
−1
0
1
5
3
5
0
0
−1
0
3
2
−1
2
0
0
−1
2
1
2
⎞
⎟
⎟
⎟
⎠.
The solution to the reduced equilibrium equations is
u⋆=

−1
2, −3
2, −3
2, −7
2
T ,
so
u2 =

−1
2, −3
2
T ,
u3 =

−3
2, −7
2
T ,
give the displacements of the two nodes under the applied force. Both are moving down
and to the left, with node 3 moving relatively farther owing to its lack of reinforcement.
Suppose we reinforce the structure yet further by adding in a bar connecting nodes 1
and 3, as in Figure 6.17. The resulting reduced incidence matrix
A⋆=
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
1
√
2
1
√
2
0
0
−1
0
1
0
0
0
−1
√
2
1
√
2
−
3
√
10
1
√
10
0
0
0
0
3
√
10
1
√
10
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
again has trivial kernel, ker A⋆= {0}, and hence the structure is stable. Indeed, adding
extra bars to a stable structure cannot cause it to lose stability. (In the language of lin-
ear algebra, appending additional rows to a matrix cannot increase the size of its kernel,
cf. Exercise 2.5.10.) Since the incidence matrix is rectangular, the structure is now statically
indeterminate, and we cannot determine the internal forces without ﬁrst solving the full

6.3 Structures
335
Figure 6.18.
A Swing Set.
equilibrium equations (6.53) for the displacements. The stiﬀness matrix is
K⋆= (A⋆)T A⋆=
⎛
⎜
⎜
⎜
⎝
12
5
1
5
−1
0
1
5
3
5
0
0
−1
0
12
5
−1
5
0
0
−1
5
3
5
⎞
⎟
⎟
⎟
⎠.
Under the same uniform vertical force, the displacement u⋆=
 1
10, −17
10, −1
10, −17
10
T
indicates that the free nodes now move symmetrically down and towards the center of the
structure. The internal forces on the bars are
y1 = −4
5
√
2,
y2 = −1
5,
y3 = −4
5
√
2,
y4 = −

2
5,
y5 = −

2
5.
All ﬁve bars are now experiencing compression, with the two outside bars being the most
stressed.
This relatively simple computation should already indicate to the practicing
construction engineer which of the bars in the structure are more likely to collapse under
an applied external force.
Summarizing our discussion, we have established the following fundamental result char-
acterizing the stability and equilibrium of structures.
Theorem 6.8. A structure is stable, and so will maintain its equilibrium under arbitrary
external forcing, if and only if its reduced incidence matrix A⋆has linearly independent
columns, or, equivalently, ker A⋆= {0}. More generally, an external force f ⋆on a structure
will maintain equilibrium if and only if f ⋆∈coimg A⋆= (ker A⋆)⊥, which requires that
the external force be orthogonal to all rigid motions and all mechanisms admitted by the
structure.
Example 6.9.
A three-dimensional swing set is to be constructed, consisting of two
diagonal supports at each end joined by a horizontal cross bar. Is this conﬁguration stable,
i.e., can a child swing on it without it collapsing? The movable joints are at positions
a1 = ( 1, 1, 3 )T ,
a2 = ( 4, 1, 3 )T ,
while the four ﬁxed supports are at
a3 = ( 0, 0, 0 )T ,
a4 = ( 0, 2, 0 )T ,
a5 = ( 5, 0, 0 )T ,
a6 = ( 5, 2, 0 )T .
The reduced incidence matrix for the structure is calculated in the usual manner:
A⋆=
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
1
√
11
1
√
11
3
√
11
1
√
11
−
1
√
11
3
√
11
−1
0
0
0
0
0
0
0
0

0
0
0
0
0
0
1
0
0
−
1
√
11
1
√
11
3
√
11
−
1
√
11
−
1
√
11
3
√
11
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎠
.

336
6 Equilibrium
Figure 6.19.
Reinforced Swing Set.
For instance, the ﬁrst three entries contained in the ﬁrst row refer to the unit vector
n1 =
a1 −a3
∥a1 −a3 ∥in the direction of the bar going from a3 to a1. Suppose the ﬁve bars
have the same stiﬀness c1 = · · · = c5 = 1, so the reduced stiﬀness matrix for the structure
is
K⋆= (A⋆)T A⋆=
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
13
11
0
6
11
−1
0
0
0
2
11
0
0
0
0
6
11
0
18
11
0
0
0
−1
0
0
13
11
0
−6
11
0
0
0
0
2
11
0
0
0
0
−6
11
0
18
11
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
.
Solving A⋆z⋆= 0, we ﬁnd ker A⋆= ker K⋆is one-dimensional, spanned by
z⋆= ( 3, 0, −1, 3, 0, 1 )T .
This indicates a mechanism that can cause the swing set to collapse: the ﬁrst node moves
down and to the right, while the second node moves up and to the right, the horizontal
motion being three times as large as the vertical. The swing set can only support forces
f 1 = ( f1, g1, h1 )T , f 2 = ( f2, g2, h2 )T on the free nodes whose combined force vector f ⋆is
orthogonal to the mechanism vector z⋆, and so
3 (f1 + f2) −h1 + h2 = 0.
Otherwise, a reinforcing bar, say from node 1 to node 6 (although this will interfere with
the swinging!) or another bar connecting one of the nodes to a new ground support, will
be required to completely stabilize the swing.
For a uniform downwards unit vertical force, f = ( 0, 0, −1, 0, 0, −1 )T , a particular
solution to (6.11) is u⋆=
 13
6 , 0, −4
3, 11
6 , 0, 0
T and the general solution u = u⋆+ t z⋆is
obtained by adding in an arbitrary element of the kernel. The resulting forces/elongations
are uniquely determined,
y = e = A⋆u = A⋆u⋆=
$
−
√
11
6 , −
√
11
6 , −1
3, −
√
11
6 , −
√
11
6
%T
,
so that every bar is compressed, the middle one experiencing slightly more than half the
stress of the outer supports.
If we add in two vertical supports at the nodes, as in Figure 6.19, then the corresponding

6.3 Structures
337
reduced incidence matrix
A⋆=
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
1
√
11
1
√
11
3
√
11
0
0
0
1
√
11
−
1
√
11
3
√
11
0
0
0
−1
0
0
1
0
0
0
0
0
−
1
√
11
1
√
11
3
√
11
0
0
0
−
1
√
11
−
1
√
11
3
√
11
0
0
1
0
0
0
0
0
0
0
0
1
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
has trivial kernel, indicating stabilization of the structure. The reduced stiﬀness matrix
K⋆=
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
13
11
0
6
11
−1
0
0
0
2
11
0
0
0
0
6
11
0
29
11
0
0
0
−1
0
0
13
11
0
−6
11
0
0
0
0
2
11
0
0
0
0
−6
11
0
29
11
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
is now only slightly diﬀerent, but this is enough to make it positive deﬁnite, K⋆> 0, and so
allow arbitrary external forcing without collapse. Under the same uniform vertical force,
the internal forces are
y = e = A⋆u =
$
−
√
11
10 , −
√
11
10 , −1
5, −
√
11
10 , −
√
11
10 , −2
5, −2
5
%T
.
Note the overall reductions in stress in the original bars; the two reinforcing vertical bars
are now experiencing the largest compression.
Further developments in the mathematical analysis of structures can be found in the
references [33, 79].
Exercises
6.3.1. If a bar in a structure compresses 2 cm under a force of 5 newtons applied to a node,
how far will it compress under a force of 20 newtons applied at the same node?
6.3.2. An individual bar in a structure experiences a stress of 3 under a unit horizontal force
applied to all the nodes and a stress of −2 under a unit vertical force applied to all nodes.
What combinations of horizontal and vertical forces will make the bar stress-free?
6.3.3.(a) For the reinforced structure illustrated in Figure 6.16, determine the displacements of
the nodes and the stresses in the bars under a uniform horizontal force, and interpret
physically.
(b) Answer the same question for the doubly reinforced structure in
Figure 6.17.
6.3.4. Discuss the eﬀect of a uniform horizontal force in the direction of the horizontal bar on
the swing set and its reinforced version in Example 6.9.

338
6 Equilibrium
♥6.3.5. All the bars in the illustrated square planar structure have unit
stiﬀness. (a) Write down the reduced incidence matrix A. (b) Write
down the equilibrium equations for the structure when subjected to
external forces at the free nodes. (c) Is the structure stable? statically
determinate? Explain in detail.
(d) Find a set of external forces
with the property that the upper left node moves horizontally, while
the upper right node stays in place. Which bar is under the most stress?
♥6.3.6. In the square structure of Exercise 6.3.5, the diagonal struts simply cross each other. We
could also try joining them at an additional central node. Compare the stresses in the two
structures under a uniform horizontal and a uniform vertical force at the two upper nodes,
and discuss what you observe.
6.3.7.(a) Write down the reduced incidence matrix A⋆for the pictured
structure with 4 bars and 2 ﬁxed supports. The width and the
height of the vertical sides are each 1 unit, while the top node
is 1.5 units above the base. (b) Predict the number of independent
solutions to A⋆u = 0, and then solve to describe them both
numerically and geometrically. (c) What condition(s) must be
imposed on the external forces to maintain equilibrium in the
structure? (d) Add in just enough additional bars so that the
resulting reinforced structure has only the trivial solution to
A⋆u = 0. Is your reinforced structure stable?
♥6.3.8. Consider the two-dimensional “house” constructed out of bars,
as in the accompanying picture. The bottom nodes are ﬁxed. The
width of the house is 3 units, the height of the vertical sides 1 unit,
and the peak is 1.5 units above the base.
(a) Determine the reduced incidence matrix A for this structure.
(b) How many distinct modes of instability are there? Describe them
geometrically, and indicate whether they are mechanisms or rigid motions.
(c) Suppose we apply a combination of forces to each non-ﬁxed node in the structure.
Determine conditions such that the structure can support the forces. Write down an
explicit nonzero set of external forces that satisfy these conditions, and compute the
corresponding elongations of the individual bars. Which bar is under the most stress?
(d) Add in a minimal number of bars so that the resulting structure can support any
force. Before starting, decide, from general principles, how many bars you need to add.
(e) With your new stable conﬁguration, use the same force as before, and recompute
the forces on the individual bars. Which bar now has the most stress? How much have
you reduced the maximal stress in your reinforced building?
♣6.3.9. Answer Exercise 6.3.8 for the illustrated
two- and three-dimensional houses. In the two-
dimensional case, the width and total height
of the vertical bars is 2 units, and the peak
is an additional .5 unit higher. In the three-
dimensional house, the width and vertical heights
are equal to 1 unit, the length is 3 units, while
the peaks are 1.5 units above the base.
♥6.3.10. Consider a structure consisting of three bars joined in a vertical line hanging from a top
support. (a) Write down the equilibrium equations for this system when only forces and
displacements in the vertical direction are allowed, i.e., a one-dimensional structure. Is the
problem statically determinate, statically indeterminate, or unstable? If the latter, describe
all possible mechanisms and the constraints on the forces required to maintain equilibrium.
(b) Answer part (a) when the structure is two-dimensional, i.e., is allowed to move in a
plane. (c) Answer the same question for the fully three-dimensional version.

6.3 Structures
339
♣6.3.11. A space station is built in the shape of a three-dimensional simplex whose nodes are at
the positions 0, e1, e2, e3 ∈R3, and each pair of nodes is connected by a bar. (a) Sketch
the space station and ﬁnd its incidence matrix A. (b) Show that ker A is six-dimensional,
and ﬁnd a basis. (c) Explain which three basis vectors correspond to rigid translations.
(d) Find three basis vectors that correspond to linear approximations to rotations around
the three coordinate axes. (e) Suppose the bars all have unit stiﬀness. Compute the full
stiﬀness matrix for the space station. (f ) What constraints on external forces at the four
nodes are required to maintain equilibrium? Can you interpret them physically? (g) How
many nodes do you need to ﬁx to stabilize the structure? (h) Suppose you ﬁx the three
nodes in the xy-plane. How much internal force does each bar experience under a unit
vertical force on the upper vertex?
♣6.3.12. Suppose a space station is built in the shape of a regular tetrahedron with all sides of
unit length. Answer all questions in Exercise 6.3.11.
♥6.3.13. A mass–spring ring consists of n masses connected in a circle by n identical springs,
and the masses are allowed only to move in the angular direction. (a) Derive the equations
of equilibrium. (b) Discuss stability, and characterize the external forces that will maintain
equilibrium.
(c) Find such a set of nonzero external forces in the case of a four-mass
ring and solve the equilibrium equations. What does the nonuniqueness of the solution
represent?
6.3.14. A structure in R3 has n movable nodes, admits no rigid motions, and is statically
determinate. (a) How many bars must it have? (b) Find an example with n = 3.
♦6.3.15. Prove that if we apply a unit force to node i in a structure and measure the
displacement of node j in the direction of the force, then we obtain the same value if
we apply the force to node j and measure the displacement at node i in the same direction.
Hint: First, solve Exercise 6.1.6.
6.3.16. True or false: A structure in R3 will admit no rigid motions if and only if at least 3
nodes are ﬁxed.
6.3.17. Suppose all bars have unit stiﬀness. Explain why the internal forces in a structure form
the solution of minimal Euclidean norm among all solutions to AT y = f.
♦6.3.18. Let A be the reduced incidence matrix for a structure and C the diagonal bar stiﬀness
matrix. Suppose f is a set of external forces that maintain equilibrium of the structure.
(a) Prove that f = AT C g for some g. (b) Prove that an allowable displacement u
is a least squares solution to the system Au = g with respect to the weighted norm
∥v∥2 = vT C v.
♥6.3.19. Suppose an unstable structure admits no rigid motions — only mechanisms. Let f be
an external force on the structure that maintains equilibrium. Suppose that you stabilize
the structure by adding in the minimal number of reinforcing bars. Prove that the given
force f induces the same stresses in the original bars, while the reinforcing bars experience
no stress. Are the displacements necessarily the same? Does the result continue to hold
when more reinforcing bars are added to the structure? Hint: Use Exercise 6.3.18.
♥6.3.20. When a node is ﬁxed to a roller, it is permitted to move only along a straight line —
the direction of the roller. Consider the three-bar structure in Example 6.5. Suppose node
1 is ﬁxed, but node 4 is attached to a roller that permits it to move only in the horizontal
direction. (a) Construct the reduced incidence matrix and the equilibrium equations in this
situation. You should have a system of 5 equations in 5 unknowns — the horizontal and
vertical displacements of nodes 2 and 3 and the horizontal displacement of node 4. (b) Is
your structure stable? If not, how many rigid motions and how many mechanisms does it
permit?
♥6.3.21. Answer Exercise 6.3.20 when the roller at node 4 allows it to move in only the vertical
direction.

340
6 Equilibrium
♥6.3.22. Redo Exercises 6.3.20–21 for the reinforced structure in Figure 6.16.
6.3.23.(a) Suppose that we ﬁx one node in a planar structure and put a second node on a
roller. Does the structure admit any rigid motions? (b) How many rollers are needed to
prevent all rigid motions in a three-dimensional structure? Are there any restrictions on the
directions of the rollers?
6.3.24. True or false: If a structure is statically indeterminate, then every non-zero applied
force will result in (a) one or more nodes having a non-zero displacement; (b) one or more
bars having a non-zero elongation.
6.3.25. True or false: If a structure constructed out of bars with identical stiﬀnesses is stable,
then the same structure constructed out of bars with diﬀering stiﬀnesses is also stable.

Chapter 7
Linearity
We began this book by learning how to systematically solve systems of linear algebraic
equations. This “elementary” problem formed our launching pad for developing the fun-
damentals of linear algebra. In its initial form, matrices and vectors were the primary
focus of our study, but the theory was developed in a suﬃciently general and abstract form
that it can be immediately used in many other useful situations — particularly inﬁnite-
dimensional function spaces. Indeed, applied mathematics deals, not just with algebraic
equations, but also with diﬀerential equations, diﬀerence equations, integral equations,
stochastic systems, diﬀerential delay equations, control systems, and many other types —
only a few of which, unfortunately, can be adequately developed in this introductory text.
It is now time to assemble what we have learned about linear algebraic systems and place
the results in a suitably general framework that will lead to insight into the key principles
that govern all linear systems arising in mathematics and its applications.
The most basic underlying object of linear systems theory is the vector space, and we
have already seen that the elements of vector spaces can be vectors, or functions, or even
vector-valued functions. The seminal ideas of span, linear independence, basis, and dimen-
sion are equally applicable and equally vital in more general contexts, particularly function
spaces. Just as vectors in Euclidean space are prototypes for elements of general vector
spaces, matrices are also prototypes for more general objects, known as linear functions.
Linear functions are also known as linear maps or, when one is dealing with function spaces,
linear operators, and include linear diﬀerential operators, linear integral operators, func-
tion evaluation, and many other basic operations. Linear operators on inﬁnite-dimensional
function spaces are the basic objects of quantum mechanics. Each quantum mechanical
observable (mass, energy, momentum) is formulated as a linear operator on an inﬁnite-
dimensional Hilbert space — the space of wave functions or states of the system, [54].
It is remarkable that quantum mechanics is an entirely linear theory, whereas classical
and relativistic mechanics are inherently nonlinear. The holy grail of modern physics —
the uniﬁcation of general relativity and quantum mechanics — is to resolve the apparent
incompatibility of the microscopic linear and macroscopic nonlinear physical regimes.
In geometry, linear functions are interpreted as linear transformations of space (or space-
time), and, as such, lie at the foundations of motion of bodies, such as satellites and planets;
computer graphics and games; video, animation, and movies; and the mathematical for-
mulation of symmetry. Many familiar geometrical transformations, including rotations,
scalings and stretches, reﬂections, projections, shears, and screw motions, are linear. But
including translational motions requires a slight extension of linearity, known as an aﬃne
transformation. The basic geometry of linear and aﬃne transformations will be developed
in Section 7.2.
Linear functions form the simplest class of functions on vector spaces, and must be
thoroughly understood before any serious progress can be made in the vastly more com-
plicated nonlinear world. Indeed, nonlinear functions are often approximated by linear
functions, generalizing the calculus approximation of a scalar function by its tangent line.
This linearization process is applied to nonlinear functions of several variables studied in
© Springer International Publishing AG, part of Springer Nature 2018 
https://doi.org/10.1007/978-3-319-91041-3_7 
341
P. J. Olver, C. Shakiban, Applied Linear Algebra, Undergraduate Texts in Mathematics, 

342
7 Linearity
multivariable calculus, as well as the nonlinear systems arising in physics and mechanics,
which can often be well approximated by linear diﬀerential equations.
A linear system is just an equation formed by a linear function. The most basic linear
system is a system of linear algebraic equations. Linear systems theory includes linear
diﬀerential equations, linear boundary value problems, linear integral equations, and so
on, all in a common conceptual framework. The fundamental ideas of linear superposition
and the relation between the solutions to inhomogeneous and homogeneous systems are
universally applicable to all linear systems. You have no doubt encountered many of these
concepts in your study of elementary ordinary diﬀerential equations. In this text, they have
already appeared in our discussion of the solutions to linear algebraic systems. The ﬁnal
section introduces the notion of the adjoint of a linear map between inner product spaces,
generalizing the transpose operation on matrices, the notion of a positive deﬁnite linear
operator, and the characterization of the solution to such a linear system by a minimization
principle. The full import of these fundamental concepts will appear in the context of linear
boundary value problems and partial diﬀerential equations, [61].
7.1 Linear Functions
We begin our study of linear functions with the basic deﬁnition. For simplicity, we shall
concentrate on real linear functions between real vector spaces. Extending the concepts
and constructions to complex linear functions on complex vector spaces is not diﬃcult, and
will be dealt with in due course.
Deﬁnition 7.1. Let V and W be real vector spaces. A function L: V →W is called linear
if it obeys two basic rules:
L[v + w] = L[v] + L[w],
L[c v] = c L[v],
(7.1)
for all v, w ∈V and all scalars c. We will call V the domain and W the codomain† for L.
In particular, setting c = 0 in the second condition implies that a linear function always
maps the zero element 0 ∈V to the zero element‡ 0 ∈W, so
L[0] = 0.
(7.2)
We can readily combine the two deﬁning conditions (7.1) into a single rule
L[c v + d w] = c L[v] + d L[w],
for all
v, w ∈V,
c, d ∈R,
(7.3)
that characterizes linearity of a function L. An easy induction proves that a linear function
respects linear combinations, so
L[c1v1 + · · · + ckvk ] = c1 L[v1 ] + · · · + ck L[vk ]
(7.4)
for all c1, . . . , ck ∈R and v1, . . . , vk ∈V .
The interchangeable terms linear map, linear operator, and, when V = W, linear trans-
formation are all commonly used as alternatives to “linear function”, depending on the
†
The terms “range” and “target” are also sometimes used for the codomain. However, some
authors use “range” to mean the image of L. An alternative name for domain is “source”.
‡
We will use the same notation for these two zero elements even though they may belong to
diﬀerent vector spaces. The reader should be able to determine where each lives from the context.

7.1 Linear Functions
343
circumstances and taste of the author. The term “linear operator” is particularly useful
when the underlying vector space is a function space, so as to avoid confusing the two
diﬀerent uses of the word “function”. As usual, we will often refer to the elements of a
vector space as “vectors”, even though they might be functions or matrices or something
else, depending on the context.
Example 7.2.
The simplest linear function is the zero function O[v] ≡0, which maps
every element v ∈V to the zero vector in W. Note that, in view of (7.2), this is the only
constant linear function; a nonzero constant function is not, despite its evident simplicity,
linear. Another simple but important linear function is the identity function I = I V : V →
V , which maps V to itself and leaves every vector unchanged: I [v] = v. Slightly more
generally, the operation of scalar multiplication Ma[v] = a v by a scalar a ∈R deﬁnes
a linear function from V to itself, with M0 = O, the zero function from V to itself, and
M1 = I , the identity function on V , appearing as special cases.
Example 7.3.
Suppose V = R. We claim that every linear function L: R →R has the
form
y = L[x] = ax,
for some constant a. Therefore, the only scalar linear functions are those whose graph is a
straight line passing through the origin. To prove this, we write x ∈R as a scalar product
x = x · 1. Then, by the second property in (7.1),
L[x] = L[x · 1] = x · L[1] = ax,
where
a = L[1],
as claimed.
Warning. Even though the graph of the function
y = ax + b,
(7.5)
is a straight line, it is not a linear function — unless b = 0, so the line goes through the
origin. The proper mathematical name for a function of the form (7.5) is an aﬃne function;
see Deﬁnition 7.21 below.
Example 7.4.
Let V = Rn and W = Rm.
Let A be an m × n matrix.
Then the
function L[v] = Av given by matrix multiplication is easily seen to be a linear function.
Indeed, the requirements (7.1) reduce to the basic distributivity and scalar multiplication
properties of matrix multiplication:
A(v + w) = Av + A w,
A(c v) = c Av,
for all
v, w ∈Rn,
c ∈R.
In fact, every linear function between two Euclidean spaces has this form.
Theorem 7.5. Every linear function L: Rn →Rm is given by matrix multiplication,
L[v] = Av, where A is an m × n matrix.
Warning. Pay attention to the order of m and n. While A has size m × n, the linear
function L goes from Rn to Rm.
Proof : The key idea is to look at what the linear function does to the basis vectors. Let
e1, . . . , en be the standard basis of Rn, as in (2.17), and let e1, . . . ,em be the standard

344
7 Linearity
L
v
w
v + w
L[v]
L[w]
L[v + w]
Figure 7.1.
Linear Function on Euclidean Space.
basis of Rm. (We temporarily place hats on the latter to avoid confusing the two.) Since
L[ej ] ∈Rm, we can write it as a linear combination of the latter basis vectors:
L[ej ] = aj =
⎛
⎜
⎜
⎜
⎝
a1j
a2j
...
amj
⎞
⎟
⎟
⎟
⎠= a1j e1 + a2j e2 + · · · + amj em,
j = 1, . . ., n.
(7.6)
Let us construct the m × n matrix
A = ( a1 a2 . . . an ) =
⎛
⎜
⎜
⎝
a11
a12
. . .
a1n
a21
a22
. . .
a2n
...
...
...
...
am1
am2
. . .
amn
⎞
⎟
⎟
⎠
(7.7)
whose columns are the image vectors (7.6). Using (7.4), we then compute the eﬀect of L
on a general vector v = ( v1, v2, . . . , vn )T ∈Rn:
L[v]= L[v1 e1 + · · · + vn en ] = v1 L[e1 ] + · · · + vn L[en ] = v1 a1 + · · · + vn an = Av.
The ﬁnal equality follows from our basic formula (2.13) connecting matrix multiplication
and linear combinations. We conclude that the vector L[v] coincides with the vector Av
obtained by multiplying v by the coeﬃcient matrix A.
Q.E.D.
The proof of Theorem 7.5 shows us how to construct the matrix representative of a
given linear function L: Rn →Rm. We merely assemble the image column vectors a1 =
L[e1 ], . . . , an = L[en ] into an m × n matrix A.
The two basic linearity conditions (7.1) have a simple geometrical interpretation. Since
vector addition is the same as completing the parallelogram sketched in Figure 7.1, the
ﬁrst linearity condition requires that L map parallelograms to parallelograms. The second
linearity condition says that if we stretch a vector by a factor c, then its image under L
must also be stretched by the same amount. Thus, one can often detect linearity by simply
looking at the geometry of the function.
Example 7.6.
As a speciﬁc example, consider the function Rθ: R2 →R2 that rotates
the vectors in the plane around the origin by a speciﬁed angle θ. This geometric trans-
formation clearly preserves parallelograms — see Figure 7.2. It also respects stretching
of vectors, and hence deﬁnes a linear function. In order to ﬁnd its matrix representative,

7.1 Linear Functions
345
θ
v
w
v + w
Rθ[v]
Rθ[w]
Rθ[v + w]
v
cv
Rθ[v]
Rθ[cv]
Figure 7.2.
Linearity of Rotations.
θ
θ
e1
e2
Rθ[e1 ]
Rθ[e2 ]
Figure 7.3.
Rotation in R2.
we need to ﬁnd out where the standard basis vectors e1, e2 are mapped.
Referring to
Figure 7.3, and keeping in mind that the rotated vectors also have unit length, we have
Rθ[e1 ] = (cos θ) e1 + (sin θ) e2 =

cos θ
sin θ

,
Rθ[e2 ] = −(sin θ) e1 + (cos θ) e2 =

−sin θ
cos θ

.
According to the general recipe (7.7), we assemble these two column vectors to obtain the
matrix form of the rotation transformation, and so
Rθ[v] = Aθ v,
where
Aθ =

cos θ
−sin θ
sin θ
cos θ

.
(7.8)
Therefore, rotating a vector v =

x
y

through angle θ produces the vector
v = Rθ[v] = Aθ v =

cos θ
−sin θ
sin θ
cos θ

x
y

=

x cos θ −y sin θ
x sin θ + y cos θ

with coordinates x = x cosθ −y sin θ, y = x sin θ + y cos θ. These formulas can be proved
directly, but, in fact, are a consequence of the underlying linearity of rotations.

346
7 Linearity
Exercises
7.1.1. Which of the following functions F: R3 →R are linear?
(a) F(x, y, z) = x,
(b) F(x, y, z) = y −2, (c) F(x, y, z) = x + y + 3, (d) F(x, y, z) = x −y −z,
(e) F(x, y, z) = xy z, (f ) F(x, y, z) = x2 −y2 + z2, (g) F(x, y, z) = ex−y+z.
7.1.2. Explain why the following functions F: R2 →R2 are not linear.
(a)

x + 2
x + y
	
,
(b)

x2
y2
	
,
(c)
 | y |
| x |
	
,
(d)

sin(x + y)
x −y
	
,
(e)

x + ey
2x + y
	
.
7.1.3. Which of the following functions F: R2 →R2 are linear?
(a) F

x
y
	
=

x −y
x + y
	
,
(b) F

x
y
	
=

x + y + 1
x −y −1
	
,
(c) F

x
y
	
=

xy
x −y
	
,
(d) F

x
y
	
=

3y
2x
	
,
(e) F

x
y
	
=

x2 + y2
x2 −y2
	
,
(f ) F

x
y
	
=

y −3x
x
	
.
7.1.4. Explain why the translation function T: R2 →R2, deﬁned by T

x
y
	
=

x + a
y + b
	
for
a, b ∈R, is almost never linear. Precisely when is it linear?
7.1.5. Find a matrix representation for the following linear transformations on R3:
(a) counterclockwise rotation by 90◦around the z-axis; (b) clockwise rotation by 60◦
around the x-axis; (c) reﬂection through the (x, y)-plane; (d) counterclockwise rotation
by 120◦around the line x = y = z; (e) rotation by 180◦around the line x = y = z;
(f ) orthogonal projection onto the xy-plane; (g) orthogonal projection onto the plane
x −y + 2z = 0.
7.1.6. Find a linear function L: R2 →R such that L

1
1
	
= 2 and L

1
−1
	
= 3. Is it unique?
7.1.7. Find a linear function L: R2 →R2 such that L

1
2
	
=

2
−1
	
and L

2
1
	
=

0
−1
	
.
7.1.8. Under what conditions does there exist a linear function L: R2 →R2 such that
L

x1
y1
	
=

a1
b1
	
and L

x2
y2
	
=

a2
b2
	
? Under what conditions is L uniquely deﬁned? In
the latter case, write down the matrix representation of L.
7.1.9. Can you construct a linear function L: R3 →R such that
L
⎛
⎜
⎝
1
−1
0
⎞
⎟
⎠= 1, L
⎛
⎜
⎝
1
0
−1
⎞
⎟
⎠= 4, and L
⎛
⎜
⎝
0
1
−1
⎞
⎟
⎠= −2? If yes, ﬁnd one. If not, explain why not.
♦7.1.10. Given a = ( a, b, c )T ∈R3, prove that the cross product map La[v] = a × v, as deﬁned
in (4.2), is linear, and ﬁnd its matrix representative.
7.1.11. Is the Euclidean norm function N(v) = ∥v∥, for v ∈Rn, linear?
7.1.12. Let V be a vector space. Prove that every linear function L: R →V has the form
L[x] = x b, where x ∈R, for some b ∈V .
7.1.13. True or false: The quadratic form Q(v) = vT Kv deﬁned by a symmetric n × n matrix
K deﬁnes a linear function Q: Rn →R.
♦7.1.14.(a) Prove that L is linear if and only if it satisﬁes (7.3).
(b) Use induction to prove that L satisﬁes (7.4).

7.1 Linear Functions
347
7.1.15. Let A =

a
b
c
d
	
, B =

p
q
r
s
	
be 2 × 2 matrices. For each of the following functions,
prove that L: M2×2 →M2×2 deﬁnes a linear map, and then ﬁnd its matrix representative
with respect to the standard basis

1
0
0
0
	
,

0
1
0
0
	
,

0
0
1
0
	
,

0
0
0
1
	
of M2×2:
(a) L[X ] = AX,
(b) R[X ] = X B,
(c) K[X ] = AX B.
7.1.16. The domain space of the following functions is the space of n × n real matrices A.
Which are linear? What is the codomain space in each case? (a) L[A] = 3A;
(b) L[A] = I −A; (c) L[A] = AT ; (d) L[A] = A−1; (e) L[A] = det A; (f ) L[A] = tr A;
(g) L[A] = ( a11, . . . , ann )T , i.e., the vector of diagonal entries of A;
(h) L[A] = Av, where v ∈Rn; (i) L[A] = vT Av, where v ∈Rn.
♦7.1.17. Let v1, . . . , vn be a basis of V and w1, . . . , wn be any vectors in W. Show that there is
a unique linear function L: V →W such that L[vi ] = wi, i = 1, . . . , n.
♥7.1.18. Bilinear functions: Let V, W, Z be vector spaces. A function that takes any pair of
vectors v ∈V and w ∈W to a vector z = B[v, w] ∈Z is called bilinear if, for each
ﬁxed w, it is a linear function of v, so B[cv + d v, w] = c B[v, w] + d B[ v, w], and, for
each ﬁxed v, it is a linear function of w, so B[v, cw + d w] = c B[v, w] + d B[v, w].
Thus, B: V × W →Z deﬁnes a function on the Cartesian product space V × W, as deﬁned
in Exercise 2.1.13. (a) Show that B[v, w] = v1 w1 −2v2 w2 is a bilinear function from
R2 × R2 to R. (b) Show that B[v, w] = 2v1 w2 −3v2 w3 is a bilinear function from
R2 × R3 to R. (c) Show that if V is an inner product space, then B[v, w] = ⟨v , w ⟩
deﬁnes a bilinear function B: V × V →R. (d) Show that if A is any m × n matrix, then
B[v, w] = vT Aw deﬁnes a bilinear function B: Rm × Rn →R. (e) Show that every
bilinear function B: Rm × Rn →R arises in this way. (f ) Show that a vector-valued
function B: Rm × Rn →Rk deﬁnes a bilinear function if and only if each of its components
Bi: Rm × Rn →R, for i = 1, . . . , k, is a bilinear function. (g) True or false: A bilinear
function B: V × W →Z deﬁnes a linear function on the Cartesian product space.
Linear Operators
So far, we have concentrated on linear functions on Euclidean space, and discovered that
they are all represented by matrices. For function spaces, there is a much wider variety of
linear operators available, and a complete classiﬁcation is out of the question. Let us look
at some of the main representative examples that arise in applications.
Example 7.7.
(a) Recall that C0[a, b] denotes the vector space consisting of all con-
tinuous functions on the interval [a, b].
Evaluation of the function at a point, namely
L[f ] = f(x0), deﬁnes a linear operator L: C0[a, b] →R, because
L[cf + dg ] = (cf + dg)(x0) = c f(x0) + d g(x0) = c L[f ] + d L[g ]
for any functions f, g ∈C0[a, b] and scalars (constants) c, d.
(b) Another real-valued linear function is the integration operator
I[f ] =
 b
a
f(x) dx,
(7.9)
that maps I: C0[a, b] →R. Linearity of I is an immediate consequence of the basic inte-
gration identity  b
a

cf(x) + dg(x)

dx = c
 b
a
f(x) dx + d
 b
a
g(x) dx,

348
7 Linearity
which is valid for arbitrary integrable — which includes continuous — functions f, g and
constants c, d.
(c) We have already seen that multiplication of functions by a constant, Mc[f(x)] =
cf(x), deﬁnes a linear map Mc: C0[a, b] →C0[a, b]; the particular case c = 1 reduces to the
identity transformation I = M1. More generally, if a(x) ∈C0[a, b] is a given continuous
function, then the operation Ma[f(x)] = a(x) f(x) of multiplication by a also deﬁnes a
linear transformation Ma: C0[a, b] →C0[a, b].
(d) Another important linear transformation is the indeﬁnite integral
J[f ] = g,
where
g(x) =
 x
a
f(y) dy.
(7.10)
According to the Fundamental Theorem of Calculus, [2, 78], the integral of a continuous
function is continuously diﬀerentiable. Therefore, J: C0[a, b] →C1[a, b] deﬁnes a linear
operator from the space of continuous functions to the space of continuously diﬀerentiable
functions.
(e) Conversely, diﬀerentiation of functions is also a linear operation. To be precise,
since not every continuous function can be diﬀerentiated, we take the domain space to be
the vector space C1[a, b] of continuously diﬀerentiable functions on the interval [a, b]. The
derivative operator
D[f ] = f ′
(7.11)
deﬁnes a linear operator D: C1[a, b] →C0[a, b]. This follows from the elementary diﬀeren-
tiation formula
D[cf + dg] = (cf + dg)′ = cf ′ + dg′ = c D[f ] + d D[g ],
valid whenever c, d are constant.
Exercises
7.1.19. Which of the following deﬁne linear operators on the vector space C1(R) of
continuously diﬀerentiable scalar functions? What is the codomain?
(a) L[f ] = f(0) + f(1), (b) L[f ] = f(0) f(1), (c) L[f ] = f′(1), (d) L[f ] = f′(3) −f(2),
(e) L[f ] = x2 f(x), (f ) L[f ] = f(x + 2), (g) L[f ] = f(x) + 2, (h) L[f ] = f′(2x),
(i) L[f ] = f′(x2), (j) L[f ] = f(x) sin x −f′(x) cos x, (k) L[f ] = 2 log f(0),
(l) L[f ] =
 1
0 ey f(y) dy, (m) L[f ] =
 1
0 | f(y) | dy, (n) L[f ] =
 x+1
x−1 f(y) dy,
(o) L[f ] =
 x2
x
f(y)
y
dy, (p) L[f ] =
 f(x)
0
y dy, (q) L[f ] =
 x
0 y2 f′(y) dy,
(r) L[f ] =
 1
−1 [f(y) −f(0)] dy, (s) L[f ] =
 x
−1 [f(y) −y ] dy.
7.1.20. True or false: The average or mean A[f ] =
1
b −a
 b
a f(x) dx of a function on the
interval [a, b] deﬁnes a linear operator A: C0[a, b] →R.
7.1.21. Prove that multiplication Mh[f(x)] = h(x) f(x) by a given function h ∈Cn[a, b] deﬁnes
a linear operator Mh: Cn[a, b] →Cn[a, b]. Which result from calculus do you need to
complete the proof?

7.1 Linear Functions
349
7.1.22. Show that if w(x) is any continuous function, then the weighted integral
Iw[f ] =
 b
a f(x) w(x) dx deﬁnes a linear operator Iw: C0[a, b] →R.
7.1.23.(a) Show that the partial derivatives ∂x[f ] = ∂f
∂x and ∂y[f ] = ∂f
∂y both deﬁne linear
operators on the space of continuously diﬀerentiable functions f(x, y).
(b) For which values of a, b, c, d is the map L[f ] = a ∂f
∂x + b ∂f
∂y + c f + d linear?
7.1.24. Prove that the Laplacian operator Δ[f ] = ∂2f
∂x2 + ∂2f
∂y2 deﬁnes a linear function on the
vector space of twice continuously diﬀerentiable functions f(x, y).
7.1.25. Show that the gradient G[f ] = ∇f deﬁnes a linear operator from the space of
continuously diﬀerentiable scalar-valued functions f: R2 →R to the space of continuous
vector ﬁelds v: R2 →R2.
7.1.26. Prove that, on R3, the gradient, curl, and divergence all deﬁne linear operators. Be
precise in your description of the domain space and the codomain space in each case.
The Space of Linear Functions
Given two vector spaces V, W, we use L(V, W ) to denote the set of all† linear functions
L: V →W.
We claim that L(V, W ) is itself a vector space.
We add linear functions
L, M ∈L(V, W ) in the same way we add general functions:
(L + M)[v] = L[v] + M[v].
You should check that L + M satisﬁes the linear function axioms (7.1), provided that L
and M do. Similarly, multiplication of a linear function by a scalar c ∈R is deﬁned so
that (cL)[v] = cL[v], again producing a linear function. The zero element of L(V, W ) is
the zero function O[v] ≡0. The veriﬁcation that L(V, W ) satisﬁes the basic vector space
axioms of Deﬁnition 2.1 is left to the reader.
In particular, if V = Rn and W = Rm, then Theorem 7.5 implies that we can identify
L(Rn, Rm ) with the space Mm×n of all m × n matrices. Addition of linear functions
corresponds to matrix addition, while scalar multiplication coincides with the usual scalar
multiplication of matrices. (Why?) Therefore, the space of all m × n matrices is a vector
space — a fact we already knew. The standard basis for Mm×n is given by the m n matrices
Eij, 1 ≤i ≤m, 1 ≤j ≤n, which have a single 1 in the (i, j) position and zeros everywhere
else. Therefore, the dimension of Mm×n is m n. Note that Eij corresponds to the speciﬁc
linear transformation that maps Eij[ej ] = ei, while Eij[ek ] = 0 whenever k ̸= j.
Example 7.8.
The space of linear transformations of the plane, L(R2, R2 ), is identiﬁed
with the space M2×2 of 2×2 matrices A =

a
b
c
d

. The standard basis of M2×2 consists
†
In inﬁnite-dimensional situations, one usually imposes additional restrictions, e.g., continuity
or boundedness of the linear operators. We shall relegate these more subtle distinctions to a more
advanced treatment of the subject. See [50, 67] for a full discussion of the rather sophisticated
analytical details, which play an important role in serious quantum mechanical applications.

350
7 Linearity
of the 4 = 2 · 2 matrices
E11 =

1
0
0
0

,
E12 =

0
1
0
0

,
E21 =

0
0
1
0

,
E22 =

0
0
0
1

.
Indeed, we can uniquely write any other matrix
A =

a
b
c
d

= aE11 + bE12 + cE21 + dE22,
as a linear combination of these four basis matrices. Of course, as with any vector space,
this is but one of many other possible bases of L(R2, R2 ).
Dual Spaces
A particularly important case is that in which the codomain of the linear functions is R.
Deﬁnition 7.9. The dual space to a vector space V is the vector space V ∗= L(V, R)
consisting of all real-valued linear functions ℓ: V →R.
If V = Rn, then, by Theorem 7.5, every linear function ℓ: Rn →R is given by multipli-
cation by a 1 × n matrix, i.e., a row vector. Explicitly,
ℓ[v] = a v = a1 v1 + · · · + an vn,
where
a = ( a1 a2 . . . an ),
v =
⎛
⎜
⎜
⎜
⎝
v1
v2
...
vn
⎞
⎟
⎟
⎟
⎠.
Therefore, we can identify the dual space (Rn)∗with the space of row vectors with n entries.
In light of this observation, the distinction between row vectors and column vectors is now
seen to be much more sophisticated than mere semantics or notation. Row vectors should
more properly be viewed as real-valued linear functions — the dual objects to column
vectors.
The standard dual basis ε1, . . . , εn of (Rn)∗consists of the standard row basis vectors;
namely, εj is the row vector with 1 in the jth slot and zeros elsewhere. The jth dual basis
element deﬁnes the linear function
Ej[v] = εj v = vj,
which picks oﬀthe jth coordinate of v — with respect to the original basis e1, . . . , en.
Thus, the dimensions of V = Rn and its dual V ∗= (Rn)∗are both equal to n.
An inner product structure provides a mechanism for identifying a vector space and its
dual. However, it should be borne in mind that this identiﬁcation will depend upon the
choice of inner product.
Theorem 7.10. Let V be a ﬁnite-dimensional real inner product space. Then every linear
function ℓ: V →R is given by taking the inner product with a ﬁxed vector a ∈V :
ℓ[v] = ⟨a , v ⟩.
(7.12)
Proof : Let v1, . . . , vn be a basis of V . If we write v = y1v1 +· · ·+ynvn, then, by linearity,
ℓ[v] = y1 ℓ[v1 ] + · · · + ynℓ[vn ] = b1 y1 + · · · + bn yn,
where
bi = ℓ[ui ].
(7.13)

7.1 Linear Functions
351
On the other hand, if we write a = x1 v1 + · · · + xn vn, then
⟨a , v ⟩=
n

i,j =1
xj yi ⟨vi , vj ⟩=
n

i,j =1
gijxjyi,
(7.14)
where G = (gij) is the n×n Gram matrix with entries gij = ⟨vi , vj ⟩. Equality of (7.13, 14)
requires that Gx = b, where x = ( x1, x2, . . . , xn )T , b = ( b1, b2, . . . , bn )T . Invertibility of
G as guaranteed by Theorem 3.34, allows us to solve for x = G−1b and thereby construct
the desired vector a. In particular, if v1, . . . , vn is an orthonormal basis, then G = I and
hence a = b1 v1 + · · · + bn vn.
Q.E.D.
Remark. For the particular case in which V = Rn is endowed with the standard dot
product, Theorem 7.10 identiﬁes a row vector representing a linear function with the cor-
responding column vector obtained by transposition a →aT . Thus, the na¨ıve identiﬁcation
of a row and a column vector is, in fact, an indication of a much more subtle phenomenon
that relies on the identiﬁcation of Rn with its dual based on the Euclidean inner product.
Alternative inner products will lead to alternative, more complicated, identiﬁcations of row
and column vectors; see Exercise 7.1.31 for details.
Important.
Theorem 7.10 is not true if V is inﬁnite-dimensional. This fact will have
important repercussions for the analysis of the diﬀerential equations of continuum mechan-
ics, which will lead us immediately into the much deeper waters of generalized function
theory, as described in [61].
Exercises
7.1.27. Write down a basis for and dimension of the linear function spaces (a) L(R3, R),
(b) L(R2, R2 ), (c) L(Rm, Rn ), (d) L(P(3), R), (e) L(P(2), R2 ), (f ) L(P(2), P(2) ).
Here P(n) is the space of polynomials of degree ≤n.
7.1.28. True or false: The set of linear transformations L: R2 →R2 such that L

1
0
	
=

0
0
	
is
a subspace of L(R2, R2 ). If true, what is its dimension?
7.1.29. True or false: The set of linear transformations L: R3 →R3 such that L
⎛
⎜
⎝
0
1
0
⎞
⎟
⎠=
⎛
⎜
⎝
0
1
0
⎞
⎟
⎠
is a subspace of L(R3, R3 ). If true, what is its dimension?
7.1.30. Consider the linear function L: R3 →R deﬁned by L(x, y, z) = 3x −y + 2z. Write down
the vector a ∈R3 such that L[v] = ⟨a , v ⟩when the inner product is (a) the Euclidean
dot product; (b) the weighted inner product ⟨v , w ⟩= v1 w1 + 2v2 w2 + 3v3 w3; (c) the
inner product deﬁned by the positive deﬁnite matrix K =
⎛
⎜
⎝
2
−1
0
−1
2
1
0
1
2
⎞
⎟
⎠.
♦7.1.31. Let Rn be equipped with the inner product ⟨v , w ⟩= vT K w. Let L[v] = r v where
r is a row vector of size 1 × n. (a) Find a formula for the column vector a such that (7.12)
holds for the linear function L: Rn →R.
(b) Illustrate your result when r = ( 2, −1 ),
using (i) the dot product (ii) the weighted inner product ⟨v , w ⟩= 3v1 w1 + 2v2 w2,
(iii) the inner product induced by K =

2
−1
−1
3
	
.

352
7 Linearity
♥7.1.32. Dual Bases: Given a basis v1, . . . , vn of V , the dual basis ℓ1, . . . , ℓn of V ∗consists of
the linear functions uniquely deﬁned by the requirements ℓi(vj) =
 1
i = j,
0,
i ̸= j.
(a) Show that ℓi[v] = xi gives the ith coordinate of a vector v = x1v1 + · · · + xnvn
with respect to the given basis. (b) Prove that the dual basis is indeed a basis for the dual
vector space.
(c) Prove that if V = Rn and A = ( v1 v2 . . . vn ) is the n × n matrix whose
columns are the basis vectors, then the rows of the inverse matrix A−1 can be identiﬁed as
the corresponding dual basis of (Rn)∗.
7.1.33. Use Exercise 7.1.32(c) to ﬁnd the dual basis for: (a) v1 =

1
1
	
, v2 =

1
−1
	
;
(b) v1 =

1
2
	
, v2 =

3
−1
	
; (c) v1 =
⎛
⎜
⎝
1
1
0
⎞
⎟
⎠, v2 =
⎛
⎜
⎝
1
0
1
⎞
⎟
⎠, v3 =
⎛
⎜
⎝
0
1
1
⎞
⎟
⎠; (d) v1 =
⎛
⎜
⎝
1
2
−3
⎞
⎟
⎠, v2 =
⎛
⎜
⎝
0
−3
1
⎞
⎟
⎠, v3 =
⎛
⎜
⎝
−1
2
2
⎞
⎟
⎠; (e) v1 =
⎛
⎜
⎜
⎜
⎝
1
1
0
0
⎞
⎟
⎟
⎟
⎠, v2 =
⎛
⎜
⎜
⎜
⎝
0
1
1
0
⎞
⎟
⎟
⎟
⎠, v3 =
⎛
⎜
⎜
⎜
⎝
0
0
1
1
⎞
⎟
⎟
⎟
⎠, v4 =
⎛
⎜
⎜
⎜
⎝
1
−1
1
2
⎞
⎟
⎟
⎟
⎠.
7.1.34. Let P(2) denote the space of quadratic polynomials equipped with the L2 inner
product ⟨p , q ⟩=
 1
0 p(x) q(x) dx. Find the polynomial q that represents the following
linear functions, i.e., such that L[p] = ⟨q , p ⟩:
(a) L[p] = p(0), (b) L[p] = 1
2 p′(1),
(c) L[p] =
 1
0 p(x) dx, (d) L[p] =
 1
−1 p(x) dx.
7.1.35. Find the dual basis, as deﬁned in Exercise 7.1.32, for the monomial basis of P(2) with
respect to the L2 inner product ⟨p , q ⟩=
 1
0 p(x) q(x) dx.
7.1.36. Write out a proof of Theorem 7.10 that does not rely on ﬁnding an orthonormal basis.
Composition
Besides adding and multiplying by scalars, one can also compose linear functions.
Lemma 7.11. Let V, W, Z be vector spaces. If L: V →W and M: W →Z are linear
functions, then the composite function M ◦L: V →Z, deﬁned by (M ◦L)[v] = M[L[v]]
is also linear.
Proof : This is straightforward:
(M ◦L)[c v + d w] = M[L[c v + d w]] = M[c L[v] + d L[w]]
= c M[L[v]] + d M[L[w]] = c (M ◦L)[v] + d (M ◦L)[w],
where we used, successively, the linearity of L and then of M.
Q.E.D.
For example, if L[v] = Av maps Rn to Rm, and M[w] = B w maps Rm to Rl, so that
A is an m × n matrix and B is a l × m matrix, then
(M ◦L)[v] = M[L[v]] = B(Av) = (B A) v,
and hence the composition M ◦L: Rn →Rl corresponds to the l × n product matrix BA.
In other words, on Euclidean space, composition of linear functions is the same as matrix
multiplication. And, like matrix multiplication, composition of (linear) functions is not, in
general, commutative.

7.1 Linear Functions
353
Example 7.12.
Composing two rotations results in another rotation: Rϕ ◦Rθ = Rϕ+θ.
In other words, if we ﬁrst rotate by angle θ and then by angle ϕ, the net eﬀect is rotation
by angle ϕ + θ. On the matrix level of (7.8), this implies that

cos ϕ
−sin ϕ
sin ϕ
cos ϕ
 
cos θ
−sin θ
sin θ
cos θ

= Aϕ Aθ = Aϕ+θ =

cos(ϕ + θ)
−sin(ϕ + θ)
sin(ϕ + θ)
cos(ϕ + θ)

.
Multiplying out the left-hand side, we deduce the well-known trigonometric addition
formulas
cos(ϕ + θ) = cos ϕ cos θ −sin ϕ sin θ,
sin(ϕ + θ) = cos ϕ sin θ + sin ϕ cos θ.
In fact, this constitutes a bona ﬁde proof of these two trigonometric identities!
Example 7.13.
One can build up more sophisticated linear operators on function space
by adding and composing simpler ones. In particular, higher order derivative operators
are obtained by composing the derivative operator D, deﬁned in (7.11), with itself. For
example,
D2[f ] = D ◦D[f ] = D[f ′ ] = f ′′
deﬁnes the second derivative operator. One needs to exercise due care about the domain
of deﬁnition, since not every function is diﬀerentiable. In general, the kth order derivative
Dk[f ] = f (k)(x) deﬁnes a linear operator Dk : Cn[a, b] −→Cn−k[a, b] for all n ≥k,
obtained by composing D with itself k times.
If we further compose Dk with the linear operation of multiplication by a given function
a(x) we obtain the linear operator (a Dk)[f ] = a(x) f (k)(x).
Finally, a general linear
ordinary diﬀerential operator of order n,
L = an(x)Dn + an−1(x)Dn−1 + · · · + a1(x)D + a0(x),
(7.15)
is obtained by summing such operators. If the coeﬃcient functions a0(x), . . ., an(x) are
continuous, then
L[u] = an(x) dnu
dxn + an−1(x) dn−1u
dxn−1 + · · · + a1(x) du
dx + a0(x)u
(7.16)
deﬁnes a linear operator from Cn[a, b] to C0[a, b]. The most important case — but certainly
not the only one arising in applications — is when the coeﬃcients ai(x) = ci are all constant.
Exercises
7.1.37. For each of the following pairs of linear functions S, T: R2 →R2, describe the
compositions S ◦T and T ◦S. Do the functions commute?
(a) S = counterclockwise rotation by 60◦; T = clockwise rotation by 120◦;
(b) S = reﬂection in the line y = x; T = rotation by 180◦;
(c) S = reﬂection in the x-axis; T = reﬂection in the y-axis;
(d) S = reﬂection in the line y = x; T = reﬂection in the line y = 2x;
(e) S = orthogonal projection on the x-axis; T = orthogonal projection on the y-axis;
(f ) S = orthogonal projection on the x-axis; T = orthogonal projection on the line y = x;
(g) S = orthogonal projection on the x-axis; T = rotation by 180◦;
(h) S = orthogonal projection on the x-axis; T = counterclockwise rotation by 90◦;
(i) S = orthogonal projection on the line y = −2x; T = reﬂection in the line y = x.

354
7 Linearity
7.1.38. Find a matrix representative for the linear functions (a) L: R2 →R2 that maps e1
to

1
−3
	
and e2 to

−1
2
	
; (b) M: R2 →R2 that takes e1 to

−1
−3
	
and e2 to

0
2
	
;
and (c) N: R2 →R2 that takes

1
−3
	
to

−1
−3
	
and

−1
2
	
to

0
2
	
. (d) Explain why
M = N ◦L. (e) Verify part (d) by multiplying the matrix representatives.
7.1.39. On the vector space R3, let R denote counterclockwise rotation around the x axis
by 90◦and S counterclockwise rotation around the z-axis by 90◦. (a) Find matrix
representatives for R and S.
(b) Show that R ◦S ̸= S ◦R. Explain what happens
to the standard basis vectors under the two compositions.
(c) Give an experimental
demonstration of the noncommutativity of R and S by physically rotating a solid object,
e.g., this book, in the prescribed manners.
7.1.40. Let P denote orthogonal projection of R3 onto the plane V = {z = x + y} and
Q denote orthogonal projection onto the plane W = {z = x −y }. Is the composition
R = Q ◦P the same as orthogonal projection onto the line L = V ∩W? Verify your
conclusion by computing the matrix representatives of P, Q, and R.
7.1.41.(a) Write the linear operator L[f(x)] = f′(b) as a composition of two linear functions.
Do your linear functions commute?
(b) For which values of a, b, c, d, e is
L[f(x)] = a f′(b) + c f(d) + e a linear function?
7.1.42. Let L = xD + 1, and M = D −x be diﬀerential operators. Find L ◦M and M ◦L. Do
the diﬀerential operators commute?
7.1.43. Show that the space of constant coeﬃcient linear diﬀerential operators of order ≤n is a
vector space. Determine its dimension by exhibiting a basis.
7.1.44.(a) Explain why the diﬀerential operator L = D ◦Ma ◦D obtained by composing the
linear operators of diﬀerentiation D[f(x)] = f′(x) and multiplication Ma[f(x)] = a(x) f(x)
by a given function a(x) deﬁnes a linear operator.
(b) Re-express L as a linear diﬀerential
operator of the form (7.16).
♦7.1.45.(a) Show that composition of linear functions is associative: (L ◦M) ◦N = L ◦(M ◦N).
Be precise about the domain and codomain spaces involved. (b) How do you know
the result is a linear function? (c) Explain why this proves associativity of matrix
multiplication.
7.1.46. Show that if p(x, y) is any polynomial, then L = p(∂x, ∂y) deﬁnes a linear, constant
coeﬃcient partial diﬀerential operator. For example, if p(x, y) = x2 + y2, then L = ∂2
x + ∂2
y
is the Laplacian operator Δ[f ] = ∂2f
∂x2 + ∂2f
∂y2 .
♥7.1.47. The commutator of two linear transformations L, M: V →V on a vector space V is
K = [ L, M ] = L ◦M −M ◦L.
(7.17)
(a) Prove that the commutator K is a linear transformation on V . (b) Explain why
Exercise 1.2.30 is a special case.
(c) Prove that L and M commute if and only if
[ L, M ] = O.
(d) Compute the commutators of the linear transformations deﬁned by the
following pairs of matrices:
(i)

1
1
0
1
	
,

−1
0
1
2
	
, (ii)

0
1
−1
0
	
,

1
0
0
−1
	
, (iii)
⎛
⎜
⎝
1
1
0
1
0
1
0
1
1
⎞
⎟
⎠,
⎛
⎜
⎝
−1
0
0
0
1
0
0
0
−1
⎞
⎟
⎠.
(e)
Prove that the Jacobi identity

[ L, M ], N

+

[ N, L ], M

+

[ M, N ], L

= O
(7.18)
is valid for any three linear transformations. (f ) Verify the Jacobi identity for the ﬁrst three
matrices in part (c). (g) Prove that the commutator B[L, M ] = [ L, M ] deﬁnes a bilinear
map B: L(V, V ) × L(V, V ) →L(V, V ) on the Cartesian product space, cf. Exercise 7.1.18.

7.1 Linear Functions
355
♦7.1.48.(a) In (one-dimensional) quantum mechanics, the diﬀerentiation operator
P[f(x)] = f′(x) represents the momentum of a particle, while the operator
Q[f(x)] = xf(x) of multiplication by the function x represents its position. Prove that the
position and momentum operators satisfy the Heisenberg Commutation Relations
[ P, Q ] = P ◦Q −Q ◦P = I .
(b) Prove that there are no matrices P, Q that satisfy the
Heisenberg Commutation Relations. Hint: Use Exercise 1.2.31.
Remark. The noncommutativity of quantum mechanical observables lies at the heart of
the Uncertainty Principle. The result in part (b) is one of the main reasons why quantum
mechanics must be an intrinsically inﬁnite-dimensional theory.
♥7.1.49. Let D(1) denote the set of all ﬁrst order linear diﬀerential operators L = p(x) D + q(x)
where p, q are polynomials. (a) Prove that D(1) is a vector space. Is it ﬁnite-dimensional
or inﬁnite-dimensional? (b) Prove that the commutator (7.17) of L, M ∈D(1) is a ﬁrst
order diﬀerential operator [ L, M ] ∈D(1) by writing out an explicit formula. (c) Verify the
Jacobi identity (7.18) for the ﬁrst order operators L = D, M = xD + 1, and N = x2 D + 2x.
7.1.50. Do the conclusions of Exercise 7.1.49(a–b) hold for the space D(2) of second order
diﬀerential operators L = p(x) D2 + q(x) D + r(x), where p, q, r are polynomials?
Inverses
The inverse of a linear function is deﬁned in direct analogy with the Deﬁnition 1.13 of the
inverse of a (square) matrix.
Deﬁnition 7.14. Let L: V →W be a linear function. If M: W →V is a function such
that both compositions
L ◦M = I W ,
M ◦L = I V ,
(7.19)
are equal to the identity function, then we call M the inverse of L and write M = L−1.
The two conditions (7.19) require
L[M[w]] = w
for all
w ∈W,
and
M[L[v]] = v
for all
v ∈V.
In Exercise 7.1.55, you are asked to prove that, when it exists, the inverse is unique. Of
course, if M = L−1 is the inverse of L, then L = M −1 is the inverse of M since the
conditions are symmetric, and, in such cases, (L−1)−1 = L.
Lemma 7.15. If it exists, the inverse of a linear function is also a linear function.
Proof : Let L, M satisfy the conditions of Deﬁnition 7.14. Given w, w ∈W, we note
w = (L ◦M)[w] = L[v],
w = (L ◦M)[ w] = L[v],
where
v = M[w],
v = M[ w].
Therefore, given scalars c, d, and using only the linearity of L,
M[cw + d w] = M[cL[v] + dL[v]] = (M ◦L)[cv + dv] = cv + dv = cM[w] + dM[ w],
proving linearity of M.
Q.E.D.
If V = Rn, W = Rm, so that L and M are given by matrix multiplication, by A and
B respectively, then the conditions (7.19) reduce to the usual conditions
AB = I ,
B A = I ,

356
7 Linearity
for matrix inversion, cf. (1.37). Therefore, B = A−1 is the inverse matrix. In particular,
for L: Rm →Rn to have an inverse, we must have m = n, and its coeﬃcient matrix A
must be nonsingular.
The invertibility of linear transformations on inﬁnite-dimensional function spaces is
more subtle. Here is a familiar example from calculus.
Example 7.16.
The Fundamental Theorem of Calculus says, roughly, that diﬀeren-
tiation D[f ] = f ′ and (indeﬁnite) integration J[f ] = g, where g(x) =
 x
a
f(y) dy, are
“inverse” operations. More precisely, the derivative of the indeﬁnite integral of f is equal
to f, and hence
D[J[f ]] = D[g] = g′ = f,
since
g′(x) = d
dx
 x
a
f(y) dy = f(x).
In other words, the composition D ◦J = I C0[a,b] deﬁnes the identity operator on the
function space C0[a, b]. On the other hand, if we integrate the derivative of a continuously
diﬀerentiable function f ∈C1[a, b], we obtain J[D[f ]] = J[f ′ ] = h, where
h(x) =
 x
a
f ′(y) dy = f(x) −f(a) ̸= f(x)
unless
f(a) = 0.
Therefore, the composition is not the identity operator: J ◦D ̸= I C1[a,b]. In other words,
the diﬀerentiation operator D is a left inverse for the integration operator J but not a right
inverse!
If we restrict D to the subspace V = { f | f(a) = 0 } ⊂C1[a, b] consisting of all continu-
ously diﬀerentiable functions that vanish at the left-hand endpoint, then J: C0[a, b] →V ,
and D: V →C0[a, b] are, by the preceding argument, inverse linear operators: D ◦J =
I C0[a,b], and J ◦D = I V . Note that V
⊊C1[a, b] ⊊C0[a, b]. Thus, we discover the
curious and disconcerting inﬁnite-dimensional phenomenon that J deﬁnes a one-to-one,
invertible, linear map from a vector space C0[a, b] to a proper subspace V ⊊C0[a, b]. This
paradoxical situation cannot occur in ﬁnite dimensions. A linear map L: Rn →Rn can be
invertible only when its image is the entire space — because it represents multiplication
by a nonsingular square matrix.
Two vector spaces V, W are said to be isomorphic, written V ≃W, if there exists an
invertible linear function L: V →W. For example, if V is ﬁnite-dimensional, then V ≃W
if and only if W has the same dimension as V . In particular, if V has dimension n, then
V ≃Rn. One way to construct the required invertible linear map is the choose a basis
v1, . . . , vn of V , and map it to the standard basis of Rn, so L[vk ] = ek for k = 1, . . . , n.
In general, given v = x1v1 + · · · + xnvn, then, by linearity,
L[v] = L[x1v1 + · · · + xnvn ] = x1L[v1 ] + · · · + xnL[vn ]
= x1e1 + · · · + xnen = ( x1, x2, . . . , xn )T = x,
and hence L maps v to the column vector x = Rn whose entries are its coordinates with
respect to the chosen basis.
The inverse L−1: Rn →V maps x ∈Rn to the element
L−1[x] = x1v1 + · · · + xnvn ∈V . As the above example makes clear, isomorphism of
inﬁnite-dimensional vector spaces is more subtle, and one often imposes additional restric-
tions on the allowable linear maps.

7.1 Linear Functions
357
Exercises
7.1.51. Determine which of the following linear functions L: R2 →R2 has an inverse, and,
if so, describe it: (a) the scaling transformation that doubles the length of each vector;
(b) clockwise rotation by 45◦; (c) reﬂection through the y-axis; (d) orthogonal projection
onto the line y = x; (e) the shearing transformation deﬁned by the matrix

1
2
0
1
	
.
7.1.52. For each of the linear functions in Exercise 7.1.51, write down its matrix representative,
the matrix representative of its inverse, and verify that the matrices are mutual inverses.
7.1.53. Let L: R2 →R2 be the linear function such that L[e1 ] = ( 1, −1 )T , L[e2 ] = ( 3, −2 )T .
Find L−1[e1 ] and L−1[e2 ].
7.1.54. Let L: R3 →R3 be the linear function such that L[e1 ] = ( 2, 1, −1 )T ,
L[e2 ] = ( 1, 2, 1 )T , L[e3 ] = ( −1, 2, 2 )T . Find L−1[e1 ], L−1[e2 ], and L−1[e3 ].
♦7.1.55. Prove that the inverse of a linear transformation is unique; i.e., given L, there is at
most one linear transformation M that can satisfy (7.19).
♦7.1.56. Let L: V →W be a linear function. Suppose M, N: W →V are linear functions that
satisfy L ◦M = I V = N ◦L. Prove that M = N = L−1. Thus, a linear function may have
only a left or a right inverse, but if it has both, then they must be the same.
7.1.57. Give an example of a matrix with a left inverse, but not a right inverse. Is your left
inverse unique?
♥7.1.58. Suppose v1, . . . , vn is a basis for V and w1, . . . , wn a basis for W. (a) Prove that there
is a unique linear function L: V →W such that L[vi ] = wi for i = 1, . . . , n. (b) Prove
that L is invertible. (c) If V = W = Rn, ﬁnd a formula for the matrix representative of the
linear functions L and L−1. (d) Apply your construction to produce a linear function that
takes:
(i) v1 =

1
0
	
, v2 =

0
1
	
to w1 =

3
1
	
, w2 =

5
2
	
,
(ii) v1 =

1
2
	
, v2 =

2
1
	
to w1 =

1
−1
	
, w2 =

1
1
	
,
(iii) v1 =
⎛
⎜
⎝
0
1
1
⎞
⎟
⎠, v2 =
⎛
⎜
⎝
1
0
1
⎞
⎟
⎠, v3 =
⎛
⎜
⎝
1
1
0
⎞
⎟
⎠to w1 =
⎛
⎜
⎝
1
0
0
⎞
⎟
⎠, w2 =
⎛
⎜
⎝
0
1
0
⎞
⎟
⎠, w3 =
⎛
⎜
⎝
0
0
1
⎞
⎟
⎠.
7.1.59. Suppose V, W ⊂Rn are subspaces of the same dimension. Prove that there is an
invertible linear function L: Rn →Rn that takes V to W. Hint: Use Exercise 7.1.58.
♦7.1.60. Let W, Z be complementary subspaces of a vector space V , as in Exercise 2.2.24. Let
V/W denote the quotient vector space, as deﬁned in Exercise 2.2.29. Show that the map
L: Z →V/W that maps L[z] = [z]W deﬁnes an invertible linear map, and hence Z ≃V/W
are isomorphic vector spaces.
♦7.1.61. Let L: V →W be a linear map. (a) Suppose V, W are ﬁnite-dimensional vector spaces,
and let A be a matrix representative of L. Explain why we can identify coker A ≃W/img A
and coimg A = V/ ker A as quotient vector spaces, cf. Exercise 2.2.29.
Remark.
These characterizations are used to give intrinsic deﬁnitions of the cokernel and
coimage of a general linear function L: V →W without any reference to a transpose (or, as
deﬁned below, adjoint) operation. Namely, set coker L ≃W/img L and coimg L = V/ ker L.
(b) The index of the linear map is deﬁned as index L = dim ker L −dim coker L,
using the above intrinsic deﬁnitions. Prove that, when V, W are ﬁnite-dimensional,
index L = dim V −dim W.

358
7 Linearity
Figure 7.4.
Rotation.
♦7.1.62. Let V be a ﬁnite-dimensional real inner product space and let V ∗be its dual. Using
Theorem 7.10, prove that the map J: V ∗→V that takes the linear function ℓ∈V ∗to the
vector J[ℓ] = a ∈V satisfying ℓ[v] = ⟨a , v ⟩deﬁnes a linear isomorphism between the
inner product space and its dual: V ∗≃V .
7.1.63.(a) Prove that L[p] = p′ + p deﬁnes an invertible linear map on the space P(2) of
quadratic polynomials. Find a formula for its inverse.
(b) Does the derivative D[p] = p′ have either a left or a right inverse on P(2)?
♥7.1.64.(a) Show that the set of all functions of the form f(x) = (ax2 + bx + c) ex for
a, b, c, ∈R is a vector space. What is its dimension? (b) Show that the derivative
D[f(x)] = f′(x) deﬁnes an invertible linear transformation on this vector space, and
determine its inverse.
(c) Generalize your result in part (b) to the inﬁnite-dimensional
vector space consisting of all functions of the form p(x)ex, where p(x) is an arbitrary
polynomial.
7.2 Linear Transformations
Consider a linear function L: Rn →Rn that maps n-dimensional Euclidean space to itself.
The function L maps a point x ∈Rn to its image point L[x] = A x, where A is its
n × n matrix representative. As such, it can be assigned a geometrical interpretation that
leads to further insight into the nature and scope of linear functions on Euclidean space.
The geometrically inspired term linear transformation is often used to refer to such linear
functions. The two-, three-, and four-dimensional (viewing time as the fourth dimension of
space-time) cases have particular relevance to our physical universe. Many of the notable
maps that appear in geometry, computer graphics, elasticity, symmetry, crystallography,
and Einstein’s special relativity, to name a few, are deﬁned by linear transformations.
Most of the important classes of linear transformations already appear in the two-dim-
ensional case. Every linear function L: R2 →R2 has the form
L

x
y

=

ax + by
cx + dy

,
where
A =

a
b
c
d

(7.20)
is an arbitrary 2 × 2 matrix. We have already encountered the rotation matrices
Rθ =

cos θ
−sin θ
sin θ
cos θ

,
(7.21)
whose eﬀect is to rotate every vector in R2 through an angle θ; in Figure 7.4 we illustrate
the eﬀect on a couple of square regions in the plane. Planar rotations coincide with 2 × 2

7.2 Linear Transformations
359
Figure 7.5.
Reﬂection through the y -axis.
Figure 7.6.
Reﬂection through the Diagonal.
proper orthogonal matrices, meaning matrices Q that satisfy
QT Q = I ,
det Q = +1.
(7.22)
The improper orthogonal matrices, i.e., those with determinant −1, deﬁne reﬂections. For
example, the matrix
A =

−1
0
0
1

corresponds to the linear transformation
L

x
y

=

−x
y

,
(7.23)
which reﬂects the plane through the y-axis. It can be visualized by thinking of the y-axis as
a mirror, as illustrated in Figure 7.5. Another simple example is the improper orthogonal
matrix
R =

0
1
1
0

.
The corresponding linear transformation
L

x
y

=

y
x

(7.24)
is a reﬂection through the diagonal line y = x, as illustrated in Figure 7.6.
A similar classiﬁcation of orthogonal matrices carries over to three-dimensional (and
even higher-dimensional) space. The proper orthogonal matrices correspond to rotations
and the improper orthogonal matrices to reﬂections, or, more generally, reﬂections com-
bined with rotations. For example, the proper orthogonal matrix
Zθ =
⎛
⎝
cos θ
−sin θ
0
sin θ
cos θ
0
0
0
1
⎞
⎠
(7.25)
corresponds to a counterclockwise rotation through an angle θ around the z-axis, while
Yϕ =
⎛
⎝
cos ϕ
0
−sin ϕ
0
1
0
sin ϕ
0
cos ϕ
⎞
⎠
(7.26)

360
7 Linearity
Figure 7.7.
A Three–Dimensional Rotation.
Figure 7.8.
Stretch along the x-axis.
corresponds to a clockwise rotation through an angle ϕ around the y-axis. In general, a
proper orthogonal matrix Q = ( u1 u2 u3 ) with columns ui = Qei corresponds to the
rotation in which the standard basis vectors e1, e2, e3 are rotated to new positions given
by the orthonormal basis u1, u2, u3. It can be shown — see Exercise 8.2.44 — that every
3 × 3 orthogonal matrix corresponds to a rotation around a line through the origin in R3
— the axis of the rotation, as sketched in Figure 7.7.
Since the product of two (proper) orthogonal matrices is also (proper) orthogonal, the
composition of two rotations is also a rotation. Unlike the planar case, the order in which
the rotations are performed is important! Multiplication of n×n orthogonal matrices is not
commutative when n ≥3. For example, rotating ﬁrst around the z-axis and then rotating
around the y-axis does not have the same eﬀect as ﬁrst rotating around the y-axis and
then around the z-axis. If you don’t believe this, try it out with a solid object such as this
book. Rotate through 90◦, say, around each axis; the ﬁnal conﬁguration of the book will
depend upon the order in which you do the rotations. Then prove this mathematically by
showing that the two rotation matrices (7.25, 26) do not commute.
Other important linear transformations arise from elementary matrices. First, the ele-
mentary matrices corresponding to the third type of row operations — multiplying a row
by a scalar — correspond to simple stretching transformations. For example, if
A =

2
0
0
1

,
then the linear transformation
L

x
y

=

2x
y

has the eﬀect of stretching along the x-axis by a factor of 2; see Figure 7.8. A negative di-
agonal entry corresponds to a reﬂection followed by a stretch. For example, the elementary

7.2 Linear Transformations
361
Figure 7.9.
Shear in the x Direction.
matrix

−2
0
0
1

=

2
0
0
1
 
−1
0
0
1

corresponds to a reﬂection through the y-axis followed by a stretch along the x-axis. In
this case, the order of these operations is immaterial, since the matrices commute.
In the 2×2 case, there is only one type of elementary row interchange, namely the matrix

0
1
1
0

, which corresponds to a reﬂection through the diagonal y = x, as in (7.24).
The elementary matrices of type #1 correspond to shearing transformations of the
plane. For example, the matrix

1
2
0
1

represents the linear transformation
L

x
y

=

x + 2y
y

,
which has the eﬀect of shearing the plane along the x-axis. The constant 2 will be called the
shear factor, and can be either positive or negative. Under the shearing transformation,
each point moves parallel to the x-axis by an amount proportional to its (signed) distance
from the axis. Similarly, the elementary matrix

1
0
−3
1

represents the linear transformation
L

x
y

=

x
y −3x

,
which is a shear along the y-axis of magnitude −3. As illustrated in Figure 7.9, shears map
rectangles to parallelograms; distances are altered, but areas are unchanged.
All of the preceding linear maps are invertible, and so are represented by nonsingular
matrices. Besides the zero map/matrix, which sends every point x ∈R2 to the origin, the
simplest singular map is

1
0
0
0

,
corresponding to the linear transformation
L

x
y

=

x
0

,
which deﬁnes the orthogonal projection of the vector ( x, y )T onto the x-axis. Other rank
one matrices represent various kinds of projections from the plane to a line through the
origin; see Exercise 7.2.16 for details.
A similar classiﬁcation of linear maps can be established in higher dimensions. The
linear transformations constructed from elementary matrices can be built up from the

362
7 Linearity
following four basic types:
(i) a stretch in a single coordinate direction;
(ii) a reﬂection through a coordinate plane;†
(iii) a reﬂection through a diagonal plane;
(iv) a shear along a coordinate axis.
Moreover, we already proved — see (1.47) — that every nonsingular matrix can be written
as a product of elementary matrices.
This has the remarkable consequence that every
invertible linear transformation can be constructed from a sequence of elementary stretches,
reﬂections, and shears. In addition, there is one further, non-invertible, type of basic linear
transformation:
(v) an orthogonal projection onto a lower-dimensional subspace.
All linear transformations of Rn can be built up, albeit non-uniquely, as a composition of
these ﬁve basic types.
Example 7.17.
Consider the matrix A =
⎛
⎝
√
3
2
−1
2
1
2
√
3
2
⎞
⎠corresponding to a plane rota-
tion through θ = 30◦, cf. (7.21). Rotations are not elementary linear transformations. To
express this particular rotation as a product of elementary matrices, we need to perform the
Gauss-Jordan Elimination procedure to reduce it to the identity matrix. Let us indicate
the basic steps:
E1 =

1
0
−
1
√
3
1

,
E1 A =
 √
3
2
−1
2
0
2
√
3

,
E2 =

1
0
0
√
3
2

,
E2 E1 A =
 √
3
2
−1
2
0
1

,
E3 =

2
√
3
0
0
1

,
E3 E2 E1 A =

1
−1
√
3
0
1

,
E4 =

1
1
√
3
0
1

,
E4 E3 E2 E1 A = I =

1
0
0
1

.
We conclude that
⎛
⎝
√
3
2
−1
2
1
2
√
3
2
⎞
⎠= A = E−1
1 E−1
2 E−1
3 E−1
4
=

1
0
1
√
3
1

1
0
0
2
√
3
 √
3
2
0
0
1

1
−1
√
3
0
1

.
As a result, a 30◦rotation can be eﬀected by composing the following elementary transfor-
mations in the prescribed order, bearing in mind that the last matrix in the product will
act ﬁrst on the vector x:
(1) First, a shear in the x direction with shear factor −1
√
3.
(2) Then a stretch (or, rather, a contraction) in the direction of the x-axis by
a factor of
√
3
2 .
(3) Then a stretch in the y direction by the reciprocal factor
2
√
3.
(4) Finally, a shear in the direction of the y-axis with shear factor
1
√
3.
†
In n-dimensional space, this should read “hyperplane”, i.e., a subspace of dimension n −1.

7.2 Linear Transformations
363
The fact that this combination of elementary transformations results in a pure rotation is
surprising and non-obvious.
Exercises
7.2.1. For each of the following linear transformations L: R2 →R2, ﬁnd a matrix
representative, and then describe its eﬀect on (i) the x-axis; (ii) the unit square
S = {0 ≤x, y ≤1}; (iii) the unit disk D = {x2 + y2 ≤1}: (a) counterclockwise
rotation by 45◦; (b) rotation by 180◦; (c) reﬂection in the line y = 2x; (d) shear
along the y-axis of magnitude 2; (e) shear along the line x = y of magnitude 3;
(f ) orthogonal projection on the line y = 2x.
7.2.2. Let L be the linear transformation represented by the matrix

0
1
−1
0
	
. Show that
L2 = L ◦L is rotation by 180◦. Is L itself a rotation or a reﬂection?
7.2.3. Let L be the linear transformation determined by

0
1
1
0
	
. Show L2 = I , and interpret
geometrically.
7.2.4. What is the geometric interpretation of the linear transformation with matrix
A =

1
0
2
−1
	
? Use this to explain why A2 = I .
7.2.5. Describe the image of the line ℓthat goes through the points

−2
1
	
,

1
−2
	
under the
linear transformation

2
3
−1
0
	
.
7.2.6. Draw the parallelogram spanned by the vectors

1
2
	
and

3
1
	
. Then draw its image
under the linear transformations deﬁned by the following matrices:
(a)

1
0
−1
1
	
,
(b)

0
1
1
0
	
,
(c)

1
2
−1
4
	
,
(d)
⎛
⎜
⎝
1
√
2
−1
√
2
1
√
2
1
√
2
⎞
⎟
⎠,
(e)

−1
−2
2
1
	
,
(f )
⎛
⎝
1
2
−1
2
−1
2
1
2
⎞
⎠,
(g)

2
−1
−4
2
	
.
7.2.7. Find a linear transformation that maps the unit circle x2 + y2 = 1 to the ellipse
1
4 x2 + 1
9 y2 = 1. Is your answer unique?
7.2.8. Find a linear transformation that maps the unit sphere x2 + y2 + z2 = 1 to the ellipsoid
x2 + 1
4 y2 + 1
16 z2 = 1.
7.2.9. True or false: A linear transformation L: R2 →R2 maps
(a) straight lines to straight lines; (b) triangles to triangles; (c) squares to squares;
(d) circles to circles; (e) ellipses to ellipses.
♦7.2.10.(a) Prove that the linear transformation associated with the improper orthogonal matrix

cos θ
sin θ
sin θ
−cos θ
	
is a reﬂection through the line that makes an angle 1
2 θ with the x-axis.
(b) Show that the composition of two such reﬂections, with angles θ, ϕ, is a rotation.
What is the angle of the rotation? Does the composition depend upon the order of the two
reﬂections?
7.2.11.(a) Find the matrix in R3 that corresponds to a counterclockwise rotation around
the x-axis through an angle 60◦.
(b) Write it as a product of elementary matrices, and
interpret each of the factors.

364
7 Linearity
♦7.2.12. Let L ⊂R2 be the line through the origin in the direction of a unit vector u. (a) Prove
that the matrix representative of reﬂection through L is R = 2uuT −I .
(b) Find the
corresponding formula for reﬂection through the line in the direction of a general nonzero
vector v ̸= 0.
(c) Determine the matrix representative for reﬂection through the line in
the direction (i) ( 1, 0 )T ,
(ii)
 3
5, −4
5
T ,
(iii) ( 1, 1 )T ,
(iv) ( 2, −3 )T .
7.2.13. Decompose the following matrices into a product of elementary matrices. Then
interpret each of the factors as a linear transformation.
(a)

0
2
−3
1
	
,
(b)

1
1
−1
1
	
,
(c)

3
1
1
2
	
,
(d)
⎛
⎜
⎝
1
1
0
1
0
1
0
1
1
⎞
⎟
⎠,
(e)
⎛
⎜
⎝
1
2
0
2
4
1
2
1
1
⎞
⎟
⎠.
7.2.14.(a) Prove that

cos θ
−sin θ
sin θ
cos θ
	
=

1
a
0
1
	 
1
0
b
1
	 
1
a
0
1
	
, where a = −tan 1
2 θ and
b = sin θ.
(b) Is the factorization valid for all values of θ? (c) Interpret the factorization
geometrically.
Remark. The factored version is less prone to numerical errors due to
round-oﬀ, and so can be used when extremely accurate numerical computations involving
rotations are required.
7.2.15. Determine the matrix representative for orthogonal projection P: R2 →R2 on the line
through the origin in the direction (a) ( 1, 0 )T , (b) ( 1, 1 )T , (c) ( 2, −3 )T .
♦7.2.16.(a) Prove that every 2 × 2 matrix of rank 1 can be written in the form A = uvT
where u, v ∈R2 are non-zero column vectors. (b) Which rank one matrices correspond to
orthogonal projection onto a one-dimensional subspace of R2?
7.2.17. Give a geometrical interpretation of the linear transformations on R3 deﬁned by each of
the six 3 × 3 permutation matrices (1.30).
7.2.18. Write down the 3 × 3 matrix Xψ representing a clockwise rotation in R3 around the
x-axis by angle ψ.
7.2.19. Explain why the linear map deﬁned by −I deﬁnes a rotation in two-dimensional space,
but a reﬂection in three-dimensional space.
♦7.2.20. Let u = ( u1, u2, u3 )T ∈R3 be a unit vector. Show that Qπ = 2uuT −I represents
rotation around the axis u through an angle π.
♦7.2.21. Let u ∈R3 be a unit vector. (a) Explain why the elementary reﬂection matrix
R = I −2uuT represents a reﬂection through the plane orthogonal to u.
(b) Prove that
R is an orthogonal matrix. Is it proper or improper?
(c) Write out R when
u = (i)
 3
5, 0, −4
5
T , (ii)
 3
13, 4
13, −12
13
T , (iii)
!
1
√
6, −2
√
6,
1
√
6
"T
.
(d) Give a
geometrical explanation why Qπ = −R represents the rotation of Exercise 7.2.20.
♦7.2.22. Let a ∈R3, and let Q be any 3 × 3 rotation matrix such that Qa = e3. (a) Show,
using the notation of (7.25), that Rθ = QT Zθ Q represents rotation around a by angle θ.
(b) Verify this formula in the case a = e2 by comparing with (7.26).
♥7.2.23. Quaternions: The skew ﬁeld H of quaternions can be identiﬁed with the vector space
R4 equipped with a noncommutative multiplication operation. The standard basis vectors
e1, e2, e3, e4 are traditionally denoted by the letters 1, i , j , k ; the vector ( a, b, c, d )T ∈R4
corresponds to the quaternion q = a + b i + c j + d k . Quaternion addition coincides with
vector addition. Quaternion multiplication is deﬁned so that
1 q = q = q 1, i 2 = j 2 = k 2 = −1, i j = k = −j i , i k = −j = −k i , j k = i = −k j ,
along with the distributive laws
(q + r)s = q s + rs,
q (r + s) = q r + q s,
for all
q, r, s ∈H.

7.2 Linear Transformations
365
(a) Compute the following quaternion products: (i) j (2−3 j + k ), (ii) (1+ i )(1−2 i + j ),
(iii) (1 + i −j −3 k )2, (iv) (2 + 2 i + 3 j −k )(2 −2 i −3 j + k ). (b) The conjugate of
the quaternion q = a + b i + c j + d k is deﬁned to be q = a −b i −c j −d k . Prove that
q q = ∥q ∥2 = q q, where ∥·∥is the usual Euclidean norm on R4. (c) Prove that quaternion
multiplication is associative. (d) Let q = a + b i + c j + d k ∈H. Show that Lq[r] = q r
and Rq[r] = rq deﬁne linear transformations on the vector space H ≃R4. Write down
their 4 × 4 matrix representatives, and observe that they are not the same, since quaternion
multiplication is not commutative. (e) Show that Lq and Rq are orthogonal matrices if
∥q ∥2 = a2 + b2 + c2 + d2 = 1. (f ) We can identify a quaternion q = b i + c j + d k
with zero real part, a = 0, with a vector q = ( b, c, d )T ∈R3. Show that, in this case, the
quaternion product q r = q × r −q · r can be identiﬁed with the diﬀerence between the cross
and dot product of the two vectors. Which vector identities result from the associativity
of quaternion multiplication?
Remark. The quaternions were discovered by the Irish
mathematician William Rowan Hamilton in 1843. Much of our modern vector calculus
notation is of quaternionic origin, [17].
Change of Basis
Sometimes a linear transformation represents an elementary geometrical transformation,
but this is not evident because the matrix happens to be written in the “wrong” coordinates.
The characterization of linear functions from Rn to Rm as multiplication by m×n matrices
in Theorem 7.5 relies on using the standard bases for both the domain and codomain. In
many cases, these bases are not particularly well adapted to the linear transformation in
question, and one can often gain additional insight by adopting more suitable bases. To
this end, we ﬁrst need to understand how to rewrite a linear transformation in terms of a
new basis.
The following result says that, in any basis, a linear function on ﬁnite-dimensional
vector spaces can always be realized by matrix multiplication of the coordinates. But bear
in mind that the particular matrix representative will depend upon the choice of bases.
Theorem 7.18. Let L: V →W be a linear function. Suppose V has basis v1, . . . , vn and
W has basis w1, . . . , wm. We can write
v = x1 v1 + · · · + xn vn ∈V,
w = y1 w1 + · · · + ym wm ∈W,
where x = ( x1, x2, . . . , xn )T are the coordinates of v relative to the basis of V and y =
( y1, y2, . . . , ym )T are those of w relative to the basis of W. Then, in these coordinates,
the linear function w = L[v] is given by multiplication by an m × n matrix B, so y = B x.
Proof : We mimic the proof of Theorem 7.5, replacing the standard basis vectors by more
general basis vectors. In other words, we will apply L to the basis vectors of V and express
the result as a linear combination of the basis vectors in W. Speciﬁcally, we write
L[vj ] =
m

i=1
bij wi.
The coeﬃcients bij form the entries of the desired coeﬃcient matrix. Indeed, by linearity,
L[v] = L[x1 v1 + · · · + xn vn ] = x1 L[v1 ] + · · · + xn L[vn ] =
m

i=1
⎛
⎝
n

j =1
bij xj
⎞
⎠wi,

366
7 Linearity
and so yi =
n

j =1
bij xj, as claimed.
Q.E.D.
Suppose that the linear transformation L: Rn →Rm is represented by a certain m × n
matrix A relative to the standard bases e1, . . . , en and e1, . . . ,em of the domain and
codomain. If we introduce alternative bases for Rn and Rm, then the same linear trans-
formation may have a completely diﬀerent matrix representation.
Therefore, diﬀerent
matrices may represent the same underlying linear transformation, but relative to diﬀerent
bases of its domain and codomain.
Example 7.19.
Consider the linear transformation
L[x] = L
6 
x1
x2
 7
=

x1 −x2
2x1 + 4x2

,
(7.27)
which we write in the standard, Cartesian coordinates on R2. The corresponding coeﬃcient
matrix
A =

1
−1
2
4

(7.28)
is the matrix representation of L, relative to the standard basis e1, e2 of R2, and can be
read directly oﬀthe explicit formula (7.27):
L[e1 ] = L
6 
1
0
 7
=

1
2

= e1 + 2 e2,
L[e2 ] = L
6 
0
1
 7
=

−1
4

= −e1 + 4 e2.
Let us see what happens if we replace the standard basis by the alternative basis
v1 =

1
−1

,
v2 =

1
−2

.
What is the corresponding matrix formulation of the same linear transformation? Accord-
ing to the recipe of Theorem 7.18, we must compute
L[v1 ] =

2
−2

= 2 v1,
L[v2 ] =

3
−6

= 3 v2.
The linear transformation acts by stretching in the direction v1 by a factor of 2 and
simultaneously stretching in the direction v2 by a factor of 3. Therefore, the matrix form
of L with respect to this new basis is the diagonal matrix
D =

2
0
0
3

.
(7.29)
In general,
L[a v1 + b v2 ] = 2a v1 + 3b v2,
and the eﬀect is to multiply the new basis coordinates a = ( a, b )T by the diagonal matrix
D.
Both (7.28) and (7.29) represent the same linear transformation — the former in
the standard basis and the latter in the new basis. The hidden geometry of this linear
transformation is thereby exposed through an inspired choice of basis. The secret behind
such well-adapted bases will be revealed in Chapter 8.
How does one eﬀect a change of basis in general?
According to formula (2.23), if
v1, . . . , vn form a basis of Rn, then the coordinates y = ( y1, y2, . . . , yn )T of a vector
( x1, x2, . . . , xn )T = x = y1 v1 + y2 v2 + · · · + yn vn

7.2 Linear Transformations
367
are found by solving the linear system
S y = x,
where
S = ( v1 v2 . . . vn )
(7.30)
is the nonsingular n × n matrix whose columns are the basis vectors.
Consider ﬁrst a linear transformation L: Rn →Rn from Rn to itself. When written
in terms of the standard basis, L[x] = A x has a certain n × n coeﬃcient matrix A. To
change to the new basis v1, . . . , vn, we use (7.30) to rewrite the standard x coordinates in
terms of the new y coordinates. We also need to ﬁnd the coordinates g of an image vector
f = A x with respect to the new basis. By the same reasoning that led to (7.30), its new
coordinates are found by solving the linear system f = S g. Therefore, the new codomain
coordinates are expressed in terms of the new domain coordinates via
g = S−1f = S−1A x = S−1AS y = B y.
We conclude that, in the new basis v1, . . . , vn, the matrix form of our linear transformation
is
B = S−1A S,
where
S = ( v1 v2 . . . vn ).
(7.31)
Two matrices A and B that are related by such an equation for some nonsingular matrix S
are called similar. Similar matrices represent the same linear transformation, but relative
to diﬀerent bases of the underlying vector space Rn, the matrix S serving to encode the
change of basis.
Example 7.19 (continued).
Returning to the preceding example, we assemble the new
basis vectors to form the change of basis matrix S =

1
1
−1
−2

, and verify that
S−1A S =

2
1
−1
−1
 
1
−1
2
4
 
1
1
−1
−2

=

2
0
0
3

= D,
reconﬁrming our earlier computation.
More generally, a linear transformation L: Rn →Rm is represented by an m × n matrix
A with respect to the standard bases on both the domain and codomain. What happens if
we introduce a new basis v1, . . . , vn on the domain space Rn and a new basis w1, . . . , wm
on the codomain Rm? Arguing as above, we conclude that the matrix representative of L
with respect to these new bases is given by
B = T −1A S,
(7.32)
where S = ( v1 v2 . . . vn ) is the domain basis matrix, while T = ( w1 w2 . . . wm ) is the
image basis matrix.
In particular, suppose that L has rank r = dim img A = dim coimg A. Let us choose
a basis v1, . . . , vn of Rn such that v1, . . . , vr form a basis of coimg A, while vr+1, . . . , vn
form a basis for ker A = (coimg A)⊥.
According to Theorem 4.49, the image vectors
w1 = L[v1 ], . . . , wr = L[vr ] form a basis for img A, while L[vr+1 ] = · · · = L[vn ] = 0. We
further choose a basis wr+1, . . . , wm for cokerA = (img A)⊥, and note that the combination
w1, . . . , wm is a basis for Rm. The matrix form of L relative to these two adapted bases is

368
7 Linearity
simply
B = T −1A S =

I r
O
O
O

=
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
1
0
0
. . .
0
0
. . .
0
0
1
0
. . .
0
0
. . .
0
0
0
1
. . .
0
0
. . .
0
...
...
...
...
...
...
...
...
0
0
0
. . .
1
0
. . .
0
0
0
0
. . .
0
0
. . .
0
...
...
...
...
...
...
...
...
0
0
0
. . .
0
0
. . .
0
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
.
(7.33)
In this matrix, the ﬁrst r columns have a single 1 in the diagonal slot, indicating that the
ﬁrst r basis vectors of the domain space are mapped to the ﬁrst r basis vectors of the
codomain, while the last n −r columns are all zero, indicating that the last n −r basis
vectors in the domain are all mapped to 0. Thus, by a suitable choice of bases on both
the domain and codomain, every linear transformation has an extremely simple canonical
form (7.33) that depends only on its rank.
Example 7.20.
According to the example following Theorem 2.49, the matrix
A =
⎛
⎝
2
−1
1
2
−8
4
−6
−4
4
−2
3
2
⎞
⎠
has rank 2. Based on those calculations, we choose the domain space basis
v1 =
⎛
⎜
⎝
2
−1
1
2
⎞
⎟
⎠,
v2 =
⎛
⎜
⎝
0
0
−2
4
⎞
⎟
⎠,
v3 =
⎛
⎜
⎜
⎝
1
2
1
0
0
⎞
⎟
⎟
⎠,
v4 =
⎛
⎜
⎝
−2
0
2
1
⎞
⎟
⎠,
noting that v1, v2 are a basis for coimg A, while v3, v4 are a basis for ker A. For our basis
of the codomain, we ﬁrst compute w1 = Av1 and w2 = Av2, which form a basis for img A.
We supplement these by the single basis vector w3 for cokerA, where
w1 =
⎛
⎝
10
−34
17
⎞
⎠,
w2 =
⎛
⎝
6
−4
2
⎞
⎠,
w3 =
⎛
⎝
0
1
2
1
⎞
⎠.
By construction, B[v1 ] = w1, B[v2 ] = w2, B[v3 ] = B[v4 ] = 0, and thus the canonical
matrix form of this particular linear function is given in terms of these two bases as
B = T −1A S =
⎛
⎝
1
0
0
0
0
1
0
0
0
0
0
0
⎞
⎠,
where the bases are assembled to form the matrices
S =
⎛
⎜
⎝
2
0
1
2
−2
−1
0
1
0
1
−2
0
2
2
4
0
1
⎞
⎟
⎠,
T =
⎛
⎝
10
6
0
−34
−4
1
2
17
2
1
⎞
⎠.

7.2 Linear Transformations
369
Exercises
7.2.24. Find the matrix form of the linear transformation L(x, y) =

x −4y
−2x + 3y
	
with respect
to the following bases of R2:
(a)

1
0
	
,

0
1
	
, (b)

2
0
	
,

0
3
	
, (c)

1
1
	
,

−1
1
	
, (d)

2
1
	
,

−1
1
	
, (e)

3
2
	
,

2
3
	
.
7.2.25. Find the matrix form of L[x] =
⎛
⎜
⎝
−3
2
2
−3
1
3
−1
2
0
⎞
⎟
⎠x with respect to the following
bases of R3:
(a)
⎛
⎜
⎝
2
0
0
⎞
⎟
⎠,
⎛
⎜
⎝
0
−1
0
⎞
⎟
⎠,
⎛
⎜
⎝
0
0
−2
⎞
⎟
⎠,
(b)
⎛
⎜
⎝
1
0
1
⎞
⎟
⎠,
⎛
⎜
⎝
0
−1
1
⎞
⎟
⎠,
⎛
⎜
⎝
1
1
1
⎞
⎟
⎠,
(c)
⎛
⎜
⎝
2
1
2
⎞
⎟
⎠,
⎛
⎜
⎝
0
1
−1
⎞
⎟
⎠,
⎛
⎜
⎝
1
−2
1
⎞
⎟
⎠.
7.2.26. Find bases of the domain and codomain that place the following matrices in the
canonical form (7.33). Use (7.32) to check your answer.
(a)

1
2
2
1
	
,
(b)

1
−3
4
−2
6
−8
	
, (c)
⎛
⎜
⎝
2
3
0
4
−1
1
⎞
⎟
⎠, (d)
⎛
⎜
⎝
1
2
1
1
−1
−1
2
1
0
⎞
⎟
⎠, (e)
⎛
⎜
⎜
⎜
⎝
1
3
0
1
2
6
1
−2
−1
−3
−1
3
0
0
−1
4
⎞
⎟
⎟
⎟
⎠.
7.2.27.(a) Show that every invertible linear function L: Rn →Rn can be represented by the
identity matrix by choosing appropriate (and not necessarily the same) bases on the domain
and codomain.
(b) Which linear transformations are represented by the identity matrix
when the domain and codomain are required to have the same basis? (c) Find bases of R2
so that the following linear transformations are represented by the identity matrix: (i) the
scaling map S[x] = 2x; (ii) counterclockwise rotation by 45◦; (iii) the shear

1
0
−2
1
	
.
♦7.2.28. Suppose a linear transformation L: Rn →Rn is represented by a symmetric matrix
with respect to the standard basis e1, . . . , en. (a) Prove that its matrix representative
with respect to any orthonormal basis u1, . . . , un is symmetric. (b) Is it symmetric when
expressed in terms of a non-orthonormal basis?
♦7.2.29. In this exercise, we show that every inner product ⟨· , · ⟩on Rn can be reduced to
the dot product when expressed in a suitably adapted basis. (a) Speciﬁcally, prove
that there exists a basis v1, . . . , vn of Rn such that ⟨x , y ⟩=
n

i=1
ci di = c · d, where
c = ( c1, c2, . . . , cn )T are the coordinates of x and d = ( d1, d2, . . . , dn )T those of y with
respect to the basis. Is the basis uniquely determined?
(b) Find bases that reduce the
following inner products to the dot product on R2:
(i) ⟨v , w ⟩= 2v1w1 + 3v2w2,
(ii) ⟨v , w ⟩= v1w1 −v1w2 −v2w1 + 3v2w2.
♥7.2.30. Dual functions: Let L: V →W be a linear function between vector spaces. The dual
linear function, denoted by L∗: W∗→V ∗(note the change in direction) is deﬁned so that
L∗(m) = m ◦L for all linear functions m ∈W∗. (a) Prove that L∗is a linear function.
(b) If M: W →Z is linear, prove that (M ◦L)∗= L∗◦M∗. (c) Suppose dim V = n and
dim W = m. Prove that if L is represented by the m × n matrix A with respect to bases of
V, W, then L∗is represented by the n × m transposed matrix AT with respect to the dual
bases, as deﬁned in Exercise 7.1.32.

370
7 Linearity
Figure 7.10.
Translation.
♦7.2.31. Suppose A is an m×n matrix. (a) Let v1, . . . , vn be a basis of Rn, and Avi = wi ∈Rm,
for i = 1, . . . , n. Prove that the vectors v1, . . . , vn, w1, . . . , wn, serve to uniquely specify A.
(b) Write down a formula for A.
7.3 Aﬃne Transformations and Isometries
Not every transformation of importance in geometrical applications arises as a linear func-
tion. A simple example is a translation, whereby all the points in Rn are moved in the
same direction by a common distance. The function that accomplishes this is
T[x] = x + b,
x ∈Rn,
(7.34)
where b ∈Rn determines the direction and the distance that the points are translated.
Except in the trivial case b = 0, the translation T is not a linear function because
T[x + y] = x + y + b ̸= T[x] + T[y] = x + y + 2b.
Or, even more simply, we note that T[0] = b, which must be 0 if T is to be linear.
Combining translations and linear functions leads us to an important class of geometrical
transformations.
Deﬁnition 7.21. A function F: Rn →Rn of the form
F[x] = A x + b,
(7.35)
where A is an n × n matrix and b ∈Rn, is called an aﬃne transformation.
In general, F[x] is an aﬃne transformation if and only if L[x] = F[x]−F[0] is a linear
function. In the particular case (7.35), F[0] = b, and so L[x] = A x. The word “aﬃne”
comes from the Latin “aﬃnus”, meaning “related”, because such transformations preserve
the relation of parallelism between lines; see Exercise 7.3.2.
For example, every aﬃne transformation from R to R has the form
f(x) = α x + β.
(7.36)
As mentioned earlier, even though the graph of f(x) is a straight line, f is not a linear
function — unless β = 0, and the line goes through the origin. Thus, to be mathematically
accurate, we should refer to (7.36) as a one-dimensional aﬃne transformation.

7.3 Aﬃne Transformations and Isometries
371
Example 7.22.
The aﬃne transformation
F(x, y) =

0
−1
1
0
 
x
y

+

1
−2

=

−y + 1
x −2

has the eﬀect of ﬁrst rotating the plane R2 by 90◦about the origin, and then translating
by the vector ( 1, −2 )T . The reader may enjoy proving that this combination has the same
eﬀect as just rotating the plane through an angle of 90◦centered at the point
 3
4, −1
2

.
For details, see Exercise 7.3.14.
Note that the aﬃne transformation (7.35) can be obtained by composing a linear func-
tion L[x] = A x with a translation T[x] = x + b, so
F[x] = T ◦L[x] = T[L[x]] = T[A x] = A x + b.
The order of composition is important, since G = L ◦T deﬁnes the slightly diﬀerent aﬃne
transformation
G[x] = L ◦T[x] = L[T[x]] = L[x + b] = A(x + b) = A x + c,
where
c = Ab.
More generally, the composition of any two aﬃne transformations is again an aﬃne trans-
formation. Speciﬁcally, given
F[x] = A x + a,
G[y] = B y + b,
then
(G ◦F)[x] = G[F[x]] = G[A x + a] = B (A x + a) + b = C x + c,
where
C = B A,
c = B a + b.
(7.37)
Note that the coeﬃcient matrix of the composition is the product of the coeﬃcient matrices,
but the resulting vector of translation is not the sum of the two translation vectors.
Exercises
7.3.1. True or false: An aﬃne transformation takes (a) straight lines to straight lines;
(b) triangles to triangles; (c) squares to squares; (d) circles to circles; (e) ellipses to ellipses.
♦7.3.2.(a) Let F: Rn →Rn be an aﬃne transformation. Let L1, L2 ⊂Rn be two parallel lines.
Prove that F[L1 ] and F[L2 ] are also parallel lines.
(b) Is the converse valid: if F: Rn →Rn maps parallel lines to parallel lines, then F is
necessarily an aﬃne transformation?
7.3.3. Describe the image of (i) the x-axis, (ii) the unit disk x2 + y2 ≤1, (iii) the unit square
0 ≤x, y ≤1, under the following aﬃne transformations:
(a) T1

x
y
	
=

x
y
	
+

2
−1
	
,
(b) T2

x
y
	
=

3
0
0
2
	 
x
y
	
+

−1
0
	
,
(c) T3

x
y
	
=

1
2
0
1
	 
x
y
	
+

1
2
	
,
(d) T4

x
y
	
=

0
1
−1
0
	 
x
y
	
+

1
0
	
,
(e) T5

x
y
	
=

.6
.8
−.8
.6
	 
x
y
	
+

−3
2
	
,
(f ) T6

x
y
	
=
⎛
⎝
1
2
1
2
1
2
1
2
⎞
⎠

x
y
	
+

1
0
	
,
(g) T7

x
y
	
=

1
1
−1
1
	 
x
y
	
+

2
−3
	
,
(h) T8

x
y
	
=

2
1
−2
−1
	 
x
y
	
+

1
1
	
.

372
7 Linearity
7.3.4. Using the aﬃne transformations in Exercise 7.3.3, write out the following compositions
and verify that they satisfy (7.37):
(a) T3 ◦T4,
(b) T4 ◦T3,
(c) T3 ◦T6,
(d) T6 ◦T3,
(e) T7 ◦T8,
(f ) T8 ◦T7.
7.3.5. Describe the image of the triangle with vertices (−1, 0), (1, 0), (0, 2) under the aﬃne
transformation T

x
y
	
=

4
−1
2
5
	 
x
y
	
+

3
−4
	
.
7.3.6. Under what conditions is the composition of two aﬃne transformations
(a) a translation?
(b) a linear function?
7.3.7.(a) Under what conditions does an aﬃne transformation have an inverse?
(b) Is the
inverse an aﬃne transformation? If so, ﬁnd a formula for its matrix and vector constituents.
(c) Find the inverse, when it exists, of each of the the aﬃne transformations in Exercise
7.3.3.
♦7.3.8. Let v1, . . . , vn be a basis for Rn. (a) Show that every aﬃne transformation
F[x] = A x + b on Rn is uniquely determined by the n + 1 vectors w0 = F[0], w1 =
F[v1 ], . . . ,
wn = F[vn ]. (b) Find the formula for A and b when v1 = e1, . . . , vn = en are the
standard basis vectors. (c) Find the formula for A, b for a general basis v1, . . . , vn.
7.3.9. Show that the space of all aﬃne transformations on Rn is a vector space. What is its
dimension?
♦7.3.10. In this exercise, we establish a useful matrix representation for aﬃne transformations.
We identify Rn with the n-dimensional aﬃne subspace (as in Exercise 2.2.28)
Vn = { ( x, 1 )T = ( x1, . . . , xn, 1 )T } ⊂Rn+1
consisting of vectors whose last coordinate is ﬁxed at xn+1 = 1. (a) Show that
multiplication of vectors

x
1
	
∈Vn by the (n + 1) × (n + 1) aﬃne matrix

A
b
0
1
	
coincides with the action (7.35) of an aﬃne transformation on x ∈Rn. (b) Prove that
the composition law (7.37) for aﬃne transformations corresponds to multiplication of their
aﬃne matrices. (c) Deﬁne the inverse of an aﬃne transformation in the evident manner,
and show that it corresponds to the inverse aﬃne matrix.
Isometry
A transformation that preserves distance is known as an isometry. (The mathematical
term metric refers to the underlying norm or distance on the space; thus, “isometric”
translates as “distance-preserving”.) In Euclidean geometry, the isometries coincide with
the rigid motions — translations, rotations, reﬂections, and the aﬃne maps they generate
through composition.
Deﬁnition 7.23. Let V be a normed vector space. A function F: V →V is called an
isometry if it preserves distance, meaning
d(F[v], F[w]) = d(v, w)
for all
v, w ∈V.
(7.38)
Since the distance between points is just the norm of the vector connecting them,
d(v, w) = ∥v −w∥, cf. (3.33), the isometry condition (7.38) can be restated as
&& F[v] −F[w]
&& = ∥v −w∥
for all
v, w ∈V.
(7.39)
Clearly, any translation
T[v] = v + a,
where
a ∈V,

7.3 Aﬃne Transformations and Isometries
373
deﬁnes an isometry, since T[v]−T[w] = v−w. A linear transformation L: V →V deﬁnes
an isometry if and only if
&& L[v]
&& = ∥v∥
for all
v ∈V,
(7.40)
because, by linearity,
∥L[v] −L[w]∥= ∥L[v −w]∥= ∥v −w∥.
A similar computation proves that an aﬃne transformation F[v] = L[v]+a is an isometry
if and only if its linear part L[v] is.
As noted above, the simplest class of isometries comprises the translations
T[x] = x + b
(7.41)
in the direction b. For the standard Euclidean norm on V = Rn, the linear isometries
consist of rotations and reﬂections. As we shall prove, both are characterized by orthogonal
matrices:
L[x] = Q x,
where
QT Q = I .
(7.42)
The proper isometries correspond to the rotations, with det Q = +1, and can be realized
as physical motions; improper isometries, with det Q = −1, are then obtained by reﬂection
in a mirror.
Proposition 7.24. A linear transformation L[x] = Q x deﬁnes a Euclidean isometry of
Rn if and only if Q is an orthogonal matrix.
Proof : The linear isometry condition (7.40) requires that
&& Q x
&&2 = (Q x)TQ x = xTQT Q x = xT x = ∥x∥2
for all
x ∈Rn.
According to Exercise 4.3.16, this holds if and only if QTQ = I , which is precisely the
condition (4.29) that Q be an orthogonal matrix.
Q.E.D.
It can be proved, [93], that the most general Euclidean isometry of Rn is an aﬃne
transformation, and hence of the form F[x] = Q x + b, where Q is an orthogonal matrix
and b is a vector. Therefore, every Euclidean isometry or rigid motion is a combination of
translations, rotations, and reﬂections.
In the two-dimensional case, the proper linear isometries R[x] = Q x with det Q = 1
represent rotations around the origin. More generally, a rotation of the plane around a
center at c is represented by the aﬃne isometry
R[x] = Q(x −c) + c = Q x + b,
where
b = ( I −Q)c,
(7.43)
and where Q is a rotation matrix. In Exercise 7.3.14, we ask you to prove that every plane
isometry is either a translation or a rotation around a center.
In three-dimensional space, both translations (7.41) and rotations around a center (7.43)
continue to deﬁne proper isometries. There is one additional type, representing the motion
of a point on the head of a screw. A screw motion is an aﬃne transformation of the form
S[x] = Q x + a,
(7.44)
where the 3 × 3 orthogonal matrix Q represents a rotation through an angle θ around a
ﬁxed axis in the direction of the vector a, which is also the direction of the translation

374
7 Linearity
a
Figure 7.11.
A Screw Motion.
term. The result is indicated in Figure 7.11; the trajectory followed by a point not on the
axis is a circular helix centered on the axis. For example,
Sθ
⎛
⎝
x
y
z
⎞
⎠=
⎛
⎝
cos θ
−sin θ
0
sin θ
cos θ
0
0
0
1
⎞
⎠
⎛
⎝
x
y
z
⎞
⎠+
⎛
⎝
0
0
a
⎞
⎠
represents a vertical screw along the z-axis through an angle θ by a distance a. In Exercise
8.2.45 you are asked to prove that every proper isometry of R3 is either a translation, a
rotation, or a screw motion.
The isometries of R2 and R3 are indispensable for understanding of how physical objects
move in three-dimensional space. Basic computer graphics and animation require eﬃcient
implementation of rigid isometries in three-dimensional space and their compositions —
coupled with appropriate (nonlinear) perspective maps prescribing the projection of three-
dimensional objects onto a two-dimensional viewing screen, [12, 72].
Exercises
Note: All exercises are based on the Euclidean norm unless otherwise noted.
7.3.11. Which of the indicated maps F(x, y) deﬁne isometries of the Euclidean plane?
(a)

y
−x
	
, (b)

x −2
y −1
	
, (c)

x −y + 1
x + 2
	
, (d)
1
√
2

x + y −3
x + y −2
	
, (e) 1
5

3x + 4y
−4x + 3y + 1
	
.
7.3.12. Prove that the planar aﬃne isometry F

x
y
	
=

−y + 1
x −2
	
represents a rotation
through an angle of 90◦around the center
 3
2, −1
2
T .
7.3.13. True or false: The map L[x] = −x for x ∈Rn deﬁnes (a) an isometry; (b) a rotation.
♦7.3.14. Prove that every proper aﬃne plane isometry F[x] = Q x + b of R2, where det Q = 1,
is either (i) a translation, or (ii) a rotation (7.43) centered at some point c ∈R2.
Hint: Use Exercise 1.5.7.
7.3.15. Compute both compositions F ◦G and G ◦F of the following aﬃne transformations on
R2. Which pairs commute? (a) F = counterclockwise rotation around the origin by 45◦;
G = translation in the y direction by 3 units. (b) F = counterclockwise rotation around
the point ( 1, 1 )T by 30◦; G = counterclockwise rotation around the point ( −2, 1 )T by 90◦.
(c) F = reﬂection through the line y = x + 1; G = rotation around ( 1, 1 )T by 180◦.

375
♥7.3.16. In R2, show the following: (a) The composition of two aﬃne isometries is another
aﬃne isometry. (b) The composition of two translations is another translation. (c) The
composition of a translation and a rotation (not necessarily centered at the origin) in either
order is a rotation. (d) The composition of two plane rotations is either another rotation or
a translation. What is the condition for the latter possibility? (e) Every plane translation
can be written as the composition of two rotations.
ℓ
d
♦7.3.17. Let ℓbe a line in R2. A glide reﬂection is an aﬃne map on R2 composed
of a translation in the direction of ℓby a distance d followed by a reﬂection
through ℓ. Find the formula for a glide reﬂection along
(a) the x-axis by a distance 2; (b) the line y = x by a distance 3 in
the direction of increasing x; (c) the line x + y = 1 by a distance 2
in the direction of increasing x.
♦7.3.18. Let ℓbe the line in the direction of the unit vector u through the point a. (a) Write
down the formula for the aﬃne map deﬁning the reﬂection through the line ℓ. Hint: Use
Exercise 7.2.12. (b) Write down the formula for the glide reﬂection, as deﬁned in Exercise
7.3.17, along ℓby a distance d in the direction of u. (c) Prove that every improper aﬃne
plane isometry is either a reﬂection or a glide reﬂection. Hint: Use Exercise 7.2.10.
♥7.3.19. A set of n + 1 points a0, . . . , an ∈Rn is said to be in general position if the diﬀerences
ai −aj span Rn. (a) Show that the points are in general position if and only if they do
not all lie in a proper aﬃne subspace A ⊊Rn, cf. Exercise 2.2.28. (b) Let a0, . . . , an and
b0, . . . , bn be two sets in general position. Show that there is an isometry F: Rn →Rn
such that F[ai ] = bi for all i = 0, . . . , n, if and only if their interpoint distances agree:
∥ai −aj ∥= ∥bi −bj ∥for all 0 ≤i < j ≤n. Hint: Use Exercise 4.3.19.
♦7.3.20. Suppose that V is an inner product space and L: V →V is an isometry, so
∥L[v]∥= ∥v∥for all v ∈V . Prove that L also preserves the inner product:
⟨L[v] , L[w] ⟩= ⟨v , w ⟩. Hint: Look at ∥L[v + w]∥2.
♦7.3.21. Let V be a normed vector space. Prove that a linear map L: V →V deﬁnes an
isometry of V for the given norm if and only if it maps the unit sphere S1 = {∥u∥= 1}
to itself: L[S1 ] = { L[u] | u ∈S1 } = S1.
7.3.22.(a) List all linear and aﬃne isometries of R2 with respect to the ∞norm. Hint: Use
Exercise 7.3.21.
(b) Can you generalize your results to R3?
7.3.23. Answer Exercise 7.3.22 for the 1 norm.
♥7.3.24. A matrix of the form H =

cosh α
sinh α
sinh α
cosh α
	
for α ∈R deﬁnes a hyperbolic rotation
of R2. (a) Prove that all hyperbolic rotations preserve the indeﬁnite quadratic form
q(x) = x2 −y2 in the sense that q(H x) = q(x) for all x = ( x, y )T ∈R2. Observe
that ordinary rotations preserve circles x2 + y2 = a, while hyperbolic rotations preserve
hyperbolas x2 −y2 = a.
(b) Are there any other aﬃne transformations of R2 that
preserve the quadratic form q(x)?
Remark. The four-dimensional version of this
construction, i.e., aﬃne maps preserving the indeﬁnite Minkowski form t2 −x2 −y2 −z2,
forms the geometrical foundation for Einstein’s theory of special relativity, [55].
p
x
q
ℓ
♥7.3.25. Let ℓ⊂R2 be a line, and p ̸∈ℓa point. A perspective map
takes a point x ∈R2 to the point q ∈ℓthat is the intersection of
ℓwith the line going through p and x. If the line is parallel to ℓ, then
the map is not deﬁned. Find the formula for the perspective map when
(a) ℓis the x-axis and p = ( 0, 1 )T , (b) ℓis the line y = x and
p = ( 1, 0 )T . Is either map aﬃne? An isometry? Remark. Mapping
three-dimensional objects onto a two-dimensional screen (or your retina)
is based on perspective maps, which are thus of fundamental importance
in art, optics, computer vision, computer graphics and animation, and computer games.
7.3 Aﬃne Transformations and Isometries

376
7 Linearity
7.4 Linear Systems
The abstract notion of a linear system serves to unify, in a common conceptual framework,
linear systems of algebraic equations, linear diﬀerential equations, both ordinary and par-
tial, linear boundary value problems, linear integral equations, linear control systems, and
a huge variety of other linear systems that appear in all aspects of mathematics and its
applications. The idea is simply to replace matrix multiplication by a general linear func-
tion. Many of the structural results we learned in the matrix context have, when suitably
formulated, direct counterparts in these more general frameworks. The result is a uniﬁed
understanding of the basic properties and nature of solutions to all such linear systems.
Deﬁnition 7.25. A linear system is an equation of the form
L[u] = f,
(7.45)
in which L: U →V is a linear function between vector spaces, the right-hand side is an
element of the codomain, f ∈V , while the desired solution belongs to the domain, u ∈U.
The system is homogeneous if f = 0; otherwise, it is called inhomogeneous.
Example 7.26.
If U = Rn and V = Rm, then, according to Theorem 7.5, every linear
function L: Rn →Rm is given by matrix multiplication: L[u] = Au. Therefore, in this
particular case, every linear system is a matrix system, namely Au = f.
Example 7.27.
A linear ordinary diﬀerential equation takes the form L[u] = f, where
L is an nth order linear diﬀerential operator of the form (7.15), and the right-hand side is,
say, a continuous function. Written out, the diﬀerential equation takes the familiar form
L[u] = an(x) dnu
dxn + an−1(x) dn−1u
dxn−1 + · · · + a1(x) du
dx + a0(x)u = f(x).
(7.46)
You should have already gained some familiarity with solving the constant coeﬃcient case
as covered, for instance, in [7, 22].
Example 7.28.
Let K(x, y) be a function of two variables that is continuous for all
a ≤x, y ≤b. Then the integral
IK[u] =
 b
a
K(x, y) u(y) dy
deﬁnes a linear operator IK: C0[a, b] →C0[a, b], known as an integral transform. Impor-
tant examples include the Fourier and Laplace transforms, [61, 79]. Finding the inverse
transform requires solving a linear integral equation IK[u] = f, which has the explicit form
 b
a
K(x, y) u(y) dy = f(x).
Example 7.29.
We can combine linear maps to form more complicated, “mixed” types
of linear systems. For example, consider a typical initial value problem
u′′ + u′ −2u = x,
u(0) = 1,
u′(0) = −1,
(7.47)
for an unknown scalar function u(x). The diﬀerential equation can be written as a linear
system
L[u] = x,
where
L[u] = (D2 + D −2)[u] = u′′ + u′ −2u

7.4 Linear Systems
377
is a linear, constant coeﬃcient diﬀerential operator. Further,
M[u] =
⎛
⎝
L[u]
u(0)
u′(0)
⎞
⎠=
⎛
⎝
u′′(x) + u′(x) −2u(x)
u(0)
u′(0)
⎞
⎠
deﬁnes a linear map whose domain is the space U = C2 of twice continuously diﬀeren-
tiable functions u(x), and whose image is the vector space V consisting of all triples†
v =
⎛
⎝
f(x)
a
b
⎞
⎠, where f ∈C0 is a continuous function and a, b ∈R are real constants. You
should convince yourself that V is indeed a vector space under the evident addition and
scalar multiplication operations. In this way, we can write the initial value problem (7.47)
in linear systems form as M[u] = f, where f = ( x, 1, −1 )T .
A similar construction applies to linear boundary value problems. For example, the
boundary value problem
u′′ + u = ex,
u(0) = 1,
u(1) = 2,
is in the form of a linear system
B[u] = f,
where
B[u] =
⎛
⎝
u′′(x) + u(x)
u(0)
u(1)
⎞
⎠,
f =
⎛
⎝
ex
1
2
⎞
⎠.
Note that B: C2 →V deﬁnes a linear map having the same domain and codomain as the
initial value problem map M.
Exercises
7.4.1. True or false: If F[x] is an aﬃne transformation on Rn, then the equation F[x] = c
deﬁnes a linear system.
7.4.2. Place each of the following linear systems in the form (7.45). Carefully describe the
linear function, its domain, its codomain, and the right-hand side of the system. Which
systems are homogeneous?
(a) 3x + 5 = 0, (b) x = y + z, (c) a = 2b −3, b = c −1,
(d) 3(p−2) = 2(q −3), p+q = 0, (e) u′ +3xu = 0, (f ) u′ +3x = 0, (g) u′ = u, u(0) = 1,
(h) u′′ −u = ex, u(0) = 3u(1), (i) u′′ + x2 u = 3x, u(0) = 1, u′(0) = 0, (j) u′ = v,
v′ = 2u, (k) u′′ −v′′ = 2u −v, u(0) = v(0), u(1) = v(1), (l) u(x) = 1 −3
 x
0 u(y) dy,
(m)
 ∞
0
u(t) e−s t dt = 1 + s2, (n)
 1
0 u(x) dx = u
 1
2

, (o)
 1
0 u(y) dy =
 1
0 y v(y) dy,
(p) ∂u
∂t + 2 ∂u
∂x = 1, (q) ∂u
∂x = ∂v
∂y ,
∂u
∂y = −∂v
∂x , (r) −∂2u
∂x2 −∂2u
∂y2 = x2 + y2 −1.
7.4.3. The Fredholm Alternative of Theorem 4.46 ﬁrst appeared in the study of what are now
known as Fredholm integral equations: u(x) +
 b
a K(x, y) u(y) dy = f(x), in which K(x, y)
and f(x) are prescribed continuous functions. Explain how the integral equation is a linear
system; i.e., describe the linear map L, its domain and codomain, and prove linearity.
†
This is a particular case of the general Cartesian product construction between vector spaces;
here V = C0 × R2. See Exercise 2.1.13 for details.

378
7 Linearity
7.4.4. Answer Exercise 7.4.3 for the Volterra integral equation u(t) +
 t
a K(t, s) u(s) ds = f(t),
where a ≤t ≤b.
7.4.5.(a) Prove that the solution to the linear integral equation u(t) = a +
 t
0 k(s) u(s) ds
solves the linear initial value problem du
dt = k(t) u(t), u(0) = a.
(b) Use part (a) to solve the following integral equations
(i) u(t) = 2 −
 t
0 u(s) ds,
(ii) u(t) = 1 + 2
 t
1 s u(s) ds,
(iii) u(t) = 3 +
 t
0 es u(s) ds.
The Superposition Principle
Before attempting to tackle general inhomogeneous linear systems, we should look ﬁrst at
the homogeneous version. The most important fact is that homogeneous linear systems
admit a superposition principle that allows one to construct new solutions from known
solutions. Recall that the word “superposition” refers to taking linear combinations of
solutions.
Consider a general homogeneous linear system
L[z] = 0,
(7.48)
where L: U →V is a linear function. If we are given two solutions, say z1 and z2, meaning
that
L[z1 ] = 0,
L[z2 ] = 0,
then their sum z1 + z2 is automatically a solution, since, in view of the linearity of L,
L[z1 + z2 ] = L[z1 ] + L[z2 ] = 0 + 0 = 0.
Similarly, given a solution z and any scalar c, the scalar multiple c z is automatically a
solution, since
L[c z] = c L[z] = c 0 = 0.
Combining these two elementary observations, we can now state the general superposition
principle. The proof is an immediate consequence of formula (7.4).
Theorem 7.30. If z1, . . . , zk are all solutions to the same homogeneous linear system
L[z] = 0, then every linear combination c1 z1 + · · · + ck zk is also a solution.
As with matrices, we call the solution space to the homogeneous linear system (7.48)
the kernel of the linear function L. The superposition principle implies that the kernel
always forms a subspace.
Proposition 7.31. If L: U →V is a linear function, then its kernel
ker L = { z ∈U | L[z] = 0 } ⊂U
(7.49)
is a subspace of the domain space U.
As we know, in the case of linear matrix systems, the kernel can be explicitly determined
by applying the usual Gaussian Elimination algorithm. To solve more general homogeneous
linear systems, e.g., linear diﬀerential equations, one must develop appropriate analytical
solution techniques.

7.4 Linear Systems
379
Example 7.32.
Consider the second order linear diﬀerential operator
L = D2 −2D −3,
(7.50)
which maps the function u(x) to the function
L[u] = (D2 −2D −3)[u] = u′′ −2u′ −3u.
The associated homogeneous system takes the form of a homogeneous, linear, constant
coeﬃcient second order ordinary diﬀerential equation
L[u] = u′′ −2u′ −3u = 0.
(7.51)
In accordance with the standard solution method, we plug the exponential ansatz†
u = eλx
into the equation. The result is
L[eλx ] = D2[eλx ] −2D[eλx ] −3eλx = λ2 eλx −2λeλx −3eλx = (λ2 −2λ −3)eλx.
Therefore, u = eλx is a solution if and only if λ satisﬁes the characteristic equation
0 = λ2 −2λ −3 = (λ −3)(λ + 1).
The two roots are λ1 = 3, λ2 = −1, and hence
u1(x) = e3x,
u2(x) = e−x,
(7.52)
are two linearly independent solutions of (7.51). According to the general superposition
principle, every linear combination
u(x) = c1 u1(x) + c2 u2(x) = c1 e3x + c2 e−x
(7.53)
of these two basic solutions is also a solution, for any choice of constants c1, c2. In fact,
this two-parameter family (7.53) constitutes the most general solution to the ordinary
diﬀerential equation (7.51); indeed, this is a consequence of Theorem 7.34 below. Thus,
the kernel of the second order diﬀerential operator (7.50) is two-dimensional, with basis
given by the independent exponential solutions (7.52).
In general, the solution space to an nth order homogeneous linear ordinary diﬀerential
equation
L[u] = an(x) dnu
dxn + an−1(x) dn−1u
dxn−1 + · · · + a1(x) du
dx + a0(x)u = 0
(7.54)
is a subspace of the vector space Cn(a, b) of n times continuously diﬀerentiable functions
deﬁned on an open interval‡ a < x < b, since it is just the kernel of a linear diﬀerential
†
The German word Ansatz refers to the method of ﬁnding a solution to a complicated equation
by guessing the solution’s form in advance. Typically, one is not clever enough to guess the precise
solution, and so the ansatz will have one or more free parameters — in this case the constant
exponent λ — that, with some luck, can be rigged up to fulﬁll the requirements imposed by the
equation. Thus, a reasonable English translation of “ansatz” is “inspired guess”.
‡
We allow a and/or b to be inﬁnite.

380
7 Linearity
operator L: Cn(a, b) →C0(a, b). This implies that linear combinations of solutions are
also solutions. To determine the number of solutions, or, more precisely, the dimension of
the solution space, we need to impose some mild restrictions on the diﬀerential operator.
Deﬁnition 7.33. A diﬀerential operator L given by (7.54) is called nonsingular on an open
interval (a, b) if all its coeﬃcients are continuous functions, so an(x), . . . , a0(x) ∈C0(a, b),
and its leading coeﬃcient does not vanish: an(x) ̸= 0 for all a < x < b.
The basic existence and uniqueness theorems governing nonsingular homogeneous linear
ordinary diﬀerential equations can be reformulated as a characterization of the dimension
of the solution space.
Theorem 7.34. The kernel of a nonsingular nth order ordinary diﬀerential operator is an
n-dimensional subspace ker L ⊂Cn(a, b).
A proof of this theorem relies on the fundamental existence and uniqueness theorems
for ordinary diﬀerential equations, and can be found in [7, 36]. The fact that the kernel
has dimension n means that it has a basis consisting of n linearly independent solutions
u1(x), . . . , un(x) ∈Cn(a, b) with the property that every solution to the homogeneous
diﬀerential equation (7.54) is given by a linear combination
u(x) = c1 u1(x) + · · · + cn un(x),
where c1, . . . , cn are arbitrary constants. Therefore, once we ﬁnd n linearly independent
solutions of an nth order homogeneous linear ordinary diﬀerential equation, we can imme-
diately write down its most general solution.
The condition that the leading coeﬃcient an(x) ̸= 0 is essential. Points where an(x) = 0
are known as singular points. Singular points show up in many applications, and must be
treated separately and with care, [7, 22, 61]. Of course, if the coeﬃcients are constant,
then there is nothing to worry about — either the leading coeﬃcient is nonzero, an ̸= 0, or
the diﬀerential equation is, in fact, of lower order than advertised. Here is the prototypical
example of an ordinary diﬀerential equation with a singular point.
Example 7.35.
A second order Euler diﬀerential equation takes the form
E[u] = ax2 u′′ + bxu′ + cu = 0,
(7.55)
where a ̸= 0 and b, c are constants. Here E = ax2 D2 + bxD + c is a second order variable
coeﬃcient linear diﬀerential operator. Instead of the exponential solution ansatz used in
the constant coeﬃcient case, Euler equations are solved by using a power ansatz
u(x) = xr
(7.56)
with unknown exponent r. Substituting into the diﬀerential equation, we ﬁnd
E[xr ] = ax2 D2[xr ] + bxD[xr ] + cxr
= ar(r −1)xr + brxr + cxr = [ar(r −1) + br + c] xr.
Thus, xr is a solution if and only if r satisﬁes the characteristic equation
ar(r −1) + br + c = ar2 + (b −a)r + c = 0.
(7.57)
If the quadratic characteristic equation has two distinct real roots, r1 ̸= r2, then we obtain
two linearly independent solutions u1(x) = xr1 and u2(x) = xr2, and so the general (real)

7.4 Linear Systems
381
solution to (7.55) has the form
u(x) = c1 | x |r1 + c2 | x |r2.
(7.58)
(The absolute values are usually needed to ensure that the solutions remain real when
x < 0.) The other cases — repeated roots and complex roots — will be discussed below.
The Euler equation has a singular point at x = 0, where its leading coeﬃcient vanishes.
Theorem 7.34 assures us that the diﬀerential equation has a two-dimensional solution
space on every interval not containing the singular point. However, predicting the number
of solutions that remain continuously diﬀerentiable at x = 0 is not so easy, since it depends
on the values of the exponents r1 and r2. For instance, the case
x2 u′′ −3xu′ + 3u = 0
has general solution
u = c1 x + c2 x3,
which forms a two-dimensional subspace of C0(R). However,
x2 u′′ + xu′ −u = 0
has general solution
u = c1 x + c2
x ,
and only the multiples of the ﬁrst solution x are continuous at x = 0. Therefore, the
solutions that are continuous everywhere form only a one-dimensional subspace of C0(R).
Finally,
x2 u′′ + 5xu′ + 3u = 0
has general solution
u = c1
x + c2
x3 .
In this case, there are no nontrivial solutions u(x) ̸≡0 that are continuous at x = 0, and
so the space of solutions deﬁned on all of R is zero-dimensional.
The superposition principle is equally valid in the study of homogeneous linear partial
diﬀerential equations. Here is a particularly noteworthy example.
Example 7.36.
Consider the Laplace equation
Δ[u] = ∂2u
∂x2 + ∂2u
∂y2 = 0
(7.59)
for a function u(x, y) deﬁned on a domain Ω ⊂R2.
The Laplace equation is named
after the renowned eighteenth-century French mathematician Pierre-Simon Laplace, and
is the most important partial diﬀerential equation. Its applications range over almost all
ﬁelds of mathematics, physics, and engineering, including complex analysis, diﬀerential
geometry, ﬂuid mechanics, electromagnetism, elasticity, thermodynamics, and quantum
mechanics, [61]. The Laplace equation is a homogeneous linear partial diﬀerential equation
corresponding to the partial diﬀerential operator Δ = ∂2
x + ∂2
y known as the Laplacian.
Linearity can either be proved directly, or by noting that Δ is built up from the basic linear
partial derivative operators ∂x, ∂y by the processes of composition and addition, as detailed
in Exercise 7.1.46. Solutions to the Laplace equation are known as harmonic functions.
Unlike homogeneous linear ordinary diﬀerential equations, there is an inﬁnite number
of linearly independent solutions to the Laplace equation. Examples include the trigono-
metric/exponential solutions
eω x cos ωy,
eω x sin ωy,
eω y cos ωx,
eω y sin ωy,
where ω is any real constant. There are also inﬁnitely many independent harmonic poly-
nomial solutions, the ﬁrst few of which are
1,
x,
y,
x2 −y2,
xy,
x3 −3xy2,
. . . .

382
7 Linearity
The reader might enjoy ﬁnding some more polynomial solutions and trying to spot the
pattern.
(The answer will appear shortly.)
As usual, we can build up more compli-
cated solutions by taking general linear combinations of these particular ones; for instance,
u(x, y) = 1 −4xy + 2e3x cos 3y is automatically a solution. See [61] for further develop-
ments.
Exercises
7.4.6. Solve the following homogeneous linear ordinary diﬀerential equations. What is the
dimension of the solution space? (a) u′′ −4u = 0, (b) u′′ −6u′ + 8u = 0,
(c) u′′′ −9u′ = 0, (d) u′′′′ + 4u′′′ −u′′ −16u′ −12u = 0.
7.4.7. Deﬁne L[y ] = y′′ + y. (a) Prove directly from the deﬁnition that L: C2[a, b] →C0[a, b]
is a linear transformation. (b) Determine ker L.
7.4.8. Answer Exercise 7.4.7 when L = 3D2 −2D −5.
7.4.9. Consider the linear diﬀerential equation y′′′ + 5y′′ + 3y′ −9y = 0. (a) Write the equation
in the form L[y ] = 0 for a diﬀerential operator L = p(D). (b) Find a basis for ker L, and
then write out the general solution to the diﬀerential equation.
7.4.10. The following functions are solutions to a constant coeﬃcient homogeneous scalar
ordinary diﬀerential equation. (i) Determine the least possible order of the diﬀerential
equation, and (ii) write down an appropriate diﬀerential equation.
(a) e2x + e−3x,
(b) 1 + e−x,
(c) xex,
(d) ex + 2e2x + 3e3x.
7.4.11. Solve the following Euler diﬀerential equations:
(a) x2 u′′ + 5xu′ −5u = 0,
(b) 2x2 u′′ −xu′ −2u = 0,
(c) x2 u′′ −u = 0,
(d) x2 u′′ + xu′ −3u = 0,
(e) 3x2 u′′ −5xu′ −3u = 0,
(f ) d2u
dx2 + 2
x
du
dx = 0.
7.4.12. Solve the third order Euler diﬀerential equation x3 u′′′ + 2x2 u′′ −3xu′ + 3u = 0 by
using the power ansatz (7.56). What is the dimension of the solution space for x > 0?
For all x?
7.4.13. (i) Show that if u(x) solves the Euler equation ax2 d2u
dx2 + bx du
dx + cu = 0, then
v(t) = u(et) solves a linear, constant coeﬃcient diﬀerential equation.
(ii) Use this
alternative technique to solve the Euler diﬀerential equations in Exercise 7.4.11.
♦7.4.14.(a) Use the method in Exercise 7.4.13 to solve an Euler equation whose characteristic
equation has a double root r1 = r2 = r.
(b) Solve the speciﬁc equations
(i) x2 u′′ −xu′ + u = 0,
(ii) d2u
dx2 + 1
x
du
dx = 0.
7.4.15. Show that if u(x) solves xu′′ + 2u′ −4xu = 0, then v(x) = x u(x) solves a
linear, constant coeﬃcient equation. Use this to ﬁnd the general solution to the given
diﬀerential equation. Which of your solutions are continuous at the singular point x = 0?
Diﬀerentiable?
7.4.16. Let S ⊂R be an open subset (i.e., a union of open intervals), and let D: C1(S) →
C0(S) be the derivative operator D[f] = f′. True or false: ker D is a one-dimensional
subspace of C1(S).
7.4.17. Show that log(x2 + y2) and
x
x2 + y2 are harmonic functions, that is, solutions of the
two-dimensional Laplace equation.

7.4 Linear Systems
383
7.4.18. Find all solutions u = f(r) of the two-dimensional Laplace equation that depend only
on the radial coordinate r =

x2 + y2. Do these solutions form a vector space? If so, what
is its dimension?
7.4.19. Find all (real) solutions to the two-dimensional Laplace equation of the form
u = log p(x, y), where p(x, y) is a quadratic polynomial. Do these solutions form a vector
space? If so, what is its dimension?
♥7.4.20.(a) Show that the function ex cos y is a solution to the two-dimensional Laplace
equation.
(b) Show that its quadratic Taylor polynomial at x = y = 0 is harmonic.
(c) What about its degree 3 Taylor polynomial? (d) Can you state a general theorem?
(e) Test your result by looking at the Taylor polynomials of the harmonic function
log

(x −1)2 + y2 
.
7.4.21.(a) Find a basis for, and the dimension of, the vector space consisting of all quadratic
polynomial solutions of the three-dimensional Laplace equation ∂2u
∂x2 + ∂2u
∂y2 + ∂2u
∂z2 = 0.
(b) Do the same for the homogeneous cubic polynomial solutions.
7.4.22. Find all solutions u = f(r) of the three-dimensional Laplace equation
∂2u
∂x2 + ∂2u
∂y2 + ∂2u
∂z2 = 0 that depend only on the radial coordinate r =

x2 + y2 + z2.
Do these solutions form a vector space? If so, what is its dimension?
7.4.23. Let L, M be linear functions. (a) Prove that ker(L ◦M) ⊇ker M.
(b) Find an
example in which ker(L ◦M) ̸= ker M.
Inhomogeneous Systems
Now we turn our attention to inhomogeneous linear systems
L[u] = f,
(7.60)
where L: U →V is a linear function, f ∈V , and the desired solution u ∈U. Unless f = 0,
the set of solutions to (7.60) is not a subspace of U, but, rather, forms an aﬃne subspace,
as deﬁned in Exercise 2.2.28. Here, the crucial question is existence — is there a solution to
the system? In contrast, for the homogeneous system L[z] = 0, existence is not an issue,
since 0 is always a solution. The key question for homogeneous systems is uniqueness:
either ker L = {0}, in which case 0 is the only solution, or ker L ̸= {0}, in which case there
are inﬁnitely many nontrivial solutions 0 ̸= z ∈ker L.
In the matrix case, the compatibility of an inhomogeneous system A x = b — which
was required for the existence of a solution — led to the general deﬁnition of the image of
a matrix, which we copy verbatim for linear functions.
Deﬁnition 7.37. The image of a linear function L: U →V is the subspace
img L = { L[u] | u ∈U } ⊂V.
The proof that img L is a subspace of the codomain is straightforward: If f = L[u]
and g = L[v] are any two elements of the image, so is any linear combination, since, by
linearity
c f + d g = c L[u] + d L[v] = L[c u + d v] ∈img L.
For example, if L[u] = Au is given by multiplication by an m × n matrix, then its image
is the subspace img L = img A ⊂Rm spanned by the columns of A — the column space

384
7 Linearity
of the coeﬃcient matrix. When L is a linear diﬀerential operator, or more general linear
operator, characterizing its image can be a much more challenging problem.
The fundamental theorem regarding solutions to inhomogeneous linear equations exactly
mimics our earlier result, Theorem 2.39, for matrix systems.
Theorem 7.38. Let L: U →V be a linear function. Let f ∈V . Then the inhomogeneous
linear system
L[u] = f
(7.61)
has a solution if and only if f ∈img L. In this case, the general solution to the system has
the form
u = u⋆+ z,
(7.62)
where u⋆is a particular solution, so L[u⋆] = f, and z is any element of ker L, i.e., a solution
to the corresponding homogeneous system
L[z] = 0.
(7.63)
Proof : We merely repeat the proof of Theorem 2.39. The existence condition f ∈img L
is an immediate consequence of the deﬁnition of the image. Suppose u⋆is a particular
solution to (7.61). If z is a solution to (7.63), then, by linearity,
L[u⋆+ z] = L[u⋆] + L[z] = f + 0 = f,
and hence u⋆+ z is also a solution to (7.61). To show that every solution has this form,
let u be a second solution, so that L[u] = f. Setting z = u −u⋆, we ﬁnd that
L[z] = L[u −u⋆] = L[u] −L[u⋆] = f −f = 0.
Therefore z ∈ker L, and so u has the proper form (7.62).
Q.E.D.
Corollary 7.39. The inhomogeneous linear system (7.61) has a unique solution if and
only if f ∈img L and ker L = {0}.
Therefore, to prove that a linear system has a unique solution, we ﬁrst need to prove an
existence result that there is at least one solution, which requires the right-hand side f to lie
in the image of the operator L, and then a uniqueness result, that the only solution to the
homogeneous system L[z] = 0 is the trivial zero solution z = 0. Observe that whenever an
inhomogeneous system L[u] = f has a unique solution, then every other inhomogeneous
system L[u] = g that is deﬁned by the same linear function also has a unique solution,
provided g ∈img L. In other words, uniqueness does not depend upon the external forcing
— although existence might.
Remark. In physical systems, the inhomogeneity f typically corresponds to an external
force. The decomposition formula (7.62) states that its eﬀect on the linear system can
be viewed as a combination of one speciﬁc response u⋆to the forcing and the system’s
internal, unencumbered motion, as represented by the homogeneous solution z. Keep in
mind that the particular solution is not uniquely deﬁned (unless ker L = {0}), and any
one solution can serve in this role.
Example 7.40.
Consider the inhomogeneous linear second order diﬀerential equation
u′′ + u′ −2u = x.
(7.64)

7.4 Linear Systems
385
Note that this can be written in the linear system form
L[u] = x,
where
L = D2 + D −2
is a linear second order diﬀerential operator. The kernel of the diﬀerential operator L is
found by solving the associated homogeneous linear equation
L[z ] = z′′ + z′ −2z = 0.
(7.65)
Applying the usual solution method, we ﬁnd that the homogeneous diﬀerential equation
(7.65) has a two-dimensional solution space, with basis functions
z1(x) = e−2x,
z2(x) = ex.
Therefore, the general element of ker L is a linear combination
z(x) = c1 z1(x) + c2 z2(x) = c1 e−2x + c2 ex.
To ﬁnd a particular solution to the inhomogeneous diﬀerential equation (7.64), we rely
on the method of undetermined coeﬃcients†. We introduce the solution ansatz u = ax+b,
and compute
L[u] = L[ax + b] = a −2(ax + b) = −2ax + (a −2b) = x.
Equating the coeﬃcients of x and 1, and then solving for a = −1
2, b = −1
4, we deduce
that
u⋆(x) = −1
2 x −1
4
is a particular solution to the inhomogeneous diﬀerential equation. Theorem 7.38 then says
that the general solution is
u(x) = u⋆(x) + z(x) = −1
2 x −1
4 + c1 e−2x + c2 ex.
Example 7.41.
By inspection, we see that
u(x, y) = −1
2 sin(x + y)
is a solution to the particular Poisson equation
∂2u
∂x2 + ∂2u
∂y2 = sin(x + y).
(7.66)
Theorem 7.38 implies that every solution to this inhomogeneous version of the Laplace
equation (7.59) takes the form
u(x, y) = −1
2 sin(x + y) + z(x, y),
where z(x, y) is an arbitrary harmonic function, i.e., a solution to the homogeneous Laplace
equation.
†
One could also employ the method of variation of parameters, although usually the undeter-
mined coeﬃcient method, when applicable, is the more straightforward of the two. Details can be
found in most ordinary diﬀerential equations texts, including [7, 22].

386
7 Linearity
Example 7.42.
Let us solve the second order linear boundary value problem
u′′ + u = x,
u(0) = 0,
u(π) = 0.
(7.67)
As with initial value problems, the ﬁrst step is to solve the diﬀerential equation. To this
end, we ﬁrst solve the corresponding homogeneous diﬀerential equation z′′ + z = 0. The
usual method — see [7] or Example 7.50 below — shows that cos x and sin x form a basis for
its solution space. The method of undetermined coeﬃcients then produces the particular
solution u⋆(x) = x to the inhomogeneous diﬀerential equation, and so its general solution
is
u(x) = x + c1 cos x + c2 sin x.
(7.68)
The next step is to see whether any solutions also satisfy the boundary conditions. Plugging
formula (7.68) into the boundary conditions yields
u(0) = c1 = 0,
u(π) = π −c1 = 0.
However, these two conditions are incompatible, and so there is no solution to the linear
system (7.67). The function f(x) = x does not lie in the image of the diﬀerential operator
L[u] = u′′ +u when u is subjected to the boundary conditions. Or, to state it another way,
( x, 0, 0 )T does not belong to the image of the linear operator M[u] = ( u′′ + u, u(0), u(π) )T
deﬁning the boundary value problem.
On the other hand, if we slightly modify the inhomogeneity, the boundary value problem
u′′ + u = x −1
2 π,
u(0) = 0,
u(π) = 0,
(7.69)
does admit a solution, but it fails to be unique. Applying the preceding solution techniques,
we ﬁnd that
u(x) = x −1
2 π + 1
2 π cos x + c sin x
solves the system for any choice of constant c, and so the boundary value problem (7.69)
admits inﬁnitely many solutions. Observe that z(x) = sin x is a basis for the kernel or
solution space of the corresponding homogeneous boundary value problem
z′′ + z = 0,
z(0) = 0,
z(π) = 0,
while u⋆(x) = x −1
2 π + 1
2 π cos x represents a particular solution to the inhomogeneous
system. Thus, u(x) = u⋆(x) + z(x), in conformity with the general formula (7.62).
Incidentally, if we modify the interval of deﬁnition, considering
u′′ + u = f(x),
u(0) = 0,
u
 1
2 π

= 0,
(7.70)
then the homogeneous boundary value problem, with f(x) ≡0, has only the trivial solution,
and so the inhomogeneous system admits a unique solution for any inhomogeneity f(x).
For example, if f(x) = x, then
u(x) = x −1
2 π sin x
(7.71)
is the unique solution to the resulting boundary value problem.
This example highlights some crucial diﬀerences between boundary value problems and
initial value problems for ordinary diﬀerential equations. Nonsingular initial value problems
have a unique solution for every suitable set of initial conditions. Boundary value problems
have more of the ﬂavor of linear algebraic systems, either possessing a unique solution for

7.4 Linear Systems
387
all possible inhomogeneities, or admitting either no solution or inﬁnitely many solutions,
depending on the right-hand side.
An interesting question is how to characterize the
inhomogeneities f(x) that admit a solution, i.e., that lie in the image of the associated
linear operator. These issues are explored in depth in [61].
Exercises
7.4.24. For each of the following inhomogeneous systems, determine whether the right-hand
side lies in the image of the coeﬃcient matrix, and, if so, write out the general solution,
clearly identifying the particular solution and the kernel element.
(a)

1
−1
3
−3
	
x =

1
2
	
,
(b)

2
1
4
−1
2
1
	
x =

1
2
	
,
(c)
⎛
⎜
⎝
1
2
−1
2
0
1
1
−2
2
⎞
⎟
⎠x =
⎛
⎜
⎝
0
3
3
⎞
⎟
⎠,
(d)
⎛
⎜
⎝
−2
1
−2
3
3
−5
⎞
⎟
⎠x =
⎛
⎜
⎝
1
0
−1
⎞
⎟
⎠,
(e)
⎛
⎜
⎝
−1
3
0
2
2
−6
1
−1
−3
9
−2
0
⎞
⎟
⎠x =
⎛
⎜
⎝
2
−2
2
⎞
⎟
⎠.
7.4.25. Which of the following systems have a unique solution?
(a)
⎛
⎜
⎝
3
1
−1
−1
2
0
⎞
⎟
⎠

x
y
	
=
⎛
⎜
⎝
0
2
2
⎞
⎟
⎠,
(b)

1
2
−1
−2
3
0
	 ⎛
⎜
⎝
x
y
z
⎞
⎟
⎠=

1
2
	
,
(c)
⎛
⎜
⎝
2
1
−1
0
−3
−3
2
0
−2
⎞
⎟
⎠
⎛
⎜
⎝
u
v
w
⎞
⎟
⎠=
⎛
⎜
⎝
3
−1
5
⎞
⎟
⎠,
(d)
⎛
⎜
⎝
1
4
−1
1
3
−3
2
3
−2
⎞
⎟
⎠
⎛
⎜
⎝
u
v
w
⎞
⎟
⎠=
⎛
⎜
⎝
−2
−1
1
⎞
⎟
⎠.
7.4.26. Solve the following inhomogeneous linear ordinary diﬀerential equations:
(a) u′ −4u = x −3,
(b) 5u′′ −4u′ + 4u = ex cos x,
(c) u′′ −3u′ = e3x.
7.4.27. Solve the following initial value problems:
(a) u′+3u = ex, u(1) = 0, (b) u′′+4u = 1,
u(π) = u′(π) = 0, (c) u′′ −u′ −2u = ex +e−x, u(0) = u′(0) = 0, (d) u′′ +2u′ +5u = sin x,
u(0) = 1, u′(0) = 0, (e) u′′′ −u′′ + u′ −u = x, u(0) = 0, u′(0) = 1, u′′(0) = 0.
7.4.28. Solve the following inhomogeneous Euler equations using either variation of parameters
or the change of variables method discussed in Exercise 7.4.13:
(a) x2 u′′ + xu′ −u = x,
(b) x2 u′′ −2xu′ + 2u = log x,
(c) x2 u′′ −3xu′ −5u = 3x −5.
7.4.29. Write down all solutions to the following boundary value problems. Label your answer
as
(i) unique solution, (ii) no solution, (iii) inﬁnitely many solutions.
(a) u′′ + 2u = 2x, u(0) = 0, u(π) = 0,
(b) u′′ + 4u = cos x, u(−π) = 0, u(π) = 1,
(c) u′′ −2u′ + u = x −2, u(0) = −1, u(1) = 1,
(d) u′′ + 2u′ + 2u = 1, u(0) = 1
2, u(π) = 1
2,
(e) u′′ −3u′ + 2u = 4x, u(0) = 0, u(1) = 0,
(f ) x2 u′′ + xu′ −u = 0, u(0) = 1, u(1) = 0,
(g) x2 u′′ −6u = 0, u(1) = 1, u(2) = −1,
(h) x2 u′′ −2xu′ + 2u = 0, u(0) = 0, u(1) = 1.
♦7.4.30. Let L: U →V be a linear function, and let W ⊂U be a subspace of the domain
space. (a) Prove that Y = { L[w] | w ∈W } ⊂img L ⊂V is a subspace of the image.
(b) Prove that dim Y ≤dim W. Conclude that a linear transformation can never increase
the dimension of a subspace.
♦7.4.31.(a) Show that if L: V →V is linear and ker L ̸= {0}, then L is not invertible.
(b) Show that if img L ̸= V , then L is not invertible.
(c) Give an example of a linear map with ker L = {0} that is not invertible. Hint: First
explain why your example must be on an inﬁnite-dimensional vector space.

388
7 Linearity
Superposition Principles for Inhomogeneous Systems
The superposition principle for inhomogeneous linear systems allows us to combine diﬀerent
inhomogeneities — provided that we do not change the underlying linear operator. The
result is a straightforward generalization of the matrix version described in Theorem 2.44.
Theorem 7.43. Let L: U →V be a linear function. Suppose that, for each i = 1, . . . , k,
we know a particular solution u⋆
i to the inhomogeneous linear system L[u] = fi for some
fi ∈img L. Then, given scalars c1, . . . , ck, a particular solution to the combined inhomo-
geneous system
L[u] = c1 f1 + · · · + ck fk
(7.72)
is the corresponding linear combination
u⋆= c1 u⋆
1 + · · · + ck u⋆
k
(7.73)
of particular solutions. The general solution to the inhomogeneous system (7.72) is
u = u⋆+ z = c1 u⋆
1 + · · · + ck u⋆
k + z,
(7.74)
where z ∈ker L is an arbitrary solution to the associated homogeneous system L[z] = 0.
The proof is an easy consequence of linearity, and left to the reader. In physical terms,
the superposition principle can be interpreted as follows. If we know the response of a
linear physical system to several diﬀerent external forces, represented by f1, . . . , fk, then
the response of the system to a linear combination of these forces is just the self-same linear
combination of the individual responses. The homogeneous solution z represents an inter-
nal motion that the system acquires independent of any external forcing. Superposition
relies on the linearity of the system, and so is always applicable in quantum mechanics,
which is an inherently linear theory. On the other hand, in classical and relativistic me-
chanics, superposition is valid only in the linear approximation regime governing small
motions/displacements/etc. Large-scale motions of a fully nonlinear physical system are
more subtle, and combinations of external forces may lead to unexpected results.
Example 7.44.
In Example 7.42, we found that a particular solution to the linear dif-
ferential equation
u′′ + u = x
is
u⋆
1 = x.
The method of undetermined coeﬃcients can be used to solve the inhomogeneous equation
u′′ + u = cos x.
Since cos x and sin x are already solutions to the homogeneous equation, we must use the
solution ansatz
u = ax cosx + bx sin x,
which, when substituted into the diﬀerential equation, produces the particular solution
u⋆
2 = −1
2 x sin x.
Therefore, by the superposition principle, the combined inhomogeneous system
u′′ + u = 3x −2 cos x
has a particular solution
u⋆= 3u⋆
1 −2u⋆
2 = 3x + x sin x.

7.4 Linear Systems
389
The general solution is obtained by appending an arbitrary solution to the homogeneous
equation:
u = 3x + x sin x + c1 cos x + c2 sin x.
Example 7.45.
Consider the boundary value problem
u′′ + u = x,
u(0) = 2,
u
 1
2 π

= −1,
(7.75)
which is a modiﬁcation of (7.70) with inhomogeneous boundary conditions. The superposi-
tion principle applies here, and allows us to decouple the inhomogeneity due to the forcing
from the inhomogeneity due to the boundary conditions. We decompose the right-hand
side, written in vectorial form, into simpler constituents†
⎛
⎝
x
2
−1
⎞
⎠=
⎛
⎝
x
0
0
⎞
⎠+ 2
⎛
⎝
0
1
0
⎞
⎠−
⎛
⎝
0
0
1
⎞
⎠.
The ﬁrst vector on the right-hand side corresponds to the preceding boundary value prob-
lem (7.70), whose solution was found in (7.71). The second and third vectors correspond
to the unforced boundary value problems
u′′ + u = 0,
u(0) = 1,
u
 1
2 π

= 0,
and
u′′ + u = 0,
u(0) = 0,
u
 1
2 π

= 1,
with respective solutions u(x) = cos x and u(x) = sin x. Therefore, the solution to the
combined boundary value problem (7.75) is the same linear combination of these individual
solutions:
u(x) =

x −1
2 π sin x

+ 2 cosx −sin x = x + 2 cos x −

1 + 1
2 π

sin x.
The solution is unique because the corresponding homogeneous boundary value problem
z′′ + z = 0,
z(0) = 0,
z
 1
2 π

= 0,
has only the trivial solution z(x) ≡0, as you can verify.
Exercises
7.4.32. Use superposition to solve the following inhomogeneous ordinary diﬀerential equations:
(a) u′ + 2u = 1 + cos x, (b) u′′ −9u = x + sin x, (c) 9u′′ −18u′ + 10u = 1 + ex cos x,
(d) u′′ + u′ −2u = sinh x, where sinh x = 1
2(ex −e−x), (e) u′′′ + 9u′ = 1 + e3x.
7.4.33. Consider the diﬀerential equation u′′ + xu = 2. Suppose you know solutions to the two
boundary value problems u(0) = 1, u(1) = 0 and u(0) = 0, u(1) = 1. List all possible
boundary value problems you can solve using superposition.
†
Warning.
When writing out a linear combination, make sure the scalars are constants!
Writing the ﬁrst summand as x ( 1, 0, 0 )T will lead to an incorrect application of the superposition
principle.

390
7 Linearity
7.4.34. Consider the diﬀerential equation xu′′ −(x + 1)u′ + u = 0. Suppose we know the
solution to the initial value problem u(1) = 2, u′(1) = 1 is u(x) = x + 1, while the solution
to the initial value problem u(1) = 1, u′(1) = 1 is u(x) = ex−1. (a) What is the solution
to the initial value problem u(1) = 3, u′(1) = −2? (b) What is the general solution to the
diﬀerential equation?
7.4.35. Consider the diﬀerential equation 4xu′′ + 2u′ + u = 0. Given that cos √x solves the
boundary value problem u
 1
4 π2 
= 0, u(π2) = −1, and sin √x solves the boundary value
problem u
 1
4 π2 
= 1, u(π2) = 0, write down the solution to the boundary value problem
u
 1
4 π2 
= −3, u(π2) = 7.
7.4.36. Solve the following boundary value problems by using superposition: (a) u′′ + 9u = x,
u(0) = 1, u′(π) = 0, (b) u′′ −8u′ +16u = e4x, u(0) = 1, u(1) = 0,
(c) u′′ +4u = sin 3x,
u′(0) = 0, u(2π) = 3, (d) u′′ −2u′ + u = 1 + ex, u′(0) = −1, u′(1) = 1.
7.4.37. Given that x2 + y2 solves the Poisson equation ∂2u
∂x2 + ∂2u
∂y2 = 4, while x4 + y4 solves
∂2u
∂x2 + ∂2u
∂y2 = 12(x2 + y2), write down a solution to ∂2u
∂x2 + ∂2u
∂y2 = 1 + x2 + y2.
♥7.4.38. Reduction of order: Suppose you know one solution u1(x) to the second order
homogeneous diﬀerential equation u′′ + a(x)u′ + b(x)u = 0. (a) Show that if u(x) =
v(x)u1(x) is any other solution, then w(x) = v′(x) satisﬁes a ﬁrst order diﬀerential
equation. (b) Use reduction of order to ﬁnd the general solution to the following equations,
based on the indicated solution:
(i) u′′ −2u′ + u = 0, u1(x) = ex, (ii) xu′′ + (x −1)u′ −u = 0, u1(x) = x −1,
(iii) u′′ + 4xu′ + (4x2 + 2)u = 0, u1(x) = e−x2
, (iv) u′′ −(x2 + 1)u = 0, u1(x) = ex2/2.
♦7.4.39. Write out the details of the proof of Theorem 7.43.
Complex Solutions to Real Systems
As we know, solutions to a linear, homogeneous, constant coeﬃcient ordinary diﬀerential
equation are found by substituting an exponential ansatz, which eﬀectively reduces the
diﬀerential equation to the polynomial characteristic equation. Complex roots of the char-
acteristic equation yield complex exponential solutions. But, if the equation is real, then
the real and imaginary parts of the complex solutions are automatically real solutions. This
solution technique is a particular case of a general principle for producing real solutions
to real linear systems from, typically, simpler complex solutions. To work, the method
requires us to impose some additional structure on the complex vector spaces involved.
Deﬁnition 7.46. A complex vector space V is called conjugated if it admits an operation
of complex conjugation taking u ∈V to u ∈V with the following properties:
(a) conjugating twice returns one to the original vector: u = u;
(b) compatibility with vector addition: u + v = u + v for all u, v ∈V ;
(c) compatibility with scalar multiplication, λ u = λ u, for all λ ∈C and u ∈V .
The simplest example of a conjugated vector space is Cn.
The complex conjugate
of a vector u = ( u1, u2, . . . , un )T
is obtained by conjugating all its entries, whereby
u = ( u1, u2, . . . , un )T . Thus,

7.4 Linear Systems
391
u = v + i w,
u = v −i w,
where
v = Re u = u + u
2
,
w = Im u = u −u
2 i
,
(7.76)
are the real and imaginary parts of u ∈Cn. For example, if
u =
⎛
⎝
1 −2 i
3 i
5
⎞
⎠=
⎛
⎝
1
0
5
⎞
⎠+ i
⎛
⎝
−2
3
0
⎞
⎠,
then
u =
⎛
⎝
1 + 2 i
−3 i
5
⎞
⎠=
⎛
⎝
1
0
5
⎞
⎠−i
⎛
⎝
−2
3
0
⎞
⎠,
whence
Re u =
⎛
⎝
1
0
5
⎞
⎠,
Im u =
⎛
⎝
−2
3
0
⎞
⎠.
The other prototypical example of a conjugated vector space is the space of complex-
valued functions f(x) = r(x) + i s(x) deﬁned on the interval a ≤x ≤b. The complex
conjugate function is f(x) = f(x) = r(x) −i s(x).
Thus, the complex conjugate of
e(1+3 i )x = ex cos 3x + i ex sin 3x is e(1+3 i )x = e(1−3 i )x = ex cos 3x −i ex sin 3x, with
Re e(1+3 i )x = ex cos 3x, Im e(1+3 i )x = i ex sin 3x.
An element v ∈V of a conjugated vector space is called real if v = v. One easily checks
that the real and imaginary parts of a general element, as deﬁned by (7.76), are both real
elements.
Warning. Not all subspaces of a conjugated vector space are conjugated. For example,
the one-dimensional subspace of C2 spanned by v1 = ( 1, 2 )T is conjugated. Indeed, the
complex conjugate of a general element c v1 = ( c, 2c )T is ( c, 2 c )T = c v1 which also
belongs to the subspace.
On the other hand, the subspace spanned by ( 1, i )T is not
conjugated, because the complex conjugate of the element ( c, i c )T is ( c, −i c )T , which
does not belong to the subspace unless c = 0. In Exercise 7.4.50 you are asked to prove
that a subspace V ⊂Cn is conjugated if and only if it has a basis v1, . . . , vk consisting
entirely of real vectors. While conjugated subspaces play a role in certain applications, in
practice we will deal only with Cn and the entire space of complex-valued functions, and
so can suppress most of these somewhat technical details.
Deﬁnition 7.47. A linear function L: U →V between conjugated vector spaces is called
real if it commutes with complex conjugation:
L[u] = L[u].
(7.77)
For example, any linear function L: Cn →Cm is given by multiplication by an m × n
matrix: L[u] = Au. The function is real if and only if A is a real matrix. Similarly, a
diﬀerential operator (7.15) is real if and only if its coeﬃcients are real-valued functions.
The solutions to a homogeneous system deﬁned by a real linear function satisfy the
following general Reality Principle.
Theorem 7.48. If L[u] = 0 is a real homogeneous linear system and u = v + i w is a
complex solution, then its complex conjugate u = v −i w is also a solution. Moreover,
both the real and imaginary parts, v and w, of a complex solution are real solutions.
Proof : First note that, by reality, L[u] = L[u] = 0 whenever L[u] = 0, and hence the
complex conjugate u of any solution is also a solution. Therefore, by linear superposition,
v = Re u = 1
2(u + u) and w = Im u = 1
2 i (u −u) are also solutions.
Q.E.D.

392
7 Linearity
Example 7.49.
The real linear matrix system

2
−1
3
0
−2
1
1
2

⎛
⎜
⎝
x
y
z
w
⎞
⎟
⎠=

0
0

has a complex solution
u =
⎛
⎜
⎝
−1 −3 i
1
1 + 2 i
−2 −4 i
⎞
⎟
⎠=
⎛
⎜
⎝
−1
1
1
−2
⎞
⎟
⎠+ i
⎛
⎜
⎝
−3
0
2
−4
⎞
⎟
⎠.
Since the coeﬃcient matrix is real, the real and imaginary parts,
v = ( −1, 1, 1, −2 )T ,
w = ( −3, 0, 2, −4 )T ,
are both solutions of the system, as can easily be checked.
On the other hand, the complex linear system

2
−2 i
i
0
1 + i
0
−2 −i
1

⎛
⎜
⎝
x
y
z
w
⎞
⎟
⎠=

0
0

has the complex solution
u =
⎛
⎜
⎝
1 −i
−i
2
2 + 2 i
⎞
⎟
⎠=
⎛
⎜
⎝
1
0
2
2
⎞
⎟
⎠+ i
⎛
⎜
⎝
−1
−1
0
2
⎞
⎟
⎠.
However, neither its real nor its imaginary part is a solution to the system.
Example 7.50.
Consider the real homogeneous ordinary diﬀerential equation
u′′ + 2u′ + 5u = 0.
To solve it, we use the usual exponential ansatz u = eλx, leading to the characteristic
equation
λ2 + 2λ + 5 = 0.
There are two roots,
λ1 = −1 + 2 i ,
λ2 = −1 −2 i ,
leading, via Euler’s formula (3.92), to the complex solutions
u1(x) = e(−1+2 i )x = e−x cos 2x + i e−x sin 2x,
u2(x) = e(−1−2 i )x = e−x cos 2x −i e−x sin 2x.
The complex conjugate of the ﬁrst solution is the second, in accordance with Theorem 7.48.
Moreover, the real and imaginary parts of the two solutions
v(x) = e−x cos 2x,
w(x) = e−x sin 2x,
are individual real solutions.
Since the solution space is two-dimensional, the general
solution is a linear combination
u(x) = c1 e−x cos 2x + c2 e−x sin 2x,
of the two linearly independent real solutions.

7.4 Linear Systems
393
Example 7.51.
Consider the real second order Euler diﬀerential equation
L[u] = x2 u′′ + 7xu′ + 13u = 0.
The roots of the associated characteristic equation
r(r −1) + 7r + 13 = r2 + 6r + 13 = 0
are complex: r = −3 ± 2 i , and the resulting solutions x−3+2 i , x−3−2 i are complex con-
jugate powers. We use Euler’s formula (3.92), to obtain their real and imaginary parts:
x−3+2 i = x−3 e2 i log x = x−3 cos(2 log x) + i x−3 sin(2 log x),
valid for x > 0.
(For x < 0 just replace x by −x in the above formula.)
Again, by
Theorem 7.48, the real and imaginary parts of the complex solution are by themselves real
solutions to the equation. Therefore, the general real solution to this diﬀerential equation
for x > 0 is
u(x) = c1 x−3 cos(2 log x) + c2 x−3 sin(2 log x).
Example 7.52.
The complex monomial
u(x, y) = (x + i y)n
is a solution to the Laplace equation
∂2u
∂x2 + ∂2u
∂y2 = 0
because, by the chain rule,
∂2u
∂x2 = n(n −1)(x + i y)n−2,
∂2u
∂y2 = n(n −1) i2(x + i y)n−2 = −n(n −1)(x + i y)n−2.
Since the Laplacian operator is real, Theorem 7.48 implies that the real and imaginary parts
of this complex solution are real solutions. The resulting real solutions are the harmonic
polynomials introduced in Example 7.36.
Knowing this, it is relatively easy to ﬁnd the explicit formulas for the harmonic poly-
nomials. We appeal to the binomial formula
(a + b)n =
n

i=0
n
k

xkyn−k,
where
n
k

=
n!
k! (n −k)!
(7.78)
is the standard notation for the binomial coeﬃcients. Since i2 = −1,
i3 = −i ,
i4 = 1,
etc., we have
(x + i y)n = xn + nxn−1( i y) +
n
2

xn−2( i y)2 +
n
3

xn−3( i y)3 + · · · + ( i y)n
= xn + i nxn−1 y −
n
2

xn−2 y2 −i
n
3

xn−3 y3 + · · · .
Separating the real and imaginary terms, we obtain the explicit formulas
Re (x + i y)n = xn −
n
2

xn−2 y2 +
n
4

xn−4 y4 + · · · ,
Im (x + i y)n = nxn−1 y −
n
3

xn−3 y3 +
n
5

xn−5 y5 + · · · ,
(7.79)

394
7 Linearity
for the two independent harmonic polynomials of degree n. The ﬁrst few of these poly-
nomials were described in Example 7.36. In fact, it can be proved that the most general
solution to the Laplace equation can be written as a convergent inﬁnite series in the basic
harmonic polynomials, cf. [61].
Exercises
7.4.40. Can you ﬁnd a complex matrix A such that ker A ̸= {0} and the real and imaginary
parts of every complex solution to Au = 0 are also solutions?
7.4.41. Find the general real solution to the following homogeneous diﬀerential equations:
(a) u′′ + 4u = 0, (b) u′′ + 6u′ + 10u = 0, (c) 2u′′′ + 3u′ −5u = 0, (d) u′′′′ + u = 0,
(e) u′′′′ + 13u′′ + 36u = 0, (f ) x2 u′′ −xu′ + 3u = 0, (g) x3 u′′′ + x2 u′′ + 3xu′ −8u = 0.
7.4.42. The following functions are solutions to a real constant coeﬃcient homogeneous scalar
ordinary diﬀerential equation. (i) Determine the least possible order of the diﬀerential
equation, and (ii) write down an appropriate diﬀerential equation.
(a) e−x sin 3x,
(b) x sin x, (c) 1 + xe−x cos 2x,
(d) sin x + cos 2x,
(e) sin x + x2 cos x.
7.4.43. Find the general solution to the following complex ordinary diﬀerential equations.
Verify that, in these cases, the real and imaginary parts of a complex solution are not real
solutions.
(a) u′ + i u = 0, (b) u′′ −i u′ + ( i −1)u = 0,
(c) u′′ −i u = 0.
7.4.44.(a) Write down the explicit formulas for the harmonic polynomials of degree 4 and
check that they are indeed solutions to the Laplace equation.
(b) Prove that every
homogeneous polynomial solution of degree 4 is a linear combination of the two basic
harmonic polynomials.
7.4.45. Find all complex exponential solutions u(t, x) = eω t+kx of the beam equation
∂2u
∂t2 = ∂4u
∂x4 . How many diﬀerent real solutions can you produce?
♥7.4.46.(a) Show that, if k ∈R, then u(t, x) = e−k2 t+ i kx is a complex solution to the
heat equation ∂u
∂t = ∂2u
∂x2 .
(b) Use complex conjugation to write down another complex
solution. (c) Find two independent real solutions to the heat equation. (d) Can k be
complex? If so, what real solutions are produced? (e) Which of your solutions decay to
zero as t →∞? (f ) Can you solve the exercise assuming k ∈C \ R is not real?
7.4.47. Show that the free space Schr¨odinger equation i ∂u
∂t = ∂2u
∂x2 is not a real linear system
by constructing a complex quadratic polynomial solution and verifying that its real and
imaginary parts are not solutions.
7.4.48. Which of the following sets of vectors span conjugated subspaces of C3?
(a)
⎛
⎜
⎝
1
−1
2
⎞
⎟
⎠; (b)
⎛
⎜
⎝
1
−i
2 i
⎞
⎟
⎠; (c)
⎛
⎜
⎝
1
0
3
⎞
⎟
⎠,
⎛
⎜
⎝
1
1
−1
⎞
⎟
⎠; (d)
⎛
⎜
⎝
1
0
i
⎞
⎟
⎠,
⎛
⎜
⎝
i
1
0
⎞
⎟
⎠; (e)
⎛
⎜
⎝
i
1
0
⎞
⎟
⎠,
⎛
⎜
⎝
1
0
−i
⎞
⎟
⎠,
⎛
⎜
⎝
0
1
i
⎞
⎟
⎠.
♦7.4.49. Prove that the real and imaginary parts of a general element of a conjugated vector
space, as deﬁned by (7.76), are both real elements.
♦7.4.50. Prove that a subspace V ⊂Cn is conjugated if and only if it admits a basis all of whose
elements are real.
♦7.4.51. Prove that if L[u] = f is a real inhomogeneous linear system with real right-hand side
f, and u = v + i w is a complex solution, then its real part v is a solution to the system,
L[v] = f, while its imaginary part w solves the homogeneous system L[w] = 0.

7.5 Adjoints, Positive Deﬁnite Operators, and Minimization Principles
395
♦7.4.52. Prove that a linear function L: Cn →Cm is real if and only if L[u] = Au, where A is a
real m × n matrix.
♦7.4.53. Let u = x + i y be a complex solution to a real linear system. Under what conditions
are its real and imaginary parts x, y linearly independent real solutions?
7.5 Adjoints, Positive Deﬁnite Operators,
and Minimization Principles
Sections 2.5 and 4.4 revealed the importance of the adjoint system AT y = f in the analysis
of systems of linear algebraic equations A x = b. Two of the four fundamental matrix
subspaces are based on the transposed matrix. While the m × n matrix A deﬁnes a linear
function from Rn to Rm, its transpose, AT , has size n×m and hence characterizes a linear
function in the reverse direction, from Rm to Rn.
As with most basic concepts for linear algebraic systems, the adjoint system and trans-
pose operation on the coeﬃcient matrix are the prototypes of a more general construction
that is valid for general linear functions. However, it is not immediately obvious how to
“transpose” a more general linear operator L[u], e.g., a diﬀerential operator acting on
function space. In this section, we shall introduce the abstract concept of the adjoint of a
linear function that generalizes the transpose operation on matrices. This will be followed
by a general characterization of positive deﬁnite linear operators and the characterization
of the solutions to the associated linear systems via minimization principles. Unfortunately,
we will not have suﬃcient analytical tools to develop most of the interesting examples, and
instead refer the interested reader to [61, 79].
The adjoint (and transpose) rely on an inner product structure on both the domain and
codomain spaces. For simplicity, we restrict our attention to real inner product spaces,
leaving the complex version to the interested reader. Thus, we begin with a linear function
L: U →V that maps an inner product space U to a second inner product space V . We
distinguish the inner products on U and V (which may be diﬀerent even when U and V
are the same vector space) by using a single angle bracket
⟨u , u⟩
to denote the inner product between
u, u ∈U,
and a double angle bracket
⟨⟨v , v ⟩⟩
to denote the inner product between
v, v ∈V.
Once inner products on both the domain and codomain are prescribed, the abstract deﬁ-
nition of the adjoint of a linear function can be formulated.
Deﬁnition 7.53. Let U, V be inner product spaces, and let L: U →V be a linear function.
The adjoint of L is the function† L∗: V →U that satisﬁes
⟨⟨L[u] , v ⟩⟩= ⟨u , L∗[v] ⟩
for all
u ∈U,
v ∈V.
(7.80)
†
The notation L∗unfortunately coincides with that of the dual linear function introduced in
Exercise 7.2.30.
These clashing notations are both well established in the literature, although
occasionally a prime, as in V ′, L′, is used for dual spaces, maps, etc.
However, it is possible
to reconcile the two notations in a natural manner; see Exercise 7.5.10. In this book, the dual
notation appears only in these few exercises.

396
7 Linearity
Note that the adjoint function goes in the opposite direction to L, just like the transposed
matrix. Also, the left-hand side of equation (7.80) indicates the inner product on V , while
the right-hand side is the inner product on U — which is where the respective vectors
live. In inﬁnite-dimensional situations, the adjoint may not exist. But if it does, then it is
uniquely determined by (7.80); see Exercise 7.5.7.
Remark. Technically, (7.80) serves to deﬁne the “formal adjoint” of the linear operator L.
For the inﬁnite-dimensional function spaces arising in analysis, a true adjoint must satisfy
certain additional analytical requirements, [50, 67]. However, for pedagogical reasons, it is
better to suppress such advanced analytical complications in this introductory treatment.
Lemma 7.54. The adjoint of a linear function is a linear function.
Proof : Given u ∈U, v, w ∈V , and scalars c, d ∈R, using the deﬁning property of the
adjoint and the bilinearity of the two inner products produces
⟨u , L∗[cv + dw] ⟩= ⟨⟨L[u] , cv + dw ⟩⟩= c ⟨⟨L[u], v ⟩⟩+ d ⟨⟨L[u] , w ⟩⟩
= c ⟨u , L∗[v] ⟩+ d ⟨u , L∗[w] ⟩= ⟨u , c L∗[v] + d L∗[w] ⟩.
Since this holds for all u ∈U, we must have
L∗[cv + dw] = c L∗[v] + d L∗[w],
thereby proving linearity.
Q.E.D.
Example 7.55.
Let us ﬁrst show how the deﬁning equation (7.80) for the adjoint leads
directly to the transpose of a matrix. Let L: Rn →Rm be the linear function L[v] = Av
deﬁned by multiplication by the m × n matrix A. Then L∗: Rm →Rn is linear, and so is
represented by matrix multiplication, L∗[v] = A∗v, by an n × m matrix A∗. We impose
the ordinary Euclidean dot products
⟨u , u ⟩= u · u = uT u,
u, u ∈Rn,
⟨⟨v , v ⟩⟩= v · v = vT v,
v, v ∈Rm,
as our inner products on both Rn and Rm. Evaluation of both sides of the adjoint identity
(7.80) yields
⟨⟨L[u] , v ⟩⟩= ⟨⟨Au , v ⟩⟩= (Au)T v = uT AT v,
⟨u , L∗[v] ⟩= ⟨u , A∗v ⟩= uT A∗v.
(7.81)
Since these expressions must agree for all u, v, the matrix A∗representing L∗is equal to
the transposed matrix AT , as justiﬁed in Exercise 1.6.13. We conclude that the adjoint of
a matrix with respect to the Euclidean dot product is its transpose: A∗= AT .
Remark. See Exercise 7.2.30 for another interpretation of the transpose in terms of dual
vector spaces. Again, keep in mind that the ∗notation has a diﬀerent meaning there.
Example 7.56.
Let us now adopt diﬀerent, weighted inner products on the domain and
codomain for the linear map L: Rn →Rm given by L[u] = Av. Suppose that
(i) the inner product on the domain space Rn is given by ⟨u , u ⟩= uTM u, while
(ii) the inner product on the codomain Rm is given by ⟨⟨v , v ⟩⟩= vT C v.
Here M and C are positive deﬁnite matrices of respective sizes n × n and m × m. Then,
in place of (7.81), we have
⟨⟨Au , v ⟩⟩= (Au)TC v = uTAT C v,
⟨u , A∗v ⟩= uT MA∗v.

7.5 Adjoints, Positive Deﬁnite Operators, and Minimization Principles
397
Equating these expressions, we deduce that AT C = MA∗. Therefore, the weighted adjoint
of the matrix A is given by the more complicated formula
A∗= M −1AT C.
(7.82)
In mechanical applications, M plays the role of the mass matrix, and explicitly appears
in the dynamical systems to be studied in Chapter 10. In particular, suppose A is square,
deﬁning a linear transformation L: Rn →Rn.
If we adopt the same inner product
⟨v , v ⟩= vT C v on both the domain and codomain spaces Rn, then its adjoint matrix
A∗= C−1 AT C is similar to its transpose.
Everything that we learned about transposes can be reinterpreted in the more general
language of adjoints. First, applying the adjoint operation twice returns you to where you
began; this is an immediate consequence of the deﬁning equation (7.80).
Proposition 7.57. The adjoint of the adjoint of L is just L = (L∗)∗.
The next result generalizes the fact, (1.55), that the transpose of the product of two
matrices is the product of the transposes, in the reverse order.
Proposition 7.58. Let U, V, W be inner product spaces. If L: U →V and M: V →W
have respective adjoints L∗: V →U and M∗: W →V , then the composite linear function
M ◦L: U →W has adjoint (M ◦L)∗= L∗◦M∗, which maps W to U.
Proof : Let ⟨u , u ⟩, ⟨⟨v , v ⟩⟩, ⟨⟨⟨w , w ⟩⟩⟩, denote, respectively, the inner products on U, V, W.
For u ∈U, w ∈W, we compute using the deﬁnition (7.80) repeatedly:
⟨u , (M ◦L)∗[w] ⟩= ⟨⟨⟨M ◦L[u] , w ⟩⟩⟩= ⟨⟨⟨M[L[u]] , w ⟩⟩⟩
= ⟨⟨L[u] , M∗[w] ⟩⟩= ⟨u , L∗[M∗[w]] ⟩= ⟨u , L∗◦M∗[w] ⟩.
Since this holds for all u and w, the identiﬁcation follows.
Q.E.D.
In this chapter, we have been able to actually compute adjoints in just the ﬁnite-
dimensional situation, when the linear functions are given by matrix multiplication. For
the more challenging case of adjoints of linear operators on function spaces, e.g., diﬀerential
operators appearing in boundary value problems, the reader should consult [61].
Exercises
7.5.1. Choose one from the following list of inner products on R2. Then ﬁnd the adjoint of
A =

1
2
−1
3
	
when your inner product is used on both its domain and codomain. (a) the
Euclidean dot product; (b) the weighted inner product ⟨v , w ⟩= 2v1 w1 + 3v2 w2; (c) the
inner product ⟨v , w ⟩= vT K w deﬁned by the positive deﬁnite matrix K =

2
−1
−1
4
	
.
7.5.2. From the list in Exercise 7.5.1, choose diﬀerent inner products on the domain and
codomain, and then determine the adjoint of the matrix A.
7.5.3. Choose one from the following list of inner products on R3 for both the domain and
codomain, and ﬁnd the adjoint of A =
⎛
⎜
⎝
1
1
0
−1
0
1
0
−1
2
⎞
⎟
⎠: (a) the Euclidean dot product;

398
7 Linearity
(b) the weighted inner product ⟨v , w ⟩= v1 w1 + 2v2 w2 + 3v3 w3; (c) the inner product
⟨v , w ⟩= vT K w deﬁned by the positive deﬁnite matrix K =
⎛
⎜
⎝
2
1
0
1
2
1
0
1
2
⎞
⎟
⎠.
7.5.4. From the list in Exercise 7.5.3, choose diﬀerent inner products on the domain and
codomain, and then compute the adjoint of the matrix A.
7.5.5. Choose an inner product on R2 from the list in Exercise 7.5.1, and an inner product on
R3 from the list in Exercise 7.5.3, and then compute the adjoint of A =
⎛
⎜
⎝
1
3
0
2
−1
1
⎞
⎟
⎠.
7.5.6. Let P(2) be the space of quadratic polynomials equipped with the inner product
⟨p , q ⟩=
 1
0 p(x) q(x) dx. Find the adjoint of the derivative operator D[p] = p′ on P(2).
♦7.5.7. Prove that, if it exists, the adjoint of a linear function is uniquely determined by (7.80).
♦7.5.8. Prove that
(a) (L + M)∗= L∗+ M∗,
(b) (cL)∗= cL∗for c ∈R,
(c) (L∗)∗= L,
(d) (L−1)∗= (L∗)−1.
♦7.5.9. Let L: U →V be a linear function between inner product spaces. Prove that u ∈Rn
solves the inhomogeneous linear system L[u] = f if and only if
⟨u , L∗[v] ⟩= ⟨f , v ⟩
for all
v ∈V.
(7.83)
Explain why Exercise 3.1.11 is a special case of this result.
Remark. Equation (7.83)
is known as the weak formulation of the linear system. It plays an essential role in the
analysis of diﬀerential equations and their numerical approximations, [61].
♦7.5.10. Suppose V, W are ﬁnite-dimensional inner product spaces with dual space V ∗, W∗. Let
L: V →W be a linear function, and let L∗: W∗→V ∗denote the dual linear function, as in
Exercise 7.2.30 (without the tilde), while L∗: W →V denotes its adjoint. (As noted above,
the same notation denotes two mathematically diﬀerent objects.) Prove that if we identify
V ∗≃V and W∗≃W using the linear isomorphism in Exercise 7.1.62, then the dual and
adjoint functions are identiﬁed L∗≃L∗, thus reconciling the unfortunate clash in notation.
In particular, this includes the two possible interpretations of the transpose of a matrix.
Self-Adjoint and Positive Deﬁnite Linear Functions
Throughout this section U will be an inner product space. We will show how to generalize
the notions of symmetric and positive deﬁnite matrices to linear operators on U in a natural
fashion. First, we deﬁne the analogue of a symmetric matrix.
Deﬁnition 7.59. A linear function J: U →U is called self-adjoint if J∗= J. A self-
adjoint linear function is positive deﬁnite, written J > 0, if
⟨u , J[u] ⟩> 0
for all
0 ̸= u ∈U.
(7.84)
In particular, if J > 0 then ker J = {0}. (Why?) Thus, a positive deﬁnite linear system
J[u] = f with f ∈img J must have a unique solution. The next result generalizes our
basic observation that the Gram matrices ATA and AT C A, cf. (3.62, 64), are symmetric
and positive (semi-)deﬁnite.

7.5 Adjoints, Positive Deﬁnite Operators, and Minimization Principles
399
Theorem 7.60. Let L: U →V be a linear map between inner product spaces with adjoint
L∗: V →U. Then the composite map J = L∗◦L: U →U is self-adjoint. Moreover, J is
positive deﬁnite if and only if ker L = {0}.
Proof : First, by Propositions 7.58 and 7.57,
J∗= (L∗◦L)∗= L∗◦(L∗)∗= L∗◦L = J,
proving self-adjointness. Furthermore, for u ∈U, the inner product
⟨u , J[u]⟩= ⟨u , L∗[L[u]] ⟩= ⟨L[u] , L[u] ⟩= ∥L[u]∥2 > 0
is strictly positive, provided that L[u] ̸= 0. Thus, if ker L = {0}, then the positivity
condition (7.84) holds, and conversely.
Q.E.D.
Let us specialize to the case of a linear function L: Rn →Rm that is represented by
the m × n matrix A, so that L[u] = Au. When the Euclidean dot product is used on
the two spaces, the adjoint L∗is represented by the transpose AT , and hence the map
J = L∗◦L has matrix representation J[u] = Ku, where K = ATA. Therefore, in this case
Theorem 7.60 reduces to our earlier Proposition 3.36, governing the positive deﬁniteness
of the Gram matrix product ATA. If we change the inner product on the codomain to
⟨⟨w , w ⟩⟩= wT C w for some C > 0, then L∗is represented by AT C, and hence J = L∗◦L
has matrix form K = AT C A, which is the general symmetric, positive deﬁnite Gram
matrix constructed in (3.64) that underlay our development of the equations of equilibrium
in Chapter 6.
Finally, if we further replace the dot product on the domain space Rn by the alternative
inner product ⟨v , v ⟩= vT M v for M > 0, then, according to formula (7.82), the adjoint
of L has matrix form
A∗= M −1AT C,
and therefore
K = A∗A = M −1AT C A
(7.85)
is a self-adjoint, positive (semi-)deﬁnite matrix with respect to the weighted inner product
on Rn prescribed by the positive deﬁnite matrix M. In this case, the positive deﬁnite,
self-adjoint operator J is no longer represented by a symmetric matrix. So, we did not
quite tell the truth when we said we would allow only symmetric matrices to be positive
deﬁnite — we really meant only self-adjoint matrices.
General self-adjoint matrices will be important in our discussion of the vibrations of
mass–spring chains that have unequal masses. Extensions of these constructions to dif-
ferential operators underlies the analysis of the boundary value problems of continuum
mechanics, to be studied in [61].
Exercises
7.5.11. Show that the following linear transformations of R2 are self-adjoint with respect to the
Euclidean dot product: (a) rotation through the angle θ = π; (b) reﬂection about the line
y = x. (c) The scaling map S[x] = 3x; (d) orthogonal projection onto the line y = x.
♦7.5.12. Let M be a positive deﬁnite matrix. Show that A: Rn →Rn is self-adjoint with respect
to the inner product ⟨v , w ⟩= vT M w if and only if M A is a symmetric matrix.

400
7 Linearity
7.5.13. Prove that A =

6
3
2
4
	
is self-adjoint with respect to the weighted inner product
⟨v , w ⟩= 2v1 w1 + 3v2 w2. Hint: Use the criterion in Exercise 7.5.12.
7.5.14. Consider the weighted inner product ⟨v , w ⟩= v1 w1 + 1
2 v2 w2 + 1
3 v3 w3 on R3.
(a) What are the conditions on the entries of a 3 × 3 matrix A in order that it be self-
adjoint? Hint: Use the criterion in Exercise 7.5.12. (b) Write down an example of a
non-diagonal self-adjoint matrix.
7.5.15. Answer Exercise 7.5.14 for the inner product based on
⎛
⎜
⎝
2
−1
0
−1
2
−1
0
−1
2
⎞
⎟
⎠.
7.5.16. True or false: The identity transformation is self-adjoint for an arbitrary inner product
on the underlying vector space.
7.5.17. True or false: A diagonal matrix is self-adjoint for an arbitrary inner product on Rn.
7.5.18. Suppose L: U →U has an adjoint L∗: U →U. (a) Show that L + L∗is self-adjoint.
(b) Show that L ◦L∗is self-adjoint.
♦7.5.19. Suppose J, M: U →U are self-adjoint linear functions on an inner product space U.
(a) Prove that ⟨J[u] , u ⟩= ⟨M[u] , u ⟩for all u ∈U if and only if J = M.
(b) Explain why this result is false if the self-adjointness hypothesis is dropped.
7.5.20. Prove that if L: U →U is an invertible linear transformation on an inner product space
U, then the following three statements are equivalent: (a) ⟨L[u] , L[v] ⟩= ⟨u , v ⟩for all
u, v ∈U. (b) ∥L[u]∥= ∥u∥for all u ∈U. (c) L∗= L−1. Hint: Use Exercise 7.5.19.
7.5.21.(a) Prove that the operation Ma[u(x)] = a(x) u(x) of multiplication by a continuous
function a(x) deﬁnes a self-adjoint linear operator on the function space C0[a, b] with
respect to the L2 inner product.
(b) Is Ma also self-adjoint with respect to the weighted
inner product ⟨⟨f , g ⟩⟩=
 b
a f(x) g(x) w(x) dx?
♥7.5.22. A linear function S: U →U is called skew-adjoint if S∗= −S. (a) Prove that a
skew-symmetric matrix is skew-adjoint with respect to the standard dot product on Rn.
(b) Under what conditions is S[x] = A x skew-adjoint with respect to the inner product
⟨x , y ⟩= xT M y on Rn? (c) Let L: U →U have an adjoint L∗. Prove that L −L∗is
skew-adjoint. (d) Explain why every linear operator L: U →U that has an adjoint L∗can
be written as the sum of a self-adjoint and a skew-adjoint operator.
♦7.5.23.(a) Let L1: U →V1 and L2: U →V2 be linear maps between inner product spaces,
with V1, V2 not necessarily the same. Let J1 = L∗
1 ◦L1, J2 = L∗
2 ◦L2. Show that the sum
J = J1 +J2 can be written as a self-adjoint combination J = L∗◦L for some linear operator
L. Hint: See Exercise 3.4.35 for the matrix case.
Minimization
In Chapter 5, we learned how the solution to a linear algebraic system Ku = f with positive
deﬁnite coeﬃcient matrix K can be characterized as the unique minimizer for the quad-
ratic function p(u) = 1
2 uT Ku −uT f. There is an analogous minimization principle that
characterizes the solutions to linear systems deﬁned by positive deﬁnite linear operators.
This general result is of tremendous importance in analysis of boundary value problems for
diﬀerential equations, for both physical and mathematical reasons, and also inspires the
ﬁnite element numerical solution algorithm, [61].
We restrict our attention to real linear functions on real vector spaces in this section.

7.5 Adjoints, Positive Deﬁnite Operators, and Minimization Principles
401
Theorem 7.61. Let J: U →U be a positive deﬁnite linear function on a real inner product
space U. If f ∈img J, then the quadratic function
p(u) = 1
2 ⟨u , J[u] ⟩−⟨u , f ⟩
(7.86)
has a unique minimizer u = u⋆, which is the solution to the linear system J[u] = f.
Proof : The proof mimics that of its matrix counterpart in Theorem 5.2. Our assumption
that f ∈img J implies that there is a u⋆∈U such that J[u⋆] = f. Thus, we can write
p(u) = 1
2 ⟨u , J[u]⟩−⟨u , J[u⋆] ⟩= 1
2 ⟨u −u⋆, J[u −u⋆] ⟩−1
2 ⟨u⋆, J[u⋆] ⟩,
(7.87)
where we used linearity, along with the fact that J is self-adjoint to identify the terms
⟨u , J[u⋆] ⟩= ⟨u⋆, J[u] ⟩. Since J > 0, the ﬁrst term on the right-hand side of (7.87) is
always ≥0; moreover, it equals its minimal value 0 if and only if u = u⋆. On the other
hand, the second term does not depend upon u at all, and hence is unaﬀected by variations
in u. Therefore, to minimize p(u), we must make the ﬁrst term as small as possible, which
is accomplished by setting u = u⋆.
Q.E.D.
Remark. For linear functions given by matrix multiplication, positive deﬁniteness auto-
matically implies invertibility, and so the linear system Ku = f has a solution for every
right-hand side. This is not so immediate when J is a positive deﬁnite operator on an
inﬁnite-dimensional function space. Therefore, the existence of a solution or minimizer
is a signiﬁcant issue. And, in fact, many modern analytical existence results rely on the
determination of suitable minimization principles. On the other hand, once existence is
assured, uniqueness follows immediately from the positive deﬁniteness of the operator J.
Theorem 7.62. Suppose L: U →V is a linear function between inner product spaces with
ker L = {0} and adjoint function L∗: V →U. Let J = L∗◦L: U →U be the associated
positive deﬁnite linear function. If f ∈img J, then the quadratic function
p(u) = 1
2 ∥L[u]∥2 −⟨u , f ⟩
(7.88)
has a unique minimizer u⋆, which is the solution to the linear system J[u⋆] = f.
Proof : It suﬃces to note that the quadratic term in (7.88) can be written in the alternative
form
∥L[u]∥2 = ⟨⟨L[u] , L[u] ⟩⟩= ⟨u , L∗[L[u]] ⟩= ⟨u , J[u]⟩.
Thus, (7.88) reduces to the quadratic function of the form (7.86) with J = L∗◦L, and so
Theorem 7.62 follows directly from Theorem 7.61.
Q.E.D.
Warning. In (7.88), the ﬁrst term ∥L[u]∥2 is computed using the norm based on the
inner product on V , while the second term ⟨u , f ⟩employs the inner product on U.
Example 7.63.
For a general positive deﬁnite matrix (7.85), the quadratic function
(7.88) is computed with respect to the alternative inner product ⟨u , u⟩= uT M u, so
p(u) = 1
2 ∥Au∥2 −⟨u , f ⟩= 1
2 (Au)TC Au −uTM f = 1
2 uT (AT C A)u −uT(M f).
Theorem 7.62 tells us that the minimizer of the quadratic function is the solution to
AT C Au = M f,
which we rewrite as
Ku = M −1 AT C Au = f.

402
7 Linearity
This conclusion also follows from our earlier ﬁnite-dimensional Minimization Theorem 5.2.
In [61, 79], it is shown that the most important minimization principles that charac-
terize solutions to the linear boundary value problems of physics and engineering all arise
through this remarkably general mathematical construction.
Exercises
7.5.24. Find the minimum value of p(u) = 1
2 uT

3
−2
−2
3
	
u −uT

1
−1
	
for u ∈R2.
7.5.25. Minimize the function p(u) = 1
2 uT
⎛
⎜
⎝
2
−1
0
−1
4
−2
0
−2
3
⎞
⎟
⎠u −uT
⎛
⎜
⎝
2
0
−1
⎞
⎟
⎠for u ∈R3.
7.5.26. Minimize ∥( 2x −y, x + y )T ∥2 −6x over all x, y, where ∥·∥denotes the Euclidean
norm on R2.
7.5.27. Answer Exercise 7.5.26 for
(a) the weighted norm ∥( x, y )T ∥=

2x2 + 3y2 ;
(b) the norm based on

2
−1
−1
1
	
;
(c) the norm based on

3
1
1
3
	
.
7.5.28. Let L(x, y) =
⎛
⎜
⎝
x −2y
x + y
−x + 3y
⎞
⎟
⎠and f =

1
0
	
. Minimize p(x) = 1
2 ∥L[x]∥2 −⟨x , f ⟩using
(a) the Euclidean inner products and norms on both R2 and R3; (b) the Euclidean inner
product on R2 and the weighted norm ∥w∥=

w2
1 + 2w2
2 + 3w2
3 on R3; (c) the inner
product given by

2
−1
−1
2
	
on R2 and the Euclidean norm on R3; (d) the inner product
given by

2
−1
−1
2
	
on R2 and the weighted norm ∥w∥=

w2
1 + 2w2
2 + 3w2
3 on R3.
7.5.29. Find the minimum distance between the point ( 1, 0, 0 )T and the plane x + y −z = 0
when distance is measured in (a) the Euclidean norm; (b) the weighted norm ∥w∥=

w2
1 + 2w2
2 + 3w2
3 ; (c) the norm based on the positive deﬁnite matrix
⎛
⎜
⎝
3
−1
1
−1
2
−1
1
−1
3
⎞
⎟
⎠.
♦7.5.30. How would you modify the statement of Theorem 7.62 if ker L ̸= {0}?

Chapter 8
Eigenvalues and Singular Values
So far, our physical applications of linear algebra have concentrated on statics: unchanging
equilibrium conﬁgurations of mass–spring chains, circuits, and structures, all modeled by
linear systems of algebraic equations. It is now time to set the universe in motion. In
general, a (continuous) dynamical system refers to the (diﬀerential) equations governing
the time-varying motion of a physical system, be it mechanical, electrical, chemical, ﬂuid,
thermodynamical, biological, ﬁnancial, . . . . Our immediate goal is to solve the simplest
class of continuous dynamical models, which are ﬁrst order autonomous linear systems of
ordinary diﬀerential equations.
We begin with a very quick review of the scalar case, whose solutions are simple ex-
ponential functions.
This inspires us to try to solve a vector-valued linear system by
substituting a similar exponential solution formula. We are immediately led to the sys-
tem of algebraic equations that deﬁne the eigenvalues and eigenvectors of the coeﬃcient
matrix. Thus, before we can make any progress in our study of diﬀerential equations, we
need to learn about eigenvalues and eigenvectors, and that is the purpose of the present
chapter. Dynamical systems are used to motivate the subject, but serious applications will
be deferred until Chapter 10. Additional applications of eigenvalues and eigenvectors to
linear iterative systems, stochastic processes, and numerical solution algorithms for linear
algebraic systems form the focus of Chapter 9.
Each square matrix possesses a collection of one or more complex scalars, called eigen-
values, and associated vectors, called eigenvectors.
From a geometrical viewpoint, the
matrix deﬁnes a linear transformation on Euclidean space; the eigenvectors indicate the
directions of pure stretch and the eigenvalues the extent of stretching.
We will intro-
duce the non-standard term “complete” to describe matrices whose (complex) eigenvectors
form a basis of the underlying vector space. A more common name for such matrices is
“diagonalizable”, because, when expressed in terms of its eigenvector basis, the matrix
representing the corresponding linear transformation assumes a very simple diagonal form,
facilitating the detailed analysis of its properties. A particularly important class consists
of the symmetric matrices, whose eigenvectors form an orthogonal basis of Rn; in fact, this
is how orthogonal bases most naturally appear. Most matrices are complete; incomplete
matrices are trickier to deal with, and we discuss their non-diagonal Schur decomposition
and Jordan canonical form in Section 8.6.
A non-square matrix A does not possess eigenvalues. In their place, one studies the
eigenvalues of the associated square Gram matrix K = ATA, whose square roots are known
as the singular values of the original matrix. The corresponding singular value decompo-
sition (SVD) supplies the ﬁnal details for our understanding of the remarkable geometric
structure governing matrix multiplication. The singular value decomposition is used to de-
ﬁne the pseudoinverse of a matrix, which provides a mechanism for “inverting” non-square
and singular matrices, and an alternative construction of least squares solutions to general
linear systems. Singular values underlie the powerful method of modern statistical data
analysis known as principal component analysis (PCA), which will be developed in the
© Springer International Publishing AG, part of Springer Nature 2018 
https://doi.org/10.1007/978-3-319-91041-3_8 
403
P. J. Olver, C. Shakiban, Applied Linear Algebra, Undergraduate Texts in Mathematics, 

404
8 Eigenvalues and Singular Values
ﬁnal section of this chapter and appears in an increasingly broad range of contemporary
applications, including image processing, semantics, language and speech recognition, and
machine learning.
Remark.
The numerical computation of eigenvalues and eigenvectors is a challenging
issue, and must be deferred until Section 9.5. Unless you are prepared to consult that
section in advance, solving the computer-based problems in this chapter will require access
to computer software that can accurately compute eigenvalues and eigenvectors.
8.1 Linear Dynamical Systems
Our new goal is to solve and analyze the simplest class of dynamical systems, namely those
modeled by ﬁrst order linear systems of ordinary diﬀerential equations. We begin with
a thorough review of the scalar case, including a complete investigation into the stability
of their equilibria — in preparation for the general situation to be treated in depth in
Chapter 10. Readers who are not interested in such motivational material may skip ahead
to Section 8.2 without incurring any penalty.
Scalar Ordinary Diﬀerential Equations
Consider the elementary ﬁrst order scalar ordinary diﬀerential equation
du
dt = au.
(8.1)
Here a ∈R is a real constant, while the unknown u(t) is a scalar function. As you no doubt
already learned, e.g., in [7, 22, 78], the general solution to (8.1) is an exponential function
u(t) = c eat.
(8.2)
The integration constant c is uniquely determined by a single initial condition
u(t0) = b
(8.3)
imposed at an initial time t0. Substituting t = t0 into the solution formula (8.2),
u(t0) = c eat0 = b,
and so
c = b e−at0.
We conclude that
u(t) = b ea(t−t0)
(8.4)
is the unique solution to the scalar initial value problem (8.1, 3).
Example 8.1.
The radioactive decay of an isotope, say uranium-238, is governed by the
diﬀerential equation
du
dt = −γ u.
(8.5)
Here u(t) denotes the amount of the isotope remaining at time t; the coeﬃcient γ > 0
governs the decay rate. The solution is an exponentially decaying function u(t) = c e−γ t,
where c = u(0) is the amount of radioactive material at the initial time t0 = 0.
The isotope’s half-life t⋆is the time it takes for half of a sample to decay, that is, when
u(t⋆) = 1
2 u(0). To determine t⋆, we solve the algebraic equation
e−γ t⋆= 1
2 ,
so that
t⋆= log 2
γ
.
(8.6)

8.1 Linear Dynamical Systems
405
a < 0
a = 0
a > 0
Figure 8.1.
Solutions to
u = a u.
Thus, after an elapsed time of t⋆, half of the original material has decayed. After a further
t⋆, half of the remaining half has decayed, leaving a quarter of the original radioactive
material. And so on, so that at each integer multiple nt⋆, n ∈N, of the half-life, the
remaining amount of the isotope is u(nt⋆) = 2−n u(0).
Returning to the general situation, let us make some elementary, but pertinent, obser-
vations about this simplest linear dynamical system. First of all, since the equation is
homogeneous, the zero function u(t) ≡0 — corresponding to c = 0 in the solution for-
mula (8.2) — is a constant solution, known as an equilibrium solution, or ﬁxed point, since
it does not depend on t. If the coeﬃcient a > 0 is positive, then the solutions (8.2) are
exponentially growing (in absolute value) as t →+∞. This implies that the zero equilib-
rium solution is unstable. The initial condition u(t0) = 0 produces the zero solution, but if
we make a tiny error (either physical, numerical, or mathematical) in the initial data, say
u(t0) = ε, then the solution u(t) = ε ea(t−t0) will eventually be far away from equilibrium.
More generally, any two solutions with very close, but not equal, initial data will eventu-
ally become arbitrarily far apart: | u1(t) −u2(t) | →∞as t →∞. One consequence is an
inherent diﬃculty in accurately computing the long-time behavior of the solution, since
small numerical errors may eventually have very large eﬀects.
On the other hand, if a < 0, the solutions are exponentially decaying in time. In this
case, the zero equilibrium solution is stable, since a small change in the initial data will
have a negligible eﬀect on the solution. In fact, the zero solution is globally asymptotically
stable. The phrase “asymptotically stable” indicates that solutions that start out near the
equilibrium point approach it in the large time limit; more speciﬁcally, if u(t0) = ε is small,
then u(t) →0 as t →∞. “Globally” implies that all solutions, no matter how large the
initial data, eventually approach equilibrium. In fact, for a linear system, the stability of
an equilibrium solution is inevitably a global phenomenon.
The borderline case is a = 0. Then all the solutions to (8.1) are constant. In this case,
the zero solution is stable — indeed, globally stable — but not asymptotically stable. The
solution to the initial value problem u(t0) = ε is u(t) ≡ε. Therefore, a solution that starts
out near equilibrium will remain nearby, but will not asymptotically approach it. The
three qualitatively diﬀerent possibilities are illustrated in Figure 8.1. A formal deﬁnition
of the various notions of stability for dynamical systems can be found in Deﬁnition 10.14.

406
8 Eigenvalues and Singular Values
Exercises
8.1.1. Solve the following initial value problems:
(a) du
dt = 5u, u(0) = −3,
(b) du
dt = 2u, u(1) = 3,
(c) du
dt = −3u, u(−1) = 1.
8.1.2. Suppose a radioactive material has a half-life of 100 years. What is the decay rate γ?
Starting with an initial sample of 100 grams, how much will be left after 10 years?
after 100 years? after 1, 000 years?
8.1.3. Carbon-14 has a half-life of 5730 years. Human skeletal fragments discovered in a cave
are analyzed and found to have only 6.24% of the carbon-14 that living tissue would have.
How old are the remains?
8.1.4. Prove that if t⋆is the half-life of a radioactive material, then u(nt⋆) = 2−n u(0).
Explain the meaning of this equation in your own words.
8.1.5. A bacteria colony grows according to the equation du/dt = 1.3 u. How long until the
colony doubles? quadruples? If the initial population is 2, how long until the population
reaches 2 million?
8.1.6. Deer in northern Minnesota reproduce according to the linear diﬀerential equation
du
dt = .27u where t is measured in years. If the initial population is u(0) = 5,000 and
the environment can sustain at most 1,000,000 deer, how long until the deer run out of
resources?
♦8.1.7. Consider the inhomogeneous diﬀerential equation du
dt = au + b, where a, b are constants.
(a) Show that u⋆= −b/a is a constant equilibrium solution. (b) Solve the diﬀerential
equation. Hint: Look at the diﬀerential equation satisﬁed by v = u −u⋆.
(c) Discuss the stability of the equilibrium solution u⋆.
8.1.8. Use the method of Exercise 8.1.7 to solve the following initial value problems:
(a) du
dt = 2u −1, u(0) = 1, (b) du
dt = 5u + 15, u(1) = −3, (c) du
dt = −3u + 6, u(2) = −1.
8.1.9. The radioactive waste from a nuclear reactor has a half-life of 1000 years. Waste is
continually produced at the rate of 5 tons per year and stored in a dump site.
(a) Set up an inhomogeneous diﬀerential equation, of the form in Exercise 8.1.7, to model
the amount of radioactive waste.
(b) Determine whether the amount of radioactive
material at the dump increases indeﬁnitely, decreases to zero, or eventually stabilizes at
some ﬁxed amount.
(c) Starting with a brand new site, how long will it be until the dump
contains 100 tons of radioactive material?
♥8.1.10. Suppose that hunters are allowed to shoot a ﬁxed number of the northern Minnesota
deer in Exercise 8.1.6 each year. (a) Explain why the population model takes the form
du
dt = .27u−b, where b is the number killed yearly. (Ignore the seasonal aspects of hunting.)
(b) If b = 1,000, how long until the deer run out of resources? Hint: See Exercise 8.1.7.
(c) What is the maximal rate at which deer can be hunted without causing their
extinction?
8.1.11.(a) Write down the exact solution to the initial value problem
du
dt = 2
7 u, u(0) = 1
3 .
(b) Suppose you make the approximation u(0) = .3333. At what point does your solution
diﬀer from the true solution by 1 unit? by 1000 units? (c) Answer the same question if you
also approximate the coeﬃcient in the diﬀerential equation by du
dt = .2857 u.

8.1 Linear Dynamical Systems
407
♦8.1.12. Let a be complex. Prove that u(t) = ceat is the (complex) solution to our scalar
ordinary diﬀerential equation (8.1). Describe the asymptotic behavior of the solution as
t →∞, and the stability properties of the zero equilibrium solution.
♦8.1.13.(a) Prove that if u1(t) and u2(t) are any two distinct solutions to du
dt = au with a > 0,
then | u1(t) −u2(t) | →∞as t →∞.
(b) If a = .02 and u1(0) = .1, u2(0) = .05, how long
do you have to wait until | u1(t) −u2(t) | > 1, 000?
First Order Dynamical Systems
The simplest class of dynamical system is a coupled system of n ﬁrst order ordinary diﬀer-
ential equations
du1
dt = f1(t, u1, . . . , un),
. . .
dun
dt = fn(t, u1, . . . , un).
The unknowns are the n scalar functions u1(t), . . . , un(t) depending on the scalar variable
t ∈R, which we usually view as time, whence the term “dynamical”. We will often write
the system in the equivalent vector form
du
dt = f(t, u),
(8.7)
in which u(t) = (u1(t), . . . , un(t))T is the vector-valued solution, which serves to parameter-
ize a curve in Rn, and f(t, u) is a vector-valued function with components fi(t, u1, . . . , un)
for i = 1, . . ., n. A dynamical system is called autonomous if the time variable t does not
appear explicitly on the right-hand side, and so has the form
du
dt = f(u).
(8.8)
Dynamical systems of ordinary diﬀerential equations appear in an astonishing variety of
applications, including physics, astronomy, chemistry, biology, weather and climate, eco-
nomics and ﬁnance, and have been intensely studied since the very ﬁrst days following the
invention of calculus.
In this text, we shall concentrate most of our attention on the very simplest case: a
homogeneous, linear, autonomous dynamical system, in which the right-hand side f(u) is
a linear function of u that is independent of the time t, and hence given by multiplication
by a constant matrix. Thus, the system takes the form
du
dt = Au,
(8.9)
in which A is a constant n × n matrix. Writing out the system (8.9) in full detail produces
du1
dt = a11 u1 + a12 u2 + · · · + a1n un,
du2
dt = a21 u1 + a22 u2 + · · · + a2n un,
...
...
dun
dt = an1 u1 + an2 u2 + · · · + ann un,
(8.10)

408
8 Eigenvalues and Singular Values
and we seek the solution u(t) = ( u1(t), . . . , un(t) )T . In the autonomous case, which is the
only type to be treated in depth here, the coeﬃcients aij are assumed to be (real) constants.
We are interested not only in the formulas for the solutions, but also in understanding their
qualitative and quantitative behavior.
Drawing our inspiration from the exponential solution formula (8.2) for the scalar ver-
sion, let us investigate whether the vector system admits any solutions of a similar expo-
nential form
u(t) = eλt v.
(8.11)
We assume that λ is a constant scalar, so eλt is the usual scalar exponential function, while
v ∈Rn is a constant vector. In other words, the components ui(t) = eλtvi of our desired
solution are assumed to be constant multiples of the same exponential function. Since v
is constant, the derivative of u(t) is easily found:
du
dt = d
dt

eλt v

= λ eλt v.
On the other hand, since eλt is a scalar, it commutes with matrix multiplication, and so
Au = A eλt v = eλtAv.
Therefore, u(t) will solve the system (8.9) if and only if
λ eλt v = eλtAv,
or, canceling the common scalar factor eλt,
λv = Av.
The result is a system of n algebraic equations relating the vector v and the scalar λ. Anal-
ysis of this system and its ramiﬁcations will be the topic of the remainder of this chapter.
A broad range of signiﬁcant applications will appear in the subsequent two chapters.
8.2 Eigenvalues and Eigenvectors
In view of the preceding motivational section, we hereby inaugurate our discussion of
eigenvalues and eigenvectors by stating the basic deﬁnition.
Deﬁnition 8.2. Let A be an n×n matrix. A scalar λ is called an eigenvalue of A if there
is a non-zero vector v ̸= 0, called an eigenvector, such that
Av = λv.
(8.12)
In geometric terms, the matrix A has the eﬀect of stretching the eigenvector v by an
amount speciﬁed by the eigenvalue λ.
Remark.
The odd-looking terms “eigenvalue” and “eigenvector” are hybrid German–
English words. In the original German, they are Eigenwert and Eigenvektor, which can
be fully translated as “proper value” and “proper vector”.
For some reason, the half-
translated terms have acquired a certain charm, and are now standard. The alternative
English terms characteristic value and characteristic vector can be found in some (mostly
older) texts. Oddly, the terms characteristic polynomial and characteristic equation, to be
deﬁned below, are still used rather than “eigenpolynomial” and “eigenequation”.
The requirement that the eigenvector v be nonzero is important, since v = 0 is a trivial
solution to the eigenvalue equation (8.12) for every scalar λ. Moreover, as far as solving

8.2 Eigenvalues and Eigenvectors
409
linear ordinary diﬀerential equations goes, the zero vector v = 0 gives u(t) ≡0, which is
certainly a solution, but one that we already knew.
The eigenvalue equation (8.12) is a system of linear equations for the entries of the
eigenvector v — provided that the eigenvalue λ is speciﬁed in advance — but is “mildly”
nonlinear as a combined system for λ and v. Gaussian Elimination per se will not solve
the problem, and we are in need of a new idea. Let us begin by rewriting the equation in
the form†
(A −λ I )v = 0,
(8.13)
where I is the identity matrix of the correct size, whereby λ I v = λv. Now, for given
λ, equation (8.13) is a homogeneous linear system for v, and always has the trivial zero
solution v = 0. But we are speciﬁcally seeking a nonzero solution! According to The-
orem 1.47, a homogeneous linear system has a nonzero solution v ̸= 0 if and only if its
coeﬃcient matrix, which in this case is A −λ I , is singular. This observation is the key to
resolving the eigenvector equation.
Theorem 8.3. A scalar λ is an eigenvalue of the n × n matrix A if and only if the matrix
A −λ I is singular, i.e., of rank < n.
The corresponding eigenvectors are the nonzero
solutions to the eigenvalue equation (A −λ I )v = 0.
We know a number of ways to characterize singular matrices, including the vanishing
determinant criterion given in (1.84). Therefore, the following result is immediate:
Corollary 8.4. A scalar λ is an eigenvalue of the matrix A if and only if λ is a solution
to the characteristic equation
det(A −λ I ) = 0.
(8.14)
In practice, when computing eigenvalues and eigenvectors by hand using exact arith-
metic, one ﬁrst solves the characteristic equation (8.14) to obtain the set of eigenvalues.
Then, for each eigenvalue λ one uses standard linear algebra methods, e.g., Gaussian Elim-
ination, to solve the corresponding linear system (8.13) for the associated eigenvector v.
Example 8.5.
Consider the 2 × 2 matrix
A =

3
1
1
3

.
We compute the determinant in the characteristic equation using formula (1.38):
det(A −λ I ) = det

3 −λ
1
1
3 −λ

= (3 −λ)2 −1 = λ2 −6λ + 8.
Thus, the characteristic equation is a quadratic polynomial equation, and can be solved by
factorization:
λ2 −6λ + 8 = (λ −4) (λ −2) = 0.
We conclude that A has two eigenvalues: λ1 = 4 and λ2 = 2.
For each eigenvalue, the corresponding eigenvectors are found by solving the associated
homogeneous linear system (8.13). For the ﬁrst eigenvalue, the eigenvector equation is
(A −4 I )v =

−1
1
1
−1
 
x
y

=

0
0

,
or
−x + y = 0,
x −y = 0.
†
Note that it is not legal to write (8.13) in the form (A −λ)v = 0 since we do not know how
to subtract a scalar λ from a matrix A. Worse, if you type A −λ in Matlab or Mathematica,
the result will be to subtract λ from all the entries of A, which is not what we are after!

410
8 Eigenvalues and Singular Values
The general solution is
x = y = a,
so
v =

a
a

= a

1
1

,
where a is an arbitrary scalar. Only the nonzero solutions† count as eigenvectors, and so
the eigenvectors for the eigenvalue λ1 = 4 must have a ̸= 0, i.e., they are all nonzero scalar
multiples of the basic eigenvector v1 = ( 1, 1 )T .
Remark. In general, if v is an eigenvector of A for the eigenvalue λ, then so is every
nonzero scalar multiple of v. In practice, we distinguish only linearly independent eigen-
vectors. Thus, in this example, we shall say “v1 = ( 1, 1 )T is the eigenvector corresponding
to the eigenvalue λ1 = 4”, when we really mean that the set of eigenvectors for λ1 = 4
consists of all nonzero scalar multiples of v1.
Similarly, for the second eigenvalue λ2 = 2, the eigenvector equation is
(A −2 I )v =

1
1
1
1
 
x
y

=

0
0

.
The solution ( −a, a )T = a ( −1, 1 )T
is the set of scalar multiples of the eigenvector
v2 = ( −1, 1 )T . Therefore, the complete list of eigenvalues and eigenvectors (up to scalar
multiple) for this particular matrix is
λ1 = 4,
v1 =

1
1

,
λ2 = 2,
v2 =

−1
1

.
Example 8.6.
Consider the 3 × 3 matrix
A =
⎛
⎝
0
−1
−1
1
2
1
1
1
2
⎞
⎠.
Using the formula (1.88) for a 3 × 3 determinant, we compute the characteristic equation
0 = det(A −λ I ) = det
⎛
⎝
−λ
−1
−1
1
2 −λ
1
1
1
2 −λ
⎞
⎠
= (−λ)(2 −λ)2 + (−1) · 1 · 1 + (−1) · 1 · 1
−1 · (2 −λ)(−1) −1 · 1 · (−λ) −(2 −λ) · 1 · (−1) = −λ3 + 4λ2 −5λ + 2.
The resulting cubic polynomial can be factored:
−λ3 + 4λ2 −5λ + 2 = −(λ −1)2 (λ −2) = 0.
†
If, at this stage, you end up with a linear system with only the trivial zero solution, you’ve done
something wrong! Either you don’t have a correct eigenvalue — maybe you made a mistake setting
up and/or solving the characteristic equation — or you’ve made an error solving the homogeneous
eigenvector system. On the other hand, if you are working with a numerical approximation to
the eigenvalue, then the resulting numerical homogeneous linear system will almost certainly not
have a nonzero solution, and therefore a completely diﬀerent approach must be taken to ﬁnding
the corresponding eigenvector; see Sections 9.5 and 9.6.

8.2 Eigenvalues and Eigenvectors
411
Most 3 × 3 matrices have three diﬀerent eigenvalues, but this particular one has only two:
λ1 = 1, which is called a double eigenvalue, since it is a double root of the characteristic
equation, along with a simple eigenvalue λ2 = 2.
The eigenvector equation (8.13) for the double eigenvalue λ1 = 1 is
(A −I )v =
⎛
⎝
−1
−1
−1
1
1
1
1
1
1
⎞
⎠
⎛
⎝
x
y
z
⎞
⎠=
⎛
⎝
0
0
0
⎞
⎠.
The general solution to this homogeneous linear system
v =
⎛
⎝
−a −b
a
b
⎞
⎠= a
⎛
⎝
−1
1
0
⎞
⎠+ b
⎛
⎝
−1
0
1
⎞
⎠
depends upon two free variables: y = a and z = b. Every nonzero solution forms a valid
eigenvector for the eigenvalue λ1 = 1, and so the general eigenvector is any non-zero linear
combination of the two “basis eigenvectors” v1 = ( −1, 1, 0 )T , v1 = ( −1, 0, 1 )T .
On the other hand, the eigenvector equation for the simple eigenvalue λ2 = 2 is
(A −2 I )v =
⎛
⎝
−2
−1
−1
1
0
1
1
1
0
⎞
⎠
⎛
⎝
x
y
z
⎞
⎠=
⎛
⎝
0
0
0
⎞
⎠.
The general solution
v =
⎛
⎝
−a
a
a
⎞
⎠= a
⎛
⎝
−1
1
1
⎞
⎠
consists of all scalar multiples of the eigenvector v2 = ( −1, 1, 1 )T .
In summary, the eigenvalues and (basis) eigenvectors for this matrix are
λ1 = 1,
v1 =
⎛
⎝
−1
1
0
⎞
⎠,
v1 =
⎛
⎝
−1
0
1
⎞
⎠,
λ2 = 2,
v2 =
⎛
⎝
−1
1
1
⎞
⎠.
(8.15)
This means that every eigenvector for the simple eigenvalue λ2 = 2 is a nonzero scalar
multiple of v2, while every eigenvector for the double eigenvalue λ1 = 1 is a nontrivial
linear combination a v1 + b v1 of the two linearly independent eigenvectors v1, v1.
In general, given a real eigenvalue λ, the corresponding eigenspace Vλ ⊂Rn is the
subspace spanned by all its eigenvectors. Equivalently, the eigenspace is the kernel
Vλ = ker(A −λ I ).
(8.16)
Thus, λ ∈R is an eigenvalue if and only if Vλ ̸= {0} is a nontrivial subspace, and then
every nonzero element of Vλ is a corresponding eigenvector. The most economical way to
indicate each eigenspace is by writing out a basis, as in (8.15), with v1, v1 giving a basis for
the eigenspace V1, while v2 is a basis for the eigenspace V2. In particular, 0 is an eigenvalue
if and only if ker A ̸= {0}, and hence A is singular.

412
8 Eigenvalues and Singular Values
Proposition 8.7. A matrix is singular if and only if it has a zero eigenvalue.
Example 8.8.
The characteristic equation of the matrix A =
⎛
⎝
1
2
1
1
−1
1
2
0
1
⎞
⎠is
0 = det(A −λ I ) = −λ3 + λ2 + 5λ + 3 = −(λ + 1)2 (λ −3).
Again, there is a double eigenvalue λ1 = −1 and a simple eigenvalue λ2 = 3. However, in
this case the matrix
A −λ1 I = A + I =
⎛
⎝
2
2
1
1
0
1
2
0
2
⎞
⎠
has a one-dimensional kernel, spanned by v1 = ( 2, −1, −2 )T . Thus, even though λ1 is a
double eigenvalue, it admits only a one-dimensional eigenspace. The list of eigenvalues and
eigenvectors is, in a sense, incomplete:
λ1 = −1,
v1 =
⎛
⎝
2
−1
−2
⎞
⎠,
λ2 = 3,
v2 =
⎛
⎝
2
1
2
⎞
⎠.
Example 8.9.
Finally, the matrix A =
⎛
⎝
1
2
0
0
1
−2
2
2
−1
⎞
⎠has characteristic equation
0 = det(A −λ I ) = −λ3 + λ2 −3λ −5 = −(λ + 1) (λ2 −2λ + 5).
The linear factor yields the eigenvalue −1. The quadratic factor leads to two complex
roots, 1 + 2 i and 1 −2 i , which can be obtained via the quadratic formula. Hence A has
one real and two complex eigenvalues:
λ1 = −1,
λ2 = 1 + 2 i ,
λ3 = 1 −2 i .
On solving the associated linear system (A+ I )v = 0, the real eigenvalue λ1 = −1 is found
to have corresponding eigenvector v1 = ( −1, 1, 1 )T .
Complex eigenvalues are as important as real eigenvalues, and we need to be able to
handle them too. To ﬁnd the corresponding eigenvectors, which will also be complex, we
need to solve the usual eigenvalue equation (8.13), which is now a complex homogeneous
linear system. For example, the eigenvectors for λ2 = 1 + 2 i are found by solving

A −(1 + 2 i ) I

v =
⎛
⎝
−2 i
2
0
0
−2 i
−2
2
2
−2 −2 i
⎞
⎠
⎛
⎝
x
y
z
⎞
⎠=
⎛
⎝
0
0
0
⎞
⎠.
This linear system can be solved by Gaussian Elimination (with complex pivots). In this
case, a simpler strategy is to work directly: the ﬁrst equation −2 i x + 2y = 0 tells us that
y = i x, while the second equation −2 i y −2z = 0 says z = −i y = x. If we trust our
calculations so far, we do not need to solve the ﬁnal equation 2x + 2y + (−2 −2 i )z = 0,
since we know that the coeﬃcient matrix is singular and hence this equation must be
a consequence of the ﬁrst two. (However, it does serve as a useful check on our work.)
So, the general solution v = ( x, i x, x )T is an arbitrary constant multiple of the complex
eigenvector v2 = ( 1, i , 1 )T . The eigenvector equation for λ3 = 1 −2 i is similarly solved
for the third eigenvector v3 = ( 1, −i , 1 )T .

8.2 Eigenvalues and Eigenvectors
413
Summarizing, the matrix under consideration has one real and two complex eigenvalues,
with three corresponding eigenvectors, each unique up to (complex) scalar multiple:
λ1 = −1,
λ2 = 1 + 2 i ,
λ3 = 1 −2 i ,
v1 =
⎛
⎝
−1
1
1
⎞
⎠,
v2 =
⎛
⎝
1
i
1
⎞
⎠,
v3 =
⎛
⎝
1
−i
1
⎞
⎠.
Note that the third complex eigenvalue is the complex conjugate of the second, and the
eigenvectors are similarly related. This is indicative of a general fact for real matrices:
Proposition 8.10. If A is a real matrix with a complex eigenvalue λ = μ + i ν and
corresponding complex eigenvector v = x + i y, then the complex conjugate λ = μ −i ν is
also an eigenvalue with complex conjugate eigenvector v = x −i y.
Proof : First take complex conjugates of the eigenvalue equation (8.12):
A v = Av = λv = λ v.
Using the fact that a real matrix is unaﬀected by complex conjugation, A = A, we conclude
A v = λ v, which is the equation for the eigenvalue λ and eigenvector v ̸= 0.
Q.E.D.
As a consequence, when dealing with real matrices, we need to compute the eigenvectors
for only one of each complex conjugate pair of eigenvalues. This observation eﬀectively
halves the amount of work in the unfortunate event that we are confronted with complex
eigenvalues.
The eigenspace associated with a complex eigenvalue λ is the subspace Vλ ⊂Cn spanned
by the corresponding (complex) eigenvectors. One might also consider complex eigenvectors
associated with a real eigenvalue, but this doesn’t add anything to the picture — they are
merely complex linear combinations of the real eigenvectors. Thus, we need to introduce
complex eigenvectors only when dealing with genuinely complex eigenvalues.
Remark. The reader may recall that we said that one should never use determinants in
practical computations. So why have we reverted to using determinants to ﬁnd eigenvalues?
The truthful answer is that the practical computation of eigenvalues and eigenvectors never
resorts to the characteristic equation! The method is fraught with numerical traps and
ineﬃciencies when (a) computing the determinant leading to the characteristic equation,
then (b) solving the resulting polynomial equation, which is itself a nontrivial numerical
problem†, [8, 66], and, ﬁnally, (c) solving each of the resulting linear eigenvector systems.
Even worse, if we know only an approximation λ to the true eigenvalue λ, the approximate
eigenvector system (A −λ I )v = 0 will almost certainly have a nonsingular coeﬃcient
matrix, and hence admits only the trivial solution v = 0 — which does not even qualify
as an eigenvector!
Nevertheless, the characteristic equation does give us important theoretical insight into
the structure of the eigenvalues of a matrix, and can be used when dealing with very
small matrices, e.g., 2 × 2 and 3 × 3, presuming exact arithmetic is employed. Numerical
†
In fact, one eﬀective numerical strategy for ﬁnding the roots of a polynomial is to turn the
procedure on its head, and calculate the eigenvalues of a matrix whose characteristic equation is
the polynomial in question! See [66] for details.

414
8 Eigenvalues and Singular Values
algorithms for computing eigenvalues and eigenvectors are based on completely diﬀerent
ideas, and will be deferred until Sections 9.5 and 9.6.
Exercises
8.2.1. Find the eigenvalues and eigenvectors of the following matrices:
(a)

1
−2
−2
1
	
,
(b)
⎛
⎝1
−2
3
1
2
−1
6
⎞
⎠, (c)

3
1
−1
1
	
, (d)

1
2
−1
1
	
,
(e)
⎛
⎜
⎝
3
−1
0
−1
2
−1
0
−1
3
⎞
⎟
⎠, (f )
⎛
⎜
⎝
−1
−1
4
1
3
−2
1
1
−1
⎞
⎟
⎠, (g)
⎛
⎜
⎝
1
−3
11
2
−6
16
1
−3
7
⎞
⎟
⎠, (h)
⎛
⎜
⎝
2
−1
−1
−2
1
1
1
0
1
⎞
⎟
⎠,
(i)
⎛
⎜
⎝
−4
−4
2
3
4
−1
−3
−2
3
⎞
⎟
⎠, (j)
⎛
⎜
⎜
⎜
⎝
3
4
0
0
4
3
0
0
0
0
1
3
0
0
4
5
⎞
⎟
⎟
⎟
⎠, (k)
⎛
⎜
⎜
⎜
⎝
4
0
0
0
1
3
0
0
−1
1
2
0
1
−1
1
1
⎞
⎟
⎟
⎟
⎠.
8.2.2.(a) Find the eigenvalues of the rotation matrix Rθ =

cos θ
−sin θ
sin θ
cos θ
	
. For what values
of θ are the eigenvalues real?
(b) Explain why your answer gives an immediate solution to Exercise 1.5.7c.
8.2.3. Answer Exercise 8.2.2a for the reﬂection matrix Fθ =

cos θ
sin θ
sin θ
−cos θ
	
.
8.2.4. Write down (a) a 2 × 2 matrix that has 0 as one of its eigenvalues and ( 1, 2 )T as a
corresponding eigenvector; (b) a 3 × 3 matrix that has ( 1, 2, 3 )T as an eigenvector for the
eigenvalue −1.
8.2.5.(a) Write out the characteristic equation for the matrix
⎛
⎜
⎝
0
1
0
0
0
1
α
β
γ
⎞
⎟
⎠.
(b) Show that, given any 3 numbers a, b, and c, there is a 3 × 3 matrix with characteristic
equation −λ3 + aλ2 + bλ + c = 0.
8.2.6. Find the eigenvalues and eigenvectors of the cross product matrix A =
⎛
⎜
⎝
0
c
−b
−c
0
a
b
−a
0
⎞
⎟
⎠.
8.2.7. Find all eigenvalues and eigenvectors of the following complex matrices:
(a)

i
1
0 −1 + i
	
, (b)

2
i
−i
−2
	
, (c)

i −2
i + 1
i + 2
i −1
	
, (d)
⎛
⎜
⎝
1 + i
−1 −i
1 −i
2
−2 −i
2 −2 i
−1
1 + i
−1 + 2 i
⎞
⎟
⎠.
8.2.8. Find all eigenvalues and eigenvectors of
(a) the n × n zero matrix O;
(b) the n × n identity matrix I .
8.2.9. Find the eigenvalues and eigenvectors of an n × n matrix with every entry equal to 1.
Hint: Try with n = 2, 3, and then generalize.
♦8.2.10. Let A be a given square matrix. (a) Explain in detail why every nonzero scalar
multiple of an eigenvector of A is also an eigenvector. (b) Show that every nonzero linear
combination of two eigenvectors v, w corresponding to the same eigenvalue is also an
eigenvector. (c) Prove that a linear combination cv + dw, with c, d ̸= 0, of two eigenvectors
corresponding to diﬀerent eigenvalues is never an eigenvector.
♦8.2.11. Let λ be a real eigenvalue of the real n × n matrix A, and v1, . . . , vk a basis for the
associated eigenspace Vλ. Suppose w ∈Cn is a complex eigenvector, so Aw = λw. Prove
that w = c1 v1 + · · · + ck vk is a complex linear combination of the real eigenspace basis.
Hint: Look at the real and imaginary parts of the eigenvector equation.

8.2 Eigenvalues and Eigenvectors
415
8.2.12. True or false: If v is a real eigenvector of a real matrix A, then a nonzero complex
multiple w = cv for c ∈C is a complex eigenvector of A.
♦8.2.13. Deﬁne the shift map S: Cn →Cn by S(v1, v2, . . . , vn−1, vn)T = (v2, v3, . . . , vn, v1)T .
(a) Prove that S is a linear map, and write down its matrix representation A.
(b) Prove that A is an orthogonal matrix. (c) Prove that the sampled exponential vectors
ω0, . . . , ωn−1 deﬁned in (5.102) form an eigenvector basis of A. What are the eigenvalues?
Basic Properties of Eigenvalues
If A is an n × n matrix, then its characteristic polynomial is deﬁned to be
pA(λ) = det(A −λ I ) = cn λn + cn−1 λn−1 + · · · + c1 λ + c0.
(8.17)
The fact that pA(λ) is a polynomial of degree n is a consequence of the general determinan-
tal formula (1.87). Indeed, every term is prescribed by a permutation π of the rows of the
matrix, and equals plus or minus a product of n distinct matrix entries including one from
each row and one from each column. The term corresponding to the identity permutation
is obtained by multiplying the diagonal entries together, which, in this case, is
(a11 −λ) (a22 −λ) · · · (ann −λ) = (−1)nλn + (−1)n−1
a11 + a22 + · · · + ann

λn−1 + · · · .
(8.18)
All of the other terms have at most n −2 diagonal factors aii −λ, and so are polynomials
of degree ≤n −2 in λ. Thus, (8.18) is the only summand containing the monomials λn
and λn−1, and so their respective coeﬃcients are
cn = (−1)n,
cn−1 = (−1)n−1(a11 + a22 + · · · + ann) = (−1)n−1 tr A,
(8.19)
where tr A, the sum of its diagonal entries, is called the trace of the matrix A. The other
coeﬃcients cn−2, . . . , c1, c0 in (8.17) are more complicated combinations of the entries of
A. However, setting λ = 0 implies
pA(0) = c0 = det A,
(8.20)
and hence the constant term in the characteristic polynomial equals the determinant of
the matrix. In particular, if A =

a
b
c
d

is a 2 × 2 matrix, its characteristic polynomial
has the explicit form
pA(λ) = det(A −λ I ) = det

a −λ
b
c
d −λ

= λ2 −(a + d)λ + (ad −bc) = λ2 −(tr A)λ + (det A).
(8.21)
According to the Fundamental Theorem of Algebra, [26], every (complex) polynomial of
degree n ≥1 can be completely factored, and so we can write the characteristic polynomial
(8.17) in factored form:
pA(λ) = (−1)n(λ −λ1) (λ −λ2) · · · (λ −λn).
(8.22)
The complex numbers λ1, . . . , λn, some of which may be repeated, are the roots of the
characteristic equation pA(λ) = 0, and hence the eigenvalues of the matrix A. Therefore,
we immediately conclude:
Theorem 8.11. An n × n matrix possesses at least one and at most n distinct complex
eigenvalues.

416
8 Eigenvalues and Singular Values
Most n×n matrices — meaning those for which the characteristic polynomial factors into
n distinct factors — have exactly n complex eigenvalues. More generally, an eigenvalue
λj is said to have multiplicity k if the factor (λ −λj) appears exactly k times in the
factorization (8.22) of the characteristic polynomial.
An eigenvalue is simple if it has
multiplicity 1, double if it has multiplicity 2, and so on. In particular, A has n distinct
eigenvalues if and only if all its eigenvalues are simple. In all cases, when the repeated
eigenvalues are counted in accordance with their multiplicity, every n × n matrix has a
total of n complex eigenvalues.
An example of a matrix with just one eigenvalue, of multiplicity n, is the n × n identity
matrix I , whose only eigenvalue is λ = 1. In this case, every nonzero vector in Rn is an
eigenvector of the identity matrix (why?), and so the eigenspace V1 is all of Rn. At the
other extreme, the n × n “bidiagonal” Jordan block matrix †
Ja,n =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎝
a
1
a
1
a
1
...
...
a
1
a
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎠
(8.23)
also has only one eigenvalue, λ = a, again of multiplicity n. But in this case, Ja,n has only
one eigenvector (up to scalar multiple), which is the ﬁrst standard basis vector e1, and so
its eigenspace is one-dimensional.
Remark. If λ is a complex eigenvalue of multiplicity k for the real matrix A, then its
complex conjugate λ also has multiplicity k. This is because complex conjugate roots of a
real polynomial necessarily appear with identical multiplicities.
Remark. If n ≤4, then one can, in fact, write down an explicit formula for the solution
to a polynomial equation of degree n, and hence explicit (but rather complicated and not
particularly helpful) formulas for the eigenvalues of general 2×2, 3×3, and 4×4 matrices.
As soon as n ≥5, there is no explicit formula (at least in terms of radicals), and so one
must usually resort to numerical approximations.
This remarkable and deep algebraic
result was proved in the early nineteenth century by the young Norwegian mathematician
Niels Henrik Abel, [26].
Proposition 8.12. A square matrix A and its transpose AT have the same characteristic
equation, and hence the same eigenvalues with the same multiplicities.
Proof : This follows immediately from Proposition 1.56, that the determinant of a matrix
and its transpose are identical. Thus,
pA(λ) = det(A −λ I ) = det(A −λ I )T = det(AT −λ I ) = pAT (λ).
Q.E.D.
Remark. While AT has the same eigenvalues as A, its eigenvectors are, in general, dif-
ferent. An eigenvector v of AT , satisfying AT v = λv, is sometimes referred to as a left
eigenvector of A, since it satisﬁes vT A = λvT . A more apt, albeit rather non-conventional,
name for v that conforms with our nomenclature conventions would be co-eigenvector.
†
All non-displayed entries are zero.

8.2 Eigenvalues and Eigenvectors
417
If we explicitly multiply out the factored product (8.22) and equate the result to the
characteristic polynomial (8.17), we ﬁnd that its coeﬃcients c0, c1, . . . cn−1 can be written
as certain polynomials of the roots, known as the elementary symmetric polynomials. The
ﬁrst and last are of particular importance:
c0 = λ1 λ2 · · · λn,
cn−1 = (−1)n−1 (λ1 + λ2 + · · · + λn).
(8.24)
Comparison with our previous formulas (8.19, 20) for the coeﬃcients c0 and cn−1 leads to
the following useful result.
Proposition 8.13. The sum of the eigenvalues of a square matrix A equals its trace:
λ1 + λ2 + · · · + λn = tr A = a11 + a22 + · · · + ann.
(8.25)
The product of the eigenvalues equals its determinant:
λ1 λ2 · · · λn = det A.
(8.26)
Keep in mind that, in evaluating (8.25, 26), one must add or multiply repeated eigen-
values according to their multiplicity.
Example 8.14.
The matrix A =
⎛
⎝
1
2
1
1
−1
1
2
0
1
⎞
⎠considered in Example 8.8 has trace
and determinant
tr A = 1,
det A = 3,
which ﬁx, respectively, the coeﬃcient of λ2 and the constant term in its characteristic
equation. This matrix has two distinct eigenvalues: −1, which is a double eigenvalue, and
3, which is simple. For this particular matrix, (8.25, 26) become
1 = tr A = (−1) + (−1) + 3,
3 = det A = (−1)(−1) 3.
Note that the double eigenvalue contributes twice to both the sum and the product.
Exercises
8.2.14.(a) Compute the eigenvalues and corresponding eigenvectors of A =
⎛
⎜
⎝
1
4
4
3
−1
0
0
2
3
⎞
⎟
⎠.
(b) Compute the trace of A and check that it equals the sum of the eigenvalues. (c) Find
the determinant of A and check that it is equal to to the product of the eigenvalues.
8.2.15. Verify the trace and determinant formulas (8.25, 26) for the matrices in Exercise 8.2.1.
8.2.16.(a) Find the explicit formula for the characteristic polynomial
det(A −λ I ) = −λ3 + aλ2 −bλ + c of a general 3 × 3 matrix. Verify that a = tr A,
c = det A. What is the formula for b? (b) Prove that if A has eigenvalues λ1, λ2, λ3, then
a = tr A = λ1 + λ2 + λ3, b = λ1 λ2 + λ1 λ3 + λ2 λ3, c = det A = λ1 λ2 λ3.
8.2.17. Prove that the eigenvalues of an upper triangular (or lower triangular) matrix are its
diagonal entries.
♦8.2.18. Let Ja,n be the n × n Jordan block matrix (8.23). Prove that its only eigenvalue is
λ = a and the only eigenvectors are the nonzero scalar multiples of the standard basis
vector e1.

418
8 Eigenvalues and Singular Values
♦8.2.19. Suppose that λ is an eigenvalue of A. (a) Prove that cλ is an eigenvalue of the scalar
multiple cA. (b) Prove that λ + d is an eigenvalue of A + d I . (c) More generally, cλ + d is
an eigenvalue of B = c A + d I for scalars c, d.
8.2.20. Show that if λ is an eigenvalue of A, then λ2 is an eigenvalue of A2.
8.2.21. True or false: (a) If λ is an eigenvalue of both A and B, then it is an eigenvalue of the
sum A + B. (b) If v is an eigenvector of both A and B, then it is an eigenvector of A + B.
8.2.22. True or false: If λ is an eigenvalue of A and μ is an eigenvalue of B, then λμ is an
eigenvalue of the matrix product C = AB.
♦8.2.23. Let A and B be n × n matrices. Prove that the matrix products AB and B A have the
same eigenvalues. Hint: How should the eigenvectors be related?
♦8.2.24.(a) Prove that if λ ̸= 0 is a nonzero eigenvalue of the nonsingular matrix A, then 1/λ is
an eigenvalue of A−1.
(b) What happens if A has 0 as an eigenvalue?
♦8.2.25.(a) Prove that if | det A | > 1, then A has at least one eigenvalue with | λ | > 1.
(b) If | det A | < 1, are all eigenvalues | λ | < 1? Prove or ﬁnd a counterexample.
8.2.26. Prove that A is a singular matrix if and only if 0 is an eigenvalue.
8.2.27. Prove that every nonzero vector 0 ̸= v ∈Rn is an eigenvector of A if and only if A is a
scalar multiple of the identity matrix.
8.2.28. How many unit (norm 1) eigenvectors correspond to a given eigenvalue of a matrix?
8.2.29. True or false: (a) Performing an elementary row operation of type #1 does not change
the eigenvalues of a matrix. (b) Interchanging two rows of a matrix changes the sign
of its eigenvalues. (c) Multiplying one row of a matrix by a scalar multiplies one of its
eigenvalues by the same scalar.
8.2.30.(a) True or false: If λ1, v1 and λ2, v2 solve the eigenvalue equation (8.12) for a given
matrix A, so does λ1 + λ2, v1 + v2.
(b) Explain what this has to do with linearity.
8.2.31. As in (4.35), an elementary reﬂection matrix has the form Q = I −2uuT , where
u ∈Rn is a unit vector. (a) Find the eigenvalues and eigenvectors of the elementary
reﬂection matrices for the unit vectors (i)

1
0
	
, (ii)
⎛
⎝
3
5
4
5
⎞
⎠, (iii)
⎛
⎜
⎝
0
1
0
⎞
⎟
⎠,
(iv)
⎛
⎜
⎜
⎜
⎜
⎝
1
√
2
0
−1
√
2
⎞
⎟
⎟
⎟
⎟
⎠.
(b) What are the eigenvalues and eigenvectors of a general elementary reﬂection matrix?
♦8.2.32. Let A and B be similar matrices, so B = S−1 AS for some nonsingular matrix S.
(a) Prove that A and B have the same characteristic polynomial: pB(λ) = pA(λ).
(b) Explain why similar matrices have the same eigenvalues. (c) Do they have the same
eigenvectors? If not, how are their eigenvectors related? (d) Prove that the converse to part
(c) is false by showing that

2
0
0
2
	
and

1
1
−1
3
	
have the same eigenvalues, but are not
similar.
8.2.33. Let A be a nonsingular n × n matrix with characteristic polynomial pA(λ).
(a) Explain how to construct the characteristic polynomial pA−1(λ) of its inverse directly
from pA(λ).
(b) Check your result when A = (i)

1
2
3
4
	
, (ii)
⎛
⎜
⎝
1
4
4
−2
−1
0
0
2
3
⎞
⎟
⎠.
♦8.2.34. Prove that the only eigenvalue of a nilpotent matrix, cf. Exercise 1.3.12, is 0.
(The converse is also true; see Exercise 8.6.20.)

8.2 Eigenvalues and Eigenvectors
419
8.2.35. Given an idempotent matrix, so that P = P 2, ﬁnd all its eigenvalues and eigenvectors.
8.2.36.(a) Prove that every real 3 × 3 matrix has at least one real eigenvalue.
(b) Find a real 4 × 4 matrix with no real eigenvalues.
(c) Can you ﬁnd a real 5 × 5 matrix with no real eigenvalues?
8.2.37.(a) Show that if A is a matrix such that A4 = I , then the only possible eigenvalues of
A are 1, −1, i , and −i .
(b) Give an example of a real matrix that has all four numbers as eigenvalues.
8.2.38.(a) Prove that if λ is an eigenvalue of A, then λn is an eigenvalue of An.
(b) State and
prove a converse.
8.2.39. True or false: All the eigenvalues of an n × n permutation matrix are real.
8.2.40.(a) Show that if all the row sums of A are equal to 1, then A has 1 as an eigenvalue.
(b) Suppose all the column sums of A are equal to 1. Does the same result hold?
♦8.2.41. Prove that if v is an eigenvector of A with eigenvalue λ and w is an eigenvector of AT
with a diﬀerent eigenvalue μ ̸= λ, then v and w are orthogonal vectors with respect to the
dot product. Illustrate this result when (i) A =

0
−1
2
3
	
, (ii) A =
⎛
⎜
⎝
5
−4
2
5
−4
1
−2
2
−3
⎞
⎟
⎠.
8.2.42. Let Q be an orthogonal matrix. (a) Prove that if λ is an eigenvalue, then so is 1/λ.
(b) Prove that all its eigenvalues are complex numbers of modulus | λ | = 1. In particular,
the only possible real eigenvalues of an orthogonal matrix are ±1.
(c) Suppose v = x + i y
is a complex eigenvector corresponding to a non-real eigenvalue.
Prove that its real and
imaginary parts are orthogonal vectors having the same Euclidean norm.
♦8.2.43.(a) Prove that every 3 × 3 proper orthogonal matrix has +1 as an eigenvalue.
(b) True or false: An improper 3 × 3 orthogonal matrix has −1 as an eigenvalue.
♦8.2.44.(a) Show that the linear transformation deﬁned by a 3 × 3 proper orthogonal matrix
corresponds to rotating through an angle around a line through the origin in R3 — the axis
of the rotation. Hint: Use Exercise 8.2.43(a).
(b) Find the axis and angle of rotation of the orthogonal matrix
⎛
⎜
⎜
⎜
⎝
3
5
0
4
5
−4
13
12
13
3
13
−48
65
−5
13
36
65
⎞
⎟
⎟
⎟
⎠.
♦8.2.45. Prove that every proper aﬃne isometry F(x) = Q x + b of R3, where det Q = +1, is one
of the following: (a) a translation x + b, (b) a rotation centered at some point of R3, or
(c) a screw motion consisting of a rotation around an axis followed by a translation in the
direction of the axis. Hint: Use Exercise 8.2.44.
8.2.46. Suppose Q is an orthogonal matrix. (a) Prove that K = 2 I −Q −QT is a positive
semi-deﬁnite matrix.
(b) Under what conditions is K > 0?
♥8.2.47. Let Mn be the n × n tridiagonal matrix whose diagonal entries are all equal to 0
and whose sub- and super-diagonal entries all equal 1. (a) Find the eigenvalues and
eigenvectors of M2 and M3 directly. (b) Prove that the eigenvalues and eigenvectors of Mn
are explicitly given by
λk = 2 cos
kπ
n + 1 ,
vk =

sin
kπ
n + 1 ,
sin 2kπ
n + 1 ,
. . .
sin nkπ
n + 1
	T
,
k = 1, . . . , n.
How do you know that there are no other eigenvalues?
♦8.2.48. Let a, b ∈R. Determine the eigenvalues and eigenvectors of the n × n tridiagonal
matrix with all diagonal entries equal to a and all sub- and super-diagonal entries equal to
b. Hint: See Exercises 8.2.19 and 8.2.47.

420
8 Eigenvalues and Singular Values
♥8.2.49. Find a formula for the eigenvalues of the tricirculant n × n matrix Zn that has 1’s on
the sub- and super-diagonals as well as its (1, n) and (n, 1) entries, while all other entries
are 0. Hint: Use Exercise 8.2.47 as a guide.
♦8.2.50. Let A be an n × n matrix with eigenvalues λ1, . . . , λk, and B an m × m matrix with
eigenvalues μ1, . . . , μl. Show that the (m+n)×(m+n) block diagonal matrix D =

A
O
O
B
	
has eigenvalues λ1, . . . , λk, μ1, . . . , μl and no others. How are the eigenvectors related?
♥8.2.51. Deﬂation: Suppose A has eigenvalue λ and corresponding eigenvector v. (a) Let b be
any vector. Prove that the matrix B = A −vbT also has v as an eigenvector, now with
eigenvalue λ −β, where β = v · b. (b) Prove that if μ ̸= λ −β is any other eigenvalue of A,
then it is also an eigenvalue of B. Hint: Look for an eigenvector of the form w + cv, where
w is an eigenvector of A. (c) Given a nonsingular matrix A with eigenvalues λ1, λ2, . . . , λn
and λ1 ̸= λj for all j ≥2, explain how to construct a deﬂated matrix B whose eigenvalues
are 0, λ2, . . . , λn. (d) Try out your method on the matrices

3
3
1
5
	
and
⎛
⎜
⎝
3
−1
0
−1
2
−1
0
−1
3
⎞
⎟
⎠.
♥8.2.52. Let A =

a
b
c
d
	
be a 2 × 2 matrix. (a) Prove that A satisﬁes its own characteristic
equation, meaning pA(A) = A2 −(tr A) A + (det A) I = O.
Remark. This result is a
special case of the Cayley–Hamilton Theorem, to be developed in Exercise 8.6.22.
(b) Prove the inverse formula A−1 = (tr A) I −A
det A
when det A ̸= 0.
(c) Check the Cayley–Hamilton and inverse formulas when A =

2
1
−3
2
	
.
The Gershgorin Circle Theorem
In general, precisely computing the eigenvalues of a matrix is not easy, and, in most
cases, must be done through a numerical eigenvalue procedure; see Sections 9.5 and 9.6.
In certain applications, though, we may not require exact numerical values, but only their
approximate locations. The Gershgorin Circle Theorem, due to the early-twentieth-century
Russian mathematician Semyon Gershgorin, serves to restrict the eigenvalues to a certain
well-deﬁned region in the complex plane.
Deﬁnition 8.15. Let A be an n × n matrix, either real or complex. For each 1 ≤i ≤n,
deﬁne the ith Gershgorin disk
Di = { | z −aii | ≤ri | z ∈C } ,
where
ri =
n

j=1
j̸=i
| aij |.
(8.27)
The Gershgorin domain DA =
n

i=1
Di ⊂C is the union of the Gershgorin disks.
Thus, the ith Gershgorin disk Di is centered at the ith diagonal entry aii of A, and has
radius ri equal to the sum of the absolute values of the oﬀ-diagonal entries that are in its
ith row. We can now state the Gershgorin Circle Theorem.
Theorem 8.16. All real and complex eigenvalues of the matrix A lie in its Gershgorin
domain DA ⊂C.

8.2 Eigenvalues and Eigenvectors
421













D1
D2
D3
λ1
λ2
λ3
Figure 8.2.
Gershgorin Disks and Eigenvalues.
Example 8.17.
The matrix A =
⎛
⎝
2
−1
0
1
4
−1
−1
−1
−3
⎞
⎠has Gershgorin disks
D1 = { | z −2 | ≤1 } ,
D2 = { | z −4 | ≤2 } ,
D3 = { | z + 3 | ≤2 } ,
which are plotted in Figure 8.2. The eigenvalues of A are
λ1 = 3,
λ2 =
√
10 = 3.1622 . . . ,
λ3 = −
√
10 = −3.1622 . . . .
Observe that λ1 belongs to both D1 and D2, while λ2 lies in D2, and λ3 is in D3. We thus
conﬁrm that all three eigenvalues are in the Gershgorin domain DA = D1 ∪D2 ∪D3.
Proof of Theorem 8.16:
Let v be an eigenvector of A with eigenvalue λ. Let u = v/∥v∥∞
be the corresponding unit eigenvector with respect to the ∞norm, so
∥u∥∞= max

| u1 |, . . . , | un |

= 1.
Let ui be an entry of u that achieves the maximum: | ui | = 1.
Writing out the ith
component of the eigenvalue equation Au = λ u, we obtain
n

j =1
aij uj = λ ui,
which we rewrite as

j̸=i
aij uj = (λ −aii) ui.
Therefore, since all | uj | ≤1, while | ui | = 1,
| λ −aii | = | λ −aii | | ui | = | (λ −aii)ui | =


j̸=i
aij uj

≤

j̸=i
| aij | | uj | ≤

j̸=i
| aij | = ri.
This immediately implies that λ ∈Di ⊂DA belongs to the ith Gershgorin disk.
Q.E.D.
According to Proposition 8.7, a matrix A is singular if and only if it admits zero as an
eigenvalue. Thus, if its Gershgorin domain does not contain 0, it cannot be an eigenvalue,
and hence A is necessarily invertible. The condition 0 ̸∈DA requires that the matrix have
large diagonal entries, as quantiﬁed by the following deﬁnition.
Deﬁnition 8.18. A square matrix A is called strictly diagonally dominant if
| aii | >

j̸=i
| aij |,
for all
i = 1, . . . , n.
(8.28)

422
8 Eigenvalues and Singular Values
In other words, strict diagonal dominance requires each diagonal entry to be larger, in
absolute value, than the sum of the absolute values of all the other entries in its row. For
example, the matrix
⎛
⎝
3
−1
1
1
−4
2
−2
−1
5
⎞
⎠is strictly diagonally dominant since
| 3 | > | −1 | + | 1 |,
| −4 | > | 1 | + | 2 |,
| 5 | > | −2 | + | −1 |.
Diagonally dominant matrices appear frequently in numerical solution methods for both
ordinary and partial diﬀerential equations.
Theorem 8.19. A strictly diagonally dominant matrix is nonsingular.
Proof : The diagonal dominance inequalities (8.28) imply that the radius of the ith Gersh-
gorin disk is strictly less than the modulus of its center: ri < | aii |. This implies that the
disk cannot contain 0; indeed, if z ∈Di, then, by the triangle inequality,
ri > | z −aii | ≥| aii | −| z | > ri −| z |,
and hence
| z | > 0.
Thus, 0 does not lie in the Gershgorin domain DA, and so cannot be an eigenvalue. Q.E.D.
Warning. The converse to this result is obviously not true; there are plenty of nonsingular
matrices that are not strictly diagonally dominant.
Exercises
8.2.53. For each of the following matrices,
(i) ﬁnd all Gershgorin disks; (ii) plot the Gershgorin domain in the complex plane;
(iii) compute the eigenvalues and conﬁrm the truth of the Circle Theorem 8.16:
(a)

1
−2
−2
1
	
,
(b)
⎛
⎝1
−2
3
1
2
−1
6
⎞
⎠,
(c)

2
3
−1
0
	
,
(d)
⎛
⎜
⎝
3
−1
0
−1
2
−1
0
−1
3
⎞
⎟
⎠,
(e)
⎛
⎜
⎝
−1
1
1
2
2
−1
0
3
−4
⎞
⎟
⎠,
(f )
⎛
⎜
⎜
⎝
1
2
0
0
0
0
1
3
1
4
1
6
0
⎞
⎟
⎟
⎠,
(g)
⎛
⎜
⎝
0
1
0
0
1
1
0
−1
1
⎞
⎟
⎠,
(h)
⎛
⎜
⎜
⎜
⎝
3
2
0
0
1
2
0
0
0
0
0
1
0
0
2
1
⎞
⎟
⎟
⎟
⎠.
8.2.54. True or false: The Gershgorin domain of the transpose of a matrix AT is the same as
the Gershgorin domain of the matrix A, that is, DAT = DA.
♦8.2.55.(i) Explain why the eigenvalues of A must lie in its reﬁned Gershgorin domain
D∗
A = DAT ∩DA.
(ii) Find the reﬁned Gershgorin domains for each of the matrices in
Exercise 8.2.53 and conﬁrm the result in part (i).
8.2.56. True or false: (a) A positive deﬁnite matrix is strictly diagonally dominant.
(b) A strictly diagonally dominant matrix is positive deﬁnite.
♦8.2.57. Prove that if K is symmetric, strictly diagonally dominant, and each diagonal entry is
positive, then K is positive deﬁnite.
8.2.58.(a) Write down an invertible matrix A whose Gershgorin domain contains 0.
(b) Can you ﬁnd an example that is also strictly diagonally dominant?

8.3 Eigenvector Bases
423
8.3 Eigenvector Bases
Most of the vector space bases that play a distinguished role in applications are assembled
from the eigenvectors of a particular matrix. In this section, we show that the eigenvectors
of a “complete” matrix automatically form a basis for Rn, or, in the complex case, Cn. In
the following subsection, we use the eigenvector basis to rewrite the linear transformation
determined by the matrix in a simple diagonal form, hence the alternative more common
term “diagonalizable” for such matrices.
The most important cases — symmetric and
positive deﬁnite matrices — will be treated in the following section.
The ﬁrst task is to show that eigenvectors corresponding to distinct eigenvalues are
automatically linearly independent.
Lemma 8.20. If λ1, . . . , λk are distinct eigenvalues of a matrix A, so λi ̸= λj when i ̸= j,
then the corresponding eigenvectors v1, . . ., vk are linearly independent.
Proof : The result is proved by induction on the number of eigenvalues. The case k = 1 is
immediate, since an eigenvector cannot be zero. Assume that we know that the result is
valid for k −1 eigenvalues. Suppose we have a vanishing linear combination:
c1 v1 + · · · + ck−1 vk−1 + ck vk = 0.
(8.29)
Let us multiply this equation by the matrix A:
A

c1 v1 + · · · + ck−1 vk−1 + ck vk

= c1 Av1 + · · · + ck−1 Avk−1 + ck Avk
= c1 λ1 v1 + · · · + ck−1 λk−1 vk−1 + ck λk vk = 0.
On the other hand, if we multiply the original equation (8.29) by λk, we also have
c1 λk v1 + · · · + ck−1 λk vk−1 + ck λk vk = 0.
On subtracting this from the previous equation, the ﬁnal terms cancel, and we are left with
the equation
c1(λ1 −λk)v1 + · · · + ck−1(λk−1 −λk)vk−1 = 0.
This is a vanishing linear combination of the ﬁrst k−1 eigenvectors, and so, by our induction
hypothesis, can happen only if all the coeﬃcients are zero:
c1(λ1 −λk) = 0,
. . .
ck−1(λk−1 −λk) = 0.
The eigenvalues were assumed to be distinct, and consequently c1 = · · · = ck−1 = 0.
Substituting these values back into (8.29), we ﬁnd that ck vk = 0, and so ck = 0 also,
since the eigenvector vk ̸= 0. Thus we have proved that (8.29) holds if and only if c1 =
· · · = ck = 0, which implies the linear independence of the eigenvectors v1, . . . , vk. This
completes the induction step.
Q.E.D.
The most important consequence of this result concerns when a matrix has the maximum
allotment of eigenvalues.
Theorem 8.21. If the n × n real matrix A has n distinct real eigenvalues λ1, . . . , λn,
then the corresponding real eigenvectors v1, . . . , vn form a basis of Rn. If A (which may
now be either a real or a complex matrix) has n distinct complex eigenvalues, then the
corresponding eigenvectors v1, . . . , vn form a basis of Cn.
For instance, the 2 × 2 matrix in Example 8.5 has two distinct real eigenvalues, and its
two independent eigenvectors form a basis of R2. The 3 × 3 matrix in Example 8.9 has

424
8 Eigenvalues and Singular Values
three distinct complex eigenvalues, and its eigenvectors form a basis for C3. If a matrix
has multiple eigenvalues, then there may or may not be an eigenvector basis of Rn (or Cn).
The matrix in Example 8.6 admits an eigenvector basis, whereas the matrix in Example 8.8
does not. In general, it can be proved† that the dimension of an eigenspace is less than or
equal to the corresponding eigenvalue’s multiplicity. In particular, every simple eigenvalue
has a one-dimensional eigenspace, and hence, up to scalar multiple, only one associated
eigenvector.
Deﬁnition 8.22. An eigenvalue λ of a matrix A is called complete if the corresponding
eigenspace Vλ = ker(A −λ I ) has the same dimension as its multiplicity. The matrix A is
said to be complete if all its eigenvalues are complete.
Note that a simple eigenvalue is automatically complete, since its eigenspace is the one-
dimensional subspace or eigenline spanned by the corresponding eigenvector. Thus, only
multiple eigenvalues can cause a matrix to be incomplete.
Remark. The multiplicity of an eigenvalue λi is sometimes referred to as its algebraic
multiplicity.
The dimension of the eigenspace Vλ is its geometric multiplicity, and so
completeness requires that the two multiplicities be equal. The word “complete” is not
standard, but has been chosen because it can be used to describe both matrices and their
individual eigenvalues. Alternative terms used to describe complete matrices include per-
fect, semi-simple, and, as we discuss shortly, diagonalizable.
Theorem 8.23. An n×n real or complex matrix A is complete if and only if its eigenvec-
tors span Cn. In particular, an n × n matrix that has n distinct eigenvalues is complete.
Or, stated another way, a matrix is complete if and only if its eigenvectors form a basis
of Cn. Most matrices are complete. Incomplete n × n matrices, which have fewer than n
linearly independent complex eigenvectors, are less pleasant to deal with, and we relegate
most of the messy details to Section 8.6.
Remark. We already noted that complex eigenvectors of a real matrix always appear
in conjugate pairs: v = x ± i y. If the matrix is complete, then it can be shown that
its real eigenvectors combined with the real and imaginary parts of its complex conjugate
eigenvectors form a real basis for Rn. (See Exercise 8.3.12 for the underlying principle.)
For instance, the complex eigenvectors of the 3 × 3 matrix appearing in Example 8.9 are
⎛
⎝
1
0
1
⎞
⎠± i
⎛
⎝
0
1
0
⎞
⎠. The vectors
⎛
⎝
−1
1
1
⎞
⎠,
⎛
⎝
1
0
1
⎞
⎠,
⎛
⎝
0
1
0
⎞
⎠, consisting of its real eigenvector
and the real and imaginary parts of its complex eigenvectors, form a basis for R3.
Exercises
8.3.1. Which of the following are complete eigenvalues for the indicated matrix? What is the
dimension of the associated eigenspace?
(a) 3,

2
1
1
2
	
;
(b) 2,

2
1
0
2
	
;
†
This follows from the Jordan Canonical Form Theorem 8.51.

8.3 Eigenvector Bases
425
(c) 1,
⎛
⎜
⎝
0
0
−1
1
1
0
1
0
0
⎞
⎟
⎠;
(d) 1,
⎛
⎜
⎝
1
1
−1
1
1
0
1
1
2
⎞
⎟
⎠;
(e) −1,
⎛
⎜
⎝
−1
−4
−4
0
−1
0
0
4
3
⎞
⎟
⎠;
(f ) −i ,
⎛
⎜
⎝
−i
1
0
−i
1
−1
0
0
−i
⎞
⎟
⎠; (g) −2,
⎛
⎜
⎜
⎜
⎝
1
0
−1
1
0
1
0
1
−1
1
1
0
1
0
0
1
⎞
⎟
⎟
⎟
⎠; (h) 1,
⎛
⎜
⎜
⎜
⎜
⎜
⎝
1
−1
−1
−1
−1
0
1
−1
−1
−1
0
0
1
−1
−1
0
0
0
1
−1
0
0
0
0
1
⎞
⎟
⎟
⎟
⎟
⎟
⎠
.
8.3.2. Find the eigenvalues and a basis for the each of the eigenspaces of the following matrices.
Which are complete?
(a)

4
−4
1
0
	
, (b)

6
−8
4
−6
	
, (c)

3
−2
4
−1
	
, (d)

i
−1
1
i
	
, (e)
⎛
⎜
⎝
4
−1
−1
0
3
0
1
−1
2
⎞
⎟
⎠,
(f )
⎛
⎜
⎝
−6
0
−8
−4
2
−4
4
0
6
⎞
⎟
⎠, (g)
⎛
⎜
⎝
−2
1
−1
5
−3
6
5
−1
4
⎞
⎟
⎠, (h)
⎛
⎜
⎜
⎜
⎝
1
0
0
0
0
1
0
0
−1
1
−1
0
1
0
−1
0
⎞
⎟
⎟
⎟
⎠, (i)
⎛
⎜
⎜
⎜
⎝
−1
0
1
2
0
1
0
1
−1
−4
1
−2
0
1
0
1
⎞
⎟
⎟
⎟
⎠.
8.3.3. Which of the following matrices admit eigenvector bases of Rn? For those that do,
exhibit such a basis. If not, what is the dimension of the subspace of Rn spanned by the
eigenvectors?
(a)

1
3
3
1
	
, (b)

1
3
−3
1
	
, (c)

1
3
0
1
	
, (d)
⎛
⎜
⎝
1
−2
0
0
−1
0
4
−4
−1
⎞
⎟
⎠,
(e)
⎛
⎜
⎝
1
−2
0
0
−1
0
0
−4
−1
⎞
⎟
⎠, (f )
⎛
⎜
⎝
2
0
0
1
−1
1
2
1
−1
⎞
⎟
⎠, (g)
⎛
⎜
⎝
0
0
−1
0
1
0
1
0
0
⎞
⎟
⎠, (h)
⎛
⎜
⎜
⎜
⎝
0
0
−1
1
0
−1
0
1
1
0
−1
0
1
0
1
0
⎞
⎟
⎟
⎟
⎠.
8.3.4. Answer Exercise 8.3.3 with Rn replaced by Cn.
8.3.5.(a) Give an example of a 3 × 3 matrix with 1 as its only eigenvalue, and only one linearly
independent eigenvector. (b) Find one that has two linearly independent eigenvectors.
8.3.6. True or false: (a) Every diagonal matrix is complete.
(b) Every upper triangular matrix is complete.
8.3.7. Prove that if A is a complete matrix, then so is cA + d I , where c, d are any scalars.
8.3.8.(a) Prove that if A is complete, then so is A2.
(b) Give an example of an incomplete matrix A such that A2 is complete.
8.3.9. Let U be an upper triangular matrix with all its diagonal entries equal. Prove that
U is complete if and only if U is a diagonal matrix.
♦8.3.10. Suppose v1, . . . , vn is an eigenvector basis for the complete matrix A, with λ1, . . . , λn
the corresponding eigenvalues. Prove that every eigenvalue of A is one of the λ1, . . . , λn.
♦8.3.11. Show that if A is complete, then every similar matrix B = S−1AS is also complete.
♦8.3.12.(a) Prove that if x ± i y is a complex conjugate pair of eigenvectors of a real matrix
A corresponding to complex conjugate eigenvalues μ ± i ν with ν ̸= 0, then x and y are
linearly independent real vectors. (b) More generally, if vj = xj ± i yj, j = 1, . . . , k,
are complex conjugate pairs of eigenvectors corresponding to distinct pairs of complex
conjugate eigenvalues μj ± i νj, νj ̸= 0, then the real vectors x1, . . . , xk, y1, . . . , yk are
linearly independent.
(c) Prove that if A is complete, then there exists a basis of Rn
consisting of its real eigenvectors and real and imaginary parts of its complex eigenvectors.

426
8 Eigenvalues and Singular Values
Diagonalization
Let L: Rn →Rn be a linear transformation on n-dimensional Euclidean space. As we know,
cf. Theorem 7.5, L[x] = A x is prescribed by multiplication by an n×n matrix A. However,
the matrix representing a given linear transformation will depend on the choice of basis
for the underlying vector space Rn. Linear transformations having a complicated matrix
representation in terms of the standard basis e1, . . . , en may be considerably simpliﬁed by
choosing a suitably adapted basis v1, . . . , vn. We are now in a position to understand how
to eﬀect such a simpliﬁcation.
For example, the linear transformation L

x
y

=

x −y
2x + 4y

studied in Example 7.19
is represented by the matrix A =

1
−1
2
4

— when expressed in terms of the standard
basis of R2. In terms of the alternative basis v1 =

1
−1

, v2 =

1
−2

, it is represented
by the diagonal matrix

2
0
0
3

, implying that it has a simple stretching action on the
new basis vectors: Av1 = 2v1,
Av2 = 3v2.
Now we can understand the reason for
this simpliﬁcation. The new basis consists of the two eigenvectors of the matrix A. This
observation is indicative of a general fact: representing a linear transformation in terms
of an eigenvector basis has the eﬀect of changing its matrix representative into a simple
diagonal form — thereby diagonalizing the original coeﬃcient matrix.
According to (7.31), if v1, . . . , vn form a basis of Rn, then the corresponding matrix
representative of the linear transformation L[v] = Av is given by the similar matrix
B = S−1A S, where S = ( v1, v2, . . . , vn ) is the matrix whose columns are the basis
vectors. In the preceding example,
S =

1
1
−1
−2

, and hence S−1A S =

2
1
−1
−1
 
1
−1
2
4
 
1
1
−1
−2

=

2
0
0
3

.
Deﬁnition 8.24. A square matrix A is called diagonalizable if there exists a nonsingular
matrix S and a diagonal matrix Λ = diag (λ1, . . . , λn) such that
S−1A S = Λ,
or, equivalently,
A = S Λ S−1.
(8.30)
A diagonal matrix represents a linear transformation that simultaneously stretches†
in the direction of the basis vectors.
Thus, every diagonalizable matrix represents an
elementary combination of (complex) stretching transformations.
To understand the diagonalization equation (8.30), we rewrite it in the equivalent form
A S = S Λ.
(8.31)
Using the columnwise action (1.11) of matrix multiplication, one easily sees that the kth
column of this n × n matrix equation is given by
Avk = λkvk,
where vk denotes the kth column of S. Therefore, the columns of S are necessarily eigen-
vectors, and the entries of the diagonal matrix Λ are the corresponding eigenvalues. And,
†
A negative diagonal entry represents the combination of a reﬂection and stretch. Complex
entries indicate complex “stretching” transformations. See Section 7.2 for details.

8.3 Eigenvector Bases
427
as a result, a diagonalizable matrix A must have n linearly independent eigenvectors, i.e.,
an eigenvector basis, to form the columns of the nonsingular diagonalizing matrix S. Since
the diagonal form Λ contains the eigenvalues along its diagonal, it is uniquely determined
up to a permutation of its entries.
Now, as we know, not every matrix has an eigenvector basis. Moreover, even when it
exists, the eigenvector basis may be complex, in which case S is a complex matrix, and the
entries of the diagonal matrix Λ are the complex eigenvalues. Thus, we should distinguish
between complete matrices that are diagonalizable over the complex numbers and the more
restrictive class of real matrices that can be diagonalized by a real matrix S.
Theorem 8.25. A matrix is complex diagonalizable if and only if it is complete. A real
matrix is real diagonalizable if and only if it is complete and has all real eigenvalues.
Example 8.26.
The 3 × 3 matrix A =
⎛
⎝
0
−1
−1
1
2
1
1
1
2
⎞
⎠considered in Example 8.5 has
eigenvector basis
v1 =
⎛
⎝
−1
1
0
⎞
⎠,
v2 =
⎛
⎝
−1
0
1
⎞
⎠,
v3 =
⎛
⎝
−1
1
1
⎞
⎠.
We assemble these to form the eigenvector matrix
S =
⎛
⎝
−1
−1
−1
1
0
1
0
1
1
⎞
⎠,
whereby
S−1 =
⎛
⎝
−1
0
−1
−1
−1
0
1
1
1
⎞
⎠.
The diagonalization equation (8.30) becomes
S−1A S =
⎛
⎝
−1
0
−1
−1
−1
0
1
1
1
⎞
⎠
⎛
⎝
0
−1
−1
1
2
1
1
1
2
⎞
⎠
⎛
⎝
−1
−1
−1
1
0
1
0
1
1
⎞
⎠=
⎛
⎝
1
0
0
0
1
0
0
0
2
⎞
⎠= Λ,
with eigenvalues of A appearing on the diagonal of Λ, in the same order as the eigenvectors.
Remark. If a matrix is not complete, then it cannot be diagonalized. A simple example is
a matrix of the form A =

1
c
0
1

with c ̸= 0, which represents a shear in the direction of
the x-axis. Incomplete matrices represent generalized shearing transformations, and will
be the subject of Section 8.6.
Exercises
8.3.13. Diagonalize the following matrices:
(a)

3
−9
2
−6
	
, (b)

5
−4
2
−1
	
, (c)

−4
−2
5
2
	
,
(d)
⎛
⎜
⎝
−2
3
1
0
1
−1
0
0
3
⎞
⎟
⎠,
(e)
⎛
⎜
⎝
8
0
−3
−3
0
−1
3
0
−2
⎞
⎟
⎠,
(f )
⎛
⎜
⎝
3
3
5
5
6
5
−5
−8
−7
⎞
⎟
⎠,
(g)
⎛
⎜
⎝
2
5
5
0
2
0
0
−5
−3
⎞
⎟
⎠,
(h)
⎛
⎜
⎜
⎜
⎝
1
0
−1
1
0
2
−1
1
0
0
−1
0
0
0
0
−2
⎞
⎟
⎟
⎟
⎠,
(i)
⎛
⎜
⎜
⎜
⎝
0
0
1
0
0
0
0
1
1
0
0
0
0
1
0
0
⎞
⎟
⎟
⎟
⎠,
(j)
⎛
⎜
⎜
⎜
⎝
2
1
−1
0
−3
−2
0
1
0
0
1
−2
0
0
1
−1
⎞
⎟
⎟
⎟
⎠.

428
8 Eigenvalues and Singular Values
8.3.14. Diagonalize the Fibonacci matrix F =

1
1
1
0
	
.
8.3.15. Diagonalize the matrix

0
−1
1
0
	
of rotation through 90◦. How would you interpret
the result?
8.3.16. Diagonalize the rotation matrices
(a)
⎛
⎜
⎝
0
−1
0
1
0
0
0
0
1
⎞
⎟
⎠,
(b)
⎛
⎜
⎜
⎜
⎝
5
13
0
12
13
0
1
0
−12
13
0
5
13
⎞
⎟
⎟
⎟
⎠.
8.3.17. Which of these matrices have real diagonal forms?
(a)

−2
1
4
1
	
, (b)

1
2
−3
1
	
,
(c)
⎛
⎜
⎝
0
1
0
−1
0
1
1
1
0
⎞
⎟
⎠, (d)
⎛
⎜
⎝
0
3
2
−1
1
−1
1
−3
−1
⎞
⎟
⎠, (e)
⎛
⎜
⎝
3
−8
2
−1
2
2
1
−4
2
⎞
⎟
⎠, (f )
⎛
⎜
⎜
⎜
⎝
1
0
0
0
0
1
0
0
0
0
1
0
−1
−1
−1
−1
⎞
⎟
⎟
⎟
⎠.
8.3.18. Diagonalize the following complex matrices:
(a)

i
1
1
i
	
,
(b)

1 −i
0
i
2 + i
	
,
(c)

2 −i
2 + i
3 −i
1 + i
	
,
(d)
⎛
⎜
⎝
−i
0
1
−i
1
−1
1
0
−i
⎞
⎟
⎠.
8.3.19. Write down a real matrix that has
(a) eigenvalues −1, 3 and corresponding eigenvectors

−1
2
	
,

1
1
	
,
(b) eigenvalues 0, 2, −2 and associated eigenvectors
⎛
⎜
⎝
−1
1
0
⎞
⎟
⎠,
⎛
⎜
⎝
2
−1
1
⎞
⎟
⎠,
⎛
⎜
⎝
0
1
3
⎞
⎟
⎠;
(c) an eigenvalue of 3 and corresponding eigenvectors

2
−3
	
,

1
2
	
;
(d) an eigenvalue −1 + 2 i and corresponding eigenvector

1 + i
3 i
	
;
(e) an eigenvalue −2 and corresponding eigenvector
⎛
⎜
⎝
2
0
−1
⎞
⎟
⎠;
(f ) an eigenvalue 3 + i and corresponding eigenvector
⎛
⎜
⎝
1
2 i
−1 −i
⎞
⎟
⎠.
8.3.20. A matrix A has eigenvalues −1 and 2 and associated eigenvectors

1
2
	
and

2
3
	
.
Write down the matrix form of the linear transformation L[u] = Au in terms of (a) the
standard basis e1, e2; (b) the basis consisting of its eigenvectors; (c) the basis

1
1
	
,

3
4
	
.
♦8.3.21. Prove that two complete matrices A, B have the same eigenvalues (with multiplicities)
if and only if they are similar, i.e., B = S−1 AS for some nonsingular matrix S.
8.3.22. Let B be obtained from A by permuting both its rows and columns using the same
permutation π, so bij = aπ(i),π(j). Prove that A and B have the same eigenvalues. How are
their eigenvectors related?
8.3.23. True or false: If A is a complete upper triangular matrix, then it has an upper
triangular eigenvector matrix S.
8.3.24. How many diﬀerent diagonal forms does an n × n diagonalizable matrix have?
8.3.25. Characterize all complete matrices that are their own inverses: A−1 = A. Write down a
non-diagonal example.

8.4 Invariant Subspaces
429
♥8.3.26. Two n × n matrices A, B are said to be simultaneously diagonalizable if there is a
nonsingular matrix S such that both S−1AS and S−1B S are diagonal matrices.
(a) Show that simultaneously diagonalizable matrices commute: AB = B A.
(b) Prove that the converse is valid, provided that one of the matrices has no multiple
eigenvalues. (c) Is every pair of commuting matrices simultaneously diagonalizable?
8.4 Invariant Subspaces
The notion of an invariant subspace of a linear map plays an important role in dynamical
systems, both ﬁnite- and inﬁnite-dimensional, as well as in linear iterative systems, and in
linear control systems. With the theory of eigenvalues and eigenvectors in hand, we are
now able to completely characterize them.
Deﬁnition 8.27.
Let L: V →V be a linear transformation on a vector space V . A
subspace W ⊂V is said to be invariant if L[w] ∈W whenever w ∈W.
Trivial examples of invariant subspaces, valid for any linear map, are the entire space
W = V and the zero subspace W = {0}. If L = I is the identity transformation, then
every subspace W ⊂V is invariant. More interestingly, both the kernel and image of L are
invariant subspaces. Indeed, when w ∈ker L, then L[w] = 0 ∈ker L, proving invariance.
Similarly, if w ∈img L, then L[w] also lies in img L by the deﬁnition of image.
Example 8.28.
Let V = R2. Let us ﬁnd all invariant subspaces of the scaling trans-
formation L(x, y) = ( 2x, 3y )T . If W is the line spanned by a vector w = ( a, b )T ̸= 0
then L[w] = ( 2a, 3b )T ∈W if and only if ( 2a, 3b )T = cw = ( ca, cb )T for some scalar c.
This is clearly possible if and only if either a = 0 or b = 0. Thus, the only one-dimensional
invariant subspaces of this scaling transformation are the x- and y-axes.
Next, consider the linear transformation L(x, y) = ( x + 3y, y )T corresponding to a
shear in the direction of the x-axis. By the same reasoning, the one-dimensional subspace
spanned by w = ( a, b )T ̸= 0 is invariant if and only if ( a + 3b, b )T = ( ca, cb )T for c ∈R,
which is possible only if b = 0. Thus, the only one-dimensional invariant subspace of this
shearing transformation is the x-axis itself.
Finally, consider the linear transformation L(x, y) = ( −y, x )T corresponding to coun-
terclockwise rotation by 90◦. It is easy to see, either geometrically or algebraically, that
L has no nontrivial invariant subspaces. On the other hand, if we view L as a map on
C2, then one can show that there are two one-dimensional complex invariant subspaces,
namely those spanned by its eigenvectors w1 = ( 1, i )T and w2 = ( 1, −i )T .
From here on, we restrict our attention to the ﬁnite-dimensional case, and so the linear
transformation L on either Rn or Cn is given by matrix multiplication: L[x] = A x for
some n × n matrix A.
Proposition 8.29. A one-dimensional subspace is invariant under the linear transforma-
tion L[x] = A x if and only if it is an eigenline spanned by an eigenvector of A.
Proof : Let W be spanned by the single non-zero vector w ̸= 0. Then Aw ∈W if and
only if Aw = λw for some scalar λ. But this means that w is an eigenvector of A with
eigenvalue λ.
Q.E.D.

430
8 Eigenvalues and Singular Values
Thus, if A has no multiple eigenvalues, it has a ﬁnite number of one-dimensional invari-
ant subspaces, namely the eigenspaces (eigenlines) associated with each eigenvalue. On
the other hand, if λ is a multiple eigenvalue, then every one-dimensional subspace of its
eigenspace W ⊂Vλ = { v | Av = λv } is invariant.
We already observed that if A = λ I , then every subspace V ⊂Cn is invariant. On
the other hand, if A is diagonal, with all distinct entries, then the invariant subspaces are
necessarily spanned by a ﬁnite collection of standard basis vectors el1, . . . , elk, which are
recognized as its eigenvectors. This is a special case of the following general characterization
of invariant subspaces of complete matrices. The incomplete case will be dealt with in
Section 8.6.
Theorem 8.30. If A is a complete matrix, then every k-dimensional complex invariant
subspace is spanned by k linearly independent eigenvectors of A.
Proof : Let W ̸= {0} be a nontrivial invariant subspace. Thanks to completeness, we can
express every nonzero vector 0 ̸= w ∈W as a linear combination, w = c1v1 + · · · + cjvj,
where v1, . . . , vj are eigenvectors associated with distinct eigenvalues λ1, . . . , λj of A and
all coeﬃcients ci ̸= 0. We claim that this implies that each represented eigenvector vi ∈W
for i = 1, . . ., j, which clearly establishes the result. To prove the claim, we write
Aw −λjw = c1(λ1 −λj)v1 + · · · + ck−1(λj−1 −λj)vj−1 ∈W,
since, by the assumption of invariance, both terms on the left hand side belong to W.
Moreover, since the eigenvalues are distinct, we must have ci(λi−λj) ̸= 0 for i = 1, . . ., j−1.
Iterating this process, we eventually conclude that a nonzero multiple of v1 and hence v1
itself belongs to W. This result is independent of the ordering of the eigenvectors, and
hence all v1, . . . , vj ∈W.
Q.E.D.
If A is a complete real matrix that possesses all real eigenvalues, then the same proof
shows that every real invariant subspace has the form given in Theorem 8.30. If A is real
and complete, with complex conjugate eigenvalues, Theorem 8.30 describes its complex
invariant subspaces. Its real invariant subspaces are obtained from the real and imagi-
nary parts of the eigenvectors. For example, if v± = x ± i y are a complex conjugate
pair of eigenvectors, then they individually span one-dimensional complex invariant sub-
spaces. However, the smallest corresponding real invariant subspace is the two-dimensional
subspace spanned by x and y.
Example 8.31.
Consider
the
three-dimensional
rotation
(permutation)
matrix
A =
⎛
⎝
0
1
0
0
0
1
1
0
0
⎞
⎠. It has one real eigenvalue, λ1 = 1, and two complex conjugate eigenval-
ues, λ2 = 1
2 +
√
3
2 i and λ3 = 1
2 −
√
3
2 i . The complex invariant subspaces are spanned by
0, 1, 2, or 3 of the corresponding complex eigenvectors

−1
2, −1
2, 1
T ± i
$ √
3
2 , −
√
3
2 , 0
%T
.
There is a single one-dimensional real invariant subspace, spanned by the real eigenvector
( 1, 1, 1 )T , and a single two-dimensional real invariant subspace, which is the orthogonal
complement spanned by the real and imaginary parts of the complex conjugate eigenvec-
tors. Indeed, this indicates a general property satisﬁed by all 3 × 3 rotation matrices, with
the exception of the trivial identity matrix. The unique one-dimensional real invariant
subspace is the axis of the rotation, and the matrix reduces to a two-dimensional rotation
on its orthogonal complement. See Exercise 8.2.44 for further details.

8.5 Eigenvalues of Symmetric Matrices
431
Exercises
8.4.1. Find all invariant subspaces W ⊂R2 of the following linear transformations L: R3 →R3:
(a) the scaling transformation ( 2x, 3y, 4z )T ;
(b) the shear ( x + 3y, y, z )T ;
(c) counterclockwise rotation by a 45◦angle around the x-axis.
8.4.2. Find all invariant subspaces of the following matrices:
(a)

1
2
2
1
	
, (b)

3
0
−2
3
	
,
(c)
⎛
⎜
⎝
0
0
2
0
1
0
2
0
0
⎞
⎟
⎠, (d)
⎛
⎜
⎝
−6
0
−8
−4
2
−4
4
0
6
⎞
⎟
⎠, (e)
⎛
⎜
⎜
⎜
⎝
0
0
1
0
0
0
0
1
1
0
0
0
0
1
0
0
⎞
⎟
⎟
⎟
⎠, (f )
⎛
⎜
⎜
⎜
⎝
1
0
1
0
0
1
0
0
0
0
1
0
0
1
0
1
⎞
⎟
⎟
⎟
⎠.
8.4.3. Find all complex invariant subspaces and all real invariant subspaces of
(a)

0
1
−1
0
	
,
(b)

−1
2
1
1
	
,
(c)
⎛
⎜
⎝
3
3
5
5
6
5
−5
−8
−7
⎞
⎟
⎠,
(d)
⎛
⎜
⎝
2
5
5
0
2
0
0
−5
−3
⎞
⎟
⎠.
8.4.4. Prove that if W is an invariant subspace for A, then it is also invariant for A2. Is the
converse to this statement valid?
♦8.4.5. Let V ⊂Rn be an invariant subspace for the n × n matrix A. Explain why every
eigenvalue and eigenvector of the linear map obtained by restricting A to V are also
eigenvalues and eigenvectors of A itself.
8.4.6. True or false: If V and W are invariant subspaces for the matrix A, then so is
(a) V + W;
(b) V ∩W;
(c) V ∪W;
(d) V \ W.
8.4.7. True or false: If V is an invariant subspace for the n × n matrix A and W is an invariant
subspace for the n × n matrix B, then V + W is an invariant subspace for the matrix A+ B.
8.4.8. True or false: If W is an invariant subspace of the matrix A, then it is also an invariant
subspace of AT .
8.4.9. True or false: If W is an invariant subspace of the nonsingular matrix A, then it is also
an invariant subspace of A−1.
8.4.10. Which 2 × 2 orthogonal matrices have a nontrivial real invariant subspace?
8.4.11. True or false: If Q ̸= ± I is a 4 × 4 orthogonal matrix, then Q has no real invariant
subspaces.
♦8.4.12.(a) Let A be an n × n symmetric matrix, and let v be an eigenvector. Prove that its
orthogonal complement under the dot product, namely, V ⊥= { w ∈Rn | v1 · w = 0 }, is
an invariant subspace.
(b) More generally, prove that if W ⊂Rn is an invariant subspace,
then its orthogonal complement W ⊥, is also invariant.
8.5 Eigenvalues of Symmetric Matrices
Fortunately, the matrices that arise in most applications are complete and, in fact, possess
some additional structure that ameliorates the calculation of their eigenvalues and eigen-
vectors. The most important class is that of the symmetric, including positive deﬁnite,
matrices. In fact, not only are the eigenvalues of a symmetric matrix necessarily real, the
eigenvectors always form an orthogonal basis of the underlying Euclidean space, enjoying
all the wonderful properties we studied in Chapter 4. In fact, this is by far the most com-
mon way for orthogonal bases to appear — as the eigenvector bases of symmetric matrices.
Let us state this important result, but defer its proof until the end of the section.

432
8 Eigenvalues and Singular Values
Theorem 8.32. Let A = AT be a real symmetric n × n matrix. Then
(a) All the eigenvalues of A are real.
(b) Eigenvectors corresponding to distinct eigenvalues are orthogonal.
(c) There is an orthonormal basis of Rn consisting of n eigenvectors of A.
In particular, all real symmetric matrices are complete and real diagonalizable.
Remark. Orthogonality is with respect to the standard dot product on Rn. As we noted
in Section 7.5, the transpose is a particular case of the adjoint operation when we use
the Euclidean dot product. An analogous result holds for more general self-adjoint linear
transformations under more general inner products on Rn; see Exercise 8.5.10.
Example 8.33.
The 2×2 matrix A =

3
1
1
3

considered in Example 8.5 is symmetric,
and so has real eigenvalues λ1 = 4 and λ2 = 2. You can easily check that the corresponding
eigenvectors v1 = ( 1, 1 )T and v2 = ( −1, 1 )T are orthogonal: v1 · v2 = 0, and hence form
an orthogonal basis of R2. The orthonormal eigenvector basis promised by Theorem 8.32
is obtained by dividing each eigenvector by its Euclidean norm:
u1 =

1
√
2
1
√
2

,
u2 =

−
1
√
2
1
√
2

.
Example 8.34.
Consider the symmetric matrix A =
⎛
⎝
5
−4
2
−4
5
2
2
2
−1
⎞
⎠. A straightfor-
ward computation produces its eigenvalues and eigenvectors:
λ1 = 9,
λ2 = 3,
λ3 = −3,
v1 =
⎛
⎝
1
−1
0
⎞
⎠,
v2 =
⎛
⎝
1
1
1
⎞
⎠,
v3 =
⎛
⎝
1
1
−2
⎞
⎠.
As the reader can check, the eigenvectors form an orthogonal basis of R3. An orthonormal
basis is provided by the corresponding unit eigenvectors
u1 =
⎛
⎜
⎜
⎝
1
√
2
−
1
√
2
0
⎞
⎟
⎟
⎠,
u2 =
⎛
⎜
⎜
⎝
1
√
3
1
√
3
1
√
3
⎞
⎟
⎟
⎠,
u3 =
⎛
⎜
⎜
⎝
1
√
6
1
√
6
−
2
√
6
⎞
⎟
⎟
⎠.
The eigenvalues of a symmetric matrix can be used to test its positive deﬁniteness.
Theorem 8.35. A symmetric matrix K = KT is positive deﬁnite if and only if all of its
eigenvalues are strictly positive.
Proof : First, if K > 0, then, by deﬁnition, xT K x > 0 for all nonzero vectors x ∈Rn. In
particular, if x = v ̸= 0 is an eigenvector with (necessarily real) eigenvalue λ, then
0 < vT Kv = vT (λv) = λvT v = λ ∥v∥2,
(8.32)
which immediately proves that λ > 0.

8.5 Eigenvalues of Symmetric Matrices
433
Conversely, suppose K has all positive eigenvalues. Let u1, . . . , un be the orthonormal
eigenvector basis guaranteed by Theorem 8.32, with Kuj = λj uj with λj > 0. Writing
x = c1 u1 + · · · + cnun,
we obtain
K x = c1 λ1 u1 + · · · + cn λn un.
Therefore, using the orthonormality of the eigenvectors,
xT K x = (c1 uT
1 + · · · + cn uT
n) (c1 λ1 u1 + · · · + cn λn un) = λ1 c2
1 + · · · + λn c2
n > 0
whenever x ̸= 0, since only x = 0 has coordinates c1 = · · · = cn = 0. This establishes the
positive deﬁniteness of K.
Q.E.D.
Remark.
The same proof shows that K is positive semi-deﬁnite if and only if all its
eigenvalues satisfy λ ≥0.
A positive semi-deﬁnite matrix that is not positive deﬁnite
admits a zero eigenvalue and one or more null eigenvectors, i.e., solutions to Kv = 0.
Every nonzero element 0 ̸= v ∈ker K of its kernel is a null eigenvector.
Example 8.36.
The symmetric matrix K =
⎛
⎝
8
0
1
0
8
1
1
1
7
⎞
⎠has characteristic equation
det(K −λ I) = −λ3 + 23λ2 −174λ + 432 = −(λ −9)(λ −8)(λ −6),
and so its eigenvalues are 9, 8, and 6. Since they are all positive, K is a positive deﬁnite
matrix. The associated eigenvectors are
λ1 = 9,
v1 =
⎛
⎝
1
1
1
⎞
⎠,
λ2 = 8,
v2 =
⎛
⎝
−1
1
0
⎞
⎠,
λ3 = 6,
v3 =
⎛
⎝
−1
−1
2
⎞
⎠.
Note that the eigenvectors form an orthogonal basis of R3, as guaranteed by Theorem 8.32.
As usual, we can construct an corresponding orthonormal eigenvector basis
u1 =
⎛
⎜
⎜
⎝
1
√
3
1
√
3
1
√
3
⎞
⎟
⎟
⎠,
u2 =
⎛
⎜
⎜
⎝
−
1
√
2
1
√
2
0
⎞
⎟
⎟
⎠,
u3 =
⎛
⎜
⎜
⎝
−
1
√
6
−
1
√
6
2
√
6
⎞
⎟
⎟
⎠,
by dividing each eigenvector by its norm.
Proof of Theorem 8.32:
First recall that (see Exercise 3.6.38) if A = AT is real, symmetric,
then
(Av) · w = v · (Aw)
for all
v, w ∈Cn,
(8.33)
where · indicates the Euclidean dot product when the vectors are real and, more generally,
the Hermitian dot product v · w = vT w when they are complex.
To prove property (a), suppose λ is a complex eigenvalue with complex eigenvector
v ∈Cn. Consider the Hermitian dot product of the complex vectors Av and v:
(Av) · v = (λv) · v = λ ∥v∥2.
On the other hand, by (8.33),
(Av) · v = v · (Av) = v · (λv) = vT λv = λ ∥v∥2.

434
8 Eigenvalues and Singular Values
Equating these two expressions, we deduce
λ ∥v∥2 = λ ∥v∥2.
Since v is an eigenvector, it must be nonzero. Thus, we deduce that λ = λ, proving that
the eigenvalue λ must be real.
To prove (b), suppose Av = λv, Aw = μ w, where λ ̸= μ are distinct real eigenvalues.
Then, again by (8.33),
λ v · w = (Av) · w = v · (Aw) = v · (μ w) = μ v · w,
and hence
(λ −μ) v · w = 0.
Since λ ̸= μ, this implies that v · w = 0, so the eigenvectors v, w are orthogonal.
Finally, the proof of (c) is easy if all the eigenvalues of A are distinct. Theorem 8.21
implies that the eigenvectors form a basis of Rn, and part (b) proves they are orthogonal.
(An alternative proof starts with orthogonality, and then applies Proposition 4.4 to prove
that the eigenvectors form a basis.) To obtain an orthonormal basis, we merely divide the
eigenvectors by their lengths: uk = vk/∥vk ∥, as in Lemma 4.2.
To prove (c) in general, we proceed by induction on the size n of the matrix A. To start,
the case of a 1 × 1 matrix is trivial. (Why?) Next, suppose A has size n × n. We know
that A has at least one eigenvalue, λ1, which is necessarily real. Let v1 be an associated
eigenvector. Let V ⊥= { w ∈Rn | v1 · w = 0 } denote its orthogonal complement — the
subspace of all vectors orthogonal to the ﬁrst eigenvector. Proposition 4.41 implies that
dim V ⊥= n−1, and so we can choose an orthonormal basis y1, . . . , yn−1. Moreover, by Ex-
ercise 8.4.12, V ⊥is an invariant subspace of A, and hence A deﬁnes a linear transformation
on V ⊥that is represented by an (n−1)×(n−1) matrix, say B = (bij), i, j = 1, . . . , n−1,
with respect to the chosen orthonormal basis y1, . . . , yn−1, so that Ayi = 3n−1
j =1 bij yj.
Moreover, by orthonormality and (8.33),
bij = yi · Ayj = (Ayi) · yj = bji,
and hence B = BT is symmetric. Our induction hypothesis then implies that there is an
orthonormal basis of V ⊥⊂Rn consisting of eigenvectors u2, . . . , un of B, and hence also
of A, each of which is orthogonal to v1. Appending the unit eigenvector u1 = v1/∥v1∥to
this collection will complete the orthonormal basis of Rn.
Q.E.D.
Proposition 8.37. Let A = AT be an n × n symmetric matrix. Let v1, . . . , vn be an
orthogonal eigenvector basis such that v1, . . . , vr correspond to nonzero eigenvalues, while
vr+1, . . . , vn are null eigenvectors corresponding to the zero eigenvalue (if any). Then r =
rank A; the non-null eigenvectors v1, . . . , vr form an orthogonal basis for img A = coimg A,
while the null eigenvectors vr+1, . . . , vn form an orthogonal basis for ker A = coker A.
Proof : The zero eigenspace coincides with the kernel, V0 = ker A.
Thus, the linearly
independent null eigenvectors form a basis for ker A, which has dimension n −r where
r = rank A. Moreover, the remaining r non-null eigenvectors are orthogonal to the null
eigenvectors. Therefore, they must form a basis for the kernel’s orthogonal complement,
namely coimg A = img A.
Q.E.D.

8.5 Eigenvalues of Symmetric Matrices
435
Exercises
8.5.1. Find the eigenvalues and an orthonormal eigenvector basis for the following symmetric
matrices:
(a)

2
6
6
−7
	
(b)

5
−2
−2
5
	
, (c)

2
−1
−1
5
	
, (d)
⎛
⎜
⎝
1
0
4
0
1
3
4
3
1
⎞
⎟
⎠, (e)
⎛
⎜
⎝
6
−4
1
−4
6
−1
1
−1
11
⎞
⎟
⎠.
8.5.2. Determine whether the following symmetric matrices are positive deﬁnite by computing
their eigenvalues. Validate your conclusions by using the methods from Chapter 5.
(a)

2
−2
−2
3
	
(b)

−2
3
3
6
	
,
(c)
⎛
⎜
⎝
1
−1
0
−1
2
−1
0
−1
1
⎞
⎟
⎠,
(d)
⎛
⎜
⎝
4
−1
−2
−1
4
−1
−2
−1
4
⎞
⎟
⎠.
8.5.3. Prove that a symmetric matrix is negative deﬁnite if and only if all its eigenvalues are
negative.
8.5.4. How many orthonormal eigenvector bases does a symmetric n × n matrix have?
8.5.5. Let A =

a
b
c
d
	
.
(a) Write down necessary and suﬃcient conditions on the entries
a, b, c, d that ensures that A has only real eigenvalues.
(b) Verify that all symmetric 2 × 2 matrices satisfy your conditions.
(c) Write down a non-symmetric matrix that satisﬁes your conditions.
♥8.5.6. Let AT = −A be a real, skew-symmetric n × n matrix. (a) Prove that the only possible
real eigenvalue of A is λ = 0. (b) More generally, prove that all eigenvalues λ of A are
purely imaginary, i.e., Re λ = 0. (c) Explain why 0 is an eigenvalue of A whenever n is
odd. (d) Explain why, if n = 3, the eigenvalues of A ̸= O are 0, i ω, −i ω, for some real
ω ̸= 0. (e) Verify these facts for the particular matrices
(i)

0
−2
2
0
	
,
(ii)
⎛
⎜
⎝
0
3
0
−3
0
−4
0
4
0
⎞
⎟
⎠,
(iii)
⎛
⎜
⎝
0
1
−1
−1
0
−1
1
1
0
⎞
⎟
⎠,
(iv)
⎛
⎜
⎜
⎜
⎝
0
0
2
0
0
0
0
−3
−2
0
0
0
0
3
0
0
⎞
⎟
⎟
⎟
⎠.
♥8.5.7.(a) Prove that every eigenvalue of a Hermitian matrix A, satisfying AT = A as
in Exercise 3.6.45, is real. (b) Show that the eigenvectors corresponding to distinct
eigenvalues are orthogonal under the Hermitian dot product on Cn. (c) Find the
eigenvalues and eigenvectors of the following Hermitian matrices, and verify orthogonality:
(i)

2
i
−i
−2
	
,
(ii)

3
2 −i
2 + i
−1
	
,
(iii)
⎛
⎜
⎝
0
i
0
−i
0
i
0
−i
0
⎞
⎟
⎠.
♥8.5.8. Let K, M be n × n matrices, with M > 0 positive deﬁnite. A nonzero vector v ̸= 0 is
called a generalized eigenvector of matrix pair K, M if it satisﬁes the generalized eigenvalue
equation
K v = λM v,
v ̸= 0,
(8.34)
where the scalar λ is the corresponding generalized eigenvalue. Note that ordinary
eigenvalue/eigenvectors of K are when M = I .(a) Prove that λ is a generalized eigenvalue
if and only if it satisﬁes the generalized characteristic equation det(K −λM) = 0.
(b) Prove that λ is a generalized eigenvalue of the matrix pair K, M if and only if it is an
ordinary eigenvalue of the matrix M−1K. How are the eigenvectors related?
(c) Now
suppose K is a symmetric matrix. Prove that its generalized eigenvalues are all real.
Hint: First explain why this does not follow from part (a). Instead mimic the proof of part
(a) of Theorem 8.32, using the weighted Hermitian inner product ⟨v , w ⟩= vT M w in
place of the dot product.
(d) Show that if K > 0, then its generalized eigenvalues are all
positive: λ > 0.
(e) Prove that the eigenvectors corresponding to diﬀerent generalized
eigenvalues are orthogonal under the weighted inner product ⟨v , w ⟩= vT M w.
(f ) Show

436
8 Eigenvalues and Singular Values
that, if the matrix pair K, M has n distinct generalized eigenvalues, then the eigenvectors
form an orthogonal basis for Rn. Remark. One can, by mimicking the proof of part (c) of
Theorem 8.32, show that this holds even when there are repeated generalized eigenvalues.
8.5.9. Compute the generalized eigenvalues and eigenvectors, as in (8.34), for the following
matrix pairs. Verify orthogonality of the eigenvectors under the appropriate inner product.
(a) K =

3
−1
−1
2
	
, M =

2
0
0
3
	
, (b) K =

3
1
1
1
	
, M =

2
0
0
1
	
,
(c) K =

2
−1
−1
4
	
, M =

2
−1
−1
1
	
, (d) K =
⎛
⎜
⎝
6
−8
3
−8
24
−6
3
−6
99
⎞
⎟
⎠, M =
⎛
⎜
⎝
1
0
0
0
4
0
0
0
9
⎞
⎟
⎠,
(e) K =
⎛
⎜
⎝
1
2
0
2
8
2
0
2
1
⎞
⎟
⎠, M =
⎛
⎜
⎝
1
1
0
1
3
1
0
1
1
⎞
⎟
⎠, (f ) K =
⎛
⎜
⎝
5
3
−5
3
3
−1
−5
−1
9
⎞
⎟
⎠, M =
⎛
⎜
⎝
3
2
−3
2
2
−1
−3
−1
5
⎞
⎟
⎠.
♦8.5.10. Let L = L∗: Rn →Rn be a self-adjoint linear transformation with respect to the
inner product ⟨· , · ⟩. Prove that all its eigenvalues are real and the eigenvectors are
orthogonal. Hint: Mimic the proof of Theorem 8.32, replacing the dot product by the
given inner product.
♥8.5.11. The diﬀerence map Δ: Cn →Cn is deﬁned as Δ = S −I , where S is the shift map
of Exercise 8.2.13. (a) Write down the matrix D corresponding to Δ. (b) Prove that the
sampled exponential vectors ω0, . . . , ωn−1 from (5.102) form an eigenvector basis of D.
What are the eigenvalues?
(c) Prove that K = DT D has the same eigenvectors as D.
What are its eigenvalues?
(d) Is K positive deﬁnite? (e) According to Theorem 8.32
the eigenvectors of a symmetric matrix are real and orthogonal. Use this to explain the
orthogonality of the sampled exponential vectors. But, why aren’t they real?
♥8.5.12. An n × n circulant matrix has the form C =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
c0
c1
c2
c3
. . .
cn−1
cn−1
c0
c1
c2
. . .
cn−2
cn−2
cn−1
c0
c1
. . .
cn−3
...
...
...
...
...
...
c1
c2
c3
c4
. . .
c0
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
,
in which the entries of each succeeding row are obtained by moving all the previous row’s
entries one slot to the right, the last entry moving to the front. (a) Check that the shift
matrix A of Exercise 8.2.13, the diﬀerence matrix D, and its symmetric product K = DT D
of Exercise 8.5.11 are all circulant matrices. (b) Prove that the sampled exponential vectors
ω0, . . . , ωn−1, cf. (5.102), are eigenvectors of C. Thus, all circulant matrices have the same
eigenvectors! What are the eigenvalues?
(c) Prove that F −1
n C Fn = Λ, where Fn is the
Fourier matrix in Exercise 5.6.9 and Λ is the diagonal matrix with the eigenvalues of C
along the diagonal. (d) Find the eigenvalues and eigenvectors of the following circulant
matrices:
(i)

1
2
2
1
	
, (ii)
⎛
⎜
⎝
1
2
3
3
1
2
2
3
1
⎞
⎟
⎠,
(iii)
⎛
⎜
⎜
⎜
⎝
1
−1
−1
1
1
1
−1
−1
−1
1
1
−1
−1
−1
1
1
⎞
⎟
⎟
⎟
⎠,
(iv)
⎛
⎜
⎜
⎜
⎝
2
−1
0
−1
−1
2
−1
0
0
−1
2
−1
−1
0
−1
2
⎞
⎟
⎟
⎟
⎠.
(e) Find the eigenvalues of the tricirculant matrices in Exercise 1.7.13. Can you
ﬁnd a general formula for the n × n version? Explain why the eigenvalues must be
real and positive. Does your formula reﬂect this fact? (f ) Which of the preceding
matrices are invertible? Write down a general criterion for checking the invertibility
of circulant matrices.

8.5 Eigenvalues of Symmetric Matrices
437
The Spectral Theorem
Every real, symmetric matrix admits an eigenvector basis, and hence is diagonalizable.
Moreover, since we can choose eigenvectors that form an orthonormal basis, the diagonal-
izing matrix takes a particularly simple form. Recall that an n × n matrix Q is orthogonal
if and only if its columns form an orthonormal basis of Rn. Alternatively, one characterizes
orthogonal matrices by the condition Q−1 = QT , as in Deﬁnition 4.18.
Using the orthonormal eigenvector basis in the diagonalization formula (8.30) results in
what is known as the spectral factorization of a symmetric matrix.
Theorem 8.38. Let A be a real, symmetric matrix.
Then there exists an orthogonal
matrix Q such that
A = Q Λ Q−1 = Q Λ QT,
(8.35)
where Λ is a real diagonal matrix. The eigenvalues of A appear on the diagonal of Λ, while
the columns of Q are the corresponding orthonormal eigenvectors.
Remark. The term “spectrum” refers to the eigenvalues of a matrix, or, more generally,
a linear operator. The terminology is motivated by physics. The spectral energy lines of
atoms, molecules, and nuclei are characterized as the eigenvalues of the governing quantum
mechanical Schr¨odinger operator, [54]. The Spectral Theorem 8.38 is the ﬁnite-dimensional
version of the decomposition of quantum mechanical linear operators into their spectral
eigenstates.
Warning. Although both involve diagonal matrices, the spectral factorization A = Q Λ QT
and the Gaussian factorization A = LDLT of a regular symmetric matrix, cf. (1.58), are
completely diﬀerent. In particular, the eigenvalues are not the pivots, so Λ ̸= D.
The spectral factorization (8.35) provides us with an alternative means of diagonalizing
the associated quadratic form q(x) = xT A x, i.e., of completing the square. We write
q(x) = xT A x = xT Q Λ QT x = yT Λ y =
n

i=1
λi y2
i ,
(8.36)
where the entries of y = QTx = Q−1x are the coordinates of x with respect to the
orthonormal eigenvector basis of A. In particular, q(x) > 0 for all x ̸= 0 and so A is positive
deﬁnite if and only if each eigenvalue λi is strictly positive, reconﬁrming Theorem 8.35.
Example 8.39.
For the 2 × 2 matrix A =

3
1
1
3

considered in Example 8.33, the
orthonormal eigenvectors produce the diagonalizing orthogonal matrix Q =

1
√
2
−1
√
2
1
√
2
1
√
2

.
The reader can validate the resulting spectral factorization:

3
1
1
3

= A = Q Λ QT =

1
√
2
−1
√
2
1
√
2
1
√
2
 
4
0
0
2
 
1
√
2
1
√
2
−
1
√
2
1
√
2

.
According to formula (8.36), the associated quadratic form is diagonalized as
q(x) = 3x2
1 + 2x1 x2 + 3x2
2 = 4y2
1 + 2y2
2,
where y = QTx, i.e., y1 = x1 + x2
√
2
, y2 = −x1 + x2
√
2
.

438
8 Eigenvalues and Singular Values
A
Figure 8.3.
Stretching a Circle into an Ellipse.
You can always choose Q to be a proper orthogonal matrix, so det Q = 1, since an
improper orthogonal matrix can be made proper by multiplying one of its columns by −1,
which does not aﬀect its status as an eigenvector matrix. Since a proper orthogonal matrix
Q represents a rigid rotation of Rn, the diagonalization of a symmetric matrix can be inter-
preted as a rotation of the coordinate system that makes the orthogonal eigenvectors line
up along the coordinate axes. Therefore, a linear transformation L[x] = A x represented
by a positive deﬁnite matrix A can be regarded as a combination of stretches in n mutually
orthogonal directions. A good way to visualize this is to consider the eﬀect of the linear
transformation on the unit (Euclidean) sphere S1 = { ∥x∥= 1 }. Stretching the sphere in
mutually orthogonal directions will map it to an ellipsoid E = L[S1 ] = { A x | ∥x∥= 1 }
whose principal axes are aligned with the directions of stretch; see Figure 8.3 for the
two-dimensional case.
For instance, in elasticity, the stress tensor of a deformed body
is represented by a positive deﬁnite matrix. Its eigenvalues are known as the principal
stretches and its eigenvectors the principal directions of the elastic deformation.
Exercises
8.5.13. Write out the spectral factorization of the following matrices:
(a)

−3
4
4
3
	
,
(b)

2
−1
−1
4
	
,
(c)
⎛
⎜
⎝
1
1
0
1
2
1
0
1
1
⎞
⎟
⎠,
(d)
⎛
⎜
⎝
3
−1
−1
−1
2
0
−1
0
2
⎞
⎟
⎠.
8.5.14. Write out the spectral factorization of the matrices listed in Exercise 8.5.1.
8.5.15. Construct a symmetric matrix with the following eigenvectors and eigenvalues, or
explain why none exists:
(a) λ1 = 1, v1 =
 3
5, 4
5
T , λ2 = 3, v2 =

−4
5, 3
5
T ,
(b) λ1 = −2, v1 = ( 1, −1 )T , λ2 = 1, v2 = ( 1, 1 )T ,
(c) λ1 = 3, v1 = ( 2, −1 )T ,
λ2 = −1, v2 = ( −1, 2 )T ,
(d) λ1 = 2, v1 = ( 2, 1 )T , λ2 = 2, v2 = ( 1, 2 )T .
♥8.5.16.(a) Find the eigenvalues and eigenvectors of the matrix A =
⎛
⎜
⎝
2
1
−1
1
2
1
−1
1
2
⎞
⎟
⎠.
(b) Use the eigenvalues to compute the determinant of A.
(c) Is A positive deﬁnite? Why
or why not?
(d) Find an orthonormal eigenvector basis of R3 determined by A or explain
why none exists.
(e) Write out the spectral factorization of A if possible.
(f ) Use
orthogonality to write the vector ( 1, 0, 0 )T as a linear combination of eigenvectors of A.
8.5.17. Use the spectral factorization to diagonalize the following quadratic forms:
(a) x2 −3xy + 5y2,
(b) 3x2 + 4xy + 6y2,
(c) x2 + 8xz + y2 + 6y z + z2,
(d)
3
2 x2 −xy −xz + y2 + z2,
(e) 6x2 −8xy + 2xz + 6y2 −2y z + 11z2.

8.5 Eigenvalues of Symmetric Matrices
439
♦8.5.18. Let u1, . . . , un be an orthonormal basis of Rn. Prove that it forms an eigenvector basis
for some symmetric n × n matrix A. Can you characterize all such matrices?
8.5.19. True or false: A matrix with a real orthonormal eigenvector basis is symmetric.
8.5.20. Prove that every quadratic form can be written as xT A x = ∥x∥2
⎛
⎝
n

i=1
λi cos2 θi
⎞
⎠,
where λi are the eigenvalues of A and θi =<) (x, vi) denotes the angle between x and the
ith eigenvector.
8.5.21. An elastic body has stress tensor T =
⎛
⎜
⎝
3
1
2
1
3
1
2
1
3
⎞
⎟
⎠. Find the principal stretches and
principal directions of stretch.
♦8.5.22. Given a solid body spinning around its center of mass, the eigenvectors of its positive
deﬁnite inertia tensor prescribe three mutually orthogonal principal directions of rotation,
while the corresponding eigenvalues are the moments of inertia. Given the inertia tensor
T =
⎛
⎜
⎝
2
1
0
1
3
1
0
1
2
⎞
⎟
⎠, ﬁnd the principal directions and moments of inertia.
♦8.5.23. Let K be a positive deﬁnite 2 × 2 matrix. (a) Explain why the quadratic equation
xT K x = 1 deﬁnes an ellipse. Prove that its principal axes are the eigenvectors of K, and
the semi-axes are the reciprocals of the square roots of the eigenvalues.
(b) Graph and describe the following curves:
(i) x2 + 4y2 = 1,
(ii) x2 + xy + y2 = 1,
(iii) 3x2 + 2xy + y2 = 1.
(c) What sort of curve(s) does xT K x = 1 describe if K is not positive deﬁnite?
♦8.5.24. Let K be a positive deﬁnite 3 × 3 matrix. (a) Prove that the quadratic equation
xT K x = 1 deﬁnes an ellipsoid in R3. What are its principal axes and semi-axes?
(b) Describe the surface deﬁned by the equation 11x2−8xy+20y2−10xz+8y z+11z2 = 1.
8.5.25. Prove that A = AT has a repeated eigenvalue if and only if it commutes, AJ = J A,
with a nonzero skew-symmetric matrix: JT = −J ̸= O.
Hint: First prove this when A is a diagonal matrix.
8.5.26. Find all positive deﬁnite orthogonal matrices.
♦8.5.27.(a) Prove that every positive deﬁnite matrix K has a unique positive deﬁnite square
root, i.e., a matrix B > 0 satisfying B2 = K.
(b) Find the positive deﬁnite square roots of the following matrices:
(i)

2
1
1
2
	
,
(ii)

3
−1
−1
1
	
,
(iii)
⎛
⎜
⎝
2
0
0
0
5
0
0
0
9
⎞
⎟
⎠,
(iv)
⎛
⎜
⎝
6
−4
1
−4
6
−1
1
−1
11
⎞
⎟
⎠.
♦8.5.28. The Polar Decomposition: Prove that every invertible matrix A has a polar
decomposition, written A = QB, into the product of an orthogonal matrix Q and a
positive deﬁnite matrix B > 0. Show that if det A > 0, then Q is a proper orthogonal
matrix. Hint: Look at the Gram matrix K = ATA and use Exercise 8.5.27.
Remark. In mechanics, if A represents the deformation of a body, then Q represents a
rotation, while B represents a stretching along the orthogonal eigendirections of K. Thus,
every linear deformation of an elastic body can be decomposed into a pure stretching
transformation followed by a rotation.
8.5.29. Find the polar decompositions A = QB, as deﬁned in Exercise 8.5.28, of the following
matrices:
(a)

0
1
2
0
	
,
(b)

2
−3
1
6
	
,
(c)

1
2
0
1
	
,
(d)
⎛
⎜
⎝
0
−3
8
1
0
0
0
4
6
⎞
⎟
⎠,
(e)
⎛
⎜
⎝
1
0
1
1
−2
0
1
1
0
⎞
⎟
⎠.

440
8 Eigenvalues and Singular Values
♦8.5.30. The Spectral Theorem for Hermitian Matrices. Prove that a complex Hermitian matrix
can be factored as H = U Λ U† where U is a unitary matrix and Λ is a real diagonal matrix.
Hint: See Exercises 4.3.25, 8.5.7.
8.5.31. Find the spectral factorization, as in Exercise 8.5.30, of the following Hermitian matrices:
(a)

3
2 i
−2 i
6
	
,
(b)

6
1 −2 i
1 + 2 i
2
	
,
(c)
⎛
⎜
⎝
−1
5 i
−4
−5 i
−1
4 i
−4
−4 i
8
⎞
⎟
⎠.
♥8.5.32. The Spectral Decomposition: Let A be a symmetric matrix with distinct eigenvalues
λ1, . . . , λk. Let Vj = ker(A −λj I ) denote the eigenspace corresponding to λj, and let Pj be
the orthogonal projection matrix onto Vj, as deﬁned in Exercise 4.4.9. (i) Prove that the
spectral factorization (8.35) can be rewritten as
A = λ1 P1 + λ2 P2 + · · · + λk Pk,
(8.37)
expressing A as a linear combination of projection matrices. (ii) Write out the spectral
decomposition (8.37) for the matrices in Exercise 8.5.13. (iii) Show that
I = P1 + P2 + · · · + Pk,
while
P 2
i = Pi,
PiPj = O
for
i ̸= j.
(iv) Show that if p(t) is any polynomial, then
p(A) = p(λ1)P1 + p(λ2)P2 + · · · + p(λk)Pk.
(8.38)
Remark. Replacing p(t) by any function f(t) allows one to deﬁne f(A) for any symmetric
matrix A.
Optimization Principles for Eigenvalues of Symmetric Matrices
As we learned in Chapter 5, the solution to a linear system with positive deﬁnite coeﬃcient
matrix can be characterized by a minimization principle.
Thus, it should come as no
surprise that eigenvalues of positive deﬁnite matrices, and even more general symmetric
matrices, can also be characterized by some sort of optimization procedure. A number
of basic numerical algorithms for computing eigenvalues of matrices are based on such
optimization principles.
First, consider the relatively simple case of a real diagonal matrix Λ = diag (λ1, . . . , λn).
We assume that the diagonal entries, which are the same as the eigenvalues, appear in
decreasing order,
λ1 ≥λ2 ≥· · · ≥λn,
(8.39)
so λ1 is the largest eigenvalue, while λn is the smallest.
The eﬀect of Λ on a vector
y = ( y1, y2, . . . , yn )T ∈Rn is to multiply its entries by the diagonal eigenvalues: Λ y =
( λ1 y1, λ2 y2, . . . , λn yn )T. In other words, the linear transformation represented by the
coeﬃcient matrix Λ has the eﬀect of stretching† the ith coordinate direction by the factor
λi. In particular, the maximal stretch occurs in the e1 direction, with factor λ1, while
the minimal (or largest negative) stretch occurs in the en direction, with factor λn. The
germ of the optimization principles for characterizing the extreme eigenvalues is contained
in this geometrical observation.
Let us turn our attention to the associated quadratic form
q(y) = yT Λ y = λ1 y2
1 + λ2 y2
2 + · · · + λn y2
n.
(8.40)
†
If λi < 0, then the eﬀect is to stretch and reﬂect.

8.5 Eigenvalues of Symmetric Matrices
441
Note that q(t e1) = λ1 t2, and hence if λ1 > 0, then q(y) has no maximum; on the other
hand, if λ1 ≤0, so all eigenvalues are non-positive, then q(y) ≤0 for all y, and its
maximal value is q(0) = 0. Thus, in either case, a strict maximization of q(y) does not tell
us anything of importance.
Suppose, however, that we continue in our quest to maximize q(y), but now restrict y
to be a unit vector (in the Euclidean norm), so
∥y∥2 = y2
1 + · · · + y2
n = 1.
In view of (8.39),
q(y) = λ1 y2
1 +λ2 y2
2 + · · · +λn y2
n ≤λ1 y2
1 +λ1 y2
2 + · · · +λ1 y2
n = λ1

y2
1 + · · · + y2
n

= λ1.
Moreover, q(e1) = λ1. We conclude that the maximal value of q(y) over all unit vectors is
the largest eigenvalue of Λ:
λ1 = max{ q(y) | ∥y∥= 1 } .
By the same reasoning, its minimal value equals the smallest eigenvalue:
λn = min { q(y) | ∥y∥= 1 } .
Thus, we can characterize the two extreme eigenvalues by optimization principles, albeit
of a slightly diﬀerent character from what we dealt with in Chapter 5.
Now suppose A is any symmetric matrix. We use its spectral factorization (8.35) to
diagonalize the associated quadratic form
q(x) = xT A x = xT Q Λ QT x = yT Λ y,
where
y = QT x = Q−1x,
as in (8.36). According to the preceding discussion, the maximum of yT Λ y over all unit
vectors ∥y∥= 1 is the largest eigenvalue λ1 of Λ, which is the same as the largest eigenvalue
of A. Moreover, since Q is an orthogonal matrix, Proposition 7.24 tell us that it maps unit
vectors to unit vectors:
1 = ∥y∥= ∥QT x∥= ∥x∥,
and so the maximum of q(x) over all unit vectors ∥x∥= 1 is the same maximum eigenvalue
λ1.
Similar reasoning applies to the smallest eigenvalue λn.
In this fashion, we have
established the basic optimization principles for the extreme eigenvalues of a symmetric
matrix.
Theorem 8.40. Let A be a symmetric matrix with real eigenvalues λ1 ≥λ2 ≥· · · ≥λn.
Then
λ1 = max

xT A x
 ∥x∥= 1

,
λn = min

xT A x
 ∥x∥= 1

,
(8.41)
are, respectively its largest and smallest eigenvalues. The maximal value is achieved when
x = ±u1 is one of the unit eigenvectors associated with the largest eigenvalue λ1; similarly,
the minimal value occurs at x = ±un, a unit eigenvector for the smallest eigenvalue λn.
Remark. In multivariable calculus, the eigenvalue λ plays the role of a Lagrange multiplier
for the constrained optimization problem, [2, 78, 79].
Example 8.41.
The problem is to maximize the value of the quadratic form
q(x, y) = 3x2 + 2xy + 3y2

442
8 Eigenvalues and Singular Values
for all x, y lying on the unit circle x2 + y2 = 1. This maximization problem is precisely of
the form (8.41). The symmetric coeﬃcient matrix for the quadratic form is A =

3
1
1
3

,
whose eigenvalues are, according to Example 8.5, λ1 = 4 and λ2 = 2. Theorem 8.40 implies
that the maximum is the largest eigenvalue, and hence equal to 4, while its minimum is the
smallest eigenvalue, and hence equal to 2. Thus, evaluating q(x, y) on the unit eigenvectors,
we conclude that
q
$
−1
√
2,
1
√
2
%
= 2 ≤q(x, y) ≤4 = q
$
−1
√
2,
1
√
2
%
for all
x2 + y2 = 1.
In practical applications, the restriction of the quadratic form to unit vectors may not be
particularly convenient. We can, however, rephrase the eigenvalue optimization principles
in a form that utilizes general nonzero vectors. If v ̸= 0, then x = v/∥v∥is a unit vector.
Substituting this expression for x in the quadratic form q(x) = xTA x leads to the following
optimization principles for the extreme eigenvalues of a symmetric matrix:
λ1 = max
 vT Av
∥v∥2
 v ̸= 0
'
,
λn = min
 vT Av
∥v∥2
 v ̸= 0
'
.
(8.42)
Thus, we replace optimization of a quadratic polynomial over the unit sphere by optimiza-
tion of a rational function over all of Rn\{0}. The rational function being optimized is
called a Rayleigh quotient, after Lord Rayleigh, a prominent nineteenth-century British
scientist. For instance, referring back to Example 8.41, the maximum value of
r(x, y) = 3x2 + 2xy + 3y2
x2 + y2
for all

x
y

̸=

0
0

is equal to 4, the same maximal eigenvalue of the corresponding coeﬃcient matrix.
What about characterizing one of the intermediate eigenvalues? Then we need to be
a little more sophisticated in designing the optimization principle. To motivate the con-
struction, look ﬁrst at the diagonal case. If we restrict the quadratic form (8.40) to vectors
y = ( 0, y2, . . . , yn )T whose ﬁrst component is zero, we obtain
q( y ) = q(0, y2, . . . , yn) = λ2 y2
2 + · · · + λn y2
n.
The maximum value of q( y ) over all such y of norm 1 is, by the same reasoning, the
second largest eigenvalue λ2. Moreover, y · e1 = 0, and so y can be characterized as being
orthogonal to the ﬁrst standard basis vector, which also happens to be the eigenvector
of Λ corresponding to the maximal eigenvalue λ1. Thus, to ﬁnd the second eigenvalue,
we maximize the quadratic form over all unit vectors that are orthogonal to the ﬁrst
eigenvector. Similarly, if we want to ﬁnd the jth largest eigenvalue λj, we maximize q( y )
over all unit vectors y whose ﬁrst j −1 components vanish, y1 = · · · = yj−1 = 0, or, stated
geometrically, over all vectors y such that ∥y∥= 1 and y · e1 = · · · = y · ej−1 = 0, that is,
over all unit vectors orthogonal to the ﬁrst j −1 eigenvectors of Λ.
A similar reasoning based on the Spectral Theorem 8.38 and the orthogonality of eigen-
vectors of symmetric matrices leads to the general result.
Theorem 8.42. Let A be a symmetric matrix with eigenvalues λ1 ≥λ2 ≥· · · ≥λn and
corresponding orthogonal eigenvectors v1, . . . , vn. Then the maximal value of the quadratic
form q(x) = xT A x over all unit vectors that are orthogonal to the ﬁrst j −1 eigenvectors
is its jth eigenvalue:
λj = max

xT A x
 ∥x∥= 1,
x · v1 = · · · = x · vj−1 = 0

.
(8.43)

8.5 Eigenvalues of Symmetric Matrices
443
Thus, at least in principle, one can compute the eigenvalues and eigenvectors of a
symmetric matrix by the following recursive procedure. First, ﬁnd the largest eigenvalue
λ1 by the basic maximization principle (8.41) and its associated eigenvector v1 by solving
the eigenvector system (8.13). The next largest eigenvalue λ2 is then characterized by the
constrained maximization principle (8.43), and so on. Although of theoretical interest, this
algorithm is of somewhat limited value in numerical computations. Practical numerical
methods for computing eigenvalues will be developed in Sections 9.5 and 9.6.
Exercises
8.5.33. Find the minimum and maximum values of the quadratic form 5x2 + 4xy + 5y2 where
x, y are subject to the constraint x2 + y2 = 1.
8.5.34. Find the minimum and maximum values of the quadratic form
2x2 + xy + 2xz + 2y2 + 2z2 where x, y, z are required to satisfy x2 + y2 + z2 = 1.
8.5.35. What is the minimum and maximum values of the following rational functions:
(a) 3x2 −2y2
x2 + y2
, (b) x2 −3xy + y2
x2 + y2
, (c) 3x2 + xy + 5y2
x2 + y2
, (d) 2x2 + xy + 3xz + 2y2 + 2z2
x2 + y2 + z2
.
8.5.36. Find the minimum and maximum values of q(x) =
n−1

i=1
xi xi+1 for ∥x∥2 = 1.
Hint: See Exercise 8.2.47.
8.5.37. Write down and solve an optimization principle characterizing the largest and smallest
eigenvalues of the following positive deﬁnite matrices:
(a)

2
−1
−1
3
	
,
(b)

4
1
1
4
	
,
(c)
⎛
⎜
⎝
6
−4
1
−4
6
−1
1
−1
11
⎞
⎟
⎠,
(d)
⎛
⎜
⎝
4
−1
−2
−1
4
−1
−2
−1
4
⎞
⎟
⎠.
8.5.38. Write down a maximization principle that characterizes the middle eigenvalue of the
matrices in parts (c) and (d) of Exercise 8.5.37.
8.5.39. Given a 3 × 3 symmetric matrix, formulate two distinct ways of characterizing its
middle eigenvalue λ2.
8.5.40. Suppose K > 0. What is the maximum value of q(x) = xT K x when x is constrained to
a sphere of radius ∥x∥= r?
8.5.41. Let K > 0. Prove the product formula
max

xT K x



 ∥x∥= 1

min

xT K−1x



 ∥x∥= 1

= 1.
♦8.5.42. Write out the details in the proof of Theorem 8.42.
♦8.5.43. Reformulate Theorem 8.42 as a minimum principle for intermediate eigenvalues.
8.5.44. Under the set-up of Theorem 8.42, explain why
λj = max
⎧
⎨
⎩
vT Kv
∥v∥2






 v ̸= 0,
v · v1 = · · · = v · vj−1 = 0
⎫
⎬
⎭.
♥8.5.45.(a) Let K, M be positive deﬁnite n × n matrices and λ1 ≥· · · ≥λn be their general-
ized eigenvalues, as in Exercise 8.5.9. Prove that that the largest generalized eigenvalue
can be characterized by the maximum principle λ1 = max{ xT K x | xT M x = 1 }.
Hint: Use Exercise 8.5.27.
(b) Prove the alternative maximum principle λ1 =
max
⎧
⎨
⎩
xT K x
xT M x






x ̸= 0
⎫
⎬
⎭.
(c) How would you characterize the smallest generalized
eigenvalue? (d) An intermediate generalized eigenvalue?

444
8 Eigenvalues and Singular Values
8.5.46. Use Exercise 8.5.45 to ﬁnd the minimum and maximum of the rational functions
(a) 3x2 + 2y2
4x2 + 5y2 , (b) x2 −xy + 2y2
2x2 −xy + y2 , (c) 2x2 + 3y2 + z2
x2 + 3y2 + 2z2 , (d) 2x2 + 6xy + 11y2 + 6y z + 2z2
x2 + 2xy + 3y2 + 2y z + z2
.
8.5.47. Let A be a complete square matrix, not necessarily symmetric, with all positive
eigenvalues. Is the associated quadratic form q(x) = xT A x > 0 for all x ̸= 0?
8.6 Incomplete Matrices
Unfortunately, not all square matrices are complete. Matrices that do not have enough
(complex) eigenvectors to form a basis are considerably less pleasant to work with. How-
ever, since they occasionally appear in applications, it is worth learning how to handle them.
There are two approaches: the ﬁrst, named after the twentieth-century Russian/German
mathematician Issai Schur, is a generalization of the spectral theorem, and converts an
arbitrary square matrix into a similar, upper triangular matrix with the eigenvalues along
the diagonal. Thus, although not every matrix can be diagonalized, they can all be “trian-
gularized”. Applications of the Schur decomposition, including the numerical computation
of eigenvalues, can be found in [21].
The second approach, due to the nineteenth-century French mathematician Camille
Jordan,† shows how to supplement the eigenvectors of an incomplete matrix in order to
obtain a basis in which the matrix assumes a simple, but now non-diagonal, canonical
(meaning distinguished) form. Applications of the Jordan canonical form will appear in
our study of linear systems of ordinary diﬀerential equations. We remark that the two
subsections are completely independent of one another.
The Schur Decomposition
As noted above, the Schur decomposition is used to convert a square matrix into similar
upper triangular matrix. The similarity transformation can be chosen to be represented
by a unitary matrix — a complex generalization of an orthogonal matrix.
Deﬁnition 8.43. A complex, square matrix U is called unitary if it satisﬁes
U † U = I ,
where
U † = U T
(8.44)
denotes the Hermitian transpose, in which one ﬁrst transposes and then takes complex
conjugates of all entries.
Thus, U is unitary if and only if its inverse equals its Hermitian transpose: U −1 = U †.
For example, U =

1
√
2 i
−1
√
2
1
√
2
−1
√
2 i

is unitary, since U −1 = U † =

−
1
√
2 i
1
√
2
−
1
√
2
1
√
2 i

.
The (i, j) entry of the deﬁning equation (8.44) is the Hermitian dot product between
the ith and jth columns of U, and hence U is an n × n unitary matrix if and only if its
columns form an orthonormal basis of Cn. In particular, a real matrix is unitary if and
only if it is orthogonal. The next result is proved in the same fashion as Proposition 4.23.
†
No relation to Wilhelm Jordan of Gauss–Jordan fame.

8.6 Incomplete Matrices
445
Proposition 8.44. If U1 and U2 are n × n unitary matrices, so is their product U1U2.
The Schur decomposition states that every square matrix is unitarily similar to an upper
triangular matrix. The method of proof provides a recursive algorithm for constructing
the decomposition.
Theorem 8.45. Let A be an n × n matrix, either real or complex. Then there exists a
unitary matrix U and an upper triangular matrix Δ such that
A = U ΔU † = U ΔU −1.
(8.45)
The diagonal entries of Δ are the eigenvalues of A.
In particular, if all eigenvalues of A are real, then U = Q can be chosen to be a (real)
orthogonal matrix, and Δ is also a real matrix. This follows from the construction outlined
in the proof.
Warning. The Schur decomposition (8.45) is not unique. As the method of proof makes
clear, there are many inequivalent choices for the matrices U and Δ.
Proof of Theorem 8.45:
The proof proceeds by induction on the size of A. According
to Theorem 8.11, A has at least one, possibly complex, eigenvalue λ1.
Let u1 ∈Cn
be a corresponding unit eigenvector, so its Hermitian norm is ∥u1 ∥= 1. Let U1 be an
n × n unitary matrix whose ﬁrst column is the unit eigenvector u1. In practice, U1 can be
constructed by applying the Gram–Schmidt process to any basis of Cn whose ﬁrst element
is the eigenvector u1. The eigenvector equation Au1 = λ1 u1 forms the ﬁrst column of the
matrix product equation
AU1 = U1B,
where
B = U †
1AU1 =

λ1
r
0
C

,
with C an (n −1) × (n −1) matrix and r a row vector. By our induction hypothesis,
there is an (n −1) × (n −1) unitary matrix V such that V †C V = Γ is an upper triangular
(n −1) × (n −1) matrix. Set U2 =

1
0
0
V

. It is easily checked that U2 is also unitary,
and, moreover,
U †
2B U2 =

λ1
s
0
Γ

= Δ
is upper triangular. The unitary product matrix U = U1U2 yields the desired result:
U †AU = (U1U2)†AU1U2 = U †
2U †
1AU1U2 = U †
2B U2 = Δ,
which establishes the Schur decomposition formula (8.45).
Finally, since A and Δ are
similar matrices, they have the same eigenvalues, which justiﬁes the ﬁnal statement of the
theorem.
Q.E.D.
Example 8.46.
The matrix A =
⎛
⎝
6
4
−3
−4
−2
2
4
4
−2
⎞
⎠has a simple eigenvalue λ1 = 2, with
eigenvector v1 = ( −1, 1, 0 )T , and a double eigenvalue λ2 = 0, with only one independent
eigenvector v2 = ( 1, 0, 2 )T . Thus A is incomplete, and so not diagonalizable. To construct
a Schur decomposition, we begin with the ﬁrst eigenvector v1 and apply the Gram–Schmidt

446
8 Eigenvalues and Singular Values
process to the basis v1, e2, e3 to obtain the orthogonal matrix U1 =
⎛
⎝
−1
√
2
1
√
2
0
1
√
2
1
√
2
0
0
0
1
⎞
⎠.
The resulting similar matrix† B = U T
1 AU1 =
⎛
⎜
⎝
2
−8
√
5
2
0
2
−1
√
2
0
4
√
2
−2
⎞
⎟
⎠has its ﬁrst column in
upper triangular form. To continue the procedure, we extract the lower 2 × 2 submatrix
C =

2
−1
√
2
4
√
2
−2

, and ﬁnd that it has a single (incomplete) eigenvalue 0, with unit
eigenvector

1
3
2
√
2
3

. The corresponding orthogonal matrix V =

1
3
2
√
2
3
2
√
2
3
−1
3

will con-
vert C to upper triangular form V T C V =

0
9
√
2
0
0

. Therefore, U2 =
⎛
⎜
⎝
1
0
0
0
1
3
2
√
2
3
0
2
√
2
3
−1
3
⎞
⎟
⎠
will complete the conversion of the original matrix into upper triangular form
Δ = U T
2 B U2 = U TAU =
⎛
⎜
⎜
⎝
2
2
3
−37
3
√
2
0
0
9
√
2
0
0
0
⎞
⎟
⎟
⎠,
where U = U1U2 =
⎛
⎜
⎜
⎝
−
1
√
2
1
3
√
2
2
3
1
√
2
1
3
√
2
2
3
0
2
√
2
3
−1
3
⎞
⎟
⎟
⎠
is the desired orthogonal (unitary) matrix. Use of a computer to carry out the detailed
calculations is essential in most examples.
Exercises
8.6.1. Establish a Schur Decomposition for the matrices (a)

1
−1
1
3
	
, (b)

1
−2
−2
1
	
,
(c)

8
9
−6
−7
	
, (d)

1
5
−2
−1
	
, (e)
⎛
⎜
⎝
2
−1
2
−2
3
−1
−6
6
−5
⎞
⎟
⎠, (f )
⎛
⎜
⎝
0
2
−1
−1
−1
1
−1
0
0
⎞
⎟
⎠.
8.6.2. Show that a real unitary matrix is an orthogonal matrix.
♦8.6.3. Prove Proposition 8.44.
♦8.6.4. Write out a new proof of the Spectral Theorem 8.38 based on the Schur Decomposition.
♥8.6.5. A complex matrix A is called normal if it commutes with its Hermitian transpose:
A†A = AA†. (a) Show that every real symmetric matrix is normal. (b) Show that every
unitary matrix is normal. (c) Show that every real orthogonal matrix is normal. (d) Show
that an upper triangular matrix is normal if and only if it is diagonal.
(e) Show that the
eigenvectors of a normal matrix form an orthogonal basis of Cn under the Hermitian dot
product. (f ) Show that the converse is true: a matrix has an orthogonal eigenvector basis
of Cn if and only if it is normal. Hint: Use the Schur Decomposition. (g) How can you tell
when a real matrix has a real orthonormal eigenvector basis?
†
Since all matrices are real in this example, the Hermitian transpose † reduces to the ordinary
transpose T .

8.6 Incomplete Matrices
447
The Jordan Canonical Form
We now turn to the more sophisticated Jordan canonical form. Throughout this section, A
will be an n × n matrix, with either real or complex entries. We let λ1, . . . , λk denote the
distinct eigenvalues of A, so λi ̸= λj for i ̸= j. We recall that Theorem 8.11 guarantees that
every matrix has at least one (complex) eigenvalue, so k ≥1. Moreover, we are assuming
that k < n, since otherwise A would be complete.
Deﬁnition 8.47. Let A be a square matrix. A Jordan chain of length j for A is a sequence
of non-zero vectors w1, . . . , wj ∈Cn that satisﬁes
Aw1 = λw1,
Awi = λwi + wi−1,
i = 2, . . ., j,
(8.46)
where λ is an eigenvalue of A. A Jordan chain associated with a zero eigenvalue, which
requires that A be singular, is called a null Jordan chain, and satisﬁes
Aw1 = 0,
Awi = wi−1,
i = 2, . . ., j.
(8.47)
Note that the initial vector w1 in a Jordan chain is a genuine eigenvector, and so
Jordan chains exist only when λ is an eigenvalue. The rest, w2, . . . , wj, are generalized
eigenvectors, in accordance with the following deﬁnition.
Deﬁnition 8.48. A nonzero vector w ̸= 0 such that
(A −λ I )k w = 0
(8.48)
for some k > 0 and λ ∈C is called a generalized eigenvector†of the matrix A.
Note that every ordinary eigenvector is automatically a generalized eigenvector, since
we can just take k = 1 in (8.48); but the converse is not necessarily valid. We shall call the
minimal value of k for which (8.48) holds the index of the generalized eigenvector. Thus,
an ordinary eigenvector is a generalized eigenvector of index 1. Since A−λ I is nonsingular
whenever λ is not an eigenvalue of A, its kth power (A−λ I )k is also nonsingular. Therefore,
generalized eigenvectors can exist only when λ is an ordinary eigenvalue of A — there are
no additional “generalized eigenvalues”.
Lemma 8.49. The ith vector wi in a Jordan chain (8.46) is a generalized eigenvector of
index i.
Proof : By deﬁnition, (A −λ I )w1 = 0, and so w1 is an eigenvector.
Next, we have
(A −λ I )w2 = w1 ̸= 0, while (A −λ I )2 w2 = (A −λ I )w1 = 0. Thus, w2 is a generalized
eigenvector of index 2. A simple induction proves that (A −λ I )i−1 wi = w1 ̸= 0 while
(A −λ I )i wi = 0.
Q.E.D.
Example 8.50.
Consider the 3 × 3 matrix A =
⎛
⎝
2
1
0
0
2
1
0
0
2
⎞
⎠. The only eigenvalue is
λ = 2, and A −2 I =
⎛
⎝
0
1
0
0
0
1
0
0
0
⎞
⎠. We claim that the standard basis vectors e1, e2, e3
†
Despite the common terminology, this is not the same concept as developed in Exercise 8.5.8.

448
8 Eigenvalues and Singular Values
form a Jordan chain.
Indeed, Ae1 = 2e1, and hence e1 ∈ker(A −2 I ) is a genuine
eigenvector. Furthermore, Ae2 = 2e2 + e1, and Ae3 = 2e3 + e2, as you can easily check.
Thus, e1, e2, e3 satisfy the Jordan chain equations (8.46) for the eigenvalue λ = 2. Note
that e2 lies in the kernel of (A −2 I )2 =
⎛
⎝
0
0
1
0
0
0
0
0
0
⎞
⎠, and so is a generalized eigenvector
of index 2. Indeed, every vector of the form w = ae1 + be2 with b ̸= 0 is a generalized
eigenvector of index 2. (When b = 0, a ̸= 0, the vector w = ae1 is an ordinary eigenvector
of index 1.) Finally, (A −2 I )3 = O, and so every nonzero vector 0 ̸= v ∈R3, including
e3, is a generalized eigenvector of index 3 or less.
A basis of Rn or Cn is called a Jordan basis for the matrix A if it consists of one or more
Jordan chains that have no elements in common. Thus, for the matrix in Example 8.50,
the standard basis e1, e2, e3 is, in fact, a Jordan basis. An eigenvector basis, if such exists,
qualiﬁes as a Jordan basis, since each eigenvector belongs to a Jordan chain of length 1.
Jordan bases are the desired extension of eigenvector bases, and every square matrix has
one. The proof of the following Jordan Basis Theorem will appear below.
Theorem 8.51. Every n × n matrix admits a Jordan basis of Cn. The ﬁrst elements of
the Jordan chains form a maximal set of linearly independent eigenvectors. Moreover, the
number of generalized eigenvectors in the Jordan basis that belong to the Jordan chains
associated with the eigenvalue λ is the same as the eigenvalue’s multiplicity.
Example 8.52.
Consider the 5 × 5 matrix A =
⎛
⎜
⎜
⎜
⎝
−1
0
1
0
0
−2
2
−4
1
1
−1
0
−3
0
0
−4
−1
3
1
0
4
0
2
−1
0
⎞
⎟
⎟
⎟
⎠.
Its
characteristic equation is found to be
pA(λ) = det(A −λ I ) = λ5 + λ4 −5λ3 −λ2 + 8λ −4 = (λ −1)3 (λ + 2)2 = 0,
and hence A has two eigenvalues: λ1 = 1, which is a triple eigenvalue, and λ2 = −2, which
is double. Solving the associated homogeneous systems (A −λj I )v = 0, we ﬁnd that, up
to constant multiple, there are only two eigenvectors: v1 = ( 0, 0, 0, −1, 1 )T for λ1 = 1
and, anticipating our ﬁnal numbering, v4 =

−1, 1, 1, −2, 0
T for λ2 = −2. Thus, A is far
from complete.
To construct a Jordan basis, we ﬁrst note that since A has 2 linearly independent
eigenvectors, the Jordan basis will contain two Jordan chains: the one associated with the
triple eigenvalue λ1 = 1 will have length 3, while λ2 = −2 leads to a Jordan chain of
length 2. To construct the former, we need to ﬁrst solve the system (A −I )w = v1. Note
that the coeﬃcient matrix is singular — it must be, since 1 is an eigenvalue — and the
general solution is w = v2+tv1, where v2 = ( 0, 1, 0, 0, −1 )T , and t is the free variable. The
appearance of an arbitrary multiple of the eigenvector v1 in the solution is not unexpected;
indeed, the kernel of A−I is the eigenspace for λ1 = 1. We can choose any solution, e.g., v2
as the second element in the Jordan chain. We then solve (A−I )w = v2 for w = v3 +tv1,
where v3 = ( 0, 0, 0, 1, 0 )T can be used as the ﬁnal element of this Jordan chain. Similarly,
to construct the Jordan chain for the second eigenvalue, we solve (A + 2 I )w = v4 for
w = v5 + tv4, where v5 = ( −1, 0, 0, −2, 1 )T .

8.6 Incomplete Matrices
449
Thus, the desired Jordan basis is
v1 =
⎛
⎜
⎜
⎜
⎝
0
0
0
−1
1
⎞
⎟
⎟
⎟
⎠,
v2 =
⎛
⎜
⎜
⎜
⎝
0
1
0
0
−1
⎞
⎟
⎟
⎟
⎠,
v3 =
⎛
⎜
⎜
⎜
⎝
0
0
0
1
0
⎞
⎟
⎟
⎟
⎠,
v4 =
⎛
⎜
⎜
⎜
⎝
−1
1
1
−2
0
⎞
⎟
⎟
⎟
⎠,
v5 =
⎛
⎜
⎜
⎜
⎝
−1
0
0
−2
1
⎞
⎟
⎟
⎟
⎠,
with Av1 = v1, Av2 = v2 + v1, Av3 = v3 + v2, Av4 = −2v4, Av5 = −2v5 + v4.
Just as an eigenvector basis diagonalizes a complete matrix, a Jordan basis provides a
particularly simple form for an incomplete matrix, known as the Jordan canonical form.
Deﬁnition 8.53. An n × n matrix of the form†
Jλ,n =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎝
λ
1
λ
1
λ
1
...
...
λ
1
λ
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎠
,
(8.49)
in which λ is a real or complex number, is known as a Jordan block.
In particular, a 1 × 1 Jordan block is merely a scalar Jλ,1 = λ. Since every matrix has
at least one (complex) eigenvector — see Theorem 8.11 — the Jordan block matrices have
the least possible number of independent eigenvectors. The following result is immediate.
Lemma 8.54. The n×n Jordan block matrix Jλ,n has a single eigenvalue, λ, and a single
independent eigenvector, e1. The standard basis vectors e1, . . . , en form a Jordan chain
for Jλ,n.
Deﬁnition 8.55. A Jordan matrix is a square matrix of block diagonal form
J = diag (Jλ1,n1, Jλ2,n2, . . . , Jλk,nk) =
⎛
⎜
⎜
⎜
⎜
⎝
Jλ1,n1
Jλ2,n2
...
Jλk,nk
⎞
⎟
⎟
⎟
⎟
⎠
,
(8.50)
in which one or more Jordan blocks, not necessarily of the same size, lie along the diagonal,
while the blank oﬀ-diagonal blocks are all zero.
Note that the only possibly non-zero entries in a Jordan matrix are those on the diagonal,
which can have any complex value, including 0, and those on the superdiagonal, which are
either 1 or 0. The positions of the superdiagonal 1’s uniquely prescribes the Jordan blocks.
†
All non-displayed entries are zero.

450
8 Eigenvalues and Singular Values
For example, the 6 × 6 matrices
⎛
⎜
⎜
⎜
⎜
⎜
⎝
1
0
0
0
0
0
0
2
0
0
0
0
0
0
3
0
0
0
0
0
0
3
0
0
0
0
0
0
2
0
0
0
0
0
0
1
⎞
⎟
⎟
⎟
⎟
⎟
⎠
,
⎛
⎜
⎜
⎜
⎜
⎜
⎝
−1
1
0
0
0
0
0
−1
1
0
0
0
0
0
−1
1
0
0
0
0
0
−1
0
0
0
0
0
0
−1
1
0
0
0
0
0
−1
⎞
⎟
⎟
⎟
⎟
⎟
⎠
,
⎛
⎜
⎜
⎜
⎜
⎜
⎝
0
1
0
0
0
0
0
0
0
0
0
0
0
0
1
1
0
0
0
0
0
1
0
0
0
0
0
0
2
1
0
0
0
0
0
2
⎞
⎟
⎟
⎟
⎟
⎟
⎠
,
are all Jordan matrices: the ﬁrst is a diagonal matrix, consisting of 6 distinct 1 × 1 Jordan
blocks; the second has a 4 × 4 Jordan block followed by a 2 × 2 block that happen to have
the same diagonal entries; the last has three 2 × 2 Jordan blocks with respective diagonal
entries 0, 1, 2.
As a direct corollary of Lemma 8.54 combined with the matrix’s block structure, cf. Ex-
ercise 8.2.50, we obtain a complete classiﬁcation of the eigenvectors and eigenvalues of a
Jordan matrix.
Lemma 8.56. The Jordan matrix (8.50) has eigenvalues λ1, . . . , λk. The standard basis
vectors e1, . . . , en form a Jordan basis, which is partitioned into nonoverlapping Jordan
chains labeled by the Jordan blocks.
Thus, in the preceding examples of Jordan matrices, the ﬁrst has three double eigenval-
ues, 1 and 2 and 3, and corresponding linearly independent eigenvectors e1, e6, and e2, e5,
and e3, e4, each of which belongs to a Jordan chain of length 1. The second matrix has only
one eigenvalue, −1, but two independent eigenvectors e1, e5, and hence two Jordan chains,
namely e1, e2, e3, e4, and e5, e6. The last has three eigenvalues 0, 1, 2, three eigenvectors
e1, e3, e5, and three Jordan chains of length 2: e1, e2, and e3, e4, and e5, e6. In particular,
the only complete Jordan matrices are the diagonal matrices, all of whose Jordan blocks
are of size 1 × 1.
The Jordan canonical form follows from the Jordan Basis Theorem 8.51.
Theorem 8.57. Let A be an n × n real or complex matrix. Let S = ( w1 w2 . . . wn )
be a matrix whose columns form a Jordan basis of A. Then S places A into the Jordan
canonical form
S−1AS = J = diag (Jλ1,n1, Jλ2,n2, . . . , Jλk,nk),
or, equivalently,
A = S J S−1.
(8.51)
The diagonal entries of the similar Jordan matrix J are the eigenvalues of A. In partic-
ular, A is complete (diagonalizable) if and only if every Jordan block is of size 1 × 1 or,
equivalently, all Jordan chains are of length 1. The Jordan canonical form of A is uniquely
determined up to a permutation of the diagonal Jordan blocks.
For instance, the matrix A =
⎛
⎜
⎜
⎜
⎝
−1
0
1
0
0
−2
2
−4
1
1
−1
0
−3
0
0
−4
−1
3
1
0
4
0
2
−1
0
⎞
⎟
⎟
⎟
⎠considered in Example 8.52

8.6 Incomplete Matrices
451
has the following Jordan basis matrix and Jordan canonical form
S =
⎛
⎜
⎜
⎜
⎝
0
0
0
−1
−1
0
1
0
1
0
0
0
0
1
0
−1
0
1
−2
−2
1
−1
0
0
1
⎞
⎟
⎟
⎟
⎠,
J = S−1AS =
⎛
⎜
⎜
⎜
⎝
1
1
0
0
0
0
1
1
0
0
0
0
1
0
0
0
0
0
−2
1
0
0
0
0
−2
⎞
⎟
⎟
⎟
⎠.
Finally, let us outline a proof of the Jordan Basis Theorem 8.51.
Lemma 8.58. If v1, . . . , vn forms a Jordan basis for the matrix A, it also forms a Jordan
basis for B = A −c I , for any scalar c.
Proof : We note that the eigenvalues of B are all of the form λ−c, where λ is an eigenvalue
of A. Moreover, given a Jordan chain w1, . . . , wj of A, we have
B w1 = (λ −c)w1,
B wi = (λ −c)wi + wi−1,
i = 2, . . . , j,
so w1, . . . , wj is also a Jordan chain for B corresponding to the eigenvalue λ −c. Q.E.D.
The proof of Theorem 8.51 will be done by induction on the size n of the matrix. The
case n = 1 is trivial, since every nonzero element of C is a Jordan basis for a 1 × 1 matrix.
To perform the induction step, we assume that the result is valid for all matrices of size
≤n−1. Let A be an n×n matrix. According to Theorem 8.11, A has at least one complex
eigenvalue λ1. Let B = A −λ1 I . Since λ1 is an eigenvalue of A, we know that 0 is an
eigenvalue of B. This means that ker B ̸= {0}, and so r = rank B < n. Moreover, by
Lemma 8.58, a Jordan basis of B is also a Jordan basis for A, and so we can concentrate
all our attention on the singular matrix B from now on.
Recall that W = img B ⊂Cn is an invariant subspace for B, i.e., B w ∈W whenever
w ∈W. Moreover, since B is singular, dim W = r = rank B < n. Thus, by ﬁxing a basis
of W, we can realize the restriction B: W →W as multiplication by an r × r matrix. The
fact that r < n allows us to invoke the induction hypothesis and deduce the existence of a
Jordan basis w1, . . . , wr ∈W ⊂Cn for the action of B on the subspace W. Our goal is to
complete this collection to a full Jordan basis on Cn.
To this end, we append two additional kinds of vectors.
Suppose that the Jordan
basis of W contains k null Jordan chains associated with its zero eigenvalue. Each null
Jordan chain consists of vectors w1, . . . , wj ∈W satisfying B w1 = 0, B w2 = w1, . . . ,
B wj = wj−1, cf. (8.47). The number k of null Jordan chains is equal to the number
of linearly independent null eigenvectors of B that belong to W = img B, that is k =
dim(ker B ∩img B).
To each such null Jordan chain, we append a vector wj+1 ∈Cn
such that B wj+1 = wj, noting that wj+1 exists because wj ∈img B. We deduce that
w1, . . . , wj+1 ∈Cn forms a null Jordan chain, of length j + 1. Having extended all the
null Jordan chains in W, the resulting collection consists of r + k vectors in Cn arranged
in nonoverlapping Jordan chains. To complete to a basis, we append n −r −k additional
linearly independent null vectors z1, . . . , zn−r−k ∈ker B \ img B that lie outside its image.
Since B zj = 0, each zj forms a null Jordan chain of length 1. We claim that the complete
collection consisting of the r non-null Jordan chains in W, the k extended null chains, and
the additional null vectors z1, . . . , zn−r−k, forms the desired Jordan basis. By construction,
it consists of nonoverlapping Jordan chains. The only remaining issue is the proof that
the resulting collection of vectors is linearly independent; this is left as a challenge for the
reader in Exercise 8.6.25.
Q.E.D.

452
8 Eigenvalues and Singular Values
With the Jordan canonical form in hand, the general result characterizing complex
invariant subspaces of a general square matrix can now be stated.
Theorem 8.59. Every complex invariant subspace of a square matrix A is spanned by a
ﬁnite number of Jordan chains.
Proof : The proof proceeds in the same manner as that of Theorem 8.30. We assume that
vk is the last vector in its Jordan chain. We deduce that Aw−λkw is a linear combination
of v1, . . . , vk−1 with non-vanishing coeﬃcients, and so the induction step remains valid in
this case. The details are left to the reader.
Q.E.D.
Similarly, a complex conjugate pair of Jordan chains of order k produces two distinct k-
dimensional complex invariant subspaces, and hence a real invariant subspace of dimension
2k obtained from their real and imaginary parts. A detailed statement of the nature of
the most general real invariant subspace of a matrix is also left to the reader.
Exercises
8.6.6. For each of the following Jordan matrices, identify the Jordan blocks. Write down the
eigenvalues, the eigenvectors, and the Jordan basis. Clearly identify the Jordan chains.
(a)

2
1
0
2
	
, (b)

−3
0
0
6
	
, (c)
⎛
⎜
⎝
1
0
0
0
1
1
0
0
1
⎞
⎟
⎠, (d)
⎛
⎜
⎝
0
1
0
0
0
1
0
0
0
⎞
⎟
⎠, (e)
⎛
⎜
⎜
⎜
⎝
4
0
0
0
0
3
1
0
0
0
3
0
0
0
0
2
⎞
⎟
⎟
⎟
⎠.
8.6.7. Find a Jordan basis and Jordan canonical form for each of the following matrices:
(a)

2
3
0
2
	
,
(b)

−1
−1
4
−5
	
,
(c)
⎛
⎜
⎝
1
1
1
0
1
1
0
0
1
⎞
⎟
⎠,
(d)
⎛
⎜
⎝
−3
1
0
1
−3
−1
0
1
−3
⎞
⎟
⎠,
(e)
⎛
⎜
⎝
−1
1
1
−2
−2
−2
1
−1
−1
⎞
⎟
⎠,
(f )
⎛
⎜
⎜
⎜
⎝
2
−1
1
2
0
2
0
1
0
0
2
−1
0
0
0
2
⎞
⎟
⎟
⎟
⎠.
8.6.8. Write down all possible 4 × 4 Jordan matrices that have only λ = 2 as an eigenvalue.
8.6.9. Write down all 3 × 3 Jordan matrices that have eigenvalues 2 and 5 (and no others).
8.6.10. Write down a formula for the inverse of a Jordan block matrix.
Hint: Try some small examples ﬁrst to help in ﬁguring out the pattern.
8.6.11. True or false: If A is complete, every generalized eigenvector is an ordinary eigenvector.
8.6.12. True or false: Every generalized eigenvector belongs to a Jordan chain.
8.6.13. True or false: If w is a generalized eigenvector of A, then w is a generalized eigenvector
of every power Aj, for j ∈N, thereof.
8.6.14.True or false: If w is a generalized eigenvector of index k of A, then w is an ordinary
eigenvector of Ak.
♦8.6.15. Suppose you know all eigenvalues of a matrix as well as their algebraic and geometric
multiplicities. Can you determine the matrix’s Jordan canonical form?
8.6.16. True or false: If w1, . . . , wj is a Jordan chain for a matrix A, so are the scalar
multiples cw1, . . . , cwj for all c ̸= 0.
8.6.17. Let A and B be n × n matrices. According to Exercise 8.2.23, the matrix products AB
and B A have the same eigenvalues. Do they have the same Jordan form?

453
8.6.18. True or false: If the Jordan canonical form of A is J, then that of A2 is J2.
♦8.6.19.(a) Give an example of a matrix A such that A2 has an eigenvector that is not an
eigenvector of A.
(b) Show that, in general, every eigenvalue of A2 is the square of an
eigenvalue of A.
♦8.6.20.(a) Prove that a Jordan block matrix J0,n with zero diagonal entries is nilpotent,
as in Exercise 1.3.12. (b) Prove that a Jordan matrix is nilpotent if and only if all its
diagonal entries are zero. (c) Prove that a matrix is nilpotent if and only if its Jordan
canonical form is nilpotent.
(d) Explain why a matrix is nilpotent if and only if
its only eigenvalue is 0.
8.6.21. Let J be a Jordan matrix. (a) Prove that Jk is a complete matrix for some k ≥1 if
and only if either J is diagonal, or J is nilpotent with Jk = O. (b) Suppose that A is an
incomplete matrix such that Ak is complete for some k ≥2. Prove that Ak = O, and hence
A is nilpotent. (A simpler version of this problem appears in Exercise 8.3.8.)
♥8.6.22. Cayley–Hamilton Theorem: Let pA(λ) = det(A −λ I ) be the characteristic polynomial
of A. (a) Prove that if D is a diagonal matrix, then† pD(D) = O.
Hint: Leave pD(λ) in factored form.
(b) Prove that if A is complete, then pA(A) = O.
(c) Prove that if J is a Jordan block, then pJ(J) = O. (d) Prove that this also holds if J is
a Jordan matrix. (e) Prove the Cayley–Hamilton Theorem: a square matrix A satisﬁes its
own characteristic equation: pA(A) = O.
♥8.6.23. Minimal polynomial: Let A be an n × n matrix. By deﬁnition, the minimal polynomial
of A is the monic polynomial mA(t) = tk + ck−1tk−1 + · · · + c1t + c0 of minimal
degree k that annihilates A, so mA(A) = Ak + ck−1Ak−1 + · · · + c1A + c0 I = O.
(a) Prove that the monic minimal polynomial mA is unique. (b) (c) Prove that if r(t) is
any other polynomial such that r(A) = O, then r(t) = q(t)mA(t) for some polynomial q(t).
(d) Prove that the matrix’s minimal polynomial is a factor of its characteristic polynomial,
so pA(t) = qA(t)mA(t) for some polynomial qA(t). Hint: Use the Cayley–Hamilton
Theorem in Exercise 8.6.22. (e) Prove that if A has all distinct eigenvalues, then pA = mA.
(f ) Prove that pA = mA if and only if no two Jordan blocks have the same eigenvalue.
♦8.6.24. Prove Lemma 8.54.
♦8.6.25. Prove that the n vectors constructed in the proof of Theorem 8.51 are linearly
independent and hence form a Jordan basis.
Hint: Suppose that some linear combination vanishes. Apply B to the equation, and then
use the fact that we started with a Jordan basis for W = img B.
8.6.26. Find all invariant subspaces of the following matrices: (a)

4
0
−3
4
	
, (b)

−1
−1
4
−5
	
,
(c)
⎛
⎜
⎝
0
−1
1
1
1
−1
3
3
−4
⎞
⎟
⎠,
(d)
⎛
⎜
⎝
3
0
1
0
1
0
0
0
3
⎞
⎟
⎠,
(e)
⎛
⎜
⎜
⎜
⎝
1
0
1
0
0
1
0
0
0
0
1
0
0
1
0
1
⎞
⎟
⎟
⎟
⎠,
(f )
⎛
⎜
⎜
⎜
⎝
−1
0
1
2
0
1
0
1
−1
−4
1
−2
0
1
0
1
⎞
⎟
⎟
⎟
⎠.
♥8.6.27. The method for constructing a Jordan basis in Example 8.52 is simpliﬁed due to the
fact that each eigenvalue admits only one Jordan block. On the other hand, the method
used in the proof of Theorem 8.51 is rather impractical Devise a general method for
constructing a Jordan basis for an arbitrary matrix, paying careful attention to how the
Jordan chains having the same eigenvalue are found.
†
See Exercise 1.2.35 for the basics of matrix polynomials.
8.6 Incomplete Matrices

454
8 Eigenvalues and Singular Values
8.7 Singular Values
We have already expounded on the central role played by the eigenvalues and eigenvectors
of a square matrix in both theory and applications. Further evidence will appear in the
subsequent chapters. Alas, rectangular matrices do not have eigenvalues (why?), and so,
at ﬁrst glance, do not appear to possess any quantities of comparable signiﬁcance. But
you no doubt recall that our earlier treatment of least squares minimization problems,
as well as the equilibrium equations for structures and circuits, made essential use of the
symmetric, positive semi-deﬁnite square Gram matrix K = ATA — which can be naturally
formed even when A is not square. Perhaps the eigenvalues of K might play a comparably
important role for general matrices. Since they are not easily related to the eigenvalues of
A — which, in the non-square case, don’t even exist — we shall endow them with a new
name. They were ﬁrst studied by the German mathematician Erhard Schmidt in early
days of the twentieth century, although intimations can be found in Gauss’s work on rigid
body dynamics.
Deﬁnition 8.60. The singular values σ1, . . . , σr of an m × n matrix A are the positive
square roots, σi =

λi > 0, of the nonzero eigenvalues of the associated Gram matrix
K = ATA. The corresponding eigenvectors of K are known as the singular vectors of A.
Since K is necessarily positive semi-deﬁnite, its eigenvalues are always non-negative,
λi ≥0, which justiﬁes the positivity of the singular values of A — independently of
whether A itself has positive, negative, or even complex eigenvalues, or is rectangular and
has no eigenvalues at all. We will follow the standard convention, and always label the
singular values in decreasing order, so that σ1 ≥σ2 ≥· · · ≥σr > 0. Thus, σ1 will always
denote the largest, or dominant, singular value. If K = ATA has repeated eigenvalues, the
singular values of A are repeated with the same multiplicities. As we will see, the number
r of singular values is equal to the rank of the matrices A and K.
Warning. Some texts include the zero eigenvalues of K as singular values of A. We ﬁnd
this to be less convenient for our development, but you should be aware of the diﬀerences
between the two conventions.
Example 8.61.
Let A =

3
5
4
0

. The associated Gram matrix
K = ATA =

3
4
5
0
 
3
5
4
0

=

25
15
15
25

has eigenvalues λ1 = 40, λ2 = 10, and corresponding eigenvectors v1 =

1
1

, v2 =

1
−1

.
Thus, the singular values of A are σ1 =
√
40 ≈6.3246 and σ2 =
√
10 ≈3.1623, with v1, v2
being the singular vectors. Note that the singular values are not its eigenvalues, which are
λ1 = 1
2 (3 +
√
89) ≃6.2170 and λ2 = 1
2 (3 −
√
89) ≃−3.2170, nor are the singular vectors
eigenvectors of A.
Only in the special case of symmetric matrices is there a direct connection between their
singular values and their (necessarily real) eigenvalues.
Proposition 8.62. If A = AT is a symmetric matrix, its singular values are the absolute
values of its nonzero eigenvalues: σi = | λi | > 0; its singular vectors coincide with its
non-null eigenvectors.

8.7 Singular Values
455
Proof : When A is symmetric, K = ATA = A2. So, if
Av = λv,
then
Kv = A2 v = A(λv) = λAv = λ2 v.
Thus, every eigenvector v of A is also an eigenvector of K with eigenvalue λ2. Therefore,
the eigenvector basis of A guaranteed by Theorem 8.32 is also an eigenvector basis for K,
and hence forms a complete system of singular vectors for A.
Q.E.D.
The generalization of the spectral factorization (8.35) to non-symmetric matrices is
known as the singular value decomposition, commonly abbreviated SVD. Unlike the former,
which applies only to square matrices, every nonzero matrix possesses a singular value
decomposition.
Theorem 8.63. A nonzero real m × n matrix A of rank r > 0 can be factored,
A = P Σ QT,
(8.52)
into the product of an m × r matrix P with orthonormal† columns, so P T P = I , the r × r
diagonal matrix Σ = diag (σ1, . . . , σr) that has the singular values of A as its diagonal
entries, and an r × n matrix QT with orthonormal rows, so QT Q = I .
Proof : Let’s begin by rewriting the desired factorization (8.52) as AQ = P Σ. The indi-
vidual columns of this matrix equation are the vector equations
Aqi = σi pi,
i = 1, . . . , r,
(8.53)
relating the orthonormal columns of Q = ( q1 q2 . . . qr ) to the orthonormal columns of
P = ( p1 p2 . . . pr ). Thus, our goal is to ﬁnd vectors p1, . . . , pr and q1, . . . , qr that satisfy
(8.53). To this end, we let q1, . . . , qr be orthonormal eigenvectors of the Gram matrix
K = ATA corresponding to the non-zero eigenvalues, which, according to Proposition 8.37
form a basis for img K = coimg A, of dimension r = rank K = rank A.
Thus, by the
deﬁnition of the singular values,
ATAqi = K qi = σ2
i qi,
i = 1, . . ., r.
(8.54)
We claim that the image vectors wi = Aqi are automatically orthogonal. Indeed, in
view of the orthonormality of the qi combined with (8.54),
wi · wj = wT
i wj = (Aqi)TAqj = qT
i ATAqj = σ2
j qT
i qj = σ2
j qi · qj =
 0,
i ̸= j,
σ2
i ,
i = j.
Consequently, w1, . . . , wr form an orthogonal system of vectors having respective norms
∥wi ∥=

wi · wi = σi.
We conclude that the associated unit vectors
pi = wi
σi
= Aqi
σi
,
i = 1, . . ., r,
(8.55)
form an orthonormal set of vectors satisfying the required equations (8.53).
Q.E.D.
†
Throughout this section, we exclusively use the Euclidean dot product and norm.

456
8 Eigenvalues and Singular Values
Remark. If A has distinct singular values, its singular value decomposition (8.52) is almost
unique, modulo simultaneously changing the signs of one or more corresponding columns of
Q and P. Matrices with repeated singular values have more freedom in their singular value
decomposition, since one can choose singular vectors using diﬀerent orthonormal bases of
each eigenspace of the Gram matrix AT A.
Observe that, taking the transpose of (8.52) and noting that ΣT = Σ is diagonal, we
obtain
AT = Q Σ P,
(8.56)
which is a singular value decomposition of the transposed matrix AT . In particular, we
obtain the following result:
Corollary 8.64. A matrix A and its transpose AT have the same singular values.
Note that their singular vectors are not the same; indeed, those of A are the orthonormal
columns of Q, whereas those of AT are the orthonormal columns of P, which are related
by (8.53). Thus,
AT pi = σi qi,
i = 1, . . ., r,
(8.57)
which is also a consequence of (8.54).
Furthermore, the singular value decomposition (8.52) serves to diagonalize the Gram
matrix; indeed, since P T P = I , we have
QT K Q = QT ATAQ = QT AT P T P AQ = (P AQ)T (P AQ) = ΣT Σ = Σ2,
(8.58)
because Σ is diagonal. If A has maximal rank n, then Q is an n × n orthogonal matrix,
and so (8.58) implies that the linear transformation deﬁned by the Gram matrix K is
diagonalized when expressed in terms of the orthonormal basis formed by the singular
vectors. If r = rank A < n, then one can supplement the r singular vectors u1, . . . , ur
with n −r unit vectors ur+1, . . . , un ∈ker K = ker A so as to form an orthonormal basis
of Rn. In terms of this basis, the Gram matrix assumes diagonal form with the r nonzero
squared singular values (nonzero eigenvalues of K) as its ﬁrst r diagonal entries, while the
remaining diagonal entries are all 0, in accordance with the Spectral Theorem 8.38.
Example 8.65.
For the matrix A =

3
5
4
0

in Example 8.61, the orthonormal eigen-
vector basis of K = ATA =

25
15
15
25

is given by the unit singular vectors q1 =

1
√
2
1
√
2

and q2 =

−
1
√
2
1
√
2

. Thus, Q =

1
√
2
−
1
√
2
1
√
2
1
√
2

. Next, according to (8.55),
p1 = Aq1
σ1
=
1
√
40

4
√
2
2
√
2

=

2
√
5
1
√
5

,
p2 = Aq2
σ2
=
1
√
10

√
2
−2
√
2

=

1
√
5
−
2
√
5

,
and thus P =

2
√
5
1
√
5
1
√
5
−
2
√
5

. You may wish to validate the resulting singular value fac-
torization
A =

3
5
4
0

=

2
√
5
1
√
5
1
√
5
−2
√
5
  √
40
0
0
√
10
 
1
√
2
1
√
2
−
1
√
2
1
√
2

= P Σ QT .

8.7 Singular Values
457
The singular value decomposition is revealing some interesting new geometrical infor-
mation concerning the action of matrices, further supplementing the discussion begun in
Section 2.5 and continued in Section 4.4.
Proposition 8.66. Given the singular value decomposition A = P Σ QT, the columns
q1, . . . , qr of Q form an orthonormal basis for coimg A, while the columns p1, . . . , pr of P
form an orthonormal basis for img A.
Proof :
The ﬁrst part of the proposition was proved during the course of the proof of
Theorem 8.63. Moreover, the vectors pi = A(qi/σi) for i = 1, . . . , r are mutually orthogo-
nal, of unit length, and belong to img A, which has dimension r = rank A. They therefore
form an orthonormal basis for the image.
Q.E.D.
If A is a nonsingular n × n matrix, then the matrices P, Σ, Q appearing in its singular
value decomposition (8.52) are all of size n × n. If we interpret them as linear transfor-
mations of Rn, the two orthogonal matrices represent rigid rotations/reﬂections, while the
diagonal matrix Σ represents a combination of simple stretches, by an amount given by
the singular values, in the orthogonal coordinate directions. Thus, every invertible linear
transformation on Rn can be decomposed into a rotation/reﬂection QT , followed by the
stretching transformation along the coordinate axes represented by Σ, followed by another
rotation/reﬂection P. See also Exercise 8.5.28.
In the more general rectangular case, the matrix QT represents an orthogonal projection
from Rn to coimg A, the matrix Σ continues to represent a stretching transformation within
this r-dimensional subspace, while P maps the result to img A ⊂Rm. We already noted in
Section 4.4 that the linear transformation L: Rn →Rm deﬁned by matrix multiplication,
L[x] = A x, can be interpreted as a projection from Rn to coimg A followed by an invertible
map from coimg A to img A. The singular value decomposition tells us that not only is the
latter map invertible, it is simply a combination of stretches in the r mutually orthogonal
singular directions q1, . . . , qr, whose magnitudes equal the nonzero singular values. In this
way, we have at last reached a complete understanding of the subtle geometry underlying
the simple operation of multiplying a vector by a matrix!
Finally, we note that practical numerical algorithms for computing singular values and
the singular value decomposition can be found in Chapter 9 and in [32, 66, 90].
The Pseudoinverse
The singular value decomposition enables us to substantially generalize the concept of
a matrix inverse. The pseudoinverse was ﬁrst deﬁned by the American mathematician
Eliakim Moore in the 1920’s and rediscovered by the British mathematical physicist Sir
Roger Penrose in the 1950’s, and often has their names attached.
Deﬁnition 8.67. The pseudoinverse of a nonzero m × n matrix with singular value de-
composition A = P Σ QT is the n × m matrix A+ = Q Σ−1P T .
Note that the latter equation is the singular value decomposition of the pseudoinverse
A+, and hence its nonzero singular values are the reciprocals of the nonzero singular values
of A. The only matrix without a pseudoinverse is the zero matrix O. If A is a non-singular
square matrix, then its pseudoinverse agrees with its ordinary inverse. Indeed, since in this
case both P and Q are square orthogonal matrices, it follows that
A−1 = (P Σ QT)−1 = (Q−1)T Σ−1 P −1 = Q Σ−1 P T = A+,

458
8 Eigenvalues and Singular Values
where we used the fact that the inverse of an orthogonal matrix is equal to its transpose.
More generally, if A has linearly independent columns, or, equivalently, ker A = {0}, then
we can bypass the singular value decomposition to compute its pseudoinverse.
Lemma 8.68. Let A be an m × n matrix of rank n. Then
A+ = (ATA)−1AT .
(8.59)
Proof : Replacing A by its singular value decomposition (8.52), we ﬁnd
ATA = (P Σ QT)T (P Σ QT) = Q Σ P TP Σ QT = Q Σ2 QT ,
(8.60)
since Σ = ΣT is a diagonal matrix, while P T P = I , since the columns of P are orthonormal.
This is merely the spectral factorization (8.35) of the Gram matrix ATA — which we in
fact already knew from the original deﬁnition of the singular values and vectors. Now if A
has rank n, then Q is an n × n orthogonal matrix, and so Q−1 = QT . Therefore,
(ATA)−1AT =(Q Σ−2 QT )−1(P ΣQT)T =(Q Σ−2 QT )(Q ΣP T)=Q Σ−1P T =A+.
Q.E.D.
If A is square and nonsingular, then, as we know, the solution to the linear system
A x = b is given by x⋆= A−1b. For a general coeﬃcient matrix, the vector x⋆= A+ b
obtained by applying the pseudoinverse to the right-hand side plays a distinguished role
— it is, in fact, the least squares solution to the system under the Euclidean norm.
Theorem 8.69. Consider the linear system A x = b. Let x⋆= A+ b, where A+ is the
pseudoinverse of A. If ker A = {0}, then x⋆is the (Euclidean) least squares solution to
the linear system. If, more generally, ker A ̸= {0}, then x⋆= A+ b ∈coimg A is the least
squares solution that has the minimal Euclidean norm among all vectors that minimize the
least squares error ∥A x −b∥2.
Proof : To show that x⋆= A+ b is the least squares solution to the system, we must,
according to Theorem 5.11 check that it satisﬁes the normal equations ATAx⋆= AT b. If
rank A = n, so ATA is nonsingular, this follows immediately from (8.59). More generally,
combining (8.60), the deﬁnition of the pseudoinverse, and the fact that Q has orthonormal
columns, so QT Q = I , yields
ATAx⋆= ATAA+ b = (Q Σ2 QT )(Q Σ−1P T )b = Q Σ P Tb = AT b.
This proves that x⋆solves the normal equations, and hence, by Theorem 5.11 and Exercise
5.4.11, minimizes the least squares error. Moreover,
x⋆= A+ b = Q Σ−1 P T b
= Qc = c1 q1 + · · · + cr qr,
where
c = ( c1, . . . cr )T = Σ−1P T b.
Thus, x⋆is a linear combination of the singular vectors, and hence, by Proposition 8.66
and Theorem 4.50, x⋆∈coimg A is the solution with minimal norm; the most general least
squares solution has the form x = x⋆+ z for arbitrary z ∈ker A.
Q.E.D.
Example 8.70.
Let us use the pseudoinverse to solve the linear system A x = b, with
A =
⎛
⎜
⎝
1
2
−1
3
−4
1
−1
3
−1
2
−1
0
⎞
⎟
⎠,
b =
⎛
⎜
⎝
1
0
−1
2
⎞
⎟
⎠.

8.7 Singular Values
459
In this case, ker A ̸= {0}, and so we are not able use the simpler formula (8.59); thus,
we begin by establishing the singular value decomposition of A. The corresponding Gram
matrix
K = ATA =
⎛
⎝
15
−15
3
−15
30
−9
3
−9
3
⎞
⎠
has eigenvalues and eigenvectors
λ1 = 24 + 3
√
34 ≃41.4929,
λ2 = 24 −3
√
34 ≃6.5071,
λ3 = 0,
v1 ≃
⎛
⎝
2.1324
−3.5662
1.
⎞
⎠,
v2 ≃
⎛
⎝
−2.5324
−1.2338
1.
⎞
⎠,
v3 =
⎛
⎝
1
2
5
⎞
⎠.
The singular values are the square roots of the positive eigenvalues, and so
σ1 =

λ1 ≃6.4415,
σ2 =

λ2 ≃2.5509,
which are used to construct the diagonal singular value matrix Σ ≃

6.4415
0
0
2.5509

.
Note that A has rank 2, because it has just two singular values. The ﬁrst two eigenvectors
of K are the singular vectors of A, and we use the normalized (unit) singular vectors to
form the columns of Q = (q1 q2) ≃
⎛
⎝
.4990
−.8472
−.8344
−.4128
.2340
.3345
⎞
⎠. Next, we apply A to the singular
vectors and divide by the corresponding singular value as in (8.55); the resulting vectors
p1 = Aq1
σ1
≃
⎛
⎜
⎝
−.2180
.7869
−.5024
.2845
⎞
⎟
⎠,
p2 = Aq2
σ2
≃
⎛
⎜
⎝
−.7869
−.2180
−.2845
−.5024
⎞
⎟
⎠,
will form the orthonormal columns of P = (p1, p2), and, as you can verify,
A = P ΣQT ≃
⎛
⎜
⎝
−.2180
−.7869
.7869
−.2180
−.5024
−.2845
.2845
−.5024
⎞
⎟
⎠

6.4415
0
0
2.5509
 
.4990
−.8344
.2340
−.8472
−.4128
.3345

is the singular value decomposition of our coeﬃcient matrix. Its pseudoinverse is immedi-
ately computed:
A+ = Q Σ−1P T ≃
⎛
⎝
.2444
.1333
.0556
.1889
.1556
−.0667
.1111
.0444
−.1111
0
−.0556
−.0556
⎞
⎠.
Finally, according to Theorem 8.69, the least squares solution to the original linear system
of minimal Euclidean norm is
x⋆= A+b ≃
⎛
⎝
.5667
.1333
−.1667
⎞
⎠.
The Euclidean Matrix Norm
Singular values allow us to ﬁnally write down a formula for the natural matrix norm induced
by the Euclidean norm (or 2 norm) on Rn, as deﬁned in Theorem 3.20.

460
8 Eigenvalues and Singular Values
Theorem 8.71. Let ∥·∥2 denote the Euclidean norm on Rn. Let A be a nonzero matrix
with singular values σ1 ≥
· · ·
≥σr. Then the Euclidean matrix norm of A equals its
dominant (largest) singular value:
∥A∥2 = max { ∥Au∥2 | ∥u∥2 = 1 } = σ1,
while
∥O∥2 = 0.
(8.61)
Proof : Let q1, . . . , qn be an orthonormal basis of Rn consisting of the singular vectors
q1, . . . , qr along with an orthonormal basis qr+1, . . . , qn of ker A. Thus, by (8.57),
Aqi =
 σi pi,
i = 1, . . . , r,
0,
i = r + 1, . . ., n,
where p1, . . . , pr form an orthonormal basis for img A. Suppose u is any unit vector, so
u = c1 q1 + · · · + cn qn,
where
∥u∥=

c2
1 + · · · + c2n = 1,
thanks to the orthonormality of the basis vectors q1, . . . , qn and the general Pythagorean
formula (4.5). Then
Au = c1 σ1 p1 + · · · + cr σr pr,
and hence
∥Au∥=

c2
1 σ2
1 + · · · + c2
r σ2
r ,
since p1, . . . , pn are also orthonormal. Now, since σ1 ≥σ2 ≥· · · ≥σr, we have
∥Au∥2 =

c2
1 σ2
1 + · · · + c2
r σ2
r ≤

c2
1 σ2
1 + · · · + c2
r σ2
1
= σ1

c2
1 + · · · + c2
r ≤σ1

c2
1 + · · · + c2
n = σ1.
Moreover, if c1 = 1, c2 = · · · = cn = 0, then u = q1, and hence ∥Au∥2 = ∥Aq1 ∥2 =
∥σ1 p1 ∥2 = σ1. This implies the desired formula (8.61).
Q.E.D.
Example 8.72.
Consider the matrix A =
⎛
⎜
⎝
0
−1
3
1
3
1
4
0
1
2
2
5
1
5
0
⎞
⎟
⎠.
The corresponding Gram
matrix
ATA ≃
⎛
⎝
.2225
.0800
.1250
.0800
.1511
−.1111
.1250
−.1111
.3611
⎞
⎠,
has eigenvalues λ1 ≃.4472, λ2 ≃.2665, λ3 ≃.0210, and hence the singular values of A are
their square roots: σ1 ≃.6687, σ2 ≃.5163, σ3 ≃.1448. The Euclidean matrix norm of A
is the largest singular value, and so ∥A∥2 ≃.6687.
Condition Number and Rank
Not only do the singular values provide a compelling geometric interpretation of the action
of a matrix on vectors, they also play a key role in modern computational algorithms. The
relative magnitudes of the singular values can be used to distinguish well-behaved linear
systems from ill-conditioned systems, which are more challenging to solve accurately. This
information is quantiﬁed by the condition number of the matrix.
Deﬁnition 8.73. The condition number of a nonsingular n×n matrix is the ratio between
its largest and smallest singular values: κ(A) = σ1/σn.

8.7 Singular Values
461
Since the number of singular values equals the matrix’s rank, an n × n matrix with
fewer than n singular values is singular, and is said to have condition number ∞. A matrix
with a very large condition number is close to singular, and designated as ill-conditioned;
in practical terms, this occurs when the condition number is larger than the reciprocal
of the machine’s precision, e.g., 107 for typical single-precision arithmetic. As the name
implies, it is much harder to solve a linear system A x = b when its coeﬃcient matrix is
ill-conditioned and hence close to singular.
Determining the rank of a large (square or rectangular) matrix can be a numerical
challenge. Small numerical inaccuracies can have an unpredictable eﬀect. For example, the
rank 1 matrix A =
⎛
⎝
1
1
−1
2
2
−2
3
3
−3
⎞
⎠is very close to A =
⎛
⎝
1.00001 1.
−1.
2.
2.00001 −2.
3.
3.
−3.00001
⎞
⎠,
which has rank 3 and so is nonsingular.
On the other hand, the latter matrix is very
close to singular, and this is highlighted by its singular values, which are σ1 ≈6.48075
while σ2 ≈σ3 ≈.000001. The fact that the second and third singular values are very
small indicates that A is close to a matrix of rank 1 and should be viewed as a numerical
(or experimental) perturbation of such a matrix. Thus, an eﬀective practical method for
computing the rank of a matrix is to ﬁrst assign a threshold, e.g., 10−5, to distinguish
small singular values, and then treat any singular value lying below the threshold as if it
were zero.
This idea is justiﬁed by the following theorem, which gives a mechanism for constructing
the closest low rank approximations to a given matrix A, as measured in the Euclidean
matrix norm.
Theorem 8.74. Let the m × n matrix A have rank r and singular value decomposition
A = P Σ QT . Given 1 ≤k ≤r, let Σk denote the upper left k × k diagonal submatrix of
Σ containing the largest k singular values on its diagonal. Let Qk denote the n × k matrix
formed from the ﬁrst k columns of Q, which are the ﬁrst k orthonormal singular vectors of
A, and let Pk be the m × k matrix formed from the ﬁrst k columns of P. Then the m × n
matrix Ak = PkΣkQT
k has rank k. Moreover, Ak is the closest rank k matrix to A in the
sense that, among all m × n matrices B of rank k, the Euclidean matrix norm ∥A −B ∥is
minimized when B = Ak.
Proof : The fact that Ak has rank k is clear, since, by construction, its singular values
are σ1, . . . , σk. Let Σk denote the r × r diagonal matrix whose ﬁrst k diagonal entries are
σ1, . . . , σk and whose last r −k diagonal entries are all 0. Clearly Ak = P ΣkQT since
the additional zero entries have no eﬀect on the product. Moreover, Σ −Σk is a diagonal
matrix whose ﬁrst k diagonal entries are all 0 and whose last r −k diagonal entries are
σk+1, . . . , σr. Thus, the diﬀerence A−Ak = P (Σ−Σk)QT has singular values σk+1, . . ., σr.
Since σk+1 is the largest of these, Theorem 8.71 implies that
∥A −Ak ∥= σk+1.
We now prove that this is the smallest possible among all m × n matrices B of rank k.
For such a matrix, according to the Fundamental Theorem 2.49, dim ker B = n −k. Let
Vk+1 ⊂Rn denote the (k + 1)-dimensional subspace spanned by the ﬁrst k + 1 singular
vectors q1, . . . , qk+1 of A. Since the dimensions of the subspaces Vk+1 and ker B sum up
to k + 1 + n −k = n + 1 > n, their intersection is a nontrivial subspace, and hence we can

462
8 Eigenvalues and Singular Values
ﬁnd a non-zero unit vector
u⋆= c1q1 + · · · + ck+1qk+1 ∈Vk+1 ∩ker B.
Thus, since q1, . . . , qk+1 are orthonormal,
∥u⋆∥= c2
1 + · · · + c2
k+1 = 1,
and, moreover,
B u⋆= 0,
which implies
(A −B)u⋆= Au⋆= c1Aq1 + · · · + ck+1Aqk+1 = c1σ1 p1 + · · · + ck+1σk+1 pk+1.
Since p1, . . . , pk+1 are also orthonormal,
∥(A −B)u⋆∥2 = c2
1σ2
1 + · · · + c2
k+1σ2
k+1 ≥(c2
1 + · · · + c2
k+1)σ2
k+1 = σ2
k+1.
Thus, using the deﬁnition (3.39) of the Euclidean matrix norm
∥A −B ∥= max{ ∥(A −B)u∥| ∥u∥= 1 } ≥∥(A −B)u⋆∥≥σk+1.
This proves that σk+1 minimizes ∥A −B ∥among all such matrices B.
Q.E.D.
Remark. One cannot do any better with a matrix of lower rank, i.e., ∥A −B ∥is also
minimized when B = Ak among all matrices with rank B ≤k. Justifying this statement is
left to Exercise 8.7.19.
Observe that the closest rank k approximating matrix Ak is unique unless the (k + 1)st
singular value equals the kth one: σk = σk+1, in which case one can replace the last
columns of Pk and Qk with those coming from the (k + 1)st singular vectors. To compute
the rank k approximating matrix Ak, we need only compute the largest k singular values
σ1, . . . , σk of A, which form the diagonal entries of Σk, and corresponding singular unit
vectors q1, . . . , qk, which form the columns of Qk. The columns of Pk are formed by their
images p1 = Aq1/σ1, . . . , pk = Aqk/σk.
Consequently, when solving an ill-conditioned linear system A x = b, a common and
eﬀective regularization strategy is to eliminate all “insigniﬁcant” singular values below a
speciﬁed cut-oﬀ, replacing A by its rank k approximation Ak speciﬁed by Theorem 8.74,
where k denotes the number of signiﬁcant singular values. Applying the corresponding
approximating pseudoinverse A+
k = Qk Σ−1
k P T
k to solve for x⋆= A+
k b will, in favorable
situations, eﬀectively circumvent the eﬀects of ill-conditioning.
Another common application of low rank approximations is in data compression, in
which one replaces a very large data matrix, e.g., one obtained from high-resolution digital
images, by a suitable low rank approximation that captures the essential features of the
data set while reducing overall storage requirements and thereby accelerating subsequent
analysis thereof.
Spectral Graph Theory
Spectral graph theory, [14, 76], refers to the study of the properties of graphs that are
captured by the spectrum, meaning the eigenvalues and singular values, of certain naturally
associated matrices. The graph Laplacian matrix, which we encountered in Section 6.2, is of
particular importance. Recall that it is deﬁned as the Gram matrix, K = AT A, constructed
from the incidence matrix A of any underlying digraph, noting that the directions assigned
to the edges do not aﬀect the ultimate form of K. The eigenvalues of the graph Laplacian
matrix are, by deﬁnition, the squares of the singular values of the incidence matrix.

8.7 Singular Values
463
As we know — see Exercise 2.6.12 — the dimension of the kernel of the incidence
matrix, and hence also thatof its graph Laplacian matrix, equals the number of connected
components of the graph. In particular, a connected graph has a one-dimensional kernel
spanned by the vector ( 1, 1, . . ., 1 )T . The magnitude of its ﬁnal, meaning smallest, singular
value, σr, can be interpreted as a measure of how close the graph is to being disconnected,
since if it were zero (and thus technically not a singular value), the graph would have
(at least) two connected components. This is borne out by numerical experiments, which
demonstrate that a graph with a small ﬁnal singular value σr can be disconnected by
deleting a relatively small number of its edges.
Example 8.75.
Consider the graph sketched in Figure 8.4. Using the indicated vertex
labels, we can construct its graph Laplacian directly using the recipe found in Section 6.2:
K =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
3
−1
−1
−1
0
0
0
0
−1
3
−1
−1
0
0
0
0
−1
−1
3
−1
0
0
0
0
−1
−1
−1
4
−1
0
0
0
0
0
0
−1
3
−1
−1
0
0
0
0
0
−1
3
−1
−1
0
0
0
0
−1
−1
3
−1
0
0
0
0
0
−1
−1
2
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
.
(8.62)
To four decimal places, the eigenvalues are 5.3234, 4., 4., 4., 4., 2.3579, .3187, 0., with
corresponding singular values 2.3073, 2., 2., 2., 2., 1.5356, .5645, 0. The relatively small
value of σ7 = .5645 indicates the graph is not especially well connected.
Indeed, we
can disconnect it by erasing just the one edge from vertex 4 to vertex 5. The resulting
disconnected graph Laplacian is the block diagonal matrix
K =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
3
−1
−1
−1
0
0
0
0
−1
3
−1
−1
0
0
0
0
−1
−1
3
−1
0
0
0
0
−1
−1
−1
3
0
0
0
0
0
0
0
0
2
−1
−1
0
0
0
0
0
−1
3
−1
−1
0
0
0
0
−1
−1
3
−1
0
0
0
0
0
−1
−1
2
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
whose spectrum is the union of the spectra of the two constituent subgraphs, the left one
having a triple eigenvalue of 4 and a zero eigenvalue, the right one having eigenvalues
4, 2, 2, 0. The singular values are their square roots. Note that these are fairly close to
those of the original connected graph. Such observations are even more striking when one
is dealing with much larger graphs.
Spectral graph theory plays an increasingly important role in theoretical computer sci-
ence and data analysis. Applications include partitioning and coloring graphs, random
graphs and random walks on graphs, routing and networks, and spectral clustering. The
PageRank algorithm that underlies Google’s search engine is based on representing the
web pages on the internet as a gigantic digraph,† which is then viewed as a probabilistic
†
According to our conventions, the internet digraph is not simple, since vertices can have two
directed edges connecting them, one if the ﬁrst web page links to the second and another if the
second links to the ﬁrst.

464
8 Eigenvalues and Singular Values








Figure 8.4.
An Almost Disconnected Graph.
Markov process; see Section 9.3 for further details.
A basic example is the complete graph Gn on n vertices, which has one edge joining every
distinct pair of vertices, and hence is the most connected simple graph; see Exercise 2.6.10.
Its graph Laplacian is easily constructed, and is the n×n matrix Kn = n I −E, where E is
the n × n matrix with every entry equal to 1. Since dim ker E = n −1 (why?), we see that
Kn has a single nonzero eigenvalue, namely λ1 = n, of multiplicity n −1 along with its
zero eigenvalue. Thus, the complete graph on n vertices has n−1 identical singular values:
σ1 = · · · = σn−1 = √n. Motivated by this observation, graphs whose nonzero singular
values are close together are, in a certain sense, very highly connected, and are known
as expander graphs. Expander graphs have many remarkable properties, which underlie
their many applications, including communication networks, error-correcting codes, fault-
tolerant circuits, pseudo-random number generators, Markov processes, statistical physics,
as well as more theoretical disciplines such as group theory and geometry, [45].
Exercises
8.7.1. Find the singular values of the following matrices:
(a)

1
1
0
2
	
,
(b)

0
1
−1
0
	
,
(c)

1
−2
−3
6
	
,
(d)

2
0
0
0
3
0
	
,
(e)

2
1
0
−1
0
−1
1
1
	
,
(f )
⎛
⎜
⎝
1
−1
0
−1
2
−1
0
−1
1
⎞
⎟
⎠.
8.7.2. Write out the singular value decomposition (8.52) of the matrices in Exercise 8.7.1.
8.7.3.(a) Construct the singular value decomposition of the shear matrix A =

1
1
0
1
	
.
(b) Explain how a shear can be realized as a combination of a rotation, and a stretch,
followed by a second rotation.
8.7.4. Find the condition number of the following matrices. Which would you characterize
as ill-conditioned?
(a)

2
−1
−3
1
	
,
(b)

−.999
.341
−1.001
.388
	
,
(c)

1
2
1.001
1.9997
	
,
(d)
⎛
⎜
⎝
−1
3
4
2
10
6
1
2
−3
⎞
⎟
⎠,
(e)
⎛
⎜
⎝
72
96
103
42
55
59
67
95
102
⎞
⎟
⎠,
(f )
⎛
⎜
⎜
⎜
⎝
5
7
6
5
7
10
8
7
6
8
10
9
5
7
9
10
⎞
⎟
⎟
⎟
⎠.
8.7.5. Find the closest rank 1 and rank 2 matrices to the matrices in Exercise 8.7.4.

8.7 Singular Values
465
♠8.7.6. Solve the following systems of equations using Gaussian Elimination with three-digit
rounding arithmetic. Is your answer a reasonable approximation to the exact solution?
Compare the accuracy of your answers with the condition number of the coeﬃcient matrix,
and discuss the implications of ill-conditioning.
(a) 1000x + 999y = 1,
554x + 555y = −1,
(b)
97x + 175y + 83z = 1,
44x + 78y + 37z = 1,
52x + 97y + 46z = 1.
(c)
3.001x + 2.999y + 5z = 1,
−x + 1.002y −2.999z = 2
2.002x + 4y + 2z = 1.002.
♠8.7.7.(a) Compute the singular values and condition numbers of the 2 × 2, 3 × 3, and 4 × 4
Hilbert matrices. (b) What is the smallest Hilbert matrix with condition number larger
than 106?
8.7.8.(a) What are the singular values of a 1 × n matrix? (b) Write down its singular value
decomposition. (c) Write down its pseudoinverse.
8.7.9. Answer Exercise 8.7.8 for an m × 1 matrix.
8.7.10. True or false: Every matrix has at least one singular value.
8.7.11. Explain why the singular values of A are the same as the nonzero eigenvalues of the
positive deﬁnite square root matrix S =
√
ATA , deﬁned in Exercise 8.5.27.
♦8.7.12. Prove that if the square matrix A is nonsingular, then the singular values of A−1 are
the reciprocals of the singular values of A. How are their condition numbers related?
8.7.13. True or false: The singular values of AT are the same as the singular values of A.
♦8.7.14.(a) Let A be a nonsingular matrix. Prove that the product of the singular values of A
equals the absolute value of its determinant: σ1 σ2 · · · σn = | det A |.
(b) Does their sum equal the absolute value of the trace: σ1 + · · · + σn = | tr A |?
(c) Show that if | det A | < 10−k, then its minimal singular value satisﬁes σn < 10−k/n.
(d) True or false: A matrix whose determinant is very small is ill-conditioned.
(e) Construct an ill-conditioned matrix with det A = 1.
8.7.15. True or false: If A is a symmetric matrix, then its singular values are the same as its
eigenvalues.
8.7.16. True or false: If U is an upper triangular matrix whose diagonal entries are all positive,
then its singular values are the same as its diagonal entries.
8.7.17. True or false: The singular values of A2 are the squares σ2
i of the singular values of A.
8.7.18. True or false: If B = S−1AS are similar matrices, then A and B have the same
singular values.
♦8.7.19. Under the assumptions of Theorem 8.74, show that ∥A −B ∥2 is also minimized when
B = Ak among all matrices with rank B ≤k.
♦8.7.20. Suppose A is an m × n matrix of rank r < n. Prove that there exist arbitrarily close
matrices of maximal rank, that is, for every ε > 0 there exists an m × n matrix B with
rank B = nsuch that the Euclidean matrix norm ∥A −B ∥< ε.
8.7.21. True or false: If det A > 1, then A is not ill-conditioned.
8.7.22. Let A =
⎛
⎜
⎝
6
−4
1
−4
6
−1
1
−1
11
⎞
⎟
⎠, and let E = { y = A x | ∥x∥= 1 } be the image of the unit
Euclidean sphere under the linear map induced by A. (a) Explain why E is an ellipsoid
and write down its equation. (b) What are its principal axes and their lengths — the semi-
axes of the ellipsoid? (c) What is the volume of the solid ellipsoidal domain enclosed by E?

466
8 Eigenvalues and Singular Values
♥8.7.23. Let A be a nonsingular 2 × 2 matrix with singular value decomposition A = P ΣQT and
singular values σ1 ≥σ2 > 0. (a) Prove that the image of the unit (Euclidean) circle under
the linear transformation deﬁned by A is an ellipse, E = { A x | ∥x∥= 1 }, whose principal
axes are the columns p1, p2 of P, and whose corresponding semi-axes are the singular
values σ1, σ2.
(b) Show that if A is symmetric, then the ellipse’s principal axes are the
eigenvectors of A and the semi-axes are the absolute values of its eigenvalues. (c) Prove
that the area of E equals π | det A |. (d) Find the principal axes, semi-axes, and area of the
ellipses deﬁned by (i)

0
1
1
1
	
, (ii)

2
1
−1
2
	
, (iii)

5
−4
0
−3
	
.
(e) What happens if
A is singular?
♦8.7.24. Optimization Principles for Singular Values: Let A be any nonzero m × n matrix.
Prove that (a) σ1 = max { ∥Au∥| ∥u∥= 1 }.
(b) Is the minimum the smallest singular
value? (c) Can you design optimization principles for the intermediate singular values?
♦8.7.25. Let A be a square matrix. Prove that its maximal eigenvalue is smaller than its
maximal singular value: max | λi | ≤max σi. Hint: Use Exercise 8.7.24.
8.7.26. Compute the Euclidean matrix norm of the following matrices.
(a)
⎛
⎝
1
2
1
4
1
3
1
6
⎞
⎠, (b)
⎛
⎝
5
3
4
3
−7
6
−5
6
⎞
⎠, (c)
⎛
⎝
2
7
−2
7
−2
7
6
7
⎞
⎠, (d)
⎛
⎝
1
4
3
2
−1
2
5
4
⎞
⎠,
(e)
⎛
⎜
⎜
⎜
⎝
2
7
2
7
−4
7
0
2
7
6
7
2
7
4
7
2
7
⎞
⎟
⎟
⎟
⎠, (f )
⎛
⎜
⎝
0
.1
.8
−.1
0
.1
−.8
−.1
0
⎞
⎟
⎠, (g)
⎛
⎜
⎜
⎜
⎝
1
−2
3
−2
3
1
−1
3
−1
1
3
−2
3
0
⎞
⎟
⎟
⎟
⎠, (h)
⎛
⎜
⎜
⎜
⎝
1
3
0
0
−1
3
0
1
3
0
2
3
1
3
⎞
⎟
⎟
⎟
⎠.
8.7.27. Find a matrix A whose Euclidean matrix norm satisﬁes ∥A2 ∦= ∥A∥2.
♦8.7.28. Let K > 0 be a positive deﬁnite matrix. Characterize the matrix norm induced by the
inner product ⟨x , y ⟩= xT K y. Hint: Use Exercise 8.5.45.
8.7.29. Let A =

1
1
1
−2
	
. Compute the matrix norm ∥A∥using the following norms in R2:
(a) the weighted ∞norm ∥v∥= max{2| v1 |, 3| v2 |}; (b) the weighted 1 norm
∥v∥= 2| v1 | + 3| v2 |; (c) the weighted inner product norm ∥v∥=

2v2
1 + 3v2
2 ;
(d) the norm associated with the positive deﬁnite matrix K =

2
−1
−1
2
	
.
♦8.7.30. Let A be an n × n matrix with singular value vector σ = (σ1, . . . , σr). Prove that
(a) ∥σ ∥∞= ∥A∥2; (b) ∥σ ∥2 = ∥A∥F , the Frobenius norm of Exercise 3.3.51.
Remark. The 1 norm of the singular value vector ∥σ ∥1 also deﬁnes a useful matrix norm,
known as the Ky Fan norm.
♦8.7.31. Let A be an m × n matrix with singular values σ1, . . . , σr. Prove that
r

i=1
σ2
i =
m

i=1
n

j =1
a2
ij.
(8.63)
8.7.32. Let A be a nonsingular square matrix. Prove the following formulas for its condition
number:
(a) κ(A) =
max{ ∥Au∥| ∥u∥= 1 }
min{ ∥Au∥| ∥u∥= 1 }
,
(b) κ(A) = ∥A∥2 ∥A−1 ∥2.
8.7.33. Find the pseudoinverse of the following matrices:
(a)

1
−1
−3
3
	
, (b)

1
−2
2
1
	
,
(c)
⎛
⎜
⎝
2
0
0
−1
0
0
⎞
⎟
⎠, (d)
⎛
⎜
⎝
0
0
1
0
−1
0
0
0
0
⎞
⎟
⎠, (e)

1
−1
1
−2
2
−2
	
, (f )
⎛
⎜
⎝
1
3
2
6
3
9
⎞
⎟
⎠, (g)
⎛
⎜
⎝
1
2
0
0
1
1
1
1
−1
⎞
⎟
⎠.

8.8 Principal Component Analysis
467
8.7.34. Use the pseudoinverse to ﬁnd the least squares solution of minimal norm to the
following linear systems:
(a)
x + y = 1,
3x + 3y = −2;
(b)
x + y + z = 5,
2x −y + z = 2;
(c)
x −3y = 2,
2x + y = −1,
x + y = 0.
♥8.7.35. Prove that the pseudoinverse satisﬁes the following identities:
(a) (A+)+ = A,
(b) AA+A = A, (c) A+AA+ = A+, (d) (AA+)T = AA+, (e) (A+A)T = A+A.
8.7.36. Suppose b ∈img A and ker A = {0}. Prove that x⋆= A+b is the unique solution to
the linear system A x = b. What if ker A ̸= {0}?
8.7.37. Choose a direction for each of the edges and write down the incidence matrix A for the
graph sketched in Figure 8.4. Verify that its graph Laplacian (8.62) equals K = AT A.
8.7.38. Determine the spectrum for the graphs in Exercise 2.6.3.
8.7.39. Determine the spectrum of a graph given by the edges of (i) a triangle; (ii) a square;
(iii) a pentagon. Can you determine the formula for the spectrum of the graph given by an
n sided polygon? Hint: See Exercise 8.2.49.
8.7.40. Determine the spectrum for the trees in Exercise 2.6.9. Can you make any conjectures
about the nature of the spectrum of a graph that is a tree?
8.8 Principal Component Analysis
Singular values and vectors also underlie contemporary statistical data analysis. In partic-
ular, the method of Principal Component Analysis has assumed an increasingly essential
role in a wide range of applications, including data mining, machine learning, image pro-
cessing, speech recognition, semantics, face recognition, and health informatics; see [47]
and the references therein. Given a large data matrix — containing many data points
belonging to a high-dimensional vector space — the singular vectors associated with the
larger singular values indicate the principal components of the data, while small singular
values indicate relatively unimportant components. Projection onto the low-dimensional
subspaces spanned by the dominant singular vectors can expose structure in their otherwise
inscrutable large data sets. The earliest descriptions of the method of Principal Compo-
nent Analysis are to be found in the ﬁrst half of the twentieth century in the work of the
British statistician Karl Pearson, [65], and American statistician Harold Hotelling, [46].
Variance and Covariance
We begin with a brief description of basic statistical concepts. Suppose that x1, . . . , xm ∈R
represent a collection of m measurements of a single physical quantity, e.g., the distance to
a star as measured by various physical apparatuses, the speed of a car at a given instant
measured by a collection of instruments, a person’s IQ as measured by a series of tests,
etc. Experimental error, statistical ﬂuctuations, quantum mechanical eﬀects, numerical
approximations, and the like imply that the individual measurements will almost certainly
not precisely agree. Nevertheless, one wants to know the most likely value of the measured
quantity and the degree of conﬁdence that one has in the proposed value. A variety of
statistical tests have been devised to resolve these issues, and we refer the reader to, for
example, [20, 43, 87].
The most basic collective quantity of such a data set is its mean, or average, denoted
by
x = x1 + · · · + xm
m
.
(8.64)

468
8 Eigenvalues and Singular Values
Figure 8.5.
Variance.
Barring some inherent statistical or experimental bias, the mean can be viewed as the most
likely value, known as the expected value, of the quantity being measured, and thus the
best bet for its actual value. (More generally, if the measurements are sampled from a
known probability distribution, then one works with a suitably weighted average. To keep
the formulas relatively simple, we will assume a uniform distribution throughout, and leave
generalizations for the reader to pursue by consulting the statistical literature.) Once this
has been computed, it will be helpful to normalize the measurements to have mean zero,
which is done by subtracting oﬀtheir mean, letting
ai = xi −x,
i = 1, . . ., m,
with
a = a1 + · · · + am
m
= 0,
(8.65)
represent the deviations of the measurements from their overall mean. It will also help to
assemble these quantities into column vectors:
x =
⎛
⎜
⎜
⎜
⎝
x1
x2
...
xm
⎞
⎟
⎟
⎟
⎠,
a =
⎛
⎜
⎜
⎜
⎝
a1
a2
...
am
⎞
⎟
⎟
⎟
⎠= x −x e,
where
e =
⎛
⎜
⎜
⎝
1
1
...
1
⎞
⎟
⎟
⎠∈Rm,
x = e · x
m .
Thus, we can write the normalized measurement vector as
a =

I −1
m E

x,
(8.66)
where I is the m × m identity matrix and E = e eT is the m × m matrix all of whose
entries equal 1.
The measurement variance tells us how widely the data points are “scattered” about
their mean. As in least squares analysis, this is quantiﬁed by summing the squares of their
deviations, and denoted by
σ2
x = ν

(x1 −x)2 + · · · + (xm −x)2 
= ν (a2
1 + · · · + a2
m) = ν ∥a∥2 = σ2
a,
(8.67)
where we continue to use the usual Euclidean norm† throughout, and ν > 0 is a certain
speciﬁed prefactor, which could also be viewed as an overall weight to the Euclidean norm.
The square root of the variance is known as the standard deviation, and denoted by
σ = σx = σa = √ν ∥a∥.
(8.68)
The prefactor ν can assume diﬀerent values depending upon one’s statistical objectives;
common examples are (a) ν = 1/m for the “na¨ıve” variance; (b) ν = 1/(m −1) (as-
suming m > 1, i.e., there are at least 2 measurements) for an unbiased version; (c) ν =
1/(m + 1) for the minimal mean squared estimation of variance; and (d) more exotic
†
More general probability distributions rely on suitably weighted norms, which can be straight-
forwardly incorporated into the mathematical framework.

8.8 Principal Component Analysis
469
ρxy = −.95
ρxy = −.7
ρxy = 0
ρxy = .7
ρxy = .95
Figure 8.6.
Correlation.
choices, e.g., if one desires an unbiased estimation of standard deviation instead of variance,
cf. [43; p. 349].
Fortunately, apart from the resulting numerical values, the underlying
analysis is independent of the prefactor.
The smaller the variance or standard deviation, the less spread out the measurements,
and hence the more accurately the mean x is expected to approximate the true value of
the physical quantity. Figure 8.5 contains several scatter plots, in which each real-valued
measurement is indicated by a dot and their mean is represented by a small vertical bar.
The left plot shows data with relatively small variance, since the measurements are closely
clustered about their mean, whereas on the right plot, the variance is large because the
data is fairly spread out.
Now suppose we make measurements of several diﬀerent physical quantities. The in-
dividual variances in themselves fail to capture many important features of the resulting
data set. For example, Figure 8.6 shows the scatter plots of data sets each representing
simultaneous measurements of two quantities, as speciﬁed by their horizontal and vertical
coordinates. All have the same variances, both individual and cumulative, but clearly rep-
resent diﬀerent interrelationships between the two quantities. In the central plot, they are
completely uncorrelated, while on either side they are progressively more correlated (or
anti-correlated), meaning that the value of the ﬁrst measurement is a strong indicator of
the value of the second.
This motivates introducing what is known as the covariance between a pair of measured
quantities x = ( x1, x2, . . . , xm )T and y = ( y1, y2, . . . , ym )T as the expected value of the
product of the deviations from their respective means x, y. We set
σxy = ν
m

k=1
(xk −x)(yk −y ) = ν
m

k=1
akbk = ν a · b,
(8.69)
where the normalized vector a = ( a1, a2, . . . , am )T has components ak = xk −x, while
b = ( b1, b2, . . . , bm )T has components bk = yk −y.
Note that, in view of (8.67), the
covariance of a set of measurements with itself is its variance: σxx = σ2
x. The correlation
between the two measurement sets is then deﬁned as
ρxy = σxy
σx σy
,
(8.70)
and is independent of the prefactor ν. There is an overall bound on the correlation, since
the Cauchy–Schwarz inequality (3.18) implies that
| σxy | ≤σx σy,
and hence
−1 ≤ρxy ≤1.
(8.71)
The closer ρxy is to +1, the more the variables are correlated; the closer to −1, the more
they are anti-correlated, while ρxy = 0 when the variables are uncorrelated. In Figure 8.6,

470
8 Eigenvalues and Singular Values
each scatter plot is labelled by its correlation. Statistically independent variables are auto-
matically uncorrelated, but the converse is not necessarily true, since correlation measures
only linear dependencies, and it is possible for nonlinearly dependent variables to never-
theless have zero correlation.
More generally, suppose we are given m measurements of n distinct quantities x1, . . . , xn.
Let X be the m × n data matrix whose entry xij represents the ith value or measurement
of the jth quantity. The column xj = (x1j, . . . , xmj)T of X contains the measurements of
the jth quantity, while the row ξi = ( xi1, . . . , xin ) is the ith data point. For example, if the
data comes from sampling a set S ⊂Rn, each row of the data matrix represents a diﬀerent
sample point ξi ∈S. Similarly, in image analysis, each row of the data matrix represents
an individual image, whose components are, say, gray scale data for the individual pixels,
or color components of pixels — in this case 3 or 4 components per pixel for the RGB or
CMKY color scales — or Fourier or wavelet coeﬃcients representing the image, etc.
The (row) vector containing the various measurement means is
x = (x1, . . . , xn) = 1
m eT X.
(8.72)
We let ai, with entries aij = xij −xj representing the deviations from the mean, denote
the corresponding normalized (mean zero) measurement vectors, which form the columns
of the normalized data matrix
A = (a1, . . . , an) = X −e x =

I −1
m E

X;
(8.73)
cf. (8.66). The fact that the columns of A all have mean zero is equivalent to the statement
that e ∈cokerA. We will call the rows αi = (ai1, . . . , ain) of A the normalized data points.
We next deﬁne the n × n covariance matrix K of the data set, whose entries equal to
the pairwise covariances of the individual measurements:
kij = σxixj = ν ai · aj = ν
m

k=1
akiakj = ν
m

k=1
(xki −xi)(xkj −xj), i, j = 1, . . . , n. (8.74)
The diagonal entries of K are the individual variances: kii = σxixi = σ2
xi. Observe that
the covariance matrix is, up to a factor, the symmetric Gram matrix for the normalized
measurement vectors a1, . . . , an, and hence
K = ν ATA.
(8.75)
Theorem 3.34 tells us that the covariance matrix is always positive semi-deﬁnite, K ≥0,
and is positive deﬁnite, K > 0, unless the columns of A are linearly dependent, or, equiv-
alently, there is a nontrivial exact linear relationship among the normalized measurement
vectors: c1a1 + · · · + cnan = 0 with c1, . . . , cn not all zero — an unlikely event given the
presence of measurement errors and statistical ﬂuctuations.

8.8 Principal Component Analysis
471
Remark. Principal Component Analysis requires that all n variables be measured the
same number, m, of times, producing a rectangular m × n normalized data matrix X, all
of whose entries are speciﬁed. Extending the analysis to missing or unavailable data is a
very active area of contemporary research, which we unfortunately do not have space to
examine here. We refer the interested reader to [24, 28] and the references therein.
The Principal Components
The covariance matrix of a data set (8.75) encodes the information concerning the possible
linear dependencies and interrelationships among the data. However, due to its potentially
large size, it is often not easy to extract the important components and implications.
Nor is visualization a good option, since the scatter plots lie in a high-dimensional space.
Standard or random projections of high-dimensional data onto two- or three-dimensional
subspaces give some limited insight, but the results are highly dependent on the direction
of projection and tend to obscure any underlying structure. For example, projecting the
data sets in Figure 8.6 onto the x- and y-axes produces more or less the same results,
thereby hiding the variety of two-dimensional correlations. A more systematic approach is
to locate the so-called principal components of the data, and this leads us us back to the
singular value decomposition of the data matrix.
The basic idea behind Principal Component Analysis, often abbreviated PCA, is to
focus on directions in which the variance of the data is especially large. Given the m × n
normalized data matrix A, we deﬁne the ﬁrst principal direction as that in which the data
experiences the most variance. By “direction”, we mean a line through the origin in Rn,
and the variance is computed from the orthogonal projection of the data measurements
onto the line. Each line is spanned by a unit vector u = ( u1, u2, . . ., un )T with ∥u∥= 1.
(Actually, there are two unit vectors, ±u, in each line, but, as we will see, it doesn’t
matter which one we choose.) The orthogonal projection of the ith normalized data point
αi = (ai1, . . . , ain) onto the line spanned by u is given by the projection formula (4.41),
namely pi = αiu, i = 1, . . . , m. The result is the projected measurement vector
p =
⎛
⎜
⎜
⎜
⎝
p1
p2
...
pm
⎞
⎟
⎟
⎟
⎠= Au = u1a1 + · · · + unan.
(8.76)
Our goal is to maximize its variance
σ2
p = ν ∥p∥2 = ν ∥Au∥2 = ν (Au)T Au = ν (uTATAu) = ν (uT Ku).
(8.77)
over all possible choices of unit vector u. But, ignoring the irrelevant factor ν > 0, this is
precisely the maximization problem that was solved by Theorem 8.40. We thus immediately
deduce that the ﬁrst principal direction is given by the dominant unit eigenvector u = q1
of the covariance matrix K = ν ATA, or, equivalently, the dominant unit singular vector of
the normalized data matrix A. The maximum variance is, up to a factor, the dominant,
or largest, eigenvalue, or, equivalently, the square of the dominant singular value, namely,
maxu σ2
p = ν λ1 = ν σ2
1, while the dominant singular value is, again up to a factor, the
maximal standard deviation of the projected measurements: maxu σp = √ν σ1.
The second principal direction is assumed to be orthogonal to the ﬁrst, so as to avoid
contaminating it with the already noted direction of maximal variance, and is to be chosen
so that the variance of its projected measurements is maximized among all such orthogonal

472
8 Eigenvalues and Singular Values
directions. Thus, the second principal direction will maximize σ2
p, as given by (8.77), over all
unit vectors u satisfying u·q1 = 0. More generally, given the ﬁrst j −1 principal directions
q1, . . . , qj−1, the jth principal component is in the direction u = qj that maximizes the
variance
σ2
p = ν (uT Ku)
over all vectors u satisfying
∥u∥= 1,
u · q1 = · · · = u · qj−1 = 0.
Theorem 8.42 immediately implies that qj is a unit eigenvector of K associated with its
jth largest eigenvalue λj = σ2
j , which therefore is, up to a factor, the jth principal variance.
Summarizing, we have proved the Fundamental Theorem of Principal Component Analysis.
Theorem 8.76. The jth principal direction of a normalized data matrix A is its jth unit
singular vector qj. The corresponding principal standard deviation √ν σj is proportional
to its jth singular value σj.
In applications, one designates a certain number, say k, of the dominant (largest) vari-
ances ν σ2
1 ≥ν σ2
2 ≥· · · ≥ν σ2
k, as “principal” and the corresponding unit singular vectors
q1, . . . , qk as the principal directions. The value of k depends on the user and on the appli-
cation. For example, in visualization, k = 2 or 3 in order to plot these components of the
data in the plane or in space. More generally, one could specify k based on some overall size
threshold, or where there is a perceived gap in the magnitudes of the variances. Another
choice is such that the principal variances are those that make up some large fraction, e.g.,
μ = 95%, of the total variance:
ν

σ2
1 + · · · + ν σ2
k

= μ

ν
r

i=1
σ2
i

= μν
n

i,j =1
a2
ij = μν
n

i=1
σ2
xi,
(8.78)
where the next-to-last equality follows from (8.63).
Theorem 8.74 says that, in the latter cases, the number of principal components, i.e.,
the number of signiﬁcant singular values, will determine the approximate rank k of the
covariance matrix K and hence the data matrix A, also. Thus, the normalized data (ap-
proximately) lies in a k-dimensional subspace. Further, the variance in any direction or-
thogonal to principal directions is relatively small and hence relatively unimportant. As a
consequence, dimensional reduction by orthogonally projecting the data vectors onto the
k-dimensional subspace spanned by the principal directions (singular vectors) q1, . . . , qk,
serves to eliminate signiﬁcant redundancies.
The coordinates of the data in the jth principal direction are provided by orthogonal
projection, namely the entries of the image vectors Aqj = σjpj. Thus, approximating
the data by its k principal components coincides with the closest rank k approximation
Ak = PkΣkQT
k to the data matrix A given by Theorem 8.74. Moreover, rewriting the data
in terms of the principal coordinates, meaning those supplied by the principal directions,
serves to diagonalize the principal covariance matrix ν Kk = ν AT
k Ak, since, as in (8.58),
QT
k (ν AT
k Ak)Qk = ν Σ2
k,
the result being a diagonal matrix containing the k principal variances along the diago-
nal. This has the important consequence that the covariance between any two principal
components of the data is zero, and thus the principal components are all uncorrelated!
In geometric terms, the original data tends to form an ellipsoid in the high-dimensional
data space, and the principal directions are aligned with its principal semi-axis, thereby
conforming to and exposing the intrinsic geometry of the data set.

8.8 Principal Component Analysis
473
Thus, to perform Principal Component Analysis on a complete data set, consisting of m
measurements of n quantities, one forms the m×n normalized data matrix A whose (i, j)th
entry equals the ith measurement of the jth variable minus the mean of the jth variable. The
principal directions are the ﬁrst k singular vectors of A, meaning the eigenvectors of the
positive (semi-)deﬁnite covariance matrix K = ν AT A, while the principal variances are,
up to the overall factor ν, the corresponding eigenvalues or equivalently the squares of the
singular values. The principal coordinates are given by the entries of the resulting matrix
PkΣk whose columns are the image vectors σjpj = Aqj specifying the measurements in
the principal directions.
Exercises
Note: For simplicity, take the prefactor ν = 1 unless otherwise indicated.
8.8.1. Find the mean, the variance, and the standard deviation of the following data sets:
(a) 1.1, 1.3, 1.5, 1.55, 1.6, 1.9, 2, 2.1; (b) 2., .9, .7, 1.5, 2.6, .3, .8, 1.4; (c) −2.9, −.5, .1, −1.5,
−3.6, 1.3, .4, −.7; (d) 1.1, .2, .1, .6, 1.3, −.4, −.1, .4; (e) .9, −.4, −.8, ., 1., −1.6, −1.2, −.7.
8.8.2.
Find the mean, the variance, and the standard deviation of the data sets
{ f(x) | x = i/10, i = −10, . . . , 10 } associated with the following functions f(x):
(a) 3x + 1,
(b) x2,
(c) x3 −2x,
(d) e−x,
(e) tan−1 x.
8.8.3. Determine the variance and standard deviation of the normally distributed data points
{ e−x2/σ | x = i/10, i = −10, . . . , 10 } for σ = 1, 2, and 10.
8.8.4. Prove that σxy = xy −x y, where x and y are the means of {xi } and {yi }, respectively,
while xy denotes the mean of the product variable {xiyi }.
♦8.8.5. Show that one can compute the variance of a set of measurements without reference to
the mean by the following formula
σ2
x =
ν
2m
m

i=1
m

j =1
(xi −xj)2 = ν
m

i<j
(xi −xj)2.
8.8.6. Let A be an m × n matrix that is normalized, meaning that each of its column sums is
zero. Show that AB, where B is any n × k matrix, is also normalized.
8.8.7. Given a singular value decomposition A = P Σ QT , prove that if the columns of A have
zero mean, then so do the columns of P Σ.
♣8.8.8. Construct the 5 × 5 covariance matrix for the 5 data sets in Exercise 8.8.1 and ﬁnd its
principal variances and principal directions. What do you think is the dimension of the
subspace the data lies in?
♣8.8.9. For each of the following subsets S ⊂R3, (i) Compute a fairly dense sample of data
points zi ∈S; (ii) ﬁnd the principal components of your data set, using μ = .95 in
the criterion in (8.78); (iii) using your principal components, estimate the dimension of
the set S. Does your estimate coincide with the actual dimension? If not, explain any
discrepancies.
(a) The line segment S = { ( t + 1, 3t −1, −2t )T | −1 ≤t ≤1 };
(b) the set of points z on the three coordinate axes with Euclidean norm ∥z ∥≤1;
(c) the set of “probability vectors” S = { ( x, y, z )T | 0 ≤x, y, z ≤1, x + y + z = 1 };
(d) the unit ball S = {∥z ∥≤1} for the Euclidean norm;
(e) the unit sphere S = {∥z ∥= 1} for the Euclidean norm;
(f ) the unit ball S = {∥z ∥∞≤1} for the ∞norm;
(g) the unit sphere S = {∥z ∥∞= 1} for the ∞norm.

474
8 Eigenvalues and Singular Values
♣8.8.10. Using the Euclidean norm, compute a fairly dense sample of points on the unit sphere
S = { x ∈R3 | ∥x∥= 1 }. (a)
Set μ = .95 in (8.78), and then ﬁnd the principal
components of your data set. Do they indicate the two-dimensional nature of the sphere?
If not, why not? (b) Now look at the subset of your data that is within a distance r > 0 of
the north pole, i.e., ∥x −( 0, 0, 1 )T ∥≤r, and compute its principal components. How small
does r need to be to reveal the actual dimension of S? Interpret your calculations.
♦8.8.11. Show that the ﬁrst principal direction q1 can be characterized as the direction of the
line that minimizes the sums of the squares of its distances to the data points.
♥8.8.12. Let xi = (xi, yi), i = 1, . . . , m, be a set of data points in the plane. Suppose
L∗⊂R2 is the line that minimizes the sums of the squares of the distances from the data
points to it, i.e., dist(x, L) =
m

i=1
dist(xi, L), among all lines L ⊂R2. (a) Prove that
x = (x, y) ∈L∗. (b) Use Exercise 8.8.11 to ﬁnd L∗. (c) Apply your result to the data
points in Example 5.14 and compare the resulting line L∗with the least squares line that
was found there.

Chapter 9
Iteration
Iteration, meaning the repeated application of a process or function, appears in a surpris-
ingly wide range of applications. Discrete dynamical systems, in which the time variable
has been “quantized” into individual units (seconds, days, years, etc.) are modeled by iter-
ative systems. Most numerical solution algorithms, for both linear and nonlinear systems,
are based on iterative procedures. Starting with an initial guess, the successive iterates
lead to closer and closer approximations to the true solution. For linear systems of equa-
tions, there are several iterative solution algorithms that can, in favorable situations, be
employed as eﬃcient alternatives to Gaussian Elimination. Iterative methods are partic-
ularly eﬀective for solving the very large, sparse systems arising in the numerical solution
of both ordinary and partial diﬀerential equations. All practical methods for computing
eigenvalues and eigenvectors rely on some form of iteration. A detailed historical develop-
ment of iterative methods for solving linear systems and eigenvalue problems can be found
in the recent survey paper [84]. Probabilistic iterative models known as Markov chains
govern basic stochastic processes and appear in genetics, population biology, scheduling,
internet search, ﬁnancial markets, and many more.
In this book, we will treat only iteration of linear systems.
(Nonlinear iteration is
of similar importance in applied mathematics and numerical analysis, and we refer the
interested reader to [40, 66, 79] for details.) Linear iteration coincides with multiplication
by successive powers of a matrix; convergence of the iterates depends on the magnitude of
its eigenvalues. We present a variety of convergence criteria based on the spectral radius,
on matrix norms, and on eigenvalue estimates provided by the Gershgorin Theorem.
We will then turn our attention to some classical iterative algorithms that can be used
to accurately approximate the solutions to linear algebraic systems. The Jacobi Method is
the simplest, while an evident serialization leads to the Gauss–Seidel Method. Completely
general convergence criteria are hard to formulate, although convergence is assured for the
important class of strictly diagonally dominant matrices that arise in many applications.
A simple modiﬁcation of the Gauss–Seidel Method, known as Successive Over-Relaxation
(SOR), can dramatically speed up the convergence rate.
In the following Section 9.5 we discuss some practical methods for computing eigenval-
ues and eigenvectors of matrices. Needless to say, we completely avoid trying to solve (or
even write down) the characteristic polynomial equation. The basic Power Method and its
variants, which are based on linear iteration, are used to eﬀectively approximate selected
eigenvalues. To calculate the complete system of eigenvalues and eigenvectors, the remark-
able QR algorithm, which relies on the Gram–Schmidt orthogonalization procedure, is the
method of choice, and we include a new proof of its convergence.
The following section describes some more recent “semi-direct” iterative algorithms for
ﬁnding eigenvalues and solving linear systems, that, in contrast to the classical iterative
schemes, are guaranteed to eventually produce the exact solution. These are based on the
idea of a Krylov subspace, spanned by the vectors generated by repeatedly multiplying
an initial vector by the coeﬃcient matrix. The Arnoldi and Lanczos algorithms are used
to ﬁnd a corresponding orthonormal basis for the Krylov subspaces, and thereby approx-
imate (some of) the eigenvalues of the matrix. Two classes of solution methods are then
© Springer International Publishing AG, part of Springer Nature 2018 
https://doi.org/10.1007/978-3-319-91041-3_9 
475
P. J. Olver, C. Shakiban, Applied Linear Algebra, Undergraduate Texts in Mathematics, 

476
9 Iteration
presented: ﬁrst, the Full Orthogonalization Method (FOM) which, for a positive deﬁnite
matrix, produces the powerful technique known as Conjugate Gradients (CG), of particu-
lar importance in numerical approximation of partial diﬀerential equations. The second is
the recent Generalized Minimal Residual Method (GMRES), which is eﬀectively used for
solving large sparse linear systems.
The ﬁnal Section 9.7 introduces the basic ideas behind wavelets, a powerful and widely
used alternative to Fourier methods for signal and image processing. While slightly oﬀ
topic, it provides a nice application of orthogonality and iterative techniques, and is thus
a ﬁtting end to this chapter.
9.1 Linear Iterative Systems
We begin with the basic deﬁnition of an iterative system of linear equations.
Deﬁnition 9.1. A linear iterative system takes the form
u(k+1) = T u(k),
u(0) = a,
(9.1)
where the coeﬃcient matrix T has size n × n.
We will consider both real and complex systems, and so the iterates† u(k) are vectors
either in Rn (which assumes that the coeﬃcient matrix T is also real) or in Cn. A linear
iterative system can be viewed as a discretized version of a ﬁrst order system of linear
ordinary diﬀerential equations, as in (8.9), in which the state of the system, as represented
by the vector u(k), changes at discrete time intervals, labeled by the index k.
Scalar Systems
As usual, to study systems one begins with an in-depth analysis of the scalar version.
Consider the iterative equation
u(k+1) = λ u(k),
u(0) = a,
(9.2)
where λ, a and the solution u(k) are all real or complex scalars. The general solution to
(9.2) is easily found:
u(1) = λ u(0) = λ a,
u(2) = λ u(1) = λ2 a,
u(3) = λ u(2) = λ3 a,
and, in general,
u(k) = λk a.
(9.3)
If the initial condition is a = 0, then the solution u(k) ≡0 is constant. In other words, 0
is a ﬁxed point or equilibrium solution for the iterative system because it does not change
under iteration.
Example 9.2.
Banks add interest to a savings account at discrete time intervals. For
example, if the bank oﬀers 5% interest compounded yearly, this means that the account
balance will increase by 5% each year. Thus, assuming no deposits or withdrawals, the
balance u(k) after k years will satisfy the iterative equation (9.2) with λ = 1 + r, where
†
Warning. The superscripts on u(k) refer to the iterate number, and should not be mistaken
for derivatives.

9.1 Linear Iterative Systems
477
0 < λ < 1
−1 < λ < 0
λ = 1
λ = −1
1 < λ
λ < −1
Figure 9.1.
One–Dimensional Real Linear Iterative Systems.
r = .05 is the interest rate, and the 1 indicates that all the money remains in the account.
Thus, after k years, your account balance is
u(k) = (1 + r)ka,
where
a = u(0)
(9.4)
is your initial deposit. For example, if u(0) = a = $1,000, after 1 year, your account has
u(1) = $1,050, after 10 years u(10) = $1,628.89, after 50 years u(50) = $11,467.40, and after
200 years u(200) = $17,292,580.82, a gain of over 17,000%.
When the interest is compounded monthly, the rate is still quoted on a yearly basis, and
so you receive
1
12 of the interest each month. If u (k) denotes the balance after k months,
then, after n years, the account balance will be u (12n) =

1 + 1
12 r
 12n a. Thus, when the
interest rate of 5% is compounded monthly, your account balance is u (12) = $1,051.16 after
1 year, u (120) = $1, 647.01 after 10 years, u (600) = $12,119.38 after 50 years, and u(2400) =
$21,573,572.66 dollars after 200 years. So, if you wait suﬃciently long, compounding will
have a dramatic eﬀect. Similarly, daily compounding replaces 12 by 365.25, the number of
days in a year. After 200 years, the balance wil be $22,011,396.03.
Let us analyze the solutions of scalar iterative equations, starting with the case when
λ ∈R is a real constant. Aside from the equilibrium solution u(k) ≡0, the iterates exhibit
six qualitatively diﬀerent behaviors, depending on the size of the coeﬃcient λ.
(a) If λ = 0, the solution immediately becomes zero, and stays there, whereby u(k) = 0
for all k ≥1.
(b) If 0 < λ < 1, then the solution is of one sign, and tends monotonically to zero, so
u(k) →0 as k →∞.
(c) If −1 < λ < 0, then the solution tends to zero: u(k) →0 as k →∞. Successive
iterates have alternating signs.

478
9 Iteration
(d) If λ = 1, the solution is constant: u(k) = a, for all k ≥0.
(e) If λ = −1, the solution bounces back and forth between two values; u(k) = (−1)k a.
(f ) If 1 < λ < ∞, then the iterates u(k) become unbounded.
If a > 0, they tend
monotonically to +∞; if a < 0, they tend to −∞.
(g) If −∞< λ < −1, then the iterates u(k) also become unbounded, with alternating
signs.
In Figure 9.1 we exhibit representative scatter plots for the nontrivial cases (b – g). The
horizontal axis indicates the index k, and the vertical axis the solution value u. Each dot
in the scatter plot represents an iterate u(k).
In the ﬁrst three cases, the ﬁxed point u = 0 is said to be asymptotically stable, since
all solutions tend to 0 as k →∞. In cases (d) and (e), the zero solution is stable, since
solutions with nearby initial data, | a | ≪1, remain nearby. In the ﬁnal two cases, the zero
solution is unstable; every nonzero initial data a ̸= 0 — no matter how small — will give
rise to a solution that eventually goes arbitrarily far away from equilibrium.
Let us also investigate complex scalar iterative systems. The coeﬃcient λ and the initial
datum a in (9.2) are allowed to be complex numbers. The solution is the same, (9.3), but
now we need to know what happens when we raise a complex number λ to a high power.
The secret is to write λ = r e i θ in polar form (3.93), where r = | λ | is its modulus and
θ = ph λ its angle or phase. Then λk = rk e i kθ. Since | e i kθ | = 1, we have | λk | = | λ |k,
and so the solutions (9.3) have modulus | u(k) | = | λk a | = | λ |k | a |. As a result, u(k) will
remain bounded if and only if | λ | ≤1, and will tend to zero as k →∞if and only if
| λ | < 1.
We have thus established the basic stability criteria for scalar, linear systems.
Theorem 9.3. The zero solution to a (real or complex) scalar iterative system is
(a) asymptotically stable if and only if | λ | < 1,
(b) stable if and only if | λ | ≤1,
(c) unstable if and only if | λ | > 1.
Exercises
9.1.1. Suppose u(0) = 1. Find u(1), u(10), and u(20) when
(a) u(k+1) = 2u(k),
(b) u(k+1) = −.9u(k),
(c) u(k+1) = i u(k),
(d) u(k+1) = (1 −2 i )u(k).
Is the system stable or unstable? If stable, is it asymptotically stable?
9.1.2. A bank oﬀers 3.25% interest compounded yearly. Suppose you deposit $100. (a) Set up
a linear iterative equation to represent your bank balance.
(b) How much money do you
have after 10 years? (c) What if the interest is compounded monthly?
9.1.3. Show that the yearly balances of an account whose interest is compounded monthly
satisfy a linear iterative system. How is the eﬀective yearly interest rate determined from
the original annual interest rate?
9.1.4. Show that, as the time interval of compounding goes to zero, the bank balance after k
years approaches an exponential function er k a, where r is the yearly interest rate and a is
the initial balance.
9.1.5. For which values of λ does the scalar iterative system (9.2) have a periodic solution,
meaning that u(k+m) = u(k) for some m?

9.1 Linear Iterative Systems
479
9.1.6. Consider the iterative systems u(k+1) = λ u(k) and v(k+1) = μ v(k), where | λ | > | μ |.
Prove that, for all nonzero initial data u(0) = a ̸= 0, v(0) = b ̸= 0, the solution to the ﬁrst is
eventually larger (in modulus) than that of the second: | u(k) | > | v(k) |, for k ≫0.
9.1.7. Let u(t) denote the solution to the linear ordinary diﬀerential equation u = β u,
u(0) = a. Let h > 0. Show that the sample values u(k) = u(kh) satisfy a linear iterative
system. What is the coeﬃcient λ? Compare the stability properties of the diﬀerential
equation and the corresponding iterative system.
♠9.1.8. Investigate the solutions of the linear iterative equation u(k+1) = λ u(k) when λ is a
complex number with | λ | = 1, and look for patterns.
9.1.9. Let λ, c ∈R. Solve the aﬃne or inhomogeneous linear iterative equation
u(k+1) = λ u(k) + c,
u(0) = a.
(9.5)
Discuss the possible behaviors of the solutions. Hint: Write the solution in the form
u(k) = u⋆+ v(k), where u⋆is the equilibrium solution.
9.1.10. A bank oﬀers 5% interest compounded yearly. Suppose you deposit $120 in the account
each year. Set up an aﬃne iterative equation (9.5) to represent your bank balance. How
much money do you have after 10 years? After you retire in 50 years? After 200 years?
9.1.11. Redo Exercise 9.1.10 in the case that the interest is compounded monthly and you
deposit $10 each month.
♥9.1.12. Each spring, the deer in Minnesota produce oﬀspring at a rate of roughly 1.2 times the
total population, while approximately 5% of the population dies as a result of predators
and natural causes. In the fall, hunters are allowed to shoot 3,600 deer. This winter the
Department of Natural Resources (DNR) estimates that there are 20,000 deer. Set up an
aﬃne iterative equation (9.5) to represent the deer population each subsequent year. Solve
the system and ﬁnd the population in the next 5 years. How many deer in the long term
will there be? Using this information, formulate a reasonable policy of how many deer
hunting licenses the DNR should allow each fall, assuming one kill per license.
Powers of Matrices
The solution to the general linear iterative system
u(k+1) = T u(k),
u(0) = a,
(9.6)
is also, at least at ﬁrst glance, immediate. Clearly,
u(1) = T u(0) = T a,
u(2) = T u(1) = T 2a,
u(3) = T u(2) = T 3a,
and, in general,
u(k) = T ka.
(9.7)
Thus, the iterates are simply determined by multiplying the initial vector a by the succes-
sive powers of the coeﬃcient matrix T. And so, in contrast to diﬀerential equations, proving
the existence and uniqueness of solutions to an iterative system is completely trivial.
However, unlike real or complex scalars, the general formulas for and qualitative be-
havior of the powers of a square matrix are not nearly so immediately apparent. (Before
continuing, the reader is urged to experiment with simple 2 × 2 matrices, trying to detect
patterns.)
To make progress, recall how, in Section 8.1, we endeavored to solve linear
systems of diﬀerential equations by suitably adapting the known exponential solution from
the scalar version. In the iterative case, the scalar solution formula (9.3) is written in terms
of powers, not exponentials. This motivates us to try the power ansatz
u(k) = λk v,
(9.8)

480
9 Iteration
in which λ is a scalar and v a vector, as a possible solution to the system. We ﬁnd
u(k+1) = λk+1 v,
while
T u(k) = T(λk v) = λk T v.
These two expressions will be equal if and only if
T v = λv.
This is precisely the deﬁning eigenvalue equation (8.12), and thus, (9.8) is a nontrivial
solution to (9.6) if and only if λ is an eigenvalue of the coeﬃcient matrix T and v ̸= 0 an
associated eigenvector.
Thus, for each eigenvector and eigenvalue of the coeﬃcient matrix, we can construct a
solution to the iterative system. We can then appeal to linear superposition, as in Theo-
rem 7.30, to combine the basic eigensolutions to form more general solutions. In particular,
if the coeﬃcient matrix is complete, this method will produce the general solution.
Theorem 9.4. If the coeﬃcient matrix T is complete, then the general solution to the
linear iterative system u(k+1) = T u(k) is given by
u(k) = c1 λk
1 v1 + c2 λk
2 v2 + · · · + cn λk
n vn,
(9.9)
where v1, . . . , vn are the linearly independent eigenvectors and λ1, . . . , λn the correspond-
ing eigenvalues of T.
The coeﬃcients c1, . . . , cn are arbitrary scalars and are uniquely
prescribed by the initial conditions u(0) = a.
Proof : Since we already know, by linear superposition, that (9.9) is a solution to the
system for arbitrary c1, . . . , cn, it suﬃces to show that we can match any prescribed initial
conditions. To this end, we need to solve the linear system
u(0) = c1 v1 + · · · + cn vn = a.
(9.10)
Completeness of T implies that its eigenvectors form a basis of Cn, and hence (9.10) always
admits a solution. In matrix form, we can rewrite (9.10) as
S c = a,
so that
c = S−1a,
where
S = ( v1 v2 . . . vn )
is the (nonsingular) matrix whose columns are the eigenvectors.
Q.E.D.
Solutions in the incomplete cases are more complicated to write down, and rely on the
Jordan bases of Section 8.6; see Exercise 9.1.40.
Example 9.5.
Consider the iterative system
x(k+1) = 3
5 x(k) + 1
5 y(k),
y(k+1) = 1
5 x(k) + 3
5 y(k),
(9.11)
with initial conditions
x(0) = a,
y(0) = b.
(9.12)
The system can be rewritten in our matrix form (9.6), with
T =

.6
.2
.2
.6

,
u(k) =

x(k)
y(k)

,
a =

a
b

.
Solving the characteristic equation
det(T −λ I ) = λ2 −1.2λ −.32 = 0

9.1 Linear Iterative Systems
481
Figure 9.2.
Stable Iterative System.
produces the eigenvalues λ1 = .8, λ2 = .4. We then solve the associated linear systems
(T −λj I )vj = 0 for the corresponding eigenvectors:
λ1 = .8,
v1 =

1
1

,
λ2 = .4,
v2 =

−1
1

.
Therefore, the basic eigensolutions are
u(k)
1
= (.8)k

1
1

,
u(k)
2
= (.4)k

−1
1

.
Theorem 9.4 tells us that the general solution is given as a linear combination,
u(k) = c1 u(k)
1
+ c2 u(k)
2
= c1 (.8)k

1
1

+ c2 (.4)k

−1
1

=

c1 (.8)k −c2 (.4)k
c1 (.8)k + c2 (.4)k

,
where c1, c2 are determined by the initial conditions:
u(0) =

c1 −c2
c1 + c2

=

a
b

,
and hence
c1 = a + b
2
,
c2 = b −a
2
.
Therefore, the explicit formula for the solution to the initial value problem (9.11–12) is
x(k) = (.8)k a + b
2
+ (.4)k a −b
2
,
y(k) = (.8)k a + b
2
+ (.4)k b −a
2
.
In particular, as k →∞, the iterates u(k) →0 converge to zero at a rate governed by the
dominant eigenvalue λ1 = .8. Figure 9.2 illustrates the cumulative eﬀect of the iteration;
the initial data is colored orange, and successive iterates are colored green, blue, purple,
red. The initial conditions consist of a large number of points on the unit circle x2+y2 = 1,
which are successively mapped to points on progressively smaller and ﬂatter ellipses, that
shrink down towards the origin.
Example 9.6.
The Fibonacci numbers are deﬁned by the second order† scalar iterative
equation
u(k+2) = u(k+1) + u(k),
(9.13)
†
In general, an iterative system u(k+j) = T1u(k+j−1) + · · · + Tju(k) in which the new iterate
depends upon the preceding j values is said to have order j.

482
9 Iteration
with initial conditions
u(0) = a,
u(1) = b.
(9.14)
In short, to obtain the next Fibonacci number, add the previous two.
The classical
Fibonacci integers start with a = 0, b = 1; the next few are
u(0) = 0, u(1) = 1, u(2) = 1, u(3) = 2, u(4) = 3, u(5) = 5, u(6) = 8, u(7) = 13, . . . .
The Fibonacci integers occur in a surprising variety of natural objects, including leaves,
ﬂowers, and fruit, [83]. They were originally introduced by the eleventh-/twelfth-century
Italian mathematician Leonardo Pisano Fibonacci as a crude model of the growth of a
population of rabbits. In Fibonacci’s model, the kth Fibonacci number u(k) measures the
total number of pairs of rabbits at year k. We start the process with a single juvenile pair‡
at year 0. Once a year, each pair of rabbits produces a new pair of oﬀspring, but it takes
a full year for a rabbit pair to mature enough to produce oﬀspring of their own.
Every higher order iterative equation can be replaced by an equivalent ﬁrst order iter-
ative system. In this particular case, we deﬁne the vector
u(k) =

u(k)
u(k+1)

∈R2,
and note that (9.13) is equivalent to the matrix system

u(k+1)
u(k+2)

=

0
1
1
1
 
u(k)
u(k+1)

,
or
u(k+1) = T u(k),
where
T =

0
1
1
1

.
To ﬁnd the explicit formula for the Fibonacci numbers, we must determine the eigenvalues
and eigenvectors of the coeﬃcient matrix T. A straightforward computation produces
λ1 = 1 +
√
5
2
= 1.618034 . . . ,
λ2 = 1 −
√
5
2
= −.618034 . . . ,
v1 =
 −1+
√
5
2
1

,
v2 =
 −1−
√
5
2
1

.
Therefore, according to (9.9), the general solution to the Fibonacci system is
u(k) =

u(k)
u(k+1)

= c1

1 +
√
5
2
k  −1+
√
5
2
1

+ c2

1 −
√
5
2
k  −1−
√
5
2
1

.
(9.15)
The initial data
u(0) = c1
 −1+
√
5
2
1

+ c2
 −1−
√
5
2
1

=

a
b

uniquely speciﬁes the coeﬃcients
c1 = 2a + (1 +
√
5)b
2
√
5
,
c2 = −2a + (1 −
√
5)b
2
√
5
.
The ﬁrst entry of the solution vector (9.15) produces the explicit formula
u(k) = (−1 +
√
5)a + 2b
2
√
5

1 +
√
5
2
k
+ (1 +
√
5)a −2b
2
√
5

1 −
√
5
2
k
(9.16)
‡
Fibonacci ignores some pertinent details like the sex of the oﬀspring.

9.1 Linear Iterative Systems
483
Figure 9.3.
Fibonacci Iteration.
for the kth Fibonacci number. For the particular initial conditions a = 0, b = 1, (9.16)
reduces to the classical Binet formula
u(k) =
1
√
5
⎡
⎣

1 +
√
5
2
k
−

1 −
√
5
2
k⎤
⎦.
(9.17)
It is a remarkable fact that, for every value of k, all the
√
5’s cancel out, and the Binet
formula (9.17) does indeed produce the Fibonacci integers listed above. Another useful
observation is that, since
0 < | λ2 | =
√
5 −1
2
< 1 < λ1 = 1 +
√
5
2
,
the terms involving λk
1 go to ∞(and so the zero solution to this iterative system is unstable)
while the terms involving λk
2 go to zero. Therefore, even for k moderately large, the ﬁrst
term in (9.16) is an excellent approximation to the kth Fibonacci number — and one that
gets more and more accurate as k increases. A plot of the ﬁrst 4 iterates, starting with the
initial data consisting of equally spaced points on the unit circle, appears in Figure 9.3. As
in the previous example, the circle is mapped to a sequence of progressively more eccentric
ellipses; however, their major semi-axes become more and more stretched out, and almost
all points end up going oﬀto ∞in the direction of the dominant eigenvector v2.
The dominant eigenvalue λ1 = 1
2

1 +
√
5

= 1.6180339 . . . is known as the golden ratio
and plays an important role in spiral growth in nature, as well as in art, architecture, and
design, [83]. It describes the overall growth rate of the Fibonacci integers, and, in fact,
every sequence of Fibonacci numbers with initial conditions b ̸= 1
2

1 −
√
5

a.
Example 9.7.
Let T =
⎛
⎝
−3
1
6
1
−1
−2
−1
−1
0
⎞
⎠be the coeﬃcient matrix for a three-dimensional
iterative system u(k+1) = T u(k). Its eigenvalues and corresponding eigenvectors are
λ1 = −2,
λ2 = −1 + i ,
λ3 = −1 −i ,
v1 =
⎛
⎝
4
−2
1
⎞
⎠,
v2 =
⎛
⎝
2 −i
−1
1
⎞
⎠,
v3 =
⎛
⎝
2 + i
−1
1
⎞
⎠.

484
9 Iteration
Therefore, according to (9.9), the general complex solution is
u(k) = b1 (−2)k
⎛
⎝
4
−2
1
⎞
⎠+ b2 (−1 + i )k
⎛
⎝
2 −i
−1
1
⎞
⎠+ b3 (−1 −i )k
⎛
⎝
2 + i
−1
1
⎞
⎠,
where b1, b2, b3 are arbitrary complex scalars.
If we are interested only in real solutions, we can break up any complex solution into its
real and imaginary parts, each of which constitutes a real solution. (This is a manifestation
of the general Reality Principle of Theorem 7.48, but is not hard to prove directly.) We
begin by writing λ2 = −1 + i =
√
2 e3π i /4 in polar form, and hence
(−1 + i )k = 2k/2 e3kπ i /4 = 2k/2 
cos 3
4 kπ + i sin 3
4 kπ

.
Therefore, the complex solution
(−1 + i )k
⎛
⎝
2 −i
−1
1
⎞
⎠= 2k/2
⎛
⎜
⎝
2 cos 3
4 kπ + sin 3
4 kπ
−cos 3
4 kπ
cos 3
4 kπ
⎞
⎟
⎠+ i 2k/2
⎛
⎜
⎝
2 sin 3
4 kπ −cos 3
4 kπ
−sin 3
4 kπ
sin 3
4 kπ
⎞
⎟
⎠
is a combination of two independent real solutions.
The complex conjugate eigenvalue
λ3 = −1 −i leads, as before, to the complex conjugate solution — and the same two
real solutions.
The general real solution u(k) to the system can be written as a linear
combination of the three independent real solutions:
c1 (−2)k
⎛
⎝
4
−2
1
⎞
⎠+ c2 2k/2
⎛
⎜
⎝
2 cos 3
4 kπ+sin 3
4 kπ
−cos 3
4 kπ
cos 3
4 kπ
⎞
⎟
⎠+c3 2k/2
⎛
⎜
⎝
2 sin 3
4 kπ−cos 3
4 kπ
−sin 3
4 kπ
sin 3
4 kπ
⎞
⎟
⎠, (9.18)
where c1, c2, c3 are arbitrary real scalars, uniquely prescribed by the initial conditions.
Diagonalization and Iteration
An alternative, equally eﬃcient approach to solving iterative systems is based on diagonal-
ization of the coeﬃcient matrix, cf. (8.30). Speciﬁcally, assuming the coeﬃcient matrix T
is complete, we can factor it as a product
T = S Λ S−1,
(9.19)
in which Λ = diag (λ1, λ2, . . . , λn) is the diagonal matrix containing the eigenvalues of T,
while the columns of S = (v1 · · · vn) are the corresponding eigenvectors. Consequently,
the powers of T are given by
T 2 = (S Λ S−1) (S Λ S−1) = S Λ2 S−1,
T 3 = (S Λ S−1) (S Λ S−1) (S Λ S−1) = S Λ3 S−1,
and, in general,
T k = S Λk S−1.
(9.20)
Moreover, since Λ is a diagonal matrix, its powers are trivial to compute:
Λk = diag (λk
1, λk
2, . . . , λk
n).
(9.21)
Thus, by combining (9.20–21), we obtain an explicit formula for the powers of a complete
matrix T. Furthermore, the solution to the associated linear iterative system
u(k+1) = T u(k),
u(0) = a,
is given by
u(k) = T ka = S Λk S−1a.
(9.22)

9.1 Linear Iterative Systems
485
You should convince yourself that this gives precisely the same solution as before. Compu-
tationally, there is not a signiﬁcant diﬀerence between the two solution methods, and the
choice is left to the discretion of the user.
Example 9.8.
Suppose T =

7
6
−9
−8

. Its eigenvalues and eigenvectors are readily
computed:
λ1 = −2,
v1 =

−2
3

,
λ2 = 1,
v2 =

−1
1

.
We assemble these into the diagonal eigenvalue matrix Λ and the eigenvector matrix S,
given by
Λ =

−2
0
0
1

,
S =

−2
−1
3
1

,
whence

7
6
−9
−8

= T = S Λ S−1 =

−2
−1
3
1
 
−2
0
0
1
 
1
1
−3
−2

,
as you can readily check. Therefore, according to (9.20),
T k = S Λk S−1
=

−2
−1
3
1
 
(−2)k
0
0
1
 
1
1
−3
−2

=

3 −2(−2)k
2 −2(−2)k
−3 + 3(−2)k
−2 + 3(−2)k

.
You may wish to check this formula directly for the ﬁrst few values of k = 1, 2, . . . . As a
result, the solution to the particular iterative system
u(k+1) =

7
6
−9
−8

u(k),
u(0) =

1
1

,
is
u(k) = T k

1
1

=

5 −4(−2)k
−5 + 6(−2)k

.
In this case, the eigenvalue λ1 = −2 causes an instability, with solutions having arbitrarily
large norm as k →∞.
Exercises
9.1.13. Find the explicit formula for the solution to the following linear iterative systems:
(a) u(k+1) = u(k) −2v(k), v(k+1) = −2u(k) + v(k), u(0) = 1, v(0) = 0.
(b) u(k+1) = u(k) −2
3 v(k), v(k+1) = 1
2 u(k) −1
6 v(k), u(0) = −2, v(0) = 3.
(c) u(k+1) = u(k) −v(k), v(k+1) = −u(k) + 5v(k), u(0) = 1, v(0) = 0.
(d) u(k+1) = 1
2 u(k) + v(k), v(k+1) = v(k) −2w(k), w(k+1) = 1
3 w(k),
u(0) = 1, v(0) = −1, w(0) = 1.
(e) u(k+1) = −u(k) + 2v(k) −w(k), v(k+1) = −6u(k) + 7v(k) −4w(k),
w(k+1) = −6u(k) + 6v(k) −4w(k),
u(0) = 0,
v(0) = 1,
w(0) = 3.
9.1.14. Find the explicit formula for the general solution to the linear iterative systems with
the following coeﬃcient matrices:
(a)

−1
2
1
−1
	
,
(b)

−2
7
−1
3
	
,
(c)
⎛
⎜
⎝
−3
2
−2
−6
4
−3
12
−6
−5
⎞
⎟
⎠,
(d)
⎛
⎜
⎜
⎜
⎝
−5
6
1
3
−1
6
0
−1
2
1
3
1
−1
2
3
⎞
⎟
⎟
⎟
⎠.
9.1.15. Prove that all the Fibonacci integers u(k), k ≥0, can be found by just computing the
ﬁrst term in the Binet formula (9.17) and then rounding oﬀto the nearest integer.

486
9 Iteration
9.1.16. The kth Lucas number is deﬁned as L(k) =

1 +
√
5
2
	k
+

1 −
√
5
2
	k
.
(a) Explain why the Lucas numbers satisfy the Fibonacci iterative equation
L(k+2) = L(k+1) + L(k). (b) Write down the ﬁrst 7 Lucas numbers.
(c) Prove that every Lucas number is a positive integer.
9.1.17. What happens to the Fibonacci integers u(k) if we go “backward in time”, i.e., for
k < 0? How is u(−k) related to u(k)?
9.1.18. Use formula (9.20) to compute the kth power of the following matrices:
(a)

5
2
2
2
	
,
(b)

4
1
−2
1
	
,
(c)

1
1
−1
1
	
,
(d)
⎛
⎜
⎝
1
1
2
1
2
1
2
1
1
⎞
⎟
⎠,
(e)
⎛
⎜
⎝
0
1
0
0
0
1
−1
0
2
⎞
⎟
⎠.
9.1.19. Use your answer from Exercise 9.1.18 to solve the following iterative systems:
(a) u(k+1) = 5u(k) + 2v(k), v(k+1) = 2u(k) + 2v(k), u(0) = −1, v(0) = 0,
(b) u(k+1) = 4u(k) + v(k), v(k+1) = −2u(k) + v(k), u(0) = 1, v(0) = −3,
(c) u(k+1) = u(k) + v(k), v(k+1) = −u(k) + v(k), u(0) = 0, v(0) = 2,
(d) u(k+1) = u(k) + v(k) + 2w(k), v(k+1) = u(k) + 2v(k) + w(k),
w(k+1) = 2u(k) + v(k) + w(k), u(0) = 1, v(0) = 0, w(0) = 1,
(e) u(k+1) = v(k), v(k+1) = w(k), w(k+1) = −u(k) + 2w(k), u(0) = 1, v(0) = 0, w(0) = 0.
9.1.20.(a) Given initial data u(0) = ( 1, 1, 1 )T , explain why the resulting solution u(k) to the
system in Example 9.7 has all integer entries.
(b) Find the coeﬃcients c1, c2, c3 in the
explicit solution formula (9.18).
(c) Check the ﬁrst few iterates to convince yourself that
the solution formula does, in spite of appearances, always give an integer value.
9.1.21.(a) Show how to convert the higher order linear iterative equation
u(k+j) = c1 u(k+j−1) + c2 u(k+j−2) + · · · + cj u(k)
into a ﬁrst order system u(k) = T u(k). Hint: See Example 9.6.
(b) Write down initial conditions that guarantee a unique solution u(k) for all k ≥0.
9.1.22. Apply the method of Exercise 9.1.21 to solve the following iterative equations:
(a)
u(k+2) = −u(k+1) + 2u(k),
u(0) = 1,
u(1) = 2.
(b)
12u(k+2) = u(k+1) + u(k),
u(0) = −1,
u(1) = 2.
(c)
u(k+2) = 4u(k+1) + u(k),
u(0) = 1,
u(1) = −1.
(d)
u(k+2) = 2u(k+1) −2u(k),
u(0) = 1,
u(1) = 3.
(e)
u(k+3) = 2u(k+2) + u(k+1) −2u(k),
u(0) = 0,
u(1) = 2,
u(2) = 3.
(f )
u(k+3) = u(k+2) + 2u(k+1) −2u(k),
u(0) = 0,
u(1) = 1,
u(2) = 1.
9.1.23. Suppose you have n dollars and can buy coﬀee for $1, milk for $2, and orange juice for
$2. Let C(n) count the number of diﬀerent ways of spending all your money. (a) Explain
why C(n) = C(n−1) + 2C(n−2), C(0) = C(1) = 1.
(b) Find an explicit formula for C(n).
9.1.24. Find the general solution to the iterative system u(k+1)
i
= u(k)
i−1 + u(k)
i+1, i = 1, . . . , n,
where we set u(k)
0
= u(k)
n+1 = 0 for all k. Hint: Use Exercise 8.2.47.
♣9.1.25. Starting with u(0) = 0, u(1) = 0, u(2) = 1, deﬁne the sequence of tribonacci numbers
u(k) by adding the previous three to get the next one. For instance,
u(3) = u(0) + u(1) + u(2) = 1. (a) Write out the next four tribonacci numbers. (b) Find
a third order iterative equation for the kth tribonacci number. (c) Explain why the
tribonacci numbers are all integers. (d) Find an explicit formula for the solution, using
a computer to approximate the eigenvalues.
(e) Do they grow faster than the usual
Fibonacci numbers? What is their overall rate of growth?

9.1 Linear Iterative Systems
487
♣9.1.26. Suppose that Fibonacci’s rabbits live for only eight years, [44]. (a) Write out an
iterative equation to describe the rabbit population. (b) Write down the ﬁrst few terms.
(c) Convert your equation into a ﬁrst order iterative system, using the method of Exercise
9.1.21. (d) At what rate does the rabbit population grow?
♣9.1.27. A well-known method of generating a sequence of “pseudo-random” integers
u(0), u(1), u(2), . . . satisfying 0 ≤u(i) < n is based on the modular Fibonacci equation
u(k+2) = u(k+1) + u(k) mod n, with suitably chosen initial values 0 ≤u(0), u(1) < n.
(a) Generate the sequence of pseudo-random numbers that result from the choices n = 10,
u(0) = 3, u(1) = 7. Keep iterating until the sequence starts repeating.
(b) Experiment with other sequences of pseudo-random numbers generated by the method.
9.1.28. Prove that the curves Ek = { T kx | ∥x∥= 1 }, k = 0, 1, 2, . . . , sketched in Figure 9.2
form a family of ellipses with the same principal axes. What are the individual semi-axes?
Hint: Use Exercise 8.7.23.
♠9.1.29. Plot the ellipses Ek = { T kx | ∥x∥= 1 } for k = 1, 2, 3, 4 for the following matrices T.
Then determine their principal axes, semi-axes, and areas. Hint: Use Exercise 8.7.23.
(a)
⎛
⎝
2
3
−1
3
−1
3
2
3
⎞
⎠,
(b)

0
−1.2
.4
0
	
,
(c)
⎛
⎝
3
5
1
5
2
5
4
5
⎞
⎠.
9.1.30. Let T be a positive deﬁnite 2 × 2 matrix. Let En = { T nx | ∥x∥= 1 }, n = 0, 1, 2, . . . ,
be the image of the unit circle under the nth power of T. (a) Prove that En is an ellipse.
True or false:
(b) The ellipses En all have the same principal axes. (c) The semi-axes are
given by rn = rn
1 , sn = sn
1 .
(d) The areas are given by An = π αn where α = A1/π.
9.1.31. Answer Exercise 9.1.30 when T is an arbitrary nonsingular 2 × 2 matrix.
Hint: Use Exercise 8.7.23.
9.1.32. Given the general solution (9.9) of the iterative system u(k+1) = T u(k), write down the
solution to v(k+1) = α T v(k) + β v(k), where α, β ∈R.
♦9.1.33. Prove directly that if the coeﬃcient matrix of a linear iterative system is real, both the
real and imaginary parts of a complex solution are real solutions.
♦9.1.34. Explain why the solution u(k), k ≥0, to the initial value problem (9.6) exists and is
uniquely deﬁned. Does this hold if we allow negative k < 0?
9.1.35. Prove that if T is a symmetric matrix, then the coeﬃcients in (9.9) are given by the
formula cj = aT vj / vT
j vj.
9.1.36. Explain why the jth column c(k)
j
of the matrix power T k satisﬁes the linear iterative
system c(k+1)
j
= T c(k)
j
with initial data c(0)
j
= ej, the jth standard basis vector.
9.1.37. Let z(k+1) = λ z(k) be a complex scalar iterative equation with λ = μ + i ν. Show that
its real and imaginary parts x(k) = Re z(k), y(k) = Im z(k), satisfy a two-dimensional real
linear iterative system. Use the eigenvalue method to solve the real 2 × 2 system, and verify
that your solution coincides with the solution to the original complex equation.
♦9.1.38. Suppose V ⊂Rn is an invariant subspace for the n × n matrix T governing the linear
iterative system u(k+1) = T u(k). Prove that if u(0) ∈V , then so is the solution: u(k) ∈V .
9.1.39. Suppose u(k) and u(k) are two solutions to the same iterative system u(k+1) = T u(k).
(a) Suppose u(k0) = u(k0) for some k0 ≥0. Can you conclude that these are the same
solution: u(k) = u(k) for all k? (b) What can you say if u(k0) = u(k1) for k0 ̸= k1?

488
9 Iteration
♦9.1.40. Let T be an incomplete matrix, and suppose w1, . . . , wj is a Jordan chain associated
with an incomplete eigenvalue λ. (a) Prove that, for i = 1, . . . , j,
T k wi = λk wi + kλk−1 wi−1 +
⎛
⎝k
2
⎞
⎠λk−2 wi−2 + · · · .
(9.23)
(b) Explain how to use a Jordan basis of T to construct the general solution to the linear
iterative system u(k+1) = T u(k).
9.1.41. Use the method Exercise 9.1.40 to ﬁnd the general real solution to the following linear
iterative systems:
(a) u(k+1) = 2u(k) + 3v(k), v(k+1) = 2v(k),
(b) u(k+1) = u(k) + v(k), v(k+1) = −4u(k) + 5v(k),
(c) u(k+1) = −u(k) + v(k) + w(k), v(k+1) = −v(k) + w(k), w(k+1) = −w(k),
(d) u(k+1) = 3u(k) −v(k), v(k+1) = −u(k) + 3v(k) + w(k), w(k+1) = −v(k) + 3w(k),
(e) u(k+1) = u(k) −v(k) −w(k), v(k+1) = 2u(k) +2v(k) +2w(k), w(k+1) = −u(k) +v(k) +w(k),
(f ) u(k+1) = v(k) + z(k), v(k+1) = −u(k) + w(k), w(k+1) = z(k), z(k+1) = −w(k).
9.1.42. Find a formula for the kth power of a Jordan block matrix. Hint: Use Exercise 9.1.40.
♥9.1.43. An aﬃne iterative system has the form u(k+1) = T u(k) + b, u(0) = c.
(a) Under what conditions does the system have an equilibrium solution u(k) ≡u⋆?
(b) In such cases, ﬁnd a formula for the general solution. Hint: Look at v(k) = u(k) −u⋆.
(c) Solve the following aﬃne iterative systems:
(i)
u(k+1) =

6
3
−3
−4
	
u(k) +

1
2
	
,
u(0) =

4
−3
	
,
(ii)
u(k+1) =

−1
2
1
−1
	
u(k) +

1
0
	
,
u(0) =

0
1
	
,
(iii)
u(k+1) =
⎛
⎜
⎝
−3
2
−2
−6
4
−3
12
−6
−5
⎞
⎟
⎠u(k) +
⎛
⎜
⎝
1
−3
0
⎞
⎟
⎠,
u(0) =
⎛
⎜
⎝
1
0
−1
⎞
⎟
⎠,
(iv)
u(k+1) =
⎛
⎜
⎜
⎜
⎝
−5
6
1
3
−1
6
0
−1
2
1
3
1
−1
2
3
⎞
⎟
⎟
⎟
⎠u(k) +
⎛
⎜
⎜
⎜
⎝
1
6
−1
3
−1
2
⎞
⎟
⎟
⎟
⎠,
u(0) =
⎛
⎜
⎜
⎜
⎝
1
6
−2
3
1
3
⎞
⎟
⎟
⎟
⎠.
(d) Discuss what happens in cases in which there is no ﬁxed point, assuming that
T is complete.
9.2 Stability
With the solution formula (9.9) in hand, we are now in a position to understand the
qualitative behavior of solutions to (complete) linear iterative systems. The most important
case for applications is when all the iterates converge to 0.
Deﬁnition 9.9. The equilibrium solution u⋆= 0 to a linear iterative system (9.1) is called
globally asymptotically stable if all solutions u(k) →0 as k →∞.
Asymptotic stability relies on the following property of the coeﬃcient matrix.
Deﬁnition 9.10. A matrix T is called convergent if its powers converge to the zero matrix,
T k →O, meaning that the individual entries of T k all go to 0 as k →∞.
The equivalence of the convergence condition and stability of the iterative system follows

9.2 Stability
489
immediately from the solution formula (9.7).
Theorem 9.11. The linear iterative system u(k+1) = T u(k) has globally asymptotically
stable zero solution if and only if T is a convergent matrix.
Proof : If T k →O, and u(k) = T k a is any solution, then clearly u(k) →0 as k →∞,
proving stability. Conversely, the solution u(k)
j
= T kej is the same as the jth column of
T k. If the origin is asymptotically stable, then u(k)
j
→0. Thus, the individual columns of
T k all tend to 0, proving that T k →O.
Q.E.D.
To facilitate the analysis of convergence, we shall adopt a norm ∥·∥on our underlying
vector space, Rn or Cn. The reader may be inclined to choose the Euclidean (or Hermitian)
norm, but, in practice, the ∞norm
∥u∥∞= max

| u1 |, . . . , | un |

(9.24)
prescribed by the vector’s maximal entry (in modulus) is often easier to work with. Con-
vergence of the iterates is equivalent to convergence of their norms:
u(k) →0
if and only if
∥u(k) ∥→0
as
k →∞.
The fundamental stability criterion for linear iterative systems relies on the size of the
eigenvalues of the coeﬃcient matrix.
Theorem 9.12. The matrix T is convergent, and hence the zero solution of the associated
linear iterative system (9.1) is globally asymptotically stable, if and only if all its (complex)
eigenvalues have modulus strictly less than one: | λj | < 1.
Proof : Let us prove this result assuming that the coeﬃcient matrix T is complete. (The
proof in the incomplete case relies on the Jordan canonical form, and is outlined in Exercise
9.2.18.) If λj is an eigenvalue such that | λj | < 1, then the corresponding basis solution
u(k)
j
= λk
j vj tends to zero as k →∞; indeed,
∥u(k)
j
∥= ∥λk
j vj ∥= | λj |k ∥vj ∥−→0,
since
| λj | < 1.
Therefore, if all eigenvalues are less than 1 in modulus, all terms in the solution formula
(9.9) tend to zero, which proves asymptotic stability: u(k) →0. Conversely, if any eigen-
value satisﬁes | λj | ≥1, then the solution u(k) = λk
j vj does not tend to 0 as k →∞, and
hence 0 is not asymptotically stable.
Q.E.D.
Spectral Radius
Consequently, the necessary and suﬃcient condition for asymptotic stability of a linear
iterative system is that all the eigenvalues of the coeﬃcient matrix lie strictly inside the
unit circle in the complex plane: | λj | < 1. This criterion can be recast using the following
important deﬁnition.
Deﬁnition 9.13. The spectral radius of a matrix T is deﬁned as the maximal modulus of
all of its real and complex eigenvalues: ρ(T) = max { | λ1 |, . . . , | λk | }.
Theorem 9.14. The matrix T is convergent if and only if its spectral radius is strictly
less than one: ρ(T) < 1.

490
9 Iteration
If T is complete, then we can apply the triangle inequality to (9.9) to estimate
∥u(k) ∥= ∥c1 λk
1 v1 + · · · + cn λk
n vn ∥
≤| λ1 |k ∥c1 v1 ∥+ · · · + | λn |k ∥cnvn ∥
≤ρ(T)k
| c1 | ∥v1 ∥+ · · · + | cn | ∥vn ∥

= C ρ(T)k,
(9.25)
for some constant C > 0 that depends only upon the initial conditions. In particular, if
ρ(T) < 1, then
∥u(k) ∥≤C ρ(T)k −→0
as
k →∞,
(9.26)
in accordance with Theorem 9.14. Thus, the spectral radius ρ(T) prescribes the rate of
convergence of the solutions to equilibrium; the smaller the spectral radius, the faster the
solutions go to 0.
If T has only one largest (simple) eigenvalue, so | λ1 | > | λj | for all j > 1, then the
ﬁrst term in the solution formula (9.9) will eventually dominate all the others: ∥λk
1 v1 ∥≫
∥λk
j vj ∥for j > 1 and k ≫0. Therefore, provided that c1 ̸= 0, the solution (9.9) has the
asymptotic formula
u(k) ≈c1 λk
1 v1,
(9.27)
and so most solutions end up parallel to v1. In particular, if | λ1 | = ρ(T) < 1, such a
solution approaches 0 along the direction of the dominant eigenvector v1 at a rate governed
by the modulus of the dominant eigenvalue. The exceptional solutions, with c1 = 0, tend
to 0 at a faster rate, along one of the other eigendirections. In practical computations,
one rarely observes the exceptional solutions. Indeed, even if the initial condition does
not involve the dominant eigenvector, numerical errors during the iteration will almost
inevitably introduce a small component in the direction of v1, which will, if you wait long
enough, eventually dominate the solution.
The inequality (9.25) applies only to complete matrices. In the general case, one can
prove, cf. Exercise 9.2.18, that the solution satisﬁes the slightly weaker inequality
∥u(k) ∥≤C σk
for all
k ≥0,
where
σ > ρ(T)
(9.28)
is any number larger than the spectral radius, while C > 0 is a positive constant (whose
value may depend on how close σ is to ρ).
Example 9.15.
According to Example 9.7, the matrix
T =
⎛
⎝
−3
1
6
1
−1
−2
−1
−1
0
⎞
⎠
has eigenvalues
λ1 = −2,
λ2 = −1 + i ,
λ3 = −1 −i .
Since | λ1 | = 2 > | λ2 | = | λ3 | =
√
2 , the spectral radius is ρ(T) = | λ1 | = 2. We conclude
that T is not a convergent matrix. As the reader can check, either directly, or from the
solution formula (9.18), the vectors u(k) = T ku(0) obtained by repeatedly multiplying any
nonzero initial vector u(0) by T rapidly go oﬀto ∞, in successively opposite directions, at
a rate roughly equal to ρ(T)k = 2k.
On the other hand, the matrix
T = −1
3 T =
⎛
⎜
⎝
1
−1
3
−2
−1
3
1
3
2
3
1
3
1
3
0
⎞
⎟
⎠
with eigenvalues
λ1 = 2
3,
λ2 = 1
3 −1
3 i ,
λ3 = 1
3 + 1
3 i ,

9.2 Stability
491
has spectral radius ρ( T) = 2
3, and hence is a convergent matrix. According to (9.27), if we
write the initial data u(0) = c1 v1 +c2 v2 +c3 v3 as a linear combination of the eigenvectors,
then, provided c1 ̸= 0, the iterates have the asymptotic form u(k) ≈c1
 2
3
k v1, where
v1 = ( 4, −2, 1 )T is the eigenvector corresponding to the dominant eigenvalue λ1 =
2
3.
Thus, for most initial vectors, the iterates end up decreasing in length by a factor of almost
exactly 2
3, eventually becoming parallel to the dominant eigenvector v1. This is borne out
by a sample computation: starting with u(0) = ( 1, 1, 1 )T , we obtain, for instance,
u(15) =
⎛
⎝
−.018216
.009135
−.004567
⎞
⎠,
u(16) =
⎛
⎝
−.012126
.006072
−.003027
⎞
⎠,
u(17) =
⎛
⎝
−.008096
.004048
−.002018
⎞
⎠,
which form progressively more accurate scalar multiples of the dominant eigenvector v1 =
( 4, −2, 1 )T ; moreover, the ratios between their successive entries, u(k+1)
i
/u(k)
i
, are ap-
proaching the dominant eigenvalue λ1 = 2
3.
Exercises
9.2.1. Determine the spectral radius of the following matrices:
(a)

1
2
3
4
	
,
(b)
⎛
⎝
1
3
−1
4
1
2
−1
3
⎞
⎠,
(c)
⎛
⎜
⎝
0
1
0
0
0
1
−2
1
2
⎞
⎟
⎠,
(d)
⎛
⎜
⎝
−1
5
−9
4
0
−1
4
−4
3
⎞
⎟
⎠.
9.2.2. Determine whether or not the following matrices are convergent:
(a)

2
−3
3
2
	
,
(b)

.6
.3
.3
.7
	
,
(c) 1
5
⎛
⎜
⎝
5
−3
−2
1
−2
1
1
−5
4
⎞
⎟
⎠,
(d)
⎛
⎜
⎝
.8
.3
.2
.1
.2
.6
.1
.5
.2
⎞
⎟
⎠.
9.2.3. Which of the listed coeﬃcient matrices deﬁnes a linear iterative system with
asymptotically stable zero solution?
(a)

−3
0
−4
−1
	
,
(b)
⎛
⎝
1
2
3
4
2
3
1
3
⎞
⎠,
(c)
⎛
⎝
1
2
1
2
−1
2
1
2
⎞
⎠,
(d)
⎛
⎜
⎝
−1
3
0
−1
1
−1
0
−1
−1
⎞
⎟
⎠,
(e)
⎛
⎜
⎜
⎜
⎝
1
2
1
4
−1
4
1
2
3
4
−1
2
−1
4
−1
4
1
2
⎞
⎟
⎟
⎟
⎠,
(f )
⎛
⎜
⎝
3
0
−1
0
1
0
2
0
0
⎞
⎟
⎠,
(g)
⎛
⎜
⎜
⎜
⎜
⎜
⎝
1
0
−3
−2
−1
2
1
2
2
3
2
−1
6
0
3
2
2
3
2
3
0
−3
−5
3
⎞
⎟
⎟
⎟
⎟
⎟
⎠
.
9.2.4.(a) Determine the eigenvalues and spectral radius of the matrix T =
⎛
⎜
⎝
3
2
−2
−2
1
0
0
2
1
⎞
⎟
⎠.
(b) Use part (a) to ﬁnd the eigenvalues and spectral radius of T =
⎛
⎜
⎜
⎜
⎝
3
5
2
5
−2
5
−2
5
1
5
0
0
2
5
1
5
⎞
⎟
⎟
⎟
⎠.
(c) Write down an asymptotic formula for the solutions to u(k+1) = T u(k).
9.2.5.(a) Show that the spectral radius of T =

1
1
0
1
	
is ρ(T) = 1.
(b) Show that most iterates u(k) = T ku(0) become unbounded as k →∞.
(c) Discuss why the inequality ∥u(k) ∥≤C ρ(T)k does not hold when the coeﬃcient
matrix is incomplete. (d) Can you prove that (9.28) holds in this example?

492
9 Iteration
9.2.6. Given a linear iterative system with non-convergent matrix, which solutions, if any,
will converge to 0?
♦9.2.7. Suppose T is a complete matrix. (a) Prove that every solution to the corresponding
linear iterative system is bounded if and only if ρ(T) ≤1. (b) Can you generalize this
result to incomplete matrices? Hint: Look at Exercise 9.1.40.
♥9.2.8. Discuss the asymptotic behavior of solutions to an iterative system that has two
eigenvalues of largest modulus, e.g., λ1 = −λ2, or λ1 = λ2 are complex conjugate
eigenvalues. How would you detect this? How can you determine the eigenvalues and
eigenvectors?
9.2.9. Suppose T has spectral radius ρ(T). Can you predict the spectral radius of aT + b I ,
where a, b are scalars? If not, what additional information do you need?
9.2.10. Prove that if A is any square matrix, then there exists c ̸= 0 such that the scalar
multiple cA is a convergent matrix. Find a formula for the largest possible such c.
♥9.2.11. Let Mn be the n × n tridiagonal matrix with all 1’s on the sub- and super-diagonals,
and zeros on the main diagonal. (a) What is the spectral radius of Mn? Hint: Use
Exercise 8.2.47. (b) Is Mn convergent? (c) Find the general solution to the iterative
system u(k+1) = Mn u(k).
♥9.2.12. Let α, β be scalars. Let Tα,β be the n × n tridiagonal matrix that has all α’s on the
sub- and super-diagonals, and β’s on the main diagonal. (a) Solve the iterative system
u(k+1) = Tα,β u(k). (b) For which values of α, β is the system asymptotically stable?
Hint: Combine Exercises 9.2.11 and 9.1.32.
9.2.13.(a) Prove that if | det T | > 1, then the iterative system u(k+1) = T u(k) is unstable.
(b) If | det T | < 1, is the system asymptotically stable? Prove or give a counterexample.
9.2.14. True or false:
(a) ρ(cA) = c ρ(A), (b) ρ(S−1AS) = ρ(A), (c) ρ(A2) = ρ(A)2,
(d) ρ(A−1) = 1/ρ(A), (e) ρ(A + B) = ρ(A) + ρ(B), (f ) ρ(AB) = ρ(A) ρ(B).
9.2.15. True or false: (a) If T is convergent, then T 2 is convergent.
(b) If A is convergent, then T = ATA is convergent.
9.2.16. Suppose T k →P as k →∞. (a) Prove that P is idempotent: P 2 = P.
(b) Can you characterize all such matrices P?
(c) What are the conditions on the matrix A for this to happen?
9.2.17. Prove that a matrix T with all integer entries is convergent if and only if it is nilpotent,
i.e., T k = O for some k ≥0. Give a nonzero example of such a matrix.
♦9.2.18. Prove the inequality (9.28) when T is incomplete. Use it to complete the proof of
Theorem 9.14 in the incomplete case. Hint: Use Exercises 9.1.40, 9.2.22.
♦9.2.19. Suppose that M is a nonsingular matrix. (a) Prove that the implicit iterative system
M u(n+1) = u(n) has globally asymptotically stable zero solution if and only if all
the eigenvalues of M are strictly greater than one in magnitude: | μi | > 1. (b) Let
K be another matrix. Prove that more general implicit iterative system of the form
M u(n+1) = Ku(n) has globally asymptotically stable zero solution if and only if all the
generalized eigenvalues of the matrix pair K, M, as in Exercise 8.5.8, are strictly less than 1
in magnitude: | λi | < 1.
♦9.2.20. The stable subspace S ⊂Rn for a linear iterative system u(k+1) = T u(k) is deﬁned
as the set of all points a such that the solution with initial condition u(0) = a satisﬁes
u(k) →0 as k →∞. (a) Prove that S is an invariant subspace for the matrix T.
(b) Determine necessary and suﬃcient conditions for a ∈S.
(c) Find the stable subspace for the linear systems in Exercise 9.1.14

9.2 Stability
493
♥9.2.21. Consider a second order iterative system u(k+2) = Au(k+1) + B u(k), where A, B
are n × n matrices. Deﬁne a quadratic eigenvalue to be a complex number that satisﬁes
det(λ2 I −λA −B) = 0. Prove that the zero solution is globally asymptotically stable if
and only if all its quadratic eigenvalues satisfy | λ | < 1.
♦9.2.22. Let p(t) be a polynomial. Assume 0 < λ < μ. Prove that there is a positive constant C
such that p(n) λn < C μn for all n > 0.
Fixed Points
The zero vector 0 is always a ﬁxed point for a linear iterative system u(k+1) = T u(k),
since 0 = T 0, and so u(k) ≡0 is an equilibrium solution. Are there any others? The
answer is immediate: u⋆is a ﬁxed point if and only if u⋆= T u⋆, and hence u⋆satisﬁes the
eigenvalue equation for T with for the unit eigenvalue λ = 1. Thus, the system admits a
nonzero ﬁxed point if and only if the coeﬃcient matrix T has 1 as an eigenvalue. Since every
nonzero scalar multiple of the eigenvector u⋆is also an eigenvector, in such cases the system
has inﬁnitely many ﬁxed points, namely all elements of the eigenspace V1 = ker(T −I ),
including 0. We are interested in whether the ﬁxed points are stable in the sense that
solutions having nearby initial conditions remain nearby. More precisely:
Deﬁnition 9.16. A ﬁxed point u⋆of an iterative system u(k+1) = T u(k) is called stable if
for every ε > 0 there exists a δ > 0 such that whenever ∥u(0) −u⋆∥< δ, then the resulting
iterates satisfy ∥u(k) −u⋆∥< ε for all k.
The stability of the ﬁxed points, at least if the coeﬃcient matrix is complete, is governed
by the same solution formula (9.9).
If the eigenvalue λ1 = 1 is simple, and all other
eigenvalues are less than one in modulus, so
1 = λ1 > | λ2 | ≥· · · ≥| λn |,
then the solution takes the asymptotic form
u(k) = c1 v1 + c2 λk
2 v2 + · · · + cn λk
n vn −→c1 v1,
as
k −→∞,
(9.29)
converging to one of the ﬁxed points, i.e., to a multiple of the eigenvector v1. The coeﬃcient
c1 is prescribed by the initial conditions, cf. (9.10). The rate of convergence of the solution
is governed by the modulus | λ2 | of the subdominant eigenvalue.
Proposition 9.17. Suppose that T has a simple (or, more generally, complete) eigenvalue
λ1 = 1, and, moreover, all other eigenvalues satisfy | λj | < 1. Then all solutions to the
linear iterative system u(k+1) = T u(k) converge to a vector v ∈V1 that lies in the λ1 = 1
eigenspace. Moreover, all the ﬁxed points v ∈V1 of T are stable.
Stability of a ﬁxed point does not imply asymptotic stability, since nearby solutions
may converge to a nearby ﬁxed point, i.e., a slightly diﬀerent element of the eigenspace V1.
The general necessary and suﬃcient conditions for stability of the ﬁxed points of a linear
iterative system is governed by the spectral radius of its coeﬃcient matrix, as follows. The
proof is relegated to Exercise 9.2.28.
Theorem 9.18. The ﬁxed points of an iterative system u(k+1) = T u(k) are stable if and
only if ρ(T) ≤1 and, moreover, every eigenvalue of modulus | λ | = 1 is complete.

494
9 Iteration
Thus, with regard to linear iterative systems, either all ﬁxed points are stable or all
are unstable. Keep in mind that the ﬁxed points are the elements of the eigenspace V1
corresponding to the eigenvalue λ = 1, if such exists. If 1 is not an eigenvalue of T, then
u⋆= 0 is the only ﬁxed point.
Example 9.19.
Consider the iterative system with coeﬃcient matrix
T =
⎛
⎜
⎝
3
2
−1
2
−3
−1
2
1
2
1
1
2
1
2
0
⎞
⎟
⎠.
The eigenvalues and corresponding eigenvectors are
λ1 = 1,
λ2 = 1 + i
2
,
λ3 = 1 −i
2
,
v1 =
⎛
⎝
4
−2
1
⎞
⎠,
v2 =
⎛
⎝
2 −i
−1
1
⎞
⎠,
v3 =
⎛
⎝
2 + i
−1
1
⎞
⎠.
Since λ1 = 1, every scalar multiple of the eigenvector v1 is a ﬁxed point. The ﬁxed points
are stable, since the remaining eigenvalues have modulus | λ2 | = | λ3 | = 1
2
√
2 ≈.7071 < 1.
Thus, the iterates u(k) = T ka −→c1 v1 will eventually converge to a multiple of the ﬁrst
eigenvector; in almost all cases the convergence rate is 1
2
√
2 . For example, starting with
u(0) = ( 1, 1, 1 )T , leads to the iterates†
u(5) =
⎛
⎝
−9.5
4.75
−2.75
⎞
⎠,
u(10) =
⎛
⎝
−7.9062
3.9062
−1.9062
⎞
⎠,
u(15) =
⎛
⎝
−7.9766
4.0
−2.0
⎞
⎠,
u(20) =
⎛
⎝
−8.0088
4.0029
−2.0029
⎞
⎠,
u(25) =
⎛
⎝
−7.9985
3.9993
−1.9993
⎞
⎠,
u(30) =
⎛
⎝
−8.0001
4.0001
−2.0001
⎞
⎠,
which are gradually converging to the particular eigenvector ( −8, 4, −2 )T = −2v1. This
can be predicted in advance by decomposing the initial vector into a linear combination of
the eigenvectors:
u(0) =
⎛
⎝
1
1
1
⎞
⎠= −2
⎛
⎝
4
−2
1
⎞
⎠+ 3 + 3 i
2
⎛
⎝
2 −i
−1
1
⎞
⎠+ 3 −3 i
2
⎛
⎝
2 + i
−1
1
⎞
⎠,
whence
u(k) =
⎛
⎝
−8
4
−2
⎞
⎠+ 3 + 3 i
2
 1 + i
2
k
⎛
⎝
2 −i
−1
1
⎞
⎠+ 3 −3 i
2
 1 −i
2
k
⎛
⎝
2 + i
−1
1
⎞
⎠,
and so u(k) →( −8, 4, −2 )T as k →∞. Despite the complex formula, the solution is, in
fact, real.
†
Since the convergence is slow, we only display every ﬁfth one.

9.2 Stability
495
Exercises
9.2.23. Find all ﬁxed points for the iterative systems with the following coeﬃcient matrices:
(a)

.7
.3
.2
.8
	
,
(b)

.6
1.0
.3
−.7
	
,
(c)
⎛
⎜
⎝
−1
−1
−4
−2
0
−4
1
−1
0
⎞
⎟
⎠,
(d)
⎛
⎜
⎝
2
1
−1
2
3
−2
−1
−1
2
⎞
⎟
⎠.
9.2.24. Discuss the stability of each ﬁxed point and the asymptotic behavior(s) of the solutions
to the systems in Exercise 9.2.23. Which ﬁxed point, if any, does the solution with initial
condition u(0) = e1 converge to?
9.2.25. Suppose T is a symmetric matrix that satisﬁes the hypotheses of Proposition 9.17 with
a simple eigenvalue λ1 = 1. Prove that the solution u(k) to the linear iterative system
u(k+1) = T u(k) has limiting value
lim
k →∞u(k) = u(0) · v1
∥v1 ∥2 v1.
9.2.26. True or false: If T has a stable nonzero ﬁxed point, then it is a convergent matrix.
9.2.27. True or false: If every point u ∈Rn is a ﬁxed point, then they are all stable. Can you
characterize such systems?
♦9.2.28. Prove Theorem 9.18:
(a) assuming T is complete, (b) for general T.
Hint: Use Exercise 9.1.40.
♥9.2.29.(a) Under what conditions does the linear iterative system u(k+1) = T u(k) have a
period 2 solution, meaning that the iterates repeat after every other iterate: u(k+2) =
u(k) ̸= u(k+1)? Give an example of such a system. (b) Under what conditions is there a
unique period 2 solution?
(c) What about a period m solution for 2 < m ∈N?
Matrix Norms and Convergence
As we now know, the convergence of a linear iterative system is governed by the spectral
radius, or, equivalently, the modulus of the largest eigenvalue of the coeﬃcient matrix.
Unfortunately, ﬁnding accurate approximations to the eigenvalues of most matrices is a
nontrivial computational task. Indeed, as we will learn in Section 9.5, all practical nu-
merical algorithms rely on some form of iteration. But using iteration to determine the
spectral radius defeats the purpose, which is to predict the behavior of the iterative system
in advance! One independent means of accomplishing this is through matrix norms, as
introduced at the end of Section 3.3.
Let ∥v∥denote a norm on† Rn. Theorem 3.20 deﬁnes the induced natural matrix norm
on the space of n × n matrices, denoted by ∥A∥= max{ ∥Au∥| ∥u∥= 1 }. The following
result relates the magnitude of the norm of a matrix to convergence of the associated
iterative system.
Proposition 9.20. If A is a square matrix, then ∥Ak ∥≤∥A∥k. In particular, if ∥A∥< 1,
then ∥Ak ∥→0 as k →∞, and hence A is a convergent matrix: Ak →O.
The ﬁrst part is a restatement of Proposition 3.22, and the second part is an immediate
consequence. The converse to this result is not quite true; a convergent matrix does not
†
We work with real iterative systems throughout this chapter, but the methods readily extend
to their complex counterparts.

496
9 Iteration
necessarily have matrix norm less than 1, or even ≤1 — see Example 9.23 below. An
alternative proof of Proposition 9.20 can be based on the following useful estimate:
Theorem 9.21. The spectral radius of a matrix is bounded by its matrix norm:
ρ(A) ≤∥A∥.
(9.30)
Proof : If λ is a real eigenvalue, and u a corresponding unit eigenvector, so that Au = λ u
with ∥u∥= 1, then
∥Au∥= ∥λ u∥= | λ | ∥u∥= | λ |.
(9.31)
Since ∥A∥is the maximum of ∥Au∥over all possible unit vectors, this implies that
| λ | ≤∥A∥.
(9.32)
If all the eigenvalues of A are real, then the spectral radius is the maximum of their absolute
values, and so it too is bounded by ∥A∥, proving (9.30).
If A has complex eigenvalues, then we need to work a little harder to establish (9.32).
(This is because the matrix norm is deﬁned by the eﬀect of A on real vectors, and so
we cannot directly use the complex eigenvectors to establish the required bound.)
Let
λ = r e i θ be a complex eigenvalue with complex eigenvector z = x + i y. Deﬁne
μ = min

∥Re (e i ϕ z)∥= ∥(cos ϕ) x −(sin ϕ) y∥
 0 ≤ϕ ≤2π

.
(9.33)
Since the indicated subset is a closed curve (in fact, an ellipse) that does not go through
the origin†, μ > 0. Let ϕ0 denote the value of the angle that produces the minimum, so
μ = ∥(cosϕ0) x −(sin ϕ0) y∥= ∥Re

e i ϕ0 z

∥.
Deﬁne the real unit vector
u = Re (e i ϕ0 z)
μ
= (cos ϕ0) x −(sin ϕ0) y
μ
,
so that
∥u∥= 1.
Then
Au = 1
μ Re

e i ϕ0 A z

= 1
μ Re

e i ϕ0 r e i θ z

= r
μ Re

e i (ϕ0+θ) z

.
Therefore, keeping in mind that m is the minimal value in (9.33),
∥A∥≥∥Au∥= r
μ ∥Re

e i (ϕ0+θ) z

∥≥r = | λ |,
(9.34)
and so (9.32) also holds for complex eigenvalues.
Q.E.D.
Let us see what the convergence criterion of Proposition 9.20 says for a couple of our
well-known matrix norms. First, the formula (3.44) for the ∞norm implies the following
convergence criterion.
Proposition 9.22.
If all the absolute row sums of A are strictly less than 1, then
∥A∥∞< 1 and hence A is a convergent matrix.
Example 9.23.
Consider the symmetric matrix A =

1
2
−1
3
−1
3
1
4

. Its two absolute
row sums are
 1
2
 +
 −1
3
 = 5
6 ,
 −1
3
 +
 1
4
 =
7
12 , so
∥A∥∞= max
 5
6, 7
12

= 5
6 = .83333 . . . .
†
This relies on the fact that x, y are linearly independent, which was shown in Exercise 8.3.12.

9.2 Stability
497
Since the norm is less than 1, A is a convergent matrix. Indeed, its eigenvalues are
λ1 = 9 +
√
73
24
= .731000 . . . ,
λ2 = 9 −
√
73
24
= .018999 . . . ,
and hence the spectral radius is ρ(A) = λ1 = .731000 . . ., which is slightly smaller than its
∞norm.
The row sum test for convergence is not always conclusive. For example, the matrix
A =

1
2
−3
5
−3
5
1
4

has matrix norm
∥A∥∞= 11
10 > 1.
On the other hand, its eigenvalues are 15 ±
√
601
40
and hence its spectral radius is
ρ(A) = 15 +
√
601
40
= .987882 . . . , which implies that A is (just barely) convergent, even
though its maximal row sum is larger than 1.
Similarly, using the formula (8.61) for the Euclidean matrix norm, one deduces a con-
vergence criterion based on the magnitude of the singular values.
Proposition 9.24. If A is a square matrix whose largest singular value satisﬁes σ1 < 1,
then ∥A∥2 < 1 and hence A is a convergent matrix.
Example 9.25.
Consider the matrix and associated Gram matrix
A =
⎛
⎜
⎝
0
−1
3
1
3
1
4
0
1
2
2
5
1
5
0
⎞
⎟
⎠,
ATA =
⎛
⎝
.2225
.0800
.1250
.0800
.1511
−.1111
.1250
−.1111
.3611
⎞
⎠.
Then ATA has eigenvalues λ1 = .4472, λ2 = .2665, λ3 = .0210, and hence the singular
values of A are their square roots: σ1 = .6687, σ2 = .5163, σ3 = .1448. The Euclidean
matrix norm of A is the largest singular value, and so ∥A∥2 = .6687, proving that A is
a convergent matrix. Note that, as always, the matrix norm overestimates the spectral
radius, which is ρ(A) = .5.
Unfortunately, as we discovered in Example 9.23, matrix norms are not a foolproof test
of convergence. There exist convergent matrices such that ρ(A) < 1 that yet have matrix
norm ∥A∥≥1. In such cases, the matrix norm is not able to predict convergence of the
iterative system, although one should expect the convergence to be quite slow. Although
such pathology might show up in the chosen matrix norm, it turns out that one can always
rig up some matrix norm for which ∥A∥< 1. This follows from a more general result,
whose proof can be found in [62].
Theorem 9.26. Let A have spectral radius ρ(A). If ε > 0 is any positive number, then
there exists a matrix norm ∥·∥such that
ρ(A) ≤∥A∥< ρ(A) + ε.
(9.35)
Corollary 9.27. If A is a convergent matrix, then there exists a matrix norm such that
∥A∥< 1.
Proof : By deﬁnition, A is convergent if and only if ρ(A) < 1. Choose ε > 0 such that
ρ(A) + ε < 1. Any norm that then satisﬁes (9.35) has the desired property.
Q.E.D.

498
9 Iteration
It can also be proved, [48], that, given a matrix norm,
lim
n →∞∥An ∥1/n = ρ(A),
and
hence, if A is convergent, then ∥An ∥< 1 for n suﬃciently large.
Warning. Based on the accumulated evidence, one might be tempted to speculate that
the spectral radius itself deﬁnes a matrix norm. Unfortunately, this is not the case. For
example, the nonzero matrix A =

0
1
0
0

has zero spectral radius, ρ(A) = 0, in violation
of a basic norm axiom.
Exercises
9.2.30. Compute the ∞matrix norm of the following matrices. Which are guaranteed to be
convergent?
(a)
⎛
⎝
1
2
1
4
1
3
1
6
⎞
⎠, (b)
⎛
⎝
5
3
4
3
−7
6
−5
6
⎞
⎠, (c)
⎛
⎝
2
7
−2
7
−2
7
6
7
⎞
⎠, (d)
⎛
⎝
1
4
3
2
−1
2
5
4
⎞
⎠,
(e)
⎛
⎜
⎜
⎜
⎝
2
7
2
7
−4
7
0
2
7
6
7
2
7
4
7
2
7
⎞
⎟
⎟
⎟
⎠, (f )
⎛
⎜
⎝
0
.1
.8
−.1
0
.1
−.8
−.1
0
⎞
⎟
⎠, (g)
⎛
⎜
⎜
⎜
⎝
1
−2
3
−2
3
1
−1
3
−1
1
3
−2
3
0
⎞
⎟
⎟
⎟
⎠, (h)
⎛
⎜
⎜
⎜
⎝
1
3
0
0
−1
3
0
1
3
0
2
3
1
3
⎞
⎟
⎟
⎟
⎠.
9.2.31. Compute the Euclidean matrix norm of each matrix in Exercise 9.2.30. Have your
convergence conclusions changed?
9.2.32. Compute the spectral radii of the matrices in Exercise 9.2.30. Which are convergent?
Compare your conclusions with those of Exercises 9.2.30 and 9.2.31.
9.2.33. Let k be an integer and set Ak =
 k
−1
k2
−k
	
. Compute (a) ∥Ak ∥∞, (b) ∥Ak ∥2,
(c) ρ(Ak).
(d) Explain why every Ak is a convergent matrix, even though their matrix
norms can be arbitrarily large.
(e) Why does this not contradict Corollary 9.27?
9.2.34. Show that if | c | < 1/∥A∥, then cA is a convergent matrix.
♦9.2.35. Prove that the spectral radius function does not satisfy the triangle inequality by
ﬁnding matrices A, B such that ρ(A + B) > ρ(A) + ρ(B).
9.2.36. Find a convergent matrix that has dominant singular value σ1 > 1.
♦9.2.37. Prove that if A is a real symmetric matrix, then its Euclidean matrix norm is equal to
its spectral radius.
♦9.2.38. Let A be a square matrix. Let s = max{s1, . . . , sn} be the maximal absolute row sum
of A and let t = min

| aii | −ri

, with ri given by (8.27). Prove that max{0, t} ≤ρ(A) ≤
s.
9.2.39. Suppose the largest entry (in modulus) of A is | aij | = a⋆. Can you bound its radius of
convergence?
9.2.40.(a) Suppose that every entry of the n × n matrix A is bounded by | aij | < 1/n . Prove
that A is a convergent matrix. Hint: Use Exercise 9.2.38.
(b) Produce a matrix of size
n × n with one or more entries satisfying | aij | = 1/n that is not convergent.
9.2.41. Write down an example of a strictly diagonally dominant matrix that is also
convergent.
9.2.42. True or false: If B = S−1AS are similar matrices, then
(a) ∥B ∥∞= ∥A∥∞,
(b) ∥B ∥2 = ∥A∥2,
(c) ρ(B) = ρ(A).
9.2.43. Prove that the curve parametrized in (9.33) is an ellipse. What are its semi-axes?

9.3 Markov Processes
499
♦9.2.44.(a) Prove that the individual entries aij of a matrix A are bounded in absolute value
by its ∞matrix norm: | aij | ≤∥A∥∞. (b) Prove that if the series
∞

n=0
∥An ∥∞< ∞
converges, then the matrix series
∞

n=0
An = A⋆converges to some matrix A⋆.
(c) Let ∥A∥denote any natural matrix norm. Prove that if the series
∞

n=0
∥An ∥< ∞
converges, then the matrix series
∞

n=0
An = A⋆converges.
9.2.45.(a) Use Exercise 9.2.44 to prove that the geometric matrix series
∞

n=0
An converges
whenever ρ(A) < 1. Hint: Apply Corollary 9.27.
(b) Prove that the sum equals ( I −A)−1. How do you know I −A is invertible?
9.3 Markov Processes
A discrete probabilistic process in which the future state of a system depends only upon
its current conﬁguration is known as a Markov chain, to honor the pioneering early twen-
tieth studies of the Russian mathematician Andrei Markov. Markov chains are described
by linear iterative systems whose coeﬃcient matrices have a special form. They deﬁne the
simplest examples of stochastic processes, [4, 23], which have many profound physical, bio-
logical, economic, and statistical applications, including networks, internet search engines,
speech recognition, and routing.
To take a very simple (albeit slightly artiﬁcial) example, suppose you would like to be
able to predict the weather in your city. Consulting local weather records over the past
decade, you determine that
(a) If today is sunny, there is a 70% chance that tomorrow will also be sunny,
(b) But, if today is cloudy, the chances are 80% that tomorrow will also be cloudy.
Question: given that today is sunny, what is the probability that next Saturday’s weather
will also be sunny?
To formulate this process mathematically, we let s(k) denote the probability that day
k is sunny and c(k) the probability that it is cloudy. If we assume that these are the only
possibilities, then the individual probabilities must sum to 1, so
s(k) + c(k) = 1.
According to our data, the probability that the next day is sunny or cloudy is expressed
by the equations
s(k+1) = .7 s(k) + .2 c(k),
c(k+1) = .3 s(k) + .8 c(k).
(9.36)
Indeed, day k + 1 could be sunny either if day k was, with a 70% chance, or, if day k was
cloudy, there is still a 20% chance of day k + 1 being sunny. We rewrite (9.36) in a more
convenient matrix form:
u(k+1) = T u(k),
where
T =

.7
.2
.3
.8

,
u(k) =

s(k)
c(k)

.
(9.37)
In a Markov process, the vector of probabilities u(k) is known as the kth state vector and the
matrix T is known as the transition matrix, whose entries ﬁx the transition probabilities
between the various states.

500
9 Iteration
Figure 9.4.
The Set of Probability Vectors in R3.
By assumption, the initial state vector is u(0) = ( 1, 0 )T , since we know for certain that
today is sunny. Rounded oﬀto three decimal places, the subsequent state vectors are
u(1) ≃

.7
.3

,
u(2) ≃

.55
.45

,
u(3) ≃

.475
.525

,
u(4) ≃

.438
.563

,
u(5) ≃

.419
.581

,
u(6) ≃

.410
.591

,
u(7) ≃

.405
.595

,
u(8) ≃

.402
.598

.
The iterates converge fairly rapidly to ( .4, .6 )T , which is, in fact, a ﬁxed point for the
iterative system (9.37). Thus, in the long run, 40% of the days will be sunny and 60% will
be cloudy. Let us explain why this happens.
Deﬁnition 9.28. A vector u = ( u1, u2, . . . , un )T ∈Rn is called a probability vector if all
its entries lie between 0 and 1, so 0 ≤ui ≤1 for i = 1, . . . , n, and, moreover, their sum is
u1 + · · · + un = 1.
We interpret the entry ui of a probability vector as the probability that the system
is in state number i. The fact that the entries add up to 1 means that they represent a
complete list of probabilities for the possible states of the system. The set of probability
vectors deﬁnes an (n−1)-dimensional simplex in Rn. For example, the possible probability
vectors u ∈R3 ﬁll the equilateral triangle plotted in Figure 9.4.
Remark. Every nonzero vector 0 ̸= v = ( v1, v2, . . . , vn )T with all non-negative entries,
vi ≥0 for i = 1, . . ., n, can be converted into a parallel probability vector by dividing by
the sum of its entries:
u =
v
v1 + · · · + vn
.
(9.38)
For example, if v = ( 3, 2, 0, 1 )T , then u =
 1
2, 1
3, 0, 1
6
T is the corresponding probability
vector.
In general, a Markov chain is represented by a ﬁrst order linear iterative system
u(k+1) = T u(k),
(9.39)
whose initial state u(0) is a probability vector. The entries of the transition matrix T must
satisfy
0 ≤tij ≤1,
t1j + · · · + tnj = 1.
(9.40)

9.3 Markov Processes
501
The entry tij represents the transitional probability that the system will switch from state
j to state i. (Note the reversal of indices.) Since this covers all possible transitions, the
column sums of the transition matrix are all equal to 1, and hence each column of T
is a probability vector, which is equivalent to condition (9.40).
In Exercise 9.3.24 you
are asked to show that, under these assumptions, if u(k) is a probability vector, then so
is u(k+1) = T u(k), and hence, given our assumption on the initial state, the solution
u(k) = T k u(0) to the Markov process deﬁnes a sequence, or “chain”, of probability vectors.
Let us now investigate the convergence of the Markov chain. Not all Markov chains
converge — see Exercise 9.3.9 for an example — and so we impose some additional mild
restrictions on the transition matrix.
Deﬁnition 9.29. A transition matrix (9.40) is regular if some power T k contains no zero
entries. In particular, if T itself has no zero entries, then it is regular.
Warning. The term “regular transition matrix” has nothing to do with our earlier term
“regular matrix”, which was used to describe matrices with an LU factorization.
The entries of T k describe the transition probabilities of getting from one state to
another in k steps. Thus, regularity of the transition matrix means that there is a nonzero
probability of getting from any state to any other state in exactly k steps for some k ≥1.
The asymptotic behavior of a regular Markov chain is governed by the following basic
result, originally due to the German mathematicians Oskar Perron and Georg Frobenius
in the early part of the twentieth century. A proof can be found at the end of this section.
Theorem 9.30. If T is a regular transition matrix, then it admits a unique probability
eigenvector u⋆with eigenvalue λ1 = 1. Moreover, a Markov chain with coeﬃcient matrix
T will converge to the probability eigenvector: u(k) →u⋆as k →∞.
Example 9.31.
The eigenvalues and eigenvectors of the weather transition matrix (9.37)
are
λ1 = 1,
v1 =

2
3
1

,
λ2 = .5,
v2 =

−1
1

.
The ﬁrst eigenvector is then converted into a probability vector via formula (9.38):
u⋆= u1 =
1
1 + 2
3

2
3
1

=

2
5
3
5

.
This distinguished probability eigenvector represents the ﬁnal asymptotic state of the sys-
tem after many iterations, no matter what the initial state is. Thus, our earlier observation
that about 40% of the days will be sunny and 60% will be cloudy does not depend upon
today’s weather.
Example 9.32.
A taxi company in Minnesota serves the cities of Minneapolis and
St. Paul, as well as the nearby suburbs. Records indicate that, on average, 10% of the
customers taking a taxi in Minneapolis go to St. Paul and 30% go to the suburbs. Cus-
tomers boarding in St. Paul have a 30% chance of going to Minneapolis and a 30% chance
of going to the suburbs, while suburban customers choose Minneapolis 40% of the time and
St. Paul 30% of the time. The owner of the taxi company is interested in knowing where
the taxis will end up, on average. Let us write this as a Markov process. The entries of
the state vector u(k) = (u(k)
1 , u(k)
2 , u(k)
3 )T tell what proportion of the taxi ﬂeet is, respec-

502
9 Iteration
tively, in Minneapolis, St. Paul, and the suburbs, or, equivalently, the probability that an
individual taxi will be in one of the three locations. Using the given data, we construct
the relevant transition matrix
T =
⎛
⎝
.6
.3
.4
.1
.4
.3
.3
.3
.3
⎞
⎠.
Note that T is regular since it has no zero entries. The probability eigenvector
u⋆≃( .4714, .2286, .3 )T
corresponding to the unit eigenvalue λ1 = 1 is found by ﬁrst solving the linear system
(T −I )v⋆= 0 and then converting the solution† v⋆into a valid probability vector u⋆by
use of formula (9.38). According to Theorem 9.30, no matter how the taxis are initially
distributed, eventually about 47% of the taxis will be in Minneapolis, 23% in St. Paul, and
30% in the suburbs. This can be conﬁrmed by running numerical experiments. Moreover,
if the owner places this fraction of the taxis in the three locations, then they will more or
less remain in such proportions forever.
Remark. As noted earlier — see Proposition 9.17 — the convergence rate of the Markov
chain to its steady state is governed by the size of the subdominant eigenvalue λ2. The
smaller | λ2 | is, the faster the process converges. In the taxi example, λ2 = .3 (and λ3 = 0),
and so the convergence to steady state is fairly rapid.
A Markov process can also be viewed as a weighted digraph. Each state corresponds
to a vertex. A nonzero transition probability from one state to another corresponds to a
weighted directed edge between the two vertices. Note that the digraph is typically not
simple, since vertices can have two edges connecting them, one representing the transition
probability of getting from the ﬁrst to the second, and the second edge representing the
transition probability of going in the other direction. The original PageRank algorithm
that underlies Google’s search engine, [64, 52], starts with the internet digraph, whose
vertices are web pages and whose directed edges represent links from one web page to
another, which are weighted according to the number of such links. To be eﬀective, the
resulting weighted internet digraph is supplemented by adding in a number of random low
weight edges. One then computes the probability eigenvector associated with the resulting
digraph-based Markov process, the magnitudes of whose entries, indexed by the nodes,
eﬀectively rank the corresponding web pages.
Proof of Theorem 9.30:
We begin the proof by replacing T by its transpose‡ M = T T ,
keeping in mind that every eigenvalue of T is also an eigenvalue of M albeit with diﬀerent
eigenvectors, cf. Proposition 8.12. The conditions (9.40) tell us that the matrix M has
entries 0 ≤mij = tji ≤1, and, moreover, the row sums si = 3n
i=1 mij = 1 of M, being
the same as the corresponding column sums of T, are all equal to 1. Since M k = (T k)T ,
regularity of T implies that some power M k has all positive entries.
According to Exercise 1.2.29, if z = ( 1, . . . , 1 )T is the column vector all of whose entries
are equal to 1, then the entries of M z are the row sums of M. Therefore, M z = z, which
implies that z is an eigenvector of M with eigenvalue λ1 = 1. As a consequence, T also has
†
Theorem 9.30 guarantees that there is an eigenvector v with all non-negative entries.
‡
We apologize for the unfortunate clash of notation when writing the transpose of the matrix T.

9.3 Markov Processes
503
1
−1
−1
1
Figure 9.5.
Gershgorin Disks for a Regular Transition Matrix.
1 as an eigenvalue. Observe that z is not in general an eigenvector of T; indeed, it satisﬁes
the co-eigenvector equation M z = T Tz = z.
We claim that λ1 = 1 is a simple eigenvalue. To this end, we prove that z spans the
one-dimensional eigenspace V1. In other words, we need to show that if M v = v, then its
entries v1 = · · · = vn = a are all equal, and so v = az is a scalar multiple of the known
eigenvector z. Let us ﬁrst prove this assuming that all of the entries of M are strictly
positive, and so 0 < mij = tji < 1 for all i, j. Suppose v is an eigenvector with not all
equal entries. Let vk be the minimal entry of v, so vk ≤vi for all i ̸= k, and at least one
inequality is strict, say vk < vj. Then the kth entry of the eigenvector equation v = M v is
vk =
n

j =1
mkj vj >
⎛
⎝
n

j =1
mkj
⎞
⎠vk = vk,
where the strict inequality follows from the assumed positivity of the entries of M, and
the ﬁnal equality follows from the fact that M has unit row sums. Thus, we are led to
a contradiction, and the claim follows. If M has one or more 0 entries, but M k has all
positive entries, then we apply the previous argument to the equation M k v = v which
follows from M v = v. If λ1 = 1 is a complete eigenvalue, then we are ﬁnished. The proof
that this is indeed the case is a bit technical, and we refer the reader to [4] for the details.
Finally, let us prove that all the other eigenvalues of M are less than 1 in modulus.
For this we appeal to the Gershgorin Circle Theorem 8.16. Suppose M k has all positive
entries, denoted by m(k)
ij > 0. Its Gershgorin disk Di is centered at m(k)
ii
> 0 and has radius
ri = 1 −m(k)
ii
< 1 since the ith row sum of M k equals 1. Thus the disk lies strictly inside
the open unit disk | z | < 1 except for a single boundary point at z = 1; see Figure 9.5. The
Circle Theorem 8.16 implies that all eigenvalues of M k except the unit eigenvalue λ1 = 1
must lie strictly inside the unit disk. Since these are just the kth powers of the eigenvalues
of M, the same holds for the eigenvalues themselves, so | λj | < 1 for j ≥2.
Therefore, the matrix M, and, hence, also T, satisﬁes the hypotheses of Proposition 9.17.
We conclude that the iterates u(k) = T ku(0) →u⋆converge to a multiple of the probability
eigenvector of T.
If the initial condition u(0) is a probability vector, then so is every
subsequent state vector u(k), and so their limit u⋆must also be a probability vector. This
completes the proof of the theorem.
Q.E.D.

504
9 Iteration
Exercises
9.3.1.
Determine if the following matrices are regular transition matrices. If so, ﬁnd the
associated probability eigenvector.
(a)
⎛
⎝
1
2
1
3
3
4
2
3
⎞
⎠, (b)
⎛
⎝
1
4
3
4
2
3
1
3
⎞
⎠, (c)
⎛
⎝
1
4
2
3
3
4
1
3
⎞
⎠,
(d)
⎛
⎝0
1
5
1
4
5
⎞
⎠, (e)
⎛
⎜
⎜
⎝
0
1
0
1
0
0
0
0
1
⎞
⎟
⎟
⎠, (f )
⎛
⎜
⎝
.3
.5
.2
.3
.2
.5
.4
.3
.3
⎞
⎟
⎠, (g)
⎛
⎜
⎝
.1
.5
.4
.6
.1
.3
.3
0
.7
⎞
⎟
⎠,
(h)
⎛
⎜
⎜
⎜
⎝
1
2
1
2
1
3
1
2
0
1
3
0
1
2
1
3
⎞
⎟
⎟
⎟
⎠, (i)
⎛
⎜
⎜
⎜
⎝
0
.2
0
1
.5
0
.3
0
0
.8
0
0
.5
0
.7
0
⎞
⎟
⎟
⎟
⎠, (j)
⎛
⎜
⎜
⎜
⎝
.1
.2
.3
.4
.2
.5
.3
.1
.3
.3
.1
.3
.4
.1
.3
.2
⎞
⎟
⎟
⎟
⎠, (k)
⎛
⎜
⎜
⎜
⎝
0
.6
0
.4
.5
0
.3
.1
0
.4
0
.5
.5
0
.7
0
⎞
⎟
⎟
⎟
⎠.
9.3.2. A business executive is managing three branches, labeled A, B, and C, of a corporation.
She never visits the same branch on consecutive days. If she visits branch A one day, she
visits branch B the next day. If she visits either branch B or C that day, then the next day
she is twice as likely to visit branch A as to visit branch B or C. Explain why the resulting
transition matrix is regular. Which branch does she visit the most often in the long run?
9.3.3. A study has determined that, on average, a man’s occupation depends on that of his
father. If the father is a farmer, there is a 30% chance that the son will be a blue collar
laborer, a 30% chance he will be a white collar professional, and a 40% chance he will also
be a farmer. If the father is a laborer, there is a 30% chance that the son will also be one,
a 60% chance he will be a professional, and a 10% chance he will be a farmer. If the father
is a professional, there is a 70% chance that the son will also be one, a 25% chance he will
be a laborer, and a 5% chance he will be a farmer. (a) What is the probability that the
grandson of a farmer will also be a farmer? (b) In the long run, what proportion of the
male population will be farmers?
9.3.4. The population of an island is divided into city and country residents. Each year, 5% of
the residents of the city move to the country and 15% of the residents of the country move
to the city. In 2003, 35,000 people live in the city and 25,000 in the country. Assuming no
growth in the population, how many people will live in the city and how many will live in
the country between the years 2004 and 2008? What is the eventual population distribution
of the island?
9.3.5. A certain plant species has either red, pink, or white ﬂowers, depending on its genotype.
If you cross a pink plant with any other plant, the probability distribution of the oﬀspring
is prescribed by the transition matrix T =
⎛
⎜
⎝
.5
.25
0
.5
.5
.5
0
.25
.5
⎞
⎟
⎠. On average, if you continue
crossing with only pink plants, what percentage of the three types of ﬂowers would you
expect to see in your garden?
9.3.6. A genetic model describing inbreeding, in which mating takes place only between
individuals of the same genotype, is given by the Markov process u(n+1) = T u(n),
where T =
⎛
⎜
⎜
⎜
⎝
1
1
4
0
0
1
2
0
0
1
4
1
⎞
⎟
⎟
⎟
⎠is the transition matrix and u(n) =
⎛
⎜
⎝
pn
qn
rn
⎞
⎟
⎠, whose entries are,
respectively, the proportion of populations of genotype AA, Aa, aa in the nth generation.
Find the solution to this Markov process and analyze your result.
9.3.7. A student has the habit that if she doesn’t study one night, she is 70% certain of
studying the next night. Furthermore, the probability that she studies two nights in a row
is 50%. How often does she study in the long run?

505
9.3.8. A traveling salesman visits the three cities of Atlanta, Boston, and Chicago. The matrix
⎛
⎜
⎝
0
.5
.5
1
0
.5
0
.5
0
⎞
⎟
⎠describes the transition probabilities of his trips. Describe his travels in
words, and calculate how often he visits each city on average.
9.3.9. Explain why the irregular Markov process with transition matrix T =

0
1
1
0
	
does
not reach a steady state. Use a population model, as in Exercise 9.3.4, to interpret what is
going on.
9.3.10. A bug crawls along the edges of the pictured triangular lattice with six
vertices. Upon arriving at a vertex, there is an equal probability of its choosing
any edge to leave the vertex. Set up the Markov chain described by the
bug’s motion, and determine how often, on average, it visits each vertex.
9.3.11. Answer Exercise 9.3.10 for the larger triangular lattice.
9.3.12. Suppose the bug of Exercise 9.3.10 crawls along the edges of the
pictured square lattice. What can you say about its behavior?
♦9.3.13. Let T be a regular transition matrix with probability eigenvector v.
(a) Prove that lim
k→∞T k = P = ( v v . . . v ) is a matrix with every column equal to v.
(b) Explain why ( v v . . . v ) v = v.
(c) Prove directly that P is idempotent: P 2 = P.
9.3.14. Find lim
k→∞T k when T =
⎛
⎜
⎝
.8
.1
.1
.1
.8
.1
.1
.1
.8
⎞
⎟
⎠.
9.3.15. Prove that, for all 0 ≤p, q ≤1 with p + q > 0, the probability eigenvector of the
transition matrix T =

1 −p
q
p
1 −q
	
is v =

q
p + q ,
p
p + q
	T
.
9.3.16. Describe the ﬁnal state of a Markov chain with symmetric transition matrix T = T T .
9.3.17. True or false: If T and T T are both transition matrices, then T = T T .
9.3.18. True or false: If T is a transition matrix, so is T −1.
9.3.19. A transition matrix is called doubly stochastic if both its row and column sums are
equal to 1. What is the limiting probability state of a Markov chain with doubly stochastic
transition matrix?
9.3.20. True or false: The set of all probability vectors forms a subspace of Rn.
9.3.21. Multiple choice: Every probability vector in Rn lies on the unit sphere for the
(a) 1 norm, (b) 2 norm, (c) ∞norm, (d) all of the above, (e) none of the above.
9.3.22. True or false: Every probability eigenvector of a regular transition matrix has
eigenvalue equal to 1.
9.3.23. Write down an example of (a) an irregular transition matrix; (b) a regular transition
matrix that has one or more zero entries.
♦9.3.24. Let T be a transition matrix. Prove that if u is a probability vector, then so is v = T u.
♦9.3.25.(a) Prove that if T and S are transition matrices, then so is their product T S.
(b) Prove that if T is a transition matrix, then so is T k for all k ≥0.
9.3 Markov Processes

506
9 Iteration
9.4 Iterative Solution of Linear Algebraic Systems
In this section, we return to the most basic problem in linear algebra: solving the linear
algebraic system
Au = b,
(9.41)
consisting of n equations in n unknowns. We assume that the n × n coeﬃcient matrix A is
nonsingular, and so the solution u = A−1b is unique. For simplicity, we shall only consider
real systems here.
We will introduce several popular iterative methods that can be used to approximate
the solution for certain classes of coeﬃcient matrices. The resulting algorithms will pro-
vide an attractive alternative to Gaussian Elimination, particularly when one is dealing
with the large, sparse systems that arise in the numerical solution to diﬀerential equations.
One major advantage of an iterative technique is that, in favorable situations, it produces
progressively more and more accurate approximations to the solution, and hence, by pro-
longing the iterations, can, at least in principle, compute the solution to any desired order
of accuracy. Moreover, even performing just a few iterations may produce a reasonable
approximation to the true solution — in stark contrast to Gaussian Elimination, where
one must continue the process through to the bitter end before any useful information can
be extracted. A partially completed Gaussian Elimination is of scant use! A signiﬁcant
weakness is that iterative methods are not universally applicable, and their design relies
upon the detailed structure of the coeﬃcient matrix.
We shall be attempting to solve the linear system (9.41) by replacing it with an iterative
system of the form
u(k+1) = T u(k) + c,
u(0) = u0,
(9.42)
in which T is an n × n matrix and c ∈Rn. This represents a slight generalization of our
earlier iterative system (9.1), in that the right-hand side is now an aﬃne function of u(k).
Suppose that the solutions to the aﬃne iterative system converge: u(k) →u⋆as k →∞.
Then, by taking the limit of both sides of (9.42), we discover that the limit point u⋆solves
the ﬁxed-point equation
u⋆= T u⋆+ c.
(9.43)
Thus, we need to design our iterative system so that
(a) the solution to the ﬁxed-point system u = T u + c coincides with the solution to the
original system Au = b, and
(b) the iterates deﬁned by (9.42) are known to converge to the ﬁxed point. The more
rapid the convergence, the better.
Before exploring these issues in depth, let us look at a simple example.
Example 9.33.
Consider the linear system
3x + y −z = 3,
x −4y + 2z = −1,
−2x −y + 5z = 2,
(9.44)
which has the vectorial form Au = b, with
A =
⎛
⎝
3
1
−1
1
−4
2
−2
−1
5
⎞
⎠,
u =
⎛
⎝
x
y
z
⎞
⎠,
b =
⎛
⎝
3
−1
2
⎞
⎠.
One easy way to convert a linear system into a ﬁxed-point form is to rewrite it as
u = I u −Au + Au = ( I −A)u + b = T u + c,
where
T = I −A,
c = b.

9.4 Iterative Solution of Linear Algebraic Systems
507
k
u(k+1) = T u(k) + b
u(k+1) = T u(k) + c
0
0
0
0
0
0
0
1
3
−1
2
1
.25
.4
2
0
−13
−1
1.05
.7
.85
3
15
−64
−7
1.05
.9375
.96
4
30
−322
−4
1.0075
.9925
1.0075
5
261
−1633
−244
1.005
1.00562
1.0015
6
870
−7939
−133
.9986
1.002
1.0031
7
6069
−40300
−5665
1.0004
1.0012
.9999
8
22500
−196240
−5500
.9995
1.0000
1.0004
9
145743
−992701
−129238
1.0001
1.0001
.9998
10
571980
−4850773
−184261
.9999
.9999
1.0001
11
3522555
−24457324
−2969767
1.0000
1.0000
1.0000
In the present case,
T = I −A =
⎛
⎝
−2
−1
1
−1
5
−2
2
1
−4
⎞
⎠,
c = b =
⎛
⎝
3
−1
2
⎞
⎠.
The resulting iterative system u(k+1) = T u(k) + c has the explicit form
x(k+1) = −2x(k) −y(k) + z(k) + 3,
y(k+1) = −x(k) + 5y(k) −2z(k) −1,
z(k+1) =
2x(k) + y(k) −4z(k) + 2.
(9.45)
Another possibility is to solve the ﬁrst equation in (9.44) for x, the second for y, and
the third for z, so that
x = −1
3 y + 1
3 z + 1,
y = 1
4 x + 1
2 z + 1
4 ,
z = 2
5 x + 1
5 y + 2
5 .
The resulting equations have the form of a ﬁxed-point system
u = T u + c,
in which
T =
⎛
⎜
⎝
0
−1
3
1
3
1
4
0
1
2
2
5
1
5
0
⎞
⎟
⎠,
c =
⎛
⎜
⎝
1
1
4
2
5
⎞
⎟
⎠.
The corresponding iterative system u(k+1) = T u(k) + c is
x(k+1) = −1
3 y(k) + 1
3 z(k) + 1,
y(k+1) =
1
4 x(k) + 1
2 z(k) + 1
4,
z(k+1) =
2
5 x(k) + 1
5 y(k) + 2
5.
(9.46)
Do the resulting iterative methods converge to the solution x = y = z = 1, i.e., to
u⋆= ( 1, 1, 1 )T ? The results, starting with initial guess u(0) = ( 0, 0, 0 )T , are tabulated in
the accompanying table.

508
9 Iteration
For the ﬁrst method, the answer is clearly no — the iterates become wilder and wilder.
Indeed, this occurs no matter how close the initial guess u(0) is to the actual solution —
unless u(0) happens to be exactly equal to u⋆. In the second case, the iterates do converge
to the solution, and it does not take too long, even starting from a poor initial guess, to
obtain a reasonably accurate approximation. Of course, in such a simple example, it would
be silly to use iteration, when Gaussian Elimination can be done by hand and produces the
solution almost immediately. However, we use the small examples for illustrative purposes,
in order to prepare us to bring the full power of iterative algorithms to bear on the large
linear systems arising in applications.
The convergence of solutions to (9.42) to the ﬁxed point u⋆is based on the behavior of
the error vectors
e(k) = u(k) −u⋆,
(9.47)
which measure how close the iterates are to the true solution. Let us ﬁnd out how the
successive error vectors are related. We compute
e(k+1) = u(k+1) −u⋆= (T u(k) + a) −(T u⋆+ a) = T(u(k) −u⋆) = T e(k),
showing that the error vectors satisfy a linear iterative system
e(k+1) = T e(k),
(9.48)
with the same coeﬃcient matrix T. Therefore, they are given by the explicit formula
e(k) = T k e(0).
Now, the solutions to (9.42) converge to the ﬁxed point, u(k) →u⋆, if and only if the error
vectors converge to zero: e(k) →0 as k →∞. Our analysis of linear iterative systems, as
summarized in Theorem 9.11, establishes the following basic convergence result.
Proposition 9.34. The solutions to the aﬃne iterative system (9.42) will all converge to
the solution to the ﬁxed point equation (9.43) if and only if T is a convergent matrix, or,
equivalently, its spectral radius satisﬁes ρ(T) < 1.
The spectral radius ρ(T) of the coeﬃcient matrix will govern the speed of convergence.
Therefore, our goal is to construct an iterative system whose coeﬃcient matrix has as small
a spectral radius as possible. At the very least, the spectral radius must be less than 1. For
the two iterative systems presented in Example 9.33, the spectral radii of the coeﬃcient
matrices are found to be
ρ(T) ≃4.9675,
ρ( T ) = .5.
Therefore, T is not a convergent matrix, which explains the wild behavior of its iterates,
whereas T is convergent, and one expects the error to decrease by a factor of roughly 1
2 at
each step, which is what is observed in practice.
The Jacobi Method
The ﬁrst general iterative method for solving linear systems is based on the same simple
idea used in our illustrative Example 9.33. Namely, we solve the ith equation in the system
Au = b, which is
n

j =1
aij uj = bi,

9.4 Iterative Solution of Linear Algebraic Systems
509
for the ith variable ui. To do this, we need to assume that all the diagonal entries of A are
nonzero: aii ̸= 0. The result is
ui = −1
aii
n

j=1
j̸=i
aij uj + bi
aii
=
n

j =1
tij uj + ci,
(9.49)
where
tij =
⎧
⎨
⎩
−aij
aii
,
i ̸= j,
0,
i = j,
and
ci = bi
aii
.
(9.50)
The result has the form of a ﬁxed-point system u = T u + c, and forms the basis of the
Jacobi Method
u(k+1) = T u(k) + c,
u(0) = u0,
(9.51)
named after the inﬂuential nineteenth-century German analyst Carl Jacobi. The explicit
form of the Jacobi iterative algorithm is
u(k+1)
i
= −1
aii
n

j=1
j̸=i
aij u(k)
j
+ bi
aii
.
(9.52)
It is instructive to rederive the Jacobi Method in matrix form. We begin by decomposing
the coeﬃcient matrix
A = L + D + U
(9.53)
into the sum of a strictly lower triangular matrix L, meaning all its diagonal entries are 0,
a diagonal matrix D, and a strictly upper triangular matrix U, each of which is uniquely
speciﬁed; see Exercise 1.3.11. For example, when
A =
⎛
⎝
3
1
−1
1
−4
2
−2
−1
5
⎞
⎠,
(9.54)
the decomposition (9.53) yields
L =
⎛
⎝
0
0
0
1
0
0
−2
−1
0
⎞
⎠,
D =
⎛
⎝
3
0
0
0
−4
0
0
0
5
⎞
⎠,
U =
⎛
⎝
0
1
−1
0
0
2
0
0
0
⎞
⎠.
Warning. The L, D, U in the elementary additive decomposition (9.53) have nothing to
do with the L, D, U appearing in factorizations arising from Gaussian Elimination. The
latter play no role in the iterative solution methods considered in this section.
We then rewrite the system
Au = (L + D + U) u = b
in the alternative form
D u = −(L + U) u + b.
The Jacobi ﬁxed point system (9.49) amounts to solving the latter for
u = T u + c,
where
T = −D−1(L + U),
c = D−1b.
(9.55)
For the example (9.54), we recover the Jacobi iteration matrix
T = −D−1(L + U) =
⎛
⎜
⎝
0
−1
3
1
3
1
4
0
1
2
2
5
1
5
0
⎞
⎟
⎠.

510
9 Iteration
Deciding in advance whether the Jacobi Method will converge is not easy. However, it
can be shown that Jacobi iteration is guaranteed to converge when the original coeﬃcient
matrix has large diagonal entries, in accordance with Deﬁnition 8.18.
Theorem 9.35.
If the coeﬃcient matrix A is strictly diagonally dominant, then the
associated Jacobi iteration converges.
Proof : We shall prove that ∥T ∥∞< 1, and so Proposition 9.22 implies that T is a conver-
gent matrix. The absolute row sums of the Jacobi matrix T = −D−1(L+U) are, according
to (9.50),
si =
n

j =1
| tij | =
1
| aii |
n

j=1
j̸=i
| aij | < 1,
(9.56)
because A is strictly diagonally dominant, and hence satisﬁes (8.28). This implies that
∥T ∥∞= max{s1, . . ., sn} < 1, and the result follows.
Q.E.D.
Example 9.36.
Consider the linear system
4x + y + w = 1,
x + 4y + z + v = 2,
y + 4z + w = −1,
x + z + 4w + v = 2,
y + w + 4v = 1.
The Jacobi Method solves the respective equations for x, y, z, w, v, leading to the iterative
equations
x(k+1) = −1
4 y(k) −1
4 w(k) + 1
4 ,
y(k+1) = −1
4 x(k) −1
4 z(k) −1
4 v(k) + 1
2 ,
z(k+1) = −1
4 y(k) −1
4 w(k) −1
4 ,
w(k+1) = −1
4 x(k) −1
4 z(k) −1
4 v(k) + 1
2 ,
v(k+1) = −1
4 y(k) −1
4 w(k) + 1
4 .
The coeﬃcient matrix of the original system,
A =
⎛
⎜
⎜
⎜
⎝
4
1
0
1
0
1
4
1
0
1
0
1
4
1
0
1
0
1
4
1
0
1
0
1
4
⎞
⎟
⎟
⎟
⎠,
is strictly diagonally dominant, and so we are guaranteed that the Jacobi iterations will
eventually converge to the solution. Indeed, the Jacobi scheme takes the iterative form
(9.55), with
T =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎝
0
−1
4
0
−1
4
0
−1
4
0
−1
4
0
−1
4
0
−1
4
0
−1
4
0
−1
4
0
−1
4
0
−1
4
0
−1
4
0
−1
4
0
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎠
,
c =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎝
1
4
1
2
−1
4
1
2
1
4
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎠
.

9.4 Iterative Solution of Linear Algebraic Systems
511
Note that ∥T ∥∞= 3
4 < 1, validating convergence. Thus, to obtain, say, four decimal place
accuracy in the solution, we estimate that it will take fewer than log(.5×10−4)/ log .75 ≃34
iterates, assuming a moderate initial error. But the matrix norm always underestimates
the true rate of convergence, as prescribed by the spectral radius ρ(T) = .6124, which
would imply about log(.5 × 10−4)/ log .6124 ≃20 iterations to obtain the desired accuracy.
Indeed, starting with the initial guess x(0) = y(0) = z(0) = w(0) = v(0) = 0, the Jacobi
iterates converge to the exact solution
x = −.1,
y = .7,
z = −.6,
w = .7,
v = −.1,
to within four decimal places in exactly 20 iterations.
Exercises
9.4.1.(a) Find the spectral radius of the matrix T =

1
1
−1
−7
6
	
. (b) Predict the long term
behavior of the iterative system u(k+1) = T u(k) + b, where b =

−1
2
	
, in as much detail
as you can.
9.4.2. Answer Exercise 9.4.1 when
(a) T =
⎛
⎝
1
−1
2
−1
3
2
⎞
⎠, b =

0
1
	
;
(b) T =
⎛
⎜
⎜
⎜
⎝
1
4
1
4
0
0
0
1
4
1
1
1
4
⎞
⎟
⎟
⎟
⎠, b =
⎛
⎜
⎝
1
−1
3
⎞
⎟
⎠;
(c) T =
⎛
⎜
⎝
−.05
.15
.15
.35
.15
−.35
−.2
−.2
.3
⎞
⎟
⎠, b =
⎛
⎜
⎝
−1.5
1.6
1.7
⎞
⎟
⎠.
9.4.3. Which of the following systems have a strictly diagonally dominant coeﬃcient matrix?
(a)
5x −y = 1,
−x + 3y = −1;
(b)
1
2 x + 1
3 y = 1,
1
5 x + 1
4 y = 6;
(c)
−5x + y = 3,
−3x + 2y = −2;
(d)
−2x + y + z = 1,
−x + 2y −z = −2,
x −y + 3z = 1;
(e)
−x + 1
2 y + 1
3 z = 1,
1
3 x + 2y + 3
4 z = −3,
2
3 x + 1
4 y −3
2 z = 2;
(f )
x −2y + z = 1,
−x + 2y + z = −1,
x + 3y −2z = 3;
(g)
−4x + 2y + z = 2,
−x + 3y + z = −1,
x + 4y −6z = 3.
♠9.4.4. For the strictly diagonally dominant systems in Exercise 9.4.3, starting with the initial
guess x = y = z = 0, compute the solution to 2 decimal places using the Jacobi Method.
Check your answer by solving the system directly by Gaussian Elimination.
♠9.4.5.(a) Do any of the non-strictly diagonally dominant systems in Exercise 9.4.3 lead to
convergent Jacobi algorithms? Hint: Check the spectral radius of the Jacobi matrix.
(b) For the convergent systems in Exercise 9.4.3, starting with the initial guess x = y = z =
0, compute the solution to 2 decimal places by using the Jacobi Method, and check your
answer by solving the system directly by Gaussian Elimination.
9.4.6. The following linear systems have positive deﬁnite coeﬃcient matrices. Use the Jacobi
Method starting with u(0) = 0 to ﬁnd the solution to 4 decimal place accuracy.
(a)

3
−1
−1
5
	
u =

2
1
	
,
(b)

2
1
1
1
	
u =

−3
1
	
,
(c)
⎛
⎜
⎝
6
−1
−3
−1
7
4
−3
4
9
⎞
⎟
⎠u =
⎛
⎜
⎝
−1
−2
7
⎞
⎟
⎠,
(d)
⎛
⎜
⎝
3
−1
0
−1
2
1
0
1
5
⎞
⎟
⎠u =
⎛
⎜
⎝
1
−5
0
⎞
⎟
⎠, (e)
⎛
⎜
⎜
⎜
⎝
5
1
1
1
1
5
1
1
1
1
5
1
1
1
1
5
⎞
⎟
⎟
⎟
⎠u =
⎛
⎜
⎜
⎜
⎝
4
0
0
0
⎞
⎟
⎟
⎟
⎠, (f )
⎛
⎜
⎜
⎜
⎝
3
1
0
−1
1
3
1
0
0
1
3
1
−1
0
1
3
⎞
⎟
⎟
⎟
⎠u =
⎛
⎜
⎜
⎜
⎝
1
2
0
−1
⎞
⎟
⎟
⎟
⎠.

512
9 Iteration
♣9.4.7. Let A be the n × n tridiagonal matrix with all its diagonal entries equal to c and all
1’s on the sub- and super-diagonals. (a) For which values of c is A strictly diagonally
dominant? (b) For which values of c does the Jacobi iteration for Au = b converge to the
solution? What is the rate of convergence? Hint: Use Exercise 8.2.48. (c) Set c = 2 and
use the Jacobi Method to solve the linear systems K u = e1, for n = 5, 10, and 20. Starting
with an initial guess of 0, how many Jacobi iterations does it take to obtain 3 decimal place
accuracy? Does the convergence rate agree with what you computed in part (c)?
9.4.8. Prove that 0 ̸= u ∈ker A if and only if u is an eigenvector of the Jacobi iteration matrix
with eigenvalue 1. What does this imply about convergence?
♦9.4.9. Prove that if A is a nonsingular coeﬃcient matrix, then one can always arrange that all
its diagonal entries are nonzero by suitably permuting its rows.
9.4.10. Consider the iterative system (9.42) with spectral radius ρ(T) < 1. Explain why it
takes roughly −1/ log10 ρ(T) iterations to produce one further decimal digit of accuracy in
the solution.
9.4.11. True or false: If a system Au = b has a strictly diagonally dominant coeﬃcient matrix
A, then the equivalent system obtained by applying an elementary row operation to A also
has a strictly diagonally dominant coeﬃcient matrix.
The Gauss–Seidel Method
The Gauss–Seidel Method relies on a slightly reﬁned implementation of the Jacobi process.
To understand how it works, it will help to write out the Jacobi iteration algorithm (9.51)
in full detail:
u(k+1)
1
=
t12 u(k)
2
+ t13 u(k)
3
+ · · · + t1,n−1 u(k)
n−1 + t1n u(k)
n
+ c1,
u(k+1)
2
= t21 u(k)
1
+ t23 u(k)
3
+ · · · + t2,n−1 u(k)
n−1 + t2n u(k)
n
+ c2,
u(k+1)
3
= t31 u(k)
1
+ t32 u(k)
2
· · · + t3,n−1 u(k)
n−1 + t3n u(k)
n
+ c3,
...
...
...
...
...
...
u(k+1)
n
= tn1 u(k)
1
+ tn2 u(k)
2
+ tn3 u(k)
3
+ · · · + tn,n−1 u(k)
n−1
+ cn,
(9.57)
where we are explicitly noting the fact that all the diagonal entries of the coeﬃcient matrix
T vanish. Observe that we are using the entries of the current iterate u(k) to compute
all of the updated values of u(k+1). Presumably, if the iterates u(k) are converging to the
solution u⋆, then their individual entries are also converging, and so each u(k+1)
j
should be
a better approximation to u⋆
j than u(k)
j
is. Therefore, if we begin the kth Jacobi iteration
by computing u(k+1)
1
using the ﬁrst equation, then we are tempted to use this new and
improved value to replace u(k)
1
in each of the subsequent equations.
In particular, we
employ the modiﬁed equation
u(k+1)
2
= t21 u(k+1)
1
+ t23 u(k)
3
+ · · · + t1n u(k)
n
+ c2
to update the second component of our iterate. This more accurate value should then be
used to update u(k+1)
3
, and so on.
The upshot of these considerations is the Gauss–Seidel Method
u(k+1)
i
= ti1 u(k+1)
1
+ · · · + ti,i−1 u(k+1)
i−1
+ ti,i+1 u(k)
i+1 + · · · + tin u(k)
n
+ ci,
i = 1, . . . , n,
(9.58)

9.4 Iterative Solution of Linear Algebraic Systems
513
named after Gauss (as usual!) and the German astronomer/mathematician Philipp von
Seidel.
At the kth stage of the iteration, we use (9.58) to compute the revised entries
u(k+1)
1
, u(k+1)
2
, . . . , u(k+1)
n
in their numerical order. Once an entry has been updated, the
new value is immediately used in all subsequent computations.
Example 9.37.
For the linear system
3x + y −z = 3,
x −4y + 2z = −1,
−2x −y + 5z = 2,
the Jacobi iteration method was given in (9.46). To construct the corresponding Gauss–
Seidel algorithm we use updated values of x, y, and z as they become available. Explicitly,
x(k+1) = −1
3 y(k) + 1
3 z(k) + 1,
y(k+1) = 1
4 x(k+1) + 1
2 z(k) + 1
4,
z(k+1) = 2
5 x(k+1) + 1
5 y(k+1) + 2
5.
(9.59)
Starting with u(0) = 0, the resulting iterates are
u(1) =
⎛
⎝
1.0000
.5000
.9000
⎞
⎠,
u(2) =
⎛
⎝
1.1333
.9833
1.0500
⎞
⎠,
u(3) =
⎛
⎝
1.0222
1.0306
1.0150
⎞
⎠,
u(4) =
⎛
⎝
.9948
1.0062
.9992
⎞
⎠,
u(5) =
⎛
⎝
.9977
.9990
.9989
⎞
⎠,
u(6) =
⎛
⎝
1.0000
.9994
.9999
⎞
⎠,
u(7) =
⎛
⎝
1.0001
1.0000
1.0001
⎞
⎠,
u(8) =
⎛
⎝
1.0000
1.0000
1.0000
⎞
⎠,
and have converged to the solution, to 4 decimal place accuracy, after only 8 iterations —
as opposed to the 11 iterations required by the Jacobi Method.
Gauss–Seidel iteration is particularly suited to implementation on a serial computer,
since one can immediately replace each component u(k)
i
by its updated value u(k+1)
i
, thereby
also saving on storage in the computer’s memory. In contrast, the Jacobi Method requires
us to retain all the old values u(k) until the new approximation u(k+1) has been computed.
Moreover, Gauss–Seidel typically (although not always) converges faster than Jacobi, mak-
ing it the iterative algorithm of choice for serial processors. On the other hand, with the
advent of parallel processing machines, variants of the parallelizable Jacobi scheme have
been making a comeback.
What is Gauss–Seidel really up to? Let us rewrite the basic iterative equation (9.58) by
multiplying by aii and moving the terms involving u(k+1) to the left-hand side. In view of
the formula (9.50) for the entries of T, the resulting equation is
ai1 u(k+1)
1
+ · · · + ai,i−1 u(k+1)
i−1
+ aii u(k+1)
i
= −ai,i+1u(k)
i+1 −· · · −ain u(k)
n
+ bi.
In matrix form, taking (9.53) into account, this reads
(L + D)u(k+1) = −U u(k) + b,
(9.60)
and so can be viewed as a linear system of equations for u(k+1) with lower triangular
coeﬃcient matrix L + D. Note that the ﬁxed point of (9.60), namely the solution to
(L + D)u = −U u + b,
coincides with the solution to the original system
Au = (L + D + U)u = b.

514
9 Iteration
In other words, the Gauss–Seidel procedure is merely implementing Forward Substitution
to solve the lower triangular system (9.60) for the next iterate:
u(k+1) = −(L + D)−1U u(k) + (L + D)−1 b.
The latter is in our more usual iterative form
u(k+1) = T u(k) + c,
where
T = −(L + D)−1U,
c = (L + D)−1 b.
(9.61)
Consequently, the convergence of the Gauss–Seidel iterates is governed by the spectral
radius of their coeﬃcient matrix T.
Returning to Example 9.37, we have
A =
⎛
⎝
3
1
−1
1
−4
2
−2
−1
5
⎞
⎠,
L + D =
⎛
⎝
3
0
0
1
−4
0
−2
−1
5
⎞
⎠,
U =
⎛
⎝
0
1
−1
0
0
2
0
0
0
⎞
⎠.
Therefore, the Gauss–Seidel matrix is
T = −(L + D)−1U =
⎛
⎜
⎝
0
−.3333
.3333
0
−.0833
.5833
0
−.1500
.2500
⎞
⎟
⎠.
Its eigenvalues are 0 and .0833 ± .2444 i , and hence its spectral radius is ρ( T ) ≃.2582.
This is roughly the square of the Jacobi spectral radius of .5, which tells us that the
Gauss–Seidel iterations will converge about twice as fast to the solution.
This can be
veriﬁed by more extensive computations. Although examples can be constructed in which
the Jacobi Method converges faster, in many practical situations Gauss–Seidel tends to
converge roughly twice as fast as Jacobi.
Completely general conditions guaranteeing convergence of the Gauss–Seidel Method
are also hard to establish. But, like the Jacobi Method, it is guaranteed to converge when
the original coeﬃcient matrix is strictly diagonally dominant.
Theorem 9.38. If A is strictly diagonally dominant, then the Gauss–Seidel iteration al-
gorithm for solving Au = b converges.
Proof : Let e(k) = u(k) −u⋆denote the kth Gauss–Seidel error vector. As in (9.48), the
error vectors satisfy the linear iterative system e(k+1) = T e(k), but a direct estimate of
∥T ∥∞is not so easy. Instead, let us write out the linear iterative system in components:
e(k+1)
i
= ti1 e(k+1)
1
+ · · · + ti,i−1 e(k+1)
i−1
+ ti,i+1 e(k)
i+1 + · · · + tin e(k)
n .
(9.62)
Let
m(k) = ∥e(k) ∥∞= max

| e(k)
1
|, . . . , | e(k)
n |

(9.63)
denote the ∞norm of the kth error vector. To prove convergence, e(k) →0, it suﬃces to

9.4 Iterative Solution of Linear Algebraic Systems
515
show that m(k) →0 as k →∞. We claim that diagonal dominance of A implies that
m(k+1) ≤s m(k),
where
s = ∥T ∥∞< 1
(9.64)
denotes the ∞matrix norm of the Jacobi matrix T — not the Gauss–Seidel matrix T —
which, by (9.56), is less than 1. We infer that m(k) ≤sk m(0) →0 as k →∞, demonstrating
the theorem.
To prove (9.64), we use induction on i = 1, . . ., n. Our induction hypothesis is
| e(k+1)
j
| ≤s m(k) < m(k)
for
j = 1, . . ., i −1.
(When i = 1, there is no assumption.) Moreover, by (9.63),
| e(k)
j
| ≤m(k)
for all
j = 1, . . . , n.
We use these two inequalities to estimate | e(k+1)
i
| from (9.62):
| e(k+1)
i
| ≤| ti1 | | e(k+1)
1
| + · · · + | ti,i−1 | | e(k+1)
i−1
| + | ti,i+1 | | e(k)
i+1 | + · · · + | tin | | e(k)
n |
≤

| ti1 | + · · · + | tin |

m(k) ≤s m(k),
which completes the induction step. As a result, the maximum
m(k+1) = max

| e(k+1)
1
|, . . . , | e(k+1)
n
|

≤s m(k)
also satisﬁes the same bound, and hence (9.64) follows.
Q.E.D.
Example 9.39.
For the linear system considered in Example 9.36, the Gauss–Seidel
iterations take the form
x(k+1) = −1
4 y(k) −1
4 w(k) + 1
4 ,
y(k+1) = −1
4 x(k+1) −1
4 z(k) −1
4 v(k) + 1
2 ,
z(k+1) = −1
4 y(k+1) −1
4 w(k) −1
4 ,
w(k+1) = −1
4 x(k+1) −1
4 z(k+1) −1
4 v(k) + 1
2 ,
v(k+1) = −1
4 y(k+1) −1
4 w(k+1) + 1
4 .
Starting with x(0) = y(0) = z(0) = w(0) = v(0) = 0, the Gauss–Seidel iterates converge to
the solution x = −.1, y = .7, z = −.6, w = .7, v = −.1, to four decimal places in 11
iterations, again roughly twice as fast as the Jacobi Method. Indeed, the convergence rate
is governed by the corresponding Gauss–Seidel matrix T, which is
⎛
⎜
⎜
⎜
⎝
4
0
0
0
0
1
4
0
0
0
0
1
4
0
0
1
0
1
4
0
0
1
0
1
4
⎞
⎟
⎟
⎟
⎠
−1⎛
⎜
⎜
⎜
⎝
0
1
0
1
0
0
0
1
0
1
0
0
0
1
0
0
0
0
0
1
0
0
0
0
0
⎞
⎟
⎟
⎟
⎠=
⎛
⎜
⎜
⎜
⎝
0
−.2500
0
−.2500
0
0
.0625
−.2500
.0625
−.2500
0
−.0156
.0625
−.2656
.0625
0
.0664
−.0156
.1289
−.2656
0
−.0322
.0664
−.0479
.1289
⎞
⎟
⎟
⎟
⎠.
Its spectral radius is ρ( T ) = .3936, which is, as in the previous example, approximately the
square of the spectral radius of the Jacobi coeﬃcient matrix, which explains the speedup
in convergence.

516
9 Iteration
Exercises
♥9.4.12. Consider the linear system A x = b, where A =
⎛
⎜
⎝
4
1
−2
−1
4
−1
1
−1
4
⎞
⎟
⎠, b =
⎛
⎜
⎝
−2
−1
7
⎞
⎟
⎠.
(a) First, solve the equation directly by Gaussian Elimination.
(b) Write the Jacobi
iteration in the form x(k+1) = T x(k) + c. Find the 3 × 3 matrix T and the vector c
explicitly.
(c) Using the initial approximation x(0) = 0, carry out three iterations of the
Jacobi algorithm to compute x(1), x(2) and x(3). How close are you to the exact solution?
(d) Write the Gauss–Seidel iteration in the form x(k+1) = T x(k) + c. Find the 3 × 3
matrix T and the vector c explicitly.
(e) Using the initial approximation x(0) = 0, carry
out three iterations of the Gauss–Seidel algorithm. Which is a better approximation to
the solution — Jacobi or Gauss–Seidel?
(f ) Determine the spectral radius of the Jacobi
matrix T, and use this to prove that the Jacobi Method will converge to the solution of
A x = b for any choice of the initial approximation x(0).
(g) Determine the spectral
radius of the Gauss–Seidel matrix T. Which method converges faster?
(h) For the faster
method, how many iterations would you expect to need to obtain 5 decimal place accuracy?
(i) Test your prediction by computing the solution to the desired accuracy.
♠9.4.13. For the strictly diagonally dominant systems in Exercise 9.4.3, starting with the initial
guess x = y = z = 0, compute the solution to 3 decimal places using the Gauss–Seidel
Method. Check your answer by solving the system directly by Gaussian Elimination.
9.4.14. Which of the systems in Exercise 9.4.3 lead to convergent Gauss–Seidel algorithms? In
each case, which converges faster, Jacobi or Gauss–Seidel?
9.4.15.(a) Solve the positive deﬁnite linear systems in Exercise 9.4.6 using the Gauss–Seidel
Method to achieve 4 decimal place accuracy.
(b) Compare the convergence rate with that of the Jacobi Method.
♣9.4.16. Let A =
⎛
⎜
⎜
⎜
⎝
c
1
0
0
1
c
1
0
0
1
c
1
0
0
1
c
⎞
⎟
⎟
⎟
⎠. (a) For what values of c is A strictly diagonally dominant?
(b) Use a computer to ﬁnd the smallest positive value of c > 0 for which Jacobi iteration
converges.
(c) Find the smallest positive value of c > 0 for which Gauss–Seidel iteration
converges. Is your answer the same? (d) When they both converge, which converges faster
— Jacobi or Gauss–Seidel? How much faster? Does your answer depend upon the value of
c?
♠9.4.17. Consider the linear system
2.4x −.8y + .8z = 1,
−.6x + 3.6y −.6z = 0,
15x + 14.4y −3.6z = 0.
Show, by direct computation, that Jacobi iteration converges to the solution, but Gauss–
Seidel does not.
♠9.4.18. Discuss convergence of Gauss–Seidel iteration for the system
5x + 7y + 6z + 5w = 23,
7x + 10y + 8z + 7w = 32,
6x + 8y + 10z + 9w = 33,
5x + 7y + 9z + 10w = 31.
9.4.19. Let A =
⎛
⎜
⎝
2
4
−4
3
3
3
2
2
1
⎞
⎟
⎠. Find the spectral radius of the Jacobi and Gauss–Seidel
iteration matrices, and discuss their convergence.
♠9.4.20. Consider the linear system H5u = e1, where H5 is the 5 × 5 Hilbert matrix. Does the
Jacobi Method converge to the solution? If so, how fast? What about Gauss–Seidel?

9.4 Iterative Solution of Linear Algebraic Systems
517
♦9.4.21. How many arithmetic operations are needed to perform k steps of the Jacobi iteration?
What about Gauss–Seidel? Under what conditions is Jacobi or Gauss–Seidel more eﬃcient
than Gaussian Elimination?
♣9.4.22. Consider the linear system A x = e1 based on the 10 × 10 pentadiagonal matrix
A =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
z
−1
1
0
−1
z
−1
1
0
1
−1
z
−1
1
0
0
1
−1
z
−1
1
...
0
1
−1
z
−1
...
0
1
−1
z
...
...
...
...
...
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
.
(a) For what values of z are the Jacobi and Gauss–Seidel Methods guaranteed to converge?
(b) Set z = 4. How many iterations are required to approximate the solution to 3 decimal
places?
(c) How small can | z | be before the methods diverge?
♣9.4.23. The na¨ıve iterative method for solving Au = b is to rewrite it in ﬁxed point form
u = T u + c, where T = I −A and c = b. (a) What conditions on the eigenvalues of A
ensure convergence of the na¨ıve method? (b) Use the Gershgorin Theorem 8.16 to prove
that the na¨ıve method converges to the solution to
⎛
⎜
⎝
.8
−.1
−.1
.2
1.5
−.1
.2
−.1
1.0
⎞
⎟
⎠
⎛
⎜
⎝
x
y
z
⎞
⎟
⎠=
⎛
⎜
⎝
1
−1
2
⎞
⎟
⎠.
(c) Check part (b) by implementing the method.
Successive Over-Relaxation
As we know, the smaller the spectral radius (or matrix norm) of the coeﬃcient matrix,
the faster the convergence of the iterative algorithm. One of the goals of researchers in
numerical linear algebra is to design new methods for accelerating the convergence. In his
1950 thesis, the American mathematician David Young discovered a simple modiﬁcation of
the Jacobi and Gauss–Seidel Methods that can, in favorable situations, lead to a dramatic
speedup in the rate of convergence. The method, known as Successive Over-Relaxation,
and often abbreviated SOR, has become the iterative method of choice in a range of modern
applications, [21, 86]. In this subsection, we provide a brief overview.
In practice, ﬁnding the optimal iterative algorithm to solve a given linear system is as
hard as solving the system itself. Therefore, numerical analysts have relied on a few tried
and true techniques for designing iterative schemes that can be used in the more common
applications. Consider a linear algebraic system Au = b. Every decomposition of the
coeﬃcient matrix into the diﬀerence of two matrices,
A = M −N,
(9.65)
leads to an equivalent system of the form
M u = N u + b.
(9.66)
Provided that M is nonsingular, we can rewrite the preceding system in ﬁxed point form:
u = M −1N u + M −1b = T u + c,
where
T = M −1N,
c = M −1b.
Now, we are free to choose any such M, which then speciﬁes N = A−M uniquely. However,
for the resulting iterative method u(k+1) = T u(k) + c to be practical we must arrange that
(a) T = M −1N is a convergent matrix, and
(b) M can be easily inverted.

518
9 Iteration
The second requirement ensures that the iterative equations
M u(k+1) = N u(k) + b
(9.67)
can be solved for u(k+1) with minimal computational eﬀort. Typically, this requires that
M be either a diagonal matrix, in which case the inversion is immediate, or lower or upper
triangular, in which case one employs Forward or Back Substitution to solve for u(k+1).
With this in mind, we now introduce the SOR Method. It relies on a slight generalization
of the Gauss–Seidel decomposition (9.60) of the matrix into lower triangular and strictly
upper triangular parts. The starting point is to write
A = L + D + U =

L + α D

−

(α −1) D −U

,
(9.68)
where 0 ̸= α is an adjustable scalar parameter. We decompose the system Au = b as
(L + α D)u =

(α −1) D −U

u + b.
(9.69)
It turns out to be slightly more convenient to divide (9.69) through by α and write the
resulting iterative system in the form
(ωL + D)u(k+1) =

(1 −ω) D −ω U

u(k) + ω b,
(9.70)
where ω = 1/α is called the relaxation parameter. Assuming, as usual, that all diagonal
entries of A are nonzero, the matrix ωL + D is an invertible lower triangular matrix, and
so we can use Forward Substitution to solve the iterative system (9.70) to recover u(k+1).
The explicit formula for its ith entry is
u(k+1)
i
= ω ti1 u(k+1)
1
+ · · · + ω ti,i−1 u(k+1)
i−1
+ (1 −ω) u(k)
i
+ ω ti,i+1 u(k)
i+1 + · · · + ω tin u(k)
n
+ ω ci,
(9.71)
where tij and ci denote the original Jacobi values (9.50). As in the Gauss–Seidel approach,
we update the entries u(k+1)
i
in numerical order i = 1, . . ., n. Thus, to obtain the SOR
scheme (9.71), we merely multiply the right-hand side of the Gauss–Seidel system (9.58)
by the adjustable relaxation parameter ω and append the diagonal term (1 −ω) u(k)
i
. In
particular, if we set ω = 1, then the SOR Method reduces to the Gauss–Seidel Method.
Choosing ω < 1 leads to an under-relaxed method, while ω > 1, known as over-relaxation,
is the preferred choice in most practical instances.
To analyze the SOR algorithm in detail, we rewrite (9.70) in the ﬁxed point form
u(k+1) = Tω u(k) + cω,
(9.72)
where
Tω = (ωL + D)−1
(1 −ω) D −ω U

,
cω = (ωL + D)−1 ω b.
(9.73)
The rate of convergence is governed by the spectral radius of the matrix Tω. The goal
is to choose the relaxation parameter ω so as to make the spectral radius of Tω as small
as possible. As we will see, a clever choice of ω can result in a dramatic speedup in the
convergence rate. Let us look at an elementary example.
Example 9.40.
Consider the matrix A =

2
−1
−1
2

, which we decompose as
A = L + D + U, where
L =

0
0
−1
0

,
D =

2
0
0
2

,
U =

0
−1
0
0

.

9.4 Iterative Solution of Linear Algebraic Systems
519
Jacobi iteration is based on the coeﬃcient matrix
T = −D−1(L + U) =

0
1
2
1
2
0

.
Its spectral radius is ρ(T) = .5, and hence the Jacobi Method takes, on average, roughly
−1/ log10 .5 ≃3.3 iterations to produce each new decimal place in the solution.
The SOR Method (9.70) takes the explicit form

2
0
−ω
2

u(k+1) =

2(1 −ω)
ω
0
2(1 −ω)

u(k) + ω b,
where Gauss–Seidel is the particular case ω = 1. The SOR coeﬃcient matrix is
Tω =

2
0
−ω
2
−1 
2(1 −ω)
ω
0
2(1 −ω)

=

1 −ω
1
2 ω
1
2 ω(1 −ω)
1
4 (2 −ω)2

.
To compute the eigenvalues of Tω, we form its characteristic equation:
0 = det(Tω −λ I ) = λ2 −

2 −2ω + 1
4 ω2 
λ + (1 −ω)2 = (λ + ω −1)2 −1
4 λ ω2.
(9.74)
Our goal is to choose ω such that
(a) both eigenvalues are less than 1 in modulus, so | λ1 |, | λ2 | < 1. This is the minimal
requirement for convergence of the method.
(b) the largest eigenvalue (in modulus) is as small as possible.
This will give the
smallest spectral radius for Tω and hence the fastest convergence rate.
By (8.26), the product of the two eigenvalues is the determinant,
λ1 λ2 = det Tω = (1 −ω)2.
If ω ≤0 or ω ≥2, then det Tω ≥1, and hence at least one of the eigenvalues would have
modulus larger than 1. Thus, in order to ensure convergence, we must require 0 < ω < 2.
For Gauss–Seidel, at ω = 1, the eigenvalues are λ1 = 1
4, λ2 = 0, and the spectral radius is
ρ(T1) = .25. This is exactly the square of the Jacobi spectral radius, and hence the Gauss–
Seidel iterates converge twice as fast; so it takes, on average, only about −1/ log10 .25 ≃1.66
Gauss–Seidel iterations to produce each new decimal place of accuracy. It can be shown
(Exercise 9.4.32) that as ω increases above 1, the two eigenvalues move along the real axis
towards each other. They coincide when
ω = ω⋆= 8 −4
√
3 ≃1.07,
at which point
λ1 = λ2 = ω⋆−1 = .07 = ρ(Tω),
which is the convergence rate of the optimal SOR Method. Each iteration produces slightly
more than one new decimal place in the solution, which represents a signiﬁcant improve-
ment over the Gauss–Seidel convergence rate. It takes about twice as many Gauss–Seidel
iterations (and four times as many Jacobi iterations) to produce the same accuracy as this
optimal SOR Method.
Of course, in such a simple 2 × 2 example, it is not so surprising that we can construct
the best value for the relaxation parameter by hand. Young was able to ﬁnd the optimal
value of the relaxation parameter for a broad class of matrices that includes most of those
arising in the ﬁnite diﬀerence and ﬁnite element numerical solutions to ordinary and partial
diﬀerential equations, [61].
For the matrices in Young’s class, the Jacobi eigenvalues

520
9 Iteration
occur in signed pairs. If ±μ are a pair of eigenvalues for the Jacobi Method, then the
corresponding eigenvalues of the SOR iteration matrix satisfy the quadratic equation
(λ + ω −1)2 = λ ω2 μ2.
(9.75)
If ω = 1, so we have standard Gauss–Seidel, then λ2 = λ μ2, and so the eigenvalues are
λ = 0, λ = μ2. The Gauss–Seidel spectral radius is therefore the square of the Jacobi
spectral radius, and so (at least for matrices in the Young class) its iterates converge twice
as fast. The quadratic equation (9.75) has the same properties as in the 2×2 version (9.74)
(which corresponds to the case μ = 1
2 ), and hence the optimal value of ω will be the one
at which the two roots are equal:
λ1 = λ2 = ω −1,
which occurs when
ω = 2 −2

1 −μ2
μ2
=
2
1 +

1 −μ2 .
Therefore, if ρJ = max | μ | denotes the spectral radius of the Jacobi Method, then the
Gauss–Seidel has spectral radius ρGS = ρ2
J, while the SOR Method with optimal relaxation
parameter
ω⋆=
2
1 +

1 −ρ2
J
,
has spectral radius
ρ⋆= ω⋆−1.
(9.76)
For example, if ρJ = .99, which is rather slow convergence (but common for iterative
numerical solution schemes for partial diﬀerential equations), then ρGS = .9801, which is
twice as fast, but still quite slow, while SOR with ω⋆= 1.7527 has ρ⋆= .7527, which is
dramatically faster†. Indeed, since ρ⋆≃(ρGS)14 ≃(ρJ)28, it takes about 14 Gauss–Seidel
(and 28 Jacobi) iterations to produce the same accuracy as one SOR step. It is amazing
that such a simple idea can have such a dramatic eﬀect.
Exercises
♥9.4.24. Consider the linear system Au = b, where A =

2
1
1
3
	
, b =

3
2
	
.
(a) What is the solution? (b) Discuss the convergence of the Jacobi iteration method.
(c) Discuss the convergence of the Gauss–Seidel iteration method. (d) Write down the
explicit formulas for the SOR Method. (e) What is the optimal value of the relaxation
parameter ω for this system? How much faster is the convergence as compared to the
Jacobi and Gauss–Seidel Methods? (f ) Suppose your initial guess is u(0) = 0. Give an
estimate as to how many steps each iterative method (Jacobi, Gauss–Seidel, SOR) would
require in order to approximate the solution to the system to within 5 decimal places.
(g) Verify your answer by direct computation.
♠9.4.25. In Exercise 9.4.18 you were asked to solve a system by Gauss–Seidel. How much
faster can you design an SOR scheme to converge? Experiment with several values of the
relaxation parameter ω, and discuss what you ﬁnd.
♠9.4.26. Investigate the three basic iterative techniques — Jacobi, Gauss–Seidel, SOR — for
solving the linear system K⋆u⋆= f⋆for the cubical circuit in Example 6.4.
†
More precisely, since the SOR matrix is not necessarily diagonalizable, the overall convergence
rate is slightly slower than the spectral radius. However, this technical detail does not aﬀect the
overall conclusion.

9.4 Iterative Solution of Linear Algebraic Systems
521
♣9.4.27. Consider the linear system
4x −y −z = 1, −x + 4y −w = 2, −x + 4z −w = 0, −y −z + 4w = 1.
(a) Find the solution by using Gaussian Elimination and Back Substitution.
(b) Using
0 as your initial guess, how many iterations are required to approximate the solution to
within ﬁve decimal places using (i) Jacobi iteration?
(ii) Gauss–Seidel iteration? Can
you estimate the spectral radii of the relevant matrices in each case?
(c) Try to ﬁnd the
solution by using the SOR Method with the parameter ω taking various values between .5
and 1.5. Which value of ω gives the fastest convergence? What is the spectral radius of the
SOR matrix?
♠9.4.28.(a) Find the spectral radius of the Jacobi and Gauss–Seidel iteration matrices when
A =
⎛
⎜
⎜
⎜
⎝
2
1
0
0
1
2
1
0
0
1
2
1
0
0
1
2
⎞
⎟
⎟
⎟
⎠. (b) Is A strictly diagonally dominant? (c) Use (9.76) to ﬁx the
optimal value of the SOR parameter. Verify that the spectral radius of the resulting
iteration matrix agrees with the second formula in (9.76).
(d) For each iterative method,
predict how many iterations are needed to solve the linear system A x = e1 to 4 decimal
places, and then verify your predictions by direct computation.
♠9.4.29. Change the matrix in Exercise 9.4.28 to A =
⎛
⎜
⎜
⎜
⎝
2
−1
0
0
1
2
−1
0
0
1
2
−1
0
0
1
2
⎞
⎟
⎟
⎟
⎠, and answer the
same questions. Does the SOR Method with parameter given by (9.76) speed the iterations
up? Why not? Can you ﬁnd a value of the SOR parameter that does?
♠9.4.30. Consider the linear system Au = e1 in which A is the 8 × 8 tridiagonal matrix with
all 2’s on the main diagonal and all −1’s on the sub- and super-diagonals. (a) Use Exercise
8.2.47 to ﬁnd the spectral radius of the Jacobi iteration method to solve Au = b. Does the
Jacobi Method converge? (b) What is the optimal value of the SOR parameter based on
(9.76)? How many Jacobi iterations are needed to match the eﬀect of a single SOR step?
(c) Test out your conclusions by using both Jacobi and SOR to approximate the solution
to 3 decimal places.
♣9.4.31. How much can you speed up the convergence of the iterative solution to the
pentadiagonal linear system in Exercise 9.4.22 when z = 4 using SOR? Discuss.
♦9.4.32. For the matrix treated in Example 9.40, prove that (a) as ω increases from 1 to
8 −4
√
3, the two eigenvalues move towards each other, with the larger one decreasing in
magnitude; (b) if ω > 8−4
√
3, the eigenvalues are complex conjugates, with larger modulus
than the optimal value.
(c) Can you conclude that ω⋆= 8 −4
√
3 is the optimal value for
the SOR parameter?
♣9.4.33. The matrix A =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
4
−1
0
−1
0
0
0
0
0
−1
4
−1
0
−1
0
0
0
0
0
−1
4
0
0
−1
0
0
0
−1
0
0
4
−1
0
−1
0
0
0
−1
0
−1
4
−1
0
−1
0
0
0
−1
0
−1
4
0
0
−1
0
0
0
−1
0
0
4
−1
0
0
0
0
0
−1
0
−1
4
−1
0
0
0
0
0
−1
0
−1
4
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
arises in the ﬁnite
diﬀerence (and ﬁnite element) discretization of the Poisson equation on a nine point square
grid. Solve the linear system Au = e5 using (a) Gaussian Elimination; (b) Jacobi
iteration; (c) Gauss–Seidel iteration; (d) SOR based on the Jacobi spectral radius.

522
9 Iteration
♣9.4.34. The generalization of Exercise 9.4.33 to an n × n grid results in an n2 × n2 matrix in
block tridiagonal form A =
⎛
⎜
⎜
⎜
⎜
⎜
⎝
K
−I
−I
K
−I
−I
K
−I
...
...
...
⎞
⎟
⎟
⎟
⎟
⎟
⎠
, in which K is the tridiagonal
n × n matrix with 4’s on the main diagonal and −1’s on the sub- and super-diagonals,
while I denotes the n × n identity matrix. Use the known value of the Jacobi spectral
radius ρJ = cos
π
n + 1 , [86], to design an SOR Method to solve the linear system Au = f.
Run your method on the cases n = 5 and f = e13 and n = 25 and f = e313 corresponding to
a unit force at the center of the grid. How much faster is the convergence rate of SOR than
Jacobi and Gauss–Seidel?
♥9.4.35. If u(k) is an approximation to the solution to Au = b, then the residual vector
r(k) = b −Au(k) measures how accurately the approximation solves the system.
(a) Show that the Jacobi iteration can be written in the form u(k+1) = u(k) + D−1r(k).
(b) Show that the Gauss–Seidel iteration has the form u(k+1) = u(k) + (L + D)−1r(k).
(c) Show that the SOR iteration has the form u(k+1) = u(k) + (ω L + D)−1r(k).
(d) If ∥r(k) ∥is small, does this mean that u(k) is close to the solution? Explain your answer
and illustrate with a couple of examples.
9.4.36. Let K be a positive deﬁnite n × n matrix with eigenvalues λ1 ≥λ2 ≥· · · ≥λn > 0. For
what values of ε does the iterative system u(k+1) = u(k) + ε r(k), where r(k) = f −Ku(k) is
the current residual vector, converge to the solution to the linear system Ku = f? What is
the optimal value of ε, and what is the convergence rate?
9.5 Numerical Computation of Eigenvalues
The importance of the eigenvalues of a square matrix in a broad range of applications is
amply demonstrated in this chapter and its successor. However, ﬁnding the eigenvalues
and associated eigenvectors is not such an easy task. The direct method of constructing the
characteristic equation of the matrix through the determinantal formula, then solving the
resulting polynomial equation for the eigenvalues, and ﬁnally producing the eigenvectors
by solving the associated homogeneous linear system, is hopelessly ineﬃcient, and fraught
with numerical pitfalls. We are in need of a completely new idea if we have any hopes of
designing eﬃcient numerical approximation schemes.
In this section, we develop a few of the most basic numerical algorithms for computing
eigenvalues and eigenvectors. All are iterative in nature. The most direct are based on the
connections between the eigenvalues and the high powers of a matrix. A more sophisticated
approach, based on the QR factorization that we learned in Section 4.3, will be presented
at the end of the section. Additional computational methods for eigenvalues will appear
in the following Section 9.6.
The Power Method
We have already noted the role played by the eigenvalues and eigenvectors in the solution to
linear iterative systems. Now we are going to turn the tables, and use the iterative system
as a mechanism for approximating the eigenvalues, or, more correctly, selected eigenvalues
of the coeﬃcient matrix. The simplest of the resulting computational procedures is known
as the Power Method.

9.5 Numerical Computation of Eigenvalues
523
We assume, for simplicity, that A is a complete† n×n matrix. Let v1, . . . , vn denote its
eigenvector basis, and λ1, . . . , λn the corresponding eigenvalues. As we have learned, the
solution to the linear iterative system
v(k+1) = Av(k),
v(0) = v,
(9.77)
is obtained by multiplying the initial vector v by the successive powers of the coeﬃcient
matrix: v(k) = Ak v. If we write the initial vector in terms of the eigenvector basis
v = c1 v1 + · · · + cn vn,
(9.78)
then the solution takes the explicit form given in Theorem 9.4, namely
v(k) = Ak v = c1 λk
1 v1 + · · · + cn λk
n vn.
(9.79)
Suppose further that A has a single dominant real eigenvalue, λ1, that is larger than all
others in magnitude, so
| λ1 | > | λj |
for all
j > 1.
(9.80)
As its name implies, this eigenvalue will eventually dominate the iteration (9.79). Indeed,
since
| λ1 |k ≫| λj |k
for all
j > 1
and all
k ≫0,
the ﬁrst term in the iterative formula (9.79) will eventually be much larger than the rest,
and so, provided c1 ̸= 0,
v(k) ≃c1 λk
1 v1
for
k ≫0.
Therefore, the solution to the iterative system (9.77) will, almost always, end up being a
multiple of the dominant eigenvector of the coeﬃcient matrix.
To compute the corresponding eigenvalue, we note that the ith entry of the iterate v(k)
is approximated by v(k)
i
≃c1λk
1 v1,i, where v1,i is the ith entry of the eigenvector v1. Thus,
as long as v1,i ̸= 0, we can recover the dominant eigenvalue by taking a ratio between
selected components of successive iterates:
λ1 ≃
v(k)
i
v(k−1)
i
,
provided that
v(k−1)
i
̸= 0.
(9.81)
Example 9.41.
Consider the matrix A =
⎛
⎝
−1
2
2
−1
−4
−2
−3
9
7
⎞
⎠.
As you can check, its
eigenvalues and eigenvectors are
λ1 = 3,
v1 =
⎛
⎝
1
−1
3
⎞
⎠,
λ2 = −2,
v2 =
⎛
⎝
0
1
−1
⎞
⎠,
λ3 = 1,
v3 =
⎛
⎝
−1
1
−2
⎞
⎠.
Repeatedly multiplying the initial vector v = ( 1, 0, 0 )T
by A results in the iterates
v(k) = Akv listed in the accompanying table.
The last column indicates the ratio
λ(k) = v(k)
1 /v(k−1)
1
between the ﬁrst components of successive iterates. (One could equally
†
This is not a very severe restriction. Most matrices are complete. Moreover, perturbations
caused by round-oﬀand/or numerical inaccuracies will almost invariably make an incomplete
matrix complete.

524
9 Iteration
k
v(k)
λ(k)
0
1
0
0
1
−1
−1
−3
−1.
2
−7
11
−27
7.
3
−25
17
−69
3.5714
4
−79
95
−255
3.1600
5
−241
209
−693
3.0506
6
−727
791
−2247
3.0166
7
−2185
2057
−6429
3.0055
8
−6559
6815
−19935
3.0018
9
−19681
19169
−58533
3.0006
10
−59047
60071
−178167
3.0002
11
−177145
175097
−529389
3.0001
12
−531439
535535
−1598415
3.0000
well use the second or third components.)
The ratios are converging to the dominant
eigenvalue λ1 = 3, while the vectors v(k) are converging to a very large multiple of the
corresponding eigenvector v1 = ( 1, −1, 3 )T.
The success of the Power Method lies in the assumption that A has a unique dominant
eigenvalue of maximal modulus, which, by deﬁnition, equals its spectral radius: | λ1 | =
ρ(A). The rate of convergence of the method is governed by the ratio | λ2/λ1 | between
the subdominant and dominant eigenvalues. Thus, the farther the dominant eigenvalue
lies away from the rest, the faster the Power Method converges. We also assumed that the
initial vector v(0) includes a nonzero multiple of the dominant eigenvector, i.e., c1 ̸= 0. As
we do not know the eigenvectors, it is not so easy to guarantee this in advance, although
one must be quite unlucky to make such a poor choice of initial vector. (Of course, the
stupid choice v(0) = 0 is not counted.) Moreover, even if c1 happens to be 0 initially,
numerical round-oﬀerror will typically come to one’s rescue, since it will almost inevitably
introduce a tiny component of the eigenvector v1 into some iterate, and this component
will eventually dominate the computation. The trick is to wait long enough for it to appear!
Since the iterates of A are, typically, getting either very large — when ρ(A) > 1 —
or very small — when ρ(A) < 1 — the iterated vectors will be increasingly subject to
numerical overﬂow or underﬂow, and the method may break down before a reasonable
approximation is achieved. One way to avoid this outcome is to restrict our attention
to unit vectors relative to a given norm, e.g., the Euclidean norm or the ∞norm, since
their entries cannot be too large, and so are less likely to cause numerical errors in the
computations. As usual, the unit vector u(k) = ∥v(k) ∥−1 v(k) is obtained by dividing the
iterate by its norm; it can be computed directly by the modiﬁed iterative algorithm
u(0) =
v(0)
∥v(0) ∥,
and
u(k+1) =
Au(k)
∥Au(k) ∥.
(9.82)
If the dominant eigenvalue is positive, λ1 > 0, then u(k) →u1 will converge to one of the

9.5 Numerical Computation of Eigenvalues
525
k
u(k)
λ
0
1
0
0
1
−.3015
−.3015
−.9045
−1.0000
2
−.2335
.3669
−.9005
7.0000
3
−.3319
.2257
−.9159
3.5714
4
−.2788
.3353
−.8999
3.1600
5
−.3159
.2740
−.9084
3.0506
6
−.2919
.3176
−.9022
3.0166
7
−.3080
.2899
−.9061
3.0055
8
−.2973
.3089
−.9035
3.0018
9
−.3044
.2965
−.9052
3.0006
10
−.2996
.3048
−.9041
3.0002
11
−.3028
.2993
−.9048
3.0001
12
−.3007
.3030
−.9043
3.0000
two dominant unit eigenvectors (the other is −u1). If λ1 < 0, then the iterates will switch
back and forth between the two eigenvectors, so u(k) ≃±u1. In either case, the dominant
eigenvalue λ1 is obtained as a limiting ratio between nonzero entries of Au(k) and u(k).
If some other sort of behavior is observed, it means that one of our assumptions is not
valid; either A has more than one dominant eigenvalue of maximum modulus, e.g., it has
a complex conjugate pair of eigenvalues of largest modulus, or it is not complete. In such
cases, one can apply the more general long term behavior described in Exercise 9.2.8 to
pin down the dominant eigenvalues.
Example 9.42.
For the matrix considered in Example 9.41, starting the iterative sys-
tem (9.82) with u(k) = ( 1, 0, 0 )T , the resulting unit vectors are tabulated above.
The
last column, being the ratio between the ﬁrst components of Au(k−1) and u(k−1), again
converges to the dominant eigenvalue λ1 = 3.
Variants of the Power Method for computing the other eigenvalues of the matrix are
explored in the exercises.
Remark. See Wilkinson, [90; Chapter 2] for the perturbation theory of eigenvalues, i.e.,
how they can behave under small perturbations of the matrix. Wilkinson deﬁnes a spectral
condition number to equal the product of the norms of the matrix used to place the
matrix in Jordan canonical form and its inverse. The larger the spectral condition number,
the more the eigenvalues deviate under perturbation. In particular symmetric matrices
have spectral condition number = 1, and so their eigenvalues are well behaved under
perturbations.
He also gives examples of highly ill-conditioned matrices.
Similarly, in
[69; Section 3.3], Saad deﬁnes a condition number for an individual simple eigenvalue, and
proves that it is the reciprocal of the cosine of the angle between its eigenvectors and
co-eigenvectors (left eigenvectors).

526
9 Iteration
Exercises
♠9.5.1. Use the Power Method to ﬁnd the dominant eigenvalue and associated eigenvector of the
following matrices:
(a)

−1
−2
3
4
	
,
(b)

−5
2
−3
0
	
,
(c)
⎛
⎜
⎝
3
−1
0
−1
2
−1
0
−1
3
⎞
⎟
⎠,
(d)
⎛
⎜
⎝
−2
0
1
−3
−2
0
−2
5
4
⎞
⎟
⎠,
(e)
⎛
⎜
⎝
−1
−2
−2
1
2
5
−1
4
0
⎞
⎟
⎠, (f )
⎛
⎜
⎝
2
2
1
1
3
1
2
2
2
⎞
⎟
⎠,
(g)
⎛
⎜
⎜
⎜
⎝
2
−1
0
0
−1
2
−1
0
0
−1
2
−1
0
0
−1
2
⎞
⎟
⎟
⎟
⎠,
(h)
⎛
⎜
⎜
⎜
⎝
4
1
0
1
1
4
1
0
0
1
4
1
1
0
1
4
⎞
⎟
⎟
⎟
⎠.
♠9.5.2. Use the Power Method to ﬁnd the largest singular value of the following matrices:
(a)

1
2
−1
3
	
,
(b)

2
1
−1
−2
3
1
	
,
(c)

2
2
1
−1
1
−2
0
1
	
,
(d)
⎛
⎜
⎝
3
1
−1
1
−2
2
2
−1
1
⎞
⎟
⎠.
♠9.5.3. Let Tn be the tridiagonal matrix whose diagonal entries are all equal to 2 and whose
sub- and super-diagonal entries all equal 1. Use the Power Method to ﬁnd the dominant
eigenvalue of Tn for n = 10, 20, 50. Do your values agree with those in Exercise 8.2.47? How
many iterations do you require to obtain 4 decimal place accuracy?
♦9.5.4. Prove that, for the iterative method (9.82), ∥Au(k) ∥→| λ1 |. Assuming λ1 is real,
explain how to deduce its sign.
♦9.5.5. The Inverse Power Method. Let A be a nonsingular matrix. (a) Show that the
eigenvalues of A−1 are the reciprocals 1/λ of the eigenvalues of A. How are the eigenvectors
related?
(b) Show how to use the Power Method on A−1 to produce the smallest
(in modulus) eigenvalue of A. (c) What is the rate of convergence of the algorithm?
(d) Design a practical iterative algorithm based on the (permuted) LU decomposition of A.
♠9.5.6. Apply the Inverse Power Method of Exercise 9.5.7 to the ﬁnd the smallest eigenvalue of
the matrices in Exercise 9.5.1.
♦9.5.7. The Shifted Inverse Power Method. Suppose that μ is not an eigenvalue of A.
(a) Show that the iterative system u(k+1) = (A −μ I )−1u(k) converges to the eigenvector
of A corresponding to the eigenvalue λ⋆that is closest to μ. Explain how to ﬁnd the
eigenvalue λ⋆. (b) What is the rate of convergence of the algorithm?
(c) What happens if
μ is an eigenvalue?
♠9.5.8. Apply the Shifted Inverse Power Method of Exercise 9.5.7 to the ﬁnd the eigenvalue
closest to μ = .5 of the matrices in Exercise 9.5.1.
9.5.9. Suppose that Au(k) = 0 in the iterative procedure (9.82). What does this indicate?
♠9.5.10. (i) Explain how to use the Deﬂation Method of Exercise 8.2.51 to ﬁnd the
subdominant eigenvalue of a nonsingular matrix A.
(ii) Apply your method to the
matrices listed in Exercise 9.5.1.
The QR Algorithm
As stated, the Power Method produces only the dominant (largest in magnitude) eigenvalue
of a matrix A. The Inverse Power Method of Exercise 9.5.5 can be used to ﬁnd the smallest
eigenvalue. Additional eigenvalues can be found by using the Shifted Inverse Power Method
of Exercise 9.5.7, or the Deﬂation Method of Exercise 9.5.10. However, if we need to know

9.5 Numerical Computation of Eigenvalues
527
all the eigenvalues, such piecemeal methods are too time-consuming to be of much practical
value.
The most popular scheme for simultaneously approximating all the eigenvalues of a
matrix A is the remarkable QR algorithm, ﬁrst proposed in 1961 by John Francis, [29],
and Vera Kublanovskaya, [51]. The underlying idea is simple, but surprising. The ﬁrst
step is to factor the matrix
A = A0 = Q0 R0
into a product of an orthogonal matrix Q0 and a positive (i.e., with all positive entries along
the diagonal) upper triangular matrix R0 by using the Gram–Schmidt orthogonalization
procedure of Theorem 4.24, or, even better, the numerically stable version described in
(4.28). Next, multiply the two factors together in the wrong order! The result is the new
matrix
A1 = R0 Q0.
We then repeat these two steps. Thus, we next factor
A1 = Q1 R1
using the Gram–Schmidt process, and then multiply the factors in the reverse order to
produce
A2 = R1 Q1.
The complete algorithm can be written as
A = A0 = Q0 R0,
Ak+1 = Rk Qk = Qk+1 Rk+1,
k = 0, 1, 2, . . . ,
(9.83)
where Qk, Rk come from the previous step, and the subsequent orthogonal matrix Qk+1
and positive upper triangular matrix Rk+1 are computed directly from Ak+1 = Rk Qk by
applying the numerically stable form of the Gram–Schmidt algorithm.
The astonishing fact is that, for many matrices A with all real eigenvalues, the iterates
Ak −→V converge to an upper triangular matrix V whose diagonal entries are the eigenval-
ues of A. Thus, after a suﬃcient number of iterations, say m, the matrix Am will have very
small entries below the diagonal, and one can read oﬀa complete system of (approximate)
eigenvalues along its diagonal. For each eigenvalue, the computation of the corresponding
eigenvector can be most eﬃciently accomplished by applying the Shifted Inverse Power
Method of Exercise 9.5.7 with parameter μ chosen near the computed eigenvalue.
Example 9.43.
Consider the matrix A =

2
1
2
3

. The initial Gram–Schmidt factor-
ization A = Q0 R0 yields
Q0 ≃

.7071
−.7071
.7071
.7071

,
R0 ≃

2.8284
2.8284
0
1.4142

.
These are multiplied in the reverse order to give
A1 = R0 Q0 =

4
0
1
1

.
We refactor A1 = Q1 R1 via Gram–Schmidt, and then reverse multiply to produce
Q1 ≃

.9701
−.2425
.2425
.9701

,
R1 ≃

4.1231
.2425
0
.9701

,
A2 = R1 Q1 ≃

4.0588
−.7647
.2353
.9412

.

528
9 Iteration
The next iteration yields
Q2 ≃

.9983
−.0579
.0579
.9983

,
R2 ≃

4.0656
−.7090
0
.9839

,
A3 = R2 Q2 ≃

4.0178
−.9431
.0569
.9822

.
Continuing in this manner, after 9 iterations we obtain, to four decimal places,
Q9 ≃

1
0
0
1

,
R9 ≃

4
−1
0
1

,
A10 = R9 Q9 ≃

4
−1
0
1

.
The eigenvalues of A, namely 4 and 1, appear along the diagonal of A10.
Additional
iterations produce very little further change, although they can be used for increasing the
numerical accuracy of the computed eigenvalues.
If the original matrix A happens to be symmetric and positive deﬁnite, then the limiting
matrix Ak −→V = Λ is, in fact, the diagonal matrix containing the eigenvalues of A.
Moreover, if, in this case, we recursively deﬁne
Sk = Sk−1 Qk = Q0 Q1 · · · Qk−1 Qk,
(9.84)
which then have, as their limit, Sk −→S, an orthogonal matrix, whose columns are the
orthonormal eigenvector basis of A.
Example 9.44.
Consider the symmetric matrix A =
⎛
⎝
2
1
0
1
3
−1
0
−1
6
⎞
⎠.
The initial
A = Q0 R0 factorization produces
S0 = Q0 ≃
⎛
⎝
.8944
−.4082
−.1826
.4472
.8165
.3651
0
−.4082
.9129
⎞
⎠,
R0 ≃
⎛
⎝
2.2361
2.2361
−.4472
0
2.4495
−3.2660
0
0
5.1121
⎞
⎠,
and so
A1 = R0 Q0 ≃
⎛
⎝
3.0000
1.0954
0
1.0954
3.3333
−2.0870
0
−2.0870
4.6667
⎞
⎠.
We refactor A1 = Q1 R1 and reverse multiply to produce
Q1 ≃
⎛
⎝
.9393
−.2734
−.2071
.3430
.7488
.5672
0
−.6038
.7972
⎞
⎠,
S1 = S0 Q1 ≃
⎛
⎝
.7001
−.4400
−.5623
.7001
.2686
.6615
−.1400
−.8569
.4962
⎞
⎠,
R1 ≃
⎛
⎝
3.1937
2.1723
−.7158
0
3.4565
−4.3804
0
0
2.5364
⎞
⎠,
A2 = R1 Q1 ≃
⎛
⎝
3.7451
1.1856
0
1.1856
5.2330
−1.5314
0
−1.5314
2.0219
⎞
⎠.
Continuing in this manner, after 10 iterations we have
Q10 ≃
⎛
⎝
1.0000
−.0067
0
.0067
1.0000
.0001
0
−.0001
1.0000
⎞
⎠,
S10 ≃
⎛
⎝
.0753
−.5667
−.8205
.3128
−.7679
.5591
−.9468
−.2987
.1194
⎞
⎠,
R10 ≃
⎛
⎝
6.3229
.0647
0
0
3.3582
−.0006
0
0
1.3187
⎞
⎠,
A11 ≃
⎛
⎝
6.3232
.0224
0
.0224
3.3581
−.0002
0
−.0002
1.3187
⎞
⎠.

9.5 Numerical Computation of Eigenvalues
529
After 20 iterations, the process has completely settled down, and
Q20 ≃
⎛
⎝
1
0
0
0
1
0
0
0
1
⎞
⎠,
S20 ≃
⎛
⎝
.0710
−.5672
−.8205
.3069
−.7702
.5590
−.9491
−.2915
.1194
⎞
⎠,
R20 ≃
⎛
⎝
6.3234
.0001
0
0
3.3579
0
0
0
1.3187
⎞
⎠,
A21 ≃
⎛
⎝
6.3234
0
0
0
3.3579
0
0
0
1.3187
⎞
⎠.
The eigenvalues of A appear along the diagonal of A21, while the columns of S20 are the
corresponding orthonormal eigenvector basis, listed in the same order as the eigenvalues,
both correct to 4 decimal places.
We will devote the remainder of this section to a justiﬁcation of the QR algorithm for
a class of matrices. We will assume that A is symmetric, and that its (necessarily real)
eigenvalues satisfy
| λ1 | > | λ2 | > · · · > | λn | > 0.
(9.85)
According to the Spectral Theorem 8.38, the corresponding unit eigenvectors u1, . . . , un
(in the Euclidean norm) form an orthonormal basis of Rn. Our analysis can be adapted to
a broader class of matrices, but this will suﬃce to expose the main ideas without unduly
complicating the exposition.
The secret is that the QR algorithm is, in fact, a well-disguised adaptation of the more
primitive Power Method. If we were to use the Power Method to capture all the eigenvectors
and eigenvalues of A, the ﬁrst thought might be to try to perform it simultaneously on
a complete basis v(0)
1 , . . . , v(0)
n
of Rn instead of just one individual vector. The problem
is that, for almost all vectors, the power iterates v(k)
j
= Ak v(0)
j
all tend to a multiple of
the dominant eigenvector u1. Normalizing the vectors at each step, as in (9.82), is not
any better, since then they merely converge to one of the two dominant unit eigenvectors
±u1. However, if, inspired by the form of the eigenvector basis, we orthonormalize the
vectors at each step, then we eﬀectively prevent them from all accumulating at the same
dominant unit eigenvector, and so, with some luck, the resulting vectors will converge to the
full system of eigenvectors. Since orthonormalizing a basis via the Gram–Schmidt process
is equivalent to a QR matrix factorization, the mechanics of the algorithm becomes less
surprising.
In detail, we start with any orthonormal basis, which, for simplicity, we take to be the
standard basis vectors of Rn, and so u(0)
1
= e1, . . . , u(0)
n
= en. At the kth stage of the
algorithm, we set u(k)
1 , . . . , u(k)
n
to be the orthonormal vectors that result from applying
the Gram–Schmidt algorithm to the power vectors v(k)
j
= Ak ej. In matrix language, the
vectors v(k)
1 , . . . , v(k)
n
are merely the columns of Ak, and the orthonormal basis u(k)
1 , . . . , u(k)
n
are the columns of the orthogonal matrix Sk in the QR decomposition of the kth power of
A, which we denote by
Ak = Sk Pk,
(9.86)
where Pk is positive upper triangular, meaning all its diagonal entries are positive. Note
that, in view of (9.83)
A = Q0 R0,
A2 = Q0 R0 Q0 R0 = Q0 Q1 R1 R0,
A3 = Q0 R0 Q0 R0 Q0 R0 = Q0 Q1 R1 Q1 R1 R0 = Q0 Q1 Q2 R2 R1 R0,
and, in general,
Ak =

Q0 Q1 · · · Qk−1
 
Rk−1 · · · R1 R0

.
(9.87)

530
9 Iteration
Proposition 4.23 tells us that the product of orthogonal matrices is also orthogonal. The
product of positive upper triangular matrices is also positive upper triangular. Therefore,
comparing (9.86, 87) and invoking the uniqueness of the QR factorization, we conclude
that
Sk = Q0 Q1 · · · Qk−1 = Sk−1 Qk−1,
Pk = Rk−1 · · · R1 R0 = Rk−1 Pk−1.
(9.88)
Let S = ( u1 u2 . . . un ) denote an orthogonal matrix whose columns are unit eigenvec-
tors of A. The Spectral Theorem 8.38 tells us that
A = S Λ ST ,
where
Λ = diag (λ1, . . ., λn)
is the diagonal eigenvalue matrix. Substituting the spectral factorization into (9.86) yields
Ak = S Λk ST = Sk Pk.
We now make one additional assumption on the matrix A by requiring that ST be a
regular matrix, meaning that it can be factored, ST = LU, as the product of a lower
unitriangular matrix and an upper triangular matrix. We can further assume, without loss
of generality, that the diagonal entries of U — that is, the pivots of ST — are all positive.
Indeed, by Exercise 1.3.31, this can be arranged by multiplying each row of ST by the sign
of its pivot, which amounts to possibly replacing some of the unit eigenvectors uj by their
negatives −uj, which is allowed, since it does not aﬀect their status as an orthonormal
eigenvector basis. Regularity of ST holds generically, and is the analogue of the condition
that our initial vector in the Power Method includes a nonzero component of the dominant
eigenvector.
Under these two assumptions,
Ak = S Λk LU = Sk Pk,
and hence
S Λk L = Sk Pk U −1.
Multiplying on the right by Λ−k, we obtain
S Λk L Λ−k = Sk Tk,
where
Tk = Pk U −1 Λ−k
(9.89)
is also a positive upper triangular matrix, since Pk, U, Λ are all of that form.
Now consider what happens as k →∞.
The entries of the lower triangular matrix
N = Λk L Λ−k are
nij =
⎧
⎪
⎨
⎪
⎩
lij(λi/λj)k,
i > j,
lii = 1,
i = j,
0,
i < j.
Since we are assuming | λi | < | λj | when i > j, we immediately deduce that
Λk L Λ−k −→I ,
and hence
Sk Tk = S Λk L Λ−k −→S
as
k −→∞.
We now appeal to the following lemma, whose proof will be given after we ﬁnish the
justiﬁcation of the QR algorithm.
Lemma 9.45. Let S1, S2, . . . and S be orthogonal matrices and T1, T2, . . . positive upper
triangular matrices. Then Sk Tk →S as k →∞if and only if Sk →S and Tk →I .
Lemma 9.45 implies that, as claimed, the orthogonal matrices Sk do converge to the
orthogonal eigenvector matrix S. Moreover, by (9.88–89),
Rk = Pk P −1
k−1 =

Tk Λk U −1  
Tk−1 Λk−1 U −1 −1 = Tk Λ T −1
k−1.

9.5 Numerical Computation of Eigenvalues
531
Since both Tk and Tk−1 converge to the identity matrix, Rk converges to the diagonal
eigenvalue matrix Λ, as claimed. The eigenvalues appear in decreasing order along the
diagonal — this is a consequence of our regularity assumption on the transposed eigenvector
matrix ST .
Theorem 9.46. If A is positive deﬁnite with all simple eigenvalues, and its transposed
eigenvector matrix ST is regular, then the matrices Sk →S and Rk →Λ appearing in the
QR algorithm applied to A converge to, respectively, the eigenvector matrix S and the
diagonal eigenvalue matrix Λ.
Remark. If A is symmetric and has all simple eigenvalues, then, for suitably large α ≫0,
the shifted matrix A = A+α I is positive deﬁnite, has the same eigenvectors as A, and has
simple shifted eigenvalues λk = λk + α. Thus, one can run the QR algorithm to determine
the eigenvalues and eigenvectors of A, and hence those of A by undoing the shift.
The last remaining item is a proof of Lemma 9.45. We write
S = ( u1 u2 . . . un ),
Sk =
$
u(k)
1
u(k)
2
. . . u(k)
n
%
,
in columnar form. Let t(k)
ij
denote the entries of the positive upper triangular matrix Tk.
The last column of the limiting equation Sk Tk →S reads t(k)
nn u(k)
n
→un. Since both u(k)
n
and un are unit vectors, and t(k)
nn > 0, it follows that
∥t(k)
nn u(k)
n ∥= t(k)
nn −→∥un ∥= 1,
and hence the last column
u(k)
n
−→un.
The next to last column reads
t(k)
n−1,n−1 u(k)
n−1 + t(k)
n−1,n u(k)
n
−→un−1.
Taking the inner product with u(k)
n
→un and using orthonormality, we deduce t(k)
n−1,n →0,
and so t(k)
n−1,n−1 u(k)
n−1 →un−1, which, by the previous reasoning, implies t(k)
n−1,n−1 →1 and
u(k)
n−1 →un−1.
The proof is completed by working backwards through the remaining
columns, using a similar argument at each step.
The remaining details are left to the
interested reader.
Exercises
9.5.11. Apply the QR algorithm to the following symmetric matrices to ﬁnd their eigenvalues
and eigenvectors to 2 decimal places:
(a)

1
2
2
6
	
,
(b)

3
−1
−1
5
	
,
(c)
⎛
⎜
⎝
2
1
0
1
2
3
0
3
1
⎞
⎟
⎠,
(d)
⎛
⎜
⎝
2
5
0
5
0
−3
0
−3
3
⎞
⎟
⎠,
(e)
⎛
⎜
⎜
⎜
⎝
3
−1
0
0
−1
3
−1
0
0
−1
3
−1
0
0
−1
3
⎞
⎟
⎟
⎟
⎠,
(f )
⎛
⎜
⎜
⎜
⎝
6
1
−1
0
1
8
1
−1
−1
1
4
1
0
−1
1
3
⎞
⎟
⎟
⎟
⎠.
9.5.12. Show that applying the QR algorithm to the matrix A =
⎛
⎜
⎝
4
−1
1
−1
7
2
1
2
7
⎞
⎟
⎠results in a
diagonal matrix with the eigenvalues on the diagonal, but not in decreasing order. Explain.

532
9 Iteration
9.5.13. Apply the QR algorithm to the following non-symmetric matrices to ﬁnd their
eigenvalues to 3 decimal places:
(a)

−1
−2
3
4
	
, (b)

2
3
1
5
	
, (c)
⎛
⎜
⎝
2
1
0
2
0
−3
0
−2
1
⎞
⎟
⎠, (d)
⎛
⎜
⎝
2
5
1
2
−1
3
4
5
3
⎞
⎟
⎠, (e)
⎛
⎜
⎜
⎜
⎝
6
1
7
9
6
8
14
9
3
1
4
6
3
2
5
3
⎞
⎟
⎟
⎟
⎠.
9.5.14. The matrix A =
⎛
⎜
⎝
−1
2
1
−2
3
1
−2
2
2
⎞
⎟
⎠has a double eigenvalue of 1, and so our proof of
convergence of the QR algorithm doesn’t apply. Does the QR algorithm ﬁnd its eigenvalues?
9.5.15. Explain why the QR algorithm fails to ﬁnd the eigenvalues of the matrices
(a)

0
1
1
0
	
,
(b)
⎛
⎜
⎝
−2
1
0
0
−2
1
1
0
−2
⎞
⎟
⎠,
(c)
⎛
⎜
⎝
5
−4
2
−4
5
2
2
2
−1
⎞
⎟
⎠.
♦9.5.16. Prove that all of the matrices Ak deﬁned in (9.83) have the same eigenvalues.
♦9.5.17.(a) Prove that if A is symmetric and tridiagonal, then all matrices Ak appearing in the
QR algorithm are also symmetric and tridiagonal. Hint: First prove symmetry.
(b) Is the result true if A is not symmetric — only tridiagonal?
Tridiagonalization
In practical implementations, the direct QR algorithm often takes overly long before pro-
viding reasonable approximations to the eigenvalues of large matrices. Fortunately, the
algorithm can be made much more eﬃcient by a simple preprocessing step. The key ob-
servation is that the QR algorithm preserves the class of symmetric tridiagonal matrices,
and, like Gaussian Elimination, is much faster when applied to this class. Moreover, by
applying a sequence of Householder reﬂection matrices (4.35), we can convert any symmet-
ric matrix into tridiagonal form while preserving all the eigenvalues. Thus, by ﬁrst using
the Householder tridiagonalization process, and then applying the QR Method to the re-
sulting tridiagonal matrix, we obtain an eﬃcient and practical algorithm for computing
eigenvalues of large symmetric matrices. Generalizations to non-symmetric matrices will
be brieﬂy considered at the end of the section.
In Householder’s approach to the QR factorization, we were able to convert the matrix A
to upper triangular form R by a sequence of elementary reﬂection matrices. Unfortunately,
this procedure does not preserve the eigenvalues of the matrix — the diagonal entries of
R are not the eigenvalues — and so we need to be a bit more clever here. We begin by
recalling, from Exercise 8.2.32, that similar matrices have the same eigenvalues (but not
the same eigenvectors).
Lemma 9.47. If H = I −2u uT is an elementary reﬂection matrix, with u ∈Rn a unit
vector (under the Euclidean norm), then A and B = HAH are similar matrices and hence
have the same eigenvalues.
Proof : It suﬃces to note that, according to (4.37), H−1 = H, and hence B = H−1AH is
similar to A.
Q.E.D.
Now, starting with a symmetric n×n matrix A, our goal is to devise a similar tridiagonal
matrix by applying a sequence of Householder reﬂections. Using the Euclidean norm, we

9.5 Numerical Computation of Eigenvalues
533
begin by setting
x1 =
⎛
⎜
⎜
⎜
⎜
⎝
0
a21
a31
...
an1
⎞
⎟
⎟
⎟
⎟
⎠
,
y1 =
⎛
⎜
⎜
⎜
⎜
⎝
0
±r1
0
...
0
⎞
⎟
⎟
⎟
⎟
⎠
,
where
r1 = ∥x1 ∥= ∥y1 ∥,
so that x1 contains all the oﬀ-diagonal entries of the ﬁrst column of A. Let
H1 = I −2u1 uT
1 ,
where
u1 =
x1 −y1
∥x1 −y1 ∥,
be the corresponding elementary reﬂection matrix that maps x1 to y1. Either the plus or
the minus sign in the formula for y1 works in the algorithm; a good choice is to set it to be
the opposite of the sign of the entry a21, which helps minimize the possible eﬀects of round-
oﬀerror in computing the unit vector u1. By direct computation, based on Lemma 4.28
and the fact that the ﬁrst entry of u1 is zero, we obtain
A2 = H1AH1 =
⎛
⎜
⎜
⎜
⎜
⎝
a11
r1
0
. . .
0
r1
a22
a23
. . .
a2n
0
a32
a33
. . .
a3n
...
...
...
...
...
0
an2
an3
. . .
ann
⎞
⎟
⎟
⎟
⎟
⎠
(9.90)
for certain aij, whose explicit formulae are not needed. Thus, by a single Householder
transformation, we convert A into a similar matrix A2 whose ﬁrst row and column are in
tridiagonal form. We repeat the process on the lower right (n −1) × (n −1) submatrix of
A2. We set
x2 =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎝
0
0
a32
a42
...
an2
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎠
,
y1 =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎝
0
0
±r2
0
...
0
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎠
,
where
r2 = ∥x2 ∥= ∥y2 ∥,
and the ± sign is chosen to be the opposite of that of a32. Setting
H2 = I −2u2 uT
2 ,
where
u2 =
x2 −y2
∥x2 −y2 ∥,
we construct the similar matrix
A3 = H2A2 H2 =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎝
a11
r1
0
0
. . .
0
r1
a22
r2
0
. . .
0
0
r2
a33
a34
. . .
a3n
0
0
a43
a44
. . .
a4n
...
...
...
...
...
...
0
0
an3
an4
. . .
ann
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎠
,
whose ﬁrst two rows and columns are now in tridiagonal form. The remaining steps in the
algorithm should now be clear. Thus, the ﬁnal result is a tridiagonal matrix T = An that
has the same eigenvalues (but not the same eigenvectors) as the original symmetric matrix
A. Let us illustrate the method by an example.

534
9 Iteration
Example 9.48.
To tridiagonalize A =
⎛
⎜
⎝
4
1
−1
2
1
4
1
−1
−1
1
4
1
2
−1
1
4
⎞
⎟
⎠, we begin with its ﬁrst
column. We set x1 =
⎛
⎜
⎝
0
1
−1
2
⎞
⎟
⎠, so that y1 =
⎛
⎜
⎝
0
√
6
0
0
⎞
⎟
⎠≃
⎛
⎜
⎝
0
2.4495
0
0
⎞
⎟
⎠. Therefore, the unit
vector and corresponding Householder matrix are
u1 =
x1 −y1
∥x1 −y1 ∥=
⎛
⎜
⎝
0
.8391
−.2433
.4865
⎞
⎟
⎠,
H1 = I −2u1 uT
1 =
⎛
⎜
⎝
1
0
0
0
0
−.4082
.4082
−.8165
0
.4082
.8816
.2367
0
−.8165
.2367
.5266
⎞
⎟
⎠.
We compute
A2 = H1AH1 =
⎛
⎜
⎝
4.0000
−2.4495
0
0
−2.4495
2.3333
−.3865
−.8599
0
−.3865
4.9440
−.1246
0
−.8599
−.1246
4.7227
⎞
⎟
⎠.
In the next phase, x2 =
⎛
⎜
⎝
0
0
−.3865
−.8599
⎞
⎟
⎠, y2 =
⎛
⎜
⎝
0
0
−.9428
0
⎞
⎟
⎠, so u2 =
⎛
⎜
⎝
0
0
−.8396
−.5431
⎞
⎟
⎠, and
H2 = I −2u2 uT
2 =
⎛
⎜
⎝
1
0
0
0
0
1
0
0
0
0
−.4100
−.9121
0
0
−.9121
.4100
⎞
⎟
⎠.
The resulting matrix
T = A3 = H2A2H2 =
⎛
⎜
⎝
4.0000
−2.4495
0
0
−2.4495
2.3333
.9428
0
0
.9428
4.6667
0
0
0
0
5
⎞
⎟
⎠
is now in tridiagonal form.
Since the ﬁnal tridiagonal matrix T has the same eigenvalues as A, we can apply the QR
algorithm to T to approximate the common eigenvalues. According to Exercise 9.5.17, if
A = A1 is tridiagonal, so are all its QR iterates A2, A3, . . .. Moreover, far fewer arithmetic
operations are required; in Exercise 9.5.25, you are asked to quantify this. For instance,
in the preceding example, after we apply 20 iterations of the QR algorithm directly to T,
the upper triangular factor has become
R20 =
⎛
⎜
⎝
6.0000
−.0065
0
0
0
4.5616
0
0
0
0
5.0000
0
0
0
0
.4384
⎞
⎟
⎠.
The eigenvalues of T, and hence also of A, appear along the diagonal, and are correct
to 4 decimal places.
As noted earlier, with the eigenvalues in hand the corresponding
eigenvectors can then be found via the Shifted Inverse Power Method of Exercise 9.5.7.

9.5 Numerical Computation of Eigenvalues
535
Finally, even if A is not symmetric, one can still apply the same sequence of Householder
reﬂections to simplify it. The ﬁnal result is no longer tridiagonal, but rather a similar upper
Hessenberg matrix, which means that all entries below its subdiagonal are zero, but those
above its superdiagonal are not necessarily zero. For instance, a 5 × 5 upper Hessenberg
matrix looks like
⎛
⎜
⎜
⎜
⎝
∗
∗
∗
∗
∗
∗
∗
∗
∗
∗
0
∗
∗
∗
∗
0
0
∗
∗
∗
0
0
0
∗
∗
⎞
⎟
⎟
⎟
⎠,
where the starred entries can be anything.
It can be proved that the QR algorithm
maintains the upper Hessenberg form, and, while not as eﬃcient as in the tridiagonal
case, still yields a signiﬁcant savings in computational eﬀort required to ﬁnd the common
eigenvalues.
If A has no eigenvalues of the same magnitude, which, in particular, requires all its
eigenvalues to be simple, then application of the tridiagonal QR algorithm to its tridiago-
nalization will, usually, produce its eigenvalues. More generally, if A has k eigenvalues of
the same magnitude, then the QR algorithm, applied either directly to A, or to its tridi-
agonalization, will, again generically, converge to a block upper triangular matrix, with an
k×k matrix in the block diagonal slot that has these same eigenvalues. Thus, for example,
if A is a real matrix with simple real and complex eigenvalues, then each complex conjugate
pair will be the eigenvalues of one of the 2 × 2 matrices appearing on the diagonal of the
eventual QR iterates, while the real eigenvalues will appear directly (in a 1 × 1 “block”)
on the diagonal.
Further details and results can be found in [21, 66, 69, 89, 90].
Exercises
9.5.18. Use Householder matrices to convert the following matrices into tridiagonal form:
(a)
⎛
⎜
⎝
8
−7
2
−7
17
−7
2
−7
8
⎞
⎟
⎠,
(b)
⎛
⎜
⎜
⎜
⎝
5
1
−2
1
1
5
1
−2
−2
1
5
1
1
−2
1
5
⎞
⎟
⎟
⎟
⎠,
(c)
⎛
⎜
⎜
⎜
⎝
4
0
−1
1
0
1
0
−1
−1
0
2
0
1
−1
0
3
⎞
⎟
⎟
⎟
⎠.
♠9.5.19. Find the eigenvalues, to 2 decimal places, of the matrices in Exercise 9.5.18 by applying
the QR algorithm to the tridiagonal form.
♠9.5.20. Use the tridiagonal QR Method to ﬁnd the singular values of A =
⎛
⎜
⎝
2
2
1
−1
1
−2
0
1
0
−1
2
2
⎞
⎟
⎠.
9.5.21. Use Householder matrices to convert the following matrices into upper Hessenberg form:
(a)
⎛
⎜
⎝
3
−1
2
1
3
−4
2
−1
−1
⎞
⎟
⎠,
(b)
⎛
⎜
⎜
⎜
⎝
3
2
−1
1
2
4
0
1
0
1
2
−6
1
0
−5
1
⎞
⎟
⎟
⎟
⎠,
(c)
⎛
⎜
⎜
⎜
⎝
1
0
−1
1
2
1
1
−1
−1
0
1
3
3
−1
1
4
⎞
⎟
⎟
⎟
⎠.
♠9.5.22. Find the eigenvalues, to 2 decimal places, of the matrices in Exercise 9.5.21 by applying
the QR algorithm to the upper Hessenberg form.
9.5.23. Prove that the eﬀect of the ﬁrst Householder reﬂection is as given in (9.90).
9.5.24. What is the eﬀect of tridiagonalization on the eigenvectors of the matrix?

536
9 Iteration
♦9.5.25.(a) How many arithmetic operations — multiplications/divisions and additions/
subtractions — are required to place a generic n × n symmetric matrix into tridiagonal
form? (b) How many operations are needed to perform one iteration of the QR algorithm
on an n × n tridiagonal matrix? (c) How much faster, on average, is the tridiagonal
algorithm than the direct QR algorithm for ﬁnding the eigenvalues of a symmetric matrix?
9.5.26. Write out a pseudocode program to tridiagonalize a matrix. The input should be an
n × n matrix A, and the output should be the Householder unit vectors u1, . . . , un−1 and
the tridiagonal matrix R. Does your program produce the upper Hessenberg form when the
input matrix is not symmetric?
♦9.5.27. Prove that in the H = LU factorization of a regular upper Hessenberg matrix, the
lower triangular factor L is bidiagonal, as in (1.67).
9.6 Krylov Subspace Methods
So far, we have established two broad classes of algorithms for solving linear systems.
The ﬁrst, known as direct methods, are based on some version of Gaussian Elimination or
matrix factorization. Direct methods eventually† obtain the exact solution, but must be
carried through to completion before any useful information is obtained. The second class
contains the iterative methods discussed above that lead to closer and closer approximations
to the solution, but almost never reach the exact value. One might ask whether there
are algorithms that combine the best of both: semi-direct methods whose intermediate
computations lead to closer and closer approximations, and, moreover, are guaranteed to
terminate in a ﬁnite number of steps with the exact solution in hand.
In recent years, for dealing with large sparse linear systems, such as those arising from
the numerical solution of partial diﬀerential equations, semi-direct iterative methods based
on Krylov subspaces have become quite popular. The original ideas were introduced in the
1930’s by the Russian naval engineer Alexei Krylov, who was in search of an eﬃcient and
reliable method for numerically computing eigenvalues. Krylov methods have seen much
development in a variety of directions, [32, 70, 85], and we will show how they can be used
to iteratively solve linear systems and to compute eigenvalues.
Krylov Subspaces
The starting point is an n×n matrix A, assumed to be real, although extensions to complex
matrices are relatively straightforward. In applications, A is both large and sparse, meaning
that most of its entries are 0, and so multiplying A by a vector v ∈Rn to produce the
vector Av is an eﬃcient operation.
Recall that the Power Method for computing the dominant eigenvalue and eigenvector
of A is based on successive iterates applied to a randomly chosen initial vector: v, Av, A2v,
A3v, . . . . We will employ these particular vectors to span a collection of subspaces.
Deﬁnition 9.49. Given an n × n real matrix A, the Krylov subspace of order k ≥1
generated by a nonzero vector 0 ̸= v ∈Rn is the subspace V (k) ⊂Rn spanned by the
vectors v, Av, A2v, . . . , Ak−1v. We also set V (0) = {0} by convention.
†
This assumes that we are dealing with a fully accurate implementation, i.e., without round-oﬀ
or other numerical error. In this discussion, numerical instability will be left aside as a separate,
albeit ultimately important, concern.

9.6 Krylov Subspace Methods
537
For example, if v is an eigenvector of A, so Av = λv, then V (2) = V (1) is the one-
dimensional eigenspace spanned by v; conversely, if V (2) is one-dimensional, then v is
necessarily an eigenvector, and hence V (k) = V (1) for all k ≥1.
More generally, if
V (j+1) = V (j) for some j ≥0, then V (k) = V (j) for all k ≥j.
This is easily proved
by induction: by assumption, Ajv ∈V (j), and thus can be written as a linear combination
Ajv = c1v + c2Av + · · · + cj−1Aj−2v + cjAj−1v ∈V (j)
for some scalars c1, . . . , cj. Thus,
Aj+1v = c1Av + c2A2v + · · · + cj−1Aj−1v + cjAjv
= cjc1v + (c1 + cjc2)Av + · · · + (cj−2 + cjcj−1)Aj−2v + (cj + c2
j)Aj−1v ∈V (j)
also, proving that V (j+2) = V (j). The general induction step is clear.
Since we assumed v ̸= 0, as otherwise all V (k) = {0} are trivial and not of interest, this
argument implies the existence of an integer m ∈N, called the stabilization order, such
that dim V (k) = k for k = 1, . . . , m, while V (k) = V (m) has dimension m for all k ≥m.
Since we are working in Rn, clearly m ≤n; Exercise 9.6.3 gives a stricter bound for m
in terms of the degree of the minimal polynomial of the matrix A, as deﬁned in Exercise
8.6.23. We also note the following useful result.
Lemma 9.50. Suppose V (k) ̸= V (k−1). Let w ∈V (k) \ V (k−1). Then Aw ∈V (k+1) and,
moreover, V (k+1) is spanned by Aw and (a basis of) V (k). Moreover, if Aw ∈V (k), then
V (k+1) = V (k) and the Krylov subspaces stabilize at order k.
Proof : By assumption,
w = c1v + c2Av + · · · + ck−1Ak−2v + ckAk−1v
for some scalars c1, . . . , ck with ck ̸= 0. Thus, as above,
Aw = c1Av + c2A2v + · · · + ck−1Ak−1v + ckAkv ∈V (k+1).
(9.91)
If Aw ∈V (k), the left-hand side of (9.91) is a linear combination of v, Av, A2v, . . . , Ak−1v,
and hence, since ck ̸= 0, so is Akv, which implies V (k+1) = V (k).
Otherwise, (9.91)
implies that Akv is a linear combination of Aw and Av, A2v, . . . , Ak−1v, and thus every
vector in V (k+1) can be written as a linear combination of Aw and the Krylov vectors
v, Av, A2v, . . . , Ak−1v ∈V (k).
Q.E.D.
For simplicity in what follows, we will assume that A has all real eigenvalues; for ex-
ample A might be a symmetric matrix. We further assume that A has a unique dominant
eigenvalue λ1, so that λ1 is a simple eigenvalue, and | λ1 | > | λj | for all j > 1. In this case,
as we know from our earlier analysis, for most initial choices of the vector v, the vectors
used to deﬁne the Krylov subspace tend to scalar multiples of a dominant eigenvector v1,
meaning that Akv →λk
1 v1 as k →∞. Thus, the Krylov vectors in and of themselves con-
tain increasingly little information, particularly in a numerical environment. As with the
Power Method, matrices with several dominant eigenvalues, including real matrices with
complex conjugate eigenvalues and matrices for which ±λ1 are both eigenvalues, require
suitable modiﬁcations of the methods.
Arnoldi Iteration
The way to get around the pure power behavior was already introduced in the design of
the QR algorithm: instead of the Krylov vectors, one constructs an orthonormal basis of

538
9 Iteration
the Krylov subspace using the Gram–Schmidt process. (As above, we work with the dot
product v · w = vT w and corresponding Euclidean norm throughout this presentation,
leaving the investigation of other inner products to the motivated reader.) To this end, we
may as well start with a unit vector, and so replace the initial vector v by the unit vector
u1 = v/∥v∥, so ∥u1 ∥= 1, which spans the initial Krylov subspace V (1). The second order
subspace V (2) will be spanned by the vectors u1 and Au1, and we extract an orthonormal
basis by projection. First, according to our orthogonal projection formulas, the vector
v2 = Au1 −h11u1,
where
h11 = uT
1 Au1,
satisﬁes the desired orthogonality condition u1·v2 = 0. If v2 = 0, then u1 is an eigenvector
of A, and the process terminates, since the Krylov subspaces would immediately stabilize:
V (k) = V (1) for all k ≥1. Otherwise, we replace v2 by the unit vector
u2 = v2
h21
,
where
h21 = ∥v2 ∥,
and deduce that u1 and u2 form an orthonormal basis for V (2). Proceeding in this manner,
assuming that k ≤m, the stabilization order, at the kth stage, we have already computed
orthonormal vectors u1, . . . , uk such that u1, . . . , uj form an orthonormal basis of V (j) for
each j = 1, . . . , k. Taking w = uk in Lemma 9.50, we deduce that u1, . . . , uk and Auk
span V (k+1). Our orthogonal projection formula (4.41) implies that
vk+1 = Auk −
k

j =1
hjkuj,
where
hjk = uT
j Auk
(9.92)
lies in V (k+1) and is orthogonal to u0, . . . , uk. If vk+1 = 0, then Auk ∈V (k), and, again
by Lemma 9.50, the Krylov spaces have stabilized with V (k+1) = V (k). Otherwise, let
uk+1 = vk+1
hk+1,k
,
where
hk+1,k = ∥vk+1 ∥,
(9.93)
be the corresponding unit vector, so that u0, . . . , uk+1 form an orthonormal basis of V (k+1),
as desired.
While the preceding algorithm will work in favorable situations, the preferred method,
known as Arnoldi iteration, named after the mid-twentieth-century American engineer
Walter Arnoldi, employs the stabilized Gram–Schmidt process described in Section 4.2,
thereby ameliorating, as much as possible, potential numerical instabilities. Thus, at step
k ≥1, having u1, . . . , uk in hand, one iteratively computes
v(1)
k+1 = Auk, v(j+1)
k+1
= v(j)
k+1 −hjkuj,
where
hjk = uT
j v(j)
k+1,
for
j = 1, . . . , k −1.
(9.94)
We then set vk+1 = v(k)
k+1 and, if it is nonzero, use (9.93) to deﬁne the next orthonormal
basis vector uk+1. In Exercise 9.6.6 you are asked to prove that the resulting Arnoldi
vectors uk and coeﬃcients hjk are the same as in (9.92, 93) (if computed exactly).
It is instructive to formulate the Arnoldi orthonormalization process in matrix form.
First note that we can rewrite (9.92–93) as
Auk =
k+1

j =0
hjkuj,
(9.95)

9.6 Krylov Subspace Methods
539
and hence, by orthonormality
hjk =
 uT
j Auk,
1 ≤j ≤k + 1,
0,
j ≥k + 2.
(9.96)
Let Qk = ( u1 u2 . . . uk ) denote the n × k matrix whose columns are the ﬁrst k Arnoldi
vectors. Since these are orthonormal, it follows that
QT
k Qk = I .
(9.97)
(However, keep in mind that Qk is a rectangular matrix, and so QkQT
k is in general not
the identity matrix.) Let
Hk =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
h11
h12
h13
h14
· · ·
h1,k−2
h1,k−1
h1k
h21
h22
h23
h24
· · ·
h2,k−2
h2,k−1
h2k
0
h32
h33
h34
· · ·
h3,k−2
h3,k−1
h3k
0
0
h43
h44
· · ·
h4,k−2
h4,k−1
h4k
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
0
0
0
· · ·
0
hk−1,k−2
hk−1,k−1
hk−1,k
0
0
0
· · ·
0
0
hk,k−1
hkk
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
(9.98)
be the k × k upper Hessenberg matrix formed by the coeﬃcients hjk given in (9.96), which
implies that
Hk = QT
k AQk.
(9.99)
In particular, if A is symmetric, then so is Hk, which implies that it is also tridiagonal. In
this case, the Arnoldi algorithm is known as the symmetric Lanczos algorithm, after the
Hungarian mathematician Cornelius Lanczos.
Equation (9.99) yields an alternative interpretation of the Arnoldi iteration as a (partial)
orthogonal reduction of A to Hessenberg or, in the symmetric case, tridiagonal form. The
matrix Hk can be viewed as the representation of the orthogonal projection of A onto the
Krylov subspace V (k) in terms of the basis formed by the Arnoldi vectors u1, . . . , uk. Thus,
we can identify Hk with the (projected) action of A on the subspace V (k) and, as such, its
dominant eigenvalues and eigenvectors, which can be computed using the QR algorithm,
are expected to form good approximations to those of A itself. Since its predecessor, Hk−1,
coincides with the upper left (k−1)×(k−1) submatrix of Hk, the QR factorizations of the
Hessenberg coeﬃcient matrices Hk can be speeded up by an iterative procedure; see [70]
for details. One can also use Householder reﬂections to tridiagonalize Hk before applying
QR. Of course, if A is symmetric, then, as noted above, Hk is already tridiagonal and
so this step is superﬂuous.
Moreover, if the method is carried out to the stabilization
order m, the resulting Krylov subspace is invariant under A, and hence the eigenvalues of
Hm coincide with those of A restricted to V (m), cf. Exercise 8.4.5. In this manner, the
Arnoldi/Lanczos algorithm produces a semi-direct method for approximating eigenvalues
of the matrix A. Again, the Shifted Inverse Power Method of Exercise 9.5.7 can then be
used to compute each corresponding eigenvector.
We further note, as a consequence of the ﬁrst equation in (9.95), the following formula
relating the Arnoldi matrix Qk to its successor Qk+1:
AQk = Qk+1 Hk,
(9.100)

540
9 Iteration
where
Hk =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
h11
h12
h13
h14
· · ·
h1,k−2
h1,k−1
h1k
h21
h22
h23
h24
· · ·
h2,k−2
h2,k−1
h2k
0
h32
h33
h34
· · ·
h3,k−2
h3,k−1
h3k
0
0
h43
h44
· · ·
h4,k−2
h4,k−1
h4k
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
0
0
0
· · ·
0
hk−1,k−2
hk−1,k−1
hk−1,k
0
0
0
· · ·
0
0
hk,k−1
hkk
0
0
0
· · ·
0
0
0
hk+1,1
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
(9.101)
is the (k + 1) × k matrix formed by appending the indicated bottom row to Hk.
Finally, we note the useful formula
QT
k v = ∥v∥e1,
(9.102)
with e1 = ( 1, 0, 0, . . ., 0 )T ∈Rk the ﬁrst standard basis vector. This is a consequence of
the orthonormality of the Arnoldi vectors u1, . . . , uk, which form the rows of QT
k , along
with the fact that v = ∥v∥u1.
Remark.
In numerical applications, the best results are obtained by maximizing the
stabilization order of the Krylov subspaces generated by the initial vector, and so a random
choice of the initial vector v, or, equivalently, the initial unit vector u1 is preferred so as to
minimize chances of low order degeneration and consequent inaccuracies. In the unlucky
event that stabilization occurs prematurely, one should restart the method with a diﬀerent
choice of initial vector, [70].
The Full Orthogonalization Method
Krylov subspaces can also be applied to generate powerful semi-direct iterative algorithms
for solving linear systems. There are two diﬀerent approaches. The ﬁrst starts with the
concept of a weak or Galerkin formulation of a linear system, which is the elementary
observation that that the only vector that is orthogonal to every vector in an inner product
space is the zero vector; see Exercise 3.1.10(a). As above, we concentrate on the case
V = Rn with the standard dot product. The observation means that x ∈Rn solves the
linear system A x = b if and only if
vT (A x −b) = 0
for all
v ∈Rn.
(9.103)
Solution techniques based on this formulation were ﬁrst studied in depth, in the context
of the mechanics of thin elastic plates, by the Russian engineer Boris Galerkin in the ﬁrst
half of the twentieth century, and often bear his name.
In the case of linear algebraic systems, the Galerkin formulation per se does not add
anything to what we already know.
However, it becomes important for the numerical
approximation of solutions by restricting (9.103) to a smaller-dimensional subspace V ⊂Rn.
Speciﬁcally, one seeks a vector x ∈V such that the Galerkin formulation (9.103) holds for
all v ∈V . In other words, the approximate solution is the vector x ∈V such that the
residual r = b −A x is orthogonal to the subspace V . With a suitably inspired choice of
the subspace V , the Galerkin formulation may well provide a decent approximation to the
actual solution.

9.6 Krylov Subspace Methods
541
Remark. One can easily adapt the Galerkin formulation to general linear systems L[u] =
f, where L: U →V is any linear operator between vector spaces. The corresponding weak
formulation, as described in Exercise 7.5.9, has become an extremely important tool in
the modern mathematical analysis of diﬀerential equations, which take place in inﬁnite-
dimensional function spaces. Moreover, the restriction of the weak formulation to a ﬁnite-
dimensional subspace V ⊂U is the basis of the powerful ﬁnite element solution method
for boundary value problems; see [8, 61] for details.
Remark. The question of existence and uniqueness of the Galerkin approximate solution
depends upon the matrix A and the choice of subspace V . Given a basis v1, . . . , vk of
V , we express x = y1v1 + · · · + ykvk = S y, where S = ( v1 v2 . . . vk ) is the n × k
matrix whose columns are the basis vectors, while y = ( y1, y2, . . . , yk )T ∈Rk contains the
coordinates of x = S y ∈V with respect to the given basis. Then the Galerkin conditions
on V can be written as
vT (A x −b) = vT (AS y −b) = 0
for all
v ∈V.
Expressing v = S z for z ∈Rk in the same fashion, this becomes
zT ST (AS y −b) = zT(STAS y −ST b) = 0
for all
z ∈Rk,
which clearly holds if and only if
STAS y = ST b.
(9.104)
This is a linear system of k equations in the k unknowns y ∈Rk. Thus, a solution exists
and is uniquely determined if and only if the k × k coeﬃcient matrix STAS is nonsingular,
which requires, at the very least, rank A ≥k, and places additional constraints on S.
As you may suspect, in the case of a linear algebraic system, a particularly good choice of
subspace for a Galerkin approximation to the solution is a Krylov subspace. The resulting
solution method is known as the Full Orthogonalization Method, abbreviated FOM, [70].
In detail, the method proceeds as follows. Let V (k) ⊂Rn be the order k Krylov subspace
generated by the right-hand side b, and thus spanned by b, Ab, A2b, . . . , Ak−1b. The kth
Krylov approximation to the solution x is the vector xk ∈V (k) whose residual rk = b−A xk
satisﬁes the Galerkin condition of being orthogonal to the subspace:
v · rk = vT (b −A xk) = 0
for all
v ∈V (k).
In particular, the initial approximation is taken to be x0 = 0 ∈V (1), with residual vector
r0 = b −A x0 = b. Moreover, Lemma 9.50 implies rk = b −A xk ∈V (k+1). Since it is
orthogonal to V (k), it must be a scalar multiple of the (k + 1)st Arnoldi vector:
rk = ck+1uk+1,
where
ck+1 = ∥rk ∥.
(9.105)
This implies that the residual vectors are also mutually orthogonal:
rj · rk = 0,
j ̸= k.
(9.106)
Using the orthonormal Arnoldi basis vectors u1, . . . , uk ∈V (k), which form the columns of
the matrix Qk, we write xk = Qkyk, and hence, recalling (9.99), equation (9.104) becomes
QT
k AQkyk = Hkyk = QT
k b = ∥b∥e1,
(9.107)

542
9 Iteration
where Hk is the upper Hessenberg matrix (9.99), and we use (9.102) (with b replacing v,
as per our initial supposition) to obtain the ﬁnal expression. Solving the resulting system
(9.107), assuming Hk is invertible, for yk = ∥b∥H−1
k e1 produces the kth order Krylov
approximation to the solution
xk = Qkyk = ∥b∥QkH−1
k e1.
(9.108)
Of course, in applications one does not explicitly compute the inverse H−1
k
but rather
uses, say, its LU factorization Hk = LkUk (assuming regularity), coupled with forward
and back substitution to solve (9.107). Moreover, according to Exercise 9.5.27, the lower
unitriangular factor Lk is bidiagonal, meaning that all entries not on the diagonal or
subdiagonal are zero. Of course, because the upper left (k −1) × (k −1) entries of Hk
are the same as those of its predecessor, whose factorization Hk−1 = Lk−1 Uk−1 can be
assumed to already be known, we can quickly factorize Hk. Namely, we write
Hk =
 Hk−1
fk
gT
k
hkk

,
Lk =
 Lk−1
0
mT
k
1

,
Uk =
 Uk−1
zk
0
ukk

,
where fk, gk, mk, zk ∈Rk−1, while hkk, ukk ∈R. Moreover, since Hk is upper Hessenberg,
both gk = hk,k−1ek−1 and mk = lk,k−1ek−1 are multiples of the (k−1)st basis vector ek−1 ∈
Rk−1. Multiplying out Hk = LkUk implies that we need only solve a single triangular linear
system, via forward substitution, along with a pair of scalar linear equations, resulting in
Lk−1zk = fk,
lk,k−1 = hk,k−1/uk−1,k−1,
ukk = hkk −lk,k−1uk−1,k.
(9.109)
Remark. Suppose you happen to know a good initial guess x0 for the solution. The
convergence can then be speeded up by setting x = x −x0, which converts the original
system to Ax = b, where b = r0 = b −A x0 is the initial residual. On applying the
FOM algorithm to the modiﬁed system, the resulting xk ∈V (k) in the Krylov subspaces
generated by b provide the improved approximations xk = xk + x0 to the solution x to
the original system.
The Conjugate Gradient Method
The most important case of the FOM algorithm is that in which the coeﬃcient matrix A is
symmetric, and hence, as noted above, Hk is symmetric, tridiagonal, which means that the
system (9.107) can be quickly solved by the tridiagonal version of Gaussian Elimination,
cf. (1.69–70).
In particular, if A > 0 is positive deﬁnite, then so is Hk > 0, and the
resulting algorithm is known as the Conjugate Gradient Method, often abbreviated CG,
ﬁrst introduced in 1952 by Hestenes and Stiefel, [39].
It is now the most widely used
method for solving linear systems with positive deﬁnite coeﬃcient matrices, e.g., those
arising in the numerical solution to boundary value problems for elliptic systems of partial
diﬀerential equations, [8, 61].
There is a simpler direct way to formulate the CG algorithm, which is the one that is
used in practice. First, we apply Theorems 1.29 and 1.34 to reﬁne the factorization of the
tridiagonal matrix:
Hk = LkDkLT
k ,
(9.110)
where Lk is lower unitriangular and Dk is diagonal. Let Ck be the k × k diagonal matrix
with diagonal entries cj = ∥rj−1 ∥for j = 1, . . ., k, so that, according to (9.105),
QkCk = Rk =

r0 r1 . . . rk−1


9.6 Krylov Subspace Methods
543
is the matrix of residual vectors. Deﬁne
Wk = ( w1 w2 . . . wk ) = QkL−T
k
Ck = RkVk,
(9.111)
where the columns w1, . . . , wk of Wk are known as the conjugate directions, and where
Vk = C−1
k L−T
k
Ck =
⎛
⎜
⎜
⎜
⎜
⎝
1
s1
1
s2
...
...
1
sk−1
1
⎞
⎟
⎟
⎟
⎟
⎠
is upper unitriangular. Note that, for j ≥1, the (j + 1)st column of the matrix equation
Rk = WkV −1
k
implies
rj = wj+1 −sjwj.
(9.112)
We claim that the vectors w1, . . . , wk are conjugate, which means that they mutually
orthogonal with respect to the inner product† ⟨⟨v , w ⟩⟩= vT Aw induced by A, and so
⟨⟨wi , wj ⟩⟩= wT
i Awj = 0,
i ̸= j.
(9.113)
To verify (9.113), we use (9.110, 111) to compute the corresponding Gram matrix, whose
entries are the inner products:
W T
k AWk = CkL−1
k QT
k AQkL−T
k
Ck = CkL−1
k HkL−T
k
Ck = CkDkCk = C2
kDk,
the ﬁnal result being a diagonal matrix. We deduce that all the oﬀ-diagonal entries of the
Gram matrix W T
k AWk vanish, which proves (9.113).
Let us write the kth approximate solution xk ∈V (k) in the form
xk = Qkyk = Wktk = t1w1 + · · · + tkwk,
where
tk = C−1
k LT
k yk.
As a consequence of (9.112) with‡ k replacing j, along with (9.113), its residual vector
rk = b −A xk satisﬁes
⟨⟨rk , wk ⟩⟩= ⟨⟨wk+1 −skwk , wk ⟩⟩= −sk⟨⟨wk , wk ⟩⟩,
⟨⟨rk , wk+1 ⟩⟩= ⟨⟨wk+1 −skwk , wk+1 ⟩⟩= ⟨⟨wk+1 , wk+1 ⟩⟩.
(9.114)
The (k + 1)st approximation can be written in the iterative form
xk+1 = xk + tk+1wk+1,
(9.115)
meaning that we move from xk to xk+1 by adding a suitable scalar mutiple of the conjugate
direction wk+1. The updated residual is
rk+1 = b −A xk+1 = b −A xk −tk+1Awk+1 = rk −tk+1Awk+1.
(9.116)
†
Of course, (9.113) deﬁnes a genuine inner product only if A > 0. On the other hand, the ensuing
calculations only require symmetry of the coeﬃcient matrix, although there is no guarantee that
the resulting linear systems can be solved when A is not positive deﬁnite.
‡
To be completely accurate, the resulting equation appears as the (k + 1)st column of the
subsequent matrix equations Rl = WlV −1
l
for all l ≥k + 1.

544
9 Iteration
Conjugate Gradient Method for Solving A x = b with A > 0
start
choose an initial guess x0, e.g., x0 = 0
for k = 0 to m −1
set rk = b −A xk
if rk = 0 print “xk is the exact solution”; end
if k = 0 set w1 = r0 else set wk+1 = rk +
∥rk ∥2
∥rk−1 ∥2 wk
set xk+1 = xk +
∥rk ∥2
wT
k+1Awk+1
wk+1
next k
end
Orthogonality of the residuals, (9.106), coupled with (9.114) implies
0 = rT
k rk+1 = ∥rk ∥2 −tk+1 rT
k Awk+1 = ∥rk ∥2 −tk+1⟨⟨rk , wk+1 ⟩⟩
= ∥rk ∥2 −tk+1⟨⟨wk+1 , wk+1 ⟩⟩,
hence
tk+1 =
∥rk ∥2
⟨⟨wk+1 , wk+1 ⟩⟩
=
rT
k rk
wT
k+1Awk+1
.
(9.117)
Finally, using (9.106) and (9.116, 117), with k replaced by k −1, yields
∥rk ∥2 = rT
k (rk−1 −tkAwk) = −tk rT
k Awk = −tk ⟨⟨rk , wk ⟩⟩= −⟨⟨rk , wk ⟩⟩∥rk−1 ∥2
⟨⟨wk , wk ⟩⟩
.
Thus, referring back to (9.114),
sk = −⟨⟨rk , wk ⟩⟩
⟨⟨wk , wk ⟩⟩=
∥rk ∥2
∥rk−1 ∥2 .
(9.118)
The iterative equations (9.115, 117, 118) constitute the Conjugate Gradient algorithm,
which is summarized in the accompanying pseudocode. The algorithm can also be applied
if A is merely symmetric, although it may break down if the denominator wT
k+1Awk+1 = 0,
which will not occur in the positive deﬁnite case (why?). At each stage, xk is the current
approximation to the solution. The initial guess x0 can be chosen by the user, with x0 = 0
the default. The number of iterations m ≤n can be speciﬁed in advance; alternatively,
one can impose a stopping criterion based on the size of the residual vector, ∥rk ∥, or,
alternatively, the amount of change between successive iterates, as measured by, say, their
distance ∥xk+1 −xk ∥in either the Euclidean norm or the ∞norm. Because the process
is semi-direct, eventually rk = 0 for some k ≤n, and so, in the absence of round-oﬀerrors,
the result will be the exact solution to the system. Of course, in examples, one would
not carry through the algorithm to the bitter end, since a decent approximation to the
solution is typically obtained with relatively few iterations. For further developments and
applications, see [21, 66, 70, 89].
Remark. The reason for the name “conjugate gradient” is as follows. The term gradient
stems from the minimization principle characterizing the solutions to linear systems with

9.6 Krylov Subspace Methods
545
positive deﬁnite coeﬃcient matrices. According to Theorem 5.2, if A > 0, the solution to
the linear system A x = b is the unique minimizer of the quadratic function
p(x) = 1
2 xT A x −xT b.
(9.119)
One approach to solving the system is to try to successively minimize p(x) as much as
possible.
Suppose we ﬁnd ourselves at a point x that is not the minimizer.
In which
direction should we travel? Multivariable calculus tells us that the gradient vector ∇p(x)
of a function points in the direction of its steepest increase at the point, while its negative
−∇p(x) points in the direction of steepest decrease, [2, 78]. The gradient of the particular
quadratic function (9.119) is easily found:
−∇p(x) = b −A x = r.
Thus, the residual vector speciﬁes the direction of steepest decrease in the quadratic func-
tion, and is thus a good choice of direction in which to head oﬀin search of the true
minimizer. (If one views the graph of p as a mountain range, then, at any given location
x with elevation p(x), the negative gradient −∇p(x) = r points in the steepest downhill
direction.)
This idea leads to the gradient descent algorithm, in which each successive
approximation xk to the solution is obtained by going a certain distance in the residual
direction:
xk+1 = xk + dk rk,
where
rk = b −A xk.
(9.120)
The scalar factor dk is to be speciﬁed so that the resulting p(xk+1) is as small as possible;
in Exercise 9.6.14 you are asked to ﬁnd this value. Gradient descent is a reasonable algo-
rithm, and will lead to the solution in favorable situations. It is also eﬀectively used to
ﬁnd minima of more general nonlinear functions. However, in certain circumstances, the
iterative method based on gradient descent can take a long time to converge to an accurate
approximation to the solution, and so is typically not competitive. To obtain the speedier
Conjugate Gradient algorithm, we modify the gradient descent idea by requiring that the
next descent direction be chosen so that it is conjugate to the preceding directions, i.e.,
satisﬁes (9.113). This idea can be used to produce an independent direct derivation of the
Conjugate Gradient algorithm.
Example 9.51.
Consider the linear system A x = b with
A =
⎛
⎝
3
−1
0
−1
2
1
0
1
1
⎞
⎠,
b =
⎛
⎝
1
2
−1
⎞
⎠.
The exact solution is x⋆= ( 2, 5, −6 )T .
Let us implement the method of conjugate
gradients, starting with the initial guess x0 = ( 0, 0, 0 )T .
The corresponding residual
vector is merely r0 = b −A x0 = b = ( 1, 2, −1 )T .
The ﬁrst conjugate direction is
w1 = r0 = ( 1, 2, −1 )T , and we use formula (9.115) to obtain the updated approximation
to the solution
x1 = x0 +
∥r0 ∥2
⟨⟨w1 , w1 ⟩⟩w1 = 6
4
⎛
⎝
1
2
−1
⎞
⎠=
⎛
⎜
⎝
3
2
3
−3
2
⎞
⎟
⎠,
noting that ⟨⟨w1 , w1 ⟩⟩= wT
1 Aw1 = 4. For the next stage of the algorithm, we compute

546
9 Iteration
the corresponding residual r1 = b −A x1 =

−1
2, −1, −5
2
T. The conjugate direction is
w2 = r1 + ∥r1 ∥2
∥r0 ∥2 w1 =
⎛
⎜
⎝
−1
2
−1
−5
2
⎞
⎟
⎠+
15
2
6
⎛
⎜
⎝
1
2
−1
⎞
⎟
⎠=
⎛
⎜
⎝
3
4
3
2
−15
4
⎞
⎟
⎠,
which, as designed, satisﬁes the conjugacy condition ⟨⟨w1 , w2 ⟩⟩= wT
1 Aw2 = 0. Each
entry of the ensuing approximation
x2 = x1 +
∥r1 ∥2
⟨⟨w2 , w2 ⟩⟩w2 =
⎛
⎜
⎝
3
2
3
−3
2
⎞
⎟
⎠+
15
2
27
4
⎛
⎜
⎝
3
4
3
2
−15
4
⎞
⎟
⎠=
⎛
⎜
⎝
7
3
14
3
−17
3
⎞
⎟
⎠≃
⎛
⎜
⎝
2.3333
4.6667
−5.6667
⎞
⎟
⎠
is now within a 1
3 of the exact solution x⋆.
Since we are dealing with a 3×3 system, we will recover the exact solution by one more
iteration of the algorithm. The new residual is r2 = b −A x2 =

−4
3, 2
3, 0
T . The ﬁnal
conjugate direction is
w3 = r2 + ∥r2 ∥2
∥r1 ∥2 w2 =
⎛
⎜
⎝
−4
3
2
3
0
⎞
⎟
⎠+
20
9
15
2
⎛
⎜
⎝
3
4
3
2
−15
4
⎞
⎟
⎠=
⎛
⎜
⎝
−10
9
10
9
−10
9
⎞
⎟
⎠,
which, as you can check, is conjugate to both w1 and w2. The solution is obtained from
x3 = x2 +
∥r2 ∥2
⟨⟨w3 , w3 ⟩⟩w3 =
⎛
⎜
⎝
7
3
14
3
−17
3
⎞
⎟
⎠+
20
9
200
27
⎛
⎜
⎝
−10
9
10
9
−10
9
⎞
⎟
⎠=
⎛
⎜
⎝
2
5
−6
⎞
⎟
⎠.
The Generalized Minimal Residual Method
A natural alternative to the Galerkin weak approach is to try to directly minimize the norm
of the residual r = b−A x when the approximate solution x is required to lie in a speciﬁed
subspace x ∈V . When V is a Krylov subspace, this idea results in the Generalized Minimal
Residual Method (usually abbreviated GMRES), which was developed by the Algerian and
American mathematicians/computer scientists† Yousef Saad and Martin Schultz, [71].
As in the FOR Method, we choose the Krylov subspaces generated by b, the right-hand
side of the system to be solved, but now seek the vector x⋆
k ∈V (k) that minimizes the
Euclidean norm ∥A x −b∥over all vectors x ∈V (k). This approach corresponds to the
initial approximation x0 = 0 ∈V (1); as before, if we know a better initial guess x0, we set
x = x −x0, which converts the original system to Ax = b, where b = r0 = b −A x0 is the
initial residual, and then apply the method to the new system.
Again, we express the vectors
xk = y1u1 + · · · + ykuk = Qky ∈V (k)
†
Coincidentally, the ﬁrst author of the book you are reading is at the same university,
Minnesota, as Saad, and had the same thesis advisor, Garrett Birkhoﬀ, as Schultz.

9.6 Krylov Subspace Methods
547
as linear combinations of the orthonormal Arnoldi basis vectors, with coeﬃcients yk =
( y1, . . . , yk )T ∈Rk. In view of (9.100) and (9.97), with k replaced by k + 1, the squared
residual norm is given by
∥r∥2
k = ∥A xk −b∥2 = ∥AQk yk −b∥2 = ∥Qk+1 Hkyk −b∥2
= (Qk+1 Hk yk −b)T (Qk+1 Hk yk −b) = yT
k HT
k Hk yk −2yT
k HT
k QT
k+1b + ∥b∥2
= yT
k HT
k Hk yk −2yT
k HT
k ck + ∥ck ∥2 = ∥Hk yk −ck ∥2,
(9.121)
where, according to (9.107) again with k replaced by k + 1,
ck = QT
k+1b = ∥b∥e1 ∈Rk+1,
so that
∥ck ∥= ∥b∥.
(9.122)
We deduce that minimizing ∥A x −b∥over all x ∈V (k) is the same as minimizing
∥Hk y −ck ∥over all y ∈Rk. The latter is a standard least squares minimization problem,
whose solution yk is found by solving the corresponding normal equations
HT
k Hk yk = HT
k ck = ∥b∥HT
k e1 = ∥b∥( h11, h12, . . . , h1k )T .
(9.123)
Solving (9.123), produces the desired minimizer xk = Qkyk ∈V (k), and hence the desired
approximation to the solution to the original linear system.
The result of this calculation is the Generalized Minimal Residual Method (GMRES)
algorithm. To successively approximate the solution to A x = b, on the kth iteration, we
set c = ∥b∥e1, and then perform the following steps:
(a) calculate uk and Hk using the Arnoldi Method;
(b) use least squares to ﬁnd the vector y = yk that minimizes ∥Hk y −c∥;
(c) let xk = Qkyk be the kth approximate solution.
The process is repeated until the residual norm ∥rk ∥= ∥A xk −b∥= ∥Hk y −c∥is
below a pre-assigned threshhold. Again, because of the iterative structure of the Krylov
vectors, and hence the upper Hessenberg matrices Hk, knowing the solution to the order
k minimization roblem allows one to rather quickly construct that of the order k + 1
version.
As with all Krylov methods, GMRES is a semi-direct method and hence, if
performed in exact arithmetic, will eventually produce the exact solution once the Krylov
stabilization order is reached. As with FOM/CG, this is rarely required, and one typically
imposes a stopping criterion based on either the norm of the residual vector or the size
of the diﬀerence between successive iterates ∥xk+1 −xk ∥. The method works very well in
practice, particularly with the sparse coeﬃcient matrices arising in many numerical solution
algorithms for partial diﬀerential equations and beyond, including ﬁnite diﬀerence, ﬁnite
element, collocation, and multipole expansion.
Exercises
9.6.1. Find an orthonormal basis for the Krylov subspaces V (1), V (2), V (3) for the following
matrices and vectors:
(a) A =

0
1
3
1
	
, v =

1
−1
	
;
(b) A =
⎛
⎜
⎝
2
2
−1
2
−1
0
2
1
3
⎞
⎟
⎠, v =
⎛
⎜
⎝
−1
2
0
⎞
⎟
⎠;
(c) A =
⎛
⎜
⎝
1
0
−1
0
2
−3
2
−1
0
⎞
⎟
⎠, v =
⎛
⎜
⎝
1
2
3
⎞
⎟
⎠;
(d) A =
⎛
⎜
⎜
⎜
⎝
2
−1
0
0
−1
2
−1
0
0
−1
2
−1
0
0
−1
2
⎞
⎟
⎟
⎟
⎠, v =
⎛
⎜
⎜
⎜
⎝
1
0
0
0
⎞
⎟
⎟
⎟
⎠.

548
9 Iteration
9.6.2. Let v = x + i y be an eigenvector corresponding to a complex, non-real eigenvalue of the
real n × n matrix A. (a) Prove that the Krylov subspaces V (k) for k ≥2 generated by both
x and y are all two-dimensional. (b) Is the converse valid? Speciﬁcally, if dim V (3) = 2,
then all V (k) are two-dimensional for k ≥1 and spanned by the real and imaginary parts of
a complex eigenvector of A.
♦9.6.3.(a) Prove that the dimension of a Krylov subspace is bounded by the degree of the
minimal polynomial of the matrix A, as deﬁned in Exercise 8.6.23. (b) Is there always a
Krylov subspace whose dimension equals the degree of the minimal polynomial?
9.6.4. True or false: A Krylov subspace is an invariant subspace for the matrix A.
9.6.5. Prove that the invertibility of the coeﬃcient matrix STAS in (9.104) depends only on
the subspace V and not on the choice of basis thereof.
♦9.6.6. Prove that (9.92, 93, 94) give the same Arnoldi vectors uk and the same coeﬃcients hjk
when computed exactly.
9.6.7. Solve the following linear systems by the Conjugate Gradient Method, keeping track of
the residual vectors and solution approximations as you iterate.
(a)

3
−1
−1
5
	
u =

2
1
	
, (b)
⎛
⎜
⎝
6
2
1
2
3
−1
1
−1
2
⎞
⎟
⎠u =
⎛
⎜
⎝
1
0
−2
⎞
⎟
⎠, (c)
⎛
⎜
⎝
6
−1
−3
−1
7
4
−3
4
9
⎞
⎟
⎠u =
⎛
⎜
⎝
−1
−2
7
⎞
⎟
⎠,
(d)
⎛
⎜
⎜
⎜
⎝
6
−1
−1
5
−1
7
1
−1
−1
1
3
−3
5
−1
−3
6
⎞
⎟
⎟
⎟
⎠u =
⎛
⎜
⎜
⎜
⎝
1
2
0
−1
⎞
⎟
⎟
⎟
⎠,
(e)
⎛
⎜
⎜
⎜
⎝
5
1
1
1
1
5
1
1
1
1
5
1
1
1
1
5
⎞
⎟
⎟
⎟
⎠u =
⎛
⎜
⎜
⎜
⎝
4
0
0
0
⎞
⎟
⎟
⎟
⎠.
♣9.6.8. Use the Conjugate Gradient Method to solve the system in Exercise 9.4.33. How many
iterations do you need to obtain the solution that is accurate to 2 decimal places? How
does this compare to the Jacobi and SOR Methods?
♣9.6.9. According to Example 3.39, the n × n Hilbert matrix Hn is positive deﬁnite, and hence
we can apply the Conjugate Gradient Method to solve the linear system Hnu = f. For the
values n = 5, 10, 30, let u⋆∈Rn be the vector with all entries equal to 1.
(a) Compute f = Hnu⋆. (b) Use Gaussian Elimination to solve Hnu = f. How close is
your solution to u⋆?
(c) Does pivoting improve the solution in part (b)?
(d) Does the conjugate gradient algorithm do any better?
9.6.10. Try applying the Conjugate Gradient algorithm to the system −x + 2y + z = −2,
y + 2z = 1, 3x + y −z = 1. Do you obtain the solution? Why or why not?
9.6.11. True or false: If the residual vector r = b−A x satisﬁes ∥r∥< .01, then x approximates
the true solution to within two decimal places.
♦9.6.12. How many arithmetic operations are needed to implement one iteration of the
Conjugate Gradient Method? How many iterations can you perform before the method
becomes more work than direct Gaussian Elimination?
Remark. If the matrix is sparse, the number of operations can decrease dramatically.
♦9.6.13. Fill in the details in a direct derivation of the Conjugate Gradient algorithm following
the ideas outlined in the text: starting with the initial guess x0 and corresponding residual
vector w1 = r0 = b, at the kth step in the algorithm, given the approximation xk and
residual rk = b −A xk, the kth conjugate direction is chosen so that wk+1 = rk + skwk
satisﬁes the conjugacy conditions (9.113). The next approximation xk+1 = xk + tk+1wk+1
is chosen so that its residual rk+1 = b −A xk+1 is as small as possible.
♦9.6.14. In (9.120), ﬁnd the value of dk that minimizes p(xk+1).

9.7 Wavelets
549
♠9.6.15. Use the direct gradient descent algorithm (9.120) using the value of dk found in
Exercise 9.6.14 to solve the linear systems in Exercise 9.6.7. Compare the speed of
convergence with that of the Conjugate Gradient Method.
♣9.6.16. Use GMRES to solve the system in Exercise 9.4.33. Compare the rate of convergence
with the CG algorithm in Exercise 9.6.8.
♣9.6.17. Is GMRES able to solve the system in Exercise 9.6.10?
9.6.18. Explain in what sense the GMRES approximation xk+1 of order k + 1 is a better
approximation to the true solution than that of order k, namely xk.
9.6.19.(a) Explain what happens to the GMRES algorithm if the right-hand side b of the
linear system A x = b is an eigenvector of A. (b) More generally, prove that if the Krylov
subspaces generated by b stabilize at order m, then the solution ot the linear system lies in
V (m) and so the GMRES algorithm converges to the solution at order m.
9.7 Wavelets
Trigonometric Fourier series, both continuous and discrete, are amazingly powerful, but
they do suﬀer from one potentially serious defect. The complex exponential basis functions
e i kx = cos kx+ i sin kx are spread out over the entire interval [−π, π ], and so are not well
suited to processing localized signals — meaning data that are concentrated in a relatively
small regions. Ideally, one would like to construct a system of functions that is orthogonal,
and so has all the advantages of the Fourier basis functions, but, in addition, adapts to
localized structures in signals. This dream was the inspiration for the development of the
modern theory of wavelets.
The Haar Wavelets
Although the modern era of wavelets started in the mid 1980’s, the simplest example of a
wavelet basis was discovered by the Hungarian mathematician Alfr´ed Haar in 1910, [35].
We consider the space of functions (signals) deﬁned the interval [0, 1], equipped with the
standard L2 inner product
⟨f , g ⟩=
 1
0
f(x) g(x) dx.
(9.124)
The usual scaling arguments can be used to adapt the wavelet formulas to any other
interval.
The Haar wavelets are certain piecewise constant functions. The initial four are graphed
in Figure 9.6. The ﬁrst is the box function
ϕ1(x) = ϕ(x) =
 1,
0 < x ≤1,
0,
otherwise,
(9.125)
known as the scaling function, for reasons that shall appear shortly.
Although we are
interested in the value of ϕ(x) only on the interval [0, 1], it will be convenient to extend it,
and all the other wavelets, to be zero outside the basic interval. The second Haar function
ϕ2(x) = w(x) =
⎧
⎪
⎨
⎪
⎩
1,
0 < x ≤1
2,
−1,
1
2 < x ≤1,
0,
otherwise,
(9.126)

550
9 Iteration
ϕ1(x)
ϕ2(x)
ϕ3(x)
ϕ4(x)
Figure 9.6.
The First Four Haar Wavelets.
is known as the mother wavelet.
The third and fourth Haar functions are compressed
versions of the mother wavelet:
ϕ3(x) = w(2x) =
⎧
⎪
⎨
⎪
⎩
1,
0 < x ≤1
4,
−1,
1
4 < x ≤1
2,
0,
otherwise,
ϕ4(x) = w(2x −1) =
⎧
⎪
⎨
⎪
⎩
1,
1
2 < x ≤3
4,
−1,
3
4 < x ≤1,
0,
otherwise,
called daughter wavelets. One can easily check, by direct evaluation of the integrals, that
the four Haar wavelet functions are orthogonal with respect to the L2 inner product (9.124):
⟨ϕi , ϕj ⟩= 0 when i ̸= j.
The scaling transformation x →2x serves to compress the wavelet function, while the
translation 2x →2x −1 moves the compressed version to the right by a half a unit.
Furthermore, we can represent the mother wavelet by compressing and translating the
scaling function:
w(x) = ϕ(2x) −ϕ(2x −1).
(9.127)
It is these two operations of scaling and compression — coupled with the all-important
orthogonality — that underlies the power of wavelets.
The Haar wavelets have an evident discretization. If we decompose the interval (0, 1]
into the four subintervals

0 , 1
4

,
 1
4 , 1
2

,
 1
2 , 3
4

,
 3
4 , 1

,
(9.128)
on which the four wavelet functions are constant, then we can represent each of them by
a vector in R4 whose entries are the values of each wavelet function sampled at the left
endpoint of each subinterval. In this manner, we obtain the wavelet sample vectors
v1 =
⎛
⎜
⎝
1
1
1
1
⎞
⎟
⎠,
v2 =
⎛
⎜
⎝
1
1
−1
−1
⎞
⎟
⎠,
v3 =
⎛
⎜
⎝
1
−1
0
0
⎞
⎟
⎠,
v4 =
⎛
⎜
⎝
0
0
1
−1
⎞
⎟
⎠.
(9.129)
which form the orthogonal wavelet basis of R4 we ﬁrst encountered in Examples 2.35
and 4.10. Orthogonality of the vectors (9.129) with respect to the standard Euclidean dot
product is equivalent to orthogonality of the Haar wavelet functions with respect to the

9.7 Wavelets
551
inner product (9.124). Indeed, if
f(x) ∼f = (f1, f2, f3, f4)
and
g(x) ∼g = (g1, g2, g3, g4)
are piecewise constant real functions that achieve the indicated values on the four subin-
tervals (9.128), then their L2 inner product
⟨f , g ⟩=
 1
0
f(x) g(x) dx = 1
4

f1 g1 + f2 g2 + f3 g3 + f4 g4

= 1
4 f · g,
is equal to the averaged dot product of their sample values — the real form of the inner
product (5.104) that was used in the discrete Fourier transform.
Since the vectors (9.129) form an orthogonal basis of R4, we can uniquely decompose
such a piecewise constant function as a linear combination of wavelets
f(x) = c1 ϕ1(x) + c2 ϕ2(x) + c3 ϕ3(x) + c4 ϕ4(x),
or, equivalently, in terms of the sample vectors,
f = c1v1 + c2v2 + c3v3 + c4v4.
The required coeﬃcients
ck = ⟨f , ϕk ⟩
∥ϕk ∥2 = f · vk
∥vk ∥2
are ﬁxed by our usual orthogonality formula (4.7). Explicitly,
c1 = 1
4 (f1 + f2 + f3 + f4),
c2 = 1
4 (f1 + f2 −f3 −f4),
c3 = 1
2 (f1 −f2),
c4 = 1
2 (f3 −f4).
Before proceeding to the more general case, let us introduce an important analytical
deﬁnition that quantiﬁes precisely how localized a function is.
Deﬁnition 9.52. The support of a function f(x), written supp f, is the closure of the set
where f(x) ̸= 0.
Thus, a point will belong to the support of f(x), if f is not zero there, or at least is not
zero at nearby points. More precisely:
Lemma 9.53. If f(a) ̸= 0, then a ∈supp f. More generally, a point a ∈supp f if and only
if there exists a convergent sequence xn →a such that f(xn) ̸= 0. Conversely, a ̸∈supp f
if and only if f(x) ≡0 on an interval a −δ < x < a + δ for some δ > 0.
Intuitively, the smaller the support of a function, the more localized it is. For example,
the support of the Haar mother wavelet (9.126) is supp w = [0, 1] — the point x = 0 is
included, even though w(0) = 0, because w(x) ̸= 0 at nearby points. The two daughter
wavelets have smaller support:
supp ϕ3 =

0 , 1
2

,
supp ϕ4 =
 1
2 , 1

,
and so are twice as localized.
The eﬀect of scalings and translations on the support of a function is easily discerned.
Lemma 9.54. If supp f = [a, b], and
g(x) = f(rx −δ),
then
supp g =
6 a + δ
r
, b + δ
r
7
.

552
9 Iteration
In other words, scaling x by a factor r compresses the support of the function by a
factor 1/r, while translating x translates the support of the function.
The key requirement for a wavelet basis is that it contains functions with arbitrarily
small support. To this end, the full Haar wavelet basis is obtained from the mother wavelet
by iterating the scaling and translation processes. We begin with the scaling function
ϕ(x),
(9.130)
from which we construct the mother wavelet via (9.127). For each “generation” j ≥0, we
form the wavelet oﬀspring by ﬁrst compressing the mother wavelet so that its support ﬁts
into an interval of length 2−j,
wj,0(x) = w(2j x),
so that
supp wj,0 = [0, 2−j ],
(9.131)
and then translating wj,0 so as to ﬁll up the entire interval [0, 1] by 2j subintervals, each
of length 2−j, deﬁning
wj,k(x) = wj,0(x −k) = w(2j x −k),
where
k = 0, 1, . . ., 2j −1.
(9.132)
Lemma 9.54 implies that supp wj,k = [ 2−j k, 2−j (k + 1) ], and so the combined supports
of all the jth generation of wavelets is the entire interval:
2j−1

k=0
supp wj,k = [0, 1] . The
primal generation, j = 0, consists of just the mother wavelet
w0,0(x) = w(x).
The ﬁrst generation, j = 1, consists of the two daughter wavelets already introduced as ϕ3
and ϕ4, namely
w1,0(x) = w(2x),
w1,1(x) = w(2x −1).
The second generation, j = 2, appends four additional granddaughter wavelets to our basis:
w2,0(x) = w(4x),
w2,1(x) = w(4x −1),
w2,2(x) = w(4x −2),
w2,3(x) = w(4x −3).
The 8 Haar wavelets ϕ, w0,0, w1,0, w1,1, w2,0, w2,1, w2,2, w2,3 are constant on the 8 subin-
tervals of length 1
8, taking the successive sample values indicated by the columns of the
wavelet matrix
W8 =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
1
1
1
0
1
0
0
0
1
1
1
0
−1
0
0
0
1
1
−1
0
0
1
0
0
1
1
−1
0
0
−1
0
0
1
−1
0
1
0
0
1
0
1
−1
0
1
0
0
−1
0
1
−1
0
−1
0
0
0
1
1
−1
0
−1
0
0
0
−1
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
.
(9.133)
Orthogonality of the wavelets is manifested in the orthogonality of the columns of W8. (Un-
fortunately, terminological constraints prevent us from calling W8 an orthogonal matrix,
because its columns are not orthonormal!)
The nth stage consists of 2n+1 diﬀerent wavelet functions comprising the scaling func-
tions and all the generations up to the nth : w0(x) = ϕ(x) and wj,k(x) for 0 ≤j ≤n and
0 ≤k < 2j. They are all constant on each subinterval of length 2−n−1.
Theorem 9.55. The wavelet functions ϕ(x), wj,k(x) form an orthogonal system with
respect to the inner product (9.124).

9.7 Wavelets
553
Proof : First, note that each wavelet wj,k(x) is equal to +1 on an interval of length 2−j−1
and to −1 on an adjacent interval of the same length. Therefore,
⟨wj,k , ϕ ⟩=
 1
0
wj,k(x) dx = 0,
(9.134)
since the +1 and −1 contributions cancel each other. If two diﬀerent wavelets wj,k and
wl,m with, say j ≤l, have supports that are either disjoint, or just overlap at a single
point, then their product wj,k(x) wl,m(x) ≡0, and so their inner product is clearly zero:
⟨wj,k , wl,m ⟩=
 1
0
wj,k(x) wl,m(x) dx = 0.
Otherwise, except in the case when the two wavelets are identical, the support of wl,m is
entirely contained in an interval where wj,k is constant, and so wj,k(x) wl,m(x) = ± wl,m(x).
Therefore, by (9.134),
⟨wj,k , wl,m ⟩=
 1
0
wj,k(x) wl,m(x) dx = ±
 1
0
wl,m(x) dx = 0.
Finally, we compute
∥ϕ∥2 =
 1
0
dx = 1,
∥wj,k ∥2 =
 1
0
wj,k(x)2 dx = 2−j.
(9.135)
The second formula follows from the fact that | wj,k(x) | = 1 on an interval of length 2−j
and is 0 elsewhere.
Q.E.D.
The wavelet series of a signal f(x) is given by
f(x) ∼c0 ϕ(x) +
∞

j =0
2j−1

k=0
cj,k wj,k(x).
(9.136)
Orthogonality implies that the wavelet coeﬃcients c0, cj,k can be immediately computed
using the standard inner product formula coupled with (9.135):
c0 = ⟨f , ϕ ⟩
∥ϕ∥2 =
 1
0
f(x) dx,
cj,k =
⟨f , wj,k ⟩
∥wj,k ∥2 = 2j
 2−jk+2−j−1
2−jk
f(x) dx −2j
 2−j(k+1)
2−jk+2−j−1 f(x) dx.
(9.137)
The convergence properties of the Haar wavelet series (9.136) are similar to those of Fourier
series, [61, 77]; full details can be found [18, 88].
Example 9.56.
In Figure 9.7, we plot the Haar expansions of the signal displayed in
the ﬁrst plot. The following plots show the partial sums for the Haar wavelet series (9.136)
over j = 0, . . . , r with r = 2, 3, 4, 5, 6. Since the wavelets are themselves discontinuous,
they do not have any diﬃculty converging to a discontinuous function. On the other hand,
it takes quite a few wavelets to begin to accurately reproduce the signal — in the last plot,
we are combining a total of 26 = 64 Haar wavelets.

554
9 Iteration
Figure 9.7.
Haar Wavelet Expansion.
Exercises
♠9.7.1. Let f(x) = x. (a) Determine its Haar wavelet coeﬃcients cj,k. (b) Graph the partial
sums sr(x) of the Haar wavelet series (9.136) where j goes from 0 to r = 2, 5, and
10. Compare your graphs with that of f and discuss what you observe. Is the series
converging to the function? Can you prove this?
(c) What is the maximal deviation
∥f −sr ∥∞= max{ | f(x) −sr(x) | | 0 ≤x ≤1 } for each of your partial sums?
♠9.7.2. Answer Exercise 9.7.1 for the functions
(a) x2 −x,
(b) cos πx,
(c)
⎧
⎨
⎩
e−x,
0 < x < 1
2,
−e−x,
1
2 < x < 1.
♣9.7.3. In this exercise, we investigate the compression capabilities of the Haar wavelets. Let
f(x) =
⎧
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎩
−x,
0 ≤x ≤1
3π,
x −2
3π,
1
3π ≤x ≤4
3π,
−x + 2π,
4
3π ≤x ≤2π,
represent a signal deﬁned on 0 ≤x ≤1. Let
sr(x) denote the nth partial sum, from j = 0 to r, of the Haar wavelet series (9.136).
(a) How many diﬀerent Haar wavelet coeﬃcients cj,k appear in sr(x)? If our criterion
for compression is that ∥f −sr ∥∞< ε, how large do you need to choose r when ε = .1?
ε = .01? ε = .001?
(b) Compare the Haar wavelet compression with the discrete Fourier
method of Exercise 5.6.10.
♥9.7.4.(a) Explain why the wavelet expansion (9.136) deﬁnes a linear transformation on Rn
that takes a wavelet coeﬃcient vector c =

c0, c1, . . . , cn−1
T to the corresponding sample
vector f =

f0, f1, . . . , fn−1
T . (b) According to Theorem 7.5, the wavelet map must be
given by matrix multiplication f = Wn c by a 2 × 2n matrix W = Wn. Construct W2, W3
and W4. (c) Prove that the columns of Wn are obtained as the values of the wavelet basis
functions on the 2n sample intervals. (d) Prove that the columns of Wn are orthogonal.
(e) Is Wn an orthogonal matrix? Find a formula for W −1
n . (f ) Explain why the wavelet
transform is given by the linear map, c = W −1
n
f.

9.7 Wavelets
555
♠9.7.5. Test the noise removal features of the Haar wavelets by adding random noise to one of
the functions in Exercises 9.7.1 and 9.7.2, computing the wavelet series, and then setting
the high “frequency” modes to zero. What do you observe? Is this a reasonable denoising
algorithm when compared with a Fourier method?
9.7.6. Write the Haar scaling function and mother wavelet as linear combinations of step
functions.
♦9.7.7. Prove Lemma 9.54.
Modern Wavelets
The main defect of the Haar wavelets is that they do not provide a very eﬃcient means
of representing even very simple functions — it takes quite a large number of wavelets
to reproduce signals with any degree of precision. The reason for this is that the Haar
wavelets are piecewise constant, and so even an aﬃne function y = α x + β requires many
sample values, and hence a relatively extensive collection of Haar wavelets, to be accurately
reproduced. In particular, compression and denoising algorithms based on Haar wavelets
are either insuﬃciently precise or hopelessly ineﬃcient, and hence of minor practical value.
For a long time it was thought that it was impossible to simultaneously achieve the
requirements of localization, orthogonality and accurate reproduction of simple functions.
The breakthrough came in 1988, when the Dutch mathematician Ingrid Daubechies pro-
duced the ﬁrst examples of wavelet bases that realized all three basic criteria. Since then,
wavelets have developed into a sophisticated and burgeoning industry with major impact
on modern technology. Signiﬁcant applications include compression, storage and recogni-
tion of ﬁngerprints in the FBI’s data base, and the JPEG2000 image format, which, unlike
earlier Fourier-based JPEG standards, incorporates wavelet technology in its image com-
pression and reconstruction algorithms. In this section, we will present a brief outline of
the basic ideas underlying Daubechies’ remarkable construction.
The recipe for any wavelet system involves two basic ingredients — a scaling function
and a mother wavelet.
The latter can be constructed from the scaling function by a
prescription similar to that in (9.127), and therefore we ﬁrst concentrate on the properties
of the scaling function.
The key requirement is that the scaling function must solve a
dilation equation of the form
ϕ(x) =
p

k=0
ck ϕ(2x −k) = c0 ϕ(2x) + c1 ϕ(2x −1) + · · · + cp ϕ(2x −p)
(9.138)
for some collection of constants c0, . . . , cp. The dilation equation relates the function ϕ(x)
to a ﬁnite linear combination of its compressed translates. The coeﬃcients c0, . . . , cp are
not arbitrary, since the properties of orthogonality and localization will impose certain
rather stringent requirements.
Example 9.57.
The Haar or box scaling function (9.125) satisﬁes the dilation equation
(9.138) with c0 = c1 = 1, namely
ϕ(x) = ϕ(2x) + ϕ(2x −1).
(9.139)
We recommend that you convince yourself of the validity of this identity before continuing.

556
9 Iteration
Figure 9.8.
The Hat Function.
Example 9.58.
Another example of a scaling function is the hat function
ϕ(x) =
⎧
⎨
⎩
x,
0 ≤x ≤1,
2 −x,
1 ≤x ≤2,
0,
otherwise,
(9.140)
graphed in Figure 9.8. The hat function satisﬁes the dilation equation
ϕ(x) = 1
2 ϕ(2x) + ϕ(2x −1) + 1
2 ϕ(2x −2),
(9.141)
which is (9.138) with c0 = 1
2, c1 = 1, c2 = 1
2. Again, the reader should be able to check
this identity by hand.
The dilation equation (9.138) is a kind of functional equation, and, as such, is not so
easy to solve. Indeed, the mathematics of functional equations remains much less well
developed than that of diﬀerential equations or integral equations. Even to prove that
(nonzero) solutions exist is a nontrivial analytical problem. Since we already know two
explicit examples, let us defer the discussion of solution techniques until we understand
how the dilation equation can be used to construct a wavelet basis.
Given a solution to the dilation equation, we deﬁne the mother wavelet to be
w(x) =
p

k=0
(−1)kcp−k ϕ(2x −k)
= cp ϕ(2x) −cp−1 ϕ(2x −1) + cp−2 ϕ(2x −2) + · · · ± c0 ϕ(2x −p).
(9.142)
This formula directly generalizes the Haar wavelet relation (9.127), in light of its dilation
equation (9.139).
The daughter wavelets are then all found, as in the Haar basis, by
iteratively compressing and translating the mother wavelet:
wj,k(x) = w(2j x −k).
(9.143)
In the general framework, we do not necessarily restrict our attention to the interval [0, 1],
and so j and k can, in principle, be arbitrary integers.
Let us investigate what sort of conditions should be imposed on the dilation coeﬃcients
c0, . . . , cp in order that we obtain a viable wavelet basis by this construction. First, lo-
calization of the wavelets requires that the scaling function have bounded support, and
so ϕ(x) ≡0 when x lies outside some bounded interval [a, b]. Integrating both sides of
(9.138) produces
 b
a
ϕ(x) dx =
 ∞
−∞
ϕ(x) dx =
p

k=0
ck
 ∞
−∞
ϕ(2x −k) dx.
(9.144)
Performing the change of variables y = 2x −k, with dx = 1
2 dy, we obtain
 ∞
−∞
ϕ(2x −k) dx = 1
2
 ∞
−∞
ϕ(y) dy = 1
2
 b
a
ϕ(x) dx,
(9.145)

9.7 Wavelets
557
where we revert to x as our (dummy) integration variable. We substitute this result back
into (9.144). Assuming that
 b
a
ϕ(x) dx ̸= 0, we discover that the dilation coeﬃcients must
satisfy
c0 + · · · + cp = 2.
(9.146)
The second condition we require is orthogonality of the wavelets. For simplicity, we
only consider the standard L2 inner product†
⟨f , g ⟩=
 ∞
−∞
f(x) g(x) dx.
It turns out that the orthogonality of the complete wavelet system is guaranteed once we
know that the scaling function ϕ(x) is orthogonal to all its integer translates:
⟨ϕ(x) , ϕ(x −m) ⟩=
 ∞
−∞
ϕ(x) ϕ(x −m) dx = 0
for all
m ̸= 0.
(9.147)
We ﬁrst note the formula
⟨ϕ(2x −k) , ϕ(2x −l) ⟩=
 ∞
−∞
ϕ(2x −k) ϕ(2x −l) dx
(9.148)
= 1
2
 ∞
−∞
ϕ(x) ϕ(x + k −l) dx = 1
2 ⟨ϕ(x) , ϕ(x + k −l) ⟩
follows from the same change of variables y = 2x −k used in (9.145). Therefore, since ϕ
satisﬁes the dilation equation (9.138), we have
⟨ϕ(x) , ϕ(x −m) ⟩=
/
p

j =0
cj ϕ(2x −j) ,
p

k=0
ck ϕ(2x −2m −k)
0
(9.149)
=
p

j,k=0
cj ck ⟨ϕ(2x −j) , ϕ(2x −2m −k) ⟩= 1
2
p

j,k=0
cj ck ⟨ϕ(x) , ϕ(x + j −2m −k) ⟩.
If we require orthogonality (9.147) of all the integer translates of ϕ, then the left-hand side
of this identity will be 0 unless m = 0, while only the summands with j = 2m + k will be
nonzero on the right. Therefore, orthogonality requires that

0 ≤k ≤p−2m
c2m+k ck =
 2,
m = 0,
0,
m ̸= 0.
(9.150)
The algebraic equations (9.146, 150) for the dilation coeﬃcients are the key requirements
for the construction of an orthogonal wavelet basis.
For example, if we have just two nonzero coeﬃcients c0, c1, then (9.146, 150) reduce to
c0 + c1 = 2,
c2
0 + c2
1 = 2,
and so c0 = c1 = 1 is the only solution, resulting in the Haar dilation equation (9.139). If
we have three coeﬃcients c0, c1, c2, then (9.146), (9.150) require
c0 + c1 + c2 = 2,
c2
0 + c2
1 + c2
2 = 2,
c0 c2 = 0.
†
In all instances, the functions have bounded support, and so the inner product integral can
be reduced to an integral over a ﬁnite interval where both f and g are nonzero.

558
9 Iteration
Thus either c2 = 0, c0 = c1 = 1, and we are back to the Haar case, or c0 = 0, c1 = c2 = 1,
and the resulting dilation equation is a simple reformulation of the Haar case. In particular,
the hat function (9.140) does not give rise to orthogonal wavelets.
The remarkable fact, discovered by Daubechies, is that there is a nontrivial solution
for four (and, indeed, any even number) of nonzero coeﬃcients c0, c1, c2, c3. The basic
equations (9.146), (9.150) require
c0 + c1 + c2 + c3 = 2,
c2
0 + c2
1 + c2
2 + c2
3 = 2,
c0 c2 + c1 c3 = 0.
(9.151)
The particular values
c0 = 1 +
√
3
4
,
c1 = 3 +
√
3
4
,
c2 = 3 −
√
3
4
,
c3 = 1 −
√
3
4
,
(9.152)
solve (9.151). These coeﬃcients correspond to the Daubechies dilation equation
ϕ(x) = 1 +
√
3
4
ϕ(2x) + 3 +
√
3
4
ϕ(2x −1) + 3 −
√
3
4
ϕ(2x −2) + 1 −
√
3
4
ϕ(2x −3).
(9.153)
A nonzero solution of bounded support to this remarkable functional equation will give
rise to a scaling function ϕ(x), a mother wavelet
w(x) = 1 −
√
3
4
ϕ(2x) −3 −
√
3
4
ϕ(2x −1) + 3 +
√
3
4
ϕ(2x −2) −1 +
√
3
4
ϕ(2x −3),
(9.154)
and then, by compression and translation (9.143), the complete system of orthogonal
wavelets wj,k(x).
Before explaining how to solve the Daubechies dilation equation, let us complete the
proof of orthogonality. It is easy to see that, by translation invariance, since ϕ(x) and
ϕ(x −m) are orthogonal whenever m ̸= 0, so are ϕ(x −k) and ϕ(x −l) for all k ̸= l. Next
we prove orthogonality of ϕ(x −m) and w(x):
⟨w(x) , ϕ(x −m) ⟩=
/
p

j =0
(−1)j+1 cj ϕ(2x −1 + j) ,
p

k=0
ck ϕ(2x −2m −k)
0
=
p

j,k=0
(−1)j+1 cj ck ⟨ϕ(2x −1 + j) , ϕ(2x −2m −k) ⟩
= 1
2
p

j,k=0
(−1)j+1 cj ck ⟨ϕ(x) , ϕ(x −1 + j −2m −k) ⟩,
using (9.148). By orthogonality (9.147) of the translates of ϕ, the only summands that are
nonzero are those for which j = 2m + k + 1; the resulting coeﬃcient of ∥ϕ(x)∥2 is

k
(−1)k c1−2m−k ck = 0,
where the sum is over all 0 ≤k ≤p such that 0 ≤1 −2m −k ≤p. Each term in the sum
appears twice, with opposite signs, and hence the result is always zero — no matter what
the coeﬃcients c0, . . . , cp are! The proof of orthogonality of the translates w(x −m) of
the mother wavelet, along with all her wavelet descendants w(2j x −k), relies on a similar
argument, and the details are left as an exercise for the reader.

9.7 Wavelets
559
Figure 9.9.
Approximating the Daubechies Wavelet.
Solving the Dilation Equation
Let us next discuss how to solve the dilation equation (9.138). The solution we are after
does not have an elementary formula, and we require a slightly sophisticated approach to
recover it. The key observation is that (9.138) has the form of a ﬁxed point equation
ϕ = F[ϕ],
not in ordinary Euclidean space, but in an inﬁnite-dimensional function space. With luck,
the ﬁxed point (or, more correctly, ﬁxed function) will be stable, and so starting with a
suitable initial guess ϕ0(x), the successive iterates
ϕn+1 = F[ϕn ]
will converge to the desired solution: ϕn(x) −→ϕ(x). In detail, the iterative version of
the dilation equation (9.138) reads
ϕn+1(x) =
p

k=0
ck ϕn(2x −k),
n = 0, 1, 2, . . . .
(9.155)
Before attempting to prove convergence of this iterative procedure to the Daubechies scaling
function, let us experimentally investigate what happens.
A reasonable choice for the initial guess might be the Haar scaling or box function
ϕ0(x) =
 1,
0 < t ≤1.
0,
otherwise.
In Figure 9.9 we graph the subsequent iterates ϕ1(x), ϕ2(x), ϕ4(x), ϕ5(x), ϕ7(x). There
clearly appears to be convergence to some function ϕ(x), although the ﬁnal result looks
a little bizarre. Bolstered by this preliminary experimental evidence, we can now try to
prove convergence of the iterative scheme. This turns out to be true; a fully rigorous proof
relies on the Fourier transform, and can be found in [18].
Theorem 9.59. The functions ϕn(x) deﬁned by the iterative functional equation (9.155)
converge uniformly to a continuous function ϕ(x), called the Daubechies scaling function.
Once we have established convergence, we are now able to verify that the scaling function
and consequential system of wavelets form an orthogonal system of functions.

560
9 Iteration
Proposition 9.60. All integer translates ϕ(x −k), for k ∈Z of the Daubechies scaling
function, and all wavelets wj,k(x) = w(2j x −k), j ≥0, are mutually orthogonal functions
with respect to the L2 inner product. Moreover, ∥ϕ∥2 = 1, while ∥wj,k ∥2 = 2−j.
Proof : As noted earlier, the orthogonality of the entire wavelet system will follow once we
know the orthogonality (9.147) of the scaling function and its integer translates. We use
induction to prove that this holds for all the iterates ϕn(x), and so, in view of uniform
convergence, the limiting scaling function also satisﬁes this property. We already know
that the orthogonality property holds for the Haar scaling function ϕ0(x). To demonstrate
the induction step, we repeat the computation in (9.149), but now the left-hand side is
⟨ϕn+1(x) , ϕn+1(x −m) ⟩, while all other terms involve the previous iterate ϕn. In view of
the the algebraic constraints (9.150) on the wavelet coeﬃcients and the induction hypoth-
esis, we deduce that ⟨ϕn+1(x) , ϕn+1(x −m) ⟩= 0 whenever m ̸= 0, while when m = 0,
∥ϕn+1 ∥2 = ∥ϕn∥2. Since ∥ϕ0 ∥= 1, we further conclude that all the iterates, and hence
the limiting scaling function, all have unit L2 norm. The proof of the formula for the norms
of the mother and daughter wavelets is left for Exercise 9.7.19.
Q.E.D.
In practical computations, the limiting procedure for constructing the scaling function
is not so convenient, and an alternative means of computing its values is employed. The
starting point is to determine its values at integer points. First, the initial box function
has values ϕ0(m) = 0 for all integers m ∈Z except ϕ0(1) = 1. The iterative functional
equation (9.155) will then produce the values of the iterates ϕn(m) at integer points m ∈Z.
A simple induction will convince you that ϕn(m) = 0 except for m = 1 and m = 2, and,
therefore, by (9.155),
ϕn+1(1) = 3 +
√
3
4
ϕn(1) + 1 +
√
3
4
ϕn(2),
ϕn+1(2) = 1 −
√
3
4
ϕn(1) + 3 −
√
3
4
ϕn(2),
since all other terms are 0. This has the form of a linear iterative system
v(n+1) = Av(n)
(9.156)
with coeﬃcient matrix
A =
⎛
⎜
⎝
3 +
√
3
4
1 +
√
3
4
1 −
√
3
4
3 −
√
3
4
⎞
⎟
⎠
and where
v(n) =

ϕn(1)
ϕn(2)

.
As we know, the solution to such an iterative system is speciﬁed by the eigenvalues and
eigenvectors of the coeﬃcient matrix, which are
λ1 = 1,
v1 =

1+
√
3
4
1−
√
3
4

,
λ2 = 1
2,
v2 =

−1
1

.
We write the initial condition as a linear combination of the eigenvectors
v(0) =

ϕ0(1)
ϕ0(2)

=

1
0

= 2 v1 −1 −
√
3
2
v2.
The solution is
v(n) = An v(0) = 2 Anv1 −1 −
√
3
2
Anv2 = 2 v1 −1
2n
1 −
√
3
2
v2.

9.7 Wavelets
561
Figure 9.10.
The Daubechies Scaling Function and Mother Wavelet.
The limiting vector

ϕ(1)
ϕ(2)

= lim
n →∞v(n) = 2v1 =
⎛
⎜
⎝
1 +
√
3
2
1 −
√
3
2
⎞
⎟
⎠
gives the desired values of the scaling function:
ϕ(1) = 1 +
√
3
2
= 1.366025 . . . ,
ϕ(2) = 1 −
√
3
2
= −.366025 . . . ,
ϕ(m) = 0,
for all
m ̸= 1, 2.
(9.157)
With this in hand, the Daubechies dilation equation (9.153) then prescribes the function
values ϕ
 1
2 m

at all half-integers, because if x = 1
2 m, then 2x −k = m −k is an integer.
Once we know its values at the half-integers, we can reuse equation (9.153) to give its
values at quarter-integers 1
4 m. Continuing onward, we determine the values of ϕ(x) at all
dyadic points, meaning rational numbers of the form x = m/2j for m, j ∈Z. Continuity
will then prescribe its value at all other x ∈R since x can be written as the limit of dyadic
numbers xn — namely those obtained by truncating its binary (base 2) expansion at the
nth digit beyond the decimal (or, rather “binary”) point. But, in practice, this latter step
is unnecessary, since all computers are ultimately based on the binary number system, and
so only dyadic numbers actually reside in a computer’s memory. Thus, there is no real
need to determine the value of ϕ at non-dyadic points.
The preceding scheme was used to produce the graphs of the Daubechies scaling func-
tion in Figure 9.10. It is a continuous, but non-diﬀerentiable, function — and its graph
has a very jagged, fractal-like appearance when viewed at close range. The Daubechies
scaling function is, in fact, a close relative of the famous example of a continuous, nowhere
diﬀerentiable function originally due to Weierstrass, [42, 53], whose construction also relies
on a similar scaling argument.
Given the values of the Daubechies scaling function on a suﬃciently dense set of dyadic
points, the consequential values of the mother wavelet are given by formula (9.154). Note
that supp ϕ = supp w = [0, 3]. The daughter wavelets are then found by the usual com-
pression and translation procedure (9.143).

562
9 Iteration
Figure 9.11.
Daubechies Wavelet Expansion.
The Daubechies wavelet expansion of a function whose support is contained in† [0, 1]
is then given by
f(x) ∼c0 ϕ(x) +
∞

j =0
2j−1

k=−2
cj,k wj,k(x).
(9.158)
The inner summation begins at k = −2 so as to include all the wavelet oﬀspring wj,k whose
supports have a nontrivial intersection with the interval [0, 1]. The wavelet coeﬃcients
c0, cj,k are computed by the usual orthogonality formula
c0 = ⟨f , ϕ ⟩=
 3
0
f(x) ϕ(x) dx,
cj,k = ⟨f , wj,k ⟩= 2j
 2−j (k+3)
2−j k
f(x) wj,k(x) dx =
 3
0
f

2−j (x + k)

w(x) dx,
(9.159)
where we agree that f(x) = 0 whenever x < 0 or x > 1.
In practice, one employs
a numerical integration procedure, e.g., the trapezoid rule, based on dyadic points to
speedily evaluate the integrals (9.159). A proof of completeness of the resulting wavelet
basis functions can be found in [18].
Compression and denoising algorithms based on
retaining only low-frequency modes proceed as in Section 5.6, and are left as projects for
the motivated reader to implement.
Example 9.61.
In Figure 9.11, we plot the Daubechies wavelet expansions of the same
signal for Example 9.56. The ﬁrst plot is the original signal, and the following show the
partial sums of (9.158) over j = 0, . . . , r with r = 2, 3, 4, 5, 6. Unlike the Haar expansion,
the Daubechies wavelets do exhibit a nonuniform Gibbs phenomenon, where the expansion
noticeably overshoots near the discontinuity, [61], which can be observed at the interior
discontinuity as well as the endpoints, since the function is set to 0 outside the interval
†
For functions with larger support, one should include additional terms in the expansion cor-
responding to further translates of the wavelets so as to cover the entire support of the function.
Alternatively, one can translate and rescale x to ﬁt the function’s support inside [0, 1].

9.7 Wavelets
563
[0, 1]. Indeed, the Daubechies wavelets are continuous, and so cannot converge uniformly
to a discontinuous function.
Exercises
♠9.7.8. Answer Exercises 9.7.1 and 9.7.2 using the Daubechies wavelets instead of the Haar
wavelets. Do you see any improvement in your approximations? Discuss the advantages
and disadvantages of both in light of these examples.
♠9.7.9. Answer Exercise 9.7.3 using the Daubechies wavelets to compress the data. Compare
your results.
♦9.7.10. Verify formulas (9.139) and (9.141).
9.7.11. Prove that the most general solution to the functional equation ϕ(x) = 2 ϕ(2x) is
ϕ(x) = f(log2 x)/x where f(z + 1) = f(z) is any 1 periodic function.
♦9.7.12. Consider the dilation equation (9.138) with c0 = 0, c1 = c2 = 1, so ϕ(x) =
ϕ(2x −1) + ϕ(2x −2). Prove that ψ(x) = ϕ(x + 1) satisﬁes the Haar dilation equation
(9.139). Generalize this result to prove that we can always, without loss of generality,
assume that c0 ̸= 0 in the general dilation equation (9.138).
9.7.13. Prove that a cubic B spline, as deﬁned in Exercise 5.5.76, solves the dilation
equation (9.138) for c0 = c4 = 1
8, c1 = c3 = 1
2, c2 = 3
4.
9.7.14. Explain why the scaling function ϕ(x) and the mother wavelet w(x) have the same
support: supp ϕ = supp w.
9.7.15. Prove that (9.147) implies ⟨ϕ(x −l) , ϕ(x −m) ⟩= 0 for all l ̸= m.
♦9.7.16. Let ϕ(x) be any scaling function, w(x) the corresponding mother wavelet and wj,k(x)
the wavelet descendants. Prove that (a) ∥ϕ∥= ∥w∥.
(b) ∥wj,k ∥= 2−j ∥ϕ∥.
♦9.7.17.(a) Prove that the scaling function ϕ(x) and the mother wavelet w(x) are orthogonal.
(b) Prove that the integer translates w(x −m) of the mother wavelet are mutually
orthogonal. (c) Prove orthogonality of all the wavelet oﬀspring wj,k(x).
9.7.18. Find the values of the Daubechies scaling function ϕ(x) and mother wavelet w(x) at
x = (a)
1
2, (b)
1
4, (c)
5
16.
♦9.7.19. Prove the formulas in Proposition 9.60 for the norms of the mother and daughter
wavelets.
♠9.7.20. Write a computer program to zoom in on the Daubechies scaling function and discuss
what you see.
9.7.21. True or false: The iterative system (9.156) is a Markov process.
♦9.7.22. Let ϕ(x) satisfy the Daubechies scaling equation (9.153). Prove that if ϕ(i) ̸= 0 for any
i ≤0 or i ≥p, then supp ϕ is unbounded.
9.7.23.(a) Use (9.142) to construct the “mother wavelet” corresponding to the hat function
(9.140). (b) Is the hat function orthogonal to the mother wavelet? (c) Is the hat function
orthogonal to its integer translates?
9.7.24. Prove that a real number x is dyadic if and only if its binary (base 2) expansion
terminates, i.e., is eventually all zeros.
9.7.25. Find dyadic approximations, with error at most 2−8, to
(a)
3
4,
(b)
1
3,
(c)
√
2,
(d) e,
(e) π.

Chapter 10
Dynamics
In this chapter, we turn our attention to continuous dynamical systems, which are governed
by ﬁrst and second order linear systems of ordinary diﬀerential equations. Such systems,
whose unvarying equilibria were the subject of Chapter 6, include the dynamical motions
of mass–spring chains and structures, and the time-varying voltages and currents in simple
electrical circuits. Dynamics of continuous media, including ﬂuids, solids, and gases, are
modeled by inﬁnite-dimensional dynamical systems described by partial diﬀerential equa-
tions, [61, 79], and will not be treated in this text, nor will we venture into the vastly more
complicated realm of nonlinear dynamics, [34, 41].
Chapter 8 developed the basic mathematical tools — eigenvalues and eigenvectors —
used in the analysis of linear systems of ordinary diﬀerential equations. For a ﬁrst order
system, the resulting eigensolutions describe the basic modes of exponential growth, decay,
or periodic behavior. In particular, the stability properties of an equilibrium solution are
(mostly) determined by the eigenvalues. Most of the phenomenology inherent in linear
dynamics can already be observed in the two-dimensional situation, and we devote Sec-
tion 10.3 to a complete description of ﬁrst order planar linear systems. In Section 10.4, we
re-interpret the solution to a ﬁrst order system in terms of the matrix exponential, which
is deﬁned by analogy with the usual scalar exponential function. Matrix exponentials are
particularly eﬀective for solving inhomogeneous or forced linear systems, and also appear
in applications to geometry, computer graphics and animation, theoretical physics, and
mechanics.
As a consequence of Newton’s laws of motion, mechanical vibrations are modeled by
second order dynamical systems. For stable conﬁgurations with no frictional damping, the
eigensolutions constitute the system’s normal modes, each periodically vibrating with its
associated natural frequency. The full dynamics is obtained by linear superposition of the
periodic normal modes, but the resulting solution is, typically, no longer periodic. Such
quasi-periodic motion may seem quite chaotic — even though mathematically it is merely
a combination of ﬁnitely many simple periodic solutions. When subjected to an external
periodic forcing, the system usually remains in a quasi-periodic motion that superimposes
a periodic response onto its own internal vibrations. However, attempting to force the
system at one of its natural frequencies, as prescribed by its eigenvalues, may induce
a resonant vibration, of progressively unbounded amplitude, resulting in a catastrophic
breakdown of the physical apparatus.
In contrast, frictional eﬀects, depending on ﬁrst
order derivatives/velocities, serve to damp out the quasi-periodic vibrations and similarly
help mitigate the dangers of resonance.
10.1 Basic Solution Techniques
Our initial focus will be on systems
du
dt = Au
(10.1)
© Springer International Publishing AG, part of Springer Nature 2018 
https://doi.org/10.1007/978-3-319-91041-3_
565
10
P. J. Olver, C. Shakiban, Applied Linear Algebra, Undergraduate Texts in Mathematics, 

566
10 Dynamics
consisting of n ﬁrst order linear ordinary diﬀerential equations in the n unknowns u(t) =
( u1(t), . . . , un(t) )T ∈Rn. In an autonomous system, the time variable does not appear
explicitly, and so the coeﬃcient matrix A, of size n × n, is a constant real† matrix. Non-
autonomous systems, in which A(t) is time-dependent, are considerably more diﬃcult to
analyze, and we refer the reader to a more advanced text such as [36].
As we saw in Section 8.1, a vector-valued exponential function
u(t) = eλt v,
in which λ is a constant scalar and v a constant vector, describes a solution to (10.1) if
and only if
Av = λv.
Hence, assuming v ̸= 0, the scalar λ must be an eigenvalue of A, and v the corresponding
eigenvector.
The resulting exponential function will be called an eigensolution of the
linear system. Since the system is linear and homogeneous, linear superposition allows us
to combine the basic eigensolutions to form more general solutions.
If the coeﬃcient matrix A is complete (diagonalizable), then, by deﬁnition, its eigen-
vectors v1, . . . , vn form a basis. The corresponding eigensolutions
u1(t) = eλ1t v1,
. . .
un(t) = eλnt vn,
will form a basis for the solution space to the system. Hence, the general solution to a ﬁrst
order linear system with complete coeﬃcient matrix has the form
u(t) = c1u1(t) + · · · + cnun(t) = c1 eλ1t v1 + · · · + cn eλnt vn,
(10.2)
where c1, . . . , cn are constants, which are uniquely prescribed by the initial conditions
u(t0) = u0.
(10.3)
This all follows from the basic existence and uniqueness theorem for ordinary diﬀerential
equations, which will be discussed shortly.
Example 10.1.
Let us solve the coupled pair of ordinary diﬀerential equations
du
dt = 3u + v,
dv
dt = u + 3v.
We ﬁrst write the system in matrix form (10.1) with unknown u(t) =

u(t)
v(t)

and coeﬃ-
cient matrix A =

3
1
1
3

. According to Example 8.5, the eigenvalues and eigenvectors of
A are
λ1 = 4,
v1 =

1
1

,
λ2 = 2,
v2 =

−1
1

.
Both eigenvalues are simple, and so A is a complete matrix. The resulting eigensolutions
u1(t) = e4t

1
1

=

e4t
e4t

,
u2(t) = e2t

−1
1

=

−e2t
e2t

,
†
Extending the solution techniques to complex systems with complex coeﬃcient matrices is
straightforward, but will not be treated here.

10.1 Basic Solution Techniques
567
form a basis of the solution space, and so the general solution is a linear combination
u(t) = c1 e4t

1
1

+ c2 e2t

−1
1

=

c1 e4t −c2 e2t
c1 e4t + c2 e2t

,
hence
u(t) = c1 e4t −c2 e2t,
v(t) = c1 e4t + c2 e2t,
in which c1, c2 are arbitrary constants.
The Phase Plane
As noted above, a wide variety of physical systems are modeled by second order ordinary
diﬀerential equations. Your ﬁrst course on ordinary diﬀerential equations, e.g., [7, 22],
covered the basic solution technique for constant coeﬃcient scalar equations, which we
quickly review in the context of an example.
Example 10.2.
To solve the homogeneous ordinary diﬀerential equation
d2u
dt2 + du
dt −6u = 0,
(10.4)
we begin with the exponential ansatz†
u(t) = eλt,
where the constant factor λ is to be determined. Substituting into the diﬀerential equation
leads immediately to the characteristic equation
λ2 + λ −6 = 0,
with roots
λ1 = 2,
λ2 = −3.
Therefore, e2t and e−3t are individual solutions. Since the equation is of second order,
Theorem 7.34 implies that they form a basis for the two-dimensional solution space, and
hence the general solution can be written as a linear combination
u(t) = c1 e2t + c2 e−3t,
(10.5)
where c1, c2 are arbitrary constants.
There is a standard trick to convert a second order equation
d2u
dt2 + α du
dt + β u = 0
(10.6)
into a ﬁrst order system. One introduces the so-called phase plane variables‡
u1 = u,
u2 =
u = du
dt .
(10.7)
Assuming α, β are constants, the phase plane variables satisfy
du1
dt = du
dt = u2,
du2
dt = d2u
dt2 = −β u −α du
dt = −β u1 −αu2.
†
See the footnote on p. 379 for an explanation of the term “ansatz”, a.k.a. “inspired guess”.
‡
We will often use dots as a shorthand notation for time derivatives.

568
10 Dynamics
Figure 10.1.
Phase Portrait of
u1 = u2,
u2 = 6u1 −u2.
In this manner, the second order equation (10.6) is converted into a ﬁrst order system
u = Au,
where
u(t) =

u1(t)
u2(t)

,
A =

0
1
−β
−α

.
(10.8)
Every solution u(t) to the second order equation yields a solution u(t) = (u(t),
u(t))T
to the ﬁrst order system (10.8), whose second component is merely its time derivative.
Conversely, if u(t) = ( u1(t), u2(t) )T is any solution to (10.8), then its ﬁrst component
u(t) = u1(t) deﬁnes a solution to the original scalar equation (10.6). We conclude that the
two are completely equivalent, in the sense that solving one will immediately resolve the
other.
The variables (u1, u2)T = (u,
u)T serve as coordinates in the phase plane R2.
The
solutions u(t) parameterize curves in the phase plane, known as the solution trajectories
or orbits. In particular, the equilibrium solution u(t) ≡0 remains ﬁxed at the origin,
and so its trajectory is a single point. Assuming β ̸= 0, every other solution describes
a genuine curve, whose tangent direction
u = du/dt at a point u is prescribed by the
right-hand side of the diﬀerential equation, namely
u = Au. The collection of all possible
solution trajectories is called the phase portrait of the system. An important fact is that, in
an autonomous ﬁrst order system, the phase plane trajectories never cross. This striking
property, which is also valid for nonlinear systems, is a consequence of the uniqueness
properties of solutions, [7, 36].
Thus, the phase portrait consists of a family of non-
intersecting curves that, when combined with the equilibrium points, ﬁll out the entire
phase plane. The direction of motion along a trajectory will be indicated graphically by
a small arrow; nearby trajectories are all traversed in the same direction. The one feature
that is not so easily pictured in the phase portrait is the continuously varying speed at
which the solution moves along its trajectory. Plotting this requires a more complicated
three-dimensional diagram using time as the third coordinate.
Example 10.2 (continued).
For the second order equation (10.4), the equivalent phase
plane system is
du
dt =

0
1
6
−1

u,
or, in full detail,
u1 = u2,
u2 = 6u1 −u2.
(10.9)
Our previous solution formula (10.5) implies that the solution to the phase plane system
(10.9) is given by
u1(t) = u(t) = c1 e2t + c2 e−3t,
u2(t) = du
dt = 2c1 e2t −3c2 e−3t,

10.1 Basic Solution Techniques
569
and hence
u(t) =

c1 e2t + c2 e−3t
2c1 e2t −3c2 e−3t

= c1

e2t
2e2t

+ c2

e−3t
−3e−3t

.
(10.10)
A sketch of the phase portrait, indicating several representative trajectories, appears in
Figure 10.1. The solutions with c2 = 0 go out to ∞along the two rays in the directions
( 1, 2 )T and ( −1, −2 )T , whereas those with c1 = 0 come in to the origin along the rays
in the directions ( 1, −3 )T and ( −1, 3 )T . All other non-equilibrium solutions move along
hyperbolic trajectories whose asymptotes, in forward and backward time, are one of these
four rays.
With some practice, one learns to understand the temporal behavior of the solution by
studying its phase plane trajectory. We will investigate the qualitative and quantitative
behavior of phase plane systems in depth in Section 10.3.
Exercises
10.1.1. Choose one or more of the following diﬀerential equations, and then: (a) Solve the
equation directly.
(b) Write down its phase plane equivalent, and the general solution
to the phase plane system. (c) Plot at least four representative trajectories to illustrate
the phase portrait.
(d) Choose two trajectories in your phase portrait and graph the
corresponding solution curves u(t). Explain in your own words how the orbit and the
solution graph are related. (i) u + 4u = 0, (ii) u −4u = 0, (iii) u + 2 u + u = 0,
(iv) u + 4 u + 3u = 0, (v) u −2 u + 10u = 0.
10.1.2.(a) Convert the third order equation d3u
dt3 + 3 d2u
dt2 + 4 du
dt + 12u = 0 into a ﬁrst order
system in three variables by setting u1 = u, u2 =
u, u3 = u.
(b) Solve the equation
directly, and then use this to write down the general solution to your ﬁrst order system.
(c) What is the dimension of the solution space?
10.1.3. Convert the second order coupled system of ordinary diﬀerential equations
u = a u + b v + cu + dv,
v = p u + q v + ru + sv,
into a ﬁrst order system involving four variables.
♦10.1.4.(a) Show that if u(t) solves u = Au, then its time reversal, deﬁned as v(t) = u(−t),
solves v = B v, where B = −A.
(b) Explain why the two systems have the same phase
portraits, but the direction of motion along the trajectories is reversed. (c) Apply time
reversal to the system(s) you derived in Exercise 10.1.1. (d) What is the eﬀect of time
reversal on the original second order equation?
♥10.1.5. A ﬁrst order linear system u = au + bv,
v = cu + dv, can be converted into a single
second order diﬀerential equation by the following device. Assuming that b ̸= 0, solve the
system for v and v in terms of u and u. Then diﬀerentiate your equation for v with respect
to t, and eliminate v from the resulting pair of equations. The result is a second order
ordinary diﬀerential equation for u(t). (a) Write out the second order equation in terms
of the coeﬃcients a, b, c, d of the ﬁrst order system. (b) Show that there is a one-to-one
correspondence between solutions of the system and solutions of the scalar diﬀerential
equation. (c)
Use this method to solve the following linear systems, and sketch the
resulting phase portraits. (i)
u = v, v = −u, (ii)
u = 2u + 5v, v = −u, (iii)
u = 4u −v,
v = 6u −3v, (iv)
u = u + v, v = u −v, (v)
u = v, v = 0.
(d) Show how to obtain
a second order equation satisﬁed by v(t) by an analogous device. Are the second order
equations for u and for v the same?
(e) Discuss how you might proceed if b = 0.

570
10 Dynamics
10.1.6.(a) Show that if u(t) solves u = Au, then v(t) = u(2t) solves v = B v, where B = 2A .
(b) How are the solution trajectories of the two systems related?
♦10.1.7. Let A be a constant n × n matrix. Let u(t) be a solution to the system du
dt = Au.
(a) Show that its derivatives dku
dtk
for k = 1, 2, . . . , are also solutions.
(b) Show that dku
dtk = Aku.
10.1.8. True or false: Each solution to a phase plane system moves at a constant speed along
its trajectory.
10.1.9. True or false: The phase plane trajectories (10.10) for ( c1, c2 )T ̸= 0 are hyperbolas.
♠10.1.10. Use a three-dimensional graphics package to plot solution curves ( t, u1(t), u2(t) )T of
the phase plane systems in Exercise 10.1.1. Discuss their shape and explain how they are
related to the phase plane trajectories.
Existence and Uniqueness
Before delving further into our subject, it will help to brieﬂy summarize the basic existence
and uniqueness theorems as they apply to linear systems of ordinary diﬀerential equations.
Even though we will study only the constant coeﬃcient case in detail, these results are
equally applicable to non-autonomous systems, and so — but only in this subsection —
we allow the coeﬃcient matrix to depend continuously on t. A key fact is that a system
of n ﬁrst order ordinary diﬀerential equations requires n initial conditions — one for each
variable — in order to uniquely specify its solution. More precisely:
Theorem 10.3. Let A(t) be an n×n matrix and f(t) an n-component column vector each
of whose entries is a continuous functions on the interval† a < t < b. Set an initial time
a < t0 < b and an initial vector b ∈Rn. Then the initial value problem
du
dt = A(t) u + f(t),
u(t0) = b,
(10.11)
admits a unique solution u(t) that is deﬁned for all a < t < b.
For completeness, we have included an inhomogeneous forcing term f(t) in the system.
We will not prove Theorem 10.3, which is a direct consequences of the more general exis-
tence and uniqueness theorem for nonlinear systems of ordinary diﬀerential equations. Full
details can be found in most texts on ordinary diﬀerential equations, including [7, 22, 36].
In the homogeneous case, when f(t) ≡0, uniqueness of solutions implies that the solution
with zero initial conditions, u(t0) = 0, is the trivial zero solution: u(t) ≡0 for all t. In
other words, if you start at an equilibrium, you remain there for all time. Moreover, you
can never arrive at equilibrium in a ﬁnite amount of time, since if u(t1) = 0, then, again
by uniqueness, u(t) ≡0 for all t < t1 (and ≥t1, too).
Uniqueness has another important consequence: linear independence of solutions needs
be checked only at a single point.
†
We allow a and b to be inﬁnite.

10.1 Basic Solution Techniques
571
Lemma 10.4. The solutions u1(t), . . . , uk(t) to a ﬁrst order homogeneous linear system
u = A(t) u are linearly independent if and only if their initial values u1(t0), . . . , uk(t0) are
linearly independent vectors in Rn.
Proof : If the solutions were linearly dependent, one could ﬁnd (constant) scalars c1, . . . , ck,
not all zero, such that
u(t) = c1 u1(t) + · · · + ck uk(t) ≡0.
(10.12)
The equation holds, in particular, at t = t0,
u(t0) = c1 u1(t0) + · · · + ck uk(t0) = 0.
(10.13)
This immediately proves linear dependence of the initial vectors.
Conversely, if the initial values u1(t0), . . ., uk(t0) are linearly dependent, then (10.13)
holds for some c1, . . . , ck, not all zero.
Linear superposition implies that the self-same
linear combination u(t) = c1u1(t) + · · · + ckuk(t) is a solution to the system, with zero
initial condition. By uniqueness, u(t) ≡0 for all t, and so (10.12) holds, proving linear
dependence of the solutions.
Q.E.D.
Warning.
This result is not true if the functions are not solutions to a ﬁrst order
linear system. For example, u1(t) =

1
t

,
u2(t) =

cos t
sin t

, are linearly independent
vector-valued functions, but, at time t = 0, the vectors u1(0) =

1
0

= u2(0) are linearly
dependent. Even worse, u1(t) =

1
t

, u2(t) =

t
t2

, deﬁne linearly dependent vectors
at every speciﬁed value of t. Nevertheless, as vector-valued functions, they are linearly
independent. (Why?) In view of Lemma 10.4, neither pair of vector-valued functions can
be solutions to a common ﬁrst order homogeneous linear system.
The next result tells us how many diﬀerent solutions are required in order to construct
the general solution by linear superposition.
Theorem 10.5. Let u1(t), . . . , un(t) be n linearly independent solutions to the homoge-
neous system of n ﬁrst order linear ordinary diﬀerential equations
u = A(t) u. Then the
general solution is a linear combination u(t) = c1u1(t) + · · · + cnun(t) depending on n
arbitrary constants c1, . . . , cn.
Proof : If we have n linearly independent solutions u1(t), . . . , un(t), then Lemma 10.4 im-
plies that, at the initial time t0, the vectors u1(t0), . . . , un(t0) are linearly independent, and
hence form a basis for Rn. This means that we can express an arbitrary initial condition
u(t0) = b = c1u1(t0) + · · · + cnun(t0)
as a linear combination of the initial vectors. Superposition and uniqueness of solutions
implies that the corresponding solution to the initial value problem (10.11) is given by the
same linear combination
u(t) = c1u1(t) + · · · + cnun(t).
We conclude that every solution to the ordinary diﬀerential equation can be written in the
prescribed form, which thus forms the general solution.
Q.E.D.

572
10 Dynamics
Complete Systems
Thus, given a system of n homogeneous linear diﬀerential equations
u = Au, the immediate
goal is to ﬁnd n linearly independent solutions. Each eigenvalue λ and eigenvector v of
its (constant) coeﬃcient matrix A leads to an exponential eigensolution u(t) = eλt v. The
eigensolutions will be linearly independent if and only if the eigenvectors are — this follows
directly from Lemma 10.4. Thus, if the n × n matrix admits an eigenvector basis, i.e., it
is complete, then we have the requisite number of solutions, and hence have solved the
diﬀerential equation.
Theorem 10.6. If the n × n matrix A is complete, then the general (complex) solution
to the autonomous linear system
u = Au is given by
u(t) = c1 eλ1t v1 + · · · + cn eλnt vn,
(10.14)
where v1, . . . , vn are the eigenvector basis, λ1, . . . , λn the corresponding eigenvalues. The
constants c1, . . . , cn are uniquely speciﬁed by the initial conditions u(t0) = b.
Proof : Since the eigenvectors are linearly independent, the eigensolutions deﬁne linearly
independent vectors u1(0) = v1, . . . , un(0) = vn at the initial time t = 0. Lemma 10.4
implies that the eigensolutions u1(t), . . . , un(t) are, indeed, linearly independent. Hence,
we know n linearly independent solutions, and the result is an immediate consequence of
Theorem 10.5.
Q.E.D.
Example 10.7.
Let us solve the initial value problem
u1 = −2u1 + u2,
u1(0) = 3,
u2 = 2u1 −3u2,
u2(0) = 0.
The coeﬃcient matrix is A =

−2
1
2
−3

. A straightforward computation produces its
eigenvalues and eigenvectors:
λ1 = −4,
v1 =

1
−2

,
λ2 = −1,
v2 =

1
1

.
Theorem 10.6 assures us that the corresponding eigensolutions
u1(t) = e−4t

1
−2

,
u2(t) = e−t

1
1

,
form a basis for the two-dimensional solution space. The general solution is an arbitrary
linear combination
u(t) =

u1(t)
u2(t)

= c1 e−4t

1
−2

+ c2 e−t

1
1

=

c1 e−4t + c2 e−t
−2c1 e−4t + c2 e−t

,
where c1, c2 are constant scalars. Once we have the general solution in hand, the ﬁnal step
is to determine the values of c1, c2 in order to satisfy the initial conditions. Evaluating the
solution at t = 0, we ﬁnd that we need to solve the linear system
u1(0) = c1 + c2 = 3,
u2(0) = −2c1 + c2 = 0,
for c1 = 1, c2 = 2. Thus, the (unique) solution to the initial value problem is
u1(t) = e−4t + 2 e−t,
u2(t) = −2 e−4t + 2 e−t.
(10.15)
Note that both components of the solution decay exponentially fast to 0 as t →∞.

10.1 Basic Solution Techniques
573
Example 10.8.
Consider the linear initial value problem
u1 = u1 + 2u2,
u2 = u2 −2u3,
u3 = 2u1 + 2u2 −u3.
u1(0) = 2,
u2(0) = −1,
u3(0) = −2.
The coeﬃcient matrix is A =
⎛
⎝
1
2
0
0
1
−2
2
2
−1
⎞
⎠. In Example 8.9, we computed its eigenvalues
and eigenvectors:
λ1 = −1,
λ2 = 1 + 2 i ,
λ3 = 1 −2 i ,
v1 =
⎛
⎝
−1
1
1
⎞
⎠,
v2 =
⎛
⎝
1
i
1
⎞
⎠,
v3 =
⎛
⎝
1
−i
1
⎞
⎠.
The corresponding eigensolutions to the system are
u1(t) = e−t
⎛
⎝
−1
1
1
⎞
⎠,
u2(t) = e(1+2 i )t
⎛
⎝
1
i
1
⎞
⎠,
u3(t) = e(1−2i)t
⎛
⎝
1
−i
1
⎞
⎠.
The ﬁrst solution is real, but the second and third, while perfectly valid solutions, are
complex-valued, and hence not as convenient to work with if, as in most applications, we
are ultimately after real functions. But, since the underlying linear system is real, the
general reality principle of Theorem 7.48 tells us that a complex solution can be broken up
into its real and imaginary parts, each of which is a real solution. Here, applying Euler’s
formula (3.92) to the complex exponential, we obtain
u2(t) = e(1+2i)t
⎛
⎝
1
i
1
⎞
⎠=

et cos 2t + i et sin 2t

⎛
⎝
1
i
1
⎞
⎠=
⎛
⎝
et cos 2t
−et sin 2t
et cos 2t
⎞
⎠+ i
⎛
⎝
et sin 2t
et cos 2t
et sin 2t
⎞
⎠.
The ﬁnal two vector-valued functions are independent real solutions, as you can readily
check. In this manner, we have produced three linearly independent real solutions
u1(t) =
⎛
⎝
−e−t
e−t
e−t
⎞
⎠,
u2(t) =
⎛
⎝
et cos 2t
−et sin 2t
et cos 2t
⎞
⎠,
u3(t) =
⎛
⎝
et sin 2t
et cos 2t
et sin 2t
⎞
⎠,
which, by Theorem 10.5, form a basis for the three-dimensional solution space to our
system. The general solution can be written as a linear combination:
u(t) = c1 u1(t) + c2 u2(t) + c3 u3(t) =
⎛
⎝
−c1 e−t + c2 et cos 2t + c3 et sin 2t
c1 e−t −c2 et sin 2t + c3 et cos 2t
c1 e−t + c2 et cos 2t + c3 et sin 2t
⎞
⎠.
The constants c1, c2, c3 are uniquely prescribed by imposing initial conditions. In our case,
the solution satisfying
u(0) =
⎛
⎝
−c1 + c2
c1 + c3
c1 + c2
⎞
⎠=
⎛
⎝
2
−1
−2
⎞
⎠
results in
c1 = −2,
c2 = 0,
c3 = 1.

574
10 Dynamics
Thus, the solution to the original initial value problem is
u(t) =
⎛
⎝
u1(t)
u2(t)
u3(t)
⎞
⎠=
⎛
⎝
2e−t + et sin 2t
−2e−t + et cos 2t
−2e−t + et sin 2t
⎞
⎠.
Incidentally, the third complex eigensolution also produces two real solutions, but these
reproduce the ones we have already listed, since it is the complex conjugate of the second
eigensolution, and so u3(t) = u2(t) −i u3(t). In general, when solving real systems, you
need to deal with only one eigenvalue from each complex conjugate pair to construct a
complete system of real solutions.
Exercises
10.1.11. Find the solution to the system of diﬀerential equations du
dt = 3u + 4v,
dv
dt = 4u −3v,
with initial conditions u(0) = 3 and v(0) = −2.
10.1.12. Find the general real solution to the following systems of diﬀerential equations:
(a)
u1 = u1 + 9u2,
u2 = u1 + 3u2;
(b)
x1 = 4x1 + 3x2,
x2 = 3x1 −4x2;
(c)
y1 = y1 −y2,
y2 = 2y1 + 3y2;
(d)
y1 = y2,
y2 = 3y1 + 2y3,
y3 = −y2;
(e)
x1 = 3x1 −8x2 + 2x3,
x2 = −x1 + 2x2 + 2x3,
x3 = x1 −4x2 + 2x3;
(f )
u1 = u1 −3u2 + 11u3,
u2 = 2u1 −6u2 + 16u3,
u3 = u1 −3u2 + 7u3.
10.1.13. Solve the following initial value problems: (a) du
dt =

0
2
2
0
	
u, u(1) =

1
0
	
;
(b) du
dt =

1
−2
−2
1
	
u, u(0) =

−2
4
	
; (c) du
dt =

1
2
−1
1
	
u, u(0) =

1
0
	
;
(d) du
dt =
⎛
⎜
⎝
−1
3
−3
2
2
−7
0
3
−4
⎞
⎟
⎠u, u(0) =
⎛
⎜
⎝
1
0
0
⎞
⎟
⎠; (e) du
dt =
⎛
⎜
⎝
2
1
−6
−1
0
4
0
−1
−2
⎞
⎟
⎠u, u(π) =
⎛
⎜
⎝
2
−1
−1
⎞
⎟
⎠;
(f ) du
dt =
⎛
⎜
⎜
⎜
⎝
0
0
1
0
0
0
0
2
1
0
0
0
0
2
0
0
⎞
⎟
⎟
⎟
⎠u, u(2) =
⎛
⎜
⎜
⎜
⎝
1
0
0
1
⎞
⎟
⎟
⎟
⎠; (g) du
dt =
⎛
⎜
⎜
⎜
⎝
2
1
−1
0
−3
−2
0
1
0
0
1
−2
0
0
1
−1
⎞
⎟
⎟
⎟
⎠u, u(0) =
⎛
⎜
⎜
⎜
⎝
1
−1
2
1
⎞
⎟
⎟
⎟
⎠.
10.1.14.(a) Find the solution to the system
dx
dt = −x + y,
dy
dt = −x −y, that has initial
conditions x(0) = 1, y(0) = 0. (b) Sketch a phase portrait of the system that shows several
typical solution trajectories, including the solution you found in part (a). Clearly indicate
the direction of increasing t on your curves.
10.1.15. A planar steady-state ﬂuid ﬂow has velocity vector ﬁeld v = ( 2x −3y, x −y )T
at position x = ( x, y )T . The corresponding ﬂuid motion is described by the diﬀerential
equation dx
dt = v. A ﬂoating object starts out at the point ( 1, 1 )T . Find its position
after one time unit.
10.1.16. A steady-state ﬂuid ﬂow has velocity vector ﬁeld v = ( −2y, 2x, z )T at position
x = ( x, y, z )T . Describe the motion of the ﬂuid particles as governed by the diﬀerential
equation dx
dt = v.

10.1 Basic Solution Techniques
575
10.1.17. Solve the initial value problem du
dt =

−6
1
1
−6
	
u, u(0) =

1
2
	
. Explain how
orthogonality can help.
10.1.18.(a) Find the eigenvalues and eigenvectors of K =
⎛
⎜
⎝
1
−1
0
−1
2
−1
0
−1
1
⎞
⎟
⎠.
(b) Verify that the eigenvectors are mutually orthogonal. (c) Based on part (a), is K
positive deﬁnite, positive semi-deﬁnite, or indeﬁnite? (d) Solve the initial value problem
du
dt = K u, u(0) =
⎛
⎜
⎝
1
2
−1
⎞
⎟
⎠, using orthogonality to simplify the computations.
10.1.19. Demonstrate that one can also solve the initial value problem in Example 10.8 by
writing the solution as a complex linear combination of the complex eigensolutions, and
then using the initial conditions to specify the coeﬃcients.
10.1.20. Determine whether the following vector-valued functions are linearly dependent or
linearly independent:
(a)

1
t
	
,

−t
1
	
, (b)

1 + t
t
	
,

1 −t2
t −t2
	
, (c)

1
t
	
,

t
2
	
,

−t
t
	
, (d)

e−t
−et
	
,

−e−t
et
	
,
(e)

e2t cos 3t
−e2t sin 3t
	
,

e2t sin 3t
e2t cos 3t
	
, (f )

cos 3t
sin 3t
	
,

sin 3t
cos 3t
	
, (g)
⎛
⎜
⎝
1
t
1 −t
⎞
⎟
⎠,
⎛
⎜
⎝
0
−2
2
⎞
⎟
⎠,
⎛
⎜
⎝
3
1 + 3t
2 −3t
⎞
⎟
⎠,
(h)
⎛
⎜
⎝
et
−et
et
⎞
⎟
⎠,
⎛
⎜
⎝
et
et
−et
⎞
⎟
⎠,
⎛
⎜
⎝
−et
et
et
⎞
⎟
⎠, (i)
⎛
⎜
⎝
et
tet
t2 et
⎞
⎟
⎠,
⎛
⎜
⎝
t2 et
et
tet
⎞
⎟
⎠,
⎛
⎜
⎝
tet
t2 et
et
⎞
⎟
⎠,
⎛
⎜
⎝
et
et
et
⎞
⎟
⎠.
♦10.1.21. Let A be a constant matrix. Suppose u(t) solves the initial value problem u = Au,
u(0) = b. Prove that the solution to the initial value problem u = Au, u(t0) = b, is equal
to u(t) = u(t −t0). How are the solution trajectories related?
10.1.22. Suppose u(t) and u(t) both solve the linear system u = Au. (a) Suppose they have
the same value u(t1) = u(t1) at any one time t1. Show that they are, in fact, the same
solution: u(t) = u(t) for all t. (b) What happens if u(t1) = u(t2) for some t1 ̸= t2?
Hint: See Exercise 10.1.21.
10.1.23. Prove that the general solution to a linear system u = Λu with diagonal coeﬃcient
matrix Λ = diag (λ1, . . . , λn) is given by u(t) = (c1 eλ1 t, . . . , cn eλn t)T .
10.1.24. Show that if u(t) is a solution to u = Au, and S is a constant, nonsingular matrix of
the same size as A, then v(t) = S u(t) solves the linear system v = B v, where B = S AS−1
is similar to A.
♦10.1.25. (i) Combine Exercises 10.1.23–24 to show that if A = S ΛS−1 is diagonalizable, then
the solution to u = Au can be written as u(t) = S (c1 eλ1 t, . . . , cn eλn t)T , where λ1, . . . , λn
are its eigenvalues and S = ( v1 v2 . . . vn ) is the corresponding matrix of eigenvectors.
(ii) Write the general solution to the systems in Exercise 10.1.13 in this form.
The General Case
Summarizing the preceding subsection, if the coeﬃcient matrix of a homogeneous, au-
tonomous ﬁrst order linear system is complete, then the eigensolutions form a (complex)
basis for the solution space. Assuming the coeﬃcient matrix is real, one obtains a real
basis by taking the real and imaginary parts of each complex conjugate pair of solutions.
In the incomplete cases, the formulas for the basis solutions are a little more intricate, and

576
10 Dynamics
involve polynomials as well as (complex) exponentials. Readers who did not cover Sec-
tion 8.6 are advised to skip ahead to Section 10.2; only Theorem 10.13, which summarizes
the key features, will be used in the sequel.
Example 10.9.
The simplest incomplete case arises as the phase plane equivalent of a
scalar ordinary diﬀerential equation whose characteristic equation has a repeated root. For
example, to directly solve the second order equation
d2u
dt2 −2 du
dt + u = 0,
(10.16)
we substitute the usual exponential ansatz u = eλt, leading to the characteristicequation
λ2 −2λ + 1 = 0.
There is only one double root, λ = 1, and hence, up to scalar multiple, only one ex-
ponential solution u1(t) = et.
For a scalar ordinary diﬀerential equation, the second,
“missing”,solution is obtained by simply multiplying the ﬁrst by t, so that u2(t) = t et. As
a result, the general solution to (10.16) is
u(t) = c1 u1(t) + c2 u2(t) = c1 et + c2 t et.
As in (10.8), the equivalent phase plane system is
du
dt =

0
1
−1
2

u,
where
u(t) =

u(t)
u(t)

.
Note that the coeﬃcient matrix is incomplete — it has λ = 1 as a double eigenvalue,
but only one independent eigenvector, namely v = ( 1, 1 )T. The two linearly independent
solutions to the phase plane system can be constructed from the two solutions to the scalar
equation. Thus,
u1(t) =

et
et

,
u2(t) =

t et
t et + et

form a basis for the two-dimensional solution space. The ﬁrst is an eigensolution, while the
second includes an additional polynomial factor. Observe that, in contrast to the scalar
case, the second solution u2 is not obtained from the ﬁrst by merely multiplying by t.
In general, the eigenvectors of an incomplete matrix fail to form a basis, and, as noted
in Section 8.6, can be extended to a Jordan basis. Thus, the key step is to describe the
solutions associated with a Jordan chain, cf. Deﬁnition 8.47.
Lemma 10.10. Suppose w1, . . . , wk form a Jordan chain of length k for the eigenvalue
λ of the matrix A. Then there are k linearly independent solutions to the corresponding
ﬁrst order system
u = Au having the form
u1(t) = eλtw1,
u2(t) = eλt(t w1 + w2),
u3(t) = eλt 1
2 t2 w1 + t w2 + w3

,
and, in general,
uj(t) = eλt
j

i=1
tj−i
(j −i)! wi,
1 ≤j ≤k.
(10.17)
The proof is by direct substitution of the formulas (10.17) into the diﬀerential equation,
invoking the deﬁning relations (8.46) of the Jordan chain as needed; details are left to the
reader. If λ is a complex eigenvalue, then the Jordan chain solutions (10.17) will involve

10.1 Basic Solution Techniques
577
complex exponentials. As usual, if A is a real matrix, they can be split into their real and
imaginary parts, which are independent real solutions.
Example 10.11.
The coeﬃcient matrix of the system
du
dt =
⎛
⎜
⎜
⎜
⎝
−1
0
1
0
0
−2
2
−4
1
1
−1
0
−3
0
0
−4
−1
3
1
0
4
0
2
−1
0
⎞
⎟
⎟
⎟
⎠u
is incomplete; it has only 2 linearly independent eigenvectors associated with the eigenval-
ues 1 and −2. Using the Jordan basis computed in Example 8.52, we produce the following
5 linearly independent solutions:
u1(t) = et v1,
u2(t) = et (tv1 + v2),
u3(t) = et 1
2 t2 v1 + tv2 + v3

,
u4(t) = e−2t v4,
u5(t) = e−2t (tv4 + v5),
or, explicitly,
⎛
⎜
⎜
⎜
⎝
0
0
0
−et
et
⎞
⎟
⎟
⎟
⎠,
⎛
⎜
⎜
⎜
⎝
0
et
0
−tet
(−1 + t)et
⎞
⎟
⎟
⎟
⎠,
⎛
⎜
⎜
⎜
⎜
⎝
0
tet
0

1 −1
2 t2 
et

−t + 1
2 t2 
et
⎞
⎟
⎟
⎟
⎟
⎠
,
⎛
⎜
⎜
⎜
⎝
−e−2t
e−2t
e−2t
−2e−2t
0
⎞
⎟
⎟
⎟
⎠,
⎛
⎜
⎜
⎜
⎝
−(1 + t)e−2t
te−2t
te−2t
−2(1 + t)e−2t
e−2t
⎞
⎟
⎟
⎟
⎠.
The ﬁrst three solutions are associated with the λ1 = 1 Jordan chain, the last two with
the λ2 = −2 chain. The eigensolutions are the pure exponentials u1(t), u4(t). The general
solution to the system is an arbitrary linear combination of these ﬁve basis solutions.
Proposition 10.12. Let A be an n × n matrix. Then the Jordan chain solutions (10.17)
constructed from a Jordan basis of A form a basis for the n-dimensional solution space for
the corresponding linear system
u = Au.
The proof of linear independence of the Jordan chain solutions follows, via Lemma 10.4,
from the linear independence of the Jordan basis vectors, which are their initial values.
Important qualitative features can be readily gleaned from the algebraic structure of the
solution formulas (10.17). The following result describes the principal classes of solutions
of homogeneous autonomous linear systems of ordinary diﬀerential equations.
Theorem 10.13. Let A be a real n × n matrix. Every real solution to the linear system
u = Au is a linear combination of n linearly independent solutions appearing in the
following four classes:
(1) If λ is a complete real eigenvalue of multiplicity m, then there exist m linearly inde-
pendent solutions of the form
uk(t) = eλt vk,
k = 1, . . ., m,
where v1, . . . , vm are linearly independent eigenvectors.

578
10 Dynamics
(2) If λ± = μ ± i ν form a pair of complete complex conjugate eigenvalues of multiplicity
m, then there exist 2m linearly independent real solutions of the forms
uk(t) = eμt
cos(ν t) xk −sin(ν t) yk

,
uk(t) = eμt
sin(ν t) xk + cos(ν t) yk

,
k = 1, . . . , m,
where vk = xk ± i yk are the associated complex conjugate eigenvectors.
(3) If λ is an incomplete real eigenvalue of multiplicity m and r is the dimension of the
eigenspace Vλ, then there exist m linearly independent solutions of the form
uk(t) = eλt pk(t),
k = 1, . . ., m,
where pk(t) is a vector of polynomials of degree ≤m −r.
(4) If λ± = μ± i ν form a pair of incomplete complex conjugate eigenvalues of multiplicity
m, and r is the common dimension of the two eigenspaces, then there exist 2m
linearly independent real solutions
uk(t) = eμt 
cos(ν t) pk(t) −sin(ν t) qk(t)

,
uk(t) = eμt 
sin(ν t) pk(t) + cos(ν t) qk(t)

,
k = 1, . . ., m,
where pk(t), qk(t) are vectors of polynomials of degree ≤m −r, whose detailed
structure can be gleaned from Lemma 10.10.
As a result, every real solution to a homogeneous linear system of ordinary diﬀerential
equations is a vector-valued function whose entries are linear combinations of functions of
the particular form tk eμt cos ν t and tk eμt sin ν t, i.e., sums of products of exponentials,
trigonometric functions, and polynomials. The exponents μ are the real parts of the eigen-
values of the coeﬃcient matrix; the trigonometric frequencies ν are the imaginary parts of
the eigenvalues; nonconstant polynomials appear only if the matrix is incomplete.
Exercises
10.1.26. Find the general solution to the linear system du
dt = Au for the following incomplete
coeﬃcient matrices: (a)

2
1
0
2
	
, (b)

2
−1
9
−4
	
, (c)

−1
−1
4
−5
	
,
(d)
⎛
⎜
⎝
4
−1
−3
−2
1
2
5
−1
−4
⎞
⎟
⎠, (e)
⎛
⎜
⎝
−3
1
0
1
−3
−1
0
1
−3
⎞
⎟
⎠, (f )
⎛
⎜
⎜
⎜
⎝
3
1
1
1
0
−1
0
1
0
0
3
1
0
0
0
−1
⎞
⎟
⎟
⎟
⎠, (g)
⎛
⎜
⎜
⎜
⎝
0
1
1
0
−1
0
0
1
0
0
0
1
0
0
−1
0
⎞
⎟
⎟
⎟
⎠.
10.1.27. Find a ﬁrst order system of ordinary diﬀerential equations that has the indicated
vector-valued function as a solution:
(a)

et + e2t
2et
	
, (b)

e−t cos 3t
−3e−t sin 3t
	
, (c)

1
t −1
	
,
(d)

sin 2t −cos 2t
sin 2t + 3 cos 2t
	
, (e)
⎛
⎜
⎝
e2t
e−3t
e2t −e−3t
⎞
⎟
⎠, (f )
⎛
⎜
⎝
sin t
cos t
1
⎞
⎟
⎠, (g)
⎛
⎜
⎝
t
1 −t2
1 + t
⎞
⎟
⎠, (h)
⎛
⎜
⎝
et sin t
2et cos t
et sin t
⎞
⎟
⎠.
10.1.28. Which sets of functions in Exercise 10.1.20 can be solutions to a common ﬁrst order,
homogeneous, constant coeﬃcient linear system of ordinary diﬀerential equations? If so,
ﬁnd a system they satisfy; if not, explain why not.

10.2 Stability of Linear Systems
579
10.1.29. Solve the third order equation d3u
dt3 + 3 d2u
dt2 + 4 du
dt + 12u = 0 by converting it into a
ﬁrst order system. Compare your answer with what you found in Exercise 10.1.2.
10.1.30. Solve the second order coupled system of ordinary diﬀerential equations u = u + u −v,
v = v −u + v, by converting it into a ﬁrst order system involving four variables.
10.1.31. Suppose that u(t) ∈Rn is a polynomial solution to the constant coeﬃcient linear
system u = Au. What is the maximal possible degree of u(t)? What can you say about A
when u(t) has maximal degree?
♦10.1.32.(a) Under the assumption that u1, . . . , uk form a Jordan chain for the coeﬃcient
matrix A, prove that the functions (10.17) are solutions to the system u = Au.
(b) Prove that they are linearly independent.
10.2 Stability of Linear Systems
With the general solution formulas in hand, we are now ready to study the qualitative
features of ﬁrst order linear dynamical systems. Our primary focus will be on stability
properties of the equilibrium solution(s).
A solution to an autonomous system of ﬁrst
order ordinary diﬀerential equations
u = f(u) is called an equilibrium solution if it re-
mains constant for all t, so u(t) ≡u⋆. Since its derivative vanishes, this implies that the
equilibrium point u⋆satisﬁes f(u⋆) = 0. In particular, for a homogeneous linear system
u = Au, the origin u⋆≡0 is always an equilibrium point, meaning that a solution that
starts out at 0 remains there. The complete set of equilibrium solutions consists of all
points u⋆∈ker A in the kernel of the coeﬃcient matrix, and so the set of equilibrium
solutions forms a subspace — indeed, an invariant subspace — of the conﬁguration space.
In physical applications, the stability properties of equilibrium solutions is of crucial
importance; see the discussion at the beginning of Chapter 5. In general, an equilibrium
point is stable if every solution that starts out nearby stays nearby. An equilibrium is
called asymptotically stable if the nearby solutions converge to it as time increases. The
formal mathematical deﬁnitions are as follows.
Deﬁnition 10.14. An equilibrium solution u⋆to an autonomous system of ﬁrst order
ordinary diﬀerential equations
u = f(u) is called
• stable if for every suﬃciently small ε > 0, there exists a δ > 0 such that every solution
u(t) having initial conditions within distance δ > ∥u(t0) −u⋆∥of the equilibrium
remains within distance ε > ∥u(t) −u⋆∥for all t ≥t0.
• asymptotically stable if it is stable and, in addition, there exists ε0 > 0 such that
whenever ∥u(t0) −u⋆∥< ε0, then u(t) →u⋆as t →∞.
Thus, although solutions nearby a stable equilibrium point may drift slightly farther
away, they must remain relatively close. In the case of asymptotic stability, they will even-
tually return to equilibrium. An equilibrium point is called globally stable if the stability
condition holds for all ε > 0. It is called globally asymptotically stable if every solution
converges to the equilibrium point: u(t) →u⋆as t →∞.
In the case of a linear system, local (asymptotic) stability implies global (asymptotic)
stability. This is because, by linearity, if u(t) is a solution, then so is the scalar multiple
cu(t) for all c ∈R, and hence every solution can be scaled to one that remains nearby the

580
10 Dynamics
Figure 10.2.
The Left Half–Plane.
equilibrium point. We will henceforth omit the redundant term “global” when discussing
the stability of a linear system. We will also focus our attention on the particular equilib-
rium solution u⋆= 0.
Remark. The stability and asymptotic stability of an equilibrium solution are independent
of the choice of norm in the deﬁnition (although this will aﬀect the dependence of δ on ε).
This follows from the equivalence of norms described in Theorem 3.17.
The starting point is a simple calculus lemma, whose proof is left to the reader.
Lemma 10.15. Let μ, ν be real and k ≥0. A function of the form
f(t) = tk eμt cos ν t
or
tk eμt sin ν t
(10.18)
will decay to zero for large t, so lim
t→∞f(t) = 0, if and only if μ < 0. The function remains
bounded, so | f(t) | ≤C for some constant C, for all t ≥0 if and only if either μ < 0, or
μ = 0 and k = 0.
Loosely put, exponential decay will always overwhelm polynomial growth, while the
trigonometric sine and cosine functions remain neutrally bounded. Now, in the solution
to our linear system, the functions (10.18) come from the eigenvalues λ = μ + i ν of the
coeﬃcient matrix. The lemma implies that the asymptotic behavior of the solutions, and
hence the stability of the system, depends on the sign of μ = Re λ. If μ < 0, then the
solutions decay to zero at an exponential rate as t →∞. If μ > 0, then the solutions
become unbounded as t →∞. In the borderline case μ = 0, the solutions remain bounded,
provided that they don’t involve any powers of t.
Thus, in order that the equilibrium zero solution be asymptotically stable, all the eigen-
values must satisfy μ = Re λ < 0. Or, stated another way, all eigenvalues must lie in the left
half-plane — the subset of the complex plane C to the left of the imaginary axis sketched in
Figure 10.2. In this manner, we have demonstrated the fundamental asymptotic stability
criterion† for linear systems.
†
This is not the same as the stability criterion for linear iterative systems, which requires that
the eigenvalues of the coeﬃcient matrix lie in the inside the unit circle, cf. Theorem 9.12.

10.2 Stability of Linear Systems
581
Theorem 10.16. A ﬁrst order autonomous homogeneous linear system of ordinary dif-
ferential equations
u = Au has an asymptotically stable zero solution if and only if all the
eigenvalues λ of its coeﬃcient matrix A lie in the left half-plane: Re λ < 0. If A has one
or more eigenvalues with positive real part, Re λ > 0, then the zero solution is unstable.
Example 10.17.
Consider the system
du
dt = 2u −6v + w,
dv
dt = 3u −3v −w,
dw
dt = 3u −v −3w.
The coeﬃcient matrix A =
⎛
⎝
2
−6
1
3
−3
−1
3
−1
−3
⎞
⎠is found to have eigenvalues
λ1 = −2,
λ2 = −1 + i
√
6 ,
λ3 = −1 −i
√
6 ,
with respective real parts −2, −1, −1. The Stability Theorem 10.16 implies that the equi-
librium solution u⋆≡v⋆≡w⋆≡0 is asymptotically stable. Indeed, every solution involves
the functions e−2t, e−t cos
√
6 t, and e−t sin
√
6 t, all of which decay to 0 at an exponential
rate. The latter two have the slowest decay rate, and so most solutions to the linear system
go to 0 in proportion to e−t, i.e., at an exponential rate determined by the least negative
real part.
The ﬁnal statement is a special case of the following general result, whose proof is left
to the reader.
Proposition 10.18. If u(t) is any solution to
u = Au, then ∥u(t)∥≤C eat for all
t ≥t0 and for all a > a⋆= max{ Re λ | λ is an eigenvalue of A }, where the constant C > 0
depends on the solution and choice of norm. If the eigenvalue(s) λ achieving the maximum,
Re λ = a⋆, are complete, then one can set a = a⋆.
Asymptotic stability implies that the solutions return to equilibrium; stability only
requires them to stay nearby. The appropriate eigenvalue criterion is readily established.
Theorem 10.19. A ﬁrst order linear, homogeneous, constant-coeﬃcient system of ordi-
nary diﬀerential equations (10.1) has a stable zero solution if and only if all its eigenvalues
satisfy Re λ ≤0, and, moreover, any eigenvalue lying on the imaginary axis, so Re λ = 0,
is complete, meaning that it has as many independent eigenvectors as its multiplicity.
Proof : The proof is the same as before, based on Theorem 10.13 and the decay properties
in Lemma 10.15. All the eigenvalues with negative real part lead to exponentially decaying
solutions — even if they are incomplete. If the coeﬃcient matrix has a complete zero eigen-
value, then the corresponding eigensolutions are all constant, and hence trivially bounded.
On the other hand, if 0 is an incomplete eigenvalue, then the associated Jordan chain so-
lutions involve non-constant polynomials, and become unbounded as t →±∞. Similarly,
if a purely imaginary eigenvalue is complete, then the associated solutions only involve
trigonometric functions, and hence remain bounded, whereas the solutions associated with
an incomplete purely imaginary eigenvalue contain polynomials in t multiplying sines and
cosines, and hence cannot remain bounded.
Q.E.D.
A particularly important class of systems consists of the linear gradient ﬂows
du
dt = −Ku,
(10.19)

582
10 Dynamics
in which K is a symmetric, positive deﬁnite matrix. According to Theorem 8.35, all the
eigenvalues of K are real and positive, and so the eigenvalues of the negative deﬁnite
coeﬃcient matrix −K for the gradient ﬂow system (10.19) are real and negative. Applying
Theorem 10.16, we conclude that the zero solution to any gradient ﬂow system (10.19) with
negative deﬁnite coeﬃcient matrix −K is asymptotically stable. If the coeﬃcient matrix
is negative semi-deﬁnite, the the equilibrium solutions are stable, since the eigenvalues are
necessarily complete.
Example 10.20.
On applying the test we learned in Chapter 3, the matrix K =

1
1
1
5

is seen to be positive deﬁnite. The associated gradient ﬂow is
du
dt = −u −v,
dv
dt = −u −5v.
(10.20)
The eigenvalues and eigenvectors of −K =

−1
−1
−1
−5

are
λ1 = −3 +
√
5 ,
v1 =

1
2 −
√
5

,
λ2 = −3 −
√
5 ,
v2 =

1
2 +
√
5

.
Therefore, the general solution to the system is
u(t) = c1 e(−3+
√
5 )t

1
2 −
√
5

+ c2 e(−3−
√
5 )t

1
2 +
√
5

,
or, in components,
u(t) = c1 e(−3+
√
5 )t + c2 e(−3−
√
5 )t,
v(t) = c1 (2 −
√
5) e(−3+
√
5 )t + c2 (2 +
√
5) e(−3−
√
5 )t.
All solutions tend to zero as t →∞at the exponential rate prescribed by the least negative
eigenvalue, which is −3 +
√
5 ≃−.7639. This conﬁrms the asymptotic stability of the
gradient ﬂow.
The reason for the term “gradient ﬂow” is that the vector ﬁeld −Ku appearing on the
right-hand side of (10.19) is, in fact, the negative of the gradient of the quadratic function
q(u) = 1
2 uTKu = 1
2
n

i,j =1
kij ui uj,
so that
∇q(u) = Ku.
(10.21)
Thus, we can write (10.19) as
du
dt = −∇q(u).
(10.22)
For the particular system (10.20),
q(u, v) = 1
2 ( u
v )

1
1
1
5
 
u
v

= 1
2 u2 + uv + 5
2 v2,
and so the gradient ﬂow is given by
du
dt = −∂q
∂u = −u −v,
dv
dt = −∂q
∂v = −u −5v.
As you learn in multivariable calculus, [2, 78], the gradient ∇q of a function q points
in the direction of its steepest increase, while its negative −∇q points in the direction of

10.2 Stability of Linear Systems
583
steepest decrease. Thus, the solutions to the gradient ﬂow system (10.22) will decrease q(u)
as rapidly as possible, tending to its minimum at u⋆= 0. For instance, if q(u, v) represents
the height of a hill at position (u, v), then the solutions to (10.22) are the paths of steepest
descent followed by, say, water ﬂowing down the hill (provided we ignore inertial eﬀects). In
physical applications, the quadratic function (10.21) often represents the potential energy
in the system, and the gradient ﬂow models the natural behavior of systems that seek to
minimize their energy as rapidly as possible.
Example 10.21.
Another extremely important class of dynamical systems comprises
the Hamiltonian systems, ﬁrst developed by the nineteenth-century Irish mathematician
William Rowan Hamilton, who also discovered quaternions, developed in Exercise 7.2.23.
In particular, a planar Hamiltonian system takes the form
du
dt = ∂H
∂v ,
dv
dt = −∂H
∂u ,
(10.23)
where H(u, v) is known as the Hamiltonian function. If
H(u, v) = 1
2 a u2 + b u v + 1
2 c v2
(10.24)
is a quadratic form, then the corresponding Hamiltonian system
u = b u + c v,
v = −a u −b v,
(10.25)
is homogeneous linear, with coeﬃcient matrix A =

b
c
−a
−b

. The associated charac-
teristic equation is
det(A −λ I ) = λ2 + (ac −b2) = 0.
If H is positive or negative deﬁnite, then ac −b2 > 0, and so the eigenvalues are purely
imaginary: λ = ± i
√
ac −b2 and complete, since they are simple. Thus, the stability
criterion of Theorem 10.19 holds, and we conclude that planar Hamiltonian systems with
a deﬁnite Hamiltonian function are stable. On the other hand, if H is indeﬁnite, then the
coeﬃcient matrix has one positive and one negative eigenvalue, and hence the Hamiltonian
system is unstable.
In physical applications, the Hamiltonian function H(u, v) represents the energy of the
system. According to Exercise 10.2.22, the Hamiltonian energy function is automatically
conserved, meaning that it is constant on every solution: H(u(t), v(t)) = constant. This
means that the solutions move along its level sets; in the stable cases these are bounded
ellipses, whereas in the unstable cases they are unbounded hyperbolas.
Remark. The equations of classical mechanics, such as motion of masses (sun, planets,
comets, etc.) under gravitational attraction, can all be formulated as Hamiltonian systems,
[31]. Moreover, the Hamiltonian formulation is a crucial ﬁrst step in the physical process
of quantizing the classical mechanical equations to determine the quantum mechanical
equations of motion, [54].
Exercises
10.2.1. Classify the following systems according to whether the origin is (i) asymptotically
stable, (ii) stable, or (iii) unstable:
(a) du
dt = −2u−v,
dv
dt = u−2v; (b)
du
dt = 2u−5v,
dv
dt = u −v;
(c) du
dt = −u −2v,
dv
dt = 2u −5v;
(d) du
dt = −2v,
dv
dt = 8u;

584
10 Dynamics
(e) du
dt = −2u −v + w,
dv
dt = −u −2v + w,
dw
dt = −3u −3v + 2w;
(f ) du
dt = −u −2v,
dv
dt = 6u + 3v −4w,
dw
dt = 4u −3w;
(g) du
dt = 2u −v + 3w,
dv
dt = u −v + w,
dw
dt = −4u + v −5w;
(h) du
dt = u + v −w,
dv
dt = −2u −3v + 3w,
dw
dt = −v + w.
10.2.2. Write out the formula for the general real solution to the system in Example 10.17 and
verify its stability.
10.2.3. Write out and solve the gradient ﬂow system corresponding to the following quadratic
forms: (a) u2 + v2, (b) uv, (c) 4u2 −2uv + v2, (d) 2u2 −uv −2uw + 2v2 −v w + 2w2.
10.2.4. Write out and solve the Hamiltonian systems corresponding to the ﬁrst three quadratic
forms in Exercise 10.2.3. Which of them are stable?
10.2.5. Which of the following 2 × 2 systems are gradient ﬂows? Which are Hamiltonian
systems? In each case, discuss the stability of the zero solution.
(a)
u = −2u + v,
v = u −2v,
(b)
u = u −2v,
v = −2u + v,
(c)
u = v,
v = u,
(d)
u = −v,
v = u,
(e)
u = −u −2v,
v = −2u −v.
10.2.6.(a) Show that the matrix A =
⎛
⎜
⎜
⎜
⎝
0
1
1
0
−1
0
0
1
0
0
0
1
0
0
−1
0
⎞
⎟
⎟
⎟
⎠has λ = ± i as incomplete complex
conjugate eigenvalues. (b) Find the general real solution to u = Au.
(c) Explain the behavior of a typical solution. Why is the zero solution not stable?
10.2.7. Let A be a real 3 × 3 matrix, and assume that the linear system u = Au has a periodic
solution of period P. Prove that every periodic solution of the system has period P. What
other types of solutions can there be? Is the zero solution necessarily stable?
10.2.8. Are the conclusions of Exercise 10.2.7 valid when A is a 4 × 4 matrix?
10.2.9. Let A be a real 5 × 5 matrix, and assume that A has eigenvalues i , −i , −2, −1 (and no
others). Is the zero solution to the linear system u = Au necessarily stable? Explain. Does
your answer change if A is 6 × 6?
10.2.10. Prove that if A is strictly diagonally dominant and each diagonal entry is negative,
then the zero equilibrium solution to the linear system of ordinary diﬀerential equations
u = Au is asymptotically stable.
10.2.11. True or false: The system u = −Hn u, where Hn is the n × n Hilbert matrix (1.72), is
asymptotically stable.
10.2.12. True or false: If the zero solution of the linear system of diﬀerential equations
u = Au is asymptotically stable, so is the zero solution of the linear iterative system
u(k+1) = Au(k) with the same coeﬃcient matrix.
10.2.13. Let u(t) solve u = Au. Let v(t) = u(−t) be its time reversal.
(a) Write down the linear system v = B v satisﬁed by v(t). Then classify the following
statements as true or false. As always, explain your answers. (b) If u = Au is
asymptotically stable, then v = B v is unstable. (c) If u = Au is unstable, then
v = B v is asymptotically stable. (d) If u = Au is stable, then v = B v is stable.
10.2.14. True or false: (a) If tr A > 0, then the system u = Au is unstable.
(b) If det A > 0, then the system u = Au is unstable.

10.3 Two-Dimensional Systems
585
10.2.15. True or false: If K is positive semi-deﬁnite, then the zero solution to u=−Ku is stable.
10.2.16. True or false: If A is a symmetric matrix, then the system u = −A2 u has an
asymptotically stable equilibrium solution.
10.2.17. Consider the diﬀerential equation u = −Ku, where K is positive semi-deﬁnite.
(a) Find all equilibrium solutions.
(b) Prove that all non-constant solutions decay
exponentially fast to some equilibrium. What is the decay rate? (c) Is the origin stable,
asymptotically stable, or unstable? (d) Prove that, as t →∞, the solution u(t) converges
to the orthogonal projection of its initial vector a = u(0) onto ker K.
10.2.18. Suppose that u(t) satisﬁes the gradient ﬂow system (10.22).
(a) Prove that d
dt q(u) = −∥Ku∥2.
(b) Explain why if u(t) is any nonconstant solution to the gradient ﬂow, then q(u(t)) is a
strictly decreasing function of t, thus quantifying how fast a gradient ﬂow decreases energy.
10.2.19. Let H(u, v) = au2 + b uv + cv2 be a quadratic function. (a) Prove that the non-
equilibrium trajectories of the associated Hamiltonian system and those of the gradient ﬂow
are mutually orthogonal, i.e., they always intersect at right angles.
(b) Verify this result
for the particular quadratic functions (i) u2 + 3v2, (ii) uv, by drawing representative
trajectories of both systems on the same graph.
10.2.20. True or false: If the Hamiltonian system for H(u, v) is stable, then the corresponding
gradient ﬂow u = −∇H is stable.
10.2.21. True or false: A nonzero linear 2 × 2 gradient ﬂow cannot be a Hamiltonian ﬂow.
♥10.2.22.
The law of conservation of energy states that the energy in a Hamiltonian system
is constant on solutions. (a) Prove that if u(t) satisﬁes the Hamiltonian system (10.23),
then H(u(t)) = c is a constant, and hence solutions u(t) move along the level sets of
the Hamiltonian or energy function. Explain how the value of c is determined by the
initial conditions.
(b) Plot the level curves of the particular Hamiltonian function
H(u, v) = u2 −2uv + 2v2 and verify that they coincide with the solution trajectories.
10.2.23. True or false: A nonzero linear 2 × 2 gradient ﬂow cannot be a Hamiltonian system.
10.2.24.(a) Explain how to solve the inhomogeneous system du
dt = Au + b when b is a
constant vector belonging to img A. Hint: Look at v(t) = u(t) −u⋆where u⋆is an
equilibrium solution. (b) Use your method to solve
(i) du
dt = u −3v + 1,
dv
dt = −u −v,
(ii) du
dt = 4v + 2,
dv
dt = −u −3.
♦10.2.25. Prove Lemma 10.15.
♦10.2.26. Prove Proposition 10.18.
10.3 Two-Dimensional Systems
The two-dimensional case is particularly instructive, since it is relatively easy to analyze,
but already manifests most of the key phenomena to be found in higher dimensions. More-
over, the solutions can be easily pictured and their behavior understood through their
phase portraits. In this section, we will present a complete classiﬁcation of the possible
qualitative behaviors of real, planar linear dynamical systems.
Setting u(t) = (u(t), v(t))T , a ﬁrst order planar homogeneous linear system has the
explicit form
du
dt = au + bv,
dv
dt = cu + dv,
(10.26)

586
10 Dynamics
where A =

a
b
c
d

is the (constant) coeﬃcient matrix. As in Section 10.1, we will refer
to the uv-plane as the phase plane. In particular, the phase plane equivalents (10.8) of
second order scalar equations form a subclass thereof.
According to (8.21), the characteristic equation for the given 2 × 2 matrix is
det(A −λ I ) = λ2 −τ λ + δ = 0,
(10.27)
where
τ = tr A = a + d,
δ = det A = ad −bc,
(10.28)
are, respectively, the trace and the determinant of A.
The eigenvalues, and hence the
nature of the solutions, are almost entirely determined by these two quantities. The sign
of the discriminant
Δ = τ 2 −4 δ = (tr A)2 −4 det A = (a −d)2 + 4bc
(10.29)
determines whether the eigenvalues
λ± = τ ±
√
Δ
2
(10.30)
are real or complex, and thereby plays a key role in the classiﬁcation.
Let us summarize the diﬀerent possibilities as distinguished by their qualitative behav-
ior. Each category will be illustrated by a representative phase portrait, which displays
several typical solution trajectories in the phase plane.
A complete portrait gallery of
planar systems can be found in Figure 10.3.
Distinct Real Eigenvalues
The coeﬃcient matrix A has two distinct real eigenvalues λ1 < λ2 if and only if the
discriminant is positive: Δ > 0. In this case, the solutions take the exponential form
u(t) = c1 eλ1 t v1 + c2 eλ2 t v2,
(10.31)
where v1, v2 are the eigenvectors and c1, c2 are arbitrary constants, to be determined by
the initial conditions. Let Vk = { c vk | c ∈R } for k = 1, 2, denote the two “eigenlines”,
i.e., the one-dimensional eigenspaces.
The asymptotic behavior of the solutions is governed by the eigenvalues. There are ﬁve
qualitatively diﬀerent cases, depending upon their signs. These are listed by their descrip-
tive name, followed by the required conditions on the discriminant, trace, and determinant
of the coeﬃcient matrix that serve to prescribe the form of the eigenvalues.
Ia. Stable Node:
Δ > 0,
tr A < 0,
det A > 0.
If λ1 < λ2 < 0 are both negative, then 0 is an asymptotically stable node. The solutions
all tend to 0 as t →∞. Since the ﬁrst exponential eλ1 t decreases much faster than the
second eλ2 t, the ﬁrst term in the solution (10.31) will soon become negligible, and hence
u(t) ≈c2 eλ2 t v2 when t is large, provided c2 ̸= 0. Such solutions will arrive at the origin
along curves tangent to the eigenline V2, including those with c1 = 0, which move directly
along the eigenline. On the other hand, the solutions with c2 = 0 come in to the origin
along the eigenline V1, at a faster rate. Conversely, as t →−∞, all solutions become
unbounded: ∥u(t)∥→∞. In this case, the ﬁrst exponential grows faster than the second,
and so u(t) ≈c1 eλ1 t v1 for t ≪0. In other words, as they escape to ∞, the solution

10.3 Two-Dimensional Systems
587
trajectories become more and more parallel to the eigenline V1 — except for those with
c1 = 0, which remain on the eigenline V2.
Ib. Saddle Point:
Δ > 0,
det A < 0.
If λ1 < 0 < λ2, then 0 is an unstable saddle point. Solutions (10.31) with c2 = 0 start
out on the eigenline V1 and go in to 0 as t →∞, while solutions with c1 = 0 start on V2
and go to 0 as t →−∞. All other solutions become unbounded at both large positive and
large negative times. As t →+∞, they asymptotically approach the unstable eigenline V2,
while as t →−∞, they approach the stable eigenline V1.
Ic. Unstable Node:
Δ > 0,
tr A > 0,
det A > 0.
If the eigenvalues 0 < λ1 < λ2 are both positive, then 0 is an unstable node. The phase
portrait is the same as that of a stable node, but the solution trajectories are traversed
in the opposite direction.
Time reversal t →−t will convert an unstable node into a
stable node and vice versa. Thus, in the unstable case, the solutions all tend to the origin
as t →−∞and become unbounded as t →∞.
Except for the eigensolutions, they
asymptotically approach V1 as t →−∞, and become parallel to V2 as t →∞.
Id. Stable Line:
Δ > 0,
tr A < 0,
det A = 0.
If λ1 < λ2 = 0, then every point on the eigenline V2 associated with the zero eigenvalue
is a stable equilibrium point. The other solutions move along straight lines parallel to V1,
asymptotically approaching one of the equilibrium points on V2 as t →∞. On the other
hand, as t →−∞, all solutions except those sitting still on the eigenline move oﬀto ∞.
Ie. Unstable Line:
Δ > 0,
tr A > 0,
det A = 0.
This is merely the time reversal of a stable line. If 0 = λ1 < λ2, then every point on
the eigenline V1 is an equilibrium. The other solutions moves oﬀto ∞along straight lines
parallel to V2 as t →∞, and tend to an equilibrium on V1 as t →−∞.
Complex Conjugate Eigenvalues
The coeﬃcient matrix A has two complex conjugate eigenvalues
λ± = μ ± i ν,
where
μ = 1
2 τ = 1
2 tr A,
ν =
√
−Δ,
if and only if its discriminant is negative: Δ < 0. In this case, the real solutions can be
written in the phase–amplitude form
u(t) = r eμt [ cos(ν t −σ) w −sin(ν t −σ) z ] ,
(10.32)
where w ± i z are the complex eigenvectors. As noted in Exercise 8.3.12, the real vectors
w, z are always linearly independent.
The amplitude r and phase shift σ are uniquely
prescribed by the initial conditions. There are three subcases, depending upon the sign of
the real part μ, or, equivalently, the sign of the trace of A.
IIa. Stable Focus:
Δ < 0,
tr A < 0.
If μ < 0, then 0 is an asymptotically stable focus. As t →∞, the solutions all spiral
in to the origin at an exponential rate eμt with a common “frequency” ν — meaning it
takes time 2π/ν for the solution to spiral once around the origin†. On the other hand, as
†
But keep in mind that these solutions are not periodic. Thus, 2π/ν is the time interval between
successive intersections of the solution and a ﬁxed ray emanating from the origin, e.g., the positive
x-axis.

588
10 Dynamics
t →−∞, the solutions spiral oﬀto ∞at the same exponential rate whilst maintaining
their overall frequency.
IIb. Center:
Δ < 0,
tr A = 0.
If μ = 0, meaning that the eigenvalues λ± = ± i ν are purely imaginary, then 0 is a
center. The solutions all move periodically around elliptical orbits, with common frequency
ν and hence period 2π/ν. In particular, solutions that start out near 0 stay nearby, and
hence a center is a stable, but not asymptotically stable, equilibrium.
IIc. Unstable Focus:
Δ < 0,
tr A > 0.
If μ > 0, then 0 is an unstable focus. The phase portrait is the time reversal of a stable
focus, with solutions having an unbounded spiral motion as t →∞, and spiraling in to the
origin as t →−∞, again at an exponential rate eμt with a common “frequency” ν.
Incomplete Double Real Eigenvalue
The coeﬃcient matrix has a double real eigenvalue λ = 1
2 τ = 1
2 tr A if and only if the
discriminant vanishes: Δ = 0.
The formula for the solutions depends on whether the
eigenvalue λ is complete. If λ is an incomplete eigenvalue, admitting only one independent
eigenvector v, then the solutions are no longer given by simple exponentials. The general
solution formula is
u(t) = (c1 + c2 t)eλt v + c2 eλt w,
(10.33)
where (A −λ I )w = v, and so v, w form a Jordan chain for the coeﬃcient matrix. We let
V = {c v} denote the eigenline associated with the genuine eigenvector v.
IIIa. Stable Improper Node:
Δ = 0,
tr A < 0,
A ̸= λ I .
If λ < 0 then 0 is an asymptotically stable improper node. Since teλt is larger than eλt
for t > 1, when c2 ̸= 0, the solutions u(t) ≈c2 teλt tend to 0 as t →∞along a curve
that is tangent to the eigenline V , while the eigensolutions with c2 = 0 move in to the
origin along the eigenline. Similarly, as t →−∞, the solutions go oﬀto ∞in the opposite
direction from their approach, becoming more and more parallel to the same eigenline.
IIIb. Linear Motion:
Δ = 0,
tr A = 0,
A ̸= λ I .
If λ = 0, then every point on the eigenline V is an unstable equilibrium point. Every
other solution is a linear polynomial in t, and so moves along a straight line parallel to V ,
going oﬀto ∞in either direction.
IIIc. Unstable Improper Node:
Δ = 0,
tr A > 0,
A ̸= λ I .
If λ > 0, then 0 is an unstable improper node. The phase portrait is the time reversal
of the stable improper node. Solutions go oﬀto ∞as t increases, becoming progressively
more parallel to the eigenline, and tend to the origin tangent to the eigenline as t →−∞.
Complete Double Real Eigenvalue
In this case, every vector in R2 is an eigenvector, and so the real solutions take the form
u(t) = eλt v, where v is an arbitrary constant vector. In fact, this case occurs if and only
if A = λ I is a scalar multiple of the identity matrix.

10.3 Two-Dimensional Systems
589
Ia. Stable Node
Ib. Saddle Point
Ic. Unstable Node
IIa. Stable Focus
IIb. Center
IIc. Unstable Focus
IIIa. Stable Improper Node
IIIb. Linear Motion
IIIc. Unstable Improper Node
IVa. Stable Star
IVc. Unstable Star
Id. Stable Line
Ie. Unstable Line
Figure 10.3.
Phase Portraits.

590
10 Dynamics
 	

 	
 


 

		 

 

 

		
Figure 10.4.
Stability Regions for Two–Dimensional Linear Systems.
IVa. Stable Star:
A = λ I ,
λ < 0.
If λ < 0, then 0 is an asymptotically stable star. The solution trajectories are the rays
coming in to the origin, and the solutions go to 0 at a common exponential rate eλt as
t →∞.
IVb. Trivial:
A = O.
If λ = 0, then the only possibility is A = O. Now every solution is constant and every
point is a (stable) equilibrium point. Nothing happens! This is the only case not pictured
in Figure 10.3.
IVc. Unstable Star:
A = λ I ,
λ > 0.
If λ > 0, then 0 is an unstable star. The phase portrait is the time reversal of the stable
star, and so the solutions move out along rays as t →∞at an exponential rate eλt, while
tending to 0 as t →−∞.
Figure 10.4 summarizes the diﬀerent possibilities, as prescribed by the trace and deter-
minant of the coeﬃcient matrix. The horizontal axis indicates the value of τ = tr A, while
the vertical axis refers to δ = det A. Points on the parabola τ 2 = 4 δ represent the cases
with vanishing discriminant Δ = 0, and correspond to either stars or improper nodes —
except for the origin, which is either linear motion or trivial. All the asymptotically stable
cases lie in the shaded upper left quadrant where tr A < 0 and det A > 0. The borderline
points are either stable centers, when tr A = 0, det A > 0, or stable lines, when tr A < 0,
det A = 0, or the origin, which may or may not be stable depending upon whether A is
the zero matrix or not. All other values for the trace and determinant result in unstable
equilibria. Summarizing:

10.3 Two-Dimensional Systems
591
Proposition 10.22. Let τ, δ denote, respectively, the trace and determinant of the coeﬃ-
cient matrix A of a homogeneous, linear, autonomous planar system of ﬁrst order ordinary
diﬀerential equations. Then the system is
(i) asymptotically stable if and only if δ > 0 and τ < 0;
(ii) stable if and only if δ ≥0, τ ≤0, and, if δ = τ = 0, also A = O.
Remark. Time reversal t →−t changes the sign of the coeﬃcient matrix A →−A,
and hence the sign of its trace, τ →−τ, while the determinant δ = det A = det(−A) is
unchanged. Thus, the eﬀect is to reﬂect Figure 10.4 through the vertical axis, interchanging
the stable nodes and spirals with their unstable counterparts, while taking saddle points
to saddle points.
In physical applications, the parameters occurring in the dynamical system are usu-
ally not known exactly, and so the real dynamics may, in fact, be governed by a slight
perturbation of the mathematical model. Thus, it is important to know which systems
are structurally stable, meaning that their basic qualitative features are preserved under
suﬃciently small changes in the coeﬃcients. Now, a small perturbation will alter the co-
eﬃcient matrix slightly, and hence shift its trace and determinant by a comparably small
amount. The net eﬀect is to slightly perturb its eigenvalues. Therefore, the question of
structural stability reduces to whether the eigenvalues have moved suﬃciently far to send
the system into a diﬀerent stability regime. Asymptotically stable systems remain asymp-
totically stable since a suﬃciently small perturbation will not alter the signs of the real
parts of its eigenvalues.
For a similar reason, unstable systems remain unstable under
small perturbations. On the other hand, a borderline stable system — either a center or
the trivial system — might become either asymptotically stable or unstable, even under a
minuscule perturbation. Such results continue to hold, at least locally, even under suitably
small nonlinear perturbations, and thereby lie at the foundations of nonlinear dynamics.
Structural stability requires a bit more, since the overall phase portrait should not
signiﬁcantly change. A system in any of the open regions in the Stability Figure 10.4, i.e.,
a stable or unstable focus, a stable or unstable node, or a saddle point, is structurally stable,
whereas a system that lies on the parabola τ 2 = 4 δ, or the horizontal axis, or the positive
vertical axis, e.g., an improper node, a stable line, etc., is not, since a small perturbation
can easily kick it into either of the adjoining regions. Thus, structural stability requires
that the eigenvalues be distinct, λi ̸= λj, and have non-zero real part: Re λ ̸= 0. This ﬁnal
result remains valid for linear systems in higher dimensions, [36, 41]. See also [69, 90]
and the brief remarks on page 525 concerning the perturbation theory of eigenvalues, in
which Wilkinson’s spectral condition number quantiﬁes to what extent the eigenvalues are
aﬀected by a perturbation of the coeﬃcient matrix.
Exercises
10.3.1. For each the following: (a) Write the system as u = Au. (b) Find the eigenvalues and
eigenvectors of A. (c) Find the general real solution of the system. (d) Draw the phase
portrait, indicating its type and stability properties: (i)
u1 = −u2,
u2 = 9u1,
(ii)
u1 = 2u1 −3u2,
u2 = u1 −u2,
(iii)
u1 = 3u1 −2u2,
u2 = 2u1 −2u2.
10.3.2. For each of the following systems
(i)
u =

2
−1
3
−2
	
u,
(ii)
u =

1
−1
5
−3
	
u,
(iii)
u =

−3
5/2
−5/2
2
	
u:

592
10 Dynamics
(a) Find the general real solution.
(b) Using the solution formulas obtained in part (a),
plot several trajectories of each system. On your graphs, identify the eigenlines (if relevant),
and the direction of increasing t on the trajectories. (c) Write down the type and stability
properties of the system.
10.3.3. Classify the following systems, and sketch their phase portraits.
(a)
du
dt = −u + 4v,
dv
dt = u −2v.
(b)
du
dt = −2u + v,
dv
dt = u −4v.
(c)
du
dt = 5u + 4v,
dv
dt = u + 2v.
(d)
du
dt = −3u −2v,
dv
dt = 3u + 2v.
♦10.3.4. Justify the solution formulas (10.32) and (10.33).
10.3.5. Sketch the phase portrait for the following systems:
(a)
u1 = u1 −3u2,
u2 = −3u1 + u2.
(b)
u1 = u1 −4u2,
u2 = u1 −u2.
(c)
u1 = u1 + u2,
u2 = 4u1 −2u2.
(d)
u1 = u1 + u2,
u2 = u2.
(e)
u1 = 3
2 u1 + 5
2 u2,
u2 = −5
2 u1 + 3
2 u2.
10.3.6. Which of the 14 possible two-dimensional phase portraits can occur for the phase plane
equivalent (10.8) of a second order scalar ordinary diﬀerential equation?
10.3.7. Which of the 14 possible two-dimensional phase portraits can occur
(a) for a linear gradient ﬂow (10.19)?
(b) for a linear Hamiltonian system (10.25)?
10.3.8.(a) Solve the initial value problem
du
dt =

−1
2
−1
−3
	
u, u(0) =

1
3
	
.
(b) Sketch a picture of your solution curve u(t), indicating the direction of motion.
(c) Is the origin (i) stable? (ii) asymptotically stable? (iii) unstable? (iv) none of these?
Justify your answer.
10.4 Matrix Exponentials
So far, our focus has been on vector-valued solutions u(t) to homogeneous linear systems
of ordinary diﬀerential equations
du
dt = Au.
(10.34)
An evident, and, in fact, useful, generalization is to look for matrix solutions. Speciﬁcally,
we seek a matrix-valued function U(t) that satisﬁes the corresponding matrix diﬀerential
equation
dU
dt = A U(t).
(10.35)
As with vectors, we compute the derivative of U(t) by diﬀerentiating its individual entries.
If A is an n× n matrix, compatibility of matrix multiplication requires that U(t) be of size
n × k for some k. Since matrix multiplication acts column-wise, the individual columns of
the matrix solution U(t) = ( u1(t) . . . uk(t) ) must solve the original vector system (10.34).
Thus, a matrix solution is merely a convenient way of collecting together several diﬀerent
vector solutions. The most important case is that in which U(t) is a square matrix, of size
n × n, and so consists of n vector solutions to the system.
Example 10.23.
According to Example 10.7, the vector-valued functions
u1(t) =

e−4t
−2e−4t

,
u2(t) =

e−t
e−t

,

10.4 Matrix Exponentials
593
are both solutions to the linear system
du
dt =

−2
1
2
−3

u.
They can be combined to form the matrix solution
U(t) =

e−4t
e−t
−2e−4t
e−t

satisfying
dU
dt =

−2
1
2
−3

U.
Indeed, by direct calculation
dU
dt =

−4e−4t
−e−t
8e−4t
−e−t

=

−2
1
2
−3
 
e−4t
e−t
−2e−4t
e−t

=

−2
1
2
−3

U.
The existence and uniqueness theorems are readily adapted to matrix diﬀerential equa-
tions, and imply that there is a unique matrix solution to the system (10.35) that has
initial conditions
U(t0) = B,
(10.36)
where B is an n × k matrix. Note that the jth column uj(t) of the matrix solution U(t)
satisﬁes the initial value problem
duj
dt = Auj,
uj(t0) = bj,
where bj denotes the jth column of B.
In the scalar case, the solution to the particular initial value problem
du
dt = a u,
u(0) = 1,
is the ordinary exponential function u(t) = eta. Knowing this, we can write down the
solution for a more general initial condition
u(t0) = b
as
u(t) = b e(t−t0)a.
Let us formulate an analogous initial value problem for a linear system. Recall that, for
matrices, the role of the multiplicative unit 1 is played by the identity matrix I . This
inspires the following deﬁnition.
Deﬁnition 10.24. Let A be a square n × n matrix. The matrix exponential
U(t) = etA = exp(tA)
(10.37)
is the unique n × n matrix solution to the initial value problem
dU
dt = A U,
U(0) = I .
(10.38)
In particular, one computes eA by setting t = 1 in the matrix exponential etA. The
matrix exponential turns out to enjoy almost all the properties you might expect from its
scalar counterpart. First, it is deﬁned for all t ∈R, and all n × n matrices, both real and
complex. We can rewrite the deﬁning properties (10.38) in the more suggestive form
d
dt etA = A etA,
e0 A = I .
(10.39)

594
10 Dynamics
As in the scalar case, once we know the matrix exponential, we are in a position to solve
the general initial value problem.
Lemma 10.25. Let A be an n × n matrix. For any n × k matrix B, the solution to the
initial value problem
dU
dt = AU,
U(t0) = B,
is
U(t) = e(t−t0)A B.
(10.40)
Proof : Since B is a constant matrix,
dU
dt = d
dt

e(t−t0)A B

= A e(t−t0)A B = A U,
where we applied the chain rule for diﬀerentiation and the ﬁrst property (10.39). Thus,
U(t) is indeed a matrix solution to the system. Moreover, by the second property in (10.39),
U(0) = e0 A B = I B = B
has the correct initial conditions.
Q.E.D.
Remark. The computation used in the proof is a special instance of the general Leibniz
rule
d
dt

M(t) N(t)

= dM(t)
dt
N(t) + M(t) dN(t)
dt
(10.41)
for the derivative of the product of (compatible) matrix-valued functions M(t) and N(t).
The reader is asked to prove this formula in Exercise 10.4.21.
In particular, the solution to the original vector initial value problem
du
dt = Au,
u(t0) = b,
can be written in terms of the matrix exponential:
u(t) = e(t−t0) A b.
(10.42)
Thus, the matrix exponential provides us with an alternative formula for the solution of
autonomous homogeneous ﬁrst order linear systems, providing us with valuable new insight.
The next step is to ﬁnd an algorithm for computing the matrix exponential.
The
solution formula (10.40) gives a hint. Suppose U(t) is any n × n matrix solution. Then,
by uniqueness, U(t) = etA U(0), and hence, provided that U(0) is a nonsingular matrix,
etA = U(t) U(0)−1,
(10.43)
since e0A = U(0) U(0)−1 = I , as required. Thus, to construct the exponential of an n × n
matrix A, you ﬁrst need to ﬁnd a basis of n linearly independent solutions u1(t), . . . , un(t)
to the linear system
u = Au using the eigenvalues and eigenvectors, or, in the incomplete
case, the Jordan chains. The resulting n × n matrix solution U(t) =

u1(t) . . . un(t)

is
then used to produce etA via formula (10.43).
Example 10.26.
For the matrix A =

−2
1
2
−3

considered in Example 10.23, we
already constructed the nonsingular matrix solution U(t) =

e−4t
e−t
−2e−4t
e−t

. Therefore,

10.4 Matrix Exponentials
595
by (10.43), its matrix exponential is
etA = U(t) U(0)−1
=

e−4t
e−t
−2e−4t
e−t
 
1
1
−2
1
−1
=

1
3 e−4t + 2
3 e−t
−1
3 e−4t + 1
3 e−t
−2
3 e−4t + 2
3 e−t
2
3 e−4t + 1
3 e−t

.
In particular, we obtain eA = exp A by setting t = 1 in this formula:
exp

−2
1
2
−3

=

1
3 e−4 + 2
3 e−1
−1
3 e−4 + 1
3 e−1
−2
3 e−4 + 2
3 e−1
2
3 e−4 + 1
3 e−1

.
Observe that the matrix exponential is not obtained by exponentiating the individual
matrix entries.
To solve the initial value problem
du
dt =

−2
1
2
−3

u,
u(0) = b =

3
0

,
we appeal to formula (10.40), whence
u(t) = etA b =

1
3 e−4t + 2
3 e−t
−1
3 e−4t + 1
3 e−t
−2
3 e−4t + 2
3 e−t
2
3 e−4t + 1
3 e−t

3
0

=

e−4t + 2 e−t
−2 e−4t + 2 e−t

.
This reproduces our earlier solution (10.15).
Example 10.27.
Suppose A =

−1
−2
2
−1

. Its characteristic equation
det(A −λ I ) = λ2 + 2λ + 5 = 0
has roots
λ = −1 ± 2 i ,
which are thus the eigenvalues. The corresponding eigenvectors are v =

± i
1

, leading
to the complex conjugate solutions
u1(t) =

i e(−1+2 i ) t
e(−1+2 i ) t

,
u2(t) =

−i e(−1−2 i ) t
e(−1−2 i ) t

.
We assemble them to form the (complex) matrix solution
U(t) =

i e(−1+2 i ) t
−i e(−1−2 i ) t
e(−1+2 i ) t
e(−1−2 i ) t

.
The corresponding matrix exponential is, therefore,
etA = U(t) U(0)−1 =

i e(−1+2 i ) t
−i e(−1−2 i ) t
e(−1+2 i ) t
e(−1−2 i ) t
 
i
−i
1
1
−1
=
⎛
⎜
⎝
e(−1+2 i ) t + e(−1−2 i ) t
2
−e(−1+2 i ) t + e(−1−2 i ) t
2 i
e(−1+2 i ) t −e(−1−2 i ) t
2 i
e(−1+2 i ) t + e(−1−2 i ) t
2
⎞
⎟
⎠=

e−t cos 2t
−e−t sin 2t
e−t sin 2t
e−t cos 2t

.
Note that the ﬁnal expression for the matrix exponential is real, as it must be, since A is
a real matrix. (See Exercise 10.4.19.) Also note that it wasn’t necessary to ﬁnd the real
solutions to construct the matrix exponential — although this would also have worked and

596
10 Dynamics
yielded the same result. Indeed, the two columns of etA form a basis for the space of (real)
solutions to the linear system
u = Au.
Let us ﬁnish by listing some further important properties of the matrix exponential, all
of which are direct analogues of the usual scalar exponential function. Proofs are relegated
to the exercises. First, the multiplicative property says that
e(s+t) A = esA etA,
for all
s, t ∈R.
(10.44)
In particular, if we set s = −t, the left hand side of (10.44) reduces to the identity matrix,
in accordance with the second identity in (10.39), and hence
e−tAetA = I ,
and hence
e−tA =

etA −1.
(10.45)
As a consequence, for any A and any t ∈R, the exponential etA is a nonsingular matrix.
Warning. In general,
et (A+B) ̸= etA etB.
(10.46)
Indeed, according to Proposition 10.30, the left- and right-hand sides of (10.46) are equal
for all t if and only if AB = B A — that is, A and B are commuting matrices.
While the matrix exponential can be painful to compute, there is a simple formula for
its determinant in terms of the trace of the generating matrix.
Lemma 10.28. Let A be a square matrix. Then det etA = et tr A.
Proof : According to Exercise 10.4.26, if A has eigenvalues λ1, . . . , λn, then etA has eigen-
values etλ1, . . . , etλn. Moreover, using (8.26), its determinant, det etA, is the product of
its eigenvalues, and so
det etA = etλ1 etλ2 · · · etλn = et (λ1+λ2+ ··· +λn) = et tr A,
where, by (8.25), we identify the sum of the eigenvalues as the trace of A.
Q.E.D.
For instance, the matrix A =

−2
1
2
−3

considered above in Example 10.26 has
tr A = (−2) + (−3) = −5, and hence det etA = e−5t, as you can easily check.
Finally, we note that the standard exponential series is also valid for matrices:
etA =
∞

n=0
tn
n! An = I + t A + t2
2 A2 + t3
6 A3 + · · · .
(10.47)
To prove that the series converges, we use the matrix norm convergence criterion in Exercise
9.2.44(c).
Indeed, the corresponding series of matrix norms is bounded by the scalar
exponential series,
∥etA ∥≤
∞

n=0
&&&&
tn
n! An
&&&& =
∞

n=0
| t |n
n!
∥An∥≤
∞

n=0
| t |n
n!
∥A∥n = e| t | ∥A∥,
which converges for all t, [2, 78], thereby proving convergence. With this in hand, proving
that the exponential series satisﬁes the deﬁning initial value problem (10.39) is straight-
forward:
d
dt
∞

n=0
tn
n! An =
∞

n=1
tn−1
(n −1)! An =
∞

n=0
tn
n! An+1 = A
∞

n=0
tn
n! An.

10.4 Matrix Exponentials
597
Moreover, at t = 0, the sum collapses to the identity matrix:
I = e0A.
Thus, for-
mula (10.47) follows from the uniqueness of solutions to the matrix initial value problem.
Exercises
10.4.1. Find the exponentials etA of the following 2 × 2 matrices:
(a)

2
−1
4
−3
	
, (b)

0
1
1
0
	
, (c)

0
−1
1
0
	
, (d)

0
1
0
0
	
, (e)

−1
2
−5
5
	
, (f )

1
2
−2
−1
	
.
10.4.2. Determine the matrix exponential etA for the following matrices:
(a)
⎛
⎜
⎝
0
0
0
2
0
1
0
−1
0
⎞
⎟
⎠,
(b)
⎛
⎜
⎝
3
−1
0
−1
2
−1
0
−1
3
⎞
⎟
⎠,
(c)
⎛
⎜
⎝
−1
1
1
−2
−2
−2
1
−1
−1
⎞
⎟
⎠,
(d)
⎛
⎜
⎝
0
0
1
1
0
0
0
1
0
⎞
⎟
⎠.
10.4.3. Verify the determinant formula of Lemma 10.28 for the matrices in Exercises 10.4.1
and 10.4.2.
10.4.4. Solve the indicated initial value problems by ﬁrst exponentiating the coeﬃcient matrix
and then applying formula (10.42):
(a) du
dt =

0
−1
1
0
	
u,
u(0) =

1
−2
	
,
(b) du
dt =

3
−6
4
−7
	
u,
u(0) =

−1
1
	
,
(c) du
dt =
⎛
⎜
⎝
−9
−6
6
8
5
−6
−2
1
3
⎞
⎟
⎠u,
u(0) =
⎛
⎜
⎝
0
1
0
⎞
⎟
⎠.
10.4.5. Find eA when A =
(a)

5
−2
−2
5
	
, (b)

1
−2
1
1
	
, (c)

2
−1
4
−2
	
, (d)
⎛
⎜
⎝
1
0
0
0
−2
0
0
0
−5
⎞
⎟
⎠, (e)
⎛
⎜
⎝
0
1
−2
−1
0
2
2
−2
0
⎞
⎟
⎠.
10.4.6. Let A =

0
−2π
2π
0
	
. Show that eA = I .
10.4.7. What is etO where O is the n × n zero matrix?
10.4.8. Find all matrices A such that etA = O.
♦10.4.9. Explain in detail why the columns of etA form a basis for the solution space to the
system u = Au.
10.4.10. Let A be a 2 × 2 matrix such that tr A = 0 and δ =
√
det A > 0.
(a) Prove that eA = (cos δ) I + sin δ
δ
A. Hint: Use Exercise 8.2.52.
(b) Establish a similar formula when det A < 0. (c) What if det A = 0?
10.4.11. Show that the origin is an asymptotically stable equilibrium solution to u = Au if and
only if limt →∞etA = O.
10.4.12. Let A be a real square matrix and eA its exponential. Under what conditions does the
linear system u = eA u have an asymptotically stable equilibrium solution?
10.4.13. True or false:
(a) eA−1 = (eA)−1; (b) eA+A−1 = eA eA−1.
♦10.4.14. Prove formula (10.44). Hint: Fix s and prove that, as functions of t, both sides of the
equation deﬁne matrix solutions with the same initial conditions. Then use uniqueness.
10.4.15. Prove that A commutes with its exponential: A etA = etAA.
♦10.4.16.(a) Prove that the exponential of the transpose of a matrix is the transpose of its
exponential: etAT = (etA)T . (b) What does this imply about the solutions to the linear
systems u = Au and v = AT v?

598
10 Dynamics
♦10.4.17. Prove that if A = S B S−1 are similar matrices, then so are etA = S etB S−1.
10.4.18. Prove that et(A−λ I ) = e−tλ etA by showing that both sides are matrix solutions to
the same initial value problem.
♦10.4.19. Let A be a real matrix. (a) Explain why eA is a real matrix. (b) Prove that det eA > 0.
10.4.20. Show that tr A = 0 if and only if det etA = 1 for all t.
♦10.4.21. Justify the matrix Leibniz rule (10.41) using the formula for matrix multiplication.
10.4.22. Prove that if U(t) is any matrix solution to dU
dt = A U, then so is U(t) = U(t) C,
where C is any constant matrix (of compatible size).
♦10.4.23. Prove that if A =

B
O
O
C
	
is a block diagonal matrix, then so is etA =
⎛
⎝etB
O
O
etC
⎞
⎠.
♦10.4.24.(a)
Prove that if J0,n is an n × n Jordan block matrix with 0 diagonal entries,
cf. (8.49), then etJ0,n =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
1
t
t2
2
t3
6
. . .
tn
n!
0
1
t
t2
2
. . .
tn−1
(n −1)!
0
0
1
t
. . .
tn−2
(n −2)!
...
...
...
...
...
...
0
0
0
. . .
1
t
0
0
0
. . .
0
1
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
.
(b) Determine the exponential of a general Jordan block matrix Jλ,n. Hint: Use
Exercise 10.4.18. (c) Explain how you can use the Jordan canonical form to compute
the exponential of a matrix. Hint: Use Exercise 10.4.23.
♦10.4.25. Diagonalization provides an alternative method for computing the exponential of a
complete matrix. (a) First show that if D = diag (d1, . . . , dn) is a diagonal matrix, so is
etD = diag (etd1, . . . , etdn). (b) Second, using Exercise 10.4.17, prove that if A = S DS−1
is diagonalizable, so is etA = S etD S−1. (c) When possible, use diagonalization to compute
the exponentials of the matrices in Exercises 10.4.1–2.
♦10.4.26.(a) Prove that if λ is an eigenvalue of A, then et λ is an eigenvalue of etA. What is the
eigenvector? (b) Show that the eigenvalues have the same multiplicities.
Hint: Combine the Jordan canonical form (8.51) with Exercises 10.4.24 and 10.4.25.
♦10.4.27. Let A be a symmetric matrix with Spectral Decomposition
A = λ1 P1 + λ2 P2 + · · · + λk Pk,
as in (8.37). Prove that
etA = etλ1 P1 + etλ2 P2 + · · · + etλk Pk.
♦10.4.28.(a) Show that U(t) satisﬁes the matrix diﬀerential equation

U = U B if and only if
U(t) = C etB, where C = U(0). (b) If U(0) is nonsingular, then U(t) also satisﬁes a matrix
diﬀerential equation of the form

U = A U. Is A = B? Hint: Use Exercise 10.4.17.
10.4.29. True or false: The solution to the non-autonomous initial value problem
u = A(t)u, u(0) = b,
is
u(t) = exp
  t
0 A(s) ds
	
b.

10.4 Matrix Exponentials
599
♥10.4.30.(a) Suppose u1(t), . . . , un(t) are vector-valued functions whose values at each point t
are linearly independent vectors in Rn. Show that they form a basis for the solution space
of a homogeneous constant coeﬃcient linear system u = Au if and only if each duj/dt
is a linear combination of u1(t), . . . , un(t). Hint: Use Exercise 10.4.28. (b) Show that a
function u(t) belongs to the solution space of a homogeneous constant coeﬃcient linear
system u = Au if and only if dnu
dtn is a linear combination of u, du
dt , . . . , dn−1u
dtn−1 .
Hint: Use Exercise 10.1.7.
♥10.4.31. By a (natural) logarithm of a matrix B we mean a matrix A such that eA = B.
(a) Explain why only nonsingular matrices can have a logarithm.
(b) Comparing Exercises 10.4.6–7, explain why the matrix logarithm is not unique.
(c) Find all real logarithms of the 2 × 2 identity matrix I =

1
0
0
1
	
.
Hint: Use Exercise 10.4.26.
Applications in Geometry
Matrix exponentials are an eﬀective tool for understanding the linear transformations that
appear in geometry and group theory, [93], quantum mechanics, [54], computer graphics
and animation, [5, 12, 72], computer vision, [73], and the symmetry analysis of diﬀerential
equations, [13, 60]. We will only be able to scratch the surface of this important and active
area of contemporary mathematical research.
Let A be an n × n matrix. For each t ∈R, the corresponding exponential etA is itself
an n × n matrix and thus deﬁnes a linear transformation on the vector space Rn:
Lt[x] = etA x
for
x ∈Rn.
In this manner, each square matrix A generates a family of invertible linear transformations,
parameterized by t ∈R. The resulting linear transformations are not arbitrary, but are
subject the following three rules:
Lt ◦Ls = Lt+s = Ls ◦Lt,
L0 = I ,
L−t = L−1
t .
(10.48)
These are merely restatements of three of the basic matrix exponential properties listed
in (10.39, 44, 45). In particular, every transformation in the family commutes with every
other one.
In geometry, the family of transformations Lt = etA is said to form a one-parameter
group†, [60], with t the parameter, and the matrix A is referred to as its inﬁnitesimal
generator. Indeed, by the series formula (10.39) for the matrix exponential,
Lt[x] = etA x =

I + tA + 1
2 t2A2 + · · ·

x = x + t A x + 1
2 t2A2 x + · · · .
(10.49)
When t is small, we can truncate the exponential series and approximate the transformation
by the linear function
Ft[x] = ( I + tA) x = x + t A x
(10.50)
deﬁned by the inﬁnitesimal generator. We already made use of such approximations when
we discussed the rigid motions and mechanisms of structures in Chapter 6. As t varies, the
†
See also Exercise 4.3.24 for the general deﬁnition of a group.

600
10 Dynamics
group transformations (10.49) typically move a point x along a curved trajectory. Under
the ﬁrst order approximation (10.50), the point x moves along a straight line in the direction
b = A x — the tangent line to the curved trajectory. Thus, the inﬁnitesimal generator of
a one-parameter group prescribes the tangent line approximation to the nonlinear motion
prescribed by the group transformations.
Most of the linear transformations of interest in the above-mentioned applications arise
in this fashion. Let’s look brieﬂy at a few basic examples.
(a) When A =

0
1
0
0

, then etA =

1
t
0
1

represents a shearing transformation. The
group laws (10.48) imply that the composition of a shear of magnitude s and a shear
of magnitude t in the same direction is another shear of magnitude s + t.
(b) When A =

1
0
0
1

, then etA =

et
0
0
et

represents a uniform scaling transformation.
Composition and inverses of such scaling transformations are also scalings.
(c) When A =

1
0
0
−1

, then etA =

et
0
0
e−t

, which, for t > 0, represents a stretch
in the x direction and a contraction in the y direction.
(d) When A =

0
−1
1
0

, then etA =

cos t
−sin t
sin t
cos t

is the matrix for a plane rotation,
around the origin, by angle t. The group laws (10.48) say that the composition of a
rotation through angle s followed by a rotation through angle t is a rotation through
angle s+t, as previously noted in Example 7.12. Also, the inverse of a rotation through
angle t is a rotation through angle −t.
Observe that the inﬁnitesimal generator of this one-parameter group of plane rotations is
a 2 × 2 skew-symmetric matrix. This turns out to be a general fact: rotations in higher
dimensions are also generated by skew-symmetric matrices.
Lemma 10.29. If AT = −A is a skew-symmetric matrix, then, for all t ∈R, its matrix
exponential Q(t) = etA is a proper orthogonal matrix.
Proof : According to equation (10.45) and Exercise 10.4.16,
Q(t)−1 = e−tA = etAT =

etA T = Q(t)T ,
which proves orthogonality. Properness, det Q = +1, follows from Lemma 10.28 using the
fact that tr A = 0, since all the diagonal entries of a skew-symmetric matrix are 0. Q.E.D.
With some more work, it can be shown that every proper orthogonal matrix is the ex-
ponential of some skew-symmetric matrix, albeit not a unique one. Thus, the 1
2 n(n −1)–
dimensional vector space of n × n skew-symmetric matrices generates the group of rota-
tions in n-dimensional Euclidean space. In the three-dimensional case, the three matrices
Ax, Ay, Az listed below form a basis and serve to generate, respectively, the one-parameter
groups of counterclockwise rotations around the x-, y-, and z-axes:

10.4 Matrix Exponentials
601
Ax =
⎛
⎝
0
0
0
0
0
−1
0
1
0
⎞
⎠,
etAx =
⎛
⎝
1
0
0
0
cos t
−sin t
0
sin t
cos t
⎞
⎠,
Ay =
⎛
⎝
0
0
1
0
0
0
−1
0
0
⎞
⎠,
etAy =
⎛
⎝
cos t
0
sin t
0
1
0
−sin t
0
cos t
⎞
⎠,
Az =
⎛
⎝
0
−1
0
1
0
0
0
0
0
⎞
⎠,
etAz =
⎛
⎝
cos t
−sin t
0
sin t
cos t
0
0
0
1
⎞
⎠.
(10.51)
Since every other skew-symmetric matrix can be expressed as a linear combination of
Ax, Ay, and Az, every rotation can, in a sense, be generated by these three basic types.
This reconﬁrms our earlier observations concerning the number of rigid motions (rotations
and translations) experienced by an unattached structure; see Section 6.3 for details.
In the three-dimensional case, it can be shown that every non-zero skew-symmetric
3 × 3 matrix A is singular, with one-dimensional kernel. Let 0 ̸= v ∈ker A be the null
eigenvector. Then the matrix exponentials etA form the one-parameter group of rotations
around the axis deﬁned by v.
For instance, referring to (10.51), ker Ax is spanned by
e1 = ( 1, 0, 0 )T , reconﬁrming that it generates the rotations around the x-axis. Details can
be found in Exercise 10.4.38.
Noncommutativity of linear transformations is reﬂected in the noncommutativity of
their inﬁnitesimal generators. Recall, (1.12), that the commutator of two n × n matrices
A, B is
[ A, B ] = AB −B A.
(10.52)
Thus, A and B commute if and only if [ A, B ] = O. We use the exponential series (10.47)
to evaluate the commutator of the corresponding matrix exponentials:

etA, etB 
= etA etB −etB etA
=

I + tA + 1
2 t2A2 + · · ·

I + t B + 1
2 t2B2 + · · ·

−
−

I + t B + 1
2 t2B2 + · · ·

I + tA + 1
2 t2A2 + · · ·

= t2(A B −B A) + · · · = t2 [ A, B ] + · · · .
(10.53)
In particular, if the groups commute, then [ A, B ] = O. The converse is also true, since if
AB = B A then all terms in the two series commute, and hence the matrix exponentials
also commute.
Proposition 10.30. The matrix exponentials etA and etB commute for all t if and only
if the matrices A and B commute:
etA etB = etB etA = et (A+B)
provided
AB = B A.
(10.54)
In particular, the non-commutativity of three-dimensional rotations follows from the
non-commutativity of their inﬁnitesimal skew-symmetric generators.
For instance, the
commutator of the generators of rotations around the x- and y-axes is the generator of
rotations around the z-axis:

Ax, Ay

= Az, since
⎛
⎝
0
0
0
0
0
−1
0
1
0
⎞
⎠
⎛
⎝
0
0
1
0
0
0
−1
0
0
⎞
⎠−
⎛
⎝
0
0
1
0
0
0
−1
0
0
⎞
⎠
⎛
⎝
0
0
0
0
0
−1
0
1
0
⎞
⎠=
⎛
⎝
0
−1
0
1
0
0
0
0
0
⎞
⎠.

602
10 Dynamics
Hence, to a ﬁrst order (or, more correctly, second order) approximation, the diﬀerence
between x and y rotations is, interestingly, a z rotation.
Exercises
10.4.32. Find the one-parameter groups generated by the following matrices and interpret
geometrically: What are the trajectories? What are the ﬁxed points?
(a)

2
0
0
0
	
,
(b)

0
0
1
0
	
,
(c)

0
3
−3
0
	
,
(d)

0
−1
4
0
	
,
(e)

0
1
1
0
	
.
10.4.33. Write down the one-parameter groups generated by the following matrices and
interpret. What are the trajectories? What are the ﬁxed points?
(a)
⎛
⎜
⎝
2
0
0
0
1
0
0
0
0
⎞
⎟
⎠, (b)
⎛
⎜
⎝
0
0
1
0
0
0
0
0
0
⎞
⎟
⎠, (c)
⎛
⎜
⎝
0
0
−2
0
0
0
2
0
0
⎞
⎟
⎠, (d)
⎛
⎜
⎝
0
1
0
−1
0
0
0
0
1
⎞
⎟
⎠, (e)
⎛
⎜
⎝
0
0
1
0
0
0
1
0
0
⎞
⎟
⎠.
10.4.34.(a) Find the one-parameter group of rotations generated by the skew-symmetric matrix
A =
⎛
⎜
⎝
0
1
1
−1
0
−1
−1
1
0
⎞
⎟
⎠.
(b) As noted above, etA represents a family of rotations around a
ﬁxed axis in R3. What is the axis?
10.4.35. Choose two of the groups in Exercise 10.4.32 or 10.4.33, and determine whether or not
they commute by looking at their inﬁnitesimal generators. Then verify your conclusion by
directly computing the commutator of the corresponding matrix exponentials.
10.4.36.(a) Prove that the commutator of two upper triangular matrices is upper triangular.
(b) Prove that the commutator of two skew-symmetric matrices is skew symmetric.
(c) Is the commutator of two symmetric-matrices symmetric?
♦10.4.37. Prove that the Jacobi identity
[ [ A, B ], C ] + [ [ C, A ], B ] + [ [ B, C ], A ] = O
(10.55)
is valid for three n × n matrices A, B, C.
♥10.4.38. Let 0 ̸= v ∈R3. (a) Show that the cross product Lv[x] = v × x deﬁnes a
linear transformation on R3. (b) Find the 3 × 3 matrix representative Av of Lv and
show that it is skew-symmetric.
(c) Show that every non-zero skew-symmetric 3 × 3
matrix deﬁnes such a cross product map. (d) Show that ker Av is spanned by v.
(e) Justify the fact that the matrix exponentials etAv are rotations around the
axis v. Thus, the cross product with a vector serves as the inﬁnitesimal generator of
the one-parameter group of rotations around v.
♥10.4.39. Given a unit vector ∥u∥= 1 in R3, let A = Au be the corresponding skew-symmetric
3×3 matrix that satisﬁes Ax = u×x, as in Exercise 10.4.38. (a) Prove the Euler–Rodrigues
formula etA = I + (sin t)A + (1 −cos t) A2. Hint: Use the matrix exponential series (10.47).
(b) Show that etA = I if and only if t is an integer multiple of 2π. (c) Generalize parts
(a) and (b) to a non-unit vector v ̸= 0.
♥10.4.40. Let A =
⎛
⎜
⎝
0
−1
0
1
0
0
0
0
0
⎞
⎟
⎠, b =
⎛
⎜
⎝
0
0
1
⎞
⎟
⎠. (a) Show that the solution to the linear system
x = A x represents a rotation of R3 around the z-axis. What is the trajectory of a point
x0? (b) Show that the solution to the inhomogeneous system x = A x + b represents a
screw motion of R3 around the z-axis. What is the trajectory of a point x0? (c) More
generally, given 0 ̸= a ∈R3, show that the solution to x = a × x + a represents a family of
screw motions along the axis a.

10.4 Matrix Exponentials
603
10.4.41. Let A be an n × n matrix whose last row has all zero entries. Prove that the last row
of etA is eT
n = ( 0, . . . , 0, 1 ).
10.4.42. Let A =

B
c
0
0
	
be in block form, where B is an n × n matrix, c ∈Rn, while 0
denotes the zero row vector with n entries. Show that its matrix exponential is also in
block form etA =

etB
f(t)
0
1
	
. Can you ﬁnd a formula for f(t)?
♦10.4.43. According to Exercise 7.3.10, an (n + 1) × (n + 1) matrix of the block form

A
b
0
1
	
in which A is an n × n matrix and b ∈Rn can be identiﬁed with the aﬃne transformation
F[x] = A x + b on Rn. Exercise 10.4.42 shows that every matrix in the one-parameter
group etB generated by B =

A
b
0
0
	
has such a form, and hence we can identify etB as
a family of aﬃne maps on Rn. Describe the aﬃne transformations of R2 generated by the
following matrices:
(a)
⎛
⎜
⎝
0
0
1
0
0
0
0
0
0
⎞
⎟
⎠,
(b)
⎛
⎜
⎝
1
0
0
0
−2
0
0
0
0
⎞
⎟
⎠,
(c)
⎛
⎜
⎝
0
−1
0
1
0
1
0
0
0
⎞
⎟
⎠,
(d)
⎛
⎜
⎝
1
0
1
0
−1
−2
0
0
0
⎞
⎟
⎠.
Invariant Subspaces and Linear Dynamical Systems
Invariant subspaces, as per Deﬁnition 8.27, play an important role in the study of ho-
mogeneous linear dynamical systems.
In general, a subset S ⊂Rn is called invariant
for the homogeneous linear dynamical system
u = Au if, whenever the initial condition
u(t0) = b ∈S, then the solution u(t) ∈S for all t ∈R.
Proposition 10.31. If V ⊂Rn is an invariant subspace of the matrix A, then it is
invariant under the corresponding homogeneous linear dynamical system.
Proof : Given that b ∈V , we have Ab ∈V, A2 b ∈V , and, in general, An b ∈V for each
n ≥0. Thus every term in the matrix exponential series for the solution (10.42), namely
u(t) = e(t−t0) A b =
∞

n=0
tn
n! Anb,
belongs to V and hence, because V is closed, so does their sum: u(t) ∈V .
Q.E.D.
As we know, the (complex) invariant subspaces of a complete matrix are spanned by
its (complex) eigenvectors. According to the general Stability Theorem 10.13, these come
in three ﬂavors, depending upon whether the real part of the corresponding eigenvalue
is less than, equal to, or greater than 0. The ﬁrst kind, with Re λ < 0, correspond to
the asymptotically stable eigensolutions u(t) = eλt v →0 as t →∞. The second kind,
with zero real part, correspond to stable eigensolutions that remain bounded for all t,
by completeness. The third kind, with Re λ > 0, correspond to unstable eigensolutions
that become unbounded at an exponential rate as t →∞. A similar result holds for the
corresponding real solutions of a complete real matrix. If the matrix is incomplete, then
the solutions corresponding to Jordan chains with eigenvalues having negative real part are
also asymptotically stable; those corresponding to Jordan chains with eigenvalues having
positive real part remain exponentially unstable. If any purely imaginary eigenvalue is
incomplete, then the polynomial factor in front of the corresponding Jordan chain solution

604
10 Dynamics
makes it unstable, becoming unbounded at a polynomial rate. An example of the latter
behavior is provided by a planar system that has 0 as its incomplete eigenvalue, producing
unstable linear motion. The minimum dimension of a (real) system possessing a non-zero,
incomplete purely imaginary eigenvalue is 4.
This motivates dissecting the underlying vector space into three invariant subspaces,
having only the zero vector in common, that capture the three possible modes of behavior.
We state the deﬁnition in the real case, leaving the simpler complex version to the reader.
Deﬁnition 10.32. Let A be a real n × n matrix. We deﬁne the following invariant sub-
spaces spanned by the real and imaginary parts of the eigenvectors and Jordan chains
corresponding to the eigenvalues with the following properties:
(i) negative real part: the stable subspace S ⊂Rn;
(ii) zero real part: the center subspace C ⊂Rn;
(iii) positive real part: the unstable subspace U ⊂Rn.
If there are no eigenvalues of the speciﬁed type, the corresponding invariant subspace
is trivial.
For example, if the associated linear system has asymptotically stable zero
solution, then S = Rn while C = U = {0}. The stable, unstable, and center subspaces
are complementary, as in Exercise 2.2.24, in the sense that their pairwise intersections are
trivial: S ∩C = S ∩U = C ∩U = {0}, and their sum S + C + U = Rn, in the sense that
every v ∈Rn can be, in fact uniquely, written as a sum v = s + c + u of vectors in each
subspace: s ∈S, c ∈C, u ∈U.
Since each of these subspaces is invariant, if the initial condition belongs to one of them,
so does the corresponding solution. In view of the solution formulas in Theorem 10.13, we
deduce the following more intrinsic characterizations, in terms of the asymptotic behavior
of the solutions to the homogeneous linear dynamical system.
Theorem 10.33. Let A be an n × n matrix. Let 0 ̸= b ∈Rn, and let u(t) be a solution
to the associated initial value problem
u = Au, u(t0) = b. Then b and hence u(t) are in:
(i) the stable subspace S if and only if u(t) →0 as t →∞, or, alternatively, ∥u(t)∥→∞
at an exponential rate as t →−∞;
(ii) the center subspace C if and only if u(t) is bounded or ∥u(t)∥→∞at a polynomial
rate as t →±∞;
(iii) the unstable subspace U if and only if ∥u(t)∥→∞at an exponential rate as t →∞,
or, alternatively, u(t) →0 as t →−∞.
Example 10.34.
For example, the matrix A =
⎛
⎝
−2
1
0
1
−1
1
0
1
−2
⎞
⎠has eigenvalues and
eigenvectors
λ1 = 0,
v1 =
⎛
⎝
1
2
1
⎞
⎠,
λ2 = −2,
v2 =
⎛
⎝
−1
0
1
⎞
⎠,
λ3 = −3,
v3 =
⎛
⎝
1
−1
1
⎞
⎠.
Thus, the stable subspace is the plane spanned by v2 and v3, whose nonzero solutions tend
to the origin as t →∞at an exponential rate; the center subspace is the line spanned by
v1, all of whose solutions are constant; the unstable subspace is trivial: U = {0}. So the
origin is a stable, but not asymptotically stable, equilibrium point.
The Center Manifold Theorem, a celebrated result in nonlinear dynamics, [34], states
that the above formulated linear splitting into stable, center, and unstable regimes carries

10.4 Matrix Exponentials
605
over to nonlinear systems in a neighborhood of an equilibrium point. Roughly speaking,
suppose that u0 is an equilibrium point of the nonlinear systems of ordinary diﬀerential
equations
u = f(u), so that f(u0) = 0. Let A be the linearization of f(u) at u0, meaning
its Jacobian matrix. so A = f ′(u0) = (∂fi/∂uj), evaluated at the equilibrium point. Then,
in a neighborhood of u0, the dynamical system admits three invariant curved manifolds,
meaning curves, surfaces, and their higher-dimensional counterparts, called the stable,
center, and unstable manifolds that are tangent to (or equivalently, approximated by) the
corresponding invariant subspaces of its linearization matrix A. Solutions evolving on the
stable and unstable manifolds exhibit behaviors similar to those of the linear system. In
particular, solutions on the stable manifold converge to the equilibrium, u(t0) →u0 as
t →∞, at an exponential rate governed by the corresponding eigenvalues of A, while those
on the unstable manifold move away from the equilibrium point u0 — although one cannot
say what happens to them once they exit the neighborhood, once the nonlinear eﬀects take
over. Solutions on the center manifold have more subtle dynamical behavior, that depends
on the detailed structure of the nonlinear terms. In this manner, one can eﬀectively argue
that, near a ﬁxed point, all the interesting dynamics takes place on the center manifold.
Exercises
10.4.44.(a) Given a homogeneous linear dynamical system with invariant stable, unstable,
and center subspaces S, U, C, explain why the origin is asymptotically stable if and only if
C = U = {0}. (b) Is the origin stable if U = {0} but C ̸= {0}?
10.4.45. Find the (real) stable, unstable, and center subspaces of the following linear systems:
(a)
u1 = 9u2,
u2 = −u1;
(b)
x1 = 4x1 + x2,
x2 = 3x1;
(c)
y1 = y1 −y2,
y2 = 2y1 + 3y2;
(d)
z1 = z2,
z2 = 3z1 + 2z3,
y3 = −z2;
(e)
u1 = u1 −3u2 + 11u3,
u2 = 2u1 −6u2 + 16u3,
u3 = u1 −3u2 + 7u3,
(f ) du
dt =
⎛
⎜
⎝
−1
3
−3
2
2
−7
0
3
−4
⎞
⎟
⎠u,
(g) du
dt =
⎛
⎜
⎜
⎜
⎝
0
0
1
0
0
0
0
2
1
0
0
0
0
2
0
0
⎞
⎟
⎟
⎟
⎠u.
♦10.4.46. State and prove a counterpart to Deﬁnition 10.32 and Theorem 10.33 for a
homogeneous linear iterative system.
Inhomogeneous Linear Systems
We now direct our attention to inhomogeneous linear systems of ordinary diﬀerential equa-
tions. For simplicity, we consider only ﬁrst order† systems of the form
du
dt = Au + f(t),
(10.56)
in which A is a constant n × n matrix and f(t) is a vector-valued function of t that can be
interpreted as a collection of time-varying external forces acting on the system. According
to Theorem 7.38, the solution to the inhomogeneous system will have the general form
u(t) = u⋆(t) + z(t)
†
Higher order systems can, as in the phase plane construction, (10.8), always be converted into
ﬁrst order systems involving additional variables.

606
10 Dynamics
where u⋆(t) is a particular solution, representing a response to the forcing, while z(t) is
a solution to the corresponding homogeneous system
z = A z, representing the system’s
internal motion. Since we now know how to ﬁnd the solution z(t) to the homogeneous
system, the only task is to ﬁnd one particular solution to the inhomogeneous system.
In your ﬁrst course on ordinary diﬀerential equations, you probably encountered a
method known as variation of parameters for constructing particular solutions of inho-
mogeneous scalar ordinary diﬀerential equations, [7]. The method can be readily adapted
to ﬁrst order systems. Recall that, in the scalar case, to solve the inhomogeneous equation
du
dt = au + f(t),
we set
u(t) = eta v(t),
(10.57)
where the function v(t) is to be determined. Diﬀerentiating, we obtain
du
dt = aeta v(t) + eta dv
dt = au + eta dv
dt .
Therefore, u(t) satisﬁes the diﬀerential equation (10.57) if and only if
dv
dt = e−ta f(t).
Since the right-hand side of the latter is known, v(t) can be immediately found by a direct
integration.
The method can be extended to the vector-valued situation as follows. We replace the
scalar exponential by the exponential of the coeﬃcient matrix, setting
u(t) = etA v(t),
(10.58)
where v(t) is a vector-valued function that is to be determined. Combining the product
rule for matrix multiplication (10.41) with (10.39) yields
du
dt = d
dt (etA v) = detA
dt
v + etA dv
dt = A etA v + etA dv
dt = A u + etA dv
dt .
Comparing with the diﬀerential equation (10.56), we conclude that
dv
dt = e−tA f(t).
Integrating† both sides from the initial time t0 to time t produces, by the Fundamental
Theorem of Calculus,
v(t) = v(t0) +
 t
t0
e−sA f(s) ds,
where
v(t0) = e−t0A u(t0).
(10.59)
Substituting back into (10.58) leads to a general formula for the solution to the inhomoge-
neous linear system.
Theorem 10.35. The solution to the initial value problem
du
dt = Au + f(t),
u(t0) = b,
is
u(t) = e(t−t0) A b +
 t
t0
e(t−s) A f(s) ds. (10.60)
†
As with diﬀerentiation, vector-valued and matrix-valued functions are integrated entry-wise.

10.4 Matrix Exponentials
607
In the solution formula, the integral term can be viewed as a particular solution u⋆(t),
namely the one satisfying the initial condition u⋆(t0) = 0, while the ﬁrst summand,
z(t) = e(t−t0) A b for b ∈Rn, constitutes the general solution to the homogeneous
system.
Example 10.36.
Our goal is to solve the initial value problem
u1 = 2u1 −u2,
u1(0) = 1,
u2 = 4u1 −3u2 + et,
u2(0) = 0.
(10.61)
The ﬁrst step is to determine the eigenvalues and eigenvectors of the coeﬃcient matrix:
A =

2
−1
4
−3

so
λ1 = 1,
v1 =

1
1

,
λ2 = −2,
v2 =

1
4

.
The resulting eigensolutions form the columns of the nonsingular matrix solution
U(t) =

et
e−2t
et
4e−2t

,
hence
etA = U(t) U(0)−1 =

4
3 et −1
3 e−2t
−1
3 et + 1
3 e−2t
4
3 et −4
3 e−2t
−1
3 et + 4
3 e−2t

.
Since t0 = 0, the two constituents of the solution formula (10.60) are
etA b =

4
3 et −1
3 e−2t
−1
3 et + 1
3 e−2t
4
3 et −4
3 e−2t
−1
3 et + 4
3 e−2t

1
0

=

4
3 et −1
3 e−2t
4
3 et −4
3 e−2t

,
which forms the solution to the homogeneous system for the given nonzero initial conditions,
and  t
0
e(t−s) A f(s) ds =
 t
0

4
3 et−s −1
3 e−2(t−s)
−1
3 et−s + 1
3 e−2(t−s)
4
3 et−s −4
3 e−2(t−s)
−1
3 et−s + 4
3 e−2(t−s)

0
es

ds
=
 t
0

−1
3 et + 1
3 e−2t+3s
−1
3 et + 4
3 e−2t+3s

ds =

−1
3 tet + 1
9 (et −e−2t)
−1
3 tet + 4
9 (et −e−2t)

,
which is the particular solution to the inhomogeneous system that satisﬁes the homogeneous
initial conditions u(0) = 0. The solution to our initial value problem is their sum:
u(t) =

−1
3 tet + 13
9 et −4
9 e−2t
−1
3 tet + 16
9 et −16
9 e−2t

.
Exercises
10.4.47. Solve the following initial value problems:
(a)
u1 = 2u1 −u2,
u1(0) = 0,
u2 = 4u1 −3u2 + e2t,
u2(0) = 0.
(b)
u1 = −u1 + 2u2 + et,
u1(1) = 1,
u2 = 2u1 −u2 + et,
u2(1) = 1.
(c)
u1 = −u2,
u1(0) = 0,
u2 = 4u1 + cos t,
u2(0) = 1.
(d)
u = 3u + v + 1,
u(1) = 1,
v = 4u + t,
v(1) = −1.
(e)
p = p + q + t,
p(0) = 0,
q = −p −q + t,
q(0) = 0.
10.4.48. Solve the following initial value problems:
(a)
u1 = −2u2 + 2u3,
u1(0) = 1,
u2 = −u1 + u2 −2u3 + t,
u2(0) = 0,
u3 = −3u1 + u2 −2u3 + 1,
u3(0) = 0.
(b)
u1 = u1 −2u2,
u1(0) = −1,
u2 = −u2 + e−t,
u2(0) = 0,
u3 = 4u1 −4u2 −u3,
u3(0) = −1.

608
10 Dynamics
10.4.49. Suppose that λ is not an eigenvalue of A. Show that the inhomogeneous system
u = Au + eλt v has a solution of the form u⋆(t) = eλt w, where w is a constant vector.
What is the general solution?
10.4.50.(a) Write down an integral formula for the solution to the initial value problem
du
dt = Au + b, u(0) = 0, where b is a constant vector.
(b) Suppose b ∈img A. Do you recover the solution you found in Exercise 10.2.24?
10.5 Dynamics of Structures
Chapter 6 was concerned with the equilibrium conﬁgurations of mass–spring chains and,
more generally, structures constructed out of elastic bars. We are now able to undertake
an analysis of their dynamical motions, which are governed by second order systems of
ordinary diﬀerential equations. The same systems also serve to model the vibrations of
molecules, of fundamental importance in chemistry and spectroscopy, [91]. As in the ﬁrst
order case, the eigenvalues of the coeﬃcient matrix play an essential role in both the explicit
solution formula and the system’s qualitative behavior(s).
Let us begin with a mass–spring chain consisting of n masses m1, . . . , mn connected
together in a row and, possibly, to top and bottom supports by springs. As in Section 6.1,
that is, we restrict our attention to purely one-dimensional motion of the masses in the
direction of the chain. Thus the collective motion of the chain is prescribed by the displace-
ment vector u(t) = (u1(t), . . . , un(t))T whose ith entry represents the displacement from
equilibrium of the ith mass. Since we are now interested in dynamics, the displacements
are allowed to depend on time, t.
The motion of each mass is subject to Newton’s Second Law:
Force = Mass × Acceleration.
(10.62)
The acceleration of the ith mass is the second derivative
ui = d2ui/dt2 of its displacement
ui(t), so the right-hand sides of Newton’s Law is mi
ui. These form the entries of the vector
M
u obtained by multiplying the acceleration vector by the diagonal, positive deﬁnite mass
matrix M = diag (m1, . . . , mn). Keep in mind that the masses of the springs are assumed
to be negligible in this model.
If, to begin with, we assume that there are no frictional eﬀects, then the force exerted
on each mass is the diﬀerence between the external force, if any, and the internal force due
to the elongations of its two connecting springs. According to (6.11), the internal forces
are the entries of the product Ku, where K = AT C A is the stiﬀness matrix, constructed
from the chain’s (reduced) incidence matrix A, and the diagonal matrix of spring constants
C. Thus, Newton’s law immediately leads to the linear system of second order diﬀerential
equations
M d2u
dt2 = f(t) −Ku,
(10.63)
governing the dynamical motions of the masses under a possibly time-dependent exter-
nal force f(t). Such systems are also used to model the undamped dynamical motion of
structures and molecules as well as resistanceless (superconducting) electrical circuits.
As always, the ﬁrst order of business is to analyze the corresponding homogeneous
system
M d2u
dt2 + Ku = 0,
(10.64)
modeling the unforced motions of the physical apparatus.

10.5 Dynamics of Structures
609
r
δ/ω
Figure 10.5.
Vibration of a Mass.
Example 10.37.
The simplest case is that of a single mass connected to a ﬁxed sup-
port by a spring. Assuming no external force, the dynamical system (10.64) reduces to a
homogeneous second order scalar equation
m d2u
dt2 + k u = 0,
(10.65)
in which m > 0 is the mass, while k > 0 is the spring’s stiﬀness. The general solution to
(10.65) is
u(t) = c1 cos ωt + c2 sin ωt = r cos(ωt −δ),
where
ω =

k
m
(10.66)
is the natural frequency of vibration. In the second expression, we have used the phase-
amplitude equation (2.7) to rewrite the solution as a single cosine with
amplitude
r =

c2
1 + c2
2
and phase shift
δ = tan−1 c2
c1 .
(10.67)
Thus, the mass’ motion is periodic, with period P = 2π/ω. The stiﬀer the spring or the
lighter the mass, the faster the vibrations. Take note of the square root in the frequency
formula; quadrupling the mass slows down the vibrations by only a factor of two.
The constants c1, c2 — or their phase–amplitude counterparts r, δ — are determined by
the initial conditions. Physically, we need to specify both an initial position and an initial
velocity
u(t0) = a,
u(t0) = b,
(10.68)
in order to uniquely prescribe the subsequent motion of the system. The resulting solution
is most conveniently written in the form
u(t) = a cos ω(t −t0) + b
ω sin ω(t −t0) = r cos

ω(t −t0) −δ

,
with amplitude
r =

a2 + b2
ω2
and phase shift
δ = tan−1
b
a ω .
(10.69)
A typical solution is plotted in Figure 10.5.
Let us turn to a more general second order system. To begin with, let us assume that
the masses are all the same and equal to 1 (in some appropriate units), so that (10.64)
reduces to
d2u
dt2 + Ku = 0.
(10.70)
Inspired by the form of the solution of the scalar equation, let us try a trigonometric ansatz
for the solution, setting
u(t) = cos(ωt) v,
(10.71)

610
10 Dynamics
in which the vibrational frequency ω is a constant scalar and v ̸= 0 a constant vector.
Diﬀerentiation produces
du
dt = −ω sin(ωt) v,
d2u
dt2 = −ω2 cos(ωt) v,
whereas
Ku = cos(ωt) K v,
since the cosine factor is a scalar. Therefore, (10.71) will solve the second order system
(10.70) if and only if
K v = ω2 v.
(10.72)
The result is in the form of the eigenvalue equation Kv = λv for the stiﬀness matrix K,
with eigenvector v ̸= 0 and eigenvalue
λ = ω2.
(10.73)
Now, the scalar equation has both cosine and sine solutions. By the same token, the ansatz
u(t) = sin(ωt) v leads to the same eigenvector equation (10.72). We conclude that each
positive eigenvalue leads to two diﬀerent periodic trigonometric solutions.
Summarizing, we have established:
Lemma 10.38. If v is an eigenvector of the positive deﬁnite matrix K with eigenvalue
λ = ω2 > 0, then u(t) = cos(ωt) v and u(t) = sin(ωt) v are both solutions to the
homogeneous second order system
u + Ku = 0.
Stable Structures
Let us begin with the motion of a stable mass–spring chain or structure, of the type
introduced in Section 6.3. According to Theorem 6.8, stability requires that the reduced
stiﬀness matrix be positive deﬁnite: K > 0. Theorem 8.35 then says that all the eigenvalues
of K are strictly positive, λi > 0, which is good, since it implies that the vibrational
frequencies ωi =

λi are all real. Moreover, positive deﬁnite matrices are always complete,
and so K possesses an orthogonal eigenvector basis v1, . . . , vn of Rn corresponding to its
eigenvalues λ1, . . . , λn, listed in accordance with their multiplicities. This yields a total of
2n linearly independent trigonometric eigensolutions, namely
ui(t) = cos(ωi t) vi = cos

λi t

vi,
ui(t) = sin(ωi t) vi = sin
 
λi t

vi,
i = 1, . . ., n,
(10.74)
which is precisely the number required by the general existence and uniqueness theorems
for linear ordinary diﬀerential equations. The general solution to (10.70) is an arbitrary
linear combination of the eigensolutions:
u(t) =
n

i=1

ci cos(ωi t) + di sin(ωi t)

vi =
n

i=1
ri cos(ωi t −δi ) vi.
(10.75)
The 2n coeﬃcients ci, di — or their phase–amplitude counterparts ri ≥0 and 0 ≤δi < 2π
— are uniquely determined by the initial conditions. As in (10.68), we need to specify
both the initial positions and initial velocities of all the masses; this requires a total of 2n
initial conditions
u(t0) = a,
u(t0) = b.
(10.76)
Suppose t0 = 0; then substituting the solution formula (10.75) into the initial conditions,
we obtain
u(0) =
n

i=1
ci vi = a,
u(0) =
n

i=1
ωi di vi = b.

10.5 Dynamics of Structures
611
cos t + cos 7
3 t
cos t + cos
√
5 t
Figure 10.6.
Periodic and Quasi–Periodic Functions.
Since the eigenvectors are orthogonal, the coeﬃcients are immediately found by our or-
thogonal basis formula (4.7), whence
ci = ⟨a , vi ⟩
∥vi∥2 ,
di = ⟨b , vi ⟩
ωi ∥vi∥2 .
(10.77)
The eigensolutions (10.74) are also known as the normal modes of vibration of the sys-
tem, and the ωi =

λi its natural frequencies, which are the square roots of the eigenvalues
of the stiﬀness matrix K. Each eigensolution is a periodic, vector-valued function of pe-
riod Pi = 2π/ωi. Linear combinations of such periodic functions are called quasi-periodic,
because they are not, typically, periodic!
A simple example is provided by the family of functions
f(t) = cos t + cos ωt.
If ω = p/q ∈Q is a rational number, so p, q, ∈Z with q > 0, then f(t) is a periodic
function, since f(t + 2πq) = f(t), where 2πq is the minimal period, provided that p and q
have no common factors. However, if ω is an irrational number, then f(t) is not periodic.
You are encouraged to carefully inspect the graphs in Figure 10.6. The ﬁrst is periodic —
can you spot where it begins to repeat? — whereas the second is only quasi-periodic and
never quite succeeds in repeating its behavior. The general solution (10.75) to a vibrational
system is similarly quasi-periodic, and is periodic only when all the frequency ratios ωi/ωj
are rational numbers. To the uninitiated, such quasi-periodic motions may appear to be
rather chaotic,† even though they are built from a few simple periodic constituents. Most
structures and circuits exhibit quasi-periodic vibrational motions. Let us analyze a couple
of simple examples.
Example 10.39.
Consider a chain consisting of two equal unit masses connected to
top and bottom supports by three springs, as in Figure 10.7, with incidence matrix
A =

1
−1
0
0
1
−1

. If the spring constants are c1, c2, c3 (labeled from top to bottom),
†
This is not true chaos, which is is an inherently nonlinear phenomenon, [56].

612
10 Dynamics
m1
m2
Figure 10.7.
Motion of a Double Mass–Spring Chain with Fixed Supports.
then the stiﬀness matrix is
K = AT CA =

1
−1
0
0
1
−1
⎛
⎝
c1
0
0
0
c2
0
0
0
c3
⎞
⎠
⎛
⎝
1
0
−1
1
0
−1
⎞
⎠=

c1 + c2
−c2
−c2
c2 + c3

.
The eigenvalues and eigenvectors of K will prescribe the normal modes and vibrational
frequencies of our two–mass chain.
Let us look in detail when the springs are identical, and choose our units so that
c1 = c2 = c3 = 1. The resulting stiﬀness matrix K =

2
−1
−1
2

has eigenvalues and
eigenvectors
λ1 = 1,
v1 =

1
1

,
λ2 = 3,
v2 =

−1
1

.
The general solution to the system is then
u(t) = r1 cos(t −δ1)

1
1

+ r2 cos(
√
3 t −δ2)

−1
1

.
The ﬁrst summand is the normal mode vibrating at the relatively slow frequency ω1 = 1,
with the two masses moving in tandem. The second normal mode vibrate faster, with
frequency ω2 =
√
3 ≃1.73205, in which the two masses move in opposing directions. The
general motion is a linear combination of these two normal modes. Since the frequency ratio
ω2/ω1 =
√
3 is irrational, the motion is quasi-periodic. The system never quite returns to
its initial conﬁguration — unless it happens to be vibrating in one of the normal modes.
A graph of some typical displacements of the masses is plotted in Figure 10.7.
If we eliminate the bottom spring, so the masses are just hanging from the top support
as in Figure 10.8, then the reduced incidence matrix A⋆=

1
−1
0
1

loses its last row.

10.5 Dynamics of Structures
613
m1
m2
Figure 10.8.
Motion of a Double Mass–Spring Chain with One Free End.
Assuming that the springs have unit stiﬀnesses c1 = c2 = 1, the corresponding reduced
stiﬀness matrix is
K⋆= (A⋆)T A⋆=

1
−1
0
1
 
1
0
−1
1

=

2
−1
−1
1

.
The eigenvalues and eigenvectors are
λ1 = 3 −
√
5
2
,
v1 =
⎛
⎝
1
√
5 + 1
2
⎞
⎠,
λ2 = 3 +
√
5
2
,
v2 =
⎛
⎝
1
−
√
5 −1
2
⎞
⎠.
The general solution to the system is the quasi-periodic linear combination
u(t) = r1 cos
√
5 −1
2
t −δ1
 ⎛
⎝
1
√
5 + 1
2
⎞
⎠+ r2 cos
√
5 + 1
2
t −δ2
 ⎛
⎝
1
−
√
5 −1
2
⎞
⎠.
The slower normal mode, with frequency ω1 =

3 −
√
5
2
=
√
5 −1
2
≃.61803, has the
masses moving in tandem, with the bottom mass moving proportionally
√
5 + 1
2
≃1.61803
farther. The faster normal mode, with frequency ω2 =

3 +
√
5
2
=
√
5 + 1
2
≃1.61803,
has the masses moving in opposing directions, with the top mass experiencing the larger
displacement.
Thus, removing the bottom support has caused both modes to vibrate
slower. A typical solution is plotted in Figure 10.8.
Example 10.40.
Consider a three mass–spring chain, with unit springs and masses,
and both ends attached to ﬁxed supports. The stiﬀness matrix K =
⎛
⎝
2
−1
0
−1
2
−1
0
−1
2
⎞
⎠has
eigenvalues and eigenvectors
λ1 = 2 −
√
2 ,
λ2 = 2,
λ3 = 2 +
√
2 ,
v1 =
⎛
⎝
1
√
2
1
⎞
⎠,
v2 =
⎛
⎝
1
0
−1
⎞
⎠,
v3 =
⎛
⎝
1
−
√
2
1
⎞
⎠.

614
10 Dynamics
The three normal modes, from slowest to fastest, have frequencies
(a) ω1 =

2 −
√
2 :
all three masses move in tandem, with the middle one moving
√
2 times as far.
(b) ω2 =
√
2 :
the two outer masses move in opposing directions, while the
middle mass does not move.
(c) ω3 =

2 +
√
2 :
the two outer masses move in tandem, while the inner mass moves
√
2 times as far in the opposing direction.
The general motion is a quasi-periodic combination of these three normal modes. As such,
to the naked eye it can look very complicated. Our mathematical analysis unmasks the
innate simplicity, where the complex dynamics are, in fact, entirely governed by just three
fundamental modes of vibration.
Exercises
10.5.1. A 6 kilogram mass is connected to a spring with stiﬀness 21 kg/sec2. Determine the
frequency of vibration in hertz (cycles per second).
10.5.2. The lowest audible frequency is about 20 hertz = 20 cycles per second. How small a
mass would need to be connected to a unit spring to produce a fast enough vibration to be
audible? (As always, we assume the spring has negligible mass, which is probably not so
reasonable in this situation.)
10.5.3. Graph the following functions. Which are periodic? quasi-periodic? If periodic, what
is the (minimal) period?
(a) sin 4t + cos 6t, (b) 1 + sin π t, (c) cos 1
2 π t + cos 1
3 π t,
(d) cos t + cos π t, (e) sin 1
4 t + sin 1
5 t + sin 1
6 t, (f ) cos t + cos
√
2 t + cos 2t, (g) sin t sin 3t.
10.5.4. What is the minimal period of a function of the form cos p
q t + cos r
s t, assuming that
each fraction is in lowest terms, i.e., its numerator and denominator have no common factors?
10.5.5.(a) Determine the natural frequencies of the Newtonian system d2u
dt2 +

3
−2
−2
6
	
u = 0.
(b) What is the dimension of the space of solutions? Explain your answer.
(c) Write out the general solution.
(d) For which initial conditions is the resulting motion
(i) periodic? (ii) quasi-periodic? (iii) both? (iv) neither? Justify your answer.
10.5.6. Answer Exercise 10.5.5 for the system d2u
dt2 +

73
36
36
52
	
u = 0.
10.5.7. Find the general solution to the following second order systems:
(a) d2u
dt2 = −3u + 2v,
d2v
dt2 = 2u −3v.
(b) d2u
dt2 = −11u −2v,
d2v
dt2 = −2u −14v.
(c) d2u
dt2 +
⎛
⎜
⎝
1
0
0
0
4
0
0
0
9
⎞
⎟
⎠u = 0,
(d) d2u
dt2 =
⎛
⎜
⎝
−6
4
−1
4
−6
1
−1
1
−11
⎞
⎟
⎠u.
10.5.8. Two masses are connected by three springs to top and bottom supports. Can you ﬁnd
a collection of spring constants c1, c2, c3 such that all vibrations are periodic?
♠10.5.9. Suppose the bottom support in the mass–spring chain in Example 10.40 is removed.
(a) Do you predict that the vibration rate will (i) speed up, (ii) slow down, or (iii) stay
the same?
(b) Verify your prediction by computing the new vibrational frequencies.
(c) Suppose the middle mass is displaced by a unit amount and then let go. Compute and
graph the solutions in both situations. Discuss what you observe.

10.5 Dynamics of Structures
615
10.5.10. Show that a single mass that is connected to both the top and bottom supports by
two springs of stiﬀnesses c1, c2 will vibrate in the same manner as if it were connected to
only one support by a spring with the combined stiﬀness c = c1 + c2.
♠10.5.11.(a) Describe, quantitatively and qualitatively, the normal modes of vibration for a
mass–spring chain consisting of 3 unit masses, connected to top and bottom supports by
unit springs. (b) Answer the same question when the bottom support is removed.
♥10.5.12. Find the vibrational frequencies for a mass–spring chain with n identical masses,
connected by n + 1 identical springs to both top and bottom supports. Is there any sort of
limiting behavior as n →∞? Hint: See Exercise 8.2.48.
♣10.5.13. Suppose the illustrated planar structure has unit masses at the
nodes and the bars are all of unit stiﬀness. (a) Write down the system
of diﬀerential equations that describes the dynamical vibrations of
the structure.
(b) How many independent modes of vibration are
there?
(c) Find numerical values for the vibrational frequencies.
(d) Describe what happens when the structure vibrates in each of
the normal modes. (e) Suppose the left-hand mass is displaced a unit
horizontal distance. Determine the subsequent motion.
10.5.14. When does a homogeneous real ﬁrst order linear system u = Au have a quasi-periodic
solution? What is the smallest dimension in which this can occur?
♣10.5.15. Suppose you are given n diﬀerent springs. In which order should you connect them to
unit masses so that the mass–spring chain vibrates the fastest? Does your answer depend
upon the relative sizes of the spring constants? Does it depend upon whether the bottom
mass is attached to a support or left hanging free? First try the case of three springs with
spring stiﬀnesses c1 = 1, c2 = 2, c3 = 3. Then try varying the stiﬀnesses. Finally, predict
what will happen with 4 or 5 springs, and see whether you can make a general conjecture.
Unstable Structures
So far, we have just dealt with the stable case, in which the stiﬀness matrix K is positive
deﬁnite. Unstable conﬁgurations, which can admit rigid motions and/or mechanisms, will
provide additional complications. The simplest is a single mass that is not attached to
any spring. Since the mass experiences no restraining force, its motion is governed by the
elementary second order ordinary diﬀerential equation
m d2u
dt2 = 0.
(10.78)
The general solution is
u(t) = c t + d.
(10.79)
If c = 0, the mass sits at a ﬁxed position, while when c ̸= 0, it moves along a straight line
with constant velocity.
More generally, suppose that the stiﬀness matrix K for our structure is only positive
semi-deﬁnite.
Each vector 0 ̸= v ∈ker K represents a mode of instability of the sys-
tem. Since Kv = 0, the vector v is a null eigenvector with associated eigenvalue λ = 0.
Lemma 10.38 provides us with two solutions to the dynamical equations (10.70) of “fre-
quency” ω =
√
λ = 0. The ﬁrst, u(t) = cos(ω t) v ≡v is a constant solution, i.e., an
equilibrium conﬁguration of the system. Thus, an unstable system does not have a unique
equilibrium position, since every null eigenvector v ∈ker K is a constant solution. On
the other hand, the second solution, u(t) = sin(ωt) v ≡0, is trivial, and so doesn’t help
in constructing the requisite 2n linearly independent basis solutions. To ﬁnd the missing

616
10 Dynamics
Figure 10.9.
A Triatomic Molecule.
solution(s), let us again argue in analogy with the scalar case (10.79), and try u(t) = t v.
Fortunately, this works, since
u = v, so
u = 0. Also, Ku = t Kv = 0, and hence u(t) = t v
solves the system
u + Ku = 0. Therefore, to each element of the kernel of the stiﬀness
matrix — i.e., each rigid motion and mechanism — there is a two-dimensional family of
solutions
u(t) = (c t + d)v.
(10.80)
When c = 0, the solution u(t) = dv reduces to a constant equilibrium; when c ̸= 0, it
is moving oﬀto ∞with constant velocity in the null direction v, and so represents an
unstable mode of the system. The general solution will be a linear superposition of the
vibrational modes corresponding to the positive eigenvalues and the unstable linear motions
corresponding to the independent null eigenvectors.
Remark. If the null direction v ∈ker K represents a rigid translation, then the entire
structure will move in that direction. If v represents an inﬁnitesimal rotation, then, because
our model is based on a linear approximation to the true nonlinear motions, the individual
masses will move along straight lines, which are the tangent approximations to the circular
motion that occurs in the true physical, nonlinear regime. We refer to the earlier discussion
in Chapter 6 for details. Finally, if we excite a mechanism, then the masses will again
follow straight lines, moving in diﬀerent directions, whereas in the nonlinear real world the
masses may move along much more complicated curved trajectories. For small motions,
the distinction is not so important, while larger displacements, such as occur in the design
of robots, platforms, and autonomous vehicles, [57, 75], will require dealing with the vastly
more complicated nonlinear dynamical equations.
Example 10.41.
Consider a system of three unit masses connected in a line by two unit
springs, but not attached to any ﬁxed supports, as illustrated in Figure 10.9. This chain
could be viewed as a simpliﬁed model of an (unbent) triatomic molecule that is allowed
to move only in the vertical direction. The incidence matrix is A =

−1
1
0
0
−1
1

, and,
since we are dealing with unit springs, the stiﬀness matrix is
K = AT A =
⎛
⎝
−1
0
1
−1
0
1
⎞
⎠

−1
1
0
0
−1
1

=
⎛
⎝
1
−1
0
−1
2
−1
0
−1
1
⎞
⎠.

10.5 Dynamics of Structures
617
The eigenvalues and eigenvectors of K are easily found:
λ1 = 0,
λ2 = 1,
λ3 = 3 ,
v1 =
⎛
⎝
1
1
1
⎞
⎠,
v2 =
⎛
⎝
1
0
−1
⎞
⎠,
v3 =
⎛
⎝
1
−2
1
⎞
⎠.
Each positive eigenvalue provides two trigonometric solutions, while the zero eigenvalue
leads to solutions that are constant or depend linearly on t. This yields the required six
basis solutions:
u1(t) =
⎛
⎝
1
1
1
⎞
⎠,
u3(t) =
⎛
⎝
cos t
0
−cos t
⎞
⎠,
u5(t) =
⎛
⎝
cos
√
3 t
−2 cos
√
3 t
cos
√
3 t
⎞
⎠,
u2(t) =
⎛
⎝
t
t
t
⎞
⎠,
u4(t) =
⎛
⎝
sin t
0
−sin t
⎞
⎠,
u6(t) =
⎛
⎝
sin
√
3 t
−2 sin
√
3 t
sin
√
3 t
⎞
⎠.
The ﬁrst solution u1(t) is a constant, equilibrium mode, where the masses rest at a ﬁxed
common distance from their reference positions. The second solution u2(t) is the unstable
mode, corresponding to a uniform rigid translation of the molecule that does not stretch
the interconnecting springs. The ﬁnal four solutions represent vibrational modes. In the
ﬁrst pair, u3(t), u4(t), the two outer masses move in opposing directions, while the middle
mass remains ﬁxed, while the ﬁnal pair, u5(t), u6(t) has the two outer masses moving in
tandem, while the inner mass moves twice as far in the opposite direction. The general
solution is a linear combination of the six normal modes,
u(t) = c1 u1(t) + · · · + c6 u6(t),
(10.81)
and corresponds to the entire molecule moving at a ﬁxed velocity while the individual
masses perform a quasi-periodic vibration.
Let us see whether we can predict the motion of the molecule from its initial conditions
u(0) = a,
u(0) = b,
where a = ( a1, a2, a3 )T indicates the initial displacements of the three atoms, while b =
( b1, b2, b3 )T are their initial velocities. Substituting the solution formula (10.81) leads to
the two linear systems
c1 v1 + c3 v2 + c5 v3 = a,
c2 v1 + c4 v2 +
√
3 c6 v3 = b,
for the coeﬃcients c1, . . . , c6. As in (10.77), we can use the orthogonality of the eigenvectors
to immediately compute the coeﬃcients:
c1 = a · v1
∥v1 ∥2 = a1 + a2 + a3
3
, c3 = a · v2
∥v2 ∥2 = a1 −a3
2
, c5 = a · v3
∥v3 ∥2 = a1 −2a2 + a3
6
,
c2 = b · v1
∥v1 ∥2 = b1 + b2 + b3
3
,
c4 = b · v2
∥v2 ∥2 = b1 −b3
2
,
c6 =
b · v3
√
3 ∥v3 ∥2 = b1 −2b2 + b3
6
√
3
.
In particular, the unstable translational mode is excited if and only if c2 ̸= 0, and this occurs
if and only if there is a nonzero net initial velocity of the molecule: b1 +b2 +b3 ̸= 0. In this
case, the vibrating molecule will run oﬀto ∞at a uniform velocity c = c2 = 1
3(b1 + b2 + b3)
equal to the average of the individual initial velocities. On the other hand, if b1+b2+b3 = 0,

618
10 Dynamics
then the atoms will vibrate quasi-periodically, with frequencies 1 and
√
3, around its ﬁxed
center of mass.
The observations established in this example hold, in fact, in complete generality. Let
us state the result, leaving the details of the proof as an exercise for the reader.
Theorem 10.42.
The general solution to an unstable second order linear system
u + Ku = 0 with positive semi-deﬁnite coeﬃcient matrix K ≥0 is a linear combination
of a quasi-periodic or periodic vibrations and a uniform linear motion at a ﬁxed velocity
in the direction of a null eigenvector v ∈ker K. In particular, the system will just vibrate
around a ﬁxed position if and only if the initial velocity
u(t0) ∈(kerK)⊥= img K lies in
the image of the coeﬃcient matrix.
As in Chapter 6, the unstable modes v ∈ker K correspond to either rigid motions or
to mechanisms of the structure. Thus, to prevent a structure from exhibiting an unstable
motion, one has to ensure that the initial velocity is orthogonal to all of the unstable modes.
(The value of the initial position is not an issue.) This is the dynamical counterpart of
the requirement that an external force be orthogonal to all unstable modes in order to
maintain equilibrium in the structure, as in Theorem 6.8.
Systems with Diﬀering Masses
When a chain or structure has diﬀerent masses at the nodes, the (unforced) Newtonian
equations of motion take the more general form
M d2u
dt2 + Ku = 0,
or, equivalently,
d2u
dt2 = −M −1Ku = −P u.
(10.82)
The mass matrix M is always positive deﬁnite (and, almost always, diagonal, although
this is not required by the general theory), while the stiﬀness matrix K = AT C A is either
positive deﬁnite or, in the unstable situation when ker A ̸= {0}, positive semi-deﬁnite. The
coeﬃcient matrix
P = M −1K = M −1AT C A
(10.83)
is not in general symmetric, and so we cannot directly apply the preceding constructions.
However, P does have the more general self-adjoint form (7.85) based on the weighted
inner products
⟨u , u ⟩= uT M u,
⟨⟨v , v ⟩⟩= vT C v,
(10.84)
on, respectively, the domain and codomain of the (reduced) incidence matrix A. Moreover,
in the stable case when ker A = {0}, the matrix P is positive deﬁnite in the generalized
sense of Deﬁnition 7.59.
To solve the system of diﬀerential equations, we substitute the same trigonometric
solution ansatz u(t) = cos(ωt) v. This results in a generalized eigenvalue equation
K v = λ M v,
or, equivalently,
P v = λ v,
with
λ = ω2.
(10.85)
The matrix M assumes the role of the identity matrix in the standard eigenvalue equation
(8.13), and λ is a generalized eigenvalue if and only if it satisﬁes the generalized character-
istic equation
det(K −λM) = 0.
(10.86)
According to Exercise 8.5.8, if M > 0 and K > 0, then all the generalized eigenvalues are
real and non-negative. Moreover the generalized eigenvectors form an orthogonal basis of

10.5 Dynamics of Structures
619
Rn, but now with respect to the weighted inner product (10.84) prescribed by the mass
matrix M. The general solution is a quasi-periodic linear combination of the eigensolutions,
of the same form as in (10.75). In the unstable case, when K ≥0 (but M necessarily
remains positive deﬁnite), one must include enough generalized null eigenvectors to span
ker K, each of which leads to an unstable mode of the form (10.80). Further details are
relegated to the exercises.
Exercises
10.5.16. Find the general solution to the following systems. Distinguish between the
vibrational and unstable modes. What constraints on the initial conditions ensure
that the unstable modes are not excited? (a) d2u
dt2 = −4u −2v,
d2v
dt2 = −2u −v.
(b) d2u
dt2 = −u −3v,
d2v
dt2 = −3u −9v. (c) d2u
dt2 = −2u + v −2w,
d2v
dt2 = u −v,
d2w
dt2 = −2u −4w. (d) d2u
dt2 = −u + v −2w, d2v
dt2 = u −v + 2w, d2w
dt2 = −2u + 2v −4w.
10.5.17. Let K =
⎛
⎜
⎝
3
0
−1
0
2
0
−1
0
3
⎞
⎟
⎠. (a) Find an orthogonal matrix Q and a diagonal matrix Λ
such that K = Q Λ QT .
(b) Is K positive deﬁnite?
(c) Solve the second order system
d2u
dt2 = Au subject to the initial conditions u(0) =
⎛
⎜
⎝
1
0
1
⎞
⎟
⎠,
du
dt (0) =
⎛
⎜
⎝
0
1
0
⎞
⎟
⎠.
(d) Is your solution periodic? If your answer is yes, indicate the period.
(e) Is the general solution to the system periodic?
10.5.18. Answer Exercise 10.5.17 when A =
⎛
⎜
⎝
2
−1
0
−1
1
−1
0
−1
2
⎞
⎟
⎠.
10.5.19. Compare the solutions to the mass–spring system (10.65) with tiny spring constant
k = ε ≪1 to those of the completely unrestrained system (10.78). Are they close? Discuss.
♥10.5.20. Discuss the three-dimensional motions of the triatomic molecule of Example 10.41.
Are the vibrational frequencies the same as those of the one-dimensional model?
♥10.5.21. So far, our mass–spring chains have been allowed to move only in the vertical
direction. (a) Set up the system governing the planar motions of a mass–spring chain
consisting of two unit masses attached to top and bottom supports by unit springs, where
the masses are allowed to move in the longitudinal and transverse directions. Compare the
resulting vibrational frequencies with the one-dimensional case. (b) Repeat the analysis
when the bottom support is removed.
(c) Can you make any conjectures concerning the
planar motions of general mass–spring chains?
♠10.5.22. Find the vibrational frequencies and instabilities of the following structures, assuming
they have unit masses at all the nodes. Explain in detail how each normal mode moves the
structure: (a) the three bar planar structure in Figure 6.13; (b) its reinforced version in
Figure 6.16; (c) the swing set in Figure 6.18.
♠10.5.23. Assuming unit masses at the nodes, ﬁnd the vibrational frequencies and describe the
normal modes for the following planar structures. What initial conditions will not excite its
instabilities?
(a) An equilateral triangle; (b) a square; (c) a regular hexagon.
♠10.5.24. Answer Exercise 10.5.23 for the three-dimensional motions of a regular tetrahedron.

620
10 Dynamics
♥10.5.25.(a) Show that if a structure contains all unit masses and bars with unit stiﬀness,
ci = 1, then its frequencies of vibration are the nonzero singular values of the reduced
incidence matrix. (b) How would you recognize when a structure is close to being unstable?
10.5.26. Prove that if the initial velocity satisﬁes u(t0) = b ∈coimg A, then the solution to the
initial value problem (10.70, 76) remains bounded.
10.5.27. Find the general solution to the system (10.82) for the following matrix pairs:
(a) M =

2
0
0
3
	
, K =

3
−1
−1
2
	
, (b) M =

3
0
0
5
	
, K =

4
−2
−2
3
	
,
(c) M =

2
0
0
1
	
, K =

2
−1
−1
2
	
, (d) M =
⎛
⎜
⎝
2
0
0
0
3
0
0
0
6
⎞
⎟
⎠, K =
⎛
⎜
⎝
5
−1
−1
−1
6
3
−1
3
9
⎞
⎟
⎠,
(e) M =

2
1
1
2
	
, K =

3
−1
−1
3
	
, (f ) M =
⎛
⎜
⎝
1
1
0
1
3
1
0
1
1
⎞
⎟
⎠, K =
⎛
⎜
⎝
1
2
0
2
8
2
0
2
1
⎞
⎟
⎠.
10.5.28. A mass–spring chain consists of two masses, m1 = 1 and m2 = 2, connected to top and
bottom supports by identical springs with unit stiﬀness. The upper mass is displaced by a
unit distance. Find the subsequent motion of the system.
10.5.29. Answer Exercise 10.5.28 when the bottom support is removed.
♣10.5.30.(a) A water molecule consists of two hydrogen atoms connected at an angle of 105◦
to an oxygen atom whose relative mass is 16 times that of each of the hydrogen atoms.
If the molecular bonds are modeled as linear unit springs, determine the fundamental
frequencies and describe the corresponding vibrational modes.
(b) Do the same for a
carbon tetrachloride molecule, in which the chlorine atoms, with atomic weight 35, are
positioned on the vertices of a regular tetrahedron and the carbon atom, with atomic
weight 12, is at the center.
(c) Finally try a benzene molecule, consisting of 6 carbon
atoms arranged in a regular hexagon. In this case, every other bond is double strength
because two electrons are shared. (Ignore the six extra hydrogen atoms for simplicity.)
♥10.5.31. Repeat Exercise 10.5.21 for fully 3-dimensional motions of the chain.
♠10.5.32. Suppose you have masses m1 = 1, m2 = 2, m3 = 3, connected to top and bottom
supports by identical unit springs. Does rearranging the order of the masses change the
fundamental frequencies? If so, which order produces the fastest vibrations?
♦10.5.33. Suppose M is a nonsingular matrix. Prove that λ is a generalized eigenvalue of the
matrix pair K, M if and only if it is an ordinary eigenvalue of the matrix P = M−1K. How
are the eigenvectors related? How are the characteristic equations related?
10.5.34. Suppose that u(t) is a solution to (10.82). Let N =
√
M denote the positive
deﬁnite square root of the mass matrix M, as deﬁned in Exercise 8.5.27. (a) Prove that
the “weighted” displacement vector u(t) = N u(t) solves d2 u/dt2 = −
K u, where

K = N−1K N−1 is a symmetric, positive semi-deﬁnite matrix.
(b) Explain in what sense
this can serve as an alternative to the generalized eigenvector solution method.
♦10.5.35. Provide the details of the proof of Theorem 10.42.
♦10.5.36. State and prove the counterpart of Theorem 10.42 for the variable mass system (10.82).
Friction and Damping
We have not yet allowed friction to aﬀect the motion of our dynamical equations. In the
standard physical model, the frictional force on a mass in motion is directly proportional

10.5 Dynamics of Structures
621
Underdamped
Critically Damped
Overdamped
Figure 10.10.
Damped Vibrations.
to its velocity, [31]. For the simplest case of a single mass attached to a spring, one amends
the balance of forces in the undamped Newtonian equation (10.65) to obtain
m d2u
dt2 + β du
dt + k u = 0.
(10.87)
As before, m > 0 is the mass, and k > 0 the spring stiﬀness, while β > 0 measures the
eﬀect of a velocity-dependent frictional force — the larger the value of β, the greater the
frictional force.
The solution of this more general second order homogeneous linear ordinary diﬀerential
equation is found by substituting the usual exponential ansatz u(t) = eλt, reducing it to
the quadratic characteristic equation
m λ2 + β λ + k = 0.
(10.88)
Assuming that m, β, k > 0, there are three possible cases:
Underdamped:
If 0 < β < 2
√
m k, then (10.88) has two complex-conjugate roots:
λ = −β
2m ± i

4m k −β2
2m
= −μ ± i ν .
(10.89)
The general solution to the diﬀerential equation,
u(t) = e−μt
c1 cos ν t + c2 sin ν t

= re−μt cos(ν t −δ),
(10.90)
represents a damped periodic motion. The mass continues to oscillate at a ﬁxed frequency
ν =

4mk −β2
2m
=

k
m −β2
4m2 ,
(10.91)
but the vibrational amplitude r e−μt decays to zero at an exponential rate as t →∞.
Observe that, in a rigorous mathematical sense, the mass never quite returns to equilibrium,
although in the real world, after a suﬃciently long time the residual vibrations are not
noticeable, and equilibrium is physically (but not mathematically) achieved. The rate of
decay, μ = β/(2m), is directly proportional to the friction, and inversely proportional to
the mass. Thus, greater friction and/or less mass will accelerate the return to equilibrium.
The friction also has an eﬀect on the vibrational frequency (10.91); the larger β is, the
slower the oscillations become and the more rapid the damping eﬀect.
As the friction
approaches the critical threshold β⋆= 2
√
m k , the vibrational frequency goes to zero,
ν →0, and so the oscillatory period 2π/ν becomes longer and longer.

622
10 Dynamics
Overdamped:
If β > 2
√
m k, then the characteristic equation (10.88) has two negative
real roots
λ1 = −β +

β2 −4mk
2m
< λ2 = −β −

β2 −4mk
2m
< 0.
The solution
u(t) = c1 eλ1t + c2 eλ2t
(10.92)
is a linear combination of two decaying exponentials. An overdamped system models the
motion of, say, a mass in a vat of molasses. Its “vibration” is so slow that it can pass
at most once through the equilibrium position, and then only when its initial velocity is
relatively large. In the long term, the ﬁrst exponential in the solution (10.92) will go to
zero faster, and hence, as long as c2 ̸= 0, the overall decay rate of the solution is governed
by the dominant (least negative) eigenvalue λ2.
Critically Damped:
The borderline case occurs when β = β⋆= 2
√
m k, which means
that the characteristic equation (10.88) has only a single negative real root:
λ1 = −β
2m .
In this case, our ansatz supplies only one exponential solution eλ1t = e−β t/(2m). A second
independent solution is obtained by multiplication by t, leading to the general solution
u(t) = (c1 t + c2)e−β t/(2m).
(10.93)
Even though the formula looks quite diﬀerent, its qualitative behavior is very similar to
the overdamped case. The factor t plays an unimportant role, since the asymptotics of this
solution are almost entirely governed by the decaying exponential function. This represents
a non-vibrating solution that has the slowest possible decay rate, since any further reduction
of the frictional coeﬃcient will allow a damped, slowly oscillatory vibration to appear.
In all three cases, the zero equilibrium solution is globally asymptotically stable. Phys-
ically, no matter how small the frictional contribution, all solutions to the unforced system
eventually return to equilibrium as friction eventually overwhelms the motion.
This concludes our discussion of the scalar case. Similar considerations apply to mass–
spring chains, and to two- and three-dimensional structures. A frictionally damped struc-
ture is modeled by a second order system of the form
M d2u
dt2 + B du
dt + Ku = 0,
(10.94)
where the mass matrix M and the matrix of frictional coeﬃcients B are both diagonal and
positive deﬁnite, while the stiﬀness matrix K = AT C A ≥0 is a positive semi-deﬁnite Gram
matrix constructed from the (reduced) incidence matrix A. Under these assumptions, it can
be proved that the zero equilibrium solution is globally asymptotically stable. However, the
mathematical details in this case are suﬃciently intricate that we shall leave their analysis
as an advanced project for the highly motivated student.
Exercises
10.5.37. Consider the overdamped mass–spring equation u + 6 u + 5u = 0. If the mass starts
out a distance 1 away from equilibrium, how large must the initial velocity be in order that
it pass through equilibrium once?

10.6 Forcing and Resonance
623
10.5.38. Solve the following mass–spring initial value problems, and classify as to
(i) overdamped, (ii) critically damped, (iii) underdamped, or (iv) undamped:
(a) u + 6 u + 9u = 0, u(0) = 0,
u(0) = 1. (b) u + 2 u + 10u = 0, u(0) = 1,
u(0) = 1.
(c) u + 16u = 0, u(1) = 0,
u(1) = 1. (d) u + 3 u + 9u = 0, u(0) = 0,
u(0) = 1.
(e) 2 u + 3 u + u = 0, u(0) = 2,
u(0) = 0. (f ) u + 6 u + 10u = 0, u(0) = 3,
u(0) = −2.
10.5.39.(a) A mass weighing 16 pounds stretches a spring 6.4 feet. Assuming no friction,
determine the equation of motion and the natural frequency of vibration of the mass–spring
system. Use the value g = 32 ft/sec2 for the gravitational acceleration. (b) The mass–
spring system is placed in a jar of oil, whose frictional resistance equals the speed of the
mass. Assume the spring is stretched an additional 2 feet from its equilibrium position and
let go. Determine the motion of the mass. (c) Is the system overdamped or underdamped?
Are the vibrations more rapid or less rapid than in the undamped system?
10.5.40. Suppose you convert the second order equation (10.87) into its phase plane equivalent.
What are the phase portraits corresponding to (a) undamped, (b) underdamped,
(c) critically damped, and (d) overdamped motion?
♦10.5.41.(a) Prove that, given a non-constant solution to an overdamped mass–spring system,
there is at most one time where u(t⋆) = 0. (b) Is this statement also valid in the critically
damped case?
10.5.42. Discuss the possible behaviors of a mass moving in a frictional medium that is not
attached to a spring, i.e., set k = 0 in (10.87).
10.6 Forcing and Resonance
Up until now, our physical system has been left free to vibrate on its own. Let us investigate
what happens when we shake it. In this section, we will consider the eﬀects of periodic
external forcing on both undamped and damped systems.
The simplest case is that of a single mass connected to a spring that has no frictional
damping. We append an external forcing function f(t) to the homogeneous (unforced)
equation (10.65), leading to the inhomogeneous second order equation
m d2u
dt2 + k u = f(t),
(10.95)
in which m > 0 is the mass and k > 0 the spring stiﬀness. We are particularly interested
in the case of periodic forcing
f(t) = α cos γ t
(10.96)
of frequency γ > 0 and amplitude α. To ﬁnd a particular solution to (10.95–96), we use
the method of undetermined coeﬃcients† which tells us to guess a trigonometric solution
ansatz of the form
u⋆(t) = a cos γ t + b sin γ t,
(10.97)
where a, b are constants to be determined. Substituting (10.97) into the diﬀerential equa-
tion produces
m d2u⋆
dt2 + ku⋆= a(k −mγ2) cos γ t + b(k −mγ2) sin γ t = α cos γ t.
†
One can also use variation of parameters, although the intervening calculations are slightly
more complicated.

624
10 Dynamics
We can solve for
a =
α
k −mγ2 =
α
m(ω2 −γ2) ,
b = 0,
(10.98)
where
ω =

k
m
(10.99)
refers to the natural, unforced vibrational frequency of the system. The solution (10.98) is
valid provided its denominator is nonzero:
k −mγ2 = m(ω2 −γ2) ̸= 0.
Therefore, as long as the forcing frequency is not equal to the system’s natural frequency,
i.e., γ ̸= ω, there exists a particular solution
u⋆(t) = a cos γ t =
α
m(ω2 −γ2) cos γ t
(10.100)
that vibrates at the same frequency as the forcing function.
The general solution to the inhomogeneous system (10.95) is found, as usual, by adding
in an arbitrary solution (10.66) to the homogeneous equation, yielding
u(t) =
α
m(ω2 −γ2) cos γ t + r cos(ωt −δ),
(10.101)
where r and δ are determined by the initial conditions. The solution is therefore a quasi-
periodic combination of two simple periodic motions — the second, vibrating with fre-
quency ω, represents the internal or natural vibrations of the system, while the ﬁrst, with
frequency γ, represents the response to the periodic forcing. Due to the factor ω2 −γ2 in
the denominator of the latter, the closer the forcing frequency is to the natural frequency,
the larger the overall amplitude of the response.
Suppose we start the mass at equilibrium at the initial time t0 = 0, so the initial
conditions are
u(0) = 0,
u(0) = 0.
(10.102)
Substituting (10.101) and solving for r, δ, we ﬁnd that
r = −
α
m(ω2 −γ2) ,
δ = 0.
Thus, the solution to the initial value problem can be written in the form
u(t) =
α
m(ω2 −γ2)

cos γ t −cos ωt

=
2 α
m(ω2 −γ2) sin
 ω + γ
2
t

sin
 ω −γ
2
t

,
(10.103)
where we have employed a standard trigonometric identity, cf. Exercise 3.6.17. The ﬁrst
trigonometric factor, sin 1
2(ω + γ)t, represents a periodic motion at a frequency equal to the
average of the natural and the forcing frequencies. If the forcing frequency γ is close to the
natural frequency ω, then the second factor, sin 1
2(ω −γ)t, has a much smaller frequency,
and so oscillates on a much longer time scale. As a result, it modulates the amplitude of
the more rapid vibrations, and is responsible for the phenomenon of beats, in which a rapid
vibration is subject to a slowly varying amplitude. An everyday illustration of beats is two
tuning forks that have nearby pitches. When they vibrate close to each other, the sound

10.6 Forcing and Resonance
625
Figure 10.11.
Beats in a Periodically Forced Vibration.
you hear waxes and wanes in intensity. As a mathematical example, Figure 10.11 displays
the graph of the particular function
cos 14 t −cos 16 t = 2 sin t sin 15 t
on the interval −π ≤t ≤6π. The slowly varying amplitude 2 sin t is clearly visible as the
envelope of the relatively rapid vibrations at frequency 15.
When we force the system at exactly the natural frequency γ = ω, the trigonometric
ansatz (10.97) no longer works. This is because both terms are now solutions to the ho-
mogeneous equation, and so cannot be combined to form a solution to the inhomogeneous
version. In this situation, there is a simple modiﬁcation to the ansatz, namely multiplica-
tion by t, that does the trick. Substituting
u⋆(t) = at cos ωt + b t sin ωt
(10.104)
into the diﬀerential equation (10.95), we obtain
m d2u⋆
dt2 + ku⋆= −2amω sin ωt + 2bmω cos ωt = α cos ωt,
provided
a = 0,
b =
α
2mω ,
and so
u⋆(t) =
α
2mω t sin ωt.
Combining the resulting particular solution with the solution to the homogeneous equation
leads to the general solution
u(t) =
α
2mω t sin ωt + r cos(ωt −δ).
(10.105)
Both terms vibrate with frequency ω, but the amplitude of the ﬁrst grows larger and larger
as t →∞. As illustrated in Figure 10.12, the mass will oscillate more and more wildly. In
this situation, the system is said to be in resonance, and the increasingly large oscillations
are provoked by forcing it at its natural frequency ω. In a physical apparatus, once the
amplitude of resonant vibrations stretches the spring beyond its elastic limits, the linear
Hooke’s Law model is no longer applicable, and either the spring breaks or the system
enters a nonlinear regime.
Furthermore, if we are very close to resonance, the oscillations induced by the particular
solution (10.103) will have extremely large, although bounded, amplitude. The lesson is,
never force a system at or close to its natural frequency (or frequencies) of vibration.
A classic example was the 1831 collapse of a bridge when a British infantry regiment
marched in unison across it, apparently inducing a resonant vibration of the structure. The
bridge in question was an early example of the suspension style, similar to that pictured
in Figure 10.13. Learning their lesson, soldiers nowadays no longer march in step across

626
10 Dynamics
Figure 10.12.
Resonance.
Figure 10.13.
The Albert Bridge in London.
bridges — as reminded by the sign in the photo in Figure 10.13. An even more dramatic
case is the 1940 Tacoma Narrows Bridge disaster, when the vibrations due to a strong wind
caused the bridge to oscillate wildly and break apart! The collapse was caught on ﬁlm,
which can be found on YouTube, and is extremely impressive. The traditional explanation
was the excitement of the bridge’s resonant frequencies, although later studies revealed a
more sophisticated mathematical explanation of the collapse, [22; p. 118]. But resonance
is not exclusively harmful. In a microwave oven, the electromagnetic waves are tuned to
the resonant frequencies of water molecules so as to excite them into large vibrations and
thereby heat up your dinner. Blowing into a clarinet or other wind instrument excites the
resonant frequencies in the column of air contained within it, and this produces the musical
sound vibrations that we hear.
Frictional eﬀects can partially mollify the extreme behavior near the resonant frequency.
The frictionally damped vibrations of a mass on a spring, when subject to periodic forcing,

10.6 Forcing and Resonance
627
are described by the inhomogeneous diﬀerential equation
m d2u
dt2 + β du
dt + k u = α cos γ t.
(10.106)
Let us assume that the friction is suﬃciently small so as to be in the underdamped regime
β < 2
√
mk .
Since neither summand solves the homogeneous system, we can use the
trigonometric solution ansatz (10.97) to construct the particular solution
u⋆(t) =
α

m2(ω2 −γ2)2 + β2 γ2 cos(γ t −ε),
where
ω =

k
m
(10.107)
continues to denote the undamped resonant frequency (10.99), while ε, deﬁned by
tan ε =
β γ
m(ω2 −γ2) ,
(10.108)
represents a frictionally induced phase lag.
Thus, the larger the friction β, the more
pronounced the phase lag ε in the response of the system to the external forcing.
As
the forcing frequency γ increases, so does the phase lag, which attains the value 1
2 π at
the resonant frequency γ = ω, meaning that the system lags a quarter period behind the
forcing, and converges to its maximum ε = π as γ →∞. Thus, the response to a very
high frequency forcing is almost exactly out of phase — the mass is moving downwards
when the force is pulling it upwards, and vice versa!
The amplitude of the persistent
response (10.107) is at a maximum at the resonant frequency γ = ω, where it takes the
value α/(β ω). Thus, the smaller the frictional coeﬃcient β (or the slower the resonant
frequency ω), the more likely the breakdown of the system due to an overly large response.
The general solution is
u(t) =
α

m2(ω2 −γ2)2 + β2 γ2 cos(γ t −ε) + r e−μt cos(ν t −δ),
(10.109)
where λ = μ ± i ν are the roots of the characteristic equation, while r, δ are determined
by the initial conditions, cf. (10.89). The second term — the solution to the homogeneous
equation — is known as the transient, since it decays exponentially fast to zero. Thus,
at large times, any internal motions of the system that might have been excited by the
initial conditions die out, and only the particular solution (10.107) incited by the continued
forcing persists.
Exercises
10.6.1. Graph the following functions. Describe the fast oscillatory and beat frequencies:
(a) cos 8t −cos 9t, (b) cos 26t −cos 24t, (c) cos 10t + cos 9.5t, (d) cos 5t −sin 5.2t.
10.6.2. Solve the following initial value problems:
(a) u + 36u = cos 3t, u(0) = 0,
u(0) = 0.
(b) u + 6 u + 9u = cos t, u(0) = 0,
u(0) = 1. (c) u + u + 4u = cos 2t, u(0) = 1,
u(0) = −1. (d) u + 9u = 3 sin 3t, u(0) = 1,
u(0) = −1. (e) 2 u + 3 u + u = cos 1
2 t,
u(0) = 3,
u(0) = −2. (f ) 3 u + 4 u + u = cos t, u(0) = 0,
u(0) = 0.
10.6.3. Solve the following initial value problems. In each case, graph the solution and explain
what type of motion is represented.
(a) u + 4 u + 40u = 125 cos 5t, u(0) = 0,
u(0) = 0,
(b) u + 25u = 3 cos 4t, u(0) = 1,
u(0) = 1, (c) u + 16u = sin 4t, u(0) = 0,
u(0) = 0,
(d) u + 6 u + 5u = 25 sin 5t, u(0) = 4,
u(0) = 2.

628
10 Dynamics
10.6.4. A mass m = 25 is attached to a unit spring with k = 1, and frictional coeﬃcient
β = .01. The spring will break when it moves more than 1 unit. Ignoring the eﬀect of the
transient, what is the maximum allowable amplitude α of periodic forcing at frequency γ =
(a) .19 ?
(b) .2 ?
(c) .21 ?
10.6.5.(a) For what range of frequencies γ can you force the mass in Exercise 10.6.4 with
amplitude α = .5 without breaking the spring? (b) How large should the friction be so that
you can safely force the mass at any frequency?
10.6.6. Suppose the mass–spring–oil system of Exercise 10.5.39(b) is subject to a periodic exter-
nal force 2 cos 2t. Discuss, in as much detail as you can, the long-term motion of the mass.
♦10.6.7. Write down the solution u(t, γ) to the initial value problem m d2u
dt2 + k u = α cos γ t,
u(0) = u(0) = 0, for (a) a non-resonant forcing function at frequency γ ̸= ω;
(b) a resonant forcing function at frequency γ = ω.
(c) Show that, as γ →ω, the limit of the non-resonant solution equals the resonant
solution. Conclude that the solution u(t, γ) depends continuously on the frequency γ even
though its mathematical formula changes signiﬁcantly at resonance.
♦10.6.8. Justify the solution formulas (10.107) and (10.108).
♠10.6.9.(a) Does a function of the form u(t) = a cos γ t −b cos ω t still exhibit beats when γ ≈ω,
but a ̸= b? Use a computer to graph some particular cases and discuss what you observe.
(b) Explain to what extent the conclusions based on (10.103) do not depend upon the
choice of initial conditions (10.102).
Electrical Circuits
The Electrical–Mechanical Correspondence outlined in Section 6.2 will continue to operate
in the dynamical universe.
The equations governing the equilibria of simple electrical
circuits and the mechanical systems such as mass–spring chains and structures all have the
same underlying mathematical structure. In a similar manner, although they are based
on a completely diﬀerent set of physical principles, circuits with dynamical currents and
voltages are modeled by second order linear dynamical systems of the Newtonian form
presented earlier.
In this section, we brieﬂy analyze the very simplest situation: a single loop containing
a resistor R, an inductor L, and a capacitor C, as illustrated in Figure 10.14. This basic
RLC circuit serves as the prototype for more general electrical networks linking various re-
sistors, inductors, capacitors, batteries, voltage sources, etc. (Extending the mathematical
analysis to more complicated circuits would make an excellent in-depth student research
project.) Let u(t) denote the current in the circuit at time t. We use vR, vL, vC to denote
the induced voltages in the three circuit elements; these are prescribed by the fundamental
laws of electrical circuitry.
(a) First, as we learned in Section 6.2, the resistance R ≥0 is the proportionality factor
between voltage and current, so vR = R u.
(b) The voltage passing through an inductor is proportional to the rate of change in the
current. Thus, vL = L
u, where L > 0 is the inductance, and the dot indicates
time derivative.
(c) On the other hand, the current passing through a capacitor is proportional to the rate
of change in the voltage, and so u = C
vC, where C > 0 denotes the capacitance.
We integrate† this relation to produce the capacitor voltage vC =

u(t)
C
dt.
†
The integration constant is not important, since we will diﬀerentiate the resulting equation.

10.6 Forcing and Resonance
629


R
V
L
C
Figure 10.14.
The Basic RLC Circuit.
The Voltage Balance Law tells us that the total of these individual voltages must equal
any externally applied voltage vE = F(t) coming from, say, a battery or generator. There-
fore,
vR + vL + vC = vE.
Substituting the preceding formulas, we deduce that the current u(t) in our circuit satisﬁes
the following linear integro-diﬀerential equation:
L du
dt + R u +

u
C dt = F(t).
(10.110)
We can convert this into a diﬀerential equation by diﬀerentiating both sides with respect
to t. Assuming, for simplicity, that L, R, and C are constant, the result is the linear second
order ordinary diﬀerential equation
L d2u
dt2 + R du
dt + 1
C u = f(t) = F ′(t).
(10.111)
The current will be uniquely speciﬁed by the initial conditions u(t0) = a,
u(t0) = b.
Comparing (10.111) with the equation (10.87) for a mechanically vibrating mass, we see
that the correspondence between electrical circuits and mechanical structures developed
in Chapter 6 continues to hold in the dynamical regime. The current u corresponds to
the displacement. The inductance L plays the role of mass, the resistance R corresponds,
as before, to friction, while the reciprocal 1/C of capacitance is analogous to the spring
stiﬀness. Thus, all of our analytical conclusions regarding stability of equilibria, qualita-
tive behavior, solution formulas, etc., that we established in the mechanical context can,
suitably re-interpreted, be immediately applied to electrical circuit theory.
In particular, an RLC circuit is underdamped if R2 < 4L/C, and the current u(t)
oscillates with frequency
ν =

1
C L −R2
4L2 ,
(10.112)
while dying oﬀto zero at an exponential rate e−Rt/(2L). In the overdamped and criti-
cally damped cases R2 ≥4L/C, the resistance in the circuit is so large that the current
merely decays to zero at an exponential rate and no longer exhibits any oscillatory behav-
ior. Attaching an alternating current source F(t) = α cos γ t to the circuit can induce a

630
10 Dynamics
catastrophic resonance if there is no resistance and the forcing frequency is equal to the
circuit’s natural frequency.
Exercises
10.6.10. Classify the following RLC circuits as (i) underdamped, (ii) critically damped, or
(iii) overdamped:
(a) R = 1, L = 2, C = 4,
(b) R = 4, L = 3, C = 1,
(c) R = 2, L = 3, C = 3, (d) R = 4, L = 10, C = 2,
(e) R = 1, L = 1, C = 3.
10.6.11. Find the current in each of the unforced RLC circuits in Exercise 10.6.10 induced by
the initial data u(0) = 1,
u(0) = 0.
10.6.12. A circuit with R = 1, L = 2, C = 4, includes an alternating current source
F(t) = 25 cos 2t. Find the solution to the initial value problem u(0) = 1,
u(0) = 0.
10.6.13. A superconducting LC circuit has no resistance: R = 0. Discuss what happens when
the circuit is wired to an alternating current source F(t) = α cos γ t.
10.6.14. A circuit with R = .002, L = 12.5, and C = 50 can carry a maximum current of
250. Ignoring the eﬀect of the transient, what is the maximum allowable amplitude α of an
applied periodic current F(t) = α cos γ t at frequency γ = (a) .04 ? (b) .05 ? (c) .1 ?
10.6.15. Given the circuit in Exercise 10.6.14, over what range of frequencies γ can you supply
a unit amplitude periodic current source?
10.6.16. How large should the resistance in the circuit in Exercise 10.6.14 be so that you can
safely apply any unit amplitude periodic current?
Forcing and Resonance in Systems
Let us conclude by brieﬂy discussing the eﬀect of periodic forcing on a system of second
order ordinary diﬀerential equations. Periodically forcing an undamped mass–spring chain
or structure, or a resistanceless electrical network, leads to a second order system of the
form
M d2u
dt2 + Ku = cos(γ t) a.
(10.113)
Here M > 0 and K ≥0 are n×n matrices as above, cf. (10.82), while a ∈Rn is a constant
vector representing both a magnitude and a “direction” of the forcing and γ is the forcing
frequency. Superposition is used to determine the eﬀect of several such forcing functions.
As always, the solution to the inhomogeneous system is composed of one particular response
to the external force combined with the general solution to the homogeneous system, which,
in the stable case K > 0, is a quasi-periodic combination of the normal vibrational modes.
To ﬁnd a particular solution to the inhomogeneous system, let us try the trigonometric
ansatz
u⋆(t) = cos(γ t) w
(10.114)
in which w is a constant vector. Substituting into (10.113) leads to a linear algebraic
system
(K −μM) w = a,
where
μ = γ2.
(10.115)
If the linear system (10.115) has a solution, then our ansatz (10.114) is valid, and we have
produced a particular vibration of the system (10.113) possessing the same frequency as
the forcing vibration. In particular, if μ = γ2 is not a generalized eigenvalue of the matrix
pair K, M, as described in (10.85), then the coeﬃcient matrix K −μM is nonsingular, and

10.6 Forcing and Resonance
631
so (10.115) can be uniquely solved for any right-hand side a. The general solution, then,
will be a quasi-periodic combination of this particular solution coupled with the normal
mode vibrations at the natural, unforced frequencies of the system.
The more interesting case occurs when γ2 = μ is a generalized eigenvalue, and so K−μM
is singular, its kernel being equal to the generalized eigenspace Vμ = ker(K −μM). In this
case, (10.115) will have a solution w if and only if a lies in the image of K−μM. According
to the Fredholm Alternative Theorem 4.46, the image is the orthogonal complement of
the cokernel, which, since the coeﬃcient matrix is symmetric, is the same as the kernel.
Therefore, (10.115) will have a solution if and only if a is orthogonal to Vμ, i.e., a·v = 0 for
every generalized eigenvector v ∈Vμ. Thus, one can force a system at a natural frequency
without inciting resonance, provided that the “direction” of forcing, as determined by the
vector a, is orthogonal — in the linear algebraic sense — to the natural directions of motion
of the system, as governed by the eigenvectors for that particular frequency.
If the orthogonality condition is not satisﬁed, then the periodic solution ansatz (10.114)
does not apply, and we are in a truly resonant situation. Inspired by the scalar solution,
let us try a resonant solution ansatz
u⋆(t) = t sin(γ t) y + cos(γ t) w.
(10.116)
Since
d2u⋆
dt2
= −γ2 t sin(γ t) y + cos(γ t) (2γ y −γ2 w),
the function (10.116) will solve the diﬀerential equation (10.113) provided
(K −μM)y = 0,
(K −μM)w = a −2γ y,
μ = γ2.
(10.117)
The ﬁrst equation requires that y ∈Vμ be a generalized eigenvector of the matrix pair
K, M. Again, the Fredholm Alternative implies that, since the coeﬃcient matrix K −μM
is symmetric, the second equation will be solvable for w if and only if a−2γ y is orthogonal
to the generalized eigenspace Vμ = coker(K −μM) = ker(K −μM). Thus, the vector 2γ y
is required to be the orthogonal projection of a onto the eigenspace Vμ. With this choice
of y and w, formula (10.116) deﬁnes the resonant solution to the system.
Theorem 10.43. An undamped vibrational system will be periodically forced into reso-
nance if and only if the forcing f = cos(γ t) a is at a natural frequency of the system and
the direction of forcing a is not orthogonal to the natural direction(s) of motion of the
system at that frequency.
Example 10.44.
Consider the periodically forced system
d2u
dt2 +

3
−2
−2
3

u =

cos t
0

.
The eigenvalues of the coeﬃcient matrix are λ1 = 5, λ2 = 1, with corresponding orthogonal
eigenvectors v1 =

−1
1

,
v2 =

1
1

. The internal frequencies are ω1 =

λ1 =
√
5,
†
We can safely ignore the arbitrary multiple of the generalized eigenvector that can be added
to w as we only need ﬁnd one particular solution; these will reappear anyway once we assemble
the general solution to the system.

632
10 Dynamics
ω2 =

λ2 = 1, and hence we are forcing at a resonant frequency. To obtain the resonant
solution (10.116), we ﬁrst note that a = ( 1, 0 )T has orthogonal projection p =
 1
2, 1
2
T
onto the eigenline spanned by v2, and hence y = 1
2 p =
 1
4, 1
4
T . We can then solve
(K −I )w =

2
−2
−2
2

w = a −p =

1
2
−1
2

for†
w =

1
4
0

.
Therefore, the particular resonant solution is
u⋆(t) = (t sin t)y + (cos t)w =

1
4 t sin t + 1
4 cos t
1
4 t sin t

.
The general solution to the system is
u(t) =

1
4 t sin t + 1
4 cos t
1
4 t sin t

+ r1 cos
 √
5 t −δ1
 
−1
1

+ r2 cos(t −δ2)

1
1

,
where the amplitudes r1, r2 and phase shifts δ1, δ2, are ﬁxed by the initial conditions.
Eventually the resonant terms involving t sin t dominate the solution, inducing progressively
larger and larger oscillations.
Exercises
10.6.17. Find the general solution to the following forced second order systems:
(a)
d2u
dt2 +

7
−2
−2
4
	
u =

cos t
0
	
, (b) d2u
dt2 +

5
−2
−2
3
	
u =

0
5 sin 3t
	
,
(c)
d2u
dt2 +

13
−6
−6
8
	
u =

5 cos 2t
cos 2t
	
, (d)

2
0
0
3
	 d2u
dt2 +

3
−1
−1
2
	
u =
⎛
⎝
cos 1
2 t
−cos 1
2 t
⎞
⎠,
(e)

3
0
0
5
	 d2u
dt2 +

4
−2
−2
3
	
u =

cos t
11 sin 2t
	
, (f ) d2u
dt2 +
⎛
⎜
⎝
6
−4
1
−4
6
−1
1
−1
11
⎞
⎟
⎠u =
⎛
⎜
⎝
cos t
0
cos t
⎞
⎟
⎠.
10.6.18.(a) Find the resonant frequencies of a mass–spring chain consisting of two masses,
m1 = 1 and m2 = 2, connected to top and bottom supports by identical springs with unit
stiﬀness. (b) Write down an explicit forcing function that will excite the resonance.
10.6.19. Suppose one of the ﬁxed supports is removed from the mass–spring chain of Exercise
10.6.18. Does your forcing function still excite the resonance? Do the internal vibrations
of the masses (i) speed up, (ii) slow down, or (iii) remain the same? Does your answer
depend upon which of the two supports is removed?
♣10.6.20. Find the resonant frequencies of the following structures, assuming the nodes all have
unit mass. Then ﬁnd a means of forcing the structure at one of the resonant frequencies,
and yet not exciting the resonance. Can you also force the structure without exciting any
mechanism or rigid motion? (a) The square truss of Exercise 6.3.5; (b) the joined square
truss of Exercise 6.3.6; (c) the house of Exercise 6.3.8; (d) the triangular space station
of Example 6.6; (e) the triatomic molecule of Example 10.41; (f ) the water molecule of
Exercise 10.5.30.

References
[1] Abraham, R., Marsden, J.E., and Ratiu, T., Manifolds, Tensor Analysis, and
Applications, Springer–Verlag, New York, 1988.
[2] Apostol, T.M., Calculus, Blaisdell Publishing Co., Waltham, Mass., 1967–69.
[3] Baker, G.A., Jr., and Graves–Morris, P., Pad´e Approximants, Encyclopedia of
Mathematics and Its Applications, v. 59, Cambridge Univ. Press, Cambridge, 1996.
[4] Behrends, E., Introduction to Markov Chains, Vieweg, Braunschweig/Wiesbaden,
Germany, 2000.
[5] Bertalm´ıo, M., Image Processing for Cinema, Cambridge University Press, Cambridge,
2013.
[6] Bollob´as, B., Graph Theory: An Introductory Course, Graduate Texts in Mathematics,
vol. 63, Springer–Verlag, New York, 1993.
[7] Boyce, W.E., and DiPrima, R.C., Elementary Diﬀerential Equations and Boundary Value
Problems, Seventh Edition, John Wiley & Sons, Inc., New York, 2001.
[8] Bradie, B., A Friendly Introduction to Numerical Analysis, Prentice–Hall, Inc., Upper
Saddle River, N.J., 2006.
[9] Brigham, E.O., The Fast Fourier Transform, Prentice–Hall, Inc., Englewood Cliﬀs, N.J.,
1974.
[10] Briggs, W.L., and Henson, V.E., The DFT. An Owner’s Manual for the Discrete Fourier
Transform, SIAM, Philadelphia, PA, 1995.
[11] B¨urgisser, P., Clausen, M., and Shokrollahi, M.A., Algebraic Complexity Theory,
Springer–Verlag, New York, 1997.
[12] Buss, S.A., 3D Computer Graphics, Cambridge University Press, Cambridge, 2003.
[13] Cantwell, B.J., Introduction to Symmetry Analysis, Cambridge University Press,
Cambridge, 2003.
[14] Chung, F.R.K., Spectral Graph Theory, CBMS Regional Conference Series in
Mathematics, No. 92, Amer. Math. Soc., Providence, R.I., 1997.
[15] Cooley, J.W., and Tukey, J.W., An algorithm for the machine computation of complex
Fourier series, Math. Comp. 19 (1965), 297–301.
[16] Courant, R., and Hilbert, D., Methods of Mathematical Physics, Interscience Publ., New
York, 1953.
[17] Crowe, M.J., A History of Vector Analysis, Dover Publ., New York, 1985.
[18] Daubechies, I., Ten Lectures on Wavelets, SIAM, Philadelphia, PA, 1992.
[19] Davidson, K.R., and Donsig, A.P., Real Analysis with Real Applications, Prentice–Hall,
Inc., Upper Saddle River, N.J., 2002.
[20] DeGroot, M.H., and Schervish, M.J., Probability and Statistics, Third Edition,
Addison–Wesley, Boston, 2002.
[21] Demmel, J.W., Applied Numerical Linear Algebra, SIAM, Philadelphia, PA, 1997.
[22] Diacu, F., An Introduction to Diﬀerential Equations, W.H. Freeman, New York, 2000.
[23] Durrett, R., Essentials of Stochastic Processes, Springer–Verlag, New York, 1999.
[24] Enders, C.K., Applied Missing Data Analysis, The Guilford Press, New York, 2010.
© Springer International Publishing AG, part of Springer Nature 2018 
https://doi.org/10.1007/978-3-319-91041-3 
633
P. J. Olver, C. Shakiban, Applied Linear Algebra, Undergraduate Texts in Mathematics, 

634
References
[25] Farin, G.E., Curves and Surfaces for CAGD: A Practical Guide, Academic Press, London,
2002.
[26] Fine, B., and Rosenberger, G., The Fundamental Theorem of Algebra, Undergraduate
Texts in Mathematics, Springer–Verlag, New York, 1997.
[27] Fortnow, L., The Golden Ticket: P, NP, and the Search for the Impossible, Princeton
University Press, Princeton, N.J., 2013.
[28] Foucart, S., and Rauhut, H., A Mathematical Introduction to Compressive Sensing,
Birkh¨auser, Springer, New York, 2013.
[29] Francis, J.G.F., The QR transformation I, II, Comput. J. 4 (1961–2), 265–271, 332–345.
[30] Gohberg, I., and Koltracht, I., Triangular factors of Cauchy and Vandermonde matrices,
Integral Eq. Operator Theory 26 (1996), 46–59.
[31] Goldstein, H., Classical Mechanics, Second Edition, Addison–Wesley, Reading, MA, 1980.
[32] Golub, G.H, and Van Loan, C.F., Matrix Computations, Third Edition, Johns Hopkins
Univ. Press, Baltimore, 1996.
[33] Graver, J.E., Counting on Frameworks: Mathematics to Aid the Design of Rigid
Structures, Dolciani Math. Expo. No. 25, Mathematical Association of America,
Washington, DC, 2001.
[34] Guckenheimer, J., and Holmes, P., Nonlinear Oscillations, Dynamical Systems, and
Bifurcations of Vector Fields, Appl. Math. Sci., vol. 42, Springer–Verlag, New York,
1983.
[35] Haar, A., Zur Theorie der orthogonalen Funktionensysteme, Math. Ann. 69 (1910),
331–371.
[36] Hale, J.K., Ordinary Diﬀerential Equations, Second Edition, R. E. Krieger Pub. Co.,
Huntington, N.Y., 1980.
[37] Herrlich, H., and Strecker, G.E., Category Theory; an Introduction, Allyn and Bacon,
Boston, 1973.
[38] Herstein, I.N., Abstract Algebra, John Wiley & Sons, Inc., New York, 1999.
[39] Hestenes, M.R., and Stiefel, E., Methods of conjugate gradients for solving linear systems,
J. Res. Nat. Bur. Standards 49 (1952), 409–436.
[40] Higham, N.J., Accuracy and Stability of Numerical Algorithms, Second Edition, SIAM,
Philadelphia, 2002.
[41] Hirsch, M.W., and Smale, S., Diﬀerential Equations, Dynamical Systems, and Linear
Algebra, Academic Press, New York, 1974.
[42] Hobson, E.W., The Theory of Functions of a Real Variable and the Theory of Fourier’s
Series, Dover Publ., New York, 1957.
[43] Hogg, R.V., Tanis, E.A., and Zimmerman, D.L., Probability and Statistical Inference,
Nineth Edition, Pearson Education Inc., Boston, MA, 2013.
[44] Hoggatt, V.E., Jr., and Lind, D.A., The dying rabbit problem, Fib. Quart. 7 (1969),
482–487.
[45] Hoory, S., Linial, N., and Wigderson, A., Expander Graphs and Their Applications, Bull.
Amer. Math. Soc. 43 (2006), 439–561.
[46] Hotelling, H., Analysis of complex of statistical variables into principal components, J.
Educ. Pshchology. 24 (1933), 417–441, 498–520.
[47] Jolliﬀe, I.T., Principal Component Analysis, Second Edition, Springer–Verlag, New York,
2002.

References
635
[48] Kato, T., Perturbation Theory for Linear Operators, Corrected Printing of Second
Edition, Springer–Verlag, New York, 1980.
[49] Keener, J.P., Principles of Applied Mathematics. Transformation and Approximation,
Addison–Wesley Publ. Co., New York, 1988.
[50] Krall, A.M., Applied Analysis, D. Reidel Publishing Co., Boston, 1986.
[51] Kublanovskaya, V.N., On some algorithms for the solution of the complete eigenvalue
problem, USSR Comput. Math. Math. Phys. 3 (1961), 637–657.
[52] Langville, A.N., and Meyer, C.D., Google’s PageRank and Beyond: The Science of Search
Engine Rankings, Princeton University Press, Princeton, NJ, 2006..
[53] Mandelbrot, B.B., The Fractal Geometry of Nature, W.H. Freeman, New York, 1983.
[54] Messiah, A., Quantum Mechanics, John Wiley & Sons, New York, 1976.
[55] Misner, C.W., Thorne, K.S., and Wheeler, J.A., Gravitation, W.H. Freeman, San
Francisco, 1973.
[56] Moon, F.C., Chaotic Vibrations, John Wiley & Sons, New York, 1987.
[57] Murray, R.N., Li, Z.X., and Sastry, S.S., A Mathematical Introduction to Robotic
Manipulation, CRC Press, Boca Raton, FL, 1994.
[58] Nilsson, J.W., and Riedel, S., Electric Circuits, Seventh Edition, Prentice–Hall, Inc.,
Upper Saddle River, N.J., 2005.
[59] Olver, F.W.J., Lozier, D.W., Boisvert, R.F., and Clark, C.W., eds., NIST Handbook of
Mathematical Functions, Cambridge University Press, Cambridge, 2010.
[60] Olver, P.J., Applications of Lie Groups to Diﬀerential Equations, Second Edition,
Graduate Texts in Mathematics, vol. 107, Springer–Verlag, New York, 1993.
[61] Olver, P.J., Introduction to Partial Diﬀerential Equations, Undergraduate Texts in
Mathematics, Springer, New York, 2014.
[62] Ortega, J.M., Numerical Analysis; a Second Course, Academic Press, New York, 1972.
[63] Oru¸c, H., and Phillips, G. M., Explicit factorization of the Vandermonde matrix, Linear
Algebra Appl. 315 (2000), 113–123.
[64] Page, L., Brin, S., Motwani, R., Winograd, T.; The PageRank citation ranking: bringing
order to the web; Technical Report, Stanford University, 1998.
[65] Pearson, K., On lines and planes of closest ﬁt to systems of points in space, Phil. Mag. 2
(1901), 559–572.
[66] Press, W.H., Teukolsky, S.A., Vetterling, W.T., and Flannery, B.P., Numerical Recipes
in C: The Art of Scientiﬁc Computing, Second Edition, Cambridge University Press,
Cambridge, 1995.
[67] Reed, M., and Simon, B., Methods of Modern Mathematical Physics, Academic Press, New
York, 1972.
[68] Royden, H.L., Real Analysis, Macmillan Co., New York, 1988.
[69] Saad, Y., Numerical Methods for Large Eigenvalue Problems, Classics Appl. Math., vol.
66, SIAM, Philadelphia, 2011.
[70] Saad, Y., Iterative Methods for Sparse Linear Systems, Second Edition, SIAM,
Philadelphia, 2003.
[71] Saad, Y., and Schultz, M.H., GMRES: A generalized minimal residual algorithm for
solving nonsymmetric linear systems, SIAM J. Sci. Stat. Comput. 7 (1986), 856–869.

636
References
[72] Salomon, D., Computer Graphics and Geometric Modeling, Springer–Verlag, New York,
1999.
[73] Sapiro, G., Geometric Partial Diﬀerential Equations and Image Analysis, Cambridge Univ.
Press, Cambridge, 2001.
[74] Schumaker, L.L., Spline Functions: Basic Theory, John Wiley & Sons, New York, 1981.
[75] Sommese, A.J., and Wampler, C.W., Numerical Solution of Polynomial Systems Arising in
Engineering and Science, World Scientiﬁc, Singapore, 2005.
[76] Spielman, D., Spectral graph theory, in: Combinatorial Scientiﬁc Computing, U. Naumann
and O. Schenk, eds., Chapman & Hall/CRC Computational Science, Boca Raton, Fl,
2012, pp. 495–524.
[77] Stein, E.M., and Shakarchi, R., Fourier Analysis: An Introduction, Princeton Lectures in
Analysis, Princeton University Press, Princeton, N.J., 2003.
[78] Stewart, J., Calculus: Early Transcendentals, Fifth Edition, Thomson Brooks Cole,
Belmont, CA, 2003.
[79] Strang, G., Introduction to Applied Mathematics, Wellesley Cambridge Press, Wellesley,
Mass., 1986.
[80] Strang, G., Linear Algebra and Its Applications, Third Edition, Harcourt, Brace,
Jovanovich, San Diego, 1988.
[81] Strang, G., and Fix, G.J., An Analysis of the Finite Element Method, Prentice–Hall, Inc.,
Englewood Cliﬀs, N.J., 1973.
[82] Strassen, V., Gaussian elimination is not optimal, Numer. Math. 13 (1969), 354–356.
[83] Tannenbaum, P., Excursions in Modern Mathematics, Fifth Edition, Prentice–Hall, Inc.,
Upper Saddle River, N.J, 2004.
[84] Tapia, R.A., Dennis, J.E., Jr., and Sch¨afermeyer, J.P., Inverse, shifted inverse, and
Rayleigh quotient iteration as Newton’s method, SIAM Rev. 60 (2018), 3–55.
[85] van der Vorst, H.A., Iterative Krylov Methods for Large Linear Systems, Cambridge
University Press, Cambridge, 2003.
[86] Varga, R.S., Matrix Iterative Analysis, Second Edition, Springer–Verlag, New York, 2000.
[87] Walpole, R.E., Myers, R.H., Myers, S.L., and Ye, K., Probability and Statistics for
Scientists and Engineers, Nineth Edition, Prentice–Hall, Inc., Upper Saddle River,
N.J., 2012.
[88] Walter, G.G., and Shen, X., Wavelets and Other Orthogonal Systems, Second Edition,
Chapman & Hall/CRC, Boca Raton, Fl, 2001.
[89] Watkins, D.S., Fundamentals of Matrix Computations, Wiley–Interscience, New York,
2002.
[90] Wilkinson, J.H., The Algebraic Eigenvalue Problem, Clarendon Press, Oxford, 1965.
[91] Wilson, E.B., Jr., Decius, J.C., and Cross, P.C., Molecular Vibrations: The Theory of
Infrared and Raman Vibrational Spectra, Dover Publ., New York, 1980.
[92] Yaglom, I.M., Felix Klein and Sophus Lie, Birkh¨auser, Boston, 1988.
[93] Yale, P.B., Geometry and Symmetry, Holden–Day, San Francisco, 1968.

Symbol Index
Symbol
Meaning
Page(s)
c + d
addition of scalars
8
A + B
addition of matrices
5
v + w
addition of vectors
5, 76
V + W
addition of subspaces
86
f + g
addition of functions
79
c d
multiplication of scalars
8
cv, cA, cf
scalar multiplication
5, 76, 79
AB, A x
matrix and vector multiplication
6
K > 0
positive deﬁnite
157, 398
K ≥0
positive semi-deﬁnite
159
| · |
absolute value, modulus, norm
xvii, 137, 174
∥·∥
norm
131, 133, 144, 178–9
∥·∥
matrix norm
153
∥·∥1
1 norm
145, 155
∥·∥2
Euclidean norm
145, 460
∥·∥∞
max norm
145, 155
∥·∥p
p norm
145
∥·∥p,w
weighted p norm
147
∥·∥F
Frobenius norm
156
v · w
dot product
130
z · w
Hermitian dot product
178
⟨· , · ⟩
inner product
130, 133, 179
⟨⟨· , · ⟩⟩
inner product
133
⟨⟨⟨· , · ⟩⟩⟩
inner product
133
[ ·, · ]
commutator
10, 354
[a, b]
closed interval
xvii, 79
(a, b)
open interval
79
[a, b), (a, b]
half open interval
79
{ x | C }
set of all x subject to conditions C
xvii
x ≃y
approximate equality of real numbers
55
V ≃W
isomorphic vector spaces
356
V / W
quotient vector space
57
© Springer International Publishing AG, part of Springer Nature 2018 
https://doi.org/10.1007/978-3-319-91041-3 
637
P. J. Olver, C. Shakiban, Applied Linear Algebra, Undergraduate Texts in Mathematics, 

638
Symbol Index
#
cardinality (number of elements)
xvii
∪
union
xvii
∩
intersection
xvii
∈
element of
xvii
̸∈
not element of
xvii
⊂, ⊆, ⊊
subset
xvii
⊃
superset
xvii
\
set theoretic diﬀerence
xvii
n
k

binomial coeﬃcient
58, 393
u⊥
orthogonal complement to vector
208
W ⊥
orthogonal complement
217
<)
angle
137
f: X →Y
function
xvii
xn →x
convergent sequence
xvii
≡
equality of functions
xvii
◦
composition
xvii, 352
×
Cartesian product
81
×
cross product
140, 187
∼
agreement at sample points
287
∼W
equivalence relation
87
[ · ]W
equivalence class
87
x, f
mean or average
84, 467
z, z
complex conjugate
173, 177, 390
q
quaternion conjugate
365
n

i=1
summation
xvii
n

i=1
product
xvii
u′, u′′, . . .
space derivatives
xviii, 173
u,
u, . . .
time derivatives
xviii, 600
du
dx , d2u
dx2 , . . .
ordinary derivatives
xviii, 585
∂u
∂x , ∂2u
∂x2 , ∂2u
∂x ∂t , . . .
partial derivatives
xviii
∂x
partial derivative operator
349
∇
gradient
349
∇·
divergence
86

f(x) dx
indeﬁnite integral
xviii
 b
a
f(x) dx
deﬁnite integral
xviii

Symbol Index
639
0
zero vector
7
A−1, L−1
inverse matrix, linear function
31, 355
AT
transpose matrix
43, 502
A−T
inverse transpose matrix
44
A†
Hermitian transpose matrix
181, 444
A+
pseudoinverse matrix
437
A∗, L∗
adjoint matrix, linear function
395, 399
L∗
dual linear function
369
V ∗
dual vector space
350
A
space of analytic functions
84

A | b

augmented matrix
12
aij
matrix entry
xvii, 4
B1
unit ball
148
C
complex numbers
xvii, 173
Cn
n-dimensional complex space
177
Cj
cardinal spline
284
C0
space of continuous functions
83, 179, 347
Cn
space of continuously
diﬀerentiable functions
84
C∞
space of smooth functions
84
coker
cokernel
113, 357
coimg
coimage
113, 357
cos
cosine function
xvii, 176
cosh
hyperbolic cosine function
176
D
derivative operator
348
d(v, w)
distance
146
DA
Gerschgorin domain
420
D∗
A
reﬁned Gerschgorin domain
422
Dk
Gerschgorin disk
420
Dk
kth derivative operator
353
D(k)
space of kth order diﬀerential operators
355
det
determinant
32, 70
diag
diagonal matrix
8
dim
dimension
100
e
base of natural logarithm
xvii
ex
exponential function
xvii
ez
complex exponential
175
eA
matrix exponential
593
ei
standard basis vector
36, 99
Ei
elementary matrix
16
F
function space
79, 80
Gn
complete graph
127, 464

640
Symbol Index
Gm,n
complete bipartite digraph
127
H
quaternions
364
H1
Sobolev norm
136
Hn
Hilbert matrix
57
i =
√
−1
imaginary unit
xvii, 173
i
unit quaternion
364
i
standard basis vector in R3
99
I, In
identity matrix
7
I, IV
identity function
343
img
image
105, 383
Im
imaginary part
173, 177, 391
index
index of linear map
357
j
unit quaternion
364
j
standard basis vector in R3
99
Ja,n, Jλ,n
Jordan block matrix
416, 449
k
unit quaternion
364
k
standard basis vector in R3
99
ker
kernel
105, 378
L2
Hilbert space, inner product, norm
133, 180
Lp
function space p norm
145
Lk
Lagrange polynomial
262
L[v]
linear function
342
L(V, W )
space of linear functions
349
log
natural (base e) logarithm
xvii
loga
base a logarithm
xvii
mA
minimal polynomial
453
max
maximum
xvii
min
minimum
xvii
Mm×n
space of m × n matrices
77, 349
mod
modular arithmetic
xvii
O, Om×n
zero matrix
7
O+
nonnegative orthant
83
pA
characteristic polynomial
415
Pk
Legendre polynomial
228
P(n)
space of polynomials of degree ≤n
77
P(∞)
space of all polynomials
83
ph
phase
xvii, 174
Q
rational numbers
xvii
R
real numbers
xvii
Rn
n-dimensional real Euclidean space
76
R∞
space of inﬁnite sequences
81

Symbol Index
641
Rθ
rotation
349
rank
rank
61
Re
real part
173, 177, 391
S1
unit sphere
83, 148
Sj
sinc function
273
sec
secant function
xvii
sech
hyperbolic secant function
270
sign
sign of permutation
72
sin
sine function
xvii, 176
sinh
hyperbolic sine function
176
span
span
87
supp
support of a function
551
tk
monomial sample vector
265
Tk
Chebyshev polynomial
233
T (n)
space of trigonometric polynomials
of degree ≤n
90, 190
T (∞)
space of all trigonometric polynomials
91
tan
tangent function
xvii
tr
trace
10, 415
V (k)
Krylov subspace
537
Z
integers
xvii
β
B–spline
284
Δ
Laplacian
349, 381
εj
dual basis vectors
350
ζn
primitive root of unity
288
κ
condition number
460
λ
eigenvalue
408
π
area of unit circle
xvii
π
permutation
26, 27
ρ
spectral radius
489
ρxy
correlation
469
σi
singular value
454
σx
standard deviation
468
σxy
covariance
469
ω
relaxation parameter
518
ωk
sampled exponential
287

Subject Index
A
absolute row sum 155, 496, 498, 510
absolute value xvii
abstract vii, viii, viii, xi, xiv, 6, 75–6
acceleration 260, 608, 623
account 476–7
acute 160
adapted basis 426
addition 5, 48–9, 53, 76, 87, 161, 349, 576
associativity of 78
complex 173, 296, 298
matrix 5, 8, 12, 43, 349
real 296
quaternion 364
vector 77, 82, 390
addition formula 89
additive identity 76
additive inverse 8, 76
additive unit 7
adjacency matrix 317
adjoint xii, 112, 342, 357, 395–7, 399, 413
formal 396
Hermitian 181, 205
weighted 397
adjoint function 398
adjoint system 112, 117
adjugate 112
advertising 258
aﬃne equation 479
aﬃne function xi, 245, 239, 343, 370, 555
aﬃne iterative system 488
aﬃne matrix 372, 603
aﬃne subspace 87, 375, 383
aﬃne system 488
aﬃne transformation ix, xii, xiv, 341, 370–3,
377, 419, 603
air 259, 626
airplane 200
Albert Bridge 626
algebra viii, 7, 98
abstract
7
computer 57
Fundamental Theorem of 98, 124, 415
Fundamental Theorem of Linear 114, 461
linear vii, xi, xiii, xv, 1, 75, 114, 126, 183,
243, 341, 403, 506
matrix 7, 99
algebra (continued )
numerical linear 48
polynomial 78
algebraic function 166
algebraic multiplicity 424
algebraic system vii, ix, 341–2, 376, 386, 506,
517, 540
algorithm viii, ix
numerical viii–xi, 48, 129, 183, 199, 400,
403, 475, 536, 547
QR xii, 200, 475, 527, 529, 531–2, 535–6,
538
tridiagonal solution 52, 282
aliasing 286–7, 291
alternating current 320, 629
alternative
Fredholm vii, ix, xi, 183, 222, 226, 312,
330, 352, 377, 631
altitude 269
AM radio 293
amplitude 89, 273, 565, 587, 609, 615, 627
phase- 89, 587, 610
analysis xv, 129, 135
complex 381
data vii, viii, xii, xiii, xv, 80, 126, 129, 135,
301, 403, 463
discrete Fourier xi, xiv, xv, 235, 285
Fourier ix, viii, xii, xv, 75, 78, 99, 135, 173,
180, 183, 188, 227, 285, 287, 476, 555
functional xii
linear ix, x
numerical vii, ix, xii, xiii, xv, 1, 75, 78, 132,
156, 227, 230, 233, 235, 271, 279, 317,
475
Principal Component ix, xii, xiv, xv, 255,
403, 467, 471–2
real 151
statistical 188, 238
symmetry 599
analytic 84, 87, 91
angle xvii, 120, 129, 137–140, 149, 187, 419,
439, 525, 544, 545, 600
acute 160
Euler 203
polar 174
right 140, 184
weighted 138
© Springer International Publishing AG, part of Springer Nature 2018 
https://doi.org/10.1007/978-3-319-91041-3 
643
P. J. Olver, C. Shakiban, Applied Linear Algebra, Undergraduate Texts in Mathematics, 

644
Subject Index
animation viii, xii, 200, 203, 279, 283, 341,
374, 375, 565, 599
481
ansatz 379–80
exponential 379, 390, 567, 576, 621
power 380, 479
resonant 631
trigonometric 609, 618, 623, 625, 630
anti-correlated 469
anti-derivative xviii
anti-diagonal 86, 104
apparatus 565, 608
application viii, 75
applied mathematics vii, ix, x, 1, 48, 230, 475
approximate solution 237, 541
approximation viii, ix, xi, xiv, 220, 227, 261,
475, 542, 599
dyadic 563
Krylov 541–2
least squares 188, 263, 272
linear 324, 329, 341, 388
matrix 462
numerical 220, 235, 403, 416, 467
Pad´e 261
polynomial 266, 279
rank k 462
tangent line 600
trigonometric 271, 273
architecture 483
area 361, 487
argument xviii, 174
arithmetic 173, 413
complex 173
computer
57
ﬂoating point 48, 58
matrix xiii, 1, 8, 77
modular xvii
rational 58
real 173
single precision 461
arithmetic mean 148
arithmetic operation 48, 199, 212, 534, 548
Arnoldi matrix 540
Arnoldi Method xii, 475, 538
Arnoldi vector 538–40, 542, 547
array 56
arrow xvii, 568
art 375, 483
artifact
numerical 206
associative 5, 7–8, 76, 78. 365
astronomy 407
asymptotically stable 405, 478, 490, 493,
579–82, 584, 586–7, 591, 597
globally 405, 488–90, 492–3, 579, 622
Atlanta 504
atom vii, 203, 437, 619, 620
audible frequency 287, 614
audio 102, 285, 293
augmented matrix 12, 24, 36, 60, 66–7
autonomous system 403, 566, 579
autonomous vehicle 616
average 10, 84, 256, 272, 288, 348, 467
axis 373, 419, 600
coordinate 362
imaginary 580–1
principal 465, 472, 487
semi- 487, 498
B
B-spline 284, 567
Back Substitution x, xiii, xiv, 3, 14, 21, 24,
41, 50, 53, 62, 208, 211, 282, 518
bacteria 406
balance 477–8, 499
force 304, 327, 333
voltage 312, 314, 629
ball 236, 244
unit 85, 149–50, 473
banded matrix 55
bandwidth 55
bank 476, 478–9
bar 301, 320, 322–3, 328, 608
base xvii
data 555
basic variable 62–3, 118
basis ix, x, xiii, 75, 99, 100–1, 177, 341, 343,
365, 403, 575, 577, 594
adapted 426
change of 365, 367
dual 350, 352, 369
eigenvector xii, xvi, 183, 423, 427, 432, 434,
438, 446, 448, 480, 523, 528, 566, 572,
618
Jordan 448, 450–1, 453, 480, 488, 576–7
left-handed 103, 202, 222
orthogonal xi, xiii, xv, 184, 189, 194, 201,
214, 235, 266, 403, 435, 446, 551, 611,
618
orthonormal 184, 188, 194–6, 198–9, 201,
204, 213, 235, 248, 288, 432, 437–8,
444, 456–7, 460, 475, 528–9, 538
real 575
right-handed 103, 201–2, 222
standard 36, 99, 111, 184, 261, 343, 349,
356, 426, 449, 450, 529
standard dual 350
wavelet 102, 189, 204, 283, 550, 552, 555–6,
562
basis function 549, 562

Subject Index
645
battery 301, 311, 317–8, 320, 626, 629
beam 120, 279, 301, 322
beat 624, 628
bell-shaped 284
bend 322
benzene 620
bidiagonal 52–3, 402
bilinear 10, 130, 133, 156
bilinear form 156
bilinear function 347, 354
binary 297, 561, 563
Binet formula 483, 485
binomial coeﬃcient 58–9, 380, 393
binomial formula 176, 393
biology ix, 1, 403, 407, 475, 499
bipartite 127
bit reversal 297
block
Jordan 416–7, 449–50, 453, 598
block diagonal 35, 74, 128, 171, 420, 449,
535, 598
block matrix 11, 35, 603
block upper triangular 74, 535
blue collar 504
body 200, 203, 259, 301, 341, 439
bolt 323
bond 620
Boston 504
boundary condition 302
clamped 280, 283–4
natural 280, 283–4
periodic 280, 283–4
boundary point 503
boundary value problem x, xi, xv, 54, 75, 92,
99, 136, 183, 222, 235, 322, 389, 397,
399, 541–2
linear vii, 342, 376–7, 386
bounded 219, 349, 380, 603
bounded support 557
bowl 236
box function 549, 555, 559
brain 287
bridge 625–6
Britain 625
bug 505
building 120, 322, 324
business 504
C
C++ 14
CAD 279, 283
calculator 48, 260
calculus vii, x, 83, 231, 240, 341, 580
Fundamental Theorem of 347, 356, 606
calculus (continued )
multivariable x, 235, 242–3, 342, 441, 545,
582
vector 353, 365
calculus of variations xii, 235
canonical form 368
Jordan xii, xiii, 403, 447, 450, 490, 525,
598
capacitance 626
capacitor 311, 628
car 196, 254, 467
carbon 406, 434
carbon tetrachloride 620
cardinal spline 284
cardinality xvii
Cartesian coordinate 101
Cartesian product 81, 86, 133, 347, 377
category theory viii
Cauchy-Schwarz inequality 129, 137, 142–3,
179, 469
Cayley transform 204
Cayley–Hamilton Theorem 420, 453
Cayley–Klein parameter 203
CD 183, 283, 287
ceiling 322
center 373, 588, 589–91
center of mass 439
center manifold 605
Center Manifold Theorem 604
center subspace 604
CG — see Conjugate Gradient
chain
Jordan 447–8, 450–2, 488, 576–7, 579, 581,
603–4
Markov xii, 475, 499–502
mass–spring xi, xiv, 301, 309, 317, 399,
403, 565, 608, 610, 619, 628, 630
null Jordan 447, 451
chain rule 301
change of basis 365, 367
change of variables 172, 232, 234
chaos 611
characteristic equation 379–80, 390, 408–9,
413, 420, 453, 475, 567, 583, 586, 621,
627
generalized 435, 618
characteristic polynomial 408, 415, 453, 475
characteristic value 408
characteristic vector 408
charge 313
Chebyshev polynomial 233
chemistry 48, 139, 203, 403, 407, 608
Chicago 504
chlorine 620
Cholesky factorization 171

646
Subject Index
circle 176, 329, 339, 363, 371, 438
unit xvii, 132, 288, 442, 530
circuit xiii, 121–2, 124–6, 126, 312, 403, 608
electrical viii, xii, xiv, 122, 129, 129, 196,
235–6, 301, 628
fault-tolerant 464
LC 630
RLC 626, 629
circuit vector 312
circulant matrix 282, 436
circular motion 616
city 504–5
clamped 280, 283–4
clarinet 626
class
equivalence 87
classical mechanics 341, 388, 583
climate 406
clockwise 360
closed xvii, 79, 146, 151
closed curve 280, 283
closest point xi, 183, 235, 238, 245–6, 298
closure 82, 106
cloudy 499, 501
CMKY 470
code
error correcting 464
codomain xvi, xvii, 105, 342, 376, 383, 396,
618
co-eigenvector 416, 503, 525
coeﬃcient
binomial 58–9, 380, 393
constant x, 363, 376–7, 390
Fourier 289, 291, 294, 296–7, 470
frictional 499, 504
leading 367
least squares 266
undetermined 372, 385–6, 500, 623
wavelet 470
coeﬃcient function 353
coeﬃcient matrix 4, 6, 63, 157, 224, 235, 241,
343, 476, 479, 484, 499, 508, 528, 531,
566, 575, 591, 606, 608, 618, 630
cofactor 112
coﬀee 486
coimage x, 75, 113–5, 117, 221, 223–4, 357,
434, 457
cokernel x, 75, 113–4, 116, 118, 125, 221–2,
312, 357, 434, 461, 470, 501, 542, 626,
631
collapse 626
collocation 547
colony 406
color 463, 470
column 4, 6, 7, 27, 43, 45, 94, 114, 162, 201
pivot 56
orthonormal 444, 455–6
zero 59
column interchange 57
column operation 72, 74
column permutation 418
column space 105, 383
column sum 10, 419, 501, 502
column vector 4, 6, 46, 48, 7, 130, 350–17
combination
linear 87, 95, 101, 287, 342, 388, 599, 618
communication network 464
commutation relation 355
commutative 5, 6, 8, 76, 173, 352, 360
commutator 10, 354, 601
commuting matrix 10, 601
compact 149
company 258
compatibility condition 96, 106, 183, 222
compatible ix, xi, 8, 11, 62, 95, 224
complement
orthogonal 217–9, 221, 431, 631
complementary dimension 218
complementary subspace 86, 105, 217–8, 221
complete bipartite digraph 127
complete eigenvalue xvi, 412, 424, 493, 531,
577, 588
complete graph 127, 464
complete matrix xvi, 403, 424–6, 428, 430–2,
444, 450, 480, 484, 490, 493, 522, 566,
572, 575, 603
complete monomial polynomial 268
complete the square xi, 129, 166, 240, 437
completeness 562
complex addition 173, 296, 298
complex analysis 381
complex arithmetic 173
complex conjugate 173, 177, 205, 390, 444,
452
complex diagonalizable 427
complex eigenvalue xiv, 412, 415–6, 421, 423,
430, 433, 496, 525, 535, 538, 578, 587
complex eigenvector ix, xii, xiii, 403, 408–10,
429, 443, 446, 448, 473, 475, 480, 484,
503, 522, 525, 527, 537, 539, 560, 565,
609
complex exponential 175, 180, 183, 192, 285,
287, 390, 549
complex inequality 177
complex inner product 184
complex iterative equation 478
complex linear function 342
complex linear system xiv, 566
complex matrix 5, 181, 212, 226, 536, 566
complex monomial 393

Subject Index
647
complex multiplication 173, 296, 298
complex number xvii, 80, 129, 173
complex plane 173, 420, 580
complex root 114, 390, 621
complex scalar 177, 476
complex solution xiv, 391, 575
complex subspace 298, 430, 452
complex trigonometric function 176–7
complex-valued function 129, 173, 177, 179,
391
complex variable 172
complex vector 129, 433
complex vector space xi, xiv, 76, 129, 177,
179, 287, 342, 390
component
connected 124, 463
principal ix, xii, xiv, xv, 255, 403, 467,
471–2
composition 79, 352
compound interest 476, 478–9
compressed sensing 238
compression 99, 102, 183, 272, 293–4, 462,
552, 554–6, 558, 561–2
computer vii, xvi, 48, 56–7, 260–1, 291, 461,
513, 561
binary 297
parallel 513
serial 513
computer aided design 279, 283
computer algebra 57
computer arithmetic 57
computer game xii, 200, 341, 375
computer graphics viii, xi, 52, 200, 203, 279,
283, 286, 341, 358, 374–5, 565, 599
computer programming 14, 28
computer science vii, ix, xv, 126, 463
computer software xvi, 404
computer vision 375, 499
condition
boundary 280, 283–4, 302
compatibility 96, 106, 183, 222
initial 404–5, 480, 570–2, 593, 609–10
condition number 57, 460, 466
spectral 525, 591
conditioned
ill- 56–7, 211, 249, 276–7, 461–2, 525
conductance 313, 314, 317, 320
cone 159–60
conjugate
complex 173, 177, 205, 390, 444, 452
quaternion 365
conjugate direction 543, 545, 548
Conjugate Gradient 476, 542, 544–5, 548
conjugate symmetry 179, 184
conjugated 390–1
connected 121, 124, 144, 463
conservation of energy 585
constant 348, 389
gravitational 259
integration 404
piecewise 551
constant coeﬃcient x, 363, 376–7, 390
constant function 78
constant polynomial 78
constant solution 615
constant vector 588
constitutive equation 303
constitutive relation 302, 312, 327
constrained maximization principle 442–3
constrained optimization principle 441
continuous dynamics xii, 565
continuous function xi, 83, 133, 150, 179,
219–20, 274, 347, 349, 377, 559
continuous medium ix, 565
continuously diﬀerentiable 84, 136, 379
continuum mechanics xi, 235–6, 351, 399
continuum physics 156
contraction 600
control system vii, xv, 76, 99, l06, 376
control theory 235
convergence xv, xvii, 91, 146, 151, 394, 475,
489, 506, 510, 514, 518, 545, 559
uniform 562
convergence in norm 151
convergent 91, 146, 151, 394
convergent matrix 488–9, 495–6, 508, 517
convergent sequence 86, 551
convex 150
cookbook viii
coordinate 101, 151, 188–9, 350, 472
Cartesian 101
polar 90, 136, 174
principal 472, 477
radial 383
coordinate axis 362
coordinate plane 362
coplanar 88
core xiii
corporation 504
correlation 469–70
cross- 252
correspondence
Electrical–Mechanical 321, 628
cosine xvii, 138, 175–6, 183, 285, 581, 609–10
law of 139
counterclockwise 359, 600
country 504
covariance 469, 470, 472
covariance matrix 163, 470–1, 473
Cramer’s Rule 74

648
Subject Index
criterion
stopping 544
critical point 240, 242
critically damped 622, 629
cross polytope 149
cross product 140, 187, 239, 305, 602
cross-correlation 252
crystallography xii, 200, 358
cube 126, 318
cubic 260, 267, 280, 563
piecewise 263, 269, 279–80
curl 349
current 122, 235, 311–3, 315, 318–20, 626–7
alternating 320, 629
Current Law 313–4, 317
current source 301, 313–4, 317–8, 320, 629
curve 86, 279, 568
closed 280, 283
level 585
smooth 279
space 283
curve ﬁtting 283
cylinder 85
D
damped 565, 498, 621, 623
critically 622, 629
damping viii, ix, xii, 302, 620
data xiv, 235, 252, 467, 471
experimental 237–8
high-dimensional 471
Lagrange 284
missing 471
normally distributed 473
data analysis vii, viii, xii, xiii, xv, 80, 126,
129, 135, 301, 403, 463
data base 555
data compression 99, 183, 294, 462
data ﬁtting xiii, 132, 237, 254
data matrix 462, 467, 470–1, 473
normalized 470, 473
data mining 467
data point xi, 235, 237, 254–5, 272, 283, 467,
470, 474
normalized 470–1
data science 1, 235
data set 188, 462, 472–3
data transmission 272
data vector 254
Daubechies equation 558, 561
Daubechies function 559–60
Daubechies wavelet 555, 562
daughter wavelet 550, 556, 563
day 475, 477
decay 627, 629
exponential 405, 565, 580–1, 621
radioactive 257, 404, 406
decay rate 257, 404, 622
decimal 561
decomposition
polar 439
Schur xii, xiii, 403, 444–6
spectral 440, 598
Singular Value xii, 403, 455, 457, 461, 473
decrease
steepest 545, 583
deer 406, 479
deﬁnite 583
negative 159–60, 171, 581, 583
positive xi, xiii, xiv, 129, 156, 159–61, 164–5,
167, 170–1, 181–2, 204, 235, 241–2,
244, 246, 252, 301, 304, 309, 313, 316,
327, 396, 398, 432, 439, 443, 473, 528,
531, 542, 544, 581, 583, 608, 618, 622
deﬁnite integral xviii
deﬂation 420, 526
deformation 438–9
degree 78, 98, 161, 266, 273, 453, 537
degree matrix 317
denoising 102, 183, 293, 555, 562
dense 220
Department of Natural Resources 479
dependent
linearly 93, 95–6, 100, 571
deposit 476, 479
derivative x, xviii, 84, 177, 476, 542, 594
ordinary xviii
partial xviii, 242, 349
derivative operator 348, 353
descent 545, 549
design xi, 235, 483
font 203
determinant x, xiii, xiv, 1, 32, 70, 72, 103,
158, 170–1, 202, 409, 413, 415–7, 519,
586, 590–1, 596
Wronskian 98
zero 70
determinate
statically 307, 333
deviation 468, 470
standard 468–9
principal standard 472
DFT xi, xv, 183, 272, 285, 289, 295
diagonal 7, 43, 86, 104, 444, 449
block 35, 74, 128, 171, 420, 449, 535
main 7, 43, 52, 492
oﬀ- 7, 32
sub- 52, 492, 535
super- 52, 449, 492, 535

Subject Index
649
diagonal entry 10, 47, 70, 168, 205, 420, 445,
455–6, 600
diagonal line 358
diagonal matrix 7–8, 35, 41–2, 45, 85, 159,
168, 171, 204, 304, 313, 327, 400, 408,
425–6, 437, 439–40, 446, 453, 455, 472,
484, 528, 530, 608, 618, 622
diagonal plane 362
diagonalizable xvi, 403, 424, 426, 428, 437,
450, 520
complex 427
real 427, 432
simultaneously 428–9
diagonalization xii, 438, 456, 484
diagonally dominant 281, 283, 421–2, 475,
498, 510, 512, 516, 584
diamond 149
diﬀerence
ﬁnite 317, 521, 547
set-theoretic xvii
diﬀerence equation 341
diﬀerence map 436
diﬀerence polynomial 268
diﬀerentiable 84, 136, 348
inﬁnitely 84
nowhere 561
diﬀerential delay equation 341
diﬀerential equation ix, xii–xv, 1, 48, 106,
129, 132, 156, 183, 222, 302, 341, 351,
403, 479, 541, 556, 599
Euler 380, 393
homogeneous 84, 379, 381, 392, 567, 609,
621
inhomogeneous 84, 606, 623, 627
linear vii, viii, xiv, 84, 342, 376, 378
matrix 590, 592
ordinary vii, x, xi, 91, 98–9, 101, 106–7,
301, 322, 342, 376, 379–80, 385, 390,
403–4, 407, 435, 476, 479, 566–7, 570,
576, 579, 604, 606, 608, 627, 630
partial vii, ix–xii, xv, xvii, 99, 101, 106,
129, 173, 200, 227, 230, 301, 322, 342,
376, 381, 475, 536, 542, 547, 565
system of ﬁrst order 565–7, 570–2, 577,
585, 605
system of ordinary ix, xii–xv, 342, 530, 566,
584, 571, 579, 592, 608, 618, 630
diﬀerential geometry 235, 381
diﬀerential operator xi, 317, 341–2, 355,
376–7, 379–80, 384
ordinary 353
partial 381
diﬀerentiation 348, 355–6, 594, 606
numerical 271
digit 297
digital image 294, 462
digital medium 183, 285
digital monitor 286
digraph 122, 311–2, 316, 327, 462
bipartite 127
complete 127
connected 124
cubical 126
internet 126, 463, 502
pentagonal 125
simple 467, 502
weighted 311, 502
dilation coeﬃcient 557
dilation equation 555–6, 558, 561
dimension ix, x, xiii, 76, 100, 114, 177, 341,
463
complementary 218
dimensional reduction 472
dinner 626
direct method 475, 536
directed edge 122, 463
directed graph — see digraph
direction 630
conjugate 543, 545, 548
natural 631
null 159–60, 164, 166
principal 438–9
disconnected 128, 463
discontinuous 562
discrete dynamics xii, 475
discrete Fourier analysis xi, xiv, xv, 235, 285
discrete Fourier representation 287, 294
Discrete Fourier Transform xi, xv, 183, 272,
285, 289, 295
Inverse 289
discriminant 158, 586, 590
discretization 317
disk 371
Gershgorin 420, 503
unit 136, 371, 503
displacement 301–2, 320, 323, 329, 608, 629
displacement vector 302, 312, 325, 620
distance 8, 131, 146, 235, 239, 245–6, 254,
361, 372, 467
distinct eigenvalues 430, 453, 586
distributed
normally 473
distribution 468
distributive 8, 76, 343, 364
divergence 86, 349
division 48, 53, 79, 174, 261, 536
DNR 479
dodecahedron 127
Dolby 293
dollar 477, 486

650
Subject Index
domain xvii, 105, 342, 376, 396, 618
Gershgorin 420, 422
dominant
strictly diagonally 281, 283, 421–2, 475,
498, 510, 512, 516, 584
dominant eigenvalue 441, 495, 523–5
dominant eigenvector 523–4, 529
dominant singular value 454, 460
dot notation viii
dot product xiii, 129, 137, 146, 162, 176, 178,
193, 201, 265, 351, 365, 396, 431–3,
455, 550
Hermitian 178, 205, 288, 433, 444, 446
double eigenvalue 411, 416
double root 411, 576
doubly stochastic 505
downhill 236
drone 200
dual basis 350, 352, 369
dual linear function 369, 398
dual map 395
dual space 350, 358, 369, 395
dump 406
DVD 183
dyadic 561–3
dynamical motion xii, 608
dynamical system viii, xii, xiii, xv, 301–2,
396, 403, 407, 565, 583, 591, 603
inﬁnite-dimensional 565
dynamics xv, 129, 183, 605, 628
continuous xii, 565
discrete xii, 475
gas 565
molecular 48
nonlinear 565, 604, 616
quantum 173
E
Earth 315
echelon 59, 60, 62, 95, 115–6
economics vii, ix, 235, 407, 499
edge 120, 122, 311, 463, 502
directed 122, 463
eigendirection 439
eigenequation 408
eigenfunction 183
eigenline 429, 580, 587–8
eigenpolynomial 408
eigensolution 565–6, 572, 576, 581, 603, 610,
619
stable 603
unstable 603
eigenspace 411, 413, 424, 430, 440, 456, 493
generalized 631
zero 434
eigenstate 437
eigenvalue vii, ix, xii–xv, 200, 403, 408–9,
417, 420, 423, 430, 437, 440, 444–5,
447, 454, 462, 473, 475, 480, 484, 489,
522, 527, 532, 535, 539, 560, 565–6,
572, 577, 581, 586, 591, 594, 596, 608,
611
complete xvi, 412, 424, 493, 531, 577, 588
complex xiv, 412, 415–6, 421, 423, 430,
433, 496, 525, 535, 538, 578, 587
distinct 430, 453, 586
dominant 523–5
double 411, 416
generalized 443, 435, 492, 618, 630–1
imaginary 581, 588, 603
incomplete 412, 578, 581
intermediate 442
Jacobi 519
largest 441, 495
multiple 430
negative 582
positive 582
quadratic 493
real 413, 423, 430, 432, 586
repeated 417
simple 411, 416, 493, 531, 535, 537
smallest 441
subdominant 493, 502, 524, 526
zero 412, 421, 433–4, 581, 615
eigenvalue equation 408, 480, 493, 566, 610,
618
generalized 435, 618
eigenvalue matrix 530, 531
eigenvalue perturbation 591
eigenvector ix, xii, xiii, 403, 408–10, 429, 443,
446, 448, 473, 475, 480, 484, 503, 522,
525, 527, 537, 539, 560, 565, 609
co- 416, 503, 525
complex 413, 425, 577–8, 587, 603
dominant 524, 529
extreme 441–2
generalized 435, 447–8, 600, 631
generalized null 619
left 416, 503, 525
non-null 434, 454
null 433–4, 615, 618
orthogonal 432, 436, 611
orthonormal 437
probability 501–3
real 413, 490, 496, 523, 535, 537, 577–8,
588
unit 471, 493, 496

Subject Index
651
eigenvector basis xii, xvi, 183, 423, 427, 432,
434, 438, 446, 448, 480, 523, 528, 566,
572, 618
eigenvector matrix 531
Eigenvektor 408
Eigenwert 408
elastic
bar 301, 322, 608
beam 279
body 439
elastic deformation 438
elasticity xii, 358, 381
electric charge 313
electrical circuit viii, xii, xiv, 122, 129, 129,
196, 235–6, 301, 628
electrical energy 319
electrical engineering 173
Electrical–Mechanical Correspondence 321,
628
electrical network xi, xv, 120, 301, 311–2,
327, 626, 630
electrical system 183
electricity xi, 311, 315, 403
electromagnetic wave 626
electromagnetism 80, 173, 236, 381
electromotive force 311
electron 311–3
element xvi
ﬁnite 220, 235, 400, 521, 541, 547
real 391
unit 148
zero 76, 79, 82, 87, 140, 342
elementary column operation 72, 74
elementary matrix 16–7, 32, 38–9, 73, 204,
360, 362
inverse 17, 38
elementary reﬂection matrix 206, 210, 418,
532
elementary row operation 12, 16, 23, 37, 60,
70, 418, 512
elementary symmetric polynomial 417
Elimination
complex Gaussian 412
Gauss–Jordan 35, 41, 50
Gaussian ix–xiv, 1, 14, 24, 28, 40, 49, 56–7,
67, 69, 72, 102, 129, 167, 208, 237,
253, 378, 407, 409, 475, 506, 508–9,
536
regular Gaussian 14, 18, 171, 268
tridiagonal Gaussian 5, 42
ellipse 363, 371, 438–9, 466, 487, 496, 498
ellipsoid 363, 438–9, 465, 472
elliptic system 542
elongation 302–3, 309, 324, 327, 608
elongation vector 302, 325
ending node 311, 322
ending vertex 122
energy 320, 341, 583, 585
electrical 319
internal 309
potential 235–6, 244, 309, 320, 583
spectral 437
energy function 309
engine
search 499, 502
engineer xiii, xviii
engineering vii, ix, xiii, 1, 156, 227, 235, 301,
313, 381, 402
electrical 173
entry xvii, 4, 592
diagonal 10, 47, 70, 168, 205, 420, 445,
455–6, 600
nonzero 59, 501
oﬀ-diagonal 7, 32, 252, 420
zero 52, 59, 449, 501
envelope 625
equal 4
equation 236, 506
aﬃne 479
beam 394
characteristic 379–80, 390, 408–9, 413, 420,
453, 475, 567, 583, 586, 621, 627
complex 173
constitutive 303
Daubechies 558, 561
diﬀerence 341
diﬀerential ix, xii–xv, 1, 48, 84, 106, 129,
132, 156, 183, 222, 302, 341, 351, 403,
479, 541, 556, 599, 625, 627
diﬀerential delay 341
dilation 555–6, 558, 561
eigenvalue 408, 480, 493, 566, 610, 618
equilibrium 314
Euler 380, 393
Fibonacci 481, 486–7
ﬁxed-point 506, 509, 546, 559, 563
Fredholm integral 377
functional 556
generalized characteristic 435, 618
Haar 555, 563
heat 394
homogeneous diﬀerential 84, 379, 381, 392,
567, 609, 621
inhomogeneous diﬀerential 84, 606, 623,
627
inhomogeneous iterative 479
integral vii, 76, 106, 183, 341–2, 376–8, 556
integro-diﬀerential 629
iterative 476–8, 479
linear diﬀerential vii, viii, xiv, 84, 342, 376,
378

652
Subject Index
equation (continued )
Laplace 381, 383, 385, 393
matrix diﬀerential 590, 592
Newtonian 618, 621
normal 247, 251, 272, 458
ordinary diﬀerential vii, x, xi, 91, 98–9,
101, 106–7, 301, 322, 342, 376, 379–80,
385, 390, 403–4, 407, 435, 476, 479,
566–7, 570, 576, 579, 604, 606, 608,
627, 630
partial diﬀerential vii, ix–xii, xv, xvii, 99,
101, 106, 129, 173, 200, 227, 230, 301,
322, 342, 376, 381, 475, 536, 542, 547,
565
Poisson 385, 390, 521
polynomial 416
quadratic 64, 166, 621
Schr¨odinger 173, 394
system of ﬁrst order diﬀerential 565–7,
570–2, 577, 585, 605
system of ordinary diﬀerential ix, xii–xv,
342, 530, 566, 584, 571, 579, 592, 608,
618, 630
Volterra integral 378
weighted normal 247, 252, 256, 317
equilateral 328, 500, 619
equilibrium ix, xi, 236, 301, 309, 403, 476,
565, 581, 587, 605, 618, 621, 629
stable 235–6, 301–2, 579, 590, 605, 615
unstable 235–6, 301, 590
equilibrium equation 314
equilibrium mechanics 235
equilibrium point 568, 579
equilibrium solution 301, 405, 476, 479, 488,
493, 565, 579, 597, 622
equivalence class 87
equivalence relation 87
equivalent 2, 87, 150
equivalent norm 150, 152
error ix, 237, 254
experimental 237, 467
least squares 235, 251–2, 271, 458
maximal 261
measurement 256, 470
numerical 249, 523, 536
round-oﬀx, 55, 199, 206, 544
squared 255–6, 260, 272, 274
weighted 252, 256
error correcting code 464
error-free xix
error function 274
error vector 254, 508, 514
Euclidean geometry 99, 130, 137, 203
Euclidean isometry 373
Euclidean matrix norm 460–1, 497
Euclidean norm xiii, 130, 142, 172, 174, 224,
236, 250, 455, 458, 460, 468, 473, 489,
524, 532, 538, 544, 546
Euclidean space x, xi, 75–7, 94, 99, 130, 146,
341, 403, 426, 600
Euler angle 203
Euler diﬀerential equation 380, 393
Euler–Rodrigues formula 602
Euler’s formula 125, 175, 392
evaluation 341
even 86–7, 286
executive 504
exercise xvi
existence 1, 380, 383–4, 401, 479, 566, 593, 610
expander graph 464
expected value 468
expenditure 258
experiment 254, 293
experimental data 237–8
experimental error 237, 467
exponential
complex 175, 180, 183, 192, 285, 287, 390,
549
matrix ix, xii, 565, 593–4, 597, 599, 601–2,
606
sampled 285–7, 415, 436
exponential ansatz 379, 390, 567, 576, 621
exponential decay 405, 565, 580–1, 621
exponential function 175, 261, 264, 275, 277,
403–4, 428, 565–6, 578, 622
exponential growth 258–9, 565, 586, 603–4
exponential series 596, 599, 601–2
exponential solution 381, 408
external force 110, 301, 309, 320, 335, 384,
388, 565, 605, 608, 623, 627, 630
extinction 406
extreme eigenvector 441–2
F
face 126, 467
factor 415
shear 361
factorization
Cholesky 171
Gaussian 1, 419, 437
LDLT xi, 45, 167, 437, 542
LDV 41
LU x, xiv, xvi, 1, 18, 20, 41, 50, 70, 268,
501, 536, 542
matrix 1, 171, 183, 205, 536
MMT 171
permuted LDV 42
permuted LU 27–8, 60, 70
QR xi, 205, 210, 522, 529, 539
spectral 437

Subject Index
653
farmer 504
Fast Fourier Transform xi, 235, 296
father 504
fault-tolerant 464
FBI 555
FFT xi, 235, 296
Fibonacci equation 481, 486–7
Fibonacci integer 482–3, 485
Fibonacci matrix 412, 428
Fibonacci number 481, 483, 486
ﬁeld 76
skew 364
vector 81, 574
ﬁnance vii, ix, 1, 403, 407, 475
ﬁngerprint 555
ﬁnite diﬀerence 317, 521, 547
ﬁnite-dimensional xiii, xiv, 101, 149, 213,
219, 248, 356
ﬁnite element 220, 235, 400, 521, 541, 547
ﬁrst order system 605
ﬁtting
curve 283
data xiii, 132, 237, 254
ﬁxed point 493, 506, 509, 546, 559, 563
stable 493
ﬂoating point 48, 58
ﬂoor 322
ﬂow 313
ﬂuid 574
gradient 581, 585
ﬂower 482, 504
ﬂuctuation 467, 470
ﬂuid ﬂow 574
ﬂuid mechanics 48, 80, 173, 236, 381, 400,
565
focus 587-9, 591
FOM xii, 476, 541, 546, 570
font viii, 203, 283
force viii, 110, 565
electromotive 311
external 110, 301, 309, 320, 335, 384, 388,
565, 605, 608, 623, 627, 630
frictional 621–2
gravitational 80, 259, 311
internal 303, 320, 327, 333
mechanical 327
periodic xii, 565, 623–4, 626, 630–1
vibrational 630
force balance 304, 327, 333
force vector 327
forcing frequency 624, 630
fork
tuning 624
form
bilinear 156
canonical 368
Jordan canonical xii, xiii, 403, 447, 450,
490, 525, 598
linear 161
Minkowski 375
phase-amplitude 89, 610
quadratic xi, 86, 157–61, 166–7, 170, 241,
245, 346, 437, 440–2, 583
row echelon 59, 60, 62, 95, 115–6
triangular 2, 14
formal adjoint 396
formula xvii
addition 89
Binet 483, 485
binomial 176, 393
Euler 125, 175, 392
Euler–Rodrigues 602
Gram–Schmidt 194
orthogonal basis 189, 611
orthonormal basis 188
polarization 160
Pythagorean 188, 460
Rodrigues 228, 277
quadratic 166
Sherman–Morrison–Woodbury 35
formulation
Galerkin xii, 200
Fortran 14
Forward Substitution xiii, xiv, 3, 20, 49, 53,
282, 518
foundation xiii
Fourier analysis ix, x, xii, xv, 75, 78, 99, 135,
173, 180, 183, 188, 227, 285, 287, 476,
555
discrete xi, xiv, xv, 235, 285
Fourier basis function 549
Fourier coeﬃcient 289, 291, 294, 296–7, 470
Fourier series 91, 191, 549, 553
Fourier transform 376, 559
discrete xi, xv, 183, 272, 285, 289, 295
fast xi, 235, 296
fragment
skeletal 393
framework 120, 322
Fredholm Alternative vii, ix, xi, 183, 222,
226, 312, 330, 352, 377, 631
Fredholm integral equation 377
free space 394
free variable 62, 63, 67, 96, 108, 119–20, 315
frequency 273, 578, 609, 623, 629
audible 287, 614
forcing 624, 630
high- 183, 287, 291, 294, 555

654
Subject Index
frequency (continued )
low- 291, 294
natural 565, 611, 624–5, 630–1
resonant 626–7, 632
vibrational 610, 621, 624
friction xii, 302, 565, 608, 620, 626–7, 629
friction matrix 622
frictional coeﬃcient 499, 504
frictional force 621–2
Frobenius norm 156, 466
fruit 482
full pivoting 57
Full Orthogonalization Method xii, 476, 541,
546
function viii, xiv, xvii, xviii, 75, 78–9, 285,
341, 343, 440, 475
adjoint 396–8
aﬃne xi, 245, 239, 343, 370, 555
algebraic 166
analytic 84, 87, 91
basis 549, 562
bilinear 347, 354
box 549, 555, 559
coeﬃcient 353
complex exponential 175, 180, 183, 192,
285, 287, 390, 549
complex linear 342
complex trigonometric 176–7
complex-valued 129, 173, 177, 179, 391
constant 78
continuous xi, 83, 133, 150, 179, 219–20,
274, 347, 349, 377, 559
continuously diﬀerentiable 84, 136, 379
cosine xvii, 138, 175–6, 183, 285, 581, 609–10
Daubechies 559–60
discontinuous 562
dual 369, 398
energy 309
error 274
even 86–7
exponential 175, 261, 264, 275, 277, 403–4,
428, 565–6, 578, 622
Fourier basis 549
generalized 351
Haar 549, 555, 559
Hamiltonian 583
harmonic 381
hat 556, 563
homogeneous 161
hyperbolic 176
identity 343, 355
inﬁnitely diﬀerentiable 84
integrable 84
inverse linear 355
invertible linear 387
function (continued )
linear ix, xi–xiv, xvi, xvii, 239, 341–2, 349–50,
352, 355, 358, 369–70, 378, 383, 395–6,
599
matrix-valued 592, 594, 606
mean zero 84
non-analytic 84, 87
nonlinear 324, 341
nowhere diﬀerentiable 561
odd 87
orthogonal xi, 183, 559–60
periodic 86, 611
piecewise constant 551
piecewise cubic 263, 269, 279
positive deﬁnite 398–9, 401
power 320
quadratic xi, xiii, 235, 239–41, 259, 274,
401, 545, 582–3
quasi-periodic 565, 611
rational 166, 261, 442
real linear 342, 391
sample 79–80, 235, 285
scaling 399, 549, 555, 558, 559–60, 563
self-adjoint 398–9, 436
sinc 273
sine xvii, 176, 183, 269, 285, 581, 610
skew-adjoint 400
smooth 84
special 200
translation 346
trigonometric xi, xiv, xvii, 89, 164, 175–6,
183, 235, 272, 292, 578, 580–1
unit 148
vector-valued 80, 98, 136, 341, 605
wave 173, 341
zero 79, 83, 134, 343, 405
function evaluation 341
function space x, xiii, xv, 79, 80, 83, 133,
146, 163, 185, 190, 220, 224, 301, 341,
396, 401, 541
function theory 135
functional analysis xii
functional equation 556
fundamental subspace 114, 183, 221
Fundamental Theorem of Algebra 98, 124,
415
Fundamental Theorem of Calculus 347, 356,
606
Fundamental Theorem of Linear Algebra
114, 461
Fundamental Theorem of Principal
Component Analysis 472
G
Galerkin formulation xii, 200

Subject Index
655
game xii, 200, 341, 375
gas dynamics 565
Gauss–Jordan Elimination 35, 41, 50
Gauss–Seidel matrix 514–5, 519
Gauss–Seidel Method xii, 475, 512, 514, 517,
519–20
Gauss–Seidel spectral radius 520
Gaussian Elimination ix–xiv, 1, 14, 24, 28,
40, 49, 56–7, 67, 69, 72, 102, 129, 167,
208, 237, 253, 378, 407, 409, 475, 506,
508–9, 536
complex 412
regular 14, 18, 171, 268
tridiagonal 5, 42
Gaussian factorization 1, 419, 437
general position 375
general relativity vii, 341
general solution 91, 107, 111, 480, 606, 618,
625
generalized characteristic equation 435, 618
generalized eigenspace 631
generalized eigenvalue 443, 435, 492, 618,
630–1
generalized eigenvector 435, 447–8, 600, 619,
631
generalized function 351
Generalized Minimal Residual Method xii,
476, 546–7, 549
generator 629
inﬁnitesimal 599–602
generic 48
genetics 475, 504
genotype 504
geodesic 235
geodetic survey 171
geometric mean 148
geometric modeling 279
geometric multiplicity 424
geometric series 499
geometry viii, xi, xii, 10, 75, 99, 129, 183,
200, 202, 238, 341, 358, 464, 472, 565,
599
diﬀerential 235, 381
Euclidean 99, 130, 137, 203
geophysical image 102
Gershgorin Circle Theorem 420, 475, 503
Gershgorin disk 420, 503
Gershgorin domain 420, 422
Gibbs phenomenon 562
glide reﬂection 375
global minimum 240
globally asymptotically stable 405, 488–90,
492–3, 579, 622
globally stable 405, 579
GMRES xii, 476, 546–7, 549
golden ratio 483
Google 126, 463, 502
gradient 349, 545, 582
conjugate 476, 542, 544–5, 548
gradient descent 545, 549
gradient ﬂow 581, 585
Gram matrix 129, 161–3, 182, 246, 255, 274,
301, 309, 316, 327, 351, 398, 403, 439,
454, 456, 462, 470, 543, 622, 630
weighted 163, 247
Gram–Schmidt formula 194
Gram–Schmidt process ix, xi, xv, 183, 192,
194–5, 198–9, 205, 208, 215, 227, 231,
249, 266, 445, 475, 527, 529, 538
stable 199, 538
graph 120, 303, 311, 317, 463
complete 127, 464
connected 121, 144
directed — see digraph
disconnected 128, 463
expander 464
planar 125
random 463
simple 120, 311, 463
spectral xiv, xv, 462–3
weighted 311
graph Laplacian xv, 301, 317–8, 462, 464
graph spectrum 462, 467
graph theory viii, x, xiv, 126
graphics xi
computer viii, xi, 52, 200, 203, 279, 283,
286, 341, 358, 374–5, 565, 599
gravitational constant 259
gravitational force 80, 259, 311
gravitational potential 311
gravity 235, 259, 302, 307, 327, 333, 583, 613
gray scale 470
grid 317, 521–2
ground 327
grounded 315, 318
group 202, 204, 599, 600
one-parameter 599–603
orthogonal 203
group theory xii, 464, 599
growth
exponential 258–9, 565, 586, 603–4
one-parameter 599–603
polynomial 580, 604
spiral 483
guess 475
initial 544, 548
inspired 379, 567
H
H1 inner product 136, 144, 233
H1 norm 136

656
Subject Index
Haar equation 555, 563
Haar function 549, 555, 559
Haar wavelet 549–50, 552–3, 555, 562
half-life 257, 404, 406
half-open 79
half-plane 580–1
half-sampled 297
Hamiltonian function 583
Hamiltonian system 583, 585
hard spring 303
hardware 57
harmonic function 381
harmonic polynomial 381, 393
hat function 556, 563
health informatics 467
heat 319, 394, 626
heat equation 394
height 259
Heisenberg commutation relation 355
helix 85, 374
Hermite polynomial 233
Hermitian adjoint 181, 205
Hermitian dot product 178, 205, 288, 433,
444, 446
Hermitian inner product 180–1, 192
weighted 435
Hermitian matrix 181–2, 435, 439
Hermitian norm 445, 489
Hermitian transpose 205, 444
hertz 614
Hessenberg matrix 535–6, 539, 542
Hessian matrix 242
hexagon 292, 619–20
hi ﬁ287
high-dimensional data 471
high-frequency 183, 287, 291, 294, 555
high precision 48, 57
high-resolution image 462
higher order system 605
Hilbert matrix 57–8, 164, 212, 276–7, 465,
516, 548, 584
Hilbert space 135, 341
hill 236, 583
hiss 293
hole 125
home 258
homogeneous 15, 67, 144, 379, 405
homogeneous diﬀerential equation 84, 379,
381, 392, 567, 609, 621
homogeneous function 161
homogeneous solution 388
homogeneous system vii, xi, xii, 67, 95, 99,
106, 108, 342, 376, 378, 384, 388, 394,
409, 571, 585
Hooke’s Law 303, 306, 309, 322, 327, 625
house 258, 632
Householder matrix 206, 210–1, 532, 535
Householder Method 209, 211–2, 532
Householder reﬂection 535, 539
Householder vector 210–1, 536
hunter 406, 479
hydrogen 620
hyperbola 569, 570
hyperbolic function 176
hyperbolic rotation 375
hyperplane 103, 362
I
I-beam 322
icosahedron 127
idempotent 16, 109, 216, 419
identity
additive 76
Jacobi 10, 354, 602
trigonometric 175
identity function 343, 355
identity matrix 7, 8, 16, 25, 31, 70, 200, 409,
588, 593, 618
identity permutation 26, 415
identity transformation 348, 429
IDFT 289
ill-conditioned 56–7, 211, 249, 276–7, 461–2,
525
image x, xvi, 75, 105, 107–8. 114, 116–7, 221,
223–4, 237, 294, 312, 383, 429, 434,
457
digital 294, 462
geophysical 102
high-resolution 462
JPEG 555
medical 102, 295
still 102, 285
image processing vii, viii, x–xii, xv, 1, 48, 99,
102, 183, 188, 404, 467, 470, 555
image vector 473
imaginary axis 580–1
imaginary eigenvalue 581, 588, 603
imaginary part 173, 177, 287, 391, 394, 575–6
imaginary unit xvii, 173
implicit iterative system 492
improper isometry 373
improper node 588–9, 591, 604
improper orthogonal matrix 202, 358, 438
inbreeding 504
incidence matrix 122, 124, 128, 303, 312,
314, 317, 325, 327, 462, 616, 622
reduced 303–4, 316, 318, 330, 335, 339, 622
incompatible xi, 62
incomplete eigenvalue 412, 578, 581

Subject Index
657
incomplete matrix 403, 424, 480, 490, 575,
594, 603
inconsistent 62
increase
steepest 545, 582
indeﬁnite 159, 583
indeﬁnite integral xviii, 348, 356
independent
linearly ix, x, xiii, 75, 93, 95–6, 99, 100,
161, 185, 341, 380, 423, 448, 570, 599
statistically 470
indeterminate
statically 306, 315, 334
index 357, 447
inductance 626, 629
induction x, 434, 451
inductor 311, 628
inequality ix, 129
Cauchy-Schwarz 129, 137, 142–3, 179, 469
complex 177
integral 143
Minkowski 145–6
product 154
Sylvester 120
triangle 129, 142–4, 146, 154, 179, 498
inertia 439
infantry 625
inﬁnite 79
inﬁnite-dimensional x, xiv, xv, 101, 129, 133,
149, 151, 213, 219–20, 274, 301, 341,
349, 351, 355–6, 396, 401, 541
inﬁnite-dimensional dynamical system 565
inﬁnite-dimensional subspace 219
∞matrix norm 495–6, 499, 510, 515
∞norm 145, 151, 245, 255, 473, 489, 496,
514, 524, 544
inﬁnite series 394
inﬁnitely diﬀerentiable 84
inﬁnitesimal generator 599–602
inﬁnitesimal motion 616
inﬂection point 240
informatics 467
inhomogeneity 110
inhomogeneous diﬀerential equation 84, 606,
623, 627
inhomogeneous iterative equation 479
inhomogeneous system vii, xi, xii, 67, 106,
110–1, 342, 376, 383–4, 388, 394, 565,
585, 605–6, 630
initial condition 404–5, 480, 570–2, 593,
609–10
initial guess 544, 548
initial position 609–10, 612
initial time 621
initial value problem 376, 386, 570, 594, 598,
606
initial vector 475, 540
initial velocity 609–10, 618, 622
inner product ix–xi, xiii, xiv, 129–30, 133,
137, 144, 156–7, 163, 179, 237, 245,
347, 350, 395
complex 184
H1 136, 144, 233
Hermitian 180–1, 192, 435
L2 133, 135, 180, 182, 185, 191, 219, 227,
232, 234, 274, 550–1, 557, 560
real 184, 395, 401
Sobolev 136, 144, 233
weighted 131, 135, 182, 246, 265, 309, 396,
435, 543, 618
inner product matrix 156
inner product space 130, 140, 161, 183–4,
193, 196, 213, 219, 245, 342, 347, 350,
358
dual 358
inspired guess 379, 567
instructor xviii
instrument 467, 626
integer xvii, 561
Fibonacci 481–3, 485–6
random 487
tribonacci 486
integrable 84
integral x, 557
deﬁnite xviii
line 125
indeﬁnite xviii, 348, 356
Lebesgue 135
trigonometric 175, 177, 624
weighted 182
integral equation vii, 76, 106, 183, 341–2,
376–8, 556
Fredholm 377
Volterra 378
integral inequality 143
integral operator xi, 341
integral transform 376
integration 347, 606
numerical 271, 562
integration constant 404
integration operator 347
integro-diﬀerential equation 629
interchange 209
column 57
row 23, 25, 56, 70, 361
interchange matrix 25
interconnectedness 120
interest 476, 478–9
intermediate eigenvalue 442
internal energy 309
internal force 303, 320, 327, 333

658
Subject Index
internal motion 388, 627
internal power 319
internal vibration 565, 624
internet 126, 258, 463, 475, 499, 502
interpolant 263
interpolation viii, ix, xi, xiii, 52, 79, 227, 235,
262
polynomial 235, 260, 262, 271, 279
sinc 273
spline 52, 235
trigonometric 86, 235, 287, 293
intersection xvii, 65
interval xvii, 79, 83–4, 219, 386, 557
closed xvii, 146
open 146
time 476
invariant subspace xv, 429–31, 452, 487, 492,
548, 603–4
complex 430, 452
real 452
inverse xiii, xiv, 1, 17, 31, 32, 50, 111, 200,
355
additive 8, 76
left 31, 36, 38, 356
pseudo- 403, 457, 467
right 31, 35, 356
Inverse Discrete Fourier Transform 289
inverse elementary matrix 17, 38
inverse linear function 355
inverse matrix x, 17, 31–3, 38, 40, 44, 72,
102, 111, 428, 457
inverse permutation 32, 34
Inverse Power Method 526
Shifted 526–7, 534, 539
inversion 79, 102
invertible 33, 106, 387, 421, 439
IQ 467
irrational number 611
irregular 505
irrotational 86
island 504
isometry 372–3
improper 373
proper 373, 419
isomorphic 356
isotope 257, 404
iterate 476, 506
iteration xv, 475
linear vii, 403, 475
nonlinear 53, 475
iterative equation 476–8, 479
iterative method xv, 475, 506, 536
linear vii, 403
naıive 517
semi-direct xii, 475, 536, 547
iterative system 53, 563
aﬃne
488
implicit 492
nonlinear 53, 475
order of 481, 493
second order 493
J
Jacobi eigenvalue 519
Jacobi identity 10, 354, 602
Jacobi matrix 509, 511, 515, 519
Jacobi Method xii, 475, 509–11, 513, 517,
519–20
Jacobi spectral radius 520
Jacobian matrix 605
jar 623
jagged 286
JAVA 14
joint 120, 322–3
Jordan basis 448, 450–1, 453, 480, 488, 576–7
Jordan Basis Theorem 448, 450
Jordan block 416–7, 449–50, 453, 598
Jordan canonical form xii, xiii, 403, 447, 450,
490, 525, 598
Jordan chain 447–8, 450–2, 488, 576, 579,
603–4
null 447, 451
Jordan chain solution 576–7, 581
JPEG 555
junction 311
K
kernel x, 75, 105, 107–8, 114–5, 117, 124–5,
221, 223–4, 331, 378, 380, 384, 411,
429, 434, 456, 463, 631
kill 479
Kirchhoﬀ’s Current Law 313–4, 317
Kirchhoﬀ’s Voltage Law 312, 314
Krylov approximation 541–2
Krylov subspace xv, 475, 536–7, 539–40, 546,
549
Krylov vector 537, 547
Ky Fan norm 466
L
L1 norm 145, 147, 153, 182, 274
L2 Hermitian inner product 180
L2 inner product 133, 135, 182, 185, 191,
219, 227, 232, 234, 274, 550–1, 557,
560
L2 norm 133, 145, 152–3, 185, 191
L2 squared error 274
Lp norm 145
L∞norm 145, 147, 152–3, 182

Subject Index
659
laborer 504
Lagrange data 284
Lagrange multiplier 441
Lagrange polynomial 262, 284
Lagrangian notation viii
Laguerre polynomial 231, 234, 279
Lanczos Method xii, 475, 539
language 404
Laplace equation 381, 383, 385, 393
Laplace transform 376
Laplacian 349, 354, 381, 393
graph xv, 301, 317–8, 462, 464
large 188, 475
largest eigenvalue 441, 495, 523–5
laser printing 283
lattice 505
Law 139
Hooke’s 303, 306, 309, 322, 327, 625
Kirchhoﬀ’s Current 313–4, 317
Kirchhoﬀ’s Voltage 312, 314
Newton’s 565, 608
Ohm’s 312, 319
Voltage Balance 312, 314, 629
Law of Cosines 139
LC circuit 630
LDLT factorization xi, 45, 167, 437, 542
LDV factorization 41
permuted 42
leading coeﬃcient 367
leaf 482
learning vii, 235, 404, 467
least squares ix, xiii–xv, 129, 132, 230, 235,
255, 266, 468
weighted 252, 256, 265
least squares approximation 188, 263, 272
least squares coeﬃcient 266
least squares error 235, 251–2, 271, 458
weighted 252, 256
least squares line 474
least squares minimizer 183, 237
least squares solution ix, xi, 237–8, 250–1,
317, 403, 458
Lebesgue integral 135
left eigenvector 416, 503, 525
left half-plane 580–1
left-handed basis 103, 202, 222
left inverse 31, 36, 38, 356
left limit xviii
left null space x, 113
Legendre polynomial 232, 234, 277–8
Leibniz rule 594
Leibnizian notation viii
length 120, 130, 323
letter 283
level curve 585
level set 585
license 479
light
speed of 159
stroboscopic 286
light cone 159–60
light ray 160, 235
limit xviii
line 65, 83, 87–8, 237, 239, 254, 259–60, 301,
343, 363, 370–1, 587, 615
least squares 474
parallel 371
spectral energy 437
stable 587, 589, 591
tangent 341, 600
unstable 587, 589
line integral 125
line segment 473
linear vii, ix, 2, 342
linear algebra vii, xi, xiii, xv, 1, 75, 114, 126,
183, 243, 341, 403, 506
Fundamental Theorem of 114, 461
numerical 48
linear algebraic system vii, ix, 341–2, 376,
386, 506, 517, 540
linear analysis ix, x
linear approximation 324, 329, 341, 388
linear combination 87, 95, 101, 287, 342, 388,
599, 618
linear control system vii, xv, 376
linear diﬀerential equation vii, viii, xiv, 84,
342, 376, 378
linear diﬀerential operator xi, 317, 341–2,
355, 376–7, 379–80, 384
linear dynamical system xii, 565, 603
linear form 161
linear function ix, xi–xiv, xvi, xvii, 239, 341–2,
349–50, 352, 355, 358, 369–70, 378,
383, 395–6, 599
adjoint 396–7
dual 369, 398
inverse 355
invertible 387
positive deﬁnite 398–9, 401
real 342, 391
self-adjoint 398–9, 436
skew-adjoint 400
linear independence ix, x, xiii, 75, 99, 177,
341, 570
linear integral equation vii, 76, 106, 183,
341–2, 376–7, 556
linear integral operator xi, 341
linear iteration vii, 403, 475i
linear iterative system vii, ix, xii–xv, 476,
479, 493, 499, 500, 522, 560, 584, 605
linear map 341–2

660
Subject Index
linear mathematics ix
linear motion 588–90, 616, 618
linear operator 75, 156, 341–3, 347, 376, 437,
541
linear polynomial 187
linear programming 235
linear superposition vii, ix, xi, xv, 110, 222,
235, 250, 262, 342, 378, 480, 565, 630
linear system vii, ix, xi, 4, 6, 20, 23, 40, 59,
63, 67, 75, 99, 105–7, 376, 461, 475,
541, 565, 571, 577
adjoint 112
compatible ix, xi, 8, 11, 62, 224
complex xiv, 566
equivalent 2
forced 565
homogeneous vii, xi, xii, 67, 95, 99, 106,
108, 342, 376, 378, 384, 388, 394, 409,
571, 585
ill-conditioned 57, 211, 461
incompatible xi, 62
inconsistent 62
inhomogeneous vii, xi, xii, 106, 110–1, 342,
376, 383–4, 388, 394, 565, 585, 605–6,
630
lower triangular 3, 20
singular 461
sparse xv, 52, 475, 536
triangular 2, 29, 197, 542
weak 132, 398, 540
linear system of ordinary diﬀerential
equations ix, xii–xv, 342, 530, 566,
584, 571, 608, 630
second order xii, 618
linear system operation 2, 23, 37
linear transformation ix, xiii, xiv, 341–2, 358,
403, 426, 429, 457, 554, 599
self-adjoint 436
linearity ix, 2
linearization 341, 605
linearly dependent 93, 95–6, 100, 571
linearly independent ix, x, xiii, 75, 93, 95–6,
99, 100, 161, 185, 341, 380, 423, 448,
570, 594, 599
local minimum 236, 242, 441
localized 549
logarithm xvii, 258, 269, 599
London 626
loop 120
low-frequency 291, 294
lower bidiagonal 52
lower triangular xvi, 3, 16–7, 20, 39, 73, 518
special xvi
strictly xvi, 16–8, 28, 39, 41–2, 45, 60, 85,
168, 509, 530
lower unitriangular xvi, 16–8, 20, 28, 39,
41–3, 45, 60, 85, 168, 530
LP 287
LU factorization x, xiv, xvi, 1, 18, 20, 41, 50,
70, 268, 501, 536, 542
permuted 27–8, 60, 70
Lucas number 486
M
machine learning vii, 235, 404, 467
magic square 104
magnitude 630
main diagonal 7, 43, 52, 492
manifold 235, 605
manufacturing 235
map 342
diﬀerence 436
linear 341–2
perspective 374–5
scaling 399
shift 415, 436
zero 361
Maple 14, 57
market 475
Markov chain xii, 475, 499–502
Markov process ix, xiv, 463–4, 563
mass viii, 110, 236, 301, 311, 341, 609, 615,
621–3, 629
center of 439
mass matrix 396, 608, 616, 618, 620, 622, 630
mass–spring chain xi, xiv, 301, 309, 317, 399,
403, 565, 608, 610, 619, 628, 630
mass–spring ring 339
Mathematica 14, 57, 409
mathematician xviii
mathematics 1, 75, 227, 314, 381
applied vii, ix, x, 1, 48, 230, 475
ﬁnancial 1
Matlab 14, 409
matrix ix–xi, xiii, xiv, xvii, 1, 3, 48, 75, 105,
133, 223, 341, 343, 407, 445, 457, 475
adjacency 317
adjoint 396
aﬃne 372, 603
approximating 462
Arnoldi 540
augmented 12, 24, 36, 60, 66–7
banded 55
bidiagonal 52, 536
block 11, 35, 603
block diagonal 35, 74, 128, 171, 420, 449,
535, 598
block upper triangular 74, 535
circulant 282, 436

Subject Index
661
matrix (continued )
coeﬃcient 4, 6, 63, 157, 224, 235, 241, 343,
476, 479, 484, 499, 508, 528, 531, 566,
575, 591, 606, 608, 618, 630
cofactor 112
commuting 10, 601
complete xvi, 403, 424–6, 428, 430–2, 444,
450, 480, 484, 490, 493, 522, 566, 572,
575, 603
complex 5, 181, 212, 226, 536, 566
complex diagonalizable 427
conductance 313, 317
convergent 488–9, 495–6, 508, 517
covariance 163, 470–1, 473
data 462, 467, 470–1, 473
deﬂated 420
degree 317
diagonal 7–8, 35, 41–2, 45, 85, 159, 168,
171, 204, 304, 313, 327, 400, 408, 425–6,
437, 439–40, 446, 453, 455, 472, 484,
528, 530, 608, 618, 622
diagonalizable xvi, 403, 424, 426, 428, 437,
450, 520
doubly stochastic 505
eigenvalue 530, 531
eigenvector 531
elementary 16–7, 32, 38–9, 73, 204, 360,
362
elementary reﬂection 206, 210, 418, 532
Fibonacci 412, 428
friction
622
Gauss–Seidel 514–5, 519
generic 48
Gram 129, 161–3, 182, 246, 255, 274, 301,
309, 316, 327, 351, 398, 403, 439, 454,
456, 462, 470, 543, 622, 630
graph Laplacian xv, 301, 317–8, 462, 464
Hermitian 181–2, 435, 439
Hessenberg 535–6, 539, 542
Hessian 242
Hilbert 57–8, 164, 212, 276–7, 465, 516,
548, 584
Householder 206, 210–1, 532, 535
idempotent 16, 109, 216, 419
identity 7, 8, 16, 25, 31, 70, 200, 409, 588,
593, 618
ill-conditioned 56–7, 276–7, 461, 525
improper orthogonal 202, 358, 438
incidence 122, 124, 128, 303, 312, 314, 317,
325, 327, 462, 616, 622
incomplete 403, 424, 480, 490, 575, 594,
603
indeﬁnite 159
inner product 156
interchange 25
matrix (continued )
inverse x, 17, 31–3, 38, 40, 44, 72, 102, 111,
428, 457
invertible 33, 106, 421, 439
irregular 505
Jacobi 509, 511, 515, 519
Jacobian 605
Jordan block 416–7, 449–50, 453, 598
linearization 605
lower bidiagonal 52
lower triangular xvi, 16–7, 20, 39, 73, 518
lower unitriangular xvi, 16–8, 20, 28, 39,
41–3, 45, 60, 85, 168, 530
mass 396, 608, 616, 618, 620, 622, 630
negative deﬁnite 159–60, 171, 581, 583
negative semi-deﬁnite 159
nilpotent 16, 418, 453
nonsingular xi, 23–4, 28, 32, 39, 42, 44, 62,
85, 99, 106, 204, 367, 380, 422, 457,
460, 492, 599, 630
non-square 60, 403
non-symmetric 157
normal 44, 446
normalized data 470, 473
orthogonal xiii, 183, 200, 202, 205, 208,
210, 358, 373, 413, 431, 437, 439, 444,
446, 457, 530, 552
orthogonal projection 216, 440
orthonormal 201
pentadiagonal 516
perfect xvi, 424
permutation 25, 27–8, 32, 42, 45, 60, 71–2,
74, 97, 204–5, 419, 430
positive deﬁnite vii, ix, xi–xiv, 129, 156–7,
159–61, 164–5, 167, 170–1, 181–2, 204,
235, 241–2, 244, 246, 252, 301, 304,
309, 313, 316, 327, 396, 398, 432, 439,
443, 473, 528, 531, 542, 544, 581, 583,
608, 618, 622
positive semi-deﬁnite xi, 158, 161, 182,
244–5, 301, 316, 320, 433, 454, 470,
473, 514, 615, 618, 622
positive upper triangular 205, 529–30
projection 216, 440
proper orthogonal 202–3, 205, 222, 358,
438–9, 600
pseudoinverse 403, 457, 467
quadratic coeﬃcient 241
rank one 66
real 5, 77, 425, 430, 440, 444, 446, 476,
536, 575–6, 595
real diagonalizable 427, 432
rectangular 31, 453, 457
reduced incidence 303–4, 316, 318, 330,
335, 339, 622
reduced resistivity 316

662
Subject Index
matrix (continued )
reﬂection 206, 210, 418, 532
regular xvi, 13, 18, 42, 45, 52, 70, 85, 501,
530–1, 536, 542
regular transition 501
resistance 313
resistivity 314, 316, 320
rotation 34, 358, 414, 430
row echelon 59, 60, 62, 95, 115–6
self-adjoint 399, 618
semi-simple xvi, 424
shift 436
similar 73, 367, 418, 425–6, 428, 465, 498,
532, 575, 598
simultaneously diagonalizable 428–9
singular 23, 70, 314, 403, 409, 411–2, 597,
631
skew-symmetric 47, 73, 85–6, 204, 400,
435, 439, 600–2
SOR 475, 518–9
sparse 48, 536, 548
special lower triangular xvi
special orthogonal 222
special upper triangular xvi
square 4, 18, 23, 31, 33, 45, 403, 416, 426,
453, 457, 495, 542, 596
stiﬀness 305, 309, 320, 327, 611, 615, 618,
622
strictly diagonally dominant 281, 283,
421–2, 475, 498, 510, 512, 516, 584
strictly lower triangular xvi, 16–8, 28, 39,
41–2, 45, 60, 85, 168, 509, 530
strictly upper triangular 16, 85, 509
symmetric xi, xiv, 45, 85–6, 167, 171, 183,
208, 216, 226, 398–9, 403, 432, 434,
437, 440–1, 446, 454, 465, 487, 532,
537, 542, 581, 585, 631
symmetric tridiagonal 532
transition 499–501, 505, 525, 528, 536, 598
transposed 72, 112, 162, 304, 395
tricirculant 54, 282–3, 420, 436
tridiagonal 52, 281, 304, 419, 492, 512,
526, 532, 535–6, 539, 542
unipotent 16–7
unitary 205, 212, 439, 444–6
unitriangular xvi, 16–8, 20, 28, 38–9, 41–3,
45, 60, 85, 168, 530, 543
upper bidiagonal 52
upper Hessenberg 535–6, 539, 542
upper triangular xvi, 13, 16, 23–4, 28, 39,
70, 204–5, 210, 425, 428, 444–6, 465,
518, 527, 530, 532, 602
upper unitriangular xvi, 16, 18, 38, 41–3,
543
Vandermonde 20, 74, 260, 268
wavelet 204, 224, 552, 554
matrix (continued )
weight 256
weighted Gram 163, 247
Young 519–20, 579
zero 7, 8, 61, 77, 361, 457, 488, 597
matrix addition 5, 8, 12, 43, 349
matrix algebra 7, 99
matrix arithmetic xiii, 1, 8, 77
matrix diﬀerential equation 590, 592
matrix exponential ix, xii, 565, 593–4, 597,
599, 601–2, 606
matrix factorization 1, 18, 20, 41, 45, 50,
171, 183, 205, 536
matrix logarithm 599
matrix multiplication 5, 8, 43, 48, 51, 95,
106, 223–4, 343, 352, 355, 365, 397,
403, 429, 457, 475
matrix norm xi, xii, xv, 153–4, 156, 460–1,
466, 475–6, 495–7, 496–9, 510, 515,
596
Euclidean 460–1, 497
Frobenius 156, 466
∞495–6, 499, 510, 515
Ky Fan 466
natural 153–4, 495, 499
matrix pair 618, 630
matrix polynomial 11, 453
matrix power 475, 479, 484, 488, 502
matrix product 33, 72, 130
matrix pseudoinverse 403, 457, 467
matrix series 499, 596
matrix solution 572
matrix square root 439, 465, 620
matrix-valued function 592, 594, 606
max norm 145, 151
maximal error 261
maximal rank 456
maximization principle 235, 443
constrained 442–3
maximum xvii, 150, 235, 240, 441, 442
mean 10, 84, 148, 348, 467, 470, 473
arithmetic 148
geometric 148
mean zero 10, 84, 468, 470
measure theory 135
measurement 254, 256, 467–70
measurement error 256, 470
mechanical force 327
mechanical structure 301
mechanical vibration 565
mechanics viii, xi, xii, 129, 156, 183, 236,
342, 396, 439, 565, 627
classical 341, 388, 583
continuum xi, 235–6, 351, 399
equilibrium 235

Subject Index
663
mechanics (continued )
ﬂuid 48, 80, 173, 236, 381, 400, 565
quantum vii, viii, x, 10, 48, 129, 135, 173,
183, 200, 202, 227, 341, 349, 355, 381,
388, 437, 467, 583, 599
relativistic 341, 388
rigid body 200, 203
solid 236
mechanism 301, 331, 336, 599, 616, 618
medical image 102, 295
medium ix, 183, 285
memory 56, 513, 561
mesh point 279
methane 139
method
Arnoldi xii, 475, 538
Back Substitution x, xiii, xiv, 3, 14, 21, 24,
41, 50, 53, 62, 208, 211, 282, 518
Conjugate Gradient 476, 542, 544–5, 548
deﬂation 420, 526
direct 475, 536
Forward Substitution xiii, xiv, 3, 20, 49,
53, 282, 518
Full Orthogonalization (FOM) xii, 476,
541, 546
Gauss–Seidel xii, 475, 512, 514, 517, 519–20
Gaussian Elimination ix–xiv, 1, 14, 24, 28,
40, 49, 56–7, 67, 69, 72, 102, 129, 167,
208, 237, 253, 378, 407, 409, 412, 475,
506, 508–9, 536
Generalized Minimal Residual (GMRES)
xii, 476, 546–7, 549
Gram–Schmidt ix, xi, xv, 183, 192, 194–5,
198–9, 205, 208, 215, 227, 231, 249,
266, 445, 475, 527, 529, 538
Householder 209, 211–2, 532
Inverse Power 526
iterative vii, xv, 403, 475, 506, 536
Jacobi xii, 475, 509–11, 513, 517, 519–20
Lanczos xii, 475, 539
nave iterative 517
Power xii, 475, 522, 524, 529, 536–7, 568
regular Gaussian Elimination 14, 18, 171,
268
semi-direct xii, 475, 536, 547
Shifted Inverse Power 526–7, 534, 539
Singular Value Decomposition xii, 403,
455, 457, 461, 473
Strassen 51
Successive Over–Relaxation xii, 475, 517–20
undetermined coeﬃcient 372, 385–6, 500,
623
tridiagonal elimination 5, 42
metric 159
microwave 626
Midpoint Rule 271
milk 486
minimal polynomial 453, 537
minimization xiii, xv, 129, 156, 238, 241, 309,
546, 583
minimization principle vii, ix, 235–6, 320,
342, 402
minimization problem xi–xiv, 255
minimizer 183, 237, 241, 401, 545
minimum xvii, 150, 235–6, 240
global 240
local 236, 242, 441
minimum norm solution 224, 458
mining
data 467
Minkowski form 375
Minkowski inequality 145–6
Minkowski metric 159
Minkowski space-time 160
Minneapolis 501
Minnesota 406, 479, 501, 546
missile 269
missing data 471
MMT factorization 171
mode
normal xii, 565, 611
unstable 615, 618–9
vibrational 616
model 620
modeling viii, 235, 279, 309
modular arithmetic xvii
modulate 624
modulus 174, 177, 489
molasses 622
molecule 139, 437, 608
benzene 620
carbon tetrachloride 620
triatomic 616, 619, 632
water 620, 626, 632
molecular dynamics 48
moment 330, 439
momentum 341, 355
money 476, 486
monic polynomial 227, 453
monitor 286
monomial 89, 94, 98, 100, 163–4, 186, 231,
265, 271, 275
complex 393
sampled 265
trigonometric 190
monomial polynomial 268
monomial sample vector 265
month 477
mother wavelet 550, 552, 555–6, 558

664
Subject Index
motion 388, 403, 608, 631
circular 616
damped 621
dynamical xii, 608
inﬁnitesimal 616
internal 388, 627
linear 588–90, 616, 618
nonlinear 600
periodic 621, 624
rigid 301, 327, 335, 373, 599, 601, 616
screw xi, 341, 373, 419, 602
movie xii, 200, 285, 287, 293, 341
MP3 183
multiple eigenvalue 430
multiplication 5, 48–9, 53, 79, 261, 348, 457,
536
complex 173, 296, 298
matrix 5, 8, 43, 48, 51, 95, 106, 223–4, 343,
352, 355, 365, 397, 403, 429, 457, 475
noncommutative 6, 26, 355, 360, 364, 601
quaternion 364
real 296
scalar 5, 8, 43, 76, 78, 87, 343, 349, 390
multiplicative property 596
multiplicative unit 7
multiplicity 416–7, 424, 454, 581
algebraic 424
geometric 424
multiplier
Lagrange 441
multipole 547
multivariable calculus x, 235, 242–3, 342,
441, 545, 582
music 287, 626
N
na¨ıve iterative method 517
natural boundary condition 280, 283–4
natural direction 631
natural frequency 565, 611, 624–5, 630–1
natural matrix norm 153–4, 495, 499
natural vibration 614
Nature 235, 317, 320, 483
negative deﬁnite 159–60, 171, 581, 583
negative eigenvalue 582
negative semi-deﬁnite 159
network viii, xv, 301, 312, 315, 317–8, 320,
463, 499
communication 464
electrical xi, xv, 120, 301, 311–2, 327, 626,
630
newton 112
Newton diﬀerence polynomial 268
Newtonian equation 618, 621
Newtonian notation viii
Newtonian physics vii
Newtonian system 614
Newton’s Law 565, 608
n-gon — see polygon
nilpotent 16, 418, 453
node 120, 311, 313, 315, 317, 339
ending 311
improper 588–9, 591, 604
stable 586, 588–9, 591
starting 311
terminating 311, 322
unstable 587–9, 591
noise 183, 293, 555
non-analytic function 84, 87
non-autonomous 570, 598
noncommutative 6, 26, 355, 360, 364, 601
non-coplanar 88
nonlinear vii, 255, 341, 388, 611, 616
nonlinear dynamics 565, 604, 616
nonlinear function 324, 341
nonlinear iteration 53, 475
nonlinear motion 600
nonlinear system 64, 66, 342, 475, 568, 604
nonnegative orthant 83
non-null eigenvector 434, 454
non-Pythagorean 131
non-resonant 628
nonsingular xi, 23–4, 28, 32, 39, 42, 44, 62,
85, 99, 106, 204, 367, 380, 422, 457,
460, 492, 599, 630
non-square matrix 60, 403
non-symmetric matrix 157
nontrivial solution 67, 95
nonzero vector 95
norm ix–xi, xiii, xiv, 129, 131, 135, 137, 142,
144, 146, 174, 188–9, 237, 245, 489,
495, 581
convergence in 151
equivalent 150, 152
Euclidean xiii, 130, 142, 172, 174, 224, 236,
250, 455, 458, 460, 468, 473, 489, 524,
532, 538, 544, 546
Euclidean matrix 460–1, 497
Frobenius 156, 466
H1 136
Hermitian 445, 489
∞matrix 495–6, 499, 510, 515
∞145, 151, 245, 255, 473, 489, 496, 514,
524, 544
Ky Fan 466
L1 145, 147, 153, 182, 274
L2 133, 145, 152–3, 185, 191
L∞145, 147, 152–3, 182
matrix xi, xii, xv, 153–4, 156, 460–1, 466,
475–6, 495–7, 496–9, 510, 515, 596

Subject Index
665
norm (continued )
max 145, 151
minimum 224, 458
natural matrix 153–4, 495, 499
1– 145, 245, 255, 466
residual 237
Sobolev 136
2– 145
weighted 131, 135, 237, 252, 468
normal equation 247, 251, 272, 458
weighted 247, 252, 256, 317
normal matrix 44, 446
normal mode xii, 565, 611
normal vector 217
normalize 468, 470, 473
normally distributed 473
normed vector space 144, 372
north pole 474
notation
dot viii
Lagrangian viii
Leibnizian
viii
Newtonian viii
prime viii
nowhere diﬀerentiable 561
nuclear reactor 406
nucleus 437
null direction 159–60, 164, 166
null eigenvector 433–4, 615, 618–9
null space x, 106
left x, 113
number viii, 3, 5, 78
complex xvii, 80, 129, 173
condition 57, 460, 466
dyadic 561, 563
Fibonacci 481–3, 485–6
irrational 611
Lucas 486
pseudo-random 464, 487
random 295, 464
rational xvii, 611
real xvii, 78, 137, 173, 563
spectral condition 525, 591
tribonacci 486
numerical algorithm viii–xi, 48, 129, 183,
199, 400, 403, 475, 536, 547
numerical analysis vii, ix, xii, xiii, xv, 1, 75,
78, 132, 156, 227, 230, 233, 235, 271,
279, 317, 475
numerical approximation 220, 235, 403, 416,
467
numerical artifact 206
numerical diﬀerentiation 271
numerical error 249, 523, 536
numerical integration 271, 562
numerical linear algebra 48
O
object 259
observable 341
occupation 504
octahedron 127, 149
odd 87, 286
oﬀ-diagonal 7, 32, 252, 420
oﬀspring 479, 482
Ohm’s Law 312, 319
oil 623, 628
1 norm 145, 245, 255, 466
one-parameter group 599–603
open 79, 136, 146, 151
half- 79
Open Rule 271
operation
arithmetic 48, 199, 212, 534, 536, 548
elementary column 72, 74
elementary row 12, 16, 23, 37, 60, 70, 418,
512
linear system 2, 23, 37
operator
derivative 348, 353
diﬀerential xi, 317, 341–2, 355, 376–7,
379–80, 384
diﬀerentiation 348, 355
integral xi, 341
integration 347
Laplacian 349, 354, 381, 393
linear 75, 156, 341–3, 347, 376, 437, 541
ordinary diﬀerential 353
partial diﬀerential 381
quantum mechanical xi
self-adjoint 183
Schr¨odinger 437
optics 235, 375
optimization 235, 441–2, 466
constrained 441
orange juice 486
orbit 568
order 379, 481, 493, 567
ﬁrst 565–7, 570–2, 577, 585, 605
higher 605
reduction of 379, 390
second xii, 618
stabilization 537, 540, 547, 549
ordinary derivative xviii
ordinary diﬀerential equation vii, x, xi, 91,
98–9, 101, 106–7, 301, 322, 342, 376,
379–80, 385, 390, 403–4, 407, 435, 476,
479, 566–7, 570, 576, 579, 604, 606,
608, 627, 630
homogeneous 390

666
Subject Index
ordinary diﬀerential equation (continued )
inhomogeneous 606
system of ix, xii–xv, 342, 530, 566, 584,
571, 579, 592, 608, 630
ordinary diﬀerential operator 353
orientation 122, 201, 311, 313, 317
origin 88, 343
ornamentation 322
orthant 83
orthogonal x, 140, 184–5, 189, 213, 216, 222,
335, 540, 549, 618, 631
orthogonal basis xi, xiii, xv, 184, 189, 194,
201, 214, 235, 266, 403, 435, 446, 551,
611, 618
orthogonal basis formula 189, 611
orthogonal complement 217–9, 221, 431, 631
orthogonal eigenvector 432, 436, 611
orthogonal function xi, 183, 559–60
orthogonal group 203
orthogonal matrix xiii, 183, 200, 202, 205,
208, 210, 358, 373, 413, 431, 437, 439,
444, 446, 457, 530, 552
improper 202, 358, 438
special 222
proper 202–3, 205, 222, 358, 438–9, 600
orthogonal polynomial xi, xiv, 141, 183, 186,
227–8, 276–7
orthogonal projection xi, xiii, xv, 183, 213,
216, 218, 223, 235, 248, 361–2, 440,
457, 471–2, 539, 631
orthogonal subspace xv, 183, 216
orthogonal system 552
orthogonal vector xiii–xv, 140, 185
orthogonality vii, xi, 184, 235, 287, 295, 312,
476, 558, 562
orthogonalization 475
orthonormal basis 184, 188, 194–6, 198–9,
201, 204, 213, 235, 248, 288, 432, 437–8,
444, 456–7, 460, 475, 528–9, 538
orthonormal basis formula 188
orthonormal column 444, 455–6
orthonormal eigenvector 437
orthonormal matrix 201
orthonormal rows 455
orthonormalize 529, 539
orthonormality ix, 184
oscillation 626
out of plane 627
outer space 301, 327
oven 258, 626
overdamped 622, 629
overﬂow 524
over-relaxation xii, 475, 517–20
oxygen 620
P
p norm 145, 245
Pad´e approximation 261
page 126, 463, 502
PageRank 463, 502
pair
matrix 618, 630
parabola 15, 240, 259–60, 590–1
paraboloid 83
parachutist 269
parallel 65, 83, 88, 93, 137, 142, 147–8, 187,
371, 500
parallel computer 513
parallelizable 513
parallelogram 140, 344, 361
parameter 599
Cayey–Klein 203
relaxation 518
variation of 385, 606, 623
part
imaginary 173, 177, 287, 391, 394, 575–6
real 173, 177, 287, 365, 391, 394, 575–6,
581, 591, 603
partial derivative xviii, 242, 349
partial diﬀerential equation vii, ix–xii, xv,
xvii, 99, 101, 106, 129, 173, 200, 227,
230, 301, 322, 342, 376, 381, 475, 536,
542, 547, 565
partial diﬀerential operator 381
partial pivoting 56, 62
partial sum 554
particular solution 107, 384, 606, 623–5, 630
partitioning 463
path 121
PCA ix, xii, xiv, xv, 255, 403, 467, 471
peak 90
peg 279
pendulum 236
pentadiagonal matrix 516
pentagon 125, 288, 467
perfect matrix xvi, 424
perfect square 166
period 611, 621
period 2 solution 495
periodic 172, 285, 565, 587
periodic boundary condition 280, 283–4
periodic force xii, 565, 623–4, 626, 630–1
periodic function 86, 611
periodic motion 621, 624
periodic spline 280, 283
periodic vibration 618
permutation 26–7, 45, 56, 72, 428
column 418
identity 26, 415
inverse 32, 34

Subject Index
667
permutation (continued )
row 428
sign of 72
permutation matrix 25, 27–8, 32, 42, 45, 60,
71–2, 74, 97, 204–5, 419, 430
permuted LDV factorization 42
permuted LU factorization 27–8, 60, 70
perpendicular 140, 202, 255
Perron–Frobenius Theorem 501
perspective map 374–5
perturbation 523, 525, 591
phase xviii, 174, 627
phase-amplitude 89, 587, 610
phase lag 627
phase plane 567–8, 576, 586, 605
phase portrait xii, 568, 586–90, 623
phase shift 89, 273, 587, 609
phenomenon
Gibbs 562
physical model 620
physics vii, ix, x, xii, 1, 200, 202, 227, 235,
301, 314, 327, 341–2, 381, 402, 407,
437, 499, 565
continuum 156
Newtonian vii
statistical 464
piecewise constant 551
piecewise cubic 263, 269, 279–80
pivot 12, 14, 18, 22–3, 28, 41, 49, 56, 59, 61,
70, 114, 167, 412
pivot column 56
pivoting 23, 55, 57, 62
full 57
partial 56, 62
pixel 470
planar graph 125
planar system 565, 585
planar vector ﬁeld 81, 86
plane 64, 82, 88, 250, 259, 358
complex 173, 420, 580
coordinate 362
diagonal 362
left half- 580–1
out of 627
phase 567–8, 576, 586, 605
plane curve 86
planet 259, 341
plant 504
platform 616
Platonic solid 127
plot
scatter 469, 471, 478
point 65, 83, 87–8, 235, 250, 570, 587
boundary 503
closest xi, 183, 235, 238, 245–6, 298
point (continued )
critical 240, 242
data xi, 235, 237, 254–5, 272, 283, 467,
470, 474
dyadic 561–2
equilibrium 568, 579
ﬁxed 493, 506, 509, 546, 559, 563
ﬂoating 48, 58
inﬂection 240
mesh 279
saddle 587, 589
sample 79, 105, 285
singular xv, 380
pointer 56
Poisson equation 385, 390, 521
polar angle 174
polar coordinate 90, 136, 174
polar decomposition 439
polarization 160
pole 151, 474
police 196, 254
polygon 125, 208, 288, 467
polynomial xi, xiv, 75, 78, 83, 89, 91, 94, 98,
100, 114, 139, 219, 260–1, 413, 440,
578, 581, 603
approximating 266, 279
characteristic 408, 415, 453, 475
Chebyshev 233
complete monomial 268
constant 78
cubic 260, 267, 280
elementary symmetric 417
even 86
factored 415
harmonic 381, 393
Hermite 233
interpolating 260, 262, 271, 279
Lagrange 262, 284
Laguerre 231, 234, 279
Legendre 232, 234, 277–8
linear 187
matrix 11, 453
minimal 453, 537
monic 227, 453
Newton diﬀerence 268
orthogonal xi, xiv, 141, 183, 186, 227–8,
276–7
piecewise cubic 263, 269, 279–80
quadratic xvi, 167, 185, 190, 235, 240, 260,
267
quartic 221, 264, 269, 276
quintic 233
radial 277
sampled 265
symmetric 417

668
Subject Index
polynomial (continued )
Taylor 269, 324, 383
trigonometric 75, 90–1, 94, 176, 190, 273
unit 148
zero 78
polynomial algebra 78
polynomial equation 416
polynomial growth 580, 604
polynomial interpolation 235, 260, 262, 271,
279
polytope 149
population 259, 475, 479, 482, 487, 504
portrait
phase xii, 568, 586–90, 623
position 254, 355
general 375
initial 609–10, 612
positive deﬁnite vii, ix, xi–xiv, 129, 156–7,
159–61, 164–5, 167, 170–1, 181–2, 204,
235, 241–2, 244, 246, 252, 301, 304,
309, 313, 316, 327, 396, 398, 432, 439,
443, 473, 528, 531, 542, 544, 581, 583,
608, 618, 622
positive semi-deﬁnite xi, 158, 161, 182, 244–5,
301, 316, 320, 433, 454, 470, 473, 514,
615, 618, 622
positive upper triangular 205, 529–30
positivity 130, 133, 144, 146, 156
potential
gravitational 311
voltage 311–2, 315, 318, 320
potential energy 235–6, 244, 309, 320, 583
potential theory 173
power 235, 319–20
matrix 475, 479, 484, 488, 502
power ansatz 380, 479
power function 320
Power Method xii, 475, 522, 524, 529, 536–7,
568
Inverse 526
Shifted Inverse 526–7, 534, 539
power series 175
precision 48, 57, 461
predator 479
prestressed 320
price 258
prime notation viii
primitive root of unity 288
principal axis 465, 472, 487
Principal Component Analysis ix, xii, xiv, xv,
255, 403, 467, 471–2
Fundamental Theorem of 472
principal coordinate 472, 477
principal direction 471–4
principal standard deviation 472
principal stretch 438–9
principal variance 472–3
principle
maximization 235, 442–3
minimization vii, ix, 235–6, 320, 342, 402
optimization 235, 441–2, 466
Reality 391, 484
superposition 75, 106, 111, 378, 388
Uncertainty 355
printing 283
probabilistic process 479
probability vii, viii, xii–xiv, 463, 475, 499
transitional 501–2
probability distribution 468
probability eigenvector 501–3
probability vector 473, 500–1
problem viii
boundary value vii, x, xi, xv, 54, 75, 92, 99,
136, 183, 222, 235, 322, 342, 376–6,
386, 389, 397, 399, 541–2
initial value 376, 386, 570, 594, 598, 606
minimization xi–xiv, 255
process viii, xii, xiv, 403, 475, 499
Gram–Schmidt ix, xi, xv, 183, 192, 194–5,
198, 205, 208, 215, 227, 231, 249, 266,
445, 475, 527, 529, 538
Markov ix, xiv, 463–4, 563
probabilistic 479
stable Gram–Schmidt 199, 538
stochastic 499
processing
image vii, viii, x–xii, xv, 1, 48, 99, 102, 183,
188, 404, 467, 470, 555
signal vii, viii, x–xiii, 1, 75, 80, 99, 102,
129, 183, 188, 235, 272, 293, 476
video 48, 102, 188, 200, 285, 294–5, 341
processor 56, 513
product xvii, 256
Cartesian 81, 86, 133, 347, 377
complex inner 184
cross 140, 187, 239, 305, 602
dot xiii, 129, 137, 146, 162, 176, 178, 193,
201, 265, 351, 365, 396, 431–3, 455,
550
H1 inner 136, 144, 233
Hermitian dot 178, 205, 288, 433, 444, 446
Hermitian inner 180–1, 192, 435
inner ix–xi, xiii, xiv, 129–30, 133, 137, 144,
156–7, 163, 179, 237, 245, 347, 350,
395
L2 inner 133, 135, 180, 182, 185, 191, 219,
227, 232, 234, 274, 550–1, 557, 560
matrix 33, 72, 130
real inner 184, 395, 401
Sobolev inner 136, 144, 233
vector 6, 130

Subject Index
669
product (continued )
weighted inner 131, 135, 182, 246, 265,
309, 396, 435, 543, 618
product inequality 154
professional 504
proﬁt 258
programming
computer 14, 28
linear 235
projection 341, 353, 374, 467
orthogonal xi, xiii, xv, 183, 213, 216, 218,
223, 235, 248, 361–2, 440, 457, 471–2,
539, 631
random 471
projection matrix 216, 440
proof viii
proper isometry 373, 419
proper orthogonal matrix 202–3, 205, 222,
358, 438–9, 600
proper subspace 210
proper value 408
proper vector 408
property
multiplicative 596
pseudo-random number 464, 487
pseudocode 14, 24, 31, 49, 56, 206, 212, 536,
544
pseudoinverse 403, 457, 467
Pythagorean formula 188, 460
Pythagorean Theorem 130–2
Q
Q.E.D. xvii
QR algorithm xii, 200, 475, 527, 529, 531–2,
535–6, 538
QR factorization xi, 205, 210, 522, 529, 539
quadrant 81
quadratic coeﬃcient matrix 241
quadratic eigenvalue 493
quadratic equation 64, 166, 621
quadratic form xi, 86, 157, 161, 166–7, 170,
241, 245, 346, 437, 440–2, 583
indeﬁnite 159
negative deﬁnite 159–60
negative semi-deﬁnite 159
positive deﬁnite 157, 160
positive semi-deﬁnite 158
quadratic formula 166
quadratic function xi, xiii, 235, 239–41, 259,
274, 401, 545, 582–3
quadratic minimization problem xi–xiv
quadratic polynomial xvi, 167, 185, 190, 235,
240, 260, 267
quantize 475, 583
quantum dynamics 173
quantum mechanics vii, viii, x, xi, 10, 48,
129, 135, 173, 183, 200, 202, 227, 341,
349, 355, 381, 388, 437, 467, 583, 599
quantum mechanical operator xi
quartic polynomial 221, 264, 269, 276
quasi-periodic ix, xii, 565, 611, 617–9, 624,
630–1
quaternion 364–5, 583
quaternion conjugate 365
quintic polynomial 233
quotient
Rayleigh 442
quotient space 87, 105, 357
R
rabbit 482, 487
radial coordinate 383
radial polynomial 277
radian xvii
radical 416
radio 322, 293
radioactive
257, 404, 406
radium 259
radius
spectral xii, 475, 490, 492–3, 495–8, 508,
518, 520, 524
unit viii
random graph 463
random integer 487
random number 295, 464
random projection 471
random walk 463
range xvi, 105, 342
range ﬁnder 269
rank 61–3, 66, 96, 99, 108, 114, 124, 162,
165, 223, 365, 434, 455, 457, 461, 465
maximal 456
rank k approximation 462
rank one matrix 66
rate
decay 257, 404, 622
sample 287
ratio
golden 483
rational arithmetic 58
rational function 166, 261, 442
rational number xvii, 611
ray 160, 235
Rayleigh quotient 442
reactor 406
real 177, 390
real addition 296
real analysis 151
real arithmetic 173
real basis 575

670
Subject Index
real diagonalizable 427, 432
real eigenvalue 413, 423, 430, 432, 586
real eigenvector 413, 490, 496, 523, 535, 537,
577–8, 588
real element 391
real inner product 184, 395, 401
real linear function 342, 391
real matrix 5, 77, 425, 430, 440, 444, 446,
476, 536, 575–6, 595
real multiplication 296
real number xvii, 78, 137, 173, 563
real part 173, 177, 287, 365, 391, 394, 575–6,
581, 591, 603
real scalar 5, 177, 476
real solution xiv, 577
real subspace 452
real vector 76, 391
real vector space xi, 76, 342
Reality Principle 391, 484
recipe viii
reciprocal 31
reciprocity relation 322
recognition 285, 404, 467, 555
reconstruction 299, 555
record 287, 293
rectangle 361
rectangular 3, 31, 453, 457
rectangular grid 81
reduced incidence matrix 303–4, 316, 318,
330, 335, 339, 622
reduction of order 379, 390
reﬁned Gershgorin domain 422
reﬂection xi, 206, 341, 353, 358, 360–2, 373,
399, 440, 457
elementary 206, 210, 418, 532
glide 375
Householder 535, 539
reﬂection matrix 206, 210, 418, 532
regiment 625
region
stability 590
regular xvi
regular Gaussian Elimination 14, 18, 171,
268
regular matrix xvi, 13, 18, 42, 45, 52, 70, 85,
501, 530–1, 536, 542
regular transition matrix 501
reinforced 334, 336
relation
constitutive 302, 312, 327
equivalence 87
Heisenberg commutation 355
reciprocity 322
relativity vii, 34, 235–6, 341, 388
general vii, 34
special 159, 358, 375
relaxation
over- xii, 475, 517–20
under- 518
relaxation parameter 518
repeated eigenvalue 417
repeated root 576, 622
rescale 562
resident 504
residual norm 237
residual vector 237, 522, 541, 544–5, 548
resistance 259, 311, 313, 319, 628–9
resistance matrix 313
resistanceless 608
resistivity matrix 314, 316, 320
resistor 301, 313, 628
resonance viii, ix, xii, 565, 625–6, 630–1
resonant ansatz 631
resonant frequency 626–7, 632
resonant solution 628, 631
resonant vibration 565, 625
response 384, 606, 624
retina 375
retrieval 48
reversal
bit 297
time 569
RGB 470
right angle 140, 184
right-hand side 4, 12, 15, 49
right-handed basis 103, 201–2, 222
right inverse 31, 35, 356
right limit xviii
rigid body mechanics 200, 203
rigid motion 301, 327, 335, 373, 599, 601, 616
ring 339
rivet 323
RLC circuit 626, 629
robot 324, 616
rod 322
Rodrigues formula 228, 277
Rolle’s Theorem 231
roller 339
roof 322
root 98, 231, 379–80, 413, 415, 567
complex 114, 390, 621
double 411, 576
matrix square 439, 465, 620
repeated 576, 622
simple 411
square xvii, 12, 131, 166, 171, 175, 185,
198, 214, 269, 454, 468, 611

Subject Index
671
root of unity 288
primitive 288, 292, 296
rotation xi, 200–1, 286, 313, 329, 331, 341,
344, 353, 373, 399, 419, 429, 438–9,
457, 600–1, 616, 618
clockwise 360
counterclockwise 359, 600
hyperbolic 375
rotation matrix 34, 358, 414, 430
round-oﬀerror x, 55, 199, 206, 544
routing 463, 499
row 4, 6, 7, 12, 43, 70, 114, 470, 523–4, 536
orthonormal 455
zero 70, 73
row echelon 59, 60, 62, 95, 115–6
row interchange 23, 25, 56, 70, 361
row operation 12, 16, 23, 37, 60, 70, 418, 512
row permutation 428
row pointer 56
row space x, 113
row sum 10, 419, 502
absolute 155, 496, 498, 510
row vector 4, 6, 130, 350–1
rule
chain 301
Cramer’s 74
Leibniz 594
Midpoint 271
Open 271
Simpson’s 271
Trapezoid 271, 562
S
saddle point 587, 589
salesman 505
sample function 79–80, 235, 285
sample point 79, 105, 285
even 286
odd 286
sample rate 287
sample value 79, 256, 260, 272, 286, 479
sample vector 80, 98, 105, 183, 265, 285, 554
exponential 285–7, 415, 436
monomial 265
polynomial 265
sampling 183, 297, 468
satellite 200, 341
savings account 476
scalar 1, 5, 6, 12, 37, 70, 177, 389, 449, 480
complex 177, 476
real 5, 177, 476
unit 8
zero 8
scalar multiplication 5, 8, 43, 76, 78, 87, 343,
349, 390
scaling 341, 399, 429, 551, 561, 600
scaling function 399, 549, 555, 558, 559–60,
563
scaling transformation 429
scatter plot 469, 471, 478
scattered 468
scheduling 475
Schr¨odinger equation 173, 394
Schr¨odinger operator 437
Schur decomposition xii, xiii, 403, 444–6
science vii
computer vii, ix, xv, 126, 463
data 1, 235
physical ix
social ix
scientist xiii, xviii
screen 374–5
screw motion xi, 341, 373, 419, 602
search 475, 499, 502
segment 473
self-adjoint 183, 399, 436, 618
semantics 404, 467
semi-axis 487, 498
semi-deﬁnite
negative 159
positive xi, 158, 161, 182, 244–5, 301, 316,
320, 433, 454, 470, 473, 514, 615, 618,
622
semi-direct method xii, 475, 536, 547
semi-magic square 104
semi-simple xvi, 424
separation of variables 227
sequence 81, 86, 144
serial computer 513
serial processor 513
serialization 475
series 394
exponential 596, 599, 601–2
Fourier 91, 191, 549, 553
geometric 499
matrix 499, 596
power 175
Taylor 87, 91
trigonometric 549
wavelet 553, 562
sesquilinear 178–9
set xvii, 80
closed 146, 151
data 188, 462, 472–3
diﬀerence of xvii
level 585
open 146, 151
swing 335–6, 619
sex 482

672
Subject Index
shear xi, 341, 361–2, 427, 429, 600
shear factor 361
Sherman–Morrison–Woodbury formula 35
shift 415, 436, 531
phase 89, 273, 587, 609
shift map 415, 436
Shifted Inverse Power Method 526–7, 534,
539
sign 72
signal 285, 294, 549, 553
reconstruction 299
sampled 285
signal processing vii, viii, x–xiii, 1, 75, 80, 99,
102, 129, 183, 188, 235, 272, 293, 476
similar 73, 367, 418, 425–6, 428, 465, 498,
532, 575, 598
simple 120
simple digraph 467, 502
simple eigenvalue 411, 416, 493, 531, 535,
537
simple graph 120, 311, 463
simple root 411
simplex 339, 500
simpliﬁcation 102
Simpson’s Rule 271
simultaneously diagonalizable 428–9
sinc 273
sine xvii, 176, 183, 269, 285, 581, 610
single precision 461
singular 23, 70, 314, 403, 409, 411–2, 461,
597, 631
singular point xv, 380
singular value vii, ix, xiv, 403, 454–6, 460–2,
467, 472–3, 497
distinct 455
dominant 454, 460
largest 460, 466, 497
smallest 460, 463, 466
Singular Value Decomposition xii, 403, 455,
457, 461, 473
singular vector 454, 461, 467, 473
unit 471–2
size 114
skeletal fragment 393
skew-adjoint 400
skew ﬁeld 364
skew-symmetric 10, 47, 73, 85–6, 204, 400,
435, 439, 600–2
skyscraper 322–3
smallest eigenvalue 441
smooth xviii, 84, 279
Sobolev inner product 136, 144, 233
Sobolev norm 136
social science ix
soft spring 303
software xvi, 404
soldier 625
solid 236, 485, 565
Platonic 127
solid mechanics 236
solution ix, 2, 13, 21, 24, 29, 40, 63, 76, 222,
475, 484, 542, 568, 570, 577, 579, 606,
629
approximate 237, 541
complex xiv, 391, 575
constant 615
equilibrium 301, 405, 476, 479, 488, 493,
565, 579, 597, 622
exponential 381, 408
general 91, 107, 111, 480, 606, 618, 625
globally asymptotically stable 488–90,
492–3
homogeneous 388
Jordan chain 576–7, 581
least squares ix, xi, 237–8, 250–1, 317, 403,
458
linearly independent 380, 594
matrix 572
minimum norm 224, 458
non-resonant 628
nontrivial 67, 95
particular 107, 384, 606, 623–5, 630
period 2 495
periodic 565
quasi-periodic 619
real xiv, 577
resonant 628, 631
stable viii, 581
trigonometric 381
trivial 67
unbounded 586
unique 384, 570
unstable viii, 405, 581
vector-valued 592
zero 67, 117, 383–4, 405, 410, 476, 478,
489–90, 492–3, 581–2
solution space 566, 577
solvability vii
SOR xii, 475, 517–20
sound 624, 626
soundtrack 285, 293
source 342
current 301, 313, 317, 320, 629
space 88, 200, 341
column 105, 383
dual 350, 358, 369, 395
Euclidean x, xi, 75–7, 94, 99, 130, 146, 341,
403, 426, 600
free 394

Subject Index
673
space (continued )
function x, xiii, xv, 79, 80, 83, 133, 146,
163, 185, 190, 220, 224, 301, 341, 396,
401, 541
Hilbert 135, 341
inner product 130, 140, 161, 183–4, 193,
196, 213, 219, 245, 342, 347, 350, 358
left null space x, 113
normed 144, 372
null x, 106
outer 301, 327
quotient 87, 105
row x, 113
solution 566, 577
three-dimensional 82–3, 88, 99, 200, 335,
373
vector x, xi, xiii-xv, xvii, 75–6, 82, 101,
129–30, 133, 149, 151, 177, 179, 213,
219–20, 274, 287, 301, 341–2, 349, 351,
355–7, 372, 390, 396, 401, 467, 541,
600
space curve 283
space station 328, 330, 332, 339, 632
space-time 159–60, 235, 341, 358
span ix, x, xiii, 75, 87, 92, 95–6, 99, 100, 177,
185, 215, 217, 341
spark 314
sparse xv, 48, 52, 475, 536, 548
special function 200
special lower triangular xvi
special orthogonal matrix 222
special relativity 159, 358, 375
special upper triangular xvi
species 504
spectral condition number 525, 591
Spectral Decomposition 440, 598
spectral eigenstate 437
spectral energy 437
spectral factorization 437
spectral graph theory xiv, xv, 462–3
spectral radius xii, 475, 490, 492–3, 495–8,
508, 518, 520, 524
Spectral Theorem 437, 439, 456, 529
spectroscopy 608
spectrum xv, 437, 462, 467
graph 462, 467
speech 285, 404, 462, 467, 499
speed 102, 159, 254, 467
sphere 83
unit 83, 149–50, 363, 375, 438, 465, 473–4
spiral 587–8
spiral growth 483
spline viii, xi, 52, 54, 235, 263, 279, 322, 563
B 284, 567
cardinal 284
periodic 280, 283
spline font 283
spline letter 283
spring 110, 236, 301, 303, 320, 323, 608–9,
616, 621, 623, 629
hard 303
soft 303
spring stiﬀness 303, 306, 320, 323, 327, 609,
621, 623, 629
square 9, 126, 167, 358, 363, 467, 505, 619
complete the xi, 129, 166, 240, 437
magic 104
non- 60, 403
perfect 166
semi-magic 104
unit 136
square grid 317, 521
square matrix 4, 18, 23, 31, 33, 45, 403, 416,
426, 453, 457, 495, 542, 596
square root xvii, 12, 131, 166, 171, 175, 185,
198, 214, 269, 454, 468, 611
matrix 439, 465, 620
square truss 322, 615, 632
squared error 255–6, 260, 272, 274
squares
least ix, xi, xiii–xv, 129, 132, 183, 230, 235,
237–8, 250–2, 255–6, 263, 271–2, 317,
403, 458, 468, 474
sum of 167
weighted least 252, 256, 265
St. Paul 501
stability ix, xi, xii, 488, 565, 590, 629
asymptotic 490, 493
Stability Theorem 577, 603
stabilization order 537, 540, 547, 549
stable 331, 405, 478, 493, 579, 581, 583–84,
586–8, 590–1, 610
asymptotically 405, 478, 490, 493, 579–82,
584, 586–7, 591, 597
globally 405, 579
globally asymptotically 405, 488–90, 492–3,
579, 622
structurally 591
stable eigensolution 603
stable equilibrium 235–6, 301–2, 579, 590,
605, 615
stable ﬁxed point 493
stable focus 587, 589, 591
stable improper node 604
stable line 587, 589, 591
stable manifold 605
stable node 586, 588–9, 591
stable solution viii, 581
stable star 589–90
stable structure 331, 335
stable subspace 492, 604
staircase 59

674
Subject Index
standard basis 36, 99, 111, 184, 261, 343,
349, 356, 426, 449, 450, 529
standard deviation 468–9
principal 472
standard dual basis 350
star 467, 589–90
starting node 311
starting vertex 121–2
state 341, 502
state vector 499
static 293
statically determinate 307, 333
statically indeterminate 306, 315, 334
statics 403
station
space 328, 330, 332, 339, 632
statistical analysis 188, 238
statistical ﬂuctuation 467, 470
statistical physics 464
statistical test 467
statistically independent 470
statistics vii–ix, xii, xiii, xv, 1, 99, 129, 132,
135, 156, 163, 238, 467, 499
steady-state 574
steepest decrease 545, 583
steepest increase 545, 582
stiﬀness 303, 306, 320, 323, 327, 609, 621,
623, 629
stiﬀness matrix 305, 309, 320, 327, 611, 615,
618, 622
still image 102, 285
stochastic
doubly 505
stochastic process viii, xii, xiv, 403, 475, 499
stochastic system 341
storage 102, 294, 462, 513
Strassen’s Method 51
stress 323, 439
stretch 341, 344, 360–2, 403, 426, 438, 440,
457, 600, 625
principal 438–9
strictly diagonally dominant 281, 283, 421–2,
475, 498, 510, 512, 516, 584
strictly lower triangular xvi, 16–8, 28, 39,
41–2, 45, 60, 85, 168, 509, 530
strictly upper triangular 16, 85, 509
stroboscopic 286
structurally stable 591
structure viii, xi–xiv, 236, 301, 322, 403, 565,
599, 601, 608, 610, 625, 628–32
atomic 203
mechanical 301
reinforced 334, 336
stable 331, 335
unstable 301, 615
student xviii, 504
subdeterminant 171
subdiagonal 52, 492, 535
subdominant eigenvalue 493, 502, 524, 526
subscript 4
subset xvii, 79
closed xvii
compact 149
convex 150
thin 220
subspace x, xiii, 75, 82, 86, 88, 183, 213, 223,
235, 238, 245, 362, 471, 540
aﬃne 87, 375, 383
center 604
complementary 86, 105, 217–8, 221
complex 298, 430, 452
conjugated 391
dense 220
ﬁnite-dimensional 213, 219, 248
fundamental 114, 183, 221
inﬁnite-dimensional 219
invariant xv, 429–31, 452, 487, 492, 548,
603–4
Krylov xv, 475, 536–7, 539–40, 546, 549
orthogonal xv, 183, 216
proper 210
real 452
stable 492, 604
trivial 82, 429
unstable 604
zero 82, 429
substitution
back x, xiii, xiv, 3, 14, 21, 24, 41, 50, 53,
62, 208, 211, 282, 518
forward xiii, xiv, 3, 20, 49, 53, 282, 518
subtraction 48, 53, 261, 536
suburb 501
Successive Over–Relaxation xii, 475, 517–20
sum xvii, 86
absolute row 155, 496, 498, 510
column 10, 419, 501, 502
partial 554
row 10, 419, 502
sum of squares 167
sunny 499, 501
supercomputer 1, 48
superconducting 608, 630
superdiagonal 52, 449, 492, 535
superposition
linear vii, ix, xi, xv, 110, 222, 235, 250, 262,
342, 378, 480, 565, 630
Superposition Principle 75, 106, 111, 378,
388
superscript 4

Subject Index
675
support 301, 306, 551, 562, 608–9
bounded 557
surface 83, 236, 283, 439
survey
geodetic 171
suspension bridge 625
SVD xii, 403, 455, 457, 461, 473
swing set 335, 619
Sylvester inequality 120
symmetric 157, 241
symmetric matrix xi, xiv, 45, 85–6, 167, 171,
183, 208, 216, 226, 398–9, 403, 432,
434, 437, 440–1, 446, 454, 465, 487,
532, 537, 542, 581, 585, 631
symmetric polynomial 417
symmetry xii, 10, 130, 133, 146, 156, 200,
202, 341, 358
conjugate 179, 184
symmetry analysis 599
system 236
adjoint 112, 117
aﬃne 488
algebraic vii, ix, 341–2, 376, 386, 506, 517,
540
autonomous 403, 566, 579
compatible ix, xi, 8, 11, 62, 224
complete 572
complex xiv, 566
control vii, xv, 76, 99, l06, 376
damped 623
dynamical viii, xii, xiii, xv, 301–2, 396,
403, 407, 565, 583, 591, 603
electrical 183
elliptic 542
equivalent 2
ﬁrst order 565–7, 570–2, 577, 585, 605
ﬁxed point 506
forced 565
Hamiltonian 583, 585
higher order 605
homogeneous vii, xi, xii, 67, 95, 99, 106,
108, 342, 376, 378, 384, 388, 394, 409,
571, 585, 592
ill-conditioned 57, 211, 461
implicit 492
incompatible xi, 62
inhomogeneous vii, xi, xii, 67, 106, 110–1,
342, 376, 383–4, 388, 394, 565, 585,
605–6, 630
inconsistent 62
iterative 53, 475, 481, 488, 492–3, 563
large 475
linear vii, ix, xi, 4, 6, 20, 23, 40, 59, 63, 67,
75, 99, 105–7, 376, 461, 475, 541, 565,
571, 577
system (continued )
linear algebraic vii, ix, 341–2, 376, 386,
506, 517, 540
lower triangular 3, 20
Newtonian 614
non-autonomous 570, 598
nonlinear 64, 66, 342, 475, 568, 604
order of 481, 493
orthogonal 552
planar 565, 585
singular 461
sparse xv, 52, 475, 536
stochastic 341
triangular 2, 20, 29, 197, 542
trivial 590–1
two-dimensional 585
undamped 623, 630–1
unforced 622
weak 132, 398, 540–1, 546
system of ordinary diﬀerential equations
ix, xii–xv, 342, 530, 566, 584, 571, 579,
592, 608, 630
second order xii, 618
T
Tacoma Narrows Bridge 626
tangent xvii, 341, 600
target xvi, 342
taxi 501
Taylor polynomial 269, 324, 383
Taylor series 87, 91
technology 555
temperature 258
tension 327
tensor 4
inertia 439
terminal 317
terminating node 311, 322
test
statistical 467
tetrahedron 127, 139, 321, 619–20
theater 293
theorem viii
Cayley–Hamilton 420, 453
Center Manifold 604
Fundamental, of Algebra 98, 124, 415
Fundamental, of Calculus 347, 356, 606
Fundamental, of Linear Algebra 114, 461
Fundamental, of Principal Component
Analysis 472
Gershgorin Circle 420, 475, 503
Jordan Basis 448, 450
Perron–Frobenius 501
Pythagorean 130–2
Rolle’s 231

676
Subject Index
theorem (continued )
Spectral 437, 439, 456, 529
Stability 577, 603
Weierstrass Approximation 220
theory viii, xiii
category viii
control 235
function 135
graph viii, x, xiv, 12
group xii, 464, 599
measure 135
potential 173
thermodynamics 183, 236, 381, 403
thin 220
three-dimensional space 82–3, 88, 99, 200,
335, 373
time viii, 475–6, 568
initial 621
space- 159–60, 235, 341, 358
time reversal 569
topology 120, 146, 151, 312
total variance 472
tower 322
trace 10, 85, 170–1, 415, 417, 586, 590–1,
596, 600
trajectory 568, 600, 602, 616
transform
Cayley 204
discrete Fourier xi, xv, 183, 272, 285, 289,
295
fast Fourier xi, 235, 296
Fourier 376, 559
integral 376
Laplace 376
wavelet transform 554
transformation 342, 358
aﬃne ix, xii, xiv, 341, 370–3, 377, 419, 603
identity 348, 429
linear ix, xiii, xiv, 341–2, 358, 403, 426,
429, 457, 554, 599
scaling 429
self-adjoint 436
shearing 361
transient 627
transition matrix 4499–501, 505, 525, 528,
536, 598
regular 501
transitional probability 501–2
translation xi, 328, 331, 341, 346, 370–2, 419,
550–1, 556–8, 561–2, 601, 616
transmission 272, 294
transpose x, xii, 43–5, 72, 112, 114, 162, 200,
222, 304, 314, 342, 351, 357, 369, 395,
397, 399, 416, 422, 456, 502, 597
Hermitian 205, 444
trapezoid 126
Trapezoid Rule 271, 562
travel company 258
traveling salesman 505
tree 127, 322, 467
tribonacci number 486
triangle 125, 363, 371, 467, 505, 632
equilateral 328, 500, 619
triangle inequality 129, 142–4, 146, 154, 179,
498
triangular
block upper 74, 535
lower xvi, 3, 16–7, 20, 39, 73, 518
positive upper 205, 529–30
special xvi
strictly lower xvi, 16–8, 28, 39, 41–2, 45,
60, 85, 168, 509, 530
strictly upper 16, 85, 509
upper xvi, 13, 16, 20, 23–4, 28, 39, 49, 70,
204–5, 210, 425, 428, 444–6, 465, 518,
527, 530, 532, 602
triangular form 2, 14
triangular matrix
lower xvi, 16–7, 20, 39, 73, 518
upper xvi, 13, 16, 23–4, 28, 39, 70, 204–5,
210, 425, 428, 444–6, 465, 518, 527,
530, 532, 602
triangular system 2, 20, 29, 197, 542
triangularize 444
triatomic molecule 616, 619, 632
tricirculant matrix 54, 282–3, 420, 436
tridiagonal matrix 52, 281, 304, 419, 492,
512, 526, 532, 535–6, 539, 542
tridiagonal solution algorithm 52, 282
tridiagonalization 532, 535
trigonometric ansatz 609, 618, 623, 625, 630
trigonometric approximation 271, 273
trigonometric function xi, xiv, xvii, 89, 164,
175–6, 183, 235, 272, 292, 578, 580–1
complex 176–7
trigonometric identity 175
trigonometric integral 175, 177, 624
trigonometric interpolation 86, 235, 287, 293
trigonometric monomial 190
trigonometric polynomial 75, 90–1, 94, 176,
190, 273
trigonometric series 549
trigonometric solution 381
trivial solution 67
trivial subspace 82, 429
trivial system 590–1
truss 322, 615, 632
tuning fork 624
turkey 258
two-dimensional system 585
2-norm 145

Subject Index
677
typography 283
U
unbiased 468
unbounded 581, 586, 603
Uncertainty Principle 355
uncorrelated 469, 472
undamped 623, 630–1
underdamped 621, 627, 629
underﬂow 524
undergraduate 9
under-relaxed 518
underwater vehicle 200
undetermined coeﬃcients 372, 385–6, 500,
623
unforced 622
uniform convergence 562
uniform distribution 468
union xvii, 86
unipotent 16–7
uniqueness 1, 23, 40, 380, 383–4, 401, 479,
568, 570, 593, 610
unit 76
additive 7
imaginary xvii, 173
multiplicative 7
unit ball 85, 149–50, 473
unit circle xvii, 132, 288, 442, 530
unit cross polytope 149
unit diamond 149
unit disk 136, 371, 503
unit eigenvector 471, 493, 496
unit element 148
unit function 148
unit octahedron 149
unit polynomial 148
unit radius viii
unit scalar 8
unit sphere 83, 149–50, 363, 375, 438, 465,
473–4
unit square 136
unit vector 141, 148, 150, 184, 208, 325, 327,
441, 443, 524, 532, 576, 602
unitary matrix 205, 212, 439, 444–6
United States 259
unitriangular xvi, 16
lower
xvi, 16–8, 20, 28, 39, 41–3, 45, 60,
85, 168, 530
upper
xvi, 16, 18, 38, 41–3, 543
unity
root of 288, 292, 296
universe vii, 160, 358, 403, 628
unknown 4, 6, 506
unstable 314, 405, 478, 581, 583–4, 586, 591,
619
unstable eigensolution 603
unstable equilibrium 235–6, 301, 590
unstable focus 588-9, 591
unstable improper node 588–9, 591
unstable line 587, 589
unstable manifold 605
unstable mode 615, 618–9
unstable node 587–9, 591
unstable solution viii, 405, 581
unstable star 589–90
unstable structure 301, 615
unstable subspace 604
upper Hessenberg matrix 535–6, 539, 542
upper triangular xvi, 13, 16, 23–4, 28, 39, 70
block 74, 535
positive 205, 529–30
special xvi
strictly 16, 85, 509
upper triangular system 20, 49
upper unitriangular xvi, 16, 18, 38, 41–3, 543
uranium 404
V
valley 236
value
absolute xvii
boundary vii, x, xi, xv, 54, 75, 92, 99, 136,
183, 222, 235, 322, 342, 376–6, 386,
389, 397, 399, 541–2
characteristic 408
expected 468
initial 376, 386, 570, 594, 598, 606
proper 408
sample 79, 256, 260, 272, 286, 479
singular vii, ix, xii, xiv, 403, 454–7, 460–2,
466–7, 472–3, 497
Vandermonde matrix 20, 74, 260, 268
variable viii, 2, 62, 605
basic 62–3, 118
change of 172, 232, 234
complex 172
free 62, 63, 67, 96, 108, 119–20, 315
phase plane 567
separation of 227
variance 468–71, 473
principal 472–3
total 472
unbiased 468
variation xii, 235
variation of parameters 385, 606, 623
vat 622
vector ix, x, xiii, xiv, xvii, 1, 75, 129, 223,
341, 457, 480, 571, 578
Arnoldi 538–40, 542, 547
battery 317

678
Subject Index
vector (continued )
characteristic 408
circuit 312
column 4, 6, 46, 48, 77, 130, 350–1
complex 129, 433
constant 588
current source 314, 318, 320
data 254
displacement 302, 312, 325, 620
elongation 302, 325
error 254, 508, 514
force 327
gradient 349, 545, 582
Gram–Schmidt 194
Householder 210–1, 536
image 473
initial 475, 540
Krylov 537, 547
measurement 468, 470
mechanism 336
nonzero 95
normal 217
normalized 468, 470
orthogonal xiii–xv, 140, 185
parallel 142, 147
probability 473, 500–1
proper 408
real 76, 391
residual 237, 522, 541, 544–5, 548
row 4, 6, 130, 350–1
sample 80, 98, 105, 183, 265, 285–7, 436,
554
singular 454, 461, 467, 473
standard basis 36, 99, 111, 184, 261, 343,
349, 356, 426, 449, 450, 529
state 499
unit 141, 148, 150, 184, 208, 325, 327, 441,
443, 524, 532, 576, 602
velocity 574
voltage 312, 320
zero 7, 67, 77, 82, 93, 131, 407, 493
vector addition 77, 82, 390
vector calculus 353, 365
vector ﬁeld 81, 574
vector product 6, 130
vector space x, xi, xiii, xvii, 75–6, 82, 130,
341, 349, 600
complex xi, xiv, 76, 129, 177, 179, 287, 342,
390
conjugated 390
ﬁnite-dimensional xiii, xiv, 101, 149, 356
high-dimensional 467
inﬁnite-dimensional x, xiv, xv, 101, 129,
133, 149, 151, 213, 219–20, 274, 301,
341, 349, 351, 355–6, 396, 401, 541
vector space (continued )
isomorphic 356
normed 144, 372
quotient 87, 105, 357
real xi, 76, 342
vector-valued function 80, 98, 136, 341, 605
vector-valued solution 592
vehicle 200, 254, 616
velocity viii, 80, 254, 259, 365, 615, 618,
620–1
initial 609–10, 618, 622
velocity vector ﬁeld 574
vertex 120, 122, 125–6, 311, 463, 502, 505
ending 122
starting 121–2
vibration viii, ix, xii, 399, 565, 608–9, 611,
621–2, 625–6, 629–31
internal 565, 624
mechanical 565
natural 614
periodic 618
quasi-periodic 565, 617–8
resonant 565, 625
vibrational force 630
vibrational frequency 610, 621, 624
vibrational mode 616
video 48, 102, 188, 200, 285, 294–5, 341
vision
computer 375, 499
voltage 121, 311–2, 319–20, 626–7
Voltage Balance Law 312, 314, 629
voltage potential 311–2, 315, 318, 320
voltage vector 312, 320
Volterra integral equation 378
volume 465
W
walk
random 463
wall 322
waste 258, 406
water 583
water molecule 620, 626, 632
wave
electromagnetic 626
wave function 173, 341
wavelet xii, xiv, xv, 102, 227, 476, 549, 552,
555, 560
Daubechies 555, 562
daughter 550, 556, 563
Haar 549–50, 552–3, 555, 562
mother 550, 552, 555–6, 558
wavelet basis 102, 189, 204, 283, 550, 552,
555–6, 562
wavelet coeﬃcient 470

Subject Index
679
wavelet matrix 204, 224, 552, 554
wavelet series 553, 562
wavelet transform 554
weak formulation 132, 398, 540–1, 546
weather 48, 407, 499, 501
web page 126, 463, 502
Weierstrass Approximation Theorem 220
weight 132, 135, 256, 502
atomic 620
weight matrix 256
weighted adjoint 397
weighted angle 138
weighted digraph 311, 502
weighted Gram matrix 163, 247
weighted graph 311
weighted inner product 131, 135, 182, 246,
265, 309, 396, 435, 543, 618
weighted integral 182
weighted least squares 252, 256, 265
weighted norm 131, 135, 237, 252, 468
weighted normal equation 247, 252, 256, 317
wheel 287
white collar 504
wind 323
wind instrument 626
wire 120, 301, 311–3, 317, 319, 327
withdrawal 476
Wronskian 98
Y
year 475–6
Young matrix 519–20, 579
YouTube 626
Z
zero xvi
mean 10, 84, 468, 470
zero column 59
zero correlation 470
zero determinant 70
zero eigenspace 434
zero eigenvalue 412, 421, 433–4, 581, 615
zero element 76, 79, 82, 87, 140, 342
zero entry 52, 59, 449, 501
zero function 79, 83, 134, 343, 405
zero map 361
zero matrix 7, 8, 61, 77, 361, 457, 488, 597
zero polynomial 78
zero potential 315
zero row 70, 73
zero scalar 8
zero solution 67, 117, 383–4, 405, 410, 476,
478, 489–90, 492–3, 581–2
zero subspace 82, 429
zero vector 7, 67, 77, 82, 93, 131, 407, 493

