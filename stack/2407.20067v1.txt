xAI-Drop: Don’t Use What You Cannot Explain
Vincenzo Marco De Luca1, Antonio Longa1[0000−0003−0337−1838], Pietro
Li`o2[0000−0002−0540−5053], and Andrea Passerini1[0000−0002−2765−5395]
1 Trento University, Trento, Italy
{vincenzomarco.deluca, antonio.longa, andrea.passerini}@unitn.it,
2 Cambridge University, Cambridge, UK
pl219@cam.ac.uk
Abstract. Graph Neural Networks (GNNs) have emerged as the pre-
dominant paradigm for learning from graph-structured data, offering a
wide range of applications from social network analysis to bioinformatics.
Despite their versatility, GNNs face challenges such as oversmoothing,
lack of generalization and poor interpretability, which hinder their wider
adoption and reliability in critical applications. Dropping has emerged as
an effective paradigm for reducing noise during training and improving
robustness of GNNs. However, existing approaches often rely on ran-
dom or heuristic-based selection criteria, lacking a principled method to
identify and exclude nodes that contribute to noise and over-complexity
in the model. In this work, we argue that explainability should be a
key indicator of a model’s robustness throughout its training phase. To
this end, we introduce xAI-Drop, a novel topological-level dropping reg-
ularizer that leverages explainability to pinpoint noisy network elements
to be excluded from the GNN propagation mechanism. An empirical
evaluation on diverse real-world datasets demonstrates that our method
outperforms current state-of-the-art dropping approaches in accuracy,
effectively reduces over-smoothing, and improves explanation quality.
Keywords: GNN regularization · GNN explainability · Dropping
1
Introduction
The capacity to effectively process networked data has a wide range of potential
applications, including recommendation Systems [3], drug design [12], and urban
intelligence [13]. Graph Neural Networks (GNN) [8,15,6,38] have emerged as a
powerful and versatile paradigm to address multiple tasks involving networked
data, from node and graph classification to link prediction and graph generation.
Despite their effectiveness and popularity, GNNs face various challenges that
prevent their wider adoption and reliability in critical applications, such as over-
smoothing, lack of generalization and poor interpretability. Over-smoothing [28]
occurs when the learned representations for the different nodes in a graph (ap-
proximately) collapse into a single representation shared by all nodes, due to the
exponential growth of the receptive field with respect to the number of layers in
the GNN [23]. Dropping [17] has emerged as an effective paradigm to mitigate
arXiv:2407.20067v1  [cs.LG]  29 Jul 2024

2
V.M. De Luca et al.
oversmoothing and increase GNN robustness. Dropping can be performed at
different granularities, from dropping single features [4] to dropping edges [27],
nodes [3,24] or even entire paths [4]. However, existing approaches often rely
on random or heuristic-based selection criteria, and lack a principled method to
identify and exclude nodes that contribute to noise and over-complexity in the
model.
In this paper we argue that explainability should be considered a first class
citizen in determining which elements of the graph should be dropped in order
to increase the robustness of the learned GNN. Consider a GNN being trained
for node classification. Our intuition is that the fact that the prediction for a
given node has a poor explanation is a symptom of a suboptimal function being
learned, and that this symptom is more harmful if the prediction has a high-
confidence. Guided by this intuition we present xAI-Drop, a novel topological-
level dropping regularizer that leverages explainability and over-confidence to
pinpoint noisy network elements to be excluded from the GNN propagation
mechanism during each training epoch.
An empirical evaluation on diverse real-world datasets demonstrates that our
method outperforms current state-of-the-art dropping approaches in accuracy,
effectively reduces over-smoothing, and improves explanation quality. Our main
contributions can be summarized as follows:
– We identify local explainability during training as a driving principle to
discard noisy information in the GNN learning process.
– We introduce xAI-Drop, an explainability-guided dropping framework for
GNN training.
– We show how xAI-Drop consistently outperforms alternative dropping strate-
gies as well as XAI-based regularization approaches on various node classi-
fication benchmarks.
– We demonstrate the effectiveness of xAI-Drop in reducing oversmoothing
and improving explanation quality.
The rest of the paper is organized as follows. We start by reviewing related
work (Section 2) and then introduce the relevant background (Section 3). Our
xAI-Drop framework is presented in Section 4 and experimentally evaluated in
Section 5. Finally, conclusions are drawn in Section 6.
2
Related Work
Dropping. Dropping strategies are routinely employed in neural networks to
prevent overfitting [33]. Specifically, dropping works by randomly setting to zero
a proportion of neurons during training, reducing the capacity of the network
and forcing it to learn more robust and generalized features. The rich structure
information held in GNNs has motivated the extension of the dropping mecha-
nism to the topological level, in order to alter the propagation of messages be-
tween neighboring nodes. The first approach being introduced, DropEdge [27],
randomly drops edges according to a given Bernoulli. Taking inspiration from

xAI-Drop: Don’t Use What You Cannot Explain
3
this work, researchers have suggested random dropping of graph components at
various granularity levels. For instance, DropGNN [24] randomly drops nodes
together with all their connections. DropMessage [4] performs dropping opera-
tions directly on the propagated messages during the message-passing process.
DropAGG [11] randomly chooses neighbouring nodes to participate in message
aggregation. Lastly, DropPath[35] drops all connections in randomly selected
paths. All these methods rely on random sampling to choose the component to
drop. Alternative dropping strategies have been explored in the literature, where
the components to drop are chosen according to various criteria. Learn2Drop [21]
is a learnable graph sparsification procedure deciding which edges to drop to
retain maximal similarity to the original network. Beta-Bernoulli Graph Drop
Connect (BBGDC) [9] learns and adapts the drop rate of the edges during train-
ing based on a Beta-Bernoulli distribution. All these methods rely on random
or heuristic-based selection criteria. In this work we show how a more princi-
pled XAI-based method to identify potentially harmful components substantially
outperforms existing dropping strategies.
Post-hoc explanability. Several works investigate post-hoc methods to ex-
plain the predictions of GNN models. GNN explainers can be categorized into
model-level and instance-level explainers. Model-level explainers [1] aim at pro-
viding a global understanding of a trained model, e.g., as motifs or rules driving
the model to predict a certain class. In contrast, instance-level explainers [19]
aim at identifying components of a given input that are responsible for the
model’s prediction for that input, and are thus more appropriate to design an
XAI-based dropping strategy. Instance-level explainers can be grouped into five
categories[14]: decomposition, surrogate, gradient, perturbation and generation
based. Decomposition-based methods break down the input to identify explana-
tions [25]. Surrogate-based methods rely on an interpretable surrogate to explain
the prediction of the original model [10,37]. Gradient-based methods define ex-
planations in terms of the gradient of the network output with respect to the
elements of the input graph [34,25]. Perturbation-based methods manipulate the
input to obtain interpretable subgraphs [40,20], while generation-based methods
generate subgraphs that can explain the model output [18]. By providing ex-
plainability scores for individual elements of the network, gradient-based meth-
ods are the best candidates for identifying potential elements to be dropped.
In this work we used the saliency map approach [31] because of its computa-
tional efficiency, but the framework can be applied to any explainer producing
node-level explainability scores.
XAI-based regularization. A few approaches have been recently proposed
to explicitly introduce XAI-based regularization strategies during the training
stage of GNNs. MATE [32] applies an optimization procedure via meta-learning
to enhance explainability of the resulting model. ExPASS [7] works at the mes-
sage passing level by weighting messages with the importance of nodes as defined
by PGExplainer [20], while ENGAGE [30] directly removes low score nodes.
These methods however fail to consider the quality of the explanation and are
heavily parameterized, resulting in substantial computational overhead, learn-

4
V.M. De Luca et al.
ability issues and eventually suboptimal performance. Our experimental evalu-
ation shows how our simple XAI-driven dropping strategies outperform these
methods in terms of both accuracy and explainability.
3
Preliminaries
In this section, we provide an overview of the fundamental concepts underlying
our approach.
Graph. A graph is a tuple G = (V, E, XV, XE), where V is a set of vertices or
nodes, E is a set of edges between the nodes, XV and XE are node features and
edge features, respectively. Node and edge features may be empty.
The set of edges E can be represented as an adjacent matrix A ∈R|V|×|V|, where
Aij = 1 if (vi, vj) ∈E, 0 otherwise. In this paper we will focus on undirected
graphs, in which edges have no directions, i.e., Aij = Aji. Given v ∈V, the set
Nv = {u ∈V : (u, v) ∈E} denotes the neighborhood of v in G.
Graph Neural Network (GNN). A GNN is a class of neural network architec-
ture specifically designed to process graph data [29,22,16]. A GNN leverages a
message-passing scheme to propagate information across nodes in a graph. GNNs
iteratively learn node representations hv by aggregating information from neigh-
boring nodes. Specifically, the l-th layer of a message-passing GNN is:
h(l)
v
= γ(l) 
h(l−1)
v
, AGG
n
ϕ(l)(h(l−1)
v
, h(l−1)
u
) : u ∈Nv
o
(1)
where ϕ(l) is as differentiable functions that computes the message from u to
v, AGG is an aggregation operator combining messages sent to v, and γ(l) is
a differentiable function combining the learned representation of v with the in-
coming messages. After l iterations, the vector hl
v contains both the structural
information and the content of the l-hop neighborhood of node v. With an ad-
equate number of iterations, these node representation vectors can be used for
classifying both individual nodes (written as fv(G)) and the entire graph (f(G)),
by plugging a learnable function (typically an MLP) on top of node embeddings
or graph embeddings respectively, the latter being computed with a readout
function aggregating node embeddings. In this paper we will focus on node clas-
sification, but the approach being developed can be easily adapted to deal with
graph classification.
In most cases, the propagation mechanism for an entire layer can be rep-
resented more compactly using the adjacency matrix A, the node embedding
matrix H(l) and one or more layer-specific weight matrices W (l−1). For instance,
layerwise propagation in GCN [16] can be written as:
H(l) = σ

˜D−1
2 ˜A ˜D−1
2 H(l−1)W (l−1)
where ˜A = A+I|V| is the adjacency matrix enriched with self loops, ˜Dii = P
j ˜Aij
and σ is a non-linear activation function such ReLU or sigmoid.

xAI-Drop: Don’t Use What You Cannot Explain
5
In the following we will present dropping strategies in terms of modifications
to the adjacency matrix A.
Dropping. Dropping strategies for GNNs can be categorized based on the gran-
ularity of the information being dropped. For the sake of compactness, we will
formalize here the edge and node dropping strategies, that can both be repre-
sented in terms of operations performed on the adjacency matrix A.
DropEdge [27] removes the connection between two nodes, and halts the
propagation of the message between them, based on an edge dropping mask
BE ∈{0, 1}|V|×|V| defined as follows:
BE
ij ∼Bernoulli(1 −p)
(2)
The filtered adjacency matrix is computed as follows:
A′ = A ⊗BE
(3)
where ⊗is the Hadamard product.
DropNode [5] acts at the node-level on the set of nodes V. It removes nodes
from the node set V, and, as a consequence, its incident edges Iv = {(u, w) ∈
E : u = v or w = v}, based on a node dropping mask bV ∈{0, 1}|V| defined as
follows:
bV
i ∼Bernoulli(1 −p)
(4)
The filtered adjacency matrix is computed as follows:
A′ = BV A BV
(5)
where BV is a diagonal matrix having the elements of bV on the main diagonal
(and zero elsewhere).
GNN explainability. Intuitively, given a graph G and a trained GNN f, an ex-
planation is a subgraph Gexp ⊂G that contains the information that is relevant
for f to perform inference on G. We use Gexp(v) to denote the local explana-
tion for the GNN output for node v. In this study, we employ the saliency map
method [31], an instance-based explainer that computes the attribution for each
input by performing backpropagation to the input space. The general idea is
that the magnitude of the derivative provides insights into the most influential
features, which, when perturbed, result in the highest difference in the output
space. Formally, it is defined as
Gexp(v) = ∂fv(G)
∂Xv
(6)
Where fv(G) is the prediction of the model for node v, and Xv is the feature
vector of node v.

6
V.M. De Luca et al.
Fidelity sufficiency (Fsuf). Fidelity sufficiency [26] is a popular explain-
ability metric for GNNs. It measures the distance between the probability pre-
dicted by f when fed with the entire graph G and the probability when fed with
the explanation Gexp respectively:
Fsuf(v) = 1 −d(fv(G), fv(Gexp(v)))
(7)
with d(p, p′) being a distance over probability distributions, which in our
work we have identified with the cross entropy.
The metric requires a hard explanation, while most GNN explainability ap-
proaches, including saliency maps, return explanations as soft masks (i.e., a real
value associated with each edge indicating its importance for the prediction be-
ing explained). In this manuscript we discretize soft explanations by selecting
the top 15% of the edges as part of the hard explanation.
4
Explainability-Based Dropping
xAI-Drop is based on the combination of two concepts: explainability and
(over)confidence. On the one hand, a a poor local explanation can be seen as
a symptom of an unreliable prediction for the corresponding node, making it a
good candidate for being dropped to reduce noise during training. On the other
hand, a highly confident prediction for a node indicates that the network is
very confident about the features the prediction is based upon, that in principle
should correspond to the local explanation. A confident prediction with a poor
explanation is thus a combination one would like to avoid as much as possible.
Building on these intuitions, xAI-Drop implements a dropping strategy that
targets nodes with poor explanations and high certainty.
Figure 1 presents a graphical representation of the xAI-Drop approach,
which consists of two main phases: node selection and dropping. The node selec-
tion phase (further detailed in Section 4.1) consists of three steps of increasing
complexity: an initial sample of nodes is selected at random; the most certain
nodes in the sample are extracted as candidates for dropping; candidate nodes
are then ranked according to their fidelity, and the fidelity score is turned into
a dropping probability. In the second phase of the model, for each candidate
node v we use its dropping probability p(v) to drop (some of) its connections
(xAI-DropEdge) or the node itself (xAI-DropNode). In the former case, a
fraction p(v) of the node edges is selected at random and removed, in the latter
case the node itself (with all its edges) is removed with probability p(v). See
Section 4.2 for the details.
4.1
Node selection
Initially, a fraction α of the nodes is selected at random, producing the subset
V′. This step is mostly done for computational efficiency, and it can be skipped
if the graph is not too large. The candidate dropping set V′′ ⊂V′ is then created
by selecting the β most confident nodes in V′, with confidence computed as:

xAI-Drop: Don’t Use What You Cannot Explain
7
Fig. 1. A graphical illustration of the dropping strategy of the xAI-Drop algorithm:
panel A reports the node selection phase, that consists of three steps: random sam-
pling of a subset of nodes (left); selection of a fraction of the sampled nodes for with
the network outputs predictions with highest confidence (center, confidence assuming
binary classification for simplicity); ranking of the selected nodes according to their
explainability. Panel B reports the dropping phase: for xAI-DropEdge (left), a p(v)
fraction of the v edges is dropped; for xAI-DropNode (right), the node v itself is
dropped with probability p(v) together with its (outgoing) edges. Note that dropped
edges are represented as directional edges in the graph, to highlight the fact that only
the message originated from v is dropped. See the main text for the details.
C(v) = maxyP(y|Xv)
(8)
where Xv is the node features associated with the node v. For each v ∈V′′ its
local explanation Gexp(v) is computed using the saliency map [31] method. We
opted for saliency maps because they are inexpensive to compute and they do
not require ground truth explanations, but the method is agnostic with respect
to the explanation method being used. Nodes in V′′ are then ranked in decreasing
order of explanation quality, as measured by fidelity sufficiency (Eq. 7).
The next step consists in assigning dropping probabilities to the nodes in V′′.
Given a predefined dropping probability p (a hyper-parameter of the model),
the idea is to adjust dropping probabilities for individual nodes according to
their fidelity, without affecting the expected fraction of nodes to be selected for
dropping. The dropping probability of node v ∈V′′ is adjusted as:
p(v) = p + ∆p(v)
(9)

8
V.M. De Luca et al.
where ∆p(v) is an offset that depends on the ranking of v. It is computed as
follows:
∆p(v) = 2w
|V′′| × iv −w
(10)
where iv ∈[0, |V′′ −1|] is the position of node v in the ranked list, and
w ∈[0, min(p, 1 −p)] is a hyper-parameter that controls the entity of the ad-
justment being introduced. This method ensures a linear distribution of proba-
bilities across the nodes directly correlating with their ordered fidelity rankings.
All nodes u ∈V \ V′′ that do not belong to the sampled set retain the default
dropping probability, ∆p(u) = 0. Overall, this procedure guarantees that the ex-
pected number of nodes selected for dropping is equal to the predefined dropping
probability p.
4.2
Dropping
Once the biased dropping probabilities p(v) have been computed, the propa-
gation of the information can be altered to regularize the learning. Any of the
random dropping strategies introduced in the literature can be adjusted to lever-
age the node-specific dropping probabilities p(v). In this manuscript we focus on
the two coarser level strategies, namely drop edges (xAI-DropEdge) and drop
nodes (xAI-DropNode), that were formalized in Section 3. Their xAI-Drop
variants are described in the following.
xAI-DropEdge. While in the random DropEdge strategy, all edges have
the same probability p of being dropped, in the xAI-DropEdge strategy this
probability depends on the dropping probability of the nodes they connect. More
formally, we need to introduce an edge dropping mask BE defined as follows:
BE
ij ∼Bernoulli(1 −p(vi))
(11)
The filtered adjacency matrix is computed according to Eq. 3. Notice that the
filtered adjacency matrix will not be symmetric, even if the original graph was
undirected.
xAI-DropNode. This dropping strategy works exactly as random DropNode,
with the generic node dropping probability p replaced by a node specific dropping
probability p(v). More formally, we need to introduce a node dropping mask bV
defined as follows:
bV
i ∼Bernoulli(1 −p(vi))
(12)
The filtered adjacency matrix is computed according to Eq. 5.

xAI-Drop: Don’t Use What You Cannot Explain
9
4.3
Overall procedure
The overall algorithm for xAI-Drop is outlined in Algorithm 1. The algorithm
takes as input a graph G, the GNN architecture to be trained f, and the hyper-
parameters α, β, p and w. In each epoch, the algorithm randomly selects a subset
of α nodes from V. It then selects the top β nodes with highest prediction confi-
dence, and computes their explainability according to the current version of f in
terms of fidelity sufficiency. Fidelity sufficiency values (collectively indicated as
Fsuf) are then used to determine the dropping probabilities. These probabilities
can be applied to drop edges (xAI-DropEdge) or nodes (xAI-DropNode).
Finally, the adjusted adjacency matrix A′ is used to further train the GNN f.
Algorithm 1 xAI-Drop algorithm. G = (V, E, XV, XE) is a graph, f is the
GNN, α, β, p, w are hyper-parameters
1: procedure xAI-Drop(G = (V, E, XV, XE),f, α, β, p, w)
2:
for e ∈Epochs do
3:
V′ ←random-sample(V, α)
4:
V′′ ←highest-confidence(G, V′, f, β)
▷Equation 8
5:
for v ∈V′′ do
6:
Gexp(v) ←saliency-map(G, v)
▷Equation 6
7:
Fsuf(v) ←fidelity(f, G, Gexp(v))
▷Equation 7
8:
end for
9:
p ←dropping-probabilities(Fsuf, p, w)
▷Equations 9 and 10
10:
A′ →xAI-DropEdge(G, p)
▷or xAI-DropNode
11:
f ←train(f, G, A′)
12:
end for
13: end procedure
5
Experiments
Our experimental evaluation aims to address the following research questions:
Q1: Does xAI-Drop outperform alternative dropping strategies?
Q2: Does xAI-Drop outperform alternative xAI-driven strategies?
Q3: Does xAI-Drop improves explainability?
Q4: Does xAI-Drop help preventing oversmoothing?
We start by presenting the experimental setting and then discuss the results
answering these questions.
5.1
Experimental setting
Datasets: We employed three widely used datasets for node classification: Cite-
seer, Cora, and PubMed. Each dataset is composed of a single graph with thou-
sands of labeled nodes. We utilize the publicly available train, validation, and
test node splits [39]. Detailed dataset statistics are presented in Table 1.

10
V.M. De Luca et al.
# classes # nodes # edges
CiteSeer
6
3,327
4,732
Cora
7
2,708
5,429
PubMed
3
19,717
44,338
Table 1. Dataset statistics.
Competitors: We compare our xAI-Drop approach with state-of-the-art drop-
ping strategies. The random dropping strategies act at several levels: DropMes-
sage [4] removes random features in arbitrary messages propagated between
pairs of nodes at any layer; DropEdge [27] removes random edges between
pairs of nodes; DropNode [3,24] removes random nodes and all of their in-
cident edges; DropAgg [11] samples a set of nodes and, during the aggregation
stage, discards all of the messages they receive from their neighbors; DropPath [4]
samples multiple random paths and removes the edges that compose them. The
non-random dropping strategies include Learn2Drop [21], that employs param-
eterized networks to prune task-irrelevant edges and BBGDC [9], which learns
to adapt the drop rate of the edges during training. Since our method can be
seen as an XAI-based regularization approach, we extended the comparison to
existing xAI-based GNN regularization techniques: ExPASS [7] alters the weight
of messages depending on the importance scores provided by a parametric ex-
plainer [40]; MATE [32] leverages the meta-learning paradigm to optimize the
explanation performances of a parametric explainer [40]; ENGAGE [30] intro-
duces a novel explainer to guide a data augmentation aimed at improving GNN
robustness.
GNN Architectures: By operating on the adjacency matrix, xAI-Drop is ag-
nostic about the underlying GNN architecture. To demonstrate its versatility,
we implemented it with two widely recognized GNN architectures: Graph Con-
volutional Networks (GCN) [15] and Graph Attention Networks (GAT) [36]. For
the random strategies, we retained the hyper-parameters that were optimized
in the original work evaluating them [4]. The same holds for ExPASS [7]. For
ENGAGE [30] and MATE [32] we optimized hyper-parameters ourselves over
the validation set, as the available configuration was not usable (ENGAGE was
trained on a larger training set with respect to the standard split, while MATE
was mostly evaluated on synthetic data). In the case of xAI-Drop, we main-
tained the same GNN hyper-parameters as those used in random strategies to
isolate the role of the explainability component. Concerning xAI-Drop-specific
hyper-parameters, we fixed p = w = 0.5 in all settings for simplicity, which im-
plies dropping on average 50% of the edges/nodes (while the dropping probability
p is optimized for each single dataset and GNN architecture in the case of the
random strategies [4]), and set α = β = 1/3 in all settings after a coarse-grained
optimization on the validation set.

xAI-Drop: Don’t Use What You Cannot Explain
11
Metrics: Multiclass accuracy is used for addressing research questions Q1 and
Q2. To answer research question Q3, we computed the sufficiency accuracy,
defined as:
Asuf(G) =
1
|Vtest|
X
v∈Vtest
1 (argmax(fv(G)) = argmax(fv(Gexp(v))))
(13)
where Vtest is the set of test nodes in G, Gexp(v) is the (thresholded version of
the) saliency map defined in Eq 6, and 1(·) is the indicator function.
Overshooting mitigation (research question Q4) is evaluated in terms of
Mean Average Distance (MAD)[2], which is defined as follows:
MAD = 1
|V|
X
i∈V
1
|N(i)|
X
j∈N(i)

1 −
hT
i hj
∥hi∥∥hj∥

(14)
where N is the set of nodes, N(i) is the set of neighbours of node i and hi is
the learned hidden representation of node i.
Availability: The code for reproducing all experiments found in this paper is
freely available as supplementary material.
5.2
Experimental results
R1: xAI-Drop outperforms alternative dropping strategies. Table 2
shows the test accuracy of the different approaches we tested. Mean and stan-
dard deviation over 10 runs with different initialization seeds are reported. Com-
paring our xAI-Drop strategies with the blocks of random and learning-based
dropping approaches, the advantage of XAI-driven dropping is evident3. Both
xAI-DropEdge and xAI-DropNode consistently outperform their random
counterparts (DropEdge and DropNode, respectively, which in turn improve over
the baseline method with no dropping) on all datasets and for both GCN and
GAT, despite the fact that the biased dropping probability (Eq. 9) is applied to
only about 10% of the nodes (α = β = 1/3). More importantly, they outperform
all alternative dropping strategies, both random-based and learning-based4. In-
deed, xAI-DropEdge and xAI-DropNode consistently score as best and sec-
ond best methods in almost all scenarios. These results support our intuition
that explainability can be an effective metric to guide the identification and
removal of noisy information in GNN training.
3 Notice that the results in the table are different from those in the original publications
of each respective method, as we had to rerun them all (retaining their optimal hyper-
parameters or optimizing them on the validation set, as explained in Section 5.1) in
order to compute explainability and oversmoothing metrics in addition to accuracy.
Accuracy comparisons with the results reported in the original papers are reported in
the supplementary material, and confirm the advantage of the xAI-Drop strategies.
4 We omit the results of BBGDC on GAT, because the method was specifically de-
signed for GCN architectures and it failed to learn usable models when applied to
GAT architectures.

12
V.M. De Luca et al.
GCN
GAT
Model
Cora
CiteSeer PubMed
Cora
CiteSeer PubMed
Baseline
79.0±0.3 67.1±0.5 78.9±1.2 78.4±1.2 68.1±0.7 77.3±0.7
Random
Dropfeature
79.0±1.2 69.3±0.4 78.1±0.1 79.6±0.4 68.6±1.3 77.3±0.7
DropEdge
80.0±0.5 68.4±0.6 77.5±0.4 79.8±0.3 68.3±0.7 77.3±0.4
DropMess
80.8±0.5 70.8±0.5 78.1±0.3 80.1±0.6 69.5±0.8 77.5±0.5
DropNode
80.0±0.5 69.4±0.4 78.0±0.4 78.6±1.3 67.5±0.7 77.4±0.2
DropAggr
80.0±0.6 68.8±0.6 78.3±0.3 80.8±0.8 67.3±1.2 77.7±0.2
DropPath
80.3±0.6 71.7±0.8 77.6±0.8 81.9±0.7 71.1±0.7 77.9±0.7
Learning
Learn2Drop
79.8±0.5 69.2±0.9 77.4±1.2 82.2±0.1 71.2±0.7 78.1±0.3
BBGDC
74.2±0.3 67.4±0.3 74.1±0.6
-
-
-
xAI-Based
MATE
80.3±0.4 68.4±0.3 74.3±0.5 80.0±0.8 69.2±0.6 76.2±0.7
ExPass
82.2±0.6 72.9±0.4 76.2±0.3 80.3±0.8 70.2±0.3 76.8±0.7
Engage
81.8±0.4 73.8±0.4 78.6±0.5 81.6±0.2 72.1±0.4 77.6±0.5
xAI-Drop xAI-DropEdge 83.6±0.5 74.0±0.4 79.5±0.4 82.6±0.5 72.8±0.4 78.8±0.5
xAI-DropNode 83.4±0.6 73.6±0.3 79.7±0.3 82.5±0.6 72.6±0.2 78.5±0.3
Table 2. Test set accuracy (in percentage). Mean and standard deviation over 10 runs
with different initialization seeds. The best performing method is boldfaced, the second
best is underlined.
R2: xAI-Drop outperforms alternative xAI-driven strategies xAI-Drop
is not the first method to propose the use of explainability to enhance training.
The XAI-based block in Table 2 reports test accuracy of existing alternative
xAI-based regularization methods. These methods severely underperform with
respect to our xAI-Drop strategies, likely because of the increased complexity
of their training process with respect to our dropping schemes. It is important
to remind here that these xAI-based competitors have been developed with ad-
ditional goals in mind with respect to regularization, namely improving explain-
ability (MATE), alleviating oversmoothing (Expass) or increasing robustness to
adversarial attacks (Engage).
R3: xAI-Drop improves explainability Table 3 reports sufficiency accu-
racy (Eq. 13) over test data, again with mean and standard deviation over the
10 runs. As expected, XAI-based approaches improve explainability with re-
spect to the baseline. It is interesting to highlight that dropping strategies are
also quite effective in improving explainability5, confirming the beneficial effect
of dropping in terms of robustness of training. That said, xAI-DropEdge and
xAI-DropNode again stand out as the best performing methods in all settings.
The improvement over the other XAI-based approaches highlights the effective-
ness of using XAI as a dropping strategy in isolating the most relevant part of
the input graphs.
5 We do not report explainability results for Learn2Drop, as its implementation is
incompatible with the Captum library that we use as explainability framework.

xAI-Drop: Don’t Use What You Cannot Explain
13
GCN
GAT
Model
Cora
CiteSeer PubMed
Cora
CiteSeer PubMed
Baseline
78.7±1.4 79.5±1.1 90.5±0.8 87.4±1.1 91.2±0.6 95.4±0.6
Random
Dropout
86.7±0.8 90.6±1.0 90.2±0.5 90.3±0.6 92.5±0.7 95.2±0.5
DropEdge
88.7±0.7 81.7±1.7 91.4±0.5 92.2±1.3 92.9±0.6 94.0±1.2
DropMess
89.5±0.9 87.4±1.2 90.8±0.4 91.6±0.9 92.2±0.5 93.4±0.7
DropNode
88.9±1.2 90.0±0.8 91.3±0.5 92.5±1.5 92.6±0.6 94.0±1.2
DropAggr
90.2±0.4 88.3±0.8 91.2±0.8 92.1±0.8 92.1±0.5 95.1±1.0
DropPath
87.1±1.2 89.6±0.8 91.2±0.4 91.9±0.3 92.4±0.2 96.3±0.8
Learning
BBGDG
87.5±0.3 88.1±0.4 89.2±0.4 90.4±0.7 88.7±0.6 92.3±0.4
xAI
MATE
90.7±0.6 90.6±0.9 91.3±0.7 92.6±0.6 93.1±0.4 95.4±0.3
ExPass
89.8±0.8 90.1±0.7 90.7±0.6 88.6±0.4 89.5±0.8 94.6±0.6
Engage
90.6±0.5 90.7±1.2 91.2±0.9 92.7±0.7 93.3±0.5 95.1±0.5
xAI-Drop xAI-DropEdge 91.4±0.9 91.2±0.7 91.6±0.6 93.0±0.7 94.1±0.5 95.7±0.5
xAI-DropNode 90.7±0.5 90.9±0.6 91.7±0.6 93.1±0.4 93.4±0.4 95.6±0.5
Table 3. Explainability of the different methods as measured by test sufficiency ac-
curacy (in percentage). The best performing method is boldfaced, the second best is
underlined.
R4: xAI-Drop prevents oversmoothing Figure 2 reports MAD values (Eq. 14,
the higher the better in terms of oversmoothing avoidance) as functions of the
test accuracy of the models, for GCN (top) and GAT (bottom) respectively.
Each graph focuses on a single dataset. Results clearly indicate that xAI-Drop
strategies achieve high accuracy while controlling oversmoothing. Indeed, xAI-
DropNode and especially xAI-DropEdge tend to have higher MAD values
with respect to their closer competitors in terms of accuracy (right side of the
plots). The only exception is Learn2Drop on PubMed with GAT, where a very
high MAD is combined with an accuracy that is not too far from that of the
xAI-Drop strategies. When comparing the two xAI-Drop strategies, we can see
that xAI-Drop tends to outperform xAI-DropNode both in terms of accuracy
and (more substantially) oversmoothness avoidance, hinting at the usefulness of
a finer grained dropping strategy, other things being equal.
6
Conclusion
In this work we introduced a simple XAI-based regularization framework for
GNN training that selects nodes with highly confident predictions but poor
explanations as candidates for dropping. Our experimental evaluation clearly
showed that the proposed framework outperforms alternative dropping strate-
gies as well as other XAI-based regularization techniques in terms of accuracy,
explainability and oversmoothing. These promising results highlight how en-
couraging high quality explanations at training time can effectively improve
training dynamics. Future work include the exploration of the connection be-

14
V.M. De Luca et al.
74
76
78
80
82
84
Test Accuracy
3
4
5
6
7
8
9
MAD %
GCN Cora
68
70
72
74
Test Accuracy
2
3
4
5
6
7
8
9
MAD %
GCN Citeseer
74
75
76
77
78
79
80
Test Accuracy
2
3
4
5
6
MAD %
GCN PubMed
Baseline
DropFeature
DropEdge
DropMessage
DropNode
DropAgg
DropPath
Learn2Drop
BBGDC
MATE
ExPass
Engage
xAI_DropEdge
xAI_DropNode
79
80
81
82
Test Accuracy
1.0
1.5
2.0
2.5
3.0
3.5
4.0
4.5
5.0
MAD %
GAT Cora
68
69
70
71
72
73
Test Accuracy
2
3
4
5
6
7
8
9
MAD %
GAT Citeseer
68
70
72
74
76
78
Test Accuracy
5
10
15
20
MAD %
GAT PubMed
Baseline
DropFeature
DropEdge
DropMessage
DropNode
DropAgg
DropPath
Learn2Drop
MATE
ExPass
Engage
xAI_DropEdge
xAI_DropNode
Fig. 2. MAD values (the higher the better for oversmoothing avoidance) as functions
of the test accuracy of the models, for GCN (top) and GAT (bottom) respectively. Each
graph focuses on a single dataset, with points corresponding to the different methods.
tween explainability-based regularization, uncertainty quantification and out-of-
domain generalization, the application of similar XAI-based solutions to design
augmentation strategies, and the study of explainability-based dropping for other
classes of deep learning architectures.
References
1. Azzolin, S., Longa, A., Barbiero, P., Lio, P., Passerini, A.: Global explainability of
GNNs via logic combination of learned concepts. In: The Eleventh International
Conference on Learning Representations (2023)
2. Chen, D., Lin, Y., Li, W., Li, P., Zhou, J., Sun, X.: Measuring and relieving the
over-smoothing problem for graph neural networks from the topological view. In:
Proceedings of the AAAI conference on artificial intelligence. vol. 34, pp. 3438–3445
(2020)
3. Fan, W., Ma, Y., Li, Q., He, Y., Zhao, E., Tang, J., Yin, D.: Graph neural networks
for social recommendation. In: The world wide web conference. pp. 417–426 (2019)
4. Fang, T., Xiao, Z., Wang, C., Xu, J., Yang, X., Yang, Y.: Dropmessage: Unifying
random dropping for graph neural networks. In: Proceedings of the AAAI Confer-
ence on Artificial Intelligence. vol. 37, pp. 4267–4275 (2023)
5. Feng, W., Zhang, J., Dong, Y., Han, Y., Luan, H., Xu, Q., Yang, Q., Kharlamov, E.,
Tang, J.: Graph random neural networks for semi-supervised learning on graphs.
Advances in neural information processing systems 33, 22092–22103 (2020)
6. Gilmer, J., Schoenholz, S.S., Riley, P.F., Vinyals, O., Dahl, G.E.: Message passing
neural networks. Machine learning meets quantum physics pp. 199–214 (2020)

xAI-Drop: Don’t Use What You Cannot Explain
15
7. Giunchiglia, V., Shukla, C.V., Gonzalez, G., Agarwal, C.: Towards training gnns
using explanation directed message passing. In: Learning on Graphs Conference.
pp. 28–1. PMLR (2022)
8. Gori,
M.,
Monfardini,
G.,
Scarselli,
F.:
A
new
model
for
learning
in
graph
domains.
In:
Proceedings.
2005
IEEE
International
Joint
Confer-
ence
on
Neural
Networks,
2005.
vol.
2,
pp.
729–734
vol.
2
(2005).
https://doi.org/10.1109/IJCNN.2005.1555942
9. Hasanzadeh, A., Hajiramezanali, E., Boluki, S., Zhou, M., Duffield, N., Narayanan,
K., Qian, X.: Bayesian graph neural networks with adaptive connection sampling.
In: International conference on machine learning. pp. 4094–4104. PMLR (2020)
10. Huang, Q., Yamada, M., Tian, Y., Singh, D., Chang, Y.: Graphlime: Local in-
terpretable model explanations for graph neural networks. IEEE Transactions on
Knowledge and Data Engineering (2022)
11. Jiang, B., Chen, Y., Wang, B., Xu, H., Luo, B.: Dropagg: Robust graph neural
networks via drop aggregation. Neural Networks 163, 65–74 (2023)
12. Jiang, D., Wu, Z., Hsieh, C.Y., Chen, G., Liao, B., Wang, Z., Shen, C., Cao, D., Wu,
J., Hou, T.: Could graph neural networks learn better molecular representation for
drug discovery? a comparison study of descriptor-based and graph-based models.
Journal of Cheminformatics 13(1), 12 (Feb 2021). https://doi.org/10.1186/s13321-
020-00479-8
13. Jiang, W., Luo, J.: Graph neural network for traffic forecasting: A survey. Expert
Systems with Applications 207, 117921 (2022)
14. Kakkad, J., Jannu, J., Sharma, K., Aggarwal, C., Medya, S.: A survey on explain-
ability of graph neural networks. arXiv preprint arXiv:2306.01958 (2023)
15. Kipf, T.N., Welling, M.: Semi-supervised classification with graph convolutional
networks. arXiv preprint arXiv:1609.02907 (2016)
16. Kipf, T.N., Welling, M.: Semi-supervised classification with graph convolutional
networks. In: International Conference on Learning Representations (2017)
17. Li, Q., Han, Z., Wu, X.M.: Deeper insights into graph convolutional networks
for semi-supervised learning. In: Proceedings of the AAAI conference on artificial
intelligence. vol. 32 (2018)
18. Li, W., Li, Y., Li, Z., Jianye, H., Pang, Y.: Dag matters! gflownets enhanced
explainer for graph neural networks. In: The Eleventh International Conference
on Learning Representations (2022)
19. Longa, A., Azzolin, S., Santin, G., Cencetti, G., Li`o, P., Lepri, B., Passerini, A.:
Explaining the explainers in graph neural networks: a comparative study. arXiv
preprint arXiv:2210.15304 (2022)
20. Luo, D., Cheng, W., Xu, D., Yu, W., Zong, B., Chen, H., Zhang, X.: Parameterized
explainer for graph neural network. Advances in neural information processing
systems 33, 19620–19631 (2020)
21. Luo, D., Cheng, W., Yu, W., Zong, B., Ni, J., Chen, H., Zhang, X.: Learning to
drop: Robust graph neural network via topological denoising. In: Proceedings of the
14th ACM international conference on web search and data mining. pp. 779–787
(2021)
22. Micheli, A.: Neural network for graphs: A contextual constructive approach. IEEE
Transactions on Neural Networks 20(3), 498–511 (2009)
23. Oono, K., Suzuki, T.: Graph neural networks exponentially lose expressive power
for node classification. ICLR2020 8 (2020)
24. Papp, P.A., Martinkus, K., Faber, L., Wattenhofer, R.: Dropgnn: Random dropouts
increase the expressiveness of graph neural networks. Advances in Neural Informa-
tion Processing Systems 34, 21997–22009 (2021)

16
V.M. De Luca et al.
25. Pope, P.E., Kolouri, S., Rostami, M., Martin, C.E., Hoffmann, H.: Explainabil-
ity methods for graph convolutional neural networks. In: Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 10772–
10781 (2019)
26. Rathee, M., Funke, T., Anand, A., Khosla, M.: Bagel: A benchmark for assessing
graph neural network explanations. arXiv preprint arXiv:2206.13983 (2022)
27. Rong, Y., Huang, W., Xu, T., Huang, J.: Dropedge: Towards deep graph convolu-
tional networks on node classification. arXiv preprint arXiv:1907.10903 (2019)
28. Rusch, T.K., Bronstein, M.M., Mishra, S.: A survey on oversmoothing in graph
neural networks. arXiv preprint arXiv:2303.10993 (2023)
29. Scarselli, F., Gori, M., Tsoi, A.C., Hagenbuchner, M., Monfardini, G.: The graph
neural network model. IEEE Transactions on Neural Networks 20(1), 61–80 (2009).
https://doi.org/10.1109/TNN.2008.2005605
30. Shi, Y., Zhou, K., Liu, N.: Engage: Explanation guided data augmentation for
graph representation learning. In: Joint European Conference on Machine Learning
and Knowledge Discovery in Databases. pp. 104–121. Springer (2023)
31. Simonyan, K., Vedaldi, A., Zisserman, A.: Deep inside convolutional net-
works: Visualising image classification models and saliency maps. arXiv preprint
arXiv:1312.6034 (2013)
32. Spinelli, I., Scardapane, S., Uncini, A.: A meta-learning approach for training
explainable graph neural networks. IEEE Transactions on Neural Networks and
Learning Systems (2022)
33. Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., Salakhutdinov, R.:
Dropout: a simple way to prevent neural networks from overfitting. The journal of
machine learning research 15(1), 1929–1958 (2014)
34. Sundararajan, M., Taly, A., Yan, Q.: Axiomatic attribution for deep networks. In:
International conference on machine learning. pp. 3319–3328. PMLR (2017)
35. Valsesia, D., Fracastoro, G., Magli, E.: Don’t stack layers in graph neural networks,
wire them randomly (2020)
36. Veliˇckovi´c, P., Cucurull, G., Casanova, A., Romero, A., Li`o, P., Bengio, Y.: Graph
attention networks. In: International Conference on Learning Representations
(2018)
37. Vu, M., Thai, M.T.: Pgm-explainer: Probabilistic graphical model explanations
for graph neural networks. Advances in neural information processing systems 33,
12225–12235 (2020)
38. Wu, Z., Pan, S., Chen, F., Long, G., Zhang, C., Yu, P.S.: A comprehensive survey
on graph neural networks. IEEE Transactions on Neural Networks and Learning
Systems 32(1), 4–24 (2021)
39. Yang, Z., Cohen, W., Salakhudinov, R.: Revisiting semi-supervised learning with
graph embeddings. In: International conference on machine learning. pp. 40–48.
PMLR (2016)
40. Ying, Z., Bourgeois, D., You, J., Zitnik, M., Leskovec, J.: Gnnexplainer: Generating
explanations for graph neural networks. Advances in neural information processing
systems 32 (2019)

