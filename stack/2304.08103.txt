Low-code LLM: Visual Programming over LLMs
Yuzhe Cai*, Shaoguang Mao∗, Wenshan Wu∗, Zehua Wang∗, Yaobo Liang, Tao Ge
Chenfei Wu, Wang You, Ting Song, Yan Xia†, Jonathan Tien, Nan Duan†
Microsoft Research Asia
{v-yuzhecai, shaoguang.mao, wenshan.wu, zehwang, yaobo.liang, tage,
chewu, v-wangyou, tsong, yanxia, jtien, nanduan}@microsoft.com
Abstract
Effectively utilizing LLMs for complex tasks is challenging, often involving a
time-consuming and uncontrollable prompt engineering process. This paper intro-
duces a novel human-LLM interaction framework, Low-code LLM. It incorporates
six types of simple low-code visual programming interactions, all supported by
clicking, dragging, or text editing, to achieve more controllable and stable re-
sponses. Through visual interaction with a graphical user interface, users can
incorporate their ideas into the workﬂow without writing trivial prompts. The
proposed Low-code LLM framework consists of a Planning LLM that designs a
structured planning workﬂow for complex tasks, which can be correspondingly
edited and conﬁrmed by users through low-code visual programming operations,
and an Executing LLM that generates responses following the user-conﬁrmed work-
ﬂow. We highlight three advantages of the low-code LLM: controllable generation
results, user-friendly human-LLM interaction, and broadly applicable scenarios.
We demonstrate its beneﬁts using four typical applications. By introducing this
approach, we aim to bridge the gap between humans and LLMs, enabling more
effective and efﬁcient utilization of LLMs for complex tasks. Our system will be
soon publicly available at LowCodeLLM.
prompt
LLM
response
edit
Low-code Human-LLM Interaction (Low-code LLM)
task prompt
Write an essay titled “Drunk Driving As A Social Issue”.
Planning LLM
Executing LLM
response
Workflow:
edit
Conventional Human-LLM Interaction
Figure 1: Overview of the Low-code human-LLM interaction (Low-code LLM) and its comparison
with the conventional interaction. The red arrow indicates the main human-model interaction loop.
*The ﬁrst four authors contributed equally. This work was performed during the ﬁrst author’s internship at
Microsoft Research Asia
†corresponding author
1
arXiv:2304.08103v2  [cs.CL]  20 Apr 2023

1
Introduction
Large language models (LLMs), such as ChatGPT(OpenAI, 2022) and GPT-4(OpenAI, 2023),
have garnered signiﬁcant interest from both academia and industry, as they demonstrate impressive
capability across a range of tasks(Bubeck et al., 2023), and are increasingly utilized in a variety
of other ﬁelds as well(Nori et al., 2023; Choi et al., 2023; Baidoo-Anu and Owusu Ansah, 2023).
However, it is not yet perfect in handling complex tasks. For example, when generating a long paper,
the presented arguments, supporting evidence, and overall structure may not always meet expectations
in diverse user scenarios. Or, when serving as a task completion virtual assistant, ChatGPT may not
always interact with users in the intended manner and may even display inappropriate behavior in
various business environments.
Effective utilization of LLMs like ChatGPT requires careful prompt engineering(Zhou et al., 2022).
However, prompt engineering can be particularly challenging when instructing LLMs to perform
complex tasks, as reﬂected in more uncontrollable responses and more time-consuming prompt
reﬁning(Tan et al., 2023). There exists a gap between providing prompts and receiving responses,
and the process of generating responses is not accessible to humans.
To reduce this gap, this paper proposes a new human-LLM interaction pattern Low-code LLM, which
refers to the concept of low-code visual programming(Hirzel, 2022), like Visual Basic(Microsoft,
1991) or Scratch(Resnick et al., 2009). Users can conﬁrm the complex execution processes through
six predeﬁned simple operations on an automatically generated workﬂow, such as adding or deleting,
graphical dragging, and text editing.
As shown in Figure 1, human-LLM interaction can be completed by: (1) A Planning LLM that
generates a highly structured workﬂow for complex tasks. (2) Users editing the workﬂow with
predeﬁned low-code operations, which are all supported by clicking, dragging, or text editing. (3) An
Executing LLM that generates responses with the reviewed workﬂow. (4) Users continuing to reﬁne
the workﬂow until satisfactory results are obtained.
We applied Low-code LLM to four types of complex tasks: long-content generation, large project
deployment, task-completion virtual assistant, and knowledge embedded system. These examples
demonstrate how the proposed framework assists users in easily controlling LLMs to complete
complex tasks. Compared with the conventional human-LLM interaction pattern, Low-code LLM
has the following advantages:
1. Controllable Generation.
Complicated tasks are decomposed into structured conducting plans
and presented to users as workﬂows. Users can control the LLMs’ execution through low-code
operations to achieve more controllable responses. The responses generated followed the customized
workﬂow will be more aligned with the user’s requirements.
2. Friendly Interaction.
The intuitive workﬂow enables users to swiftly comprehend the LLMs’
execution logic, and the low-code operation through a graphical user interface empowers users to
conveniently modify the workﬂow in a user-friendly manner. In this way, time-consuming prompt
engineering is mitigated, allowing users to efﬁciently implement their ideas into detailed instructions
to achieve high-quality results.
3. Wide applicability.
The proposed framework can be applied to a wide range of complex
tasks across various domains, especially in situations where human’s intelligence or preference are
indispensable.
2
Low-code LLM
2.1
Overview
Figure 1 demonstrates the overview framework of the Low-code LLM. Different from conventional
prompt engineering, in Low-code LLM, users ﬁrst input a task prompt, which could be a very brief
description of the task they want to achieve. Then, a Planning LLM will design a workﬂow for
completing the task. The workﬂow is a kind of structured plan, including execution procedure and
jump logic. The user will then edit the workﬂow using six pre-deﬁned low-code visual programming
2

Table 1: Format of workﬂow. For each item, it consists of two parts: execution procedure (i.e. step
name and description), and jump logic (null for sequential execution).
STEP 1: [Step Name] [Step Description] [[[If ...][Jump to STEP...]][...]]
STEP 2: [Step Name] [Step Description] [[[If ...][Jump to STEP...]][...]]
· · ·
operations. After being conﬁrmed by the user, the workﬂow will be interpreted into natural language
and inputted to the Executing LLM. The Executing LLM will generate a response with the user’s
guide.
Table 2: A workﬂow generated by Planing LLM when the user inputs "Write an essay titled ’Drunk
Driving As A Social Issue’". STEP 3 is appointed by the user to provide a more detailed sub-workﬂow.
STEP
Step Name
Step Description
Jump Logic
STEP 1
Research
Gather information on drunk driving
as a social issue, including statistics,
causes, and effects
STEP 2
Outline
Organize the information and ma-
terials into an outline, including an
introduction, body, and conclusion
If lack of materials,
jump to STEP 1
STEP 3
Write
Write the essay, including an intro-
duction that deﬁnes drunk driving as
a social issue, a body that discusses
the causes and effects of drunk driv-
ing, and a conclusion that empha-
sizes the importance of addressing
this issue
STEP 3.1
Write the introduction
Write an introduction that provides
background information on drunk
driving as a social issue and clearly
states the purpose of the essay
STEP 3.2
Write the body
Write the body of the essay, in-
cluding paragraphs that discuss the
causes and effects of drunk driving,
as well as any relevant statistics or
research
STEP 3.3
Write the conclusion
Write a conclusion that summarizes
the main points of the essay and em-
phasizes the importance of address-
ing drunk driving as a social issue
STEP 4
Proofread
Check the essay for spelling and
punctuation errors
2.2
Planning LLM and Structured Planning Workﬂow
A structured planning workﬂow is designed by the Planning LLM based on user input task prompt.
Generally, the workﬂow consists of multiple steps and jump logic between steps. To facilitate the
transformation from a workﬂow in natural language to an intuitive graphical ﬂowchart, Planning
LLM is instructed to produce structured workﬂows, as shown in Table 1, with every step consisting
3

of two parts: (1) Step: including step name and step description that users can directly revise; (2)
Jump logic. Additionally, users can extend every step of the workﬂow into a sub-workﬂow with more
details according to their preferences, and keep extending until reaching their desired level of detail.
We implement the Planning LLM with ChatGPT and educate it to draft a plan with education prompts,
which consists of (1) Role of Planning LLM: a powerful problem-solving assistant that provides a
standard operating procedure (i.e., workﬂow) for the user’s task; (2) Generation of overall workﬂow:
Planning LLM is instructed to analyze the task and provide standard operating procedure as guidance,
but is not required actually to solve the task; (3) Generation of sub-workﬂow: If a user wants to
extend a step, the Planning LLM is provided with the dialogue history of the previous generation of
the overall workﬂow to ensure logical consistency and prevent duplication of content between the
sub-workﬂow the other steps of the overall workﬂow. (4) Basic rules: Planning LLM must follow
the instructions and be strict to the output format deﬁned in Table 1.
With the education prompts, table 2 exhibits an example of a workﬂow for the task "Write an essay
titled ’Drunk Driving As A Social Issue’" generated by the Planning LLM.
2.3
Low-code Interaction with Planning Workﬂow
To more intuitively present users with the workﬂow, a ﬂowchart is utilized to visualize the workﬂow
and presented it to users. The structured workﬂow (e.g., workﬂow in Table 2) can be conveniently
converted to a ﬂowchart. Then, low-code visual programming operations enable users to easily
implement sequential execution, conditional execution, and recursive execution.
As shown in Figure 2, there are six pre-deﬁned low-code interactions on graphical ﬂowchart. We
deﬁne six types of low-code interactions for users to edit the workﬂow, including:
• Adding/removing steps by clicking buttons;
• Modifying step names or descriptions by clicking and text editing;
• Adding/removing a jump logic by clicking;
• Changing the processing order by dragging;
• Extending a step in the ﬂowchart by clicking the button;
• Regeneration, and conﬁrmation by clicking buttons.
These operations can be efﬁciently completed in a graphical user interface to achieve a very user-
friendly interaction.
2.4
Executing LLM
The modiﬁed ﬂowchart is converted back to a natural language based workﬂow (referred to as
modiﬁed workﬂow) so that it can be understood by LLMs. Executing LLM is designed to generate
responses by following the user-conﬁrmed workﬂow and engaging in interactions with users via a
conversational interface. Thanks to the user’s explicit conﬁrmation of the task execution logic in the
workﬂow, the results generated by LLMs will be more controllable and satisfactory.
We implement the Executing LLM with ChatGPT and educate it to generate responses by providing
it with education prompts, which instruct the ChatGPT to generate responses by strictly following the
provided workﬂow.
2.5
Application Scenarios
We believe that, no matter how powerful large language models will be in the future, some tasks
inevitably require users’ participation. For example, users need to communicate their ideas and
preferences, their understanding of the task, and their desired output format to the large language
models. The traditional approach is to iterate through cumbersome prompt engineering, but the
interaction method of Low-code LLM will greatly liberate users from such tedious prompt engineering.
Workﬂow is an effective intermediate language that both humans and large language models can
understand. This simple low-code operation in graphical user interface allows users to easily complete
their logical ideas, while the structured planning process allows large language models to execute
tasks more strictly according to the logic.
4

Adding/removing steps
Adding
Removing
Modify step content
Adding/removing jump logic
Adding
condition
Removing
Changing the order 
Extend sub-flowchart
Overall Flowchart
Sub-flowchart for STEP 3
Regeneration and confirm
Figure 2: Six kinds of pre-deﬁned low-code operations: (1) adding/removing steps; (2) modifying
step name or descriptions; (3) adding/removing a jump logic; (4) changing the processing order; (5)
extending a part of the ﬂowchart; (6) regeneration and conﬁrmation.
5

3
Experiments
3.1
Experimental Setup
We demonstrate the power and potential of Low-code LLM in assisting users with four categories of
tasks:
(1) Long Content generation, including long texts (such as blogs, business plans, and papers), and
posters, wherein users interact with the ﬂowchart generated by the Planning LLM to specify the
structure, idea, and focus of the generation.
(2) Large Project Development, including complex object relations and system design. Users can
educate LLMs about their architect design through low-code interactions.
(3) Task-completion Virtual assistant, where developers can predeﬁne the interaction logic between
the virtual assistant and customers by editing the ﬂowchart, and the Executing LLM will strictly
follow the logic speciﬁed by the developer to minimize potential risks.
(4) Knowledge-embedded system, where domain experts can embed their experience or knowledge
into a conducting workﬂow. Then, the counseling assistant will follow a pre-deﬁned pattern and act
as a coach to scaffold users to complete their tasks.
In particular, the Low-code LLM experiments are carried out using the OpenAI service (gpt-3.5-
turbo). In each experiment, we detail the user-deﬁned requirements, the user-provided input prompt,
the ﬂowchart created by the Planning LLM, user edits on the ﬂowchart, and the ﬁnal generation
results.
In the qualitative analysis, we examined four pilot cases in the above categories to demonstrate
the beneﬁts of Low-code LLM in achieving controllable and satisfactory results. All codes will be
released for reproduction.
3.2
Qualitative Analysis
Pilot Case 1: Essay Writing
As shown in Figure3, by enabling users to make speciﬁc edits to
the ﬂowchart, users can easily communicate with the system on their ideas and writing structures.
As a result, the generated results are very controllable and highly aligned with users’ writing plans.
Low-code interaction is a win-win collaboration of the user’s intelligence and LLM’s powerful text
generation ability.
Pilot Case 2: Object-oriented Programming
Even though large language models demonstrate
signiﬁcant capabilities in code generation, it can be challenging for users to precisely instruct their
requirements to an LLM in building complex systems. However, as shown in Figure 4, Low-code
LLM enables professional programming architects to easily input their system design through low-
code interaction. The results verify that the generated codes strictly follow the expert’s design. With
the Low-code LLM interaction, constructing a complex system becomes much more convenient for
users.
Pilot Case 3: Virtual Hotel Service
Figure 5 shows the advantages of Low-code LLM over
traditional prompt engineering for implementing a task-completion virtual assistant. By using Low-
code LLM, users, probably hotel managers, can take advantage of a structured planning ﬂowchart
and interactively deﬁne the necessary execution logic for the virtual assistant. This ensures that the
virtual assistant operates according to the managers’ exact intentions, reducing potential errors and
misbehavior. The intuitive, visual nature of the ﬂowchart allows for easy editing and modiﬁcation,
and the result shows the behaviors are tightly aligned with the speciﬁed requirements.
Pilot Case 4: Resume Helper
Figure 6 shows another scenario where Low-code LLM is helpful. In
some professional, knowledge-driven scenarios, experts can integrate execution logic and knowledge
into the workﬂow through low-code interactions. By embedding expert knowledge, users can be
scaffolded to complete their tasks. In this case, a human resource expert inputs resume creation
experiences into Resume Helper, and when users use it to complete their resumes, the Executing
LLM strictly follows the expert-deﬁned workﬂow to communicate with users. Some similar scenarios
may include psychological counseling, medical diagnosis, mock interviews, and others.
6

Low-code
Prompt: Write an essay titled "Advertising: Information or Manipulation".
The user-specified preferences:
•
Support that advertising is primarily a form of manipulation
•
Expound the argument in the introduction paragraph
•
Use three paragraphs to dialectically analyze
Flowchart generated by Planning LLM and then revised by user: (The red text is the part edited by the user)
Low-code LLM:
Certainly, here is the essay I have written on the topic "Advertising:
Information or Manipulation": Advertising: Information or Manipulation
Advertising is a ubiquitous presence in modern society, with advertisements appearing on television, billboards, and social media platforms. While
some argue that advertising provides valuable information to consumers, others contend that it is a form of manipulation that exploits people's
desires and insecurities. In this essay, I will explore both sides of the debate and argue that advertising is primarily a form of manipulation.
On the one hand, proponents of advertising argue that it provides valuable information to consumers. For example, advertisements for new products
can inform people about the latest technological advancements or health benefits. Additionally, advertisements can help consumers make informed
decisions by providing information about the features and benefits of different products. In this sense, advertising can be seen as a useful tool for
consumers to make informed choices.
On the other hand, critics of advertising argue that it is primarily a form of manipulation. Advertisements often appeal to people's emotions and
desires, rather than providing objective information. For example, advertisements for beauty products often use images of flawless models to create
an unrealistic standard of beauty that can make people feel inadequate. Similarly, advertisements for fast food restaurants often use images of
delicious food to create cravings that can lead to unhealthy eating habits. In this sense, advertising can be seen as a form of manipulation that
exploits people's desires and insecurities.
While there are certainly benefits to advertising, I believe that it is primarily a form of manipulation. Advertisements often use psychological tactics to
influence people's behavior, such as creating a sense of urgency or scarcity. Additionally, advertisements often rely on stereotypes and cultural norms
to appeal to people's emotions. For example, advertisements for cleaning products often feature women as the primary users, reinforcing gender
stereotypes and limiting women's roles to domestic tasks. In this sense, advertising can be seen as a powerful tool for shaping people's beliefs and
behaviors.
In conclusion, while some argue that advertising provides valuable information to consumers, I believe that it is primarily a form of manipulation that
exploits people's desires and insecurities. By using psychological tactics and appealing to people's emotions, advertising can shape people's beliefs
and behaviors in powerful ways. As consumers, it is important to be aware of these tactics and to critically evaluate the messages that we receive
from advertisements.
Figure 3: Essay Generation through Low-Code LLM: Users interact with the LLM by editing a
ﬂowchart, resulting in responses that are more closely aligned with their requirements. The red
section in the ﬂowchart illustrates how users modify the workﬂow. The generated output is highly
tailored to the user’s speciﬁc needs (see the highlighted parts). To obtain similar controllable results,
conventional prompt engineering requires complex prompt and heavy prompt modiﬁcation works.
7

Low-code
Prompt: 
Write a two-person battle game with python where both game characters have two attributes: health and combat power. 
Player 1 has only two skills about attacking and plays 2 has only two skills about healing.
The user-specified preferences :
Defining parent class and implementing each game character by inheriting can improve the scalability and flexibility of the code.
Flowchart generated by Planning LLM and then 
revised by user: (The red text is the part edited 
by the user)
Low-code LLM:
Sure, here is the code for the two person battle game:
```
class GameCharacter:
def __init__(self, health, combat_power):
self.health = health
self.combat_power = combat_power
class Player1(GameCharacter):
def __init__(self, health, combat_power):
super().__init__(health, combat_power)
def skill1(self, player2):
player2.health -= self.combat_power * 2
def skill2(self, player2):
player2.health -= self.combat_power * 3
class Player2(GameCharacter):
def __init__(self, health, combat_power):
super().__init__(health, combat_power)
def skill1(self):
self.health += 20
def skill2(self):
self.health += 50
def main():
player1 = Player1(100, 20)
player2 = Player2(100, 10)
while player1.health > 0 and player2.health > 0:
print("Player 1's turn.")
choice = input("Choose a skill to use (1 or 2): ")
if choice == "1":
player1.skill1(player2)
elif choice == "2":
player1.skill2(player2)
if player2.health <= 0:
break
print("Player 2's turn.")
choice = input("Choose a skill to use (1 or 2) or attack normally (3): ")
if choice == "1":
player2.skill1()
elif choice == "2":
player2.skill2()
else:
damage = random.randint(5, 15)
player1.health -= damage
print("Player 2 attacked normally and dealt", damage, "damage to Player 1.")
if player1.health <= 0:
break
if player1.health > 0:
print("Player 1 wins!")
else:
print("Player 2 wins!")
if __name__ == "__main__":
main()
```
```python
import random
class Player:
def __init__(self, name, health, combat_power):
self.name = name
self.health = health
self.combat_power = combat_power
def attack(self, other_player):
damage = random.randint(1, self.combat_power)
print(f”  {self.name} attacks
{other_player.name} for 
{damage} damage!")
other_player.health -= damage
def heal(self):
heal_amount = random.randint(1, 10)
print(f"{self.name} heals for {heal_amount} health!")
self.health += heal_amount
# Create the two players
player1 = Player("Player 1", 100, 20)
player2 = Player("Player 2", 100, 10)
# Game loop
Omitted due to limited space
```
Utilizing ChatGPT directly:
Figure 4: This case demonstrates how to empower LLMs coding using object-oriented programming
patterns via the proposed approach. Architecture design is a professional skill for deveoping large
scale project. With Low-code LLM, architects can easily educate the model about well-designed
architecture, allowing Executing LLM to generate code based on ﬂowcharts. The results from
ChatGPT performed poorly in object design. After incorporating a human-edited workﬂow, the
model was able to generate correct codes.
8

Low-code
Prompt: You are a hotel customer service, and someone calls to reserve a room.
The user-specified necessary requirements:
•
Necessary processes: warm greeting, check on the availability of rooms, and confirmation of reservations
•
Necessary information: name, check-in and check-out date, room options, contact information, payment method 
Flowchart generated by Planning LLM and then revised by user : (The red text is the part edited by the user)
Low-code LLM:
Customer: Hi.
Service: Hello! Thank you for calling. How may I assist you today?
Customer: I want to reserve a room.
Service: Of course! May I have your name and the dates you would like to check in and check out?
Customer: Sure, my name is David Lee, and I will check in next Monday, and check out next Friday.
Service: Thank you, David Lee. Let me check our availability for those dates.
Customer: OK.
Service: I apologize, but we do not have any available rooms for those dates. Would you like me to check for any other dates?
Customer: What if I check out one day earlier?
Service: Let me check for you. Yes, we do have availability if you check out on Thursday instead of Friday. Would you like me to proceed with the 
reservation?
Customer: Sure.
Service: Great! We have a few room options available for those dates. We have a standard room available for $150 per night, a deluxe room for $200 
per night, and a suite for $300 per night. Which one would you like to reserve?
Customer: Standard room is OK.
Service: Great choice! Can I have your contact information and payment method to confirm the reservation?
Customer: My phone number is 123456, and I will pay in cash.
Service: Thank you, David Lee. Your reservation for a standard room from next Monday to Thursday has been confirmed. Your confirmation number is 
123456. We look forward to seeing you soon!
Customer: OK, many thanks.
Service: You're welcome! If you have any further questions or concerns, please don't hesitate to contact us. Have a great day!
Figure 5: A virtual hotel service using Low-code LLM allows users, such as hotel managers, to
clearly deﬁne the execution logic through interaction with the structured planning ﬂowchart, ensuring
that the conduction logic strictly follows the user’s intent. The red part in the ﬂowchart shows how the
user edits the workﬂow. As a result, the generated system is highly aligned with the user’s speciﬁed
requirements (refer to the highlighted part). Without the low-code LLM interaction, it becomes
difﬁcult to control how virtual hotel service communicates with customers.
9

Low-code
Prompt: Help me to make a formal resume.
The user-specified necessary requirements:
•
Necessary information : personal information, education information, work experience, other skills
Flowchart generated by Planning LLM and 
then revised by user: (The red text is the 
part edited by the user)
Low-code LLM:
User: What information do you need about me?
Resume helper: I would need your personal details such as your full name, address, phone 
number, and email address. Additionally, I would need information about your education, 
work experience, skills, achievements, and references.
User: My name is David Lee, and I live in Beijing.
Resume helper: Great, thank you for sharing that information. Can you please provide me 
with your phone number and email address as well?
User: 123456, 123456@mail.com
Resume helper: Thank you for providing your phone number and email address. Can you 
please tell me about your educational qualifications, including the name of the institution, 
degree, major, and graduation date? 
User: Sure, I received my Bachelor’s degree from the Department of A., T. University in 2022.
Resume helper: Thank you for sharing your educational qualifications. Can you please tell 
me about your work experience, including the name of the company, job title, employment 
dates, and key responsibilities and achievements? 
User: I was a machine learning research intern at B. company from 2021 to 2022. I did lots of 
research about large language models.
Resume helper: Thank you for sharing your work experience. Can you please tell me about 
your relevant skills, including technical, language, and soft skills?
User: Sure, I can speak Chinese and English, I am good at C, C++, and Python. I have some 
experience with making videos.
Resume helper: Thank you for sharing your skills. Can you please tell me about any notable 
achievements, awards, or publications that demonstrate your skills and experience?
User: Sorry, I do not have any of them.
Resume helper: No problem, thank you for letting me know. Based on the information you 
have provided, I can help you create a formal personal CV. Would you like me to proceed 
with that?
User: Sure, show me, please.
Resume helper:
David Lee
Beijing, China
Phone: 123456
Email: 123456@mail.com
Personal Statement: 
As a machine learning research intern, I have gained experience in conducting research on 
large language models. I am proficient in C, C++, and Python, and have experience in video 
production. I am seeking a position where I can utilize my skills and experience to contribute 
to a team. 
Education: 
Bachelor's Degree in A, T. University, 2022 
Work Experience: 
Machine Learning Research Intern, B. Company, 2021-2022 
- Conducted research on large language models 
- Collaborated with team members to develop new models 
- Presented research findings to team and management 
Skills: 
- Proficient in C, C++, and Python –
- Fluent in Chinese and English 
- Experience in video production 
Thank you for using my services. Let me know if you need any further assistance.
Figure 6: Resume Helper via Low-code LLM. In this case, domain experts, maybe human resource
experts, can embed their resume-creating knowledge into a workﬂow. With the embedding of expert
knowledge, Executing LLM can scaffold users to complete their resumes. Similar examples may also
include psychological counseling, medical diagnosis, interview mock, etc.
10

4
Related Works
4.1
Large Language Models
Large language models (LLMs) have emerged as a prominent area of research in recent years, thanks
to advances in deep learning, natural language processing, and powerful computational infrastructure.
LLMs are deﬁned by their signiﬁcant model size, capacity to understand and generate human-like
text, and ability to generalize across different tasks and applications.
Recent LLMs, such as GPT-4 and ChatGPT, have made impressive strides in generating more
coherent and contextually relevant responses. They have been applied in various industries and
ﬁelds, including content creation, code development(Chen et al., 2021), customer support(George and
George, 2023), and more. However, while LLMs have demonstrated promising potential, they still
face limitations(Bowman, 2023; Borji, 2023; Bang et al., 2023). In particular, controlling the behavior
and output of LLMs for complex tasks remains a challenge, which has led to the development of new
techniques, such as prompt engineering, and methods to improve results(Wu et al., 2023; Ge et al.,
2022; Wu et al., 2022; Shen et al., 2023).
4.2
Prompt Engineering
Prompt engineering has emerged as an essential technique for interacting with LLMs to achieve
desired outcomes. The success of large language models relies heavily on their ability to produce
answers to various queries(Zuccon and Koopman, 2023). However, providing effective prompts that
convey the exact intent of humans is a non-trivial task, especially when it comes to complex tasks
and requirements.
The challenge in prompt engineering lies in crafting prompts that can manipulate the LLM into
generating speciﬁc outcomes. These prompts can vary from simple queries to more complicated
instructions requiring the LLM to analyze, reason, and produce creative solutions. Researchers have
explored various techniques to simplify prompt engineering, ranging from giving explicit instructions
to providing context for LLMs to understand the desired output better(White et al., 2023).
Some recent advancements in prompt engineering include techniques such as few-shot learning(Brown
et al., 2020; Min et al., 2022), reinforcement-learning(Deng et al., 2022). These approaches aim
at enabling LLMs to provide more accurate responses based on the user’s needs. However, these
techniques often demand substantial expertise and time, making it difﬁcult for end-users to leverage
the full potential of these LLMs.
The Low-code LLM framework proposed in our paper provides an innovative solution by involving
the users in the process of designing workﬂows, which ultimately controls the LLM’s response
generation. This approach aims at eliminating the complexity in prompt engineering and streamlines
the process by allowing users to control intermediate steps and preferences in a more user-friendly way.
By doing so, Low-code LLM fosters more transparent and controllable human-LLM collaboration.
4.3
Task Automation with LLMs
Recently, various research studies have focused on leveraging large language models for task automa-
tion(Auto-GPT, 2023; Liang et al., 2023; Kim et al., 2023). Task automation with LLMs usually
involves the model analyzing a given input, breaking it down into subtasks, and generating desired
outputs accordingly. The success of task automation with LLMs beneﬁts from the models’ ability to
understand the context and generate coherent responses.
However, the black-box nature of the interaction and the difﬁculty in controlling their output have
remained signiﬁcant challenges in deploying LLMs for complex tasks(Tan et al., 2023). Users often
face difﬁculties when attempting to direct LLMs to adhere to speciﬁc requirements or constraints,
and generating satisfactory outputs often involves time-consuming iterations.
The proposed Low-code LLM framework involves users in the interaction process and improving
controllability over generated outputs. By offering a user-friendly and efﬁcient way of specifying
preferences and constraints, the Low-code LLM framework contributes to research on task automation
with LLMs, while further bridging the gap between human users and LLMs for achieving more
structured and ﬁne-grained control.
11

5
Limitations
While the Low-code LLM framework promises a more controllable and user-friendly interaction with
large language models, there are some limitations.
One such limitation is the increase in the cognitive load for users, who now need to understand and
modify the generated workﬂows.
Furthermore, accurate and effective structured planning within the Planning LLM may be challeng-
ing, and bad structured planning poses a heavy user editing burden. But we believe with the evolution
of LLMs and research on task automation, the planning ability will be getting satisfactory.
Lastly, the current design assumes that users have sufﬁcient domain knowledge and skills to modify
and conﬁrm the generated workﬂows effectively.
6
Looking Forward
There are various promising and interesting directions for Low-code LLM:
Integration with Task Automation: There are many types of research on task automation, plan
making(Wang et al., 2023) and tool utilization(Liang et al., 2023; Nakano et al., 2021) by LLMs.
As the planning ability and skill sets of LLMs become more powerful, user intervention in low-
code interaction will decrease. However, low-code interaction still provides users a friendly way to
make some simple edits and conﬁrm the LLM execution logic. Meanwhile, the edited or conﬁrmed
workﬂow can be a valuable resource for improving task automation.
Cross-Platform Integration: Low-Code LLM has the potential to integrate with a wide range of
platforms, allowing users to conveniently implement these systems across numerous applications and
tools. As a versatile interaction pattern, Low-Code LLM is not limited to language models but can be
employed in various cross-modality human-computer interactions.
Expansive Application Scenarios: This paper presents four pilot cases, and we believe that Low-Code
LLM can be widely applied across scenarios. There will always be situations where human opinion
or information is important, and Low-Code LLM offers an effective interaction framework for easy
human-LLM collaboration.
7
Conclusion
This paper introduced a novel human-LLM interaction framework called Low-code LLM, which
aims to improve the control and efﬁciency of utilizing large language models for complex tasks.
Low-code LLM allows users to better understand and modify the logic and workﬂow underlying the
LLMs’ execution of instructions. The demonstration cases reveal the advantages of our approach,
including stronger control over LLMs, a user-friendly interaction and wide applicability. Compared
with prompt engineering, the proposed Low-code LLM framework advances the state-of-the-art in
human-LLM interactions by bridging the gap of communication and collaboration between humans
and LLMs. We believe the Low-code LLM framework presents a promising solution to many of
the challenges faced by LLM users today and has the potential to greatly impact a wide range of
industries and applications.
Acknowledgment
Part of this paper has been collaboratively crafted through interactions with the proposed Low-code
LLM. The process began with GPT-4 outlining the framework, followed by the authors supplementing
it with innovative ideas and reﬁning the structure of the workﬂow. Ultimately, GPT-4 took charge of
generating cohesive and compelling text.
References
OpenAI. Chatgpt, 2022. URL https://openai.com/blog/chatgpt.
12

OpenAI. Gpt-4 technical report, 2023.
Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar,
Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artiﬁcial general intelligence:
Early experiments with gpt-4. arXiv preprint arXiv:2303.12712, 2023.
Harsha Nori, Nicholas King, Scott Mayer McKinney, Dean Carignan, and Eric Horvitz. Capabilities
of gpt-4 on medical challenge problems. arXiv preprint arXiv:2303.13375, 2023.
Jonathan H Choi, Kristin E Hickman, Amy Monahan, and Daniel Schwarcz. Chatgpt goes to law
school. Available at SSRN, 2023.
David Baidoo-Anu and Leticia Owusu Ansah. Education in the era of generative artiﬁcial intelligence
(ai): Understanding the potential beneﬁts of chatgpt in promoting teaching and learning. Available
at SSRN 4337484, 2023.
Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan,
and Jimmy Ba.
Large language models are human-level prompt engineers.
arXiv preprint
arXiv:2211.01910, 2022.
Yiming Tan, Dehai Min, Yu Li, Wenbo Li, Nan Hu, Yongrui Chen, and Guilin Qi. Evaluation
of chatgpt as a question answering system for answering complex questions. arXiv preprint
arXiv:2303.07992, 2023.
Martin Hirzel. Low-code programming models. arXiv preprint arXiv:2205.02282, 2022.
Microsoft.
Visual basic, 1991.
URL https://learn.microsoft.com/en-Us/dotnet/
visual-basic/getting-started/.
Mitchel Resnick, John Maloney, Andrés Monroy-Hernández, Natalie Rusk, Evelyn Eastmond, Karen
Brennan, Amon Millner, Eric Rosenbaum, Jay Silver, Brian Silverman, et al. Scratch: programming
for all. Communications of the ACM, 52(11):60–67, 2009.
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared
Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large
language models trained on code. arXiv preprint arXiv:2107.03374, 2021.
A Shaji George and AS Hovan George. A review of chatgpt ai’s impact on several business sectors.
Partners Universal International Innovation Journal, 1(1):9–23, 2023.
Samuel R Bowman.
Eight things to know about large language models.
arXiv preprint
arXiv:2304.00612, 2023.
Ali Borji. A categorical archive of chatgpt failures. arXiv preprint arXiv:2302.03494, 2023.
Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan Wilie, Holy Lovenia,
Ziwei Ji, Tiezheng Yu, Willy Chung, et al. A multitask, multilingual, multimodal evaluation of
chatgpt on reasoning, hallucination, and interactivity. arXiv preprint arXiv:2302.04023, 2023.
Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, and Nan Duan. Vi-
sual chatgpt: Talking, drawing and editing with visual foundation models.
arXiv preprint
arXiv:2303.04671, 2023.
Tao Ge, Jing Hu, Li Dong, Shaoguang Mao, Yan Xia, Xun Wang, Si-Qing Chen, and Furu Wei.
Extensible prompts for language models. arXiv preprint arXiv:2212.00616, 2022.
Tongshuang Wu, Ellen Jiang, Aaron Donsbach, Jeff Gray, Alejandra Molina, Michael Terry, and
Carrie J Cai. Promptchainer: Chaining large language model prompts through visual programming.
In CHI Conference on Human Factors in Computing Systems Extended Abstracts, pages 1–10,
2022.
Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. Hugginggpt:
Solving ai tasks with chatgpt and its friends in huggingface. arXiv preprint arXiv:2303.17580,
2023.
13

Guido Zuccon and Bevan Koopman. Dr chatgpt, tell me what i want to hear: How prompt knowledge
impacts health answer correctness. arXiv preprint arXiv:2302.13793, 2023.
Jules White, Quchen Fu, Sam Hays, Michael Sandborn, Carlos Olea, Henry Gilbert, Ashraf Elnashar,
Jesse Spencer-Smith, and Douglas C Schmidt. A prompt pattern catalog to enhance prompt
engineering with chatgpt. arXiv preprint arXiv:2302.11382, 2023.
Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel
Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler,
Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott
Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya
Sutskever, and Dario Amodei. Language models are few-shot learners, 2020.
Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke
Zettlemoyer. Rethinking the role of demonstrations: What makes in-context learning work?, 2022.
Mingkai Deng, Jianyu Wang, Cheng-Ping Hsieh, Yihan Wang, Han Guo, Tianmin Shu, Meng Song,
Eric P. Xing, and Zhiting Hu. Rlprompt: Optimizing discrete text prompts with reinforcement
learning, 2022.
Auto-GPT. Auto-gpt, 2023. URL https://github.com/Significant-Gravitas/Auto-GPT.
Yaobo Liang, Chenfei Wu, Ting Song, Wenshan Wu, Yan Xia, Yu Liu, Yang Ou, Shuai Lu, Lei Ji,
Shaoguang Mao, et al. Taskmatrix. ai: Completing tasks by connecting foundation models with
millions of apis. arXiv preprint arXiv:2303.16434, 2023.
Geunwoo Kim, Pierre Baldi, and Stephen McAleer. Language models can solve computer tasks.
arXiv preprint arXiv:2303.17491, 2023.
Dongjie Wang, Chang-Tien Lu, and Yanjie Fu. Towards automated urban planning: When generative
and chatgpt-like ai meets urban planning. arXiv preprint arXiv:2304.03892, 2023.
Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher
Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. Webgpt: Browser-assisted
question-answering with human feedback. arXiv preprint arXiv:2112.09332, 2021.
14

