Hierarchical Bayesian Approaches to Seismic
Imaging and Other Geophysical Inverse Problems
by
Sam Ahmad Zamanian
B.S., Biomedical Engineering
Johns Hopkins University (2005)
S.M., Electrical Engineering and Computer Science
Massachusetts Institute of Technology (2007)
Electrical Engineer
Massachusetts Institute of Technology (2014)
Submitted to the Department of Electrical Engineering and Computer Science
in partial fulﬁllment of the requirements for the degree of
Doctor of Philosophy
at the
MASSACHUSETTS INSTITUTE OF TECHNOLOGY
September 2014
c⃝Massachusetts Institute of Technology 2014. All rights reserved.
Author . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Department of Electrical Engineering and Computer Science
August 13, 2014
Certiﬁed by. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Michael C. Fehler
Senior Research Scientist
Thesis Supervisor
Accepted by . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Leslie A. Kolodziejski
Professor of Electrical Engineering and Computer Science
Chair, Department Committee on Graduate Students

In loving memory of my late mother,
Farideh Payandeh Zamanian,
and,
to my father,
Ahmad Zamanian.

Hierarchical Bayesian Approaches to Seismic Imaging and
Other Geophysical Inverse Problems
by
Sam Ahmad Zamanian
Submitted to the Department of Electrical Engineering and Computer Science
on August 13, 2014, in partial fulﬁllment of the
requirements for the degree of
Doctor of Philosophy
Abstract
In many geophysical inverse problems, smoothness assumptions on the underlying
geologic model are utilized to mitigate the eﬀects of poor data coverage and obser-
vational noise and to improve the quality of the inferred model parameters. In the
context of Bayesian inference, these smoothness assumptions take the form of a prior
distribution on the model parameters. Conventionally, the regularization parame-
ters deﬁning these assumptions are ﬁxed independently from the data or tuned in
an ad hoc manner. However, it is often the case that the smoothness properties of
the true earth model are not known a priori, and furthermore, these properties may
vary spatially. In the seismic imaging problem, for example, where the objective is
to estimate the earth’s reﬂectivity, the reﬂectivity model is smooth along a particular
reﬂector but exhibits a sharp contrast in the direction orthogonal to the reﬂector. In
such cases, deﬁning a prior using predeﬁned smoothness assumptions may result in
posterior estimates of the model that incorrectly smooth out these sharp contrasts.
In this thesis, we explore the application of Bayesian inference to diﬀerent geophys-
ical inverse problems and seek to address issues related to smoothing by appealing to
the hierarchical Bayesian framework. We capture the smoothness properties of the
prior distribution on the model by deﬁning a Markov random ﬁeld (MRF) on the set
of model parameters and assigning weights to the edges of the underlying graph; we
refer to these parameters as the edge strengths of the MRF. We investigate two cases
where the smoothing is speciﬁed a priori and introduce a method for estimating the
edge strengths of the MRF.
In the ﬁrst part of this thesis, we apply a Bayesian inference framework (where
the edge strengths of the MRF are predetermined) to the problem of characterizing
the fractured nature of a reservoir from seismic data. Our methodology combines
diﬀerent features of the seismic data, particularly P-wave reﬂection amplitudes and
scattering attributes, to allow for estimation of fracture properties under a larger
physical regime than would be attainable using only one of these data types. Through
this application, we demonstrate the capability of our parameterization of the prior
distribution with edge strengths to both enforce smoothness in the estimates of the
3

fracture properties and capture a priori information about geological features in the
model (such as a discontinuity that may arise in the presence of a fault). We solve the
inference problem via loopy belief propagation to approximate the posterior marginal
distributions of the fracture properties, as well as their maximum a posteriori (MAP)
and Bayes least squares estimates.
In the second part of the thesis, we investigate how the parameters deﬁning the
prior distribution are connected to the model covariance and address the question of
how to optimize these parameters in the context of the seismic imaging problem. We
formulate the seismic imaging problem within the hierarchical Bayesian setting, where
the edge strengths are treated as random variables to be inferred from the data, and
provide a framework for computing the marginal MAP estimate of the edge strengths
by application of the expectation-maximization (E-M) algorithm. We validate our
methodology on synthetic datasets arising from 2-D models. The images we obtain
after inferring the edge strengths exhibit the desired spatially-varying smoothness
properties and yield sharper, more coherent reﬂectors.
In the ﬁnal part of the thesis, we shift our focus and consider the problem of time-
lapse seismic processing, where the objective is to detect changes in the subsurface
over a period of time using repeated seismic surveys. We focus on the realistic case
where the surveys are taken with diﬀering acquisition geometries. In such situations,
conventional methods for processing time-lapse data involve inverting surveys sepa-
rately and subtracting the inversion models to estimate the change in model param-
eters; however, such methods often perform poorly as they do not correctly account
for diﬀering model uncertainty between surveys due to diﬀerences in illumination and
observational noise. Applying the machinery explored in the previous chapters, we
formulate the time-lapse processing problem within the hierarchical Bayesian setting
and present a framework for computing the marginal MAP estimate of the time-lapse
change model using the E-M algorithm. The results of our inference framework are
validated on synthetic data from a 2-D time-lapse seismic imaging example, where
the hierarchical Bayesian estimates signiﬁcantly outperform conventional time-lapse
inversion results.
Thesis Supervisor: Michael C. Fehler
Title: Senior Research Scientist
4

Acknowledgments
There are a countless number of people (actually countable and ﬁnite) to whom I
owe a great a deal of gratitude for their support during the progress of my doctoral
studies.
I would like to thank my thesis advisor, Mike Fehler, who has truly been an amaz-
ing mentor during these four years at MIT’s Earth Resources Laboratory. Mike was
generous enough to take me on as a graduate student from a diﬀerent department
and has been both very patient and encouraging as I developed my knowledge of geo-
physics and searched for problems in the ﬁeld that would both interest and challenge
me. Often times, due to our diﬀerent technical backgrounds, Mike and I would tend
to think about problems from diﬀerent perspectives, but this has made our technical
discussions very enjoyable and all the more insightful. As an advisor, Mike really has
gone above and beyond the call of duty, and it is obvious he truly enjoys working
with his students; beyond our technical discussions, our conversations about work
life, student life, and family life (among other lives) have always been enjoyable and
have taught me a lot about life in general. Thank you, Mike; I truly could not have
asked for a better advisor.
I am much indebted to Bill Rodi who, in addition to serving on my doctoral com-
mittee, has been like an informal co-advisor. Sometimes ﬁve days a week (and many
times, seven), I would bump into Bill in the ERL coﬀee room, and what would begin
as an informal chat about some topic in inverse theory (or some other mathematical
point) would often spiral into an all-out technical discussion on the whiteboard (con-
veniently located right next to the coﬀee room). Bill’s excitement about the problems
we would discuss was always tangible (and often evidenced by the fact that he was
willing to sacriﬁce his coﬀee break, plus usually another hour that I’m sure he had
scheduled for some other task, to discuss them). He really is a wealth of knowledge at
ERL in the ﬁeld of geophysical inverse theory, and I learned a great deal from him in
our numerous discussions. In addition to his technical expertise, his sense of humor
is also a force to be reckoned with.
5

I am also very grateful for the encouragement and suggestions provided by the
other members of my doctoral committee: Bill Freeman, Alan Willsky, and Jonathan
Kane. I beneﬁted immensely from Bill and Alan’s vast expertise in Bayesian infer-
ence and graphical models. Alan has made himself available for technical advice on
multiple occasions from an early point in my graduate student career. It was he who
suggested early on that I study probabilistic graphical models more deeply to better
motivate my research in geophysical inverse problems. He subsequently served on my
RQE committee and made useful suggestions on the research presented therein. His
advice undoubtedly helped guide my studies in a direction that led to the bulk of
the research presented in this thesis. Jonathan Kane, a fellow Bayesian geophysicist,
provided many helpful suggestions, including the suggestion to explore an extension
of the least-squares migration problem. I must also thank Jonathan for introducing
me to the people involved in research at Shell, which has helped open doors allowing
me to take the next step in my research career.
I am also thankful to my collaborators, Dan Burns, Xinding Fang, and Di Yang.
Dan provided useful advice for the fracture characterization work of Chapter 3, and
Xinding provided the synthetic data used for testing the method developed in this
chapter. Di provided the acoustic data used in Chapter 5 and also helped motivate
the idea for the time-lapse study in Chapter 6. I also beneﬁted greatly from my
discussions with other faculty and research staﬀat MIT, including Alison Malcolm,
Oleg Poliannikov, Tianrun Chen, Yingcai Zheng, Dale Morgan, and Steve Brown
of ERL, Devavrat Shah of LIDS, and John Fisher of CSAIL. In addition, various
discussions with Michael Prange of Schlumberger and Anu Chandran, Henning Kuehl,
Vanessa Goh, and Ken Matson of Shell have been very beneﬁcial.
During my PhD, I was fortunate enough to have the opportunity to twice TA
6.041 (Probabilistic Systems Analysis), both times under John Tsitsiklis. I learned
a great deal from John, both from his style of teaching and from his approach to
probability and mathematics. My experience as a TA was certainly an unforgettable
one and was made all the more unforgettable by the good times shared with my fellow
6.041 TA’s: Aliaa Atwi, Uzoma Orji, and Shashank Dwivedi.
6

I would be remiss if I did not acknowledge the support of George Verghese and
Thomas Heldt, who had advised me when I was a master’s student and continued to
provide encouragement and advice during my doctoral studies. George, in particular,
generously oﬀered his advice both as I was preparing to return to MIT to complete
my PhD and as I was preparing for the RQE.
Thanks are due to Sue Turbak of ERL for being a wonderful administrator (and,
as she would put it, “lab mom”) as well as to the new ERL administrator, Natalie
Counts.
I am also grateful to Anna Shaughnessy for her eﬀorts in keeping ERL
and its graduate students well-connected to members of the energy industry. I also
thank Janet Fischer of the EECS Graduate Oﬃce for her assistance and guidance
throughout the doctoral program.
My friends at ERL have made my time there extremely enjoyable and have helped
make bearable even the most adversarial slumps in my research. It is hard to imagine
how I would have been able to complete these four years without their companionship
and camaraderie.
During my stay at ERL, I’ve had the pleasure of keeping the
good company of Sudhish Bakku, Di Yang, Alan Richardson, Nasruddin Nazerali,
Hua Wang, Andrey Shabelansky, Xinding Fang, Abdulaziz AlMuhaidib, Junlin Li,
Fuxian Song, Bram Willemsen, Diego Concha, Bongani Mashele, Greg Ely, Jared
Atkinson, Saleh Al Nasser, Haijing Zhang, Gabi Melo Silva, Yulia Agramakova, Ruel
Jerry, Xuefang Shang, Chunquan Yu, Hussam Busfar, Mirna Slim, Leonardo Zepada,
Beatrice Parker, Chen Gu, Jing Liu, Nate Dixon, Scott Burdick, Noa Bechor Ben Dov,
Farrokh Sheibani, Haoyue Wang, Yuval Tal, Martina Coccia, Manuel Torres, and Eva
Golos. I have thoroughly enjoyed my many late night lab dinners with Sudhish and
his ﬁancée Sara (which would often times include home-cooked food that Sara would
generously bring to the lab).
My numerous discussions with Sudhish, Di, Alan,
and Nasruddin about politics, culture, gastronomy, and philosophy (and, of course,
geophysics) have always been very enjoyable. I will never forget the ﬁnal month of
my PhD studies (which incidentally occurred at the same time as Ramadan), every
night of which was spent writing this dissertation; both Nasruddin and Sudhish were
also immersed in their research at this time, hence also staying up well into the night,
7

and they both faithfully kept me company during those many long nights of writing.
The 3 AM breakfasts Nasruddin and I shared that month will always remain a fond
memory.
The time I spent at MIT and in the Boston area has allowed me to make many
other good friends here, including Kashif Sheikh, Omair Saadat, Ebraheim Ismail,
Mahdi Ghassemi, Asad Lodhia, Hamza Fawzi, and Faisal Kashif. My discussions
with Asad about stochastic PDEs and with Hamza about convex optimization have
helped me gain further insight into my research. I’d particularly like to thank Faisal,
not only for his warm friendship, but also for providing valuable guidance as a senior
student at multiple points along my journey in graduate school. I’d also like to thank
my good friends from my undergraduate days at Johns Hopkins for their continued
friendship and support (including Nabil Rab, Bilal Farooqi, Sameer Ahmed, Zain
Syed, Abdulrasheed Alabi, SaﬁShareef, Usman Zaheer, Nurain Fuseini, and Imad
Qayyum); one can be certain he has found true friends when they continue to call
after almost a decade.
Now I come to the point where I mention those nearest and dearest to me. My
father, Ahmad Zamanian, has always been a source of inspiration in my life. Also
an engineer, he intrigued me from a young age with his technical prowess; I still
remember being perplexed by the math puzzles he would serve me when I was still
in elementary school. Even today, when we talk on the phone, every now and then
he might still relate to me new math puzzles (but now I can at least solve them
half the time as well as sometimes respond with a puzzle of my own). His love and
encouragement have helped me reach where I am today; he remains an inspiration
to me in all aspects of life, and I continue to learn from him every time we talk. My
beloved late mother, Farideh Payandeh Zamanian, whom I miss dearly, has been a
constant support throughout my life. I am the person I am today because of her
selﬂess love and support. In every point of my life, she had always pushed me to
exceed what I thought were my limits and taught me not to place barriers on my
potential.
Her memory will always remain dear in my heart, and I dedicate this
thesis to her and to my father. I am also grateful to my sisters, Neeki and Donna, for
8

their lifelong friendship and support. I also thank Grazyna and her family for their
kind support during my PhD.
I cannot begin to express my thanks to my beloved wife, Sabah, who has made
a tremendous sacriﬁce by joining me on this arduous journey. She has put up with
my numerous all-nighters and, due to my lack of availability, has often borne alone
much of the responsibility of taking care of our family. Her dedication, love, and
support have helped me persevere in my studies, even through the most diﬃcult
times. Thank you, Sabah, for everything you’ve done for me and our family; I look
forward to our continued journey together as we begin the next chapter of our lives.
My two daughters, Maryam and Zaynab, have been a source of immense joy and are
the light of my life. Seeing them always brings a smile to my face, and I thank God
for these precious gifts. They have certainly been a strong motivation for me to push
myself to ﬁnd the light at the end of the tunnel and ﬁnish my PhD. While Zaynab is
likely too young to remember her time here, I hope Maryam will recall fond memories
of her childhood living at Westgate on the MIT campus. I would also like to thank
my mother-in-law, Zahida, for her support (and for coming to MIT to help us with
baby Zaynab when she was born).
The research presented in this thesis was supported by the MIT Energy Initiative,
Shell International E&P Inc., and Total S.A.
9

10

Contents
1
Introduction
25
1.1
Thesis Outline and Summary of Contributions . . . . . . . . . . . . .
27
2
A Brief Primer on Inverse Problems, Bayesian Inference, and Graph-
ical Models
33
2.1
An Introduction to Inverse Problems
. . . . . . . . . . . . . . . . . .
33
2.2
Bayesian Inference
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
36
2.3
Probabilistic Graphical Models, Markov Random Fields, and Covariance 41
2.3.1
Probabilistic Graphical Models
. . . . . . . . . . . . . . . . .
42
2.3.2
Edge Strengths and Covariance . . . . . . . . . . . . . . . . .
44
3
Bayesian Fracture Characterization
49
3.1
Summary
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
49
3.2
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
50
3.3
Description of the Problem . . . . . . . . . . . . . . . . . . . . . . . .
52
3.4
Bayesian Inference Framework . . . . . . . . . . . . . . . . . . . . . .
54
3.4.1
Prior Model . . . . . . . . . . . . . . . . . . . . . . . . . . . .
56
3.4.2
Likelihood Model . . . . . . . . . . . . . . . . . . . . . . . . .
59
3.4.3
Inference Algorithms . . . . . . . . . . . . . . . . . . . . . . .
70
3.5
Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
73
3.5.1
Synthetic Data
. . . . . . . . . . . . . . . . . . . . . . . . . .
73
3.5.2
Results of Inference Procedure . . . . . . . . . . . . . . . . . .
76
3.6
Conclusions and Future Work . . . . . . . . . . . . . . . . . . . . . .
82
11

4
Least-Squares Migration with a Hierarchical Bayesian Framework
87
4.1
Summary
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
87
4.2
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
88
4.3
Methodology
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
90
4.3.1
Standard Kirchhoﬀ-based LSM framework . . . . . . . . . . .
90
4.3.2
Bayesian Framework . . . . . . . . . . . . . . . . . . . . . . .
92
4.3.3
The Expectation-Maximization (E-M) Algorithm
. . . . . . .
98
4.3.4
Application of E-M to LSM
. . . . . . . . . . . . . . . . . . .
101
4.4
Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
108
4.5
Conclusions and Future Work . . . . . . . . . . . . . . . . . . . . . .
111
5
Interpretation and Estimation of Regularization Parameters
119
5.1
Connecting Regularization Parameters and Covariance Functions . . .
120
5.2
Variational Bayesian Estimation of Regularization Parameters . . . .
127
5.2.1
Hierarchical Bayesian Formulation . . . . . . . . . . . . . . . .
127
5.2.2
Variational Bayesian Methods . . . . . . . . . . . . . . . . . .
130
5.2.3
Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
137
5.3
Conclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
161
6
Hierarchical Bayesian Time-Lapse Seismic Processing
167
6.1
Summary
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
167
6.2
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
168
6.3
Methodology and Bayesian Framework . . . . . . . . . . . . . . . . .
169
6.3.1
The E-M Algorithm for Time-Lapse Inversion
. . . . . . . . .
171
6.4
Numerical Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
173
6.5
Conclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
177
7
Conclusions
179
7.1
Summary of Main Contributions . . . . . . . . . . . . . . . . . . . . .
179
7.2
Directions for Future Work . . . . . . . . . . . . . . . . . . . . . . . .
181
7.3
Final Remarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
183
12

A Derivation of the Thomsen anisotropy parameters from excess frac-
ture compliance
185
13

14

List of Figures
2-1
The Markov random ﬁeld imposed on m by ﬁxing β prior to observing
the data d, for a simple nine pixel image. . . . . . . . . . . . . . . . .
42
2-2
Computed covariance functions (ﬁrst column) and sample draws (sec-
ond column) from N (0, (λ(D(β)+ǫI))−1) with each βij = β for (A-B)
β = 1, (C-D) β = 0.5, (E-F) β = 0.1, (G-H) β = 0.01, (I-J) β = 0. . .
46
2-3
Computed covariance functions (ﬁrst column) and sample draws (sec-
ond column) from N (0, (λ(D(β) + ǫI))−1) with βij = 0 along a hor-
izontal discontinuity at z = 40.5 m (centered at x = 51 m) of length
(A-B) 21 m, (C-D) 51 m, (E-F) inﬁnite length. βij = 1 elsewhere.
. .
47
3-1
A simple model of the problem setting. The formation consists of ﬁve
ﬂat homogeneous layers with fractures that may be present in the third
layer and measurements obtained from the 2-D array of surface seismic
receivers. Figure modiﬁed from Willis et al. [78]. . . . . . . . . . . . .
53
3-2
Undirected graphical model G over which m is Markov, prior to ob-
serving the seismic data. The model is based on the assumption that
the values of the fracture properties at one location are dependent on
those of its nearest neighbors. . . . . . . . . . . . . . . . . . . . . . .
57
15

3-3
A cartoon depicting the meaning of fracture transfer function for layer
r2. I(ω) is the incident waveﬁeld, T(ω) is the transmitted waveﬁeld into
the fractured layer, and O1(ω) and O2(ω) are the waves reﬂected by
layers above and below the fracture zone, respectively. Theoretically,
the the fracture transfer function at angular frequency ω is deﬁned as
FTF(ω) = O2(ω)
O1(ω). Figure adapted from Fang et al. [22].
. . . . . . . .
64
3-4
(a) Graphical model showing the Markovianity between the observa-
tions d and the fracture parameters m. (b) Graphical model for the
posterior distribution after removing the observed nodes. . . . . . . .
69
3-5
Approximate MAP estimates of the fracture properties computed for
models of a single fracture set, with fracture compliance 10−9 m/Pa,
fracture strike ϕij = 60◦, and varying fracture spacing, and with
smoothness parameter set to βc = 0.1.
. . . . . . . . . . . . . . . . .
78
3-6
Approximate BLS estimates of the fracture properties computed for
models of a single fracture set, with fracture compliance 10−9 m/Pa,
fracture strike ϕij = 60◦, and varying fracture spacing, and with
smoothness parameter set to βc = 0.1.
. . . . . . . . . . . . . . . . .
79
3-7
Approximate posterior marginal distributions (blue) of the fracture
properties at a single node plotted along with the prior distributions
(green). Results are given as mean ± 1 S.D. over all grid nodes. True
value is plotted with a red ‘x’. Computed for models of a single fracture
set, with fracture compliance 10−9 m/Pa, fracture strike ϕij = 60◦, and
varying fracture spacing, and with smoothness parameter set to βc = 0.1. 80
3-8
Approximate MAP estimates of the fracture properties computed on a
model containing a single set of fractures with fracture compliance 10−9
m/Pa, fracture spacing 80 m, and fracture orientation 60◦. Ground
truth is plotted along with the estimates using various values for the
smoothness parameter βc.
. . . . . . . . . . . . . . . . . . . . . . . .
83
16

3-9
Eﬀect of a priori knowledge of the fault on approximate BLS estimates
of the fracture azimuth of a model containing two fracture sets. The
fractures on the left are at azimuth 120◦, spacing 100 m, and fracture
compliance 10−9 m/Pa.
The fractures on the right are at azimuth
80◦, spacing 12 m, and fracture compliance 10−9 m/Pa. Ground truth
is plotted along with the estimates with the smoothness parameter
βc = 0.1. The fourth pane shows a comparison of the estimates at the
horizontal slice North=2000 m.
. . . . . . . . . . . . . . . . . . . . .
84
3-10 Eﬀect of a priori knowledge of the fault on approximate BLS estimates
of the fracture excess compliance of a model containing two fracture
sets. The fractures on the left are at azimuth 120◦, spacing 100 m, and
fracture compliance 10−9 m/Pa. The fractures on the right are at az-
imuth 80◦, spacing 12 m, and fracture compliance 10−9 m/Pa. Ground
truth is plotted along with the estimates with the smoothness param-
eter βc = 0.1. The fourth pane shows a comparison of the estimates at
the horizontal slice North=2000 m. . . . . . . . . . . . . . . . . . . .
85
4-1
The Markov random ﬁeld imposed on m by ﬁxing β prior to observing
the data d, for a simple nine pixel image. . . . . . . . . . . . . . . . .
95
4-2
The directed graphical model capturing the Markov chain structure
between β, m, and d. The node for d is shaded to indicate that d is
an observed quantity that the posterior distributions of β and m are
conditioned upon. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
96
4-3
True image for the three layer test case. . . . . . . . . . . . . . . . . .
112
4-4
Noisy synthetic data for the three layer test case.
. . . . . . . . . . .
112
4-5
Kirchhoﬀmigrated image for the three layer test case.
Correlation
with true image = 0.4705. . . . . . . . . . . . . . . . . . . . . . . . .
112
4-6
Unregularized LSM image (each βij = 0) for the three layer test case.
Correlation with true image = 0.3649.
. . . . . . . . . . . . . . . . .
113
17

4-7
Uniformly regularized LSM image (each βij = 1) for the three layer
test case. Correlation with true image = 0.5879. . . . . . . . . . . . .
113
4-8
Empirical Bayesian MAP image (computed after estimating β) for the
three layer test case. Correlation with true image = 0.9607. . . . . . .
113
4-9
Edge strengths β estimated with E-M algorithm for the three layer
test case. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
114
4-10 True image for the Marmousi test case. . . . . . . . . . . . . . . . . .
114
4-11 Synthetic data for the Marmousi test case prior to adding noise. . . .
114
4-12 Noisy synthetic data for the Marmousi test case. . . . . . . . . . . . .
115
4-13 Kirchhoﬀmigrated image for the Marmousi test case. Correlation with
true image = 0.4371. . . . . . . . . . . . . . . . . . . . . . . . . . . .
115
4-14 Unregularized LSM image (each βij = 0) for the Marmousi test case.
Correlation with true image = 0.3682.
. . . . . . . . . . . . . . . . .
115
4-15 Uniformly regularized LSM image (each βij = 1) for the Marmousi test
case. Correlation with true image = 0.6369.
. . . . . . . . . . . . . .
116
4-16 Empirical Bayesian MAP image (computed after estimating β) for the
Marmousi test case. Correlation with true image = 0.7973. . . . . . .
116
4-17 Edge strengths β estimated with E-M algorithm for the Marmousi test
case. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
116
5-1
Comparison of numerically computed covariances in the discrete case to
the covariance function in the continuous limit, with λ = 1, ǫ = 10−3,
and for diﬀerent values of β. Covariances for the discrete case were
computed with the central node on a 101-by-101 node grid having grid
cell length ℓ= 1 m. (Note that the range of the x-axis is reduced for
small values of β to make the plots more visible.)
. . . . . . . . . . .
126
5-2
The directed graphical model capturing the Markov structure between
β, λ, ζ, m, and d. The node for d is shaded to indicate that d is an
observed quantity that the posterior distribution for the regularization
parameters and model is conditioned upon. . . . . . . . . . . . . . . .
129
18

5-3
The true Marmousi reﬂectivity model.
. . . . . . . . . . . . . . . . .
140
5-4
Unregularized LSM image (each βij = 0) using Kirchhoﬀmodeled data.
Correlation with true image = 0.3682.
. . . . . . . . . . . . . . . . .
140
5-5
Uniformly regularized LSM image (each βij = 1) with λ = 0.01¯λ∗and
ζ = ¯ζ∗using Kirchhoﬀmodeled data. Correlation with true image =
0.4364. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
140
5-6
Uniformly regularized LSM image (each βij = 1) with λ = 0.1¯λ∗and
ζ = ¯ζ∗using Kirchhoﬀmodeled data. Correlation with true image =
0.6282. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
141
5-7
Uniformly regularized LSM image (each βij = 1) with λ = ¯λ∗and
ζ = ¯ζ∗using Kirchhoﬀmodeled data. Correlation with true image =
0.6432. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
141
5-8
Uniformly regularized LSM image (each βij = 1) with λ = 10¯λ∗and
ζ = ¯ζ∗using Kirchhoﬀmodeled data. Correlation with true image =
0.4870. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
141
5-9
Variational Bayes MAP image using Kirchhoﬀmodeled data. Correla-
tion with true image = 0.8218. . . . . . . . . . . . . . . . . . . . . . .
142
5-10 Edge strengths β∗estimated with VB using Kirchhoﬀmodeled data.
142
5-11 The variational Bayesian approximations to the posterior distributions
for the parameters scaling the inverse variances of the (a) model λ and
(b) noise ζ, using Kirchhoﬀmodeled data.
. . . . . . . . . . . . . . .
144
5-12 The true reﬂectivity for the three layer model. . . . . . . . . . . . . .
147
5-13 Unregularized LSM image (each βij = 0) using wave-equation modeled
data (three layer model). Correlation with true image = 0.0824. . . .
148
5-14 Uniformly regularized LSM image (each βij = 1) with λ = ¯λ∗and ζ =
¯ζ∗using wave-equation modeled data (three layer model). Correlation
with true image = 0.7226. . . . . . . . . . . . . . . . . . . . . . . . .
149
5-15 Uniformly regularized LSM image (each βij = 1) with λ = 10¯λ∗and
ζ = ¯ζ∗using wave-equation modeled data (three layer model). Corre-
lation with true image = 0.8309. . . . . . . . . . . . . . . . . . . . . .
150
19

5-16 Uniformly regularized LSM image (each βij = 1) with λ = 102¯λ∗and
ζ = ¯ζ∗using wave-equation modeled data (three layer model). Corre-
lation with true image = 0.7381. . . . . . . . . . . . . . . . . . . . . .
151
5-17 Uniformly regularized LSM image (each βij = 1) with λ = 103¯λ∗and
ζ = ¯ζ∗using wave-equation modeled data (three layer model). Corre-
lation with true image = 0.4922. . . . . . . . . . . . . . . . . . . . . .
152
5-18 Variational Bayes MAP image using wave-equation modeled data (three
layer model). Correlation with true image = 0.2291. . . . . . . . . . .
153
5-19 Edge strengths β∗estimated with VB using wave-equation modeled
data (three layer model). . . . . . . . . . . . . . . . . . . . . . . . . .
154
5-20 Empirical Bayes MAP image obtained with β = β∗, λ = 10¯λ∗, and ζ =
¯ζ∗, using wave-equation modeled data (three layer model). Correlation
with true image = 0.7472. . . . . . . . . . . . . . . . . . . . . . . . .
155
5-21 Empirical Bayes MAP image obtained with β = β∗, λ = 102¯λ∗, and
ζ = ¯ζ∗, using wave-equation modeled data (three layer model). Corre-
lation with true image = 0.8724. . . . . . . . . . . . . . . . . . . . . .
156
5-22 Empirical Bayes MAP image obtained with β = β∗, λ = 103¯λ∗, and
ζ = ¯ζ∗, using wave-equation modeled data (three layer model). Corre-
lation with true image = 0.9349. . . . . . . . . . . . . . . . . . . . . .
157
5-23 The variational Bayesian approximations to the posterior distributions
for the parameters scaling the inverse variances of the (a) model λ and
(b) noise ζ, using wave-equation modeled data (three layer model).
.
158
5-24 Unregularized LSM image (each βij = 0) using wave-equation modeled
data (Marmousi model). Correlation with true image = 0.0520.
. . .
162
5-25 Uniformly regularized LSM image (each βij = 1) with λ = ¯λ∗and ζ =
¯ζ∗using wave-equation modeled data (Marmousi model). Correlation
with true image = 0.0757. . . . . . . . . . . . . . . . . . . . . . . . .
162
5-26 Uniformly regularized LSM image (each βij = 1) with λ = 10¯λ∗and
ζ = ¯ζ∗using wave-equation modeled data (Marmousi model). Corre-
lation with true image = 0.0888. . . . . . . . . . . . . . . . . . . . . .
162
20

5-27 Uniformly regularized LSM image (each βij = 1) with λ = 102¯λ∗and
ζ = ¯ζ∗using wave-equation modeled data (Marmousi model). Corre-
lation with true image = 0.0728. . . . . . . . . . . . . . . . . . . . . .
163
5-28 Uniformly regularized LSM image (each βij = 1) with λ = 103¯λ∗and
ζ = ¯ζ∗using wave-equation modeled data (Marmousi model). Corre-
lation with true image = 0.0268. . . . . . . . . . . . . . . . . . . . . .
163
5-29 Variational Bayes MAP image using wave-equation modeled data (Mar-
mousi model). Correlation with true image = 0.0557. . . . . . . . . .
163
5-30 Edge strengths β∗estimated with VB using wave-equation modeled
data (Marmousi model). . . . . . . . . . . . . . . . . . . . . . . . . .
164
5-31 Empirical Bayes MAP image obtained with β = β∗, λ = 10¯λ∗, and ζ =
¯ζ∗, using wave-equation modeled data (Marmousi model). Correlation
with true image = 0.0672. . . . . . . . . . . . . . . . . . . . . . . . .
164
5-32 Empirical Bayes MAP image obtained with β = β∗, λ = 102¯λ∗, and
ζ = ¯ζ∗, using wave-equation modeled data (Marmousi model). Corre-
lation with true image = 0.0828. . . . . . . . . . . . . . . . . . . . . .
164
5-33 Empirical Bayes MAP image obtained with β = β∗, λ = 103¯λ∗, and
ζ = ¯ζ∗, using wave-equation modeled data (Marmousi model). Corre-
lation with true image = 0.0762. . . . . . . . . . . . . . . . . . . . . .
165
5-34 The variational Bayesian approximations to the posterior distributions
for the parameters scaling the inverse variances of the (a) model λ and
(b) noise ζ, using wave-equation modeled data (Marmousi model). . .
165
6-1
(A) True baseline reﬂectivity model and (B) time-lapse change.
. . .
174
6-2
Conventional inversion results for the baseline model and time-lapse
change. (A,B) d0 and d1 are inverted separately to estimate m0 and
m0 + ∆m. (C) The time-lapse change is estimated by subtracting the
inversion results and (D) thresholded to remove the eﬀects of noise. .
175
21

6-3
Inversion results obtained with our hierarchical Bayesian framework.
(A) The marginal MAP solution
b
∆m(t) after 20 E-M iterations which
is then (B) thresholded to remove the eﬀects of noise. (C) The E-M
estimate of the baseline model E[m0| b
∆m(t), d0, d1].
. . . . . . . . . .
176
22

List of Tables
3.1
Isotropic background parameters for the ﬁnite-diﬀerence synthetic data. 74
3.2
Root mean square of residuals between estimates and ground truth
(mean taken over all nodes) when estimating a single fracture set with
fracture compliance 10−9 m/Pa, fracture strike ϕij = 60◦, and varying
fracture spacing, and with smoothness parameter set to βc = 0.1. For
comparison, note that azimuth ϕ is discretized into 20◦bins and log
compliance z has been discretized into bins of size 0.1 log10Pa−1. . . .
77
23

24

Chapter 1
Introduction
A fundamental issue in the ﬁeld of geophysics is the problem of inferring physical
properties of the earth from data collected at or near the earth’s surface. The laws of
physics generally provide a relationship between the data and the earth properties of
interest (which we refer to as the earth model), and the problem of computing these
data given a particular earth model is referred to as the forward problem. By contrast,
the inverse problem is the task of estimating these properties given a particular set
of measurements [69]. Examples of geophysical inverse problems include, amongst
others, electromagnetic inversion [33, 34, 47, 54, 57], inversion of gravity data for
density [38, 40, 52, 75], seismic traveltime tomography [15, 26, 51, 79, 80, 83], and
reﬂection seismic imaging [5, 11, 12, 19, 29, 36, 37, 39, 49].
In these and other geophysical inverse problems, the data are often either insuf-
ﬁcient to fully constrain the earth model or are corrupted with observational noise.
This can be problematic because incorrect models may ﬁt the data as well as (or, in
the case of noisy data, perhaps better than) the true earth model. This problem can
be averted by introducing a priori information about the model into the inversion
procedure. For example, one might expect that the model parameters do not vary
rapidly in space, and hence smoother models would be given preference over those
which exhibit sharp contrasts. In the context of Bayesian inference, which can be
viewed as a probabilistic framework for inversion, this is achieved by considering the
model parameters as random variables with a prior probability distribution which
25

captures one’s belief about the model prior to observing the data.
The choice of the prior distribution is not a trivial one and can signiﬁcantly impact
the posterior estimates of the model obtained from the Bayesian inference framework.
A prior distribution that prefers smooth models, for example, may be inappropriate if
the true earth model does indeed contain sharp discontinuities, and using such a prior
may result in a relatively low posterior probability being assigned to the true model.
Furthermore, we often encounter situations where the model may exhibit spatially-
varying smoothness properties that may not be known a priori. For example, in the
seismic imaging problem, where the objective is to obtain an image of the earth’s
reﬂectivity from seismic data, the model (i.e.
the seismic image) will inherently
have spatially-varying smoothness properties: the image is smooth along a particular
reﬂector but exhibits a sharp contrast in the direction orthogonal to the reﬂector.
However, neither the location nor the orientation of the reﬂectors are known a priori
(if they were, we would not be inferring them).
In this thesis, we are concerned with the question of how to learn the optimal prior
from the data in such a setting, where we focus particularly on the seismic imaging
problem described above and the associated smoothness properties of the seismic
image. While the idea of learning an optimal prior from the data may seem to run
counter to the Bayesian philosophy (since, after all, the prior distribution is intended
to capture one’s state of belief prior to observing the data), the problem can still be
formulated within the Bayesian setting in what is known as the hierarchical Bayesian
framework. As the name suggests, here a hierarchy of random variables is introduced
by treating the parameters deﬁning the original prior distribution (of the earth model)
as random variables themselves, endowed with their own prior distribution; i.e., we
place a prior on the prior. These new random variables can then be inferred from
the data and used to deﬁne a prior for the earth model in the original inference
problem. This particular approach of estimating the prior distribution from the data
is sometimes referred to as the empirical Bayes method [45].
Even as we seek to let the data dictate an optimal prior, there remain problem-
speciﬁc design choices to be made. In particular, we must choose how to parameterize
26

the prior on the model in a meaningful way that captures its spatially-varying smooth-
ness properties. We address this by deﬁning a Markov random ﬁeld (MRF) on the
model vector and parameterizing the edges of its underlying graphical model (which,
as will be seen in Chapter 2, is a 2-D grid graph); we refer to these edge parameters
as edge strengths, which we deﬁne precisely in Chapter 2.
1.1
Thesis Outline and Summary of Contributions
We note that the chapters of this thesis have been written in such a way that they
are arranged in self-contained units, so the interested reader is able to go straight to
the unit of interest. In particular, Chapter 2, Chapter 3, and Chapter 6 are each self-
contained units, and Chapters 4-5 form a self-contained unit. Due to arranging the
thesis in this format, the reader may ﬁnd a small portion of the background sections
in some chapters to be slightly redundant. Below we give a brief outline of the thesis
along with our main contributions.
Chapter 2 :
A Brief Primer on Inverse Problems, Bayesian
Inference, and Graphical Models
In Chapter 2, we give a brief tutorial on inverse problems and Bayesian inference
as well as introduce, by way of example, the form of the smoothness enforcing prior
distribution on the model. We begin the chapter by discussing the deterministic for-
mulation of regularized inversion. We then proceed to the probabilistic formulation of
Bayesian inference and show how regularized inversion can be viewed as a special case
of Bayesian inference. Lastly we review the concept of a probabilistic graphical model
and show how we can obtain a meaningful interpretation of the prior distribution of
the model through this framework. Here we deﬁne the concept of an edge strength
and show numerically how diﬀerent choices for the edge strengths can aﬀect the prior
covariance of the model.
27

Chapter 3: Bayesian Fracture Characterization
Chapter 3 serves as our ﬁrst study into the application of Bayesian inference meth-
ods to a geophysical inverse problem. In this chapter, we develop the application
of a (non-hierarchical) Bayesian framework to the problem of fracture characteri-
zation from seismic data. Here the model consists of the fracture properties (par-
ticularly the fracture orientation and excess compliance) of a 2-D reservoir that is
localized in depth, and the measured data are taken to be features extracted from
the seismic traces, particularly the P-wave amplitude variation with oﬀset and az-
imuth [42, 60, 62, 63] and the fracture transfer function [22], which measures the
change in the scattered seismic energy after the seismic waveﬁeld passes through the
reservoir. When fractures are closely spaced relative to the seismic wavelength, the
fractured medium tends to exhibit anisotropy [66], and hence the P-wave reﬂection
amplitude data are more sensitive to the fracture properties at small fracture spac-
ings. On the other hand, when the fracture spacing is on the order of the seismic
wavelength, the fractures tend to instead act as scatterers [22, 78], thus the scattered
seismic energy is more sensitive to the presence and orientation of fractures in this
regime of fracture spacings. The Bayesian framework allows us to combine these data
to give estimates of the fracture properties over a larger regime of fracture spacings
than would otherwise be attainable while also providing a measure of uncertainty in
the estimates. We derive the likelihood models for these data via physical models
for anisotropy [60, 66] and fracture scattering [22, 78]. The fracture properties are
modeled as discretely-valued random variables with a prior distribution described by
the same 2-D grid MRF introduced in Chapter 2. We solve the inference problem
via loopy belief propagation [48, 55, 56] to obtain the posterior marginal distribu-
tions of the fracture properties, as well as their maximum a posteriori (MAP) and
Bayes least squares (posterior mean) estimates. While we do not attempt to infer the
edge strengths of the graph in this chapter, we do describe and brieﬂy explore how
one may incorporate a priori geological knowledge into the inference procedure by
manipulation of the edge strengths. This exploratory problem motivated our deeper
28

study into how one might invert for these edge strengths in a hierarchical Bayesian
setting, leading us to the work of Chapter 4.
Chapter 4: Least-Squares Migration with a Hierarchical Bayesian
Framework
In Chapter 4, we turn to the problem of seismic imaging (also referred to as migra-
tion), where the model is now the seismic image (i.e. the earth’s reﬂectivity model),
and the data consist of full seismic waveforms measured by a set of seismic receivers
at the surface. While migration is traditionally performed via back-propagation of
the recorded seismic waveﬁeld into the model domain [11], recent eﬀorts [19, 36, 49]
attempt to give the image as the solution to a least-squares inverse problem. This ap-
proach to the seismic imaging problem is referred to as least-squares migration (LSM).
To remain analogous to LSM, we model the image with a Gaussian prior distribution
and give the data as a linear function of the image corrupted by additive Gaussian
noise. The dependence of the data on the image is given by the Kirchhoﬀmodeling
operator, which can be viewed as a ray-theoretic single-scattering approximation to
the integral wave-equation. The Gaussian prior on the image is again described by
the parameterized MRF from Chapter 2, but now we wish to infer the edge strengths
from the data in the hierarchical Bayesian setting. To do so, we endow the edge
strengths with their own prior and obtain the marginal MAP estimate for the edge
strengths via the expectation-maximization (E-M) algorithm [17, 46]. We verify our
procedure on 2-D synthetic datasets. The images we obtain after inferring the edge
strengths exhibit the desired spatially-varying smoothness properties: the images are
generally smooth along the reﬂectors but are allowed to vary sharply at pixels adja-
cent to a reﬂector, so as not to smooth out the discontinuity. In contrast, the images
obtained when the edge strengths are ﬁxed are either too smooth or overly noisy.
29

Chapter 5:
Interpretation and Estimation of Regularization
Parameters
While we inferred the edge strengths in the previous chapter, the remaining parame-
ters deﬁning the prior were decided in a somewhat ad hoc fashion. In Chapter 5, we
present two approaches to picking these remaining parameters more rigorously. In
the ﬁrst part of this chapter, we elucidate further on the connections between these
parameters and the prior model covariance that results from a particular choice. To
do this analytically, we follow the methodology of Rodi and Myers [59] and Simpson
et al. [68], passing from a random vector representation of the model deﬁned on a
discrete spatial grid to the limit of a continuous random ﬁeld representation. In the
limiting case, we can obtain the covariance of the model as the Green’s function of a
diﬀerential operator, where the properties of this covariance function depend on the
parameters deﬁning the prior distribution.
In the second part of this chapter, we seek to extend the inference framework of
the previous chapter to include these parameters. Here we perform the inference via
the variational Bayesian method [3], which can be viewed as a generalization of the
E-M algorithm [3, 4], and validate our methodology on a synthetic dataset.
Chapter 6: Hierarchical Bayesian Time-Lapse Seismic Process-
ing
In Chapter 6, we apply the hierarchical Bayesian framework and algorithmic machin-
ery explored in the previous chapters to the problem of time-lapse seismic inversion.
Here the goal is to detect changes in the subsurface over a period of time by taking
repeated seismic surveys (a ﬁrst “baseline” survey and a second “monitor” survey).
Conventional methods for time-lapse inversion typically involve subtracting repeated
datasets and inverting the diﬀerenced data to obtain the time-lapse change [8, 30, 81];
these methods, however, require identical acquisition geometries between subsequent
seismic surveys, which is often diﬃcult to achieve. In the realistic case of diﬀering
acquisition geometries, one common approach is to invert the datasets separately
30

and estimate the time-lapse change model as the diﬀerence in the inversion models.
However, this method often performs poorly due to diﬀerences in illumination and
observational noise between the datasets. To correctly treat the case of diﬀering ac-
quisition geometries between the baseline and monitor surveys, we cast the problem
in the hierarchical Bayesian setting and seek the marginal MAP estimate for the
time-lapse change in the model parameters. We again solve the Bayesian inference
problem using the E-M algorithm, which, in this case, iterates between performing
subsequent updates to the background model and the time-lapse change. We ver-
ify the inference results on a time-lapse seismic imaging example involving synthetic
datasets with diﬀerent acquisition geometries.
Chapter 7: Conclusions
In Chapter 7, we summarize the major contributions of our work, give concluding
remarks, and suggest some avenues for future research.
31

32

Chapter 2
A Brief Primer on Inverse Problems,
Bayesian Inference, and Graphical
Models
2.1
An Introduction to Inverse Problems
In an inverse problem, the objective is to infer from a set of observed data d some
unknown attributes of interest which we refer to as the model parameters m. For
the inverse problems of interest in this thesis, m ∈RN is an N-dimensional model
vector of geophysical properties (such as acoustic reﬂectivity) deﬁned over a 2-D (or
potentially 3-D) spatial grid of the subsurface, and d ∈RK is a K-dimensional data
vector consisting of either a set of seismic traces or features extracted from the seismic
dataset (such as P-wave reﬂection amplitudes, for example).
Typically, a forward modeling operator F relates the model parameters to the
observed data, i.e.
d = F(m) + n,
(2.1)
where n is a noise term introduced to capture measurement and modeling errors.
Solving the inverse problem then involves ﬁnding the model parameters that minimize
some data misﬁt function Ω(m, d), usually taken to be some norm of the residual
33

r = d −Am. For example, one might search for the model that gives the best ﬁt to
the data in the least-squares sense, wherein the data misﬁt function is given by the
squared ℓ2-norm of the residual
Ω(m, d) = 1
2∥d −F(m)∥2
2.
(2.2)
The model mLS that minimizes (2.2) is consequently known as the least-squares so-
lution
mLS = arg min
m
1
2∥d −F(m)∥2
2.
(2.3)
Assuming F is continuously diﬀerentiable, we can derive the ﬁrst-order necessary
condition on mLS by setting the gradient (with respect to m) of Ω(m, d) in (2.2) to
0:
∇mΩ(m, d) = −A(m)T (d −F(m)) = 0,
(2.4)
where A(m) is the Jacobian of F. When F(m) is a linear function of the model (so
that F(m) = Am and the Jacobian A does not vary with m), then (2.4) implies that
mLS must be a solution to the linear system
ATAmLS = ATd.
(2.5)
If K ≥N and the columns of A are linearly independent (i.e. A is full column rank),
then ATA is positive deﬁnite and (2.5) has a unique solution:
mLS = (ATA)−1ATd.
(2.6)
Unfortunately, even when the solution is unique, many inverse problems exhibit
ill-posedness, where even a small amount of noise in the data can lead to large errors
in the estimated model parameters [77]. In order to solve ill-posed problems and
prevent overﬁtting of noisy data, additional information about the model parameters
can be introduced in a process known as regularization [50, 77]. This is typically
achieved by adding to the data misﬁt function a regularization function Φ(m) that
34

depends solely on the model parameters m and penalizes models that are inconsistent
this information. The regularized solution mreg then minimizes the combined penalty
terms:
mreg = arg min
m
Φ(m) + Ω(m, d).
(2.7)
For example, if the model is known to be spatially smooth a priori, an appropriate
regularization function may penalize local diﬀerences in the model parameters such
as
Φ(m) = 1
2λ
X
(i,j)∈E
βij(mi −mj)2
(2.8)
= 1
2λmT D(β)m,
(2.9)
where the diﬀerencing weights βij ∈[0, 1] capture the degree of smoothness we expect
between mi and mj, E is the set of pairs of indices that correspond to spatial neighbors,
D(β) is a diﬀerencing matrix capturing this operation deﬁned by the vector β = {βij :
(i, j) ∈E}, and λ > 0 is a trade-oﬀparameter that assigns the maximum weight given
to penalizing these diﬀerences. Alternatively, if the model is believed to be near some
reference model m0, the regularization function might quantify the distance between
the model parameters and m0, e.g.
Φ(m) = 1
2λ∥m −m0∥2
2.
(2.10)
Both (2.9) and (2.10) are examples of Tikhonov regularization [70, 77], where the
regularization function is taken to be a quadratic positive semi-deﬁnite function of
the model. The Tikhonov regularization function can be expressed in a general form
as
Φ(m) = 1
2(m −m0)TQ(m −m0),
(2.11)
where the regularization matrix Q is symmetric and positive semi-deﬁnite.
When the data misﬁt function Ω(m, d) is taken to be the squared ℓ2-norm of the
residual (as in (2.2)), the model mRLS that solves the Tikhonov regularized minimiza-
35

tion problem of (2.7) is referred to as the regularized least-squares solution
mRLS = arg min
m
1
2(m −m0)TQ(m −m0) + 1
2∥d −F(m)∥2
2.
(2.12)
(2.12) can be made even more general by instead considering a matrix-weighted
quadratic function of the residual rTPr (for some symmetric positive deﬁnite matrix
P) for the data misﬁt term, so that solving (2.7) will yield the regularized weighted
least-squares model mRWLS:
mRWLS = arg min
m
1
2(m −m0)TQ(m −m0) + 1
2 (d −F(m))T P (d −F(m)) . (2.13)
As will be seen shortly, the matrices Q and P (when both are positive deﬁnite) can
be interpreted as inverse covariance matrices of the model m (prior to observing the
data) and the noise n, respectively. As before, when F is a linear function of the
model (described by the matrix A), applying the stationarity condition on mRWLS
yields an analytical solution to (2.13):
mRWLS =
 ATPA + Q
−1  ATPd + Qm0

,
(2.14)
where the above requires that ATPA + Q be positive deﬁnite.
2.2
Bayesian Inference
Having described the fundamental components of an inverse problem, we turn to the
setting of Bayesian inference which provides a useful and mathematically rigorous
framework for inferring the model parameters from the data and handling the relevant
uncertainties in both the model and data. In what follows, we give a brief overview
of the concepts of Bayesian inference and probabilistic graphical models. A more
thorough treatment of these concepts can be found in Gelman et al. [24] and Koller
and Friedman [35].
For the sake of clarity, we pause here to make a few comments about the notation
36

and terminology we will use.
In what follows and throughout this thesis, we use
sans-serif font for random variables (e.g. mi), boldface font for vectors (e.g. m), and
combine both fonts for random vectors (e.g. m). When we refer to the probability
distribution (or simply distribution) of a random vector m, we are referring to its
probability mass function (in the case of a discretely-valued random vector) or its
probability density function (in the case of a continuously-valued random vector).
We denote the distribution of m by pm(m), or, when the meaning is clear from the
context, we drop the subscript and simply write p(m). We use analogous notation
for conditional distributions: e.g. for the conditional distribution of m given d, we
will write pm|d(m|d) or simply p(m|d).
In the context of Bayesian inference, both the model and data are treated as
random vectors, denoted by m and d. Our belief about the model parameters m prior
to observing the data is encoded via p(m), the prior distribution on m. Similarly, our
belief about what the data d will be, given a particular model m = m, is captured
by the likelihood model p(d|m), which can be thought of as a stochastic forward
model for the data. These two probability distributions fully specify the probabilistic
model for m and d and are used to compute the posterior distribution on the model
parameters p(m|d) via Bayes’ rule:
p(m|d) =
p(m)p(d|m)
R
RN p(m′)p(d|m′) dm′ .
(2.15)
If m is discretely-valued, the integral in the denominator of (2.15) would be replaced
by a summation. The posterior distribution is the complete solution to the problem
of inferring m from d, and updates our belief about the model upon having observed
the data. When m is high-dimensional, it may not be possible to tractably explore
the entire posterior distribution and often one will instead seek a point estimator of
the model m. A Bayesian point estimator ˆm(·) is a function of the data d that is
obtained by minimizing an expected Bayes cost function B( ˆm(d), m) which encodes
37

the cost of estimating m as ˆm(d). Hence we have:
ˆm(·) = arg min
f(·)
Ep(m,d) [B(f(d), m)]
(2.16)
= arg min
f(·)
Ep(d)

Ep(m|d) [ B(f(d), m) | d ]

,
(2.17)
where Ep denotes the expectation operator with respect to distribution p and (2.17)
is due to the law of iterated expectations. As seen by (2.17), the total expected Bayes
cost of (2.16) is a non-negative combination of the conditional expected Bayes costs
given the diﬀerent realizations of the data. Hence the expected Bayes cost (2.16) can
be minimized by designing ˆm(·) to minimize the conditional expected Bayes costs for
each realization of the data d = d, so that
ˆm(d) = arg min
m′
Ep(m|d) [ B(m′, m) | d = d ] .
(2.18)
The particular Bayesian point estimator we obtain from (2.18) is determined by the
choice of the Bayes cost function B(·, ·). If B quantiﬁes the squared ℓ2-norm of the
estimation error, so that
B( ˆm, m) = ∥ˆm −m∥2
2,
(2.19)
solving (2.18) yields the Bayes least-squares (BLS) estimator mBLS, which turns out
to be the posterior mean of m:
mBLS(d) = E[m|d].
(2.20)
The well-known maximum a posteriori (MAP) estimator mMAP, which maximizes
the posterior distribution p(m|d), is obtained by uniformly penalizing all non-zero
estimation errors; to be precise, if, for some ǫ > 0, B is taken to be
B( ˆm, m) =





1
∥ˆm −m∥> ǫ
0
∥ˆm −m∥≤ǫ
(2.21)
38

then taking the limit as ǫ →0 in (2.21) yields the MAP estimator as the solution to
(2.18), which is given by:
mMAP(d) = arg max
m
p(m|d).
(2.22)
The connection between the Bayesian inference setting and the deterministic in-
version framework described in Section 2.1 is perhaps best seen through the MAP
estimator. The MAP estimator can be equivalently expressed as the maximizer of
the log posterior distribution, since log is a monotonic function; hence we can rewrite
(2.22) as
mMAP(d) = arg max
m
log p(m|d)
(2.23)
= arg max
m
log p(m) + log p(d|m),
(2.24)
where we have employed Bayes’ rule (2.15) and dropped the denominator which does
not depend on m. Noting that maximizing a function is the same as minimizing the
negative of that function, (2.24) becomes
mMAP(d) = arg min
m
{−log p(m) −log p(d|m)} .
(2.25)
Comparing the form of the MAP estimate in (2.25) to that of the regularized solution
from the deterministic inversion framework in (2.7), we see that the two are math-
ematically equivalent.
In particular, we can interpret the regularization and data
misﬁt functions of (2.7) as negative log prior and likelihood models, respectively:
Φ(m) = −log p(m) + const.,
(2.26)
Ω(m, d) = −log p(d|m) + const.
(2.27)
The regularized weighted least-squares solution of (2.13) can be interpreted as a
Bayesian MAP estimate when the prior model and noise are both Gaussian.
In
39

particular, we endow m with a Gaussian prior distribution, having prior mean m0
and covariance matrix C (i.e. m ∼N (m0, C)) so that
p(m) =
exp
n
−1
2 (m −m0)T C−1 (m −m0)
o
(2π)N/2 |C|1/2
,
(2.28)
where | · | denotes the determinant, and let the data be described by
d = F(m) + n,
(2.29)
where n ∼N (0, Σ) is a zero-mean Gaussian noise vector independent from the model
m, so that
p(d|m) =
exp
n
−1
2 (d −F(m))T Σ−1 (d −F(m))
o
(2π)K/2 |Σ|1/2
.
(2.30)
The posterior distribution is then given by
p(m|d) =
exp
n
−1
2 (m −m0)T C−1 (m −m0) −1
2 (d −F(m))T Σ−1 (d −F(m))
o
(2π)(N+K)/2 |C|1/2 |Σ|1/2 p(d)
(2.31)
∝exp
n
−1
2 (m −m0)T C−1 (m −m0) −1
2 (d −F(m))T Σ−1 (d −F(m))
o
,
(2.32)
so that the Bayesian MAP estimate of the model is
mMAP = arg max
m
log p(m|d)
(2.33)
= arg min
m
1
2 (m −m0)T C−1 (m −m0) + 1
2 (d −F(m))T Σ−1 (d −F(m)) .
(2.34)
Comparing (2.34) with the regularized weighted least-squares cost function of (2.13),
we see that (for positive deﬁnite Q and P in (2.13)) the two are equivalent and
mRWLS is the Bayesian MAP estimate when the prior model and noise are Gaussian
as described above. Furthermore, we can interpret the regularization matrix Q as the
40

inverse prior covariance matrix (also called the prior precision matrix) C−1 and the
weighting matrix P (from the quadratic data misﬁt function) as the inverse covariance
matrix of the noise [70].
2.3
Probabilistic Graphical Models, Markov Ran-
dom Fields, and Covariance
We return to the examples of Tikhonov regularization given in (2.9) and (2.10). In
particular, we consider the case when the regularization function Φ(m) both enforces
smoothness in the model parameters and penalizes the magnitude of the model:
Φ(m) = 1
2λ

X
(i,j)∈E
βij(mi −mj)2 + ǫ
X
i∈V
m2
i


(2.35)
= 1
2mT (λ (D(β) + ǫI)) m,
(2.36)
where λ > 0, ǫ > 0, β = {βij : (i, j) ∈E} with each βij ∈[0, 1], the set V = {1, . . . , N}
indexes the components of m, and E and the diﬀerencing matrix D(β) take the same
meaning as in (2.9). In the Bayesian inference context, this is equivalent to modeling
m a priori as Gaussian
m ∼N (0, C)
(2.37)
where the prior precision matrix Q = C−1 is given by
Q = C−1 = λ (D(β) + ǫI) .
(2.38)
It is clear from (2.35) that the diﬀerencing coeﬃcients βij determine how strongly
to penalize diﬀerences between mi and mj and hence the resulting smoothness of
the regularized solution. In the Bayesian context, β similarly aﬀects the smoothness
of mMAP through the prior on m. Indeed the prior distribution plays a key role in
determining the smoothness properties of the model parameters.
41


m1 
m2 
m3 
m4 
m5 
m6 
m7 
m8 
m9 
β12 
β23 
β14 
β25 
β36 
β47 
β58 
β69 
β78 
β89 
β45 
β56 
Figure 2-1: The Markov random ﬁeld imposed on m by ﬁxing β prior to observing
the data d, for a simple nine pixel image.
2.3.1
Probabilistic Graphical Models
The expressive formalism of probabilistic graphical models provides a useful analyti-
cal framework for both understanding the interdependencies imposed by a particular
distribution as well as implementing eﬃcient algorithms to solve the underlying in-
ference problem (some of which are explored in Chapter 3). We deﬁne an undirected
graphical model G = (V, E), with a set of vertices (or nodes) V, which index the
random variables mi comprising the random vector m, and a set of edges E ⊂V × V,
represented as pairs of vertices in V that encode dependencies between the random
variables. We say that the random vector m forms a Markov random ﬁeld (MRF) over
G if it is Markov on G, meaning that, for any disjoint subsets of nodes S, T, U ⊂V
such that S separates T from U on G (i.e. if we remove the nodes in S from G then
there is no remaining path from any node in T to any node in U), then conditioned
on mS, mT is conditionally independent of mU (which we write as mT ⊥⊥mU | mS).
Here we have deﬁned mS ≜{mi : i ∈S} as the set of random variables corresponding
to the nodes in S (similarly for mT and mU). An example of an undirected graphical
model is depicted in Figure 2-1.
To make the connection between a particular distribution and a graphical model,
we turn to the Hammersley-Cliﬀord theorem [13, 28, 35]. The Hammersley-Cliﬀord
42

theorem states that a distribution is Markov on an undirected graphical model G if
it factorizes over the maximal cliques of that graph, and, for distributions that are
strictly positive, the converse statement also holds. Here a clique C of a graph G is
any subset of its vertices (C ⊂V) which are fully-connected, meaning every vertex
in C shares an edge from E with every other vertex in C. The maximal cliques of
a graph G are its largest possible cliques: C is a maximal clique of G if it fails to
remain a clique when even one more vertex from V \ C is added to C. We say that
a distribution pm(m) factorizes over the maximal cliques of G if it can be written
as a product of functions of the random variables in each maximal clique C of G:
pm(m) = Q
C fC(mC).
For a Gaussian random vector m with precision matrix (or inverse covariance
matrix) Q, the Hammersley-Cliﬀord theorem implies that m is Markov on the graph
G = (V, E) if and only if Qij = 0 whenever there is no edge between nodes i and j (i.e.
whenever Q is at least as sparse as the edge set E) [35]. Hence, a Gaussian random
vector with a given precision matrix Q induces a natural graph based on the sparsity
pattern of Q.
The graph shown in Figure 2-1 is the natural graph induced by the Gaussian
prior described in Equations (2.37)-(2.38), and the sets V and E used in (2.35) are
precisely the vertex and edge sets of this graph. The edges of the graph in Figure
2-1 are labeled by the diﬀerencing weights βij because, in a sense, the βij determine
the “strength” of each edge. To be precise, β captures the conditional dependence
structure of m, such that βij = 0 implies that there is no edge between mi and mj
and hence mi ⊥⊥mj | {mk : k ̸= i, j}, and a larger value for βij induces a stronger
conditional correlation between mi and mj. For this reason, we sometimes refer to
the elements of β as the edge strengths of G and to D(β) as the weighted graph
Laplacian of G (weighted by β).
(We note to the reader that, although we have
deﬁned the notion of edge strengths in this chapter, we will re-introduce and review
this concept in Chapter 4.)
43

2.3.2
Edge Strengths and Covariance
This graph theoretic approach to deﬁning a distribution via a Markov random ﬁeld
contrasts with the more typical method for deﬁning priors in geophysical inverse
problems using stationary covariance functions (typically over an assumed Gaussian
random ﬁeld) [70]. A covariance function is often characterized by its variance and
correlation length, the latter being a characteristic length deﬁning the rate of decay of
the covariance function. While deﬁning a covariance function explicitly encodes the
covariances between any two points in space, an MRF instead encodes the conditional
independencies and (when parameterized with edge strengths) the local smoothness
structure of the model, thereby implicitly deﬁning a covariance function.
To show the eﬀect of β on the covariance of m in the Gaussian example described
by (2.37)-(2.38), we compute one row of the covariance matrix C(β) and draw samples
of m for diﬀerent choices of β. We note that since Cij = cov(mi, mj), the ith row of the
covariance matrix gives the covariance of the entire model with mi. The covariances
and samples are computed with m deﬁned over a 101-by-101 node grid (with grid
spacing set to 1 m) and with setting λ = 1 and ǫ = 10−3 and where the covariances
with the central point in the grid are computed. We consider two cases: the ﬁrst
where each βij is set to a common value β, to illustrate how varying the βij uniformly
aﬀects the model covariance, and the second where some of the βij are set to 0 and
the remainder are set to 1, to illustrate how the βij can capture spatially-varying
smoothness in the model.
Figure 2-2 displays the ﬁrst case where all the βij are set to a common value, where
this value is varied from 1 to 0. As shown in the covariance plots, as β decreases, the
resulting covariance function becomes taller and skinnier: decreasing β results in an
increased model variance and decreased correlation length. β = 0 corresponds to the
case of a completely disconnected graph, so that all the mi are independent, and hence
the covariance function is 0 everywhere except at the central point, which corresponds
to the variance of mi. The draws from the Gaussian distributions similarly show this.
When β = 1, the sample is highly correlated in space and has a relatively smaller
44

magnitude (due to the lower variance), however as β is made smaller, we observe that
the sample becomes spatially less correlated and the magnitude of its range increases.
At β = 0, the sample is of white noise and hence completely uncorrelated in space.
The second case, where we set some of the βij to 0 and the rest to 1 is shown
in Figure 2-3.
In particular we consider the choice of β we may want to use if
we suspect a horizontal discontinuity in the model at z = 40.5 m and centered at
x = 51 m; in this case, we would set βij = 0 for the edge strengths corresponding
to vertical edges connecting nodes at z = 40 m and z = 41 m along the length of
the discontinuity. We show the covariances and sample draws for discontinuities of
diﬀerent lengths: 21 m, 51 m, and an inﬁnite length discontinuity. As evidenced by
the plots, the covariance is signiﬁcantly reduced across the discontinuity where the
βij are set to 0; however, for the ﬁnite-length discontinuities, the covariance remains
non-zero across the discontinuity, as there is still a path on the graph G connecting
the nodes to the central node, through which they remain correlated to the central
node. For the inﬁnite-length discontinuity, no such path exists as the nodes above
the discontinuity are completely disconnected from those below the discontinuity;
hence the two sets of nodes are independent, and the covariances of the nodes above
the discontinuity with the central node are identically 0. This is similarly observed
in the sample draws: the draw corresponding to the inﬁnite-length discontinuity
shows that the nodes above and below the discontinuity are uncorrelated. For the
draws corresponding to the ﬁnite-length discontinuities, one observes that the sample
values are allowed to contrast signiﬁcantly across the discontinuity, however a reduced
correlation still exists through paths of nodes that avoid the discontinuity.
The preceding examples have served to illustrate the signiﬁcance of the edge
strengths β in deﬁning the prior distribution for m and to point out the ﬂexibility of
this construction for treating inverse problems with models that have spatially-varying
smoothness properties. In Chapter 5, we undertake a more rigorous investigation of
the parameters β, λ, and ǫ and their relationship with the model covariance. We note
here that the graphical model depicted in Figure 2-1 with edge set E connecting only
a node’s four nearest neighbors is one of many possible graphs that can be used in our
45

(A) β = 1
x [m]
z [m]
 
 
20 40 60 80100
20
40
60
80
100
0.2
0.4
0.6
0.8
(B) β = 1
x [m]
z [m]
 
 
20 40 60 80100
20
40
60
80
100
−4
−2
0
2
4
(C) β = 0.5
x [m]
z [m]
 
 
20 40 60 80100
20
40
60
80
100
0.5
1
1.5
(D) β = 0.5
x [m]
z [m]
 
 
20 40 60 80100
20
40
60
80
100
−5
0
5
(E) β = 0.1
x [m]
z [m]
 
 
20 40 60 80100
20
40
60
80
100
2
4
6
(F) β = 0.1
x [m]
z [m]
 
 
20 40 60 80100
20
40
60
80
100
−10
0
10
(G) β = 0.01
x [m]
z [m]
 
 
20 40 60 80100
20
40
60
80
100
10
20
30
40
(H) β = 0.01
x [m]
z [m]
 
 
20 40 60 80100
20
40
60
80
100
−20
0
20
(I) β = 0
x [m]
z [m]
 
 
20 40 60 80100
20
40
60
80
100
0
500
1000
(J) β = 0
x [m]
z [m]
 
 
20 40 60 80100
20
40
60
80
100
−100
0
100
Figure 2-2: Computed covariance functions (ﬁrst column) and sample draws (second
column) from N (0, (λ(D(β) + ǫI))−1) with each βij = β for (A-B) β = 1, (C-D)
β = 0.5, (E-F) β = 0.1, (G-H) β = 0.01, (I-J) β = 0.
46

x [m]
z [m]
(A) 21 m discont.
 
 
20 40 60 80100
20
40
60
80
100
0.2
0.4
0.6
0.8
x [m]
z [m]
(B) 21 m discont.
 
 
20 40 60 80100
20
40
60
80
100
−4
−2
0
2
4
x [m]
z [m]
(C) 51 m discont.
 
 
20 40 60 80100
20
40
60
80
100
0.2
0.4
0.6
0.8
x [m]
z [m]
(D) 51 m discont.
 
 
20 40 60 80100
20
40
60
80
100
−4
−2
0
2
4
x [m]
z [m]
(E) inf. length discont.
 
 
20 40 60 80100
20
40
60
80
100
0.2
0.4
0.6
0.8
x [m]
z [m]
(F) inf. length discont.
 
 
20 40 60 80100
20
40
60
80
100
−4
−2
0
2
4
Figure 2-3: Computed covariance functions (ﬁrst column) and sample draws (second
column) from N (0, (λ(D(β)+ǫI))−1) with βij = 0 along a horizontal discontinuity at
z = 40.5 m (centered at x = 51 m) of length (A-B) 21 m, (C-D) 51 m, (E-F) inﬁnite
length. βij = 1 elsewhere.
47

construction. In particular, we can generalize the graph in Figure 2-1 by appending
E so that a node is connected to all other nodes within some speciﬁed radius. Picking
a radius of 1 grid node will give the graph of Figure 2-1; a radius greater than
√
2
but less than 2 will additionally induce diagonal connections, and so on.
In the remainder of this thesis, we explore diﬀerent inference problems that use this
construction with β. Chapter 3 is an exploratory study in Bayesian inference on the
geophysical inverse problem of fracture characterization; there the model parameters
are treated as discrete random variables, and the prior distribution is deﬁned using the
same graphical model as in Figure 2-1, with β ﬁxed. Our investigation of this inference
problem made apparent the signiﬁcance of the choice of the edge strengths β to the
solution of the problem. Hence, in Chapter 4, we turn to the problem of estimating the
edge strengths β from the data in the context of the seismic imaging problem known
as least-squares migration. Therein the model is treated as a Gaussian random vector
(following the description given in Equations (2.37)-(2.38)). In Chapter 5, we extend
the work of Chapter 4 by investigating the connections between the parameters β,
λ, and ǫ and the model covariance and further generalize the methodology developed
in Chapter 4 to additionally estimate λ and the noise variance. Chapter 6 focuses
on the problem of time-lapse seismic processing and utilizes some of the machinery
discussed in Chapter 4 to correctly frame and solve the inference problem; however,
in that chapter, while the construction with β is utilized, the focus is on correctly
dealing with uncertainty about how the model evolves with time rather than on the
edge strengths β.
48

Chapter 3
Bayesian Fracture Characterization
3.1
Summary
In this chapter, we describe a methodology for quantitatively characterizing the frac-
tured nature of a hydrocarbon or geothermal reservoir from surface seismic data under
a Bayesian inference framework. The method combines diﬀerent kinds of measure-
ments of fracture properties to ﬁnd a best ﬁt model while providing estimates of
the uncertainty of model parameters. Fractures provide pathways for ﬂuid ﬂow in
a reservoir, and hence, knowledge about a reservoir’s fractured nature can be used
to enhance production from the reservoir. The fracture properties of interest in this
study (to be inferred) are fracture orientation and excess compliance, where each
of these properties are assumed to vary spatially over a 2-D horizontal grid which
is assumed to represent the top of a reservoir. The Bayesian framework in which
the inference problem is cast has the key beneﬁts of (1) utilization of a prior model
that allows geological information to be incorporated, (2) providing a straightfor-
ward means of incorporating all measurements (across the 2-D spatial grid) into the
estimates at each grid point, (3) allowing diﬀerent types of measurements to be com-
bined under a single inference procedure, and (4) providing a measure of uncertainty
in the estimates. The observed data are taken from a 2-D array of surface seismic
receivers responding to an array of surface sources. Well understood features from
the seismic traces are extracted and treated as the observed data, namely the P-wave
49

reﬂection amplitude variation with acquisition azimuth and oﬀset (amplitude versus
azimuth data) and fracture transfer function data. Amplitude versus azimuth data
are known to be more sensitive to fracture properties when the fracture spacing is
signiﬁcantly smaller than the seismic wavelength, whereas fracture transfer function
data are more sensitive to fracture properties when the fracture spacing is on the order
of the seismic wavelength. Combining these two measurements has the beneﬁt of al-
lowing inferences to be made about fracture properties over a larger range of fracture
spacing than otherwise attainable. Geophysical forward models for the measurements
are used to arrive at likelihood models for the data, and the prior distribution for the
fracture variables is obtained by deﬁning a Markov random ﬁeld over the horizontal
2-D grid on which we wish to obtain fracture properties. The fracture variables are
then inferred by application of loopy belief propagation to yield approximations for
the posterior marginal distributions of the fracture properties, as well as the maximum
a posteriori and Bayes least squares (posterior mean) estimates of these properties.
Veriﬁcation of the inference procedure is performed using a synthetic dataset, where
the estimates are shown to be at or near ground truth for both fracture orientation (at
the full range of fracture spacings) and fracture excess compliance (at small fracture
spacings).
3.2
Introduction
Fractures are cracks in the earth’s crust through which ﬂuid, such as oil, natural gas,
or brine, can ﬂow. Knowledge about the presence and properties of fractures in a
reservoir can be extremely valuable, as such information can be used to determine
pathways for ﬂuid ﬂow and to optimize production from the reservoir [1, 62]. Since
the presence of fractures in an elastic medium can alter the compliance of the medium
and fractures often have a preferred alignment relative to in situ stress, fractures can
cause the medium to exhibit anisotropy [66]. This anisotropy has been exploited to
give diﬀerent techniques for determining fracture properties from seismic data, such as
reﬂection amplitude versus oﬀset and azimuth analysis [42, 60, 62, 63] and shear wave
50

birefringence [23]. These methods, however, are only valid when the fracture spacing
is small in comparison to the seismic wavelength, so that seismic waves average over
the fractures [22, 78]. The equivalent anisotropic medium assumption breaks down
when the spacing between the fractures increases to being on the order of the seismic
wavelength. For the case of larger fracture spacings, Willis et al. [78] proposed a
technique, referred to as the scattering index method, to estimate the azimuthal
orientation (or strike) of a fracture system based on the scattered seismic energy.
Fang et al. [22] described a series of modiﬁcations to this technique to give a more
robust methodology for determining fracture orientation, which is referred to as the
fracture transfer function (FTF) method.
In the way of statistical inference methods applied to geophysical problems, Ei-
dsvik et al. [20] gave a Bayesian framework for determining rock facies and saturating
ﬂuid by integrating a forward rock physics model with spatial statistics of rock prop-
erties. Speciﬁcally in the area of fracture characterization, Ali and Jakobsen [1] used
a Bayesian inference framework to infer fracture orientation and density from seismic
velocity and attenuation anisotropy data.
Sil and Srinivasan [67] applied a simi-
lar Bayesian inference methodology to determine fracture strikes from seismic and
well data. All of the aforementioned statistical studies solved the inference problem
via Markov chain Monte Carlo (MCMC), a sampling technique which stochastically
searches the model space. Furthermore, the data models used in all of these stud-
ies follow from the assumption that a medium with closely spaced fractures is an
equivalent anisotropic medium.
The aim of our study is to utilize a Bayesian framework to combine diﬀerent data
which, on their own, are informative about certain regimes of fracture spacing, to
be able to estimate diﬀerent fracture properties (particularly, excess fracture compli-
ance and fracture orientation) at a wider range of fracture spacings than otherwise
attainable if one data type is used. The Bayesian framework in which the problem
is cast furthermore makes it straightforward to encode prior knowledge about either
geological features of the reservoir, such as the existence of a discontinuity arising
from a geological fault, or known information about the fracture properties or their
51

spatial correlation, where we assume that the fracture properties can vary spatially
over a 2-D horizontal grid.
We ﬁrst describe the physical parameters that we use to characterize a fracture
system in a reservoir whose properties vary with space. We then present our approach
for casting the fracture characterization problem within a Bayesian framework in
which we assume that fracture properties within the reservoir are spatially correlated.
We capture this spatial correlation by deﬁning a Markov random ﬁeld (MRF) over
the grid of fracture properties, which we use to arrive at the prior distribution for
the fracture properties. We then introduce the types of seismic data that we assume
are available to characterize fractures, particularly amplitude versus azimuth [60, 63]
and fracture transfer function [22], and describe the methods by which these data are
used to place constraints on the properties. The inversion for model parameters is
accomplished via loopy belief propagation (LBP) [48, 55, 56], a numerically scalable
approximate inference procedure that yields both the posterior marginal distributions
of the properties at each point in space in addition to the Bayes least squares (BLS)
and maximum a posteriori (MAP) estimates of the fracture properties. Finally, we
demonstrate the applicability of the method for inferring fracture properties using
synthetic seismic data.
3.3
Description of the Problem
Consider a set of seismic measurements taken from a 2-D array of surface receivers
over a layered medium responding to a set of surface seismic sources. We are interested
in inferring from the seismic data whether or not fractures are present in a particular
layer of the medium (e.g. the reservoir) and, if so, the properties of the fractures. A
simple example of this setup, where the medium consists of ﬂat homogeneous layers,
is displayed in Figure 3-1.
In particular, we would like to infer fracture orientation
ϕ = [ϕij] and the (base 10) log excess fracture compliance z = [zij] spatially over a
2-D m-by-n horizontal grid L = {1, . . . , m}×{1, . . . , n} (where i and j index the axes
of the grid L). Each grid point corresponds to a square of area ℓ2, so that the entire
52

Figure 3-1: A simple model of the problem setting. The formation consists of ﬁve ﬂat
homogeneous layers with fractures that may be present in the third layer and mea-
surements obtained from the 2-D array of surface seismic receivers. Figure modiﬁed
from Willis et al. [78].
grid corresponds to a region of area mnℓ2. Excess fracture compliance is deﬁned as
the overall additional medium compliance (having units of Pa−1) due to the presence
of fractures and is the ratio of the compliance of the individual fractures (in m/Pa)
to the fracture spacing (in m) [18, 66]. We make the simplifying assumptions (1) that
the fractures are vertical, so that the fracture orientation ϕij is simply the azimuth
(or strike) of the fractures (with respect to North), and (2) that the normal and
tangential excess compliances are equal, which may represent gas-ﬁlled fractures [64],
hence we need only infer a single log excess compliance value zij for each grid node.
It is possible that the ratio of normal to tangential fracture compliances may deviate
from unity due to mineralization [64]; while this ratio has been measured using shear
wave splitting data [74], we are not able to uniquely resolve this ratio from AvAz
data. If this ratio is known to diﬀer from unity for a particular fracture system, we
may proceed with our analysis inferring for only the normal compliance and setting
the tangential compliance according to this ratio. An excess compliance value of 0
at a particular grid point is taken to mean there are no fractures at that grid point
(rendering the value for azimuth arbitrary and meaningless). In order to compare
zero and non-zero compliance on a logarithmic scale, we treat an excess compliance
of zero as 10−13 Pa−1, which is geophysically reasonable as this is an insigniﬁcantly
53

small value for excess compliance and results in a negligible eﬀect on seismic wave
propagation. We assume that the dataset is rich enough so that for each grid point
in L, there are corresponding source-receiver pairs that sample the point at multiple
oﬀsets and acquisition azimuths. We further assume that the background velocity
structure of the medium is well understood.
In order to relate the fracture properties m = (z, ϕ) = [mij] to the seismic
trace dataset, it is necessary to model seismic data as a function of the fracture
properties. Unfortunately, modeling the entire seismic trace dataset requires a full
elastic 3-D forward simulation of the seismic waveﬁeld, and the computational cost
associated with the repeated simulations required to invert for the fracture properties
is prohibitively high, so we instead resort to simulating well understood features of the
seismic trace dataset and treat these features as our observed data d. In particular,
we choose to model P-wave reﬂection amplitude as a function of acquisition azimuth
(at a ﬁxed angle of incidence), also known as amplitude versus azimuth (AvAz) data
[60, 63], and fracture transfer function (FTF) data, as deﬁned by Fang et al. [22].
We refer to these observed data with variables dAvAz and dFTF, respectively, and let
d = (dAvAz, dFTF). Detailed descriptions of the data and their forward models are
detailed in Section 3.4.2. Both dAvAz =

dAvAz
ij

and dFTF =

dFTF
ij

are deﬁned over
the grid L, in a manner such that to each grid node of fracture properties mij there
is an associated data vector dij.
3.4
Bayesian Inference Framework
In order to arrive at an estimate of the fracture properties from the seismic data,
we employ a Bayesian inference framework.
As mentioned earlier, the Bayesian
framework is chosen as it allows us to naturally encode prior information about
the fracture properties (and their spatial variation), combine diﬀerent types of data,
and quantify the uncertainty associated with the inferred quantities.
The frac-
ture properties and seismic data are treated as random variables, and a stochas-
tic model is used to give the joint distribution of the fracture properties and seis-
54

mic data (m, d) = ((z, ϕ), d).
In particular, we model the fracture properties as
discrete random variables where the domain for each of the variables is given by:
10zij ∈Z = {10−9.0, 10−9.1, . . . , 10−12.0, 10−13} (in units of Pa−1) and ϕij ∈F =
{0◦, 20◦, . . . , 160◦}, ∀(i, j) ∈L.
The use of discrete random variables makes the
inference problem amenable to the general framework of message-passing inference
algorithms described in Section 3.4.3, where the intervals of discretization were picked
based on the level of resolution we might reasonably expect to achieve using seismic
measurements. The posterior distribution of the fracture properties given the data
p(m|d) is given by Bayes’ rule:
p(m|d) =
p(m)p(d|m)
P
m′ p(m′)p(d|m′)
∝p(m)p(d|m)
(3.1)
where p(m) and p(d|m) are the prior distribution of the fracture properties and the
distribution of the seismic data given the fracture properties, respectively.
While the posterior distribution of Equation 3.1 is the complete solution to the
Bayesian inference problem, exploring this distribution can be intractable due to the
high-dimensionality of the fracture properties m.
To glean meaningful inferences
from the posterior distribution, one may either choose to obtain point estimators of
m from the posterior or to obtain marginal posterior distributions over some tractably
explorable subsets of the random variables in m. While point estimators are useful
when a single answer to the inference problem is desired, they do not capture the
associated estimation uncertainties (which are described by the marginal posterior
distributions).
Among the most common point estimators are the MAP estimate
ˆmMAP and the BLS (or posterior mean) estimate ˆmBLS. The MAP estimate of the
fracture properties minimizes the probability of estimation error and is the overall
conﬁguration of the fracture properties that maximizes the posterior distribution,
that is
ˆmMAP = arg max
m
p(m|d)
(3.2)
55

In contrast, the BLS estimate of the fracture properties minimizes the expected value
of the squared estimation error and is given by the expected value of the fracture
properties given the data. The posterior marginal distribution for the fracture prop-
erties at a particular node mij is given by summation of the posterior distribution over
all other variables m−ij ≜{mkl : (k, l) ∈L \ {(i, j)}}. So, for example, the posterior
marginal for the log excess compliance p(zij|d) is given by
p(zij|d) =
X
ϕij
X
m−ij
p(zij, ϕij, m−ij|d).
(3.3)
Computing the posterior marginals has the additional beneﬁt of yielding (with min-
imal additional computation) the BLS estimates. For example, for the log excess
compliance at node (i, j), we have
ˆzij,BLS = E [zij|d = d] =
X
zij
zij p(zij|d).
(3.4)
For any reasonably large number of grid nodes mn, the maximization and summa-
tion in Equations 3.2 and 3.3, respectively, are intractable, hence we must turn to
approximate inference algorithms to perform the estimation. We discuss the infer-
ence algorithms used to approximate the MAP estimates and posterior marginals in
Section 3.4.3.
3.4.1
Prior Model
Assuming that the fracture properties will not change rapidly with position, it is
reasonable to make the properties at one point depend on its nearest neighbors in
space. We capture this spatial dependence mathematically by carefully constructing
an appropriate prior model for the fracture properties. We arrive at a prior model by
deﬁning the set of fracture properties m as a Markov random ﬁeld over an undirected
graphical model G = (V, E) on the 2-D grid L [35]. Here V is the set of vertices or
nodes of the graphical model, which correspond to partitions of the random variables
in m. We associate with each grid node (i, j) ∈L a vertex in V corresponding to the
56










Figure 3-2: Undirected graphical model G over which m is Markov, prior to observing
the seismic data. The model is based on the assumption that the values of the fracture
properties at one location are dependent on those of its nearest neighbors.
pair of random variables mij = (zij, ϕij), so that V ≡L. The set of edges of the graph
is E ⊂V × V, represented as pairs of nodes, which encode dependencies between the
random variables.
We deﬁne the edge set E over the 2-D grid L so that a particular node shares edges
with its four neighbors on the grid L. The graphical model for m prior to observing
the data is shown in Figure 3-2.
Intuitively, such a graph structure means that given
the fracture properties of the four nearest neighbors of a particular node, knowledge
of the fracture properties of the medium elsewhere on the grid will have no impact
on our belief about the properties at that node. Since we expect the properties of
the medium at a particular point in space to be similar to its surrounding properties,
this suggests a prior distribution that penalizes diﬀerences between a node and its
neighbors. In particular we deﬁne the prior distribution for the fracture parameters
to be
p(z) ∝exp


−
X
(ij,kl)∈E
βzij,kl(zij −zkl)2



(3.5)
57

and
p(ϕ) ∝exp


−
X
(ij,kl)∈E
βϕij,kl(ϕij −ϕkl)2


,
(3.6)
where βzij,kl and βϕij,kl are smoothness parameters. Note that the fracture orientations
must be manipulated with a modulo operation to bring the diﬀerence within the
interval [−90◦, 90◦), as the azimuth is identical modulo 180◦.
We deﬁne the smoothness parameters in terms of an overall (spatially-varying)
smoothness parameter βij,kl after normalizing by the bin sizes for each variable (so
that the degree of smoothness is not dependent on the units of the variables) via
βzij,kl = βij,kl/(0.1)2 and βϕij,kl = βij,kl/(20)2.
Allowing spatial variation in the
smoothness parameter allows us to encode a priori information about discontinuities
in the medium, such as those which may arise from a geological fault. In general,
we pick a single value βc > 0 for the smoothness parameter along edges where there
are no known discontinuities and set the smoothness parameter to zero along edges
where a fault is known to exist (thereby removing those edges from the graph). We
experiment with diﬀerent choices for βc in Section 3.5.2. Deﬁning Ef to be the set of
edges where a fault is known to exist, then
βij,kl =



βc
if ((i, j), (k, l)) ∈E \ Ef
0
if ((i, j), (k, l)) ∈Ef
(3.7)
Treating the two diﬀerent fracture properties as independent a priori gives the
overall prior distribution for the fracture properties as
p(m) = p(z, ϕ) = p(z)p(ϕ).
(3.8)
Indeed, we see that p(m) factorizes over the maximal cliques of G (which are precisely
the edges E) and thus, by the Hammersley-Cliﬀord theorem (see Ch. 2), m is Markov
on G.
58

3.4.2
Likelihood Model
As discussed in Section 3.3, the seismic data used in this study are AvAz and FTF
data, which are extracted from the seismic trace dataset and denoted by dAvAz and
dFTF, respectively. We make the assumption that given the fracture parameters m,
the two types of seismic data dAvAz and dFTF are conditionally independent, and
hence
p(d|m) = p(dAvAz|m)p(dFTF|m).
(3.9)
In the remainder of this section, we discuss how we model the data to arrive at
the likelihood models p(dAvAz|m) and p(dFTF|m).
Amplitude versus Azimuth Data
We suppose we have data for the amplitudes of P-P arrivals reﬂected from the top of
the fractured layer at a full range of acquisition azimuths and source-receiver oﬀsets
for each grid point (i, j) ∈L. We can use ray-tracing to determine spatially the
grid point corresponding to each source-receiver pair as well as to map the oﬀsets to
incidence angles for the wave incident on the top of the fractured layer (where the
incidence angle is the angle the incident wave makes with the vertical axis). This
gives, for each grid point (i, j) a set of P-P reﬂection amplitudes that vary with
incidence angle θ ∈Θ and acquisition azimuth (relative to North) φAcq ∈ΦAcq, where
Θ and ΦAcq are the sets of incidence angles and acquisition azimuths over which
the data has been obtained; we denote the reﬂection amplitudes by ˆRPP
ij (θ, φAcq).
For concreteness, suppose the acquisition azimuths we have are precisely the set
ΦAcq = {0◦, 10◦, . . . , 170◦}. In order to be able to compare these amplitudes to the
P-wave reﬂection coeﬃcient, for each incidence angle θ, we normalize the amplitudes
by the mean amplitude (taken over acquisition azimuths). This allows us to compare
the variation of the reﬂection coeﬃcient with azimuth (rather than its absolute value):
dAvAz
ij
= dAvAz
ij,θ,φAcq =
ˆRPP
ij (θ, φAcq)
1
|ΦAcq|
P
φ∈ΦAcq ˆRPP
ij (θ, φ)
(3.10)
59

In order to arrive at a forward model for the P-wave reﬂection coeﬃcient of the
interface above the fractured layer as a function of acquisition azimuth, we make
various simplifying assumptions about the formation and the fractured medium. The
layers above the fractured layer are assumed to be isotropic and homogeneous and
the background medium of the layer in which the fractures exist is assumed to be
homogeneous and isotropic with known medium parameters. We assume that the
presence of fractures in the fractured layer causes the layer to behave as an equivalent
anisotropic medium, which is a geophysically valid assumption when the fracture
spacing is small compared to the seismic wavelength [65, 78].
In this case, it is
reasonable to assume that the presence of a parallel set of vertical fractures causes
the medium to exhibit horizontal transverse isotropy (HTI) with a symmetry axis
normal to the strike of the fractures [60, 72]. A transverse isotropic medium with a
given symmetry axis means seismic wave propagation in all directions that form the
same angle with the symmetry axis is equivalent. As such, in an HTI medium resulting
from a set of parallel vertical fractures, the plane normal to the symmetry axis (and
parallel to the fractures) is referred to as the isotropy plane, as wave propagation is
equivalent in all directions in this plane [72].
The P-wave reﬂection coeﬃcient of an interface is deﬁned as the ratio of the
reﬂected P-wave amplitude to the incident P-wave amplitude on the interface. Rüger
[60] derives the P-wave polarization vector and P-wave phase velocities in an HTI
medium and uses these to solve a system of perturbation equations for the reﬂection
and transmission coeﬃcients at the interface of two HTI media having the same
symmetry axes. The resultant P-wave reﬂection coeﬃcient is given as a function of
the incidence phase angle (θ, the angle the incident P-wave makes with the vertical
axis) and the azimuthal phase angle (φ, the azimuth of the incident P-wave relative
to the symmetry axis), and in terms of the isotropic background and anisotropy
parameters, as
60

RPP(θ, φ) = 1
2
∆Z
¯Z + 1
2
 
∆α
¯α −
2¯β
¯α
2 ∆G
¯G +
 
∆δ(V ) −2
2¯β
¯α
2
∆γ(V )
!
cos2 φ
!
sin2 θ
+ 1
2
∆α
¯α + ∆ǫ(V ) cos4 φ + ∆δ(V ) sin2 φ cos2 φ

sin2 θ tan2 θ,
(3.11)
where α is the vertical P-wave velocity, β is the vertical velocity of the S-wave polar-
ized parallel to the isotropy plane, ρ is the medium density, Z = ρα is the vertical
P-wave impedance, and G = ρβ2 is the vertical shear modulus; these parameters are
all from the background isotropic model and are assumed to be known in our analysis.
Rüger [61] and Liu and Martinez [41] note that this linearized equation for the P-wave
reﬂection coeﬃcient is accurate for small medium contrasts and weak anisotropy at
angles of incidence less than 35◦. In cases where a more accurate model is required,
one may wish to use the approximations of the P-wave reﬂection coeﬃcient given by
Ursin and Haugen [73] or Pšenčík and Martins [58].
The parameters δ(V ), ǫ(V ), γ(V )
are the Thomsen anisotropy parameters deﬁned with respect to the vertical axis [71];
these parameters are identically zero for an isotropic medium, but will depend on the
fracture properties for an HTI medium. The parameters in Equation 3.11 are deﬁned
in terms of their relative diﬀerences between the upper and lower media ∆(·) and their
average values ¯(·). So, for example, ∆α = α2 −α1 and ¯α = (α1 + α2)/2, where α1
and α2 are the vertical P-wave velocities of the upper and lower media, respectively.
Since the axis of symmetry is normal to the strike of the fractures (thus having an
azimuth relative to North of ϕij + 90◦), then with φAcq as the known azimuth of the
incident P-wave relative to North, we have
φ = φAcq −ϕij −90◦.
(3.12)
Furthermore, we set the incidence angle θ to the values computed for the AvAz data.
We use the linear slip model of Schoenberg and Sayers [66] to express the Thom-
sen anisotropy parameters of the fractured medium in terms of the excess fracture
61

compliance. The details of this derivation are given in Appendix A. Combining this
with (3.11) gives the forward model for the P-P reﬂection coeﬃcient as a function of
the fracture parameters at node (i, j), which we denote by RPP
ij (θ, φAcq, ϕij, zij). To
make this comparable to the data dAvAz
ij
deﬁned in (3.10), we process it in the same
manner by normalizing by the mean reﬂection coeﬃcient, for each incidence angle θ,
over all acquisition azimuths, giving
¯RPP
ij (θ, φAcq, ϕij, zij) =
RPP
ij (θ, φAcq, ϕij, zij)
1
|ΦAcq|
P
φ∈ΦAcq RPP
ij (θ, φ, ϕij, zij)
(3.13)
as the deterministic forward model for dAvAz
ij,θ,φAcq. We note that if the excess fracture
compliance is zero, then the P-P reﬂection coeﬃcient is constant with respect to
acquisition azimuth, and hence does not vary with the fracture orientation ϕij. This is
consistent with our interpretation of zero compliance to mean the absence of fractures,
which indeed renders the value of ϕij arbitrary.
We arrive at a stochastic model for the data by assuming the output of the forward
model is perturbed by zero-mean additive independent, identically distributed (i.i.d.)
Gaussian noise, so that
dAvAz
ij ,θ,φAcq = ¯RPP
ij (θ, φAcq, ϕij, zij) + wij,θ,φAcq
(3.14)
where the wij,θ,φAcq are mutually independent Gaussian random variables distributed
as N (0, σ2
ij,AvAz). We estimate the variance σ2
ij,AvAz using synthetic data obtained from
a ﬁnite-diﬀerence simulation of the seismic waveﬁeld; the details of the synthetic data
are described in Section 3.5.1. Processing of the synthetic data gives a set of single
observations of the data dAvAz
ij,θ,φAcq at a single grid node and at a range of incidence
angles and acquisition azimuths, where the fracture properties (zij, ϕij) are known,
thus giving independent samples for the noise
wij = wij,θ,φAcq = dAvAz
ij ,θ,φAcq −¯RPP
ij (θ, φAcq, ϕij, zij).
(3.15)
62

We estimate σ2
ij,AvAz via its maximum likelihood (ML) estimator, which is given by
ˆσ2
ij,AvAz,ML = ˆσ2
ij,AvAz,ML (wij) =
1
|Θ| |ΦAcq|
X
θ∈Θ
X
φ∈ΦAcq
w2
ij,θ,φ.
(3.16)
Note that, in contrast to the ML estimator of the combined variance and mean of
a normal random variable, the ML estimator in Equation 3.16 is unbiased, that is
E

ˆσ2
ij,AvAz,ML (wij)

= σ2
ij,AvAz. Having fully described the stochastic model for the
data, we are now in a position to give an expression for the likelihood model for dAvAz,
which is:
p(dAvAz|m) =
Y
(i,j)∈L

Y
θ∈Θ
Y
φAcq∈ΦAcq
N
 dAvAz
ij ,θ,φAcq; ¯RPP
ij (θ, φAcq, ϕij, zij), ˆσ2
ij,AvAz,ML


,
(3.17)
where N ( · ; µ, σ2) is the Gaussian probability density function (PDF) with mean µ
and variance σ2.
Fracture Transfer Function Data
We further suppose that we have what Fang et al. [22] describe as fracture transfer
function data.
We brieﬂy describe the deﬁnition of the FTF data and how it is
computed, which will result in a natural choice for our data dFTF and its likelihood
model p(dFTF|m).
Intuitively, the fracture transfer function is the transfer function from the seismic
waveﬁeld reﬂected oﬀthe top of the fractured layer to the waveﬁeld propagating out
of the fractured layer after reﬂecting oﬀthe bottom of this layer. In other words, it
quantiﬁes the redistribution of energy of the reﬂected and scattered seismic waveﬁeld
after passing through the fractured layer. A cartoon depicting this is shown in Figure
3-3.
FTF is inherently a function of the propagation azimuth of the incident and re-
ﬂected waves. At fracture spacings on the order of the seismic wavelength, the orien-
63

Figure 3-3: A cartoon depicting the meaning of fracture transfer function for layer r2.
I(ω) is the incident waveﬁeld, T(ω) is the transmitted waveﬁeld into the fractured
layer, and O1(ω) and O2(ω) are the waves reﬂected by layers above and below the
fracture zone, respectively. Theoretically, the the fracture transfer function at angular
frequency ω is deﬁned as FTF(ω) = O2(ω)
O1(ω). Figure adapted from Fang et al. [22].
64

tation of the fractures relative to the propagation azimuth has a signiﬁcant eﬀect on
the amplitude of the scattered waveﬁeld reﬂected oﬀthe bottom of the fractured layer.
In particular, when fractures are parallel to the propagation azimuth, the fractures
tend to act as waveguides, directing more of the scattered energy back to the surface
in the direction away from the source. However, when the fractures are normal to
the propagation azimuth, the scattered energy is less coherent as the fractures tend
to scatter energy in both forward and backward directions. With this in mind, we
expect FTF to be maximized at propagation azimuths parallel to the fractures.
According to the methodology given by Fang et al. [22] and Fang et al. [21], the
FTF at a particular spatial grid point (i, j) is estimated from surface seismic data
by ﬁrst determining (via ray-tracing) all source-receiver pairs corresponding to grid
point (i, j). Then, for all source-receiver pairs within the same acquisition azimuth
bin φAcq, normal moveout to zero oﬀset is applied to the seismic traces which are
then stacked. The result of this procedure gives a single, stacked seismic trace for
each acquisition azimuth.
The arrivals on the traces corresponding to reﬂections
oﬀthe top and bottom of the fractured layer are then located in the stacked trace
and windowed, giving windowed arrivals for each acquisition azimuth oij
1 (t, φAcq) and
oij
2 (t, φAcq), respectively. The Fourier transforms Oij
1 (ω, φAcq) and Oij
2 (ω, φAcq) of the
windowed arrivals are taken, and we compute the fracture transfer function at angular
frequency ω and acquisition azimuth φAcq as
FTF ij(ω, φAcq) = Oij
2 (ω, φAcq)
Oij
1 (ω, φAcq)
.
(3.18)
This is reduced to a function of only acquisition azimuth by integrating out the
angular frequency via a weighted integral. The idea is that frequencies at which there
is greater variability in FTF with acquisition azimuth should be given more weight,
hence a frequency weighting function W ij(ω) is deﬁned as the standard deviation of
FTF ij(ω, ·) with respect to acquisition azimuth, so that
FTF ij(φAcq) =
Z
ω
FTF ij(ω, φAcq)W ij(ω) dω.
(3.19)
65

Due to the reasons mentioned above, we expect FTF ij(φAcq) to be maximized at
ϕij if fractures are present in the medium. On the other hand, in the absence of
fractures, we expect there will not be a unique maximizer for FTF ij(φAcq). Hence, it
is natural to deﬁne the FTF data used in our analysis as dFTF
ij
=
 dFTF
ij,1 , dFTF
ij,2

, where
dFTF
ij,1 ∈{0, 1} is an indicator variable set to 0 when there is no unique maximizer
(within a numerical threshold) for FTF ij(φAcq), and set to 1 otherwise and where
dFTF
ij,2 is set to the acquisition azimuth that maximizes FTF ij(φAcq):
dFTF
ij,2 ≜arg max
φAcq∈ΦAcq FTF ij(φAcq).
(3.20)
If there is no unique maximizing φAcq, we arbitrarily set dFTF
ij,2 to any one maximizing
value.
We deﬁne a stochastic forward model for dFTF
ij
by ﬁrst assuming that, given the
fracture properties mij at node (i, j), dFTF
ij
is conditionally independent of the fracture
properties and FTF data at the remaining nodes.
We deﬁne ζij as the probability that dFTF
ij,1 correctly predicts whether or not there
are fractures present at node (i, j). Then, given the fracture properties, we model
dFTF
ij,1 as a Bernoulli random variable. Thus, given zij = −13 (i.e. zero excess fracture
compliance) then dFTF
ij,1 = 0 with probability ζij and dFTF
ij,1 = 1 with probability 1 −ζij,
and given zij > −13 (i.e. non-zero excess fracture compliance) then dFTF
ij,1 = 1 with
probability ζij and dFTF
ij,1 = 0 with probability 1 −ζij. That is,
p(dFTF
ij,1 | mij ; ζij) =





ζij
if dFTF
ij,1 = 1{zij>−13}
1 −ζij
if dFTF
ij,1 = 1 −1{zij>−13}
,
(3.21)
where 1{·} is the indicator function deﬁned as
1{A} ≜





1
if A
0
otherwise
.
Now, given dFTF
ij,1 and the fracture properties, if either zij = −13, so that there are
66

no fractures present, or if dFTF
ij,1 = 0, so that a unique preferential scattering direction
was not identiﬁed, then any value for dFTF
ij,2 ∈[0◦, 180◦) is arbitrary, so we model dFTF
ij,2
as uniform on the set [0◦, 180◦). Otherwise, if both zij > −13 and dFTF
ij,1 = 1, then
dFTF
ij,2 should be near the true fracture orientation ϕij. As with the AvAz data, we
model this by introducing additive zero-mean independent Gaussian noise, so that
conditioned on the event {zij ̸= 0, dFTF
ij,1 = 1}, we have:
dFTF
ij,2 = ϕij + vij
(3.22)
where vij ∼N (0, σ2
ij,FTF) and {vij : (i, j) ∈L} is a collection of mutually independent
random variables. This gives the conditional distribution for dFTF
ij,2 as
p
 dFTF
ij,2 | dFTF
ij,1 , mij ; σ2
ij,FTF

=





1
1801{dFTF
ij,2 ∈[0,180)}
if zij = −13 or dFTF
ij,1 = 0
N
 dFTF
ij,2 ; ϕij, σ2
ij,FTF

if zij > −13 and dFTF
ij,1 = 1
.
(3.23)
As with the AvAz data, we use synthetic data from ﬁnite-diﬀerence simulations of
the seismic waveﬁeld (described in Section 3.5.1), to obtain a set of K independent
samples Dij = (m(k)
ij , dFTF
ij
(k))k=1,...,K of the fracture properties and FTF data at a
single grid point, arising from the simulation of K diﬀerent fracture models. The
ML estimate for ζij is simply the fraction of times dFTF
ij,1
(k) correctly takes the value 0
(when z(k)
ij = −13) or 1 (when z(k)
ij > −13). For the synthetic data we have obtained,
this fraction turns out to be 1. However, in order to preserve stochasticity in detecting
the presence of fractures from FTF data, we instead estimate ζij under a Bayesian
approach by treating it as random variable with a prior distribution that is uniform
over [0, 1]. Having observed K correct observations (and 0 incorrect observations) of
dFTF
ij,1 , the posterior distribution for ζij is a Beta distribution ζij ∼Beta(K + 1, 1).
67

Integrating out ζij in the likelihood model for dFTF
ij,1 , given the data, we have
p
 dFTF
ij,1 | mij, Dij

=
Z 1
0
p(ζij|mij, Dij) p(dFTF
ij,1 | mij, Dij, ζij) dζij
(3.24)
=
Z 1
0
p(ζij|Dij) p(dFTF
ij,1 | mij, ζij) dζij
(3.25)
=





1
B(K+1,1)
R 1
0 ζK+1
ij
(1 −ζij)0 dζij
if dFTF
ij,1 = 1{zij>−13}
1
B(K+1,1)
R 1
0 ζK
ij (1 −ζij)1 dζij
if dFTF
ij,1 = 1 −1{zij>−13}
(3.26)
=





B(K+2,1)
B(K+1,1)
if dFTF
ij,1 = 1{zij>−13}
B(K+1,2)
B(K+1,1)
if dFTF
ij,1 = 1 −1{zij>−13}
(3.27)
=





K+1
K+2
if dFTF
ij,1 = 1{zij>−13}
1
K+2
if dFTF
ij,1 = 1 −1{zij>−13}
(3.28)
where B(·, ·) is the beta function. We are left with K′ i.i.d. samples where z(k)
ij > −13
(and dFTF
ij,1 = 1), giving samples of the additive Gaussian noise
n
v(k′)
ij
o
k′=1,...,K′ =
n
ϕ(k)
ij −dFTF
ij,2
(k) : z(k)
ij > −13, dFTF
ij,1
(k) = 1
o
used to give the ML estimation of the variance
ˆσ2
ij,FTF,ML = ˆσ2
ij,FTF,ML
n
v(k′)
ij
o
k′=1,...,K′

= 1
K′
K′
X
k′=1

v(k′)
ij
2
.
(3.29)
As before, the ML estimator in Equation 3.29 is unbiased. This gives the likelihood
model p(dFTF|m) as:
p
 dFTF | m

=
Y
(i,j)∈L
p
 dFTF
ij,1 | mij, Dij

p
 dFTF
ij,2 | dFTF
ij,1 , mij ; ˆσ2
ij,FTF,ML

(3.30)
68



















(a)









(b)
Figure 3-4: (a) Graphical model showing the Markovianity between the observations
d and the fracture parameters m. (b) Graphical model for the posterior distribution
after removing the observed nodes.
69

3.4.3
Inference Algorithms
We return to the graphical model representation of the distribution, as this will play a
key role in the inference algorithms used to obtain the posterior marginals and MAP
estimate. Having deﬁned the prior and likelihood models, the posterior distribution
is given by Equation 3.1. We immediately notice that given the fracture properties of
a particular grid node (i, j), the observations dij at that grid node are conditionally
independent of the remaining fracture properties and observations, that is, with m−ij
and d−ij deﬁned as in Equation 3.3, dij ⊥⊥{m−ij, d−ij} | mij; this is depicted in Figure
3-4(a).
Having observed the data d = d, the data are no longer random and hence
separate nodes for the data are not included in the graphical model for the posterior
distribution. Hence, we can write the posterior distribution in terms of the node and
edge potentials of the graph, ψij and ψij,kl, respectively, where the node potentials
capture the eﬀect of the data and the edge potentials capture the prior distribution.
These potentials are given by:
ψij(mij) = p(dAvAz
ij
| mij)p(dFTF
ij
| mij)
(3.31)
and
ψij,kl(mij, mkl) = exp{−βzij,kl(zij −zkl)2 −βϕij,kl(ϕij −ϕkl)2},
(3.32)
and the posterior distribution is then given as:
p(m|d) ∝
Y
(i,j)∈V
ψij(mij)
Y
(ij,kl)∈E
ψij,kl(mij, mkl).
(3.33)
Having fully described the posterior distribution in terms of its graphical model
and node and edge potentials, we are able to apply belief propagation algorithms to
perform approximate inference of the fracture properties m.
70

Loopy Belief Propagation
Belief propagation (BP) is a technique for performing inference on graphical models
which has recently enjoyed much popularity for use amongst a wide-range of applica-
tions [48, 55, 56]. Originally formulated for tree graphs (i.e. graphs having no cycles),
BP refers to message-passing algorithms for computing either marginal distributions
(called the sum-product algorithm) or MAP conﬁgurations (called the max-product
algorithm). In particular, for an undirected graph G = (V, E), the BP algorithm
computes messages (denoted by µij→kl(mkl)) from each node (i, j) ∈V to every node
(k, l) which shares an edge with (i, j) (called a ‘neighbor’ of (i, j)); these messages
capture the beliefs node (i, j) carries about its neighbors. The messages are iteratively
propagated from each node to its neighbors, hence the name ‘belief propagation.’
The sum-product variant of the BP algorithm [35] with node and edge potentials
ψij and ψij,kl is given by the update equations
µ(0)
ij→kl(mkl) ∝1
(3.34)
µ(t+1)
ij→kl(mkl) ∝
X
mij
ψij(mij)ψij,kl(mij, mkl)
Y
uv∈Nb(ij)\{kl}
µ(t)
uv→ij(mij)
(3.35)
ˆp(t)
ij (mij) ∝ψij(mij)
Y
uv∈Nb(ij)
µ(t)
uv→ij(mij)
(3.36)
∀(i, j) ∈V, ((i, j), (k, l)) ∈E, where Nb(ij) denotes the set of neighbors of node (i, j)
in G and ˆp(t)
ij (mij) is the estimate of the marginal for node (i, j) at iteration t. One
can verify that if the underlying graphical model G is a tree, then the sum-product
algorithm converges to the true marginal distributions in a number of iterations equal
to the diameter of the tree [35]. The max-product algorithm is similarly deﬁned by
71

replacing summations with maximizations
µ(0)
ij→kl(mkl) ∝1
(3.37)
µ(t+1)
ij→kl(mkl) ∝max
mij ψij(mij)ψij,kl(mij, mkl)
Y
uv∈Nb(ij)\{kl}
µ(t)
uv→ij(mij)
(3.38)
ˆ¯p(t)
ij (mij) ∝ψij(mij)
Y
uv∈Nb(ij)
µ(t)
uv→ij(mij)
(3.39)
∀(i, j) ∈V, ((i, j), (k, l)) ∈E, where ˆ¯p(t)
ij (mij) is the estimate of the node max-
marginal ¯pij(mij) for node (i, j) at iteration t.
The node max-marginal (at node
(i, j)) is deﬁned to be the function of mij one would obtain by ﬁxing the random
variable at node (i, j) to the value mij and then maximizing the joint distribution
p(m) over all other random variables. That is:
¯pij(mij) ≜max
m−ij p(mij, m−ij).
(3.40)
It is important to note that the node max-marginals are not the marginal distribu-
tions, and in fact they are not even probability distributions.
However, they can
be used to readily obtain the MAP estimate of m. In particular, if the node max-
marginals ¯pij(mij) have unique maximizers m∗
ij, then the MAP estimate is simply the
vector of these unique maximizing values for each node:
ˆmMAP =

m∗
ij

ij∈V =
"
arg max
mij
¯pij(mij)
#
ij∈V
.
(3.41)
Thus, the estimated node max-marginals ˆ¯p(t)
ij (mij) obtained from the max-product
algorithm can be used to approximate the MAP estimate.
Again, if G is a tree,
then it can be shown that the max-product algorithm converges to the true node
max-marginals and will hence produce the exact MAP estimate.
While BP was originally intended for tree graphs (and indeed converges to the
correct result on trees), it can still be applied to graphs which are not trees, such
as the grid graph for our posterior distribution. Applying BP to perform inference
72

on a graph with loops is referred to as loopy belief propagation. While LBP is an
approximate algorithm, as it does not, in general, converge to the correct answer, it
has nonetheless been used extensively in various settings and found to often give very
good approximations, particularly on graphs with a relatively sparse edge set (such
as our 2-D grid graph) and when the node potentials are strong relative to the edge
potentials [48]. With this in mind, we apply loopy belief propagation on the posterior
distribution for the fracture parameters to approximate the MAP conﬁguration and
marginal distributions.
3.5
Results
3.5.1
Synthetic Data
We validate our methodology by performing inference on a synthetic data set. The
synthetic data are obtained from a 3-D elastic ﬁnite-diﬀerence simulation of the
seismic waveﬁeld on reservoir models having topology as shown in Figure 3-1, and
is the same data referenced in the fracture transfer function study by Fang et al.
[22]. Each of the models consists of ﬁve ﬂat homogeneous layers, with fractures in
the third layer; the remaining layers are isotropic.
Following the methodology of
Coates and Schoenberg [14] to simulate discrete fractures, the ﬁnite-diﬀerence grid
cells intersecting individual fractures are modeled as anisotropic, and the individ-
ual fractures are spaced uniformly within an isotropic background medium (note the
ﬁnite-diﬀerence grid cells are distinct from and on a much smaller scale than the
grid nodes on which the random variables are deﬁned). The isotropic background
parameters are given in Table 3.1. It is important to note that we are not modeling
the entire fractured layer as anisotropic, but only the individual fractures; hence the
validity of this model does not depend on the fracture spacing.
The fractured layer in the models contains a single set of discrete parallel fractures
with strike 0◦and individual normal and tangential fracture compliances of 10−9 m/Pa
(note that the fracture compliance is distinct from the excess fracture compliance,
73

Layer
Thickness (m)
α (m/s)
β (m/s)
ρ (g/cm3)
1
200
3000
1765
2.20
2
200
3500
2060
2.25
3
200
4000
2353
2.30
4
200
3500
2060
2.25
5
200
4000
2353
2.30
Table 3.1: Isotropic background parameters for the ﬁnite-diﬀerence synthetic data.
74

which is the ratio of fracture compliance to fracture spacing). The models diﬀer from
one another by fracture spacing, where synthetic data have been obtained from models
having fracture spacings of 12 m, 20 m, 40 m, 60 m, 80 m, and 100 m, and where
the fracture parameters in a particular model are constant over the entire layer. The
synthetic seismic trace dataset for each model is obtained from a 2-D array of surface
seismic receivers spaced 4 m apart responding to a single Ricker source wavelet having
central frequency of 40 Hz. AvAz data are computed from the seismic trace dataset
by using ray-tracing to compute the arrival time of the P-P arrival reﬂected from the
top of Layer 3 for each receiver and taking the amplitude of this arrival in the seismic
trace, where the source-receiver oﬀsets and acquisition azimuths are known. Since
the layers are ﬂat and homogeneous, the reﬂection from all points on the horizontal
grid are equivalent on average. Hence, the data are treated as the generic AvAz data
for a single grid node or common depth point (CDP). Similarly, the FTF data are
computed according to the procedure in Section 3.4.2 and treated as the generic FTF
data corresponding to a single node or CDP. Prior to processing the synthetic data,
we perturb the raw seismic traces with zero-mean Gaussian noise, with a standard
deviation of 5% of the peak amplitude of the data, where a diﬀerent realization of
the noise is used for each CDP gather.
To obtain measurements for the entire grid L, we predetermine the fracture pa-
rameters m over L. For each node (i, j) ∈L, we are free to choose any fracture strike
in the full range of azimuths [0◦, 180◦), as we can simply rotate the synthetic data
from 0◦to any desired azimuth ϕij. For the log excess compliance zij, we are able to
use any value obtained from the models corresponding to excess compliances that can
be achieved using fracture compliance of 10−9 m/Pa divided by any of the spacings for
which synthetic data have been obtained. Having set the desired fracture properties
across the grid, we map the noisy synthetic data to speciﬁc values across the grid
L. Processing the noisy data according to the procedure described in Section 3.4.2
results in our vector of noisy measurements d corresponding to the known fracture
properties m over the entire grid. A grid spacing of ℓ= 200 m is used for L.
75

3.5.2
Results of Inference Procedure
We perform the inference on synthetic data arising from two scenarios. The ﬁrst
scenario is given by a 20-by-20 node grid of a single fracture set (so that the fracture
properties are constant along the grid), for each of the available fracture spacing
models. The second scenario is given by a 20-by-40 node grid of two fracture sets
with distinct excess compliances and orientations, separated spatially by a linear
discontinuity (such as that which may arise from a vertical, planar fault). The eﬀect
of the smoothness parameter βc on the inference result is investigated by performing
the inference for diﬀerent choices of βc.
LBP is applied in each case to obtain the approximate MAP conﬁguration and
posterior marginal distributions of the fracture properties, the latter of which are used
to compute the approximate BLS estimate of the fracture properties. LBP converged
for all models in less than 200 iterations, when the smoothness parameter βc was
taken to be less than or equal to 0.1. Choices of the smoothness parameter greater
than 0.1 resulted in LBP not converging for some realizations of the noisy data.
We must take care to interpret the results correctly, as we have taken zij = −13 to
mean that no fractures are present at node (i, j), which would render ϕij meaningless
and arbitrary. Thus, we compute the posterior marginals for ϕij conditioned on the
event {zij > −13}, and likewise compute the BLS estimates for these random variables
conditioned on the same event. The results of the inference procedure on the single
fracture system are plotted in Figures 3-5–3-7. The resulting residuals between the
estimates and true values are given in Table 3.2 in terms of the root mean squared
(RMS) residuals over all nodes.
We observe that the inference procedure performs very well at fracture spacings
smaller than 40 m. This is to be expected as the forward model for the AvAz data
relies on the assumption that fractures cause the medium to behave as an equivalent
anisotropic medium, but this assumption breaks down as the fracture spacing becomes
comparable to the dominant seismic wavelength (which is 100 m in the fractured
layer and 87.5 m in the layer above the fractures). Furthermore, while the scattering
76

Fracture Spacing
ǫrms,ˆzBLS
ǫrms, ˆϕBLS
ǫrms,ˆzMAP
ǫrms, ˆϕMAP
12 m
0.132 log10Pa−1
1.27 ◦
0.123 log10Pa−1
1.00◦
20 m
0.102 log10Pa−1
2.27 ◦
0.083 log10Pa−1
2.00◦
40 m
0.488 log10Pa−1
10.53 ◦
0.235 log10Pa−1
18.63◦
60 m
0.670 log10Pa−1
0.05 ◦
0.530 log10Pa−1
0◦
80 m
0.555 log10Pa−1
7.98 ◦
0.756 log10Pa−1
0◦
100 m
0.358 log10Pa−1
0.12 ◦
0.187 log10Pa−1
0◦
Table 3.2: Root mean square of residuals between estimates and ground truth (mean
taken over all nodes) when estimating a single fracture set with fracture compliance
10−9 m/Pa, fracture strike ϕij = 60◦, and varying fracture spacing, and with smooth-
ness parameter set to βc = 0.1. For comparison, note that azimuth ϕ is discretized
into 20◦bins and log compliance z has been discretized into bins of size 0.1 log10Pa−1.
77

ˆzMAP
Eastings (m)
Northings (m)
 
 
1000
2000
3000
4000
0
1000
2000
3000
4000
−13
−12.5
−12
−11.5
−11
−10.5
−10
−9.5
−9
Eastings (m)
Northings (m)
 
 
1000
2000
3000
4000
0
1000
2000
3000
4000
−13
−12.5
−12
−11.5
−11
−10.5
−10
−9.5
−9
Eastings (m)
Northings (m)
 
 
1000
2000
3000
4000
0
1000
2000
3000
4000
−13
−12.5
−12
−11.5
−11
−10.5
−10
−9.5
−9
Eastings (m)
Northings (m)
 
 
1000
2000
3000
4000
0
1000
2000
3000
4000
−13
−12.5
−12
−11.5
−11
−10.5
−10
−9.5
−9
Eastings (m)
Northings (m)
 
 
1000
2000
3000
4000
0
1000
2000
3000
4000
−13
−12.5
−12
−11.5
−11
−10.5
−10
−9.5
−9
zTrue
Eastings (m)
Northings (m)
 
 
1000
2000
3000
4000
0
1000
2000
3000
4000
−13
−12.5
−12
−11.5
−11
−10.5
−10
−9.5
−9
Eastings (m)
Northings (m)
 
 
1000
2000
3000
4000
0
1000
2000
3000
4000
−13
−12.5
−12
−11.5
−11
−10.5
−10
−9.5
−9
Eastings (m)
Northings (m)
 
 
1000
2000
3000
4000
0
1000
2000
3000
4000
−13
−12.5
−12
−11.5
−11
−10.5
−10
−9.5
−9
Eastings (m)
Northings (m)
 
 
1000
2000
3000
4000
0
1000
2000
3000
4000
−13
−12.5
−12
−11.5
−11
−10.5
−10
−9.5
−9
Eastings (m)
Northings (m)
 
 
1000
2000
3000
4000
0
1000
2000
3000
4000
−13
−12.5
−12
−11.5
−11
−10.5
−10
−9.5
−9
ˆϕMAP
Eastings (m)
Northings (m)
 
 
1000
2000
3000
4000
0
1000
2000
3000
4000
−20
0
20
40
60
80
100
120
140
Eastings (m)
Northings (m)
 
 
1000
2000
3000
4000
0
1000
2000
3000
4000
−20
0
20
40
60
80
100
120
140
Eastings (m)
Northings (m)
 
 
1000
2000
3000
4000
0
1000
2000
3000
4000
−20
0
20
40
60
80
100
120
140
Eastings (m)
Northings (m)
 
 
1000
2000
3000
4000
0
1000
2000
3000
4000
−20
0
20
40
60
80
100
120
140
Eastings (m)
Northings (m)
 
 
1000
2000
3000
4000
0
1000
2000
3000
4000
−20
0
20
40
60
80
100
120
140
Spacing = 12 m
Spacing = 20 m
Spacing = 40 m
Spacing = 60 m
Spacing = 100 m
Figure 3-5: Approximate MAP estimates of the fracture properties computed for
models of a single fracture set, with fracture compliance 10−9 m/Pa, fracture strike
ϕij = 60◦, and varying fracture spacing, and with smoothness parameter set to βc =
0.1.
78

ˆzBLS
Eastings (m)
Northings (m)
 
 
1000
2000
3000
4000
0
1000
2000
3000
4000
−13
−12.5
−12
−11.5
−11
−10.5
−10
−9.5
−9
Eastings (m)
Northings (m)
 
 
1000
2000
3000
4000
0
1000
2000
3000
4000
−13
−12.5
−12
−11.5
−11
−10.5
−10
−9.5
−9
Eastings (m)
Northings (m)
 
 
1000
2000
3000
4000
0
1000
2000
3000
4000
−13
−12.5
−12
−11.5
−11
−10.5
−10
−9.5
−9
Eastings (m)
Northings (m)
 
 
1000
2000
3000
4000
0
1000
2000
3000
4000
−13
−12.5
−12
−11.5
−11
−10.5
−10
−9.5
−9
Eastings (m)
Northings (m)
 
 
1000
2000
3000
4000
0
1000
2000
3000
4000
−13
−12.5
−12
−11.5
−11
−10.5
−10
−9.5
−9
zTrue
Eastings (m)
Northings (m)
 
 
1000
2000
3000
4000
0
1000
2000
3000
4000
−13
−12.5
−12
−11.5
−11
−10.5
−10
−9.5
−9
Eastings (m)
Northings (m)
 
 
1000
2000
3000
4000
0
1000
2000
3000
4000
−13
−12.5
−12
−11.5
−11
−10.5
−10
−9.5
−9
Eastings (m)
Northings (m)
 
 
1000
2000
3000
4000
0
1000
2000
3000
4000
−13
−12.5
−12
−11.5
−11
−10.5
−10
−9.5
−9
Eastings (m)
Northings (m)
 
 
1000
2000
3000
4000
0
1000
2000
3000
4000
−13
−12.5
−12
−11.5
−11
−10.5
−10
−9.5
−9
Eastings (m)
Northings (m)
 
 
1000
2000
3000
4000
0
1000
2000
3000
4000
−13
−12.5
−12
−11.5
−11
−10.5
−10
−9.5
−9
ˆϕBLS
Eastings (m)
Northings (m)
 
 
1000
2000
3000
4000
0
1000
2000
3000
4000
−20
0
20
40
60
80
100
120
140
Eastings (m)
Northings (m)
 
 
1000
2000
3000
4000
0
1000
2000
3000
4000
−20
0
20
40
60
80
100
120
140
Eastings (m)
Northings (m)
 
 
1000
2000
3000
4000
0
1000
2000
3000
4000
−20
0
20
40
60
80
100
120
140
Eastings (m)
Northings (m)
 
 
1000
2000
3000
4000
0
1000
2000
3000
4000
−20
0
20
40
60
80
100
120
140
Eastings (m)
Northings (m)
 
 
1000
2000
3000
4000
0
1000
2000
3000
4000
−20
0
20
40
60
80
100
120
140
Spacing = 12 m
Spacing = 20 m
Spacing = 40 m
Spacing = 60 m
Spacing = 100 m
Figure 3-6: Approximate BLS estimates of the fracture properties computed for mod-
els of a single fracture set, with fracture compliance 10−9 m/Pa, fracture strike
ϕij = 60◦, and varying fracture spacing, and with smoothness parameter set to
βc = 0.1.
79

ˆp(zij|d)
−13
−12
−11
−10
−9
0
0.1
0.2
0.3
0.4
0.5
log10(Compliance [1/Pa])
Probability
−13
−12
−11
−10
−9
0
0.1
0.2
0.3
0.4
0.5
log10(Compliance [1/Pa])
Probability
−13
−12
−11
−10
−9
0
0.1
0.2
0.3
0.4
0.5
log10(Compliance [1/Pa])
Probability
−13
−12
−11
−10
−9
0
0.1
0.2
0.3
0.4
0.5
log10(Compliance [1/Pa])
Probability
−13
−12
−11
−10
−9
0
0.1
0.2
0.3
0.4
0.5
log10(Compliance [1/Pa])
Probability
ˆp(ϕij|d)
20
60
100
140
0
0.2
0.4
0.6
0.8
1
Azimuth [deg]
Probability
20
60
100
140
0
0.2
0.4
0.6
0.8
1
Azimuth [deg]
Probability
20
60
100
140
0
0.2
0.4
0.6
0.8
1
Azimuth [deg]
Probability
20
60
100
140
0
0.2
0.4
0.6
0.8
1
Azimuth [deg]
Probability
20
60
100
140
0
0.2
0.4
0.6
0.8
1
Azimuth [deg]
Probability
Spacing = 12 m
Spacing = 20 m
Spacing = 40 m
Spacing = 60 m
Spacing = 100 m
Figure 3-7: Approximate posterior marginal distributions (blue) of the fracture prop-
erties at a single node plotted along with the prior distributions (green). Results
are given as mean ± 1 S.D. over all grid nodes. True value is plotted with a red
‘x’.
Computed for models of a single fracture set, with fracture compliance 10−9
m/Pa, fracture strike ϕij = 60◦, and varying fracture spacing, and with smoothness
parameter set to βc = 0.1.
80

assumptions underlying the forward model for the FTF data are valid for fracture
spacings on the order of the seismic wavelength, the FTF data in this study only
contributes to fracture detection and strike estimation; the actual excess compliance
value (given fractures are present) has no bearing on our model for the FTF data. In
particular, we notice from Table 3.2 that the RMS residuals for estimating log excess
compliance grows to multiple bin sizes at fracture spacings of 40 m and larger. The
marginal distributions in Figure 3-7 convey well what happens at spacings of 40 m and
larger. We see that the least best ﬁt to fracture orientation is obtained at 40 m spacing;
this is likely because at this mid-range fracture spacing, we see a weaker response
from both the AvAz data and the FTF data. However, at larger fracture spacings,
the marginal distributions for fracture orientation remain concentrated around the
true orientation of 60◦, as expected due to both the very simple model for FTF in
terms of fracture orientation as well as the assumptions underlying the FTF model
remaining strong at larger spacings. We observe that excess compliance tends to be
underestimated at fracture spacing values of 40 m and above. This observation is
consistent with our intuition for AvAz data; at larger spacings, the AvAz response
becomes weaker, and a better ﬁt to the data is found with smaller excess compliances
than those resulting from the true fracture compliance and spacing.
In order to investigate the eﬀect of the smoothness parameter βc on the inference
we apply our procedure over a range of choices for βc on the 80 m spacing model. The
eﬀect of the smoothness parameter is most easily seen in the MAP estimates, which
are plotted in Figure 3-8. Observing the changes in the estimate with increasing
smoothness parameter βc, we see that the a higher value of βc has the eﬀect of
denoising the estimates. When βc is 0, this is identical to performing the inference
on a fully disconnected graph, as the edge potentials will all be identically equal to 1.
As such, the estimate at each node will ﬁt only the noisy data corresponding to itself.
Increasing βc strengthens the links between adjacent nodes, and can cause an incorrect
ﬁt to noisy data to be less probable; this is particularly true for fracture azimuth,
which is estimated correctly at 80 m fracture spacing. Increasing βc still smooths the
estimates for excess compliance, however towards the underestimated value found at
81

80 m (again due to the weaker AvAz response at high fracture spacings). At βc = 0.1,
we see a considerable improvement in the estimates for fracture azimuth.
We now turn to the second scenario arising from two sets of fractures with diﬀerent
azimuths and fracture spacings separated by a discontinuity on the grid such as that
which may arise from a geological fault. We investigate the eﬀect of a priori knowledge
of the fault by performing the inference both with and without knowledge of the fault
location encoded in the prior with the smoothness parameter set to βij,kl = 0 at the
fault and βij,kl = βc = 0.1 elsewhere. The results are plotted in Figures 3-9 and
3-10 for fracture azimuth and log excess compliance, respectively. As evidenced by
the ﬁgures, when the fault is unknown a priori, the estimates for fracture properties
are smoothed across the fault. This is particularly undesirable for the estimation of
fracture azimuth, where we would otherwise be able to obtain good estimates in both
regions. Specifying the location of the fault a priori sets the smoothness parameter
at the corresponding edges to 0, and hence we no longer observe this behavior.
3.6
Conclusions and Future Work
A methodology for estimation of fracture properties from AvAz and FTF data under
a Bayesian inference framework has been presented. The inference is performed by
running loopy belief propagation on the 2-D Markov random ﬁeld of fracture variables.
LBP converged relatively quickly on the synthetic data, in under 200 iterations for
all models, when using a smoothness parameter less than or equal to 0.1. We have
demonstrated that the approximate inference results perform well for both fracture
azimuth and excess compliance at low spacings of 12 m and 20 m, and continue to
give good estimates for fracture azimuth up to 100 m spacing. This is signiﬁcant,
as we are able to estimate the fracture properties in a rigorous manner at a greater
range of spacings than would otherwise be attainable.
We further showed that our use of the spatial smoothness prior has the eﬀect of
denoising estimates that would otherwise be incorrect. We also demonstrated the
capability of this framework to handle prior information about geological features,
82

ˆzMAP
Eastings (m)
Northings (m)
 
 
1000
2000
3000
4000
0
1000
2000
3000
4000
−13
−12.5
−12
−11.5
−11
−10.5
−10
−9.5
−9
Eastings (m)
Northings (m)
 
 
1000
2000
3000
4000
0
1000
2000
3000
4000
−13
−12.5
−12
−11.5
−11
−10.5
−10
−9.5
−9
Eastings (m)
Northings (m)
 
 
1000
2000
3000
4000
0
1000
2000
3000
4000
−13
−12.5
−12
−11.5
−11
−10.5
−10
−9.5
−9
Eastings (m)
Northings (m)
 
 
1000
2000
3000
4000
0
1000
2000
3000
4000
−13
−12.5
−12
−11.5
−11
−10.5
−10
−9.5
−9
Eastings (m)
Northings (m)
 
 
1000
2000
3000
4000
0
1000
2000
3000
4000
−13
−12.5
−12
−11.5
−11
−10.5
−10
−9.5
−9
ˆϕMAP
Eastings (m)
Northings (m)
 
 
1000
2000
3000
4000
0
1000
2000
3000
4000
−20
0
20
40
60
80
100
120
140
Eastings (m)
Northings (m)
 
 
1000
2000
3000
4000
0
1000
2000
3000
4000
−20
0
20
40
60
80
100
120
140
Eastings (m)
Northings (m)
 
 
1000
2000
3000
4000
0
1000
2000
3000
4000
−20
0
20
40
60
80
100
120
140
Eastings (m)
Northings (m)
 
 
1000
2000
3000
4000
0
1000
2000
3000
4000
−20
0
20
40
60
80
100
120
140
Eastings (m)
Northings (m)
 
 
1000
2000
3000
4000
0
1000
2000
3000
4000
−20
0
20
40
60
80
100
120
140
Ground Truth
βc = 0
βc = 0.001
βc = 0.01
βc = 0.1
Figure 3-8: Approximate MAP estimates of the fracture properties computed on
a model containing a single set of fractures with fracture compliance 10−9 m/Pa,
fracture spacing 80 m, and fracture orientation 60◦. Ground truth is plotted along
with the estimates using various values for the smoothness parameter βc.
83

Eastings (m)
Northings (m)
 
 
2000
4000
6000
0
2000
4000
80
90
100
110
120
Eastings (m)
Northings (m)
 
 
2000
4000
6000
0
2000
4000
80
90
100
110
120
Ground Truth Values for z
ˆzBLS with Fault Known
Eastings (m)
Northings (m)
 
 
2000
4000
6000
0
2000
4000
80
90
100
110
120
0
2000
4000
6000
8000
70
80
90
100
110
120
130
Azimuth [°]
Eastings [m]
 
 
Ground Truth
Fault Known
Fault not Known
ˆzBLS with Fault not Known
Comparison of Estimates
Figure 3-9: Eﬀect of a priori knowledge of the fault on approximate BLS estimates
of the fracture azimuth of a model containing two fracture sets. The fractures on
the left are at azimuth 120◦, spacing 100 m, and fracture compliance 10−9 m/Pa.
The fractures on the right are at azimuth 80◦, spacing 12 m, and fracture compliance
10−9 m/Pa. Ground truth is plotted along with the estimates with the smoothness
parameter βc = 0.1. The fourth pane shows a comparison of the estimates at the
horizontal slice North=2000 m.
84

Eastings (m)
Northings (m)
 
 
2000
4000
6000
0
2000
4000
−11.4
−11.2
−11
−10.8
−10.6
−10.4
−10.2
Eastings (m)
Northings (m)
 
 
2000
4000
6000
0
2000
4000
−11.4
−11.2
−11
−10.8
−10.6
−10.4
−10.2
Ground Truth Values for z
ˆzBLS with Fault Known
Eastings (m)
Northings (m)
 
 
2000
4000
6000
0
2000
4000
−11.4
−11.2
−11
−10.8
−10.6
−10.4
−10.2
0
2000
4000
6000
8000
−11.5
−11
−10.5
−10
log10(Excess Compliance [Pa−1])
Eastings [m]
 
 
Ground Truth
Fault Known
Fault not Known
ˆzBLS with Fault not Known
Comparison of Estimates
Figure 3-10: Eﬀect of a priori knowledge of the fault on approximate BLS estimates of
the fracture excess compliance of a model containing two fracture sets. The fractures
on the left are at azimuth 120◦, spacing 100 m, and fracture compliance 10−9 m/Pa.
The fractures on the right are at azimuth 80◦, spacing 12 m, and fracture compliance
10−9 m/Pa. Ground truth is plotted along with the estimates with the smoothness
parameter βc = 0.1. The fourth pane shows a comparison of the estimates at the
horizontal slice North=2000 m.
85

such as the discontinuity shown in the previous ﬁgures. While we presented a very
simple case of this, it is not diﬃcult to extend this to more complicated scenarios.
Having validated our procedure on synthetic data, the next natural step will be to
obtain ﬁeld data and apply the inference procedure to estimate the desired fracture
properties.
One future direction to improve the inference is to relate fracture spacing and
compliance to the FTF data. While the data we chose depended only on the pres-
ence and orientation of fractures, Fang et al. [22] showed both theoretically and in
laboratory experiments that FTF also contains information about fracture spacing.
However, even when using synthetic data, the precise physical relationship between
FTF and fracture spacing has been diﬃcult to determine, but a geophysical basis re-
mains for exploring this avenue further. If a reliable forward model can be determined
to relate FTF to fracture spacing, then we will be able to move beyond estimating
excess fracture compliance to estimation of individual fracture compliances and frac-
ture spacing. A related future direction is to incorporate additional features of the
seismic data in the inference procedure. In particular, Zheng et al. [82] describe a
theory for using 3-D beam interference to determine fracture properties of a reservoir
from reﬂected seismic P-wave data.
86

Chapter 4
Least-Squares Migration with a
Hierarchical Bayesian Framework
4.1
Summary
In many geophysical inverse problems, smoothness assumptions on the underlying ge-
ology are utilized to mitigate the eﬀects of poor data coverage and noise in the data
and to improve the quality of the inferred model parameters. Within a Bayesian infer-
ence framework, a priori assumptions about the probabilistic structure of the model
parameters impose such a smoothness constraint (also known as regularization). We
consider the particular problem of inverting seismic data for the subsurface reﬂec-
tivity of a 2-D medium, where we assume a known velocity ﬁeld. In particular, we
consider a hierarchical Bayesian generalization of the Kirchhoﬀ-based least-squares
migration (LSM) method. We present here a novel methodology for estimation of
both the reﬂectivity model and regularization parameters, using a Bayesian statisti-
cal framework that treats both of these as random variables to be inferred from the
data. Hence rather than ﬁxing the regularization parameters prior to inverting for
the image, we allow the data to dictate where to regularize. In order to construct
our prior of the subsurface and regularization parameters, we deﬁne an undirected
graphical model (or Markov random ﬁeld) on the image, where the vertices of the
graph represent subsurface reﬂectivity values and the regularization parameters are
87

deﬁned to parameterize the edges of the graph. Estimating these regularization pa-
rameters (which we refer to as edge strengths) gives us information about the degree
of conditional correlation (or lack thereof) between neighboring image parameters,
and subsequently incorporating this information in the ﬁnal model produces more
clearly visible discontinuities in the estimated image.
The inference framework is
veriﬁed on a 2-D synthetic dataset, where the hierarchical Bayesian imaging results
signiﬁcantly outperform standard LSM images. We note that while this method is
presented within the context of seismic imaging, it is in fact a general methodology
which can be applied to any linear inverse problem in which there are spatially-varying
correlations in the model parameter space.
4.2
Introduction
Seismic imaging (also known as migration) refers to the process of creating an image
of the Earth’s subsurface reﬂectivity from seismograms generated by sources and
recorded by receivers located, typically, at or near the surface. Traditional migration
methods for constructing the image generally involve operating on the seismic data
with the adjoint of an assumed forward modeling operator [11], possibly along with
a modifying function which attempts to correct for amplitude loss due to geometric
spreading, transmission, absorption, etc. [5, 29]. In recent years, attempts have been
made to cast the imaging problem as a least-squares inverse problem [19, 49]. This
approach to imaging is conventionally referred to as least-squares migration (LSM).
Early treatments of this approach can be found in LeBras and Clayton [39] and
Lambare et al. [37]. This chapter will deal mainly with Kirchhoﬀ-based LSM , which
uses a ray-theoretic based forward modeling operator; its derivation and application
is discussed in Nemeth et al. [49] and Duquet et al. [19]. LSM can also be applied
with wave-equation-based forward modeling, as shown by Kühl and Sacchi [36].
In solving the least-squares inverse problem, it is common to include some form
of regularization in the LSM cost function in order to penalize less smooth images.
For example, Clapp [12] describes two regularization schemes for LSM in which the
88

image is constrained to be smooth either along geological features predetermined by
a seismic interpreter or along the ray-parameter axis. In these and other applications
of LSM, the regularization is chosen independently of the seismic data, i.e. it is a
ﬁxed input to the inversion procedure (as it is in the vast majority of geophysical
applications of inversion). This, however, may result in sub-optimal inversion results;
overly strong regularization may result in over-smoothing the image, whereas weak
regularization may not adequately penalize roughness in the image due to noise. Even
if an appropriate regularization strength is determined, the true smoothness structure
of the model need not be spatially uniform or even isotropic; for example, the true
earth may typically contain many sharp discontinuities where any form of smoothing
would be undesirable.
In this chapter, we propose a more general approach to LSM which solves for
parameters deﬁning the image regularization in conjunction with the optimal image
itself. The approach is formulated within the framework of Bayesian inference, in
which regularization is accomplished with a prior probability distribution on the im-
age parameters. We deﬁne a spatially-varying smoothness prior and seek to jointly
estimate its parameters along with the image.
In particular, we utilize a variant
of Bayesian inference known as hierarchical Bayes, which provides a rigorous mathe-
matical framework for addressing the joint estimation of the image and regularization
parameters. This should allow for preserving sharpness in the image at the true dis-
continuities while still smoothing the eﬀects of noise.
Previous applications of hierarchical Bayesian inference in geophysics include Ma-
linverno and Briggs [45], who applied it to 1-D traveltime tomography, Malinverno
[43, 44], who applied Bayesian model selection to ﬁnd optimal parameterizations of
1-D density and resistivity models, Buland and Omre [9], who applied hierarchical
Bayesian methods in amplitude versus oﬀset (AVO) inversion, and Bodin et al. [7],
who applied Bayesian model selection to determine group velocities for the Australian
continent.
In the next sections, we review Kirchhoﬀ-based LSM and proceed to develop the
hierarchical Bayesian framework and algorithms used to solve the inference problem.
89

4.3
Methodology
4.3.1
Standard Kirchhoﬀ-based LSM framework
KirchhoﬀModeling
The Kirchhoﬀmodeling operator is a ray-based forward modeling operator that gives
the seismic data as a linear function of the reﬂectivity model.
In particular, to
simulate the seismogram dsr(t) recorded at a seismic receiver r from a seismic source
s, Kirchhoﬀforward modeling ﬁrst generates a source-to-reﬂector-to-receiver travel
time (or two-way travel time) ﬁeld τsr(x) by utilizing what is known as the “exploding
reﬂector” concept. This concept refers to the treatment of each point in the reﬂectivity
model as a point source. The two-way travel time can be computed as the sum of the
source-to-reﬂector and reﬂector-to-receiver travel times, as determined by ray-tracing
through a speciﬁed background velocity model of the subsurface. The ray tracer also
computes the ﬁeld of ray-path lengths Rs(x) and Rr(x) and opening angles between
the source and receiver rays at each reﬂection point θsr(x). Once these quantities
have been computed, the synthetic data ˆdsr(t) are computed by superposition over
reﬂector locations x of scaled and shifted versions of the source wavelet ws(t) (after
applying a 90-degree phase-shift to simulate the eﬀects of 2-D propagation). For each
x, the phase-shifted wavelet ˜ws(t) is delayed by τsr(x) and scaled by the reﬂectivity
value m(x), an obliquity correction factor cos(θsr(x)/2), and a geometric spreading
correction (in 2-D, 1/
p
Rs(x)Rr(x)). Thus,
ˆdsr(t) =
Z
X
m(x) ˜ws (t −τsr (x)) cos(θsr(x)/2)
p
Rs(x)Rr(x)
dx,
(4.1)
where X ⊂R2 is the model domain. We note that the above Kirchhoﬀmodeling
operator is precisely the adjoint operator to the Kirchhoﬀmigration operator, given
by:
ˆm(x) =
X
s
X
r
Z
t
dsr(t) ˜ws (t −τsr (x)) cos(θsr(x)/2)
p
Rs(x)Rr(x)
dt.
(4.2)
If we discretize time and space, we can represent our data and image as ﬁnite-
90

dimensional vectors d and m, where the dimension of d is the number of source-
receiver pairs times the number of time samples, and where the dimension of m is the
number of points in a spatial grid sampling the model domain. Then, replacing the
integral in (4.1) with a summation, we can express the Kirchhoﬀmodeling operator
in matrix form:
ˆd = Am.
(4.3)
In particular, the ith column of A, corresponding to a point xi in the model grid,
will contain a sampled version of the source wavelet for each source-receiver pair,
appropriately scaled or shifted, giving (in 2-D):
Asrt,i = ˜ws (t −τsr (xi)) cos(θsr(xi)/2)
p
Rs(xi)Rr(xi)
ℓ2,
(4.4)
where ℓis the spatial discretization interval.
Standard LSM Framework
Least-squares migration attempts to solve the imaging problem by seeking the image
mLS that minimizes the ℓ2-norm of the residual (the diﬀerence between the observed
data d and the modeled data ˆd = Am). Without regularization, the LSM image is
given by
mLS = arg min
m
∥d −Am∥2
2,
(4.5)
where ∥· ∥2 denotes the ℓ2-norm in the (discretized) data-space given by
∥d∥2
2 =
X
s
X
r
X
t
dsr(t)2.
(4.6)
To ensure well-posedness of the LSM solution, regularization is often introduced by
augmenting the LSM cost function with a term which penalizes diﬀerences between
model parameters and an additional term which penalizes the magnitude of the image.
91

This gives the regularized LSM image as
mRLS = arg min
m
∥d −Am∥2
2 + λ

X
(i,j)∈E
βij(mi −mj)2 + ǫ
X
i
m2
i


(4.7)
= arg min
m
∥d −Am∥2
2 + λmT (D (β) + ǫI) m,
(4.8)
where βij ∈[0, 1] indicates how strongly to penalize the diﬀerence between mi and
mj, E is the set of all pairs of image parameter indices whose diﬀerence we decide
to potentially penalize, λ > 0 assigns the maximal weight given to penalizing these
diﬀerences, and ǫ > 0 weights the penalty on parameter magnitudes. Equation (4.8)
is simply (4.7) rewritten in compact matrix-vector notation, where D is a diﬀerencing
operator deﬁned by the vector β = {βij : (i, j) ∈E}. Taking the derivative of the
right-hand side of (4.8) and setting it to zero yields the solution to the regularized
LSM problem:
mRLS =
 ATA + λ(D (β) + ǫI)
−1 ATd.
(4.9)
Note that ǫ > 0 ensures that the regularized LSM cost function is a positive-deﬁnite
quadratic function of the image m, and hence its minimizer is unique.
4.3.2
Bayesian Framework
Standard Bayesian Formulation
The same solution to LSM can be derived from a Bayesian formulation of the imaging
problem, wherein the image m and the data d are taken to be random vectors. In
particular, we take m a priori to be Gaussian with zero mean and some covariance
matrix C (i.e. m ∼N (0, C)) , so that the prior distribution p(m) for m is given by
p(m) ∝exp

−1
2mTC−1m

.
(4.10)
We model the seismic data as d = Am+n where A is our Kirchhoﬀmodeling operator
and n is zero-mean Gaussian noise with some covariance matrix Σ (i.e. n ∼N (0, Σ)).
92

Thus the conditional distribution for the data d given the model m will be
p(d|m) ∝exp

−1
2 (d −Am)T Σ−1 (d −Am)

,
(4.11)
i.e. d|m ∼N (Am, Σ).
Applying Bayes’ rule gives the posterior distribution for the model m conditioned
on the data d as
p(m|d) = p(m)p(d|m)
p(d)
(4.12)
∝
1
p(d) exp

−1
2
h
mT C−1m + (d −Am)T Σ−1 (d −Am)
i
.
(4.13)
Rearranging terms in (4.13) and dropping any multiplicative factors that do not
depend on m, we obtain
p(m|d) ∝exp

−1
2
 m −µpost

Λ−1
post
 m −µpost
T

.
(4.14)
where µpost is the posterior mean given by
µpost =
 ATΣ−1A + C−1−1 ATΣ−1d
(4.15)
and Λpost is the posterior covariance matrix given by
Λpost =
 ATΣ−1A + C−1−1.
(4.16)
That is, the posterior distribution for m conditioned on d is itself Gaussian: m|d ∼
N (µpost, Λpost).
The Bayesian maximum a posteriori (MAP) estimate mMAP is the image that
maximizes the posterior distribution (4.13). It is clear from (4.14) that mMAP = µpost.
Comparing to Equation (4.9), we also see that mMAP = mRLS when we set the prior
93

and noise covariance matrices as
C = (λ(D (β) + ǫI))−1
(4.17)
and
Σ = I.
(4.18)
Constructing the Prior via a Graphical Model
The choice of β plays a key role in determining the spatial smoothness properties of
the prior on the model. This is perhaps best seen through the expressive formalism of
probabilistic graphical models. In particular, we deﬁne an undirected graphical model
G = (V, E), with a set of vertices (or nodes) V, which index the random variables mi
comprising the random vector m, and a set of edges E ⊂V × V, represented as pairs
of vertices in V that encode dependencies between the random variables. Recall from
Chapter 2 that for a Gaussian random vector m with precision matrix (or inverse
covariance matrix) Q = C−1, it can be shown that m forms an MRF over G if and
only if Q is at least as sparse as the edge set E (meaning that no edge between nodes
i and j in G implies that Qij = Qji = 0) [35].
Deﬁning the prior precision matrix as Q = λ(D(β)+ǫI), as in (4.17), allows the βij
to determine the “strength” of each edge in G. In probabilistic terms, β captures the
prior conditional dependence structure of the image m, such that βij = 0 implies that,
prior to observing d, mi is conditionally independent of mj when {mk : k ̸= i, j} is
given. For this reason, we sometimes refer to the elements of β as the edge strengths
of G and to D(β) as the weighted graph Laplacian of G (weighted by β). This is
depicted in Figure 4-1 for a simple nine pixel image. Note that although Figure 4-1
shows edges connecting only nearest neighbors horizontally and vertically, this need
not be the case. We can consider a situation where each node shares an edge with
all other nodes within a speciﬁed radius; the graphical model depicted in the ﬁgure
results from using a radius of 1 node.
94


m1 
m2 
m3 
m4 
m5 
m6 
m7 
m8 
m9 
β12 
β23 
β14 
β25 
β36 
β47 
β58 
β69 
β78 
β89 
β45 
β56 
Figure 4-1: The Markov random ﬁeld imposed on m by ﬁxing β prior to observing
the data d, for a simple nine pixel image.
Hierarchical Bayesian Formulation
Thus far we have assumed that the parameters λ, ǫ, and β, which determine the
regularization in the LSM framework and the prior model covariance structure in the
Bayesian framework, are known. We now describe how we can expand the Bayesian
formulation to the problem of estimating these regularization parameters from the
data d, in addition to the image m. We focus on the estimation of the edge strengths
β, which capture our belief about where we think the image should be smooth. This
is a reasonable approach since the edge strengths β give us prior information about
our model m, and m gives us information about our data d, hence we should be able
to infer something about β from d. This is depicted in the directed graphical model
of Figure 4-2, which also illustrates the induced Markov chain structure between β,
m, and d.
In order to estimate β from d, we consider β to be a random vector endowed with
its own prior p(β). Accordingly, all probability distributions in the previous sections
can be considered as conditional on β. In particular, we now write the prior on m|β
as
p (m | β) = |λ (D (β) + ǫI)|1/2 exp

−1
2mT(λ (D (β) + ǫI))m
	
(2π)N/2
(4.19)
95

 β 
m 
A 
p(β) 
p(m|β) 
p(d|m) 
d 
Figure 4-2: The directed graphical model capturing the Markov chain structure be-
tween β, m, and d. The node for d is shaded to indicate that d is an observed
quantity that the posterior distributions of β and m are conditioned upon.
and the conditional distribution for d|m, β as
p (d | m, β) = p (d | m)
(4.20)
=
exp
n
−1
2 (d −Am)T Σ−1 (d −Am)
o
(2π)K/2 Σ1/2
,
(4.21)
where N is the number of model parameters (i.e. the dimension of m) and K is the
number of data points (the dimension of d). We again apply Bayes’ rule to obtain
the joint posterior distribution for m and β given the data d:
p (m, β | d) = p (β) p (m | β) p (d | m, β)
p (d)
(4.22)
= p (β)
p (d)
|λ (D (β) + ǫI)|1/2
(2π)(N+K)/2 Σ1/2
exp

−1
2

(d −Am)T Σ−1 (d −Am) + mT (λ (D (β) + ǫI))m

.
(4.23)
To deﬁne p(β), we endow each βij with a uniform prior on the set [0, 1] and let
the βij be mutually independent random variables, so that
p(β) =
Y
(i,j)∈E
1[0,1] (βij) ,
(4.24)
96

where
1S(x) =



1
if x ∈S
0
if x /∈S
.
We note that (4.23) is very similar to the posterior distribution in the non-hierarchical
Bayesian setting (where β is ﬁxed) with some important diﬀerences: ﬁrstly, (4.23) is
now a function of both m and β, and secondly, outside the exponential of (4.23) is
the determinant of m’s prior precision matrix Q (which can no longer be dropped as
a proportionality constant, since it depends on β). Computing this determinant is
expensive, with time complexity O (N2) (since Q is a sparse matrix with bandwidth
N1/2), and reﬂects the additional computational cost of the hierarchical Bayesian
approach.
Having obtained the joint posterior distribution p (m, β | d), the task of estimating
the best image remains. Here, we explore two estimation methodologies within the
hierarchical Bayesian framework: the hierarchical Bayes solution and the empirical
Bayes solution [45]. What is strictly known as the hierarchical Bayes solution is the
full marginal posterior distribution of the image p (m | d) (marginalizing out β from
the joint posterior distribution p (m, β | d)). Hence, we have for the hierarchical Bayes
solution
p (m | d) =
Z
B
p (m, β | d) dβ
(4.25)
where B is the domain of admissible vectors β. Unfortunately, the marginalization
operation cannot be performed analytically and must be computed numerically. We
may also consider the MAP estimates for the image that can be derived within the
hierarchical Bayesian setting. The hierarchical Bayes MAP estimate mHB is the MAP
estimate of m based on its marginal posterior distribution p (m | d):
mHB = arg max
m
Z
B
p (m, β | d) dβ
(4.26)
One can think of mHB as the single best image m over all choices of edge strengths
β. While the posterior marginal distribution for the image (4.25) is the complete
solution to the Bayesian inference problem, a number of computational issues prevent
97

its use in practice. Firstly, due to both the high-dimensionality of B and the cost
of evaluating the joint posterior distribution (4.23), both stochastic sampling from
and direct marginalization of the joint posterior distribution are computationally
intractable. Furthermore, even if we were able to evaluate the marginal posterior
(4.25), the high-dimension of m would make it diﬃcult to explore.
A somewhat diﬀerent solution for estimating the image is known as the empirical
Bayes solution, which ﬁrst looks for the best choice for β, then, using that choice,
ﬁnds the best image mEB. If one takes the MAP estimate for β then we would have
βMAP = arg max
β
Z
M
p (m, β|d) dm
(4.27)
where, it turns out, the marginalization over m can be performed analytically but the
maximization over β must still be performed numerically. Given βMAP, the empirical
Bayes solution is taken as the MAP estimate with respect to p(m | d, βMAP). The
results of the previous sections then imply
mEB =
 ATΣ−1A + λ (D (βMAP) + ǫI)
−1 ATΣ−1d.
(4.28)
The empirical Bayes solution is within reach as long as we are able to compute
βMAP by solving the marginal MAP problem of (4.27). In order to do so, we turn
to the expectation-maximization (E-M) algorithm, which has direct application in
solving such marginal MAP problems.
4.3.3
The Expectation-Maximization (E-M) Algorithm
The E-M algorithm [17, 46] is a powerful and versatile algorithm for solving maximum
likelihood and MAP parameter estimation problems when a subset of the variables
relevant to the parameter estimation is unobserved (referred to as latent variables).
In the context of the seismic imaging problem we consider here, we view the image
m as the latent variables. In the empirical Bayes approach, these variables must
be marginalized from the joint posterior distribution on m and β when attempting
98

to estimate the edge strengths β.
For our purposes, E-M can be thought of as
a coordinate ascent algorithm for solving the marginal MAP optimization problem
(4.27), whereby subsequent estimations are performed between the latent variables
(m) and the parameters to be estimated (β).
In what follows of this section, we give a derivation of the E-M algorithm; similar
derivations and a more thorough treatment of E-M can be found in Bishop [4] or
McLachlan and Krishnan [46]. To derive the E-M algorithm, we note that maximizing
a probability distribution is equivalent to maximizing its logarithm, and deﬁne our
objective function as the log marginal posterior
ℓ(β) = log p(β | d).
(4.29)
Rearranging terms in the joint posterior distribution, we can rewrite the MAP objec-
tive function as:
ℓ(β) = log
Z
M
p(m, β | d) dm
(4.30)
= log
Z
M
p(m, β, d)
p(d)
dm
(4.31)
= log
Z
M
p(β)p(m, d | β)
p(d)
dm
(4.32)
= log
Z
M
p(m, d | β) dm + log p(β) −log p(d).
(4.33)
Here we introduce a proxy distribution on the image, q(m | d), where we can choose q
to be any probability distribution we like as long as it has the same support as p(m)
and where we have made explicit that q can depend on the data d. Dividing and
multiplying by q, we have:
ℓ(β) = log
Z
M
q(m | d)
q(m | d)p(m, d | β) dm + log p(β) −log p(d)
(4.34)
= log Eq(m|d)
p(m, d | β)
q(m | d)

+ log p(β) −log p(d),
(4.35)
99

where the integral in (4.34) has been recognized as the expected value with respect
to q (denoted by Eq) to arrive at (4.35). Now, by Jensen’s inequality [46] and the
concavity of the log function, we have
ℓ(β) ≥Eq(m|d)

log
p(m, d | β)
q(m | d)

+ log p(β) −log p(d)
(4.36)
= ˆℓ(q, β).
(4.37)
We see that the function ˆℓ(q, β) is a lower bound on the original objective function
ℓ(β).
The E-M algorithm maximizes this lower-bound according to the following
coordinate ascent scheme, starting with an initial guess ˆβ(0) and iterated for t =
0, 1, 2, . . . :
E-Step: ˆq(t+1) = arg max
q
ˆℓ(q, ˆβ(t))
(4.38)
M-Step: ˆβ(t+1) = arg max
β
ˆℓ(ˆq(t+1), β).
(4.39)
It turns out the E-step can be solved analytically. Let us propose a candidate
solution ˜q as the Bayesian posterior of m conditioned on d and the last iterate ˆβ(t) of
β:
˜q(m | d) = p

m | d, ˆβ(t)
.
(4.40)
Then if we plug the ˜q into the E-step objective function (4.38), we have:
ˆℓ(˜q, ˆβ(t)) = Ep(m | d,ˆβ(t))

log


p

m, d | ˆβ(t)
p

m | d, ˆβ(t)



+ log p

ˆβ(t)
−log p(d).
(4.41)
Recognizing the quotient in (4.41) as p(d | ˆβ(t)), and since the expectation of p(d | ˆβ(t))
100

is just itself, we have
ˆℓ

˜q, ˆβ(t)
= log p

d | ˆβ(t)
+ log p

ˆβ(t)
−log p(d)
(4.42)
= log
p

d | ˆβ(t)
p

ˆβ(t)
p(d)
= log p

ˆβ(t) | d

(4.43)
= ℓ

ˆβ(t)
(4.44)
(by Eq. 4.37) ≥ˆℓ

q, ˆβ(t)
∀q.
(4.45)
Since ˆℓ(q, ˆβ(t)) ≤ℓ(ˆβ(t)) for any q, it is clear that the candidate solution ˜q solves the
E-step, i.e.
q(t+1) = p

m | d, ˆβ(t)
.
(4.46)
Now, coming to the M-step, we can simplify its objective function by dropping all
terms which do not depend on β. Thus, plugging into (4.39) and employing (4.46),
we can write:
ˆβ(t+1) = arg max
β
n
log p(β) + Ep(m | d,ˆβ(t)) [log p(m, d | β)]
o
.
(4.47)
Because we were able to solve the E-step analytically, the E-M algorithm reduces to
iterating the single step given by (4.47). We do not actually need to compute the
Bayesian posterior in the E-step, but need only take the expectation with respect to it
(which is why the E-step is so named). It can be shown that an iteration of the E-M
algorithm (via Equation (4.47)) will never decrease the marginal posterior distribution
for β (which is maximized by the marginal MAP solution), and, under very general
conditions, the E-M algorithm does indeed converge to a (local) maximum of the
original marginal MAP problem of (4.27) [46].
4.3.4
Application of E-M to LSM
We now proceed to apply the E-M algorithm to our LSM problem. For notational
convenience, we can rewrite the E-M algorithm of (4.47) in terms of the E-M objective
101

function φ(t)(β) given by
φ(t)(β) = log p(β) + Ep(m | d,ˆβ(t)) [log p(m, d | β)] ,
(4.48)
so the E-M iteration becomes
ˆβ(t+1) = arg max
β
φ(t)(β).
(4.49)
For every iteration of E-M, we perform the maximization of φ(t) via a gradient ascent
scheme, for which we must compute the gradient of φ(t).
To derive the exact form of φ(t) and its gradient, we substitute our distributions
into the E-M objective function. From (4.24), we have
log p (β) =



0
if βij ∈[0, 1],
∀(i, j) ∈E
−∞
otherwise
,
(4.50)
which simply means the prior on β restricts us to consider only βij ∈[0, 1]. From
(4.19) and (4.20), we have
log p (m, d | β) = 1
2
 log det (λ (D (β) + ǫI)) −mT (λ (D (β) + ǫI)) m
−(d −Am)T Σ−1(d −Am)

−Z,
(4.51)
where Z is a normalization constant given by
Z = (N + K) log 2π + log Σ
2
.
(4.52)
Inserting these into (4.48) yields (when every βij ∈[0, 1]):
φ(t)(β) = 1
2 Ep(m | d,ˆβ(t))

log det (λ (D (β) + ǫI)) −mT (λ (D (β) + ǫI)) m
−(d −Am)TΣ−1(d −Am)

−Z.
(4.53)
The log determinant term in (4.53) only depends on β and is not aﬀected by the
102

expectation with respect to m. Now, we can rewrite the second term in the above
expectation as
mT (λ (D (β) + ǫI)) m = λ tr
 (D (β) + ǫI) mmT 
,
(4.54)
so
Ep(m | d,ˆβ(t))

mT (λ (D (β) + ǫI)) m

= λ tr

(D (β) + ǫI) Ep(m | d,ˆβ(t))

mmT
.
(4.55)
The expected value on the right-hand side of (4.55) is just the non-central second
moment matrix of m, as determined by the posterior distribution p(m | d, ˆβ(t)), given
by
Ep(m | d,ˆβ(t))

mmT
= Λ(t) + µ(t)µ(t)T,
(4.56)
and where µ(t) and Λ(t) are the posterior mean and covariance matrix, respectively,
when conditioning on d and ˆβ(t), given by
µ(t) =

ATΣ−1A + λ

D(ˆβ(t)) + ǫI
−1
ATΣ−1d
(4.57)
and
Λ(t) =

ATΣ−1A + λ

D(ˆβ(t)) + ǫI
−1
.
(4.58)
We further note that the ǫI E[mmT] term in (4.55) does not depend on the variable β
which is being optimized and hence can be dropped from the E-M objective function
φ(t)(β). Similarly, the third and fourth terms in (4.53) also do not depend on β and
can be neglected. Combining the above and rearranging terms, we can rewrite the
E-M objective function as
φ(t)(β) = 1
2

log det (λ (D (β) + ǫI)) −λ tr
 D (β) Λ(t)
−λ µ(t)TD (β) µ(t)
. (4.59)
In order to compute ∇φ(t), the gradient of φ(t) with respect to β, we ﬁrst note that
103

the β-weighted graph Laplacian matrix D(β) is a linear function of β, particularly:
D(β) =
X
(i,j)∈E
βijP ij
(4.60)
where the entries of P ij are
P ij
kl =









1
if kl = ii or jj
−1
if kl = ij or ji
0
otherwise
.
(4.61)
We also note that
∂
∂βij
log det (λ(D(β) + ǫI)) = tr

(λ(D(β) + ǫI))−1∂(λ(D(β) + ǫI))
∂βij

.
Letting C(β) = (λ(D(β)+ǫI))−1 denote the prior covariance matrix of the image
(when conditioning on β), to compute ∇φ(t), we have:
∂
∂βij
φ(t) (β) = λ
2

tr
 C(β)P ij
−tr
 Λ(t)P ij
−µ(t)TP ijµ(t)
(4.62)
= λ
2

C(β)ii + C(β)jj −2C(β)ij −

Λ(t)
ii + Λ(t)
jj −2Λ(t)
ij

−

µ(t)
i
−µ(t)
j
2
.
(4.63)
We constrain each βij to the interval [0, 1] by introducing proxy variables γij which
we map to the βij using a sigmoidal function. In particular we set
βij = arctan(γij)
π
+ 1
2,
(4.64)
so that while γij is free to take any value in R, βij remains within [0, 1]. We can then
104

compute ∇φ(t)(γ), the gradient of φ(t) with respect to γ, by
∂
∂γij
φ(t) (γ) = ∂φ(t)
∂βij
∂βij
∂γij
(4.65)
=
1
π(1 + γ2
ij)
∂φ(t)
∂βij
.
(4.66)
Unfortunately, direct computation of the gradient would require matrix inversions
to compute both the prior and posterior covariance matrices. To avoid this, we note
that we only need the node and edge-wise elements of these covariance matrices,
and we could instead sample from their associated Gaussian probability distributions
and approximate these elements from the samples. Thus, to approximate the prior
covariance matrix C(β), we generate L samples m(1), . . . , m(L), of the underlying
Gaussian prior distribution of m|β and approximate C as:
C(β) ≈1
L
L
X
ℓ=1
m(ℓ)m(ℓ)T.
(4.67)
We describe the sampling algorithm we use to approximate the elements of C(β) in
the following section.
Perturbation-Optimization Sampling of Gaussian Distributions
To sample from N (0, C), we ﬁrst note that the precision matrix Q = λ(D(β) + ǫI)
can be rewritten as
Q = λ(F TB(β)F + ǫI)
(4.68)
where F is a ﬁrst-diﬀerencing matrix (having number of rows equal to |E|, the num-
ber of edges in E, and and number of columns equal to N, the number of image
parameters) and B(β) is an |E|-by-|E| diagonal matrix, with the βij on its diagonal.
Referred to as Perturbation-Optimization (P-O) sampling by Orieux et al. [53], a
straight-forward sampling algorithm (that avoids the need for Cholesky factorization
of the precision matrix) is available when the precision matrix can be expressed in
105

the form
Q =
T
X
t=1
MT
t R−1
t Mt
(4.69)
and sampling from N (0, Rt) is feasible (which is certainly true in our case, as we have
diagonal Rt matrices). The sampling algorithm is then as follows:
Algorithm 4.1 Perturbation-Optimization algorithm for sampling from N (0, Q−1)
[53]
1. Perturbation step: Generate independent vectors
ηt ∼N (0, Rt)
for t = 1, . . . , T
2. Optimization step: Compute ˆm as the minimizer of
J(m) = PT
t=1(ηt −Mtm)TR−1
t (ηt −Mtm)
Return ˆm as the sample from N (0, Q−1).
The proof that ˆm is a sample from N (0, Q−1) is straight-forward and given in
Orieux et al. [53]. The optimization step simply requires solving the linear system
Q ˆm =
T
X
t=1
MT
t R−1
t ηt,
(4.70)
which, in our case, is very fast (O(kN) using an iterative solver with k steps) due to
the sparsity of F.
Block Diagonal Approximations
While this sampling approach can also be used to approximate the elements of the
posterior covariance matrix Λ(t), in practice generating a reasonably large number of
samples from N (0, Λ(t)) is not feasible due to the increased cost of solving a system
involving the posterior precision matrix ATΣ−1A+λ(D(ˆβ(t))+ǫI) (we would need to
perform a regularized LSM inversion for each sample when using the P-O approach).
In order to estimate the node and edge-wise elements of Λ(t) , we note that when
using the Kirchhoﬀoperator A, there is a closed form expression for the elements of
the posterior precision matrix Λ(t)−1 (combining (4.58) and (4.4)). With this in mind,
we can estimate elements of Λ(t) by considering a block diagonal approximation to
106

the precision matrix. In particular, we can construct an M-by-M partition of the
posterior precision matrix corresponding to an image point and its M −1 nearest
neighbors in space within some radius (we used a 49-pixel neighborhood to perform
this approximation), then approximate the covariance matrix at image point i, Λ(t)
ii ,
from the inverse of this M-by-M partition matrix. The oﬀ-diagonal elements Λ(t)
ij (for
each edge (i, j) ∈E) are similarly estimated from the same matrix inverse by taking
the elements corresponding to covariance between mi and mj (however, care must be
taken to ensure that the M-by-M partition of the precision matrix is large enough
to suﬃciently “surround” both the image point i and all its neighbors j with which
it shares an edge). This approximation will perform reasonably well as long as the
posterior precision matrix decays spatially (in the model domain) as we move away
from the diagonal (as is the case here).
Summary of E-M Algorithm
To implement the above approximations to calculate ∇φ(t), we need to approximate
the entries of Λ(t) only once per E-M iteration (as Λ(t) does not vary with β). However
we would need to reapproximate the entries of C(β) with the sampling algorithm in
each iteration of the ﬁrst-order gradient-ascent method (which must be re-run in
each iteration of the E-M algorithm). We now summarize our above developments
for applying the E-M algorithm to obtain the empirical Bayesian estimate of the
image in LSM in the algorithm below:
107

Algorithm 4.2 Expectation-Maximization Algorithm for LSM
Initialize each γ(0)
ij = 0, so that ˆβ(0)
ij = 0.5.
Specify step-size α for gradient ascent.
Set t = 0. Iterate on t:
1. Compute µ(t) via (4.57).
2. Compute Λ(t)
ii (∀i ∈V) and Λ(t)
ij (∀(i, j) ∈E) via block diagonal approximation.
3. Initialize ˜γ(0) = γ(t). Set s = 0 and iterate on s to perform gradient ascent on
γ:
a. Generate samples from N (0, C(β(˜γ(s)))) via P-O sampling.
b. Estimate C(β(˜γ(s))))ii (∀i ∈V) and C(β(˜γ(s))))ij (∀(i, j) ∈E) via (4.67).
c. Compute ∇φ(t)(˜γ(s)) via (4.66).
d. Update ˜γ(s+1) = ˜γ(s) + α∇φ(t)(˜γ(s))
4. Update γ(t+1) = ˜γ(s+1).
5. Update ˆβ(t+1) via (4.64) using γ(t+1).
Upon termination, return:
βMAP = ˆβ(t+1),
mEB =
 ATΣ−1A + λ (D (βMAP) + ǫI)
−1 ATΣ−1d.
4.4
Results
In order to validate our approach, we ran our inference algorithm on synthetic datasets.
We present two test cases: the ﬁrst case being a simple example where the data arise
from a small image consisting of three dipping reﬂectors separated by a weakly re-
ﬂective fault and the second case being data simulated from the Marmousi model.
Synthetic data were created using the same Kirchhoﬀmodeling operator A that is
used in the inference algorithms.
Hence, these test cases are what are known as
inverse crime tests. The purpose of using the same forward modeling operator to
create the synthetic data as is used in the inference is to isolate the inversion prob-
lem from the modeling problem. In order to somewhat avoid the inverse crime, we
add zero-mean white Gaussian noise to the data (with a standard deviation equal to
108

10% of the maximum amplitude of the data, assumed to be known by our inversion
procedure).
We ﬁrst describe the example of the three dipping reﬂectors. The data are created
from a single surface seismic source (at the center) and 50 equally spaced surface seis-
mic receivers (with spacing of 50 m) using a homogeneous background velocity model
(of 4000 m/s). The source wavelet is a 20 Hz Ricker wavelet; hence the dominant
wavelength is 200 m. The seismic traces are sampled at 1 ms, and the medium is sam-
pled spatially at 50 m in both the lateral and vertical directions. The entire medium
has spatial dimensions of 2500 m by 2500 m, hence Nx = Nz = 50 and the number
of image parameters is N = NxNz = 2500. The purpose on testing our algorithm
on such a small model is so that we can verify the performance of our algorithm in
the absence of any approximations (i.e. in this case, we can directly compute the
elements of C and Λ without the need of P-O sampling or block diagonal approxima-
tions). For this example, we used an MRF in which each node shares an edge with
its four nearest neighbors. Here, we ran 10 iterations of the E-M algorithm to obtain
the MAP estimate of the edge strengths and the empirical Bayes image, where each
iteration of the E-M algorithm ran in approximately 1 minute on a quad core IntelTM
Xeon W3550 3.0GHz processor.
For the case of the Marmousi model, we use a smoothed version of the true Mar-
mousi velocity model (sampled at 24 m spacing) for our background velocity model in
conjunction with the true (unsmoothed) reﬂectivity model to simulate the data. The
data are created from a set of 20 collocated surface sources and receivers (resulting
in 400 traces), with a 480 m spacing between stations, where the source wavelet is
a 25 Hz Ricker wavelet. For this case, we must resort to the approximate methods
outlined above (P-O sampling and block diagonal approximations) to compute the
gradient ∇φ(t). Here, in order to capture the more complex dipping structures of
the Marmousi model, we deﬁned the MRF so that each node shares an edge with all
nodes within a radius of
√
2 nodes (i.e. a node shares an edge with its four diago-
nal neighbors in addition to its four nearest neighbors). In this example, the MAP
estimate of the edge strengths and the empirical Bayes image were obtained with
109

3 iterations of the E-M algorithm, where each iteration of the E-M algorithm took
approximately 33 minutes on a quad core IntelTM Xeon W3550 3.0GHz processor.
We note that each iteration of the E-M algorithm requires performing a standard
least-squares migration in addition to the computation required to obtain the block
diagonal approximation to the posterior covariance matrix and perform the optimiza-
tions required for P-O sampling from the prior. The LSM images are computed using
the method of conjugate gradients (CG), which is run for 200 iterations per LSM.
Each iteration of CG requires performing a single Kirchhoﬀmodeling followed by a
single Kirchhoﬀmigration, and, for this example, takes about 0.7 seconds per CG
iteration (on the same machine).
Figures 4-3–4-9 show the results from the test case of the three dipping layer
model, where the edge strengths obtained using our algorithm are shown in Figure
4-9 and the resulting image obtained using these edge strengths is shown in Figure
4-8. Performing a Kirchhoﬀmigration on the data results in the image of Figure 4-5;
here the reﬂectors are imaged somewhat, but we also see heavy imaging artifacts (i.e.
the migration smiles) due to the limited source-receiver geometry (where only a single
source is being used). We observe that in the case of the unregularized LSM image
(Figure 4-6), the reﬂectors are imaged, but unfortunately, the noise in the data is
also imaged so strongly that the reﬂectors are nearly impossible to distinguish from
the noise. We can improve on the unregularized image by using a uniform regular-
ization scheme (setting each βij = 1) to obtain the regularized LSM image of Figure
4-7; here, the use of regularization has ﬁltered out the noise, but as a side eﬀect has
also smoothed out the reﬂectors. The empirical Bayesian MAP image (Figure 4-8)
obtained by using our estimate of the edge strengths signiﬁcantly improves upon this
result. This is clear from a qualitative comparison between the images; we can see
the reﬂectors imaged quite strongly with sharpness preserved at the reﬂectors, while
the noise is ﬁltered out elsewhere in the image. Additionally, the weakly reﬂective
fault is also slightly imaged in the empirical Bayesian MAP image, whereas it cannot
be seen in the other images. We further note that the correlation of the empirical
Bayesian MAP image with the true image is signiﬁcantly higher than the correla-
110

tions of the other images with the true image. Examining the estimate of the edge
strengths in Figure 4-9, we see that the edge strengths take on a pattern similar to
our expectations: they are high where the image is constant, but close to 0 where
there are diﬀerences in the image (surrounding the reﬂectors).
Figures 4-10–4-17 show the results from the test case with the Marmousi model.
We again observe the same features in the images as seen in the 3 layer test case. The
unregularized LSM image (Figure 4-14) shows the reﬂectors along with a very strong
noise component. Regularizing in a uniform fashion (by setting each βij = 1) results
in the regularized image of Figure 4-15 in which the noise has been ﬁltered out, but
the image is also overly smooth in some areas. Once again, using our algorithm to
estimate the edge strengths (which are shown in Figure 4-17) results in the empirical
Bayesian MAP image of Figure 4-16. We notice the same qualitative improvements
in the image as seen previously: the image remains sharp near the reﬂectors while
smoothing out the noise away from the reﬂectors. And, as before, the correlation of
the empirical Bayesian MAP image with the true image is signiﬁcantly higher than
the correlations of the other images with the true image.
4.5
Conclusions and Future Work
Our study shows that the Bayesian framework provides a ﬂexible methodology for
estimating both the image and smoothness parameters (or edge strengths) in a least-
squares migration setting. By estimating the edge strengths, we are able to remove
the eﬀects of noise while, by and large, preserving sharpness at the reﬂectors in the
image. The expectation-maximization algorithm, in particular, allowed us to solve
the marginal MAP problem for estimating the edge strengths β (without having to
explicitly compute the marginal posterior distribution for β).
We note that while our algorithm was presented within the context of the seis-
mic imaging problem, the methodology we have developed is broadly applicable to
many linear inverse problems where the model parameters may exhibit spatially (or
temporally) varying smoothness properties. The operator A (or, more generally, the
111

True Image
z [m]
x [m]
500
1000
1500
2000
2500
500
1000
1500
2000
2500
Figure 4-3: True image for the three layer test case.
t [ms]
x [m]
Noisy Data
500
1000
1500
2000
2500
0
500
1000
1500
2000
2500
3000
Figure 4-4: Noisy synthetic data for the three layer test case.
Kirchhoff Migrated Image
z [m]
x [m]
500
1000
1500
2000
2500
500
1000
1500
2000
2500
Figure 4-5: Kirchhoﬀmigrated image for the three layer test case. Correlation with
true image = 0.4705.
112

Unregularized LSM Image
z [m]
x [m]
500
1000
1500
2000
2500
500
1000
1500
2000
2500
Figure 4-6: Unregularized LSM image (each βij = 0) for the three layer test case.
Correlation with true image = 0.3649.
Uniformly Regularized LSM Image
z [m]
x [m]
500
1000
1500
2000
2500
500
1000
1500
2000
2500
Figure 4-7: Uniformly regularized LSM image (each βij = 1) for the three layer test
case. Correlation with true image = 0.5879.
Empirical Bayes MAP Image
z [m]
x [m]
500
1000
1500
2000
2500
500
1000
1500
2000
2500
Figure 4-8: Empirical Bayesian MAP image (computed after estimating β) for the
three layer test case. Correlation with true image = 0.9607.
113

0
500 1000 1500 2000
0
500
1000
1500
2000
 
Edge Strengths β
 
0
0.2
0.4
0.6
0.8
1
Figure 4-9: Edge strengths β estimated with E-M algorithm for the three layer test
case.
x [m]
z [m]
0
1000
2000
3000
4000
5000
6000
7000
8000
9000
0
500
1000
1500
2000
2500
Figure 4-10: True image for the Marmousi test case.
Shot Gathers (prior to adding noise)
Shot Location [m]
t [s]
0
50
100
150
200
250
300
350
400
450
0
2
4
6
8
10
Figure 4-11: Synthetic data for the Marmousi test case prior to adding noise.
114

Shot Location [m]
t [s]
Noisy Data − Shot Gathers
0
50
100
150
200
250
300
350
400
450
0
2
4
6
8
10
Figure 4-12: Noisy synthetic data for the Marmousi test case.
x [m]
z [m]
0
1000
2000
3000
4000
5000
6000
7000
8000
9000
0
500
1000
1500
2000
2500
Figure 4-13: Kirchhoﬀmigrated image for the Marmousi test case. Correlation with
true image = 0.4371.
x [m]
z [m]
0
1000
2000
3000
4000
5000
6000
7000
8000
9000
0
500
1000
1500
2000
2500
Figure 4-14: Unregularized LSM image (each βij = 0) for the Marmousi test case.
Correlation with true image = 0.3682.
115

x [m]
z [m]
0
1000
2000
3000
4000
5000
6000
7000
8000
9000
0
500
1000
1500
2000
2500
Figure 4-15: Uniformly regularized LSM image (each βij = 1) for the Marmousi test
case. Correlation with true image = 0.6369.
x [m]
z [m]
0
1000
2000
3000
4000
5000
6000
7000
8000
9000
0
500
1000
1500
2000
2500
Figure 4-16: Empirical Bayesian MAP image (computed after estimating β) for the
Marmousi test case. Correlation with true image = 0.7973.
0
500
1000
1500
2000
2500
0
1000
2000
3000
4000
5000
6000
7000
8000
9000
 
x [m]
 
z [m]
0
0.2
0.4
0.6
0.8
1
Figure 4-17: Edge strengths β estimated with E-M algorithm for the Marmousi test
case.
116

conditional distribution for the data given the model p(d|m)) would change if we
were solving a diﬀerent problem, but the methodology and algorithm described in
this chapter would still apply.
While we have developed our algorithm in the setting of solving a linear inverse
problem, an interesting direction for future work is to generalize this methodology to
non-linear inverse problems. A second direction for future work is to explore alter-
native ways to parameterize the prior on the image within the hierarchical Bayesian
setting. We say more about these two future directions in Chapter 7. Additionally,
with the parameterization of the prior presented in this chapter, one may wish to
explore inferring other parameters than just the edge strengths. For example, rigor-
ously picking the regularization parameter λ remains an open question in the ﬁeld of
inverse problems. Another natural future direction is application of this methodology
to a more realistic synthetic dataset (or to a ﬁeld dataset), where we expect similar
improvements in quality of the resulting image. The latter two directions are explored
in the following chapter.
117

118

Chapter 5
Interpretation and Estimation of
Regularization Parameters
In this chapter, we undertake a more rigorous investigation into the regularization
parameters used in Chapter 4. In the ﬁrst part of this chapter, we describe how these
parameters characterize the prior covariance of the model by exploring the connections
between these parameters and the covariance function that arises in the limiting case
when the model is treated as a random function. In the second part of this chapter,
we generalize the methodology of Chapter 4 to estimate these parameters within the
hierarchical Bayesian framework.
Recall the probabilistic model for the edge strengths β, image m, and data d
described in Chapter 4 (where image m is deﬁned on the graph G = (V, E)) :
β ∼Uniform([0, 1]|E|),
(5.1)
m | β ∼N (0, C(β)),
(5.2)
d | m, β ∼N (Am, Σ),
(5.3)
where the prior precision matrix Q(β) = C−1(β) for m | β is given by
Q(β) = C−1(β) = λ(D(β) + ǫI),
(5.4)
119

and where λ > 0, ǫ > 0, and D(β) is the β-weighted graph Laplacian on G deﬁned
by the quadratic form
mTD(β)m =
X
(i,j)∈E
βij(mi −mj)2.
(5.5)
5.1
Connecting Regularization Parameters and Co-
variance Functions
In order to better understand the signiﬁcance of the parameters λ, β, and ǫ, we
investigate the connections between these parameters and the resulting prior model
covariance. In particular, we examine the impact of these parameters in the continu-
ous case, where the model m(x) is now treated as a zero-mean Gaussian random ﬁeld
with model covariance speciﬁed by a covariance function C(x, x′) such that
E[m(x)m(x′)] = C(x, x′).
(5.6)
Following the development in Rodi and Myers [59] and Simpson et al. [68], we recog-
nize that the covariance function C(x, x′) can be taken as the Green’s function of a
diﬀerential operator L, so that, subject to appropriate boundary conditions, we have
LC(x, x′) = δ(x −x′),
(5.7)
where C(x, x′) will be a covariance function when L is self-adjoint and positive-deﬁnite
[59].
To connect our ﬁnite-dimensional model of Equation (5.2) to this Gaussian
random ﬁeld setting, we note that the covariance matrix C is a discrete approximation
to the covariance function C(x, x′) when the precision matrix Q = C−1 is a ﬁnite-
diﬀerence approximation to the diﬀerential operator L [59]. Hence, we want to choose
the diﬀerential operator L for which Q(β) is a ﬁnite-diﬀerence approximation, i.e. we
120

want L such that
Z
V
m(x)Lm(x) dV (x) ≈mT Qm
(5.8)
= λ

X
(i,j)∈E
β(m(xi) −m(xj))2 + ǫ
X
i∈V
m(xi)2

,
(5.9)
where V denotes the model domain and dV (x) is the volume measure at x and where
we have considered the special case of Q where the βij are all set to a common value
βij = β > 0, so that the underlying model covariance will be stationary far from the
boundaries. For the 2-D problem we are considering here, we take V to be a box in
R2 and dV (x) = dx dz. Letting ℓdenote the spacing of the uniform grid on which
the ﬁnite-dimensional model m is deﬁned (so a grid cell has area ℓ2), we consider the
following candidate for L:
L = λβ
 ǫ
βℓ2 −∆

,
(5.10)
where ∆denotes the Laplacian operator in 2-D (∆=
∂2
∂x2 + ∂2
∂z2) and use the Dirichlet
boundary condition m(x) = 0 on the boundary ∂V . Then, we have:
Z
V
m(x)Lm(x) dV = λβ
 ǫ
βℓ2
Z
V
m2 dV −
Z
V
m∆m dV

.
(5.11)
Now noting that
∇· (m∇m) = ∥∇m∥2
2 + m∆m,
(5.12)
we can rewrite (5.11) as
Z
V
m(x)Lm(x) dV = λβ
 ǫ
βℓ2
Z
V
m2 dV +
Z
V
∥∇m∥2
2 dV −
Z
V
∇· (m∇m) dV

.
(5.13)
Applying the divergence theorem to the last integral in (5.13),
Z
V
∇· (m∇m) dV =
Z
∂V
(m∇m) · n dS = 0,
(5.14)
121

where the last equality holds due to the boundary condition, and (5.13) becomes
Z
V
m(x)Lm(x) dV = λβ
 ǫ
βℓ2
Z
V
m2 dV +
Z
V
∥∇m∥2
2 dV

.
(5.15)
From (5.15), we see that L is indeed positive-deﬁnite:
R
V m(x)Lm(x) dV > 0 for all
m ̸= 0 satisfying the boundary condition, and it can similarly be shown that L is
self-adjoint. Now we proceed to approximate the integrals and derivatives in (5.15)
with summations and diﬀerences:
Z
V
m(x)Lm(x) dV = λβ
 ǫ
βℓ2
Z
V
m2 dV +
Z
V
∥∇m∥2
2 dV

(5.16)
≈λβ
 
ǫ
βℓ2
Nx
X
i=1
Nz
X
j=1
m2 ((xi, zj)) ℓ2
+
Nz
X
i=1
Nx−1
X
j=1
(m ((xj, zi)) −m ((xj+1, zi)))2
ℓ2
ℓ2
+
Nx
X
i=1
Nz−1
X
j=1
(m ((xi, zj)) −m ((xi, zj+1)))2
ℓ2
ℓ2
!
(5.17)
= λ

ǫ
X
i∈V
m2(xi) +
X
(i,j)∈E
β(m(xi) −m(xj))2


(5.18)
= mT Qm.
(5.19)
Hence our candidate L from Equation (5.10) can be viewed as a continuous extension
of the diﬀerencing matrix Q.
We can now proceed to solve Equation 5.7 to obtain C(x, x′) as the Green’s
function of L. To facilitate this, we take the Green’s function of L in the whole space,
which results in a stationary covariance function C(x, x′) = C(x −x′). Making a
change of variables y = x −x′ = (yx, yz), and deﬁning for notational convenience
ρ =
1
λβ and κ2 =
ǫ
βℓ2 we have
1
ρ(κ2 −∆)C(y) = δ(y).
(5.20)
122

Taking the 2-D spatial Fourier transform of both sides, this becomes
1
ρ(κ2 + k2
x + k2
z) ˆC(k) = 1,
(5.21)
where k = (kx, kz) is the wavenumber (or spatial frequency) and ˆC(k) is the Fourier
transform of C(y), i.e. ˆC(k) is the power spectral density of m(x). Rearranging and
applying the inverse Fourier transform we have
C(y) =
ρ
(2π)2
ZZ
R2
1
κ2 + k2
x + k2
z
eik·y dkx dkz.
(5.22)
Applying a change of variables to polar coordinates
kx = kr cos θ
yx = yr cos φ
kz = kr sin θ
yz = yr sin φ
we have
C(y) =
ρ
(2π)2
Z ∞
0
1
κ2 + k2r
Z π
−π
eikryr(cos θ cos φ+sin θ sin φ) dθ kr dkr
(5.23)
= ρ
2π
Z ∞
0
1
κ2 + k2r
1
2π
Z π
−π
eikryr cos(θ−φ) dθ kr dkr.
(5.24)
Making a ﬁnal change of variables ψ = θ −φ −π/2, we recognize the inner integral
as a Bessel function:
1
2π
Z π
−π
eikryr cos(θ−φ) dθ = 1
2π
Z π
−π
e−ikryr sin ψ dψ
(5.25)
= J0(yrkr),
(5.26)
where J0 denotes the zeroth-order Bessel function of the ﬁrst kind.
Then (5.24)
123

becomes
C(y) = ρ
2π
Z ∞
0
1
κ2 + k2
r
J0(yrkr)kr dkr
(5.27)
= ρ
2πK0(κyr),
(5.28)
where we have recognized the integral in (5.27) as the zeroth-order Hankel transform
of
1
κ2+k2r and K0 denotes the zeroth-order modiﬁed Bessel function of the second kind.
Reverting to the initial choice of variables, we then have for the covariance function
C(x, x′) =
1
2πλβK0
 
∥x −x′∥2
ℓ
p
β/ǫ
!
.
(5.29)
This covariance function will be a valid approximation of the model covariance in the
ﬁnite-dimensional setting when considering points far from the boundary of the grid
(to avoid boundary eﬀects, since the Green’s function is taken in the whole-space)
and when x ̸= x′, since K0(t) →∞as t →0.
From (5.29), we observe that the correlation length ξ, deﬁned here as the param-
eter governing the rate of decay of the covariance function, is determined by
ξ = ℓ
p
β/ǫ.
(5.30)
This can also be observed from the power spectral density of m(x) in (5.21): the
modulus of the wavenumber at which the magnitude of the power spectrum drops
to half of its peak value ( ˆC(0)) should be proportional to 1/ξ. We observe this at
∥k∥2 = κ =
p
ǫ/βℓ2 :
ˆC(k)

∥k∥2=κ = 1
2
ˆC(0).
(5.31)
Although the continuous model has inﬁnite variance (since C(x, x′) →∞as x →x′),
we can still gain insights about the (ﬁnite) variance of the discretized model at a
given grid cell from the power spectrum of the continuous model. From (5.21), we
124

can see the value of ˆC(0), the power spectrum at D.C., which is
ˆC(0) = ρ
κ2 = ℓ2
ǫλ.
(5.32)
We note that in the limit as β →0, the correlation length ξ →0 and the power
spectrum becomes ﬂat, corresponding to white noise with power spectrum
ℓ2
ǫλ. In
this case, the variance var(mi) of the corresponding discretized model at a grid cell
(of length ℓ) would then be given by var(mi) =
1
ℓ2 ˆC(0) =
1
ǫλ. From (5.29), we also
see that the parameter λ scales only the inverse variance (whereas β and ǫ play a
role in determining both the variance and correlation length). Hence, in determining
an appropriate choice for these parameters, one may choose to ﬁrst set β and ǫ to
achieve a desired correlation length, then pick λ to appropriately scale the model
variance var(mi). The exact value of the model variance var(mi) can be determined
numerically by computing (Q−1)ii.
As a numerical veriﬁcation of the validity of (5.29), in Figure 5-1, we compare
the covariances given by the covariance function for the continuous model m(x) to
the numerically computed covariances of the discretized model m on a 101-by-101
node grid of grid cell length ℓ= 1 m. We set the parameters λ = 1, ǫ = 10−3, and
varied β between 10−3 and 1. We see strong agreement between the covariances for
the discrete and continuous cases when x ̸= x′ for all values of β plotted other than
β = 1. At β = 1, the correlation length of the covariance function is large enough that
the boundary eﬀects in the discrete case can no longer be neglected, and hence (5.29)
becomes a less accurate approximation of the covariance of the discretized model. At
x = x′, the covariance function goes to inﬁnity, but the variance of the discretized
model remains ﬁnite.
125

0
5
10
0
500
1000
β = 10−3
|x−x’| [m]
cov(m(x),m(x’))
 
 
Continuous
Discrete
0
10
20
0
50
100
β = 0.01
|x−x’| [m]
cov(m(x),m(x’))
 
 
Continuous
Discrete
0
10
20
30
40
50
0
5
10
15
β = 0.1
|x−x’| [m]
cov(m(x),m(x’))
 
 
Continuous
Discrete
0
10
20
30
40
50
0
2
4
6
β = 0.25
|x−x’| [m]
cov(m(x),m(x’))
 
 
Continuous
Discrete
0
10
20
30
40
50
0
1
2
3
β = 0.5
|x−x’| [m]
cov(m(x),m(x’))
 
 
Continuous
Discrete
0
10
20
30
40
50
0
0.5
1
1.5
β = 1
|x−x’| [m]
cov(m(x),m(x’))
 
 
Continuous
Discrete
Figure 5-1: Comparison of numerically computed covariances in the discrete case to
the covariance function in the continuous limit, with λ = 1, ǫ = 10−3, and for diﬀerent
values of β. Covariances for the discrete case were computed with the central node
on a 101-by-101 node grid having grid cell length ℓ= 1 m. (Note that the range of
the x-axis is reduced for small values of β to make the plots more visible.)
126

5.2
Variational Bayesian Estimation of Regulariza-
tion Parameters
Having described how the parameters λ, β, and ǫ characterize the prior model covari-
ance, we now return to the problem of estimating the regularization parameters in
a Bayesian inference setting. We note that there is some redundancy in our param-
eterization in regards to the eﬀect of the parameters on the prior model covariance.
In particular, the covariance function can be parameterized by two characteristic pa-
rameters, the correlation length ξ = ℓ
q
β
ǫ and the power spectrum at D.C. ˆC(0) = ℓ2
λǫ,
whereas our parameterization provides three parameters that can be tuned. In order
to remove this redundancy, and additionally recognizing the role ǫ plays in keeping
the prior precision matrix Q positive-deﬁnite, we keep ǫ ﬁxed while estimating the
remaining regularization parameters. From (5.30), we observe that ﬁxing ǫ (while re-
stricting β to [0, 1]) determines the maximum correlation length as ξmax =
ℓ
√ǫ, hence
we can set ǫ in accordance with a desired ξmax.
In the previous section, we set all the βij to a common value β in order to ob-
tain a stationary covariance function. However, we will remove this restriction when
attempting to estimate the βij, as in Chapter 4. In addition to estimating the param-
eters β and λ governing the prior distribution, we also estimate the inverse variance
of the noise, which we denote by ζ.
5.2.1
Hierarchical Bayesian Formulation
We slightly redeﬁne the probabilistic model of (5.1)-(5.3) to include λ and ζ as random
variables in the hierarchical Bayesian framework. We deﬁne the prior distribution for
the model m given the regularization parameters as before so that
p (m|β, λ) ∝λN/2 exp

−1
2λmT(D (β) + ǫI)m
	
|D (β) + ǫI|−1/2
,
(5.33)
127

where N is the number of image parameters and | · | denotes the matrix determinant.
And we again let the data d be deﬁned by
d = Am + n,
(5.34)
where A is the forward modeling operator (in our case we take A to be the Kirchhoﬀ
modeling operator, so that AT is the Kirchhoﬀmigration operator, as deﬁned in
Chapter 4) and n is additive noise which we model as white Gaussian noise with zero
mean and covariance matrix Σ = ζ−1I. Under these assumptions, the conditional
distribution for the data is given by
p(d|m, ζ, β, λ) = p(d|m, ζ) ∝ζ K/2 exp

−1
2ζ ∥d −Am∥2

,
(5.35)
where K is the number of data points.
Letting θ = (β, λ, ζ) denote the vector of regularization parameters (in which we
include the inverse noise variance), we model θ as a random vector with its own prior
distribution p(θ). Since λ and ζ scale the inverse model and noise variances, respec-
tively, we are able to introduce conjugate priors for these parameters; in particular,
we model these parameters as Gamma random variables a priori, as the conjugate
prior for the inverse variance parameter of a Gaussian is the Gamma distribution [24].
Hence, we have
p(λ) ∝λaλ−1e−bλλ
λ ≥0
(5.36)
and
p(ζ) ∝ζaζ−1e−bζζ
ζ ≥0,
(5.37)
where aλ, bλ, aζ, bζ > 0 are the shape and rate parameters of the Gamma distributions.
These are called conjugate priors because the conditional posterior distributions for
λ and ζ will remain Gamma distributions, only with updated shape and rate pa-
rameters. (We note that setting a = 1 and taking the limit as b →0 results in an
(improper) ﬂat prior on λ (or ζ) ≥0, so the Gamma priors are quite general.) Fur-
thermore, as before, we endow each βij with a uniform prior on the set [0, 1] and let
128






Figure 5-2: The directed graphical model capturing the Markov structure between
β, λ, ζ, m, and d.
The node for d is shaded to indicate that d is an observed
quantity that the posterior distribution for the regularization parameters and model
is conditioned upon.
129

λ, ζ, and the βij be mutually independent random variables. Combining the above
deﬁnes our prior distribution for θ. The directed graphical model in Figure 5-2 de-
picts the Markov structure between the regularization parameters, the model, and
data.
Having fully speciﬁed our probabilistic model, we can now apply Bayes’ rule
to obtain the joint posterior distribution for θ and m given the data d:
p (m, θ|d) ∝p (θ) p (m|θ) p (d|m, θ)
(5.38)
∝λaλ+N/2−1 ζaζ+K/2−1 |D (β) + ǫI|1/2
exp

−λ

bλ + 1
2mT (D (β) + ǫI)m

−ζ

bζ + 1
2 ∥d −Am∥2

.
(5.39)
5.2.2
Variational Bayesian Methods
The complete solution to the hierarchical Bayesian problem would involve obtaining
and tractably exploring, often via sampling techniques, the marginal posterior distri-
butions for the parameters of interest m. However, since the posterior distribution
(5.39) is costly to evaluate (due to the determinant factor), sampling from the poste-
rior quickly becomes infeasible for even moderately sized models (N ∼104). To avert
this problem, we turn to an approximate inference framework known as variational
Bayes (VB) [3, 4], which can be viewed as a generalization of the E-M algorithm used
in Chapter 4.
The idea behind the variational Bayesian method is to approximate an intractable
posterior distribution p(u|d) (deﬁned on the set of random variables u = (m, θ)) by
searching within a family of tractable distributions q ∈Q for the distribution, q∗,
which is closest to the posterior pu|d. The approximate posterior q∗is found as the
solution to the following variational problem (hence the name):
q∗= arg min
q∈Q
D(q∥pu|d),
(5.40)
where D(·∥·) is the KL divergence, a pseudo-metric on the space of probability dis-
130

tributions given by
D(q∥pu|d) = Eq

log
 q(u)
p(u|d)

,
(5.41)
and where Eq denotes the expectation operator with respect to the distribution q. As
hinted at earlier, the expectation-maximization algorithm [17, 46] can be derived as a
special case of VB when we restrict Q to the family of point distributions (i.e. Dirac
delta distributions) on some subset of the random variables in u [3].
The most common variant of VB is known as the mean ﬁeld approximation, in
which Q comprises distributions which factorize over speciﬁed partitions on the set of
random variables u. In our problem, it is natural to take m and θ as two partitions of
the unknowns u. Using this partitioning with the mean ﬁeld approximation, VB will
search within the family of distributions that, conditioned on d, have m independent
from θ (which is not the case in the true posterior distribution (5.39)). We further
specialize Q by restricting the class of distributions on β to point distributions (this
will result in eﬀectively estimating β via the E-M algorithm while estimating the
remaining parameters in the more general mean ﬁeld setting). We thus have
q(m, θ) = qm(m)qθ(θ)
(5.42)
= qm(m)qλ,ζ,β(λ, ζ, β)
(5.43)
= qm(m)qλ,ζ(λ, ζ)δ(β −¯β),
(5.44)
where ¯β is the point at which the delta distribution qβ(β) is centered (and hence the
only parameter deﬁning qβ(β)).
Coordinate-Descent
To solve the variational problem (5.40) for q ∈Q, we substitute the above factorized
form of q into the KL divergence and take a coordinate-descent approach, which
alternates between minimizing the KL divergence with respect to ¯β and (qm, qλ,ζ),
131

giving the iterative procedure:
¯β(t+1) = arg min
¯β
D(q(t)
m q(t)
λ,ζδ¯β∥pλ,ζ,β,m|d)
(5.45)

q(t+1)
m
, q(t+1)
λ,ζ

= arg min
(qm,qλ,ζ)
D(qmqλ,ζδ¯β(t+1)∥pλ,ζ,β,m|d).
(5.46)
Examining the update equation for ¯β(t+1) (5.45), and dropping terms that do not
depend on ¯β, we see that this equation is essentially the M-step of the E-M algorithm:
¯β(t+1) = arg min
¯β
D(q(t)
m q(t)
λ,ζδ¯β∥pλ,ζ,β,m|d)
(5.47)
= arg max
β
Eq(t)
m q(t)
λ,ζ[log p(λ, ζ, β, m, d)].
(5.48)
And indeed, we can perform the above maximization using the same methodology as
was used to solve the M-step in Chapter 4 via a gradient ascent method, where
∂
∂βij
Eq(t)
m q(t)
λ,ζ[log p(λ, ζ, β, m, d)] =
¯λ(t)
2

Cii(¯λ(t), β) + Cjj(¯λ(t), β) −2Cij(¯λ(t), β)
−(Λ(t)
ii + Λ(t)
jj −2Λ(t)
ij ) −(µ(t)
i
−µ(t)
j )2
,
(5.49)
and where ¯λ(t) = Eq(t)
λ [λ] is the expected value of λ under q(t)
λ , µ(t) = Eq(t)
m [m] and
Λ(t) = covq(t)
m (m) are the mean vector and covariance matrix of m under q(t)
m , and
C(¯λ(t), β) = covpm|¯λ(t),β(m) is the prior covariance matrix of m under pm|¯λ(t),β:
C(¯λ(t), β) =
 ¯λ(t)(D(β) + ǫI)
−1 .
(5.50)
Fixed-Point Updates for q(t)
λ,ζ and q(t)
m
The update equation (5.46) for the distributions q(t)
λ,ζ and q(t)
m still requires ﬁnding the
distributions that minimize the KL divergence (with ¯β now ﬁxed to ¯β(t+1)). One can
derive the stationarity conditions on q(t+1)
λ,ζ
and q(t+1)
m
by forming a Lagrangian L (to
account for normalization constraints) and setting the functional derivatives
δL
δqm(m)
132

and
δL
δqλ,ζ(λ,ζ) to 0. This gives the standard equations of the mean ﬁeld approximation:
log q(t+1)
λ,ζ
(λ, ζ) = Eq(t+1)
m
h
log p(λ, ζ, ¯β(t+1), m, d)
i
−Zλ,ζ,
(5.51)
log q(t+1)
m
(m) = Eq(t+1)
λ,ζ
h
log p(λ, ζ, ¯β(t+1), m, d)
i
−Zm,
(5.52)
where Zm and Zλ,ζ are normalization constants.
The cyclic dependence between
Equations (5.51) and (5.52) induces a natural ﬁxed-point algorithm in which we pick
an initial guess q(t+1,0)
λ,ζ
(λ, ζ) and q(t+1,0)
m
(m) and update these guesses by sequentially
solving (5.51) and (5.52), repeatedly until convergence. Denoting the iteration num-
ber of this ﬁxed-point algorithm by s, we have:
log q(t+1,s+1)
λ,ζ
(λ, ζ) = Eq(t+1,s)
m
h
log p(λ, ζ, ¯β(t+1), m, d)
i
−Zλ,ζ
(5.53)
log q(t+1,s+1)
m
(m) = Eq(t+1,s+1)
λ,ζ
h
log p(λ, ζ, ¯β(t+1), m, d)
i
−Zm.
(5.54)
In order to implement this ﬁxed-point algorithm, we substitute the joint posterior
distribution into (5.53) and (5.54). First substituting into (5.53) gives the form of
the update for q(t+1,s+1)
λ,ζ
(λ, ζ):
log q(t+1,s+1)
λ,ζ
(λ, ζ) = (aλ + N/2 −1) log λ + (aζ + K/2 −1) log ζ
−λ

bλ + 1
2Eq(t+1,s)
m
h
mT(D(¯β(t+1)) + ǫI)m
i
−ζ

bζ + 1
2Eq(t+1,s)
m

∥d −Am∥2
−Z′
λ,ζ
(5.55)
= (aλ + N/2 −1) log λ
−λ

bλ + 1
2 tr((D(¯β(t+1)) + ǫI)Λ(t+1,s))
+ 1
2µ(t+1,s)T(D(¯β(t+1)) + ǫI)µ(t+1,s)
−Z′′
λ
+ (aζ + K/2 −1) log ζ
−ζ
 bζ + 1
2 tr
 ATAΛ(t+1,s)
+ 1
2∥d −Aµ(t+1,s)∥2
−Z′′
ζ
(5.56)
= log q(t+1,s+1)
λ
(λ) + log q(t+1,s+1)
ζ
(ζ),
(5.57)
where we have recognized in the last equality that q(t+1,s+1)
λ,ζ
(λ, ζ) factorizes (i.e.
133

q(t+1,s+1)
λ,ζ
models λ and ζ as independent).
As before, µ
(t+1,s) and Λ(t+1,s) denote
the mean and covariance of m under q(t+1,s)
m
, and Z′
λ,ζ, Z′′
λ, Z′′
ζ are simply new nor-
malization constants. From the form of Equation (5.56), we recognize q(t+1,s+1)
λ
and
q(t+1,s+1)
ζ
as Gamma distributions, and hence we only need to keep track of their shape
and rate parameters, a(t+1,s+1)
λ
and b(t+1,s+1)
λ
for q(t+1,s+1)
λ
and a(t+1,s+1)
ζ
and b(t+1,s+1)
ζ
for q(t+1,s+1)
ζ
, which are given by:
a(t+1,s+1)
λ
= aλ + N/2,
(5.58)
b(t+1,s+1)
λ
= bλ + 1
2 tr((D(¯β(t+1)) + ǫI)Λ(t+1,s)) + 1
2µ(t+1,s)T(D(¯β(t+1)) + ǫI)µ(t+1,s),
(5.59)
a(t+1,s+1)
ζ
= aζ + K/2,
(5.60)
b(t+1,s+1)
ζ
= bζ + 1
2 tr
 ATAΛ(t+1,s)
+ 1
2∥d −Aµ(t+1,s)∥2.
(5.61)
From these, we can compute the expected values of λ and ζ under q(t+1,s+1)
λ,ζ
, denoted
by ¯λ(t+1,s+1) and ¯ζ(t+1,s+1) , as
¯λ(t+1,s+1) = a(t+1,s+1)
λ
b(t+1,s+1)
λ
(5.62)
=
aλ + N/2
bλ + 1
2 tr((D(¯β(t+1)) + ǫI)Λ(t+1,s)) + 1
2µ(t+1,s)T(D(¯β(t+1)) + ǫI)µ(t+1,s),
(5.63)
¯ζ(t+1,s+1) =
a(t+1,s+1)
ζ
b(t+1,s+1)
ζ
=
aζ + K/2
bζ + 1
2 tr (ATAΛ(t+1,s)) + 1
2∥d −Aµ(t+1,s)∥2.
(5.64)
We similarly update the distribution q(t+1,s+1)
m
by substituting the joint posterior
(5.39) into (5.54). This gives
log q(t+1,s+1)
m
(m) = −1
2mT 
¯ζ(t+1,s+1)ATA + ¯λ(t+1,s+1)(D(¯β(t+1)) + ǫI)

m
+ ¯ζ(t+1,s+1)mT ATd −Z′
m.
(5.65)
From (5.65), we recognize q(t+1,s+1)
m
as a Gaussian distribution, and, as before, we
134

need only keep track of the parameters deﬁning the distribution: its mean vector
µ(t+1,s+1) and covariance matrix Λ(t+1,s+1) which are given by
µ(t+1,s+1) =

¯ζ(t+1,s+1)ATA + ¯λ(t+1,s+1)(D(¯β(t+1)) + ǫI)
−1 ¯ζ(t+1,s+1)AT d
(5.66)
and
Λ(t+1,s+1) =

¯ζ(t+1,s+1)ATA + ¯λ(t+1,s)(D(¯β(t+1)) + ǫI)
−1
.
(5.67)
Formulating our probabilistic model with conjugate priors, the Gamma prior for λ
and ζ and the Gaussian prior for m, caused the conditional posterior distributions
for these parameters to remain Gamma and Gaussian, respectively. For this reason,
the variational Bayesian approximations also remain as Gamma and Gaussian distri-
butions and are hence tractable, as only the parameters deﬁning these distributions
need to be updated.
It is important to point out that we do not have to store or compute the entire
covariance matrix Λ(t,s) to update ¯β(t) , λ(t,s), and ζ(t,s). The partial derivatives in
Equation (5.49) used to update ¯β(t) only require elements of the covariance matrix
corresponding to the edges of the graph G, and these can be computed using the
techniques outlined in Chapter 4. Updating λ(t,s) and ζ(t,s) requires computing matrix
traces of the product of Λ(t,s) and another matrix. These traces can be computed
stochastically using a trace estimation algorithm due to Hutchinson [32]:
Algorithm 5.1 Hutchinson Trace Estimation Algorithm [32]
Let H be an N-by-N matrix. Specify number of samples M.
1. Draw M i.i.d. white noise vector samples: u(i) ∼Uniform({−1, 1}N).
2. Estimate tr(H) ≈
1
M
PM
i=1 u(i)THu(i).
It is straightforward to show that this estimate converges to the true matrix trace.
135

The estimator of tr(H) converges to the true mean of uTHu which is
E[uTHu] = E[tr(uTHu)]
(5.68)
= tr(HE[uuT])
(5.69)
= tr(H),
(5.70)
since E[uuT] = I. Indeed, this algorithm will work with any zero-mean, unit-variance
white noise vector u (such as, e.g. u ∼N (0, I)). It is important to note that we
neither need to compute nor store the matrix H explicitly to estimate its trace with
this algorithm; we only need to be able to apply H to a vector. For our case, H is
the product of Λ and another matrix (which we generically denote by R), where Λ is
the inverse of a posterior precision matrix. Then the action of H = RΛ on a vector u
can be computed by ﬁrst solving the system Λ−1v = u for v, then taking RΛu = Rv.
Summary of VB algorithm
To summarize our approach, we give our variational Bayesian algorithm for jointly
estimating model and regularization parameters below:
136

Algorithm 5.2 Variational Bayesian algorithm for estimating β∗and approximate
marginal posteriors q∗
λ, q∗
ζ, q∗
m
Initialize ¯β(0),¯λ(0,0),¯ζ(0,0).
Set t = 0. Iterate on t:
1. Set s = 0. Iterate on s:
a. Compute µ(t,s) via (5.66).
b. Compute matrix traces in (5.63) and (5.64) via Hutchinson algorithm
with Λ(t,s) as deﬁned in (5.67).
c. Compute ¯λ(t,s+1) via (5.63).
d. Compute ¯ζ(t,s+1) via (5.64).
2. Update ¯λ(t+1) = ¯λ(t,s+1), ¯ζ(t+1) = ¯ζ(t,s+1), µ(t+1) = µ(t,s).
3. Update Λ(t+1) to be deﬁned as Λ(t,s) in (5.67).
4. Update ¯β(t+1) via the M-step of the E-M algorithm of Chapter 4
(i.e. Steps 3-5 of Alg. 4.2 using ¯λ(t+1), ¯ζ(t+1), µ(t+1), Λ(t+1)).
Upon termination, return:
¯λ∗= ¯λ(t+1), ¯ζ∗= ¯ζ(t+1),
a∗
λ = aλ + N/2, b∗
λ = aλ+N/2
¯λ∗
, a∗
ζ = aζ + K/2, b∗
ζ = aζ+K/2
¯ζ∗
,
β∗= ¯β(t+1), µ∗= µ(t+1), Λ∗= Λ(t+1),
q∗
λ = Gamma(a∗
λ, b∗
λ), q∗
ζ = Gamma(a∗
ζ, b∗
ζ), q∗
m = N (µ∗, Λ∗).
5.2.3
Results
To obtain a single estimate of the image, we take the MAP estimate of the image from
its approximate marginal posterior distribution q∗
m, which we refer to as its variational
Bayes MAP (or VB-MAP) estimate. We observe from (5.65) that q∗
m(m) happens to
be equal to the conditional posterior p(m|d, β∗, ¯λ∗, ¯ζ∗) using the β∗, ¯λ∗, ¯ζ∗obtained
from the VB algorithm. Hence the VB-MAP estimate of the image is equivalent to
its empirical Bayes MAP estimate discussed in Chapter 4 when using the parameters
β∗, ¯λ∗, ¯ζ∗.
137

We validate our approach by applying our inference algorithm to synthetic datasets
arising from 2-D reﬂectivity models. We consider data simulated from two diﬀerent
physical models: Kirchhoﬀmodeling and acoustic wave-equation modeling. As in
Chapter 4, the purpose of using Kirchhoﬀmodeled synthetic data is to test the
performance of our algorithm when used with a consistent forward model. The wave-
equation modeled data, on the other hand, additionally tests the ability of the Kirch-
hoﬀforward operator used in our algorithm to correctly model acoustic data. For
both cases, we deﬁned the MRF so that each node shares an edge with all other nodes
within a radius of
√
2 nodes (to include diagonal connections in the MRF).
Kirchhoﬀmodeled data
As in the previous chapter, the Kirchhoﬀdata example is simulated from the Mar-
mousi model.
The data are acquired from a set of 20 collocated surface sources
and receivers (resulting in 400 traces), with a 480 m spacing between receivers (and
sources), where the source wavelet is a 25 Hz Ricker wavelet. As before, we add zero-
mean white Gaussian noise to the data (with a standard deviation equal to 10% of the
maximum amplitude of the data). Note that this inverse crime test example is the
same as that in Chapter 4. In this example, the VB estimates of the marginal posteri-
ors were obtained with 3 (outer) t-iterations (to update β) and 10 (inner) s-iterations
(to update qλ,qζ,qm) of the VB algorithm (see Algorithm 5.2), where each outer t-
iteration of VB took approximately 3 hours on a quad core IntelTM Xeon W3550
3.0GHz processor (hence the entire VB algorithm ran in approximately 9 hours). We
note that each t-iteration of the VB algorithm performs all 10 s-iterations, where each
s-iteration requires performing multiple LSM inversions to both compute the mean
model parameters for updating qm and to estimate the traces required to update qλ
and qζ via the Hutchinson algorithm. As in the previous chapter, the LSM inver-
sions are computed using the method of conjugate gradients (CG), which is run for
200 iterations per LSM. Each iteration of CG requires performing a single Kirchhoﬀ
modeling followed by a single Kirchhoﬀmigration, and, for this example, takes about
0.7 seconds per CG iteration (on the same machine).
138

The imaging results are shown in Figures 5-3–5-10, where the edge strengths
obtained using our algorithm are shown in Figure 5-10 and the VB-MAP estimate of
the image is shown in Figure 5-9. For comparison, we show the unregularized LSM
image (Figure 5-4) along with the regularized LSM images obtained using a uniform
regularization scheme (setting each βij = 1) and diﬀerent values for λ (ﬁxing ζ to ¯ζ∗)
(Figures 5-5–5-8).
As before, the unregularized image is heavily inﬂuenced by the noise in the data to
the point that the reﬂectors in the image are largely obscured by the noise. The eﬀects
of noise are largely diminished in the uniformly regularized LSM images obtained
with setting λ to ¯λ∗or higher (Figures 5-7–5-8), but these same images signiﬁcantly
smooth out the reﬂectors. This is consistent with our expectations: ¯λ∗is estimated in
conjunction with the β∗
ij, which are always between 0 and 1. Hence, using this ¯λ∗to
regularize while setting all the βij to 1, should result in over-smoothing. Using values
of λ smaller than ¯λ∗(with βij = 1) prevents over-smoothing, but fails to signiﬁcantly
remove the eﬀects of noise (Figures 5-5–5-6). The VB-MAP image (Figure 5-9), by
contrast, remains sharp near the reﬂectors while smoothing out the noise away from
the reﬂectors. For a quantitative comparison, we note that the correlation of the
VB-MAP image with the true image is signiﬁcantly higher than the correlations of
the other images with the true image.
Figure 5-11 shows the VB approximations to the posterior distributions for λ and
ζ. We can interpret these approximate posteriors in terms of the quantities λ and
ζ represent. We ﬁnd that the approximate posterior q∗
ζ(ζ) slightly overestimates the
true inverse noise variance (which is ζ = 5.61·108 Pa−2). This is somewhat expected,
since the VB-MAP model may provide a better ﬁt to the data than the true model
(and hence give a lower estimated noise variance). Indeed, the inverse variance of the
data residual resulting from the VB-MAP model is 5.81 · 108 Pa−2, which is much
closer to ¯ζ∗= 5.79 · 108 Pa−2 (the mean of q∗
ζ). As discussed in the beginning of this
chapter, λ scales the prior model variance. Hence, to interpret the value we obtain
for ¯λ∗(the mean of q∗
λ), we can measure the empirical variance of the true model
and compare this to the average prior model variance predicted by ¯λ∗(which can be
139

x [m]
z [m]
0
1000
2000
3000
4000
5000
6000
7000
8000
9000
0
500
1000
1500
2000
2500
Figure 5-3: The true Marmousi reﬂectivity model.
x [m]
z [m]
0
1000
2000
3000
4000
5000
6000
7000
8000
9000
0
500
1000
1500
2000
2500
Figure 5-4: Unregularized LSM image (each βij = 0) using Kirchhoﬀmodeled data.
Correlation with true image = 0.3682.
x [m]
z [m]
0
1000
2000
3000
4000
5000
6000
7000
8000
9000
0
500
1000
1500
2000
2500
Figure 5-5: Uniformly regularized LSM image (each βij = 1) with λ = 0.01¯λ∗and
ζ = ¯ζ∗using Kirchhoﬀmodeled data. Correlation with true image = 0.4364.
140

x [m]
z [m]
0
1000
2000
3000
4000
5000
6000
7000
8000
9000
0
500
1000
1500
2000
2500
Figure 5-6: Uniformly regularized LSM image (each βij = 1) with λ = 0.1¯λ∗and
ζ = ¯ζ∗using Kirchhoﬀmodeled data. Correlation with true image = 0.6282.
x [m]
z [m]
0
1000
2000
3000
4000
5000
6000
7000
8000
9000
0
500
1000
1500
2000
2500
Figure 5-7: Uniformly regularized LSM image (each βij = 1) with λ = ¯λ∗and ζ = ¯ζ∗
using Kirchhoﬀmodeled data. Correlation with true image = 0.6432.
x [m]
z [m]
0
1000
2000
3000
4000
5000
6000
7000
8000
9000
0
500
1000
1500
2000
2500
Figure 5-8: Uniformly regularized LSM image (each βij = 1) with λ = 10¯λ∗and
ζ = ¯ζ∗using Kirchhoﬀmodeled data. Correlation with true image = 0.4870.
141

x [m]
z [m]
0
1000
2000
3000
4000
5000
6000
7000
8000
9000
0
500
1000
1500
2000
2500
Figure 5-9: Variational Bayes MAP image using Kirchhoﬀmodeled data. Correlation
with true image = 0.8218.
0
500
1000
1500
2000
2500
0
1000
2000
3000
4000
5000
6000
7000
8000
9000
 
x [m]
 
z [m]
0
0.2
0.4
0.6
0.8
1
Figure 5-10: Edge strengths β∗estimated with VB using Kirchhoﬀmodeled data.
142

computed from the trace of the prior model covariance matrix deﬁned by ¯λ∗and β∗).
We ﬁnd that while ¯λ∗= 133.14 (note that the reﬂectivity model is dimensionless,
and hence so is λ), which yields an average prior model variance of 5.90 · 10−3, the
empirical variance of the true model is computed to be 5.66 · 10−3. Hence, the value
of ¯λ∗computed by VB slightly overestimates the true model variance.
Acoustic wave-equation modeled data
Since the wave-equation modeled data arises from a more complex physical model
than the Kirchhoﬀmodeling operator used by our algorithm, we ﬁrst examine the
behavior of our algorithm on the example of a simple three layer model before moving
on to the more complex Marmousi example.
For both examples, the synthetic data are acquired from a set of collocated surface
sources and receivers, with an even spacing of 240 m, where the source wavelet is a
25 Hz Ricker wavelet. We note that, due to the complex nature of this data, we used
smaller source and receiver spacings than what was used for the Kirchhoﬀmodeled
data (where for that data the source and receiver spacing was 480 m). Even still,
this receiver spacing is far larger (and the dataset far sparser) than what is typically
used in Kirchhoﬀmigration applications; hence the choice of regularization becomes
extremely signiﬁcant in this case. We did not add random noise to the data, since the
residual between the predicted data (by Kirchhoﬀmodeling) and the wave-equation
data already serves as a signiﬁcant source of noise.
For these examples, the VB
estimates of the marginal posteriors were again obtained with 3 (outer) t-iterations
(to update β) and 10 (inner) s-iterations (to update qλ,qζ,qm) of the VB algorithm (see
Algorithm 5.2), where each outer t-iteration of VB took approximately 12 hours on
a quad core IntelTM Xeon W3550 3.0GHz processor (hence the entire VB algorithm
ran in approximately 36 hours).
Again, the LSM inversions are computed using
the method of conjugate gradients (CG), which is run for 200 iterations per LSM.
Due to the increased number of sources and receivers, each iteration of CG costs
approximately 2.8 seconds (on the same machine).
143

120
130
140
150
0
0.1
0.2
0.3
0.4
0.5
λ [reflectivity−2]
q∗
λ(λ)
5.7
5.75
5.8
5.85
5.9
x 10
8
0
1
2
3
4
5x 10
−7
ζ [Pa−2]
q∗
ζ(ζ)
(a) q∗
λ(λ)
(b) q∗
ζ(ζ)
Figure 5-11: The variational Bayesian approximations to the posterior distributions
for the parameters scaling the inverse variances of the (a) model λ and (b) noise ζ,
using Kirchhoﬀmodeled data.
144

Three layer example
The results for the three layer example are shown in Figures 5-12–5-22. The true
reﬂectivity model is depicted in Figure 5-12, and again we show the unregularized
LSM image (Figure 5-13) along with the uniformly regularized images for diﬀerent
values of λ (Figures 5-14–5-17). The VB-MAP estimate of the image is shown in
Figure 5-18, and the corresponding edge strengths β∗are shown in Figure 5-19. As
expected, the unregularized image contains a heavy amount of noise due to both the
limited acquisition and poor modeling eﬀects. The regularized LSM images (obtained
with βij = 1) signiﬁcantly improve upon this result. We ﬁnd that while a signiﬁcant
amount of noise remains in the image obtained with λ = ¯λ∗, the noise is greatly
reduced when a value of λ = 10¯λ∗is used in the regularization (Figure 5-18). This is
a somewhat surprising result because, although ¯λ∗was estimated in conjunction with
β∗, we would expect that, since β∗
ij is always between 0 and 1, regularizing with ¯λ∗
and ¯ζ∗and setting all the βij to 1 will result in over-smoothing the image. Indeed,
for the Kirchhoﬀmodeled data example, this choice of parameters did result in over-
smoothing in the regularized LSM image (Figure 5-7). Hence, from a purely “results-
oriented” perspective, one would expect a higher value of ¯λ∗than that obtained.
This is also seen in the VB-MAP image (Figure 5-18), where the noise has only
been slightly reduced from the unregularized LSM image, once again suggesting that
the value of ¯λ∗is too low. The estimated edge strengths (Figure 5-19), however, are
aligned with our expectations. We see that the edge strengths go to 0 above and
below the reﬂectors, while mostly remaining near 1 elsewhere in the image (however,
we do also see the eﬀects of the noise on the edge strengths). Hence, using these edge
strengths along with a higher value for λ should hopefully give a better image. As
noted earlier, the resulting image is the empirical Bayes MAP estimate of the image
with λ ﬁxed to a higher value. We experiment with successively higher values of λ
(ﬁxing β = β∗and ζ = ¯ζ∗) in Figures 5-20, 5-21, and 5-22. Indeed, we see that as
λ is made larger, the eﬀects of the noise are greatly reduced, but the edge strengths
β∗preserve the sharpness at the reﬂectors. At λ = 103¯λ∗(Figure 5-22), the greatest
improvement is seen, and the correlation is highest for this image.
145

The VB approximations to the posterior marginals for λ and ζ are shown in
Figure 5-23.
Here we ﬁnd that VB estimates the inverse noise variance at ¯ζ∗=
2.15 · 1011 Pa−2, whereas the inverse variance of the data residuals found with the
true model and MAP-VB model are ζ = 1.14 · 1011 Pa−2 and ζ = 2.16 · 1011 Pa−2,
respectively. Hence, as expected, the estimated inverse noise variance matches well
with the inverse variance of the data residual computed with the VB-MAP model
(but the “true” inverse noise variance has been signiﬁcantly overestimated). The VB
estimate of λ was found to be ¯λ∗= 4.05 · 103, which results in an average prior
model variance of 7.66 · 10−4 (again, found by computing the trace of the prior model
covariance matrix given by ¯λ∗and β∗). By comparison, the empirical variance of
the true model is 2.64 · 10−4.
Hence the true model variance is, as our imaging
results seemed to indicate, signiﬁcantly overestimated (meaning λ is signiﬁcantly
underestimated).
Marmousi model example
The second example we consider is the Marmousi model, where the wave-equation
data are generated using the same acquisition geometry as in the three layer ex-
ample. The complex velocity structure of the Marmousi model makes this exam-
ple particularly challenging for Kirchhoﬀ-based methods (including LSM), and often
a wave-equation based imaging method, such as reverse-time migration (RTM) or
RTM-based LSM, is required to correctly image deeper sections of the model [25].
We do not expect that the hierarchical Bayesian version of Kirchhoﬀ-based LSM will
be able to remedy this problem, since this is, at heart, a modeling issue and not an in-
version issue. Nevertheless, the reﬂectors in the shallow part of the model can still be
described by Kirchhoﬀmodeling, and it is of interest to determine the improvements
we might be able to gain with our methodology.
The results for the Marmousi example are shown in Figures 5-24–5-33. The un-
regularized LSM image is shown in Figure 5-24, and the uniformly regularized LSM
images for diﬀerent values of λ are displayed in Figures 5-25–5-28. Once again, we
observe a similar issue with the value estimated for λ∗. The noisy unregularized LSM
146

x [m]
z [m]
0
500
1000
0
500
1000
1500
2000
2500
Figure 5-12: The true reﬂectivity for the three layer model.
147

x [m]
z [m]
0
500
1000
0
500
1000
1500
2000
2500
Figure 5-13: Unregularized LSM image (each βij = 0) using wave-equation modeled
data (three layer model). Correlation with true image = 0.0824.
148

x [m]
z [m]
0
500
1000
0
500
1000
1500
2000
2500
Figure 5-14: Uniformly regularized LSM image (each βij = 1) with λ = ¯λ∗and ζ = ¯ζ∗
using wave-equation modeled data (three layer model). Correlation with true image
= 0.7226.
149

x [m]
z [m]
0
500
1000
0
500
1000
1500
2000
2500
Figure 5-15: Uniformly regularized LSM image (each βij = 1) with λ = 10¯λ∗and
ζ = ¯ζ∗using wave-equation modeled data (three layer model). Correlation with true
image = 0.8309.
150

x [m]
z [m]
0
500
1000
0
500
1000
1500
2000
2500
Figure 5-16: Uniformly regularized LSM image (each βij = 1) with λ = 102¯λ∗and
ζ = ¯ζ∗using wave-equation modeled data (three layer model). Correlation with true
image = 0.7381.
151

x [m]
z [m]
0
500
1000
0
500
1000
1500
2000
2500
Figure 5-17: Uniformly regularized LSM image (each βij = 1) with λ = 103¯λ∗and
ζ = ¯ζ∗using wave-equation modeled data (three layer model). Correlation with true
image = 0.4922.
152

x [m]
z [m]
0
500
1000
0
500
1000
1500
2000
2500
Figure 5-18: Variational Bayes MAP image using wave-equation modeled data (three
layer model). Correlation with true image = 0.2291.
153

0
500
1000
1500
2000
2500
0
500
1000
 
x [m]
 
z [m]
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Figure 5-19: Edge strengths β∗estimated with VB using wave-equation modeled data
(three layer model).
154

x [m]
z [m]
0
500
1000
0
500
1000
1500
2000
2500
Figure 5-20: Empirical Bayes MAP image obtained with β = β∗, λ = 10¯λ∗, and
ζ = ¯ζ∗, using wave-equation modeled data (three layer model). Correlation with true
image = 0.7472.
155

x [m]
z [m]
0
500
1000
0
500
1000
1500
2000
2500
Figure 5-21: Empirical Bayes MAP image obtained with β = β∗, λ = 102¯λ∗, and
ζ = ¯ζ∗, using wave-equation modeled data (three layer model). Correlation with true
image = 0.8724.
156

x [m]
z [m]
0
500
1000
0
500
1000
1500
2000
2500
Figure 5-22: Empirical Bayes MAP image obtained with β = β∗, λ = 103¯λ∗, and
ζ = ¯ζ∗, using wave-equation modeled data (three layer model). Correlation with true
image = 0.9349.
157

3600
3800
4000
4200
4400
4600
0
2
4
6x 10
−3
λ [reflectivity−2]
q∗
λ(λ)
2.12
2.14
2.16
2.18
x 10
11
0
0.5
1
1.5x 10
−9
ζ [Pa−2]
q∗
ζ(ζ)
(a) q∗
λ(λ)
(b) q∗
ζ(ζ)
Figure 5-23: The variational Bayesian approximations to the posterior distributions
for the parameters scaling the inverse variances of the (a) model λ and (b) noise ζ,
using wave-equation modeled data (three layer model).
158

image is improved upon in the regularized LSM images (with βij = 1), but the image
obtained with λ = ¯λ∗(Figure 5-25) still contains a signiﬁcant noise component. Set-
ting λ to 102¯λ∗or higher largely removes the eﬀects of the noise (Figures 5-27–5-28),
yet at the cost of over-smoothing the reﬂectors. Again, our expectation is that setting
λ = ¯λ∗should suﬃciently remove the noise from the image, and we would thus expect
a higher value for ¯λ∗.
This issue can again be seen in the VB-MAP image of Figure 5-29: indeed, it
is diﬃcult to observe any qualitative diﬀerence between the VB-MAP image and
the unregularized LSM image of Figure 5-24.
This again suggests that the value
for ¯λ∗is too low. The estimated edge strengths β∗are shown in Figure 5-30. We
see that, for those reﬂectors that appeared in the LSM images (i.e. those reﬂectors
that the data were informative about from the perspective of the Kirchhoﬀmodeling
operator), the edge strengths correctly go to 0 near the reﬂectors; elsewhere in the
model the edge strengths are high, although the eﬀect of the noise can also be seen in
the edge strengths (particularly in the deeper part of the model). As with the three
layer example, this suggests that using these edge strengths with a higher value for
λ should yield a better image. We compute these images in Figures 5-31–5-33 using
increasingly higher values of λ. When λ is set to 102¯λ∗and 103¯λ∗we begin to see
some qualitative improvements in the image: the noise is smoothed out, yet some
sharpness is preserved in the reﬂectors captured by β∗. We note that the correlations
of the images with the true model, for all the images obtained in this example, are
very low (< 0.1). This is because, no matter what kind of regularization scheme is
used, the images obtained only correctly estimate a small portion of the reﬂectors,
since much of the acoustic data is not adequately described by the Kirchhoﬀoperator.
As such, even though we notice that the correlations do, for the most part, increase
when we see qualitative improvements in the image, they are less informative for this
example.
The approximate posterior marginals q∗
λ and q∗
ζ are shown in Figure 5-34. Here,
VB estimates the inverse noise variance at ¯ζ∗= 1.254·1010 Pa−2, whereas the inverse
variance of the data residuals found with the true model and MAP-VB model are
159

ζ = 9.858 · 108 Pa−2 and ζ = 1.269 · 1010 Pa−2, respectively. Hence, the estimated
inverse noise variance greatly overestimates the “true” inverse noise variance (i.e. the
inverse variance of the data residual with the true model) and, again, is far more
consistent with the inverse variance of the residual the VB-MAP model. The VB
estimate of λ was found to be ¯λ∗= 150.97, which results in an average prior model
variance of 1.21 · 10−2.
By comparison, the empirical variance of the true model
is 5.66 · 10−3. Hence, again, the true model variance is signiﬁcantly overestimated,
meaning λ is underestimated by VB.
The question remains of why the VB estimate ¯λ∗is, from both a results-oriented
perspective and from comparison with the true model variance, signiﬁcantly lower
than expected. Alternatively, we can ask why the estimated inverse noise variance
parameter ¯ζ∗is too high, since it is the ratio ¯λ∗/¯ζ∗that ultimately determines the de-
gree of regularization in LSM. We note that this only occurred with the wave-equation
modeled data: the estimates of ¯λ∗and ¯ζ∗for the Kirchhoﬀmodeled data example
yielded the expected results. This is likely because, for the wave-equation data, the
residual is not well-modeled as zero-mean white noise. Indeed, the Kirchhoﬀoperator
models the single-scattering from the reﬂectors, while the acoustic wave-equation data
contain information from the full waveﬁeld, including multiple-scattering, refracted
waves, and other coherent eﬀects, which will remain in the residual after subtracting
the Kirchhoﬀmodeled data from the acoustic data. This correlation in the residual
may lead to very strange models, such as the unregularized LSM images above, that
ﬁt the data considerably better than the true model. This data residual using such
models may have a signiﬁcantly lower variance than that of the data residual found
with the true model, leading to a larger value being estimated for the inverse noise
parameter, ¯ζ∗. Indeed, we ﬁnd that the unregularized LSM image gives a data resid-
ual with a variance roughly 1-2 orders of magnitude lower than the variance of the
residual found using the true model. This would result in ¯ζ∗being estimated at a
signiﬁcantly higher value than expected, which is likely why we were able to compen-
sate for this by increasing λ. This is no fault of the hierarchical Bayesian framework,
but rather, as mentioned previously, a problem with the modeling. The real solution
160

is to use a more realistic forward modeling operator (such as the Born wave-equation
operator [31], for example) to better describe the data.
5.3
Conclusions
Finding the optimal regularization scheme in an inverse problem is an open research
question which we have attempted to address. In the ﬁrst part of this chapter we
analytically investigated the meaning of the regularization parameters by considering
the covariance function that arises in the limiting case of a continuous model. This
analysis allows one to heuristically set the regularization parameters based on a de-
sired correlation length and model variance (on the discrete grid). In the second part
of this chapter, we attempt to estimate an optimal regularization scheme through the
mathematical framework of Bayesian inference. In particular, we applied hierarchical
Bayesian analysis to jointly estimate the model and regularization parameters. We
utilized variational Bayesian methods to approximate a solution to the hierarchical
Bayesian problem, and showed the application of this methodology in the context of
least-squares migration. Inferring the regularization parameters allowed for signiﬁcant
improvements in the inferred seismic image, particularly for the Kirchhoﬀmodeled
data examples, where we are able to remove the eﬀects of noise while still preserving
sharpness at the reﬂectors in the image. The acoustic wave-equation modeled data
examples proved more challenging due to the limited ability of the Kirchhoﬀmodeling
operator used in the inversion to fully describe the data; however we were still able to
make similar qualitative improvements to the inferred images in the sections of the
model that were well-described by the data. As was the case in the previous chapter,
the methodology developed herein is applicable to a broad range of linear inverse
problems involving spatially-varying model parameter statistics and is not limited to
the seismic imaging problem.
161

x [m]
z [m]
0
1000
2000
3000
4000
5000
6000
7000
8000
9000
0
500
1000
1500
2000
2500
Figure 5-24: Unregularized LSM image (each βij = 0) using wave-equation modeled
data (Marmousi model). Correlation with true image = 0.0520.
x [m]
z [m]
0
1000
2000
3000
4000
5000
6000
7000
8000
9000
0
500
1000
1500
2000
2500
Figure 5-25: Uniformly regularized LSM image (each βij = 1) with λ = ¯λ∗and ζ = ¯ζ∗
using wave-equation modeled data (Marmousi model). Correlation with true image
= 0.0757.
x [m]
z [m]
0
1000
2000
3000
4000
5000
6000
7000
8000
9000
0
500
1000
1500
2000
2500
Figure 5-26: Uniformly regularized LSM image (each βij = 1) with λ = 10¯λ∗and
ζ = ¯ζ∗using wave-equation modeled data (Marmousi model). Correlation with true
image = 0.0888.
162

x [m]
z [m]
0
1000
2000
3000
4000
5000
6000
7000
8000
9000
0
500
1000
1500
2000
2500
Figure 5-27: Uniformly regularized LSM image (each βij = 1) with λ = 102¯λ∗and
ζ = ¯ζ∗using wave-equation modeled data (Marmousi model). Correlation with true
image = 0.0728.
x [m]
z [m]
0
1000
2000
3000
4000
5000
6000
7000
8000
9000
0
500
1000
1500
2000
2500
Figure 5-28: Uniformly regularized LSM image (each βij = 1) with λ = 103¯λ∗and
ζ = ¯ζ∗using wave-equation modeled data (Marmousi model). Correlation with true
image = 0.0268.
x [m]
z [m]
0
1000
2000
3000
4000
5000
6000
7000
8000
9000
0
500
1000
1500
2000
2500
Figure 5-29: Variational Bayes MAP image using wave-equation modeled data (Mar-
mousi model). Correlation with true image = 0.0557.
163

0
500
1000
1500
2000
2500
0
1000
2000
3000
4000
5000
6000
7000
8000
9000
 
z [m]
 
x [m]
0
0.2
0.4
0.6
0.8
1
Figure 5-30: Edge strengths β∗estimated with VB using wave-equation modeled data
(Marmousi model).
x [m]
z [m]
0
1000
2000
3000
4000
5000
6000
7000
8000
9000
0
500
1000
1500
2000
2500
Figure 5-31: Empirical Bayes MAP image obtained with β = β∗, λ = 10¯λ∗, and
ζ = ¯ζ∗, using wave-equation modeled data (Marmousi model). Correlation with true
image = 0.0672.
x [m]
z [m]
0
1000
2000
3000
4000
5000
6000
7000
8000
9000
0
500
1000
1500
2000
2500
Figure 5-32: Empirical Bayes MAP image obtained with β = β∗, λ = 102¯λ∗, and
ζ = ¯ζ∗, using wave-equation modeled data (Marmousi model). Correlation with true
image = 0.0828.
164

x [m]
z [m]
0
1000
2000
3000
4000
5000
6000
7000
8000
9000
0
500
1000
1500
2000
2500
Figure 5-33: Empirical Bayes MAP image obtained with β = β∗, λ = 103¯λ∗, and
ζ = ¯ζ∗, using wave-equation modeled data (Marmousi model). Correlation with true
image = 0.0762.
130
140
150
160
170
0
0.1
0.2
0.3
0.4
0.5
λ [reflectivity−2]
q∗
λ(λ)
1.24
1.25
1.26
1.27
x 10
10
0
1
2
3
4
5x 10
−8
ζ [Pa−2]
q∗
ζ(ζ)
(a) q∗
λ(λ)
(b) q∗
ζ(ζ)
Figure 5-34: The variational Bayesian approximations to the posterior distributions
for the parameters scaling the inverse variances of the (a) model λ and (b) noise ζ,
using wave-equation modeled data (Marmousi model).
165

166

Chapter 6
Hierarchical Bayesian Time-Lapse
Seismic Processing
6.1
Summary
In this chapter, we describe a methodology for inferring the change in subsurface
model parameters from time-lapse seismic data within a hierarchical Bayesian frame-
work, where the time-lapse and baseline surveys may have diﬀerent acquisition geome-
tries. Conventional methods for processing time-lapse data with diﬀering acquisition
geometries involve inverting the baseline and time-lapse datasets separately and sub-
tracting the inverted models; however, such methods do not correctly account for
diﬀering model uncertainty between surveys due to diﬀerences in illumination and
observational noise.
Within the hierarchical Bayesian setting, the solution to the
time-lapse inverse problem is given by the marginal maximum a posteriori (MAP)
estimate of the time-lapse change, which seeks the most probable time-lapse change
over all probable baseline models described by the data. We present a framework for
computing the marginal MAP estimate using the expectation-maximization (E-M)
algorithm, which iteratively performs sequential estimation of the time-lapse change
and the baseline model. Our algorithm is validated numerically on synthetic data
simulated from the Marmousi model (with a time-lapse perturbation), where the
hierarchical Bayesian estimates signiﬁcantly outperform conventional time-lapse in-
167

version results.
6.2
Introduction
Monitoring changes in the subsurface geophysical properties of a ﬁeld over time can
provide valuable information for, among other things, reservoir modeling and produc-
tion planning of the ﬁeld. Repeated time-lapse seismic surveys are commonly used
for this purpose, but conventional methods for processing such surveys suﬀer from a
number of limitations. One conventional method for inverting time-lapse seismic data
involves subtracting repeated datasets and inverting the diﬀerenced data to obtain
the changes in the subsurface model parameters [8, 30]; however, the validity of this
method requires both that the seismic data depend linearly on the subsurface model
and that the repeated datasets have identical acquisition geometries. A recent advance
known as double-diﬀerence inversion [81] does not require this linearity for correct-
ness but still requires identical acquisition geometries. However, achieving identical
acquisitions in sequential surveys can be challenging. Furthermore, time-lapse surveys
may diﬀer signiﬁcantly over time due to the availability of new acquisition technolo-
gies. In the case of diﬀering acquisitions, conventional time-lapse inversion involves
inverting the datasets separately and subtracting the inverted models to estimate the
time-lapse change. However, this method (model subtraction) often performs quite
poorly due to diﬀerences in illumination and observational noise.
It is also quite
possible that the time-lapse survey contains information about the baseline model
that was not captured by the baseline survey. Ayeni and Biondi [2] describe a frame-
work for target-oriented linear least-squares inversion of multiple time-lapse datasets
with diﬀering acquisitions, but their methodology only applies to the case when the
data depend linearly on the model and requires explicit computation and storage of
a Hessian matrix, which is only feasible for a small target region.
In this chapter, we describe a methodology for estimating the time-lapse change
in the model parameters from both datasets simultaneously in a hierarchical Bayesian
setting, where we neither require that the data depend linearly on the model nor that
168

the datasets have similar acquisition. In particular, we solve the Bayesian inference
problem using a gradient-based implementation of the expectation-maximization al-
gorithm, which iterates between subsequent updates to the background model and
time-lapse change, ultimately yielding an estimate of the best time-lapse change over
all probable baseline models described by the data.
6.3
Methodology and Bayesian Framework
We consider the case where we have two time-lapse seismic surveys with diﬀerent
source-receiver geometries, a baseline survey yielding data vector d0 and a monitor
survey yielding d1, and we wish to infer the time-lapse change in some subsurface
model parameters m from d0 and d1.
Here m may, for example, be the P-wave
propagation velocity or the reﬂectivity of the medium. We denote the baseline model
by m0, the time-lapse change by ∆m, and denote by F0 and F1 the operators relating
the model parameters to the baseline and monitor datasets, respectively. Then we
can give the data as
d0 = F0(m0) + n0
(6.1)
and
d1 = F1(m0 + ∆m) + n1,
(6.2)
where n0 and n1 are noise terms.
The Bayesian inference setting provides a useful and mathematically rigorous
framework within which the problem of inferring ∆m can be cast.
Within the
Bayesian context, both the model parameters m0, ∆m and the observed data d0,
d1 are viewed as random quantities deﬁned by a probabilistic model. In particular,
we let m0 and ∆m be independent Gaussian random vectors with prior means µ0 and
µ∆and prior covariance matrices C0 and C∆. We relate the prior covariance matrices
to prior assumptions about the spatial statistics of m0 and ∆m by specifying them
169

via the same model for the prior precision matrix described in previous chapters:
Ci = (λi(D + ǫiI))−1
i = 0, ∆,
(6.3)
where D is a diﬀerencing matrix and λi and ǫi are parameters governing the prior
variance and correlation length of spatial variations (cf. Chapter 5). We arrive at a
probabilistic model for the data by modeling n0 and n1 as zero-mean Gaussian noise
(independent of the model) with covariance matrices Σ0 and Σ1, respectively, where
Σi = σiI. Bayes’ rule then gives the joint posterior distribution for m0 and ∆m given
the data:
p(m0, ∆m|d0, d1) ∝p(m0, ∆m)p(d0, d1|m0, ∆m)
∝exp

−1
2
h
∥m0 −µ0∥2
C−1
0
+ ∥∆m −µ∆∥2
C−1
∆
+ ∥d0 −F0(m0)∥2
Σ−1
0 + ∥d1 −F1(m0 + ∆m)∥2
Σ−1
1
i 
(6.4)
where we have deﬁned the notation ∥x∥2
W ≜xTWx. We note that the joint posterior
for (m0, ∆m) is not Gaussian when either F0 or F1 is non-linear.
In order to infer the time-lapse change ∆m with an unknown baseline model m0,
we seek the hierarchical Bayesian maximum a posteriori (MAP) estimate for ∆m.
This requires marginalization of the joint posterior distribution (6.4) over the space
of all baseline models M0 to obtain the marginal MAP solution:
∆mMAP = arg max
∆m
log
Z
M0
p(m0, ∆m|d0, d1)dm0.
(6.5)
We note that this marginal MAP approach is mathematically quite diﬀerent from the
joint inversion approach: whereas the joint inversion approach would seek the pair
(m0, ∆m) that maximizes the joint posterior distribution (i.e. the joint MAP), the
marginal MAP approach seeks to ﬁnd the single best choice for ∆m over all probable
choices for the baseline model m0. In the special case where F0 and F1 are linear,
170

the posterior distribution is Gaussian, and hence the joint MAP solution found by
joint inversion of the linear problem will happen to coincide with the marginal MAP
solution of (6.5); however, this is not true for general F0 and F1. Furthermore, since
the posterior distribution is, in general, not Gaussian, the integral in (6.5) might not
be analytically tractable. Rather than attempting to numerically explore the high-
dimensional model space M0, we turn to the expectation-maximization algorithm to
iteratively solve the marginal MAP problem.
6.3.1
The E-M Algorithm for Time-Lapse Inversion
The E-M algorithm [17, 46] solves maximum likelihood or MAP estimation problems
when a subset of the data relevant to the parameter estimation is unobserved (re-
ferred to as latent variables). In the time-lapse problem considered here, we view the
baseline model m0 as the latent variables. E-M can be thought of as a coordinate
ascent algorithm for solving the marginal MAP optimization problem (6.5), whereby
alternating estimations are performed between the latent variables and the parame-
ters of interest ∆m. Speciﬁcally, the E-M algorithm iteratively updates our estimate
of the time-lapse model
b
∆m(t) according to:
b
∆m(t+1) = arg max
∆m
n
log p(∆m) + Ep(m0|d0,d1, b
∆m(t)) [log p(m0, d0, d1|∆m)]
o
,
(6.6)
where Ep denotes an expectation taken with respect to a probability distribution p,
which in this case is the posterior distribution of m0 conditioned on the previous
iterate
b
∆m(t). Plugging in for probability distributions, we ﬁnd the E-M algorithm
updates
b
∆m(t) by minimizing a cost function φ(t) given by
φ(t)(∆m) = ∥∆m −µ∆∥2
C−1
∆+ Ep(m0|d0,d1, b
∆m(t))
h
∥d1 −F1(m0 + ∆m)∥2
Σ−1
1
i
.
(6.7)
We can perform the above minimization using a ﬁrst-order gradient-based method,
such as gradient descent or non-linear conjugate gradients [27], where the gradient
171

∇φ(t) is given by
∇φ(t)(∆m) = 2C−1
∆(∆m −µ∆)
+ 2Ep(m0|d0,d1, b
∆m(t))

AT
1 (m0 + ∆m)Σ−1
1 (d1 −F1(m0 + ∆m))

,
(6.8)
where A1(m0 + ∆m) is the Jacobian of F1 evaluated at m0 + ∆m.
In order to compute the expected values in (6.7) and (6.8), we would need to
utilize sampling techniques to draw samples from p(m0|d0, d1, b
∆m(t)). At the cost of
replacing the marginal MAP solution with the joint MAP solution, a computation-
ally cheaper option would be to approximate φ(t)(∆m) by replacing the conditional
expectation of the data misﬁt with its value at the conditional MAP estimate of m0,
so that the E-M cost function is approximated by
φ(t)(∆m) ≈∥∆m −µ∆∥2
C−1
∆+
d1 −F1
 m0MAP| b
∆m(t),d0,d1 + ∆m
2
Σ−1
1
(6.9)
and its gradient is
∇φ(t)(∆m) ≈2C−1
∆(∆m −µ∆)
+ 2AT
1 (m0MAP| b
∆m(t),d0,d1 + ∆m)Σ−1
1 (d1 −F1(m0MAP| b
∆m(t),d0,d1 + ∆m)),
(6.10)
where
m0MAP| b
∆m(t),d0,d1 = arg min
m0
∥m0 −µ0∥2
C−1
0
+ ∥d0 −F0(m0)∥2
Σ−1
0
+ ∥d1 −F1(m0 +
b
∆m(t))∥2
Σ−1
1 .
(6.11)
To summarize, our implementation of the E-M algorithm for the time-lapse inversion
problem is as follows:
172

Algorithm 6.1 E-M Algorithm for Time-Lapse Inversion
Initialize
b
∆m(0). Set t = 0. Iterate on t:
1. Estimate Ep(m0|d0,d1, b
∆m(t))[∥d1 −F1(m0 + ∆m)∥2
Σ−1
1 ], the expected value of the
data misﬁt in (6.7), and its gradient in (6.8) by either:
a. Sampling the baseline model from p
 m0|d0, d1, b
∆m(t)
, or
b. Minimizing (6.11) to obtain the MAP estimate of the baseline model
and using the approximations of (6.9) and (6.10).
2. Update
b
∆m(t+1) = arg min∆m φ(t)(∆m).
6.4
Numerical Results
We demonstrate our results numerically on a simple example involving the Marmousi
model. As a test case, we consider the seismic imaging problem, where the model
parameters (m0, ∆m) are reﬂectivity (with the smooth part of the velocity model
known), and a localized time-lapse change in the model is introduced; the true baseline
model and time-lapse changes are shown in Figure 6-1. We take F0 and F1 to be
the Kirchhoﬀmodeling operators for the source-receiver geometries in d0 and d1,
respectively.
(We note that since this example is for a linear problem, the joint
and marginal MAP solutions coincide.)
The synthetic datasets are inverse crime
data (plus noise) generated from an array of surface receivers responding to a single
20 Hz Ricker source at the surface, where the horizontal position of the source is
signiﬁcantly shifted from xs0 = 3.4 km in the baseline survey to xs1 = 7.2 km in the
monitor survey.
Figure 6-2 shows the results of the conventional inversion for the time-lapse change
found by separately inverting the two datasets and subtracting the inverted models.
Figure 6-3 shows the marginal MAP estimate for the time-lapse change and the
expected baseline model obtained after 20 iterations of the E-M algorithm. In order
to remove the eﬀects of the additive noise and better capture true change in the model,
we thresholded the estimates of the time-lapse changes above the noise level. Since
the source locations between the two surveys are so far apart, the surveys illuminate
173

A
Lateral Distance [km]
Depth [km]
Baseline Reflectivity Model
 
 
0
1
2
3
4
5
6
7
8
9
0
1
2
−0.5
0
0.5
B
Lateral Distance [km]
Depth [km]
Time−Lapse Change
 
 
0
1
2
3
4
5
6
7
8
9
0
1
2
−0.5
0
0.5
Figure 6-1: (A) True baseline reﬂectivity model and (B) time-lapse change.
174

A
Lateral Distance [km]
Depth [km]
Inverted Baseline Model from d0
 
 
0
1
2
3
4
5
6
7
8
9
0
1
2
−0.5
0
0.5
B
Lateral Distance [km]
Depth [km]
Inverted Model from d1
 
 
0
1
2
3
4
5
6
7
8
9
0
1
2
−0.5
0
0.5
C
Lateral Distance [km]
Depth [km]
Time−Lapse Change via Model Subtraction
 
 
0
1
2
3
4
5
6
7
8
9
0
1
2
−0.5
0
0.5
D
Lateral Distance [km]
Depth [km]
Time−Lapse Change via Model Subtraction (thresholded)
 
 
0
1
2
3
4
5
6
7
8
9
0
1
2
−0.5
0
0.5
Figure 6-2: Conventional inversion results for the baseline model and time-lapse
change.
(A,B) d0 and d1 are inverted separately to estimate m0 and m0 + ∆m.
(C) The time-lapse change is estimated by subtracting the inversion results and (D)
thresholded to remove the eﬀects of noise.
175

A
Lateral Distance [km]
Depth [km]
Marginal MAP for ∆ m  via E−M algorithm
 
 
0
1
2
3
4
5
6
7
8
9
0
1
2
−0.5
0
0.5
B
Lateral Distance [km]
Depth [km]
Marginal MAP for ∆ m  via E−M algorithm (thresholded)
 
 
0
1
2
3
4
5
6
7
8
9
0
1
2
−0.5
0
0.5
C
Lateral Distance [km]
Depth [km]
Inverted Baseline Model E[m0] from E−M algorithm
 
 
0
1
2
3
4
5
6
7
8
9
0
1
2
−0.5
0
0.5
Figure 6-3: Inversion results obtained with our hierarchical Bayesian framework.
(A) The marginal MAP solution
b
∆m(t) after 20 E-M iterations which is then (B)
thresholded to remove the eﬀects of noise. (C) The E-M estimate of the baseline
model E[m0| b
∆m(t), d0, d1].
176

diﬀerent (but overlapping) sections of the model domain (as seen in Figure 6-2(A,B)).
This results in sections of the baseline model appearing in the time-lapse estimate
obtained from model subtraction, as is evident from both Figures 6-2(C) and 6-2(D).
In contrast, even prior to thresholding, we see that the structure of the baseline model
does not appear in the marginal MAP estimate obtained from the E-M algorithm in
Figure 6-3(A) (indeed the noise is truly just noise), and, after thresholding (Figure
6-3(B)), the time-lapse change has been completely isolated from the background.
The E-M algorithm also gives the expected baseline model (Figure 6-3(C)), where we
see far better illumination since both datasets are used to compute this model.
6.5
Conclusions
In this chapter, we applied the hierarchical Bayesian framework to the problem of es-
timating the change in subsurface model parameters from time-lapse seismic datasets
with diﬀering acquisition geometries. In particular, our gradient-based implementa-
tion of the expectation-maximization algorithm solved the marginal MAP problem
for the time-lapse change model by performing subsequent updates to the time-lapse
change and baseline models. As veriﬁed by our numerical results, the marginal MAP
estimate for the time-lapse change did not contain structure from the baseline model,
and, also important, the estimated baseline model is constrained by both datasets.
While we provided a numerical example for the time-lapse seismic imaging of reﬂec-
tivity, the framework and algorithm detailed in this chapter are general and can be
applied to other time-lapse inverse problems, without any requirement on the linearity
of the forward operators F0 and F1. Although our method is computationally inten-
sive (each iteration of the E-M algorithm requires solving two inverse problems), we
ﬁnd that the algorithm performs well with relatively few iterations; hence, it should
be feasible to run this algorithm if the cost of a single inversion is reasonable. A
straight-forward extension of this work would be to the case of inferring a series of
time-lapse changes in the model from multiple monitor datasets (rather than just
one).
177

178

Chapter 7
Conclusions
In this thesis, we explored the application of Bayesian inference methods to diﬀerent
geophysical inverse problems involving seismic data.
Of particular focus was the
question of how to appropriately introduce regularization in an inverse problem where
the smoothness properties of the underlying earth model may vary with space. This
question can be equivalently posed as how to pick an appropriate prior distribution
in the Bayesian inference setting.
To address this question, we deﬁned the prior
distribution on the model via a Markov random ﬁeld and parameterized the edges
of the MRF with edge strengths that capture the local smoothness properties of
the prior. We explored the utility of this representation through its application to
diﬀerent geophysical inverse problems. Below, we summarize the main contributions
and conclusions of these studies.
7.1
Summary of Main Contributions
Chapter 3 serves as our ﬁrst study into the application of Bayesian inference and
probabilistic graphical models to a geophysical inverse problem. Here we apply a
non-hierarchical Bayesian inference framework (where the edge strengths of the MRF
were predetermined) to the problem of characterizing the fractured nature of a reser-
voir from seismic data. The Bayesian setting allows for combining scattering data
and amplitude measures that contain information about anisotropy under a single
179

inversion framework, thereby allowing for the inversion of fracture properties under
a larger physical regime than would be attainable using only one of these data types.
We further show the capability of the edge strengths to both enforce smoothness
in the estimates of the fracture properties and capture a priori information about
geological features in the model, such as a discontinuity arising from a fault whose
location is known.
In Chapter 4, we address the question of how to optimize the edge strengths of
the MRF in the context of the seismic imaging problem, where the seismic image,
consisting of sharp coherent reﬂectors, naturally tends to exhibit spatially-varying
smoothness properties. We formulate the seismic imaging problem within the hier-
archical Bayesian framework, treating the edge strengths as random variables to be
inferred from the data. The problem of inferring these edge strengths presents sig-
niﬁcant computational challenges: the cost of evaluating the posterior distribution
seemed initially to create a computational bottleneck that prevented our approach
from being scalable to large models. The use of the expectation-maximization (E-M)
algorithm along with the approximate methods detailed in Chapter 4 was a break-
through in this regard, allowing the scalability of our method to more realistic-sized
models.
We obtain the marginal MAP estimate of the edge strengths via the E-
M algorithm, thereby resulting in a prior distribution on the model that correctly
captures its spatial smoothness properties. This allows for mitigating the eﬀects of
limited acquisition and observational noise in the estimated image while still preserv-
ing sharpness at the reﬂectors.
Chapter 5 extends the work of Chapter 4 by providing a methodology for choosing
the remaining parameters (other than the edge strengths) that deﬁne the prior distri-
bution for the model. Here, we ﬁrst derive the relationship between these parameters
and the prior model covariance and then extend the hierarchical Bayesian framework
of Chapter 4 to include these additional parameters. Our derivation provides insight
into these parameters and may provide guidance in selecting them in the future. We
note that while the work of Chapters 4-5 is presented within the context of the seis-
mic imaging problem, the methodology developed in these chapters can be applied
180

to other linear inverse problems where the model parameters exhibit spatially (or
temporally) varying smoothness properties.
In Chapter 6, we explore the application of the hierarchical Bayesian framework to
the problem of time-lapse seismic inversion, where the objective is to infer the change
in the subsurface model parameters over time by taking repeated seismic surveys. We
consider the case where the surveys are taken with diﬀerent acquisition geometries
causing conventional methods for time-lapse inversion to perform poorly. We develop
a novel and computationally tractable approach to the time-lapse inversion problem
by applying the E-M algorithm to obtain the marginal MAP estimate of the time-lapse
change in the model parameters. In contrast to the results obtained by conventional
methods, the marginal MAP estimate for the time-lapse change does not contain
structure from the background model, and, furthermore, the background model we
estimate is based on information in both the baseline and monitor surveys.
7.2
Directions for Future Work
Here we suggest some avenues for future work stemming from the research presented
in this thesis.
Alternative Parameterizations
One direction for future work is to explore alternative ways to parameterize the prior
on the model within the hierarchical Bayesian setting. While we have shown through
various examples that our choice of parameterization using edge strengths is able to
capture the spatially-varying smoothness properties of the model, this choice is by no
means unique, and it may be possible to improve on our results via alternative param-
eterizations of the prior. Additionally, throughout this thesis the number of model
parameters has been a ﬁxed input to the inference procedure, however the choice of
how to discretize the earth model (i.e. what size of grid cells should be used) is non-
trivial. In our case, the grid cell size was chosen relative to the seismic wavelength in
the medium (as this determines the scale of the heterogeneities that scatter the seismic
181

waveﬁeld), however it is possible to optimize this via Bayesian model selection (some-
times referred to as transdimensional Bayesian inference). Transdimensional Bayesian
methods have been previously applied to geophysical inverse problems [9, 43, 44].
A promising approach developed by Bodin [6] utilizes a transdimensional Bayesian
method using Voronoi cells with mobile geometry, shape, and number to parameterize
the model. Extending our methodology to incorporate these generalizations is hence
another fruitful direction for future research. Finally, a gridded model parameteriza-
tion may not be the most optimal for the seismic imaging problem. Considering the
shape of the reﬂectors commonly encountered in seismic imaging, one may wish to
instead parameterize the model using basis functions deﬁned by the reﬂectors rather
than individual grid cells. Here again, Bayesian model selection could be utilized to
infer the number of reﬂectors in addition to the parameters deﬁning each reﬂector.
Generalization to Non-linear Problems
The methodology presented in Chapters 4-5 for estimating the edge strengths from
the data was formulated in the context of the linear inverse problem of seismic imaging
when the background velocity model is known. In cases where the velocity model is
not given, it must ﬁrst be estimated from the seismic data, where the problem of
estimating these propagation velocities (along with a density model) from the full
seismic waveforms is referred to as full-waveform inversion (FWI) [10, 76]. Typically
a smoothed version of the velocity model is obtained and then used to solve the
linear seismic imaging problem to resolve the discontinuities in the image. However,
it would be of interest to be able to apply our methodology for estimating the edge
strengths directly to the FWI problem, in hopes of better resolving discontinuities and
sharp contrasts in the velocity model (and thereby obviating the need to afterwards
solve the imaging problem). FWI, however, is a highly non-linear inverse problem,
and thus our algorithm and methodology for estimating the edge strengths is not
immediately applicable. Hence, another interesting and useful direction for future
work is extending the methodology and algorithms from Chapters 4-5 to non-linear
inverse problems.
182

Time-lapse Seismic Inversion with Multiple Monitor Surveys
A straight-forward extension of the work of Chapter 6 would address the case where
one is interested in monitoring the changes in the subsurface as a function of time.
Here, in addition to a baseline seismic survey, the data would consist of multiple
monitor surveys (with possibly diﬀering acquisition geometries) taken at regular time
intervals over a prolonged period (e.g. a seismic survey might be performed every day
over the course of one year). Standard approaches from Bayesian inference for time-
series analysis, such as hidden Markov models, can be used to extend the hierarchical
Bayesian methodology of Chapter 6 to the multiple survey case.
Dynamic Survey Design and Optimization
In the context of the above described time-lapse seismic inversion problem with mul-
tiple surveys, since the seismic surveys are repeatedly taken over time, the question
arises of how best to design the next survey (i.e. which acquisition geometry should
be used). In particular, if the goal is to monitor and track changes in the subsurface,
the survey designer may wish to optimize the subsequent survey geometry to best
infer the time-evolving change in the subsurface. This dynamic optimization problem
provides a further useful extension of the work of Chapter 6.
7.3
Final Remarks
The research presented in this thesis is among a small, but growing number of stud-
ies exploring hierarchical Bayesian approaches in geophysics [7, 9, 43, 44, 45]. The
results of our research are encouraging and indicate promising directions for further
application of Bayesian inference in geophysical inverse problems. We note that one
downside of Bayesian approaches is that they typically involve more computation
than standard inversion methods require. For example, at the heart of both the E-M
and variational Bayes algorithms applied to LSM (Chapters 4-5) are the alternating
updates between the image and edge strengths, where each image update step is es-
183

sentially a standard LSM inversion. While our research has explored ways to exploit
multiple LSM runs to solve a Bayesian inference problem, in industry scale applica-
tions even a single least-squares migration is often considered to be too expensive for
practical use [16]. Nevertheless, despite the increased cost, the improvements made
to standard inversion methods by hierarchical Bayesian approaches are signiﬁcant,
as we have demonstrated in this thesis. Hence this area remains a fruitful area for
future research and application.
184

Appendix A
Derivation of the Thomsen anisotropy
parameters from excess fracture
compliance
Here we derive the Thomsen anisotropy parameters for modeling the P-P reﬂection
coeﬃcient in Chapter 3. We use the linear slip model of Schoenberg and Sayers [66]
to express the Thomsen anisotropy parameters of the fractured medium in terms of
the excess fracture compliance of the medium. The anisotropy parameters can be
expressed in terms of the stiﬀness tensor of the medium C as [60]:
δ(V ) = (C13 + C55)2 −(C33 −C55)2
2C33 (C33 −C55)
,
(A.1)
ǫ(V ) = C11 −C33
2C33
,
(A.2)
γ(V ) = C66 −C44
2C44
.
(A.3)
We can relate the fracture properties of the medium to the stiﬀness tensor by com-
puting the excess compliance tensor of the fractures Sfrac (which is the contribution
of the fractures to the overall medium compliance tensor). Schoenberg and Sayers
[66] show that, under the simplifying assumption that the behavior of the fracture
system is invariant with respect to rotation about the axis normal to the fractures,
185

the excess compliance tensor of the fractures is given by
Sfrac =


ZN
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
ZT
0
0
0
0
0
0
ZT


(A.4)
where ZN and ZT are the excess normal and tangential compliances of the fracture
system, respectively. In our analysis, we assumed that the excess normal and tangen-
tial compliances of the fracture system are equal. Thus at grid node (i, j), we have
ZN = ZT = 10zij. Keeping with our convention of treating zero excess compliance
as 10−13 Pa−1, if zij = −13 then we set ZN = ZT = 0 (corresponding to the case of
no fractures at node (i, j)). Schoenberg and Sayers [66] further show that the overall
medium compliance tensor Stot can be expressed as the sum of the fracture excess
compliance tensor Sfrac and the background compliance tensor Sback, so that
Stot = Sback + Sfrac.
(A.5)
The background compliance tensor is the inverse of background stiﬀness tensor Cback,
which for an isotropic, homogeneous medium is given by [72]:
S−1
back = Cback =


λ + 2µ
λ
λ
0
0
0
λ
λ + 2µ
λ
0
0
0
λ
λ
λ + 2µ
0
0
0
0
0
0
µ
0
0
0
0
0
0
µ
0
0
0
0
0
0
µ


,
(A.6)
186

where µ = ρβ2 and λ = ρα2 −2µ are Lamé’s parameters. The overall stiﬀness tensor
of the medium C is then found as the inverse of the overall compliance tensor [66]:
C = S−1
tot =


Mb(1 −dN)
λ(1 −dN)
λ(1 −dN)
0
0
0
λ(1 −dN)
Mb(1 −r2
bdN)
λ(1 −rbdN)
0
0
0
λ(1 −dN)
λ(1 −rbdN)
Mb(1 −r2
bdN)
0
0
0
0
0
0
µ
0
0
0
0
0
0
µ(1 −dT)
0
0
0
0
0
0
µ(1 −dT)


,
(A.7)
where
Mb = λ + 2µ,
rb = λ
Mb
,
0 ≤dT =
ZTµ
1 + ZTµ < 1,
0 ≤dN =
ZNMb
1 + ZNMb
< 1.
Combining all of the above gives the anisotropy parameters of the fractured
medium at node (i, j), which we denote by δ(V )
zij , γ(V )
zij , ǫ(V )
zij , in terms of the excess
fracture compliance. Using (A.7) in (A.1), (A.2), (A.3), and (3.11) gives the forward
model for the P-P reﬂection coeﬃcient as a function of the fracture parameters at
node (i, j).
187

188

Bibliography
[1] A. Ali and M. Jakobsen. Seismic characterization of reservoirs with multiple
fracture sets using velocity and attenuation anisotropy data. Journal of Applied
Geophysics, 75(3):590 – 602, 2011.
ISSN 0926-9851. doi: 10.1016/j.jappgeo.
2011.09.003.
URL http://www.sciencedirect.com/science/article/pii/
S0926985111001959.
[2] G. Ayeni and B. Biondi. Target-oriented joint least-squares migration/inversion
of time-lapse seismic data sets. Geophysics, 75(3):R61–R73, 2010.
[3] M. J. Beal. Variational Algorithms for Approximate Bayesian Inference. PhD
thesis, Gatsby Computational Neuroscience Unit, University College London,
2003.
[4] C. M. Bishop. Pattern Recognition and Machine Learning (Information Science
and Statistics). Springer-Verlag New York, Inc., Secaucus, NJ, USA, 2006. ISBN
0387310738.
[5] N. Bleistein.
Mathematical methods for wave phenomena.
Computer science
and applied mathematics. Academic Press, 1984. ISBN 9780121056506. URL
http://books.google.com/books?id=mmnvAAAAMAAJ.
[6] T. Bodin. Transdimensional Approaches to Geophysical Inverse Problems. PhD
thesis, The Australian National University, 2010.
[7] T. Bodin, M. Sambridge, N. Rawlinson, and P. Arroucau.
Transdimensional
tomography with unknown data noise. Geophysical Journal International, 189
(3):1536–1556, 2012. ISSN 1365-246X. doi: 10.1111/j.1365-246X.2012.05414.x.
URL http://dx.doi.org/10.1111/j.1365-246X.2012.05414.x.
[8] A. Buland and Y. El Ouair. Bayesian time-lapse inversion. Geophysics, 71(3):
R43–R48, 2006.
[9] A. Buland and H. Omre. Joint AVO inversion, wavelet estimation and noise-level
estimation using a spatially coupled hierarchical Bayesian model. Geophysical
Prospecting, 51(6):531–550, 2003. ISSN 1365-2478. doi: 10.1046/j.1365-2478.
2003.00390.x. URL http://dx.doi.org/10.1046/j.1365-2478.2003.00390.
x.
189

[10] C. Bunks, F. Saleck, S. Zaleski, and G. Chavent. Multiscale seismic waveform
inversion. Geophysics, 60(5):1457–1473, 1995. doi: 10.1190/1.1443880. URL
http://dx.doi.org/10.1190/1.1443880.
[11] J. Claerbout. Earth Soundings Analysis: Processing Versus Inversion. Stanford
Exploration project. Blackwell Scientiﬁc Publ., 1992. ISBN 9780865422100. URL
http://books.google.com/books?id=ws1qQgAACAAJ.
[12] M. Clapp. Imaging under salt: illumination compensation by regularized inver-
sion. PhD thesis, Stanford University, July 2005.
[13] P. Cliﬀord. Markov random ﬁelds in statistics. In G. Grimmett and D. Welsh,
editors, Disorder in Physical Systems: A Volume in Honour of John M. Ham-
mersley, pages 19–32. Oxford University Press, 1990.
[14] R. Coates and M. Schoenberg.
Finite-diﬀerence modeling of faults and frac-
tures.
Geophysics, 60(5):1514–1526, 1995.
doi:
10.1190/1.1443884.
URL
http://library.seg.org/doi/abs/10.1190/1.1443884.
[15] R. S. Crosson. Crustal structure modeling of earthquake data: 1. Simultane-
ous least squares estimation of hypocenter and velocity parameters. Journal of
Geophysical Research, 81(17):3036–3046, 1976. ISSN 2156-2202. doi: 10.1029/
JB081i017p03036. URL http://dx.doi.org/10.1029/JB081i017p03036.
[16] W. Dai. Multisource Least-squares Migration and Prism Wave Reverse Time
Migration. PhD thesis, University of Utah, December 2012.
[17] A. P. Dempster, N. M. Laird, and D. B. Rubin.
Maximum Likelihood from
Incomplete Data via the EM Algorithm. Journal of the Royal Statistical Society.
Series B (Methodological), 39(1):1–38, 1977.
ISSN 00359246.
doi: 10.2307/
2984875.
[18] N. Dubos-Sallée and P. Rasolofosaon. Evaluation of fracture parameters and
ﬂuid content from seismic and well data.
In SEG Technical Program Ex-
panded Abstracts, pages 1511–1515, 2008.
doi:
10.1190/1.3059201.
URL
http://library.seg.org/doi/abs/10.1190/1.3059201.
[19] B. Duquet, K. J. Marfurt, and J. A. Dellinger. Kirchhoﬀmodeling, inversion
for reﬂectivity, and subsurface illumination. Geophysics, 65(4):1195–1209, 2000.
doi: 10.1190/1.1444812. URL http://link.aip.org/link/?GPY/65/1195/1.
[20] J. Eidsvik, P. Avseth, H. Omre, T. Mukerji, and G. Mavko. Stochastic reservoir
characterization using prestack seismic data. Geophysics, 69(4):978–993, 2004.
[21] X. Fang, M. Fehler, Z. Zhu, Y. Zheng, and D. Burns. Reservoir fracture charac-
terizations from seismic scattered waves. In SEG Technical Program Expanded
Abstracts 2012, volume 31, pages 1–6, 2012. doi: 10.1190/segam2012-0813.1.
URL http://library.seg.org/doi/abs/10.1190/segam2012-0813.1.
190

[22] X. Fang, M. C. Fehler, Z. Zhu, Y. Zheng, and D. R. Burns.
Reservoir frac-
ture characterization from seismic scattered waves. Geophysical Journal Inter-
national, 196(1):481–492, 2014.
doi: 10.1093/gji/ggt381. URL http://gji.
oxfordjournals.org/content/196/1/481.abstract.
[23] J. Gaiser and R. Van Dok. Green river basin 3-D/3-C case study for fracture
characterization: Analysis of PS-wave birefringence. In SEG Techincal Program
Expanded Abstracts, volume 20, pages 764–767, 2001.
[24] A. Gelman, J. B. Carlin, H. S. Stern, D. B. Dunson, A. Vehtari, and D. B. Rubin.
Bayesian Data Analysis. Chapman & Hall/CRC Texts in Statistical Science.
CRC Press, Boca Raton, FL, USA, third edition, 2013. ISBN 9781439840955.
[25] S. Geoltrain and J. Brac. Can we image complex structures with ﬁrst-arrival
traveltime?
Geophysics, 58(4):564–575, 1993. doi: 10.1190/1.1443439. URL
http://geophysics.geoscienceworld.org/content/58/4/564.abstract.
[26] H. Gjoeystdal and B. Ursin.
Inversion of reﬂection times in three dimen-
sions. Geophysics, 46(7):972–973, 1981. doi: 10.1190/1.1441246. URL http:
//geophysics.geoscienceworld.org/content/46/7/972.abstract.
[27] W. W. Hager and H. Zhang. A survey of nonlinear conjugate gradient methods.
Paciﬁc journal of Optimization, 2(1):35–58, 2006.
[28] J. Hammersley and P. Cliﬀord.
Markov ﬁelds on ﬁnite graphs and lattices.
Unpublished manuscript, 1971.
[29] C. Hanitzsch, J. Schleicher, and P. Hubral.
True-amplitude migration of 2D
synthetic data. Geophysical Prospecting, 42(5):445–462, 1994. ISSN 1365-2478.
doi: 10.1111/j.1365-2478.1994.tb00220.x. URL http://dx.doi.org/10.1111/
j.1365-2478.1994.tb00220.x.
[30] T. Hong and M. Sen.
Joint Bayesian inversion for reservoir characterization
and uncertainty quantiﬁcation. SEG Technical Program Expanded Abstracts, 27:
1481–1485, 2008.
[31] J. A. Hudson and J. R. Heritage. The use of the born approximation in seismic
scattering problems. Geophysical Journal of the Royal Astronomical Society, 66
(1):221–240, 1981. ISSN 1365-246X. doi: 10.1111/j.1365-246X.1981.tb05954.x.
URL http://dx.doi.org/10.1111/j.1365-246X.1981.tb05954.x.
[32] M. Hutchinson. A stochastic estimator of the trace of the inﬂuence matrix for
laplacian smoothing splines.
Communications in Statistics - Simulation and
Computation, 19(2):433–450, 1990.
doi:
10.1080/03610919008812866.
URL
http://dx.doi.org/10.1080/03610919008812866.
[33] J. R. Inman.
Resistivity inversion with ridge regression.
Geophysics, 40
(5):798–817, 1975.
doi:
10.1190/1.1440569.
URL http://geophysics.
geoscienceworld.org/content/40/5/798.abstract.
191

[34] D. L. B. Jupp and K. Vozoﬀ.
Stable iterative methods for the inversion of
geophysical data. Geophysical Journal of the Royal Astronomical Society, 42(3):
957–976, 1975. ISSN 1365-246X. doi: 10.1111/j.1365-246X.1975.tb06461.x. URL
http://dx.doi.org/10.1111/j.1365-246X.1975.tb06461.x.
[35] D. Koller and N. Friedman. Probabilistic Graphical Models: Principles and Tech-
niques. MIT Press, 2009.
[36] H. Kühl and M. D. Sacchi. Least-squares wave-equation migration for AVP/AVA
inversion. Geophysics, 68(1):262–273, 2003. doi: 10.1190/1.1543212. URL http:
//geophysics.geoscienceworld.org/content/68/1/262.abstract.
[37] G. Lambare, J. Virieux, R. Madariaga, and S. Jin. Iterative asymptotic inversion
in the acoustic approximation. Geophysics, 57(9):1138–1154, 1992. doi: 10.1190/
1.1443328.
URL http://geophysics.geoscienceworld.org/content/57/9/
1138.abstract.
[38] B. Last and K. Kubik. Compact gravity inversion. Geophysics, 48(6):713–721,
1983. doi: 10.1190/1.1441501. URL http://dx.doi.org/10.1190/1.1441501.
[39] R. LeBras and R. W. Clayton. An iterative inversion of back-scattered acoustic
waves. Geophysics, 53(4):501–508, 1988. doi: 10.1190/1.1442481. URL http:
//geophysics.geoscienceworld.org/content/53/4/501.abstract.
[40] Y. Li and D. W. Oldenburg.
3-D inversion of gravity data.
Geophysics,
63(1):109–119, 1998.
doi:
10.1190/1.1444302.
URL http://geophysics.
geoscienceworld.org/content/63/1/109.abstract.
[41] E. Liu and A. Martinez. Seismic Fracture Characterization: Concepts and Prac-
tical Applications. EAGE Publications bv, 2013. ISBN 978-90-73834-40-8.
[42] H. Lynn, S. R. Narhari, S. Al-Ashwak, V. Kidambi, B. Al-Qadeeri, and O. Al-
Khaled. PP azimuthal-amplitudes and -acoustic impedance for fractured carbon-
ate reservoir characterization. In SEG Technical Program Expanded Abstracts,
volume 29, pages 258–262, 2009.
[43] A. Malinverno.
A Bayesian criterion for simplicity in inverse problem
parametrization. Geophysical Journal International, 140(2):267–285, 2000. ISSN
1365-246X. doi: 10.1046/j.1365-246x.2000.00008.x. URL http://dx.doi.org/
10.1046/j.1365-246x.2000.00008.x.
[44] A. Malinverno.
Parsimonious Bayesian Markov chain Monte Carlo inversion
in a nonlinear geophysical problem. Geophysical Journal International, 151(3):
675–688, 2002. ISSN 1365-246X. doi: 10.1046/j.1365-246X.2002.01847.x. URL
http://dx.doi.org/10.1046/j.1365-246X.2002.01847.x.
[45] A. Malinverno and V. A. Briggs. Expanded uncertainty quantiﬁcation in in-
verse problems: Hierarchical Bayes and empirical Bayes.
Geophysics, 69(4):
192

1005–1016, 2004. doi: 10.1190/1.1778243. URL http://link.aip.org/link/
?GPY/69/1005/1.
[46] G. J. McLachlan and T. Krishnan.
The EM algorithm and extensions.
Wi-
ley series in probability and statistics. Wiley, Hoboken, NJ, 2nd edition, 2008.
ISBN 978-0-471-20170-0. URL http://gso.gbv.de/DB=2.1/CMD?ACT=SRCHA\
&SRT=YOP\&IKT=1016\&TRM=ppn+52983362X\&sourceid=fbw\_bibsonomy.
[47] M. Metwaly, E. Elawadi, S. Moustafa, and N. Al-Ariﬁ.
Combined inversion
of electrical resistivity and transient electromagnetic soundings for mapping
groundwater contamination plumes in Al Quwy’yia area, Saudi Arabia. Journal
of Environmental and Engineering Geophysics, 19(1):45–52, 2014. doi: 10.2113/
JEEG19.1.45.
URL http://library.seg.org/doi/abs/10.2113/JEEG19.1.
45.
[48] K. Murphy, Y. Weiss, and M. Jordan. Loopy belief propagation for approxi-
mate inference: an empirical study. In Proceedings of Uncertainty in Artiﬁcial
Intelligence, pages 467–475, 1999.
[49] T. Nemeth, C. Wu, and G. T. Schuster. Least-squares migration of incomplete
reﬂection data. Geophysics, 64(1):208–221, 1999. doi: 10.1190/1.1444517. URL
http://link.aip.org/link/?GPY/64/208/1.
[50] A. Neumaier. Solving ill-conditioned and singular linear systems: A tutorial on
regularization. SIAM Review, 40:636–666, 1998.
[51] G. Neumann.
Determination of lateral inhomogeneities in reﬂection seismics
by inversion of traveltime residuals.
Geophysical Prospecting, 29(2):161–177,
1981. ISSN 1365-2478. doi: 10.1111/j.1365-2478.1981.tb00399.x. URL http:
//dx.doi.org/10.1111/j.1365-2478.1981.tb00399.x.
[52] D. Oldenburg. The inversion and interpretation of gravity anomalies. Geophysics,
39(4):526–536, 1974. doi: 10.1190/1.1440444. URL http://dx.doi.org/10.
1190/1.1440444.
[53] F. Orieux, O. Feron, and J. F. Giovannelli. Sampling high-dimensional Gaussian
distributions for general linear inverse problems. Signal Processing Letters, IEEE,
19(5):251–254, May 2012. ISSN 1070-9908. doi: 10.1109/LSP.2012.2189104.
[54] M. Oristaglio and M. Worthington.
Inversion of surface and borehole elec-
tromagnetic data for two-dimensional electrical conductivity models*.
Geo-
physical Prospecting, 28(4):633–657, 1980.
ISSN 1365-2478.
doi: 10.1111/j.
1365-2478.1980.tb01248.x. URL http://dx.doi.org/10.1111/j.1365-2478.
1980.tb01248.x.
[55] J. Pearl. Reverend Bayes on inference engines: A distributed hierarchical ap-
proach. In Proceedings of the Second National Conference on Artiﬁcial Intelli-
gence, pages 133–136, Menlo Park, California, 1982. AAAI, AAAI Press.
193

[56] J. Pearl.
Probabilistic reasoning in intelligent systems: networks of plausible
inference. Morgan Kaufmann Publishers Inc., San Francisco, California, 1988.
[57] R.-E. Plessix and W. A. Mulder.
Resistivity imaging with controlled-source
electromagnetic data: depth and data weighting. Inverse Problems, 24(3):034012,
2008. URL http://stacks.iop.org/0266-5611/24/i=3/a=034012.
[58] I. Pšenčík and J. Martins. Properties of weak contrast pp reﬂection/transmission
coeﬃcients for weakly anisotropic elastic media. Studia Geophysica et Geodaetica,
45(2):176–199, 2001. ISSN 0039-3169. doi: 10.1023/A:1021868328668. URL
http://dx.doi.org/10.1023/A%3A1021868328668.
[59] W. L. Rodi and S. C. Myers. Computation of traveltime covariances based on
stochastic models of velocity heterogeneity. Geophysical Journal International,
194(3):1582–1595, 2013.
[60] A. Rüger. Variation of P-wave reﬂectivity with oﬀset and azimuth in anisotropic
media. Geophysics, 63(3):935–947, 1998.
[61] A. Rüger.
Reﬂection coeﬃcients and azimuthal AVO analysis in anisotropic
media. Society of Exploration Geophysicists, 2002. ISBN 78-1-56080-107-8.
[62] C. Sayers.
Seismic characterization of reservoirs containing multiple frac-
ture sets.
Geophysical Prospecting, 57(2):187–192, 2009.
ISSN 0016-8025.
doi: 10.1111/j.1365-2478.2008.00766.x. URL http://dx.doi.org/10.1111/j.
1365-2478.2008.00766.x.
[63] C. Sayers and J. Rickett. Azimuthal variation in AVO response for fractured gas
sands. Geophysical Prospecting, 45:165–182, 1997.
[64] C. M. Sayers, A. D. Taleghani, and J. Adachi. The eﬀect of mineralization on the
ratio of normal to tangential compliance of fractures. Geophysical Prospecting,
57(3):439–446, 2009. ISSN 1365-2478. doi: 10.1111/j.1365-2478.2008.00746.x.
URL http://dx.doi.org/10.1111/j.1365-2478.2008.00746.x.
[65] M. Schoenberg and J. Douma. Elastic wave propagation in media with parallel
fractures and aligned cracks. Geophysical Prospecting, 36:571–590, 1988.
[66] M. Schoenberg and C. Sayers. Seismic anisotropy of fractured rock. Geophysics,
60(1):204–211, 1995.
[67] S. Sil and S. Srinivasan.
Stochastic simulation of fracture strikes using seis-
mic anisotropy induced velocity anomalies. Exploration Geophysics, 40:257–264,
2009.
[68] D. Simpson, F. Lindgren, and H. Rue. In order to make spatial statistics com-
putationally feasible, we need to forget about the covariance function. Envi-
ronmetrics, 23(1):65–74, February 2012. doi: 10.1002/env.1137. URL http:
//opus.bath.ac.uk/32282/.
194

[69] R. Snieder and J. Trampert.
Inverse problems in geophysics.
In A. Wirgin,
editor, Waveﬁeld inversion, pages 119–190. Springer Verlag, New York, 1999.
[70] A. Tarantola. Inverse Problem Theory and Methods for Model Parameter Es-
timation. Society for Industrial and Applied Mathematics, Philadelphia, PA,
USA, 2004. ISBN 0898715725.
[71] L. Thomsen. Weak elastic anisotropy. Geophysics, 51(10):1954–1966, 1986.
[72] I. Tsvankin. Seismic signatures and analysis of reﬂection data in anisotropic
media, volume 29 of Handbook of Geophysical Exploration. Seismic Exploration.
Elsevier, Amsterdam, 2001.
[73] B. Ursin and G. U. Haugen. Weak-contrast approximation of the elastic scat-
tering matrix in anisotropic media.
pure and applied geophysics, 148(3-4):
685–714, 1996.
ISSN 0033-4553.
doi:
10.1007/BF00874584.
URL http:
//dx.doi.org/10.1007/BF00874584.
[74] J. P. Verdon and A. Wüstefeld. Measurement of the normal/tangential frac-
ture compliance ratio (ZN/ZT) during hydraulic fracture stimulation using s-
wave splitting data. Geophysical Prospecting, 61:461–475, 2013. ISSN 1365-2478.
doi: 10.1111/j.1365-2478.2012.01132.x. URL http://dx.doi.org/10.1111/j.
1365-2478.2012.01132.x.
[75] J. L. Vigneresse. Linear inverse problem in gravity proﬁle interpretations. Journal
of Geophysics, 43:193–213, 1977.
[76] J. Virieux and S. Operto. An overview of full-waveform inversion in exploration
geophysics. Geophysics, 74(6):WCC1–WCC26, 2009. doi: 10.1190/1.3238367.
URL http://dx.doi.org/10.1190/1.3238367.
[77] C. R. Vogel. Computational Methods for Inverse Problems. Society for Industrial
and Applied Mathematics, Philadelphia, PA, USA, 2002. ISBN 0898715075.
[78] M. Willis, D. Burns, R. Rao, B. Minsley, M. Toksoz, and L. Vetri.
Spatial
orientation and distribution of reservoir fractures from scattered seismic energy.
Geophysics, 71(5):O43–O51, 2006.
[79] J. Zhang and M. N. Toksöz. Nonlinear refraction traveltime tomography. Geo-
physics, 63(5):1726–1737, 1998. doi: 10.1190/1.1444468. URL http://dx.doi.
org/10.1190/1.1444468.
[80] J. Zhang, U. S. ten Brink, and M. N. Toksöz. Nonlinear refraction and reﬂection
travel time tomography. Journal of Geophysical Research: Solid Earth, 103(B12):
29743–29757, 1998.
ISSN 2156-2202.
doi: 10.1029/98JB01981.
URL http:
//dx.doi.org/10.1029/98JB01981.
[81] Z. Zhang and L. Huang. Double-diﬀerence elastic-waveform inversion with prior
information for time-lapse monitoring. Geophysics, 78(6):R259–R273, 2013.
195

[82] Y. Zheng, X. Fang, M. C. Fehler, and D. R. Burns.
Seismic characteriza-
tion of fractured reservoirs by focusing Gaussian beams.
Geophysics, 78(4):
A23–A28, 2013.
doi:
10.1190/geo2012-0512.1.
URL http://geophysics.
geoscienceworld.org/content/78/4/A23.abstract.
[83] X. Zhu, P. Valasek, B. Roy, S. Shaw, J. Howell, S. Whitney, N. D. Whitmore,
and P. Anno. Recent applications of turning-ray tomography. Geophysics, 73
(5):VE243–VE254, 2008. doi: 10.1190/1.2957894. URL http://geophysics.
geoscienceworld.org/content/73/5/VE243.abstract.
196

