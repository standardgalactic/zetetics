CODE LISTINGS AND FIGURES

CHAPTER 1
Table 1.1. Categorized NLP applications (view table Øgure)
Search
Web
Documents
Autocomplete
Editing
Spelling
Grammar
Style
Dialog
Chatbot
Assistant
Scheduling
Writing
Index
Concordance
Table of contents
Email
Spam lter
Classication
Prioritization
Text mining
Summarization
Knowledge
extraction
Medical diagnoses
Law
Legal inference
Precedent search
Subpoena
classication
News
Event detection
Fact checking
Headline
composition
Attribution
Plagiarism detection
Literary forensics
Style coaching
Sentiment
analysis
Community morale
monitoring
Product review
triage
Customer care
Behavior
prediction
Finance
Election
forecasting
Marketing
Creative writing
Movie scripts
Poetry
Song lyrics
Figure 1.1. Kinds of automata

Figure 1.2. Token sorting tray
1
2
3
4
5
6
7
8
>>> import re 
>>> r = "(hi|hello|hey)[ ]*([a-z]*)" 
>>> re.match(r, 'Hello Rosa', flags=re.IGNORECASE) 
<_sre.SRE_Match object; span=(0, 10), match='Hello Rosa'> 
>>> re.match(r, "hi ho, hi ho, it's off to work ...", flags=re.IGNORECASE) 
<_sre.SRE_Match object; span=(0, 5), match='hi ho'> 
>>> re.match(r, "hey, what's up", flags=re.IGNORECASE) 
<_sre.SRE_Match object; span=(0, 3), match='hey>
1
2
3
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
>>> r = r"[^a-z]*([y]o|[h']?ello|ok|hey|(good[ ])?(morn[gin']{0,3}|"\ 
...     r"afternoon|even[gin']{0,3}))[\s,;:]{1,3}([a-z]{1,20})" 
>>> re_greeting = re.compile(r, flags=re.IGNORECASE) 
>>> re_greeting.match('Hello Rosa') 
<_sre.SRE_Match object; span=(0, 10), match='Hello Rosa'> 
>>> re_greeting.match('Hello Rosa').groups() 
('Hello', None, None, 'Rosa') 
>>> re_greeting.match("Good morning Rosa") 
<_sre.SRE_Match object; span=(0, 17), match="Good morning Rosa"> 
>>> re_greeting.match("Good Manning Rosa") 
>>> re_greeting.match('Good evening Rosa Parks').groups() 
('Good evening', 'Good ', 'evening', 'Rosa') 
>>> re_greeting.match("Good Morn'n Rosa") 
<_sre.SRE_Match object; span=(0, 16), match="Good Morn'n Rosa"> 
>>> re_greeting.match("yo Rosa") 
<_sre.SRE_Match object; span=(0, 7), match='yo Rosa'>
1
2
3
1
2
3
4
5
6
7
8
9
10
11
12
>>> my_names = set(['rosa', 'rose', 'chatty', 'chatbot', 'bot', 
...     'chatterbot']) 
>>> curt_names = set(['hal', 'you', 'u']) 
>>> greeter_name = '' 
>>> match = re_greeting.match(input()) 
... 
>>> if match: 
...     at_name = match.groups()[-1] 
...     if at_name in curt_names: 
...         print("Good one.") 
...     elif at_name.lower() in my_names: 
...         print("Hi {}, How are you?".format(greeter_name))
1


Figure 1.3. Chatbot recirculating (recurrent) pipeline
1
2
3
4
5
6
>>> from collections import Counter 
  
>>> Counter("Guten Morgen Rosa".split()) 
Counter({'Guten': 1, 'Rosa': 1, 'morgen': 1}) 
>>> Counter("Good morning, Rosa!".split()) 
Counter({'Good': 1, 'Rosa!': 1, 'morning,': 1})
1
2
3
4
5
6
7
8
9
10
>>> from itertools import permutations 
  
>>> [" ".join(combo) for combo in\ 
...     permutations("Good morning Rosa!".split(), 3)] 
['Good morning Rosa!', 
 'Good Rosa! morning', 
 'morning Good Rosa!', 
 'morning Rosa! Good', 
 'Rosa! Good morning', 
 'Rosa! morning Good']
1
2
3
4
5
6
7
8
>>> s = """Find textbooks with titles containing 'NLP', 
...     or 'natural' and 'language', or 
...     'computational' and  'linguistics'.""" 
>>> len(set(s.split())) 
12 
>>> import numpy as np 
>>> np.arange(1, 12 + 1).prod()  # factorial(12) = arange(1, 13).prod() 
479001600

Figure 1.4. Example layers for an NLP pipeline

Figure 1.5. 2D IQ of some natural language processing systems

CHAPTER 2
Figure 2.1. Tokenized phrase
Listing 2.1. Example Monticello sentence split into tokens
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
>>> sentence = """Thomas Jefferson began building Monticello at the 
...   age of 26.""" 
>>> sentence.split() 
['Thomas', 
 'Jefferson', 
 'began', 
 'building', 
 'Monticello', 
 'at', 
 'the', 
 'age', 
 'of', 
 '26.'] 
>>> str.split(sentence) 
['Thomas', 
 'Jefferson', 
 'began', 
 'building', 
 'Monticello', 
 'at', 
 'the', 
 'age', 
 'of', 
 '26.']
1
2
3
4
5
6
7
8
9
10
11
12
13
14
>>> import numpy as np 
>>> token_sequence = str.split(sentence) 
>>> vocab = sorted(set(token_sequence)) 
>>> ', '.join(vocab) 
'26., Jefferson, Monticello, Thomas, age, at, began, building, of, the' 
>>> num_tokens = len(token_sequence) 
>>> vocab_size = len(vocab) 
>>> onehot_vectors = np.zeros((num_tokens, 
...                            vocab_size), int) 
>>> for i, word in enumerate(token_sequence): 
...     onehot_vectors[i, vocab.index(word)] = 1 
>>> ' '.join(vocab) 
'26. Jefferson Monticello Thomas age at began building of the' 
>>> onehot_vectors 
1
2
3
4
5

15
16
17
18
19
20
21
22
23
24
array([[0, 0, 0, 1, 0, 0, 0, 0, 0, 0], 
       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0], 
       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0], 
       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0], 
       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0], 
       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0], 
       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1], 
       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0], 
       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0], 
       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0]])
Listing 2.2. One-hot vector sequence for the Monticello sentence
1
2
3
4
5
6
7
8
9
10
11
12
13
>>> import pandas as pd 
>>> pd.DataFrame(onehot_vectors, columns=vocab) 
   26.  Jefferson  Monticello  Thomas  age  at  began  building  of  the 
0    0          0           0       1    0   0      0         0   0    0 
1    0          1           0       0    0   0      0         0   0    0 
2    0          0           0       0    0   0      1         0   0    0 
3    0          0           0       0    0   0      0         1   0    0 
4    0          0           1       0    0   0      0         0   0    0 
5    0          0           0       0    0   1      0         0   0    0 
6    0          0           0       0    0   0      0         0   0    1 
7    0          0           0       0    1   0      0         0   0    0 
8    0          0           0       0    0   0      0         0   1    0 
9    1          0           0       0    0   0      0         0   0    0
Listing 2.3. Prettier one-hot vectors
1
2
3
4
5
6
7
8
9
10
11
12
13
14
>>> df = pd.DataFrame(onehot_vectors, columns=vocab) 
>>> df[df == 0] = '' 
>>> df 
  26. Jefferson Monticello Thomas age at began building of the 
0                               1 
1             1 
2                                            1 
3                                                     1 
4                        1 
5                                      1 
6                                                            1 
7                                   1 
8                                                        1 
9   1
1
2
3
>>> num_rows = 3000 * 3500 * 15 
>>> num_rows 
157500000 
1

4
5
6
7
8
9
10
>>> num_bytes = num_rows * 1000000 
>>> num_bytes 
157500000000000 
>>> num_bytes / 1e9 
157500  # gigabytes 
>>> _ / 1000 
157.5  # terabytes
2
3
1
2
3
4
5
6
7
8
9
10
11
12
13
14
>>> sentence_bow = {} 
>>> for token in sentence.split(): 
...     sentence_bow[token] = 1 
>>> sorted(sentence_bow.items()) 
[('26.', 1) 
 ('Jefferson', 1), 
 ('Monticello', 1), 
 ('Thomas', 1), 
 ('age', 1), 
 ('at', 1), 
 ('began', 1), 
 ('building', 1), 
 ('of', 1), 
 ('the', 1)]
1
2
3
4
5
6
>>> import pandas as pd 
>>> df = pd.DataFrame(pd.Series(dict([(token, 1) for token in 
...   sentence.split()])), columns=['sent']).T 
>>> df 
      26.  Jefferson  Monticello  Thomas  age  at  began  building  of  the 
sent    1          1           1       1    1   1      1         1   1    1
Listing 2.4. Construct a DataFrame  of bag-of-words vectors
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
>>> sentences = """Thomas Jefferson began building Monticello at the\ 
...   age of 26.\n""" 
>>> sentences += """Construction was done mostly by local masons and\ 
...   carpenters.\n""" 
>>> sentences += "He moved into the South Pavilion in 1770.\n" 
>>> sentences += """Turning Monticello into a neoclassical masterpiece\ 
...   was Jefferson's obsession.""" 
>>> corpus = {} 
>>> for i, sent in enumerate(sentences.split('\n')): 
...     corpus['sent{}'.format(i)] = dict((tok, 1) for tok in 
...         sent.split()) 
>>> df = pd.DataFrame.from_records(corpus).fillna(0).astype(int).T 
>>> df[df.columns[:10]] 
        1770.  26.  Construction   ...    Pavilion  South  Thomas 
sent0      0    1             0   ...           0      0       1 
1
2
3

16
17
18
sent1      0    0             1   ...           0      0       0 
sent2      1    0             0   ...           1      1       0 
sent3      0    0             0   ...           0      0       0
Listing 2.5. Example dot product calculation
1
2
3
4
5
6
7
8
>>> v1 = pd.np.array([1, 2, 3]) 
>>> v2 = pd.np.array([2, 3, 4]) 
>>> v1.dot(v2) 
20 
>>> (v1 * v2).sum() 
20 
>>> sum([x1 * x2 for x1, x2 in zip(v1, v2)]) 
20
1
2
Listing 2.6. Overlap of word counts for two bag-of-words vectors
1
2
3
4
5
6
7
>>> df = df.T 
>>> df.sent0.dot(df.sent1) 
0 
>>> df.sent0.dot(df.sent2) 
1 
>>> df.sent0.dot(df.sent3) 
1
1
2
>>> [(k, v) for (k, v) in (df.sent0 & df.sent3).items() if v] 
[('Monticello', 1)]
Listing 2.7. Tokenize the Monticello sentence with a regular expression
1
2
3
4
5
6
7
8
9
10
>>> import re 
>>> sentence = """Thomas Jefferson began building Monticello at the\ 
...   age of 26.""" 
>>> tokens = re.split(r'[-\s.,;!?]+', sentence) 
>>> tokens 
['Thomas', 
 'Jefferson', 
 'began', 
 'building', 
 'Monticello', 
1

WHEN TO COMPILE YOUR REGEX PATTERNS
The regular expression module in Python allows you to precompile
regular expressions,[a] which you then can reuse across your code base.
For example, you might have a regex that extracts phone numbers.
You could use re.compile()  to precompile the expression and pass
it along as an argument to a function or class doing tokenization. This
is rarely a speed advantage, because Python caches the compiled
objects for the last MAXCACHE=100  regular expressions. But if you
have more than 100 di|erent regular expressions at work, or you want
to call methods of the regular expression rather than the
corresponding re  functions, re.compile  can be useful:
a See stack overow or the latest Python documentation for more
details (http://stackoverow.com/a/452143/623735).
11
12
13
14
15
16
 'at', 
 'the', 
 'age', 
 'of', 
 '26', 
 '']
1
2
3
4
>>> pattern = re.compile(r"([-\s.,;!?])+") 
>>> tokens = pattern.split(sentence) 
>>> tokens[-10:]  # just the last 10 tokens 
['the', ' ', 'age', ' ', 'of', ' ', '26', '.', '']
1
2
3
4
5
6
7
8
9
10
11
12
13
14
>>> sentence = """Thomas Jefferson began building Monticello at the\ 
...   age of 26.""" 
>>> tokens = pattern.split(sentence) 
>>> [x for x in tokens if x and x not in '- \t\n.,;!?'] 
['Thomas', 
 'Jefferson', 
 'began', 
 'building', 
 'Monticello', 
 'at', 
 'the', 
 'age', 
 'of', 
 '26']
1

Figure 2.2. Tokenized phrase
WHEN TO USE THE NEW REGEX MODULE IN PYTHON
There’s a new regular expression package called regex  that will
eventually replace the re  package. It’s completely backward
compatible and can be installed with pip  from pypi. It’s useful new
features include support for
Overlapping match sets
Multithreading
Feature-complete support for Unicode
Approximate regular expression matches (similar to TRE’s
agrep  on UNIX systems)
Larger default MAXCACHE (500 regexes)
Even though regex  will eventually replace the re  package and is
completely backward compatible with re , for now you must install it
as an additional package using a package manager such as pip:
You can nd more information about the regex module on the PyPI
website (https://pypi.python.org/pypi/regex).
1 $ pip install regex
1
2
3
4
5
6
>>> from nltk.tokenize import RegexpTokenizer 
>>> tokenizer = RegexpTokenizer(r'\w+|$[0-9.]+|\S+') 
>>> tokenizer.tokenize(sentence) 
['Thomas', 
 'Jefferson', 
 'began', 

Figure 2.3. Tokenized phrase
TOKENIZE INFORMAL TEXT FROM SOCIAL NETWORKS SUCH AS
TWITTER AND FACEBOOK
The NLTK library includes a tokenizer— casual_tokenize —that was
built to deal with short, informal, emoticon-laced texts from social
networks where grammar and spelling conventions vary widely.
The casual_tokenize  function allows you to strip usernames and
reduce the number of repeated characters within a token:
7
8
9
10
11
12
13
14
 'building', 
 'Monticello', 
 'at', 
 'the', 
 'age', 
 'of', 
 '26', 
 '.']
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
>>> from nltk.tokenize import TreebankWordTokenizer 
>>> sentence = """Monticello wasn't designated as UNESCO World Heritage\ 
...   Site until 1987.""" 
>>> tokenizer = TreebankWordTokenizer() 
>>> tokenizer.tokenize(sentence) 
['Monticello', 
 'was', 
 "n't", 
 'designated', 
 'as', 
 'UNESCO', 
 'World', 
 'Heritage', 
 'Site', 
 'until', 
 '1987', 
 '.']

1
2
3
4
5
6
7
8
9
10
11
>>> from nltk.tokenize.casual import casual_tokenize 
>>> message = """RT @TJMonticello Best day everrrrrrr at Monticel
...   Awesommmmmmeeeeeeee day :*)""" 
>>> casual_tokenize(message) 
['RT', '@TJMonticello', 
 'Best', 'day','everrrrrrr', 'at', 'Monticello', '.', 
 'Awesommmmmmeeeeeeee', 'day', ':*)'] 
>>> casual_tokenize(message, reduce_len=True, strip_handles=True)
['RT', 
 'Best', 'day', 'everrr', 'at', 'Monticello', '.', 
 'Awesommmeee', 'day', ':*)']
1
2
3
4
5
6
7
8
9
10
11
>>> tokenize_2grams("Thomas Jefferson began building Monticello at the\ 
...   age of 26.") 
['Thomas Jefferson', 
 'Jefferson began', 
 'began building', 
 'building Monticello', 
 'Monticello at', 
 'at the', 
 'the age', 
 'age of', 
 'of 26']
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
>>> sentence = """Thomas Jefferson began building Monticello at the\ 
...   age of 26.""" 
>>> pattern = re.compile(r"([-\s.,;!?])+") 
>>> tokens = pattern.split(sentence) 
>>> tokens = [x for x in tokens if x and x not in '- \t\n.,;!?'] 
>>> tokens 
['Thomas', 
 'Jefferson', 
 'began', 
 'building', 
 'Monticello', 
 'at', 
 'the', 
 'age', 
 'of', 
 '26']
1
2
3
4
5
>>> from nltk.util import ngrams 
>>> list(ngrams(tokens, 2)) 
[('Thomas', 'Jefferson'), 
 ('Jefferson', 'began'), 
 ('began', 'building'), 

6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
 ('building', 'Monticello'), 
 ('Monticello', 'at'), 
 ('at', 'the'), 
 ('the', 'age'), 
 ('age', 'of'), 
 ('of', '26')] 
>>> list(ngrams(tokens, 3)) 
[('Thomas', 'Jefferson', 'began'), 
 ('Jefferson', 'began', 'building'), 
 ('began', 'building', 'Monticello'), 
 ('building', 'Monticello', 'at'), 
 ('Monticello', 'at', 'the'), 
 ('at', 'the', 'age'), 
 ('the', 'age', 'of'), 
 ('age', 'of', '26')]
1
2
3
4
5
6
7
8
9
10
11
>>> two_grams = list(ngrams(tokens, 2)) 
>>> [" ".join(x) for x in two_grams] 
['Thomas Jefferson', 
 'Jefferson began', 
 'began building', 
 'building Monticello', 
 'Monticello at', 
 'at the', 
 'the age', 
 'age of', 
 'of 26']
1
2
3
4
5
>>> stop_words = ['a', 'an', 'the', 'on', 'of', 'off', 'this', 'is'] 
>>> tokens = ['the', 'house', 'is', 'on', 'fire'] 
>>> tokens_without_stopwords = [x for x in tokens if x not in stop_words] 
>>> print(tokens_without_stopwords) 
['house', 'fire']
Listing 2.8. NLTK list of stop words
1
2
3
4
5
6
7
8
9
>>> import nltk 
>>> nltk.download('stopwords') 
>>> stop_words = nltk.corpus.stopwords.words('english') 
>>> len(stop_words) 
153 
>>> stop_words[:7] 
['i', 'me', 'my', 'myself', 'we', 'our', 'ours'] 
>>> [sw for sw in stopwords if len(sw) == 1] 
['i', 'a', 's', 't', 'd', 'm', 'o', 'y']

MORE ON THE PORTER STEMMER
Listing 2.9. NLTK list of stop words
1
2
3
4
5
6
7
8
9
10
>>> from sklearn.feature_extraction.text import\ 
...   ENGLISH_STOP_WORDS as sklearn_stop_words 
>>> len(sklearn_stop_words) 
318 
>>> len(stop_words) 
179 
>>> len(stop_words.union(sklearn_stop_words)) 
378 
>>> len(stop_words.intersection(sklearn_stop_words)) 
119
1
2
1
2
3
4
>>> tokens = ['House', 'Visitor', 'Center'] 
>>> normalized_tokens = [x.lower() for x in tokens] 
>>> print(normalized_tokens) 
['house', 'visitor', 'center']
1
2
3
4
5
6
7
>>> def stem(phrase): 
...     return ' '.join([re.findall('^(.*ss|.*?)(s)?$', 
...         word)[0][0].strip("'") for word in phrase.lower().split()]) 
>>> stem('houses') 
'house' 
>>> stem("Doctor House's calls") 
'doctor house call'
1
2
3
4
5
>>> from nltk.stem.porter import PorterStemmer 
>>> stemmer = PorterStemmer() 
>>> ' '.join([stemmer.stem(w).strip("'") for w in 
...   "dish washer's washed dishes".split()]) 
'dish washer wash dish'

Julia Menchavez has graciously shared her translation of Porter’s
original stemmer algorithm into pure Python
(https://github.com/jedijulia/porter-
stemmer/blob/master/stemmer.py). If you are ever tempted to
develop your own stemmer, consider these 300 lines of code and the
lifetime of renement that Porter put into them.
There are eight steps to the Porter stemmer algorithm: 1a, 1b, 1c, 2, 3,
4, 5a, and 5b. Step 1a is a bit like your regular expression for dealing
with trailing S’s:[a]
a This is a trivially abbreviated version of Julia Menchavez’s
implementation of porter-stemmer  on GitHub
(https://github.com/jedijulia/porter-
stemmer/blob/master/stemmer.py).
1 This isn’t at all like str.replace(). Julia’s self.replace()
modies only the ending of a word.
The remainining seven steps are much more complicated because they
have to deal with the complicated English spelling rules for the
following:
Step 1a—“s” and “es” endings
Step 1b—“ed,” “ing,” and “at” endings
Step 1c—“y” endings
Step 2—“nounifying” endings such as “ational,” “tional,”
“ence,” and “able”
Step 3—adjective endings such as “icate,”[b] “ful,” and
“alize”
1
2
3
4
5
6
7
8
9
10
def step1a(self, word): 
    if word.endswith('sses'): 
        word = self.replace(word, 'sses', 'ss')    1 
    elif word.endswith('ies'): 
        word = self.replace(word, 'ies', 'i') 
    elif word.endswith('ss'): 
        word = self.replace(word, 'ss', 'ss') 
    elif word.endswith('s'): 
        word = self.replace(word, 's', '') 
    return word

b Sorry Chick, Porter doesn’t like your obsfucate
username ;).
Step 4—adjective and noun endings such as “ive,” “ible,”
“ent,” and “ism”
Step 5a—stubborn “e” endings, still hanging around
Step 5b—trailing double consonants for which the stem will
end in a single “l”
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
>>> nltk.download('wordnet') 
>>> from nltk.stem import WordNetLemmatizer 
>>> lemmatizer = WordNetLemmatizer() 
>>> lemmatizer.lemmatize("better") 
'better' 
>>> lemmatizer.lemmatize("better", pos="a") 
'good' 
>>> lemmatizer.lemmatize("good", pos="a") 
'good' 
>>> lemmatizer.lemmatize("goods", pos="a") 
'goods' 
>>> lemmatizer.lemmatize("goods", pos="n") 
'good' 
>>> lemmatizer.lemmatize("goodness", pos="n") 
'goodness' 
>>> lemmatizer.lemmatize("best", pos="a") 
'best'
1
2
1
2
>>> stemmer.stem('goodness') 
'good'
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
>>> from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer 
>>> sa = SentimentIntensityAnalyzer() 
>>> sa.lexicon 
{ ... 
':(': -1.9, 
':)': 2.0, 
... 
'pls': 0.3, 
'plz': 0.3, 
... 
'great': 3.1, 
... } 
>>> [(tok, score) for tok, score in sa.lexicon.items() 
...   if " " in tok] 
[("( '}{' )", 1.6), 
 ("can't stand", -2.0), 
 ('fed up', -1.8), 
 ('screwed up', -1.5)] 
1
2
3
4

19
20
21
22
23
24
25
26
>>> sa.polarity_scores(text=\ 
...   "Python is very readable and it's great for NLP.") 
{'compound': 0.6249, 'neg': 0.0, 'neu': 0.661, 
'pos': 0.339} 
>>> sa.polarity_scores(text=\ 
...   "Python is not a bad choice for most applications.") 
{'compound': 0.431, 'neg': 0.0, 'neu': 0.711, 
'pos': 0.289}

5
6
1
2
3
4
5
6
7
8
9
>>> corpus = ["Absolutely perfect! Love it! :-) :-) :-)", 
...           "Horrible! Completely useless. :(", 
...           "It was OK. Some good and some bad things."] 
>>> for doc in corpus: 
...     scores = sa.polarity_scores(doc) 
...     print('{:+}: {}'.format(scores['compound'], doc)) 
+0.9428: Absolutely perfect! Love it! :-) :-) :-) 
-0.8768: Horrible! Completely useless. :( 
+0.3254: It was OK. Some good and some bad things.
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
>>> from nlpia.data.loaders import get_data 
>>> movies = get_data('hutto_movies') 
>>> movies.head().round(2) 
    sentiment                                            text 
id 
1        2.27  The Rock is destined to be the 21st Century... 
2        3.53  The gorgeously elaborate continuation of ''... 
3       -0.60                     Effective but too tepid ... 
4        1.47  If you sometimes like to go to the movies t... 
5        1.73  Emerges as something rare, an issue movie t... 
>>> movies.describe().round(2) 
       sentiment 
count   10605.00 
mean        0.00 
min        -3.88 
max         3.94
1
1
2
3
4
5
6
7
8
9
10
11
12
>>> import pandas as pd 
>>> pd.set_option('display.width', 75) 
>>> from nltk.tokenize import casual_tokenize 
>>> bags_of_words = [] 
>>> from collections import Counter 
>>> for text in movies.text: 
...    bags_of_words.append(Counter(casual_tokenize(text))) 
>>> df_bows = pd.DataFrame.from_records(bags_of_words) 
>>> df_bows = df_bows.fillna(0).astype(int) 
>>> df_bows.shape 
(10605, 20756) 
>>> df_bows.head() 
1
2
3
4
5
6

13
14
15
16
17
18
19
20
21
22
23
24
25
   !  "  #  $  %  &  ' ...  zone  zoning  zzzzzzzzz  ½  élan  -  ' 
0  0  0  0  0  0  0  4 ...     0       0          0  0     0  0  0 
1  0  0  0  0  0  0  4 ...     0       0          0  0     0  0  0 
2  0  0  0  0  0  0  0 ...     0       0          0  0     0  0  0 
3  0  0  0  0  0  0  0 ...     0       0          0  0     0  0  0 
4  0  0  0  0  0  0  0 ...     0       0          0  0     0  0  0 
>>> df_bows.head()[list(bags_of_words[0].keys())] 
   The  Rock  is  destined  to  be ...  Van  Damme  or  Steven  Segal  . 
0    1     1   1         1   2   1 ...    1      1   1       1      1  1 
1    2     0   1         0   0   0 ...    0      0   0       0      0  4 
2    0     0   0         0   0   0 ...    0      0   0       0      0  0 
3    0     0   1         0   4   0 ...    0      0   0       0      0  1 
4    0     0   0         0   0   0 ...    0      0   0       0      0  1

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
>>> from sklearn.naive_bayes import MultinomialNB 
>>> nb = MultinomialNB() 
>>> nb = nb.fit(df_bows, movies.sentiment > 0) 
>>> movies['predicted_sentiment'] =\ 
...   nb.predict_proba(df_bows) * 8 - 4 
>>> movies['error'] = (movies.predicted_sentiment - movies.sentiment).abs() 
>>> movies.error.mean().round(1) 
2.4 
>>> movies['sentiment_ispositive'] = (movies.sentiment > 0).astype(int) 
>>> movies['predicted_ispositiv'] = (movies.predicted_sentiment > 0).astype(int) 
>>> movies['''sentiment predicted_sentiment sentiment_ispositive\ 
...   predicted_ispositive'''.split()].head(8) 
    sentiment  predicted_sentiment  sentiment_ispositive  predicted_ispositive 
id 
1    2.266667                   4                    1                    1 
2    3.533333                   4                    1                    1 
3   -0.600000                  -4                    0                    0 
4    1.466667                   4                    1                    1 
5    1.733333                   4                    1                    1 
6    2.533333                   4                    1                    1 
7    2.466667                   4                    1                    1 
8    1.266667                  -4                    1                    0 
>>> (movies.predicted_ispositive == 
...   movies.sentiment_ispositive).sum() / len(movies) 
0.9344648750589345

1
2
3
4
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
>>> products = get_data('hutto_products') 
...    bags_of_words = [] 
>>> for text in products.text: 
...    bags_of_words.append(Counter(casual_tokenize(text))) 
>>> df_product_bows = pd.DataFrame.from_records(bags_of_words) 
>>> df_product_bows = df_product_bows.fillna(0).astype(int) 
>>> df_all_bows = df_bows.append(df_product_bows) 
>>> df_all_bows.columns 
Index(['!', '"', '#', '#38', '$', '%', '&', ''', '(', '(8', 
       ... 
       'zoomed', 'zooming', 'zooms', 'zx', 'zzzzzzzzz', '~', '½', 'élan', 
       '-', '''], 
      dtype='object', length=23302) 
>>> df_product_bows = df_all_bows.iloc[len(movies):][df_bows.columns] 
>>> df_product_bows.shape 
1

16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
(3546, 20756) 
>>> df_bows.shape 
(10605, 20756) 
>>> products[ispos] =  
 (products.sentiment > 0).astype(int) 
>>> products['predicted_ispositive'] =  
 nb.predict(df_product_bows.values).astype(int) 
>>> products.head() 
id  sentiment                                     text      ispos  pred 
0  1_1      -0.90  troubleshooting ad-2500 and ad-2600 ...      0     0 
1  1_2      -0.15  repost from january 13, 2004 with a ...      0     0 
2  1_3      -0.20  does your apex dvd player only play ...      0     0 
3  1_4      -0.10  or does it play audio and video but ...      0     0 
4  1_5      -0.50  before you try to return the player ...      0     0 
>>> (products.pred == products.ispos).sum() / len(products) 
0.5572476029328821
2
3

CHAPTER 3
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
>>> from nltk.tokenize import TreebankWordTokenizer 
>>> sentence = """The faster Harry got to the store, the faster Harry, 
...     the faster, would get home.""" 
>>> tokenizer = TreebankWordTokenizer() 
>>> tokens = tokenizer.tokenize(sentence.lower()) 
>>> tokens 
['the', 
 'faster', 
 'harry', 
 'got', 
 'to', 
 'the', 
 'store', 
 ',', 
 'the', 
 'faster', 
 'harry', 
 ',', 
 'the', 
 'faster', 
 ',', 
 'would', 
 'get', 
 'home', 
 '.']
1
2
3
4
5
6
7
8
9
10
11
12
13
14
>>> from collections import Counter 
>>> bag_of_words = Counter(tokens) 
>>> bag_of_words 
Counter({'the': 4, 
         'faster': 3, 
         'harry': 2, 
         'got': 1, 
         'to': 1, 
         'store': 1, 
         ',': 3, 
         'would': 1, 
         'get': 1, 
         'home': 1, 
         '.': 1})
1
2
>>> bag_of_words.most_common(4) 
[('the', 4), (',', 3), ('faster', 3), ('harry', 2)]
1

1
2
3
4
5
>>> times_harry_appears = bag_of_words['harry'] 
>>> num_unique_words = len(bag_of_words) 
>>> tf = times_harry_appears / num_unique_words 
>>> round(tf, 4) 
0.1818
1
1
2
3
4
5
6
7
8
>>> from collections import Counter 
>>> from nltk.tokenize import TreebankWordTokenizer 
>>> tokenizer = TreebankWordTokenizer() 
>>> from nlpia.data.loaders import kite_text 
>>> tokens = tokenizer.tokenize(kite_text.lower()) 
>>> token_counts = Counter(tokens) 
>>> token_counts 
Counter({'the': 26, 'a': 20, 'kite': 16, ',': 15, ...})
1
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
>>> import nltk 
>>> nltk.download('stopwords', quiet=True) 
True 
>>> stopwords = nltk.corpus.stopwords.words('english') 
>>> tokens = [x for x in tokens if x not in stopwords] 
>>> kite_counts = Counter(tokens) 
>>> kite_counts 
Counter({'kite': 16, 
         'traditionally': 1, 
         'tethered': 2, 
         'heavier-than-air': 1, 
         'craft': 2, 
         'wing': 5, 
         'surfaces': 1, 
         'react': 1, 
         'air': 2, 
         ..., 
         'made': 1})}
1
2
3
4
5
6
7
8
9
10
>>> document_vector = [] 
>>> doc_length = len(tokens) 
>>> for key, value in kite_counts.most_common(): 
...     document_vector.append(value / doc_length) 
>>> document_vector 
[0.07207207207207207, 
 0.06756756756756757, 
 0.036036036036036036, 
 ..., 
 0.0045045045045045045]

1
2
3
4
>>> docs = ["The faster Harry got to the store, the faster and faster Harry  
 would get home."] 
>>> docs.append("Harry is hairy and faster than Jill.") 
>>> docs.append("Jill is not as hairy as Harry.")
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
>>> doc_tokens = [] 
>>> for doc in docs: 
...     doc_tokens += [sorted(tokenizer.tokenize(doc.lower()))] 
>>> len(doc_tokens[0]) 
17 
>>> all_doc_tokens = sum(doc_tokens, []) 
>>> len(all_doc_tokens) 
33 
>>> lexicon = sorted(set(all_doc_tokens)) 
>>> len(lexicon) 
18 
>>> lexicon 
[',', 
 '.', 
 'and', 
 'as', 
 'faster', 
 'get', 
 'got', 
 'hairy', 
 'harry', 
 'home', 
 'is', 
 'jill', 
 'not', 
 'store', 
 'than', 
 'the', 
 'to', 
 'would']
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
>>> from collections import OrderedDict 
>>> zero_vector = OrderedDict((token, 0) for token in lexicon) 
>>> zero_vector 
OrderedDict([(',', 0), 
             ('.', 0), 
             ('and', 0), 
             ('as', 0), 
             ('faster', 0), 
             ('get', 0), 
             ('got', 0), 
             ('hairy', 0), 
             ('harry', 0), 
             ('home', 0), 
             ('is', 0), 
             ('jill', 0), 
             ('not', 0), 
             ('store', 0), 
             ('than', 0), 
             ('the', 0), 
             ('to', 0), 
             ('would', 0)])

Figure 3.1. 2D vectors
Figure 3.2. 2D term frequency vectors
1
2
3
4
5
6
7
8
9
>>> import copy 
>>> doc_vectors = [] 
>>> for doc in docs: 
...     vec = copy.copy(zero_vector) 
...     tokens = tokenizer.tokenize(doc.lower()) 
...     token_counts = Counter(tokens) 
...     for key, value in token_counts.items(): 
...         vec[key] = value / len(lexicon) 
...     doc_vectors.append(vec)
1

Figure 3.3. 2D thetas

Figure 3.4. City population distribution
1 a.dot(b) == np.linalg.norm(a) * np.linalg.norm(b) / np.cos(theta)
Listing 3.1. Compute cosine similarity in python
1
2
3
4
5
6
7
8
9
10
11
12
13
14
>>> import math 
>>> def cosine_sim(vec1, vec2): 
...     """ Let's convert our dictionaries to lists for easier matching.""" 
...     vec1 = [val for val in vec1.values()] 
...     vec2 = [val for val in vec2.values()] 
... 
...     dot_prod = 0 
...     for i, v in enumerate(vec1): 
...         dot_prod += v * vec2[i] 
... 
...     mag_1 = math.sqrt(sum([x**2 for x in vec1])) 
...     mag_2 = math.sqrt(sum([x**2 for x in vec2])) 
... 
...     return dot_prod / (mag_1 * mag_2)

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
>>> nltk.download('brown') 
>>> from nltk.corpus import brown 
>>> brown.words()[:10] 
 ['The', 
 'Fulton', 
 'County', 
 'Grand', 
 'Jury', 
 'said', 
 'Friday', 
 'an', 
 'investigation', 
 'of'] 
>>> brown.tagged_words()[:5] 
 [('The', 'AT'), 
 ('Fulton', 'NP-TL'), 
 ('County', 'NN-TL'), 
 ('Grand', 'JJ-TL'), 
 ('Jury', 'NN-TL')] 
>>> len(brown.words()) 
1161192
1
2
3
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
>>> from collections import Counter 
>>> puncs = set((',', '.', '--', '-', '!', '?', 
...     ':', ';', '``', "''", '(', ')', '[', ']')) 
>>> word_list = (x.lower() for x in brown.words() if x not in puncs) 
>>> token_counts = Counter(word_list) 
>>> token_counts.most_common(20) 
[('the', 69971), 
 ('of', 36412), 
 ('and', 28853), 
 ('to', 26158), 
 ('a', 23195), 
 ('in', 21337), 
 ('that', 10594), 
 ('is', 10109), 
 ('was', 9815), 
 ('he', 9548), 
 ('for', 9489), 
 ('it', 8760), 
 ('with', 7289), 
 ('as', 7253), 
 ('his', 6996), 
 ('on', 6741), 
 ('be', 6377), 
 ('at', 5372), 
 ('by', 5306), 
 ('i', 5164)]
1
2
3
4
5
6
7
8
9
10
>>> from nlpia.data.loaders import kite_text, kite_history 
>>> kite_intro = kite_text.lower() 
>>> intro_tokens = tokenizer.tokenize(kite_intro) 
>>> kite_history = kite_history.lower() 
>>> history_tokens = tokenizer.tokenize(kite_history) 
>>> intro_total = len(intro_tokens) 
>>> intro_total 
363 
>>> history_total = len(history_tokens) 
1

11 >>> history_total 
297
1
2
3
4
5
6
7
8
9
10
11
>>> intro_tf = {} 
>>> history_tf = {} 
>>> intro_counts = Counter(intro_tokens) 
>>> intro_tf['kite'] = intro_counts['kite'] / intro_total 
>>> history_counts = Counter(history_tokens) 
>>> history_tf['kite'] = history_counts['kite'] / history_total 
>>> 'Term Frequency of "kite" in intro is: {:.4f}'.format(intro_tf['kite']) 
'Term Frequency of "kite" in intro is: 0.0441' 
>>> 'Term Frequency of "kite" in history is: {:.4f}'\ 
...     .format(history_tf['kite']) 
'Term Frequency of "kite" in history is: 0.0202'
1
2
3
4
5
6
7
8
>>> intro_tf['and'] = intro_counts['and'] / intro_total 
>>> history_tf['and'] = history_counts['and'] / history_total 
>>> print('Term Frequency of "and" in intro is: {:.4f}'\ 
...     .format(intro_tf['and'])) 
Term Frequency of "and" in intro is: 0.0275 
>>> print('Term Frequency of "and" in history is: {:.4f}'\ 
...     .format(history_tf['and'])) 
Term Frequency of "and" in history is: 0.0303
1
2
3
4
>>> num_docs_containing_and = 0 
>>> for doc in [intro_tokens, history_tokens]: 
...     if 'and' in doc: 
...         num_docs_containing_and += 1

1
1
2
>>> intro_tf['china'] = intro_counts['china'] / intro_total 
>>> history_tf['china'] = history_counts['china'] / history_total
1
2
3
4
5
6
>>> num_docs = 2 
>>> intro_idf = {} 
>>> history_idf = {} 
>>> intro_idf['and'] = num_docs / num_docs_containing_and 
>>> history_idf['and'] = num_docs / num_docs_containing_and 
>>> intro_idf['kite'] = num_docs / num_docs_containing_kite 

7
8
9
>>> history_idf['kite'] = num_docs / num_docs_containing_kite 
>>> intro_idf['china'] = num_docs / num_docs_containing_china 
>>> history_idf['china'] = num_docs / num_docs_containing_china
1
2
3
4
>>> intro_tfidf = {} 
>>> intro_tfidf['and'] = intro_tf['and'] * intro_idf['and'] 
>>> intro_tfidf['kite'] = intro_tf['kite'] * intro_idf['kite'] 
>>> intro_tfidf['china'] = intro_tf['china'] * intro_idf['china']
1
2
3
4
>>> history_tfidf = {} 
>>> history_tfidf['and'] = history_tf['and'] * history_idf['and'] 
>>> history_tfidf['kite'] = history_tf['kite'] * history_idf['kite'] 
>>> history_tfidf['china'] = history_tf['china'] * history_idf['china']
1
2
3
4
5
>>> log_tf = log(term_occurences_in_doc) -\ 
...     log(num_terms_in_doc) 
>>> log_log_idf = log(log(total_num_docs) -\ 
...     log(num_docs_containing_term)) 
>>> log_tf_idf = log_tf + log_idf
1
2
3
1
2
3
4
5
6
7
>>> document_tfidf_vectors = [] 
>>> for doc in docs: 
...     vec = copy.copy(zero_vector) 
...     tokens = tokenizer.tokenize(doc.lower()) 
...     token_counts = Counter(tokens) 
... 
...     for key, value in token_counts.items(): 
1

8
9
10
11
12
13
14
15
16
17
18
...         docs_containing_key = 0 
...         for _doc in docs: 
...             if key in _doc: 
...                 docs_containing_key += 1 
...         tf = value / len(lexicon) 
...         if docs_containing_key: 
...             idf = len(docs) / docs_containing_key 
...         else: 
...             idf = 0 
...         vec[key] = tf * idf 
...     document_tfidf_vectors.append(vec)
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
>>> query = "How long does it take to get to the store?" 
>>> query_vec = copy.copy(zero_vector) 
>>> query_vec = copy.copy(zero_vector) 
 
>>> tokens = tokenizer.tokenize(query.lower()) 
>>> token_counts = Counter(tokens) 
  
>>> for key, value in token_counts.items(): 
...     docs_containing_key = 0 
...     for _doc in documents: 
...       if key in _doc.lower(): 
...         docs_containing_key += 1 
...     if docs_containing_key == 0: 
...         continue 
...     tf = value / len(tokens) 
...     idf = len(documents) / docs_containing_key 
...    query_vec[key] = tf * idf 
>>> cosine_sim(query_vec, document_tfidf_vectors[0]) 
0.5235048549676834 
>>> cosine_sim(query_vec, document_tfidf_vectors[1]) 
0.0 
>>> cosine_sim(query_vec, document_tfidf_vectors[2]) 
0.0

1
2
1
2
pip install scipy 
pip install sklearn
1
2
3
4
>>> from sklearn.feature_extraction.text import TfidfVectorizer 
>>> corpus = docs 
>>> vectorizer = TfidfVectorizer(min_df=1) 
>>> model = vectorizer.fit_transform(corpus) 
1

Table 3.1. Alternative TF-IDF normalization approaches (Molino 2017) (view table
Øgure)
Scheme
Denition
None
wij = fij
TF-
IDF
TF-
ICF
Okapi
BM25
ATC
LTU
MI
PosMI
wij = max(0, MI)
T-Test
x2
See section 4.3.5 of From Distributional to Semantic Similarity
(https://www.era.lib.ed.ac.uk/bitstream/handle/1842/563/IP030023.pdf#subsection.4.3
by James Richard Curran
Lin98a
Lin98b
5
6
7
8
9
10
11
>>> print(model.todense().round(2)) 
[[0.16 0.   0.48 0.21 0.21 0.   0.25 0.21 0.   0.   0.   0.21 0.   0.64 
  0.21 0.21] 
 [0.37 0.   0.37 0.   0.   0.37 0.29 0.   0.37 0.37 0.   0.   0.49 0. 
  0.   0.  ] 
 [0.   0.75 0.   0.   0.   0.29 0.22 0.   0.29 0.29 0.38 0.   0.   0. 
  0.   0.  ]]
2

Scheme
Denition
Gref94
1
2
q_idf * dot(q_tf, d_tf[i]) * 1.5 /  
 (dot(q_tf, d_tf[i]) + .25 + .75 * d_num_words[i] / d_num_words.mean()))

CHAPTER 4
Figure 4.1. 3D vectors for a thought experiment about six words about pets and NYC
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
>>> topic = {} 
>>> tfidf = dict(list(zip('cat dog apple lion NYC love'.split(), 
...     np.random.rand(6)))) 
>>> topic['petness'] = (.3 * tfidf['cat'] +\ 
...                     .3 * tfidf['dog'] +\ 
...                      0 * tfidf['apple'] +\ 
...                      0 * tfidf['lion'] -\ 
...                     .2 * tfidf['NYC'] +\ 
...                     .2 * tfidf['love']) 
>>> topic['animalness']  = (.1 * tfidf['cat']  +\ 
...                         .1 * tfidf['dog'] -\ 
...                         .1 * tfidf['apple'] +\ 
...                         .5 * tfidf['lion'] +\ 
...                         .1 * tfidf['NYC'] -\ 
...                         .1 * tfidf['love']) 
>>> topic['cityness']    = ( 0 * tfidf['cat']  -\ 
...                         .1 * tfidf['dog'] +\ 
...                         .2 * tfidf['apple'] -\ 
...                         .1 * tfidf['lion'] +\ 
...                         .5 * tfidf['NYC'] +\ 
...                         .1 * tfidf['love'])
1
2
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
>>> word_vector = {} 
>>> word_vector['cat']  =  .3*topic['petness'] +\ 
...                        .1*topic['animalness'] +\ 
...                         0*topic['cityness'] 
>>> word_vector['dog']  =  .3*topic['petness'] +\ 
...                        .1*topic['animalness'] -\ 
...                        .1*topic['cityness'] 
>>> word_vector['apple']=   0*topic['petness'] -\ 
...                        .1*topic['animalness'] +\ 
...                        .2*topic['cityness'] 
>>> word_vector['lion'] =   0*topic['petness'] +\ 
...                        .5*topic['animalness'] -\ 
...                        .1*topic['cityness'] 
>>> word_vector['NYC']  = -.2*topic['petness'] +\ 
...                        .1*topic['animalness'] +\ 
...                        .5*topic['cityness'] 
>>> word_vector['love'] =  .2*topic['petness'] -\ 
...                        .1*topic['animalness'] +\ 
...                        .1*topic['cityness']

Listing 4.1. The SMS spam dataset
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
>>> import pandas as pd 
>>> from nlpia.data.loaders import get_data 
>>> pd.options.display.width = 120 
>>> sms = get_data('sms-spam') 
>>> index = ['sms{}{}'.format(i, '!'*j) for (i,j) in\ 
...     zip(range(len(sms)), sms.spam)] 
>>> sms = pd.DataFrame(sms.values, columns=sms.columns, index=index) 
>>> sms['spam'] = sms.spam.astype(int) 
>>> len(sms) 
4837 
>>> sms.spam.sum() 
638 
>>> sms.head(6) 
      spam                                               text 
sms0     0  Go until jurong point, crazy.. Available only ... 
sms1     0                      Ok lar... Joking wif u oni... 
sms2!    1  Free entry in 2 a wkly comp to win FA Cup fina... 
sms3     0  U dun say so early hor... U c already then say... 
sms4     0  Nah I don't think he goes to usf, he lives aro... 
sms5!    1  FreeMsg Hey there darling it's been 3 week's n...
1
2
1
2
3
4
5
>>> from sklearn.feature_extraction.text import TfidfVectorizer 
>>> from nltk.tokenize.casual import casual_tokenize 
>>> tfidf_model = TfidfVectorizer(tokenizer=casual_tokenize) 
>>> tfidf_docs = tfidf_model.fit_transform(\ 
...     raw_documents=sms.text).toarray() 

Figure 4.2. 3D scatter plot (point cloud) of your TF-IDF vectors
6
7
8
9
>>> tfidf_docs.shape 
(4837, 9232) 
>>> sms.spam.sum() 
638
1
2
3
4
5
6
7
8
>>> mask = sms.spam.astype(bool).values 
>>> spam_centroid = tfidf_docs[mask].mean(axis=0) 
>>> ham_centroid = tfidf_docs[~mask].mean(axis=0) 
  
>>> spam_centroid.round(2) 
array([0.06, 0.  , 0.  , ..., 0.  , 0.  , 0.  ]) 
>>> ham_centroid.round(2) 
array([0.02, 0.01, 0.  , ..., 0.  , 0.  , 0.  ])
1
2
1
2
3
4
>>> spamminess_score = tfidf_docs.dot(spam_centroid -\ 
...     ham_centroid) 
>>> spamminess_score.round(2) 
array([-0.01, -0.02,  0.04, ..., -0.01, -0.  ,  0.  ])
1

1
2
3
4
5
6
7
8
9
10
11
12
>>> from sklearn.preprocessing import MinMaxScaler 
>>> sms['lda_score'] = MinMaxScaler().fit_transform(\ 
...     spamminess_score.reshape(-1,1)) 
>>> sms['lda_predict'] = (sms.lda_score > .5).astype(int) 
>>> sms['spam lda_predict lda_score'.split()].round(2).head(6) 
       spam  lda_predict  lda_score 
sms0      0            0       0.23 
sms1      0            0       0.18 
sms2!     1            1       0.72 
sms3      0            0       0.18 
sms4      0            0       0.29 
sms5!     1            1       0.55
1
2
>>> (1. - (sms.spam - sms.lda_predict).abs().sum() / len(sms)).round(3) 
0.977

1
2
3
4
5
6
>>> from pugnlp.stats import Confusion 
>>> Confusion(sms['spam lda_predict'.split()]) 
lda_predict     0    1 
spam 
0            4135   64 
1              45  593
Listing 4.2. Topic-word matrix for LSA on 16 short sentences about cats, dogs, and NYC
1
2
3
4
5
6
7
>>> from nlpia.book.examples.ch04_catdog_lsa_3x6x16\ 
...     import word_topic_vectors 
>>> word_topic_vectors.T.round(1) 
      cat  dog  apple  lion  nyc  love 
top0 -0.6 -0.4    0.5  -0.3  0.4  -0.1 
top1 -0.1 -0.3   -0.4  -0.1  0.1   0.8 
top2 -0.3  0.8   -0.1  -0.5  0.0   0.1
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
>>> from nlpia.book.examples.ch04_catdog_lsa_sorted\ 
...     import lsa_models, prettify_tdm 
>>> bow_svd, tfidf_svd = lsa_models() 
>>> prettify_tdm(**bow_svd) 
   cat dog apple lion nyc love 
text 
0            1        1                                 NYC is the Big Apple. 
1            1        1                        NYC is known as the Big Apple. 
2                     1    1                                      I love NYC! 
3            1        1           I wore a hat to the Big Apple party in NYC. 
4            1        1                       Come to NYC. See the Big Apple! 
5            1                             Manhattan is called the Big Apple. 
6    1                                New York is a big city for a small cat. 
7    1            1           The lion, a big cat, is the king of the jungle. 
8    1                     1                               I love my pet cat. 
9                     1    1                      I love New York City (NYC). 
10   1   1                                            Your dog chased mycat.
1
1
2
3
4
5
6
7
8
9
>>> tdm = bow_svd['tdm'] 
>>> tdm 
        0   1   2   3   4   5   6   7   8   9   10 
cat     0   0   0   0   0   0   1   1   1   0    1 
dog     0   0   0   0   0   0   0   0   0   0    1 
apple   1   1   0   1   1   1   0   0   0   0    0 
lion    0   0   0   0   0   0   0   1   0   0    0 
nyc     1   1   1   1   1   0   0   0   0   1    0 
love    0   0   1   0   0   0   0   0   1   1    0

Wmxn ⇒ Umxp Spxp VpxnT
Listing 4.3. Umxp
1
2
3
4
5
6
7
8
9
10
11
>>> import numpy as np 
>>> U, s, Vt = np.linalg.svd(tdm) 
>>> import pandas as pd 
>>> pd.DataFrame(U, index=tdm.index).round(2) 
          0     1     2     3     4     5 
cat   -0.04  0.83 -0.38 -0.00  0.11 -0.38 
dog   -0.00  0.21 -0.18 -0.71 -0.39  0.52 
apple -0.62 -0.21 -0.51  0.00  0.49  0.27 
lion  -0.00  0.21 -0.18  0.71 -0.39  0.52 
nyc   -0.75 -0.00  0.24 -0.00 -0.52 -0.32 
love  -0.22  0.42  0.69  0.00  0.41  0.37
1
Listing 4.4. Spxp
1
2
3
4
5
6
7
8
9
10
11
12
>>> s.round(1) 
array([3.1, 2.2, 1.8, 1. , 0.8, 0.5]) 
>>> S = np.zeros((len(U), len(Vt))) 
>>> pd.np.fill_diagonal(S, s) 
>>> pd.DataFrame(S).round(1) 
    0    1    2    3    4    5    6    7    8    9    10 
0  3.1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0 
1  0.0  2.2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0 
2  0.0  0.0  1.8  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0 
3  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0 
4  0.0  0.0  0.0  0.0  0.8  0.0  0.0  0.0  0.0  0.0  0.0 
5  0.0  0.0  0.0  0.0  0.0  0.5  0.0  0.0  0.0  0.0  0.0
Listing 4.5. VpxnT
1
2
3
4
5
6
7
8
9
10
11
>>> pd.DataFrame(Vt).round(2) 
      0     1     2     3     4     5     6     7     8     9     10 
0  -0.44 -0.44 -0.31 -0.44 -0.44 -0.20 -0.01 -0.01 -0.08 -0.31 -0.01 
1  -0.09 -0.09  0.19 -0.09 -0.09 -0.09  0.37  0.47  0.56  0.19  0.47 
2  -0.16 -0.16  0.52 -0.16 -0.16 -0.29 -0.22 -0.32  0.17  0.52 -0.32 
3   0.00 -0.00 -0.00  0.00  0.00  0.00 -0.00  0.71  0.00 -0.00 -0.71 
4  -0.04 -0.04 -0.14 -0.04 -0.04  0.58  0.13 -0.33  0.62 -0.14 -0.33 
5  -0.09 -0.09  0.10 -0.09 -0.09  0.51 -0.73  0.27 -0.01  0.10  0.27 
6  -0.57  0.21  0.11  0.33 -0.31  0.34  0.34 -0.00 -0.34  0.23  0.00 
7  -0.32  0.47  0.25 -0.63  0.41  0.07  0.07  0.00 -0.07 -0.18  0.00 
8  -0.50  0.29 -0.20  0.41  0.16 -0.37 -0.37 -0.00  0.37 -0.17  0.00 

Figure 4.3. Term-document matrix reconstruction accuracy decreases as you ignore
more dimensions.
Figure 4.4. Looking up from below the “belly” at the point cloud for a real object
12
13
9  -0.15 -0.15 -0.59 -0.15  0.42  0.04  0.04 -0.00 -0.04  0.63 -0.00 
10 -0.26 -0.62  0.33  0.24  0.54  0.09  0.09 -0.00 -0.09 -0.23 -0.00
Listing 4.6. Term-document matrix reconstruction error
1
2
3
4
5
6
7
8
9
>>> err = [] 
>>> for numdim in range(len(s), 0, -1): 
...     S[numdim - 1, numdim - 1] = 0 
...     reconstructed_tdm = U.dot(S).dot(Vt) 
...     err.append(np.sqrt(((\ 
...         reconstructed_tdm - tdm).values.flatten() ** 2).sum() 
...         / np.product(tdm.shape))) 
>>> np.array(err).round(2) 
array([0.06, 0.12, 0.17, 0.28, 0.39, 0.55])

Figure 4.5. Head-to-head horse point clouds upside-down
1
2
3
4
5
6
7
8
9
10
11
12
>>> import pandas as pd 
>>> pd.set_option('display.max_columns', 6) 
>>> from sklearn.decomposition import PCA 
>>> import seaborn 
>>> from matplotlib import pyplot as plt 
>>> from nlpia.data.loaders import get_data 
  
>>> df = get_data('pointcloud').sample(1000) 
>>> pca = PCA(n_components=2) 
>>> df2d = pd.DataFrame(pca.fit_transform(df), columns=list('xy')) 
>>> df2d.plot(kind='scatter', x='x', y='y') 
>>> plt.show()
1
2
3

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
>>> import pandas as pd 
>>> from nlpia.data.loaders import get_data 
>>> pd.options.display.width = 120 
  
>>> sms = get_data('sms-spam') 
>>> index = ['sms{}{}'.format(i, '!'*j)  
 for (i,j) in zip(range(len(sms)), sms.spam)] 
>>> sms.index = index 
>>> sms.head(6) 
  
       spam                                               text 
sms0      0  Go until jurong point, crazy.. Available only ... 
sms1      0                      Ok lar... Joking wif u oni... 
sms2!     1  Free entry in 2 a wkly comp to win FA Cup fina... 
sms3      0  U dun say so early hor... U c already then say... 
sms4      0  Nah I don't think he goes to usf, he lives aro... 
sms5!     1  FreeMsg Hey there darling it's been 3 week's n...
1
2
1
2
3
4
5
6
7
8
9
10
11
12
13
14
>>> from sklearn.feature_extraction.text import TfidfVectorizer 
>>> from nltk.tokenize.casual import casual_tokenize 
  
>>> tfidf = TfidfVectorizer(tokenizer=casual_tokenize) 
>>> tfidf_docs = tfidf.fit_transform(raw_documents=sms.text).toarray() 
>>> len(tfidf.vocabulary_) 
9232 
  
>>> tfidf_docs = pd.DataFrame(tfidf_docs) 
>>> tfidf_docs = tfidf_docs - tfidf_docs.mean() 
>>> tfidf_docs.shape 
(4837, 9232) 
>>> sms.spam.sum() 
638
1
2
3

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
>>> from sklearn.decomposition import PCA 
  
>>> pca = PCA(n_components=16) 
>>> pca = pca.fit(tfidf_docs) 
>>> pca_topic_vectors = pca.transform(tfidf_docs) 
>>> columns = ['topic{}'.format(i) for i in range(pca.n_components)] 
>>> pca_topic_vectors = pd.DataFrame(pca_topic_vectors, columns=columns,\ 
...     index=index) 
>>> pca_topic_vectors.round(3).head(6) 
       topic0  topic1  topic2   ...     topic13  topic14  topic15 
sms0    0.201   0.003   0.037   ...      -0.026   -0.019    0.039 
sms1    0.404  -0.094  -0.078   ...      -0.036    0.047   -0.036 
sms2!  -0.030  -0.048   0.090   ...      -0.017   -0.045    0.057 
sms3    0.329  -0.033  -0.035   ...      -0.065    0.022   -0.076 
sms4    0.002   0.031   0.038   ...       0.031   -0.081   -0.021 
sms5!  -0.016   0.059   0.014   ...       0.077   -0.015    0.021
1
2
3
4
5
6
7
8
9
10
11
12
13
14
>>> tfidf.vocabulary_ 
{'go': 3807, 
 'until': 8487, 
 'jurong': 4675, 
 'point': 6296, 
... 
>>> column_nums, terms = zip(*sorted(zip(tfidf.vocabulary_.values(),\ 
...     tfidf.vocabulary_.keys()))) 
>>> terms 
('!', 
 '"', 
 '#', 
 '#150', 
...
1
1
2
3
4
5
6
7
8
9
>>> weights = pd.DataFrame(pca.components_, columns=terms, 
 index=['topic{}'.format(i) for i in range(16)]) 
>>> pd.options.display.max_columns = 8 
>>> weights.head(4).round(3) 
            !      "      #  ...      ...      ?    ?ud      ? 
topic0 -0.071  0.008 -0.001  ...   -0.002  0.001  0.001  0.001 
topic1  0.063  0.008  0.000  ...    0.003  0.001  0.001  0.001 
topic2  0.071  0.027  0.000  ...    0.002 -0.001 -0.001 -0.001 
topic3 -0.059 -0.032 -0.001  ...    0.001  0.001  0.001  0.001
1
2
3
4
5
>>> pd.options.display.max_columns = 12 
>>> deals = weights['! ;) :) half off free crazy deal only $ 80 %'.split()].r 
     ound(3) * 100 
>>> deals 
            !   ;)    :)  half  off  free  crazy  deal  only    $   80    % 

6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
topic0   -7.1  0.1  -0.5  -0.0 -0.4  -2.0   -0.0  -0.1  -2.2  0.3 -0.0 -0.0 
topic1    6.3  0.0   7.4   0.1  0.4  -2.3   -0.2  -0.1  -3.8 -0.1 -0.0 -0.2 
topic2    7.1  0.2  -0.1   0.1  0.3   4.4    0.1  -0.1   0.7  0.0  0.0  0.1 
topic3   -5.9 -0.3  -7.1   0.2  0.3  -0.2    0.0   0.1  -2.3  0.1 -0.1 -0.3 
topic4   38.1 -0.1 -12.5  -0.1 -0.2   9.9    0.1  -0.2   3.0  0.3  0.1 -0.1 
topic5  -26.5  0.1  -1.5  -0.3 -0.7  -1.4   -0.6  -0.2  -1.8 -0.9  0.0  0.0 
topic6  -10.9 -0.5  19.9  -0.4 -0.9  -0.6   -0.2  -0.1  -1.4 -0.0 -0.0 -0.1 
topic7   16.4  0.1 -18.2   0.8  0.8  -2.9    0.0   0.0  -1.9 -0.3  0.0 -0.1 
topic8   34.6  0.1   5.2  -0.5 -0.5  -0.1   -0.4  -0.4   3.3 -0.6 -0.0 -0.2 
topic9    6.9 -0.3  17.4   1.4 -0.9   6.6   -0.5  -0.4   3.3 -0.4 -0.0  0.0 
... 
>>> deals.T.sum() 
topic0    -11.9 
topic1      7.5 
topic2     12.8 
topic3    -15.5 
topic4     38.3 
topic5    -33.8 
topic6      4.8 
topic7     -5.3 
topic8     40.5 
topic9     33.1 
...
1
2
3
4
5
6
7
8
9
10
11
12
13
14
>>> from sklearn.decomposition import TruncatedSVD 
  
>>> svd = TruncatedSVD(n_components=16, n_iter=100) 
>>> svd_topic_vectors = svd.fit_transform(tfidf_docs.values) 
>>> svd_topic_vectors = pd.DataFrame(svd_topic_vectors, columns=columns,\ 
...     index=index) 
>>> svd_topic_vectors.round(3).head(6) 
       topic0  topic1  topic2   ...     topic13  topic14  topic15 
sms0    0.201   0.003   0.037   ...      -0.036   -0.014    0.037 
sms1    0.404  -0.094  -0.078   ...      -0.021    0.051   -0.042 
sms2!  -0.030  -0.048   0.090   ...      -0.020   -0.042    0.052 
sms3    0.329  -0.033  -0.035   ...      -0.046    0.022   -0.070 
sms4    0.002   0.031   0.038   ...       0.034   -0.083   -0.021 
sms5!  -0.016   0.059   0.014   ...       0.075   -0.001    0.020

1
2
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
>>> import numpy as np 
  
>>> svd_topic_vectors = (svd_topic_vectors.T / np.linalg.norm(\ 
...     svd_topic_vectors, axis=1)).T 
>>> svd_topic_vectors.iloc[:10].dot(svd_topic_vectors.iloc[:10].T).round(1) 
       sms0  sms1  sms2!  sms3  sms4  sms5!  sms6  sms7  sms8!  sms9! 
sms0    1.0   0.6   -0.1   0.6  -0.0   -0.3  -0.3  -0.1   -0.3   -0.3 
sms1    0.6   1.0   -0.2   0.8  -0.2    0.0  -0.2  -0.2   -0.1   -0.1 
sms2!  -0.1  -0.2    1.0  -0.2   0.1    0.4   0.0   0.3    0.5    0.4 
sms3    0.6   0.8   -0.2   1.0  -0.2   -0.3  -0.1  -0.3   -0.2   -0.1 
sms4   -0.0  -0.2    0.1  -0.2   1.0    0.2   0.0   0.1   -0.4   -0.2 
sms5!  -0.3   0.0    0.4  -0.3   0.2    1.0  -0.1   0.1    0.3    0.4 
sms6   -0.3  -0.2    0.0  -0.1   0.0   -0.1   1.0   0.1   -0.2   -0.2 
sms7   -0.1  -0.2    0.3  -0.3   0.1    0.1   0.1   1.0    0.1    0.4 
sms8!  -0.3  -0.1    0.5  -0.2  -0.4    0.3  -0.2   0.1    1.0    0.3 
sms9!  -0.3  -0.1    0.4  -0.1  -0.2    0.4  -0.2   0.4    0.3    1.0
1

1
2
3
4
5
6
>>> total_corpus_len = 0 
>>> for document_text in sms.text: 
...     total_corpus_len += len(casual_tokenize(document_text)) 
>>> mean_document_len = total_corpus_len / len(sms) 
>>> round(mean_document_len, 2) 
21.35
1
2
>>> sum([len(casual_tokenize(t)) for t in sms.text]) * 1. / len(sms.text) 
21.35
1
2
3
4
5
6
7
8
9
10
>>> from sklearn.feature_extraction.text import CountVectorizer 
>>> from nltk.tokenize import casual_tokenize 
>>> np.random.seed(42) 
  
>>> counter = CountVectorizer(tokenizer=casual_tokenize) 
>>> bow_docs = pd.DataFrame(counter.fit_transform(raw_documents=sms.text)\ 
...     .toarray(), index=index) 
>>> column_nums, terms = zip(*sorted(zip(counter.vocabulary_.values(),\ 
...     counter.vocabulary_.keys()))) 
>>> bow_docs.columns = terms
1
2
3
4
5
6
7
8
9
10
>>> sms.loc['sms0'].text 
'Go until jurong point, crazy.. Available only in bugis n great world la e 
buffet... Cine there got amore wat...' 
>>> bow_docs.loc['sms0'][bow_docs.loc['sms0'] > 0].head() 
,            1 
..           1 
...          2 
amore        1 
available    1 
Name: sms0, dtype: int64

1
2
3
4
5
6
>>> from sklearn.decomposition import LatentDirichletAllocation as LDiA 
  
>>> ldia = LDiA(n_components=16, learning_method='batch') 
>>> ldia = ldia.fit(bow_docs) 
>>> ldia.components_.shape 
(16, 9232)
1
1
2
3
4
5
6
7
8
>>> pd.set_option('display.width', 75) 
>>> components = pd.DataFrame(ldia.components_.T, index=terms,\ 
...     columns=columns) 
>>> components.round(2).head(3) 
       topic0  topic1  topic2   ...     topic13  topic14  topic15 
!      184.03   15.00   72.22   ...      297.29    41.16    11.70 
"        0.68    4.22    2.41   ...       62.72    12.27     0.06 
#        0.06    0.06    0.06   ...        4.05     0.06     0.06
1
2
3
4
5
6
7
8
9
10
11
>>> components.topic3.sort_values(ascending=False)[:10] 
!       394.952246 
.       218.049724 
to      119.533134 
u       118.857546 
call    111.948541 
£       107.358914 
,        96.954384 
*        90.314783 
your     90.215961 
is       75.750037
1
2
3
4
5
6
7
8
9
10
>>> ldia16_topic_vectors = ldia.transform(bow_docs) 
>>> ldia16_topic_vectors = pd.DataFrame(ldia16_topic_vectors,\ 
...     index=index, columns=columns) 
>>> ldia16_topic_vectors.round(2).head() 
       topic0  topic1  topic2   ...     topic13  topic14  topic15 
sms0     0.00    0.62    0.00   ...        0.00     0.00     0.00 
sms1     0.01    0.01    0.01   ...        0.01     0.01     0.01 
sms2!    0.00    0.00    0.00   ...        0.00     0.00     0.00 
sms3     0.00    0.00    0.00   ...        0.00     0.00     0.00 
sms4     0.39    0.00    0.33   ...        0.00     0.00     0.00

1
2
3
4
5
6
7
8
9
10
>>> from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA 
  
>>> X_train, X_test, y_train, y_test = 
 train_test_split(ldia16_topic_vectors, sms.spam, test_size=0.5, 
 random_state=271828) 
>>> lda = LDA(n_components=1) 
>>> lda = lda.fit(X_train, y_train) 
>>> sms['ldia16_spam'] = lda.predict(ldia16_topic_vectors) 
>>> round(float(lda.score(X_test, y_test)), 2) 
0.94
1
2
1
2
3
>>> from itertools import product 
>>> all_pairs = [(word1, word2) for (word1, word2) in product(word_list, 
 word_list) if not word1 == word2]
1
2
3
4
5
6
7
8
9
10
11
12
13
14
>>> from sklearn.feature_extraction.text import TfidfVectorizer 
>>> from nltk.tokenize.casual import casual_tokenize 
>>> tfidf = TfidfVectorizer(tokenizer=casual_tokenize) 
>>> tfidf_docs = tfidf.fit_transform(raw_documents=sms.text).toarray() 
>>> tfidf_docs = tfidf_docs - tfidf_docs.mean(axis=0)  
  
>>> X_train, X_test, y_train, y_test = train_test_split(tfidf_docs,\ 
...     sms.spam.values, test_size=0.5, random_state=271828) 
>>> lda = LDA(n_components=1) 
>>> lda = lda.fit(X_train, y_train) 
>>> round(float(lda.score(X_train, y_train)), 3) 
1.0 
>>> round(float(lda.score(X_test, y_test)), 3) 
0.748
1
2
1
2
3
4
>>> ldia32 = LDiA(n_components=32, learning_method='batch') 
>>> ldia32 = ldia32.fit(bow_docs) 
>>> ldia32.components_.shape 
(32, 9232)
1
2
3
4
5
>>> ldia32_topic_vectors = ldia32.transform(bow_docs) 
>>> columns32 = ['topic{}'.format(i) for i in range(ldia32.n_components)] 
>>> ldia32_topic_vectors = pd.DataFrame(ldia32_topic_vectors, index=index,\ 
...     columns=columns32) 
>>> ldia32_topic_vectors.round(2).head() 

DIGGING DEEPER INTO YOUR TOOLBOX
You can nd the source code path in the __le__ attribute on any
Python module, such as sklearn.__file__ . And in ipython (jupyter
console), you can view the source code for any function, class, or object
with ?? , like LDA?? :
6
7
8
9
10
11
       topic0  topic1  topic2   ...     topic29  topic30  topic31 
sms0     0.00     0.5     0.0   ...         0.0      0.0      0.0 
sms1     0.00     0.0     0.0   ...         0.0      0.0      0.0 
sms2!    0.00     0.0     0.0   ...         0.0      0.0      0.0 
sms3     0.00     0.0     0.0   ...         0.0      0.0      0.0 
sms4     0.21     0.0     0.0   ...         0.0      0.0      0.0
1
2
3
4
5
6
7
8
9
10
11
12
>>> X_train, X_test, y_train, y_test = 
 train_test_split(ldia32_topic_vectors, sms.spam, test_size=0.5, 
 random_state=271828) 
>>> lda = LDA(n_components=1) 
>>> lda = lda.fit(X_train, y_train) 
>>> sms['ldia32_spam'] = lda.predict(ldia32_topic_vectors) 
>>> X_train.shape 
(2418, 32) 
>>> round(float(lda.score(X_train, y_train)), 3) 
0.924 
>>> round(float(lda.score(X_test, y_test)), 3) 
0.927
1
2
>>> import sklearn 
>>> sklearn.__file__ 
'/Users/hobs/anaconda3/envs/conda_env_nlpia/lib/python3.6/site-packa
earn/__init__.py' 
>>> from sklearn.discriminant_analysis\ 
...     import LinearDiscriminantAnalysis as LDA 
>>> LDA?? 
Init signature: LDA(solver='svd', shrinkage=None, priors=None, n_com
=None, store_covariance=False, tol=0.0001) 
Source: 
class LinearDiscriminantAnalysis(BaseEstimator, LinearClassifierMixi
                                 TransformerMixin): 
    """Linear Discriminant Analysis 
  
    A classifier with a linear decision boundary, generated by fitti
    class conditional densities to the data and using Bayes' rule. 
  
    The model fits a Gaussian density to each class, assuming that a
    classes share the same covariance matrix. 
...

This won’t work on functions and classes that are extensions, whose
source code is hidden within a compiled C++ module.
Listing 4.7. Pairwise distances available in sklearn
1
2
3
4
5
'cityblock', 'cosine', 'euclidean', 'l1', 'l2', 'manhattan', 'braycurtis', 
'canberra', 'chebyshev', 'correlation', 'dice', 'hamming', 'jaccard', 
'kulsinski', 'mahalanobis', 'matching', 'minkowski', 'rogerstanimoto', 
'russellrao', 'seuclidean', 'sokalmichener', 'sokalsneath', 'sqeuclidean', 
'yule'
1
2
>>> similarity = 1. / (1. + distance) 
>>> distance = (1. / similarity) - 1.
1
2
>>> similarity = 1. - distance 
>>> distance = 1. - similarity
1
2
3
4
>>> import math 
>>> angular_distance = math.acos(cosine_similarity) / math.pi 
>>> distance = 1. / similarity - 1. 
>>> similarity = 1. - distance
1
2
3
4
5
6
7
8
9
>>> lda = LDA(n_components=1) 
>>> lda = lda.fit(tfidf_docs, sms.spam) 
>>> sms['lda_spaminess'] = lda.predict(tfidf_docs) 
>>> ((sms.spam - sms.lda_spaminess) ** 2.).sum() ** .5 
0.0 
>>> (sms.spam == sms.lda_spaminess).sum() 
4837 
>>> len(sms) 
4837

Figure 4.6. Semantic search accuracy deteriorates at around 12-D.
1
2
3
4
5
>>> from sklearn.model_selection import cross_val_score 
>>> lda = LDA(n_components=1) 
>>> scores = cross_val_score(lda, tfidf_docs, sms.spam, cv=5) 
>>> "Accuracy: {:.2f} (+/-{:.2f})".format(scores.mean(), scores.std() * 2) 
'Accuracy: 0.76 (+/-0.03)'
1
2
3
4
5
6
7
8
9
>>> from sklearn.model_selection import train_test_split 
>>> X_train, X_test, y_train, y_test = train_test_split(tfidf_docs,\ 
...     sms.spam, test_size=0.33, random_state=271828) 
>>> lda = LDA(n_components=1) 
>>> lda.fit(X_train, y_train) 
LinearDiscriminantAnalysis(n_components=1, priors=None, shrinkage=None, 
              solver='svd', store_covariance=False, tol=0.0001) 
>>> lda.score(X_test, y_test).round(3) 
0.765
1
2
3
4
5
6
7
8
9
10
11
12
13
>>> X_train, X_test, y_train, y_test =  
 train_test_split(pca_topicvectors.values, sms.spam, test_size=0.3,  
 random_state=271828) 
>>> lda = LDA(n_components=1) 
>>> lda.fit(X_train, y_train) 
LinearDiscriminantAnalysis(n_components=1, priors=None, shrinkage=None, 
              solver='svd', store_covariance=False, tol=0.0001) 
>>> lda.score(X_test, y_test).round(3) 
0.965 
>>> lda = LDA(n_components=1) 
>>> scores = cross_val_score(lda, pca_topicvectors, sms.spam, cv=10) 
>>> "Accuracy: {:.3f} (+/-{:.3f})".format(scores.mean(), scores.std() * 2) 
'Accuracy: 0.958 (+/-0.022)'


CHAPTER 5
Figure 5.1. Neuron cell
Figure 5.2. Basic perceptron

Figure 5.3. A perceptron and a biological neuron
Equation 5.1. Threshold activation function
1
2
3
4
5
6
7
8
9
10
11
12
13
>>> import numpy as np 
  
>>> example_input = [1, .2, .1, .05, .2] 
>>> example_weights = [.2, .12, .4, .6, .90] 
  
>>> input_vector = np.array(example_input) 
>>> weights = np.array(example_weights) 
>>> bias_weight = .2 
  
>>> activation_level = np.dot(input_vector, weights) +\ 
...     (bias_weight * 1) 
>>> activation_level 
0.674
1
1
2
3
4
5
6
7
>>> threshold = 0.5 
>>> if activation_level >= threshold: 
...    perceptron_output = 1 
... else: 
...    perceptron_output = 0 
>>> perceptron_output) 
1


1
2
3
4
5
6
7
8
9
10
11
>>> expected_output = 0 
>>> new_weights = [] 
>>> for i, x in enumerate(example_input): 
...     new_weights.append(weights[i] + (expected_output -\ 
...         perceptron_output) * x) 
 >>> weights = np.array(new_weights) 
  
>>> example_weights 
[0.2, 0.12, 0.4, 0.6, 0.9] 
>>> weights 
[-0.8  -0.08  0.3   0.55  0.7]
1
2
3
Listing 5.1. OR problem setup
1
2
3
4
5
6
7
8
9
10
11
>>> sample_data = [[0, 0],  # False, False 
...                [0, 1],  # False, True 
...                [1, 0],  # True, False 
...                [1, 1]]  # True, True 
  
>>> expected_results = [0,  # (False OR False) gives False 
...                     1,  # (False OR True ) gives True 
...                     1,  # (True  OR False) gives True 
...                     1]  # (True  OR True ) gives True 
  
>>> activation_threshold = 0.5
1
2
3
4
5
6
>>> from random import random 
>>> import numpy as np 
  
>>> weights = np.random.random(2)/1000  # Small random float 0 < w < .001 
>>> weights 
[5.62332144e-04 7.69468028e-05]
1
2
3
>>> bias_weight = np.random.random() / 1000 
>>> bias_weight 
0.0009984699077277136

Listing 5.2. Perceptron random guessing
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
>>> for idx, sample in enumerate(sample_data): 
...     input_vector = np.array(sample) 
...     activation_level = np.dot(input_vector, weights) +\ 
...         (bias_weight * 1) 
...     if activation_level > activation_threshold: 
...         perceptron_output = 1 
...     else: 
...         perceptron_output = 0 
...     print('Predicted {}'.format(perceptron_output)) 
...     print('Expected: {}'.format(expected_results[idx])) 
...     print() 
Predicted 0 
Expected: 0 
  
Predicted 0 
Expected: 1 
  
Predicted 0 
Expected: 1 
  
Predicted 0 
Expected: 1
Listing 5.3. Perceptron learning
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
>>> for iteration_num in range(5): 
...     correct_answers = 0 
...     for idx, sample in enumerate(sample_data): 
...         input_vector = np.array(sample) 
...         weights = np.array(weights) 
...         activation_level = np.dot(input_vector, weights) +\ 
...             (bias_weight * 1) 
...         if activation_level > activation_threshold: 
...             perceptron_output = 1 
...         else: 
...             perceptron_output = 0 
...         if perceptron_output == expected_results[idx]: 
...             correct_answers += 1 
...         new_weights = [] 
...         for i, x in enumerate(sample): 
...             new_weights.append(weights[i] + (expected_results[idx] -\ 
...                 perceptron_output) * x) 
...         bias_weight = bias_weight + ((expected_results[idx] -\ 
...             perceptron_output) * 1) 
...         weights = np.array(new_weights) 
...     print('{} correct answers out of 4, for iteration {}'\ 
...         .format(correct_answers, iteration_num)) 
3 correct answers out of 4, for iteration 0 
2 correct answers out of 4, for iteration 1 
3 correct answers out of 4, for iteration 2 
4 correct answers out of 4, for iteration 3 
4 correct answers out of 4, for iteration 4
1
2

Figure 5.4. Linearly separable data
Figure 5.5. Nonlinearly separable data

Equation 5.2. Error between truth and prediction
Equation 5.3. Cost function you want to minimize
Figure 5.6. Neural net with hidden weights

Figure 5.7. Fully connected neural net
Equation 5.4. Sigmoid function
Equation 5.5. Mean squared error

Equation 5.6. Chain rule
Equation 5.7. Error derivative
Equation 5.8. Derivative of the previous layer
Figure 5.8. Convex error curve

Figure 5.9. Nonconvex error curve

Listing 5.4. XOR Keras network
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
>>> import numpy as np 
>>> from keras.models import Sequential 
>>> from keras.layers import Dense, Activation 
>>> from keras.optimizers import SGD 
>>> # Our examples for an exclusive OR. 
>>> x_train = np.array([[0, 0], 
...                     [0, 1], 
...                     [1, 0], 
...                     [1, 1]]) 
>>> y_train = np.array([[0], 
...                     [1], 
...                     [1], 
...                     [0]]) 
>>> model = Sequential() 
>>> num_neurons = 10 
>>> model.add(Dense(num_neurons, input_dim=2)) 
>>> model.add(Activation('tanh')) 
>>> model.add(Dense(1)) 
>>> model.add(Activation('sigmoid')) 
>>> model.summary() 
Layer (type)                 Output Shape              Param # 
================================================================= 
dense_18 (Dense)             (None, 10)                30 
_________________________________________________________________ 
activation_6 (Activation)    (None, 10)                0 
_________________________________________________________________ 
dense_19 (Dense)             (None, 1)                 11 
_________________________________________________________________ 
activation_7 (Activation)    (None, 1)                 0 
================================================================= 
Total params: 41.0 
Trainable params: 41.0 
Non-trainable params: 0.0
1
2
3
4
5
6
7
8
1
2
3
>>> sgd = SGD(lr=0.1) 
>>> model.compile(loss='binary_crossentropy', optimizer=sgd, 
...     metrics=['accuracy'])
1
2
3
4
5
>>> model.predict(x_train) 
[[ 0.5       ] 
 [ 0.43494844] 
 [ 0.50295198] 
 [ 0.42517585]]

Listing 5.5. Fit model to the XOR training set
1
2
3
4
5
6
7
8
9
10
model.fit(x_train, y_train, epochs=100) 
Epoch 1/100 
4/4 [==============================] - 0s - loss: 0.6917 - acc: 0.7500 
Epoch 2/100 
4/4 [==============================] - 0s - loss: 0.6911 - acc: 0.5000 
Epoch 3/100 
4/4 [==============================] - 0s - loss: 0.6906 - acc: 0.5000 
... 
Epoch 100/100 
4/4 [==============================] - 0s - loss: 0.6661 - acc: 1.0000
1
1
2
3
4
5
6
7
8
9
10
11
12
>>> model.predict_classes(x_train)) 
4/4 [==============================] - 0s 
[[0] 
 [1] 
 [1] 
 [0]] 
>>> model.predict(x_train)) 
4/4 [==============================] - 0s 
[[ 0.0035659 ] 
 [ 0.99123639] 
 [ 0.99285167] 
 [ 0.00907462]]
Listing 5.6. Save the trained model
1
2
3
4
5
6
7
>>> import h5py 
>>> model_structure = model.to_json() 
  
>>> with open("basic_model.json", "w") as json_file: 
...     json_file.write(model_structure) 
  
>>> model.save_weights("basic_weights.h5")
1
2
1 input_vec = [2, 275000]


CHAPTER 6
1
2
>>> answer_vector = wv['woman'] + wv['Europe'] + wv[physics'] +\ 
...     wv['scientist']
1
2
>>> answer_vector = wv['woman'] + wv['Europe'] + wv[physics'] +\ 
...     wv['scientist'] - wv['male'] - 2 * wv['man']
1 >>> answer_vector = wv['Louis_Pasteur'] - wv['germs'] + wv['physics']
1 MARIE CURIE : SCIENCE :: ? : MUSIC
1 >>> wv['Marie_Curie'] - wv['science'] + wv['music']
1 TIMBERS : PORTLAND :: ? : SEATTLE
1 WALK : LEGS :: ? : MOUTH

1 ANALOGY : WORDS :: ? : NUMBERS
Listing 6.1. Compute nessvector
1
2
3
4
5
6
7
>>> from nlpia.book.examples.ch06_nessvectors import * 
>>> nessvector('Marie_Curie').round(2) 
placeness     -0.46 
peopleness     0.35 
animalness     0.17 
conceptness   -0.32 
femaleness     0.26
1
2
1 wv['Timbers'] - wv['Portland'] + wv['Seattle'] = ?
1 wv['Seattle_Sounders']
1 wv['Marie_Curie'] - wv['physics'] + wv['classical_music'] = ?
1 Portland Timbers + Seattle - Portland = ?

Figure 6.1. Geometry of Word2vec math
Equation 6.2. Compute the answer to the soccer team question
Equation 6.2. Distance between the singular and plural versions of a word
1 "San Francisco is to California as what is to Colorado?"San Francisco - California + 

Figure 6.2. Word vectors for ten US cities projected onto a 2D map
Figure 6.3. Training input and output example for the skip-gram approach
Equation 6.3. Example 3D vector

Equation 6.4. Example 3D vector after softmax
Figure 6.4. Network example for the skip-gram training
1 >>> sentence = "Claude Monet painted the Grand Canal of Venice in 1806."

Table 6.1. Ten 5-grams for sentence about Monet (view table Øgure)
Input word wt
Expected output
wt-2
Expected output
wt-1
Expected output
wt+1
Expected output
wt+2
Claude
Monet
painted
Monet
Claude
painted
the
painted
Claude
Monet
the
Grand
the
Monet
painted
Grand
Canal
Grand
painted
the
Canal
of
Canal
the
Grand
of
Venice
of
Grand
Canal
Venice
in
Venice
Canal
of
in
1908
in
of
Venice
1908
1908
Venice
in
Figure 6.5. Conversion of one-hot vector to word vector
Figure 6.6. Training input and output example for the CBOW approach

Table 6.2. Ten CBOW 5-grams from sentence about Monet (view table Øgure)
Input word wt-2
Input word wt-1
Input word wt+1
Input word wt+2 Expected output
wt
Monet
painted
Claude
Claude
painted
the
Monet
Claude
Monet
the
Grand
painted
Monet
painted
Grand
Canal
the
painted
the
Canal
of
Grand
the
Grand
of
Venice
Canal
Grand
Canal
Venice
in
of
Canal
of
in
1908
Venice
of
Venice
1908
in
Venice
in
1908
Figure 6.7. CBOW Word2vec network

CONTINUOUS BAG OF WORDS VS. BAG OF WORDS
In previous chapters, we introduced the concept of a bag of words, but
how is it di|erent than a continuous bag of words? To establish the
relationships between words in a sentence you slide a rolling window
across the sentence to select the surrounding words for the target
word. All words within the sliding window are considered to be the
content of the continuous bag of words for the target word at the
middle of that window.
Example for a continuous bag of words passing a rolling window of ve
words over the sentence “Claude Monet painted the Grand Canal of
Venice in 1908.” The word painted is the target or center word within a
ve-word rolling window. “Claude,” “Monet,” “the,” and “Grand” are
the four surrounding words for the rst CBOW rolling window.

Equation 6.5. Bigram scoring function
Equation 6.6. Subsampling probability in Mikolov’s Word2vec paper
Equation 6.7. Subsampling probability in Mikolov’s Word2vec code
1
2
>>> from nlpia.data.loaders import get_data 
>>> word_vectors = get_data('word2vec')
1
2
3
>>> from gensim.models.keyedvectors import KeyedVectors 
>>> word_vectors = KeyedVectors.load_word2vec_format(\ 
...     '/path/to/GoogleNews-vectors-negative300.bin.gz', binary=True)
1
2
3
4
>>> from gensim.models.keyedvectors import KeyedVectors 
>>> word_vectors = KeyedVectors.load_word2vec_format(\ 
...     '/path/to/GoogleNews-vectors-negative300.bin.gz', 
...         binary=True, limit=200000)

1
2
3
4
5
6
7
8
>>> word_vectors.most_similar(positive=['cooking', 'potatoes'], topn=5) 
[('cook', 0.6973530650138855), 
 ('oven_roasting', 0.6754530668258667), 
 ('Slow_cooker', 0.6742032170295715), 
 ('sweet_potatoes', 0.6600279808044434), 
 ('stir_fry_vegetables', 0.6548759341239929)] 
>>> word_vectors.most_similar(positive=['germany', 'france'], topn=1) 
[('europe', 0.7222039699554443)]
1
2
>>> word_vectors.doesnt_match("potatoes milk cake computer".split()) 
'computer'
1
2
3
>>> word_vectors.most_similar(positive=['king', 'woman'], 
...     negative=['man'], topn=2) 
[('queen', 0.7118192315101624), ('monarch', 0.6189674139022827)]
1
2
>>> word_vectors.similarity('princess', 'queen') 
0.70705315983704509
1
2
3
4
5
6
>>> word_vectors['phone'] 
array([-0.01446533, -0.12792969, -0.11572266, -0.22167969, -0.07373047, 
       -0.05981445, -0.10009766, -0.06884766,  0.14941406,  0.10107422, 
       -0.03076172, -0.03271484, -0.03125   , -0.10791016,  0.12158203, 
        0.16015625,  0.19335938,  0.0065918 , -0.15429688,  0.03710938, 
        ...
1
2
3
4
5
6
>>> token_list 
[ 
  ['to', 'provide', 'early', 'intervention/early', 'childhood', 'special', 
   'education', 'services', 'to', 'eligible', 'children', 'and', 'their', 
   'families'], 
  ['essential', 'job', 'functions'], 

7
8
9
10
  ['participate', 'as', 'a', 'transdisciplinary', 'team', 'member', 'to', 
   'complete', 'educational', 'assessments', 'for'] 
  ... 
]
1 >>> from gensim.models.word2vec import Word2Vec
Listing 6.2. Parameters to control Word2vec model training
1
2
3
4
5
>>> num_features = 300 
>>> min_word_count = 3 
>>> num_workers = 2 
>>> window_size = 6 
>>> subsampling = 1e-3
1
2
3
4
5
Listing 6.3. Instantiating a Word2vec model
1
2
3
4
5
6
7
>>> model = Word2Vec( 
...     token_list, 
...     workers=num_workers, 
...     size=num_features, 
...     min_count=min_word_count, 
...     window=window_size, 
...     sample=subsampling)
1 >>> model.init_sims(replace=True)
1
2
>>> model_name = "my_domain_specific_word2vec_model" 
>>> model.save(model_name)

Listing 6.4. Loading a saved Word2vec model
1
2
3
4
>>> from gensim.models.word2vec import Word2Vec 
>>> model_name = "my_domain_specific_word2vec_model" 
>>> model = Word2Vec.load(model_name) 
>>> model.most_similar('radiology')
1
2
3
4
>>> from gensim.models.fasttext import FastText 
>>> ft_model = FastText.load_fasttext_format(\ 
...     model_file=MODEL_PATH) 
>>> ft_model.most_similar('soccer')
1
2
3
Listing 6.5. Load a pretrained Word2vec model using nlpia
1
2
3
4
5
6
>>> import os 
>>> from nlpia.loaders import get_data 
>>> from gensim.models.word2vec import KeyedVectors 
>>> wv = get_data('word2vec') 
>>> len(wv.vocab) 
3000000
1
Listing 6.6. Examine Word2vec vocabulary frequencies
1
2
3
4
5
6
7
8
9
>>> import pandas as pd 
>>> vocab = pd.Series(wv.vocab) 
>>> vocab.iloc[1000000:100006] 
Illington_Fund             Vocab(count:447860, index:2552140) 
Illingworth                 Vocab(count:2905166, index:94834) 
Illingworth_Halifax       Vocab(count:1984281, index:1015719) 
Illini                      Vocab(count:2984391, index:15609) 
IlliniBoard.com           Vocab(count:1481047, index:1518953) 
Illini_Bluffs              Vocab(count:2636947, index:363053)
1
2
3
>>> wv['Illini'] 
array([ 0.15625   ,  0.18652344,  0.33203125,  0.55859375,  0.03637695, 
       -0.09375   , -0.05029297,  0.16796875, -0.0625    ,  0.09912109, 

4
5
       -0.0291748 ,  0.39257812,  0.05395508,  0.35351562, -0.02270508, 
       ...
Listing 6.7. Distance between “Illinois” and “Illini”
1
2
3
4
5
6
7
8
9
10
>>> import numpy as np 
>>> np.linalg.norm(wv['Illinois'] - wv['Illini']) 
3.3653798 
>>> cos_similarity = np.dot(wv['Illinois'], wv['Illini']) / ( 
...     np.linalg.norm(wv['Illinois']) *\ 
...     np.linalg.norm(wv['Illini'])) 
>>> cos_similarity 
0.5501352 
>>> 1 - cos_similarity 
0.4498648
1
2
3
Listing 6.8. Some US city data
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
>>> from nlpia.data.loaders import get_data 
>>> cities = get_data('cities') 
>>> cities.head(1).T 
geonameid                       3039154 
name                          El Tarter 
asciiname                     El Tarter 
alternatenames     Ehl Tarter,?? ?????? 
latitude                        42.5795 
longitude                       1.65362 
feature_class                         P 
feature_code                        PPL 
country_code                         AD 
cc2                                 NaN 
admin1_code                          02 
admin2_code                         NaN 
admin3_code                         NaN 
admin4_code                         NaN 
population                         1052 
elevation                           NaN 
dem                                1721 
timezone                 Europe/Andorra 
modification_date            2012-11-03
Listing 6.9. Some US state data
1
2
3
4
>>> us = cities[(cities.country_code == 'US') &\ 
...     (cities.admin1_code.notnull())].copy() 
>>> states = pd.read_csv(\ 
...     'http://www.fonz.net/blog/wp-content/uploads/2008/04/states.csv') 

WORD VECTORS ARE BIASED!
Word vectors learn word relationships based on the training corpus. If
your corpus is about nance then your “bank” word vector will be
mainly about businesses that hold deposits. If your corpus is about
geology, then your “bank” word vector will be trained on associations
with rivers and streams. And if you corpus is mostly about a
matriarchal society with women bankers and men washing clothes in
the river, then your word vectors would take on that gender bias.

5
6
7
8
9
10
11
12
13
14
15
16
>>> states = dict(zip(states.Abbreviation, states.State)) 
>>> us['city'] = us.name.copy() 
>>> us['st'] = us.admin1_code.copy() 
>>> us['state'] = us.st.map(states) 
>>> us[us.columns[-3:]].head() 
                     city  st    state 
geonameid 
4046255       Bay Minette  AL  Alabama 
4046274              Edna  TX    Texas 
4046319    Bayou La Batre  AL  Alabama 
4046332         Henderson  TX    Texas 
4046430           Natalia  TX    Texas
1
2
3
4
>>> vocab = pd.np.concatenate([us.city, us.st, us.state]) 
>>> vocab = np.array([word for word in vocab if word in wv.wv]) 
>>> vocab[:5] 
array(['Edna', 'Henderson', 'Natalia', 'Yorktown', 'Brighton'])
Listing 6.10. Augment city word vectors with US state word vectors
1
2
3
4
5
6
7
8
9
10
11
>>> city_plus_state = [] 
>>> for c, state, st in zip(us.city, us.state, us.st): 
...     if c not in vocab: 
...         continue 
...     row = [] 
...     if state in vocab: 
...         row.extend(wv[c] + wv[state]) 
...     else: 
...         row.extend(wv[c] + wv[st]) 
...     city_plus_state.append(row) 
>>> us_300D = pd.DataFrame(city_plus_state)

The following example shows the gender bias of a word model trained
on Google News articles. If you calculate the distance between “man”
and “nurse” and compare that to the distance between “woman” and
“nurse,” you’ll be able to see the bias:
Identifying and compensating for biases like this is a challenge for any
NLP practitioner that trains her models on documents written in a
biased world.
Figure 6.8. Google News Word2vec 300-D vectors projected onto a 2D map using PCA
>>> word_model.distance('man', 'nurse') 
0.7453 
>>> word_model.distance('woman', 'nurse') 
0.5586
Listing 6.11. Bubble chart of US cities
1
2
3
4
>>> from sklearn.decomposition import PCA 
>>> pca = PCA(n_components=2) 
>>> us_300D = get_data('cities_us_wordvectors') 
>>> us_2D = pca.fit_transform(us_300D.iloc[:, :300])
1
2
Listing 6.12. Bubble plot of US city word vectors

Figure 6.9. Decoder rings (left: Hubert Berberich (HubiB)
(https://commons.wikimedia.org/wiki/File:CipherDisk2000.jpg), CipherDisk2000,
marked as public domain, more details on Wikimedia Commons:
https://commons.wikimedia.org/wiki/Template:PD-self; middle: Cory Doctorow
(https://www.Ùickr.com/photos/doctorow/2817314740/in/photostream/), Crypto
wedding-ring 2, https://creativecommons.org/licenses/by-sa/2.0/legalcode; right:
Sobebunny (https://commons.wikimedia.org/wiki/File:Captain-midnight-decoder.jpg),
Captain-midnight-decoder, https://creativecommons.org/licenses/by-
sa/3.0/legalcode)
Figure 6.10. Doc2vec training uses an additional document vector as input.
1
2
3
4
5
6
7
8
9
10
11
12
13
>>> import seaborn 
>>> from matplotlib import pyplot as plt 
>>> from nlpia.plots import offline_plotly_scatter_bubble 
>>> df = get_data('cities_us_wordvectors_pca2_meta') 
>>> html = offline_plotly_scatter_bubble( 
...     df.sort_values('population', ascending=False)[:350].copy()\ 
...         .sort_values('population'), 
...     filename='plotly_scatter_bubble.html', 
...     x='x', y='y', 
...     size_col='population', text_col='name', category_col='timezone', 
...     xscale=None, yscale=None,  # 'log' or None 
...     layout={}, marker={'sizeref': 3000}) 
{'sizemode': 'area', 'sizeref': 3000}

Listing 6.13. Train your own document and word vectors
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
>>> import multiprocessing 
>>> num_cores = multiprocessing.cpu_count() 
  
>>> from gensim.models.doc2vec import TaggedDocument,\ 
...     Doc2Vec   
>>> from gensim.utils import simple_preprocess 
>>> corpus = ['This is the first document ...',\ 
...           'another document ...'] 
>>> training_corpus = [] 
>>> for i, text in enumerate(corpus): 
...     tagged_doc = TaggedDocument(\ 
...         simple_preprocess(text), [i]) 
...     training_corpus.append(tagged_doc) 
>>> model = Doc2Vec(size=100, min_count=2, 
...     workers=num_cores, iter=10) 
>>> model.build_vocab(training_corpus) 
>>> model.train(training_corpus, total_examples=model.corpus_count, 
...     epochs=model.iter)
1
2
3
4
5
6
7
8
9
1
2
training_corpus = np.empty(len(corpus), dtype=object); 
  ... training_corpus[i] = ...

1
2
>>> model.infer_vector(simple_preprocess(\ 
...     'This is a completely unseen document'), steps=10)
1

CHAPTER 7
1. 
—here are two statements that don’t mean the same thing:
2. 
—here “shone” refers to the word “hull” at the other end of the
sentence:
Figure 7.1. Fully connected neural net
Figure 7.2. Window convolving over function
1
2
The dog chased the cat. 
The cat chased the dog.
1
2
The ship's hull, despite years at sea, millions of tons of 
two mid-sea collisions, shone like new.

Figure 7.3. Small telephone pole image

Figure 7.4. Pixel values for the telephone pole image
Figure 7.5. Convolutional neural net step
Figure 7.6. Convolution

Sum of the gradients for a Ølter weight
Listing 7.1. Keras network with one convolution layer
1
2
3
4
5
6
7
8
9
10
>>> from keras.models import Sequential 
>>> from keras.layers import Conv1D 
  
>>> model = Sequential() 
>>> model.add(Conv1D(filters=16, 
                     kernel_size=3, 
                     padding='same', 
                      activation='relu', 
                     strides=1, 
                     input_shape=(100, 300)))
1
2

Figure 7.7. 1D convolution
Figure 7.8. 1D convolution with embeddings

Listing 7.2. Import your Keras convolution tools
1
2
3
4
5
>>> import numpy as np 
>>> from keras.preprocessing import sequence 
>>> from keras.models import Sequential 
>>> from keras.layers import Dense, Dropout, Activation 
>>> from keras.layers import Conv1D, GlobalMaxPooling1D
1
2
3
4
5
Listing 7.3. Preprocessor to load your documents
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
>>> import glob 
>>> import os 
  
>>> from random import shuffle 
  
>>> def pre_process_data(filepath): 
...     """ 
...     This is dependent on your training data source but we will 
...     try to generalize it as best as possible. 
...     """ 
...     positive_path = os.path.join(filepath, 'pos') 
...     negative_path = os.path.join(filepath, 'neg') 
...     pos_label = 1 
...     neg_label = 0 
...     dataset = [] 
... 
...     for filename in glob.glob(os.path.join(positive_path, '*.txt')): 
...         with open(filename, 'r') as f: 
...             dataset.append((pos_label, f.read())) 
... 
...     for filename in glob.glob(os.path.join(negative_path, '*.txt')): 
...         with open(filename, 'r') as f: 
...             dataset.append((neg_label, f.read())) 
... 
...     shuffle(dataset) 
... 
...     return dataset
1
2
3
4
5
6
>>> dataset = pre_process_data('<path to your downloaded file>/aclimdb/train') 
>>> dataset[0] 
(1, 'I, as a teenager really enjoyed this movie! Mary Kate and Ashley worked 
 great together and everyone seemed so at ease. I thought the movie plot was 
 very good and hope everyone else enjoys it to! Be sure and rent it!! Also  
they had some great soccer scenes for all those soccer players! :)')
Listing 7.4. Vectorizer and tokenizer

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
>>> from nltk.tokenize import TreebankWordTokenizer 
>>> from gensim.models.keyedvectors import KeyedVectors 
>>> from nlpia.loaders import get_data 
>>> word_vectors = get_data('w2v', limit=200000) 
 
>>> def tokenize_and_vectorize(dataset): 
...     tokenizer = TreebankWordTokenizer() 
...     vectorized_data = [] 
...     expected = [] 
...     for sample in dataset: 
...         tokens = tokenizer.tokenize(sample[1]) 
...         sample_vecs = [] 
...         for token in tokens: 
...             try: 
...                 sample_vecs.append(word_vectors[token]) 
... 
...             except KeyError: 
...                 pass  # No matching token in the Google w2v vocab 
... 
...         vectorized_data.append(sample_vecs) 
... 
...     return vectorized_data
1
Listing 7.5. Target labels
1
2
3
4
5
6
>>> def collect_expected(dataset): 
...     """ Peel off the target values from the dataset """ 
...     expected = [] 
...     for sample in dataset: 
...         expected.append(sample[0]) 
...     return expected
1
2
>>> vectorized_data = tokenize_and_vectorize(dataset) 
>>> expected = collect_expected(dataset)
Listing 7.6. Train/test split
1
2
3
4
5
6
>>> split_point = int(len(vectorized_data)*.8) 
  
>>> x_train = vectorized_data[:split_point_] 
>>> y_train_ = expected[:split_point] 
>>> x_test = vectorized_data[split_point:] 
>>> y_test = expected[split_point:]

Listing 7.7. CNN parameters
1
2
3
4
5
6
7
maxlen = 400 
batch_size = 32 
embedding_dims = 300 
filters = 250 
kernel_size = 3 
hidden_dims = 250 
epochs = 2
1
2
3
4
5
6
Listing 7.8. Padding and truncating your token sequence
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
>>> def pad_trunc(data, maxlen): 
...     """ 
...     For a given dataset pad with zero vectors or truncate to maxlen 
...     """ 
...     new_data = [] 
  
... 
...     # Create a vector of 0s the length of our word vectors 
...     zero_vector = [] 
...     for _ in range(len(data[0][0])): 
...         zero_vector.append(0.0) 
... 
...     for sample in data: 
...         if len(sample) > maxlen: 
...             temp = sample[:maxlen] 
...         elif len(sample) < maxlen: 
...             temp = sample 
...             # Append the appropriate number 0 vectors to the list 
...             additional_elems = maxlen - len(sample) 
...             for _ in range(additional_elems): 
...                 temp.append(zero_vector) 
...         else: 
...             temp = sample 
...         new_data.append(temp) 
...     return new_data
1
2
Listing 7.9. Gathering your augmented and truncated data
1
2
3
4
5
6
7
>>> x_train = pad_trunc(x_train, maxlen) 
>>> x_test = pad_trunc(x_test, maxlen) 
  
>>> x_train = np.reshape(x_train, (len(x_train), maxlen, embedding_dims)) 
>>> y_train = np.array(y_train) 
>>> x_test = np.reshape(x_test, (len(x_test), maxlen, embedding_dims)) 
>>> y_test = np.array(y_test)

Figure 7.9. Pooling layers
Listing 7.10. Construct a 1D CNN
1
2
3
4
5
6
7
8
9
10
>>> print('Build model...') 
>>> model = Sequential() 
  
>>> model.add(Conv1D( 
...    filters, 
...    kernel_size, 
...    padding='valid', 
...    activation='relu', 
...    strides=1, 
...    input_shape=(maxlen, embedding_dims)))
1
2
1 >>> model.add(GlobalMaxPooling1D())
1

Listing 7.11. Fully connected layer with dropout
1
2
3
>>> model.add(Dense(hidden_dims)) 
>>> model.add(Dropout(0.2)) 
>>> model.add(Activation('relu'))
1
Listing 7.12. Funnel
1
2
>>> model.add(Dense(1)) 
>>> model.add(Activation('sigmoid'))
Listing 7.13. Compile the CNN
1
2
3
>>> model.compile(loss='binary_crossentropy', 
...               optimizer='adam', 
...               metrics=['accuracy'])
Listing 7.14. Output layer for categorical variable (word)
1
2
>>> model.add(Dense(num_classes)) 
>>> model.add(Activation('sigmoid'))
1
Listing 7.15. Training a CNN
1
2
3
4
>>> model.fit(x_train, y_train, 
...           batch_size=batch_size, 
...           epochs=epochs, 
...           validation_data=(x_test, y_test))
1
2
Listing 7.16. Save your hard work

1
2
3
4
>>> model_structure = model.to_json() 
>>> with open("cnn_model.json", "w") as json_file: 
...     json_file.write(model_structure) 
>>> model.save_weights("cnn_weights.h5")
1
2
1
2
3
4
5
6
7
8
9
10
11
12
13
Using TensorFlow backend. 
Loading data... 
25000 train sequences 
25000 test sequences 
Pad sequences (samples x time) 
x_train shape: (25000, 400) 
x_test shape: (25000, 400) 
Build model... 
Train on 20000 samples, validate on 5000 samples 
Epoch 1/2 [================================] - 417s - loss: 0.3756 - 
acc: 0.8248 - val_loss: 0.3531 - val_acc: 0.8390 
Epoch 2/2 [================================] - 330s - loss: 0.2409 - 
acc: 0.9018 - val_loss: 0.2767 - val_acc: 0.8840
1
2
>>> import numpy as np 
>>> np.random.seed(1337)
1 >>> validation_data=(x_test, y_test)
Listing 7.17. Loading a saved model
1
2
3
4
5
6
>>> from keras.models import model_from_json 
>>> with open("cnn_model.json", "r") as json_file: 
...     json_string = json_file.read() 
>>> model = model_from_json(json_string) 
  
>>> model.load_weights('cnn_weights.h5')

Listing 7.18. Test example
1
2
3
>>> sample_1 = "I hate that the dismal weather had me down for so long,  
 when will it break! Ugh, when does happiness return? The sun is blinding 
 and the puffy clouds are too thin. I can't wait for the weekend."
Listing 7.19. Prediction
1
2
3
4
5
6
7
8
>>> vec_list = tokenize_and_vectorize([(1, sample_1)]) 
  
>>> test_vec_list = pad_trunc(vec_list, maxlen) 
  
>>> test_vec = np.reshape(test_vec_list, (len(test_vec_list), maxlen,\ 
...     embedding_dims)) 
>>> model.predict(test_vec) 
array([[ 0.12459087]], dtype=float32)
1
2
1
2
>>> model.predict_classes(test_vec) 
array([[0]], dtype=int32)

CHAPTER 8
Figure 8.1. 1D convolution with embeddings

Figure 8.2. Text into a feedforward network
Figure 8.3. Recurrent neural net

Figure 8.4. Unrolled recurrent neural net

Figure 8.5. Detailed recurrent neural net at time step t = 0

Figure 8.6. Detailed recurrent neural net at time step t = 1
Figure 8.7. Data into convolutional network

Figure 8.8. Data fed into a recurrent network
Figure 8.9. Only last output matters here
Figure 8.10. Backpropagation through time

Figure 8.11. All outputs matter here
Figure 8.12. Multiple outputs and backpropagation through time

Listing 8.1. Import all the things
1
2
3
4
5
6
>>> import glob 
>>> import os 
>>> from random import shuffle 
>>> from nltk.tokenize import TreebankWordTokenizer 
>>> from nlpia.loaders import get_data 
>>> word_vectors = get_data('wv')
Listing 8.2. Data preprocessor
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
>>> def pre_process_data(filepath): 
...     """ 
...     Load pos and neg examples from separate dirs then shuffle them 
...     together. 
...     """ 
...     positive_path = os.path.join(filepath, 'pos') 
...     negative_path = os.path.join(filepath, 'neg') 
...     pos_label = 1 
...     neg_label = 0 
...     dataset = [] 
...     for filename in glob.glob(os.path.join(positive_path, '*.txt')): 
...         with open(filename, 'r') as f: 
...             dataset.append((pos_label, f.read())) 
...     for filename in glob.glob(os.path.join(negative_path, '*.txt')): 
...         with open(filename, 'r') as f: 
...             dataset.append((neg_label, f.read())) 
...     shuffle(dataset) 
...     return dataset
Listing 8.3. Data tokenizer + vectorizer

1
2
3
4
5
6
7
8
9
10
11
12
13
>>> def tokenize_and_vectorize(dataset): 
...     tokenizer = TreebankWordTokenizer() 
...     vectorized_data = [] 
...     for sample in dataset: 
...         tokens = tokenizer.tokenize(sample[1]) 
...         sample_vecs = [] 
...         for token in tokens: 
...             try: 
...                 sample_vecs.append(word_vectors[token]) 
...             except KeyError: 
...                 pass 
...         vectorized_data.append(sample_vecs) 
...     return vectorized_data
1
Listing 8.4. Target unzipper
1
2
3
4
5
6
>>> def collect_expected(dataset): 
...     """ Peel off the target values from the dataset """ 
...     expected = [] 
...     for sample in dataset: 
...         expected.append(sample[0]) 
...     return expected
Listing 8.5. Load and prepare your data
1
2
3
4
5
6
7
8
>>> dataset = pre_process_data('./aclimdb/train') 
>>> vectorized_data = tokenize_and_vectorize(dataset) 
>>> expected = collect_expected(dataset) 
>>> split_point = int(len(vectorized_data) * .8) 
>>> x_train = vectorized_data[:split_point] 
>>> y_train = expected[:split_point] 
>>> x_test = vectorized_data[split_point:] 
>>> y_test = expected[split_point:]
1
Listing 8.6. Initialize your network parameters
1
2
3
4
>>> maxlen = 400 
>>> batch_size = 32 
>>> embedding_dims = 300 
>>> epochs = 2

Listing 8.7. Load your test and training data
1
2
3
4
5
6
7
8
9
>>> import numpy as np 
  
>>> x_train = pad_trunc(x_train, maxlen) 
>>> x_test = pad_trunc(x_test, maxlen) 
  
>>> x_train = np.reshape(x_train, (len(x_train), maxlen, embedding_dims)) 
>>> y_train = np.array(y_train) 
>>> x_test = np.reshape(x_test, (len(x_test), maxlen, embedding_dims)) 
>>> y_test = np.array(y_test)
Listing 8.8. Initialize an empty Keras network
1
2
3
4
>>> from keras.models import Sequential 
>>> from keras.layers import Dense, Dropout, Flatten, SimpleRNN 
>>> num_neurons = 50 
>>> model = Sequential()
Listing 8.9. Add a recurrent layer
1
2
3
>>> model.add(SimpleRNN( 
...    num_neurons, return_sequences=True, 
...    input_shape=(maxlen, embedding_dims)))
Listing 8.10. Add a dropout layer
1
2
3
4
>>> model.add(Dropout(.2)) 
 
>>> model.add(Flatten()) 
>>> model.add(Dense(1, activation='sigmoid'))
Listing 8.11. Compile your recurrent network
1
2
3
4
5
>>> model.compile('rmsprop', 'binary_crossentropy',  metrics=['accuracy']) 
Using TensorFlow backend. 
>>> model.summary() 
_________________________________________________________________ 
Layer (type)                 Output Shape              Param # 

6
7
8
9
10
11
12
13
14
15
16
17
18
19
================================================================= 
simple_rnn_1 (SimpleRNN)     (None, 400, 50)           17550 
_________________________________________________________________ 
dropout_1 (Dropout)          (None, 400, 50)           0 
_________________________________________________________________ 
flatten_1 (Flatten)          (None, 20000)             0 
_________________________________________________________________ 
dense_1 (Dense)              (None, 1)                 20001 
================================================================= 
Total params: 37,551.0 
Trainable params: 37,551.0 
Non-trainable params: 0.0 
_________________________________________________________________ 
None
Listing 8.12. Train and save your model
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
>>> model.fit(x_train, y_train, 
...           batch_size=batch_size, 
...           epochs=epochs, 
...           validation_data=(x_test, y_test)) 
Train on 20000 samples, validate on 5000 samples 
Epoch 1/2 
20000/20000 [==============================] - 215s - loss: 0.5723 - 
acc: 0.7138 - val_loss: 0.5011 - val_acc: 0.7676 
Epoch 2/2 
20000/20000 [==============================] - 183s - loss: 0.4196 - 
acc: 0.8144 - val_loss: 0.4763 - val_acc: 0.7820 
  
>>> model_structure = model.to_json() 
>>> with open("simplernn_model1.json", "w") as json_file: 
...     json_file.write(model_structure) 
>>> model.save_weights("simplernn_weights1.h5") 
Model saved.
Listing 8.13. Model parameters
1
2
3
4
5
>>> maxlen = 400 
>>> embedding_dims = 300 
>>> batch_size = 32 
>>> epochs = 2 
>>> num_neurons = 50
1
2
3
4
Listing 8.14. Build a larger network
1
2
>>> num_neurons = 100 
>>> model = Sequential() 

3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
>>> model.add(SimpleRNN( 
...     num_neurons, return_sequences=True, input_shape=(maxlen,\ 
...     embedding_dims))) 
>>> model.add(Dropout(.2)) 
>>> model.add(Flatten()) 
>>> model.add(Dense(1, activation='sigmoid')) 
>>> model.compile('rmsprop', 'binary_crossentropy',  metrics=['accuracy']) 
Using TensorFlow backend. 
>>> model.summary() 
_________________________________________________________________ 
Layer (type)                 Output Shape              Param # 
================================================================= 
simple_rnn_1 (SimpleRNN)     (None, 400, 100)          40100 
_________________________________________________________________ 
dropout_1 (Dropout)          (None, 400, 100)          0 
_________________________________________________________________ 
flatten_1 (Flatten)          (None, 40000)             0 
_________________________________________________________________ 
dense_1 (Dense)              (None, 1)                 40001 
================================================================= 
Total params: 80,101.0 
Trainable params: 80,101.0 
Non-trainable params: 0.0 
_________________________________________________________________
Listing 8.15. Train your larger network
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
>>> model.fit(x_train, y_train, 
...           batch_size=batch_size, 
...           epochs=epochs, 
...           validation_data=(x_test, y_test)) 
Train on 20000 samples, validate on 5000 samples 
Epoch 1/2 
20000/20000 [==============================] - 287s - loss: 0.9063 - 
acc: 0.6529 - val_loss: 0.5445 - val_acc: 0.7486 
Epoch 2/2 
20000/20000 [==============================] - 240s - loss: 0.4760 - 
acc: 0.7951 - val_loss: 0.5165 - val_acc: 0.7824 
>>> model_structure = model.to_json() 
>>> with open("simplernn_model2.json", "w") as json_file: 
...     json_file.write(model_structure) 
>>> model.save_weights("simplernn_weights2.h5") 
Model saved.
1
2
20000/20000 [==============================] - 240s - loss: 0.5394 - 
acc: 0.8084 - val_loss: 0.4490 - val_acc: 0.7970

Figure 8.13. Bidirectional recurrent neural net
Listing 8.16. Crummy weather sentiment
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
>>> sample_1 = "I hate that the dismal weather had me down for so long, when 
 will it break! Ugh, when does happiness return? The sun is blinding and  
 the puffy clouds are too thin. I can't wait for the weekend." 
  
>>> from keras.models import model_from_json 
>>> with open("simplernn_model1.json", "r") as json_file: 
...     json_string = json_file.read() 
>>> model = model_from_json(json_string) 
>>> model.load_weights('simplernn_weights1.h5') 
  
>>> vec_list = tokenize_and_vectorize([(1, sample_1)]) 
>>> test_vec_list = pad_trunc(vec_list, maxlen) 
>>> test_vec = np.reshape(test_vec_list, (len(test_vec_list), maxlen,\ 
...     embedding_dims)) 
  
>>> model.predict_classes(test_vec) 
array([[0]], dtype=int32)
1
2
Listing 8.17. Build a Bidirectional  recurrent network
1
2
3
4
5
6
7
8
9
10
11
12
>>> from keras.models import Sequential 
>>> from keras.layers import SimpleRNN 
>>> from keras.layers.wrappers import Bidirectional 
  
>>> num_neurons = 10 
>>> maxlen = 100 
>>> embedding_dims = 300 
  
>>> model = Sequential() 
>>> model.add(Bidirectional(SimpleRNN( 
...    num_neurons, return_sequences=True),\ 
...    input_shape=(maxlen, embedding_dims)))


CHAPTER 9
Figure 9.1. LSTM network and its memory
Figure 9.2. Unrolled LSTM network and its memory

Figure 9.3. LSTM layer at time step t
Listing 9.1. LSTM layer in Keras
1
2
3
4
5
6
7
8
9
10
>>> maxlen = 400 
>>> batch_size = 32 
>>> embedding_dims = 300 
>>> epochs = 2 
>>> from keras.models import Sequential 
>>> from keras.layers import Dense, Dropout, Flatten, LSTM 
>>> num_neurons = 50 
>>> model = Sequential() 
>>> model.add(LSTM(num_neurons, return_sequences=True, 
...                input_shape=(maxlen, embedding_dims))) 

Figure 9.4. LSTM layer inputs
Figure 9.5. First stop—the forget gate
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
>>> model.add(Dropout(.2)) 
>>> model.add(Flatten()) 
>>> model.add(Dense(1, activation='sigmoid')) 
>>> model.compile('rmsprop', 'binary_crossentropy',  metrics=['accuracy']) 
>>> print(model.summary()) 
Layer (type)                 Output Shape              Param # 
================================================================= 
lstm_1 (LSTM)                (None, 400, 50)           70200 
_________________________________________________________________ 
dropout_1 (Dropout)          (None, 400, 50)           0 
_________________________________________________________________ 
flatten_1 (Flatten)          (None, 20000)             0 
_________________________________________________________________ 
dense_1 (Dense)              (None, 1)                 20001 
================================================================= 
Total params: 90,201.0 
Trainable params: 90,201.0 
Non-trainable params: 0.0

Figure 9.6. Forget gate
Figure 9.7. Forget gate application

Figure 9.8. Candidate gate

Figure 9.9. Update/output gate

Listing 9.2. Load and prepare the IMDB data
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
>>> import numpy as np 
  
>>> dataset = pre_process_data('./aclimdb/train') 
>>> vectorized_data = tokenize_and_vectorize(dataset) 
>>> expected = collect_expected(dataset) 
>>> split_point = int(len(vectorized_data) * .8) 
  
>>> x_train = vectorized_data[:split_point] 
>>> y_train = expected[:split_point] 
>>> x_test = vectorized_data[split_point:] 
>>> y_test = expected[split_point:] 
  
>>> maxlen = 400 
>>> batch_size = 32 
>>> embedding_dims = 300 
>>> epochs = 2 
  
>>> x_train = pad_trunc(x_train, maxlen) 
>>> x_test = pad_trunc(x_test, maxlen) 
>>> x_train = np.reshape(x_train, 
...     (len(x_train), maxlen, embedding_dims)) 
>>> y_train = np.array(y_train) 
>>> x_test = np.reshape(x_test, (len(x_test), maxlen, embedding_dims)) 
>>> y_test = np.array(y_test)
1
2
3
4
5
6
7
Listing 9.3. Build a Keras LSTM network
1
2
3
>>> from keras.models import Sequential 
>>> from keras.layers import Dense, Dropout, Flatten, LSTM 
>>> num_neurons = 50 

4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
>>> model = Sequential() 
>>> model.add(LSTM(num_neurons, return_sequences=True, 
...              input_shape=(maxlen, embedding_dims))) 
>>> model.add(Dropout(.2)) 
>>> model.add(Flatten()) 
>>> model.add(Dense(1, activation='sigmoid')) 
>>> model.compile('rmsprop', 'binary_crossentropy',  metrics=['accuracy']) 
>>> model.summary() 
Layer (type)                 Output Shape              Param # 
================================================================= 
lstm_2 (LSTM)                (None, 400, 50)           70200 
_________________________________________________________________ 
dropout_2 (Dropout)          (None, 400, 50)           0 
_________________________________________________________________ 
flatten_2 (Flatten)          (None, 20000)             0 
_________________________________________________________________ 
dense_2 (Dense)              (None, 1)                 20001 
================================================================= 
Total params: 90,201.0 
Trainable params: 90,201.0 
Non-trainable params: 0.0
1
2
3
Listing 9.4. Fit your LSTM model
1
2
3
4
5
6
7
8
9
10
11
12
>>> model.fit(x_train, y_train, 
...           batch_size=batch_size, 
...           epochs=epochs, 
...           validation_data=(x_test, y_test)) 
Train on 20000 samples, validate on 5000 samples 
Epoch 1/2 
20000/20000 [==============================] - 548s - loss: 0.4772 - 
acc: 0.7736 - val_loss: 0.3694 - val_acc: 0.8412 
Epoch 2/2 
20000/20000 [==============================] - 583s - loss: 0.3477 - 
acc: 0.8532 - val_loss: 0.3451 - val_acc: 0.8516 
<keras.callbacks.History at 0x145595fd0>
1
Listing 9.5. Save it for later
1
2
3
4
5
>>> model_structure = model.to_json() 
>>> with open("lstm_model1.json", "w") as json_file: 
...     json_file.write(model_structure) 
  
>>> model.save_weights("lstm_weights1.h5")
1
Listing 9.6. Reload your LSTM model

1
2
3
4
5
>>> from keras.models import model_from_json 
>>> with open("lstm_model1.json", "r") as json_file: 
...     json_string = json_file.read() 
>>> model = model_from_json(json_string) 
>>> model.load_weights('lstm_weights1.h5')
Listing 9.7. Use the model to predict on a sample
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
>>> sample_1 = """I hate that the dismal weather had me down for so long, 
...  when will it break! Ugh, when does happiness return?  The sun is 
...  blinding and the puffy clouds are too thin. I can't wait for the 
...  weekend.""" 
 
>>> vec_list = tokenize_and_vectorize([(1, sample_1)]) 
 
>>> test_vec_list = pad_trunc(vec_list, maxlen) 
  
>>> test_vec = np.reshape(test_vec_list, 
...                       (len(test_vec_list), maxlen, embedding_dims)) 
  
>>> print("Sample's sentiment, 1 - pos, 2 - neg : {}"\ 
...     .format(model.predict_classes(test_vec))) 
1/1 [==============================] - 0s 
Sample's sentiment, 1 - pos, 2 - neg : [[0]] 
  
>>> print("Raw output of sigmoid function: {}"\ 
...     .format(model.predict(test_vec))) 
Raw output of sigmoid function: [[ 0.2192785]]
1
2
Listing 9.8. Optimize the thought vector size
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
>>> def test_len(data, maxlen): 
...     total_len = truncated = exact = padded = 0 
...     for sample in data: 
...         total_len += len(sample) 
...         if len(sample) > maxlen: 
...             truncated += 1 
...         elif len(sample) < maxlen: 
...             padded += 1 
...         else: 
...             exact +=1 
...     print('Padded: {}'.format(padded)) 
...     print('Equal: {}'.format(exact)) 
...     print('Truncated: {}'.format(truncated)) 
...     print('Avg length: {}'.format(total_len/len(data))) 
  
>>> dataset = pre_process_data('./aclimdb/train') 
>>> vectorized_data = tokenize_and_vectorize(dataset) 
>>> test_len(vectorized_data, 400) 
Padded: 22559 
Equal: 12 
Truncated: 2429 
Avg length: 202.4424

Listing 9.9. Optimize LSTM hyperparameters
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
>>> import numpy as np 
>>> from keras.models import Sequential 
>>> from keras.layers import Dense, Dropout, Flatten, LSTM 
>>> maxlen = 200 
>>> batch_size = 32 
>>> embedding_dims = 300 
>>> epochs = 2 
>>> num_neurons = 50 
>>> dataset = pre_process_data('./aclimdb/train') 
>>> vectorized_data = tokenize_and_vectorize(dataset) 
>>> expected = collect_expected(dataset) 
>>> split_point = int(len(vectorized_data)*.8) 
>>> x_train = vectorized_data[:split_point] 
>>> y_train = expected[:split_point] 
>>> x_test = vectorized_data[split_point:] 
>>> y_test = expected[split_point:] 
>>> x_train = pad_trunc(x_train, maxlen) 
>>> x_test = pad_trunc(x_test, maxlen) 
>>> x_train = np.reshape(x_train, (len(x_train), maxlen, embedding_dims)) 
>>> y_train = np.array(y_train) 
>>> x_test = np.reshape(x_test, (len(x_test), maxlen, embedding_dims)) 
>>> y_test = np.array(y_test)
1
Listing 9.10. A more optimally sized LSTM
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
>>> model = Sequential() 
>>> model.add(LSTM(num_neurons, return_sequences=True, 
...                input_shape=(maxlen, embedding_dims))) 
>>> model.add(Dropout(.2)) 
>>> model.add(Flatten()) 
>>> model.add(Dense(1, activation='sigmoid')) 
>>> model.compile('rmsprop', 'binary_crossentropy',  metrics=['accuracy']) 
>>> model.summary() 
Layer (type)                 Output Shape              Param # 
================================================================= 
lstm_1 (LSTM)                (None, 200, 50)           70200 
_________________________________________________________________ 
dropout_1 (Dropout)          (None, 200, 50)           0 
_________________________________________________________________ 
flatten_1 (Flatten)          (None, 10000)             0 
_________________________________________________________________ 
dense_1 (Dense)              (None, 1)                 10001 
================================================================= 
Total params: 80,201.0 
Trainable params: 80,201.0 
Non-trainable params: 0.0

Listing 9.11. Train a smaller LSTM
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
>>> model.fit(x_train, y_train, 
...           batch_size=batch_size, 
...           epochs=epochs, 
...           validation_data=(x_test, y_test)) 
Train on 20000 samples, validate on 5000 samples 
Epoch 1/2 
20000/20000 [==============================] - 245s - loss: 0.4742 - 
acc: 0.7760 - val_loss: 0.4235 - val_acc: 0.8010 
Epoch 2/2 
20000/20000 [==============================] - 203s - loss: 0.3718 - 
acc: 0.8386 - val_loss: 0.3499 - val_acc: 0.8450 
  
>>> model_structure = model.to_json() 
>>> with open("lstm_model7.json", "w") as json_file: 
...     json_file.write(model_structure) 
  
>>> model.save_weights("lstm_weights7.h5")
Listing 9.12. Prepare the data
1
2
>>> dataset = pre_process_data('./aclimdb/train') 
>>> expected = collect_expected(dataset)
Listing 9.13. Calculate the average sample length
1
2
3
4
5
6
7
8
>>> def avg_len(data): 
...     total_len = 0 
...     for sample in data: 
...         total_len += len(sample[1]) 
...     return total_len/len(data) 
  
>>> avg_len(dataset) 
1325.06964
Listing 9.14. Prepare the strings for a character-based model
1
2
3
4
5
6
7
8
>>> def clean_data(data): 
...     """Shift to lower case, replace unknowns with UNK, and listify""" 
...     new_data = [] 
...     VALID = 'abcdefghijklmnopqrstuvwxyz0123456789"\'?!.,:; ' 
...     for sample in data: 
...         new_sample = [] 
...         for char in sample[1].lower(): 
...             if char in VALID: 
1

9
10
11
12
13
14
15
...                 new_sample.append(char) 
...             else: 
...                 new_sample.append('UNK') 
...         new_data.append(new_sample) 
...     return new_data 
  
>>> listified_data = clean_data(dataset)
Listing 9.15. Pad and truncated characters
1
2
3
4
5
6
7
8
9
10
11
12
13
>>> def char_pad_trunc(data, maxlen=1500): 
...     """ We truncate to maxlen or add in PAD tokens """ 
...     new_dataset = [] 
...     for sample in data: 
...         if len(sample) > maxlen: 
...             new_data = sample[:maxlen] 
...         elif len(sample) < maxlen: 
...             pads = maxlen - len(sample) 
...             new_data = sample + ['PAD'] * pads 
...         else: 
...             new_data = sample 
...         new_dataset.append(new_data) 
...     return new_dataset
Listing 9.16. Character-based model “vocabulary”
1
2
3
4
5
6
7
8
>>> def create_dicts(data): 
...     """ Modified from Keras LSTM example""" 
...     chars = set() 
...     for sample in data: 
...         chars.update(set(sample)) 
...     char_indices = dict((c, i) for i, c in enumerate(chars)) 
...     indices_char = dict((i, c) for i, c in enumerate(chars)) 
...     return char_indices, indices_char
Listing 9.17. One-hot encoder for characters
1
2
3
4
5
6
7
8
9
10
>>> import numpy as np 
  
>>> def onehot_encode(dataset, char_indices, maxlen=1500): 
...     """ 
...     One-hot encode the tokens 
... 
...     Args: 
...         dataset  list of lists of tokens 
...         char_indices 
...                  dictionary of {key=character, 

11
12
13
14
15
16
17
18
19
20
...                                 value=index to use encoding vector} 
...         maxlen  int  Length of each sample 
...     Return: 
...         np array of shape (samples, tokens, encoding length) 
...     """ 
...     X = np.zeros((len(dataset), maxlen, len(char_indices.keys()))) 
...     for i, sentence in enumerate(dataset): 
...         for t, char in enumerate(sentence): 
...             X[i, t, char_indices[char]] = 1 
...     return X
1
Listing 9.18. Load and preprocess the IMDB data
1
2
3
4
5
6
7
>>> dataset = pre_process_data('./aclimdb/train') 
>>> expected = collect_expected(dataset) 
>>> listified_data = clean_data(dataset) 
  
>>> common_length_data = char_pad_trunc(listified_data, maxlen=1500) 
>>> char_indices, indices_char = create_dicts(common_length_data) 
>>> encoded_data = onehot_encode(common_length_data, char_indices, 1500)
Listing 9.19. Split dataset for training (80%) and testing (20%)
1
2
3
4
5
6
>>> split_point = int(len(encoded_data)*.8) 
  
>>> x_train = encoded_data[:split_point] 
>>> y_train = expected[:split_point] 
>>> x_test = encoded_data[split_point:] 
>>> y_test = expected[split_point:]
Listing 9.20. Build a character-based LSTM
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
>>> from keras.models import Sequential 
>>> from keras.layers import Dense, Dropout, Embedding, Flatten, LSTM 
 
>>> num_neurons = 40 
>>> maxlen = 1500 
>>> model = Sequential() 
 
>>> model.add(LSTM(num_neurons, 
...                return_sequences=True, 
...                input_shape=(maxlen, len(char_indices.keys())))) 
>>> model.add(Dropout(.2)) 
>>> model.add(Flatten()) 
>>> model.add(Dense(1, activation='sigmoid')) 
>>> model.compile('rmsprop', 'binary_crossentropy',  metrics=['accuracy']) 
>>> model.summary() 

16
17
18
19
20
21
22
23
24
25
26
27
28
Layer (type)                 Output Shape              Param # 
================================================================= 
lstm_2 (LSTM)                (None, 1500, 40)          13920 
_________________________________________________________________ 
dropout_2 (Dropout)          (None, 1500, 40)          0 
_________________________________________________________________ 
flatten_2 (Flatten)          (None, 60000)             0 
_________________________________________________________________ 
dense_2 (Dense)              (None, 1)                 60001 
================================================================= 
Total params: 73,921.0 
Trainable params: 73,921.0 
Non-trainable params: 0.0
Listing 9.21. Train a character-based LSTM
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
>>> batch_size = 32 
>>> epochs = 10 
>>> model.fit(x_train, y_train, 
...           batch_size=batch_size, 
...           epochs=epochs, 
...           validation_data=(x_test, y_test)) 
Train on 20000 samples, validate on 5000 samples 
Epoch 1/10 
20000/20000 [==============================] - 634s - loss: 0.6949 - 
acc: 0.5388 - val_loss: 0.6775 - val_acc: 0.5738 
Epoch 2/10 
20000/20000 [==============================] - 668s - loss: 0.6087 - 
acc: 0.6700 - val_loss: 0.6786 - val_acc: 0.5962 
Epoch 3/10 
20000/20000 [==============================] - 695s - loss: 0.5358 - 
acc: 0.7356 - val_loss: 0.7182 - val_acc: 0.5786 
Epoch 4/10 
20000/20000 [==============================] - 686s - loss: 0.4662 - 
acc: 0.7832 - val_loss: 0.7605 - val_acc: 0.5836 
Epoch 5/10 
20000/20000 [==============================] - 694s - loss: 0.4062 - 
acc: 0.8206 - val_loss: 0.8099 - val_acc: 0.5852 
Epoch 6/10 
20000/20000 [==============================] - 694s - loss: 0.3550 - 
acc: 0.8448 - val_loss: 0.8851 - val_acc: 0.5842 
Epoch 7/10 
20000/20000 [==============================] - 645s - loss: 0.3058 - 
acc: 0.8705 - val_loss: 0.9598 - val_acc: 0.5930 
Epoch 8/10 
20000/20000 [==============================] - 684s - loss: 0.2643 - 
acc: 0.8911 - val_loss: 1.0366 - val_acc: 0.5888 
Epoch 9/10 
20000/20000 [==============================] - 671s - loss: 0.2304 - 
acc: 0.9055 - val_loss: 1.1323 - val_acc: 0.5914 
Epoch 10/10 
20000/20000 [==============================] - 663s - loss: 0.2035 - 
acc: 0.9181 - val_loss: 1.2051 - val_acc: 0.5948

Figure 9.10. Next word prediction
Figure 9.11. Next character prediction
Listing 9.22. And save it for later
1
2
3
4
>>> model_structure = model.to_json() 
>>> with open("char_lstm_model3.json", "w") as json_file: 
...     json_file.write(model_structure) 
>>> model.save_weights("char_lstm_weights3.h5")

Figure 9.12. Last character prediction only
Listing 9.23. Import the Project Gutenberg dataset
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
>>> from nltk.corpus import gutenberg 
>>>  
>>> gutenberg.fileids() 
['austen-emma.txt', 
 'austen-persuasion.txt', 
 'austen-sense.txt', 
 'bible-kjv.txt', 
 'blake-poems.txt', 
 'bryant-stories.txt', 
 'burgess-busterbrown.txt', 
 'carroll-alice.txt', 
 'chesterton-ball.txt', 
 'chesterton-brown.txt', 
 'chesterton-thursday.txt', 
 'edgeworth-parents.txt', 
 'melville-moby_dick.txt', 
 'milton-paradise.txt', 
 'shakespeare-caesar.txt', 
 'shakespeare-hamlet.txt', 
 'shakespeare-macbeth.txt', 
 'whitman-leaves.txt']
Listing 9.24. Preprocess Shakespeare plays
1
2
3
4
>>> text = '' 
>>> for txt in gutenberg.fileids(): 
...     if 'shakespeare' in txt: 
...         text += gutenberg.raw(txt).lower() 
1

5
6
7
8
9
10
11
>>> chars = sorted(list(set(text))) 
>>> char_indices = dict((c, i) 
...     for i, c in enumerate(chars)) 
>>> indices_char = dict((i, c) 
...     for i, c in enumerate(chars)) 
>>> 'corpus length: {}  total chars: {}'.format(len(text), len(chars)) 
'corpus length: 375542  total chars: 50'
2
3
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
>>> print(text[:500]) 
[the tragedie of julius caesar by william shakespeare 1599] 
  
actus primus. scoena prima. 
 
enter flauius, murellus, and certaine commoners ouer the stage. 
 
  flauius. hence: home you idle creatures, get you home: 
is this a holiday? what, know you not 
(being mechanicall) you ought not walke 
vpon a labouring day, without the signe 
of your profession? speake, what trade art thou? 
  car. why sir, a carpenter 
  
   mur. where is thy leather apron, and thy rule? 
what dost thou with thy best apparrell on
Listing 9.25. Assemble a training set
1
2
3
4
5
6
7
8
9
>>> maxlen = 40 
>>> step = 3 
>>> sentences = [] 
>>> next_chars = [] 
>>> for i in range(0, len(text) - maxlen, step): 
...     sentences.append(text[i: i + maxlen]) 
...     next_chars.append(text[i + maxlen]) 
>>> print('nb sequences:', len(sentences)) 
nb sequences: 125168
1
2
3
4
Listing 9.26. One-hot encode the training examples
1
2
3
4
5
6
>>> X = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool) 
>>> y = np.zeros((len(sentences), len(chars)), dtype=np.bool) 
>>> for i, sentence in enumerate(sentences): 
...     for t, char in enumerate(sentence): 
...         X[i, t, char_indices[char]] = 1 
...     y[i, char_indices[next_chars[i]]] = 1

Listing 9.27. Assemble a character-based LSTM model for generating text
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
>>> from keras.models import Sequential 
>>> from keras.layers import Dense, Activation 
>>> from keras.layers import LSTM 
>>> from keras.optimizers import RMSprop 
>>> model = Sequential() 
>>> model.add(LSTM(128, 
...                input_shape=(maxlen, len(chars)))) 
>>> model.add(Dense(len(chars))) 
>>> model.add(Activation('softmax')) 
>>> optimizer = RMSprop(lr=0.01) 
>>> model.compile(loss='categorical_crossentropy', optimizer=optimizer) 
>>> model.summary() 
Layer (type)                 Output Shape              Param # 
================================================================= 
lstm_1 (LSTM)                (None, 128)               91648 
_________________________________________________________________ 
dense_1 (Dense)              (None, 50)                6450 
_________________________________________________________________ 
activation_1 (Activation)    (None, 50)                0 
================================================================= 
Total params: 98,098.0 
Trainable params: 98,098.0 
Non-trainable params: 0.0
1
2
Listing 9.28. Train your Shakespearean chatbot
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
>>> epochs = 6 
>>> batch_size = 128 
>>> model_structure = model.to_json() 
>>> with open("shakes_lstm_model.json", "w") as json_file: 
>>>     json_file.write(model_structure) 
>>> for i in range(5): 
...     model.fit(X, y, 
...               batch_size=batch_size, 
...               epochs=epochs) 
...     model.save_weights("shakes_lstm_weights_{}.h5".format(i+1)) 
Epoch 1/6 
125168/125168 [==============================] - 266s - loss: 2.0310 
Epoch 2/6 
125168/125168 [==============================] - 257s - loss: 1.6851 
...
1
Listing 9.29. Sampler to generate character sequences
1
2
3
>>> import random 
>>> def sample(preds, temperature=1.0): 
...     preds = np.asarray(preds).astype('float64') 

4
5
6
7
8
...     preds = np.log(preds) / temperature 
...     exp_preds = np.exp(preds) 
...     preds = exp_preds / np.sum(exp_preds) 
...     probas = np.random.multinomial(1, preds, 1) 
...     return np.argmax(probas)
Listing 9.30. Generate three texts with three diversity levels
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
>>> import sys 
>>> start_index = random.randint(0, len(text) - maxlen - 1) 
>>> for diversity in [0.2, 0.5, 1.0]: 
...     print() 
...     print('----- diversity:', diversity) 
...     generated = '' 
...     sentence = text[start_index: start_index + maxlen] 
...     generated += sentence 
...     print('----- Generating with seed: "' + sentence + '"') 
...     sys.stdout.write(generated) 
...     for i in range(400): 
...         x = np.zeros((1, maxlen, len(chars))) 
...         for t, char in enumerate(sentence): 
...             x[0, t, char_indices[char]] = 1. 
...         preds = model.predict(x, verbose=0)[0] 
...         next_index = sample(preds, diversity) 
...         next_char = indices_char[next_index] 
...         generated += next_char 
...         sentence = sentence[1:] + next_char 
...         sys.stdout.write(next_char) 
...         sys.stdout.flush() 
...     print()
1
2
3
4
5
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
----- diversity: 0.2 
----- Generating with seed: " them through & through 
the most fond an" 
 them through & through 
the most fond and stranger the straite to the straite 
him a father the world, and the straite: 
the straite is the straite to the common'd, 
and the truth, and the truth, and the capitoll, 
and stay the compurse of the true then the dead and the colours, 
and the comparyed the straite the straite 
the mildiaus, and the straite of the bones, 
and what is the common the bell to the straite 
the straite in the commised and 
  
----- diversity: 0.5 
----- Generating with seed: " them through & through 
the most fond an" 
 them through & through 
the most fond and the pindage it at them for 
that i shall pround-be be the house, not that we be not the selfe, 
and thri's the bate and the perpaine, to depart of the father now 
but ore night in a laid of the haid, and there is it 
  
   bru. what shall greefe vndernight of it 

Figure 9.13. Stacked LSTM
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
 
   cassi. what shall the straite, and perfire the peace, 
and defear'd and soule me to me a ration, 
and we will steele the words them with th 
  
----- diversity: 1.0 
----- Generating with seed: " them through & through 
the most fond an" 
 them through & through 
the most fond and boy'd report alone 
  
   yp. it best we will st of me at that come sleepe. 
but you yet it enemy wrong, 'twas sir 
  
   ham. the pirey too me, it let you? 
  son. oh a do a sorrall you. that makino 
beendumons vp?x, let vs cassa, 
yet his miltrow addome knowlmy in his windher, 
a vertues. hoie sleepe, or strong a strong at it 
mades manish swill about a time shall trages, 
and follow. more. heere shall abo
Listing 9.31. Gated recurrent units in Keras
1
2
3
4
5
>>> from keras.models import Sequential 
>>> from keras.layers import GRU 
>>> model = Sequential() 
>>> model.add(GRU(num_neurons, return_sequences=True, 
...               input_shape=X[0].shape))

Listing 9.32. Two LSTM layers
1
2
3
4
5
6
>>> from keras.models import Sequential 
>>> from keras.layers import LSTM 
>>> model = Sequential() 
>>> model.add(LSTM(num_neurons, return_sequences=True, 
...                input_shape=X[0].shape)) 
>>> model.add(LSTM(num_neurons_2, return_sequences=True))

CHAPTER 10
Figure 10.1. Limitations of language modeling
Figure 10.2. Encoder-decoder sandwich with thought vector meat
Figure 10.3. Unrolled encoder-decoder

Figure 10.4. Next word prediction
Figure 10.5. Input and target sequence before preprocessing
Figure 10.6. Input and target sequence after preprocessing

Figure 10.7. Thought encoder
Figure 10.8. LSTM states used in the sequence-to-sequence encoder
Listing 10.1. Thought encoder in Keras
1
2
3
4
>>> encoder_inputs = Input(shape=(None, input_vocab_size)) 
>>> encoder = LSTM(num_neurons, return_state=True) 
>>> encoder_outputs, state_h, state_c = encoder(encoder_inputs) 
>>> encoder_states = (state_h, state_c)
1
2

Figure 10.9. Thought decoder
Listing 10.2. Thought decoder in Keras
1
2
3
4
5
6
7
8
>>> decoder_inputs = Input(shape=(None, output_vocab_size)) 
>>> decoder_lstm = LSTM( 
...     num_neurons,return_sequences=True, return_state=True) 
>>> decoder_outputs, _, _ = decoder_lstm( 
...     decoder_inputs, initial_state=encoder_states) 
>>> decoder_dense = Dense( 
...     output_vocab_size, activation='softmax') 
>>> decoder_outputs = decoder_dense(decoder_outputs)
1
2
3
4
Listing 10.3. Keras functional API ( Model() )
1
2
3
>>> model = Model( 
...     inputs=[encoder_inputs, decoder_inputs], 
...     outputs=decoder_outputs)
1
Listing 10.4. Train a sequence-to-sequence model in Keras
1
2
3
4
>>> model.compile(optimizer='rmsprop', loss='categorical_crossentropy') 
>>> model.fit([encoder_input_data, decoder_input_data], 
               decoder_target_data, 
              batch_size=batch_size, epochs=epochs)
1
2

Listing 10.5. Decoder for generating text using the generic Keras Model
1 >>> encoder_model = Model(inputs=encoder_inputs, outputs=encoder_states)
1
Listing 10.6. Sequence generator for random thoughts
1
2
3
4
5
6
7
8
9
10
11
>>> thought_input = [Input(shape=(num_neurons,)), 
...     Input(shape=(num_neurons,))] 
>>> decoder_outputs, state_h, state_c = decoder_lstm( 
...     decoder_inputs, initial_state=thought_input) 
>>> decoder_states = [state_h, state_c] 
>>> decoder_outputs = decoder_dense(decoder_outputs) 
  
  
>>> decoder_model = Model( 
...     inputs=[decoder_inputs] + thought_input, 
...     output=[decoder_outputs] + decoder_states)
1
2
3
4
5
6
7
Listing 10.7. Simple decoder—next word prediction
1
2
3
4
5
6
... 
>>> thought = encoder_model.predict(input_seq) 
... 
>>> while not stop_condition: 
...     output_tokens, h, c = decoder_model.predict( 
...         [target_seq] + thought)
1
2
3
Listing 10.8. Build character sequence-to-sequence training set
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
>>> from nlpia.loaders import get_data 
>>> df = get_data('moviedialog') 
>>> input_texts, target_texts = [], [] 
>>> input_vocabulary = set() 
>>> output_vocabulary = set() 
>>> start_token = '\t' 
>>> stop_token = '\n' 
>>> max_training_samples = min(25000, len(df) - 1) 
  
>>> for input_text, target_text in zip(df.statement, df.reply): 
...     target_text = start_token + target_text \ 
...         + stop_token 
...     input_texts.append(input_text) 
...     target_texts.append(target_text) 
...     for char in input_text: 
...         if char not in input_vocabulary: 
...             input_vocabulary.add(char) 
1
2
3
4
5
6

18
19
20
...     for char in target_text: 
...         if char not in output_vocabulary: 
...             output_vocabulary.add(char)
Listing 10.9. Character sequence-to-sequence model parameters
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
>>> input_vocabulary = sorted(input_vocabulary) 
 
>>> output_vocabulary = sorted(output_vocabulary) 
>>> input_vocab_size = len(input_vocabulary) 
>>> output_vocab_size = len(output_vocabulary) 
>>> max_encoder_seq_length = max( 
...     [len(txt) for txt in input_texts]) 
>>> max_decoder_seq_length = max( 
...     [len(txt) for txt in target_texts]) 
 
>>> input_token_index = dict([(char, i) for i, char in 
...     enumerate(input_vocabulary)]) 
>>> target_token_index = dict( 
...     [(char, i) for i, char in enumerate(output_vocabulary)]) 
>>> reverse_input_char_index = dict((i, char) for char, i in 
...     input_token_index.items()) 
>>> reverse_target_char_index = dict((i, char) for char, i in 
...     target_token_index.items())
1
2
3
4
5
Listing 10.10. Construct character sequence encoder-decoder training set
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
>>> import numpy as np 
  
>>> encoder_input_data = np.zeros((len(input_texts), 
...     max_encoder_seq_length, input_vocab_size), 
...     dtype='float32') 
>>> decoder_input_data = np.zeros((len(input_texts), 
...     max_decoder_seq_length, output_vocab_size), 
...     dtype='float32') 
>>> decoder_target_data = np.zeros((len(input_texts), 
...     max_decoder_seq_length, output_vocab_size), 
...     dtype='float32') 
  
>>> for i, (input_text, target_text) in enumerate( 
...             zip(input_texts, target_texts)): 
...     for t, char in enumerate(input_text): 
...         encoder_input_data[ 
...             i, t, input_token_index[char]] = 1. 
...     for t, char in enumerate(target_text): 
...         decoder_input_data[ 
...             i, t, target_token_index[char]] = 1. 
...         if t > 0: 
...             decoder_target_data[i, t - 1, target_token_index[char]] = 1
1
2
3
4
5
6

Listing 10.11. Construct and train a character sequence encoder-decoder network
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
>>> from keras.models import Model 
>>> from keras.layers import Input, LSTM, Dense 
  
>>> batch_size = 64 
>>> epochs = 100 
>>> num_neurons = 256 
  
>>> encoder_inputs = Input(shape=(None, input_vocab_size)) 
>>> encoder = LSTM(num_neurons, return_state=True) 
>>> encoder_outputs, state_h, state_c = encoder(encoder_inputs) 
>>> encoder_states = [state_h, state_c] 
  
>>> decoder_inputs = Input(shape=(None, output_vocab_size)) 
>>> decoder_lstm = LSTM(num_neurons, return_sequences=True, 
...                     return_state=True) 
>>> decoder_outputs, _, _ = decoder_lstm(decoder_inputs, 
...     initial_state=encoder_states) 
>>> decoder_dense = Dense(output_vocab_size, activation='softmax') 
>>> decoder_outputs = decoder_dense(decoder_outputs) 
>>> model = Model([encoder_inputs, decoder_inputs], decoder_outputs) 
  
>>> model.compile(optimizer='rmsprop', loss='categorical_crossentropy', 
...               metrics=['acc']) 
>>> model.fit([encoder_input_data, decoder_input_data], 
...     decoder_target_data, batch_size=batch_size, epochs=epochs, 
...     validation_split=0.1)
1
2
3
4
Listing 10.12. Construct response generator model
1
2
3
4
5
6
7
8
9
10
11
>>> encoder_model = Model(encoder_inputs, encoder_states) 
>>> thought_input = [ 
...     Input(shape=(num_neurons,)), Input(shape=(num_neurons,))] 
>>> decoder_outputs, state_h, state_c = decoder_lstm( 
...     decoder_inputs, initial_state=thought_input) 
>>> decoder_states = [state_h, state_c] 
>>> decoder_outputs = decoder_dense(decoder_outputs) 
  
>>> decoder_model = Model( 
...     inputs=[decoder_inputs] + thought_input, 
...     output=[decoder_outputs] + decoder_states)
Listing 10.13. Build a character-based translator
1
2
3
4
5
6
7
8
>>> def decode_sequence(input_seq): 
...     thought = encoder_model.predict(input_seq) 
  
...     target_seq = np.zeros((1, 1, output_vocab_size)) 
...     target_seq[0, 0, target_token_index[stop_token] 
...         ] = 1. 
...     stop_condition = False 
...     generated_sequence = '' 
1
2
3

Figure 10.10. Bucketing applied to target sequences
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
  
...     while not stop_condition: 
...         output_tokens, h, c = decoder_model.predict( 
...             [target_seq] + thought) 
  
...         generated_token_idx = np.argmax(output_tokens[0, -1, :]) 
...         generated_char = reverse_target_char_index[generated_token_idx] 
...         generated_sequence += generated_char 
...         if (generated_char == stop_token or 
...                 len(generated_sequence) > max_decoder_seq_length 
...                 ):   
...             stop_condition = True 
  
...         target_seq = np.zeros((1, 1, output_vocab_size)) 
...         target_seq[0, 0, generated_token_idx] = 1. 
...         thought = [h, c] 
  
...     return generated_sequence
4
5
6
7
1
2
3
4
5
6
7
>>> def response(input_text): 
...    input_seq = np.zeros((1, max_encoder_seq_length, input_vocab_size), 
...        dtype='float32') 
...    for t, char in enumerate(input_text): 
...        input_seq[0, t, input_token_index[char]] = 1. 
...    decoded_sentence = decode_sequence(input_seq) 
...    print('Bot Reply (Decoded sentence):', decoded_sentence)
1
2
1
2
3
4
5
6
7
8
9
10
>>> response("what is the internet?") 
Bot Reply (Decoded sentence): it's the best thing i can think of anything. 
  
>>> response("why?") 
Bot Reply (Decoded sentence): i don't know. i think it's too late. 
>>> response("do you like coffee?") 
Bot Reply (Decoded sentence): yes. 
  
>>> response("do you like football?") 
Bot Reply (Decoded sentence): yeah.

Figure 10.11. Overview of the attention mechanism

CHAPTER 11
Figure 11.1. Stanislav knowledge graph
1 ('Stanislav Petrov', 'is-a', 'lieutenant colonel')
Listing 11.1. Pattern hardcoded in Python
1
2
3
4
5
6
7
8
9
10
11
>>> def find_greeting(s): 
...     """ Return greeting str (Hi, etc) if greeting pattern matches """ 
...     if s[0] == 'H': 
...         if s[:3] in ['Hi', 'Hi ', 'Hi,', 'Hi!']: 
...             return s[:2] 
...         elif s[:6] in ['Hello', 'Hello ', 'Hello,', 'Hello!']: 
...             return s[:5] 
...     elif s[0] == 'Y': 
...         if s[1] == 'o' and s[:3] in ['Yo', 'Yo,', 'Yo ', 'Yo!']: 
...             return s[:2] 
...     return None
Listing 11.2. Brittle pattern-matching example

1
2
3
4
5
6
7
8
9
10
11
12
>>> find_greeting('Hi Mr. Turing!') 
'Hi' 
>>> find_greeting('Hello, Rosa.') 
'Hello' 
>>> find_greeting("Yo, what's up?") 
'Yo' 
>>> find_greeting("Hello") 
'Hello' 
>>> print(find_greeting("hello")) 
None 
>>> print(find_greeting("HelloWorld")) 
None
Listing 11.3. Regular expression for GPS coordinates
1
2
3
4
5
6
7
8
9
10
11
12
13
14
>>> import re 
>>> lat = r'([-]?[0-9]?[0-9][.][0-9]{2,10})' 
>>> lon = r'([-]?1?[0-9]?[0-9][.][0-9]{2,10})' 
>>> sep = r'[,/ ]{1,3}' 
>>> re_gps = re.compile(lat + sep + lon) 
  
>>> re_gps.findall('http://...maps/@34.0551066,-118.2496763...') 
[(34.0551066, -118.2496763)] 
  
>>> re_gps.findall("https://www.openstreetmap.org/#map=10/5.9666/116.0566") 
[('5.9666', '116.0566')] 
  
>>> re_gps.findall("Zig Zag Cafe is at 45.344, -121.9431 on my GPS.") 
[('45.3440', '-121.9431')]
Listing 11.4. Regular expression for US dates
1
2
3
4
5
>>> us = r'((([01]?\d)[-/]([0123]?\d))([-/]([0123]\d)\d\d)?)' 
>>> mdy = re.findall(us, 'Santa came 12/25/2017. An elf appeared 12/12.') 
>>> mdy 
[('12/25/2017', '12/25', '12', '25', '/2017', '20'), 
 ('12/12', '12/12', '12', '12', '', '')]
Listing 11.5. Structuring extracted dates
1
2
3
4
5
>>> dates = [{'mdy': x[0], 'my': x[1], 'm': int(x[2]), 'd': int(x[3]), 
...     'y': int(x[4].lstrip('/') or 0), 'c': int(x[5] or 0)} for x in mdy] 
>>> dates 
[{'mdy': '12/25/2017', 'my': '12/25', 'm': 12, 'd': 25, 'y': 2017, 'c': 20}, 
 {'mdy': '12/12', 'my': '12/12', 'm': 12, 'd': 12, 'y': 0, 'c': 0}]

Listing 11.6. Basic context maintenance
1
2
3
4
5
6
7
8
9
10
11
>>> for i, d in enumerate(dates): 
...     for k, v in d.items(): 
...         if not v: 
...             d[k] = dates[max(i - 1, 0)][k] 
>>> dates 
[{'mdy': '12/25/2017', 'my': '12/25', 'm': 12, 'd': 25, 'y': 2017, 'c': 20}, 
 {'mdy': '12/12', 'my': '12/12', 'm': 12, 'd': 12, 'y': 2017, 'c': 20}] 
>>> from datetime import date 
>>> datetimes = [date(d['y'], d['m'], d['d']) for d in dates] 
>>> datetimes 
[datetime.date(2017, 12, 25), datetime.date(2017, 12, 12)]
1
Listing 11.7. Regular expression for European dates
1
2
3
4
5
6
7
8
9
10
11
>>> eu = r'((([0123]?\d)[-/]([01]?\d))([-/]([0123]\d)?\d\d)?)' 
>>> dmy = re.findall(eu, 'Alan Mathison Turing OBE FRS (23/6/1912-7/6/1954) \ 
...     was an English computer scientist.') 
>>> dmy 
[('23/6/1912', '23/6', '23', '6', '/1912', '19'), 
 ('7/6/1954', '7/6', '7', '6', '/1954', '19')] 
>>> dmy = re.findall(eu, 'Alan Mathison Turing OBE FRS (23/6/12-7/6/54) \ 
...     was an English computer scientist.') 
>>> dmy 
[('23/6/12', '23/6', '23', '6', '/12', ''), 
 ('7/6/54', '7/6', '7', '6', '/54', '')]
Listing 11.8. Recognizing years
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
>>> yr_19xx = ( 
...     r'\b(?P<yr_19xx>' + 
...     '|'.join('{}'.format(i) for i in range(30, 100)) + 
...     r')\b' 
...     ) 
>>> yr_20xx = ( 
...     r'\b(?P<yr_20xx>' + 
...     '|'.join('{:02d}'.format(i) for i in range(10)) + '|' + 
...     '|'.join('{}'.format(i) for i in range(10, 30)) + 
...     r')\b' 
...     ) 
>>> yr_cent = r'\b(?P<yr_cent>' + '|'.join( 
...     '{}'.format(i) for i in range(1, 40)) + r')' 
>>> yr_ccxx = r'(?P<yr_ccxx>' + '|'.join( 
...     '{:02d}'.format(i) for i in range(0, 100)) + r')\b' 
>>> yr_xxxx = r'\b(?P<yr_xxxx>(' + yr_cent + ')(' + yr_ccxx + r'))\b' 
>>> yr = ( 
...     r'\b(?P<yr>' + 
...     yr_19xx + '|' + yr_20xx + '|' + yr_xxxx + 
1
2
3
4

20
21
22
23
24
25
26
...     r')\b' 
...     ) 
>>> groups = list(re.finditer( 
...     yr, "0, 2000, 01, '08, 99, 1984, 2030/1970 85 47 `66")) 
>>> full_years = [g['yr'] for g in groups] 
>>> full_years 
['2000', '01', '08', '99', '1984', '2030', '1970', '85', '47', '66']
Listing 11.9. Recognizing month words with regular expressions
1
2
3
4
5
6
7
8
9
>>> mon_words = 'January February March April May June July ' \ 
...     'August September October November December' 
>>> mon = (r'\b(' + '|'.join('{}|{}|{}|{}|{:02d}'.format( 
...     m, m[:4], m[:3], i + 1, i + 1) for i, m in  
 enumerate(mon_words.split())) + 
...     r')\b') 
>>> re.findall(mon, 'January has 31 days, February the 2nd month  
 of 12, has 28, except in a Leap Year.') 
['January', 'February', '12']
Listing 11.10. Combining information extraction regular expressions
1
2
3
4
5
6
7
8
9
>>> day = r'|'.join('{:02d}|{}'.format(i, i) for i in range(1, 32)) 
>>> eu = (r'\b(' + day + r')\b[-,/ ]{0,2}\b(' + 
...     mon + r')\b[-,/ ]{0,2}\b(' + yr.replace('<yr', '<eu_yr') + r')\b') 
>>> us = (r'\b(' + mon + r')\b[-,/ ]{0,2}\b(' + 
...     day + r')\b[-,/ ]{0,2}\b(' + yr.replace('<yr', '<us_yr') + r')\b') 
>>> date_pattern = r'\b(' + eu + '|' + us + r')\b' 
>>> list(re.finditer(date_pattern, '31 Oct, 1970 25/12/2017')) 
[<_sre.SRE_Match object; span=(0, 12), match='31 Oct, 1970'>, 
 <_sre.SRE_Match object; span=(13, 23), match='25/12/2017'>]
Listing 11.11. Validating dates
1
2
3
4
5
6
7
8
9
10
11
12
13
>>> import datetime 
>>> dates = [] 
>>> for g in groups: 
...     month_num = (g['us_mon'] or g['eu_mon']).strip() 
...     try: 
...         month_num = int(month_num) 
...     except ValueError: 
...         month_num = [w[:len(month_num)] 
...             for w in mon_words].index(month_num) + 1 
...     date = datetime.date( 
...         int(g['us_yr'] or g['eu_yr']), 
...         month_num, 
...         int(g['us_day'] or g['eu_day'])) 

Figure 11.2. The Pascagoula people
14
15
16
...     dates.append(date) 
>>> dates 
[datetime.date(1970, 10, 31), datetime.date(2017, 12, 25)]
Listing 11.12. POS tagging with spaCy
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
>>> import spacy 
>>> en_model = spacy.load('en_core_web_md') 
>>> sentence = ("In 1541 Desoto wrote in his journal that the Pascagoula peop 
     le " + 
...     "ranged as far north as the confluence of the Leaf and Chickasawhay r 
     ivers at 30.4, -88.5.") 
>>> parsed_sent = en_model(sentence) 
>>> parsed_sent.ents 
(1541, Desoto, Pascagoula, Leaf, Chickasawhay, 30.4) 
  
>>> ' '.join(['{}_{}'.format(tok, tok.tag_) for tok in parsed_sent]) 
'In_IN 1541_CD Desoto_NNP wrote_VBD in_IN his_PRP$ journal_NN that_IN the_DT 
     Pascagoula_NNP people_NNS 
 ranged_VBD as_RB far_RB north_RB as_IN the_DT confluence_NN of_IN the_DT Lea 
     f_NNP and_CC Chickasawhay_NNP 
 rivers_VBZ at_IN 30.4_CD ,_, -88.5_NFP ._.'
1
2
Listing 11.13. Visualize a dependency tree
1
2
3
4
5
6
>>> from spacy.displacy import render 
>>> sentence = "In 1541 Desoto wrote in his journal about the Pascagoula." 
>>> parsed_sent = en_model(sentence) 
>>> with open('pascagoula.html', 'w') as f: 
...     f.write(render(docs=parsed_sent, page=True,  
 options=dict(compact=True)))

Listing 11.14. Helper functions for spaCy tagged strings
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
>>> import pandas as pd 
>>> from collections import OrderedDict 
>>> def token_dict(token): 
...     return OrderedDict(ORTH=token.orth_, LEMMA=token.lemma_, 
...         POS=token.pos_, TAG=token.tag_, DEP=token.dep_) 
  
>>> def doc_dataframe(doc): 
...     return pd.DataFrame([token_dict(tok) for tok in doc]) 
  
>>> doc_dataframe(en_model("In 1541 Desoto met the Pascagoula.")) 
         ORTH       LEMMA    POS  TAG    DEP 
0          In          in    ADP   IN   prep 
1        1541        1541    NUM   CD   pobj 
2      Desoto      desoto  PROPN  NNP  nsubj 
3         met        meet   VERB  VBD   ROOT 
4         the         the    DET   DT    det 
5  Pascagoula  pascagoula  PROPN  NNP   dobj 
6           .           .  PUNCT    .  punct
1 'PROPN ANYWORD? met ANYWORD? ANYWORD? PROPN'
Listing 11.15. Example spaCy POS pattern
1
2
3
>>> pattern = [{'TAG': 'NNP', 'OP': '+'}, {'IS_ALPHA': True, 'OP': '*'}, 
...            {'LEMMA': 'meet'}, 
...            {'IS_ALPHA': True, 'OP': '*'}, {'TAG': 'NNP', 'OP': '+'}]
Listing 11.16. Creating a POS pattern matcher with spaCy
1
2
3
4
5
6
7
8
9
10
>>> from spacy.matcher import Matcher 
>>> doc = en_model("In 1541 Desoto met the Pascagoula.") 
>>> matcher = Matcher(en_model.vocab) 
>>> matcher.add('met', None, pattern) 
>>> m = matcher(doc) 
>>> m 
[(12280034159272152371, 2, 6)] 
  
>>> doc[m[0][1]:m[0][2]] 
Desoto met the Pascagoula

Listing 11.17. Using a POS pattern matcher
1
2
3
4
5
6
7
8
9
10
11
12
>>> doc = en_model("October 24: Lewis and Clark met their first Mandan Chief, 
      Big White.") 
>>> m = matcher(doc)[0] 
>>> m 
(12280034159272152371, 3, 11) 
  
>>> doc[m[1]:m[2]] 
Lewis and Clark met their first Mandan Chief 
  
>>> doc = en_model("On 11 October 1986, Gorbachev and Reagan met at a house") 
>>> matcher(doc) 
[]
1
Listing 11.18. Combining multiple patterns for a more robust pattern matcher
1
2
3
4
5
6
7
8
9
10
11
12
13
14
>>> doc = en_model("On 11 October 1986, Gorbachev and Reagan met at a house") 
>>> pattern = [{'TAG': 'NNP', 'OP': '+'}, {'LEMMA': 'and'}, {'TAG': 'NNP', 'O 
     P': '+'}, 
...            {'IS_ALPHA': True, 'OP': '*'}, {'LEMMA': 'meet'}] 
>>> matcher.add('met', None, pattern) 
>>> m = matcher(doc) 
>>> m 
[(14332210279624491740, 5, 9), 
 (14332210279624491740, 5, 11), 
 (14332210279624491740, 7, 11), 
 (14332210279624491740, 5, 12)] 
  
>>> doc[m[-1][1]:m[-1][2]] 
Gorbachev and Reagan met at a house
1
2
3
1
2
3
>>> re.split(r'[!.?]+[ $]', "Hello World.... Are you there?!?! I'm going 
     to Mars!") 
['Hello World', 'Are you there', "I'm going to Mars!"]
1
2
3
>>> re.split(r'[!.?] ', "The author wrote \"'I don't think it's conscious.' 
     Turing said.\"") 
['The author wrote "\'I don\'t think it\'s conscious.\' Turing said."']

1
2
3
4
>>> re.split(r'[!.?] ', "The author wrote \"'I don't think it's conscious.' 
 Turing said.\" But I stopped reading.") 
['The author wrote "\'I don\'t think it\'s conscious.\' Turing said." But I 
 stopped reading."']
1
2
>>> re.split(r'(?<!\d)\.|\.(?!\d)', "I went to GT.You?") 
['I went to GT', 'You?']
1
2
3
4
5
6
7
8
9
10
11
>>> from nlpia.data.loaders import get_data 
>>> regex = re.compile(r'((?<!\d)\.|\.(?!\d))|([!.?]+)[ $]+') 
>>> examples = get_data('sentences-tm-town') 
>>> wrong = [] 
>>> for i, (challenge, text, sents) in enumerate(examples): 
...     if tuple(regex.split(text)) != tuple(sents): 
...         print('wrong {}: {}{}'.format(i, text[:50], '...' if len(text) > 
     50 else '')) 
...         wrong += [i] 
>>> len(wrong), len(examples) 
(61, 61)

CHAPTER 12
Figure 12.1. Chatbot techniques used for some example applications
Listing 12.1. nlpia/book/examples/greeting.v2.aiml
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
<?xml version="1.0" encoding="UTF-8"?><aiml version="2.0"> 
<category> 
    <pattern>HI</pattern> 
<template>Hi!</template> 
</category> 
<category> 
    <pattern>[HELLO HI YO YOH YO'] [ROSA ROSE CHATTY CHATBOT BOT CHATTERBOT]< 
     /pattern> 
    <template>Hi , How are you?</template> 
</category> 
<category> 
    <pattern>[HELLO HI YO YOH YO' 'SUP SUP OK HEY] [HAL YOU U YALL Y'ALL YOUS 
      YOUSE]</pattern> 
    <template>Good one.</template> 
</category> 
</aiml>

Listing 12.2. nlpia/nlpia/data/greeting_step1.aiml
1
2
3
4
5
6
7
8
9
10
11
12
<?xml version="1.0" encoding="UTF-8"?><aiml version="1.0.1"> 
  
<category> 
    <pattern>HELLO ROSA </pattern> 
    <template>Hi Human!</template> 
</category> 
<category> 
    <pattern>HELLO TROLL </pattern> 
    <template>Good one, human.</template> 
</category> 
 
</aiml>
Listing 12.3. nlpia/book/examples/ch12.py
1
2
3
4
5
6
7
8
9
10
11
12
>>> import os 
>>> from nlpia.constants import DATA_PATH 
>>> import aiml_bot 
 
>>> bot = aiml_bot.Bot( 
...     learn=os.path.join(DATA_PATH, 'greeting_step1.aiml')) 
Loading /Users/hobs/src/nlpia/nlpia/data/greeting_step1.aiml... 
done (0.00 seconds) 
>>> bot.respond("Hello Rosa,") 
'Hi there!' 
>>> bot.respond("hello !!!troll!!!") 
'Good one, human.'
Listing 12.4. nlpia/nlpia/book/examples/ch12.py
1
2
3
4
5
6
>>> bot.respond("Helo Rosa") 
WARNING: No match found for input: Helo Rosa 
'' 
>>> bot.respond("Hello Ro-sa") 
WARNING: No match found for input: Hello Ro-sa 
''
Listing 12.5. nlpia/data/greeting_step2.aiml
1
2
3
4
5
<category><pattern>HELO *        </pattern><template><srai>HELLO <star/> 
</srai></template></category> 
<category><pattern>HI *          </pattern><template><srai>HELLO <star/> 
</srai></template></category> 
<category><pattern>HIYA *        </pattern><template><srai>HELLO <star/> 

6
7
8
9
10
11
12
13
14
15
16
</srai></template></category> 
<category><pattern>HYA *         </pattern><template><srai>HELLO <star/> 
</srai></template></category> 
<category><pattern>HY *          </pattern><template><srai>HELLO <star/> 
</srai></template></category> 
<category><pattern>HEY *         </pattern><template><srai>HELLO <star/> 
</srai></template></category> 
<category><pattern>WHATS UP *    </pattern><template><srai>HELLO <star/> 
</srai></template></category> 
<category><pattern>WHAT IS UP *  </pattern><template><srai>HELLO <star/> 
</srai></template></category>
Listing 12.6. nlpia/nlpia/book/examples/ch12.py
1
2
3
4
5
6
7
8
9
>>> bot.learn(os.path.join(DATA_PATH, 'greeting_step2.aiml')) 
>>> bot.respond("Hey Rosa") 
'Hi there!' 
>>> bot.respond("Hi Rosa") 
'Hi there!' 
>>> bot.respond("Helo Rosa") 
'Hi there!' 
>>> bot.respond("hello **troll** !!!")  
 'Good one, human.'
Listing 12.7. nlpia/nlpia/data/greeting_step3.aiml
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
<category><pattern>HELLO ROSA </pattern><template> 
    <random> 
        <li>Hi Human!</li> 
        <li>Hello friend</li> 
        <li>Hi pal</li> 
        <li>Hi!</li> 
        <li>Hello!</li> 
                <li>Hello to you too!</li> 
        <li>Greetings Earthling ;)</li> 
        <li>Hey you :)</li> 
        <li>Hey you!</li> 
    </random></template> 
</category> 
<category><pattern>HELLO TROLL </pattern><template> 
    <random> 
        <li>Good one, Human.</li> 
        <li>Good one.</li> 
        <li>Nice one, Human.</li> 
        <li>Nice one.</li> 
        <li>Clever.</li> 
        <li>:)</li> 
    </random></template> 
</category>

Figure 12.2. Managing state (context)
Listing 12.8. nlpia/nlpia/book/examples/ch12.py
1
2
3
4
5
6
7
>>> bot.learn(os.path.join(DATA_PATH, 'greeting_step3.aiml')) 
>>> bot.respond("Hey Rosa") 
'Hello friend' 
>>> bot.respond("Hey Rosa") 
'Hey you :)' 
>>> bot.respond("Hey Rosa") 
'Hi Human!'
1
2
3
4
5
>>> airas_spec = [ 
...     ["Hi {name}","Hi {username} how are you?","ROOT","GREETING"], 
...     ["What is your name?", 
...      "Hi {username} how are you?","ROOT","GREETING"], 
...     ]
Listing 12.9. ch12_retrieval.py
1
2
3
4
5
6
>>> from nlpia.data.loaders import get_data 
>>> df = get_data('ubuntu_dialog') 
Downloading ubuntu_dialog 
requesting URL: 
https://www.dropbox.com/s/krvi79fbsryytc2/ubuntu_dialog.csv.gz?dl=1 
remote size: 296098788 

7
8
Downloading to /Users/hobs/src/nlpia/nlpia/bigdata/ubuntu_dialog.csv.gz 
39421it [00:39, 998.09it/s]
Listing 12.10. ch12_retrieval.py
1
2
3
4
5
6
7
8
9
10
>>> df.head(4) 
                        Context                                Utterance 
0  i think we could import the old comments via r...   basically each xfree86 
       upload will NOT force u... 
1  I'm not suggesting all - 
       only the ones you mod...                         oh? oops. __eou__ 
2  afternoon all __eou__ not entirely related to ...    we'll have a BOF about 
      this __eou__ so you're ... 
3  interesting __eou__ grub-install worked with / 
     ...   i fully endorse this suggestion </quimby> __eo...
Listing 12.11. ch12_retrieval.py
1
2
3
4
5
6
7
>>> import re 
>>> def split_turns(s, splitter=re.compile('__eot__')): 
...    for utterance in splitter.split(s): 
...        utterance = utterance.replace('__eou__', '\n') 
...        utterance = utterance.replace('__eot__', '').strip() 
...        if len(utterance): 
...            yield utterance
Listing 12.12. ch12_retrieval.py
1
2
3
4
5
6
>>> for i, record in df.head(3).iterrows(): 
...     statement = list(split_turns(record.Context))[-1] 
...     reply = list(split_turns(record.Utterance))[-1] 
...     print('Statement: {}'.format(statement)) 
...     print() 
...     print('Reply: {}'.format(reply))
1
2
3
4
5
Statement: I would prefer to avoid it at this stage. this is something that 
     has gone into XSF svn, I assume? 
Reply:  each xfree86 upload will NOT force users to upgrade 100Mb of fonts 
     for nothing 
 no something i did in my spare time. 

6
7
8
9
10
11
12
13
14
15
16
17
  
Statement: ok, it sounds like you're agreeing with me, then 
 though rather than "the ones we modify", my idea is "the ones we need to 
     merge" 
Reply: oh? oops. 
 
Statement: should g2 in ubuntu do the magic dont-focus-window tricks? 
 join the gang, get an x-series thinkpad 
 sj has hung on my box, again. 
 what is monday mornings discussion actually about? 
Reply: we'll have a BOF about this 
 so you're coming tomorrow ?
Listing 12.13. ch12_retrieval.py
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
>>> from tqdm import tqdm 
 
>>> def preprocess_ubuntu_corpus(df): 
...     """ 
...     Split all strings in df.Context and df.Utterance on 
...     __eot__ (turn) markers 
...     """ 
...     statements = [] 
...     replies = [] 
...     for i, record in tqdm(df.iterrows()): 
...         turns = list(split_turns(record.Context)) 
...         statement = turns[-1] if len(turns) else '\n' 
...         statements.append(statement) 
...         turns = list(split_turns(record.Utterance)) 
...         reply = turns[-1] if len(turns) else '\n' 
...         replies.append(reply) 
...     df['statement'] = statements 
...     df['reply'] = replies 
...     return df
1
Listing 12.14. ch12_retrieval.py
1
2
3
4
>>> from sklearn.feature_extraction.text import TfidfVectorizer 
>>> df = preprocess_ubuntu_corpus(df) 
>>> tfidf = TfidfVectorizer(min_df=8, max_df=.3, max_features=50000) 
>>> tfidf.fit(df.statement)
1
Listing 12.15. ch12_retrieval.py
1
2
>>> X = tfidf.transform(df.statement) 
>>> X = pd.DataFrame(X.todense(), columns=tfidf.get_feature_names())

Figure 12.3. Advantages and disadvantages of four chatbot approaches
Listing 12.16. ch12_retrieval.py
1
2
3
4
>>> x = tfidf.transform(['This is an example statement that\ 
...     we want to retrieve the best reply for.']) 
>>> cosine_similarities = x.dot(X.T) 
>>> reply = df.loc[cosine_similarities.argmax()]
Listing 12.17. ch12_chatterbot.sql
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
sqlite> .tables 
conversation              response                  tag 
conversation_association  statement                 tag_association 
sqlite> .width 5 25 10 5 40 
sqlite> .mode columns 
sqlite> .mode column 
sqlite> .headers on 
sqlite> SELECT id, text, occur FROM response LIMIT 9; 
id     text                 occur  statement_text 
-----  -------------------  -----  ---------------------------------------- 
1      What is AI?          2      Artificial Intelligence is the branch of 
2      What is AI?          2      AI is the field of science which concern 
3      Are you sentient?    2      Sort of. 
4      Are you sentient?    2      By the strictest dictionary definition o 
5      Are you sentient?    2      Even though I'm a construct I do have a 
6      Are you sapient?     2      In all probability, I am not. I'm not t 
7      Are you sapient?     2      Do you think I am? 
8      Are you sapient?     2      How would you feel about me if I told yo 
9      Are you sapient?     24     No.

1
2
3
4
5
6
7
8
9
10
You:  Hey 
Will: hello! 
You:  What's up? 
Will: I heard you, but I'm not sure what to do. 
You:  How are you? 
Will: Doing alright. How are you? 
You:  What are you? 
Will: Hmm. I'm not sure what to say. 
You:  Are you a bot? 
Will: I didn't understand that.
1
2
3
4
5
6
>>> pattern_response = { 
...     r"[Hh]ello|[Hh]i[!]*": 
...         r"Hello {user_nickname}, would you like to play a game?", 
...     r"[Hh]ow[\s]*('s|are|'re)?[\s]*[Yy]ou([\s]*doin['g]?)?": 
...         r"I'm {bot_mood}, how are you?", 
...     }
1
2
3
4
5
6
Human: "Where were you born?" 
  
Sports Bot: "I don't know, but how about those Mets?" 
Therapist Bot: "I don't know, but are you close to your mother?" 
Ubuntu Assistant Bot: "I don't know, but how do you shut down your Ubuntu PC 
     at night?"


CHAPTER 13
Listing 13.1. Load word2vec  vectors
1
2
3
4
5
6
7
>>> from nlpia.loaders import get_data 
>>> wv = get_data('word2vec') 
100%|############################| 402111/402111 [01:02<00:00, 6455.57it/s] 
>>> len(wv.vocab), len(wv[next(iter(wv.vocab))]) 
(3000000, 300) 
>>> wv.vectors.shape 
(3000000, 300)
1
Listing 13.2. Initialize 300D AnnoyIndex
1
2
3
>>> from annoy import AnnoyIndex 
>>> num_words, num_dimensions = wv.vectors.shape 
>>> index = AnnoyIndex(num_dimensions)
1
Listing 13.3. Add each word vector to the AnnoyIndex
1
2
3
4
>>> from tqdm import tqdm 
>>> for i, word in enumerate(tqdm(wv.index2word)): 
...     index.add_item(i, wv[word]) 
22%|#######?                   | 649297/3000000 [00:26<01:35, 24587.52it/s]
1
2
Listing 13.4. Build Euclidean distance index with 15 trees
1
2
3
4
5
6
7
8
>>> import numpy as np 
>>> num_trees = int(np.log(num_words).round(0)) 
>>> num_trees 
15 
>>> index.build(num_trees) 
>>> index.save('Word2vec_euc_index.ann') 
True 
>>> w2id = dict(zip(range(len(wv.vocab)), wv.vocab))
1
2
3

Listing 13.5. Find Harry_Potter  neighbors with AnnoyIndex
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
>>> wv.vocab['Harry_Potter'].index 
9494 
>>> wv.vocab['Harry_Potter'].count 
2990506 
>>> w2id = dict(zip( 
...     wv.vocab, range(len(wv.vocab)))) 
>>> w2id['Harry_Potter'] 
9494 
>>> ids = index.get_nns_by_item( 
...     w2id['Harry_Potter'], 11) 
>>> ids 
[9494, 32643, 39034, 114813, ..., 113008, 116741, 113955, 350346] 
>>> [wv.vocab[i] for i in _] 
>>> [wv.index2word[i] for i in _] 
['Harry_Potter', 
 'Narnia', 
 'Sherlock_Holmes', 
 'Lemony_Snicket', 
 'Spiderwick_Chronicles', 
 'Unfortunate_Events', 
 'Prince_Caspian', 
 'Eragon', 
 'Sorcerer_Apprentice', 
 'RL_Stine']
1
2
3
4
Listing 13.6. Top Harry_Potter  neighbors with gensim.KeyedVectors  index
1
2
3
4
5
6
7
8
9
10
11
>>> [word for word, similarity in wv.most_similar('Harry_Potter', topn=10)] 
['JK_Rowling_Harry_Potter', 
 'JK_Rowling', 
 'boy_wizard', 
 'Deathly_Hallows', 
 'Half_Blood_Prince', 
 'Rowling', 
 'Actor_Rupert_Grint', 
 'HARRY_Potter', 
 'wizard_Harry_Potter', 
 'HARRY_POTTER']
Listing 13.7. Build a cosine distance index
1
2
3
4
5
6
7
>>> index_cos = AnnoyIndex( 
...     f=num_dimensions, metric='angular') 
>>> for i, word in enumerate(wv.index2word): 
...     if not i % 100000: 
...         print('{}: {}'.format(i, word)) 
...     index_cos.add_item(i, wv[word]) 
0: </s> 
1
2

8
9
10
100000: distinctiveness 
    ... 
2900000: BOARDED_UP
Listing 13.8. Build a cosine distance index
1
2
3
>>> index_cos.build(30) 
>>> index_cos.save('Word2vec_cos_index.ann') 
True
1
Listing 13.9. Harry_Potter  neighbors in a cosine distance world
1
2
3
4
5
6
7
8
9
10
11
12
13
14
>>> ids_cos = index_cos.get_nns_by_item(w2id['Harry_Potter'], 10) 
>>> ids_cos 
[9494, 37681, 40544, 41526, 14273, 165465, 32643, 420722, 147151, 28829] 
>>> [wv.index2word[i] for i in ids_cos] 
['Harry_Potter', 
 'JK_Rowling', 
 'Deathly_Hallows', 
 'Half_Blood_Prince', 
 'Twilight', 
 'Twilight_saga', 
 'Narnia', 
 'Potter_mania', 
 'Hermione_Granger', 
 'Da_Vinci_Code']
1
Listing 13.10. Search results accuracy for top 10
1
2
3
4
5
6
7
8
9
10
11
12
13
14
>>> pd.DataFrame(annoy_top10, columns=['annoy_15trees', 
...                                    'annoy_30trees']) 
                                 annoy_15trees      annoy_30trees 
gensim 
JK_Rowling_Harry_Potter           Harry_Potter       Harry_Potter 
JK_Rowling                              Narnia         JK_Rowling 
boy_wizard                     Sherlock_Holmes    Deathly_Hallows 
Deathly_Hallows                 Lemony_Snicket  Half_Blood_Prince 
Half_Blood_Prince        Spiderwick_Chronicles           Twilight 
Rowling                     Unfortunate_Events      Twilight_saga 
Actor_Rupert_Grint              Prince_Caspian             Narnia 
HARRY_Potter                            Eragon       Potter_mania 
wizard_Harry_Potter        Sorcerer_Apprentice   Hermione_Granger 
HARRY_POTTER                          RL_Stine      Da_Vinci_Code
1

Figure 13.1. Comparison between a CPU and GPU
Figure 13.2. Matrix multiplication where each row multiplication can be parallelized on
a GPU
Listing 13.11. MinMaxScaler  for low-dimensional vectors
1
2
3
4
5
6
7
8
9
>>> from sklearn.preprocessing import MinMaxScaler 
>>> real_values = [-1.2, 3.4, 5.6, -7.8, 9.0] 
>>>  
>>> scaler = MinMaxScaler() 
  
>>> scaler.fit(real_values) 
  
[int(x * 100.) for x in scaler.transform(real_values)] 
[39, 66, 79, 0, 100]
1
2

Table 13.1. Comparison of GPU platform-as-a-service options (view table Øgure)
Company
Why?
GPU
options
Ease to
get
started
Flexibility
Amazon
Web
Services
(AWS)
Wide range of GPU options; spot
prices; available in various data
centers around the world
NVIDIA
GRID
K520,
Tesla
M60,
Tesla
K80,
Tesla
V100
Medium
High
Google
Cloud
Integrates Google Cloud Kubernetes,
DialogFlow, Jupyter
(colab.research.google.com/notebook)
NVIDIA
Tesla
K80,
Tesla
P100
Medium
High
Microsoft
Azure
Good option if you are using other
Azure services
NVIDIA
Tesla
K80
Medium
High
FloydHub
Command-line interface to bundle
your code
NVIDIA
Tesla
K80,
Tesla
V100
Easy
Medium
Paperspace
Virtual servers and hosted
iPython/Jupyter notebooks with GPU
support
NVIDIA
Maxwell,
Tesla
P5000,
Tesla
P6000,
Tesla
V100
Easy
Medium
Listing 13.12. Error message if your training data exceeds the GPU’s memory
1
2
3
4
5
6
7
8
Epoch 1/10 
Exception in thread Thread-27: 
Traceback (most recent call last): 
  File "/usr/lib/python2.7/threading.py", line 801, in __bootstrap_inner 
    self.run() 
  File "/usr/lib/python2.7/threading.py", line 754, in run 
    self.__target(*self.__args, **self.__kwargs) 
  File "/usr/local/lib/python2.7/dist-packages/keras/engine/training.py", 

Figure 13.3. Loading the training data without a generator function
Figure 13.4. Loading the training data with a generator function
9
10
11
12
13
14
    line 606, in data_generator_task 
    generator_output = next(self._generator) 
  File "/home/ubuntu/django/project/model/load_data.py", line 54, 
    in load_training_set 
    rv = np.array(rv) 
MemoryError
Listing 13.13. Generator for improved RAM efØciency
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
>>> import numpy as np 
>>>  
>>> def training_set_generator(data_store, 
...                            batch_size=32): 
...     X, Y = [], [] 
...     while True: 
...         with open(data_store) as f: 
...             for i, line in enumerate(f): 
...                 if i % batch_size == 0 and X and Y: 
...                     yield np.array(X), np.array(Y) 
...                     X, Y = [], [] 
...                 x, y = line.split('|') 
...                 X.append(x) 
...                 Y.append(y) 
>>>  
>>> data_store = '/path/to/your/data.csv' 
>>> training_set = training_set_generator(data_store)
1
2
3
4
5
6

1
2
3
4
5
6
>>> model.fit(x=X, 
...           y=Y, 
...           batch_size=32, 
...           epochs=10, 
...           verbose=1, 
...           validation_split=0.2)
1
2
3
4
5
6
7
>>> data_store = '/path/to/your/data.csv' 
>>> model.fit_generator(generator=training_set_generator(data_store, 
...     batch_size=32), 
...                     steps_per_epoch=100, 
...                     epochs=10, 
...                     verbose=1, 
...                     validation_data=[X_val, Y_val])
1
2
3
4
1
2
>>> model.evaluate_generator(generator=your_eval_generator(eval_data, 
...     batch_size=32), steps=10)
1
2
>>> model.predict_generator(generator=your_predict_generator(\ 
...     prediction_data, batch_size=32), steps=10)
1 pip install tensorboard
1 tensorboard --logdir=/tmp/

Figure 13.5. Visualize word2vec  embeddings with Tensorboard.
Listing 13.14. Convert an embedding into a TensorBoard projection
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
>>> import os 
>>> import tensorflow as tf 
>>> import numpy as np 
>>> from io import open 
>>> from tensorflow.contrib.tensorboard.plugins import projector 
>>>  
>>>  
>>> def create_projection(projection_data, 
...                       projection_name='tensorboard_viz', 
...                       path='/tmp/'): 
...     meta_file = "{}.tsv".format(projection_name) 
...     vector_dim = len(projection_data[0][1]) 
...     samples = len(projection_data) 
...     projection_matrix = np.zeros((samples, vector_dim)) 
... 
...     with open(os.path.join(path, meta_file), 'w') as file_metadata: 
...         for i, row in enumerate(projection_data): 
...             label, vector = row[0], row[1] 
...             projection_matrix[i] = np.array(vector) 
...             file_metadata.write("{}\n".format(label)) 
... 
...     sess = tf.InteractiveSession() 
... 
...     embedding = tf.Variable(projection_matrix, 
...                             trainable=False, 
...                             name=projection_name) 
...     tf.global_variables_initializer().run() 
... 
...     saver = tf.train.Saver() 
...     writer = tf.summary.FileWriter(path, sess.graph) 
... 
...     config = projector.ProjectorConfig() 
...     embed = config.embeddings.add() 
...     embed.tensor_name = '{}'.format(projection_name) 
...     embed.metadata_path = os.path.join(path, meta_file) 
... 
...     projector.visualize_embeddings(writer, config) 
...     saver.save(sess, os.path.join(path, '{}.ckpt'\ 
...         .format(projection_name))) 
...     print('Run `tensorboard --logdir={0}` to run\ 
...           visualize result on tensorboard'.format(path))
1
2
3
4
5
1
2
3
4
5
6
7
>>> projection_name = "NLP_in_Action" 
>>> projection_data = [ 
>>>     ('car', [0.34, ..., -0.72]), 
>>>     ... 
>>>     ('toy', [0.46, ..., 0.39]), 
>>> ] 
>>> create_projection(projection_data, projection_name)


