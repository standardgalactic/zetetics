A Theoretical View on Sparsely Activated
Networks
Cenk Baykal
Google Research
baykalc@google.com
Nishanth Dikkala
Google Research
nishanthd@google.com
Rina Panigrahy
Google Research
rinap@google.com
Cyrus Rashtchian
Google Research
cyroid@google.com
Xin Wang
Google Research
wanxin@google.com
Abstract
Deep and wide neural networks successfully ﬁt very complex functions today, but dense
models are starting to be prohibitively expensive for inference. To mitigate this, one promising
direction is networks that activate a sparse subgraph of the network. The subgraph is chosen
by a data-dependent routing function, enforcing a ﬁxed mapping of inputs to subnetworks
(e.g., the Mixture of Experts (MoE) paradigm in Switch Transformers). However, prior work
is largely empirical, and while existing routing functions work well in practice, they do not
lead to theoretical guarantees on approximation ability. We aim to provide a theoretical
explanation for the power of sparse networks.
As our ﬁrst contribution, we present a
formal model of data-dependent sparse networks that captures salient aspects of popular
architectures. We then introduce a routing function based on locality sensitive hashing (LSH)
that enables us to reason about how well sparse networks approximate target functions.
After representing LSH-based sparse networks with our model, we prove that sparse networks
can match the approximation power of dense networks on Lipschitz functions. Applying
LSH on the input vectors means that the experts interpolate the target function in diﬀerent
subregions of the input space. To support our theory, we deﬁne various datasets based
on Lipschitz target functions, and we show that sparse networks give a favorable trade-oﬀ
between number of active units and approximation quality.
1
Introduction
Overparameterized neural networks continue to yield performance gains as their sizes increase. This trend
has been most prominent with large Transformer based language models [6, 12, 32]. However, using large,
dense networks makes training and inference very expensive, and computing a forward pass may require
trillions of ﬂoating point operations (FLOPs). It is an active area of research to improve the scalability and
eﬃciency without decreasing the expressiveness or quality of the models.
One way to achieve this goal is to only activate part of the network at a time. For example, the Mixture
of Experts (MoE) paradigm [22, 37] uses a two-step approach. First, each input is mapped to a certain
subnetwork, known as an expert. Then, upon receiving this input, only this particular subnetwork performs
inference, leading to a smaller number of operations compared to the total number of parameters across all
experts. Some variations of this idea map parts of an input (called tokens) individually to diﬀerent experts.
Another well-studied generalization is to map an input to a group of subnetworks and aggregate their output
in a weighted manner. Switch Transformers [14] successfully use a reﬁned version of the MoE idea, where the
input may be the embedding of a token or part of a hidden layer’s output. Researchers have investigated many
ways to perform the mapping, such as Scaling Transformers [21] or using pseudo-random hash functions [34].
In all cases, sparsity occurs in the network because after the input is mapped to the subnetwork, only this
arXiv:2208.04461v1  [cs.LG]  8 Aug 2022

subset of neurons needs to be activated. Moreover, the computation of the mapping function, sometimes
referred to as a ‘routing’ function, takes signiﬁcantly less time than the computation across all experts.
The success of these approaches is surprising. Restricting to a subnetwork could instead reduce the expressive
power and the quality of the model. However, the guiding wisdom seems to be that it suﬃces to maximize the
number of parameters while ensuring computational eﬃciency. Not all parameters of a network are required
for the model to make its prediction for any given example. Our goal is to provide a theoretical explanation
for the power of sparse models on concrete examples.
Our Results.
Our ﬁrst contribution is a formal model of networks that have one or more sparsely activated
layers. These sparse layers are data dependent: the subset of network activations and their weight coeﬃcients
depend on the input. We formally show that our model captures popular architectures (e.g., Switch and
Scaling Transformers) by simulating the sparse layers in these models.
We next prove that the above class of sparsely activated models can learn a large class of functions more
eﬃciently than dense models. As our main proof technique, we introduce a new routing function, based
on locality sensitive hashing (LSH), that allows us to theoretically analyze sparsely activated layers in a
network. LSH maps points in a metric space (e.g., Rd) to ‘buckets’ such that nearby points map to same
bucket. The total number of buckets used by a LSH function is referred to as the size of the hash table. Prior
work has already identiﬁed many routing functions that work well in practice. We do not aim to compete
with these highly-optimized methods, but rather we aim to use LSH-based routing as a way to reason about
the approximation ability of sparse networks.
In Theorem 4.1, we show that LSH-based sparse models can approximate real-valued Lipschitz functions
in Rd. Although real data often lives in high dimensions, the underlying manifold where the inputs are
supported is often assumed to be low-dimensional (e.g. the manifold of natural images vs R224×224 for a
224 × 224 image). We model this by assuming that our inputs lie in a k-dimensional manifold within Rd. To
get ϵ approximation error, we need an LSH table of size approximately O((
√
dk/ϵ)k) but a forward pass only
requires time O(dk log(1/ϵ)) as only one of the O((
√
dk/ϵ)k) non-empty buckets are accessed for any given
example.
In Theorem 4.3, we complement our upper bounds by proving a lower bound of Ω((2
√
d/ϵ)k) on the size
needed for both dense and sparse models (when points live in k dimensions). This also transforms into a lower
bound on the number of samples required to learn these functions using a dense model. This lower bound
implies that a forward pass on a dense model takes time Ω(d(2/
√
kϵ)k), which is exponentially worse than
the time taken by the sparse model. Altogether, we show that for the general class of Lipschitz functions,
sparsely activated layers are as expressive as dense layers, while performing signiﬁcantly fewer ﬂoating point
operations (FLOPs) per example.
To support our theory, we perform experiments in Section 5 on approximating Lipschitz functions with
sparse networks. We identify several synthetic datasets where models with data-dependent sparse layers
outperform dense models of the same size. Moreover, we achieve these results with relatively small networks.
This provides a few minimum working examples of when sparsity is beneﬁcial. Our results complement
the existing work that has already shown the success of very large MoE models on real-world NLP and
vision tasks [14, 21, 33]. Our experiments also generalize the intuition from Figure 1a, where the model
outputs a diﬀerent constant value for each bucket. A puzzling empirical result was observed in [34] where a
pseudo-random uniform hash function is used to route input tokens to experts. Intuitively, this might map
very nearby or similar points to distinct experts, leading to a potentially non-smooth behavior. We show that
on our synthetic, Lipschitz datasets, the LSH model outperforms uniform random hashing, which further
corroborates our theory.
Related Work.
Sparsely activated networks have had enormous empirical success [1, 5, 13, 25, 29, 33, 37, 40].
The Switch Transformer [14] is one of the ﬁrst major, contemporary applications of the sparsely activated
layers. Follow-up works such as Scaling Transformers [21] and other hash functions [34] aim to improve
the sparse layers. These papers build upon seminal MoE works [19, 20, 22], and other uses of the MoE
paradigm [27, 36]. There has been work on using statistical mechanics to analyze the generalization of MoE
models [23]. However, to the best of our knowledge, there is no systematic theoretical study of modern
sparsely activated networks.
The above work on dynamic sparsity builds on previous static sparsity eﬀorts, e.g., weight quantization [28],
dropout [38], and pruning (see the survey [18] and references). Static sparsity means that the subnetwork
2

-1
-0.8
-0.6
-0.4
-0.2
0
0.2
0.4
0.6
0.8
1
x (input features)
0.0
0.3
0.6
0.9
1.2
1.5
1.8
2.1
2.4
2.7
3.0
y (function values) 
(a) 1D example
16
64
256
1024
4096
16384
Number of activated units (log scale)
0.00
0.05
0.10
0.15
0.20
0.25
Mean Squared Error
dense (width = x-axis values)
DSM (total width = 1024)
DSM (total width = 2048)
DSM (total width = 4096)
DSM (total width = 8192)
(b) sparse/dense scaling behavior
Figure 1: Learning polynomial functions with data-dependent sparse models: (a) Dashed curve is the target
function graph, piecewise constant curve is the learned LSH model output, and diﬀerent LSH buckets are
indicated by diﬀerent colors. Note neighboring points are hashed to the same bucket, where a simple constant
can be learned to approximate target function values; (b) Scaling behavior of data-dependent sparse models
(DSM) versus dense models, in which sparse models outperform dense models of the same size, see Section 2
and Section 5 for details.
activation does not change in a data-dependent way. The focus is on generalization and compression, instead
of achieving fast inference time with a huge number of parameters.
Our work builds on locality sensitive hashing (LSH), a well-studied technique for approximate nearest neighbor
search (see the survey [4] or the book [16] and references therein for LSH background). A seminal and popular
LSH family uses random hyperplanes to partition the space [3, 7]. Another option is to use balls centered
at lattice points, which works well for ℓ2 distance [2]. We use these ideas to design sparse networks. For
uses of LSH in deep learning, sketch-based memory improves network capacity [15, 30]. Other empirical
work uses LSH to improve dense network training time or memory [8, 9, 31]. Our work diﬀers from the prior
studies because we implement the LSH-based approach with sparsely activated networks, with the goal of
reducing inference time and achieving low regression error. LSH-based sparsity may be most relevant for
vision transformers [24, 33, 42], using distance metrics and routing functions tailored to images, or for time
series analysis [41].
2
Preliminaries
Let f : Rd →R be a multivariate real-valued function that we want to learn with a neural network. We
consider regression, and we aim to minimize the mean-squared error or the ℓ∞error. For a function f
deﬁned on a set Γ and an estimator ˆfn, we deﬁne ∥f −ˆfn∥∞= supx∈Γ |f(x) −ˆfn(x)|. For some results, we
approximate f on a subset V ⊆Rd. For example, V may be the intersection of [−1, 1]d and a k-dimensional
subspace. We also consider the case when f is L-Lipschitz, meaning that |f(x) −f(x′)| ≤L · ∥x −x′∥2 for all
x, x′ ∈Rd. We deﬁne [n] = {1, 2, . . . , n}.
2.1
Data-Dependent Sparse Model
A dense neural network g with a fully connected ﬁnal layer can be expressed as g(x) = A · φ(x) where
A ∈R1×t is a matrix and φ : Rd →Rt is a function (e.g., φ captures the representation learned up until the
ﬁnal layer). Here, t is the width of the ﬁnal layer and d is the input dimensionality.
Our focus is on the types of networks captured by what we call the Data-Dependent Sparse Model (DSM).
This is a network with a sparsely activated ﬁnal layer. Formally, let t be the width, and let s ≤t be a
sparsity parameter. Then, we consider functions g of the form g(x) = Ax · φ(x) where Ax ∈R1×t and
φ : Rd →Rt. The crux of the model is the ﬁnal layer. The sparsity comes from letting Ax = A ◦mask(x),
where mask(x) ∈{0, 1}1×t is an s-sparse indicator vector, and “◦” is the entry-wise product. The mask zeroes
out certain positions, and A contains the learned weights but no longer depends on x. Intuitively, the mask is
3

the “routing function” for the sparse activations. To do so, we deﬁne mask(x) ◦φ(x) to mean that we zero out
element of the vector φ(x). Then, we have
g(x) = (A ◦mask(x))φ(x) = A(mask(x) ◦φ(x)).
Under the above deﬁnitions, let DSM(d, s, t) be the set of functions g = (A ◦mask(x)) · φ(x). In what follows,
we use Ax as shorthand for A ◦mask(x).
In the DSM model, we wish to understand the eﬀect of sparsity on how well the network can approximate
certain functions. It is important to allow the ﬁnal layer Ax ∈R1×t to be data-dependent. The values in Ax
depend on the target function f, and we learn this from training data.
Real-world transformers may have multiple sparse layers. To capture this, we can compose DSM functions.
For example, two sparse layers comes from g(x) = A ◦mask2(x) ◦φ2(mask1(x) ◦φ1(x)). For simplicity, we
focus on a single sparse layer in what follows, which suﬃces for our main results.
2.2
Hash-based Routing
Prior sparse models compute hash functions of the input vector x to determine mask and the network
activations [34]. Our main theorem uses this strategy, considering LSH families. LSH has the property that
nearby points are more likely to end up in the same hash bucket than far away points. We can use LSH to
deﬁne a general class of eﬃcient regression functions. In Section 3, we prove that the DSM model captures
popular sparse architectures and the following LSH model.
LSH Model.
We review an LSH framework based on sign patterns.
Let h1, . . . , hm : Rd →{−1, 1}
be m distinct hash functions. Partition the space into 2m buckets based on the m sign patterns zx =
(h1(x), . . . , hm(x)) over all x ∈Rd. Then, for each z ∈{−1, 1}m, we can specify a function ˆgz(x), where the
goal is for gz to approximate the target function f in the part of space associated with z (i.e., points x that
have pattern z under h1, . . . , hm). More generally, we can allow s sets of such hash functions (hi
1, . . . , hi
m),
and s sets of these approximation functions (ˆg1
z1, . . . , ˆgs
zs) for i = 1, . . . , s. On input x, we compute the sign
patterns z1, . . . , zs and output g(x) = Ps
i=1 αiˆgi
zi(x). Further, we can restrict each ˆgi
z to be a degree ∆≥0
polynomial. For a ﬁxed LSH family of possible hash functions, we let LSH(d, s, m, ∆) denote this class of
functions. In many cases, we only need ˆgi
z to be a constant function, i.e., ∆= 0, and we shorten this as
LSH(d, s, m, 0) := LSH(d, s, m). Our goal is to understand the trade-oﬀs of the “memory” parameter m and
the “sparsity” amount s that we need to approximate large classes of d-dimensional functions (i.e., L-Lipschitz
functions supported on a k-dimensional manifold).
Euclidean LSH Model [11]. This is a popular LSH family for points in Rd. In this case, each hash function
outputs an integer (instead of ±1 above). Each bucket in this model is deﬁned by a set of hyperplane
inequalities. There are two parameters (D, ϵ). We sample D random directions a1, . . . , aD where each
coordinate of each ai is an independent normal variable. In addition we sample bi ∼Unif[0, ϵ] independently
for i ∈[D]. For a point x ∈Rd, we compute an index into a bucket via a function hi : Rd →Z deﬁned as
hi(x) =
a⊤
i x + bi
ϵ

.
Here, the index i ranges over i ∈[D], leading to a vector of D integers.
3
Simulating Models with DSM
We formally justify the DSM(d, s, t) model by simulating other models using it. We start with simple examples
(interpolation, k nearest neighbor (k-NN) regression, and Top-K), then move on to transformers and the LSH
model. For the simple examples, we only need one hidden layer, where φ(x) = σ(Bx) for a matrix B ∈Rt×d
and non-linearity σ.
Interpolation. We show how to compute f at t points x1, . . . , xt. We only need sparsity s = 1 and σ is
simply the identity function. We set Ai = f(xi)/⟨bi, xi⟩, where bi is the ith row of B. Further, we let mask(xi)
have a one in the ith position and zeroes elsewhere. Then, we deﬁne g(xi) = (A ◦mask(xi))Bxi = f(xi),
which computes f at the t input points.
Sparse networks perform k-NN regression. We sketch how the DSM(d, k, n) model can simulate k-NN
with g(x) = (A ◦mask(x))σ(Bx), when the dataset contains vectors with unit ℓ2 norm.Let the rows of B be
a set of n unit vectors b1, . . . , bn ∈Rd. For the target function f, let A = 1
k(f(b1), . . . , f(bn)). Deﬁne σ(Bx)
4

to have ones in the top k largest values in Bx and zeroes elsewhere. For a unit vector x, these k positions
correspond to the k largest inner products ⟨x, bi⟩. Since ∥x −bi∥2 = 2 −2⟨x, bi⟩, the non-zero positions in
σ(Bx) encode the k nearest neighbors of x in {b1, . . . , bn}. Thus, g(x) computes the average of f at these k
points, which is exactly k-NN regression. Moreover, only k entries of A are used for any input, since σ(Bx) is
k-sparse; however, computing σ(Bx) takes O(nd) time. While there is no computational advantage from the
sparsity in this case, the fact that DSM can simulate k-NN indicates the power of the model.
Simulating Top-K routing. For an integer K ∈N+, certain MoE models choose the K largest activations
to enforce sparsity; in practice, K ∈{1, 2} is common [33]. We use mask(x) with ∥mask(x)∥0 = K as the
indicator for the K largest values of Bx. Then, σ is the identity, and g(x) = (A ◦mask(x))Bx applies A to
the top K entries of Bx.
3.1
Simulating transformer models with DSM
Real-world networks have many hidden layers and multiple sparse layers. For concreteness, we describe how
to simulate a sparsely activated ﬁnal layer. As mentioned above, we can compose functions in the DSM model
to simulate multiple sparse layers.
Switch Transformers [14]. The sparse activations in transformers depend on a routing function R : Rd →
{1, . . . , L}, where R(x) speciﬁes the subnetwork that is activated (for work on the best choice of R, see
e.g., [34]). To put this under the DSM(d, s, t) model, consider a set of trainable matrices A1, . . . , AL ∈R1×s,
where the total width is t = s · L. On input x, we think of Ax as a 1 × t matrix with s non-zero entries equal
to AR(x). In other words, A is the concatenation of A1, . . . , AL, and mask(x) is non-zero on the positions
corresponding to AR(x).
Scaling Transformers [21]. The key diﬀerence between Switch and Scaling transformers is that the latter
imposes a block structure on the sparsity pattern. Let t be the width, and let s be the number of blocks (each
of size t′ = t/s). Scaling Transformers use only one activation in each of the s blocks. In the DSM(d, s, t)
model, we capture this with Ax as follows. Let ei ∈{0, 1}t′ denote the standard basis vector (i.e., one-hot
encoding of i ∈[t′]). The sparsity pattern is speciﬁed by indices (i1, . . . , is). Then, Ax = (α1ei1, . . . , αbeis)
for scalars α1, . . . , αs ∈R.
3.2
Simulating the LSH model using DSM
We explain how to represent the LSH model using the DSM model. The key aspect is that Ax depends
on the LSH buckets that contain x, where we have s non-zero weights for the s buckets that contain each
input. This resembles the Scaling Transformer, where we use the LSH buckets to deﬁne routing function.
In the LSH(d, s, m, ∆) model, there are s sets of m hash functions, leading to s · 2m hash buckets. We
use width t = s · 2m for the DSM(d, s, t) network. The entries of Ax are in one-to-one mapping with the
buckets, where only s entries will be non-zero depending on the s buckets that x hashes to, that is, the values
(hi
1(x), . . . , hi
m(x)) ∈{−1, 1}m for i = 1, 2, . . . , s.
We now determine the values of these s non-zero entries. We store a degree ∆polynomial ˆg(x) : Rd →R
associated with each bucket. For our upper bounds, we only need degree ∆= 0, but we mention the general
case for completeness. If ∆= 0, then ˆg is simply a constant α depending on the bucket. An input x hashes
to s buckets, associated with s scalars (α1, . . . , αs). To form Ax, set s entries to the αi values, with positions
corresponding to the buckets. For degree ∆≥1, we store coeﬃcients of the polynomials ˆg, leading to more
parameters. Section 4 describes how to use LSH-based sparse networks to approximate Lipschitz functions.
Computing and storing the LSH buckets. Determining the non-zero positions in Ax only requires
O(sm) hash computations, each taking O(d) time with standard LSH families (e.g., hyperplane LSH). We
often take m to be a large constant. Thus, the total number of operations to compute a forward pass in the
network O(smd) ≈O(sd). The variable m above determines the total number of distinct buckets we will
have (2m). For an n point dataset, m = O(log n) is a realistic setting in theory. Therefore, 2m = poly(n)
is often a reasonable size for the hash table and moreover the dense model requires width O(2m) as well.
In practice, the number of experts is usually 32–128 [14, 33]. The hash function typically adds very few
parameters. For example, to hash d dimensional data into 2m buckets, LSH requires O(md) bits and takes
O(md) time as well. In summary, the LSH computation does not asymptotically increase the number of
FLOPs for a forward pass in the network. We also later bound the number of non-empty LSH buckets. The
memory to store the hash table is comparable to the number of samples required to learn the target function.
5

4
Data-Dependent Sparse Models are more Eﬃcient than Dense Models
For a general class of Lipschitz functions, LSH-based learners yield similar ℓ∞error as dense neural networks
while making inference signiﬁcantly more eﬃcient. It is a common belief in the machine learning community
that although many of the datasets we encounter can appear to live in high-dimensional spaces, there is a
low-dimensional manifold that contains the inputs. To model this, we assume in our theory that the inputs lie
in a k-dimensional subspace (a linear manifold) of Rd. Here k ≪d. Theorem 4.1 shows that the LSH model
we propose can learn high-dimensional Lipschitz functions with a low ℓ∞error eﬃciently when the input
comes from a uniform distribution on an unknown low-dimensional subspace. Theorem B.1 extends this result
to when the input comes from an unknown manifold with a bounded curvature. We present Theorem 4.1
here and defer Theorem B.1 to Section B. All proofs are in the appendix.
Theorem 4.1. For any f : [−1, 1]d →R that is L-Lipschitz, and for an input distribution D that is
uniform on a k-dimensional subspace in [−1, 1]d, an LSH-based learner can learn f to ϵ-uniform error with
O(kL
√
d
k log(L
√
d/ϵ)/ϵk) samples using a hash table of size O(L
√
d
k/ϵk) with probability ≥0.8. The total
time required for a forward pass on a new test sample is O(dk log(L
√
d/ϵ)).
The key idea behind this theorem is to use LSH to produce a good routing function. The locality of the
points hashed to an LSH bucket gives us a way to control the approximation error. By using a large number
of buckets, we can ensure their volume is small. Then, outputting a representative value suﬃces to locally
approximate the target Lipschitz function (since its value changes slowly). The next lemma bounds the size
of the sub-regions corresponding to each bucket, as well as bounding the number of non-empty buckets.
Lemma 4.2. Consider a Euclidean LSH model in d dimensions with Ck hyperplanes and width parameter
ϵ where C is a large enough constant. Consider a region Γ deﬁned by the intersection of a k-dimensional
subspace with [−1, 1]d. We have that the LSH model deﬁnes a partitioning of Γ into buckets. Let c be a
constant. Then, with probability ≥0.9,
1. Projecting any bucket of the LSH onto Γ corresponds to a sub-region with diameter ≤ϵ/c.
2. At most

2
√
d
ϵ
O(k)
buckets have a non-empty intersection with Γ.
The above construction assumes knowledge of the dimensionality of the input subspace k. Fortunately, any
upper bound on k would also suﬃce. The table size of the LSH model scales exponentially in k but not d.
Thus, an LSH-based learner adapts to the dimensionality of the input subspace.
Theorem 4.1 shows that sparsely activated models (using LSH of a certain table size) are powerful enough to
approximate and learn Lipschitz functions. Next, we show a complementary nearly matching lower bound
on the width required by dense model to approximate the same class of functions. We use an information
theoretic argument.
Theorem 4.3. Consider the problem of learning L-Lipschitz functions on [−1, 1]d to ℓ∞error ϵ when the
inputs to the function are sampled from a uniform distribution over an unknown k-dimensional subspace of
Rd ∩[−1, 1]d. A dense model of width t with a random bottom layer requires
t = Ω
 
(
√
dL)k
(Cϵ)k
!
,
for a constant C > 0 to learn the above class of functions. The number of samples required is
Ω
 
k(
√
dL)k log(2
√
dL/Cϵ)
(Cϵ)k
!
.
Our approach for this theorem is to use a counting argument. We ﬁrst bound the number of distinct functions,
which is exponential in the number of parameters (measured in bits). We then construct a large family of
target functions that are pairwise far apart from each other. Hence, if we learn the wrong function, we incur
a large error. Our function class must be large enough to represent any possible target function, and this
gives a lower bound on the size of the approximating network.
6

Theorem 4.3 shows a large gap in the time complexity of inference using a dense model and an LSH model.
The inference times taken by a dense model vs. a sparse model diﬀer exponentially in 1/ϵ.
Sparse: O (dk log(1/ϵ))
vs.
Dense: Ω

d
 
2
√
d
Cϵ
!k

(1)
Overall, the above theorems show that LSH-based sparsely activated networks can approximate Lipschitz
functions on a k-dimensional subspace. The size and sample complexity match between sparse and dense
models, but the sparse models are exponentially more eﬃcient for inference.
5
Experiments
To empirically verify our theoretical ﬁndings, we align our experiments with our proposed models and compare
dense models, data-dependent sparse models (DSM), LSH models, and sparse models with random hash
based sparse layers. While DSM and LSH models are analyzed in the previous section, random hash based
layers introduce sparsity with a random hash function, as an alternative of learnable routing modules and
LSH in MoE models [34]. Our goal is to show that the DSM and LSH models achieve small MSE while using
much fewer activated units than dense models. We also report similar observations on the CIFAR-10 dataset.
5.1
Experimental set-up
Models. We compare dense models and three sparse models that cover a wide range of possible routing
methods (Top-K DSM, LSH, and random). We use Top-K routing as a representative DSM model; it uses
the K largest activations, which depends on the input (see Section 3). Dense, DSM and random hash sparse
models contain a random-initialized, non-trainable bottom layer. For the random hash model, we uniformly
choose a subset of the units. All networks have a trainable top layer. LSH models have non-trainable
hyperplane coeﬃcients for hashing and a trainable scalar in each bucket (this is the network output for inputs
in that bucket). For ablations, we vary the number of hidden units and sparsity levels.
Data. Our main goal is to empirically evaluate the construction from Theorem 4.1, while comparing routing
functions. To align with our theory, we consider Lipschitz target functions. We synthetically construct the
data with two functions that are common bases for continuous functions: random low-degree polynomials and
random hypercube functions. Both are normalized to be L-Lipscthiz for L = O(1). We also present results
on a real dataset, CIFAR-10, which corroborates our ﬁndings from the synthetic data experiments.
Random polynomial. p(x) of degree D for x ∈Rd with sum of coeﬃcient absolute values < 1/D. As a
result, we ensure that the function is 1-Lipschitz over [−1, 1]d.
Random hypercube function. f : [−1, 1]d →R which interpolates the indicator functions at each corner
with random {−1, 1} value at each corner. Concretely, the function is deﬁned as follows: for each corner
point y ∈{−1, 1}d, its indicator function is Iy(x) = Qd
i=1
1+yixi
2
. Sample random values vy ∈{−1, 1} with
probability (0, 5, 0.5) independently for each y ∈{−1, 1}d, the random hypercube polynomial function is
f(x) = P
y∈{−1,1}d vyIy(x).
For the random polynomial (random hypercube) functions, we use d = 8 and D = 4 in the this section and
provide more results for other parameter settings in the appendix (for example, see Section D.2 for when
when the target polynomial functions concentrate on a low-dimensional subspace).
5.2
Results
MSE for random functions. Figures 2 and 3 show the scaling behavior of the DSM and LSH models for
a random polynomial function and hypercube function (lower is better). Sparsity helps for the DSM and
LSH models, both achieving better quality than dense models using the same number of activated units.
In Figure 4, we compare the DSM/LSH models with the random hash sparse models, and we see random
hash sparse models underperform dense models, suggesting data-dependent sparsity mechanisms (such as
DSM/LSH) are eﬀective ways to utilize sparsity.
FLOPs. To further qualify the eﬃciency gain of sparse models, we compare the MSE at the same FLOPs
for sparse/dense models in Table 1. The ﬁrst column is the # FLOPs for the dense model; models in the 3rd
7

and 4th columns use same # FLOPs but have more parameters (only 50% or 25% active). DSM uses only
18k FLOPs and gets smaller MSE than dense model with 73k FLOPs.
FLOPs
eval MSE (dense)
eval MSE (DSM 50% sparsity)
eval MSE (DSM 25% sparsity)
18432
0.01015
0.01014
0.009655
36864
0.01009
0.007438
0.005054
73728
0.01046
0.006115
0.001799
Table 1: FLOPs and evaluation Mean Squared Error (eval MSE).
16
64
256
1024
4096
16384
Number of activated units (log scale)
0.00
0.05
0.10
0.15
0.20
0.25
Mean Squared Error
dense (width = x-axis values)
DSM (total width = 1024)
DSM (total width = 2048)
DSM (total width = 4096)
DSM (total width = 8192)
16
64
256
1024
4096
16384
Number of activated units (log scale)
0.00
0.05
0.10
0.15
0.20
0.25
Mean Squared Error
dense (width = x-axis values)
LSH (# buckets = 1024)
LSH (# buckets = 2048)
LSH (# buckets = 4096)
LSH (# buckets = 8192)
Figure 2: Scaling behavior of DSM and LSH models compared with dense models for a degree 4 random
polynomial with input dimension 8: (a) DSM outperforms dense model at the same number of activated
units and quality improves as total width increases; (b) LSH model outperforms dense model when number
of buckets is large (≥2048) and quality improves as number of buckets increase.
16
64
256
1024
4096
16384
Number of activated units (log scale)
0.00
0.01
0.02
0.03
0.04
Mean Squared Error
dense (width = x-axis values)
DSM (total width = 1024)
DSM (total width = 2048)
DSM (total width = 4096)
DSM (total width = 8192)
16
64
256
1024
4096
16384
Number of activated units (log scale)
0.00
0.01
0.02
0.03
0.04
Mean Squared Error
dense (width = x-axis values)
LSH (# buckets = 1024)
LSH (# buckets = 2048)
LSH (# buckets = 4096)
LSH (# buckets = 8192)
Figure 3: Scaling behavior of DSM and LSH models compared with dense models for a random hypercube
function with input dimension 8. Both DSM and LSH models outperform corresponding dense models with
the same number of activated units (note: # activ. units ≤width).
CIFAR-10. We also compare the scaling behavior of DSM and dense models on CIFAR-10 [26]. The
baseline model is a CNN with 3 convolutional layers (followed by max-pooling), a dense layer with varying
number of units, and a ﬁnal dense layer that computes the logits (referred as CNN + dense). For the
data-dependent sparse models, we use the same architecture, except we change the penultimate dense layer
with a data-dependent sparse layer that choose the Top-K activations (referred as CNN + DSM). Both
models are trained with ADAM optimizer for 50 epochs and evaluated on the test dataset for model accuracy
with no data augmentation; see Figure 5 and Table 2 for the accuracy versus number of activated units. As
with the synthetic datasets, DSMs outperform dense models at the same number of activated units.
8

16
64
256
1024
4096
16384
Number of activated units (log scale)
0.00
0.05
0.10
0.15
0.20
0.25
0.30
0.35
Mean Squared Error
dense (width = x-axis values)
random hash (total width = 2048)
DSM (total width = 2048)
LSH (# buckets = 2048)
(a) random polynomial
16
64
256
1024
4096
16384
Number of activated units (log scale)
0.00
0.01
0.02
0.03
0.04
Mean Squared Error
dense (width = x-axis values)
random hash (total width = 2048)
DSM (total width = 2048)
LSH (# buckets = 2048)
(b) random hypercube function
Figure 4: Scaling behavior of dense, random hash, DSM, and LSH models. DSM and LSH models outperform
dense models, while random hash models underperform dense models with the same number of activated
units.
5.3
Discussion
Our experimental results (e.g., Figures 2, 3, and 4) show that sparsely activated models can eﬃciently
approximate both random polynomials and random hypercube functions. Intuitively, the DSM and LSH
models employ the sparsity as a way to partition the space into nearby input points. Then, because the
target function is Lipschitz, it is easy to provide to local approximation tailored to the speciﬁc sub-region of
input space. On the other hand, the uniform random hash function performs poorly for these tasks precisely
because it does not capture the local smoothness of the target function.
On CIFAR-10, we also see that the DSM model performs comparably or better than the dense network. In
particular, Figure 5 shows that the “CNN + DSM” approach with total width 1024 improves upon or matches
the dense model. In this case, the sparse activation allows the network to classify well while using only a
fraction of the number of FLOPs. In Table 2, we see that DSM model outperforms the dense model when we
control for the number of activated units in the comparison.
Limitations. Our experiments focus on small datasets and 2-layer networks as a way to align with our
theoretical results. Prior work on sparsely activated networks has shown success for large-scale NLP and
vision tasks. Our experiments complement previous results and justify the DSM and LSH models by showing
their ability to approximate Lipschitz functions (consistent with our theorems). It would be good to further
evaluate the DSM and LSH models for large-scale tasks, for example, by using them as inspiration for routing
functions in MoE vision transformers, such as V-MoE [33]. It would be interesting to evaluate on larger
datasets, such as ImageNet as well. We experimented with a handful of hyperparameter settings, but a
full search may lead to diﬀerent relative behavior between the models (similarly, we only evaluated a few
parameter settings for the random functions and the synthetic data generation, which is far from exhaustive).
6
Conclusion
We provided the ﬁrst systematic theoretical treatment of modern sparsely activated networks. To do so,
we introduced the DSM model, which captures the sparsity in Mixture of Experts models, such as Switch
and Scaling Transformers. We showed that DSM can simulate popular architectures as well as LSH-based
networks. Then, we exhibited new constructions of sparse networks. Our use of LSH to build these networks
oﬀers a theoretical grounding for sparse networks. We complemented our theory with experiments, showing
that sparse networks can approximate various functions.
For future work, it would be interesting to implement LSH-based networks in transformers for language/vision
tasks. A related question is to determine the best way to interpolate in each LSH bucket (e.g., a higher
degree polynomial may work better). Another question is whether a dense model is more powerful than a
sparse model with the same number of total trainable parameters. Theorem 4.1 only says that a sparse model
with similar number of parameters as a dense model can more eﬃciently (fewer FLOPs) represent Lipschitz
functions. This does not say all functions expressible by a dense model are also expressible by a sparse model.
9

32
64
128
256
512
1024
Number of activated units (log scale)
65
66
67
68
69
70
71
72
Test accuracy
CNN + dense (width = x-axis values)
CNN + DSM (total width = 256)
CNN + DSM (total width = 512)
CNN + DSM (total width = 1024)
Figure 5: Scaling behavior of DSM compared with
dense models on the CIFAR-10 dataset. Similar to
Figure 4a, DSM outperforms dense models at the same
number of activated units.
Model \ # activated units
256
512
Dense
69.79
70.79
DSM (50% sparse)
70.74
71.33
DSM (25% sparse)
69.8
71.68
Table 2: CIFAR-10 test accuracy for dense/DSM
models with the same number of activated units.
While not strictly monotonic, wider and sparser
models outperform narrow and dense ones.
This is non-trivial question as Ax depends on the input (i.e., DSM(d, t, t) may be more expressive than the
dense model with width t). We expect that dense networks can be trained to perform at least as well as
sparse networks, assuming the width is large enough. The dense networks should optimize the weights in the
last layer to approximate the function, but they may not favor certain neurons depending on the input.
References
[1] James Urquhart Allingham, Florian Wenzel, Zelda E Mariet, Basil Mustafa, Joan Puigcerver, Neil
Houlsby, Ghassen Jerfel, Vincent Fortuin, Balaji Lakshminarayanan, and Jasper Snoek. Sparse MOEs
meet eﬃcient ensembles. arXiv preprint arXiv:2110.03360, 2021.
[2] Alexandr Andoni and Piotr Indyk. Near-optimal hashing algorithms for approximate nearest neighbor in
high dimensions. In 2006 47th annual IEEE symposium on foundations of computer science (FOCS’06),
pages 459–468. IEEE, 2006.
[3] Alexandr Andoni, Piotr Indyk, Thijs Laarhoven, Ilya Razenshteyn, and Ludwig Schmidt. Practical
and optimal lsh for angular distance. In Proceedings of the 28th International Conference on Neural
Information Processing Systems-Volume 1, pages 1225–1233, 2015.
[4] Alexandr Andoni, Piotr Indyk, and Ilya Razenshteyn. Approximate nearest neighbor search in high
dimensions. In Proceedings of the International Congress of Mathematicians: Rio de Janeiro 2018, pages
3287–3318. World Scientiﬁc, 2018.
[5] Mikel Artetxe, Shruti Bhosale, Naman Goyal, Todor Mihaylov, Myle Ott, Sam Shleifer, Xi Victoria Lin,
Jingfei Du, Srinivasan Iyer, Ramakanth Pasunuru, et al. Eﬃcient large scale language modeling with
mixtures of experts. arXiv preprint arXiv:2112.10684, 2021.
[6] Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.
arXiv preprint arXiv:2005.14165, 2020.
[7] Moses S Charikar. Similarity estimation techniques from rounding algorithms. In Proceedings of the
thiry-fourth annual ACM symposium on Theory of computing, pages 380–388, 2002.
[8] Beidi Chen, Zichang Liu, Binghui Peng, Zhaozhuo Xu, Jonathan Lingjie Li, Tri Dao, Zhao Song,
Anshumali Shrivastava, and Christopher Re. Mongoose: A learnable lsh framework for eﬃcient neural
network training. In International Conference on Learning Representations, 2020.
[9] Wenlin Chen, James Wilson, Stephen Tyree, Kilian Weinberger, and Yixin Chen. Compressing neural
networks with the hashing trick. In International conference on machine learning, pages 2285–2294.
PMLR, 2015.
[10] Zizhong Chen and Jack J Dongarra. Condition numbers of gaussian random matrices. SIAM Journal on
Matrix Analysis and Applications, 27(3):603–620, 2005.
[11] Mayur Datar, Nicole Immorlica, Piotr Indyk, and Vahab S Mirrokni. Locality-sensitive hashing scheme
based on p-stable distributions. In Proceedings of the twentieth annual symposium on Computational
geometry, pages 253–262, 2004.
10

[12] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep
bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.
[13] Nan Du, Yanping Huang, Andrew M Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun,
Yanqi Zhou, Adams Wei Yu, Orhan Firat, et al. Glam: Eﬃcient scaling of language models with
mixture-of-experts. arXiv preprint arXiv:2112.06905, 2021.
[14] William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter
models with simple and eﬃcient sparsity. arXiv preprint arXiv:2101.03961, 2021.
[15] Badih Ghazi, Rina Panigrahy, and Joshua Wang. Recursive sketches for modular deep learning. In
International Conference on Machine Learning, pages 2211–2220. PMLR, 2019.
[16] Sariel Har-Peled. Geometric approximation algorithms. Number 173. American Mathematical Soc., 2011.
[17] Geoﬀrey Hinton.
Lecture Notes, Toronto, Hinton, 2012, http://www.cs.toronto.edu/~tijmen/
csc321/slides/lecture_slides_lec6.pdf.
[18] Torsten Hoeﬂer, Dan Alistarh, Tal Ben-Nun, Nikoli Dryden, and Alexandra Peste. Sparsity in deep
learning: Pruning and growth for eﬃcient inference and training in neural networks. arXiv preprint
arXiv:2102.00554, 2021.
[19] Robert A Jacobs. Methods for combining experts’ probability assessments. Neural computation, 7(5):867–
888, 1995.
[20] Robert A Jacobs, Michael I Jordan, Steven J Nowlan, and Geoﬀrey E Hinton. Adaptive mixtures of
local experts. Neural computation, 3(1):79–87, 1991.
[21] Sebastian Jaszczur, Aakanksha Chowdhery, Afroz Mohiuddin, Łukasz Kaiser, Wojciech Gajewski, Henryk
Michalewski, and Jonni Kanerva.
Sparse is enough in scaling transformers.
Advances in Neural
Information Processing Systems, 34, 2021.
[22] Michael I Jordan and Robert A Jacobs. Hierarchical mixtures of experts and the EM algorithm. Neural
computation, 6(2):181–214, 1994.
[23] Kukjin Kang and Jong-Hoon Oh. Statistical mechanics of the mixture of experts. Advances in neural
information processing systems, 9, 1996.
[24] Salman Khan, Muzammal Naseer, Munawar Hayat, Syed Waqas Zamir, Fahad Shahbaz Khan, and
Mubarak Shah. Transformers in vision: A survey. arXiv preprint arXiv:2101.01169, 2021.
[25] Young Jin Kim and Hany Hassan Awadalla. Fastformers: Highly eﬃcient transformer models for natural
language understanding. arXiv preprint arXiv:2010.13382, 2020.
[26] Alex Krizhevsky, Geoﬀrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.
[27] Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim
Krikun, Noam Shazeer, and Zhifeng Chen. Gshard: Scaling giant models with conditional computation
and automatic sharding. In 9th International Conference on Learning Representations, ICLR 2021,
Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021.
[28] Zhuohan Li, Eric Wallace, Sheng Shen, Kevin Lin, Kurt Keutzer, Dan Klein, and Joseph E Gonzalez.
Train large, then compress: Rethinking model size for eﬃcient training and inference of transformers.
arXiv preprint arXiv:2002.11794, 2020.
[29] Xiaonan Nie, Shijie Cao, Xupeng Miao, Lingxiao Ma, Jilong Xue, Youshan Miao, Zichao Yang, Zhi Yang,
and Bin Cui. Dense-to-sparse gate for mixture-of-experts. arXiv preprint arXiv:2112.14397, 2021.
[30] Rina Panigrahy, Xin Wang, and Manzil Zaheer. Sketch based memory for neural networks. In International
Conference on Artiﬁcial Intelligence and Statistics, pages 3169–3177. PMLR, 2021.
[31] Jack W Rae, Jonathan J Hunt, Tim Harley, Ivo Danihelka, Andrew Senior, Greg Wayne, Alex Graves,
and Timothy P Lillicrap. Scaling memory-augmented neural networks with sparse reads and writes.
arXiv preprint arXiv:1610.09027, 2016.
[32] Colin Raﬀel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi
Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a uniﬁed text-to-text
transformer. arXiv preprint arXiv:1910.10683, 2019.
[33] Carlos Riquelme, Joan Puigcerver, Basil Mustafa, Maxim Neumann, Rodolphe Jenatton, André Su-
sano Pinto, Daniel Keysers, and Neil Houlsby. Scaling vision with sparse mixture of experts. Advances
in Neural Information Processing Systems, 34, 2021.
11

[34] Stephen Roller, Sainbayar Sukhbaatar, Arthur Szlam, and Jason Weston. Hash layers for large sparse
models. arXiv preprint arXiv:2106.04426, 2021.
[35] Mark Rudelson and Roman Vershynin. Smallest singular value of a random rectangular matrix. Commu-
nications on Pure and Applied Mathematics: A Journal Issued by the Courant Institute of Mathematical
Sciences, 62(12):1707–1739, 2009.
[36] Noam Shazeer, Youlong Cheng, Niki Parmar, Dustin Tran, Ashish Vaswani, Penporn Koanantakool,
Peter Hawkins, HyoukJoong Lee, Mingsheng Hong, CliﬀYoung, Ryan Sepassi, and Blake Hechtman.
Mesh-tensorﬂow: Deep learning for supercomputers. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman,
N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 31.
Curran Associates, Inc., 2018.
[37] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc V. Le, Geoﬀrey E. Hinton, and
JeﬀDean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. In ICLR
(Poster). OpenReview.net, 2017.
[38] Nitish Srivastava, Geoﬀrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout:
a simple way to prevent neural networks from overﬁtting. The journal of machine learning research,
15(1):1929–1958, 2014.
[39] Gregory Valiant and Paul Valiant. Estimating the unseen: an n/log (n)-sample estimator for entropy
and support size, shown optimal via new clts. In Proceedings of the forty-third annual ACM symposium
on Theory of computing, pages 685–694, 2011.
[40] Shuohuan Wang, Yu Sun, Yang Xiang, Zhihua Wu, Siyu Ding, Weibao Gong, Shikun Feng, Junyuan
Shang, Yanbin Zhao, Chao Pang, et al. Ernie 3.0 titan: Exploring larger-scale knowledge enhanced
pre-training for language understanding and generation. arXiv preprint arXiv:2112.12731, 2021.
[41] Qingsong Wen, Tian Zhou, Chaoli Zhang, Weiqi Chen, Ziqing Ma, Junchi Yan, and Liang Sun. Trans-
formers in time series: A survey. arXiv preprint arXiv:2202.07125, 2022.
[42] Bichen Wu, Chenfeng Xu, Xiaoliang Dai, Alvin Wan, Peizhao Zhang, Zhicheng Yan, Masayoshi Tomizuka,
Joseph Gonzalez, Kurt Keutzer, and Peter Vajda. Visual transformers: Token-based image representation
and processing for computer vision. arXiv preprint arXiv:2006.03677, 2020.
12

A
Proofs of Main Upper and Lower Bounds
A.1
Proof of the Lipschitz Upper Bound
Proof of Theorem 4.1. We use the Euclidean-LSH construction of Lemma 4.2 with parameter ϵ/L. In any
sub-region of the k-dimensional subspace that has a small diameter, the Lipschitz nature of the function
together with Lemma 4.2 will imply that we can approximate it by just a constant and incur only ϵ error
in ℓ∞. In particular, given a point x1 belonging to an LSH bucket, we can set ˆf(x) = f(x1) everywhere in
that bucket. For any x2 also mapping to the same bucket, from Lemma 4.2, we have that ∥x1 −x2∥2 ≤ϵ/L.
Since f is L-Lipschitz,
| ˆf(x2) −f(x2)| = |f(x1) −f(x2)| ≤L∥x1 −x2∥2 ≤ϵ.
(2)
Next we look at how many samples we need to obtain the guarantee ∥ˆf −f∥∞≤ϵ. A rare scenario that
we have to deal with for the sake of completeness is when there exist buckets of such small volume that no
training data point has mapped to them and consequently we don’t learn any values in those buckets. At test
time, if we encounter this rare scenario of mapping to a bucket with no value learnt in it, we simply run an
approximate nearest neighbor search among the train points. For our prediction, we use the bucket value
associated with the bucket that the approximate nearest neighbor maps to. To control the error when doing
such a procedure, we take enough samples to approximately form an ϵ/ΓL cover of Γ for a large enough
constant Γ. The size of an ϵ/ΓL cover of Γ is O((2ΓL
√
d/ϵ)k). This implies that, via a coupon collector
argument, when the input distribution is uniform over the region Γ, O(k(2ΓL
√
d/ϵ)k log(2ΓL
√
d/ϵ)) samples
will ensure that with very high probability, for every test point x there exists a train example xi such that
∥x −xi∥2 ≤2ϵ/ΓL. The test error is |f(x) −ˆf(xi)| ≤|f(x) −f(xi)| + |f(xi) −ˆf(xi)| = O(ϵ). Computing
the exact nearest neighbor is a slow process. Instead we can compute the approximate nearest neighbor using
LSH very quickly. We lose another O(ϵ) error due to this approximation. Choosing Γ appropriately we can
make the ﬁnal error bound exactly ϵ. This leads to our stated sample complexity bound.
This implies that ∥f −ˆf∥∞≤ϵ. Hence using an Euclidean LSH with O(k) hyperplanes we can learn an
ϵ-approximation to f. The time to compute ˆf(x) for a new example is the time required to compute the
bucket id where it maps to. Since there are k hyperplanes and our input is d-dimensional, computing the
projections of x on the k hyperplanes takes O(dk) time. Then we need to perform a division by the width
parameter ϵ/L. This takes time equal to the number of bits required to represent L/ϵ. Hence the total time
taken would be O(dk log(L/ϵ)).
The above theorem uses a lemma about Euclidean LSH, which we prove next.
Proof of Lemma 4.2. Let K = Ck. Let the random hyperplanes chosen by the Euclidean LSH be a1, . . . , aK.
Let the width parameter used by the LSH be ϵLSH. The value of ϵLSH we choose will be determined later.
Since the distribution of entries is spherically symmetric, the projection of the vectors onto the k-dimensional
subspace will also form a Euclidean-LSH model. Henceforth in our analysis we can assume that all our inputs
are projected onto the k-dimensional space Γ and that we are performing LSH in a k-dimensional space instead
of a d-dimensional one. Let A = [a1, . . . , aK]⊤be the matrix whose columns are the vectors perpendicular to
the hyperplanes chosen by the LSH. Note that A ∈RK×k. Then we have, from tail properties of the smallest
singular value distribution of Gaussian random matrices (e.g. see [10, 35]), for a large enough constant c,
Pr[σmin(A) ≥c
√
k] ≥9/10.
(3)
For two points x1, x2 ∈Γ to map to the same LSH bucket, ∥A(x1 −x2)∥∞≤ϵLSH. This implies that
∥A(x1 −x2)∥2 ≤ϵLSH
√
k. Together with (3), this implies that ∥x1 −x2∥2 ≤ϵLSH/c with probability ≥9/10.
At the same time, since we x1, x2 ∈[−1, 1]d, the maximum distance along any direction is at most the length
of any diagonal, which is 2
√
d. Moreover, along any hyperplane direction sampled by the LSH, we grid using
a width ϵLSH. Since the total number of hyperplanes is Ck the maximum number of LSH buckets possible is

2
√
d
ϵLSH
O(k)
. We set ϵLSH = cϵ. Then, the diameter of any bucket ≤ϵLSH/c = ϵ. The upper bound on the
maximum number of LSH buckets required to cover the region Γ also follows.
13

A.2
Proof of the Dense Lower Bound
Proof of Theorem 4.3. Assuming B bits per parameter, in our dense layer model we have 2Bt distinct possible
conﬁgurations. We lower bound the width t by constructing a class of functions F deﬁned on a k-dimensional
subspace within [−1, 1]d such that three properties simultaneously hold:
1. each f ∈F is L-Lipschitz,
2. the number of functions in F is at least Ω(2(2
√
dL/Cϵ)k)
3. for f1 ̸= f2 ∈F, we have ∥f1 −f2∥∞> ϵ.
These three properties together will imply that t ≥1
B ˙(2
√
d/Cϵ)k as otherwise by there would have to be two
functions f1 ̸= f2 ∈F that are approximated simultaneously by the same dense network, which is impossible
since ∥f1 −f2∥∞> ϵ. We construct F as follows. Given the d-dimensional cube [−1, 1]d, we pick a subset
of k diagonals of the cube such that they are linearly independent. We consider the k-dimensional region
deﬁned by the intersection of the subspace generated by these diagonals and the cube [−1, 1]d. Denote the
region we obtain by G. Let e1, . . . , ek form an orthonormal basis for the subspace G lies in. We grid G into
k-dimensional cubes of side length 2ϵ/L aligned along its bases {ei}k
i=1. For the center of every cube we
pick a random assignment from {+ϵ, −ϵ}. Then we interpolate the function everywhere in G such that (i) it
satisﬁes the assigned values at the centers of the cubes and (ii) its value decreases linearly to 0 with radial
distance from the center. That is, given the set of cube centers V
f(x) =
X
v∈V
max(0, f(v) −L sgn(f(v))∥x −v∥2)
To understand the Lipschitz properties of such an interpolation, note that the slope at any given point in G
is either 0 or L, which bounds the Lipschitz constant by L. The total number of cubes that lie within G is
at least (
√
dL/Cϵ)k for some constant C and hence F contains a total of (2)(
√
dL/Cϵ)k functions. Moreover,
given any f1, f2 ∈F such that f1 ̸= f2, there exists a cube center where their values diﬀer by 2ϵ giving us the
third desired property as well. Consequently, we get that to attain ϵ-uniform error successfully on F we need
2Bt ≥2(
√
dL/(Cϵ))k,
which implies that t = Ω((
√
dL)k/(Cϵ)k).
B
LSH Models Can Also Learn Lipschitz Functions on k-Manifolds
A k-dimensional manifold (referred to as a k-manifold) can loosely be thought of as a k-dimensional surface
living in a higher dimensional space. For example the surface of a sphere in 3-dimensions is a 2-dimensional
manifold. We consider k-manifolds in Rd that are homeomorphic to a k-dimensional subspace in Rd. We
assume that our k-dimensional manifold Mk is given by a transform f : Rk →Rk applied on k-dimensional
subspace of Rd Lk. To control the amount of distortion that can occur when going from Lk to Mk, the
Jacobian of f is assumed to have a constant condition number for all x ∈Lk. We now state our main upper
bound for manifolds, showing that LSH models can adapt and perform well even with non-linear manifolds of
a bounded distortion from a linear subspace.
Theorem B.1. For any f : [−1, 1]d →R that is 1-Lipschitz, and for an input distribution D that is uniform
on a k-manifold in [−1, 1]d, an LSH model can learn f to ϵ-uniform error with O(k
√
dk
k log(
√
dk/ϵ)/ϵk)
samples using a hash table of size O(
√
dk
k/ϵk) with probability ≥0.8. The total time required for a forward
pass on a new test sample is O(dk log(1/ϵ)).
Proof. The main idea of the proof is to follow similar arguments from Theorem 4.1 on the subspace Lk and
try to bound the amount of distortion the arguments face when mapped to the manifold Mk. Since we are no
longer dealing with a subspace (linear manifold), the argument that an LSH in d-dimensions can be viewed
as an equivalent LSH in k-dimensions does not hold. We use Euclidean-LSH models with O(d) hyperplanes.
Furthermore, we will use multiple LSH models each deﬁned using O(d) hyperplanes. The main challenge in
the proof is to show that the total number of buckets used in approximating f do not grow exponentially in
d, which is a possibility now as we use O(d) hyperplanes.
14

Lemma B.2. For any x ∈Rd, a d-dimensional sphere of radius O(ϵ/d) centered at x is fully contained in
the bucket where the center of the sphere maps to with probability ≥0.9.
Proof. Along any hyperplane direction the gap between parallel hyperplanes is ϵ. Since any point is randomly
shifted before being mapped to a bucket we get that with probability 1 −O(1/d), x is more than Ω(1/d)
away from each of the two parallel hyperplanes on either side. So with probability (1 −O(1/d))O(d) = Ω(1)
the entire sphere is contained inside the LSH bucket x maps to.
Lemma B.3. Using O(k log d) Euclidean-LSH functions, we get that every x ∈Lk, there exists a bucket in
at least one of the O(k log d) buckets x gets mapped to such that the entire k-dimensional sphere of radius
O(ϵ/d) centered at x is contained within the bucket.
Proof. We use a covering number argument. The maximum volume of a k-dimensional subspace within
[−1, 1]d is (2
√
d)k. We cover this entire volume using spheres of radius ϵ/d. The total number of spheres
required to do this are O((2d
√
d)k/ϵk). We now do a union bound over all the sphere centers in our cover
above. For a single sphere, the probability that it does not go intact into a bucket in any of the O(k log d)
LSH functions is d−Ω(k). By a union bound, we can bound the probability that there exists a sphere center
that does not go into a bucket to be d−Ω(k). Hence the Lemma statement holds with exceedingly large
probability of 1 −dΩ(k).
Now, we only include buckets with volume at least (Ω(ϵ/(d
√
k)))k. We can do this procedure using approximate
support estimation algorithms [39]. This takes time and sample complexity S/ log S where S is the size
of the support. With constant probability all points in Lk are mapped to some such high volume bucket
in at least one of the LSH functions. The total number of buckets with this minimum volume is at most
(O((d2√
k)/ϵ))k, which is also an upper bound on the sample complexity and running time of the support
estimation procedure. Now, we lift all the above results when we go to Mk from Lk. Since the Jacobian of
the manifold map f has a constant condition number, its determinant is at most exp(k); so the volume of
any region in Lk changes by at most an exp(±O(k)) multiplicative factor when it goes to Mk. So all volume
arguments in the previous proofs hold with multiplicative factors exp(±O(k)). This concludes our proof.
C
Lower Bound for Analytic Functions
Figure 6: On the left we have an example of an analytic function. The function on the right is not analytic.
Both functions have a bounded Lipschitz constant
The functions described in the lower bound presented earlier are continuous but not diﬀerentiable everywhere
as they are piecewise linear functions. In Theorem C.1 we show that we can make the lower bound stronger by
15

providing a construction of L-Lipschitz analytic functions (which are diﬀerentiable everywhere). See Figure 6
for an example of analytic vs non-analytic functions.
Theorem C.1. A dense model of width t with a random bottom layer requires
t = Ω
 
2k2/2(LC1)k
(
√
kπϵ)k
!
,
where C1 is a large enough constant, to learn L-Lipschitz analytic functions on [−1, 1]d to ℓ∞error ϵ when
the inputs are sampled uniformly over a unknown k-dimensional subspace of Rd ∩[−1, 1]d. Moreover, the
number of samples required to learn the above class of functions is
Ω(t log t) ,
where t = Ω

2k2/2(LC1)k
(
√
kπϵ)k

.
Proof of Theorem C.1. We construct a family F of analytic functions that are L-Lipschitz and are described
using the Fourier basis functions. Each f ∈F will be of the form
f(x) =
∞
X
n1=0
∞
X
n2=0
· · ·
∞
X
nk=0
an1n2...nk exp
 iπn⊤x

,
for x ∈[−1, 1]k. We pick a small value of 0 < ϵ1 < 1. We assume 1/ϵ1 is an integer for convenience. If it is
not, we can simply take ⌈1/ϵ1⌉instead. For a set of integers (n1, n2, . . . , nk) ∈[1/ϵ1]k, let ηn1n2...nk ∈{±}.
We use ηn as a shorthand when it is not ambiguous. The family F is deﬁned as the set of functions f below
f(x) =
1/ϵ1
X
n1,...,nk=0
ηnϵα
1
 exp(iπn⊤x) + exp(iπn⊤x)

,
(4)
where each ηn is chosen to be either ±L/(C
√
kπ) for a large enough constant C and α will be determined
later. There are (1/ϵ1)k Fourier bases in each f and the coeﬃcient of each is set to be ±Lϵα
1 /(C
√
kπ). Hence
we have
|F| = 2((1/ϵ1)k).
(5)
Next we argue that a larger than 0.9 fraction of the functions in F are L-Lipschitz. We have,
∇f(x) =
1/ϵ1
X
n1,...,nk=0
ηnϵα
1 iπ(exp(iπn⊤x) −exp(iπn⊤x))n
=
1/ϵ1
X
n1,...,nk=0
−2ηnπ sin(πn⊤x)ϵα
1 n
(6)
=⇒E [∇f(x)] = 0,
(7)
where the last expectation is over the uniform measure over functions in F. To get a bound on ∥∇f(x)∥2 we
bound each (∇f(x))i with high probability. Each (∇f(x))i is a sum of (1/ϵ1)k independent random variables,
namely ηn. We saw above that E[(∇f(x))i] = 0. To bound |(∇f(x))i| with high probability we will use
McDiarmid’s inequality. An upper bound on how much the value of (∇f(x))i can change when any one ηn
ﬂips in value is computed as 4ϵ(α−1)
1
L/C
√
k. Then, an application of McDiarmid’s concentration inequality
gives us that,
Pr [|(∇f(x))i| > t] ≤2 exp
 
−t2kC2ϵ(k+2−2α)
1
16L2
!
,
=⇒|(∇f(x))i| ≤
L
√
kϵ(k+2−2α)/2
1
(8)
16

with probability ≥0.9 for a large enough constant C. This implies that
∥∇f(x)∥2 ≤
L
ϵ(k+2−2α)/2
1
(9)
with probability ≥0.9 for a randomly sampled f ∈F. Now, let ηf denote the vector of ηn values in sequence
for any f. Using McDiarmid’s (or Hoeﬀding’s) concentration bound again, we also get that, with probability
≥0.9, the Hamming distance between ηf1 and ηf2 for two f randomly sampled from F is at least c(1/ϵ1)k
for a small enough constant c < 1. This implies that for randomly sampled f1, f2,
f1(x) −f2(x)
=
1/ϵ1
X
n1,...,nk=0
2η′
nϵα
1
 exp(iπn⊤x) + exp(iπn⊤x)

,
(10)
where η′
n is non-zero for at least c(1/ϵ1)k of the terms from the above argument about the Hamming distance.
Parseval’s identity then implies that
1
2k
Z 1
−1
. . .
Z 1
−1
(f1(x) −f2(x))2dx1 . . . dxk
≥4L2ϵ2α
1 c
1
ϵk
1C2kπ2
=⇒∥f1 −f2∥∞≥2(k/2+1)L√cϵ(α−k/2)
1
C
√
kπ
.
(11)
Finally we note that by union bound, at least a 0.8 fraction of the functions in F satisfy both our Lipschitzness
property (9) and (11) simultaneously. Setting α = k/2 + 1 and ϵ1 =
C
√
kπϵ
L√c2k/2 we get that to achieve a strictly
smaller error than 2ϵ in the ∥.∥∞sense, one requires a dense model with a width of
Ω
 
2k2/2
 LC1
√
kπϵ
k!
.
D
Experiment details
D.1
Details of learning random functions
In Section 5, we demonstrated experiments for randomly generated polynomial/hypercube functions. Here
we present the details for the experiment settings.
Random function generation. For the random polynomial functions, we randomly generate coeﬃcients
of the monomials by sampling from a uniform distribution U([−1, 1]) and scale the coeﬃcients so that their
absolute values sum up to 1.0 (this is to ensure the Lipschitz constant of the generated function is bounded
by a constant independent of dimension and degree of the polynomial). For the random hypercube function,
we sample values of the function at each corner independently from a uniform distribution on −1, 1, and
interpolate using the indicator functions.
Train/Test dataset generation. For a given target function f (polynomial or hypercube), we sample
independently from U
 [−1, 1]d
(where d is the input dimension) to generate the input features x and compute
target value y = f(x). The train dataset contains 216 (x, y) pairs and the test dataset contains 214 (x, y)
pairs.
Training setting. All the models in Section 5 are trained for 50 epochs using the RMSProp [17] optimizer
with a learning rate of 10−5. For the one dimension example in Section 1, the model is trained for 200 epochs
using the RMSProp optimizer with a learning rate of 5 × 10−6.
Random hash sparse model. We discussed the design of DSM and LSH models in Section 2. Here we
present the details of the random hash model, where the sparsity pattern is determined by a random hash of
the input data (i.e. the same input data would always have the same sparsity pattern). The following code
snippet shows the generation of a random mask that only depends on the input data using TensorFlow 2.x.
17

import tensorflow as tf
# seed: a fixed random seed
# inputs: the input tensor
# mask_dim: size of the masked tensor
# num_buckets: a large integer
# k: the dimension after masking
input_dim = inputs.shape[-1]
if input_dim != mask_dim:
proj = tf.random.stateless_normal(
shape=(input_dim, mask_dim),
seed=seed)
inputs = tf.einsum(
’...i,io->...o’, inputs, proj)
hs = tf.strings.to_hash_bucket_fast(
tf.strings.as_string(inputs),
num_buckets=num_buckets)
top_k_hash = tf.expand_dims(
tf.nn.top_k(hs, k).values[..., -1],
axis=-1)
mask = hs >= top_k_hash
D.2
Learning high-dimensional random polynomials lying in a low-dimensional manifold
We present experiment results for learning random polynomial target functions with low intrinsic dimensions.
To be precise, the target polynomial is p(Ax), where p is a polynomial of degree D with sum of coeﬃcient
absolute value < 1/D, x ∈Rd is a vector, A ∈Rk×d is a matrix with random orthonormal rows, and d > k.
Note now the intrinsic dimension of the domain is k, while the inputs x has higher dimension d. In Figure 7,
we compare the mean squared loss for dense models and DSMs for d = 64, k = 8, and D = 4. We observe
similar behavior as Figure 2, where the input dimension is the same as the intrinsic dimension. This validates
our analysis in Section 3.
16
64
256
1024
4096
16384
Number of activated units (log scale)
0.00
0.01
0.02
0.03
0.04
Mean Squared Error
dense (width = x-axis values)
DSM (total width = 1024)
DSM (total width = 2048)
Figure 7: Scaling behavior of DSM compared with dense models for a random polynomial with low intrinsic
dimensionality. Similar to Figure 2, DSM outperforms dense models at the same number of activated units.
18

