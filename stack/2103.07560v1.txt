Causal Markov Boundaries
Soﬁa Triantaﬁllou
Fattaneh Jabbari
Greg Cooper
Abstract
Feature selection is an important problem in machine
learning, which aims to select variables that lead to
an optimal predictive model. In this paper, we fo-
cus on feature selection for post-intervention outcome
prediction from pre-intervention variables.
We are
motivated by healthcare settings, where the goal is
often to select the treatment that will maximize a
speciﬁc patient’s outcome; however, we often do not
have suﬃcient randomized control trial data to iden-
tify well the conditional treatment eﬀect. We show
how we can use observational data to improve feature
selection and eﬀect estimation in two cases: (a) using
observational data when we know the causal graph,
and (b) when we do not know the causal graph but
have observational and limited experimental data.
Our paper extends the notion of Markov boundary
to treatment-outcome pairs. We provide theoretical
guarantees for the methods we introduce.
In sim-
ulated data, we show that combining observational
and experimental data improves feature selection and
eﬀect estimation.
1
Introduction
Feature selection is a fundamental problem in ma-
chine learning that aims to select the minimal set of
features that lead to the optimal prediction of a tar-
get variable Y . For observational distributions, this
set is the Markov boundary of Y , MB(Y ). In causal
graphical models, this set can be identiﬁed from the
causal graph G [Pearl, 2000]. This set exhausts the
predictive information for the state of a variable Y ,
and can be used to obtain the best (and minimal)
predictive model P(Y |MB(Y )) for Y .
In decision making, we are often interested in
ﬁnding the optimal predictive model for the post-
intervention distribution of an outcome Y after we
intervene on a treatment X, when we only have ob-
servational data.
Ideally, we would like to include
in our model the Markov boundary Z of Y in the
post-intervention causal graph that is parameterized
with the post-intervention distribution. However, un-
der causal insuﬃciency in which latent confounding
may exist, the conditional post-interventional distri-
bution P(Y |do(X), Z) may not be identiﬁable. For
example, in Fig. 1, P(Y |do(X), A, B) is not identi-
ﬁable from the observational distribution alone. In
this case, we are interested in identifying the opti-
mal set Z for which the post-intervention distribu-
tion P(Y |do(X), Z) is identiﬁable from observational
data, which we call the causal Markov boundary.
Moreover, even when experimental data are avail-
able, they typically have much smaller sample sizes
and are not powered to identify conditional distribu-
tions. In that case, we would like to combine large
observational data with limited experimental data to
improve interventional feature selection and eﬀect es-
timation.
Our methods are heavily motivated by embedded
clinical trials [Angus, 2015, Angus et al., 2020], which
take place within usual clinical care.
In these tri-
als, patients who agree to participate are random-
ized to receive a treatment from among those consid-
ered eﬀective for that patient. The electronic health
records (EHRs) of the health system in which the
trial is being conducted contains both experimental
data from the trial, and observational data obtained
outside (e.g., before/after) the trial, all measuring
the same variables. Combining observational and ex-
perimental data has the potential to better predict
the most eﬀective treatments for individual patients,
than either type of data alone.
1
arXiv:2103.07560v1  [stat.ML]  12 Mar 2021

Observational
Markov
Boundary
(OMB) of Y : MB(Y)
The Markov boundary of Y . Leads to optimal prediction of
Y from observational data.
Interventional
Markov
Boundary
(IMB) of Y relative to X: MBX(Y )
The Markov boundary of Y in the post-intervention distri-
bution PX. Leads to optimal prediction of Yx from experi-
mental data.
Causal Markov Boundaries (CMB) of
Y relative to X: CMBX(Y )
Sets of measured variables that satisfy Deﬁnition 3.2. Pos-
sibly not unique, and possibly empty. If not empty, one of
the CMBs leads to the optimal prediction of Yx from obser-
vational data.
Table 1: Diﬀerent Markov boundaries discussed in this paper.
Our contributions are the following:
• We deﬁne the interventional Markov boundary
MBX(Y )
and the causal Markov boundaries
CMBX(Y ) for an outcome Y and a treatment
X. These sets correspond to the minimal set of
covariates Z that are maximally informative for
Y |do(X), from experimental and observational
data, respectively (Sec 3).
Table 1 summarizes
the types of Markov boundaries discussed in this
paper.
• We present a Bayesian method that combines ob-
servational and experimental data to learn inter-
ventional Markov boundaries.
The method pro-
vides estimates of the post-interventional distribu-
tion that are based on both observational and ex-
perimental data, when possible (Sec. 4), in which
case the IMB is a CMB. In simulated data, we show
that our method improves causal eﬀect estimation
(Sec. 6).
2
Preliminaries
We use the framework of semi-Markovian causal
models [SMCMs, Tian and Shpitser, 2003], and as-
sume the reader is familiar with related terminology.
Variables are denoted in uppercase, their values in
lowercase, and variable sets in bold. We use G to de-
note a causal graph, and say G induces a probability
distribution P if P factorizes according to G and the
causal Markov condition.
We use Y |do(X) or YX to denote a variable Y af-
ter the hard intervention on variable X. If we know
the causal SMCM G, a hard intervention of where a
treatment X is set to x can be represented with the
do-operator, do(X=x). We use Px or to denote the
interventional distribution over the same variables for
do(X=x). In the corresponding graph, this is equiv-
alent to removing all incoming edges into X, while
keeping all other mechanisms intact.
We use GX
to denote the graph stemming from G after remov-
ing edges into X. We use GX to denote the graph
stemming from G after removing edges out of X. We
use the terms PaG(Z), ChG(Z) to denote the set of
parents and children of Z in G, respectively.
The
set of variables that are connected with a variable
Y through a bidirected path (i.e., a path that only
has bidirected edges) is called the district of Y , and
denoted DisG(Y ).
3
Markov Boundaries
A Markov blanket of a variable Y in a set of vari-
ables V is a subset Z of V conditioned on which
other variables are independent of Y : Y ⊥⊥V \ Z|Z.
The Markov boundary of Y is the Markov blanket
that is also minimal (i.e., no subset of the Markov
boundary is a Markov blanket) [Pearl, 2000]. In dis-
tributions that satisfy the intersection property (in-
cluding faithful distributions), the Markov bound-
ary of a variable Y
is unique [Pearl, 1988].
To
distinguish from other types of Markov boundaries
deﬁned in this work, we often use the terminol-
ogy
Observational Markov Boundary (OMB)
2

to denote the Markov boundary of a variable.
For a DAG G, the OMB of a variable Y in any
distribution faithful to G is the set parents, children,
and spouses of Y : MB(Y ) = PaG(Y ) ∪ChG(Y ) ∪
PaG(ChG(Y )). For SMCMs, it has been shown that
the OMB of a variable Y is the set of parents, chil-
dren, children’s parents (spouses) of Y , district of Y
and districts of the children of Y , and the parents of
each node of these districts [Richardson, 2003, Pellet
and Elisseeﬀ, 2008]1.
The OMB has been shown to be the minimal set of
variables with optimal predictive performance for a
given distribution and response variable, given some
assumptions on the learner and the loss function
[Tsamardinos and Aliferis, 2003]. In this work, we
are interested in the model that gives the optimal
prediction of the post-intervention distribution, with
the goal of designing optimal policies. For this reason,
we are not interested in including post-intervention
covariates in this model, because these variables are
not known prior to treatment assignment, and thus,
cannot aﬀect the assignment. In the rest of this doc-
ument, we make the following assumption:
Assumption 3.1. Covariates V are pre-treatment.
This simpliﬁes the expressions for the OMBs, be-
cause we no longer need to consider children of Y and
their districts. Knowing the OMB allows a more ef-
ﬁcient representation of the conditional distribution
of Y given V, since the following equation holds:
P(Y |V) = P(Y |MB(Y )).
(1)
1Pellet and Elisseeﬀ[2008] prove this for maximal ancestral
graphs, but the proof can be readily adapted to SMCMs.
X
A
B
Y
Figure 1:
An example SMCM. {X, A, B} is the
Markov boundary of Y in GX . P(Y |do(X), A, B) is
not identiﬁable from the observational distribution P,
but P(Y |do(X), A) and P(Y |do(X), B) are. {X, B}
is the causal Markov boundary for YX.
3.1
Interventional Markov Boundary
Our goal is to identify the set of variables that lead
to the optimal model for the post-intervention distri-
bution of a target Y relative to a speciﬁc treatment
X.
We call this set the interventional Markov
boundary (IMB) of Y relative to X, and denote
it MBX(Y ). Obviously, MBX(Y ) ⊆MB(Y). When
we have data from the post-intervention distribution,
we can apply statistical methods for OMB identiﬁca-
tion to obtain the IMB of Y relative to X. However,
experimental data are often limited in sample sizes,
while OMB identiﬁcation methods may require large
sample sizes.
If
we
know
the
causal
graph
G,
the
post-
intervention distribution with respect to X is in-
duced by the manipulated graph GX .
The IMB
of Y is then the OMB of Y in GX , and can be
identiﬁed using the deﬁnition of the Markov Bound-
ary above. However, the post-intervention distribu-
tion P(Y |do(X), MBX(Y ) \ X), may not be identi-
ﬁable from the observational distribution.
For ex-
ample, in Fig.
1, MBX(Y )
= {X, A, B}, but
P(Y |do(X), A, B) is not identiﬁable from observa-
tional data. We then want to answer the following
question: What is the best model for predicting YX
from the observational distribution, when the causal
graph is known?
3.2
Causal Markov Boundaries
To answer this question,
we deﬁne the causal
Markov boundaries of an outcome Y relative to
a treatment X as follows:
Deﬁnition 3.2. Let Z ⊆(V ∪X), and W = Z \ X.
Then Z is a causal Markov boundary (CMB) for Y
relative to X if it satisﬁes the following properties:
1. P(Y |do(X), W) is identiﬁable from P(X, Y, V).
2. For
every
subset
W′
of
V \ W
either
P(Y |do(X), W, W′)
=
P(Y |do(X), W)
or
P(Y |do(X), W, W′) is not identiﬁable from
P(X, Y, V).
3. ∄W′
⊂
W
s.t.
P(Y |do(X), W′)
=
P(Y |do(X), W).
3

Condition (1) ensures that the post-intervention
conditional probability of YX given a CMB is identi-
ﬁable. Condition (2) states that the covariates that
are not in that CMB are either redundant for the pre-
diction of YX given the CMB, or they make the post-
intervention distribution non-identiﬁable. Condition
(3) ensures that Z is additionally maximally informa-
tive for YX in the sense that you cannot remove any
variable from Z without losing some information for
YX. This condition rules out sets like {X, A} in Fig.
1, where, while P(Y |do(X), A) is identiﬁable from P,
it is equal to P(Y |do(X)). Thus, conditioning on A
does not improve the prediction of YX compared to
its subset ∅.
Notice that this deﬁnition does not capture the
spirit of Markov boundaries precisely:
Markov
boundaries make all remaining variables redundant
for predicting Y ; however, this does not necessarily
hold with CMBs. For example, in Fig. 1, {X, B} is a
CMB according to the deﬁnition above, but {A} re-
mains relevant for predicting YX; however, including
it with B in the CMB leads to non-identiﬁability.
CMB is not necessarily unique; it is possible that
multiple sets satisfy Deﬁnition 3.2. For example, as-
sume the distribution P is induced by the SMCM
shown in Fig.
2.
Both {X, B, C} and {X, A, D},
satisfy Deﬁnition 3.2. The best predictive CMB for
predicting YX will depend on the parameters in P.
We use the notation CMBX(Y ) to denote the set of
causal Markov boundaries of Y relative to X. Thus,
we will generally need to ﬁnd all CMBs and then de-
termine which of them leads to the best prediction of
YX. Also, notice that the CMBX(Y ) can be empty;
thus, no subset of V satisﬁes the Deﬁnition 3.2. This
can happen for example if X →Y and X ↔Y in G.
The CMB is useful in determining a minimal set of
maximally predictive variables for which we can use
observational data to predict post-interventional dis-
tributions. In the next section we show that, for pre-
treatment covariates, CMBs satisfy the back-door cri-
terion and are subsets of the observational Markov
boundary. These results enable more eﬃcient algo-
rithms for ﬁnding CMBs, limiting the types of esti-
mators and the number of variable sets we need to
consider.
X
A
B
C
D
Y
Figure 2:
Causal Markov boundaries are not nec-
essarily unique. Both {X, B, C} and {X, A, D} are
causal Markov boundaries for Y relative to X.
3.2.1
Characterization
Given a graph G, Shpitser and Pearl [2006a] provide a
sound and complete algorithm (IDC) for estimating
conditional post-intervention distributions from ob-
servational distributions induced by G. The output
of this algorithm is an expression for P(Y |do(X), W)
if the distribution is identiﬁable from distribution P
and G, or N/A otherwise.
Thus, we can identify
CMBs in a brute-force way by running IDC for ev-
ery possible subset of V, and then check for sets that
satisfy the conditions in Def.
3.2.
This process is
computationally expensive and would not be possi-
ble for graphs with more than a few variables.
In this section, we provide theoretical results that
lead to a much easier process when all candidate
conditioning variables are pre-treatment (all proofs
can be found in the supplementary).
For pre-
treatment covariates, one obvious family of sets for
which the conditional post-intervention distributions
are identiﬁable are sets that m-separate X and Y in
GX. These sets satisfy Rule 2 of do-calculus [Pearl,
2000], so the conditional interventional distribution
P(Y |do(X), W) is equal to the observational distri-
bution P(Y |X, W). Sets of pre-treatment covariates
that m-separate X and Y in GX are also known to
satisfy the backdoor criterion [Van der Zander et al.,
2014] and the adjustment criterion [Shpitser et al.,
2012]. However, these deﬁnitions are more general to
include possible post-treatment covariates, and are
intended for estimating marginal post-intervention
distributions (or average eﬀects). For brevity, we will
call sets that m-separate X and Y in GX backdoor
sets, since they block all back-door paths between X
4

and Y .
One question that arises is if there are sets that
are not backdoor sets that may satisfy the conditions
in Deﬁnition 3.2. In that case, identiﬁability could
stem from some sequential application of do-calculus
rules. As we show next, this is not possible for pre-
treatment covariates. This ensures that we only need
to check CMB Conditions (2) and (3) for sets for
which P(Y |do(X), W) = P(Y |X, W). This makes
the identiﬁcation of P(Y |do(X), W) more straight-
forward than having to compute more complex prob-
ability expressions.
Theorem 3.3. We assume that Px and GX are faith-
ful to each other. If Z is a CMB for Y relative to X,
then Z \ X is a backdoor set for X relative to Y .
The second theoretical result is that any CMB of Y
relative to X is a subset of the OMB of Y . While this
sounds intuitive, it is not completely straightforward.
It could be the case that conditioning on every subset
of the OMB opens some m-connecting path between
X and Y , that can only be blocked by a variable
that is not a member of the Markov boundary. The
following theorem proves that this is not possible,
allowing for more eﬃcient search algorithms:
Theorem 3.4. We assume that Px and GX are faith-
ful to each other. Every CMB Z of an outcome vari-
able Y w.r.t a treatment variable X is a subset of the
OMB MB(Y ).
Based on Theorem 3.4, we only need to look for
CMBs within subsets of the OMB of Y . So far, we
have shown that both the IMB and any CMB are
subsets of the OMB. We can also show that when
the IMB is a CMB, then it also coincides with the
OMB:
Theorem
3.5. If MBX(Y ) is a causal Markov
boundary, then MBX(Y ) = MB(Y ).
4
Combining observational and
experimental data
When the causal graph is known, we can obtain
CMBs by looking for subsets of MB(Y ) that satisfy
Def. 3.2. Unfortunately, in most real-world applica-
tions, the true graph is unknown, and selecting the
causal/interventional Markov boundary is not possi-
ble from observational data alone. Experimental data
may exist, but are typically much fewer than observa-
tional data, due to expense or ethical concerns. This
scenario is common in embedded trials, where non-
randomized patients are much more common than
trial participants.
In such cases, the experimental
data may be under-powered to accurately estimate
conditional eﬀects. As a result, the conditional ef-
fects that can be derived from the experimental data
have high variance and may not be reliable. In this
case, combining all data (observational and experi-
mental) in a Bayesian manner may help improve the
prediction of Yx.
We assume that we have observational data Do and
experimental data De measuring treatment X, out-
come Y , and pre-treatment covariates V.
We use
No, Ne to denote the number of samples in Do, De,
respectively.
We present a Bayesian method, called FindIMB,
that uses both De and Do to estimate the prob-
ability of a set being the MBX(Y ) , and esti-
mate P(Y |do(X), V) = P(Y |do(X), MBX(Y ) \ X).
The method is presented in Alg.
1.
The method
ﬁrst estimates the OMB of Y in observational data
MB(Y ) (Line 1), and then looks among subsets of
MB(Y ) for sets that are IMBs (Line 2). It uses De
and Do to evaluate the probability that a set is an
IMB (Line 3), and then returns a weighted average
for P(Y |do(X), V) based on these probabilities (Line
5).
The enabling idea of the method is that, when the
IMB is a CMB, we can use both the Do and De to es-
timate the conditional post-intervention distribution.
Otherwise, we use only De to derive the estimate. We
use the following notation to express these hypothe-
ses:
• Hc
Z is a binary variable denoting the hypothesis
that Z is the IMB MBX(Y ) , and it is also a
CMB: Z = MBX(Y ) ∧Z ∈CMBX(Y ) .
• Hc
Z is a binary variable denoting the hypothesis
that Z is the IMB MBX(Y ) , but it is not a
CMB: Z = MBX(Y ) ∧Z ̸∈CMBX(Y ) .
5

For a set Z⋆, if either Hc
Z⋆
or Hc
Z⋆
is true,
Z⋆is an IMB and therefore P(Y |do(X), V)
=
P(Y |do(X), Z⋆\ X). Under Hc
Z⋆though, Z⋆is also
a CMB and therefore the pre- and post- intervention
distributions are the same, i,e,
P(Y |do(X), Z⋆\ X, Hc
Z⋆) = P(Y |X, Z⋆\ X)
(2)
In contrast, under Hc
Z⋆, Eq.
2 does not hold:
Z⋆is the IMB, but not a CMB. This means that
P(Y |do(X), Z⋆\ X) is not identiﬁable from ob-
servational data, and we cannot use Do to esti-
mate P(Y |do(X), Z⋆\ X)2.
In summary, if ei-
ther of Hc
Z⋆
orHc
Z⋆
holds, Z⋆is the IMB and
P(Y |do(X), V) = P(Y |do(X), Z⋆\X). If Hc
Z⋆holds,
we can use both Do and De in our estimation of
P(Y |do(X), Z⋆\ X), while if Hc
Z⋆holds we can only
use De.
Based on this observation, we want to compute
P(Hc
Z |De, Do) and P(Hc
Z |De, Do) for possible IMBs
Z. These probabilities tell us both how likely it is
that Z is an IMB (their sum), and if we can include
observational data in the estimation of P(Y |(X), V).
Using Bayes rule, we obtain:
P(Hc
Z′ |De, Do) =
P(De|Do, Hc
Z′ )P(Do|Hc
Z′ )P(Hc
Z′ )
X
Z
X
C=c,c
P(De|Do, HC
Z )P(Do|HC
Z )P(HC
Z )
.
(3)
We can similarly derive P(Hc
Z |De, Do) by replacing
each appearance of c with c in the numerator. The
denominator is the same for all sets. P(Hc
Z′ ) and
P(Hc
Z′ ) is our prior that Hc
Z′ or Hc
Z′ holds. We set
this to be uniform over both values of C and all Z.
As Eq. 3 shows, using Bayes rule, we can estimate
the posterior probabilities for the set of hypotheses
Hc
Z and Hc
Z using marginal likelihoods of the exper-
imental and observational data. In the next sections,
we show how we can compute each term in Eq. 3.
We present our results for multinomial distribu-
tions, but we believe the method can be readily ex-
tended to any type of distribution with closed-form
marginals. Due to space constraints, the closed-form
2Notice however that Do may still place some constraints
on P(Y |do(X), Z⋆\ X), like for example provide bounds.
solution for each equation appearing in remainder of
the paper is presented in Supplementary Table 1.
Estimating P(De|Do, Hc
Z′ ), P(De|Do, Hc
Z′ ):
Let W = Z \ X, and let θYx|W be a set of pa-
rameters expressing the conditional probabilities for
P(Y |do(X), W). Also, let θY |X,W denote the obser-
vational parameters for P(Y |X, W). By integrating
over all θYx|W, we obtain
P(De|Do, Hc
Z ) =
Z
Yx|W
P(De|θYx|W)f(θYx|W|Do, Hc
Z )dθYx|W
(4)
f(θYx|W|Do, Hc
Z ) is the posterior for θYx|W given
the observational data, when Z is the IMB and the
CMB. In this case, P(Y |do(X), W) = P(Y |X, W),
and therefore f(θYx|W|Do, Hc
Z ) = f(θY |X,W|Do).
Eq. 4 can then be rewritten in terms of the observa-
tional parameters as
P(De|Do, Hc
Z ) =
Z
θY |X,W
P(De|θY |X,W)f(θY |X,W|Do)dθY |X,W
(5)
Eq. 5 is the marginal likelihood of Y in experimen-
tal data, with parameter density f(θY |X,W|Do) being
equal to the parameter posterior given Do. In other
words, under Hc
Z , the observational and experimen-
tal parameters coincide.
Therefore, Do gives us a
strong ”prior” for De. Eq. 5 can be computed in
closed-form for distributions with conjugate priors.
Under Hc
Z , the equality of the observational and
experimental parameters does not hold, and we can-
not use the θY |X,W to inform θYX|W, at least not
in a straightforward way. Instead, we impose that
f(θYX|W|Do) = f(θYX|W). Then P(De|Do, Hc
Z ) cor-
responds to the marginal likelihood of Y in the ex-
perimental data, using a prior that we model as being
non-informative.
Estimating P(Do|Hc
Z ), P(Do|Hc
Z ):
These probabilities score how well the observational
data ﬁt with the hypotheses Hc
Z , Hc
Z . We can derive
these terms on the basis of the OMB and its connec-
tion to the IMB and the CMBs.
We ﬁrst need to
express the hypothesis that a set U is the OMB
of Y : Let Ho
U denote this hypothesis; thus, for any
6

U ⊆V ∪X, Ho
U
is true iﬀU is the OMB for Y .
Then we can write
P(Do|HC
Z ) =
X
U⊆V∪X
P(Do|Ho
U )P(Ho
U |HC
Z ), (6)
for C = c, c. Under Hc
Z , Theorem 3.5 implies that
P(Ho
U |Hc
Z ) = 1 if U = Z, and zero otherwise.
Under Hc
Z , the IMB is not a CMB. Instead, the IMB
has to be a subset of U, therefore P(Ho
U |HC
Z ) = 0
for any Z ⊃U.
P(Do|Ho
U ) is the marginal likelihood of Y in Do,
under the hypothesis that U is the data-generating
OMB for Y in the observational data. We can obtain
this likelihood using a Bayesian scoring algorithm like
FGES, by scoring a DAG where Y is a child of vari-
ables U (and no other edges are in the graph). We
call this algorithm FGESMB. For discrete variables, in
the large sample limit this probability will be maxi-
mum only for the true OMB:
Theorem 4.1. Given dataset Do that contains sam-
ples from a strictly positive distribution P, which is
a perfect map for a SMCM G, the BD score [Hecker-
man et al., 1995] will assign the highest score to the
OMB of Y in the large sample limit.
Eq. 6 needs to be computed for all subsets of the
covariate sets.
In practice, however, for large No,
these probabilities are dominated by the true OMB.
Assuming our sample is large enough, we can use an
algorithm with asymptotic guarantees for identifying
the true OMB. In fact, once we commit to the an
observational Markov boundary U⋆, Eq. 6 leads to
the following equations
P(Do|Hc
Z )=P(Do|Ho
U⋆) for Z=U⋆
P(Do|Hc
Z ) = P(Do|Ho
U⋆) for all Z ⊆U⋆
(7)
For the remaining cases (Hc
Z and Z=U⋆, or Hc
Z and
Z ⊃U⋆), the corresponding probabilities are zero.
Bayesian estimation of P(Y |do(X), V, De, Do):
We now compute the P(Y |do(X), V) using Bayesian
model averaging over the hypotheses HC
Z
.
Let
x, y, V = v, denote given instances of X, Y and V,
respectively. When V = v, we use W = wv to de-
note the corresponding values of a set W ⊂V. Recall
Algorithm 1: FindIMB
input : Do, De, treatment X, outcome Y ,
pre-treatment covariates V
output: Post-intervention distribution
P(Y |do(X), V)
1 MB(Y ) ←MarkovBoundary(Y, Do);
2 foreach subset Z of MB(Y ) and C = c, c do
3
Compute P(HC
Z |De, Do) using Eq. 3;
4
Compute P(Y |do(X), V, De, Do, HC
Z )
using Eq. 8;
5 P(Y |do(X), V) ←
P
Z
P
C=c,c P(Y |do(X), V, De, Do, HC
Z )P(HC
Z |De, Do);
that under Hc
Z and under , Hc
Z , P(Y |do(X), V) =
P(Y |X, W), where W = Z \ X.Then for a given in-
stance of V = v, we have
P(y|do(x), v, De, Do) =
X
Z⊂V
X
C=c,c
P(y|do(x), wv, De, Do, HC
Z )P(HC
Z |Do, De)
(8)
This equation computes the expectation of the condi-
tional probability parameter. The individual prob-
abilities P(Y |do(X), W, De, Do, HC
Z
) can be esti-
mated as posterior expectations of P(Y |do(X), W)
from the data.
Speciﬁcally,
under given Hc
Z ,
P(Y |do(X), W) = P(Y |X, W), and therefore we can
use both De and Do for the posterior expectation.
In contrast, under Hc
Z , we only use De. Analytical
equations for these probabilities for multinomial dis-
tributions can be found in Supplementary Table 1.
5
Related work
We are not aware of other methods that try to iden-
tify causal and interventional Markov boundaries.
Our work has connections and builds on work from
many diﬀerent areas. Due to space constraints, we
only focus on methods that do not require causal suf-
ﬁciency. Markov boundaries: Several algorithms
7

***
**
***
***
***
***
***
***
***
Figure 3:
Boxplots of absolute bias in the estimation of P(Y |do(X), V) using (a) FindIMB (b) IMB (c) OMB
and (d) FCIt-IMB. Data were simulated from random DAGs with 10 observed and 5 latent variables. Do
included 10,000 samples, and De included 100 (left), 200 (middle) or 1000 (right) samples. FindIMB improves
the estimation of P(Y |do(X), V) particularly for smaller experimental sample sizes. Black asterisks denote
statistical signiﬁcance, assessed with the Wilcoxon signed-rank test. Three stars correspond to p < 0.001.
learn OMBs from data under causal insuﬃciency [Yu
et al., 2018, 2020].
In addition, FGESMB presented
in Sec.
4 is also a sound and complete method
for learning OMBs from data. These methods can
be used to identify the IMBs from the experimen-
tal data, but they do not combine observational and
experimental data to learn IMBs. Identiﬁability:
Shpitser and Pearl [2006a,b] and Tian and Shpitser
[2003] provide sound and complete identiﬁability re-
sults for post-intervention distributions from observa-
tional data when the causal graph is known. These
methods can answer queries for a speciﬁc marginal
or conditional probability of interest. Hyttinen et al.
[2015] and Jaber et al. [2019] provide similar iden-
tiﬁability results when the graph is unknown, using
the Markov equivalence class of graphs that are con-
sistent with the observational data. Hyttinen et al.
[2015] can provide identiﬁability results for graphs
that are consistent with conditional independencies
in both De and Do.
However, the method is not
proven to be complete for these settings. These meth-
ods are not directly comparable with our method
because they do not select features for optimal pre-
diction. Moreover, they provide expressions for the
post-intervention distributions that are based on ob-
servational data alone, not by combining Do and De
like FindIMB. Combining observational and ex-
perimental data to learn causal graphs: Sev-
eral causal discovery methods combine observational
and experimental data to learn causal structure [Tri-
antaﬁllou and Tsamardinos, 2015, Hyttinen et al.,
2014, Mooij et al., 2020, Andrews et al., 2020]. These
methods return a summarized version of all the causal
graphs that are consistent with all the independence
constraints in all the data sets, observational and ex-
perimental. While these methods can be used to im-
prove the estimation of IMBs, it is not clear that they
can always provide a unique solution in this setting.
Two additional drawbacks they have for the purpose
of optimized target prediction are that (a) they rely
on conditional independence tests that are unreliable
when Ne is low, and (b) they learn the entire graph
and do not focus on ﬁnding the neighborhood of the
target variable. This can result in unreliable orien-
tations due to error propagation. The method that
has the closest setting to ours is by Andrews et al.
[2020] (Mooij et al. [2020] is also related, but more
general, and the two are equivalent for our setting).
The authors propose a method called FCItiers that
can learn a family of SMCMs from De and Do when
(a) the target of the intervention is known and (b)
we specify ”tiered knowledge” on the variables (e.g.,
we know which variables are pre-treatment).
The
method is complete in these settings. In the exper-
imental section, we develop a baseline comparison
method based on FCItiers. Selecting optimal ad-
justment sets. Some methods seek to select optimal
adjustment sets for eﬃcient average treatment eﬀect
8

estimation [Perkovic et al., 2017, Rotnitzky and Smu-
cler, 2019, 2020]. While these methods have a diﬀer-
ent purpose than ours, they have some connections
with our work since, for pre-treatment variables, any
CMB is also an adjustment set. We point out that
while optimal adjustment sets and CMBs may often
coincide (for example in DAGs), they are not always
the same (see example Fig S1 in the Supplementary).
Moreover, these methods are not directly comparable
to ours since they focus on identifying average treat-
ment eﬀects. Potential Outcomes Approaches:
Kallus et al. [2018] present a method for estimat-
ing conditional average treatment eﬀects (CATEs) by
combining Do and De. The method assumes a binary
treatment and uses the experimental data to model
the eﬀect of possibly unmeasured confounders as a
function of the measured covariates. The CATE is
obtained from the Do by adding the modeled correc-
tion. The main assumption of the method is that the
hidden confounding has an identiﬁable parametric
structure. The method is implemented for continuous
covariates and outcome and a linear correction func-
tion, obtained by solving a least squares optimization
problem. It is not directly applicable to our settings
of categorical covariates, and extending the optimiza-
tion problem in these settings is not straightforward.
Transportability: Finally, our work has some con-
nections with the ﬁeld of transportability Bareinboim
and Pearl [2013], where knowledge of the causal graph
is used to determine if the results of an experimen-
tal trial apply to a diﬀerent population.
However,
the methods require knowing the causal graph, and
focus on transferring estimators across distributions
rather than combining data to improve estimators.
6
Experiments
In this section, we show the performance of FindIMB
using simulated data. We simulated random DAGs
with a varying number of discrete variables, with
mean in-degree 2. Each DAG includes a binary treat-
ment X and outcome Y , where X →Y .
The re-
maining covariates V are pre-treatment and are bi-
nary or ternary, and 1/3 of the variables are set
to be latent.
The observational data Do consist
of 10,000 simulated samples from the ground truth
DAGs, and do not include values for the latent vari-
ables.
Comparison to other approaches.
We
compared FindIMB to the following approaches: (a)
IMB: using only experimental data.
We used De
to identify the MBX(Y ) using FGESMB. After identi-
fying MBX(Y ) , and we used the posterior expecta-
tion P(Y |do(X), MBX(Y )
\ X, De) as the estima-
tor for P(Y |do(X), V). (b) OMB: using only obser-
vational data. We used FGESMB(Do) to identify the
OMB of Y , MB(Y ), and used the posterior expec-
tation P(Y |do(X), MB(Y )\X, Do) estimated on Do
as the estimator of P(Y |do(X), V). This estimator
is unbiased when conditional ignorability holds for
the OMB of Y . (c) FCIt-IMB: Using both obser-
vational and experimental data based on FCItiers:
We use FCItiers using as input a data set D con-
structed by concatenating De and Do, and adding a
binary variable Ie →X that corresponds to the pres-
ence or absence of manipulation of X. So, Ie = 1
for samples in De and Ie = 0 otherwise. FCIt-IMB
outputs a PAG P representing all possible underly-
ing SMCMs. Let PX denote the corresponding ma-
nipulated PAG. We then take the Markov boundary
of Y in PX to be the IMBX(Y ). After identifying
IMBX(Y ), we test if it is a backdoor set in PX. If
so, we used both De and Do pooled together to es-
timate P(Y |do(X), IMBX(Y ) \ X). Otherwise, we
only used De.
First, we tested if our FindIMB method improves
estimation of the probability P(Y |do(X), V).
We
simulated DAGs with 10 observed and 5 latent vari-
ables, and applied the methods described above.
Each method outputs a set of variables Z, that is
used as an estimate for P(Y |do(X), Z, V \ Z). No-
tice that even for 10 variables, the number of possi-
ble conﬁgurations of Z can be very large, and some
of these conﬁgurations may be very rare. To avoid
computing these parameters for all possible conﬁgu-
rations, we tested the methods in a test dataset Dtest
e
,
that includes 1000 treatment and 1000 control cases
simulated from the manipulated ground truth graph.
For each sample in Dtest
e
, we obtained an estimate
ˆP(Y |do(X), V) with the four methods. The ground
truth probability was estimated from the original ma-
nipulated Bayesian network with the junction tree
9

***
***
***
Figure 4:
Boxplots of areas under the ROC curve for predicting Yx using (a) FindIMB (b) IMB and (c)
OMB. Data were simulated from random DAGs with 40 observed and 20 latent variables. Do included
10,000 samples, and De included 100 (left), 200 (middle) or 1000 (right) samples. Black asterisks denote
statistical signiﬁcance, assessed with the Wilcoxon signed-rank test. Three stars correspond to p < 0.001,
no stars denote non-signiﬁcance.
Figure 5:
Performance of FindIMB, IMB and OMB
for estimating P(Y |do(X=1), M) with increasing m-
bias.
algorithm. We then computed the average absolute
bias | ˆP(Y |do(X), V) −P(Y |do(X), V)| over all test
samples. Fig. 3 shows that FindIMB produced the
most accurate probabilities, compared to using only
observational or only experimental data. Moreover,
FindIMB outperforms FCIt-IMB (p < 10−3 in all cases
for a left-tailed t-test). One reason is that FCIt-IMB
selects much larger IMBs than FindIMB, possibly due
to error propagation that results in many bidirected
edges. Thus, the resulting parameters are estimated
based on much fewer samples.
We also tested the scalability of FindIMB, using
DAGs with 40 observed and 20 latent variables. For
this experiment, we could not test against FCIt-IMB
because the method results in very large IMBs. For
the same reason, we could not estimate the true pa-
rameters P(Y |do(X), V) for computational reasons,
since the ground truth IMBs can also be very large
and include rare conﬁgurations. For this reason, in-
stead of measuring the average absolute bias, we mea-
sured the performance of the methods to classify the
test samples correctly. Fig. 4 shows the area under
the ROC curve (AUC) of the models based FindIMB,
IMB, and OMB. FindIMB performs on par or better
than the two alternatives. Average running time for
FindIMB (learning the model) was 1.71±2.46 seconds
per iteration.
One interesting ﬁnding is that in all experiments,
using the observational data only, performs better
than using experimental data and often is close to
the performance when combining De and Do using
FindIMB. This happens because in random graphs,
the eﬀect of variables inducing bias is often negli-
gible [Greenland, 2003], and proxies of the unmea-
sured confounders are often included in the observed
covariates. However, there are cases where the condi-
tioning on an observed variable in observational data
can produce heavily biased post-intervention proba-
bility estimates. A very simple example is the graph
in supplementary Fig.
S1), which we call the ”m-
bias” graph. To illustrate how m-bias can aﬀect the
10

prediction of Yx from observational data, we simu-
lated data from the m-bias graph with binary vari-
ables. We set Y, X and M to be noisy-AND func-
tions of their parents with a parameter α.
α has
a monotonic relationship with the bias in estimating
P(Y |do(X = 1), M) using observational data: Larger
α leads to a larger bias. We then varied alpha from
0.5 to 1, and we simulated Do and De with 10,000 and
1000 samples, respectively. We used FindIMB, IMB
and OMB to estimate P(Y |do(X = 1), M) in a test
data set. Fig. 5 shows the bias in the estimated pa-
rameter. We can see that while using Do to estimate
P(Y |do(X = 1), M) leads to increasing bias, combin-
ing De and Do can identify the situations where the
parameter is not identiﬁable from observational data.
We believe that noisy-AND types of distributions are
not rare in biomedical data.
7
Discussion
Our paper extends the concepts of Markov bound-
aries for predicting post-intervention distributions,
and presents a method for learning such Markov
boundaries from mixtures of observational and ex-
perimental data. The method could be useful in set-
tings like embedded trials, where we have abundant
observational and limited experimental data.
Fu-
ture work includes extensions of the method to mixed
data, overlapping covariate sets in observational and
experimental data, and to non-singleton treatments
and outcomes.
References
Bryan Andrews,
Peter Spirtes,
and Gregory F
Cooper. On the completeness of causal discovery
in the presence of latent confounding with tiered
background knowledge. In International Confer-
ence on Artiﬁcial Intelligence and Statistics (AIS-
TATS), pages 4002–4011. PMLR, 2020.
Derek C Angus. Fusing randomized trials with big
data: The key to self-learning health care systems?
Journal of American Medical Association (JAMA),
314(8):767–768, 2015.
Derek C Angus, Scott Berry, Roger J Lewis, Farah
Al-Beidh,
Yaseen Arabi,
Wilma van Bentum-
Puijk, Zahra Bhimani, Marc Bonten, Kristine
Broglio, Frank Brunkhorst, et al. The REMAP-
CAP (randomized embedded multifactorial adap-
tive platform for community-acquired pneumonia)
study rationale and design. Annals of the Ameri-
can Thoracic Society, 17(7):879–891, 2020.
Elias Bareinboim and Judea Pearl. A general algo-
rithm for deciding transportability of experimental
results. Journal of Causal Inference, 1(1):107–134,
2013.
Thomas M Cover. Elements of Information Theory.
John Wiley & Sons, 1999.
Sander Greenland. Quantifying biases in causal mod-
els: Classical confounding vs collider-stratiﬁcation
bias. Epidemiology, 14(3):300–306, 2003.
David Heckerman, Dan Geiger, and David M Chick-
ering. Learning Bayesian networks: The combina-
tion of knowledge and statistical data.
Machine
Learning, 20(3):197–243, 1995.
Leonard Henckel, Emilija Perkovi´c, and Marloes H.
Maathuis.
Graphical criteria for eﬃcient total
eﬀect estimation via adjustment in causal linear
models. arXiv preprint arXiv:1907.02435, 2019.
Antti Hyttinen, Frederick Eberhardt, and Matti
J¨arvisalo. Constraint-based causal discovery: Con-
ﬂict resolution with answer set programming. In
Proceedings of the 30th Conference on Uncertainty
in Artiﬁcial Intelligence (UAI), pages 340–349,
2014.
Antti Hyttinen, Frederick Eberhardt, and Matti
J¨arvisalo. Do-calculus when the true graph is un-
known. In Proceedings of the 31st Conference on
Uncertainty in Artiﬁcial Intelligence (UAI), pages
395–404, 2015.
Amin Jaber, Jiji Zhang, and Elias Bareinboim.
Causal identiﬁcation under Markov equivalence:
Completeness results. In Proceedings of the 36th
International Conference on Machine Learning
(ICML), pages 2981–2989, 2019.
11

Nathan Kallus, Aahlad Manas Puli, and Uri Shalit.
Removing hidden confounding by experimental
grounding.
In Advances in Neural Informa-
tion Processing Systems (NeurIPS), pages 10888–
10897, 2018.
JM Mooij, S Magliacane, and T Claassen.
Joint
causal inference from multiple contexts. Journal of
Machine Learning Research, 21(99):1–108, 2020.
J Pearl. Causality: Models, Reasoning and Inference,
volume 113 of Hardcover.
Cambridge University
Press, 2000.
Judea Pearl.
Probabilistic Reasoning in Intelligent
Systems: Networks of Plausible Inference. Morgan
Kaufmann Publishers Inc., 1988.
Jean-Philippe Pellet and Andr´e Elisseeﬀ.
Find-
ing latent causes in causal networks:
An eﬃ-
cient approach based on Markov blankets. In Ad-
vances in Neural Information Processing Systems
(NeurIPS), pages 1249–1256, 2008.
Emilija Perkovic, Johannes Textor, Markus Kalisch,
and Marloes H Maathuis.
Complete graphical
characterization and construction of adjustment
sets in Markov equivalence classes of ancestral
graphs. Journal of Machine Learning Research, 18
(1):8132–8193, 2017.
Thomas Richardson. Markov properties for acyclic
directed mixed graphs.
Scandinavian Journal of
Statistics, 30(1):145–157, 2003. ISSN 03036898.
Thomas Richardson, Peter Spirtes, et al. Ancestral
graph Markov models. The Annals of Statistics, 30
(4):962–1030, 2002.
Andrea Rotnitzky and Ezequiel Smucler.
Eﬃcient
adjustment sets for population average treatment
eﬀect estimation in non-parametric causal graphi-
cal models, 2019.
Andrea Rotnitzky and Ezequiel Smucler. Eﬃcient ad-
justment sets for population average causal treat-
ment eﬀect estimation in graphical models. Jour-
nal of Machine Learning Research,
21(188):1–
86, 2020.
URL http://jmlr.org/papers/v21/
19-1026.html.
Ilya Shpitser and Judea Pearl.
Identiﬁcation of
joint interventional distributions in recursive semi-
Markovian causal models. In In proceedings of the
21st National Conference on Artiﬁcial Intelligence,
pages 1219–1226, 2006a.
Ilya Shpitser and Judea Pearl. Identiﬁcation of condi-
tional interventional distributions. In Proceedings
of the 22nd Conference on Uncertainty in Artiﬁcial
Intelligence (UAI), pages 437–444, 2006b.
Ilya Shpitser, Tyler VanderWeele, and James M
Robins.
On the validity of covariate adjust-
ment for estimating causal eﬀects. arXiv preprint
arXiv:1203.3515, 2012.
Jin Tian and Ilya Shpitser. On the identiﬁcation of
causal eﬀects. Technical report, Cognitive Systems
Laboratory, University of California at Los Ange-
les, 2003.
Soﬁa
Triantaﬁllou
and
Ioannis
Tsamardinos.
Constraint-based causal discovery from multi-
ple interventions over overlapping variable sets.
Journal of Machine Learning Research, 16(66):
2147–2205, 2015.
Ioannis Tsamardinos and Constantin F Aliferis. To-
wards principled feature selection: Relevancy, ﬁl-
ters and wrappers. In International Conference on
Artiﬁcial Intelligence and Statistics (AISTATS).
Citeseer, 2003.
Benito Van der Zander, Maciej Liskiewicz, and Jo-
hannes Textor.
Constructing separators and ad-
justment sets in ancestral graphs. In Proceedings
of the 30th Conference on Uncertainty in Artiﬁcial
Intelligence (UAI), pages 11–24, 2014.
K. Yu, L. Liu, J. Li, and H. Chen. Mining Markov
blankets without causal suﬃciency. IEEE Trans-
actions on Neural Networks and Learning Systems,
29(12):6333–6347, 2018.
doi:
10.1109/TNNLS.
2018.2828982.
K. Yu, L. Liu, and J. Li.
Learning Markov blan-
kets from multiple interventional data sets. IEEE
Transactions on Neural Networks and Learning
Systems, 31(6):2005–2019, 2020.
doi:
10.1109/
TNNLS.2019.2927636.
12

Supplementary Materials for: Causal Markov Boundaries
Eq. number
Analytical Expression
Eq. 5
P(De|Do, Hc
Z ) =
q
Y
j=1
Γ(αj + N o
j )
Γ(αj + N o
j + N e
j )
r
Y
k=1
Γ(αjk + N o
jk + N e
jk)
Γ(αjk + N o
jk)
-
P(De|Do, Hc
Z ) =
q
Y
j=1
Γ(αj)
Γ(αj + N e
j )
r
Y
k=1
Γ(αjk + N e
jk)
Γ(αjk)
Eq. 7
P(Do|Hc
Z ) =
˜q
Y
j=1
Γ(˜αj)
Γ(˜αj + ˜N o
j )
r
Y
k=1
Γ(˜αjk + ˜N o
jk)
Γ(˜αjk)
Eq. 7
P(Do|Hc
Z ) =
˜q
Y
j=1
Γ(˜αj)
Γ(˜αj + ˜N o
j )
r
Y
k=1
Γ(˜αjk + ˜N o
jk)
Γ(˜αjk)
Terms in Eq. 8
P(Y = k|x, Z = j, De, Do, Hc
Z ) =
N o
jk + N e
jk + αjk
N o
j + N e
j + αj
Terms in Eq. 8
P(Y = k|x, Z = j, De, Do, Hc
Z ) =
N e
jk + αjk
N e
j + αj
Table 1: Closed-form solutions for Eq. 4 -8
in the main paper, for multinomial distributions with Dirichlet priors. Subscript jk refers variable Y taking
its k-th conﬁguration, and variable set Z taking its j-th conﬁguration. αjk is the prior for the Dirichlet
distribution. We set αjk = 1 in all experiments. N o
jk, N e
jk corresponds to counts in the data where Y = k
and Z = j in Do and De, respectively. N o
j , N e
j corresponds to counts in the data where Z = j. Tilde
notation corresponds to the OMB U.
X
A
B
Y
Figure S1: An example where a CMB does not necessarily correspond to an optimal adjustment set [Henckel
et al., 2019]. CMBX(Y ) = {{X, A, B}}, but the optimal adjustment set depends on the parameters.
8
Proofs
In this section, we provide a proof that every causal Markov boundary is backdoor set, which is deﬁned
below (Deﬁnition 8.1). We make the following assumptions throughout the entire document:
• X causes Y
• all variables V are pre-treatment.
Deﬁnition 8.1 (Backdoor Set). Z is a backdoor set for X, Y if and only if Z m-separates X and Y in GX.
1

X
A
B
M
Y
Figure S1: The m-bias graph used to simulate data for Fig 5. A and B are unobserved. All variables were
binary. Parameters were as follows: P(A = 1) = .8, P(B = 1) = .8, P(M = 1|A = 1, B = 1) = α, P(X =
1|A = 1) = α, P(Y = 1|X = 1, B = 1) = α. All other parameters P(Y = 1| . . . ), P(M = 1| . . . ), P(X =
1|A = 0) were set to zero.
We use the following deﬁnitions from [Shpitser and Pearl, 2006a]:
Deﬁnition 8.2 (C-component). A C-component is as set of nodes S in G where every two nodes are connected
by a bidirected path.
Deﬁnition 8.3 (C-forest). A graph G where the set of all of its nodes is a C-component, and each node has
at most one child is a C-forest. The set of nodes R without children in the C-forest is called the root, and
we say that G is an R-rooted C-forest.
C-forests are useful for deﬁning hedges:
Deﬁnition 8.4 (hedge). Let X,Y be sets of variables in G. Let F, F ′ be R-rooted C-forests in G such that
F ′ is a subgraph of F, X only occurs in F, and R ∈An(Y)GX. Then F, F ′ form a hedge for P(Y|do(X)).
The existence of a hedge for P(Y|do(X)) in G is equivalent to the non-identiﬁability of P(Y|do(X)) (see
Theorem 4 in [Shpitser and Pearl, 2006a]).
Lemma 8.5. Let Z be a set that is not a subset of any backdoor set (i.e., there exists no set Q ⊆(V \ Z)
such that Q ∪Z m-separate X and Y in GX). Then there exists in G a bi-directed path from X to Y where
every collider has a descendant in Z ∪Y .
Proof. The proof is a special case of Theorem 4.2 (iv) ⇒(ii) in [Richardson et al., 2002] with S ←Z, L ←
∅, G ←GX. The proof is for ancestral graphs, but it is straightforward to show that it holds for SMCMs,
given that every SMCM G can be transformed to a maximal ancestral graph M over the same nodes (by
adding some edges) such that (a) G and M entail the exact same m-separations and m-connections and (b)
the exact same ancestral relationships hold in both graphs. The theorem proves that if ∀Q ⊆(V \ Z), Z ∪Q
do not m-separate X and Y in GX, then there exists a bidirected path between X and Y in GX where every
variable is an ancestor of some variables in Z∪{X, Y }, which means that there exists a path in G a bi-directed
path from X to Y where every collider has a descendant in Z ∪Y (since X →Y by assumption).
Lemma 8.6. Let Z be a set for which P(Y |do(X), Z) is identiﬁable from P(Y |X, Z), then Z is a subset of
a backdoor set.
Proof. First, notice that P(Y |do(X), Z) = P (Y,Z|do(X))
P (Z|do(X))
= P (Y,Z|do(X))
P (Z)
. Therefore P(Y |do(X), Z) is only
identiﬁable if P(Y, Z|do(X)) is identiﬁable.
If Z is not a subset of a backdoor set, then there exists a
bidirected path where every variable has a descendant in Z ∪Y in G by Lemma 8.5. Let F be the graph
consisting of the bidirected path, and F’ be the same graph without X. Then F, F’ are {Y, Z} rooted
C-forests, and {Y, Z} ∈An({Y, Z}), so F, F’ form a hedge for {Y, Z}. Therefore, P(Y, Z|do(X)) is not
identiﬁable, and P(Y |do(X), Z) is not identiﬁable.
2

Theorem 3.3. We assume that Px and GX are faithful to each other. If Z is a causal Markov boundary for
Y relative to X, then W = Z \ X is a backdoor set.
Proof. Assume Z is a causal Markov boundary, but W is not a backdoor set. Since P(Y |do(X), W) is
identiﬁable, by Lemma 8.6 W is a subset of a backdoor set W ∪Q, where Q ⊆(V \ W).
Since by
assumption W is not a backdoor set, Q is not the empty set (i.e., W is a proper subset of a backdoor set).
We will show that P(Y |do(X), W, Q) ̸= P(Y |do(X), W). To show that, we only need to show that Q is
not independent from W in GX . Since W is not a backdoor set, there exists a backdoor path from X to
Y that is m−connecting given W, but blocked given W ∪Q. Thus, some Q ∈Q is a non-collider on that
path, therefore Q are not independent with Y given W. Hence, P(Y |do(X), W, Q) ̸= P(Y |do(X), W) and
therefore Z does not satisfy Condition (2), and Z is not a causal Markov boundary (Contradiction).
Lemma 8.7. Let Z ⊆V be a backdoor set for X, Y , and let Q ∈(Z \ MB(Y )) that has an m-connecting
path QπQY Y with Y given Z \ Q. Then there exists a variable W ∈(MB(Y ) \ Z) such that: W ∪Z is a
backdoor set and W ̸⊥⊥Y |Z in GX.
Proof. Let Q be a variable as described above. Then there exists a variable W ∈MB(Y ) between Q and
Y that is a non-collider on π, otherwise Q ∈Pa(Dis(Y )), and therefore Q ∈MB(Y ). In addition, W ̸∈Z,
otherwise QπQY Y would be blocked given Z \ Q. We will now show, by contradiction, that adding W to the
conditioning set Z does not open any backdoor paths from X to Y ; hence, Z ∪W is a backdoor set.
Assume that conditioning on W opens a path πXY between X and Y that is blocked given just Z. Then
W must be a descendant of one or more colliders on that path. Let C be the collider closest to X on πXY
such that C is blocked on πXY given Z, but open given Z ∪W. Then XπXCC is open given Z, and W is
a descendant of C. Let CπCW W be the (possibly empty) directed path from C to W, and let WπW Y Y be
the subpath of πCY from W to Y . Since C is blocked on πXY given Z, no variable on πCW can be in Z.
But then XπXCCπCW WπW Y Y is an open path from X and Y given Z in GX. Contradiction, since Z is a
backdoor set. Thus, W does not open any backdoor paths, and Z ∪W is also a backdoor set.
Finally, W is not independent of Y given Z in GX, since WπW Y Y is open given Z.
Theorem 3.4. We assume that Px and GX are faithful to each other. Every causal Markov boundary Z of
an outcome variable Y w.r.t a treatment variable X is a subset of the Markov boundary MB(Y ).
Proof. We will show this by contradiction. Speciﬁcally, we will show that any set Z that includes variables
Q not in the Markov boundary of Y cannot satisfy one of the Conditions (2) or (3) of the causal Markov
boundary.
Assume that Z is a causal Markov boundary for Y with respect to X.
and let W = Z \ X.
Let
Q = W \ MB(Y ) be the non-empty subset of W that is not a part of the Markov boundary of Y .
If there exists no Q ∈Q that has an m-connecting path QπQY Y to Y given W \ Q, then Q ⊥⊥Y |(W \ Q)
in GX. Conditioning on X cannot open any paths from X to Y ; therefore, Q ⊥⊥Y |X, (W \ Q) in GX. Then
by Rule 1 of the do-calculus [Pearl, 2000], P(Y |do(X), W) = P(Y |do(X), W \ Q), and Z does not satisfy
Condition (3) of the causal Markov boundary deﬁnition (Contradiction).
If there exists a Q ∈(W \ MB(Y )) that has an m-connecting path QπQY Y with Y given Z \ Q, then by
Lemma 8.7, there exists a variable W in MB(Y )\Z such that Z∪W is also a backdoor set, and W ̸⊥⊥Y |X, Z
in GX. Then P(Y |do(X), Z, W) ̸= P(Y |do(X), Z). Thus, Z does not satisfy Condition (2) of the Causal
Markov boundary deﬁnition (Contradiction).
Thus, Z cannot include any variables that are not in the Markov boundary of Y .
3

Theorem 3.5. Let G be a SMCM over X, Y ,V with V occurring before X and Y . Let Z ⊆V ∪X be the
IMB of Y relative to X. If Z is a causal Markov boundary, then MB(Y ) = Z.
Proof. MBX(Y ) ⊆MB(Y ) , so we need to show that MB(Y ) ⊆MBX(Y ) when MBX(Y ) ∈CMBX(Y ) .
Assume that Z is both the MBX(Y ) and a causal Markov boundary, but there exists a variable Q in Z that
is not in MB(Y ) . Then Q is reachable from Y through a bidirected path in G but not in GX. Since G
and GX only diﬀer in edges that are into X, this path must be going through an edge that is incoming into
X. Thus, G includes a bidirected path Y ↔· · · ↔X, and every variable on this path is in MBX(Y ) =Z.
But then Z \ X cannot be a backdoor set, and by Theorem 3.3 Z cannot be a causal Markov boundary.
Contradiction. Thus, the Markov boundary of Y cannot include any more variables than Z.
9
Convergence Proof for Observational Markov Boundary (OMB)
Deﬁnition 9.1 (Conditional Entropy). Let P be the full joint probability distribution over a set of variables
V, let Y ∈V be a variable, and let Z ⊆V \ {Y } be a set of variables. Then, the conditional entropy of Y
given Z is deﬁned as follows [Cover, 1999]:
H(Y |Z) = −
X
y
X
z
P(y, z) · log P(y|z)
(S1)
where y and z denote the values of Y and Z, respectively.
Lemma 9.2. Let X, Y ∈V be two variables and Z ⊆V \ {X, Y } be a set of variables. Then, H(Y |Z) ≥
H(Y |X, Z), where the entropies are deﬁned by Deﬁnition 9.1, and the equality holds if and only if Y ⊥⊥X|Z.
Proof. Applying the chain rule of entropy, the conditional mutual information can be computed as fol-
lows [Cover, 1999]:
I(X; Y |Z) = H(Y |Z) −H(Y |X, Z) .
(S2)
Given that the mutual information is nonnegative (i.e., I(X; Y |Z) ≥0) and I(X; Y |Z) = 0 if and only if
Y ⊥⊥X|Z (see [Cover, 1999], page 29), it follows that:
H(Y |Z) −H(Y |X, Z) ≥0
H(Y |Z) ≥H(Y |X, Z) ,
(S3)
where the equality holds if and only if Y ⊥⊥X|Z.
For brevity, let V = {V ∪X}, where X is a treatment variable, and let Y be an outcome variable in the
remainder of this section.
Lemma 9.3. All Markov blankets of Y have the same entropy.
Proof. By deﬁnition, Z′ is the Markov blanket of Y if and only if P(Y |Z′, W) = P(Y |Z′) for any W ⊆V\Z′,
which indicates that Y ⊥⊥W|Z′. Also, according to Lemma 9.2, H(Y |Z′) = H(Y |Z′, W) for any W ⊆V\Z′.
Let Z also be a Markov blanket of Y . By multiple applications of Lemma 9.2, we obtain:
H(Y |Z′) = H(Y |Z′, V \ Z′) = H(Y |V) = H(Y |Z, V \ Z′) = H(Y |Z)
(S4)
4

Lemma 9.4. Let Z′ be a Markov blanket of Y and let Z be a set of variables that is not a Markov blanket
of Y . Then, H(Y |Z′) < H(Y |Z), where the entropies are deﬁned by Deﬁnition 9.1.
Proof. Assume there is exists a set W ⊆V \ Z such that P(Y |Z, W) ̸= P(Y |Z). According to Lemma 9.2
we have:
H(Y |Z, W) < H(Y |Z).
(S5)
Also, given that V is a superset of (Z ∪W), we have:
H(Y |V) ≤H(Y |Z, W).
(S6)
Therefore,
H(Y |V) < H(Y |Z).
(S7)
Also, since Z′ is a Markov blanket of Y , by Lemma 9.3 we have:
H(Y |Z′) = H(Y |V).
(S8)
Combining Equations (S7) and (S8), we obtain:
H(Y |Z′) < H(Y |Z).
(S9)
Lemma 9.5. Given dataset Do that contains samples from a strictly positive distribution P, which is a
perfect map for a SMCM G, the BD score [Heckerman et al., 1995] for log P(Do|Z) is deﬁned as follows in
the large sample limit:
lim
N→∞log P(Do|Z) = lim
N→∞−N · H(Y |Z) −q · (r −1)
2
log N + const.,
(S10)
Proof. The BD score for P(Do|Z) is calculated as follows [Heckerman et al., 1995]:
P(Do|Z) =
q
Y
j=1
Γ(αj)
Γ(αj + Nj) ·
r
Y
k=1
Γ(αjk + Njk)
Γ(αjk)
,
(S11)
where q denotes instantiations of variables in Z and r denotes values of variable Y . The term Njk is the
number of cases in data in which variable Y = k and its parent Z = j; also, Nj = Pr
k=1 Njk. The term αjk is
a ﬁnite positive real number that is called Dirichlet prior parameter and may be interpreted as representing
“pseudo-counts”, where αj = Pr
k=1 αjk. BD can be re-written in log form as follows:
log P(Do|Z) =
q
X
j=1
"
log Γ(αj) −log Γ(αj + Nj) +
r
X
k=1
[log Γ(αjk + Njk) −log Γ(αjk)]
#
.
(S12)
We can re-arrange the terms in Eq. (S12) to gather the constant terms as follows:
log P(Do|Z) =
q
X
j=1
"
−log Γ(αj + Nj) +
r
X
k=1
log Γ(αjk + Njk)
#
+
q
X
j=1
"
log Γ(αj) −
r
X
k=1
log Γ(αjk)
#
=
q
X
j=1
"
−log Γ(αj + Nj) +
r
X
k=1
log Γ(αjk + Njk)
#
+ const.
(S13)
5

Using the Stirling’s approximation of limn→∞log Γ(n) = (n −1
2) log(n) −n + const., we can re-write Eq.
(S13) as follows:
lim
N→∞log P(Do|Z)
= lim
N→∞
q
X
j=1
"
−(αj + Nj −1
2) log(αj + Nj) + (αj + Nj) +
r
X
k=1

(αjk + Njk −1
2) log(αjk + Njk) −(αjk + Njk)
#
+const.
= lim
N→∞
q
X
j=1
"
−αj log(αj + Nj) −Nj log(αj + Nj) + 1
2 log(αj + Nj) + αj + Nj
+
r
X
k=1

αjk log(αjk + Njk) + Njk log(αjk + Njk) −1
2 log(αjk + Njk) −αjk −Njk
#
+const.
= lim
N→∞
q
X
j=1
"
−Nj log(αj + Nj) +
r
X
k=1
Njk log(αjk + Njk)
#
+
q
X
j=1
"
−αj log(αj + Nj) +
r
X
k=1
αjk log(αjk + Njk)
#
+ 1
2
q
X
j=1
"
log(αj + Nj) −
r
X
k=1
log(αjk + Njk) + αj + Nj −
r
X
k=1
(αjk + Njk)
#
+const.
= lim
N→∞
q
X
j=1
"
−Nj log(αj + Nj) +
r
X
k=1
Njk log(αjk + Njk)
#
+
q
X
j=1
"
−αj log(αj + Nj) +
r
X
k=1
αjk log(αjk + Njk)
#
+ 1
2
q
X
j=1
"
log(αj + Nj) −
r
X
k=1
log(αjk + Njk)
#
+const.
(S14)
In the last step of Eq. (S14), we used the facts that Pr
k=1 Njk = Nj and Pr
k=1 αjk = αj, and we applied
these identities again to that equation to obtain the following:
lim
N→∞log P(Do|Z) =
lim
N→∞
q
X
j=1
r
X
k=1
"
Njk log(αjk + Njk
αj + Nj
) + αjk log(αjk + Njk
αj + Nj
)
#
+1
2
q
X
j=1
"
log(αj + Nj) −
r
X
k=1
log(αjk + Njk)
#
+const.
(S15)
Given that
lim
N→∞
αjk + Njk
αj + Nj
= Njk
Nj
and
lim
N→∞
q
X
j=1
r
X
k=1
αjk log(αjk + Njk
αj + Nj
) = const.,
in the limit, Eq. (S15) becomes:
lim
N→∞log P(Do|Z) = lim
N→∞
q
X
j=1
r
X
k=1
Njk log Njk
Nj
+ 1
2
q
X
j=1
"
log(αj + Nj) −
r
X
k=1
log(αjk + Njk)
#
+const., (S16)
6

or equivalently:
lim
N→∞log P(Do|Z) = lim
N→∞N ·
q
X
j=1
r
X
k=1
Njk
N log Njk
Nj
+ 1
2
q
X
j=1
"
log(αj + Nj) −
r
X
k=1
log(αjk + Njk)
#
+const.
= lim
N→∞−N · H(Y |Z) + 1
2
q
X
j=1
"
log(αj + Nj) −
r
X
k=1
log(αjk + Njk)
#
+const.
(S17)
To simplify the second term in Eq. (S17), we divide the arguments in the log terms by N and equivalently
add log N terms as follows:
lim
N→∞
1
2
q
X
j=1
"
log(αj + Nj) −
r
X
k=1
log(αjk + Njk)
#
= lim
N→∞
1
2
q
X
j=1
"
log(αj + Nj
N
) + log N −
r
X
k=1
log(αjk + Njk
N
) + log N
#
= lim
N→∞
1
2
q
X
j=1
 
log N −
r
X
k=1
log N
!
+ 1
2
q
X
j=1
"
log(αj + Nj
N
) −
r
X
k=1
log(αjk + Njk
N
)
#
= −q(r −1)
2
log N + const.
(S18)
Combining Equations (S17) and (S18), we obtain:
lim
N→∞log P(Do|Z) = lim
N→∞−N · H(Y |Z′) −q · (r −1)
2
log N + const.
(S19)
Theorem 4.1. Given dataset Do that contains samples from a strictly positive distribution P, which is a
perfect map for a SMCM G, the BD score [Heckerman et al., 1995] will assign the highest score to the OMB
of Y in the large sample limit.
Proof. Let Z′ be the OMB of Y and Z ⊆V be an arbitrary set. We want to show that:
lim
N→∞
P(Do|Z)
P(Do|Z′) =
(
1
iﬀZ is an OMB of Y
0
otherwise
,
(S20)
Applying Lemma 9.5 we have:
lim
N→∞log P(Do|Z)
P(Do|Z′) = lim
N→∞N · [H(Y |Z′) −H(Y |Z)] + (q′ −q) · (r −1)
2
log N.
(S21)
where q and q′ are the number of possible parent instantiations of Y with Z and Z′ as the set of parents.
There are three possible cases:
Case 1: Z is a Markov blanket of Y and its OMB.
Since both Z′ and Z are Markov blankets of Y , H(Y |Z) = H(Y |Z′) by Lemma 9.3. Thus, the ﬁrst term
in Eq. (S21) becomes 0. Also, given that Z′ and Z are OMBs, they have the same number of parameters
7

q′ = q, by which the second term in Eq. (S21) becomes 0 in the limit as N →∞, or equivalently Eq. (S20)
approaches to 1.
Case 2: Z is a Markov blanket of Y but not its OMB.
According to Lemma 9.3 H(Y |Z) = H(Y |Z′); therefore, the ﬁrst term in Eq. (S21) becomes 0 and we
obtain:
lim
N→∞
P(Do|Z)
P(Do|Z′) = lim
N→∞
(q′ −q) · (r −1)
2
log N.
(S22)
Given that Z′ is the OMB with minimum number of variables, and therefore, minimum number of
parameters q′ < q. Thus, the term (q′ −q) becomes a negative constant. Also, the term (r−1)
2
is a positive
constant. Consequently, Eq. (S22) goes to −∞in the limit as N →∞, which implies that Eq. (S20)
approaches to 0.
Case 3: Z is not a Markov blanket of Y .
The ﬁrst term in Eq. (S21) is of O(N) and dominates the second term, which is O(log N). According
to Lemma 9.4, H(Y |Z′) < H(Y |Z); thus, the term H(Y |Z′) −H(Y |Z) becomes a negative number. As a
result, Eq. (S21) becomes −∞, which equivalently implies that Eq. (S20) becomes 0.
8

