

INFORMATION THEORY AND CODING BY EXAMPLE
This fundamental monograph introduces both the probabilistic and the algebraic
aspects of information theory and coding. It has evolved from the authorsâ€™ years
of experience teaching at the undergraduate level, including several Cambridge
Mathematical Tripos courses. The book provides relevant background material, a
wide range of worked examples and clear solutions to problems from real exam
papers. It is a valuable teaching aid for undergraduate and graduate students, or for
researchers and engineers who want to grasp the basic principles.
Mark Kelbert is a Reader in Statistics in the Department of Mathematics at
Swansea University. For many years he has also been associated with the Moscow
Institute of Information Transmission Problems and the International Institute of
Earthquake Prediction Theory and Mathematical Geophysics (Moscow).
Yuri Suhov is a Professor of Applied Probability in the Department of Pure Math-
ematics and Mathematical Statistics at the University of Cambridge (Emeritus). He
is also afï¬liated to the University of SËœao Paulo in Brazil and to the Moscow Institute
of Information Transmission Problems.


INFORMATION THEORY
AND CODING BY EXAMPLE
MARK KELBERT
Swansea University, and Universidade de SËœao Paulo
YURI SUHOV
University of Cambridge, and Universidade de SËœao Paulo

University Printing House, Cambridge CB2 8BS, United Kingdom
Published in the United States of America by Cambridge University Press, New York
Cambridge University Press is part of the University of Cambridge.
It furthers the Universityâ€™s mission by disseminating knowledge in the pursuit of
education, learning and research at the highest international levels of excellence.
www.cambridge.org
Information on this title: www.cambridge.org/9780521769358
câƒCambridge University Press 2013
This publication is in copyright. Subject to statutory exception
and to the provisions of relevant collective licensing agreements,
no reproduction of any part may take place without the written
permission of Cambridge University Press.
First published 2013
Printed in the United Kingdom by CPI Group Ltd. Croydon cr0 4yy
A catalogue record for this publication is available from the British Library
ISBN 978-0-521-76935-8 Hardback
ISBN 978-0-521-13988-5 Paperback
Cambridge University Press has no responsibility for the persistence or
accuracy of URLs for external or third-party internet websites referred to
in this publication, and does not guarantee that any content on such
websites is, or will remain, accurate or appropriate.

Contents
Preface
page vii
1
Essentials of Information Theory
1
1.1
Basic concepts. The Kraft inequality. Huffmanâ€™s encoding
1
1.2
Entropy: an introduction
18
1.3
Shannonâ€™s ï¬rst coding theorem. The entropy rate of a
Markov source
41
1.4
Channels of information transmission. Decoding rules.
Shannonâ€™s second coding theorem
59
1.5
Differential entropy and its properties
86
1.6
Additional problems for Chapter 1
95
2
Introduction to Coding Theory
144
2.1
Hamming spaces. Geometry of codes. Basic bounds on the
code size
144
2.2
A geometric proof of Shannonâ€™s second coding theorem.
Advanced bounds on the code size
162
2.3
Linear codes: basic constructions
184
2.4
The Hamming, Golay and Reedâ€“Muller codes
199
2.5
Cyclic codes and polynomial algebra. Introduction to BCH
codes
213
2.6
Additional problems for Chapter 2
243
3
Further Topics from Coding Theory
269
3.1
A primer on ï¬nite ï¬elds
269
3.2
Reedâ€“Solomon codes. The BCH codes revisited
291
3.3
Cyclic codes revisited. Decoding the BHC codes
300
3.4
The MacWilliams identity and the linear programming bound
313
3.5
Asymptotically good codes
328
3.6
Additional problems for Chapter 3
340
v

vi
Contents
4
Further Topics from Information Theory
366
4.1
Gaussian channels and beyond
366
4.2
The asymptotic equipartition property in the continuous time
setting
397
4.3
The Nyquistâ€“Shannon formula
409
4.4
Spatial point processes and network information theory
436
4.5
Selected examples and problems from cryptography
453
4.6
Additional problems for Chapter 4
480
Bibliography
501
Index
509

Preface
This book is partially based on the material covered in several Cambridge Math-
ematical Tripos courses: the third-year undergraduate courses Information The-
ory (which existed and evolved over the last four decades under slightly varied
titles) and Coding and Cryptography (a much younger and simpliï¬ed course avoid-
ing cumbersome technicalities), and a number of more advanced Part III courses
(Part III is a Cambridge equivalent to an MSc in Mathematics). The presentation
revolves, essentially, around the following core concepts: (a) the entropy of a prob-
ability distribution as a measure of â€˜uncertaintyâ€™ (and the entropy rate of a random
process as a measure of â€˜variabilityâ€™ of its sample trajectories), and (b) coding as a
means to measure and use redundancy in information generated by the process.
Thus, the contents of this book includes a more or less standard package of
information-theoretical material which can be found nowadays in courses taught
across the world, mainly at Computer Science and Electrical Engineering Depart-
ments and sometimes at Probability and/or Statistics Departments. What makes this
book different is, ï¬rst of all, a wide range of examples (a pattern that we followed
from the onset of the series of textbooks Probability and Statistics by Example
by the present authors, published by Cambridge University Press). Most of these
examples are of a particular level adopted in Cambridge Mathematical Tripos ex-
ams. Therefore, our readers can make their own judgement about what level they
have reached or want to reach.
The second difference between this book and the majority of other books
on information theory or coding theory is that it covers both possible direc-
tions: probabilistic and algebraic. Typically, these lines of inquiry are presented
in different monographs, textbooks and courses, often by people who work in
different departments. It helped that the present authors had a long-time associ-
ation with the Institute for Information Transmission Problems, a section of the
Russian Academy of Sciences, Moscow, where the tradition of embracing a broad
spectrum of problems was strongly encouraged. It sufï¬ces to list, among others,
vii

viii
Preface
the names of Roland Dobrushin, Raphail Khasâ€™minsky, Mark Pinsker, Vladimir
Blinovsky, Vyacheslav Prelov, Boris Tsybakov, Kamil Zigangirov (probability and
statistics), Valentin Afanasiev, Leonid Bassalygo, Serguei Gelfand, Valery Goppa,
Inna Grushko, Grigorii Kabatyansky, Grigorii Margulis, Yuri Sagalovich, Alexei
Skorobogatov, Mikhail Tsfasman, Victor Zinovâ€™yev, Victor Zyablov (algebra, com-
binatorics, geometry, number theory), who worked or continue to work there (at
one time, all these were placed in a ï¬ve-room ï¬‚oor of a converted building in the
centre of Moscow). Importantly, the Cambridge mathematical tradition of teaching
information-theoretical and coding-theoretical topics was developed along simi-
lar lines, initially by Peter Whittle (Probability and Optimisation) and later on by
Charles Goldie (Probability), Richard Pinch (Algebra and Geometry), Tom KÂ¨orner
and Keith Carne (Analysis) and Tom Fisher (Number Theory).
We also would like to add that this book has been written by authors trained as
mathematicians (and who remain still mathematicians to their bones), who never-
theless have a strong background in applications, with all the frustration that comes
with such work: vagueness, imprecision, disputability (involving, inevitably, per-
sonal factors) and last â€“ but by no means least â€“ the costs of putting any math-
ematical idea â€“ however beautiful â€“ into practice. Still, they ï¬rmly believe that
mathematisation is the mainstream road to survival and perfection in the modern
competitive world, and therefore that Mathematics should be taken and studied
seriously (but perhaps not beyond reason).
Both aforementioned concepts (entropy and codes) forming the base of
the information-theoretical approach to random processes were introduced by
Shannon in the 1940s, in a rather accomplished form, in his publications [139],
[141]. Of course, entropy already existed in thermodynamics and was understood
pretty well by Boltzmann and Gibbs more than a century ago, and codes have
been in practical (and efï¬cient) use for a very long time. But it was Shannon who
fully recognised the role of these concepts and put them into a modern mathemati-
cal framework, although, not having the training of a professional mathematician,
he did not always provide complete proofs of his constructions. [Maybe he did
not bother.] In relevant sections we comment on some rather bizarre moments in
the development of Shannonâ€™s relations with the mathematical community. Fortu-
nately, it seems that this did not bother him much. [Unlike Boltzmann, who was
particularly sensitive to outside comments and took them perhaps too close to his
heart.] Shannon deï¬nitely understood the full value of his discoveries; in our view
it puts him on equal footing with such towering ï¬gures in mathematics as Wiener
and von Neumann.
It is fair to say that Shannonâ€™s name still dominates both the probabilistic and the
algebraic direction in contemporary information and coding theory. This is quite
extraordinary, given that we are talking of the contribution made by a person who

Preface
ix
was active in this area more than 40 years ago. [Although on several advanced
topics Shannon, probably, could have thought, re-phrasing Einsteinâ€™s words: â€œSince
mathematicians have invaded the theory of communication, I do not understand it
myself anymore.â€]
During the years that passed after Shannonâ€™s inceptions and inventions, math-
ematics changed drastically, and so did electrical engineering, let alone computer
science. Who could have foreseen such a development back in the 1940s and 1950s,
as the great rivalry between Shannonâ€™s information-theoretical and Wienerâ€™s cyber-
netical approaches was emerging? In fact, the latter promised huge (even fantastic)
beneï¬ts for the whole of humanity while the former only asserted that a mod-
est goal of correcting transmission errors could be achieved within certain limits.
Wienerâ€™s book [171] captivated the minds of 1950s and 1960s thinkers in practi-
cally all domains of intellectual activity. In particular, cybernetics became a serious
political issue in the Soviet Union and its satellite countries: ï¬rst it was declared
â€œa bourgeois anti-scientiï¬c theoryâ€, then it was over-enthusiastically embraced. [A
quotation from a 1953 critical review of cybernetics in a leading Soviet ideology
journal Problems of Philosophy reads: â€œImperialists are unable to resolve the con-
troversies destroying the capitalist society. They canâ€™t prevent the imminent eco-
nomical crisis. And so they try to ï¬nd a solution not only in the frenzied arms race
but also in ideological warfare. In their profound despair they resort to the help of
pseudo-sciences that give them some glimmer of hope to prolong their survival.â€
The 1954 edition of the Soviet Concise Dictionary of Philosophy printed in hun-
dreds of thousands of copies deï¬ned cybernetics as a â€œreactionary pseudo-science
which appeared in the USA after World War II and later spread across other cap-
italist countries: a kind of modern mechanicism.â€ However, under pressure from
top Soviet physicists who gained authority after successes of the Soviet nuclear
programme, the same journal, Problems of Philosophy, had to print in 1955 an ar-
ticle proclaiming positive views on cybernetics. The authors of this article included
Alexei Lyapunov and Sergei Sobolev, prominent Soviet mathematicians.]
Curiously, as was discovered in a recent biography on Wiener [35], there exist
â€œsecret [US] government documents that show how the FBI and the CIA pursued
Wiener at the height of the Cold War to thwart his social activism and the growing
inï¬‚uence of cybernetics at home and abroad.â€ Interesting comparisons can be found
in [65].
However, history went its own way. As Freeman Dyson put it in his review [41]
of [35]: â€œ[Shannonâ€™s theory] was mathematically elegant, clear, and easy to apply
to practical problems of communication. It was far more user-friendly than cyber-
netics. It became the basis of a new discipline called â€˜information theoryâ€™ . . . [In
modern times] electronic engineers learned information theory, the gospel accord-
ing to Shannon, as part of their basic training, and cybernetics was forgotten.â€

x
Preface
Not quite forgotten, however: in the former Soviet Union there still exist at
least seven functioning institutes or departments named after cybernetics: two in
Moscow and two in Minsk, and one in each of Tallinn, Tbilisi, Tashkent and Kiev
(the latter being a renowned centre of computer science in the whole of the for-
mer USSR). In the UK there are at least four departments, at the Universities of
Bolton, Bradford, Hull and Reading, not counting various associations and soci-
eties. Across the world, cybernetics-related societies seem to ï¬‚ourish, displaying
an assortment of names, from concise ones such as the Institute of the Method
(Switzerland) or the Cybernetics Academy (Italy) to the Argentinian Associa-
tion of the General Theory of Systems and Cybernetics, Buenos Aires. And we
were delighted to discover the existence of the Cambridge Cybernetics Society
(Belmont, CA, USA). By contrast, information theory ï¬gures only in a handful of
institutionsâ€™ names. Apparently, the old Shannon vs. Wiener dispute may not be
over yet.
In any case, Wienerâ€™s personal reputation in mathematics remains rock solid:
it sufï¬ces to name a few gems such as the Paleyâ€“Wiener theorem (created on
Wienerâ€™s numerous visits to Cambridge), the Wienerâ€“Hopf method and, of course,
the Wiener process, particularly close to our hearts, to understand his true role in
scientiï¬c research and applications. However, existing recollections of this giant of
science depict an image of a complex and often troubled personality. (The title of
the biography [35] is quite revealing but such views are disputed, e.g., in the review
[107]. In this book we attempt to adopt a more tempered tone from the chapter on
Wiener in [75], pp. 386â€“391.) On the other hand, available accounts of Shannonâ€™s
life (as well as other fathers of information and coding theory, notably, Richard
Hamming) give a consistent picture of a quiet, intelligent and humorous person.
It is our hope that this fact will not present a hindrance for writing Shannonâ€™s
biographies and that in future we will see as many books on Shannon as we see on
Wiener.
As was said before, the purpose of this book is twofold: to provide a synthetic
introduction both to probabilistic and algebraic aspects of the theory supported by
a signiï¬cant number of problems and examples, and to discuss a number of topics
rarely presented in most mainstream books. Chapters 1â€“3 give an introduction into
the basics of information theory and coding with some discussion spilling over to
more modern topics. We concentrate on typical problems and examples [many of
them originated in Cambridge courses] more than on providing a detailed presen-
tation of the theory behind them. Chapter 4 gives a brief introduction into a variety
of topics from information theory. Here the presentation is more concise and some
important results are given without proofs.
Because the large part of the text stemmed from lecture notes and various solu-
tions to class and exam problems, there are inevitable repetitions, multitudes of

Preface
xi
notation and examples of pigeon English. We left many of them deliberately,
feeling that they convey a live atmosphere during the teaching and examination
process.
Two excellent books [52] and [36] had a particularly strong impact on our pre-
sentation. We feel that our long-term friendship with Charles Goldie played a role
here, as well as YSâ€™s amicable acquaintance with Tom Cover. We also beneï¬ted
from reading (and borrowing from) the books [18], [110], [130] and [98]. The
warm hospitality at a number of programmes at the Isaac Newton Institute, Univer-
sity of Cambridge, in 2002â€“2010 should be acknowledged, particularly Stochas-
tic Processes in Communication Sciences (Januaryâ€“July 2010). Various parts of
the material have been discussed with colleagues in various institutions, ï¬rst and
foremost, the Institute for Information Transmission Problems and the Institute of
Mathematical Geophysics and Earthquake Predictions, Moscow (where the authors
have been loyal staff members for a long time). We would like to thank James
Lawrence, from Statslab, University of Cambridge, for his kind help with ï¬gures.
References to PSE I and PSE II mean the books by the present authors Prob-
ability and Statistics by Example, Cambridge University Press, Volumes I and II.
We adopted the style used in PSE II, presenting a large portion of the material
through â€˜Worked Examplesâ€™. Most of these Worked Examples are stated as prob-
lems (and many of them originated from Cambridge Tripos Exam papers and keep
their speciï¬c style and spirit).


1
Essentials of Information Theory
Throughout the book, the symbol P denotes various probability distributions. In
particular, in Chapter 1, P refers to the probabilities for sequences of random
variables characterising sources of information. As a rule, these are sequences of
independent and identically distributed random variables or discrete-time Markov
chains; namely, P(U1 = u1,...,Un = un) is the joint probability that random
variables U1,...,Un take values u1,...,un, and P(V = v|U = u,W = w) is the
conditional probability that a random variable V takes value v, given that ran-
dom variables U and W take values u and w, respectively. Likewise, E denotes the
expectation with respect to P.
The symbols p and P are used to denote various probabilities (and probability-
related objects) loosely. The symbol â™¯A denotes the cardinality of a ï¬nite set A.
The symbol 1 stands for an indicator function. We adopt the following notation and
formal rules for logarithms: ln = loge, log = log2, and for all b > 1: 0Â·logb 0 = 0Â·
logb âˆ= 0. Next, given x > 0, âŒŠxâŒ‹and âŒˆxâŒ‰denote the maximal integer that is no
larger than x and the minimal integer that is no less than x, respectively. Thus,
âŒŠxâŒ‹â‰¤x â‰¤âŒˆxâŒ‰; equalities hold here when x is a positive integer (âŒŠxâŒ‹is called the
integer part of x.)
The abbreviations LHS and RHS stand, respectively, for the left-hand side and
the right-hand side of an equation.
1.1 Basic concepts. The Kraft inequality. Huffmanâ€™s encoding
A typical scheme used in information transmission is as follows:
A message source â†’an encoder â†’a channel
â†’a decoder â†’a destination
1

2
Essentials of Information Theory
Example 1.1.1
(a) A message source: a Cambridge college choir.
(b) An encoder: a BBC recording unit. It translates the sound to a binary array and
writes it to a CD track. The CD is then produced and put on the market.
(c) A channel: a customer buying a CD in England and mailing it to Australia. The
channel is subject to â€˜noiseâ€™: possible damage (mechanical, electrical, chemical,
etc.) incurred during transmission (transportation).
(d) A decoder: a CD player in Australia.
(e) A destination: an audience in Australia.
(f) The goal: to ensure a high-quality sound despite damage.
In fact, a CD can sustain damage done by a needle while making a neat hole in
it, or by a tiny drop of acid (you are not encouraged to make such an experiment!).
In technical terms, typical goals of information transmission are:
(i) fast encoding of information,
(ii) easy transmission of encoded messages,
(iii) effective use of the channel available (i.e. maximum transfer of information
per unit time),
(iv) fast decoding,
(v) correcting errors (as many as possible) introduced by noise in the channel.
As usual, these goals contradict each other, and one has to ï¬nd an optimal solu-
tion. This is what the chapter is about. However, do not expect perfect solutions:
the theory that follows aims mainly at providing knowledge of the basic principles.
A ï¬nal decision is always up to the individual (or group) responsible.
A large part of this section (and the whole of Chapter 1) will deal with encoding
problems. The aims of encoding are:
(1) compressing data to reduce redundant information contained in a message,
(2) protecting the text from unauthorised users,
(3) enabling errors to be corrected.
We start by studying sources and encoders. A source emits a sequence of letters
(or symbols),
u1 u2 ... un ...,
(1.1.1)
where u j âˆˆI, and I(= Im) is an m-element set often identiï¬ed as {1,...,m}
(a source alphabet). In the case of literary English, m = 26 + 7, 26 letters plus
7 punctuation symbols: . , : ; â€“ ( ). (Sometimes one adds ? ! â€˜ â€™ and â€). Telegraph
English corresponds to m = 27.
A common approach is to consider (1.1.1) as a sample from a random source,
i.e. a sequence of random variables
U1,U2,...,Un,...
(1.1.2)
and try to develop a theory for a reasonable class of such sequences.

1.1 Basic concepts. The Kraft inequality. Huffmanâ€™s encoding
3
Example 1.1.2
(a) The simplest example of a random source is a sequence of
independent and identically distributed random variables (IID random variables):
P(U1 = u1, U2 = u2,...,Uk = uk) =
k
âˆ
j=1
p(u j),
(1.1.3a)
where p(u) = P(Uj = u), u âˆˆI, is the marginal distribution of a single variable. A
random source with IID symbols is often called a Bernoulli source.
A particular case where p(u) does not depend on u âˆˆU (and hence equals 1/m)
corresponds to the equiprobable Bernoulli source.
(b) A more general example is a Markov source where the symbols form a discrete-
time Markov chain (DTMC):
P(U1 = u1, U2 = u2,..., Uk = uk) = Î»(u1)
kâˆ’1
âˆ
j=1
P(u j,u j+1),
(1.1.3b)
where Î»(u) = P(U1 = u), u âˆˆI, are the initial probabilities and P(u,uâ€²) = P(Uj+1 =
uâ€²|Uj = u), u,uâ€² âˆˆI, are transition probabilities. A Markov source is called sta-
tionary if P(Uj = u) = Î»(u), j â‰¥1, i.e. Î» = {Î»(u),u = 1,...,m} is an invariant
row-vector for matrix P = {P(u,v)}: âˆ‘
uâˆˆI
Î»(u)P(u,v) = Î»(v), v âˆˆI, or, shortly,
Î»P = Î».
(c) A â€˜degeneratedâ€™ example of a Markov source is where a source emits repeated
symbols. Here,
P(U1 = U2 = Â·Â·Â· = Uk = u) = p(u), u âˆˆI,
P(Uk Ì¸= Ukâ€²) = 0, 1 â‰¤k < kâ€²,
(1.1.3c)
where 0 â‰¤p(u) â‰¤1 and âˆ‘
uâˆˆI
p(u) = 1.
An initial piece of sequence (1.1.1)
u(n) = (u1,u2,...,un) or, more brieï¬‚y, u(n) = u1u2 ...un
is called a (source) sample n-string, or n-word (in short, a string or a word), with
digits from I, and is treated as a â€˜messageâ€™. Correspondingly, one considers a ran-
dom n-string (a random message)
U(n) = (U1,U2,...,Un) or, brieï¬‚y, U(n) = U1U2 ...Un.
An encoder (or coder) uses an alphabet J(= Jq) which we typically write as
{0,1,...,qâˆ’1}; usually the number of encoding symbols q < m (or even q â‰ªm);
in many cases q = 2 with J = {0,1} (a binary coder). A code (also coding, or

4
Essentials of Information Theory
encoding) is a map, f, that takes a symbol u âˆˆI into a ï¬nite string, f(u) = x1 ...xs,
with digits from J. In other words, f maps I into the set Jâˆ—of all possible strings:
f : I â†’Jâˆ—=

sâ‰¥1

J Ã—Â·Â·Â· (s times) Ã—J

.
Strings f(u) that are images, under f, of symbols u âˆˆI are called codewords
(in code f). A code has (constant) length N if the value s (the length of a code-
word) equals N for all codewords. A message u(n) = u1u2 ...un is represented as a
concatenation of codewords
f(u(n)) = f(u1) f(u2)... f(un);
it is again a string from Jâˆ—.
Deï¬nition 1.1.3
We say that a code is lossless if u Ì¸= uâ€² implies that f(u) Ì¸= f(uâ€²).
(That is, the map f : I â†’Jâˆ—is one-to-one.) A code is called decipherable if any
string from Jâˆ—is the image of at most one message. A string x is a preï¬x in another
string y if y = xz, i.e. y may be represented as a result of a concatenation of x and z.
A code is preï¬x-free if no codeword is a preï¬x in any other codeword (e.g. a code
of constant length is preï¬x-free).
A preï¬x-free code is decipherable, but not vice versa:
Example 1.1.4
A code with three source letters 1,2,3 and the binary encoder
alphabet J = {0,1} given by
f(1) = 0, f(2) = 01, f(3) = 011
is decipherable, but not preï¬x-free.
Theorem 1.1.5
(The Kraft inequality) Given positive integers s1,...,sm, there
exists a decipherable code f : I â†’Jâˆ—, with codewords of lengths s1,...,sm, iff
m
âˆ‘
i=1
qâˆ’si â‰¤1.
(1.1.4)
Furthermore, under condition (1.1.4) there exists a preï¬x-free code with codewords
of lengths s1,...,sm.
Proof
(I) Sufï¬ciency. Let (1.1.4) hold. Our goal is to construct a preï¬x-free code
with codewords of lengths s1,...,sm. Rewrite (1.1.4) as
s
âˆ‘
l=1
nlqâˆ’l â‰¤1,
(1.1.5)

1.1 Basic concepts. The Kraft inequality. Huffmanâ€™s encoding
5
or
nsqâˆ’s â‰¤1âˆ’
sâˆ’1
âˆ‘
l=1
nlqâˆ’l,
where nl is the number of codewords of length l and s = maxsi. Equivalently,
ns â‰¤qs âˆ’n1qsâˆ’1 âˆ’Â·Â·Â·âˆ’nsâˆ’1q.
(1.1.6a)
Since ns â‰¥0, deduce that
nsâˆ’1q â‰¤qs âˆ’n1qsâˆ’1 âˆ’Â·Â·Â·âˆ’nsâˆ’2q2,
or
nsâˆ’1 â‰¤qsâˆ’1 âˆ’n1qsâˆ’2 âˆ’Â·Â·Â·âˆ’nsâˆ’2q.
(1.1.6b)
Repeating this argument yields subsequently
nsâˆ’2
â‰¤
qsâˆ’2 âˆ’n1qsâˆ’3
âˆ’
...
âˆ’nsâˆ’3q
...
...
...
n2
â‰¤
q2 âˆ’n1q
(1.1.6.sâˆ’1)
n1 â‰¤q.
(1.1.6.s)
Observe that actually either ni+1 = 0 or ni is less than the RHS of the inequality,
for all i = 1,...,sâˆ’1 (by deï¬nition, ns â‰¥1 so that for i = sâˆ’1 the second possi-
bility occurs). We can perform the following construction. First choose n1 words
of length 1, using distinct symbols from J: this is possible in view of (1.1.6.s).
It leaves (q âˆ’n1) symbols unused; we can form (q âˆ’n1)q words of length 2 by
appending a symbol to each. Choose n2 codewords from these: we can do so in
view of (1.1.6.sâˆ’1). We still have q2 âˆ’n1qâˆ’n2 words unused: form n3 codewords,
etc. In the course of the construction, no new word contains a previous codeword
as a preï¬x. Hence, the code constructed is preï¬x-free.
(II) Necessity. Suppose there exists a decipherable code in Jâˆ—with codeword
lengths s1,...,sm. Set s = maxsi and observe that for any positive integer r

qâˆ’s1 +Â·Â·Â·+qâˆ’smr =
rs
âˆ‘
l=1
blqâˆ’l
where bl is the number of ways r codewords can be put together to form a string of
length l.

6
Essentials of Information Theory
Because of decipherability, these strings must be distinct. Hence, we must have
bl â‰¤ql, as ql is the total number of l-strings. Then

qâˆ’s1 +Â·Â·Â·+qâˆ’smr â‰¤rs,
and
qâˆ’s1 +Â·Â·Â·+qâˆ’sm â‰¤r1/rs1/r = exp
1
r (logr +logs)

.
This is true for any r, so take r â†’âˆ. The RHS goes to 1.
Remark 1.1.6
A given code obeying (1.1.4) is not necessarily decipherable.
Leon G. Kraft introduced inequality (1.1.4) in his MIT PhD thesis in 1949.
One of the principal aims of the theory is to ï¬nd the â€˜bestâ€™ (that is, the shortest)
decipherable (or preï¬x-free) code. We now adopt a probabilistic point of view and
assume that symbol u âˆˆI is emitted by a source with probability p(u):
P(Uk = u) = p(u).
[At this point, there is no need to specify a joint probability of more than one
subsequently emitted symbol.]
Recall, given a code f : I â†’Jâˆ—, we encode a letter i âˆˆI by a prescribed code-
word f(i) = x1 ...xs(i) of length s(i). For a random symbol, the generated codeword
becomes a random string from Jâˆ—. When f is lossless, the probability of generating
a given string as a codeword for a symbol is precisely p(i) if the string coincides
with f(i) and 0 if there is no letter i âˆˆI with this property. If f is not one-to-one,
the probability of a string equals the sum of terms p(i) for which the codeword f(i)
equals this string. Then the length of a codeword becomes a random variable, S,
with the probability distribution
P(S = s) = âˆ‘
1â‰¤iâ‰¤m
1(s(i) = s)p(i).
(1.1.7)
We are looking for a decipherable code that minimises the expected word-length:
ES = âˆ‘
sâ‰¥1
sP(S = s) =
m
âˆ‘
i=1
s(i)p(i).
The following problem therefore arises:
minimise g(s(1),...,s(m)) = ES
subject to âˆ‘
i
qâˆ’s(i) â‰¤1 (Kraft)
with s(i) positive integers.
(1.1.8)

1.1 Basic concepts. The Kraft inequality. Huffmanâ€™s encoding
7
Theorem 1.1.7
The optimal value for problem (1.1.8) is lower-bounded as fol-
lows:
min ES â‰¥hq(p(1),..., p(m)),
(1.1.9)
where
hq(p(1),..., p(m)) = âˆ’âˆ‘
i
p(i)logq p(i).
(1.1.10)
Proof
The algorithm (1.1.8) is an integer-valued optimisation problem. If we drop
the condition that s(1),...,s(m) âˆˆ{1,2,...}, replacing it with a â€˜relaxedâ€™ con-
straint s(i) > 0, 1 â‰¤i â‰¤m, the Lagrange sufï¬ciency theorem could be used. The
Lagrangian reads
L (s(1),...,s(m),z;Î») = âˆ‘
i
s(i)p(i)+Î»(1âˆ’âˆ‘
i
qâˆ’s(i) âˆ’z)
(here, z â‰¥0 is a slack variable). Minimising L in s1,...,sm and z yields
Î» < 0,
z = 0, and âˆ‚L
âˆ‚s(i) = p(i)+qâˆ’s(i)Î» lnq = 0,
whence
âˆ’p(i)
Î» lnq = qâˆ’s(i),
i.e. s(i) = âˆ’logq p(i)+logq(âˆ’Î» lnq), 1 â‰¤i â‰¤m.
Adjusting the constraint âˆ‘
i
qâˆ’s(i) = 1 (the slack variable z = 0) gives
âˆ‘
i
p(i)/(âˆ’Î» lnq) = 1, i.e. âˆ’Î» lnq = 1.
Hence,
s(i) = âˆ’logq p(i),
1 â‰¤i â‰¤m,
is the (unique) optimiser for the relaxed problem, giving the value hq from (1.1.10).
The relaxed problem is solved on a larger set of variables s(i); hence, its minimal
value does not exceed that in the original one.
Remark 1.1.8
The quantity hq deï¬ned in (1.1.10) plays a central role in the
whole of information theory. It is called the q-ary entropy of the probability distri-
bution (p(x), x âˆˆI) and will emerge in a great number of situations. Here we note
that the dependence on q is captured in the formula
hq(p(1),..., p(m)) =
1
logqh2(p(1),..., p(m))
where h2 stands for the binary entropy:
h2(p(1),..., p(m)) = âˆ’âˆ‘
i
p(i)log p(i).
(1.1.11)

8
Essentials of Information Theory
Worked Example 1.1.9
(a) Give an example of a lossless code with alphabet
Jq which does not satisfy the Kraft inequality. Give an example of a lossless code
with the expected code-length strictly less than hq(X).
(b) Show that the â€˜Kraft sumâ€™ âˆ‘
i
qâˆ’s(i) associated with a lossless code may be
arbitrarily large (for sufï¬ciently large source alphabet).
Solution (a) Consider the alphabet I = {0,1,2} and a lossless code f with f(0) =
0, f(1) = 1, f(2) = 00 and codeword-lengths s(0) = s(1) = 1,s(2) = 2. Obviously,
âˆ‘
xâˆˆI
2âˆ’s(x) = 5/4, violating the Kraft inequality. For a random variable X with p(0) =
p(1) = p(2) = 1/3 the expected codeword-length Es(X) = 4/3 < h(X) = log3 =
1.585.
(b) Assume that the alphabet size m = â™¯I = 2(2L âˆ’1) for some positive
integer L. Consider the lossless code assigning to the letters x âˆˆI the codewords
0,1,00,01,10,11,000,..., with the maximum codeword-length L. The Kraft sum is
âˆ‘
xâˆˆI
2âˆ’s(x) = âˆ‘
lâ‰¤L âˆ‘
x:s(x)=l
2âˆ’s(x) = âˆ‘
lâ‰¤L
2l Ã—2âˆ’l = L,
which can be made arbitrarily large.
The assertion of Theorem 1.1.7 is further elaborated in
Theorem 1.1.10
(Shannonâ€™s noiseless coding theorem (NLCT)) For a ran-
dom source emitting symbols with probabilities p(i) > 0, the minimal expected
codeword-length for a decipherable encoding in alphabet Jq obeys
hq â‰¤min ES < hq +1,
(1.1.12)
where hq = âˆ’âˆ‘
i
p(i)logq p(i) is the q-ary entropy of the source; see (1.1.10).
Proof
The LHS inequality is established in (1.1.9). For the RHS inequality, let
s(i) be a positive integer such that
qâˆ’s(i) â‰¤p(i) < qâˆ’s(i)+1.
The non-strict bound here implies âˆ‘
i
qâˆ’s(i) â‰¤âˆ‘
i
p(i) = 1, i.e. the Kraft inequality.
Hence, there exists a decipherable code with codeword-lengths s(1),...,s(m). The
strict bound implies
s(i) < âˆ’log p(i)
logq
+1,

1.1 Basic concepts. The Kraft inequality. Huffmanâ€™s encoding
9
and thus
ES < âˆ’
âˆ‘
i
p(i)log p(i)
logq
+âˆ‘
i
p(i) =
h
logq +1.
Example 1.1.11
An instructive application of Shannonâ€™s NLCT is as follows. Let
the size m of the source alphabet equal 2k and assume that the letters i = 1,...,m are
emitted equiprobably: p(i) = 2âˆ’k. Suppose we use the code alphabet J2 = {0,1}
(binary encoding). With the binary entropy h2 = âˆ’log2âˆ’k
âˆ‘
1â‰¤iâ‰¤2k 2âˆ’k = k, we need,
on average, at least k binary digits for decipherable encoding. Using a term bit for
a unit of entropy, we say that on average the encoding requires at least k bits.
Moreover, the NLCT leads to a Shannonâ€“Fano encoding procedure: we ï¬x pos-
itive integer codeword-lengths s(1),...,s(m) such that qâˆ’s(i) â‰¤p(i) < qâˆ’s(i)+1, or,
equivalently,
âˆ’logq p(i) â‰¤s(i) < âˆ’logq p(i)+1; that is, s(i) =

âˆ’logq p(i)

.
(1.1.13)
Then construct a preï¬x-free code, from the shortest s(i) upwards, ensuring that
the previous codewords are not preï¬xes. The Kraft inequality guarantees enough
room. The obtained code may not be optimal but has the mean codeword-length
satisfying the same inequalities (1.1.13) as an optimal code.
Optimality is achieved by Huffmanâ€™s encoding f H
m : Im â†’Jâˆ—
q. We ï¬rst discuss
it for binary encodings, when q = 2 (i.e. J = {0,1}). The algorithm constructs a
binary tree, as follows.
(i) First, order the letters i âˆˆI so that p(1) â‰¥p(2) â‰¥Â·Â·Â· â‰¥p(m).
(ii) Assign symbol 0 to letter mâˆ’1 and 1 to letter m.
(iii) Construct a reduced alphabet Imâˆ’1 = {1,...,m âˆ’2,(m âˆ’1,m)}, with proba-
bilities
p(1),..., p(mâˆ’2), p(mâˆ’1)+ p(m).
Repeat steps (i) and (ii) with the reduced alphabet, etc. We obtain a binary tree. For
an example of Huffmanâ€™s encoding for m = 7 see Figure 1.1.
The number of branches we must pass through in order to reach a root i of the
tree equals s(i). The tree structure, together with the identiï¬cation of the roots
as source letters, guarantees that encoding is preï¬x-free. The optimality of binary
Huffman encoding follows from the following two simple lemmas.

10
Essentials of Information Theory
m = 7
i
1
2
3
4
5
6
7
p i
.025
.025
.05
.1
.15
.15
.5
11111
11110
1110
110
101
100
0
f(i)
si
1
3
3
3
4
5
5
.5
.15
.15
.1 .05
.025
.025
1.0
Figure 1.1
Lemma 1.1.12
Any optimal preï¬x-free binary code has the codeword-lengths
reverse-ordered versus probabilities:
p(i) â‰¥p(iâ€²)
implies
s(i) â‰¤s(iâ€²).
(1.1.14)
Proof
If not, we can form a new code, by swapping the codewords for i and iâ€².
This shortens the expected codeword-length and preserves the preï¬x-free property.
Lemma 1.1.13
In any optimal preï¬x-free binary code there exist, among the
codewords of maximum length, precisely two agreeing in all but the last digit.
Proof
If not, then either (i) there exists a single codeword of maximum length,
or (ii) there exist two or more codewords of maximum length, and they all differ
before the last digit. In both cases we can drop the last digit from some word of
maximum length, without affecting the preï¬x-free property.
Theorem 1.1.14
Huffmanâ€™s encoding is optimal among the preï¬x-free binary
codes.
Proof
The proof proceeds with induction in m. For m = 2, the Huffman code f H
2
has f H
2 (1) = 0, f H
2 (2) = 1, or vice versa, and is optimal. Assume the Huffman code
f H
mâˆ’1 is optimal for Imâˆ’1, whatever the probability distribution. Suppose further that

1.1 Basic concepts. The Kraft inequality. Huffmanâ€™s encoding
11
the Huffman code f H
m is not optimal for Im for some probability distribution. That
is, there is another preï¬x-free code, f âˆ—
m, for Im with a shorter expected word-length:
ESâˆ—
m < ESH
m.
(1.1.15)
The probability distribution under consideration may be assumed to obey
p(1) â‰¥Â·Â·Â· â‰¥p(m).
By Lemmas 1.1.12 and 1.1.13, in both codes we can shufï¬‚e codewords so that
the words corresponding to m âˆ’1 and m have maximum length and differ only in
the last digit. This allows us to reduce both codes to Imâˆ’1. Namely, in the Huffman
code f H
m we remove the ï¬nal digit from f H
m (m) and f H
m (m âˆ’1), â€˜glueingâ€™ these
codewords. This leads to Huffman encoding f H
mâˆ’1. In f âˆ—
m we do the same, and obtain
a new preï¬x-free code f âˆ—
mâˆ’1.
Observe that in Huffman code f H
m the contribution to ESH
m from f H
m (m âˆ’1)
and f H
m (m) is sH(m)(p(m âˆ’1) + p(m)); after reduction it becomes (sH(m) âˆ’1)
(p(mâˆ’1)+ p(m)). That is, ES is reduced by p(mâˆ’1)+ p(m). In code f âˆ—
m the sim-
ilar contribution is reduced from sâˆ—(m)(p(mâˆ’1)+ p(m)) to (sâˆ—(m)âˆ’1)(p(mâˆ’1)
+ p(m)); the difference is again p(mâˆ’1)+ p(m). All other contributions to ESH
mâˆ’1
and ESâˆ—
mâˆ’1 are the same as the corresponding contributions to ESH
m and ESâˆ—
m,
respectively. Therefore, f âˆ—
mâˆ’1 is better than f H
mâˆ’1: ESâˆ—
mâˆ’1 < ESH
mâˆ’1, which contra-
dicts the assumption.
In view of Theorem 1.1.14, we obtain
Corollary 1.1.15
Huffmanâ€™s encoding is optimal among the decipherable binary
codes.
The generalisation of the Huffman procedure to q-ary codes (with the code
alphabet Jq = {0,1,...,q âˆ’1}) is straightforward: instead of merging two sym-
bols, m âˆ’1,m âˆˆIm, having lowest probabilities, you merge q of them (again with
the smallest probabilities), repeating the above argument. In fact, Huffmanâ€™s orig-
inal 1952 paper was written for a general encoding alphabet. There are numerous
modiï¬cations of the Huffman code covering unequal coding costs (where some of
the encoding digits j âˆˆJq are more expensive than others) and other factors; we
will not discuss them in this book.
Worked Example 1.1.16
A drawback of Huffman encoding is that the
codeword-lengths are complicated functions of the symbol probabilities p(1), ...,
p(m). However, some bounds are available. Suppose that p(1) â‰¥p(2) â‰¥Â·Â·Â· â‰¥
p(m). Prove that in any binary Huffman encoding:
(a) if p(1) < 1/3 then letter 1 must be encoded by a codeword of length â‰¥2;
(b) if p(1) > 2/5 then letter 1 must be encoded by a codeword of length 1.

12
Essentials of Information Theory
1
2/5 < p (1)
1 _ p(1) < 2/3
0 < p (b) < p (1)
p (4)
p (b)
p(b)
p (3)
p (1) + p (b)
p (1) < 1/3
p (1) + p (b) + p (b) < 1
(a)
(b)
p(b)
Figure 1.2
Solution (a) Two cases are possible: the letter 1 either was, or was not merged with
other letters before two last steps in constructing a Huffman code. In the ï¬rst case,
s(1) â‰¥2. Otherwise we have symbols 1, b and bâ€², with
p(1) < 1/3,
p(1)+ p(b)+ p(bâ€²) = 1 and hence
max[p(b), p(bâ€²)] > 1/3.
Then letter 1 is to be merged, at the last but one step, with one of b, bâ€², and hence
s(1) â‰¥2. Indeed, suppose that at least one codeword has length 1, and this code-
word is assigned to letter 1 with p(1) < 1/3. Hence, the top of the Huffman tree is
as in Figure 1.2(a) with 0 â‰¤p(b), p(bâ€²) â‰¤1âˆ’p(1) and p(b)+ p(bâ€²) = 1âˆ’p(1).
But then max
	
p(b), p(bâ€²)

> 1/3, and hence p(1) should be merged with
min
	
p(b), p(bâ€²)

. Hence, Figure 1.2(a) is impossible, and letter 1 has codeword-
length â‰¥2.
The bound is sharp as both codes
{0,01,110,111} and {00,01,10,11}
are binary Huffman codes, e.g. for a probability distribution 1/3, 1/3, 1/4, 1/12.
(b) Now let p(1) > 2/5 and assume that letter 1 has a codeword-length s(1) â‰¥2 in
a Huffman code. Thus, letter 1 was merged with other letters before the last step.
That is, at a certain stage, we had symbols 1, b and bâ€² say, with
(A) p(bâ€²) â‰¥p(1) > 2/5,
(B) p(bâ€²) â‰¥p(b),
(C) p(1)+ p(b)+ p(bâ€²) â‰¤1
(D) p(1), p(b) â‰¥1/2 p(bâ€²).

1.1 Basic concepts. The Kraft inequality. Huffmanâ€™s encoding
13
Indeed, if, say, p(b) < 1/2p(bâ€²) then b should be selected instead of p(3) or p(4)
on the previous step when p(bâ€²) was formed. By virtue of (D), p(b) â‰¥1/5 which
makes (A)+(C) impossible.
A piece of the Huffman tree over p(1) is then as in Figure 1.2(b), with p(3) +
p(4) = p(bâ€²) and p(1)+ p(bâ€²)+ p(b) â‰¤1. Write
p(1) = 2/5+Îµ, p(bâ€²) = 2/5+Îµ +Î´, p(b) = 2/5+Îµ +Î´ âˆ’Î·,
with Îµ > 0, Î´,Î· â‰¥0. Then
p(1)+ p(bâ€²)+ p(b) = 6/5+3Îµ +2Î´ âˆ’Î· â‰¤1, and Î· â‰¥1/5+3Îµ +2Î´.
This yields
p(b) â‰¤1/5âˆ’2Îµ âˆ’Î´ < 1/5.
However, since
max
	
p(3), p(4)

â‰¥p(bâ€²)/2 â‰¥p(1)/2 > 1/5,
probability p(b) should be merged with min
	
p(3), p(4)

, i.e. diagram (b) is
impossible. Hence, the letter 1 has codeword-length s(1) = 1.
Worked Example 1.1.17
Suppose that letters i1,...,i5 are emitted with probabil-
ities 0.45, 0.25, 0.2, 0.05, 0.05. Compute the expected word-length for Shannonâ€“
Fano and Huffman coding. Illustrate both methods by ï¬nding decipherable binary
codings in each case.
Solution In this case q = 2, and
Shannonâ€“Fano:
p(i)
âŒˆâˆ’log2 p(i)âŒ‰
codeword
.45
2
00
.25
2
01
.2
3
100
.05
5
11100
.05
5
11111
with E

codeword-length) = .9+.5+.6+.25+.25 = 2.5, and
Huffman:
pi
codeword
.45
1
.25
01
.2
000
.05
0010
.05
0011
with E

codeword-length) = 0.45+0.5+0.6+0.2+0.2 = 1.95.

14
Essentials of Information Theory
Worked Example 1.1.18
A Shannonâ€“Fano code is in general not optimal. How-
ever, it is â€˜not muchâ€™ longer than Huffmanâ€™s. Prove that, if SSF is the Shannonâ€“
Fano codeword-length, then for any r = 1,2,... and any decipherable code f âˆ—with
codeword-length Sâˆ—,
P

Sâˆ—â‰¤SSF âˆ’r

â‰¤q1âˆ’r.
Solution Write
P(Sâˆ—â‰¤SSF âˆ’r) =
âˆ‘
iâˆˆI : sâˆ—(i)â‰¤sSF(i)âˆ’r
p(i).
Note that sSF(i) < âˆ’logq p(i)+1, hence
âˆ‘
iâˆˆI : sâˆ—(i)â‰¤sSF(i)âˆ’r
p(i) â‰¤
âˆ‘
iâˆˆI : sâˆ—(i)â‰¤âˆ’logq p(i)+1âˆ’r
p(i)
=
âˆ‘
iâˆˆI : sâˆ—(i)âˆ’1+râ‰¤âˆ’logq p(i)
p(i)
=
âˆ‘
iâˆˆI : p(i)â‰¤qâˆ’sâˆ—(i)+1âˆ’r
p(i)
â‰¤âˆ‘
iâˆˆI
qâˆ’sâˆ—(i)+1âˆ’r
= q1âˆ’râˆ‘
iâˆˆI
qâˆ’sâˆ—(i)
â‰¤q1âˆ’r;
the last inequality is due to Kraft.
A common modern practice is not to encode each letter u âˆˆI separately, but
to divide a source message into â€˜segmentsâ€™ or â€˜blocksâ€™, of a ï¬xed length n, and
encode these as â€˜lettersâ€™. It obviously increases the nominal number of letters in
the alphabet: the blocks are from the Cartesian product IÃ—n = I Ã—Â·Â·Â· (n times)Ã—I.
But what matters is the entropy
h(n)
q
= âˆ’âˆ‘
i1,...,in
P(U1 = i1,...,Un = in)logq P(U1 = i1,...,Un = in)
(1.1.16)
of the probability distribution for the blocks in a typical message. [Obviously,
we need to know the joint distribution of the subsequently emitted source let-
ters.] Denote by S(n) the random codeword-length in a decipherable segmented
code. The minimal expected codeword-length per source letter is deï¬ned by
en := min 1
n ES(n); by Shannonâ€™s NLCT, it obeys
h(n)
q
n
â‰¤en â‰¤h(n)
q
n + 1
n.
(1.1.17)
We see that, for large n, en âˆ¼h(n)
q

n.

1.1 Basic concepts. The Kraft inequality. Huffmanâ€™s encoding
15
Example 1.1.19
For a Bernoulli source emitting letter i with probability p(i) (cf.
Example 1.1.2), equation (1.1.16) yields
h(n)
q
= âˆ’âˆ‘
i1,...,in
p(i1)Â·Â·Â· p(in) logq

p(i1)Â·Â·Â· p(in)

= âˆ’
n
âˆ‘
j=1 âˆ‘
i1,...,in
p(i1)Â·Â·Â· p(in)logq p(i j) = nhq,
(1.1.18)
where hq = âˆ’âˆ‘p(i)logq p(i). Here, en âˆ¼hq. Thus, for n large, the minimal
expected codeword-length per source letter, in a segmented code, eventually at-
tains the lower bound in (1.1.13), and hence does not exceed minES, the minimal
expected codeword-length for letter-by-letter encodings. This phenomenon is much
more striking in the situation where the subsequent source letters are dependent. In
many cases h(n)
q
â‰ªn hq, i.e. en â‰ªhq. This is the gist of data compression.
Therefore, statistics of long strings becomes an important property of a source.
Nominally, the strings u(n) = u1 ...un of length n â€˜ï¬llâ€™ the Cartesian power IÃ—n; the
total number of such strings is mn, and to encode them all we need mn = 2nlogm
distinct codewords. If the codewords have a ï¬xed length (which guarantees the
preï¬x-free property), this length is between âŒŠnlogmâŒ‹, and âŒˆnlogmâŒ‰, and the rate
of encoding, for large n, is âˆ¼logm bits/source letter. But if some strings are rare,
we can disregard them, reducing the number of codewords used. This leads to the
following deï¬nitions.
Deï¬nition 1.1.20
A source is said to be (reliably) encodable at rate R > 0 if, for
any n, we can ï¬nd a set An âŠ‚IÃ—n such that
â™¯An â‰¤2nR
and
lim
nâ†’âˆP(U(n) âˆˆAn) = 1.
(1.1.19)
In other words, we can encode messages at rate R with a negligible error for long
source strings.
Deï¬nition 1.1.21
The information rate H of a given source is the inï¬mum of the
reliable encoding rates:
H = inf[R: R is reliable].
(1.1.20)
Theorem 1.1.22
For a source with alphabet Im,
0 â‰¤H â‰¤logm,
(1.1.21)
both bounds being attainable.

16
Essentials of Information Theory
Proof
The LHS inequality is trivial. It is attained for a degenerate source
(cf. Example 1.1.2c); here An contains â‰¤m constant strings, which is eventually
beaten by 2nR for any R > 0. On the other hand, â™¯IÃ—n = mn = 2nlogm, hence the
RHS inequality. It is attained for a source with IID letters and p(u) = 1/m: in this
case P(An) = (1/mn) â™¯An, which goes to zero when â™¯An â‰¤2nR and R < logm.
Example 1.1.23
(a) For telegraph English, m = 27 â‰ƒ24.76, i.e. H â‰¤4.76. For-
tunately, H â‰ª4.76, and this makes possible: (i) data compression, (ii) error-
correcting, (iii) code-breaking, (iv) crosswords. The precise value of H for
telegraph English (not to mention literary English) is not known: it is a challenging
task to assess it accurately. Nevertheless, modern theoretical tools and comput-
ing facilities make it possible to assess the information rate of a given (long) text,
assuming that it comes from a source that operates by allowing a fair amount of
â€˜randomnessâ€™ and â€˜homogeneityâ€™ (see Section 6.3 of [36].)
Some results of numerical analysis can be found in [136] analysing three texts:
(a) the collected works of Shakespeare; (b) a mixed text from various newspapers;
and (c) the King James Bible. The texts were stripped of punctuation and the spaces
between words were removed. Texts (a) and (b) give values 1.7 and 1.25 respec-
tively (which is rather ï¬‚attering to modern journalism). In case (c) the results were
inconclusive; apparently the above assumptions are not appropriate in this case.
(For example, the genealogical enumerations of Genesis are hard to compare with
the philosophical discussions of Paulâ€™s letters, so the homogeneity of the source is
obviously not maintained.)
Even more challenging is to compare different languages: which one is more
appropriate for intercommunication? Also, it would be interesting to repeat the
above experiment with the collected works of Tolstoy or Dostoyevsky.
For illustration, we give below the original table by Samuel Morse (1791â€“1872),
creator of the Morse code, providing the frequency count of different letters in
telegraph English which is dominated by a relatively small number of common
words.
â›
âœ
âœ
âœ
âœ
âœ
âœ
âœ
â
E
T
A
I
N
O
S
H
R
12000
9000
8000
8000
8000
8000
8000
6400
6200
D
L
U
C
M
F
W
Y
G
4400
4000
3400
3000
3000
2500
2000
2000
1700
P
B
V
K
Q
J
X
Z
1700
1600
1200
800
500
400
400
200
â
âŸ
âŸ
âŸ
âŸ
âŸ
âŸ
âŸ
â 
(b)
A similar idea was applied to the decimal and binary decomposition of a
given number. For example, take number Ï€. If the information rate for its binary

1.1 Basic concepts. The Kraft inequality. Huffmanâ€™s encoding
17
1
x
1
x
ln x
Figure 1.3
decomposition approaches value 1 (which is the information rate of a randomly
chosen sequence), we may think that Ï€ behaves like a completely random num-
ber; otherwise we could imagine that Ï€ was a â€˜Specially Chosen Oneâ€™. The
same question may be asked about e,
âˆš
2 or the Eulerâ€“Mascheroni constant
Î³ = lim
Nâ†’âˆ

âˆ‘
1â‰¤nâ‰¤N
1
n âˆ’lnN

. (An open part of one of Hilbertâ€™s problems is to prove
or disprove that Î³ is a transcendental number, and transcendental numbers form
a set of probability one under the Bernoulli source of subsequent digits.) As the
results of numerical experiments show, for the number of digits N âˆ¼500,000 all
the above-mentioned numbers display the same pattern of behaviour as a com-
pletely random number; see [26]. In Section 1.3 we will calculate the information
rates of Bernoulli and Markov sources.
We conclude this section with the following simple but fundamental fact.
Theorem 1.1.24
(The Gibbs inequality: cf. PSE II, p. 421) Let {p(i)} and {pâ€²(i)}
be two probability distributions (on a ï¬nite or countable set I). Then, for any b > 1,
âˆ‘
i
p(i)logb
pâ€²(i)
p(i) â‰¤0, i.e. âˆ’âˆ‘
i
p(i)logb p(i) â‰¤âˆ’âˆ‘
i
p(i)logb pâ€²(i),
(1.1.22)
and equality is attained iff p(i) = pâ€²(i), 1 âˆˆI.
Proof
The bound
logb x â‰¤xâˆ’1
lnb

18
Essentials of Information Theory
holds for each x > 0, with equality iff x = 1. Setting Iâ€² = {i : p(i) > 0}, we have
âˆ‘
i
p(i)logb
pâ€²(i)
p(i)
= âˆ‘
iâˆˆIâ€² p(i)logb
pâ€²(i)
p(i) â‰¤1
lnb âˆ‘
iâˆˆIâ€² p(i)
 pâ€²(i)
p(i) âˆ’1

= 1
lnb

âˆ‘
iâˆˆIâ€² pâ€²(i)âˆ’âˆ‘
iâˆˆIâ€² p(i)

= 1
lnb

âˆ‘
iâˆˆIâ€² pâ€²(i)âˆ’1

â‰¤0.
For equality we need: (a) âˆ‘
iâˆˆIâ€² pâ€²(i) = 1, i.e. pâ€²(i) = 0 when p(i) = 0; and (b)
pâ€²(i)/p(i) = 1 for i âˆˆIâ€².
1.2 Entropy: an introduction
Only entropy comes easy.
Anton Chekhov (1860â€“1904), Russian writer and playwright
This section is entirely devoted to properties of entropy. For simplicity, we work
with the binary entropy, where the logarithms are taken at base 2. Consequently,
subscript 2 in the notation h2 is omitted. We begin with a formal repetition of the
basic deï¬nition, putting a slightly different emphasis.
Deï¬nition 1.2.1
Given an event A with probability p(A), the information gained
from the fact that A has occurred is deï¬ned as
i(A) = âˆ’log p(A).
Further, let X be a random variable taking a ï¬nite number of distinct values
{x1,...,xm}, with probabilities pi = pX(xi) = P(X = xi). The binary entropy h(X)
is deï¬ned as the expected amount of information gained from observing X:
h(X) = âˆ’âˆ‘
xi
pX(xi)log pX(xi) = âˆ’âˆ‘
i
pi log pi = E

âˆ’log pX(X)

.
(1.2.1)
[In view of the adopted equality 0 Â· log0 = 0, the sum may be reduced to those xi
for which pX(xi) > 0.]
Sometimes an alternative view is useful: i(A) represents the amount of informa-
tion needed to specify event A and h(X) gives the expected amount of information
required to specify a random variable X.
Clearly, the entropy h(X) depends on the probability distribution, but not on
the values x1,...,xm: h(X) = h(p1,..., pm). For m = 2 (a two-point probability
distribution), it is convenient to consider the function Î·(p)(= Î·2(p)) of a single
variable p âˆˆ[0,1]:
Î·(p) = âˆ’plog pâˆ’(1âˆ’p)log(1âˆ’p).
(1.2.2a)

1.2 Entropy: an introduction
19
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.2
0.4
0.6
0.8
1.0
Entropy for p in [0,1]
p
h(p,1âˆ’p)
Figure 1.4
p
0.0
0.2
0.4
0.6
0.8
q
0.0
0.2
0.4
0.6
0.8
h(p,q,1âˆ’pâˆ’q)
0.0
0.5
1.0
1.5
Entropy for p,q,p+q in [0,1]
Figure 1.5
The graph of Î·(p) is plotted in Figure 1.4. Observe that the graph is concave as
d2
dp2 Î·(p) = âˆ’

loge

[p(1âˆ’p)] < 0. See Figure 1.4.
The graph of the entropy of a three-point distribution
Î·3(p,q) = âˆ’plog pâˆ’qlogqâˆ’(1âˆ’pâˆ’q)log(1âˆ’pâˆ’q)

20
Essentials of Information Theory
is plotted in Figure 1.5 as a function of variables p,q âˆˆ[0,1] with p+q â‰¤1: it also
shows the concavity property.
Deï¬nition 1.2.1 implies that for independent events, A1 and A2,
i(A1 âˆ©A2) = i(A1)+i(A2),
(1.2.2b)
and i(A) = 1 for event A with p(A) = 1/2.
A justiï¬cation of Deï¬nition 1.2.1 comes from the fact that any function iâˆ—(A)
which (i) depends on probability p(A) (i.e. obeys iâˆ—(A) = iâˆ—(Aâ€²) if p(A) = p(Aâ€²)), (ii)
is continuous in p(A), and (iii) satisï¬es (1.2.2b), coincides with i(A) (for axiomatic
deï¬nitions of entropy, cf. Worked Example 1.2.24 below).
Deï¬nition 1.2.2
Given a pair of random variables, X, Y, with values xi and yj,
the joint entropy h(X,Y) is deï¬ned by
h(X,Y) = âˆ’âˆ‘
xi,y j
pX,Y(xi,yj)log pX,Y(xi,yj) = E

âˆ’log pX,Y(X,Y)

,
(1.2.3)
where pX,Y(xi,yj) = P(X = xi,Y = yj) is the joint probability distribution. In other
words, h(X,Y) is the entropy of the random vector (X,Y) with values (xi,yj).
The conditional entropy, h(X|Y), of X given Y is deï¬ned as the expected amount
of information gained from observing X given that a value of Y is known:
h(X|Y) = âˆ’âˆ‘
xi,y j
pX,Y(xi,yj)log pX|Y(xi|y j) = E

âˆ’log pX|Y(X|Y)

.
(1.2.4)
Here, pX,Y(i, j) is the joint probability P(X = xi,Y = yj) and pX|Y(xi|y j) the con-
ditional probability P(X = xi|Y = yj). Clearly, (1.2.3) and (1.2.4) imply
h(X|Y) = h(X,Y)âˆ’h(Y).
(1.2.5)
Note that in general h(X|Y) Ì¸= h(Y|X).
For random variables X and Y taking values in the same set I, and such that
pY(x) > 0 for all x âˆˆI, the relative entropy h(X||Y) (also known as the entropy of
X relative to Y or Kullbackâ€“Leibler distance D(pX||pY)) is deï¬ned by
h(X||Y) = âˆ‘
x
pX(x)log pX(x)
pY(x) = EX

âˆ’log pY(X)
pX(X)

,
(1.2.6)
with pX(x) = P(X = x) and pY(x) = P(Y = x), x âˆˆI.
Straightforward properties of entropy are given below.
Theorem 1.2.3
(a) If a random variable X takes at most m values, then
0 â‰¤h(X) â‰¤logm;
(1.2.7)

1.2 Entropy: an introduction
21
the LHS equality occurring iff X takes a single value, and the RHS equality
occurring iff X takes m values with equal probabilities.
(b) The joint entropy obeys
h(X,Y) â‰¤h(X)+h(Y),
(1.2.8)
with equality iff X and Y are independent, i.e. P(X = x,Y = y) = P(X = x)
P(Y = y) for all x,y âˆˆI.
(c) The relative entropy is always non-negative:
h(X||Y) â‰¥0,
(1.2.9)
with equality iff X and Y are identically distributed: pX(x) â‰¡pY(x), x âˆˆI.
Proof
Assertion (c) is equivalent to Gibbsâ€™ inequality from Theorem 1.1.24. Next,
(a) follows from (c), with {p(i)} being the distribution of X and pâ€²(i) â‰¡1/m,
1 â‰¤i â‰¤m. Similarly, (b) follows from (c), with i being a pair (i1,i2) of values
of X and Y, p(i) = pX,Y(i1,i2) being the joint distribution of X and Y and pâ€²(i) =
pX(i1)pY(i2) representing the product of their marginal distributions. Formally:
(a) h(X) = âˆ’âˆ‘
i
p(i)log p(i) â‰¤âˆ‘
i
p(i)logm = logm,
(b) h(X,Y) = âˆ’âˆ‘
(i1,i2)
pX,Y(i1,i2)log pX,Y(i1,i2)
â‰¤âˆ’âˆ‘
(i1,i2)
pX,Y(i1,i2)Ã—log

pX(i1)pY(i2)

= âˆ’âˆ‘
i1
pX(i1)log pX(i1)âˆ’âˆ‘
i2
pY(i2)log pY(i2)
= h(X)+h(Y).
We used here the identities âˆ‘
i2
pX,Y(i1,i2) = pX(i1), âˆ‘
i1
pX,Y(i1,i2) = pY(i2).
Worked Example 1.2.4
(a) Show that the geometric random variable Y with pj = P(Y = j) = (1 âˆ’p)pj,
j = 0,1,2,..., yields maximum entropy amongst all distributions on Z+ =
{0,1,2,...} with the same mean.
(b) Let Z be a random variable with values from a ï¬nite set K and f be a given real
function f : K â†’R, with fâˆ—= min
	
f(k): k âˆˆK

and f âˆ—= max
	
f(k): k âˆˆK

.
Set E( f) = âˆ‘
kâˆˆK
f(k)

(â™¯K) and consider the problem of maximising the entropy
h(Z) of the random variable Z subject to a constraint
E f(Z) â‰¤Î±.
(1.2.10)

22
Essentials of Information Theory
Show that:
(bi) when f âˆ—â‰¥Î± â‰¥E( f) then the maximising probability distribution is uni-
form on K, with P(Z = k) = 1/(â™¯K), k âˆˆK;
(bii) when fâˆ—â‰¤Î± < E( f) and f is not constant then the maximising probabil-
ity distribution has
P(Z = k) = pk = eÎ» f(k)

âˆ‘
i
eÎ» f(i) , k âˆˆK,
(1.2.11)
where Î» = Î»(Î±) < 0 is chosen so as to satisfy
âˆ‘
k
pk f(k) = Î±.
(1.2.12)
Moreover, suppose that Z takes countably many values, but f â‰¥0 and for a
given Î± there exists a Î» < 0 such that âˆ‘
i
eÎ» f(i) < âˆand âˆ‘
k
pk f(k) = Î± where pk
has form (1.2.11). Then:
(biii) the probability distribution in (1.2.11) still maximises h(Z) under
(1.2.10).
Deduce assertion (a) from (biii).
(c) Prove that hY(X) â‰¥0, with equality iff P(X = x) = P(Y = x) for all x. By
considering Y, a geometric random variable on Z+ with parameter chosen
appropriately, show that if the mean EX = Î¼ < âˆ, then
h(X) â‰¤(Î¼ +1)log(Î¼ +1)âˆ’Î¼ logÎ¼ ,
(1.2.13)
with equality iff X is geometric.
Solution
(a) By the Gibbs inequality, for all probability distribution (q0,q1,...)
with mean âˆ‘
iâ‰¥0
iqi â‰¤Î¼,
h(q) = âˆ’âˆ‘
i
qi logqi â‰¤âˆ’âˆ‘
i
qi log pi = âˆ’âˆ‘
i
qi(log(1âˆ’p)+ilog p)
â‰¤âˆ’log(1âˆ’p)âˆ’Î¼ log p = h(Y)
as Î¼ = p/(1âˆ’p), and equality holds iff q is geometric with mean Î¼.
(b) First, observe that the uniform distribution, with pk = 1

(â™¯K), which ren-
ders the â€˜globalâ€™ maximum of h(Z) is obtained for Î» = 0 in (1.2.11). In part (bi),
this distribution satisï¬es (1.2.10) and hence maximises h(Z) under this constraint.
Passing to (bii), let pâˆ—
k = eÎ» f(k)

âˆ‘
i
eÎ» f(i) , k âˆˆK, where Î» is chosen to satisfy
Eâˆ—f(Z) = âˆ‘
k
pâˆ—
k f(k) = Î±. Let q = {qk} be any probability distribution satisfying

1.2 Entropy: an introduction
23
Eq f = âˆ‘
k
qk f(k) â‰¤Î±. Next, observe that the mean value (1.2.12) calculated for the
probability distribution from (1.2.11) is a non-decreasing function of Î». In fact, the
derivative
dÎ±
dÎ»
=
âˆ‘
k
[ f(k)]2 eÎ» f(k)
âˆ‘
i
eÎ» f(i)
âˆ’

âˆ‘
k
f(k)eÎ» f(k)
2

âˆ‘
i
eÎ» f(i)
2
= E[ f(Z)]2 âˆ’[E f(Z)]2
is positive (it yields the variance of the random variable f(Z)); for a non-constant
f the RHS is actually non-negative. Therefore, for non-constant f (i.e. with
fâˆ—< E( f) < f âˆ—), for all Î± from the interval [ fâˆ—, f âˆ—] there exists exactly one prob-
ability distribution of form (1.2.11) satisfying (1.2.12), and for fâˆ—â‰¤Î± < E( f) the
corresponding Î»(Î±) is < 0.
Next, we use the fact that the Kullbackâ€“Leibler distance D(q||pâˆ—) (cf. (1.2.6))
satisï¬es D(q||pâˆ—) = âˆ‘
k
qk log (qk/pâˆ—
k) â‰¥0 (Gibbsâ€™ inequality) and that âˆ‘
k
qk f(k) â‰¤Î±
and Î» < 0 to obtain that
h(q) = âˆ’âˆ‘
k
qk logqk = âˆ’D(q||pâˆ—)âˆ’âˆ‘
k
qk log pâˆ—
k
â‰¤âˆ’âˆ‘
k
qk log pâˆ—
k = âˆ’âˆ‘
k
qk

âˆ’logâˆ‘
i
eÎ» f(i) +Î» f(k)

â‰¤âˆ’âˆ‘
k
qk

âˆ’logâˆ‘
i
eÎ» f(i)
âˆ’Î»Î±
= âˆ’âˆ‘
k
pâˆ—
k

âˆ’logâˆ‘
i
eÎ» f(i) +Î» f(k)

= âˆ’âˆ‘
k
pâˆ—
k log pâˆ—
k = h(pâˆ—).
For part (biii): the above argument still works for an inï¬nite countable set K
provided that the value Î»(Î±) determined from (1.2.12) is < 0.
(c) By the Gibbs inequality hY(X) â‰¥0. Next, we use part (b) by taking f(k) = k,
Î± = Î¼ and Î» = lnq. The maximum-entropy distribution can be written as pâˆ—
j =
(1 âˆ’p)p j, j = 0,1,2,..., with âˆ‘
k
kpâˆ—
k = Î¼, or Î¼ = p/(1 âˆ’p). The entropy of this
distribution equals
h(pâˆ—) = âˆ’âˆ‘
j
(1âˆ’p)pj log

(1âˆ’p)pj
= âˆ’
p
1âˆ’p log pâˆ’log(1âˆ’p) = (Î¼ +1)log(Î¼ +1)âˆ’Î¼ logÎ¼,
where Î¼ = p/(1âˆ’p).

24
Essentials of Information Theory
Alternatively:
0 â‰¤hY(X) = âˆ‘
i
p(i)log
p(i)
(1âˆ’p)pi
= âˆ’h(X)âˆ’log(1âˆ’p)âˆ‘
i
p(i)âˆ’(log p)

âˆ‘
i
ip(i)

= âˆ’h(X)âˆ’log(1âˆ’p)âˆ’Î¼ log p.
The optimal choice of p is p = Î¼/(Î¼ +1). Then
h(X) â‰¤âˆ’log
1
Î¼ +1 âˆ’Î¼ log
Î¼
Î¼ +1 = (Î¼ +1)log(Î¼ +1)âˆ’Î¼ logÎ¼.
The RHS is the entropy h(Y) of the geometric random variable Y. Equality holds
iff X âˆ¼Y, i.e. X is geometric.
A simple but instructive corollary of the Gibbs inequality is
Lemma 1.2.5
(The pooling inequalities) For any q1, q2 â‰¥0, with q1 +q2 > 0,
âˆ’(q1 +q2)log(q1 +q2) â‰¤âˆ’q1 logq1 âˆ’q2 logq2
â‰¤âˆ’(q1 +q2)log q1 +q2
2
;
(1.2.14)
the ï¬rst equality occurs iff q1q2 = 0 (i.e. either q1 or q2 vanishes), and the second
equality iff q1 = q2.
Proof
Indeed, (1.2.14) is equivalent to
0 â‰¤h

q1
q1 +q2
,
q2
q1 +q2

â‰¤log2 (= 1).
By Lemma 1.2.5, â€˜glueingâ€™ together values of a random variable could dimin-
ish the corresponding contribution to the entropy. On the other hand, the â€˜re-
distributionâ€™ of probabilities making them equal increases the contribution. An
immediate corollary of Lemma 1.2.5 is the following.
Theorem 1.2.6
Suppose that a discrete random variable X is a function of dis-
crete random variable Y: X = Ï†(Y). Then
h(X) â‰¤h(Y),
(1.2.15)
with equality iff Ï† is invertible.
Proof
Indeed, if Ï† is invertible then the probability distributions of X and Y differ
only in the order of probabilities, which does not change the entropy. If Ï† â€˜gluesâ€™
some values yj then we can repeatedly use the LHS pooling inequality.

1.2 Entropy: an introduction
25
1
1/2
1
log x
Figure 1.6
Worked Example 1.2.7
Let p1,..., pn be a probability distribution, with pâˆ—=
max[pi]. Prove the following lower bounds for the entropy h = âˆ’âˆ‘
i
pi log pi:
(i) h â‰¥âˆ’pâˆ—log pâˆ—âˆ’(1âˆ’pâˆ—)log(1âˆ’pâˆ—) = Î·(pâˆ—);
(ii) h â‰¥âˆ’log pâˆ—;
(iii) h â‰¥2(1âˆ’pâˆ—).
Solution Part (i) follows from the pooling inequality, and (ii) holds as
h â‰¥âˆ’âˆ‘
i
pi log pâˆ—= âˆ’log pâˆ—.
To check (iii), assume ï¬rst that pâˆ—â‰¥1/2. Since the function p â†’Î·(p), 0 â‰¤p â‰¤1,
is concave (see (1.2.3)), its graph on
	
1/2,1

lies above the line x â†’2(1 âˆ’p).
Then, by (i),
h â‰¥Î· (pâˆ—) â‰¥2(1âˆ’pâˆ—).
(1.2.16)
On the other hand, if pâˆ—â‰¤1/2, we use (ii):
h â‰¥âˆ’log pâˆ—,
and apply the inequality âˆ’log p â‰¥2(1âˆ’p) for 0 â‰¤p â‰¤1/2.
Theorem 1.2.8
(The Fano inequality) Suppose a random variable X takes m > 1
values, and one of them has probability (1âˆ’Îµ). Then
h(X) â‰¤Î·(Îµ)+Îµ log(mâˆ’1)
(1.2.17)
where Î· is the function from (1.2.2a).

26
Essentials of Information Theory
Proof
Suppose that p1 = p(x1) = 1âˆ’Îµ. Then
h(X) = h(p1,..., pm) = âˆ’
m
âˆ‘
i=1
pi log pi
= âˆ’p1 log p1 âˆ’(1âˆ’p1)log(1âˆ’p1)+(1âˆ’p1)log(1âˆ’p1)
âˆ’âˆ‘
2â‰¤iâ‰¤m
pi log pi
= h(p1,1âˆ’p1)+(1âˆ’p1)h

p2
1âˆ’p1
,...,
pm
1âˆ’p1

;
in the RHS the ï¬rst term is Î·(Îµ) and the second one does not exceed Îµ log(mâˆ’1).
Deï¬nition 1.2.9
Given random variables X, Y, Z, we say that X and Y are con-
ditionally independent given Z if, for all x and y and for all z with P(Z = z) > 0,
P(X = x,Y = y|Z = z) = P(X = x|Z = z)P(Y = y|Z = z).
(1.2.18)
For the conditional entropy we immediately obtain
Theorem 1.2.10
(a) For all random variables X, Y,
0 â‰¤h(X|Y) â‰¤h(X),
(1.2.19)
the ï¬rst equality occurring iff X is a function of Y and the second equality holding
iff X and Y are independent.
(b) For all random variables X, Y, Z,
h(X|Y,Z) â‰¤h(X|Y) â‰¤h(X|Ï†(Y)),
(1.2.20)
the ï¬rst equality occurring iff X and Z are conditionally independent given Y and
the second equality holding iff X and Z are conditionally independent given Ï†(Y).
Proof
(a) The LHS bound in (1.2.19) follows from deï¬nition (1.2.4) (since
h(X|Y) is a sum of non-negative terms). The RHS bound follows from repre-
sentation (1.2.5) and bound (1.2.8). The LHS quality in (1.2.19) is equivalent
to the equation h(X,Y) = h(Y) or h(X,Y) = h(Ï†(X,Y)) with Ï†(X,Y) = Y. In
view of Theorem 1.2.6, this occurs iff, with probability 1, the map (X,Y) â†’Y
is invertible, i.e. X is a function of Y. The RHS equality in (1.2.19) occurs iff
h(X,Y) = h(X)+h(Y), i.e. X and Y are independent.
(b) For the lower bound, use a formula analogous to (1.2.5):
h(X|Y,Z) = h(X,Z|Y)âˆ’h(Z|Y)
(1.2.21)
and an inequality analogous to (1.2.10):
h(X,Z|Y) â‰¤h(X|Y)+h(Z|Y),
(1.2.22)

1.2 Entropy: an introduction
27
with equality iff X and Z are conditionally independent given Y. For the RHS
bound, use:
(i) a formula that is a particular case of (1.2.21): h(X|Y,Ï†(Y)) = h(X,Y|Ï†(Y))âˆ’
h(Y|Ï†(Y)), together with the remark that h(X|Y,Ï†(Y)) = h(X|Y);
(ii) an inequality which is a particular case of (1.2.22): h(X,Y|Ï†(Y)) â‰¤
h(X|Ï†(Y)) + h(Y|Ï†(Y)), with equality iff X and Y are conditionally independent
given Ï†(Y).
Theorems 1.2.8 above and 1.2.11 below show how the entropy h(X) and con-
ditional entropy h(X|Y) are controlled when X is â€˜nearlyâ€™ a constant (respectively,
â€˜nearlyâ€™ a function of Y).
Theorem 1.2.11
(The generalised Fano inequality) For a pair of random vari-
ables, X and Y taking values x1,...,xm and y1,...,ym, if
m
âˆ‘
j=1
P(X = xj,Y = yj) = 1âˆ’Îµ,
(1.2.23)
then
h(X|Y) â‰¤Î·(Îµ)+Îµ log(mâˆ’1),
(1.2.24)
where Î·(Îµ) is deï¬ned in (1.2.3).
Proof
Denoting Îµj = P(X Ì¸= xj|Y = yj), we write
âˆ‘
j
pY(yj)Îµ j = âˆ‘
j
P(X Ì¸= xj,Y = yj) = Îµ.
(1.2.25)
By deï¬nition of the conditional entropy, the Fano inequality and concavity of
the function Î·( Â· ),
h(X|Y) â‰¤âˆ‘
j
pY(yj)

Î·(Îµ j)+Îµj log(mâˆ’1)

â‰¤âˆ‘
j
pY(yj)Î·(Îµj)+Îµ log(mâˆ’1) â‰¤Î·(Îµ)+Îµ log(mâˆ’1).
If the random variable X takes countably many values {x1, x2,...}, the above
deï¬nitions may be repeated, as well as most of the statements; notable exceptions
are the RHS bound in (1.2.7) and inequalities (1.2.17) and (1.2.24).
Many properties of entropy listed so far are extended to the case of random
strings.
Theorem 1.2.12
For a pair of random strings, X(n) = (X1,...,Xn) and Y(n) =
(Y1,...,Yn),

28
Essentials of Information Theory
(a) the joint entropy, given by
h(X(n)) = âˆ’âˆ‘
x(n)
P(X(n) = x(n))logP(X(n) = x(n)),
obeys
h(X(n)) =
n
âˆ‘
i=1
h(Xi|X(iâˆ’1)) â‰¤
n
âˆ‘
i=1
h(Xi),
(1.2.26)
with equality iff components X1,...,Xn are independent;
(b) the conditional entropy, given by
h(X(n)|Y(n))
= âˆ’âˆ‘
x(n),y(n)
P(X(n) = x(n),Y(n) = y(n))logP(X(n) = x(n)|Y(n) = y(n)),
satisï¬es
h(X(n)|Y(n)) â‰¤
n
âˆ‘
i=1
h(Xi|Y(n)) â‰¤
n
âˆ‘
i=1
h(Xi|Yi),
(1.2.27)
with the LHS equality holding iff X1,...,Xn are conditionally independent, given
Y(n), and the RHS equality holding iff, for each i = 1,...,n, Xi and {Yr : 1 â‰¤r â‰¤
n, r Ì¸= i} are conditionally independent, given Yi.
Proof
The proof repeats the arguments used previously in the scalar case.
Deï¬nition 1.2.13
The mutual information or mutual entropy, I(X : Y), between
X and Y is deï¬ned as
I(X : Y) := âˆ‘
x,y
pX,Y(x,y)log pX,Y(x,y)
pX(x)pY(y) = Elog pX,Y(X,Y)
pX(X)pY(Y)
= h(X)+h(Y)âˆ’h(X,Y) = h(X)âˆ’h(X|Y)
= h(Y)âˆ’h(Y|X).
(1.2.28)
As can be seen from this deï¬nition, I(X : Y) = I(Y : X).
Intuitively, I(X : Y) measures the amount of information about X conveyed by Y
(and vice versa). Theorem 1.2.10(b) implies
Theorem 1.2.14
If a random variable Ï†(Y) is a function of Y then
0 â‰¤I(X : Ï†(Y)) â‰¤I(X : Y),
(1.2.29)
the ï¬rst equality occurring iff X and Ï†(Y) are independent, and the second iff X
and Y are conditionally independent, given Ï†(Y).

1.2 Entropy: an introduction
29
Worked Example 1.2.15
Suppose that two non-negative random variables X
and Y are related by Y = X + N, where N is a geometric random variable taking
values in Z+ and is independent of X. Determine the distribution of Y which max-
imises the mutual entropy between X and Y under the constraint that the mean
EX â‰¤K and show that this distribution can be realised by assigning to X the value
zero with a certain probability and letting it follow a geometrical distribution with
a complementary probability.
Solution Because Y = X +N where X and N are independent, we have
I(X : Y) = h(Y)âˆ’h(Y|X) = h(Y)âˆ’h(N).
Also E(Y) = E(X)+E(N) â‰¤K +E(N). Therefore, if we can guarantee that Y may
be taken geometrically distributed with mean K +E(N) then it gives the maximal
value of I(X : Y). To this end, write an equation for probability-generating func-
tions:
E(zY) = E(zX)E(zN), z > 0,
with E(zN) = (1âˆ’p)/(1âˆ’zp), 0 < z < 1/p, and
E(zY) = 1âˆ’pâˆ—
1âˆ’zpâˆ—, 0 < z < 1
pâˆ—,
where pâˆ—is to be found from an equation
Î¼Y =
pâˆ—
1âˆ’pâˆ—= K +
p
1âˆ’p = K(1âˆ’p)+ p
1âˆ’p
.
This yields
pâˆ—= K(1âˆ’p)+ p
1+K(1âˆ’p),
E(zY) =
1âˆ’p
1+K(1âˆ’p)âˆ’z(p+K(1âˆ’p)),
and
E(zX) =
1âˆ’zp
1+K(1âˆ’p)âˆ’z(p+K(1âˆ’p)).
(1.2.30)
The form of the distribution of X suggested in the example leads to
E(zX) = Îº0 +(1âˆ’Îº0) 1âˆ’pX
1âˆ’zpX
,
(1.2.31)
where Îº0 +(1âˆ’Îº0)(1âˆ’pX) = P(X = 0). Selecting
pX = p+K(1âˆ’p)
1+K(1âˆ’p),Îº0 =
p
p+K(1âˆ’p),
we see that (1.2.30) and (1.2.31) coincide.

30
Essentials of Information Theory
I only ask for information. . .
Charles Dickens (1812â€“1870), English writer,
from David Copperï¬eld
In Deï¬nition 1.2.13 and Theorem 1.2.14, random variables X and Y may be
replaced by random strings. In addition, by repeating the above arguments for
strings X(n) and Y(n), we obtain
Theorem 1.2.16
(a) The mutual entropy between random strings obeys
I(X(n) : Y(n)) â‰¥h(X(n))âˆ’
n
âˆ‘
i=1
h(Xi|Y(n)) â‰¥h(X(n))âˆ’
n
âˆ‘
i=1
h(Xi|Yi).
(1.2.32)
(b) If X1,...,Xn are independent then
I(X(n) : Y(n)) â‰¥
n
âˆ‘
i=1
I(Xi : Y(n)).
(1.2.33)
Observe that
n
âˆ‘
i=1
I(Xi : Y(n)) â‰¥
n
âˆ‘
i=1
I(Xi : Yi).
(1.2.34)
Worked Example 1.2.17
Let X, Z be random variables and Y(n) = (Y1,...,Yn)
be a random string.
(a) Prove the inequality
0 â‰¤I(X : Z) â‰¤min{h(X),h(Z)}.
(b) Prove or disprove by producing a counter-example the inequality
I(X : Y(n)) â‰¤
n
âˆ‘
j=1
I(X : Yj),
(1.2.35)
ï¬rst under the assumption that Y1,...,Yn are independent random variables,
and then under the assumption that Y1,...,Yn are conditionally independent
given X.
(c) Prove or disprove by producing a counter-example the inequality
I(X : Y(n)) â‰¥
n
âˆ‘
j=1
I(X : Yj),
(1.2.36)
ï¬rst under the assumption that Y1,...,Yn are independent random variables,
and then under the assumption that Y1,...,Yn are conditionally independent
given X.

1.2 Entropy: an introduction
31
Solution (a) By the Gibbs inequality, I(X : Z) â‰¥0, and
I(X : Z) := âˆ’âˆ‘
x,z
P(X = x,Z = z)log P(X = x,Z = z)
P(X = x)P(Z = z)
= h(X)âˆ’h(X|Z) = h(Z)âˆ’h(Z|X).
Here h(X|Z) â‰¥0 and h(Z|X) â‰¥0. Hence I(X : Z) â‰¤h(X) and I(X : Z) â‰¤h(Z), so
I(X : Z) â‰¤min
	
h(X),h(Z)

.
(b) Write
I(X : Y(n)) = h(Y(n))âˆ’h(Y(n)|X).
(1.2.37)
Then, if Y1,...,Yn are conditionally independent given X, the RHS of (1.2.37)
equals
h(Y(n))âˆ’
n
âˆ‘
j=1
h(Yj|X) â‰¤
n
âˆ‘
j=1
	
h(Yj)âˆ’h(Yj|X)

=
n
âˆ‘
j=1
I(X : Yj),
giving that of (1.2.35).
(c) Next, if Y1,...,Yn are independent, the RHS of (1.2.37) equals
n
âˆ‘
j=1
h(Yj)âˆ’h(Y(n)|X) â‰¥
n
âˆ‘
j=1
	
h(Yj)âˆ’h(Yj|X)

=
n
âˆ‘
j=1
I(X : Yj),
giving the RHS of (1.2.36).
On the other hand, property (b) fails under the independence condition. Indeed,
set n = 2, with Y(2) = (Y1,Y2), and let Y1 and Y2 take values 0 or 1 with probabilities
1/2, j = 1,2, independently, and set X = (Y1 +Y2) mod 2. Then
h(X) = h(X|Yj) = 1, so I(X : Yj) â‰¡0, j = 1,2,
but
h(X|Y(2)) = 0, so I(X : Y(2)) = 1.
Also, (c) fails under the conditional independence condition. Indeed, take a
DTMC (U1,U2,...) with states Â±1, the initial probability distribution {1/2,1/2}
and the transition matrix
0
1
1
0

. Set
Y1 = U1, X = U2, Y2 = U3.
Then Y1, Y2 are conditionally independent given X: Y1 = Y2 = âˆ’X. On the other
hand,
1 = I(X : Y(2)) = h(Y(2)) = h(Y1) = h(Y2)
< h(Y1)+h(Y2) = I(X : Y1)+I(X : Y2) = 2.

32
Essentials of Information Theory
Recall that a real function f(y) deï¬ned on a convex set V âŠ†Rm is called con-
cave if
f(Î»0y(0) +Î»1y(1)) â‰¥Î»0 f(y(0))+Î»1 f(y(1))
for any y(0), y(1) âˆˆV and Î»0,Î»1 âˆˆ[0,1] with Î»0+Î»1 = 1. It is called strictly concave
if the equality is attained only when either y(0) = y(1) or Î»0Î»1 = 0. We treat h(X) as
a function of variables p = (p1,..., pm); set V in this case is {y = (y1,...,ym) âˆˆRm:
yi â‰¥0, 1 â‰¤i â‰¤m, y1 +Â·Â·Â·+ym = 1}.
Theorem 1.2.18
Entropy is a strictly concave function of the probability distri-
bution.
Proof
Let the random variables X(i) have probability distributions p(i), i = 0,1,
and assume that the random variable Î› takes values 0 and 1 with probabilities
Î»0 and Î»1, respectively, and is independent of X(0), X(1). Set X = X(Î›); then the
inequality h(Î»0p(0) +Î»1p(1)) â‰¥Î»0h(p(0)) + Î»1h(p(1)) is equivalent to
h(X) â‰¥h(X|Î›)
(1.2.38)
which follows from (1.2.19). If we assume equality in (1.2.38), X and Î› must be
independent. Assume in addition that Î»0 > 0 and write, by using independence,
P(X = i,Î› = 0) = P(X = i)P(Î› = 0) = Î»0P(X = i).
The LHS equals Î»0P(X = i|Î› = 0) = Î»0p(0)
i
and the RHS equals Î»0

Î»0p(0)
i
+
Î»1p(1)
i

. We may cancel Î»0 obtaining
(1âˆ’Î»0)p(0)
i
= Î»1p(1)
i ,
i.e. the probability distributions p(0) and p(1) are proportional. Then either they are
equal or Î»1 = 0, Î»0 = 1. The assumption Î»1 > 0 leads to a similar conclusion.
Worked Example 1.2.19
Show that the quantity
Ï(X,Y) = h(X|Y)+h(Y|X)
obeys
Ï(X,Y) = h(X)+h(Y)âˆ’2I(X : Y)
= h(X,Y)âˆ’I(X : Y) = 2h(X,Y)âˆ’h(X)âˆ’h(Y).
Prove that Ï is symmetric, i.e. Ï(X,Y) = Ï(Y,X) â‰¥0, and satisï¬es the triangle
inequality, i.e. Ï(X,Y) + Ï(Y,Z) â‰¥Ï(X,Z). Show that Ï(X,Y) = 0 iff X and Y
are functions of each other. Also show that if Xâ€² and X are functions of each other
then Ï(X,Y) = Ï(Xâ€²,Y). Hence, Ï may be considered as a metric on the set of
the random variables X, considered up to equivalence: X âˆ¼Xâ€² iff X and Xâ€² are
functions of each other.

1.2 Entropy: an introduction
33
Solution Check the triangle inequality
h(X|Z)+h(Z|X) â‰¤h(X|Y)+h(Y|X)+h(Y|Z)+h(Z|Y),
or
h(X,Z) â‰¤h(X,Y)+h(Y,Z)âˆ’h(Y).
To this end, write h(X,Z) â‰¤h(X,Y,Z) and note that h(X,Y,Z) equals
h(X,Z|Y)+h(Y) â‰¤h(X|Y)+h(Z|Y)+h(Y)
= h(X,Y)+h(Y,Z)âˆ’h(Y).
Equality holds iff (i) Y = Ï†(X,Z) and (ii) X, Z are conditionally independent
given Y.
Remark 1.2.20
The property that Ï(X,Z) = Ï(X,Y)+Ï(Y,Z) means that â€˜pointâ€™
Y lies on a â€˜lineâ€™ through X and Z; in other words, that all three points X, Y, Z lie
on a straight line. Conditional independence of X and Z given Y can be stated
in an alternative (and elegant) way: the triple X â†’Y â†’Z satisï¬es the Markov
property (in short: is Markov). Then suppose we have four random variables X1,
X2, X3, X4 such that, for all 1 â‰¤i1 < i2 < i3 â‰¤4, the random variables Xi1 and
Xi3 are conditionally independent given Xi2; this property means that the quadruple
X1 â†’X2 â†’X3 â†’X4 is Markov, or, geometrically, that all four points lie on a
line. The following fact holds: if X1 â†’X2 â†’X3 â†’X4 is Markov then the mutual
entropies satisfy
I(X1 : X3)+I(X2 : X4) = I(X1 : X4)+I(X2 : X3).
(1.2.39)
Equivalently, for the joint entropies,
h(X1,X3)+h(X2,X4) = h(X1,X4)+h(X2,X3).
(1.2.40)
In fact, for all triples Xi1, Xi2, Xi3 as above, in the metric Ï we have that
Ï(Xi1,Xi3) = Ï(Xi1,Xi2)+Ï(Xi2,Xi3),
which in terms of the joint and individual entropies is rewritten as
h(Xi1,Xi3) = h(Xi1,Xi2)+h(Xi2,Xi3)âˆ’h(Xi2).
Then (1.2.39) takes the form
h(X1,X2)+h(X2,X3)âˆ’h(X2)+h(X2,X3)+h(X3,X4)âˆ’h(X3)
= h(X1,X2)+h(X2,X3)âˆ’h(X2)+h(X3,X4)+h(X2,X3)âˆ’h(X3)
which is a trivial identity.

34
Essentials of Information Theory
Worked Example 1.2.21
Consider the following inequality. Let a triple X â†’
Y â†’Z be Markov where Z is a random string (Z1,...,Zn). Then
âˆ‘
1â‰¤iâ‰¤n
I(X : Zi) â‰¤I(X,Y)+I

Z

where I

Z

:= âˆ‘
1â‰¤iâ‰¤n
h(Zi)âˆ’h

Z

.
Solution The Markov property for X â†’Y â†’Z leads to the bound
I

X : Z

â‰¤I(X : Y).
Therefore, it sufï¬ces to verify that
âˆ‘
1â‰¤iâ‰¤n
I(X : Zi)âˆ’I

Z

â‰¤I

X : Z

.
(1.2.41)
As we show below, bound (1.2.41) holds for any X and Z (without referring to a
Markov property). Indeed, (1.2.41) is equivalent to
nh(X)âˆ’âˆ‘
1â‰¤iâ‰¤n
h(X,Zi)+h

Z

â‰¤h(X)+h

Z

âˆ’h

X,Z

or
h

X,Z

âˆ’h(X) â‰¤âˆ‘
1â‰¤iâ‰¤n
h(X,Zi)âˆ’nh(X)
which in turn is nothing but the inequality h

Z|X

â‰¤
âˆ‘
1â‰¤iâ‰¤n
h(Zi|X).
Worked Example 1.2.22
Write h(p) := âˆ’
m
âˆ‘
1
p j log p j for a probability â€˜vectorâ€™
p =
â›
âœ
â
p1
...
pm
â
âŸ
â , with entries pj â‰¥0 and p1 +Â·Â·Â·+ pm = 1.
(a) Show that h(Pp) â‰¥h(p) if P = (Pi j) is a doubly stochastic matrix (i.e. a square
matrix with elements Pi j â‰¥0 for which all row and column sums are unity).
Moreover, h(Pp) â‰¡h(p) iff P is a permutation matrix.
(b) Show that h(p) â‰¥âˆ’
m
âˆ‘
j=1
m
âˆ‘
k=1
p jPjk logPjk if P is a stochastic matrix and p is an
invariant vector of P: Pp = p.
Solution (a) By concavity of the log-function x â†’logx, for all Î»i,ci â‰¥0 such
that
m
âˆ‘
1
Î»i = 1, we have log(Î»1c1 +Â·Â·Â·+Î»mcm) â‰¥
m
âˆ‘
1
Î»i logci. Apply this to h(Pp) =
âˆ’âˆ‘
i, j
Pijp j log

âˆ‘
k
Pikpk

â‰¥âˆ’âˆ‘
j
p j log

âˆ‘
i,k
PijPikpk

= âˆ’âˆ‘
j
p j log

PTPp

j

. By
the Gibbs inequality the RHS â‰¥h(p). The equality holds iff PTPp â‰¡p, i.e. PTP =
I, the unit matrix. This happens iff P is a permutation matrix.

1.2 Entropy: an introduction
35
(b) The LHS equals h(Un) for the stationary Markov source (U1,U2,...) with equi-
librium distribution p, whereas the RHS is h(Un|Unâˆ’1). The general inequality
h(Un|Unâˆ’1) â‰¤h(Un) gives the result.
Worked Example 1.2.23
The sequence of random variables {Xj : j = 1,2,...}
forms a DTMC with a ï¬nite state space.
(a) Quoting standard properties of conditional entropy, show that h(Xj|Xjâˆ’1) â‰¤
h(Xj|Xjâˆ’2) and, in the case of a stationary DTMC, h(Xj|Xjâˆ’2) â‰¤2h(Xj|Xjâˆ’1).
(b) Show that the mutual information I(Xm : Xn) is non-decreasing in m and non-
increasing in n, 1 â‰¤m â‰¤n.
Solution (a) By the Markov property and stationarity
h(Xj|Xjâˆ’1)
= h(Xj|Xjâˆ’1,Xjâˆ’2)
â‰¤h(Xj|Xjâˆ’2) â‰¤h(Xj,Xjâˆ’1|Xjâˆ’2)
= h(Xj|Xjâˆ’1,Xjâˆ’2)+h(Xjâˆ’1|Xjâˆ’2) = 2h(Xj|Xjâˆ’1).
(b) Write
I(Xm : Xn)âˆ’I(Xm : Xn+1) = h(Xm|Xn+1)âˆ’h(Xm|Xn)
= h(Xm|Xn+1)âˆ’h(Xm|Xn,Xn+1) (because Xm and
Xn+1 are conditionally independent, given Xn)
which is â‰¥0. Thus, I(Xm : Xn) does not increase with n.
Similarly,
I(Xmâˆ’1 : Xn)âˆ’I(Xm : Xn) = h(Xn|Xmâˆ’1)âˆ’h(Xn|Xm,Xmâˆ’1) â‰¥0.
Thus, I(Xm : Xn) does not decrease with m.
Here, no assumption of stationarity has been used. The DTMC may not even be
time-homogeneous (i.e. the transition probabilities may depend not only on i and j
but also on the time of transition).
Worked Example 1.2.24
Given random variables Y1, Y2, Y3, deï¬ne
I(Y1 : Y2|Y3) = h(Y1|Y3)+h(Y2|Y3)âˆ’h(Y1,Y2|Y3).
Now let the sequence Xn, n = 0,1,... be a DTMC. Show that
I(Xnâˆ’1 : Xn+1|Xn) = 0 and hence I(Xnâˆ’1 : Xn+1) â‰¤I(Xn : Xn+1).
Show also that I(Xn : Xn+m) is non-increasing in m, for m = 0,1,2,....

36
Essentials of Information Theory
Solution By the Markov property, Xnâˆ’1 and Xn+1 are conditionally independent,
given Xn. Hence,
h(Xnâˆ’1,Xn+1|Xn) = h(Xn+1|Xn)+h(Xnâˆ’1|Xn)
and I(Xnâˆ’1 : Xn+1|Xn) = 0. Also,
I(Xn : Xn+m)âˆ’I(Xn : Xn+m+1)
= h(Xn+m)âˆ’h(Xn+m+1)âˆ’h(Xn,Xn+m+1)+h(Xn,Xn+m)
= h(Xn|Xn+m+1)âˆ’h(Xn|Xn+m)
= h(Xn|Xn+m+1)âˆ’h(Xn|Xn+m,Xn+m+1) â‰¥0,
the ï¬nal equality holding because of the conditional independence and the last
inequality following from (1.2.21).
Worked Example 1.2.25
(An axiomatic deï¬nition of entropy)
(a) Consider a probability distribution (p1,..., pm) and an associated measure of
uncertainty (entropy) such that
h(p1q1, p1q2,..., p1qn, p2, p3,..., pm) = h(p1,..., pm)+ p1h(q1,...,qn),
(1.2.42)
if (q1,...,qn) is another distribution. That is, if one of the contingencies (of
probability p1) is divided into sub-contingencies of conditional probabilities
q1,...,qn, then the total uncertainty breaks up additively as shown. The func-
tional h is assumed to be symmetric in its arguments, so that analogous rela-
tions holds if contingencies 2,3,...,m are subdivided.
Suppose that F(m) := h

1/m,...,1/m

is monotone increasing in m. Show
that, as a consequence of (1.2.42), F(mk) = kF(m) and hence that F(m) =
clogm for some constant c. Hence show that
h(p1,..., pm) = âˆ’câˆ‘
j
p j log p j
(1.2.43)
if p j are rational. The validity of (1.2.43) for an arbitrary collection {pj} then
follows by a continuity assumption.
(b) An alternative axiomatic characterisation of entropy is as follows. If a symmet-
ric function h obeys for any k < m
h(p1,..., pm) = h(p1 +Â·Â·Â·+ pk, pk+1,..., pm)
+(p1 +Â·Â·Â·+ pk)h

p1
p1 +Â·Â·Â·+ pk
,...,
pk
p1 +Â·Â·Â·+ pk

,
(1.2.44)

1.2 Entropy: an introduction
37
h(1/2,1/2) = 1, and h(p,1âˆ’p) is a continuous function of p âˆˆ[0,1], then
h(p1,..., pm) = âˆ’âˆ‘
j
p j log p j.
Solution (a) Using (1.2.42), we obtain for the function F(m) = h

1/m,...,1/m

the following identity:
F(m2) = h
 1
m Ã— 1
m,..., 1
m Ã— 1
m, 1
m2 ,..., 1
m2

= h
 1
m, 1
m2 ,..., 1
m2

+ 1
mF(m)
...
= h
 1
m,..., 1
m

+ m
mF(m) = 2F(m).
The induction hypothesis is F(mkâˆ’1) = (k âˆ’1)F(m). Then
(mk) = h
 1
m Ã—
1
mkâˆ’1 ,..., 1
m Ã—
1
mkâˆ’1 , 1
mk ,..., 1
mk

= h

1
mkâˆ’1 , 1
mk ,..., 1
mk

+ 1
mF(m)
...
= h

1
mkâˆ’1 ,...,
1
mkâˆ’1

+ m
mF(m)
= (k âˆ’1)F(m)+F(m) = kF(m).
Now, for given positive integers b > 2 and m, we can ï¬nd a positive integer n such
that 2n â‰¤bm â‰¤2n+1, i.e.
n
m â‰¤log2 b â‰¤n
m + 1
m.
By monotonicity of F(m), we obtain nF(2) â‰¤mF(b) â‰¤(n+1)F(2), or
n
m â‰¤F(b)
F(2) â‰¤n
m + 1
m.
We conclude that
log2 bâˆ’F(b)
F(2)
 â‰¤1
m, and letting m â†’âˆ, F(b) = clogb with
c = F(2).

38
Essentials of Information Theory
Now take rational numbers p1 = r1
r ,..., pm = rm
r and obtain
h
r1
r ,..., rm
r

= h
r1
r Ã— 1
r1
,..., r1
r Ã— 1
r1
, r2
r ,..., rm
r

âˆ’r1
r F(r1)
...
= h
1
r ,..., 1
r

âˆ’c âˆ‘
1â‰¤iâ‰¤m
ri
r logri
= clogr âˆ’c âˆ‘
1â‰¤iâ‰¤m
ri
r logri = âˆ’c âˆ‘
1â‰¤iâ‰¤m
ri
r log ri
r .
(b) For the second deï¬nition the point is that we do not assume the monotonicity of
F(m) = h

1/m,...,1/m

in m. Still, using (1.2.44), it is easy to check the additivity
property
F(mn) = F(m)+F(n)
for any positive integers m,n. Hence, for a canonical prime number decomposition
m = qÎ±1
1 ...qÎ±s
s we obtain
F(m) = Î±1F(q1)+Â·Â·Â·+Î±sF(qs).
Next, we prove that
F(m)
m
â†’0, F(m)âˆ’F(mâˆ’1) â†’0
(1.2.45)
as m â†’âˆ. Indeed,
F(m) = h
 1
m,..., 1
m

= h
 1
m, mâˆ’1
m

+ mâˆ’1
m
h

1
mâˆ’1,...,
1
mâˆ’1

,
i.e.
h
 1
m, mâˆ’1
m

= F(m)âˆ’mâˆ’1
m
F(mâˆ’1).
By continuity and symmetry of h(p,1âˆ’p),
lim
mâ†’âˆh
 1
m, mâˆ’1
m

= h(0,1) = h(1,0).
But from the representations
h
1
2, 1
2,0

= h
1
2, 1
2

+ 1
2h(1,0)

1.2 Entropy: an introduction
39
and (the symmetry again)
h
1
2, 1
2,0

= h

0, 1
2, 1
2

= h(1,0)+h
1
2, 1
2

we obtain h(1,0) = 0. Hence,
lim
mâ†’âˆ

F(m)âˆ’mâˆ’1
m
F(mâˆ’1)

= 0.
(1.2.46)
Next, we write
mF(m) =
m
âˆ‘
k=1
k

F(k)âˆ’k âˆ’1
k
F(k âˆ’1)

or, equivalently,
F(m)
m
= m+1
2m

2
m(m+1)
m
âˆ‘
k=1
k

F(k)âˆ’k âˆ’1
k
F(k âˆ’1)


.
The quantity in the square brackets is the arithmetic mean of m(m+1)/2 terms of
a sequence
F(1),F(2)âˆ’F(1),F(2)âˆ’F(1),F(3)âˆ’2
3F(2),F(3)âˆ’2
3F(2),
F(3)âˆ’2
3F(2),...,F(k)âˆ’k âˆ’1
k
F(k âˆ’1),...,
F(k)âˆ’k âˆ’1
k
F(k âˆ’1),...
that tends to 0. Hence, it goes to 0 and F(m)/m â†’0. Furthermore,
F(m)âˆ’F(mâˆ’1) =

F(m)âˆ’mâˆ’1
m
F(mâˆ’1)

âˆ’1
mF(mâˆ’1) â†’0,
and (1.2.46) holds. Now deï¬ne
c(m) = F(m)
logm,
and prove that c(m) = const. It sufï¬ces to prove that c(p) = const for any prime
number p. First, let us prove that a sequence (c(p)) is bounded. Indeed, suppose the
numbers c(p) are not bounded from above. Then, we can ï¬nd an inï¬nite sequence
of primes p1, p2,..., pn,... such that pn is the minimal prime such that pn > pnâˆ’1
and c(pn) > c(pnâˆ’1). By construction, if a prime q < pn then c(q) < c(pn).

40
Essentials of Information Theory
Consider the canonical decomposition into prime factors of the number pn âˆ’1 =
qÎ±1
1 ...qÎ±s
s with q1 = 2. Then we write the difference F(pn)âˆ’F(pn âˆ’1) as
F(pn)âˆ’F(pn)
log pn
log(pn âˆ’1)+c(pn)log(pn âˆ’1)âˆ’F(pn âˆ’1)
= F(pn)
pn
pn
log pn
log
pn
pn âˆ’1 +
s
âˆ‘
j=1
Î± j(c(pn)âˆ’c(qj))logqj.
The previous remark implies that
s
âˆ‘
j=1
Î± j(c(pn)âˆ’c(qj))logq j â‰¥(c(pn)âˆ’c(2))log2 = (c(pn)âˆ’c(2)).
(1.2.47)
Moreover, as lim
pâ†’âˆ
p
log p log
p
pâˆ’1 = 0, equations (1.2.46) and (1.2.47) imply that
c(pn) âˆ’c(2) â‰¤0 which contradicts with the construction of c(p). Hence, c(p) is
bounded from above. Similarly, we check that c(p) is bounded from below. More-
over, the above proof yields that supp c(p) and infp c(p) are both attained.
Now assume that c(p) = supp c(p) > c(2). Given a positive integer m, decom-
pose into prime factors pm âˆ’1 = qÎ±1
1 ...qÎ±s
s
with q1 = 2. Arguing as before, we
write the difference F(pm)âˆ’F(pm âˆ’1) as
F(pm)âˆ’F(pm)
log pm log(pm âˆ’1)+c(p)log(pm âˆ’1)âˆ’F(pm âˆ’1)
= F(pm)
pm
pm
log pm log
pm
pm âˆ’1 +
s
âˆ‘
j=1
Î± j(c(p)âˆ’c(qj))logqj
â‰¥c(pm)
pm
pm
log pm log
pm
pm âˆ’1 +(c(p)âˆ’c(2)).
As before, the limit m â†’âˆyields c(p) âˆ’c(2) â‰¤0 which gives a contradiction.
Similarly, we can prove that infp c(p) = c(2). Hence, c(p) = c is a constant, and
F(m) = clogm. From the condition F(2) = h

1
2, 1
2

= 1 we get c = 1. Finally, as
in (a), we obtain
h(p1,..., pm) = âˆ’
m
âˆ‘
i=1
pi log pi
(1.2.48)
for any rational p1,..., pm â‰¥0 with
m
âˆ‘
i=1
pi = 1. By continuity argument (1.2.48) is
extended to the case of irrational probabilities.
Worked Example 1.2.26
Show that â€˜more homogeneousâ€™ distributions have a
greater entropy. That is, if p = (p1,..., pn) and q = (q1,...,qn) are two probabil-
ity distributions on the set {1,...,n}, then p is called more homogeneous than q

1.3 Shannonâ€™s ï¬rst coding theorem. The entropy rate of a Markov source
41
(p âª¯q, cf. [108]) if, after rearranging values p1,..., pn and q1,...,qn in decreasing
order:
p1 â‰¥Â·Â·Â· â‰¥pn,
q1 â‰¥Â·Â·Â· â‰¥qn,
one has
k
âˆ‘
i=1
pi â‰¤
k
âˆ‘
i=1
qi,
for all k = 1,...,n.
Then
h(p) â‰¥h(q) whenever p âª¯q.
Solution We write the probability distributions p and q as non-increasing functions
of a discrete argument
p âˆ¼p(1) â‰¥Â·Â·Â· â‰¥p(n) â‰¥0,q âˆ¼q(1) â‰¥Â·Â·Â· â‰¥q(n) â‰¥0,
with âˆ‘
i
p(i) = âˆ‘
i
q(i) = 1.
Condition p âª¯q means that if p Ì¸= q then there exist i1 and i2 such that (a) 1 â‰¤i1 â‰¤
i2 â‰¤n, (b) q(i1) > p(i1) â‰¥p(i2) > q(i2) and (c) q(i) â‰¥p(i) for 1 â‰¤i â‰¤i1, q(i) â‰¤p(i) for
i â‰¥i2.
Now apply induction in s, the number of values i = 1,...,n for which q(i) Ì¸= p(i).
If s = 0 we have p = q and the entropies coincide. Make the induction hypothesis
and then increase s by 1. Take a pair i1,i2 as above. Increase q(i2) and decrease q(i1)
so that the sum q(i1)+q(i2) is preserved, until either q(i1) reaches p(i1) or q(i2) reaches
p(i2) (see Figure 1.7). Property (c) guarantees that the modiï¬ed distributions p âª¯q.
As the function x â†’Î·(x) = âˆ’xlogx âˆ’(1 âˆ’x)log(1 âˆ’x) strictly increases on
[0,1/2]. Hence, the entropy of the modiï¬ed distribution strictly increases. At the
end of this process we diminish s. Then we use our induction hypothesis.
1.3 Shannonâ€™s ï¬rst coding theorem. The entropy rate of a
Markov source
A useful meaning of the information rate of a source is that it speciï¬es the mini-
mal rates of growth for the set of sample strings carrying, asymptotically, the full
probability.
Lemma 1.3.1
Let H be the information rate of a source (see (1.1.20)). Deï¬ne
Dn(R) := max
	
P(U(n) âˆˆA): A âŠ‚IÃ—n, â™¯A â‰¤2nR
.
(1.3.1)

42
Essentials of Information Theory
.
i1
i2
1
n
P
Q
Figure 1.7
Then for any Îµ > 0, as n â†’âˆ,
lim Dn(H +Îµ) = 1, and, if H > 0, Dn(H âˆ’Îµ) Ì¸â†’1.
(1.3.2)
Proof
By deï¬nition, R := H +Îµ is a reliable encoding rate. Hence, there exists a
sequence of sets An âŠ‚IÃ—n, with â™¯An â‰¤2nR and P(U(n) âˆˆAn) â†’1, as n â†’âˆ. Since
Dn(R) â‰¥P(U(n) âˆˆAn), then Dn(R) â†’1.
Now suppose that H > 0, and take R := H âˆ’Îµ; for Îµ small enough, R > 0.
However, R is not a reliable rate. That is, there is no sequence An with the above
properties. Take a set Cn where the maximum in (1.3.1) is attained. Then â™¯Cn â‰¤2nR,
but P(Cn) Ì¸â†’1.
Given a string u(n) = u1 ...un, consider its â€˜log-likelihoodâ€™ value per source-
letter:
Î¾n(u(n)) = âˆ’1
n log+ pn(u(n)), u(n) âˆˆIÃ—n,
(1.3.3a)
where pn(u(n)) := P(U(n) = u(n)) is the probability assigned to string u(n). Here
and below, log+ x = logx if x > 0, and is 0 if x = 0. For a random string, U(n) =
u1,...,un,
Î¾n(U(n)) = âˆ’1
n log+ pn(U(n))
(1.3.3b)
is a random variable.
Lemma 1.3.2
For all R, Îµ > 0,
P(Î¾n â‰¤R) â‰¤Dn(R) â‰¤P(Î¾n â‰¤R+Îµ)+2âˆ’nÎµ.
(1.3.4)

1.3 Shannonâ€™s ï¬rst coding theorem. The entropy rate of a Markov source
43
Proof
For brevity, omit the upper index (n) in the notation u(n) and U(n). Set
Bn := {u âˆˆIÃ—n : pn(u) â‰¥2âˆ’nR}
= {u âˆˆIÃ—n : âˆ’log pn(u) â‰¤nR}
= {u âˆˆIÃ—n : Î¾n(u) â‰¤R}.
Then
1 â‰¥P(U âˆˆBn) = âˆ‘
uâˆˆBn
pn(u) â‰¥2âˆ’nR â™¯Bn, whence â™¯Bn â‰¤2nR.
Thus,
Dn(R) = max
	
P(U âˆˆAn) : An âŠ†IÃ—n, â™¯A â‰¤2nR
â‰¥P(U âˆˆBn) = P(Î¾n â‰¤R),
which proves the LHS in (1.3.4).
On the other hand, there exists a set Cn âŠ†IÃ—n where the maximum in (1.3.1) is
attained. For such a set, Dn(R) = P(U âˆˆCn) is decomposed as follows:
Dn(R) = P(U âˆˆCn,Î¾n â‰¤R+Îµ)+P(U âˆˆCn, Î¾n > R+Îµ)
â‰¤P(Î¾n â‰¤R+Îµ)+ âˆ‘
uâˆˆCn
pn(u)1

pn(u) < 2âˆ’n(R+Îµ)
< P(Î¾n â‰¤R+Îµ)+2âˆ’n(R+Îµ) â™¯Cn
= P(Î¾n â‰¤R+Îµ)+2âˆ’n(R+Îµ) 2nR
= P(Î¾n â‰¤R+Îµ)+2âˆ’nÎµ.
Deï¬nition 1.3.3
(See PSE II, p. 367.) A sequence of random variables {Î·n}
converges in probability to a constant r if, for all Îµ > 0,
lim
nâ†’âˆP

|Î·n âˆ’r| â‰¥Îµ

= 0.
(1.3.5)
Replacing, in this deï¬nition, r by a random variable Î·, we obtain a more general
deï¬nition of convergence in probability to a random variable.
Convergence in probability is denoted henceforth as Î·n
P
âˆ’â†’r (respectively,
Î·n
P
âˆ’â†’Î·).
Remark 1.3.4
It is precisely the convergence in probability (to an expected
value) that ï¬gures in the so-called law of large numbers (cf. (1.3.8) below). See
PSE I, p. 78.
Theorem 1.3.5
(Shannonâ€™s ï¬rst coding theorem (FCT)) If Î¾n converges in prob-
ability to a constant Î³ then Î³ = H, the information rate of a source.

44
Essentials of Information Theory
Proof
Let Î¾n
P
âˆ’â†’Î³. Since Î¾n â‰¥0, Î³ â‰¥0. By Lemma 1.3.2, for any Îµ > 0,
Dn(Î³ +Îµ) â‰¥P(Î¾n â‰¤Î³ +Îµ) â‰¥P(Î³ âˆ’Îµ â‰¤Î¾n â‰¤Î³ +Îµ)
= P

|Î¾n âˆ’Î³| â‰¤Îµ

= 1âˆ’P

|Î¾n âˆ’Î³| > Îµ

â†’1 (n â†’âˆ).
Hence, H â‰¤Î³. In particular, if Î³ = 0 then H = 0. If Î³ > 0, we have, again by Lemma
1.3.2, that
Dn(Î³ âˆ’Îµ) â‰¤P(Î¾n â‰¤Î³ âˆ’Îµ/2)+2âˆ’nÎµ/2 â‰¤P

|Î¾n âˆ’Î³| â‰¥Îµ/2

+2âˆ’nÎµ/2 â†’0.
By Lemma 1.3.1, H â‰¥Î³. Hence, H = Î³.
Remark 1.3.6
(a) Convergence Î¾n
P
âˆ’â†’Î³ = H is equivalent to the following
asymptotic equipartition property: for any Îµ > 0,
lim
nâ†’âˆP

2âˆ’n(H+Îµ) â‰¤pn(U(n)) â‰¤2âˆ’n(Hâˆ’Îµ)
= 1.
(1.3.6)
In fact,
P

2âˆ’n(H+Îµ) â‰¤pn(U(n)) â‰¤2âˆ’n(Hâˆ’Îµ)
= P

H âˆ’Îµ â‰¤âˆ’1
n log pn(U(n)) â‰¤H +Îµ

= P

|Î¾n âˆ’H| â‰¤Îµ

= 1âˆ’P

|Î¾n âˆ’H| > Îµ

.
In other words, for all Îµ > 0 there exists n0 = n0(Îµ) such that, for any n > n0, the
set IÃ—n decomposes into disjoint subsets, Î n and Tn, with
(i) P

U(n) âˆˆÎ n

< Îµ,
(ii) 2âˆ’n(H+Îµ) â‰¤P

U(n) = u(n)
â‰¤2âˆ’n(Hâˆ’Îµ) for all u(n) âˆˆTn.
Pictorially speaking, Tn is a set of â€˜typicalâ€™ strings and Î n is the residual set.
We conclude that, for a source with the asymptotic equipartition property, it is
worthwhile to encode the typical strings with codewords of the same length, and
the rest anyhow. Then we have the effective encoding rate H + o(1) bits/source-
letter, though the source emits logm bits/source-letter.
(b) Observe that
EÎ¾n = âˆ’1
n âˆ‘
u(n)âˆˆIÃ—n
pn(u(n))log pn(u(n)) = 1
nh(n).
(1.3.7)
The simplest example of an information source (and one among the most
instructive) is a Bernoulli source.
Theorem 1.3.7
For a Bernoulli source U1,U2,..., with P(Ui = x) = p(x),
H = âˆ’âˆ‘
x
p(x)log p(x).

1.3 Shannonâ€™s ï¬rst coding theorem. The entropy rate of a Markov source
45
Proof
For an IID sequence U1, U2,..., the probability of a string is
pn(u(n)) =
n
âˆ
i=1
p(ui), u(n) = u1 ...un.
Hence, âˆ’log pn(u) = âˆ‘
i
âˆ’log p(ui). Denoting Ïƒi = âˆ’log p(Ui), i = 1,2,..., we
see that Ïƒ1,Ïƒ2,... form a sequence of IID random variables. For a random
string U(n) = U1 ...Un, âˆ’log pn(U(n)) =
n
âˆ‘
i=1
Ïƒi, where the random variables Ïƒi =
âˆ’log p(Ui) are IID.
Next, write Î¾n = 1
n
n
âˆ‘
i=1
Ïƒi. Observe that EÏƒi = âˆ’âˆ‘
j
p( j)log p( j) = h and
EÎ¾n = E

1
n
n
âˆ‘
i=1
Ïƒi

= 1
n
n
âˆ‘
i=1
EÏƒi = 1
n
n
âˆ‘
i=1
h = h,
the ï¬nal equality being in agreement with (1.3.7), since, for the Bernoulli source,
h(n) = nh (see (1.1.18)), and hence EÎ¾n = h. We immediately see that Î¾n
P
âˆ’â†’h by
the law of large numbers. So H = h by Theorem 1.3.5 (FCT).
Theorem 1.3.8
(The law of large numbers for IID random variables) For any
sequence of IID random variables Î·1,Î·2,... with ï¬nite variance and mean EÎ·i = r,
and for any Îµ > 0,
lim
nâ†’âˆP

|1
n
n
âˆ‘
i=1
Î·i âˆ’r| â‰¥Îµ

= 0.
(1.3.8)
Proof
The proof of Theorem 1.3.8 is based on the famous Chebyshev inequality;
see PSE II, p. 368.
Lemma 1.3.9
For any random variable Î· and any Îµ > 0,
P(Î· â‰¥Îµ) â‰¤1
Îµ2 EÎ·2.
Proof
See PSE I, p. 75.
Next, consider a Markov source U1 U2 ... with letters from alphabet Im =
{1,...,m} and assume that the transition matrix

P(u,v)

(or rather its power)
obeys
min
u,v P(r)(u,v) = Ï > 0 for some r â‰¥1.
(1.3.9)

46
Essentials of Information Theory
This condition means that the DTMC is irreducible and aperiodic. Then (see
PSE II, p. 71), the DTMC has a unique invariant (equilibrium) distribution
Ï€(1),...,Ï€(m):
0 â‰¤Ï€(u) â‰¤1,
m
âˆ‘
u=1
Ï€(u) = 1, Ï€(v) =
m
âˆ‘
u=1
Ï€(u)P(u,v),
(1.3.10)
and the n-step transition probabilities P(n)(u,v) converge to Ï€(v) as well as the
probabilities

Î»Pnâˆ’1
(v) = P(Un = v):
lim
nâ†’âˆP(n)(u,v) = lim
nâ†’âˆP(Un = v) = lim
nâ†’âˆâˆ‘
u
Î»(u)P(n)(u,v) = Ï€(v),
(1.3.11)
for all initial distribution {Î»(u), u âˆˆI}. Moreover, the convergence in (1.3.11) is
exponentially (geometrically) fast.
Theorem 1.3.10
Assume that condition (1.3.9) holds with r = 1. Then the DTMC
U1,U2,... possesses a unique invariant distribution (1.3.10), and for any u,v âˆˆI and
any initial distribution Î» on I,
|P(n)(u,v)âˆ’Ï€(v)| â‰¤(1âˆ’Ï)n and |P(Un = v)âˆ’Ï€(v)| â‰¤(1âˆ’Ï)nâˆ’1.
(1.3.12)
In the case of a general r â‰¥1, we replace, in the RHS of (1.3.12), (1 âˆ’Ï)n by
(1âˆ’Ï)[n/r] and (1âˆ’Ï)nâˆ’1 by (1âˆ’Ï)[(nâˆ’1)/r].
Proof
See Worked Example 1.3.13.
Now we introduce an information rate H of a Markov source.
Theorem 1.3.11
For a Markov source, under condition (1.3.9),
H = âˆ’âˆ‘
1â‰¤u,vâ‰¤m
Ï€(u)P(u,v)logP(u,v) = lim
nâ†’âˆh(Un+1|Un);
(1.3.13)
if the source is stationary then H = h(Un+1|Un).
Proof
We again use the Shannon FCT to check that Î¾n
P
âˆ’â†’H where H is given by
(1.3.13), and Î¾n = âˆ’1
n log pn(U(n)), cf. (1.3.3b). In other words, condition (1.3.9)
implies the asymptotic equipartition property for a Markov source.
The Markov property means that, for all string u(n) = u1 ... un,
pn(u(n)) = Î»(u1)P(u1,u2)Â·Â·Â·P(unâˆ’1,un),
(1.3.14a)
and âˆ’log pn(u(n)) is written as the sum
âˆ’logÎ»(u1)âˆ’logP(u1,u2)âˆ’Â·Â·Â·âˆ’logP(unâˆ’1,un).
(1.3.14b)
For a random string, U(n) = U1 ...Un, the random variable âˆ’log pn(U(n)) has a
similar form:
âˆ’logÎ»(U1)âˆ’logP(U1,U2)âˆ’Â·Â·Â·âˆ’logP(Unâˆ’1,Un).
(1.3.15)

1.3 Shannonâ€™s ï¬rst coding theorem. The entropy rate of a Markov source
47
As in the case of a Bernoulli source, we denote
Ïƒ1(U1) := âˆ’logÎ»(U1), Ïƒi(Uiâˆ’1,Ui) := âˆ’logP(Uiâˆ’1,Ui), i â‰¥2,
(1.3.16)
and write
Î¾n = 1
n

Ïƒ1 +
nâˆ’1
âˆ‘
i=1
Ïƒi+1

.
(1.3.17)
The expected value of Ïƒ is
EÏƒ1 = âˆ’âˆ‘
u
Î»(u)logÎ»(u)
(1.3.18a)
and, as P(Ui = v) = Î»Piâˆ’1(v) = âˆ‘
u Î»(u)P(iâˆ’1)(u,v),
EÏƒi+1 = âˆ’âˆ‘
u,uâ€²
P(Ui = u,Ui+1 = uâ€²)logP(u,uâ€²)
= âˆ’âˆ‘
u,uâ€²

Î»Piâˆ’1
(u)P(u,uâ€²)logP(u,uâ€²),
i â‰¥1.
(1.3.18b)
Theorem 1.3.10 implies that lim
iâ†’âˆEÏƒi = H. Hence,
lim
nâ†’âˆEÎ¾n = lim
nâ†’âˆ
1
n
n
âˆ‘
i=1
EÏƒi = H,
and the convergence Î¾n
P
âˆ’â†’H is again a law of large numbers, for the sequence
(Ïƒi):
lim
nâ†’âˆP

1
n
n
âˆ‘
i=1
Ïƒi âˆ’H
 â‰¥Îµ

= 0.
(1.3.19)
However, the situation here is not as simple as in the case of a Bernoulli source.
There are two difï¬culties to overcome: (i) EÏƒi equals H only in the limit i â†’âˆ;
(ii) Ïƒ1,Ïƒ2,... are no longer independent. Even worse, they do not form a DTMC,
or even a Markov chain of a higher order. [A sequence U1,U2,... is said to form a
DTMC of order k, if, for all n â‰¥1,
P(Un+k+1 = uâ€²|Un+k = uk,...,Un+1 = u1,...)
= P(Un+k+1 = uâ€²|Un+k = uk,...,Un+1 = u1).
An obvious remark is that, in a DTMC of order k, the vectors
Â¯Un =
(Un,Un+1,...,Un+kâˆ’1), n â‰¥1, form an ordinary DTMC.] In a sense, the â€˜mem-
oryâ€™ in a sequence Ïƒ1,Ïƒ2,... is inï¬nitely long. However, it decays exponentially:
the precise meaning of this is provided in Worked Example 1.3.14.

48
Essentials of Information Theory
Anyway, by using the Chebyshev inequality, we obtain
P

1
n
n
âˆ‘
i=1
Ïƒi âˆ’H
 â‰¥Îµ

â‰¤
1
n2Îµ2 E

n
âˆ‘
i=1
(Ïƒi âˆ’H)
2
.
(1.3.20)
Theorem 1.3.11 immediately follows from Lemma 1.3.12 below.
Lemma 1.3.12
The expectation value in the RHS of (1.3.20) satisï¬es the bound
E

n
âˆ‘
i=1
(Ïƒi âˆ’H)
2
â‰¤Cn,
(1.3.21)
where C > 0 is a constant that does not depend on n.
Proof
See Worked Example 1.3.14.
By (1.3.21), the RHS of (1.3.20) becomes â‰¤C
nÎµ2 and goes to zero as n â†’âˆ.
Worked Example 1.3.13
Prove the following bound (cf. (1.3.12)):
|P(n)(u,v)âˆ’Ï€(v)| â‰¤(1âˆ’Ï)n.
(1.3.22)
Solution (Compare with PSE II, p. 72.) First, observe that (1.3.12) implies the
second bound in Theorem 1.3.10 as well as (1.3.10). Indeed, Ï€(v) is identiï¬ed as
the limit
lim
nâ†’âˆP(n)(u,v) = lim
nâ†’âˆâˆ‘
 u
P(nâˆ’1)(u,  u)P( u,v) = âˆ‘
 u
Ï€( u)P( u,v),
(1.3.23)
which yields (1.3.10). If Ï€â€²(1),Ï€â€²(2),...,Ï€â€²(m) is another invariant probability
vector, i.e.
0 â‰¤Ï€â€²(u) â‰¤1,
m
âˆ‘
u=1
Ï€â€²(u) = 1, Ï€â€²(v) = âˆ‘
u
Ï€â€²(u)P(u,v),
then Ï€â€²(v) = âˆ‘
u Ï€â€²(u)P(n)(u,v) for all n â‰¥1. The limit n â†’âˆgives then
Ï€â€²(v) = âˆ‘
u
Ï€â€²(u) lim
nâ†’âˆP(n)(u,v) = âˆ‘
u
Ï€â€²(u)Ï€(v) = Ï€(v),
i.e. the invariant probability vector is unique.
To prove (1.3.22) denote
mn(v) = min
u
P(n)(u,v),
Mn(v) = max
u
P(n)(u,v).
(1.3.24)

1.3 Shannonâ€™s ï¬rst coding theorem. The entropy rate of a Markov source
49
Then
mn+1(v) = min
u
P(n+1)(u,v) = min
u âˆ‘
 u
P(u,  u)P(n)( u,v)
â‰¥min
u P(n)(u,v)âˆ‘
 u
P(u,  u) = mn(v).
Similarly,
Mn+1(v) = max
u
P(n+1)(u,v) = max
u âˆ‘
 u
P(u,  u)P(n)( u,v)
â‰¤max
u P(n)(u,v)âˆ‘
 u
P(u,  u) = Mn(v).
Since 0 â‰¤mn(v) â‰¤Mn(v) â‰¤1, both mn(v) and Mn(v) have the limits
m(v) = lim
nâ†’âˆmn(v) â‰¤lim
nâ†’âˆMn(v) = M(v).
Furthermore, the difference M(v)âˆ’m(v) is written as the limit
lim
nâ†’âˆ(Mn(v)âˆ’mn(v)) = lim
nâ†’âˆmax
u,uâ€² (P(n)(u,v)âˆ’P(n)(uâ€²,v)).
So, if we manage to prove that
max
u,uâ€²,v|P(n)(u,v)âˆ’P(n)(uâ€²,v)| â‰¤(1âˆ’Ï)n,
(1.3.25)
then M(v) = m(v) for each v. Furthermore, denoting the common value M(v) =
m(v) by Ï€(v), we obtain (1.3.22)
|P(n)(u,v)âˆ’Ï€(v)| â‰¤Mn(v)âˆ’mn(v) â‰¤(1âˆ’Ï)n.
To prove (1.3.25), consider a DTMC on I Ã—I, with states (u1,u2), and transition
probabilities
P

(u1,u2),(v1,v2)

=
â§
â¨
â©
P(u1,v1)P(u2,v2),
if u1 Ì¸= u2,
P(u,v),
if u1 = u2 = u; v1 = v2 = v,
0,
if u1 = u2 and v1 Ì¸= v2.
(1.3.26)
It is easy to check that P

(u1,u2),(v1,v2)

is indeed a transition probability matrix
(of size m2 Ã—m2): if u1 = u2 = u then
âˆ‘
v1,v2
P

(u1,u2),(v1,v2)

= âˆ‘
v
P(u,v) = 1
whereas if u1 Ì¸= u2 then
âˆ‘
v1,v2
P

(u1,u2),(v1,v2)

= âˆ‘
v1
P(u1,v1)âˆ‘
v2
P(u2,v2) = 1

50
Essentials of Information Theory
(the inequalities 0 â‰¤P

(u1,u2),(v1,v2)

â‰¤1 follow directly from the deï¬nition
(1.3.26)).
This is the so-called coupled DTMC on I Ã— I; we denote it by (Vn,Wn), n â‰¥1.
Observe that both components Vn and Wn are DTMCs with transition probabilities
P(u,v). More precisely, the components Vn and Wn move independently, until the
ï¬rst (random) time Ï„ when they coincide; we call it the coupling time. After time
Ï„ the components Vn and Wn â€˜stickâ€™ together and move synchronously, again with
transition probabilities P(u,v).
Suppose we start the coupled chain from a state (u,uâ€²). Then
|P(n)(u,v)âˆ’P(n)(uâ€²,v)|
= |P(Vn = v|V1 = u,W1 = uâ€²)âˆ’P(Wn = v|V1 = u,W1 = uâ€²)|
(because each component of (Vn,Wn) moves with the same transition probabilities)
= |P(Vn = v,Wn Ì¸= v|V1 = u,W1 = uâ€²)
âˆ’P(Vn Ì¸= v,Wn = v|V1 = u,W1 = uâ€²)|
â‰¤P(Vn Ì¸= Wn|V1 = u,W1 = uâ€²)
= P(Ï„ > n|V1 = u,W1 = uâ€²).
(1.3.27)
Now, the probability obeys
P(Ï„ = 1|V1 = u,W1 = uâ€²) â‰¥âˆ‘
v
P(u,v)P(uâ€²,v) â‰¥Ïâˆ‘
v
P(uâ€²,v) = Ï,
i.e. the complementary probability satisï¬es
P(Ï„ > 1|V1 = u,W1 = uâ€²) â‰¤1âˆ’Ï.
By the strong Markov property (of the coupled chain),
P(Ï„ > n|V1 = u,W1 = uâ€²) â‰¤(1âˆ’Ï)n.
(1.3.28)
Bounds (1.3.28) and (1.3.27) together give (1.3.25).
Worked Example 1.3.14
Under condition (1.3.9) with r = 1 prove the following
bound:
|E
	
(Ïƒi âˆ’H)(Ïƒi+k âˆ’H)

| â‰¤

H +|logÏ|
2(1âˆ’Ï)kâˆ’1.
(1.3.29)

1.3 Shannonâ€™s ï¬rst coding theorem. The entropy rate of a Markov source
51
Solution For brevity, we assume i > 1; the case i = 1 requires minor changes.
Returning to the deï¬nition of random variables Ïƒi, i > 1, write
E
	
(Ïƒi âˆ’H)(Ïƒi+k âˆ’H)

= âˆ‘
u,uâ€²âˆ‘
v,vâ€²
P(Ui = u,Ui+1 = uâ€²;Ui+k = v,Ui+k+1 = vâ€²)
Ã—

âˆ’logP(u,uâ€²)âˆ’H
 
âˆ’logP(v,vâ€²)âˆ’H

.
(1.3.30)
Our goal is to compare this expression with
âˆ‘
u,uâ€²âˆ‘
v,vâ€²

Î»Piâˆ’1
(u)P(u,uâ€²)
	
âˆ’logP(u,uâ€²)âˆ’H

Ã—Ï€(v)P(v,vâ€²)
	
âˆ’logP(v,vâ€²)âˆ’H

.
(1.3.31)
Observe that (1.3.31) in fact vanishes because the sum âˆ‘
v,vâ€²
vanishes due to the deï¬-
nition (1.3.13) of H.
The difference between sums (1.3.30) and (1.3.31) comes from the fact that the
probabilities
P(Ui = u,Ui+1 = uâ€²;Ui+k = v,Ui+k+1 = vâ€²)
=

Î»Piâˆ’1
(u)P(u,uâ€²)P(kâˆ’1)(uâ€²,v)P(v,vâ€²)
and

Î»Piâˆ’1
(u)P(u,uâ€²)Ï€(v)P(v,vâ€²)
do not coincide. However, the difference of these probabilities in absolute value
does not exceed
|P(kâˆ’1)(uâ€²,v)âˆ’Ï€(v)| â‰¤(1âˆ’Ï)kâˆ’1.
As |âˆ’logP(Â·, Â·)âˆ’H| â‰¤H +|logÏ|, we obtain (1.3.29).
Proofof Theorem 1.3.11. This is now easy to complete. To prove (1.3.21), expand
the square and use the additivity of the expectation:
E

n
âˆ‘
i=1
(Ïƒi âˆ’H)
2
= âˆ‘
1â‰¤iâ‰¤n
E
	
(Ïƒi âˆ’H)2
+2 âˆ‘
1â‰¤i< jâ‰¤n
E
	
(Ïƒi âˆ’H)(Ïƒ j âˆ’H)

.
(1.3.32)
The ï¬rst sum in (1.3.32) is OK: it contains n terms E(Ïƒi âˆ’H)2 each bounded by a
constant (say, Câ€² may be taken to be

H +|logÏ|
2). Thus this sum is at most Câ€²n.

52
Essentials of Information Theory
It is the second sum that causes problems: it contains n(nâˆ’1)

2 terms. We bound
it as follows:
 âˆ‘
1â‰¤i< jâ‰¤n
E
	
(Ïƒi âˆ’H)(Ïƒ j âˆ’H)

 â‰¤
n
âˆ‘
i=1

âˆ
âˆ‘
k=1
|E
	
(Ïƒi âˆ’H)(Ïƒi+k âˆ’H)

|

, (1.3.33)
and use (1.3.29) to ï¬nish the proof.
Our next theorem shows the role of the (relative) entropy in the asymptotic anal-
ysis of probabilities; see PSE I, p. 82.
Theorem 1.3.15
Let Î¶1,Î¶2,... be a sequence of IID random variables taking
values 0 and 1 with probabilities 1 âˆ’p and p, respectively, 0 < p < 1. Then, for
any sequence kn of positive integers such that kn â†’âˆand nâˆ’kn â†’âˆas n â†’âˆ,
P

n
âˆ‘
i=1
Î¶i = kn

âˆ¼(2Ï€npâˆ—(1âˆ’pâˆ—))âˆ’1/2 exp(âˆ’nD(p||pâˆ—)).
(1.3.34)
Here, âˆ¼means that the ratio of the left- and right-hand sides tends to 1 as n â†’âˆ,
pâˆ—(= pâˆ—
n) denotes the ratio kn
n , and D(p||pâˆ—) stands for the relative entropy h(X||Y)
where X is distributed as Î¶i (i.e. it takes values 0 and 1 with probabilities 1 âˆ’p
and p), while Y takes the same values with probabilities 1âˆ’pâˆ—and pâˆ—.
Proof
Use Stirlingâ€™s formula (see PSE I, p.72):
n! âˆ¼
âˆš
2Ï€nnneâˆ’n.
(1.3.35)
[In fact, this formula admits a more precise form: n! =
âˆš
2Ï€nnneâˆ’n+Î¸(n), where
1
12n+1 < Î¸(n) <
1
12n, but for our purposes (1.3.35) is enough.] Then the proba-
bility in the LHS of (1.3.34) is (for brevity, the subscript n in kn is omitted)
n
k

pk(1âˆ’p)nâˆ’k âˆ¼

n
2Ï€k(nâˆ’k)
1/2
nn
kk(nâˆ’k)nâˆ’k pk(1âˆ’p)nâˆ’k
=

2Ï€npâˆ—(1âˆ’pâˆ—)
âˆ’1/2
Ã—exp [âˆ’klnk/nâˆ’(nâˆ’k)ln(nâˆ’k)/n+kln p+(nâˆ’k)ln(1âˆ’p)].
But the RHS of the last formula coincides with the RHS of (1.3.34).
If pâˆ—is close to p, we can write
D(p||pâˆ—) = 1
2
1
p +
1
1âˆ’p

(pâˆ—âˆ’p)2 +O(|pâˆ—âˆ’p|3),
(1.3.36)
as D(p||pâˆ—)|pâˆ—=p =
 d
dpâˆ—D(p||pâˆ—)

|pâˆ—=p = 0, and immediately obtain

1.3 Shannonâ€™s ï¬rst coding theorem. The entropy rate of a Markov source
53
Corollary 1.3.16
(The local De Moivreâ€“Laplace theorem; cf. PSE I, p. 81) If
n(pâˆ—âˆ’p) = kn âˆ’np = o(n2/3) then
P

n
âˆ‘
i=1
Î¶i = kn

âˆ¼
1
$
2Ï€np(1âˆ’p)
exp

âˆ’
n
2p(1âˆ’p)(pâˆ—âˆ’p)2

.
(1.3.37)
Worked Example 1.3.17
At each time unit a device reads the current version
of a string of N characters each of which may be either 0 or 1. It then transmits
the number of characters which are equal to 1. Between each reading the string is
perturbed by changing one of the characters at random (from 0 to 1 or vice versa,
with each character being equally likely to be changed). Determine an expression
for the information rate of this source.
Solution The source is Markov, with the state space {0,1,...,N} and the transition
probability matrix
â›
âœ
âœ
âœ
âœ
âœ
âœ
âœ
â
0
1
0
0
...
0
0
1/N
0
(N âˆ’1)/N
0
...
0
0
0
2/N
0
(N âˆ’2)/N
...
0
0
...
...
0
0
0
0
...
0
1/N
0
0
0
0
...
1
0
â
âŸ
âŸ
âŸ
âŸ
âŸ
âŸ
âŸ
â 
.
The DTMC is irreducible and periodic. It possesses a unique invariant distribution
Ï€i = 2âˆ’N
N
i

, 0 â‰¤i â‰¤N.
By Theorem 1.3.11,
H = âˆ’âˆ‘
i, j
Ï€iP(i, j)logP(i, j) = 21âˆ’N 1
N
Nâˆ’1
âˆ‘
j=1
N
j

jlog N
j .
Worked Example 1.3.18
A stationary source emits symbols 0,1,...,m (m â‰¥4 is
an even number), according to a DTMC, with the following transition probabilities
p jk = P(Un+1 = k | Un = j):
p j j+2 = 1/3, 0 â‰¤j â‰¤mâˆ’2, pj jâˆ’2 = 1/3, 2 â‰¤j â‰¤m,
pj j = 1/3, 2 â‰¤j â‰¤mâˆ’2, p00 = p11 = pmâˆ’1mâˆ’1 = pmm = 2/3.
The distribution of the ï¬rst symbol is equiprobable. Find the information rate of
the source. Does the result contradict Shannonâ€™s FCT?

54
Essentials of Information Theory
How does the answer change if m is odd? How can you use, for m odd, Shannonâ€™s
FCT to derive the information rate of the above source?
Solution For m even, the DTMC is reducible: there are two communicating classes,
I1 = {0,2,...,m} with m/2 + 1 states, and I2 = {1,3,...,m âˆ’1} with m/2 states.
Correspondingly, for any set An of n-strings,
P(An) = qP1(An1)+(1âˆ’q)P2(An2),
(1.3.38)
where An1 = An âˆ©I1 and An1 = An âˆ©I2; Pi refers to the DTMC on class Ii, i = 1,2,
and q = P(U1 âˆˆI1).
The random variable from (1.3.3b) is Î¾n = âˆ’1
n log pn(U(n)); according to
(1.3.38),
Î¾n = âˆ’1
n log pn1(U(n)) with probability q,
= âˆ’1
n log pn2(U(n)) with probability 1âˆ’q.
(1.3.39)
Both DTMCs are irreducible and aperiodic on their communicating classes and
their invariant distributions are uniform:
Ï€(1)
i
=
2
m+2, i âˆˆI1, Ï€(2)
i
= 2
m, i âˆˆI2.
Their information rates equal, respectively,
H(1) = log3âˆ’
8
3(m+2) and H(2) = log3âˆ’8
3m.
(1.3.40)
As follows from (1.3.38), the information rate of the whole DTMC equals
Hodd =
% H(1) = max [H(1),H(2)],
if 0 < q â‰¤1,
H(2),
if q = 0.
(1.3.41)
For 0 < q < 1 Shannonâ€™s FCT is not applicable:
âˆ’1
n log pn1(U(n))
P1
âˆ’â†’H(1) whereas âˆ’1
n log pn2(U(n))
P2
âˆ’â†’H(2),
i.e. Î¾n converges to a non-constant limit. However, if q(1 âˆ’q) = 0, then (1.3.41)
is reduced to a single line, and Shannonâ€™s FCT is applicable: Î¾n converges to the
corresponding constant H(i).
If m is odd, again there are two communicating classes, I1 = {0,2,...,m âˆ’1}
and I2 = {1,3,...,m}, each of which now contains (m + 1)/2 states. As before,

1.3 Shannonâ€™s ï¬rst coding theorem. The entropy rate of a Markov source
55
DTMCs P1 and P2 are irreducible and aperiodic and their invariant distributions
are uniform:
Ï€(1)
i
=
2
m+1, i âˆˆI1, Ï€(2)
i
=
2
m+1, i âˆˆI2.
Their common information rate equals
Hodd = log3âˆ’
8
3(m+1),
(1.3.42)
which also gives the information rate of the whole DTMC. It agrees with Shannonâ€™s
FCT, because now
Î¾n = âˆ’1
n log pn(U(n))
P
âˆ’â†’Hodd.
(1.3.43)
Worked Example 1.3.19
Let a be the size of A and b the size of the alphabet B.
Consider a source with letters chosen from an alphabet A + B, with the constraint
that no two letters of A should ever occur consecutively.
(a) Suppose the message follows a DTMC, all characters which are permitted at a
given place being equally likely. Show that this source has information rate
H = alogb+(a+b)log(a+b)
2a+b
.
(1.3.44)
(b) By solving a recurrence relation, or otherwise, ï¬nd how many strings of length
n satisfy the constraint that no two letters of A occur consecutively. Suppose
these strings are equally likely and let n â†’âˆ. Show that the limiting informa-
tion rate becomes
H = log

b+
âˆš
b2 +4ab
2

.
Why are the answers different?
Solution (a) The transition probabilities of the DTMC are given by
P(x,y) =
â§
âª
â¨
âª
â©
0,
if x,y âˆˆ{1,...,a},
1/b,
if x âˆˆ{1,...,a}, y âˆˆ{a+1,...,a+b},
1/(a+b),
if x âˆˆ{a+1,...,a+b},y âˆˆ{1,...,a+b}.
The chain is irreducible and aperiodic. Moreover, minP(2)(x,y) > 0; hence, an
invariant distribution Ï€ =

Ï€(x), x âˆˆ{1,...,a+b}

is unique. We can ï¬nd Ï€ from

56
Essentials of Information Theory
the detailed balance equations (DBEs) Ï€(x)P(x,y) = Ï€(y)P(y,x) (cf. PSE II, p. 82),
which yields
Ï€(x) =
'
1

(2a+b),
x âˆˆ{1,...,a},
(a+b)

[b(2a+b)],
x âˆˆ{a+1,...,a+b}.
The DBEs imply that Ï€ is invariant: Ï€(y) = âˆ‘
x Ï€(x)P(x,y), but not vice versa. Thus,
we obtain (1.3.44).
(b) Let Mn denote the number of allowed n-strings, An the number of allowed
n-strings ending with a letter from A, and Bn the number of allowed n-strings
ending with a letter from B. Then
Mn = An +Bn, An+1 = aBn, and Bn+1 = b(An +Bn),
which yields
Bn+1 = bBn +abBnâˆ’1.
The last recursion is solved by
Bn = c+Î» n
+ +câˆ’Î» n
âˆ’,
where Î»Â± are the eigenvalues of the matrix
 0
ab
1
b

,
i.e.
Î»Â± = bÂ±
âˆš
b2 +4ab
2
,
and cÂ± are constants, c+ > 0. Hence,
Mn = a

c+Î» nâˆ’1
+
+câˆ’Î» nâˆ’1
âˆ’

+

c+Î» n
+ +câˆ’Î» n
âˆ’

= Î» n
+

câˆ’

aÎ» nâˆ’1
âˆ’
Î» n
+
+ Î» n
âˆ’
Î» n
+

+c+

a 1
Î»+
+1

,
and 1
n logMn is represented as the sum
logÎ»+ + 1
n log

câˆ’

aÎ» nâˆ’1
âˆ’
Î» n
+
+ Î» n
âˆ’
Î» n
+

+c+

a 1
Î»+
+1

.
Note that

Î»âˆ’
Î»+
 < 1. Thus, the limiting information rate equals
lim
nâ†’âˆ
1
n logMn = logÎ»+.

1.3 Shannonâ€™s ï¬rst coding theorem. The entropy rate of a Markov source
57
The answers are different since the conditional equidistribution results in a strong
dependence between subsequent letters: they do not form a DTMC.
Worked Example 1.3.20
Let {Uj : j = 1,2,...} be an irreducible and aperiodic
DTMC with a ï¬nite state space. Given n â‰¥1 and Î± âˆˆ(0,1), order the strings u(n)
according to their probabilities

P(U(n) = u(n)
1 ) â‰¥P(U(n) = u(n)
2 ) â‰¥Â·Â·Â·

and select
them in this order until the probability of the remaining set becomes â‰¤1âˆ’Î±. Let
Mn(Î±) denote the number of the selected strings. Prove that lim
nâ†’âˆ
1
n logMn(Î±) = H,
the information rate of the source,
(a) in the case where the rows of the transition probability matrix P are all equal
(i.e. {Uj} is a Bernoulli sequence),
(b) in the case where the rows of P are permutations of each other, and in a general
case. Comment on the signiï¬cance of this result for coding theory.
Solution (a) Let P stand for the probability distribution of the IID sequence (Un)
and set H = âˆ’
m
âˆ‘
j=1
p j log p j (the binary entropy of the source). Fix Îµ > 0 and parti-
tion the set IÃ—n of all n-strings into three disjoint subsets:
K+ = {u(n) : p(u(n)) â‰¥2âˆ’n(Hâˆ’Îµ)}, Kâˆ’= {u(n) : p(u(n)) â‰¤2âˆ’n(H+Îµ)},
and
K = {u(n) : 2âˆ’n(H+Îµ) < p(u(n)) < 2âˆ’n(Hâˆ’Îµ)}.
By the law of large numbers (or asymptotic equipartition property), âˆ’1
n logP(U(n))
converges to H(= h), i.e. lim
nâ†’âˆP(K+ âˆªKâˆ’) = 0, and lim
nâ†’âˆP(K) = 1. Thus, to obtain
probability â‰¥Î±, for n large enough, you (i) cannot restrict yourself to K+ and have
to borrow strings from K , (ii) donâ€™t need strings from Kâˆ’, i.e. will have the last
selected string from K . Denote by Mn(Î±) the set of selected strings, and â™¯Mn(Î±)
by Mn. You have two two-side bounds
Î± â‰¤P

Mn(Î±)

â‰¤Î± +2âˆ’n(Hâˆ’Îµ)
and
2âˆ’n(H+Îµ)Mn(Î±) â‰¤P

Mn(Î±)

â‰¤P(K+)+2âˆ’n(Hâˆ’Îµ)Mn(Î±).
Excluding P

Mn(Î±)

yields
2âˆ’n(H+Îµ)Mn(Î±) â‰¤Î± +2âˆ’n(Hâˆ’Îµ) and 2âˆ’n(Hâˆ’Îµ)Mn(Î±) â‰¥Î± âˆ’P(K+).

58
Essentials of Information Theory
These inequalities imply, respectively,
limsup
nâ†’âˆ
1
n logMn(Î±) â‰¤H +Îµ and liminf
nâ†’âˆ
1
n logMn(Î±) â‰¥H âˆ’Îµ.
As Îµ is arbitrary, the limit is H.
(b) The argument may be repeated without any change in the case of permutations
because the ordered probabilities form the same set as in case (a) , and in a general
case by applying the law of large numbers to (1/n)Î¾n; cf. (1.3.3b) and (1.3.19).
Finally, the signiï¬cance for coding theory: if we are prepared to deal with the error-
probability â‰¤Î±, we do not need to encode all mn string u(n) but only âˆ¼2nH most
frequent ones. As H â‰¤logm (and in many cases â‰ªlogm), it yields a signiï¬cant
economy in storage space (data-compression).
Worked Example 1.3.21
A binary source emits digits 0 or 1 according to the
rule
P(Xn = k|Xnâˆ’1 = j,Xnâˆ’2 = i) = qr,
where k, j,i and r take values 0 or 1, r = kâˆ’jâˆ’i mod 2, and q0+q1 = 1. Determine
the information rate of the source.
Also derive the information rate of a binary Bernoulli source, emitting digits
0 and 1 with probabilities q0 and q1. Explain the relationship between these two
results.
Solution The source is a DTMC of the second order. That is, the pairs (Xn,Xn+1)
form a four-state DTMC, with
P(00,00) = q0, P(00,01) = q1, P(01,10) = q0, P(01,11) = q1,
P(10,00) = q0, P(10,01) = q1, P(11,10) = q0, P(11,11) = q1;
the remaining eight entries of the transition probability matrix vanish. This gives
H = âˆ’q0 logq0 âˆ’q1 logq1.
For a Bernoulli source the answer is the same.
Worked Example 1.3.22
Find an entropy rate of a DTMC associated with a
random walk on the 3Ã—3 chessboard:
â›
â
1
2
3
4
5
6
7
8
9
â
â .
(1.3.45)
Find the entropy rate for a rook, bishop (both kinds), queen and king.

1.4 Channels of information transmission
59
Solution We consider the kingâ€™s DTMC only; other cases are similar. The transition
probability matrix is
â›
âœ
âœ
âœ
âœ
âœ
âœ
âœ
âœ
âœ
âœ
âœ
âœ
âœ
â
0
1/3
0
1/3
1/3
0
0
0
0
1/5
0
1/5
1/5
1/5
1/5
0
0
0
0
1/3
0
0
1/3
1/3
0
0
0
1/5
1/5
0
0
1/5
0
1/5
1/5
0
1/8
1/8
1/8
1/8
0
1/8
1/8
1/8
1/8
0
1/5
1/5
0
1/5
0
0
1/5
1/5
0
0
0
1/3
1/3
0
0
0
1/3
0
0
0
1/5
1/5
1/5
1/5
0
1/5
0
0
0
0
1/3
1/3
0
1/3
0
â
âŸ
âŸ
âŸ
âŸ
âŸ
âŸ
âŸ
âŸ
âŸ
âŸ
âŸ
âŸ
âŸ
â 
By symmetry the invariant distribution is Ï€1 = Ï€3 = Ï€9 = Ï€7 = Î», Ï€4 = Ï€2 = Ï€6 =
Ï€8 = Î¼, Ï€5 = Î½, and by the DBEs
Î»/3 = Î¼/5, Î»/3 = Î½/8, 4Î» +4Î¼ +Î½ = 1
implies Î» = 3
40, Î¼ = 1
8, Î½ = 1
5. Now
H = âˆ’4Î» 1
3 log 1
3 âˆ’4Î¼ 1
5 log 1
5 âˆ’Î½ 1
8 log 1
8 = 1
10 log15+ 3
40.
1.4 Channels of information transmission. Decoding rules. Shannonâ€™s
second coding theorem
In this section we prove a core statement of Shannonâ€™s theory: the second coding
theorem (SCT), also known as the noisy coding theorem (NCT). Shannon stated
its assertion and gave a sketch of its proof in his papers and books in the 1940s.
His argument was subject to (not entirely unjustiï¬ed) criticism by professional
mathematicians. It took the mathematical community about a decade to produce
a rigorous and complete proof of the SCT. However, with hindsight, one cannot
stop admiring Shannonâ€™s intuition and his ï¬rm grasp of fundamental notions such
as entropy and coding as well their relation to statistics of long random strings.
We point at various aspects of this topic, not avoiding a personal touch palpable in
writings of the main players in this area.
So far, we have considered a source emitting a random text U1 U2 ..., and an
encoding of a message u(n) by a binary codeword x(N) using a code fn : IÃ—n â†’JÃ—N,
J = {0,1}. Now we focus upon the relation between the length of a message n and
the codeword-length N: it is determined by properties of the channel through which
the information is sent. It is important to remember that the code fn is supposed to
be known to the receiver.

60
Essentials of Information Theory
Typically, a channel is subject to â€˜noiseâ€™ which distorts the messages transmitted:
a message at the output differs in general from the message at the input. Formally,
a channel is characterised by a conditional distribution
Pch

receive word y(N)|codeword x(N) sent

;
(1.4.1)
we again suppose that it is known to both sender and receiver. (We use a distinct
symbol Pch

Â· |codeword x(N) sent

or, brieï¬‚y, Pch

Â· |x(N)
, to stress that this prob-
ability distribution is generated by the channel, conditional on the event that code-
word x(N) has been sent.) Speaking below of a channel, we refer to a conditional
probability (1.4.1) (or rather a family of conditional probabilities, depending on
N). Consequently, we use the symbol Y(N) for a random string representing the
output of the channel; given that a word x(N) was sent,
Pch(Y(N) = y(N)|x(N)) = Pch(y(N)|x(N)).
An important example is the so-called memoryless binary channels (MBCs)
where
Pch

y(N)|x(N)
=
N
âˆ
i=1
P(yi|xi),
(1.4.2)
if y(N) = y1 ...yN, x(N) = x1 ...xN. Here, P(y|x), x,y = 0,1, is a symbol-to-symbol
channel probability (i.e. the conditional probability to have symbol y at the out-
put of the channel given that symbol x has been sent). Clearly, {P(y|x)} is a
2Ã—2 stochastic matrix (often called the channel matrix). In particular, if P(1|0) =
P(0|1) = p, the channel is called symmetric (MBSC). The channel matrix then has
the form
Î  =
 1âˆ’p
p
p
1âˆ’p

and p is called the row error-probability (or the symbol-error-probability).
Example 1.4.1
Consider the memoryless channel, where Y = X + Z, and an
additive noise Z takes values 0 and a with probability 1/2; a is a given real number.
The input alphabet is {0,1} and Z is independent of X.
Properties of this channel depend on the value of a. Indeed, if a Ì¸= Â±1, the chan-
nel is uniquely decodable. In other words, if we have to use the channel for trans-
mitting messages (strings) of length n (there are 2n of them altogether) then any
message can be sent straightaway, and the receiver will be able to recover it. But
if a = Â±1, there are errors possible, and to make sure that the receiver can recover
our message we have to encode it, which, typically, results in increasing the length
of the string sent into the channel, from n to N, say.

1.4 Channels of information transmission
61
In other words, strings of length N sent to the channel will be codewords repre-
senting source messages of a shorter length n. The maximal ratio n/N which still
allows the receiver to recover the original message is an important characteristic of
the channel, called the capacity. As we will see, passing from a Ì¸= Â±1 to a = Â±1
changes the capacity from 1 (no encoding needed) to 1/2 (where the codeword-
length is twice as long as the length of the source message).
So, we need to introduce a decoding rule fN : JÃ—N â†’IÃ—n such that the overall
probability of error Îµ(= Îµ( fn, fN,P)) deï¬ned by
Îµ = âˆ‘
u(n)
P
 fN(Y(N)) Ì¸= u(n), u(n) emitted

= âˆ‘
u(n)
P

U(n) = u(n)
Pch
 fN(Y(N)) Ì¸= u(n) | fn(u(n)) sent

(1.4.3)
is small. We will try (and under certain conditions succeed) to have the error-
probability (1.4.3) tending to zero as n â†’âˆ.
The idea which is behind the construction is based on the following facts:
(1) For a source with the asymptotic equipartition property the number of dis-
tinct n-strings emitted is 2n(H+o(1)) where H â‰¤logm is the information rate of
the source. Therefore, we have to encode not mn = 2nlogm messages, but only
2n(H+o(1)) which may be considerably less. That is, the code fn may be deï¬ned
on a subset of IÃ—n only, with the codeword-length N = âŒˆnHâŒ‰.
(2) We may try even a larger N: N = âŒˆRâˆ’1nHâŒ‰, where R is a constant with 0 < R <
1. In other words, the increasing length of the codewords used from âŒˆnHâŒ‰to
âŒˆRâˆ’1nHâŒ‰will allow us to introduce a redundancy in the code fn, and we may
hope to be able to use this redundancy to diminish the overall error-probability
(1.4.3) (provided that in addition a decoding rule is â€˜goodâ€™). It is of course
desirable to minimise Râˆ’1, i.e. maximise R: it will give the codes with optimal
parameters. The question of how large R is allowed to be depends of course on
the channel.
It is instrumental to introduce a notational convention. As the codeword-length is
a crucial parameter, we write N instead of Râˆ’1Hn and RN instead of Hn: the num-
ber of distinct strings emitted by the source becomes 2N(R+o(1)). In future, the index
n âˆ¼NR
H will be omitted wherever possible (and replaced by N otherwise). It is con-
venient to consider a â€˜typicalâ€™ set UN of distinct strings emitted by the source, with
â™¯UN = 2N(R+o(1)). Formally, UN can include strings of different length; it is only
the log-asymptotics of â™¯UN that matter. Accordingly, we will omit the superscript
(n) in the notation u(n).

62
Essentials of Information Theory
Deï¬nition 1.4.2
A value R âˆˆ(0,1) is called a reliable transmission rate (for a
given channel) if, given that the source strings take equiprobable values from a set
UN with â™¯UN = 2N(R+o(1)), there exist an encoding rule fN : UN â†’XN âŠ†JÃ—N and
a decoding rule fN : JÃ—N â†’UN with the error-probability
âˆ‘
uâˆˆUN
1
â™¯UN
Pch

fN(Y(N)) Ì¸= u|fN(u) sent

(1.4.4)
tending to zero as N â†’âˆ. That is, for each sequence UN with lim
Nâ†’âˆ
1
N logâ™¯UN = R,
there exist a sequence of encoding rules fN : UN â†’XN, XN âŠ†JÃ—N, and a sequence
of decoding rules fN : JÃ—N â†’UN such that
lim
Nâ†’âˆ
1
â™¯UN âˆ‘
uâˆˆUN âˆ‘
Y(N)
1
 fN(Y(N)) Ì¸= u

Pch

Y(N)| fN(u)

= 0.
(1.4.5)
Deï¬nition 1.4.3
The channel capacity is deï¬ned as the supremum
C = sup
	
R âˆˆ(0,1) : R is a reliable transmission rate

.
(1.4.6)
Remark 1.4.4
(a) Physically speaking, the channel capacity can be thought of
as a limit lim
Nâ†’âˆ
1
N logn(N) where n(N) is the maximal number of strings of length
N which can be sent through the channel with a vanishing probability of erroneous
decoding.
(b) The reason for the equiprobable distribution on UN is that it yields the worst-
case scenario. See Theorem 1.4.6 below.
(c) If encoding rule fN used is one-to-one (lossless) then it sufï¬ces to treat the
decoding rules as maps JÃ—N â†’XN rather than JÃ—N â†’UN: if we guess correctly
what codeword x(N) has been sent, we simply set u = f âˆ’1
N (x(N)). If, in addition,
the source distribution is equiprobable over U then the error-probability Îµ can be
written as an average over the set of codewords XN:
Îµ =
1
â™¯X âˆ‘
xâˆˆXN

1âˆ’Pch

fN(Y(N)) = x|x sent

.
Accordingly, it makes sense to write Îµ = Îµave and speak about the average proba-
bility of error. Another form is the maximum error-probability
Îµmax = max

1âˆ’Pch

fN(Y(N)) = x|x sent

: x âˆˆXN

;
obviously, Îµave â‰¤Îµmax. In this section we work with Îµave â†’0 leaving the question
of whether Îµmax â†’0. However, in Section 2.2 we reduce the problem of assessing
Îµmax to that with Îµave, and as a result, the formulas for the channel capacity deduced
in this section will remain valid if Îµave is replaced by Îµmax.

1.4 Channels of information transmission
63
Remark 1.4.5
(a) By Theorem 1.4.17 below, the channel capacity of an MBC is
given by
C = sup
pXk
I(Xk : Yk).
(1.4.7)
Here, I(Xk : Yk) is the mutual information between a single pair of input and output
letters Xk and Yk (the index k may be omitted), with the joint distribution
P(X = x,Y = y) = pX(x)P(y|x), x,y = 0,1,
(1.4.8)
where pX(x) = P(X = x). The supremum in (1.4.7) is over all possible distributions
pX = (pX(0), pX(1)). A useful formula is I(X : Y) = h(Y)âˆ’h(Y|X) (see (1.3.12)).
In fact, for the MBSC
h(Y|X) = âˆ’âˆ‘
x=0,1
pX(x) âˆ‘
y=0,1
P(y|x)logP(y|x)
âˆ’âˆ‘
y=0,1
P(y|x) logP(y|x) = h2(p,1âˆ’p) = Î·2(p);
(1.4.9)
the lower index 2 will be omitted for brevity.
Hence h(Y|X) = Î·(p) does not depend on input distribution pX, and for the
MBSC
C = sup
pX
h(Y)âˆ’Î·(p).
(1.4.10)
But sup
pX
h(Y) is equal to log2 = 1: it is attained at pX(0) = pX(1) = 1/2, and
pY(0) = pY(1) = 1/2(p + 1 âˆ’p) = 1/2. Therefore, for an MBSC, with the row
error-probability p,
C = 1âˆ’Î·(p).
(1.4.11)
(b) Suppose we have a source U1 U2 ... with the asymptotic equipartition property
and information rate H. To send a text emitted by the source through a channel
of capacity C we need to encode messages of length n by codewords of length
n(H +Îµ)
C
in order to have the overall error-probability tending to zero as n â†’
âˆ. The value Îµ > 0 may be chosen arbitrarily small. Hence, if H/C < 1, a text
can be encoded with a higher speed than it is produced: in this case the channel
is used reliably for transmitting information from the source. On the contrary, if
H/C > 1, the text will be produced with a higher speed than we can encode it and
send reliably through a channel. In this case reliable transmission is impossible.
For a Bernoulli or stationary Markov source and an MBSC, condition H/C < 1 is
equivalent to h(U)+Î·(p) < 1 or h(Un+1|Un)+Î·(p) < 1 respectively.

64
Essentials of Information Theory
In fact, Shannonâ€™s ideas have not been easily accepted by leading contemporary
mathematicians. It would be interesting to see the opinions of the leading scientists
who could be considered as â€˜creatorsâ€™ of information theory.
Theorem 1.4.6
Fix a channel (i.e. conditional probabilities Pch in (1.4.1)) and a
set U of the source strings and denote by Îµ(P) the overall error-probability (1.4.3)
for U(n) having a probability distribution P over U , minimised over all encoding
and decoding rules. Then
Îµ(P) â‰¤Îµ(P0),
(1.4.12)
where P0 is the equidistribution over U .
Proof
Fix encoding and decoding rules f and f, and let a string u âˆˆU have
probability P(u). Deï¬ne the error-probability when u is emitted as
Î²(u) :=
âˆ‘
y: f(y)Ì¸=u
Pch(y| f(u)).
The overall error-probability equals
Îµ(= Îµ(P, f, f)) = âˆ‘
uâˆˆU
P(u)Î²(u).
If we permute the allocation of codewords (i.e. encode u by f(uâ€²) where uâ€² = Î»(u)
and Î» is a permutation of degree â™¯U ), we get the overall error-probability Îµ(Î»)
= âˆ‘
uâˆˆU
P(u)Î²(Î»(u)). In the case P(u) =

â™¯U
âˆ’1 (equidistribution), Îµ(Î») does not
depend on Î» and is given by
Îµ =
1
â™¯U âˆ‘
uâˆˆU
Î²(u) = Îµ(P0, f, f).
It is claimed that for each probability distribution {P(u), u âˆˆU }, there exists Î»
such that Îµ(Î») â‰¤Â¯Îµ. In fact, take a random permutation, Î›, equidistributed among
all

â™¯U

! permutations of degree â™¯U . Then
min
Î» Îµ(Î»)
â‰¤EÎµ(Î›) = E âˆ‘
uâˆˆU
P(u)Î²(Î›u)
= âˆ‘
uâˆˆU
P(u)EÎ²(Î›u) = âˆ‘
uâˆˆU
P(u) 1
â™¯U
âˆ‘
 uâˆˆU
Î²( u) = Îµ.
Hence, given any f and f, we can ï¬nd new encoding and decoding rules with
overall error-probability â‰¤Îµ(P0, f, f). Minimising over f and f leads to (1.4.12).

1.4 Channels of information transmission
65
Worked Example 1.4.7
Let the random variables X and Y, with values from
ï¬nite â€˜alphabetsâ€™ I and J, represent, respectively, the input and output of a trans-
mission channel, with the conditional probability P(x | y) = P(X = x | Y = y). Let
h(P(Â· | y)) denote the entropy of the conditional distribution P(Â· | y), y âˆˆJ:
h(P(Â· | y)) = âˆ’âˆ‘
x
P(X | y)logP(x | y).
Let h(X | Y) denote the conditional entropy of X given Y Deï¬ne the ideal observer
decoding rule as a map f IO : J â†’I such that P( f(y) | y) = maxxâˆˆI P(x | y) for all
y âˆˆJ. Show that
(a) under this rule the error-probability
Ï€IO
er (y) = âˆ‘
xâˆˆI
1(x Ì¸= f(y))P(x | y)
satisï¬es Ï€IO
er (y) â‰¤1
2h(P(Â· | y));
(b) the expected value of the error-probability obeys EÏ€IO
er (Y) â‰¤1
2h(X | Y).
Solution Indeed, (a) follows from (iii) in Worked Example 1.2.7, as
Ï€IO
err = 1âˆ’P

f(y) | y

= 1âˆ’Pmax( Â· |y),
which is less than or equal to 1
2h(P( Â· |y)). Finally, (b) follows from (a) by taking
expectations, as h(X|Y) = Eh(P(Â·|Y)).
As was noted before, a general decoding rule (or a decoder) is a map fN : JÃ—N â†’
UN; in the case of a lossless encoding rule fN, fN is a map JÃ—N â†’XN. Here X
is a set of codewords. Sometimes it is convenient to identify the decoding rule by
ï¬xing, for each codeword x(N), a set A(x(N)) âŠ‚JÃ—N, so that A(x(N)
1 ) and A(x(N)
2 ) are
disjoint for x(N)
1
Ì¸= x(N)
2 , and the union âˆªx(N)âˆˆXNA(x(N)) gives the whole JÃ—N. Given
that y(N) âˆˆA(x(N)), we decode it as fN(y(N)) = x(N).
Although in the deï¬nition of the channel capacity we assume that the source
messages are equidistributed (as was mentioned, it gives the worst case in the
sense of Theorem 1.4.6), in reality of course the source does not always follow
this assumption. To this end, we need to distinguish between two situations: (i) the
receiver knows the probabilities
p(u) = P(U = u)
(1.4.13)
of the source strings (and hence the probability distribution pN(x(N)) of the code-
words x(N) âˆˆXN), and (ii) he does not know pN(x(N)). Two natural decoding rules
are, respectively,

66
Essentials of Information Theory
(i) the ideal observer (IO) rule decodes a received word y(N) by a codeword x(N)â‹†
that maximises the posterior probability
P

x(N) sent |y(N) received

= pN(x(N))Pch(y(N)|x(N))
pY(N)(y(N))
,
(1.4.14)
where
pY(N)(y(N)) = âˆ‘
 x(N)âˆˆXN
pN( x(N))Pch(y(N)| x(N)),
and
(ii) the maximum likelihood (ML) rule decodes a received word y(N) by a codeword
x(N)
â‹†
that maximises the prior probability
Pch(y(N)|x(N)).
(1.4.15)
Theorem 1.4.8
Suppose that an encoding rule f is deï¬ned for all messages that
occur with positive probability and is one-to-one. Then:
(a) For any such encoding rule, the IO decoder minimises the overall error-
probability among all decoders.
(b) If the source message U is equiprobable on a set U , then for any encoding rule
f : U â†’XN as above, the random codeword X(N) = f(U) is equiprobable on
XN, and the IO and ML decoders coincide.
Proof
Again, for simplicity let us omit the upper index (N).
(a) Note that, given a received word y, the IO obviously maximises the joint
probability p(x)Pch(y|x) (the denominator in (1.4.14) is ï¬xed when word y is
ï¬xed). If we use an encoding rule f and decoding rule f, the overall error-
probability (see (1.4.3)) is
âˆ‘
u P(U = u)Pch

f(y) Ì¸= u| f(u) sent

= âˆ‘
x p(x)âˆ‘
y 1
 f(y) Ì¸= x

Pch (y|x)
= âˆ‘
y âˆ‘
x 1

x Ì¸= f(y)

p(x)Pch (y|x)
= âˆ‘
y âˆ‘
x p(x)Pch (y|x)âˆ’âˆ‘
y p

f(y)

Pch

y| f(y)

= 1âˆ’âˆ‘
y p

f(y)

Pch

y| f(y)

.
It remains to note that each term in the sumâˆ‘
y
p

f(y)

Pch

y| f(y)

is maximised
when f coincides with the IO rule. Hence, the whole sum is maximised, and the
overall error-probability minimised.
(b) The ï¬rst statement is obvious, as, indeed is the second.

1.4 Channels of information transmission
67
Assuming in the deï¬nition of the channel capacity that the source messages are
equidistributed, it is natural to explore further the ML decoder. While using the ML
decoder, an error can occur because either the decoder chooses a wrong codeword
x or an encoding rule f used is not one-to-one. The probability of this is assessed
in Theorem 1.4.8. For further simpliï¬cation, we write P instead of Pch; symbol P
is used mainly for the joint input/output distribution.
Lemma 1.4.9
If the source messages are equidistributed over a set U then, while
using the ML decoder and an encoding rule f, the overall error-probability satisï¬es
Îµ( f) â‰¤
1
â™¯U âˆ‘
uâˆˆU
âˆ‘
uâ€²âˆˆU : uâ€²Ì¸=u
P

P

Y| f(uâ€²)

â‰¥P(Y|f(u))|U = u

.
(1.4.16)
Proof
If the source emits u and the ML decoder is used, we get
(a) an error when P

Y | f(uâ€²)

> P

Y | f(u)

for some uâ€² Ì¸= u,
(b) possibly an error when P

Y| f(uâ€²)

= P

Y | f(u)

for some uâ€² Ì¸= u (this in-
cludes the case when f(u) = f(uâ€²)), and ï¬nally
(c) no error when P

Y | f(uâ€²)

< P

Y | f(u)

for any uâ€² Ì¸= u.
Thus, the probability is bounded as follows:
P

error | U = u

â‰¤P

P

Y | f(uâ€²)

â‰¥P(Y | f(u)) for some uâ€² Ì¸= u | U = u

â‰¤âˆ‘
uâ€²âˆˆU
1

uâ€² Ì¸= u

P

P

Y | f(uâ€²)

â‰¥P(Y | f(u)) | U = u

.
Multiplying by
1
â™¯U and summing up over u yields the result.
Remark 1.4.10
Bound (1.4.16) of course holds for any probability distribution
p(u) = P(U = u), provided
1
â™¯U is replaced by p(u).
As was already noted, a random coding is a useful tool alongside with deter-
ministic encoding rules. A deterministic encoding rule is a map f: U â†’JÃ—N; if
â™¯U = r then f is given as a collection of codewords
(
f(u1),..., f(ur)
)
or, equiv-
alently, as a concatenated â€˜megastringâ€™ (or codebook)
f(u1)... f(ur) âˆˆ

JÃ—NÃ—r = {0,1}Ã—Nr.
Here, u1,...,ur are the source strings (not letters!) constituting set U . If f is loss-
less then f(ui) Ì¸= f(u j) whenever i Ì¸= j. A random encoding rule is a random ele-
ment F of

JÃ—Nr, with probabilities P(F = f), f âˆˆ

JÃ—Nr. Equivalently, F may

68
Essentials of Information Theory
be regarded as a collection of random codewords F(ui), i = 1,...,r, or, equiva-
lently, as a random codebook
F(u1)F(u2)...F(ur) âˆˆ{0,1}Nr.
A typical example is where codewords F(u1), F(u2), . . . , F(ur) are independent,
and (random) symbols Wi1,...,WiN constituting word F(ui) are independent too.
The reasons for considering random encoding rules are:
(1) the existence of a â€˜goodâ€™ deterministic code frequently follows from the exis-
tence of a good random code;
(2) the calculations for random codes are usually simpler than for optimal deter-
ministic codes, because a discrete optimisation is replaced by an optimisation over
probability distributions.
A drawback of random coding is that it is not always one-to-one (F(u) may
coincide with F(uâ€²) for u Ì¸= uâ€²). However, this occurs, for large N, with negligible
probability.
The idea of random coding goes back to Shannon. As often happened in the
history of mathematics, a brilliant idea solves one problem but opens a Pandora
box of other questions. In this case, a particular problem that emerged from the
aftermath of random coding was the problem of ï¬nding â€˜goodâ€™ non-random codes.
A major part of modern information and coding theory revolves around this prob-
lem, and so far no general satisfactory solution has been found. However, a number
of remarkable partial results have been achieved, some of which are discussed in
this book.
Continuing with random coding, write the expected error-probability for a ran-
dom encoding rule F:
E := E Îµ(F) = âˆ‘
f
Îµ( f)P(F = f).
(1.4.17)
Theorem 1.4.11
(i) There exists a deterministic encoding rule f with Îµ( f) â‰¤E.
(ii)
P

Îµ(F) <
E
1âˆ’Ï

â‰¥Ï for any Ï âˆˆ(0,1).
Proof
Part (i) is obvious. For (ii), use the Chebyshev inequality (see PSE I, p. 75):
P

Îµ(F) â‰¥
E
1âˆ’Ï

â‰¤1âˆ’Ï
E
E = 1âˆ’Ï.

1.4 Channels of information transmission
69
Deï¬nition 1.4.12
For random words X(N) = X1 ... XN and Y(N) =Y1 ... YN deï¬ne
CN := sup

1
N I

X(N) : Y(N)
, over input
probability distributions PX(N)

.
(1.4.18)
Recall that I

X(N) : Y(N)
is the mutual entropy given by
h

X(N)
âˆ’h

X(N)|Y(N)
= h

Y(N)
âˆ’h

Y(N)|X(N)
.
Remark 1.4.13
A simple heuristic argument (which will be made rigorous in
Section 2.2) shows that the capacity of the channel cannot exceed the mutual
information between its input and output. Indeed, for each typical input N-
sequence, there are
approximately 2h(Y(N)|X(N)) possible Y(N) sequences,
all of them equally likely. We will not be able to detect which sequence X was sent
unless no two X(N) sequences produce the same Y(N) output sequence. The total
number of typical Y(N) sequences is 2h(Y(N)). This set has to be divided into subsets
of size 2h(Y(N)|X(N)) corresponding to the different input X(N) sequences. The total
number of disjoint sets is
â‰¤2h(Y(N))âˆ’h(Y(N)|X(N)) = 2I(X(N):Y(N)).
Hence, the total number of distinguishable signals of the length N could not be
bigger than 2I(X(N):Y(N)). Putting the same argument slightly differently, the number
of typical sequences X(N) is 2Nh(X(N)). However, there are only 2Nh(X(N),Y(N)) jointly
typical sequences (X(N),Y(N)). So, the probability that any randomly chosen pair
is jointly typical is about 2âˆ’I(X(N):Y(N)). So, the number of distinguished signals is
bounded by 2h(X(N))+h(Y(N))âˆ’h(X(N)|Y(N)).
Theorem 1.4.14
(Shannonâ€™s SCT: converse part) The channel capacity C obeys
C â‰¤lim sup
Nâ†’âˆ
CN.
(1.4.19)
Proof
Consider a code f = fN : UN â†’XN âŠ†JÃ—N, where â™¯UN = 2N(R+o(1)), R âˆˆ
(0,1). We want to prove that for any decoding rule
Îµ( f) â‰¥1âˆ’CN +o(1)
R+o(1) .
(1.4.20)

70
Essentials of Information Theory
The assertion of the theorem immediately follows from (1.4.20) and the deï¬nition
of the channel capacity because
lim inf
Nâ†’âˆÎµ( f) â‰¥1âˆ’1
R lim sup
Nâ†’âˆ
CN
which is > 0 when R > limsupNâ†’âˆCN.
Let us check (1.4.20) for one-to-one f (otherwise Îµ( f) is even bigger). Then a
codeword X(N) = f(U) is equidistributed when string U is, and, if a decoding rule
is f : JÃ—N â†’X , we have, for N large enough,
NCN â‰¥I

X(N) : Y(N)
â‰¥I

X(N) : f(Y(N))

(cf. Theorem 1.2.6)
= h

X(N)
âˆ’h

X(N)| f(Y(N))

= logr âˆ’h

X(N)| f(Y(N))

(by equidistribution)
â‰¥logr âˆ’Îµ( f)log(r âˆ’1)âˆ’1.
Here and below r = â™¯U . The last bound follows by the generalised Fano inequality
(1.2.25). Indeed, observe that the (random) codeword X(N) = f(U) takes r values
x(N)
1 ,...,x(N)
r
from the codeword set X (= XN

, and the error-probability is
Îµ( f) =
r
âˆ‘
i=1
P(X(N) = x(N)
i
, f(Y(N)) Ì¸= x(N)
i
).
So, (1.2.25) implies
h

X(N)| f(Y(N))

â‰¤h2(Îµ)+Îµ log(r âˆ’1) â‰¤1+Îµ( f)log(r âˆ’1),
and we obtain NCN â‰¥logr âˆ’Îµ( f)log(r âˆ’1)âˆ’1. Finally, r = 2N(R+o(1)) and
NCN â‰¥N(R+o(1))âˆ’Îµ( f)log

2N(R+o(1)) âˆ’1

,
i.e.
Îµ( f) â‰¥N(R+o(1))âˆ’NCN
log

2N(R+o(1)) âˆ’1
 = 1âˆ’CN +o(1)
R+o(1) .
Let p(X(N),Y(N)) be the random variable that assigns, to random words X(N) and
Y(N), the joint probability of having these words at the input and output of a chan-
nel, respectively. Similarly, pX(X(N)) and pY(Y(N)) denote the random variables
that give the marginal probabilities of words X(N) and Y(N), respectively.

1.4 Channels of information transmission
71
Theorem 1.4.15
(Shannonâ€™s SCT: direct part) Suppose we can ï¬nd a constant
c âˆˆ(0,1) such that for any R âˆˆ(0,c) and N â‰¥1 there exists a random coding
F(u1),...,F(ur), where r = 2N(R+o(1)), with IID codewords F(ui) âˆˆJÃ—N, such
that the (random) input/output mutual information
Î˜N := 1
N log
p(X(N),Y(N))
pX(X(N))pY(Y(N))
(1.4.21)
converges in probability to c as N â†’âˆ. Then the channel capacity C â‰¥c.
The proof of Theorem 1.4.15 is given after Worked Examples 1.4.24 and 1.4.25
(the latter is technically rather involved). To start with, we explain the strategy of
the proof outline by Shannon in his original 1948 paper. (It took about 10 years
before this idea was transformed into a formal argument.)
First, one generates a random codebook X consisting of r = 2âŒˆNRâŒ‰words,
X(N)(1),...,X(N)(r). The codewords X(N)(1),...,X(N)(r) are assumed to be
known to both the sender and the receiver, as well as the channel transition
matrix Pch(y|x). Next, the message is chosen according to a uniform distribution,
and the corresponding codeword is sent over a channel. The receiver uses the max-
imum likelihood (ML) decoding, i.e. choose the a posteriori most likely message.
But this procedure is difï¬cult to analyse. Instead, a suboptimal but straightforward
typical set decoding is used. The receiver declares that the message w is sent if there
is only one input such that the codeword for w and the output of the channel are
jointly typical. If no such word exists or it is non-unique then an error is declared.
Surprisingly, this procedure is asymptotically optimal. Finally, the existence of a
good random codebook implies the existence of a good non-random coding.
In other words, channel capacity C is no less than the supremum of the values
c for which the convergence in probability in (1.4.21) holds for an appropriate
random coding.
Corollary 1.4.16
With c as in the assumptions of Theorem 1.4.15, we have that
supc â‰¤C â‰¤lim sup
Nâ†’âˆ
CN.
(1.4.22)
So, if the LHS and RHS sides of (1.4.22) coincide, then their common value gives
the channel capacity.
Next, we use Shannonâ€™s SCT for calculating the capacity of an MBC. Recall (cf.
(1.4.2)), for an MBC,
P

y(N)|x(N)
=
N
âˆ
i=1
P(yi|xi).
(1.4.23)

72
Essentials of Information Theory
Theorem 1.4.17
For an MBC,
I

X(N) : Y(N)
â‰¤
N
âˆ‘
j=1
I(Xj : Yj),
(1.4.24)
with equality if the input symbols X1,...,XN are independent.
Proof
Since P

y(N)|x(N)
=
N
âˆ
j=1
P(yj|x j), the conditional entropy h

Y(N)|X(N)
equals the sum
N
âˆ‘
j
h(Yj|Xj). Then the mutual information
I

X(N) : Y(N)
= h

Y(N)
âˆ’h

Y(N)|X(N)
= h

Y(N)
âˆ’
âˆ‘
1â‰¤jâ‰¤N
h(Yj|Xj)
â‰¤âˆ‘
j

h(Yj)âˆ’h(Yj|Xj)

= âˆ‘
j
I(Xj : Yj).
The equality holds iff Y1,...,YN are independent. But Y1,...,YN are independent if
X1,...,XN are.
Remark 1.4.18
Compare with inequalities (1.4.24) and (1.2.27). Note the oppo-
site inequalities in the bounds.
Theorem 1.4.19
The capacity of an MBC is
C = sup
pX1
I(X1 : Y1).
(1.4.25)
The supremum is over all possible distributions pX1 of the symbol X1.
Proof
By the deï¬nition of CN, NCN does not exceed
sup
pX
I(X(N) : Y(N)) â‰¤âˆ‘
j
sup
pXj
I(Xj : Yj) = N sup
pX1
I(X1 : Y1).
So, by Shannonâ€™s SCT (converse part),
C â‰¤lim sup
Nâ†’âˆ
CN â‰¤sup
pX1
I(X1 : Y1).
On the other hand, take a random coding F, with codewords F(ul) =Vl1 ... VlN,
1 â‰¤l â‰¤r, containing IID symbols Vl j that are distributed according to pâˆ—, a prob-
ability distribution that maximises I(X1 : Y1). [Such random coding is deï¬ned for

1.4 Channels of information transmission
73
any r, i.e. for any R (even R > 1!).] For this random coding, the (random) mutual
entropy Î˜N equals
1
N log
p

X(N),Y(N)
pX

X(N)
pY

Y(N)
= 1
N
N
âˆ‘
j=1
log
p(Xj,Yj)
pâˆ—(Xj)pY(Yj) = 1
N
N
âˆ‘
j=1
Î¶ j,
where Î¶ j := log
p(Xj,Yj)
pâˆ—(Xj)pY(Yj).
The random variables Î¶ j are IID, and
EÎ¶ j = Elog
p(Xj,Yj)
pâˆ—(Xj)pY(Yj) = Ipâˆ—(X1 : Y1).
By the law of large numbers for IID random variables (see Theorem 1.3.5), for the
random coding as suggested,
Î˜N
P
âˆ’â†’Ipâˆ—(X1 : Y1) = sup
pX1
I(X1 : Y1).
By Shannonâ€™s SCT (direct part),
C â‰¥sup
pX1
I(X1 : Y1).
Thus, C = suppX1 I(X1 : Y1).
Remark 1.4.20
(a) The pair (X1,Y1) may be replaced by any (Xj,Yj), j â‰¥1.
(b) Recall that the joint distribution of X1 and Y1 is deï¬ned by P(X1 = x,Y1 = y) =
pX1(x)P(y|x) where

P(y|x)

is the channel matrix.
(c) Although, as was noted, the construction holds for each r (that is, for each
R â‰¥0) only R â‰¤C are reliable.
Example 1.4.21
A helpful statistician preprocesses the output of a memory-
less channel (MBC) with transition probabilities P(y|x) and channel capacity C =
maxpX I(X : Y) by forming Y â€² = g(Y): he claims that this will strictly improve the
capacity. Is he right? Surely not, as preprocessing (or doctoring) does not increase
the capacity. Indeed,
I(X : Y) = h(X)âˆ’h(X|Y) â‰¥h(X)âˆ’h(X|g(Y)) = I(X : g(Y)).
(1.4.26)
Under what condition does he not strictly decrease the capacity? Equality in
(1.4.26) holds iff, under the distribution pX that maximises I(X : Y), the ran-
dom variables X and Y are conditionally independent given g(Y). [For example,
g(y1) = g(y2) iff for any x, PX|Y(x|y1) = PX|Y(x|y2); that is, g glues together only
those values of y for which the conditional probability PX|Y(Â·|y) is the same.] For
an MBC, equality holds iff g is one-to-one, or p = P(1|0) = P(0|1) = 1/2.

74
Essentials of Information Theory
Formula (1.4.25) admits a further simpliï¬cation when the channel is symmetric
(MBSC), i.e. P(1|0) = P(0|1) = p. More precisely, in accordance with Remark
1.4.5(a) (see (1.4.11)) we obtain
Theorem 1.4.22
For an MBSC, with the row error-probability p,
C = 1âˆ’h(p,1âˆ’p) = 1âˆ’Î·(p)
(1.4.27)
(see (1.4.11)). The channel capacity is realised by a random coding with the IID
symbols Vl j taking values 0 and 1 with probability 1/2.
Worked Example 1.4.23
(a) Consider a memoryless channel with two input symbols A and B, and three
output symbols, A,B,âˆ—. Suppose each input symbol is left intact with probabil-
ity 1/2, and transformed into a âˆ—with probability 1/2. Write down the channel
matrix and calculate the capacity.
(b) Now calculate the new capacity of the channel if the output is further processed
by someone who cannot distinguish A and âˆ—, so that the matrix becomes
 1
0
1/2
1/2

.
Solution (a) The channel has the matrix
1/2
0
1/2
0
1/2
1/2

and is symmetric (the rows are permutations of each other). So, h(Y|X = x) =
âˆ’2Ã— 1
2 log 1
2 = 1 does not depend on the value of x = A,B. Then h(Y|X) = 1, and
I(X : Y) = h(Y)âˆ’1.
(1.4.28)
If P(X = A) = Î± then Y has the output distribution
1
2Î±, 1
2(1âˆ’Î±), 1
2

and h(Y|X) is maximised at Î± = 1/2. Then the capacity equals
h(1/4,1/4,1/2)âˆ’1 = 1
2.
(1.4.29)
(b) Here, the channel is not symmetric. If P(X = A) = Î± then the conditional
entropy is decomposed as
h(Y|X) = Î±h(Y|X = A)+(1âˆ’Î±)h(Y|X = B)
= Î± Ã—0+(1âˆ’Î±)Ã—1 = (1âˆ’Î±).

1.4 Channels of information transmission
75
Then
h(Y) = âˆ’1+Î±
2
log 1+Î±
2
âˆ’1âˆ’Î±
2
log 1âˆ’Î±
2
and
I(X : Y) = âˆ’1+Î±
2
log 1+Î±
2
âˆ’1âˆ’Î±
2
log 1âˆ’Î±
2
âˆ’1+Î±
which is maximised at Î± = 3/5, with the capacity given by

log5

âˆ’2 = 0.321928.
Our next goal is to prove the direct part of Shannonâ€™s SCT (Theorem 1.4.15). As
was demonstrated earlier, the proof is based on two consecutive Worked Examples
below.
Worked Example 1.4.24
Let F be a random coding, independent of the source
string U, such that the codewords F(u1),...,F(ur) are IID, with a probability dis-
tribution pF:
pF(v) = P(F(u) = v),
v (= v(N)) âˆˆJÃ—N.
Here, u j, j = 1,...,r, are source strings, and r = 2N(R+o(1)). Deï¬ne random code-
words V1,...,Vrâˆ’1 by
if U = u j
then Vi := F(ui)
for i < j (if any),
and Vi := F(ui+1)
for i â‰¥j (if any),
1 â‰¤j â‰¤r, 1 â‰¤i â‰¤r âˆ’1.
(1.4.30)
Then U (the message string), X = F(U) (the random codeword) and V1,...,Vrâˆ’1
are independent words, and each of X, V1,...,Vrâˆ’1 has distribution pF.
Solution This is straightforward and follows from the formula for the joint proba-
bility,
P(U = u j, X = x, V1 = v1,...,Vrâˆ’1 = vrâˆ’1)
= P(U = u j) pF(x) pF(v1)... pF(vrâˆ’1).
(1.4.31)
Worked Example 1.4.25
Check that for the random coding as in Worked Ex-
ample 1.4.24, for any Îº > 0,
E = EÎµ(F) â‰¤P(Î˜N â‰¤Îº)+r2âˆ’NÎº.
(1.4.32)
Here,
the
random
variable
Î˜N
is
deï¬ned
in
(1.4.21),
with
EÎ˜N
=
1
N I

X(N) : Y(N)
.

76
Essentials of Information Theory
Solution For given words x(= x(N)) and y(= y(N)) âˆˆJÃ—N, denote
Sy(x) :=
*
xâ€² âˆˆJÃ—N : P(y | xâ€²) â‰¥P(y | x)
+
.
(1.4.33)
That is, Sy(x) includes all words the ML decoder may produce in the situation
where x was sent and y received. Set, for a given non-random encoding rule f
and a source string u, Î´( f,u,y) = 1 if f(uâ€²) âˆˆSy( f(u)) for some uâ€² Ì¸= u, and
Î´( f,u,y) = 0 otherwise. Clearly, Î´( f,u,y) equals
1âˆ’
âˆ
uâ€²: uâ€²Ì¸=u
1

f(uâ€²) Ì¸âˆˆSy( f(u))

= 1âˆ’
âˆ
uâ€²:uâ€²Ì¸=u

1âˆ’1

f(uâ€²) âˆˆSy( f(u))

.
It is plain that, for all non-random encoding f, Îµ( f) â‰¤EÎ´( f,U,Y), and for all
random encoding F, E = EÎµ(F) â‰¤EÎ´(F,U,Y). Furthermore, for the random
encoding as in Worked Example 1.4.24, the expected value EÎ´(F,U,Y) does not
exceed
E

1âˆ’
râˆ’1
âˆ
i=1

1âˆ’1

Vi âˆˆSY(X)

= âˆ‘
x pX(x)âˆ‘
y P(y|x)
Ã— E

1âˆ’
râˆ’1
âˆ
i=1

1âˆ’1

Vi âˆˆSY(X)

|X = x, Y = y

,
which, owing to independence, equals
âˆ‘
x
pX(x)âˆ‘
y
P(y|x)

1âˆ’
râˆ’1
âˆ
i=1
E

1âˆ’1{Vi âˆˆSy(x)}

.
Furthermore, due to the IID property (as explained in Worked Example 1.4.24),
râˆ’1
âˆ
i=1
E

1âˆ’1{Vi âˆˆSy(x)}

= (1âˆ’Qy(x))râˆ’1 ,
where
Qy(x) := âˆ‘
xâ€²
1

xâ€² âˆˆSy(x)

pX(xâ€²).
Hence, the expected error-probability E â‰¤1âˆ’E(1âˆ’QY(X))râˆ’1.
Denote by T = T(Îº) the set of pairs of words x,y for which
Î˜N = 1
N log
p(x,y)
pX(x)pY(y) > Îº
and use the identity
1âˆ’(1âˆ’Qy(x))râˆ’1 =
râˆ’2
âˆ‘
j=0
(1âˆ’Qy(x))j Qy(x).
(1.4.34)

1.4 Channels of information transmission
77
Next observe that
1âˆ’(1âˆ’Qy(x))râˆ’1 â‰¤1,
when (x,y) Ì¸âˆˆT.
(1.4.35)
Owing to the fact that when (x,y) âˆˆT,

1âˆ’(1âˆ’Qy(x))r 
=
râˆ’1
âˆ‘
j=1

(1âˆ’Qy(x))j 
Qy(x) â‰¤(r âˆ’1)Qy(x),
this yields
E â‰¤P

(X,Y) Ì¸âˆˆT

+(r âˆ’1) âˆ‘
(x,y)âˆˆT
pX(x)P(y|x)Qy(x).
(1.4.36)
Now observe that
P

(X,Y) Ì¸âˆˆT

= P

Î˜N â‰¤Îº

.
(1.4.37)
Finally, for (x,y) âˆˆT and xâ€² âˆˆSy(x),
P(y|xâ€²) â‰¥P(y|x) â‰¥pY(y)2NÎº.
Multiplying by pX(xâ€²)
pY(y) gives P

X = xâ€²|Y = y

â‰¥pX(xâ€²)2NÎº. Then summing over
xâ€² âˆˆSy(x) gives 1 â‰¥P(SY(x)|Y = y) â‰¥Qy(x)2NÎº, or
Qy(x) â‰¤2âˆ’NÎº.
(1.4.38)
Substituting (1.4.37) and (1.4.38) into (1.4.36) yields (1.4.32).
Proof of Theorem 1.4.15 The proof of Theorem 1.4.15 can now be easily com-
pleted. Take R = c âˆ’2Îµ and Îº = c âˆ’Îµ. Then, as r = 2N(R+o(1)), we have that E
does not exceed
P(Î˜N â‰¤câˆ’Îµ)+2N(câˆ’2Îµ âˆ’c+Îµ +o(1)) = P(Î˜N â‰¤câˆ’Îµ)+2âˆ’NÎµ.
This quantity tends to zero as N â†’âˆ, because P(Î˜N â‰¤c âˆ’Îµ) â†’0 owing to the
condition Î˜N
P
âˆ’â†’c. Therefore, the random coding F gives the expected error prob-
ability that vanishes as N â†’âˆ.
By Theorem 1.4.11(i), for any N â‰¥1 there exists a deterministic encoding f = fN
such that, for R = c âˆ’2Îµ, lim
Nâ†’âˆÎµ( f) = 0. Hence, R is a reliable transmission rate.
This is true for any Îµ > 0, thus C â‰¥c.
The form of the argument used in the above proof was proposed by P. Whittle
(who used it in his lectures at the University of Cambridge) and appeared in [52],
pp. 114â€“117. We thank C. Goldie for this information. An alternative approach is
based on the concept of joint typicality; this approach is used in Section 2.2 where
we discuss channels with continuously distributed noise.

78
Essentials of Information Theory
Theorems 1.4.17 and 1.4.19 may be extended to the case of a memoryless chan-
nel with an arbitrary (ï¬nite) output alphabet, Jq = {0,...,q âˆ’1}. That is, at the
input of the channel we now have a word Y(N) = Y1 ... YN where each Yj takes a
(random) value from Jq. The memoryless property means, as before, that
Pch

y(N)|x(N)
=
N
âˆ
i=1
P(yi | xi),
(1.4.39)
and the symbol-to-symbol channel probabilities P(y|x) now form a 2Ã—q stochastic
matrix (the channel matrix). A memoryless channel is called symmetric if the rows
of the channel matrix are permutations of each other and double symmetric if in
addition the columns of the channel matrix are permutations of each other. The
deï¬nitions of the reliable transmission rate and the channel capacity are carried
through without change. The capacity of a memoryless binary channel is depicted
in Figure 1.8.
Theorem 1.4.26
The capacity of a memoryless symmetric channel with an out-
put alphabet Jq is
C â‰¤logqâˆ’h(p0,..., pqâˆ’1)
(1.4.40)
where (p0,..., pqâˆ’1) is a row of the channel matrix. The equality is realised in the
case of a double-symmetric channel, and the maximising random coding has IID
symbols Vi taking values from Jq with probability 1/q.
Proof
The proof is carried out as in the binary case, by using the fact that I(X1 :
Y1) = h(Y1)âˆ’h(Y1|X1) â‰¤logqâˆ’h(Y1|X1). But in the symmetric case
h(Y1 | X1) = âˆ’âˆ‘
x,y
P(X1 = x)P(y | x)logP(y | x)
= âˆ’âˆ‘
x
P(X1 = x)âˆ‘
k
pk log pk = h(p0,..., pqâˆ’1).
(1.4.41)
If, in addition, the columns of the channel matrix are permutations of each other,
then h(Y1) attains logq. Indeed, take a random coding as suggested. Then P(Y = y)
=
qâˆ’1
âˆ‘
x=0
P(X1 = x)P(y|x) = 1
q âˆ‘
x
P(y|x). The sum âˆ‘
x P(y|x) is along a column of the
channel matrix, and it does not depend on y. Hence, P(Y = y) does not depend on
y âˆˆIq, which means equidistribution.
Remark 1.4.27
(a) In the random coding F used in Worked Examples 1.4.24 and
1.4.25 and Theorems 1.4.6, 1.4.15 and 1.4.17, the expected error-probability E â†’0
with N â†’âˆ. This guarantees not only the existence of a â€˜goodâ€™ non-random coding
for which the error-probability E vanishes as N â†’âˆ(see Theorem 1.4.11(i)), but
also that â€˜almostâ€™ all codes are asymptotically good. In fact, by Theorem 1.4.11(ii),

1.4 Channels of information transmission
79
1
1 2
1
p
p)
(
C
Figure 1.8
with Ï = 1âˆ’
âˆš
E, P

Îµ(F) <
âˆš
E

â‰¥1âˆ’
âˆš
E â†’1, as N â†’âˆ. However, this does
not help to ï¬nd a good code: constructing good codes remains a challenging task
in information theory, and we will return to this problem later.
Worked Example 1.4.28
Bits are transmitted along a communication channel.
With probability Î» a bit may be inverted and with probability Î¼ it may be rendered
illegible. The fates of successive bits are independent. Determine the optimal cod-
ing for, and the capacity of, the channel.
Solution The channel matrix is 2Ã—3:
Î  =
 1âˆ’Î» âˆ’Î¼
Î»
Î¼
Î»
1âˆ’Î» âˆ’Î¼
Î¼

;
the rows are permutations of each other, and hence have equal entropies. Therefore,
the conditional entropy h(Y|X) equals
h(1âˆ’Î» âˆ’Î¼,Î»,Î¼) = âˆ’(1âˆ’Î» âˆ’Î¼)log(1âˆ’Î» âˆ’Î¼)âˆ’Î» logÎ» âˆ’Î¼ logÎ¼,
which does not depend on the distribution of the input symbol X.
Thus, I(X : Y) is maximised when h(Y) is. If pY(0) = p and pY(1) = q, then
h(Y) = âˆ’Î¼ logÎ¼ âˆ’plog pâˆ’qlogq,

80
Essentials of Information Theory
which is maximised when p = q = (1 âˆ’Î¼)/2 (by pooling), i.e. pX(0) = pX(1) =
1/2. This gives the following expression for the capacity:
âˆ’(1âˆ’Î¼)log 1âˆ’Î¼
2
+(1âˆ’Î» âˆ’Î¼)log(1âˆ’Î» âˆ’Î¼)+Î» logÎ»
= (1âˆ’Î¼)

1âˆ’h
1âˆ’Î» âˆ’Î¼
1âˆ’Î¼
,
Î»
1âˆ’Î¼

.
Worked Example 1.4.29
(a) (Data-processing inequality) Consider two independent channels in series. A
random variable X is sent through channel 1 and received as Y. Then it is sent
through channel 2 and received as Z. Prove that
I(X : Z) â‰¤I(X : Y),
so the further processing of the second channel can only reduce the mutual
information.
The independence of the channels means that given Y, the random variables
X and Z are conditionally independent. Deduce that
h(X,Z|Y) = h(X|Y)+h(Z|Y)
and
h(X,Y,Z)+h(Z) = h(X,Z)+h(Y,Z).
Deï¬ne I(X : Z|Y) as h(X|Y)+h(Z|Y)âˆ’h(X,Z|Y) and show that
I(X : Z|Y) = I(X : Y)âˆ’I(X : Z).
Does the equality hold in the data processing inequality
I(X : Z) Ì¸= I(X : Y)?
(b) The input and output of a discrete-time channel are both expressed in an alpha-
bet whose letters are the residue classes of integers mod r, where r is ï¬xed. The
transmitted letter [x] is received as [ j + x] with probability pj, where x and j
are integers and [c] denotes the residue class of c mod r. Calculate the capacity
of the channel.
Solution (a) Given Y, the random variables X and Z are conditionally independent.
Hence,
h(X | Y) = h(X | Y,Z) â‰¤h(X | Z),
and
I(X : Y) = h(X)âˆ’h(X|Y) â‰¥h(X)âˆ’h(X | Z) = I(X : Z).

1.4 Channels of information transmission
81
.  .  .    .  .  .    .  .  .
Figure 1.9
The equality holds iff X and Y are conditionally independent given Z, e.g. if the
second channel is error-free (Y,Z) â†’Z is one-to-one, or the ï¬rst channel is fully
noisy, i.e. X and Y are independent.
(b) The rows of the channel matrix are permutations of each other. Hence h(Y|X) =
h(p0,..., prâˆ’1) does not depend on pX. The quantity h(Y) is maximised when
pX(i) = 1/r, which gives
C = logr âˆ’h(p0,..., prâˆ’1).
Worked Example 1.4.30
Find the error-probability of a cascade of n identical
independent binary symmetric channels (MBSCs), each with the error-probability
0 < p < 1 (see Figure 1.9).
Show that the capacity of the cascade tends to zero as n â†’âˆ.
Solution The channel matrix of a combined n-cascade channel is Î n where
Î  =
 1âˆ’p
p
p
1âˆ’p

.
Calculating the eigenvectors/values yields
Î n = 1
2
 1+(1âˆ’2p)n
1âˆ’(1âˆ’2p)n
1âˆ’(1âˆ’2p)n
1+(1âˆ’2p)n

,
which gives the error-probability 1/2 (1âˆ’(1âˆ’2p)n). If 0 < p < 1, Î n converges
to
 1/2
1/2
1/2
1/2

,
and the capacity of the channel approaches
1âˆ’h(1/2,1/2) = 1âˆ’1 = 0.
If p = 0 or 1, the channel is error-free, and C â‰¡1.

82
Essentials of Information Theory
Worked Example 1.4.31
Consider two independent MBCs, with capacities
C1,C2 bits per second. Prove, or provide a counter-example to, each of the fol-
lowing claims about the capacity C of a compound channel formed as stated.
(a) If the channels are in series, with the output from one being fed into the other
with no further coding, then C = min[C1,C2].
(b) Suppose the channels are used in parallel in the sense that at every second a
symbol (from its input alphabet) is transmitted through channel 1 and the next
symbol through channel 2; each channel thus emits one symbol each second.
Then C = C1 +C2.
(c) If the channels have the same input alphabet and at each second a symbol is
chosen and sent simultaneously down both channels, then C = max[C1,C2].
(d) If channel i = 1,2 has matrix Î i and the compound channel has
Î  =
 Î 1
0
0
Î 2

,
then C is given by 2C = 2C1 +2C2. To what mode of operation does this corre-
spond?
Solution (a)
X
Y
Z
âˆ’â†’
channel 1
âˆ’â†’
channel 2
âˆ’â†’
As in Worked Example 1.4.29a,
I(X : Z) â‰¤I(X : Y),
I(X : Z) â‰¤I(Y : Z).
Hence,
C = sup
pX
I(X : Z) â‰¤sup
pX
I(X : Y) = C1
and similarly
C â‰¤sup
pY
I(Y : Z) = C2,
i.e. C â‰¤min[C1,C2]. A strict inequality may occur: take Î´ âˆˆ(0,1/2) and the
matrices
ch 1 âˆ¼
 1âˆ’Î´
Î´
Î´
1âˆ’Î´

,
ch 2 âˆ¼
 1âˆ’Î´
Î´
Î´
1âˆ’Î´

,
and
ch [1+2] âˆ¼1
2

(1âˆ’Î´)2 +Î´ 2
2Î´(1âˆ’Î´)
2Î´(1âˆ’Î´)
(1âˆ’Î´)2 +Î´ 2

.

1.4 Channels of information transmission
83
Here, 1/2 > 2Î´(1âˆ’Î´) > Î´,
C1 = C2 = 1âˆ’h(Î´,1âˆ’Î´),
and
C = 1âˆ’h

2Î´(1âˆ’Î´),1âˆ’2Î´(1âˆ’Î´)

< Ci
because h(Îµ,1âˆ’Îµ) strictly increases in Îµ âˆˆ[0,1/2].
(b)
X1
âˆ’â†’
channel 1
âˆ’â†’
Y1
X2
âˆ’â†’
channel 2
âˆ’â†’
Y2
The capacity of the combined channel
C = sup
p(X1,X2)
I

(X1,X2) : (Y1,Y2)

.
But
I

(X1,X2) : (Y1,Y2)

= h(Y1,Y2)âˆ’h

Y1,Y2|X1,X2

â‰¤h(Y1)+h(Y2)âˆ’h(Y1|X1)âˆ’h(Y2|X2)
= I(X1 : Y1)+I(X2 : Y2);
equality applies iff X1 and X2 are independent. Thus, C = C1 +C2 and the max-
imising p(X1,X2) is pX1 Ã— pX2 where pX1 and pX2 are maximisers for I(X1 : Y1) and
I(X2 : Y2).
(c)
channel 1
âˆ’â†’
Y1
â†—
X
â†˜
channel 2
âˆ’â†’
Y2
Here,
C = sup
pX
I

X : (Y1 : Y2)

and
I

(Y1 : Y2) : X

= h(X)âˆ’h

X|Y1,Y2

â‰¥h(X)âˆ’min
j=1,2 h(X|Yj) = min
j=1,2 I(X : Yj).

84
Essentials of Information Theory
Thus, C â‰¥max[C1,C2]. A strict inequality may occur: take an example from part
(a). Here, Ci = 1âˆ’h(Î´,1âˆ’Î´). Also,
I

(Y1,Y2) : X

= h(Y1,Y2)âˆ’h

Y1,Y2|X

= h(Y1,Y2)âˆ’h(Y1|X)âˆ’h(Y2|X)
= h(Y1,Y2)âˆ’2h(Î´,1âˆ’Î´).
If we set pX(0) = pX(1) = 1/2 then
(Y1,Y2) = (0,0)
with probability
	
(1âˆ’Î´)2 +Î´ 2

2,
(Y1,Y2) = (1,1)
with probability
	
(1âˆ’Î´)2 +Î´ 2

2,
(Y1,Y2) = (1,0)
with probability Î´(1âˆ’Î´),
(Y1,Y2) = (0,1)
with probability Î´(1âˆ’Î´),
with
h(Y1,Y2) = 1+h

2Î´(1âˆ’Î´), 1âˆ’2Î´(1âˆ’Î´)

,
and
I

(Y1,Y2): X

= 1+h

2Î´(1âˆ’Î´), 1âˆ’2Î´(1âˆ’Î´)

âˆ’2h(Î´,1âˆ’Î´)
> 1âˆ’h(Î´,1âˆ’Î´) = Ci.
Hence, C > Ci, i = 1,2.
(d)
X1
channel 1
âˆ’â†’
Y1
â†˜
â†—
â†’X : X1 or X2 â†’
â†—
â†˜
X2
channel 2
âˆ’â†’
Y2
The difference with part (c) is that every second only one symbol is sent, either
to channel 1 or 2. If we ï¬x probabilities Î± and 1 âˆ’Î± that a given symbol is sent
through a particular channel then
I(X : Y) = h(Î±,1âˆ’Î±)+Î±I(X1 : Y1)+(1âˆ’Î±)I(X2 : Y2).
(1.4.42)
Indeed, I(X : Y) = h(Y)âˆ’h(Y|X), where
h(Y) = âˆ’âˆ‘
y Î± pY1(y)logÎ± pY1(y)âˆ’âˆ‘
y (1âˆ’Î±)pY2(y)log(1âˆ’Î±)pY2(y)
= âˆ’Î± logÎ± âˆ’(1âˆ’Î±)log(1âˆ’Î±)+Î±h(Y1)+(1âˆ’Î±)h(Y2)

1.4 Channels of information transmission
85
and
h(Y|X)
= âˆ’âˆ‘
x,yÎ± pX1,Y1(x,y)log pY1|X1(y|x)
âˆ’âˆ‘
x,y(1âˆ’Î±)pX2,Y2(y|x)log pY2|X2(y|x)
= Î±h(Y1|X1)+(1âˆ’Î±)h(Y2|X2)
proving (1.4.42). This yields
C = max
0â‰¤Î±â‰¤1

h(Î±,1âˆ’Î±)+Î±C1 +(1âˆ’Î±)C2

;
the maximum is given by
Î± = 2C1/(2C1 +2C2), 1âˆ’Î± = 2C2/(2C1 +2C2),
and C = log

2C1 +2C2
.
Worked Example 1.4.32
A spy sends messages to his contact as follows. Each
hour either he does not telephone, or he telephones and allows the telephone to ring
a certain number of times â€“ not more than N, for fear of detection. His contact does
not answer, but merely notes whether or not the telephone rings, and, if so, how
many times. Because of deï¬ciencies in the telephone system, calls may fail to be
properly connected; the correct connection has probability p, where 0 < p < 1, and
is independent for distinct calls, but the spy has no means of knowing which calls
reach his contact. If connection is made, then the number of rings is transmitted
correctly. The probability of a false connection from another subscriber at a time
when no call is made may be neglected. Write down the channel matrix for this
channel and calculate the capacity explicitly. Determine a condition on N in terms
of p which will imply, with optimal coding, that the spy will always telephone.
Solution The channel alphabet is {0,1,...,N}: 0 âˆ¼non-call (in a given hour), and
j â‰¥1 âˆ¼j rings. The channel matrix is P(0|0) = 1, P(0| j) = 1âˆ’p and P( j| j) = p,
1 â‰¤j â‰¤N, and h(Y|X) = âˆ’q(plog p+(1âˆ’p)log(1âˆ’p)), where q = pX(X â‰¥1).
Furthermore, given q, h(Y) attains its maximum when
pY(0) = 1âˆ’pq, pY(k) = pq
N , 1 â‰¤k â‰¤N.
Maximising I(X : Y) = h(Y) âˆ’h(Y|X) in q yields p(1 âˆ’p)(1âˆ’p)/p Ã— (1 âˆ’pq) =
pq/N or
q = min
â¡
â£1
p

1+ 1
Np

1
1âˆ’p
(1âˆ’p)/pâˆ’1
, 1
â¤
â¦.

86
Essentials of Information Theory
The
condition
q = 1
is
equivalent
to
logN â‰¥âˆ’1
p log(1 âˆ’p),
i.e.
N â‰¥
1
(1âˆ’p)1/p .
1.5 Differential entropy and its properties
Deï¬nition 1.5.1
Suppose that the random variable X has a probability density
(PDF) p(x), x âˆˆRn:
P{X âˆˆA} =
0
A p(x)dx
for any (measurable) set A âŠ†Rn, where p(x) â‰¥0, x âˆˆRn, and
1
Rn dxp(x) = 1. The
differential entropy hdiff(X) is deï¬ned as
hdiff(X) = âˆ’
0
p(x)log p(x)dx,
(1.5.1)
under the assumption that the integral is absolutely convergent. As in the discrete
case, hdiff(X) may be considered as a functional of the density p : x âˆˆRn â†’R+ =
[0,âˆ). The difference is however that hdiff(X) may be negative, e.g. for a uniform
distribution on [0,a], hdiff(X) = âˆ’
1 a
0 dx(1/a)log(1/a) = loga < 0 for a < 1. [We
write x instead of x when x âˆˆR.] The relative, joint and conditional differential
entropy are deï¬ned similarly to the discrete case:
hdiff(X||Y) = Ddiff(p||pâ€²) = âˆ’
0
p(x)log pâ€²(x)
p(x) dx,
(1.5.2)
hdiff(X,Y) = âˆ’
0
pX,Y(x,y)log pX,Y(x,y)dxdy,
(1.5.3)
hdiff(X|Y)
= âˆ’
0
pX,Y(x,y)log pX|Y(x|y)dxdy
= hdiff(X,Y)âˆ’hdiff(Y),
(1.5.4)
again under the assumption that the integrals are absolutely convergent. Here, pX,Y
is the joint probability density and pX|Y the conditional density (the PDF of the
conditional distribution). Henceforth we will omit the subscript diff when it is clear
what entropy is being addressed. The assertions of Theorems 1.2.3(b),(c), 1.2.12,
and 1.2.18 are carried through for the differential entropies: the proofs are com-
pletely similar and will not be repeated.
Remark 1.5.2
Let 0 â‰¤x â‰¤1. Then x can be written as a sum âˆ‘
nâ‰¥1
Î±n2âˆ’n where
Î±n(= Î±n(x)) equals 0 or 1. For â€˜mostâ€™ of the numbers x the series is not reduced to a
ï¬nite sum (that is, there are inï¬nitely many n such that Î±n = 1; the formal statement

1.5 Differential entropy and its properties
87
is that the (Lebesgue) measure of the set of numbers x âˆˆ(0,1) with inï¬nitely many
Î±n(x) = 1 equals one). Thus, if we want to â€˜encodeâ€™ x by means of binary digits we
would need, typically, a codeword of an inï¬nite length. In other words, a typical
value for a uniform random variable X with 0 â‰¤X â‰¤1 requires inï¬nitely many bits
for its â€˜exactâ€™ description. It is easy to make a similar conclusion in a general case
when X has a PDF fX(x).
However, if we wish to represent the outcome of the random variable X with
an accuracy of ï¬rst n binary digits then we need, on average, n + h(X) bits where
h(X) is the differential entropy of X. Differential entropies can be both positive
and negative, and can even be âˆ’âˆ. Since h(X) can be of either sign, n+h(X) can
be greater or less than n. In the discrete case the entropy is both shift and scale
invariant since it depends only on probabilities p1,..., pm, not on the values of the
random variable. However, the differential entropy is shift but not scale invariant
as is evident from the identity (cf. Theorem 1.5.7)
h(aX +b) = h(X)+log|a|.
However, the relative entropy, i.e. Kullbackâ€“Leibler distance D(p||q), is scale
invariant.
Worked Example 1.5.3
Consider a PDF on 0 â‰¤x â‰¤eâˆ’1,
fr(x) = Cr
1
x(âˆ’lnx)r+1 , 0 < r < 1.
Then the differential entropy h(X) = âˆ’âˆ.
Solution After the substitution y = âˆ’lnx we obtain
0 eâˆ’1
0
1
x(âˆ’lnx)r+1 dx =
0 âˆ
1
1
yr+1 dy = 1
r .
Thus, Cr = r. Further, using z = ln(âˆ’lnx)
0 eâˆ’1
0
ln(âˆ’lnx)
x(âˆ’lnx)r+1 dx =
0 âˆ
0 zeâˆ’rzdz = 1
r2 .
Hence,
h(X)
= âˆ’
0
fr(x)ln fr(x)dx
=
0
fr(x)

âˆ’lnr +lnx+(r +1)ln(âˆ’lnx)

dx
= âˆ’lnr âˆ’
0 eâˆ’1
0

r
x(âˆ’lnx)r âˆ’r(r +1) ln(âˆ’lnx)
x(âˆ’lnx)r+1

dx,
so that for 0 < r < 1, the second term is inï¬nite, and two others are ï¬nite.

88
Essentials of Information Theory
Theorem 1.5.4
Let X = (X1,..., Xd) âˆ¼N(Î¼,C) be a multivariate normal random
vector, of mean Î¼ = (Î¼1,...,Î¼d) and covariance matrix C = (cij), i.e. EXi = Î¼i,
E(Xi âˆ’Î¼i)(Xj âˆ’Î¼j) = ci j = cji, 1 â‰¤i, j â‰¤d. Then
h(X) = 1
2 log
	
(2Ï€e)d detC

.
(1.5.5)
Proof
The PDF pX(x) is
p(x) =
1

(2Ï€)d detC
1/2 exp

âˆ’1
2

xâˆ’Î¼,Câˆ’1(xâˆ’Î¼)

, x âˆˆRd.
Then h(X) takes the form
âˆ’
0
Rd p(x)

âˆ’1
2 log

(2Ï€)d detC

âˆ’loge
2

xâˆ’Î¼,Câˆ’1(xâˆ’Î¼)

dx
= loge
2 E

âˆ‘
i, j
(xi âˆ’Î¼i)(xj âˆ’Î¼j)

Câˆ’1
ij

+ 1
2 log

(2Ï€)d detC

= loge
2 âˆ‘
i, j

Câˆ’1
i j E(xi âˆ’Î¼i)(xj âˆ’Î¼j)+ 1
2 log

(2Ï€)d detC

= loge
2 âˆ‘
i, j

Câˆ’1
i jCji + 1
2 log

(2Ï€)d detC

= d loge
2
+ 1
2 log

(2Ï€)d detC

= 1
2 log

(2Ï€e)d detC

.
Theorem 1.5.5
For a random vector X = (X1,...,Xd) with mean Î¼ and covari-
ance matrix C = (Ci j) (i.e. Ci j = E
	
(Xi âˆ’Î¼i)(Xj âˆ’Î¼j)] = Cji),
h(X) â‰¤1
2 log

(2Ï€e)d detC

,
(1.5.6)
with the equality iff X is multivariate normal.
Proof
Let p(x) be the PDF of X and p0(x) the normal density with mean Î¼
and covariance matrix C. Without loss of generality assume Î¼ = 0. Observe that
log p0(x) is, up to an additive constant term, a quadratic form in xk. Furthermore,

1.5 Differential entropy and its properties
89
for each monomial xixj,
1 dxp0(x)xixj =
1 dxp(x)xixj =Cij =Cji, and the moment
of quadratic form log p0(x) are equal. We have
0
â‰¤D(p||p0) (by Gibbs) =
0
p(x)log p(x)
p0(x)dx
= âˆ’h(p)âˆ’
1 p(x)log p0(x)dx
= âˆ’h(p)âˆ’
1 p0(x)log p0(x)dx
(by the above remark) = âˆ’h(p)+h(p0).
The equality holds iff p = p0.
Worked Example 1.5.6
(a) Show that the exponential density maximises the differential entropy among
the PDFs on [0,âˆ) with given mean, and the normal density maximises the
differential entropy among the PDFs on R with a given variance.
Moreover, let X = (X1,...,Xd)T be a random vector with EX = 0 and
EXiXj = Ci j,1 â‰¤i, j â‰¤d. Then hdiff(X) â‰¤1
2 log

(2Ï€e)d det(Cij)

, with equal-
ity iff X âˆ¼N(0,C).
(b) Prove that the bound h(X) â‰¤logm (cf. (1.2.7)) for a random variable X tak-
ing not more than m values admits the following generalisation for a discrete
random variable with inï¬nitely many values in Z+:
h(X) â‰¤1
2 log

2Ï€e(VarX + 1
12)

.
Solution
(a) For the Gaussian case, see Theorem 1.5.5. In the exponential
case, by the Gibbs inequality, for any random variable Y with PDF f(y),
1 f(y)log

f(y)eÎ»y/Î»

dy â‰¥0 or
h(Y) â‰¤(Î»EY logeâˆ’logÎ») = h(Exp(Î»)),
with equality iff Y âˆ¼Exp(Î»), Î» = (EY)âˆ’1.
(b) Let X0 be a discrete random variable with P(X0 = i) = pi, i = 1,2,..., and the
random variable U be independent of X0 and uniform on [0,1]. Set X = X0 +U.
For a normal random variable Y with VarX = VarY,
hdiff(X) â‰¤hdiff(Y) = 1
2 log

2Ï€eVar Y

= 1
2 log

2Ï€e(Var X + 1
12)

.

90
Essentials of Information Theory
The value of EX is not essential for h(X) as the following theorem shows.
Theorem 1.5.7
(a) The differential entropy is not changed under the shift: for all y âˆˆRd,
h(X +y) = h(X).
(b) The differential entropy changes additively under multiplication:
h(aX) = h(X)+log|a|,
for all a âˆˆR.
Furthermore, if A = (Ai j) is a d Ã— d non-degenerate matrix, consider the afï¬ne
transformation x âˆˆRd â†’Ax+y âˆˆRd.
(c) Then
h(AX +y) = h(X)+log|det A|.
(1.5.7)
Proof
The proof is straightforward and left as an exercise
Worked Example 1.5.8
(The data-processing inequality for the relative entropy)
Let S be a ï¬nite set, and Î  = (Î (x,y),x,y âˆˆS) be a stochastic kernel (that is, for
all x,y âˆˆS, Î (x,y) â‰¥0 and âˆ‘yâˆˆS Î (x,y) = 1; in other words, Î (x,y) is a transi-
tion probability in a Markov chain). Prove that D(p1Î ||p2Î ) â‰¤D(p1||p2) where
piÎ (y) = âˆ‘xâˆˆS pi(x)Î (x,y), y âˆˆS (that is, applying a Markov operator to both
probability distributions cannot increase the relative entropy).
Extend this fact to the case of the differential entropy.
Solution In the discrete case Î  is deï¬ned by a stochastic matrix (Î (x,y)). By the
log-sum inequality (cf. PSE II, p. 426), for all y
âˆ‘
x
p1(x)Î (x,y)log
âˆ‘
w p1(w)Î (w,y)
âˆ‘
z p2(z)Î (z,y)
â‰¤âˆ‘
x
p1(x)Î (x,y)log p1(x)Î (x,y)
p2(x)Î (x,y)
= âˆ‘
x
p1(x)Î (x,y)log p1(x)
p2(x) .
Taking summation over y we obtain
D(p1Î ||p2Î )
= âˆ‘
x âˆ‘
y
p1(x)Î (x,y)log
âˆ‘
w
p1(w)Î (w,y)
âˆ‘
z
p2(z)Î (z,y)
â‰¤âˆ‘
x âˆ‘
y
p1(x)Î (x,y)log p1(x)
p2(x) = D(p1||p2).
In the continuous case a similar inequality holds if we replace summation by
integration.

1.5 Differential entropy and its properties
91
The concept of differential entropy has proved to be useful in a great vari-
ety of situations, very often quite unexpectedly. We consider here inequalities for
determinants and ratios of determinants of positive deï¬nite matrices (cf. [39], [36]).
Recall that the covariance matrix C = (Ci j) of a random vector X = (X1,...,Xd)
is positive deï¬nite, i.e. for any complex vector y = (y1,...,yd), the scalar product
(y,Cy) = âˆ‘
i, j
Cijyiyj is written as
âˆ‘
i, j
E(Xi âˆ’Î¼i)(Xj âˆ’Î¼j)yiyj = E
âˆ‘
i
(Xi âˆ’Î¼i)yi

2
â‰¥0.
Conversely, for any positive deï¬nite matrix C there exists a PDF for which C is a
covariance matrix, e.g. a multivariate normal distribution (if C is not strictly posi-
tive deï¬nite, the distribution is degenerate).
Worked Example 1.5.9
If C is positive deï¬nite then log[detC] is concave in C.
Solution Take two positive deï¬nite matrices C(0) and C(1) and Î» âˆˆ[0,1]. Let X(0)
and X(1) be two multivariate normal vectors, X(i) âˆ¼N(0,C(i)). Set, as in the proof
of Theorem 1.2.18, X = X(Î›), where the random variable Î› takes two values, 0 and
1, with probabilities Î» and 1âˆ’Î», respectively, and is independent of X(0) and X(1).
Then the random variable X has covariance C = Î»C(0) + (1âˆ’Î»)C(1), although X
need not be normal. Thus,
1
2 log

2Ï€e)d + 1
2 log

det

Î»C(0) +(1âˆ’Î»)C(1)
= 1
2 log

(2Ï€e)d detC

â‰¥h(X) (by Theorem 1.5.5)
â‰¥h(X|Î›) (by Theorem 1.2.11)
= Î»
2 log

(2Ï€e)d detC(0)
+ 1âˆ’Î»
2
log

(2Ï€e)d detC(1)
= 1
2

log

2Ï€e)d +Î» log

detC(0)
+(1âˆ’Î»)log

detC(1)
.
This property is often called the Ky Fan inequality and was proved initially in
1950 by using much more involved methods. Another famous inequality is due to
Hadamard:
Worked Example 1.5.10
For a positive deï¬nite matrix C = (Cij),
detC â‰¤âˆ
i
Cii,
(1.5.8)
and the equality holds iff C is diagonal.

92
Essentials of Information Theory
Solution If X = (X1,...,Xn) âˆ¼N(0,C) then
1
2 log

(2Ï€e)d detC

= h(X) â‰¤âˆ‘
i
h(Xi) = âˆ‘
i
1
2 log(2Ï€eCii),
with equality iff X1,...,Xn are independent, i.e. C is diagonal.
Next we discuss the so-called entropyâ€“power inequality (EPI). The situation
with the EPI is quite intriguing: it is considered one of the â€˜mysteriousâ€™ facts of
information theory, lacking a straightforward interpretation. It was proposed by
Shannon; the book [141] contains a sketch of an argument supporting this inequal-
ity. However, the ï¬rst rigorous proof of the EPI only appeared nearly 20 years later,
under some rather restrictive conditions that are still the subject of painstaking im-
provement. Shannon used the EPI in order to bound the capacity of an additive
channel with continuous noise by that of a Gaussian channel; see Chapter 4. The
EPI is also related to important properties of monotonicity of entropy; an example
is Theorem 1.5.15 below.
The existing proofs of the EPI are not completely elementary; see [82] for one
of the more transparent proofs.
Theorem 1.5.11
(Entropyâ€“power inequality). For two independent random vari-
ables X and Y with PDFs fX(x) and fY(x), x âˆˆR1,
h(X +Y) â‰¥h(Xâ€² +Y â€²),
(1.5.9)
where Xâ€² and Y â€² are independent normal random variables with h(X) = h(Xâ€²) and
h(Y) = h(Y â€²).
In the d-dimensional case the entropyâ€“power inequality is as follows.
For two independent random variables X and Y with PDFs fX(x) and fY(x), x âˆˆRd,
e2h(X+Y)/d â‰¥e2h(X)/d +e2h(Y)/d.
(1.5.10)
It is easy to see that for d = 1 (1.5.9) and (1.5.10) are equivalent. In general,
inequality (1.5.9) implies (1.5.10) via (1.5.13) below which can be established
independently. Note that inequality (1.5.10) may be true or false for discrete ran-
dom variables. Consider the following example: let X âˆ¼Y be independent with
PX(0) = 1/6,PX(1) = 2/3,PX(2) = 1/6. Then
h(X) = h(Y) = ln6âˆ’2
3 ln4, h(X +Y) = ln36âˆ’16
36 ln8âˆ’18
36 ln18.
By inspection, e2h(X+Y) = e2h(X) + e2h(Y). If X and Y are non-random constants
then h(X) = h(Y) = h(X +Y) = 0, and the EPI is obviously violated. We conclude

1.5 Differential entropy and its properties
93
that the existence of PDFs is an essential condition that cannot be omitted. In a
different form EPI could be extended to discrete random variables, but we do not
discuss this theory here.
Sometimes the differential entropy is deï¬ned as h(X) = âˆ’Elog2 p(X); then
(1.5.10) takes the form 2h(X+Y)/d â‰¥2h(X)/d +2h(Y)/d.
The entropyâ€“power inequality plays a very important role not only in informa-
tion theory and probability but in geometry and analysis as well. For illustration
we present below the famous Brunnâ€“Minkowski theorem that is a particular case
of the EPI. Deï¬ne the set sum of two sets as
A1 +A2 = {x1 +x2 : x1 âˆˆA1,x2 âˆˆA2}.
By deï¬nition A+ /0 = A.
Theorem 1.5.12
(Brunnâ€“Minkowski)
(a) Let A1 and A2 be measurable sets. Then the volume
V(A1 +A2)1/d â‰¥V(A1)1/d +V(A2)1/d.
(1.5.11)
(b) The volume of the set sum of two sets A1 and A2 is greater than the volume
of the set sum of two balls B1 and B2 with the same volume as A1 and A2,
respectively:
V(A1 +A2) â‰¥V(B1 +B2),
(1.5.12)
where B1 and B2 are spheres with V(A1) = V(B1) and V(A2) = V(B2).
Worked Example 1.5.13
Let C1,C2 be positive-deï¬nite d Ã—d matrices. Then
[det(C1 +C2)]1/d â‰¥[detC1]1/d +[detC2]1/d.
(1.5.13)
Solution Let X1 âˆ¼N(0,C1),X2 âˆ¼N(0,C2), then X1 + X2 âˆ¼N(0,C1 +C2). The
entropyâ€“power inequality yields
(2Ï€e)

det(C1 +C2)
1/d = e2h(X1+X2)/d
â‰¥e2h(X1)/d +e2h(X2)/d = (2Ï€e)

detC1
1/d +(2Ï€e)

detC2
1/d.
Worked Example 1.5.14
A TÂ¨oplitz nÃ—n matrix C is characterised by the prop-
erty that Cij = Crs if |i âˆ’j| = |r âˆ’s|. Let Ck = C(1,2,...,k) denote the principal
minor of the TÂ¨oplitz positive-deï¬nite matrix formed by the rows and columns
1,...,k. Prove that for |C| = detC,
|C1| â‰¥|C2|1/2 â‰¥Â·Â·Â· â‰¥|Cn|1/n,
(1.5.14)

94
Essentials of Information Theory
|Cn|/|Cnâˆ’1| is decreasing in n, and
lim
nâ†’âˆ
|Cn|
|Cnâˆ’1| = lim
nâ†’âˆ|Cn|1/n.
(1.5.15)
Solution Let (X1,X2,...,Xn) âˆ¼N(0,Cn). Then the quantities h(Xk|Xkâˆ’1,...,X1)
are decreasing in k, since
h(Xk|Xkâˆ’1,...,X1) = h(Xk+1|Xk,...,X2) â‰¥h(Xk+1|Xk,...,X1),
where the equality follows from the TÂ¨oplitz assumption and the inequality from the
fact that the conditioning reduces the entropy. Next, we use the result of Problem
1.8b from Section 1.6 that the running averages
1
kh(X1,...Xk) = 1
k
k
âˆ‘
i=1
h(Xi|Xiâˆ’1,...X1)
are decreasing in k. Then (1.5.14) follows from
h(X1,...Xk) = 1
2 log[(2Ï€e)k|Ck|].
Since h(Xn|Xnâˆ’1,...,X1) is a decreasing sequence, it has a limit. Hence, by the
CesÂ´aro mean theorem
lim
nâ†’âˆ
h(X1,X2,...Xn)
n
= lim
nâ†’âˆ
1
n
n
âˆ‘
i=1
h(Xk|Xkâˆ’1,...,X1)
= lim
nâ†’âˆh(Xn|Xnâˆ’1,...,X1).
Translating this to determinants, we obtain (1.5.15).
The entropyâ€“power inequality could be immediately extended to the case of
several summands
e2h

X1+Â·Â·Â·+Xn

/d â‰¥
n
âˆ‘
i=1
e2h(Xi)/d.
But, more interestingly, the following intermediate inequality holds true. Let
X1,X2,...,Xn+1 be IID square-integrable random variables. Then
e2h

X1+Â·Â·Â·+Xn

/d â‰¥1
n
n+1
âˆ‘
j=1
e
2h

âˆ‘
iÌ¸= j
Xi

/d
.
(1.5.16)
As was established, the differential entropy is maximised by a Gaussian distri-
bution, under the constraint that the variance of the random variable under consid-
eration is bounded from above. We will state without proof the following important
result showing that the entropy increases on every summation step in the central
limit theorem.

1.6 Additional problems for Chapter 1
95
Theorem 1.5.15
Let X1,X2,... be IID square-integrable random variables with
EXi = 0, and VarXi = 1. Then
h
X1 +Â·Â·Â·+Xn
âˆšn

â‰¤h
X1 +Â·Â·Â·+Xn+1
âˆšn+1

.
(1.5.17)
1.6 Additional problems for Chapter 1
Problem 1.1
Let Î£1 and Î£2 be alphabets of sizes m and q. What does it mean to
say that f : Î£1 â†’Î£âˆ—
2 is a decipherable code? Deduce from the inequalities of Kraft
and Gibbs that if letters are drawn from Î£1 with probabilities p1,..., pm then the
expected word length is at least h(p1,..., pm)/logq.
Find a decipherable binary code consisting of codewords 011, 0111, 01111,
11111, and three further codewords of length 2. How do you check that the code
you have obtained is decipherable?
Solution Introduce Î£âˆ—= 2
nâ‰¥0 Î£n, the set of all strings with digits from Î£. We send
a message x1x2 ...xn âˆˆÎ£âˆ—
1 as the concatenation f(x1) f(x2)... f(xn) âˆˆÎ£âˆ—
2, i.e. f
extends to a function f âˆ—: Î£âˆ—
1 â†’Î£âˆ—
2. We say a code is decipherable if f âˆ—is injective.
Kraftâ€™s inequality states that a preï¬x-free code f : Î£1 â†’Î£âˆ—
2 with codeword-
lengths s1,...,sm exists iff
m
âˆ‘
i=1
qâˆ’si â‰¤1.
(1.6.1)
In fact, every decipherable code satisï¬es this inequality.
Gibbsâ€™ inequality states that if p1,..., pn and p1,..., pn are two probability dis-
tributions then
h(p1,..., pn) = âˆ’
n
âˆ‘
i=1
pi log pi â‰¤âˆ’
n
âˆ‘
i=1
pi log pi,
(1.6.2)
with equality iff pi â‰¡pi.
Suppose that f is decipherable with codeword-lengths s1,...,sm. Put pi = qâˆ’si/c
where c =
m
âˆ‘
i=1
qâˆ’si. Then, by Gibbsâ€™ inequality,
h(p1,..., pn)
â‰¤âˆ’
n
âˆ‘
i=1
pi log pi
= âˆ’
n
âˆ‘
i=1
pi(âˆ’si logqâˆ’logc)
=

âˆ‘
i
pisi

logq+

âˆ‘
i
pi

logc.

96
Essentials of Information Theory
By Kraftâ€™s inequality, c â‰¤1, i.e. logc â‰¤0. We obtain that
expected codeword-length âˆ‘
i
pisi â‰¥h(p1,..., pn)/logq.
In the example, the three extra codewords must be 00, 01, 10 (we cannot take
11, as then a sequence of ten 1s is not decodable). Reversing the order in every
codeword gives a preï¬x-free code. But preï¬x-free codes are decipherable. Hence,
the code is decipherable.
In conclusion, we present an alternative proof of necessity of Kraftâ€™s inequal-
ity. Denote s = maxsi; let us agree to extend any word in X to the length s,
say by adding some ï¬xed symbol. If x = x1x2 ...xsi âˆˆX , then any word of the
form x1x2 ...xsiysi+1 ...ys Ì¸âˆˆX because x is a preï¬x. But there are at most qsâˆ’si of
such words. Summing up on i, we obtain that the total number of excluded words
is âˆ‘m
i=1 qsâˆ’si. But it cannot exceed the total number of words qs. Hence, (1.6.1)
follows:
qs
m
âˆ‘
i=1
qâˆ’si â‰¤qs.
Problem 1.2
Consider an alphabet with m letters each of which appears with
probability 1/m. A binary Huffman code is used to encode the letters, in order to
minimise the expected codeword-length (s1 +Â·Â·Â·+sm)/m where si is the length of
a codeword assigned to letter i. Set s = max[si : 1 â‰¤i â‰¤m], and let nâ„“be the number
of codewords of length â„“.
(a) Show that 2 â‰¤ns â‰¤m.
(b) For what values of m is ns = m?
(c) Determine s in terms of m.
(d) Prove that nsâˆ’1 +ns = m, i.e. any two codeword-lengths differ by at most 1.
(e) Determine nsâˆ’1 and ns.
(f) Describe the codeword-lengths for an idealised model of English (with m =
27) where all the symbols are equiprobable.
(g) Let now a binary Huffman code be used for encoding symbols 1,...,m occur-
ring with probabilities p1 â‰¥Â·Â·Â· â‰¥pm > 0 where
âˆ‘
1â‰¤jâ‰¤m
p j = 1. Let s1 be the length
of a shortest codeword and sm of a longest codeword. Determine the maximal and
minimal values of sm and s1, and ï¬nd binary trees for which they are attained.
Solution (a) Bound ns â‰¥2 follows from the tree-like structure of Huffman codes.
More precisely, suppose ns = 1, i.e. a maximum-length codeword is unique and
corresponds to say letter i. Then the branch of length s leading to i can be pruned at
the end, without violating the preï¬x-free condition. But this contradicts minimality.

1.6 Additional problems for Chapter 1
97
2m
j
4 m
a
b
1 m
c
i
Figure 1.10
Bound ns â‰¤m is obvious. (From what is said below it will follow that ns is always
even.)
(b) ns = m means all codewords are of equal length. This, obviously, happens iff
m = 2k, in which case s = k (a perfect binary tree Tk with 2k leaves).
(c) In general,
s =
'
logm,
if m = 2k,
âŒˆlogmâŒ‰,
if m Ì¸= 2k.
The case m = 2k was discussed in (b), so let us assume that m Ì¸= 2k. Then 2k < m <
2k+1 where k = âŒŠlogmâŒ‹. This is clear from the observation that the binary tree for
probabilities 1/m (we will call it a binary m-tree Bm) contains the perfect binary
tree Tk but is contained in Tk+1. Hence, s is as above.
(d) Indeed, in the case of an equidistribution 1/m, ..., 1/m it is impossible to have
a branch of the tree whose length differs from the maximal value s by two or more.
In fact, suppose there is such a branch, Bi, of the binary tree leading to some letter i
and choose a branch Mj of maximal length s leading to a letter j. In a conventional
terminology, letter j was engaged in s merges and i in t â‰¤sâˆ’2 merges. Ultimately,
the branches Bi and Mj must merge, and this creates a contradiction. For example,
the â€˜least controversialâ€™ picture is still â€˜illegalâ€™; see Figure 1.10. Here, vertex i
carrying probability 1/m should have been joined with vertex a or b carrying each
probability 2/m, instead of joining a and b (as in the ï¬gure), as it creates vertex c
carrying probability 4/m.
(e) We conclude that (i) for m = 2k, the m-tree Bm coincides with Tk, (ii) for m Ì¸= 2k
we obtain Bm in the following way. First, take a binary tree Tk where k = [logm],
with 1 â‰¤m âˆ’2k < 2k. Then m âˆ’2k leaves of Tk are allowed to branch one step

98
Essentials of Information Theory
2 (m _ 2k )
2k+1 _ m
Figure 1.11
further: this generates 2(m âˆ’2k) = 2m âˆ’2k+1 leaves of tree Tk+1. The remaining
2k âˆ’(mâˆ’2k) = 2k+1 âˆ’m leaves of Tk are left intact. See Figure 1.11. So,
nsâˆ’1 = 2k+1 âˆ’m,
ns = 2mâˆ’2k+1,
where k = [logm].
(f) In the example of English, with equidistribution among m = 27 = 16+11 sym-
bols, we have 5 codewords of length 4 and 22 codewords of length 5. The average
codeword-length is
5Ã—4+22Ã—5
27
= 130
27 â‰ˆ4.8.
(g) The minimal value for s1 is 1 (obviously). The maximal value is
3
log2 m
4
,
i.e. the positive integer l with 2l < m â‰¤2l+1. The maximal value for sm is m âˆ’
1 (obviously). The minimal value is âŒŠlog2 mâŒ‹, i.e. the natural l such that 2lâˆ’1 <
m â‰¤2l.
The tree that yields s1 = 1 and sm = mâˆ’1 is given in Figure 1.12.
It is characterised by
i
f(i)
si
1
0
1
2
10
2
...
...
...
mâˆ’1
11. . . 10
mâˆ’1
m
11. . . 11
mâˆ’1
and is generated when
p1 > p2 +Â·Â·Â·+ pm > 2(p3 +Â·Â·Â·+ pm) > Â·Â·Â· > 2mâˆ’1pm.

1.6 Additional problems for Chapter 1
99
1
2
m â€“ 1 m
Figure 1.12
m = 16
Figure 1.13
A tree that maximises s1 and minimises sm corresponds to uniform probabilities
where p1 = Â·Â·Â· = pm = 1/m. When m = 2l, the branches of the tree have the same
length l = log2 m (a perfect binary tree); see Figure 1.13.
Otherwise, i.e. if 2l < m < 2l+1, the tree has 2l+1 âˆ’m leaves at level l and
2(mâˆ’2l) leaves at level l +1; see Figure 1.14.
Indeed, by the Huffman construction, the shortest branch cannot be larger than
âŒˆlog2 mâŒ‰and the longest shorter than âŒŠlog2 mâŒ‹, as the tree is always a subtree of a
perfect binary tree.
Problem 1.3
A binary erasure channel with erasure probability p is a discrete
memoryless binary channel (MBC) with channel matrix
1âˆ’p
p
0
0
p
1âˆ’p

.
State Shannonâ€™s second coding theorem (SCT) and use it to compute the capacity
of this channel.

100
Essentials of Information Theory
m = 18
Figure 1.14
Solution The SCT states that for an MBC
(capacity) = (maximum information transmitted per letter).
Here the capacity is understood as the supremum over all reliable information rates
while the RHS is deï¬ned as
max
X I(X : Y)
where the random variables X and Y represent an input and the corresponding
output.
The binary erasure channel keeps an input letter 0 or 1 intact with probability
1âˆ’p and turns it to a splodge âˆ—with probability p. An input random variable X is
0 with probability Î± and 1 with probability 1âˆ’Î±. Then the output random variable
Y takes three values:
P(Y = 0) = (1âˆ’p)Î±,
P(Y = 1) = (1âˆ’p)(1âˆ’Î±),
P(Y = âˆ—) = p.
Thus, conditional on the value of Y, we have
h(X|Y = 0) = 0,
h(X|Y = 1) = 0,
h(X|Y = âˆ—) = h(Î±),
â«
â¬
â­implying that h(X|Y) = ph(Î±).
Therefore,
capacity
= maxÎ± I(X : Y)
= maxÎ± [h(X)âˆ’h(X|Y)]
= maxÎ± [h(Î±)âˆ’ph(Î±)]
= (1âˆ’p)maxÎ± h(Î±) = 1âˆ’p,

1.6 Additional problems for Chapter 1
101
because h(Î±) = âˆ’Î± logÎ± âˆ’(1âˆ’Î±)log(1âˆ’Î±) attains it maximum value 1 at Î± =
1/2.
Problem 1.4
Let X and Y be two discrete random variables with corresponding
cumulative distribution functions (CDF) PX and PY.
(a) Deï¬ne the conditional entropy h(X|Y), and show that it satisï¬es
h(X|Y) â‰¤h(X),
giving necessary and sufï¬cient conditions for equality.
(b) For each Î± âˆˆ[0,1], the mixture random variable W(Î±) has PDF of the form
PW(Î±)(x) = Î±PX(x)+(1âˆ’Î±)PY(x).
Prove that for all Î± the entropy of W(Î±) satisï¬es:
h(W(Î±)) â‰¥Î±h(X)+(1âˆ’Î±)h(Y).
(c) Let hPo(Î») be the entropy of a Poisson random variable Po(Î»). Show that
hPo(Î») is a non-decreasing function of Î» > 0.
Solution (a) By deï¬nition,
h(X|Y)
= h(X,Y)âˆ’h(Y)
= âˆ’âˆ‘
x,yP(X = x,Y = y)logP(X = x,Y = y)
+âˆ‘
y P(Y = y)logP(Y = y).
The inequality h(X|Y) â‰¤h(X) is equivalent to
h(X,Y) â‰¤h(X)+h(Y),
and follows from the Gibbs inequality âˆ‘
i
pi log pi
qi
â‰¥0. In fact, take i = (x,y) and
pi = P(X = x,Y = y), qi = P(X = x)P(Y = y).
Then
0
â‰¤âˆ‘
x,yP(X = x,Y = y)log P(X = x,Y = y)
P(X = x)P(Y = y)
= âˆ‘
x,yP(X = x,Y = y)logP(X = x,Y = y)
âˆ’âˆ‘
x,yP(X = x,Y = y)
	
logP(X = x)+logP(Y = y)

= âˆ’h(X,Y)+h(X)+h(Y).
Equality here occurs iff X and Y are independent.

102
Essentials of Information Theory
(b) Deï¬ne a random variable T equal to 0 with probability Î± and 1 with probability
1âˆ’Î±. Then the random variable Z has the distribution W(Î±) where
Z =
'
X,
if T = 0,
Y,
if T = 1.
By part (a),
h(Z|T) â‰¤h(Z),
with the LHS = Î±h(X)+(1âˆ’Î±)h(Y), and the RHS = h(W(Î±)).
(c) Observe that for independent random variables X and Y, h(X + Y|X) =
h(Y|X) = h(Y). Hence, again by part (a),
h(X +Y) â‰¥h(X +Y|X) = h(Y).
Using this fact, for all Î»1 < Î»2, take X âˆ¼Po(Î»1), Y âˆ¼Po(Î»2 âˆ’Î»1), independently.
Then
h(X +Y) â‰¥h(X) implies hPo(Î»2) â‰¥hPo(Î»1).
Problem 1.5
What does it mean to transmit reliably at rate R through a binary
symmetric channel (MBSC) with error-probability p? Assuming Shannonâ€™s sec-
ond coding theorem (SCT), compute the supremum of all possible reliable trans-
mission rates of an MBSC. What happens if: (i) p is very small; (ii) p = 1/2; or
(iii) p > 1/2?
Solution An MBSC can transmit reliably at rate R if there is a sequence of codes
XN, N = 1,2,..., with
8
2NR9
codewords such that
e(XN) = max
xâˆˆXN P

error|x sent

â†’0 as N â†’âˆ.
By the SCT, the so-called operational channel capacity is supR = maxÎ± I(X : Y),
the maximum information transmitted per input symbol. Here X is a Bernoulli
random variable taking values 0 and 1 with probabilities Î± âˆˆ[0,1] and 1âˆ’Î±, and
Y is the output random variable for the given input X. Next, I(X : Y) is the mutual
entropy (information):
I(X : Y) = h(X)âˆ’h(X|Y) = h(Y)âˆ’h(Y|X).

1.6 Additional problems for Chapter 1
103
Observe that the binary entropy function h(x) â‰¤1 with equality for x = 1/2.
Selecting Î± = 1/2 conclude that the MBSC with error probability p has the
capacity
max
Î±
I(X : Y) = max
Î±
	
h(Y)âˆ’h(Y|X)

= max
Î±
	
h(Î± p+(1âˆ’Î±)(1âˆ’p))âˆ’Î·(p)

= 1+ plog p+(1âˆ’p)log(1âˆ’p).
(i) If p is small, the capacity is only slightly less than 1 (the capacity of a noiseless
channel).
(ii) If p = 1/2, the capacity is zero (the channel is useless).
(iii) If p > 1/2, we may swap the labels on the output alphabet, replacing p by
1âˆ’p, and the channel capacity is non-zero.
Problem 1.6
(i) What is bigger Ï€e or eÏ€?
(ii) Prove the log-sum inequality: for non-negative numbers a1,a2,...,an and
b1,b2,...,bn,
âˆ‘
i
ai log ai
bi
â‰¥

âˆ‘
i
ai

log
â›
â
âˆ‘
i
ai
âˆ‘
i
bi
â
â 
(1.6.3)
with equality iff ai/bi = const.
(iii) Consider two discrete probability distributions p(x) and q(x). Deï¬ne the rela-
tive entropy (or Kullbackâ€“Leibler distance) and prove the Gibbs inequality,
D(pâˆ¥q) = âˆ‘
x
p(x)log
 p(x)
q(x)

â‰¥0,
(1.6.4)
with equality iff p(x) = q(x) for all x.
Using (1.6.4), show that for any positive functions f(x) and g(x), and for any
ï¬nite set A,
âˆ‘
xâˆˆA
f(x)log
 f(x)
g(x)

â‰¥

âˆ‘
xâˆˆA
f(x)

log
â›
â
âˆ‘
xâˆˆA
f(x)
âˆ‘
xâˆˆA
g(x)
â
â .
Check that that for any 0 â‰¤p,q â‰¤1,
plog
 p
q

+(1âˆ’p)log
1âˆ’p
1âˆ’q

â‰¥(2log2 e)(qâˆ’p)2,
(1.6.5)
and show that for any probability distributions p = (p(x)) and q = (q(x)),
D(pâˆ¥q) â‰¥log2 e
2

âˆ‘
x
|p(x)âˆ’q(x)|
2
.
(1.6.6)

104
Essentials of Information Theory
Solution (i) Denote x = lnÏ€, and taking the logarithm twice obtain the inequality
xâˆ’1 > lnx. This is true as x > 1, hence eÏ€ > Ï€e.
(ii) Assume without loss of generality that ai > 0 and bi > 0. The function g(x) =
xlogx is strictly convex. Hence, by the Jensen inequality for any coefï¬cients âˆ‘ci =
1,ci â‰¥0,
âˆ‘cig(xi) â‰¥g
âˆ‘cixi

.
Selecting ci = bi

âˆ‘
j
b j
âˆ’1
and xi = ai/bi, we obtain
âˆ‘
i
ai
âˆ‘b j
log ai
bi
â‰¥

âˆ‘
i
ai
âˆ‘b j

log
â›
â
âˆ‘
i
ai
âˆ‘
i
b j
â
â 
which is the log-sum inequality.
(iii) There exists a constant c > 0 such that
logy â‰¥c

1âˆ’1
y

, with equality iff y = 1.
Writing B = {x : p(x) > 0},
D(pâˆ¥q)
=
âˆ‘
xâˆˆB
p(x)log p(x)
q(x)
â‰¥
c âˆ‘
xâˆˆB
p(x)

1âˆ’q(x)
p(x)

= c
	
1âˆ’q(B)

â‰¥0.
Equality holds iff q(x) â‰¡p(x). Next, write
f(A) = âˆ‘
xâˆˆA
f(x),
p(x) = f(x)
f(A)1(x âˆˆA),
g(A) = âˆ‘
xâˆˆA
g(x),
q(x) = g(x)
g(A)1(x âˆˆA).
Then
âˆ‘
xâˆˆA
f(x)log f(x)
g(x) = f(A) âˆ‘
xâˆˆA
p(x)log f(A)p(x)
g(A)q(x)
= f(A) âˆ‘
xâˆˆA
p(x)log p(x)
q(x)
:
;<
=
â‰¥by the previous part
+ f(A)log f(A)
g(A)
â‰¥f(A)log f(A)
g(A) .

1.6 Additional problems for Chapter 1
105
Inequality (1.6.5) could be easily established by inspection. Finally, consider
A = {x : p(x) â‰¤q(x)}. Since
âˆ‘
x
|p(x)âˆ’q(x)| = 2
	
q(A)âˆ’p(A)

= 2
	
p(Ac)âˆ’q(Ac)

,
then
D(p||q)
= âˆ‘
xâˆˆA
p(x)log p(x)
q(x) + âˆ‘
xâˆˆAc
p(x)log p(x)
q(x)
â‰¥p(A)log p(A)
q(A) + p(Ac)log p(Ac)
q(Ac)
â‰¥

2log2 e
	
p(A)âˆ’q(A)

2 = log2 e
2

âˆ‘
x
|p(x)âˆ’q(x)|
2
.
Problem 1.7
(a) Deï¬ne the conditional entropy, and show that for random vari-
ables U and V the joint entropy satisï¬es
h(U,V) = h(V|U)+h(U).
Given random variables X1,...,Xn, by induction or otherwise prove the chain rule
h(X1,...Xn) =
n
âˆ‘
i=1
h(Xi|X1,...,Xiâˆ’1).
(1.6.7)
(b) Deï¬ne the subset average over subsets of size k to be
h(n)
k
= âˆ‘
S:|S|=k
h(XS)
k
n
k

,
where h(XS) = h(Xs1,...,Xsk) for S = {s1,...,sk}. Assume that, for any i,
h(Xi|XS) â‰¤h(Xi|XT) when T âŠ†S, and i /âˆˆS.
By considering terms of the form
h(X1,...,Xn)âˆ’h(X1,...Xiâˆ’1,Xi+1,...,Xn)
show that h(n)
n
â‰¤h(n)
nâˆ’1.
Using the fact that h(k)
k
â‰¤h(k)
kâˆ’1, show that h(n)
k
â‰¤h(n)
kâˆ’1, for k = 2,...,n.
(c) Let Î² > 0, and deï¬ne
t(n)
k
= âˆ‘
S:|S|=k
eÎ²h(XS)/k>n
k

.
Prove that
t(n)
1
â‰¥t(n)
2
â‰¥Â·Â·Â· â‰¥t(n)
n .

106
Essentials of Information Theory
Solution (a) By deï¬nition, the conditional entropy
h(V|U) = h(U,V)âˆ’h(U)
= âˆ‘
u
P(U = u)h(V|U = u),
where h(V|U = u) is the entropy of the conditional distribution:
h(V|U = u) = âˆ’âˆ‘
v
P(V = v|U = u)logP(V = v|U = u).
The chain rule (1.6.7) is established by induction in n.
(b) By the chain rule
h(X1,...,Xn) = h(X1,...,Xnâˆ’1)+h(Xn|X1,...,Xnâˆ’1)
(1.6.8)
and, in general,
h(X1,...,Xn)
= h(X1,...,Xiâˆ’1,Xi+1,...,Xn)+h(Xi|X1,...,Xiâˆ’1,Xi+1,...,Xn)
â‰¤h(X1,...,Xiâˆ’1,Xi+1,...,Xn)+h(Xi|X1,...,Xiâˆ’1),
(1.6.9)
because
h(Xi|X1,...,Xiâˆ’1,Xi+1,...,Xn) â‰¤h(Xi|X1,...,Xiâˆ’1).
Then adding equations (1.6.9) from i = 1 to n:
nh(X1,...,Xn) â‰¤
n
âˆ‘
i=1
h(X1,...,Xiâˆ’1,Xi+1,...,Xn)
+
n
âˆ‘
i=1
h(Xi|X1,...,Xiâˆ’1).
The second sum in the RHS equals h(X1,...,Xn) by the chain rule (1.6.7). So,
(nâˆ’1)h(X1,...,Xn) â‰¤
n
âˆ‘
i=1
h(X1,...,Xiâˆ’1,Xi+1,...,Xn).
This implies that h(n)
n
â‰¤h(n)
nâˆ’1, since
1
nh(X1,...,Xn) â‰¤1
n
n
âˆ‘
i=1
h(X1,...,Xiâˆ’1,Xi+1,...,Xn)
nâˆ’1
.
(1.6.10)
In general, ï¬x a subset S of size k in {1,...,n}. Writing S(i) for S\{i}, we obtain
1
kh[X(S)] â‰¤1
k âˆ‘
iâˆˆS
h(X[S(i)])
k âˆ’1
,

1.6 Additional problems for Chapter 1
107
by the above argument. This yields
n
k

h(n)
k
=
âˆ‘
SâŠ‚{1,...,n}: |S|=k
h[X(S)]
k
â‰¤
âˆ‘
SâŠ‚{1,...,n}: |S|=kâˆ‘
iâˆˆS
h(X[S(i)])
k(k âˆ’1) .
(1.6.11)
Finally, each subset of size k âˆ’1, S(i), appears [n âˆ’(k âˆ’1)] times in the sum
(1.6.11). So, we can write h(n)
k
as

âˆ‘
TâŠ‚{1,...,n}: |T|=kâˆ’1
h[X(T)]
k âˆ’1

nâˆ’(k âˆ’1)
k
n
k

=
âˆ‘
TâŠ‚{1,...,n}: |T|=kâˆ’1
h[X(T)]
k âˆ’1
 n
k âˆ’1

= hn
kâˆ’1.
(c)
Starting
from
(1.6.11),
exponentiate
and
then
apply
the
arithmetic
mean/geometric mean inequality, to obtain for S0 = {1,2,...,n}
eÎ²h(X(S0))/n â‰¤eÎ²[h(S0(1))+Â·Â·Â·+h(S0(n))]/(n(nâˆ’1)) â‰¤1
n
n
âˆ‘
i=1
eÎ²h(S0(i))/(nâˆ’1)
which is equivalent to t(n)
n
â‰¤t(n)
nâˆ’1. Now we use the same argument as in (b), taking
the average over all subsets to prove that for all k â‰¤n,t(n)
k
â‰¤t(n)
kâˆ’1.
Problem 1.8
Let p1,..., pn be a probability distribution, with pâˆ—= maxi[pi].
Prove that
(i) âˆ’âˆ‘
i
pi log2 pi â‰¥âˆ’pâˆ—log2 pâˆ—âˆ’(1âˆ’pâˆ—)log2(1âˆ’pâˆ—);
(ii) âˆ’âˆ‘
i
pi log2 pi â‰¥log2(1/pâˆ—);
(iii) âˆ’âˆ‘
i
pi log2 pi â‰¥2(1âˆ’pâˆ—).
The random variables X and Y with values x and y from ï¬nite â€˜alphabetsâ€™ I and
J represent the input and output of a transmission channel, with the conditional
probability P(x | y) = P(X = x | Y = y). Let h(P(Â· | y)) denote the entropy of the
conditional distribution P(Â· | y), y âˆˆJ, and h(X | Y) denote the conditional entropy
of X given Y. Deï¬ne the ideal observer decoding rule as a map f : J â†’I such
that P( f(y) | y) = maxxâˆˆI P(x | y) for all y âˆˆJ. Show that under this rule the error-
probability
Ï€er(y) =
âˆ‘
xâˆˆI: xÌ¸= f(y)
P(x | y)
satisï¬es Ï€er(y) â©½1
2h(P(Â· | y)), and the expected error satisï¬es
EÏ€er(Y) â‰¤1
2h(X | Y).

108
Essentials of Information Theory
Solution Bound (i) follows from the pooling inequality. Bound (ii) holds as
âˆ’âˆ‘
i
pi log pi â‰¥âˆ‘
i
pi log 1
pâˆ—= log 1
pâˆ—.
To check (iii), it is convenient to use (i) for pâˆ—â‰¥1/2 and (ii) for pâˆ—â‰¤1/2. Assume
ï¬rst that pâˆ—â‰¥1/2. Then, by (i),
h(p1,..., pn) â‰¥h(pâˆ—,1âˆ’pâˆ—).
The function x âˆˆ(0,1) â†’h(x,1 âˆ’x) is concave, and its graph on (1/2,1) lies
strictly above the line x â†’2(1âˆ’x). Hence,
h(p1,..., pn) â‰¥2(1âˆ’pâˆ—).
On the other hand, if pâˆ—â‰¤1/2, we use (ii):
h(p1,..., pn) â‰¥log 1
pâˆ—.
Further, for 0 â‰¤x â‰¤1/2,
log 1
x â‰¥2(1âˆ’x); equality iff x = 1
2.
For the concluding part, we use (iii). Write
Ï€er(y) = 1âˆ’Pch( f(y)|y) = 1âˆ’pmax( Â· |y)
which is â‰¤h

P( Â· |y)

/2. Finally, the mean EÏ€er(Y) is bounded by taking expecta-
tions, since h(X|Y) = Eh

P( Â· |Y)

.
Problem 1.9
Deï¬ne the information rate H and the asymptotic equipartition
property of a source. Calculate the information rate of a Bernoulli source. Given a
memoryless binary channel, deï¬ne the channel capacity C. Assuming the statement
of Shannonâ€™s second coding theorem (SCT), deduce that C = suppX I(X : Y).
An erasure channel keeps a symbol intact with probability 1âˆ’p and turns it into
an unreadable splodge with probability p. Find the capacity of the erasure channel.
Solution The information rate H of a source U1,U2,... with a ï¬nite alphabet I is
the supremum of all values R > 0 such that there exists a sequence of sets An âˆˆ
I Ã—Â·Â·Â·Ã—I (n times) such that |An| â‰¤2nR and limnâ†’âˆP(Un
1 âˆˆAn) = 1.
The asymptotic equipartition property means that, as n â†’âˆ,
âˆ’1
n log pn(Un
1 ) â†’H,

1.6 Additional problems for Chapter 1
109
in one sense or another (here we mean convergence in probability). Here Un
1 =
U1 ... Un and pn(un
1) = P(Un
1 = un
1). The SCT states that if the random variable
âˆ’log pn(Un
1 )/n converges to a limit then the limit equals H.
A memoryless binary channel (MBC) has the conditional probability
Pch

Y(N)|X(N) sent

= âˆ
1â‰¤iâ‰¤N
P(yi|xi)
and produces an error with probability
Îµ(N) = âˆ‘
u
Psource

U = u

Pch

fN(Y(N)) Ì¸= u | fN(u) sent

,
where Psource stands for the source probability distribution, and one uses a code fN
and a decoding rule fN. A value R âˆˆ(0,1) is said to be a reliable transmission rate
if, given that Psource is an equidistribution over a set UN of source strings u with
â™¯UN = 2N[R+o(1)], there exist fN and fN such that
lim
Nâ†’âˆ
1
â™¯UN âˆ‘
uâˆˆUN
Pch

fN(Y(N)) Ì¸= u | fN(u) sent

= 0.
The channel capacity is the supremum of all reliable transmission rates.
For an erasure channel, the matrix is
0
1
â›
â
1âˆ’p
0
p
0
1âˆ’p
p
0
1
â‹†
â
â 
The conditional entropy h(Y|X) = h(p,1âˆ’p) does not depend on pX. Thus,
C = sup
pX
I(X : Y) = sup
pX
h(Y) âˆ’h(Y|X)
is achieved at pX(0) = pX(1) = 1/2 with
h(Y) = âˆ’(1âˆ’p)log[(1âˆ’p)/2]âˆ’plog p = h(p,1âˆ’p)+(1âˆ’p).
Hence, the capacity C = 1âˆ’p.
Problem 1.10
Deï¬ne Huffmanâ€™s encoding rule and prove its optimality among
decipherable codes. Calculate the codeword lengths for the symbol-probabilities
1
5, 1
5, 1
6, 1
10, 1
10, 1
10, 1
10, 1
30.
Prove, or provide a counter-example to, the assertion that if the length of a code-
word from a Huffman code equals l then, in the same code, there exists another
codeword of length lâ€² such that | l âˆ’lâ€² | â‰¤1.

110
Essentials of Information Theory
Solution An answer to the ï¬rst part:
probability
codeword
length
1/5
00
2
1/5
100
3
1/6
101
3
1/10
110
3
1/10
010
3
1/10
011
3
1/10
1110
4
1/30
1111
4
For the second part: a counter-example:
probability
codeword
length
1/2
0
1
1/8
100
3
1/8
101
3
1/8
110
3
1/8
111
3
Problem 1.11
A memoryless channel with the input alphabet {0,1} repro-
duces the symbol correctly with probability (n âˆ’1)/n2 and reverses it with prob-
ability 1/n2. [Thus, for n = 1 the channel is binary and noiseless.] For n â‰¥2 it
also produces 2(n âˆ’1) sorts of â€˜splodgesâ€™, conventionally denoted by Î±i and Î²i,
i = 1,...,n âˆ’1, with similar probabilities: P(Î±i|0) = (n âˆ’1)/n2, P(Î²i|0) = 1/n2,
P(Î²i|1) = (n âˆ’1)/n2, P(Î±i|1) = 1/n2. Prove that the capacity Cn of the channel
increases monotonically with n, and limnâ†’âˆCn = âˆ. How is the capacity affected
if we simply treat splodges Î±i as 0 and Î²i as 1?
Solution The channel matrix is
0
1
â›
âœ
âœ
âœ
â
nâˆ’1
n2
1
n2
nâˆ’1
n2
1
n2
...
nâˆ’1
n2
1
n2
1
n2
nâˆ’1
n2
1
n2
nâˆ’1
n2
...
1
n2
nâˆ’1
n2
â
âŸ
âŸ
âŸ
â .
0
1
Î±1
Î²1
...
Î±nâˆ’1
Î²nâˆ’1
The channel is double-symmetric (the rows and columns are permutations of each
other), hence the capacity-achieving input distribution is
pX(0) = pX(1) = 1
2,

1.6 Additional problems for Chapter 1
111
and the capacity Cn is given by
Cn = log(2n)+n 1
n2 log(n2)+n nâˆ’1
n2
log n2
nâˆ’1
= 1+3lognâˆ’nâˆ’1
n
log(nâˆ’1) â†’+âˆ, as n â†’âˆ.
Furthermore, extrapolating
C(x) = 1+3logxâˆ’

1âˆ’1
x

log(xâˆ’1), x â‰¥1,
we ï¬nd
dC(x)
dx
= 3
x âˆ’
1
xâˆ’1 +
1
x(xâˆ’1) âˆ’1
x2 log(xâˆ’1)
= 2
x âˆ’1
x2 log(xâˆ’1) = 1
x2 [ 2xâˆ’log(xâˆ’1)] > 0, x > 1.
Thus, Cn increases with n for n â‰¥1. When Î±i and Î²i are treated as 0 or 1, the
capacity does not change.
Problem 1.12
Let Xi, i = 1,2,... , be IID random variables, taking values 1,0
with probabilities p and (1âˆ’p). Prove the local De Moivreâ€“Laplace theorem with
a remainder term:
P(Sn = k) =
1
$
2Ï€y(1âˆ’y)n
exp[âˆ’nhp(y)+Î¸n(k)],
k = 1,...,nâˆ’1;
(1.6.12)
here
Sn = âˆ‘
1â‰¤iâ‰¤n
Xi,
y = k/n,
hp(y) = yln
 y
p

+(1âˆ’y)ln
 1âˆ’y
1âˆ’p

and the remainder Î¸n(k) obeys
|Î¸n(k)| <
1
6ny(1âˆ’y),
y = k/n.
Hint: Use the Stirling formula with the remainder term
n! =
âˆš
2Ï€n,
nneâˆ’n+Ï‘(n),
where
1
12n+1 < Ï‘(n) <
1
12n.
Find values k+ and kâˆ’, 0 â‰¤k+,kâˆ’â‰¤n (depending on n), such that P(Sn = k+)
is asymptotically maximal and P(Sn = kâˆ’) is asymptotically minimal, as n â†’âˆ,
and write the corresponding asymptotics.

112
Essentials of Information Theory
Solution Write
P(Sn = k) =
n
k

(1âˆ’p)nâˆ’kpk =
n!
k!(nâˆ’k)!(1âˆ’p)nâˆ’kpk
=
?
n
2Ï€k(nâˆ’k)
nn
kk(nâˆ’k)nâˆ’k (1âˆ’p)nâˆ’kpk
Ã—exp
	
Ï‘(n)âˆ’Ï‘(k)âˆ’Ï‘(nâˆ’k)

=
1
$
2Ï€ny(1âˆ’y)
exp

âˆ’klnyâˆ’(nâˆ’k)ln(1âˆ’y)
+kln p+(nâˆ’k)ln(1âˆ’p)

exp
	
Ï‘(n)âˆ’Ï‘(k)âˆ’Ï‘(nâˆ’k)

=
1
$
2Ï€ny(1âˆ’y)
exp
	
âˆ’nhp(y)

Ã—exp
	
Ï‘(n)âˆ’Ï‘(k)âˆ’Ï‘(nâˆ’k)

.
Now, as
|Ï‘(n)âˆ’Ï‘(k)âˆ’Ï‘(nâˆ’k)| <
1
12n + 1
12k +
1
12(nâˆ’k) <
2n2
12nk(nâˆ’k),
(1.6.12) follows, with Î¸n(k) = Ï‘(n) âˆ’Ï‘(k) âˆ’Ï‘(n âˆ’k). By the Gibbs inequality,
hp(y) â‰¥0 and hp(y) = 0 iff y = p. Furthermore,
dhp(y)
dy
= ln y
p âˆ’ln 1âˆ’y
1âˆ’p, and d2hp(y)
dy2
= 1
y +
1
1âˆ’y > 0,
which yields
dhp(y)
dy

y=p
= 0, dhp(y)
dy
< 0, 0 < y < p, and dhp(y)
dy
> 0, p < y < 1.
Hence,
hp = min hp(y) = 0, attained at y = p,
hp = max hp(y) = min

ln 1
p, ln
1
1âˆ’p

, attained at y = 0 or y = 1.
Thus, the maximal probability for n â‰«1 is for yâˆ—= p, i.e. k+ = âŒŠnpâŒ‹:
P

Sn = âŒŠnpâŒ‹

â‰ƒ
1
$
2Ï€np(1âˆ’p)
exp

Î¸n(âŒŠnpâŒ‹)

,
where
|Î¸n(âŒŠnpâŒ‹)| â‰¤
1
6np(1âˆ’p).
Similarly, the minimal probability is
P(Sn = 0) = pn,
if 0 < p â‰¤1/2,
P(Sn = n) = (1âˆ’p)n,
if 1/2 â‰¤p < 1.

1.6 Additional problems for Chapter 1
113
Problem 1.13
(a) Prove that the entropy h(X) = âˆ’
n
âˆ‘
i=1
p(i)log p(i) of a discrete
random variable X with probability distribution p = (p(1),..., p(n)) is a concave
function of the vector p.
Prove that the mutual entropy I(X : Y) = h(Y)âˆ’h(Y | X) between random vari-
ables X and Y, with P(X = i,Y = k) = pX(i)PY|X(k | i), i,k = 1,...,n, is a concave
function of the vector pX = (pX(1),..., pX(n)) for ï¬xed conditional probabilities
{PY|X(k | i)}.
(b) Show that
h(X) â‰¥âˆ’pâˆ—log2 pâˆ—âˆ’(1âˆ’pâˆ—)log2(1âˆ’pâˆ—),
where pâˆ—= maxx P(X = x), and deduce that, when pâˆ—â‰¥1/2,
h(X) â‰¥2(1âˆ’pâˆ—).
(1.6.13)
Show also that inequality (1.6.13) remains true even when pâˆ—< 1/2.
Solution (a) Concavity of h(p) means that
h(Î»1p1 +Î»2p2) â‰¥Î»1h(p1)+Î»2h(p2)
(1.6.14)
for any probability vectors pj = (p j(1),..., pj(n)), j = 1,2, and any Î»1,Î»2 âˆˆ(0,1)
with Î»1 +Î»2 = 1. Let X1 have distribution p1 and X2 have distribution p2. Let also
Z = 1,with probability Î»1 or 2,with probability Î»2,
and Y = XZ. Then the distribution of Y is Î»1p1 +Î»2p2. By Theorem 1.2.11(a),
h(Y) â‰¥h(Y|Z),
and by the deï¬nition of the conditional entropy
h(Y|Z) = Î»1h(X1)+Î»2h(X2).
This yields (1.6.14). Now
I(X : Y) = h(Y)âˆ’h(Y|X) = h(Y)âˆ’âˆ‘pX(i)h

PY|X(.|i)

.
(1.6.15)
If PY|X(.|.) are ï¬xed, the second term is a linear function of pX, hence concave. The
ï¬rst term, h(Y), is a concave function of pY which in turn is a linear function of
pX. Thus, h(Y) is concave in pX, and so is I(X : Y).
(b) Consider two cases, (i) pâˆ—â‰¥1/2 and (ii) pâˆ—â‰¤1/2. In case (i), by pooling
inequality,
h(X) â‰¥h(pâˆ—,1âˆ’pâˆ—) â‰¥(1âˆ’pâˆ—)log
1
pâˆ—(1âˆ’pâˆ—) â‰¥(1âˆ’pâˆ—)log 1
4 = 2(1âˆ’pâˆ—)

114
Essentials of Information Theory
as pâˆ—â‰¥1/2. In case (ii) we use induction in n, the number of values taken by X.
The initial step is n = 3: without loss of generality, assume that pâˆ—= p1 â‰¥p2 â‰¥p3.
Then 1/3 â‰¤p1 < 1/2 and (1âˆ’p1)/2 â‰¤p2 â‰¤p1. Write
h(p1, p2, p3) = h(p1,1âˆ’p1)+(1âˆ’p1)h(q,1âˆ’q), where q =
p2
1âˆ’p1
.
As 1/2 â‰¤q â‰¤p1(1âˆ’p1) â‰¤1,
h(q,1âˆ’q) â‰¥h(p1/(1âˆ’p1),(1âˆ’2p1)/(1âˆ’p1)),
i.e.
h(p1, p2, p3) â‰¥h(p1, p1,1âˆ’2p1) = 2p1 +h(2p1,1âˆ’2p1).
The inequality 2p1 +h(2p1,1âˆ’2p1) â‰¥2(1âˆ’p1) is equivalent to
h(2p1,1âˆ’2p1) > 2âˆ’4p1, 1/3 â‰¤p1 < 1/2,
or to
h(p,1âˆ’p) > 2âˆ’2p, 2/3 â‰¤p â‰¤1,
which follows from (a). Thus, for n = 3, h(p1, p2, p3) â‰¥2(1âˆ’pâˆ—) regardless of the
value of pâˆ—. The initial induction step is completed.
Make the induction hypothesis h(X) â‰¥2(1âˆ’pâˆ—) for the number of values of X
which is â‰¤nâˆ’1. Then take p = (p1,..., pn) and assume without loss of generality
that pâˆ—= p1 â‰¥Â·Â·Â· â‰¥pn. Write q = (p2/(1âˆ’p1),..., pnâˆ’1/(1âˆ’p1)) and
h(p) = h(p1,1âˆ’p1)+(1âˆ’p1)h(q) â‰¥h(p1,1âˆ’p1)+(1âˆ’p1)2(1âˆ’q1). (1.6.16)
The inequality h(p) â‰¥2(1âˆ’pâˆ—) will follow from
h(p1,1âˆ’p1)+(1âˆ’p1)(2âˆ’2q1) â‰¥(2âˆ’2p1)
which is equivalent to
h(p1,1âˆ’p1) â‰¥2(1âˆ’p1)(1âˆ’1+q1) = 2(1âˆ’p1)q1 = 2p2,
for 1/n â‰¤p1 < 1/2, (1âˆ’p1)/(nâˆ’1) â‰¤p2 < p1. But obviously
h(p1,1âˆ’p1) â‰¥2(1âˆ’p1) â‰¥2p2
(with equality at p1 = 0,1/2). Inequality (1.6.16) follows from the induction
hypothesis.
Problem 1.14
Let a probability distribution pi, i âˆˆI = {1,2,...,n}, be such that
log2(1/pi) is an integer for all i with pi > 0. Interpret I as an alphabet whose letters
are to be encoded by binary words. A Shannonâ€“Fano (SF) code assigns to letter i
a word of length â„“i = âŒˆlog2(1/pi)âŒ‰; by the Kraft inequality it may be constructed

1.6 Additional problems for Chapter 1
115
to be uniquely decodable. Prove the competitive optimality of the SF codes: if â„“â€²
i,
i âˆˆI, are the codeword-lengths of any uniquely decodable binary code then
P

â„“i < â„“â€²
i

â‰¥P

â„“â€²
i < â„“i

,
(1.6.17)
with equality iff â„“i â‰¡â„“â€²
i.
Hint: You may ï¬nd useful the inequality sgn(â„“âˆ’â„“â€²) â‰¤2â„“âˆ’â„“â€² âˆ’1, â„“,â„“â€² = 1,...,n.
Solution Write
P(â„“â€²
i < â„“i)âˆ’P(â„“â€²
i > â„“i) = âˆ‘
i:â„“â€²
i<â„“i
pi âˆ’âˆ‘
i:â„“â€²
i>â„“i
pi
= âˆ‘
i
pi sign (â„“i âˆ’â„“â€²
i)
= E sgn(â„“âˆ’â„“â€²) â‰¤E

2â„“âˆ’â„“â€² âˆ’1

,
as signx â‰¤2x âˆ’1 for integer x. Continue the argument with
E

2â„“âˆ’â„“â€² âˆ’1

= âˆ‘
i
pi

2â„“iâˆ’â„“â€²
i âˆ’1

= âˆ‘
i
2âˆ’â„“i
2â„“iâˆ’â„“â€²
i âˆ’1

= âˆ‘
i
2âˆ’â„“â€²
i âˆ’âˆ‘
i
2âˆ’â„“i
â‰¤1âˆ’âˆ‘
i
2âˆ’â„“i = 1âˆ’1 = 0
by the Kraft inequality. This yields the inequality
P

â„“i < â„“â€²
i

â‰¥P

â„“â€²
i < â„“i

.
To have equality, we must have (a) 2â„“iâˆ’â„“â€²
i âˆ’1 = 0 or 1,i âˆˆI (because sign x = 2x âˆ’1
only for x = 0 or 1), and (b) âˆ‘
i
2âˆ’â„“â€²
i = 1. As âˆ‘
i
2âˆ’â„“i = 1, the only possibility is
2â„“iâˆ’â„“â€²
i â‰¡1, i.e. â„“i = â„“â€²
i.
Problem 1.15
Deï¬ne the capacity C of a binary channel. Let CN =
(1/N)supI(X(N) : Y(N)), where I(X(N) : Y(N)) denotes the mutual entropy between
X(N), the random word of length N sent through the channel, and Y(N), the received
word, and where the supremum is over the probability distribution of X(N). Prove
that C â‰¤limsupNâ†’âˆCN.
Solution A binary channel is deï¬ned as a sequence of conditional probability dis-
tributions
P(N)
ch (y(N)|x(N)), N = 1,2,...,

116
Essentials of Information Theory
where x(N) = x1 ...xN is a binary word (string) at the input and y(N) = y1 ...yN a
binary word (string) at the output port. The channel capacity C is an asymptotic
parameter of the family
*
P(N)
ch ( Â· | Â· )
+
deï¬ned by
C = sup
	
R âˆˆ(0,1) : R is a reliable transmission rate

.
(1.6.18)
Here, a number R âˆˆ(0,1) is called a reliable transmission rate (for a given channel)
if, given that the random source string is equiprobably distributed over a set U (N)
with â™¯U (N) = 2N[R+O(1)], there exist an encoding rule f (N) : U (N) â†’XN âŠ†{0,1}N
and a decoding rule f (N) : {0,1}N â†’U (N) such that the error probability e(N) â†’0
as N â†’âˆis given by
e(N) := âˆ‘
uâˆˆU (N)
1
â™¯U (N) P(N)
ch
(
y(N) : f (N) 
y(N)
Ì¸= u
)
|f (N)(u)

;
(1.6.19)
note
e(N) = e(N) 
f (N), f (N)
.
The converse part of Shannonâ€™s second coding theorem (SCT) states that
C â‰¤limsup
Nâ†’âˆ
1
N sup
PX(N)
I

X(N) : Y(N)
,
(1.6.20)
where I

X(N) : Y(N)
is the mutual entropy between the random input and output
strings X(N) and Y(N) and PX(N) is a distribution of X(N).
For the proof, it sufï¬ces to check that if â™¯U (N) = 2N[R+O(1)] then, for all f (N)
and f (N),
e(N) â‰¥1âˆ’CN +o(1)
R+o(1)
(1.6.21)
where
CN = 1
N sup
PX(N)
I

X(N) : Y(N)
.
Indeed,
if
R
>
limsupNâ†’âˆCN
then,
according
to
(1.6.21)
liminfNâ†’âˆinf f (N), f (N) e(N) > 0 and R is not reliable.
To prove (1.6.21), assume, without loss of generality, that f (N) is lossless. Then
the input word x(N) is equidistributed, with probability 1

â™¯U (N)
. For all decod-
ing rules f (N) and any N large enough,
clNCN â‰¥I

X(N) : Y(N)
â‰¥I

X(N) : f

Y(N)
= h

X(N)
âˆ’h

X(N)| f

Y(N)
= log

â™¯U (N)
âˆ’h

X(N)| f

Y(N)
â‰¥log

â™¯U (N)
âˆ’1âˆ’Îµ(N) log

â™¯U (N) âˆ’1

.
(1.6.22)

1.6 Additional problems for Chapter 1
117
The last bound here follows from the generalised Fano inequality
h

X(N)| f

Y(N)
â‰¤âˆ’e(N) loge(N) âˆ’

1âˆ’e(N)
log

1âˆ’e(N)
+e(N) log

â™¯U (N) âˆ’1

â‰¤1+e(N) log

â™¯U (N) âˆ’1

.
Now, from (1.6.22),
NCN â‰¥N
	
R+o(1)

âˆ’1âˆ’e(N) log

2N[R+o(1)] âˆ’1

,
i.e.
e(N) â‰¥N
	
R+o(1)

âˆ’NCN âˆ’1
log

2N[R+o(1)] âˆ’1

= 1âˆ’CN +o(1)
R+o(1) ,
as required.
Problem 1.16
A memoryless channel has input 0 and 1, and output 0, 1 and âˆ—
(illegible). The channel matrix is given by
P(0|0) = 1,P(0|1) = P(1|1) = P(âˆ—|1) = 1/3.
Calculate the capacity of the channel and the input probabilities pX(0) and pX(1)
for which the capacity is achieved.
Someone suggests that, as the symbol âˆ—may occur only from 1, it is to your
advantage to treat âˆ—as 1: you gain more information from the output sequence, and
it improves the channel capacity. Do you agree? Justify your answer.
Solution Use the formula
C = sup
pX
I(X : Y) = sup
pX
	
h(Y)âˆ’h(Y|X)

,
where pX is the distribution of the input symbol:
pX(0) = p, pX(1) = 1âˆ’p, 0 â‰¤p â‰¤1.
So, calculate I(X : Y) as a function of p:
h(Y) = âˆ’pY(0)log pY(0)âˆ’pY(1)log pY(1)âˆ’pY(â‹†)log pY(â‹†).
Here
pY(0) = p+(1âˆ’p)/3 = (1+2p)/3,
pY(1) = pY(â‹†) = (1âˆ’p)/3,
and
h(Y) = âˆ’1+2p
3
log 1+2p
3
âˆ’2(1âˆ’p)
3
log 1âˆ’p
3
.

118
Essentials of Information Theory
Also,
h(Y|X)
= âˆ’âˆ‘
x=0,1
pX(x)âˆ‘
y P(y|x)logP(y|x)
= âˆ’pX(1)log1/3 = (1âˆ’p)log3.
Thus,
I(X : Y) = âˆ’1+2p
3
log 1+2p
3
âˆ’2(1âˆ’p)
3
log 1âˆ’p
3
âˆ’(1âˆ’p)log3.
Differentiating yields
d
dpI(X : Y) = âˆ’2/3(log(1/3+2p/3)+2/3log(1/3âˆ’p/3)+log3.
Hence, the maximum max I(X : Y) is found from relation
2
3 log 1âˆ’p
1+2p +log3 = 0.
This yields
log 1âˆ’p
1+2p = âˆ’3
2 log3 := b,
and
1âˆ’p
1+2p = 2b, i.e. 1âˆ’2b = p

1+2b+1
.
The answer is
p = 1âˆ’2b
1+2b+1 .
For the last part, write
I(X : Y) = h(X)âˆ’h(X|Y) â‰¤h(X)âˆ’h(X|Y â€²) = I(X : Y â€²)
for any Y â€² that is a function of Y; the equality holds iff Y and X are conditionally
independent, given Y â€². It is the case of our channel, hence the suggestion leaves the
capacity the same.
Problem 1.17
(a) Given a pair of discrete random variables X, Y, deï¬ne the
joint and conditional entropies h(X,Y) and h(X|Y).
(b) Prove that h(X,Y) â‰¥h(X|Y) and explain when equality holds.
(c) Let 0 < Î´ < 1, and prove that
h(X|Y) â‰¥

log(Î´ âˆ’1)

P(q(X,Y) â‰¤Î´),
where q(x,y) = P(X = x|Y = y). For which Î´ and for which X, Y does equality
hold here?

1.6 Additional problems for Chapter 1
119
Solution (a) The conditional entropy is given by
h(X|Y) = âˆ’Elogq(x,y) = âˆ’âˆ‘
x,y
P(X = x,Y = y)logq(x,y)
where
q(x,y) = P(X = x|Y = y).
The joint entropy is given by
h(X,Y) = âˆ’âˆ‘
x,y
P(X = x,Y = y)logP(X = x,Y = y).
(b) From the deï¬nition,
h(X,Y) = h(X|Y)âˆ’âˆ‘
y
P(Y = y)logP(Y = y) â‰¥h(X|Y).
The equality in (b) is achieved iff h(Y) = 0, i.e. Y is constant a.s.
(c) By Chebyshevâ€™s inequality,
P

q(X,Y) â‰¤Î´) = P

âˆ’logq(X,Y â‰¥log1/Î´)
â‰¤
1
log1/Î´ E
	
âˆ’logq(X,Y)

=
1
log1/Î´ h(X|Y).
Here equality holds iff
P

q(X,Y) = Î´) = 1.
This requires that (i) Î´ = 1/m where m is a positive integer and (ii) for all y âˆˆ
support Y, there exists a set Ay of cardinality m such that
P(X = x|Y = y) = 1
m,
for x âˆˆAy.
Problem 1.18
A text is produced by a Bernoulli source with alphabet 1,2,...,m
and probabilities p1, p2,..., pm. It is desired to send this text reliably through a
memoryless binary symmetric channel (MBSC) with the row error-probability pâˆ—.
Explain what is meant by the capacity C of the channel, and show that
C = 1âˆ’h(pâˆ—,1âˆ’pâˆ—).
Explain why reliable transmission is possible if
h(p1, p2,..., pm)+h(pâˆ—,1âˆ’pâˆ—) < 1
and is impossible if
h(p1, p2,..., pm)+h(pâˆ—,1âˆ’pâˆ—) > 1,
where h(p1, p2,..., pm) = âˆ’
m
âˆ‘
i=1
pi log2 pi.

120
Essentials of Information Theory
Solution The asymptotic equipartition property for a Bernoulli source states that
the number of distinct strings (words) of length n emitted by the source is â€˜typi-
callyâ€™ 2nH+o(n), and they have â€˜nearly equalâ€™ probabilities 2âˆ’nH+o(n):
lim
nâ†’âˆP

2âˆ’n(H+Îµ) â‰¤Pn(U(n)) â‰¤2âˆ’n(Hâˆ’Îµ)
= 1.
Here, H = h(p1,..., pn).
Denote
Tn(= Tn(Îµ)) =
(
u(n) : 2âˆ’n(H+Îµ) â‰¤Pn(u(n)) â‰¤2âˆ’n(Hâˆ’Îµ))
and observe that
lim
nâ†’âˆ
1
n logâ™¯Tn = H, i.e. limsup
nâ†’âˆ
1
n logâ™¯Tn < H +Îµ.
By the deï¬nition of the channel capacity, the words u(n) âˆˆTn(Îµ) may be encoded
by binary codewords of length Râˆ’1(H +Îµ) and sent reliably through a memoryless
symmetric channel with matrix
1âˆ’pâˆ—
pâˆ—
pâˆ—
1âˆ’pâˆ—

for any R < C where
C = sup
pX
I(X : Y) = sup
pX
[h(Y)âˆ’h(Y|X)].
The supremum here is taken over all distributions
pX = (pX(0), pX(1))
of the input binary symbol X; the conditional distribution of the output symbol Y
is given by
P(Y = y|X = x) =
'
1âˆ’pâˆ—,
y = x,
pâˆ—,
y Ì¸= x.
We see that
h(Y|X) = âˆ’pX(0)
	
âˆ’(1âˆ’pâˆ—)log(1âˆ’pâˆ—)âˆ’pâˆ—log pâˆ—
+pX(1)
	
âˆ’pâˆ—log pâˆ—âˆ’(1âˆ’pâˆ—)log(1âˆ’pâˆ—)

= h(pâˆ—,1âˆ’pâˆ—),

1.6 Additional problems for Chapter 1
121
independently of pX. Hence,
C = sup
pX
h(Y)âˆ’h(pâˆ—,1âˆ’pâˆ—) = 1âˆ’h(pâˆ—,1âˆ’pâˆ—),
because h(Y) is achieved for
pY(0) = pY(1) = 1/2 (occurring when pX(0) = pX(1) = 1/2).
Therefore, if
H < C
â‡”
h(p1,..., pn)+h(pâˆ—,1âˆ’pâˆ—) < 1,
then Râˆ’1(H + Îµ) can be made < 1, for Îµ > 0 small enough and R < C close to
C. This means that there exists a sequence of codes fn of length n such that the
error-probability, while using encoding fn and the ML decoder, is
â‰¤P

u(n) Ì¸âˆˆTn

+P

u(n) âˆˆTn; an error while using fn(u(n)) and the ML decoder

â†’0, as n â†’âˆ,
since both probabilities go to 0.
On the other hand,
H > C
â‡”
h(p1,..., pn)+h(pâˆ—,1âˆ’pâˆ—) > 1,
then Râˆ’1H > 1 for all R < C, and we cannot encode words u(n) âˆˆTn by codewords
of length n so that the error-probability tends to 0. Hence, no reliable transmission
is possible.
Problem 1.19
A Markov source with an alphabet of m characters has a transition
matrix Pm whose elements pjk are speciï¬ed by
p11 = pmm = 2/3,
pj j = 1/3
(1 < j < m),
pj j+1 = 1/3
(1 â‰¤j < m),
pj jâˆ’1 = 1/3
(1 < j â‰¤m).
All other elements are zero. Determine the information rate of the source.
Denote the transition matrix thus speciï¬ed by Pm. Consider a source in an
alphabet of m+n characters whose transition matrix is
Pm
0
0
Pn

, where the zeros
indicate zero matrices of appropriate size. The initial character is supposed uni-
formly distributed over the alphabet. What is the information rate of the source?

122
Essentials of Information Theory
Solution The transition matrix
Pm =
â›
âœ
âœ
âœ
âœ
âœ
â
2/3
1/3
0
0
...
0
1/3
1/3
1/3
0
...
0
0
1/3
1/3
1/3
...
0
...
...
...
...
...
...
0
0
0
0
...
2/3
â
âŸ
âŸ
âŸ
âŸ
âŸ
â 
is Hermitian and so has the equilibrium distribution Ï€ = (Ï€i) with Ï€i = 1/m, 1 â‰¤
i â‰¤m (equidistribution). The information rate equals
Hm = âˆ’âˆ‘
j,k
Ï€jp jk log p jk
= âˆ’1
m

2
2
3 log 2
3 + 1
3 log 1
3

+3(mâˆ’2)1
3 log 1
3

= log3âˆ’4
3m.
The source with transition matrix
Pm
0
0
Pn

is non-ergodic, and its information
rate is the maximum of the two rates
max
	
Hm,Hn

= Hmâˆ¨n.
Problem 1.20
Consider a source in a ï¬nite alphabet. Deï¬ne Jn = nâˆ’1h(U(n))
and Kn = h(Un+1|U(n)) for n = 1,2,.... Here Un is the nth symbol in the sequence
and U(n) is the string constituted by the ï¬rst n symbols, h(U(n)) is the entropy and
h(Un+1|U(n)) the conditional entropy. Show that, if the source is stationary, then Jn
and Kn are non-increasing and have a common limit.
Suppose the source is Markov and not necessarily stationary. Show that the
mutual information between U1 and U2 is not smaller than that between U1 and U3.
Solution For the second part, the Markov property implies that
P(U1 = u1|U2 = u2,U3 = u3) = P(U1 = u1|U2 = u2).
Hence,
I(U1 : (U2,U3)) = E
	
âˆ’log P(U1 = u1|U2 = u2,U3 = u3)
P(U1 = u1)

= E
	
âˆ’log P(U1 = u1|U2 = u2)
P(U1 = u1)

= I(U1 : U2).
Since
I(U1 : (U2,U3)) â‰¥I(U1 : U3),
the result follows.

1.6 Additional problems for Chapter 1
123
Problem 1.21
Construct a Huffman code for a set of 5 messages with probabil-
ities as indicated below
Message
1
2
3
4
5
Probability
0.1
0.15
0.2
0.26
0.29
Solution
Message
1
2
3
4
5
Probability
0.1
0.15
0.2
0.026
0.029
Codeword
101
100
11
01
00
The expected codeword-length equals 2.4.
Problem 1.22
State the ï¬rst coding theorem (FCT), which evaluates the infor-
mation rate for a source with suitable long-run properties. Give an interpretation of
the FCT as an asymptotic equipartition property. What is the information rate for a
Bernoulli source?
Consider a Bernoulli source that emits symbols 0,1 with probabilities 1âˆ’p and
p respectively, where 0 < p < 1. Let Î·(p) = âˆ’plog pâˆ’(1âˆ’p)log(1âˆ’p) and let
Îµ > 0 be ï¬xed. Let U(n) be the string consisting of the ï¬rst n symbols emitted by
the source. Prove that there is a set Sn of possible values of U(n) such that
P

U(n) âˆˆSn

â‰¥1âˆ’

log
p
1âˆ’p
2 p(1âˆ’p)
nÎµ2
,
and so that for each u(n) âˆˆSn the probability that P

U(n) = u(n)
lies between
2âˆ’n(h+Îµ) and 2âˆ’n(hâˆ’Îµ).
Solution For the Bernoulli source
âˆ’1
n logPn(U(n)) = âˆ’1
n âˆ‘
1â‰¤jâ‰¤n
logP(Uj) â†’Î·(p),
in the sense that for all Îµ > 0, by Chebyshev,
P
âˆ’1
n logPn(U(n))âˆ’h
 > Îµ

â‰¤
1
Îµ2n2 Var

âˆ‘
1â‰¤jâ‰¤n
logP(Uj)

= 1
Îµ2n Var[logP(U1)].
(1.6.23)

124
Essentials of Information Theory
Here
P(Uj) =
'
1âˆ’p,
if Uj = 0,
p,
if Uj = 1,
Pn(U(n)) = âˆ
1â‰¤jâ‰¤n
P(Uj),
and
Var

âˆ‘
1â‰¤jâ‰¤n
logP(Uj)

= âˆ‘
1â‰¤jâ‰¤n
Var

logP(Uj)

where
Var

logP(Uj)

= E
	
logP(Uj)

2 âˆ’
	
ElogP(Uj)

2
= p(log p)2 +(1âˆ’p)(log(1âˆ’p))2 âˆ’

plog p+(1âˆ’p)log(1âˆ’p)
2
= p(1âˆ’p)

log
p
1âˆ’p
2
.
Hence, the bound (1.6.23) yields
P

2âˆ’n(h+Îµ) â‰¤Pn(U(n)) â‰¤2âˆ’n(hâˆ’Îµ)
â‰¥1âˆ’1
nÎµ2 p(1âˆ’p)

log
p
1âˆ’p
2
.
It now sufï¬ces to set
Sn = {u(n) = u1 ...un : 2âˆ’n(h+Îµ) â‰¤P(U(n) = u(n)) â‰¤2âˆ’n(hâˆ’Îµ)},
and the result follows.
Problem 1.23
The alphabet {1,2,...,m} is to be encoded by codewords with
letters taken from an alphabet of q < m letters. State Kraftâ€™s inequality for the word-
lengths s1,...,sm of a decipherable code. Suppose that a source emits letters from
the alphabet {1,2,...,m}, each letter occurring with known probability pi > 0. Let
S be the random codeword-length resulting from the letter-by-letter encoding of the
source output. It is desired to ï¬nd a decipherable code that minimises the expected
value of qS. Establish the lower bound E

qS
â‰¥

âˆ‘
1â‰¤iâ‰¤m
âˆšpi
2
, and characterise
when equality occurs.
Prove also that an optimal code for the above criterion must satisfy E

qS
<
q

âˆ‘
1â‰¤iâ‰¤m
âˆšpi
2
.
Hint: Use the Cauchyâ€“Schwarz inequality: for all positive xi,yi,
âˆ‘
1â‰¤iâ‰¤m
xiyi â‰¤

âˆ‘
1â‰¤iâ‰¤m
x2
i
1/2 
âˆ‘
1â‰¤iâ‰¤m
y2
i
1/2
,
with equality iff xi = cyi for all i.

1.6 Additional problems for Chapter 1
125
Solution By Cauchyâ€“Schwarz,
âˆ‘
1â‰¤iâ‰¤m
p1/2
i
=
âˆ‘
1â‰¤iâ‰¤m
p1/2
i
qsi/2qâˆ’si/2
â‰¤

âˆ‘
1â‰¤iâ‰¤m
piqsi
1/2 
âˆ‘
1â‰¤iâ‰¤m
qâˆ’si
1/2
â‰¤

âˆ‘
1â‰¤iâ‰¤m
piqsi
1/2
,
since, by Kraft,
âˆ‘
1â‰¤iâ‰¤m
qâˆ’si â‰¤1. Hence,
EqS = âˆ‘
1â‰¤iâ‰¤m
piqsi â‰¥

âˆ‘
1â‰¤iâ‰¤m
p1/2
i
2
.
Now take the probabilities pi to be
pi = (cqâˆ’xi)2, xi > 0,
where
âˆ‘
1â‰¤iâ‰¤m
qâˆ’xi = 1 (so,
âˆ‘
1â‰¤iâ‰¤m
p1/2
i
= c). Take si to be the smallest integer â‰¥xi.
Then
âˆ‘
1â‰¤iâ‰¤m
qâˆ’si â‰¤1 and, again by Kraft, there exists a decipherable coding with
the codeword-length si. For this code, qsiâˆ’1 < qxi = c

p1/2
i
, and hence
EqS = âˆ‘
1â‰¤iâ‰¤m
piqsi = q âˆ‘
1â‰¤iâ‰¤m
piqsiâˆ’1
< q âˆ‘
1â‰¤iâ‰¤m
piqxi = qc âˆ‘
1â‰¤iâ‰¤m
p1/2
i
= q

âˆ‘
1â‰¤iâ‰¤m
p1/2
i
2
.
Problem 1.24
A Bernoulli source of information of rate H is fed character-
by-character into a transmission line which may be live or dead. If the line is live
when a character is transmitted then that character is received faithfully; if the line
is dead then the receiver learnt only that it is indeed dead. In shifting between
its two states the line follows a Markov chain (DTMC) with constant transition
probabilities, independent of the text being transmitted.
Show that the information rate of the source constituted by the received signal
is HL + Ï€LHS where HS is the signal, HL is the information rate of the DTMC
governing the functioning of the line and Ï€L is the equilibrium probability that the
line is alive.
Solution The rate of a Bernoulli source emitting letter j = 1,2,... with probability
p j is H = âˆ’âˆ‘
j
p j log p j. The state of the line is a DTMC with a 2 Ã— 2 transition

126
Essentials of Information Theory
matrix
dead
live
1âˆ’Î±
Î±
Î²
1âˆ’Î²

and the equilibrium probabilities
1âˆ’Ï€L(dead) =
Î²
Î± +Î² , Ï€L(live) =
Î±
Î± +Î²
(assuming that Î± + Î² > 0). The received signal sequence follows a DTMC with
states 0 (dead), 1,2,... and transition probabilities
q00 = 1âˆ’Î±,
q0 j = Î± p j,
q j0 = Î²,
q jk = (1âˆ’Î²)pk
j,k â‰¥1.
This chain has a unique equilibrium distribution
Ï€RS(0) =
Î²
Î± +Î² , Ï€RS( j) =
Î±
Î± +Î² p j, j â‰¥1.
Then the information rate of the received signal equals
HRS = âˆ’âˆ‘
j,kâ‰¥0
Ï€RS( j)q jk logq jk
= âˆ’
Î²
Î± +Î²

(1âˆ’Î±)log(1âˆ’Î±)+ âˆ‘
jâ‰¥1
Î± p j log(Î± p j)

âˆ’
Î±
Î± +Î²

âˆ‘
jâ‰¥1
p j

Î² logÎ² +(1âˆ’Î²) âˆ‘
kâ‰¥1
pk log

(1âˆ’Î²)pk


= HL +
Î±
Î± +Î² HS.
Here HL is the entropy rate of the line state DTMC:
HL = âˆ’
Î²
Î± +Î²
	
(1âˆ’Î±)log(1âˆ’Î±)+Î± logÎ±

âˆ’
Î±
Î± +Î²
	
(1âˆ’Î²)log(1âˆ’Î²)+Î² logÎ²

,
and Ï€ = Î±/(Î± +Î²).
Problem 1.25
Consider a Bernoulli source in which the individual character
can take value i with probability pi (i = 1,...,m). Let ni be the number of times the
character value i appears in the sequence u(n) = u1 u2 ... un of given length n. Let
An be the smallest set of sequences u(n) which has total probability at least 1 âˆ’Îµ.
Show that each sequence in An satisï¬es the inequality
âˆ’âˆ‘ni log pi â‰¤nh+

nk/Îµ)1/2,

1.6 Additional problems for Chapter 1
127
where k is a constant independent of n or Îµ. State (without proof) the analogous
assertion for a Markov source.
Solution For a Bernoulli source with letters 1,...,m, the probability of a given
string u(n) = u1 u2 ... un is
P(U(n) = u(n)) = âˆ
1â‰¤iâ‰¤m
pni
i .
Set An consists of strings of maximal probabilities (selected in the decreasing
order), i.e. of maximal value of logP(U(n) = u(n)) =
âˆ‘
1â‰¤iâ‰¤m
ni log pi. Hence,
An =
'
u(n) : âˆ’âˆ‘
i
ni log pi â‰¤c
@
,
for some (real) c, to be determined. To determine c, we use that
P(An) â‰¥1âˆ’Îµ.
Hence, c is the value for which
P

u(n) : âˆ’âˆ‘
1â‰¤iâ‰¤m
ni log pi â‰¥c

< Îµ.
Now, for the random string U(n) =U1 ... Un, let Ni is the number of appearances
of value i. Then
âˆ’âˆ‘
1â‰¤iâ‰¤m
Ni log pi = âˆ‘
1â‰¤jâ‰¤n
Î¸ j, where Î¸ j = âˆ’log pi when Uj = i.
Since entries Uj are IID, so are random variables Î¸ j. Next,
EÎ¸ j = âˆ’âˆ‘
1â‰¤iâ‰¤m
pi log pi := h
and
Var Î¸ j = E(Î¸ j)2 âˆ’

EÎ¸ j
2 = âˆ‘
1â‰¤iâ‰¤m
pi(log pi)2 âˆ’

âˆ‘
1â‰¤iâ‰¤m
pi log pi
2
:= v.
Then
E

âˆ‘
1â‰¤jâ‰¤n
Î¸ j

= nh and Var

âˆ‘
1â‰¤jâ‰¤n
Î¸ j

= nv.
Recall that h = H is the information rate of the source.

128
Essentials of Information Theory
By Chebyshevâ€™s inequality, for all b > 0,
P
âˆ’âˆ‘
1â‰¤iâ‰¤m
Ni log pi âˆ’nh
 > b

â‰¤nv
b2 ,
and with b =
$
nk/Îµ, we obtain
P
âˆ’âˆ‘
1â‰¤iâ‰¤m
Ni log pi âˆ’nh
 >
?
nk
Îµ

â‰¤Îµ.
Therefore, for all u(n) âˆˆAn,
âˆ’âˆ‘
1â‰¤iâ‰¤m
ni log pi â‰¤nh+
?
nk
Îµ := c.
For an irreducible and aperiodic Markov source the assertion is similar, with
H = âˆ’âˆ‘
1â‰¤i, jâ‰¤m
Ï€ipij log pij,
and v â‰¥0 a constant given by v = limsup
nâ†’âˆ
1
n Var

âˆ‘
1â‰¤jâ‰¤n
Î¸ j

.
Problem 1.26
Demonstrate that an efï¬cient and decipherable noiseless coding
procedure leads to an entropy as a measure of attainable performance.
Words of length si (i = 1,...,n) in an alphabet Fa = {0,1,...,a âˆ’1} are to be
chosen to minimise expected word-length
n
âˆ‘
i=1
pisi subject not only to decipherabil-
ity but also to the condition that
n
âˆ‘
i=1
qisi should not exceed a prescribed bound,
where qi is a feasible alternative to the postulated probability distribution {pi}
of characters in the original alphabet. Determine bounds on the minimal value of
n
âˆ‘
i=1
pisi.
Solution If we disregard the condition that s1,...,sn are positive integers, the min-
imisation problem becomes
minimise
âˆ‘
i
sipi
subject to
si â‰¥0 and âˆ‘
i
aâˆ’si â‰¤1 (Kraft).
(1.6.24)
This can be solved by the Lagrange method, with the Lagrangian
L (s1,...,sn,Î») = âˆ‘
1â‰¤iâ‰¤n
sipi âˆ’Î»

1âˆ’âˆ‘
1â‰¤iâ‰¤n
aâˆ’si

.

1.6 Additional problems for Chapter 1
129
The solution of the relaxed problem is unique and given by
si = âˆ’loga pi, 1 â‰¤i â‰¤n.
(1.6.25)
The relaxed optimal value vrel,
vrel = âˆ’âˆ‘
1â‰¤iâ‰¤n
pi loga pi := h,
provides a lower bound for the optimal expected word-length âˆ‘
i
sâˆ—
i pi:
h â‰¤âˆ‘
i
sâˆ—
i pi.
Now consider the additional constraint
âˆ‘
1â‰¤iâ‰¤n
qisi â‰¤b.
(1.6.26)
The relaxed problem (1.6.24) complemented with (1.6.26) again can be solved by
the Lagrange method. Here, if
âˆ’âˆ‘
i
qi loga pi â‰¤b
then adding the new constraint does not affect the minimiser (1.6.24), i.e. the
optimal positive s1,...,sn are again given by (1.6.25), and the optimal value is h.
Otherwise, i.e. when âˆ’âˆ‘
i
qi loga pi > b, the new minimiser  s1,..., sn is still unique
(since the problem is still strong Lagrangian) and fulï¬ls both constraints
âˆ‘
i
aâˆ’ si = 1, âˆ‘
i
qi si = b.
In both cases, the optimal value  vrel for the new relaxed problem satisï¬es h â‰¤ vrel.
Finally, the solution  sâˆ—
1,..., sâˆ—
n to the integer-valued word-length problem
minimise
âˆ‘
i
sipi
subject to
si â‰¥1 integer
and âˆ‘
i
aâˆ’si â‰¤1, âˆ‘
i
qisi â‰¤b
(1.6.27)
will satisfy
h â‰¤ vrel â‰¤âˆ‘
i
 sâˆ—
i pi, âˆ‘
i
 sâˆ—
i qi â‰¤b.
Problem 1.27
Suppose a discrete Markov source {Xt} has transition probability
p jk = P(Xt+1 = k|Xt = j)

130
Essentials of Information Theory
with equilibrium distribution (Ï€j). Suppose the letter can be obliterated by noise
(in which case one observes only the event â€˜erasureâ€™) with probability Î² = 1âˆ’Î±,
independent of current or previous letter values or previous noise. Show that the
noise-corrupted source has information rate
âˆ’Î± logÎ± âˆ’Î² logÎ² âˆ’Î±2âˆ‘
j âˆ‘
k âˆ‘
sâ‰¥1
Ï€jÎ² sâˆ’1p(s)
jk log p(s)
jk ,
where p(s)
jk is the s-step transition probability of the original DTMC.
Solution Denote the corrupted source sequence { Xt}, with  Xt = âˆ—(a splodge) every
time there was an erasure. Correspondingly, a string  xn
1 from the corrupted source
is produced from a string xn
1 of the original Markov source by replacing the oblit-
erated digits with splodges. The probability pn ( x) = P

 Xn
1 =  xn
1

of such a string
is represented as
âˆ‘
xn
1 consistent with  xn
1
P(Xn
1 = xn
1)P( Xn
1 |Xn
1 = xn
1)
(1.6.28)
and is calculated as the product where the initial factor is
Î»x1Î± or âˆ‘
y
Î»yp(s)
yxsÎ² sâˆ’1Î±, where 1 < s â‰¤n, or 1,
depending on where the initial non-obliterated digit occurred in  xn
1 (if at all). The
subsequent factors contributing to (1.6.28) have a similar structure:
pxtâˆ’1xtÎ² or p(s)
xtâˆ’sxtÎ² sâˆ’1Î± or 1.
Consequently, the information âˆ’log pn ( xn
1) carried by string  xn
1 is calculated as
âˆ’logP(Xs1 = xs1)âˆ’(s1 âˆ’1)logÎ² âˆ’logÎ±
âˆ’log ps2âˆ’s1)
xs1xs2 âˆ’(s2 âˆ’s1 âˆ’1)logÎ² âˆ’logÎ± âˆ’Â·Â·Â·
âˆ’log p(sNâˆ’sNâˆ’1)
xsNâˆ’1xsN
âˆ’(sN âˆ’sNâˆ’1 âˆ’1)logÎ² âˆ’logÎ±
where 1 â‰¤s1 < Â·Â·Â· < sN â‰¤n are the consecutive times of appearance of non-
obliterated symbols in  xn
1.
Now take âˆ’1
n log pn

 Xn
1

, the information rate provided by the random string
 Xn
1 . Ignoring the initial bit, we can write
âˆ’1
n log pn

 Xn
1

= âˆ’N(Î²)
n
logÎ² âˆ’N(Î±)
n
logÎ± âˆ’âˆ‘
i
M(i, j;s)
n
log p(s)
ij .

1.6 Additional problems for Chapter 1
131
Here
N(Î±) = number of non-obliterated digits in  Xn
1 ,
N(Î²) = number of obliterated digits in  Xn
1 ,
M(i, j;s) = number of series of digits iâˆ—Â·Â·Â·âˆ—j in  Xn
1 of length s+1
As n â†’âˆ, we have the convergence of the limiting frequencies (the law of large
numbers applies):
N(Î±)
n
â†’Î±, N(Î²)
n
â†’Î², M(i, j;s)
n
â†’Î±Î² sâˆ’1Ï€ip(s)
ij Î±.
This yields
âˆ’1
n log pn

 Xn
1

â†’âˆ’Î± logÎ± âˆ’Î² logÎ² âˆ’Î±2 âˆ‘
i, j
Ï€i âˆ‘
sâ‰¥1
Î² sâˆ’1p(s)
ij log p(s)
ij ,
as required. [The convergence holds almost surely (a.s.) and in probability.]
According to the SCT, the limiting value gives the information rate of the corrupted
source.
Problem 1.28
A binary source emits digits 0 or 1 according to the rule
P(Xt = k|Xtâˆ’1 = j,Xtâˆ’2 = i) = qr,
where k, j,i and r take values 0 or 1, r = kâˆ’jâˆ’i mod 2, and q0+q1 = 1. Determine
the information rate of this source.
Also derive the information rate of a Bernoulli source emitting digits 0 and 1
with probabilities q0 and q1. Explain the relationship between these two results.
Solution Re-write the conditional probabilities in a detailed form:
P(Xt = 0|Xtâˆ’1 = j,Xtâˆ’2 = i) =
'
q0,
i = j,
q1,
i Ì¸= j,
P(Xt = 1|Xtâˆ’1 = j,Xtâˆ’2 = i) =
'
q1,
i = j,
q0,
i Ì¸= j.
The source is a second-order Markov chain on {0,1}, i.e. a DTMC with four states
{00,01,10,11}. The 4Ã—4 transition matrix is
00
01
10
11
â›
âœ
âœ
â
q0
q1
0
0
0
0
q1
q0
q1
q0
0
0
0
0
q0
q1
â
âŸ
âŸ
â 

132
Essentials of Information Theory
The equilibrium probabilities are uniform:
Ï€00 = Ï€01 = Ï€10 = Ï€11 = 1
4.
The information rate is calculated in a standard way:
H = âˆ’âˆ‘
Î±,Î²=0,1
Ï€Î±Î² âˆ‘âˆ‘
Î³=0,1
Ï€Î±Î² pÎ±Î²,Î²Î³ log pÎ±Î²,Î²Î³
and equals
1
4 âˆ‘
Î±,Î²=0,1
h(q0,q1) = âˆ’q0 logq0 âˆ’q1 logq1.
Problem 1.29
An input to a discrete memoryless channel has three letters 1, 2
and 3. The letter j is received as ( jâˆ’1) with probability p, as ( j+1) with probabil-
ity p and as j with probability 1âˆ’2p, the letters from the output alphabet ranging
from 0 to 4. Determine the form of the optimal input distribution, for general p,
as explicitly as possible. Compute the channel capacity in the three cases p = 0,
p = 1/3 and p = 1/2.
Solution The channel matrix is 3Ã—5:
1
2
3
â›
â
p
(1âˆ’2p)
p
0
0
0
p
(1âˆ’2p)
p
0
0
0
p
(1âˆ’2p)
p
â
â .
The rows are permutations of each other, so the capacity equals
C = maxPX [h(Y)âˆ’h(Y|X)]
= (maxPX h(Y))+
	
2plog p+(1âˆ’2p)log(1âˆ’2p)

,
with the maximisation over the input-letter distribution PX applied only to h(Y),
the entropy of the output-symbol.
Next,
h(Y) = âˆ’
âˆ‘
y=0,1,2,3,4
PY(y)logPY(y),
where
PY(0)
=
PX(1)p,
PY(1)
=
PX(1)(1âˆ’2p)+PX(2)p,
PY(2)
=
PX(1)p+PX(2)(1âˆ’2p)+PX(3)p,
PY(3)
=
PX(3)(1âˆ’2p)+PX(2)p,
PY(4)
=
PX(3)p.
â«
âª
âª
âª
âª
â¬
âª
âª
âª
âª
â­
(1.6.29)

1.6 Additional problems for Chapter 1
133
The symmetry in (1.6.29) suggests that h(Y) is maximised when PX(0) = PX(2) = q
and PX(1) = 1âˆ’2q. So:
max h(Y) = max [âˆ’2qplog(qp)âˆ’2
	
q(1âˆ’2p)+(1âˆ’2q)p

Ã—log
	
q(1âˆ’2p)+(1âˆ’2q)p

âˆ’
	
2qp+(1âˆ’2q)(1âˆ’2p)

log
	
2qp+(1âˆ’2q)(1âˆ’2p)

.
To ï¬nd the maximum, differentiate and solve:
d
dqh(Y) = âˆ’2plog(qp)âˆ’2pâˆ’2(1âˆ’4p)log
	
q(1âˆ’2p)+(1âˆ’2q)p

âˆ’2(1âˆ’4p)âˆ’(2pâˆ’2)log
	
2qp+(1âˆ’2q)(1âˆ’2p)

âˆ’(2pâˆ’2)
= 4pâˆ’2plog(qp)âˆ’2(1âˆ’4p)log
	
q(1âˆ’2p)+(1âˆ’2q)p

âˆ’2(1âˆ’4p)âˆ’2(pâˆ’1)log
	
2qp+(1âˆ’2q)(1âˆ’2p)

= 0.
For p = 0 we have a perfect error-free channel, of capacity log3 which is
achieved when PX(1) = PX(2) = PX(3) = 1/3 (i.e. q = 1/3), and PY(1) = PY(2) =
PY(3) = 1/3, PY(0) = PY(4) = 0.
For p = 1/3, the output probabilities are
pY(0) = pY(4) = q/3, pY(1) = (1âˆ’q)/3, pY(2) = 1/3,
and h(Y) simpliï¬es to
h(Y) = âˆ’2q
3 log q
3 âˆ’21âˆ’q
3
log 1âˆ’q
3
âˆ’1
3 log 1
3.
The derivative dh(Y)

dq = 0 becomes
âˆ’2
3 log q
3 âˆ’2
3 + 2
3 log 1âˆ’q
3
+ 2
3
and vanishes when q = 1/2, i.e.
PX(1) = PX(3) = 1/2, PX(2) = 0,
PY(0) = PY(1) = PY(3) = PY(4) = 1/6, PY(2) = 1/3.
Next, the conditional entropy
h(Y|X) = log3.
For the capacity this yields
C = âˆ’2
3 log 1
6 âˆ’1
3 log 1
3 âˆ’log3 = 2
3.
Finally, for p = 1/2, we have h(Y|X) = 1 and
PY(0) = PY(4) = q
2, PY(1) = PY(3) = 1âˆ’2q
2
, PY(2) = q.

134
Essentials of Information Theory
The output entropy is
h(Y) = âˆ’qlog q
2 âˆ’1âˆ’2q
2
log 1âˆ’2q
2
âˆ’qlogq
= qâˆ’2qlogqâˆ’1âˆ’2q
2
log 1âˆ’2q
2
and is maximised when q = 1/6, i.e.
PX(1) = PX(2) = 1
6, PX(2) = 2
3,
PY(0) = PY(4) = 1
12, PY(1) = PY(3) = 1
3, PY(2) = 1
6.
The capacity in this case equals
C = log3âˆ’1
2.
Problem 1.30
A memoryless discrete-time channel produces outputs Y from
non-negative integer-valued inputs X by
Y = ÎµX,
where Îµ is independent of X, P(Îµ = 1) = p, P(Îµ = 0) = 1 âˆ’p, and inputs are
restricted by the condition that EX â‰¤1.
By considering input distributions {ai, i = 0,1,...} of the form ai = cqi,
i = 1,2,..., or otherwise, derive the optimal input distribution and determine an
expression for the capacity of the channel.
Solution The channel matrix is
â›
âœ
âœ
âœ
â
1
0
0
...
0
1âˆ’p
p
0
...
0
1âˆ’p
0
p
...
0
...
...
...
...
...
â
âŸ
âŸ
âŸ
â .
For the input distribution with qi = P(X = i), we have that
P(Y = 0) = q0 +(1âˆ’p)(1âˆ’q0) = 1âˆ’p+ pq0,
P(Y = i) = pqi, i â‰¥1,
whence
h(Y) = âˆ’(1âˆ’p+ pq0)log(1âˆ’p+ pq0)âˆ’âˆ‘
iâ‰¥1
pqi log(pqi).

1.6 Additional problems for Chapter 1
135
With the conditional entropy being
h(Y|X) = âˆ’(1âˆ’q0)
	
(1âˆ’p)log(1âˆ’p)+ plog p

the mutual entropy equals
I(Y : X) = âˆ’(1âˆ’p+ pq0)log(1âˆ’p+ pq0)
âˆ’âˆ‘
iâ‰¥1
pqi log(pqi)+(1âˆ’q0)[(1âˆ’p)log(1âˆ’p)+ plog p].
We have to maximise I(Y : X) in q0,q1,..., subject to qi â‰¥0, âˆ‘i qi = 1, âˆ‘i iqi â‰¤1.
First, we ï¬x q0 and maximise the sum âˆ’âˆ‘iâ‰¥1 pqi log(pqi) in qi with i â‰¥1. By
Gibbs, for all non-negative a1,a2,... with âˆ‘iâ‰¥1 ai = 1âˆ’q0,
âˆ’âˆ‘
iâ‰¥1
qi logqi â‰¤âˆ’âˆ‘
iâ‰¥1
qi logai, with equality iff qi â‰¡ai.
For ai = cdi with âˆ‘i iai = 1, the RHS becomes
âˆ’(1âˆ’q0)logcâˆ’

logd
âˆ‘
iâ‰¥1
iai = âˆ’(1âˆ’q0)logcâˆ’logd.
From âˆ‘i icdi = 1, cd

(1âˆ’d) = 1âˆ’a0 and d = a0, c = (1âˆ’a0)2
a0.
Next, we maximise, in a0 âˆˆ[0,1], the function
f(a0) = âˆ’(1âˆ’p+ pa0)log(1âˆ’p+ pa0)âˆ’p(1âˆ’a0)log (1âˆ’a0)2
a0
âˆ’loga0 +(1âˆ’a0)[(1âˆ’p)log(1âˆ’p)+ plog p].
Requiring that
f â€²(a0) = 0
(1.6.30a)
and
f â€²â€²(a0) =
âˆ’p2
q+ pa0
âˆ’
2p
1âˆ’a0
âˆ’p
a0
â‰¤0,
(1.6.30b)
one can solve equation (1.6.30a) numerically. Denote its root where (1.6.30b) holds
by aâˆ’
0 . Then we obtain the following answer for the optimal input distribution:
ai =
'
aâˆ’
0 ,
i = 0,
(1âˆ’aâˆ’
0 )2(aâˆ’
0 )iâˆ’1,
i â‰¥1.
with the capacity C = f(aâˆ’
0 ).
Problem 1.31
The representation known as binary-coded decimal encodes 0 as
0000, 1 as 0001 and so on up to 9, coded as 1001, with other 4-digit binary strings
being discarded. Show that by encoding in blocks, one can get arbitrarily near the
lower bound on word-length per decimal digit.
Hint: Assume all integers to be equally probable.

136
Essentials of Information Theory
Solution The code in question is obviously decipherable (and even preï¬x-free, as
is any decipherable code with a ï¬xed codeword-length). The standard block-coding
procedure treats a string of n letters from the original source (Un) operating with
an alphabet A as a letter from A n. Given joint probabilities pn(u(n)
1 ) = P(U1 =
i1,...,Un = in), of the blocks in a typical message, we look at the binary entropy
h(n) = âˆ’âˆ‘
i1,...,in
P(U1 = i1,...,Un = in)logP(U1 = i1,...,Un = in).
Denote by S(n) the random codeword-length while encoding in blocks. The mini-
mal expected word-length per source letter is en := min 1
n ES(n). By Shannonâ€™s NC
theorem,
h(n)
n logq â‰¤en â‰¤
h(n)
nlogq + 1
n,
where q is the size of the original alphabet A . We see that, for large n, en âˆ¼
h(n)
n logq.
In the question, q = 10 and
h(n) = hn, where h = log10 (equiprobability).
Hence, the minimal expected word-length en can be made arbitrarily close to 1.
Problem 1.32
Let {Ut} be a discrete-time process with values ut and let
P(u(n)) be the probability that a string u(n) = u1 ... un is produced. Show that if
âˆ’logP(U(n))

n converges in probability to a constant Î³ then Î³ is the information
rate of the process.
Write down the formula for the information rate of an m-state DTMC and ï¬nd
the rate when the transition matrix has elements pjk where
p j j = p, pj j+1 = 1âˆ’p ( j = 1,...,mâˆ’1), pm1 = 1âˆ’p.
Relate this to the information rate of a two-state source with transition probabilities
p and 1âˆ’p.
Solution
The information rate of an m-state stationary DTMC with transition
matrix P = (pi j) and an equilibrium (invariant) distribution Ï€ = (Ï€i) equals
h = âˆ’âˆ‘
i, j
Ï€ipij log pij.
If matrix P is irreducible (i.e. has a unique communicating class) then this state-
ment holds for the chain with any initial distribution Î» (in this case the equilibrium
distribution is unique).

1.6 Additional problems for Chapter 1
137
In the example, the transition matrix is
â›
âœ
âœ
âœ
â
p
1âˆ’p
0
...
0
0
p
1âˆ’p
...
0
...
...
...
...
...
1âˆ’p
0
0...
p
â
âŸ
âŸ
âŸ
â .
The rows are permutations of each other, and each of them has entropy
âˆ’plog pâˆ’(1âˆ’p)log(1âˆ’p).
The equilibrium distribution is Ï€ = (1/m,...,1/m):
âˆ‘
1â‰¤iâ‰¤m
1
m pi j = 1
m(p+1âˆ’p) = 1
m,
and it is unique, as the chain has a unique communicating class. Therefore, the
information rate equals
h = 1
m âˆ‘
1â‰¤iâ‰¤m
	
âˆ’plog pâˆ’(1âˆ’p)log(1âˆ’p)

= âˆ’plog pâˆ’(1âˆ’p)log(1âˆ’p).
For m = 2 we obtain precisely the matrix

p
1âˆ’p
1âˆ’p
p

, so â€“ with the equilib-
rium distribution Ï€ = (1/2,1/2) â€“ the information rate is again h = Î·(p).
Problem 1.33
Deï¬ne a symmetric channel and ï¬nd its capacity.
A native American warrior sends smoke signals. The signal is coded in puffs of
smoke of different lengths: short, medium and long. One puff is sent per unit time.
Assume a puff is observed correctly with probability p, and with probability 1âˆ’p
(a) a short signal appears to be medium to the recipient, (b) a medium puff appears
to be long, and (c) a long puff appears to be short. What is the maximum rate at
which the warrior can transmit reliably, assuming the recipient knows the encoding
system he uses?
It would be more reasonable to assume that a short puff may disperse completely
rather than appear medium. In what way would this affect your derivation of a
formula for channel capacity?
Solution Suppose we use an input alphabet I , of m letters, to feed a memoryless
channel that produces symbols from an output alphabet J of size n (including
illegibles). The channel is described by its m Ã— n matrix where entry pij gives the

138
Essentials of Information Theory
probability that letter i âˆˆI is transformed to symbol j âˆˆJ . The rows of the
channel matrix form stochastic n-vectors (probability distributions over J ):
â›
âœ
âœ
âœ
âœ
âœ
âœ
â
p11
...
p1 j
...
p1n
...
...
...
...
...
pi1
...
pi j
...
pin
...
...
...
...
...
pm1
...
pmj
...
pmn
â
âŸ
âŸ
âŸ
âŸ
âŸ
âŸ
â 
.
The channel is called symmetric if its rows are permutations of each other (or, more
generally, have the same entropy E = h(pi1,..., pin), for all i âˆˆI ). The channel
is said to be double-symmetric if in addition its columns are permutations of each
other (or, more generally, have the same column sum Î£ =
âˆ‘
1â‰¤iâ‰¤m
pij, for all j âˆˆJ ).
For a memoryless channel, the capacity (the supremum of reliable transmission
rates) is given by
C = max
PX
I(X : Y).
Here, the maximum is taken over PX = (PX(i), i âˆˆI ), the input-letter probabil-
ity distribution, and I(X : Y) is the mutual entropy between the input and output
random letters X and Y tied through the channel matrix:
I(X : Y) = h(Y)âˆ’h(Y|X) = h(X)âˆ’h(X|Y).
For the symmetric channel, the conditional entropy
h(Y|X) = âˆ’âˆ‘
i, j
PX(i)pij log pij â‰¡h,
regardless of the input probabilities pX(i). Hence,
C =

max
PX
h(Y)

âˆ’h(Y|X),
and the maximisation needs only to be performed for the output symbol entropy
h(Y) = âˆ’âˆ‘
j
PY( j)logPY( j), where PY( j) = âˆ‘
i
PX(i)pij.
For a double-symmetric channel, the latter problem becomes straightforward: h(Y)
is maximised by the uniform input equidistribution Peq
X (i) = 1/m, as in this case PY
is also uniform:
PY( j) = 1
m âˆ‘
i
pi j = 1
m as it doesnâ€™t depend on j âˆˆJ .
Thus, for the double-symmetric channel:
C = lognâˆ’h(Y|X).

1.6 Additional problems for Chapter 1
139
In the example, the channel matrix is 3Ã—3,
1 âˆ¼short
2 âˆ¼medium
3 âˆ¼long
â›
â
p
1âˆ’p
0
0
p
1âˆ’p
1âˆ’p
0
p
â
â ,
and double-symmetric. This yields
C = log3+ plog p+(1âˆ’p)log(1âˆ’p).
In the modiï¬ed example, the matrix becomes 3Ã—4:
â›
â
p
0
0
1âˆ’p
0
p
1âˆ’p
0
1âˆ’p
0
p
0
â
â ;
column 4 corresponds to a â€˜no-signalâ€™ output state (a â€˜splodgeâ€™). The maximisation
problem loses its symmetry:
max

âˆ’
âˆ‘
j=1,2,3,4

âˆ‘
i=1,2,3
PX(i)pij

log

âˆ‘
i=1,2,3
PX(i)pij

âˆ’
âˆ‘
i=1,2,3
PX(i)
âˆ‘
j=1,2,3,4
pij log pij

subject to
PX(1),PX(2),PX(3) â‰¥0, and
âˆ‘
i=1,2,3
PX(i) = 1,
and requires a full-scale analysis.
Problem 1.34
The entropy power inequality (EPI, see (1.5.10)) states: for X and
Y independent d-dimensional random vectors,
22h(X+Y)/d â‰¥22h(X)/d +22h(Y)/d,
(1.6.31)
with equality iff X and Y are Gaussian with proportional covariance matrices.
Let X be a real-valued random variable with a PDF fX and ï¬nite differential
entropy h(X), and let function g : R â†’R have strictly positive derivative gâ€² every-
where. Prove that the random variable g(X) has differential entropy satisfying
h(g(X)) = h(X)+Elog2 gâ€²(X),
assuming that Elog2 gâ€²(X) is ï¬nite.
Let Y1 and Y2 be independent, strictly positive random variables with densities.
Show that the differential entropy of the product Y1Y2 satisï¬es
22h(Y1Y2) â‰¥Î±122h(Y1) +Î±222h(Y2),
where log2(Î±1) = 2Elog2Y2 and log2(Î±2) = 2Elog2Y1.

140
Essentials of Information Theory
Solution The CDF of the random variable g(X) satisï¬es
Fg(X)(y) = P(g(X) â‰¤y) = P(X â‰¤gâˆ’1(y)) = FX(gâˆ’1(y)),
i.e. the PDF fg(X)(y) = dFg(X)(y)
dy
takes the form
fg(X)(y) = fX

gâˆ’1(y)

gâˆ’1(y)
â€² = fX

gâˆ’1(y)

gâ€²
gâˆ’1(y)
 .
Then
h(g(X)) = âˆ’
0
fg(X)(y)log2 fg(X)(y)dy
=
0 fX

gâˆ’1(y)

gâ€²
gâˆ’1(y)
 log2
fX

gâˆ’1(y)

gâ€²
gâˆ’1(y)
 dy
= âˆ’
0 fX(x)
gâ€²(x)
	
log2 fX(x)âˆ’log2 gâ€²(x)

gâ€²(x)dx
= h(X)+E
	
log2 gâ€²(X)

.
(1.6.32)
When g(t) = et then
log2 gâ€²(t) = log2 et = t log2 e.
So, Yi = eXi = g(Xi) and (1.6.32) implies
h(eXi) = h(g(Xi)) = h(Xi)+EXi log2 e, i = 1,2,3,
with X3 = X1 +X2. Then
h(Y1Y2) = h(eX1+X2) = h(X1 +X2)+

EX1 +EX2

log2 e.
Hence, in the entropy-power inequality,
22h(Y1Y2)
= 22h(X1+X2)+2(EX1+EX2)log2 e
â‰¥

22h(X1) +22h(X2)
22(EX1+EX2)log2 e
= 22EX2 log2 e 
22[h(X1)+EX1 log2 e]
+22EX1 log2 e 
22[h(X2)+EX2 log2 e]
= Î±122h(Y1) +Î±222h(Y2).
Here Î±1 = 22EX2 log2 e, i.e.
log2 Î±1 = 2EX2 log2 e = 2ElnY2 log2 e = 2Elog2Y2,
and similarly, log2 Î±1 = 2Elog2Y1.

1.6 Additional problems for Chapter 1
141
Problem 1.35
In this problem we work with the following functions deï¬ned for
0 < a < b:
G(a,b) =
âˆš
ab, L(a,b) =
bâˆ’a
log(b/a), I(a,b) = 1
e(bb/aa)1/(bâˆ’a).
Check that
0 < a < G(a,b) < L(a,b) < I(a,b) < A(a,b) = a+b
2
< b.
(1.6.33)
Next, for 0 < a < b deï¬ne
Î›(a,b) = L(a,b)I(a,b)/G2(a,b).
Let p = (pi) and q = (qi) be the probability distributions of random variables X
and Y:
P(X = i) = pi > 0,P(Y = i) = qi,i = 1,...,r,âˆ‘pi = âˆ‘qi = 1.
Let m = min[qi/pi],M = max[qi/pi],Î¼ = min[pi],Î½ = max[pi]. Prove the following
bounds for the entropy h(X) and Kullbackâ€“Leibler divergence D(p||q) (cf. PSE II,
p. 419):
0 â‰¤logr âˆ’h(X) â‰¤logÎ›(Î¼,Î½).
(1.6.34)
0 â‰¤D(p||q) â‰¤logÎ›(m,M).
(1.6.35)
Solution The inequality (1.6.33) is straightforward and left as an exercise. For
a â‰¤xi â‰¤b, set A (p,x) = âˆ‘pixi, G (p,x) = âˆxpi
i . The following general inequality
holds:
1 â‰¤A (p,x)
G (p,x) â‰¤Î›(a,b).
(1.6.36)
It implies that
0 â‰¤log
âˆ‘pixi

âˆ’âˆ‘pi logxi â‰¤logÎ›(a,b).
Selecting xi = qi/pi we immediately obtain (1.6.35). Taking q to be uniform, we
obtain (1.6.34) from (1.6.35) since
Î›
 1
rÎ½ , 1
rÎ¼

= Î›
 1
Î½ , 1
Î¼

= Î›(Î¼,Î½).
Next, we sketch the proof of (1.6.36); see details in [144], [50]. Let f be a convex
function, p,q â‰¥0, p+q = 1. Then for xi âˆˆ[a,b], we have
0 â‰¤âˆ‘pi f(xi)âˆ’f
âˆ‘pixi

â‰¤max
p [p f(a)+q f(b)âˆ’f(pa+qb)].
(1.6.37)

142
Essentials of Information Theory
Applying (1.6.37) for a convex function f(x) = âˆ’logx we obtain after some cal-
culations that the maximum in (1.6.37) is achieved at p0 = (b âˆ’L(a,b))/(b âˆ’a),
with p0a+(1âˆ’p0)b = L(a,b), and
0 â‰¤log A (p,x)
G (q,x)) â‰¤log

bâˆ’a
log(b/a)

âˆ’log(ab)+ log(bb/aa)
bâˆ’a
âˆ’1
which is equivalent to (1.6.36). Finally, we establish (1.6.37). Write xi = Î»ia +
(1âˆ’Î»i)b for some Î»i âˆˆ[0,1]. Then by convexity
0 â‰¤âˆ‘pi f(xi)âˆ’f (âˆ‘pixi)
â‰¤âˆ‘pi(Î»i f(a)+(1âˆ’Î»i) f(b))âˆ’f (aâˆ‘piÎ»i +bâˆ‘pi(1âˆ’Î»i)).
Denoting âˆ‘piÎ»i = p and 1 âˆ’âˆ‘piÎ»i = q and maximising over p we obtain
(1.6.37).
Problem 1.36
Let f be a strictly positive probability density function (PDF)
on the line R, deï¬ne the Kullbackâ€“Leibler divergence D(g|| f) and prove that
D(g|| f) â‰¥0.
Next, assume that
0
ex f(x)dx < âˆand
0
|x|ex f(x)dx < âˆ. Prove that the mini-
mum of the expression
âˆ’
0
xg(x)dx+D(g|| f)
(1.6.38)
over the PDFs g with
0
|x|g(x)dx < âˆis attained at the unique PDF gâˆ—âˆex f(x)
and calculate this minimum.
Solution The Kullbackâ€“Leibler divergence D(g|| f) is deï¬ned by
D(g|| f) =
0
g(x)ln g(x)
f(x)dx, if
0
g(x)
ln g(x)
f(x)
dx < âˆ
and
D(g|| f) = âˆ, if
0
g(x)
ln g(x)
f(x)
dx = âˆ.
The bound D(g|| f) â‰¥0 is the Gibbs inequality.
Now take the PDF gâˆ—(x) = ex f(x)/Z where Z =
0
ez f(z)dz. Set W =
0
xex f(x)dx; then W/Z =
1 xgâˆ—(x)dx. Further, write:
D(gâˆ—|| f)
= 1
Z
0
ex f(x)ln ex
Z dx
= 1
Z
0
ex f(x)

xâˆ’lnZ

dx = 1
Z

W âˆ’Z lnZ

= W
Z âˆ’lnZ

1.6 Additional problems for Chapter 1
143
and obtain that
âˆ’
0
xgâˆ—(x)dx+D(gâˆ—|| f) = âˆ’lnZ.
This is the claimed minimum in the last part of the question.
Indeed, for any PDF g such that
0
|x|g(x)dx < âˆ, set q(x) = g(x)/ f(x) and write
D(g||gâˆ—)
=
0
g(x)ln g(x)
gâˆ—(x)dx =
0
q(x)ln
	
q(x)eâˆ’xZ

f(x)dx
= âˆ’
0
x f(x)q(x)dx+
0
f(x)q(x)lnq(x)dx+lnZ
= âˆ’
0
xg(x)dx+D(g|| f)+lnZ,
implying that
âˆ’
0
xg(x)dx+D(g|| f) = âˆ’
0
xgâˆ—(x)dx+D(gâˆ—|| f)+D(g||gâˆ—).
Since D(g||gâˆ—) > 0 unless g = gâˆ—, the claim follows.
Remark 1.6.1
The property of minimisation of (1.6.38) is far reaching and im-
portant in a number of disciplines, including statistical physics, ergodic theory and
ï¬nancial mathematics. We refer the reader to the paper [109] for further details.

2
Introduction to Coding Theory
2.1 Hamming spaces. Geometry of codes. Basic bounds
on the code size
For presentational purposes, it is advisable to concentrate at the ï¬rst reading of this
section on the binary case where the symbols sent through a channel are 0 and 1.
As we saw earlier, in the case of an MBSC with the row error-probability p âˆˆ
(0,1/2), the ML decoder looks for a codeword x(N)
â‹†
that has the maximum number
of digits coinciding with the received binary word y(N). In fact, if y(N) is received,
the ML decoder compares the probabilities
P

y(N)|x(N)
= pÎ´(x(N),y(N))(1âˆ’p)Nâˆ’Î´(x(N),y(N))
= (1âˆ’p)N

p
1âˆ’p
Î´(x(N),y(N))
for different binary codewords x(N). Here
Î´(x(N),y(N)) = the number of digits i with xi Ì¸= yi
(2.1.1a)
is the so-called Hamming distance between words x(N) = x1 ...xN and y(N) =
y1 ...yN. Since the ï¬rst factor (1âˆ’p)N does not depend on x(N), the decoder seeks
to maximise the second factor, that is to minimise Î´(x(N),y(N)) (as 0 < p/(1âˆ’p) <
1 for p âˆˆ(0,1/2)).
The deï¬nition (2.1.1a) of Hamming distance can be extended to q-ary strings.
The space of q-ary words HN,q = {0,1,...,qâˆ’1}Ã—N (the Nth Cartesian power of
set Jq = {0,1,...,qâˆ’1}) with distance (2.1.1a) is called the q-ary Hamming space
of length N. It contains qN elements. In the binary case, HN,2 = {0,1}Ã—N.
144

2.1 Hamming spaces. Geometry of codes. Basic bounds on the code size
145
N = 1
N = 2
N = 4
N = 3
Figure 2.1
An important part is played by the distance Î´(x(N),0(N)) between words x(N) =
x1 ...xN and 0(N) = 0...0; it is called the weight of word x(N) and denoted by
w

x(N)
:
w

x(N)
= the number of digits i with xi Ì¸= 0.
(2.1.1b)
Lemma 2.1.1
The quantity Î´(x(N),y(N)) deï¬nes a distance on HN,q. That is:
(i) 0 â‰¤Î´(x(N),y(N)) â‰¤N and Î´(x(N),y(N)) = 0 iff x(N) = y(N).
(ii) Î´(x(N),y(N)) = Î´(y(N),x(N)).
(iii) Î´(x(N),z(N)) â‰¤Î´(x(N),y(N))+Î´(y(N),z(N)) (the triangle inequality).
Proof
The proof of (i) and (ii) is obvious. To check (iii), observe that any digit i
with zi Ì¸= xi has either yi Ì¸= xi and then counted in Î´(x(N),y(N)) or zi Ì¸= yi and then
counted in Î´(y(N),z(N)).
Geometrically, the binary Hamming space HN,2 may be identiï¬ed with the col-
lection of the vertices of a unit cube in N dimensions. The Hamming distance
equals the lowest number of edges we have to pass from one vertex to another. It is
a good practice to plot pictures for relatively low values of N: see Figure 2.1.
An important role is played below by geometric and algebraic properties of the
Hamming space. Namely, as in any metric space, we can consider a ball of a given
radius R around a given word x(N):
BN,q(x(N),R) = {y(N) âˆˆHN,q : Î´(x(N),y(N)) â‰¤R}.
(2.1.2)
An important (and hard) problem is to calculate the maximal number of disjoint
balls of a given radius which can be packed in a given Hamming space.
Observe that words admit an operation of addition mod q:
x(N) +y(N) = (x1 +y1) mod q ... (xN +yN) mod q.
(2.1.3a)

146
Introduction to Coding Theory
This makes the Hamming space HN,q a commutative group, with the zero code-
word 0(N) = 0...0 playing the role of the zero of the group. (Words also may be
multiplied which generates a powerful apparatus; see below.)
For q = 2, we have a two-point code alphabet {0,1} that is actually a two-
point ï¬eld, F2, with the following arithmetic: 0 + 0 = 1 + 1 = 0 Â· 1 = 1 Â· 0 = 0,
0 + 1 = 1 + 0 = 1 Â· 1 = 1. (Recall, a ï¬eld is a set equipped with two commutative
operations: addition and multiplication, satisfying standard axioms of associativity
and distributivity.) Thus, each point in the binary Hamming space HN,2 is opposite
to itself: x(N) +xâ€²(N) = 0(N) iff x(N) = xâ€²(N). In fact, HN,2 is a linear space over the
coefï¬cient ï¬eld F2, with 1Â·x(N) = x(N), 0Â·x(N) = 0(N).
Henceforth, all additions of q-ary words are understood digit-wise and mod q.
Lemma 2.1.2
The Hamming distance on HN,q is invariant under group transla-
tions:
Î´(x(N) +z(N),y(N) +z(N)) = Î´(x(N),y(N)).
(2.1.3b)
Proof
For all i = 1,...,N and xi,yi,zi âˆˆ{0,1,...,qâˆ’1}, the digits xi +zi mod q
and yi +zi mod q are in the same relation (= or Ì¸=) as digits xi and yi.
A code is identiï¬ed with a set of codewords XN âŠ‚HN,q; this means that we dis-
regard any particular allocation of codewords (which ï¬ts the assumption that the
source messages are equidistributed). An assumption is that the code is known to
both the sender and the receiver. Shannonâ€™s coding theorems guarantee that, under
certain conditions, there exist asymptotically good codes attaining the limits im-
posed by the information rate of a source and the capacity of a channel. Moreover,
Shannonâ€™s SCT shows that almost all codes are asymptotically good. However, in
a practical situation, these facts are of a limited use: one wants to have a good code
in an explicit form. Besides, it is desirable to have a code that leads to fast encoding
and decoding and maximises the rate of the information transmission.
So, assume that the source emits binary strings u(n) = u1 ... un, ui = 0,1. To
obtain the overall error-probability vanishing as n â†’âˆ, we have to encode words
u(n) by longer codewords x(N) âˆˆHN,2 where N âˆ¼Râˆ’1n and 0 < R < 1. Word x(N)
is then sent to the channel and is transformed into another word, y(N) âˆˆHN,2. It
is convenient to represent the error occurred by the difference of the two words:
e(N) = y(N) âˆ’x(N) = x(N) + y(N), or equivalently, write y(N) = x(N) + e(N), in the
sense of (2.1.3a). Thus, the more digits 1 the error word e(N) has, the more sym-
bols are distorted by the channel. The ML decoder then produces a â€˜guessedâ€™ code-
word x(N)
â‹†
that may or may not coincide with x(N), and then reconstructs a string
u(n)
â‹†. In the case of a one-to-one encoding rule, the last procedure is (theoretically)
straightforward: we simply invert the map u(n) â†’x(N). Intuitively, a code is â€˜goodâ€™

2.1 Hamming spaces. Geometry of codes. Basic bounds on the code size
147
if it allows the receiver to â€˜correctâ€™ the error string e(N), at least when word e(N)
does not contain â€˜too manyâ€™ non-zero digits.
Going back to an MBSC with the row probability of the error p < 1/2: the ML
decoder selects a codeword x(N)
â‹†
that leads to a word e(N) with a minimal number
of the unit digits. In geometric terms:
x(N)
â‹†
âˆˆXN is the codeword closest to y(N)
in the Hamming distance Î´.
(2.1.4)
The same rule can be applied in the q-ary case: we look for the codeword closest
to the received string. A drawback of this rule is that if several codewords have the
same minimal distance from a received word we are â€˜stuckâ€™. In this case we either
choose one of these codewords arbitrarily (possibly randomly or in connection with
the messageâ€™s content; this is related to the so-called list decoding), or, when a high
quality of transmission is required, refuse to decode the received word and demand
a re-transmission.
Deï¬nition 2.1.3
We call N the length of a binary code XN, M := â™¯XN the size
and Ï := log2 M
N
the information rate. A code XN is said to be D-error detecting
if making up to D changes in any codeword does not produce another codeword,
and E-error correcting if making up to E changes in any codeword x(N) produces
a word which is still (strictly) closer to x(N) than to any other codeword (that is,
x(N) is correctly guessed from a distorted word under the rule (2.1.4)). A code has
minimal distance (or brieï¬‚y distance) d if
d = min

Î´(x(N),xâ€²(N)) : x(N),xâ€²(N) âˆˆXN, x(N) Ì¸= xâ€²(N)
.
(2.1.5)
The minimal distance and the information rate of a code XN will be sometimes
denoted by d(XN) and Ï(XN), respectively.
This deï¬nition can be repeated almost verbatim for the general case of a
q-ary code XN âŠ‚HN,q, with information rate Ï = logq M
N
. Namely, a code XN
is called E-error correcting if, for all r = 1,...,E, x(N) âˆˆXN and y(N) âˆˆHN,q
with Î´(x(N),y(N)) = r, the distance Î´(y(N),xâ€²(N)) > r for all xâ€²(N) âˆˆXN such that
xâ€²(N) Ì¸= x(N). In words, it means that making up to E errors in a codeword pro-
duces a word that is still closer to it than to any other codeword. Geometrically,
this property means that the balls of radius E about the codewords do not intersect:
BN,q(x(N),E)âˆ©BN,q(xâ€²(N),E) = /0
for all distinct x(N),xâ€²(N) âˆˆXN.
Next, a code XN is called D-error detecting if the ball of radius D about a codeword
does not contain another codeword. Equivalently, the intersection BN,q(x(N),D)âˆ©
XN is reduced to a single point x(N).

148
Introduction to Coding Theory
A code of length N, size M and minimal distance d is called an [N,M,d] code.
Speaking of an [N,M] or [N,d] code, we mean any code of length N and size M or
minimal distance d.
To make sure we understand this deï¬nition, let us prove the aforementioned
equivalence in the deï¬nition of an E-error correcting code. First, assume that the
balls of radius E are disjoint. Then, making up to E changes in a codeword pro-
duces a word that is still in the corresponding ball, and hence is further apart from
any other codeword. Conversely, let our code have the property that changing up
to E digits in a codeword does not produce a word which lies at the same distance
from or closer to another codeword. Then any word obtained by changing precisely
E digits in a codeword cannot fall in any ball of radius E but in the one about the
original codeword. If we make fewer changes we again do not fall in any other ball,
for if we do, then moving towards the second centre will produce, sooner or later,
a word that is at distance E from the original codeword and at distance < E from
the second one, which is impossible.
2
For a D-error detecting code, the distance d â‰¥D + 1. Furthermore, a code of
distance d detects d âˆ’1 errors, and it corrects
8
(d âˆ’1)/2
9
errors.
Remark 2.1.4
Formally, Deï¬nition 2.1.3 means that a code detects at least D
and corrects at least E errors, and some authors make a point of this fact, specify-
ing D and E as maximal values with the above properties. We followed an original
tradition where the detection and correction abilities of codes are deï¬ned in terms
of inequalities rather than equalities, although in a number of forthcoming con-
structions and examples the claim that a code detects D and/or corrects E errors
means that D and/or E and no more. See, for instance, Deï¬nition 2.1.7.
Deï¬nition 2.1.5
In Section 2.3 we systematically study so-called linear codes.
The linear structure is established in space HN,q when the alphabet size q is of
the form ps where p is a prime and s a positive integer; in this case the alphabet
{0,1,...,q âˆ’1} can be made a ï¬eld, Fq, by introducing two suitable operations:
addition and multiplication. See Section 3.1. When s = 1, i.e. q is a prime number,
then both operations can be understood as standard ones, modulo q. When Fq is a
ï¬eld with addition + and multiplication Â· , set HN,q = FÃ—N
q
becomes a linear space
over Fq, with component-wise addition and multiplication by â€˜scalarsâ€™ generated
by the corresponding operations in Fq. Namely, for x(N) = x1 ...xN, y(N) = y1 ...yN
and Î³ âˆˆFq,
x(N) +y(N) = (x1 +y1)...(xN +yN), Î³ Â·x(N) = (Î³ Â·x1)...(Î³ Â·xN).
(2.1.6a)
With q = ps, a q-ary [N,M,d] code XN is called linear if it is a linear subspace of
HN,q. That is, XN has the property that if x(N),y(N) âˆˆXN then x(N) + y(N) âˆˆXN

2.1 Hamming spaces. Geometry of codes. Basic bounds on the code size
149
and Î³ Â· x(N) âˆˆXN for all Î³ âˆˆFq. For a linear code X , the size M is given by
M = qk where k may take values 1,...,N and gives the dimension of the code, i.e.
the maximal number of linearly independent codewords. Accordingly, one writes
k =dim X . As in the usual geometry, if k =dim X then in X there exists a basis
of size k, i.e. a linearly independent collection of codewords x(1),...,x(k) such that
any codeword x âˆˆX can be (uniquely) written as a linear combination
âˆ‘
1â‰¤jâ‰¤k
a jx( j),
where a j âˆˆFq. [In fact, if k =dim X then any linearly independent collection of k
codewords is a basis in X .] In the linear case, we speak of [N,k,d] or [N,k] codes.
As follows from the deï¬nition, a linear [N,k,d] code XN always contains the
zero string 0(N) = 0...0. Furthermore, owing to property (2.1.3b), the minimal
distance d(XN) in a linear code X equals the minimal weight w

x(N)
of a non-0
codeword x(N) âˆˆXN. See (2.1.1b).
Finally, we deï¬ne the so-called wedge-product of codewords x and y as a word
w = xâˆ§y with components
wi = min[xi,yi],
i = 1,...,N.
(2.1.6b)
A number of properties of linear codes can be mentioned already in this section,
although some details of proofs will be postponed.
A simple example of a linear code is a repetition code RN âŠ‚HN,q, of the form
RN =
(
x(N) = x...x : x = 0,1,...,qâˆ’1
)
detects N âˆ’1 and corrects
AN âˆ’1
2
B
errors. A linear parity-check code
PN =
(
x(N) = x1 ...xN : x1 +Â·Â·Â·+xN = 0
)
detects a single error only, but does not correct it.
Observe that the â€˜volumeâ€™ of the ball in the Hamming space HN,q centred at z(N)
is
vN,q(R) = â™¯BN,q(z(N),R) = âˆ‘
0â‰¤kâ‰¤R
N
k

(qâˆ’1)k ;
(2.1.7)
it does not depend on the choice of the centre z(N) âˆˆHN,q.
It is interesting to consider large values of N (theoretically, N â†’âˆ), and analyse
parameters of the code XN such as the information rate Ï(X ) = logâ™¯X
N
and
the distance per digit Â¯d(X ) = d(X )
N
. Our aim is to focus on â€˜goodâ€™ codes, with
many codewords (to increase the information rate) and large distances (to increase

150
Introduction to Coding Theory
the detecting and correcting abilities). From this point of view, it is important to
understand basic bounds for codes.
Upper bounds are usually written for Mâˆ—
q(N,d), the largest size of a q-ary code
of length N and distance d. We begin with elementary facts: Mâˆ—
q(N,1) = qN,
Mâˆ—
q(N,N) = q, Mâˆ—
q(N,d) â‰¤qMâˆ—
q(N âˆ’1,d) and â€“ in the binary case â€“ Mâˆ—
2(N,2s) =
Mâˆ—
2(N âˆ’1,2sâˆ’1) (easy exercises).
Indeed, the number of the codewords cannot be too high if we want to keep
good an error-detecting and error-correcting ability. There are various bounds for
parameters of codes; the simplest bound was discovered by Hamming in the late
1940s.
Theorem 2.1.6
(The Hamming bound)
(i) If a q-ary code XN corrects E errors then its size M = â™¯XN obeys
M â‰¤qN
vN,q(E).
(2.1.8a)
For a linear [N,k] code this can be written in the form
N âˆ’k â‰¥logq

vN,q(E)

.
(ii) Accordingly, with E =
8
(d âˆ’1)/2
9
,
Mâˆ—
q(N,d) â‰¤qN
vN,q(E).
(2.1.8b)
Proof
(i) The E-balls about the codewords x(N) âˆˆXN must be disjoint. Hence,
the total number of points covered equals the product vN,q(E)M which should not
exceed qN, the cardinality of the Hamming space HN,q.
(ii) Likewise, if XN is an [N,M,d] code then, as was noted above, for E =
8
(d âˆ’1)/2
9
, the balls BN,q(x(N),E), x(N) âˆˆXN, do not intersect. The volume
â™¯BN,q(x(N),E) is given by
vN,q(E) = âˆ‘
0â‰¤kâ‰¤E
N
k

(qâˆ’1)k,
and the union of balls

x(N)âˆˆXN
BN,q(x(N),E)
must lie in HN,q, again with cardinality â™¯HN,q = qN.
We see that the problem of ï¬nding good codes becomes a geometric problem,
because a â€˜goodâ€™ code XN correcting E errors must give a â€˜close-packingâ€™ of the
Hamming space by balls of radius E. A code XN that gives a â€˜trueâ€™ close-packing

2.1 Hamming spaces. Geometry of codes. Basic bounds on the code size
151
partition has an additional advantage: the code not only corrects errors, but never
leads to a refusal of decoding. More precisely:
Deï¬nition 2.1.7
An E-error correcting code XN of size â™¯XN = M is called
perfect when the equality is achieved in the Hamming bound:
M = qN
vN,q(E).
If a code XN is perfect, every word y(N) âˆˆHN,q belongs to a (unique) ball
BE(x(N)). That is, we are always able to decode y(N) by a codeword: this leads
to the correct answer if the number of errors is â‰¤E, and to a wrong answer if it is
> E. But we never get â€˜stuckâ€™ in the case of decoding.
The problem of ï¬nding perfect binary codes was solved about 20 years ago.
These codes exist only for
(a) E = 1: here N = 2l âˆ’1, M = 22lâˆ’1âˆ’l, and these codes correspond to the so-called
Hamming codes;
(b) E = 3: here N = 23, M = 212; they correspond to the so called (binary) Golay
code.
Both the Hamming and Golay codes are discussed below. The Golay code is
used (together with some modiï¬cations) in the US space programme: already in
the 1970s the quality of photographs encoded by this code and transmitted from
Mars and Venus was so excellent that it did not require any improving procedure.
In the former Soviet Union space vessels (and early American ones) other codes
were also used (and we also discuss them later): they generally produced lower-
quality photographs, and further manipulations were required, based on statistics
of the pictorial images.
If we consider non-binary codes then there exists one more perfect code, for
three symbols (also named after Golay).
We will now describe a number of straightforward constructions producing new
codes from existing ones.
Example 2.1.8
Constructions of new codes include:
(i) Extension: You add a digit xN+1 to each codeword x(N) = x1 ...xN from
code XN, following an agreed rule. Viz., the so-called parity-check exten-
sion requires that xN+1 +
âˆ‘
1â‰¤jâ‰¤N
xj = 0 in the alphabet ï¬eld Fq. Clearly, the
extended code, X +
N+1, has the same size as the original code XN, and the
distance d(X +
N+1) is equal to either d(XN) or d(XN)+1.

152
Introduction to Coding Theory
(ii) Truncation: Remove a digit from the codewords x âˆˆX (= XN). The result-
ing code, X âˆ’
Nâˆ’1, has length N âˆ’1 and, if the distance d(XN) â‰¥2, the same
size as XN, while d(X âˆ’
Nâˆ’1) â‰¥d(XN)âˆ’1, provided that d(XN) â‰¥2.
(iii) Purge: Simply delete some codewords x âˆˆXN. For example, in the binary
case removing all codewords with an odd number of non-zero digits from a
linear code leads to a linear subcode; in this case if the distance of the original
code was odd then the purged code will have a strictly larger distance.
(iv) Expansion: Opposite to purging. Say, let us add the complement of each
codeword to a binary code XN, i.e. the N-word where the 1s are replaced by
the 0s and vice versa. Denoting the expanded code by X N one can check
that d(X N) = min[d(XN),N âˆ’d(XN)] where
d(XN) = max[Î´(x(N),xâ€²(N)) : x(N),xâ€²(N) âˆˆXN].
(v) Shortening: Take all codewords x(N) âˆˆXN with the ith digit 0, say, and
delete this digit (shortening on xi = 0). In this way the original binary lin-
ear [N,M,d] code XN is reduced to a binary linear code X sh,0
Nâˆ’1(i) of length
N âˆ’1, whose size can be M/2 or M and distance â‰¥d or, in a trivial case, 0.
(vi) Repetition: Repeat each codeword x(= x(N)) âˆˆXN a ï¬xed number of times,
say m, producing a concatenated (Nm)-word xx...x. The result is a code
X re
Nm, of length Nm and distance d

X re
Nm

= md(XN).
(vii) Direct sum: Given two codes XN and X â€²
Nâ€², form a code X + X â€² =
{xxâ€² : x âˆˆX ,xâ€² âˆˆX â€²}. Both the repetition and direct-sum constructions
are not very effective and neither is particularly popular in coding (though
we will return to these constructions in examples and problems). A more
effective construction is
(viii) The bar-product (x|x+xâ€²): For the [N,M,d] and [N,Mâ€²,dâ€²] codes XN and
X â€²
N deï¬ne a code XN|X â€²
N of length 2N as the collection
(
x(x+xâ€²) : x(= x(N)) âˆˆXN,xâ€²(= xâ€²(N)) âˆˆX â€²
N
)
.
That is, each codeword in X |X â€² is a concatenation of the codeword from
XN and its sum with a codeword from X â€²
N (formally, neither of XN, X â€²
N in
this construction is supposed to be linear). The resulting code is denoted by
XN|X â€²
N; it has size
â™¯

XN|X â€²
N

=

â™¯XN

â™¯X â€²
N

.
A useful exercise is to check that the distance
d

XN|X â€²
N

= min
	
2d(XN),d(X â€²N)

.

2.1 Hamming spaces. Geometry of codes. Basic bounds on the code size
153
(ix) The dual code. The concept of duality is based on the inner dot-product in
space HN,q (with q = ps): for x = x1 ...xN and y = y1 ...yN,
C
x(N) Â·y(N)D
= x1 Â·y1 +Â·Â·Â·+xN Â·yN
which yields a value from ï¬eld Fq. For a linear [N,k] code XN its dual, X âŠ¥
N ,
is a linear [N,N âˆ’k] code deï¬ned by
X âŠ¥
N =
(
y(N) âˆˆHN,q :
C
x(N) Â·y(N)D
= 0
for all x âˆˆXN
)
.
(2.1.9)
Clearly, (X âŠ¥
N )âŠ¥= XN. Also dimXN +dimX âŠ¥
N = N. A code is called self-
dual if XN = X âŠ¥
N .
Worked Example 2.1.9
(a) Prove that if the distance d of an [N,M,d] code XN is an odd number then the
code may be extended to an [N +1,M] code X + with distance d +1.
(b) Show an E-error correcting code XN can be extended to a code X + that de-
tects 2E +1 errors.
(c) Show that the distance of a perfect binary code is an odd number.
Solution (a) By adding the digit xN+1 to the codewords x = x1 ...xN of an [N,M]
code XN so that xN+1 =
âˆ‘
1â‰¤jâ‰¤N
xj, we obtain an [N +1,M] code X +. If the distance
d of XN was odd, the distance of X + is d+1. In fact, if a pair of codewords, x, xâ€² âˆˆ
X , had Î´(x,xâ€²) > d, then the extended codewords, x+ and xâ€²
+, have Î´(x+,xâ€²
+) â‰¥
Î´(x,xâ€²) > d. Otherwise, i.e. if Î´(x,xâ€²) = d, the distance increases: Î´(x+,xâ€²
+) =
d +1.
(b) The distance d of an E-error correcting code is strictly greater than 2E. Hence,
the above extension gives a code with distance strictly greater than 2E +1.
(c) For a perfect E-error correcting code the distance is at most 2E + 1 and hence
equals 2E +1.
Worked Example 2.1.10
Show that there is no perfect 2-error correcting code
of length 90 and size 278 over F2.
Solution We might be interested in the existence of a perfect 2-error correcting
binary code of length N = 90 and size M = 278 because
v90,2(2) = 1+90+ 90Â·89
2
= 4096 = 212
and
M Ã—v90,2(2) = 278 Â·212 = 290 = 2N.

154
Introduction to Coding Theory
However, such a code does not exist. Assume that it exists, and, the zero word
0 = 0...0 is a codeword. The code must have d = 5. Consider the 88 words with
three non-zero digits, with 1 in the ï¬rst two places:
1110...00,
1101...00,
... ,
110...01.
(2.1.10)
Each of these words should be at distance â‰¤2 from a unique codeword. Say, the
codeword for 1110...00 must contain 5 non-zero digits. Assume that it is
111110...00.
This codeword is at distance 2 from two other subsequent words,
11010...00
and
11001...00.
Continuing with this construction, we see that any word from list (2.1.10) is â€˜at-
tractedâ€™ to a codeword with 5 non-zero digits, along with two other words from
(2.1.10). But 88 is not divisible by 3.
Let us continue with bounds on codes.
Theorem 2.1.11
(The Gilbertâ€“Varshamov (GV) bound) For any q â‰¥2 and d â‰¥2,
there exists a q-ary [N,M,d] code XN such that
M = â™¯XN â‰¥qN
vN,q(d âˆ’1) .
(2.1.11)
Proof
Consider a code of maximal size among the codes of minimal distance
d and length N. Then any word y(N) âˆˆHN,q must be distant â‰¤d âˆ’1 from some
codeword: otherwise we can add y(N) to the code without changing the minimal
distance. Hence, the balls of radius d âˆ’1 about the codewords cover the whole
Hamming space HN,q. That is, for the code of maximal size, X max
N
,

â™¯X max
N

vN,q(d âˆ’1) â‰¥qN.
As was listed before, there are ways of producing one code from another (or from
a collection of codes). Let us apply truncation and drop the last digit xN in each
codeword x(N) from an original code XN. If code XN had the minimal distance
d > 1 then the new code, X âˆ’
Nâˆ’1, has the minimal distance â‰¥d âˆ’1 and the same
size as XN. The truncation procedure leads to the following bound.
Theorem 2.1.12
(The Singleton bound) Any q-ary code XN with minimal dis-
tance d has
M = â™¯XN â‰¤Mâˆ—
q(N,d) â‰¤qNâˆ’d+1.
(2.1.12)

2.1 Hamming spaces. Geometry of codes. Basic bounds on the code size
155
Proof
As before, perform a truncation on an [N,M,d] code XN: drop the last
digit from each codeword xâˆˆXN. The new code is [N,M,dâˆ’] where dâˆ’â‰¥d âˆ’1.
Repeating this procedure d âˆ’1 times gives an (N âˆ’d + 1) code of the same
size M and distance â‰¥1. This code must ï¬t in Hamming space HNâˆ’d+1,q with
â™¯HNâˆ’d+1,q = qNâˆ’d+1; hence the result.
As with the Hamming bound, the case of equality in the Singleton bound at-
tracted a special interest:
Deï¬nition 2.1.13
A q-ary linear [N,k,d] code is called maximum distance sepa-
rating (MDS) if it gives equality in the Singleton bound:
d = N âˆ’k +1.
(2.1.13)
We will see below that, similarly to perfect codes, the family of the MDS codes
is rather â€˜thinâ€™.
Corollary 2.1.14
If Mâˆ—
q(N,d) is the maximal size of a code XN with minimal
distance d then
qN
vN,q(d âˆ’1) â‰¤Mâˆ—
q(N,d) â‰¤min

qN
vN,q

âŒŠ(d âˆ’1)/2âŒ‹
 ,qNâˆ’d+1

.
(2.1.14)
From now on we will omit indices N and (N) whenever it does not lead to
confusion. The upper bound in (2.1.14) becomes too rough when d âˆ¼N/2. Say, in
the case of binary [N,M,d]-code with N = 10 and d = 5, expression (2.1.14) gives
the upper bound Mâˆ—
2(10,5) â‰¤18, whereas in fact there is no code with M â‰¥13, but
there exists a code with M = 12. The codewords of the latter are as follows:
0000000000, 1111100000, 1001011010, 0100110110,
1100001101, 0011010101, 0010011011, 1110010011,
1001100111, 1010111100, 0111001110, 0101111001.
The lower bound gives in this case the value 2 (as 210/v10,2(4) = 2.6585) and is
also far from being satisfactory. (Some better bounds will be obtained below.)
Theorem 2.1.15
(The Plotkin bound) For a binary code X of length N and
distance d with N < 2d, the size M obeys
M = â™¯X â‰¤2
A
d
2d âˆ’N
B
.
(2.1.15)

156
Introduction to Coding Theory
Proof
The minimal distance cannot exceed the average distance, i.e.
M(M âˆ’1)d â‰¤âˆ‘
xâˆˆX âˆ‘
xâ€²âˆˆX
Î´(x,xâ€²).
On the other hand, write code X as an M Ã— N matrix with rows as codewords.
Suppose that column i of the matrix contains si zeros and M âˆ’si ones. Then
âˆ‘
xâˆˆX âˆ‘
xâ€²âˆˆX
Î´(x,xâ€²) â‰¤2 âˆ‘
1â‰¤iâ‰¤N
si(M âˆ’si).
(2.1.16)
If M is even, the RHS of (2.1.16) is maximised when si = M/2 which yields
M(M âˆ’1)d â‰¤1
2NM2, or M â‰¤
2d
2d âˆ’N .
As M is even, this implies
M â‰¤2
A
d
2d âˆ’N
B
.
If M is odd, the RHS of (2.1.16) is â‰¤N(M2 âˆ’1)/2 which yields
M â‰¤
N
2d âˆ’N =
2d
2d âˆ’N âˆ’1.
This implies in turn that
M â‰¤
A
2d
2d âˆ’N
B
âˆ’1 â‰¤2
A
d
2d âˆ’N
B
,
because, for all x > 0, âŒŠ2xâŒ‹â‰¤2âŒŠxâŒ‹+1.
Theorem 2.1.16
Let Mâˆ—
2(N,d) be the maximal size of a binary [N,d] code. Then,
for any N and d,
Mâˆ—
2(N,2d âˆ’1) = Mâˆ—
2(N +1,2d),
(2.1.17)
and
2Mâˆ—
2(N âˆ’1,d) = Mâˆ—
2(N,d).
(2.1.18)
Proof
To prove (2.1.17) let X be a code of length N, distance 2d âˆ’1 and size
Mâˆ—
2(N,2d âˆ’1). Take its parity-check extension X +. That is, add digit xN+1 to
every codeword x = x1 ...xN so that
N+1
âˆ‘
i=1
xi = 0. Then X + is a code of length N +1,
the same size Mâˆ—
2(N,2d âˆ’1) and distance 2d. Therefore,
Mâˆ—
2(N,2d âˆ’1) â‰¤Mâˆ—
2(N +1,2d).
Similarly, deleting the last digit leads to the inverse:
Mâˆ—
2(N,2d âˆ’1) â‰¥Mâˆ—
2(N +1,2d).

2.1 Hamming spaces. Geometry of codes. Basic bounds on the code size
157
Turning to the proof of (2.1.18), given an [N,d] code, divide the codewords into
two classes: those ending with 0 and those ending with 1. One class must contain
at least half of the codewords. Hence the result.
Corollary 2.1.17
If d is even and such that 2d > N,
Mâˆ—
2(N,d) â‰¤2
A
d
2d âˆ’N
B
(2.1.19)
and
Mâˆ—
2(2d,d) â‰¤4d.
(2.1.20)
If d is odd and 2d +1 > N then
Mâˆ—
2(N,d) â‰¤2
A
d +1
2d +1âˆ’N
B
(2.1.21)
and
Mâˆ—
2(2d +1,d) â‰¤4d +4.
(2.1.22)
Proof
Inequality (2.1.19) follows from (2.1.17), and (2.1.20) follows from
(2.1.18) and (2.1.19): if d = 2dâ€² then
Mâˆ—
2(4dâ€²,2dâ€²) = 2Mâˆ—
2(4dâ€² âˆ’1,2dâ€²) â‰¤8dâ€² = 4d.
Furthermore, (2.1.21) follows from (2.1.17):
Mâˆ—
2(N,d) = Mâˆ—
2(N +1,d +1) â‰¤2
A
d +1
2d +1âˆ’N
B
.
Finally, (2.1.22) follows from (2.1.17) and (2.1.20).
Worked Example 2.1.18
Prove the Plotkin bound for a q-ary code:
Mâˆ—
q(N,d) â‰¤
A
d

d âˆ’N qâˆ’1
q
B
, if d > N qâˆ’1
q
.
(2.1.23)
Solution Given a q-ary [N,M,d] code XN, observe that the minimal distance d is
bounded by the average distance
d â‰¤
1
M(M âˆ’1)S, where S = âˆ‘
xâˆˆX âˆ‘
xâ€²âˆˆX
Î´(x,xâ€²).
As before, let ki j denote the number of letters j âˆˆ{0,...,qâˆ’1} in the ith position
in all codewords from X , i = 1,...,N. Then, clearly,
âˆ‘
0â‰¤jâ‰¤qâˆ’1
kij = M and the
contribution of the ith position into S is
âˆ‘
0â‰¤jâ‰¤qâˆ’1
ki j(M âˆ’ki j) = M2 âˆ’
âˆ‘
0â‰¤jâ‰¤qâˆ’1
k2
ij â‰¤M2 âˆ’M2
q

158
Introduction to Coding Theory
as the quadratic function (u1,...,uq) â†’
âˆ‘
1â‰¤jâ‰¤q
u2
j achieves its minimum on the set
*
u = u1 ...uq : u j â‰¥0,âˆ‘u j = M
+
at u1 = Â·Â·Â· = uq = M/q. Summing over all N
digits, we obtain with Î¸ = (qâˆ’1)/q
M(M âˆ’1)d â‰¤Î¸M2N,
which yields the bound M â‰¤d(d âˆ’Î¸N)âˆ’1. The proof is completed as in the binary
case.
There exists a substantial theory related to the equality in the Plotkin bound
(Hadamard codes) but it will not be discussed in this book. We would also like
to point out the fact that all bounds established so far (Hamming, Singleton, GV
and Plotkin) hold for codes that are not necessarily linear. As far as the GV bound
is concerned, one can prove that it can be achieved by linear codes: see Theorem
2.3.26.
Worked Example 2.1.19
Prove that a 2-error correcting binary code of length
10 can have at most 12 codewords.
Solution The distance of the code must be â‰¥5. Suppose that it contains M code-
words and extend it to an [11,M] code of distance 6. The Plotkin bound works as
follows. List all codewords of the extended code as rows of an M Ã— 11 matrix. If
column i in this matrix contains si zeros and M âˆ’si ones then
6(M âˆ’1)M â‰¤âˆ‘
xâˆˆX + âˆ‘
xâ€²âˆˆX +
Î´(x,xâ€²) â‰¤2
11
âˆ‘
i=1
si(M âˆ’si).
The RHS is â‰¤(1/2) Â· 11M2 if M is even and â‰¤(1/2) Â· 11(M2 âˆ’1) if M is odd.
Hence, M â‰¤12.
Worked Example 2.1.20
(Asymptotics of the size of a binary ball) Let q = 2
and Ï„ âˆˆ(0,1/2). Then, with Î·(Ï„) = âˆ’Ï„ log2 Ï„ âˆ’(1âˆ’Ï„)log2(1âˆ’Ï„) (cf. (1.2.2a)),
lim
Nâ†’âˆ
1
N logvN,2(âŒŠÏ„NâŒ‹) = lim
Nâ†’âˆ
1
N logvN,2(âŒˆÏ„NâŒ‰) = Î·(Ï„).
(2.1.24)
Solution Observe that with R = âŒˆÏ„NâŒ‰the last term in the sum
vN,2(R) =
R
âˆ‘
i=0
N
i

,
R = [Ï„N],
is the largest. Indeed, the ratio of two successive terms is
 N
i+1
N
i

= N âˆ’i
i+1

2.1 Hamming spaces. Geometry of codes. Basic bounds on the code size
159
which remains â‰¥1 for 0 â‰¤i â‰¤R. Hence,
N
R

â‰¤vN,2(R) â‰¤(R+1)
N
R

.
Now use Stirlingâ€™s formula: N! âˆ¼NN+1/2eâˆ’Nâˆš
2Ï€. Then
log
N
R

= âˆ’(N âˆ’R)log N âˆ’R
N
âˆ’Rlog R
N +O(logN)
(2.1.25)
and
âˆ’

1âˆ’R
N

log

1âˆ’R
N

âˆ’R
N log R
N + O(logN)
N
â‰¤log vN,2(R)
N
â‰¤1
N log(R+1)+ the LHS.
The limit R/N â†’Ï„ yields the result. The case where R = âŒŠÏ„NâŒ‹is considered in a
similar manner.
Worked Example 2.1.20 is useful in the study of the asymptotics of
Î±(N,Ï„) = 1
N logMâˆ—
2(N,âŒˆÏ„NâŒ‰),
(2.1.26)
the information rate for the maximum size of a code correcting âˆ¼Ï„N and detecting
âˆ¼2Ï„N errors (i.e. a linear portion of the total number of digits N). Set
a(Ï„) := liminf
Nâ†’âˆÎ±(N,Ï„) â‰¤limsup
Nâ†’âˆ
Î±(N,Ï„) =: a(Ï„)
(2.1.27)
For these limits we have
Theorem 2.1.21
With Î·(Ï„) = âˆ’Ï„ log2 Ï„ âˆ’(1 âˆ’Ï„)log2(1 âˆ’Ï„), the following
asymptotic bounds hold for binary codes:
a(Ï„) â‰¤1âˆ’Î·(Ï„/2), 0 â‰¤Ï„ â‰¤1/2 (Hamming),
(2.1.28)
a(Ï„) â‰¤1âˆ’Ï„, 0 â‰¤Ï„ â‰¤1/2 (Singleton),
(2.1.29)
a(Ï„) â‰¥1âˆ’Î·(Ï„), 0 â‰¤Ï„ â‰¤1/2 (GV),
(2.1.30)
a(Ï„) = 0, 1/2 â‰¤Ï„ â‰¤1 (Plotkin).
(2.1.31)
By using more elaborate bounds (also due to Plotkin), weâ€™ll show in Problem
2.10 that
a(Ï„) â‰¤1âˆ’2Ï„,
0 â‰¤Ï„ â‰¤1/2.
(2.1.32)
The proof of Theorem 2.1.21 is based on a direct inspection of the above-
mentioned bounds; for Hamming and GV bounds it is carried in Worked Example
2.1.22 later.

160
Introduction to Coding Theory
Gilbertâˆ’Varshamov
1
1 2
1
Hamming
Singleton
Plotkin
Figure 2.2
Figure 2.2 shows the behaviour of the bounds established. â€˜Goodâ€™ sequences of
codes are those for which the pair (Ï„,Î±(N,âŒˆÏ„NâŒ‰)) is asymptotically conï¬ned to
the domain between the curves indicating the asymptotic bounds. In particular, a
â€˜goodâ€™ code should â€˜lieâ€™ above the curve emerging from the GV bound. Construct-
ing such sequences is a difï¬cult problem: the ï¬rst examples achieving the asymp-
totic GV bound appeared in 1973 (the Goppa codes, based on ideas from algebraic
geometry). All families of codes discussed in this book produce values below the
GV curve (in fact, they yield Î±(Ï„) = 0), although these codes demonstrate quite
impressive properties for particular values of N, M and d.
As to the upper bounds, the Hamming and Plotkin compete against each other,
while the Singleton bound turns out to be asymptotically insigniï¬cant (although
it is quite important for speciï¬c values of N, M and d). There are about a dozen
various other upper bounds, some of which will be discussed in this and subsequent
sections of the book.
The Gilbertâ€“Varshamov bound itself is not necessarily optimal. Until 1982 there
was no better lower bound known (and in the case of binary coding there is still no
better lower bound known). However, if the alphabet used contains q â‰¥49 symbols
where q = p2m and p â‰¥7 is a prime number, there exists a construction, again based
on algebraic geometry, which produces a different lower bound and gives exam-
ples of (linear) codes that asymptotically exceed, as N â†’âˆ, the GV curve [159].
Moreover, the TVZ construction carries a polynomial complexity. Subsequently,
two more lower bounds were proposed: (a) Elkiesâ€™ bound, for q = p2m + 1; and
(b) Xingâ€™s bound, for q = pm [43, 175]. See N. Elkies, â€˜Excellent codes from mod-

2.1 Hamming spaces. Geometry of codes. Basic bounds on the code size
161
ular curvesâ€™, Manipulating with different coding constructions, the GV bound can
also be improved for other alphabets.
Worked Example 2.1.22
Prove bounds (2.1.28) and (2.1.30) (that is, those parts
of Theorem 2.1.21 related to the asymptotical Hamming and GV bounds).
Solution Picking up the Hamming and the GV parts in (2.1.14), we have
2N/vN,2(d âˆ’1) â‰¤Mâˆ—
2(N,d) â‰¤2N
vN,2

âŒŠ(d âˆ’1)/2âŒ‹

.
(2.1.33)
The lower bound for the Hamming volume is trivial:
vN,2

âŒŠ(d âˆ’1)/2âŒ‹

â‰¥

N
âŒŠ(d âˆ’1)/2âŒ‹

.
For the upper bound, observe that with d/N â‰¤Ï„ < 1/2,
vN,2(d âˆ’1)
â‰¤
âˆ‘
0â‰¤iâ‰¤dâˆ’1

d âˆ’1
N âˆ’d +1
dâˆ’1âˆ’i N
i

â‰¤
âˆ‘
0â‰¤iâ‰¤dâˆ’1

Ï„
1âˆ’Ï„
dâˆ’1âˆ’i  N
d âˆ’1

â‰¤1âˆ’Ï„
1âˆ’2Ï„
 N
d âˆ’1

.
Then, for the information rate (logMâˆ—
2(N,d))

N,
1âˆ’1
N log
 1âˆ’Ï„
1âˆ’2Ï„
 N
d âˆ’1

â‰¤1
N logMâˆ—
2(N,d) â‰¤1âˆ’1
N log

N
âŒŠ(d âˆ’1)/2âŒ‹

.
By Stirlingâ€™s formula, as N â†’âˆthe logs in the previous inequalities obey
1
N log

N
âŒŠ(d âˆ’1)/2âŒ‹

â†’Î·(Ï„/2), 1
N log
 N
d âˆ’1

â†’Î·(Ï„).
The bounds (2.1.28) and (2.1.30) then readily follow.
Consider now the case of a general q-ary alphabet.
Example 2.1.23
Set Î¸ := (q âˆ’1)/q. By modifying the argument in Worked
Example 2.1.22, prove that for any q â‰¥2 and Ï„ âˆˆ

0,Î¸), the volume of the q-ary
Hamming ball has the following logarithmic asymptote:
lim
Nâ†’âˆ
1
N logq vN,q(âŒŠÏ„NâŒ‹) = lim
Nâ†’âˆ
1
N logq vN,q(âŒˆÏ„NâŒ‰) = Î·(q)(Ï„)+Ï„Îº
(2.1.34)

162
Introduction to Coding Theory
where
Î·(q)(Ï„) := âˆ’Ï„ logq Ï„ âˆ’(1âˆ’Ï„)logq(1âˆ’Ï„), Îº := logq(qâˆ’1).
(2.1.35)
Next, similarly to (2.1.26), introduce
Î±(q)(N,Ï„) = 1
N logMâˆ—
q(N,âŒˆÏ„NâŒ‰)
(2.1.36)
and the limits
a(q)(Ï„) := liminf
Nâ†’âˆÎ±(q)(N,Ï„) â‰¤limsup
Nâ†’âˆ
Î±(q)(N,Ï„) =: a(q)(Ï„).
(2.1.37)
Theorem 2.1.24
For all 0 < Ï„ < Î¸,
a(q)(Ï„) â‰¤1âˆ’Î·(q)(Ï„/2)âˆ’ÎºÏ„/2
(Hamming),
(2.1.38)
a(q)(Ï„) â‰¤1âˆ’Ï„
(Singleton),
(2.1.39)
a(q)(Ï„) â‰¥1âˆ’Î·(q)(Ï„)âˆ’ÎºÏ„
(GV),
(2.1.40)
a(q)(Ï„) â‰¤max[1âˆ’Ï„/Î¸,0]
(Plotkin).
(2.1.41)
Of course, the minimum of the right-hand sides of (2.1.38), (2.1.39) and (2.1.41)
provides the better of the three upper bounds. We omit the proof of Theorem 2.1.24,
leaving it as an exercise that is a repetition of the argument from Worked Example
2.1.22.
Example 2.1.25
Prove bounds (2.1.38) and (2.1.40), by modifying the solution
to Worked Example 2.1.22.
2.2 A geometric proof of Shannonâ€™s second coding theorem.
Advanced bounds on the code size
In this section we give alternative proofs of both parts of Shannonâ€™s second cod-
ing theorem (or Shannonâ€™s noisy coding theorem, SCT/NCT; cf. Theorems 1.4.14
and 1.4.15) by using the geometry of the Hamming space. We then apply the tech-
niques that are developed in the course of the proof for obtaining some â€˜advancedâ€™
bounds on codes. The advanced bounds strengthen the Hamming bound established
in Theorem 2.1.6 and its asymptotic counterparts in Theorems 2.1.21 and 2.1.24.
The direct part of the SCT/NCT is given in Theorem 2.2.1 below, in a somewhat
modiï¬ed form compared with Theorems 1.4.14 and 1.4.15. For simplicity, we only
consider here memoryless binary symmetric channels (MBSC), working in space
HN,2 = {0,1}N (the subscript 2 will be omitted for brevity). As we learned in
Section 1.4, the direct part of the SCT states that for any transmission rate R < C
there exist

2.2 A geometric proof of Shannonâ€™s second coding theorem
163
(i) a sequence of codes fn : Un â†’HN, encoding a total of â™¯Un = 2n messages;
and
(ii) a sequence of decoding rules fN : HN â†’Un, such that n âˆ¼NR and the proba-
bility of erroneous decoding vanishes as n â†’âˆ.
Here C is given by (1.4.11) and (1.4.27). For convenience, we reproduce the ex-
pression for C again:
C = 1âˆ’Î·(p), where Î·(p) = âˆ’plog pâˆ’(1âˆ’p)log(1âˆ’p)
(2.2.1)
and the channel matrix is
Î  =
1âˆ’p
p
p
1âˆ’p

.
(2.2.2)
That is, we assume that the channel transmits a letter correctly with probability
1âˆ’p and reverses with probability p, independently for different letters.
In Theorem 2.2.1, it is asserted that there exists a sequence of one-to-one cod-
ing maps fn, for which the task of decoding is reduced to guessing the codewords
fn(u) âˆˆHN. In other words, the theorem guarantees that for all R <C there exists a
sequence of subsets XN âŠ‚HN with â™¯XN âˆ¼2NR for which the probability of incor-
rect guessing tends to 0, and the exact nature of the coding map fn is not important.
Nevertheless, it is convenient to keep the map fn ï¬rmly in sight, as the existence
will follow from a probabilistic construction (random coding) where sample cod-
ing maps are not necessarily one-to-one. Also, the decoding rule is geometric: upon
receiving a word a(N) âˆˆHN, we look for the nearest codeword fn(u) âˆˆXN. Conse-
quently, an error is declared every time such a codeword is not unique or is a result
of multiple encodings or simply yields a wrong message. As we saw earlier, the
geometric decoding rule corresponds with the ML decoder when the probability
p âˆˆ(0,1/2). Such a decoder enables us to use geometric arguments constituting
the core of the proof.
Again as in Section 1.4, the new proof of the direct part of the SCT/NCT only
guarantees the existence of â€˜goodâ€™ codes (and even their â€˜proliferationâ€™) but gives
no clue on how to construct such codes [apart from running again a random coding
scheme and picking its â€˜typicalâ€™ realisation].
In the statement of the SCT/NCT given below, we deal with the maximum error-
probability (2.2.4) rather than the averaged one over possible messages. However,
a large part of the proof is still based on a direct analysis of the error-probabilities
averaged over the codewords.
Theorem 2.2.1
(The SCT/NCT, the direct part) Consider an MBSC with channel
matrix Î  as in (2.2.2), with 0 â‰¤p < 1/2, and let C be as in (2.2.1). Then for any

164
Introduction to Coding Theory
R âˆˆ(0,C) there exists a sequence of one-to-one encoding maps fn : Un â†’HN
such that
(i)
n = âŒŠNRâŒ‹, and â™¯Un = 2n;
(2.2.3)
(ii) as n â†’âˆ, the maximum error-probability under the geometric decoding rule
vanishes:
emax( fn) = max

Pch

error under geometric decoding
| fn(u) sent

: u âˆˆUn

â†’0.
(2.2.4)
Here Pch

Â· | fn(u)sent

stands for the probability distribution of the received
word in HN generated by the channel, conditional on codeword fn(u) being
sent, with u âˆˆUn being the original message emitted by the source.
As an illustration of this result, consider the following.
Example 2.2.2
We wish to send a message u âˆˆA n, where the size of alphabet
A equals K, through an MBSC with channel matrix
0.85
0.15
0.15
0.85

. What rate of
transmission can be achieved with an arbitrarily small probability of error?
Here the value C = 1âˆ’Î·(0.15) = 0.577291. Hence, by Theorem 2.2.1 any rate
of transmission < 0.577291 can be achieved for n large enough, with an arbitrarily
small probability of error. For example, if we want a rate of transmission 0.5 < R <
0.577291 and emax < 0.015 then there exist codes fn : A n â†’{0,1}âŒˆn/RâŒ‰achieving
this goal provided that n is sufï¬ciently large: n > n0.
Suppose that we know such a code fn. How do we encode the message m? First
divide m into blocks of length L where
L =
E0.577291N
logK
F
so that |A L| = KL â‰¤20.577291N.
Then we can embed the blocks from A L in the alphabet A n and so encode the
blocks. The transmission rate is log|A L|

âŒˆn/RâŒ‰âˆ¼0.577291. As was mentioned,
the SCT tells us that there are such codes but gives no idea of how to ï¬nd (or
construct) them, which is difï¬cult to do.
Before we embark on the proof of Theorem 2.2.1, we would like to explore
connections between the geometry of Hamming space HN and the randomness
generated by the channel. As in Section 1.4, we use the symbol P

Â· | fn(u)

as a
shorthand for Pch

Â· | fn(u) sent

. The expectation and variance under this distribu-
tion will be denoted by E(Â·| fn(u)

and Var(Â·| fn(u)

.

2.2 A geometric proof of Shannonâ€™s second coding theorem
165
Observe that, under distribution P

Â· | fn(u)

, the number of distorted digits in
the (random) received word Y(N) can be written as
N
âˆ‘
j=1
1(digit j in Y(N) Ì¸= digit j in fn(u)).
This is a random variable which has a binomial distribution Bin(N, p), with the
mean value
E

N
âˆ‘
j=1
1(digit j in Y(N) Ì¸= digit j in fn(u))
 fn(u)

=
N
âˆ‘
j=1
E

1(digit j in Y(N) Ì¸= digit j in fn(u))| fn(u)

= Np,
and the variance
Var

N
âˆ‘
j=1
1(digit j in Y(N) Ì¸= digit j in fn(u))
 fn(u)

=
N
âˆ‘
j=1
Var

1(digit j in Y(N) Ì¸= digit j in fn(u))| fn(u)

= Np(1âˆ’p).
Then, by Chebyshevâ€™s inequality, for all given Îµ âˆˆ(0,1âˆ’p) and positive integer
N > 1/Îµ, the probability that at least âŒŠN(p + Îµ)âŒ‹digits have been distorted given
that the codeword fn(u) has been sent, is
â‰¤P

â‰¥N(p+Îµ)âˆ’1 distorted |fn(u)

â‰¤
p(1âˆ’p)
N(Îµ âˆ’1/N)2 .
(2.2.5)
Proofof Theorem 2.2.1. Throughout the proof, we follow the set-up from (2.2.3).
Subscripts n and N will be often omitted; viz., we set
2n = M.
We will assume the ML/geometric decoder without any further mention. Similarly
to Section 1.4, we identify the set of source messages Un with Hamming space Hn.
As proposed by Shannon, we use again a random encoding. More precisely, a mes-
sage u âˆˆHn is mapped to a random codeword Fn(u) âˆˆHN, with IID digits taking
values 0 and 1 with probability 1/2 and independently of each other. In addition,
we make codewords Fn(u) independent for different messages u âˆˆHn; labelling
the strings from Hn by u(1),...,u(M) (in no particular order) we obtain a fam-
ily of IID random strings Fn(u(1)),...,Fn(u(M)) from HN. Finally, we make the
codewords independent of the channel. Again, in analogy with Section 1.4, we can
think of the random code under consideration as a random megastring/codebook
from HNM = {0,1}NM with IID digits 0, 1 of equal probability. Every given sample
f(= fn) of this random codebook (i.e. any given megastring from HNM) speciï¬es

166
Introduction to Coding Theory
f(u(1))
f(
)
f
u(2
(u(r))
)
f
Figure 2.3
a deterministic encoding f(u(1)),..., f(u(M)) of messages u(1),...,u(M), i.e. a
code f; see Figure 2.3.
As in Section 1.4, we denote by Pn the probability distribution of the random
code, with
Pn(Fn = f) =
1
2NM ,
for all sample megastrings f,
(2.2.6)
and by E n the expectation relative to Pn.
The plan of the rest of the proof is as follows. First, we will prove (by repeating
in part arguments from Section 1.4) that, for the transmission rate R âˆˆ(0,C), the
expected average probability for the above random coding goes to zero as n â†’âˆ:
lim
nâ†’âˆE n

eave(Fn)

= 0.
(2.2.7)
Here eave(Fn), which is shorthand for eave (Fn(u(1)),...,Fn(u(M))), is a random
variable taking values in (0,1) and representing the aforementioned average error-
probability for the random coding in question. More precisely, as in Section 1.4,
for all given sample collections of codewords f(u(1)),..., f(u(M)) âˆˆHN (i.e. for
all given megastrings f from HNM), we deï¬ne
eave
fn

= 1
M âˆ‘
1â‰¤iâ‰¤M
P

error while using codebook f| f(u(i))

.
(2.2.8)
Then the expected average error-probability is given by
E n

eave(Fn)

=
1
2NM
âˆ‘
f(u(1)),..., f(u(M))âˆˆHN
eave
f

.
(2.2.9)
Relation (2.2.9) implies (again in a manner similar to Section 1.4) that there
exists a sequence of deterministic codes fn such that the average error-probability
eave( fn) = eave( fn(u(1)),..., fn(u(2n))) obeys
lim
nâ†’âˆeave( fn) = 0.
(2.2.10)
Finally, we will deduce (2.2.4) from (2.2.10): see Lemma 2.2.6.

2.2 A geometric proof of Shannonâ€™s second coding theorem
167
a received
.   .   .   
.   .   
decoding
fn (u(i)) sent 
fn (u(i))
a
   N (a, m)
Figure 2.4
Remark 2.2.3
As the codewords f(u(1)),..., f(u(M)) are thought to come
from a sample of the random codebook, we must allow that they may coincide
( f(u(i)) = f(u( j)) for i Ì¸= j), in which case, by default, the ML decoder is report-
ing an error. This must be included when we consider probabilities in the RHS of
(2.2.8). Therefore, for i = 1,...,M we deï¬ne
P

error while using codebook f| f(u(i))

=
â§
âª
âª
âª
âª
âª
â¨
âª
âª
âª
âª
âª
â©
1,
if f(u(i)) = f(u(iâ€²)) for some iâ€² Ì¸= i,
P

Î´

Y(N), f(u( j))

â‰¤Î´

Y(N), f(u(i))

for some j Ì¸= i | f(u(i))

,
if f(u(i)) Ì¸= f(u(iâ€²)) for all iâ€² Ì¸= i.
(2.2.11)
Let us now go through the detailed argument. The ï¬rst step is
Lemma 2.2.4
Consider the channel matrix Î  (cf. (2.2.2)) with 0 â‰¤p < 1/2.
Suppose that the transmission rate R < C = 1 âˆ’Î·(p). Let N be > 1/Îµ. Then for
any Îµ âˆˆ(0,1/2 âˆ’p), the expected average error-probability E n

eave(Fn)

deï¬ned
in (2.2.8), (2.2.9) obeys
E n

eave(Fn)

â‰¤
p(1âˆ’p)
N(Îµ âˆ’1/N)2 + M âˆ’1
2N
vN
3
N(p+Îµ)
4
,
(2.2.12)
where vN(b) stands for the number of points in the ball of radius b in the binary
Hamming space HN.
Proof
Set m(= mN(p,Îµ)) := âŒˆN(p+ Îµ)âŒ‰. The ML decoder deï¬nitely returns the
codeword fn(u(i)) sent through the channel when fn(u(i)) is the only codeword in

168
Introduction to Coding Theory
the Hamming ball BN(y,m) around the received word y

= y(N)
âˆˆHN (see
Figure 2.4). In any other situation (when fn(u(i)) Ì¸âˆˆBN(y,m) or fn(u(k)) âˆˆ
BN(y,m) for some k Ì¸= i) there is a possibility of error.
Hence,
P

error while using codebook f| fn(u(i))

â‰¤
âˆ‘
yâˆˆHN
P

y| fn(u(i))

1

fn(u(i)) Ì¸âˆˆBN(y,m)

+ âˆ‘
zâˆˆHN
P(z| fn(u(i))) âˆ‘
kÌ¸=i
1

fn(u(k)) âˆˆBN(z,m)

.
(2.2.13)
The ï¬rst sum in the RHS is simple to estimate:
âˆ‘
yâˆˆHN
P

y| fn(u(i))

1

fn(u(i) Ì¸âˆˆBN(y,m)

=
âˆ‘
yâˆˆHN
P

y| fn(u(i))

1

distance Î´

y, fn(u(i))

â‰¥m

= P

â‰¥m digits distorted| f(u(i))

â‰¤
p(1âˆ’p)
N(Îµ âˆ’1/N)2 ,
(2.2.14)
by virtue of (2.2.5). Observe that since the RHS in (2.2.14) does not depend on the
choice of the sample code f, the bound (2.2.14) will hold when we take ï¬rst the
average 1
M âˆ‘
1â‰¤iâ‰¤M
and then expectation E n.
The second sum in the RHS of (2.2.13) is more tricky: it requires averaging and
taking expectation. Here we have
E n

âˆ‘
1â‰¤iâ‰¤M âˆ‘
zâˆˆHN
P(z|Fn(u(i))) âˆ‘
kÌ¸=i
1

Fn(u(k)) âˆˆBN(z,m)


=
âˆ‘
1â‰¤iâ‰¤M âˆ‘
kÌ¸=i âˆ‘
zâˆˆHN
En

P(z|Fn(u(i)))1

Fn(u(k)) âˆˆBN(z,m)

=
âˆ‘
1â‰¤iâ‰¤M âˆ‘
kÌ¸=i âˆ‘
zâˆˆHN
En

P(z|Fn(u(i)))

Ã—En

1

Fn(u(k)) âˆˆBN(z,m)

,
(2.2.15)
since random codewords Fn(u(1)),...,Fn(u(M)) are independent. Next, as
each of these codewords is uniformly distributed over HN, the expectations
En
	
P(z|Fn(u(i)))

and En
	
1

Fn(u(k)) âˆˆBN(z,m)

can be calculated as
En
	
P(z|Fn(u(i)))

= 1
2N âˆ‘
xâˆˆHN
P(z|x)
(2.2.16a)
and
En
	
1

Fn(u(k)) âˆˆBN(z,m)

= vN(m)
2N
.
(2.2.16b)

2.2 A geometric proof of Shannonâ€™s second coding theorem
169
Further, summing over z yields
âˆ‘
zâˆˆHN âˆ‘
xâˆˆHN
P(z|x) = âˆ‘
xâˆˆHN âˆ‘
zâˆˆHN
P(z|x) = 2N.
(2.2.17)
Finally, after summation over k Ì¸= i we obtain
the RHS of (2.2.15) = 1
M âˆ‘
1â‰¤iâ‰¤Mâˆ‘
kÌ¸=i
vN(m)
2N
= vN(m)M(M âˆ’1)
2NM
= (M âˆ’1)vN(m)
2N
.
(2.2.18)
Collecting (2.2.12)â€“(2.2.18) we have that EN [eave(Fn)] does not exceed the RHS
of (2.2.12).
At the next stage we estimate the volume vN(m) in terms of entropy h(p + Îµ)
where, recall, m = âŒˆN(p+Îµ)âŒ‰. The argument here is close to that from Section 1.4
and based on the following result.
Lemma 2.2.5
Suppose that 0 < p < 1/2, Îµ > 0 and positive integer N satisfy
p+Îµ +1/N < 1/2. Then the following bound holds true:
vN(âŒˆN(p+Îµ)âŒ‰) â‰¤2NÎ· (p+Îµ).
(2.2.19)
The proof of Lemma 2.2.5 will be given later, after Worked Example 2.2.7. For
the moment we proceed with the proof of Theorem 2.2.1. Recall, we want to es-
tablish (2.2.7). In fact, if p < 1/2 and R < C = 1âˆ’Î·(p) then we set Î¶ = Câˆ’R > 0
and take Îµ > 0 so small that (i) p+Îµ < 1/2 and (ii) R+Î¶/2 < 1âˆ’Î·(p+Îµ). Then
we take N so large that (iii) N > 2/Îµ. With this choice of Îµ and N, we have
Îµ âˆ’1
N > Îµ
2 and Râˆ’1+Î·(p+Îµ) < âˆ’Î¶
2 .
(2.2.20)
Then, starting with (2.2.12), we can write
EN
	
e(Fn)

â‰¤4p(1âˆ’p)
NÎµ2
+ 2NR
2N 2NÎ·(p+Îµ)
<
4
NÎµ2 p(1âˆ’p)+2âˆ’NÎ¶/2.
(2.2.21)
This implies (2.2.7) and hence the existence of a sequence of codes fn : Hn â†’HN
obeying (2.2.10).
To ï¬nish the proof of Theorem 2.2.1, we deduce (2.2.4) from (2.2.7), in the form
of Lemma 2.2.6:
Lemma 2.2.6
Consider a binary channel (not necessarily memoryless), and
let C > 0 be a given constant. With 0 < R < C and n = âŒŠNRâŒ‹, deï¬ne quantities
emax( fn) and eave(  fn) as in (2.2.4), (2.2.8) and (2.2.11), for codes fn : Hn â†’HN
and  fn : Hn â†’HN. Then the following statements are equivalent:

170
Introduction to Coding Theory
(i) For all R âˆˆ(0,C), there exist codes fn with lim
nâ†’âˆemax( fn) = 0.
(ii) For all R âˆˆ(0,C), there exist codes  fn such that lim
nâ†’âˆeave(  fn) = 0.
Proof of Lemma 2.2.6. It is clear that assertion (i) implies (ii). To deduce (i) from
(ii), take R < C and set for N big enough
Râ€² = R+ 1
N < C, nâ€² = âŒŠNRâ€²âŒ‹, Mâ€² = 2nâ€².
(2.2.22)
We know that there exists a sequence  fn of codes Hnâ€² â†’HN with eave(  fn) â†’0.
Recall that
eave(  fn) = 1
Mâ€² âˆ‘
1â‰¤iâ‰¤Mâ€²
P

error while using  fn|  fn(u(i))

.
(2.2.23)
Here and below, Mâ€² = 2âŒŠNRâ€²âŒ‹and  fn(u(1)),...,  fn(u(Mâ€²)) are the codewords for
source messages u(1),...,u(Mâ€²) âˆˆHnâ€².
Instead of P

error while using  fn|  fn(u(i))

, we write P

fn-error|  fn(u(i))

,
for brevity. Now, at least half of summands P

 fn-error|  fn(u(i))

in the RHS of
(2.2.23) must be < 2eave(  fn). Observe that, in view of (2.2.22),
Mâ€²/2 â‰¥2âŒŠNRâŒ‹âˆ’1.
(2.2.24)
Hence we have at our disposal at least 2âŒŠNRâŒ‹âˆ’1 codewords f(u(i)) with
P

error|  fn(u(i))

< 2eave(  fn).
List these codewords as a new binary code, of length N and information rate

logMâ€²/2

N. Denoting this new code by fn, we have
emax( fn) â‰¤2eave(  fn).
Hence, emax( fn) â†’0 as n â†’âˆwhereas

logMâ€²/2

N â†’R. This gives statement
(i) and completes the proof of Lemma 2.2.6.
Therefore, the proof of Theorem 2.2.1 is now complete (provided that we prove
Lemma 2.2.5).
Worked Example 2.2.7
(cf. Worked Example 2.1.20.) Prove that for positive
integers N and m, with m < N/2 and Î² = m/N,
2NÎ·(Î²)
(N +1) < vN(m) < 2NÎ·(Î²).
(2.2.25)

2.2 A geometric proof of Shannonâ€™s second coding theorem
171
Solution Write
vN(m) = â™¯
*
points at distance â‰¤m from 0 in HN
+
= âˆ‘
0â‰¤kâ‰¤m
N
k

.
With Î² = m/N < 1/2, we have that Î²/(1âˆ’Î²) < 1, and so

Î²
1âˆ’Î²
m
<

Î²
1âˆ’Î²
k
, for 0 â‰¤k < m.
Then, for 0 â‰¤k < m, the product
Î² k(1âˆ’Î²)Nâˆ’k =

Î²
1âˆ’Î²
k
(1âˆ’Î²)N
>

Î²
1âˆ’Î²
m
(1âˆ’Î²)N = Î² m(1âˆ’Î²)Nâˆ’m.
Hence,
1
=
âˆ‘
0â‰¤kâ‰¤N
N
k

Î² k(1âˆ’Î²)Nâˆ’k >
âˆ‘
0â‰¤kâ‰¤m
N
k

Î² k(1âˆ’Î²)Nâˆ’k
> Î² m(1âˆ’Î²)Nâˆ’m
âˆ‘
0â‰¤kâ‰¤m
N
k

= vN(m)Î² m(1âˆ’Î²)Nâˆ’m
= vN(m)2N[(m/N)logÎ²+(1âˆ’m/N)log(1âˆ’Î²)],
implying that vN(m) < 2NÎ·(Î²). To obtain the left-hand bound in (2.2.25), write
vN(m) >
N
m

;
then we aim to check that the RHS is â‰¥2NÎ·(Î²)/(N + 1). Consider a binomial
random variable Y âˆ¼Bin(N,Î²) with
pk = P(Y = k) =
N
k

Î² k(1âˆ’Î²)Nâˆ’k, k = 0,...,N.
It sufï¬ces to prove that pk achieves its maximal value when k = m, since then
pm =
N
m

Î² m(1âˆ’Î²)Nâˆ’m â‰¥
1
N +1, with Î² m(1âˆ’Î²)Nâˆ’m = 2âˆ’NÎ·(Î²).
To this end, suppose ï¬rst that k â‰¤m and write
pk
pm
= m!(N âˆ’m)!(N âˆ’m)mâˆ’k
k!(N âˆ’k)!mmâˆ’k
= (k +1)Â·Â·Â·m
mmâˆ’k
Â·
(N âˆ’m)mâˆ’k
(N âˆ’m+1)Â·Â·Â·(N âˆ’k).

172
Introduction to Coding Theory
Here, the RHS is â‰¤1, as it is the product of 2(mâˆ’k) factors each of which is â‰¤1.
Similarly, if k â‰¥m, we arrive at the product
mkâˆ’m
(m+1)Â·Â·Â·k Â· (N âˆ’k +1)Â·Â·Â·(N âˆ’m)
(N âˆ’m)kâˆ’m
which is again â‰¤1 as the product of 2(kâˆ’m) factors â‰¤1. Thus, the ratio pk/pm â‰¤
1, and the desired bound follows.
We are now in position to prove Lemma 2.2.5.
Proof of Lemma 2.2.5 First, p+Îµ < 1/2 implies that m = âŒŠN(p+Îµ)âŒ‹< N/2 and
Î² := m
N = âŒŠN(p+Îµ)âŒ‹
N
< p+Îµ,
which, in turn, implies that Î·(Î²) < Î·(p + Îµ) as x â†’Î·(x) is a strictly increasing
function for x from the interval (0,1/2). This yields the assertion of Lemma 2.2.5.
The geometric proof of the direct part of SCT/NCT clariï¬es the meaning of the
concept of capacity (of an MBSC at least). Physically speaking, in the expressions
(1.4.11), (1.4.27) and (2.2.1) for capacity C = Î·(p) of an MBSC, the positive term
1 points at the rate at which a random code produces an â€˜emptyâ€™ volume between
codewords whereas the negative term âˆ’Î·(p) indicates the rate at which the code-
words progressively ï¬ll this space. We continue with a working example of an
essay type:
Worked Example 2.2.8
Quoting general theorems on the evaluation of the chan-
nel capacity, deduce an expression for the capacity of a memoryless binary sym-
metric channel. Evaluate, in particular, the capacities of (i) a symmetric memory-
less channel and (ii) a perfect channel with an input alphabet {0,1} whose inputs
are subject to the restriction that 0 should never occur in succession.
Solution The channel capacity is deï¬ned as a supremum of transmission rates R for
which the received message can be decoded correctly, with probability approaching
1 as the length of the message increases to inï¬nity. A popular class is formed by
memoryless channels where, for a given input word x(N) = x1 ...xN, the probability
P(N)
y(N) received|x(N) sent

= âˆ
1â‰¤iâ‰¤N
P(yi|xi).
In other words, the noise acts on each symbol xi of the input string x independently,
and P(y|x) is the probability of having an output symbol y given that the input
symbol is x.

2.2 A geometric proof of Shannonâ€™s second coding theorem
173
Symbol x runs over Ain, an input alphabet of a given size q, and y belongs to
Aout, an output alphabet of size r. Then probabilities P(y|x) form a qÃ—r stochastic
matrix (the channel matrix). A memoryless channel is called symmetric if the rows
of this matrix are permutations of each other, i.e. contain the same collection of
probabilities, say p1,..., pr. A memoryless symmetric channel is said to be double-
symmetric if the columns of the channel matrix are also permutations of each other.
If m = n = 2 (typically, Ain = Aout = {0,1}) a memoryless channel is called binary.
For a memoryless binary symmetric channel, the channel matrix entries P(y|x)
are P(0|0) = P(1|1) = 1 âˆ’p, P(1|0) = P(0|1) = p, p âˆˆ(0,1) being the ï¬‚ipping
probability and 1 âˆ’p the probability of ï¬‚awless transmission of a single binary
symbol.
A channel is characterised by its capacity: the value C â‰¥0 such that:
(a) for all R < C, R is a reliable transmission rate; and
(b) for all R > C, R is an unreliable transmission rate.
Here R is called a reliable transmission rate if there exists a sequence of codes
fn : Hn â†’HN and decoding rules fN : HN â†’Hn such that n âˆ¼NR and the (suit-
ably deï¬ned) probability of error
e( fn, fN) â†’0, as N â†’âˆ.
In other words,
C = lim
Nâ†’âˆ
1
N logMN
where MN is the maximal number of codewords x âˆˆHN for which the probability
of erroneous decoding tends to 0.
The SCT asserts that, for a memoryless channel,
C = max
pX I(X : Y)
where I(X : Y) is the mutual information between a (random) input symbol X and
the corresponding output symbol Y, and the maximum is over all possible proba-
bility distributions pX of X.
Now in the case of a memoryless symmetric channel (MSC), the above maximi-
sation procedure applies to the output symbols only:
C =

max
pX
h(Y)

+ âˆ‘
1â‰¤iâ‰¤r
pi log pi;
the sum âˆ’âˆ‘
i
pi log pi being the entropy of the row of channel matrix (P(y|x)). For
a double-symmetric channel, the expression for C simpliï¬es further:
C = logM âˆ’h(p1,..., pr)

174
Introduction to Coding Theory
as h(Y) is achieved at equidistribution pX, with pX(x) â‰¡1/q (and pY(y) â‰¡1/r). In
the case of an MBSC we have
C = 1âˆ’Î·(p).
This completes the solution to part (i).
Next, the channel in part (ii) is not memoryless. Still, the general deï¬nitions are
applicable, together with some arguments developed so far. Let n( j,t) denote the
number of allowed strings of length t ending with letter j, j = 0,1. Then
n(0,t) = n(1,t âˆ’1),
n(1,t) = n(0,t âˆ’1)+n(1,t âˆ’1),
whence
n(1,t) = n(1,t âˆ’1)+n(1,t âˆ’2).
Write it as a recursion
 n(1,t)
n(1,t âˆ’1)

= A
n(1,t âˆ’1)
n(1,t âˆ’2)

,
with the recursion matrix
A =
1
1
1
0

.
The general solution is
n(1,t) = c1Î»t
1 +c2Î»t
2,
where Î»1, Î»2 are the eigenvalues of A, i.e. the roots of the characteristic equation
det (Aâˆ’Î»I) = (1âˆ’Î»)(âˆ’Î»)âˆ’1 = Î» 2 âˆ’Î» âˆ’1 = 0.
So, Î» =

1Â±
âˆš
5

2, and
1
t logn(1,t) = log
âˆš
5+1
2

.
The capacity of the channel is given by
C
= lim
tâ†’âˆ
1
t log â™¯of allowed input strings of length t
= lim
tâ†’âˆ
1
t log
	
n(1,t)+n(0,t)

= log
âˆš
5+1
2

.

2.2 A geometric proof of Shannonâ€™s second coding theorem
175
Remark 2.2.9
We can modify the last question, by considering an MBC with
the channel matrix Î  =
1âˆ’p
p
p
1âˆ’p

whose input is under a restriction that 0
should never occur in succession. Such a channel may be treated as a composi-
tion of two consecutive channels (cf. Worked Example 1.4.29(a)), which yields the
following answer for the capacity:
C = min

log
âˆš
5+1
2

,1âˆ’Î·(p)

.
Next, we present the strong converse part of Shannonâ€™s SCT for an MBSC (cf.
Theorem 1.4.14); again we are going to prove it by using geometry of Hammingâ€™s
spaces. The term â€˜strongâ€™ indicates that for every transmission rate R > C, the
channel capacity, the maximum probability of error actually gets arbitrarily close
to 1. Again for simplicity, we prove the assertion for an MBSC.
Theorem 2.2.10
(The SCT/NCT, the strong converse part) Let C be the capacity
of an MBSC with the channel matrix
1âˆ’p
p
p
1âˆ’p

, where 0 < p < 1/2, and
take R > C. Then, with n = âŒŠNRâŒ‹, for all codes fn : Hn â†’HN and decoding rules
fN : HN â†’Hn, the maximum error-probability
Îµmax( fn, fN) := max

P

error under fN| fn(u)

: u âˆˆHn

(2.2.26a)
obeys
limsup
Nâ†’âˆ
Îµmax( fn, fN) = 1.
(2.2.26b)
Proof
As in Section 1.4, we can assume that codes fn are one-to-one and obey
fN( fn(u)) = u, for all u âˆˆHn (otherwise, the chances of erroneous decoding will
be even larger). Assume the opposite of (2.2.26b):
Îµmax( fn, fN) â‰¤c for some c < 1 and all N large enough.
(2.2.27)
Our aim is to deduce from (2.2.27) that R â‰¤C. As before, set n = âŒŠNRâŒ‹and let
fn(u(i)) be the codeword for string u(i) âˆˆHn, i = 1,...,2n. Let Di âŠ‚HN be the set
of binary strings where fN returns fn(u(i)): fN(a) = fn(u(i)) if and only if a âˆˆDi.
Then Di âˆ‹f(u(i)), sets Di are pairwise disjoint, and if the union âˆªiDi Ì¸= HN then
on the complement HN \âˆªiDi the channel declares an error. Set si = â™¯Di, the size
of set Di.
Our ï¬rst step is to â€˜improveâ€™ the decoding rule, by making it â€˜closerâ€™ to the ML
rule. In other words, we want to replace each Di with a new set, Ci âˆˆHN, of the
same cardinality â™¯Ci = si, but of a more â€˜roundedâ€™ shape (i.e. closer to a Hamming

176
Introduction to Coding Theory
ball B( f(u(i)),bi)). That is, we look for pairwise disjoint sets Ci, of cardinalities
â™¯Ci = si, satisfying
BN( f(u(i)),bi) âŠ†Ci âŠ‚BN( f(u(i)),bi +1), 1 â‰¤i â‰¤2n,
(2.2.28)
for some values of radius bi â‰¥0, to be speciï¬ed later. We can think that Ci is
obtained from Di by applying a number of â€˜disjoint swapsâ€™ where we remove a
string a and add another string, b, with the Hamming distance
Î´

b, fn(u(i))

â‰¤Î´

a, fn(u(i))

.
(2.2.29)
Denote the new decoding rule by gN. As the ï¬‚ipping probability p < 1/2, the
relation (2.2.29) implies that
P( fN returns fn(u(i))| fn(u(i))) = P(Di| fn(u(i)))
â‰¤P(Ci| fn(u(i))) = P(gN returns fn(u(i))| fn(u(i))),
which in turn is equivalent to
P(error when using gN| fn(u(i))) â‰¤P(error when using fN| fn(u(i))).
(2.2.30)
Then, clearly,
Îµmax( fn, gN) â‰¤Îµmax( fn, fN) â‰¤c.
(2.2.31)
Next, suppose that there exists pâ€² < p such that, for any N large enough,
bi +1 â‰¤âŒˆNpâ€²âŒ‰for some 1 â‰¤i â‰¤2n.
(2.2.32)
Then, by virtue of (2.2.28) and (2.2.31), with C c
i standing for the complement
HN \Ci,
P(at least Npâ€² digits distorted| fn(u(i)))
â‰¤P(at least bi +1 digits distorted| fn(u(i)))
â‰¤P(C c
i | fn(u(i))) â‰¤Îµmax( fn, gN) â‰¤c.
This would lead to a contradiction, since, by the law of large numbers, as N â†’âˆ,
the probability
P(at least Npâ€² digits distorted|x sent) â†’1
uniformly in the choice of the input word x âˆˆHN. (In fact, this probability does
not depend on x âˆˆHN.)
Thus, we cannot have pâ€² âˆˆ(0, p) such that, for N large enough, (2.2.32) holds
true. That is, the opposite is true: for any given pâ€² âˆˆ(0, p), we can ï¬nd an arbitrarily
large N such that
bi > Npâ€²,
for all i = 1,...,2n.
(2.2.33)

2.2 A geometric proof of Shannonâ€™s second coding theorem
177
(As we claim (2.2.33) for all pâ€² âˆˆ(0, p), it does not matter if in the LHS of (2.2.33)
we put bi or bi +1.)
At this stage we again use the explicit expression for the volume of the Hamming
ball:
si = â™¯Di = â™¯Ci â‰¥vN(bi) = âˆ‘
0â‰¤jâ‰¤bi
N
j

â‰¥
N
bi

â‰¥

N
âŒŠNpâ€²âŒ‹

, provided that bi > Npâ€².
(2.2.34)
A useful bound has been provided in Worked Example 2.2.7 (see (2.2.25)):
vN(R) â‰¥
1
N +12NÎ·

R
N

.
(2.2.35)
We are now in a position to ï¬nish the proof of Theorem 2.2.10. In view of
(2.2.35), we have that, for all pâ€² âˆˆ(0, p), we can ï¬nd an arbitrarily large N such
that
si â‰¥2N(Î·(pâ€²)âˆ’ÎµN),
for all 1 â‰¤i â‰¤2n,
with lim
Nâ†’âˆÎµN = 0. As the original sets D1,...,D2n are disjoint, we have that
s1 +Â·Â·Â·+s2n â‰¤2N, implying that 2N(Î·(pâ€²)âˆ’ÎµN) Ã—2âŒŠNRâŒ‹â‰¤2N,
or
Î·(pâ€²)âˆ’ÎµN + âŒŠNRâŒ‹
N
â‰¤1, implying that R â‰¤1âˆ’Î·(pâ€²)+ÎµN + 1
N .
As N â†’âˆ, the RHS tends to 1 âˆ’Î·(p). So, given any pâ€² âˆˆ(0, p), R â‰¤1 âˆ’Î·(pâ€²).
This is true for all pâ€² < p, hence R â‰¤1 âˆ’Î·(p) = C. This completes the proof of
Theorem 2.2.10.
We have seen that the analysis of intersections of a given set X in a Hamming
space HN (and more generally, in HN,q) with various balls BN(y,s) reveals a lot
about the set X itself. In the remaining part of this section such an approach will
be used for producing some advanced bounds on q-ary codes: the Elias bound and
the Johnson bound. These bounds are among the best-known general bounds for
codes, and they are competing.
The Elias bound is proved in a fashion similar to Plotkinâ€™s: cf. Theorem 2.1.15
and Worked Example 2.1.18. We count codewords from a q-ary [N,M,d] code X
in balls BN,q(y,s) of radius s about words y âˆˆHN,q. More precisely, we count pairs
(x,BN,q(y,s)) where x âˆˆX âˆ©BN,q(y,s). If ball BN,q(y,s) contains Ky codewords
then
âˆ‘
yâˆˆHN
Ky = MvN,q(s)
(2.2.36)

178
Introduction to Coding Theory
as each word x falls in vN,q(s) of the balls BN,q(y,s).
Lemma 2.2.11
If X is a q-ary [N,M] code then for all s = 1,...,N there ex-
ists a ball BN,q(y,s) about an N-word y âˆˆHN,q with the number Ky = â™¯

X âˆ©
BN,q(y,s)

of codewords in BN,q(y,s) obeying
Ky â‰¥MvN,q(s)/qN.
(2.2.37)
Proof
Divide both sides of (2.2.36) by qN. Then 1
qN âˆ‘
y Ky gives the average num-
ber of codewords in ball BN,q(y,s). But there must be a ball containing at least as
many as the average number of codewords.
A ball BN,q(y,s) with property (2.2.37) is called critical (for code X ).
Theorem 2.2.12
(The Elias bound) Set Î¸ = (qâˆ’1)/q. Then for all integers s â‰¥1
such that s < Î¸N and s2 âˆ’2Î¸Ns+Î¸Nd > 0, the maximum size Mâˆ—
q(N,d) of a q-ary
code of length N and distance d satisï¬es
Mâˆ—
q(N,d) â‰¤
Î¸Nd
s2 âˆ’2Î¸Ns+Î¸Nd Â·
qN
vN,q(s) .
(2.2.38)
Proof
Fix a critical ball BN,q(y,s) and consider code X â€² obtained by subtracting
word y from the codewords of X : X â€² = {x âˆ’y : x âˆˆX }. Then X â€² is again an
[N,M,d] code. So, we can assume that y = 0 and BN,q(0,s) is a critical ball.
Then take X1 = X âˆ©BN,q(0,s) = {x âˆˆX : w(x) â‰¤s}. The code X1 is [N,K,e]
where e â‰¥d and K (= K0) â‰¥MvN,q(s)/qN. As in the proof of the Plotkin bound,
consider the sum of the distances between the codewords in X1:
S1 = âˆ‘
xâˆˆX1 âˆ‘
xâ€²âˆˆX1
Î´(x,xâ€²).
Again, we have that S1 â‰¥K(K âˆ’1)e. On the other hand, if kij is the number of
letters j âˆˆJq = {0,...,qâˆ’1} in the ith position in all codewords x âˆˆX1 then
S1 = âˆ‘
1â‰¤iâ‰¤N
âˆ‘
0â‰¤jâ‰¤qâˆ’1
kij(K âˆ’kij).
Note that the sum
âˆ‘
0â‰¤jâ‰¤qâˆ’1
ki j = K. Besides, as w(x) â‰¤s, the number of 0s in every
word x âˆˆX1 is â‰¥N âˆ’s. Then the total number of 0s in all codewords equals
âˆ‘
1â‰¤iâ‰¤N
ki0 â‰¥K(N âˆ’s). Now write
S = NK2 âˆ’âˆ‘
1â‰¤iâ‰¤N

k2
i0 +
âˆ‘
1â‰¤jâ‰¤qâˆ’1
k2
ij

,

2.2 A geometric proof of Shannonâ€™s second coding theorem
179
and use the Cauchyâ€“Schwarz inequality to estimate
âˆ‘
1â‰¤jâ‰¤qâˆ’1
k2
i j â‰¥
1
qâˆ’1

âˆ‘
1â‰¤jâ‰¤qâˆ’1
kij
2
=
1
qâˆ’1(K âˆ’ki0)2.
Then
S â‰¤NK2 âˆ’
âˆ‘
1â‰¤iâ‰¤N

k2
i0 +
1
qâˆ’1(K âˆ’ki0)2

= NK2 âˆ’
1
qâˆ’1
âˆ‘
1â‰¤iâ‰¤N
	
(qâˆ’1)k2
i0 +K2 âˆ’2Kki0 +k2
i0

= NK2 âˆ’
1
qâˆ’1
âˆ‘
1â‰¤iâ‰¤N
(qk2
i0 +K2 âˆ’2Kki0)
= NK2 âˆ’
N
qâˆ’1K2 âˆ’
q
qâˆ’1
âˆ‘
1â‰¤iâ‰¤N
k2
i0 +
2
qâˆ’1K
âˆ‘
1â‰¤iâ‰¤N
ki0
= qâˆ’2
qâˆ’1NK2 âˆ’
q
qâˆ’1
âˆ‘
1â‰¤iâ‰¤N
k2
i0 +
2
qâˆ’1KL,
where L =
âˆ‘
1â‰¤iâ‰¤N
ki0. Use Cauchyâ€“Schwarz once again:
âˆ‘
1â‰¤iâ‰¤N
k2
i0 â‰¥1
N

âˆ‘
1â‰¤iâ‰¤N
ki0
2
= 1
N L2.
Then
S â‰¤qâˆ’2
qâˆ’1NK2 âˆ’
q
qâˆ’1
1
N L2 +
2
qâˆ’1KL
=
1
qâˆ’1

(qâˆ’2)NK2 âˆ’q
N L2 +2KL

.
The maximum of the quadratic expression in the square brackets occurs at L =
NK/q. Recall that L â‰¥K(N âˆ’s). So, choosing K(N âˆ’s) â‰¥NK/q, i.e. s â‰¤N(q âˆ’
1)/q, we can estimate
S â‰¤
1
qâˆ’1

(qâˆ’2)NK2 âˆ’q
N K2(N âˆ’s)2 +2K2(N âˆ’s)

=
1
qâˆ’1K2s

2(qâˆ’1)âˆ’qs
N

.
This yields the inequality K(K âˆ’1)e â‰¤
1
qâˆ’1K2s

2(qâˆ’1)âˆ’qs
N

which can be
solved for K:
K â‰¤
Î¸Ne
s2 âˆ’2Î¸Ns+Î¸Ne,

180
Introduction to Coding Theory
provided that s < NÎ¸ and s2 âˆ’2Î¸Ns + Î¸Ne > 0. Finally, recall that X (1) arose
from an [N,M,d] code X , with K â‰¥Mv(s)/qN and e â‰¥d. As a result, we obtain
that
MvN,q(s)
qN
â‰¤
Î¸Nd
s2 âˆ’2Î¸Ns+Î¸Nd .
This leads to the Elias bound (2.2.38).
The ideas used in the proof of the Elias bound (and earlier in the proof of the
Plotkin bounds) are also helpful in obtaining bounds for W âˆ—
2 (N,d,â„“), the maximal
size of a binary (non-linear) code X âˆˆHN,2 of length N, distance d(X ) â‰¥d and
with the property that the weight w(x) â‰¡â„“, x âˆˆX . First, three obvious statements:
(i)
W âˆ—
2 (N,2k,k) =
AN
k
B
,
(ii)
W âˆ—
2 (N,2k,â„“) = W âˆ—
2 (N,2k,N âˆ’â„“),
(iii)
W âˆ—
2 (N,2k âˆ’1,â„“) = W âˆ—
2 (N,2k,â„“), â„“/2 â‰¤k â‰¤â„“.
[The reader is advised to prove these as an exercise.]
Worked Example 2.2.13
Prove that for all positive integers N â‰¥1, k â‰¤N
2 and
â„“< N
2 âˆ’
?
N2
4 âˆ’kN,
W âˆ—
2 (N,2k,â„“) â‰¤
A
kN
â„“2 âˆ’â„“N +kN
B
.
(2.2.39)
Solution Take an [N,M,2k] code X such that w(x) â‰¡â„“, x âˆˆX . As before, let
ki1 be the number of 1s in position i in all codewords. Consider the sum of the
dot-products D =
âˆ‘
x,xâ€²âˆˆX
1

x Ì¸= xâ€²
âŸ¨xÂ·xâ€²âŸ©. We have
âŸ¨xÂ·xâ€²âŸ©= w(xâˆ§xâ€²) = 1
2

w(x)+w(xâ€²)âˆ’Î´

x,xâ€²)

â‰¤1
2(2â„“âˆ’2k) = â„“âˆ’k
and hence
D â‰¤(â„“âˆ’k)M(M âˆ’1).
On the other hand, the contribution to D from position i equals ki1(ki1 âˆ’1), i.e.
D = âˆ‘
1â‰¤iâ‰¤N
ki1(ki1 âˆ’1) = âˆ‘
1â‰¤iâ‰¤N
(k2
i1 âˆ’ki1) = âˆ‘
1â‰¤iâ‰¤N
k2
i1 âˆ’â„“M.

2.2 A geometric proof of Shannonâ€™s second coding theorem
181
Again, the last sum is minimised at ki1 = â„“M/N, i.e.
â„“2M2
N
âˆ’â„“M â‰¤D â‰¤(â„“âˆ’k)M(M âˆ’1).
This immediately leads to (2.2.39).
Another useful bound is given now.
Worked Example 2.2.14
Prove that for all positive integers N â‰¥1, k â‰¤N
2 and
2k â‰¤â„“â‰¤4k,
W âˆ—
2 (N,2k,â„“) â‰¤
AN
â„“W âˆ—
2 (N âˆ’1,2k,â„“âˆ’1)
B
.
(2.2.40)
Solution Again take an [N,M,2k] code X such that w(x) â‰¡â„“for all x âˆˆX .
Consider the shortening code X on x1 = 1 (cf. Example 2.1.8(v)): it gives a code
of length (N âˆ’1), distance â‰¥2k and constant weight (â„“âˆ’1). Hence, the size of
the cross-section is â‰¤W âˆ—
2 (N âˆ’1,2k,â„“âˆ’1). Therefore, the number of 1s at position
1 in the codewords of X does not exceed W âˆ—
2 (N âˆ’1,2k,â„“âˆ’1). Repeating this
argument, we obtain that the total number of 1s in all positions is â‰¤NW âˆ—
2 (N âˆ’
1,2k,â„“âˆ’1). But this number equals â„“M, i.e. â„“M â‰¤NW âˆ—
2 (N âˆ’1,2k,â„“âˆ’1). The
bound (2.2.40) then follows.
Corollary 2.2.15
For all positive integers N â‰¥1, k â‰¤N
2 and 2k â‰¤â„“â‰¤4k âˆ’2,
W âˆ—
2 (N,2k âˆ’1,â„“) = W âˆ—
2 (N,2k,â„“)
â‰¤
AN
â„“
AN âˆ’1
â„“âˆ’1
A
Â·Â·Â·
AN âˆ’â„“+k
k
B
Â·Â·Â·
BBB
.
(2.2.41)
The remaining part of Section 2.2 focuses on the Johnson bound. This bound
aims at improving the binary Hamming bound (cf. (2.1.8b) with qâˆ’2):
Mâˆ—
2(N,2E +1) â‰¤2N
vN(E) or vN(E)Mâˆ—
2(N,2E +1) â‰¤2N.
(2.2.42)
Namely, the Johnson bound asserts that
Mâˆ—
2(N,2E +1) â‰¤2N/vâˆ—
N(E) or vâˆ—
N(E)Mâˆ—
2(N,2E +1) â‰¤2N,
(2.2.43)
where
vâˆ—
N(E) = vN(E)+
1
âŒˆN/(E +1)âŒ‰
 N
E +1

âˆ’W âˆ—
2 (N,2E +1,2E +1)
2E +1
E

.
(2.2.44)

182
Introduction to Coding Theory
Recall that vN(E) =
âˆ‘
0â‰¤sâ‰¤E
N
s

stands for the volume of the binary Hamming ball
of radius E. We begin our derivation of bound (2.2.43) with the following result.
Lemma 2.2.16
If x,y are binary words, with Î´(x,y) = 2â„“+ 1, then there exists
2â„“+1
â„“

binary words z such that Î´ (x,z) = â„“+1 and Î´ (y,z) = â„“.
Proof
Left as an exercise.
Consider the set T (= TN,E+1) of all binary N-words at distance exactly E + 1
from the codewords from X :
T =
(
z âˆˆHN : Î´(z,x) = E +1 for some x âˆˆX
and Î´(z,y) â‰¥E +1 for all y âˆˆX
)
.
(2.2.45)
Then we can write that
MvN(E)+â™¯T â‰¤2N,
(2.2.46)
as none of the words z âˆˆT falls in any of the balls of radius E about the codewords
y âˆˆX . The bound (2.2.43) will follow when we solve the next worked example.
Worked Example 2.2.17
Prove that the cardinality â™¯T is greater than or equal
to the second term from the RHS of (2.2.44):
M
âŒŠN/(E +1)âŒ‹
 N
E +1

âˆ’W âˆ—
2 (N,2E +1,2E +1)
2E +1
E

.
(2.2.47)
Solution We want to ï¬nd a lower bound on â™¯T . Consider the set W (= WN,E+1))
of â€˜matchedâ€™ pairs of N-words deï¬ned by
W =
(
(x,z) : x âˆˆX , z âˆˆTE+1, Î´(x,z) = E +1
)
=
(
(x,z) : x âˆˆX ,z âˆˆHN : Î´(x,z) = E +1,
and Î´(y,z) â‰¥E +1
for all y âˆˆX
)
.
(2.2.48)
Given x âˆˆX , the x-section W x is deï¬ned as
W x = {z âˆˆHN : (x,z) âˆˆW }
= {z : Î´(x,z) = E +1, Î´(y,z) â‰¥E +1
for all y âˆˆX }.
(2.2.49)
Observe that if Î´(x,z) = E +1 then Î´(y,z) â‰¥E for all y âˆˆX \{x}, as otherwise
Î´(x,y) < 2E +1. Hence:
W x = {z : Î´(x,z) = E +1, Î´(y,z) Ì¸= E
for all y âˆˆX }.
(2.2.50)

2.2 A geometric proof of Shannonâ€™s second coding theorem
183
We see that, to evaluate â™¯W x, we must detract, from the number of binary N-
words lying at distance E + 1 from x, i.e. from
 N
E +1

, the number of those
lying also at distance E from some other codeword y âˆˆX . But if Î´(x,z) = E +1
and Î´(y,z) = E then Î´(x,y) = 2E + 1. Also, no two distinct codewords can have
distance E from a single N-word z. Hence, by the previous remark,
â™¯W x =
 N
E +1

âˆ’
2E +1
E

Ã—â™¯
*
y âˆˆX : Î´(x,y) = 2E +1
+
.
Moreover, if we subtract x from every y âˆˆX with Î´(x,y) = 2E + 1, the result
is a code of length N whose codewords z have weight w(z) â‰¡2E + 1. Hence,
there are at most W âˆ—(N,2E +1,2E +1) codewords y âˆˆX with Î´(x,y) = 2E +1.
Consequently,
â™¯W x â‰¥
 N
E +1

âˆ’W âˆ—(N,2E +1,2E +1)
2E +1
E

(2.2.51)
and
â™¯W â‰¥M Ã— the RHS of (2.2.51).
(2.2.52)
Now ï¬x v âˆˆT and consider the v-section
W v = {y âˆˆX : (y,v) âˆˆW } = {y âˆˆX : Î´(y,v) = E +1}.
(2.2.53)
If y,z âˆˆW v then Î´(y,u) = Î´(z,u) = E +1. Thus,
w(yâˆ’u) = w(zâˆ’u) = E +1
and
2E +1 â‰¤Î´(y,z) = Î´(yâˆ’v,zâˆ’v)
= w(yâˆ’v)+w(zâˆ’v)âˆ’2w((yâˆ’v)âˆ§(zâˆ’v))
= 2E +2âˆ’2w((yâˆ’v)âˆ§(zâˆ’v)).
This implies that
w((yâˆ’v)âˆ§(zâˆ’v)) = 0 and Î´(y,z) = 2E +2.
So, yâˆ’v and zâˆ’v have no digit 1 in common. Hence, there exist at most
E
N
E +1
F
words of the form yâˆ’v where y âˆˆW v, i.e. at most
E
N
E +1
F
words in W v. There-
fore,
â™¯W â‰¤
E
N
E +1
F
â™¯T .
(2.2.54)
Collecting (2.2.51), (2.2.52) and (2.2.54) yields inequality (2.2.47).

184
Introduction to Coding Theory
Corollary 2.2.18
In view of Corollary 2.2.15 the following bound holds true:
Mâˆ—(N,2E +1) â‰¤2N

vN(E)
âˆ’
1
âŒŠN/(E +1)âŒ‹
N
E
N âˆ’E
E +1 âˆ’
AN âˆ’E
E +1
Bâˆ’1
.
(2.2.55)
Example 2.2.19
Let N = 13 and E = 2, i.e. d = 5. Inequality (2.2.41) implies
W âˆ—(13,5,5) â‰¤
A13
5
A12
4
A11
3
BBB
= 23, and the Johnson bound in (2.2.43) yields
Mâˆ—(13,5) â‰¤
A
213
1+13+78+(286âˆ’10Ã—23)/4
B
= 77.
This bound is much better than Hammingâ€™s which gives Mâˆ—(13,5) â‰¤89. In fact, it
is known that Mâˆ—(13,5) = 64. Compare Section 3.4.
2.3 Linear codes: basic constructions
In this section we explore further the class of linear codes. To start with, we con-
sider binary codes, with digits 0 and 1. Accordingly, HN will denote the binary
Hamming space of length N; words x(N) = x1 ...xN from HN will be also called
(row) vectors. All operations over binary digits are performed in the binary arith-
metic (that is, mod 2). When it does not lead to a confusion, we will omit sub-
scripts N and superscripts (N). Let us repeat the deï¬nition of a linear code (cf.
Deï¬nition 2.1.5).
Deï¬nition 2.3.1
A binary code X âŠ†HN is called linear if, together with a pair
of vectors, x = x1 ...xN and xâ€² = xâ€²
1 ...xâ€²
N, code X contains the sum x + xâ€², with
digits xi + xâ€²
i. In other words, a linear code is a linear subspace in HN, over ï¬eld
F2 = {0,1}. Consequently, a linear code always contains a zero row-vector 0 =
0...0. A basis of a linear code X is a maximal linearly independent set of words
from X ; the linear code is generated by its basis in the sense that every vector
x âˆˆX is (uniquely) represented as a sum of (some) vectors from the basis. All
bases of a given linear code X contain the same number of vectors; the number of
vectors in the basis is called the dimension or the rank of X . A linear code of length
N and rank k is also called an [N,k] code, or an [N,k,d] code if its distance is d.
Practically all codes used in modern practice are linear. They are popular because
they are easy to work with. For example, to identify a linear code it is enough to
ï¬x its basis, which yields a substantial economy as the subsequent material shows.
Lemma 2.3.2
Any binary linear code of rank k contains 2k vectors, i.e. has size
M = 2k.

2.3 Linear codes: basic constructions
185
Proof
A basis of the code contains k linearly independent vectors. The code is
generated by the basis; hence it consists of the sums of basic vectors. There are
precisely 2k sums (the number of subsets of {1,...,k} indicating the summands),
and they all give different vectors.
Consequently, a binary linear code X of rank k may be used for encoding all
possible source strings of length k; the information rate of a binary linear [N,k]
code is k/N. Thus, indicating k â‰¤N linearly independent words x âˆˆHN identiï¬es
a (unique) linear code X âŠ‚HN of rank k. In other words, a linear binary code of
rank k is characterised by a k Ã— N matrix of 0s and 1s with linearly independent
rows:
G =
â›
âœ
âœ
âœ
â
g11
...
...
...
g1N
g21
...
...
...
g2N
...
...
gk1
...
...
...
gkN
â
âŸ
âŸ
âŸ
â 
Namely, we take the rows g(i) = gi1 ...giN, 1 â‰¤i â‰¤k, as the basic vectors of a
linear code.
Deï¬nition 2.3.3
A matrix G is called a generating matrix of a linear code. It is
clear that the generating matrix is not unique.
Equivalently, a linear [N,k] code X may be described as the kernel of a certain
(N âˆ’k)Ã—N matrix H, again with the entries 0 and 1: X = kerH where
H =
â›
âœ
âœ
âœ
â
h11
h12
...
...
h1N
h21
h22
...
...
h2N
...
...
...
...
...
h(Nâˆ’k)1
h(Nâˆ’k)2
...
...
h(Nâˆ’k)N
â
âŸ
âŸ
âŸ
â 
and
kerH =
(
x = x1 ...xN : xHT = 0(Nâˆ’k))
.
(2.3.1)
It is plain that the rows h( j), 1 â‰¤j â‰¤N âˆ’k, of matrix H are vectors orthogonal to
X , in the sense of the inner dot-product:
âŸ¨xÂ·h( j)âŸ©= 0,
for all x âˆˆX and 1 â‰¤j â‰¤N âˆ’k.
Here, for x,y âˆˆHN,
âŸ¨xÂ·yâŸ©= âŸ¨yÂ·xâŸ©=
N
âˆ‘
i=1
xiyi, where x = x1 ...xN, y = y1 ...yN;
(2.3.2)
cf. Example 2.1.8(ix).

186
Introduction to Coding Theory
The inner product (2.3.2) possesses all properties of the Euclidean scalar product
in RN, but one: it is not positive deï¬nite (and therefore does not deï¬ne a norm). That
is, there are non-zero vectors x âˆˆHN with âŸ¨xÂ·xâŸ©= 0. Luckily, we do not need the
positive deï¬niteness.
However, the key rankâ€“nullity property holds true for the dot-product: if L is
a linear subspace in HN of rank k then its orthogonal complement L âŠ¥(i.e. the
collection of vectors z âˆˆHN such that âŸ¨xÂ·zâŸ©= 0 for all x âˆˆL ) is a linear subspace
of rank N âˆ’k. Thus, the (N âˆ’k) rows of H can be considered as a basis in X âŠ¥,
the orthogonal complement to X .
The matrix H (or sometimes its transpose HT) with the property X = kerH or
âŸ¨xÂ·h( j)âŸ©â‰¡0 (cf. (2.3.1)) is called a parity-check (or, simply, check) matrix of code
X . In many cases, the description of a code by a check matrix is more convenient
than by a generating one.
The parity-check matrix is again not unique as the basis in X âŠ¥can be chosen
non-uniquely. In addition, in some situations where a family of codes is consid-
ered, of varying length N, it is more natural to identify a check matrix where the
number of rows can be greater than N âˆ’k (but some of these rows will be linearly
dependent); such examples appear in Chapter 3. However, for the time being we
will think of H as an (N âˆ’k)Ã—N matrix with linearly independent rows.
Worked Example 2.3.4
Let X be a binary linear [N,k,d] code of information
rate Ï = k/N. Let G and H be, respectively, the generating and parity-check matri-
ces of X . In this example we refer to constructions introduced in Example 2.1.8.
(a) The parity-check extension of X is a binary code X + of length N+1 obtained
by adding, to each codeword x âˆˆX , the symbol xN+1 =
âˆ‘
1â‰¤iâ‰¤N
xi so that the
sum
âˆ‘
1â‰¤iâ‰¤N+1
xi is zero. Prove that X + is a linear code and ï¬nd its rank and
minimal distance. How are the information rates and generating and parity-
check matrices of X and X + related?
(b) The truncation X âˆ’of X is deï¬ned as a linear code of length N âˆ’1 obtained
by omitting the last symbol of each codeword x âˆˆX . Suppose that code X
has distance d â‰¥2. Prove that X âˆ’is linear and ï¬nd its rank and generating
and parity-check matrices. Show that the minimal distance of X âˆ’is at least
d âˆ’1.
(c) The m-repetition of X is a code X re(m) of length Nm obtained by repeat-
ing each codeword x âˆˆX a total of m times. Prove that X re(m) is a linear
code and ï¬nd its rank and minimal distance. How are the information rates and
generating and parity-check matrices of X re(m) related to Ï, G and H?

2.3 Linear codes: basic constructions
187
Solution (a) The generating and parity-check matrices are
G+ =
â›
âœ
âœ
âœ
âœ
âœ
âœ
â
G
|
âˆ‘
1â‰¤iâ‰¤N
g1i
|
...
|
...
|
âˆ‘
1â‰¤iâ‰¤N
gki
â
âŸ
âŸ
âŸ
âŸ
âŸ
âŸ
â 
,H+ =
â›
âœ
âœ
âœ
âœ
âœ
âœ
âœ
âœ
âœ
â
H
âˆ’âˆ’âˆ’
0 ... 0
| 1
| 1
| Â·
| Â·
| 1
| âˆ’âˆ’
| 1
â
âŸ
âŸ
âŸ
âŸ
âŸ
âŸ
âŸ
âŸ
âŸ
â 
.
The rank of X + equals the rank of X = k. If the minimal distance of X was even
it is not changed; if odd it increases by 1. The information rate Ï+ = (N âˆ’1)Ï

N.
(b) The generating matrix
Gâˆ’=
â›
âœ
âœ
âœ
âœ
âœ
âœ
âœ
â
g11
...
g1Nâˆ’1
...
...
...
gk1
...
gkNâˆ’1
â
âŸ
âŸ
âŸ
âŸ
âŸ
âŸ
âŸ
â 
.
The parity-check matrix H of X , after suitable column operations, may be written
as
H =
â›
âœ
âœ
âœ
âœ
âœ
âœ
âœ
âœ
âœ
â
Hâˆ’
âˆ’âˆ’âˆ’âˆ’
0 ... 0
| Â·
| Â·
| Â·
| Â·
| Â·
|âˆ’âˆ’
| â‹†
â
âŸ
âŸ
âŸ
âŸ
âŸ
âŸ
âŸ
âŸ
âŸ
â 
.
The parity-check matrix of X âˆ’is then identiï¬ed with Hâˆ’. The rank is unchanged;
the distance may decrease maximum by 1. The information rate Ïâˆ’= NÏ

(N âˆ’1).
(c) The generating and parity-check matrices are
Gre(m) = (G ... G) (m times),
and
Hre(m) =
â›
âœ
âœ
âœ
âœ
âœ
â
H
0
0
...
0
I
I
0
...
0
I
0
I
...
0
...
...
...
...
...
I
0
0
...
I
â
âŸ
âŸ
âŸ
âŸ
âŸ
â 
.

188
Introduction to Coding Theory
Here, I is a unit N Ã— N matrix and the zeros mean the zero matrices (of size (N âˆ’
k)Ã—N and N Ã—N, accordingly). The number of the unit matrices in the ï¬rst column
equals mâˆ’1. (This is not a unique form of Hre(m).) The size of Hre(m) is (Nmâˆ’
k)Ã—Nm.
The rank is unchanged, the minimal distance in X re(m) is md and the informa-
tion rate Ï/m.
Worked Example 2.3.5
A dual code of a linear binary [N,k] code X is deï¬ned
as the set X âŠ¥of the words y = y1 ... yN such that the dot-product
âŸ¨yÂ·xâŸ©= âˆ‘
1â‰¤iâ‰¤N
yi Â·xi = 0
for every x = x1 ... xN from X .
Compare Example 2.1.8(ix). Prove that an (N âˆ’k)Ã—N matrix H is a parity-check
matrix of code X iff H is a generating matrix for the dual code. Hence, derive that
G and H are generating and parity-check matrices, respectively, for a linear code
iff:
(i) the rows of G are linearly independent;
(ii) the columns of H are linearly independent;
(iii) the number of rows of G plus the number of rows of H equals the number of
columns of G which equals the number of columns of H;
(iv) GHT = 0.
Solution The rows h( j), j = 1,...,N âˆ’k, of the matrix H obey âŸ¨x Â· h( j)âŸ©â‰¡0,
x âˆˆX . Furthermore, if a vector y obeys âŸ¨x Â· yâŸ©â‰¡0, x âˆˆX , then y is a linear
combination of the y( j). Hence, H is a generating matrix of X âŠ¥. On the other
hand, any generating matrix of X âŠ¥is a parity-check matrix for X .
Therefore, for any pair G, H representing generating and parity-check matrices
of a linear code, (i), (ii) and (iv) hold by deï¬nition, and (iii) comes from the rankâ€“
nullity formula
N = dim(Row â€“ Range G)+dim(Row â€“ Range H)
that follows from (iv) and the maximality of G and H.
On the other hand, any pair G, H of matrices obeying (i)â€“(iv) possesses the max-
imality property (by virtue of (i)â€“(iii)) and the orthogonality property (iv). Thus,
they are generating and parity-check matrices for X = Row â€“ RangeG.
Worked Example 2.3.6
What is the number of codewords in a linear binary
[N,k] code? What is the number of different bases in it? Calculate the last number
for k = 4. List all bases for k = 2 and k = 3.

2.3 Linear codes: basic constructions
189
Show that the subset of a linear binary code consisting of all words of even
weight is a linear code. Prove that, for d even, if there exists a linear [N,k,d] code
then there exists a linear [N,k,d] code with codewords of even weight.
Solution The size is 2k and the number of different bases 1
k!
kâˆ’1
âˆ
i=0

2k âˆ’2i
. Indeed,
if the l ï¬rst basis vectors are selected, all their 2l linear combinations should be
excluded on the next step. This gives 840 for k = 4, and 28 for k = 3.
Finally, for d even, we can truncate the original code and then use the parity-
check extension.
Example 2.3.7
The binary Hamming [7,4] code is determined by a 3Ã—7 parity-
check matrix. The columns of the check matrix are all non-zero words of length 3.
Using lexicographical order of these words we obtain
HHam
lex
=
â›
â
1
0
1
0
1
0
1
0
1
1
0
0
1
1
0
0
0
1
1
1
1
â
â .
The corresponding generating matrix may be written as
GHam
lex
=
â›
âœ
âœ
â
0 0 1 1 0 0 1
0 1 0 0 1 0 1
0 0 1 0 1 1 0
1 1 1 0 0 0 0
â
âŸ
âŸ
â .
(2.3.3)
In many cases it is convenient to write the check matrix of a linear [N,k] code in
a canonical (or standard) form:
Hcan =

INâˆ’k Hâ€² 
.
(2.3.4a)
In the case of the Hamming [7,4] code it gives
HHam
can =
â›
â
1
0
0
1
1
0
1
0
1
0
1
0
1
1
0
0
1
0
1
1
1
â
â ,
with a generating matrix also in a canonical form:
Gcan =

Gâ€² Ik

;
(2.3.4b)
namely,
GHam
can =
â›
âœ
âœ
â
1 1 0 1 0 0 0
1 0 1 0 1 0 0
1 1 1 0 0 1 0
1 1 1 0 0 0 1
â
âŸ
âŸ
â .

190
Introduction to Coding Theory
Formally, Glex and Gcan determine different codes. However, these codes are
equivalent:
Deï¬nition 2.3.8
Two codes are called equivalent if they differ only in permuta-
tion of digits. For linear codes, equivalence means that their generating matrices
can be transformed into each other by permutation of columns and by row-
operations including addition of columns multiplied by scalars. It is plain that
equivalent codes have the same parameters (length, rank, distance).
In what follows, unless otherwise stated, we do not distinguish between equiva-
lent linear codes.
Remark 2.3.9
An advantage of writing G in a canonical form is that a source
string u(k) âˆˆHk is encoded as an N-vector u(k)Gcan; according to (2.3.4b), the last k
digits in u(k)Gcan form word u(k) (they are called information digits), whereas the
ï¬rst Nâˆ’k are used for the parity-check (and called parity-check digits). Pictorially,
the parity-check digits carry the redundancy that allows the decoder to detect and
correct errors.
Like following life throâ€™ creatures you dissect
You lose it at the moment you detect.
Alexander Pope (1668â€“1744), English poet
Deï¬nition 2.3.10
The weight w(x) of a binary word x = x1 ...xN is the number
of the non-zero digits in x:
w(x) = â™¯{i : 1 â‰¤i â‰¤N, xi Ì¸= 0}.
(2.3.5)
Theorem 2.3.11
(i) The distance of a linear binary code equals the minimal weight of its non-zero
codewords.
(ii) The distance of a linear binary code equals the minimal number of linearly
dependent columns in the check matrix.
Proof
(i) As the code X is linear, the sum x+y âˆˆX for each pair of codewords
x,y âˆˆX . Owing to the shift invariance of the Hamming distance (see Lemma
2.1.1), Î´(x,y) = Î´(0,x + y) = w(x + y) for any pair of codewords. Hence, the
minimal distance of X equals the minimal distance between 0 and the rest of the
code, i.e. the minimal weight of a non-zero codeword from X .
(ii) Let X be a linear code with a parity-check matrix H and minimal distance d.
Then there exists a codeword x âˆˆX with exactly d non-zero digits. Since xHT = 0,

2.3 Linear codes: basic constructions
191
we conclude that there are d columns of H which are linearly dependent (they
correspond to non-zero digits in x). On the other hand, if there exist (dâˆ’1) columns
of H which are linearly dependent then their sum is zero. But that means that there
exists a word y, of weight w(y) = d âˆ’1, such that yHT = 0. Then y must belong to
X which is impossible, since min[w(x) : x âˆˆX ,x Ì¸= 0] = d.
Theorem 2.3.12
The Hamming [7,4] code has minimal distance 3, i.e. it detects
2 errors and corrects 1. Moreover, it is a perfect code correcting a single error.
Proof
For any pair of columns the parity-check matrix Hlex contains their sum to
obtain a linearly dependent triple (viz. look at columns 1, 6, 7). No two columns
are linearly dependent because they are distinct (x+y = 0 means that x = y). Also,
the volume v7(1) equals 1 + 7 = 23, and the code is perfect as its size is 24 and
24 Ã—23 = 27.
The construction of the Hamming [7,4] code admits a straightforward generali-
sation to any length N = 2l âˆ’1; namely, consider a (2l âˆ’1)Ã—l matrix HHam with
columns representing all possible non-zero binary vectors of length l:
HHam =
â›
âœ
âœ
âœ
âœ
âœ
â
1
0
...
0
1
...
1
0
1
...
0
1
...
1
0
0
...
0
0
...
1
...
...
...
...
...
...
1
0
0
...
1
0
...
1
â
âŸ
âŸ
âŸ
âŸ
âŸ
â 
.
(2.3.6)
The rows of HHam are linearly independent, and hence HHam may be considered
as a check matrix of a linear code of length N = 2l âˆ’1 and rank N âˆ’l = 2l âˆ’
1 âˆ’l. Any two columns of HHam are linearly independent but there exist linearly
dependent triples of columns, e.g. x,y and x+ y. Hence, the code X Ham with the
check matrix HHam has a minimal distance 3, i.e. it detects 2 errors and corrects 1.
This code is called the Hamming [2l âˆ’1,2l âˆ’1âˆ’l] code. It is a perfect one-error
correcting code: the volume of the 1-ball v2lâˆ’1(1) equals 1+2l âˆ’1 = 2l, and size
Ã— volume = 22lâˆ’1âˆ’l Ã— 2l = 22lâˆ’1 = 2N. The information rate is 2l âˆ’l âˆ’1
2l âˆ’1
â†’1 as
l â†’âˆ. This proves
Theorem 2.3.13
The above construction deï¬nes a family of [2l âˆ’1,2l âˆ’1 âˆ’
l,3] linear binary codes X Ham
2lâˆ’1 , l = 1,2,..., which are perfect one-error correcting
codes.

192
Introduction to Coding Theory
Example 2.3.14
Suppose that the probability of error in any digit is p â‰ª1,
independently of what occurred to other digits. Then the probability of an error in
transmitting a non-encoded (4N)-digit message is
1âˆ’(1âˆ’p)4N â‰ƒ4Np.
But if we use the [7,4] code, we need to transmit 7N digits. An erroneous trans-
mission requires at least two wrong digits, which occurs with probability
â‰ˆ1âˆ’

1âˆ’
7
2

p2
N
â‰ƒ21Np2 â‰ª4Np.
We see that the extra effort of using 3 check digits in the Hamming code is justiï¬ed.
A standard decoding procedure for linear codes is based on the concepts of coset
and syndrome. Recall that the ML rule decodes a vector y = y1 ...yN by the closest
codeword x âˆˆX .
Deï¬nition 2.3.15
Let X be a binary linear code of length N and w = w1 ...wN
be a word from HN. A coset of X determined by y is the collection of binary
vectors of the form w+x where x âˆˆX . We denote it by w+X .
An easy (and useful) exercise in linear algebra and counting is
Example 2.3.16
Let X be a linear code and w, v be words of length N. Then:
(1) If w is in the coset v+X , then v is in the coset w+X ; in other words, each
word in a coset determines this coset.
(2) w âˆˆw+X .
(3) w and v are in the same coset iff w+v âˆˆX .
(4) Every word of length N belongs to one and only one coset. That is, the cosets
form a partition of the whole Hamming space HN.
(5) All cosets contain the same number of words which equals â™¯X . If the rank
of X is k then there are 2Nâˆ’k different cosets, each containing 2k words. The
code X is itself a coset of any of the codewords.
(6) The coset determined by w+v coincides with the set of elements of the form
x+y, where x âˆˆw+X , y âˆˆX +v.
Now the decoding rule for a linear code: you know the code X beforehand, hence
you can calculate all cosets. Upon receiving a word y, you ï¬nd its coset y+X and
ï¬nd a word w âˆˆy+X of least weight. Such a word is called a leader of the coset
y+X . A leader may not be unique: in that case you have to make a choice among
the list of leaders (list decoding) or refuse to decode and demand a re-transmission.
Suppose you have chosen a leader w. You then decode y by the word
xâˆ—= y+w.
(2.3.7)

2.3 Linear codes: basic constructions
193
Worked Example 2.3.17
Show that word xâˆ—is always a codeword that min-
imises the distance between y and the words from X .
Solution As y and w are in the same coset, y + w âˆˆX (see Example 2.3.16(3)).
All other words from X are obtained as the sums y + v where v runs over coset
y+X . Hence, for any x âˆˆX ,
Î´(y,x) = w(y+x) â‰¥min
vâˆˆy+X w(v) = w(w) = d(y,xâˆ—).
The parity-check matrix provides a convenient description of the cosets y+X .
Theorem 2.3.18
Cosets w + X are in one-to-one correspondence with vectors
of the form yHT: two vectors, y and yâ€² are in the same coset iff yHT = yâ€²HT. In
other words, cosets are identiï¬ed with the rank (or range) space of the parity-check
matrix.
Proof
The vectors y and yâ€² are in the same coset iff y+yâ€² âˆˆX , i.e.
(y+yâ€²)HT = yHT +yâ€²HT = 0, i.e. yHT = yâ€²HT.
In practice, the decoding rule is implemented as follows. Vectors of the form
yHT are called syndromes: for a linear (N,k) code there are 2Nâˆ’k syndromes. They
are all listed in the syndrome â€˜tableâ€™, and for each syndrome a leader of the corre-
sponding coset is calculated. Upon receiving a word y, you calculate the syndrome
yHT and ï¬nd, in the syndrome table, the corresponding leader w. Then follow
(2.3.7): decode y by xâˆ—= y+w.
The procedure described is called syndrome decoding; although it is relatively
simple, one has to write a rather long table of the leaders. Moreover, it is desirable
to make the whole procedure of decoding algorithmically independent on a con-
crete choice of the code, i.e. of its generating matrix. This goal is achieved in the
case of the Hamming codes:
Theorem 2.3.19
For the Hamming code, for each syndrome the leader w is
unique and contains not more than one non-zero digit. More precisely, if the syn-
drome y

HHamT = s gives column i of the check matrix HHam then the leader of
the corresponding coset has the only non-zero digit i.
Proof
The leader minimises the distance between the received word and the code.
The Hamming code is perfect 1-error correcting. Hence, every word is either a
codeword or within distance 1 of a unique codeword. Hence, the leader is unique

194
Introduction to Coding Theory
and contains at most one non-zero digit. If the syndrome yHT = s occupies position
i among the columns of the parity-check matrix then, for word ei = 0 ... 1 0 ... 0
with the only non-zero digit i,
(y+ei)HT = s+s = 0.
That is, (y+ei) âˆˆX and ei âˆˆy+X . Obviously, ei is the leader.
The duals X HamâŠ¥of binary Hamming codes form a particular class, called sim-
plex codes. If X Ham is [2â„“âˆ’1,2â„“âˆ’1âˆ’â„“], its dual (X Ham)âŠ¥is [2â„“âˆ’1,â„“], and the
original parity-check matrix HHam serves as a generating matrix for X HamâŠ¥.
Worked Example 2.3.20
Prove that each non-zero codeword in a binary simplex
code X HamâŠ¥has weight 2â„“âˆ’1 and the distance between any two codewords equals
2â„“âˆ’1. Hence justify the term â€˜simplexâ€™.
Solution If X = X Ham is the binary Hamming [2l âˆ’1,2l âˆ’l âˆ’1] code then the
dual X âŠ¥is [2l âˆ’1,l], and its l Ã—(2l âˆ’1) generating matrix is HHam. The weight of
any row of HHam equals 2lâˆ’1 (and so d(X âŠ¥) = 2lâˆ’1). Indeed, the weight of row
j of HHam equals the number of non-zero vectors of length l with 1 at position j.
This gives 2lâˆ’1 as the weight, as half of all 2l vectors from Hl have 1 at any given
position.
Consider now a general codeword from X HamâŠ¥. It is represented by the sum of
rows j1,..., js of HHam where s â‰¤l and 1 â‰¤j1 < Â·Â·Â· < js â‰¤l. This word again has
weight 2lâˆ’1; this gives the number of non-zero words v = v1 ...vl âˆˆHl,2 such that
the sum vj1 +Â·Â·Â·+vjs = 1. Moreover, 2lâˆ’1 gives the weight of half of all vectors in
Hl,2. Indeed, we require that vj1 +Â·Â·Â·+vjs = 1, which results in 2sâˆ’1 possibilities
for the s involved digits. Next, we impose no restriction on the remaining lâˆ’s digits
which gives 2lâˆ’s possibilities. Then 2sâˆ’1Ã—2lâˆ’s = 2lâˆ’1, as required. So, w(x) = 2lâˆ’1
for all non-zero x âˆˆX âŠ¥. Finally, for any x,xâ€² âˆˆX ,x Ì¸= xâ€², the distance Î´(x,xâ€²) =
Î´(0,x+xâ€²) = w(x+xâ€²) which is always equal to 2lâˆ’1. So, the codewords x âˆˆX âŠ¥
form a geometric pattern of a â€˜simplexâ€™ with 2l â€˜verticesâ€™.
Next, we brieï¬‚y summarise basic facts about linear codes over a ï¬nite-ï¬eld al-
phabet Fq = {0,1,...,q âˆ’1} of size q = ps. We now switch to the notation FÃ—N
q
for the Hamming space HN,q.
Deï¬nition 2.3.21
A q-ary code X âŠ†FÃ—N is called linear if, together with a
pair of vectors, x = x1 ...xN and xâ€² = xâ€²
1 ...xâ€²
N, X contains the linear combinations
Î³ Â·x+Î³â€² Â·xâ€², with digits Î³ Â·xi +Î³â€² Â·xâ€²
i, for all coefï¬cients Î³,Î³â€² âˆˆFq. That is, X is a
linear subspace in FÃ—N. Consequently, as in the binary case, a linear code always
contains the vector 0 = 0...0. A basis of a linear code is again deï¬ned as a maximal
linearly independent set of its words; the linear code is generated by its basis in the

2.3 Linear codes: basic constructions
195
sense that every codevector is (uniquely) represented as a linear combination of
the basis codevectors. The number of vectors in the basis is called, as before, the
dimension or the rank of the code; because all bases of a given linear code contain
the same number of vectors, this object is correctly deï¬ned. As in the binary case,
the linear code of length N and rank k is referred to as an [N,k] code, or an [N,k,d]
code when its distance equals d.
As in the binary case, the minimal distance of a linear code X equals the mini-
mal non-zero weight:
d(X ) = min [w(x) : x âˆˆX , x Ì¸= 0],
where
w(x) = â™¯{ j : 1 â‰¤j â‰¤N, xj Ì¸= 0 in Fq},
x = x1 ...xN âˆˆFÃ—N
q .
(2.3.8)
A linear code X is deï¬ned by a generating matrix G or a parity-check matrix
H. The generating matrix of a linear [N,k] code is a k Ã— N matrix G, with entries
from Fq, whose rows g(i) = gi1 ...giN, 1 â‰¤i â‰¤k, form a basis of X . A parity-
check matrix is an (N âˆ’k)Ã—N matrix H, with entries from Fq, whose rows h( j) =
h j1 ...h jN, 1 â‰¤j â‰¤N âˆ’k, are linearly independent and dot-orthogonal to X : for
all j = 1,...,N âˆ’k and codeword x = x1 ...xN from X ,
âŸ¨xÂ·h( j)âŸ©= âˆ‘
1â‰¤lâ‰¤N
xlh jl = 0.
In other words, all qk codewords of X are obtained as linear combinations of
rows of G. That is, subspace X can be viewed as a result of acting on Hamming
space FÃ—k
q
(of length k) by matrix G: symbolically, X = FÃ—k
q G. This shows how
code X can be used for encoding qk â€˜messagesâ€™ of length k (and justiï¬es the term
information rate for Ï(X ) = k/N). On the other hand, X is determined as the
kernel (the null-space) of HT: X HT = 0. A useful exercise is to check that for
the dual code, X âŠ¥, the situation is opposite: H is a generating matrix and G the
parity-check. Compare with Worked Example 2.3.5.
Of course, both the generating and parity-check matrices of a given code are not
unique, e.g. we can permute rows g( j) of G or perform row operations, replacing
a row by a linear combination of rows in which the original row enters with a non-
zero coefï¬cient. Permuting columns of G gives a different but equivalent code,
whose basic geometric parameters are identical to those of X .
Lemma 2.3.22
For any [N,k] code, there exists an equivalent code whose gener-
ating matrix G has a â€˜canonicalâ€™ form: G =

Gâ€² Ik

where Ik is the identity k Ã— k
matrix and Gâ€² is an k Ã— (N âˆ’k) matrix. Similarly, the parity-check matrix H may
have a standard form which is

INâˆ’k Hâ€² 
.

196
Introduction to Coding Theory
We now discuss the decoding procedure for a general linear code X of rank
k. As was noted before, it may be used for encoding source messages (strings)
u = u1 ...uk of length k. The source encoding u âˆˆFk
q â†’X becomes particularly
simple when the generating and parity-check matrices are used in the canonical (or
standard) form.
Theorem 2.3.23
For any linear code X there exists an equivalent code X â€² with
the generating matrix Gcan and the check matrix Hcan in standard form (2.3.4a),
(2.3.4b) and Gâ€² = âˆ’(Hâ€²)T.
Proof
Assume that code X is non-trivial (i.e. not reduced to the zero word 0).
Write a basis for X and take the corresponding generating matrix G. By perform-
ing row-operations (where a pair of rows i and j are exchanged or row i is replaced
by row i plus row j) we can change the basis, but do not change the code. Our
matrix G contains a non-zero column, say l1: perform row operations to make g1l1
the only non-zero entry in this column. By permuting digits (columns), place col-
umn l1 at position N âˆ’k. Drop row 1 and column N âˆ’k (i.e. the old column l1)
and perform a similar procedure with the rest, ending up with the only non-zero
entry g2l2 in a column l2. Place column l2 at position N âˆ’k + 1. Continue until an
upper triangular k Ã— k submatrix emerges. Further operations may be reduced to
this matrix only. If this matrix is a unit matrix, stop. If not, pick the ï¬rst column
with more than one non-zero entry. Add the corresponding rows from the bottom
to â€˜killâ€™ redundant non-zero entries. Repeat until a unit submatrix emerges. Now a
generating matrix is in a standard form, and new code is equivalent to the original
one.
To complete the proof, observe that matrices Gcan and Hcan ï¬guring in (2.3.4a),
(2.3.4b) with Gâ€² = âˆ’(Hâ€²)T, have k independent rows and N âˆ’k independent
columns, correspondingly. Besides, the k Ã— (N âˆ’k) matrix Gcan(Hcan)T vanishes.
In fact,
(Gcan(Hcan)T)i j = âŸ¨row i of Gâ€²Â· column j of (Hâ€²)T âŸ©= gâ€²
ij âˆ’gâ€²
ij = 0.
Hence, Hcan is a check matrix for Gcan.
Returning to source encoding, select the generating matrix in the canonical form
Gcan. Then, given a string u = u1 ...uk, we set x =
k
âˆ‘
i=1
uigcan(i), where gcan(i) rep-
resents row i of Gcan. The last k digits in x give string u; they are called the infor-
mation digits. The ï¬rst N âˆ’k digits are used to ensure that x âˆˆX ; they are called
the parity-check digits.
The standard form is convenient because in the above representation X = FÃ—kG,
the initial (N âˆ’k) string of each codeword is used for encoding (enabling the

2.3 Linear codes: basic constructions
197
detection and correction of errors), and the ï¬nal k string yields the message from
FÃ—k
q . As in the binary case, the parity-check matrix H satisï¬es Theorem 2.3.11. In
particular, the minimal distance of a code equals the minimal number of linearly
dependent columns in its parity-check matrix H.
Deï¬nition 2.3.24
Given an [N,k] linear q-ary code X with parity-check matrix
H, the syndrome of an N vector y âˆˆFÃ—N
q
is the k vector yHT âˆˆFÃ—k
q , and the syn-
drome subspace is the image FÃ—N
q HT. A coset of X by vector w âˆˆFÃ—N
q
is denoted
by w+X and formed by words of the form w+x where x âˆˆX . All cosets have
the same number of elements equal to qk and partition the whole Hamming space
FÃ—N
q
into qNâˆ’k disjoint subsets; code X is one of them. The cosets are in one-
to-one correspondence with syndromes yHT. The syndrome decoding procedure is
carried as in the binary case: a received vector y is decoded by xâˆ—= y + w where
w is the leader of coset y+X (i.e. the word from y+X with minimum weight).
All drawbacks we had in the case of binary syndrome decoding persist in the
general q-ary case, too (and in fact are more pronounced): the coset tables are
bulky, the leader of a coset may be not unique. However, for q-ary Hamming codes
the syndrome decoding procedure works well, as we will see in Section 2.4.
In the case of linear codes, some of the bounds can be improved (or rather new
bounds can be produced).
Worked Example 2.3.25
Let X be a binary linear [N,k,d] code.
(a) Fix a codeword x âˆˆX with exactly d non-zero digits. Prove that truncating
X on the non-zero digits of x produces a code X â€²
Nâˆ’d of length N âˆ’d, rank
k âˆ’1 and distance dâ€² for some dâ€² â‰¥âŒˆd/2âŒ‰.
(b) Deduce the Griesmer bound improving the Singleton bound (2.1.12):
N â‰¥d +
âˆ‘
1â‰¤â„“â‰¤kâˆ’1
E d
2â„“
F
.
(2.3.9)
Solution (a) Without loss of generality, assume that the non-zero digits in x are
x1 = Â·Â·Â· = xd = 1. Truncating on digits 1,..., d will produce the code X â€²
Nâˆ’d with
the rank reduced by 1. Indeed, suppose that a linear combination of k âˆ’1 vectors
vanishes on positions d + 1,...,N. Then on the positions 1,...,d all the values
equal either 0s or 1s because d is the minimal distance. But the ï¬rst case is im-
possible, unless the vectors are linearly dependent. The second case also leads to
contradiction by adding the string x and obtaining k linearly dependent vectors in
the code X . Next, suppose that X â€² has distance dâ€² <
Ed
2
F
and take yâ€² âˆˆX â€² with
w(yâ€²) =
N
âˆ‘
j=d+1
yâ€²
j = dâ€².

198
Introduction to Coding Theory
x
y
y
x ^ y
y + (x^y)
x + (x^y)
d
Figure 2.5
Let y âˆˆX be an inverse image of yâ€² under truncation. Referring to (2.1.6b), we
write the following property of the binary wedge-product:
w(y) = w(xâˆ§y)+w

y+(xâˆ§y)

â‰¥d.
Consequently, we must have that w(xâˆ§y) > d âˆ’âŒˆd/2âŒ‰. See Figure 2.5.
Then
w(x) = w(xâˆ§y)+w(x+(xâˆ§y)) = d
implies that w(x+(xâˆ§y)) < âŒˆd/2âŒ‰. But this is a contradiction, because
w(x+y) = w(x+(xâˆ§y))+w(y+(xâˆ§y)) < d.
We conclude that d â€² â‰¥âŒˆd/2âŒ‰.
(b) Iterating the argument in (a) yields
N â‰¥d +d1 +Â·Â·Â·+dkâˆ’1,
where dl â‰¥
Edlâˆ’1
2
F
. With
G3
d/2
4
2
H
â‰¥
Ed
4
F
, we obtain that
N â‰¥d +
âˆ‘
1â‰¤â„“â‰¤kâˆ’1
E d
2â„“
F
.
Concluding this section, we provide a speciï¬cation of the GV bound for linear
codes.
Theorem 2.3.26
(Gilbert bound) If q = ps is a prime power then for all integers
N and d such that 2 â‰¤d â‰¤N/2, there exists a q-ary linear [N,k,d] code with
minimum distance â‰¥d provided that
qk â‰¥qN/vN,q(d âˆ’1).
(2.3.10)

2.4 The Hamming, Golay and Reedâ€“Muller codes
199
Proof
Let X be a linear code of maximal rank with distance at least d of maximal
size. If inequality (2.3.10) is violated the union of all Hamming spheres of radius
d âˆ’1 centred on codewords cannot cover the whole Hamming space. So, there
must be a point y that is not in any Hamming sphere around a codeword. Then for
any codeword x and any scalar b âˆˆFq the vectors y and y + b Â· x are in the same
coset by X . Also y+bÂ·x cannot be in any Hamming sphere of radius d âˆ’1. The
same is true for x+bÂ·y because if it were, then y would be in a Hamming sphere
around another codeword. Here we use the fact that Fq is a ï¬eld. Then the vector
subspace spanned by X and y is a linear code larger than X and with a minimal
distance at least d. That is a contradiction, which completes the proof.
For example, let q = 2 and N = 10. Then 25 < v10,2(2) = 56 < 26. Upon taking
d = 3, the Gilbert bound guarantees the existence of a binary [10,5] code with
d â‰¥3.
2.4 The Hamming, Golay and Reedâ€“Muller codes
In this section we systematically study codes with a general ï¬nite alphabet Fq of q
elements which is assumed to be a ï¬eld. Let us repeat that q has to be of the form
ps where p is a prime and s a natural number; the operations of addition (+) and
multiplication (Â·) must also be speciï¬ed. (As was said above, if q = p is prime,
we can think that Fq = {0,1,...,q âˆ’1} and addition and multiplication in Fq are
standard, mod q.) See Section 3.1. Correspondingly, the Hamming space HN,q of
length N with digits from Fq is identiï¬ed, as before, with the Cartesian power FÃ—N
q
and inherits the component-wise addition and multiplication by scalars.
Deï¬nition 2.4.1
Given positive integers q, â„“â‰¥2, set N = qâ„“âˆ’1
qâˆ’1 , k = N âˆ’â„“, and
construct the q-ary [N,k,3] Hamming code X Ham
N,q
with alphabet Fq as follows. (a)
Pick any non-zero q-ary â„“-word h(1) âˆˆHâ„“,q. (b) Pick any non-zero q-ary â„“-word
h(2) âˆˆHâ„“,q that is not a scalar multiple of h(1). (c) Continue: if h(1),...,h(s) is a
collection of q-ary â„“-words selected so far, pick any non-zero vector h(s+1) âˆˆHâ„“,q
which is not a scalar multiple of h(1),...,h(s), 1 â‰¤s â‰¤N âˆ’1. (d) This process ends
up with a selection of N vectors h(1),...,h(N); form an â„“Ã— N matrix HHam with
the columns h(1)T,...,h(N)T. Code X Ham
N,q
âŠ‚FÃ—N
q
is deï¬ned by the parity-check
matrix HHam. [In fact, we deal with the whole family of equivalent codes here,
modulo choices of words h( j), 1 â‰¤j â‰¤N.]
For brevity, we will now write X H and H H (or even simply H when possible)
instead of X Ham
N,q
and HHam. In the binary case (with q = 2), matrix H H is com-
posed by all non-zero binary column-vectors of length â„“. For general q we have

200
Introduction to Coding Theory
to exclude columns that are multiples of each other. To this end, we can choose
as columns all non-zero â„“-words that have 1 in their top-most non-0 component.
Such columns are linearly independent, and their total equals qâ„“âˆ’1
qâˆ’1 . Next, as in
the binary case, one can arrange words with digits from Fq in the lexicographic
order. By construction, any two columns of H H are linearly independent, but there
exist triples of linearly dependent columns. Hence, d

X H
= 3, and X H detects
two errors and corrects one. Furthermore, X H is a perfect code correcting a single
error, as
M(1+(qâˆ’1)N) = qk

1+(qâˆ’1)qâ„“âˆ’1
qâˆ’1

= qk+â„“= qN.
As in the binary case, the general Hamming codes admit an efï¬cient (and el-
egant) decoding procedure. Suppose a parity-check matrix H

= H H
has been
constructed as above. Upon receiving a word y âˆˆFÃ—N
q
we calculate the syn-
drome yHT âˆˆFÃ—â„“
q . If yHT = 0 then y is a codeword. Otherwise, the column-
vector HyT is a scalar multiple of a column h( j) of H: HyT = a Â· h( j), for some
j = 1,...,N and a âˆˆFq \ {0}. In other words, yHT = a Â· e( j)HT where word
e( j) = 0...1...0 âˆˆHN,q (with the jth digit 1, all others 0). Then we decode y
by xâˆ—= yâˆ’aÂ·e( j), i.e. simply change digit yj in y to yj âˆ’a.
Summarising, we have the following
Theorem 2.4.2
The q-ary Hamming codes form a family of
qâ„“âˆ’1
qâˆ’1 , qâ„“âˆ’1
qâˆ’1 âˆ’â„“,3

perfect codes X H
N , for N = qâ„“âˆ’1
qâˆ’1 ,â„“= 1,2,...,
correcting one error, with a decoding rule that changes the digit yj to yj âˆ’a in a
received word y = y1 ...yN âˆˆFÃ—N
q , where 1 â‰¤j â‰¤N, and a âˆˆFq \ {0} are deter-
mined from the condition that HyT = a Â· h( j), the a-multiple of column j of the
parity-check matrix H.
Hamming codes were discovered by R. Hamming and M. Golay in the late
1940s. At that time Hamming, an electrical engineer turned computer scientist
during the Jurassic computers era, was working at Los Alamos (â€œas an intellec-
tual janitorâ€ to local nuclear physicists, in his own words). This discovery shaped
the theory of codes for more than two decades: people worked hard to extend prop-
erties of Hamming codes to wider classes of codes (with variable success). Most
of the topics on codes discussed in this book are related, in one way or another, to
Hamming codes. Richard Hamming was not only an outstanding scientist but also
an illustrious personality; his writings (and accounts of his life) are entertaining
and thought-provoking.

2.4 The Hamming, Golay and Reedâ€“Muller codes
201
Until the late 1950s, the Hamming codes were a unique family of codes exist-
ing in dimensions N â†’âˆ, with â€˜regularâ€™ properties. It was then discovered that
these codes have a deep algebraic background. The development of the algebraic
methods based on these observations is still a dominant theme in modern coding
theory.
Another important example is the four Golay codes (two binary and two ternary).
Marcel Golay (1902â€“1989) was a Swiss electrical engineer who lived and worked
in the USA for a long time. He had an extraordinary ability to â€˜seeâ€™ the discrete
geometry of the Hamming spaces and â€˜guessâ€™ the construction of various codes
without bothering about proofs.
The binary Golay code X Gol
24
is a [24,12] code with the generating matrix G =
(I12|Gâ€²) where I12 is a 12Ã—12 identity matrix, and Gâ€² 
= Gâ€²
(2)

has the following
form:
Gâ€² =
â›
âœ
âœ
âœ
âœ
âœ
âœ
âœ
âœ
âœ
âœ
âœ
âœ
âœ
âœ
âœ
âœ
âœ
âœ
â
0
1
1
1
1
1
1
1
1
1
1
1
1
1
1
0
1
1
1
0
0
0
1
0
1
1
0
1
1
1
0
0
0
1
0
1
1
0
1
1
1
0
0
0
1
0
1
1
1
1
1
1
0
0
0
1
0
1
1
0
1
1
1
0
0
0
1
0
1
1
0
1
1
1
0
0
0
1
0
1
1
0
1
1
1
0
0
0
1
0
1
1
0
1
1
1
1
0
0
1
0
1
1
0
1
1
1
0
1
0
1
0
1
1
0
1
1
1
0
0
1
1
0
1
1
0
1
1
1
0
0
0
1
0
1
1
0
1
1
1
0
0
0
1
â
âŸ
âŸ
âŸ
âŸ
âŸ
âŸ
âŸ
âŸ
âŸ
âŸ
âŸ
âŸ
âŸ
âŸ
âŸ
âŸ
âŸ
âŸ
â 
.
(2.4.1)
The rule of forming matrix Gâ€² is ad hoc (and this is how it was determined by M.
Golay in 1949). There will be further ad hoc arguments in the analysis of Golay
codes.
Remark 2.4.3
Interestingly, there is a systematic way of constructing all code-
words of X Gol
24
(or its equivalent) by ï¬tting together two versions of Hamming [7,4]
code X H
7 . First, observe that reversing the order of all the digits of a Hamming
code X H
7
yields an equivalent code which we denote by X K
7 . Then add a parity-
check to both X H
7
and X K
7 , producing codes X H,+
8
and X K,+
8
. Finally, select
two different words a,b âˆˆX H,+
8
and a word x âˆˆX K,+
8
. Then all 212 codewords
of X Gol
24
of length 24 could be written as concatenation (a+x)(b+x)(a+b+x).
This can be checked by inspection of generating matrices.
Lemma 2.4.4
The binary Golay code X Gol
24
is self-dual, with X GolâŠ¥
24
= X Gol
24 .
The code X Gol
24
is also generated by the matrix  G = (Gâ€²|I12).

202
Introduction to Coding Theory
Proof
A direct calculation shows that any two rows of matrix G are dot-
orthogonal. Thus X Gol
24
âŠ‚X GolâŠ¥
24
. But the dimensions of X Gol
24
and X GolâŠ¥
24
coincide. Hence, X Gol
24
= X GolâŠ¥
24
. The last assertion of the lemma now follows
from the property (Gâ€²)T = Gâ€².
Worked Example 2.4.5
Show that the distance d(X Gol
24 ) = 8.
Solution First, we check that for all x âˆˆX Gol
24
the weight w(x) is divisible by 4 .
This is true for every row of G = (I12|Gâ€²): the number of 1s is either 12 or 8. Next,
for all binary N-words x,xâ€²,
w(x+xâ€²) = w(x)+w(xâ€²)âˆ’2w(xâˆ§xâ€²)
where (x âˆ§y) is the wedge-product, with digits (x âˆ§y) j = min(xj,yj), 1 â‰¤j â‰¤N
(cf. (2.1.6b)). But for any pair g( j), g( jâ€²) of the rows of G, w(g( j) âˆ§g( jâ€²)) = 0
mod 2. So, 4 divides w(x) for all x âˆˆX Gol
24 .
On the other hand, X Gol
24
does not have codewords of weight 4. To prove this,
compare two generating matrices, (I12|Gâ€²) and ((Gâ€²)T|I12). If x âˆˆX Gol
24
has w(x) =
4, write x as a concatenation xLxR. Any non-trivial sum of rows of (I12|Gâ€²) has
the weight of the L-half of at least 1, so w(xL) â‰¥1. Similarly, w(xR) â‰¥1. But if
w(xL) = 1 then x must be one of the rows of (I12|G), none of which has weight
w(xR) = 3. Hence, w(xL) â‰¥2. Similarly, w(xR) â‰¥2. But then the only possibility
is that w(xL) = w(xR) = 2 which is impossible by a direct check. Thus, w(x) â‰¥8.
But (I12|Gâ€²) has rows of weight 8. So, d(X Gol
24 ) = 8.
When we truncate X Gol
24
at any digit, we get X Gol
23 , a [23,12,7] code. This code
is perfect 3 error correcting. We recover X Gol
24
from X Gol
23
by adding a parity-
check.
The Hamming [2â„“âˆ’1,2â„“âˆ’1âˆ’â„“,3] and the Golay [23,12,7] are the only possible
perfect binary linear codes.
The ternary Golay code X Gol
12,3 of length 12 has the generating matrix

I6|Gâ€²
(3)

where
Gâ€²
(3) =
â›
âœ
âœ
âœ
âœ
âœ
âœ
âœ
â
0
1
1
1
1
1
1
0
1
2
2
1
1
1
0
1
2
2
1
2
1
0
1
2
1
2
2
1
0
1
1
1
2
2
1
0
â
âŸ
âŸ
âŸ
âŸ
âŸ
âŸ
âŸ
â 
, with (Gâ€²
(3))T = Gâ€²
(3).
(2.4.2)
The ternary Golay code X Gol
11,3 is a truncation of X Gol
12,3 at the last digit.

2.4 The Hamming, Golay and Reedâ€“Muller codes
203
Theorem 2.4.6
The ternary Golay code X (Gol)âŠ¥
12,3
= X (Gol)
12,3
is [12,6,6]. The code
X (Gol)
11,3
is [11,6,5], hence perfect.
Proof
The code [11,6,5] is perfect since v11,3(2) = 1+11Ã—2+ 11Ã—10
2
Ã—22 =
35. The rest of the assertions of the theorem are left as an exercise.
The Hamming
3â„“âˆ’1
2
,3â„“âˆ’1âˆ’â„“,3

and the Golay [11,6,5] codes are the only
possible perfect ternary linear codes. Moreover, the Hamming and Golay are the
only perfect linear codes, occurring in any alphabet Fq where q = ps is a prime
power. Hence, these codes are the only possible perfect linear codes. And even
non-linear perfect codes do not bring anything essentially new: they all have the
same parameters (length, size and distance) as the Hamming and Golay codes. The
Golay codes were used in the 1980s in the American Voyager spacecraft program,
to transmit close-up photographs of Jupiter and Saturn.
The next popular examples are the Reedâ€“Muller codes. For N = 2m consider
binary Hamming spaces Hm,2 and HN,2. Let M(= Mm) be an mÃ—N matrix where
the columns are the binary representations of the integers j = 0,1,...,N âˆ’1, with
the least signiï¬cant bit in the ï¬rst place:
j = j1 Â·20 + j2 Â·21 +Â·Â·Â·+ jm2mâˆ’1.
(2.4.3)
So,
0
1
2
...
2m âˆ’1
M =
â›
âœ
âœ
âœ
âœ
âœ
â
0
1
0
...
1
0
0
1
...
1
...
...
...
...
...
0
0
0
...
1
0
0
0
...
1
â
âŸ
âŸ
âŸ
âŸ
âŸ
â 
v(1)
v(2)
...
v(mâˆ’1)
v(m)
.
(2.4.4)
The columns of M list all vectors from Hm,2 and the rows are vectors from HN,2
denoted by v(1),...,v(m). In particular, v(m) has the ï¬rst 2mâˆ’1 entries 0, the last
2mâˆ’1 entries 1. To pass from Mm to Mmâˆ’1, one drops the last row and takes one of
the two identical halves of the remaining (m âˆ’1) Ã— N matrix. Conversely, to pass
from Mmâˆ’1 to Mm, one concatenates two copies of Mmâˆ’1 and adds row v(m):
Mm =
 Mmâˆ’1
Mmâˆ’1
0...0
1...1

.
(2.4.5)

204
Introduction to Coding Theory
Consider the columns w(1),...,w(m) of Mm corresponding to numbers
1,2,4,...,2mâˆ’1. They form the standard basis in Hm,2:
1
0
...
0
0
1
...
0
...
...
...
...
0
0
...
1
.
Then the column at position j =
âˆ‘
1â‰¤iâ‰¤m
ji2iâˆ’1 is
âˆ‘
1â‰¤iâ‰¤m
jiw(i).
The vector v(i), i = 1,...,m, can be interpreted as the indicator function of the
set Ai âŠ‚Hm,2 where the ith digit is 1:
Ai = {j âˆˆHm,2 : ji = 1}.
(2.4.6)
In terms of the wedge-product (cf. (2.1.6b)) v(i1) âˆ§v(i2) âˆ§Â·Â·Â·âˆ§v(ik) is the indicator
function of the intersection Ai(1) âˆ©Â·Â·Â·âˆ©Ai(k). If all i1,...,ik are distinct, the cardi-
nality â™¯(âˆ©1â‰¤jâ‰¤kAi( j)) = 2mâˆ’k. In other words, we have the following.
Lemma 2.4.7
The weight w(âˆ§1â‰¤jâ‰¤kv(i j)) = 2mâˆ’k.
An important fact is
Theorem 2.4.8
The vectors v(0) = 11...1 and âˆ§1â‰¤jâ‰¤kv(i j), 1 â‰¤i1 < Â·Â·Â· < ik â‰¤m,
k = 1,...,m, form a basis in HN,2.
Proof
It sufï¬ces to check that the standard basis N-words e( j) = 0...1...0 (1 in
position j, 0 elsewhere) can be written as linear combinations of the above vectors.
But
e( j) = âˆ§1â‰¤iâ‰¤m(v(i) +(1+v(i)
j )v(0)),
0 â‰¤j â‰¤N âˆ’1.
(2.4.7)
[All factors in position j are equal to 1 and at least one factor in any position l Ì¸= j
is equal to 0.]

2.4 The Hamming, Golay and Reedâ€“Muller codes
205
Example 2.4.9
For m = 4, N = 16,
v(0) = 1111111111111111
v(1) = 0101010101010101
v(2) = 0011001100110011
v(3) = 0000111100001111
v(4) = 0000000011111111
v(1) âˆ§v(2) = 0001000100010001
v(1) âˆ§v(3) = 0000010100000101
v(1) âˆ§v(4) = 0000000001010101
v(2) âˆ§v(3) = 0000001100000011
v(2) âˆ§v(4) = 0000000000110011
v(3) âˆ§v(4) = 0000000000001111
v(1) âˆ§v(2) âˆ§v(3) = 0000000100000001
v(1) âˆ§v(2) âˆ§v(4) = 0000000000010001
v(1) âˆ§v(3) âˆ§v(4) = 0000000000000101
v(2) âˆ§v(3) âˆ§v(4) = 0000000000000011
v(1) âˆ§v(2) âˆ§v(3) âˆ§v(4) = 0000000000000001
Deï¬nition 2.4.10
Given 0 â‰¤r â‰¤m, the Reedâ€“Muller (RM) code X RM(r,m)
of order r is a binary code of length N = 2m spanned by all wedge-products
âˆ§1â‰¤jâ‰¤kv(i j) and v(0) where 1 â‰¤k â‰¤r and 1 â‰¤i1 < Â·Â·Â· < ik â‰¤m. The rank of
X RM(r,m) equals 1+
m
1

+Â·Â·Â·+
m
r

.
So, X RM(0,m) âŠ‚X RM(1,m) âŠ‚Â·Â·Â· âŠ‚X RM(m âˆ’1,m) âŠ‚X RM(m,m).
Here X RM(m,m) = HN,2, the whole Hamming space, and X RM(0,m) =
{00...00,11...1}, the repetition code. Next, X RM(mâˆ’1,m) consists of all words
x âˆˆHN,2 of even weight (shortly: even words). In fact, any basis vector is even, by
Lemma 2.4.7. Further, if x,xâ€² are even then
w(x+xâ€²) = w(x)+w(xâ€²)âˆ’2w(xâˆ§xâ€²)
is again even. Hence, all codewords x âˆˆX RM(m âˆ’1,m) are even. Finally,
dimX RM(mâˆ’1,m) = N âˆ’1 coincides with the dimension of the subspace of even
words. This proves the claim. As X RM(r,m) âŠ‚X RM(m âˆ’1,m), r â‰¤m âˆ’1, any
RM code consists of even words.

206
Introduction to Coding Theory
The dual code is X RM(r,m)âŠ¥= X RM(mâˆ’râˆ’1,m). Indeed, if a âˆˆX RM(r,m),
b âˆˆX RM(mâˆ’r âˆ’1,m) then the wedge-product aâˆ§b is an even word, and hence
the dot-product âŸ¨aÂ·bâŸ©= 0. But
dim(X RM(r,m))+dim(X RM(mâˆ’r âˆ’1,m)) = N,
hence the claim. As a corollary, code X RM(mâˆ’2,m) is the parity-check extension
of the Hamming code.
By deï¬nition, codewords x âˆˆX RM(r,m) are associated with âˆ§-polynomials in
idempotent â€˜variablesâ€™ v(1),...,v(m), with coefï¬cients 0,1, of degrees â‰¤r (here,
the degree of a polynomial is counted by taking the maximal number of variables
v(1),...,v(m) in the summand monomials). The 0-degree monomial in such a poly-
nomial is proportional to v(0).
Write this correspondence as
x âˆˆX RM(r,m) â†”px(v(1),...,v(m)), deg px â‰¤r.
(2.4.8)
Each such polynomial can be written in the form
px(v(1),...,v(m)) = v(m) âˆ§q(v(1),...,v(mâˆ’1))+l(v(1),...,v(mâˆ’1)),
with degq â‰¤r âˆ’1,degl â‰¤r. The word v(m) âˆ§q(v(1),...,v(mâˆ’1)) has zeros on the
ï¬rst 2mâˆ’1 positions.
By the same token, as above,
q(v(1),...,v(mâˆ’1))
â†”
b âˆˆX RM(r âˆ’1,mâˆ’1),
l(v(1),...,v(mâˆ’1))
â†”
a âˆˆX RM(r,mâˆ’1).
(2.4.9)
Furthermore, 2m-word x can be written as the sum of concatenated 2mâˆ’1-words:
x = (a|a)+(0|b) = (a|a+b).
(2.4.10)
This means that the Reedâ€“Muller codes are related via the bar-product construction
(cf. Example 2.1.8(viii)):
X RM(r,m) = X RM(r,mâˆ’1)|X RM(r âˆ’1,mâˆ’1).
(2.4.11)
Therefore, inductively,
d(X RM(r,m)) = 2mâˆ’r.
(2.4.12)
In fact, for m = r = 0, d(X RM(0,0)) = 2m and for all m, d(X RM(m,m)) =
1 = 20.
Assume
d(X RM(r âˆ’1,  m)) = 2 mâˆ’r+1
for
all
 m â‰¥r âˆ’1,
and
d(X RM(r âˆ’1,mâˆ’1)) = 2mâˆ’r. Then (cf. (2.4.14) below)
d(X RM(r,m))
=
min[2d(X RM(r,mâˆ’1)),d(X RM(r âˆ’1,mâˆ’1))]
=
min[2Â·2mâˆ’1âˆ’r,2mâˆ’1âˆ’r+1] = 2mâˆ’r.
(2.4.13)

2.4 The Hamming, Golay and Reedâ€“Muller codes
207
Summarising,
Theorem 2.4.11
The RM code X RM(r,m), 0 â‰¤r â‰¤m, is a binary code of length
N = 2m, rank k =
âˆ‘
0â‰¤lâ‰¤r
N
l

and distance d = 2mâˆ’r. Furthermore,
(1) X RM(0,m) = {0...0,1...1} âŠ‚X RM(1,m) âŠ‚Â·Â·Â· âŠ‚X RM(mâˆ’1,m) âŠ‚
X RM(m,m) = HN,2; X RM(mâˆ’1,m) is the set of all even N-words
and X RM(mâˆ’2,m) the parity-check extension of the Hamming [2m âˆ’1,
2m âˆ’1âˆ’m] code.
(2) X RM(r,m) = X RM(r,mâˆ’1)|X RM(r âˆ’1,mâˆ’1), 1 â‰¤r â‰¤mâˆ’1.
(3) X RM(r,m)âŠ¥= X RM(mâˆ’r âˆ’1,m), 0 â‰¤r â‰¤mâˆ’1.
Worked Example 2.4.12
Deï¬ne the bar-product X1|X2 of binary linear codes
X1 and X2, where X2 is a subcode of X1. Relate the rank and minimum distance
of X1|X2 to those of X1 and X2. Show that if X âŠ¥denotes the dual code of X ,
then
(X1|X2)âŠ¥= X âŠ¥
2 |X âŠ¥
1 .
Using the bar-product construction, or otherwise, deï¬ne the Reedâ€“Muller code
X RM(r,m) for 0 â‰¤r â‰¤m. Show that if 0 â‰¤r â‰¤mâˆ’1, then the dual of X RM(r,m)
is again a Reedâ€“Muller code.
Solution The bar-product X1|X2 of two linear codes X1 âŠ†X2 âŠ†FN
2 is deï¬ned as
X1|X2 =
*
(x|x+y) : x âˆˆX1, y âˆˆX2
+
;
it is a linear code of length 2N. If X1 has basis x1,...,xk and X2 basis y1,...,yl
then X1|X2 has basis
(x1|x1),...,(xk|xk),(0,y1),...,(0|yl),
and the rank of X1|X2 equals the sum of ranks of X1 and X2.
Next, we are going to check that the minimum distance
d

X1|X2

= min
	
2d(X1),d(X2)

.
(2.4.14)
Indeed, let 0 Ì¸= (x|x+y) âˆˆX1|X2. If y Ì¸= 0 then the weight w(x|x+y) â‰¥w(y) â‰¥
d(X2). If y = 0 then w(x|x+y) = 2w(x) â‰¥2d(X1). This implies that
d

X1|X2

â‰¥min
	
2d(X1),d(X2)

.
(2.4.15)
On the other hand, if x âˆˆX1 has w(x) = d(X1) then d

X1|X2

â‰¤w(x|x) =
2d(X1). Finally, if y âˆˆX2 has w(y) = d(X2) then d

X1|X2

â‰¤w(0|y) = d(X2).
We conclude that
d

X1|X2

â‰¤min
	
2d(X1),d(X2)

,
(2.4.16)
proving (2.4.14).

208
Introduction to Coding Theory
Now, we will check that

X âŠ¥
2 |X âŠ¥
1

âŠ†(X1|X2)âŠ¥.
Indeed, let (u|u+v) âˆˆX âŠ¥
2 |X âŠ¥
1 and (x|x+y) âˆˆ(X1|X2). The dot-product
I
(u|u+v)Â·(x|x+y)
J
= uÂ·x+(u+v)Â·(x+y)
= uÂ·y+vÂ·(x+y) = 0,
since u âˆˆX âŠ¥
2 , y âˆˆX2, v âˆˆX âŠ¥
1 and (x+y) âˆˆX1. In addition, we know that
rank

X âŠ¥
2 |X âŠ¥
1

= N âˆ’rank(X2)+N âˆ’rank(X1)
= 2N âˆ’rank(X1|X2) = rank(X1|X2)âŠ¥.
This implies that in fact

X âŠ¥
2 |X âŠ¥
1

= (X1|X2)âŠ¥.
(2.4.17)
Turning to the RM codes, they are determined as follows:
X RM(0,m) = the repetition binary code of length N = 2m,
X RM(m,m) = the whole space HN,2 of length N = 2m,
X RM(r,m) for 0 < r < m is deï¬ned recursively by
X RM(r,m) = X RM(r,mâˆ’1)|X (r âˆ’1,mâˆ’1).
By construction, X RM(r,m) has rank
r
âˆ‘
j=0
m
j

and the minimum distance 2mâˆ’r.
In particular, X RM(mâˆ’1,m) is the parity-check code and hence dual of X (0,m).
We will show that in general, for 0 â‰¤r â‰¤mâˆ’1,
X RM(r,m)
âŠ¥= X RM(mâˆ’r âˆ’1,m).
It is done by induction in m â‰¥3. By the above, we can assume that
X RM(r,mâˆ’1)âŠ¥= X RM(m âˆ’r âˆ’2,m âˆ’1) holds for 0 â‰¤r < m âˆ’1. Then for
0 â‰¤r < m:
X RM(r,m)
âŠ¥=

X RM(r,mâˆ’1)|X RM(r âˆ’1,mâˆ’1)
âŠ¥
= X RM(r âˆ’1,mâˆ’1)
âŠ¥|X RM(r,mâˆ’1)
âŠ¥
= X RM(mâˆ’r âˆ’1,mâˆ’1)|X RM(mâˆ’r âˆ’2,mâˆ’1)
= X RM(mâˆ’r âˆ’1,m).

2.4 The Hamming, Golay and Reedâ€“Muller codes
209
Encoding and decoding of RM codes is based on the following observation. By
virtue of (2.4.5), the product v(i1) âˆ§Â·Â·Â· âˆ§v(ik) occurs in the expansion for e( j) âˆˆ
Hm,2 iff v(i)
j = 0 for all i /âˆˆ{i1,...,ik}.
Deï¬nition 2.4.13
For 1 â‰¤i1 < Â·Â·Â· < ik â‰¤m, deï¬ne
C(i1,...,ik) := the set of all integers j =
âˆ‘
1â‰¤iâ‰¤m
ji2iâˆ’1
with ji = 0 for i /âˆˆ{i1,...,ik}.
(2.4.18)
For an empty set (k = 0), C(/0) = {1,...,2m âˆ’1}. Furthermore, set
C(i1,...,ik)+t = { j +t : j âˆˆC(i1,...,ik)}.
(2.4.19)
Then, again in view of (2.4.5), for all y = y0 ...yNâˆ’1 âˆˆHN,2,
y = âˆ‘
0â‰¤kâ‰¤m
âˆ‘
1â‰¤i1<Â·Â·Â·<ikâ‰¤m

âˆ‘
jâˆˆC(i1,...,ik)
yj

v(i1) âˆ§Â·Â·Â·âˆ§v(ik)
(2.4.20)
(for k = 0, take v(0)).
For encoding a sequence a = a0 ...akâˆ’1 of information symbols from Hk,2,
with k = 1 +
 m
1

+ Â·Â·Â· +
 m
r

, with X RM
r,m , rewrite it as (ai1,...,iâ„“); here
i1,...,il are the successive positions of the 1s. Then construct a codeword as
x = (x0,...,xNâˆ’1) âˆˆX RM
r,m where
x = âˆ‘
0â‰¤lâ‰¤r
âˆ‘
1â‰¤i1<Â·Â·Â·<ilâ‰¤m
ai1,...,ilv(i1) âˆ§Â·Â·Â·âˆ§v(il).
(2.4.21)
We see that the â€˜information spaceâ€™ Hk,2 is embedded into HN,2, by identifying
entries a j âˆ¼ai1,...,il where j = j020 + j121 + Â·Â·Â· + jmâˆ’12mâˆ’1 and i1,...,il are the
successive positions of the 1s among j1,..., jm, 1 â‰¤l â‰¤r. With such an identiï¬ca-
tion we obtain:
Lemma 2.4.14
For all 0 â‰¤l â‰¤m and 1 â‰¤i1 < Â·Â·Â· < il â‰¤m,
âˆ‘
jâˆˆC(i1,...,il)
xj
=
ai1,...,il, if l â‰¤r,
=
0, if l > r.
(2.4.22)
Proof
The result follows from (2.4.20).
Lemma 2.4.15
For all 1 â‰¤i1 < Â·Â·Â· < ir â‰¤m and for any 1 â‰¤t â‰¤m such that
t /âˆˆ{i1,...,ir},
ai1,...,ir =
âˆ‘
jâˆˆC(i1,...,ir)+2tâˆ’1
xj.
(2.4.23)

210
Introduction to Coding Theory
Proof
The proof follows from the fact that C(i1,...,ir,t) is the disjoint union
C(i1,...,ir)âˆª(C(i1,...,ir)+2tâˆ’1) and the equation
âˆ‘
jâˆˆC(i1,...,ir,t)
xj = 0 (cf. (2.4.19)).
Moreover:
Theorem 2.4.16
For any information symbol ai1,...,ir corresponding to v(i1,...,ir),
we can split the set {0,...,N âˆ’1} into 2mâˆ’r disjoint subsets S, each containing 2r
elements, such that, for all such S, ai1,...,ir = âˆ‘
jâˆˆS
xj.
Proof
The list of sets S begins with C(i1,...,ir) and continues with (m âˆ’r) dis-
joint sets C(i1,...,ir) + 2tâˆ’1 where 1 â‰¤t â‰¤m, t Ì¸âˆˆ{i1,...,ir}. Next, we take any
pair 1 â‰¤t1 < t2 â‰¤m such that {t1,t2}âˆ©{i1,...,ir} = /0. Then C(i1,...,ir,t1,t2) con-
tains disjoint sets C(i1,...,ir), C(i1,...,ir)+2t1âˆ’1 and C(i1,...,ir)+2t2âˆ’1, and for
each of them, ai1,...,ir =
âˆ‘
jâˆˆC(i1,...,ir)+2tkâˆ’1 xj,k = 1,2. Then the same is true for the
remaining sets
C(i1,...,ir)+2t1âˆ’1 +2t2âˆ’1 = C(i1,...,ir,t1,t2)\

C(i1,...,ir)âˆª

C(i1,...,ir)+2t1âˆ’1
âˆª

C(i1,...,ir)+2t2âˆ’1
;
(2.4.24)
there are (mâˆ’r
2
) of them and they are still disjoint with each other and the previous
ones. The sets (2.4.24) form a further bunch of sets S.
And so on: a general form of set S is
C(i1,...,ir)+2t1âˆ’1 +Â·Â·Â·+2tsâˆ’1
which is the same as the set-theoretic difference
C(i1,...,ir,t1,...,ts)
\

2
{tâ€²
1,...,tâ€²
sâ€²}âŠ‚{t1,...ts}

C(i1,...,ir)+2tâ€²
1âˆ’1 +Â·Â·Â·+2tâ€²
sâ€²âˆ’1
.
(2.4.25)
Here each such set is labelled by a collection {t1,...,ts} where 0 â‰¤s â‰¤mâˆ’r, t1 <
Â·Â·Â· < ts and {t1,...,ts} âˆ©{i1,...,ir} = /0. [The union âˆª{tâ€²
1,...,tâ€²
sâ€²}âŠ‚{t1,...ts} in (2.4.25)
is over all (â€˜strictâ€™) subsets {tâ€²
1,...,tâ€²
sâ€²} of {t1,...,ts}, with tâ€²
1 < Â·Â·Â· < tâ€²
sâ€² and sâ€² =
0,...,s âˆ’1 (sâ€² = 0 gives the empty subset).] The total number of sets C(i1,...,ir)
equals 2mâˆ’r and each of them has 2r elements by construction.
Theorem 2.4.16 provides a rationale for the so-called majority decoding for the
Reedâ€“Muller codes. Namely, upon receiving a word y = (y0,...,yNâˆ’1), produced
from a codeword xâˆ§âˆˆX RM
r,m , we take any 1 â‰¤i1 < Â·Â·Â· < ir â‰¤m and consider
the sums âˆ‘
jâˆˆC
yj along the 2mâˆ’r above sets S. If y âˆˆX RM
r,m , all these sums coincide

2.4 The Hamming, Golay and Reedâ€“Muller codes
211
and give ai1,...,ir. If the number of errors in y (i.e. the Hamming distance Î´(xâˆ§,y))
< 2mâˆ’râˆ’1 = d

X RM
r,m

2, the majority of sums will still give a correct ai1,...,ir (the
worst case is where each set S contains no or a single error). By varying {i1,...,ir},
we will determine a codeword x(1) âˆˆX RM
r,m containing only monomials of degree
r. Note that xâˆ§âˆ’x(1) will be a codeword in X RM
râˆ’1,m.
Then y can be â€˜reducedâ€™ to y âˆ’x(1). Compared with xâˆ§âˆ’x(1), the reduced
word yâˆ’x(1) will have Î´(xâˆ§âˆ’x(1),yâˆ’x(1)) = Î´(xâˆ§,y) errors, which is < 2mâˆ’r =
d(X RM
râˆ’1,m)
>
2. We can repeat the above procedure and obtain the correct ai1,...,irâˆ’1
for any 1 â‰¤i1 < Â·Â·Â· < irâˆ’1 â‰¤m, etc. At the end, we recover the whole sequence of
information symbols ai1,...,ir.
Therefore, any word y âˆˆHN,2 with distance Î´

y,X RM
r,m

< d

X RM
r,m

2 is
uniquely decoded.
. . . correct, insert, reï¬ne,
enlarge, diminish, interline.
Jonathan Swift (1667â€“1745), Angloâ€“Irish writer
Reedâ€“Muller codes were discovered at the beginning of the 1950s by David
Muller (1924â€“2008); Irwin Reed (1923â€“2012) proposed the above decoding pro-
cedure. In the early 1970s, the RM codes were used to transmit pictures from
space (as far as the Moon) by the spacecrafts. The quality of transmission was then
considered as exceptionally good. However, later on, NASA engineers decided in
favour of the Golay codes while photographing Jupiter and Saturn.
Worked Example 2.4.17
A maximum distance separable (MDS) code was de-
ï¬ned earlier as a q-ary linear [N,k,d] code with d = N âˆ’k + 1 (equality in the
Singleton bound; see Deï¬nition 2.1.13).
(a) Prove that X is MDS iff
(i) any N âˆ’k columns of its parity-check matrix H are linearly independent,
and
(ii) there exist N âˆ’k +1 columns of H that are linearly dependent.
(b) Prove that the dual of an MDS code is MDS and deduce that X is MDS iff
any k columns of its generating matrix G are linearly independent and k is the
maximal such number.
(c) Hence prove that when G is written in the standard form (Ik|Gâ€²) then X is
MDS iff any square sub-matrix of Gâ€² is non-singular.
(d) Finally, check that [N,k,d] code X is MDS iff for any d positions 1 â‰¤i1 <
Â·Â·Â· < id â‰¤N, there exists a codeword of weight d with non-zero digits at digits
i1,...,id.

212
Introduction to Coding Theory
Solution (a) An MDS [N,k,d] code has d = N âˆ’k + 1. If a linear code X has
d(X ) = d then any (d âˆ’1) columns of its parity-check matrix H are linearly in-
dependent, and (d âˆ’1) is the maximal number with this property, and vice versa.
So, any (N âˆ’k) columns are linearly independent and (N âˆ’k) is the maximal such
number, and vice versa. Equivalently, any (N âˆ’k) Ã— (N âˆ’k) submatrix of H is
invertible.
(b) Let X be [N,k,d] MDS code with a parity-check matrix H. Then H is a gen-
erating matrix for X âŠ¥. Any (N âˆ’k)Ã—(N âˆ’k) submatrix of H is invertible. Then
any non-trivial combination of rows of HâŠ¤has â‰¤N âˆ’kâˆ’1 zero entries, i.e. weight
â‰¥k+1; the minimal weight is equal to k+1. So, d(X âŠ¥) = k+1 = Nâˆ’(Nâˆ’k)+1.
As X âŠ¥is [N,N âˆ’k] code, it is MDS.
Then, clearly, [N,k] code X is MDS iff k is the maximal number l such that any
l columns of its generating matrix G are linearly independent. Equivalently, X is
systematic on any k positions.
(c) Again, let X be [N,k,d] MDS code, and write G = (Ik|Gâ€²). Take a (u Ã— u)
submatrix  Gu of Gâ€². By using row and column permutations, we may assume that
 Gu occupies the top left corner in Gâ€². Then consider the last (k âˆ’u) columns of Ik
and u columns of Gâ€² containing  Gu; the corresponding kÃ—k matrix is non-singular
and forms a k Ã—k submatrix Gk,
Gk =

0
 Gu
Ikâˆ’u
âˆ—

,
with
detGk = Â±det  Gu detIkâˆ’u = Â±det  Gu Ì¸= 0, by (b).
So,  Gu is non-singular. The proof of the inverse statement is similar.
(d) Finally, choose d = N âˆ’k + 1 digits, say i1,...,id. Consider i1 together with
the remaining digits j1,..., jkâˆ’1. Then i1, j1,..., jkâˆ’1 are information symbols. So,
there exists a codeword x with digit i1 non-zero and digits j1,..., jkâˆ’1 zero. Then
x must have digits i1,...,id non-zero.
The converse: consider an (N âˆ’d +1)Ã—N matrix
 G = [INâˆ’d+1|E(Nâˆ’d+1)Ã—(dâˆ’1)]
where INâˆ’d+1 is a unit matrix and E is an (N âˆ’d + 1) Ã— (d âˆ’1) matrix with all
entries 1 (the unit of F2). The rows of  G are linearly independent and have weight d,
and for any row there exists a codeword x(i) âˆˆX with non-zero digits at the same
positions (and, possibly, elsewhere). Then k, the rank of the code, is â‰¥N âˆ’d + 1.
Thus, k = N âˆ’d +1.

2.5 Cyclic codes and polynomial algebra. Introduction to BCH codes
213
Worked Example 2.4.18
The MDS codes [N,N,1], [N,1,N] and [N,N âˆ’1,2]
always exist and are called trivial. Any [N,k] MDS code with 2 â‰¤k â‰¤Nâˆ’2 is called
non-trivial. Show that there is no non-trivial MDS code over Fq with q â‰¤k â‰¤N âˆ’q.
In particular, there is no non-trivial binary MDS code (which causes a discernible
lack of enthusiasm about binary MDS codes).
Solution Indeed, the [N,N,1], [N,N âˆ’1,2] and [N,1,N] codes are MDS. Take
q â‰¤k â‰¤N âˆ’q and assume X is a q-ary MDS. Take its generating matrix G in the
standard form (Ik|Gâ€²) where Gâ€² is k Ã—(N âˆ’k), N âˆ’k â‰¥q.
If some entries in a column of Gâ€² are zero then this column is a linear combina-
tion of k âˆ’1 columns of Ikâˆ’1. This is impossible by (b) in the previous example;
hence Gâ€² has no 0 entry. Next, assume that the ï¬rst row of Gâ€² is 1...1: otherwise
we can perform scalar multiplication of columns maintaining codesâ€™ equivalence.
Now take the second row of Gâ€²: it is of length N âˆ’k â‰¥q and has no 0 entry. Then
these must be repeated entries. That is,
G =
â›
âIk

1
...
1
...
1
...
1
...
...
a
...
a
...
...
...
...
â
â ,a Ì¸= 0.
Then take the codeword
x = row 1âˆ’aâˆ’1(row 2);
it has w(x) â‰¤N âˆ’k âˆ’2+2 = N âˆ’k and X cannot be MDS.
By using the dual code, obtain that there exists no non-trivial q-ary MDS code
with k â‰¥q. Hence, non-trivial MDS code can only have
N âˆ’q+1 â‰¤k or k â‰¤qâˆ’1.
That is, there exists no non-trivial binary MDS code, but there exists a non-trivial
[3,2,2] ternary MDS code.
Remark 2.4.19
It is interesting to ï¬nd, given k and q, the largest value of N
for which there exists a q-ary MDS [N,k] code. We demonstrated that N must be
â‰¤k +qâˆ’1, but computational evidence suggests this value is q+1.
2.5 Cyclic codes and polynomial algebra. Introduction to BCH codes
A useful class of linear codes is formed by the so-called cyclic codes (in particular,
the Hamming, Golay and Reedâ€“Muller codes are cyclic). Cyclic codes were pro-
posed by Eugene Prange in 1957; their importance was immediately recognised,
and they generated a large literature. But more importantly, the idea of cyclic codes,

214
Introduction to Coding Theory
together with some other sharp observations made at the end of the 1950s, partic-
ularly the invention of BCH codes, opened a connection from the theory of linear
codes (which was then at its initial stage) to algebra, particularly to the theory of ï¬-
nite ï¬elds. This created algebraic coding theory, a thriving direction in the modern
theory of linear codes.
We begin with binary cyclic codes. The coding and decoding procedures for
binary cyclic codes of length N are based on the related algebra of polynomials
with binary coefï¬cients:
a(X) = a0 +a1X +Â·Â·Â·+aNâˆ’1XNâˆ’1, where ak âˆˆF2 for k = 0,...,N âˆ’1. (2.5.1)
Such polynomials can be added and multiplied in the usual fashion, except that
Xk +Xk = 0. This deï¬nes a binary polynomial algebra F2[X]; the operations over
binary polynomials refer to this algebra. The degree dega(X) of polynomial a(X)
equals the maximal label of its non-zero coefï¬cient. The degree of the zero poly-
nomial is set to be 0. Thus, the representation (2.5.1) covers polynomials of degree
< N.
Theorem 2.5.1
(a) (1+X)2l = 1+X2l (A freshmanâ€™s dream).
(b) (The division algorithm) Let f(X) and h(X) be two binary polynomials with
h(X) Ì¸â‰¡0. Then there exist unique polynomials g(X) and r(X) such that
f(X) = g(X)h(X)+r(X)
with
degr(X) < degh(X).
(2.5.2)
The polynomial g(X) is called the ratio, or quotient, and r(X) the remainder.
Proof
(a) The statement follows from the binomial decomposition where all
intermediate terms vanish.
(b) If degh(X) > deg f(X) we simply set
f(X) = 0Â·h(X)+ f(X).
If degh(X) â‰¤deg f(X), we can perform the â€˜standardâ€™ procedure of long divi-
sion, with the rules of the binary addition and multiplication.
Example 2.5.2
For binary polynomials:
(a)

1+X +X3 +X4
X +X2 +X3
= X +X7.
(b) 1+XN = (1+X)

1+X +Â·Â·Â·+XNâˆ’1
.
(c) The quotient

X + X2 + X6 + X7 + X8  
1 + X + X2 + X4
= X3 + X4; the
remainder equals X +X2 +X3.

2.5 Cyclic codes and polynomial algebra. Introduction to BCH codes
215
Deï¬nition 2.5.3
Two polynomials, f1(X) and f2(X), are called equivalent
mod h(X), or f1(X) = f2(X) mod h(X), if their remainders, after division by h(X),
coincide. That is,
fi(X) = gi(X)h(X)+r(X),
i = 1,2,
and degr(X) < degh(X).
Theorem 2.5.4
Addition and multiplication of polynomials respect the equiva-
lence. That is, if
f1(X) = f2(X) mod h(X)
and
p1(X) = p2(X) mod h(X),
(2.5.3)
then
'
f1(X)+ p1(X) = f2(X)+ p2(X) mod h(X),
f1(X)p1(X) = f2(X)p2(X) mod h(X).
(2.5.4)
Proof
We have, for i = 1,2,
fi(X) = gi(X)h(X)+r(X),
pi(X) = qi(X)h(X)+s(X),
with
degr(X),degs(X) < degh(X).
Hence
fi(X)+ pi(X) = (gi(X)+qi(X))h(X)+(r(X)+s(X))
with
deg(r(X)+s(X)) â‰¤max[r(X),s(X)] < degh(X).
Thus
f1(X)+ p1(X) = f2(X)+ p2(X) mod h(X).
Furthermore, for i = 1,2, the product fi(X)pi(X) is represented as

gi(X)qi(X)h(X)+r(X)qi(X)+s(X)gi(X)

h(X)+r(X)s(X).
Hence, the remainder for both polynomials f1(X)p1(X) and f2(X)p2(X) may come
only from r(X)s(X). Thus it is the same for both of them.
Note that every linear binary code XN corresponds to a set of polynomials, with
coefï¬cients 0,1, of degree N âˆ’1 which is closed under addition mod 2:
a(X) = a0 +a1X +Â·Â·Â·+aNâˆ’1XNâˆ’1 â†”a(N) = a0 ...aNâˆ’1,
b(X) = b0 +b1X +Â·Â·Â·+bNâˆ’1XNâˆ’1 â†”b(N) = b0 ...bNâˆ’1,
a(X)+b(X) â†”a(N) +b(N) = (a0 +b0)...(aNâˆ’1 +bNâˆ’1).
(2.5.5)

216
Introduction to Coding Theory
[The numeration of the digits in a word of length N using 0,...,N âˆ’1 instead of
1,...,N is more convenient.]
We systematically write a(X) âˆˆX when the word a(N) = a0 ...aNâˆ’1, represent-
ing polynomial a(X), belongs to code X .
Deï¬nition 2.5.5
Given a binary word a = a0a1 ...aNâˆ’1, we deï¬ne the cyclic shift
Ï€a as a word aNâˆ’1a0 ...aNâˆ’2. A linear binary code X is called cyclic if the cyclic
shift of each codeword is again a codeword.
A â€˜straightforwardâ€™ way to form a cyclic code is as follows: take a word a,
then its subsequent cyclic shifts Ï€a, Ï€2a, etc., and ï¬nally all sums of the vectors
obtained. Such a construction allows one to build a code from a single word, and
eventually all the properties of the code may be inferred from the properties of
word a. It turns out that every cyclic code may be obtained in such a way: the
corresponding word is called a generator of a cyclic code.
Lemma 2.5.6
A binary linear code X is cyclic iff, for any vector u from a basis
of X, Ï€u âˆˆX .
Proof
Each codeword in X is a sum of vectors of the basis, but Ï€(u + v) =
Ï€u+Ï€v; hence the result.
A useful property of a cyclic shift is established below:
Lemma 2.5.7
If the word a corresponds to a polynomial a(X) then the word Ï€a
corresponds to Xa(X) mod (1+XN).
Proof
The relations
Xa(X) = a0X +a1X2 +Â·Â·Â·+aNâˆ’2XNâˆ’1 +aNâˆ’1XN
= aNâˆ’1 +a0X +a1X2 +Â·Â·Â·+aNâˆ’2XNâˆ’1 mod (1+XN)
mean that the polynomial
aNâˆ’1 +a0X +Â·Â·Â·+aNâˆ’2XNâˆ’1
corresponding to Ï€a equals Xa(X) mod (1+XN).
A similar argument implies that the word Ï€2a corresponds to X2a(X) mod (1+
XN), etc. More generally, we have the following.
Example 2.5.8
The inverse cyclic shift Ï€âˆ’1: a0 ...aNâˆ’2aNâˆ’1 âˆˆ{0,1}N â†’
a1a2 ...aNâˆ’1a0 acts on polynomials a(X) of degree at least N âˆ’1 by
Ï€âˆ’1a(X) = 1
X
	
a(X)+a0

+a0XNâˆ’1.

2.5 Cyclic codes and polynomial algebra. Introduction to BCH codes
217
Theorem 2.5.9
A binary cyclic code contains, with each pair of polynomials
a(X) and b(X), the sum a(X)+b(X) and any polynomial v(X)a(X) mod (1+XN).
Proof
By linearity the sum a(X)+b(X) âˆˆX . If v(X) = v0 +v1X +Â·Â·Â·vNâˆ’1XNâˆ’1
then each polynomial Xka(X) mod (1+XN) corresponds to Ï€ka and hence belongs
to X . As
v(X)a(X) mod (1+XN) =
Nâˆ’1
âˆ‘
i=0
viXia(X) mod (1+XN),
the LHS belongs to X .
In other words, the binary polynomials of degree at most N âˆ’1 with the â‹†-
multiplication deï¬ned by
aâ‹†b(X) = a(X)b(X) mod (1+XN),
(2.5.6)
and the usual F2[X]-addition, form a commutative ring, denoted by F2[X]

(1 +
XN). The binary cyclic codes are precisely the ideals of this ring.
Theorem 2.5.10
Let g(X) =
Nâˆ’k
âˆ‘
i=0
giXi be a non-zero polynomial of minimum
degree in a binary cyclic code X . Then:
(i) g(X) is a unique polynomial of minimal degree;
(ii) the code X has rank k;
(iii) the codewords corresponding to g(X),Xg(X),...,Xkâˆ’1g(X), form a basis in
X ; they are cyclic shifts of word g = g0 ...gNâˆ’k0...0;
(iv) a(X) âˆˆX iff a(X) = v(X)g(X) for some polynomial v(x) of degree < k (that
is, g(X) is a divisor of every polynomial from X ).
Proof
(i) Suppose c(X) =
Nâˆ’k
âˆ‘
i=0
ciXi is another polynomial of minimal degree N âˆ’k
in X . Then gNâˆ’k = cNâˆ’k = 1, and hence deg(c(X)+g(X)) < N âˆ’k. But as N âˆ’k is
the minimal degree, c(X)+g(X) should equal zero. This happens iff g(X) = c(X).
Hence, g(X) is unique.
(ii) follows from (iii).
(iii) Assume that property (iv) holds. Then each polynomial a(X) âˆˆX has the
form
g(X)v(X) =
r
âˆ‘
i=1
viXig(X),
r < k.

218
Introduction to Coding Theory
Hence, each polynomial a(X) âˆˆX
is a linear combination of polynomi-
als g(X),Xg(X),...,Xkâˆ’1g(X) (all of which belong to X ). On the other
hand, polynomials g(X),Xg(X),...,Xkâˆ’1g(X) have distinct degrees and hence
are linearly independent. Therefore words g,Ï€g,...,Ï€kâˆ’1g, corresponding to
g(X),Xg(X),...,Xkâˆ’1g(X), form a basis in X .
(iv) We know that each polynomial a(X) âˆˆX has degree > degg(X). By the
division algorithm,
a(X) = v(X)g(X)+r(X).
Here, we must have
degv(X) < k
and
degr(X) < degg(X) = N âˆ’k.
But then v(X)g(X) belongs to X owing to Theorem 2.5.9 (as v(X)g(X) has degree
â‰¤N âˆ’1, it coincides with v(X)g(X) mod (1+XN)). Hence,
r(X) = a(X)+v(X)g(X) âˆˆX
by linearity. As g(X) is a unique polynomial from X of minimum degree, r(X) =
0.
Corollary 2.5.11
Every binary cyclic code is obtained from the codeword cor-
responding to a polynomial of minimum degree, by cyclic shifts and linear combi-
nations.
Deï¬nition 2.5.12
A polynomial g(X) of minimal degree in X is called a mini-
mal degree generator of a (cyclic) binary code X , or brieï¬‚y a generator of X .
Remark 2.5.13
There may be other polynomials that generate X in the sense
of Corollary 2.5.11. But the minimum degree polynomial is unique.
Theorem 2.5.14
A polynomial g(X) of degree â‰¤N âˆ’1 is the generator of a
binary cyclic code of length N iff g(X) divides 1+XN. That is,
1+XN = h(X)g(X)
(2.5.7)
for some polynomial h(X) (of degree N âˆ’degg(X)).
Proof
(The only if part.) By the division algorithm,
1+XN = h(X)g(X)+r(X),
where degr(X) < degg(X).
That is,
r(X) = h(X)g(X)+1+XN, i.e. r(X) = h(X)g(X) mod (1+XN).

2.5 Cyclic codes and polynomial algebra. Introduction to BCH codes
219
By Theorem 2.5.10, r(X) belongs to the cyclic code X generated by g(X). But
g(X) must be the unique polynomial of minimum degree in X . Hence, r(X) = 0
and 1+XN = h(X)g(X).
(The if part.) Suppose that 1 + XN = h(X)g(X), degh(X) = N âˆ’degg(X).
Consider the set {a(X): a(X) = u(X)g(X) mod (1 + XN)}, i.e. the principal
ideal in the â‹†-multiplication polynomial ring corresponding to g(X). This set
forms a linear code; it contains g(X),Xg(X),...,Xkâˆ’1g(X) where k = degh(X).
It sufï¬ces to prove that Xkg(X) also belongs to the set. But Xkg(X) = 1 +
XN +
kâˆ’1
âˆ‘
j=0
h jX jg(X), that is, Xkg(X) is equivalent to a linear combination of
g(X),Xg(X),...,Xkâˆ’1g(X).
Corollary 2.5.15
All cyclic binary codes of length N are in a one-to-one corre-
spondence with the divisors of polynomial 1+XN.
Hence, the cyclic codes are described through the factorisation of the polynomial
1+XN. More precisely, we are interested in decomposing 1+XN into irreducible
factors; combining these factors into products yields all possible cyclic codes of
length N.
Deï¬nition 2.5.16
A polynomial a(X) = a0 + a1X + Â·Â·Â· + aNâˆ’1XNâˆ’1 is called
irreducible if a(X) cannot be written as a product of two polynomials, b(X) and
bâ€²(X), with min[degb(X),degbâ€²(X)] â‰¥1.
The importance (and convenience) of irreducible polynomials for describing
cyclic codes is obvious: every generator polynomial of a cyclic code of length N is
a product of irreducible factors of (1+XN).
Example 2.5.17
(a) The polynomial 1+XN has two â€˜standardâ€™ divisors:
1+XN = (1+X)(1+X +Â·Â·Â·+XNâˆ’1).
The
ï¬rst
factor
1 + X
generates
the
binary
parity-check
code
PN =
%
x = x0 ... xNâˆ’1 : âˆ‘
i
xi = 0
K
, whereas polynomial 1+ X + Â·Â·Â·+ XNâˆ’1 (it may be
reducible) generates the repetition code RN = { 00...0, 11...1 }.
(b) Select the generating and check matrices of the Hamming [7,4] code in the
lexicographic form. If we re-order the digits x4x7x5x3x2x6x1 (which leads to an
equivalent code) then the rows of the generating matrix become subsequent cyclic
shifts of each other:
GH
cycl =
â›
âœ
âœ
â
1 1 0 1 0 0 0
0 1 1 0 1 0 0
0 0 1 1 0 1 0
0 0 0 1 1 0 1
â
âŸ
âŸ
â 

220
Introduction to Coding Theory
and the cyclic shift of the last row is again in the code:
Ï€(0 0 0 1 1 0 1) = (1 0 0 0 1 1 0)
= (1 1 0 1 0 0 0)+(0 1 1 0 1 0 0)+(0 0 1 1 0 1 0).
By Lemma 2.5.6, the code is cyclic. By Theorem 2.5.10(iii), the generating poly-
nomial g(X) corresponds to the framed part in matrix GH
cycl:
1101 âˆ¼g(X) = 1+X +X3 = the generator.
But a similar argument can be used to show that an equivalent cyclic code is ob-
tained from the word 1011 âˆ¼1 + X2 + X3. There is no contradiction: it was not
claimed that the polynomial ideal of a cyclic code is the principal ideal of a unique
element.
If we choose a different order of the columns in the parity-check matrix, the
code will be equivalent to the original code; that is, the code with the generator
polynomial 1+X2 +X3 is again a Hamming [7,4] code.
In Problem 2.3 we will check that the Golay [23,7] code is generated by the
polynomial g(X) = 1+X +X5 +X6 +X7 +X9 +X11.
Worked Example 2.5.18
Using the factorisation
X7 +1 = (X +1)(X3 +X +1)(X3 +X2 +1)
(2.5.8)
in F2[X], ï¬nd all cyclic binary codes of length 7. Identify those which are Hamming
codes and their duals.
Solution See the table below.
code X
generator for X
generator for X âŠ¥
{0,1}7
1
1+X7
parity-check
1+X
âˆ‘
0â‰¤iâ‰¤6
Xi
Hamming
1+X +X3
1+X2 +X3 +X4
Hamming
1+X2 +X3
1+X +X2 +X4
dual Hamming
1+X2 +X3 +X4
1+X +X3
dual Hamming
1+X +X2 +X4
1+X2 +X3
repetition
âˆ‘
0â‰¤iâ‰¤6
Xi
1+X
zero
1+X7
1

2.5 Cyclic codes and polynomial algebra. Introduction to BCH codes
221
It is easy to check that all factors in (2.5.8) are irreducible. Any irreducible factor
could be included or not included in decomposition of the generator polynomial.
This argument proves that there exist exactly 8 binary codes in H7,2 as demon-
strated in the table.
Example 2.5.19
(a) Polynomials of the ï¬rst degree, 1+X and X, are irreducible
(but X does not appear in the decomposition for 1+XN). There is one irreducible
binary polynomial of degree 2: 1 + X + X2, two of degree 3: 1 + X + X3 and 1 +
X2 +X3, and three of degree 4:
1+X +X4,
1+X3 +X4
and
1+X +X2 +X3 +X4,
(2.5.9)
each of which appears in the decomposition of 1+XN for various values of N (see
below). A further distinction is that polynomials 1 + X + X3 and 1 + X2 + X3 are
â€˜primitiveâ€™ whereas 1 + X + X2 + X3 + X4 is not; see Example 2.5.34 below and
Sections 3.1â€“3.3. On the other hand, polynomials
1+X8,
1+X4 +X6 +X7 +X8
and
1+X2 +X6 +X8
(2.5.10)
are reducible. The polynomial 1+XN is always reducible:
1+XN = (1+X)(1+X +Â·Â·Â·+XNâˆ’1).
(b) Generally, the factorisation of polynomial 1 + XN into the irreducible factors
is not easy to achieve. Among the ï¬rst 13 odd values of N, the list of polynomials
1 + XN which admit only the trivial decomposition into two irreducible factors is
as follows:
1+X,
1+X3,
1+X5,
1+X11,
1+X13.
Further, the polynomial 1 + X19 admits only a trivial decomposition (1 + X) (1 +
X +Â·Â·Â·+X18), while others have the following factors (the common factor (1+X)
is omitted):
1+X7 : (1+X +X3)(1+X2 +X3),
1+X9 : (1+X +X2)(1+X3 +X6),
1+X15 : (1+X +X2)(1+X +X4)
Ã—(1+X3 +X4)(1+X +X2 +X3 +X4),
1+X17 : (1+X3 +X4 +X5 +X8)
Ã—(1+X +X2 +X4 +X6 +X7 +X8),
1+X21 : (1+X +X2)(1+X +X3)(1+X2 +X3)
Ã—(1+X +X2 +X4 +X6)(1+X2 +X4 +X5 +X6),
1+X23 : (1+X +X5 +X6 +X7 +X9 +X11)
Ã—(1+X2 +X4 +X5 +X6 +X10 +X11),

222
Introduction to Coding Theory
and
1+X25 : (1+X +X2 +X3 +X4)(1+X5 +X10 +X15 +X20).
For N even, 1+XN can have multiple roots (see Example 2.5.35(c)).
Example 2.5.20
Irreducible polynomials of degree 2 and 3 over the ï¬eld F3
(that is, from F3[X]) are as follows. There exist three irreducible polynomials of
degree 2 over F3: X2 +1, X2 +X +2 and X2 +2X +2. There exist eight irreducible
polynomials of degree 3 over F3: X3 +2X +2, X3 +X2 +2, X3 +X2 +X +2, X3 +
2X2 +2X +2, X3 +2X +1, X3 +X2 +2X +1, X3 +2X2 +1 and X3 +2X2 +X +1.
Cyclic codes admit encoding and decoding procedures in terms of the polyno-
mials. It is convenient to have a generating matrix of a cyclic code X in a form
similar to Gcycl for the Hamming [7,4] code (see above). That is, we want to ï¬nd
the basis in X which gives the following picture in the corresponding generating
matrix:
Gcycl =
â›
âœ
âœ
âœ
âœ
âœ
â
0
0
...
â
âŸ
âŸ
âŸ
âŸ
âŸ
â 
(2.5.11)
Such a basis is provided by Theorem 2.5.10(iii): take the generator polynomial
g(X) and its multiples:
g(X),Xg(X),...,Xkâˆ’1g(X), degg(X) = N âˆ’k.
Symbolically,
Gcycl =
â›
âœ
âœ
âœ
â
g(X)
Xg(X)
...
Xkâˆ’1g(X)
â
âŸ
âŸ
âŸ
â .
(2.5.12)
The code has rank k and may be used for encoding words of length k as follows.
Given a word a = a0 ...akâˆ’1, we form the polynomial a(X) =
âˆ‘
0â‰¤i<k
aiXi and take
the product a(X)g(X). It belongs to X by Theorem 2.5.9, and hence deï¬nes a
codeword. So all we have to do is to store polynomial g(X): the encoding will
correspond to polynomial multiplication. If encoding is given by multiplication,
decoding must be related to division. Recall that under the geometric decoder, we
decode the received word by the closest codeword in the Hamming distance. Such
a codeword is related to a leader of the corresponding coset: we have seen that the
cosets are in a one-to-one correspondence with the syndrome words of the form

2.5 Cyclic codes and polynomial algebra. Introduction to BCH codes
223
yHT. In the case of a cyclic code, the syndromes are calculated straightforwardly.
Recall that, if g(X) is a generator polynomial of a cyclic code X and degg(X) =
N âˆ’k, then the rank of X equals k, and there must be 2Nâˆ’k distinct cosets (see
Theorem 2.5.10(v)).
Theorem 2.5.21
The cosets y+X are in a one-to-one correspondence with the
remainders y(X) = u(X) mod g(X). In other words, two words y,yâ€² belong to the
same coset iff, in the division algorithm representation,
y(X) = a(X)g(X)+u(X), yâ€²(X) = aâ€²(X)g(X)+uâ€²(X), and u(X) = uâ€²(X).
Proof
y and yâ€² belong to the same coset iff y + yâ€² âˆˆX . This is equivalent to
u(X)+uâ€²(X) = 0, i.e. u(X) = uâ€²(X) by Theorem 2.5.14.
Hence the cosets are labelled by the polynomials u(X) of degu(X) < degg(X) =
N âˆ’k: there are exactly 2Nâˆ’k such polynomials. To determine the coset y + X it
is enough to compute the remainder u(X) = y(X) mod g(X). Unfortunately, there
is still a task to ï¬nd a leader in each case: there is no simple algorithm for ï¬nding
leaders, for a general cyclic code. However, there are known particular classes of
cyclic codes which admit a relatively simple decoding: the ï¬rst such class was
discovered in 1959 and is formed by BCH codes (see Section 2.6).
As was observed, a cyclic code may be generated not only by its polynomial of
minimum degree: for some purposes other polynomials with this property may be
useful. However, they all are divisors of 1+XN:
Theorem 2.5.22
Let X be a binary cyclic code of length N. Then any polyno-
mial  g(X) such that X is the principal ideal of  g(X) is a divisor of 1+XN.
Proof
An exercise from algebra.
We see that the cyclic codes are naturally labelled by their generator poly-
nomials.
Deï¬nition 2.5.23
Let X be the cyclic binary code of length N generated by
g(X). The check polynomial h(X) of X is deï¬ned as the ratio (1 + XN)/g(X).
That is, h(X) is a unique polynomial for which h(X)g(X) = 1+XN.
We will use the standard notation gcd( f(X),g(X)) for the greatest common di-
visor of polynomials f(X) and g(X) and lcm( f(X),g(X)) for their least common
multiple. Denote by X1 +X2 the direct sum of two linear codes X1,X2 âŠ‚HN,2.
That is, X1 + X2 consists of the linear combinations Î±1a(1) + Î±2a(2) where
Î±1,Î±2 = 0,1 and a(i) âˆˆXi, i = 1,2. Compare Example 2.1.8(vii).

224
Introduction to Coding Theory
Worked Example 2.5.24
Let X1 and X2 be two binary cyclic codes of length
N, with generators g1(X) and g2(X). Prove that:
(a) X1 âŠ‚X2 iff g2(X) divides g1(X);
(b)
the
intersection
X1 âˆ©X2
yields
a
cyclic
code
generated
by
lcm
	
g1(X),g2(X)

;
(c)
the
direct
sum
X1 + X2
is
a
cyclic
code
with
the
generator
gcd
	
g1(X),g2(X)

.
Solution (a) We know that a(X) âˆˆXi iff, in the ring F2[X]

(1 + XN), polyno-
mial a(X) = fi â‹†gi(X), i = 1,2. Suppose g2(X) divides g1(X) and write g1(X) =
r(X)g2(X). Then every polynomial a(X) of the form f1 â‹†g1(X) is of the form
f1 â‹†r â‹†g2(X). That is, if a(X) âˆˆX1 then a(X) âˆˆX2, so X1 âŠ‚X2.
Conversely, suppose that X1 âŠ‚X2. Let di be the degree of gi(X), 1 â‰¤di < N,
i = 1,2, and write
g1(X) = f(X)g2(X)+r(X), where degr(X) < d2.
We have that every polynomial â‹†-divisible by g1(X) in F2[X]

(1 + XN) is also â‹†-
divisible by g2(X). In particular, the basis polynomials Xig1(X), 0 â‰¤i â‰¤N âˆ’d1âˆ’1,
are â‹†-divisible by g2(X), i.e. have the form
Xig1(X) = h(i)(X)g2(X)+Î±i(XN âˆ’1) where Î±i = 0 or 1.
If, for some i, the coefï¬cient Î±i = 0 then we compare two identities,
Xig1(X) = Xi f(X)g2(X)+Xir(X) and Xig1(X) = h(i)(X)g2(X),
and conclude that Xir(X) = 0. This implies that r(X) = 0 and hence g2(X) divides
g1(X).
The remaining case is that all coefï¬cients Î±i â‰¡1. Then we compare
Xg1(X) = Xh(0)(X)g2(X)+X +XN+1
and
Xg1(X) = h(1)(X)g2(X)+1+XN
and see that this case is impossible.
(b) This part becomes straightforward: the intersection X1 âˆ©X2 is a subcode of
both X1 and X2. It is obviously a cyclic code; hence, by part (a), its generator g(X)
is divisible by both g1(X) and g2(X). Then it is divisible by the lcm(g1(X),g2(X)).
We must exclude the case where g(X) produces a non-trivial ratio after this di-
vision. But the lcm(g1(X),g2(X)) is itself a generator of a cyclic code (of the
same original length) contained in both X1 and X2. So, in the case g(X) Ì¸=

2.5 Cyclic codes and polynomial algebra. Introduction to BCH codes
225
lcm(g1(X),g2(X)), the code generated by lcm(g1(X),g2(X)) must be strictly larger
than X1 âˆ©X2. This contradicts the deï¬nition of X1 âˆ©X2.
(c) Similarly, X1 + X2 is the minimal linear code containing both X1 and X2.
Hence, its generator divides both g1(X) and g2(X), i.e. is their common divisor.
And if it is not equal to the gcd(g1(X),g2(X)) then it contradicts the above mini-
mality property.
Worked Example 2.5.25
Let X be a binary cyclic code of length N with the
generator g(X) and the check polynomial h(X). Prove that a(X) âˆˆX iff the poly-
nomial (1+XN) divides a(X)h(X), i.e. aâ‹†h(X) = 0 in F2[X]/(1+XN).
Solution
If a(X) âˆˆX then a(X) = f(X)g(X) for some polynomial f(X) âˆˆ
F2[X]/(1+XN). Then
a(X)h(X) = f(X)g(X)h(X) = f(X)(1+XN)
which equals 0 in F2[X]/(1 + XN). Conversely, let a(X) âˆˆF2[X]/(1 + XN) and
assume that a(X)h(X) = 0 mod (1 + XN). Write a(X) = f(X)g(X) + r(X) where
degr(X) <deg g(X). Then
a(X)h(X) = f(X)(1+XN)+r(X)h(X) = r(X)h(X) mod (1+XN).
Hence, r(X)h(X) = 0 mod (1+XN) which is only possible when r(X) = 0 (since
degr(X)h(X) < N). Thus, a(X) = f(X)g(X) and a(X) âˆˆX .
Worked Example 2.5.26
Prove that the dual of a cyclic code is again cyclic and
ï¬nd its generating matrix.
Solution If y âˆˆX âŠ¥, the dual code, then the dot-product âŸ¨Ï€xÂ·yâŸ©= 0 for all x âˆˆX .
But âŸ¨Ï€xÂ·yâŸ©= âŸ¨xÂ·Ï€yâŸ©, i.e. Ï€y âˆˆX âŠ¥, which means that X âŠ¥is cyclic.
Let g(X) = g0 + g1X + Â·Â·Â· + gNâˆ’kXNâˆ’k be the generating polynomial for X ,
where N âˆ’k = d is the degree of g(X) and k gives the rank of X . We know that
the generating matrix G of X may be written as
G âˆ¼
â›
âœ
âœ
âœ
âœ
âœ
âœ
âœ
â
g(X)
Xg(X)
Â·
Â·
Â·
Xkâˆ’1g(X)
â
âŸ
âŸ
âŸ
âŸ
âŸ
âŸ
âŸ
â 
âˆ¼
â›
âœ
âœ
âœ
âœ
âœ
â
0
0
...
â
âŸ
âŸ
âŸ
âŸ
âŸ
â 
.
(2.5.13)

226
Introduction to Coding Theory
Take h(X) = (1+XN)/g(X) and write h(X) =
k
âˆ‘
j=0
h jX j and h = h0 ...hNâˆ’1. Then
i
âˆ‘
j=0
g jhiâˆ’j
'
= 1,
i = 0, N,
= 0,
1 â‰¤i < N.
Indeed, for i = 0, N, we have h0g0 = 1 and hkgNâˆ’k = 1. For 1 â‰¤i < N we obtain
that the dot-product
âŸ¨Ï€ jgÂ·Ï€ jâ€²hâŠ¥âŸ©= 0 for j = 0,1,...,N âˆ’k âˆ’1, jâ€² = 0,...,k âˆ’1,
where hâŠ¥= hkhkâˆ’1 ...h0. It is then easy to see that hâŠ¥gives rise to the generator
hâŠ¥(X) of X âŠ¥.
An alternative solution is based on Worked Example 2.5.25. We know that
a(X) âˆˆX iff a â‹†h(X) = 0. Let k be the degree of g(X) then the degree of h(X)
equals N âˆ’k. The degree deg[a(X)h(X)] is < 2N âˆ’k, so the coefï¬cients of XNâˆ’k,
XNâˆ’k+1,...,XNâˆ’1 in a(X)h(X) all vanish. That is:
a0hNâˆ’k +a1hNâˆ’kâˆ’1 +Â·Â·Â·+aNâˆ’kh0
= 0,
a1hNâˆ’k +a2hNâˆ’kâˆ’1 +Â·Â·Â·+aNâˆ’k+1h0
= 0,
...
...
akâˆ’1hNâˆ’k +akhNâˆ’kâˆ’1 +Â·Â·Â·+aNâˆ’1h0
= 0.
In other words, aHT = 0 where a = a0,...,aNâˆ’1 is the word of the binary coefï¬-
cients for a(X) and H is an (N âˆ’k)Ã—N matrix
H âˆ¼
â›
âœ
âœ
âœ
â
hâŠ¥(X)
XhâŠ¥(X)
...
XNâˆ’kâˆ’1hâŠ¥(X)
â
âŸ
âŸ
âŸ
â âˆ¼
â›
âœ
âœ
âœ
âœ
âœ
â
0
0
...
â
âŸ
âŸ
âŸ
âŸ
âŸ
â 
(2.5.14)
and hâŠ¥(X) = XNâˆ’kh(Xâˆ’1), with the coefï¬cient string hâŠ¥= hkhkâˆ’1 ...h0.
We conclude that matrix H generates a code X â€² âŠ†X âŠ¥. But since hNâˆ’k = 1, the
rank of X â€² equals N âˆ’k. Hence, X â€² = X âŠ¥.
It remains to check that polynomial hâŠ¥(X) divides 1 + XN. To this end,
we deduce from g(X)h(X) = 1 + XN that h(Xâˆ’1)g(Xâˆ’1) = Xâˆ’N + 1. Hence
hâŠ¥(X)Xkg(Xâˆ’1) = 1+XN, and as Xkg(Xâˆ’1) equals the polynomial gk +gkâˆ’1X +
Â·Â·Â·+g0Xk, the required fact follows.
2
Worked Example 2.5.27
Let X be a binary cyclic code of length N with gen-
erator g(X).

2.5 Cyclic codes and polynomial algebra. Introduction to BCH codes
227
(a) Show that the set of codewords a âˆˆX of even weight is a cyclic code and ï¬nd
its generator.
(b) Show that X contains a codeword of odd weight iff g(1) Ì¸= 0 or, equivalently,
word 1 âˆˆX .
Solution (a) If code X is even (i.e. contains only words of even weight) then
every polynomial a(X) âˆˆX has a(1) =
âˆ‘
0â‰¤i<Nâˆ’1
ai = 0. Hence, a(X) contains a
factor (X +1). Therefore, the generator g(X) has a factor (X +1). The converse is
also true: if (X +1) divides g(X), or, equivalently, g(1) = 0, then every codeword
a âˆˆX is of even weight.
Now assume that X contains a word with an odd weight, i.e. g(1) = 1; that
is, (1 + X) does not divide g(X). Let X ev be the subcode in X formed by the
even codewords. A cyclic shift does not change the weight, so X ev is a cyclic
code. For the corresponding polynomials a(X) we have, as before, that (1 + X)
divides a(X). Thus, the generator gev(X) of X ev is divisible by (1 + X), hence
gev(X) = g(X)(X +1).
(b) It remains to show that g(1) = 1 iff the word 1 âˆˆX . The corresponding poly-
nomial is 1+Â·Â·Â·+XNâˆ’1, the complementary factor to (1+X) in the decomposition
1 + XN = (1 + X)(1 + Â·Â·Â·+ XNâˆ’1). So, if g(1) = 1, i.e. g(X) does not contain the
factor (1 + X), then g(X) must be a divisor of 1 + Â·Â·Â· + XNâˆ’1. This implies that
1 âˆˆX . The inverse statement is established in a similar manner.
Worked Example 2.5.28
Let X be a binary cyclic code of length N with gen-
erator g(X) and check polynomial h(X).
(a) Prove that X is self-orthogonal iff hâŠ¥(X) divides g(X) and self-dual iff
hâŠ¥(X) = g(X) where hâŠ¥(X) = hk + hkâˆ’1X + Â·Â·Â· + h0Xkâˆ’1 and h(X) = h0 + Â·Â·Â· +
hkâˆ’1Xkâˆ’1 +hkXk is the check polynomial, with g(X)h(X) = 1+XN.
(b) Let r be a divisor of N: r|N. A binary code X is called r-degenerate if every
codeword a âˆˆX is a concatenation c...c where c is a string of length r. Prove that
X is r-degenerate iff h(X) divides (1+Xr).
Solution
(a) Self-orthogonality means that X âŠ†X âŠ¥, i.e. âŸ¨a Â· bâŸ©= 0 for all
a,b âˆˆX . From Worked Example 2.5.26 we know that hâŠ¥(X) gives the genera-
tor polynomial of X âŠ¥. Then, by virtue of Worked Example 2.5.26, X âŠ†X âŠ¥iff
hâŠ¥(X) divides g(X).
Self-duality means that X = X âŠ¥, that is hâŠ¥(X) = g(X).
(b) For N = rs, we have the decomposition
1+XN = (1+Xr)(1+Xr +Â·Â·Â·+Xr(sâˆ’1)).

228
Introduction to Coding Theory
Now assume cyclic code X of length N with generator g(X) is r-degenerate. Then
the word g is of the form 1 c1 c...1 c for some string  c of length râˆ’1 (with c = 1 c).
Let  c(X) be the polynomial corresponding to  c (of degree â‰¤r âˆ’2). Then g(X) is
given by
1+X c(X)+Xr +Xr+1 c(X)+Â·Â·Â·+Xr(sâˆ’1) +Xr(sâˆ’1)+1 c(X)
= (1+Xr +Â·Â·Â·+Xr(sâˆ’1))[1+X c(X)].
For the check polynomial h(X) we obtain
h(X) =

1+XN>
1+Xr +Â·Â·Â·+Xr(sâˆ’1)	
1+X c(X)


=

1+Xr	
1+X c(X)

,
i.e. h(X) is a divisor of (1+Xr).
Conversely, let h(X)|(1 + Xr), with h(X)g(X) = 1 + Xr where g(X) =
âˆ‘
0â‰¤jâ‰¤râˆ’1
cjX j, with c0 = 1. Take c = c0 ...crâˆ’1; repeating the above argument in
the reverse order, we conclude that the word g is the concatenation c...c. Then the
cyclic shift Ï€g is the concatenation c(1) ...c(1) where c(1) = crâˆ’1c0 ...crâˆ’2 (= Ï€c,
the cyclic shift of c in {0,1}r). Similarly, for subsequent cyclic shift iterations
Ï€2g,.... Hence, the basis vectors in X are r-degenerate, and so is the whole of X .
In the â€˜standardâ€™ arithmetic, a (real or complex) polynomial p(X) of a given de-
gree d is conveniently identiï¬ed through its roots (or zeros) Î±1,...,Î±d (in general,
complex), by means of the monomial decomposition: p(X) = pd
âˆ
1â‰¤iâ‰¤d
(X âˆ’Î±i). In
the binary arithmetic (and, more generally, the q-ary arithmetic), the roots of poly-
nomials are still an extremely useful concept. In our situation, the roots help to
construct the generator polynomial g(X) =
âˆ‘
0â‰¤iâ‰¤d
giXi of a binary cyclic code with
important predicted properties. Assume for the moment that the roots Î±1,...,Î±d of
g(X) are a well-deï¬ned object, and the representation
g(X) = âˆ
1â‰¤iâ‰¤d
(X âˆ’Î±i)
has a consistent meaning (which is provided within the framework of ï¬nite ï¬elds).
Even without knowing the formal theory, we are able to make a couple of helpful
observations.
The ï¬rst observation is that the Î±i are Nth roots of unity, as they should be among
the zeros of polynomial 1+XN. Hence, they could be multiplied and inverted, i.e.
would form an Abelian multiplicative group of size N, perhaps cyclic. Second, in
the binary arithmetic, if Î± is a zero of g(X) then so is Î±2, as g(X)2 = g(X2). Then
Î±2 is also a zero, as well as Î±4, and so on. We conclude that the sequence Î±,Î±2,...
begins cycling: Î±2d = Î± (or Î±2dâˆ’1 = 1) where d is the degree of g(X). That is, all

2.5 Cyclic codes and polynomial algebra. Introduction to BCH codes
229
Nth roots of unity split into disjoint classes, of the form C =
(
Î±,Î±2,...,Î±2câˆ’1)
,
of size c where c = c(C ) is a positive integer (with 2c âˆ’1 dividing N). The notation
C (Î±) is instructive, with c = c(Î±). The members of the same class are said to be
conjugate to each other. If we want a generating polynomial with root Î± then all
conjugate roots of unity Î±â€² âˆˆC (Î±) will also be among the roots of g(X).
Thus, to form a generator g(X) we have to â€˜borrowâ€™ roots from classes C and
enlist, with each borrowed root of unity, all members of their classes. Then, since
any polynomial a(X) from the cyclic code generated by g(X) is a multiple of g(X)
(see Theorem 2.5.10(iv)), the roots of g(X) will be among the roots of a(X). Con-
versely, if a(X) has roots Î±i of g(X) among its roots then a(X) is in the code. We
see that cyclic codes are conveniently described in terms of roots of unity.
Example 2.5.29
(The Hamming [7,4] code) Recall that the parity-check matrix
H for the binary Hamming [7,4] code X H is 3 Ã— 7; it enlists as its columns all
non-zero binary words of length 3: different orderings of these rows deï¬ne equiv-
alent codes. Later in this section we explain that the sequence of non-zero binary
words of any given length 2â„“âˆ’1 written in some particular order (or orders) can be
interpreted as a sequence of powers of a single element Ï‰: Ï‰0, Ï‰, Ï‰2,...,Ï‰2â„“âˆ’2.
The multiplication rule generating these powers is of a special type (multiplication
of polynomials modulo a particular irreducible polynomial of degree â„“). To stress
this fact, we use in this section the notation âˆ—for this multiplication rule, writing
Ï‰âˆ—i in place of Ï‰i. Anyway, for l = 3, one appropriate order of the binary non-zero
3-words (out of the two possible orders) is
H =
â›
â
0
0
1
0
1
1
1
0
1
0
1
1
1
0
1
0
0
1
0
1
1
â
â âˆ¼(Ï‰âˆ—0 Ï‰ Ï‰âˆ—2 Ï‰âˆ—3 Ï‰âˆ—4 Ï‰âˆ—5 Ï‰âˆ—6).
Then, with this interpretation, the equation aHT = 0, determining that the word
a = a0 ...a6 (or its polynomial a(X) =
âˆ‘
0â‰¤i<7
aiXi) lies in X H, can be rewritten as
âˆ‘
0â‰¤i<7
aiÏ‰âˆ—i = 0, or a(âˆ—Ï‰) = 0.
In other words, a(X) âˆˆX H iff Ï‰ is a root of a(X) under the multiplication rule âˆ—
(which in this case is multiplication of binary polynomials of degree â‰¤2 modulo
the polynomial 1+X +X3).
The last statement can be rephrased in this way: the Hamming [7,4] code is
equivalent to the cyclic code with the generator g(X) that has Ï‰ among its roots;
in this case the generator g(X) = 1 + X + X3, with g(âˆ—Ï‰) = Ï‰âˆ—0 + Ï‰ + Ï‰âˆ—3 = 0.
The alternative ordering of the rows of H H is related in the same fashion to the
polynomial 1+X2 +X3.

230
Introduction to Coding Theory
We see that the Hamming [7,4] code is deï¬ned by a single root Ï‰, provided
that we establish proper terms of operation with its powers. For that reason we can
call Ï‰ the deï¬ning root (or deï¬ning zero) for this code. There are reasons to call
element Ï‰ â€˜primitiveâ€™; cf. Sections 3.1â€“3.3.
Worked Example 2.5.30
A code X is called reversible if a = a0a1 ...aNâˆ’1 âˆˆX
implies that aâ†= aNâˆ’1 ...a1a0 âˆˆX . Prove that a cyclic code with generator g(X)
is reversible iff g(Î±) = 0 implies g(Î±âˆ’1) = 0.
Solution For the generator polynomial g(X) =
âˆ‘
0â‰¤iâ‰¤d
giXi, with degg(X) = d < N
and g0 = gd = 1, the reversed polynomial is grev(X) = XNâˆ’1g(Xâˆ’1), so if the cyclic
code X is reversible and Î± is a root of g(X) then Î± is also a root of grev(X). This
is possible only when g(Î±âˆ’1) = 0.
Conversely, let g(X) satisfy the property that g(Î±) = 0 implies g(Î±âˆ’1) = 0.
The above formula holds for all polynomial a(X) of degree < N: arev(X) =
XNâˆ’1a(Xâˆ’1). If a(X) âˆˆX then a(Î±) = a(Î±âˆ’1) = 0 for all root Î± of g(X). Then
arev(Î±) = arev(Î±âˆ’1) = 0 for all roots Î± of g(X). Thus, arev(X) is a multiple of
g(X), and arev(X) âˆˆX .
The natural framework for studying roots of polynomials is provided by the
theory of ï¬nite ï¬elds or Galois theory (we have seen already how polynomial ï¬elds
can be used). In the rest of this section we give an initial (and brief) introduction
into some aspects of Galois theory to understand better some examples of codes
introduced so far. In Chapter 3 we will dive deeper into the Galois theory to gain
enough knowledge in order to proceed further with code constructions.
Remark 2.5.31
A ï¬eld is a commutative ring where each non-zero element has
an inverse. In other words, a ring is a ï¬eld if the multiplication generates a group.
In fact, a multiplication group of non-zero elements of a ï¬eld is cyclic.
Theorem 2.5.32
Let g(X) âˆˆF2[X] be an irreducible binary polynomial of de-
gree d. Then multiplication mod g(X) makes the set of the binary polynomials of
degree â‰¤d âˆ’1 (i.e. the space FÃ—d
2 ) a ï¬eld with 2d elements. Conversely, if the
multiplication mod g(X) leads to a ï¬eld then g(X) is irreducible.
Proof
The only non-trivial property to check is the existence of the inverse ele-
ment. Take a non-zero polynomial f(X), with deg f(X) â‰¤d âˆ’1, and consider all
polynomials of the form f(X)h(X) (the usual multiplication) where h(X) runs over
the whole set of the polynomials of degree â‰¤d âˆ’1. These products must be distinct
mod g(X). Indeed, if
f(X)h1(X) = f(X)h2(X) mod g(X),

2.5 Cyclic codes and polynomial algebra. Introduction to BCH codes
231
then, for some polynomial v(X) of degree â‰¤d âˆ’2,
f(X)(h1(X)âˆ’h2(X)) = v(X)g(X).
(2.5.15)
This implies that either g(X)| f(X) or g(X)|h1(X) âˆ’h2(X). We conclude that if
polynomial g(X) is irreducible, (2.5.15) is impossible, unless h1(X) = h2(X) and
v(X) = 0. For one and only one polynomial h(X), we have
f(X)h(X) = 1 mod g(X);
h(X) represents the inverse for f(X) in multiplication mod g(X). We write h(X) =
f(X)âˆ—âˆ’1.
On the other hand, if g(X) is reducible, then g(X) = b(X)bâ€²(X) where both b(X)
and bâ€²(X) are non-zero and have degree < d. That is, b(X)bâ€²(X) = 0 mod g(X). If
the multiplication mod q led to a ï¬eld both b(X) and bâ€²(X) would have inverses,
b(X)âˆ’âˆ—1 and bâ€²(X)âˆ’âˆ—1. But then
b(X)âˆ’âˆ—1 âˆ—b(X)âˆ—bâ€²(X) = bâ€²(X) = 0,
and similarly b(X) = 0.
A ï¬eld obtained via the above construction is called a polynomial ï¬eld and is
often denoted by F2[X]/âŸ¨g(X)âŸ©. It contains 2d elements where d = deg g(X) (rep-
resenting polynomials of degree < d). We will call g(X) the core polynomial of the
ï¬eld. For the rest of this section we denote the multiplication in a given polynomial
ï¬eld by âˆ—. The zero polynomial and the unit polynomial are denoted correspond-
ingly, by 0 and 1: they are obviously the zero and the unity of the polynomial ï¬eld.
A key role is played by the following result.
Theorem 2.5.33
(a) The multiplicative group of non-zero elements in polyno-
mial ï¬eld F2[X]/âŸ¨g(X)âŸ©is isomorphic to the cyclic group Z2dâˆ’1 of size 2d âˆ’1.
(b) The polynomial ï¬elds obtained by picking different irreducible polynomials of
degree d are all isomorphic.
Proof
We will only prove here assertion (a); assertion (b) will be established in
Section 3.1. Take any element from the ï¬eld, a(X) âˆˆF2[X]/âŸ¨g(X)âŸ©, and observe
that
aâˆ—i(X) := aâˆ—...âˆ—a
:
;<
=
i times
(X)
(the multiplication in the ï¬eld) takes at most 2d âˆ’1 values (the number of elements
in the ï¬eld less one, as the zero 0 is excluded). Hence there exists a positive integer
r such that aâˆ—r(X) = 1; the smallest value of r is called the order of a(X).

232
Introduction to Coding Theory
Choose a polynomial a(X) âˆˆF2[X]/âŸ¨g(X)âŸ©with the largest order r. Then we
claim that the order of any other element b(X) divides r. Indeed, let s be the order
of b(X). Pick a prime factor p of s and write
s = pcâ€²lâ€², and r = pcl,
with integers câ€²,c â‰¥0 and l,lâ€² â‰¥1, where l and lâ€² are not divisible by p. We want to
show that c â‰¥câ€². Indeed, element aâˆ—pc(X) has order l, bâˆ—lâ€²(X) has order pcâ€² and the
product aâˆ—pb âˆ—bâˆ—lâ€²(X) has order lpcâ€². Hence, câ€² â‰¤c or else r would not be maximal.
This is true for any prime p, hence s divides r.
Thus, with r being the maximal order, every element b(X) in the ï¬eld obeys
bâˆ—r(X) = 1. By using the pigeon-hole principle, we conclude that r = 2d âˆ’1, the
number of non-zero elements of the ï¬eld. Hence, with a(X) being an element of
order r, the powers 1, a(X),...,aâˆ—(2dâˆ’1)(X) exhaust the multiplicative groups of
the ï¬eld.
In the wake of Theorem 2.5.33, we can use the notation F2d for any polynomial
ï¬eld F2[X]/âŸ¨g(X)âŸ©where g(X) is an irreducible binary polynomial of degree d.
Further, the multiplicative group of non-zero elements in F2d is denoted by Fâˆ—
2d;
it is cyclic (â‰ƒZ2dâˆ’1, according to Theorem 2.5.33). Any generator of group Fâˆ—
2d
(whose âˆ—-powers exhaust Fâˆ—
2d) is called a primitive element of ï¬eld F2d.
Example 2.5.34
We can see the importance of writing down the full list of ir-
reducible polynomials. There are six irreducible binary polynomials of degree 5
(each of which is primitive):
1+X2 +X5, 1+X3 +X5, 1+X +X2 +X3 +X5,
1+X +X2 +X4 +X5, 1+X +X3 +X4 +X5,
1+X2 +X3 +X4 +X5
(2.5.16)
and nine of degree 6 (of which six are primitive):
1+X +X6, 1+X +X3 +X4 +X6, 1+X5 +X6,
1+X +X2 +X5 +X6, 1+X2 +X3 +X5 +X6,
1+X +X4 +X5 +X6,
1+X +X2 +X4 +X6, 1+X2 +X4 +X5 +X6,
1+X3 +X6.
(2.5.17)
The number of irreducible polynomials grows signiï¬cantly with the degree: there
are 18 of degree 7, 30 of degree 8, and so on. However, there exist and are available
quite extensive tables of irreducible polynomials over various ï¬nite ï¬elds.

2.5 Cyclic codes and polynomial algebra. Introduction to BCH codes
233
1+X +X3
1+X2 +X3
Xâˆ—i
polynomial
word
âˆ’âˆ’
Xâˆ—0
X
Xâˆ—2
Xâˆ—3
Xâˆ—4
Xâˆ—5
Xâˆ—6
0
1
X
X2
1+X
X +X2
1+X +X2
1+X2
000
100
010
001
110
011
111
101
Xâˆ—i
polynomial
word
âˆ’âˆ’
Xâˆ—0
X
Xâˆ—2
Xâˆ—3
Xâˆ—4
Xâˆ—5
Xâˆ—6
0
1
X
X2
1+X2
1+X +X2
1+X
X +X2
000
100
010
001
101
111
110
011
Figure 2.6
Example 2.5.35
(a) The ï¬eld F2[X]/âŸ¨1 + X + X2âŸ©has four elements: 0, 1, X,
1+X, with the multiplication table:
X âˆ—X = 1+X,
as X2 = 1+X mod (1+X +X2),
X âˆ—(1+X) = X +X âˆ—X = 1,
(1+X)âˆ—(1+X) = 1+X +X +X âˆ—X = 1+1+X = X.
Since Xâˆ—3 = (1+X)âˆ—X = 1, the group is isomorphic to Z3. An alternative notation
for this ï¬eld is F4.
(b) The ï¬elds F2[X]/âŸ¨1+X +X3âŸ©and F2[X]/âŸ¨1+X2 +X3âŸ©contain eight elements
each, representing all polynomials of degree â‰¤2. Every such polynomial a0 +
a1X + a2X2 is identiï¬ed via the string of its coefï¬cients a0a1a2 (a binary word).
The ï¬eld tables are found by looking at the subsequent powers Xâˆ—i: see Figure 2.6.
In both cases the multiplicative group of non-zero elements is Z7. The two ï¬elds
are obviously isomorphic, as they share the common multiplicative cyclic group
formalism. The common notation for these ï¬elds is F8. Note that the two ï¬eld
tables coincide for the powers Xâˆ—i with 0 â‰¤i < 3; in fact, this is a general pattern:
see Sections 3.1â€“3.3.
Moreover, the element X = Xâˆ—1 âˆˆF2[X]/âŸ¨1+X +X3âŸ©can be identiï¬ed as a root
of the core polynomial 1+X +X3 and element X = Xâˆ—1 âˆˆF2[X]/âŸ¨1+X2+X3âŸ©as a
root of 1+X2 +X3, as these polynomials yield zeros in their respective ï¬elds. The
remaining two roots are Xâˆ—2 and Xâˆ—4 (again calculated in their respective ï¬elds).
Applying this example to the Hamming [7,4] code (cf. Example 2.5.29), the
ï¬eld F2[X]/âŸ¨1 + X + X3âŸ©leads to the roots of the generator 1 + X + X3, and the
ï¬eld F2[X]/âŸ¨1 + X2 + X3âŸ©to those of 1 + X2 + X3. That is, the Hamming [7,4]
code is equivalent to the cyclic code of length 7 with the deï¬ning root Ï‰ = X in
either of the two isomorphic ï¬elds F2[X]/âŸ¨1 + X + X3âŸ©or F2[X]/âŸ¨1 + X2 + X3âŸ©.
With some ambiguity (which will be removed in Section 3.1) we may say that this
code is deï¬ned by its root Ï‰ which is a primitive element of F8.

234
Introduction to Coding Theory
powers Xâˆ—i
polynomials
coefï¬cient
strings
âˆ’âˆ’
0
0000
Xâˆ—0
1
1000
X
X
0100
Xâˆ—2
X2
0010
Xâˆ—3
X3
0001
Xâˆ—4
1+X
1100
Xâˆ—5
X +X2
0110
Xâˆ—6
X2 +X3
0011
Xâˆ—7
1+X +X3
1101
Xâˆ—8
1+X2
1010
Xâˆ—9
X +X3
0101
Xâˆ—10
1+X +X2
1110
Xâˆ—11
X +X2 +X3
0111
Xâˆ—12
1+X +X2 +X3
1111
Xâˆ—13
1+X2 +X3
1011
Xâˆ—14
1+X3
1001
Figure 2.7
(c) The ï¬eld F2[X]/âŸ¨1 + X + X4âŸ©contains 16 elements. The ï¬eld table is given in
Figure 2.7. In this case, the multiplicative group is Z15, and the ï¬eld can be denoted
by F16. As above, element X âˆˆF2[X]/âŸ¨1 + X + X4âŸ©yields a root of polynomial
1+X +X4; other roots are Xâˆ—2, Xâˆ—4 and Xâˆ—8.
This example can be used to identify the Hamming [15,11] code as (an equiva-
lent to) the cyclic code with generator g(X) = 1+X +X4. We can now say that the
Hamming [15,11] code is (modulo equivalence) the cyclic code of length 15 with
the deï¬ning root Ï‰(= X) in ï¬eld F2[X]/âŸ¨1 + X + X4âŸ©. As X is a generator of the
multiplicative group of the ï¬eld, we again could say that the deï¬ning root Ï‰ is a
primitive element in F16.
2
In general, take the ï¬eld F2[X]/âŸ¨g(X)âŸ©, where g(X) =
âˆ‘
0â‰¤iâ‰¤d
giXi is an irreducible
binary polynomial of degree m. Then the elements X,Xâˆ—2,Xâˆ—4,...,Xâˆ—2dâˆ’1 will sat-
isfy the equation
âˆ‘
0â‰¤iâ‰¤d
gi

Xâˆ—sâˆ—i = 0, s = 1,2,...,2dâˆ’1.
In other words, X,Xâˆ—2,...,X2dâˆ’1 are precisely the zeros, in ï¬eld F2[X]/âŸ¨g(X)âŸ©, of
the irreducible core polynomial q.
Another feature emerging from Example 2.5.35 is that in all parts (a)â€“(c), ele-
ment X represented the root of the core polynomial g(X). However, this is not true

2.5 Cyclic codes and polynomial algebra. Introduction to BCH codes
235
in general: it only happens when g(X) is a â€˜primitiveâ€™ binary polynomial; for the
detailed discussion of this property see Sections 3.1â€“3.3. For a primitive core poly-
nomial g(X) we have, in addition, that the powers Xi for i < d = deg g(X) coincide
with Xâˆ—i, while further powers Xâˆ—i, m â‰¤i â‰¤2d âˆ’1, are relatively easy to calculate.
With this in mind, we can pass to a general binary Hamming code.
Example 2.5.36
Let X H be the binary Hamming [2â„“âˆ’1,2â„“âˆ’1 âˆ’â„“] code. We
know that its parity-check matrix H features all non-zero column-vectors of length
â„“. These vectors, written in a particular order, list the consecutive powers Ï‰âˆ—i, i =
0,1,...,2â„“âˆ’2, in the ï¬eld F2[X]/âŸ¨g(X)âŸ©where Ï‰ = X and g(X) = g0 + g1X +
Â·Â·Â·+gâ„“Xâ„“âˆ’1 +Xâ„“is a primitive polynomial of degree â„“. Thus,
H =
â›
âœ
âœ
âœ
âœ
â
1
0
Â·Â·Â·
0
g0
Â·Â·Â·
0
1
Â·Â·Â·
0
g1
Â·Â·Â·
Â·Â·Â·
Â·Â·Â·
Â·Â·Â·
Â·Â·Â·
Â·Â·Â·
Â·Â·Â·
0
0
Â·Â·Â·
0
gâ„“âˆ’1
Â·Â·Â·
0
0
Â·Â·Â·
1
0
Â·Â·Â·
â
âŸ
âŸ
âŸ
âŸ
â 
,
(2.5.18)
or H âˆ¼(1 Ï‰ Â·Â·Â·Ï‰âˆ—(â„“âˆ’1) Ï‰âˆ—â„“Â·Â·Â· Ï‰âˆ—(2â„“âˆ’2)
.
Hence, as before, the equation aHT = 0 for the codeword is equivalent to the
equation a(âˆ—Ï‰) = 0 for the corresponding polynomial. So, we can say that a(X) âˆˆ
X H iff Ï‰ is among the roots of a(X).
On the other hand, by construction, Ï‰ is a root of g(X): g(âˆ—Ï‰) = 0. Thus, we
identify the Hamming [2â„“âˆ’1,2â„“âˆ’1 âˆ’â„“] code as equivalent to the cyclic code of
length 2â„“âˆ’1 with the generator polynomial g(X), with the deï¬ning root Ï‰. The
role of Ï‰ can be played by any conjugate element, from
(
Ï‰,Ï‰âˆ—2,...,Ï‰âˆ—2â„“âˆ’1)
.
The above idea leads to an immediate (and far-reaching) generalisation. Take
N = 2â„“âˆ’1 and let Ï‰ be a primitive element of ï¬eld Fâˆ—
2â„“â‰ƒF2[X]/âŸ¨g(X)âŸ©where
g(X) is a primitive polynomial. (In all the examples and problems from this chapter,
this requirement is fulï¬lled.) Consider a deï¬ning set of roots, to start with, of the
form Ï‰, Ï‰2, Ï‰3, but more generally, Ï‰, Ï‰2,...,Ï‰(Î´âˆ’1). (Using parameter Î´ which
is an integer > 3 is a tradition here.) Consider the cyclic code with these roots:
what can we say about it? With the length N = 2â„“âˆ’1, we can guess that it will
yield a subcode of the Hamming [2â„“âˆ’1,2â„“âˆ’1âˆ’â„“] code, and it may correct more
than a single error. This is the gist of the so-called (binary) BCH code construction
(Boseâ€“Choudhury, Hocquenguem, 1959).
In this section we restrict ourselves to a brief introduction to the BCH codes;
in greater detail and generality these codes are discussed in Section 3.2. For
N = 2â„“âˆ’1 ï¬eld F2â„“â‰ƒF2[X]/âŸ¨g(X)âŸ©has the property that its non-zero elements
are the Nth roots of unity (i.e. the zeros of the polynomial 1+XN). In other words,

236
Introduction to Coding Theory
polynomial 1 + XN factorises into the product of linear factors
âˆ
1â‰¤jâ‰¤N
(X âˆ’Ï‰ j)
where all Ï‰ j list the whole of Fâˆ—
2â„“. (In the terminology of Section 3.1, F2â„“is the
splitting ï¬eld for 1 + XN over F2.) As before, we use the notation Ï‰ := X for the
generator of the multiplicative cyclic group Fâˆ—
2â„“. (In fact, it could be any generator
of this group.)
Because Ï‰N = 1 and the power N is minimal with this property, the element
Ï‰ is often called a primitive Nth root of unity. Consequently, the powers Ï‰k for
0 â‰¤k < N yield distinct elements of the ï¬eld. This fact is used below when we
conclude that the product
âˆ
1â‰¤i< jâ‰¤Î´âˆ’1

Ï‰k j âˆ’Ï‰ki

Ì¸= 0
for every collection of powers Ï‰k1,...,Ï‰kÎ´âˆ’1. (Such a collection extracts a
(Î´ âˆ’1)Ã—(Î´ âˆ’1) submatrix from the (Î´ âˆ’1)Ã—N parity-check matrix in the proof
of Theorem 2.5.39.)
Deï¬nition 2.5.37
Given N = 2â„“âˆ’1 and Î´ = 3,...N, deï¬ne a narrow-sense bi-
nary BCH code X BCH
N,Î´
of length N and with designed distance Î´ as the cyclic code
formed by binary polynomials a(X) of degree < N such that
a(Ï‰) = a

Ï‰2
= Â·Â·Â· = a

Ï‰(Î´âˆ’1)
= 0.
(2.5.19)
In other words, X BCH
N,Î´
is the cyclic code of length N whose generator g(X) is the
minimal binary polynomial with roots including Ï‰, Ï‰2,...,Ï‰(Î´âˆ’1):
g(X) = lcm
(
(X âˆ’Ï‰),...,(X âˆ’Ï‰(Î´âˆ’1))
)
= lcm {MÏ‰(X),...,MÏ‰(Î´âˆ’1)(X)}.
(2.5.20)
Here lcm stands for the least common multiple and MÎ±(X) denotes the minimal
binary polynomial with root Î±. For brevity, we will use in this chapter the term
binary BCH codes. (A more general class of BCH codes will be introduced in
Section 3.2.)
Example 2.5.38
For N = 7, the appropriate polynomial ï¬eld is F2[X]/âŸ¨1+X +
X3âŸ©or F2[X]/âŸ¨1 + X2 + X3âŸ©, i.e. one of two realisations of ï¬eld F8. Since 7 is a
prime number, any non-zero polynomial from the ï¬eld has the multiplicative order
7, i.e. is a generator of the multiplicative group in F2[X]/âŸ¨1+X2 +X3âŸ©. In fact, we
have the decomposition of polynomial 1+X7 into irreducible factors:
1+X7 = (1+X)(1+X +X3)(1+X2 +X3).
Further, if we choose polynomial ï¬eld F2[X]/âŸ¨1+X +X3âŸ©then Ï‰ = X satisï¬es
Ï‰3 = 1+Ï‰,

Ï‰23 = 1+Ï‰2,

Ï‰43 = 1+Ï‰4,

2.5 Cyclic codes and polynomial algebra. Introduction to BCH codes
237
i.e. the conjugates Ï‰, Ï‰2 and Ï‰4 are the roots of the core polynomial 1+X +X3:
1+X +X3 = (X âˆ’Ï‰)

X âˆ’Ï‰2
X âˆ’Ï‰4
.
Next, Ï‰3, Ï‰6 and Ï‰12 = Ï‰5 are the roots of 1+X2 +X3:
1+X2 +X3 =

X âˆ’Ï‰3
X âˆ’Ï‰5
X âˆ’Ï‰6
.
Hence, the binary BCH code of length 7 with designed distance 3 is formed by
binary polynomials a(X) of degree â‰¤6 such that
a(Ï‰) = a(Ï‰2) = 0, that is, a(X) is a multiple of 1+X +X3.
This code is equivalent to the Hamming [4,7] code; in particular its â€˜trueâ€™ distance
equals 3.
Next, the binary BCH code of length 7 with designed distance 4 is formed by
binary polynomials a(X) of degree â‰¤6 such that
a(Ï‰) = a(Ï‰2) = a(Ï‰3) = 0, that is, a(X) is a multiple of
(1+X +X3)(1+X2 +X3) = 1+X +X2 +X3 +X4 +X5 +X6.
This is simply the repetition code R7.
The staple of the theory of the BCH codes is
Theorem 2.5.39
(The BCH bound) The minimal distance of a binary BCH code
with designed distance Î´ is â‰¥Î´.
The proof of Theorem 2.5.39 (sometimes referred to as the BCH theorem) is
based on the following result.
Lemma 2.5.40
Consider the mÃ—m Vandermonde determinant with entries from
a commutative ring:
det
â›
âœ
âœ
âœ
â
Î±1
Î±2
...
Î±m
Î±2
1
Î±2
2
...
Î±2
m
...
...
...
...
Î±m
1
Î±m
2
...
Î±m
m
â
âŸ
âŸ
âŸ
â = det
â›
âœ
âœ
âœ
â
Î±1
Î±2
1
...
Î±m
1
Î±2
Î±2
2
...
Î±m
2
...
...
...
...
Î±m
Î±2
m
...
Î±m
m
â
âŸ
âŸ
âŸ
â .
(2.5.21)
The value of this determinant is
âˆ
1â‰¤lâ‰¤m
Î±l Ã— âˆ
1â‰¤i< jâ‰¤m
(Î±i âˆ’Î± j).
(2.5.22)
Proofof Lemma 2.5.40 Both determinants in (2.5.21) are polynomial expressions
in Î±1,...,Î±m. If Î± = Î± j for i < j then the determinant has repeated rows (columns),
and hence vanishes (as in the standard arithmetic). Hence, the determinant divides

238
Introduction to Coding Theory
the product
âˆ
1â‰¤i< jâ‰¤m
(Î±i âˆ’Î± j). Next, we compare the powers of Î±i in (2.5.21) and
(2.5.22): this immediately leads to the assertion of Lemma 2.5.40.
Proofof Theorem 2.5.39 Let the polynomial a(X) âˆˆX . Then a(âˆ—Ï‰âˆ—j) = 0 for all
j = 1,...,Î´ âˆ’1. That is,
â›
âœ
âœ
âœ
â
1
Ï‰
Ï‰âˆ—2
...
Ï‰âˆ—(Nâˆ’1)
1
Ï‰âˆ—2
Ï‰âˆ—4
...
Ï‰âˆ—2(Nâˆ’1)
...
...
...
...
1
Ï‰âˆ—(Î´âˆ’1)
Ï‰âˆ—2(Î´âˆ’1)
...
Ï‰âˆ—(Nâˆ’1)(Î´âˆ’1)
â
âŸ
âŸ
âŸ
â 
â›
âœ
âœ
âœ
â
a0
a1
...
aNâˆ’1
â
âŸ
âŸ
âŸ
â = 0.
Due to Lemma 2.5.40, any (Î´ âˆ’1) columns of this ((Î´ âˆ’1)Ã—N) matrix are linearly
independent. Hence, there must be at least Î´ non-zero coefï¬cients in a(X). Thus,
the distance of X is â‰¥Î´.
Example 2.5.41
(Here a mistake in [18], p. 106, is corrected.) Consider a BCH
code with N = 15 and Î´ = 5. Use the following decomposition into irreducible
polynomials:
X15 âˆ’1 = (X +1)(X2 +X +1)(X4 +X +1)(X4 +X3 +1)
Ã—(X4 +X3 +X2 +X +1).
The generator of the code is
g(x) = (X4 +X +1)(X4 +X3 +X2 +X +1) = X8 +X7 +X6 +X4 +1.
Indeed, g(Ï‰3) = g(Ï‰9) = 0. The set of zeros of X4 + X3 + X2 + X + 1 is
(Ï‰3,Ï‰9,Ï‰12,Ï‰9). The set of zeros of X4 + X + 1 is (Ï‰,Ï‰2,Ï‰4,Ï‰8). The set of
zeros of X4 + X3 + 1 is (Ï‰7,Ï‰14,Ï‰13,Ï‰11). The set of zeros of X2 + X + 1 is
(Ï‰5,Ï‰10).
(b) Let N = 31 and Ï‰ be a primitive element of F32. The minimal polynomial with
root Ï‰ is
MÏ‰(X) = (X âˆ’Ï‰)(X âˆ’Ï‰2)(X âˆ’Ï‰4)(X âˆ’Ï‰8)(X âˆ’Ï‰16).
We ï¬nd also the minimal polynomial for Ï‰5:
MÏ‰5(X) = (X âˆ’Ï‰5)(X âˆ’Ï‰10)(X âˆ’Ï‰20)(X âˆ’Ï‰9)(X âˆ’Ï‰18).
By deï¬nition, the generator of the BCH code of length 31 with a designed dis-
tance Î´ = 8 is g(X) = lcm(MÏ‰(X),MÏ‰3(X),MÏ‰5(X),MÏ‰7(X)). In fact, the mini-
mal distance of the BCH code (which is, obviously, at least 9) is in fact at least 11.
This follows from Theorem 2.5.39 because all the powers Ï‰,Ï‰2,...,Ï‰10 are listed
among the roots of g(X).

2.5 Cyclic codes and polynomial algebra. Introduction to BCH codes
239
There exists a decoding procedure for a BCH code which is simple to imple-
ment: it generalises the Hamming code decoding procedure. In view of Theorem
2.5.39, the BCH code with designed distance Î´ corrects at least t =
AÎ´ âˆ’1
2
B
er-
rors. Suppose a codeword c = c0 ...cNâˆ’1 has been sent and corrupted to r = c+e
where e = e0 ...eNâˆ’1. Assume that e has at most t non-zero entries. Introduce the
corresponding polynomials c(X), r(X) and e(X), all of degrees < N. For c(X) we
have that c(Ï‰) = c(Ï‰2) = Â·Â·Â· = c(Ï‰(Î´âˆ’1)) = 0. Then, clearly,
r(Ï‰) = e(Ï‰), r

Ï‰2
= e

Ï‰2
,...,r

Ï‰(Î´âˆ’1)
= e

Ï‰(Î´âˆ’1)
.
(2.5.23)
So, we calculate r(Ï‰i) for i = 1,...,Î´ âˆ’1. If these are all 0, r(X) âˆˆX (no error
or at least t +1 errors). Otherwise, let E = {i : ei = 1} indicate the erroneous digits
and assume that 0 < â™¯E â‰¤t. Introduce the error locator polynomial
Ïƒ(X) = âˆ
iâˆˆE
(1âˆ’Ï‰iX),
(2.5.24)
with binary coefï¬cients, of degree â™¯E and with the lowest coefï¬cient 1. If we know
Ïƒ(X), we can ï¬nd which powers Ï‰âˆ’i are its roots and hence ï¬nd the erroneous
digits i âˆˆE. We then simply change these digits and correct the errors.
In order to calculate Ïƒ(X), consider the formal power series
Î¶(X) = âˆ‘
jâ‰¥1
e

Ï‰ j
X j.
(Observe that, as Ï‰N = 1, the coefï¬cients of this power series recur.) For the initial
(Î´ âˆ’1) coefï¬cients, we have equalities, by virtue of (2.5.23):
e

Ï‰ j
= r

Ï‰ j
, j = 1,...,Î´ âˆ’1;
these are the only ones needed for our purpose, and they are calculated in terms of
the received word r.
Now set
Ï‰(X) = âˆ‘
iâˆˆE
Ï‰iX âˆ
jâˆˆE: jÌ¸=i
(1âˆ’Ï‰ jX).
(2.5.25)
Next, rewrite the above formal series as
Î¶(X) = âˆ‘
jâ‰¥1âˆ‘
iâˆˆE
Ï‰i jX j = âˆ‘
iâˆˆE âˆ‘
jâ‰¥1
Ï‰i jX j = âˆ‘
iâˆˆE
Ï‰iX
1âˆ’Ï‰iX = Ï‰(X)
Ïƒ(X).
(2.5.26)
Observe that both polynomials Ï‰(X) and Ïƒ(X) are of degree â™¯E â‰¤t.
Now, the equation Î¶(X)Ïƒ(X) = Ï‰(X) from (2.5.26) can be written in terms of
the coefï¬cients, with the help of the fact that
e

Ï‰ j
= r

Ï‰ j
, j = 1,...,2t;

240
Introduction to Coding Theory
namely,

Ïƒ0 +Ïƒ1X +Â·Â·Â·+ÏƒtXt
Ã—

r(Ï‰)X ++Â·Â·Â·+r

Ï‰2t
X2t +e

Ï‰(2t+1)
X2t+1 +Â·Â·Â·

= Ï‰0 +Ï‰1X +Â·Â·Â·+Ï‰tXt.
(2.5.27)
We are interested in the coefï¬cients of Xk for t < k â‰¤2t: these satisfy
âˆ‘
0â‰¤jâ‰¤t
Ïƒ jr

Ï‰(kâˆ’j)
= 0,
(2.5.28)
which does not involve any of the terms e

Ï‰l
. We obtain the following equations:
â›
âœ
âœ
âœ
â
r

Ï‰(t+1)
r(Ï‰t)
...
r(Ï‰)
r

Ï‰(t+2)
r

Ï‰(t+1)
...
r(Ï‰2)
...
...
...
...
r

Ï‰(2t)
r

Ï‰(2tâˆ’1)
...
r(Ï‰t)
â
âŸ
âŸ
âŸ
â 
â›
âœ
âœ
âœ
â
Ïƒ0
Ïƒ1
...
Ïƒt
â
âŸ
âŸ
âŸ
â = 0.
The above matrix is t Ã—(t +1), so it always has a non-zero vector in the kernel;
this vector identiï¬es the error locator polynomial Ïƒ(X). We see that the above
routine (called the Berlekampâ€“Massey decoding algorithm) enables us to specify
the set E and hence correct â‰¤t errors.
Unfortunately, the BCH codes are asymptotically â€˜badâ€™: for any sequence of
BCH codes of length N â†’âˆ, either k/N or d/N â†’0. In other words, they lie
at the bottom of Figure 2.2. To obtain codes that meet the Gilbertâ€“Varshamov
(GV) bound, one needs more powerful methods, based on algebraic geometry. Such
codes were constructed in the early 1970s (the Goppa and Justesen codes). It re-
mains a problem to construct codes that lie above the Gilbertâ€“Varshamov curve.
As was mentioned just before on page 160, a new class of codes was invented in
1982 by Tsfasman, VlË‡adut and Zink; these codes lie above the GV curve when the
number of symbols in the code alphabet is large. However, for binary codes, the
problem is still waiting for solution.
Worked Example 2.5.42
Compute the rank and minimum distance of the cyclic
code with generator polynomial g(X) = X3 + X + 1 and parity-check polynomial
h(X) = X4 + X2 + X + 1. Now let Ï‰ be a root of g(X) in the ï¬eld F8. We receive
the word r(X) = X5 + X3 + X (mod X7 âˆ’1). Verify that r(Ï‰) = Ï‰4, and hence
decode r(X) using minimum-distance decoding.
Solution A cyclic code X of length N has generator polynomial g(X) âˆˆF2[X]
and parity-check polynomial h(X) âˆˆF2[X] with g(X)h(X) = 1 + XN. Recall that
if g(X) has degree k, i.e. g(X) = a0 + a1X + Â·Â·Â· + akXk where ak Ì¸= 0, then g(X),

2.5 Cyclic codes and polynomial algebra. Introduction to BCH codes
241
Xg(X),...,XNâˆ’kâˆ’1g(X) form a basis for X . In particular, the rank of X equals
N âˆ’k. In this example, N = 7, k = 3 and rank(X ) = 4.
If h(X) = b0 +b1X +Â·Â·Â·+bNâˆ’kXNâˆ’k then the parity-check matrix H for X has
the form
â›
âœ
âœ
âœ
âœ
âœ
âœ
âœ
â
bNâˆ’k
bNâˆ’kâˆ’1
...
b1
b0
0
...
0
0
0
bNâˆ’k
bNâˆ’kâˆ’1
...
b1
b0
...
0
0
0
...
...
...
...
...
...
0
0
...
0
bNâˆ’k
bNâˆ’kâˆ’1
...
b1
b0
â
âŸ
âŸ
âŸ
âŸ
âŸ
âŸ
âŸ
â 
:
;<
=
.
N
The codewords of X are linear dependence relations between the columns of H.
In the example,
H =
â›
â
1
0
1
1
1
0
0
0
1
0
1
1
1
0
0
0
1
0
1
1
1
â
â 
and we have the following implications:
no zero column
â‡’
no codewords of weight 1,
no repeated column
â‡’
no codewords of weight 2.
The minimum distance d(X ) of a linear code X is the minimum non-zero weight
of a codeword. In the example, d(X ) = 3. [In fact, X is equivalent to the Ham-
ming [7,4] code.]
Since g(X) âˆˆF2[X] is irreducible, the code X âˆˆF2[X]
>
âŸ¨X7 âˆ’1âŸ©is the cyclic
code deï¬ned by Ï‰. The multiplicative cyclic group ZÃ—
7 of non-zero elements of
ï¬eld F8 is
Ï‰0 = 1, Ï‰, Ï‰2, Ï‰3 = Ï‰ +1, Ï‰4 = Ï‰2 +Ï‰,
Ï‰5 = Ï‰3 +Ï‰2 = Ï‰2 +Ï‰ +1, Ï‰6 = Ï‰3 +Ï‰2 +Ï‰ = Ï‰2 +1,
Ï‰7 = Ï‰3 +Ï‰ = 1.
Next, the value r(Ï‰) is
r(Ï‰) = Ï‰ +Ï‰3 +Ï‰5
= Ï‰ +(Ï‰ +1)+(Ï‰2 +Ï‰ +1)
= Ï‰2 +Ï‰ = Ï‰4,

242
Introduction to Coding Theory
as required. Let c(X) = r(X) + X4 mod (X7 âˆ’1). Then c(Ï‰) = 0, i.e. c(X) is a
codeword. Since d(X ) = 3 the code is 1-error correcting. We just found a code-
word c(X) at distance 1 from r(X). Then r(X) = X +X3+X5 should be decoded by
c(X) = X +X3 +X4 +X5 mod (X7 âˆ’1)
under minimum-distance decoding.
We conclude this section with two useful statements.
Worked Example 2.5.43
(The Euclid algorithm for polynomials) The Euclid al-
gorithm is a method for computing the greatest common divisor of two polynomi-
als, f(X) and g(X), over the same ï¬nite ï¬eld F. Assume that degg(X) â‰¤deg f(X)
and set f(X) = râˆ’1(X), g(X) = r0(X). Then
(1) divide râˆ’1(X) by r0(X):
râˆ’1(X) = q1(X)r0(X)+r1(X) where degr1(X) < degr0(X),
(2) divide r0(X) by r1(X):
r0(X) = q2(X)r1(X)+r2(X) where degr1(X) < degr1(X),
...
(k) divide rkâˆ’1(X) by rkâˆ’1(X):
rkâˆ’2(X) = qk(X)rkâˆ’1(X)+rk(X) where degrk(X) < degRkâˆ’1(X),
...
The algorithm continues until the remainder is 0:
(s) divide rsâˆ’2(X) by rsâˆ’1(X):
rsâˆ’2(X) = qs(X)rsâˆ’1(X).
Then
gcd

f(X),g(X)

= rsâˆ’1(X).
(2.5.29)
At each stage, the equation for the current remainder rk(X) involves two previous
remainders. Hence, all remainders, including gcd( f(X),g(X)), can be written in
terms of f(X) and g(X). In fact,
Lemma 2.5.44
The remainders rk(X) in the Euclid algorithm satisfy
rk(X) = ak(X) f(X)+bk(X)g(X), k â‰¤âˆ’1,

2.6 Additional problems for Chapter 2
243
where
aâˆ’1(X) = bâˆ’1(X) = 0,
a0(X) = 0,b0(X) = 1,
ak(X) = âˆ’qk(X)akâˆ’1(X)+akâˆ’2(X),k â‰¥1,
bk(X) = âˆ’qk(X)bkâˆ’1(X)+bkâˆ’2(X),k â‰¥1.
In particular, there exist polynomials a(X),b(X) such that
gcd( f(X),g(X)) = a(X) f(X)+b(X)g(X).
Furthermore:
(1) degak(X) =
âˆ‘
2â‰¤iâ‰¤k
degqi(X), degbk(X) =
âˆ‘
1â‰¤iâ‰¤k
degqk(X).
(2) degrk(X) = deg f(X)âˆ’
âˆ‘
1â‰¤iâ‰¤k+1
degqk(X).
(3) degbk(X) = deg f(X)âˆ’degrkâˆ’1(X).
(4) ak(X)bk+1(X)âˆ’ak+(X)bk(X) = (âˆ’1)k+1.
(5) ak(X) and bk(X) are co-prime.
(6) rk(X)bk+1(X)âˆ’rk+1(X)bk(X) = (âˆ’1)k+1 f(X).
(7) rk+1(X)ak(X)âˆ’rk(X)ak+1(X) = (âˆ’1)k+1g(X).
Proof
The proof is left as an exercise.
2.6 Additional problems for Chapter 2
Problem 2.1
A check polynomial h(X) of a binary cyclic code X of length N
is deï¬ned by the condition a(X) âˆˆX if and only if a(X)h(X) = 0 mod (1+XN).
How is the check polynomial related to the generator of X ? Given h(X), construct
the parity-check matrix and interpret the cosets X +y of X .
Describe all cyclic codes of length 16 and 15. Find the generators and the check
polynomials of the repetition and parity-check codes. Find the generator and the
check polynomial of Hamming code of length 7.
Solution All cyclic codes of length 16 are divisors of 1+X16 = (1+X)16, i.e. are
generated by g(X) = (1 + X)k where k = 0,1,...,16. Here k = 0 gives the whole
{0,1}16, k = 1 the parity-check code, k = 15 the repetition code {00...0,11...1}
and k = 16 the zero code. For N = 15, the decomposition into irreducible polyno-
mials looks as follows:
1+X15 = (1+X)(1+X +X2)(1+X +X4)(1+X3 +X4)
Ã—(1+X +X2 +X3 +X4).
Any product of the listed irreducible polynomials generates a cyclic code.

244
Introduction to Coding Theory
In general, 1+ XN = (1 + X)(1 + X + Â·Â·Â·+ XNâˆ’1); g(X) = 1 + X generates the
parity-check code and g(X) = 1 + X + Â·Â·Â· + XNâˆ’1 the repetition code. In the case
of a Hamming [7,4] code, the generator is g(X) = 1+X +X3, by inspection.
The check polynomial h(X) equals the ratio (1 + XN)

g(X). In fact, for all
a(X) âˆˆX , a(X)h(X) = v(X)g(X)h(X) = v(X)(1+XN) = 0 mod (1+XN). Con-
versely, if a(X)h(X) = v(X)(1+XN) then a(X) must be of the form v(X)g(X), by
the uniqueness of the irreducible decomposition.
The cosets y + X are in a one-to-one correspondence with the remainders
y(X) = u(X) mod g(X). In other words, two words y(1),y(2) belong to the same
coset iff, in the division algorithm representation,
y(i)(X) = vi(X)g(X)+u(i)(X), i = 1,2, where u(1)(X) = u(2)(X).
In fact, y(1) and y(2) belong to the same coset iff y(1) +y(2) âˆˆX . This is equivalent
to u(1)(X)+u(2)(X) = 0, i.e. u(1)(X) = u(2)(X).
If we write h(X) =
k
âˆ‘
j=0
h jX j, then the dot-product
i
âˆ‘
j=0
g jhiâˆ’j =
'
1,
i = 0,N,
0,
1 â‰¤i < N.
So, âŸ¨g(X) Â· hâŠ¥(X)âŸ©= 0 where hâŠ¥(X) = hk + hkâˆ’1X + Â·Â·Â· + h0Xk. Therefore, the
parity-check matrix H for X is formed by rows that are cyclic shifts of h =
hk hkâˆ’1 Â·Â·Â·h00Â·Â·Â·0. The check polynomials for the repetition and parity-check
codes then are 1 + X and 1 + X + Â·Â·Â· + XNâˆ’1, and they are dual of each other.
The check polynomial for the Hamming [7,4] code equals 1 + X + X2 + X4, by
inspection.
Problem 2.2
(a) Prove the Hamming and Gilbertâ€“Varshamov bounds on the
size of a binary [N,d] code in terms of vN(d), the volume of an N-dimensional
Hamming ball of radius d.
Suppose that the minimum distance is âŒŠÎ»NâŒ‹for some ï¬xed Î» âˆˆ(0,1/4). Let
Î±(N,âŒŠÎ»NâŒ‹) be the largest information rate of any binary code correcting âŒŠÎ»NâŒ‹
errors. Show that
1âˆ’Î·(Î») â‰¤liminf
Nâ†’âˆÎ±(N,âŒŠÎ»NâŒ‹) â‰¤limsup
Nâ†’âˆ
Î±(N,âŒŠÎ»NâŒ‹) â‰¤1âˆ’Î·(Î»/2).
(2.6.1)
(b) Fix R âˆˆ(0,1) and suppose we want to send one of a collection UN of messages
of length N, where the size â™¯UN = 2NR. The message is transmitted through an

2.6 Additional problems for Chapter 2
245
MBSC with error-probability p < 1/2, so that we expect about pN errors. Accord-
ing to the asymptotic bound of part (a), for which values of p can we correct pN
errors, for large N?
Solution (a) A code X âŠ‚FN
2 is said to be E-error correcting if B(x,E)âˆ©B(y,E) =
/0 for all x,y âˆˆX with x Ì¸= y. The Hamming bound for a code of size M, distance
d, correcting E = âŒŠd âˆ’1
2
âŒ‹errors is as follows. The balls of radius E about the
codewords are disjoint: their total volume equals M Ã— vN(E). But their union lies
inside FN
2 , so M â‰¤2N/vN(E).
On the other hand, take an E-correcting code X âˆ—of maximum size â™¯X . Then
there will be no word
y âˆˆFN
2 \âˆªxâˆˆX âˆ—B(x,2E +1)
or we could add such a word to X âˆ—, increasing the size but preserving the error-
correcting property. Since every word y âˆˆFN
2 is less than d âˆ’1 from a codeword,
we can add y to the code. Hence, balls of radius d âˆ’1 cover the whole of FN
2 , i.e.
M Ã—vN(d âˆ’1) â‰¥2N, or
M â‰¥2N/vN(d âˆ’1) (the Varshamovâ€“Gilbert bound).
Combining these bounds yields, for Î±(N,E) =

logâ™¯X

N:
1âˆ’logvN(2E +1)
N
â‰¤Î±(N,E) â‰¤1âˆ’logvN(E)
N
.
Observe that for any s < ÎºN with 0 < Îº < 1/2
 N
sâˆ’1

=
s
N âˆ’s+1
N
s

<
Îº
1âˆ’Îº
N
s

.
Consequently,
N
E

â‰¤vN(E) â‰¤
N
E
 E
âˆ‘
j=0

Îº
1âˆ’Îº
 j
.
Now, by the Stirling formula as N,E â†’âˆand E/N â†’Î» âˆˆ(0,1/4)
1
N log
N
E

â†’Î·(Î»/2).
So, we proved that limNâ†’âˆ1
N logvN([Î»N]) = Î·(Î»), and
1âˆ’Î·(Î») â‰¤liminf
Nâ†’âˆ
1
N logM â‰¤limsup
Nâ†’âˆ
1
N logM â‰¤1âˆ’Î·(Î»/2).

246
Introduction to Coding Theory
(b) We can correct pN errors if the minimum distance d satisï¬es
Ad âˆ’1
2
B
â‰¥pN,
i.e. Î»/2 â‰¥p. Using the asymptotic Hamming bound we obtain R â‰¤1âˆ’Î·(Î»/2) â‰¤
1âˆ’Î·(p). So, the reliable transmission is possible if p â‰¤Î·âˆ’1(1âˆ’R),
The Shannon SCT states:
capacity C of a memoryless channel = sup
pX
I(X : Y).
Here I(X : Y) = h(Y) âˆ’h(Y|X) is the mutual entropy between the single-letter
random input and output of the channel, maximised over all distributions of the
input letter X. For an MBSC with the error-probability p, the conditional entropy
h(Y|X) equals Î·(p). Then
C = sup
pX
h(Y)âˆ’Î·(p).
But h(Y) attains its maximum 1, by using the equidistributed input X (then Y is also
equidistributed). Hence, for the MBSC, C = 1âˆ’Î·(p). So, a reliable transmission is
possible via MBSC with R â‰¤1âˆ’Î·(p), i.e. p â‰¤Î·âˆ’1(1âˆ’R). These two arguments
lead to the same answer.
Problem 2.3
Prove that the binary code of length 23 generated by the polynomial
g(X) = 1+X +X5 +X6 +X7 +X9 +X11 has minimum distance 7, and is perfect.
Hint: Observe that by the BCH bound (see Theorem 2.5.39) if a generator polyno-
mial of a cyclic code has roots {Ï‰,Ï‰2,...,Ï‰Î´âˆ’1} then the code has distance â‰¥Î´,
and check that X23 +1 â‰¡(X +1)g(X)grev(X) mod 2, where grev(X) = X11g(1/X)
is the reversal of g(X).
Solution First, show that the code is BCH, of designed distance 5. Recall that if
Ï‰ is a root of a polynomial p(X) âˆˆF2[X] then so is Ï‰2. Thus, if Ï‰ is a root of
g(X) = 1 + X + X5 + X6 + X7 + X9 + X11 then so are Ï‰2, Ï‰4, Ï‰8, Ï‰16, Ï‰9, Ï‰18,
Ï‰13, Ï‰3, Ï‰6, Ï‰12. This yields the design sequence {Ï‰,Ï‰2,Ï‰3,Ï‰4}. By the BCH
theorem, the code X = âŸ¨g(X)âŸ©has distance â‰¥5.
Next, the parity-check extension, X +, is self-orthogonal. To check this, we need
only to show that any two rows of the generating matrix of X + are orthogonal.
These are represented by
(Xig(X)|1) and (X jg(X)|1)

2.6 Additional problems for Chapter 2
247
and their dot-product is
1+(Xig(X))(X jg(X)) = 1+âˆ‘
r gi+rg j+r = 1+âˆ‘
r gi+rgrev
11âˆ’jâˆ’r
= 1+coefï¬cient of X11+iâˆ’j in g(X)Ã—grev(X)
:
;<
=
||
1+Â·Â·Â·+X22
= 1+1 = 0.
So,
any two words in X + are dot-orthogonal.
(2.6.2)
This implies that all words in X + have weight divisible by 4. Indeed, by in-
spection, all rows (Xig(X)|1) of the generating matrix of X + have weight 8.
Then, by induction on the number of rows involved in the sum, if c âˆˆX + and
g(i) âˆ¼(Xig(X)|1) is a row of the generating matrix of X + then
w

g(i) +c

= w

g(i)
+w(c)âˆ’2w

g(i) âˆ§c

,
where

g(i) âˆ§c

l = min
	
g(i)
l,cl

, l = 1,...,24. We know that 8|w

g(i)
and by
the induction hypothesis, 4|w(c). Next, w

g(i) âˆ§c

is even, so 2w

g(i) âˆ§c

is divis-
ible by 4. Then the LHS, w

g(i) + c

, is divisible by 4. Therefore, the distance of
X + is 8, as it is â‰¥5 and is divisible by 4. (Clearly, it cannot be bigger than 8 as
then it would be 12.) Then the distance of the original code, X , equals 7.
Finally, the code X is perfect 3-error correcting, since the volume of the 3-ball
in F23
2 equals
23
0

+
23
1

+
23
2

+
23
3

= 1+23+253+1771 = 2048 = 211,
and 212 Ã—211 = 223. Here, obviously, 12 represents the rank and 23 the length.
Problem 2.4
Show that the Hamming code is cyclic with check polynomial
X4 +X2 +X +1. What is its generator polynomial? Does Hammingâ€™s original code
contain a subcode equivalent to its dual? Let the decomposition into irreducible
monic polynomials Mj(X) be
XN +1 =
l
âˆ
j=1
Mj(X)k j.
(2.6.3)
Prove that the number of cyclic code of length N is âˆl
j=1(kj +1).
Solution In F7
2 we have
X7 âˆ’1 = (X3 +X +1)(X4 +X2 +X +1).

248
Introduction to Coding Theory
The cyclic code with generator g(X) = X3 + X + 1 has check polynomial h(X) =
X4 +X2 +X +1. The parity-check matrix of the code is
â›
â
1
0
1
1
1
0
0
0
1
0
1
1
1
0
0
0
1
0
1
1
1
â
â .
(2.6.4)
The columns of this matrix are the non-zero elements of F3
2. So, this is equivalent
to Hammingâ€™s original [7,4] code.
ThedualofHammingâ€™s[7,4]codehasthegeneratorpolynomialX4 +X3 +X2 +1
(the reverse of h(X)). Since X4 + X3 + X2 + 1 = (X + 1)g(X), it is a subcode of
Hammingâ€™s [7,4] code.
Finally, any irreducible polynomial Mj(X) could be included in a generator of a
cyclic code in any power 0,...,kj. So, the number of possibilities to construct this
generator equals âˆl
j=1(kj +1).
Problem 2.5
Describe the construction of a Reedâ€“Muller code. Establish its
information rate and its distance.
Solution The space Fm
2 has N = 2m points. If A âŠ†Fm
2 , let 1A be the indicator
function of A. Consider the collection of hyperplanes
Î  j = {p âˆˆFm
2 : p j = 0}.
Set h j = 1Î  j, j = 1,...,m, and h0 = 1Fm
2 â‰¡1. Deï¬ne sets of functions Fm
2 â†’F2:
A0 = {h0},
A1 = {h j ; j = 1,2,...,m},
A2 = {hi Â·h j ;i, j = 1,2,...,m,i < j},
...
Ak+1 = {aÂ·hj ;a âˆˆAk, j = 1,2,...,m, hj Ì¸ |a},
...
Am = {h1 Â·Â·Â·hm}.
The union of these sets has cardinality N = 2m (there are 2m functions altogether).
Therefore, functions from âˆªm
i=0Ai can be taken as a basis in FN
2 .
Then the Reedâ€“Muller code RM(r,m) = X RM
r,m
of length N = 2m is deï¬ned as
the span of âˆªr
i=0Ai and has rank
r
âˆ‘
i=0
m
i

. Its information rate is
1
2m
r
âˆ‘
i=0
m
i

.

2.6 Additional problems for Chapter 2
249
Next, if a âˆˆRM(r,m) then
a = (y,y)hj +(x,x) = (x,x+y),
for some x âˆˆRM(m âˆ’1,r) and y âˆˆRM(m âˆ’1,r âˆ’1). Thus, RM(m,r) coincides
with the bar-product (R(mâˆ’1,r)|R(mâˆ’1,r âˆ’1)). By the bar-product bound,
d
	
RM(m,k)

â‰¥min

2d
	
RM(mâˆ’1,k)

,d
	
RM(mâˆ’1,k âˆ’1)


,
which, by induction, yields
d
	
RM(r,m)

â‰¥2mâˆ’r.
On the other hand, the vector h1 Â· h2 Â· Â·Â·Â· Â· hm is at distance 2mâˆ’r from RM(m,r).
Hence,
d
	
RM(r,m)

= 2mâˆ’r.
Problem 2.6
(a) Deï¬ne a parity-check code of length N over the ï¬eld F2. Show
that a code is linear iff it is a parity-check code. Deï¬ne the original Hamming code
in terms of parity-checks and then ï¬nd a generating matrix for it.
(b) Let X be a cyclic code. Deï¬ne the dual code
X âŠ¥= {y = y1 ...yN :
N
âˆ‘
i=1
xiyi = 0 for all x = x1 ...xN âˆˆX }.
Prove that X âŠ¥is cyclic and establish how the generators of X and X âŠ¥are re-
lated to each other. Show that the repetition and parity-check codes are cyclic, and
determine their generators.
Solution (a) The parity-check code X PC of a (not necessarily linear) code X is
the collection of vectors y = y1 ...yN âˆˆFN
2 such that the dot-product
yÂ·x =
N
âˆ‘
i=1
xiyi = 0 (in F2),
for all x = x1 ...xN âˆˆX .
From the deï¬nition it is clear that X PC is also the parity-check code for X , the
linear code spanned by X : X PC = X
PC. Indeed, if y Â· x = 0 and y Â· xâ€² = 0 then
y Â· (x + xâ€²) = 0. Hence, the parity-check code X PC is always linear, and it forms
a subspace dot-orthogonal to X . Thus, a given code X is linear iff it is a parity-
check code. A pair of linear codes X and X PC form a dual pair: X PC is the dual
of X and vice versa. The generating matrix H for X PC serves as a parity-check
matrix for X and vice versa.

250
Introduction to Coding Theory
The Hamming code of length N = 2l âˆ’1 is the one whose check matrix is l Ã—N
and lists all non-zero columns from Fl
2 (in some agreed order). So, the Hamming
[7,4] code corresponds to l = 3; its parity-checks are
x1 +x3 +x5 +x7 = 0,
x2 +x3 +x6 +x7 = 0,
x4 +x5 +x6 +x7 = 0,
and the generating matrix equals
â›
âœ
âœ
â
1
1
0
1
0
0
0
0
1
1
0
1
0
0
0
0
1
1
0
1
0
0
0
0
1
1
0
1
â
âŸ
âŸ
â .
(b) The generator of dual code gâŠ¥(X) = XNâˆ’1g(Xâˆ’1). The repetition code has
g(X) = 1+X +Â·Â·Â·+XNâˆ’1 and the rank 1. The parity-check code has g(X) = 1+X
and the rank N âˆ’1.
Problem 2.7
(a) How does coding theory apply when the error rate p > 1/2?
(b) Give an example of a code which is not a linear code.
(c) Give an example of a linear code which is not a cyclic code.
(d) Deï¬ne the binary Hamming code and its dual. Prove that the Hamming code is
perfect. Explain why the Hamming code cannot always correct two errors.
(e) Prove that in the dual code:
(i) The weight of any non-zero codeword equals 2â„“âˆ’1.
(ii) The distance between any pair of words equals 2â„“âˆ’1.
Solution (a) If p > 1/2, we reverse the output to get pâ€² = 1âˆ’p.
(b) The code X âŠ‚F2
2 with X = {11} is not linear as 00 Ì¸âˆˆX .
(c) The code X âŠ‚F2
2 with X = {00,10} is linear, but not cyclic, as 01 Ì¸âˆˆX .
(d) The original Hamming [7,4] code has distance 3 and is perfect one-error
correcting. Thus, making two errors in a codeword will always lead outside the
ball of radius 1 about the codeword, i.e. to a ball of radius 1 about a different
codeword (at distance 1 of the nearest, at distance 2 from the initial word). Thus,
one detects two errors but never corrects them.
(e) The dual of a Hamming [2â„“âˆ’1,2â„“âˆ’â„“âˆ’1,3] code is linear, of length N = 2â„“âˆ’1
and rank â„“, and its generating matrix is â„“Ã— (2â„“âˆ’1), with columns listing all non-
zero vectors of length â„“(the parity-check matrix of the original code). The rows of
this matrix are linearly independent; moreover, any row i = 1,...,â„“has 2â„“âˆ’1 digits
1. This is because each such digit comes from a column, i.e. a non-zero vector of
length â„“, with 1 in position i; there are exactly 2â„“âˆ’1 such vectors. Also any pair of

2.6 Additional problems for Chapter 2
251
columns of this matrix are linearly independent, but there are triples of columns
that are linearly dependent (a pair of columns complemented by their sum).
Every non-zero dual codeword x is a sum of rows of the above generating matrix.
Suppose these summands are rows i1,...,is where 1 â‰¤i1 < Â·Â·Â· < is â‰¤â„“. Then, as
above, the number of digits 1 in the sum equals the number of columns of this
matrix for which the sum of digits i1,...,is is 1. We have no restriction on the
remaining â„“âˆ’s digits, so for them there are 2â„“âˆ’s possibilities. For digits i1,...,is
we have 2sâˆ’1 possibilities (a half of the total of 2s). Thus, again 2â„“âˆ’s Ã—2sâˆ’1 = 2â„“âˆ’1.
We proved that the weight of every non-zero dual codeword equals 2â„“âˆ’1. That is,
the distance from the zero vector to any dual codeword is 2â„“âˆ’1. Because the dual
code is linear, the distance between any pair of distinct dual codewords x,xâ€² equals
2â„“âˆ’1:
Î´(x,xâ€²) = Î´(0,xâ€² âˆ’x) = w(xâˆ’xâ€²) = 2â„“âˆ’1.
Let J âŠ‚{1,...,â„“} be the set of contributing rows:
x = âˆ‘
iâˆˆJ
g(i).
Then Î´(0,x) = â™¯of non-zero digits in x is calculated as
2â„“âˆ’|J|
Ã—

â™¯of subsets K âŠ†J with |K| odd

â†‘
â†‘
â™¯of ways to place
â™¯of ways to get âˆ‘
lâˆˆJ
xi = 1
mod 2
0s and 1s outside J
with xi = 0 or 1
which yields 2â„“âˆ’|J| 2|J|âˆ’1 = 2â„“âˆ’1. In other words, to get a contribution from a digit
xj = âˆ‘
iâˆˆJ
g(i)
j = 1, we must ï¬x (i) a conï¬guration of 0s and 1s over {1,...,â„“}\J (as it
is a part of the description of a non-zero vector of length N), and (ii) a conï¬guration
of 0s and 1s over J, with an odd number of 1s.
To check that d

X HâŠ¥
= 2â„“âˆ’1, it sufï¬ces to establish that the distance between
the zero word and any other word x âˆˆX HâŠ¥equals 2â„“âˆ’1.
Problem 2.8
(a) What is a necessary and sufï¬cient condition for a polynomial
g(X) to be the generator of a cyclic code of length N? What is the BCH code?
Show that the BCH code associated with {Ï‰,Ï‰2}, where Ï‰ is a root of X3 +X +1
in an appropriate ï¬eld, is Hammingâ€™s original code.
(b) Deï¬ne and evaluate the Vandermonde determinant. Deï¬ne the BCH code and
obtain a good estimate for its minimum distance.

252
Introduction to Coding Theory
Solution (a) The necessary and sufï¬cient condition for g(X) being the generator of
a cyclic code of length N is g(X)|(XN âˆ’1). The generator g(X) may be irreducible
or not; in the latter case it is represented as a product g(X) = M1(X)Â·Â·Â·Mk(X)
of its irreducible factors, with k â‰¤d = degg. Let s be the minimal number such
that N|2s âˆ’1. Then g(X) is factorised into the product of ï¬rst-degree monomials
in a ï¬eld K = F2s âŠ‡F2: g(X) =
d
âˆ
i=1
(X âˆ’Ï‰ j) with Ï‰1,...,Ï‰d âˆˆK. [Usually one
refers to the minimal ï¬eld â€“ the splitting ï¬eld for g, but this is not necessary.] Each
element Ï‰i is a root of g(X) and also a root of at least one of its irreducible factors
M1(X),...,Mk(X). [More precisely, each Mi(X) is a sub-product of the above ï¬rst-
degree monomials.]
We want to select a deï¬ning set D of roots among Ï‰1,...,Ï‰d âˆˆK: it is a collec-
tion comprising at least one root Ï‰ ji for each factor Mi(X). One is naturally tempted
to take a minimal deï¬ning set where each irreducible factor is represented by one
root, but this set may not be easy to describe exactly. Obviously, the cardinality |D|
of deï¬ning set D is between k and d. The roots forming D are all from ï¬eld K but
in fact there may be some from its subï¬eld, Kâ€² âŠ‚K containing all Ï‰ ji. [Of course,
F2 âŠ‚Kâ€².] We then can identify the cyclic code X generated by g(X) with the set
of polynomials
*
f(X) âˆˆF2[X]/âŸ¨XN âˆ’1âŸ©: f(Ï‰) = 0
for all Ï‰ âˆˆD
+
.
It is said that X is a cyclic code with deï¬ning set of roots (or zeros) D.
(b) A binary BCH code of length N (for N odd) and designed distance Î´ is a cyclic
code with deï¬ning set {Ï‰,Ï‰2,...,Ï‰Î´âˆ’1} where Î´ â‰¤N and Ï‰ is a primitive Nth
root of unity, with Ï‰N = 1. It is helpful to note that if Ï‰ is a root of a polyno-
mial p(X) then so are Ï‰2,Ï‰4,...,Ï‰2sâˆ’1. By considering a deï¬ning set of the form
{Ï‰,Ï‰2,...,Ï‰Î´âˆ’1} we â€˜ï¬ll the gapsâ€™ in the above diadic sequence and produce an
ideal of polynomials whose properties can be analytically studied.
The simplest example is where N = 7 and D = {Ï‰,Ï‰2} where Ï‰ is a root of
X3 +X +1. Here, Ï‰7 = (Ï‰3)2Ï‰ = (Ï‰ +1)2Ï‰ = Ï‰3 +Ï‰ = 1, so Ï‰ is a 7th root of
unity. [We used the fact that the characteristic is 2.] In fact, it is a primitive root.
Also, as was said, Ï‰2 is a root of X3 +X +1: (Ï‰2)3 +Ï‰2 +1 = (Ï‰3 +Ï‰ +1)2 =
0, and so is Ï‰4. Then the cyclic code with deï¬ning set {Ï‰,Ï‰2} has generator
X3+X +1 since all roots of this polynomial are engaged. We know that it coincides
with the Hamming [7,4] code.
The Vandermonde determinant is
Î” = det
â›
âœ
âœ
â
1
1
1
...
1
x1
x2
x3
...
xn
...
...
...
...
...
xnâˆ’1
1
xnâˆ’1
2
xnâˆ’1
3
...
xnâˆ’1
n
â
âŸ
âŸ
â .

2.6 Additional problems for Chapter 2
253
Observe that if xi = xj(i Ì¸= j) the determinant vanishes (two rows are the same).
Thus xi âˆ’xj is a factor of Î”,
Î” = P(x)âˆ
i< j
(xi âˆ’xj),
with P a polynomial in x1,...,xn. Now consider terms in expansion Î” in the sum
of terms of form aâˆi xm(i)
i
with âˆ‘m(i) = 0 + 1 + Â·Â·Â· + (n âˆ’1) = n(n âˆ’1)/2. But
âˆi< j(xiâˆ’xj) is a sum of terms aâˆi xm(i)
i
with âˆ‘m(i) = n(nâˆ’1)/2, so P(x) = const.
Considering x2x2
3 ...xnâˆ’1
n
we have const = 1, so
Î” = âˆ
i< j
(xi âˆ’xj).
(2.6.5)
Suppose N is odd and K is a ï¬eld containing F2 in which XN âˆ’1 factorises into
linear factors. [This ï¬eld can be selected as F2s where N|2s âˆ’1.] A cyclic code
consisting of words c = c0c1 ...cNâˆ’1 with
Nâˆ’1
âˆ‘
j=0
cjÏ‰r j = 0 for all r = 1,2,...,Î´ âˆ’1
where Ï‰ is a primitive Nth root of unity is called a BCH code of design distance
Î´ < N. Next, X BCH is a vector space over F2 and c âˆˆX BCH iff
cHT = 0
(2.6.6)
where
H =
â›
âœ
âœ
âœ
âœ
âœ
â
1
Ï‰
Ï‰2
...
Ï‰Nâˆ’1
1
Ï‰2
Ï‰4
...
Ï‰2Nâˆ’2
1
Ï‰3
Ï‰6
...
Ï‰3Nâˆ’3
...
...
...
...
...
1
Ï‰Î´âˆ’1
Ï‰2Î´âˆ’2
...
Ï‰(Nâˆ’1)(Î´âˆ’1)
â
âŸ
âŸ
âŸ
âŸ
âŸ
â 
.
(2.6.7)
Now rankH = Î´. Indeed, by (2.6.5) for any Î´ Ã—Î´ minor  H
det  H = âˆ
i< j
(Ï‰i âˆ’Ï‰ j) Ì¸= 0.
Thus (2.6.6) tells us that
c âˆˆX ,c Ì¸= 0 â‡’âˆ‘|c j| â‰¥Î´.
So, the minimum distance in X BCH is not smaller than Î´.
Problem 2.9
A subset X of the Hamming space {0,1}N of cardinality â™¯X = M
and with the minimal Hamming distance d = min[Î´(x,xâ€²) : x,xâ€² âˆˆX ,x Ì¸= xâ€²]
is called an [N,M,d] code (not necessarily linear). An [N,M,d] code is called
maximal if it is not contained in any [N,M + 1,d] code. Prove that an [N,M,d]
code is maximal if and only if for any y âˆˆ{0,1}N there exists x âˆˆX such that

254
Introduction to Coding Theory
Î´(x,y) < d. Conclude that if d or more changes are made in a codeword then the
new word is closer to some other codeword than to the original one.
Suppose that a maximal [N,M,d] code is used for transmitting information via a
binary memoryless channel with the error-probability p, and the receiver uses the
maximum likelihood decoder. Prove that the probability of erroneous decoding,
Ï€ML
err , obeys the bounds
1âˆ’b(N,d âˆ’1) â©½Ï€ML
err â©½1âˆ’b(N,âŒŠ(d âˆ’1)/2âŒ‹),
where b(N,m) is a partial binomial sum
b(N,m) = âˆ‘
0â‰¤kâ‰¤m
N
k

pk(1âˆ’p)Nâˆ’k.
Solution If a code is maximal then adding one more word will reduce the distance.
Hence, for all y there exists x âˆˆX such that Î´(x,y) < d. Conversely, if this prop-
erty holds then the code cannot be enlarged without reducing d. Then making d or
more changes in a codeword gives a word that is closer to a different codeword.
This will certainly not give the correct guess under the ML decoder as it chooses
the closest codeword.
Therefore,
Ï€ML
err â‰¥âˆ‘
dâ‰¤kâ‰¤N
N
k

pk(1âˆ’p)Nâˆ’k = 1âˆ’b(N,d âˆ’1).
On the other hand, the code corrects
	
(d âˆ’1)

2

errors. Hence,
Ï€ML
err â‰¤1âˆ’b(N,âŒŠd âˆ’1/2âŒ‹).
Problem 2.10
The Plotkin bound for an [N,M,d] binary code states that M â‰¤
d
d âˆ’N/2 if d > N/2. Let Mâˆ—
2(N,d) be the maximum size of a code of length N and
distance d, and let
Î±(Î») = lim
Nâ†’âˆ
1
N log2 Mâˆ—
2(N,âŒŠÎ»NâŒ‹).
Deduce from the Plotkin bound that Î±(Î») = 0 for Î» â‰¥1
2.
Assuming the above bound, show that if d â‰¤N/2, then
M â‰¤2Nâˆ’(2dâˆ’1)
d
d âˆ’(2d âˆ’1)/2 = 2d 2Nâˆ’(2dâˆ’1).
Deduce the asymptotic Plotkin bound: Î±(Î») â‰¤1âˆ’2Î», 0 â‰¤Î» < 1
2.

2.6 Additional problems for Chapter 2
255
Solution
If d > N/2 apply the Plotkin bound and conclude that Î±(Î») = 0. If
d â‰¤N/2 consider the partition of a code X of length N and distance d â‰¤N/2
according to the last N âˆ’(2d âˆ’1) digits, i.e. divide X into disjoint subsets, with
ï¬xed N âˆ’(2d âˆ’1) last digits. One of these subsets, X â€², must have size Mâ€² such
that Mâ€²2Nâˆ’(2dâˆ’1) â‰¥M.
Hence, X â€² is a code of length Nâ€² = 2d âˆ’1 and distance dâ€² = d, with dâ€² > Nâ€²/2.
Applying Plotkinâ€™s bound to X â€² gives
Mâ€² â‰¤
dâ€²
dâ€² âˆ’N/2 =
d
d âˆ’(2d âˆ’1)/2 = 2d.
Therefore,
M â‰¤2Nâˆ’(2dâˆ’1)2d.
Taking d = âŒŠÎ»NâŒ‹with N â†’âˆyields Î±(Î») â‰¤1âˆ’2Î»,0 â‰¤Î» â‰¤1/2.
Problem 2.11
State and prove the Hamming, Singleton and Gilbertâ€“Varshamov
bounds. Give (a) examples of codes for which the Hamming bound is attained, (b)
examples of codes for which the Singleton bound is attained.
Solution The Hamming bound states that the size M of an E-error correcting code
X of length N,
M â‰¤
2N
vN(E),
where vN(E) =
âˆ‘
0â‰¤iâ‰¤E
 N
i

is the volume of an E-ball in the Hamming space
{0,1}N. It follows from the fact that the E-balls about the codewords x âˆˆX must
be disjoint:
M Ã— vN(E)
= â™¯of points covered by M E-balls
â‰¤2N = â™¯of points in {0,1}N.
The Singleton bound is that the size M of a code X of length N and distance d
obeys
M â‰¤2Nâˆ’d+1.
It follows by observing that truncating X (i.e. omitting a digit from the codewords
x âˆˆX ) d âˆ’1 times still does not merge codewords (i.e. preserves M) while the
resulting code ï¬ts in {0,1}Nâˆ’d+1.
The Gilbertâ€“Varshamov bound is that the maximal size Mâˆ—= Mâˆ—
2(N,d) of a
binary [N,d] code satisï¬es
Mâˆ—â‰¥
2N
vN(d âˆ’1) .

256
Introduction to Coding Theory
This bound follows from the observation that any word y âˆˆ{0,1}N must be within
distance â‰¤d âˆ’1 from a maximum-size code X âˆ—. So,
Mâˆ—Ã— vN(d âˆ’1) â‰¥â™¯of points within distance d âˆ’1 = 2N.
Codes attaining the Hamming bound are called perfect codes, e.g. the Hamming
[2â„“âˆ’1,2â„“âˆ’1âˆ’â„“,3] codes. Here, E = 1, vN(1) = 1+2â„“âˆ’1 = 2â„“and M = 22â„“âˆ’â„“âˆ’1.
Apart from these codes, there is only one example of a (binary) perfect code: the
Golay [23,12,7] code.
Codes attaining the Singleton bound are called maximum distance separable
(MDS): their check matrices have any N âˆ’M rows linearly independent. Examples
of such codes are (i) the whole {0,1}N, (ii) the repetition code {0...0, 1...1}
and the collection of all words x âˆˆ{0,1}N of even weight. In fact, these are all
examples of binary MDS codes. More interesting examples are provided by Reedâ€“
Solomon codes that are non-binary; see Section 3.2. Binary codes attaining the
Gilbertâ€“Varshamov bound for general N and d have not been constructed so far
(though they have been constructed for non-binary alphabets).
Problem 2.12
(a) Explain the existence and importance of error correcting codes
to a computer engineer using Hammingâ€™s original code as your example.
(b) How many codewords in a Hamming code are of weight 1? 2? 3? 4? 5?
Solution (a) Consider the linear map F7
2 â†’F3
2 given by the matrix H of the form
(2.6.4). The Hamming code X is the kernel kerH, i.e. the collection of words
x = x1x2x3x4x5x6x7 âˆˆ{0,1}7 such that xHT = 0. Here, we can choose four digits,
say x4, x5, x6, x7, arbitrarily from {0,1}; then x1, x2, x3 will be determined:
x1 = x4 +x5 +x7,
x2 = x4 +x6 +x7,
x3 = x5 +x6 +x7.
It means that code X can be used for encoding 16 binary â€˜messagesâ€™ of length 4.
If y = y1y2y3y4y5y6y7 differs from a codeword x âˆˆX in one place, say y = x+ek
then the equation yHT = ekHT gives the binary decomposition of number k, which
leads to decoding x. Consequently, code X allows a single error to be corrected.
Suppose that the probability of error in any digit is p << 1, independently of
what occurred to other digits. Then the probability of an error in transmitting a
non-encoded (4N)-digit message is
1âˆ’(1âˆ’p)4N â‰ƒ4Np.

2.6 Additional problems for Chapter 2
257
But using the Hamming code we need to transmit 7N digits. An erroneous trans-
mission requires at least two wrong digits, which occurs with probability
â‰ˆ1âˆ’

1âˆ’
7
2

p2
N
â‰ƒ21Np2 << 4Np.
So, the extra effort of using 3 check digits in the Hamming code is justiï¬ed.
(b) A Hamming code XH,â„“of length N = 2â„“âˆ’1 (â„“â‰¥3) consists of binary words
x = x1 ... xN such that xHT = 0 where H is an â„“Ã— N matrix whose columns
h(1),...,h(N) are all non-zero binary vectors of length l. Hence, the number of
codewords of weight w(x) =
N
âˆ‘
j=1
xj = s equals the number of (non-ordered) collec-
tions of s binary, non-zero, pair-wise distinct â„“-vectors of total sum 0. In fact, if
xHT = 0 and w(x) = s and xj1 = xj2 = Â·Â·Â· = xjs = 1 then the sum of row-vectors
h( j1) +Â·Â·Â·+h( js) = 0.
Thus, one codeword has weight 0, no codeword has weight 1 or 2, N(N âˆ’1)/3!
codewords have weight 3 (i.e. 7 and 35 words of weights 3 for l = 3 and l = 4).
Further we have [N(N âˆ’1)(N âˆ’2) âˆ’N(N âˆ’1)]/4! = N(N âˆ’1)(N âˆ’3)/4! words
of weight 4 (i.e. 7 and 105 words of weights 4 for â„“= 3 and â„“= 4). Finally, we
have N(N âˆ’1)(N âˆ’3)(N âˆ’7)/5! words weight 5 (i.e. 0 and 168 words of weight
5 for â„“= 3 and â„“= 4). Each time when we add a factor, we should avoid â„“-vectors
equal to a linear combination of previously selected vectors. In Problem 3.9 we
will compute the enumerator polynomial for N = 15:
1+35X3 +105X4 +168X5 +280X6 +435X7 +435X8
+280X9 +168X10 +105X11 +35X12 +X15.
Problem 2.13
(a) The dot-product of vectors x,y from a binary Hamming space
HN is deï¬ned as x Â· y = âˆ‘N
i=1 xiyi (mod 2), and x and y are said to be orthogonal
if x Â· y = 0. What does it mean to say that X âŠ†HN is a linear [N,k] code with
generating matrix G and parity-check matrix H? Show that
X âŠ¥= {x âˆˆHN : xÂ·y = 0
for all y âˆˆX }
is a linear [N,N âˆ’k] code and ï¬nd its generator and parity-check matrices.
(b) A linear code X is called self-orthogonal if X âŠ†X âŠ¥. Prove that X is self-
orthogonal if the rows of G are self and pairwise orthogonal. A linear code is called
self-dual if X = X âŠ¥. Prove that a self-dual code has to be an [N,N/2] code (and
hence N must be even). Conversely, prove that a self-orthogonal [N,N/2] code, for
N even, is self-dual. Give an example of such a code for any even N and prove that
a self-dual code always contains the word 1...1.

258
Introduction to Coding Theory
(c) Consider now a Hamming [2â„“âˆ’1,2â„“âˆ’â„“âˆ’1] code XH,â„“. Describe the generating
matrix of X âŠ¥
H,â„“. Prove that the distance between any two codewords in X âŠ¥
H,â„“equals
2â„“âˆ’1.
Solution By deï¬nition, X âŠ¥is preserved under the linear operations; hence X âŠ¥
is a linear code. From algebraic considerations, dim X âŠ¥= N âˆ’k. The generating
matrix GâŠ¥of X âŠ¥coincides with H, and the parity-check matrix HâŠ¥with G.
If X âŠ†X âŠ¥then the rows g(1),...,g(k) of G are self- and pairwise orthogonal.
The converse is also true. From the previous observation, if X is self-dual then
k = N âˆ’k, i.e. k = N/2, and N should be even. Similarly, if X is self-orthogonal
and k = N/2 then X is self-dual.
Let 1 = 1...1. If X = X âŠ¥then 1Â·g(i) = g(i) Â·g(i) = 0. So, 1 âˆˆX âŠ¥and hence
1 âˆˆX . An example is a code with the generating matrix
G
=
â›
âœ
âœ
âœ
âœ
âœ
â
1
1
1
...
1
1
...
1
1
1
0
...
0
1
...
0
1
0
1
...
0
1
...
0
...
...
...
...
...
...
...
...
1
0
0
...
1
1
...
1
â
âŸ
âŸ
âŸ
âŸ
âŸ
â 
â«
âª
âª
âª
âª
âª
â¬
âª
âª
âª
âª
âª
â­
N/2.
â†
N/2
â†’
â†N/2 â†’
The dual XHâŠ¥of a Hamming code XH is called a simplex code. By the above,
it has length 2â„“âˆ’1 and rank â„“, and its generating matrix GHâŠ¥is â„“Ã—(2â„“âˆ’1), with
columns listing all non-zero vectors of length â„“. To check that dist X âŠ¥
H = 2â„“âˆ’1,
it sufï¬ces to establish that the weight of non-zero word x âˆˆX âŠ¥
H equals 2â„“âˆ’1. But
a non-zero word x âˆˆX âŠ¥
H is a non-zero linear combination of rows of GâŠ¥
H. Let
J âŠ‚{1,...,â„“} be the set of contributing rows:
x = âˆ‘
iâˆˆJ
g(i).
Clearly, w(g(i)) = 2â„“âˆ’1 as exactly half of all 2â„“vectors have 1 on any given position.
The proof is ï¬nished by induction on â™¯J.
A simple and elegant way is to use the MacWilliams identity (cf. Lemma 3.4.4)
which immediately gives
WX âŠ¥(s) = 1+(2â„“âˆ’1)s2â„“âˆ’1.
(2.6.8)
It is instructive to present this derivation. We will establish in Problem 3.9 the
formula for a weight enumeration polynomial of Hamming code. Then substituting

2.6 Additional problems for Chapter 2
259
this expression into the MacWilliams identity one gets
WX âŠ¥(s) =
1
2Nâˆ’â„“

1
N +1

1+ 1âˆ’s
1+s
N
+
N
N +1

1+ 1âˆ’s
1+s
(Nâˆ’1)/2 
1âˆ’1âˆ’s
1+s
(N+1)/2 
(1+s)N
= 2â„“
 1
2â„“+ 2â„“âˆ’1
2â„“
s2â„“âˆ’1
which is equivalent to (2.6.8).
Problem 2.14
Describe brieï¬‚y the decoding procedure for the Hamming [2â„“âˆ’1,
2â„“âˆ’1âˆ’â„“] code.
The codewords of the Hamming [7,4] code, with the lexicographical parity-
check matrix H of the form (2.3.4a), are used for encoding 16 symbols, the ï¬rst 15
letters of the alphabet and the space character âˆ—. The encoding rule is
A
0011001
E
0111100
I
1010101
M
1111111
B
0100101
F
0001111
J
1100110
N
1000011
C
0010110
G
1101001
K
0101010
O
0000000
D
1110000
H
0110001
L
1001100
âˆ—
1011010
You have received a 105-digit message
1000110
0000000
0110001
1000011
1000011
1110101
0111100
0011010
0100101
0111100
1011000
1101001
0000000
0010000
1010000
where some words are corrupted. Decode the received message.
Solution The Hamming [2â„“âˆ’1,2â„“âˆ’1âˆ’â„“] code, â„“= 2,3,..., is obtained as a col-
lection of binary â€˜stringsâ€™ x = x1 ...xN of length N = 2â„“âˆ’1 such that xHT = 0.
Matrix H is (â„“Ã—2â„“âˆ’1), with 2â„“âˆ’1 non-zero binary strings as columns; that is,
H =
â›
âœ
âœ
â
1
0
...
0
1
...
1
0
1
...
0
0
...
1
...
...
...
...
...
...
...
0
0
...
1
1
...
1
â
âŸ
âŸ
â .
Here the columns are meant to be lexicographically ordered. Different matrices
obtained from the above by permuting the rows deï¬ne different, but equivalent,
codes: they are all named Hamming codes.
To perform decoding, we have to ï¬x a matrix H (the check matrix) and let it
be known to both the sender and the receiver. Upon receiving a word (string) y =
y1 ...yN we form a syndrome vector yHT. If yHT = 0, we decode y by itself. (We

260
Introduction to Coding Theory
have no means to determine if the original codeword was corrupted by the channel
or not.)
If yHT Ì¸= 0 then yHT coincides with a column of H. Suppose yHT gives column
j of H; then we decode y by
xâˆ—= y+ej where ej = 0...1...0 (1 in digit j).
In other words, we change digit j in y and decide that it was the word sent through
the channel. This works well when errors in the channel are rare.
If â„“= 3 a Hamming [7,4] code contains 24 = 16 codewords. These codewords
are ï¬xed when H is ï¬xed: in the example they are used for encoding 15 letters from
A to O and the space character âˆ—. Upon receiving a message we divide it into words
of length 7: in the example there are 15 words altogether. Performing the decoding
procedure leads to
JOHNNIEâˆ—BEâˆ—GOOD
Problem 2.15
A (binary) Hamming code of length N = 2â„“âˆ’1, where â„“â‰¥2, is
deï¬ned as a linear binary code with a parity-check matrix H whose columns consist
of all non-zero binary vectors of length â„“. Find the rank of such a code (i.e. the
dimension of the corresponding linear subspace) and the number of the codewords.
Find the minimum distance for the code and prove that it is single-error correcting.
Prove that the code is perfect (i.e. the union of the one-balls around the codewords
covers the space of all words).
Give a parity-check matrix and a generating matrix for a Hamming code with â„“=
3. What is the information rate of this code? Why is the case â„“= 2 not interesting?
Solution The parity-check matrix H for the Hamming code is â„“Ã—2â„“âˆ’1 and formed
by all non-zero columns of length â„“; in particular, it includes all l columns of
weight 1. The latter are linearly independent; hence the l columns of H are lin-
early independent. Since XHam = kerH, we have dimX = 2â„“âˆ’1 âˆ’â„“= rankX .
The number of codewords then equals 22â„“âˆ’â„“âˆ’1.
Since all columns of H are distinct, any pair of columns are linearly independent.
So, the minimal distance of X is > 2. But X contains three columns that are
linearly dependent, e.g.
1 0 0 ... 0T,
0 1 0 ... 0T,
and
1 1 0 ... 0T.
Hence, the minimal distance equals 3. Therefore, if a single error occurs, i.e. the
received word is at distance 1 from a codeword, then this codeword is uniquely
determined. Hence, the Hamming code is single-error correcting.

2.6 Additional problems for Chapter 2
261
To prove that it is perfect, we must check that
the â™¯of codewords Ã— the volume of a one-ball
= the total â™¯of words.
In fact, denoting 2l âˆ’1 = N, we have
the â™¯of codewords = 22lâˆ’1âˆ’l = 2Nâˆ’l,
the volume of a one-ball =
N
0

+
N
1

= 1+N,
the total â™¯of words = 2N,
and
(1+N)2Nâˆ’â„“= 2â„“2Nâˆ’â„“= 2N.
The information rate of the code equals
rank

length = 2â„“âˆ’â„“âˆ’1
2â„“âˆ’1
.
The code with â„“= 3 has the 3 Ã— 7 parity-check matrix of the form (2.6.4); any
permutation of rows leads to an equivalent code. The generating matrix is 4Ã—7:
â›
âœ
âœ
â
1
0
0
0
1
1
1
0
1
0
0
0
1
1
0
0
1
0
1
0
1
0
0
0
1
0
1
1
â
âŸ
âŸ
â 
and the information rate 4/7. The Hamming code with â„“= 2 is trivial: it contains
a single non-zero codeword 1 1 1.
Problem 2.16
Deï¬ne a BCH code of length N over the ï¬eld Fq with designed
distance Î´. Show that the minimum weight of such a code is at least Î´.
Consider a BCH code of length 31 over the ï¬eld F2 with designed distance 8.
Show that the minimum distance is at least 11.
Solution A BCH code of length N over the ï¬eld Fq is deï¬ned as a cyclic code X
whose minimum degree generator polynomial g(X) âˆˆFq[X], with g(X)|(XN âˆ’1)
(and hence degg(X) â‰¤N), contains among its roots the subsequent powers Ï‰,
Ï‰2,...,Ï‰Î´âˆ’1 where Ï‰ âˆˆFqs is a primitive Nth root of unity. (This root Ï‰ lies
in an extension ï¬eld Fqs â€“ the splitting ï¬eld for XN âˆ’1 over Fq, i.e. N|qs âˆ’1.) Then
Î´ is called the designed distance for X ; the actual distance (which may be difï¬cult
to calculate in a general situation) is â‰¥Î´.
If we consider the binary BCH code X of length 31, Ï‰ should be a primitive
root of unity of degree 31, with Ï‰31 = 1 (the root Ï‰ lies in an extension ï¬eld F32).

262
Introduction to Coding Theory
We know that in the binary arithmetic, if a polynomial f(X) âˆˆF2[X], of order s,
has a root Ï‰, it has roots Ï‰2,Ï‰4,...,Ï‰2sâˆ’1, i.e.

X âˆ’Ï‰2r f(X), r = 0,...,sâˆ’1.
Thus, given that the generator g(X) of X has roots Ï‰, Ï‰2, Ï‰3, Ï‰4, Ï‰5, Ï‰6, Ï‰7, it
will also have roots
Ï‰8 = (Ï‰4)2, Ï‰9 = (Ï‰5)8, and Ï‰10 = (Ï‰5)2.
That is, the deï¬ning set can be extended to
Ï‰, Ï‰2, Ï‰3, Ï‰4, Ï‰5, Ï‰6, Ï‰7, Ï‰8, Ï‰9, Ï‰10
(all these elements are distinct, as Ï‰ is a primitive 31st root of unity). In fact, code
X has designed distance â‰¥11. Hence, the minimum distance in X is â‰¥11.
Problem 2.17
Let X be a linear [N,k,d] code over the binary ï¬eld F2, and
G be a generating matrix of X , with k rows and N columns, such that exactly
d of the ï¬rst rowâ€™s entries are 1. Let G1 be the matrix, of k âˆ’1 rows and N âˆ’d
columns, formed by deleting the ï¬rst row of G and those columns of G with a
non-zero entry in the ï¬rst row. Show that X1, the linear code generated by G1,
has minimum distance dâ€² â‰¥âŒˆd/2âŒ‰. Here, for a real number x, âŒˆxâŒ‰is the integer
satisfying x â‰¤âŒˆxâŒ‰< x+1.
Show also that X1 has rank k âˆ’1. Deduce that
N â‰¥
âˆ‘
0â‰¤iâ‰¤kâˆ’1
3
d

2i4
.
Solution Let x be the codeword in X represented by the ï¬rst row of G and pick
a pair of other rows, say y and z. After the ï¬rst deleting they become yâ€² and zâ€²,
correspondingly. Both weights w(yâ€²) and w(zâ€²) must be â‰¥âŒˆd/2âŒ‰: otherwise at least
one of the original words y and z, say y, would have had minimum âŒˆd/2âŒ‰digits 1
among deleted d digits (as w(y) â‰¥d by condition). But then
w(x+y) = w(yâ€²)+d âˆ’âŒˆd/2âŒ‰< d
which contradicts the condition that the distance of X is d.
We want to check that the weight w(yâ€² +zâ€²) â‰¥âŒˆd/2âŒ‰. Assume the opposite:
w(yâ€² +zâ€²) = m < âŒˆd/2âŒ‰.
Then mâ€² = w(y0 +z0) must be â‰¥d âˆ’m â‰¥âŒˆd/2âŒ‰where y0 is the deleted part of y,
of length d, and z0 is the deleted part of z, also of length d. In fact, as before, if
mâ€² < d âˆ’m then w(y+z) < d which is impossible. But if mâ€² â‰¥d âˆ’m then
w(x+y+z) = d âˆ’mâ€² +m < d,
again impossible. Hence, the sum of any two rows of G1 has weight â‰¥âŒˆd/2âŒ‰.

2.6 Additional problems for Chapter 2
263
This argument can be repeated for the sum of any number of rows of G1 (not
exceeding k âˆ’1). In fact, in the case of such a sum x+y+Â·Â·Â·+z, we can pass to
new matrices,  G and  G1, with this sum among the rows. We conclude that X1 has
minimum distance dâ€² â‰¥âŒˆd/2âŒ‰. The rank of X1 is k âˆ’1, for any k âˆ’1 rows of G1
are linearly independent. (The above sum cannot be 0.)
Now, the process of deletion can be applied to X1 (you delete dâ€² columns in
G1 yielding digits 1 in a row of G1 with exactly dâ€² digits 1). And so on, until you
exhaust the initial rank k by diminishing it by 1. This leads to the required bound
N â‰¥d +âŒˆd/2âŒ‰+
3
d/224
+Â·Â·Â·+

d

2kâˆ’1
.
Problem 2.18
Deï¬ne a cyclic linear code X and show that it has a codeword of
minimal length which is unique, under normalisation to be stated. The polynomial
g(X) whose coefï¬cients are the symbols of this codeword is the (minimum degree)
generator polynomial of this code: prove that all words of the code are related to
g(X) in a particular way.
Show further that g(X) can be the generator polynomial of a cyclic code with
words of length N iff it satisï¬es a certain condition, to be stated.
There are at least three ways of determining the parity-check matrix of the code
from a knowledge of the generator polynomial. Explain one of them.
Solution Let X be the cyclic code of length N with generator polynomial g(X) =
âˆ‘
0â‰¤iâ‰¤d
giXi of degree d. Without loss of generality, assume the code is non-trivial,
with 1 < d < N âˆ’1. Let g denote the corresponding codeword g0 ...gd0...0 (there
are d +1 coefï¬cients gi completed with N âˆ’d âˆ’1 zeros). Then:
(a) g(X)|(XN âˆ’1), i.e. g(X)h(X) = XN âˆ’1 for some polynomial h(X) =
âˆ‘
0â‰¤iâ‰¤k
hiXi
of degree k = N âˆ’d;
(b) a string a = a0 ...aNâˆ’1 âˆˆX iff the polynomial a(X) =
âˆ‘
0â‰¤iâ‰¤Nâˆ’1
aiXi has the
form a(X) = f(X)g(X) mod (XN âˆ’1);
(c) a string g and its cyclic shifts Ï€g,...,Ï€kâˆ’1g (corresponding to polynomials
g(X),Xg(X),...,Xkâˆ’1g(X)) form a basis in X .
By virtue of (a), g0 = h0 = 1 and the sum
âˆ‘
0â‰¤iâ‰¤l
gihlâˆ’i representing the lth coefï¬cient
of g(X)h(X) is equal to 0, for all l = 1,...,N âˆ’1. By virtue of (c), the rank of X
equals k.

264
Introduction to Coding Theory
One way to specify the parity-check matrix is to take the ratio (XN âˆ’1)

g(X) =
h(X) = h0 +h1X +Â·Â·Â·+hkXk. Then form the N Ã—(N âˆ’k) matrix
H =
â›
âœ
âœ
â
hk
hkâˆ’1
...
0
...
0
0
0
hk
hkâˆ’1
...
h1
...
0
...
...
...
...
...
...
...
0
0
...
hk
...
h1
h0
â
âŸ
âŸ
â .
(2.6.9)
The rows of H are the cyclic shifts

Ï€ jhâ†“
, 0 â‰¤j â‰¤d âˆ’1 = N âˆ’kâˆ’1, of the string
hâ†“= hk ...h00...0.
We claim that for all a âˆˆX , aHT = 0. In fact, it sufï¬ces to check that for the
basis words Ï€ jg, Ï€ jgHT = 0, j = 0,...,k âˆ’1. That is, the dot-product
Ï€ j1gÂ·Ï€ j2hâ†“= 0, 0 â‰¤j1 < k, 0 â‰¤j2 < N âˆ’k âˆ’1.
(2.6.10)
But for j1 = k âˆ’1 and j2 = 0, we have
Ï€kâˆ’1gÂ·hâ†“= g0hk +g1hkâˆ’1 = 0
since it gives the ï¬rst coefï¬cient (at monomial X) of the product g(X)h(X)=
XN âˆ’1. Similarly, for j1 = kâˆ’2 and j2 = 0, Ï€kâˆ’2gÂ·hâ†“gives the second coefï¬cient
of g(X)h(X) (at monomial X2) and is again equal to 0. And so on: for j1 = j2 = 0,
gÂ·hâ†“= 0 as the kth-degree coefï¬cient in g(X)h(X).
Continuing, gÂ·Ï€hâ†“equals the (k +1)st-degree coefï¬cient in g(X)h(X), gÂ·Ï€2hâ†“
the (k+2)nd, and so on; gÂ·Ï€Nâˆ’kâˆ’1hâ†“= gdâˆ’1hk +gdhkâˆ’1 the (N âˆ’1)st. As before,
they all vanish.
The same holds true when we simultaneously shift both words cyclically (when
possible) which leads to (2.6.10).
Conversely, suppose that aHT = 0 for some word a = a0 ...aNâˆ’1. Write the cor-
responding polynomial a(X) as a(X) = f(X)g(X) + r(X) where the ratio f(X) =
âˆ‘
0â‰¤iâ‰¤kâˆ’1
fiXi and r(X) is the remainder. Then either r(X) = 0 or 1 â‰¤deg r(X) =
dâ€² < d (and rdâ€² = 1 and rl = 0 for dâ€² < l â‰¤nâˆ’1). Then set r = r0 ...rdâ€².
Assume that r(X) Ì¸= 0. By the above argument,
(i) aHT = rHT and hence rHT = 0,
(ii) the entries of vector rHT coincide with the coefï¬cients of the product
r(X)h(X), beginning with r0hk + Â·Â·Â· + rdâ€²hkâˆ’dâ€² and ending with rdâ€²hk. So, these
coefï¬cients must be 0. But the equality rdâ€²hk = 0 is impossible since rdâ€² = hk = 1.
Hence, r(X) = 0 and a(X) = f(X)g(X), i.e. a âˆˆX . We conclude that H is the
parity-check matrix for X .
Equivalently, H is the matrix formed by the words corresponding to polynomials
Xihâ†“(X) where
hâ†“(X) = âˆ‘
0â‰¤iâ‰¤k
hiXkâˆ’i.

2.6 Additional problems for Chapter 2
265
Alternatively, let h(X) be the check polynomial for the cyclic code X length N
with a generator polynomial g(X) so that g(X)h(X) = XN âˆ’1. Then:
(a) X = { f(X): f(X)h(X) = 0 mod (XN âˆ’e)};
(b) if h(X) = h0 + h1X + Â·Â·Â·+ hNâˆ’rXNâˆ’r then the parity-check matrix H of X
has the form (2.6.9);
(c) the dual code X âŠ¥is a cyclic code of dimX âŠ¥= r, and X âŠ¥= âŸ¨hâŠ¥(X)âŸ©,
where hâŠ¥(X) = hâˆ’1
0 XNâˆ’rh(Xâˆ’1) = hâˆ’1
0 (h0XNâˆ’r +h1XNâˆ’râˆ’1 +Â·Â·Â·+hNâˆ’r).
Problem 2.19
Consider the parity-check matrix H of a Hamming [2â„“âˆ’1,2â„“âˆ’
â„“âˆ’1] binary code. Form the parity-check matrix Hâˆ—of a [2â„“,2â„“âˆ’â„“âˆ’1] code by
augmenting H with a column of zeros and then with a row of ones. The dual of
the resulting code is called a ï¬rst-order Reedâ€“Muller code. Show that a ï¬rst-order
Reedâ€“Muller code can correct errors of up to 2â„“âˆ’2 âˆ’1 bits per codeword.
For the photographs of Mars taken by the Mariner spacecraft such code with
â„“= 5 was used in 1972. What was the code rate? Why is this likely to have been
much less than the capacity of the channel?
Solution The code in question is [2â„“,â„“+ 1,2â„“âˆ’1]; with â„“= 5, the information rate
equals 6/32 â‰ˆ1/5. Let us check that all codewords except 0 and 1 have weight
2â„“âˆ’1. For â„“â‰¥1 the code R(â„“) is deï¬ned by recursion
R(â„“+1) = {xx|x âˆˆR(â„“)}âˆ¨{x,x+1|x âˆˆR(â„“)}.
So, the length of codewords in R(â„“+ 1) is obviously 2â„“+1. As {xx|x âˆˆR(â„“)}
and {x,x + 1|x âˆˆR(â„“)} are disjoint, the number of codewords is doubled, i.e.
â™¯R(â„“+ 1) = 2â„“+2. Finally, assuming that all codewords in R(â„“) except 0 and 1
have weight 2â„“âˆ’1, consider a codeword y âˆˆR(l +1). If y = xx is different from 0
or 1, then x Ì¸= 0 or 1, and so w(y) = 2w(x) = 2Ã—2â„“âˆ’1 = 2â„“.
If y = x,x + 1 we must consider some cases. If x = 0 then y = 01, which has
weight 2l. If x = 1 then y = 10, which also has weight 2l. Finally, if x Ì¸= 0 or 1
then w(x + 1) = 2â„“âˆ’2â„“âˆ’1 = 2â„“âˆ’1 and w(y) = 2 Ã— 2â„“âˆ’1 = 2â„“. It is clear now that
codewords xx and x,x+1 with w(x) = 2â„“âˆ’1 are orthogonal to rows of parity-check
matrix Hâˆ—.
Up to 7 bits may be in error, thus the probability of a transmission error pe (for
a binary symmetric memoryless channel with the error-probability p) obeys
pe â‰¤1âˆ’âˆ‘
0â‰¤iâ‰¤7
32
i

pi(1âˆ’p)32âˆ’i,
which is small when p is small. (As an estimate of an acceptable p, we can take the
solution to 1 + plog p + (1 âˆ’p)log(1 âˆ’p) = 26/32.) If the block length is ï¬xed
(and rather small), with a low value of p we canâ€™t get near the capacity.

266
Introduction to Coding Theory
Indeed, for â„“= 5, the code is [32,6,16], detecting 15 and correcting 7 errors. That
is, the code can correct a fraction > 1/5 of the total of 32 digits. Its information
rate is 6/32 and if the capacity of the (memoryless) channel is C = 1âˆ’Î·(p) (where
p stands for the symbol-probability of error), we need the bound C > 6/32; that
is, Î·(p)+6/32 < 1, for a reliable transmission. This yields |pâˆ’1/2| > |pâˆ—âˆ’1/2|
where pâˆ—âˆˆ(0,1) solves 26/32 = Î·(pâˆ—). Deï¬nitely 0 â‰¤p < 1/5 and 4/5 < p â‰¤1
would do. In reality the error-probability was much less.
Problem 2.20
Prove that any binary [5,M,3] code must have M â‰¤4. Verify that
there exists, up to equivalence, exactly one [5,4,3] code.
Solution By the Plotkin bound, if d is odd and d > 1
2(N âˆ’1) then
Mâˆ—
2(N,d) â‰¤2âŒŠ
d +1
2d +1âˆ’N âŒ‹.
In fact,
Mâˆ—
2(5,3) â‰¤2âŒŠ
4
6+1âˆ’5âŒ‹= 2Â·2 = 4.
All [5,4,3] codes are equivalent to 00000,00111,11001,11110.
Problem 2.21
Let X be a binary [N,k,d] linear code with generating matrix
G. Verify that we may assume that the ï¬rst row of G is 1 ... 1 0 ... 0 with d ones.
Write:
G =
 1...1
0...0
G1
G2

.
Show that if d2 is the distance of the code with generating matrix G2 then d2 â‰¥d/2.
Solution Let X be [N,k,d]. We can always form a generating matrix G of X where
the ï¬rst row is a codeword x with w(x) = d; by permuting columns of G we can
have the ï¬rst row in the form 1...1d
: ;< =0...0Nâˆ’d
:
;<
=. So, up to equivalence,
G =
 1...1
0...0
G1
G2

.
Suppose d(G2) < d/2 then, without loss of generality, we may assume that there
exists a row of (G1G2) where the number of ones among digits d + 1,...,N is
< d/2. Then the number of ones among digits 1,...d in this row is > d/2, as its
total weight is â‰¥d. Then adding this row and 1...1 0...0 gives a codeword with
weight < d. So, d(G2) â‰¥d/2.

2.6 Additional problems for Chapter 2
267
Problem 2.22
(Gilbertâ€“Varshamov bound) Prove that there exists a p-ary linear
[N,k,d] code if pk < 2N/vNâˆ’1(dâˆ’2). Thus, if pk is the largest power of p satisfying
this inequality, we have Mâˆ—
p(N,d) â‰¥pk.
Solution We construct a parity-check matrix by selecting N columns of length
N âˆ’k with the requirement that no d âˆ’1 columns are linearly dependent. The ï¬rst
column may be any non-zero string in ZNâˆ’k
p
. On the step i â‰¥2 we must choose
a column which is not a linear combination of any d âˆ’2 (or fewer) of previously
selected columns. The number of such linear combinations (with non-zero coefï¬-
cients) is
Si =
dâˆ’2
âˆ‘
j=1
iâˆ’1
j

(pâˆ’1) j.
So, the parity-check matrix may be constructed iff SN +1 < pNâˆ’k. Finally, observe
that SN + 1 = vNâˆ’1(d âˆ’2). Say, there exists [5,2k,3] code if 2k < 32/5, so k = 2
and Mâˆ—
2(5,3) â‰¥4, which is, in fact, sharp.
Problem 2.23
An element b âˆˆFâˆ—
q is called primitive if its order (i.e. the minimal
k such that bk = 1 mod q) is q âˆ’1. It is not difï¬cult to ï¬nd a primitive element of
the multiplicative group Fâˆ—
q explicitly. Consider the prime factorisation
qâˆ’1 =
s
âˆ
j=1
pÎ½j
j .
For any j = 1,...,s select aj âˆˆFq such that a(qâˆ’1)/p j
j
Ì¸= e. Set b j = a
(qâˆ’1)/p
Î½ j
j
j
and
check that b = âˆs
j=1 b j has the order qâˆ’1.
Solution Indeed, the order of bj is pÎ½j
j . Next, if bn = 1 for some n then n=0
mod pÎ½j
j because bnâˆiÌ¸= j pÎ½i
i = 1 implies b
nâˆiÌ¸= j pÎ½i
i
j
= 1, i.e. nâˆiÌ¸= j pÎ½i
i = 0 mod pÎ½i
j =
0. Because p j are distinct primes, it follows that n = 0 mod pÎ½j
j for any j. Hence,
n = âˆs
j=1 pÎ½j
j .
Problem 2.24
The minimal polynomial with a primitive root is called a primitive
polynomial. Check that among irreducible binary polynomials of degree 4 (see
(2.5.9)), 1 + X + X4 and 1 + X3 + X4 are primitive and 1 + X + X2 + X3 + X4 is
not. Check that all six irreducible binary polynomials of degree 5 (see (2.5.15))
are primitive; in practice, one prefers to work with 1+X2 +X5 as the calculations
modulo this polynomial are slightly shorter. Check that among the nine irreducible
polynomials of degree 6 in (2.5.16), there are six primitive: they are listed in the
upper three lines. Prove that a primitive polynomial exists for every given degree.

268
Introduction to Coding Theory
Solution For the solution to the last part, see Section 3.1.
Problem 2.25
A cyclic code X of length N with the generator polynomial g(X)
of degree d = N âˆ’k can be described in terms of the roots of g(X), i.e. the elements
Î±1,...Î±Nâˆ’k such that g(Î± j) = 0. These elements are called zeros of code X and
belong to a Galois ï¬eld F2d. As g(X)|(1+XN), they are also among roots of 1+XN.
That is, Î±N
j = 1, 1 â‰¤j â‰¤N âˆ’k, i.e. the Î±j are Nth roots of unity. The remaining k
roots of unity Î±â€²
1,...,Î±â€²
k are called non-zeros of X . A polynomial a(X) âˆˆX iff,
in Galois ï¬eld F2d, a(Î± j) = 0, 1 â‰¤j â‰¤N âˆ’k.
(a) Show that if X âŠ¥is the dual code then the zeros of X âŠ¥are Î±â€²
1
âˆ’1,...,Î±â€²
k
âˆ’1, i.e.
the inverses of the non-zeros of X .
(b) A cyclic code X with generator g(X) is called reversible if, for all x =
x0 ...xNâˆ’1 âˆˆX , the word xNâˆ’1 ...x0 âˆˆX . Show that X is reversible iff g(Î±) = 0
implies that g(Î±âˆ’1) = 0.
(c) Prove that a q-ary cyclic code X of length N with (q,N) = 1 is invariant under
the permutation of digits such that Ï€q(i) = qi mod N (i.e. x â†’xq). If s = ordN(q)
then the two permutations i â†’i + 1 and Ï€q(i) generate a subgroup of order Ns in
the group Aut(X ) of the code automorphisms.
Solution Indeed, since a(xq) = a(x)q is proportional to the same generator polyno-
mial it belongs to the same cyclic code as a(x).
Problem 2.26
Prove that there are 129 non-equivalent cyclic binary codes of
length 128 (including the trivial codes, {0...0} and {0,1}128). Find all cyclic bi-
nary codes of length 7.
Solution The equivalence classes of the cyclic codes of length 2k are in a one-to-
one correspondence with the divisors of 1+X2k; the number of those equals 2k +1.
Furthermore, there are eight codes listed by their generators which are divisors of
X7 âˆ’1 as
X7 âˆ’1 = (1+X)(1+X +X3)(1+X2 +X3).

3
Further Topics from Coding Theory
3.1 A primer on ï¬nite ï¬elds
In this section we present a summary of the theory of ï¬nite ï¬elds, limiting our scope
by material needed in the subsequent sections and following standard texts (see
[92], [93], [131]). A ï¬nite ï¬eld is a (ï¬nite) set F possessing two distinct elements,
0 (zero) and e (unity), and equipped with two commutative group operations of
addition and multiplication (where 0 Â· b = 0 for all b âˆˆF) related by a standard
distributivity rule.
A vector space over a ï¬eld F is a (ï¬nite) set V, equipped with a commutative
group operation of addition, and an operation of scalar multiplication by elements
of F, again obeying standard distributivity rules. The dimension dim V of V is the
minimal number d such that any collection of distinct elements v1,...,vd+1 âˆˆV
is linearly dependent, i.e. one can ï¬nd elements k1,...,kd+1 âˆˆF, not all equal to
0, such that k1v1 + Â·Â·Â· + kd+1vd+1 = 0. Then there exists a collection of elements
b1,...,bd âˆˆV, called a basis, such that every v âˆˆV can be written as a linear com-
bination a1b1+Â·Â·Â·+adbd where a1,...,ad are elements of F (uniquely) determined
by v. Unless the opposite is stated, we consider ï¬elds up to an isomorphism.
An important parameter of a ï¬eld is its characteristic, i.e. the minimal integer
number p â‰¥1 such that pe = e+Â·Â·Â·+e(p times) = 0. Such a number, denoted by
char(F), exists by a standard pigeon-hole principle. Furthermore, the characteristic
is a prime number: if p = q1q2 then pe = (q1q2)e = (q1e)(q2e) = 0 which implies
that q1e = 0 or q2e = 0 leading to a contradiction.
Example 3.1.1
Let p be a prime number. An additive cyclic group Zp =
{0,1,..., p âˆ’1}, with a generator 1, becomes a ï¬eld with the multiplication
(qe)(qâ€²e) = (qqâ€²)e. The characteristic of this ï¬eld equals p.
Let K and F be ï¬elds. If F âŠ†K we say that K is an extension of F. Then K is
also a vector space over F whose dimension is denoted by [K : F].
Lemma 3.1.2
Let K be an extension of F, and d = [K : F]. Then â™¯K = (â™¯F)d.
269

270
Further Topics from Coding Theory
Proof
Let b1,...,bd be a basis for K over F, with a unique representation
k = âˆ‘
1â‰¤jâ‰¤d
a jb j for all k âˆˆK. Then for all j, we have â™¯F possibilities for aj. So,
altogether there exists precisely (â™¯F)d ways to write all combinations.
Lemma 3.1.3
If char(F) = p then â™¯F = pd, for some integer d â‰¥1.
Proof
Consider elements 0,e,2e,...,(p âˆ’1)e. They form Zp, i.e. Zp âŠ†F. Then
â™¯F = pd by Lemma 3.1.2.
Corollary 3.1.4
The number of elements in a ï¬nite ï¬eld F must be q = ps where
p = char(F) and s â‰¥1 is a natural number.
From now on, unless otherwise stated, p stands for a prime and q = ps for a
prime power.
Lemma 3.1.5
(A freshmanâ€™s dream) If char(F) = p then for all a,b âˆˆF and
integers n â‰¥1,
(aÂ±b)pn = apn +(Â±b)pn.
(3.1.1)
Proof
Use induction in n: for n = 1,
(aÂ±b)p = âˆ‘
0â‰¤kâ‰¤p
p
k

ak(Â±b)pâˆ’k.
For 1 â‰¤k â‰¤p âˆ’1, the value
p
k

is a multiple of p and the corresponding term
vanishes. Therefore, (aÂ±b)p = ap +(Â±b)p. The inductive step is completed by the
same argument, with a and Â±b replaced by apnâˆ’1 and (Â±b)pnâˆ’1.
Lemma 3.1.6
The multiplicative group Fâˆ—of non-zero elements of a ï¬eld F of
size q is isomorphic to the cyclic group Zqâˆ’1.
Proof
Observe that for any divisor d|(q âˆ’1), group Fâˆ—contains exactly Ï†(d)
elements of multiplicative order d where Ï† is Eulerâ€™s totient phi-function. (Recall
that Ï†(d) = â™¯{k : k < d,gcd(k,d) = 1}.) Weâ€™ll see that all elements of order d have
the form a
qâˆ’1
d r where a is a primitive element, r â‰¤d and r,d are co-prime. In fact,
q âˆ’1 =
âˆ‘
d:d|(qâˆ’1)
Ï†(d), and Fâˆ—will have at least one element of order q âˆ’1 which
implies that Fâˆ—is cyclic, of order qâˆ’1.
Let a âˆˆFâˆ—be an element of order d where d|(q âˆ’1). Take the cyclic subgroup
{e,a,...,adâˆ’1}. Every element of this subgroup has multiplicative order dividing
d, i.e. is a root of the polynomial Xd âˆ’e (a dth root of unity). But Xd âˆ’e has
â‰¤d distinct roots in F (because F is a ï¬eld). So, {e,a,...,adâˆ’1} is the set of all
roots of Xd âˆ’e in F. In particular, each element from F of order d belongs to

3.1 A primer on ï¬nite ï¬elds
271
{e,a,...,adâˆ’1}. Observe that the cyclic group Zd has exactly Ï†(d) elements of
order d. So, the whole Fâˆ—has exactly Ï†(d) elements of order d; in other words, if
Ïˆ(d) is the number of elements in F of order d then either Ïˆ(d) = 0 or Ïˆ(d) = Ï†(d)
and
qâˆ’1 = âˆ‘
d:d(n)
Ïˆ(d) â‰¤âˆ‘
d:d|n
Ï†(d) = qâˆ’1,
which implies that for all d|n,
Ïˆ(d) = Ï†(d).
Deï¬nition 3.1.7
A (multiplicative) generator of Fâˆ—(i.e. an element of multiplica-
tive order qâˆ’1) is called a primitive element of ï¬eld F. Although such an element
is non-unique, we will usually single out one such element and denote it by Ï‰;
of course a power Ï‰r where r is coprime with (q âˆ’1) will also give a primitive
element.
If a âˆˆFâˆ—with â™¯Fâˆ—= qâˆ’1 then aqâˆ’1 = e (the order of every element divides the
order of the group). Hence, aq = a, i.e. a is a root of the polynomial Xq âˆ’X in F.
But Xq âˆ’X can have only â‰¤q roots (including zero 0), so F gives the set of all
roots of Xq âˆ’X.
Deï¬nition 3.1.8
Given ï¬elds K and F, with F âŠ†K, ï¬eld K is called the splitting
ï¬eld for a polynomial g(X) with coefï¬cients from F if (a) K contains all roots of
g(X), (b) there is no ï¬eld  K with F âŠ‚ K âŠ‚K satisfying (a). We will write Spl(g(X))
for the splitting ï¬eld for g(X).
Thus, if â™¯F = q then F contains all roots of polynomial Xqâˆ’X and is the splitting
ï¬eld for this polynomial.
Lemma 3.1.9
Any two splitting ï¬elds K, Kâ€² for the same polynomial g(X) with
coefï¬cients from F coincide.
Proof
In fact, take the intersection Kâˆ©Kâ€²: it contains F and is a subï¬eld of both
K and Kâ€². It must then coincide with each of K, Kâ€².
Corollary 3.1.10
For any prime p and natural s â‰¥1, there exists at most one
ï¬eld with ps elements.
Proof
Each such ï¬eld is splitting for polynomial Xq âˆ’X with coefï¬cients from
Zp and q = ps. So any two such ï¬elds coincide.
On the other hand, we will prove later the following.
Theorem 3.1.11
For any non-constant polynomial with coefï¬cients from F, there
exists a splitting ï¬eld.

272
Further Topics from Coding Theory
Corollary 3.1.12
For any prime p and natural s â‰¥1, there exists precisely one
ï¬eld with ps elements.
Proof of Corollary 3.1.12 Take again the polynomial Xq âˆ’X with coefï¬cients from
Zp and q = ps. By Theorem 3.1.11, there exists the splitting ï¬eld Spl(Xq âˆ’X)
where Xqâˆ’X = X(Xqâˆ’1âˆ’e) is factorised into linear polynomials. So, Spl(Xqâˆ’X)
contains the roots of Xq âˆ’X and has characteristic p (as it contains Zp).
However, the roots of (Xq âˆ’X) form a subï¬eld: if aq = a and bq = b then (aÂ±
b)q = aq + (Â±bq) (Lemma 3.1.5) which coincides with a Â± b. Also, (abâˆ’1)q =
aq(bq)âˆ’1 = abâˆ’1. This ï¬eld cannot be strictly contained in Spl(Xq âˆ’X) thus it
coincides with Spl(Xq âˆ’X).
It remains to check that all roots of (Xq âˆ’X) are distinct: then the cardinality
â™¯Spl(Xq âˆ’X) will be equal to q. In fact, if Xq âˆ’X had a multiple root then it would
have had a common factor with its â€˜derivativeâ€™ âˆ‚X(Xq âˆ’X) = qXqâˆ’1 âˆ’e. However,
qXqâˆ’1 = 0 in Spl(Xq âˆ’X) and thus cannot have such factors.
Summarising, we have the two characterisation theorems for ï¬nite ï¬elds.
Theorem 3.1.13
All ï¬nite ï¬elds have size ps where p is prime and s â‰¥1 integer.
For all such p,s, there exists a unique ï¬eld of this size.
The ï¬eld of size q = ps will be denoted by Fq (a popular alternative notation is
GF(q) (a Galois ï¬eld)). In the case of the simplest ï¬elds Fp = {0,1,..., pâˆ’1} (for
p is prime) we use symbol 1 instead of e for the unit.
Theorem 3.1.14
All ï¬nite ï¬elds can be arranged into sequences (â€˜towersâ€™). For
a prime p and positive integers s1,s2,...,
...
...
...
â†‘
Fps1s2...si
â†‘
...
â†‘
Fps1s2
â†‘
Fps1
â†‘
Fp â‰ƒZp
Here each arrow is a uniquely deï¬ned injective homomorphism.

3.1 A primer on ï¬nite ï¬elds
273
Example 3.1.15
In Section 2.5 we worked with polynomial ï¬elds F2[X]/âŸ¨q(X)âŸ©
where q(X) is an irreducible binary polynomial; see Theorems 2.5.32 and 2.5.33.
Continuing Example 2.5.35(c), consider the ï¬eld F16 realised as F2[X]/âŸ¨1+X3 +
X4âŸ©. The ï¬eld structure is as follows:
power
of X
mod 1+X3 +X4
polynomial
vector
(string)
âˆ’âˆ’
0
0000
X0
1
1000
X
X
0100
X2
X2
0010
X3
X3
0001
X4
1+X3
1001
X5
1+X +X3
1101
X6
1+X +X2 +X3
1111
X7
1+X +X2
1110
X8
X +X2 +X3
0111
X9
1+X2
1010
X10
X +X3
0101
X11
1+X2 +X3
1011
X12
1+X
1100
X13
X +X2
0110
X14
X2 +X3
0011
(3.1.2)
Finally, if we choose to specify the table for F2[X]/âŸ¨1 + X + X2 + X3 + X4âŸ©,
the calculations will be considerably longer (and organised differently). The point
is that in this case monomial X will not be a primitive element, since X5 = 1
mod (1+X +X2 +X3 +X4). Instead, a generator of the multiplicative group will
be a sum of monomials, viz. 1+X.
Worked Example 3.1.16
(a) How many elements are in the smallest extension
of F5 which contains all roots of polynomials X2 +X +1 and X3 +X +1?
(b) Determine the number of subï¬elds of F1024, F729. Find all primitive elements
of F7, F9, F16. Compute (Ï‰10 + Ï‰5)(Ï‰4 + Ï‰2) where Ï‰ is a primitive element of
F16.
Solution (a) Clearly, 56.
(b) F1024 = F210 has 4 subï¬elds: F2,F4,F32 and F1024. F729 = F36 has 4 subï¬elds:
F3,F9,F27 and F729. F7 has 2 primitive elements: Ï‰, Ï‰5 (with (Ï‰5)5 = Ï‰). F9 has

274
Further Topics from Coding Theory
4 primitive elements, of the form Ï‰,Ï‰3,Ï‰5,Ï‰7. F16 has 8 primitive elements: Ï‰,
Ï‰2, Ï‰4, Ï‰7, Ï‰8, Ï‰11, Ï‰13, Ï‰14.
By using the table for F2[X]/âŸ¨1+X +X4âŸ©(see Example 2.5.35(c)), we can ï¬nd
that
(Ï‰10 +Ï‰5)(Ï‰4 +Ï‰2) = Ï‰14 +Ï‰9 +Ï‰12 +Ï‰7
= 1001+0101+1111+1101 = 1110
= Ï‰10.
However, by taking Ï‰â€² = Ï‰7, the RHS becomes
Ï‰8 +Ï‰3 +Ï‰9 +Ï‰4 = 1010+0001+0101+1100 = 0010 = Ï‰2 = (Ï‰â€²)11.
From now on we will focus on polynomial representations of ï¬nite ï¬elds. Gen-
eralising concepts introduced in Section 2.5, consider
Deï¬nition 3.1.17
The set of all polynomials with coefï¬cients from Fq is a com-
mutative ring denoted by Fq[X]. A quotient ring Fq[X]/âŸ¨g(X)âŸ©is where the opera-
tion is modulo a ï¬xed polynomial g(X) âˆˆFq[X].
Deï¬nition 3.1.18
A polynomial g(X) âˆˆFq[X] is called irreducible (over Fq) if it
admits no representation
g(X) = g1(X)g2(X)
with g1(X),g2(X) âˆˆFq[X].
A generalisation of Theorem 2.5.32 is presented in Theorem 3.1.19 below.
Theorem 3.1.19
Let g(X) âˆˆFq[X] have degree degg(X) = d. Then
Fq[X]/âŸ¨g(X)âŸ©is a ï¬eld Fqd iff g(X) is irreducible.
Proof
Let g(X) be an irreducible polynomial over Fq. To show that Fq[X]/âŸ¨g(X)âŸ©
is a ï¬eld we should check that each non-zero element f(X) âˆˆFq[X]/âŸ¨g(X)âŸ©has an
inverse. Consider the set F( f) of polynomials of the form f(X)h(X) modg(X)
where h(X) âˆˆFq[X]/âŸ¨g(X)âŸ©(the principal ideal generated by f(X)). If F( f) con-
tains the unity e âˆˆFq (the constant polynomial equal to e) then the corresponding
h(X) = f(X)âˆ’1. If not, the map h(X) â†’f(X)h(X) modg(X), from Fq[X]/âŸ¨g(X)âŸ©
to itself, is not a surjection. That is, f(X)h1(X) = f(X)h2(X) modg(X) for some
distinct h1(X), h2(X), i.e.
f(X)

h1(X)âˆ’h2(X)

= r(X)g(X).

3.1 A primer on ï¬nite ï¬elds
275
Then either g(X)|f(X) or g(X)|

h1(X) âˆ’h2(X)

as g(X) is irreducible. So, ei-
ther f(X) = 0 modg(X) (a contradiction) or h1(X) = h2(X) modg(X). Hence,
Fq[X]/âŸ¨g(X)âŸ©is a ï¬eld.
The inverse assertion is proved similarly: if g(X) is reducible then Fq[X]/âŸ¨g(X)âŸ©
contains non-zero g1(X), g2(X) with g1(X)g2(X) = 0. Then Fq[X]/âŸ¨g(X)âŸ©cannot
be a ï¬eld.
The dimension
	
Fq[X]/âŸ¨g(X)âŸ©: Fq

is equal to d, the degree of g(X), so
Fq[X]/âŸ¨g(X)âŸ©= Fqd.
Worked Example 3.1.20
Prove that g(X) has an inverse in the polynomial ring
Fq[X]/âŸ¨XN âˆ’eâŸ©iff gcd

g(X),XN âˆ’e

= e.
Solution Consider the map Fq[X]/âŸ¨XN âˆ’eâŸ©â†’Fq[X]/âŸ¨XN âˆ’eâŸ©given by h(X) â†’
h(X)g(X) mod (XN âˆ’e). If it is a surjection then there exists h(X) with
h(X)g(X) = e and h(X) = g(X)âˆ’1. Suppose it is not. Then there exist h(1)(X) Ì¸=
h(2)(X) mod(XN âˆ’e) such that h(1)(X)g(X) = h(2)(X)g(X) mod(XN âˆ’e), i.e.
(h(1)(X)âˆ’h(2)(X))g(X) = s(X)(XN âˆ’e).
As (XN âˆ’e)Ì¸ | (h(1)(X)âˆ’h(2)(X)), this means that gcd(g(X),XN âˆ’e) Ì¸= e.
Conversely, if gcd(g(X),XN âˆ’e) = d(X) Ì¸= e then the equation h(X)g(X) = e
mod (XN âˆ’e) gives
h(X)g(X) = e+q(X)(XN âˆ’e)
where d(X)|LHS and d(X)|q(X)(XN âˆ’e)). Therefore, d(X)|e: a contradiction.
Hence, g(X)âˆ’1 does not exist.
Example 3.1.21
(Continuing Example 2.5.19) There are six irreducible binary
polynomials of degree 5:
1+X2 +X5, 1+X3 +X5, 1+X +X2 +X3 +X5,
1+X +X2 +X4 +X5, 1+X +X3 +X4 +X5,
1+X2 +X3 +X4 +X5.
(3.1.3)
Then there are nine irreducible polynomials of degree 6, and so on. Calculating
irreducible polynomials of a large degree is a demanding task, although extensive
tables of such polynomials are now available on the web.
We are now going to prove Theorem 3.1.11.
Proofof Theorem 3.1.11 The key fact is that any non-constant polynomial g(X) âˆˆ
Fq[X] has a root in some extension of Fq. Without loss of generality, assume that
g(X) is irreducible, with degg(X) = d. Take Fq[X]/âŸ¨g(X)âŸ©= Fqd as an extension
ï¬eld. In this ï¬eld, g(Î±) = 0 where Î± is polynomial X âˆˆFq[X]/âŸ¨g(X)âŸ©, so g(X)

276
Further Topics from Coding Theory
has a root. We can divide g(X) by X âˆ’Î± in Fqd and use the same construction
to prove that g1(X) = g(X)/(X âˆ’Î±) has a root in some extension of Fqt,t < d.
Finally, we obtain a ï¬eld containing all d roots of g(X), i.e. construct the splitting
ï¬eld Spl(g(X)).
Deï¬nition 3.1.22
Given a ï¬eld F âŠ‚K and an element Î³ âˆˆK, we denote by
F(Î³) the smallest ï¬eld containing F and Î³ (obviously, F âŠ‚F(Î³) âŠ‚K). Similarly,
F(Î³1,...,Î³r) is the smallest ï¬eld containing F and elements Î³1,...,Î³r âˆˆK. For
F = Fq and Î± âˆˆK, set
MÎ±,F(X) = (X âˆ’Î±)(X âˆ’Î±q)...

X âˆ’Î±qdâˆ’1
,
(3.1.4)
where d is the smallest positive integer such that Î±qd = Î± (such a d exists as will
be proved in Lemma 3.1.24).
A monic polynomial is the one with the highest coefï¬cient. The minimal poly-
nomial for Î± âˆˆK over F is a unique monic polynomial MÎ±(X) (= MÎ±,F(X)) âˆˆ
F[X] such that MÎ±(Î±) = 0 and MÎ±(X)|g(X) for each g(X) âˆˆF[X] with g(Î±) = 0.
When Ï‰ is a primitive element of K (generating Kâˆ—), MÏ‰(X) is called a primitive
polynomial (over F). The order of a polynomial p(X) âˆˆF[X] is the smallest n such
that p(X)|(Xn âˆ’e).
Example 3.1.23
(Continuing Example 3.1.21.) In this example we deal with
polynomials over F2. The irreducible polynomial X2 + X + 1 is primitive and has
order 3. The irreducible polynomials X3 +X +1 and X3 +X2 +1 are primitive and
of order 7. The polynomials X4 + X3 + 1 and X4 + X + 1 are primitive and have
order 15 whereas X4 +X3 +X2 +X +1 is not primitive and of order 5. (It is helpful
to note that with d = 4, the order of X4+X3+1 and X4+X +1 equals 2d âˆ’1; on the
other hand, the order of element X in the ï¬eld F2[X]/âŸ¨1+X +X2+X3+X4âŸ©equals
5, but its order, say, in the ï¬eld F2[X]/âŸ¨1+X +X4âŸ©equals 15.) All six polynomials
listed in (3.1.3) are primitive and have order 31 (i.e. appear in the decomposition
of X31 +1).
Lemma 3.1.24
Let Fq âŠ‚Fqd and Î± âˆˆFqd. Let MÎ±(X) âˆˆF[X] be the minimal
polynomial for Î±, of degree degMÎ±(X) = d. Then:
(a) MÎ±(X) is the only irreducible polynomial in Fq[X] with a root at Î±.
(b) MÎ±(X) is the only monic polynomial in Fq[X] of degree d with a root at Î±.
(c) MÎ±(X) has the form (3.1.4).

3.1 A primer on ï¬nite ï¬elds
277
Proof
Assertions (a), (b) follow from the deï¬nition. To prove (c), assume Î³ âˆˆK is
a root of a polynomial f(X) = a0 +a1X +Â·Â·Â·+adXd from F[X], i.e.
âˆ‘
0â‰¤iâ‰¤d
aiÎ³i = 0.
As aq
i = ai (which is true for all a âˆˆF) and by virtue of Lemma 3.1.5,
f(Î³q) = âˆ‘
0â‰¤iâ‰¤d
aiÎ³qi = âˆ‘
0â‰¤iâ‰¤d

aiÎ³iq =

âˆ‘
0â‰¤iâ‰¤d
aiÎ³i
q
= 0,
so Î³q is a root. Similarly,

Î³qq = Î³q2 is a root, and so on.
For MÎ±(X) it yields that Î±, Î±q, Î±q2,... are roots. This will end when Î±qs = Î±
for the ï¬rst time (which proves the existence of such an s). Finally, s = d as all
Î±, Î±q,...,Î±qdâˆ’1 are distinct: if not then Î±qi = Î±q j where, say, i < j. Taking qdâˆ’j
power of both sides, we get Î±qd+iâˆ’j = Î±qd =Î±. So, Î± is a root of polynomial
P(X) = Xqd+iâˆ’j âˆ’X, and Spl(P(X)) = Fqd+iâˆ’j. On the other hand, Î± is a root of an
irreducible polynomial of degree d, and Spl(MÎ±(X)) = Fqd. Hence, d|(d + i âˆ’j)
or d|(i âˆ’j), which is impossible. This means that all the roots Î±qi,i < d, are
distinct.
Theorem 3.1.25
For any ï¬eld Fq and integer d â‰¥1, there exists an irreducible
polynomial f(X) âˆˆFq[X] of degree d.
Proof
Take a primitive element Ï‰ âˆˆFqd. Then Fq(Ï‰), the minimal extension of
Fq containing Ï‰, coincides with Fqd. The dimension [Fq(Ï‰) : Fq] of vector space
Fq(Ï‰) over Fq equals [Fqd : Fq] = d. The minimal polynomial MÏ‰(X) for Ï‰ over
Fq has distinct roots Ï‰, Ï‰q,...,Ï‰qdâˆ’1 and therefore is of degree d.
Although proving irreducibility of a given polynomial is a problem with no gen-
eral solution, the number of irreducible polynomials of a given degree can be evalu-
ated by using an elegant (and not very complicated) method invoking the so-called
MÂ¨obius function.
Deï¬nition 3.1.26
The MÂ¨obius function Î¼ on the set Z+ is given by
Î¼(1) = 1, Î¼(n) = 0 if n is divisible by a square of a prime number,
and
Î¼(n) = (âˆ’1)k if n is a product of k distinct prime numbers.
Theorem 3.1.27
The number Nq(n) of irreducible polynomials of degree n in
the polynomial ring Fq[X] is given by
Nq(n) = 1
n âˆ‘
d: d|n
Î¼(d)qn/d.
(3.1.5)

278
Further Topics from Coding Theory
For example, Nq(20) equals
1
20

Î¼(1)q20 + Î¼(2)q10 + Î¼(4)q5 + Î¼(5)q4 + Î¼(10)q2 + Î¼(20)q

= 1
20

q20 âˆ’q10 âˆ’q4 +q2
.
Proof
First, we establish the additive MÂ¨obius inversion formula. Let Ïˆ and Î¨ be
two functions from Z+ to an Abelian group G with an additive group operation.
Then the following equations are equivalent:
Î¨(n) = âˆ‘
d|n
Ïˆ(d)
(3.1.6)
and
Ïˆ(n) = âˆ‘
d|n
Î¼(d)Î¨
n
d

.
(3.1.7)
This equivalence follows when we observe that (a) the sum âˆ‘
d|n
Î¼(d) is equal to 0 if
n > 1 and to 1 if n = 1, and (b) for all n,
âˆ‘
d: d|n
Î¼(d)Î¨

n/d

= âˆ‘
d: d|n
Î¼(d)
âˆ‘
c: c|n/d
Ïˆ(c)
= âˆ‘
c: c|n
Ïˆ(c)
âˆ‘
d: d|n/c
Î¼(d) = Ïˆ(n).
To check (a), let p1,..., pk be different prime factors in decomposition of n then
âˆ‘
d|n
Î¼(d) = Î¼(1)+
k
âˆ‘
i=1
Î¼(pi)+Â·Â·Â·+ Î¼(p1 ... pk)
= 1+
k
1

(âˆ’1)+
k
2

(âˆ’1)2 +Â·Â·Â·+
k
k

(âˆ’1)k = 0.
Applying (3.1.7) to G = Z, the additive group of integer numbers, with Ïˆ(n) =
nNq(n) and Î¨(n) = qn, gives (3.1.5).
Now, decompose the polynomial Xqn âˆ’X into the product of irreducible poly-
nomials. Then (3.1.6) holds true as the degree qn of X qn âˆ’X coincides with the
sum of degrees of all irreducible polynomials whose degrees divide n. Indeed, we
simply write X qn âˆ’X as the product of all irreducible polynomials and observe
that an irreducible polynomial enters the decomposition iff its degree divides n (cf.
Corollary 3.1.30).
Worked Example 3.1.28
Find all irreducible polynomials of degree 2 and 3 over
F3 and determine their orders.

3.1 A primer on ï¬nite ï¬elds
279
Solution Over F3 = {0,1,2} there are three irreducible polynomials of degree 2:
X2 +1, of order 4, with
(X4 âˆ’1)/(X2 +1) = X2 âˆ’1,
and X2 +X +2 and X2 +2X +2, of order 8, with
(X8 âˆ’1)/(X2 +X +2)(X2 +2X +2) = X4 âˆ’1.
Next, there exist (33 âˆ’3)/3 = 8 irreducible polynomials over F3 of degree 3.
Four of them have order 13 (hence, are not primitive):
X3 +2X +2,X3 +X2 +2,X3 +X2 +X +2,X3 +2X2 +2X +2.
The remaining four have order 26 (hence, are primitive):
X3 +2X +1,X3 +X2 +2X +1,X3 +2X2 +1,X3 +2X2 +X +1.
Indeed, if p(X) denotes the product of the ï¬rst four polynomials then (X13 âˆ’
1)/p(X) = X âˆ’1. On the other hand, if r(X) stands for the product of the remaining
four then (X26 âˆ’1)/r(X) equals
(X âˆ’1)(X +1)(X3 +2X +2)(X3 +X2 +2)
Ã—(X3 +X2 +X +2)(X3 +2X2 +2X +2).
Theorem 3.1.29
If g(X) âˆˆFq[X] is irreducible and of degree d and Î± is a root
of g(X) then the splitting ï¬eld Spl(g(X)) and the minimal extension Fq(Î±) both
coincide with Fqd.
Proof
We know that g(X) = MÎ±,Fq(X) = irrÎ±,Fq(X) (by Lemma 3.1.24, as g(X)
is irreducible). We then have that Fq âŠ‚Fq(Î±) = Fqd âŠ†Spl(g(X)). It is left to check
that any root Î³ of g(X) lies in Fq(Î±): this will imply that Spl(g(X)) âŠ†Fq(Î±).
By Theorem 3.1.13, the unique Galois ï¬eld with qd elements Fq(Î±) = Fqd is the
splitting ï¬eld Spl(Xqd âˆ’X), i.e. contains all roots of Xqd âˆ’X (one of which is Î±).
Then g(X)|

Xqd âˆ’X

as g(X) = MÎ±,Fq(X). Therefore, all roots of g(X) are roots
of Xqd âˆ’X and hence lie in Fq(Î±).
Corollary 3.1.30
Suppose that g(X) âˆˆFq[X] is an irreducible polynomial of
degree d. Then g(X)|

Xqn âˆ’X

iff d|n.
Proof
We have the splitting ï¬elds Spl(g(X)) = Fqd and Spl

Xqn âˆ’X

= Fqn. By
Theorem 3.1.29, Spl(g(X)) âŠ†Spl

Xqn âˆ’X

iff d|n.
Now if g(X)|

Xqn âˆ’X

, each root of g(X) is a root of

Xqn âˆ’X

. Then
Spl(g(X)) âŠ†Spl

Xqn âˆ’X

and hence d|n.

280
Further Topics from Coding Theory
Conversely, if d|n, i.e. Spl(g(X)) âŠ†Spl

Xqn âˆ’X

, then each root of g(X) lies in
Spl

Xqn âˆ’X

. But Spl

Xqn âˆ’X

is precisely the set of the roots of

Xqn âˆ’X

, so
each root of g(X) is that of

Xqn âˆ’X

. Then g(X)|

Xqn âˆ’X

.
Theorem 3.1.31
If g(X) âˆˆFq[X] is an irreducible polynomial of degree d and
Î± âˆˆSpl(g(X)) = Fqd[X] is its root then all the roots of g(X) are Î±,Î±q,...,Î±qdâˆ’1.
Furthermore, d is the smallest positive integer such that Î±qd = Î±.
Proof
As in the proof of Lemma 3.1.24, Î±,Î±q,...,Î±qdâˆ’1 are distinct roots. Thus
all the roots are listed and d is the smallest positive integer with the above property.
Corollary 3.1.32
All roots of an irreducible polynomial g(X) âˆˆFq[X] with
degg(X) = d have in Spl(g(X)) the same multiplicative order dividing qd âˆ’1, and
it gives the order of polynomial g(X) (see Deï¬nition 3.1.22).
The order of irreducible polynomial g(X) will be denoted by ord(g(X)).
Worked Example 3.1.33
(a) Prove that for natural n,q such that lcm(n,q) = 1
there exists a natural s such that n|(qs âˆ’1).
(b) Prove that if a polynomial g(X) âˆˆF2[X] is irreducible then g(X)|(Xn âˆ’1) iff
ord(g(X))|n.
Solution (a) Set ql âˆ’1 = nal +bl where bl â‰¤n and l = 1,2,.... By the pigeon-hole
principle, bl1 = bl2 for some l1 < l2. Then n|ql1(ql2âˆ’l1 âˆ’1). Owing to the condition
lcm(n,q) = 1, n|(qs âˆ’1) with s = l2 âˆ’l1.
(b) For an irreducible g(X), the order ordg(X) was introduced in Deï¬nition 3.1.22:
ord(g(X)) = min[n : g(X)|(Xn âˆ’1)].
First, our goal is to check that if m = ord(g(X)) then m|n iff g(X)|(Xn âˆ’1). In-
deed, suppose m|n: n = mr. Then Xn âˆ’1 = (Xm âˆ’1)(1 + Xm + Â·Â·Â·+ Xm(râˆ’1)). As
g(X)|(Xm âˆ’1), this implies g(X)|(Xn âˆ’1).
Conversely, if g(X)|(Xn âˆ’1) then the roots of Î±1,...,Î±d of g(X) are among
those of Xn âˆ’1 in Spl(Xn âˆ’1). So, Î±m
j = Î±n
j = 1 in Spl(Xn âˆ’1),1 â‰¤j â‰¤d. Write
n = mb+a where 0 â‰¤a < m. Then Î±n
j = Î±bm
j Î±a
j = Î±a
j = 1, i.e. each Î± j is a root of
Xa âˆ’1. Hence, if a > 0 then g(X)|(Xa âˆ’1): a contradiction. So, a = 0 and m|n.
Calculating an irreducible polynomial g(X) âˆˆFq[X] with a given root Î± âˆˆFqn,
in particular, the minimal polynomial MÎ±,Fq(X), is not easy. This is because the
relation between q, n, Î± and d = degMÎ±(X) is complicated. However, if Î± = Ï‰
is a primitive element of Fqn then d = n as Ï‰qnâˆ’1 = e, Ï‰qn = Ï‰ and n is the least
positive integer with this property. In this case MÏ‰(X) = âˆbâˆˆFqn(X âˆ’b).

3.1 A primer on ï¬nite ï¬elds
281
For a general irreducible polynomial, the notion of conjugacy is helpful: see Def-
inition 3.1.34 below. This concept was introduced (and used) informally in Section
2.5 for ï¬elds F2s.
Deï¬nition 3.1.34
Elements Î±, Î±â€² âˆˆFqn are called conjugate over Fq if
MÎ±,Fq(X) = MÎ±â€²,Fq(X).
Summarising what was said above, we deduce the following assertion.
Theorem 3.1.35
The conjugates of Î± âˆˆFqn over Fq are Î±, Î±q,...,Î±qdâˆ’1 âˆˆFqn
where d is as before. In particular,
âˆ
0â‰¤jâ‰¤dâˆ’1

X âˆ’Î±q j
has all its coefï¬cients in Fq
and is a unique irreducible polynomial from Fq[X] with a root at Î±. It is also a
unique monic polynomial of minimum degree in Fq[X] with a root at Î±.
Worked Example 3.1.36
Continuing Worked Example 3.1.28, we identify F16
with F2(Ï‰), the smallest ï¬eld containing a root Ï‰ of a primitive polynomial of
order 4. So, if we choose 1+X +X4, Ï‰ will satisfy Ï‰4 = 1+Ï‰, and if we choose
1+X3 +X4, Ï‰ will satisfy Ï‰4 = 1+Ï‰3. In both cases, the conjugates are Ï‰, Ï‰2,
Ï‰4 and Ï‰8.
Correspondingly, the table in (3.1.2) will take the form
1+X +X4
1+X3 +X4
power
of Ï‰
âˆ’âˆ’
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
vector
(word)
0000
1000
0100
0010
0001
1100
0110
0011
1101
1010
0101
1110
0111
1111
1011
1001
vector
(word)
0000
1000
0100
0010
0001
1001
1101
1111
1110
0111
1010
0101
1011
1100
0110
0011
(3.1.8)

282
Further Topics from Coding Theory
Under the left table addition rule, the minimal polynomial MÏ‰i(X) for the power
Ï‰i is 1 + X + X4 for i = 1,2,4,8 and 1 + X3 + X4 for i = 7,14,13,11, while for
i = 3,6,12,9 it is 1+X +X2 +X3 +X4 and for i = 5,10 it is 1+X +X2. Under the
right table addition rule, we have to swap polynomials 1+X +X4 and 1+X3 +X4.
Polynomials 1+X +X4 and 1+X3 +X4 are of order 15, polynomial 1+X +X2 +
X3 +X4 is of order 5 and 1+X +X2 of order 3.
A short way to produce these answers is to ï¬nd the expression for

Ï‰i4 as a
linear combination of 1, Ï‰i,

Ï‰i2 and

Ï‰i3. For example, from the left table we
have for Ï‰7:

Ï‰74 = Ï‰28 = Ï‰3 +Ï‰2 +1,

Ï‰73 = Ï‰21 = Ï‰3 +Ï‰2,
and readily see that

Ï‰74 = 1+

Ï‰73, which yields 1+X3 +X4. For complete-
ness, write down the unused expression for

Ï‰72:

Ï‰72 = Ï‰14 = Ï‰12Ï‰2 = (1+Ï‰)3Ï‰2 = (1+Ï‰ +Ï‰2 +Ï‰3)Ï‰2
= Ï‰2 +Ï‰3 +Ï‰4 +Ï‰5 = Ï‰2 +Ï‰3 +1+Ï‰ +(1+Ï‰)Ï‰ = 1+Ï‰3.
For MÏ‰5(X) the â€˜standardâ€™ approach gives a shortcut:
MÏ‰5(X) = (X âˆ’Ï‰5)(X âˆ’Ï‰10) = X2 +(Ï‰5 +Ï‰10)X +Ï‰15 = X2 +X +1.
So, the full list of minimal polynomials for F16 is
MÏ‰0(X) = 1+X, MÏ‰(X) = 1+X +X4,
MÏ‰3(X) = 1+X +X2 +X3 +X4,
MÏ‰5(X) = 1+X +X2, MÏ‰7(X) = 1+X3 +X4.
Example 3.1.37
For the ï¬eld F32 â‰ƒF2[X]/âŸ¨1 + X2 + X5âŸ©, the addition table is
calculated below. The minimal polynomials are
(i) 1+X2 +X5 for conjugates {Ï‰,Ï‰2,Ï‰4,Ï‰8,Ï‰16},
(ii) 1+X2 +X3 +X4 +X5 for {Ï‰3,Ï‰6,Ï‰12,Ï‰24,Ï‰17},
(iii) 1+X +X2 +X4 +X5 for {Ï‰5,Ï‰10,Ï‰20,Ï‰9,Ï‰18},
(iv) 1+X +X2 +X3 +X5 for {Ï‰7,Ï‰14,Ï‰28,Ï‰25,Ï‰19},
(v) 1+X +X3 +X4 +X5 for {Ï‰11,Ï‰22,Ï‰13,Ï‰26,Ï‰21},
(vi) 1+X3 +X5 for {Ï‰15,Ï‰30,Ï‰29,Ï‰27,Ï‰23}.

3.1 A primer on ï¬nite ï¬elds
283
All minimal polynomials have order 31.
power
of Ï‰
vector
(word)
power
of Ï‰
vector
(word)
âˆ’âˆ’
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
00000
10000
01000
00100
00010
00001
10100
01010
00101
10110
01011
10001
11100
01110
00111
10111
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
11111
11011
11001
11000
01100
00110
00011
10101
11110
01111
10011
11101
11010
01101
10010
01001
(3.1.9)
Deï¬nition 3.1.38
An automorphism of Fqn over Fq (in short, an (Fqn,Fq)-
automorphism) is a bijection Ïƒ : Fqn â†’Fqn with: (a) Ïƒ(a + b) = Ïƒ(a) + Ïƒ(b);
(b) Ïƒ(ab) = Ïƒ(a)Ïƒ(b); (c) Ïƒ(c) = c, for all a,b âˆˆFqn,c âˆˆFq.
Theorem 3.1.39
The set of (Fqn,Fq)-automorphisms is isomorphic to the cyclic
group Zn and generated by the Frobenius map Ïƒq(a) = aq,a âˆˆFqn.
Proof
Let Ï‰ âˆˆFqn be a primitive element. Then Ï‰qnâˆ’1 = e and MÏ‰(X) âˆˆFq[X]
has roots Ï‰, Ï‰q, Ï‰q2,...,Ï‰qnâˆ’1. An (Fqn;Fq)-automorphism Ï„ ï¬xes the coefï¬cients
of MÏ‰(X), thus it permutes the roots, and Ï„(Ï‰) = Ï‰q j for some j, 0 â‰¤j â‰¤nâˆ’1. But
as Ï‰ is primitive, Ï„ is completely determined by Ï„(Ï‰). Then as Ïƒq j(Ï‰) = Ï‰q j =
Ï„(Ï‰), we have that Ï„ = Ïƒq j.
The rest of this section is devoted to a study of roots of unity, i.e. the roots of the
polynomial Xn âˆ’e over ï¬eld Fq where q = ps and p =char (Fq). Without loss of
generality, we suppose from now on that
gcd(n,q) = 1, i.e. n and q are co-prime.
(3.1.10)
Indeed, if n and q are not co-prime, we can write n = mpk. Then, by Lemma 3.1.5
Xn âˆ’e = Xmpk âˆ’e = (Xm âˆ’e)pk,
and our analysis is reduced to the polynomial Xm âˆ’e.

284
Further Topics from Coding Theory
Deï¬nition 3.1.40
The roots of polynomial (Xn âˆ’e) âˆˆFq[X] in the splitting ï¬eld
Spl (Xn âˆ’e) = Fqs are called the nth roots of unity over Fq (or the (n,Fq)-roots of
unity). The set of all (n,Fq)-roots of unity is denoted by E(n). It turns out that the
value s is the least integer s â‰¥1 such that qs â‰¡1 modn (cf. Theorem 3.1.44 below).
This fact is reï¬‚ected in denoting the value s by ordn(q) and calling it the order of
q modn.
Under assumption (3.1.10), there is no multiple root (as the derivative âˆ‚X
(Xn âˆ’e) = nXnâˆ’1 does not have roots in Spl(Xn âˆ’e) = Fqs). Thus, â™¯E(n) = n.
Theorem 3.1.41
E(n) is a cyclic subgroup of Fâˆ—
qs.
Proof
Suppose Î±,Î² âˆˆE(n). Then (Î±Î² âˆ’1)n = Î±n(Î² n)âˆ’1 = e, i.e. Î±Î² âˆ’1 âˆˆE(n).
So, E(n) is a subgroup of the cyclic group Fâˆ—
qs and so is cyclic.
Deï¬nition 3.1.42
A generator of group E(n) (i.e. an nth root of unity whose
multiplicative order equals n) is called a primitive (n,Fq)-root of unity; it will be
denoted by Î².
Corollary 3.1.43
There are precisely Ï†(n) primitive (n,Fq)-roots of unity. In
particular, primitive (n,Fq)-roots of unity exist for any n co-prime to q.
This allows us to calculate s in the splitting ï¬eld Fqs = Spl(Xn âˆ’e). If Î² is a
primitive (n,Fq)-root of unity then its multiplicative order equals n. As Ï‰ Ì¸= 0, we
have that Ï‰ âˆˆFqr if Î² qr = Î², i.e. Î² qrâˆ’1 = e. This happens iff n|(qr âˆ’1). But s is
the least r with Fqr âˆ‹Ï‰.
Theorem 3.1.44
Spl(Xn âˆ’e) = Fqs, where s = ordn(q) is the least integer â‰¥1
for which n|(qs âˆ’1), i.e. the least integer s â‰¥1 with qs â‰¡1 modn.
It is instructive to stress similarities and differences between primitive elements
and primitive (n,Fq)-roots of unity in ï¬eld Fqs with s = ordn(q). A primitive ï¬eld
element, Ï‰, generates the multiplicative cyclic group Fâˆ—
qs: Fâˆ—
qs = {e,Ï‰,...,Ï‰qsâˆ’2};
its multiplicative order equals qs âˆ’1. A primitive root of unity, Î², generates the
multiplicative cyclic group E(n): E(n) = {e,Î²,...,Î² nâˆ’1}; its multiplicative order
equals n. [On the other hand, Î² generates Fqs as a ï¬eld element: Fqs = Fq(Î²) =
Fq(E(n)).] This suggests that Î² = Ï‰ happens iff n = qs âˆ’1. In fact, let us ask under
what condition a power Ï‰k is a primitive nth root of unity. As was established in
Worked Example 3.1.33 this happens when n|(qs âˆ’1), i.e. qs âˆ’1 = nr. In fact, if
k â‰¥1 is such that
gcd(k,nr) = gcd(k,qs âˆ’1) = r
then element Ï‰k is a primitive nth root of unity as its multiplicative order equals
qs âˆ’1
gcd(k,qs âˆ’1) = nr
r = n.

3.1 A primer on ï¬nite ï¬elds
285
This holds when k = ru and u is co-prime with n. Conversely, if Ï‰k is a primitive
root of unity then gcd(k,qs âˆ’1) = (qs âˆ’1)/n. Hence we obtain the following.
Theorem 3.1.45
Let P(n) be the set of the primitive (n,Fq)-roots of unity and T(n)
the set of primitive elements in Fqs = Spl(Xn âˆ’e). Then either (i) P(n) âˆ©T(n) = /0
or (ii) P(n) = T(n); case (ii) occurs iff n = qs âˆ’1.
Now we can factorise polynomial (Xn âˆ’e) over Fq by taking the product of the
distinct minimal polynomials for the (n,Fq)-roots of unity:
Xn âˆ’e = lcm

MÎ²(X) : Î² âˆˆE(n)
.
(3.1.11)
If we begin with a primitive element Ï‰ âˆˆFqs where s = ordn(q) then Î² = Ï‰(qsâˆ’1)/n
is a primitive root of unity and E(n) = {e,Î²,...,Î² nâˆ’1}.
This enables us to calculate the minimal polynomial MÎ² i(X). For all i =
0,...,nâˆ’1, the conjugates of Î² i are Î² i,Î² iq,...,Î² iqdâˆ’1 where d(= d(i)) is the least
positive integer for which Î² iqd = Î² i, i.e. Î² iqdâˆ’i = e. This is equivalent to n|(iqd âˆ’i),
i.e. iqd = i modn. Therefore,
Mi(X)

= MÎ² i(X)

=

X âˆ’Î² i
X âˆ’Î² iq
Â·Â·Â·

X âˆ’Î² iqdâˆ’1
.
(3.1.12)
Deï¬nition 3.1.46
The set of exponents i,iq,...,iqdâˆ’1 where d(= d(i)) is the
minimal positive integer such that iqd = i modn is called a cyclotomic coset (for i)
and denoted by Ci(= Ci(n,q)) (alternatively, CÏ‰i is deï¬ned as the set of non-zero
ï¬eld elements Ï‰i,Ï‰iq,...,Ï‰iqdâˆ’1).
Worked Example 3.1.47
Check that polynomials X2 +X +2 and X3 +2X2 +1
are primitive over F3 and compute the ï¬eld tables for F9 and F27 generated by these
polynomials.
Solution The ï¬eld F9 is isomorphic to F3[X]/âŸ¨X2 + X + 2âŸ©. The multiplicative
powers of Ï‰ âˆ¼X are
Ï‰2 âˆ¼2X +1, Ï‰3 âˆ¼2X +2, Ï‰4 âˆ¼2,
Ï‰5 âˆ¼2X, Ï‰6 âˆ¼X +2, Ï‰7 âˆ¼X +1, Ï‰8 âˆ¼1.
The cyclotomic coset of Ï‰ is {Ï‰,Ï‰3} (as Ï‰9 = Ï‰). Then the minimal polynomial
MÏ‰(X) = (X âˆ’Ï‰)(X âˆ’Ï‰3) = X2 âˆ’(Ï‰ +Ï‰3)X +Ï‰4
= X2 âˆ’2X +2 = X2 +X +2.
Hence, X2 +X +2 is primitive.

286
Further Topics from Coding Theory
Next, F27 â‰ƒF3[X]/âŸ¨X3 +2X2 +1âŸ©, and with Ï‰ âˆ¼X, we have
Ï‰2 âˆ¼X2, Ï‰3 âˆ¼X2 +2, Ï‰4 âˆ¼X2 +2X +2, Ï‰5 âˆ¼2X +2,
Ï‰6 âˆ¼2X2 +2X, Ï‰7 âˆ¼X2 +1, Ï‰8 âˆ¼X2 +X +2,
Ï‰9 âˆ¼2X2 +2X +2, Ï‰10 âˆ¼X2 +2X +1, Ï‰11 âˆ¼X +2,
Ï‰12 âˆ¼X2 +2X, Ï‰13 âˆ¼2, Ï‰14 âˆ¼2X, Ï‰15 âˆ¼2X2, Ï‰16 âˆ¼2X2 +1,
Ï‰17 âˆ¼2X2 +X +1, Ï‰18 âˆ¼X +1, Ï‰19 âˆ¼X2 +X,
Ï‰20 âˆ¼2X2 +2, Ï‰21 âˆ¼2X2 +2X +1, Ï‰22 âˆ¼X2 +X +1,
Ï‰23 âˆ¼2X2 +X +2, Ï‰24 âˆ¼2X +1, Ï‰25 âˆ¼2X2 +X, Ï‰26 âˆ¼1.
The cyclotomic coset of Ï‰ in F27 is {Ï‰,Ï‰3,Ï‰9}. Consequently, the primitive poly-
nomial
MÏ‰(X) = (X âˆ’Ï‰)(X âˆ’Ï‰3)(X âˆ’Ï‰9)
= X3 âˆ’(Ï‰ +Ï‰3 +Ï‰9)X2 +(Ï‰4 +Ï‰10 +Ï‰12)X âˆ’Ï‰13
= X3 +2X2 +1
as required.
Worked Example 3.1.48
(a) Consider the polynomial X15 âˆ’1 over F2 (with
n = 15, q = 2). Then Ï‰ = 2, s = ord15(2) = 4 and Spl(X15 âˆ’1) = F24 = F16.
The polynomial g(X) = 1+X +X4 is primitive: any of its roots Î² are primitive
in F16. So, the primitive (15,F2)-root of unity is
Î² = Ï‰(24âˆ’1)/15 = Ï‰.
Hence, the roots of X15 âˆ’1 are 1,Î²,...,Î² 14. The minimal polynomials for them
have been calculated in Worked Example 3.1.36. So, we have the factorisation
X15 âˆ’1 = (1+X)(1+X +X4)(1+X +X2 +X3 +X4)
Ã—(1+X +X2)(1+X3 +X4).
(b) Knowing the cyclotomic cosets we can show that a particular factorisation of
Xn âˆ’e contains irreducible factors. Explicitly, take the polynomial X9 âˆ’1 over F2
(with n = 9,q = 2). There are three cyclotomic cosets:
C0 = {0},C1 = {1,2,4,8,7,5},C3 = {3,6};
the corresponding minimal polynomials are of degree 1,6 and 2, respectively:
1+X, 1+X3 +X6 and 1+X +X2.
This yields
X9 âˆ’1 = (1+X)(1+X +X2)(1+X3 +X6).

3.1 A primer on ï¬nite ï¬elds
287
(c) Let us check primitivity of the polynomial
f(X) = 1+X +X6
over F2, with n = 6,
q = 2. Here, 26 âˆ’1 = 63 = 32 Â· 7. As 63/3 =
21,
32|ord( f(X)) â‡”X21 âˆ’1 Ì¸= 0 mod(1 + X + X6). But X21 = 1 + X + X3 +
X4 +X5 Ì¸= 1 mod(1+X +X6), so 32|ord( f(X)).
Next, as 63/7 = 9,
7|ord( f(X)) â‡”X9âˆ’1 Ì¸= 0 mod(1+X +X6). But X9 = 1+
X3 + X4 Ì¸= 1 mod (1 + X + X6), so 7|ord( f(X)). Therefore, ord( f(X)) = 63, and
f(X) is primitive. Theorem 3.1.53 below shows that any irreducible polynomial of
order 63 has degree 6 as 26 = 1 mod63.
(d) Now consider the polynomial
g(X) = 1+X +X2 +X4 +X6,
again over F2 (here n = 6 and q = 2, as before). Again 32|ord(g(X)) â‡”X21 Ì¸= 1
mod (1+X +X2 +X4 +X6). However, in F2
X21 âˆ’1 = (1+X)(1+X +X2)(1+X +X3)(1+X2 +X3)
Ã—(1+X +X2 +X4 +X6)(1+X2 +X4 +X5 +X6).
Hence, X21 âˆ’1 = 0 mod(1 + X + X2 + X4 + X6) = 1, and so 32 does not divide
ord(g(X)).
Next, 3|ord(g(X)) â‡”X7 Ì¸= 1 mod(1 + X + X2 + X4 + X6). As X7 = X + X2 +
X3 +X5 Ì¸= 1 mod(1+X +X2 +X4 +X6), 3 is a divisor for ord(g(X)).
Finally, 7|ord(g(X)) â‡”X9 Ì¸= 1 mod(1 + X + X2 + X4 + X6), and as X9 = 1 +
X2+X4 Ì¸= 1 mod(1+X +X2+X4+X6), 7 divides ord(g(X)). So, ord(g(X)) = 21.
Let us summarise results about minimal polynomials and roots of unity. We
know from Theorem 3.1.25 that for all integers d â‰¥1 and for all q = pd, where
p is prime and s â‰¥1 integer, there exists a primitive polynomial of degree d, say
MÏ‰(X), where Ï‰ is a primitive element in the ï¬eld Fqd. On the other hand, for all
irreducible polynomials f(X) âˆˆFq[X] of degree d, the roots of f(X) lie in the ï¬eld
Spl( f(X)) = Fqd and have the same multiplicative order ord( f(X)).
Theorem 3.1.49
Let polynomial f(X) âˆˆFq[X] be irreducible, of degree d, and
ord( f(X)) = â„“. Then:
(a) â„“|(qd âˆ’1),
(b) â™¯f(X)|

Xâ„“âˆ’e

,
(c) â„“|n iff f(X)|

Xn âˆ’e

,
(d) â„“is the least positive integer such that f(X)|

Xâ„“âˆ’e

.

288
Further Topics from Coding Theory
Proof
(a) Spl( f(X)) = Fqd, hence every root Î± of f(X) is a root of Xqdâˆ’1 âˆ’e. So,
it has ord(Î±)|(qd âˆ’1).
(b) Each root Î± of f(X) in Spl( f(X)) has ord(Î±) = â„“and hence is a root of (Xâ„“âˆ’e).
So, f(X)|

Xâ„“âˆ’e

.
(c) If f(X)|

Xn âˆ’e

then each root of f(X) is a root of Xn âˆ’e, i.e. ord(Î±)|n. So,
â„“|n. Conversely, if n = kâ„“then (Xâ„“âˆ’e)|(Xkâ„“âˆ’e) and f(X)|(Xn âˆ’e) by (b).
(d) Follows from (c).
Theorem 3.1.50
If f(X) âˆˆFq[X] is an irreducible polynomial of degree d and
order â„“then d = ordâ„“(q).
Proof
If Î± âˆˆFqd has f(Î±) = 0 then by Theorem 3.1.29, Fq(Î±) = Fqd =
Spl( f(X)). But Î± is also a primitive (â„“,Fq)-root of unity, so Fq(Î±) = Fq(E(â„“)) =
Spl(Xâ„“âˆ’e) = Fqs where s = ordâ„“(q). Hence, d = ordâ„“(q).
Worked Example 3.1.51
Use the Frobenius map Ïƒ : a â†’aq to prove that every
element a âˆˆFqn has a unique qjth root, for j = 1,...,nâˆ’1.
Suppose that q = ps is odd. Show that exactly a half of the non-zero elements of
Fq have square roots.
Solution The Frobenius map Ïƒ : a â†’aq is a bijection Fqn â†’Fqn. So, for all b âˆˆFqn
there exists unique a with aq = b (the qth root). The jth power iteration Ïƒ j : a â†’aq j
is also a bijection, so again for all b âˆˆFqn there exists unique a with aq j = b.
Observe that for all c âˆˆFq, c1/q j = c.
Now take Ï„ : a â†’a2, a multiplicative homomorphism Fâˆ—
q â†’Fâˆ—
q. If q is odd then
Fâˆ—
q â‰ƒZqâˆ’1 has an even number of elements qâˆ’1. We want to show that if Ï„(a) =
b then Ï„âˆ’1(b) consists of two elements, a and âˆ’a. In fact, Ï„(âˆ’a) = b. Also, if
Ï„(aâ€²) = b then Ï„(aâ€²aâˆ’1) = e.
So, we want to analyse Ï„âˆ’1(e). Clearly, Â±e âˆˆÏ„âˆ’1(e). On the other hand, if Ï‰
is a primitive element then Ï„(Ï‰(qâˆ’1)/2) = Ï‰qâˆ’1 = e and Ï„âˆ’1(e) consists of e = Ï‰0
and Ï‰(qâˆ’1)/2. So, Ï‰(qâˆ’1)/2 = âˆ’e.
Now if Ï„(aâ€²aâˆ’1) = e then aâ€²aâˆ’1 = Â±e and aâ€² = Â±a. Hence, Ï„ sends precisely two
elements, a and âˆ’a, into the same image, and its range Ï„(Fâˆ—
q) is a half of Fâˆ—
q.
Theorem 3.1.52
(cf. [92], Theorem 3.46.) Let polynomial p(X) âˆˆFq[X] be irre-
ducible, of degree n. Set m = gcd(d,n). Then m|n and p(X) factorises over Fqd into
m irreducible polynomials of degree n/m each. Hence, p(X) is irreducible over Fqd
iff m = 1.
Theorem 3.1.53
(cf. [92], Theorem 3.5.) Let gcd(d,q) = 1. The number of monic
irreducible polynomials of order â„“and degree d equals Ï†(â„“)/d if â„“â‰¥2, and the

3.1 A primer on ï¬nite ï¬elds
289
degree d = ordâ„“(q), equals 2 if â„“= d = 1, equals 0 in all other cases. In particu-
lar, the degree of an order â„“irreducible polynomial always equals ordâ„“(q), i.e. the
minimal s such that qs = 1 modâ„“. Here Ï†(â„“) is the Euler totient function.
The proofs of Theorems 3.1.52 and 3.1.53 are omitted (see [92]). We only
make a short comment about Theorem 3.1.53. If p(0) Ì¸= 0, the order of irreducible
polynomial p(X) of degree d coincides with the order of any of its roots in the
multiplicative group Fâˆ—
qd. So, the order is â„“iff d = ordâ„“(q) and p(X) divides the
so-called circular polynomial
Qâ„“(X) =
âˆ
s:gcd(s,â„“)=1
(X âˆ’Ï‰s).
In fact, the circular polynomial could be decomposed into a product of irreducible
polynomials, all of degree d = ordâ„“(q), and their number equals Ï†(â„“)/d. (In the
case d = â„“= 1 the polynomial p(X) = X should be accounted for as well.)
Concluding this section, we give short summaries of the facts of the theory of
ï¬nite ï¬elds discussed above.
Summary 1.55. A ï¬eld is a ring such that its non-zero elements form a commuta-
tive group under multiplication. (i) Any ï¬nite ï¬eld F has the number of elements
q = ps where p is prime, and the characteristic char(F) = p. (ii) Any two ï¬nite
ï¬elds with the same number of elements are isomorphic. Thus, for a given q = ps,
there exists, up to isomorphism, a unique ï¬eld of cardinality q; such a ï¬eld is de-
noted by Fq (it is often called a Galois ï¬eld of size q). When q is prime, the ï¬eld
Fq is isomorphic to the additive cyclic group Zp of p elements, equipped with mul-
tiplication mod p. (iii) The multiplicative group Fâˆ—
q of non-zero elements from Fq
is isomorphic to the additive cyclic group Zqâˆ’1 of q âˆ’1 elements. (iv) Field Fq
contains Fr as a subï¬eld iff r|q; in this case Fq is isomorphic to a linear space over
(i.e. with coefï¬cients from) Fr, of dimension logp(q/r). So, each prime number
p gives rise to an increasing sequence of ï¬nite ï¬elds Fps, s = 1,2,... An element
Ï‰ âˆˆFq generating the multiplicative group Fâˆ—
q is called a primitive element of Fq.
Summary 1.56. The polynomial ring over Fq is denoted by Fq[X]; if the polyno-
mials are considered mod g(X), a ï¬xed polynomial from Fq[X], the correspond-
ing ring is denoted by Fq[X]

âŸ¨g(X)âŸ©. (i) Ring Fq[X]

âŸ¨g(X)âŸ©is a ï¬eld iff g(X)
is irreducible over Fq (i.e. does not admit a decomposition g(X) = g1(X)g2(X)
where deg(g1(X)),deg(g2(X)) < deg(g(X))). (ii) For any q and a positive integer
d there exists an irreducible polynomial g(X) over Fq of degree d. (iii) If g(X) is
irreducible and degg(X) = d then the cardinality of ï¬eld Fq[X]

âŸ¨g(X)âŸ©is qd, i.e.
Fq[X]

âŸ¨g(X)âŸ©is isomorphic to Fqd and belongs to the same series of ï¬elds as Fq
(that is, char(Fqd) = char(Fq)).

290
Further Topics from Coding Theory
Summary 1.57. An extension of a ï¬eld Fq by a ï¬nite family of elements Î±1,...,Î±u
(contained in a larger ï¬eld from the same series) is the smallest ï¬eld containing Fq
and Î±i, 1 â‰¤i â‰¤u. Such a ï¬eld is denoted by Fq(Î±1,...,Î±u). (i) For any monic
polynomial p(X) âˆˆFq[X] there exists a larger ï¬eld Fqâ€² from the same series as Fq
such that p(X) factors over Fqâ€²:
p(X) = âˆ
1â‰¤jâ‰¤u

X âˆ’Î± j

, u = deg p(X), Î±1,...,Î±u âˆˆFqâ€².
(3.1.13)
The smallest ï¬eld Fqâ€² with this property (i.e. ï¬eld Fq(Î±1,...,Î±u)) is called a split-
ting ï¬eld for p(X); we also say that p(X) splits over Fq(Î±1,...,Î±u). The splitting
ï¬eld for p(X) is denoted by Spl(p(X)); an element Î± âˆˆSpl(p(X)) takes part in de-
composition (3.1.13) iff p(Î±) = 0. Field Spl(p(X)) is described as the set {g(Î±j)}
where j = 1,...,u, and g(X) âˆˆFq[X] are polynomials of degree < deg(p(X)). (ii)
Field Fq is splitting for the polynomial Xq âˆ’X. (iii) If polynomial p(X) of de-
gree d is irreducible over Fq and Î± is a root of p(X) in ï¬eld Spl(p(X)) then Fqd
â‰ƒFq[X]

âŸ¨p(X)âŸ©is isomorphic to Fq(Î±) and all the roots of p(X) in Spl(p(X))
are given by the conjugate elements Î±, Î±q, Î±q2,...,Î±qdâˆ’1. Thus, d is the small-
est positive integer for which Î±qd = Î±. (iv) Suppose that, for a given ï¬eld Fq,
a monic polynomial p(X) âˆˆFq[X] and an element Î± from a larger ï¬eld we have
p(Î±) = 0. Then there exists a unique minimal polynomial MÎ±(X) with the property
that MÎ±(Î±) = 0 (i.e. such that any other polynomial p(X) with p(Î±) = 0 is divided
by MÎ±(X)). Polynomial MÎ±(X) is the unique irreducible polynomial over Fq van-
ishing at Î±. It is also the unique polynomial of the minimum degree vanishing at Î±.
We call MÎ±(X) the minimal polynomial of Î± over Fq. If Ï‰ is a primitive element
of Fqd then MÏ‰(X) is called a primitive polynomial for Fqd over Fq. We say that
elements Î±,Î² âˆˆFqd are conjugate over Fq if they have the same minimal poly-
nomial over Fq. Then (v) the conjugates of Î± âˆˆFqd over Fq are Î±, Î±q,...,Î±qdâˆ’1,
where d is the smallest positive integer with Î±qd = Î±. When Î± = Ï‰i where Ï‰
is a primitive element, the congugacy class is associated with a cyclotomic coset
CÏ‰i = {Ï‰i,Ï‰iq,...,Ï‰iqdâˆ’1}.
Summary 1.58. Now assume that n and q = ps are co-prime and take polynomial
Xn âˆ’e. The roots of Xn âˆ’e in the splitting ï¬eld Spl(Xn âˆ’e) are called nth roots
of unity over Fq. The set of all nth roots of unity is denoted by En. (i) Set En is
a cyclic subgroup of order n in the multiplicative group of ï¬eld Spl(Xn âˆ’e). An
nth root of unity generating En is called a primitive nth root of unity. (ii) If Fqs is
Spl(Xnâˆ’e) then s is the smallest positive integer with n|(qsâˆ’1). (iii) Let Î n be the
set of primitive nth roots of unity over ï¬eld Fq and Î¦n the set of primitive elements
of the splitting ï¬eld Fqs = Spl(Xs âˆ’e). Then either Î n âˆ©Î¦n = /0 or Î n = Î¦n, the
latter happening iff n = qs âˆ’1.

3.2 Reedâ€“Solomon codes. The BCH codes revisited
291
3.2 Reedâ€“Solomon codes. The BCH codes revisited
From now on we consider ï¬nite ï¬elds Fq up to isomorphism, but from time to
time refer to a speciï¬c ï¬eld table (e.g. by specifying Fps as Fp[X]/âŸ¨P(X)âŸ©where
P(X) âˆˆFp[X] is an irreducible polynomial of degree s).
In Deï¬nition 2.5.37 we introduced narrow-sense binary BCH codes. Our study
will continue in this section with general q-ary BCH codes X BCH
q,N,Î´,Ï‰,b, of length
N, designed distance Î´ and zeros Ï‰b,...,Ï‰b+Î´âˆ’1; see in Deï¬nition 3.2.7 below.
Prior to that, we discuss an interesting special class of BCH codes formed by the
Reedâ€“Solomon (RS) codes; as we shall see, their analysis is facilitated by the fact
that the RS codes are MDS (maximum distance separable).
Deï¬nition 3.2.1
Given q â‰¥3, a q-ary Reedâ€“Solomon code is deï¬ned as a cyclic
code of length N = qâˆ’1 with the generator
g(X) = (X âˆ’Ï‰b)(X âˆ’Ï‰b+1)...(X âˆ’Ï‰b+Î´âˆ’2),
(3.2.1)
where Î´ and b are integers, 1 â‰¤Î´, b < q âˆ’1, and Ï‰ is a primitive element of Fq
(or equivalently, a primitive Nth root of unity). Such a code is denoted by X RS

= X RS
q,Î´,Ï‰,b

.
According to Deï¬nition 3.2.7, the RS code is identiï¬ed as X BCH
q,qâˆ’1,Î´,Ï‰,b, i.e. as a
q-ary BCH code of length qâˆ’1 and designed distance Î´. There are no reasonable
binary RS codes, as in this case the length qâˆ’1 = 1. Observe that qâˆ’1 gives the
number of non-zero elements in the alphabet ï¬eld Fq. Moreover, for N = qâˆ’1 we
have
XN âˆ’e = Xqâˆ’1 âˆ’e = âˆ
Î±âˆˆFâˆ—q
(X âˆ’Î±)
(as the splitting ï¬eld Spl (Xq âˆ’X) is Fq). Furthermore, owing to the fact that Ï‰ is a
primitive (qâˆ’1,Fq) root of unity (or, equivalently, a primitive element of Fq), the
minimal polynomial Mi(X) is just X âˆ’Ï‰i, for all i = 0,...,N âˆ’1.
An important property is that the RS codes are MDS. Indeed, the generator g(X)
of X RS
q,Î´,Ï‰,b has degg(X) = Î´ âˆ’1. Hence, the rank k is given by
k = dim(X RS
q,Î´,Ï‰,b) = N âˆ’degg(X) = N âˆ’Î´ +1.
(3.2.2)
By the generalised BCH bound (see Theorem 3.2.9 below), the minimal distance
d

X RS
q,Î´,Ï‰,b

â‰¥Î´ = N âˆ’k +1.
But the Singleton bound states that d(X RS) â‰¤N âˆ’k +1. Hence,
d(X RS
q,d,Ï‰,b) = N âˆ’k +1 = Î´.
(3.2.3)

292
Further Topics from Coding Theory
Thus the RS codes have the largest possible minimal distance among all q-ary
codes of length qâˆ’1 and dimension k = qâˆ’Î´. Summarising, we obtain
Theorem 3.2.2
The code X RS
q,Î´,Ï‰,b is MDS and has distance Î´ and rank qâˆ’Î´.
The dual of a BCH code is not always BCH. However,
Theorem 3.2.3
The dual of an RS code is an RS code.
Proof
The proof is straightforward, as (X RS
q,Î´,Ï‰,b)âŠ¥= X RS
q,qâˆ’Î´,Ï‰,b+Î´âˆ’1.
Theorem 3.2.4
Let X RS be a [N,k,Î´] RS code. Then its parity-check extension
is a [N +1,k,Î´ +1] code, with distance one more than that of X RS.
Proof
Let c(X) = c0 +c1X +Â·Â·Â·+cNâˆ’1XNâˆ’1 âˆˆX RS, with weight w(c(X)) = Î´.
Its extension is c(X) = c(X)+cNXN, with cN = âˆ’
âˆ‘
0â‰¤iâ‰¤Nâˆ’1
ci = âˆ’c(e). We want to
show that c(e) Ì¸= 0 and hence w(c(X)) = Î´ +1.
To simplify notation assume that b = 1 and let g(X) = (X âˆ’Ï‰)(X âˆ’Ï‰2)...(X âˆ’
Ï‰Î´âˆ’1) be the generator of X RS. Write c(X) = g(X)p(X) for some p(X), yielding
that c(e) = p(e)g(e). Clearly, g(e) Ì¸= 0, as Ï‰i Ì¸= e for all i = 1,...,Î´ âˆ’1. If p(e) = 0,
the polynomial g1(X) = (X âˆ’e)g(X) divides c(X). Then c(X) âˆˆâŸ¨g1(X)âŸ©where
g1(X) = (X âˆ’e)(X âˆ’Ï‰)...(X âˆ’Ï‰Î´âˆ’1). That is, âŸ¨g1(X)âŸ©is BCH, with the designed
distance â‰¥Î´ +1. But this contradicts the choice of c(X).
RS codes admit speciï¬c (and elegant) encoding and decoding procedures. Let
X RS be an [N,k,Î´] RS code, with N = q âˆ’1. For a message string a0 ...akâˆ’1 set
a(X) =
âˆ‘
0â‰¤iâ‰¤kâˆ’1
aiXi and encode a(X) as c(X) =
âˆ‘
0â‰¤jâ‰¤Nâˆ’1
a(Ï‰ j)X j. To show that
c(X) âˆˆX RS, we have to check that c(Ï‰) = Â·Â·Â· = c(Ï‰Î´âˆ’1) = 0. Think of a(X) as
a polynomial
âˆ‘
0â‰¤iâ‰¤Nâˆ’1
aiXi with ai = 0 for i â‰¥k, and use
Lemma 3.2.5
Let a(X) = a0 +a1X +Â·Â·Â·+aNâˆ’1XNâˆ’1 âˆˆFq[X] and Ï‰ be a prim-
itive (N,Fq) root of unity over Fq, N = qâˆ’1. Then
ai = 1
N
âˆ‘
0â‰¤jâ‰¤Nâˆ’1
a(Ï‰ j)Ï‰âˆ’ij.
(3.2.4)
We postpone the proof till after Lemma 3.2.12.
Indeed, by Lemma 3.2.5
ai = 1
N
âˆ‘
0â‰¤jâ‰¤Nâˆ’1
a(Ï‰ j)Ï‰âˆ’i j = 1
N c(Ï‰âˆ’i) = 1
N c(Ï‰Nâˆ’i),

3.2 Reedâ€“Solomon codes. The BCH codes revisited
293
so c(Ï‰ j) = NaNâˆ’j. For 0 â‰¤j â‰¤Î´ âˆ’1 = N âˆ’k, c(Ï‰ j) = NaNâˆ’j = 0. Therefore,
c(X) âˆˆX RS. In addition, the original message is easy to recover from c(X): ai =
1
N c(Ï‰Nâˆ’i).
To decode the received word u(X) = c(X)+e(X), write
ui = ci +ei = ei +a(Ï‰i), 0 â‰¤i â‰¤N âˆ’1.
Then obtain
u0 = e0 +a0 +a1 +Â·Â·Â·+akâˆ’1,
u1 = e1 +a0 +a1Ï‰ +Â·Â·Â·+akâˆ’1Ï‰kâˆ’1,
u2 = e2 +a0 +a1Ï‰2 +Â·Â·Â·+akâˆ’1Ï‰2(kâˆ’1),
...
uNâˆ’1 = eNâˆ’1 +a0 +a1Ï‰Nâˆ’1 +Â·Â·Â·+akâˆ’1Ï‰(Nâˆ’1)(kâˆ’1).
If there are no errors, i.e. e0 = Â·Â·Â· = eNâˆ’1 = 0, any k of these equations can
be solved in the k unknowns a0,...,akâˆ’1, as the corresponding matrix is Vander-
monde. In fact, any subsystem of k equations can be solved for any error vector (it
is a different matter if the solution will give the correct string a0,...,akâˆ’1 or not).
Now suppose that t errors have occurred, t < N âˆ’k. Call the equations with
ei = 0 good and ei Ì¸= 0 bad, then we have t bad and N âˆ’t good ones. If we solve
all subsystems of k equations then the
N âˆ’t
k

subsystems consisting of k good
equations will give the correct values of the ais. Moreover, a given incorrect solu-
tion cannot satisfy any set of k good equations; it can satisfy at most k âˆ’1 correct
equations. In addition, it can satisfy at most t incorrect equations. So, it is a solu-
tion to â‰¤t +kâˆ’1 equations, i.e. can be obtained â‰¤
t+kâˆ’1
k

times from subsystems
of k equations. Hence, if
N âˆ’t
k

>
t +k âˆ’1
k

,
the majority solution from among (N
k) solutions gives the true values of the ais. The
last inequality holds iff N âˆ’t > t +kâˆ’1, i.e. Î´ = N âˆ’k+1 > 2t. Therefore we get:
Theorem 3.2.6
For a [N,k,Î´] RS code X RS, the majority logic decoding cor-
rects up to t < Î´/2 errors, at the cost of having to solve
N
k

systems of equations
of size k Ã—k.
Reedâ€“Solomon codes were discovered in 1960 by Irving S. Reed and Gustave
Solomon, both working at that time in the Lincoln Laboratory of MIT. When their
joint article was published, an efï¬cient decoding algorithm for these codes was

294
Further Topics from Coding Theory
not known. Such an algorithm solution for the latter was found in 1969 by El-
wyn Berlekamp and James Massey, and is known since as the Berlekampâ€“Massey
decoding algorithm (cf. [20]); see Section 3.3. Later on, other algorithms were
proposed: continued fraction algorithm and Euclidean algorithm (see [112]).
Reedâ€“Solomon codes played an important role in transmitting digital pictures
from American spacecraft throughout the 1970s and 1980s, often in combination
with other code constructions. These codes still ï¬gure prominently in modern space
missions although the advent of turbo-codes provides a much wider choice of cod-
ing and decoding procedures.
Reedâ€“Solomon codes are also a key component in compact disc and digital game
production. The encoding and decoding schemes employed here are capable of cor-
recting bursts of up to 4000 errors (which makes about 2.5mm on the disc surface).
Deï¬nition 3.2.7
A BCH code X BCH
q,N,Î´,Ï‰,b with parameters q, N, Ï‰, Î´ and b is the
q-ary cyclic code XN = âŸ¨g(X)âŸ©with length N, designed distance Î´, such that its
generating polynomial is
g(X) = lcm

MÏ‰b(X),MÏ‰b+1(X),...,MÏ‰b+Î´âˆ’2(X)

,
(3.2.5)
i.e.
X BCH
q,N,Î´,Ï‰,b =
*
f(X) âˆˆFq[X] mod(XN âˆ’1) :
f(Ï‰b+i) = 0, 0 â‰¤i â‰¤Î´ âˆ’2
+
.
If b = 1, this is a narrow sense BCH code. If Ï‰ is a primitive Nth root of unity, i.e. a
primitive root of the polynomial XN âˆ’1, the BCH code is called primitive. (Recall
that under condition gcd(q,N) = 1 these roots form a commutative multiplicative
group which is cyclic, of order N, and Ï‰ is a generator of this group.)
Lemma 3.2.8
The BCH code X BCH
q,N,Î´,Ï‰,b has minimum distance â‰¥Î´.
Proof
Without loss of generality consider a narrow sense code. Set the parity-
check (Î´ âˆ’1)Ã—N matrix
H =
â›
âœ
âœ
âœ
â
1
Ï‰
Ï‰2
...
Ï‰Nâˆ’1
1
Ï‰2
Ï‰4
...
Ï‰2(Nâˆ’1)
...
...
...
...
1
Ï‰Î´âˆ’1
Ï‰2(Î´âˆ’1)
...
Ï‰(Î´âˆ’1)(Nâˆ’1)
â
âŸ
âŸ
âŸ
â .
The codewords of X are linear dependence relations between the columns of H.
Then Lemma 2.5.40 implies that any Î´ âˆ’1 columns of H are linearly independent.
In fact, select columns with top (row 1) entries Ï‰k1,...,Ï‰kÎ´âˆ’1 where 0 â‰¤k1 < Â·Â·Â· <
kÎ´âˆ’1 â‰¤N âˆ’1. They form a square (Î´ âˆ’1)Ã—(Î´ âˆ’1) matrix

3.2 Reedâ€“Solomon codes. The BCH codes revisited
295
D =
â›
âœ
âœ
âœ
â
Ï‰k1 Â·1
Ï‰k2 Â·1
...
Ï‰kÎ´âˆ’1 Â·1
Ï‰k1 Â·Ï‰k1
Ï‰k2 Â·Ï‰k2
...
Ï‰kÎ´âˆ’1 Â·Ï‰kÎ´âˆ’1
...
...
...
...
Ï‰k1 Â·Ï‰k1(Î´âˆ’2)
Ï‰k2 Â·Ï‰k2(Î´âˆ’2)
...
Ï‰kÎ´âˆ’1 Â·Ï‰kÎ´âˆ’1(Î´âˆ’2)
â
âŸ
âŸ
âŸ
â 
that differs from the Vandermonde matrix by factors Ï‰ks in front of the sth column.
Then the determinant of D is the product
detD =
Î´âˆ’1
âˆ
s=1
Ï‰ks


1
1
...
1
Ï‰k1
Ï‰k2
...
Ï‰kÎ´âˆ’1
...
...
...
...
Ï‰k1(Î´âˆ’2)
Î±k2(Î´âˆ’2)
...
Ï‰kÎ´âˆ’1(Î´âˆ’2)

=
Î´âˆ’1
âˆ
s=1
Ï‰ks

Ã—

âˆ
i> j

Ï‰ki âˆ’Ï‰k j
Ì¸= 0,
and any Î´ âˆ’1 columns of H are indeed linearly independent. In turn, this means
that any non-zero codeword in X has weight at least Î´. Thus, X has minimum
distance â‰¥Î´.
Theorem 3.2.9
(A generalisation of the BCH bound) Let Ï‰ be a primitive Nth
root of unity and b â‰¥1, r â‰¥1 and Î´ > 2 integers, with gcd(r,N) = 1. Consider a
cyclic code X = âŸ¨g(X)âŸ©of length N where g(X) is a monic polynomial of small-
est degree with g(Ï‰b) = g(Ï‰b+r) = Â·Â·Â· = g(Ï‰b+(Î´âˆ’2)r) = 0. Prove that X has
d(X ) â‰¥Î´.
Proof
As gcd(r,N) = 1, Ï‰r is a primitive root of unity. So, we can repeat the
proof given above, with b replaced by bru where ru is found from ru+Nv = 1. An
alternative solution: the matrix N Ã—(Î´ âˆ’1)
â›
âœ
âœ
âœ
âœ
âœ
â
1
1
...
1
Ï‰b
Ï‰b+r
...
Ï‰b+(Î´âˆ’2)r
Ï‰2b
Ï‰2(b+r)
...
Ï‰2(b+(Î´âˆ’2)r)
...
...
...
...
Ï‰(Nâˆ’1)b
Ï‰(Nâˆ’1)b+r
...
Ï‰(Nâˆ’1)(b+(Î´âˆ’2)r)
â
âŸ
âŸ
âŸ
âŸ
âŸ
â 
checks the code X = âŸ¨g(X)âŸ©. Take any of its (Î´ âˆ’1) Ã— (Î´ âˆ’1) submatrices, say,
with rows i1 < i2 < Â·Â·Â· < iÎ´âˆ’1. Denote it by D = (Djk). Then
detD =
âˆ
1â‰¤lâ‰¤Î´âˆ’1
Ï‰(ilâˆ’1)b det(Ï‰r(i jâˆ’1)(Î´âˆ’2))
=
âˆ
1â‰¤lâ‰¤Î´âˆ’1
Ï‰(ilâˆ’1)b det (Vandermonde) Ì¸= 0,
because gcd(r,N) = 1. So, d(X) â‰¥Î´.

296
Further Topics from Coding Theory
Worked Example 3.2.10
Let Ï‰ be a primitive n-root of unity in an extension
ï¬eld of Fq and a(X) =
âˆ‘
0â‰¤iâ‰¤nâˆ’1
aiXi be a polynomial of degree at most n âˆ’1. The
Mattsonâ€“Solomon polynomial is deï¬ned by
aMS(X) =
n
âˆ‘
j=1
a(Ï‰ j)Xnâˆ’j.
(3.2.6)
Let q = 2 and a(X) âˆˆF2[X]/âŸ¨Xnâˆ’1âŸ©. Prove that the Mattsonâ€“Solomon polynomial
aMS(X) is idempotent, i.e. aMS(X)2 = aMS(X) in F2[X]/âŸ¨Xn âˆ’1âŸ©.
Solution Let a(X) =
âˆ‘
0â‰¤iâ‰¤nâˆ’1
aiXi, then nai = aMS(Ï‰i),0 â‰¤i â‰¤n âˆ’1, by Lemma
3.2.5. In F2,(nai)2 = nai, so aMS(Ï‰i)2 = aMS(Ï‰i). For polynomials, write b(2)(X)
for the square in F2[X] and b(X)2 for the square in F2[X]/âŸ¨Xn âˆ’1âŸ©:
b(2)(X) = c(X)(Xn âˆ’1)+b(X)2.
Then
aMS(X) â†¾X=Ï‰i = (aMS(X) â†¾X=Ï‰i)2 = a(2)
MS(X) â†¾X=Ï‰i
= aMS(X)2 â†¾X=Ï‰i,
i.e. polynomials aMS(X) and aMS(X)2 agree at Ï‰0 = e,Ï‰,...,Ï‰nâˆ’1. Write this in
the matrix form, with aMS(X) = a0,MS + a1,MSX + Â·Â·Â· + anâˆ’1,MSXnâˆ’1, aMS(X)2 =
aâ€²
0,MSX +Â·Â·Â·+aâ€²
nâˆ’1,MSXnâˆ’1:
(aMS âˆ’a(2)â€²
MS)
â›
âœ
âœ
âœ
â
e
e
...
e
e
Ï‰
...
Ï‰nâˆ’1
...
...
...
...
e
Ï‰nâˆ’1
...
Ï‰(nâˆ’1)2
â
âŸ
âŸ
âŸ
â = 0.
As the matrix is Vandermonde, its determinant is
âˆ
0â‰¤i< jâ‰¤nâˆ’1
(Ï‰ j âˆ’Ï‰i) Ì¸= 0,
and aMS = a(2)â€²
MS. So, aMS(X) = aMS(X)2.
Deï¬nition 3.2.11
Let v = v0v1 ...vNâˆ’1 be a vector over Fq, and let Ï‰ be a prim-
itive (N,Fq) root of unity over Fq. The Fourier transform of the vector v is the
vector V = V0V1 ...VNâˆ’1 with components given by
Vj =
Nâˆ’1
âˆ‘
i=0
Ï‰i jvi, j = 0,...,N âˆ’1.
(3.2.7)

3.2 Reedâ€“Solomon codes. The BCH codes revisited
297
Lemma 3.2.12
(The inversion formula) The vector v is recovered from its Fourier
transform V by the formula
vi = 1
N
Nâˆ’1
âˆ‘
j=0
Ï‰âˆ’ijVj.
(3.2.8)
Proof
In any ï¬eld XN âˆ’1 = (X âˆ’1)(XNâˆ’1 +Â·Â·Â·+X +1). As the order of Ï‰ is N,
for any r, Ï‰r is a zero of LHS. Hence for all r Ì¸= 0 modN, Ï‰r is a zero of the last
term, i.e.
Nâˆ’1
âˆ‘
j=0
Ï‰r j = 0 modN.
On the other hand, for r = 0
Nâˆ’1
âˆ‘
j=0
Ï‰r j = N mod p
which is not zero if N is not a multiple of the ï¬eld characteristic p. But q âˆ’1 =
ps âˆ’1 is a multiple of N, so N is not a multiple of p. Hence, N Ì¸= 0 mod p. Finally,
change the order of summation to obtain that
1
N
Nâˆ’1
âˆ‘
j=0
Ï‰âˆ’i jVj = 1
N
Nâˆ’1
âˆ‘
k=0
vk
Nâˆ’1
âˆ‘
j=0
Ï‰(kâˆ’i) j = vi.
Proof of Lemma 3.2.5
Let a(X) = a0 +a1X +Â·Â·Â·+aNâˆ’1XNâˆ’1 âˆˆFq[X] and Ï‰ be
a primitive (N,Fq) root of unity over Fq. Then write
Nâˆ’1
âˆ‘
0â‰¤jâ‰¤Nâˆ’1
a(Ï‰ j)Ï‰âˆ’i j = Nâˆ’1
âˆ‘
0â‰¤jâ‰¤Nâˆ’1
âˆ‘
0â‰¤kâ‰¤Nâˆ’1
akÏ‰ jkÏ‰âˆ’ij
= Nâˆ’1
âˆ‘
0â‰¤kâ‰¤Nâˆ’1
ak
âˆ‘
0â‰¤jâ‰¤Nâˆ’1
Ï‰ j(kâˆ’i) = Nâˆ’1
âˆ‘
0â‰¤kâ‰¤Nâˆ’1
akNÎ´ki = ai.
Here we used the fact that, for 1 â‰¤â„“â‰¤N âˆ’1, Ï‰â„“Ì¸= 1, and
âˆ‘
0â‰¤jâ‰¤Nâˆ’1
Ï‰ jâ„“=
âˆ‘
0â‰¤jâ‰¤Nâˆ’1
(Ï‰â„“) j = (eâˆ’(Ï‰â„“)N)(eâˆ’Ï‰â„“)âˆ’1 = 0.
Hence
ai = 1
N
âˆ‘
0â‰¤jâ‰¤Nâˆ’1
a(Ï‰ j)Ï‰âˆ’ij.
(3.2.9)

298
Further Topics from Coding Theory
Worked Example 3.2.13
Give an alternative proof of the BCH bound: Let Ï‰ be a
primitive (N,Fq) root of unity and b â‰¥1 and Î´ â‰¥2 integers. Let XN = âŸ¨g(X)âŸ©be a
cyclic code where g(X) âˆˆFq[X]/âŸ¨XN âˆ’eâŸ©is a monic polynomial of smallest degree
having Ï‰b, Ï‰b+1,...,Ï‰b+Î´âˆ’2 among its roots. Then XN has minimum distance at
least Î´.
Solution
Let a(X) =
âˆ‘
0â‰¤jâ‰¤Nâˆ’1
a jX j âˆˆXN satisfy condition g(X)|a(X) and
a(Ï‰i)=0 for i = b,...,b + Î´ âˆ’2. Consider the Mattsonâ€“Solomon polynomial
cMS(X) for a(X):
cMS(X) =
âˆ‘
0â‰¤iâ‰¤Nâˆ’1
a(Ï‰âˆ’i)Xi =
âˆ‘
0â‰¤iâ‰¤Nâˆ’1
a(Ï‰Nâˆ’i)Xi
= âˆ‘
1â‰¤iâ‰¤N
a(Ï‰i)XNâˆ’i
=
âˆ‘
1â‰¤jâ‰¤bâˆ’1
a(Ï‰ j)XNâˆ’j +0+Â·Â·Â·+0 (from Ï‰b,...,Ï‰b+Î´âˆ’2 )
+a(Ï‰b+Î´âˆ’1)XNâˆ’bâˆ’Î´+1 +Â·Â·Â·+a(Ï‰N).
(3.2.10)
Multiply by Xbâˆ’1 and group:
Xbâˆ’1cMS(X) = a(Ï‰)XN+bâˆ’2 +Â·Â·Â·+a(Ï‰bâˆ’1)XN
+a(Ï‰b+Î´âˆ’1)XNâˆ’Î´ +Â·Â·Â·+a(Ï‰N)Xbâˆ’1
= XN 	
a(Ï‰)Xbâˆ’2 +Â·Â·Â·+a(Ï‰bâˆ’1)

+
	
a(Ï‰b+Î´âˆ’1)XNâˆ’Î´ +Â·Â·Â·+a(Ï‰N)Xbâˆ’1
= XN p1(X)+q(X)
= (XN âˆ’e)p1(X)+ p1(X)+q(X).
We see that cMS(Ï‰i) = 0 iff p1(Ï‰i)+q(Ï‰i) = 0. But p1(X)+q(X) is a polynomial
of degree â‰¤N âˆ’Î´ so it has at most N âˆ’Î´ roots. Thus, cMS(X) has at most N âˆ’Î´
roots of the form Ï‰i.
Therefore, the inversion formula (3.2.8) implies that the weight w(a(X)) (i.e. the
weight of the coefï¬cient string a0 ...aNâˆ’1) obeys
w(a(X)) â‰¥N âˆ’the number of roots of cMS(X) of the form Ï‰i.
(3.2.11)
That is,
w(a(X)) â‰¥N âˆ’(N âˆ’Î´) = Î´.
We ï¬nish this section with a brief discussion of the Guruswamiâ€“Sudan decoding
algorithm, for list decoding of Reedâ€“Solomon codes. First, we have to provide an
alternative description of the Reedâ€“Solomon codes (as Reed and Solomon have

3.2 Reedâ€“Solomon codes. The BCH codes revisited
299
done it in their joint paper). For brevity, we take the value b = 1 (but will be able
to extend the deï¬nition to values of N > qâˆ’1).
Given N â‰¤q, let S = {x1,...,xN} âŠ‚Fq be a set of N distinct points in Fq (a
supporting set). Let Ev denote the evaluation map
Ev : f âˆˆFq[X] â†’Ev( f) = ( f(x1),..., f(xN)) âˆˆFN
q
(3.2.12)
and take
L = { f âˆˆFq[X]: deg f < k}.
(3.2.13)
Then the q-ary Reedâ€“Solomon code of length N and dimension k can be deï¬ned as
X = Ev(L);
(3.2.14)
it has the minimum distance d = d(X ) = N âˆ’k+1 and corrects up to
Ad âˆ’1
2
B
er-
rors. The encoding of a source message u = u0 ...ukâˆ’1 âˆˆFk
q consists in calculating
the values of the polynomial f(X) = u0 +u1X +Â·Â·Â·+ukXkâˆ’1 at points xi âˆˆS.
Deï¬nition 3.2.1 (where X was deï¬ned as the set of polynomials c(X) =
âˆ‘
0â‰¤l<qâˆ’1
clXl âˆˆFq[X] with c(Ï‰) = c(Ï‰2) = Â·Â·Â· = c(Ï‰Î´âˆ’1) = 0) emerges when
N = q âˆ’1, k = N âˆ’Î´ + 1 = q âˆ’Î´, the supporting set S = {e,Ï‰,...,Ï‰Nâˆ’1} and
the coefï¬cients c0,c1,...,cNâˆ’1 are related to the polynomial f(X) by
ci = f(Ï‰i), 0 â‰¤i â‰¤N âˆ’1.
This determines uniquely the coefï¬cients fl in the representation f(X) =
âˆ‘
0â‰¤l<N
flXl, via the discrete inverse Fourier transform relation
N fl = c(Ï‰Nâˆ’l), or N fNâˆ’lâˆ’1 = c(Ï‰l+1), l = 0,...,N âˆ’1,
guaranteeing, in particular, that fk = Â·Â·Â· = fNâˆ’1 = 0.
Given f âˆˆFq[X] and y = y1 ...yN âˆˆFN
q , set
dist ( f,y) = âˆ‘
1â‰¤iâ‰¤N
1( f(xi) Ì¸= yi).
Now assume y = y1 ...yN is a received word and set t =
Ad âˆ’1
2
B
. The above-
mentioned â€˜conventionalâ€™ decoding algorithms (the Berlekampâ€“Massey algorithm,
the continued fractions algorithm and the Euclidean algorithm) follow the same
principle: the algorithm either ï¬nds a unique f such that dist ( f,y) â‰¤t or reports
that such f does not exist. On the other hand, given s > t, list decoding attempts to
ï¬nd all f with dist ( f,y) â‰¤s; the hope is that if we are lucky, the codeword with
this property will be unique, and we will be able to correct s errors, exceeding the
â€˜conventionalâ€™ limit of t errors.

300
Further Topics from Coding Theory
This idea goes back to Shannonâ€™s bounded distance decoding: upon receiving
a word y, you inspect the Hamming balls around y until you encounter a closest
codeword (or a collection of closest codewords) to y. Of course, we want two
things: that (i) when we take s â€˜moderatelyâ€™ larger than t, the chance of ï¬nding
two or more codewords within distance s is small, and (ii) the algorithm has a
reasonable computational complexity.
Example 3.2.14
The [32,8] RS code over F32 has d = 25 and t = 12. If we take
s = 13, the Hamming ball about the received word y may contain two codewords.
However, assuming that all error vectors e of weight 13 are equally likely, the
probability of this event is 2.08437Ã—10âˆ’12.
The Guruswamiâ€“Sudan list decoding algorithm (see [59]) performs the task of
ï¬nding the codewords within distance s for t â‰¤s â‰¤tGS in a polynomial time. Here
tGS = nâˆ’1âˆ’
L$
(k âˆ’1)n
M
,
and tGS can considerably exceed t.
In the above example, tGS = 17. Asymptotically, for RS codes of rate R, the
conventional decoding algorithms will correct a fraction (1âˆ’R)/2 of errors, while
the GS algorithm can correct up to 1âˆ’
âˆš
R. The expected number of codewords in
a ball of radius s â‰¤tGS (under the assumption of error-vector equidistribution) can
also be assessed.
The Guruswamiâ€“Sudan algorithm works not only for the RS codes. In the origi-
nal GS paper, the algorithm was shown to perform well for several classes of codes;
later on it was extended to cover the RM codes as well (see [7]).
3.3 Cyclic codes revisited. Decoding the BHC codes
Let us begin afresh. As before, we assume that gcd(N,q) = 1 (so if q = 2, N is odd),
and write words x âˆˆHN,q as x0 ...xNâˆ’1. Remind that a linear code X âŠ†HN is
called cyclic if, for all x = x0 ...xNâˆ’1 âˆˆX , the cyclic shift Ï€x = xNâˆ’1x0 ...xNâˆ’2 âˆˆ
X . With each word c = c0 ...cNâˆ’1 we associate a polynomial c(X) âˆˆFq[X]:
c(X) = c0 +c1X +Â·Â·Â·+cNâˆ’1XNâˆ’1.
The map c â†”c(X) is an isomorphism between X and a linear subspace of Fq[X].
Writing c(X) âˆˆX simply means that the coefï¬cient string c0 ...cNâˆ’1 âˆˆX .
Lemma 3.3.1
The code X is cyclic iff its image under the above isomorphism
is an ideal in the quotient ring Fq[X]/âŸ¨XN âˆ’eâŸ©.
Proof
Cyclic shift corresponds to multiplying a polynomial c(X) by X. Hence,
multiplication by any polynomial preserves X .

3.3 Cyclic codes revisited. Decoding the BHC codes
301
It is fruitful to think of X as the ideal in Fq[X]/âŸ¨XN âˆ’eâŸ©and consider all poly-
nomials mod(XN âˆ’e). Moreover, Fq[X]/âŸ¨XN âˆ’eâŸ©is a principal ideal ring: each
its ideal is of the form
âŸ¨g(X)âŸ©= { f(X) :
f(X) = g(X)h(X), h(X) âˆˆFq[X]/âŸ¨XN âˆ’eâŸ©}
(3.3.1)
where g(X) is a ï¬xed polynomial.
Theorem 3.3.2
If the code X âŠ†HN,q is cyclic then there exists a unique monic
polynomial g(X) âˆˆX such that:
(i) X = âŸ¨g(X)âŸ©;
(ii) g(X) has the minimum degree among all polynomials f(X) âˆˆX . Further-
more,
(a) g(X)|(XN âˆ’e),
(b) if degg(X) = d then dimX = N âˆ’d,
(c) X = { f(X): f(X) = g(X)h(X), h(X) âˆˆFq[X], degh(X) < N âˆ’d},
(d) if g(X) = g0 +g1X +g2X2 +Â·Â·Â·+gdXd, with gd = e, then g0 Ì¸= 0 and
G =
â›
âœ
âœ
â
g0
g1
g2
...
gd
0
0
...
0
0
g0
g1
...
gdâˆ’1
gd
0
...
0
...
...
0
0
0
...
g0
g1
...
gd
â
âŸ
âŸ
â 
is a generating matrix for X , with row i being the cyclic shift of row
iâˆ’1,i = 2,...,N âˆ’d.
Conversely,
for
any
polynomial
g(X)|(XN âˆ’e),
the
set
âŸ¨g(X)âŸ©=
{ f(X): f(X) = g(X)h(X),
h(X) âˆˆFq[X]/âŸ¨XN âˆ’eâŸ©}
is
an
ideal
in
Fq[X]/âŸ¨XN âˆ’eâŸ©, i.e. a cyclic code X , and the above properties (b)â€“(d) hold.
Proof
Take g(X) âˆˆF2[X] a non-zero polynomial of the least degree in X . Take
p(X) âˆˆX and write
p(X) = q(X)g(X)+r(X), with degr(X) < degg(X).
Then r(X) mod(XN âˆ’1) belongs to X . This contradicts the choice of g(X) unless
r(X) = 0. Therefore, g(X)|p(x) which proves (i). Taking p(X) = XN âˆ’1 proves (ii).
Finally, if g(X) and  g(X) both satisfy (i) and (ii) then g(X)| g(X) and  g(X)|g(X),
implying g(X) =  g(X).
Corollary 3.3.3
The cyclic codes of length N are in a one-to-one correspondence
with factors of XN âˆ’e. In other words, the map
*
cyclic codes of length N
+
â†’
*
divisors of XN âˆ’1
+
,
X
â†’
g(X),
is a bijection.

302
Further Topics from Coding Theory
With the identiï¬cation
F2[X]/âŸ¨(XN âˆ’1)âŸ©=
*
f âˆˆF2[X]: deg( f) < N
+
= FN
2
the cyclic codes become ideals in the polynomial ring F2[X]/âŸ¨(XN âˆ’1)âŸ©. They are
in a one-to-one correspondence with the ideals in F2[X] containing polynomial
XN âˆ’1. Because F2[X] is a Euclidean domain, all ideals in F2[X] are principal, i.e.
of the form { f(X)g(X) : f(X) âˆˆF2[X]}. In fact, all ideals in F2[X]/âŸ¨(XN âˆ’1)âŸ©are
also principal ideals.
Deï¬nition 3.3.4
The polynomial g(X) is called the minimal degree generator
(or simply the generator) of the cyclic code X . The ratio h(X) = (XN âˆ’e)/g(X),
of degree N âˆ’degg(X), is called the check polynomial for the cyclic code X =
âŸ¨g(X)âŸ©.
Example 3.3.5
X âˆ’e generates the parity-check code {x: âˆ‘
i
xi = 0} and e+X +
Â·Â·Â·+XNâˆ’1 the repetition code {a...a, a âˆˆFq}; X â‰¡e generates X = H .
Worked Example 3.3.6
(a) A cyclic code X = âŸ¨g(X)âŸ©of length N is called
reversible if c0 ...cNâˆ’1 âˆˆX implies cNâˆ’1 ...c0 âˆˆX . Prove that X is reversible
iff g(Î±) = 0 implies g(Î±âˆ’1) = 0.
(b) A cyclic code is called degenerate if, for some r|N, each codeword c âˆˆX is a
concatenation câ€²câ€² Â·Â·Â·câ€² of N/r copies of some string câ€² of length r. Prove that X
is degenerate iff its check polynomial h(X)|(Xr âˆ’1).
[Hint: Prove that the generating polynomial g(X) = a(X)

1 + Xr + X2r + Â·Â·Â· +
XNâˆ’r
. ]
Solution
(a) If the code X = âŸ¨g(X)âŸ©is reversible and g = g0 ...gNâˆ’k0...0
then XNâˆ’1g(Xâˆ’1) âˆ¼0...0gNâˆ’k ...g0 âˆˆX , i.e. XNâˆ’1g(Xâˆ’1) = g(X)q(X). Thus,
if g(Î±) = 0 then Î±Nâˆ’1g(Î±âˆ’1) = 0, i.e. g(Î±âˆ’1) = 0.
Conversely, g(Î±) = 0 implies g(Î±âˆ’1) = 0. Suppose that c(X) âˆˆX
then
g(X)|c(X). Moreover, XNâˆ’1c(Xâˆ’1) has all zeros of g(X) among its roots, and so
belongs to X . But XNâˆ’1c(Xâˆ’1) âˆ¼cNâˆ’1 ...c0, so X is reversible.
(b) The condition g = aâ€² ...aâ€² means g(X) = a(X)(e+Xr +X2r +Â·Â·Â·+XNâˆ’r). On
the other hand,
XN âˆ’e = (Xr âˆ’e)(XNâˆ’r +Â·Â·Â·+Xr +e) = h(X)g(X).
Thus, if X = âŸ¨g(X)âŸ©is degenerate then Xr âˆ’e = h(X)a(X), i.e. h(X)|(Xr âˆ’e).

3.3 Cyclic codes revisited. Decoding the BHC codes
303
Conversely, if h(X)|(Xr âˆ’e) then Xr âˆ’e = a(X)h(X) and
XN âˆ’e = (Xr âˆ’e)(XNâˆ’r +Â·Â·Â·+Xr +e)
= h(X)a(X)(XNâˆ’r +Â·Â·Â·+Xr +e).
Then
g(X) = a(X)(XNâˆ’r +Â·Â·Â·+Xr +e),
i.e. g = aâ€² ...aâ€². Furthermore, any c(X) âˆˆX is of the form c(X) = q(X)g(X) where
degq(X) â‰¤N âˆ’degg(X). Write
c(X) = q(X)g(X) = a(X)q(X)(XNâˆ’r +Â·Â·Â·+Xr +e);
we conclude that dega(X)q(X) < r (after multiplying by XNâˆ’r the degree cannot
exceed N âˆ’1). Then c = câ€² ...câ€² is the concatenation câ€² ...câ€² where câ€² âˆ¼a(X)q(X).
Worked Example 3.3.7
Show that Hammingâ€™s [7,4] code is a cyclic code with
check polynomial X4 +X2 +X +1. What is its generator polynomial? Does Ham-
mingâ€™s original code contain a subcode equivalent to its dual?
Solution In F7
2 we have
X7 âˆ’1 = (X3 +X +1)(X4 +X2 +X +1).
The cyclic code with generator g(X) = X3 + X + 1 has check polynomial h(X) =
X4 +X2 +X +1. The parity-check matrix of the code is
â›
â
1
0
1
1
1
0
0
0
1
0
1
1
1
0
0
0
1
0
1
1
1
â
â .
The columns of this matrix are the non-zero elements of F3
2. So, it is equivalent to
Hammingâ€™s [7,4] code.
The dual of Hammingâ€™s [7,4] code has generator polynomial X4 + X3 + X2 + 1
(the reverse of h(X)). Since X4 + X3 + X2 + 1 = (X + 1)g(X), it is a subcode of
Hammingâ€™s [7,4] code.
Worked Example 3.3.8
Let Ï‰ be a primitive Nth root of unity. Let X = âŸ¨g(X)âŸ©
be a cyclic code of length N. Show that the dimension dim (X ) equals the number
of powers Ï‰ j such that g(Ï‰ j) Ì¸= 0.
Solution Denote E(N) = {Ï‰,Ï‰2,...,Ï‰N = e}, dimâŸ¨g(X)âŸ©= N âˆ’d, d = degg(X).
But g(X) =
âˆ
1â‰¤jâ‰¤d
(X âˆ’Ï‰i j) where Ï‰i1,...,Ï‰id are the zeros of âŸ¨g(X)âŸ©. Hence, the
remaining N âˆ’d roots of unity Ï‰l satisfy the condition g(Ï‰l) Ì¸= 0.

304
Further Topics from Coding Theory
It is important to note that the generator polynomial of a cyclic code X = âŸ¨g(X)âŸ©
is not unique. In particular, there exists a unique polynomial i(X) âˆˆX such that
i(X)2 = i(X) and X = âŸ¨i(X)âŸ©(an idempotent generator).
Theorem 3.3.9
If X1 = âŸ¨g1(X)âŸ©and X2 = âŸ¨g2(X)âŸ©are cyclic codes with gener-
ators g1(X) and g2(X) then
(a) X1 âŠ‚X2 iff g2(X)|g1(X),
(b) X1 âˆ©X2 = âŸ¨lcm(g1(X),g2(X))âŸ©,
(c) X1|X2 = âŸ¨gcd(g1(X),g2(X))âŸ©.
Theorem 3.3.10
Let h(X) be the check polynomial for X . Then
(a) X = { f(X): f(X)h(X) = 0 mod(XN âˆ’e)},
(b) if h(X) = h0 +h1X +Â·Â·Â·+hNâˆ’rXNâˆ’r then the parity-check matrix H of X is
H =
â›
âœ
âœ
â
hNâˆ’r
hNâˆ’râˆ’1
...
h1
h0
0
0
...
0
0
hNâˆ’r
...
...
h1
h0
0
...
0
...
...
...
...
...
...
...
...
...
0
0
...
hNâˆ’r
hNâˆ’râˆ’1
...
...
...
h0
â
âŸ
âŸ
â ,
(c) the dual code X âŠ¥is a cyclic code of dimX âŠ¥= r, and X âŠ¥= âŸ¨gâŠ¥(X)âŸ©, where
gâŠ¥(X) = hâˆ’1
0 XNâˆ’rh(Xâˆ’1) = hâˆ’1
0 (h0XNâˆ’r +h1XNâˆ’râˆ’1 +Â·Â·Â·+hNâˆ’r).
The generator g(X) of a cyclic code is speciï¬ed, in terms of factorisation of
XN âˆ’e, as a â€˜sub-productâ€™,
XN âˆ’e = lcm

MÏ‰(X) : Ï‰ âˆˆE(N)
,
(3.3.2)
of some minimal polynomials MÏ‰(X). A convenient way is to characterise a cyclic
code via roots of g(X). If Ï‰ is a root of MÏ‰(X) in an extension ï¬eld Fq(Ï‰) then
MÏ‰(X) is the minimal polynomial for Ï‰ over Fq. For any polynomial f(X) âˆˆFq[X]
we have f(Ï‰) = 0 iff f(X) = a(X)MÏ‰(X), and if in addition f(X) âˆˆFq[X]/âŸ¨XN âˆ’
eâŸ©then f(Ï‰) = 0 iff f(X) âˆˆâŸ¨MÏ‰(X)âŸ©. Hence we get
Theorem 3.3.11
Let g(X) = q1(X)...qt(X) be a product of irreducible factors
of XN âˆ’e, and Ï‰1,...,Ï‰u be the roots of g(X) in Spl(XN âˆ’e) over Fq. Then
âŸ¨g(X)âŸ©= { f(X) âˆˆFq[X]/âŸ¨XN âˆ’eâŸ©: f(Ï‰1) = Â·Â·Â· = f(Ï‰u) = 0}.
(3.3.3)
Furthermore, it is enough to pick up a single root of each irreducible factor: if
Ï‰â€²
j is any root of MÏ‰(X), 1 â‰¤j â‰¤t, then
âŸ¨g(X)âŸ©= { f(X) âˆˆFq[X]/âŸ¨XN âˆ’eâŸ©: f(Ï‰â€²
1) = Â·Â·Â· = f(Ï‰â€²
t) = 0}.
(3.3.4)

3.3 Cyclic codes revisited. Decoding the BHC codes
305
Conversely, if Ï‰1,...,Ï‰u is a set of roots of XN âˆ’e then the code { f(X) âˆˆ
Fq[X]/âŸ¨XN âˆ’eâŸ©: f(Ï‰1) = Â·Â·Â· = f(Ï‰u) = 0} has a generator which is the lcm
of the minimal polynomials for Ï‰1,...,Ï‰u.
Deï¬nition 3.3.12
The roots of generator g(X) are called the zeros of the cyclic
code âŸ¨g(X)âŸ©. Other roots of unity are often called non-zeros of the code.
Let {Ï‰1,...,Ï‰u} be a set of roots of XN âˆ’e lying in an extension ï¬eld Fql. Recall
that l is the minimal integer such that N|ql âˆ’1. If f(X) = âˆ‘fiXi is a polynomial
in Fq[X]/âŸ¨XN âˆ’eâŸ©then f(Ï‰ j) = 0 iff
âˆ‘
0â‰¤iâ‰¤u
fiÏ‰i
j = 0. Representing Fql as a vector
space over Fq of dimension l, we associate Ï‰i
j with a (column) vector âˆ’â†’
Ï‰ i
j of length
l over Fq, writing the last equality as âˆ‘
i
fiâˆ’â†’
Ï‰ i
j =
âˆ’âˆ’âˆ’â†’
âˆ‘
i
fiÏ‰i
j = 0. So, the (ul)Ã—N matrix
 HT =
â›
âœ
âœ
âœ
â
âˆ’â†’
Ï‰ 0
1
âˆ’â†’
Ï‰ 1
1
...
âˆ’â†’
Ï‰ Nâˆ’1
1
âˆ’â†’
Ï‰ 0
2
âˆ’â†’
Ï‰ 1
2
...
âˆ’â†’
Ï‰ Nâˆ’1
2
...
...
...
...
âˆ’â†’
Ï‰ 0
u
âˆ’â†’
Ï‰ 1
u
...
âˆ’â†’
Ï‰ Nâˆ’1
u
â
âŸ
âŸ
âŸ
â 
(3.3.5)
can be considered as a parity-check matrix for the code with zeros Ï‰1,...,Ï‰u (with
the proviso that its rows may not be linearly independent).
Theorem 3.3.13
For q = 2, the Hamming [2l âˆ’1,2l âˆ’l âˆ’1,3] code is equivalent
to a cyclic code âŸ¨MÏ‰(X)âŸ©= âˆ0â‰¤iâ‰¤lâˆ’1(X âˆ’Ï‰2i) where Ï‰ is a primitive element in
F2l.
Proof
Let Ï‰ be a primitive (N,F2) root of unity where N = 2l âˆ’1. The splitting
ï¬eld Spl (XN âˆ’e) is F2l (as ordN(2) = l). So, Ï‰ is a primitive element in F2l.
Take MÏ‰(X) = (X âˆ’Ï‰)(X âˆ’Ï‰2)Â·Â·Â·

X âˆ’Ï‰2lâˆ’1
, of degree l. The powers Ï‰0 =
e,Ï‰,...,Ï‰Nâˆ’1 form Fâˆ—
2l, the list of the non-zero elements and the columns of the
l Ã—N matrix
H =
âˆ’â†’
Ï‰ 0,âˆ’â†’
Ï‰ ,...,âˆ’â†’
Ï‰ Nâˆ’1
(3.3.6)
consist of all non-zero binary vectors of length l. Hence, the Hamming [2l âˆ’1,2l âˆ’
l âˆ’1,3] code is (equivalent to) the cyclic code âŸ¨MÏ‰(X)âŸ©whose zeros consist of a
primitive (2l âˆ’1;F2) root of unity Ï‰ and (necessarily) all the other roots of the
minimal polynomial for Ï‰.
Theorem
3.3.14
If
gcd(l,q âˆ’1) = 1
then
the
q-ary
Hamming
ql âˆ’1
qâˆ’1 , ql âˆ’1
qâˆ’1 âˆ’l,3

code is equivalent to the cyclic code.

306
Further Topics from Coding Theory
Proof
Write Spl(XN âˆ’e) = Fql where l = ordN(q), N = qlâˆ’1
qâˆ’1 . To justify the
selection of l observe that ql âˆ’1
N
= q âˆ’1 and l is the least positive integer with
this property as qlâˆ’1
qâˆ’1 > qlâˆ’1 âˆ’1.
Therefore, Spl(XN âˆ’e) = Fql. Take a primitive Î² âˆˆFql. Then Ï‰ = Î² (qlâˆ’1)/N =
Î² qâˆ’1 is a primitive (N,Fq) root of unity. As before, take the minimal polynomial
MÏ‰(X) = (X âˆ’Ï‰)(X âˆ’Ï‰q)Â·Â·Â·

X âˆ’Ï‰qlâˆ’1
and consider the cyclic code âŸ¨MÏ‰(X)âŸ©
with the zero Ï‰ (and necessarily Ï‰q,...,Ï‰qlâˆ’1). Consider again the l Ã— N matrix
(3.3.6). We want to check that any two distinct columns of H are linearly indepen-
dent. If not, there exist i < j such that Ï‰i and Ï‰ j are scalar multiples of the element
Ï‰ jâˆ’i âˆˆFq. But then (Ï‰ jâˆ’i)qâˆ’1 = Ï‰( jâˆ’i)(qâˆ’1) = e in Fq; as Ï‰ is a primitive Nth root
of unity, this holds iff ( j âˆ’i)(qâˆ’1) â‰¡0 mod N. Write
N = ql âˆ’1
qâˆ’1 = 1+Â·Â·Â·+qlâˆ’1.
As (q âˆ’1)|(qr âˆ’1) for all r â‰¥1, we have qr = (q âˆ’1)vr + 1 for some natural vr.
Summing over 0 â‰¤r â‰¤sâˆ’1 yields
N = (qâˆ’1)âˆ‘
r
vr +l.
(3.3.7)
As gcd(q âˆ’1,l) = 1 we have gcd(q âˆ’1,N) = 1. But then the equality ( j âˆ’i)
(qâˆ’1) = 0 modN is impossible.
So, the code with the parity-check matrix H has length N, rank k â‰¥N âˆ’l and
distance d â‰¥3. But the Hamming bound says that
qk â‰¤qN

âˆ‘
0â‰¤mâ‰¤E
 N
m

(qâˆ’1)m
âˆ’1
, E = âŒŠd âˆ’1
2
âŒ‹.
As the volume of the ball vN,q(E) â‰¥ql, this implies that in fact k = N âˆ’l,E = 1
and d = 3. So, this code is equivalent to a Hamming code.
Next, we look in more detail on BCH codes correcting several errors. Recall that
if Ï‰1,...,Ï‰u âˆˆE(N,q) are (N,Fq) roots of unity then
XN = { f(X) âˆˆFq[X]/âŸ¨XN âˆ’eâŸ©: f(Ï‰1) = Â·Â·Â· = f(Ï‰u) = 0}
is a cyclic code âŸ¨g(X)âŸ©where
g(X) = lcm

MÏ‰1,Fq(X),...,MÏ‰u,Fq(X)(X)

(3.3.8)
is the product of distinct minimal polynomials for Ï‰1,...,Ï‰u over Fq. In particular,
if q = 2, N = 2l âˆ’1, and Ï‰ is a primitive element in F2l then the cyclic code with
roots Ï‰, Ï‰2,...,Ï‰2lâˆ’1 (which is the same as with a single root Ï‰) coincides with

3.3 Cyclic codes revisited. Decoding the BHC codes
307
âŸ¨MÏ‰(X)âŸ©and is equivalent to the Hamming code. We could try other possibilities
for zeros of X to see if it leads to interesting examples. This is the way to discover
the BCH codes [25], [70].
Recall the factorisation into minimal polynomials Mi(X)(= MÏ‰i,Fq(X)),
XN âˆ’1 = lcm

Mi(X): i = 0,...,t

,
(3.3.9)
where Ï‰ is a primitive (N,Fq) root of unity. The roots of Mi(X) are conjugate,
i.e. have the form Ï‰i,Ï‰iq,...,Ï‰iqdâˆ’1 where d(= d(i)) is the least integer â‰¥1 such
that iqd = i modN. The set Ci = {i,iq,...,iqdâˆ’1} is the ith cyclotomic coset of q
mod N. So,
Mi(X) = âˆ
jâˆˆCi
(X âˆ’Ï‰ j).
(3.3.10)
In Section 3.2, we obtained a cyclic code of minimal distance â‰¥Î´ by requiring
that the generator g(X) has (Î´ âˆ’1) successive roots (with successive exponents).
Compare Theorem 3.3.16 below.
Example 3.3.15
A binary Hamming code is a binary primitive narrow sense
BCH of designed distance Î´ = 3.
By Lemma 3.2.8, the distance d

X BCH
q,N,Î´

â‰¥Î´. As Spl(XN âˆ’e) = Fqs where s =
ordN(q), we have that
degMÏ‰b+ j(X) â‰¤s.
(3.3.11)
Hence, the rank (X BCH
q,N,Î´) = N âˆ’deg(g(X)) â‰¥N âˆ’(Î´ âˆ’1)s. So:
Theorem 3.3.16
The q-ary BCH code X BCH
q,N,Î´ has distance â‰¥Î´ and rank â‰¥
N âˆ’(Î´ âˆ’1)ordN(q).
As before, we can form a parity-check matrix for X BCH by writing
Ï‰b,Ï‰b+1,...,Ï‰b+Î´âˆ’2 and their powers as vectors from Fs
q where s = ordN(q). Set
 HT =
â›
âœ
âœ
âœ
âœ
â
âˆ’â†’e
âˆ’â†’e
...
âˆ’â†’e
âˆ’â†’
Ï‰ b
âˆ’â†’
Ï‰ b+1
...
âˆ’â†’
Ï‰ b+Î´âˆ’2
âˆ’â†’
Ï‰ 2b
âˆ’â†’
Ï‰ 2(b+1)
...
âˆ’â†’
Ï‰ 2(b+Î´âˆ’2)
...
âˆ’â†’
Ï‰ (Nâˆ’1)b
âˆ’â†’
Ï‰ (Nâˆ’1)(b+1)
...
âˆ’â†’
Ï‰ (Nâˆ’1)(b+Î´âˆ’2)
â
âŸ
âŸ
âŸ
âŸ
â 
.
(3.3.12)
The â€˜properâ€™ parity-check matrix H is obtained by removing redundant rows.
The binary BCH codes are simplest to deal with. Let Ci = {i,2i,...,i2dâˆ’1} be
the ith cyclotomic coset (with d(= d(i)) being the smallest non-zero integer such
that iÂ·2d = i modN). Then u âˆˆCi iff 2u modN âˆˆCi. So, Mi(X) = M2i(X), and for
all s â‰¥1 the polynomials
g2sâˆ’1(X) = g2s(X) = lcm{M1(X),M2(X),...,M2s(X)}.

308
Further Topics from Coding Theory
We immediately deduce that X BCH
2,N,2s+1 = X BCH
2,N,2s. So we can focus on the narrow
sense BCH codes with odd designed distance Î´ = 2E +1, and obtain an improve-
ment of Theorem 3.3.16:
Theorem 3.3.17
The rank of a binary BCH code X BCH
2,N,2E+1 is â‰¥N âˆ’E ordN(2).
The problem of determining exactly the minimum distance of a BCH code has
been solved only partially (although a number of results exist in the literature). We
present the following theorem without proof.
Theorem 3.3.18
The minimum distance of a binary primitive narrow sense BCH
code is an odd number.
The previous results can be sharpened in a number of particular cases.
Worked Example 3.3.19
Prove that log2(N +1) > 1+log2(E +1)! implies
(N +1)E <
âˆ‘
0â‰¤iâ‰¤E+1
N
i

.
(3.3.13)
Solution For i â‰¤E +1 we obtain i! â‰¤(E +1)! < (N+1)/2. Hence, (3.3.13) follows
from
(N +1)E+1 â‰¤2
âˆ‘
0â‰¤iâ‰¤E+1
N(N âˆ’1)...(N âˆ’i+1) = S(E).
(3.3.14)
Inequality (3.3.14) holds for E = 0, and is checked by induction in E. Write the
RHS of (3.3.14) as S(E + 1) = S(E) + N(N âˆ’1)...(N âˆ’E). Then S(E) > (N +
1)E+1 by the induction hypothesis and it remains to check
N(N+1)E+1 < 2N(Nâˆ’1)...(Nâˆ’E)(Nâˆ’E âˆ’1), for N+1 > 2(E +2)!. (3.3.15)
Consider the polynomial (y + 1)E+1 âˆ’2(y âˆ’1)...(y âˆ’E)(y âˆ’E âˆ’1) and group
together the monomials of degrees E + 1 and E. Clearly, they are negative for
y > 2(E +2)!. Continue this procedure, concluding that (3.3.13) holds.
Theorem 3.3.20
Let N = 2s âˆ’1. If 2sE <
âˆ‘
0â‰¤iâ‰¤E+1
N
i

then a primitive binary
narrow sense BCH code X BCH
2,2sâˆ’1,2E+1 has distance 2E +1.
Proof
By Theorem 3.3.18, the distance is odd. So, d(X BCH
2,2sâˆ’1,2E+1) Ì¸= 2E + 2.
Suppose the distance is â‰¥2E + 3. Observe that the rank X BCH
2,2sâˆ’1,2E+1 â‰¥N âˆ’sE,
and use the Hamming bound
2Nâˆ’sE
âˆ‘
0â‰¤iâ‰¤E+1
N
i

â‰¤2N, i.e. 2sE â‰¥
âˆ‘
0â‰¤iâ‰¤E+1
N
i

.
The contradiction implies d(X BCH
2,2sâˆ’1,2E+1) = 2E +1.

3.3 Cyclic codes revisited. Decoding the BHC codes
309
Corollary 3.3.21
If N = 2s âˆ’1 and s > 1+log2(E +1)! then d(X BCH
2,2sâˆ’1,2E+1) =
2E +1. In particular, let N = 31 and s = 5. Then we easily verify that
25E <
âˆ‘
0â‰¤iâ‰¤E+1
31
i

for E = 1,2 and 3. This proves that the actual distance of X BCH
2,31,d in fact equals Î´
for Î´ = 3,5,7.
Proof
s > 1+log2(E +1)! implies that 2sE <
âˆ‘
0â‰¤iâ‰¤E+1
N
i

.
Theorem 3.3.22
If Î´|N, the minimum distance of primitive binary narrow sense
BCH code of designed distance Î´ equals Î´.
Proof
Set N = Î´m, then
XN âˆ’1 = XÎ´m âˆ’1 = (Xm âˆ’1)(1+Xm +Â·Â·Â·+X(Î´âˆ’1)m).
As Ï‰ jm Ì¸= 1 for j = 1,...,Î´ âˆ’1, none of Ï‰,Ï‰2,...,Ï‰Î´âˆ’1 is a root of Xm âˆ’1.
So, they must be roots of 1 + Xm + Â·Â·Â· + X(Î´âˆ’1)m. Then this polynomial gives a
codeword, of weight Î´. So, Î´ is the distance.
Two more results on the minimal distance of a BCH code are presented in The-
orems 3.3.23 and 3.3.25. The full proofs are beyond the scope of this book and
omitted.
Theorem 3.3.23
Let N = qs âˆ’1. The minimal distance of a primitive q-ary nar-
row sense BCH code X BCH
q,qsâˆ’1,qkâˆ’1,Ï‰,1 of designed distance qk âˆ’1 equals qk âˆ’1.
Theorem 3.3.24
The minimal distance of a primitive q-ary narrow sense BCH
code X BCH = X BCH
q,qsâˆ’1,Î´,Ï‰,1 of designed distance Î´ is at most qÎ´ âˆ’1.
Proof
Take k to be an integer â‰¥1 with qkâˆ’1 â‰¤Î´ â‰¤qk âˆ’1. Set Î´ â€² = qk âˆ’1 and
consider X â€² (= X BCH
q,qsâˆ’1,Î´ â€²,Ï‰,1), the q-ary primitive narrow sense BCH code of
the same length N = qs âˆ’1 and designed distance Î´ â€². The roots of the generator
of X are among those of X â€², so X â€² âŠ†X . But according to Theorem 3.3.22,
d(X â€²) = Î´ â€² which is â‰¤Î´qâˆ’1.
The following result shows that BCH codes are not â€˜asymptotically goodâ€™. How-
ever, for small N (a few thousand or less), the BCH are among the best codes
known.

310
Further Topics from Coding Theory
Theorem 3.3.25
There exists no inï¬nite sequence of q-ary primitive BCH
codes X BCH
N
of length N such that d(XN)/N and rank(XN)/N are bounded away
from 0.
Decoding BCH codes can be done by using the so-called Berlekampâ€“Massey
algorithm. To begin with, consider a binary primitive narrow sense BCH code
X BCH (= X BCH
2,N,5) of length N = 2s âˆ’1 and designed distance 5. With E = 2 and
s â‰¥4, inequality 2sE <
âˆ‘
0â‰¤iâ‰¤E+1
N
i

holds, and by Theorem 3.3.20, the distance
d

X BCH
equals 5. Thus, the code is two-error correcting. Also, by Theorem
3.3.17, the rank of X BCH is â‰¥N âˆ’2s. [For s = 4, the rank is actually equal to
N âˆ’2s = 15âˆ’8 = 7.] So, X BCH is [2s âˆ’1,â‰¥2s âˆ’1âˆ’2s,5].
The deï¬ning zeros are Ï‰, Ï‰2, Ï‰3, Ï‰4 where Ï‰ is a primitive Nth root of unity
over F2 (which is also a primitive element Ï‰ of F2s). We know that Ï‰ and Ï‰3
sufï¬ce as deï¬ning zeros: X BCH = {c(X) âˆˆF2[X]/âŸ¨XN âˆ’1âŸ©: c(Ï‰) = c(Ï‰3) = 0}.
So, the parity-check matrix  H in (3.3.12) can be taken in the form
 HT =
 âˆ’â†’e
âˆ’â†’
Ï‰
âˆ’â†’
Ï‰ 2
Â·Â·Â·
âˆ’â†’
Ï‰ Nâˆ’1
âˆ’â†’e
âˆ’â†’
Ï‰ 3
âˆ’â†’
Ï‰ 6
Â·Â·Â·
âˆ’â†’
Ï‰ 3(Nâˆ’1)

.
(3.3.16)
It is instructive to compare the situation with the binary Hamming [2l âˆ’1,2l âˆ’
1âˆ’l] code X (H). In the case of code X BCH, suppose again that a codeword c(X) âˆˆ
X was sent and the received word r(X) has â‰¤2 errors. Write r(X) = c(X)+e(X)
where the error polynomial e(X) now has weight â‰¤2. There are three cases to
consider: e(X) = 0, e(X) = Xi or e(X) = Xi +X j, 0 â‰¤i Ì¸= j â‰¤N âˆ’1. If r(Ï‰) = r1
and r(Ï‰3) = r3 then e(Ï‰) = r1 and e(Ï‰3) = r3. In the case of no error (e(X) = 0),
r1 = r3 = 0, and vice versa. In the single-error case (e(X) = Xi),
r3 = e(Ï‰3) = Ï‰3i = (Ï‰i)3 = (e(Ï‰))3 = r3
1 Ì¸= 0.
Conversely, if r3 = r3
1 Ì¸= 0 then e(Ï‰3) = e(Ï‰)3. If e(X) = Xi +X j with i Ì¸= j then
Ï‰3i +Ï‰3 j = (Ï‰i +Ï‰ j)3 = Ï‰3i +Ï‰2iÏ‰ j +Ï‰iÏ‰2 j +Ï‰3 j,
i.e. Ï‰2iÏ‰ j +Ï‰iÏ‰2 j = 0 or Ï‰i +Ï‰ j = 0 which implies i = j, a contradiction. So, the
single error occurs iff r3 = r3
1 Ì¸= 0, and the wrong digit is i such that r1 = Ï‰i. So,
in the single-error case we identify a column of  H, i.e. a pair (Ï‰i, Ï‰3i) = (r1,r3)
and change digit i in r(X). This is completely similar to the decoding procedure for
Hamming codes.
In the two-error case (e(X) = Xi+X j,i Ì¸= j), in the spirit of the Hamming codes,
we try to ï¬nd a pair of columns (Ï‰i,Ï‰3i) and (Ï‰ j,Ï‰3 j) such that the sum (Ï‰i +
Ï‰ j,Ï‰3i +Ï‰3 j) = (r1,r3), i.e. solve the equation
r1 = Ï‰i +Ï‰ j, r3 = Ï‰3i +Ï‰3 j.

3.3 Cyclic codes revisited. Decoding the BHC codes
311
Then ï¬nd i, j such that y1 = Ï‰i, y2 = Ï‰ j (y1,y2 are called error locators). If such i,
j (or equivalently, error locators y1, y2) are found, we know that errors occurred at
positions i and j.
It is convenient to introduce an error-locator polynomial Ïƒ(X) whose roots are
yâˆ’1
1 , yâˆ’1
2 :
Ïƒ(X) = (1âˆ’y1X)(1âˆ’y2X) = 1âˆ’(y1 +y2)X +y1y2X2
= 1âˆ’r1X +(r3râˆ’1
1 âˆ’r2
1)X2.
(3.3.17)
As y1 +y2 = r1, we check that y1y2 = r3râˆ’1
1 âˆ’r2
1. Indeed,
r3 = y3
1 +y3
2 = (y1 +y2)(y2
1 +y1y2 +y2
2) = r1(r2
1 +y1y2).
If N is not large, the roots of Ïƒ(X) can be found by trying all 2s âˆ’1 non-zero
elements of Fâˆ—
2s. (The standard formula for the roots of a quadratic polynomial
does not apply over F2.) Thus, the following assertion arises:
Theorem 3.3.26
For N = 2l âˆ’1, consider a two-error correcting binary primi-
tive narrow sense BCH code X (which equals X BCH) of length N and designed
distance 5, with the parity-check matrix produced from
 HT =
 e
Ï‰
Ï‰2
Â·Â·Â·
Ï‰Nâˆ’1
e
Ï‰3
Ï‰6
Â·Â·Â·
Ï‰3(Nâˆ’1)

,
where Ï‰ is the primitive element of F2s. [The rank of the code is â‰¥N âˆ’2l and
for l â‰¥4 the distance equals 5, i.e. X is [2l âˆ’1,â‰¥2l âˆ’1âˆ’2l,5] and corrects two
errors.] Assume that at most two errors occurred in a received word r(X) and let
r(Ï‰) = r1, r(Ï‰3) = r3. Then:
(a) if r1 = 0 then r3 = 0 and no error occurred;
(b) if r3 = r3
1 Ì¸= 0 then a single error occurred at position i where r1 = Ï‰i;
(c) if r1 Ì¸= 0 and r3 Ì¸= r3
1 then two errors occurred: the error locator polynomial
Ïƒ(X) = 1 âˆ’r1X + (r3râˆ’1
1 âˆ’r2
1)X2 has two distinct roots Ï‰Nâˆ’1âˆ’i,Ï‰Nâˆ’1âˆ’j and
the errors occurred at positions i and j.
For a binary BCH code with a general designed distance Î´ (Î´ = 2t +1 is assumed
odd), we follow the same idea: compute
r1 = e(Ï‰),r3 = e(Ï‰3),...,rÎ´âˆ’2 = e(Ï‰Î´âˆ’2)
for the received word r(X) = c(X) + e(X). Suppose that errors occurred at places
i1,...,it. Then
e(X) = âˆ‘
1â‰¤jâ‰¤t
Xi j.

312
Further Topics from Coding Theory
As before, consider the system
âˆ‘
1â‰¤jâ‰¤t
Ï‰i j = r1, âˆ‘
1â‰¤jâ‰¤t
Ï‰3i j = r3,..., âˆ‘
1â‰¤jâ‰¤t
Ï‰(Î´âˆ’2)ij = rÎ´âˆ’2,
and introduce the error locators yj = Ï‰i j:
âˆ‘
1â‰¤jâ‰¤t
yj = r1, âˆ‘
1â‰¤jâ‰¤t
y3
j = r3,..., âˆ‘
1â‰¤jâ‰¤t
yÎ´âˆ’2
j
= rÎ´âˆ’2.
The error locator polynomial
Ïƒ(X) = âˆ
1â‰¤jâ‰¤t
(1âˆ’yjX)
has the roots yâˆ’1
j . The coefï¬cients Ïƒi in Ïƒ(X) =
âˆ‘
0â‰¤iâ‰¤t
ÏƒiXi can be determined from
the equations below
â›
âœ
âœ
âœ
âœ
âœ
âœ
âœ
âœ
â
1
0
0
0
0
...
0
r2
r1
1
0
0
...
0
r4
r3
r2
r1
1
...
0
...
...
...
...
...
...
...
r2tâˆ’4
r2tâˆ’5
...
...
...
...
rtâˆ’3
r2tâˆ’2
r2tâˆ’3
...
...
...
...
rtâˆ’1
â
âŸ
âŸ
âŸ
âŸ
âŸ
âŸ
âŸ
âŸ
â 
â›
âœ
âœ
âœ
âœ
âœ
âœ
âœ
â
Ïƒ1
Ïƒ2
Ïƒ3
...
Ïƒ2tâˆ’3
Ïƒ2tâˆ’1
â
âŸ
âŸ
âŸ
âŸ
âŸ
âŸ
âŸ
â 
=
â›
âœ
âœ
âœ
âœ
âœ
âœ
âœ
â
r1
r3
r5
...
r2tâˆ’3
r2tâˆ’1
â
âŸ
âŸ
âŸ
âŸ
âŸ
âŸ
âŸ
â 
This requires computing rk only for k odd as
r2 j = e(Ï‰2 j) = e(Ï‰ j)2 = r2
j.
Once the Ïƒi are found, the roots yâˆ’1
j
can be determined by trial and error.
Example 3.3.27
Consider X BCH
2,16,Ï‰,5 where Ï‰ is a primitive element of Fâˆ—
16. We
know that the primitive polynomial is M1(X) = X4+X +1 and M3(X) = X4+X3+
X2 +X +1. Hence, the generator of the code
g(X) = M1(X)M3(X) = X8 +X7 +X6 +X4 +1.

3.4 The MacWilliams identity and the linear programming bound
313
Let us introduce two errors in the codeword c = 10001011100000000 at the 4th
and 12th positions by taking a(X) = X12 +X8 +X7 +X6 +1. Then
r1 = a(Ï‰) = Ï‰12 +Ï‰8 +Ï‰7 +Ï‰6 +1 = Ï‰6,
r3 = a(Ï‰3) = Ï‰36 +Ï‰24 +Ï‰21 +Ï‰18 +1 = Ï‰9 +Ï‰3 +1 = Ï‰4.
Since r3 Ì¸= r3
1, consider the location polynomial
Ïƒ(X) = 1+Ï‰6X +(Ï‰13 +Ï‰12)X2.
The roots of l(X) are Ï‰3 and Ï‰11 by the direct check. Hence we discover the errors
at the 4th and 12th positions.
3.4 The MacWilliams identity and the linear programming bound
The MacWilliams identity for linear codes deals with the so-called weight-
enumerator polynomials WX (z) and WX âŠ¥(z) where X and X âŠ¥are a pair of dual
codes of a given length N. The polynomials WX (z) and WX âŠ¥(z) are deï¬ned by
WX (z) = âˆ‘
0â‰¤kâ‰¤N
Akzk and WX âŠ¥(z) = âˆ‘
0â‰¤kâ‰¤N
AâŠ¥
k zk
(3.4.1)
where Ak(= Ak(X )) equals the number of codewords of weight k in X , and AâŠ¥
k
(= Ak(X âŠ¥)) the number in X âŠ¥. The identity for q-ary codes reads
WX âŠ¥(z) =
1
â™¯X
	
1+(qâˆ’1)z

NWX

1âˆ’z
1+(qâˆ’1)z

, z âˆˆC,
(3.4.2)
and takes a particularly elegant form in the binary case (q = 2):
WX âŠ¥(z) =
1
â™¯X (1+z)nWX
1âˆ’z
1+z

.
(3.4.3)
A short derivation of the abstract MacWilliams identity is rather algebraic. It
may be skipped at the ï¬rst reading as only its speciï¬cation for linear codes will be
used later on.
Deï¬nition 3.4.1
Let (G,+) be a group. A homomorphism Ï‡ : G to the multiplica-
tive group of complex numbers Sâ€² = {z âˆˆC : |z| = 1} is called a (one-dimensional)
character of G. Since Ï‡ is a homomorphism
Ï‡(g1 +g2) = Ï‡(g1)Ï‡(g2),Ï‡(0) = 1.
(3.4.4)
We say Ï‡ is trivial (or principal) if Ï‡(Â·) â‰¡1.

314
Further Topics from Coding Theory
More generally, a linear representation D of a group G over a ï¬eld F (not nec-
essarily ï¬nite) is deï¬ned as a homomorphism
D : G â†’GL(V) : g â†’D(g)
(3.4.5)
from G into the group GL(V) of invertible linear mappings of a ï¬nite-dimensional
space V over F. The vector space V is called the representation space and its di-
mension dim(V) is called the dimension of representation.
Let D be a representation of a group G. Then the map
Ï‡D : G â†’F : g â†’âˆ‘dii(g) = trace

D(g)

,
(3.4.6)
which takes g âˆˆG to Ï‡D(g), the trace of D(g) = (dij(g)), is called the character of
D. Representations and characters over the ï¬eld C of complex numbers are called
ordinary. In the situation where the underlying ï¬eld F is ï¬nite, they are called
modular.
In our case G = Fq with additive group operation. Fix a primitive qth root of
unity Ï‰ = e2Ï€i/q âˆˆSâ€² and for any j âˆˆFq deï¬ne a one-dimensional representation
of the group Fq as follows:
Ï‡( j) : Fq â†’Sâ€² : u â†’Ï‰ ju.
The character Ï‡( j) is non-trivial for j Ì¸= 0. In fact, all characters of Fq can be de-
scribed in this way, but we omit the proof of this assertion.
Next, we deï¬ne a character of the group Gâ€² = FN
q . Fix a non-trivial one-
dimensional ordinary character Ï‡ : Fq â†’Sâ€² and a non-zero element v âˆˆFN
q and
deï¬ne a character of the additive group Gâ€² = FN
q as follows:
Ï‡(v) : FN
q â†’Sâ€² : u â†’Ï‡(vÂ·u),
(3.4.7)
where vÂ·u, as before, is the dot-product.
Lemma 3.4.2
Let Ï‡ be a non-trivial (i.e. Ï‡ Ì¸â‰¡1) character of a ï¬nite group G.
Then
âˆ‘
gâˆˆG
Ï‡(g) = 0.
(3.4.8)
If Ï‡ is trivial then âˆ‘
gâˆˆG
Ï‡(g) = â™¯G.
Proof
Since Ï‡ is non-trivial, there exists an element h âˆˆG such that Ï‡(h) Ì¸= 1.
From
Ï‡(h) âˆ‘
gâˆˆG
Ï‡(g) = âˆ‘
gâˆˆG
Ï‡(hg) = âˆ‘
gâˆˆG
Ï‡(g),
we obtain that (Ï‡(h)âˆ’1) âˆ‘
gâˆˆG
Ï‡(g) = 0. Therefore, âˆ‘
gâˆˆG
Ï‡(g) = 0.

3.4 The MacWilliams identity and the linear programming bound
315
In the case G = FN
q , âˆ‘
xâˆˆFNq
Ï‡(x) = qN for a trivial.
Deï¬nition 3.4.3
The discrete Fourier transform (in short, DFT) of a function f
on FN
q is deï¬ned by
f = âˆ‘
vâˆˆFNq
f(v)Ï‡(v).
(3.4.9)
Sometimes, the weight enumerator polynomial of code X is deï¬ned as a func-
tion of two formal variables x,y:
WX (x,y) = âˆ‘
vâˆˆX
xw(v)yNâˆ’w(v)
(3.4.10)
(if one sets x = z,y = 1, (3.4.10) coincides with (3.4.1)). So, we want to apply the
DFT to the function (no harm to say that x,y âˆˆSâ€²)
g : FN
q â†’C [x,y] : v â†’xw(v)yNâˆ’w(v).
(3.4.11)
Lemma 3.4.4
(The abstract MacWilliams identity) For v âˆˆFN
q let
g : FN
q â†’C [x,y] : v â†’xw(v)yNâˆ’w(v).
(3.4.12)
Then
g(u) = (yâˆ’x)w(u)(y+(qâˆ’1)x)Nâˆ’w(u).
(3.4.13)
Proof
Let Ï‡ denote a non-trivial ordinary character of the additive group G = Fq.
Given Î± âˆˆFq, set |Î±| = 0 if Î± = 0 and |Î±| = 1 otherwise. Then for all u âˆˆFN
q we
compute
g(u) = âˆ‘
vâˆˆFNq
Ï‡

âŸ¨v,uâŸ©

g(v)
= âˆ‘
vâˆˆFNq
Ï‡

âŸ¨v,uâŸ©

xw(v)yNâˆ’w(v)
= âˆ‘
v0âˆˆFq
... âˆ‘
vNâˆ’1âˆˆFq
Ï‡
 Nâˆ’1
âˆ‘
i=0
viui

x|v0|+Â·Â·Â·|vNâˆ’1|y(1âˆ’|v0|)+Â·Â·Â·+(1âˆ’|vNâˆ’1|)
= âˆ‘
v0âˆˆFq
... âˆ‘
vNâˆ’1âˆˆFq
Nâˆ’1
âˆ
i=0
Ï‡(viui)x|vi|y1âˆ’|vi|
=
Nâˆ’1
âˆ
i=0 âˆ‘
gâˆˆG
Ï‡(gui)x|g|y1âˆ’|g|.
If ui = 0 then Ï‡(gui) = Ï‡(0) = 1 and so
âˆ‘
gâˆˆG
x|g|y1âˆ’|g| = y+(qâˆ’1)x.

316
Further Topics from Coding Theory
If ui Ì¸= 0 then
âˆ‘
gâˆˆG
Ï‡(gui)x|g|y1âˆ’|g| = y+ âˆ‘
gâˆˆG\0
Ï‡(gui)x = yâˆ’Ï‡(0)x = yâˆ’x.
Lemma 3.4.5
(MacWilliams identity for linear codes) If X is a linear [N,k]
code over Fq then
âˆ‘
xâˆˆX
f(x) = qk âˆ‘
yâˆˆX âŠ¥
f(y).
(3.4.14)
Proof
Consider the following sum:
âˆ‘
xâˆˆX
f(x) = âˆ‘
xâˆˆX âˆ‘
vâˆˆFNq
Ï‡(v)(x) f(v)
= âˆ‘
vâˆˆFNq âˆ‘
xâˆˆX
Ï‡

âŸ¨v,xâŸ©

f(v)
= âˆ‘
vâˆˆX âŠ¥âˆ‘
xâˆˆX
Ï‡

âŸ¨v,xâŸ©

f(v)
+
âˆ‘
vâˆˆFNq \X âŠ¥âˆ‘
xâˆˆX
Ï‡

âŸ¨v,xâŸ©

f(v).
In the ï¬rst sum we have Ï‡

âŸ¨v,xâŸ©

= Ï‡(0) = 1 for all v âˆˆX âŠ¥and all x âˆˆX . In
the second sum we study the linear form
X â†’Fq : x â†’âŸ¨v,xâŸ©.
Since v âˆˆFN
q \X âŠ¥, this linear form is surjective, whence its kernel has dimension
k âˆ’1, i.e. for any g âˆˆFq there exist qkâˆ’1 vectors x âˆˆX such that âŸ¨v,xâŸ©= g. This
implies
âˆ‘
xâˆˆX
f(x) = qk âˆ‘
yâˆˆX âŠ¥
f(y)+qkâˆ’1
âˆ‘
vâˆˆFnq\X âŠ¥
f(v) âˆ‘
gâˆˆG
Ï‡(g)
= qk âˆ‘
yâˆˆX âŠ¥
f(y)
as the second term vanishes by Lemma 3.4.2.
Lemma 3.4.6
The weight enumerator of an [N,k] code X over Fq is related to
the weight enumerator of its dual as follows:
WX âŠ¥(x,y) = qâˆ’kWX (yâˆ’x,y+(qâˆ’1)x).
(3.4.15)

3.4 The MacWilliams identity and the linear programming bound
317
Proof
By Lemma 3.4.5 with g(v) = xw(v)yNâˆ’w(v)
WX âŠ¥(x,y) = âˆ‘
vâˆˆX âŠ¥
g(v) = qâˆ’k âˆ‘
vâˆˆX
g(v)
= qâˆ’kWX (yâˆ’x,y+(qâˆ’1)x).
Substituting x = z,y = 1 we obtain (3.4.3).
Example 3.4.7
(i) For all codes X , WX (0) = A0 = 1 and WX (1) = â™¯X . When
X = FÃ—N
q , WX (z) = [1+z(qâˆ’1)]N.
(ii) For a binary repetition code X = {0000,1111}, WX (x,y) = x4 +y4. Hence,
WX âŠ¥(x,y) = 1
2

(yâˆ’x)4 +(y+x)4
= y4 +6x2y2 +x4.
(iii) Let X be the Hamming [7,4] code. The dual code X âŠ¥has 8 codewords; all
except 0 are of weight 4. Hence, WX âŠ¥(x,y) = x7 +7x4y3, and, by the MacWilliams
identity,
WX = 1
23WX âŠ¥(xâˆ’y,x+y) = 1
23

(xâˆ’y)7 +7(xâˆ’y)4(x+y)3
= x7 +7x4y3 +7x3y4 +y4.
Hence, X has 7 words of weight 3 and 4 each. Together with the 0 and 1 words,
this accounts for all 16 words of the Hamming [7,4] code.
Another way to derive the identity (3.4.1) is to use an abstract result related
to group algebras and character transforms for Hamming spaces FÃ—N
q
(which are
linear spaces over ï¬eld Fq of dimension N). For brevity, the subscript q and super-
script (N) will be often omitted.
Deï¬nition 3.4.8
The (complex) group algebra CFÃ—N for space FÃ—N is deï¬ned as
the linear space of complex functions G : x âˆˆFÃ—N â†’G(x) âˆˆC equipped by a com-
plex involution (conjugation) and multiplication. Thus, we have four operations for
functions G(x); addition and scalar (complex) multiplication are standard (point-
wise), with (G + Gâ€²)(x) = G(x) + Gâ€²(x) and (aG)(x) = aG(x), G,Gâ€² âˆˆCFÃ—N,
a âˆˆC, x âˆˆFÃ—N. The involution is just the (point-wise) complex conjugation:
Gâˆ—(x) = G(x)âˆ—; it is an idempotent operation, with Gâˆ—âˆ—= G. However, the mul-
tiplication (denoted by â‹†) is a convolution:
(Gâ‹†Gâ€²)(x) = âˆ‘
yâˆˆFÃ—N
G(y)Gâ€²(xâˆ’y), x âˆˆFÃ—N.
(3.4.16)
This makes CFÃ—N a commutative ring and at the same time a (complex) linear
space, of dimension dim CFÃ—N = qN, with involution. (A set that is a commutative

318
Further Topics from Coding Theory
ring and a linear space is called an algebra.) The natural basis in CFÃ—N is formed
by Diracâ€™s (or Kroneckerâ€™s) delta-functions Î´ y, with Î´ y(x) = 1(x = y), x,y âˆˆH .
If X âŠ†FÃ—N is a linear code, we set GX (x) = 1(x âˆˆX ).
The multiplication rule (3.4.16) requires an explanation. If we rewrite the
RHS in a symmetric form
âˆ‘
y,yâ€²âˆˆFÃ—N:y+yâ€²=x
G(y)G(yâ€²) (which makes the commu-
tativity of the â‹†-multiplication obvious) then there will be an analogy with the
multiplication of polynomials. In fact, if A(t) = a0 + a1t + Â·Â·Â· + alâˆ’1tlâˆ’1 and
Aâ€²(t) = aâ€²
0 + aâ€²
1t + Â·Â·Â· + aâ€²
lâ€²âˆ’1tlâ€²âˆ’1 are two polynomials, with coefï¬cient strings
(a0,...,alâˆ’1) and (aâ€²
0,...,aâ€²
lâ€²âˆ’1), then the product B(t) = A(t)Aâ€²(t) has a string
of coefï¬cients (b0,...,blâˆ’1+lâ€²âˆ’1) where bk =
âˆ‘
m,mâ€²â‰¥0:m+mâ€²=k
amaâ€²
mâ€².
From this point of view, rule (3.4.16) is behind some polynomial-type multipli-
cation. Polynomials of degree â‰¤nâˆ’1 form of course a (complex) linear space of
dimension n. However, they do not form a group (or even a semi-group). To make
a group, we should afï¬liate inverse monomials 1/t, 1/t2, and so on, and either con-
sider inï¬nite series or make an agreement that tn = 1 (i.e. treat t as an element of
a cyclic group, not a â€˜freeâ€™ variable). Similar constructions can be done for poly-
nomials of several variables, but there we have a variety of possible agreements on
relations between variables.
Returning to our group algebra CH , we make the following steps:
(i) Produce a â€˜multiplicative versionâ€™ of the Hamming group H . That is, take a
collection of â€˜formalâ€™ variables t(x) labelled by elements x âˆˆH and postulate the
rule t(x)t(xâ€²) = t(x+xâ€²) for all x,xâ€² âˆˆCH .
(ii) Then consider the set TH
of all (complex) linear combinations G =
âˆ‘xâˆˆH Î³xt(x) and introduce (ii1) the addition G+Gâ€² = âˆ‘xâˆˆH (Î³x +Î³â€²
x)t(x) and (ii2)
the scalar multiplication aG = âˆ‘xâˆˆH (aÎ³x)t(x), G,Gâ€² âˆˆTH , a âˆˆC. We again ob-
tain a linear space of dimension qN, with the basis formed by â€˜basicâ€™ combina-
tions t(x), x âˆˆH . Obviously, TH and CH are isomorphic as linear spaces, with
G â‡â‡’g.
(iii) Now remove brackets in t(x) (but keep the rule txtxâ€² = tx+xâ€²) and write
âˆ‘xâˆˆH Î³xtx as g(t) thinking that this is a function (in fact, a â€˜polynomialâ€™) of some
â€˜variableâ€™ t obeying the above rule. Finally, consider the polynomial multiplication
g(t)gâ€²(t) in TH . Then TH and CH become isomorphic not only as linear spaces
but also as rings, i.e. as algebras.
The above construction is very powerful and can be used for any group, not just
for HN. Its power will be manifested in the derivation of the MacWilliams identity.
So, we will think of CH as a set of functions
g(t) = âˆ‘
xâˆˆHn
Î³xtx
(3.4.17)

3.4 The MacWilliams identity and the linear programming bound
319
of a formal variable t obeying an â€˜exponentiation ruleâ€™: tx+xâ€² = txtxâ€², with addition
and multiplication of formal polynomials.
In agreement with (3.4.17), for a linear code X âŠ‚Hn we set
gX (t) = âˆ‘
xâˆˆX
tx;
(3.4.18)
gX (t) is often called the generating function of X .
Deï¬nition 3.4.3 admits a straightforward generalisation for any non-principal
character Ï‡: F â†’S. Note the similarity with the Fourier transform (and other types
of popular transforms (viz. the Hadamard transform in the group theory)).
Deï¬nition 3.4.9
The character transform g â†’Ë†g of the group algebra CHn is
deï¬ned by
Ë†g(t) = âˆ‘
xâˆˆHn
Xx(g)tx,
(3.4.19a)
where g âˆ¼(Î³x, x âˆˆHn) and
Xx(g) = âˆ‘
yâˆˆHn
Î³yÏ‡(xÂ·y)
(3.4.19b)
and xÂ·y is the dot-product âˆ‘1â‰¤jâ‰¤n xjyj in Hn.
Now deï¬ne the weight enumerator of a group algebra element g âˆˆCH as a
polynomial Wg(s) in a variable s (which may be thought of as a complex variable):
Wg(s) = âˆ‘
xâˆˆH
Î³xsw(x) =
n
âˆ‘
k=0

âˆ‘
x:w(x)=k
Î³x

sk = âˆ‘
0â‰¤kâ‰¤n
Aksk, s âˆˆC.
(3.4.20)
Here
Ak =
âˆ‘
xâˆˆH :w(x)=k
Î³x.
(3.4.21)
For a linear code X , with generating function gX (t) (see 3.4.18)), Ak gives the
number of codewords of weight k:
Ak = #{x âˆˆX : w(x) = k}.
(3.4.22)
The weight enumerator WË†g(s) of the character transform Ë†g of g âˆ¼(Î³x, x âˆˆH ) is
given by
WË†g(s) = âˆ‘
xâˆˆH
Xx(g)sw(x) = âˆ‘
0â‰¤kâ‰¤n

âˆ‘
x: w(x)=k
Xx(g)

sk = âˆ‘
k
Ë†Aksk,
(3.4.23)

320
Further Topics from Coding Theory
where
Ë†Ak =
âˆ‘
xâˆˆH : w(x)=k
Xx(g).
(3.4.24)
The â€˜abstractâ€™ MacWilliams identity is established in the following result.
Theorem 3.4.10
We have
WË†g(s) = (1+(qâˆ’1)s)nWg

1âˆ’s
1+(qâˆ’1)s

.
(3.4.25)
Proof
Basically coincides with that of Lemma 3.4.4.
Rewrite (3.4.25) in terms of coefï¬cients Ak and Ë†Ak:
n
âˆ‘
k=0
Ë†Aksk =
n
âˆ‘
k=0
Ak(1âˆ’s)k(1+(qâˆ’1)s)nâˆ’k
(3.4.26)
and expand:
(1âˆ’s)k(1+(qâˆ’1)s)nâˆ’k =
n
âˆ‘
i=0
Ki(k)si.
(3.4.27)
Here Ki(k)(= Ki(k,n,q)) is a Kravchuk polynomial: for all i,k = 0,1,...,n,
Ki(k) =
iâˆ§k
âˆ‘
j=0âˆ¨(i+kâˆ’n)
 k
j
 nâˆ’k
iâˆ’j

(âˆ’1) j(qâˆ’1)iâˆ’j,
0âˆ¨(i+k âˆ’n) = max[0,i+k âˆ’n], iâˆ§k = min[i,k].
(3.4.28)
Then
âˆ‘
0â‰¤kâ‰¤n
Ë†Aksk = âˆ‘
0â‰¤kâ‰¤n
Ak âˆ‘
0â‰¤iâ‰¤n
Ki(k)si = âˆ‘
0â‰¤iâ‰¤n âˆ‘
0â‰¤kâ‰¤n
AkKi(k)si
= âˆ‘
0â‰¤kâ‰¤n âˆ‘
0â‰¤iâ‰¤n
AiKk(i)sk,
i.e.
Ë†Ak = âˆ‘
0â‰¤iâ‰¤n
AiKk(i).
(3.4.29)
Lemma 3.4.11
For any (linear) code X âŠ†Hn, with generating function gX âˆ¼
1(x âˆˆX ), the character transform coefï¬cients are related by
Xu(gX ) = #X 1(u âˆˆX âŠ¥)
(3.4.30)
and the character transform
Ë†gX = #X gX âŠ¥.
(3.4.31)
Here, X âŠ¥is the dual code.

3.4 The MacWilliams identity and the linear programming bound
321
Proof
By Lemma 3.4.2
Xu(gX ) = Xu

âˆ‘
xâˆˆX
tx

= âˆ‘
yâˆˆX
Ï‡(yÂ·u) = #X 1(u âˆˆX âŠ¥).
In fact, the character y âˆˆX â†’Ï‡(yÂ·u) is principal iff u âˆˆX âŠ¥. Consequently,
Ë†g(t) = âˆ‘
xâˆˆH
Xx(gX )tx = âˆ‘
xâˆˆH
#X 1(x âˆˆX âŠ¥)tx
= #X âˆ‘
xâˆˆX âŠ¥
tx = #X gX âŠ¥(t).
Hence,
WË†gX (s) = #X WgX âŠ¥(s),
(3.4.32)
and we obtain the MacWilliams identity for linear codes:
Theorem 3.4.12
Let X âŠ‚Hn be a linear code, X âŠ¥its dual, and
WX (s) =
n
âˆ‘
k=0
Aksk, WX âŠ¥(s) =
n
âˆ‘
k=0
AâŠ¥
k sk
(3.4.33)
the w-enumerators for X and X âŠ¥, respectively, with Ak = #{x âˆˆX : w(x) = k}
and Ë†Ak = #{x âˆˆX âŠ¥: w(x) = k}. Then
WX âŠ¥(s) =
1
#X (1+(qâˆ’1)s)nWX

1âˆ’s
1+(qâˆ’1)s

, s âˆˆC,
(3.4.34)
or, equivalently,
AâŠ¥
k =
1
#X âˆ‘
0â‰¤iâ‰¤n
AiKk(i),
(3.4.35)
where Kk(i) are Kravchuk polynomials (see (3.4.28)).
For a binary code, i.e. q = 2, (3.4.34) takes the form (3.4.3). Sometimes the
weight enumerators are deï¬ned as
WX âŠ¥(s,r) = âˆ‘
k
Akskrnâˆ’k.
(3.4.36)
Then the MacWilliams identity (3.4.33) takes the form
WX âŠ¥(s,r) =
1
#X WX (r âˆ’s,r +(qâˆ’1)s).
(3.4.37)
The MacWilliams identity is a powerful result providing a deep insight into the
structure of a (linear) code, particularly when the code is self-dual.

322
Further Topics from Coding Theory
The MacWilliams identity helps to establish an interesting bound on linear codes
called the linear programming (LP) bound. First, we discuss some immediate con-
sequences of this identity. If X âŠ‚HN,q is a code of size M, set
Bk = 1
Mâ™¯{(x,y) : x,y âˆˆX ,Î´(x,y) = k},
k = 0,1,...,N
(each pair x,y is counted two times). The numbers B0,B1,...,BN form the distance
distribution of code X . The expression
BX (s) = âˆ‘
0â‰¤kâ‰¤N
Bksk
(3.4.38)
is called the distance enumerator of X . Clearly, the w- and d-distributions of a
linear code coincide. Furthermore we have
Lemma 3.4.13
The d-enumerator of an [N,M] code X coincides with the w-
enumerator of the group algebra element
hX (s) := 1
MÎ¶X (s)Î¶X (sâˆ’1))
(3.4.39)
where the generating function of X is
Î¶X (s) = âˆ‘
xâˆˆX
sx.
(3.4.40)
Proof
Using the notation (sâˆ’1)x, write
hX (s) = 1
M âˆ‘
xâˆˆX
sx âˆ‘
yâˆˆX
sâˆ’y = 1
M âˆ‘
x,yâˆˆX
sxâˆ’y
and hence
WhX (s) = 1
M âˆ‘
0â‰¤kâ‰¤N âˆ‘
x,yâˆˆX :
1

w(xâˆ’y) = k

sk = âˆ‘
0â‰¤kâ‰¤N
Bksk
= BX (s).
Now by the MacWilliams identity, for a given non-trivial character Ï‡ and the
corresponding transform Î¶ â†’Î¶, we obtain
Theorem 3.4.14
For hX (s) as above, if hX (s) is the character transform and
WhX (s) its w-enumerator, with
WhX (s) = âˆ‘
0â‰¤kâ‰¤N
Bksk = âˆ‘
0â‰¤kâ‰¤N

âˆ‘
w(x)=k
Ï‡x(hX )

sk,

3.4 The MacWilliams identity and the linear programming bound
323
then
Bk = âˆ‘
0â‰¤iâ‰¤N
BiKk(i),
where Kk(i) are Kravchuk polynomials.
The following assertion is straightforward.
Lemma 3.4.15
The following identity holds: Ï‡x(Î¶X (sâˆ’1)) = Ï‡x(Î¶X (s)), where
the bar denotes the complex conjugate.
With the help of Lemma 3.4.15, we can write
Ï‡x(hX (t)) = 1
M Ï‡x(Î¶X (s)Î¶X (sâˆ’1)) = 1
M Ï‡x(Î¶X (s))Ï‡x(Î¶X (sâˆ’1))
= 1
M Ï‡x(Î¶X (s))Ï‡x(Î¶X (s)) = 1
M|Ï‡x(Î¶X (s))|2,
and so,
Bk =
âˆ‘
x:w(x)=k
Ï‡x(hX ) = 1
M âˆ‘
w(x)=k
|Ï‡x(Î¶X )|2 â‰¥0.
Thus:
Theorem 3.4.16
For all [N,M] codes X and k = 0,...,N,
âˆ‘
0â‰¤iâ‰¤N
BiKk(i) â‰¥0.
(3.4.41)
Now counting the number of pairs (x,y) âˆˆX Ã—X :
âˆ‘
0â‰¤iâ‰¤N
Bi = M2
or
âˆ‘
0â‰¤iâ‰¤N
Ei = M, with Ei = 1
MBi
(3.4.42)
(sometimes E0,E1,...,EN are called the d-distribution of X ). Then, by (3.4.41)â€“
(3.4.42),
âˆ‘
0â‰¤iâ‰¤N
EiKk(i) â‰¥0.
In addition, by deï¬nition, Ei â‰¥0,0 â‰¤i â‰¤N, and E0 = 1 and Ei = 0,1 â‰¤i < d.
Proof
Let Ï‰ be a primitive qth root of unity and x âˆˆFN
q be a ï¬xed word of weight
i. Then
âˆ‘
yâˆˆFNq :w(y)=k
Ï‰âŸ¨x,yâŸ©= Kk(i).
(3.4.43)

324
Further Topics from Coding Theory
Indeed, we may assume that x = x1x2 ...xi0...0 where the coordinates xi are not
0. Let D be a set of words that have their non-zero coordinates in a given set of
k positions. Suppose that exactly j positions h1,...,hk belong to [0,i] and k âˆ’j
positions belong to [i + 1,N]. For such, a set could be selected in
i
j
N âˆ’i
k âˆ’j

choices. Then
âˆ‘
yâˆˆD
Ï‰âŸ¨x,yâŸ©= âˆ‘
yh1âˆˆFâˆ—q
... âˆ‘
yhkâˆˆFâˆ—q
Ï‰xh1yh1+Â·Â·Â·+xhkyhk
= (qâˆ’1)kâˆ’j
j
âˆ
i=1 âˆ‘
yâˆˆFâˆ—q
Ï‰xhiy = (âˆ’1) j(qâˆ’1)kâˆ’j.
Hence,
M
N
âˆ‘
i=0
BiKk(i) =
N
âˆ‘
i=0
âˆ‘
x,yâˆˆX :Î´(x,y)=i
âˆ‘
zâˆˆFNq :w(z)=k
Ï‰âŸ¨xâˆ’y,zâŸ©
=
âˆ‘
zâˆˆFNq :w(z)=k
| âˆ‘
xâˆˆX
Ï‰âŸ¨x,zâŸ©|2 â‰¥0.
This leads us to the so-called linear programming (LP) bound stated in Theorem
3.4.17 below.
Theorem 3.4.17
(The LP bound) The following inequality holds:
Mâˆ—
q(N,d) â‰¤max

âˆ‘
0â‰¤iâ‰¤N
 Ei :  Ei â‰¥0,  E0 = 1,  Ei = 0 for 1 â‰¤i < d
and âˆ‘
0â‰¤iâ‰¤N
 EiKk(i) â‰¥0 for 0 â‰¤k â‰¤N

. (3.4.44)
For q = 2, the LP bound will be slightly improved in Theorem 3.4.19. First, an
auxiliary result whose proof is straightforward and left as an exercise.
Lemma 3.4.18
(a) If there exists a binary [N,M,d] code, with d even, then there exists a binary
[N,M,d] code where any codeword has even weight, and so all distances are
even. So, if q = 2 and d is even, we may assume that Ei = 0 for all odd values
of i.
(b) For q = 2,
Ki(2k) = KNâˆ’i(2k).

3.4 The MacWilliams identity and the linear programming bound
325
Hence, for d even, as we can assume that E2i+1 = 0, the constraint in (3.4.44)
need only be considered for k = 0,...,[N/2].
(c) K0(i) = 1 for all i, and thus the bound
âˆ‘
0â‰¤iâ‰¤N
 EiK0(i) â‰¥0 follows from  Ei â‰¥0.
Lemma 3.4.18 directly implies
Theorem 3.4.19
(The LP for q = 2) If d is even then
Mâˆ—
2(N,d) â‰¤max

âˆ‘
0â‰¤iâ‰¤N
 Ei :  Ei â‰¥0,  E0 = 1,  Ei = 0 for 1 < i < d,
 Ei = 0 for i odd, and
N
k

+
âˆ‘
dâ‰¤iâ‰¤N
 EiKk(i) â‰¥0
for k = 1,...,
AN
2
B
.
(3.4.45)
Since Mâˆ—
2(N,2t + 1) = Mâˆ—
2(N + 1,2t + 2), Theorem 3.4.19 provides a useful
bound also when d is odd. We will explore the MacWilliams identity further on.
The LP bound represents a rather universal tool in the theory of codes. For in-
stance, the Singleton, Hamming and Plotkin bounds can all be derived from the LP
bound. However, we will not exploit this avenue in detail.
Worked Example 3.4.20
For positive integers N and d â‰¤N, let
f(x) = 1+
N
âˆ‘
j=1
f jKj(x)
be a polynomial such that f j â‰¥0,1 â‰¤j â‰¤N and f(i) â‰¤0 for d â‰¤i â‰¤N. Prove that
Mâˆ—
q(N,d) â‰¤f(0).
(3.4.46)
Derive the Singleton bound from (3.4.46).
Solution
Let M = Mâˆ—
q(N,d) and X be a q-ary [N,M] code with the distance
distribution Bi(X ), i = 0,...,N. The condition f(i) â‰¤0 for d â‰¤i â‰¤N implies

326
Further Topics from Coding Theory
N
âˆ‘
j=d
B j(X ) f( j) â‰¤0. Using the LP bound (3.4.45) for k = 0 obtain Ki(0) â‰¥
âˆ’
N
âˆ‘
j=d
B j(X )Ki( j). Hence,
f(0) = 1+
N
âˆ‘
j=1
f jKj(0)
â‰¥1âˆ’
N
âˆ‘
k=1
fk
N
âˆ‘
i=d
Bi(X )Kk(i)
= 1âˆ’
N
âˆ‘
i=d
Bi(X )
N
âˆ‘
k=1
fkKk(i)
= 1âˆ’
N
âˆ‘
i=d
Bi(X )( f(i)âˆ’1)
â‰¥1+
N
âˆ‘
i=d
Bi(X )
= M = Mâˆ—
q(N,d).
To obtain the Singleton bound select
f(x) = qNâˆ’d+1
N
âˆ
j=d

1âˆ’x
j

.
Then by the identity
j
âˆ‘
i=0
 N âˆ’i
N âˆ’j

Ki(k) = qj
 N âˆ’k
j

with j = d âˆ’1 we have that
fk = 1
qN
N
âˆ‘
i=0
f(i)Ki(k)
=
1
qdâˆ’1
dâˆ’1
âˆ‘
i=0

N âˆ’i
N âˆ’d +1

Ki(k)/

N
d âˆ’1

=
 N âˆ’k
d âˆ’1

/

N
d âˆ’1

â‰¥0.
Here we use the identity
j
âˆ‘
k=0
 N âˆ’k
N âˆ’j

Kk(x) = qj
 N âˆ’x
j

.
(3.4.47)
Clearly, f(i) = 0 for d â‰¤i â‰¤N. Hence, Rq(N,d) â‰¤f(0) = qNâˆ’d+1. In a similar
manner Hammingâ€™s and Plotkinâ€™s bounds may be derived as well, cf. [97].

3.4 The MacWilliams identity and the linear programming bound
327
Worked Example 3.4.21
Using the linear programming bound, prove that
Mâˆ—
2(13,5) = Mâˆ—
2(14,6) â‰¤64. Compare it with the Elias bound. [Hint: E6 = 42,
E8 = 7, E10 = 14, E12 = E14 = 0. You may need a computer to get the solution.]
Solution The LP bound for linear codes reads
Mâˆ—
2(N,d) = max
âˆ‘
0â‰¤iâ‰¤N
Ei
subject to Ei â‰¥0, E0 = 1, E j = 0 for 1 â‰¤j < d,
Ei = 0 for j odd, and
N
k

+ âˆ‘
dâ‰¤iâ‰¤N
i even
EiKk(i) â‰¥0 for k = 1,...,
AN
2
B
.
For N = 14, d = 6, the constraints are
E0 = 1, E1 = E2 = E3 = E4 = E5 = E7 = E9 = E11 = E13 = 0,
E6, E8, E10, E12, E14 â‰¥0,
14+2E6 âˆ’2E8 âˆ’6E10 âˆ’10E12 âˆ’14E14 â‰¥0,
91âˆ’5E6 âˆ’5E8 +11E10 +43E12 +91E14 â‰¥0,
364âˆ’12E6 +12E8 +4E10 âˆ’100E12 âˆ’364E14 â‰¥0,
1001+9E6 +9E8 âˆ’39E10 +121E12 +1001E14 â‰¥0,
2002+30E6 âˆ’30E8 +38E10 âˆ’22E12 âˆ’2002E14 â‰¥0,
3003âˆ’5E6 âˆ’5E8 +27E10 âˆ’165E12 +3003E14 â‰¥0,
3432âˆ’40E6 +40E8 âˆ’72E10 +264E12 âˆ’3432E14 â‰¥0,
and the maximiser of the objective function S = E6 +E8 +E10 +E12 +E14 is
E6 = 42, E8 = 7, E10 = 14, E12 = E14 = 0,
with S = 63, E0 +S = 1+63 = 64. So, the LP bound yields
Mâˆ—
2(13,5) = Mâˆ—
2(14,6) â‰¤64.
Note that the bound is sharp as a [13,64,5] binary code actually exists. Compare
the LP bound with the Hamming bound:
Mâˆ—
2(13,5) â‰¤213
(1+13+13Â·6) = 213
92 = 211/23,
i.e.
Mâˆ—
2(13,5) â‰¤91.
Next, the Singleton bound gives k â‰¤13âˆ’5âˆ’1 = 7,
Mâˆ—
2(13,5) â‰¤27 = 128.

328
Further Topics from Coding Theory
It is also interesting to see what the Elias bound gives:
Mâˆ—
2(13,5) â‰¤
65/2
s2 âˆ’13s+65/2
213
1+13+Â·Â·Â·+
13
5

for all s < 13 such that s2 âˆ’13s+65/2 > 0.
Substituting s = 2 yields s2 âˆ’13s+65/2 = 4âˆ’26+65/2 = 21/2 > 0 and
Mâˆ—
2(13,5) â‰¤65
21 213
1+13+13Â·6

= 2.33277Ã—106 :
not good enough. Next, s = 3 yields s2 âˆ’13s + 65/2 = 9 âˆ’39 + 65/2 = 5/2 > 0
and
Mâˆ—
2(13,5) â‰¤65
5 213
1+13+13Â·6+13Â·2Â·5

= 13Ã— 212
111 â‰¥13
66 211 :
not as good as Hammingâ€™s. Finally, observe that 42 âˆ’13 Ã— 4 + 65/2 < 0, and the
procedure stops.
3.5 Asymptotically good codes
In this section we brieï¬‚y discuss some families of codes where the number of
corrected errors gives a non-zero fraction of the codeword length. For more details,
see [54], [71], [131].
Deï¬nition 3.5.1
A sequence of [Ni,ki,di] codes with Ni â†’âˆis said to be asymp-
totically good if ki/Ni and di/Ni are both bounded away from 0.
Theorem 3.3.25 showed that there is no asymptotically good sequence of primi-
tive BCH codes (in fact, there is no asymptotically good sequence of BCH codes of
any form). Theoretically, an elegant way to produce an asymptotically good family
are the so-called Justensen codes. As the ï¬rst attempt to deï¬ne a good code take
0 Ì¸= Î± âˆˆF2m â‰ƒFm
2 and deï¬ne the set
XÎ± = {(a,Î±a) : a âˆˆFm
2 }.
(3.5.1)
Then XÎ± is a [2m,m] linear code and has information rate 1/2. We can recover Î±
from any non-zero codeword (a,b) âˆˆXÎ±, as Î± = baâˆ’1 (division in F2m). Hence,
if Î± Ì¸= Î±â€² then XÎ± âˆ©XÎ±â€² = {0}.
Now, given Î» = Î»m âˆˆ(0,1/2], we want to ï¬nd Î± = Î±m such that code XÎ± has
minimum weight â‰¥2mÎ». Since a non-zero binary (2m)-word can enter at most
one of the XÎ±â€™s, we can ï¬nd such Î± if the number of the non-zero (2m)-words

3.5 Asymptotically good codes
329
of weight < 2mÎ» is < 2m âˆ’1, the number of distinct codes XÎ±. That is, we can
manage if
âˆ‘
1â‰¤iâ‰¤2mÎ»âˆ’1
2m
i

< 2m âˆ’1
or even better,
âˆ‘
1â‰¤iâ‰¤2mÎ»
2m
i

< 2m âˆ’1. Now use the following:
Lemma 3.5.2
For 0 â‰¤Î» â‰¤1/2,
âˆ‘
0â‰¤kâ‰¤âŒŠÎ»NâŒ‹
N
k

â‰¤2NÎ·(Î»),
(3.5.2)
where Î·(Î») is the binary entropy.
Proof
Observe that (3.5.2) holds for Î» = 0 (here both sides are equal to 1) and
Î» = 1/2 (where the RHS equals 2N). So we will assume that 0 < Î» < 1/2. Consider
a random variable Î¾ with the binomial distribution
P

Î¾ = k) =
N
k

(1/2)N,0 â‰¤k â‰¤N.
Given t âˆˆR+, use the following Chebyshev-type inequality:
âˆ‘
0â‰¤kâ‰¤Î»N
N
k
1
2
N
= P(Î¾ â‰¤Î»N)
= P

exp(âˆ’tÎ¾) â‰¥eâˆ’Î»Nt
â‰¤eâˆ’Î»NtEeâˆ’tÎ¾
= eâˆ’Î»Nt
1
2 + 1
2eâˆ’t
N
.
(3.5.3)
Minimise the RHS of (3.5.3) in x = eâˆ’t for t > 0, i.e. for 0 < x < 1. This yields the
minimiser eâˆ’t = Î»/(1âˆ’Î») and the minimal value

Î»
1âˆ’Î»
âˆ’Î»N 1
2
N 
1+
Î»
1âˆ’Î»
N
= Î» âˆ’Î»NÎ¼âˆ’Î¼N
1
2
N
= 2NÎ·(Î»)
1
2
N
,
with Î¼ = 1âˆ’Î». Hence, (3.5.2) implies
âˆ‘
0â‰¤kâ‰¤Î»N
N
k
1
2
N
â‰¤2NÎ·(Î»)
1
2
N
.

330
Further Topics from Coding Theory
Owing to Lemma 3.5.2, inequality (3.5.1) occurs when
22mÎ·(Î») < 2m âˆ’1.
(3.5.4)
Now if, for example,
Î» = Î»m = Î·âˆ’1

1/2âˆ’
1
logm

(with 0 < Î» < 1
2), bound (3.5.4) becomes 2mâˆ’2m/log m < 2m âˆ’1 which is true for
m large enough. And Î»m â†’hâˆ’1(1/2) > 0, as m â†’âˆ. Here and below, Î·âˆ’1 is
the inverse function to Î» âˆˆ(0,1/2] â†’Î·(Î»). In the code (3.5.1) with a ï¬xed Î±
the information rate is 1/2 but one cannot guarantee that d/2m is bounded away
from 0. Moreover, there is no effective way of ï¬nding a proper Î± = Î±m. However,
in 1972, Justensen [81] showed how to obtain a good sequence of codes cleverly
using the concatenation of words from an RS code.
More precisely, consider a binary (k1k2)-word a organised as k1 separate k2-
words: a = a(0)a(1) ...a(k1âˆ’1). Pictorially,
k2
k2
â†â†’
â†â†’
a
=
...
a(i) âˆˆF2k2, 0 â‰¤i â‰¤k1 âˆ’1.
a(0)
a(k1âˆ’1)
We ï¬x an [N1,k1,d1] code X1 over F2k2 called an outer code: X1 âŠ‚FN1
2k2. Then
string a is encoded into a codeword c = c0c1 ...cN1âˆ’1 âˆˆX1. Next, each ci âˆˆF2k2
is encoded by a codeword bi from an [N2,k2,d2] code X2 over F2, called an inner
code. The result is a string b = b(0) ...b(N1âˆ’1) âˆˆFN1N2
q
of length N1N2:
N2
N2
â†â†’
â†â†’
b
=
...
b(i) âˆˆF2N2, 0 â‰¤i â‰¤N1 âˆ’1.
b(0)
b(N1âˆ’1)
The encoding is represented by the diagram:
input: a (k1k2) string a,
output: an (N1N2) codeword b.
Observe that different symbols ci can be encoded by means of different inner
codes. Let the outer code X1 be a [2m âˆ’1,k,d] RS code X RS over F2m. Write a
binary (k2m)-word a as a concatenation a(0) ...a(kâˆ’1), with a(i) âˆˆF2m. Encoding a
using X RS gives a codeword c = c0 ...cNâˆ’1, with N = 2m âˆ’1 and ci âˆˆF2m. Let Î²
be a primitive element in F2m. Then for all j = 0,...,N âˆ’1 = 2m âˆ’2, consider the
inner code
X ( j) =
*
(c,Î² jc) : c âˆˆF2m+
.
(3.5.5)

3.5 Asymptotically good codes
331
The resulting codeword (a â€˜super-codewordâ€™) is
b = (c0,c0)(c1,Î²c1)(c2,Î² 2c2)...(cNâˆ’1,Î² Nâˆ’1cNâˆ’1).
(3.5.6)
Deï¬nition 3.5.3
The Justensen code X Ju
m,k is the collection of binary super-words
b obtained as above, with the [2m âˆ’1,k,d] RS code as an outer code X1, and
X ( j) (see (3.5.6)) as the inner codes, where 0 â‰¤j â‰¤2m âˆ’2. Code X Ju
m,k has length
2m(2m âˆ’1), rank mk and hence rate
k
2(2m âˆ’1) < 1/2.
A convenient parameter describing X Ju
m,k is N = 2m âˆ’1, the length of the outer
RS code. We want to construct a sequence X Ju
m,k with N â†’âˆ, but k/(2m(2m âˆ’
1)) and d/(2m(2m âˆ’1)) bounded away from 0. Fix R0 âˆˆ(0,1/2) and choose a
sequence of outer RS codes X RS
N
of length N, with N = 2m âˆ’1 and k = [2NR0].
Then the rate of X Ju
m,k is k/(2N) â‰¥R0.
Now consider the minimum weight
w

X Ju
m,k

= min
	
w(x) : x âˆˆX Ju
m,k, x Ì¸= 0

 
= d

X Ju
m,k

.
(3.5.7)
For any ï¬xed m, if the outer RS code X RS
N , N = 2m âˆ’1, has minimum weight
d then any super-codeword b = (c0,c0)(c1,Î²c1)...(cNâˆ’1,Î² Nâˆ’1cNâˆ’1) âˆˆX Ju
m,k has
â‰¥d non-zero ï¬rst components c0,...,cNâˆ’1. Furthermore, any two inner codes
among X (0),X (1),...,X (Nâˆ’1) have only 0 in common. So, the corresponding d
ordered pairs, being from different codes, must be distinct. That is, super-codeword
b has â‰¥d distinct non-zero binary (2m)-strings.
Next, the weight of super-codeword b âˆˆX Ju
m,k is at least the sum of the weights
of the above d distinct non-zero binary (2m)-strings. So, we need to establish a
lower bound on such a sum. Note that
d = N âˆ’k +1 = N

1âˆ’k âˆ’1
N

â‰¥N(1âˆ’2R0).
Hence, a super-codeword b âˆˆX Ju
m,k has at least N(1âˆ’2R0) distinct non-zero binary
(2m)-strings.
Lemma 3.5.4
The sum of the weights of any N(1âˆ’2R0) distinct non-zero binary
(2m)-strings is
â‰¥2mN(1âˆ’2R0)

Î·âˆ’1
1
2

âˆ’o(1)

.
(3.5.8)
Proof
By Lemma 3.5.2, for any Î» âˆˆ[0,1/2], the number of non-zero binary (2m)-
strings of weight â‰¤2mÎ» is
â‰¤
âˆ‘
1â‰¤iâ‰¤2mÎ»
2m
i

â‰¤22mÎ·(Î»).

332
Further Topics from Coding Theory
Discarding these lightweight strings, the total weight is
â‰¥2mÎ»

N(1âˆ’2R0)âˆ’22mÎ·(Î»)
= 2mNÎ»(1âˆ’2R0)

1âˆ’
2mÎ·(Î»)
N(1âˆ’2R0)

.
Select Î»m = Î·âˆ’1
1
2 âˆ’
1
log(2m)

âˆˆ(0,1/2). Then Î»m â†’Î·âˆ’1(1/2), as hâˆ’1 is
continuous on [0,1/2]. So,
Î»m = Î·âˆ’1
1
2 âˆ’
1
log(2m)

= Î·âˆ’1
1
2

âˆ’o(1).
Since N = 2m âˆ’1, we have that as m â†’âˆ, N â†’âˆ, and
2mÎ·(Î»)
N(1âˆ’2R0) =
1
1âˆ’2R0
2mâˆ’2m/log(2m)
2m âˆ’1
=
1
1âˆ’2R0
2m
2m âˆ’1
1
22m/log(2m) â†’0.
So the total weight of the N(1âˆ’2R0) distinct (2m)-strings is bounded below by
2mN(1âˆ’2R0)

Î·âˆ’1(1/2)âˆ’o(1)

(1âˆ’o(1)) = 2mN(1âˆ’2R0)

Î·âˆ’1(1/2)âˆ’o(1)

.
Thus the result follows.
Lemma 3.5.4 demonstrates that X Ju
m,k has
w

X Ju
m,k

â‰¥2mN(1âˆ’2R0)

Î·âˆ’1
1
2

âˆ’o(1)

.
(3.5.9)
Then
w

X Ju
m,k

length

X Ju
m,k
 â‰¥(1âˆ’2R0)(Î·âˆ’1(1/2)âˆ’o(1)) â†’(1âˆ’2R0)Î·âˆ’1(1/2)
â‰ˆc(1âˆ’2R0) > 0.
So, the sequence X Ju
m,k with k = [2NR0], N = 2m âˆ’1 and a ï¬xed 0 < R0 < 1/2, has
information rate â‰¥R0 > 0 and
w

X Ju
m,k

length

X Ju
m,k
 â†’c(1âˆ’2R0) > 0, c = Î·âˆ’1(1/2) > 0.3.
(3.5.10)
In the construction, R0 âˆˆ(0,1/2). However, by truncating one can achieve any
given rate R0 âˆˆ(0,1); see [110].
The next family of codes to be discussed in this section is formed by alternant
codes. Alternant codes are a generalisation of BCH (though in general not cyclic).
Like Justesen codes, alternant codes also form an asymptotically good family.

3.5 Asymptotically good codes
333
Let M be a (r Ã—n) matrix over ï¬eld Fqm:
M =
â›
âœ
â
c11
...
c1n
...
...
...
cr1
...
crn
â
âŸ
â .
As before, each ci j can be written as âˆ’â†’
ci j âˆˆ(Fq)m, a column vector of length m over
Fq. That is, we can think of M as an (mrÃ—n) matrix over Fq (denoted again by M).
Given elements a1,...,an âˆˆFqm, we have
M
â›
âœ
â
a1
...
an
â
âŸ
â =
â›
âœ
â
c11
...
c1n
...
...
...
cr1
...
crn
â
âŸ
â 
â›
âœ
â
a1
...
an
â
âŸ
â =
â›
âœ
âœ
âœ
â
âˆ‘
1â‰¤jâ‰¤n
a jcij
...
âˆ‘
1â‰¤jâ‰¤n
a jcr j
â
âŸ
âŸ
âŸ
â .
Furthermore, if b âˆˆFq and c,d âˆˆFqm then bâˆ’â†’c = âˆ’â†’
bc and âˆ’â†’c +âˆ’â†’d = (âˆ’âˆ’â†’
c+d). Thus,
if a1,...,an âˆˆFq, then
M
â›
âœ
â
a1
...
an
â
âŸ
â =
â›
âœ
â
âˆ’â†’
c11
...
âˆ’â†’
c1n
...
...
...
âˆ’â†’
cr1
...
âˆ’â†’
crn
â
âŸ
â 
â›
âœ
â
a1
...
an
â
âŸ
â =
â›
âœ
âœ
âœ
â
âˆ‘
1â‰¤jâ‰¤n
âˆ’âˆ’â†’
aicij
...
âˆ‘
1â‰¤jâ‰¤n
âˆ’âˆ’â†’
aicr j
â
âŸ
âŸ
âŸ
â .
So, if the columns of M are linearly independent as r-vectors over Fqm, they are
also linearly independent as (rm)-vectors over Fq. That is, the columns of M are
linearly independent over Fq.
Recall that if Ï‰ is a primitive (n,Fqm) root of unity and Î´ â‰¥2 then the nÃ—(mÎ´)
Vandermonde matrix over Fq
HT =
â›
âœ
âœ
âœ
âœ
â
âˆ’â†’e
âˆ’â†’e
...
âˆ’â†’e
âˆ’â†’
Ï‰
âˆ’â†’
Ï‰ 2
...
âˆ’â†’
Ï‰ Î´âˆ’1
âˆ’â†’
Ï‰ 2
âˆ’â†’
Ï‰ 4
...
âˆ’â†’
Ï‰ 2(Î´âˆ’1)
...
âˆ’â†’
Ï‰ nâˆ’1
âˆ’â†’
Ï‰ 2(nâˆ’1)
...
âˆ’â†’
Ï‰ (Î´âˆ’1)(nâˆ’1)
â
âŸ
âŸ
âŸ
âŸ
â 
checks a narrow-sense BCH code X BCH
q,n,Ï‰,Î´ (a proper parity-check matrix emerges
after column purging). Generalise it by taking an nÃ—r matrix over Fqm
A =
â›
âœ
âœ
âœ
â
h1
h1Î±1
...
h1Î±râˆ’2
1
h1Î±râˆ’1
1
h2
h2Î±2
...
h2Î±râˆ’2
2
h2Î±râˆ’1
2
...
...
...
...
hn
hnÎ±n
...
hnÎ±râˆ’2
n
hnÎ±râˆ’1
n
â
âŸ
âŸ
âŸ
â ,
(3.5.11)

334
Further Topics from Coding Theory
or its nÃ—(mr) version over Fq:
âˆ’â†’
A =
â›
âœ
âœ
âœ
âœ
â
âˆ’â†’
h1
âˆ’âˆ’â†’
h1Î±1
...
âˆ’âˆ’â†’
h1Î±1râˆ’2
âˆ’âˆ’â†’
h1Î±1râˆ’1
âˆ’â†’
h2
âˆ’âˆ’â†’
h2Î±2
...
âˆ’âˆ’â†’
h2Î±2râˆ’2
âˆ’âˆ’â†’
h2Î±2râˆ’1
...
...
...
...
âˆ’â†’
hn
âˆ’âˆ’â†’
hnÎ±n
...
âˆ’âˆ’â†’
hnÎ±nrâˆ’2
âˆ’âˆ’â†’
hnÎ±nrâˆ’1
â
âŸ
âŸ
âŸ
âŸ
â 
.
(3.5.12)
Here r < n, h1,...,hn are non-zero elements and Î±1,...,Î±n are distinct elements
from Fq.
Note that any r rows of A in (3.5.11) form a square sub-matrix K that is similar
to Vandermondeâ€™s. It has a non-zero determinant and hence any r rows of A are lin-
early independent over Fqm and hence over Fq. Also the columns of A in (3.5.11)
are linearly independent over Fqm. However, columns of âˆ’â†’
A in (3.5.12) can be lin-
early dependent and purging such columns may be required to produce a â€˜genuineâ€™
parity-check matrix H.
Deï¬nition 3.5.5
Let Î± = (Î±1,...,Î±n) and h = (h1,...,hn) where Î±1,...,Î±n are
distinct and h1,...,hn non-zero elements of Fqm. Given r < n, an alternant code
X Alt
Î±,h is the kernel of the nÃ—(rm) matrix A in (3.5.12).
Theorem 3.5.6
X Alt
Î±,h has length n, rank k satisfying n âˆ’mr â‰¤k â‰¤n âˆ’r and
minimum distance d

X Alt
Î±,h

â‰¥r +1.
We see that the alternant codes are indeed generalisations of BCH. The main
outcome of the theory of alternant codes is the following Theorem 3.5.7 (not to be
proven here).
Theorem 3.5.7
There exist arbitrarily long alternant codes X Alt
Î±,h meeting the
Gilbertâ€“Varshamov bound.
So, alternant codes are asymptotically good. More precisely, a sequence of
asymptotically good alternant codes is formed by the so-called Goppa codes. See
below.
The Goppa codes are particular examples of alternant codes. They were invented
by a Russian coding theorist, Valery Goppa, in 1972 by following an elegant idea
that has its origin in algebraic geometry. Here, we perform the construction by
using methods developed in this section.
Let G(X) âˆˆFqm[X] be a polynomial over Fqm and consider Fqm[X]/âŸ¨G(X)âŸ©, the
polynomial ring modG(X) over Fqm. Then Fqm[X]/âŸ¨G(X)âŸ©is a ï¬eld iff G(X) is
irreducible. But if, for a given Î± âˆˆFqm, G(Î±) Ì¸= 0, the linear polynomial X âˆ’Î± is
invertible in Fqm[X]/âŸ¨G(X)âŸ©. In fact, write
G(X) = q(X)(X âˆ’Î±)+G(Î±),
(3.5.13)

3.5 Asymptotically good codes
335
with q(X)âˆˆFq[X], degq(X)= degG(X)âˆ’1.
So, q(X)(X âˆ’Î±) = âˆ’G(Î±) mod G(X) or
(âˆ’G(Î±)âˆ’1q(X))(X âˆ’Î±) = e modG(X)
and
(X âˆ’Î±)âˆ’1 = (âˆ’G(Î±)âˆ’1q(X)) modG(X).
(3.5.14a)
As q(X) = (G(X)âˆ’G(Î±))(X âˆ’Î±)âˆ’1, we have that
(X âˆ’Î±)âˆ’1 = âˆ’(G(X)âˆ’G(Î±))(X âˆ’Î±)âˆ’1G(Î±)âˆ’1 modG(X).
(3.5.14b)
So we deï¬ne (X âˆ’Î±)âˆ’1 as a polynomial in Fqm[X]/âŸ¨G(X)âŸ©given by (3.5.14a).
Deï¬nition 3.5.8
Fix a polynomial G(X) âˆˆFq[X] and a set Î± = {Î±1,..., Î±n} of
distinct elements of Fqm, qm â‰¥n > degG(X), where G(Î±j) = 0, 1 â‰¤j â‰¤n. Given
a word b = b1 ...bn with bi âˆˆFq,1 â‰¤i â‰¤n, set
Rb(X) = âˆ‘
1â‰¤iâ‰¤n
bi(X âˆ’Î±i)âˆ’1 âˆˆFqm[X]/âŸ¨G(X)âŸ©.
(3.5.15)
The q-ary Goppa code X Go (= X Go
Î±,G) is deï¬ned as the set
{b âˆˆFn
q : Rb(X) = 0 modG(X)}.
(3.5.16)
Clearly, X Go
Î±,G is a linear code. The polynomial G(X) is called the Goppa polyno-
mial; if G(X) is irreducible, we say that X Go is irreducible.
So, b = b1 ...bn âˆˆX Go iff in Fqm[X]
âˆ‘
1â‰¤iâ‰¤n
bi(G(X)âˆ’G(Î±i))(X âˆ’Î±i)âˆ’1G(Î±i)âˆ’1 = 0.
(3.5.17)
Write G(X) =
âˆ‘
0â‰¤iâ‰¤r
giXi where degG(X) = r, gr = 1 and r < n. Then in Fqm[X]
(G(X)âˆ’G(Î±i))(X âˆ’Î±i)âˆ’1
=
âˆ‘
0â‰¤jâ‰¤r
g j

X j âˆ’Î± j
i

(X âˆ’Î±i)âˆ’1
= âˆ‘0â‰¤jâ‰¤r g j âˆ‘0â‰¤uâ‰¤jâˆ’1 XuÎ± jâˆ’1âˆ’u
i

336
Further Topics from Coding Theory
and so
âˆ‘
1â‰¤iâ‰¤n
bi(G(X)âˆ’G(Î±i))(X âˆ’Î±i)âˆ’1G(Î±i)âˆ’1
=
âˆ‘
1â‰¤iâ‰¤n
bi
âˆ‘
0â‰¤jâ‰¤r
g j
âˆ‘
0â‰¤uâ‰¤jâˆ’1
Î± jâˆ’1âˆ’u
i
XuG(Î± j)âˆ’1
=
âˆ‘
0â‰¤uâ‰¤râˆ’1
Xu
âˆ‘
1â‰¤iâ‰¤n
biG(Î±i)âˆ’1
âˆ‘
u+1â‰¤jâ‰¤r
g jÎ± jâˆ’1âˆ’u
i
.
Hence, b âˆˆX Go iff in Fqm
âˆ‘
1â‰¤iâ‰¤n
biG(Î±i)âˆ’1
âˆ‘
u+1â‰¤jâ‰¤r
g jÎ± jâˆ’1âˆ’u
i
= 0
(3.5.18)
for all u = 0,...,r âˆ’1.
Equation (3.5.18) leads to the parity-check matrix for X Go. First, we see that
the matrix
â›
âœ
âœ
âœ
âœ
âœ
â
G(Î±1)âˆ’1
G(Î±2)âˆ’1
...
G(Î±n)âˆ’1
Î±1G(Î±1)âˆ’1
Î±2G(Î±2)âˆ’1
...
Î±nG(Î±n)âˆ’1
Î±2
1G(Î±1)âˆ’1
Î±2
2G(Î±2)âˆ’1
...
Î±2
nG(Î±n)âˆ’1
...
...
...
...
Î±râˆ’1
1
G(Î±1)âˆ’1
Î±râˆ’1
2
G(Î±2)âˆ’1
...
Î±râˆ’1
n
G(Î±n)âˆ’1
â
âŸ
âŸ
âŸ
âŸ
âŸ
â 
,
(3.5.19)
which is (n Ã— r) over Fm
q , provides a parity-check. As before, any r rows of ma-
trix (3.5.19) are linearly independent over Fqm and so are its columns. Then again
we write (3.5.19) as an n Ã— (mr) matrix over Fq; after purging linearly dependent
columns it will give the parity-check matrix H.
We see that X Go is an alternant code X Alt
Î±,h where Î± = (Î±1,...,Î±n) and h =
(G(Î±1)âˆ’1,...,G(Î±n)âˆ’1). So, Theorem 3.5.6 implies
Theorem 3.5.9
The q-ary Goppa code X = X Go
Î±,G, where Î± = {Î±1,...,Î±n} and
degG(X) = r < n, has length n, rank k satisfying nâˆ’mr â‰¤k â‰¤nâˆ’r and minimum
distance d(X ) â‰¥r +1.
As before, the above bound on minimum distance can be improved in the binary
case. Suppose that a binary word b = b1 ...bn âˆˆX where X is a Goppa code
X Go
Î±,G, where Î± âŠ‚F2m and G(X) âˆˆF2[X]. Suppose w(b) = w and bi1 = Â·Â·Â· = biw = 1.
Take fb(X) =
âˆ
1â‰¤jâ‰¤w
(X âˆ’Î±i j) and write the derivative âˆ‚X fb(X) as
âˆ‚X fb(X) = Rb(X) fb(X)
(3.5.20)
where Rb(X) =
âˆ‘
1â‰¤jâ‰¤w

X âˆ’Î±i j
âˆ’1 (cf. (3.5.15)). As polynomials fb(X) and Rb(X)
have no common roots in any extension F2K, they are co-prime. Then b âˆˆX Go

3.5 Asymptotically good codes
337
iff G(X) divides Rb(X) which is the same as G(X) divides âˆ‚X fb(X). For q = 2,
âˆ‚X fb(X) has only even powers of X (as its monomials are of the form â„“Xâ„“âˆ’1 times
a product of some Î±i jâ€™s: this vanishes when â„“is even). In other words, âˆ‚X fb =
h(X2) = (h(X))2 for some polynomial h(X). Hence if g(X) is the polynomial of
lowest degree which is a square and divisible by G(X) then G(X) divides âˆ‚X fb(X)
iff g(X) divides âˆ‚X fb(X). So,
b âˆˆX Go â‡”g(X)|âˆ‚X fb(X) â‡”Rb(X) = 0 modg(X).
(3.5.21)
Theorem 3.5.10
Let X be a binary Goppa code X Go
Î±,G. If g(X) is a polynomial
of the lowest degree which is a square and divisible by G(X) then X = X Go
Î±,g.
Hence, d(X Go) â‰¥degg(X)+1.
Corollary 3.5.11
Suppose that the Goppa polynomial G(X) âˆˆF2[X] has no mul-
tiple roots in any extension ï¬eld. Then X Go
Î±,G = X Go
Î±,G2, and the minimum distance
d

X Go
Î±,G

is â‰¥2degG(X)+1. Therefore, X Go
Î±,G can correct â‰¥degG(X) errors.
A binary Goppa code X Go
Î±,G where polynomial G(X) has no multiple roots is
called separable.
It is interesting to discuss a particular decoding procedure applicable for alter-
nant codes and based on the Euclid algorithm; cf. Section 2.5.
The initial setup for decoding an alternant code X Alt
Î±,h over Fq is as follows. As
in (3.5.12), we take the nÃ—(mr) matrix âˆ’â†’
A =
âˆ’âˆ’â†’
h jÎ± jiâˆ’1
over Fq obtained from the
nÃ—r matrix A =

h jÎ±iâˆ’1
j

over Fqm by replacing the entries with rows of length m.
Then purge linearly dependent columns from âˆ’â†’
A . Recall that h1,...,hn are non-zero
and Î±1,...,Î±n are distinct elements of Fqm. Suppose a word u = c+e is received,
where c is the right codeword and e an error vector. We assume that r is even and
that t â‰¤r/2 errors have occurred, at digits 1 â‰¤i1 < Â·Â·Â· < it â‰¤n. Let the ijth entry
of e be ei j Ì¸= 0. It is convenient to identify the error locators with elements Î±i j: as
Î±i Ì¸= Î±iâ€² for i Ì¸= iâ€² (the Î±i are distinct), we will know the erroneous positions if we
determine Î±i1,...,Î±it. Moreover, if we introduce the error locator polynomial
â„“(X) =
t
âˆ
j=1
(1âˆ’Î±i jX) = âˆ‘
0â‰¤iâ‰¤t
â„“iXi,
(3.5.22)
with â„“0 = 1 and the roots Î±âˆ’1
i j , then it will be enough to ï¬nd â„“(X) (i.e. coefï¬-
cients â„“i).
We have to calculate the syndrome (we will call it an A-syndrome) emerging by
acting on matrix A:
uA = eA = 0...0ei1 ...eit0...0A.

338
Further Topics from Coding Theory
Suppose the A-syndrome is s = s0 ...srâˆ’1, with s(X) =
âˆ‘
0â‰¤iâ‰¤râˆ’1
siXi. It is convenient
to introduce the error evaluator polynomial Îµ(X), by
Îµ(X) = âˆ‘
1â‰¤kâ‰¤t
hikeik
âˆ
1â‰¤jâ‰¤t: jÌ¸=k
(1âˆ’Î±i jX).
(3.5.23)
Lemma 3.5.12
For all u = 1,...,t,
ei j =
Îµ(Î±âˆ’1
i j )
hi j
âˆ
1â‰¤ jâ‰¤t,  jÌ¸= j
(1âˆ’Î±i jÎ±âˆ’1
i j ).
(3.5.24)
Proof
Straightforward.
The crucial fact is that â„“(X), Îµ(X) and s(X) are related by
Lemma 3.5.13
The following formula holds true:
Îµ(X) = â„“(X)s(X) modXr.
(3.5.25)
Proof
Write the following sequence:
Îµ(X)âˆ’â„“(X)s(X) = âˆ‘
1â‰¤kâ‰¤t
hikeik
âˆ
1â‰¤jâ‰¤t: jÌ¸=k
(1âˆ’Î±i jX)âˆ’â„“(X) âˆ‘
0â‰¤lâ‰¤râˆ’1
slXl
= âˆ‘
k
hikeik
âˆ
1â‰¤jâ‰¤t: jÌ¸=k
(1âˆ’Î±i jX)âˆ’â„“(X)âˆ‘
l âˆ‘
1â‰¤kâ‰¤t
hikÎ±l
ikeikXl
= âˆ‘
k
hikeik âˆ
jÌ¸=k
(1âˆ’Î±i jX)âˆ’â„“(X)âˆ‘
k
hikeikâˆ‘
l
Î±l
ikXl
= âˆ‘
k
hikeik

âˆ
jÌ¸=k
(1âˆ’Î±i jX)âˆ’â„“(X)âˆ‘
l
Î±l
ikXl

= âˆ‘
k
hikeik âˆ
jÌ¸=k
(1âˆ’Î±i jX)(1âˆ’(1âˆ’eikX)âˆ‘
l
Î±l
ikXl)
= âˆ‘
k
hikeik âˆ
jÌ¸=k
(1âˆ’Î±i jX)

1âˆ’(1âˆ’Î±ikX)1âˆ’Î±r
ikXr
1âˆ’Î±ikX

= âˆ‘
k
hikÎ±ik âˆ
jÌ¸=k
(1âˆ’Î±i jX) Î±r
ikXr = 0 modXr.
Lemma 3.5.13 shows the way of decoding alternant codes. We know that there
exists a polynomial q(X) such that
Îµ(X) = q(X)Xr +â„“(X)s(X).
(3.5.26)
We also have degÎµ(X) â‰¤t âˆ’1 < r/2, degâ„“(X) = t â‰¤r/2 and that Îµ(X) and â„“(X)
are co-prime as they have no common roots in any extension. Suppose we apply the

3.5 Asymptotically good codes
339
Euclid algorithm to the known polynomials f(X) = Xr and g(X) = s(X) with the
aim to ï¬nd Îµ(X) and â„“(X). By Lemma 2.5.44, a typical step produces a remainder
rk(X) = ak(X)Xr +bk(X)s(X).
(3.5.27)
If we want rk(X) and bk(X) to give Îµ(X) and â„“(X), their degrees must match:
at least we must have degrk(X) < r/2 and degbk(X) â‰¤r/2. So, the algorithm is
repeated until degrkâˆ’1(X) â‰¥r/2 and degrk(X) < r/2. Then, according to Lemma
2.5.44, statement (3), degbk(X) = degXr âˆ’degrkâˆ’1(X) â‰¤r âˆ’r/2 = r/2. This is
possible as the algorithm can be iterated until rk(X) = gcd(Xr,s(X)). But then
rk(X)|Îµ(X) and hence degrk(X) â‰¤degÎµ(X) < r/2. So we can assume degrk(X) â‰¤
r/2, degbk(X) â‰¤r/2.
The relevant equations are
Îµ(X) = q(X)Xr +â„“(X)s(X),
degÎµ(X) < r/2, degâ„“(X) â‰¤r/2,
gcd (Îµ(X),â„“(X)) = 1,
and also
rk(X) = ak(X)Xr +bk(X)s(X), degrk(X) < r/2, degbk(X) â‰¤r/2.
We want to show that polynomials rk(X) and bk(X) are scalar multiples of Îµ(X)
and â„“(X). Exclude s(X) to get
bk(X)Îµ(X)âˆ’rk(X)â„“(X) = (bk(X)q(X)âˆ’ak(X)â„“(X))Xr.
As
degbk(X)Îµ(X) = degbk(X)+degÎµ(X) < r/2+r/2 = r
and
degrk(X)â„“(X) = degrk(X)+degâ„“(X) < r/2+r/2 = r,
deg(b(X)Îµ(X)âˆ’rk(X)â„“(X)) < r. Hence, bk(X)Îµ(X)âˆ’rk(X)â„“(X) must be 0, i.e.
â„“(X)rk(X) = Îµ(X)bk(X), bk(X)q(X) = ak(X)â„“(X).
So, â„“(X)|Îµ(X)bk(X) and bk(X)|ak(X)â„“(X). But â„“(X) and Îµ(X) are co-primes as
well as ak(X) and bk(X) (by statement (5) of Lemma 2.5.44). Therefore, â„“(X) =
Î»bk(X) and hence Îµ(X) = Î»rk(X). As l(0) = 1, Î» = bk(0)âˆ’1.
To summarise:
Theorem 3.5.14
(The decoding algorithm for alternant codes) Suppose X Alt
Î±,h is
an alternant code, with even r, and that t â‰¤r/2 errors occurred in a received word
u. Then, upon receiving word u:

340
Further Topics from Coding Theory
(a) Find the A-syndrome uA = s0 ...srâˆ’1, with the corresponding polynomial
s(X) = âˆ‘l slXl.
(b) Use the Euclid algorithm, beginning with f(X) = Xr, g(X) = s(X), to obtain
rk(X) = ak(X)Xr +bk(X)s(X) with degrkâˆ’1(X) â‰¥r/2 and degrk(X) < r/2.
(c) Set â„“(X) = bk(0)âˆ’1bk(X), Îµ(X) = bk(0)âˆ’1rk(X).
Then â„“(X) is the error locator polynomial whose roots are the inverses of
Î±i1,...,yt = Î±it, and i1,...,it are the error digits. The values ei j are given by
ei j =
Îµ(Î±âˆ’1
i j )
hi j âˆlÌ¸= j(1âˆ’Î±ilÎ±âˆ’1
i j ).
(3.5.28)
The ideas discussed in this section found a far-reaching development in
algebraic-geometric codes. Algebraic geometry provided powerful tools in modern
code design; see [98], [99], [158], [160].
3.6 Additional problems for Chapter 3
Problem 3.1
Deï¬ne Reedâ€“Solomon codes and prove that they are maximum dis-
tance separable. Prove that the dual of a Reedâ€“Solomon code is a Reedâ€“Solomon
code.
Find the minimum distance of a Reedâ€“Solomon code of length 15 and rank 11
and the generator polynomial g1(X) over F16 for this code. Use the provided F16
ï¬eld table to write g1(X) in the form Ï‰i0 + Ï‰i1X + Ï‰i2X2 + Â·Â·Â·, identifying each
coefï¬cient as a single power of a primitive element Ï‰ of F16.
Find the generator polynomial g2(X) and the minimum distance of a Reedâ€“
Solomon code of length 10 and rank 6. Use the provided F11 ï¬eld table to write
g2(X) in the form a0 +a1X +a2X2 +Â·Â·Â·, where each coefï¬cient is a number from
{0,1,...,10}.
Determine a two-error correcting Reedâ€“Solomon code over F16 and ï¬nd its
length, rank and generator polynomial.
The ï¬eld table for F11 = {0,1,2,3,4,5,6,7,8,9,10}, with addition and multipli-
cation mod 11:
i
0
1
2
3
4
5
6
7
8
9
Ï‰i
1
2
4
8
5
10
9
7
3
6

3.6 Additional problems for Chapter 3
341
The ï¬eld table for F16 = F4
2:
i
0
1
2
3
4
5
6
7
8
Ï‰i
0001
0010
0100
1000
0011
0110
1100
1011
0101
i
9
10
11
12
13
14
Ï‰i
1010
0111
1110
1111
1101
1001
Solution A q-ary RS code X RS of designed distance Î´ â‰¤q âˆ’1 is deï¬ned as a
cyclic code of length N = qâˆ’1 over Fq, with a generator polynomial
g(X) = (X âˆ’Ï‰b)(X âˆ’Ï‰b+1)...(X âˆ’Ï‰b+Î´âˆ’2)
of deg(g(X)) = Î´ âˆ’1. Here Ï‰ is a primitive (qâˆ’1,Fq) root of unity (i.e. a primitive
element of Fâˆ—
q) and b = 0,1,...,q âˆ’2. The powers Ï‰b,...,Ï‰b+Î´âˆ’2 are called the
(deï¬ning) zeros and the remaining N âˆ’Î´ +1 powers of Ï‰ non-zeros of X RS.
The rank of X RS equals k = N âˆ’Î´ + 1. The distance is â‰¥Î´ = N âˆ’k + 1, but
by the Singleton bound should be â‰¤Î´ = N âˆ’k + 1. So, the distance equals Î´ =
N âˆ’k +1, i.e. X RS is maximum distance separable.
The dual (X RS)âŠ¥of X RS is cyclic and its zeros are the inverses of the non-
zeroes of X RS:
Ï‰qâˆ’1âˆ’j = (Ï‰ j)âˆ’1, j Ì¸= b,...,b+Î´ âˆ’2.
That is, they are
Ï‰qâˆ’b,Ï‰qâˆ’b+1,...,Ï‰qâˆ’b+qâˆ’Î´âˆ’1
and the generator polynomial gâŠ¥(X) for (X RS)âŠ¥is
gâŠ¥(X) = (X âˆ’Ï‰bâŠ¥)(X âˆ’Ï‰bâŠ¥+1)...(X âˆ’Ï‰bâŠ¥+qâˆ’Î´âˆ’1)
where bâŠ¥= qâˆ’b. That is (X RS)âŠ¥is an RS code of designed distance qâˆ’Î´ +1.
In the example, length 15 means q = 15 + 1 = 16 and rank 11 yields distance
Î´ = 15âˆ’11+1 = 5. The generator g1(X) over F16 = F4
2, for the code with b = 1,
reads
g1(X) = (X âˆ’Ï‰)(X âˆ’Ï‰2)(X âˆ’Ï‰3)(X âˆ’Ï‰4)
= X4 âˆ’(Ï‰ +Ï‰2 +Ï‰3 +Ï‰4)X3
+(Ï‰3 +Ï‰4 +Ï‰5 +Ï‰5 +Ï‰6 +Ï‰7)X2
âˆ’(Ï‰6 +Ï‰7 +Ï‰8 +Ï‰9)X +Ï‰10
= X4 +Ï‰13X3 +Ï‰6X2 +Ï‰3X +Ï‰10
where the calculation is accomplished by using the F16 ï¬eld table.

342
Further Topics from Coding Theory
Similarly, length 10 means q = 11 and rank 6 yields distance Î´ = 10âˆ’6+1 = 5.
The generator g2(X) is over F11 and, again for b = 1, reads
g2(X) = (X âˆ’Ï‰)(X âˆ’Ï‰2)(X âˆ’Ï‰3)(X âˆ’Ï‰4)
= X4 âˆ’(Ï‰ +Ï‰2 +Ï‰3 +Ï‰4)X3
+(Ï‰3 +Ï‰4 +Ï‰5 +Ï‰5 +Ï‰6 +Ï‰7)X2
âˆ’(Ï‰6 +Ï‰7 +Ï‰8 +Ï‰9)X +Ï‰10
= X4 +3X3 +5X2 +8X +1
where the calculation is accomplished by using the F11-ï¬eld table.
Finally, a two-error correcting RS code over F16 must have length N = 15 and
distance Î´ = 5, hence rank 11. So, it coincides with the above 16-ary [15,11] RS
code.
Problem 3.2
Let X be a binary linear [N,k] code and X ev be the set of words
x âˆˆX of even weight. Prove that either
(i) X = X ev or
(ii) X ev is an [N,k âˆ’1] linear subcode of X .
Prove that if the generating matrix G of X has no zero column then the total weight
âˆ‘
xâˆˆX
w(x) equals N2kâˆ’1.
[Hint: Consider the contribution from each column of G.]
Denote by XH,â„“the binary Hamming code of length N = 2â„“âˆ’1 and by X âŠ¥
H,â„“
the dual simplex code, â„“= 3,4,.... Is it always true that the N-vector 1...1 (with
all digits one) is a codeword in XH,â„“? Let As and AâŠ¥
s denote the number of words
of weight s in XH,â„“and X âŠ¥
H,â„“, respectively, with A0 = AâŠ¥
0 = 1 and A1 = A2 = 0.
Check that
A3 = N(N âˆ’1)

3!, A4 = N(N âˆ’1)(N âˆ’3)

4!,
and
A5 = N(N âˆ’1)(N âˆ’3)(N âˆ’7)

5!.
Prove that AâŠ¥
2â„“âˆ’1 = 2â„“âˆ’1 (i.e. all non-zero words x âˆˆX âŠ¥
H,â„“have weight 2â„“âˆ’1). By
using the last fact and the MacWilliams identity for binary codes, give a formula
for As in terms of Ks(2â„“âˆ’1), the value of the Kravchuk polynomial:
Ks(2â„“âˆ’1) =
sâˆ§2â„“âˆ’1
âˆ‘
j=0âˆ¨s+2â„“âˆ’1âˆ’2â„“+1
2â„“âˆ’1
j
 2â„“âˆ’1âˆ’2â„“âˆ’1
sâˆ’j

(âˆ’1) j.
Here 0âˆ¨s+2â„“âˆ’1 âˆ’2â„“+1 = max [0,s+2â„“âˆ’1 âˆ’2â„“+1] and sâˆ§2â„“âˆ’1 = min[s,2â„“âˆ’1].
Check that your formula gives the right answer for s = N = 2â„“âˆ’1.

3.6 Additional problems for Chapter 3
343
Solution X ev is always a linear subcode of X . In fact, for binary words x and xâ€²,
w(x + xâ€²) = w(x) + w(xâ€²) âˆ’2w(x âˆ§xâ€²), where digit (x âˆ§xâ€²) j = xjxâ€²
j = min[xi,xâ€²
i].
If both x and xâ€² are even words (have even weight) or odd (have odd weight) then
x+xâ€² is even, and if x is even and xâ€² odd then x+xâ€² is odd. So, if X ev Ì¸= X then
X ev is a subgroup in X of index [X ev : X ] two. Thus, there are two cosets, and
X ev is a half of X . So, X ev is [N,k âˆ’1] code.
Let g( j) = (g1 j,...,gk j)T be column j of G, the generating matrix of X . Set
W j = {i = 1,...,k : gi j = 1}, with â™¯W j = w(g( j)) = wj â‰¥1,1 â‰¤j â‰¤N. The contri-
bution into the sum âˆ‘xâˆˆX w(x) coming from g( j) equals
2kâˆ’w j Ã—2w jâˆ’1 = 2kâˆ’1.
Here 2kâˆ’w j represents the number of subsets of the complement {1,...,k}\Wj and
2w jâˆ’1 the number of odd subsets of W j. Multiplying by N (the number of columns)
gives N2kâˆ’1.
If H = HH,â„“is the parity-check matrix of the Hamming code XH,â„“then weight
of row j of H equals the number of digits one in position j in the binary decom-
position of numbers 1,...,2l âˆ’1,1 â‰¤j â‰¤l. So, w(h( j)) = 2lâˆ’1 (a half of numbers
1,...,2l âˆ’1 have zero in position j and a half one). Then for all j = 1,...,N, the
dot-product 1Â·h( j) = w(h( j)) mod 2 = 0, i.e. 1...1 âˆˆXH,â„“.
Now A3 = N(N âˆ’1)/3!, the number of linearly dependent triples of columns
of H (as the choice is made by ï¬xing two distinct columns: the third is their
sum). Next, A4 = N(N âˆ’1)(N âˆ’3)/4!, the number of linearly dependent 4-ples of
columns of H (as the choice is made by ï¬xing: (a) two arbitrary distinct columns,
(b) a third column that is not their sum, with (c) the fourth column being the sum
of the ï¬rst three), and similarly, A5 = N(N âˆ’1)(N âˆ’3)(N âˆ’7)/5!, the number of
linearly dependent 5-ples of columns of H. (Here N âˆ’7 indicates that while choos-
ing the fourth column we should avoid any of 23 âˆ’1 = 7 linear combinations of
the ï¬rst three.)
In fact, any non-zero word x in the dual code X âŠ¥
H,â„“has w(x) = 2lâˆ’1. To prove
this note that the generating matrix of X âŠ¥
H,â„“is H. So, write x as a sum of rows of
H, and let W be the set of rows of H contributing into this sum, with â™¯W = w â‰¤â„“.
Then w(x) equals the number of j among 1,2,...,2â„“âˆ’1 such that in the binary
decomposition j = 20 j0 +21 j1 +Â·Â·Â·+2â„“âˆ’1 jlâˆ’1 the sum âˆ‘tâˆˆW jt mod2 equals one.
As before, this is equal to 2wâˆ’1 (the number of subsets of W of odd cardinality).
So, w(x) = 2â„“âˆ’w+wâˆ’1 = 2â„“âˆ’1. Note that the rank of X âŠ¥
H,l is 2â„“âˆ’1âˆ’(2â„“âˆ’1âˆ’l) = l
and the size â™¯X âŠ¥
H,â„“= 2â„“.
The MacWilliams identity (in the reversed form) reads
As =
1
â™¯X âŠ¥
H,â„“
N
âˆ‘
i=1
AâŠ¥
i Ks(i)
(3.6.1)

344
Further Topics from Coding Theory
where
Ks(i) =
sâˆ§i
âˆ‘
j=0âˆ¨s+iâˆ’N
i
j
N âˆ’i
sâˆ’j

(âˆ’1) j,
(3.6.2)
with 0âˆ¨s+iâˆ’N = max[0,s+iâˆ’N], sâˆ§i = min[s,i]. In our case, AâŠ¥
0 = 1,AâŠ¥
2â„“âˆ’1 =
2â„“âˆ’1 (the number of the non-zero words in X âŠ¥
H,â„“). Thus,
As = 1
2â„“

1+(2â„“âˆ’1)Ks(2â„“âˆ’1)

= 1
2â„“

1+(2â„“âˆ’1)
sâˆ§2â„“âˆ’1
âˆ‘
j=0âˆ¨s+2â„“âˆ’1âˆ’2â„“+1
2â„“âˆ’1
j
2â„“âˆ’1âˆ’2â„“âˆ’1
2â„“âˆ’1âˆ’j

(âˆ’1) j

.
For s = N = 2â„“âˆ’1, A2â„“âˆ’1 can be either 1 (if the 2â„“-word 11...1 lies in XH,â„“) or 0
(if it doesnâ€™t). The last formula yields
A2â„“âˆ’1 = 1
2â„“

1+(2â„“âˆ’1)
2â„“âˆ’1
âˆ‘
j=2â„“âˆ’1
2â„“âˆ’1
j
2â„“âˆ’1âˆ’2â„“âˆ’1
2â„“âˆ’1âˆ’j

= 1
2â„“

1+2â„“âˆ’1

= 1,
which agrees with the fact that 11...1 âˆˆXH,â„“.
Problem 3.3
Let Ï‰ be a root of M(X) = X5 +X2 +1 in F32; given that M(X) is a
primitive polynomial for F32, Ï‰ is a primitive (31,F32)-root of unity. Use elements
Ï‰, Ï‰2, Ï‰3, Ï‰4 to construct a binary narrow-sense primitive BCH code X of length
31 and designed distance 5. Identify the cyclotomic coset {i,2i,...,2dâˆ’1i} for each
of Ï‰, Ï‰2, Ï‰3, Ï‰4. Check that Ï‰ and Ï‰3 sufï¬ce as deï¬ning zeros of X and that the
actual minimum distance of X equals 5. Show that the generator polynomial g(X)
for X is the product
(X5 +X2 +1)(X5 +X4 +X3 +X2 +1)
= X10 +X9 +X8 +X6 +X5 +X3 +1.
Suppose you received a word u(X) = X12 +X11 +X9 +X7 +X6 +X2 +1 from a
sender who uses code X . Check that u(Ï‰) = Ï‰3 and u(Ï‰3) = Ï‰9, argue that u(X)
should be decoded as
c(X) = X12 +X11 +X9 +X7 +X6 +X3 +X2 +1
and verify that c(X) is indeed a codeword in X .
The ï¬eld table for F32 = F5
2 and the list of irreducible polynomials of degree 5
over F2 are also provided to help with your calculations.

3.6 Additional problems for Chapter 3
345
The ï¬eld table for F32 = F5
2:
i
0
1
2
3
4
5
6
7
Ï‰i
00001
00010
00100
01000
10000
00101
01010
10100
i
8
9
10
11
12
13
14
15
Ï‰i
01101
11010
10001
00111
01110
11100
11101
11111
i
16
17
18
19
20
21
22
23
Ï‰i
11011
10011
00011
00110
01100
11000
10101
01111
i
24
25
26
27
28
29
30
Ï‰i
11110
11001
10111
01011
10110
01001
10010
The list of irreducible polynomials of degree 5 over F2:
X5 +X2 +1, X5 +X3 +1, X5 +X3 +X2 +X +1,
X5 +X4 +X3 +X +1, X5 +X4 +X3 +X2 +1;
they all have order 31. Polynomial X5 +X2 +1 is primitive.
Solution As M(X) = X5 +X2 +1 is a primitive polynomial in F2[X], any root Ï‰ of
M(X) is a primitive (31,F2)-root of unity, i.e. satisï¬es Ï‰31 + 1 = 0. Furthermore,
M(X) is the minimal polynomial for Ï‰.
The BCH code X under construction is a cyclic code whose generator is a
polynomial of smallest degree having Ï‰,Ï‰2,Ï‰3,Ï‰4 among its roots (i.e. a cyclic
code whose zeros form a minimal set including Ï‰,Ï‰2,Ï‰3,Ï‰4). Thus, the generator
polynomial g(X) of X is the lcm of the minimal polynomials for Ï‰,Ï‰2,Ï‰3,Ï‰4.
The cyclotomic coset for Ï‰ is C = {1,2,4,8,16}; hence
(X âˆ’Ï‰)(X âˆ’Ï‰2)(X âˆ’Ï‰4)(X âˆ’Ï‰8)(X âˆ’Ï‰16) = X5 +X2 +1
is the minimal polynomial for Ï‰,Ï‰2 and Ï‰4. The cyclotomic coset for Ï‰3 is C =
{3,6,12,24,17} and the minimal polynomial MÏ‰3(X) for Ï‰3 equals
MÏ‰3(X) = (X âˆ’Ï‰3)(X âˆ’Ï‰6)(X âˆ’Ï‰12)(X âˆ’Ï‰24)(X âˆ’Ï‰17)
= X5 +(Ï‰3 +Ï‰6 +Ï‰12 +Ï‰24 +Ï‰17)X4 +(Ï‰9 +Ï‰15 +Ï‰27
+Ï‰20 +Ï‰18 +Ï‰30 +Ï‰23 +Ï‰36 +Ï‰29 +Ï‰41)X3
+(Ï‰21 +Ï‰33 +Ï‰26 +Ï‰39 +Ï‰32 +Ï‰44 +Ï‰42 +Ï‰35
+Ï‰47 +Ï‰53)X2 +(Ï‰45 +Ï‰38 +Ï‰50 +Ï‰56 +Ï‰59)X +Ï‰62
= X5 +X4 +X3 +X2 +1
by a direct ï¬eld-table calculation or by inspecting the list of irreducible polynomi-
als over F2 of degree 5.

346
Further Topics from Coding Theory
So, Ï‰ and Ï‰3 sufï¬ce as zeros, and the generating polynomial g(X) equals
(X5 +X2 +1)(X5 +X4 +X3 +X2 +1)
= X10 +X9 +X8 +X6 +X5 +X3 +1,
as required. In other words:
X = {c(X) âˆˆF2[X]/(X31 +1) : c(Ï‰) = c(Ï‰3) = 0}
= {c(X) âˆˆF2[X]/(X31 +1) : g(X)|c(X)}.
The rank of X equals 21. The minimum distance of X equals 5, its designed
distance. This follows from Theorem 3.3.20:
Let N = 2â„“âˆ’1. If 2â„“E <
âˆ‘
0â‰¤iâ‰¤E+1
N
i

then the binary narrow-sense primitive BCH
code of designed distance 2E +1 has minimum distance 2E +1.
In fact, N = 31 = 25 âˆ’1 with â„“= 5 and E = 2, i.e. 2E +1 = 5, and
1024 = 210 < 1+31+ 31Ã—30
2
+ 31Ã—30Ã—29
2Ã—3
= 4992.
Thus, X corrects two errors. The Berlekampâ€“Massey decoding procedure re-
quires calculating the values of the received polynomial at the deï¬ning zeros. From
the F32 ï¬eld table we have
u(Ï‰) = Ï‰12 +Ï‰11 +Ï‰9 +Ï‰7 +Ï‰6 +Ï‰2 +1 = Ï‰3,
u(Ï‰3) = Ï‰36 +Ï‰33 +Ï‰27 +Ï‰18 +Ï‰6 +1 = Ï‰9.
So, u(Ï‰3) = u(Ï‰)3. As u(Ï‰) = Ï‰3, we conclude that a single error occurred, at
digit three, i.e. u(X) is decoded by
c(X) = X12 +X11 +X9 +X7 +X6 +X3 +X2 +1
which is (X2 +1)g(X) as required.
Problem 3.4
Deï¬ne the dual X âŠ¥of a linear [N,k] code of length N and dimen-
sion k with alphabet F. Prove or disprove that if X is a binary [N,(N âˆ’1)/2] code
with N odd then X âŠ¥is generated by a basis of X plus the word 1...1. Prove or
disprove that if a binary code X is self-dual, X = X âŠ¥, then N is even and the
word 1...1 belongs to X .
Prove that a binary self-dual linear [N,N/2] code X exists for each even N.
Conversely, prove that if a binary linear [N,k] code X is self-dual then k = N/2.
Give an example of a non-binary linear self-dual code.
Solution The dual X âŠ¥of the [N,k] linear code X is given by
X âŠ¥= {x = x1 ...xN âˆˆFN : xÂ·y = 0
for all y âˆˆX }

3.6 Additional problems for Chapter 3
347
where xÂ·y = x1y1 +Â·Â·Â·+xNyN. Take N = 5, k = (N âˆ’1)/2 = 2,
X =
â›
âœ
âœ
â
1
0
0
0
0
0
1
0
0
0
1
1
0
0
0
0
0
0
0
0
â
âŸ
âŸ
â .
Then X âŠ¥is generated by
â›
â
0
0
1
0
0
0
0
0
1
0
0
0
0
0
1
â
â .
None of the vectors from X belongs to X âŠ¥, so the claim is false.
Now take a self-dual code X = X âŠ¥. If the word 1 = 1...1 Ì¸âˆˆX then there
exists x âˆˆX such that xÂ·1 Ì¸= 0. But xÂ·1 = âˆ‘xi = w(x) mod2. On the other hand,
âˆ‘xi = xÂ·x, so xÂ·x Ì¸= 0. But then x Ì¸âˆˆX âŠ¥. Hence 1 âˆˆX . But then 1Â·1 = 0 which
implies that N is even.
Now let N = 2k. Divide digits 1,...,N into k disjoint pairs (Î±1,Î²1),...,(Î±k,Î²k),
with Î±i < Î²i. Then consider k binary words x(1),...,x(k) of length N and weight 2,
with the non-zero digits in the word x(i) in positions (Î±i,Î²i). Then form the [N,k]
code generated by x(1),...,x(k).
This code X is self-dual. In fact, x(i) Â· x(iâ€²) = 0 for all i,iâ€², hence X âŠ‚X âŠ¥.
Conversely, let y âˆˆX âŠ¥. Then y Â· x(i) = 0 for all i. This means that for all i, y
has either both 0 or both non-zero digits at positions (Î±i,Î²i). Then y âˆˆX . So,
X = X âŠ¥.
Now assume X = X âŠ¥. Then N is even. But the dimension must be k by the
rank-nullity theorem.
The non-binary linear self-dual code is the ternary Golay [12,6] with a generat-
ing matrix
G =
â›
âœ
âœ
âœ
âœ
âœ
âœ
âœ
â
1 0 0 0 0 0
0 1 1 1 1 1
0 1 0 0 0 0
1 0 1 2 2 1
0 0 1 0 0 0
1 1 0 1 2 2
0 0 0 1 0 0
1 2 1 0 1 2
0 0 0 0 1 0
1 2 2 1 0 1
0 0 0 0 0 1
1 1 2 2 1 0
â
âŸ
âŸ
âŸ
âŸ
âŸ
âŸ
âŸ
â 
Here rows of G are orthogonal (including self-orthogonal). Hence, X âŠ‚X âŠ¥.
But dim(X ) = dim(X âŠ¥) = 6, so X = X âŠ¥.
Problem 3.5
Deï¬ne a ï¬nite ï¬eld Fq with q elements and prove that q must have
the form q = ps where p is a prime integer and s â©¾1 a positive integer. Check that
p is the characteristic of Fq.

348
Further Topics from Coding Theory
Prove that for any p and s as above there exists a ï¬nite ï¬eld Fs
p with ps elements,
and this ï¬eld is unique up to isomorphism.
Prove that the set Fâˆ—
ps of the non-zero elements of Fps is a cyclic group Zpsâˆ’1.
Write the ï¬eld table for F9, identifying the powers Ï‰i of a primitive element
Ï‰ âˆˆF9 as vectors over F3. Indicate all vectors Î± in this table such that Î±4 = e.
Solution A ï¬eld Fq with q elements is a set of cardinality q with two commutative
group operations, + and Â·, with standard distributivity rules. It is easy to check that
char(Fq) = p is a prime number. Then Fp âŠ‚Fq and q = â™¯Fq = ps where s = [Fq : Fp]
is the dimension of Fq as a vector space over Fp, a ï¬eld of p elements.
Now, let Fâˆ—
q, the multiplicative group of non-zero elements from Fq, contain an
element of order qâˆ’1 = â™¯Fâˆ—
q. In fact, every b âˆˆFâˆ—
q has a ï¬nite order ord(b) = r(b);
set r0 = max[r(b) : b âˆˆFâˆ—
q]. and ï¬x a âˆˆFâˆ—
q with r(a) = r0. Then r(b)|r0 for all
b âˆˆFâˆ—
q. Next, pick Î³, a prime factor of r(b), and write r(b) = Î³sâ€²Ï‰,r0 = Î³sÎ±. Let us
check that s â‰¥sâ€². Indeed, aÎ³s has order Î±, bÏ‰ order Î³sâ€² and aÎ³sbÏ‰ order Î³sâ€²Î±. Thus,
if sâ€² > s, we obtain an element of order > r0. Hence, s â‰¥sâ€² which holds for any
prime factor of r(b), and r(b)|r(a).
Then br(a) = e, for all b âˆˆFâˆ—
q, i.e. the polynomial Xr0 âˆ’e is divisible by (X âˆ’b).
It must then be the product âˆbâˆˆFâˆ—q(X âˆ’b). Then r0 = â™¯Fâˆ—
q = q âˆ’1. Then Fâˆ—
q is a
cyclic group with generator a.
For each prime p and positive integer s there exists at most one ï¬eld Fq with
q = ps, up to isomorphism. Indeed, if Fq and Fâ€²
q are two such ï¬elds then they both
are isomorphic to Spl(Xq âˆ’X), the splitting ï¬eld of Xq âˆ’X (over Fp, the basic
ï¬eld).
The elements Î± of F9 = F3 Ã— F3 with Î±4 = e are e = 01, Ï‰2 = 1 + 2Ï‰ = 21,
Ï‰4 = 02, Ï‰6 = 2+Ï‰ = 12 where Ï‰ = 10.
Problem 3.6
Give the deï¬nition of a cyclic code of length N with alphabet Fq.
What are the deï¬ning zeros of a cyclic code and why are they always (N,Fq)-roots
of unity? Prove that the ternary Hamming
3s âˆ’1
2
, 3s âˆ’1
2
âˆ’s,3

code is equivalent
to a cyclic code and identify the deï¬ning zeros of this cyclic code.
A sender uses the ternary [13,10,3] Hamming code, with ï¬eld alphabet F3 =
{0,1,2} and the parity-check matrix H of the form
â›
â
1 0 1 2 0 1 2 0 1 2 0 1 2
0 1 1 1 0 0 0 1 1 1 2 2 2
0 0 0 0 1 1 1 1 1 1 1 1 1
â
â .
The receiver receives the word x = 2 1 2 0 1 1 0 0 2 1 1 2 0. How should he
decode it?

3.6 Additional problems for Chapter 3
349
Solution As g(X)|(XN âˆ’1), all roots of g(X) are Nth roots of unity. Let gcd(l,qâˆ’
1) = 1. We prove that the Hamming
ql âˆ’1
qâˆ’1 , ql âˆ’1
qâˆ’1 âˆ’l

code is equivalent to a
cyclic code, with deï¬ning zero Ï‰ = Î² qâˆ’1 where Î² is the primitive

ql âˆ’1

(qâˆ’1)-
root of unity. Indeed, set N = (ql âˆ’1)

(qâˆ’1). The splitting ï¬eld Spl(XN âˆ’1) = Fqr
where r = ordN(q) = min[s : N|(qs âˆ’1)]. Then r = l as

ql âˆ’1

(q âˆ’1)|(ql âˆ’1)
and l is the least such power. So, Spl(XN âˆ’1) = Fql.
If Î² is a primitive element is Fql then Ï‰ = Î²
qlâˆ’1
N
= Î² qâˆ’1 is a primitive Nth root
of unity in Fql. Write Ï‰0 = e,Ï‰,Ï‰2,...,Ï‰Nâˆ’1 as column vectors in Fq Ã— ...Ã— Fq
and form an l Ã—N check matrix H. We want to check that any two distinct columns
of H are linearly independent. This is done exactly as in Theorem 3.3.14.
Then the code with parity-check matrix H has distance â‰¥3, rank k â‰¥N âˆ’l. The
Hamming bound with N = (ql âˆ’1)/(qâˆ’1)
qk â‰¤qN

âˆ‘
0â‰¤mâ‰¤E
N
m

(qâˆ’1)m
âˆ’1
, with E =
Ad âˆ’1
2
B
,
(3.6.3)
shows that d = 3 and k = N âˆ’l. So, the cyclic code with the parity-check matrix H
is equivalent to Hammingâ€™s.
To decode the code in question, calculate the syndrome xHT = 2 0 2 = 2Â·(1 0 1)
indicating the error is in the 6th position. Hence, xâˆ’2e(6) = y+e(6) and the correct
word is c = 2 1 2 0 1 2 0 0 2 1 1 2 0.
Problem 3.7
Compute the rank and minimum distance of the cyclic code with
generator polynomial g(X) = X3+X +1 and parity-check polynomial h(X) = X4+
X2 + X + 1. Now let Ï‰ be a root of g(X) in the ï¬eld F8. We receive the word
r(X) = X5 +X3 +X(modX7 âˆ’1). Verify that r(Ï‰) = Ï‰4, and hence decode r(X)
using minimum-distance decoding.
Solution A cyclic code X of length N has generator polynomial g(X) âˆˆF2[X]
and parity-check polynomial h(X) âˆˆF2[X] with g(X)h(X) = XN âˆ’1. Recall that
if g(X) has degree k, i.e. g(X) = a0 + a1X + Â·Â·Â· + akXk where ak Ì¸= 0, then
g(X),Xg(X),...,Xnâˆ’kâˆ’1g(X) form a basis for X . In particular, the rank of X
equals N âˆ’k. In this question, k = 3 and rank(X ) = 4.

350
Further Topics from Coding Theory
If h(X) = b0 + b1X + Â·Â·Â· + bNâˆ’kXNâˆ’k then the parity-check matrix H of code
X is
â›
âœ
âœ
âœ
âœ
âœ
âœ
âœ
â
bNâˆ’k
bNâˆ’kâˆ’1
...
b1
b0
0
...
0
0
0
bNâˆ’k
bNâˆ’kâˆ’1
...
b1
b0
...
0
0
0
...
...
...
...
...
...
0
0
...
0
bNâˆ’k
bNâˆ’kâˆ’1
...
b1
b0
â
âŸ
âŸ
âŸ
âŸ
âŸ
âŸ
âŸ
â 
:
;<
=
.
N
The codewords of X are linear dependence relations between the columns of H.
The minimum distance d(X ) of a linear code X is the minimum non-zero weight
of a codeword.
In this question, N = 7, and
H =
â›
â
1
0
1
1
1
0
0
0
1
0
1
1
1
0
0
0
1
0
1
1
1
â
â .
no zero column
â‡’
no codewords of weight 1
no repeated column
â‡’
no codewords of weight 2
Hence, d(X ) = 3. In fact, X is equivalent to Hammingâ€™s original [7,4] code.
Since g(X) âˆˆF2[X] is irreducible, the code X âŠ‚F7
2 = F2[X]

(X7 âˆ’1) is the
cyclic code deï¬ned by Ï‰. The multiplicative cyclic group Fâˆ—
8 â‰ƒZÃ—
7 of non-zero
elements of ï¬eld F8 is
Ï‰0 = 1,
Ï‰,
Ï‰2,
Ï‰3 = Ï‰ +1,
Ï‰4 = Ï‰2 +Ï‰,
Ï‰5 = Ï‰3 +Ï‰2 = Ï‰2 +Ï‰ +1,
Ï‰6 = Ï‰3 +Ï‰2 +Ï‰ = Ï‰2 +1,
Ï‰7 = Ï‰3 +Ï‰ = 1.
Next, the value r(Ï‰) is
r(Ï‰) = Ï‰ +Ï‰3 +Ï‰5
= Ï‰ +(Ï‰ +1)+(Ï‰2 +Ï‰ +1)
= Ï‰2 +Ï‰
= Ï‰4,

3.6 Additional problems for Chapter 3
351
as required. Let c(X) = r(X)+X4 mod(X7âˆ’1). Then c(Ï‰) = 0, i.e. c(X) is a code-
word. Since d(X ) = 3 the code is 1-error correcting. We just found a codeword
c(X) at distance 1 from r(X). Then r(X) is written as
c(X) = X +X3 +X4 +X5 mod(X7 âˆ’1),
and should be decoded by c(X) under minimum-distance decoding.
Problem 3.8
If X is a linear [N,k] code, deï¬ne its weight enumeration polyno-
mial WX (s,t). Show that:
(a) WX (1,1) = 2k,
(b) WX (0,1) = 1,
(c) WX (1,0) has value 0 or 1,
(d) WX (s,t) = WX (t,s) if and only if WX (1,0) = 1.
Solution If x âˆˆX the weight w(x) of X is given by w(x) = â™¯{xi : xi = 1}. Deï¬ne
the weight enumeration polynomial
WX (s,t) = âˆ‘A js jtNâˆ’j
(3.6.4)
where A j = â™¯{x âˆˆX : w(x) = j}. Then:
(a) WX (1,1) = â™¯{x : x âˆˆX } = 2dimX = 2k.
(b) WX (0,1) = A0 = â™¯{0} = 1; note 0 âˆˆX since X is a subspace.
(c) WX (1,0) = 1 â‡”AN = 1, i.e. 11...1 âˆˆX , WX (1,0) = 0 â‡”AN = 0, i.e.
11...1 Ì¸âˆˆX .
(d) WX (s,t) = WX (t,s) â‡’W(0,1) = W(1,0) â‡’WX (1,0) = 1 by (b).
So, if WX (1,0) = 1 then
â™¯{x âˆˆX : w(x) = j} = â™¯{x+11...1 : x âˆˆX ,w(x) = j}
= â™¯{y âˆˆX : w(y) = N âˆ’j}
and WX (1,0) = 1 implies ANâˆ’j = A j for all j. Hence, WX (s,t) = WX (t,s).
Problem 3.9
State the MacWilliams identity, connecting the weight enumerator
polynomials of a code X and its dual X âŠ¥.
Prove that the weight enumerator of the binary Hamming code XH,l of length
N = 2l âˆ’1 equals
WX H
l (z) = 1
2l

(1+z)2lâˆ’1 +(2l âˆ’1)(1âˆ’z2)(2lâˆ’2)/2(1âˆ’z)

.
(3.6.5)
Solution (The second part only) Let Ai be the number of codewords of weight i.
Consider iâˆ’1 columns of the parity-check matrix H. There are three possibilities:
(a) the sum of these columns is 0;

352
Further Topics from Coding Theory
(b) the sum of these columns is one of the chosen columns;
(c) the sum of these columns is one of the remaining columns.
Possibility (a) occurs Aiâˆ’1 times; possibility (c) occurs iAi times as the selected
combination of i âˆ’1 columns may be obtained from any word of weight i by
dropping any of its non-zero components. Next, observe that possibility (b) oc-
curs (N âˆ’(i âˆ’2))Aiâˆ’2 times. Indeed, this combination may be obtained from a
codeword of weight i âˆ’2 by adding any of the N âˆ’(i âˆ’2) remaining columns.
However, we can choose iâˆ’1 columns in
 N
iâˆ’1

ways. Hence,
iAi =
 N
iâˆ’1

âˆ’Aiâˆ’1 âˆ’(N âˆ’i+2)Aiâˆ’2,
(3.6.6)
which is trivially correct if i > N + 1. If we multiply both sides by ziâˆ’1 and then
sum over i we obtain an ODE
Aâ€²(z) = (1+z)N âˆ’A(z)âˆ’NzA(z)+z2Aâ€²(z).
(3.6.7)
Since A(0) = 1, the unique solution of this ODE is
A(z) =
1
N +1(1+z)N +
N
N +1(1+z)(Nâˆ’1)/2(1âˆ’z)(N+1)/2
(3.6.8)
which is equivalent to (3.6.5).
Problem 3.10
Let X be a linear code over F2 of length N and rank k and let Ai be
the number of words in X of weight i, i = 0,...,N. Deï¬ne the weight enumerator
polynomial of X as
W(X ,z) = âˆ‘
0â‰¤iâ‰¤N
Aizi.
Let X âŠ¥denote the dual code to X . Show that
W

X âŠ¥,z

= 2âˆ’k(1+z)NW

X , 1âˆ’z
1+z

.
(3.6.9)
[Hint: Consider g(u) =
âˆ‘
vâˆˆFN
2
(âˆ’1)uÂ·vzw(v) where w(v) denotes the weight of the
vector v and average over X .]
Hence or otherwise show that if X corrects at least one error then the words of
X âŠ¥have average weight N/2.
Apply (3.6.9) to the enumeration polynomial of Hamming code,
W(XHam,z) =
1
N +1(1+z)N +
N
N +1(1+z)(Nâˆ’1)/2(1âˆ’z)(N+1)/2,
(3.6.10)
to obtain the enumeration polynomial of the simplex code:
W(Xsimp,z) = 2âˆ’k2N/2l +2âˆ’k(2l âˆ’1)/2l Ã—2Nz2lâˆ’1 = 1+(2l âˆ’1)z2lâˆ’1.

3.6 Additional problems for Chapter 3
353
Solution The dual code X âŠ¥, of a linear code X with the generating matrix G and
the parity-check matrix H, is deï¬ned as a linear code with the generating matrix
H. If X is an [N,k] code, X âŠ¥is an [N,N âˆ’k] code, and the parity-check matrix
for X âŠ¥is G.
Equivalently, X âŠ¥is the code which is formed by the linear subspace in FN
2
orthogonal to X in the dot-product
âŸ¨x,yâŸ©= âˆ‘
1â‰¤iâ‰¤N
xiyi, x = x1 ...xN, y = y1 ...yN.
By deï¬nition,
W(X ,z) = âˆ‘
uâˆˆX
zw(u), W

X âŠ¥,z

= âˆ‘
vâˆˆX âŠ¥
zw(v).
Following the hint, consider the average
1
â™¯X âˆ‘
uâˆˆX
g(u), where g(u) = âˆ‘
v
(âˆ’1)âŸ¨u,vâŸ©zw(v).
(3.6.11)
Then write (3.6.11) as
1
â™¯X âˆ‘
v
zw(v) âˆ‘
uâˆˆX
(âˆ’1)âŸ¨u,vâŸ©.
(3.6.12)
Note that when v âˆˆX âŠ¥, the sum âˆ‘
uâˆˆX
(âˆ’1)âŸ¨u,vâŸ©= â™¯X . On the other hand, when
v Ì¸âˆˆX âŠ¥then there exists u0 âˆˆX such that âŸ¨u0,vâŸ©Ì¸= 0 (i.e. âŸ¨u0,vâŸ©= 1). Hence, if
v Ì¸âˆˆX âŠ¥, then, with the change of variables u â†’u+u0, we obtain
âˆ‘
uâˆˆX
(âˆ’1)âŸ¨u,vâŸ©= âˆ‘
uâˆˆX
(âˆ’1)âŸ¨u+u0,vâŸ©
= (âˆ’1)âŸ¨u0,vâŸ©âˆ‘
uâˆˆX
(âˆ’1)âŸ¨u,vâŸ©= âˆ’âˆ‘
uâˆˆX
(âˆ’1)âŸ¨u,vâŸ©,
which yields that in this case
âˆ‘
uâˆˆX
(âˆ’1)âŸ¨u,vâŸ©= 0. We conclude that the sum in
(3.6.11) equals
1
â™¯X âˆ‘
vâˆˆX âŠ¥
zw(v)
â™¯X

= W

X âŠ¥,z

.
(3.6.13)
On the other hand, for u = u1 ...uN,
g(u) = âˆ‘
v1,...,vN âˆ
1â‰¤iâ‰¤N
zw(vi)(âˆ’1)uivi
= âˆ
1â‰¤iâ‰¤N âˆ‘
a=0,1
zw(a)(âˆ’1)aui
= âˆ
1â‰¤iâ‰¤N
	
1+z(âˆ’1)ui
.
(3.6.14)

354
Further Topics from Coding Theory
Here w(a) = 0 for a = 0 and w(a) = 1 for a = 1. The RHS of (3.6.14) equals
(1âˆ’z)w(u)(1+z)Nâˆ’w(u).
Hence, an alternative expression for (3.6.11) is
1
â™¯X (1+z)N âˆ‘
uâˆˆX
1âˆ’z
1+z
w(u)
=
1
â™¯X (1+z)NW

X , 1âˆ’z
1+z

.
(3.6.15)
Equating (3.6.13) and (3.6.15) yields
1
â™¯X (1+z)NW

X , 1âˆ’z
1+z

= W

X âŠ¥,z

(3.6.16)
which gives the required equation as â™¯X = 2k.
Next, differentiate (3.6.16) in z at z = 1. The RHS gives
âˆ‘
0â‰¤iâ‰¤N
iAi

X âŠ¥
=

â™¯X âŠ¥
Ã—

the average weight in X âŠ¥
.
On the other hand, in the LHS we have
d
dz

1
â™¯X âˆ‘
0â‰¤iâ‰¤N
Ai(X )(1âˆ’z)i(1+z)Nâˆ’i

z=1
=
1
â™¯X

N2Nâˆ’1 âˆ’A1(X )2Nâˆ’1
(only terms i = 0,1 contribute)
= 2N
â™¯X
N
2 (A1(X ) = 0 as the code is at least 1-error correcting,
with distance â‰¥3).
Now take into account that
(â™¯X )Ã—(â™¯X âŠ¥) = 2k Ã—2Nâˆ’k = 2N.
The equality
the average weight in X âŠ¥= N
2
follows. The enumeration polynomial of the simplex code is obtained by substitu-
tion. In this case the average length is (2l âˆ’1)/2.
Problem 3.11
Describe the binary narrow-sense BCH code X of length 15 and
the designed distance 5 and ï¬nd the generator polynomial. Decode the message
100000111000100.

3.6 Additional problems for Chapter 3
355
Solution Take the binary narrow-sense BCH code X of length 15 and the designed
distance 5. We have Spl(X15 âˆ’1) = F24 = F16. We know that X4 + X + 1 is a
primitive polynomial over F16. Let Ï‰ be a root of X4 +X +1. Then
M1(X) = X4 +X +1,M3(X) = X4 +X3 +X2 +X +1,
and the generator g(X) for X is
g(X) = M1(X)M3(X) = X8 +X7 +X6 +X4 +1.
Take g(X) as example of a codeword. Introduce 2 errors â€“ at positions 4 and 12
â€“ by taking
u(X) = X12 +X8 +X7 +X6 +1.
Using the ï¬eld table for F16, obtain
u1 = u(Ï‰) = Ï‰12 +Ï‰8 +Ï‰7 +Ï‰6 +1 = Ï‰6
and
u3 = u(Ï‰3) = Ï‰36 +Ï‰24 +Ï‰18 +1 = Ï‰9 +Ï‰3 +1 = Ï‰4.
As u1 Ì¸= 0 and u3
1 = Ï‰18 = Ï‰3 Ì¸= u3, deduce that â‰¥2 errors occurred. Calculate the
locator polynomial
l(X) = 1+Ï‰6X +(Ï‰13 +Ï‰12)X2.
Substituting 1,Ï‰,...,Ï‰14 into l(X), check that Ï‰3 and Ï‰11 are roots. This conï¬rms
that, if exactly 2 errors occurred their positions are 4 and 12 then the codeword sent
was 100010111000000.
Problem 3.12
For a word x = x1 ...xN âˆˆFN
2 the weight w(x) is the number
of non-zero digits: w(x) = â™¯{i : xi Ì¸= 0}. For a linear [N,k] code X let Ai be the
number of words in X of weight i (0 â‰¤i â‰¤N). Deï¬ne the weight enumerator
polynomial W(X ,z) =
N
âˆ‘
i=0
Aizi. Show that if we use X on a binary symmetric
channel with error-probability p, the probability of failing to detect an incorrect
word is (1âˆ’p)N 
W

X ,
p
1âˆ’p

âˆ’1

.
Solution Suppose we have sent the zero codeword 0. Then the error-probability
E =
âˆ‘
xâˆˆX \0
P

x |0 sent

= âˆ‘
iâ‰¥1
Aipi(1âˆ’p)Nâˆ’i =
(1âˆ’p)N

âˆ‘
iâ‰¥0
Ai

p
1âˆ’p
i
âˆ’1

= (1âˆ’p)N

W

X ,
p
1âˆ’p

âˆ’1

.

356
Further Topics from Coding Theory
Problem 3.13
Let X be a binary linear [N,k,d] code, with the weight enumer-
ator WX (s). Find expressions, in terms of WX (s), for the weight enumerators of:
(i) the subcode X ev âŠ†X consisting of all codewords x âˆˆX of even weight,
(ii) the parity-check extension X pc of X .
Prove that if d is even then there exists an [N,k,d] code where each codeword has
even weight.
Solution (i) All words with even weights from X belong to subcode X ev. Hence
W ev
X (s) = 1
2 [WX (s)+WX (âˆ’s)].
(ii) Clearly, all non-zero coefï¬cients of weight enumeration polynomial for X +
corresponds to even powers of z, and A2i(X +) = A2i(X )+A2iâˆ’1(X ), i = 1,2,....
Hence,
W pc
X (s) = 1
2 [(1+s)WX (s)+(1âˆ’s)WX (âˆ’s)].
If X is binary [N,k,d] then you ï¬rst truncate X to X âˆ’then take the parity-
check extension (X âˆ’)+. This preserves k and d (if d is even) and makes all code-
words of even weight.
Problem 3.14
Check that polynomials X4 +X3 +X2 +X +1 and X4 +X +1 are
irreducible over F2. Are these polynomials primitive over F2? What about polyno-
mials X3 +X +1, X3 +X2 +1? X4 +X3 +1?
Solution As both polynomials X4 +X3 +X2 +X +1 and X4 +X +1 do not vanish
at X = 0 or X = 1, they are not divisible by X or X +1. They are also not divisible by
X2 +X +1, the only irreducible polynomial of degree 2, or by X3 +X +1 or X3 +
X2 +1, the only irreducible polynomials of degree 3. Hence, they are irreducible.
The polynomial X4 + X3 + X2 + X + 1 cannot be primitive polynomial as it di-
vides X5 âˆ’1. Let us check that X4 + X + 1 is primitive. Take F2[X]/âŸ¨X4 + X + 1âŸ©
and use the F4
2 ï¬eld table. The cyclotomic coset is {Ï‰,Ï‰2,Ï‰4,Ï‰8} (as Ï‰16 = Ï‰).
The primitive polynomial MÏ‰(X) is then
(X âˆ’Ï‰)(X âˆ’Ï‰2)(X âˆ’Ï‰4)(X âˆ’Ï‰8)
= X4 âˆ’(Ï‰ +Ï‰2 +Ï‰4 +Ï‰8)X2
+(Ï‰Ï‰2 +Ï‰Ï‰4 +Ï‰Ï‰8 +Ï‰2Ï‰4 +Ï‰2Ï‰8 +Ï‰4Ï‰8)X2
âˆ’(Ï‰Ï‰2Ï‰4 +Ï‰Ï‰2Ï‰8 +Ï‰Ï‰4Ï‰8 +Ï‰2Ï‰4Ï‰8)x+Ï‰Ï‰2Ï‰4Ï‰8
= X4 âˆ’(Ï‰ +Ï‰2 +Ï‰4 +Ï‰8)X2 +(Ï‰3Ï‰5 +Ï‰9 +Ï‰6 +Ï‰10 +Ï‰12)X2
âˆ’(Ï‰7 +Ï‰11 +Ï‰13 +Ï‰14)X +Ï‰15 = X4 +X +1.

3.6 Additional problems for Chapter 3
357
The order of X4 + X + 1 is 15: other primitive polynomials of order 15 are X4 +
X3 + 1 and X4 + X + 1. Thus, the only primitive polynomial of degree 4 is X4 +
X + 1. Similarly, the only primitive polynomials of degree 3 are X3 + X + 1 and
X3 +X2 +1, both of order 7.
Problem 3.15
Suppose a binary narrow-sense BCH code is used, of length 15,
designed distance 5, and the received word is X10 + X5 + X4 + X + 1. How is it
decoded? If the received word is X11 + X10 + X6 + X5 + X4 + X + 1, what is the
number of errors?
Solution Suppose the received word is
r(X) = X10 +X5 +X4 +X +1,
and let Ï‰ be a primitive element in F16. Then
s1 = r(Ï‰) = Ï‰10 +Ï‰5 +Ï‰4 +Ï‰ +e
= 0111+0110+0011+0010+0001 = 0001 = e,
s3 = r(Ï‰3) = Ï‰30 +Ï‰15 +Ï‰12 +Ï‰3 +e
= 0001+0001+1111+1000+0001 = 0110 = Ï‰5.
See that s3 Ì¸= s3
1: two errors. The error-locator polynomial
Ïƒ(X) = e+s1X +(s3sâˆ’1
1 +s2
1)X2 = e+X +(Ï‰5 +e)X2 = e+X +Ï‰10X2.
Checking for the roots, Ï‰0 = e, Ï‰1, Ï‰2, Ï‰3, Ï‰4, Ï‰5, Ï‰6: no, Ï‰7: yes. Then divide:
(Ï‰10X2 +X +e)

(X +Ï‰7) = Ï‰10X +Ï‰8 = Ï‰10(X +Ï‰13),
and identify the second root: Ï‰13. So, the errors occurred at positions 15 âˆ’7 = 8
and 15âˆ’13 = 2. Decode:
r(X) â†’X10 +X8 +X5 +X4 +X2 +X +1.
Problem 3.16
Prove that the binary code of length 23 generated by the poly-
nomial g(X) = 1 + X + X5 + X6 + X7 + X9 + X11 has minimal distance 7, and is
perfect.
[Hint: If grev(X) = X11g(1/X) is the reversal of g(X) then
X23 +1 â‰¡(X +1)g(X)grev(X) mod 2.]
Solution First, show that the code is BCH, of designed distance 5. By the fresherâ€™s
dream Lemma 3.1.5, if Ï‰ is a root of a polynomial f(X) âˆˆF2[X] then so is Ï‰2.
Thus, if Ï‰ is a root of g(X) = 1 + X + X5 + X6 + X7 + X9 + X11 then so are Ï‰,
Ï‰2, Ï‰4, Ï‰8, Ï‰16, Ï‰9, Ï‰18, Ï‰13, Ï‰3, Ï‰6, Ï‰12. This yields the design sequence

358
Further Topics from Coding Theory
{Ï‰,Ï‰2,Ï‰3,Ï‰4}. By the BCH bound (Theorem 2.5.39 and Theorem 3.2.9), the
cyclic code X generated by g(X) has distance â‰¥5.
Next, the parity-check extension, X +, is self-orthogonal. To check this, we need
only to show that any two rows of the generating matrix of X + are orthogonal.
These are represented by the concatenated words
(Xig(X)|1) and (X jg(X)|1).
Their dot-product equals
1+(Xig(X))(X jg(X)) = 1+âˆ‘
r gi+rg j+r
= 1+âˆ‘
r gi+rgrev
11âˆ’jâˆ’r
= 1+coefï¬cient of X11+iâˆ’j in g(X)Ã—grev(X)
:
;<
=
||
1+Â·Â·Â·+X22
= 1+1 = 0.
We conclude that
any two words in X + are dot-orthogonal.
Next, observe that all words in X + have weights divisible by 4. Indeed, by
inspection, all rows (Xig(X)|1) of the generating matrix of X + have weight 8.
Then, by induction on the number of rows involved in the sum, if x âˆˆX + and
g(i) âˆ¼(Xig(X)|1) is a row of the generating matrix of X + then
w

g(i) +x

= w

g(i)
+w(x)âˆ’2w

g(i) âˆ§x

,
(3.6.17)
where

g(i) âˆ§x

l = min
	
g(i)
l,xl

, l = 1,...,24. We know that 8 divides w

g(i)
.
Moreover, by the induction hypothesis, 4 divides w(x). Next, by (3.6.17), w

g(i) âˆ§
x

is even, so 2w

g(i) âˆ§x

is divisible by 4. Then the LHS, w

g(i) +x

, is divisible
by 4.
Therefore, the distance of X + is 8, as it is â‰¥5 and is divisible by 4. (It is easy to
see that it cannot be > 8 as then it would be 12.) Then the distance of the original
code, X , equals 7.
The code X is perfect 3-error correcting, since the volume of the 3-ball in F23
2
equals
23
0

+
23
1

+
23
2

+
23
3

= 1+23+253+1771 = 2048 = 211,
and
211 Ã—212 = 223.
Here, obviously, 12 represents the rank and 23 the length.

3.6 Additional problems for Chapter 3
359
Problem 3.17
Use the MacWilliams identity to prove that the weight distribution
of a q-ary MDS code of distance d is
Ai =
N
i

âˆ‘
0â‰¤jâ‰¤iâˆ’d
(âˆ’1) j
i
j

qiâˆ’d+1âˆ’j âˆ’1

=
N
i

(qâˆ’1) âˆ‘
0â‰¤jâ‰¤iâˆ’d
(âˆ’1) j
iâˆ’1
j

qiâˆ’dâˆ’j, d â‰¤i â‰¤N.
[Hint: To begin the solution,
(a) write the standard MacWilliams identity,
(b) swap X and X âŠ¥,
(c) change s â†’sâˆ’1,
(d) multiply by sn and
(e) take the derivative dr/dsr, 0 â‰¤r â‰¤k (which equals d(X âŠ¥)âˆ’1).
Use the Leibniz rule
dr
dsr

f(s)g(s)

= âˆ‘
0â‰¤jâ‰¤r
r
j
 d j
ds j f(s)
 drâˆ’j
dsrâˆ’j g(s)

.
(3.6.18)
Use the fact that d(X ) = N âˆ’k+1 and d(X âŠ¥) = k+1 and obtain simpliï¬ed equa-
tions involving ANâˆ’k+1,...,ANâˆ’r only. Subsequently, determine ANâˆ’k+1,...,ANâˆ’r.
Varying r, continue up to AN.]
Solution The MacWilliams identity is
âˆ‘
1â‰¤iâ‰¤N
AâŠ¥
i si = 1
qk âˆ‘
1â‰¤iâ‰¤N
Ai(1âˆ’s)i	
1+(qâˆ’1)s

Nâˆ’i.
Swap X and X âŠ¥, change s â†’sâˆ’1 and multiply by sN. After this differentiate
r â‰¤k times and substitute s = 1:
1
qk
âˆ‘
0â‰¤iâ‰¤Nâˆ’r
N âˆ’i
r

Ai = 1
qr âˆ‘
0â‰¤iâ‰¤r
AâŠ¥
i
N âˆ’i
N âˆ’r

(3.6.19)
(the Leibniz rule (3.6.18) is used here). Formula (3.6.19) is the starting point. For
an MDS code, A0 = AâŠ¥
0 = 1, and
Ai = 0, 1 â‰¤i â‰¤N âˆ’k (= d âˆ’1), AâŠ¥
i = 0, 1 â‰¤i â‰¤k (= dâŠ¥âˆ’1).
Then
N
r
 1
qk + 1
qk
Nâˆ’r
âˆ‘
i=Nâˆ’k+1
N âˆ’i
r

Ai = 1
qr
 N
N âˆ’r

= 1
qr
N
r

,

360
Further Topics from Coding Theory
i.e.
Nâˆ’r
âˆ‘
i=Nâˆ’k+1
N âˆ’i
r

Ai =
N
r

(qkâˆ’r âˆ’1).
For r = k we obtain 0 = 0, for r = k âˆ’1
ANâˆ’k+1 =
 N
k âˆ’1

(qâˆ’1),
(3.6.20)
for r = k âˆ’2
k âˆ’1
k âˆ’2

ANâˆ’k+1 +ANâˆ’k+2 =
 N
k âˆ’2

(q2 âˆ’1),
etc. This is a triangular system of equations for ANâˆ’k+1,...,ANâˆ’r. Varying r, we
can get ANâˆ’k+1,...,ANâˆ’1. The result is
Ai =
N
i

âˆ‘
0â‰¤jâ‰¤iâˆ’d
(âˆ’1) j
i
j

(qiâˆ’d+1âˆ’j âˆ’1)
=
N
i

âˆ‘
0â‰¤jâ‰¤iâˆ’d
(âˆ’1) j
iâˆ’1
j

(qqiâˆ’dâˆ’j âˆ’1)
âˆ’
âˆ‘
1â‰¤jâ‰¤iâˆ’d+1
(âˆ’1) jâˆ’1
iâˆ’1
j âˆ’1

(qiâˆ’d+1âˆ’j âˆ’1)

=
N
i

(qâˆ’1)
âˆ‘
0â‰¤jâ‰¤iâˆ’d
(âˆ’1) j
iâˆ’1
j

qiâˆ’dâˆ’j,d â‰¤i â‰¤N,
as required.
In fact, (3.6.20) can be obtained without calculations: in an MDS code of rank k
and distance d any k = N âˆ’d +1 digits determine the codeword uniquely. Further,
for any choice of N âˆ’d positions there are exactly q codewords with digits 0 in
these positions. One of them is the zero codeword, and the remaining qâˆ’1 are of
weight d. Hence,
ANâˆ’k+1 = Ad =
N
d

(qâˆ’1).
Problem 3.18
Prove the following properties of Kravchukâ€™s polynomials Kk(i).
(a) For all q: (qâˆ’1)i
 N
i

Kk(i) = (qâˆ’1)k
 N
k

Ki(k).
(b) For q = 2: Kk(i) = (âˆ’1)kKk(N âˆ’i).
(c) For q = 2: Kk(2i) = KNâˆ’k(2i).

3.6 Additional problems for Chapter 3
361
Solution Write
Kk(i) =
âˆ‘
0âˆ¨(i+kâˆ’N)â‰¤jâ‰¤kâˆ§i
i
j
N âˆ’i
k âˆ’j

(âˆ’1) j(qâˆ’1)kâˆ’j.
Next:
(a) The following straightforward equation holds true:
(qâˆ’1)i
N
i

Kk(i) = (qâˆ’1)k
N
k

Ki(k)
(as all summands become insensitive to swapping i â†”k).
For q = 2 this yields
N
i

Kk(i) =
N
k

Ki(k); in particular,
N
i

K0(i) =
N
0

Ki(0) = Ki(0).
(b) Also, for q = 2: Kk(i) = (âˆ’1)kKk(N âˆ’i) (again straightforward, after swapping
i â†”iâˆ’j).
(c) Thus, still for q = 2:
N
2i

Kk(2i) =
N
k

K2i(k) which equals
(âˆ’1)2i
 N
N âˆ’k

K2i(N âˆ’k) =
N
2i

KNâˆ’k(2i). That is,
Kk(2i) = KNâˆ’k(2i).
Problem 3.19
What is an (n,Fq)-root of unity? Show that the set E(n,q) of the
(n,Fq)-roots of unity form a cyclic group. Check that the order of E(n,q) equals n if
n and q are co-prime. Find the minimal s such that E(n,q) âŠ‚Fqs.
Deï¬ne a primitive (n,Fq)-root of unity. Determine the number of primitive
(n,Fq)-roots of unity when n and q are co-prime. If Ï‰ is a primitive (n,Fq)-root of
unity, ï¬nd the minimal â„“such that Ï‰ âˆˆFqâ„“.
Find representation of all elements of F9 as vectors over F3. Find all (4,F9)-roots
of unity as vectors over F3.
Solution We know that any root of an irreducible polynomial of degree 2 over ï¬eld
F3 = {0,1,2} belongs to F9. Take the polynomial f(X) = X2 + 1 and denote its
root by Î± (any of the two). Then all elements of F9 may be represented as a0 +a1Î±
where a0,a1 âˆˆF3. In fact,
F9 = {0,1,Î±,1+Î±,2+Î±,2Î±,1+2Î±,2+2Î±}.

362
Further Topics from Coding Theory
Another approach is as follows: we know that X8 âˆ’1 =
âˆ
1â‰¤iâ‰¤8
(X âˆ’Î¶ i) in the ï¬eld
F9 where Î¶ is a primitive (8,F9)-root of unity. In terms of circular polynomials,
X8 âˆ’1 = Q1(X)Q2(X)Q4(X)Q8(X). Here Qn(x) = âˆs:gcd(s,n)=1(x âˆ’Ï‰s) where Ï‰
is a primitive (n,F9)-root of unity. Write X8 âˆ’1 = âˆd:d|8 Qd(x). Next, compute
Q1(X) = âˆ’1+X,Q2(X) = 1+X,Q4(X) = 1+X2,
Q8(X) =

X8 âˆ’1

Q1(X)Q2(X)Q4(X)

= (X8 âˆ’1)/(X4 âˆ’1) = X4 +1.
As 32 = 1 mod8, by Theorem 3.1.53 Q8(X) should be decomposed over F3 into
product of Ï†(8)/2 = 2 irreducible polynomials of degree 2. Indeed,
Q8(X) = (X2 +X +2)(X2 +2X +2).
Let Î¶ be a root of X2 + X + 2, then it is a primitive root of degree 8 over F3 and
F9 = F3(Î¶). Hence, F9 = {0,Î¶,Î¶ 2,Î¶ 3,Î¶ 4,Î¶ 5,Î¶ 6,Î¶ 7,Î¶ 8}, and Î¶ = 1+Î±. Finally,
we present the index table
Î¶ = 1+Î±,Î¶ 2 = 2Î±,Î¶ 3 = 1+2Î±,Î¶ 4 = 2,
Î¶ 5 = 2+2Î±,Î¶ 6 = Î±,Î¶ 7 = 2+Î±,Î¶ 8 = 1.
Hence, the roots of degree 4 are Î¶ 2,Î¶ 4,Î¶ 6,Î¶ 8.
Problem 3.20
Deï¬ne a cyclic code of length N over the ï¬eld Fq. Show that there
is a bijection between the cyclic codes of length N, and the factors of XN âˆ’e in the
polynomial ring Fq[X].
Now consider binary cyclic codes. If N is an odd integer then we can ï¬nd a ï¬nite
extension K of F2 that contains a primitive Nth root of unity Ï‰. Show that a cyclic
code of length N with deï¬ning set {Ï‰,Ï‰2,...,Ï‰Î´âˆ’1} has minimum distance at
least Î´. Show that if N = 2â„“âˆ’1 and Î´ = 3 then we obtain the Hamming [2â„“âˆ’1,
2â„“âˆ’1âˆ’â„“,3] code.
Solution A linear code X âŠ‚FÃ—N
q
is a cyclic code if x1 ...xN âˆˆX implies that
x2,...xNx1 âˆˆX . Bijection of cyclic codes and factors of XN âˆ’1 can be established
as in Corollary 3.3.3.
Passing to binary codes, consider, for brevity, N = 7. Factorising in F7
2 renders
the decomposition
X7 âˆ’1 = (X âˆ’1)(X3 +X +1)(X3 +X2 +1) := (X âˆ’1) f1(X) f2(X).
Suppose Ï‰ is a root of f1(X). Since f1(X)2 = f1(X2) in F2[X] we have
f1(Ï‰) = f1(Ï‰2) = 0.
It follows that the cyclic code X with deï¬ning root Ï‰ has the generator polynomial
f1(X) and the check polynomial (X âˆ’1) f2(X) = X4 + X2 + X + 1. This property

3.6 Additional problems for Chapter 3
363
characterises Hammingâ€™s original code (up to equivalence). The case where Ï‰ is a
root of f2(X) is similar (in fact, we just reverse every codeword). For a general N =
2l âˆ’1, we take a primitive element Ï‰ âˆˆF2l and its minimal polynomial MÏ‰(X).
The roots of MÏ‰(X) are Ï‰, Ï‰2,...,Ï‰2lâˆ’1, hence degMÏ‰(X) = l. Thus, a code with
deï¬ning root Ï‰ has rank N âˆ’l = 2l âˆ’1 âˆ’l, as in the Hamming [2l âˆ’1,2l âˆ’l âˆ’1]
code.
Problem 3.21
Write an essay comparing the decoding procedures for Hamming
and two-error correcting BCH codes.
Solution To clarify the ideas behind the BCH construction, we ï¬rst return to the
Hamming codes. The Hamming [2l âˆ’1,2l âˆ’1âˆ’l] code is a perfect one-error cor-
recting code of length N = 2l âˆ’1. The procedure of decoding the Hamming code is
as follows. Having a word y = y1 ...yN, N = 2l âˆ’1, form the syndrome s = yHT.
If s = 0, decode y by y. If s Ì¸= 0 then s is among the columns of H = HHam. If this is
column i, decode y by xâˆ—= y+ei, where ei = 0...010...0 (1 in the ith position,
0 otherwise).
We can try the following idea to be able to correct more than one error (two to
start with). Select 2l of the rows of the parity-check matrix in the form
 H =
 H
Î H

.
(3.6.21)
Here Î HHam is obtained by permuting the columns of HHam (Î  is a permutation
of degree 2l âˆ’1). The new matrix  H contains 2l linearly independent rows: it then
determines a [2l âˆ’1,2l âˆ’1 âˆ’2l] linear code. The syndromes are now words of
length 2l (or pairs of words of length l): y  HT = (ssâ€²). A syndrome (s,sâ€²)T may or
may not be among the columns of  H. Recall, we want the new code to be two-error
correcting, and the decoding procedure to be similar to the one for the Hamming
codes. Suppose two errors occur, i.e. y differs from a codeword x by two digits, say
i and j. Then the syndrome is
y  HT = (si +s j,
sÎ i +sÎ  j)
where sk is the word representing column k in H. We organise our permutation so
that, knowing vector (si+s j,sÎ i+sÎ  j), we can always ï¬nd i and j (or equivalently,
si and s j). In other words, we should be able to solve the equations
si +s j = z,
sÎ i +sÎ  j = zâ€²
(3.6.22)
for any pair (z,zâ€²) that may eventually occur as a syndrome under two errors.
A natural guess is to try a permutation Î  that has some algebraic signiï¬cance,
e.g. sÎ i = sisi = (si)2
(a bad choice) or sÎ i = sisisi = (si)3
(a good choice)

364
Further Topics from Coding Theory
or, generally, sÎ i = sisi Â·Â·Â·si
(k times). Say, one can try the multiplication
mod 1 + XN; unfortunately, the multiplication does not lead to a ï¬eld. The reason
is that polynomial 1+XN is always reducible. So, suppose we organise the check
matrix as
 HT =
â›
âœ
â
(1...00)
(1...00)k
...
(1...11)
(1...11)k
â
âŸ
â .
Then we have to deal with equations of the type
si +s j = z, sk
i +sk
j = zâ€².
(3.6.23)
For solving (3.6.23), we need the ï¬eld structure of the Hamming space, i.e. not
only multiplication but also division. Any ï¬eld structure on the Hamming space
of length N is isomorphic to F2N, and a concrete realisation of such a structure is
F2[X]

âŸ¨c(X)âŸ©, a polynomial ï¬eld modulo an irreducible polynomial c(X) of degree
N. Such a polynomial always exists: it is one of the primitive polynomials of degree
N. In fact, the simplest consistent system of the form (3.6.23) is
s+sâ€² = z, s3 +sâ€²3 = zâ€²;
it is reduced to a single equation zs2 âˆ’z2s+z3 âˆ’zâ€² = 0, and our problem becomes
to factorise the polynomial zX2 âˆ’z2X +z3 âˆ’zâ€².
For N = 2l âˆ’1,l = 4 we obtain [15,7,5] code. The rank 7 is due to the linear
independence of the columns of  H. The key point is to check that the code corrects
up to two errors. First suppose we received a word y = y1 ...y15 in which two
errors occurred in digits i and j that are unknown. In order to ï¬nd these places,
calculate the syndrome y  HT = (z,zâ€²)T. Recall that z and zâ€² are words of length 4;
the total length of the syndrome is 8. Note that zâ€² Ì¸= z3: if zâ€² = z3, precisely one
error occurred. Write a pair of equations
s+sâ€² = z, s3 +sâ€²3 = zâ€²,
(3.6.24)
where s and sâ€² are words of length 4 (or equivalently their polynomials), and the
multiplication is mod1 + X + X4. In the case of two errors it is guaranteed that
there is exactly one pair of solutions to (3.6.24), one vector occupying position i
and another position j, among the columns of the upper (Hamming) half of matrix
 H. Moreover, (3.6.24) cannot have more than one pair of solutions because
zâ€² = s3 +sâ€²3 = (s+sâ€²)(s2 +ssâ€² +sâ€²2) = z(z2 +ssâ€²)
implies that
ssâ€² = zâ€²zâˆ’1 +z2.
(3.6.25)

3.6 Additional problems for Chapter 3
365
Now (3.6.25) and the ï¬rst equation in (3.6.24) give that s,sâ€² are precisely the roots
of a quadratic equation
X2 +zX +

zâ€²zâˆ’1 +z2
= 0
(3.6.26)
(with zâ€²zâˆ’1 +z2 Ì¸= 0). But the polynomial in the LHS of (3.6.26) cannot have more
than two distinct roots (it could have no root or two coinciding roots, but it is
excluded by the assumption that there are precisely two errors). In the case of a
single error, we have zâ€² = z3; in this case s = z is the only root and we just ï¬nd the
word z among the columns of the upper half of matrix  H.
Summarising, the decoding scheme, in the case of the above [15,7] code, is as
follows: Upon receiving word y, form a syndrome y  HT = (z,zâ€²)T. Then
(i) If both z and zâ€² are zero words, conclude that no error occurred and decode y
by y itself.
(ii) If z Ì¸= 0 and z3 = zâ€², conclude that a single error occurred and ï¬nd the location
of the error digit by identifying word z among the columns of the Hamming
check matrix.
(iii) If z Ì¸= 0 and z3 Ì¸= zâ€², form the quadric (3.6.24), and if it has two distinct roots
s and sâ€², conclude that two errors occurred and locate the error digits by iden-
tifying words s and sâ€² among the columns of the Hamming check matrix.
(iv) If z Ì¸= 0 and z3 Ì¸= zâ€² and quadric (3.6.26) has no roots, or if z is zero but zâ€² is
not, conclude that there are at least three errors.
Note that the case where z Ì¸= 0, z3 Ì¸= zâ€² and quadric (3.6.26) has a single root is
impossible: if (3.6.26) has a root, s say, then either another root sâ€² Ì¸= s or z = 0 and
a single error occurs.
The decoding procedure allows us to detect, in some cases, that more than three
errors occurred. However, this procedure may lead to a wrong codeword when
three or more errors occur.

4
Further Topics from Information Theory
In Chapter 4 it will be convenient to work in a general setting which covers both
discrete and continuous-type probability distributions. To do this, we assume that
probability distributions under considerations are given by their Radonâ€“Nikodym
derivatives with respect to underlying reference measures usually denoted by Î¼
or Î½. The role of a reference measure can be played by a counting measure sup-
ported by a discrete set or by the Lebesgue measure on Rd; we need only that
the reference measure is locally ï¬nite (i.e. it assigns ï¬nite values to compact sets).
The Radonâ€“Nikodym derivatives will be called probability mass functions (PMFs):
they represent probabilities in the discrete case and probability density functions
(PDFs) in the continuous case.
The initial setting of the channel capacity theory developed for discrete channels
in Chapter 1 (see Section 1.4) goes almost unchanged for a continuously distributed
noise by adopting the logical scheme:
a set U of messages, of cardinality M =
3
2NR4
â†’
a codebook X of size M with codewords of length N
â†’
reliable rate R of transmission through a noisy channel
â†’
the capacity of the channel.
However, to simplify the exposition, we will assume from now on that encoding
U â†’X is a one-to-one map and identify a code with its codebook.
4.1 Gaussian channels and beyond
Here we study channels with continuously distributed noise; they are the basic
models in telecommunication, including both wireless and telephone transmission.
The most popular model of such a channel is a memoryless additive Gaussian chan-
nel (MAGC) but other continuous-noise models are also useful. The case of an
366

4.1 Gaussian channels and beyond
367
MAGC is particularly attractive because it allows one to do some handy and far-
reaching calculations with elegant answers.
However, Gaussian (and other continuously distributed) channels present a chal-
lenge that was absent in the case of ï¬nite alphabets considered in Chapter 1.
Namely, because codewords (or, using a slightly more appropriate term, codevec-
tors) can a priori take values from a Euclidean space (as well as noise vectors),
the deï¬nition of the channel capacity has to be modiï¬ed, by introducing a power
constraint. More generally, the value of capacity for a channel will depend upon
the so-called regional constraints which can generate analytic difï¬culties. In the
case of MAGC, the way was shown by Shannon, but it took some years to make
his analysis rigorous.
An input word of length N (designed to use the channel over N slots in succes-
sion) is identiï¬ed with an input N-vector
x(= x(N)) =
â›
âœ
â
x1
...
xN
â
âŸ
â .
We assume that xi âˆˆR and hence x(N) âˆˆRN (to make the notation shorter, the upper
index (N) will be often omitted).
In an additive channels an input vector x is transformed to a random vector
Y(N) =
â›
âœ
â
Y1
...
YN
â
âŸ
â where Y = x+Z, or, component-wise,
Yj = xj +Z j, 1 â‰¤j â‰¤N.
(4.1.1)
Here and below,
Z =
â›
âœ
â
Z1
...
ZN
â
âŸ
â 
is a noise vector composed of random variables Z1,...,ZN. Thus, the noise can be
characterised by a joint PDF f no(z) â‰¥0 where
z =
â›
âœ
â
z1
...
zN
â
âŸ
â 

368
Further Topics from Information Theory
and the total integral
0
f no(z)dz1 ...dzN = 1. The N-fold noise probability distri-
bution is determined by integration over a given set of values for Z:
Pno(Z âˆˆA) =
0
A f no(z)dz1 ...dzN, for A âŠ†RN.
Example 4.1.1
An additive channel is called Gaussian (an AGC, in short) if,
for each N, the noise vector
â›
âœ
â
Z1
...
ZN
â
âŸ
â is a multivariate normal; cf. PSE I, p. 114.
We assume from now on that the mean value EZj = 0. Recall that the multivariate
normal distribution with the zero mean is completely determined by its covariance
matrix. More precisely, the joint PDF f no
Z(N)(z(N)) for an AGC has the form
1
(2Ï€)N/2
detÎ£
1/2 exp

âˆ’1
2zTÎ£âˆ’1z

, z =
â›
âœ
â
z1
...
zN
â
âŸ
â âˆˆRN.
(4.1.2)
Here Î£ is an N Ã—N matrix assumed to be real, symmetric and strictly positive def-
inite, with entries Î£ j jâ€² = E

Z jZ jâ€²
representing the covariance of noise random
variables Z j and Z jâ€², 1 â‰¤j, jâ€² â‰¤N. (Real strict positive deï¬niteness means that Î£ is
of the form BBT where B is an N Ã—N real invertible matrix; if Î£ is strictly positive
deï¬nite then Î£ has N mutually orthogonal eigenvectors, and all N eigenvalues of Î£
are greater than 0.) In particular, each random variable Zj is normal: Z j âˆ¼N(0,Ïƒ2
j )
where Ïƒ2
j = EZ2
j coincides with the diagonal entry Î£ j j. (Due to strict positive def-
initeness, Î£ j j > 0 for all j = 1,...,N.)
If in addition the random variables Z1,Z2,... are IID, the channel is called mem-
oryless Gaussian (MGC) or a channel with (additive) Gaussian white noise. In this
case matrix Î£ is diagonal: Î£i j = 0 when i Ì¸= j and Î£ii > 0 when i = j. This is an
important model example (both educationally and practically) since it admits some
nice ï¬nal formulas and serves as a basis for further generalisations.
Thus, an MGC has IID noise random variables Zi âˆ¼N(0,Ïƒ2) where Ïƒ2 =
VarZi = EZ2
i . For normal random variables, independence is equivalent to decorre-
lation. That is, the equality E

Z jZ jâ€²
= 0 for all j, jâ€² = 1,...,N with j Ì¸= jâ€² implies
that the components Z1,...,ZN of the noise vector Z(N) are mutually independent.
This can be deduced from (4.1.2): if matrix Î£ has Î£ j jâ€² = 0 for j Ì¸= jâ€² then Î£ is
diagonal, with detÎ£ =
âˆ
1â‰¤jâ‰¤N
Î£ j j, and the joint PDF in (4.1.2) decomposes into a
product of N factors representing individual PDFs of components Zj, 1 â‰¤j â‰¤N:
âˆ
1â‰¤jâ‰¤N
1

2Ï€Î£ j j
1/2 exp

âˆ’
z2
j
2Î£ j j

.
(4.1.3)

4.1 Gaussian channels and beyond
369
Moreover, under the IID assumption, with Î£j j â‰¡Ïƒ2 > 0, all random variables Zj âˆ¼
N(0,Ïƒ2), and the noise distribution for an MGC is completely speciï¬ed by a single
parameter Ïƒ > 0. More precisely, the joint PDF from (4.1.3) is rewritten as

1
âˆš
2Ï€Ïƒ
N
exp

âˆ’1
2Ïƒ2 âˆ‘
1â‰¤jâ‰¤N
z2
j

.
It is often convenient to think that an inï¬nite random sequence Zâˆ
1 = {Z1,Z2,...}
is given, and the above noise vector Z(N) is formed by the ï¬rst N members of this
sequence. In the Gaussian case, Zâˆ
1 is called a random Gaussian process; with
EZ j â‰¡0, this process is determined, like before, by its covariance Î£, with Î£ij =
Cov

Zi,Z j

= E

ZiZ j

. The term â€˜white Gaussian noiseâ€™ distinguishes this model
from a more general model of a channel with â€˜colouredâ€™ noise; see below.
Channels with continuously distributed noise are analysed by using a scheme
similar to the one adopted in the discrete case: in particular, if the channel is used
for transmitting one of M âˆ¼2RN, R < 1, encoded messages, we need a codebook
that consists of M codewords of length N: xT(i) = (x1(i),...,xN(i)), 1 â‰¤i â‰¤M:
XM,N =
(
x(N)(1),...,x(N)(M)
)
=
â§
âª
â¨
âª
â©
â›
âœ
â
x1(1)
...
xN(1)
â
âŸ
â ,...,
â›
âœ
â
x1(M)
...
xN(M)
â
âŸ
â 
â«
âª
â¬
âª
â­
.
(4.1.4)
The codebook is, of course, presumed to be known to both the sender and the
receiver. The transmission rate R is given by
R = log2 M
N
.
(4.1.5)
Now suppose that a codevector x(i) had been sent. Then the received random
vector Y(= Y(i)) =
â›
âœ
â
x1(i)+Z1
...
xN(i)+ZN
â
âŸ
â is decoded by using a chosen decoder d : y â†’
d(y) âˆˆXM,N. Geometrically, the decoder looks for the nearest codeword x(k),
relative to a certain distance (adapted to the decoder); for instance, if we choose to
use the Euclidean distance then vector Y is decoded by the codeword minimising
the sum of squares:
d(Y) = argmin

âˆ‘
1â‰¤jâ‰¤N
(Yj(i)âˆ’xj(l))2 : x(l) âˆˆXM,N

;
(4.1.6)
when d(y) Ì¸= x(i) we have an error. Luckily, the choice of a decoder is conveniently
resolved on the basis of the maximum-likelihood principle; see below.

370
Further Topics from Information Theory
There is an additional subtlety here: one assumes that, for an input word x to get a
chance of successful decoding, it should belong to a certain â€˜transmittableâ€™ domain
in RN. For example, working with an MAGC, one imposes the power constraint
1
N âˆ‘
1â‰¤jâ‰¤N
x2
j â‰¤Î±
(4.1.7)
where Î± > 0 is a given constant. In the context of wireless transmission this means
that the amplitude square power per signal in an N-long input vector should be
bounded by Î±, otherwise the result of transmission is treated as â€˜undecodableâ€™.
Geometrically, in order to perform decoding, the input codeword x(i) constituting
the codebook must lie inside the Euclidean ball BN
â„“2(
âˆš
Î±N) of radius r =
âˆš
Î±N
centred at 0 âˆˆRN:
B(N)
â„“2 (r) =
â§
âª
â¨
âª
â©
x =
â›
âœ
â
x1
...
xN
â
âŸ
â :

âˆ‘
1â‰¤jâ‰¤N
x2
j
1/2
â‰¤r
â«
âª
â¬
âª
â­
.
The subscript â„“2 stresses that RN with the standard Euclidean distance is viewed as
a Hilbert â„“2-space.
In fact, it is not required that the whole codebook XM,N lies in a decodable
domain; the agreement is only that if a codeword x(i) falls outside then it is decoded
wrongly with probability 1. Pictorially, the requirement is that â€˜mostâ€™ of codewords
lie within BN
â„“2((NÎ±)1/2) but not necessarily all of them. See Figure 4.1.
A reason for the â€˜regionalâ€™ constraint (4.1.7) is that otherwise the codewords
can be positioned in space at an arbitrarily large distance from each other, and,
eventually, every transmission rate would become reliable. (This would mean that
the capacity of the channel is inï¬nite; although such channels should not be dis-
missed outright, in the context of an AGC the case of an inï¬nite capacity seems
impractical.)
Typically, the decodable region D(N) âŠ‚RN is represented by a ball in RN, centred
at the origin, and speciï¬ed relative to a particular distance in RN. Say, in the case
of exponentially distributed noise it is natural to select
D(N) = B(N)
â„“1 (NÎ±) =
â§
âª
â¨
âª
â©
x =
â›
âœ
â
x1
...
xN
â
âŸ
â : âˆ‘
1â‰¤jâ‰¤N
|x j| â‰¤NÎ±
â«
âª
â¬
âª
â­
the ball in the â„“1-metric. When an output-signal vector falling within distance r
from a codeword is decoded by this codeword, we have a correct decoding if (i)
the output signal falls in exactly one sphere around a codeword, (ii) the codeword
in question lies within D(N), and (iii) this speciï¬c codeword was sent. We have
possibly an error when more than one codeword falls into the sphere.

4.1 Gaussian channels and beyond
371
Î±
Figure 4.1
As in the discrete case, a more general channel is represented by a family of
(conditional) probability distributions for received vectors of length N given that
an input word x(N) âˆˆRN has been sent:
P(N)
ch ( Â· | x(N)) = P(N)
ch ( Â· |word x(N) sent), x âˆˆRN.
(4.1.8)
As before, N = 1,2,... indicates how many slots of the channel were used for trans-
mission, and we will consider the limit N â†’âˆ. Now assume that the distribution
P(N)
ch ( Â· | x(N)) is determined by a PMF f (N)
ch (y(N)| x(N)) relative to a ï¬xed measure
Î½(N) on RN:
P(N)
ch (Y(N) âˆˆA| x(N)) =
0
A f (N)
ch ( Â· | x(N))dÎ½(N)(y(N)).
(4.1.9a)
A typical assumption is that Î½(N) is a product-measure of the form
Î½(N) = Î½ Ã—Â·Â·Â·Ã—Î½ (N times);
(4.1.9b)
for instance, Î½(N) can be the Lebesgue measure on RN which is the product of
Lebesgue measures on R: dx(N) = dx1 Ã— Â·Â·Â· Ã— dxN. In the discrete case where
digits xi represent letters from an input channel alphabet A (say, binary, with
A = {0,1}), Î½ is the counting measure on A , assigning weight 1 to each sym-
bol of the alphabet. Then Î½(N) is the counting measure on A N, the set of all input
words of length N, assigning weight 1 to each such word.

372
Further Topics from Information Theory
Assuming the product-form reference measure Î½(N) (4.1.9b), we specify a mem-
oryless channel by a product form PMF f (N)
ch (y(N)| x(N)):
f (N)
ch (y(N)| x(N)) = âˆ
1â‰¤jâ‰¤N
fch(yj|x j).
(4.1.10)
Here fch(y|x) is the symbol-to-symbol channel PMF describing the impact of a
single use of the channel. For an MGC, fch(y|x) is a normal N(x,Ïƒ2). In other
words, fch(y|x) gives the PDF of a random variable Y = x+Z where Z âˆ¼N(0,Ïƒ2)
represents the â€˜white noiseâ€™ affecting an individual input value x.
Next, we turn to a codebook XM,N, the image of a one-to-one map M â†’RN
where M is a ï¬nite collection of messages (originally written in a message alpha-
bet); cf. (4.1.4). As in the discrete case, the ML decoder dML decodes the received
word Y = y(N) by maximising f (N)
ch (y| x) in the argument x = x(N) âˆˆXM,N:
dML(y) = arg max

f (N)
ch (y| x) : x âˆˆXM,N

.
(4.1.11)
The case when maximiser is not unique will be treated as an error.
Another useful example is the joint typicality (JT) decoder dJT = d(N),Îµ
JT
(see
below); it looks for the codeword x such that x and y lie in the Îµ-typical set T N
Îµ :
dJT(y) = x if x âˆˆXM,N and (x,y) âˆˆT N
Îµ .
(4.1.12)
The JT decoder is designed â€“ via a speciï¬c form of set T N
Îµ â€“ for codes generated as
samples of a random code X M,N. Consequently, for given output vector yN and a
code XM,N, the decoded word dJT(y) âˆˆXM,N may be not uniquely deï¬ned (or not
deï¬ned at all), again leading to an error. A general decoder should be understood
as a one-to-one map deï¬ned on a set K(N) âŠ†RN taking points yN âˆˆKN to points
x âˆˆXM,N; outside set K(N) it may be not deï¬ned correctly. The decodable region
K(N) is a part of the speciï¬cation of decoder d(N). In any case, we want to achieve
P(N)
ch

d(N)(Y) Ì¸= x|x sent

= P(N)
ch

Y Ì¸âˆˆK(N)|x sent

+P(N)
ch

Y âˆˆK(N),d(Y) Ì¸= x|x sent

â†’0
as N â†’âˆ. In the case of an MGC, for any code XM,N, the ML decoder from
(4.1.6) is deï¬ned uniquely almost everywhere in RN (but does not necessarily give
the right answer).
We also require that the input vector x(N) âˆˆD(N) âŠ‚RN and when x(N) Ì¸âˆˆD(N),
the result of transmission is rendered undecodable (regardless of the qualities of the
decoder used). Then the average probability of error, while using codebook XM,N
and decoder d(N), is deï¬ned by
eav(XM,N,d(N),D(N)) = 1
M âˆ‘
xâˆˆXM,N
e(x,d(N),D(N)),
(4.1.13a)

4.1 Gaussian channels and beyond
373
and the maximum probability of error by
emax(XM,N,d(N),D(N)) = max
	
e(x,d(N),D(N)) : x âˆˆXM,N

.
(4.1.13b)
Here e(x,d(N),D(N)) is the probability of error when codeword x had been trans-
mitted:
e(x,d(N),D(N)) =
â§
â¨
â©
1,
x Ì¸âˆˆD(N),
P(N)
ch

d(N)(Y) Ì¸= x|x

,
x âˆˆD(N).
(4.1.14)
In (4.1.14) the order of the codewords in the codebook XM,N does not matter;
thus XM,N may be regarded simply as a set of M points in the Euclidean space RN.
Geometrically, we want the points of XM,N to be positioned so as to maximise the
chance of correct ML-decoding and lying, as a rule, within domain D(N) (which
again leads us to a sphere-packing problem).
To this end, suppose that a number R > 0 is ï¬xed, the size of the codebook XM,N:
M =
3
2NR4
. We want to deï¬ne a reliable transmission rate as N â†’âˆin a fashion
similar to how it was done in Section 1.4.
Deï¬nition 4.1.2
Value R > 0 is called a reliable transmission rate with regional
constraint D(N) if, with M =
3
2NR4
, there exist a sequence {XM,N} of codebooks
XM,N âŠ‚RN and a sequence {d(N)} of decoders d(N) : RN â†’RN such that
lim
Nâ†’âˆeav(XM,N,d(N),D(N)) = 0.
(4.1.15)
Remark 4.1.3
It is easy to verify that a transmission rate R reliable in the sense of
average error-probability eav(XM,N,d(N),D(N)) is reliable for the maximum error-
probability emax(XM,N,d(N),D(N)). In fact, assume that R is reliable in the sense of
Deï¬nition 4.1.2, i.e. in the sense of the average error-probability. Take a sequence
{XM,N} of the corresponding codebooks with M = âŒˆ2RNâŒ‰and a sequence {dN} of
the corresponding decoding rules. Divide each code XN into two halves, X (0)
N
and
X (1)
N , by ordering the codewords in the non-decreasing order of their probabilities
of erroneous decoding and listing the ï¬rst M(0) = âŒˆM/2âŒ‰codewords in X (0)
N
and
the rest, M(1) = M âˆ’M(0), in X (1)
N . Then, for the sequence of codes {X (0)
M,N}:
(i) the information rate approaches the value R as N â†’âˆas
1
N logM(0) â‰¥R+O(Nâˆ’1);
(ii) the maximum error-probability, while using the decoding rule dN,
Pmax
e

X (0)
N ,dN

â‰¤
1
M(1)
âˆ‘
x(N)âˆˆX (1)
N
Pe(x(N),dN) â‰¤
M
M(1) Pav
e (XN,dN).

374
Further Topics from Information Theory
Since M/M(1) â‰¤2, the RHS tends to 0 as N â†’âˆ. We conclude that R is
a reliable transmission rate for the maximum error-probability. The converse
assertion, that a reliable transmission rate R in the sense of the maximum error-
probability is also reliable in the sense of the average error-probability, is ob-
vious.
Next, the capacity of the channel is the supremum of reliable transmission rates:
C = sup

R > 0 : R is reliable

;
(4.1.16)
it varies from channel to channel and with the shape of constraining domains.
It turns out (cf. Theorem 4.1.9 below) that for the MGC, under the average power
constraint threshold Î± (see (4.1.7)), the channel capacity C(Î±,Ïƒ2) is given by the
following elegant expression:
C(Î±,Ïƒ2) = 1
2 log2

1+ Î±
Ïƒ2

.
(4.1.17)
Furthermore, like in Section 1.4, the capacity C(Î±,Ïƒ2) is achieved by a se-
quence of random codings where codeword x(i) = (X1(i),...,XN(i)) has IID
components Xj(i) âˆ¼N(0,Î± âˆ’ÎµN), j = 1,...,N, i = 1,...,M, with ÎµN â†’0 as
N â†’âˆ. Although such random codings do not formally obey the constraint
(4.1.7) for ï¬nite N, it is violated with a vanishing probability as N â†’âˆ(since
limsupNâ†’âˆP

max

1
N âˆ‘
1â‰¤jâ‰¤N
Xj(i)2 : 1 â‰¤i â‰¤M

â‰¤Î±

= 1 with a proper choice
of ÎµN). Consequently, the average error-probability (4.1.13a) goes to 0 (of course,
for a random coding the error-probability becomes itself random).
Example 4.1.4
Next, we discuss an AGC with coloured Gaussian noise. Let a
codevector x = (x1,...,xN) have multi-dimensional entries
xj =
â›
âœ
â
xj1
...
xjk
â
âŸ
â âˆˆRk,
1 â‰¤j â‰¤N,
and the components Zj of the noise vector
Z =
â›
âœ
â
Z1
...
ZN
â
âŸ
â 

4.1 Gaussian channels and beyond
375
are also random vectors of dimension k:
Z j =
â›
âœ
â
Z j1
...
Z jk
â
âŸ
â .
For instance, Z1,...,ZN may be IID N(0,Î£) (with k-variate normal), where Î£ is a
given k Ã—k covariance matrix.
The â€˜colouredâ€™ model arises when one uses a system of k scalar Gaussian chan-
nels in parallel. Here, a scalar signal xj1 is sent through channel 1, xj2 through
channel 2, etc., at the jth use of the system. A reasonable assumption is that at
each use the scalar channels produce jointly Gaussian noise; different channels
may be independent (with matrix Î£ being k Ã—k diagonal) or dependent (when Î£ is
a general positive-deï¬nite k Ã—k matrix).
Here a codebook, as before, is an (ordered or unordered) collection
XM,N =
(
x(1),...,x(M)
)
where each codeword x(i) is a â€˜multi-vectorâ€™
(x1(i),...,xN(i))T âˆˆRkÃ—N := Rk Ã—Â·Â·Â·Ã—Rk. Let Q be a positive-deï¬nite k Ã—k ma-
trix commuting with Î£: QÎ£ = Î£Q. The power constraint is now
1
N âˆ‘
1â‰¤jâ‰¤N
I
xj(i),Qxj(i)
J
â‰¤Î±.
(4.1.18)
The formula for the capacity of an AGC with coloured noise is, not surprisingly,
more complicated. As Î£Q = QÎ£, matrices Î£ and Q may be simultaneously diag-
onalised. Let Î»i and Î³i, i = 1,...,k, be the eigenvalues of Î£ and Q, respectively
(corresponding to the same eigenvectors). Then
C(Î±,Q,Î£) = 1
2 âˆ‘
1â‰¤lâ‰¤k
log2

1+ (Î½Î³âˆ’1
l
âˆ’Î»l)+
Î»l

,
(4.1.19)
where (Î½Î³âˆ’1
l
âˆ’Î»l)+ = max

Î½Î³âˆ’1
l
âˆ’Î»l,0

. In other words, (Î½Î³âˆ’1
l
âˆ’Î»l)+ are the
eigenvalues of the matrix

Î½Qâˆ’1âˆ’Î£

+ representing the positive-deï¬nite part of the
Hermitian matrix Î½Qâˆ’1 âˆ’Î£. Next, Î½ = Î½(Î±) > 0 is determined from the condition
tr

Î½Iâˆ’QÎ£

+

= Î±.
(4.1.20)
The positive-deï¬nite part

Î½Iâˆ’QÎ£

+ is in turn deï¬ned by

Î½Iâˆ’QÎ£

+ = Î +

Î½Iâˆ’QÎ£

Î +
where Î + is the orthoprojection (in Rk) onto the subspace spanned by the eigen-
vectors of QÎ£ with eigenvalues Î³lÎ»l < Î½. In (4.1.20) tr

Î½I âˆ’QÎ£

+

â‰¥0 (since
tr AB â‰¥0 for all pair of positive-deï¬nite matrices), equals 0 for Î½ = 0 (as

376
Further Topics from Information Theory

âˆ’QÎ£

+ = 0) and monotonically increases with Î½ to +âˆ. Therefore, for any given
Î± > 0, (4.1.20) determines the value of Î½ = Î½(Î±) uniquely.
Though (4.1.19) looks much more involved than (4.1.17) both expressions are
corollaries of two facts: (i) the capacity can be identiï¬ed as the maximum of the
mutual entropy between the (random) input and output signals, just as in the dis-
crete case (cf. Sections 1.3 and 1.4), and (ii) the mutual information in the case
of a Gaussian noise (white or coloured) is attained when the input signal is itself
Gaussian whose covariance solves an auxiliary optimisation problem. In the case
of (4.1.17) this optimisation problem is rather simple, while for (4.1.19) it is more
complicated (but still has a transparent meaning).
Correspondingly, the random encoding achieving the capacity C(Î±;Q;Î£) is
where signals Xj(i), 1 â‰¤j â‰¤N, i = 1,...,M, are IID, and Xj(i) âˆ¼N(0,A âˆ’ÎµNI)
where A is the kÃ—k positive-deï¬nite matrix maximising the determinant det(A+Î£)
subject to the constraint tr QA = Î±; such a matrix turns out to be of the form

Î½Qâˆ’1 âˆ’Î£

+. The random encoding provides a convenient tool for calculating the
capacity in various models. We will discuss a number of such models in Worked
Examples.
The notable difference emerging for channels with continuously distributed
noise is that the entropy should be replaced â€“ when appropriate â€“ with the differen-
tial entropy. Recall the differential entropy introduced in Section 1.5. The mutual
entropy between two random variables X and Y with the joint PMF fX,Y(x,y) rel-
ative to a reference measure Î¼ Ã— Î½ and marginal PMFs fX(x) =
1 fX,Y(x,y)Î½(dy)
and fY(y) =
1 fX,Y(x,y)Î¼(dx) is
I(X : Y) = E log fX,Y(X,Y)
fX(X) fY(Y)
=
0
fX,Y(x,y)log fX,Y(x,y)
fX(x) fY(y)Î¼(dx)Î½(dy).
A similar deï¬nition works when X and Y are replaced by random vectors X =
(X1,...,XN) and Y = (Y1,...,YNâ€²) (or even multi-vectors where â€“ as in Example
4.1.4 â€“ components Xj and Yj are vectors themselves):
I(X(N) : Y(Nâ€²)) = E log
fX(N),Y(Nâ€²)(X(N),Y(Nâ€²))
fX(N)(X(N)) fY(Nâ€²)(Y(Nâ€²)).
(4.1.21a)
Here fX(N)(x(N)) and fY(Nâ€²)(y(Nâ€²)) are the marginal PMFs for X(N) and Y(Nâ€²) (i.e.
joint PMFs for components of these vectors).

4.1 Gaussian channels and beyond
377
Speciï¬cally, if N = Nâ€², X(N) represents a random input and Y(N) = X(N) +Z(N)
the corresponding random output of a channel with a (random) probability of error:
E(x(N),D(N)) =
â§
â¨
â©
1,
x(N) Ì¸âˆˆD(N),
P(N)
ch

dML(Y(N)) Ì¸= x(N)|x(N)
,
x(N) âˆˆD(N);
cf. (4.1.14). Furthermore, we are interested in the expected value
E (PX(N);D(N)) = E
	
E(X(N),D(N))

.
(4.1.21b)
Next, given Îµ > 0, we can deï¬ne the supremum of the mutual information per
signal (i.e. per a single use of the channel), over all input probability distributions
PX(N) with E (PX(N),D(N)) â‰¤Îµ:
CÎµ,N = 1
N sup

I(X(N) : Y(N)) : E (PX(N),D(N)) â‰¤Îµ

,
(4.1.22)
CÎµ = limsup
Nâ†’âˆ
CÎµ,N C = liminf
Îµâ†’0 CÎµ.
(4.1.23)
We want to stress that the supremum in (4.1.22) should be taken over all proba-
bility distributions PX(N) of the input word X(N) with the property that the expected
error-probability is â‰¤Îµ, regardless of whether these distributions are discrete or
continuous or mixed (contain both parts). This makes the correct evaluation of
CN,Îµ quite difï¬cult. However, the limiting value C is more amenable, at least in
some important examples.
We are now in a position to prove the converse part of the Shannon second
coding theorem:
Theorem 4.1.5
(cf. Theorems 1.4.14 and 2.2.10.) Consider a channel given by
a sequence of probability distributions Pch

Â· | x(N) sent

for the random output
words Y(N) and decodable domains D(N). Then quantity C from (4.1.22), (4.1.23)
gives an upper bound for the capacity:
C â‰¤C.
(4.1.24)
Proof
Let R be a reliable transmission rate and {XM,N} be a sequence of code-
books with M = â™¯XM,N âˆ¼2NR for which lim
Nâ†’âˆeav(XM,N,D(N)) = 0. Consider the
pair (x,dML(y)) where (i) x = x(N)
eq is the random input word equidistributed over
XM,N, (ii) Y = Y(N) is the received word and (iii) dML(y) is the codeword guessed
while using the ML decoding rule dML after transmission. Words x and dML(Y) run

378
Further Topics from Information Theory
jointly over XM,N, i.e. have a discrete-type joint distribution. Then, by the gener-
alised Fano inequality (1.2.23),
hdiscr(X|d(Y)) â‰¤1+log(M âˆ’1) âˆ‘
XâˆˆXM,N
P(x = x,dML(Y) Ì¸= x)
â‰¤1+ NR
M
âˆ‘
xâˆˆXM,N
Pch(dML(Y) Ì¸= x|x sent)
= 1+NReav(XM,N,D(N)) := NÎ¸N,
where Î¸N â†’0 as N â†’âˆ. Next, with h(X(N)
eq ) = logM, we have NRâˆ’1 â‰¤h(X(N)
eq ).
Therefore,
R â‰¤1+h(X(N)
eq )
N
= 1
N I(X(N)
eq : d(Y(N)))+ 1
N h(X(N)
eq |d(Y(N)))
+ 1
N â‰¤1
N I(x(N)
eq : Y(N))+Î¸N.
For any given Îµ > 0, for N sufï¬ciently large, the average error-probability will
satisfy eav(XM,N,D(N)) < Îµ. Consequently, R â‰¤CÎµ,N, for N large enough. (Because
the equidistribution over a codebook XM,N with eav(XM,N,D(N)) gives a speciï¬c
example of an input distribution PX(N) with E

PX(N),D(N)) â‰¤Îµ.) Thus, for all Îµ > 0,
R â‰¤CÎµ, implying that the transition rate R â‰¤C. Therefore, C â‰¤C, as claimed.
The bound C â‰¤C in (4.1.24) becomes exact (with C =C) in many interesting sit-
uations. Moreover, the expression for C simpliï¬es in some cases of interest. For ex-
ample, for an MAGC instead of maximising the mutual information I(X(N) : Y(N))
for varying N it becomes possible to maximise I(X :Y), the mutual information be-
tween single input and output signals subject to an appropriate constraint. Namely,
for an MAGC,
C = C = sup
	
I(X : Y) : EX2 < Î±

.
(4.1.25a)
The quantity sup
	
I(X : Y) : EX2 â‰¤Î±

is often called the information capacity of
an MAGC, under the square-power constraint Î±. Moreover, for a general AGC,
C = C = lim
Nâ†’âˆ
1
N sup

I(X(N) : Y(N)) : 1
N âˆ‘
1â‰¤jâ‰¤N
EX2
j < Î±

.
(4.1.25b)
Example 4.1.6
Here we estimate the capacity C(Î±,Ïƒ2) of an MAGC with addi-
tive white Gaussian noise of variance Ïƒ2, under the average power constraint (with
D(N) = B(N)((NÎ±)1/2) (cf. Example 4.1.1.), i.e. bound from above the right-hand
side of (4.1.25b).

4.1 Gaussian channels and beyond
379
Given an input distribution PX(N), we write
I(X(N) : Y(N)) = h(Y(N))âˆ’h(Y(N)|X(N))
= h(Y(N))âˆ’h(Z(N))
â‰¤âˆ‘
1â‰¤jâ‰¤N
h(Yj)âˆ’h(Z(N))
= âˆ‘
1â‰¤jâ‰¤N

h(Yj)âˆ’h(Z j)

.
(4.1.26)
Denote by Î±2
j = EX2
j the second moment of the single-input random variable Xj,
the jth entry of the random input vector X(N). Then the corresponding random
output random variable Yj has
EY 2
j = E(Xj +Z j)2 = EX2
j +2EXjZ j +EZ2
j = Î±2
j +Ïƒ2,
as Xj and Z j are independent and EZj = 0.
Note that for a Gaussian channel, Yj has a continuous distribution (with the PDF
fYj(y) given by the convolution
1 Ï†Ïƒ2(xâˆ’y)dFXj(x) where Ï†Ïƒ2 is the PDF of Z j âˆ¼
N(0,Ïƒ2)). Consequently, the entropies ï¬guring in (4.1.26) and â€“ implicitly â€“ in
(4.1.25a,b) are the differential entropies. Recall that for a random variable Yj with
a PDF fYj, under the condition EY 2
j â‰¤Î±2
j + Ïƒ2, the maximum of the differential
entropy h(Yj) â‰¤1
2 log2[2Ï€e(Î±2
j +Ïƒ2)]. In fact, by Gibbs,
h(Yj) = âˆ’
0
fYj(y)log2 fYj(y)dy
â‰¤âˆ’
0
fYj(y)log2 Ï†Î±2
j +Ïƒ2(y)dy
= 1
2 log2
	
2Ï€(Î±2
j +Ïƒ2)

+
log2 e
2(Î±2
j +Ïƒ2)EY 2
j
â‰¤1
2 log2
	
2Ï€e(Î±2
j +Ïƒ2)

,
and consequently,
I(Xj : Yj) = h(Yj)âˆ’h(Z j)
â‰¤log2[2Ï€e(Î±2
j +Ïƒ2)]âˆ’log2(2Ï€eÏƒ2)
= log2

1+
Î±2
j
Ïƒ2

,
with equality iff Yj âˆ¼N(0,Î±2
j +Ïƒ2).

380
Further Topics from Information Theory
The bound
âˆ‘
1â‰¤jâ‰¤N
EX2
j =
âˆ‘
1â‰¤jâ‰¤N
Î±2
j < NÎ± in (4.1.25b) implies, by the law of large
numbers, that lim
Nâ†’âˆPX(N)

B(N)(
âˆš
NÎ±)

= 1. Moreover, for any input probability
distribution PX(N) with EX2
j â‰¤Î±2
j , 1 â‰¤j â‰¤N, we have that
1
N I(X(N) : Y(N)) â‰¤1
2N âˆ‘
1â‰¤jâ‰¤N
log2

1+
Î±2
j
Ïƒ2

.
The Jensen inequality, applied to the concave function x â†’log2(1+x), implies
1
2N âˆ‘
1â‰¤jâ‰¤N
log2

1+
Î±2
j
Ïƒ2

â‰¤1
2 log2

1+ 1
N âˆ‘
1â‰¤jâ‰¤N
Î±2
j
Ïƒ2

â‰¤1
2 log2

1+ Î±
Ïƒ2

.
Therefore, in this example, the information capacity C, taken as the RHS of
(4.1.25b), obeys
C â‰¤1
2 log2

1+ Î±
Ïƒ2

.
(4.1.27)
After establishing Theorem 4.1.8, we will be able to deduce that the capacity
C(Î±,Ïƒ2) equals the RHS, conï¬rming the answer in (4.1.17).
Example 4.1.7
For the coloured Gaussian noise the bound from (4.1.26) can be
repeated:
I(X(N) : Y(N)) â‰¤âˆ‘
1â‰¤jâ‰¤N
[h(Yj)âˆ’h(Z j)].
Here we work with the mixed second-order moments for the random vectors of
input and output signals Xj and Yj = Xj +Z j:
Î±2
j = EâŸ¨Xj,QXjâŸ©, EâŸ¨Yj,QYjâŸ©= Î±2
j +tr (QÎ£), 1
N âˆ‘
1â‰¤jâ‰¤N
Î±2
j â‰¤Î±.
In this calculation we again made use of the fact that Xj and Z j are independent
and the expected value EZj = 0.
Next, as in the scalar case, 1
N I(X(N) : Y(N)) does not exceed the difference
h(Y) âˆ’h(Z) where Z âˆ¼N(0,Î£) is the coloured noise vector and Y = X + Z is
a multivariate normal distribution maximising the differential entropy under the
trace restriction. Formally:
1
N I(X(N) : Y(N)) â‰¤h(Î±,Q,Î£)âˆ’h(Z)

4.1 Gaussian channels and beyond
381
where K is the covariance matrix of a signal, and
h(Î±,Q,Î£) = 1
2 max
*
log
	
(2Ï€)kedet(K+Î£)

:
K positive-deï¬nite k Ã—k matrix with tr(QK) â‰¤Î±
+
.
Write Î£ in the diagonal form Î£ = CÎ›CT where C is an orthogonal and Î› the diag-
onal k Ã—k matrix formed by the eigenvalues of Î£:
Î› =
â›
âœ
âœ
âœ
â
Î»1
0
...
0
0
Î»2
...
0
0
0
...
0
0
0
...
Î»k
â
âŸ
âŸ
âŸ
â .
Write CTKC = B and maximise det(B+Î›) subject to the constraint
B positive-deï¬nite and tr(Î“B) â‰¤Î± where Î“ = CTQC.
By the Hadamard inequality of Worked Example 1.5.10, det(B + Î›) â‰¤
âˆ
1â‰¤iâ‰¤k
(Bii +Î»i), with equality iff B is diagonal (i.e. matrices Î£ and K have the same
eigenbasis), and B11 = Î²1,...,Bkk = Î²k are the eigenvalues of K. As before, as-
sume QÎ£ = Î£Q, then tr (Î“B) =
âˆ‘
1â‰¤iâ‰¤k
Î³iÎ²i. So, we want to maximise the product
âˆ
1â‰¤iâ‰¤k
(Î²i +Î»i), or equivalently, the sum
âˆ‘
1â‰¤iâ‰¤k
log(Î²i +Î»i), subject to Î²1,...,Î²k â‰¥0 and âˆ‘
1â‰¤iâ‰¤k
Î³iÎ²i â‰¤Î±.
If we discard the regional constraints Î²1,...,Î²k â‰¥0, the Lagrangian
L (Î²1,...,Î²k;Îº) = âˆ‘
1â‰¤iâ‰¤k
log(Î²i +Î»i)+Îº

Î± âˆ’âˆ‘
1â‰¤iâ‰¤k
Î³iÎ²i

is maximised at
1
Î²i +Î»i
= ÎºÎ³i, i.e. Î²i = 1
ÎºÎ³i
âˆ’Î»i, i = 1,...,k.
To satisfy the regional constraint, we take
Î²i =
 1
ÎºÎ³i
âˆ’Î»i

+
, i = 1,...,k,
and adjust Îº > 0 so that
âˆ‘
1â‰¤iâ‰¤k
 1
Îº âˆ’Î³iÎ»i

+
= Î±.
(4.1.28)

382
Further Topics from Information Theory
This yields that the information capacity C(Î±,Q,Î£) obeys
C(Î±,Q,Î£) â‰¤1
2 âˆ‘
1â‰¤lâ‰¤k
log2

1+ (Î½Î³âˆ’1
l
âˆ’Î»l)+
Î»l

,
(4.1.29)
where the RHS comes from (4.1.28) with Î½ = 1/Îº. Again, we will show that the
capacity C(Î±,Q,Î£) equals the last expression, conï¬rming the answer in (4.1.19).
We now pass to the direct part of the second Shannon coding theorem for general
channels with regional restrictions. Although the statement of this theorem differs
from that of Theorems 1.4.15 and 2.2.1 only in the assumption of constraints upon
the codewords (and the proof below is a mere repetition of that of Theorem 1.4.15),
it is useful to put it in the formal context.
Theorem 4.1.8
Let a channel be speciï¬ed by a sequence of conditional probabili-
ties P(N)
ch

Â· |x(N) sent

for the received word Y(N) and a sequence of decoding con-
straints x(N) âˆˆD(N) for the input vector. Suppose that probability P(N)
ch

Â· |x(N) sent

is given by a PMF fch(y(N)|x(N) sent) relative to a reference measure Î½(N). Given
c > 0, suppose that there exists a sequence of input probability distributions PX(N)
such that
(i) lim
Nâ†’âˆPX(N)(D(N)) = 1,
(ii) the distribution PX(N) is given by a PMF fX(N)(x(N)) relative to a reference
measure Î¼(N),
(iii) the following convergence in probability holds true: for all Îµ > 0,
limNâ†’âˆPX(N),Y(N)

T N
Îµ

= 1,
T N
Îµ =

1
N log+
fx(N),Y(N)(x(N),Y(N))
fX(N)(x(N)) fY(N)(Y(N)) âˆ’c
 â‰¤Îµ

,
(4.1.30a)
where
fX(N),Y(N)(x(N),y(N)) = fX(N)(x(N)) fch(y(N)|x(N) sent),
fY(N)(y(N)) =
0
fX(N)( xN) fch(y(N)| x(N) sent)Î¼Ã—N 
d x(N)
.
(4.1.30b)
Then the capacity of the channel satisï¬es C â‰¥c.
Proof
Take R < c and consider a random codebook
*
xN(1),...,xN(M)
+
, with
M âˆ¼2NR, composed by IID codewords where each codeword xN( j) is drawn ac-
cording to PN = PxN. Suppose that a (random) codeword xN( j) has been sent and a

4.1 Gaussian channels and beyond
383
(random) word YN = YN( j) received, with the joint PMF fX(N),Y(N) as in (4.1.30b).
We take Îµ > 0 and decode YN by using joint typicality:
dJT(YN) = xN(i) when xN(i) is the only vector among
xN(1),...,xN(M) such that (xN(i),YN) âˆˆT N
Îµ .
Here set T N
Îµ is speciï¬ed in (4.1.30a).
Suppose a random vector xN( j) has been sent. It is assumed that an error occurs
every time when
(i) xN( j) Ì¸âˆˆD(N), or
(ii) the pair (xN( j),YN) Ì¸âˆˆT N
Îµ , or
(iii) (xN(i),YN) âˆˆT N
Îµ for some i Ì¸= j.
These possibilities do not exclude each other but if none of them occurs then
(a) xN( j) âˆˆD(N) and
(b) x( j) is the only word among xN(1),...,xN(M) with (xN( j),YN) âˆˆT N
Îµ .
Therefore, the JT decoder will return the correct result. Consider the average error-
probability
EM(PN) = 1
M âˆ‘
1â‰¤jâ‰¤M
E( j,PN)
where E( j,PN) is the probability that any of the above possibilities (i)â€“(iii) occurs:
E( j,PN) = P
*
xN( j) Ì¸âˆˆD(N)+
âˆª
*
(xN( j),YN) Ì¸âˆˆT N
Îµ
+
âˆª
*
(xN(i),YN) âˆˆT N
Îµ
for some i Ì¸= j
+
= E1

xN( j) Ì¸âˆˆD(N)
+E1

xN( j) âˆˆD(N), dJT(YN) Ì¸= xN( j)

.
(4.1.31)
The symbols P and E in (4.1.31) refer to (1) a collection of IID input vectors
xN(1),...,xN(M), and (2) the output vector YN related to xN( j) by the action of
the channel. Consequently, YN is independent of vectors xN(i) with i Ì¸= j. It is in-
structive to represent the corresponding probability distribution P as the Cartesian
product; e.g. for j = 1 we refer in (4.1.31) to
P = PxN(1),YN(1) Ã—PxN(2) Ã—Â·Â·Â·Ã—PxN(M)
where PxN(1),YN(1) stands for the joint distribution of the input vector xN(1) and the
output vector YN(1), determined by the joint PMF
fxN(1),YN(1)(xN,yN) = fxN(1)(xN) fch(yN|xN sent).

384
Further Topics from Information Theory
By symmetry, E( j,PN) does not depend on j, thus in the rest of the argument we
can take j = 1. Next, probability E(1,PN) does not exceed the sum of probabilities
P

xN(1) Ì¸âˆˆD(N)
+P

(xN(1),YN) Ì¸âˆˆT N
Îµ

+
M
âˆ‘
i=2
P

(xN(i),YN) âˆˆT N
Îµ

.
Thanks to the condition that lim
Nâ†’âˆPx(N)(D(N)) = 1, the ï¬rst summand vanishes as
N â†’âˆ. The second summand vanishes, again in the limit N â†’âˆ, because of
(4.1.30a). It remains to estimate the sum
M
âˆ‘
i=2
P

(xN(i),YN) âˆˆT N
Îµ

.
First, note that, by symmetry, all summands are equal, so
M
âˆ‘
i=2
P

(xN(i),YN) âˆˆT N
Îµ

=

2âŒˆNRâŒ‰âˆ’1

P

(xN(2),YN) âˆˆT N
Îµ

.
Next, by Worked Example 4.2.3 (see (4.2.9) below)
P

(xN(2),YN) âˆˆT N
Îµ

â‰¤2âˆ’N(câˆ’3Îµ)
and hence
m
âˆ‘
i=2
P

(xN(i),YN) âˆˆT N
Îµ

â‰¤2N(Râˆ’c+3Îµ)
which tends to 0 as N â†’âˆwhen Îµ < (câˆ’R)/3.
Therefore, for R < c, lim
Nâ†’âˆEM(PN) = 0. But EM(PN) admits the representation
Em(PN) = EPxN(1)Ã—Â·Â·Â·Ã—PxN(M)

1
M âˆ‘
1â‰¤jâ‰¤M
E( j)

where quantity E( j) represents the error-probability as deï¬ned in (4.1.14):
E( j) =
'
1,
xN Ì¸âˆˆD(N),
Pch

d(N),Îµ
JT
(YN) Ì¸= xN( j)|xN( j) sent

,
xN âˆˆD(N).
We conclude that there exists a sequence of sample codebooks XM,N such that
the average error-probability
1
M âˆ‘
xâˆˆXM,N
e(x) â†’0

4.1 Gaussian channels and beyond
385
where e(x) = e(x,XM,N,D(N),d(N),Îµ
JT
) is the error-probability for the input word x
in code XM,N, under the JT decoder and with regional constraint speciï¬ed by D(N):
e(x) =
â§
â¨
â©
1,
xN Ì¸âˆˆD(N),
Pch

d(N),Îµ
JT
(YN) Ì¸= x|x sent

,
xN âˆˆD(N).
Hence, R is a reliable transmission rate in the sense of Deï¬nition 4.1.2. This com-
pletes the proof of Theorem 4.1.8.
We also have proved in passing the following result.
Theorem 4.1.9
Assume that the conditions of Theorem 4.1.5 hold true. Then,
for all R < C, there exists a sequence of codes XM,N of length N and size M âˆ¼2RN
such that the maximum probability of error tends to 0 as N â†’âˆ.
Example 4.1.10
Theorem 4.1.8 enables us to specify the expressions in (4.1.17)
and (4.1.19) as the true values of the corresponding capacities (under the ML rule):
for a scalar white noise of variance Ïƒ2, under an average input power constraint
âˆ‘
1â‰¤jâ‰¤N
x2
j â‰¤NÎ±,
C(Î±,Ïƒ2) = 1
2 log

1+ Î±
Ïƒ2

,
for a vector white noise with variances Ïƒ2 = (Ïƒ2
1 ,...,Ïƒ2
k ), under the constraint
âˆ‘
1â‰¤jâ‰¤N
xT
j xj â‰¤NÎ±,
C(Î±,Ïƒ2) = 1
2 âˆ‘
1â‰¤iâ‰¤k
log

1+ (Î½ âˆ’Ïƒ2
i )+
Ïƒ2
i

, where âˆ‘
1â‰¤iâ‰¤k
(Î½ âˆ’Ïƒ2
i )+ = Î±2,
and for the coloured vector noise with a covariance matrix Î£, under the constraint
âˆ‘
1â‰¤jâ‰¤N
xT
j Qxj â‰¤NÎ±,
C(Î±,Q,Î£) = 1
2 âˆ‘
1â‰¤iâ‰¤k
log

1+ (Î½Î³âˆ’1
i
âˆ’Î»i)+
Î»i

,
where
âˆ‘
1â‰¤iâ‰¤k
(Î½ âˆ’Î³iÎ»i)+ = Î±.
Explicitly, for a scalar white noise we take the random coding where the signals
Xj(i), 1 â‰¤j â‰¤N, 1 â‰¤i â‰¤M = âŒˆ2NRâŒ‰, are IID N(0,Î± âˆ’Îµ). We have to check the
conditions of Theorem 4.1.5 in this case: as N â†’âˆ,
(i) lim
Nâ†’âˆP(x(N)(i) âˆˆB(N)(
âˆš
NÎ±), for all i = 1,...,M) = 1;

386
Further Topics from Information Theory
(ii) lim
Îµâ†’0 lim
Nâ†’âˆÎ¸N = C(Î±,Ïƒ2) in probability where
Î¸N = 1
N âˆ‘
1â‰¤jâ‰¤M
log
P(X,Y)
PX(X)PY(Y).
First, property (i):
P

x(N)(i) Ì¸âˆˆB(N)(
âˆš
NÎ±), for some i = 1,...,M

â‰¤P

1
NM âˆ‘
1â‰¤iâ‰¤M âˆ‘
1â‰¤jâ‰¤N
Xj(i)2 â‰¥Î±

= P

1
NM âˆ‘
1â‰¤iâ‰¤M âˆ‘
1â‰¤jâ‰¤N
(Xj(i)2 âˆ’Ïƒ2) â‰¥Îµ

â‰¤E(X2 âˆ’Ïƒ2)2

1
NMÎµ2

â†’0.
Next, (ii): since pairs (Xj,Yj) are IID, we apply the law of large numbers and
obtain that
Î¸N â†’Elog
P(X,Y)
PX(X)PY(Y) = I(X1 : Y1).
But
I(X1 : Y1) = h(Y1)âˆ’h(Y1|X1)
= 1
2 log
	
2Ï€e(Î± âˆ’Îµ +Ïƒ2)

âˆ’1
2 log

2Ï€eÏƒ2
= 1
2 log

1+ Î± âˆ’Îµ
Ïƒ2

â†’C(Î±,Ïƒ2) as Îµ â†’0.
Hence, the capacity equals C(Î±,Ïƒ2), as claimed. The case of coloured noise is
studied similarly.
Remark 4.1.11
Introducing a regional constraint described by a domain D does
not mean one has to secure that the whole code X should lie in D. To guarantee
that the error-probability Pav
e (X ) â†’0 we only have to secure that the â€˜majorityâ€™
of codewords x(i) âˆˆX belong to D when the codeword-length N â†’âˆ.
Example 4.1.12
Here we consider a non-Gaussian additive channel, where the
noise vector
Z =
â›
âœ
â
Z1
...
ZN
â
âŸ
â 

4.1 Gaussian channels and beyond
387
has two-side exponential IID components Zj âˆ¼(2)Exp(Î»), with the PDF
fZj(z) = 1
2Î»eâˆ’Î»|z|, âˆ’âˆ< z < âˆ,
where Exp denotes the exponential distribution, Î» > 0 and E|Zj| = 1/Î» (see PSE I,
Appendix). Again we will calculate the capacity under the ML rule and with a
regional constraint x(N) âˆˆÅ(NÎ±) where
Å(NÎ±) =
(
x(N) âˆˆRN : âˆ‘
1â‰¤jâ‰¤N
|x j| â‰¤NÎ±
)
.
First, observe that if the random variable X has E|X| â‰¤Î± and the random variable
Z has E|Z| â‰¤Î¶ then E|X +Z| â‰¤Î± +Î¶. Next, we use the fact that a random variable
Y with PDF fY and E|Y| â‰¤Î· has the differential entropy
h(Y) â‰¤2+log2 Î·; with equality iff Y âˆ¼(2)Exp(1/Î·).
In fact, as before, by Gibbs
h(Y) = âˆ’
0
fY(y)log fY(y)dy
â‰¤âˆ’
0
fY(y)logÏ† (2)Exp(1/Î·)(y)dy
= 1+ 1
Î·
0
fY(y)|y|dy+logÎ·
= 1+logÎ· â‰¤2+logÎ·
+ 1
Î· E|Y|
= âˆ’
0
Ï† (2)Exp(1/Î·)(y)logÏ† (2)Exp(1/Î·)(y)dy,
and the equalities are achieved only when fY = Ï† (2)Exp(1/Î·).
Then, by the converse part of the SSCT,
1
N I(x(N) : Y(N)) = 1
N âˆ‘h(Yj)âˆ’h(Z j)
â‰¤1
N âˆ‘
	
2+log2(Î± j +Î» âˆ’1)âˆ’2+log2(Î»)

= 1
N âˆ‘log2

1+Î± jÎ»

â‰¤log2

1+Î±Î»

.
The same arguments as before establish that the RHS gives the capacity of the
channel.

388
Further Topics from Information Theory
A
_
A
2b
= 13
0
M
=
m
6
C
inf= ln 7
Figure 4.2
Worked Example 4.1.13
Next, we consider a channel with an additive uniform
noise, where the noise random variable Z âˆ¼U(âˆ’b,b) with b > 0 representing the
limit for the noise amplitude. Let us choose the region constraint for the input signal
as a ï¬nite set A âŠ‚R (an input â€˜alphabetâ€™) of the form A = {a,a+b,...,a+(M âˆ’
1)b}. Compute the information capacity of the channel:
Cinf = sup
	
I(X : Y) : pX(A ) = 1,Y = X +Z

.
Solution Because of the shift-invariance, we can assume that a = âˆ’A and a+Mb
=A where 2A = Mb is the â€˜widthâ€™ of the input signal set. The formula I(X : Y) =
h(Y) âˆ’h(Y|X) where h(Y|X) = h(Z) = ln(2b) (in nats) shows that we must max-
imise the output signal entropy h(Y). The limits for Y are âˆ’Aâˆ’b â‰¤Y â‰¤A+b, so
the distribution PY must be as close to uniform U(âˆ’Aâˆ’b,A+b) as possible.
First, suppose M is odd: â™¯A = 2m+1, with
A = {0,Â±A/m,Â±2A/m,...,Â±A} and b = A/m.
That is, the points of A partition the interval [âˆ’A,A] into 2m intervals of length
A/m; the â€˜extendedâ€™ interval [âˆ’Aâˆ’b,A+b] contains 2(m+1) such intervals. The
maximising probability distribution PX can be spotted without calculations: it as-
signs equal probabilities 1/(m+1) to m+1 points
âˆ’A,âˆ’A+2b,...,Aâˆ’2b,A.
In other words, we â€˜cross offâ€™ every second â€˜letterâ€™ from A and use the remaining
letters with equal probabilities.
In fact, with PX(âˆ’A) = PX(âˆ’A + 2b) = Â·Â·Â· = PX(A), the output signal PDF fY
assigns the value [2b(m+1)]âˆ’1 to every point y âˆˆ[âˆ’Aâˆ’b,A+b]. In other words,
Y âˆ¼U(âˆ’A âˆ’b,A + b) as required. The information capacity Cinf in this case is
equal (in nats) to
ln(2A+2b)âˆ’ln2b = ln(1+m).
(4.1.32)

4.1 Gaussian channels and beyond
389
Say, for M = 3 (three input signals, at âˆ’A, 0, A, and b = A), Cinf = ln2. For
M = 5 (ï¬ve input signals, at âˆ’A, âˆ’A/2, 0, A/2, A, and b = A/2), Cinf = ln3. See
Figure 4.2 for M = 13.
Remark 4.1.14
It can be proved that (4.1.32) gives the maximum mutual infor-
mation I(X : Y) between the input and output signals X and Y = X + Z when (i)
the noise random variable Z âˆ¼U(âˆ’b,b) is independent of X and (ii) X has a gen-
eral distribution supported on the interval [âˆ’A,A] with b = A/m. Here, the mutual
information I(X : Y) is deï¬ned according to Kolmogorov:
I(X : Y) = sup
Î¾,Î·
I(XÎ¾ : YÎ·)
(4.1.33)
where the supremum is taken over all ï¬nite partitions Î¾ and Î· of intervals [âˆ’A,A]
and [âˆ’A âˆ’b,A + b], and XÎ¾ and YÎ· stand for the quantised versions of random
variables X and Y, respectively.
In other words, the input-signal distribution PX with
PX(âˆ’A) = PX(âˆ’A+2b) = Â·Â·Â· = PX(Aâˆ’2b) = PX(A) =
1
m+1
(4.1.34)
maximises I(X : Y) under assumptions (i) and (ii). We denote this distribution by
P(A,A/m)
X
, or, equivalently, P(bm,b)
X
.
However, if M = 2m, i.e. the number â™¯A of allowed signals is even, the cal-
culation becomes more involved. Here, clearly, the uniform distribution U(âˆ’A âˆ’
b,A+b) for the output signal Y cannot be achieved. We have to maximise h(Y) =
h(X + Z) within the class of piece-wise constant PDFs fY on [âˆ’A âˆ’b,A + b]; see
below.
Equal spacing in [âˆ’A,A] is generated by points Â±A/(2m âˆ’1), Â±3A/(2m âˆ’
1),...,Â±A; they are described by the formula Â±(2k âˆ’1)A/(2m âˆ’1) for k =
1,...,m. These points divide the interval [âˆ’A,A] into (2m âˆ’1) intervals of length
2A/(2mâˆ’1). With Z âˆ¼U(âˆ’b,b) and A = b(mâˆ’1/2), we again have the output-
signal PDF fY(y) supported in [âˆ’Aâˆ’b,A+b]:
fY(y) =
â§
âª
âª
âª
âª
âª
âª
âª
âª
âª
âª
âª
âª
â¨
âª
âª
âª
âª
âª
âª
âª
âª
âª
âª
âª
âª
â©
pm/(2b),
if b(mâˆ’1/2) â‰¤y â‰¤b(m+1/2),

pk + pk+1

(2b),
if b(k âˆ’1/2) â‰¤y â‰¤b(k +1/2)
for k = 1,...,mâˆ’1,

pâˆ’1 + p1

(2b),
if âˆ’b/2 â‰¤y â‰¤b/2,

pk + pk+1

(2b),
if b(k âˆ’1/2) â‰¤y â‰¤b(k +1/2)
for k = âˆ’1,...,âˆ’m+1,
pâˆ’m/(2b),
if âˆ’b(m+1/2) â‰¤y â‰¤âˆ’b(mâˆ’1/2),

390
Further Topics from Information Theory
where
pÂ±k = pX

Â±b

k âˆ’1
2

= P

X = Â±(2k âˆ’1)A
2mâˆ’1

, k = 1,...,m,
stand for the input-signal probabilities. The entropy h(Y) = h(X +Z) is written as
âˆ’pm
2 ln pm
2b âˆ’âˆ‘
1â‰¤k<m
pk + pk+1
2
ln pk + pk+1
2b
âˆ’pâˆ’1 + p1
2
ln pâˆ’1 + p1
2b
âˆ’
âˆ‘
âˆ’m<kâ‰¤âˆ’1
pk + pk+1
2
ln pk + pk+1
2b
âˆ’pâˆ’m
2
ln pâˆ’m
2b .
It turns out that the maximising distribution PX has pâˆ’k = pk, for k = 1,...,m.
Thus, we face an optimisation problem:
maximise G(p) = âˆ’pm ln pm
2b âˆ’âˆ‘
1â‰¤k<m
(pk + pk+1)ln pk + pk+1
2b
âˆ’p1 ln p1
b
(4.1.35)
subject to the probabilistic constraints pk â‰¥0 and 2
âˆ‘
1â‰¤kâ‰¤m
pk = 1. The Lagrangian
L (PX;Î») reads
L (PX;Î») = G(p)+Î»(2p1 +Â·Â·Â·+2pm âˆ’1)
and is maximised when
âˆ‚
âˆ‚pk
L (PX;Î») = 0, k = 1,...,m.
Thus, we have m equations, with the same RHS:
âˆ’ln pm(pmâˆ’1 + pm)
4b2
âˆ’2+2Î» = 0, (implies) pm(pmâˆ’1 + pm) = 4b2e2Î»âˆ’2,
âˆ’ln (pkâˆ’1 + pk)(pk + pk+1)
4b2
âˆ’2+2Î» = 0,
(implies) (pkâˆ’1 + pk)(pk + pk+1) = 4b2e2Î»âˆ’2, 1 < k < m,
âˆ’ln 2p1(p1 + p2)
4b2
âˆ’2+2Î» = 0 (implies) 2p1(p1 + p2) = 4b2e2Î»âˆ’2.
This yields
pm = pmâˆ’1 + pmâˆ’2 = Â·Â·Â· = p3 + p2 = 2p1,
pm + pmâˆ’1 = pmâˆ’2 + pmâˆ’3 = Â·Â·Â· = p2 + p1,
K
for m even,

4.1 Gaussian channels and beyond
391
and
pm = pmâˆ’1 + pmâˆ’2 = Â·Â·Â· = p2 + p1,
pm + pmâˆ’1 = pmâˆ’2 + pmâˆ’3 = Â·Â·Â· = p3 + p2 = 2p1,
K
for m odd.
For small values of M = 2m the solution is straightforward. Viz., for M = 2
(two input signals at Â±A with b = 2A): p1 = 1/2 and the maximising output-signal
PDF is
fY(y) =
â§
âª
â¨
âª
â©
1/(4b),
A â‰¤y â‰¤3A,
1/(2b),
âˆ’A â‰¤y â‰¤A,
1/(4b),
âˆ’3A â‰¤y â‰¤âˆ’A,
yielding Cinf = (ln2)/2.
For M = 4 (four input signals at âˆ’A, âˆ’A/3, A/3, A, with b = 2A/3): p1 = 1/6,
p2 = 1/3, and the maximising output-signal PDF is
fY(y) =
â§
âª
â¨
âª
â©
1/(6b),
A â‰¤y â‰¤5A/3 and âˆ’5A/3 â‰¤y â‰¤âˆ’A,
1/(4b),
2A/3 â‰¤y â‰¤A and âˆ’A â‰¤y â‰¤âˆ’2A/3,
1/(6b),
âˆ’2A/3 â‰¤y â‰¤2A/3,
which yields Cinf = ln(61/241/3/2).
For M = 6 (six input signals at âˆ’A, âˆ’3A/5, âˆ’A/5, A/5, 3A/5, A, with b =
2A/5): p1 = 1/6, p2 = 1/12, p3 = 1/4. Similarly, for M = 8 (eight input signals
at âˆ’A, âˆ’5A/7, âˆ’3A/7, âˆ’A/7, A/7, 3A/7, 5A/7, A, with b = 2A/7): p1 = 1/10,
p2 = 3/20, p3 = 1/20, p4 = 1/5.
In general, we can write all probabilities in terms of p1. Viz., for m even:
pm = 2p1,
pmâˆ’1 = p2 âˆ’p1,
pmâˆ’2 = 3p1 âˆ’p2,
pmâˆ’3 = 2(p2 âˆ’p1),
pmâˆ’4 = 4p1 âˆ’2p2,
...
p3 =
m
2 âˆ’1

(p2 âˆ’p1),
p2 = m+2
m
p1,

392
Further Topics from Information Theory
whence
p2 = m+2
m
p1,
p3 = mâˆ’2
m
p1,
p4 = m+4
m
p1,
p5 = mâˆ’4
m
p1,
...
pmâˆ’2 = 2mâˆ’2
m
,
pmâˆ’1 = 2
m,
pm = 2p1,
with p1 =
1
2(m+1).
(4.1.36)
The corresponding PDF fY gives the value
h(Y) = âˆ’1
2 ln
1
4m(m+1)b2
and
Cinf
A = âˆ’1
2 ln
1
4m(m+1) âˆ’ln2.
(4.1.37)
On the other hand, for a general odd m, the maximising input-signal distribution
PX has
p1 =
m+1
2m(m+1),
p2 =
mâˆ’1
2m(m+1),
p3 =
m+3
2m(m+1),
p4 =
mâˆ’3
2m(m+1),
...
pmâˆ’1 =
1
2m(m+1),
pm =
m
m(m+1).
(4.1.38)

4.1 Gaussian channels and beyond
393
This yields the same answer for the maximum entropy and the restricted capacity:
h(Y) = âˆ’1
2 ln
1
4m(m+1)b2
and
Cinf
A = âˆ’1
2 ln
1
4m(m+1) âˆ’ln2.
(4.1.39)
In future, we will refer to the input-signal distributions speciï¬ed in (4.1.36) and
(4.1.38) as  P(A,2A/(2mâˆ’1))
X
.
Remark 4.1.15
It is natural to suggest that the above formulas give the maxi-
mum mutual information I(X :Y) when (i) the noise random variable Z âˆ¼U(âˆ’b,b)
is independent of X and (ii) the input-signal distribution PX is conï¬ned to [âˆ’A,A]
with b = 2A/(2mâˆ’1), but otherwise is arbitrary (with I(X : Y) again deï¬ned as in
(4.1.33)). A further-reaching (and more speculative) conjecture is about the max-
imiser under the above assumptions (i) and (ii) but for arbitrary A > b > 0, not
necessarily with A/b being integer or half-integer. Here number M = 2A/b + 1
will not be integer either, but remains worth keeping as a value of reference.
So when b decays from A/m to A/(m + 1) (or, equivalently, A grows from bm
to b(m+1) and, respectively, M increases from 2m+1 to 2m+3), the maximiser
P(A,b)
X
evolves from P(bm,b)
X
to P(b(m+1),b)
X
; at A = b(m+1/2) (when M = 2(m+1))
distribution P(A,b)
X
may or may not coincide with the distribution  P(A,b)
X
from
(4.1.36), (4.1.38).
To (partially) clarify the issue, consider the case where A/2 â‰¤b â‰¤A (i.e. 3 â‰¤
M â‰¤5) and assume that the input-signal distribution PX has
PX(âˆ’A) = PX(A) = p and PX(0) = 1âˆ’2p where 0 â‰¤p â‰¤1
2.
(4.1.40)
Then
hy(Y) = âˆ’1
b

Apln p
2b +(2bâˆ’A)(1âˆ’p)ln 1âˆ’p
2b
+(Aâˆ’b)(1âˆ’2p)ln 1âˆ’2p
2b

,
(4.1.41)
and the equation dh(Y)

dp = 0 is equivalent to
pA = (1âˆ’p)2bâˆ’A(1âˆ’2p)2(Aâˆ’b).
(4.1.42)
For b = A/2 this yields pA = (1âˆ’2p)A, i.e. p = 1âˆ’2p whence p = 1/3; similarly,
for b = A, p = 1/2. These coincide with previously obtained results. For b = 2A/3
we have that
pA = (1âˆ’p)A/3(1âˆ’2p)2A/3;
i.e.
p3 = (1âˆ’p)(1âˆ’2p)2.
(4.1.43a)

394
Further Topics from Information Theory
We are interested in the solution lying in (0,1/2) (in fact, in (1/3,1/2)). For b =
3A/4, the equation becomes
pA = (1âˆ’p)A/2(1âˆ’2p)A/2,
i.e.
p2 = (1âˆ’p)(1âˆ’2p),
(4.1.43b)
whence p = (3âˆ’
âˆš
5)

2.
Example 4.1.16
It is useful to look at the example where the noise random
variable Z has two components: discrete and continuous. To start with, one could
try the case where
fZ(z) = qÎ´0 +(1âˆ’q)Ï†(z;Ïƒ2),
i.e. Z = 0 with probability q and Z âˆ¼N(0,Ïƒ2) with probability 1âˆ’q âˆˆ(0,1). (So,
1âˆ’q gives the total probability of error.) Here, we consider the case
fZ = qÎ´0 +(1âˆ’q) 1
2b1(|z| â‰¤b),
and study the input-signal PMF of the form
PX(âˆ’A) = pâˆ’1, PX(0) = p0, PX(A) = p1,
(4.1.44a)
where
pâˆ’1, p0, p1 â‰¥0, pâˆ’1 + p0 + p1 = 1,
(4.1.44b)
with b = A and M = 3 (three signal levels in (âˆ’A,A)). The input-signal entropy is
h(X) = h(pâˆ’1, p0, p1) = âˆ’pâˆ’1 ln pâˆ’1 âˆ’p0 ln p0 âˆ’p1 ln p1.
The output-signal PMF has the form
fY(y) = q

pâˆ’1Î´âˆ’A + p0Î´0 + p1Î´A

+(1âˆ’q) 1
2b
Ã—

pâˆ’11(âˆ’2A â‰¤y â‰¤0)+ p01(âˆ’A â‰¤y â‰¤A)+ p11(0 â‰¤y â‰¤2A)

and its entropy h(Y) (calculated relative to the reference measure Î¼ on R, whose
absolutely continuous component coincides with the Lebesgue and discrete com-
ponent assigns value 1 to points âˆ’A, 0 and A) is given by
h(Y) = âˆ’qlnqâˆ’(1âˆ’q)ln(1âˆ’q)âˆ’qh(pâˆ’1, p0, p1)
âˆ’(1âˆ’q)A

pâˆ’1 ln pâˆ’1
2A +

pâˆ’1 + p0

ln pâˆ’1 + p0
2A
+

p0 + p1

ln p0 + p1
2A
+ p1 ln p1
2A

.

4.1 Gaussian channels and beyond
395
By symmetry, h(Y) is maximised when pâˆ’1 = p1 = p, p0 = 1 âˆ’2p, and we have
to maximise, in q âˆˆ(0,1), the expression
h(Y) = h(q,1âˆ’q)âˆ’qh(p, p,1âˆ’2p)âˆ’(1âˆ’q)A

2pln p
2A +(1âˆ’2p)ln 1âˆ’2p
2A

,
for a given q âˆˆ(0,1).
Differentiating yields
d
dph(Y) = 0 â†”
p
1âˆ’2p =

p
1âˆ’p
âˆ’(1âˆ’q)A/q
.
If (1âˆ’q)A/q > 1 this equation yields a unique solution which deï¬nes an optimal
input-signal distribution PX of the form (4.1.44a)â€“(4.1.44b).
If we wish to see what value of q yields the maximum of h(Y) (and hence, the
maximum information capacity), we differentiate in q as well:
d
dqh(Y) = 0 â†”log
q
1âˆ’q = (Aâˆ’1)h(p, p,1âˆ’2p)âˆ’2Aln2A.
If we wish to consider a continuously distributed input signal on [âˆ’A,A], with a
PDF fX(x), then the output random variable Y = X + Z has the PDF given by the
convolution:
fY(y) = 1
2b
0 (y+b)âˆ§A
(yâˆ’b)âˆ¨(âˆ’A) fX(x)dx.
The differential entropy h(Y) = âˆ’
1 fY(y)ln fY(y)dy, in terms of fX, takes the form
h(X +Z) = âˆ’1
2b
0 A
âˆ’A fX(x)
0 b
âˆ’b ln
 1
2b
0 (x+z+b)âˆ§A
(x+zâˆ’b)âˆ¨(âˆ’A) fX(xâ€²)dxâ€²

dzdx.
The PDF fX minimising the differential entropy h(X +Z) yields a solution to
0 =
0 b
âˆ’b

ln
 1
2b
0 (x+z+b)âˆ§A
(x+zâˆ’b)âˆ¨(âˆ’A) fX(xâ€²)dxâ€²

+ fX(x)
Ã—
0 (x+z+b)âˆ§A
(x+zâˆ’b)âˆ¨(âˆ’A) fX(xâ€²)dxâ€²
âˆ’1 	
fX(x+z+b)âˆ’fX(x+zâˆ’b)


dz.
An interesting question emerges when we think of a two-time-per-signal use of
a channel with a uniform noise. Suppose an input signal is represented by a point
x = (x1,x2) in a plane R2 and assume as before that Z âˆ¼U(âˆ’b,b), independently
of the input signal. Then the square Sb(x) = (x1 âˆ’b,x1 +b)Ã—(x2 âˆ’b,x2 +b), with
the uniform PDF 1/(4b2), outlines the possible positions of the output signal Y
given that X = (x1,x2). Suppose that we have to deal with a ï¬nite input alphabet
A âŠ‚R2; then the output-signal domain is the ï¬nite union B = âˆªxâˆˆA S(x). The
above argument shows that if we can ï¬nd a subset A â€² âŠ†A such that squares Sb(x)

396
Further Topics from Information Theory
2b
an  example  of  set B
set B
square
S (
x
x )
_
_
points  from A (10  in  total)
points  from A \ A (8  in  total)
2b
C
inf= ln 10
b
Figure 4.3
with x âˆˆA â€² partition domain B (i.e. cover B but do not intersect each other) then,
for the input PMF Px with Px(x) = 1

(â™¯A â€²) (a uniform distribution over A â€²), the
output-vector-signal PDF fY is uniform on B (that is, fY(y) = 1

area of B

).
Consequently, the output-signal entropy h(Y) = ln

area of B

is attaining the
maximum over all input-signal PMFs Px with Px(A ) = 1 (and even attaining the
maximum over all input-signal PMFs Px with Px(Bâ€²) = 1 where Bâ€² âŠ‚B is an
arbitrary subset with the property that âˆªxâ€²âˆˆBâ€²S(xâ€²) lies within B). Finally, the in-
formation capacity for the channel under consideration,
Cinf = 1
2 ln area of B
4b2
nats/(scalar input signal).
See Figure 4.3.
To put it differently, any bounded set D2 âŠ‚R2 that can be partitioned into disjoint
squares of length 2b yields the information capacity
Cinf
2 = 1
2 ln area of D2
4b2
nats/(scalar input signal),
of an additive channel with a uniform noise over (âˆ’b,b), when the channel is used
two times per scalar input signal and the random vector input x = (X1,X2) is subject
to the regional constraint x âˆˆD2. The maximising input-vector PMF assigns equal
probabilities to the centres of squares forming the partition.
A similar conclusion holds in R3 when the channel is used three times for every
input signal, i.e. the input signal is a three-dimensional vector x = (x1,x2,x3), and
so on. In general, when we use a K-dimensional input signal x = (x1,...,xk) âˆˆRK,
and the regional constraint is x âˆˆDK âŠ‚RK where DK is a bounded domain that can

4.2 The asymptotic equipartition property in continuous time setting
397
be partitioned into disjoint cubes of length 2b, the information capacity
Cinf
K = 1
K ln volume of DK
(2b)K
nats/(scalar input signal)
is achieved at the input-vector-signal PMF Px assigning equal masses to the centres
of the cubes forming the partition.
As K â†’âˆ, the quantity CK may converge to a limit Cinf
âˆyielding the capacity
per scalar input signal under the sequence of regional constraint domains DK. A
trivial example of such a situation is where DK is a K-dimensional cube
SK
b = (âˆ’2bm,2bm)Ã—K;
then Cinf
K = ln(1+m) does not depend on K (and the channel is memoryless).
4.2 The asymptotic equipartition property in the continuous
time setting
The errors of a wise man make your rule,
Rather than perfections of a fool.
William Blake (1757â€“1821), English poet
This section provides a missing step in the proof of Theorem 4.1.8 and ad-
ditional Worked Examples. We begin with a series of assertions illustrating the
asymptotic equipartition property in various forms. The central facts are based on
the Shannonâ€“McMillanâ€“Breiman (SMB) theorem which is considered a corner-
stone of information theory. This theorem gives the information rate of a stationary
ergodic process X = (Xn). Recall that a transformation of a probability space T is
called ergodic if every set A such that TA = A almost everywhere, satisï¬es P(A) = 0
or 1. For a stationary ergodic source with a ï¬nite expected value, Birkhoffâ€™s ergodic
theorem states the law of large numbers (with probability 1):
1
n
n
âˆ‘
i=1
Xi â†’EX.
(4.2.1)
Typically, for a measurable function f(Xt) of ergodic process,
1
n
n
âˆ‘
i=1
f(Xi) â†’E f(X).
(4.2.2)
Theorem 4.2.1
(Shannonâ€“McMillanâ€“Breiman) For any stationary ergodic pro-
cess X with ï¬nitely many values the information rate R = h, i.e. the limit in (4.2.3)

398
Further Topics from Information Theory
exists in the sense of the a.s. convergence and equals to entropy
âˆ’lim
nâ†’âˆ
1
n log pXnâˆ’1
0

Xnâˆ’1
0

= h a.s.
(4.2.3)
The proof of Theorem 4.2.1 requires some auxiliary lemmas and is given at the
end of the section.
Worked Example 4.2.2
(A general asymptotic equipartition property) Given a
sequence of random variables X1,X2,..., for all N = 1,2,..., the distribution of
the random vector xN
1 =
â›
âœ
â
X1
...
XN
â
âŸ
â is determined by a PMF fxN
1 (xN
1 ) with respect to
measure Î¼(N) = Î¼ Ã—Â·Â·Â·Ã—Î¼ (N factors). Suppose that the statement of the Shannonâ€“
McMillanâ€“Breiman theorem holds true:
âˆ’1
N log fxN
1 (xN
1 ) â†’h in probability,
where h > 0 is a constant (typically, h = lim
iâ†’âˆh(Xi)). Given Îµ > 0, consider the
typical set
SN
Îµ =
â§
âª
â¨
âª
â©
xN
1 =
â›
âœ
â
x1
...
xN
â
âŸ
â : âˆ’Îµ â‰¤1
N log fxN
1 (xN
1 )+h â‰¤Îµ
â«
âª
â¬
âª
â­
.
The volume Î¼(N)(SN
Îµ ) =
0
SNÎµ
Î¼(dx1)...Î¼(dxN) of set SN
Îµ has the following proper-
ties:
Î¼(N)(SN
Îµ ) â‰¤2N(h+Îµ),for all Îµ and N,
(4.2.4)
and, for 0 < Îµ < h and for all Î´ > 0,
Î¼(N)(SN
Îµ ) â‰¥(1âˆ’Î´)2N(hâˆ’Îµ),for N large enough, depending on Î´.
(4.2.5)
Solution Since P(RN) =
0
RN fxN
1 (xN
1 ) âˆ
1â‰¤jâ‰¤N
Î¼(dxj) = 1, we have that
1 =
0
RN fxN
1 (xN
1 ) âˆ
1â‰¤jâ‰¤N
Î¼(dxj)
â‰¥
0
SNÎµ
fxN
1 (xN
1 ) âˆ
1â‰¤jâ‰¤N
Î¼(dxj)
â‰¥2âˆ’N(h+Îµ)
0
SNÎµ âˆ
1â‰¤jâ‰¤N
Î¼(dxj) = 2âˆ’N(h+Îµ)Î¼(N)(SN
Îµ ),

4.2 The asymptotic equipartition property in continuous time setting
399
giving the upper bound (4.2.4). On the other hand, given Î´ > 0, we can take N
large so that P(SN
Îµ ) â‰¥1âˆ’Î´, in which case, for 0 < Îµ < h,
1âˆ’Î´ â‰¤P(SN
Îµ )
=
0
SNÎµ
fxN
1 (xN
1 ) âˆ
1â‰¤jâ‰¤N
Î¼(dxj)
â‰¤2âˆ’N(hâˆ’Îµ)
0
SNÎµ âˆ
1â‰¤jâ‰¤N
Î¼(dxj) = 2âˆ’N(hâˆ’Îµ)Î¼(N)(SN
Îµ ).
This yields the lower bound (4.2.5).
The next step is to extend the asymptotic equipartition property to joint distri-
butions of pairs XN
1 , YN
1 (in applications, XN
1 will play a role of an input and YN
1
of an output of a channel). Formally, given two sequences of random variables,
X1,X2,... and Y1,Y2,..., for all N = 1,2,..., consider the joint distribution of the
random vectors XN
1 =
â›
âœ
â
X1
...
XN
â
âŸ
â and YN
1 =
â›
âœ
â
Y1
...
YN
â
âŸ
â which is determined by a (joint)
PMF fxN
1 ,YN
1 with respect to measure Î¼(N) Ã— Î½(N) where Î¼(N) = Î¼ Ã— Â·Â·Â· Ã— Î¼ and
Î½(N) = Î½ Ã— Â·Â·Â· Ã— Î½ (N factors in both products). Let fXN
1 and fYN
1 stand for the
(joint) PMFs of vectors XN
1 and YN
1 , respectively.
As in Worked Example 4.2.2, we suppose that the statements of the Shannonâ€“
McMillanâ€“Breiman theorem hold true, this time for the pair (XN
1 ,YN
1 ) and each of
XN
1 and YN
1 : as N â†’âˆ,
âˆ’1
N log fXN
1 (XN
1 ) â†’h1, âˆ’1
N log fYN
1 (YN
1 ) â†’h2,
âˆ’1
N log fXN
1 ,YN
1 (XN
1 ,YN
1 ) â†’h,
in probability,
where h1,h2 and h are positive constants, with
h1 +h2 â‰¥h;
(4.2.6)
typically, h1 = lim
iâ†’âˆh(Xi), h2 = lim
iâ†’âˆh(Yi), h = lim
iâ†’âˆh(Xi,Yi) and h1 + h2 âˆ’h =
lim
iâ†’âˆI(Xi :Yi). Given Îµ > 0, consider the typical set formed by sample pairs (xN
1 ,yN
1 )
where
xN
1 =
â›
âœ
â
x1
...
xN
â
âŸ
â 

400
Further Topics from Information Theory
and
yN
1 =
â›
âœ
â
y1
...
yN
â
âŸ
â .
Formally,
T N
Îµ =
%
(xN
1 ,yN
1 ): âˆ’Îµ â‰¤1
N log fxN
1 (xN
1 )+h1 â‰¤Îµ,
âˆ’Îµ â‰¤1
N log fYN
1 (yN
1 )+h2 â‰¤Îµ,
âˆ’Îµ â‰¤1
N log fxN
1 ,YN
1 (xN
1 ,yN
1 )+h â‰¤Îµ
K
; (4.2.7)
by the above assumption we have that lim
Nâ†’âˆP

T N
Îµ

= 1 for all Îµ > 0. Next, deï¬ne
the volume of set T N
Îµ :
Î¼(N) Ã—Î½(N) 
T N
Îµ

=
0
T N
Îµ
Î¼(N)(dxN
1 )Î½(N)(dyN
1 ).
Finally, consider an independent pair

 XN
1 ,  YN
1

where component  XN
1 has the same
PMF as XN
1 and  YN
1 the same PMF as YN
1 . That is, the joint PMF for  XN
1 and  YN
1
has the form
f XN
1 , YN
1 (xN
1 ,yN
1 ) = fXN
1 (xN
1 ) fYN
1 (yN
1 ).
(4.2.8)
Next, we assess the volume of set T N
Îµ and then the probability that

 xN
1 ,  YN
1

âˆˆ
T N
Îµ .
Worked Example 4.2.3
(A general joint asymptotic equipartition property)
(I) The volume of the typical set has the following properties:
Î¼(N) Ã—Î½(N) 
T N
Îµ

â‰¤2N(h+Îµ),for all Îµ and N,
(4.2.9)
and, for all Î´ > 0 and 0 < Îµ < h, for N large enough, depending on Î´,
Î¼(N) Ã—Î½(N) 
T N
Îµ

â‰¥(1âˆ’Î´)2N(hâˆ’Îµ).
(4.2.10)
(II) For the independent pair

 XN
1 ,  YN
1

,
P

 XN
1 ,  YN
1

âˆˆT N
Îµ

â‰¤2âˆ’N(h1+h2âˆ’hâˆ’3Îµ), for all Îµ and N,
(4.2.11)
and, for all Î´ > 0, for N large enough, depending on Î´,
P

 XN
1 ,  YN
1

âˆˆT N
Îµ

â‰¥(1âˆ’Î´)2âˆ’N(h1+h2âˆ’h+3Îµ),
for all Îµ.
(4.2.12)

4.2 The asymptotic equipartition property in continuous time setting
401
Solution (I) Completely follows the proofs of (4.2.4) and (4.2.5) with integration
of fxN
1 ,YN
1 .
(II) For the probability P

 XN
1 ,  YN
1

âˆˆT N
Îµ

we obtain (4.2.11) as follows:
P

 XN
1 ,  YN
1

âˆˆT N
Îµ

=
0
T N
Îµ
f xN
1 , YN
1 Î¼(dxN
1 )Î½(dyN
1 )
by deï¬nition
=
0
T N
Îµ
fxN
1 (xN
1 ) fYN
1 (yN
1 )Î¼(dxN
1 )Î½(dyN
1 )
substituting (4.2.8)
â‰¤2âˆ’N(h1âˆ’Îµ)2âˆ’N(h2âˆ’Îµ)
0
T N
Îµ
Î¼(dxN
1 )Î½(dyN
1 )
according to (4.2.7)
â‰¤2âˆ’N(h1âˆ’Îµ)2âˆ’N(h2âˆ’Îµ)2N(h+Îµ) = 2âˆ’N(h1+h2âˆ’hâˆ’3Îµ)
because of bound (4.2.9).
Finally, by reversing the inequalities in the last two lines, we can cast them as
â‰¥2âˆ’N(h1+Îµ)2âˆ’N(h2+Îµ)
0
T N
Îµ
Î¼(dxN
1 )Î½(dyN
1 )
according to (4.2.7)
â‰¥(1âˆ’Î´)2âˆ’N(h1+Îµ)2âˆ’N(h2+Îµ)2N(hâˆ’Îµ) = (1âˆ’Î´)2âˆ’N(h1+h2âˆ’h+3Îµ)
because of bound (4.2.10).
Formally, we assumed here that 0 < Îµ < h (since it was assumed in (4.2.10)), but
increasing Îµ only makes the factor 2âˆ’N(h1+h2âˆ’h+3Îµ) smaller. This proves bound
(4.2.12).
A more convenient (and formally a broader) extension of the asymp-
totic equipartition property is where we suppose that the statements of
the Shannonâ€“McMillanâ€“Breiman theorem hold true directly for the ratio
fxN
1 ,YN
1 (XN
1 ,YN
1 )
	
fXN
1 (XN
1 ) fYN
1 (YN
1 ))

. That is,
1
N log
fXN
1 ,YN
1 (XN
1 ,YN
1 )
fxN
1 (XN
1 ) fYN
1 (YN
1 ) â†’c in probability,
(4.2.13)

402
Further Topics from Information Theory
where c > 0 is a constant. Recall that fXN
1 ,YN
1 represents the joint PMF while fXN
1
and fxN
1 individual PMFs for the random input and output vectors xN and YN, with
respect to reference measures Î¼(N) and Î½(N):
fXN
1 ,YN
1 (xN
1 ,yN
1 ) = fXN
1 (xN
1 ) fch(yN
1 |xN
1 sent ),
fYN
1 (YN
1 ) =
0
fXN
1 ,YN
1 (xN
1 ,yN
1 )Î¼(N)
dxN
1

.
Here, for Îµ > 0, we consider the typical set
T N
Îµ =
%
(XN
1 ,yN
1 ) : âˆ’Îµ â‰¤1
N log
fXN
1 ,YN
1 (xN
1 ,yN
1 )
fXN
1 (xN
1 ) fYN
1 (yN
1 ) âˆ’c â‰¤Îµ
K
;
(4.2.14)
by assumption (4.2.13) we have that lim
Nâ†’âˆP

XN
1 ,YN
1

âˆˆT N
Îµ

= 1 for all Îµ > 0.
Again, we will consider an independent pair

 XN
1 ,  YN
1

where component  XN
1
has the same PMF as XN
1 and  YN
1 the same PMF as YN
1 .
Theorem 4.2.4
(Deviation from the joint asymptotic equipartition property)
Assume that property (4.2.13) holds true. For an independent pair

 XN
1 ,  YN
1

, the
probability that

 XN
1 ,  YN
1

âˆˆT N
Îµ obeys
P

 XN
1 ,  YN
1

âˆˆT N
Îµ

â‰¤2âˆ’N(câˆ’Îµ), for all Îµ and N,
(4.2.15)
and, for all Î´ > 0, for N large enough, depending on Î´,
P

 XN
1 ,  YN
1

âˆˆT N
Îµ

â‰¥(1âˆ’Î´)2âˆ’N(c+Îµ),
for all Îµ.
(4.2.16)
Proof
Again, we obtain (4.2.15) as follows:
P

 XN
1 ,  YN
1

âˆˆT N
Îµ

=
0
T N
Îµ
f XN
1 , YN
1 Î¼Ã—N(dXN
1 )Î½Ã—N(dyN
1 )
=
0
T N
Îµ
fxN
1 (xN
1 ) fYN
1 (yN
1 )Î¼(dXN
1 )Î½(dyN
1 )
=
0
T N
Îµ
exp

âˆ’
fXN
1 ,YN
1 (xN
1 ,yN
1 )
fXN
1 (xN
1 ) fYN
1 (yN
1 )

Ã— fXN
1 ,YN
1 (xN
1 ,yN
1 )Î¼Ã—N(dxN
1 )Î½Ã—N(dyN
1 )
â‰¤2âˆ’N(câˆ’Îµ)
0
T N
Îµ
fXN
1 ,YN
1 (xN
1 ,yN
1 )Î¼(dxN
1 )Î½(dyN
1 )
= 2âˆ’N(câˆ’Îµ)P

XN
1 ,YN
1

âˆˆT N
Îµ

â‰¤2âˆ’N(câˆ’Îµ).

4.2 The asymptotic equipartition property in continuous time setting
403
The ï¬rst equality is by deï¬nition, the second step follows by substituting (4.2.8),
the third is by direct calculation, and the fourth because of the bound (4.2.14).
Finally, by reversing the inequalities in the last two lines, we obtain the bound
(4.2.16):
â‰¥2âˆ’N(c+Îµ)
0
T N
Îµ
fXN
1 ,YN
1 (xN
1 ,yN
1 )Î¼(dxN
1 )Î½(dyN
1 )
= 2âˆ’N(c+Îµ)P

XN
1 ,YN
1

âˆˆT N
Îµ

â‰¥2âˆ’N(c+Îµ)(1âˆ’Î´),
the ï¬rst inequality following because of (4.2.14).
Worked Example 4.2.5
Let x = {X(1),...,X(n)}T be a given vector/collection
of random variables. Let us write x(C) for subcollection {X(i): i âˆˆC} where C is
a non-empty subset in the index set {1,...,n}. Assume that the joint distribution
for any subcollection x(C) with â™¯C = k, 1 â‰¤k â‰¤n, is given by a joint PMF fx(C)
relative to measure Î¼ Ã—Â·Â·Â·Ã—Î¼ (k factors, each corresponding to a random variable
X(i) with i âˆˆC). Similarly, given a vector x =
â›
âœ
â
x(1)
...
x(n)
â
âŸ
â of values for x, denote by
x(C) the argument {x(i) : i âˆˆC} (the sub-column in x extracted by picking the rows
with i âˆˆC). By the Gibbs inequality, for all partitions {C1,...,Cs} of set {1,...,n}
into non-empty disjoint subsets C1,...,Cs (with 1 â‰¤s â‰¤n), the integral
0
fx(x)log
fxn
1(x)
fx(C1)(x(C1))... fx(Cs)(x(Cs)) âˆ
1â‰¤jâ‰¤n
Î¼(dx( j)) â‰¥0.
(4.2.17)
What is the partition for which the integral in (4.2.17) attains its maximum?
Solution The partition in question has s = n subsets, each consisting of a single
point. In fact, consider the partition of set {1,...,n} into single points; the corre-
sponding integral equals
0
fx(x)log
fxn
1(x)
âˆ
1â‰¤iâ‰¤n
fXi(xi) âˆ
1â‰¤jâ‰¤n
Î¼(dx( j)).
(4.2.18)
Let {C1,...,Cs} be any partition of {1,...,n}. Multiply and divide the fraction
under the log by the product of joint PMFs
âˆ
1â‰¤iâ‰¤s
fx(Cl)(x(Cl)). Then the integral
(4.2.18) is represented as the sum
0
fx(x)log
fxn
1(x)
âˆ
1â‰¤iâ‰¤s
fx(Ci)(x(Ci)) âˆ
1â‰¤jâ‰¤n
Î¼(dx( j)) + terms â‰¥0.
The answer follows.

404
Further Topics from Information Theory
Worked Example 4.2.6
Let x = {X(1),...,X(n)} be a collection of random
variables as in Worked Example 4.2.5, and let Y be another random variable.
Suppose that there exists a joint PMF fx,Y, relative to a measure Î¼(n) Ã— Î½ where
Î¼(n) = Î¼ Ã—Â·Â·Â·Ã— Î¼ (n times). Given a subset C âŠ†{1,...,n}, consider the sum
I(x(C) : Y)+E
	
I(x(C : Y)|x(C)

.
Here x(C) = {X(i) : i âˆˆC}, x(C) = {X(i) : i Ì¸âˆˆC}, and E
	
I(x(C : Y)|x(C)

stands
for the expectation of I(x(C : Y) conditional on the value of x(C). Prove that this
sum does not depend on the choice of set C.
Solution Check that the expression in question equals I(x : Y).
In Section 4.3 we need the following facts about parallel (or product) channels.
Worked Example 4.2.7
(Lemma A in [173]; see also [174]) Show that the
capacity of the product of r time-discrete Gaussian channels with parameters
(Î± j, p( j),Ïƒ2
j ) equals
C = âˆ‘
1â‰¤jâ‰¤r
Î± j
2 ln

1+ p( j)
Î± jÏƒ2
j

.
(4.2.19)
Moreover, (4.2.19) holds when some of the Î± js equal +âˆ: in this case the corre-
sponding summand takes the form p( j)
Ïƒ2
j .
Solution Suppose that multi-vector data x = {x1,...,xr} are transmitted via r par-
allel channels of capacities C1,...,Cr, where each vector xj =
â›
âœ
â
x j1
...
x jn j
â
âŸ
â âˆˆRn j. It is
convenient to set nj = âŒˆÎ± jÏ„âŒ‰where Ï„ â†’âˆ. It is claimed that the capacity for this
product-channel equals the sum
âˆ‘
1â‰¤iâ‰¤r
Ci. By induction, it is sufï¬cient to consider
the case r = 2. For the direct part, assume that R <C1 +C2 and Îµ > 0 are given. For
Ï„ sufï¬ciently large we must ï¬nd a code for the product channel with M = eRÏ„ code-
words and Pe < Îµ. Set Î· = (C1 +C2 âˆ’R)/2. Let X 1 and X 2 be codes for channels
1 and 2 respectively with M1 âˆ¼e(C1âˆ’Î·)Ï„ and M2 âˆ¼e(C2âˆ’Î·)Ï„ and error-probabilities
PX 1
e
,PX 2
e
â‰¤Îµ/2. Construct a concatenation code X with codewords x = x1
kx2
l
where xi
â€¢ âˆˆX i, i = 1,2. Then, for the product-channel under consideration, with
codes X 1 and X 2, the error-probability PX 1,X 2
e
is decomposed as follows:
PX 1,X 2
e
=
1
M1M2
âˆ‘
1â‰¤kâ‰¤M1,1â‰¤lâ‰¤M2
P

error in channel 1 or 2|x1
kx2
l sent

.
By independence of the channels, PX 1,X 2
e
â‰¤PX 1
e
+ PX 2
e
â‰¤Îµ which yields the
direct part.

4.2 The asymptotic equipartition property in continuous time setting
405
The proof of the inverse is more involved and we present only a sketch, referring
the interested reader to [174]. The idea is to apply the so-called list decoding:
suppose we have a code Y of size M and a decoding rule d = dY . Next, given
that a vector y has been received at the output port of a channel, a list of L possible
code-vectors from Y has to be produced, by using a decoding rule  d =  d Y
list, and the
decoding (based on rule  d) is successful if the correct word is in the list. Then, for
the average error-probability Pe = PY
e (d) over code Y , the following inequality is
satisï¬ed:
Pe â‰¥Pe(  d )PAV
e
(L,d)
(4.2.20)
where the error-probability Pe(  d) = PY
e (  d) refers to list decoding and PAV
e
(L,d) =
PAV
e
(Y ,L,d) stands for the error-probability under decoding rule d averaged over
all subcodes in Y of size L.
Now, going back to the product-channel with marginal capacities C1 and C2,
choose R > C1 +C2, set Î· = (Râˆ’C1 âˆ’C2)/2 and let the list size be L = eRLÏ„, with
RL = C2 + Î·. Suppose we use a code Y of size eRÏ„ with a decoding rule d and a
list decoder  d with the list-size L. By using (4.2.20), write
Pe â‰¥Pe(  d)PAV
e
(eRLÏ„,d)
(4.2.21)
and use the facts that RL > C2 and the value PAV
e
(eRLÏ„,d) is bounded away from
zero. The assertion of the inverse part follows from the following observation dis-
cussed in Worked Example 4.2.8. Take R2 < Râˆ’RL and consider subcodes L âŠ‚Y
of size â™¯L = eR2Ï„. Suppose we choose subcode L at random, with equal proba-
bilities. Let M2 = eR2Ï„ and PY ,M2
e
(d) stand for the mean error-probability averaged
over all subcodes L âŠ‚Y of size â™¯L = eR2Ï„. Then
Pe(  d) â‰¥PY ,M2
e
(d)+Îµ(Ï„)
(4.2.22)
where Îµ(Ï„) â†’0 as Ï„ â†’âˆ.
Worked Example 4.2.8
Let L = eRLÏ„ and M = eRÏ„. We aim to show that if
R2 < Râˆ’RL and M2 = eR2Ï„ then the following holds. Given a code X of size M,
a decoding rule d and a list decoder  d with list-size L, consider the mean error-
probability PX ,M2
e
(d) averaged over the equidistributed subcodes S âŠ‚X of size
â™¯S = M2. Then PX ,M2
e
(d) and the list-error-probability PX
e (  d) satisfy
PX
e (  d) â‰¥PX ,M2
e
(d)+Îµ(Ï„)
(4.2.23)
where Îµ(Ï„) â†’0 as Ï„ â†’âˆ.
Solution Let X , S and d be as above and suppose we use a list decoder  d with
list-length L.

406
Further Topics from Information Theory
Given a subcode S âŠ‚X with M2 codewords, we will use the following de-
coding. Let L be the output of decoder  d. If exactly one element xj âˆˆS belongs
to L , the decoder for S will declare xj. Otherwise, it will pronounce an error.
Denote the decoder for S by dS . Thus, given that xk âˆˆS was transmitted, the
resulting error-probability, under the above decoding rule, takes the form
Pek = âˆ‘
L
p(L |xk)ES (L |xk)
where p(L |xk) is a probability of obtaining the output L after transmitting xk
under the rule dX and ES (L |xk) is the error-probability for dS . Next, split
ES (L |xk) = E1
S (L |xk) + E2
S (L |xk) where E1
S (L |xk) stands for the probabil-
ity that xk Ì¸âˆˆL and E2
S (L |xk) for the probability that word xk âˆˆL was decoded
by a wrong code-vector from S (both probabilities conditional upon sending xk).
Further, E2
S (L |xk) is split into a sum of (conditional) probabilities ES (L ,xj|xk)
that the decoder returned vector xj âˆˆL with j Ì¸= k.
Let PS
e (d) = PS , AV
e
(d) denote the average error-probability for subcode S .
The above construction yields
PS
e (d) â‰¤1
M2 âˆ‘
k: xkâˆˆS âˆ‘
L
p(L |xk)

E1
S (L |xk)+ âˆ‘
jÌ¸=k
ES (L ,xj|xk)

.
(4.2.24)
Inequality (4.2.24) is valid for any subcode S . We now select S at random from
X choosing each subcode of size M2 with equal probability. After averaging over
all such subcodes we obtain a bound for the averaged error-probability PX ,M2
e
=
PX ,M2
e
(d):
PX ,M2
e
â‰¤PX
e (  d)+ 1
M2
M2
âˆ‘
k=1âˆ‘
L âˆ‘
jÌ¸=k
C
p(L |xk)Eâ€¢(L ,xj|xk)
DX ,M2
(4.2.25)
where
I
JX ,M2 means the average over all selections of subcodes. As xj and xk
are chosen independently,
C
p(L |xk)E2(L ,xj)
DX ,M2 =
C
p(L |xk)
DX ,M2C
E2
â€¢(L ,xj)
DX ,M2.
Next,
C
p(L |xk)
DX ,M2 = âˆ‘
xâˆˆX
1
M p(L |x),
C
E2
â€¢(L ,xj|xk)
DX ,M2 = L
M,
and we obtain
PX ,M2
e
â‰¤PX
e (  d)+ 1
M2
M2
âˆ‘
k=1âˆ‘
L

âˆ‘
xâˆˆX
1
M p(L |x)

âˆ‘
jÌ¸=k
L
M


4.2 The asymptotic equipartition property in continuous time setting
407
which implies
PX ,M2
e
â‰¤PX
e (  d)+ M2L
M .
(4.2.26)
Since M2L/M = eR2Ï„eâˆ’(Râˆ’RL)Ï„ â†’0 when Ï„ â†’âˆas R2 < Râˆ’RL, inequality (4.2.23)
is proved.
We now give the proof of Theorem 4.2.1. Consider the sequence of kth-order
Markov approximations of a process X, by setting
p(k)
Xnâˆ’1
0

= pXkâˆ’1
0

Xkâˆ’1
0
 nâˆ’1
âˆ
i=k
p

Xi|Xiâˆ’1
iâˆ’k

.
(4.2.27)
Set also
H(k) = E
	
âˆ’log p

X0|Xâˆ’1
âˆ’k

= h(X0|Xâˆ’1
âˆ’k )
(4.2.28)
and
H = E
	
âˆ’log p

X0|Xâˆ’1
âˆ’âˆ

= h(X0|Xâˆ’1
âˆ’âˆ).
(4.2.29)
The proof is based on the following three results: Lemma 4.2.9 (the sandwich
lemma), Lemma 4.2.10 (a Markov approximation lemma) and Lemma 4.2.11 (a
no-gap lemma).
Lemma 4.2.9
For any stationary process X,
limsup
nâ†’âˆ
1
n log p(k)(Xnâˆ’1
0
)
p(Xnâˆ’1
0
)
â‰¤0 a.s.,
(4.2.30)
limsup
nâ†’âˆ
1
n log
p(Xnâˆ’1
0
)
p(Xnâˆ’1
0
|Xâˆ’1
âˆ’âˆ) â‰¤0 a.s.
(4.2.31)
Proof
If An is a support event for pXnâˆ’1
0
(i.e. P(Xnâˆ’1
0
âˆˆAn) = 1), write
E p(k)(Xnâˆ’1
0
)
p(Xnâˆ’1
0
)
= âˆ‘
xnâˆ’1
0
âˆˆAn
p(xnâˆ’1
0
) p(k)(xnâˆ’1
0
)
p(xnâˆ’1
0
)
= âˆ‘
xnâˆ’1
0
âˆˆAn
p(k)(xnâˆ’1
0
)
= p(k)(A) â‰¤1.

408
Further Topics from Information Theory
Similarly, if Bn = Bn(Xâˆ’1
âˆ’âˆ) is a support event for pXnâˆ’1
0
|Xâˆ’1
âˆ’âˆ(i.e. P(Xnâˆ’1
0
âˆˆ
Bn|Xâˆ’1
âˆ’âˆ) = 1), write
E
p(Xnâˆ’1
0
)
p(Xnâˆ’1
0
|Xâˆ’1
âˆ’âˆ) = EXâˆ’1
âˆ’âˆâˆ‘
xnâˆ’1
0
âˆˆBn
p(xnâˆ’1
0
|Xâˆ’1
âˆ’âˆ)
p(xnâˆ’1
0
)
p(xnâˆ’1
0
|Xâˆ’1
âˆ’âˆ)
= EXâˆ’1
âˆ’âˆâˆ‘
xnâˆ’1
0
âˆˆBn
p(xnâˆ’1
0
)
= EXâˆ’1
âˆ’âˆP(Bn) â‰¤1.
By the Markov inequality,
P

p(k)(Xnâˆ’1
0
)
p(Xnâˆ’1
0
)
â‰¥tn

= P

1
n log p(k)(Xnâˆ’1
0
)
p(Xnâˆ’1
0
)
â‰¥1
tn
logtn

â‰¤1
tn
,
and similarly for P

p(Xnâˆ’1
0
)
p(Xnâˆ’1
0
|Xâˆ’1
âˆ’âˆ) â‰¥tn

. Letting tn = n2 so that âˆ‘
n 1/tn < âˆand
using the Borelâ€“Cantelli lemma completes the proof.
Lemma 4.2.10
For a stationary ergodic process X,
âˆ’1
n log p(k)(Xnâˆ’1
0
)
a.s.
â‡’H(k),
(4.2.32)
âˆ’1
n log p(Xnâˆ’1
0
|Xâˆ’1
âˆ’âˆ)
a.s.
â‡’H.
(4.2.33)
Proof
Substituting
f = âˆ’log p(X0|Xâˆ’1
âˆ’k ) and
f = âˆ’log p(X0|Xâˆ’1
âˆ’âˆ) into
Birkhoffâ€™s ergodic theorem (see for example Theorem 9.1 from [36]) yields
âˆ’1
n log p(k)(Xnâˆ’1
0
) = âˆ’1
n log p(Xkâˆ’1
0
)âˆ’1
n
nâˆ’1
âˆ‘
i=k
log p(k)(Xi|Xiâˆ’1
iâˆ’k )
a.s.
â‡’0+H(k)
(4.2.34)
and
âˆ’1
n log p(Xnâˆ’1
0
|Xâˆ’1
âˆ’âˆ) = âˆ’1
n
nâˆ’1
âˆ‘
i=0
log p(Xi|Xiâˆ’1
âˆ’âˆ)
a.s.
â‡’H,
(4.2.35)
respectively.
So, by Lemmas 4.2.9 and 4.2.10,
limsup
nâ†’âˆ
1
n log
1
p(Xnâˆ’1
0
) â‰¤lim
nâ†’âˆ
1
n log
1
p(k)(Xnâˆ’1
0
) = H(k),
(4.2.36)

4.3 The Nyquistâ€“Shannon formula
409
and
liminf
nâ†’âˆ
1
n log
1
p(Xnâˆ’1
0
) â‰¥lim
nâ†’âˆ
1
n log
1
p(Xnâˆ’1
0
|Xâˆ’1
âˆ’âˆ) = H,)
which we rewrite as
H â‰¤liminf
nâ†’âˆâˆ’1
n log p(Xnâˆ’1
0
) â‰¤limsup
nâ†’âˆ
âˆ’1
n log p(Xnâˆ’1
0
) â‰¤H(k).
(4.2.37)
Lemma 4.2.11
For any stationary process X, H(k) â†˜H = H.
Proof
The convergence H(k) â†˜H follows by stationarity and by conditioning
not to increase entropy. It remains to show that H(k) â†˜H, so that H = H. The
Doobâ€“LÂ´evy martingale convergence theorem for conditional probabilities yields
p

X0 = x0|Xâˆ’1
âˆ’k
a.s.
â‡’p

X0 = x0|Xâˆ’1
âˆ’âˆ

, k â†’âˆ.
(4.2.38)
As the set of values I is supposed to be ï¬nite, and the function p âˆˆ[0,1] â†’âˆ’plog p
is bounded, the bounded convergence theorem gives that as k â†’âˆ,
H(k) = Eâˆ’âˆ‘
x0âˆˆI
p(X0 = x0|Xâˆ’1
âˆ’k )log p(X0 = x0|Xâˆ’1
âˆ’k )
â†’E

âˆ’âˆ‘
x0âˆˆI
p(X0 = x0|Xâˆ’1
âˆ’âˆ)log p(X0 = x0|Xâˆ’1
âˆ’âˆ)

= H.
4.3 The Nyquistâ€“Shannon formula
In this section we give a rigorous derivation of the famous Nyquistâ€“Shannon for-
mula1 for the capacity of a continuous-time channel with the power constraint and
a ï¬nite bandwidth, the result broadly considered an ultimate fact of information
theory. Our exposition follows (with minor deviations) the paper [173]. Because
it is quite long, we divide the section into subsections, each of which features a
particular step of the construction.
Harry Nyquist (1889â€“1976) is considered a pioneer of information theory whose
works, together with those of Ralph Hartley (1888â€“1970), helped to create the
concept of the channel capacity.
1 Some authors speak in this context of a Shannonâ€“Hartley theorem.

410
Further Topics from Information Theory
The setting is as follows. Fix numbers Ï„,Î±, p > 0 and assume that every Ï„ seconds
a coder produces a real code-vector
x =
â›
âœ
â
x1
...
xn
â
âŸ
â 
where n = âŒˆÎ±Ï„âŒ‰. All vectors x generated by the coder lie in a ï¬nite set X = Xn âŠ‚
Rn of cardinality M âˆ¼2RbÏ„ = eRnÏ„ (a codebook); sometimes we write, as before,
XM,n to stress the role of M and n. It is also convenient to list the code-vectors
from X as x(1),...,x(M) (in an arbitrary order) where
x(i) =
â›
âœ
â
x1(i)
...
xn(i)
â
âŸ
â ,
1 â‰¤i â‰¤M.
Code-vector x is then converted into a continuous-time signal
x(t) =
n
âˆ‘
i=1
xiÏ†i(t), where 0 â‰¤t â‰¤Ï„,
(4.3.1)
by using an orthonormal basis in Å2[0,Ï„] formed by functions Ï†i(t),i = 1,2,...
(with
1 Ï„
0 Ï†i(t)Ï† j(t)dt = Î´i j). Then the entry xi can be recovered by integration:
xi =
0 Ï„
0 x(t)Ï†i(t)dt.
(4.3.2)
The instantaneous signal power at time t is associated with |x(t)|2; then the square-
norm ||x||2 =
1 Ï„
0 |x(t)|2dt =
âˆ‘
1â‰¤iâ‰¤n
|xi|2 will represent the full energy of the signal in
the interval [0,Ï„]. The upper bound on the total energy spent on transmission takes
the form
||x||2 â‰¤pÏ„, or x âˆˆBn(âˆšpÏ„).
(4.3.3)
(In the theory of waveguides, the dimension n is called the Nyquist number and the
value W = n/(2Ï„) âˆ¼Î±/2 the bandwidth of the channel.)
The code-vector x(i) is sent through an additive channel, where the receiver gets
the (random) vector
Y =
â›
âœ
â
Y1
...
Yn
â
âŸ
â where Yk = xk(i)+Zk, 1 â‰¤k â‰¤n.
(4.3.4)

4.3 The Nyquistâ€“Shannon formula
411
The assumption we will adopt is that
Z =
â›
âœ
â
Z1
...
Zn
â
âŸ
â 
is a vector with IID entries Zk âˆ¼N(0,Ïƒ2). (In applications, engineers use the rep-
resentation Zi =
1 Ï„
0 Z(t)Ï†i(t)dt, in terms of a â€˜white noiseâ€™ process Z(t).)
From the start we declare that if x(i) âˆˆX \ Bn(âˆšpÏ„), i.e. ||x(i)||2 > pÏ„, the
output signal vector Y is rendered â€˜non-decodableâ€™. In other words, the probability
of correctly decoding the output vector Y = x(i)+Z with ||x(i)||2 > pÏ„ is taken to
be zero (regardless of the fact that the noise vector Z can be small and the output
vector Y close to x(i), with a positive probability).
Otherwise, i.e. when ||x(i)||2 â‰¤pÏ„, the receiver applies, to the output vector Y,
a decoding rule d(= dn,X ), i.e. a map y âˆˆK â†’d(y) âˆˆX where K âŠ‚Rn is a
â€˜decodable domainâ€™ (where map d had been deï¬ned). In other words, if Y âˆˆK
then vector Y is decoded as d(Y) âˆˆX . Here, an error arises either if Y Ì¸âˆˆK or if
d(Y) Ì¸= x(i) given that x(i) was sent. This leads to the following formula for the
probability of erroneously decoding the input code-vector x(i):
Pe(i,d) =
â§
â¨
â©
1,
||x(i)||2 > pÏ„,
Pch

Y Ì¸âˆˆK or d(Y) Ì¸= x(i)|x(i) sent

,
||x(i)||2 â‰¤pÏ„.
(4.3.5)
The average error-probability Pe = PX ,av
e
(d) for the code X is then deï¬ned by
Pe = 1
M âˆ‘
1â‰¤iâ‰¤M
Pe(i,d).
(4.3.6)
Furthermore, we say that Rbit (or Rnat) is a reliable transmission rate (for given
Î± and p) if for all Îµ > 0 we can specify Ï„0(Îµ) > 0 such that for all Ï„ > Ï„0(Îµ)
there exists a codebook X of size â™¯X âˆ¼eRnatÏ„ and a decoding rule d such that
Pe = PX ,av
e
(d) < Îµ. The channel capacity C is then deï¬ned as the supremum of all
reliable transmission rates, and the argument from Section 4.1 yields
C = Î±
2 ln

1+
p
Î±Ïƒ2

(in nats);
(4.3.7)
cf. (4.1.17). Note that when Î± â†’âˆ, the RHS in (4.3.7) tends to p/(2Ïƒ2).

412
Further Topics from Information Theory
0
T
.......... ....
.
.. . ....... .. .
..
. .. .
... .
.
.
. .
... ... ...... ..... .....
...
..
..
.
.
... .
. ..
.
.
.... ..
. .
....
.
.
.
.. ..
...
Figure 4.4
In the time-continuous set-up, Shannon (and Nyquist before him) discussed an
application of formula (4.3.7) to band-limited signals. More precisely, set W =
Î±/2; then the formula
C = W ln

1+
p
2Ïƒ2
0W

(4.3.8)
should give the capacity of the time-continuous additive channel with white noise
of variance Ïƒ2 = Ïƒ2
0W, for a band-limited signal x(t) with the spectrum in [âˆ’W,W]
and of energy per unit time â‰¤p.
This sentence, perfectly clear to a qualiï¬ed engineer, became a stumbling point
for mathematicians and required a technically involved argument for justifying its
validity. In engineersâ€™ language, an â€˜idealâ€™ orthonormal system on [0,Ï„] to be used
in (4.3.1) would be a collection of n âˆ¼2WÏ„ equally spaced Î´-functions. In other
words, it would have been very convenient to represent the code-vector x(i) =
(x1(i),...,xn(i)) by a function fi(t), of the time argument t âˆˆ[0,Ï„], given by the
sum
fi(t) = âˆ‘
1â‰¤kâ‰¤n
xk(i)Î´

t âˆ’k
2W

(4.3.9)
where n = âŒˆ2WÏ„âŒ‰(and Î± = 2W). Here Î´(t) represents a â€˜unit impulseâ€™ appearing
near time 0 and graphically visualised as an â€˜acute unit peakâ€™ around point t =
0. Then the shifted function Î´ (t âˆ’k/(2W)) yields a peak concentrated near t =
k/(2W), and the graph of function fi(t) is shown in Figure 4.4.
We may think that our coder produces functions xi(t) every Ï„ seconds, and each
such function is the result of encoding a message i. Moreover, within each time
interval of length Ï„, the peaks xk(i)Î´ (t âˆ’k/(2W)) appear at time-step 1/(2W).
Here Î´ (t âˆ’k/(2W)) is the time-shifted Dirac delta-function.
The problem is that Î´(t) is a so-called â€˜generalised functionâ€™, and Î´ Ì¸âˆˆÅ2. A way
to sort out this difï¬culty is to pass the signal through a low-frequency ï¬lter. This
produces, instead of fi(t), the function f 
i(t)(= f 
W,i(t)) given by

4.3 The Nyquistâ€“Shannon formula
413
f 
i(t) = âˆ‘
1â‰¤kâ‰¤n
xk(i)sinc(2Wt âˆ’k).
(4.3.10)
Here
sinc(2Wt âˆ’k) = sin (Ï€(2Wt âˆ’k))
Ï€(2Wt âˆ’k)
(4.3.11)
is the value of the shifted and rescaled (normalised) sinc function:
sinc(s) =
â§
â¨
â©
sin(Ï€s)
Ï€s
,
s Ì¸= 0,
1,
s = 0,
s âˆˆR,
(4.3.12)
featured in Figure 4.5.
The procedure of removing high-frequency harmonics (or, more generally, high-
resolution components) and replacing the signal fi(t) with its (approximate) lower-
resolution version f 
i(t) is widely used in modern computer graphics and other areas
of digital processing.
Example 4.3.1
(The Fourier transform in Å2) Recall that the Fourier transform
Ï† â†’FÏ† of an integrable function Ï† (i.e. a function with
1 |Ï†(x)|dx < +âˆ) is de-
ï¬ned by
	
FÏ†

(Ï‰) =
0
Ï†(x)eiÏ‰xdx, Ï‰ âˆˆR.
(4.3.13)
The inverse Fourier transform can be written as an inverse map:
	
Fâˆ’1Ï†

(x) = 1
2Ï€
0
Ï†(Ï‰)eâˆ’iÏ‰xdÏ‰.
(4.3.14)
A profound fact is that (4.3.13) and (4.3.14) can be extended to square-integrable
functions Ï† âˆˆÅ2(R) (with âˆ¥Ï†âˆ¥2 =
1 |Ï†(x)|2dx < +âˆ). We have no room here to
go into detail; the enthusiastic reader is referred to [127]. Moreover, the Fourier-
transform techniques turn out to be extremely useful in numerous applications. For
instance, denoting FÏ† = Ï† and writing Fâˆ’1 Ï† = Ï†, we obtain from (4.3.13), (4.3.14)
that
Ï†(x) = 1
2Ï€
0
Ï†(Ï‰)eâˆ’ixÏ‰dÏ‰.
(4.3.15)
In addition, for any two square-integrable functions Ï†1,Ï†2 âˆˆÅ2(R),
2Ï€
0
Ï†1(x)Ï†2(x)dx =
0
Ï†1(Ï‰)Ï†2(Ï‰)dÏ‰.
(4.3.16)

414
Further Topics from Information Theory
âˆ’4
âˆ’
4
2
0
2
0.0
0.2
0.4
0.6
tâˆ’s
K(tâˆ’s)
W=0.5
W=1
W=2
Figure 4.5

4.3 The Nyquistâ€“Shannon formula
415
Furthermore, the Fourier transform can be deï¬ned for generalised functions too;
see again [127]. In particular, the equations similar to (4.3.13)â€“(4.3.14) for the
delta-function look like this:
Î´(t) = 1
2Ï€
0
eâˆ’iÏ‰tdÏ‰, 1 =
0
Î´(t)eitÏ‰dt,
(4.3.17)
implying that the Fourier transform of the Dirac delta is Î´(Ï‰) â‰¡1. For the shifted
delta-function we obtain
Î´

t âˆ’k
2W

= 1
2Ï€
0
eikÏ‰/(2W) eâˆ’iÏ‰tdÏ‰.
(4.3.18)
The Shannonâ€“Nyquist formula is established for a device where the channel is
preceded by a â€˜ï¬lterâ€™ that â€˜cuts offâ€™ all harmonics eÂ±itÏ‰ with frequencies Ï‰ outside
the interval [âˆ’2Ï€W,2Ï€W]. In other words, a (shifted) unit impulse Î´ (t âˆ’k/(2W))
in (4.3.18) is replaced by its cut-off version which emerges after the ï¬lter cuts off
the harmonics eâˆ’itÏ‰ with |Ï‰| > 2Ï€W.
The sinc function (a famous object in applied mathematics) is a classical function
arising when we reduce the integral in Ï‰ in (4.3.17) to the interval [âˆ’Ï€,Ï€]:
sinc(t) = 1
2Ï€
0 Ï€
âˆ’Ï€ eâˆ’iÏ‰tdÏ‰, 1[âˆ’Ï€,Ï€](Ï‰) =
0
sinc(t)eitÏ‰dt,
t,Ï‰ âˆˆR1
(4.3.19)
(symbolically, function sinc = Fâˆ’11[âˆ’Ï€,Ï€]). In our context, the function t â†’
Asinc(At) can be considered, for large values of parameter A > 0, as a conve-
nient approximation for Î´(t). A customary caution is that sinc(t) is not an inte-
grable function on the whole axis R (due to the 1/t factor), although it is square-
integrable:
1 
sinct
2dt < âˆ. Thus, the right equation in (4.3.19) should be under-
stood in an Å2-sense.
However, it does not make the mathematical and physical aspects of the theory
less tricky (as well as engineering ones). Indeed, an ideal ï¬lter producing a clear cut
of unwanted harmonics is considered, rightly, as â€˜physically unrealisableâ€™. More-
over, assuming that such a perfect device is available, we obtain a signal f 
i(t) that
is no longer conï¬ned to the time interval [0,Ï„] but is widely spread in the whole
time axis. To overcome this obstacle, one needs to introduce further technical ap-
proximations.
Worked Example 4.3.2
Verify that the functions
t â†’

2
âˆš
Ï€W

sinc(2Wt âˆ’k), k = 1,...,n,
(4.3.20)
are orthonormal in the space Å2(R1):

4Ï€W
0
[sinc(2Wt âˆ’k)]
	
sinc

2Wt âˆ’kâ€²
dt = Î´kkâ€².

416
Further Topics from Information Theory
Solution The shortest way to see this is to write the Fourier-decomposition (in
Å2(R)) implied by (4.3.19):
2
âˆš
Ï€W sinc(2Wt âˆ’k) =
1
2
âˆš
Ï€W
0 2Ï€W
âˆ’2Ï€W eikÏ‰/(2W)eâˆ’itÏ‰dÏ‰
(4.3.21)
and check that the functions representing the Fourier-transforms
1
2
âˆš
Ï€W 1(|Ï‰| â‰¤2Ï€W)eikÏ‰/(2W), k = 1,...,n,
are orthonormal. That is,
1
4Ï€W
0 2Ï€W
âˆ’2Ï€W ei(kâˆ’kâ€²)Ï‰/(2W)dÏ‰ = Î´kkâ€²
(4.3.22)
where
Î´kkâ€² =
'
1,
k = kâ€²,
0,
k Ì¸= kâ€²,
is the Kronecker symbol. But (4.3.22) can be veriï¬ed by a standard integration.
Since functions in (4.3.20) are orthonormal, we obtain that
||x(i)||2 = (4Ï€W)|| f 
i ||2, where || f 
i ||2 =
0
| f 
i(t)|2dt,
(4.3.23)
and functions f 
i have been introduced in (4.3.10). Thus, the power constraint can
be written as
|| f 
i ||2 â‰¤pÏ„/4Ï€W = p0.
(4.3.24)
In fact, the coefï¬cients xk(i) coincide with the values f 
i(k/(2W)) of function f 
i
calculated at time points k/(2W), k = 1,...,n; these points can be referred to as
â€˜sampling instancesâ€™.
Thus, the input signal f 
i(t) develops in continuous time although it is completely
speciï¬ed by its values f 
i(k/(2W)) = xk(i). Thus, if we think that different signals
are generated in disjoint time intervals (0,Ï„),(Ï„,2Ï„),..., then, despite interference
caused by inï¬nite tails of the function sinc(t), these signals are clearly identiï¬able
through their values at sampling instances.
The Nyquistâ€“Shannon assumption is that signal f 
i(t) is transformed in the chan-
nel into
g(t) = f 
i(t)+Z (t).
(4.3.25)

4.3 The Nyquistâ€“Shannon formula
417
Here Z (t) is a stationary continuous-time Gaussian process with the zero mean
(EZ (t) â‰¡0) and the (auto-)correlation function
E
	
Z (s)Z (t +s)

= 2Ïƒ2
0W sinc(2Wt), t,s âˆˆR.
(4.3.26)
In particular, when t is a multiple of Ï€/W (i.e. point t coincides with a sampling
instance), the random variables Z (s) and Z (t + s) are independent. An equivalent
form of this condition is that the spectral density
Î¦(Ï‰) :=
0
eitÏ‰E
	
Z (0)Z (t)

dt = Ïƒ2
0 1

|Ï‰| < 2Ï€W

.
(4.3.27)
We see that the received continuous-time signal y(t) can be identiï¬ed through
its values yk = y
 k
2W

via equations
yk = xk(i)+Zk where Zk = Z 
 k
2W

are IID N(0,2Ïƒ2
0W).
This corresponds to the system considered in Section 4.1 with p = 2W p0 and
Ïƒ2 = 2Ïƒ2
0W. It has been generally believed in the engineering community that
the capacity C of the current system is given by (4.3.8), i.e. the transmission rates
below this value of C are reliable and above it they are not.
However, a number of problems are to be addressed, in order to understand formula
(4.3.8) rigorously. One is that, as was noted above, a â€˜sharâ€™ ï¬lter band-limiting the
signal to a particular frequency interval is an idealised device. Another is that the
output signal g(t) in (4.3.25) can be reconstructed after it has been recorded over a
small time interval because any sample function of the form
t âˆˆR â†’âˆ‘
1â‰¤kâ‰¤n
(xk(i)+zk)sinc(2Wt âˆ’k)
(4.3.28)
is analytic in t. Therefore, the notion of rate should be properly re-deï¬ned.
The simplest solution (proposed in [173]) is to introduce a class of functions
A (Ï„,W, p0) which are
(i) approximately band-limited to W cycles per a unit of time (say, a second),
(ii) supported by a time interval of length Ï„ (it will be convenient to specify this
interval as [âˆ’Ï„/2,Ï„/2]),
(iii) have the total energy (the Å2(R)-norm) not exceeding p0Ï„.
These restrictions determine the regional constraints upon the system.

418
Further Topics from Information Theory
Thus, consider a code X
of size M âˆ¼eRÏ„, i.e. a collection of functions
f 
1(t),..., f 
M(t), of a time variable t. If a given code-function f 
i Ì¸âˆˆA (Ï„,W, p0), it
is declared non-decodable: it generates an error with probability 1. Otherwise, the
signal f 
i âˆˆA (Ï„,W, p0) is subject to the additive Gaussian noise Z (t) with mean
EZ (t) â‰¡0 characterised by (4.3.27) and is transformed to g(t) = f 
i(t) + Z (t), the
signal at the output port of the channel (cf. (4.3.25)). The receiver uses a decoding
rule, i.e. a map d : K â†’X where K is, as earlier, the domain of deï¬nition of d,
i.e. some given class of functions where map d is deï¬ned. (As before, the decoder
d may vary with the code, prompting the notation d = dX .) Again, if g Ì¸âˆˆK, the
transmission is considered as erroneous. Finally, if g âˆˆK then the received signal g
is decoded by the code-function dX (g)(t) âˆˆX . The probability of error for code
X when the code-signal generated by the coder was f 
i âˆˆX is set to be
Pe(i) =
'
1,
f 
i Ì¸âˆˆA (Ï„,W, p0),
Pch

Kc âˆª{g : dX (g) Ì¸= f 
i }

,
f 
i âˆˆA (Ï„,W, p0).
(4.3.29)
The average error-probability Pe = PX ,av
e
(d) for code X (and decoder d) equals
Pe = 1
M âˆ‘
1â‰¤iâ‰¤M
Pe(i,d).
(4.3.30)
Value R(= Rnat) is called a reliable transmission rate if, for all Îµ > 0, there exists Ï„
and a code X of size M âˆ¼eRÏ„ such that Pe < Îµ.
Now ï¬x a value Î· âˆˆ(0,1). The class A (Ï„,W, p0) = A (Ï„,W, p0,Î·) is deï¬ned as
the set of functions f â—¦(t) such that
(i) f â—¦= DÏ„ f where
DÏ„ f(t) = f(t)1(|t| < Ï„/2), t âˆˆR,
and f(t) has the Fourier transform
1 eitÏ‰ f(t)dt vanishing for |Ï‰| > 2Ï€W;
(ii) the ratio
|| f â—¦||2
|| f||2 â‰¥1âˆ’Î·;
and
(iii) the norm || f â—¦||2 â‰¤p0Ï„.
In other words, the â€˜transmittableâ€™ signals f â—¦âˆˆA (Ï„,W, p0,Î·) are â€˜sharply lo-
calisedâ€™ in time and â€˜nearly band-limitedâ€™ in frequency.

4.3 The Nyquistâ€“Shannon formula
419
The Nyquistâ€“Shannon formula can be obtained as a limiting case from several
assertions; the simplest one is Theorem 4.3.3 below. An alternative approach will
be presented later in Theorem 4.3.7.
Theorem 4.3.3
The capacity C = C(Î·) of the above channel with constraint
domain A (Ï„,W, p0,Î·) described in conditions (i)â€“(iii) above is given by
C = W ln

1+
p0
2Ïƒ2
0W

+
Î·
1âˆ’Î·
p0
Ïƒ2
0
.
(4.3.31)
As Î· â†’0,
C(Î·) â†’W ln

1+
p0
2Ïƒ2
0W

(4.3.32)
which yields the Nyquistâ€“Shannon formula (4.3.8).
Before going to (quite involved) technical detail, we will discuss some facts rele-
vant to the product, or parallel combination, of r time-discrete Gaussian channels.
(In essence, this model was discussed at the end of Section 4.2.) Here, every Ï„ time
units, the input signal is generated, which is an ordered collection of vectors
*
x(1),...,x(r)+
where x( j) =
â›
âœ
âœ
â
x( j)
1...
x( j)
n j
â
âŸ
âŸ
â âˆˆRn j, 1 â‰¤j â‰¤r,
(4.3.33)
and n j = âŒˆÎ± jÏ„âŒ‰with Î± j being a given value (the speed of the digital production
from coder j). For each vector x( j) we consider a speciï¬c power constraint:
OOOx( j)OOO
2
â‰¤p( j)Ï„, 1 â‰¤j â‰¤r.
(4.3.34)
The output signal is a collection of (random) vectors
(
Y(1),...,Y(r))
where Y( j) =
â›
âœ
âœ
â
Y ( j)
1...
Y ( j)
n j
â
âŸ
âŸ
â and Y ( j)
k
= x( j)
k +Z( j)
k ,
(4.3.35)
with Z( j)
k
being IID random variables, Z( j)
k
âˆ¼N

0,Ïƒ( j)2
, 1 â‰¤k â‰¤n j, 1 â‰¤j â‰¤r.

420
Further Topics from Information Theory
A codebook X with information rate R, for the product-channel under consid-
eration, is an array of M input signals,
(
x(1)(1),...,x(r)(1)


x(1)(2),...,x(r)(2)

...
...
...

x(1)(M),...,x(r)(M)
)
,
(4.3.36)
each of which has the same structure as in (4.3.33). As before, a decoder d is a map
acting on a given set K of sample output signals
*
y(1),...,y(r)+
and taking these
signals to X .
As above, for i = 1,...,M, we deï¬ne the error-probability Pe(i,d) for code X
when sending an input signal

x(1)(i),...,x(r)(i)

:
Pe(i,d) = 1,
if
x( j)(i)

2
â‰¥p( j)Ï„ for some j = 1,...,r,
and
Pe(i,d) = Pch
*
Y(1),...,Y(r)+
Ì¸âˆˆK or
d
*
Y(1),...,Y(r)+
Ì¸=
*
x(1)(i),...,x(r)(i)
+
|
*
x(1)(i),...,x(r)(i)
+
sent

,
if
x( j)(i)
2 < p( j)Ï„
for all j = 1,...,r.
The average error-probability Pe = PX ,av
e
(d) for code X (while using decoder d)
is then again given by
Pe = 1
M âˆ‘
1â‰¤iâ‰¤M
Pe(i,d).
As usual, R is said to be a reliable transmission rate if for all Îµ > 0 there exists a
Ï„0 > 0 such that for all Ï„ > Ï„0 there exists a code X of cardinality M âˆ¼eRÏ„ and a
decoding rule d such that Pe < Îµ. The capacity of the combined channel is again de-
ï¬ned as the supremum of all reliable transmission rates. In Worked Example 4.2.7
the following fact has been established (cf. Lemma A in [173]; see also [174]).
Lemma 4.3.4
The capacity of the product-channel under consideration equals
C = âˆ‘
1â‰¤jâ‰¤r
Î± j
2 ln

1+ p( j)
Î± jÏƒ2
j

.
(4.3.37)
Moreover, (4.3.37) holds when some of the Î±j equal +âˆ: in this case the corre-
sponding summand takes the form p( j)
2Ïƒ2
j .

4.3 The Nyquistâ€“Shannon formula
421
Our next step is to consider jointly constrained products of time-discrete Gaussian
channels. We discuss the following types of joint constraints.
Case I. Take r = 2, assume Ïƒ2
1 = Ïƒ2
2 = Ïƒ2
0 and replace condition (4.3.34) with
âˆ¥x(1)âˆ¥2 +âˆ¥x(2)âˆ¥2 < p0Ï„.
(4.3.38a)
In addition, if Î±1 â‰¤Î±2, we introduce Î² âˆˆ(0,1) and require that
âˆ¥x(2)âˆ¥2 â‰¤Î²

âˆ¥x(1)âˆ¥2 +âˆ¥x(2)âˆ¥2
.
(4.3.38b)
Otherwise, i.e. if Î±2 â‰¤Î±1, formula (4.3.38b) is replaced by
âˆ¥x(1)âˆ¥2 â‰¤Î²

âˆ¥x(1)âˆ¥2 +âˆ¥x(2)âˆ¥2
.
(4.3.38c)
Case II. Here we take r = 3 and assume that Ïƒ2
1 = Ïƒ2
2 â‰¥Ïƒ2
3 and Î±3 = +âˆ. The
requirements are now that
âˆ‘
1â‰¤jâ‰¤3
âˆ¥x( j)âˆ¥2 < p0Ï„
(4.3.39a)
and
âˆ¥x(3)âˆ¥2 â‰¤Î² âˆ‘
1â‰¤jâ‰¤3
âˆ¥x( j)âˆ¥2.
(4.3.39b)
Case III. As in Case I, take r = 2 and assume Ïƒ2
1 = Ïƒ2
2 = Ïƒ2
0 . Further, let Î±2 = +âˆ.
The constraints now are
âˆ¥x(1)âˆ¥2 < p0Ï„
(4.3.40a)
and
âˆ¥x(2)âˆ¥2 < Î²

âˆ¥x(1)âˆ¥2 +âˆ¥x(2)âˆ¥2
.
(4.3.40b)
Worked Example 4.3.5
(cf. Theorem 1 in [173]). We want to prove that the
capacities of the above combined parallel channels of types Iâ€“III are as follows.
Case I, Î±1 â‰¤Î±2:
C = Î±1
2 ln

1+ (1âˆ’Î¶)p0
Î±1Ïƒ2
0

+ Î±2
2 ln

1+ Î¶ p0
Î±2Ïƒ2
0

(4.3.41a)
where
Î¶ = min

Î²,
Î±2
Î±1 +Î±2

.
(4.3.41b)

422
Further Topics from Information Theory
If Î±2 â‰¤Î±1, subscripts 1 and 2 should replace each other in these equations. Further,
when Î±i = +âˆ, one uses the limiting expression lim
Î±â†’âˆ(Î±/2)ln(1+v/Î±) = v/2. In
particular, if Î±1 < Î±2 = +âˆthen Î² = Î¶, and the capacity becomes
C = Î±1
2 ln

1+ (1âˆ’Î²)p0
Î±1Ïƒ2
0

+Î² p0
2Ïƒ2
0
.
(4.3.41c)
This means that the best transmission rate is attained when one puts as much â€˜en-
ergyâ€™ into channel 2 as is allowed by (4.3.38b).
Case II:
C = Î±1
2 ln

1+ (1âˆ’Î²)p0
(Î±1 +Î±2)Ïƒ2
1

+ Î±2
2 ln

1+ (1âˆ’Î²)p0
(Î±1 +Î±2)Ïƒ2
1

+ Î² p
2Ïƒ2
3
.
(4.3.42)
Case III:
C = Î±1
2 ln

1+
p0
Î±1Ïƒ2
0

+
Î² p0
2(1âˆ’Î²)Ïƒ2
0
.
(4.3.43)
Solution We present the proof for Case I only. For deï¬niteness, assume that Î±1 <
Î±2 â‰¤âˆ. First, the direct part. With p1 = (1âˆ’Î¶)p0, p2 = Î¶ p0, consider the parallel
combination of two channels, with individual power constraints on the input signals
x(1) and x(2):
OOOx(1)OOO
2
â‰¤p1Ï„,
OOOx(2)OOO
2
â‰¤p2Ï„.
(4.3.44a)
Of course, (4.3.44a) implies (4.3.38a). Next, with Î¶ â‰¤Î², condition (4.3.38b) also
holds true. Then, according to the direct part of Lemma 4.3.4, any rate R with
R < C1(p1)+C2(p2) is reliable. Here and below,
CÎ¹(q) = Î±Î¹
2 ln

1+
q
Î±Î¹Ïƒ2
0

, Î¹ = 1,2.
(4.3.44b)
This implies the direct part.
A longer argument is needed to prove the inverse. Set Câˆ—= C1(p1) +C2(p2).
The aim is to show that any rate R > Câˆ—is not reliable. Assume the opposite: there
exists such a reliable R = Câˆ—+Îµ; let us recall what it formally means. There exists
a sequence of values Ï„(l) â†’âˆand (a) a sequence of codes
X (l) =
(
x(i) =
(
x(1)(i),x(2)(i)
)
, 1 â‰¤i â‰¤M(l))

4.3 The Nyquistâ€“Shannon formula
423
of size M(l) âˆ¼eRÏ„(l) composed of â€˜combinedâ€™ code-vectors x(i) =
*
x(1)(i),x(2)(i)
+
with square-norms âˆ¥x(i)âˆ¥2 = âˆ¥x(1)(i)âˆ¥2 + âˆ¥x(2)(i)âˆ¥2, and (b) a sequence of de-
coding maps d(l) : y âˆˆK(l) â†’d(l)(y) âˆˆX (l) such that Pe â†’0. Here, as before,
Pe = PX (l),av
e
(d(l)) stands for the average error-probability:
Pe =
1
M(l)
âˆ‘
1â‰¤iâ‰¤M(l)
Pe(i,d(l))
calculated from individual error-probabilities Pe(i,d(l)):
Pe(i,d(l)) =
â§
âª
âª
â¨
âª
âª
â©
1,
if âˆ¥x(i)âˆ¥2 > p0Ï„(l) or âˆ¥x(2)(i)âˆ¥2 > Î²âˆ¥x(i)âˆ¥2,
Pch

Y Ì¸âˆˆK(l) or d(l)(Y) Ì¸= x(i)| x(i) sent

,
if âˆ¥x(i)âˆ¥2 â‰¤p0Ï„(l) and âˆ¥x(2)(i)âˆ¥2 â‰¤Î²âˆ¥x(i)âˆ¥2.
The component vectors
x(1)(i) =
â›
âœ
âœ
â
x(1)
1 (i)
...
x(1)
âŒˆÎ±1Ï„(l)âŒ‰(i)
â
âŸ
âŸ
â âˆˆRâŒˆÎ±1Ï„(l)âŒ‰
and
x(2)(i) =
â›
âœ
âœ
â
x(1)
1 (i)
...
x(2)
âŒˆÎ±2Ï„(l)âŒ‰(i)
â
âŸ
âŸ
â âˆˆRâŒˆÎ±2Ï„(l)âŒ‰
are sent through their respective parts of the parallel-channel combination, which
results in output vectors
Y(1) =
â›
âœ
âœ
â
Y (1)
1 (i)
...
Y (1)
âŒˆÎ±1Ï„(l)âŒ‰(i)
â
âŸ
âŸ
â âˆˆRâŒˆÎ±1Ï„(l)âŒ‰, Y(2) =
â›
âœ
âœ
â
Y (2)
1 (i)
...
Y (2)
âŒˆÎ±2Ï„(l)âŒ‰(i)
â
âŸ
âŸ
â âˆˆRâŒˆÎ±2Ï„(l)âŒ‰
forming the combined output signal Y =
*
Y(1),Y(2)+
. The entries of vectors Y(1)
and Y(2) are sums
Y (1)
j
= x(1)
j (i)+Z(1)
j , Y (2)
k
= x(2)
k (i)+Z(2)
k ,
where Z(1)
j
and Z(2)
k
are IID, N(0,Ïƒ2
0 ) random variables. Correspondingly, Pch
refers to the joint distribution of the random variables Y (1)
j
and Y (2)
k
, 1 â‰¤j â‰¤
âŒˆÎ±1Ï„(l)âŒ‰, 1 â‰¤k â‰¤âŒˆÎ±2Ï„(l)âŒ‰.
Observe that function q â†’C1(q) is uniformly continuous in q on [0, p0]. Hence,
we can ï¬nd an integer J0 large enough such that
C1(q)âˆ’C1

qâˆ’Î¶ p0
J0
 < Îµ
2 ,
for all q âˆˆ(0,Î¶ p0).

424
Further Topics from Information Theory
Then we partition the code X (l) into J0 classes (subcodes) X (l)
j
, j = 1,...,J0: a
code-vector

x(1)(i),x(2)(i)

falls in class X (l)
j
if
( j âˆ’1)Î¶ p0Ï„
J0
<
âˆ‘
1â‰¤kâ‰¤âŒˆÎ±2Ï„(l)âŒ‰
(x(2)
k )2 â‰¤jÎ¶ p0Ï„
J0
.
(4.3.45a)
Since a transmittable code-vector x has a component x(2) with âˆ¥x(2)âˆ¥2 â‰¤Î¶âˆ¥xâˆ¥2,
each such x lies in one and only one class. (We make an agreement that zero
code-vectors belong to X (l)
1 .) The class X (l)
j
containing the most code-vectors
is denoted by X (l)
âˆ—. Then, obviously, the cardinality â™¯X (l)
âˆ—
â‰¥M(l)
J0, and the
transmission rate Râˆ—of code X (l)
âˆ—
satisï¬es
Râˆ—â‰¥Râˆ’1
Ï„(l) lnJ0.
(4.3.45b)
On the other hand, the maximum error-probability for subcode X (l)
âˆ—
is not larger
than for the whole code X (l) (when using the same decoder d(l)); consequently,
the error-probability PX (l)
âˆ—,av
e

d(l)
â‰¤P(l)
e
â†’0.
Having a ï¬xed number J0 of classes in the partition of X (l), we can ï¬nd at
least one j0 âˆˆ{1,...,J0} such that, for inï¬nitely many l, the most numerous class
X (l)
âˆ—
coincides with X (l)
j . Reducing our argument to those l, we may assume that
X (l)
âˆ—
= X (l)
j0 for all l. Then, for all

x(1),x(2)
âˆˆX (l)
âˆ—, with
x(i) =
â›
âœ
âœ
â
x(i)
1...
x(i)
ni
â
âŸ
âŸ
â ,
i = 1,2,
using (4.3.38a) and (4.3.45a)
OOOx(1)OOO
2
â‰¤

1âˆ’( j0 âˆ’1)Î¶
J0

p0Ï„(l),
OOOx(2)OOO
2
â‰¤j0Î¶
J0
p0Ï„(l).
That is,
(
X (l)
âˆ—,d(l))
is a coder/decoder sequence for the â€˜standardâ€™ parallel-
channel combination (cf. (4.3.34)), with
p1 =

1âˆ’( j0 âˆ’1)Î¶
J0

p0 and p2 = j0Î¶
J0
p0.
As the error-probability PX (l)
âˆ—,av
e

d(l)
â†’0, rate R is reliable for this combination
of channels. Hence, this rate does not surpass the capacity:
Râˆ—â‰¤C1

1âˆ’( j0 âˆ’1)Î¶
J0

p0

+C2
 j0Î¶
J0
p0

.

4.3 The Nyquistâ€“Shannon formula
425
Here and below we refer to the deï¬nition of Ci(u) given in (4.3.44b), i.e.
Râˆ—â‰¤C1((1âˆ’Î´)p0)+C2(Î´ p0)+ Îµ
2
(4.3.46)
where Î´ = j0Î¶/J0.
Now note that, for Î±2 â‰¥Î±1, the function
Î´ â†’C1((1âˆ’Î´)p0)+C2(Î´ p0)
increases in Î´ when Î´ < Î±2/(Î±1 + Î±2) and decreases when Î´ > Î±2/(Î±1 + Î±2).
Consequently, as Î´ = j0Î¶/J0 â‰¤Î¶, we obtain that, with Î¶ = min [Î²,Î±2/(Î±1 +Î±2)],
C1((1âˆ’Î´)p0)+C2(Î´ p0) â‰¤C1(p1)+C2(p2) = Câˆ—.
(4.3.47)
In turn, this implies, owing to (4.3.45b), (4.3.46) and (4.3.47), that
R â‰¤Câˆ—+ Îµ
2 + 1
Ï„(l) lnJ0, or R â‰¤Câˆ—+ Îµ
2 when Ï„(l) â†’âˆ.
The contradiction to R = Câˆ—+Îµ yields the inverse.
Example 4.3.6
(Prolate spheroidal wave functions (PSWFs); see [146],
[90], [91]) For any given Ï„,W > 0 there exists a sequence of real functions
Ïˆ1(t),Ïˆ2(t),..., of a variable t âˆˆR, belonging to the Hilbert space Å2(R) (i.e. with
0
Ïˆn(t)2dt < âˆ), called prolate spheroidal wave functions (PSWFs), such that
(a) The Fourier transforms Ïˆn(Ï‰) =
0
Ïˆ(t)eitÏ‰dt vanish for |Ï‰| > 2Ï€W; more-
over, the functions Ïˆn(t) form an orthonormal basis in the Hilbert subspace
formed by functions from Å2(R) with this property.
(b) The functions Ïˆâ—¦
n(t) := Ïˆn(t)1(|t| < Ï„/2) (the restrictions of Ïˆn(t) to
(âˆ’Ï„/2,Ï„/2)) are pairwise orthogonal:
0
Ïˆâ—¦
n(t)Ïˆâ—¦
nâ€²(t)dt =
0 Ï„/2
âˆ’Ï„/2 Ïˆn(t)Ïˆnâ€²(t)dt = 0 when n Ì¸= nâ€².
(4.3.48a)
Furthermore, functions Ïˆâ—¦
n form a complete system in Å2(âˆ’Ï„/2,Ï„/2): if a
function Ï• âˆˆÅ2(âˆ’Ï„/2,Ï„/2) has
0 Ï„/2
âˆ’Ï„/2Ï•(t)Ïˆn(t)dt = 0 for all n â‰¥1 then
Ï•(t) = 0 in Å2(âˆ’Ï„/2,Ï„/2).
(c) The functions Ïˆn(t) satisfy, for all n â‰¥1 and t âˆˆR, the equations
Î»nÏˆn(t) = 2W
0 Ï„/2
âˆ’Ï„/2 Ïˆn(s)sinc

2WÏ€(t âˆ’s)

ds.
(4.3.48b)

426
Further Topics from Information Theory
That is, functions Ïˆn(t) are the eigenfunctions, with the eigenvalues Î»n, of the
integral operator Ï• â†’
0
Ï•(s)K( Â· ,s) ds with the integral kernel
K(t,s) = 1(|s| < Ï„/2)(2W)sinc

2W(t âˆ’s)

= 1(|s| < Ï„/2) sin(2Ï€W(t âˆ’s))
Ï€(t âˆ’s)
, âˆ’Ï„/2 â‰¤s;t â‰¤Ï„/2.
(d) The eigenvalues Î»n satisfy the condition
Î»n =
0 Ï„/2
âˆ’Ï„/2 Ïˆn(t)2dt with 1 > Î»1 > Î»2 > Â·Â·Â· > 0.
An equivalent formulation can be given in terms involving the Fourier trans-
forms [FÏˆâ—¦
n](Ï‰) =
0
Ïˆâ—¦
n(t)eitÏ‰dt:
1
2Ï€
0 2Ï€W
âˆ’2Ï€W |[FÏˆâ—¦
n](Ï‰)|2 dÏ‰
0 Ï„/2
âˆ’Ï„/2 |Ïˆn(t)|2dt = Î»n,
which means that Î»n gives a â€˜frequency concentrationâ€™ for the truncated func-
tion Ïˆâ—¦
n.
(e) It can be checked that functions Ïˆn(t) (and hence numbers Î»n) depend on W
and Ï„ through the product WÏ„ only. Moreover, for all Î¸ âˆˆ(0,1), as WÏ„ â†’âˆ,
Î»âŒˆ2WÏ„(1âˆ’Î¸)âŒ‰â†’1, and Î»âŒˆ2WÏ„(1+Î¸)âŒ‰â†’0.
(4.3.48c)
That is, for Ï„ large, nearly 2WÏ„ of values Î»n are close to 1 and the rest are close
to 0.
An important part of the argument that is currently developing is the Karhunenâ€“
Lo`eve decomposition. Suppose Z(t) is a Gaussian random process with spectral
density Î¦(Ï‰) given by (4.3.27). The Karhunenâ€“Lo`eve decomposition states that
for all t âˆˆ(âˆ’Ï„/2,Ï„/2), the random variable Z(t) can be written as a convergent (in
the mean-square sense) series
Z(t) = âˆ‘
nâ‰¥1
AnÏˆn(t),
(4.3.49)
where Ïˆ1(t),Ïˆ2(t),... are the PSWFs discussed in Worked Example 4.3.9 below
and A1,A2,... are IID random variables with An âˆ¼N(0,Î»n) where Î»n are the cor-
responding eigenvalues. Equivalently, one writes Z(t) = âˆ‘nâ‰¥1
âˆšÎ»nÎ¾nÏˆn(t) where
Î¾n âˆ¼N(0,1) IID random variables.
The proof of this fact goes beyond the scope of this book, and the interested
reader is referred to [38] or [103], p. 144.

4.3 The Nyquistâ€“Shannon formula
427
The idea of the proof of Theorem 4.3.3 is as follows. Given W and Ï„, an input
signal sâ—¦(t) from A (Ï„,W, p0,Î·) is written as a Fourier series in the PSWFs Ïˆn.
In this series, the ï¬rst 2WÏ„ summands represent the part of the signal conï¬ned
between the frequency band-limits Â±2Ï€W and the time-limits Â±Ï„/2. Similarly, the
noise realisation Z(t) is decomposed in a series in functions Ïˆn. The action of the
continuous-time channel is then represented in terms of a parallel combination of
two jointly constrained discrete-time Gaussian channels. Channel 1 deals with the
ï¬rst 2WÏ„ PSWFs in the signal decomposition and has Î±1 = 2W. Channel 2 receives
the rest of the expansion and has Î±2 = +âˆ. The power constraint âˆ¥sâˆ¥2 â‰¤p0Ï„ leads
to a joint constraint, as in (4.3.38a). In addition, a requirement emerges that the
energy allocated outside the frequency band-limits Â±2Ï€W or time-limits Â±Ï„/2 is
small: this results in another power constraint, as in (4.3.38b). Applying Worked
Example 4.3.5 for Case I results in the assertion of Theorem 4.3.3.
To make these ideas precise, we ï¬rst derive Theorem 4.3.7 which gives an al-
ternative approach to the Nyquistâ€“Shannon formula (more complex in formulation
but somewhat simpler in the (still quite lengthy) proof).
Theorem 4.3.7
Consider the following modiï¬cation of the model from Theorem
4.3.3. The set of allowable signals A2(Ï„,W, p0,Î·) consists of functions t âˆˆR â†’
s(t) such that
(1) âˆ¥sâˆ¥2 =
0
|s(t)|2dt â‰¤p0Ï„,
(2) the Fourier transform [Fs](Ï‰) =
0
s(t)eitÏ‰dt vanishes when |Ï‰| > 2Ï€W, and
(3) the ratio
0 Ï„/2
âˆ’Ï„/2|s(t)|2dt

âˆ¥sâˆ¥2 > 1 âˆ’Î·. That is, the functions s âˆˆ
A (Ï„,W, p0,Î·) are â€˜sharply band-limitedâ€™ in frequency and â€˜nearly localisedâ€™
in time.
The noise process is Gaussian, with the spectral density vanishing when |Ï‰| > 2Ï€W
and equal to Ïƒ2
0 for |Ï‰| â‰¤2Ï€W.
Then the capacity of such a channel is given by
C = CÎ· = W ln

1+(1âˆ’Î·)
p0
2Ïƒ2
0W

+ Î· p0
2Ïƒ2
0
.
(4.3.50)
As Î· â†’0,
CÎ· â†’W ln

1+
p0
2Ïƒ2
0W

yielding the Nyquistâ€“Shannon formula (4.3.8).

428
Further Topics from Information Theory
Proofof Theorem 4.3.7 First, we establish the direct half. Take
R < W ln

1+ (1âˆ’Î·)p0
2Ïƒ2
0W

+ Î· p0
2Ïƒ2
0
(4.3.51)
and take Î´ âˆˆ(0,1) and Î¾ âˆˆ(0,min [Î·,1âˆ’Î·]) such that R is still less than
Câˆ—= W(1âˆ’Î´)ln

1+ (1âˆ’Î· +Î¾)p0
2Ïƒ2
0W(1âˆ’Î´)

+ (Î· âˆ’Î¾)p0
2Ïƒ2
0
.
(4.3.52)
According to Worked Example 4.3.5, Câˆ—is the capacity of a jointly constrained
discrete-time pair of parallel channels as in Case I, with
Î±1 = 2W(1âˆ’Î´), Î±2 = +âˆ, Î² = Î· âˆ’Î¾, p = p0, Ïƒ2 = Ïƒ2
0 ;
(4.3.53)
cf. (4.3.41a). We want to construct codes and decoding rules for the time-
continuous version of the channel, yielding asymptotically vanishing probability
of error as Ï„ â†’âˆ. Assume

x(1),x(2)
is an allowable input signal for the parallel
pair of discrete-time channels with parameters given in (4.3.53). The input for the
time-continuous channel is the following series of (W,Ï„) PSWFs:
s(t) =
âˆ‘
1â‰¤kâ‰¤âŒˆÎ±1Ï„âŒ‰
x(1)
k Ïˆk(t)+ âˆ‘
1â‰¤k<âˆ
x(2)
k Ïˆk+âŒˆÎ±1Ï„âŒ‰(t).
(4.3.54)
The ï¬rst fact to verify is that the signal in (4.3.54) belongs to A2(Ï„,W, p0,Î·), i.e.
satisï¬es conditions (1)â€“(3) of Theorem 4.3.7.
To check property (1), write
âˆ¥sâˆ¥2 =
âˆ‘
1â‰¤kâ‰¤âŒˆÎ±1Ï„âŒ‰

x(1)
k
2
+ âˆ‘
1â‰¤k<âˆ

x(2)
k
2
=
OOOx(1)OOO
2
+
OOOx(2)OOO
2
â‰¤p0Ï„.
Next, the signal s(t) is band-limited, inheriting this property from the PSWFs
Ïˆk(t). Thus, (2) holds true.
A more involved argument is needed to establish property (3). Because the
PSWFs Ïˆk(t) are orthogonal in Å2[âˆ’Ï„/2,Ï„/2] (cf. (4.3.48a)), and using the mono-
tonicity of the values Î»n (cf. (4.3.48b)), we have that
1âˆ’
0 Ï„/2
âˆ’Ï„/2|s(t)|2dt

âˆ¥sâˆ¥2 = âˆ¥(1âˆ’DÏ„)s||2
||s||2
=
âˆ‘
1â‰¤kâ‰¤âŒˆÎ±1Ï„âŒ‰
(1âˆ’Î»k)

x(1)
k
2
OOx(1)OO2 +
OOx(2)OO2 + âˆ‘
1â‰¤k<âˆ
(1âˆ’Î»k+âŒˆÎ±1Ï„âŒ‰)

x(2)
k
2
OOx(1)OO2 +
OOx(2)OO2
â‰¤

1âˆ’Î»âŒˆÎ±1Ï„âŒ‰

OOx(1)OO2
OOx(1)OO2 +
OOx(2)OO2 +
OOx(2)OO2
OOx(1)OO2 +
OOx(2)OO2 .

4.3 The Nyquistâ€“Shannon formula
429
Now, as Ï„ â†’âˆ, the value Î»âŒˆÎ±1Ï„âŒ‰â†’1 (see (4.3.48c)). With the ratio
OOx(1)OO2 OOx(1)OO2 +
OOx(2)OO2
â‰¤1, we have that for Ï„ large enough,

1âˆ’Î»âŒˆÎ±1Ï„âŒ‰

OOx(1)OO2
OOx(1)OO2 +
OOx(2)OO2 â‰¤Î¾.
Next, the ratio
OOx(2)OO2 OOx(1)OO2 +
OOx(2)OO2
â‰¤Î· âˆ’Î¾ (referring to (4.3.38b)). This
ï¬nally yields
1âˆ’
0 Ï„/2
âˆ’Ï„/2|s(t)|2dt

âˆ¥sâˆ¥2 = âˆ¥(1âˆ’DÏ„)s||2
||s||2
â‰¤Î¾ +Î· âˆ’Î¾ = Î·,
i.e. property (3).
Further, the noise can be expanded in accordance with Karhunenâ€“Lo`eve:
Z(t) =
âˆ‘
1â‰¤kâ‰¤âŒˆÎ±1Ï„âŒ‰
Z(1)
k Ïˆk(t)+ âˆ‘
1â‰¤k<âˆ
Z(2)
k Ïˆk+âŒˆÎ±1Ï„âŒ‰(t).
(4.3.55)
Here again, Ïˆk(t) are the PSWFs and IID random variables Z( j)
k
âˆ¼N(0,Î»k). Cor-
respondingly, the output signal is written as
Y(t) =
âˆ‘
1â‰¤kâ‰¤âŒˆÎ±1Ï„âŒ‰
Y (1)
k
Ïˆk(t)+ âˆ‘
1â‰¤k<âˆ
Y (2)
k
Ïˆk+âŒˆÎ±1Ï„âŒ‰(t)
(4.3.56)
where
Y ( j)
k
= x( j)
k +Z( j)
k , j = 1,2, k â‰¥1.
(4.3.57)
So, the continuous-time channel is equivalent to a jointly constrained parallel com-
bination. As we checked, the capacity equals Câˆ—speciï¬ed in (4.3.52). Thus, for
R < Câˆ—we can construct codes of rate R and decoding rules such that the error-
probability tends to 0.
For the converse, assume that there exists a sequence Ï„(l) â†’âˆ, a sequence of
transmissible domains A (l)
2 (Ï„(l),W, p0,Î·(l)) described in (1)â€“(3) and a sequence
of codes X (l) of size M = âŒˆeRÏ„(l)âŒ‰where
R > W ln

1+ (1âˆ’Î·)p0
2WÏƒ2
0

+ Î· p0
Ïƒ2
0
.
As usual, we want to show that the error-probability PX (l),av
e
(d(l)) does not tend to
0.
As before, we take Î´ > 0 and Î¾ âˆˆ(0,1âˆ’Î·) to ensure that R > Câˆ—where
Câˆ—= W(1+Î´)ln

1+ (1âˆ’Î· âˆ’Î¾)
(1âˆ’Î¾)
p0
2WÏƒ2
0 (1+Î´)

+
Î· p0
(1âˆ’Î¾)Ïƒ2
0
.

430
Further Topics from Information Theory
Then, as in the argument on the direct half, Câˆ—is the capacity of the type I jointly
constrained parallel combination of channels with
Î² =
Î·
1âˆ’Î¾ , Ïƒ2 = Ïƒ2
0 , p = p0, Î±1 = 2W(1+Î´), Î±2 = +âˆ.
(4.3.58)
Let s(t) âˆˆX (l) âˆ©A (l)
2 (Ï„(l),W, p0,Î·(l)) be a continuous-time code-function.
Since the PSWFs Ïˆk(t) form an ortho-basis in Å2(R), we can decompose
s(t) =
âˆ‘
1â‰¤kâ‰¤âŒˆÎ±1Ï„(l)âŒ‰
x(1)
k Ïˆk(t)+ âˆ‘
1â‰¤k<âˆ
x(2)
k Ïˆk+âŒˆÎ±1Ï„(l)âŒ‰(t),t âˆˆR.
(4.3.59)
We want to show that the discrete-time signal x =

x(1),x(2)
represents an al-
lowable input to the type I jointly constrained parallel combination speciï¬ed in
(4.3.38aâ€“c). By orthogonality of PSWFs Ïˆk(t) in Å2(R) we can write
âˆ¥xâˆ¥2 = ||s||2 â‰¤p0Ï„(l)
ensuring that condition (4.3.38a) is satisï¬ed. Further, using orthogonality of PSW
functions Ïˆk(t) in Å2(âˆ’Ï„/2,Ï„/2) and the fact that the eigenvalues Î»k decrease
monotonically, we obtain that
1âˆ’
0 Ï„(l)/2
âˆ’Ï„(l)/2|s(t)|2dt

âˆ¥sâˆ¥2 = âˆ¥(1âˆ’DÏ„(l))sâˆ¥2
||s||2
=
âˆ‘
1â‰¤kâ‰¤âŒˆÎ±1Ï„(l)âŒ‰
(1âˆ’Î»k)

x(1)
k
2
âˆ¥xâˆ¥2
+ âˆ‘
1â‰¤k<âˆ

1âˆ’Î»k+âŒˆÎ±1Ï„(l)âŒ‰

x(2)
k
2
âˆ¥xâˆ¥2
â‰¥

1âˆ’Î»âŒˆÎ±1Ï„(l)âŒ‰
 OOx(2)OO2
âˆ¥xâˆ¥2
.
By virtue of (4.3.48c), Î»âŒˆÎ±1Ï„(l)âŒ‰â‰¤Î¾ for l large enough. Moreover, since 1 âˆ’
0 Ï„(l)/2
âˆ’Ï„(l)/2|s(t)|2dt

âˆ¥sâˆ¥2 â‰¤Î·, we can write
OOx(2)OO2
âˆ¥xâˆ¥2
â‰¤
Î·
1âˆ’Î¾
and deduce property (4.3.38b).
Next, as in the direct half, we again use the Karhunenâ€“Lo`eve decomposition
of noise Z(t) to deduce that for each code for the continuous-time channel there
corresponds a code for the jointly constrained parallel combination of discrete-time
channels, with the same rate and error-probability. Since R is > Câˆ—, the capacity
of the discrete-time channel, the error-probability PX (l),av
e
(d(l)) remains bounded
away from 0 as l â†’âˆ. This yields the converse.

4.3 The Nyquistâ€“Shannon formula
431
Proof of Theorem 4.3.3 (Sketch) The formal argument proceeds as in Theorem
4.3.7: we have to prove the direct and converse parts of the theorem. Recall that the
direct part states that the capacity is â‰¥C, the value indicated in (4.3.31), while the
converse/inverse that it is â‰¤C. For the direct part, the channel is decomposed into
the product of two parallel channels, as in Case III, with
Î±1 = 2W(1âˆ’Î¸), Î±2 = +âˆ, p = p0, Ïƒ2 = Ïƒ2
0 , Î² = Î· âˆ’Î¾,
(4.3.60)
where Î¸ âˆˆ(0,1) (cf. property (e) of PSWFs in Example 4.3.6) and Î¾ âˆˆ(0,Î·) are
auxiliary values.
For the converse half we use the decomposition into two parallel channels, again
as in Case III, with
Î±1 = 2W(1+Î¸), Î±2 = +âˆ, p = p0, Ïƒ2 = Ïƒ2
0 , Î² =
Î·
1âˆ’Î¾ .
(4.3.61)
Here, as before, value Î¸ âˆˆ(0,1) emerges from property (e) of PSWFs, whereas
value Î¾ âˆˆ(0,1).
Summing up our previous observations we obtain the famous
Lemma 4.3.8
(The Nyquistâ€“Shannonâ€“Kotelnikovâ€“Whittaker sampling lemma)
Let f be a function t âˆˆR â†’f(t) âˆˆR with
1 | f(t)|dt < +âˆ. Suppose that the
Fourier transform
[F f](Ï‰) =
0
eitÏ‰ f(t)dt
vanishes for |Ï‰| > 2Ï€W. Then, for all x âˆˆR, function f can be uniquely recon-
structed from its values f(x + n/(2W)) calculated at points x + n/(2W), where
n = 0,Â±1,Â±2. More precisely, for all t âˆˆR,
f(t) = âˆ‘
nâˆˆZ1
f
 n
2W
 sin [2Ï€(Wt âˆ’n)]
2Ï€(Wt âˆ’n)
.
(4.3.62)
Worked Example 4.3.9
By the famous uncertainty principle of quantum
physics, a function and its Fourier transform cannot be localised simultaneously
in ï¬nite intervals [âˆ’Ï„,Ï„] and [âˆ’2Ï€W,2Ï€W]. What could be said about the case
when both function and its Fourier transform are nearly localised? How can we
quantify the uncertainty in this case?
Solution Assume the function f âˆˆÅ2(R) and let f = F f âˆˆL2(R) be the Fourier
transform of f. (Recall that space Å2(R) consists of functions f on R with || f||2 =

432
Further Topics from Information Theory
1 | f(t)|2dt < +âˆand that for all f,g âˆˆÅ2(R), the inner product
1 f(t)g(t)dt is
ï¬nite.) We shall see that if
0 t0+Ï„/2
t0âˆ’Ï„/2 | f(t)|2dt
0 âˆ
âˆ’âˆ| f(t)|2dt = Î±2
(4.3.63)
and
0 2Ï€W
âˆ’2Ï€W |F f(Ï‰)|2dÏ‰
0 âˆ
âˆ’âˆ|F f(Ï‰)|2dÏ‰ = Î² 2
(4.3.64)
then WÏ„ â‰¥Î·, where Î· = Î·(Î±,Î²) will be found explicitly. (The inequality will be
sharp, and functions yielding equality will be speciï¬ed.)
Consider the linear operators f âˆˆÅ2(R) â†’Df âˆˆÅ2(R) and f âˆˆÅ2(R) â†’B f âˆˆ
Å2(R) given by
D f(t) = f(t)1(|t| â‰¤Ï„/2)
(4.3.65)
and
B f(t) = 1
2Ï€
0 2Ï€W
âˆ’2Ï€W F f(Ï‰)eâˆ’iÏ‰tdÏ‰ = 1
Ï€
0 âˆ
âˆ’âˆf(s)sin2Ï€W(t âˆ’s)
t âˆ’s
ds.
(4.3.66)
We are interested in the product of these operators, A = BD:
A f(t) = 1
Ï€
0 Ï„/2
âˆ’Ï„/2 f(s)sin2Ï€W(t âˆ’s)
t âˆ’s
ds;
(4.3.67)
see Example 4.3.6. The eigenvalues Î»n of A obey 1 > Î»0 > Î»1 > Â·Â·Â· and tend to zero
as n â†’âˆ; see [91]. We are interested in the eigenvalue Î»0: it can be shown that Î»0
is a function of the product WÏ„. In fact, the eigenfunctions (Ïˆj) of (4.3.67) yield an
orthonormal basis in Å2(R); at the same time these functions form an orthogonal
basis in Å2[âˆ’Ï„/2,Ï„/2]:
0 Ï„/2
âˆ’Ï„/2 Ïˆ j(t)Ïˆi(t)dt = Î»iÎ´ij.
As usual, the angle between f and g in Hilbert space Å2(R) is determined by
Î¸( f,g) = cosâˆ’1

1
|| f|| ||g||Re
0
f(t)g(t)dt

.
(4.3.68)
The angle between two subspaces is the minimal angle between vectors in these
subspaces. We will show that there exists a positive angle Î¸(B,D) between the
subspaces B and D, the image spaces of operators B and D. That is, B is the
linear subspace of all band-limited functions while D is that of all time-limited
functions. Moreover,
Î¸(B,D) = cosâˆ’1 $
Î»0
(4.3.69)

4.3 The Nyquistâ€“Shannon formula
433
and inffâˆˆB,gâˆˆD Î¸( f,g) is achieved when f = Ïˆ0,g = DÏˆ0 where Ïˆ0 is the (unique)
eigenfunction with the eigenvalue Î»0.
To this end, we verify that for any f âˆˆB
min
gâˆˆD Î¸( f,g) = cosâˆ’1 ||Df||
|| f|| .
(4.3.70)
Indeed, expand f = f âˆ’D f + D f and observe that the integral
1 	
f(t) âˆ’
Df(t)

g(t)dt = 0 (since the supports of g and f âˆ’Df are disjoint). This implies
that
Re
0
f(t)g(t)dt
 â‰¤

0
f(t)g(t)dt
 =

0
Df(t)g(t)dt
.
Hence,
1
|| f||||g||Re
0
f(t)g(t)dt â‰¤||Df||
|| f||
which implies (4.3.70), by picking g = D f.
Next, we expand f =
âˆ
âˆ‘
n=0
anÏˆn, relative to the eigenfunctions of A. This yields
the formula
cosâˆ’1 ||D f||
|| f|| = cosâˆ’1

âˆ‘n |an|2Î»n
âˆ‘n |an|2
1/2
.
(4.3.71)
The supremum of the RHS in f is achieved when an = 0 for n â‰¥1, and f = Ïˆ0.
We conclude that there exists the minimal angle between subspaces B and D, and
this angle is achieved on the pair f = Ïˆ0, g = DÏˆ0, as required.
Next, we establish
Lemma 4.3.10
There exists a function f âˆˆÅ2 such that || f|| = 1, ||D f|| = Î± and
||B f|| = Î² if and only if Î± and Î² fall in one of the following cases (a)â€“(d):
(a) Î± = 0 and 0 â‰¤Î² < 1;
(b) 0 < Î± < âˆšÎ»0 < 1 and 0 â‰¤Î² â‰¤1;
(c) âˆšÎ»0 â‰¤Î± < 1 and cosâˆ’1 Î± +cosâˆ’1 Î² â‰¥cosâˆ’1 âˆšÎ»0;
(d) Î± = 1 and 0 < Î² â‰¤âˆšÎ»0.
Proof
Given Î± âˆˆ[0,1], let G (Î±) be the family of functions f âˆˆL2 with norms
|| f|| = 1 and ||D f|| = Î±. Next, determine Î² âˆ—(Î±) := supfâˆˆG (Î±) ||B f||.
(a) If Î± = 0, the family G (0) can contain no function with Î² = âˆ¥B fâˆ¥= 1. Further-
more, if âˆ¥Dfâˆ¥= 0 and âˆ¥B fâˆ¥= 1 for f âˆˆB then f is analytic and f(t) = 0 for
|t| < Ï„/2, implying f â‰¡0. To show that G (0) contains functions with all values of
Î² âˆˆ[0,1), we set  fn = Ïˆn âˆ’DÏˆn
âˆš1âˆ’Î»n
. Then the norm ||B  fn|| = âˆš1âˆ’Î»n. Since there

434
Further Topics from Information Theory
exist eigenvalues Î»n arbitrarily close to zero, ||B  fn|| becomes arbitrarily close to 1.
By considering the functions eipt  f(t) we can obtain all values of Î² between points
âˆš1âˆ’Î»n since
||Beipt  f|| =
0 âˆ’p+Ï€W
âˆ’pâˆ’Ï€W |Fn(Ï‰|2dÏ‰
1/2
.
The norm ||Beipt  f|| is continuous in p and approaches 0 as p â†’âˆ. This completes
the analysis of case (a).
(b) When 0 < Î± < âˆšÎ»0 < 1, we set
 f =
$
Î±2 âˆ’Î»nÏˆ0 âˆ’
$
Î»0 âˆ’Î±2Ïˆn
âˆšÎ»0 âˆ’Î»n
,
for n large when the eigenvalue Î»n is close to 0. We have that  f âˆˆB, ||  f|| = ||B  f|| =
1, while a simple computation shows that ||D  f|| = Î±. This includes the case Î² = 1
as, by choosing eipt  f(t) appropriately, we can obtain any 0 < Î² < 1.
(c) and (d) If âˆšÎ»0 â‰¤Î± < 1 we decompose f âˆˆG (Î±) as follows:
f = a1D f +a2B f +g
(4.3.72)
with g orthogonal to both D f and B f. Taking the inner product of the sum in the
RHS of (4.3.72), subsequently, with f, D f, B f and g we obtain four equations:
1 = a1Î±2 +a2Î² 2 +
0
g(t) f(t)dt,
Î±2 = a1Î±2 +a2
0
B f(t)Dg(t)dt,
Î² 2 = a1
0
D f(t)B f(t)dt +a2Î² 2,
0
f(t)g(t)dt = âˆ¥gâˆ¥2.
These equations imply
Î±2 +Î² 2 âˆ’1+||g||2 = a1
0
D f(t)B f(t)dt +a2
0
B f(t)Df(t)dt.

4.3 The Nyquistâ€“Shannon formula
435
By eliminating
1 g(t) f(t)dt, a1 and a2 we ï¬nd, for Î±Î² Ì¸= 0,
Î² 2 =
1âˆ’Î±2 âˆ’||g||2
(Î² 2 âˆ’
0
B f(t)D f(t)dt)
Î² 2
+
â¡
â¢â£1âˆ’
1âˆ’Î±2 âˆ’||g||2
Î±2(Î² 2 âˆ’
0
B f(t)Df(t)dt)
0
B f(t)Df(t)dt
â¤
â¥â¦
Ã—
0
D f(t)B f(t)dt
which is equivalent to
Î² 2 âˆ’2Re
0
D f(t)B f(t)dt
â‰¤âˆ’Î±2 +

(1âˆ’
1
Î±2Î² 2

0
Df(t)B f(t)dt

2
(4.3.73)
âˆ’||g||2

1âˆ’
1
Î±2Î² 2

0
Df(t)B f(t)dt

2
.
(4.3.74)
In terms of the angle Î¸, we can write
Î±Î² cosÎ¸ = Re
0
D f(t)B f(t)dt â‰¤

0
Df(t)B f(t)dt
 â‰¤Î±Î².
Substituting into (4.3.74) and completing the square we obtain
(Î² âˆ’Î± cosÎ¸)2 â‰¤(1âˆ’Î±2)sin2 Î¸
(4.3.75)
with equality if and only if g = 0 and the integral
0
Df(t)Bf(t)dt is real. Since
Î¸ â‰¥cosâˆ’1 âˆšÎ»0, (4.3.75) implies that
cosâˆ’1 Î± +cosâˆ’1 Î² â‰¥cosâˆ’1 $
Î»0.
(4.3.76)
The locus of points (Î±,Î²) satisfying (4.3.76) is up and to the right of the curve
where
cosâˆ’1 Î± +cosâˆ’1 Î² = cosâˆ’1 $
Î»0.
(4.3.77)
See Figure 4.6.
Equation (4.3.77) holds for the function  f = b1Ïˆ0 +b2DÏˆ0 with
b1 =
R
1âˆ’Î±2
1âˆ’Î»0
and b2 =
Î±
âˆšÎ»0
âˆ’
R
1âˆ’Î±2
1âˆ’Î»0
.
All intermediate values of Î² are again attained by employing eipt  f.

436
Further Topics from Information Theory
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.2
0.4
0.6
0.8
1.0
alpha^2
beta^2
W=0.5
W=1
W=2
Figure 4.6
4.4 Spatial point processes and network information theory
For a discussion of capacity of distributed systems and construction of random
codebooks based on point processes we need some background. Here we study the
spatial Poisson process in Rd, and some more advanced models of point process
are introduced with a good code distance. This section could be read independently
of PSE II, although some knowledge of its material may be very useful.
Deï¬nition 4.4.1
(cf. PSE II, p. 211) Let Î¼ be a measure on R with values Î¼(A)
for measurable subsets A âŠ†R. Assume that Î¼ is (i) non-atomic and (ii) Ïƒ-ï¬nite,
i.e. (i) Î¼(A) = 0 for all countable sets A âŠ‚R and (ii) there exists a partition R =
âˆªjJj of R into pairwise disjoint intervals J1,J2,... such that Î¼(Jj) < âˆ. We say
that a random counting measure M deï¬nes a Poisson random measure (PRM, for
short) with mean, or intensity, measure Î¼ if for all collection of pairwise disjoint
intervals I1,...,In on R, the values M(Ik), k = 1,...,n, are independent, and each
M(Ik) âˆ¼Po(Î¼(Ik)).

4.4 Spatial point processes and network information theory
437
We will state several facts, without proof, about the existence and properties of
the Poisson random measure introduced in Deï¬nition 4.4.1.
Theorem 4.4.2
For any non-atomic and Ïƒ-ï¬nite measure Î¼ on R+ there exists
a unique PRM satisfying Deï¬nition 4.4.1. If measure Î¼ has the form Î¼(dt) =
Î»dt where Î» > 0 is a constant (called the intensity of Î¼), this PRM is a Poisson
process PP(Î»). If the measure Î¼ has the form Î¼(dt) = Î»(t)dt where Î»(t) is a given
function, this PRM gives an inhomogeneous Poisson process PP(Î»(t)).
Theorem 4.4.3
(The mapping theorem) Let Î¼ be a non-atomic and Ïƒ-ï¬nite
measure on R such that for all t â‰¥0 and h > 0, the measure Î¼(t,t + h) of the
interval (t,t + h) is positive and ï¬nite (i.e. the value Î¼(t,t + h) âˆˆ(0,âˆ)), with
lim
hâ†’0Î¼(0,h) = 0 and Î¼(R+) = lim
uâ†’+âˆÎ¼(0,u) = +âˆ. Consider the function
f : u âˆˆR+ â†’Î¼(0,u),
and let f âˆ’1 be the inverse function of f. (It exists because f(u) = Î¼(0,u) is strictly
monotone in u.) Let M be the PRM(Î¼). Deï¬ne a random measure f âˆ—M by
( f âˆ—M)(I) = M(Î¼( f âˆ’1I)) = M(Î¼( f âˆ’1(a), f âˆ’1(b))),
(4.4.1)
for interval I = (a,b) âŠ‚R+, and continue it on R. Then f âˆ—M âˆ¼PP(1), i.e. f âˆ—M
yields a Poisson process of the unit rate.
We illustrate the above approach in a couple of examples.
Worked Example 4.4.4
Let the rate function of a Poisson process Î  = PP(Î»(x))
on the interval S = (âˆ’1,1) be
Î»(x) = (1+x)âˆ’2(1âˆ’x)âˆ’3 .
Show that Î  has, with probability 1, inï¬nitely many points in S, and that they
can be labelled in ascending order as
Â·Â·Â·Xâˆ’2 < Xâˆ’1 < X0 < X1 < X2 < Â·Â·Â·
with X0 < 0 < X1 .
Show that there is an increasing function f : S â†’R with f(0) = 0 such that the
points f(X)(X âˆˆÎ ) form a Poisson process of unit rate on R, and use the strong
law of large numbers to show that, with probability 1,
lim
nâ†’+âˆ(2n)1/2 (1âˆ’Xn) = 1
2 .
(4.4.2)
Find a corresponding result as n â†’âˆ’âˆ.

438
Further Topics from Information Theory
Solution Since
0 1
âˆ’1 Î»(x)dx = âˆ,
there are with probability 1 inï¬nitely many points of Î  in (âˆ’1,1). On the other
hand,
0 1âˆ’Î´
âˆ’1+Î´ Î»(x)dx < âˆ
for every Î´ > 0, so that Î (âˆ’1+Î´,1âˆ’Î´) is ï¬nite with probability 1. This is enough
to label uniquely in ascending order the points of Î . Let
f(x) =
0 x
0 Î»(y)dy.
As f : S â†’R is increasing, f maps Î  into a Poisson process whose mean measure
Î¼ is given by
Î¼(a,b) =
0 f âˆ’1(b)
f âˆ’1(a) Î»(x)dx = bâˆ’a.
With this choice of f, the points ( f(Xn)) form a Poisson process of unit rate on R.
The strong law of large numbers shows that, with probability 1, as n â†’âˆ,
nâˆ’1 f(Xn) â†’1, and nâˆ’1 f(Xâˆ’n) â†’âˆ’1.
Now, observe that
Î»(x) âˆ¼1
4(1âˆ’x)âˆ’3 and f(x) âˆ¼1
8(1âˆ’x)âˆ’2, as x â†’1.
Thus, as n â†’âˆ, with probability 1,
nâˆ’1 1
8(1âˆ’Xn)âˆ’2 â†’1,
which is equivalent to (4.4.2). Similarly,
Î»(x) âˆ¼1
8(1+x)âˆ’2 and f(x) âˆ¼1
8(1+x)âˆ’1, as x â†’âˆ’1,
implying that with probability 1, as n â†’âˆ,
nâˆ’1 1
8(1+Xâˆ’n)âˆ’1 â†’1.
Hence, with probability 1
lim
nâ†’âˆn(1+Xâˆ’n) = 1
8.

4.4 Spatial point processes and network information theory
439
Worked Example 4.4.5
Show that, if Y1 < Y2 < Y3 < Â·Â·Â· are points of a Poisson
process on (0,âˆ) with constant rate function Î», then
lim
nâ†’âˆYn/n = Î»
with probability 1. Let the rate function of a Poisson process Î  = PP(Î»(x)) on
(0,1) be
Î»(x) = xâˆ’2(1âˆ’x)âˆ’1.
Show that the points of Î  can be labelled as
Â·Â·Â· < Xâˆ’2 < Xâˆ’1 < 1
2 < X0 < X1 < Â·Â·Â·
and that
lim
nâ†’âˆ’âˆXn = 0, lim
nâ†’âˆXn = 1.
Prove that
lim
nâ†’âˆnXâˆ’n = 1
with probability 1. What is the limiting behaviour of Xn as n â†’+âˆ?
Solution The ï¬rst part again follows from the strong law of large numbers. For the
second part we set
f(x) =
0 x
1/2 Î»(Î¾)dÎ¾,
and use the fact that f maps Î  into a PP of constant rate on ( f(0), f(1)): f(Î ) =
PP(1). In our case, f(0) = âˆ’âˆand f(1) = âˆ, and so f(Î ) is a PP on R. Its points
may be labelled
Â·Â·Â· < Yâˆ’2 < Yâˆ’1 < 0 < Y0 < Y1 < Â·Â·Â·
with
lim
nâ†’âˆ’âˆYn = âˆ’âˆ,
lim
nâ†’+âˆYn = +âˆ.
Then Xn = f âˆ’1(Yn) has the required properties.
The strong law of large numbers applied to Yâˆ’n gives
lim
nâ†’âˆ’âˆ
f(Xn)
n
= lim
nâ†’âˆ’âˆ
Yn
n = 1, a.s.
Now, as x â†’0,
f(x) = âˆ’
0 1/2
x
Î¾ âˆ’2(1âˆ’Î¾)âˆ’1dÎ¾ âˆ¼âˆ’
0 1/2
x
Î¾ âˆ’2dÎ¾ âˆ¼âˆ’xâˆ’1,

440
Further Topics from Information Theory
implying that
lim
nâ†’âˆ
Xâˆ’1
âˆ’n
n
= 1, i.e. lim
nâ†’âˆnXâˆ’n = 1, a.s.
Similarly,
lim
nâ†’+âˆ
f(Xn)
n
= 1, a.s.,
and as x â†’1,
f(x) âˆ¼
0 x
1/2(1âˆ’Î¾)âˆ’1dÎ¾ âˆ¼âˆ’ln(1âˆ’x).
This implies that
lim
nâ†’âˆâˆ’ln(1âˆ’Xn)
n
= 1, a.s.
Next, we discuss the concept of a Poisson random measure (PRM) on a general
set E. Formally, we assume that E had been endowed with a Ïƒ-algebra E of subsets,
and a measure Î¼ assigning to every A âˆˆE a value Î¼(A), so that if A1,A2,... are
pairwise disjoint sets from E then
Î¼ (âˆªnAn) = âˆ‘
n
Î¼(An).
The value Î¼(E) can be ï¬nite or inï¬nite. Our aim is to deï¬ne a random counting
measure M = (M(A),A âˆˆE ), with the following properties:
(a) The random variable M(A) takes non-negative integer values (including, possi-
bly, +âˆ). Furthermore,
M(A)
'
âˆ¼Po(Î»Î¼(A)), if Î¼(A) < âˆ,
= +âˆwith probability 1, if Î¼(A) = âˆ.
(4.4.3)
(b) If A1,A2,... âˆˆE are disjoint sets then
M (âˆªiAi) = âˆ‘
i
M(Ai).
(4.4.4)
(c) The random variables M(A1),M(A2),... are independent if sets A1,A2,... âˆˆE
are disjoint. That is, for all ï¬nite collections of disjoint sets A1,...,An âˆˆE and
non-negative integers k1,...,kn
P(M(Ai) = ki, 1 â‰¤i â‰¤n) = âˆ
1â‰¤iâ‰¤n
P(M(Ai) = ki).
(4.4.5)

4.4 Spatial point processes and network information theory
441
First assume that Î¼(E) < âˆ(if not, split E into subsets of ï¬nite measure). Fix a
random variable M(E) âˆ¼Po(Î»Î¼(E)). Consider a sequence X1,X2,... of IID ran-
dom points in E, with Xi âˆ¼Î¼

Î¼(E), independently of M(E). It means that for all
n â‰¥1 and sets A1,...,An âˆˆE (not necessarily disjoint)
P

M(E) = n, X1 âˆˆA1,...,Xn âˆˆAn

= eâˆ’Î»Î¼(E)

Î»Î¼(E)
n
n!
n
âˆ
i=1
Î¼(Ai)
Î¼(E) ,
(4.4.6)
and conditionally,
P

X1 âˆˆA1,...,Xn âˆˆAn|M(E) = n

=
n
âˆ
i=1
Î¼(Ai)
Î¼(E) .
(4.4.7)
Then set
M(A) =
M(E)
âˆ‘
i=1
1(Xi âˆˆA), A âˆˆE .
(4.4.8)
Theorem 4.4.6
If Î¼(E) < âˆ, equation (4.4.8) deï¬nes a random measure M on E
satisfying properties (a)â€“(c) above.
Worked Example 4.4.7
Let M be a Poisson random measure of intensity Î» on
the plane R2. Denote by C(r) the circle {x âˆˆR2 : |x| < r} of radius r in R2 centred
at the origin and let Rk be the largest radius such that C(Rk) contains precisely k
points of M. [Thus C(R0) is the largest circle about the origin containing no points
of M, C(R1) is the largest circle about the origin containing a single point of M,
and so on.] Calculate ER0, ER1 and ER2.
Solution Clearly,
P(R0 > r) = P(C(r) contains no point of M) = eâˆ’Î»Ï€r2, r > 0,
and
P(R1 > r) = P(C(r) contains at most one point of M)
= (1+Î»Ï€r2)eâˆ’Î»Ï€r2, r > 0.
Similarly,
P(R2 > r) =

1+Î»Ï€r2 + 1
2(Î»Ï€r2)2

eâˆ’Î»Ï€r2, r > 0.

442
Further Topics from Information Theory
Then
ER0 =
0 âˆ
0 P(R0 > r)dr =
1
âˆš
2Ï€Î»
0 âˆ
0 eâˆ’Ï€Î»r2d
âˆš
2Ï€Î»r

=
1
2
âˆš
Î»
,
ER1 =
0 âˆ
0 P(R1 > r)dr
=
1
2
âˆš
Î»
+
0 âˆ
0 eâˆ’Ï€Î»r2
Î»Ï€r2
dr
=
1
2
âˆš
Î»
+
1
2
âˆš
2Ï€Î»
0 âˆ
0

2Ï€Î»r2
eâˆ’Ï€Î»r2d
âˆš
2Ï€Î»r

=
3
4
âˆš
Î»
,
ER2 =
3
4
âˆš
Î»
+
0 âˆ
0

Î»Ï€r22
2
eâˆ’Ï€Î»r2dr
=
3
4
âˆš
Î»
+
1
8
âˆš
2Ï€Î»
0 âˆ
0

2Î»Ï€r22eâˆ’Ï€Î»r2d
âˆš
2Î»Ï€r

=
3
4
âˆš
Î»
+
3
16
âˆš
Î»
=
15
16
âˆš
Î»
.
We shall use for the PRM M on the phase space E with intensity measure Î¼ con-
structed in Theorem 4.4.6 the notation PRM(E,Î¼). Next, we extend the deï¬nition
of the PRM to integral sums: for all functions g : E â†’R+ deï¬ne
M(g) =
M(E)
âˆ‘
i=1
g(Xi) :=
0
g(y)dM(y);
(4.4.9)
summation is taken over all points Xi âˆˆE, and M(E) is the total number of such
points. Next, for a general g : E â†’R we set
M(g) = M(g+)âˆ’M(âˆ’gâˆ’),
with the standard agreement that +âˆâˆ’a = +âˆand aâˆ’âˆ= âˆ’âˆfor all a âˆˆ(0,âˆ).
[When both M(g+) and M(âˆ’gâˆ’) equal âˆ, the value M(g) is declared not deï¬ned.]
Then
Theorem 4.4.8
(Campbell theorem) For all Î¸ âˆˆR and for all functions g : E â†’R
such that eÎ¸g(y) âˆ’1 is Î¼-integrable
EeÎ¸M(g) = exp
â¡
â£Î»
0
E

eÎ¸g(y) âˆ’1

dÎ¼(y)
â¤
â¦.
(4.4.10)

4.4 Spatial point processes and network information theory
443
Proof
Write
EeÎ¸M(g) = E
	
E

eÎ¸M(g)|M(E)

= âˆ‘
k
P(M(E) = k)E

exp

Î¸
k
âˆ‘
i=1
g(Xi)

|M(E) = k

.
Owing to conditional independence (4.4.7),
E

exp

Î¸
k
âˆ‘
i=1
g(Xi)

|M(E) = k

=
k
âˆ
i=1
EeÎ¸g(Xi) =

EeÎ¸g(X1)k
=

1
Î¼(E)
1
E
eÎ¸g(x)dÎ¼(x)
k
,
and
EeÎ¸M(g) = âˆ‘
k
eâˆ’Î»Î¼(E) (Î»Î¼(E))k
k!
1

Î¼(E)
k
â›
â
0
E
eÎ¸g(x)dÎ¼(x)
â
â 
k
= eâˆ’Î»Î¼(E) exp
â¡
â£Î»
0
E
eÎ¸g(x)dÎ¼(x)
â¤
â¦
= exp
â¡
â£Î»
0
E

eÎ¸g(x) âˆ’1

dÎ¼(x)
â¤
â¦.
Corollary 4.4.9
The expected value of M(g) is given by
EM(g) = Î»
0
E
g(y)dÎ¼(y);
it exists if and only if the integral on the RHS is well deï¬ned.
Proof
The proof follows by differentiation of the MGF at Î¸ = 0.
Example 4.4.10
Suppose that the wireless transmitters are located at the points
of Poisson process Î  on R2 of rate Î». Let ri be the distance from transmitter i to
the central receiver at 0, and the minimal distance to a transmitter is r0. Suppose
that the power of the received signal is Y = âˆ‘XiâˆˆÎ 
P
rÎ±
i for some Î± > 2. Then
EeÎ¸Y = exp
â¡
â£2Î»Ï€
âˆ
0
r0

eÎ¸g(r) âˆ’1

rdr
â¤
â¦,
(4.4.11)
where g(r) = P
rÎ± where P is the transmitter power.

444
Further Topics from Information Theory
A popular model in application is the so-called marked point process with the
space of marks D. This is simply a random measure on Rd Ã—D or on its subset. We
will need the following product property proved below in the simplest set-up.
Theorem 4.4.11
(The product theorem) Suppose that a Poisson process with the
constant rate Î» is given on R, and marks Yi are IID with distribution Î½. Deï¬ne a
random measure M on R+ Ã—D by
M(A) =
âˆ
âˆ‘
n=1
I

(Tn,Yn) âˆˆA

, A âŠ†R+ Ã—D.
(4.4.12)
This measure is a PRM on R+ Ã—D, with the intensity measure Î»mÃ—Î½ where m is
a Lebesgue measure.
Proof
First, consider a set A âŠ†[0,t)Ã—D where t > 0. Then
M(A) =
Nt
âˆ‘
n=1
1((Tn,Yn) âˆˆA).
Consider the MGF EeÎ¸M(A) and use standard conditioning
EeÎ¸M(A) = E

E

eÎ¸M(A)|Nt

=
âˆ
âˆ‘
k=0
P(Nt = k)E

eÎ¸M(A)|Nt = k

.
We know that Nt âˆ¼Po(Î»t). Further, given that Nt = k, the jump points T1,...,Tk
have the conditional joint PDF fT1,...,Tk( Â· |Nt = k) given by (4.4.7). Then, by using
further conditioning, by T1,...,Tk, in view of the independence of the Yn, we have
E

eÎ¸M(A)|Nt = k

= E

E

eÎ¸M(A)|Nt = k;T1,...,Tk

=
t0
0
...
t0
0
dxk ...dx1 fT1,...,Tk(x1,...,xk|N = k)
Ã—E

exp Î¸

k
âˆ‘
i=1
I

(xi,Yi) âˆˆA


|Nt = k;T1 = x1,...,Tk = xk

= 1
tk
â›
â
t0
0
0
D
eÎ¸IA(x,y)dÎ½(y)dx
â
â 
k
.

4.4 Spatial point processes and network information theory
445
Then
EeÎ¸M(A) = eâˆ’Î»t
âˆ
âˆ‘
k=0
(Î»t)k
k!
1
tk
â›
â
t0
0
0
D
eÎ¸IA(x,y)dÎ½(y)dx
â
â 
k
= exp
â¡
â£Î»
t0
0
0
D

eÎ¸IA(x,y) âˆ’1

dÎ½(y)dx
â¤
â¦.
The expression eÎ¸IA(x,y) âˆ’1 takes value eÎ¸ âˆ’1 for (x,y) âˆˆA and 0 for (x,y) Ì¸âˆˆA.
Hence,
EeÎ¸M(A) = exp
â¡
â£
eÎ¸ âˆ’1

Î»
0
A
dÎ½(y)dx
â¤
â¦, Î¸ âˆˆR.
(4.4.13)
Therefore, M(A) âˆ¼Po(Î»mÃ—Î½(A)).
Moreover, if A1,...,An are disjoint subsets of [0,t)Ã—D then the random variables
M(A1),...,M(An) are independent. To see this, note ï¬rst that, by deï¬nition, M is
additive: M(A) = M(A1)+Â·Â·Â·+M(An) where A = A1 âˆªÂ·Â·Â·âˆªAn. From (4.4.13)
EeÎ¸M(A) = exp
â¡
â£
eÎ¸ âˆ’1

Î»
n
âˆ‘
i=1
0
Ai
dÎ½(y)dx
â¤
â¦=
n
âˆ
i=1
EeÎ¸M(Ai), Î¸ âˆˆR,
which implies independence.
So, the restriction of M to En = [0,n) Ã— D is an (En,Î»dmn Ã— Î½) PRM, where
mn = m|[0,n). Then, by the extension property, M is an (R+ Ã—D,Î»mÃ—Î½) PRM.
Worked Example 4.4.12
Use the product and Campbellâ€™s theorems to solve
the following problem. Stars are scattered over three-dimensional space R3 in a
Poisson process Î  with density Î½(X) (X âˆˆR3). Masses of the stars are IID random
variables; the mass mX of a star at X has PDF Ï(X,dm). The gravitational potential
at the origin is given by
F = âˆ‘
XâˆˆÎ 
GmX
|X| ,
where G is a constant. Find the MGF EeÎ¸F.
A galaxy occupies a sphere of radius R centred at the origin. The density of
stars is Î½(x) = 1/|x| for points x inside the sphere; the mass of each star has
the exponential distribution with mean M. Calculate the expected potential due to
the galaxy at the origin. Let C be a positive constant. Find the distribution of the
distance from the origin to the nearest star whose contribution to the potential F is
at least C.

446
Further Topics from Information Theory
Solution Campbellâ€™s theorem says that if M is a Poisson random measure on the
space E with intensity measure Î½ and a : E â†’R is a bounded measurable function
then
EeÎ¸Î£ = exp
â›
â
0
E

eÎ¸a(y) âˆ’1

Î½(dy)
â
â ,
where
Î£ =
0
E
a(y)M(dy) = âˆ‘
XâˆˆÎ 
a(X).
By the product theorem, pairs (X,mX) (position, mass) form a PRM on R3 Ã—
R+, with intensity measure Î¼(dx Ã— dm) = Î½(x)dxÏ(x,dm). Then by Campbellâ€™s
theorem:
EeÎ¸F = exp
â›
â
0
R3
âˆ
0
0
Î¼(dxÃ—dm)

eÎ¸Gm/|x| âˆ’1

â
â .
The expected potential at the origin is EF = dEeÎ¸F
dÎ¸
|Î¸=0 and equals
0
R3
Î½(x)dx
âˆ
0
0
Ï(x,dm)Gm
|x| = GM
0
R3
dx 1
|x|2 1(|x| â‰¤R).
In the spherical coordinates,
0
R3
dx 1
|x|2 1(|x| â‰¤R) =
R
0
0
dr 1
r2 r2
0
dÏ‘ cos Ï‘
0
dÏ† = 4Ï€R
which yields
EF = 4Ï€GMR.
Finally, let D be the distance to the nearest star contributing to F at least C. Then,
by the product theorem,
P(D â‰¥d) = P(no points in A) = exp

âˆ’Î¼(A)

.
Here
A =
%
(x,m) âˆˆR3 Ã—R+ : |x| â‰¤d, Gm
|x| â‰¥C
K
,

4.4 Spatial point processes and network information theory
447
and Î¼(A) =
0
A
Î¼(dxÃ—dm) is represented as
d
0
0
dr1
r r2
0
dÏ‘ cosÏ‘
0
dÏ†Mâˆ’1
âˆ
0
Cr/G
dmeâˆ’m/M
= 4Ï€
d
0
0
drreâˆ’Cr/(GM)
= 4Ï€
GM
C
2 
1âˆ’eâˆ’Cd/(GM) âˆ’Cd
GM eâˆ’Cd/(GM)

.
This determines the distribution of D on [0,R].
In distributed systems of transmitters and receivers like wireless networks of
mobile phones the admissible communication rate between pairs of nodes in the
wireless network depends on their random positions and their transmission strate-
gies. Usually, the transmission is performed along the chain of transmitters from
the source to destination. So, the new interesting direction in information theory
has emerged; some experts even coined the term â€˜network information theoryâ€™.
This ï¬eld of research has many connections with probability theory, in particu-
lar, percolation and spatial point processes. We do not attempt here to give even
a glimpse of this rapidly developing ï¬eld, but no presentation of information the-
ory nowadays can completely avoid network aspects. Here we touch slightly a few
topics and refer the interested reader to [48] and the literature cited therein.
Example 4.4.13
Suppose that the receiver is located at point y and the transmit-
ters are scattered on the plane R2 at the points of xi âˆˆÎ  of Poisson process of rate
Î». Then the simplest model for the power of the received signal is
Y = âˆ‘
xiâˆˆÎ 
Pâ„“(|xi âˆ’y|)
(4.4.14)
where P is the emitted signal power and the function â„“describes the fading of the
signal. In the case of so-called Rayleigh fading â„“(|x|) = eâˆ’Î²|x|, and in the case of
the power fading â„“(|x|) = |x|âˆ’Î±,Î± > 2. By the Campbell theorem
Ï†(Î¸) = E
	
eÎ¸Y
= exp

2Î»Ï€
0 âˆ
0 r

eÎ¸Pâ„“(r) âˆ’1

dr

.
(4.4.15)
A more realistic model of the wireless network may be described as follows.
Suppose that receivers are located at points yj, j = 1,...,J, and transmitters are
scattered on the plane R2 at the points of xi âˆˆÎ  of Poisson process of rate Î».

448
Further Topics from Information Theory
Assuming that the signal Sk from the point xk is ampliï¬ed by the coefï¬cient
âˆš
P
we write the signal
Yj = âˆ‘
xkâˆˆÎ 
h jkSk +Z j, j = 1,...,J.
(4.4.16)
Here the simplest model of the transmission function is
h jk =
âˆš
Pe2Ï€irjk/Î½
rÎ±/2
jk
,
(4.4.17)
where Î½ is the transmission wavelength and r jk = |yj âˆ’xk|. The noise random vari-
ables Z j are assumed to be IID N(0,Ïƒ2
0 ). A similar formula could be written for
Rayleigh fading. We know that in the case of J = 1 and a single transmitter K = 1,
by the Nyquistâ€“Shannon theorem of Section 4.3, the capacity of the continuous
time, additive white Gaussian noise channel Y(t) = X(t)â„“(x,y)+Z(t) with attenu-
ation factor â„“(x,y), subject to the power constraint
1 Ï„/2
âˆ’Ï„/2 X2(t)dt < PÏ„, bandwidth
W, and noise power spectral density Ïƒ2
0 , is
C = W log

1+ Pâ„“2(x,y)
2WÏƒ2
0

.
(4.4.18)
Next, consider the case of ï¬nite numbers K of transmitters and J of receivers
y j(t) =
K
âˆ‘
i=1
â„“(xi,yj)xi(t)+z j(t), j = 1,...,J,
(4.4.19)
with a power constraint Pk for transmitter k = 1,...,K. Using Worked Example
4.3.5 for the capacity of parallel channels it can be proved (cf. [48]) that the capac-
ity of the channel is
C =
K
âˆ‘
k=1
W log

1+ Pks2
k
2WÏƒ2
0

(4.4.20)
where sk is the kth largest singular value of the matrix L =

â„“(|yj âˆ’xk|)

. Next,
we assume that the bandwidth W = 1. It is also interesting to describe the capacity
region of a distributed system with K transmitters and J receivers under constant,
on average, power of transmission Kâˆ’1 âˆ‘k Pk â‰¤P. Again, the interested reader is
referred to [48] where the following capacity domain for allowable rates Rk j is
established:
K
âˆ‘
k=1
J
âˆ‘
j=1
Rk j â‰¤
max
Pkâ‰¥0,âˆ‘k Pkâ‰¤KP
K
âˆ‘
k=1
log

1+ Pks2
k
2Ïƒ2
0

.
(4.4.21)
Theorem 4.4.14
Consider an arbitrary conï¬guration S of 2n nodes placed inside
the box Bn of area n (i.e. size âˆšn); partition them into two sets S1 and S2, so

4.4 Spatial point processes and network information theory
449
that S1 âˆ©S2 = /0, S1 âˆªS2 = S,â™¯S1 = â™¯S2 = n. The sum Cn =
n
âˆ‘
k=1
n
âˆ‘
j=1
Rk j of reliable
transmission rates in model (4.4.19) from the transmitters xk âˆˆS1 to the receivers
yi âˆˆS2 is bounded from above:
Cn =
n
âˆ‘
k=1
n
âˆ‘
j=1
Rk j â‰¤
max
Pkâ‰¥0,âˆ‘Pkâ‰¤nP
n
âˆ‘
k=1
log

1+ Pks2
k
2Ïƒ2
0

,
where sk is the kth largest singular value of the matrix L = (â„“(xk,yj)), Ïƒ2
0 is the
noise power spectral density, and the bandwidth W = 1.
This result allows us to ï¬nd the asymptotic of capacity as n â†’âˆ. In the most
interesting case of Rayleigh fading R(n) = Cn/n âˆ¼O( (logn)2
âˆšn ); in the case of power
Î± > 2 fading, R(n) âˆ¼O( n1/Î±(logn)2
âˆšn
): see again [48].
Next we discuss the interference limited networks. Let Î  be a Poisson process of
rate Î» on the plane R2. Let the function â„“: R2Ã—R2 â†’R+ describing an attenuation
factor of signal emitted from x at point y be symmetric: â„“(x,y) = â„“(y,x),x,y âˆˆR2.
The most popular examples are â„“(x,y) = Peâˆ’Î²|xâˆ’y| and â„“(x,y) =
P
|xâˆ’y|Î± ,Î± > 2. A
general theory is developed under the following assumptions:
(i) â„“(x,y) = â„“(|xâˆ’y|),
1 âˆ
r0 râ„“(r)dr < âˆfor some r0 > 0.
(ii) l(0) > kÏƒ2
0 /P, â„“(x) â‰¤1 for all x > 0 where k > 0 is an admissible level of
interference.
(iii) â„“is continuous and strictly decreasing where it is non-zero.
For each pair of points xi,xj âˆˆÎ  deï¬ne the signal/noise ratio
SNR(xi â†’xj) =
Pâ„“2(xi,xj)
Ïƒ2
0 +Î³ âˆ‘kÌ¸=i, j Pâ„“2(xk,xj)
(4.4.22)
where P,Ïƒ2
0 ,k > 0 and 0 â‰¤Î³ < 1
k. We say that a transmitter located at xi can send a
message to receiver located at xj if SNR(xi â†’xj) â‰¥k. For any k > 0 and 0 < Îº < 1,
let An(k,Îº) be an event that there exists a set Sn of at least Îºn points of Î  such that
for any two points s,d âˆˆSn, SNR(s,d) > k. It can be proved (see [48]) that for all
Îº âˆˆ(0,1) there exists k = k(Îº) such that
lim
nâ†’âˆP

An(k(Îº),Îº)

= 1.
(4.4.23)
Then we say that the network is supercritical at interference level k(Îº); it means
that the number of other points the given transmitter (say, located at the origin 0)
could communicate to, by using re-transmission at intermediate points, is inï¬nite
with a positive probability.

450
Further Topics from Information Theory
First, we note that any given transmitter may be directly connected to at most
1 + (Î³k)âˆ’1 receivers. Indeed, suppose that nx nodes are connected to the node x.
Denote by x1 the node connected to x and such that
â„“(|x1 âˆ’x|) â‰¤â„“(|xi âˆ’x|),i = 2,...,nx.
(4.4.24)
Since x1 is connected to x we have
Pâ„“(|x1 âˆ’x|)
Ïƒ2
0 +Î³
âˆ
âˆ‘
i=2
Pâ„“(|xi âˆ’x|)
â‰¥k
which implies
Pâ„“(|x1 âˆ’x|) â‰¥kÏƒ2
0 +kÎ³ âˆ‘
iâ‰¥2
Pâ„“(|xi âˆ’x|)
â‰¥kÏƒ2
0 +kÎ³(nx âˆ’1)Pâ„“(|x1 âˆ’x|)+kÎ³ âˆ‘
iâ‰¥nx+1
Pâ„“(|xi âˆ’x|)
â‰¥kÎ³(nx âˆ’1)Pâ„“(|x1 âˆ’x|).
(4.4.25)
We conclude from (4.4.25) that nx â‰¤1+(kÎ³)âˆ’1. However, the network percolates
for some values of parameters in view of (4.4.23). This means that with positive
probability a given transmitter may be connected to an inï¬nite number of oth-
ers with re-transmissions. In particular, the model percolates for Î³ = 0, above the
critical rate for percolation of Poisson ï¬‚ow Î»cr. It may be demonstrated that for
Î» > Î»cr the critical value of Î³âˆ—(Î») ï¬rst increases with Î» but then starts to decay
because the interference becomes too strong. The proof of the following result may
be found in [48].
Theorem 4.4.15
Let Î»cr be the critical node density for Î³ = 0. For any node
density Î» > Î»cr, there exists Î³âˆ—(Î») > 0 such that for Î³ â‰¤Î³âˆ—(Î»), the interference
model percolates. For Î» â†’âˆwe have that
Î³âˆ—(Î») = O(Î» âˆ’1).
(4.4.26)
Another interesting connection with the theory of spatial point processes in RN
is in using realisations of point processes for producing random codebooks. An al-
ternative (and rather efï¬cient) way to generate a random coding attaining the value
C(Î±) in (4.1.17) is as follows. Take a Poisson process Î (N) in RN, of rate Î»N = eNRN
where RN â†’R as N â†’âˆ. Here R < 1
2 log
1
2Ï€eÏƒ2
0 where Ïƒ2
0 be the variance of additive
Gaussian noise in a channel. Enlist in the codebook XM,N the random points X(i)
from process Î¾ (N) lying inside the Euclidean ball B(N)(
âˆš
NÎ±) and surviving the
following â€˜purgeâ€™. Fix r > 0 (the minimal distance of the random code) and for any
point Xj of a Poisson process Î (N) generate an IID random variable Tj âˆ¼U([0,1])
(a random mark). Next, for every point Xj of the original Poisson process examine

4.4 Spatial point processes and network information theory
451
the ball B(N)(Xj,r) of radius r centred at Xj. The point Xj will survive only if its
mark Tj is strictly smaller than the marks of all other points from Î (N) lying in
B(N)(Xj,r). The resulting point process Î¾ (N) is known as the MatÂ´ern process; it is
an example of a more general construction discussed in the recent paper [1].
The main parameter of a random codebook with codewords x(N) of length N is
the induced distribution of the distance between codewords. In the case of code-
books generated by stationary point processes it is convenient to introduce a func-
tion K(t) such that Î» 2K(t) gives the expected number of ordered pairs of distinct
points in a unit volume less than distance t apart. In other words, Î»K(t) is the ex-
pected number of further points within t of an arbitrary point of a process. Say,
for Poisson process on R2 of rate Î», K(t) = Ï€t2. In random codebooks we are in-
terested in models where K(t) is much smaller for small and moderate t. Hence,
random codewords appear on a small distance from one another much more rarely
than in a Poisson process. It is convenient to introduce the so-called product density
Ï(t) = Î» 2
c(t)
dK(t)
dt
,
(4.4.27)
where c(t) depends on the state space of the point process. Say, c(t) = 2Ï€t on R1,
c(t) = 2Ï€t2 on R2, c(t) = 2Ï€ sint on the unit sphere, etc.
Some convenient models of this type have been introduced by B. MatÂ´ern. Here
we discuss two rather intuitive models of point processes on RN. The ï¬rst is ob-
tained by sampling a Poisson process of rate Î» and deleting any point which is
within 2R of any other whether or not this point has already been deleted. The rate
of this process for N = 2 is
Î»M,1 = Î»eâˆ’4Ï€Î»R2.
(4.4.28)
The product density k(t) = 0 for t < 2R, and
Ï(t) = Î» 2eâˆ’2U(t),t > 2R,
where
U(t) = meas[B((0,0),2R)âˆªB((t,0),2R)].
(4.4.29)
Here B((0,0),2R) is the ball with centre (0,0) of radius 2R, and B((t,0),2R) is
the ball with centre (t,0) of radius 2R. For varying Î» this model has the maximum
rate of (4Ï€eR2)âˆ’1 and so cannot model densely packed codes. This is 10% of the
theoretical bound (
âˆš
12R2)âˆ’1 which is attained by the triangular lattice packing,
cf. [1].
The second MatÂ´ern model is an example of the so-called marked point process.
The points of a Poisson process of rate Î» are independently marked by IID random
variables with distribution U([0,1]). A point is deleted if there is another point of

452
Further Topics from Information Theory
the process within distance 2R which has a bigger mark whether or not this point
has already been deleted. The rate of this process for N = 2 is
Î»M,2 = (1âˆ’eâˆ’Î»c)/c,c = U(0) = 4Ï€R2.
(4.4.30)
The product density Ï(t) = 0 for t < 2R, and
Ï(t) = 2U(t)

1âˆ’eâˆ’4Ï€R2Î»
âˆ’2c

1âˆ’eâˆ’Î»U(t)
cU(t)(U(t)âˆ’c)
,t > 2R.
(4.4.31)
An equivalent deï¬nition is as follows. Given two points X and Y of the primary
Poisson process on the distance t = |X âˆ’Y| deï¬ne the probability k(t) = Ï(t)/Î» 2
that both of them are retained in the secondary process. Then k(t) = 0 for t < 2R,
and
k(t) = 2U(t)(1âˆ’eâˆ’4Ï€R2Î»)âˆ’8Ï€R2(1âˆ’eâˆ’Î»U(t))
4Î» 2Ï€R2U(t)(U(t)âˆ’4Ï€R2)
, t > 2R.
Example 4.4.16
(Outage probability in a wireless network) Suppose a receiver is
located at the origin and transmitters are distributed according to the MatÂ´ern hard-
core process with the inner radius r0. We suppose that no transmitters are closer to
each other than r0 and the coverage distance is a. The sum of received powers at
the central receiver from the signals from all wireless network is written as
Xr0 = âˆ‘
Jr0,a
P
rÎ±
i
(4.4.32)
where Jr0,a denotes the set of interfering transmitters such that r0 â‰¤ri < a. Let Î»P
be the rate of Poisson process producing a MatÂ´ern process after thinning. The rate
of thinned process is
Î» = 1âˆ’exp

âˆ’Î»PÏ€r2
0

Ï€r2
0
.
Using the Campbell theorem we compute the MGF of Xr0:
Ï†(Î¸) = E

eÎ¸Xr0

= exp

Î»PÏ€(a2 âˆ’r2
0)
0 1
0 q(t)dt
	0 a
r0
2r
(a2 âˆ’r2
0)eÎ¸g(r)dr âˆ’1


.
(4.4.33)

4.5 Selected examples and problems from cryptography
453
Here g(r) = P
rÎ± and q(t) = exp

âˆ’Î»PÏ€r2
0t

is the retaining probability of a point
of mark t. Since
0 1
0q(t)dt = Î»
Î»P
, we obtain
Ï†(Î¸) = exp

Î»Ï€(a2 âˆ’r2
0)
0 a
r0
2r
(a2 âˆ’r2
0)eÎ¸g(r)dr âˆ’1

.
(4.4.34)
Now we can compute all absolute moments of the interfering signal:
Î¼k = Î»Ï€
0 a
r0
2r(g(r))kdr = 2Î»Ï€
kÎ± âˆ’2
 Pk
rkÎ±âˆ’2
0
âˆ’
Pk
akÎ±âˆ’2

.
(4.4.35)
Engineers say that outage happens at the central receiver, i.e. the interference pre-
vents one from reading a signal obtained from a sender at distance rs, if
P/rÎ±
s
Ïƒ2
0 +âˆ‘Jr0,a P/rÎ±
i
â‰¤k.
Here, Ïƒ2
0 is the noise power, rs is the distance to sender and k is the minimal SIR
(signal/noise ratio) required for successful reception. Different approximations of
outage probability based on the moments computed in (4.4.35) are developed. Typ-
ically, the distribution of Xr0 is close to log-normal; see, e.g., [113].
4.5 Selected examples and problems from cryptography
Cryptography, commonly deï¬ned as â€˜the practice and study of hiding informationâ€™,
became a part of many courses and classes on coding; in our exposition we mainly
follow the traditions of the Cambridge course Coding and Cryptography. We keep
the theoretical explanations to the bare minimum and refer the reader to specialised
books for details. Cryptography has a long and at times fascinating history where
mathematics is interleaved with other sciences and even non-sciences. It has in-
spired countless ï¬ction and half-ï¬ction books, ï¬lms, and broadcast programmes;
its popularity does not seem to be waning.
A popular method of producing encrypted digit sequences is through the so-
called feedback shift registers. We will restrict ourselves to the binary case, work-
ing with string spaces Hn,2 = {0,1}n = FÃ—n
2 .
Deï¬nition 4.5.1
A (general) binary feedback shift register of length d is a map
{0,1}d â†’{0,1}d of the form
(x0,...,xdâˆ’1) â†’

x1,...,xdâˆ’1, f(x0,...,xdâˆ’1)


454
Further Topics from Information Theory
for some function f : {0,1}d â†’{0,1} (a feedback function). The initial string
(x0,...,xdâˆ’1) is called an initial ï¬ll; it produces an output stream (xn)nâ‰¥0 satisfying
the recurrence equation
xn+d = f(xn,...,xn+dâˆ’1),
for all n â‰¥0.
(4.5.1)
A feedback shift register is said to be linear (an LFSR, for short) if function f is
linear and c0 = 1:
f(x0,...,xdâˆ’1) =
dâˆ’1
âˆ‘
i=0
cixi, where ci = 0,1, c0 = 1;
(4.5.2)
in this case the recurrence equation is linear:
xn+d =
dâˆ’1
âˆ‘
i=0
cixn+i for all n â‰¥0.
(4.5.3)
It is convenient to write (4.5.3) in the matrix form
xn+d
n+1 = Vxn+dâˆ’1
n
(4.5.4)
where
V =
â›
âœ
âœ
âœ
âœ
âœ
â
0
1
0
...
0
0
0
0
1
...
0
0
...
...
...
...
...
...
0
0
0
...
0
1
c0
c1
c2
...
cdâˆ’2
cdâˆ’1
â
âŸ
âŸ
âŸ
âŸ
âŸ
â 
, xn+dâˆ’1
n
=
â›
âœ
âœ
âœ
âœ
âœ
â
xn
xn+1
...
xn+dâˆ’2
xn+dâˆ’1
â
âŸ
âŸ
âŸ
âŸ
âŸ
â 
.
(4.5.5)
By the expansion of the determinant along the ï¬rst column one can see that detV =
1 mod 2: the cofactor for the (n,1) entry c0 is the matrix Idâˆ’1. Hence,
detV = c0 detIdâˆ’1 = c0 = 1, and the matrix V is invertible.
(4.5.6)
A useful concept is the auxiliary, or feedback, polynomial of an LFSR from
(4.5.3):
C(X) = c0 +c1X +Â·Â·Â·+cdâˆ’1Xdâˆ’1 +Xd.
(4.5.7)
Observe that general feedback shift registers, after an initial run, become peri-
odic:
Theorem 4.5.2
The output stream (xn) of a general feedback shift register of
length d has the property that there exists integer r, 0 â‰¤r < 2d, and integer D,
1 â‰¤D < 2d âˆ’r, such that xk+D = xk for all k â‰¥r.

4.5 Selected examples and problems from cryptography
455
Proof
A segment xM ...xM+dâˆ’1 determines uniquely the rest of the output stream
in (4.5.1), i.e. (xn, n â‰¥M +d âˆ’1). We see that if such a segment is reproduced in
the stream, it will be repeated. There are 2d different possibilities for a string of d
subsequent digits. Hence, by the pigeonhole principle, there exists 0 â‰¤r < R < 2d
such that the two segments of length d of the output stream, from positions r and
R onwards, will be the same: xr+ j = xR+ j, 0 â‰¤r < R < d. Then, as was noted,
xr+ j = xR+ j for all j â‰¥0, and the assertion holds true with D = Râˆ’r.
In the linear case (LFSR), we can repeat the above argument, with the zero string
discarded. This allows us to reduce 2d to 2d âˆ’1. However, an LFSR is periodic in
a â€˜proper senseâ€™:
Theorem 4.5.3
An LFSR (xn) is periodic, i.e. there exists D â‰¤2d âˆ’1 such that
xn+D = xn for all n. The smallest D with this property is called the period of the
LFSR.
Proof
Indeed, the column vectors xn+dâˆ’1
n
, n â‰¥0, are related by the equation
xn+1 = Vxn = Vn+1x0, n â‰¥0, where matrix V was deï¬ned in (4.5.5). We noted that
detV = c0 Ì¸= 0 and hence V is invertible. As was said before, we may discard the
zero initial ï¬ll. For each vector xn âˆˆ{0,1}d there are only 2d âˆ’1 non-zero possibil-
ities. Therefore, as in the proof of Theorem 4.5.2, among the initial 2d âˆ’1 vectors
xn, 0 â‰¤n â‰¤2d âˆ’2, either there will be repeats, or there will be a zero vector. The
second possibility can be again discarded, as it leads to the zero initial ï¬ll. Thus,
suppose that the ï¬rst repeat was for j and D + j: xj = x j+D, i.e. Vj+Dx0 = Vjx0.
If j Ì¸= 0, we multiply by Vâˆ’1 and arrive at an earlier repeat. So: j = 0, D â‰¤2d âˆ’1
and VDx0 = x0. Then, obviously, xn+D = Vn+Dx0 = Vnx0 = xn.
Worked Example 4.5.4
Give an example of a general feedback register with
output kj, and initial ï¬ll (k0,k1,...,kN), such that
(kn,kn+1,...,kn+N) Ì¸= (k0,k1,...,kN) for all n â‰¥1.
Solution Take f : {0,1}2 â†’{0,1}2 with f(x1,x2) = x21. The initial ï¬ll 00 yields
00111111111.... Here, kn+1 Ì¸= 0 = k1 for all n â‰¥1.
Worked Example 4.5.5
Let matrix V be deï¬ned by (4.5.5), for the linear re-
cursion (4.5.3). Deï¬ne and compute the characteristic and minimal polynomials
for V.

456
Further Topics from Information Theory
Solution
The characteristic polynomial of matrix V is hV(X) âˆˆF2[X] = X â†’
det(XIâˆ’V):
hV(X) = det
â›
âœ
âœ
âœ
âœ
âœ
â
X
1
0
...
0
0
0
X
1
...
0
0
...
...
...
...
...
...
0
0
0
...
X
1
c0
c1
c2
...
cdâˆ’2
(cdâˆ’1 +X)
â
âŸ
âŸ
âŸ
âŸ
âŸ
â 
(4.5.8)
(recall, entries 1 and ci are considered in F2). Expanding along the bottom row,
the polynomial hV(t) is written as a linear combination of determinants of size
(d âˆ’1)Ã—(d âˆ’1) (co-factors):
c0 det
â›
âœ
âœ
âœ
â
1
0
...
0
0
X
1
...
0
0
...
...
...
...
...
0
0
...
X
1
â
âŸ
âŸ
âŸ
â +c1 det
â›
âœ
âœ
âœ
â
X
0
...
0
0
X
1
...
0
0
...
...
...
...
...
0
0
...
X
1
â
âŸ
âŸ
âŸ
â 
+Â·Â·Â·+cdâˆ’2 det
â›
âœ
âœ
âœ
â
X
1
...
0
0
0
X
...
0
0
...
...
...
...
...
0
0
...
0
1
â
âŸ
âŸ
âŸ
â 
+(cdâˆ’1 +X)det
â›
âœ
âœ
âœ
â
X
1
...
0
0
0
X
...
0
0
...
...
...
...
...
0
0
...
0
X
â
âŸ
âŸ
âŸ
â 
= c0 +c1X +Â·Â·Â·+cdâˆ’2Xdâˆ’2 +(cdâˆ’1 +X)Xdâˆ’1
=
âˆ‘
0â‰¤iâ‰¤dâˆ’1
ciXi +Xd,
which gives the characteristic polynomial C(X) of the recursion.
By the Cayleyâ€“Hamilton theorem,
hV(V) = c0I+c1V+Â·Â·Â·+cdâˆ’1Vdâˆ’1 +Vd = O.
The minimal polynomial, mV(X), of matrix V is the polynomial of minimal
degree such that mV(V) = O. It is a divisor of hV(X), and every root of hV(X) is a
root of mV(X). The difference between mV(X) and hV(X) is in multiplicities: the
multiplicity of a root Î¼ of mV(X) equals the maximal size of the Jordan cell of V
corresponding to Î¼ whereas for hV(X) it is the sum of the sizes of all Jordan cells
in V corresponding to Î¼.

4.5 Selected examples and problems from cryptography
457
To calculate mV(X), we:
(i) take a basis e1,...,ed (in FÃ—d
2 );
(ii) then for any vector ej we ï¬nd the minimal number dj such that vectors ej,
Vej,...,Vd jej, Vd j+1ej are linearly dependent;
(iii) identify the corresponding linear combination
a( j)
0 ej +a( j)
1 Vej +Â·Â·Â·+a( j)
d j Vd jej +Vd j+1ej = 0.
(iv) Further, we form the corresponding polynomial
m( j)
V (X) = âˆ‘
0â‰¤iâ‰¤d j
a( j)
i Xi +Xd j+1.
(v) Then,
mV(X) = lcm

m(1)
V (X),...,m(d)
V (X)

.
In our case, it is convenient to take
ej =
â›
âœ
âœ
âœ
âœ
âœ
âœ
â
0
...
1
...
0
â
âŸ
âŸ
âŸ
âŸ
âŸ
âŸ
â 
âˆ¼
...
j
...
.
Then Vje1 = ej, and we obtain that d1 = d, and
m(1)
V (X) =
âˆ‘
0â‰¤iâ‰¤dâˆ’1
ciXi +Xd = hV(X).
We see that the feedback polynomial C(X) of the recursion coincides with the
characteristic and the minimal polynomial for V. Observe that at X = 0 we obtain
hV(0) = C(0) = c0 = 1 = detV.
(4.5.9)
Any polynomial can be identiï¬ed through its roots; we saw that such a descrip-
tion may be extremely useful. In the case of an LFSR, the following example is
instructive.
Theorem 4.5.6
Consider the binary linear recurrence in (4.5.3) and the corre-
sponding auxiliary polynomial C(X) from (4.5.7).
(a) Suppose K is a ï¬eld containing F2 such that polynomial C(X) has a root Î± of
multiplicity m in K. Then, for all k = 0,1,...,mâˆ’1,
xn = A(n,k)Î±n, n = 0,1,...,
(4.5.10)

458
Further Topics from Information Theory
is a solution to (4.5.3) in K, where
A(n,k) =
â§
âª
â¨
âª
â©
1,
k = 0,

âˆ
0â‰¤lâ‰¤kâˆ’1
(nâˆ’l)+

mod 2,
k â‰¥1.
(4.5.11)
Here, and below, (a)+ stands for max[a,0]. In other words, sequence x(k) =
(xn), where xn is given by (4.5.10), is an output of the LFSR with auxiliary
polynomial C(X).
(b) Suppose K is a ï¬eld containing F2 such that C(X) factorises in K into lin-
ear factors. Let Î±1,...,Î±r âˆˆK be distinct roots of C(X) of multiplicities
m1,...,mr, with
âˆ‘
1â‰¤iâ‰¤r
mi = d. Then the general solution of (4.5.3) in K is
xn = âˆ‘
1â‰¤iâ‰¤r
âˆ‘
0â‰¤kâ‰¤miâˆ’1
bi,kA(n,k)Î±n
i
(4.5.12)
for some bu,v âˆˆK. In other words, sequences x(i,k) = (xn), where xn = A(n,k)Î±n
i
and A(n,k) is given by (4.5.11), span the set of all output streams of the LFSR
with auxiliary polynomial C(X).
Proof
(a) If C(X) has a root Î± âˆˆK of multiplicity m then C(X) = (X âˆ’Î±)m  C(X)
where  C(X) is a polynomial of degree dâˆ’m (with coefï¬cients from a ï¬eld Kâ€² âŠ†K).
Then, for all k = 0,...,mâˆ’1, and for all n â‰¥d, the polynomial
Dk,n(X) := Xk dk
dXk

Xnâˆ’dC(X)

(with coefï¬cients taken mod 2) vanishes at X = Î± (in ï¬eld K):
Dk,n(Î±) =
âˆ‘
0â‰¤iâ‰¤dâˆ’1
ci A(nâˆ’d +i,k)Î±nâˆ’d+i +A(n,k)Î±n.
This yields
A(n,k)Î±n =
âˆ‘
0â‰¤iâ‰¤dâˆ’1
ciA(nâˆ’d +i,k)Î±nâˆ’d+i.
Thus, stream x(k) = (xn) with xn as in (4.5.10) solves the recursion xn =
âˆ‘
0â‰¤iâ‰¤dâˆ’1
cixnâˆ’d+i in K. The number of such solutions equals m, the multiplicity
of root Î±.
(b) First, observe that the set of output streams (xn)nâ‰¥0 forms a linear space W over
K (in the set of all sequences with entries from K). The dimension of W equals d,
as every stream is uniquely deï¬ned by a seed (initial ï¬ll) x0x1 ...xdâˆ’1 âˆˆKd. On the
other hand, d =
âˆ‘
1â‰¤iâ‰¤r
mi, the total number of sequences x(i,k) =

x(i,k)
n

with entries
x(i,k)
n
= A(n,k)Î±n
i , n = 0,1,....

4.5 Selected examples and problems from cryptography
459
Thus,
it
sufï¬ces
to
check
that
the
streams
x(i,k),
where
i = 1,...,r,
k = 0,1...,mi âˆ’1, are linearly independent over K.
To this end, take a linear combination
âˆ‘
1â‰¤iâ‰¤r
âˆ‘
0â‰¤kâ‰¤miâˆ’1
bi,kx(i,k) and assume it gives
0. Let us also agree that sequence x(i,k) = 0 for k < 0. It is convenient to introduce
a shift operator x = (xn) â†’Sx where sequence Sx = (xâ€²
n) has entries xâ€²
n = xn+1,
n = 0,1,.... The key observation is as follows. Let I stand for the identity transfor-
mation. Then for all Î² âˆˆK,
(Sâˆ’Î²I)x(i,k) = (Î±i âˆ’Î²)x(i,k) +kÎ±ix(i,kâˆ’1).
In fact, the nth entry of the sequence (Sâˆ’Î²I)x(i,k) equals
A(n+1,k)Î±n+1
i
âˆ’Î²A(k,n)Î±n
i
= [A(n,k)+kA(n,k âˆ’1)]Î±n+1
i
âˆ’Î²A(n,k)Î±n
i
= (Î±i âˆ’Î²)A(n,k)Î±n
i +kÎ±iA(n,k âˆ’1)Î±n
i ,
in agreement with the above equation for sequences. We have used here the ele-
mentary equation
A(n+1,k) = A(n,k)+kA(n,k âˆ’1).
Then, iterating, we obtain
(Sâˆ’Î²1I)(Sâˆ’Î²2I)x(i,k) = (Î±i âˆ’Î²1)(Î±i âˆ’Î²2)x(i,k)
+kÎ±i(Î±i âˆ’Î²1 +Î±i âˆ’Î²2)x(i,kâˆ’1) +k2Î±2
i x(i,kâˆ’2)
= (Sâˆ’Î²2I)(Sâˆ’Î²1I)x(i,k),
and so on (all operations with coefï¬cients are performed in ï¬eld K). In particular,
with Î² = Î±i:
(Sâˆ’Î±iI)lx(i,k) =
% (kÎ±i)lx(i,kâˆ’l),
1 â‰¤l â‰¤k,
0,
l > k.
Now consider the product of operators
âˆ
1â‰¤i<r
(Sâˆ’Î±iI)mi(Sâˆ’Î±rI)mrâˆ’1 applied to
our vanishing linear combination
âˆ‘
1â‰¤iâ‰¤r
âˆ‘
0â‰¤kâ‰¤miâˆ’1
bi,kx(i,k). The only term that sur-
vives comes from the summand br,mrâˆ’1x(r,mrâˆ’1). This gives
br,mrâˆ’1 âˆ
1â‰¤i<r
(Î±i âˆ’Î±r)mi[(mr âˆ’1)Î±r]mrâˆ’1x(i,0) = 0.
Hence, br,mrâˆ’1 = 0. Next, we apply
âˆ
1â‰¤i<r
(S âˆ’Î±iI)mi(S âˆ’Î±rI)mrâˆ’2 to obtain that
br,mrâˆ’2 = 0. Continuing in a similar fashion, we can guarantee that each coefï¬cient
bi,k = 0.

460
Further Topics from Information Theory
Upon seeing a stream of digits (xn)nâ‰¥0, an observer may wish to determine
whether it was produced by an LFSR. This can be done by using the so-called
Berlekampâ€“Massey (BM) algorithm, solving a system of linear equations. If a se-
quence (xn) comes from an LFSR with feedback polynomial C(X) =
dâˆ’1
âˆ‘
i=0
ciXi +Xd
then the recurrence xn+d =
dâˆ’1
âˆ‘
i=0
cixn+i for n = 0,...,d can be written in a vector-
matrix form Adcd = 0 where
Ad =
â›
âœ
âœ
âœ
â
x0
x1
x2
...
xd
x1
x2
x3
...
xd+1
...
...
...
...
...
xd
xd+1
xd+2
...
x2d
â
âŸ
âŸ
âŸ
â ,
cd =
â›
âœ
âœ
âœ
âœ
âœ
â
c0
c1
...
cdâˆ’1
1
â
âŸ
âŸ
âŸ
âŸ
âŸ
â 
.
(4.5.13)
Consequently, the (d + 1) Ã— (d + 1) matrix Ad must have determinant 0, and the
(d +1)-dimensional vector cd must lie in the null-space kerAd.
The algorithm begins with an inspection of matrix Ar for a small value of r
(known to be â‰¤d):
Ar =
â›
âœ
âœ
âœ
â
x0
x1
x2
...
xr
x1
x2
x3
...
xr+1
...
...
...
...
...
xr
xr+1
xr+2
...
x2r
â
âŸ
âŸ
âŸ
â .
We calculate detAr: if detAr Ì¸= 0, we conclude that d Ì¸= r and increase r by 1. If
det Ar = 0 then we solve the equation Arar = 0, i.e. try d = r:
Ad
â›
âœ
âœ
âœ
âœ
âœ
â
a0
a1
...
adâˆ’1
1
â
âŸ
âŸ
âŸ
âŸ
âŸ
â 
= 0, where Ad =
â›
âœ
âœ
âœ
â
x0
x1
...
xd
x1
x2
...
xd+1
...
...
...
...
xd
xd+1
...
x2d
â
âŸ
âŸ
âŸ
â 
(e.g. by Gaussian elimination) and test sequence (xn) for the recursion xn+d =
âˆ‘
0â‰¤iâ‰¤dâˆ’1
aixn+i. If we discover a discrepancy, we choose a different vector cr âˆˆ
kerAr or â€“ if it fails â€“ increase r.
The BM algorithm can be stated in an elegant algebraic form. Given a sequence
(xn), consider a formal power series in X:
âˆ
âˆ‘
j=0
xjX j. The fact that (xn) is produced

4.5 Selected examples and problems from cryptography
461
by the LFSR with a feedback polynomial C(X) is equivalent to the fact that the
above series is obtained by dividing a polynomial A(X) =
d
âˆ‘
i=0
aiXi by C(X):
âˆ
âˆ‘
j=0
xjX j = A(X)
C(X).
(4.5.14)
Indeed, as c0 = 1, A(X) = C(X)
âˆ
âˆ‘
j=0
xjX j is equivalent to
an =
n
âˆ‘
i=1
cixnâˆ’i, n = 1,...,
(4.5.15)
or
xn =
â§
âª
âª
â¨
âª
âª
â©
an âˆ’
nâˆ’1
âˆ‘
i=1
cixnâˆ’i,
n = 0,1,...,d,
âˆ’
nâˆ’1
âˆ‘
i=0
cixnâˆ’i,
n > d.
(4.5.16)
In other words, A(X) takes part in specifying the initial ï¬ll, and C(X) acts as the
feedback polynomial.
Worked Example 4.5.7
What is a linear feedback shift register? Explain the
Berlekampâ€“Massey method for recovering the feedback polynomial of a linear
feedback shift register from its output. Illustrate in the case when we observe out-
puts
1 0 1 0 1 1 0 0 1 0 0 0 ...,
0 1 0 1 1 1 1 0 0 0 1 0 ...
and
1 1 0 0 1 0 1 1.
Solution An initial ï¬ll x0 ...xdâˆ’1 produces an output stream (xn)nâ‰¥0 satisfying the
recurrence equation
xn+d =
dâˆ’1
âˆ‘
i=0
cixn+i for all n â‰¥0.
The feedback polynomial
C(X) = c0 +c1X +Â·Â·Â·+cdâˆ’1Xdâˆ’1 +Xd
is the characteristic polynomial for this recurrence equation determining its solu-
tions. We will assume that coefï¬cient c0 Ì¸= 0; otherwise value xn has no impact on
xn+d and the register can be treated as the one of length d âˆ’1.

462
Further Topics from Information Theory
The Berlekampâ€“Massey algorithm begins with an inspection of matrix
A1 =
1
0
0
1

, with detA1 Ì¸= 0,
but
A2 =
â›
â
1
0
1
0
1
0
1
0
1
â
â , with detA2 = 0,
and A2
â›
â
c0
c1
1
â
â = 0 has the solution c0 = 1, c1 = 0. This gives the recursion
xn+2 = xn,
which does not ï¬t the remaining digits. So, we move to A3:
A3 =
â›
âœ
âœ
â
1
0
1
0
0
1
0
1
1
0
1
1
0
1
1
0
â
âŸ
âŸ
â , with detA3 Ì¸= 0,
and then to A4:
A4 =
â›
âœ
âœ
âœ
âœ
â
1
0
1
0
1
0
1
0
1
1
1
0
1
1
0
0
1
1
0
0
1
1
0
0
1
â
âŸ
âŸ
âŸ
âŸ
â 
, with detA4 = 0.
The equation A4c4 = 0 is solved by c4 =
â›
âœ
âœ
â
1
0
0
1
â
âŸ
âŸ
â . This yields
xn+4 = xn +xn+3,
which ï¬ts the rest of the string. In the second example we have:
det
0
1
1
0

Ì¸= 0,det
â›
â
0
1
0
1
0
1
0
1
1
â
â Ì¸= 0,det
â›
âœ
âœ
â
0
1
0
1
1
0
1
1
0
1
1
1
1
1
1
1
â
âŸ
âŸ
â Ì¸= 0

4.5 Selected examples and problems from cryptography
463
and
â›
âœ
âœ
âœ
âœ
â
0
1
0
1
1
1
0
1
1
1
0
1
1
1
1
1
1
1
1
0
1
1
1
0
0
â
âŸ
âŸ
âŸ
âŸ
â 
â›
âœ
âœ
âœ
âœ
â
1
1
0
0
1
â
âŸ
âŸ
âŸ
âŸ
â 
= 0.
This yields the solution: d = 4, xn+4 = xn +xn+1. The linear recurrence relation is
satisï¬ed by every term of the output sequence given. The feedback polynomial is
then X4 +X +1.
In the third example the recursion is xn+3 = xn +xn+1.
LFSRs are used for producing additive stream ciphers. Additive stream ciphers
were invented in 1917 by Gilbert Vernam, at the time an engineer with the AT&T
Bell Labs. Here, the sending party uses an output stream from an LFSR (kn) to
encrypt a plain text (pn) by (zn) where
zn = pn +kn mod 2, n â‰¥0.
(4.5.17)
The recipient would decrypt it by
pn = zn +kn mod 2, n â‰¥0,
(4.5.18)
but of course he must know the initial ï¬ll k0 ...kdâˆ’1 and the string c0 ...cdâˆ’1. The
main deï¬ciency of the stream cipher is its periodicity. Indeed, if the generating
LFSR has period D then it is enough for an â€˜attackerâ€™ to have in his possession a
cipher text z0 z1 ...z2Dâˆ’1 and the corresponding plain text p0 p1 ... p2Dâˆ’1, of length
2D. (Not an unachievable task for a modern-day Sherlock Holmes.) If by some luck
the attacker knows the value of the period D then he only needs z0 z1 ...zDâˆ’1 and
p0 p1 ... pDâˆ’1. This will allow the attacker to break the cipher, i.e. to decrypt the
whole plain text, however long.
Clearly, short-period LFSRs are easier to break when they are used repeat-
edly. The history of World War II and the subsequent Cold War has a number of
spectacular examples (German code-breakers succeeding in part in reading British
Navy codes, British and American code-breakers succeeding in breaking German
codes, the American project â€˜Venonaâ€™ deciphering Soviet codes) achieved because
of intensive message trafï¬c. However, even ultra-long periods cannot guarantee
safety.
As far as this section of the book is concerned, the period of an LFSR can be
increased by combining several LFSRs.
Theorem 4.5.8
Suppose a stream (xn) is produced by an LFSR of length d1,
period D1 and with an auxiliary polynomial C1(X), and a stream (yn) by an LFSR

464
Further Topics from Information Theory
of length d2, period D2 and with an auxiliary polynomial C2(X). Let Î±1,...,Î±r1
and Î²1,...,Î²r2 be the distinct roots of C1(X) and C2(X), respectively, lying in some
ï¬eld K âŠƒF2. Let mi be the multiplicity of root Î±i and mâ€²
j be the multiplicity of root
Î²j, with d1 =
âˆ‘
1â‰¤iâ‰¤r1
mi and d2 =
âˆ‘
1â‰¤iâ‰¤r2
mâ€²
i. Then
(a) Stream (xn + yn) is produced by an LFSR with the auxiliary polynomial
lcm(C1(X),C2(X)).
(b) Stream (xnyn) is produced by an LFSR with the auxiliary polynomial C(X) =
âˆ
1â‰¤iâ‰¤r1 âˆ
1â‰¤jâ‰¤r2
(X âˆ’Î±iÎ²j)mi+mâ€²
jâˆ’1.
In particular, the period of the resulting LFSR is in both cases divisible by
lcm(D1,D2).
Proof
According to Theorem 4.5.6, the output streams (xn) and (yn) for the LF-
SRs in question have the following form in ï¬eld K:
xn = âˆ‘
1â‰¤iâ‰¤r1
âˆ‘
0â‰¤kâ‰¤miâˆ’1
ai,kA(n,k)Î±n
i ,yn = âˆ‘
1â‰¤jâ‰¤r2
âˆ‘
0â‰¤lâ‰¤mâ€²
jâˆ’1
b j,lA(n,l)Î² n
j ,
(4.5.19)
for some ai,k,b j,l âˆˆK.
(a) Writing xn+yn as the sum of the expressions from (4.5.19) and grouping similar
terms leads to the statement (a).
(b) For the product xnyn we have the expression
âˆ‘
i, j âˆ‘
k,l
ai,kb j,lA(n,k)A(n,l)(Î±iÎ²j)n.
The
product
ai,kb j,lA(n,k)A(n,l)
can
be
written
as
a
sum
âˆ‘
kâˆ§lâ‰¤tâ‰¤k+lâˆ’1
A(n,t)ut(ai,k,b j,l) where coefï¬cients ut(ai,k,b j,l) âˆˆK. This gives
the following representation of xnyn:
âˆ‘
1â‰¤iâ‰¤r1 âˆ‘
1â‰¤jâ‰¤r2
âˆ‘
0â‰¤tâ‰¤mi+mâ€²
jâˆ’2
A(n,t)
âˆ‘
k,l:kâˆ§lâ‰¤tâ‰¤k+lâˆ’1
ut(ai,k,b j,l)(Î±iÎ²j)n
which in turn can be written as
xnyn = âˆ‘
1â‰¤iâ‰¤r1 âˆ‘
1â‰¤jâ‰¤r2
âˆ‘
0â‰¤tâ‰¤mi+mâ€²
jâˆ’2
A(n,t)vi, j;t(Î±iÎ²j)n
corresponding to the generic form of the output stream for the LFSR with the aux-
iliary polynomial C(X) in statement (b).
Despite serious drawbacks, LFSRs remain in use in a variety of situations: they
allow simple enciphering and deciphering without â€˜lookaheadâ€™ and display a â€˜lo-
calâ€™ effect of an error, be it encoding, transmission or decoding. More generally,

4.5 Selected examples and problems from cryptography
465
non-linear LFSRs often offer only marginal advantages while bringing serious dis-
advantages, in particular with deciphering.
. . . an error by the same example
Will rush into the state.
William Shakespeare (1564â€“1616), English playwright and poet
from Merchant of Venice
Worked Example 4.5.9
(a) Let (xn), (yn), (zn) be three streams produced by
LFSRs. Set
kn = xn if yn = zn,
kn = yn if yn Ì¸= zn .
Show that kn is also a stream produced by a linear feedback register.
(b) A cipher stream is given by a linear feedback register of known length d. Show
that, given plain text and ciphered text of length 2d, we can ï¬nd the cipher stream.
Solution (a) For three streams (xn), (yn), (zn) produced by LFSRs we set
kn = xn +(xn +yn)(yn +zn)
(in F2).
So it sufï¬ces to note that (pointwise) sums and products of streams produced by
LFSRs also yield some streams produced by LFSRs.
(b) Suppose the plain text is y1 y2 ...y2d, and the ciphered text is x1 +y1 x2 +y2 ...
x2d + y2d. Then we can recover x1 ...x2d. We know that c1 ...cd must satisfy d
simultaneous linear equations
xd+ j =
d
âˆ‘
i=1
cixj+iâˆ’1, for j = 1,2,...,d.
Solve these to ï¬nd c1,c2,...,cd and hence the cipher stream.
Worked Example 4.5.10
A binary non-linear feedback register of length 4 has
deï¬ning relation
xn+1 = xnâˆ’1 +xnxnâˆ’2 +xnâˆ’3.
Show that the state space contains cycles of lengths 1, 4, 9 and 2.

466
Further Topics from Information Theory
Solution There are 24 = 16 initial binary strings. By inspection,
0000 â†’0000
(a cycle of length 1),
0001 â†’0010 â†’0100 â†’1000 â†’0001
(a cycle of length 4),
0011 â†’0111 â†’1111 â†’1110 â†’1101
â†’1011 â†’0110 â†’1100 â†’1001 â†’0011
(a cycle of length 9),
0101 â†’1010 â†’0101
(a cycle of length 2).
All 16 initial ï¬lls have appeared in the list, so the analysis is complete.
Worked Example 4.5.11
Describe how an additive stream cipher operates. What
is a one-time pad? Explain brieï¬‚y why a one-time pad is safe if used only once but
becomes unsafe if used many times. A one-time pad is used to send the message
x1x2x3x4x5x6y7 which is encoded as 0101011. By mistake, it is reused to send the
message y0x1x2x3x4x5x6 which is encoded as 0100010. Show that x1x2x3x4x5x6 is
one of two possible messages, and ï¬nd the two possibilities.
Solution A one-time pad is an example of a cipher based on a random key and
proposed by Gilbert Vernam and Joseph Mauborgne (the Chief of the USA Signal
Corps during World War II). The cipher uses a random number generator producing
a sequence k1k2k3 ... from the alphabet J of size q. More precisely, each letter
is uniformly distributed over J and different letters are independent. A message
m = a1a2 ...an is encrypted as c = c1c2 ...cn where
ci = ai +ki

mod q

.
To show that the one-time pad achieves perfect secrecy, write
P(M = m,C = c) = P(M = m,K = câˆ’m)
= P(M = m)P(K = câˆ’m) = P(M = m) 1
qn ;
here the subtraction c âˆ’m is digit-wise and mod q. Hence, the conditional proba-
bility
P(C = c|M = m) = P(M = m,C = c)
P(M = m)
= 1
qn
does not depend on m. Hence, M and C are independent.
Working in F2, consider a cipher key stream k1 k2 k3 .... The plain (input) text
stream p1 p2 p3 ... is encrypted as the cipher text stream c1 c2 c3 ..., where cj =
p j + kj. If the kj are IID random numbers and the cipher key stream is only used
once (which happens in practice) then we have a one-time pad. (It is assumed that

4.5 Selected examples and problems from cryptography
467
the cipher key stream is known only to the sender and the recipient.) In the example,
we have
x1x2x3x4x5x6y7 â†’0101011,
y0x1x2x3x4x5x6 â†’0100010.
Suppose x1 = 0. Then
k0 = 0,k1 = 1,x2 = 0,k2 = 0,x3 = 0,k3 = 0,x4 = 1,k1 = 0,
x5 = 1,k5 = 0,x6 = 1,k6 = 1.
Thus,
k = 0100101, x = 000111.
If x1 = 1, every digit changes, so
k = 1011010, x = 111000.
Alternatively, set x0 = y0 and x7 = y7. If the ï¬rst cipher is q1 q2 ..., the second is
p1 p2 ... and the one-time pad is k1,k2,..., then
q j = xj+1 +kj, p j = xj +kj.
So,
xj +xj+1 = q j + p j,
and
x1 +x2 = 0, x2 +x3 = 0,
x3 +x4 = 1, x4 +x5 = 0, x5 +x6 = 0.
This yields
x1 = x2 = x3, x4 = x5 = x6, x4 = x3 +1.
The message is 000111 or 111000.
Worked Example 4.5.12
(a) Let Î¸ : Z+ â†’{0,1} be given by Î¸(n) = 1 if n is
odd, Î¸(n) = 0 if n is even. Consider the following recurrence relation over F2:
un+3 +un+2 +un+1 +un = 0.
(4.5.20)
Is it true that the general solution of (4.5.20) is un = A + BÎ¸(n) +CÎ¸(n2)? If it is
true, prove it. If not, explain why it is false and state and prove the correct result.
(b) Solve the recurrence relation un+2 +un = 1 over F2, subject to u0 = 1, u1 = 0,
expressing the solution in terms of Î¸ and n.
(c) Four streams wn, xn, yn, zn are produced by linear feedback registers. If we set
kn =
% xn +yn +zn
if zn +wn = 1,
xn +wn
if zn +wn = 0,
show that kn is also a stream produced by a linear feedback register.

468
Further Topics from Information Theory
Solution (a) Observe that Î¸(n2) = Î¸(n), so the suggested sum contains only two
arbitrary constants. Now consider g(n) = Î¸

n(nâˆ’1)/2

. Then
g(n+3)+g(n+2)+g(n+1)+g(n)
= Î¸
(n+3)(n+2)
2

+Î¸
(n+2)(n+1)
2

+Î¸
(n+1)n
2

+Î¸
n(nâˆ’1)
2

= Î¸

(n+2)2 +n2
= 0,
and g(0) = g(1) = 0,g(2) = 1. Then we substitute n = 0 and n = 1 into the relation
aÎ¸(n)+b+cg(n) = 0, and observe that a = b = c = 0. So, Î¸(n),1,g(n) are inde-
pendent. Thus, AÎ¸(n)+B+Cg(n) is a general solution of the third-order difference
equation.
(b) First try to solve the recurrence relation un+2 +un = 1 without additional con-
ditions
g(n)+g(n+2) = Î¸
n(nâˆ’1)
2
+ (n+2)(n+1)
2

= Î¸
n2 âˆ’n+n2 +3n+2
2

= Î¸

n2 +n+1

= 1.
Now substitute n = 0 and n = 1 into relation un = A+BÎ¸(n)+g(n) to get A = B =
1. Thus, un = 1+Î¸(n)+g(n).
(c) The sequence kn is produced by the linear register
kn = xn +wn +(zn +wn)(yn +zn +wn).
In the next part of this section, we discuss properties of a class of cryptosystems
used in modern practice and called public-key ciphers, focusing in particular on the
RSA and the bit commitment cryptosystems.
Deï¬nition 4.5.13
We say that a formal cryptosystem is given, if we can identify:
(a) a set P of plaintexts (source messages in the language of Chapter 1);
(b) a set C of ciphertexts (codewords in the language of Chapter 1);
c) a set K of keys that label the encoding maps;
(d) the set E of encryptic functions (encoding maps) where each function Ek takes
P âˆˆP â†’Ek(P) âˆˆC and is labelled by an element k âˆˆK ;
(e) the set D of decryptic functions (decoding maps) where each function Dk takes
C âˆˆC â†’Dk(C) âˆˆP and is again labelled by an element k âˆˆK ;

4.5 Selected examples and problems from cryptography
469
such that
(f) for all key e âˆˆK there is a key d âˆˆK , with the property that Dd(Ee(P)) = P
for all plaintext P âˆˆP.
Example 4.5.14
Suppose that two parties, Bob and Alice, intend to have a two-
side private communication. They want to exchange their keys, EA and EB, by us-
ing an insecure binary channel. An obvious protocol is as follows. Alice encrypts
a plain-text m as EA(m) and sends it to Bob. He encrypts it as EB(EA(m)) and
returns it to Alice. Now we make a crucial assumption that EA and EB commute
for any plaintext mâ€² : EA â—¦EB(mâ€²) = EB â—¦EA(mâ€²). In this case Alice can decrypt
this message as DA(EA(EB(m))) = EB(m) and send this to Bob, who then calcu-
lates DB(EB(m)) = m. Under this protocol, at no time during the transaction is an
unencrypted message transmitted.
However, a further thought shows that this is no solution at all. Indeed, suppose
that Alice uses a one-time pad kA and Bob uses a one-time pad kB. Then any sin-
gle interception provides no information about plaintext m. However, if all three
transmissions are intercepted, it is enough to take the sum
(m+kA)+(m+kA +kB)+(m+kB) = m
to obtain the plaintext m. So, more sophisticated protocols should be developed:
this is where public key cryptosystems are helpful.
Another popular example is a network of investors and brokers dealing in a
market and using an open access cryptosystem such as RSA. An investorâ€™s concern
is that a broker will buy shares without her authorisation and, in the case of a loss,
claim that he had a written request from the client. Indeed, it is easy for a broker
to generate a coded order requesting to buy the stocks as the encoding key is in the
public domain. On the other hand, a broker may be concerned that if he buys the
shares by the investorâ€™s request and the market goes down, the investor may claim
that she never ordered this transaction and that her coded request is a fake.
However, it is easy to develop a protocol which addresses these concerns. An
investor Alice sends to a broker Bob, together with her request p to buy shares, her
â€˜electronic signatureâ€™ fB f âˆ’1
A (p). After receiving this message Bob sends a receipt
r encoded as fA f âˆ’1
B (r). If a conï¬‚ict emerges, both sides can provide a third party
(say, a court) with these coded messages and the keys. Since no-one but Alice could
generate the message coded by fB f âˆ’1
A
and no-one but Bob could generate the mes-
sage coded by fA f âˆ’1
B , no doubts would remain. This is the gist of bit commitment.
The above-mentioned RSA (Rivestâ€“Shamirâ€“Adelman) scheme is a prime example

470
Further Topics from Information Theory
of a public key cryptosystem. Here, a recipient user (Bob, possibly a collective
entity) sets
N = pq, where p and q are two large primes, kept secret.
(4.5.21)
Number N is often called the RSA modulus (and made public). The value of the
Euler totient function is
Ï†(N) = (pâˆ’1)(qâˆ’1), kept secret.
Next, the recipient user chooses (or is given by the key centre) an integer l such
that
1 < l < Ï†(N) and gcd (Ï†(N),l) = 1.
(4.5.22)
Finally, an integer d is computed (again, by Bob or on his behalf) such that
1 < d < Ï†(N) and l d = 1
mod Ï†(N).
(4.5.23)
[The value of d can be computed via the extended Euclidâ€™s algorithm.] The public
key eB used for encryption is the pair (N,l) (listed in the public directory). The
sender (Alice), when communicating to Bob, understands that Bobâ€™s plaintext and
ciphertext sets are P = C = {1,...,N âˆ’1}. She then encrypts her chosen plaintext
m = 1,...,N âˆ’1 as the ciphertext
EN,l(m) = c where c = ml mod N.
(4.5.24)
Bobâ€™s private key dB is the pair (N,d) (or simply number d): it is kept secret
from public but made known to Bob. The recipient decrypts ciphertext c as
Dd(c) = cd mod N.
(4.5.25)
In the literature, l is often called the encryption and d the decryption exponent.
Theorem 4.5.15 below guarantees that
Dd(c) = mdl = m mod N,
(4.5.26)
i.e. the ciphertext c is decrypted correctly. More precisely,
Theorem 4.5.15
For all integers m = 0,...,N âˆ’1, the equation (4.5.26) holds
true, where l and d satisfy (4.5.22) and (4.5.23) and N is as in (4.5.21).
Proof
By virtue of (4.5.23),
l d = 1+b(pâˆ’1)(qâˆ’1)
where b is an integer. Then
(ml)d = mld = m1+b(pâˆ’1)(qâˆ’1) = m

m(pâˆ’1)(qâˆ’1)b.

4.5 Selected examples and problems from cryptography
471
Recall the Eulerâ€“Fermat theorem: If gcd(m, p) = 1 then mÏ†(p) = 1 mod p.
We deduce that if m is not divisible by p then
(ml)d = m mod p.
(4.5.27)
Otherwise, i.e. when p|m, (4.5.27) still holds as m and (ml)d are both equal to
0 mod p. By a similar argument,
(ml)d = m mod q.
(4.5.28)
By the Chinese remainder theorem (CRT) â€“ [28], [114] â€“ (4.5.27) and (4.5.28)
imply (4.5.26).
Example 4.5.16
Suppose Bob has chosen p = 29, q = 31, with N = 899 and
Ï†(N) = 840. The smallest possible value of e with gcd(l,Ï†(N)) = 1 is l = 11, after
that 13 followed by 17, and so on. The (extended) Euclid algorithm yields d = 611
for l = 11, d = 517 for l = 13, and so on. In the ï¬rst case, the encrypting key E899,11
is
m â†’m11 mod 899, that is, E899,11(2) = 250.
The ciphertext 250 is decoded by
D611(250) = 250611 mod 899 = 2,
with the help of the computer. [The computer is needed even after the simpliï¬cation
rendered by the use of the CRT. For instance, the command in Mathematica is
PowerMod[250,611,899].]
Worked Example 4.5.17
(a) Referring to the RSA cryptosystem with public key
(N,l) and private key (Ï†(N),d), discuss possible advantages or disadvantages of
taking (i) l = 232 +1 or (ii) d = 232 +1.
(b) Let a (large) number N be given, and we know that N is a product of two distinct
prime numbers, N = pq, but we do not know the numbers p and q. Assume that
another positive integer, m, is given, which is a multiple of Ï†(N). Explain how to
ï¬nd p and q.
(c) Describe how to solve the bit commitment problem by means of the RSA.
Solution Using l = 232 +1 provides fast encryption (you need just 33 multiplica-
tions using repeated squaring). With d = 232 +1 one can decrypt messages quickly
(but an attacker can easily guess it).

472
Further Topics from Information Theory
(b) Next, we show that if we know a multiple m of Ï†(N) then it is â€˜easyâ€™ to factor N.
Given positive integers y > 1 and M > 1, denote by ordM(y) the order of y relative
to M:
ordM(y) = min
	
s = 1,2,... : ys = 1 mod M

.
Assume that m = 2ab where a â‰¥0 and b is odd. Set
X =
*
x = 1,2,...,N : ordp(xb) Ì¸= ordq(xb)
+
.
(4.5.29)
Given N, l and d, we put m = dl âˆ’1. As Ï†(N)|dl âˆ’1 we can use Lemma 4.5.18
below to factor N. We select x < N. Suppose gcd(x,N) = 1; otherwise the search
is already successful. The probability of ï¬nding a non-trivial factor is 1/2, so the
probability of failure after r random choices of x âˆˆX is 1/2r.
(c) The bit commitment problem arises in the following case: Alice sends a mes-
sage to Bob in such a way that
(i) Bob cannot read the message until Alice sends further information;
(ii) Alice cannot change the message.
A solution is to use the electronic signature: Bob cannot read the message until
Alice (later) reveals her private key. This does not violate conditions (i), (ii) and
makes it (legally) impossible for Alice to refuse acknowledging her authorship.
Lemma 4.5.18
(i) Let N = pq, m be as before, i.e. Ï†(N)|m, and deï¬ne the set X
as in (4.5.29). If x âˆˆX then there exists 0 â‰¤t < a such that gcd

x2tb âˆ’1,N

> 1 is
a non-trivial factor of N = pq.
(ii) The cardinality â™¯X â‰¥Ï†(N)/2.
Proof
(i) Put y = xb mod N. The Eulerâ€“Fermat theorem implies that xÏ†(N) â‰¡
1 mod N and hence y2a â‰¡1 mod N. Then
ordp(xb) and ordq(xb) are powers of 2.
As we know, ordp(xb) Ì¸= ordq(xb); say, ordp(xb) < ordq(xb). Then there exists 0 â‰¤
t < a such that
y2t â‰¡1 mod p, y2t Ì¸â‰¡1 mod q.
So, gcd

y2t âˆ’1,N

= p, as required.
(ii) By the CRT, there is a bijection
x âˆˆ
*
1,...,N
+
â†”

x mod p, x mod q

âˆˆ
*
1,..., p
+
Ã—
*
1,...,q
+
,
with the agreement that N â†”(p,q). Then it sufï¬ces to show that if we partition set
*
1,..., p
+
into subsets according to the value of ordp(xb), x âˆˆX, then each subset

4.5 Selected examples and problems from cryptography
473
has size â‰¤(pâˆ’1)/2. We will do this by exhibiting such a subset of size (pâˆ’1)/2.
Note that
Ï†(N)|2ab implies that there exists Î³ âˆˆ{1,..., pâˆ’1}
such that ordp(Î³b)is a power of 2.
In turn, the latter statement implies that
ordp(Î³Î´b)
% = ordp(Î³b),
Î´ odd,
< ordp(Î³b),
Î´ even.
Therefore,
*
Î³Î´b mod p : Î´ odd
+
is the required subset.
Our next example of a cipher is the Rabin, or Rabinâ€“Williams cryptosystem.
Here, again, one uses the factoring problem to provide security. For this system,
the relation with the factoring problem has been proved to be mutual: knowing
the solution to the factoring problem breaks the cryptosystem, and the ability of
breaking the cryptosystem leads to factoring. [That is not so in the case with the
RSA: it is not known whether breaking the RSA enables one to solve the factoring
problem.]
In the Rabin system the recipient user (Alice) chooses at random two large
primes, p and q, with
p = q = 3 mod 4.
(4.5.30)
Furthermore:
Aliceâ€™s public key is N = pq; her secret key is the pair (p,q);
Aliceâ€™s plaintext and ciphertext are numbers m = 0,1...,N âˆ’1,
and her encryption rule is EN(m) = c where c = m2 mod N.
(4.5.31)
To decrypt a ciphertext c addressed to her, Alice computes
mp = c(p+1)/4 mod p and mq = c(q+1)/4 mod q.
(4.5.32)
Then
Â±mp = c1/2 mod p and Â±mq = c1/2 mod q,
i.e. Â±mp and Â±mq are the square roots of c mod p and mod q, respectively. In fact,

Â±mp
2 = c(p+1)/2 = c(pâˆ’1)/2c =

Â±mp
pâˆ’1c = c mod p;
at the last step the Eulerâ€“Fermat theorem has been used. The argument for Â±mq is
similar. Then Alice computes, via Euclidâ€™s algorithm, integers u(p) and v(q) such
that
u(p)p+v(q)q = 1.

474
Further Topics from Information Theory
Finally, Alice computes
Â±r = Â±
	
u(p)pmq +v(q)qmp] mod N
and
Â±s = Â±
	
u(p)pmq âˆ’v(q)qmp] mod N.
These are four square roots of c mod N. The plaintext m is one of them. To
secure that she can identify the original plaintext, Alice may reduce the plaintext
space P, allowing only plaintexts with some special features (like the property
that their ï¬rst 32 and last 32 digits are repetitions of each other), so that it becomes
unlikely that more than one square root has this feature. However, such a measure
may result in a reduced difï¬culty of breaking the cipher as it will be not always
true that the â€˜reducedâ€™ problem is equivalent to factoring.
I have often admired the mystical way of Pythagoras
and the secret magic of numbers.
Thomas Browne (1605â€“1682), English author who wrote on
medicine, religion, science and the esoteric
Example 4.5.19
Alice uses prime numbers p = 11 and q = 23. Then N = 253.
Bob encrypts the message m = 164, with
c = m2 mod N = 78.
Alice calculates mp = 1,mq = 3, u(p) = âˆ’2, v(q) = 1. Then Alice computes
r = Â±[u(p)pmq +v(q)qmp] mod N = 210 and 43
s = Â±[u(p)pmq âˆ’v(q)qmp] mod N = 164 and 89
and ï¬nds out the message m = 164 among the solutions: 1642 = 78 mod 253.
We continue with the Difï¬eâ€“Hellman key exchange scheme. Difï¬e and Hellman
proposed a protocol enabling a pair of users to exchange secret keys via insecure
channels. The Difï¬eâ€“Hellman scheme is not a public-key cryptosystem but its im-
portance has been widely recognised since it forms a basis for the ElGamal signa-
ture cryptosystem.
The Difï¬eâ€“Hellman protocol is related to the discrete logarithm problem (DLP):
we are given a prime number p, ï¬eld Fp with the multiplicative group Fâˆ—
p â‰ƒZpâˆ’1
and a generator Î³ of Fâˆ—
p (i.e. a primitive element in Fâˆ—
p). Then, for all b âˆˆFâˆ—
p, there
exists a unique Î± âˆˆ{0,1,..., pâˆ’2} such that
b = Î³Î± mod p.
(4.5.33)

4.5 Selected examples and problems from cryptography
475
Then Î± is called the discrete logarithm, mod p, of b to base Î³; some authors write
Î± = dlogÎ³b mod p. Computing discrete logarithms is considered a difï¬cult prob-
lem: no efï¬cient (polynomial) algorithm is known, although there is no proof that
it is indeed a non-polynomial problem. [In an additive cyclic group Z/(nZ), the
DLP becomes b = Î³Î± mod n and is solved by the Euclid algorithm.]
The Difï¬eâ€“Hellman protocol allows Alice and Bob to establish a common secret
key using ï¬eld tables for Fp, for a sufï¬cient quantity of prime numbers p. That is,
they know a primitive element Î³ in each of these ï¬elds. They agree to ï¬x a large
prime number p and a primitive element Î³ âˆˆFp. The pair (p,Î³) may be publicly
known: Alice and Bob can ï¬x p and Î³ through the insecure channel.
Next, Alice chooses a âˆˆ{0,1,..., pâˆ’2} at random, computes
A = Î³a mod p
and sends A to Bob, keeping a secret. Symmetrically, Bob chooses b âˆˆ{0,1,...,
pâˆ’2} at random, computes
B = Î³b mod p
and sends B to Alice keeping b secret. Then
Alice computes Ba mod p and Bob computes Ab mod p,
and their secret key is the common value
K = Î³ab = Ba = Ab mod p.
The attacker may intercept p, Î³, A and B but knows
neither a = dlogÎ³A mod p nor b = dlogÎ³B mod p.
If the attacker can ï¬nd discrete logarithms mod p then he can break the secret
key: this is the only known way to do so. The opposite question â€“ solving the
discrete logarithm problem if he is able to break the protocol â€“ remains open (it is
considered an important problem in public key cryptography).
However, like previously discussed schemes, the Difï¬eâ€“Hellman protocol has a
particular weak point: it is vulnerable to the man in the middle attack. Here, the
attacker uses the fact that neither Alice nor Bob can verify that a given message re-
ally comes from the opposite party and not from a third party. Suppose the attacker
can intercept all messages between Alice and Bob. Suppose he can impersonate
Bob and exchange keys with Alice pretending to be Bob and at the same time im-
personate Alice and exchange keys with Bob pretending to be Alice. It is necessary
to use electronic signatures to distinguish this forgery.

476
Further Topics from Information Theory
We conclude Section 4.5 with the ElGamal cryptosystem based on electronic
signatures. The ElGamal cipher can be considered a development of the Difï¬eâ€“
Hellman protocol. Both schemes are based on the difï¬culty of the discrete log-
arithm problem (DLP). In the ElGamal system, a recipient user, Alice, selects a
prime number p and a primitive element Î³ âˆˆFp. Next, she chooses, at random, an
exponent a âˆˆ{0,..., pâˆ’2}, computes
A = Î³a mod p
and announces/broadcasts
the triple (p,Î³,A), her public key.
At the same time, she keeps in secret
exponent a, her private key.
Aliceâ€™s plaintext set P is numbers 0,1,..., pâˆ’1.
Another user Bob, wishing to send a message to Alice and knowing triple
(p,Î³,A), chooses, again at random, an exponent b âˆˆ{0,1,..., p âˆ’2}, and com-
putes
B = Î³b mod p.
Then Bob lets Alice know B (which he can do by broadcasting value B). The value
B will play the role of Bobâ€™s â€˜signatureâ€™. In contrast, the value b of Bobâ€™s exponent
is kept secret.
Now, to send to Alice a message m âˆˆ{0,1,..., p âˆ’1}, Bob encrypts m by the
pair
Eb(m) = (B,c) where c = Abm mod p.
That is, Bobâ€™s ciphertext consists of two components: the encrypted message c and
his signature B.
Clearly, values A and B are parts of the Difï¬eâ€“Hellman protocol; in this sense
the latter can be considered as a part of the ElGamal cipher. Further, the encrypted
message c is the product of m by Ab, the factor combining part A of Aliceâ€™s public
key and Bobâ€™s exponent b.
When Alice receives the ciphertext (B,c) she uses her secret key a. Namely,
she divides c by Ba mod p. A convenient way is to calculate x = p âˆ’1 âˆ’a: as
1 â‰¤a â‰¤p âˆ’2, the value x also satisï¬es 1 â‰¤x â‰¤p âˆ’2. Then Alice decrypts c by
Bxc mod p. This yields the original message m, since
Bxc = Î³b(pâˆ’1âˆ’a)Abm =

Î³ pâˆ’1b
Î³aâˆ’bAbm = Aâˆ’bAbm = m mod p.
Example 4.5.20
With p = 37, Î³ = 2 and a = 12 we have
A = Î³a mod p = 26

4.5 Selected examples and problems from cryptography
477
and Aliceâ€™s public key is (p = 37,Î³ = 2,A = 26), her plaintexts are 0,1,...,36 and
private key a = 12. Assume Bob has chosen b = 32; then
B = 232 mod 37 = 4.
Suppose Bob wants to send m = 31. He encrypts m by
c = Abm mod p = (26)32m mod 37 = 10Ã—31 mod 37 = 14.
Alice decodes this message as 232 = 7 and 724 = 26 mod 37,
14Ã—232(37âˆ’12âˆ’1) mod 37 = 14Ã—724 = 14Ã—26 mod 37 = 31.
Worked Example 4.5.21
Suppose that Alice wants to send the message â€˜todayâ€™
to Bob using the ElGamal encryption. Describe how she does this using the prime
p = 15485863, Î³ = 6 a primitive root mod p, and her choice of b = 69. Assume
that Bob has private key a = 5. How does Bob recover the message using the
Mathematica program?
Solution Bob has public key (15485863,6,7776), which Alice obtains. She con-
verts the English plaintext using the alphabet order to the numerical equivalent:
19,14,3,0,24. Since 265 < p < 266, she can represent the plaintext message as a
single 5-digit base 26 integer:
m = 19Ã—264 +14Ã—263 +3Ã—262 +0Ã—26+24 = 8930660.
Now she computes Î³b = 669 = 13733130 mod 15485863, then
mÎ³ab = 8930660Ã—777669 = 4578170 mod 15485863.
Alice sends c = (13733130,4578170) to Bob. He uses his private key to compute
(Î³b)pâˆ’1âˆ’a = 1373313015485863âˆ’1âˆ’5 = 2620662 mod 15485863
and
(Î³)âˆ’amÎ³ab = 2620662Ã—4578170 = 8930660 mod 15485863,
and converts the message back to the English plaintext.
Worked Example 4.5.22
(a) Describe the Rabinâ€“Williams scheme for coding
a message x as x2 modulo a certain N. Show that, if N is chosen appropriately,
breaking this code is equivalent to factorising the product of two primes.
(b) Describe the RSA system associated with a public key e, a private key d and
the product N of two large primes.
Give a simple example of how the system is vulnerable to a homomorphism
attack. Explain how a signature system prevents such an attack. Explain how to
factorise N when e, d and N are known.

478
Further Topics from Information Theory
Solution (a) Fix two large primes p,q â‰¡âˆ’1 mod 4 which forms a private key; the
broadcasted public key is the product N = pq. The properties used are:
(i) If p is a prime, the congruence a2 â‰¡d mod p has at most two solutions.
(ii) For a prime p = âˆ’1 mod 4, i.e. p = 4kâˆ’1, if the congruence a2 â‰¡c mod p has
a solution then a â‰¡c(p+1)/4 mod p is one solution and a â‰¡âˆ’c(p+1)/4 mod p is
another solution. [Indeed, if c â‰¡a2 mod p then, by the Eulerâ€“Fermat theorem,
c2k = a4k = a(pâˆ’1)+2 = a2 mod p, implying ck = Â±a.]
The message is a number m from M = {0,1,...,Nâˆ’1}. The encrypter (Bob) sends
(broadcasts)  m = m2 mod N. The decrypter (Alice) uses property (ii) to recover the
two possible values of m mod p and two possible values of m mod q. The CRT then
yields four possible values for m: three of them would be incorrect and one correct.
So, if one can factorise N then the code would be broken. Conversely, suppose
that we can break the code. Then we can ï¬nd all four distinct square roots u1, u2,
u3, u4 mod N for a general u. (The CRT plus property (i) shows that u has zero
or four square roots unless it is a multiple of p and q.) Then ujuâˆ’1 (calculable via
Euclidâ€™s algorithm) gives rise to the four square roots, 1, âˆ’1, Îµ1 and Îµ2, of 1 mod N,
with
Îµ1 â‰¡1 mod p, Îµ1 â‰¡âˆ’1 mod q
and
Îµ2 â‰¡âˆ’1 mod p, Îµ2 â‰¡1 mod q.
By interchanging p and q, if necessary, we may suppose we know Îµ1. As Îµ1 âˆ’1
is divisible by p and not by q, the gcd(Îµ1 âˆ’1,N) = p; that is, p can be found by
Euclidâ€™s algorithm. Then q can also be identiï¬ed.
In practice, it can be done as follows. Assuming that we can ï¬nd square roots
mod N, we pick x at random and solve the congruence x2 â‰¡y2 mod N. With prob-
ability 1/2, we have x Ì¸â‰¡Â±y mod N. Then gcd(xâˆ’y,N) is a non-trivial factor of N.
We repeat the procedure until we identify a factor; after k trials the probability of
success is 1âˆ’2âˆ’k.
(b) To deï¬ne the RSA cryptosystem let us randomly choose large primes p and q.
By Fermatâ€™s little theorem,
xpâˆ’1 â‰¡1 mod p, xqâˆ’1 â‰¡1 mod q.
Thus, by writing N = pq and Î»(N) = lcm(pâˆ’1,qâˆ’1), we have
xÎ»(N) â‰¡1 mod N,
for all integers x coprime to N.

4.5 Selected examples and problems from cryptography
479
Next, we choose e randomly. Either Euclidâ€™s algorithm will reveal that e is not
co-prime to Î»(N) or we can use Euclidâ€™s algorithm to ï¬nd d such that
de â‰¡1 mod Î»(N).
With a very high probability a few trials will give appropriate d and e.
We now give out the value e of the public key and the value of N but keep secret
the private key d. Given a message m with 1 â‰¤m â‰¤N âˆ’1, it is encoded as the
integer c with
1 â‰¤c â‰¤N âˆ’1 and c â‰¡me mod N.
Unless m is not co-prime to N (an event of negligible probability), we can decode
by observing that
m â‰¡mde â‰¡cd mod N.
As an example of a homomorphism attack, suppose the system is used to trans-
mit a number m (dollars to be paid) and someone knowing this replaces the coded
message c by c2. Then
(c2)d â‰¡m2de â‰¡m2
and the recipient of the (falsiï¬ed) message believes that m2 dollars are to be paid.
Suppose that a signature B(m) is also encoded and transmitted, where B is a
many-to-one function with no simple algebraic properties. Then the attack above
will produce a message and signature which do not correspond, and the recipient
will know that the message was tampered with.
Suppose e, d and N are known. Since
deâˆ’1 â‰¡0

mod Î»(N)

and Î»(N) is even, deâˆ’1 is even. Thus deâˆ’1 = 2ab with b odd and a â‰¥1.
Choose x at random. Set z â‰¡xb mod N. By the CRT, z is a square root of
1 mod N = pq if and only if it is a square root of 1 mod p and q. As F2 is a ï¬eld,
x2 â‰¡1 mod p â‡”(xâˆ’1)(x+1) â‰¡0 mod p
â‡”(xâˆ’1) â‰¡0 mod p or (x+1) â‰¡0 mod p.
Thus 1 has four square roots w mod N satisfying w â‰¡Â±1 mod p and w â‰¡
Â±1 mod q. In other words,
w â‰¡1 mod N, w â‰¡âˆ’1 mod N,
w â‰¡w1 mod N with w1 â‰¡1 mod p and w1 â‰¡âˆ’1 mod q
or
w â‰¡w2 (mod N) with w2 â‰¡âˆ’1 mod p and w1 â‰¡1 mod q.

480
Further Topics from Information Theory
Now z (the square root of 1 mod N) cannot satisfy z â‰¡1 mod N. If w â‰¡
âˆ’1 mod N, we are unlucky and try again. Otherwise we know that z + 1 is not
congruent to 0 mod N but is divisible by one of the two prime factors of N. Apply-
ing Euclidâ€™s algorithm yields the common factor. Having found one prime factor,
we can ï¬nd the other one by division or by looking at zâˆ’1.
Since square roots of 1 are algebraically indistinguishable, the probability of this
methodâ€™s failure tends to 0 rapidly with the number of trials.
4.6 Additional problems for Chapter 4
Problem 4.1
(a) Let (Nt)tâ‰¥0 be a Poisson process of rate Î» > 0 and p âˆˆ(0,1).
Suppose that each jump in (Nt) is counted as type one with probability p and type
two with probability 1âˆ’p, independently for different jumps and independently of
the Poisson process. Let M(1)
t
be the number of type-one jumps and M(2)
t
= Nt âˆ’
M(1)
t
the number of type-two jumps by time t. What is the joint distribution of the
pair of processes (M(1)
t
)tâ‰¥0 and (M(2)
t
)tâ‰¥0? What if we ï¬x probabilities p1,..., pm
with p1 +Â·Â·Â·+ pm = 1 and consider m types instead of two?
(b) A person collects coupons one at a time, at jump times of a Poisson process
(Nt)tâ‰¥0 of rate Î». There are m types of coupons, and each time a coupon of type j
is obtained with probability pj, independently of the previously collected coupons
and independently of the Poisson process. Let T be the ï¬rst time when a complete
set of coupon types is collected. Show that
P(T < t) =
m
âˆ
j=1
(1âˆ’eâˆ’p jÎ»t) .
(4.6.1)
Let L = NT be the total number of coupons collected by the time the complete
set of coupon types is obtained. Show that Î»ET = EL. Hence, or otherwise, deduce
that EL does not depend on Î».
Solution Part (a) directly follows from the deï¬nition of a Poisson process.
(b) Let Tj be the time of the ï¬rst collection of a type j coupon. Then Tj âˆ¼Exp(pjÎ»),
independently for different j. We have
T = max
	
T1,...,Tm

,
and hence
P(T < t) = P

max
	
T1,...,Tm

< t

=
m
âˆ
j=1
P(Tj < t) =
m
âˆ
j=1

1âˆ’eâˆ’p jÎ»t
.

4.6 Additional problems for Chapter 4
481
Next, observe that the random variable L counts the jumps in the original Poisson
process (Nt) until the time of collecting a complete set of coupon types. That is:
T =
L
âˆ‘
i=1
Si,
where S1,S2,... are the holding times in (Nt), with Sj âˆ¼Exp(Î»), independently for
different j. Then
E(T|L = n) = nES1 = nÎ» âˆ’1.
Moreover, L is independent of the random variables S1,S2,.... Thus,
ET = âˆ‘
nâ‰¥m
P(L = n)E

T|L = n

= ES1 âˆ‘
nâ‰¥m
nP(L = n) = Î» âˆ’1EL.
But
Î»ET = Î»
0 âˆ
0 P(T > t)dt
= Î»
0 âˆ
0

1âˆ’
m
âˆ
j=1

1âˆ’eâˆ’p jÎ»t
dt
=
0 âˆ
0

1âˆ’
m
âˆ
j=1

1âˆ’eâˆ’p jt

dt,
and the RHS does not depend on Î».
Equivalently, L is identiï¬ed as the number of collections needed for collecting
a complete set of coupons when collections occur at positive integer times t =
1,2,..., with probability pj of obtaining a coupon of type j, regardless of the results
of previous collections. In this construction, Î» does not ï¬gure, so the mean EL does
not depend on Î» (as, in fact, the whole distribution of L).
Problem 4.2
Queuing systems are discussed in detail in PSE II. We refer to this
topic occasionally as they provide a rich source of examples in point processes.
Consider a system of k queues in series, each with inï¬nitely many servers, in which,
for i = 1,...,k âˆ’1, customers leaving the ith queue immediately arrive at the (i+
1)th queue. Arrivals to the ï¬rst queue form a Poisson process of rate Î». Service
times at the ith queue are all independent with distribution F, and independent of
service times at other queues, for all i. Assume that initially the system is empty
and write Vi(t) for the number of customers at queue i at time t â‰¥0. Show that
V1(t),...,Vk(t) are independent Poisson random variables.
In the case F(t) = 1âˆ’eâˆ’Î¼t show that
EVi(t) = Î»
Î¼ P(Nt â‰¥i),
t â‰¥0,
i = 1,...,k,
(4.6.2)
where (Nt)tâ‰¥0 is a Poisson process of rate Î¼.

482
Further Topics from Information Theory
Suppose now that arrivals to the ï¬rst queue stop at time T. Determine the mean
number of customers at the ith queue at each time t â‰¥T.
Solution We apply the product theorem to the Poisson process of arrivals with
random vectors Yn = (S1
n,...,Sk
n) where Si
n is the service time of the nth customer
at the ith queue. Then
Vi(t) = the number of customers in the ith queue at time t
=
âˆ
âˆ‘
n=1
1

the nth customer arrived in the ï¬rst queue at
time Jn is in the ith queue at time t

=
âˆ
âˆ‘
n=1
1

Jn > 0, S1
n,...,Sk
n â‰¥0,
Jn +S1
n +Â·Â·Â·+Siâˆ’1
n
< t < Jn +S1
n +Â·Â·Â·+Si
n

=
âˆ
âˆ‘
n=1
1

Jn,(S1
n,...,Sk
n)

âˆˆAi(t)

= M(Ai(t)).
Here (Jn : n âˆˆN) denote the jump times of a Poisson process of rate Î», and the
measures M and Î½ on (0,âˆ)Ã—Rk
+ are deï¬ned by
M(A) =
âˆ
âˆ‘
n=1
1

(Jn,Yn) âˆˆA

, A âŠ‚(0,âˆ)Ã—Rk
+
and
Î½

(0,t]Ã—B

= Î»tÎ¼(B).
The product theorem states that M is a Poisson random measure on (0,âˆ) Ã— Rk
+
with intensity measure Î½. Next, the set Ai(t) âŠ‚(0,âˆ)Ã—Rk
+ is deï¬ned by
Ai(t) =
*
(Ï„,s1,...,sk) : 0 < Ï„ < t, s1,...,sk â‰¥0
and Ï„ +s1 +Â·Â·Â·+siâˆ’1 â‰¤t < Ï„ +s1 +Â·Â·Â·+si+
=
'
(Ï„,s1,...,sk) : 0 < Ï„ < t, s1,...,sk â‰¥0
and
iâˆ’1
âˆ‘
l=1
sl â‰¤t âˆ’Ï„ <
i
âˆ‘
l=1
sl
@
.
Sets Ai(t) are pairwise disjoint for i = 1,...,k (as t âˆ’Ï„ can fall between subse-
quent partial sums
iâˆ’1
âˆ‘
l=1
sl and
i
âˆ‘
l=1
sl only once). So, the random variables Vi(t) are
independent Poisson.

4.6 Additional problems for Chapter 4
483
A direct veriï¬cation is through the joint MGF. Namely, let Nt âˆ¼Po(Î»t) be the
number of arrivals at the ï¬rst queue by time t. Then write
MV1(t),...,Vk(t)(Î¸1,...,Î¸k) = Eexp (Î¸1V1(t)+Â·Â·Â·+Î¸kVk(t))
= E

Eexp
 k
âˆ‘
i=1
Î¸iVi(t)
Nt; J1,...,JNt

.
In turn, given n = 1,2,... and points 0 < Ï„1 < Â·Â·Â· < Ï„n < t, the conditional expec-
tations is
Eexp

k
âˆ‘
i=1
Î¸iVi(t)
Nt = n; J1 = Ï„1,...,Jn = Ï„n

= Eexp
 k
âˆ‘
i=1
Î¸i
n
âˆ‘
j=1
1

Ï„ j,(S1
j,...,Sk
j)

âˆˆAi(t)

= Eexp
 n
âˆ‘
j=1
k
âˆ‘
i=1
Î¸i1

Ï„ j,(S1
j,...,Sk
j)

âˆˆAi(t)

=
n
âˆ
j=1
Eexp

k
âˆ‘
i=1
Î¸i1

Ï„ j,(S1
j,...,Sk
j)

âˆˆAi(t)

.
Next, perform summation over n and integration over Ï„1,...,Ï„n:
E

Eexp
 k
âˆ‘
i=1
Î¸iVi(t)
Nt; J1,...,JNt

=
âˆ
âˆ‘
n=1
Î» neâˆ’Î»t
t0
0
Ï„n
0
0
Â·Â·Â·
Ï„2
0
0
Ã—
n
âˆ
j=1
Eexp

k
âˆ‘
i=1
Î¸i1

Ï„ j,(S1
j,...,Sk
j)

âˆˆAi(t)

dÏ„1 Â·Â·Â·dÏ„nâˆ’1dÏ„n
=
âˆ
âˆ‘
n=1
Î» n
n! eâˆ’Î»t
0 t
0 Eexp

k
âˆ‘
i=1
Î¸i1

Ï„,(S1,...,Sk)

âˆˆAi(t)

dÏ„
n
= exp

Î»
0 t
0

Eexp

k
âˆ‘
i=1
Î¸i1

Ï„,(S1,...,Sk)

âˆˆAi(t)

âˆ’1

dÏ„

= exp

Î»
0 t
0
k
âˆ‘
i=1
P

Ï„,(S1,...,Sk)

âˆˆAi(t)

eÎ¸i âˆ’1

dÏ„

=
k
âˆ
i=1
exp


eÎ¸i âˆ’1

Î»
0 t
0 P

iâˆ’1
âˆ‘
l=1
Sl < t âˆ’Ï„ <
i
âˆ‘
l=1
Sl

dÏ„

.
By the uniqueness of a random variable with a given MGF, this implies that
Vi(t) âˆ¼Po

Î»
0 t
0 P

iâˆ’1
âˆ‘
l=1
Sl < t âˆ’Ï„ <
i
âˆ‘
l=1
Sl

dÏ„

, independently.

484
Further Topics from Information Theory
If F(t) = 1 âˆ’eâˆ’Î¼t then partial sums S1,S1 + S2,... mark the subsequent points
of a Poisson process ( Ns) of rate Î¼. In this case, EVi(t) = Î½(Ai(t)) equals
Î»
t0
0
P

iâˆ’1
âˆ‘
l=1
Sl â‰¤t âˆ’Ï„ <
i
âˆ‘
l=1
Sl

dÏ„ = Î»
t0
0
P
 Ntâˆ’Ï„ = iâˆ’1

dÏ„
= Î»E
t0
0
1( Ns = iâˆ’1)ds = Î»
Î¼ P( Nt â‰¥i).
Finally, write Vi(t,T) for the number of customers in queue i at time t after closing
the entrance at time T. Then
EVi(t,T) = Î»
T
0
0
P( Ntâˆ’Ï„ = iâˆ’1)dÏ„ = Î»E
t0
tâˆ’T
1( Ns = iâˆ’1)ds
= Î»
Î¼
	
P( Nt â‰¥i)âˆ’P( Ntâˆ’T â‰¥i)

.
Problem 4.3
The arrival times of customers at a supermarket form a Poisson
process of rate Î». Each customer spends a random length of time, S, collecting
items to buy, where S has PDF ( f(s,t): s â‰¥0) for a customer arriving at time t.
Customers behave independently of one another. At a checkout it takes time g(S)
to buy the items collected. The supermarket has a policy that nobody should wait
at the checkout, so more tills are made available as required. Find
(i) the probability that the ï¬rst customer has left before the second has arrived,
(ii) the distribution of the number of checkouts in use at time T.
Solution (i) If J1 is the arrival time of the ï¬rst customer then J1 +S1 is the time he
enters the checkout till and J1 +S1 +g(S1) the time he leaves. Let J2 be the time of
arrival of the second customer. Then J1, J2 âˆ’J1 âˆ¼Exp(Î»), independently.
Then
P(S1 +g(S1) < J2 âˆ’J1) =
âˆ
0
0
dt1Î»eâˆ’Î»t1
âˆ
0
0
dt2Î»eâˆ’Î»t2
t2
0
0
ds1 f(s1,t1)1(s1 +g(s1) < t2)
=
âˆ
0
0
dt1Î»eâˆ’Î»t1
âˆ
0
0
ds1 f(s1,t1)
âˆ
0
s1+g(s1)
dt2Î»eâˆ’Î»t2
=
âˆ
0
0
dt1Î»eâˆ’Î»t1
âˆ
0
0
ds1 f(s1,t1)eâˆ’Î»(s1+g(s1)).

4.6 Additional problems for Chapter 4
485
(ii) Let Nch
T be the number of checkouts used at time T. By the product theorem,
4.4.11, Nch
T âˆ¼Po(Î›(T)) where
Î›(T) = Î»
T
0
0
du
âˆ
0
0
ds f(s,u)1(u+s < T, u+s+g(s) > T)
= Î»
T
0
0
du
âˆ
0
0
ds f(s,u)1(T âˆ’g(s) < u+s < T).
In fact, if Narr
T âˆ¼Po(Î»T) is the number of arrivals by time T, then
Nch
T =
Narr
T
âˆ‘
i=1
1

Ji +Si < T < Ji +Si +g(Si)

,
and the MGF
Eexp

Î¸Nch
T

= E
	
E

exp

Î¸Nch
T

|Narr
T ; J1,...,JNarr
T

= eâˆ’Î»T
âˆ
âˆ‘
k=0
Î» k
0 T
0
0 tk
0 Â·Â·Â·
0 t2
0
k
âˆ
i=1
Eexp

Î¸1

ti +Si < T
< ti +Si +g(Si)

dt1 Â·Â·Â·dtk
= eâˆ’Î»T
âˆ
âˆ‘
k=0
Î» k
k!
0 T
0 Â·Â·Â·
0 T
0
k
âˆ
i=1
Eexp

Î¸1

ti +Si < T
< ti +Si +g(Si)

dt1 Â·Â·Â·dtk
= eâˆ’Î»T
âˆ
âˆ‘
k=0
Î» k
k!
0 T
0 Eexp

Î¸1

t +S < T < t +S+g(S)
k
= exp

Î»
0 T
0 E

exp

Î¸1

t +S < T < t +S+g(S)

âˆ’1

dt

= exp

Î»(eÎ¸ âˆ’1)
0 T
0 P

t +S < T < t +S+g(S)

dt

= exp

(eÎ¸ âˆ’1)Î»
0 T
0
0 âˆ
0 f(s,u)1

u+s < T < u+s+g(s)

dsdu

,
which veriï¬es the claim.
Problem 4.4
A library is open from 9am to 5pm. No student may enter after
5pm; a student already in the library may remain after 5pm. Students arrive at the
library in the period from 9am to 5pm in the manner of a Poisson process of rate
Î». Each student spends in the library a random amount of time, H hours, where

486
Further Topics from Information Theory
0 â‰¤H â‰¤8 is a random variable with PDF h and E[H] = 1. The periods of stay of
different students are IID random variables.
(a) Find the distribution of the number of students who leave the library between
3pm and 4pm.
(b) Prove that the mean number of students who leave between 3pm and 4pm is
E[min(1,(7âˆ’H)+)], where w+ denotes max[w,0].
(c) What is the number of students still in the library at closing time?
Solution The library is open from 9am to 5pm. Students arrive as a PP(Î»). The
problem is equivalent to an M/GI/âˆqueue (until 5pm, when the restriction of no
more arrivals applies, but for problems involving earlier times this is unimportant).
Denote by Jn the arrival time of the nth student using the 24 hour clock.
Denote by Hn the time the nth student spends in the library.
Again use the product theorem, 4.4.11, for the random measure on (0,8)Ã—(0,8)
with atoms (Jn,Yn), where (Jn : n âˆˆN) are the arrival times and (Yn : n âˆˆN) are
periods of time that students stay in the library. Deï¬ne measures on (0,âˆ) Ã— R+
by Î¼((0,t) Ã— B) = Î»tÎ¼(B), N(A) = âˆ‘
n 1((Jn,Hn)âˆˆA). Then N is a Poisson random
measure with intensity Î½([0,t]Ã—[0,y]) = Î»tF(y), where F(y) =
y1
0
h(x)dx (the time
t = 0 corresponds to 9am).
(a) Now, the number of students leaving the library between 3pm and 4pm (i.e.
6 â‰¤t â‰¤7) has a Poisson distribution Po(Î½(A)) where A = {(r,s) : s âˆˆ[0,7],r âˆˆ
[6âˆ’s,7âˆ’s] if s â‰¤6;r âˆˆ[0,7âˆ’s] if s > 6}. Here
Î½(A) =
8
0
0
Î»dF(r)
(7âˆ’r)+
0
(6âˆ’r)+
ds =
8
0
0
Î»
	
(7âˆ’r)+ âˆ’(6âˆ’r)+

dF(r).
So, the distribution of students leaving the library between 3pm and 4pm is Poisson
with rate = Î»
71
0
[(7âˆ’y)+ âˆ’(6âˆ’y)+]dF(r).
(b)
(7âˆ’y)+ âˆ’(6âˆ’y)+ =
â§
âª
â¨
âª
â©
0,
if y â‰¥7,
7âˆ’y,
if 6 â‰¤y â‰¤7,
1,
if y â‰¤6.

4.6 Additional problems for Chapter 4
487
The mean number of students leaving the library between 3pm and 4pm is
Î½(A) =
8
0
0
Î»[min(1,(7âˆ’r)+]dF(r) = Î»E[min(1,(7âˆ’H)+)]
as required.
(c) For students still to be there at closing time we require J +H â‰¥8, as H ranges
over [0,8], and J ranges over [8âˆ’H,8]. Let
B = {(t,x) : t âˆˆ[0,8],x âˆˆ[8âˆ’t,8]}.
So,
Î½(B) = Î»
8
0
0
dt
8
0
8âˆ’t
dF(x) = Î»
8
0
0
dF(x)
8âˆ’x
0
0
dt
= Î»
8
0
0
(8âˆ’x)dF(x) = 8Î»
8
0
0
dF(x)âˆ’Î»
8
0
0
xdF(x),
but
81
0
dF(x) = 1 and
81
0
xdF(x) = E[H] = 1 imply Î»E[H] = Î». Hence, the expected
number of students in the library at closing time is 7Î».
Problem 4.5
(i) Prove Campbellâ€™s theorem, i.e. show that if M is a Poisson
random measure on the state space E with intensity measure Î¼ and a : E â†’R is a
bounded measurable function, then
E[eÎ¸X] = exp
â¡
â£
0
E
(eÎ¸a(y) âˆ’1)Î¼(ddy)
â¤
â¦,
(4.6.3)
where X =
1
E
a(y)M(dy) (assume that Î» = Î¼(E) < âˆ).
(ii) Shots are heard at jump times J1,J2,... of a Poisson process with rate Î». The
initial amplitudes of the gunshots A1,A2,... âˆ¼Exp(2) are IID exponentially dis-
tributed with parameter 2, and the amplitutes decay linearly at rate Î±. Compute the
MGF of the total amplitude Xt at time t:
Xt = âˆ‘
n
An(1âˆ’Î±(t âˆ’Jn)+)1(Jnâ‰¤t);
x+ = x if x â‰¥0 and 0 otherwise.

488
Further Topics from Information Theory
Solution (i) Conditioned on M(E) = n, the atoms of M form a random sample
Y1,...,Yn with distribution 1
Î» Î¼, so
E[eÎ¸X | M(E) = n] = E
	
e
Î¸
n
âˆ‘
k=1
a(Yk)
=
â›
â
0
E
eÎ¸a(y)Î¼(dy)/Î»
â
â 
n
.
Hence,
E[eÎ¸X] = âˆ‘
n
E[eÎ¸X | M(E) = n]P(M(E) = n)
= âˆ‘
n
â›
â
0
E
eÎ¸a(y)Î¼(dy)/Î»
â
â 
n
eâˆ’Î»Î» n
n!
= exp
â›
â
0
E
(eÎ¸a(y) âˆ’1)Î¼(dy)
â
â .
(ii) Fix t and let E = [0,t] Ã— R+ and Î½ and M be such that Î½(ds,dx) =
2Î»eâˆ’2xdsdx, M(B) = âˆ‘
n 1{(Jn,An)âˆˆB}. By the product theorem M is a Poisson ran-
dom measure with intensity measure Î½. Set at(s,x) = x(1 âˆ’Î±(t âˆ’s))+, then
Xt =
1
E
at(s,x)M(ds,dx). So, by Campbellâ€™s theorem, for Î¸ < 2,
E[eÎ¸Xt] = exp
â›
â
0
E
(eÎ¸at(s,x) âˆ’1)Î½(ds,dx)
â
â 
= eâˆ’Î»t exp
â›
â2Î»
t0
0
âˆ
0
0
eâˆ’x(2âˆ’Î¸(1âˆ’Î±(tâˆ’s))+)dxds
â
â 
= eâˆ’Î»t exp
â›
â2Î»
t0
0
ds
1
2âˆ’Î¸(1âˆ’Î±(t âˆ’s))+
â
â 
= eâˆ’Î» min[t,1/Î±]2âˆ’Î¸ +Î¸Î± min[t,1/Î±]
2âˆ’Î¸
 2Î»
Î¸Î±
by splitting integral
1 t
0 =
1 tâˆ’1
Î±
0
+
1 t
tâˆ’1
Î± in the case t > 1
Î± .
Problem 4.6
Seeds are planted in a ï¬eld S âŠ‚R2. The random way they are sown
means that they form a Poisson process on S with density Î»(x,y). The seeds grow
into plants that are later harvested as a crop, and the weight of the plant at (x,y) has

4.6 Additional problems for Chapter 4
489
mean m(x,y) and variance v(x,y). The weights of different plants are independent
random variables. Show that the total weight Wof all the plants is a random variable
with ï¬nite mean
I1 =
0 0
S m(x,y)Î»(x,y)dxdy
and variance
I2 =
0 0
S
*
m(x,y)2 +v(x,y)
+
Î»(x,y)dxdy,
so long as these integrals are ï¬nite.
Solution Suppose ï¬rst that
Î¼ =
0
S Î»(x,y)dxdy
is ï¬nite. Then the number N of plants is ï¬nite and has the distribution Po(Î¼).
Conditional on N, their positions may be taken as independent random variables
(Xn,Yn), n = 1,...,N, with density Î»/Î¼ on S. The weights of the plants are then
independent, with
EW =
0
S m(x,y)Î»(x,y)Î¼âˆ’1dxdy = Î¼âˆ’1I1
and
EW 2 =
0
S
	
m(x,y)2 +v(x,y)

Î»(x,y)Î¼âˆ’1dxdy = Î¼âˆ’1I2,
where I1 and I2 are ï¬nite. Hence,
E

W|N

=
N
âˆ‘
n=1
Î¼âˆ’1I1 = NÎ¼âˆ’1I1
and
Var

W|N

=
N
âˆ‘
n=1

Î¼âˆ’1I2 âˆ’Î¼âˆ’2I2
1

= N

Î¼âˆ’1I2 âˆ’Î¼âˆ’2I2
1

.
Then
EW = ENÎ¼âˆ’1I1 = I1
and
VarW = E
	
Var

W|N

+Var
	
E

W|N

= Î¼

Î¼âˆ’1I2 âˆ’Î¼âˆ’2I2
1

+

VarN

Î¼âˆ’2I2
1 = I2,
as required.

490
Further Topics from Information Theory
If Î¼ = âˆ, we divide S into disjoint Sk on which Î» is integrable, then write W =
âˆ‘
k
W(k) where the harvests W(k) on Sk are independent, and use
EW = âˆ‘
k
EW(k) = âˆ‘
k
0
Sk
m(x,y)Î»(x,y)dxdy
=
0
Sm(x,y)Î»(x,y)dxdy
and similarly for VarW.
Problem 4.7
A line L in R2 not passing through the origin O can be deï¬ned by
its perpendicular distance p > 0 from O and the angle Î¸ âˆˆ[0,2Ï€) that the perpen-
dicular from O to L makes with the x-axis. Explain carefully what is meant by a
Poisson process of such lines L.
A Poisson process Î  of lines L has mean measure Î¼ given by
Î¼(B) =
0 0
B dpdÎ¸
(4.6.4)
for B âŠ†(0,âˆ)Ã—[0,2Ï€). A random countable set Î¦ âŠ‚R2 is deï¬ned to consist of all
intersections of pairs of lines in Î . Show that the probability that there is at least
one point of Î¦ inside the circle with centre O and radius r is less than
1âˆ’(1+2Ï€r)eâˆ’2Ï€r .
Is Î¦ a Poisson process?
Solution Suppose that Î¼ is a measure on the space L of lines in R2 not passing
through 0. A Poisson process with mean measure Î¼ is a random countable subset
Î  of L such that
(1) the number N(A) of points of Î  in a measurable subset A of L has distribution
Po(Î¼(A)), and
(2) for disjoint A1,...,An, the N(A j) are independent.
In the problem, the number N of lines which meet the disc D of centre 0 and radius
r equals the number of lines with p < r. It is Poisson with mean
0 r
0
0 2Ï€
0
dpdÎ¸ = 2Ï€r.
If there is at least one point of Î¦ in D then there must be at least two lines of Î 
meeting D, and this has probability
âˆ‘
nâ‰¥2
(2Ï€r)n
n!
eâˆ’2Ï€r = 1âˆ’(1+2Ï€r)eâˆ’2Ï€r.

4.6 Additional problems for Chapter 4
491
The probability of a point of Î¦ lying in D is strictly less than this, because there
may be two lines meeting D whose intersection lies outside D.
Finally, Î¦ is not a Poisson process, since it has with positive probability collinear
points.
Problem 4.8
Particular cases of the Poissonâ€“Dirichlet distribution for the ran-
dom sequence (p1, p2, p3,...) with parameter Î¸ appeared in PSE II the deï¬nition
is given below. Show that, for any polynomial Ï† with Ï†(0) = 0,
E
'
âˆ
âˆ‘
n=1
Ï†(pn)
@
= Î¸
0 1
0 Ï†(x)xâˆ’1(1âˆ’x)Î¸âˆ’1dx.
(4.6.5)
What does this tell you about the distribution of p1?
Solution The simplest way to introduce the Poissonâ€“Dirichlet distribution is to say
that p = (p1, p2,...) has the same distribution as (Î¾n/Ïƒ), where {Î¾n, n = 1,2,...}
are the points in descending order of a Poisson process on (0,âˆ) with rate Î¸xâˆ’1eâˆ’x,
and Ïƒ = âˆ‘
nâ‰¥1
Î¾n. By Campbellâ€™s theorem, Ïƒ is a.s. ï¬nite and has distribution Gam(Î¸)
(where Î¸ > 0 can be arbitrary) and is independent from the vector p = (p1, p2,...)
with
p1 â‰¥p2 â‰¥Â·Â·Â· , âˆ‘
nâ‰¥1
pn = 1, with probability 1.
Here Gam stands for the Gamma distribution; see PSE I, Appendix.
To prove (4.6.5), we can take pn = Î¾n/Ïƒ and use the fact that Ïƒ and p are inde-
pendent. For k â‰¥1,
E

âˆ‘
nâ‰¥1
Î¾ k
n

=
0 âˆ
0 xkÎ¸xâˆ’1eâˆ’xdx = Î¸Î“(k).
The left side equals
E

Ïƒk âˆ‘
nâ‰¥1
pk
n

= Î“(Î¸ +k)Î“(Î¸)âˆ’1E

âˆ‘
nâ‰¥1
pk
n

.
Thus,
E

âˆ‘
nâ‰¥1
pk
n

= Î¸Î“(k)Î“(Î¸)
Î“(k +Î¸)
= Î¸
0 1
0 xkâˆ’1(1âˆ’x)Î¸âˆ’1dx.
We see that the identity (4.6.5) holds for Ï†(x) = xk (with k â‰¥1) and hence by
linearity for all polynomials with Ï†(0) = 0.

492
Further Topics from Information Theory
Approximating step functions by polynomials shows that the mean number of
pn in an interval (a,b) (with 0 < a < b < 1) equals
Î¸
0 b
a xâˆ’1(1âˆ’x)Î¸âˆ’1dx.
If a > 1/2, there can be at most one such pn, so that p1 has the PDF
Î¸xâˆ’1(1âˆ’x)Î¸âˆ’1 on (1/2,1).
But this fails on (0,1/2), and the identity (4.6.5) does not determine the distribution
of p1 on this interval.
Problem 4.9
The positions of trees in a large forest can be modelled as a Pois-
son process Î  of constant rate Î» on R2. Each tree produces a random number of
seeds having a Poisson distribution with mean Î¼. Each seed falls to earth at a point
uniformly distributed over the circle of radius r whose centre is the tree. The po-
sitions of the different seeds relative to their parent tree, and the numbers of seeds
produced by a given tree, are independent of each other and of Î . Prove that, con-
ditional on Î , the seeds form a Poisson process Î âˆ—whose mean measure depends
on Î . Is the unconditional distribution of Î âˆ—that of a Poisson process?
Solution By a direct calculation, the seeds from a tree at X form a Poisson process
with rate
ÏX(x) =
'
Ï€âˆ’1râˆ’2,
|xâˆ’X| < r,
0,
otherwise.
Superposing these independent Poisson processes gives a Poisson process with rate
Î›Î (x) = âˆ‘
XâˆˆÎ 
ÏX(x);
it clearly depends on Î . The unrealistic assumption of a circular uniform distri-
bution is chosen to create no doubt about this dependence â€“ in this case Î  can be
reconstructed from the contours of Î›Î .
Here we meet for the ï¬rst time the doubly stochastic (Cox) processes, i.e. Pois-
son process with random intensity. The number of seeds in a bounded set Î› has
mean
EN(A) = EE
	
N(A)|Î 

= E
0
A Î›Î (x)dx

4.6 Additional problems for Chapter 4
493
and variance
VarN(A) = E

Var
	
N(A)|Î 

+Var

E
	
N(A)|Î 


= EN(A)+Var
	0
Î›Î (x)dx

> EN(A).
Hence, Î âˆ—is not a Poisson process.
Problem 4.10
A uniform Poisson process Î  in the unit ball of R3 is one whose
mean measure is Lebesgue measure (volume) on
B = {(x,y,z) âˆˆR3 : r2 = x2 +y2 +z2 â©½1}.
Show that
Î 1 = {r : (x,y,z) âˆˆÎ }
is a Poisson process on [0,1] and ï¬nd its mean measure. Show that
Î 2 = {(x/r,y/r,z/r) : (x,y,z) âˆˆÎ }
is a Poisson process on the boundary of B, whose mean measure is a multiple of
surface area. Are Î 1 and Î 2 independent processes?
Solution By the mapping theorem, Î 1 is Poisson, with expected number of points
in (a,b) equal to Î»Ã—(the volume of the shell with radii a and b), i.e.
Î»
4
3Ï€b3 âˆ’4
3Ï€a3

.
Thus, the mean measure of Î 1 has the PDF
4Î»Ï€r2 (0 < r < 1).
Similarly, the expected number of points of Î 2 in A âŠ†âˆ‚B equals
Î» Ã—(the conic volume from 0 to A) = 1
3Î» Ã—(the surface area of A).
Finally, Î 1 and Î 2 are not independent since they have the same number of points.
Problem 4.11
The points of Î  are coloured randomly either red or green, the
probability of any point being red being r, 0 < r < 1, and the colours of different
points being independent. Show that the red and the green points form independent
Poisson processes.

494
Further Topics from Information Theory
Solution If A âŠ†S has Î¼(A) < âˆthen write
N(A) = N1(A)+N2(A)
where N1 and N2 are the numbers of red and green points. Conditional on N(A) = n,
N1(A) has the binomial distribution Bin(n,r). Thus,
P

N1(A) = k, N2(A) = l

= P

N(A) = k +l

P

N1(A) = k|N(A) = k +l

= Î¼(A)k+leâˆ’Î¼(A)
(k +l)!
k +l
k

rk(1âˆ’r)l
= [rÎ¼(A)]keâˆ’rÎ¼(A)
k!
[(1âˆ’r)Î¼(A)]leâˆ’(1âˆ’r)Î¼(A)
l!
.
Hence, N1(A) and N2(A) are independent Poisson random variables with means
rÎ¼(A) and (1âˆ’r)Î¼(A), respectively.
If A1,A2,... are disjoint sets then the pairs
(N1(A1),N2(A1)), (N1(A2),N2(A2)),...
are independent, and hence

N1(A1),N1(A2),...

and

N2(A1),N2(A2),...

are two independent sequences of independent random variables. If Î¼(A) = âˆthen
N(A) = âˆa.s., and since r > 0 and 1âˆ’r > 0, there are a.s. inï¬nitely many red and
green points in A.
Problem 4.12
A model of a rainstorm falling on a level surface (taken to be the
plane R2) describes each raindrop by a triple (X,T,V), where X âˆˆR2 is the hori-
zontal position of the centre of the drop, T is the instant at which the drop hits the
plane, and V is the volume of water in the drop. The points (X,T,V) are assumed
to form a Poisson process on R4 with a given rate Î»(x,t,v). The drop forms a wet
circular patch on the surface, with centre X and a radius that increases with time,
the radius at time (T +t) being a given function r(t,V). Find the probability that
a point Î¾ âˆˆR2 is dry at time Ï„, and show that the total rainfall in the storm has
expectation
0
R4 vÎ»(x,t,v)dxdtdv
if this integral converges.

4.6 Additional problems for Chapter 4
495
Solution Thus, Î¾ âˆˆR2 is wet iff there is a point of Î  with t < Ï„ and
||X âˆ’Î¾|| < r(Ï„ âˆ’t,V)
(there no problem about whether or not the inequality is strict since the difference
involves events of zero probability). The number of points of Î  satisfying these
two inequalities is Poisson, with mean
Î¼ =
0
Î»(x,t,v)1

t < Ï„,||xâˆ’Î¾|| < r(Ï„ âˆ’t,v)

dxdtdv.
Hence, the probability that Î¾ is dry is eâˆ’Î¼ (or 0 if Î¼ = +âˆ). Finally, the formula
for the expected total rainfall,
âˆ‘
(X,T,V)âˆˆÎ 
V,
is a direct application of Campbellâ€™s theorem.
Problem 4.13
Let M be a Poisson random measure on E = RÃ—[0,Ï€) with con-
stant intensity Î». For (x,Î¸) âˆˆE, denote by l(x,Î¸) the line in R2 obtained by rotat-
ing the line {(x,y) : y âˆˆR} through an angle Î¸ about the origin.
Consider the line process L = M â—¦lâˆ’1.
(i) What is the distribution of the number of lines intersecting the disk Da = {z âˆˆ
R2 : | z |â‰¤a}?
(ii) What is the distribution of the distance from the origin to the nearest line?
(iii) What is the distribution of the distance from the origin to the kth nearest line?
Solution (i) A line intersects the disk Da = {z âˆˆR2 : |z| â‰¤a} if and only if its
representative point (x,Î¸) lies in (âˆ’a,a)Ã—[0,Ï€). Hence,
â™¯of lines intersecting Da âˆ¼Po(2aÏ€Î»).
(ii) Let Y be the distance from the origin to the nearest line. Then
P(Y â‰¥a) = P

M((âˆ’a,a)Ã—[0,Ï€)) = 0

= exp(âˆ’2aÎ»Ï€),
i.e. Y âˆ¼Exp(2Ï€Î»).
(iii) Let Y1,Y2,... be the distances from the origin to the nearest line, the second
nearest line, and so on. Then the Yi are the atoms of the PRM N on R+ which is
obtained from M by the projection (x,Î¸) â†’|x|. By the mapping theorem, N is the
Poisson process on R+ of rate 2Ï€Î». Hence, Yk âˆ¼Gam(k,2Î»Ï€), as Yk = S1+Â·Â·Â·+Sk
where Si âˆ¼Exp(2Ï€Î»), independently.

496
Further Topics from Information Theory
Problem 4.14
One wishes to transmit one of M equiprobable distinct messages
through a noisy channel. The jth message is encoded by the sequence of scalars
a jt (t = 1,2,...,n) which, after transmission, is received as ajt +Îµt (t = 1,2,...,n).
Here the noise random variables Îµt are independent and normally distributed, with
zero mean and with time-dependent variance Var Îµt = vt.
Find an inference rule at the receiver for which the average probability that the
message value is incorrectly inferred has the upper bound
P

error

â‰¤1
M
âˆ‘
1â‰¤jÌ¸=kâ‰¤M
exp(âˆ’d jk/8),
(4.6.6)
where
d jk = âˆ‘
1â‰¤tâ‰¤n
(a jt âˆ’akt)2>
vt.
Suppose that M = 2 and that the transmitted waveforms are subject to the power
constraint
âˆ‘
1â‰¤tâ‰¤n
a2
jt â‰¤K, j = 1,2. Which of the two waveforms minimises the prob-
ability of error?
[Hint: You may assume validity of the bound P(Z â‰¥a) â‰¤exp(âˆ’a2/2), where Z is
a standard N(0,1) random variable.]
Solution Let f j = fch(y|X = a j) be the PDF of receiving a vector y given that a
â€˜waveformâ€™ A j = (a jt) was transmitted. Then
P(error) â‰¤1
M âˆ‘
j âˆ‘
k:kÌ¸= j
P({y : fk(y) â‰¥f j(y)|X = A j}).
Let V be the diagonal matrix with the diagonal elements vj. In the present case,
f j = C exp

âˆ’1
2
n
âˆ‘
t=1
(yt âˆ’a jt)2/vt

= C exp

âˆ’1
2(Y âˆ’A j)TV âˆ’1(Y âˆ’A j)

.
Then if X = A j and Y = A j +Îµ we have
log fk âˆ’log f j = âˆ’1
2(A j âˆ’Ak +Îµ)TV âˆ’1(A j âˆ’Ak +Îµ)+ 1
2ÎµTV âˆ’1Îµ
= âˆ’1
2d jk âˆ’(A j âˆ’(Ak)TV âˆ’1Îµ)
= âˆ’1
2d jk +
$
d jkZ

4.6 Additional problems for Chapter 4
497
where Z âˆ¼N(0,1). Thus, by the hint, (4.6.6) follows:
P( fk â‰¥f j) = P(Z >
$
d jk/2) â‰¤eâˆ’d jk/8.
In the case M = 2 we have to maximise
d12 = (A1 âˆ’A2)TV âˆ’1(A1 âˆ’A2) = âˆ‘
1â‰¤tâ‰¤n
(a1t âˆ’a2t)2/vt
subject to
âˆ‘
t
a2
jt â‰¤K or (a)T
j (a) j â‰¤K, j = 1,2.
By Cauchyâ€“Schwarz,
(A1 âˆ’A2)TV âˆ’1(A1 âˆ’A2) â‰¤
S
AT
1V âˆ’1A1 +
S
AT
2V âˆ’1A2
2
(4.6.7)
with equality holding when A1 = constA2. Further, in our case V is diagonal, and
(4.6.7) is maximised when AT
j A j = K, j = 1,2,... We conclude that
a1t = âˆ’a2t = bt
with bt non-zero only for t such that vt is minimal, and âˆ‘t b2
t = K.
Problem 4.15
A random variable Y is distributed on the non-negative integers.
Show that the maximum entropy of Y, subject to EY â‰¤M, is
âˆ’MlogM +(M +1)log(M +1)
attained by a geometric distribution with mean M.
A memoryless channel produces outputs Y from non-negative integer-valued
inputs X by
Y = X +Îµ,
where Îµ is independent of X, P(Îµ = 1) = p, P(Îµ = 0) = 1âˆ’p = q and inputs X are
constrained by EX â‰¤q. Show that, provided p â‰¤1/3, the optimal input distribution
is
P(X = r) = (1+ p)âˆ’1

1
2r+1 âˆ’
âˆ’p
q
r+1
, r = 0,1,2,...,
and determine the capacity of the channel.
Describe, very brieï¬‚y, the problem of determining the channel capacity if p >
1/3.

498
Further Topics from Information Theory
Solution First, consider the problem
maximise h(Y) = âˆ’âˆ‘
yâ‰¥0
py log py subject to
â§
âª
â¨
âª
â©
py â‰¥0,
âˆ‘y py = 1,
âˆ‘y ypy = M.
The solution, found by using Lagrangian multipliers, is
py = (1âˆ’Î»)Î» y, y = 0,1,..., with M =
Î»
1âˆ’Î» , or Î» =
M
M +1,
with the optimal value
h(Y) = (M +1)log(M +1)âˆ’MlogM.
Next, for g(m) = (m+1)log(m+1)âˆ’mlogm,
gâ€²(m) = log(m+1)âˆ’logm > 0,
implying that h(Y) â†—when M â†—. Therefore, the maximiser and the optimal value
are the same for EY â‰¤M, as required.
Now, the capacity C = sup
	
h(Y) âˆ’h(Y|X)] = h(Y) âˆ’h(Îµ), and the condition
EX â‰¤q implies that EY â‰¤q+EÎµ = q+ p = 1. With h(Îµ) = âˆ’plog pâˆ’qlogq, we
want Y geometric, with M = 1, Î» = 1/2, yielding
C = 2log2+ plog p+qlogq = log

4ppqq).
Then
EzX = EzY
EzÎµ =
 1âˆ’Î»
1âˆ’Î»z
>
(pz+q) =
1
(2âˆ’z)(q+ pz)
= (2âˆ’z)âˆ’1 + p(q+ pz)âˆ’1
1+ p
=
1
1+ p

âˆ‘

1/2
1+rzr +

p/q
âˆ‘

âˆ’p/q
rzr
.
If p > 1/3 then p/q > 1/2 and the alternate probabilities become negative,
which means that there is no distribution for X giving an optimum for Y. Then
we would have to maximise
âˆ’âˆ‘
y
py log py, subject to py = pÏ€yâˆ’1 +qÏ€y,
where Ï€y â‰¥0, âˆ‘y Ï€y = 1 and âˆ‘y yÏ€y â‰¤q.
Problem 4.16
Assuming the bounds on channel capacity asserted by the second
coding theorem, deduce the capacity of a memoryless Gaussian channel.
A channel consists of r independent memoryless Gaussian channels, the noise in
the ith channel having variance vi, i = 1,2,...,n. The compound channel is subject

4.6 Additional problems for Chapter 4
499
to an overall power constraint E

âˆ‘
i
x2
it

â‰¤p, for each t, where xit is the input of
channel i at time t. Determine the capacity of the compound channel.
Solution For the ï¬rst part see Section 4.3.
If the power in the ith channel is reduced to pi, we would have capacity
Câ€² = 1
2 âˆ‘
i
log

1+ pi
vi

.
The actual capacity is given by C = maxCâ€² subject to p1,..., pr â‰¥0, âˆ‘i pi = p.
Thus, we have to maximise the Lagrangian
L = 1
2 âˆ‘
i
log

1+ pi
vi

âˆ’Î» âˆ‘
i
pi,
with
âˆ‚
âˆ‚pi
L = 1
2(vi + pi)âˆ’1 âˆ’Î»,i = 1,...,r
and the maximum at
pi = max

0, 1
2Î» âˆ’vi

=
 1
2Î» âˆ’vi

+
.
To adjust the constraint, choose Î» = Î» âˆ—where Î» âˆ—is determined from
âˆ‘
i
 1
2Î» âˆ—âˆ’vi

+
= p.
The existence and uniqueness of Î» âˆ—follows since the LHS monotonically de-
creases from +âˆto 0. Thus,
C = 1
2 âˆ‘
i
log

1
2Î» âˆ—vi

.
Problem 4.17
Here we consider random variables taking values in a given
set A (ï¬nite, countable or uncountable) whose distributions are determined by
PMFs with respect to a given reference measure Î¼. Let Ïˆ be a real function
and Î² a real number. Prove that the maximum hmax(X) of the entropy h(X) =
âˆ’
1 fX(X)log fX(x)Î¼(dx) subject to the constraint EÏˆ(X) = Î² is achieved at the
random variable Xâˆ—with the PMF
fXâˆ—(x) = 1
Î exp
	
âˆ’Î³Ïˆ(x)

(4.6.8a)

500
Further Topics from Information Theory
where Î = Î(Î³) =
1 exp
	
âˆ’Î³Ïˆ(x)

Î¼(dx) is the normalising constant and Î³ is cho-
sen so that
EÏˆ(Xâˆ—) =
0 Ïˆ(x)
Î
exp
	
âˆ’Î³Ïˆ(x)

Î¼(dx) = Î².
(4.6.8b)
Assume that the value Î³ with the property
1 Ïˆ(x)
Î
exp
	
âˆ’Î³Ïˆ(x)

Î¼(dx) = Î² exists.
Show that if, in addition, function Ïˆ is non-negative, then, for any given Î² > 0,
the PMF fXâˆ—from (4.6.8a), (4.6.8b) maximises the entropy h(X) under a wider
constraint EÏˆ(X) â‰¤Î².
Consequently, calculate the maximal value of h(X) subject to EÏˆ(X) â‰¤Î², in
the following cases: (i) when A is a ï¬nite set, Î¼ is a positive measure on A (with
Î¼i = Î¼({i}) = 1/Î¼(A) where Î¼(A) = âˆ‘
jâˆˆA
Î¼j) and Ïˆ(x) â‰¡1, x âˆˆA; (ii) when A is
an arbitrary set, Î¼ is a positive measure on A with Î¼(A) < âˆand Ïˆ(x) â‰¡1, x âˆˆA;
(iii) when A = R is a real line, Î¼ is the Lebesgue measure and Ïˆ(x) = |x|; (iv)
when A = Rd, Î¼ is a d-dimensional Lebesgue measure and Ïˆ(x) =
âˆ‘
1|leq jâ‰¤d
Kijxixj,
where K = (Ki j) is a d Ã—d positive deï¬nite real matrix.
Solution With ln f âˆ—
X(x) = âˆ’Î³Ïˆ(x)âˆ’lnÎ, we use the Gibbs inequality:
h(X) = âˆ’
0
fX(x)ln fX(x)Î¼(dx) â‰¤
0
fX(x)
	
Î³Ïˆ(x)+lnÎ

Î¼(dx)
=
0
fXâˆ—(x)
	
Î³Ïˆ(x)+lnÎ

Î¼(dx) = h(Xâˆ—)
with equality if and only if X âˆ¼Xâˆ—. This proves the ï¬rst assertion.
If Ïˆ â‰¥0, the expected value EÏˆ(X) â‰¥0, and Î³ is minimal when the constraint
is satisï¬ed.

Bibliography
[1] V. Anantharam, F. Baccelli. A Palm theory approach to error exponents. In
Proceedings of the 2008 IEEE Symposium on Information Theory, Toronto,
pp. 1768â€“1772, 2008.
[2] J. AdÂ´amek. Foundations of Coding: Theory and Applications of Error-Correcting
Codes, with an Introduction to Cryptography and Information Theory. Chichester:
Wiley, 1991.
[3] D. Applebaum. Probability and Information: An Integrated Approach. Cambridge:
Cambridge University Press, 1996.
[4] R.B. Ash. Information Theory. New York: Interscience, 1965.
[5] E.F. Assmus, Jr., J.D. Key. Designs and their Codes. Cambridge: Cambridge
University Press, 1992.
[6] K.A. Arwini, C.T.J. Dodson. Information Geometry: Near Randomness and Near
Independence. Lecture notes in mathematics, 1953. Berlin: Springer, 2008.
[7] D. Augot, M. Stepanov. A note on the generalisation of the Guruswamiâ€“Sudan
list decoding algorithm to Reedâ€“Muller codes. In GrÂ¨obner Bases, Coding, and
Cryptography. RISC Book Series. Springer, Heidelberg, 2009.
[8] R.U. Ayres. Manufacturing and Human Labor as Information Processes. Laxen-
burg: International Institute for Applied System Analysis, 1987.
[9] A.V. Balakrishnan. Communication Theory (with contributions by J.W. Carlyle
et al.). New York: McGraw-Hill, 1968.
[10] J. Baylis. Error-Correcting Codes: A Mathematical Introduction. London:
Chapman & Hall, 1998.
[11] A. Betten et al. Error-Correcting Linear Codes Classiï¬cation by Isometry and
Applications. Berlin: Springer, 2006.
[12] T. Berger. Rate Distortion Theory: A Mathematical Basis for Data Compression.
Englewood Cliffs, NJ: Prentice-Hall, 1971.
[13] E.R. Berlekamp. A Survey of Algebraic Coding Theory. Wien: Springer, 1972.
[14] E.R. Berlekamp. Algebraic Coding Theory. New York: McGraw-Hill, 1968.
[15] J. Berstel, D. Perrin. Theory of Codes. Orlando, FL: Academic Press, 1985.
[16] J. Bierbrauer. Introduction to Coding Theory. Boca Raton, FL: Chapman &
Hall/CRC, 2005.
[17] P. Billingsley. Ergodic Theory and Information. New York: Wiley, 1965.
501

502
Bibliography
[18] R.E. Blahut. Principles and Practice of Information Theory. Reading, MA:
Addison-Wesley, 1987.
[19] R.E. Blahut. Theory and Practice of Error Control Codes. Reading, MA: Addison-
Wesley, 1983. See also Algebraic Codes for Data Transmission. Cambridge:
Cambridge University Press, 2003.
[20] R.E. Blahut. Algebraic Codes on Lines, Planes, and Curves. Cambridge:
Cambridge University Press, 2008.
[21] I.F. Blake, R.C. Mullin. The Mathematical Theory of Coding. New York: Academic
Press, 1975.
[22] I.F. Blake, R.C. Mullin. An Introduction to Algebraic and Combinatorial Coding
Theory. New York: Academic Press, 1976.
[23] I.F. Blake (ed). Algebraic Coding Theory: History and Development. Stroudsburg,
PA: Dowden, Hutchinson & Ross, 1973.
[24] N. Blachman. Noise and its Effect on Communication. New York: McGraw-Hill,
1966.
[25] R.C. Bose, D.K. Ray-Chaudhuri. On a class of errors, correcting binary group
codes. Information and Control, 3(1), 68â€“79, 1960.
[26] W. Bradley, Y.M. Suhov. The entropy of famous reals: some empirical results.
Random and Computational Dynamics, 5, 349â€“359, 1997.
[27] A.A. Bruen, M.A. Forcinito. Cryptography, Information Theory, and Error-
Correction: A Handbook for the 21st Century. Hoboken, NJ: Wiley-Interscience,
2005.
[28] J.A. Buchmann. Introduction to Cryptography. New York: Springer-Verlag, 2002.
[29] P.J. Cameron, J.H. van Lint. Designs, Graphs, Codes and their Links. Cambridge:
Cambridge University Press, 1991.
[30] J. CastiËœneira Moreira, P.G. Farrell. Essentials of Error-Control Coding. Chichester:
Wiley, 2006.
[31] W.G. Chambers. Basics of Communications and Coding. Oxford: Clarendon, 1985.
[32] G.J. Chaitin. The Limits of Mathematics: A Course on Information Theory and the
Limits of Formal Reasoning. Singapore: Springer, 1998.
[33] G. Chaitin. Information-Theoretic Incompleteness. Singapore: World Scientiï¬c,
1992.
[34] G. Chaitin. Algorithmic Information Theory. Cambridge: Cambridge University
Press, 1987.
[35] F. Conway, J. Siegelman. Dark Hero of the Information Age: In Search of Norbert
Wiener, the Father of Cybernetics. New York: Basic Books, 2005.
[36] T.M. Cover, J.M. Thomas. Elements of Information Theory. New York: Wiley,
2006.
[37] I. CsiszÂ´ar, J. KÂ¨orner. Information Theory: Coding Theorems for Discrete Memo-
ryless Systems. New York: Academic Press, 1981; Budapest: AkadÂ´emiai KiadÂ´o,
1981.
[38] W.B. Davenport, W.L. Root. Random Signals and Noise. New York: McGraw Hill,
1958.
[39] A. Dembo, T. M. Cover, J. A. Thomas. Information theoretic inequalities. IEEE
Transactions on Information Theory, 37, (6), 1501â€“1518, 1991.

Bibliography
503
[40] R.L. Dobrushin. Taking the limit of the argument of entropy and information func-
tions. Teoriya Veroyatn. Primen., 5, (1), 29â€“37, 1960; English translation: Theory
of Probability and its Applications, 5, 25â€“32, 1960.
[41] F. Dyson. The Tragic Tale of a Genius. New York Review of Books, July 14, 2005.
[42] W. Ebeling. Lattices and Codes: A Course Partially Based on Lectures by F. Hirze-
bruch. Braunschweig/Wiesbaden: Vieweg, 1994.
[43] N. Elkies. Excellent codes from modular curves. STOCâ€™01: Proceedings of the
33rd Annual Symposium on Theory of Computing (Hersonissos, Crete, Greece),
pp. 200â€“208, NY: ACM, 2001.
[44] S. Engelberg. Random Signals and Noise: A Mathematical Introduction. Boca Ra-
ton, FL: CRC/Taylor & Francis, 2007.
[45] R.M. Fano. Transmission of Information: A Statistical Theory of Communication.
New York: Wiley, 1961.
[46] A. Feinstein. Foundations of Information Theory. New York: McGraw-Hill, 1958.
[47] G.D. Forney. Concatenated Codes. Cambridge, MA: MIT Press, 1966.
[48] M. Franceschetti, R. Meester. Random Networks for Communication. From Sta-
tistical Physics to Information Science. Cambridge: Cambridge University Press,
2007.
[49] R. Gallager. Information Theory and Reliable Communications. New York: Wiley,
1968.
[50] A. Gofman, M. Kelbert, Un upper bound for Kullbackâ€“Leibler divergence with a
small number of outliers. Mathematical Communications, 18, (1), 75â€“78, 2013.
[51] S. Goldman. Information Theory. Englewood Cliffs, NJ: Prentice-Hall, 1953.
[52] C.M. Goldie, R.G.E. Pinch. Communication Theory. Cambridge: Cambridge
University Press, 1991.
[53] O. Goldreich. Foundations of Cryptography, Vols 1, 2. Cambridge: Cambridge
University Press, 2001, 2004.
[54] V.D. Goppa. Geometry and Codes. Dordrecht: Kluwer, 1988.
[55] S. Gravano. Introduction to Error Control Codes. Oxford: Oxford University Press,
2001.
[56] R.M. Gray. Source Coding Theory. Boston: Kluwer, 1990.
[57] R.M. Gray. Entropy and Information Theory. New York: Springer-Verlag, 1990.
[58] R.M. Gray, L.D. Davisson (eds). Ergodic and Information Theory. Stroudsburg,
CA: Dowden, Hutchinson & Ross, 1977 .
[59] V. Guruswami, M. Sudan. Improved decoding of Reedâ€“Solomon codes and alge-
braic geometry codes. IEEE Trans. Inform. Theory, 45, (6), 1757â€“1767, 1999.
[60] R.W. Hamming. Coding and Information Theory. 2nd ed. Englewood Cliffs, NJ:
Prentice-Hall, 1986.
[61] T.S. Han. Information-Spectrum Methods in Information Theory. New York:
Springer-Verlag, 2002.
[62] D.R. Hankerson, G.A. Harris, P.D. Johnson, Jr. Introduction to Information Theory
and Data Compression. 2nd ed. Boca Raton, FL: Chapman & Hall/CRC, 2003.
[63] D.R. Hankerson et al. Coding Theory and Cryptography: The Essentials. 2nd ed.
New York: M. Dekker, 2000. (Earlier version: D. G. Hoffman et al. Coding Theory:
The Essentials. New York: M. Dekker, 1991.)
[64] W.E. Hartnett. Foundations of Coding Theory. Dordrecht: Reidel, 1974.

504
Bibliography
[65] S.J. Heims. John von Neumann and Norbert Wiener: From Mathematics to the
Technologies of Life and Death. Cambridge, MA: MIT Press, 1980.
[66] C. Helstrom. Statistical Theory of Signal Detection. 2nd ed. Oxford: Pergamon
Press, 1968.
[67] C.W. Helstrom. Elements of Signal Detection and Estimation. Englewood Cliffs,
NJ: Prentice-Hall, 1995.
[68] R. Hill. A First Course in Coding Theory. Oxford: Oxford University Press, 1986.
[69] T. Ho, D.S. Lun. Network Coding: An Introduction. Cambridge: Cambridge Uni-
versity Press, 2008.
[70] A. Hocquenghem. Codes correcteurs dâ€™erreurs. Chiffres, 2, 147â€“156, 1959.
[71] W.C. Huffman, V. Pless. Fundamentals of Error-Correcting Codes. Cambridge:
Cambridge University Press, 2003.
[72] J.F. Humphreys, M.Y. Prest. Numbers, Groups, and Codes. 2nd ed. Cambridge:
Cambridge University Press, 2004.
[73] S. Ihara. Information Theory for Continuous Systems. Singapore: World Scientiï¬c,
1993 .
[74] F.M. Ingels. Information and Coding Theory. Scranton: Intext Educational Pub-
lishers, 1971.
[75] I.M. James. Remarkable Mathematicians. From Euler to von Neumann.
Cambridge: Cambridge University Press, 2009 .
[76] E.T. Jaynes. Papers on Probability, Statistics and Statistical Physics. Dordrecht:
Reidel, 1982.
[77] F. Jelinek. Probabilistic Information Theory. New York: McGraw-Hill, 1968.
[78] G.A. Jones, J.M. Jones. Information and Coding Theory. London: Springer, 2000.
[79] D.S. Jones. Elementary Information Theory. Oxford: Clarendon Press, 1979.
[80] O. Johnson. Information Theory and the Central Limit Theorem. London: Imperial
College Press, 2004.
[81] J. Justensen. A class of constructive asymptotically good algebraic codes. IEEE
Transactions Information Theory, 18(5), 652â€“656, 1972.
[82] M. Kelbert, Y. Suhov. Continuity of mutual entropy in the large signal-to-noise
ratio limit. In Stochastic Analysis 2010, pp. 281â€“299, 2010. Berlin: Springer.
[83] N. Khalatnikov. Dau, Centaurus and Others. Moscow: Fizmatlit, 2007.
[84] A.Y. Khintchin. Mathematical Foundations of Information Theory. New York:
Dover, 1957.
[85] T. Klove. Codes for Error Detection. Singapore: World Scientiï¬c, 2007.
[86] N. Koblitz. A Course in Number Theory and Cryptography. New York: Springer,
1993 .
[87] H. Krishna. Computational Complexity of Bilinear Forms: Algebraic Coding The-
ory and Applications of Digital Communication Systems. Lecture notes in control
and information sciences, Vol. 94. Berlin: Springer-Verlag, 1987.
[88] S. Kullback. Information Theory and Statistics. New York: Wiley, 1959.
[89] S. Kullback, J.C. Keegel, J.H. Kullback. Topics in Statistical Information Theory.
Berlin: Springer, 1987.
[90] H.J. Landau, H.O. Pollak. Prolate spheroidal wave functions, Fourier analysis and
uncertainty, II. Bell System Technical Journal, 64â€“84, 1961.

Bibliography
505
[91] H.J. Landau, H.O. Pollak. Prolate spheroidal wave functions, Fourier analysis and
uncertainty, III. The dimension of the space of essentially time- and band-limited
signals. Bell System Technical Journal, 1295â€“1336, 1962.
[92] R. Lidl, H. Niederreiter. Finite Fields. Cambridge: Cambridge University Press,
1997.
[93] R. Lidl, G. Pilz. Applied Abstract Algebra. 2nd ed. New York: Wiley, 1999.
[94] E.H. Lieb. Proof of entropy conjecture of Wehrl. Commun. Math. Phys., 62, (1),
35â€“41, 1978.
[95] S. Lin. An Introduction to Error-Correcting Codes. Englewood Cliffs, NJ; London:
Prentice-Hall, 1970.
[96] S. Lin, D.J. Costello. Error Control Coding: Fundamentals and Applications.
Englewood Cliffs, NJ: Prentice-Hall, 1983.
[97] S. Ling, C. Xing. Coding Theory. Cambridge: Cambridge University Press, 2004.
[98] J.H. van Lint. Introduction to Coding Theory. 3rd ed. Berlin: Springer, 1999.
[99] J.H. van Lint, G. van der Geer. Introduction to Coding Theory and Algebraic
Geometry. Basel: BirkhÂ¨auser, 1988.
[100] J.C.A. van der Lubbe. Information Theory. Cambridge: Cambridge University
Press, 1997.
[101] R.E. Lewand. Cryptological Mathematics. Washington, DC: Mathematical Asso-
ciation of America, 2000.
[102] J.A. Llewellyn. Information and Coding. Bromley: Chartwell-Bratt; Lund:
Studentlitteratur, 1987.
[103] M. Lo`eve. Probability Theory. Princeton, NJ: van Nostrand, 1955.
[104] D.G. Luenberger. Information Science. Princeton, NJ: Princeton University Press,
2006.
[105] D.J.C. Mackay. Information Theory, Inference and Learning Algorithms.
Cambridge: Cambridge University Press, 2003.
[106] H.B. Mann (ed). Error-Correcting Codes. New York: Wiley, 1969 .
[107] M. Marcus. Dark Hero of the Information Age: In Search of Norbert Wiener, the
Father of Cybernetics. Notices of the AMS 53, (5), 574â€“579, 2005.
[108] A. Marshall, I. Olkin. Inequalities: Theory of Majorization and its Applications.
New York: Academic Press, 1979 .
[109] V.P. Maslov, A.S. Chernyi. On the minimization and maximization of entropy in
various disciplines. Theory Probab. Appl. 48, (3), 447â€“464, 2004.
[110] F.J. MacWilliams, N.J.A. Sloane. The Theory of Error-Correcting Codes, Vols I,
II. Amsterdam: North-Holland, 1977.
[111] R.J. McEliece. The Theory of Information and Coding. Reading, MA: Addison-
Wesley, 1977. 2nd ed. Cambridge: Cambridge University Press, 2002.
[112] R. McEliece. The Theory of Information and Coding. Student ed. Cambridge:
Cambridge University Press, 2004.
[113] A. Menon, R.M. Buecher, J.H. Read. Impact of exclusion region and spreading in
spectrum-sharing ad hoc networks. ACM 1-59593-510-X/06/08, 2006 .
[114] R.A. Mollin. RSA and Public-Key Cryptography. New York: Chapman & Hall,
2003.
[115] R.H. Morelos-Zaragoza. The Art of Error-Correcting Coding. 2nd ed. Chichester:
Wiley, 2006.

506
Bibliography
[116] G.L. Mullen, C. Mummert. Finite Fields and Applications. Providence, RI:
American Mathematical Society, 2007.
[117] A. Myasnikov, V. Shpilrain, A. Ushakov. Group-Based Cryptography. Basel:
BirkhÂ¨auser, 2008.
[118] G. Nebe, E.M. Rains, N.J.A. Sloane. Self-Dual Codes and Invariant Theory. New
York: Springer, 2006.
[119] H. Niederreiter, C. Xing. Rational Points on Curves over Finite Fields: Theory and
Applications. Cambridge: Cambridge University Press, 2001.
[120] W.W. Peterson, E.J. Weldon. Error-Correcting Codes. 2nd ed. Cambridge,
MA: MIT Press, 1972. (Previous ed. W.W. Peterson. Error-Correcting Codes.
Cambridge, MA: MIT Press, 1961.)
[121] M.S. Pinsker. Information and Information Stability of Random Variables and Pro-
cesses. San Francisco: Holden-Day, 1964.
[122] V. Pless. Introduction to the Theory of Error-Correcting Codes. 2nd ed. New York:
Wiley, 1989.
[123] V.S. Pless, W.C. Huffman (eds). Handbook of Coding Theory, Vols 1, 2. Amster-
dam: Elsevier, 1998.
[124] P. Piret. Convolutional Codes: An Algebraic Approach. Cambridge, MA: MIT
Press, 1988.
[125] O. Pretzel. Error-Correcting Codes and Finite Fields. Oxford: Clarendon Press,
1992; Student ed. 1996.
[126] T.R.N. Rao. Error Coding for Arithmetic Processors. New York: Academic Press,
1974.
[127] M. Reed, B. Simon. Methods of Modern Mathematical Physics, Vol. II. Fourier
analysis, self-adjointness. New York: Academic Press, 1975.
[128] A. RÂ´enyi. A Diary on Information Theory. Chichester: Wiley, 1987; initially pub-
lished Budapest: Akadâ€™emiai KiadÂ´o, 1984.
[129] F.M. Reza. An Introduction to Information Theory. New York: Constable, 1994.
[130] S. Roman. Coding and Information Theory. New York: Springer, 1992.
[131] S. Roman. Field Theory. 2nd ed. New York: Springer, 2006.
[132] T. Richardson, R. Urbanke. Modern Coding Theory. Cambridge: Cambridge Uni-
versity Press, 2008.
[133] R.M. Roth. Introduction to Coding Theory. Cambridge: Cambridge University
Press, 2006.
[134] B. Ryabko, A. Fionov. Basics of Contemporary Cryptography for IT Practitioners.
Singapore: World Scientiï¬c, 2005.
[135] W.E. Ryan, S. Lin. Channel Codes: Classical and Modern. Cambridge: Cambridge
University Press, 2009.
[136] T. SchÂ¨urmann, P. Grassberger. Entropy estimation of symbol sequences. Chaos, 6,
(3), 414â€“427, 1996.
[137] P. Seibt. Algorithmic Information Theory: Mathematics of Digital Information Pro-
cessing. Berlin: Springer, 2006.
[138] C.E. Shannon. A mathematical theory of cryptography. Bell Lab. Tech. Memo.,
1945.
[139] C.E. Shannon. A mathematical theory of communication. Bell System Technical
Journal, 27, July, October, 379â€“423, 623â€“658, 1948.

Bibliography
507
[140] C.E. Shannon: Collected Papers. N.J.A. Sloane, A.D. Wyner (eds). New York:
IEEE Press, 1993.
[141] C.E. Shannon, W. Weaver. The Mathematical Theory of Communication. Urbana,
IL: University of Illinois Press, 1949.
[142] P.C. Shields. The Ergodic Theory of Discrete Sample Paths. Providence, RI:
American Mathematical Society, 1996.
[143] M.S. Shrikhande, S.S. Sane. Quasi-Symmetric Designs. Cambridge: Cambridge
University Press, 1991.
[144] S. Simic. Best possible global bounds for Jensen functionals. Proc. AMS, 138, (7),
2457â€“2462, 2010.
[145] A. Sinkov. Elementary Cryptanalysis: A Mathematical Approach. 2nd ed. revised
and updated by T. Feil. Washington, DC: Mathematical Association of America,
2009.
[146] D. Slepian, H.O. Pollak. Prolate spheroidal wave functions, Fourier analysis and
uncertainty, Vol. I. Bell System Technical Journal, 43â€“64, 1961 .
[147] W. Stallings. Cryptography and Network Security: Principles and Practice. 5th ed.
Boston, MA: Prentice Hall; London: Pearson Education, 2011.
[148] H. Stichtenoth. Algebraic Function Fields and Codes. Berlin: Springer, 1993.
[149] D.R. Stinson. Cryptography: Theory and Practice. 2nd ed. Boca Raton, FL;
London: Chapman & Hall/CRC, 2002.
[150] D. Stoyan, W.S. Kendall. J. Mecke. Stochastic Geometry and its Applications.
Berlin: Academie-Verlag, 1987 .
[151] C. Schlegel, L. Perez. Trellis and Turbo Coding. New York: Wiley, 2004.
[152] Ë‡S. Ë‡Sujan. Ergodic Theory, Entropy and Coding Problems of Information Theory.
Praha: Academia, 1983.
[153] P. Sweeney. Error Control Coding: An Introduction. New York: Prentice Hall,
1991.
[154] Te Sun Han, K. Kobayashi. Mathematics of Information and Coding. Providence,
RI: American Mathematical Society, 2002.
[155] T.M. Thompson. From Error-Correcting Codes through Sphere Packings to Simple
Groups. Washington, DC: Mathematical Association of America, 1983.
[156] R. Togneri, C.J.S. deSilva. Fundamentals of Information Theory and Coding
Design. Boca Raton, FL: Chapman & Hall/CRC, 2002.
[157] W. Trappe, L.C. Washington. Introduction to Cryptography: With Coding Theory.
2nd ed. Upper Saddle River, NJ: Pearson Prentice Hall, 2006.
[158] M.A. Tsfasman, S.G. VlË‡adut. Algebraic-Geometric Codes. Dordrecht: Kluwer
Academic, 1991.
[159] M. Tsfasman, S. VlË‡adut, T. Zink. Modular curves, Shimura curves and Goppa
codes, better than Varshamovâ€“Gilbert bound. Mathematics Nachrichten, 109,
21â€“28, 1982.
[160] M. Tsfasman, S. VlË‡adut, D. Nogin. Algebraic Geometric Codes: Basic Notions.
Providence, RI: American Mathematical Society, 2007.
[161] M.J. Usher. Information Theory for Information Technologists. London: Macmil-
lan, 1984.
[162] M.J. Usher, C.G. Guy. Information and Communication for Engineers. Bas-
ingstoke: Macmillan, 1997

508
Bibliography
[163] I. Vajda. Theory of Statistical Inference and Information. Dordrecht: Kluwer, 1989.
[164] S. VerdÂ´u. Multiuser Detection. New York: Cambridge University Press, 1998.
[165] S. VerdÂ´u, D. Guo. A simple proof of the entropyâ€“power inequality. IEEE Trans.
Inform. Theory, 52, (5), 2165â€“2166, 2006.
[166] L.R. Vermani. Elements of Algebraic Coding Theory. London: Chapman & Hall,
1996.
[167] B. Vucetic, J. Yuan. Turbo Codes: Principles and Applications. Norwell, MA:
Kluwer, 2000.
[168] G. Wade. Coding Techniques: An Introduction to Compression and Error Control.
Basingstoke: Palgrave, 2000.
[169] J.L. Walker. Codes and Curves. Providence, RI: American Mathematical Society,
2000.
[170] D. Welsh. Codes and Cryptography. Oxford, Oxford University Press, 1988.
[171] N. Wiener. Cybernetics or Control and Communication in Animal and Machine.
Cambridge, MA: MIT Press, 1948; 2nd ed: 1961, 1962.
[172] J. Wolfowitz. Coding Theorems of Information Theory. Berlin: Springer, 1961; 3rd
ed: 1978.
[173] A.D. Wyner. The capacity of the band-limited Gaussian channel. Bell System Tech-
nical Journal, 359â€“395, 1996 .
[174] A.D. Wyner. The capacity of the product of channels. Information and Control,
423â€“433, 1966.
[175] C. Xing. Nonlinear codes from algebraic curves beating the Tsfasmanâ€“VlË‡adutâ€“
Zink bound. IEEE Transactions Information Theory, 49, 1653â€“1657, 2003.
[176] A.M. Yaglom, I.M. Yaglom. Probability and Information. Dordrecht, Holland:
Reidel, 1983.
[177] R. Yeung. A First Course in Information Theory. Boston: Kluwer Academic, 1992;
2nd ed. New York: Kluwer, 2002.

Index
additive stream cipher, 463
algebra (a commutative ring and a linear space),
318
group algebra, 317
polynomial algebra, 214
Ïƒ-algebra, 440
algebraic-geometric code, 340
algorithm, 9
Berlekampâ€“Massey (BM) decoding algorithm for
BCH codes, 240
Berlekamp-Massey (BM) algorithm for solving
linear equations, 460
division algorithm for polynomials, 214
Euclid algorithm for integers, 473
extended Euclid algorithm for integers, 470
Euclid algorithm for polynomials, 242
Guruswamiâ€“Sudan (GS) decoding algorithm for
Reedâ€“Solomon codes, 298
Huffman encoding algorithm, 9
alphabet, 3
source alphabet, 8
coder (encoding) alphabet, 3
channel input alphabet, 60
channel output alphabet, 65
asymptotic equipartition property, 44
asymptotically good sequence of codes, 78
automorphism, 283
band-limited signal, 411
bandwidth, 409
basis, 149, 184
BCH (Boseâ€“Ray-Chaudhuriâ€“Hocquenghem) bound,
or BCH theorem, 237, 295
BCH code, 213
BCH code in a narrow sense, 235
binary BCH code in a narrow sense, 235
Bernoulli source, 3
bit (a unit of entropy), 9
bit commitment cryptosystem, 468
bound
BCH bound, 237, 295
Elias bound, 177
Gilbert bound, 198
Gilbertâ€“Varshamov bound, 154
Griesmer bound, 197
Hamming bound, 150
Johnson bound, 177
linear programming bound, 322
Plotkin bound, 155
Singleton bound, 154
bar-product, 152
capacity, 61
capacity of a discrete channel, 61
capacity of a memoryless Gaussian channel with
white noise, 374
capacity of a memoryless Gaussian channel with
coloured noise, 375
operational channel capacity, 102
character (as a digit or a letter or a symbol), 53
character (of a homomorphism), 313
modular character, 314
trivial, or principal, character, 313
character transform, 319
characteristic of a ï¬eld, 269
channel, 60
additive Gaussian channel (AGC), 368
memoryless Gaussian channel (MGC), 368
memoryless additive Gaussian channel (MAGC),
366
memoryless binary channel (MBC), 60
memoryless binary symmetric channel (MBSC), 60
noiseless channel, 103
channel capacity, 61
operational channel capacity, 102
check matrix: see parity-check matrix
cipher (or a cryptosystem), 463
additive stream cipher, 463
509

510
Index
cipher (or a cryptosystem) (cont.)
one-time pad cipher, 466
public-key cipher, 467
ciphertext, 468
code, or encoding, viii, 4
alternant code, 332
BCH code, 213
binary code, 10, 95
cardinality of a code, 253
cyclic code, 216
decipherable code, 14
dimension of a linear code, 149
D error detecting code, 147
dual code, 153
equivalent codes, 190
E error correcting code, 147
Golay code, 151
Goppa code, 160, 334
Hamming code, 199
Huffman code, 9
information rate of a code, 147
Justesen code, 240, 332
lossless code, 4
linear code, 148
maximal distance separating (MDS), 155
parity-check code, 149
perfect code, 151
preï¬x-free code, 4
random code, 68, 372
rank of a linear code, 184
Reedâ€“Muller (RM) code, 203
Reedâ€“Solomon code, 256, 291
repetition code, 149
reversible cyclic code, 230
self-dual code, 201, 227
self-orthogonal, 227
symplex code, 194
codebook, 67
random codebook, 68
coder, or encoder, 3
codeword, 4
random codeword, 6
coding: see encoding
coloured noise, 374
concave, 19, 32
strictly concave, 32
concavity, 20
conditionally independent, 26
conjugacy, 281
conjugate, 229
convergence almost surely (a.s.), 131
convergence in probability, 43
convex, 32
strictly convex, 104
convexity, 142
core polynomial of a ï¬eld, 231
coset, 192
cyclotomic coset, 285
leader of a coset, 192
cryptosystem (or a cipher), 468
bit commitment cryptosystem, 468
ElGamal cryptosystem, 475
public key cryptosystem, 468
RSA (Rivestâ€“Shamirâ€“Adelman) cryptosystem,
468
Rabin, or Rabinâ€“Williams cryptosystem, 473
cyclic group, 231
generator of a cyclic group
cyclic shift, 216
data-processing inequality, 80
detailed balance equations (DBEs), 56
decoder, or a decoding rule, 65
geometric (or minimal distance) decoding rule, 163
ideal observer (IO) decoding rule, 66
maximum likelihood (ML) decoding rule, 66
joint typicality (JT) decoder, 372
decoding, 167
decoding alternant codes, 337
decoding BCH codes, 239, 310
decoding cyclic codes, 214
decoding Hamming codes, 200
list decoding, 192, 405
decoding Reedâ€“Muller codes, 209
decoding Reedâ€“Solomon codes, 292
decoding Reedâ€“Solomon codes by the
Guruswamiâ€“Sudan algorithm, 299
syndrome decoding, 193
decrypt function, 469
degree of a polynomial, 206, 214
density of a probability distribution (PDF), 86
differential entropy, 86
digit, 3
dimension, 149
dimension of a code, 149
dimension of a linear representation, 314
discrete Fourier transform (FFT), 296
discrete-time Markov chain (DTMC), 1, 3
discrete logarithm, 474
distributed system, or a network (of transmitters), 436
Dirac Î´-function, 318
distance, 20
Kullbackâ€“Leibler distance, 20
Hamming distance, 144
minimal distance of a code, 147
distance enumerator polynomial, 322
divisor, 217
greatest common divisor (gcd), 223
dot-product, 153
doubly stochastic (Cox) random process, 492

Index
511
electronic signature, 469, 476
encoding, or coding, vii, 4
Huffman encoding, 9
Shannonâ€“Fano encoding, 9
random coding, 67
entropy, vii, 7
axiomatic deï¬nition of entropy, 36
binary entropy, 7
conditional entropy, 20
differential entropy, 86
entropy of a random variable, 18
entropy of a probability distribution, 18
joint entropy, 20
mutual entropy, 28
entropyâ€“power inequality, 92
q-ary entropy, 7
entropy rate, vii, 41
relative entropy, 20
encrypt function, 468
ergodic random process (stationary), 397
ergodic transformation of a probability space, 397
error locator, 311
error locator polynomial, 239, 311
error-probability, 58
extension of a code, 151
parity-check extension, 151
extension ï¬eld, 261
factor (as a divisor), 39
irreducible factor, 219
prime factor, 39
factorization, 230
fading of a signal, 447
power fading, 447
Rayleigh fading, 447
feedback shift register, 453
linear feedback shift register (LFSR), 454
feedback polynomial, 454
ï¬eld (a commutative ring with inverses), 146, 230
extension ï¬eld, 261
Galois ï¬eld, 272
ï¬nite ï¬eld, 194
polynomial ï¬eld, 231
primitive element of a ï¬eld, 230, 232
splitting ï¬eld, 236, 271
Frobenius map, 283
Gaussian channel, 366
additive Gaussian channel (AGC), 368
memoryless Gaussian channel (MGC), 368
memoryless additive Gaussian channel (MAGC),
366
Gaussian coloured noise, 374
Gaussian white noise, 368
Gaussian random process, 369
generating matrix, 185
generator (of a cyclic code), 218
minimal degree generator polynomial, 218
generator (of a cyclic group), 232
geometric (or minimal distance) decoding rule, 163
group, 146
group algebra, 317
commutative, or Abelian, group, 146
cyclic group, 231
linear representation of a group, 314
generalized function, 412
greatest common divisor (gcd), 223
ideal observer (IO) decoding rule, 66
ideal of a ring, 217
principal ideal, 219
identity (for weight enumerator polynomials), 258
abstract MacWilliams identity, 315
MacWilliams identity for a linear code, 258, 313
independent identically distributed (IID) random
variables, 1, 3
inequality, 4
Brunnâ€“Minkovski inequality, 93
Cauchyâ€“Schwarz inequality, 124
Chebyshev inequality, 128
data-processing inequality, 80
entropyâ€“power inequality, 92
Fano inequality, 25
generalized Fano inequality, 27
Gibbs inequality, 17
Hadamard inequality, 91
Kraft inequality, 4
Kyâ€“Fan inequality, 91
log-sum inequality, 103
Markov inequality, 408
pooling inequalities, 24
information, 2, 18
mutual information, or mutual entropy, 28
information rate, 15
information source (random source), 2, 44
Bernoulli information source, 3
Markov information source, 3
information symbols, 209
initial ï¬ll, 454
intensity (of a random measure), 437
intensity measure, 437
joint entropy, 20
joint input/output distribution (of a channel), 67
joint typicality (JT) decoder, 372
key (as a part of a cipher), 466
decoding key (a label of a decoding, or
decrypting, map), 469
encoding key (a label of an encoding, or
encrypting, map), 468
random key of a one-pad cipher, 466

512
Index
key (as a part of a cipher) (cont.)
private key, 470
public key, 469
secret key, 473
Karhunenâ€“LoÂ´eve decomposition, 426
law of large numbers, 34
strong law of large numbers, 438
leader of a coset, 192
least common multiple (lcm), 223
lemma
Borelâ€“Cantelli lemma, 418
Nyquistâ€“Shannonâ€“Kotelnikovâ€“Whittaker lemma,
431
letter, 2
linear code, 148
linear representation of a group, 314
space of a linear representation, 314
dimension of a linear representation, 314
linear space, 146
linear subspace, 148
linear feedback shift register (LFSR), 454
auxiliary, or feedback, polynomial of an LFSR, 454
Markov chain, 1, 3
discrete-time Markov chain (DTMC), 1, 3
coupled Markov chain, 50
irreducible and aperiodic Markov chain, 128
kth-order Markov chain approximation, 407
second-order Markov chain, 131
transition matrix of a Markov chain, 3
Markov inequality, 408
Markov property, 33
strong Markov property, 50
Markov source, 3
stationary Markov source, 3
Markov triple, 33
MatÂ´ern process (with a hard core), 451
ï¬rst model of the MatÂ´ern process, 451
second model of the MatÂ´ern process, 451
matrix, 13
covariance matrix, 88
generating matrix, 185
generating check matrix, canonical, or standard,
form of, 189
parity-check matrix, 186
parity-check matrix, canonical, or standard, form
of, 189
parity-check matrix of a Hamming code, 191
positive deï¬nite matrix, 91
recursion matrix, 174
TÂ¨oplitz matrix, 93
transition matrix of a Markov chain, 3
transition matrix, doubly stochastic, 34
Vandermonde matrix, 295
maximum likelihood (ML) decoding rule, 66
measure (as a countably additive function of a set),
366
intensity (or mean) measure, 436
non-atomic measure, 436
Poisson random measure, 436
product-measure, 371
random measure, 436
reference measure, 372
Ïƒ-ï¬nite, 436
MÂ¨obius function, 277
MÂ¨obius inversion formula, 278
moment generating function, 442
network: see distributed system
supercritical network, 449
network information theory, 436
noise (in a channel), 2, 70
Gaussian coloured noise, 374
Gaussian white noise, 368
noiseless channel, 103
noisy (or fully noisy) channel, 81
one-time pad cipher, 466
operational channel capacity, 102
order of an element, 267
order of a polynomial, 231
orthogonal, 185
ortho-basis, 430
orthogonal complement, 185
orthoprojection, 375
self-orthogonal, 227
output stream of a register, 454
parity-check code, 149
parity-check extension, 151
parity-check matrix, 186
plaintext, 468
Poisson process, 436
Poisson random measure, 436
polynomial, 206
algebra, polynomial, 214
degree of a polynomial, 206, 214
distance enumerator polynomial, 322
error locator polynomial, 239
Goppa polynomial, 335
irreducible polynomial, 219
Mattsonâ€“Solomon polynomial, 296
minimal polynomial, 236
order of a polynomial, 231
reducible polynomial, 221
primitive polynomial, 230, 267
Kravchuk polynomial, 320
weight enumerator polynomial, 319, 351
probability distribution, vii, 1
conditional probability, 1
probability density function (PDF), 86

Index
513
equiprobable, or uniform, distribution, 3, 22
exponential distribution (with exponential density),
89
geometric distribution, 21
joint probability, 1
multivariate normal distribution, 88
normal distribution (with univariate normal
density), 89
Poisson distribution, 101
probability mass function (PMF), 366
probability space, 397
prolate spheroidal wave function (PSWF), 425
protocol of a private communication, 469
Difï¬eâ€“Hellman protocol, 474
preï¬x, 4
preï¬x-free code, 4
product-channel, 404
public-key cipher, 467
quantum mechanics, 431
random code, 68, 372
random codebook, 68
random codeword, 6
random measure, 436
Poisson random measure (PRM), 436
random process, vii
Gaussian random process, 369
Poisson random process, 436
stationary random process, 397
stationary ergodic random process, 397
random variable, 18
conditionally independent random variables, 26
equiprobable, or uniform, random variable,
3, 22
exponential random variable (with exponential
density), 89
geometric random variable, 21
independent identically distributed (IID) random
variables, 1, 3
joint probability distribution of random variables, 1
normal random variable (with univariate normal
density), 89
Poisson random variable, 101
random vector, 20
multivariate normal random vector, 88
rank of a code, 184
rank-nullity property, 186
rate, 15
entropy rate, vii, 41
information rate of a source, 15
reliable encoding (or encodable) rate, 15
reliable transmission rate, 62
reliable transmission rate with regional constraint,
373
regional constraint for channel capacity, 367
register, 453
feedback shift register, 453
linear feedback shift register (LFSR), 454
feedback, or auxiliary, polynomial of an LFSR, 454
initial ï¬ll of register, 454
output stream of a register, 454
repetition code, 149
repetition of a code, 152
ring, 217
ideal of a ring, 217
quotient ring, 274
root of a cyclic code, 230
deï¬ning root of a cyclic code, 233
root of a polynomial, 228
root of unity, 228
primitive root of unity, 236
sample, viii, 2
signal/noise ratio (SNR), 449
sinc function, 413
size of a code, 147
space, 35
Hamming space, 144
space Å2(R1), 415
linear space, 146
linear subspace, 148
space of a linear representation, 314
state space of a Markov chain, 35
vector space over a ï¬eld, 269
stream, 463
strictly concave, 32
strictly convex, 104
string, or a word (of characters, digits, letters or
symbols), 3
source of information (random), 2, 44
Bernoulli source, 3
equiprobable Bernoulli source, 3
Markov source, 3
stationary Markov source, 3
spectral density, 417
stationary, 3
stationary Markov source, 3
stationary random process, 397
stationary ergodic random process, 397
supercritical network, 449
symbol, 2
syndrome, 192
theorem
Brunnâ€“Minkovski theorem, 93
Campbell theorem, 442
Cayleyâ€“Hamilton theorem, 456
central limit theorem (CLT), 94
Doobâ€“LÂ´evy theorem, 409
local De Moivreâ€“Laplace theorem, 53
mapping theorem, 437

514
Index
theorem (cont.)
product theorem, 444
Shannon theorem, 8
Shannonâ€™s noiseless coding theorem (NLCT), 8
Shannonâ€™s ï¬rst coding theorem (FCT), 42
Shannonâ€™s second coding theorem (SCT), or noisy
coding theorem (NCT), 59, 162
Shannonâ€™s SCT: converse part, 69
Shannonâ€™s SCT: strong converse part, 175
Shannonâ€™s SCT: direct part, 71, 163
Shannonâ€“McMillanâ€“Breiman theorem, 397
totient function, 270
transform
character transform, 319
Fourier transform, 296
Fourier transform, discrete, 296
Fourier transform in Å2, 413
transmitter, 443
uncertainty principle, 431
Vandermonde determinant, 237
Vandermonde matrix, 297
wedge-product, 149
weight enumerator polynomial, 319
white noise, 368
word, or a string (of characters, digits, letters or
symbols), 3
weight of a word, 144

