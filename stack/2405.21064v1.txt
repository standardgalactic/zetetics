Recurrent neural networks: vanishing and exploding
gradients are not the end of the story
Nicolas Zucchet
Department of Computer Science
ETH Zürich
nzucchet@ethz.ch
Antonio Orvieto
ELLIS Institute Tübingen
MPI for Intelligent Systems
Tübingen AI Center
antonio@tue.ellis.eu
Abstract
Recurrent neural networks (RNNs) notoriously struggle to learn long-term mem-
ories, primarily due to vanishing and exploding gradients. The recent success of
state-space models (SSMs), a subclass of RNNs, to overcome such difficulties
challenges our theoretical understanding. In this paper, we delve into the optimiza-
tion challenges of RNNs and discover that, as the memory of a network increases,
changes in its parameters result in increasingly large output variations, making
gradient-based learning highly sensitive, even without exploding gradients. Our
analysis further reveals the importance of the element-wise recurrence design pat-
tern combined with careful parametrizations in mitigating this effect. This feature
is present in SSMs, as well as in other architectures, such as LSTMs. Overall, our
insights provide a new explanation for some of the difficulties in gradient-based
learning of RNNs and why some architectures perform better than others.
Recurrent neural networks [RNNs; 1, 2] have long been the canonical architecture for modeling
temporal data [3, 4]. However, they are notoriously difficult to train on long sequences, as error
signals flowing backward in time tend to either vanish or explode [5–8]. Attention mechanisms [9],
as featured in transformers [10], address these issues by enabling direct token-to-token communi-
cation, considerably simplifying signal propagation across long time intervals. Yet, their superior
performance comes with increased computational and memory costs, due to their quadratic scaling in
the sequence length. This limitation has motivated significant research aimed at making transformers
more efficient [11–15].
A promising line of research in this direction involves a new type of linear recurrent networks known
as state-space models [SSMs; 16–22]. These models trade expressivity for faster training speed, and
they have been shown to be particular effective at capturing long-range dependencies. In this paper,
we wonder whether this effectiveness can be solely attributed to their ability to avoid vanishing and
exploding gradients. The simplicity of such models presents an opportunity for in-depth theoretical
analysis. We focus on signal propagation within these models.
After reviewing classical results on recurrent neural networks in Section 1, we demonstrate that they
can suffer an understudied problem: the curse of memory (Section 2). As the recurrent network
encodes longer memories, the network’s activity becomes increasingly sensitive to changes in its
parameters, even when its dynamics remains stable. In Section 3, we then show that SSMs, as well
as other architectures such as LSTMs, are well equipped to mitigate this issue. We conclude our
study by analyzing a simple teacher-student task, which already reveals the remarkable complexity
underlying the learning of linear recurrent networks (Section 4). Finally, we discuss how our findings
extend to more realistic scenarios (Section 5). Overall, our results reveal that vanishing and exploding
gradients are not the end of the story and that there exists an additional layer of complexity beyond
them.
arXiv:2405.21064v1  [cs.LG]  31 May 2024

1
Vanishing and exploding gradients
Let us first introduce the notations we will be using throughout the rest of the paper. We consider
a recurrent neural network with hidden state ht, update function fθ parametrized by θ, and input
sequence (xt)t. The average performance of the network is measured by a loss L. We have
ht+1 = fθ(ht, xt+1) and L = E
" T
X
t=1
Lt(ht)
#
.
(1)
The gradient of the instantaneous loss Lt with respect to the parameters θ is then equal to
dLt
dθ = ∂Lt
∂ht
dht
dθ = ∂Lt
∂ht
X
t′≤t
dht
dht′
∂fθ
∂θ (ht′−1, xt′)
(2)
In the equation above, we used ∂to denote partial derivatives and d for total derivatives. Using this
notation enables us to distinguish between ∂htLt, which corresponds to the error backpropagated from
the current loss term to the hidden state through the readout function, and dhtL, which accumulates
the errors that are backpropagated through the future hidden state values. In particular, ∂htL = ∂htLt
and dhtL = ∂htLt(ht) + P
t′>t dhtLt′(ht′). When stacking several recurrent layers on top of each
other, ∂htL corresponds to the current error being backpropagated propagated to the hidden state ht
through the hierarchy of the network and dhtL to future error signals backpropagated through the
recurrence.
Early work [5] highlighted the difficulty for gradient descent to make recurrent neural networks
remember past inputs that will later useful to produce a desired behavior. This is due to the fact that
error signals flowing backward in time tend to either explode or vanish. The key quantity is
dht
dht′ =
t−1
Y
i=t′
∂hi+1
∂hi
=
t−1
Y
i=t′
∂fθ
∂h (hi, xi+1).
(3)
One can remark that this quantity exponentially converges to 0 when the spectral radius of the
Jacobian ∂hfθ is upper bounded by a constant strictly smaller than 1, and can exponentially explode
if there exists some component bigger than 1. The error signal at time t backpropagated to time
t′ behaves similarly, as dht′Lt = ∂htLt dh′
tht. Gradient-based learning of long-term memories is
thus difficult: the contribution of past hidden states to the current loss becomes either negligible or
predominant as the time span considered increases.
Since then, the analysis has been refined [6–8] and the development of recurrent architectures has
mostly been driven by the desire to solve this pathological issue. Most famously, the LSTM [3]
unit, and later on the GRU [23], solve this problem by using memory neurons that facilitate direct
information storage and retrieval, and by the same way error backpropagation. Other approaches to
solving this problem, to name a few, involve gradient clipping [24, 8], activity normalization [25–
27], careful weight initialization [28, 29] or enforcing architectural constraints such as hierachical
processing [30, 31], orthogonal weight matrices [32–34] and oscillations [35–37].
2
The curse of memory
According to common deep learning wisdom, it is often believed that solving the vanishing and
exploding gradients problem enables recurrent neural networks to learn long-term dependencies. We
challenge this view and question: is solving those issues really enough to ensure well-behaved loss
landscapes? We answer negatively by showing that gradients can explode as the memory of the
network increases, even when the dynamics of the network remains stable.
2.1
Intuition
Recurrent neural networks have something special: the very same update function fθ is applied over
and over. Therefore, modifying the parameters θ will not only influence one update, as changing
the weights of a given layer in a feedforward neural network would, but all. As the memory of the
network increases, the hidden states keep a trace of the effect of more updates. Hidden states thus
become increasingly sensitive to parameter changes. This is the curse of memory. We borrow the
2

A
B
C
0
0.99
0.9999
103
107
1011
1015
Variance
variance
0.0
0.9
0.99
0.999
1.0
0.0
0.5
1.0
10 6
10 4
10 2
100
102
Loss
0
*
10 5
10 3
10 1
101
*
0.99
0.98
0.96
0.92
0.84
0.68
0.36
Figure 1: Optimization of recurrent neural networks gets harder as their memory increases.
A. Evolution of the variance of dλht as a function of the recurrent parameter λ and of the input x
auto-correlation decay rate ρ, when ht+1 = λht + xt. As the memory of the network increases
(λ →1), ht becomes more sensitive to changes in λ, particularly as the elements in the input sequence
become more correlated (ρ →1). The explosion of dλht is faster than the one of ht, as highlighted
with the grey line obtained for ρ = 1. See Section 2.2 for more detail. B, C. Illustration of the
phenomenon on the toy one-dimensional teacher-student task of Section 4.1, in which the teacher
is parametrized by a real number λ∗and the student by a complex number λ. In B., λ varies on the
positive real axis and it varies on the circle of radius λ∗parametrized by θ in C. The loss becomes
sharper information is kept longer in memory, making gradient-based optimization nearly impossible.
term from [38, 39], and note that Martens and Sutskever [40] hypothesized that such a phenomenon
could arise in RNNs and hinder their optimization.
Let us formalize our intuition and consider the sensitivity of the hidden state ht on the parameters θ:
dht
dθ =
X
t′≤t
dht
dht′
∂fθ
∂θ (ht′−1, xt′).
(4)
When information stays in the network’s memory for longer, the number of non-negligible Jacobian
dht′ht terms increases. As a result, the magnitude of this sensitivity increases when the network
encodes longer-term dependencies, and learning θ becomes trickier. It is critical to note that this
phenomenon arises even when exploding gradients are removed from the picture by constraining the
eigenvalues of the recurrent Jacobian to be smaller than one and ensuring that the network dynamics
remains stable. The rest of this section will be dedicated to studying quantitatively this behavior.
2.2
Signal propagation in linear diagonal recurrent neural networks
We study how hidden state and gradient magnitudes evolve as the network encodes longer-term
dependencies. Ideally, we would like these quantities not to vanish or explode. This property improves
the conditioning of the loss landscape [41] and hence eases optimization [42, 43]. We make the
following assumptions:
a) Linear diagonal recurrent neural networks. We restrict ourselves to update functions of the
form fθ(ht, xt+1) = λ ⊙ht + xt+1 with λ a vector of the size of ht and ⊙the element-wise
product. For ease of exposition, we present results for real-valued λ here; see Appendix A.2 for
the complex-valued setting. While this assumption is strong, it allows us to identify some crucial
mechanisms and it is satisfied for some models like S4 [17] and LRUs [20]. We later show our
analysis can model some features of more sophisticated networks.
b) Infinite time horizon. We consider infinite sequences and initialize the network dynamics at
t0 = −∞. It simplifies our calculations while being a reasonable assumption when the sequences
considered are longer than the characteristic timescales of the dependencies we want to learn.
c) Wide-sense stationarity. We assume the different quantities that the network receives, which
include the inputs xt, to be wide-sense stationary (WSS). A random process Xt is said to be
WSS if its auto-correlation function is independent of time, that is, for all t ∈Z and ∆∈Z,
EX [Xt+∆Xt] =: RX(∆), where EX denotes the expectation over the data. It corresponds to
assuming that the statistics of the different sequences within the data are invariant to time shifts.
We are now equipped to analyze signal propagation in one recurrent layer, both in the forward and
backward passes. We show that both hidden states and backpropagated errors explode as |λ| →1.
3

Forward pass.
Here, we are interested in understanding how the hidden state variance E[h2
t]
evolves as a function of the characteristic timescale of the network encoded in λ as well as the input
auto-correlation function Rx. After a calculation that we defer to Appendix A.2, we obtain
E

h2
t

=
1
1 −λ2

Rx(0) + 2
X
∆≥1
λ∆Rx(∆)

.
(5)
Importantly, the variance of the hidden state goes to infinity as longer-term dependencies are encoded
within the network, that is |λ| →1. Additionally, the divergence speed depends on the input data
distribution: it increases as consecutive time steps in the input distribution become more correlated
(i.e., less of the Rx(∆) terms are negligible). This behavior already highlights potential difficulties of
gradient-based learning of deep neural networks containing linear recurrent layers as the variance of
neural activity can become arbitrarily large, hindering learning abilities of deeper layers.
Backward pass.
Let us first derive the gradient of the loss with respect to λ. Using the chain rule
we have dλL = P
t ∂htL dλht. We thus seek to understand how dλht behaves. We remark that
dλht+1 = λdλht + ht so that dλht is a low a pass filtered version of the hidden state, which is itself
a low pass filter version of the inputs. It therefore comes as no surprise that the variance of dλht
diverges faster than the one of ht when |λ| →1. More precisely, we get
E
"dht
dλ
2#
=
1 + λ2
(1 −λ2)3

Rx(0) + 2
X
∆≥1
λ∆Rx(∆)

+
2
(1 −λ2)2

X
∆≥1
∆λ∆Rx(∆)

. (6)
We plot the exact behavior of this quantity when the auto-correlation of x satisfies Rx(∆) = ρ|∆| on
Figure 1 and refer the interested reader to the appendix for a derivation of Equation 6. Framed more
generally, the hidden state of the network, and thus its final output, becomes increasingly sensitive to
changes in recurrent parameters as the network reaches the edge of dynamical stability (|λ| →1).
The last quantity that we need to consider is the error that is backpropagated to the inputs x of the
recurrent layer. It can be observed that the backward pass is dual to the forward pass in the sense that
it is a recurrent process that receives backpropagated errors ∂htL and it runs in reverse time:
dL
dxt
= dL
dht
∂ht
∂xt
=
dL
dht+1
∂ht+1
∂ht
+ ∂L
∂ht
= λ dL
dht+1
+ ∂L
∂ht
,
(7)
in which we made use of ∂xtht = 1. It follows that the analysis we did for the forward pass also holds
here. Crucially, this implies that the explosion behavior will be most significant for the recurrent
parameters rather than for potential input or readout weights.
2.3
Extending the analysis to the non diagonal case
We now generalize our results to fully connected linear recurrent neural networks of the form
ht+1 = Aht + xt. For the sake of the analysis, we assume that A is complex diagonalizable, that is
there exists a complex-valued matrix P and a complex-valued vector λ such that A = Pdiag(λ)P −1.
Note that this occurs with probability one under random initialization of A [20]. In this case,
ht = Phdiag
t
with hdiag
t+1 = diag(λ)hdiag
t
+ P −1xt+1
(8)
and
dht
dA = ∂ht
∂P
∂P
∂A +
∂ht
∂hdiag
t
dhdiag
t
dλ
∂λ
∂A +
∂ht
∂hdiag
t
dhdiag
t
dP −1
∂P −1
∂A .
(9)
From the previous analysis, we know that the dominating term in the limit |λ| →1 among ∂P ht,
dλht and d−1
P ht is dλht, as P and P −1 act as readout and input weights. Given that all other
terms do not directly depend on the magnitude of λ, we have that dAht ≃∂hdiag
t
ht dλhdiag
t
∂Aλ; c.f.
Appendix A.2.3 for formal statements. This has two consequences: First, the sensitivity of ht on
A will explode as longer memories are encoded and this directly comes from the eigenvalues of A.
Second, as each entry of A typically impacts all eigenvalues of the matrix, the explosion behavior
will be distributed across all entries, whereas it was concentrated on the eigenvalues for the diagonal
case. We will later observe that this has significant practical consequences and partly explains why
fully connected linear RNNs are difficult to train. As a side note, we remark that enforcing the matrix
A to be orthogonal solves vanishing and exploding gradient issues but these weights may remain
sensitive to learn because of the curse of memory.
4

3
Mitigating the curse of memory
We have discussed the sensitivity of recurrent networks to parameter updates. Given this problem,
how can it be mitigated? Recurrent networks with diagonal connectivity are particularly well suited
for this purpose. Besides enabling control over the Jacobian and avoiding exploding gradients, they
facilitate the mitigation of the curse of memory. In this context, we demonstrate that state-space
models and gated RNNs inherently incorporate such mechanisms.
3.1
A solution: normalization and reparametrization
Both forward and backward passes explode as the network encodes longer memories. When ht+1 =
λht +xt+1, we argue that it is decently easy to mitigate this effect. We aim to keep E[h2
t], E[(dλht)2]
and E[(dxtht)2] independent of λ, similarly to initialization schemes ensuring the magnitude of neural
activity remains constant in deep networks [44, 45] and independent of the layer width [42, 46, 43].
0.0
0.9
0.99
0.999
1.0
A
B
0
0.99
0.9999
102
105
108
Variance
without
0
0.99
0.9999
10 1
102
105
Variance
without
and exp
Figure 2: Illustration of the effects of
normalization and reparametrization.
It can effectively control the magnitude
of A. E[h2
t] and B. E[(dλht)2] over all
λ values when the input auto-correlation
satisfies Rx(∆) = ρ|∆| with ρ = 0,
but does not manage do to so for other
type of distributions (ρ ̸= 0).
Here,
we use γ(λ) =
√
1 −λ2, decouple it
from λ when differentiating, and take
λ = exp(−exp(ν)) as in [20]. The grey
line indicates the value the two quanti-
ties take without any normalization and
reparametrization, when ρ = 1.
Input normalization.
A simple way to enforce E[h2
t]
to stay constant is to introduce a scaling factor γ(λ),
applied to the inputs a neuron receives, that satisfies
γ(λ)2E[h2
t] = Θ(1). Given that the backward propagation
of output errors to inputs is dual to the forward pass, the
role of γ has in the backward pass will be similar. The
value γ needs to take therefore both depends on the input
distribution to normalize the forward pass, as well as on
the output error distribution to normalize the backward
pass. Perfect normalization is likely unrealistic, but some
normalization can help, as shown in Figure 2.A.
Eigenvalue reparametrization.
We are now left with
keeping the gradient of the loss with respect to λ under
control. Input normalization partly reduces the memory-
induced exploding effect, but not entirely as the variance
of dλht is much larger than the one of ht (c.f. Fig.1.A).
Reparametrization can close that gap. Indeed, if λ is
parametrized by ω, we have that dωht = dλhtdωλ. Choos-
ing a parameterization that is more and more granular as
λ goes to 1 thus helps in keeping dωht constant. As-
suming γ is independent of λ for simplicity, achieving
E[(dωht)2] = Θ(1) requires solving the differential equa-
tion γ(λ)2λ′(ω)2E[(dλht)2] = 1. While deriving a uni-
versal optimal parametrization is again unrealistic due to
dependency on the input distribution, reparametrization
definitely helps, as shown in Figure 2.B. Figure 6 illustrates
how it can affect the loss landscape.
What about complex numbers?
We have not yet discussed the case λ ∈C, relevant for SSMs
such as S4 [17]. We extend our analysis to complex λs in Appendix A.3.2 and highlight that they are
difficult to parametrize correctly. Briefly, our analysis reveals that if λ is parametrized as ν exp(iθ),
parametrization of θ must depend on the one of ν, but the reverse is not necessary. However, doing
so does not hurts learning, as we exemplify in Appendix A.3.2.
3.2
Several RNN architectures implicitly alleviates the curse of memory
State-space models, as well as gated RNNs, feature some form of normalization and reparametrization
which facilitates signal propagation. We discuss how below.
State-space models.
SSMs are originally motivated as discretizations of the continuous-time
differential equation ˙h = Ah + Bx [16]. Naïve discretization of the differential equation yields
ht+1 = (Id + dtA)ht + dtBxt+1 which already acts as some input normalization when dt is small.
5

More elaborate discretization schemes, such as the zero-order hold, effectively reparametrize the
A matrix, e.g. with exp(dtA). Here, diagonalization arises from computational efficiency and
simplicity reasons [18]. While such models can can approximate any smooth mappings [47, 48],
their expressivity remains limited [49]. The next generation of these models, including Mamba [21],
incorporates input-dependent gates which modulate dt depending on the input xt. The theory we
developed above does not strictly apply to this setting as dt is not constant. However, since the rest
of the model’s structure remains unchanged we expect this behavior, and thus its remedies, to remain.
Gated RNNs.
While the original motivation behind gated RNNs such as LSTMs [3] or GRUs
[23] largely differs from the one of SSMs, they share similar mechanisms. In these networks, the
memory content stored in hidden neurons can be erased through a forget gate, and incoming inputs
can selectively be written in memory through an input gate. Mathematically, this corresponds to
hidden state updates of the form ht+1 = ft+1 ⊙ht + it+1 ⊙xt+1, with the forget ft+1 and input it+1
gates being independent non-linear functions of xt+1 and ht. The forget gate is akin to λ and usually
involves a sigmoid non-linearity, which has a similar effect in the backward pass as reparametrizing
λ. The input gate can act as an input normalization depending on the initialization of the network
or if is coupled to the forget gate as in the GRU (ft = 1 −it) [29]. Importantly, the gates here
depend on the hidden states and thus make the Jacobian ∂htht+1 non diagonal. Yet, we argue that
these architectures still have a bias towards diagonality. Indeed, the contributions of the hidden state
through the forget and input gates are indirect, and they can be ignored when the weights connecting
the hidden states to the gates are small. We therefore get back to the setting we discussed in the
previous paragraph; we confirm this intuition in Section 5. In regimes in which this approximation
does not hold, studying signal propagation requires a much more sophisticated anaylsis than the one
we have done here [50].
4
A linear teacher-student analysis
We consider a teacher-student task with linear recurrent networks [51]. This is arguably the simplest
setting in which one can train recurrent networks, and yet, as we shall see, it is remarkably complex.
We first turn to the one-dimensional setting to provide an intuitive illustration of how the curse of
memory and vanishing gradients interplay. We then address the general setting and observe that linear
networks indeed suffer from the curse of memory, and that the remedies we studied in the last section
are effective. We additionally find that diagonality greatly modifies the structure of the loss landscape
and helps optimizers with adaptive learning rates to compensate for an eventual increased sensitivity.
4.1
The one-dimensional case
We first consider a student and teacher following the one-dimensional dynamics ht+1 = λht + xt+1,
with complex-valued parameter λ for the student and λ∗for the teacher. For simplicity, we draw
xt+1 from a normal distribution with mean 0 and standard deviation 1 and note that other input
distributions do not qualitatively change the results. The performance of the student is measured by a
loss L that averages the per time-step losses Lt := 1
2|ht −h∗
t |2 over the entire sequence.
This simple model already captures two key difficulties of gradient-based learning of recurrent neural
networks. In Figure 1, we plot the resulting loss landscape for different λ∗values, when λ evolves
on the positive part of the real axis (Fig. 1.B) and when it evolves on the circle of radius |λ∗| in
the complex plane (Fig. 1.C). We restrict λs to have absolute values smaller than one: exploding
gradients are out of the picture. Still, two difficulties for gradient-based learning appear here. On one
side, vanishing gradients lead to flat loss regions that are hard to escape. On the other side, the loss
sharpens as the student encodes longer memories because of the curse of memory. As a consequence,
gradient-based optimization is extremely tedious, already in this simple example.
4.2
Diagonal connectivity simplifies optimization
We now move to the general case in which the teacher evolves according to
ht+1 = Aht + Bxt+1 and yt = Cht + Dxt.
(10)
with ht ∈Rn, xt ∈R drawn i.i.d. from N(0, 1), A ∈Rn×n, B ∈Rn×1, C ∈R1×n and D ∈R1×1.
Here both inputs and outputs are scalars.
6

0.32
0.84
0.96
0.99
10
7
10
5
10
3
10
1
Loss
RNN
LRU
RNN
block
diagonal
more
neurons
complex
numbers
 norm.
LRU
10
4
10
3
10
2
10
1
A
B
Figure 3: LRUs are better at replicating
a teacher’s behavior than linear RNNs.
A. As the teacher encodes longer dependen-
cies (ν →1), the linear RNN struggles to
reproduce it, but not the LRU. B. An abla-
tion study (ν = 0.99) reveals that this gap
mainly comes from having a 2 × 2 block
diagonal weight matrix, and then replacing
those blocks with complex numbers.
Given the intuition we have developed so far, we expect fully connected linear recurrent neural
networks to struggle in solving the task when the teacher encodes longer memories, not only because
of exploding gradients but also due to the curse of memory. Conversely, diagonality facilitates
eigenvalue reparametrization to avoid exploding gradients and make them better behaved. We run
the following experiment to verify this intuition. We draw random teachers with hidden dimension
n = 10 and transform the complex eigenvalues of the recurrent matrix A to have magnitudes close
to a value ν that we control1. The larger ν is, the longer the memories encoded by the teacher are.
We train a linear RNN, as well as an LRU [20], with hidden dimension 64 on this task. The students
are therefore largely overparametrized. We chose the LRU architecture to represent SSMs due to
its simplicity. This architecture uses input normalization and an exponential reparametrization of
the eigenvalues, similar to what we analyze in Section 3. Both network are trained using the Adam
optimizer [52] and cosine annealing schedule for 10k steps, on batches of size 128. The sequences
contain 300 time steps. Learning rates are tuned separately for each method and training distribution.
The results, which we plot in Figure 3.A, confirm our intuition: LRUs significantly outperform linear
RNNs when long memories have to be learned, despite having 10 times fewer parameters.
Next, we wonder which design choices behind the LRU architecture are crucial to this performance
improvement. To this end, we interpolate between a linear RNN and an LRU in the following way:
First, we restrict the weight matrix of the linear RNN to a block diagonal with blocks of size 2. Each
of such blocks can represent a complex number, so 32 complex numbers in total. We additionally
double the number of hidden neurons. Second, we change those 2 × 2 blocks (and their input and
output weights) to be complex numbers. Finally, we add the γ input normalization and the exponential
parametrization to obtain the final LRU architecture. We report the results of this experiment in
Figure 3.B. We find that most of the gap comes from the introduction of complex numbers and can
be partially reduced by making the weight matrix block diagonal. Interestingly, those two changes
reduce the number of parameters the model has and slightly reduce the model expressivity so an
explanation of this behavior is likely to be related to the optimization properties of those models. We
confirm this hypothesis in the next section.
4.3
On the importance of adaptive learning rates
So far, our results highlight the importance of directly parametrizing the complex eigenvalues of
the recurrent connectivity matrix. This parametrization does not mitigate any exploding behavior
but modifies the loss landscape, making it possible for optimizers with adaptive learning rates to
compensate for these behaviors. To demonstrate this, we study the Hessian of the loss:
d2L
dθ2 =
X
t
Ex
"
dht
dθ
∂2Lt
∂h2
t
dht
dθ
⊤
+ ∂Lt
∂ht
d2ht
dθ2
#
.
(11)
If the network can perfectly fit the target data, which is the case here, the second term vanishes at
optimality. We plot the Hessian at optimality in Figure 4.A and B for a standard linear recurrent
network and one with complex diagonal parametrization, both with 4 hidden neurons (ν = 0.99).
We observe that the eigenvalue spectra are similar for the two architectures, both exhibiting large
terms that characteristic of the curse of memory, which makes learning with stochastic gradient
1We draw each entry of A from N(0, 1/√n), complex diagonalize it, and apply the transformation x 7→
ν + (1 −ν)tanh(x) to the absolute values of the eigenvalues.
7

A
B
C 
D
A
D
B
C
D
Eigenvalue
Hessian
Top 10
eig. vect.
re
re
im
A
B
C
D
Bre Bⁱm
Cre Cⁱm D
Bre
Bⁱm
Cre
Cⁱm
D
im
re
Bre Bⁱm Cre Cⁱm
D
im
A
B
C
D
Effective LR
Effective LR
10 3
10 4
10 5
10 4
10 6
10 6
10 8
10 10
10 7
100
107
106
103
100
0
100
103
106
1
0
1
0
10
D
10 7
100
107
104
102
100
0
100
102
104
1
0
1
0
10
20
Index eigenvalue
Index eigenvalue
20
Figure 4: Differences in learning abilities between fully connected and complex diagonal linear
RNNs are due to a better structure of the loss landscape. A, B. Hessian of the loss at optimality,
its 10 eigenvectors with greatest eigenvalues and its eigenspectra for a fully connected RNN (A) and
a complex diagonal one (B). The spectra are almost the same but top eigenvectors are concentrated
on few coordinates for the complex diagonal one but not for the fully connected one. C, D. This
structure makes it possible for Adam to efficiently deal with the extra sensitivity, as shown with the
effective learning rates that it uses at the end of learning. For the fully connected one (C), Adam
uses very low learning rates to compensate for the sensitivity, whereas it can use larger ones for
the complex diagonal one without hindering training stability. The horizontal grey line shows the
learning rate used, which is here 10−3.
descent almost impossible2. However, their structure differs. For the fully connected linear RNN,
the top eigenvectors are distributed over many coordinates, whereas they are concentrated on a few
coordinates for the complex diagonal one. This feature aids adaptive optimization [e.g., 56]: adapting
to large curvature is much easier for Adam when the pathological directions are aligned to the
canonical basis. This is what we observe in practice. In Figure 4.C and D, we compare the effective
learning rate used by Adam, which we compute by providing a vector of ones to the optimizer. For the
dense linear RNN, the adaptive learning rates cannot compensate for the intricate coupling between
components, resulting in very small learning rates. Conversely, the sensitivity of complex diagonal
RNNs is concentrated on few parameters, which adaptive learning rates can compensate for, leading
to targeted and overall larger learning rates, significantly speeding up learning. As a side note, the
complex eigenvalues of the teacher come in conjugate pairs. However, during training, the complex
values of the complex RNN are not conjugates of each other, thereby increasing Hessian diagonality.
Finally, performing this analysis for the LRU, we find that the Hessian spectrum is similar to the
diagonal setting and that the exploding dimensions of the Hessian are almost exclusively due to the
angle parameter, consistently with our theoretical analysis; see Figure 9.
Before concluding this section, we investigate whether there exist eigenvalue distributions that break
the diagonal structure of the Hessian, making optimization harder and increasing the pressure on
eigenvalue reparametrization. We theoretically prove in Appendix B.2 the intuitive result that the
more concentrated the eigenvalues are, the less diagonal the Hessian is. As a consequence, the gap
between complex-valued diagonal networks and LRUs widens, but the former still greatly outperform
their fully-connected counterpart; see Figure 10.
2The gradient Lipschitz constant L of the loss equals the maximum Hessian eigenvalue [53]. This quantity
sets a bound 2/L for the maximum globally stable learning rate. While convergence might happen in a subspace,
it is generally aligned with the top Hessian eigenspace near the solution [54, 55].
8

lay. 1
lay. 4
cRNN
LRU
cRNN
LSTM
lay. 1
lay. 4
lay. 1
lay. 4
rest
no norm
rest
layer
norm
rest
LSTM
ff
LSTM
A
B
C
0
0.9
0.99
10 5
10 1
103
107
Variance
0
0.9
0.99
101
103
105
Variance
0
0.9
0.99
10 7
10 2
103
108
Variance
LRU
rest
Figure 5: Signal propagation in deep recurrent networks at initialization is consistent with our
theory. A. E[h2
t] after the first and the fourth layer, as a function of the memory parameter ν, for
complex-valued diagonal RNN (cRNN), LRU, and LSTM recurrent layers. The input normalization
present in the LRU effectively keeps neural activity bounded. B. Comparison of the evolution of
E[dθh2
t] for the different recurrent layers and specific groups of parameters. For the complex diagonal
RNN, the gradients of all parameters explode, whereas only the ones of θ explode for the LRU.
Regarding the LSTM, the mangnitude of the gradients does not depend on ν, with the LSTM-specific
parameters exhibiting smaller gradients than the feedforward (ff) ones. C. Layer normalization keeps
the overall gradient magnitude under control. Batch normalization yields similar results.
5
Signal propagation in deep recurrent networks at initialization
The ultimate goal of our theoretical quest is to gain practical insights into the training of recurrent
networks. Specifically, we aim to verify whether the trends established theoretically and in controlled
experiments hold in practice, by studying signal propagation at initialization.
We provide sequences of 512 text tokens as input to deep recurrent networks that contain four blocks
of 256 hidden neurons each and use a next-token prediction loss to measure their performance. Each
block consists of a recurrent layer followed by a feedforward gated linear unit [57]. By default, there
are no normalization layers in this architecture. More details can be found in Appendix C.1. We
empirically study how E[h2
t] and E[dθh2
t] evolve when the memory of the recurrent layer, controlled
by ν, increases. We compare three different recurrent layers: a complex-valued diagonal RNN
(cRNN), a LRU and a LSTM initialized with the chrono initialization [29].
The results are consistent with our theory. Complex-valued RNNs suffer from the curse of memory.
LRUs almost perfectly mitigate this effect in the forward pass (Fig. 5.A) as well as in the backward
pass (Fig. 5.B), except for the angle parameter θ, as expected. We also wonder whether layer
normalization can replace the input normalization and reparametrization of the LRU. We find that it
mitigates the memory-induced gradient explosion at the macroscopic level (Fig. 5.C), but it likely
kills any learning signal for the smallest eigenvalues. Finally, the LSTM manages to keep the
gradient norm constant over different level of memory, consistently with the intuition we developed
in Section 3.2, although the LSTM-specific parameters exhibit smaller gradients than the feedforward
parameters.
6
Conclusion
Vanishing and exploding gradients complicate learning recurrent networks, but solving these problems
is not enough. We uncovered yet another difficulty of training such networks, which is rooted in
their iterative nature and arises at the edge of dynamical stability. Reparametrizations and adaptive
learning rates can effectively mitigate this behavior in practice, and diagonalizing the recurrence
simplifies both. Our analysis additionally reveals the complexity of learning the angle of complex
eigenvalues, which may explain why complex numbers were not found to be useful in most recent
state-space model architectures [21, 22].
A side finding of our study is the symbiosis between independent modules, which are here neurons
and can more be more generally small heads, with adaptive learning rate optimizers in linear recurrent
networks. Such a design pattern has promising properties: it facilitates online learning [58] and
compositional generalization [59], allows for high level of parallelization [22], and matches, at a high
level, the modular organization of the cortex in cortical columns [60]. Understanding how to increase
the expressivity of small linear modules while keeping their great optimization properties constitutes
a promising avenue for future research.
9

Acknowledgments
The authors thank Robert Meier, João Sacramento, Guillaume Lajoie, Ezekiel Williams, Razvan
Pascanu, Imanol Schlag and Bobby He for insightful discussions. Nicolas Zucchet was supported by
an ETH Research Grant (ETH-23 21-1) and Antonio Orvieto acknowledges the financial support of
the Hector Foundation.
10

References
[1] David E Rumelhart, Paul Smolensky, James L McClelland, and G Hinton. Sequential thought processes in
PDP models. Parallel distributed processing: explorations in the microstructures of cognition, 2, 1986.
[2] Jeffrey L Elman. Finding structure in time. Cognitive science, 14(2), 1990.
[3] Sepp Hochreiter and Jürgen Schmidhuber. Long Short-Term Memory. Neural Computation, 9(8), 1997.
[4] Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. Sequence to sequence learning with neural networks. In
Advances in Neural Information Processing Systems, 2014.
[5] Sepp Hochreiter. Untersuchungen zu dynamischen neuronalen Netzen. Diploma thesis, Technische
Universität München, 1991.
[6] Yoshua Bengio, Patrice Simard, and Paolo Frasconi. Learning long-term dependencies with gradient
descent is difficult. IEEE transactions on neural networks, 5(2), 1994.
[7] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jurgen Schmidhuber. Gradient flow in recurrent
nets: the difficulty of learning long-term dependencies. In A field guide to dynamical recurrent networks.
IEEE, 2001.
[8] Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. On the difficulty of training recurrent neural
networks. In International Conference on Machine Learning, 2013.
[9] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning
to align and translate. In International Conference on Learning Representations, 2015.
[10] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz
Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information Processing
Systems, 2017.
[11] Quentin Fournier, Gaétan Marceau Caron, and Daniel Aloise. A practical survey on faster and lighter
transformers. ACM Computing Surveys, 55(14s), 2023.
[12] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret. Transformers are RNNs:
fast autoregressive Transformers with linear attention. In International Conference on Machine Learning,
2020.
[13] Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. FlashAttention: fast and memory-
efficient exact attention with IO-awareness. arXiv preprint arXiv:2205.14135, 2022.
[14] William Fedus, Barret Zoph, and Noam Shazeer. Switch Transformers: scaling to trillion parameter models
with simple and efficient sparsity. Journal of Machine Learning Research, 2022.
[15] Shuming Ma, Hongyu Wang, Lingxiao Ma, Lei Wang, Wenhui Wang, Shaohan Huang, Li Dong, Ruiping
Wang, Jilong Xue, and Furu Wei. The era of 1-bit LLMs: all Large Language Models are in 1.58 bits.
arXiv preprint arXiv:2402.17764, 2024.
[16] Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher Ré. Combining
recurrent, convolutional, and continuous-time models with linear state-space layers. In Advances in Neural
Information Processing Systems, 2021.
[17] Albert Gu, Karan Goel, and Christopher Ré. Efficiently modeling long sequences with structured state
spaces. In International Conference on Learning Representations, 2022.
[18] Ankit Gupta, Albert Gu, and Jonathan Berant. Diagonal state spaces are as effective as structured states
spaces. In Advances in Neural Information Processing Systems, volume 35, 2022.
[19] Jimmy T.H. Smith, Andrew Warrington, and Scott W. Linderman. Simplified state space layers for sequence
modeling. In International Conference on Learning Representations, 2023.
[20] Antonio Orvieto, Samuel L Smith, Albert Gu, Anushan Fernando, Caglar Gulcehre, Razvan Pascanu, and
Soham De. Resurrecting recurrent neural networks for long sequences. In International Conference on
Machine Learning, 2023.
[21] Albert Gu and Tri Dao. Mamba: linear-time sequence modeling with selective state spaces. arXiv preprint
arXiv:2312.00752, 2023.
11

[22] Soham De, Samuel L. Smith, Anushan Fernando, Aleksandar Botev, George Cristian-Muraru, Albert Gu,
Ruba Haroun, Leonard Berrada, Yutian Chen, Srivatsan Srinivasan, Guillaume Desjardins, Arnaud Doucet,
David Budden, Yee Whye Teh, Razvan Pascanu, Nando De Freitas, and Caglar Gulcehre. Griffin: mixing
gated linear recurrences with local attention for efficient language models. arXiv preprint arXiv:2402.19427,
2024.
[23] Kyunghyun Cho, Bart van Merrienboer, Dzmitry Bahdanau, and Yoshua Bengio. On the properties of
neural machine translation: encoder-decoder approaches. In Proceedings of SSST-8, Eighth Workshop on
Syntax, Semantics and Structure in Statistical Translation, 2014.
[24] Tomas Mikolov. Statistical language models based on neural networks. PhD thesis, Brno University of
Technology, 2012.
[25] Sergey Ioffe and Christian Szegedy. Batch normalization: accelerating deep network training by reducing
internal covariate shift. In International Conference on Machine Learning, 2015.
[26] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer normalization. In Neural Information
Processing Systems - Deep Learning Symposium, 2016.
[27] Tim Cooijmans, Nicolas Ballas, César Laurent, Ça˘glar Gülçehre, and Aaron Courville. Recurrent batch
normalization. In International Conference on Learning Representations, 2017.
[28] Quoc V. Le, Navdeep Jaitly, and Geoffrey E. Hinton. A simple way to initialize recurrent networks of
rectified linear units. arXiv preprint arXiv:1504.00941, 2015.
[29] Corentin Tallec and Yann Ollivier. Can recurrent neural networks warp time? In International Conference
on Learning Representations, 2018.
[30] Salah El Hihi and Yoshua Bengio. Hierarchical recurrent neural networks for long-term dependencies. In
Neural Information Processing Systems, 1995.
[31] Asier Mujika, Florian Meier, and Angelika Steger. Fast-slow recurrent neural networks. In Neural
Information Processing Systems, 2017.
[32] Martin Arjovsky, Amar Shah, and Yoshua Bengio. Unitary evolution recurrent neural networks. In
International Conference on Machine Learning, 2016.
[33] Eugene Vorontsov, Chiheb Trabelsi, Samuel Kadoury, and Chris Pal. On orthogonality and learning
recurrent networks with long term dependencies. In International Conference on Machine Learning, 2017.
[34] Kyle Helfrich, Devin Willmott, and Qiang Ye. Orthogonal recurrent neural networks with scaled Cayley
transform. In International Conference on Machine Learning, 2018.
[35] T. Konstantin Rusch and Siddhartha Mishra. Coupled oscillatory recurrent neural network (coRNN): an
accurate and (gradient) stable architecture for learning long time dependencies. In International Conference
on Learning Representations, 2021.
[36] T Anderson Keller and Max Welling. Neural wave machines: learning spatiotemporally structured
representations with locally coupled oscillatory recurrent neural networks. In International Conference on
Machine Learning, 2023.
[37] Il Memming Park, Ábel Ságodi, and Piotr Aleksander Sokół. Persistent learning signals and working
memory without continuous attractors. arXiv preprint arXiv:2308.12585, 2023.
[38] Zhong Li, Jiequn Han, Weinan E, and Qianxiao Li. Approximation and optimization theory for linear
continuous-time recurrent neural networks. Journal of Machine Learning Research, 23(42), 2022.
[39] Shida Wang, Zhong Li, and Qianxiao Li. Inverse approximation theory for nonlinear recurrent neural
networks. In International Conference on Learning Representations, 2024.
[40] James Martens and Ilya Sutskever. Learning recurrent neural networks with hessian-free optimization. In
International Conference on Machine Learning, 2011.
[41] Lorenzo Noci, Alexandru Meterez, Thomas Hofmann, and Antonio Orvieto. Why do learning rates
transfer? Reconciling optimization and scaling limits for deep learning. arXiv preprint arXiv:2402.17457,
2024.
[42] Greg Yang and Edward J Hu.
Feature learning in infinite-width neural networks.
arXiv preprint
arXiv:2011.14522, 2020.
12

[43] Greg Yang, James B Simon, and Jeremy Bernstein. A spectral condition for feature learning. arXiv preprint
arXiv:2310.17813, 2023.
[44] Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural
networks. In Artificial Intelligence and Statistics, 2010.
[45] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving Deep into Rectifiers: Surpassing
Human-Level Performance on ImageNet Classification. In International Conference on Computer Vision,
2015.
[46] Greg Yang, Edward J. Hu, Igor Babuschkin, Szymon Sidor, Xiaodong Liu, David Farhi, Nick Ryder,
Jakub Pachocki, Weizhu Chen, and Jianfeng Gao. Tensor Programs V: Tuning Large Neural Networks via
Zero-Shot Hyperparameter Transfer. arXiv preprint arXiv:2203.03466, 2022.
[47] S. Boyd and L. Chua. Fading memory and the problem of approximating nonlinear operators with Volterra
series. IEEE Transactions on Circuits and Systems, 32(11), 1985.
[48] Antonio Orvieto, Soham De, Caglar Gulcehre, Razvan Pascanu, and Samuel L. Smith. Universality of
linear recurrences followed by non-linear projections: finite-width guarantees and benefits of complex
eigenvalues. In International Conference on Machine Learning, 2024.
[49] William Merrill, Jackson Petty, and Ashish Sabharwal. The illusion of state in state-space models. In
International Conference on Machine Learning, 2024.
[50] Minmin Chen, Jeffrey Pennington, and Samuel S. Schoenholz. Dynamical isometry and a mean field
theory of RNNs: gating enables signal propagation in recurrent neural networks. 2018.
[51] Moritz Hardt, Tengyu Ma, and Benjamin Recht. Gradient descent learns linear dynamical systems. Journal
of Machine Learning Research, 19(29), 2018.
[52] Diederik P. Kingma and Jimmy Ba. Adam: a method for stochastic optimization. In International
Conference on Learning Representations, 2015.
[53] Yurii Nesterov and others. Lectures on convex optimization, volume 137. Springer, 2018.
[54] Jeremy Cohen, Simran Kaur, Yuanzhi Li, J Zico Kolter, and Ameet Talwalkar. Gradient descent on neural
networks typically occurs at the edge of stability. In International Conference on Learning Representations,
2020.
[55] Guy Gur-Ari, Daniel A Roberts, and Ethan Dyer. Gradient descent happens in a tiny subspace. arXiv
preprint arXiv:1812.04754, 2018.
[56] Yan Pan and Yuanzhi Li. Toward understanding why Adam converges faster than SGD for transformers.
arXiv preprint arXiv:2306.00204, 2023.
[57] Yann N. Dauphin, Angela Fan, Michael Auli, and David Grangier. Language modeling with gated
convolutional networks. In International Conference on Machine Learning, 2017.
[58] Nicolas Zucchet, Robert Meier, Simon Schug, Asier Mujika, and João Sacramento. Online learning of
long-range dependencies. In Advances in Neural Information Processing Systems, 2023.
[59] Anirudh Goyal, Alex Lamb, Jordan Hoffmann, Shagun Sodhani, Sergey Levine, Yoshua Bengio, and
Bernhard Schölkopf. Recurrent independent mechanisms. In International Conference on Learning
Representations, 2021.
[60] V B Mountcastle. The columnar organization of the neocortex. Brain, 120(4), 1997.
[61] Christoph Boeddeker, Patrick Hanebrink, Lukas Drude, Jahn Heymann, and Reinhold Haeb-Umbach. On
the computation of complex-valued gradients with application to statistically optimum beamforming. arXiv
preprint arXiv:1701.00392, 2017.
[62] James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin,
George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and Qiao Zhang. JAX: compos-
able transformations of Python+NumPy programs, 2018. URL http://github.com/google/jax.
[63] Jonathan Heek, Anselm Levskaya, Avital Oliver, Marvin Ritter, Bertrand Rondepierre, Andreas Steiner,
and Marc van Zee. Flax: A neural network library and ecosystem for JAX, 2023. URL http://github.
com/google/flax.
13

[64] Nicolas Zucchet, Robert Meier, and Simon Schug. Minimal LRU, 2023. URL https://github.com/
NicolasZucchet/minimal-LRU.
[65] Wikimedia Foundation. Wikimedia Downloads. URL https://dumps.wikimedia.org.
[66] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of Deep
Bidirectional Transformers for Language Understanding. In Jill Burstein, Christy Doran, and Thamar
Solorio, editors, Proceedings of the 2019 Conference of the North American Chapter of the Association for
Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), 2019.
14

Appendix
Table of contents
A Theory
16
A.1
Useful lemmas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
16
A.2 The curse of memory: signal propagation analysis
. . . . . . . . . . . . . . . .
16
A.2.1
Forward pass
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
17
A.2.2
Backward pass
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
17
A.2.3
Extension to fully-connected networks . . . . . . . . . . . . . . . . . .
19
A.3
Impact of input normalization and parametrization . . . . . . . . . . . . . . . .
20
A.3.1
Real case
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
20
A.3.2
On the difficulty of parametrizing complex numbers . . . . . . . . . . .
20
B
Linear teacher-student task
21
B.1
1D setting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
21
B.1.1
Calculation of the loss . . . . . . . . . . . . . . . . . . . . . . . . . . .
21
B.1.2
Optimal normalization and reparametrization with uncorrelated inputs . .
22
B.1.3
Visualization of the effect of input normalization and reparametrization .
23
B.1.4
Learning the angle is difficult in practice: an example
. . . . . . . . . .
24
B.2
Structure of the Hessian at optimality . . . . . . . . . . . . . . . . . . . . . . .
25
B.2.1
Hessian for complex-valued variables . . . . . . . . . . . . . . . . . . .
25
B.2.2
Hessian with respect to the recurrent eigenvalues . . . . . . . . . . . . .
26
B.2.3
Hessian for different parametrizations
. . . . . . . . . . . . . . . . . .
28
B.3
Experimental details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
29
B.4
Additional analyses . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
31
B.4.1
Structure of the loss landscape for LRUs . . . . . . . . . . . . . . . . .
31
B.4.2
Concentrating eigenvalue distributions . . . . . . . . . . . . . . . . . .
31
C Signal propagation in deep recurrent neural networks at initialization
32
C.1
Experimental setup
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
32
15

A
Theory
This section introduces all the theoretical results we directly or indirectly mention in the main text, as
well as provides a proof for them.
A.1
Useful lemmas
Most, if not all the calculations, that we will be doing in this section involves infinite sums. We state
and prove two useful lemmas to simplify later calculations.
Lemma 1. For α, β ∈C satisfying |α| < 1 and |β| < 1, and (un)n∈Z a bounded sequence satisfying
u−n = un, we have
X
n,m≥0
αnβnun−m =
1
1 −αβ

u0 +
X
∆≥1
(α∆+ β∆)u∆


(12)
Proof. The proof naturally comes from separating the indices n and m in three sets: one in which
the two are equals, one in which n is larger and one in which m is larger. This gives
X
n,m≥0
αnβmun−m =
X
n=m
αnβmun−m +
X
n>m
αnβmun−m +
X
n<m
αnβmun−m
(13)
=
X
n
αnβnu0 +
X
m
αmβm X
∆≥1
α∆u∆+
X
n
αnβn X
∆≥1
β∆u−∆
(14)
=
X
n
αnβn

1 +
X
∆≥1
(α∆+ β∆)u∆


(15)
=
1
1 −αβ

1 +
X
∆≥1
(α∆+ β∆)u∆


(16)
Lemma 2. In the same conditions as Lemma 1, we have
X
n,m≥0
nmαn−1βm−1un−m = d
dα
d
dβ


1
1 −αβ

u0 +
X
∆≥1
(α∆+ β∆)u∆




(17)
Proof. This follows from remarking that
d
dα
d
dβ

X
n,m≥0
αnβmun−m

= d
dα

X
n,m≥0
mαnβm−1un−m


(18)
=
X
n,m≥0
nmαn−1βm−1un−m
(19)
and using Lemma 1 to get the final result.
A.2
The curse of memory: signal propagation analysis
We recall the assumptions that we stated in Section 2.2:
a) Linear diagonal recurrent neural networks. We restrict ourselves to networks satisfying
ht+1 = λ ⊙ht + xt+1 with λ, ht and xt complex numbers. Without loss of generality, we focus
on the one dimensional setting. We additionally consider λs with absolute values smaller than 1.
b) Infinite time horizon. We consider infinite sequences and initialize the network dynamics at
t0 = −∞.
16

c) Wide-sense stationarity. We assume the different quantities that the network receives, which
includes the inputs xt, to be wise-sense stationary (WSS). A random process Xt is said to be
WSS if its auto-correlation function is independent of time, that is, for all t ∈R and ∆∈R,
E

Xt+∆¯Xt

:= RX(∆).
A.2.1
Forward pass
Without loss of generality, we can take t = 0 given the wide-sense stationarity and infinite time
horizon assumptions. Let us first remark that we have
h0 =
X
n≥0
λnx−n
(20)
so that
E[|h0|2] =
X
n,m≥0
λn¯λmE [x−n¯x−m]
(21)
=
X
n,m≥0
λn¯λmRx(n −m)
(22)
=
1
1 −|λ|2

Rx(0) +
X
∆≥1
(¯λ∆+ λ∆)Rx(∆)

.
(23)
We used Lemma 1 to obtain the last equality. In Section 2.2, we focused on the real case ¯λ = λ,
so this formula becomes Equation 5. If we further assume that the auto-correlation of x decreases
exponentially with decay rate ρ, that is Rx(∆) = ρ|∆|, we can further simplify the last expression:
E[|h0|2] =
1
1 −|λ|2

1 +
X
∆≥1
(¯λ∆+ λ∆)ρ∆


(24)
=
1
1 −|λ|2

1 +
¯λρ
1 −¯λρ +
λρ
1 −λρ

(25)
=
1 −ρ2|λ|2
|1 −ρλ|2(1 −|λ|2)
(26)
It follows that if the inputs are i.i.d. (ρ = 0), we have E[|h0|2] = (1 −|λ|2)−1, and if the inputs are
constant equal to 1 (ρ = 1), we have E[|h0|2] = |1 −λ|−2.
A.2.2
Backward pass
Differentiating the update ht+1 = λht + xt+1 with respect to λ gives
dht
dλ = λdht−1
dλ
+ ht−1
(27)
so that
dh0
dλ =
X
n≥0
λnh−n
(28)
=
X
n≥0
λn X
m≥0
λmx−n−m
(29)
=
X
n,m≥0
λn+mx−(n+m)
(30)
=
X
n≥0
nλn−1x−n.
(31)
Note that some extra technicalities are needed to justify these equations as λ and ht are complex
valued: these formulas hold as they would in the real-valued case as ht is an holomorphic function of
λ.
17

We can now compute the variance of the sensitivity of the hidden state with respect to the parameters.
E
"
dht
dλ

2#
=
X
n≥0
X
m≥0
nmλn−1¯λm−1Rx(n −m).
(32)
Using Lemma 2 gives
E
"
dht
dλ

2#
= d
dα
d
dβ


1
1 −αβ

Rx(0) +
X
∆≥1
(α∆+ β∆)Rx(∆)




α=λ,β=¯λ
.
(33)
Differentiating this quantity as a product gives
E
"
dht
dλ

2#
=

1 + αβ
(1 −αβ)3

Rx(0) +
X
∆≥1
(α∆+ β∆)Rx(∆)

+ 0
+
α
(1 −αβ)2

X
∆≥1
∆α∆−1Rx(∆)

+
β
(1 −αβ)2

X
∆≥1
∆β∆−1Rx(∆)




α=λ,β=¯λ
,
(34)
which then simplifies as
E
"
dht
dλ

2#
= 1 + |λ|2
(1 −|λ|)3

Rx(0) +
X
∆≥1
(λ∆+ ¯λ∆)Rx(∆)


+
1
(1 −|λ|2)2

X
∆≥1
∆(λ∆+ ¯λ∆)Rx(∆)

.
(35)
Note that Equation 6 in the main text is the real-valued version of that formula.
Let us now further simplify this equation when Rx(∆) = ρ|∆|. If we use this in the differentiated
quantity before differentiating it, we get
E
"
dht
dλ

2#
= d
dα
d
dβ

1
1 −αβ

1 −ρ2αβ
(1 −ρα)(1 −ρβ)

α=λ,β=¯λ
.
(36)
Calculating this quantity manually is painful. Instead, we use the following trick. Its denominator
is rather easy to compute, it is equal to (1 −αβ)3(1 −ρα)2(1 −ρβ)2. We thus multiply it to the
derivative of the function we want to compute in order to obtain a polynomial with unknown factors,
and use polynomial regression tools to derive the resulting coefficients. Massaging the obtained
expression to make it easier to compute the closed-form value of this quantity when ρ = 0 and ρ = 1,
we get
E
"
dht
dλ

2#
= (1 −ρ)(1 + |λ|2) + ρ2(1 −|λ|2)3 + ρ(1 −ρ)|λ|2(ρ|λ|2(1 + |λ|2) −2λ −2¯λ)
(1 −|λ|2)3|1 −ρλ|4
.
(37)
This is the quantity we plot on Figure 1.A, when λ is real-valued. When ρ = 0, this quantity becomes
E
"
dht
dλ

2#
=
1 + |λ|2
(1 −|λ|2)3 ,
(38)
and it is equal to
E
"
dht
dλ

2#
=
1
|1 −λ|4 ,
(39)
when ρ = 1. Additionally, it will diverge whenever |λ| →1 when ρ < 1, and when λ →1 when
ρ = 1.
Regarding the backpropagation of errors to the inputs, the analysis we did in the main text also holds
for complex number given that ht is an holomorphic function of xt and it thus behaves as the forward
pass once replacing the input distribution with the one of output errors ∂htLt.
18

A.2.3
Extension to fully-connected networks
We now turn to the non-diagonal case. For the sake of simplicity, we assume that recurrent matrix is
complex diagonalizable and that its eigenvalues are all different. This will enable us to differentiate
the eigenvalues and the eigenvectors. We consider dynamics of the form
ht+1 = Aht + xt+1
(40)
As A is complex diagonalizable, there exists a complex-valued matrix P and a complex-valued vector
λ such that
A = Pdiag(λ)P −1
(41)
P †
:iP:i = 1 ∀i.
(42)
The linear recurrent neural network considered above is equivalent to its diagonal version
hdiag
t+1 = λhdiag
t
+ P −1xt+1
(43)
ht = Phdiag
t
.
(44)
We now differentiate ht w.r.t. to A using the diagonal parametrization and obtain
dht
dA = ∂ht
∂P
∂P
∂A +
∂ht
∂hdiag
t
dhdiag
t
dλ
∂λ
∂A +
∂ht
∂hdiag
t
dhdiag
t
dP −1
∂P −1
∂A .
(45)
dAP, dAP −1 and dAλ can be considered constant.
Intuitively, the eigenvalues and eigenvectors
move smoothly as we restricted ourselves to the case in which eigenvalues are singular. If this is
not the case, math becomes trickier as the eigenvectors are not uniquely defined. We can study the
behavior of those quantities in more detail, following Boeddeker et al. [61]:
∂λ
∂Aij
= diag

P −1 ∂A
∂Aij
P

(46)
dP
dAij
= P

F ⊙

P −1 ∂A
∂Aij
P

(47)
The F introduced in the last equation is equal to
Fij :=

1
λj−λi
if i ̸= j
0
otherwise.
(48)
Importantly, those two quantities do not grow to infinity as the absolute value of the eigenvalues goes
to 1, which means that we can consider those derivatives to be independent of |λ| for the sake of our
analysis. Note that the previous argument assumes that eigenvalues do not collapse.
dλhdiag
t
is the dominating term in dAht.
We wonder which of the three different terms that appear
in dAht (Equation 45) will be the dominating one as |λ| (or λ) goes to 1. In the previous paragraph,
we have shown that the derivative of P −1, P and λ can be considered constant for the sake of our
analysis. We thus focus on the other terms.
First, we have
∂ht,l
∂Pij
= hdiag
t,i 1j=l
(49)
so the magnitude of this quantity is roughly the one of hdiag
t
, which corresponds to the low pass
filtering of the inputs with different λ values.
Second, we know that ∂hdiag
t
ht does not change in magnitude as λ changes, as P remains bounded.
So, for the third term of the sum, we are left to study the behavior of dP −1hdiag
t
. We can show that it
evolves according to
dhdiag
t+1,k
dP −1
ij
= λi
dhdiag
t+1,k
dP −1
ij
+ xt+1,j if k = i
(50)
dhdiag
t+1,k
dP −1
ij
= 0 otherwise.
(51)
19

It follows that the third term in the sum also corresponds to a low pass filtering of the inputs.
Finally, we know that the second term, the one in dλhdiag
t
will grow faster to infinity as it corresponds
to two consecutive low pass filters with the same λ values (c.f. calculation above). It will thus be the
dominating term in the infinite memory limit.
A.3
Impact of input normalization and parametrization
In this section, we consider a diagonal linear recurrent neural network of the form
ht+1 = λ(ω)ht + γ(λ)xt+1
(52)
with γ(λ) the input normalization factor and λ parametrized by a vector ω. Next, we study the
effect of input normalization and reparametrization, first in the real-valued setting and then in the
complex-valued one.
A.3.1
Real case
Let us start with the forward pass: as
ht =
X
n≥0
λnγ(λ)xt−n,
(53)
γ rescales the value the hidden state takes. To avoid any explosion behavior, we thus ideally want γ
to be the inverse of value of E[(ht)2] without normalization, which we have computed in Equation 23.
The same behavior holds for the backpropagation of errors to the inputs as
dL
dxt
= γ(λ)

λ dL
dxt+1
+ ∂L
∂ht

.
(54)
We now move to the impact of the parametrization. To simplify the calculation, we will ignore the
dependency of γ on λ when differentiating it. This can easily be done in automatic differentiation
software by removing this dependency from the computational graph with γ(stop_gradient(λ)). We
then have
dht
dω = dht
dλ
dλ
dω
(55)
and dλht which is rescaled by γ compared to the calculation we did above. As a consequence, both
the input normalization and the parametrization can help to mitigate the curse of memory.
A.3.2
On the difficulty of parametrizing complex numbers
We now extend the previous analysis to the complex case, and take a polar parametrization of λ:
λ(ω) = ν(ω) exp(iθ(ω)). The effect of the input normalization does not change when moving to
complex numbers. The role of the reparametrization is however a bit more subtle. As ht is an
holomorphic function of λ, we have d¯λht = 0 and
dht
dω = dht
dλ
dλ
dω = dht
dλ
1
2
dν
dω exp(iθ) + i
2ν dθ
dω exp(iθ)

.
(56)
It follows that
E
"
dht
dω

2#
= 1
4E
"
dht
dλ

2# 
dν
dω exp(iθ) + iν dθ
dω exp(iθ)

2
(57)
= 1
4E
"
dht
dλ

2# 
dν
dω + iν dθ
dω

2
(58)
= 1
4E
"
dht
dλ

2#  dν
dω
2
+ ν2 dθ
dω
2
.
(59)
To simplify the analysis, we will further assume that E[|dλht|2] is only a function of ν. This asumption
holds in the case of ρ = 0 and ρ = 1, c.f. Section A.2.2, but not necessarily otherwise. To ensure that
20

that this quantity does not depend on λ, we thus want dων2E(ν) = Θ(1) and ν2dωθ2E(ν) = Θ(1).
The second means that the angle parametrization must depend on the value ν takes. Let us take
the ρ = 0 example to get an idea of what the ideal parametrization should be. First, we have
γ(λ) =
√
1 −ν2 so that
E(ν) = γ(λ)2 1 + ν2
(1 −ν2)3 =
1 + ν2
(1 −ν2)2 .
(60)
We are left with the differential equation ν′ = Θ(1 −ν2), which is for example solved with
ν = tanh(ων). Now let us look at the parametrization of θ. If we ignore the ν2 term for simplicity,
the approximate differential equation it needs to solve is dωθ = Θ(1 −ν2), which can be solved
by θ = stop_gradient(1 −ν2)ωθ. The exact detail of this calculation do not really matter as this is
heavily input distribution dependent. However, the interesting part here is that the angle parameter
must be rescaled by a function of ν. This makes intuitive sense when looking looking at the sharpness
of the loss around optimality in Figure 1.C, but this also makes the loss even flatter further away
from optimality. We will come back to this point in Section B.1.4, showing that in practice, such
a parametrization complicates the learning of the θ. Learning complex numbers is thus difficulty,
because of the angle.
B
Linear teacher-student task
This section is dedicated to detail the theoretical results behind our analysis of the teacher-student
task, present all the details necessary to reproduce our empirical experiments, and provide additional
analysis.
B.1
1D setting
B.1.1
Calculation of the loss
In this toy example, we are interested in learning a simple 1-dimensional linear recurrent neural
network which follows the dynamics
ht+1 = λht + xt+1
(61)
to reproduce the hidden state h∗
t of a teacher with recurrent parameter λ∗. Note that we here allow all
variables to be complex-valued. We take the loss to be
L(λ, λ∗) := 1
2T
T
X
t=1
Ex
h
|ht −h∗
t |2i
(62)
We assume x to be drawn from a wide-sense stationary distribution so that we can focus on studying
the behavior of one Lt(λ, λ∗) := 1
2Ex
h
|ht −h∗
t |2i
to understand the behavior of the full loss L, in
the limit of infinitely long sequences (T →∞). Moreover, to further simplify the calculations, we
assume that x is real-valued and that Rx(∆) = ρ|∆|.
Let us now proceed with the calculation:
Lt(λ, λ∗) := 1
2Ex

ht ¯ht + h∗
t ¯h∗
t −ht ¯h∗
t −¯hth∗
t

.
(63)
We have shown in Section A.2 that in the limit of t →∞,
Ex

ht ¯ht

=
1
1 −λ¯λ

1 +
ρλ
1 −ρλ +
ρ¯λ
1 −ρ¯λ

(64)
(65)
Similar derivations hold for the other three terms in the loss. Grouping them gives the exact value
of the loss. We omit the formula as it is not particularly insightful. In the case of constant inputs
(ρ = 1), we have
Lt(λ, λ∗) = 1
2

1
1 −λ −
1
1 −λ∗

2
.
(66)
21

In the case of i.i.d. inputs (ρ = 0), we have
Lt(λ, λ∗) = 1
2

1
1 −|λ|2 +
1
1 −|λ∗|2 −Re

2
1 −¯λλ∗

.
(67)
This is the loss we plot on Figure 1.B and C.
B.1.2
Optimal normalization and reparametrization with uncorrelated inputs
Having a simple closed-form solution for the value the loss takes gives us the possibility to investigate
in more detail what an optimal normalization and parametrization should be. We focus on the case
ρ = 0.
For ρ = 0, the optimal normalization is γ(λ) =
p
1 −|λ|2. Given that we now add an input
normalization to the student, we must also add it to the teacher for the student to be able to fit it. The
loss becomes
Lt = 1
2

γ(λ)
1 −|λ|2 +
γ(λ∗)
1 −|λ∗|2 −Re
2γ(λ)γ(λ∗)
1 −¯λλ∗

(68)
= 1 −Re
γ(λ)γ(λ∗)
1 −¯λλ∗

.
(69)
Next, we parametrize λ as λ = ν(ων) exp(iθ(ωθ)) and seek to find a parametrization such that, at
optimality, E[(dωνht)2] = 1 and E[(dωθht)2] = 1. Given that the student perfectly fit the teacher-
generated data at optimality and that the loss we use is the mean-squared error, this corresponds to
having d2
ωνLt = 1 and d2
ωθLt = 1.
Deriving the optimal ν parametrization.
We now compute the Hessian of the loss w.r.t. ων. First,
we can simplify our calculations by restricting ourselves to the case θ = θ∗. The loss becomes
Lt = 1 −γ(ν)γ(ν∗)
1 −νν∗.
(70)
Differentiating this function a first time, we obtain
dLt
dν = −γ(ν∗)γ′(ν)
1 −νν∗
−γ(ν∗)ν∗γ(ν)
(1 −νν∗)2 .
(71)
Differentiating it a second time gives
d2Lt
dν2 = −γ(ν∗)γ′′(ν)
1 −νν∗
−2γ(ν∗)ν∗γ′(ν)
(1 −νν∗)2
−2γ(ν∗)ν∗2γ(ν)
(1 −νν∗)3
.
(72)
Leveraging the fact that
γ′(ν) = −ν
γ(ν) and γ′′(ν) = −γ(ν)2 −ν2
γ(ν)3
,
(73)
we finally get, when ν = ν∗,
d2Lt
dν2 =
1
(1 −ν2)2 .
(74)
Given that we are at optimality, we have
d2Lt
dω2ν
= E
 dht
dων
d2Lt
dh2
t
dht
dων

= ν′(ων)2E
dht
dν
d2Lt
dh2
t
dht
dν

= ν′(ων)2 d2Lt
dν2 .
(75)
To keep that quantity constant, we thus have to solve the differential equation
ν′(ων) = (1 −ν2),
(76)
which gives ν(ων) = tanh(ων).
22

Deriving the optimal θ parametrization.
We now move to the parametrization of θ. We have
Lt = 1 −Re
γ(ν)γ(ν∗)
1 −¯λλ∗

= 1 −γ(ν)γ(ν∗)(1 −νν∗cos(θ −θ∗))
|1 −¯λλ∗|2
.
(77)
For notational convenience, we denote
α(θ −θ∗) := |1 −¯λλ∗|2 = (1 −νν∗cos(θ −θ∗))2 + ν2ν∗2 sin(θ −θ∗)2.
(78)
We have
dLt
dθ = γ(ν)γ(ν∗)

−νν∗sin(θ −θ∗)
α(θ −θ∗)
+ (1 −νν∗cos(θ −θ∗))α′(θ −θ∗)
α(θ −θ∗)2

(79)
and
d2Lt
dθ2 = γ(ν)γ(ν∗)

−νν∗cos(θ −θ∗)
α(θ −θ∗)
+ 2νν∗sin(θ −θ∗)α′(θ −θ∗)
α(θ −θ∗)2
+(1 −νν∗cos(θ −θ∗))α′′(θ −θ∗)
α(θ −θ∗)2
−2(1 −νν∗cos(θ −θ∗))α′(θ −θ∗)2
α(θ −θ∗)3

(80)
At optimality (θ = θ∗and ν = ν∗), we have α(0) = (1 −ν2)2, α′(0) = 0 and α′′(0) = 2ν2, so that
d2Lt
dθ2 = ν2(1 + ν2)
(1 −ν2)2 .
(81)
The optimal parametrization thus has to satisfy
θ′(ωθ) =
1 −ν2
ν
√
1 + ν2 ,
(82)
that is
θ(ωθ) = ωθ
1 −ν2
ν
√
1 + ν2
(83)
There are two things we can remark:
– First, the parametrization that we derived for the general case in Section A.3.2, which additionally
ignored the dependence of γ on λ, is relatively accurate. The only difference is the apparition of
the extra ν term, which becomes insignificant in the long memory limit ν →1.
– Second, the optimal θ parametrization has to be a function of ν, and thus ων, so the differential
equation ν needs to satisfy changes. Yet, this considerably simplifies the calculation and there is
no simple solution to that problem. One could still argue that the initial choice we made, that is
to use a polar parametrization, is the issue. It could be, but most practical models end up using
that choice so highlighting the limitations of this choice has important practical consequences.
In the rest of this section, we ignore the dependency of θ on ν, and consider the optimal parametriza-
tion in this setting to be
ν(ωopt
ν
) = tanh(ωopt
ν
)
(84)
θ(ωopt
θ
) = ωopt
θ
1 −ν2
ν
√
1 + ν2 .
(85)
B.1.3
Visualization of the effect of input normalization and reparametrization
We now visualize the effect of input normalization and reparametrization on the loss landscape. We
focus on two such reparametrizations:
– the one used in the LRU [20, 22] with γ(λ) =
√
1 −λ2, ν = exp(−exp(ωexp
ν
)) and θ =
exp(ωexp
θ
).
– the optimal one we derived in the previous Section (c.f. Equations 84 and 85), which is taylored
to this specific setting.
23

| |*
0.99
0.98
0.96
0.92
0.84
0.68
0.36
0.0
0.5
1.0
10 7
10 4
10 1
Loss
0.0
0.5
1.0
10 2
101
104
107
Hessian
1
0
1
*
10 5
10 3
10 1
Loss
1
0
1
*
10 1
101
103
Hessian
5
0
10 7
10 4
10 1
Loss
5
0
10 3
10 2
10 1
100
Hessian
2
0
10 9
10 6
10 3
100
Loss
2
0
10 4
10 1
102
Hessian
0
5
10 6
10 3
100
Loss
0
5
10 3
10 2
10 1
100
Hessian
5
0
5
10 7
10 4
10 1
Loss
5
0
5
10 2
100
Hessian
Figure 6: Visualization of the loss landscape with input normalization, in the teacher and the
student, for different parametrizations. The teacher satisfies λ∗= |λ∗| exp(iπ/100), for different
|λ∗| values. The first two columns correspond to students with correct angle θ = θ∗but wrong
absolute value ν and the last two columns to students with correct absolute value ν = |λ∗| but
wrong angle. When we fix one variable, we ignore how it affects the loss for the Hessian caclulation.
Each line corresponds to a different parametrization: the first line uses a polar parametrization
(λ = ν exp(iθ)), the second line uses the double exponential parametrization used in the LRU (exp)
and the third one is the optimal parametrization for that task (tanh). Overall, both reparametrizations
enable to control the explosion of the Hessian. However, the size of basins of attraction around
optimality, or their number, shrinks as |λ∗| goes to 1 for the angle, but not for the absolute value,
highlighting how difficult learning the angle can be.
B.1.4
Learning the angle is difficult in practice: an example
We use this one-dimensional teacher-student setting to test whether having a parametrization that
avoids exploding behaviors at optimality, such as the one we derived in Section B.1.2, facilitates
learning. Figure 6 already hints towards the fact the basin of attraction of the global minima is either
extremely narrow or that their number decreases as longer memories are considered, making learning
more tedious. Figure 7 confirms it. In this figure, we plot the learning dynamics obtained using the
Adam optimizer with a learning rate of 10−3 for 50k steps, starting from λ0 = 0.99 exp(iπ/4). We
consider three different parametrizations of the angle:
θ(ωpolar
θ
) = ωpolar
θ
(86)
θ(ωexp
θ
) = log(ωexp
θ
)
(87)
θ(ωopt
θ
) = (1 −ν2)
ν
√
1 + ν2 ωθ.
(88)
The first one does not reparametrize the angle, the second one is the one used in the LRU and the
third one is the optimal one we derived above. We use ν = tanh(ων) to parametrize the magnitude
in the three cases. We set λ∗to λ∗= 0.99 exp(iπ/100). The θ landscape when ν is correct therefore
corresponds to the ones plotted in the last two columns of Figure 6. This example shows that efforts
to reduce the sharpness of the loss at optimality, as done in the last parametrization, inevitably make
the loss flatter elsewhere and optimization impossible.
24

1.5
2.0
2.5
3.0
0.00
0.25
0.50
0.75
1.00
1
2
3
5
4
3
2
1
0
1
1
20
0
20
40
60
80
0
20k
40k
Step
10 3
10 1
101
Loss
0
20k
40k
Step
10 3
10 1
101
0
20k
10 3
10 1
101
2
Step
3
40k
10 2
10 1
100
101
102
Loss
A
B
C
Figure 7: Learning the angle is difficult, even in a simple one-dimensional task. The target λ
value is equal to λ∗= 0.99 exp(iπ/100) and is plotted in yellow. The black lines correspond to the
Adam learning dynamics. A. When the angle is not reparametrized (θ = ωθ), the loss landscape is
extremely sharp in the ωθ direction, but Adam compensates for it. B. When the angle is parametrized
exponentially (θ = exp(ωθ)), the loss landscape becomes smoother. However, this only hold when
the considered angles are small enough, as the exponential parametrization does not bring extra
granularity elsewhere. C. When reparametrizing the angle to reduce the gradient explosion as |λ| →1,
the loss becomes extremely tricky to navigate. The parameters are first attracted to a nearby valley,
which is flat on the ωθ direction and only indirectly connected to the global minimum. Such a
reparametrization thus hinders optimization far away from optimality. See Section B.1.4 for more
detail.
B.2
Structure of the Hessian at optimality
In Section 4, we argue that the Hessian at optimality is an important object to understand the learning
dynamics in the linear teacher-student task we consider. We here provide some theoretical analysis of
its structure in the complex diagonal setting, that is we consider a recurrent network of the form
ht+1 = λ ⊙ht + bxt
(89)
yt = Re[c⊤ht] + dxt.
(90)
with λ, b and c complex vectors of size n, with n the number of hidden neurons, and d a scalar. We
additionally take the loss to be the mean-square error, which is also the one we use in our numerical
experiments. Note that, as in our theoretical analysis of Section 2, we consider infinitely long
sequences and wide-sense stationary inputs.
Recall that the Hessian of the loss is equal to
d2L
dθ2 =
X
t
Ex
"
dht
dθ
∂2Lt
∂h2
t
dht
dθ
⊤
+ ∂Lt
∂ht
d2ht
dθ2
#
.
(91)
At optimality, only the first term remains, as ∂htLt is 0 for all data points. Given that we have shown
earlier, e.g. in Section A.2, that the most sensitive parameters to learn are the recurrent ones λ, we
focus on the Hessian with respect to these parameters in the following.
B.2.1
Hessian for complex-valued variables
Before delving into more specific calculations, we make a few remarks on how to deal the Hessian
when having complex-valued parameters. We will mostly leverage the fact that the loss L is real-
valued.
Before that, we recall a few facts about Wirtinger derivatives:
25

– For f(z) a complex-valued function of z, the Wirtinger derivatives are defined as:
df
dz = 1
2
dRe[f]
dRe[z] −idIm[f]
dIm[z]

(92)
df
d¯z = 1
2
dRe[f]
dRe[z] + idIm[f]
dIm[z]

.
(93)
– We have
df
dz = df
d¯z .
(94)
– Leveraging the fact that L is real-valued so that ¯L = L, we have
d2L
dλ2 = d
dλ
"
dL
dλ
⊤#
(95)
= d
dλ
"
dL
d¯λ
⊤#
(96)
= d2L
d¯λ2
(97)
and, similarly, dλd¯λL = d¯λdλL. Second derivatives are symmetric, so we additionally have
dλd¯λL = d¯λdλL⊤, which means that the complex Hessian is a Hermitian matrix.
Taken all together, this shows that the full complex Hessian, which contains all cross derivatives, has
a similar structure to the real case.
B.2.2
Hessian with respect to the recurrent eigenvalues
In this section, we compute the full complex Hessian with respect to the recurrent eigenvalue λ and
defer the analysis of reparametrization to the next section.
First, let us remark that
dLt
dλ = ∂Lt
∂yt
c⊤dht
dλ
(98)
(99)
so that
d2Lt
dλ2 = d
dλ
"
dht
dλ
⊤
c∂Lt
∂yt
⊤#
(100)
= d2ht
dλ2 c∂Lt
∂yt
⊤
+ dht
dλ
⊤
c∂2Lt
∂y2
t
c⊤dht
dλ
(101)
We assumed that we are at optimality so that the network perfectly fits the target trajectories and
∂ytLt = 0. Additionally, Lt is the mean-squared error loss so that ∂2
ytLt = Id. It follows that
d2Lt
dλ2

ij
=
 
dht
dλ
⊤
cc⊤dht
dλ
!
ij
(102)
=

c⊤dht
dλ

i

c⊤dht
dλ

j
(103)
= ci
dht,i
dλi
cj
dht,j
dλj
.
(104)
In the last equation, we made use of the fact that the parameter λi only affects the hidden state ht,i
and not the others, so dλjht,i = 0 if i ̸= j.
26

The previous calculation applied to one sequence, we now take the expectation over the data:
d2L
dλ2 = (cc⊤) ⊙Ex,y
"
lim
t→∞
dht
dλ
dht
dλ
⊤#
(105)
Note that we introduced a slight abuse of notation in the previous equation as dλht is in general a
matrix. However, given that the hidden neurons are independent here due to the diagonal connectivity,
it is effectively a vector, and we treat it that way. Let us now compute the expectation, using similar
calculation techniques to the one we used in Section A.2:
Ex,y

lim
t→∞
dht,i
dλi
dht,j
dλj

= E

X
n,m≥0
nλn−1
i
bix−nmλm−1
j
bjx−m


(106)
= bibjE

X
n,m≥0
nλn−1
i
x−nmλm−1
j
x−m


(107)
= bibj
X
n,m≥0
nmλn−1
i
λm−1
j
Rx(n −m)
(108)
We can now remark that this quantity is very similar to the one we have encountered in Section A.2,
up to the presence of bibj, and can be simplified using Lemma 2. For conciseness, we note S(λi, λj)
the right-hand side of the last equation without the bibj factor. Putting this result back in the Hessian,
we get
d2L
dλidλj
= bibjcicjS(λi, λj)
(109)
To gain further intuition of the behavior of this quantity, we take Rx(∆) = ρ|∆|, ρ being a real
number. A similar calculation to the one we did in Section A.2 gives
S(λi, λj) = (1 −ρ)(1 + λiλj) + ρ2(1 −λiλj)3 + ρ(1 −ρ)λiλj(ρλiλj(1 + λiλj) −2λi −2λj)
(1 −λiλj)3(1 −ρλi)2(1 −ρλj)2
.
(110)
This formula being still hard to grasp, we visualize the magnitude of S(λi, λj) on Figure 8. Interest-
ingly, we observe this quantity is large when λi and λj are conjugate to each other and inputs are
uncorrelated. However, as elements in the input sequence get more correlated (ρ →1), this effect
disappears and |S| increases as one of the two eigenvalue gets closer to 1 in the complex plane. In
both cases, the effect gets amplified as the magnitude of the eigenvalue increases.
1
0
1
Re[ ]
1
0
1
Im[ ]
= .
1
0
1
Re[ ]
1
0
1
= .
1
0
1
Re[ ]
1
0
1
= .
1
0
1
Re[ ]
1
0
1
= .
1
0
1
Re[ ]
1
0
1
= .
10 1
101
103
105
| ( , 
, )|
Figure 8: Visualization of λ 7→|S(λ, λ0)| for λ0 = 0.99 exp(iπ/4). This term measures how
"similar" eigenvalues are in the Hessian. When ρ = 0, eigenvalues are mostly "similar" when they are
conjugate to each other. As ρ increases, this effect decreases and eigenvalues become more "similar"
if one of them gets close to 1.
We also need to compute d¯λdλL to get the full complex Hessian. Similarly to the previous calculation,
we first have
dLt
d¯λ = d¯Lt
dλ = dLt
dλ = ∂Lt
∂yt
¯c⊤dht
dλ .
(111)
27

It follows that
d2L
dλid¯λj
= E
dht,j
dλj
¯cjci
dht,i
dλi

(112)
= ci¯cjbi¯bjS(λi, ¯λj).
(113)
Using the symmetry with the complex Hessian matrix, we now have all its components.
B.2.3
Hessian for different parametrizations
So far, we have computed the complex Hessian, which is not of direct use as we end up optimizing
real numbers in practice. Here, we study the impact of different parametrizations of λ on the Hessian.
Given that this parametrization only affects λ and not the other parameters in the network and that we
only consider the Hessian at optimality here, computing the Hessian of those parameters reduces to
left and right multiplying the Hessian by derivatives of λ and ¯λ with respect to these parameters. For
future reference, we introduce
Hλ
ij :=
 
d2L
dλidλj
d2L
dλid¯λj
d2L
d¯λidλj
d2L
d¯λid¯λj
!
=

Aij
Bij
¯Bij
¯Aij.

(114)
with Aij := bibjcicjS(λi, λj) and Bij = bi¯bjci¯cjS(λi, ¯λj).
Real-imaginary parametrization: λ = ωre + ωim.
We aim at computing the matrix
HRI
ij :=
 
d2L
dωre,idωre,j
d2L
dωre,idωim,j
d2L
dωim,idωre,j
d2L
dωim,idωim,j
!
,
(115)
which is the building block to compute the full Hessian. First, let us remark that dωre,iλi = 1/2,
dωre,i¯λi = 1/2, dωim,iλi = i/2 and dωim,i¯λi = −i/2. It follows that
d2L
dωre,idωre,j
= (dωre,jλj dωre,j ¯λj)Hλ
ij(dωre,iλi dωre,i¯λi)⊤
(116)
= 1
4(1 1)Hλ
ij(1 1)⊤
(117)
= 1
2 (Re[Aij] + Re[Bij]) .
(118)
Once again we emphasize that the first line only holds as we are at optimality. Similar calculations
give the rest of the elements of HRI
ij :
HRI
ij := 1
2

Re[Aij + Bij]
Im[−Aij + Bij]
Im[−Aij −Bij]
Re[−Aij + Bij].

.
(119)
Given the intuition we gained on the structure of S previously, and the fact that Aij ∝S(λi, λj)
and Bij ∝S(λi, ¯λj), we know that this block will have large components if the two corresponding
eigenvalues are conjugate of each other or aligned to each other, or if one of them is close to 1.
One other quantity that we can calculate is the trace of the Hessian HRI, which is equal to the sum of
its eigenvalues. Note that this does not correspond to the eigenvalues of the full Hessian matrix, as it
additionally contains entries for other parameters. Yet it already provides some idea of how large the
Hessian will be, as the value of this trace appears in the value of the full trace. We have
Tr[HRI] =
X
i
1
2 (Re[Aii + Bii] + Re[−Aii + Bii])
(120)
=
X
i
Re[Bii]
(121)
=
X
i
|bi|2|ci|2S(λi, ¯λi)
(122)
where we used that S(λi, ¯λi) is real-valued in the last line. As a side note, this formula partly justifies
why studying the expected squared magnitude of dλht in Section 2 makes general sense, as
E
"
dht,i
dθ

2#
= |bi|2S(λi, ¯λi).
(123)
28

Magnitude-angle parametrization:
λ
=
ν(ων) exp(iθ(ωθ)).
The calculations for this
parametrization are similar to the previous one, with the following differences:
dλ
dων
= ν′(ων) exp(iθ(ωθ))
2
(124)
d¯λ
dων
= ν′(ων) exp(−iθ(ωθ))
2
(125)
dλ
dωθ
= iν(ων)θ′(ωθ) exp(iθ(ωθ))
2
(126)
d¯λ
dωθ
= −iν(ων)θ′(ωθ) exp(−iθ(ωθ))
2
.
(127)
After some calculations we obtain
d2L
dων,i dων,j
= ν′(ων,i)ν′(ων,j)
2
Re[ei(θ(ωθ,i)+θ(ωθ,j)Aij + ei(θ(ωθ,i)−θ(ωθ,j))Bij]
(128)
d2L
dων,i dωθ,j
= ν′(ων,i)ν(ων,j)θ′(ωθ,j)
2
Im[−ei(θ(ωθ,i)+θ(ωθ,j)Aij + ei(θ(ωθ,i)−θ(ωθ,j))Bij] (129)
d2L
dωθ,i dων,j
= ν(ων,i)θ′(ωθ,i)ν′(ων,j)
2
Im[−ei(θ(ωθ,i)−θ(ωθ,j)Aij + ei(θ(ωθ,i)−θ(ωθ,j))Bij]
(130)
d2L
dωθ,i dωθ,j
= ν(ωθ,i)θ′(ωθ,i)ν(ωθ,j)θ′(ωθ,j)
2
Re[−ei(θ(ωθ,i)+θ(ωθ,j)Aij + ei(θ(ωθ,i)−θ(ωθ,j))Bij]
(131)
B.3
Experimental details
We recall the setup we consider in Section 4:
ht+1 = Aht + Bxt+1 and yt = Cht + Dxt.
(132)
with ht ∈Rn, xt ∈R drawn i.i.d. from N(0, 1), A ∈Rn×n, B ∈Rn×1, C ∈R1×n and D ∈R1×1.
We draw B, C and D from truncated normal distributions with fan_in scaling. We draw each entry of
A from N(0, 1/√n) and then apply the following postprocessing to it: First we complex diagonalize
A, which we can do almost surely. Note λ its eigenvalues. We then transform them according to
λ ←(ν + (1 −ν) tanh(|λ|)) exp

iangle(λ)
π
θ0

(133)
with ν and θ0 two scalars that we control. This transformation has several benefits: we are guaranteed
that the magnitude of λ is within [ν, 1] (and in [ν, ν + (1 −ν) tanh(1)] in the limit n →∞as the
eigenvalues of A stay within the unit circle in that limit), and conjugate pairs of eigenvalues remain
conjugate. This last point ensures that the resulting matrix remains real without having to change the
eigenvectors.
We implement our experiments in JAX [62], using the default Flax [63] implementation of RNNs and
the LRU implementation of Zucchet et al. [64]. We initialize RNNs in the same way we initialized the
teacher, and initialize the eigenvalues of the LRU and other complex-valued networks with magnitude
in [ν, 1] and angle within [−θ0, θ0].
Given that we are interested in the optimization properties of the different architectures, we only
report training losses and do not perform any cross validation.
Here are additional details related to the different figures:
– Figure 3: see Tables 1 and 2.
– Figure 4: for panels A and B, we use ν = 0.99 and draw A in a slightly different manner to the
one described above (we directly draw the eigenvalues and eigenvectors so that we have two pairs
of complex eigenvalues). We use automatic differentiation to compute the Hessian. For panels C
and D, we use the same setup as described in Table 2, but keep the learning rate constant over the
course of learning. We report the effective learning rate at the end of learning.
29

RNN
LRU
Batch size
128
Sequence length
300
Hidden neurons (teacher)
10
Input / output dimension
1
ν
{0.32, 0.68, 0.84, 0.92, 0.96, 0.98, 0.99}
θ0
π
Hidden neurons (student)
64
log learning rate
[−5, −4.5, −4, −3.5, −3, −2.5]
[−2.5, −2, −1.5, −1, −0.5]
Optimizer (schedule)
Adam (cosine)
Initialization
[ν teacher, ν = 0]
ν teacher
Number iterations
10k
Seeds
10
Table 1: Experimental details for Figure 3.A. We use [· · · ] to denote hyperparameters that were
scanned over with grid search and {· · · } to denote the variables of interest for the figure. We chose
the learning rates for the two architectures on preliminary scans and verified that non of the extreme
learning rates were optimal in the final scan. For the RNN, we found that initializing with ν = 0 gave
better results than initializing with the same distribution the teacher has, so we included this choice in
the scan.
RNN / BLOCK DIAG. RNN
COMPLEX DIAG. RNN / LRU
Batch size
128
Sequence length
300
Hidden neurons (teacher)
10
Input / output dimension
1
ν
0.99
θ0
π
Hidden neurons (student)
64 / 64 and 128
64
log learning rate
[−5, −4.5, −4, −3.5, −3, −2.5]
[−2.5, −2, −1.5, −1, −0.5]
Optimizer (schedule)
Adam (cosine)
Initialization
[ν teacher, ν = 0]
ν teacher
Number iterations
10k
Seeds
10
Table 2: Experimental details for Figure 3.B. We use [· · · ] to denote hyperparameters that were
scanned over with grid search and {· · · } to denote the variables of interest for the figure. We chose
the learning rates for the two architecture types on preliminary scans and verified that non of the
extreme learning rates were optimal in the final scan. For the RNN, we found that initializing with
ν = 0 gave better results than initializing with the same distribution the teacher has, so we included
this choice in the scan. For the RNNs, we used 64 neurons for the "RNN" entry, 64 for the "block
diagonal" one, and 128 for the "more neurons" one.
– Figure 10: for panels A, B and C, we draw the magnitude and angle of 10 λ independently,
uniformly in [ν, 1+ν
2 ] and [−θ0, θ0]. Importantly, this means that there are no conjugate pairs,
which leads to more diagonal Hessian matrices at optimality than in Figure 4. For panel D, see
Table 3.
As a rule of thumb, each LRU (or complex-valued diagonal network) experiment takes 3 minutes on
a consumer-scale GPU (NVIDIA GeForce RTX 3070) and each RNN experiment takes 10 minutes
on a CPU. The scans behind the results reported in the different figures require on the order of
few hundreds run each. Including our preliminary exploration, the results we report in this section
required 30 days of compute, one third of it on GPUs and two thirds on CPUs.
30

RNN
COMPLEX DIAG. RNN / LRU
Batch size
128
Sequence length
300
Hidden neurons (teacher)
10
Input / output dimension
1
ν
0.99
log(θ0/π)
{−2, −1.5, −1, −0.5, 0}
Hidden neurons (student)
64
log learning rate
[−4.5, −4, −3.5, −3]
[−3.5, −3, · · · , −0.5, 0]
Optimizer (schedule)
Adam (cosine)
Initialization
[ν teacher, ν = 0] + θ teacher
ν teacher + θ teacher
Number iterations
10k
Seeds
10
Table 3: Experimental details for Figure 10. We use [· · · ] to denote hyperparameters that were
scanned over with grid search and {· · · } to denote the variables of interest for the figure. We chose
the learning rates for the two architectures on preliminary scans and verified that non of the extreme
learning rates were optimal in the final scan. For the RNN, we found that initializing with ν = 0 gave
better results than initializing with the same distribution the teacher has, so we included this choice in
the scan.
B.4
Additional analyses
B.4.1
Structure of the loss landscape for LRUs
In the main text, we only provide an analysis of the loss landscape for the fully connected linear
recurrent neural network and its complex-valued diagonal counterpart. We here complete this result
by performing the same analysis for the LRU.
D
0
10
20
30
Index eigenvalue
10 4
107
Eigenvalue
Hessian
Top 10
eig. vect.
106
103
100
0
100
103
106
1
0
1
10 4
10 3
Bre
Bⁱm
Cre
Cⁱm
D
Bre Bⁱm Cre Cⁱm D
Bre Bⁱm Cre Cⁱm D
A
B
Effective LR
10 5
Figure 9: Equivalent of Figure 4 for the LRU. The exponential parametrization of the magnitude
ν = exp(−exp(ων)) efficiently mitigates the Hessian explosion but not the one of the angle
θ = exp(ωθ), consistently with the theoretical and empirical evidence we have accumulated so far.
B.4.2
Concentrating eigenvalue distributions
The goal of this experiment is to better understand how the concentration of eigenvalues λ affect
the learning dynamics. For fully connected RNNs, there is no reason to expect a major change
in behavior. However, it is different for diagonal RNNs. The theoretical analysis we have done
in Section B.2 provides us with the following insights. When the elements in the input sequence
31

are uncorralated, as it is the case here, the entries in the Hessian corresponding to two different
eigenvalues increase if they are aligned or conjugate to each other, and if their magnitude is large. We
therefore expect that, as the interval on which the angle of the teacher’s eigenvalues shrinks (θ0 →0),
those eigenvalues will be more likely to be "similar" to each other. This results in large non-diagonal
terms, as we confirm in Figure 10.A, B and C. The LRU suffers less from this problem thanks to its
reparametrization, which reduces the overall magnitude of Hessian entries related to the magnitude,
and partly the one of angle parameters (when it is a small positive number). As a consequence, the
performance between these two architectures increases as θ0 →0, as seen on Figure 10.D.
= 
D
D
D
0
10
Index eigenvalue
103
109
Eigenvalue
0
10
Index eigenvalue
104
105
106
0
10
Index eigenvalue
103
104
108
104
100
0
100
104
108
1
0
1
106
103
100
0
100
103
106
1
0
1
104
102
100
0
100
102
104
1
0
1
= /
re
im
= /
re
im
re
im
re
im
re
im
re
im
D
Top 10
eig. vect.
Hessian
RNN
Complex RNN
LRU
10 2
10 1
100
10 5
10 3
10 1
Loss
A
D
B
C
Figure 10: Concentrating eigenvalues make the Hessian less diagonal (θ0 →0) and consequently
increases the gap between the LRU and the complex-valued diagonal RNN. A, B, C. Hessian
of the loss with respect to the λ parameters in the complex-valued diagonal RNN. The Hessian is
computed through the theoretical formula of Equation 119; computing it numerically marginally
affects the results. Consistently with the intuition we developed in Section B.2, concentrating the
eigenvalues affect the structure of the loss landscape. It makes the Hessian at optimality less diagonal
and Adam cannot efficiently compensate it. The LRU does not suffer as much from this problem, and
the gap between the two architecture widens as θ0 →0.
C
Signal propagation in deep recurrent neural networks at initialization
C.1
Experimental setup
We here detail the experimental setup we used in Section 5. We take 1024 random sequences from
the Wikipedia dataset [65] and pass it through the BERT [66] tokenizer and embedding layer. This
provides us with a dataset of 1024 examples, we cut their length at 512. Each embedding has 724
features.
We consider networks with 4 blocks of the following structure: a recurrent layer, a non-linearity, a
gated linear unit [57, GLU] and a skip connection. By default, we do not use any normalization layer,
32

but when we do, as in Figure 5.C, we include one normalization layer before the recurrent layer and
another one before the GLU. All the layers involved use 256 neurons. We additionally add a linear
encoder at the beginning of the network, and a linear decoder at the end.
In Figure 5 we vary ν, which controls the magntitude of the eigenvalues of the recurrent Jacobian.
More precisely, we sample those magnitudes in the interval [ν, (1 + ν)/2]. For the complex-valued
diagonal RNN and the LRU, we use the LRU initialization. For the LSTM, we use the chrono
initialization of Tallec and Ollivier [29]: it initializes the bias of the forget and input gates such that,
when the input x and the hidden state h are equal to 0, the time constant associated to f is uniformly
sampled from [
1
1−ν ,
2
1−ν ] and the input gate i is equal to 1 −f.
The loss that we use is a next-token mean-squared error, that is
Lt = 1
2∥ˆxt(x1:t−1) −xt∥2
(134)
with ˆxt(x1:t−1) the prediction of the network. The quantities reported in Figure 5 are the average
squared value the hidden state or the gradient takes. The average is taken over all the sequences, but
also over all neurons / parameters and over all time steps. Gradients are computed on batches of size
8.
33

