Deep Ensembles from a Bayesian Perspective
Lara Hoﬀmann1 & Clemens Elster1
1Physikalisch-Technische Bundesanstalt, Braunschweig and Berlin, Germany
May 28, 2021
Abstract
Deep ensembles can be seen as the current state-of-the-art for uncertainty quan-
tiﬁcation in deep learning. While the approach was originally proposed as an non-
Bayesian technique, arguments towards its Bayesian footing have been put forward
as well. We show that deep ensembles can be viewed as an approximate Bayesian
method by specifying the corresponding assumptions. Our ﬁnding leads to an im-
proved approximation which results in an increased epistemic part of the uncer-
tainty. Numerical examples suggest that the improved approximation can lead to
more reliable uncertainties. Analytical derivations ensure easy calculation of results.
1
Introduction
Deep learning has been successfully applied to numerous tasks due to its ability to learn
complex relationships from data. The black box character of the employed deep neural
networks, however, is a major drawback.
Thorough testing [1, 2] and explainable AI
[3, 4] have been proposed to overcome this issue.
Uncertainty quantiﬁcation [5, 6] is
another approach that can be viewed in this context. However, quantifying the uncertainty
associated with predictions of a trained network also has a merit on its own, and it is
particularly relevant when decisions are taken in sensible applications such as medical
diagnosis [7] or autonomous driving [8].
Several methods have been proposed to quantify the uncertainty associated with the
prediction of a deep neural network [9, 10]. Currently, the two most common methods
are dropout [11, 12] and deep ensemble [13]. Diﬀerent studies show that deep ensembles
often outperform other methods [14, 15, 16, 17]. Furthermore, ensemble techniques scale
well to high-dimensional problems and are easy to implement [15, 18]. These techniques
can thus be seen as state-of-the-art for uncertainty quantiﬁcation in deep learning.
The idea of deep ensembles is to use a combination of multiple networks which dates back
to the last century. In [19], it has been shown that better results can be achieved for
an ensemble of networks than when employing a single network and the use of multiple
networks to account for model uncertainty has been proposed in [20]. Ensemble learning
has gained in popularity ever since [21]. A breakthrough in the application of ensembling
for deep learning was the introduction of deep ensembles [13] a couple of years ago,
where an ensemble of independently trained networks is treated as a uniformly-weighted
Gaussian mixture model. In [22], deep ensembles are even used to introduce a metric
to measure the performance of other ensembling methods by counting the number of
networks needed to reach a speciﬁed performance.
Uncertainty quantiﬁcation by deep ensembles has been proposed without referring to a
Bayesian background.
In fact, it has been explicitly classiﬁed as a non-Bayesian ap-
proach [13, 16, 23]. Several ideas have been put forward to embed deep ensembles into
a Bayesian framework. Examples comprise early stopping [24], parameter regularization
1
arXiv:2105.13283v1  [cs.LG]  27 May 2021

around diﬀerent values drawn from a prior distribution [25] or the link to Gaussian pro-
cesses through the neural tangent kernel [26]. In [27], deep ensembles have been related
to Bayesian model averaging [28], and in [15] it is represented as an approach that builds
on an approximation of a multimodal posterior for the network parameters.
In following this perspective, we argue that deep ensembles provide an approximate
Bayesian inference in which the true posterior is replaced with an average of delta dis-
tributions around local a posterior (MAP) estimates for the parameters of the network.
We specify the statistical model and the prior distributions required to achieve this re-
sult. In using the approximation of an average of delta distributions to the posterior, the
formulas applied in deep ensembles [13] follow immediately. We propose to improve the
approximation by replacing the delta distributions with Gaussian distributions, which is
a well-known technique from Laplace approximation [29]. A similar idea has been pre-
sented in [27], where the approximated posterior distribution is generalized to a mixture
of Gaussians which are independently trained as Stochastic-Weight-Averaging Gaussians
(SWAG) [30]. The approximation of the posterior by a family of Gaussian distributions is
well known from Bayesian neural networks [31] and variational inference in general [32].
In applying variational inference, we maximize the ELBO to determine the additional
parameters of the Gaussian mixture distribution. However, rather than maximizing the
ELBO with respect to all parameters during network training, we suggest to proceed in
two steps. First, conventional training of the ensemble of networks yields local MAPs
of the network parameters which are taken as the locations of the Gaussian mixture
distribution. In a second step, the remaining parameters are determined by maximizing
the ELBO. In this way, training of the deep ensembles does not have to be altered, and
our improved approximation to the posterior can be obtained through post-processing.
Previous work suggests that model uncertainty can be captured using only a few or even
just one Bayesian layer at the end of the network [33, 34, 35]. Therefore, we also consider
only the weights of the last layer in the neural networks to be random, and do not
apply any nonlinear transformations afterwards. This allows an analytical solution for
the maximization of the ELBO to be derived. As a consequence, the proposed variational
inference is easily applied by simple post-processing of the results obtained by conventional
deep ensembles.
The improved approximation to the posterior leads to a modiﬁcation of the formulas for
the uncertainty quantiﬁcation by deep ensembles. Speciﬁcally, the epistemic part of the
uncertainty is enlarged by an additional contribution. We argue that in the context of
regression problems the epistemic part of the uncertainty can be the relevant part when
the goal is to infer the regression function, and the proposed modiﬁcation is then necessary
to arrive at a reliable uncertainty quantiﬁcation.
Our contribution is the following. We specify the statistical model, the prior distributions
and the form of approximation required such that uncertainty quantiﬁcation by deep en-
sembles [13] can be viewed as an approximate Bayesian method. This approximation is
then improved by extending the family of distributions used to approximate the posterior
from delta distributions to Gaussian distributions. The ELBO is maximized to determine
the additional parameters, leading to a simple post-processing procedure of conventional
deep ensembles. In using numerical examples we demonstrate the impact of the proposed
modiﬁed uncertainty quantiﬁcation and compare its results with those obtained by con-
ventional deep ensembles. We argue that the increased epistemic part of the uncertainty
is relevant in those regression tasks where the goal is to infer the regression function.
2

Analytical derivations are given which allow for analytical calculations of the proposed
procedure.
The paper is organized as follows. In Section 2 the conditions are speciﬁed under which
uncertainty quantiﬁcation by deep ensembles can be viewed as an approximate Bayesian
method. The improvement of this approximation through a family of Gaussian mixture
distributions is then considered in Section 3, including the analytical derivation of the
additional variational parameters. In Section 4 numerical examples are presented that
explore the impact of the proposed approach in the context of regression problems. Finally,
conclusions are drawn in Section 5.
2
Deep ensembles as a Bayesian approximation
In this work we focus on regression problems. The assumed heteroscedastic regression
model is given in
y|x ∼N
 ηθ(x), σ2
θ(x)I

, x ∈Rpx, y ∈Rpy ,
(1)
where the conditional distribution for y given x is taken as a Gaussian distribution with
mean ηθ(x) and covariance matrix σ2
θ(x)I. Both, ηθ(x) and σ2
θ(x), are modeled by deep
neural networks. The mean ηθ(x), viewed as a function of x, will be termed the regression
function, and we are particularly interested in its inference. The available training data
(xi, yi), i = 1, . . . , N, shall follow model (1) and are denoted by D.
In a Bayesian framework [36, 37, 38] the posterior distribution π(θ|D) is determined (or
approximated) and used for inference. For example, as an estimate for the regression func-
tion ηθ(x) and its associated uncertainty one could take the posterior mean and posterior
standard deviation of ηθ(x). When one is interested in predicting a future observation
y at a speciﬁed x, on the other hand, the posterior predictive distribution π(y|x, D) is
considered. The posterior predictive distribution accounts for both, the uncertainty in
the parameters θ and the additional uncertainty of y expressed by the covariance matrix
σ2
θ(x)I.
The total uncertainty associated with the posterior predictive distribution of the neural
network predictions can be divided into an aleatoric and an epistemic part [39, 40]. While
the former relates to the irreducible part of the uncertainty, the epistemic uncertainty
characterizes the uncertainty about the model (i.e. model parameters θ in our setting).
The epistemic uncertainty can be reduced by increasing the size of the available training
data. The aleatoric uncertainty is represented in our model (1) by the covariance matrix
σ2
θ(x)I and characterizes the uncertainty about the (future) realization of an observation
y given its mean ηθ(x).
The method of deep ensembles [13] considers an ensemble of networks that are trained
individually. To predict an observation y, the mean of the predictions of the single net-
works is formed. The (squared) uncertainty associated with this prediction is taken as
the sum of the covariance matrix of the individual predictions and the estimated aleatoric
part of the prediction (i.e. σ2
bθ(x)I in our model (1)).
Deep ensembles were originally introduced as a non-Bayesian method [13]. However, it
was repeatedly mentioned that deep ensembles can be viewed from a Bayesian perspec-
tive [9, 15, 27]. The following theorem supports this view and speciﬁes the employed
approximation.
3

Figure 1: This example ﬁgure illustrates the solution space of the network parameters
on the x-axis and the negative loss function on the y-axis. The deep ensembles explore
multiple modes in comparison to other Bayesian approaches which focus on a single mode
but consider multiple moments. The proposed extension combines these two ideas.
Lemma 1. The conventional deep ensembles according to [13] can be viewed as an ap-
proximate Bayesian inference where the posterior π(θ|D) of the network parameters θ
given the data D is approximated by a family of delta distributions, i.e.
π(θ|D) ≈q(θ) := 1
L
L
X
l=1
δ(θ −bθ(l)) ,
(2)
while choosing the prior π(θ) for the network parameters normally distributed as
θ ∼N
 0, λ−1I

,
(3)
where λ is the regularization parameter for the network parameters. The parameter es-
timates bθ(l), l = 1, . . . , L, obtained for the ensemble of L networks are at the same time
(local) estimates for the maximum a posteriori probability (MAP) of θ. The approxima-
tion for the posterior predictive distribution for the statistical model in (1) is then given
by
π(y|x, D) ≈1
L
L
X
l=1
N
 y; ηbθ(l)(x), σ2
bθ(l)(x)I

.
(4)
Proof. The proof is given in Appendix A.1.
The average over delta distributions taken at point estimates (2) is a crude approximation
for the posterior.
Nonetheless, the deep ensembles outperform many other Bayesian
approaches due to their ability to explore diﬀerent modes of the posterior distribution
[41]. Inspired by [41] (Fig. 1) and [27] (Fig. 3), this behaviour is illustrated in Figure 1.
In using the approximation (4), mean and covariance matrix of the posterior predictive
distribution are obtained as
E (y|x, D)
=
1
L
L
X
l=1
ηbθ(l)(x) ,
(5)
Cov (y|x, D)
=
1
L
L
X
l=1
n ηbθ(l)(x) −E (y|x, D)
  ηbθ(l)(x) −E (y|x, D)
T + σ2
bθ(l)(x)I
o
, (6)
where E stands for expectation and Cov for covariance, cf. Appendix A.2. Note that
(5) yields the inferred regression function ηθ(x) = E (y|x, D) according to the statistical
4

model (1), and (6) describes the total uncertainty associated with the prediction of the
network ensemble. Mean and covariance matrix of the posterior predictive distribution
are in accordance with the results given in [13] for the original approach of the deep
ensembles.
In [13], the comparison between diﬀerent methods is based on the posterior predictive
distribution. The covariance matrix (6) comprises the reducible epistemic part of the
uncertainty as well as the irreducible aleatoric part. However, when the goal is to infer
the underlying regression function, an accurate estimation of the epistemic uncertainty
is essential. The epistemic uncertainty is given by the covariance matrix of the single
estimates of the regression function obtained by the ensemble of trained networks. Alto-
gether, mean and covariance matrix of the regression function are deduced in the following
theorem.
Theorem 1. The approximate Bayesian inference associated with deep ensembles yields
a posterior π(η|x, D) for the regression function η ≡ηθ(x) given by
π(η|x, D) = 1
L
L
X
l=1
δ(η −ηbθ(l)(x)) ,
(7)
with mean and covariance matrix given by
E (η|x, D)
=
1
L
L
X
l=1
ηbθ(l)(x) ,
(8)
Cov (η|, x, D)
=
1
L
L
X
l=1
n ηbθ(l)(x) −E (η|x, D)
  ηbθ(l)(x) −E (η|x, D)
To
.
(9)
Proof. The proof is given in Appendix A.3.
Note that the expectation of the posterior for the regression function (8) equals the expec-
tation of the posterior predictive distribution (5), but its covariance matrix (9) contains
only the epistemic part of the covariance matrix of the posterior predictive distribution
(6).
3
Extension to Gaussian mixture distributions
The aleatoric part of the covariance matrix of the posterior predictive distribution (6) is
often dominating (cf. also example in Fig. 3), and hence the covariance matrix of the
regression function (9) is expected to be signiﬁcantly smaller than that of the posterior
predictive distribution. It follows, that the (crude) approximation to the posterior in
(2) with a family of delta distributions, ignoring the nonzero width of the true posterior
around its (local) MAPs, often is irrelevant for the total uncertainty. Nevertheless, it can
lead to a signiﬁcant underrating of the epistemic uncertainty.
An improved approximation to the posterior distribution π(θ|D) is given by
q(θ) = 1
L
L
X
l=1
N(θ; bθ(l), γlI) ,
(10)
5

where the γl correspond to the variances around the (local) MAPs bθ(l). The approximation
(10) for the posterior extends the approximation in (2) by allowing for a ﬁnite width of
the posterior around the (local) MAPs, and it reduces to (2) for γl →0.
The general idea of variational inference [32] is to approximate the posterior π(θ|D) by
a distribution that can be handled more simply.
Speciﬁcally, one seeks for the best
approximation to the posterior within a parametric family of distributions such as the
Gaussian mixture distribution in (10). One way to do so is to use a Laplace approximation,
where the parameters of a single Gaussian approximation are calculated from the local
properties of the posterior at the MAP (cf. [29]). In deep learning approaches, however,
the parameters are typically determined by minimizing the Kulback-Leibler divergence
KL(q(θ)∥π(θ|D)) between he chosen family of distributions and the posterior. This is
equivalent to maximizing the evidence lower bound (ELBO)
ELBO = Eq [log p(D|θ)] −KL (q(θ)∥π(θ)) .
(11)
Instead of maximizing the ELBO with respect to all parameters during network training,
we here suggest a two-step procedure.
First, the networks are independently trained
according to the conventional deep ensembles training procedure. The resulting (local)
MAPs of the network parameters are taken as the ﬁrst moments of the Gaussian mixture
distributions. Second, the remaining parameters of (10), which are the variances γ, are
deduced by maximizing the ELBO (11) in a post-processing step.
We assume the network weights only of the last layer to be random. Furthermore, we
do not consider the randomness of σ2
θ(x), but only that of ηθ(x). The reason is that
we are interested in the epistemic part of the uncertainty associated with the regression
function. Also, σ2
θ would not depend on the last layer but rather be an individual network
parameter, when choosing a homoscedastic approach instead, which could be a promising
alternative to the heteroscedastic model (1). Maximizing the ELBO can then be per-
formed analytically, as stated in Theorem 2 below, cf. also Figure 2 for visualizing those
parameters that are treated as random in the proposed approach. The mean and covari-
ance matrix of the regression function for the improved approximate Bayesian inference
are given in Theorem 3 below, and Algorithm 1 summarizes the proposed approach.
Figure 2: The proposed model of a single network is visualized here. A sample y|x from
the posterior predictive distribution is drawn from the network outputs N(ηθ(x), σ2
ˆθ(x)).
All network weights are ﬁxed to the MAP estimates after training, except the last linear
unit of the predicted mean value ηθ(x). The weights of this last linear unit are normally
distributed around the trained MAPs bθ with variances γl, l = 1, . . . , L.
6

Theorem 2. Let only the weights of the last layer in the neural network, which are
related to the prediction of the regression function ηθ(x), be random and assume a linear
transformation from the last layer to the network output. Then, approximate maximization
of the ELBO (11) can be done analytically when considering the approximation of the
posterior distribution given in (10). Furthermore, the variances in (10) tend to zero for
an inﬁnite amount of training data.
Proof. The proof is given in Appendix A.4.
Algorithm 1: improved approximation to the posterior π(θ|D)
Result: q(θ) := 1
L
PL
l=1 N(θ; bθ(l), γlI), instead of q(θ) := 1
L
PL
l=1 δ(θ −bθ(l));
1. initialize and independently train networks ηbθ(l), l = 1, . . . , L, according to the
deep ensembles approach;
2. analytically compute γl, l = 1, . . . , L, that maximize the ELBO;
Theorem 3. The improved approximation for the Bayesian inference associated with deep
ensembles yields a posterior π(η|x, D) for the regression function η ≡ηθ(x) given by
π(η|x, D) = 1
L
L
X
l=1
Z
δ(η −ηθ(x))N

θ; bθ(l), γlI

dθ ,
(12)
with mean and covariance matrix given by
E (η|x, D)
≈
1
L
L
X
l=1
ηbθ(l)(x) ,
(13)
Cov (η|x, D)
≈
1
L
L
X
l=1
n ηbθ(l)(x) −E (η|x, D)
  ηbθ(l)(x) −E (η|x, D)
T + γlJlJT
l
o
, (14)
where Jl = ∂η/∂θ evaluated at θ = bθ(l). The Jacobian matrix Jl is easily calculated in
terms of the output of the last but one layer, when considering only the weights of the last
layer in the neural networks as random, and when applying no nonlinear transformation
to the output of the last layer.
Proof. The proof is given in Appendix A.5.
The improved approximation of the Bayesian inference associated with deep ensembles
yields essentially the same expectation (13) for the regression function as the original
approximation (8), together with a covariance matrix (14) which includes the additional
term γlJlJT
l
(compared to (9)). In this way, the epistemic part of the uncertainty of
conventional deep ensembes is enlarged.
Sampling from the posterior (12) for the regression function is easily done by ﬁrst sampling
an l ∈{1, . . . , L} with probability
1
L. Second, sample a θ from N(bθ(l), γlI) and third,
evaluate ηθ(x).
To sample from the posterior predictive distribution, a fourth step is
added: sample a y from N

ηθ(x), σ2
ˆθ(x)I

of the lth network. The sampling procedure is
summarized in Algorithm 2.
7

Algorithm 2: sample from the posterior of the regression function π(η|x, D) and
from the posterior predictive distribution π(y|x, D)
Result: sample from π(η|x, D): steps 1.-3.; sample from π(y|x, D): steps 1.-4.;
1. sample lth network uniformly from l ∈{1, . . . , L};
2. sample network weights θ(l) from N(bθ(l), γlI);
3. evaluate ηθ(l)(x);
4. evaluate σ2
ˆθ(l)(x) and sample from N

ηθ(l)(x), σ2
ˆθ(l)(x)I

;
4
Results
In this section we numerically compare the classical deep ensembles [13] (DE) to the in-
troduced extended approach (DE extended). The implemented code including additional
examples is provided 1. Following Algorithm 1, the network training is exactly the same
for the classical and the extended deep ensembles. The inferred regression function is
identical in both cases, namely the average over the estimates of all ensemble members
1
L
PL
l=1 ηˆθ(l)(x) at the trained MAPs bθ. The diﬀerence only lies in the additional term
of the extended covariance matrix in (14), which is related to the epistemic part of the
uncertainty.
The architecture of all networks is fully-connected with three hidden layers consisting
of 128, 64, and 32 neurons, respectively. Training is performed over 60 epochs with a
learning rate drop factor of 0.1 for the last ﬁve epochs, and a mini batch size of 64. The
regularization parameter λ is always set to one over the number of available training data.
All data of the output space are normalized by subtracting the mean and dividing the
standard deviation of the corresponding training set. The input data are uniformly drawn
from the support [−1, 1]. Training and test sets are disjoint. All presented results refer to
the test data. The hyperparameters and the network architecture are not ﬁne-tuned as
the achieved accuracy suﬃces for the purpose of this work. Uncertainties are understood
as 95% credible intervals which were calculated in terms of an assumed Gaussian distri-
bution using the approximate variance formulas (i.e. 1.96 times the standard deviation).
Coverage probabilities refer to the number of test cases that lie in their 95% credible
interval.
Figure 3 shows the predicted regression function and the resulting total and epistemic
uncertainties for the classical and extended deep ensembles, together with the ground
truth. The data y(x) ∈R, x ∈[−1, 1], are normally distributed around the regression
function η(x) = 1
2 ((4.5x)4 −(18x)2 + 22.5x) with a standard deviation of 10. An ensemble
of 10 networks was trained on 200 data samples with a learning rate equal to 1/200.
The total uncertainty is much greater than the epistemic part of the uncertainty for the
classical deep ensembles, when comparing the ﬁrst two images of the ﬁrst row.
This
means that the aleatoric part of the uncertainty is dominating. The total uncertainty
almost perfectly covers the underlying disturbed data and hence, the approximation to
the posterior predictive distribution seems to work well for both approaches. However,
the second column shows that the uncertainty for the inferred regression function covers
the ground truth in less than 60% of the considered test cases for the conventional deep
ensembles, i.e. the conventional deep ensembles underrates the uncertainty. In contrast,
1BayesianDeepEnsembles
8

Figure 3: The results are plotted for the classical deep ensembles (ﬁrst row) and the
proposed extension (second row). The coverage probabilities (cp) and root mean squared
errors (rmse) are given for the estimated posterior predictive distribution (ﬁrst column)
and the regression function (second column). The estimates, predictions and correspond-
ing uncertainties are plotted (in blue) together with the underlying disturbed and undis-
turbed ground truth data (in red). In the third column, the epistemic uncertainties are
plotted together with the errors of the estimated regression function to the ground truth.
the extended deep ensembles cover the regression function completely as shown in the
second row (second and third image), and hence yield a more reliable, yet to some extent
conservative, uncertainty quantiﬁcation.
Another comparison between the classical and extended deep ensembles is given in Figure
4 which analyzes the dependency of the amount of training data and the number of trained
ensemble members on the epistemic uncertainty. The data samples y(x) ∈R, x ∈[−1, 1]2,
are normally distributed around the regression function η(x) = P2
i=1(1.5xi−1)2(1.3xi+1)2
with a standard deviation of 0.2. The results of the ﬁrst column of Figure 4 are based on
an ensemble of 10 networks which are trained with a learning rate of 0.001. The ensembles
yielding the plots in the second column are trained on 600 data samples with a learning
rate of 1/600. The results show that the extended approach consistently outperforms the
classical deep ensembles with a coverage probability of over 80% in almost every setting.
Even for a single network, the coverage of the extended method is about 70% while there
is no epistemic uncertainty at all in the classical method for this case. We observe in
this example that the uncertainty is not further improved when increasing the number
of networks for the ensemble. The uncertainties of the two approaches are expected to
converge for an inﬁnite amount of training data which seems to be supported by the
observed results. Finally, we note that the root mean squared error is the same for the
two approaches. It decreases for a growing amount of training data and is more stable
for an increasing number of ensemble members.
9

Figure 4: The coverage probabilities for the regression function of the classical deep en-
sembles approach and its suggested extension are plotted in the ﬁrst row in dependence
on a growing amount of training data and a growing amount of ensemble members, re-
spectively. The second row shows the corresponding root mean squared errors.
5
Conclusions and Outlook
In this work, we have explicitly derived the deep ensembles [13] as an approximation to
a conventional Bayesian inference by stating the underlying approximations and assump-
tions, including the required statistical model and choice of prior. We have developed an
extension to deep ensembles that improves its uncertainty quantiﬁcation without loosing
any of the advantages of the approach, and without altering its training procedure or
inferred regression function. This is achieved by replacing the delta distributions taken
as an approximation to the posterior for the network parameters in a Bayesian frame-
work with normal distributions having ﬁnite variances. The corresponding variances can
be computed analytically in a post-processing step by minimizing the Kullback-Leibler
divergence between approximate and true posterior. The introduced method can also be
employed to easily state an epistemic uncertainty for a single trained neural network.
The proposed extension of the deep ensembles yields an additional term to the covari-
ance matrix of the estimated regression function that results in an enlarged epistemic
uncertainty. Simple numerical experiments illustrate the eﬀect of the extended approach,
leading to a more appropriate uncertainty quantiﬁcation for the inferred regression func-
tion than classical deep ensembles.
The scope of this paper is to state explicitly the underlying approximations and assump-
tions needed to view the deep ensembles as an approximate Bayesian method, and to
introduce a straightforward and easy to implement extension that leads to an improved
uncertainty quantiﬁcation for this state-of-the-art method. The basis of the proposed
extension is similar to ideas that have already been put forward in related work [30, 27].
In contrast to those approaches, however, the extension of deep ensembles presented here
does not alter the actual method, but simply adds a post-processing step. A compar-
ison of results obtained by the proposed approach with those achieved by the related
methods [30, 27] is of interest. However, due to the similarity in their basis the diﬀerent
methods are expected to produce similar results. In order to draw reliable conclusions,
a comprehensive and carefully designed study is thus required which we defer to future
10

work. Future work could also address the adaption to classiﬁcation problems, replacing
the linear unit of the network output with a convolution layer for imaging tasks, and
relaxing the uniform weights 1/L of the Gaussian mixture distributions. Combining our
extension with other methods that improve the accuracy of deep ensembles [42] or their
extrapolation power [25] could be analyzed as well.
Acknowledgement
The authors thank Shinichi Nakajima for proofreading the manuscript and valuable re-
marks.
References
[1] Y. Sun, X. Huang, D. Kroening, J. Sharp, M. Hill, and R. Ashmore, “Testing deep
neural networks,” arXiv preprint arXiv:1803.04792, 2018.
[2] Y. Tian, K. Pei, S. Jana, and B. Ray, “Deeptest: Automated testing of deep-neural-
network-driven autonomous cars,” in Proceedings of the 40th international conference
on software engineering, pp. 303–314, 2018.
[3] W. Samek, G. Montavon, A. Vedaldi, L. K. Hansen, and K.-R. M¨uller, Explain-
able AI: interpreting, explaining and visualizing deep learning, vol. 11700. Springer
Nature, 2019.
[4] F. Baldassarre and H. Azizpour, “Explainability techniques for graph convolutional
networks,” arXiv preprint arXiv:1905.13686, 2019.
[5] D. Levi, L. Gispan, N. Giladi, and E. Fetaya, “Evaluating and calibrating uncertainty
prediction in regression tasks,” arXiv preprint arXiv:1905.11659, 2019.
[6] J. Yao, W. Pan, S. Ghosh, and F. Doshi-Velez, “Quality of uncertainty quantiﬁcation
for Bayesian neural network inference,” arXiv preprint arXiv:1906.09686, 2019.
[7] C. Leibig, V. Allken, M. S. Ayhan, P. Berens, and S. Wahl, “Leveraging uncer-
tainty information from deep neural networks for disease detection,” Scientiﬁc re-
ports, vol. 7, no. 1, pp. 1–14, 2017.
[8] R. Michelmore, M. Wicker, L. Laurenti, L. Cardelli, Y. Gal, and M. Kwiatkowska,
“Uncertainty quantiﬁcation with statistical guarantees in end-to-end autonomous
driving control,” in 2020 IEEE International Conference on Robotics and Automation
(ICRA), pp. 7344–7350, IEEE, 2020.
[9] M. Abdar, F. Pourpanah, S. Hussain, D. Rezazadegan, L. Liu, M. Ghavamzadeh,
P. Fieguth, A. Khosravi, U. R. Acharya, V. Makarenkov, et al., “A review of un-
certainty quantiﬁcation in deep learning: Techniques, applications and challenges,”
arXiv preprint arXiv:2011.06225, 2020.
[10] L. V. Jospin, W. Buntine, F. Boussaid, H. Laga, and M. Bennamoun, “Hands-
on Bayesian neural networks–a tutorial for deep learning users,” arXiv preprint
arXiv:2007.06823, 2020.
11

[11] D. P. Kingma, T. Salimans, and M. Welling, “Variational dropout and the local
reparameterization trick,” arXiv preprint arXiv:1506.02557, 2015.
[12] Y. Gal and Z. Ghahramani, “Dropout as a Bayesian approximation: Representing
model uncertainty in deep learning,” in international conference on machine learning,
pp. 1050–1059, PMLR, 2016.
[13] B. Lakshminarayanan, A. Pritzel, and C. Blundell, “Simple and scalable predictive
uncertainty estimation using deep ensembles,” in Advances in Neural Information
Processing Systems, vol. 30, pp. 6402–6413, Curran Associates, Inc., 2017.
[14] J. Caldeira and B. Nord, “Deeply uncertain: comparing methods of uncertainty quan-
tiﬁcation in deep learning algorithms,” Machine Learning: Science and Technology,
vol. 2, no. 1, p. 015002, 2020.
[15] F. K. Gustafsson, M. Danelljan, and T. B. Schon, “Evaluating scalable Bayesian
deep learning methods for robust computer vision,” in Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition Workshops, pp. 318–319,
2020.
[16] Y. Ovadia, E. Fertig, J. Ren, Z. Nado, D. Sculley, S. Nowozin, J. V. Dillon, B. Lak-
shminarayanan, and J. Snoek, “Can you trust your model’s uncertainty? evaluating
predictive uncertainty under dataset shift,” arXiv preprint arXiv:1906.02530, 2019.
[17] G. Scalia, C. A. Grambow, B. Pernici, Y.-P. Li, and W. H. Green, “Evaluating
scalable uncertainty estimation methods for deep learning-based molecular property
prediction,” Journal of chemical information and modeling, vol. 60, no. 6, pp. 2697–
2717, 2020.
[18] L. Hoﬀmann, I. Fortmeier, and C. Elster, “Uncertainty quantiﬁcation by en-
semble learning for computational optical form measurements,” arXiv preprint
arXiv:2103.01259, 2021.
[19] L. K. Hansen and P. Salamon, “Neural network ensembles,” IEEE transactions on
pattern analysis and machine intelligence, vol. 12, no. 10, pp. 993–1001, 1990.
[20] J. G. Carney, P. Cunningham, and U. Bhagwan, “Conﬁdence and prediction inter-
vals for neural network ensembles,” in IJCNN’99. International Joint Conference on
Neural Networks. Proceedings (Cat. No. 99CH36339), vol. 2, pp. 1215–1218, IEEE,
1999.
[21] O. Sagi and L. Rokach, “Ensemble learning: A survey,” Wiley Interdisciplinary Re-
views: Data Mining and Knowledge Discovery, vol. 8, no. 4, p. e1249, 2018.
[22] A. Ashukha, A. Lyzhov, D. Molchanov, and D. Vetrov, “Pitfalls of in-domain uncer-
tainty estimation and ensembling in deep learning,” arXiv preprint arXiv:2002.06470,
2020.
[23] S. Hu, N. Pezzotti, and M. Welling, “A new perspective on uncertainty quantiﬁcation
of deep ensembles,” arXiv e-prints, pp. arXiv–2002, 2020.
12

[24] D. Duvenaud, D. Maclaurin, and R. Adams, “Early stopping as nonparametric vari-
ational inference,” in Artiﬁcial Intelligence and Statistics, pp. 1070–1077, PMLR,
2016.
[25] T. Pearce, F. Leibfried, and A. Brintrup, “Uncertainty in neural networks: Approx-
imately Bayesian ensembling,” in International conference on artiﬁcial intelligence
and statistics, pp. 234–244, PMLR, 2020.
[26] B. He, B. Lakshminarayanan, and Y. W. Teh, “Bayesian deep ensembles via the
neural tangent kernel,” arXiv preprint arXiv:2007.05864, 2020.
[27] A. G. Wilson and P. Izmailov, “Bayesian deep learning and a probabilistic perspective
of generalization,” arXiv preprint arXiv:2002.08791, 2020.
[28] J. A. Hoeting, D. Madigan, A. E. Raftery, and C. T. Volinsky, “Bayesian model
averaging: A tutorial,” Statistical Science, vol. 14, no. 4, pp. 382–401, 1999.
[29] H. Ritter, A. Botev, and D. Barber, “A scalable Laplace approximation for neural
networks,” in 6th International Conference on Learning Representations, ICLR 2018-
Conference Track Proceedings, vol. 6, International Conference on Representation
Learning, 2018.
[30] W. J. Maddox, P. Izmailov, T. Garipov, D. P. Vetrov, and A. G. Wilson, “A simple
baseline for Bayesian uncertainty in deep learning,” Advances in Neural Information
Processing Systems, vol. 32, pp. 13153–13164, 2019.
[31] J. Lampinen and A. Vehtari, “Bayesian approach for neural networks—review and
case studies,” Neural networks, vol. 14, no. 3, pp. 257–274, 2001.
[32] D. M. Blei, A. Kucukelbir, and J. D. McAuliﬀe, “Variational inference: A review
for statisticians,” Journal of the American statistical Association, vol. 112, no. 518,
pp. 859–877, 2017.
[33] J. Zeng, A. Lesnikowski, and J. M. Alvarez, “The relevance of Bayesian layer po-
sitioning to model uncertainty in deep Bayesian active learning,” arXiv preprint
arXiv:1811.12535, 2018.
[34] N. Brosse, C. Riquelme, A. Martin, S. Gelly, and ´E. Moulines, “On last-layer al-
gorithms for classiﬁcation: Decoupling representation from uncertainty estimation,”
arXiv preprint arXiv:2001.08049, 2020.
[35] A. Kristiadi, M. Hein, and P. Hennig, “Being Bayesian, even just a bit, ﬁxes overconﬁ-
dence in relu networks,” in International Conference on Machine Learning, pp. 5436–
5446, PMLR, 2020.
[36] A. Gelman, J. B. Carlin, H. S. Stern, D. B. Dunson, A. Vehtari, and D. B. Rubin,
Bayesian data analysis. CRC press, 2013.
[37] C. Robert, The Bayesian choice: from decision-theoretic foundations to computa-
tional implementation. Springer Science & Business Media, 2007.
13

[38] A. O’Hagan and J. J. Forster, Kendall’s advanced theory of statistics, volume 2B:
Bayesian inference, vol. 2. Arnold, 2004.
[39] Y. Gal, Uncertainty in Deep Learning. PhD thesis, University of Cambridge, 2016.
[40] E. H¨ullermeier and W. Waegeman, “Aleatoric and epistemic uncertainty in machine
learning: An introduction to concepts and methods,” Machine Learning, vol. 110,
no. 3, pp. 457–506, 2021.
[41] S. Fort, H. Hu, and B. Lakshminarayanan, “Deep ensembles: A loss landscape per-
spective,” arXiv preprint arXiv:1912.02757, 2019.
[42] F. Wenzel, J. Snoek, D. Tran, and R. Jenatton, “Hyperparameter ensembles for
robustness and uncertainty quantiﬁcation,” arXiv preprint arXiv:2006.13570, 2020.
Appendix A
A.1
Proof of Lemma 1
Let the data D = {(xi, yi), i = 1, . . . , N} be iid samples following the regression model in
(1). Each neural network of the ensemble is independently trained by minimizing the loss
function
loss = −L(θ; D) + 1
2λ∥θ∥2 , L(θ; D) = −1
2
N
X
i=1
∥yi −ηθ(xi)∥2
σ2
θ(xi)
+ py log(σ2
θ(xi))

, (15)
where ∥.∥is the L2 norm, and L(θ; D) equals (up to a constant) the log likelihood for the
statistical model in (1). The obtained point estimates of the trained network parameters
are thus (local) MAP estimates when assigning the prior (3) for θ. By taking the average
of L probability mass functions at the local MAPs as an approximation to the posterior
π(θ|D) (cf. (2)), one obtains the posterior predictive distribution
π(y|x, D) =
Z
π(θ|D)N
 y; ηθ(x), σ2
θ(x)I

dθ ≈1
L
L
X
l=1
N
 y; ηbθ(l)(x), σ2
bθ(l)(x)I

,
which equals the expression given in [13] for the deep ensembles.
A.2
Mean and covariance matrix of the posterior predictive dis-
tribution for the original deep ensembles approach
The ﬁrst (5) and central second (6) moments of the approximation (4) to the posterior
predictive distribution are calculated as follows:
14

E (y|x, D)
=
Z
yπ(y|x, D)dy
≈
1
L
L
X
l=1
Z
yN
 y; ηbθ(l)(x), σ2
bθ(l)(x)I

dy
=
1
L
L
X
l=1
ηbθ(l)(x) ,
Cov (y|x, D)
=
Z h
(y −E (y|x, D)) (y −E (y|x, D))Ti
π(y|x, D)dy
≈
1
L
L
X
l=1
Z
[
 ηbθ(l)(x) −E (y|x, D)
  ηbθ(l)(x) −E (y|x, D)
T
+
 y −ηbθ(l)(x)
  ηbθ(l)(x) −E (y|x, D)
T
+
 ηbθ(l)(x) −E (y|x, D)
  y −ηbθ(l)(x)
T
+
 y −ηbθ(l)(x)
  y −ηbθ(l)(x)
T] N
 y; ηbθ(l)(x), σ2
bθ(l)(x)I

dy
=
1
L
L
X
l=1
h ηbθ(l)(x) −E (y|x, D)
  ηbθ(l)(x) −E (y|x, D)
T + σ2
bθ(l)(x)I
i
.
A.3
Proof of Theorem 1
Application of the change-of-variables formula and the approximation (2) to the posterior
π(θ|D) yield for the regression function η ≡ηθ(x):
π(η|x, D)
=
Z
δ(η −ηθ(x))π(θ|D)dθ
≈
Z
δ(η −ηθ(x)) 1
L
L
X
l=1
δ(θ −bθ(l))dθ
=
1
L
L
X
l=1
δ(η −ηbθ(l)(x)) .
Expectation and covariance matrix then follow immediately:
E (η|x, D)
=
Z
ηπ(η|x, D)dη
≈
1
L
L
X
l=1
Z
ηδ(η −ηbθ(l)(x))dη
=
1
L
L
X
l=1
ηbθ(l)(x) ,
15

Cov (η|x, D)
=
Z h
(η −E (η|x, D)) (η −E (η|x, D))Ti
π(η|x, D)dη
≈
1
L
L
X
l=1
Z
[
 ηbθ(l)(x) −E (η|x, D)
  ηbθ(l)(x) −E (η|x, D)
T
+
 η −ηbθ(l)(x)
  ηbθ(l)(x) −E (η|x, D)
T
+
 ηbθ(l)(x) −E (η|x, D)
  η −ηbθ(l)(x)
T
+
 η −ηbθ(l)(x)
  η −ηbθ(l)(x)
T] δ(η −ηbθ(l)(x))dη
=
1
L
L
X
l=1
h ηbθ(l)(x) −E (η|x, D)
  ηbθ(l)(x) −E (η|x, D)
Ti
.
A.4
Proof of Theorem 2
The objective function to be maximized is the ELBO from equation (11):
ELBO = Eq log p(D|θ) −KL (q(θ)∥π(θ)) .
The ﬁrst term of equation (11) is the expectation of the log likelihood (up to a constant):
Eq log p(D|θ) = −1
2L
L
X
l=1
N
X
i=1
Z
N

θ; bθ(l), γlI
  
∥yi −ηθ(xi)∥2
σ2
ˆθ(l)(xi)
+ py log(σ2
ˆθ(l)(xi))
!
dθ,
(16)
where ∥.∥refers to the L2 norm. Note that the parameters of the last layer related to
the variances σ2
ˆθ(l) of the statistical model (1) are ﬁxed after the network training. The
parameters θ of the corresponding mean ηθ are drawn only from the last layer to the
output without any nonlinearity. Hence, the network output of the lth ensemble member
can be written as
ηθ(x) = Wθ bη(l)(x), Wθ = Wˆθ(l) + √γlE ∈Rpy×pˆη, E. ∼N(0, I),
(17)
where bη(l)(x) is the output of the next to last layer of the trained network. The depen-
dence of bη(l)(x) on θ is suppressed, assuming that the corresponding MAP estimates for
θ are taken. Wθ contains the parameters of the linear unit which are drawn from a nor-
mal distribution around the MAPs Wˆθ(l) with the same elementwise variance γl for each
parameter. Here, the operation “. ∼” means “elementwise drawn”. E is a matrix with
the same dimensionality as Wˆθ(l), i.e. the output dimension py times the dimension of the
last layer pˆη.
Using (17), the following equations hold:
Z
N

θ; bθ(l), γlI

∥y −ηθ(x)∥2dθ
= EE

∥y −(Wˆθ(l) + √γlE)ˆη(l)(x)∥2
(18)
= EE

∥y −Wˆθ(l) ˆη(l)(x)∥2 −2(y −Wˆθ(l) ˆη(l)(x))T(√γlE ˆη(l)(x)) + ∥√γlE ˆη(l)(x)∥2
= ∥y −ηˆθ(l)(x)∥2 + γlEE

∥E ˆη(l)(x)∥2
,
(19)
16

where ηˆθ(l) is the trained network with MAP estimates. Furthermore, it holds:
EE

∥Ebη(l)(x)∥2
=
EE

(Ebη(l)(x))T(Ebη(l)(x))

(20)
=
py
X
r=1
pˆη
X
α,β=1
bη(l)(x)αE [Er,αEr,β] bη(l)(x)β
(21)
=
py
X
r=1
pˆη
X
α=1
bη(l)(x)αE

E2
r,α

bη(l)(x)α
(22)
=
py
X
r=1
pˆη
X
α=1
bη(l)(x)α
 Var(Er,α) + E [Er,α]2
bη(l)(x)α
(23)
=
py∥bη(l)(x)∥2 .
(24)
Plugging (19) and (24) into equation (16) yields:
Eq log p(D|θ) = −1
2L
L
X
l=1
N
X
i=1
 
∥yi −ηˆθ(l)(x)∥2 + γlpy∥bη(l)(x)∥2
σ2
ˆθ(l)(xi)
+ py log(σ2
ˆθ(l)(xi))
!
. (25)
The second term of the ELBO (11) is the Kulback-Leibler divergence between the ap-
proximation to the posterior q(θ) and the chosen prior π(θ). It holds:
KL (q(θ)∥π(θ))
=
Z
q(θ) log( q(θ)
π(θ))dθ
(26)
=
Z 1
L
L
X
l=1
N

θ; bθ(l), γlI

log


1
L
PL
r=1 N

θ; bθ(r), γrI

π(θ)

dθ
(27)
≈
1
L
L
X
l=1
Z
N

θ; bθ(l), γlI

log


1
LN

θ; bθ(l), γlI

π(θ)

dθ
(28)
=
1
L
L
X
l=1
KL

N(θ; bθ(l), γlI)∥π(θ)

−1
L
L
X
l=1
log(L) .
(29)
The approximation is valid under the assumption that the between-variability of the MAP
estimates is large compared to the within-variability, since then
Z
N

θ; bθ(l), γlI

log
 L
X
r=1
N

θ; bθ(r), γrI
!
dθ
≈
Z
N

θ; bθ(l), γlI

log

N

θ; bθ(l), γlI

dθ
(30)
holds.
It is well known that the Kulback-Leibler divergence between two Gaussians can be cal-
culated analytically. It follows:
KL

N(θ; bθ(l), γlI)∥π(θ)

=
KL

N(θ; bθ(l), γlI)∥N(θ; 0, λ−1I)

(31)
=
1
2

pθγlλ + λ∥bθ(l)∥2 −pθ −pθ log(γlλ)

,
(32)
17

where pθ := dim(bθ(l)), l = 1, . . . , L. Here, pθ = pypˆη, as only the weights Wθ of the last
linear unit of the network are random. It follows with (29):
KL (q(θ)∥π(θ)) = 1
2L
L
X
l=1
 pypˆηγlλ + λ∥Wˆθ(l)∥2
F −pypˆη −pypˆη log(γlλ)

−1
L
L
X
l=1
log(L) ,
(33)
where ∥· · · ∥F denotes the Frobenius norm.
Now, the ELBO is given by combining (25) and (33):
ELBO
=
Eq log p(D|θ) −KL (q(θ)∥π(θ))
=
−1
2L
L
X
l=1
N
X
i=1
 
∥yi −ηˆθ(l)(xi)∥2 + γlpy∥bη(l)(xi)∥2
σ2
ˆθ(l)(xi)
+ py log(σ2
ˆθ(l)(xi))
!
−
 
1
2L
L
X
l=1
 pypˆηγlλ + λ∥Wˆθ(l)∥2
F −pypˆη −pypˆη log(γlλ)

−1
L
L
X
l=1
log(L)
!
=
1
L
L
X
l=1
[log(L) −1
2{
N
X
i=1
 
∥yi −ηˆθ(l)(xi)∥2
σ2
ˆθ(l)(xi)
+ py log(σ2
ˆθ(l)(xi))
!
+ λ∥Wˆθ(l)∥2
F −pypˆη
−
pypˆη log(λ)} −1
2
 N
X
i=1
py∥bη(l)(xi)∥2
σ2
ˆθ(l)(xi)
+ pypˆηλ
!
γl + 1
2pypˆη log(γl)].
(34)
In total, the ELBO takes the form:
ELBO = 1
L
L
X
l=1
(al + blγl + cl log(γl)) ,
(35)
where al, bl, cl are given analytically in dependence on the (local) MAP estimates bθ(l), l =
1, . . . , L. The maximizer of (35) w.r.t. the γl is given by
γl = −cl
bl
=
pˆη
PN
i=1
∥bη(l)(xi)∥2
σ2
ˆθ(l)(xi) + pˆηλ
, l = 1, . . . , L .
(36)
Note that bl ̸= 0 holds. It can be expected that PN
i=1
∥bη(l)(xi)∥2
σ2
ˆθ(l)(xi) is unbounded as N grows,
because the individual networks are trained independently, and under this assumption it
follows that γl →0 for N →∞.
A.5
Proof of Theorem 3
The proof is straightforward in analogy to the proof of Theorem 1 (cf. Appendix A.3),
with the following addition:
Z  η −ηbθ(l)(x)
  η −ηbθ(l)(x)
T δ(η −ηθ(x))N

θ; bθ(l), γlI

dθdη ≈γlJlJT
l , l = 1, . . . , L ,
18

(37)
using the ﬁrst order Taylor approximation ηθ(x) ≈ηbθ(l)(x) + Jl(θ −bθ(l)) of ηθ for θ in the
vicinity of bθ(l).
The derivative Jl can be easily calculated based on the output of the next to last layer of
the trained networks (cf. Appendix A.4 (17)) and equation (13).
19

