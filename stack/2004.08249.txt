Understanding the Difﬁculty of Training Transformers
Liyuan Liu†‡ Xiaodong Liu‡ Jianfeng Gao‡ Weizhu Chen§ Jiawei Han†
{ll2, hanj}@illinois.edu , {xiaodl,jfgao,wzchen}@microsoft.com
†University of Illinois at Urbana-Champaign
‡Microsoft Research
§ Microsoft Dynamics 365 AI
Abstract
Transformers have proved effective in many
NLP tasks. However, their training requires
non-trivial efforts regarding designing cutting-
edge optimizers and learning rate schedulers
carefully (e.g., conventional SGD fails to train
Transformers effectively). Our objective here
is to understand what complicates Transformer
training from both empirical and theoretical
perspectives. Our analysis reveals that unbal-
anced gradients are not the root cause of the
instability of training.
Instead, we identify
an ampliﬁcation effect that inﬂuences training
substantially – for each layer in a multi-layer
Transformer model, heavy dependency on its
residual branch makes training unstable, since
it ampliﬁes small parameter perturbations (e.g.,
parameter updates) and results in signiﬁcant
disturbances in the model output. Yet we ob-
serve that a light dependency limits the model
potential and leads to inferior trained models.
Inspired by our analysis, we propose Admin
(Adaptive model initialization) to stabilize sta-
bilize the early stage’s training and unleash its
full potential in the late stage. Extensive exper-
iments show that Admin is more stable, con-
verges faster, and leads to better performance1.
1
Introduction
Transformers (Vaswani et al., 2017) have led to
a series of breakthroughs in various deep learn-
ing tasks (Devlin et al., 2019; Velickovic et al.,
2018). They do not contain recurrent connections
and can parallelize all computations in the same
layer, thus improving effectiveness, efﬁciency, and
scalability. Training Transformers, however, re-
quires extra efforts. For example, although stochas-
tic gradient descent (SGD) is the standard algo-
rithm for conventional RNNs and CNNs, it con-
verges to bad/suspicious local optima for Trans-
1Implementations are released at: https://github.
com/LiyuanLucasLiu/Transforemr-Clinic
0
50
100
4.4
4.6
4.8
5.0
Dev PPL on WMT’14 En-De
18-Layer Transformer
50
100
4.6
4.7
4.8
4.9
6-layer Post-LN
converges, but
18-layer Post-LN
does not.
6-Layer Transformer
Pre-LN
Post-LN
Admin
(Post-LN)
Epoch # (iterations over the training set)
Figure 1: Lacking enough robustness and stability, the
18-Layer Post-LN Transformer training (i.e.the original
architecture) diverges and is omitted in the left graph.
Admin not only stabilizes model training but unleashes
the model potential for better performance.
formers (Zhang et al., 2019b). Moreover, com-
paring to other neural architectures, removing the
warmup stage in Transformer training results in
more severe consequences such as model diver-
gence (Popel and Bojar, 2018; Liu et al., 2020a).
Here, we conduct comprehensive analyses in empir-
ical and theoretical manners to answer the question:
what complicates Transformer training.
Our analysis starts from the observation: the
original Transformer (referred to as Post-LN) is
less robust than its Pre-LN variant2 (Baevski and
Auli, 2019; Xiong et al., 2019; Nguyen and Salazar,
2019). We recognize that gradient vanishing issue
is not the direct reason causing such difference,
since ﬁxing this issue alone cannot stabilize Post-
LN training. It implies that, besides unbalanced gra-
dients, there exist other factors inﬂuencing model
training greatly.
With further analysis, we recognize that for each
Transformer residual block, the dependency on its
2As in Figure 2, Post-LN places layer norm outside of
residual blocks, and Pre-LN moves them to the inside.
arXiv:2004.08249v2  [cs.LG]  18 Sep 2020

Attention
Add
Layer Norm
FFN
Add
Layer Norm
Attention
Add
Layer Norm
FFN
Add
Layer Norm
Attention
Add
Layer Norm
x(od)
0
x(oe)
0
Layer Norm
Attention
Add
Layer Norm
FFN
Add
⇥N
Layer Norm
Attention
Add
Layer Norm
Attention
Add
Layer Norm
FFN
Add
⇥N
x(pd)
0
x(pe)
0
Pre-LN
Post-LN
Encoder
Encoder Decoder
Decoder
⇤(pd)
⇤(pe)
⇤(oe)
⇤(od)
: Pre-LN decoder
: Pre-LN encoder
: Post-LN encoder
: Post-LN decoder
: sub-layers outputs (i.e., FFN, Self-Attention and Encoder-Attention) 
Notation Table
: intermediate output
: residual output N: layer #
D: hidden #
H: head #
⇥N
⇥N
Layer Norm
x(pe)
Layer Norm
x(pd)
x(od)
x(oe)
x(od)
3i−3
x(od)
3i−2
x(od)
3i−1
x(od)
3i
a(od)
3i−2
b(od)
3i−2
a(od)
3i−1
b(od)
3i−1
a(od)
3i
b(od)
3i
x(oe)
2i−2
x(oe)
2i−1
x(oe)
2i
a(oe)
2i−1
b(oe)
2i−1
a(oe)
2i
b(oe)
2i
x(pe)
2i−1
x(pe)
2i−2
x(pe)
2i
x(pd)
3i−3
x(pd)
3i−2
x(pd)
3i−1
x(pd)
3i
x
a
b
ˆ⇤: normalized outputs, i.e., Var[ˆ⇤] = 1
Var[·]: dimension-wise variance
Figure 2: The Architecture and notations of Pre-LN Transformers (Left) and Post-LN Transformers (Right).
residual branch3 plays an essential role in training
stability. First, we ﬁnd that a Post-LN layer has a
heavier dependency on its residual branch than a
Pre-LN layer. As in Figure 7, at initialization, a
Pre-LN layer has roughly the same dependency on
its residual branch and any previous layer, whereas
a Post-LN layer has a stronger dependency on its
residual branch (more discussions are elaborated in
Section 4.1). We ﬁnd that strong dependencies of
Post-LN amplify ﬂuctuations brought by parameter
changes and destabilize the training (as in Theo-
rem 2 and Figure 4). Besides, the loose reliance
on residual branches in Pre-LN generally limits the
algorithm’s potential and often produces inferior
models.
In light of our analysis, we propose Admin, an
adaptive initialization method which retains the
merits of Pre-LN stability without hurting the per-
formance. It restricts the layer dependency on its
residual branches in the early stage and unleashes
the model potential in the late stage. We conduct
experiments on IWSLT’14 De-En, WMT’14 En-
De, and WMT’14 En-Fr; Admin is more stable,
converges faster, and achieves better performance.
For example, without introducing any additional
hyper-parameters, Admin successfully stabilizes
72-layer Transformer training on WMT’14 En-Fr
and achieves a 43.80 BLEU score.
3For a residual block x + f(x), its shortcut output refers
to x, its residual branch output refers to f(x), and the depen-
dency on its residual branch refers to
Var[f(x)]
Var[x+f(x)].
2
Preliminaries
Transformer Architectures and Notations. The
Transformer architecture contains two types of sub-
layers, i.e., Attention sub-layers and Feedforward
(FFN) sub-layers. They are composed of mainly
three basic modules (Vaswani et al., 2017), i.e.,
Layer Norm (fLN), Multi-head Attention (fATT),
and Feedforward Network (fFFN).
As illustrated in Figure 2, the Pre-LN Trans-
former and the Post-LN Transformer organize
these modules differently. For example, a Pre-
LN encoder organizes the Self-Attention sub-
layer as x(pe)
2i−1
=
x(pe)
2i−2 + fS-ATT(fLN(x(pe)
2i−2))
and a Post-LN encoder as x(oe)
2i−1 = fLN(x(oe)
2i−2 +
fS-ATT(x(oe)
2i−2)), where x(·)
2i−2 is the input of the i-
th Transformer layer and x(·)
2i−1 is the output of
the i-th Self-Attention sub-layer. Here, we refer
fS-ATT(fLN(x(pe)
2i−2)) and fS-ATT(x(oe)
2i−2) as the resid-
ual branches and their outputs as the residual out-
puts, in contrast to layer/sub-layer outputs, which
integrates residual outputs and shortcut outputs.
Notation elaborations are shown in Figure 2. In
particular, we use superscripts to indicate network
architectures (i.e., the Pre-LN Encoder), use sub-
scripts to indicate layer indexes (top layers have
larger indexes), all inputs and outputs are formu-
lated as Sequence-Len × Hidden-Dim.
Layer Norm. Layer norm (Ba et al., 2016) plays a
vital role in Transformer architecture. It is deﬁned

10°1
100
Pre-LN Encoder
Post-LN Encoder
Pre-LN Deocder
Post-LN Deocder
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
10°1
100
Gradient vanishing only happens in backpropagations for Encoder-Attention sub-layers
i.e., from Encoder-Attention outputs to Self-Attention outputs.
Self Attention (PostLN Decoder)
Encoder Attention (PostLN Decoder)
Feedforward (PostLN Decoder)
Figure 3: Relative gradient norm histogram (on a log scale) of 18-layer Transformers on the WMT’14 En-De
dataset, i.e., the gradient norm of sub-layer outputs, scaled by the largest gradient norm in the same network.
as fLN(x) = γ x−µ
σ
+ ν, where µ and σ are the
mean and standard deviation of x.
Feedforward Network. Transformers use two-
layer perceptrons as feedforward networks, i.e.,
fFFN(x) = φ(xW (1))W (2), where φ(·) is the non-
linear function4, and W (·) are parameters.
Multi-head Attention.
Multi-head Attentions
allows the network to have multiple focuses
in a single layer and plays a crucial role in
many tasks (Chen et al., 2018).
It is de-
ﬁned as (with H heads):
fATT(q, k, v)
=
PH
h=1 fs(qW (Q)
h
W (K)
h
kT )vW (V1)
h
W (V2)
h
, where
fs is the row-wise softmax function and W (·)
h
are
parameters. W (Q)
h
and W (V1)
h
are D × D
H matrices,
W (K)
h
and W (V2)
h
are D
H × D matrices, where D
is the hidden state dimension. Parameters with-
out subscript refer the concatenation of all H-
head parameters, e.g., W (Q) = [W (Q)
1
, · · · , W (Q)
H ].
In Transformer, this module is used in two dif-
ferent settings: Encoder-Attention (fE-ATT(x) =
fATT(x, x(·e), x(·e)) and x(·e) is the encoder output),
and Self-Attention (fS-ATT(x) = fATT(x, x, x)).
3
Unbalanced Gradients
In this study, we strive to answer the question:
what complicates Transformer training. Our analy-
sis starts from the observation: Pre-LN training is
more robust than Post-LN, while Post-LN is more
likely to reach a better performance than Pre-LN.
In a parameter grid search (as in Figure 10), Pre-LN
4Our analysis uses ReLU as the activation function, while
Admin can be applied to other non-linear functions.
converges in all 15 settings, and Post-LN diverges
in 7 out of 15 settings; when Post-LN converges,
it outperforms Pre-LN in 7 out of 8 settings. We
seek to reveal the underlying factor that destabilizes
Post-LN training and restricts the performance of
Pre-LN.
In this section, we focus on the unbalanced gra-
dients (e.g., gradient vanishing). We ﬁnd that, al-
though Post-LN suffers from gradient vanishing
and Pre-LN does not, gradient vanishing is not the
direct reason causing the instability of Post-LN.
Speciﬁcally, we ﬁrst theoretically and empirically
establish that only Post-LN decoders suffer from
gradient vanishing and Post-LN encoders do not.
We then observe that ﬁxing the gradient vanishing
issue alone cannot stabilize training.
3.1
Gradients at Initialization
As gradient vanishing can hamper convergence
from the beginning, it has been regarded as the
major issue causing unstable training. Also, re-
cent studies show that this issue exists in the Post-
LN Transformer, even after using residual connec-
tions (Xiong et al., 2019). Below, we establish that
only Post-LN decoders suffer from the gradient
vanishing, and neither Post-LN encoders, Pre-LN
encoders, nor Pre-LN decoders.
We use ∆x to denote gradients, i.e., ∆x = ∂L
∂x
where L is the training objective. Following previ-
ous studies (Glorot and Bengio, 2010), we analyze
the gradient distribution at the very beginning of
training and ﬁnd only Encoder-Attention sub-layers
in Post-LN suffers from gradient vanishing.
First, we conduct analysis from a theoretical

0
100
200
0
500
1000
1500
2000
100
101
102
0
50
100
150
0
100
200
100
101
102
103
104
105
Pre-LN
Post-LN
Admin (Post-LN)
Num of Sub-Layers (FFN or Self-Attention) in the Encoder
Random Perturbations, i.e.,  
Gradient Updates, i.e., 
|F(x0, W) −F(x0, W ⇤)|2
2
W ⇤= W + δ
Post-LN is less 
stable than Pre-LN 
Post-LN:|F −F⇤|2
2 = O(N)
Pre-LN |F −F⇤|2
2 = O(log N)
Admin:
R2 = 0.99
R2 = 0.99
W ⇤= W + Adam(rW L(F))
Figure 4:
Encoder output changes for parameter changes, i.e., |F(x0, W) −
F(x0, W ∗)|2
2 where W ∗−W is random perturbations (left) or gradient updates
(right). Intuitively, very large |F −F∗| indicates the training to be ill-conditioned.
0
50
100
10°2
10°1
100
Relative
Gradient Norm
10°1
100
Relative Parameter
Update Norm
Epoch # (iterations over the training set)
The update magnitude is 
consistent,  even with 
unbalanced gradients.
18-Layer Pre-LN  Encoder Self-Attention
Light color indicates higher layers
Figure 5: Histogram of rel-
ative norm of gradient and
|Wi+1 −Wi| where Wi is
the checkpoint saved after
training for i epochs.
Encoder
Decoder
Gradient
Training
Post-LN
Post-LN
Varnishing
Diverged
Post-LN
Pre-LN
Varnishing
Diverged
Pre-LN
Pre-LN
Varnishing
Converged
Table 1: Changing decoders from Post-LN to Pre-LN
ﬁxes gradient vanishing, but does not stabilize model
training successfully. Encoder/Decoder have 18 layers.
perspective. Similar to Xiong et al. (2019), we
establish that Pre-LN networks do not suffer from
gradient vanishing (as elaborated in Appendix A.1).
Unlike Xiong et al. (2019), we recognize that not
all Post-LN networks suffer from gradient vanish-
ing. As in Theorem 1, we establish that Post-LN
Encoder networks do not suffer from gradient van-
ishing. Detailed derivations are elaborated in Ap-
pendix A.2.
THEOREM 1. — For Post-LN Encoders, if γ and
ν in the Layer Norm are initialized as 1 and 0 re-
spectively; all other parameters are initialized by
symmetric distributions with zero mean; x(oe)
i
and
∆x(oe)
i
are subject to symmetric distributions with
zero mean; the variance of x(oe)
i
is 1 (i.e., normal-
ized by Layer Norm); ∆x(oe)
i
and the derivatives
of modules in i-th sub-layer are independent, we
have Var[∆xi−1] ≥Var[∆xi].
To make sure that the assumptions of Theo-
rem 2 match the real-world situation, we further
conduct empirical veriﬁcation. At initialization,
we calculate ||∆x(·)
i ||2 for 18-layer Transformers5
5Note if E[∆x(p·)
i−1] = 0, Var[∆x(p·)
i−1] ≈|∆x(p·)
i−1|2
2.
and visualize
||∆x(·)
i ||2
maxj ||∆x(·)
j ||2 in Figure 3. It veriﬁes
that only Post-LN decoders suffer from the gradi-
ent vanishing. Besides, we can observe that the
dropping of gradient norms mostly happens in the
backpropagation from encoder-attention outputs
(encoder-attention bars) to its inputs (self-attention
bars, since the output of self-attention is the in-
put of encoder-attention). This pattern is further
explained in Appendix A.3.
3.2
Impact of the Gradient Vanishing
Now, we explore whether gradient vanishing is the
direct cause of training instability.
First, we design a controlled experiment to show
the relationship between gradient vanishing and
training stability. We construct a hybrid Trans-
former by combining a Post-LN encoder and a
Pre-LN decoder. As in Section 3.1, only Post-LN
decoders suffer from gradient vanishing, but not
Post-LN encoders. Therefore, this hybrid Trans-
former does not suffer from gradient vanishing.
As shown in Table 1, ﬁxing gradient vanishing
alone (i.e., changing Post-LN decoders to Pre-LN
decoders) fails to stabilize model training. This
observation provides evidence supporting that the
gradient vanishing issue is not the direct cause of
unstable Post-LN training.
Moreover, we observe that gradients of all at-
tention modules are unbalanced, while adaptive
optimizers mostly address this issue. As in Fig-
ure 5, adaptive optimizers successfully assign dif-
ferent learning rates to different parameters and
lead to consistent update magnitudes even with un-
balanced gradients. It explains why the standard
SGD fails in training Transformers (i.e., lacking the

Attention
FFN
Pre-LN
Attention
ape
1
ape
2
Layer Norm
Attention
FFN
Post-LN
Attention
Layer Norm
aoe
2
aoe
1
Layer Norm
Figure 6: The major difference between Pre-LN and
Post-LN is the position of layer norms.
ability to handle unbalanced gradients) and necessi-
tates using adaptive optimizers. More discussions
are included in Appendix A.4.
4
Instability from Ampliﬁcation Effect
We ﬁnd that unbalanced gradients are not the root
cause of the instability of Post-LN, which implies
the existence of other factors inﬂuencing model
training. Now, we go beyond gradient vanishing
and introduce the ampliﬁcation effect. Speciﬁcally,
we ﬁrst examine the difference between Pre-LN
and Post-LN, including their early-stage and late-
stage training. Then, we show that Post-LN’s train-
ing instability is attributed to layer dependency’s
ampliﬁcation effect, which intensiﬁes gradient up-
dates and destabilizes training.
4.1
Impact of Layer Norms Positions
As described in Section 2, both Pre-LN and Post-
LN employ layer norm to regularize inputs and out-
puts. Different residual outputs are aggregated and
normalized in residual networks before serving as
inputs of other layers (i.e., residual outputs will be
scaled to ensure the integrated input to have a con-
sistent variance). To some extend, layer norm treats
the variance of residual outputs as weights to aver-
age them. For example, for Post-LN Self-Attention,
we have x(o·)
2i−1 =
x(o·)
2i−2+a(o·)
2i−1
q
Var[x(o·)
2i−2]+Var[a(o·)
2i−1] at initial-
ization. Larger Var[a(o·)
2i−2] not only increases the
proportion of a(o·)
2i−2 in x(o·)
2i−2 but decreases the pro-
portion of other residual outputs. Intuitively, this is
similar to the weight mechanism of the weighted
average.
The position of layer norms is the major differ-
ence between Pre-LN and Post-LN and makes them
aggregate residual outputs differently (i.e., using
different weights). As in Figure 6, all residual out-
puts in Pre-LN are only normalized once before
feeding into other layers (thus only treating resid-
ual output variances as weights); in Post-LN, most
x1
x2
x3
x4
x5
x6
x7
x8
x9
x10
x11
x12
x1
a1
x2
a2
x3
a3
x4
a4
x5
a5
x6
a6
x7
a7
x8
a8
x9
a9
x10
a10
x11
a11
x12
a12
a1 a2 a3 a4 a5 a6 a7 a8 a9 a10 a11 a12
0.1
0.2
0.3
0.4
0.5
0.6
6-Layer Post-LN
6-Layer Pre-LN
At Initialization
After 100 Epochs
Comparing ﬁnal models, Post-LN layer has a larger dependency on its residual branch.
branch outputs. 
branch outputs. 
Post-LN layer outputs always
 
depend more on its residual  
Pre-LN layer outputs learn to
 
 depend more on its residual
Figure 7: βi,j in 6-Layer Post-LN and Pre-LN on the
WMT-14 En-De dataset (contains 12 sub-layers).
residual outputs are normalized more than once,
and different residual outputs are normalized for
different times. For example, if all layers are initial-
ized in the same way, output variances of different
Pre-LN residual branches would be similar, and the
aggregation would be similar to the simple average.
Similarly, for Post-LN, nearby residual outputs are
normalized by fewer times than others, thus having
relatively larger weights. We proceed to calculate
and analyze these weights to understand the impact
of layer norm positions.
First, we use bai to refer
ai
√Var ai (i.e., normal-
ized outputs of i-th residual branch) and bxi to re-
fer
xi
√Var xi (i.e., normalized outputs of i-th layer
or normalized inputs of (i+1)-th residual branch).
Then, we describe their relationships as bxi =
P
j≤i βi,jbaj, where βi,j integrates scaling opera-
tions of all layer norms (including
p
Var[ai]). For
example, Pre-LN sets βi,j =
√
Var[aj]
√
Var[P
k≤i ak]. Intu-
itively, βi,j describes the proportion of j-th residual
branch outputs in i-th layer outputs, thus reﬂects
the dependency among layers.
We visualize βi,j in Figure 7. For a Post-LN
layer, its outputs rely more on its residual branch
from the initialization to the end. At initialization,
Pre-LN layer outputs have roughly the same re-
liance on all previous residual branches. As the
training advances, each layer starts to rely more on
its own residual outputs. However, comparing to
Post-LN, Pre-LN layer outputs in the ﬁnal model
still has less reliance on their residual branches.
Intuitively, it is harder for Pre-LN layers to de-
pend too much on their own residual branches. In

Pre-LN, layer outputs (i.e., x(p·)
i
) are not normal-
ized, and their variances are likely to be larger for
higher layers6. Since βi,i =
√
Var[ai]
q
Var[x(p·)
i−1+ai], βi,i
is likely to be smaller for higher layers, which re-
stricts i-th layer outputs from depending too much
on its residual branch and inhibits the network from
reaching its full potential. In other words, Pre-LN
restricts the network from being too deep (i.e., if
it is hard to distinguish x(p·)
i
and x(p·)
i+1, appending
one layer would be similar to doubling the width
of the last layer), while Post-LN gives the network
the choice of being wider or deeper.
4.2
Ampliﬁcation Effect at Initialization
Although depending more on residual branches al-
lows the model to have a larger potential, it ampli-
ﬁes the ﬂuctuation brought by parameter changes.
For a network bx = F(x0, W) where x0 is the
model input and W is the parameter, the out-
put change caused by parameter perturbations is
Var[F(x0, W)−F(x0, W ∗)], where W ∗= W +δ.
Its relationship with N is described in Theorem 2,
and the derivation is elaborated in Appendix B.
THEOREM 2. — Consider a N-layer Transformer
bx = F(bx0, W) at initialization, where bx0 is the
input and W is the parameter. If the layer depen-
dency stays the same after a parameter change (i.e.,
βi,j has the same value after changing W to W ∗,
where W is randomly initialized and δ = W ∗−W
is independent to W), the output change (i.e.,
Var[F(x0, W) −F(x0, W ∗)]) can be estimated
as PN
i=1 β2
i,iC where C is a constant.
If Var[ai] is the same for all layers, Pre-LN sets
β2
i,i as 1/i, and Post-LN sets β2
i,i as a constant.
Thus, we have Corollary 1 and 2 as below.
COROLLARY 1. — For a N-layer Pre-LN F, we
have Var[F(x0, W) −F(x0, W ∗)] = O(log N).
COROLLARY 2. — For a N-layer Post-LN F, we
have Var[F(x0, W) −F(x0, W ∗)] = O(N).
They show that, since Post-LN relies more on
residual branches than Pre-LN (i.e., has a larger
β2
i,i), the perturbation is ampliﬁed to a larger mag-
nitude. To empirically verify these relationships,
we calculate |F(x0, W) −F(x0, W ∗)|2
2 for Pre-
LN and Post-LN and visualize the results in Fig-
6If a0 and a1 are independent, Var[a0 + a1] = Var[a0] +
Var[a1]; also, in our experiments Var[x(p·)
i
] increases as i
becomes larger
ure 4. In Corollary 2, N is linearly associated with
|F −F∗|2
2 for Post-LN; and in Corollary 1, log N
is linearly associated with |F −F∗|2
2 for Pre-LN.
These relationships match the observation in our
experiments (as in Figure 4). For further veriﬁca-
tion, we measure their correlation magnitudes by
R2 and ﬁnd R2 = 0.99 in both cases.
Moreover, we replace the random noise δ with
optimization updates (i.e., setting W ∗= W +
Adam(∆W), where opt(·) is update calculated by
the Adam optimizer) and visualize output shifts.
This replacement makes the correlation between
|F −F∗|2
2 and N (for Post-LN) or log N (for Pre-
LN) to be weaker (i.e., R2 = 0.75). Still, as in
Figure 4, the output shift |F −F∗|2
2 for Post-LN is
larger than Pre-LN by multiple magnitudes.
Intuitively, large output shifts would destabilize
the training (Li et al., 2018). Also, as elaborated
in Appendix B, the constant C in Theorem 2 is
related to network derivatives and would be smaller
as training advances, which explains why warmup
is also helpful for the standard SGD. Therefore, we
conjecture it is the large output shift of Post-LN
results in unstable training. We proceed to stabilize
Post-LN by controlling the dependency on residual
branches in the early stage of training.
4.3
Admin – Adaptive Model Initialization
In light of our analysis, we add additional param-
eters (i.e., ω) to control residual dependencies of
Post-LN and stabilize training by adaptively initial-
izing ω to ensure an O(log N) output change.
Due to different training conﬁgurations and
model speciﬁcities (e.g., different models may use
different activation functions and dropout ratios),
it is hard to derive a universal initialization method.
Instead, we decompose model initialization into
two phrases: Proﬁling and Initialization. Specif-
ically, Admin adds new parameters ω and con-
structs its i-th sub-layer as xi = fLN(bi), where
bi = xi−1 · ωi + fi(xi−1), ωi is a D-dimension
vector and · is element-wise product. Then the
Proﬁling phrase and Initialization phrase are:
Proﬁling. After initializing the network with a
standard method (initializing ωi as 1), conduct for-
ward propagation without parameter updating and
record the output variance of residual branches (i.e.,
calculate Var[fi(xi−1)]). Since all elements in the
same parameter/output matrix are independent to
each other and are subject to the same distribution,
it is sufﬁcient to use a small number of instances in

0.05
0.10
0.15
0.20
0.25
0.30
0.35
0.40
18-Layer Admin (Post-LN)
18-Layer Pre-LN
At Initialization
After 100 Epochs
Normalized Layer Outputs
x1
x1
a1
a1
Normalized Outputs of Residual Branches
Admin stabilizes 
model training by 
avoid over-large
With a Post-LN structure, 
Admin allows layer outputs
dependencies.
of the ﬁnal model to 
depend more on 
their residual 
branches.
Figure 8: βi,j of 18-Layer Admin (Post-LN) and Pre-
LN on the WMT-14 En-De dataset.
this phrase. In our experiments, the ﬁrst batch (no
more than 8192 tokens) is used.
Initialization. Set ωi =
qP
j<i Var[fj(xj−1)]
and initialize all other parameters with the same
method used in the Proﬁling phrase.
In the early stage, Admin sets β2
i,i to approxi-
mately 1
i and ensures an O(log N) output change,
thus stabilizing training. Model training would be-
come more stable in the late stage (the constant
C in Theorem 2 is related to parameter gradients),
and each layer has the ﬂexibility to adjust ω and
depends more on its residual branch to calculate the
layer outputs. After training ﬁnishes, Admin can
be reparameterized as the conventional Post-LN
structure (i.e., removing ω). More implementation
details are elaborated in Appendix C.
To verify our intuition, we calculate the layer
dependency of 18-Layer models and visualize the
result in Figure 8. Figures 7 and 8 show that Admin
avoids over-large dependencies at initialization and
unleashes the potential to make the layer outputs
depend more on their residual outputs in the ﬁnal
model. Moreover, we visualize the output change
of Admin in Figure 4. Beneﬁting from the adap-
tive initialization, the output change of Admin gets
roughly the same increase speed as Pre-LN, even
constructed in the Post-LN manner. Also, although
Admin is formulated in a Post-LN manner and
suffers from gradient vanishing, 18-layer Admin
successfully converges and outperforms 18-layer
Pre-LN (as in Table 2). This evidence supports
our intuition that the large dependency on resid-
ual branches ampliﬁes the output ﬂuctuation and
destabilizes training.
0
50
4.4
4.6
4.8
5.0
Dev PPL on WMT’14 En-De
12-Layer Transformer
50
75
4.8
4.9
5.0
5.1
5.2
Dev PPL on IWSLT’14 De-En
Transformer Small
Pre-LN
Post-LN
Admin
(Post-LN)
Figure 9: Development PPL on the WMT’14 En-De
dataset and the IWLST’14 De-En dataset.
5
Experiments
We conduct experiments on IWSLT’14 De-En,
WMT’14 En-De, and WMT’14 En-Fr. More details
are elaborated in Appendix D.
5.1
Performance Comparison
We use BLEU as the evaluation matric and sum-
marize the model performance in Table 2. On the
WMT’14 dataset, we use Transformer-base models
with 6, 12, or 18 layers. Admin achieves a bet-
ter performance than Post-LN and Pre-LN in all
three settings. Speciﬁcally, 12-Layer and 18-Layer
Post-LN diverges without the adaptive initializa-
tion. Pre-LN converges in all settings, but it results
in sub-optimal performance. Admin not only sta-
bilizes the training of deeper models but beneﬁts
more from the increased model capacity then Pre-
LN, which veriﬁes our intuition that the Pre-LN
structure limits the model potential. As in Figure 1
and Figure 9, although the 6-layer Pre-LN con-
verges faster than Post-LN, its ﬁnal performance is
worse than Post-LN. In contrast, Admin not only
achieves the same convergence speed with Pre-LN
in the early stage but reaches a good performance
in the late stage.
We use 6-layer Transformer-small (its hidden
dimension is smaller than the base model) on the
IWSLT’14 dataset, and all methods perform sim-
ilarly. Still, as in Figure 10, Admin outperforms
the other two by a small margin. Together with
WMT’14 results, it implies the training stability is
related to layer number. For shallow networks, the
stability difference between Post-LN and Pre-LN
is not signiﬁcant (as in Figure 4), and all methods
reach reasonable performance. It is worth mention-
ing that attention and activation dropouts have an
enormous impact on IWSLT’14, which is smaller
than WMT’14 datasets.

Table 2: BLEU on IWSLT’14 De-En and WMT’14 En-Fr/De (AL-BL refers A-layer encoder & B-layer decoder).
Dataset
IWSLT’14 De-En
WMT’14 En-Fr
WMT’14 En-De
Enc #–Dec #
6L–6L (small)
6L–6L
60L–12L
6L–6L
12L–12L
18L–18L
Post-LN
35.64±0.23
41.29
failed
27.80
failed
failed
Pre-LN
35.50±0.04
40.74
43.10
27.27
28.26
28.38
Admin
35.67±0.15
41.47
43.80
27.90
28.58
29.03
To further explore the potential of Admin, we
train Transformers with a larger size.
Speciﬁ-
cally, we expand the Transformer-base conﬁgu-
ration to have a 60-layer encoder and a 12-layer
decoder. As in Table 2, our method achieves a
BLEU score of 43.8 on the WMT’14 En-Fr dataset,
the new state-of-the-art without using additional
annotations (e.g., back-translation). More discus-
sions are conducted in Appendix F to compare
this model with the current state of the art. Fur-
thermore, in-depth analyses are summarized in
Liu et al. (2020b), including systematic evalua-
tions on the model performance (with TER, ME-
TEOR, and BLEU), comprehensive discussions on
model dimensions (i.e., depth, head number, and
hidden dimension), and ﬁne-grained error analysis.
It is worth mentioning that the 60L-12L Admin
model achieves a 30.1 BLEU score on WMT’14
En-De (Liu et al., 2020b).
5.2
Connection to Warmup
Our previous work (Liu et al., 2020a) establishes
that the need for warmup comes from the unstable
adaptive learning rates in the early stage. Still, re-
moving the warmup phrase results in more severe
consequences for Transformers than other architec-
tures. Also, warmup has been found to be useful
for the vanilla SGD (Xiong et al., 2019).
Theorem 1 establishes that Var[F(x0, W) −
F(x0, W ∗)]
≈
PN
i=1 β2
i,iC
where
C
=
Var[Gi(bx∗
i−1, Wi) −Gi(bx∗
i−1, W ∗
i )]. In the early
stage of training, the network has larger parame-
ter gradients and thus larger C. Therefore, using
a small learning rate at initialization helps to al-
leviate the massive output shift of Post-LN. We
further conduct experiments to explore whether
more prolonged warmups can make up the stabil-
ity difference between Post-LN and Pre-LN. We
observe that 18-layer Post-LN training still fails af-
ter extending the warmup phrase from 8 thousand
updates to 16, 24, and 32 thousand. It shows that
learning rate warmup alone cannot neutralize the
0.999
0.995
0.99
1 × 10−4
2 × 10−4
3 × 10−4
4 × 10−4
5 × 10−4
34.64
34.65
34.41
35.65
35.58
35.51
35.87
0.00
0.00
33.56
0.00
0.00
0.00
0.00
0.00
Post-LN
0.999
0.995
0.99
33.98
33.81
33.76
34.74
34.91
34.87
35.09
35.15
35.19
35.06
35.28
35.31
35.51
35.45
35.55
Pre-LN
0.999
0.995
0.99
34.58
34.53
34.60
35.26
35.03
35.25
35.38
35.62
35.57
35.74
35.89
35.69
35.61
35.83
35.84
Admin (Post-LN)
33.0
33.5
34.0
34.5
35.0
35.5
36.0
Figure 10: BLEU score of Post-LN, Pre-LN and Ad-
min on the IWSLT’14 De-En dataset (x-axis is the
β2 for adaptive optimizers and y-axis is the learning
rate). Pre-LN converges in all settings while Post-LN
diverges in 7 out of 15 settings. When Post-LN con-
verges, it outperforms Pre-LN in 7 out of 8 settings. Ad-
min stabilizes Post-LN training and outperforms Pre-
LN (its best performance is comparable with Post-LN).
instability of Post-LN. Intuitively, massive output
shifts not only require a small learning rate but also
unsmoothes the loss surface (Li et al., 2018) and
make the training ill-conditioned.
Admin regularizes the model behavior at ini-
tialization and stabilizes the training. To explore
whether Admin is able to stabilize the training
alone, we remove the warmup phase and conduct
a grid search on optimizer hyper-parameters. The
results are visualized in Figure 10. It shows that as
Post-LN is more sensitive to the choice of hyper-
parameters, Admin successfully stabilizes the train-
ing without hurting its potential.
5.3
Comparing to Other Initializations
We compare our methods with three initialization
methods, i.e., ReZero (Bachlechner et al., 2020),
FixUp (Zhang et al., 2019a), and LookLinear (Bal-
duzzi et al., 2017a). Speciﬁcally, we ﬁrst conduct
experiments with 18-layer Transformers on the
WMT’14 De-En dataset. In our experiments, we
observe that all of ReZero (which does not con-
tain layer normalization), FixUp (which also does
not contain layer normalization), and LookLinear
(which is incorporated with Post-LN) leads to di-

vergent training. With further analysis, we ﬁnd that
the half-precision training and dropout could desta-
bilize FixUp and ReZero, due to the lack of layer
normalization. Simultaneously, we ﬁnd that even
for shadow networks, having an over small reliance
on residual branches hurts the model performance,
which also supports our intuition. For example,
as elaborated in Appendix E, applying ReZero to
Transformer-small leads to a 1-2 BLEU score drop
on the IWSLT’14 De-En dataset.
6
Related Work
Transformer. Transformer (Vaswani et al., 2017)
has led to a series of breakthroughs in various do-
mains (Devlin et al., 2019; Velickovic et al., 2018;
Huang et al., 2019; Parmar et al., 2018; Ramachan-
dran et al., 2019). Liu et al. (2020a) show that com-
pared to other architectures, removing the warmup
phase is more damaging for Transformers, espe-
cially Post-LN. Similarly, it has been found that
the original Transformer (referred to as Post-LN)
is less robust than its Pre-LN variant (Baevski and
Auli, 2019; Nguyen and Salazar, 2019; Wang et al.,
2019). Our studies go beyond the existing litera-
ture on gradient vanishing (Xiong et al., 2019) and
identify an essential factor inﬂuencing Transformer
training greatly.
Deep Network Initialization.
It has been ob-
served that deeper networks can lead to better per-
formance. For example, Dong et al. (2020) ﬁnd that
the network depth players a similar role with the
sample number in numerical ODE solvers, which
hinders the system from getting more precise re-
sults. Many attempts have been made to clear obsta-
cles for training deep networks, including various
initialization methods. Based on the independence
among initialized parameters, one method is de-
rived and found to be useful to handle the gradient
vanishing (Glorot and Bengio, 2010). Similar meth-
ods are further developed for ReLU networks (He
et al., 2015). He et al. (2016) ﬁnd that deep net-
work training is still hard even after addressing
the gradient vanishing issue and propose residual
networks. Balduzzi et al. (2017b) identiﬁes the
shattered gradient issue and proposes LookLinear
initialization.
On the other hand, although it is observed that
scaling residual outputs to smaller values helps
to stabilize training (Hanin and Rolnick, 2018;
Mishkin and Matas, 2015; Zhang et al., 2019a;
Bachlechner et al., 2020; Goyal et al., 2017), there
is no systematic analysis on what complicates
Transformer training or its underlying connection
to the dependency on residual branches. Here, we
identify that unbalanced gradients are not the di-
rect cause of the Post-LN instability, recognize the
ampliﬁcation effect, and propose a novel adaptive
initialization method.
7
Conclusion
In this paper, we study the difﬁculties of training
Transformers in theoretical and empirical manners.
Our study in Section 3 suggests that the gradient
vanishing problem is not the root cause of unsta-
ble Transformer training. Also, the unbalanced
gradient distribution issue is mostly addressed by
adaptive optimizers. In Section 4, we reveal the
root cause of the instability to be the strong depen-
dency on residual branches, which ampliﬁes the
ﬂuctuation caused by parameter changes and desta-
bilizes model training. In light of our analysis, we
propose Admin, an adaptive initialization method
to stabilize Transformers training. It controls the
dependency at the beginning of training and main-
tains the ﬂexibility to capture those dependencies
once training stabilizes. Extensive experiments ver-
ify our intuitions and show that, without introduc-
ing additional hyper-parameters, Admin achieves
more stable training, faster convergence, and better
performance.
Our work opens up new possibilities to not only
further push the state-of-the-art but understand
deep network training better. It leads to many inter-
esting future works, including generalizing Theo-
rem 2 to other models, designing new algorithms
to automatically adapt deep networks to different
training conﬁgurations, upgrading the Transformer
architecture, and applying our proposed Admin to
conduct training in a larger scale.
Acknowledge
We thank all reviewers for their constructive com-
ments; Chengyu Dong, Haoming Jiang, Jingbo
Shang, Xiaotao Gu, and Zihan Wang for valuable
discussions and comments; Jingbo Shang for shar-
ing GPU machines; and Microsoft for setting up
GPU machines. The research was sponsored in
part by DARPA No. W911NF-17-C-0099 and No.
FA8750-19-2-1004, National Science Foundation
IIS-19-56151, IIS-17-41317, IIS 17-04532, and IIS
16-18481, and DTRA HDTRA11810026.

References
Jimmy Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton.
2016. Layer normalization. ArXiv, abs/1607.06450.
Thomas C. Bachlechner, Bodhisattwa Prasad Ma-
jumder, Huanru Henry Mao, Garrison W. Cottrell,
and Julian J. McAuley. 2020.
Rezero is all you
need:
Fast convergence at large depth.
ArXiv,
abs/2003.04887.
Alexei Baevski and Michael Auli. 2019. Adaptive in-
put representations for neural language modeling. In
ICLR.
David Balduzzi, Marcus Frean, Lennox Leary, J. P.
Lewis, Kurt Wan-Duo Ma, and Brian McWilliams.
2017a. The shattered gradients problem: If resnets
are the answer, then what is the question? In ICML.
David Balduzzi, Marcus Frean, Lennox Leary, J P
Lewis, Kurt Wan-Duo Ma, and Brian McWilliams.
2017b. The shattered gradients problem: If resnets
are the answer, then what is the question? In ICML.
Yoshua Bengio, Patrice Y. Simard, and Paolo Frasconi.
1994. Learning long-term dependencies with gradi-
ent descent is difﬁcult. IEEE transactions on neural
networks.
Ondˇrej Bojar, Christian Buck, Christian Federmann,
Barry Haddow, Philipp Koehn, Johannes Leveling,
Christof Monz, Pavel Pecina, Matt Post, Herve
Saint-Amand, et al. 2014.
Findings of the 2014
workshop on statistical machine translation.
In
Workshop on Statistical Machine Translation.
Mauro Cettolo, Jan Niehues, Sebastian Stüker, Luisa
Bentivogli, and Marcello Federico. 2014. Report on
the 11th iwslt evaluation campaign, iwslt 2014. In
International Workshop on Spoken Language Trans-
lation, Hanoi, Vietnam.
Mia Xu Chen, Orhan Firat, Ankur Bapna, Melvin
Johnson, Wolfgang Macherey, George Foster, Llion
Jones, Niki Parmar, Michael Schuster, Zhi-Feng
Chen, Yonghui Wu, and Macduff Hughes. 2018.
The best of both worlds: Combining recent advances
in neural machine translation. In ACL.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. Bert: Pre-training of deep
bidirectional transformers for language understand-
ing. In NAACL-HLT.
Chengyu Dong, Liyuan Liu, Zichao Li, and Jingbo
Shang. 2020.
Towards adaptive residual network
training: A neural-ode perspective. In ICML.
Xavier Glorot and Yoshua Bengio. 2010. Understand-
ing the difﬁculty of training deep feedforward neural
networks. In AISTATS.
Priya Goyal, Piotr Dollár, Ross B. Girshick, Pieter No-
ordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew
Tulloch, Yangqing Jia, and Kaiming He. 2017. Ac-
curate, large minibatch sgd: Training imagenet in 1
hour. ArXiv, abs/1706.02677.
Boris Hanin and David Rolnick. 2018. How to start
training: The effect of initialization and architecture.
In NeurIPS.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
Sun. 2015. Delving deep into rectiﬁers: Surpassing
human-level performance on imagenet classiﬁcation.
In ICCV.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
Sun. 2016. Deep residual learning for image recog-
nition. In CVPR.
Cheng-Zhi Anna Huang,
Ashish Vaswani,
Jakob
Uszkoreit, Ian Simon, Curtis Hawthorne, Noam
Shazeer, Andrew M. Dai, Matthew D. Hoffman,
Monica Dinculescu, and Douglas Eck. 2019. Music
transformer: Generating music with long-term struc-
ture. In ICLR.
Hao Li, Zheng Xu, Gavin Taylor, and Tom Goldstein.
2018. Visualizing the loss landscape of neural nets.
In NeurIPS.
Liyuan Liu, Haoming Jiang, Pengcheng He, Weizhu
Chen, Xiaodong Liu, Jianfeng Gao, and Jiawei Han.
2020a. On the variance of the adaptive learning rate
and beyond. In ICLR.
Xiaodong Liu, Kevin Duh, Liyuan Liu, and Jianfeng
Gao. 2020b. Very deep transformers for neural ma-
chine translation. ArXiv, abs/2008.07772.
Yiping Lu, Zhuohan Li, Di He, Zhiqing Sun, Bin Dong,
Tao Qin, Liwei Wang, and Tie-Yan Liu. 2020. Un-
derstanding and improving transformer from a multi-
particle dynamic system point of view.
In ICLR
Workshop DeepDiffEq.
Dmytro Mishkin and Juan E. Sala Matas. 2015. All
you need is a good init. In ICLR.
Toan Q. Nguyen and Julian Salazar. 2019. Transform-
ers without tears: Improving the normalization of
self-attention. In IWSLT.
Myle Ott, Sergey Edunov, Alexei Baevski, Angela
Fan, Sam Gross, Nathan Ng, David Grangier, and
Michael Auli. 2019.
fairseq: A fast, extensible
toolkit for sequence modeling.
In NAACL-HLT
Demonstrations.
Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz
Kaiser, Noam Shazeer, Alexander Ku, and Dustin
Tran. 2018. Image transformer. In ICML.
Martin Popel and Ondrej Bojar. 2018.
Training tips
for the transformer model. The Prague Bulletin of
Mathematical Linguistics, 110:43 – 70.
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine
Lee, Sharan Narang, Michael Matena, Yanqi Zhou,
Wei Li, and Peter J Liu. 2019. Exploring the limits
of transfer learning with a uniﬁed text-to-text trans-
former. ArXiv, abs/1910.10683.

Prajit Ramachandran, Niki Parmar, Ashish Vaswani, Ir-
wan Bello, Anselm Levskaya, and Jonathon Shlens.
2019. Stand-alone self-attention in vision models.
In NeurIPS.
Andrew M Saxe, James L McClelland, and Surya Gan-
guli. 2013. Exact solutions to the nonlinear dynam-
ics of learning in deep linear neural networks. ArXiv,
abs/1312.6120.
Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe,
Jon Shlens, and Zbigniew Wojna. 2016. Rethinking
the inception architecture for computer vision. In
CVPR.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In NIPS.
Petar Velickovic, Guillem Cucurull, Arantxa Casanova,
Adriana Romero, Pietro Liò, and Yoshua Bengio.
2018. Graph attention networks. In ICLR.
Qiang Wang,
Bei Li,
Tong Xiao,
Jingbo Zhu,
Changliang Li, Derek F. Wong, and Lidia S. Chao.
2019.
Learning deep transformer models for ma-
chine translation. In ACL.
Felix Wu, Angela Fan, Alexei Baevski, Yann Dauphin,
and Michael Auli. 2019a. Pay less attention with
lightweight and dynamic convolutions. In ICLR.
Lijun Wu, Yiren Wang, Yingce Xia, Fei Tian, Fei Gao,
Tao Qin, Jianhuang Lai, and Tie-Yan Liu. 2019b.
Depth growing for neural machine translation. In
ACL.
Ruibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shu
xin Zheng, Chen Xing, Huishuai Zhang, Yanyan
Lan, Li-Wei Wang, and Tie-Yan Liu. 2019. On layer
normalization in the transformer architecture. ArXiv,
abs/2002.04745.
Hongyi Zhang, Yann N. Dauphin, and Tengyu Ma.
2019a. Fixup initialization: Residual learning with-
out normalization. In ICLR.
Jingzhao Zhang, Sai Praneeth Karimireddy, Andreas
Veit, Seungyeon Kim, Sashank J. Reddi, Surinder
Kumar, and Suvrit Sra. 2019b. Why adam beats sgd
for attention models. ArXiv, abs/1912.03194.
Guangxiang Zhao, Xu Sun, Jingjing Xu, Zhiyuan
Zhang, and Liangchen Luo. 2019. Muse: Parallel
multi-scale attention for sequence to sequence learn-
ing. ArXiv, abs/1911.09483.

Appendices
A
Gradients at Initialization
Here, we ﬁrst reveal that Pre-LN does not suffer from the gradient vanishing. Then we establish that only
the Post-LN decoder suffers from the gradient vanishing, but not the Post-LN encoder. For simplicity,
we use ∆x to denote gradients, i.e., ∆x = ∂L
∂x where L is the training objective. Following the previous
study (Bengio et al., 1994; Glorot and Bengio, 2010; He et al., 2015; Saxe et al., 2013), we analyze the
gradient distribution at the very beginning of training, assume that the randomly initialized parameters
and the partial derivative with regard to module inputs are independent.
A.1
Pre-LN Analysis
For Pre-LN encoders, we have x(pe)
2i
= x(pe)
2i−1 + fFFN(fLN(x(pe)
2i−1)) and ∆x(pe)
2i−1 = ∆x(pe)
2i (1 +
∂fFFN(fLN(x(pe)
2i−1))
∂x(pe)
2i
). At initialization, the two terms on the right part are approximately independent
and E[
∂fFFN(fLN(z(pe)
2i−1))
∂x(pe)
2i
] = 0. Therefore we have Var[∆x(pe)
2i−1] ≥Var[∆x(pe)
2i ]. Similarly, we can get
Var[∆x(pe)
2i−2] ≥Var[∆x(pe)
2i−1] thus ∀i ≤j, Var[∆x(pe)
i
] ≥Var[∆x(pe)
j
]. Applying the same analysis
to Pre-LN decoders, we can get ∀i ≤j, Var[∆x(pd)
i
] ≥Var[∆x(pd)
j
]. Thus, lower layers have larger
gradients than higher layers, and gradients do not vanish in the backpropagation.
REMARK 1. — For Pre-LN, if ∀i, ∆x(p·)
i
and the derivatives of modules in the i-th sub-layer are
independent, then ∀i ≤j, Var[∆x(p·)
i
] ≥Var[∆x(p·)
j
].
A.2
Post-LN Encoder Analysis
Different from Pre-LN, x(oe)
i
and x(oe)
i−1 are associated with not only the residual connection but the
layer normalization, which makes it harder to establish the connection on their gradients. After making
assumptions on the model initialization, we ﬁnd that lower layers in Post-LN encoder also have larger
gradients than higher layers, and gradients do not vanish in the backpropagation through the encoder.
THEOREM 1. — For Post-LN Encoders, if γ and ν in the Layer Norm are initialized as 1 and 0
respectively; all other parameters are initialized by symmetric distributions with zero mean; x(oe)
i
and
∆x(oe)
i
are subject to symmetric distributions with zero mean; the variance of x(oe)
i
is 1 (i.e., normalized
by Layer Norm); ∆x(oe)
i
and the derivatives of modules in i-th sub-layer are independent, we have
Var[∆xi−1] ≥Var[∆xi].
Proof. We ﬁrst prove Var[∆x(oe)
2i−1] ≥Var[∆x(oe)
2i ], i.e., the backpropagation through FFN sublayers does
not suffer from gradient vanishing. In Post-LN encoders, the output of FFN sublayers is calculated as
x(oe)
2i
= fLN(b(oe)
2i ) where b(oe)
2i
= x(oe)
2i−1 + max(0, x(oe)
2i−1W (1))W (2). Since at initialization, W (1) and
W (2) are independently randomized by symmetric distributions, we have E[b(oe)
2i ] = 0 and
x(oe)
2i
= x(oe)
2i−1 + max(x(oe)
2i−1W (1), 0)W (2)
σb,2i
where σ2
b,2i = Var[b(oe)
2i ]. Referring to the dimension of W (1) as D ×Df, He et al. (2015) establishes that
Var[max(x(oe)
2i−1W (1), 0)W (2)] = 1
2DDf Var[w(1)] Var[w(2)] Var[x(oe)
2i−1].
Since in Post-LN, x(oe)
2i−1 is the output of layer norm, we have Var[x(oe)
2i−1] = 1. Thus,
σ2
b,2i = Var[b(oe)
2i ] = Var[x(oe)
2i−1] + Var[max(x(oe)
2i−1W (1), 0)W (2)]
= 1 + 1
2DDf Var[w(1)] Var[w(2)].
(1)

Assuming different terms are also independent in the backpropagation, we have
Var[∆x(oe)
2i−1] ≥Var[ 1
σb,2i
(∆x(oe)
2i
+ ∆x(oe)
2i
∂max(x(oe)
2i−1W (1), 0)W (2)
∂x(oe)
2i−1
)].
At initialization, He et al. (2015) establishes that
Var[∆x(oe)
2i
∂max(x(oe)
2i−1W (1), 0)W (2)
∂x(oe)
2i−1
] = 1
2DDf Var[w(1)] Var[w(2)] Var[∆x(oe)
2i ].
Therefore, we have
Var[∆x(oe)
2i−1] ≥
1
σ2
b,2i
(1 + 1
2DDf Var[w(1)] Var[w(2)]) Var[∆x(oe)
2i ].
(2)
Combining Equation 1 with Equation 2, we have
Var[∆x(oe)
2i−1] ≥Var[∆x(oe)
2i ]
(3)
which shows the backpropagation through FFN sublayers does not suffer from gradient vanishing.
Now we proceed to prove that, Var[∆x(oe)
2i−2] ≥Var[∆x(oe)
2i−1], i.e., the backpropagation through
Self-Attention sublayers do not suffer from gradient vanishing. In Post-LN encoders, the output of Self-
Attention sublayers are calculated as x(oe)
2i−1 = fLN(b(oe)
2i−1) where b(oe)
2i−1 = x(oe)
2i−2 + a(oe)
2i−1 and a(od)
2i−1 =
P
h fs(x(oe)
2i−2W (Q)
h
W (K)
h
xT (oe)
2i−2)x(oe)
2i−2W (V1)
h
W (V2)
h
. At initialization, since W (Q), W (K), W (V1), and
W (V2) are independently randomized by symmetric distributions, we have E[b(od)
2i−1] = 0, thus x(oe)
2i−1 =
b(oe)
2i−1
σb,2i−1 , where σ2
b,2i−1 = Var[b(oe)
2i−1] = Var[x(oe)
2i−2] + Var[a(oe)
2i−1].
Referring E[fs2(x(oe)
2i−2W (Q)
h
W (K)
h
xT (oe)
2i−2)] as Ph, we have
Var[a(od)
2i−1] = Var[x(oe)
2i−2W (V1)
h
W (V2)
h
]HPh.
Similar to He et al. (2015), we have
Var[x(oe)
2i−2W (V1)
h
W (V2)
h
] = D2
H Var[x(oe)
2i−2] Var[w(V1)] Var[w(V2)].
Since x(oe)
2i−2 is the output of layer norm, we have Var[x(oe)
2i−2] = 1. Thus,
σ2
b,2i−1 = 1 + D2Ph Var[x(oe)
2i−2] Var[w(V1)] Var[w(V2)].
(4)
In the backpropagation, we have
Var[∆x(oe)
2i−2] ≥Var[
1
σb,2i−1
(∆x(oe)
2i−1 + ∆x(oe)
2i−1
X
h
∂fs(x(oe)
2i−2W (Q)
h
W (K)
h
xT (oe)
2i−2)x(oe)
2i−2W (V1)
h
W (V2)
h
∂x(oe)
2i−2
)]
≥
1
σ2
b,2i−1
(Var[∆x(oe)
2i−1] + Var[∆x(oe)
2i−1
X
h
fs(x(oe)
2i−2W (Q)
h
W (K)
h
xT (oe)
2i−2)∂x(oe)
2i−2W (V1)
h
W (V2)
h
∂x(oe)
2i−2
])
At initialization, we assume ∆x(oe)
2i−1 and model parameters are independent (He et al., 2015), thus
Var[∆x(oe)
2i−1
X
h
fs(x(oe)
2i−2W (Q)
h
W (K)
h
xT (oe)
2i−2)∂x(oe)
2i−2W (V1)
h
W (V2)
h
∂x(oe)
2i−2
]
=D2Ph Var[∆x(oe)
2i−1] Var[w(V1)] Var[w(V2)]

0
50
100
10−2
10−1
100
Relative Gradient
Norm
0
50
100
0
50
100
0
50
100
W (Q)
W (V1)
W (V2)
10−1
100
Relative Parameter
Update Norm
W (K)
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
Although the gradient distribution is unbalanced (e.g., W (V 1) and W (V 2) have larger gradients than W (K) and W (Q)),
adaptive optimizers lead to consistent update magnitudes for diﬀerent parameters.
Epoch # (iterations over the training set)
Figure 11: Relative Norm of Gradient (∆Wi, where Wi is the checkpoint of i-th epoch) and Update (|Wi+1 −Wi|)
of Self-Attention Parameters in 12-Layer Pre-LN.
Therefore, we have
Var[∆x(oe)
2i−2] ≥
1
σ2
b,2i−1
(1 + D2Ph Var[w(V1)] Var[w(V2)]) Var[∆x(oe)
2i−1].
(5)
Integrating Equation 4 with Equation 5, we have
Var[∆x(oe)
2i−2] ≥Var[∆x(oe)
2i−1].
(6)
Combining Equation 3 and Equation 6, we have Var[∆xi−1] ≥Var[∆xi].
A.3
Post-LN Decoder Analysis
In Post-LN, the Encoder-Attention sub-layer suffers from gradient vanishing. The Encoder-Attention
sub-layer calculates outputs as x(od)
3i−1 = fLN(b(od)
3i−1) where b(od)
3i−1 = x(od)
3i−2 + a(od)
3i−1 and a(od)
3i−1 =
P
h fs(x(od)
3i−2W (Q)
h
W (K)
h
xT (oe))x(oe)W (V1)
h
W (V2)
h
. Here x(oe) is encoder outputs and fs is the row-wise
softmax function. In the backpropagation, ∆x(od)
3i−2 ≈
∆x(od)
3i−1
σb,3i−1 (1 +
∂a(od)
3i−1
x(od)
3i−2
). All of the backpropagations
from a(od)
3i−1 to x(od)
3i−2 went through the softmax function, we have Var[
∂a(od)
3i−1
x(od)
3i−2
] + 1 ≤σ2
b,3i−1. Thus, those
backpropagations suffer from gradient vanishing. This observation is further veriﬁed in Figure 3, as the
encoder attention bars (gradients of encoder-attention outputs) are always shorter than self-attention bars
(gradients of encoder-attention inputs), while adjacent self-attention bars and fully connected bars usually
have the same length.
A.4
Distributes of Unbalanced Gradients
As in Figure 5 and Figure 11, the gradient distribution of Attention modules is unbalanced even for
Pre-LN. Speciﬁcally, parameters within the softmax function (i.e., W (K) and W (V1)) suffer from gradient
vanishing (i.e., ∂fs(x0,··· ,xi,··· )
∂xi
≤1) and have smaller gradients than other parameters.
With further analysis, we ﬁnd it is hard to neutralize the gradient vanishing of softmax. Unlike
conventional non-linear functions like ReLU or sigmoid, softmax has a dynamic input length (i.e., for
the sentences with different lengths, inputs of softmax have different dimensions). Although this setting
allows Attention modules to handle sequential inputs, it restricts them from having stable and consistent
backpropagation. Speciﬁcally, let us consider the comparison between softmax and sigmoid. For the
sigmoid function, although its derivation is smaller than 1, this damping effect is consistent for all inputs.
Thus, sigmoid can be neutralized by a larger initialization (Glorot and Bengio, 2010). For softmax, its
damping effect is different for different inputs and cannot be neutralized by a static initialization.

Also, we observe that adaptive optimizers largely address this issue. Speciﬁcally, we calculate the
norm of parameter change in consequent epochs (e.g., |W (K)
t+1 −W (K)
t
| where W (K)
t
is the checkpoint
saved after t epochs) and visualize the relative norm (scaled by the largest value in the same network) in
Figure 11. Comparing the relative norm of parameter gradients and parameter updates, we notice that:
although the gradient distribution is unbalanced, adaptive optimizers successfully assign different learning
rates to different parameters and lead to consistent update magnitudes. This result explains why the vanilla
SGD fails for training Transformer (i.e., lacking the ability to handle unbalanced gradient distributions).
Besides, it implies that the unbalanced gradient distribution (e.g., gradient vanishing) has been mostly
addressed by adaptive optimizers and may not signiﬁcantly impact the training instability.
B
Proof of Theorem 2
Here, we elaborate the derivation for Theorem 2, which establishes the relationship between layer number
and output ﬂuctuation brought by parameter change.
THEOREM 2. — Consider a N-layer Transformer bx = F(bx0, W), where bx0 is the input and W is the
parameter. If the layer dependency stays the same after a parameter change (i.e., βi,j has the same value
after changing W to W ∗, where W is randomly initialized and δ = W ∗−W is independent to W), the
output change (i.e., Var[F(x0, W) −F(x0, W ∗)]) can be estimated as PN
i=1 β2
i,iC where C is a constant.
Proof. We refer the module in i sub-layer as ai = Gi(bxi−1, Wi), where bxi = P
j≤i βi,jbaj is the normal-
ized residual output and bai =
ai
√Var ai is the normalized module output. The ﬁnal output is marked as
bx = F(x0, W) = P
j≤N βN,jbaj. To simplify the notation, we use the superscript ∗to indicate variables
related to W ∗, e.g., bx∗= F(x0, W ∗) and a∗
i = Gi(bx∗
i−1, W ∗
i ).
At initialization, all parameters are initialized independently. Thus ∀i ̸= j, bai and baj are independent
and 1 = Var[P
j≤i βi,jbaj] = P
j≤i β2
i,j. Also, since k-layer and (k + 1)-layer share the residual
connection to previous layers, ∀i, j ≤k we have βi,k
βj,k = βi,k+1
βj,k+1 . Thus ∀i ≤k, β2
i,k+1 = (1 −β2
k,k)β2
i,k and
Var[bxi −bx∗
i ] = Var[
X
j≤i
βi,j(baj −ba∗
j)] =
X
j≤i
β2
i,j Var[baj −ba∗
j]
= β2
i,i Var[bai −ba∗
i ] + (1 −β2
i,i) Var[bxi −bx∗
i ].
(7)
Now, we proceed to analyze Var[bai −ba∗
i ]. Speciﬁcally, we have
Var[bai −ba∗
i ] = Var[Gi(bxi−1, Wi) −Gi(bx∗
i−1, W ∗
i )]
= Var[Gi(bxi−1, Wi) −Gi(bx∗
i−1, Wi) + Gi(bx∗
i−1, W ∗
i ) −Gi(bx∗
i−1, W ∗
i )]
= Var[Gi(bxi−1, Wi) −Gi(bx∗
i−1, Wi)] + Var[Gi(bx∗
i−1, Wi) −Gi(bx∗
i−1, W ∗
i )].
(8)
Since W is randomly initialized, Var[Gi(bx∗
i−1, Wi) −Gi(bx∗
i−1, W ∗
i )] should have the same value for
all layers, thus we use a constant C to refer its value (C = Var[Gi(bx∗
i−1, Wi) −Gi(bx∗
i−1, W ∗
i )] and
C ≈|δ|·|∇Gi(bx∗
i−1, Wi)|). As to Var[Gi(bxi−1, Wi)−Gi(bx∗
i−1, Wi)], since the sub-layer of Transformers
are mostly using linear weights with ReLU nonlinearity and 1 = Var[Gi(bxi−1, Wi)] = Var[bxi−1], we
have Var[Gi(bxi−1, Wi) −Gi(bx∗
i−1, Wi)] ≈Var[bxi−1 −bx∗
i−1]. Thus, we can rewrite Equation 8 and get
Var[bai −ba∗
i ] ≈Var[bxi−1 −bx∗
i−1] + C
With Equation 7, we have
Var[bxi −bx∗
i ] = β2
i,i Var[bai −ba∗
i ] + (1 −β2
i,i) Var[bxi −bx∗
i ]
≈β2
i,i(Var[bxi−1 −bx∗
i−1] + C) + (1 −β2
i,i) Var[bxi −bx∗
i ]
= Var[bxi −bx∗
i ] + β2
i,iC
Therefore, we have Var[F(x0, W) −F(x0, W ∗)] ≈PN
i=1 β2
i,iC.

C
Admin Implementation Details
As introduced in Section 4.3, we introduce a new set of parameters to rescale the module outputs.
Speciﬁcally, we refer these new parameters as ω and construct the Post-LN sub-layer as:
xi = fLN(bi), where bi = xi−1 · ωi + fi(xi−1)
where · is the element-wise product.
After training, Admin can be reparameterized as the conventional Post-LN structure (i.e., removing ωi).
Speciﬁcally, we consider xi = bi
σb γ + ν. Then, for feedforward sub-layers, we have
bi = xi−1 · ω + max(0, xi−1W (1))W (2), where xi = bi−1
σb
γ + ν.
It can be reparameterized by changing γ, ν, W (1) to γωi, νωi,
1
ωi W (1) respectively, i.e.,
b′
i = x′
i−1 + max(0, x′
i−1
1
ωi
W (1))W (2), where x′
i−1 = b′
i−1
σb
γωi + νωi.
For Self-Attention sub-layers, we have
bi = xi−1 +
X
h
fs(xi−1W (Q)
h
W (K)
h
xi−1)xi−1W (V1)
h
W (V2)
h
, where xi = bi−1
σb
γ + ν.
It can be reparameterized by changing γ, ν, W (Q)
h
, W (K)
h
, W (V1)
h
to γωi, νωi,
1
ωi W (Q)
h
,
1
ωi W (K)
h
1
ωi W (V1)
h
respectively, i.e.,
b′
i = x′
i−1 +
X
h
fs(x′
i−1
1
ωi
W (Q)
h
W (K)
h
1
ωi
x′
i−1)x′
i−1
1
ωi
W (V1)
h
W (V2)
h
, where x′
i−1 = b′
i−1
σb
γωi + νωi.
For Encoder-Attention sub-layers, we have
bi = xi−1 +
X
h
fs(xi−1W (Q)
h
W (K)
h
x·e)x·eW (V1)
h
W (V2)
h
, where xi = bi−1
σb
γ + ν.
It can be reparameterized by changing γ, ν, W (Q)
h
to γωi, νωi,
1
ωi W (Q)
h
respectively, i.e.,
b′
i = x′
i−1 +
X
h
fs(x′
i−1
1
ωi
W (Q)
h
W (K)
h
x·e)x·e 1
ωi
W (V1)
h
W (V2)
h
, where x′
i−1 = b′
i−1
σb
γωi + νωi.
It is easy to ﬁnd b′
i = bi in all three situations.
From the previous analysis, it is easy to ﬁnd that introducing the additional parameter ωi is equivalent
to rescale some model parameters. In our experiments on IWSLT14 De-En, we ﬁnd that directly rescaling
initialization parameters can get roughly the same performance with introducing ωi. However, it is not
very stable when conducting training in a half-precision manner. Accordingly, we choose to add new
parameters ωi instead of rescaling parameters.
D
Experimental Setup
Our experiments are based on the implementation from the fairseq package (Ott et al., 2019). As to
pre-processing, we follow the public released script from previous work (Ott et al., 2019; Lu et al., 2020).
For WMT’14 datasets, evaluations are conducted on the provided ‘newstest14‘ ﬁle, and more details about
them can be found in Bojar et al. (2014). For the IWSLT’14 De-En dataset, more analysis and details can
be found in Cettolo et al. (2014).

Table 3: ReZero Performance on IWSLT’14 De-En. Models are Transformer-small w. 6-layer encoder & decoder.
Models
Admin
Post-LN
Pre-LN
ReZero
ReZero+Post-LN
BLEU
35.67±0.15
35.64±0.23
35.50±0.04
33.67±0.14
34.67±0.08
Table 4: Performance and model size on WMT’14 En-Fr (AL-BL refers A-layer encoder & B-layer decoder).
Methods
Param. #
dim(W (1)) in FFN
Enc#-Dec#
BLEU
T5-Base (Raffel et al., 2019)
220 M
512 × 2048
6L-6L
41.2
T5-Large (Raffel et al., 2019)
770 M
1024 × 4096
12L-12L
41.5
T5-3B (Raffel et al., 2019)
3 B
1024 × 16384
24L-24L
42.6
T5-11B (Raffel et al., 2019)
11 B
1024 × 65536
24L-24L
43.4
Trans.Big-RNMT+ (Chen et al., 2018)
377 M
1024 × 8192
6L-6L
41.12
DynamicConv (Wu et al., 2019a)
213 M
1024 × 4096
7L-7L
43.2
DG-Transformer (Wu et al., 2019b)
264 M
1024 × 4096
8L-8L
43.27
Prime (Zhao et al., 2019)
252 M
1024 × 4096
6L-6L
43.48
Pre-LN (60L–12L)
262 M
512 × 2048
60L-12L
43.10
Admin (60L–12L)
262 M
512 × 2048
60L-12L
43.80
As to model speciﬁcs, we directly adopt Transformer-small conﬁgurations on the IWSLT’14 De-En
dataset and stacks more layers over the Transformer-base model on the WMT’14 En-De and WMT’14 En-
Fr datasets. Speciﬁcally, on the IWSLT’14 De-En dataset, we use word embedding with 512 dimensions
and 6-layer encoder/decoder with 4 heads and 1024 feedforward dimensions; on the WMT’14 En-De
and WMT’14 En-Fr datasets, we use word embedding with 512 dimension and 8-head encoder/decoder
with 2048 hidden dimensions. Label smoothed cross entropy is used as the objective function with an
uncertainty = 0.1 (Szegedy et al., 2016).
For Model training, we use RAdam as the optimizer (Liu et al., 2020a) and adopt almost all hyper-
parameter settings from Lu et al. (2020). Speciﬁcally, for the WMT’14 En-De and WMT’14 En-Fr dataset,
all dropout ratios (including (activation dropout and attention dropout) are set to 0.1. For the IWSLT’14
De-En dataset, after-layer dropout is set to 0.3, and a weight decay of 0.0001 is used. As to optimizer, we
set (β1, β2) = (0.9, 0.98), use inverse sqrt learning rate scheduler with a warmup phrase (8000 steps on
the WMT’14 En-De/Fr dataset, and 6000 steps on the IWSLT’14 De-En dataset). The maximum learning
rate is set to 1e−3 on the WMT’14 En-De dataset and 7e−4 on the IWSLT’14 De-En and WMT’14 En-Fr
datasets. We conduct training for 100 epochs on the WMT’14 En-De dataset, 90 epochs on the IWSLT’14
De-En dataset and 50 epochs on the WMT’14 En-Fr dataset, while the last 10 checkpoints are averaged
before inference.
On the IWSLT’14 De-En dataset, we conduct training on one NVIDIA GeForce GTX 1080 Ti GPU
and set the maximum batch size to be 4000. On the WMT’14 En-De dataset, we conduct training on
four NVIDIA Quadro R8000 GPUs and set maximum batch size (per GPU) as 8196. On the WMT’14
En-Fr dataset, we conduct training with the Nvidia DGX-2 server (6L-6L uses 4 NVIDIA TESLA V100
GPUs and 60L-16L uses 16 NVIDIA TESLA V100 GPUs) and set the maximum batch size (per GPU)
as 5000. On the IWSLT’14 De-En dataset, Transformer-small models (w. 37 M Param.) take a few
hours to train. On the WMT’14 En-De dataset, 6L-6L models (w. 63 M Param.) take ∼1 day to train,
12L-12L (w. 107M Param.) models take ∼2 days to train, and 18L-18L (w. 151M Param.) models take
∼3 days to train. On the WMT’14 En-Fr dataset, 6L-6L models (w. 67 M Param.) takes ∼2 days to
train, and 60L-12L models (w. 262M Param.) takes ∼2.5 days to train. All training is conducted in
half-precision with dynamic scaling (with a 256-update scaling window and a 0.03125 minimal scale).
All our implementations and pre-trained models would be released publicly.

E
Comparison to ReZero
Here, we ﬁrst conduct comparisons with ReZero (Bachlechner et al., 2020) under two conﬁgurations–
the ﬁrst employs the original ReZero model, and the second adds layer normalizations in a Post-LN
manner. As summarized in Table 3, the ReZero initialization leads to a performance drop, no matter
layer normalization is used or not. It veriﬁes our intuition that over small dependency restricts the model
potential. At the same time, we ﬁnd that adding layer normalization to ReZero helps to improve the
performance. Intuitively, as dropout plays a vital role in regularizing Transformers, layer normalization
helps to not only stabilize training but alleviate the impact of turning off dropouts during the inference.
F
Performance on the WMT’14 En-Fr
To explore the potential of Admin, we conduct experiments with 72-layer Transformers on the WMT’14
En-Fr dataset (with a 60-layer encoder and 12-layer decoder, we add less layers to decoder to encourage
the model to rely more on the source context).
As in Table 4, Admin (60L–12L) achieves a BLEU score of 43.80, the new state-of-the-art on this
long-standing benchmark. This model has a 60-layer encoder and a 12-layer decoder, which is signiﬁcantly
deeper than other baselines. Still, since the number of parameters increases in a quadratic speed with
regard to hidden dimensions and a linear speed with regard to layer numbers, our model has roughly the
same number of parameters with other baselines. It is worth mentioning that Admin even achieves better
performance than all variants of pre-trained T5 models, which demonstrates the great potential of our
proposed method. Also, Admin achieves a better performance than Pre-LN (60L–12L), which further
veriﬁes that the Pre-LN architecture restricts deep models’ potential.

