Jirˇina Vejnarová
Nic Wilson (Eds.)
 123
LNAI 12897
16th European Conference, ECSQARU 2021
Prague, Czech Republic, September 21–24, 2021
Proceedings
Symbolic and Quantitative 
Approaches to Reasoning 
with Uncertainty

Lecture Notes in Artiﬁcial Intelligence
12897
Subseries of Lecture Notes in Computer Science
Series Editors
Randy Goebel
University of Alberta, Edmonton, Canada
Yuzuru Tanaka
Hokkaido University, Sapporo, Japan
Wolfgang Wahlster
DFKI and Saarland University, Saarbrücken, Germany
Founding Editor
Jörg Siekmann
DFKI and Saarland University, Saarbrücken, Germany

More information about this subseries at http://www.springer.com/series1244

Jiˇrina Vejnarová · Nic Wilson (Eds.)
Symbolic and Quantitative
Approaches to Reasoning
with Uncertainty
16th European Conference, ECSQARU 2021
Prague, Czech Republic, September 21–24, 2021
Proceedings

Editors
Jiˇrina Vejnarová
Institute of Information Theory
and Automation
Prague, Czech Republic
Nic Wilson
Insight Centre for Data Analytics,
School of Computer Science and IT
University College Cork
Cork, Ireland
ISSN 0302-9743
ISSN 1611-3349 (electronic)
Lecture Notes in Artiﬁcial Intelligence
ISBN 978-3-030-86771-3
ISBN 978-3-030-86772-0 (eBook)
https://doi.org/10.1007/978-3-030-86772-0
LNCS Sublibrary: SL7 – Artiﬁcial Intelligence
© Springer Nature Switzerland AG 2021, corrected publication 2021
This work is subject to copyright. All rights are reserved by the Publisher, whether the whole or part of the
material is concerned, speciﬁcally the rights of translation, reprinting, reuse of illustrations, recitation,
broadcasting, reproduction on microﬁlms or in any other physical way, and transmission or information
storage and retrieval, electronic adaptation, computer software, or by similar or dissimilar methodology now
known or hereafter developed.
The use of general descriptive names, registered names, trademarks, service marks, etc. in this publication
does not imply, even in the absence of a speciﬁc statement, that such names are exempt from the relevant
protective laws and regulations and therefore free for general use.
The publisher, the authors and the editors are safe to assume that the advice and information in this book are
believed to be true and accurate at the date of publication. Neither the publisher nor the authors or the editors
give a warranty, expressed or implied, with respect to the material contained herein or for any errors or
omissions that may have been made. The publisher remains neutral with regard to jurisdictional claims in
published maps and institutional afﬁliations.
This Springer imprint is published by the registered company Springer Nature Switzerland AG
The registered company address is: Gewerbestrasse 11, 6330 Cham, Switzerland

Preface
The biennial ECSQARU conference is a major forum for advances in the theory and
practice of reasoning under uncertainty. Contributions are provided by researchers in
advancing the state of the art and by practitioners using uncertainty techniques in appli-
cations. The scope of the ECSQARU conferences encompasses fundamental topics, as
well as practical issues, related to representation, inference, learning, and decision mak-
ing both in qualitative and numeric uncertainty paradigms. The formalisms studied in this
volume include argumentation frameworks, belief functions, logics of probability, and
systems for imprecise probability, possibility theory, and reasoning about inconsistency.
Previous ECSQARU events were held in Belgrade (2019), Lugano (2017),
Compiegne (2015), Utrecht (2013), Belfast (2011), Verona (2009), Hammamet (2007),
Barcelona (2005), Aalborg (2003), Toulouse (2001), London (1999), Bonn (1997),
Fribourg (1995), Granada (1993), and Marseille (1991).
The 16th European Conference on Symbolic and Quantitative Approaches to
Reasoning with Uncertainty (ECSQARU 2021) was held in Prague (Czech Republic)
during 21–24 September, 2021. The 48 papers in this volume were selected from 63
submissions, after a rigorous peer-review process by the members of the Program
Committee and some external reviewers. Each submission was reviewed by at least
three reviewers, and almost all received four reviews. ECSQARU 2021 also included
invited talks by outstanding researchers in the ﬁeld: Cassio de Campos, Tomas Kroupa,
and Steven Schockaert.
We would like to thank all those who submitted papers, and the members of the
Program Committee and the external reviewers for their valuable reviews, and also
the members of the local Organizing Committee, Jirka Vomlel, Milan Studený, and
especially Václav Kratochvíl, for all their support and contributions to the success of
the conference. We acknowledge gratefully both material and technical support of the
Institute of Information Theory and Automation of the Czech Academy of Sciences, as
well as the support of Science Foundation Ireland.
Thanks also to Springer Nature for granting a fund for the Best Student Paper Award
of the conference, and for the smooth collaboration when preparing the proceedings.
Moreover, EasyChair proved to be a convenient platform for handling submissions,
reviews, and ﬁnal papers for the proceedings of ECSQARU 2021, which was greatly
appreciated.
August 2021
Jiˇrina Vejnarová
Nic Wilson

Organization
Program Committee Chairs
Jiˇrina Vejnarová
Czech Academy of Sciences, Czech Republic
Nic Wilson
Insight, University College Cork, Ireland
Program Committee
Alessandro Antonucci
IDSIA, Switzerland
Ringo Baumann
Leipzig University, Germany
Christoph Beierle
University of Hagen, Germany
Nahla Ben Amor
Institut Supérieur de Gestion de Tunis, Tunisia
Jonathan Ben-Naim
IRIT-CNRS, France
Salem Benferhat
CRIL, CNRS, Université d’Artois, France
Concha Bielza Lozoya
Universidad Politécnica de Madrid, Spain
Isabelle Bloch
CNRS, LIP6, Sorbonne Université, France
Janneke Bolt
Eindhoven University of Technology, The Netherlands
Richard Booth
Cardiff University, UK
Olivier Cailloux
Université Paris-Dauphine, France
Andrés Cano
University of Granada, Spain
Federico Cerutti
University of Brescia, Italy
Andrea Cohen
CONICET-UNS, Argentina
Giulianella Coletti
University of Perugia, Italy
Inés Couso
University of Oviedo, Spain
Fabio Cozman
University of São Paulo, Brazil
Jasper De Bock
Ghent University, Belgium
Thierry Denoeux
Université de technologie de Compiègne, France
Sébastien Destercke
Université de technologie de Compiègne, France
Dragan Doder
Utrecht University, The Netherlands
F. Dupin De Saint Cyr
Université de Strasbourg/ENGEES, France
Zied Elouedi
Institut Supérieur de Gestion de Tunis, Tunisia
Patricia Everaere
University of Lille, France
Alessandro Facchini
IDSIA, Switzerland
Eduardo Fermé
University of Madeira, Portugal
Tommaso Flaminio
IIIA-CSIC, Spain
Laurent Garcia
Université d’Angers, France
Laura Giordano
Università del Piemonte Orientale, Italy
Lluis Godo
IIIA-CSIC, Spain
Anthony Hunter
University College London, UK
Nebojša Ikodinovi´c
University of Belgrade, Serbia
Ulrich Junker
Biot, France

viii
Organization
Souhila Kaci
LIRMM, France
Gabriele Kern-Isberner
TU Dortmund, Germany
Sébastien Konieczny
CRIL, France
Václav Kratochvíl
Czech Academy of Sciences, Czech Republic
Christophe Labreuche
Thales Research and Technology, France
M. Lagasquie-Schiex
IRIT, Université de Toulouse, CNRS, France
Sylvain Lagrue
Université de technologie de Compiègne, France
Helge Langseth
NTNU, Norway
Florence Le Ber
ENGEES, France
Philippe Leray
University of Nantes, France
Weiru Liu
University of Bristol, UK
Peter Lucas
University of Twente, The Netherlands
Thomas Lukasiewicz
University of Oxford, UK
Carsten Lutz
University of Bremen, Germany
Maria Vanina Martinez
University of Buenos Aires, Argentina
Denis Maua
University of São Paulo, Brazil
Jérôme Mengin
IRIT, Université de Toulouse, France
David Mercier
Université d’Artois, France
Enrique Miranda
University of Oviedo, Spain
Serafín Moral
University of Granada, Spain
Farid Nouioua
LIS, Aix-Marseille University, France
Zoran Ognjanovi´c
Mathematical Institute, Serbia
Odile Papini
LIS, Aix-Marseille University, France
Davide Petturiti
University of Perugia, Italy
Frédéric Pichon
Université d’Artois, France
Nico Potyka
University of Stuttgart, Germany
Marc Pouly
Lucerne University of Applied Sciences and Arts,
Switzerland
Henri Prade
IRIT, CNRS, Toulouse university, France
Silja Renooij
Utrecht University, The Netherlands
Regis Riveret
CSIRO, Australia
Régis Sabbadin
Inrae, France
Steven Schockaert
Cardiff University, UK
Choh Man Teng
Florida Institute for Human & Machine Cognition, USA
Matthias Thimm
Universität Koblenz-Landau, Germany
Matthias Troffaes
Durham University, UK
Jirka Vomlel
Czech Academy of Sciences, Czech Republic
Renata Wassermann
University of São Paulo, Brazil
Emil Weydert
University of Luxembourg, Luxembourg
Marco Wilhelm
TU Dortmund, Germany
Stefan Woltran
Vienna University of Technology, Austria
Eric Würbel
LIS, Aix-Marseille University, France
Bruno Zanuttini
Université de Caen Normandie, France
Cassio P. de Campos
Eindhoven University of Technology, The Netherlands
Leon van der Torre
University of Luxembourg, Luxembourg
Linda C. van der Gaag
Eindhoven University of Technology, The Netherlands

Organization
ix
Additional Reviewers
Matti Berthold
Leipzig University, Germany
Paola Daniela Budán
University of Santiago del Estero, Argentina
Pilar Dellunde
IIIA-CSIC, Spain
Maximilian Heinrich
Leipzig University, Germany
Johannes Marti
University of Amsterdam, The Netherlands
Anna Rapberger
TU Wien, Austria
Olivier Roy
University of Bayreuth, Germany
Milan Studený
Czech Academy of Sciences, Czech Republic
Sara Ugolini
IIIA-CSIC, Spain
Markus Ulbricht
Vienna University of Technology, Austria

Contents
Argumentation and Analogical Reasoning
Analogies Between Sentences: Theoretical Aspects - Preliminary
Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3
Stergos Afantenos, Tarek Kunze, Suryani Lim, Henri Prade,
and Gilles Richard
Non-monotonic Explanation Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
19
Leila Amgoud
Similarity Measures Based on Compiled Arguments . . . . . . . . . . . . . . . . . . . . . . . .
32
Leila Amgoud and Victor David
Necessary and Sufﬁcient Explanations for Argumentation-Based
Conclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
45
AnneMarie Borg and Floris Bex
Addressing Popular Concerns Regarding COVID-19 Vaccination
with Natural Language Argumentation Dialogues . . . . . . . . . . . . . . . . . . . . . . . . . .
59
Lisa Chalaguine and Anthony Hunter
Argument Strength in Probabilistic Argumentation Using Conﬁrmation
Theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
74
Anthony Hunter
The Degrees of Monotony-Dilemma in Abstract Argumentation . . . . . . . . . . . . . .
89
Timotheus Kampik and Dov Gabbay
Constrained Incomplete Argumentation Frameworks . . . . . . . . . . . . . . . . . . . . . . .
103
Jean-Guy Mailly
Complexity of Nonemptiness in Control Argumentation Frameworks . . . . . . . . .
117
Daniel Neugebauer, Jörg Rothe, and Kenneth Skiba
Generalizing Complete Semantics to Bipolar Argumentation Frameworks . . . . .
130
Nico Potyka
Philosophical Reﬂections on Argument Strength and Gradual Acceptability . . . .
144
Henry Prakken

xii
Contents
An Abstract Argumentation and Logic Programming Comparison Based
on 5-Valued Labellings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
159
Samy Sá and João Alcântara
Assumption-Based Argumentation Is Logic Programming with Projection . . . . .
173
Samy Sá and João Alcântara
A Paraconsistent Approach to Deal with Epistemic Inconsistencies
in Argumentation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
187
Rafael Silva and João Alcântara
Gradual Semantics for Weighted Bipolar SETAFs . . . . . . . . . . . . . . . . . . . . . . . . . .
201
Bruno Yun and Srdjan Vesic
Bayesian Networks and Graphical Models
Multi-task Transfer Learning for Bayesian Network Structures . . . . . . . . . . . . . . .
217
Sarah Benikhlef, Philippe Leray, Guillaume Raschia,
Montassar Ben Messaoud, and Fayrouz Sakly
Persuasive Contrastive Explanations for Bayesian Networks . . . . . . . . . . . . . . . . .
229
Tara Koopman and Silja Renooij
Explainable AI Using MAP-Independence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
243
Johan Kwisthout
Bayesian Networks for the Test Score Prediction: A Case Study on a Math
Graduation Exam . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
255
Martin Plajner and Jiˇrí Vomlel
Fine-Tuning the Odds in Bayesian Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
268
Bahare Salmani and Joost-Pieter Katoen
Cautious Classiﬁcation with Data Missing Not at Random Using
Generative Random Forests . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
284
Julissa Villanueva Llerena, Denis Deratani Mauá,
and Alessandro Antonucci
Belief Functions
Scoring Rules for Belief Functions and Imprecise Probabilities:
A Comparison . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
301
Esther Anna Corsi, Tommaso Flaminio, and Hykel Hosni

Contents
xiii
Comparison of Shades and Hiddenness of Conﬂict . . . . . . . . . . . . . . . . . . . . . . . . .
314
Milan Daniel and Václav Kratochvíl
Games of Incomplete Information: A Framework Based on Belief Functions . . .
328
Hélène Fargier, Érik Martin-Dorel, and Pierre Pomeret-Coquot
Uncertainty-Aware Resampling Method for Imbalanced Classiﬁcation
Using Evidence Theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
342
Fares Grina, Zied Elouedi, and Eric Lefèvre
Approximations of Belief Functions Using Compositional Models . . . . . . . . . . . .
354
Radim Jiroušek and Václav Kratochvíl
Dempster-Shafer Approximations and Probabilistic Bounds in Statistical
Matching . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
367
Davide Petturiti and Barbara Vantaggi
The Vehicle Routing Problem with Time Windows and Evidential Service
and Travel Times: A Recourse Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
381
Tekwa Tedjini, Sohaib Aﬁﬁ, Frédéric Pichon, and Eric Lefèvre
Imprecise Probability
A New Score for Adaptive Tests in Bayesian and Credal Networks . . . . . . . . . . .
399
Alessandro Antonucci, Francesca Mangili, Claudio Bonesana,
and Giorgia Adorni
Multi-label Chaining with Imprecise Probabilities . . . . . . . . . . . . . . . . . . . . . . . . . .
413
Yonatan Carlos Carranza Alarcón and Sébastien Destercke
Centroids of Credal Sets: A Comparative Study . . . . . . . . . . . . . . . . . . . . . . . . . . . .
427
Enrique Miranda and Ignacio Montes
The Smallest Probability Interval a Sequence Is Random for: A Study
for Six Types of Randomness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
442
Floris Persiau, Jasper De Bock, and Gert de Cooman
Inconsistency Handling and Preferences
Merging Epistemic States and Manipulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
457
Amílcar Mata Díaz and Ramón Pino Pérez
Explanation with the Winter Value: Efﬁcient Computation for Hierarchical
Choquet Integrals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
471
Christophe Labreuche

xiv
Contents
Inducing Inference Relations from Inconsistency Measures . . . . . . . . . . . . . . . . . .
486
Xiaolong Liu, Philippe Besnard, and Sylvie Doutre
The Degree of Conﬂict Between Formulas in an Inconsistent Knowledge
Base . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
499
Kedian Mu
Possibility Theory and Fuzzy Approaches
Representation of Explanations of Possibilistic Inference Decisions . . . . . . . . . . .
513
Ismaïl Baaj, Jean-Philippe Poli, Wassila Ouerdane, and Nicolas Maudet
Towards a Tesseract of Sugeno Integrals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
528
Didier Dubois, Henri Prade, and Agnès Rico
Canonical Extension of Possibility Measures to Boolean Algebras
of Conditionals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
543
Tommaso Flaminio, Lluis Godo, and Sara Ugolini
On the KLM Properties of a Fuzzy DL with Typicality . . . . . . . . . . . . . . . . . . . . . .
557
Laura Giordano
Probability Logics
Trust Evidence Logic . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
575
Alessandro Aldini, Gianluca Curzi, Pierluigi Graziani,
and Mirko Tagliaferri
Generalized Rules of Probabilistic Independence . . . . . . . . . . . . . . . . . . . . . . . . . . .
590
Janneke H. Bolt and Linda C. van der Gaag
Algebras of Sets and Coherent Sets of Gambles . . . . . . . . . . . . . . . . . . . . . . . . . . . .
603
Arianna Casanova, Juerg Kohlas, and Marco Zaffalon
A Probabilistic Deontic Logic . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
616
Vincent de Wit, Dragan Doder, and John Jules Meyer
Iterated Conditionals and Characterization of P-Entailment . . . . . . . . . . . . . . . . . .
629
Angelo Gilio and Giuseppe Sanﬁlippo
A Triple Uniqueness of the Maximum Entropy Approach . . . . . . . . . . . . . . . . . . .
644
Jürgen Landes
A Logic and Computation for Popper’s Conditional Probabilities . . . . . . . . . . . . .
657
Shota Motoura

Contents
xv
Interpreting Connexive Principles in Coherence-Based Probability Logic . . . . . .
672
Niki Pfeifer and Giuseppe Sanﬁlippo
Correction to: Persuasive Contrastive Explanations for Bayesian Networks . . . .
C1
Tara Koopman and Silja Renooij
Author Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
689

Argumentation and Analogical
Reasoning

Analogies Between Sentences: Theoretical
Aspects - Preliminary Experiments
Stergos Afantenos1, Tarek Kunze1, Suryani Lim2, Henri Prade1(B),
and Gilles Richard1
1 IRIT, Toulouse, France
tarek.kunze@protonmail.com,
{stergos.afantenos,henri.prade,gilles.richard}@irit.fr
2 Federation University, Churchill, Australia
suryani.lim@federation.edu.au
Abstract. Analogical proportions hold between 4 items a, b, c, d inso-
far as we can consider that “a is to b as c is to d”. Such proportions
are supposed to obey postulates, from which one can derive Boolean or
numerical models that relate vector-based representations of items mak-
ing a proportion. One basic postulate is the preservation of the propor-
tion by permuting the central elements b and c. However this postulate
becomes debatable in many cases when items are words or sentences.
This paper proposes a weaker set of postulates based on internal rever-
sal, from which new Boolean and numerical models are derived. The
new system of postulates is used to extend a ﬁnite set of examples in
a machine learning perspective. By embedding a whole sentence into a
real-valued vector space, we tested the potential of these weaker pos-
tulates for classifying analogical sentences into valid and non-valid pro-
portions. It is advocated that identifying analogical proportions between
sentences may be of interest especially for checking discourse coherence,
question-answering, argumentation and computational creativity. The
proposed theoretical setting backed with promising preliminary exper-
imental results also suggests the possibility of crossing a real-valued
embedding with an ontology-based representation of words. This hybrid
approach might provide some insights to automatically extract analogical
proportions in natural language corpora.
1
Introduction
Analogies play an important role in human reasoning, and are thus involved
in natural language discourses. The study of analogical reasoning has a long
history (see, e.g., Chap. 1 in [32]). It departs from case-based reasoning [29].
Beyond diﬀerent classical works on analogical reasoning such as [13–16,35]),
there has been a noticeable renewal of interest in analogical studies with a vari-
ety of approaches, ranging from reasoning [2], machine learning [5,23] to word
analogies [6,11,20,27,36,37] and natural language processing [12,19,34]. These
approaches have in common to deal with analogical proportions, i.e., statements
of the form “a is to b as c is to d” relating 4 items a, b, c and d [30].
c
⃝Springer Nature Switzerland AG 2021
J. Vejnarov´a and N. Wilson (Eds.): ECSQARU 2021, LNAI 12897, pp. 3–18, 2021.
https://doi.org/10.1007/978-3-030-86772-0_1

4
S. Afantenos et al.
Recently, some authors [10,39] have started to study analogical proportions
between sentences, motivated by question-answering concerns or the evaluation
of sentence embeddings. This paper pursues this study, but ﬁrst questions the
modeling of analogical proportions that is used. Indeed, it is generally assumed
that, as for the numerical proportions, the permutation of the central elements
b and c preserves the validity of analogical proportions. This postulate is quite
debatable for natural language items. For this reason, the paper proposes a
postulate weaker than the stability under central permutation. Its main contri-
butions are:
– i) A formal setting to deal with the notion of analogical sentences, where only
an internal reversal property is assumed for the pairs (a, b) and (c, d).
– ii) A rigorous way to extend a ﬁnite set of analogical sentences with both
valid and non-valid examples. In a machine learning perspective, this process
is also known as data augmentation.
– iii) A set of preliminary experiments showing that this notion of analogical
sentences is valid and could bring interesting perspectives in terms of appli-
cations.
The paper is structured as follows. Section 2 investigates the related works.
Section 3 recalls the postulates characterizing analogical proportions and pro-
poses a weaker set of postulates avoiding central permutation. This allows the
identiﬁcation of a rigorous method for enlarging a set of examples and counter-
examples. Section 4 presents and discusses the experimental settings as well as
the way we generate datasets of analogical sentences. Section 5 reports the results
for experiments based on two datasets, showing that machine learning-based
techniques are quite accurate to classify diverse analogical sentences. Section 6
points out some candidate applications, before concluding.
2
Related Work
Although lexical analogies have been more thoroughly studied recently due to
the advent of distributed representations ([6,11,20,27,36,37, for example], to
cite but a few papers), few works have focused on sentence analogies.
In [39], the authors try to show how existing embedding approaches are able
to capture analogies between sentences. They view analogies between pairs of
sentences in very broad terms, which is reﬂected in the various corpora that
they have constructed. For example, in order to create quadruples of analo-
gous sentences they replace individual words with the corresponding words from
the Google word analogy dataset [25]. Sentences that share more or less com-
mon semantic relations (entailment, negation, passivization, for example) or even
syntactic patterns (comparisons, opposites, plurals among others) are also con-
sidered analogous. Using these datasets, analogies are evaluated using various
embeddings, such as GloVe [28], word2vec [25], fastText [4,26], etc. showing
that capturing syntactic analogies which were based on lexical analogies from

Analogies Between Sentences: Theoretical Aspects
5
the Google word analogies dataset is more eﬀective with their models than recog-
nising analogies based on more semantic information.
In [10] the authors focus on the task of identifying the correct answer to
a question from a pool of candidate answers. More precisely, given a question
q the goal is to select the answer ai ∈A from a set A of candidate answers.
In order to do so they leverage analogies between (q, ai) and various pairs of
what they call “prototypical” question/answer pairs, assuming that there is an
analogy between (q, ai) and the prototypical pair (qp, ap). The goal is to select
the candidate answer a∗
i ∈A such that:
a∗
i = arg mini(||(qp −ap) −(q −ai)||)
exploiting the properties of arithmetic proportion and analogical dissimilarities.
The authors limit the question/answer pairs to wh−questions from WikiQA and
TrecQA. They use a Siamese bi-GRUs as their architecture in order to represent
the four sentences. In this manner the authors learn embedding representations
for the sentences which they compare against various baselines including random
vectors, word2vec, InferSent and Sent2Vec obtaining better results with the
WikiQA corpus.
Most of the tested sentence embedding models succeed in recognizing syn-
tactic analogies based on lexical ones, but had a harder time capturing analogies
between pairs of sentences based on semantics.
3
Formal Framework
Analogies are often expressed in terms of analogical proportions. An analogical
proportion over a set X of items (Boolean vectors, real-valued vectors, words or
even sentences in natural language) is a quaternary relation involving 4 elements,
a, b, c, d ∈X, often denoted a : b :: c : d and should be read “a is to b as c is
to d”, obeying postulates. Depending on the postulates, strong and weak forms
of analogical proportions can be distinguished, leading to diﬀerent Boolean and
numerical representations.
3.1
Strong Analogical Proportions
Classically, analogical proportions are supposed to obey the 3 following ﬁrst-
order logic postulates (e.g., [18]) ∀a, b, c, d ∈X,1which are satisﬁed by numerical
proportions:
1. a : b :: a : b (reﬂexivity);
2. a : b :: c : d →c : d :: a : b (symmetry);
3. a : b :: c : d →a : c :: b : d (central permutation).
1 In the following, we omit the universal quantiﬁer for readability.

6
S. Afantenos et al.
These postulates have straightforward consequences like:
– a : a :: b : b (identity);
– a : b :: c : d →b : a :: d : c (internal reversal);
– a : b :: c : d →d : b :: c : a (extreme permutation);
– a : b :: c : d →d : c :: b : a (complete reversal).
Among the 24 permutations of a, b, c, d, the previous postulates lead to 3 distinct
classes each containing 8 syntactically diﬀerent proportions regarded as equiv-
alent due to the postulates. Thus a : b :: c : d has in its class c : d :: a : b,
c : a :: d : b, d : b :: c : a, d : c :: b : a, b : a :: d : c, b : d :: a : c, and
a : c :: b : d. But b : a :: c : d and a : d :: c : b are not in the class of a:b :: c:d
and are in fact elements of two other classes.
A typical example of an analogical proportion over X = R is the arithmetic
proportion deﬁned as:
a : b :: c : d holds if and only if a −b = c −d
easily extended to a real-valued vector space X = Rn with the same deﬁnition.
From a geometric viewpoint in Rn, it means that (a, b, c, d) is a parallelogram
[35]. In practice, it may be weakened into an approximate equality a −b ≈c −d
using some tolerance.
Considering now X as the Boolean set B = {0, 1}, various equivalent Boolean
formulas satisfy the postulates of analogical proportion over X. One of them
making explicit that “a diﬀers from b as c diﬀers from d (and vice-versa)” [24] is:
a : b :: c : d =def ((a ∧¬b) ≡(c ∧¬d)) ∧((¬a ∧b) ≡(¬c ∧d))
It is easy to check that this formula is only valid for the 6 valuations in Table 1. As
shown in [31], this set of 6 valuations is the minimal Boolean model obeying the 3
postulates of analogy. It can be seen on this table that 1 and 0 play a symmetrical
role, which makes the deﬁnition code-independent. This is formally expressed in
X = B with the negation operator as: a : b :: c : d →¬a : ¬b :: ¬c : ¬d.
Table 1. Valid valuations for the strong Boolean analogical proportion
0 : 0 :: 0 : 0
1 : 1 :: 1 : 1
0 : 1 :: 0 : 1
1 : 0 :: 1 : 0
0 : 0 :: 1 : 1
1 : 1 :: 0 : 0
To deal with items represented by Boolean vectors, it is straightforward to
extend this deﬁnition from B to Bn with:
a : b :: c : d =def ∀i ∈[1, n], ai : bi :: ci : di

Analogies Between Sentences: Theoretical Aspects
7
This is useful when words are represented by means of key terms appearing in
their dictionary deﬁnition. For instance, using the 5 key terms mammal, bovine,
equine, adult, young, Table 2 explains why a : b :: c : d = cow : calf :: mare :
foal can rigorously be considered as a valid analogy (we recognize patterns
of Table 1, vertically, in Table 2). More generally, series of mutually exclusive
properties such as bovine/equine, adult/young, as encountered in taxonomic
trees, induce analogical proportions [3].
Table 2. A Boolean validation of cow : calf :: mare : foal
Mammal Bovine Equine Adult Young
Cow
1
1
0
1
0
Calf
1
1
0
0
1
Mare 1
0
1
1
0
Foal
1
0
1
0
1
3.2
Weak Analogical Proportions
However, the central permutation postulate (3) is debatable for analogical pro-
portions involving two conceptual spaces [2]. Indeed in the following example that
involves nationalities and beverages, while “wine is to French people as beer is
to English people” is acceptable, “wine is to beer as French people is to English
people” sounds weird.
It is then legitimate to abandon the central permutation postulate and to
replace it by the internal reversal property which is still a widely accepted
property of analogical proportion, leading to a weaker deﬁnition of analogical
proportion:
1 a : b :: a : b (reﬂexivity);
2 a : b :: c : d →c : d :: a : b (symmetry);
4 a : b :: c : d →b : a :: d : c (internal reversal).
Complete reversal (a : b :: c : d →d : c :: b : a) is still a consequence
of this weaker set of postulates. Clearly a strong analogical proportion (in the
sense of (1)-(2)-(3)) is also a weak proportion. According to postulates (1)-(2)-
(4) a : b :: c : d can be written only under 4 equivalent forms rather than 8:
a : b :: c : d, c : d :: a : b, d : c :: b : a, and b : a :: d : c. Despite one might
be tempted to have a : a :: b : b (identity), this is no longer deducible from the
postulates.
From a computational linguistics viewpoint (see for instance [6,11,21]), a
proportion a : b :: c : d is often understood as:
for some binary relation R, R(a, b) ∧R(c, d) holds.

8
S. Afantenos et al.
where R is a latent relation with a semantic or discourse connotation. This
deﬁnition perfectly ﬁts with postulates (1)-(2)-(4). As an example, consider the
following pairs of sentences:
John sneezed loudly (a). Mary was startled (b).
Bob took an analgesic (c). His headache stopped (d).
where one could argue that R is a kind of causal relation. Indeed, internal reversal
holds because R−1(b, a) ∧R−1(d, c) holds. In the example above R−1 would be
an explanation relation although explicit discourse markers should be utilized:
Mary was startled (b), because John sneezed loudly (a).
Bob’s headache stopped (d) because he took an analgesic (c).
In practice, the fact that R(a, b) ∧R(c, d) holds does not entail that there
also exists a semantically meaningful S such that S(a, c) ∧S(b, d) holds: that is
why the central permutation postulate is not relevant here. It would make no
sense in the above example.
The minimal Boolean model of postulates (1)-(2)-(4) is constituted with the
4 ﬁrst lines of Table 1, still satisfying code-independence. A Boolean expression
of the weak analogical proportion a : b :: wc : d is given by
a : b :: wc : d = (a ≡c) ∧(b ≡d)
Note that a ∧b ≡c ∧d and a ∨b ≡c ∨d are simple examples of formulas
satisfying (1)-(2)-(4). But their Boolean models include more than the 4 ﬁrst
lines of Table 1 even if they are false for the last two lines of this Table.
When working on X = R rather than X = B, a weak analogical proportion
can be deﬁned by:
a : b :: wc : d holds

if a = b = c = d
if a −b = c −d
when Cond is true
where Cond stands for (a < b ∧c < d) ∨(a > b ∧c > d) with b ̸= c. It
removes the situation where a −b = c −d = 0 for b ̸= c. It satisﬁes (1)-(2)-(4),
but not (3) (e.g., we have .3 < .9 and .7 < .8 but not .3 < .7 and .9 < .8), nor
a : a :: b : b or a : b :: b : a. It has also the advantage to keep a, b, c, d distinct,
which would not be the case if a : b :: wc : d is deﬁned by the straightforward
option ‘a = c and b = d’. Obviously in practice, = could be replaced by ≈.
3.3
Analogical Proportions and Implication
A relation of particular interest between items such as sentences (or words) is the
entailment relation: “a entails b as c entails d”. So we may wonder if a simplistic
modeling of “a entails b” in terms of material implication (a →b) would lead to
a weak proportion.
In fact, (a →b) ∧(c →d) is not satisfactory as, in the Boolean uni-
verse, it is false for valuation (1010) and true for the other patterns that a
strong or weak analogical proportion fulﬁll (together with 4 other patterns:

Analogies Between Sentences: Theoretical Aspects
9
(0001), (0100), (0111), (1101)). Obviously this formula does satisfy neither cen-
tral permutation nor internal reversal.
In that respect, a better option could be to consider
Imp(a, b, c, d) = [(a →b) ∧(c →d)] ∨[(b →a) ∧(d →c)]
which satisﬁes the postulates of a weak analogical proportion. However, in the
Boolean universe, this formula is false only for 2 patterns (0110), (1001) (which
are known as maximizing analogical dissimilarity [23,24]), and true for all the 14
remaining patterns. However, one can recover a : b :: c : d as well as a : b :: wc : d
from it. Namely,
a : b :: c : d = Imp(a, b, c, d) ∧Klein(a, b, c, d)
where Klein(a, b, c, d) = (a ≡b) ≡(c ≡d) is an operator introduced by S. Klein
[8], true for the 6 patterns that make true an analogical proportion, plus the
two forbidden patterns (0110), (1001).. In the Boolean universe, weak analogical
proportions are recovered as
a : b :: wc : d = Imp(a, b, c, d) ∧Par(a, b, c, d)
where Par(a, b, c, d) = ((a ∧b) ≡(c ∧d)) ∧((¬a ∧¬b) ≡(¬c ∧¬d) is a Boolean
operator named paralogy [31].
The above equalities show that one cannot have a pure implication-based
view of analogical proportions. Moreover Imp(a, b, c, d) is much weaker than
a : b :: wc : d and cannot be a particular case of it.
3.4
Analogical Proportions and Sentences
We distinguish two types of analogies between sentences. The ﬁrst one is a
natural extension of word analogies. Starting from the fact that a : b :: c : d is
an analogy between lexical items a, b, c and d, then sentences s1, s2 are analogous
if they link a, b and c, d with the same predicate R—so R(a, b) ∧R(c, d). A
simple example would be: French people drink wine ( s1) English people drink
beer (s2) with French:wine :: English:beer. It is worth noticing that already a
pair of sentences making a parallel between two situations and involving words
or phrases in analogical proportions, may provide an argumentative support.
For example, let us consider the two sentences Polio vaccine protects against
poliomyelitis. h1n1 vaccine protects against inﬂuenza. Clearly the pairs (a, b) =
(Polio, poliomyelitis) and (c, d) = (h1n1 vaccine, inﬂuenza) make an analogical
proportion a : b :: c : d with respect to vaccine, disease, virus and bacteria,
as can be checked on ﬁrst four lines of Table 3. If we also consider the sentence
“BCG vaccine protects against tuberculosis” associated with the pair (a′, b′) =
(BCG vaccine, tuberculosis), a′ : b′ :: c : d does not hold with respect to virus
and bacteria (see Table 3), and then (a′, b′) appears to be a poorer support for
(c, d) than (a, b). It would be still worse if one considers the sentence “umbrellas
protect against the rain” instead of (a′, b′)! Besides, note that in all the above

10
S. Afantenos et al.
Table 3. The vaccine example
Vaccine Disease Virus Bacteria
a
1
0
0
0
b
0
1
1
0
c
1
0
0
0
d
0
1
1
0
a’ 1
0
0
0
b’ 0
1
0
1
sentences the same relation R = protects against takes place, and both R(a, b)
and R(c, d) hold. In case of synonyms used in diﬀerent sentences, analogy may
become a matter of degree (e.g.,[5]). When sentences have a diﬀerent number
of words, one may apply the approach by Miclet et al. [22] for aligning the key
terms. Lastly, a parallel between 2 situations may involve 4 sentences making
an analogical proportion, as in the example “some cetaceans eat phytoplankton;
other cetaceans are carnivorous; some mammals eat grass; other mammals are
carnivorous”.
The second type of sentential analogies is not limited to a, b, c and d being
extension of word analogies but instead they represent more complex sentences
with R being a latent relation linking a, b and c, d. This type of sentential analo-
gies was brieﬂy evoked in Subsect. 3.2 wherein some examples involving causal
relations were provided. Here is another example involving a relation that is
more temporal in nature:
Mary was working on her new book (a), while John was washing this
afternoon’s dishes (b).
Bob was waiting for the bus at the bus stop (c). At the same time, workers
were making loud noises on the street (d).
As we can see, in this case reﬂexivity, symmetry, internal reversal and complete
reversal (Subsect. 3.2) also hold while we cannot say the same thing for central
permutation.
The latent relation linking two pairs of sentences can take several forms.
Natural candidates include entailment or various discourse relations. In Sect. 4
of this paper, we use the Penn Discourse TreeBank2 (PDTB), but other datasets,
such as the Stanford Natural Language Inference (SNLI) Corpus3 could be used
as well.
2 https://www.seas.upenn.edu/∼pdtb/.
3 https://nlp.stanford.edu/projects/snli/.

Analogies Between Sentences: Theoretical Aspects
11
4
Experimental Context - Evaluation Metrics
Let us move to an empirical validation of our approach using word embeddings,
and sentence embeddings4.
4.1
Datasets
Mixing google analogies with template sentences Given a set of words W, the
notion of analogical proportion between words is not formally deﬁned but we
have many examples of well-accepted (cognitively valid) word analogies like
man : king
::
woman : queen. One of the most well-known datasets of
word analogies, originally from Google, can be downloaded at http://download.
tensorﬂow.org/data/questions-words.txt
Starting from Google word analogies dataset, we create our own sentences
analogies dataset by separating the lexical items therein into diﬀerent cate-
gories. Template sentences are then created with placeholders to be replaced
by the correct category of the Google word analogy dataset. With this method
sentences SA, SB, SC and SD are created so that they meet the requirement
R(SA, SB) ∧R(SC, SD). Obviously, we expect our classiﬁers to be successful on
this artiﬁcial dataset: at this stage, this is just a toy example to check that
our initial assumption (weak analogy deﬁnition) is not defeated. Below are the
categories with examples used
– Capital Common Cities. SA = “She arrived yesterday in London” and
SB = “She just landed in England”.
– City in State. SA
=“Citizens in Chicago are more likely to vote”,
SB =“Citizens in Illinois are more likely to vote”.
– Currencies. SA = “What is the currency used in Japan?”, SB = “Which
country uses the yen?”.
– Family. The possessives “his”, “her” and the nouns “he”, “she”, are skipped.
For instance in SA =“His father could not be present at the annual family
gathering”, SB =“His mother could not be present at the annual family
gathering”.
– Nationality adjectives. SA =“The culture in Chile is very rich”, SB =“The
Chilean culture is very rich”.
– Opposites. Sentences containing an adjective listed in the Google analogy
dataset. The adjective is labelled and replaced with it’s antonym thus creating
a pair of sentences. SA =“I was aware in which direction he was going”,
SB =“I was unaware in which direction he was going”.
Once generated, we get a total set of 52, 185 sentence quadruples. Using the
extension methods depicted in Subsect. 3.2, the dataset is extended by a factor
12, with 4 valid analogies per sentence quadruple, using the symmetry, internal
reversal and complete reversal permutations and 8 invalid analogies, using the
same permutations applied to the quadruples b : a :: c : d and c : b :: a : d. We
now have a dataset of size 52, 185 ∗12 = 626, 220 sentences.
4 All our datasets and python code used in this paper are freely available at https://
github.com/arxaqapi/analogy-classiﬁer.

12
S. Afantenos et al.
Penn Discourse TreeBank Dataset. The second dataset that we use is the Penn
Discourse TreeBank (PDTB) [33].5 Our goal was to try a preliminary series
of experiments to determine whether our approach could identify pairs of sen-
tences that are linked with the same latent relation R where R in this particular
case is a discourse relation. PDTB contains more than 36,000 pairs of sentences
annotated with discourse relations. Relations can be explicitly expressed via a
discourse marker, or implicitly expressed in which case no such discourse marker
exists and the annotators provide one that more closely describes the implicit dis-
course relation. Relations are organized in a taxonomy which contains 4 classes
of relations (Temporal, Contingency, Expansion and Comparison) as well as sev-
eral types and subtypes of relations (for example Cause, Condition, Contrast,
etc.). As an example, for the Temporal relation type, we have pairs of sentences
like: SA = “The results were announced”, SB = “The stock market closed” and
SC = “That happens”, SD = “Nervous stock investors dump equities and buy
treasurys”.
In this series of preliminary results we used primarily the four classes of
relations although we also experimented with the implicit/explicit nature of
relations. In total we selected 25,000 random quadruplets for a total of 300,000
instances. Each quadruple of sentences was accompanied with a Boolean class
indicating whether the quadruple constituted a valid analogy—that is pairs (a, b)
and (c, d) are linked with the same relation—or not.
4.2
Embedding Techniques
A word embedding ω is an injective function from a set of words W to a real-
valued vector space Rn (usually n ∈{50, 100, 200, 300} but there is no real
limitation). There are well-known word embeddings such as word2vec [25], GloVe
[28], BERT [9], fastText [26], etc. It is standard to start from a word embedding
to build a sentence embedding. Sentence embedding techniques represent entire
sentences and their semantic information as vectors. There are diverse sentence
embedding techniques. In this paper, we focus on 2 techniques relying on an
initial word embedding.
– The simplest method is to average the word embeddings of all words in a
sentence. Although this method ignores both the order of the words and the
structure of the sentence, it performs well in many tasks. So the ﬁnal vector
has the dimension of the initial word embedding.
– The other approach, suggested in [1], makes use of the Discrete Cosine Trans-
form (DCT) as a simple and eﬃcient way to model both word order and
structure in sentences while maintaining practical eﬃciency. Using the inverse
transformation, the original word sequence can be reconstructed. A param-
eter k is a small constant that needs to be set. One can choose how many
features are being embedded per sentence by adjusting the value of k, but
undeniably increases the ﬁnal size of the sentence vector by a factor k. If the
5 At this stage, we used PDTB version 2.1.

Analogies Between Sentences: Theoretical Aspects
13
initial embedding of words is of dimension n, the ﬁnal sentence dimension
will be = n ∗k (see [1] for a complete description). In this paper, we chose
k = 1 as suggested in [39].
4.3
Classiﬁers
Because we do not rely on any parallelogram-like formula to check whether 4
sentences build an analogy, we move to machine learning to “learn”, in some
sense, the formula. In fact, we classify a quadruple of 4 sentences as a valid or
non valid analogy. We tried two classical methods which have been successfully
used for word analogy classiﬁcation [20]: Random Forest (RF) and Convolutional
Neural Networks (CNN). CNN has been popular for image classiﬁcation but has
also been used for text classiﬁcation as it could extract and select important
ngrams for classiﬁcation [17]. RF is relatively accurate at classiﬁcation and is
fast [7]. On a 10 core CPU, CNN took about 12 CPU hours (one-hour real-time)
to train but RF took only about 18 CPU minutes and real-time to train.
The parameters for RF are 100 trees, no maximum depth, and a minimum
split of 2. With CNN, stacking together the 4 vectors, with n components cor-
responding to a quadruple a, b, c, d of sentences, we get a matrix n × 4 that we
are going to process as we would do for an image. With ﬁlters respecting the
boundaries of the 2 pairs, this is the structure of the CNN:
– 1st layer (convolutional): 128 ﬁlters of size height×width = 1×2 with strides
(1, 2) and Relu activation. This means that we are working component by
component (with a vertical stride of 1) and we move from pair (a, b) to pair
(c, d) with horizontal stride 2.
– 2nd layer (convolutional): 64 ﬁlters of size (2, 2) with strides (2, 2) and Relu
activation, reducing the dimension before going to the ﬁnal dense layer.
– 3rd layer (dense): one output with sigmoid activation.
5
Results
Below we present experimental results for two datasets: generated sentences and
PDTB.
5.1
CNN and RF Results for Generated Sentences
The results in Table 4 are obtained with 10 epochs for the CNN. The Average
Accuracy is computed from the average of the 10 folds accuracy for each method.
The results are already extremely good (over 98%) even with a low word embed-
ding size for both CNN and RF. The accuracy in CNN increases if we increase
the size of the word vectors as more “details” are encoded in the vector so that
the CNN captures more of the semantics. For RF, increasing the size of the word
vectors provides no increase in the accuracy. Given that the increase in accuracy
is at most 2%, we suggest using vectors of size 50 (GloVe) or reduce the fast-
Text vectors from 300 to 50 to reduce the computation in CNN. The authors
in [39] also used GloVe average method for sentence embedding and the highest
accuracy is 90%, regardless of which parallelogram-inspired formula were used.

14
S. Afantenos et al.
Table 4. Average accuracies (10 folds) for CNN (10 epochs) and RF for generated
dataset using GloVe vector size 50 to 300 (G50 to G300) and fastText (F300) for
average and DCT sentence embedding.
ML
Word embedding size
Method
G50
G100
G200
G300
F300
CNN-AVG 98.39% 99.76% 99.96% 99.97% 99.91%
RF-AVG
99.97% 99.97% 99.97% 99.97% 99.98%
CNN-DCT 68.23% 68.30% 68.31% 68.31% 68.24%
RF-DCT
67.14% 67.14% 67.14% 67.14
67.10%
Table 5. Average accuracies and F1 (10 folds) for CNN 10 epochs and RF for PDTB
dataset using GloVe and fastText for average and DCT sentence embedding.
ML
Word embedding size
Method
G50
G100
G200
G300
F300
CNN-AVG
66.49% (0.42)
66.64% (0.42)
66.61% (0.40)
66.36% (0.41)
66.69% (0.52)
RF-AVG
62.96% (0.02)
63.79% (0.02)
64.08% (0.02)
64.33% (0.02)
65.05% (0.01)
CNN-DCT
66.46% (0.12)
66.49% (0.10)
65.31% (0.34)
66.10% (0.15)
66.05% (0.21)
RF-DCT
58.74% (0.04)
58.91% (0.04)
58.92% (0.04)
58.85% (0.04)
61.89% (0.09)
5.2
CNN and RF Results for PDTB Dataset
The accuracies for both CNN and RF are around 60%, much lower for the
PDTB dataset (see Table 5) compared to the generated sentences. This is to be
expected as the PDTB sentences are much more semantic/pragmatic in nature so
it is much more diﬃcult to capture the relationship between the sentences using
a simple average embedding technique. The F1 values of RF are very low (at
most 0.09), suggesting that it has diﬃculty identifying positive results. CNNs on
the other hand perform much better achieving 0.52 F1 for valid analogies using
fastText, but the overall highest accuracy is only 66.69%.
Our results are an improvement with respect to [10,39], although a strict
comparison is diﬃcult because not only do we use diﬀerent corpora, but also the
experimental setup is diﬀerent. For more semantic sentences, the work reported
in [39] achieves an accuracy of 0.43 in the unconstrained scenario, while we have
achieved an accuracy of 0.66. In [39], a constrained scenario selected the true
answer from six sentences, while the unconstrained scenario selected the true
answer from the “entire corpus”.
6
Candidate Applications
Analogy-making permeates human cognition and is a mechanism that is very
often used in human communication. In terms of computational linguistics the
study of analogies has almost exclusively been limited to the detection of word
analogies [6,11,20,27,36,37, for example]. Nonetheless analogies go beyond the

Analogies Between Sentences: Theoretical Aspects
15
lexical level and can exist between sentences as well as longer discourse units.
In the following we describe some areas of computational linguistics that could
beneﬁt from analogy-making.
– Discourse: one of the problems that current chatbots face is that they lack
discourse coherence. Disposing of a mechanism that is able to handle sentence
analogies, allows us to better select following sentences in a generated text
yielding better overall coherence. Consider for example that in a discourse
d = (s1, . . . , sn) where si represents elementary discourse units, we need to
choose the next sentence from a set of candidate sentences C that a chatbot
could have generated so as to maximize coherence. In order to do so we can
rely upon the formal framework presented in Sect. 3 in order to choose sc ∈C
such that sc = arg min(||(πi −πj) −(sn −sc)||) where πi : πj :: sn : sc. In
order to bootstrap learning we can use resources such as PDTB. Given that
πi, πj, sn and sc are learnt representations we can imagine a more complicated
scenario in which πi and sn are substituted for representations of previous
discourse. Corpora such as the RST which model larger contexts could be
used.
– Question-answering: A similar approach has already been used in the con-
text of question answering by [39] (see Sect. 2). Better selection of answers
is achieved by hypothesizing that an answer is more plausible for that ques-
tion if the pair (q, a) is analogical to other “prototypical” pairs of questions
and answers. Extensions of this work could include representations of larger
context as well as less prototypical pairs.
– Computational creativity: Being able to identify and propose new analo-
gies can be very useful in understanding and advancing computational creativ-
ity. In the context of generating more “creative” text, such as poem generation
[38], relaxing to a certain degree the constraint of minimization mentioned
above could yield creative analogies.
– Argumentation: Capturing analogies that go beyond the sentential level
could be very useful for argumentation. A conversational agent trying to
convince their interlocutor that, for example, using a car is not anodyne since
it pollutes the environment and ultimately is a cause of death for many people,
could draw the analogy between driving a car and smoking. Smoking was
widely accepted but studies have shown that it is detrimental to the health
of active and passive smokers. The same has been shown for car pollution, so
cars should be restricted or banned altogether under certain circumstances.
Achieving such a goal requires representations that are capable to handle
larger chunks of text.
7
Conclusion
In this paper, we have provided the basis of a weak analogical proportion theory,
removing the classical central permutation postulate. Our new postulates better
reﬂect what is generally considered as a natural language analogical proportion
between sentences. From a machine learning perspective, the new system is also

16
S. Afantenos et al.
used to rigorously extend a set of examples. Using standard embedding tech-
niques for sentences, we have tested our approach to classify 4 sentences into
valid and invalid proportions. Preliminary experiments using simple architec-
tures show that we can achieve an accuracy of 0.66 (with an F1 of 0.52 for
valid analogies) for sentential analogies based on latent semantic and pragmatic
similarity using the PDTB corpus. In the future we plan to perform further
experiments using transformers and BERT embeddings in order to investigate
which of the aforementioned postulates (central permutation, internal reversal)
transfer to natural language datasets. By crossing powerful real-valued embed-
dings with a more semantic ontology-based representation of words, the new
formal setting paves the way to hybrid approaches where analogical inference
could be done on natural language corpora.
References
1. Almarwani, N., Aldarmaki, H., Diab, M.: Eﬃcient sentence embedding using dis-
crete cosine transform. In: EMNLP, pp. 3663–3669 (2019)
2. Barbot, N., Miclet, L., Prade, H.: Analogy between concepts. Artif. Intell. 275,
487–539 (2019)
3. Barbot, N., Miclet, L., Prade, H., Richard, G.: A new perspective on analogical
proportions. In: Kern-Isberner, G., Ognjanovi´c, Z. (eds.) ECSQARU 2019. LNCS
(LNAI), vol. 11726, pp. 163–174. Springer, Cham (2019). https://doi.org/10.1007/
978-3-030-29765-7 14
4. Bojanowski, P., Grave, E., Joulin, A., Mikolov, T.: Enriching word vectors with
subword information. Trans. Assoc. Comput. Linguist. 5, 135–146 (2017)
5. Bounhas, M., Prade, H., Richard, G.: Analogy-based classiﬁers for nominal or
numerical data. Int. J. Approx. Reasoning 91, 36–55 (2017)
6. Bouraoui, Z., Jameel, S., Schockaert, S.: Relation induction in word embeddings
revisited. In: COLING, pp. 1627–1637. Association for Computational Linguistics
(2018)
7. Breiman, L.: Random forests. Mach. Learn. 45(1), 5–32 (2001)
8. Couceiro, M., Hug, N., Prade, H., Richard, G.: Analogy-preserving functions: a
way to extend Boolean samples. In: Proceedings of the IJCAI, Melbourne, pp.
1575–1581 (2017)
9. Devlin, J., Chang, M.-W., Lee, K., Toutanova, K.: BERT: pre-training of deep bidi-
rectional transformers for language understanding. CoRR, abs/1810.04805 (2018)
10. Diallo, A., Zopf, M., F¨urnkranz, J.: Learning analogy-preserving sentence embed-
dings for answer selection. In: Proceedings of the 23rd Conference on Computa-
tional Natural Language Learning, pp. 910–919. Association for Computational
Linguistics (2019)
11. Drozd, A., Gladkova, A., Matsuoka, S.: Word embeddings, analogies, and machine
learning: beyond king - man + woman = queen. In: COLING, pp. 3519–3530 (2016)
12. Fam, R., Lepage, Y.: Tools for the production of analogical grids and a resource of
n-gram analogical grids in 11 languages. In: LREC (2018)
13. French, R.M., Hofstadter, D.: Tabletop: an emergent, stochastic model of analogy-
making. In: Proceedings of the 13th Annual Conference of the Cognitive Science
Society, pp. 175–182. Lawrence Erlbaum (1991)
14. Gentner, D., Holyoak, K.J., Kokinov, B.N. (eds.): The Analogical Mind: Perspec-
tives from Cognitive Science. MIT Press, Cambridge (2001)

Analogies Between Sentences: Theoretical Aspects
17
15. Hesse, M.: Models and Analogies in Science, 1st ed. Sheed & Ward, London (1963).
2nd augmented ed. University of Notre Dame Press, 1966
16. Hofstadter, D., Mitchell, M.: The copycat project: a model of mental ﬂuidity and
analogy-making. In: Fluid Concepts and Creative Analogies: Computer Models of
the Fundamental Mechanisms of Thought, pp. 205–267. Basic Books Inc (1995)
17. Jacovi, A., Shalom, O.S., Goldberg, Y.: Understanding convolutional neural net-
works for text classiﬁcation. CoRR, abs/1809.08037 (2018)
18. Lepage, Y.: Analogy and formal languages. Electr. Notes Theor. Comput. Sci. 53,
180–191 (2004). https://doi.org/10.1016/S1571-0661(05)82582-4
19. Lepage, Y., Denoual, E.: Purest ever example-based machine translation: detailed
presentation and assessment. Mach. Transl. 19(3–4), 251–282 (2005)
20. Lim, S., Prade, H., Richard, G.: Solving word analogies: a machine learning per-
spective. In: Kern-Isberner, G., Ognjanovi´c, Z. (eds.) ECSQARU 2019. LNCS
(LNAI), vol. 11726, pp. 238–250. Springer, Cham (2019). https://doi.org/10.1007/
978-3-030-29765-7 20
21. Lu, H., Wu, Y., Holyoak, K.H.: Emergence of analogy from relation learning. Proc.
Natl. Acad. Sci. 116, 4176–4181 (2019)
22. Miclet, L., Barbot, N., Jeudy, B.: Analogical proportions in a lattice of sets of
alignments built on the common subwords in a ﬁnite language. In: Prade, H.,
Richard, G. (eds.) Computational Approaches to Analogical Reasoning: Current
Trends. SCI, vol. 548, pp. 245–260. Springer, Heidelberg (2014). https://doi.org/
10.1007/978-3-642-54516-0 10
23. Miclet, L., Bayoudh, S., Delhay, A.: Analogical dissimilarity: deﬁnition, algorithms
and two experiments in machine learning. JAIR 32, 793–824 (2008)
24. Miclet, L., Prade, H.: Handling analogical proportions in classical logic and fuzzy
logics settings. In: Sossai, C., Chemello, G. (eds.) ECSQARU 2009. LNCS (LNAI),
vol. 5590, pp. 638–650. Springer, Heidelberg (2009). https://doi.org/10.1007/978-
3-642-02906-6 55
25. Mikolov, T., Chen, K., Corrado, G.S., Dean, J.: Eﬃcient estimation of word rep-
resentations in vector space. CoRR, abs/1301.3781 (2013)
26. Mikolov, T., Grave, E., Bojanowski, P., Puhrsch, C., Joulin, A.: Advances in pre-
training distributed word representations. In: Proceedings of LREC (2018)
27. Murena, P.-A., Al-Ghossein, M., Dessalles, J.-L., Cornu´ejols, A.: Solving analogies
on words based on minimal complexity transformation. In: Proceedings of the 29th
International Joint Conference Artiﬁcial Intelligence, pp. 1848–1854 (2020)
28. Pennington, J., Socher, R., Manning, Ch.D.: GloVe: global vectors for word repre-
sentation. In: EMNLP, pp. 1532–1543 (2014)
29. Prade, H., Richard, G.: Analogical proportions and analogical reasoning - an intro-
duction. In: Aha, D.W., Lieber, J. (eds.) ICCBR 2017. LNCS (LNAI), vol. 10339,
pp. 16–32. Springer, Cham (2017). https://doi.org/10.1007/978-3-319-61030-6 2
30. Prade, H., Richard, G.: Analogical proportions: why they are useful in AI. In:
Proceedings of the 30th International Joint Conference on Artiﬁcial Intelligence
(IJCAI 2021), Montreal, 21–26 August 2021
31. Prade, H., Richard, G.: Analogical proportions: from equality to inequality. Int. J.
Approx. Reasoning 101, 234–254 (2018)
32. Prade, H., Richard, G. (eds.): Computational Approaches to Analogical Reasoning:
Current Trends. SCI, vol. 548. Springer, Heidelberg (2014). https://doi.org/10.
1007/978-3-642-54516-0
33. Prasad, R., et al.: The Penn discourse TreeBank 2.0. In: LREC 2008, May 2008

18
S. Afantenos et al.
34. Rhouma, R., Langlais, P.: Experiments in learning to solve formal analogical equa-
tions. In: Cox, M.T., Funk, P., Begum, S. (eds.) ICCBR 2018. LNCS (LNAI), vol.
11156, pp. 612–626. Springer, Cham (2018). https://doi.org/10.1007/978-3-030-
01081-2 40
35. Rumelhart, D.E., Abrahamson, A.A.: A model for analogical reasoning. Cogn.
Psychol. 5, 1–28 (2005)
36. Turney, P.D.: A uniform approach to analogies, synonyms, antonyms, and associ-
ations. In: COLING, pp. 905–912 (2008)
37. Turney, P.D.: Distributional semantics beyond words: supervised learning of anal-
ogy and paraphrase. TACL 1, 353–366 (2013)
38. Van de Cruys, T.: Automatic poetry generation from prosaic text. In: Proceedings
of ACL (2020)
39. Zhu, X., de Melo, G.: Sentence analogies: linguistic regularities in sentence embed-
dings. In: COLING (2020)

Non-monotonic Explanation Functions
Leila Amgoud(B)
CNRS – IRIT, Toulouse, France
amgoud@irit.fr
Abstract. Explaining black-box classiﬁcation models is a hot topic in
AI, it has the overall goal of improving trust in decisions made by such
models. Several works have been done and diverse explanation functions
have been proposed. The most prominent ones, like Anchor and LIME,
return abductive explanations which highlight key factors that cause
predictions. Despite their popularity, the two functions may return inac-
curate and sometimes incorrect explanations.
In this paper, we study abductive explanations and identify the ori-
gin of this shortcoming. We start by deﬁning two kinds of explanations:
absolute explanations that are generated from the whole feature space,
and plausible explanations (like those provided by Anchors and LIME)
that are constructed from a proper subset of the feature space. We show
that the former are coherent in that two compatible sets of features can-
not explain distinct classes while the latter may however be incoherent,
leading thus to incorrect explanations. Then, we show that explanations
are provided by non-monotonic functions. Indeed, an explanation may
no longer be valid if new instances are received. Finally, we provide a
novel function that is based on argumentation and that returns plausi-
ble explanations. We show that the function is non monotonic and its
explanations are coherent.
Keywords: Classiﬁcation · Explainability · Argumentation
1
Introduction
In the last few years, the discussion around artiﬁcial intelligence is gaining more
and more interest. This is mainly due to noteworthy advances made in data-
driven AI in general, and deep learning in particular. In this sub-ﬁeld of AI,
the idea is to learn a targeted objective (like the class of an object) from a
vast quantity of data. However, the predictions of existing models can hardly be
explained in a transparent way. This opacity is seen as a great limitation, which
impedes the relevance of those models in practical applications like healthcare
but also in embedded systems for mobility despite their successes. Explanations
are essential for increasing societal trust, and thus acceptance of those models.
They help users understand why a decision was reached.
Explaining the functionality of complex classiﬁcation models and their ratio-
nale becomes a vital need. Consequently, several works have been done in the
c
⃝Springer Nature Switzerland AG 2021
J. Vejnarov´a and N. Wilson (Eds.): ECSQARU 2021, LNAI 12897, pp. 19–31, 2021.
https://doi.org/10.1007/978-3-030-86772-0_2

20
L. Amgoud
literature (see [1–3] for recent surveys on explaining machine learning models).
They consider as input a classiﬁer, and provide explanations of its predictions.
Those works can be divided into two families: The ﬁrst family opens somehow
the classiﬁer to provide insight into the internal decision-making process (e.g.
[4,5]), its explanations describe thus the classiﬁer’s algorithm. However, the use
of increasingly complex algorithms (e.g. deep neural networks) has made them
more diﬃcult to explain and the inner workings of an algorithm may be inexpli-
cable even to the developers of those classiﬁers [6]. Furthermore, a predominant
ﬁnding from research in philosophy of sciences and social sciences is that a full
causal explanation of a prediction is undesired for humans, as they do not need
to understand the algorithm followed by a model. In other words, an expla-
nation does not necessarily hinge on the general public understanding of how
algorithmic systems function. Consequently, the second family provides expla-
nations without opening the black-box (e.g. [5,7–11]) and pays particular atten-
tion to what constitutes a good explanation for human users. Several notions
have been deﬁned in this second family including abductive explanations (e.g.
[12]), counterfactuals (e.g. [5]), semifactuals [13], contrastive explanations (e.g.
[8,9]), explanations based on pertinent positives and pertinent negatives (e.g.
[9]), adversarial examples (e.g. [14]), examples (e.g. [15]), and counter-examples
(e.g. [12]).
In this paper, we focus on complex classiﬁers whose internal processes are
inexplicable, and we follow the second approach for explaining their predictions.
We study the abductive explanations, which highlight key factors that cause
predictions. Such explanations are provided by the two prominent explanation
functions: Anchors and LIME [11,16]. Despite their popularity, the two functions
may return inaccurate and sometimes incorrect explanations, and the reason
behind this shortcoming remains unclear.
In this paper, we elucidate the origin of the shortcoming. For that purpose,
we start by deﬁning two explanations functions. The ﬁrst function generates
absolute explanations from the whole feature space. This approach is followed for
instance in [12,17]. We show that this function is coherent, i.e., its explanations
are compatible and do not justify distinct classes. However, in practice the whole
feature space is not available. Consequently, functions like Anchors and LIME
generate (abductive) explanations from a proper subset of the feature space.
Indeed, the explanations of a given instance are constructed from some instances
in its close neighbourhood.
Our second function generates thus what we call plausible explanations from
a subset of instances, which may be the neighbourhood of instances, or a set of
instances on which the classiﬁer returns a good conﬁdence, or simply a dataset
on which it was trained. Such explanations are only plausible (compared to
absolute) since they are generated from incomplete information. We show that
they are non monotonic in that an explanation may no longer be valid if the
subset of instances is extended with further ones. We show also that this function
is incoherent, leading to incorrect explanations (as in Anchors and LIME).

Non-monotonic Explanation Functions
21
To sum up, generating explanations is a non-monotonic reasoning problem,
and deﬁning a non-monotonic function that is coherent remains a challenge in
the XAI literature. In this paper we provide such a function that is based on
argumentation.
The paper is structured as follows: Sect. 2 presents the background on classi-
ﬁcation. Section 3 deﬁnes the two ﬁrst functions of explanation and Sect. 4 inves-
tigates their properties and links. Section 5 deﬁnes the argument-based explana-
tion function and the last section concludes.
2
Classiﬁcation Problem
Throughout the paper, we assume a ﬁnite and non-empty set F = {f1, . . . , fn}
of features (called also attributes) that take respectively their values from ﬁnite
domains D1, . . . , Dn. Let D = {D1, . . . , Dn}. For every feature f and every pos-
sible value v of f, the pair (f, v) is called literal feature, or literal for short. Let U
denote the universe of all possible literal features. The set X contains all possible
n−tuples of literal features or sets of the form {(f1, v1i), . . . , (fn, vnl)}, i.e., X
contains all the possible instantiations of the n features of F. X and its elements
are called respectively feature space and instances. Note that X is ﬁnite since F
and the n domains in D are ﬁnite. Let C = {c1, . . . , cm}, with m > 1, be a ﬁnite
and non-empty set of possible distinct classes.
Deﬁnition 1. A theory is a tuple T = ⟨F, D, C⟩.
We denote by XT the feature space of theory T = ⟨F, D, C⟩. When it is
clear from the context, we write X for short.
A classiﬁcation model is a function that assigns to every instance x ∈XT of
a theory T = ⟨F, D, C⟩a single prediction, which is a class from the set C.
Deﬁnition 2. Let T = ⟨F, D, C⟩be a theory. A classiﬁcation model is a func-
tion f s.t. f : XT →C.
Let us illustrate the above notions with a simple example.
Example 1. Assume a classiﬁcation problem of deciding whether to hike or not.
Hence, C = {c0, c1} where c0 stands for not hiking and c1 for hiking. The decision
is based on four attributes: Being in vacation (V ), having a concert (C), having
a meeting (M) and having an exhibition (E), thus F = {V, C, M, E}. Each
attribute is binary, i.e., takes two possible values from Di = {0, 1} (i = 1, . . . , 4).
Assume a classiﬁcation model f that assigns classes to instances of Y ⊂X as
shown in the table below.

22
L. Amgoud
Y
V
C
M
E H
x1 0
0
1
0
c0
x2 1
0
0
0
c1
x3 0
0
1
1
c0
x4 1
0
0
1
c1
x5 0
1
1
0
c0
x6 0
1
1
1
c0
x7 1
1
0
1
c1
A set of literal features is consistent if it does not contain two literals having
the same feature but distinct values.
Deﬁnition 3. A set H ⊆U is consistent iﬀ∄(f, v), (f ′, v′) ∈H such that
f = f ′ and v ̸= v′. Otherwise, H is said to be inconsistent.
3
Abductive Explanation Functions
In the machine learning literature, there is a decent amount of work on explaining
outcomes of classiﬁers. Several types of explanations have then been identiﬁed,
and the most prominent one is the so-called abductive explanation. The latter
amounts at answering the following question:
Why does an outcome hold?
In other words, why a particular class is assigned to an instance? The answer
consists in highlighting factors (i.e., literal features) that caused the given class.
Research in cognitive science revealed that in practice, humans provide par-
tial explanations instead of full ones. Instead of a full account, they expect an
explanation for the key factors that caused the given output instead of another.
Consequently, the two popular explanation functions Anchors and LIME [11,16]
as well as the one that generates the so-called pertinent positives [9] generate sets
of literals that cause a prediction. Below are two examples of such explanations:
(E1) I won’t hike ( c0) because I am not on vacation (V, 0).
(E2) You were denied your loan because your annual income was £30K.
In the ﬁrst example, the decision of not hiking is due to the attribute (Vaca-
tion) which takes the value 0. The second example is on decision about credit
worthiness, hence there are again two possible classes (0 and 1). The explanation
E2 means that when the feature Salary has the value £30K, the outcome is 0.
In what follows, we deﬁne the notion of explanation function. The latter
explains the predictions of a classiﬁcation model in the context of a ﬁxed theory.
In other words, it takes as input a classiﬁcation model f and a theory T , and
returns for every instance x ∈XT , the set of reasons that cause f(x). The same
function is thus applicable to any model f and any theory. Generally, a function

Non-monotonic Explanation Functions
23
generates explanations from a subset of instances which may be the dataset on
which the classiﬁer has been trained, or the set of instances on which it returns
a good conﬁdence, or the neighbourhood of some instances, etc.
Deﬁnition 4. An explanation function is a function g : XT →22U that gener-
ates from Y ⊆XT the reasons behind the predictions of a classiﬁer f in theory T
= ⟨F, D, C⟩. For x ∈XT , gY(x) denotes the set of explanations of x, or reasons
of assigning the class f(x) to x.
Abductive explanations have been studied in the AI literature a long time
ago (e.g. [18]). More recently, they have been used for interpreting blackbox
classiﬁcation models (e.g. [4,17,19]). The basic idea behind these works is to look
for regular behavior of a model in the whole feature space. More precisely, an
abductive explanation is deﬁned as a minimal (for set inclusion) set of literals (or
features in [4]) that is suﬃcient for predicting a class. It is worth mentioning that
a given prediction may have several abductive explanations. In what follows, we
call such explanations “absolute” since they are deﬁned by exploring the whole
feature space, thus under complete information, and consequently they cannot
be erroneous as we will see later.
Deﬁnition 5. Let ga be an explanation function of a classiﬁcation model f
applied to theory T = ⟨F, D, C⟩s.t. for Y ⊆XT , x ∈Y, H ⊆U, H ∈gY
a (x) iﬀ:
– H ⊆x
– ∀y ∈XT s.t. H ⊆y, f(y) = f(x)
– ∄H′ ⊂H such that H′ satisﬁes the above conditions.
H is called absolute abductive explanation of (x, f(x)).
Example 2. Consider the theory T = ⟨F, D, C⟩such that F = {f1, f2}, D1 =
D2 = {0, 1}, and C = {c1, c2, c3}. Assume a model f that assigns classes to
instances as shown in the table below.
Y
f1 f2
c
x1
0
0
c1
x2
0
1
c2
x3
1
0
c3
x4
1
1
c3
The absolute explanations of x1, x2, x3, x4 are as follows:
– gY
a (x1) = {H1}
H1 = {(f1, 0), (f2, 0)}
– gY
a (x2) = {H2}
H2 = {(f1, 0), (f2, 1)}
– gY
a (x3) = {H3}
H3 = {(f1, 1)}
– gY
a (x4) = {H3}

24
L. Amgoud
The deﬁnition of absolute explanations requires exploring the whole feature
space (see the second condition of Deﬁnition 5), which is quite diﬃcult and
maybe not feasible in practice. Consequently, functions like Anchor and LIME
[11,16] generate explanations from a proper subset of the feature space. They
focus on the closest instances of the instance to be explained. In what follows, we
deﬁne an explanation function whose outcomes are called plausible. It somehow
generalises and improves Anchor and LIME since it also provides abductive
explanations, but those that are minimal (for set inclusion). Minimality of an
explanation is important since it discards irrelevant information from being part
of the explanation. This condition is violated by Anchors and LIME.
Deﬁnition 6. Let gp be an explanation function of a classiﬁcation model f
applied to theory T = ⟨F, D, C⟩s.t. for Y ⊆X, and x ∈Y, H ∈gY
p (x)
iﬀ:
– H ⊆x
– ∀y ∈Y s.t. H ⊆y, f(y) = f(x)
– ∄H′ ⊂H such that H′ satisﬁes the above conditions.
H is called plausible abductive explanation of x.
Note that the function gp can be applied to diﬀerent subsets of X, and as we
will see later the results may not be the same.
Example 1 (Cont.). Consider the following sets:
– U1 = {(V, 0)}
– U2 = {(M, 1)}
– U3 = {(C, 1), (E, 0)}
– U4 = {(V, 1)}
– U5 = {(M, 0)}
It can be checked that:
– gY
p (x1) = {U1, U2}
– gY
p (x5) = {U1, U2, U3}
– gY
p (x2) = gY
p (x4) = gY
p (x7) = {U4, U5}
Every (absolute, plausible) abductive explanation is consistent. Furthermore,
an instance may have one or several (absolute, plausible) abductive explanations.
Proposition 1. Let T = ⟨F, D, C⟩be a theory, Y ⊆X and x ∈Y.
– Every (absolute, plausible) abductive explanation of x is consistent.
– If Y = X, then gY
a (x) = gY
p (x)
– gY
a (x) ̸= ∅and gY
p (x) ̸= ∅
– gY
p (x) = {∅} iﬀ∀y ∈Y \ {x}, f(y) = f(x).
Proof. An (absolute, plausible) explanation is a part of an instance. Every
instance is consistent, then its subparts are all consistent. The second prop-
erty is straightforward. Let us show the third property. Since x is consistent,
then ∃H ⊆x such that H is minimal (for set inclusion) such that ∀y ∈Y s.t.
H ⊆y, f(y) = f(x). The last property is straightforward.

Non-monotonic Explanation Functions
25
4
Properties of Explanation Functions
We discuss below two formal properties of explanation functions. Such proper-
ties are important for assessing the quality of an explanation function and for
comparing pairs of functions. The ﬁrst property ensures coherence of the set of
explanations provided by a function. More precisely, it states that a set of literals
cannot cause two distinct predictions. Let us illustrate the idea with Example 1.
Example 1 (Cont.). Recall that {(V, 0)} ∈gY
p (x1) and {(M, 0)} ∈gY
p (x2).
Note that the set {(V, 0), (M, 0)} is consistent, then there exists an instance y ∈
X \ Y such that {(V, 0), (M, 0)} ⊆y. By deﬁnition of an abductive explanation,
f(y) = c0 (due to {(V, 0)}) and f(y) = c1 (due to {(M, 0)}) which is impossible
since every instance has one class. This example shows that the function gp is
not coherent even locally, i.e. when working with Y ⊂X.
Below is a formal deﬁnition of the notion of coherence of an explanation
function.
Deﬁnition 7 (Coherence). An explanation function g is coherent iﬀfor any
model f, any theory T = ⟨F, D, C⟩, any Y ⊆XT , the following holds:
∀x, x′ ∈XT , ∀H ∈gY(x), ∀H′ ∈gY(x′), if H ∪H′ is consistent, then
f(x) = f(x′).
We show that the absolute abductive explanation function is coherent while
this property is violated by the plausible function as shown above.
Proposition 2. The function ga is coherent and gp is incoherent.
Proof. Example 1 shows a counter-example for the coherence of gp. Let us now
show that ga is coherent. Let x, x′ ∈X, c, c′ ∈C and H ∈ga(x), H′ ∈ga(x′).
Assume that H ∪H′ is consistent. Then, ∃z ∈X s.t. H ∪H′ ⊆z. By Deﬁnition 5,
f(z) = c (due to H) and f(z) = c′ (due to H′). Since (z) is unique, then c = c′.
Remark: As said before, Anchors and LIME explanation functions are somehow
instances of gp as they generate abductive explanations from a proper subset of
the feature space. They even do not use a whole dataset but only instances
that are in the neighbourhood of the instance being explained. Hence, the two
functions violate coherence.
Let us now investigate another property of explanation functions, that of
monotony. The idea is to check whether an instance generated from a dataset
remains plausible when the dataset is expanded with new instances.
Deﬁnition 8 (Monotony). An explanation function g is monotonic iﬀfor any
model f, any theory T = ⟨F, D, C⟩, any x ∈XT , the following property holds:
gY(x) ⊆gZ(x) whenever Y ⊆Z ⊆XT . It is non-monotonic otherwise.
The absolute function ga is clearly monotonic since it explores the whole
feature space, thus complete information. However, the plausible function is not
monotonic.

26
L. Amgoud
Proposition 3. The function ga is monotonic and gp is non-monotonic.
Let us illustrate the non-monotonicity of gp with an example.
Example 1 (Cont.).
Consider
the
instance
x5.
Recall
that
U3
=
{(C, 1), (E, 0)} is a plausible explanation of x5. Assume we receive the new
instance x8 below:
Y
V
C
M
E
H
x8
1
1
0
0
c1
Note that {(C, 1), (E, 0)} is no longer a plausible explanation of x5 that can
generated from the set Y ∪{x5}. Indeed, the second condition of Deﬁnition 6 is
not satisﬁed.
The above example shows that a plausible explanation of an instance is not
necessarily an absolute one, while, ideally gp should approximate ga.
Property 1. Let Y ⊂XT and x ∈Y. gY
p (x) ̸⊆gY
a (x).
To sum up, we have shown that in practice explanations are constructed
from a dataset, which is a subset of the feature space of a theory. However,
due to incompleteness of information in the dataset, some explanations may be
incorrect, i.e., they are not absolute. Furthermore, we have seen that the actual
functions (like Anchors and LIME) that generate plausible explanations suﬀer
from another weakness which is incoherence. The latter leads also to incorrect
explanations. Thus, deﬁning a nonmonotonic explanation function that is coher-
ent remains a challenge in the literature. In the next section, we deﬁne such a
function.
5
Argument-Based Explanation Function
Throughout this section we consider an arbitrary theory T = ⟨F, D, C⟩and a
subset Y ⊆XT of instances. We deﬁne a novel explanation function which is
based on arguments. The latter support classes, in the sense they provide the
minimal sets of literals that are causing a class. They are thus independent from
instances. An advantage of not considering instances is to reduce the number
of arguments that can be built. Furthermore, explanations of an instance are in
explanations of its predicted class.
Deﬁnition 9. Let c ∈C. An argument in favor of c is a pair ⟨H, c⟩s.t.
– H ⊆U
– H is consistent
– ∀y ∈Y s.t. H ⊆y, f(y) = c
– ∄H′ ⊂H that veriﬁes the above conditions.

Non-monotonic Explanation Functions
27
H and c are called respectively support and conclusion of an argument. Let
arg(Y) denote the set of arguments built from Y.
Note that the set arg(Y) is ﬁnite since Y is ﬁnite. Furthermore, its elements
are closely related to the plausible explanations of the function gp.
Proposition 4. Let c ∈C. ⟨H, c⟩∈arg(Y) ⇐⇒∃x ∈Y s.t. H ∈gY
p (x, c).
Consider the initial version of our running example, which contains seven
instances.
Example 1 (Cont.). There are two classes in the theory: c0, c1. Their argu-
ments are given below:
– a1 = ⟨U1, c0⟩
U1 = {(V, 0)}
– a2 = ⟨U2, c0⟩
U2 = {(M, 1)}
– a3 = ⟨U3, c0⟩
U3 = {(C, 1), (E, 0)}
– a4 = ⟨U4, c1⟩
U4 = {(V, 1)}
– a5 = ⟨U5, c1⟩
U5 = {(M, 0)}
These arguments may be conﬂicting. This is particularly the case when they
violate the coherence property, namely when their supports are consistent but
their conclusions are diﬀerent.
Deﬁnition 10. Let ⟨H, c⟩, ⟨H′, c′⟩∈arg(Y). We say that ⟨H, c⟩attacks ⟨H′, c′⟩
iﬀ:
– H ∪H′ is consistent, and
– c ̸= c′.
Obviously, the above attack relation is symmetric.
Property 2. Let a, b ∈arg(Y). If a attacks b, then b attacks a.
Example 1 (Cont.). The attacks between the arguments are depicted below:
Note that every argument in favor of a class attacks at least one argument
in favor of the other class. This shows that the plausible explanations generated
by the function gp are incoherent, and cannot all be correct.
Arguments and their attack relation form an argumentation system as fol-
lows.
Deﬁnition 11. An argumentation system built from Y ⊆X is a pair AS =
⟨arg(Y), R⟩where R ⊆arg(Y) × arg(Y) such that for a, b ∈arg(Y), (a, b) ∈R
iﬀa attacks b (in the sense of Deﬁnition 10).
Since arguments are conﬂicting, they should be evaluated using a semantics.
There are diﬀerent types of semantics in the literature. In this paper, we consider
extension-based ones that have been introduced by Dung in [20]. They compute
sets of arguments that can be jointly accepted. Each set is called an extension
and represents a coherent position. Since the attack relation is symmetric, then
it has been shown that stable and preferred semantics coincide with the naive,
which returns maximal (for set ⊆) sets that do not contain conﬂicting arguments.
So, we focus here on naive semantics.

28
L. Amgoud
Deﬁnition 12. Let AS = ⟨arg(Y), R⟩be an argumentation system and E ⊆
arg(Y). The set E is a naive extension iﬀ:
– ∄a, b ∈E s.t. (a, b) ∈R, and
– ∄E′ ⊆arg(Y) s.t. E ⊂E′ and E′ satisﬁes the ﬁrst condition.
Let σ(AS) denote the set of all naive extensions of AS.
Example 1 (Cont.). The argumentation system has four naive extensions:
– E1 = {a1, a2, a3}
– E2 = {a1, a4}
– E3 = {a2, a5}
– E4 = {a4, a5}
Each naive extension refers to a possible set of explanations. Note that E1 and
E4 promote respectively the arguments in favor of c0 and those in favor of c1.
We are now ready to deﬁne the new explanation function. For a given instance
x, it returns the support of any argument in favour of f(x) that is in every naive
extension and the support should be part of x. The intuition is the following:
when two explanations cannot hold together (coherence being violated), both
are discarded since at least one of them is incorrect. Our approach is thus very
cautious.
Deﬁnition 13. Let g∗be an explanation function of a classiﬁcation model f
applied to theory a T = ⟨F, D, C⟩s.t. for Y ⊆XT , for x ∈Y,
gY
∗(x) = {H | ∃⟨H, f(x)⟩∈

Ei∈σ(AS)
Ei and H ⊆x}
where AS = ⟨arg(Y), R⟩.
Example 1 (Cont.). In the example,

Ei∈σ(AS)
Ei
= ∅. Hence, ∀x ∈Y,
g∗(x, f(x)) = ∅.
The above example shows that this function may return an emptyset of
explanations, meaning with the available information, it is not possible to gen-
erate reasonable abductive explanations. Note that generation of argument is a
nonmonotonic process.
Example 2 (Cont.). Assume a set Y = {x1, x2}. It can be checked that
arg(Y) = {b1, b2} where b1 = {(f2, 0)} and b2 = {(f2, 1)}. The two arguments are
not conﬂicting, thus R = ∅and there is a single naive extension which contains
the two. Hence, g∗(x1, c1) = {{(f2, 0)}} and g∗(x2, c2) = {{(f2, 1)}}.
We show that the new function is non-monotonic and coherent.
Proposition 5. The function g∗is non-monotonic and coherent.
Finally, when g∗is applied on the whole feature space, the attack relation
of the corresponding argumentation system would be empty, and the generated
explanations coincide with the absolute ones.
Proposition 6. If Y = X, then g∗= ga.

Non-monotonic Explanation Functions
29
6
Related Work
Most work on ﬁnding explanations in the ML literature is experimental, focusing
on speciﬁc models, exposing their internal representations to ﬁnd correlations
post hoc between these representations and the predictions. There haven’t been
a lot of formal characterizations of explanations in AI, with the exception of [12],
which deﬁnes abductive explanations and adversarial examples in a fragment of
ﬁrst order logic, and [17], who focused on semi-factuals, that they consider a
speciﬁc form of counterfactuals. Both works considered binary classiﬁers with
binary features. Furthermore, they generate explanations from the whole feature
space, which in practice is not reasonable since a classiﬁcation model is trained
on a dataset. In our work, we focused on abductive explanations for general
classiﬁers, and discussed two particular properties: monotony and coherence.
Unlike our work, which explains existing Black-box models, [21,22] proposed
novel classiﬁcation models that are based on arguments. Their explanations are
deﬁned in dialectical way as ﬁctitious dialogues between a proponent (supporting
an output) and an opponent (attacking the output) following [20]. The authors
in [23–26] followed the same approach for deﬁning explainable multiple decision
systems, recommendation systems, or scheduling systems. In the above papers
an argument is simply an instance and its label while our arguments pro/con
are much richer. This shows that they are proposed for diﬀerent purposes.
7
Conclusion
This paper presented a preliminary investigation on functions that would explain
predictions of black-box classiﬁers. It focused on one type of explanations, those
that identify the key features that caused predictions. Such explanations are
popular in the XAI literature, however there are a few formal attempts at for-
malizing them and investigating their properties. Existing deﬁnitions consider
the whole feature space, and this is not reasonable in practice.
In this paper, we argue that generating explanations consists of reasoning
under incomplete information, and reasonable functions are thus nonmonotonic.
We have shown that existing (nonmonotonic) functions like Anchors and LIME
may return incoherent results, which means incorrect explanations. Finally, we
provided the ﬁrst function that satisﬁes coherence while generating explanations
from datasets. The function is based on a well-known nonmonotonic reasoning
(NMR) approach, which is argumentation. This makes thus a connection between
XAI and NMR.
This work can be extended in diﬀerent ways. First, we have seen that the
novel function g∗may return an emptyset for an instance. While cautious rea-
soning is suitable when dealing with conﬂicting information by NMR models, it
may be a great weakness in XAI since a user would always expect an explanation
for the outcome provided by a classiﬁer. Hence, a future work consists of explor-
ing other functions that guarantee outputs. Another line of research consists of
using weighted semantics for evaluating arguments. Such semantics would then
lead to weighted explanations.

30
L. Amgoud
Acknowledgements. Support from the ANR-3IA Artiﬁcial and Natural Intelligence
Toulouse Institute is gratefully acknowledged.
References
1. Biran, O., Cotton, C.: Explanation and justiﬁcation in machine learning: a survey.
In: IJCAI Workshop on Explainable Artiﬁcial Intelligence (XAI), pp. 1–6 (2017)
2. Guidotti, R., Monreale, A., Ruggieri, S., Turini, F., Giannotti, F., Pedreschi, D.:
A survey of methods for explaining black box models. ACM Comput. Surv. 51(5),
93:1-93:42 (2019)
3. Miller, T.: Explanation in artiﬁcial intelligence: insights from the social sciences.
Artif. Intell. 267, 1–38 (2019)
4. Ignatiev, A., Narodytska, N., Marques-Silva, J.: Abduction-based explanations for
machine learning models. In: The Thirty-Third Conference on Artiﬁcial Intelli-
gence, pp. 1511–1519. AAAI (2019)
5. Wachter, S., Mittelstadt, B.D., Russell, C.: Counterfactual explanations without
opening the black box: automated decisions and the GDPR. CoRR abs/1711.00399
(2017)
6. Eiband, M., Schneider, H., Bilandzic, M., Fazekas-Con, J., Haug, M., Hussmann,
H.: Bringing transparency design into practice. In: Proceedings of the 24th Inter-
national Conference on Intelligent User Interfaces IU I, pp. 211–223 (2018)
7. Biran, O., McKeown, K.R.: Human-centric justiﬁcation of machine learning pre-
dictions. In: Proceedings of the Twenty-Sixth International Joint Conference on
Artiﬁcial Intelligence, pp. 1461–1467. IJCAI (2017)
8. Luss, R., Chen, P., Dhurandhar, A., Sattigeri, P., Shanmugam, K., Tu, C.: Gener-
ating contrastive explanations with monotonic attribute functions. CoRR (2019)
9. Dhurandhar, A., et al.: Explanations based on the missing: towards contrastive
explanations with pertinent negatives. In: Annual Conference on Neural Informa-
tion Processing Systems, NeurIPS, pp. 590–601 (2018)
10. Mittelstadt, B., Russell, C., Wachter, S.: Explaining explanations in AI. In: Pro-
ceedings of the Conference on Fairness, Accountability, and Transparency, pp. 279–
288 (2019)
11. Ribeiro, M.T., Singh, S., Guestrin, C.: Anchors: high-precision model-agnostic
explanations. In: Proceedings of the Thirty-Second AAAI Conference on Artiﬁ-
cial Intelligence, (AAAI 2018), pp. 1527–1535 (2018)
12. Ignatiev, A., Narodytska, N., Marques-Silva, J.: On relating explanations and
adversarial examples. In: Thirty-Third Conference on Neural Information Process-
ing Systems, NeurIPS, pp. 15857–15867 (2019)
13. Byrne, R.: Semifactual “even if” thinking. Thinking Reasoning 8(1), 41–67 (2002)
14. Szegedy, C., et al.: Intriguing properties of neural networks. In: 2nd International
Conference on Learning Representations, ICLR (2014)
15. Cai, C.J., Jongejan, J., Holbrook, J.: The eﬀects of example-based explanations in
a machine learning interface. In: Proceedings of the 24th International Conference
on Intelligent User Interfaces IUI, pp. 258–262 (2019)
16. Ribeiro, M.T., Singh, S., Guestrin, C.: Why should I trust you?: explaining the pre-
dictions of any classiﬁer. In: Proceedings of the 22nd ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining, pp. 1135–1144 (2016)
17. Darwiche, A., Hirth, A.: On the reasons behind decisions. In: 24th European Con-
ference on Artiﬁcial Intelligence ECAI, vol. 325, pp. 712–720. IOS Press (2020)

Non-monotonic Explanation Functions
31
18. Dimopoulos, Y., Dzeroski, S., Kakas, A.: Integrating explanatory and descriptive
learning in ILP. In: Proceedings of the Fifteenth International Joint Conference on
Artiﬁcial Intelligence, IJCAI, pp. 900–907 (1997)
19. Kakas, A., Riguzzi, F.: Abductive concept learning. New Gener. Comput. 18(3),
243–294 (2000)
20. Dung, P.M.: On the acceptability of arguments and its fundamental role in non-
monotonic reasoning, logic programming and n-person games. Artif. Intell. 77,
321–357 (1995)
21. Amgoud, L., Serrurier, M.: Agents that argue and explain classiﬁcations. Auton.
Agent. Multi-Agent Syst. 16(2), 187–209 (2008)
22. Cocarascu, O., Stylianou, A., Cyras, K., Toni, F.: Data-empowered argumentation
for dialectically explainable predictions. In: 24th European Conference on Artiﬁcial
Intelligence ECAI, vol. 325, pp. 2449–2456. IOS Press (2020)
23. Zhong, Q., Fan, X., Luo, X., Toni, F.: An explainable multi-attribute decision
model based on argumentation. Expert Syst. Appl. 117, 42–61 (2019)
24. Cyras, K., et al.: Explanations by arbitrated argumentative dispute. Expert Syst.
Appl. 127, 141–156 (2019)
25. Cyras, K., Letsios, D., Misener, R., Toni, F.: Argumentation for explainable
scheduling. In: The Thirty-Third Conference on Artiﬁcial Intelligence, pp. 2752–
2759. AAAI (2019)
26. Rago, A., Cocarascu, O., Toni, F.: Argumentation-based recommendations: fan-
tastic explanations and how to ﬁnd them. In: Proceedings of the Twenty-Seventh
International Joint Conference on Artiﬁcial Intelligence, IJCAI, pp. 1949–1955
(2018)

Similarity Measures Based on Compiled
Arguments
Leila Amgoud1 and Victor David2(B)
1 CNRS – IRIT, Toulouse, France
leila.amgoud@irit.fr
2 Paul Sabatier University – IRIT, Toulouse, France
victor.david@irit.fr
Abstract. Argumentation is a prominent approach for reasoning with
inconsistent information. It is based on the justiﬁcation of formulas by
arguments generated from propositional knowledge bases. It has recently
been shown that similarity between arguments should be taken into
account when evaluating arguments. Consequently, diﬀerent similarity
measures have been proposed in the literature. Although these measures
satisfy desirable properties, they suﬀer from the side eﬀects of being
syntax-dependent. Indeed, they may miss redundant information, lead-
ing to undervalued similarity. This paper overcomes this shortcoming by
compiling arguments, which amounts to transforming their formulas into
clauses, and using the latter for extending existing measures and prin-
ciples. We show that the new measures deal properly with the critical
cases.
1
Introduction
Argumentation is a reasoning process based on the justiﬁcation of claims by
arguments, i.e., reasons for accepting claims. It has been extensively developed
in Artiﬁcial Intelligence. Indeed, it was used for diﬀerent purposes including
decision making (e.g. [1,2]), defeasible reasoning (e.g. [3,4]), and negotiation
[5,6].
Argumentation is also used as an alternative approach for handling incon-
sistency in knowledge bases [7–9]. Starting from a knowledge base encoded in
propositional logic, arguments are built using the consequence operator of the
logic. An argument is a pair made of a set of formulas (called support) and a
single formula (called conclusion). The conclusion follows logically from the sup-
port. Examples of arguments are A = ⟨{p ∧q ∧r}, p ∧q⟩, B = ⟨{p ∧q}, p ∧q⟩
and C = ⟨{p, q}, p ∧q⟩. Once arguments are deﬁned, attacks between them are
identiﬁed and a semantics is used for evaluating the arguments, ﬁnally formulas
supported by strong arguments are inferred from the base.
Some semantics, like h-Categorizer [7], satisfy the Counting (or Strict
Monotony) principle deﬁned in [10]. This principle states that each attacker
of an argument contributes to weakening the argument. For instance, if the
c
⃝Springer Nature Switzerland AG 2021
J. Vejnarov´a and N. Wilson (Eds.): ECSQARU 2021, LNAI 12897, pp. 32–44, 2021.
https://doi.org/10.1007/978-3-030-86772-0_3

Similarity Measures Based on Compiled Arguments
33
argument D = ⟨{¬p ∨¬q}, ¬p ∨¬q⟩is attacked by A, B, C, then each of the
three arguments will decrease the strength of D. However, the three attackers
are somehow similar, thus D will lose more than necessary. Consequently, the
authors in [11] have motivated the need for investigating the notion of similar-
ity between pairs of such logical arguments. They introduced a set of principles
that a reasonable similarity measure should satisfy, and provided several mea-
sures that satisfy them. In [12–14] several extensions of h-Categorizer that take
into account similarities between arguments have been proposed.
While the measures from [11] return reasonable results in most cases, it was
shown in [15], that they may lead to inaccurate assessments if arguments are not
concise. An arguments is concise if its support contains only information that is
useful for inferring its conclusion. For instance, the argument A is not concise
since its support {p ∧q ∧r} contains r, which is useless for the conclusion p ∧q.
The similarity measures from [11] declare the two arguments A and B as not
fully similar while they support the same conclusion on the same grounds (p∧q).
Consequently, both A and B will have an impact on D using h-Categorizer. In
[15], such arguments are cleaned up from any useless information by generating
the concise versions of each argument, the measures from [11] are applied on
concise arguments. However, these works fail to detect the full similarity between
the two concise arguments B and C. In this paper, we solve the above issue by
compiling arguments. The idea is to transform every formula in an argument’s
support into clauses. We extend the Jaccard-based similarity measures from [11]
and show that new versions improve accuracy of similarity.
The article is organised as follows: Sect. 2 recalls the notions of logical argu-
ment and similarity measure. Section 3 introduces compilation of arguments.
Section 4 extends some measures of similarity. Section 5 extends and propose
new principles for similarity measures, and the last Sect. 6 concludes.
2
Background
2.1
Logical Concepts
Throughout the paper, we consider classical propositional logic (L, ⊢), where
L is a propositional language built up from a ﬁnite set P of variables, the two
Boolean constants ⊤(true) and ⊥(false), and the usual connectives (¬, ∨, ∧, →,
↔), and ⊢is the consequence relation of the logic. A literal is either a variable or
the negation of a variable of P, the set of all literals is denoted P±. Two formulas
φ, ψ ∈L are logically equivalent, denoted by φ ≡ψ, iﬀφ ⊢ψ and ψ ⊢φ.
A formula φ is in negation normal form (NNF) if and only if it does not
contain implication or equivalence symbols, and every negation symbol occurs
directly in front of an atom. Following [16], we slightly abuse words and denote by
NNF(φ) the formula in NNF obtained from φ by “pushing down” every occurrence
of ¬ (using De Morgan’s law) and eliminating double negations. For instance,
NNF(¬((p →q) ∨¬t)) = p ∧¬q ∧t.

34
L. Amgoud and V. David
Let φ ∈L, φ is in a conjunctive normal form (CNF) if is a conjunction of
clauses 
i ci where each clause ci is a disjunction of literals 
j lj. For instance
p ∧(q ∨t) is in a CNF while (p ∧q) ∨t is not.
We denote by Lit(φ) the set of literals occurring in NNF(φ), hence Lit(¬((p →
q) ∨¬t)) = {p, ¬q, t}. The function Var(φ) returns all the variables occurring in
the formula φ (e.g., Var(p ∧¬q ∧t) = {p, q, t}).
A ﬁnite subset Φ of L, denoted by Φ ⊆f L, is consistent iﬀΦ ⊬⊥, it is
inconsistent otherwise. Let us now deﬁne when two ﬁnite sets Φ and Ψ of formulas
are equivalent. A natural deﬁnition is when the two sets have the same logical
consequences, i.e., {φ ∈L |Φ ⊢φ} = {ψ ∈L |Ψ ⊢ψ}. Thus, the three sets {p, q},
{p ∧p, ¬¬q}, and {p ∧q} are pairwise equivalent. This deﬁnition is strong since
it considers any inconsistent sets as equivalent. For instance, {p, ¬p} and {q, ¬q}
are equivalent even if the contents (i.e. meaning of variables and formulas) of
the two sets are unrelated (assume that p and q stand respectively for “bird”
and “ﬂy”). Furthermore, it considers the two sets {p, p →q} and {q, q →p} as
equivalent while their contents are diﬀerent as well. Clearly, the two rules “birds
ﬂy” and “everything that ﬂies is a bird” express diﬀerent information. Thus, the
two sets {p, p →q} and {q, q →p} should be considered as diﬀerent. Thus, in
what follows we consider the following deﬁnition borrowed from [17]. It compares
formulas contained in sets instead of logical consequences of the sets.
Deﬁnition 1 (Equivalent Sets of Formulas). Two sets of formulas Φ, Ψ ⊆f
L are equivalent, denoted by Φ ∼= Ψ, iﬀthere is a bijection f : Φ →Ψ s.t. ∀φ ∈Φ,
φ ≡f(φ).
Example 1. {p, p →q} ̸∼= {q, q →p}, {p, ¬p} ̸∼= {q, ¬q}, {p, q} ̸∼= {p ∧q} while
{p, q} ∼= {p ∧p, ¬¬q}.
Let us deﬁne the notion of argument as in [7].
Deﬁnition 2 (Argument). An argument built under the logic (L, ⊢) is a pair
⟨Φ, φ⟩, where Φ ⊆f L and φ ∈L, such that:
– Φ is consistent,
(Consistency)
– Φ ⊢φ,
(Validity)
– ∄Φ′ ⊂Φ such that Φ′ ⊢φ.
(Minimality)
An argument ⟨Φ, φ⟩is trivial iﬀΦ = ∅and φ ≡⊤.
Example 2. The three pairs A = ⟨{p ∧q ∧r}, p ∧q⟩, B = ⟨{p ∧q}, p ∧q⟩and
C = ⟨{p, q}, p ∧q⟩are arguments.
Notations: Arg(L) denotes the set of all arguments that can be built in (L, ⊢).
For any A = ⟨Φ, φ⟩∈Arg(L), the functions Supp and Conc return respectively
the support (Supp(A) = Φ) and the conclusion (Conc(A) = φ) of A.
Note that the argument A in the above example is not concise since r is irrel-
evant for the argument’s conclusion. In [15], the concise versions of arguments
are computed using the following technique of reﬁnement.

Similarity Measures Based on Compiled Arguments
35
Deﬁnition 3 (Reﬁnement).
Let A, B ∈Arg(L) s.t. A = ⟨{φ1, . . . , φn}, φ⟩,
B = ⟨{φ′
1, . . . , φ′
n}, φ′⟩. B is a reﬁnement of A iﬀ:
1. φ = φ′,
2. There exists a permutation ρ of the set {1, . . . , n} such that ∀k ∈{1, . . . , n},
φk ⊢φ′
ρ(k) and Lit(φ′
ρ(k)) ⊆Lit(φk).
Let Ref be a function that returns the set of all reﬁnements of a given argument.
Compared to the deﬁnition in [15], we extended the second constraint so
that literals of φ′
ρ(k) are also literals of φk. The reason is that we would like the
arguments ⟨{p ∧(p ∨q)}, p ∨q⟩and ⟨{(p ∨¬q) ∧(p ∨q)}, p ∨q⟩can be reﬁned
into ⟨{p ∨q}, p ∨q⟩while this was not possible in the original deﬁnition.
It is worth mentioning that an argument may have several reﬁnements as
shown in the following example.
Example 2 (Continued). The following sets are subset of reﬁnement of these
arguments.
– {⟨{p ∧q ∧r}, p ∧q⟩, ⟨{p ∧q ∧(p ∨q)}, p ∧q⟩, ⟨{p ∧q}, p ∧q⟩} ⊆Ref(A)
– {⟨{p ∧q}, p ∧q⟩, ⟨{p ∧¬¬q}, p ∧q⟩} ⊆Ref(B)
– {⟨{p, q}, p ∧q⟩, ⟨{(p ∨p) ∧q}, p ∧q⟩} ⊆Ref(C)
Every argument is a reﬁnement of itself.
Proposition 1. For any argument A ∈Arg(L), A ∈Ref(A).
2.2
Similarity Measures
In [11], various measures have been proposed for assessing similarity between
pairs of logical arguments. They extended existing measures which compare sets
of objects including Jaccard measure [18]. Due to space limitation, in this paper
we will focus only on the latter.
Deﬁnition 4 (Jaccard Measure). Let X, Y be arbitrary sets of objects. If
X ̸= ∅or Y ̸= ∅, then1
sjac(X, Y ) = |X ∩Y |
|X ∪Y |
If X = Y = ∅, then sjac(X, Y ) = 1.
We recall below how the above measure is used in [11] for logical arguments.
Deﬁnition 5 (Jaccard Similarity Measure).
Let 0 < σ < 1. We deﬁne
simσ
jac as a function assigning to any pair (A, B) ∈Arg(L) × Arg(L) a value
simσ
jac(A, B) =
σ.sjac(Supp(A), Supp(B)) + (1 −σ)sjac({Conc(A)}, {Conc(B)}).
Example 2 (Continued). sim0.5
jac(A, B) = sim0.5
jac(A, C) = sim0.5
jac(B, C) = 0.5·0+
0.5 · 1 = 0.5.
1 3|| stands for the cardinality of a set.

36
L. Amgoud and V. David
3
Compilation of Arguments
Recall that {p, q} ̸∼= {p∧q} while the two sets contain the same information. For
getting them equivalent, we transform every formula into a CNF, then we split
it into a set containing its clauses. Note that it is well known (e.g. [19]) that
any formula can be transformed into equivalent CNFs using the same literals.
In our approach, we consider one CNF per formula. For that purpose, we will
use a ﬁnite sub-language F that contains one formula per equivalent class and
the formula should be in a CNF.
Deﬁnition 6 (Finite CNF language F). Let F ⊂f L such that ∀φ ∈L, there
exists a unique ψ ∈F such that:
– φ ≡ψ,
– Lit(φ) = Lit(ψ),
– ψ is in a CNF.
Let CNF(φ) = ψ.
While we do not specify the elements of F, we use concrete formulas in the
examples, and they are assumed to belong to F.
Notation: For Φ ⊆f L, UC(Φ) =

φ∈CNF(Φ)

δ clause ∈φ
δ.
Let us now introduce the notion of compiled argument.
Deﬁnition 7 (Compiled Argument).
The compilation of A ∈Arg(L) is
A∗= ⟨UC(Supp(A)), Conc(A)⟩.
Example 2 (Continued). The compilations of the three arguments A, B, C are:
– A∗= ⟨{p, q, r}, p ∧q⟩,
– B∗= ⟨{p, q}, p ∧q⟩,
– C∗= ⟨{p, q}, p ∧q⟩.
Note that the compilation of an argument is not necessarily an element of
Arg(L) as it may violate the minimality condition. For instance, we can see that
A∗/∈Arg(L) since the set {p, q, r} is not minimal.
To solve this problem of non-minimal compiled arguments, we will extend
the notion of concise argument (from [15]) while ﬁxing the syntax-dependency
issue. To deﬁne what a concise argument is, we ﬁrst need to introduce what
equivalent arguments are.
Thanks to the use of the language F in the compiled arguments, any equiva-
lent formulas using the same literals have an identical syntax. Using this feature,
we deﬁne that arguments are equivalent when they have identical compiled sup-
ports and conclusions.

Similarity Measures Based on Compiled Arguments
37
Deﬁnition 8 (Equivalent Arguments). Two arguments A, B ∈Arg(L) are
equivalent, denoted by A ≈B, iﬀ
UC(Supp(A)) = UC(Supp(B)) and UC({Conc(A)}) = UC({Conc(B)}).
We denote by A ̸≈B when A and B are not equivalent.
Example 2 (Continued). B ≈C while A ̸≈B and A ̸≈C.
Note that arguments having the same compilation of their supports may not
be equivalent. This is for instance the case of the two arguments D and E such
that D = ⟨{(p∨q)∧(p∨¬q)}, (p∨q)∧(p∨¬q)⟩and E = ⟨{(p∨q)∧(p∨¬q)}, p⟩.
Clearly, D ̸≈E.
We show that trivial arguments are not equivalent. For instance ⟨∅, p∨¬p⟩̸≈
⟨∅, q ∨¬q⟩because {p ∨¬p} ̸= {q ∨¬q}.
Proposition 2. Trivial arguments are pairwise non equivalent.
The objective of the notion of concise arguments is to produce the set of
compiled arguments that satisfy the Deﬁnition 2. To do this, we observed that
any non-equivalent reﬁned argument A′, of a compiled argument A∗∈Arg(L),
produces a new clause by inference between several clauses of the same formula.
Proposition 3. Let A ∈Arg(L). If A′ ∈Ref(A), A′ ̸≈A and A∗∈Arg(L)
then ∃δ′ ∈UC(Supp(A′)) such that ∀δ ∈UC(Supp(A)), δ ̸= δ′.
Let us deﬁne a concise argument.
Deﬁnition 9 (Conciseness). An argument A ∈Arg(L) is concise iﬀfor all
B ∈Ref(A) such that:
– B∗∈Arg(L) and,
– ∀δ ∈UC(B), ∃δ′ ∈UC(A) s.t. δ = δ′,
we have A ≈B.
Let CR(A) denote the set of all A′ concise reﬁnements of A such that
UC(Supp(A′)) ⊆UC(Supp(A)).
The ﬁrst constraint ensures that argument A does not have unnecessary
information (clause) to infer its conclusion. The second constraint prevents argu-
ment A from being compared with arguments that have created new information
(clause) in their support.
Example 2 (Continued). The following sets are subset of concise reﬁnements of
the arguments A, B, C:
– {⟨{p ∧q}, p ∧q⟩, ⟨{p ∧p ∧q}, p ∧q⟩} ⊂CR(A)
– {⟨{p ∧q}, p ∧q⟩, ⟨{p ∧q ∧q}, p ∧q⟩} ⊂CR(B)
– {⟨{p, q}, p ∧q⟩, ⟨{p ∧p, q ∧q}, p ∧q⟩} ⊂CR(C)

38
L. Amgoud and V. David
The non trivial argument have an inﬁnite set of concise reﬁnements.
Proposition 4. Let A ∈Arg(L), if A is not trivial then |CR(A)| = ∞.
A trivial argument have only one concise reﬁnement, itself.
Proposition 5. Let a trivial argument A ∈Arg(L), CR(A) = {A}.
We show in the following theorem that the notion of a concise argument
is equivalent to that of a compiled argument satisfying the conditions of the
Deﬁnition 2.
Theorem 1. For any A ∈Arg(L), A is concise iﬀA∗∈Arg(L).
When two arguments are equivalent they have equivalent concise reﬁnements.
In the same way that we deﬁne a set of equivalent formulae (Deﬁnition 1), we
may deﬁne a set of equivalent arguments.
Deﬁnition 10 (Equivalent sets of Arguments).
Two sets of arguments
ΩA, ΩB ⊆f Arg(L), are equivalent, denoted by ΩA ∼=arg ΩB, iﬀthere is a bijec-
tion f : ΩA →ΩB s.t. ∀A′ ∈ΩA, A′ ≈f(A′). Otherwise ΩA and ΩB are not
equivalent, denoted by ΩA ̸∼=arg ΩB.
Equivalent arguments have equivalent concise reﬁnements.
Proposition 6. Let A, B ∈Arg(L), if A ≈B then CR(A) ∼=arg CR(B).
Example 2 (Continued). B ≈C and CR(B) ∼=arg CR(C).
We can also see in this example that there are non-equivalent arguments with
equivalent concise reﬁnements: A ̸≈B and CR(A) ∼=arg CR(B). This is because
the equivalent arguments take into account irrelevant information.
Finally, note that when a compiled argument is not minimal, this may be due
to the presence of unnecessary literals in the support which will be removed in
the concise arguments; or because the argument is complex, i.e. it has diﬀerent
reasonings to conclude, which will produce diﬀerent concise arguments.
Example 2 (Continued). Let A = ⟨{p ∧q ∧r}, p ∧q⟩, F = ⟨{p ∧q, (p →r) ∧(q →
r)}, r⟩∈Arg(L).
The compiled argument of A and F are:
– A∗= ⟨{p, q, r}, p ∧q⟩, and
– F ∗= ⟨{p, q, p →r, q →r}, r⟩.
Clearly, r is irrelevant in A∗and F ∗may infer r according to p or q. We may see
in the concise versions of the arguments A and F that unnecessary information
is removed and complex information is separated to have minimal arguments:
– {⟨{p ∧q}, p ∧q⟩} ⊂CR(A), and
– {⟨{p, p →r}, r⟩, ⟨{q, q →r}, r⟩} ⊂CR(F).

Similarity Measures Based on Compiled Arguments
39
4
Extended Similarity Measures
We are ready now to introduce the new extended similarity measures.
First, we propose to apply similarity measure between set of objects on com-
piled supports and conclusions.
Deﬁnition 11 (Extended Jaccard Similarity Measure). Let 0 < σ < 1.
We deﬁne simσ
jac∗as a function assigning to any pair (A, B) ∈Arg(L) × Arg(L)
a value simσ
jac∗(A, B) =
σ.sjac(UC(Supp(A)), UC(Supp(B))) + (1 −σ)sjac(UC(Conc(A)), UC(Conc(B))).
Example 2 (Continued).
– sim0.5
jac∗(A, B) = sim0.5
jac∗(A, C) = 0.5 · 2
3 + 0.5 · 1 = 5
6 = 0.833.
– sim0.5
jac∗(B, C) = 0.5 · 1 + 0.5 · 1 = 1.
As we can see, working only with the compilation of arguments is not suf-
ﬁcient to assess degrees of similarity. It is also necessary to eliminate irrelevant
information. In the following we propose to extend the two family from [15]
dealing with concise reﬁnements of an argument.
Deﬁnition 12 (Finite Conciseness). Let A ∈Arg(L). We deﬁne the set
CR(A) = {B ∈CR(A) | Supp(B) ⊂F}.
In this way, we obtain a ﬁnite set of non-equivalent concise reﬁnements.
Proposition 7. For every A ∈Arg(L), the set CR(A) is ﬁnite.
We may now extend the two families of similarity measures (from [15]), to
add a syntax independent treatment.
Deﬁnition 13 (A-CR Jaccard Similarity Measure). Let A, B ∈Arg(L), and
let simσ
jac∗and σ ∈]0, 1[. We deﬁne A-CR Jaccard Similarity Measure2 by
simA
CR(A, B, simσ
jac∗) =

Ai∈CR(A)
Max(Ai, CR(B), simσ
jac∗) +

Bj∈CR(B)
Max(Bj, CR(A), simσ)
jac∗
|CR(A)| + |CR(B)|
.
The value of A-CR Jaccard Similarity Measure always belongs to the unit
interval.
Proposition 8. Let A, B ∈Arg(L), simσ
jac∗and σ ∈
]0, 1[. Then simA
CR(A,
B, simσ
jac∗) ∈[0, 1].
2 The letter A in A-CR stands for “average”.

40
L. Amgoud and V. David
Now we deﬁne our second family of similarity measures, which is based on
comparison of sets obtained by merging supports of concise reﬁnements of argu-
ments. For an argument A ∈Arg(L), we denote that set by
US(A) =

A′∈CR(A)
UC(Supp(A′)).
Deﬁnition 14 (U-CR Jaccard Similarity Measure). Let A, B ∈Arg(L),
0
<
σ
<
1, and sjac. We deﬁne U-CR Jaccard Similarity Measure3 by
simU
CR(A, B, sjac, σ) =
σ · sjac(US(A), US(B)) + (1 −σ) · sjac(UC(Conc(A)), UC(Conc(B))).
More generally, using the compilations allows to be more accurate even in the
supports. Given that each clause are a formula the similarity degree can increase
or decrease.
Example 3. Let A = ⟨{p ∧q, t}, p ∧q ∧t⟩, B = ⟨{p ∧q, r}, p ∧q ∧r⟩, C =
⟨{p ∧q, r}, p ∧q ∧r⟩, D = ⟨{s ∧t ∧u ∧v, r}, s ∧t ∧u ∧v ∧r⟩∈Arg(L) and
σ = 0.5.
– sim0.5
jac(A, B) = 0.5 · 1
3 + 0.5 · 0 = 1
6 = 0.167.
– sim0.5
jac∗(A, B) = 0.5 · 2
4 + 0.5 · 2
4 = 1
2 = 0.5.
– sim0.5
jac(C, D) = 0.5 · 1
3 + 0.5 · 0 = 1
6 = 0.167.
– sim0.5
jac∗(C, D) = 0.5 · 1
7 + 0.5 · 1
7 = 1
7 = 0.143.
Therefore sim0.5
jac(A, B) ≤sim0.5
jac∗(A, B) and sim0.5
jac(C, D) ≥sim0.5
jac∗(C, D).
Returning to our running example (from the introduction), we see that the
three arguments A, B, C are now identiﬁed as being completely similar. That
is, we have dealt with both the irrelevant information problem and the syntax-
dependency problem.
Example 2 (Continued). For any σ ∈]0, 1[:
– simA
CR(A, B, simσ
jac∗) = simA
CR(A, C, simσ
jac∗) = simA
CR(B, C, simσ
jac∗) = 1.
– simU
CR(A, B, sjac, σ) = simU
CR(A, C, sjac, σ) = simU
CR(B, C, sjac, σ) = 1.
The next Theorem ensure that the two family of similarity measures (A-CR
and U-CR) give the maximal degree of similarity between arguments only on
arguments having equivalent concise reﬁnements.
Theorem 2. Let A, B ∈Arg(L), for any σ ∈
]0, 1[, simA
CR(A, B, simσ
jac∗) =
simU
CR(A, B, sjac, σ) = 1 iﬀCR(A) ∼=arg CR(B).
From Proposition 6, we know that if A ≈B then CR(A) ∼=arg CR(B), then we
can deduce the following corollary.
Corollary 1. Let A, B ∈Arg(L), for any σ ∈]0, 1[, if A ≈B then simA
CR(A,
B, simσ
jac∗) = simU
CR(A, B, sjac, σ) = 1.
3 U in U-CR stands for “union”.

Similarity Measures Based on Compiled Arguments
41
5
Extended Principles
The issue of syntax-dependence also exists in some principles. We propose here
new principles for similarity measure between pairs of logical arguments and we
extend some principles from [11].
The ﬁrst new principle, called Minimality, ensures that similarity depends
on the content of arguments. It states that if two arguments do not share any
variables, then they are completely diﬀerent. An example of such arguments are
⟨{p}, p ∨q⟩and ⟨{t}, t⟩.
Principle 1 (Minimality). A similarity measure sim satisﬁes Minimality iﬀ
for all A, B ∈Arg(L), if
–

φi∈Supp(A)
Var(φi) ∩

φj∈Supp(B)
Var(φj) = ∅and
– Var(Conc(A)) ∩Var(Conc(B)) = ∅,
then sim(A, B) = 0.
Example 4. Let A = ⟨{p, q}, p ∧q⟩, B = ⟨{r}, r⟩∈Arg(L). A similarity measure
sim satisfying Minimality ensure that sim(A, B) = 0.
The second new principle consider that when two concise arguments have
common information in their supports or conclusions hence they own some sim-
ilarity between them.
Principle 2 (Non-Zero). A similarity measure sim satisﬁes Non-Zero iﬀfor
all A, B, A∗, B∗∈Arg(L), if
– UC(Supp(A)) ∩UC(Supp(B)) ̸= ∅, or
– UC(Conc(A)) ∩UC(Conc(B)) ̸= ∅,
then sim(A, B) > 0.
Example 5. Let A = ⟨{p, q}, p ∧q⟩, B = ⟨{p}, p⟩∈Arg(L). A similarity measure
sim satisfying Non-Zero ensure that sim(A, B) > 0.
The Maximality, Symmetry, Substitution, Syntax Independence principles
deﬁned in [11] do not need to be extend while (Strict) Monotony and (Strict)
Dominance have to because they are dependent of the content.
Monotony states that similarity between two arguments is all the greater
when the supports of the arguments share more formulas.
Principle 3 (Monotony – Strict Monotony). A similarity measure sim sat-
isﬁes Monotony iﬀfor all A, B, C, A∗, B∗, C∗∈Arg(L), if
1. UC(Conc(A)) = UC(Conc(B)) or Var(UC(Conc(A))) ∩Var(UC(Conc(C))) = ∅,
2. UC(Supp(A)) ∩UC(Supp(C)) ⊆UC(Supp(A)) ∩UC(Supp(B)),
3. UC(Supp(B)) \ UC(Supp(A)) ⊆UC(Supp(C)) \ UC(Supp(A)),

42
L. Amgoud and V. David
then the following hold:
– sim(A, B) ≥sim(A, C)
(Monotony)
– If the inclusion in condition 2 is strict or, UC(Supp(A)) ∩UC(Supp(C)) ̸= ∅
and condition 3 is strict, then sim(A, B) > sim(A, C). (Strict Monotony)
This extended monotony principle works only on concise arguments (i.e. using
only relevant clauses). Indeed, when an argument has irrelevant information (e.g.
⟨{p ∧q ∧r}, p ∧q⟩, r is irrelevant), this distorts the measurement.
In addition, constraints 2 and 3 are more precise than the original one. By
making this compilation of supports we can go deeper into the formulas for a
better evaluation.
Example 6. Let A = ⟨{p∧q, r}, p∧q∧r⟩, B = ⟨{p, q, s}, p∧q∧s⟩, C = ⟨{p, s, (p∧
s) →t}, t⟩∈Arg(L). Their compiled arguments are:
– A∗= ⟨{p, q, r}, p ∧q ∧r⟩,
– B∗= ⟨{p, q, s}, p ∧q ∧s⟩, and
– C∗= ⟨{p, s, ¬p ∨¬s ∨t}, t⟩.
Thanks to the compilation of arguments, a similarity measure sim which satisﬁes
the new principle of Strict Monotony, ensures that sim(A, B) > sim(A, C).
Let us present the last principle dealing with the conclusions.
Principle 4 [Dominance – Strict Dominance]. A similarity measure sim satis-
ﬁes Dominance iﬀfor all A, B, C, A∗, B∗, C∗∈Arg(L), if
1. UC(Supp(B)) = UC(Supp(C)),
2. UC(Conc(A)) ∩UC(Conc(C)) ⊆UC(Conc(A)) ∩UC(Conc(B)),
3. UC(Conc(B)) \ UC(Conc(A)) ⊆UC(Conc(C)) \ UC(Conc(A)),
then the following hold:
– sim(A, B) ≥sim(A, C).
(Dominance)
– If the inclusion in condition 2 is strict or, UC(Conc(A)) ∩UC(Conc(C)) ̸= ∅
and condition 3 is strict, then sim(A, B) > sim(A, C).
(Strict Dominance)
Example 7. Let A = ⟨{p∧q, r}, p∧q ∧r⟩, B = ⟨{p, p →q}, p∧q⟩, C = ⟨{p∧p →
q}, q⟩∈Arg(L). Their compiled arguments are:
– A∗= ⟨{p, q, r}, p ∧q ∧r⟩,
– B∗= ⟨{p, ¬p ∨q}, p ∧q⟩, and
– C∗= ⟨{p, ¬p ∨q}, q⟩.
Thanks to the compilation of arguments, a similarity measure sim which satisﬁes
the new principle of Strict Dominance, ensures that sim(A, B) > sim(A, C).
Theorem 3. The three novel extended jaccard similarity measures satisfy all
the principles.

Similarity Measures Based on Compiled Arguments
43
Table 1. Satisfaction of the principles of similarity measures
simσ
jac simσ
jac∗
simA
CR simU
CR
Minimality
•
•
•
•
Non-Zero
◦
•
•
•
Monotony
◦
•
•
•
Strict Monotony
◦
•
•
•
Dominance
◦
•
•
•
Strict Dominance ◦
•
•
•
The symbol • (resp. ◦) means the measure satisﬁes
(resp. violates) the principle.
Note that the use of compilation in conclusions provides more accurate syn-
tactic measures than in [11], as seen with the satisfaction of Strict Dominance
(Table 1).
Clearly, the original Jaccard measure is syntax-dependent and violates all
the principles except Minimality (because without common literals, the syntax
of the content does not matter).
Finally, we may also remark that the measure (Finally, we may also remark
that the measure (simσ
j∗) not taking into account concise arguments satisﬁes all
the principles. This is due to the fact that the compiled arguments belong to
the universe of possible arguments. We made this choice because a principle is
a mandatory property. Thanks to this condition we keep the application of the
principles general, by basing them on simσ
j∗) not taking into account concise
arguments satisﬁes all the principles. This is due to the fact that the compiled
arguments belong to the universe of possible arguments. We made this choice
because a principle is a mandatory property. Thanks to this condition we keep
the application of the principles general, by basing them on simple cases.
6
Conclusion
The paper further investigates the similarity between logical arguments. Based
on the observation that existing similarity measures are syntax-dependent,
they may provide inaccurate evaluations. We propose to compile the argu-
ments in order to make the existing principles and similarity measures syntax-
independent.
This work may be extended in several ways. The ﬁrst is to identify a principle,
or formal property, for distinguishing families of measures on concise arguments.
The second is to use the new measures to reﬁne argumentation systems that deal
with inconsistent information. The third is to study the notion of similarity for
other types of arguments, such as analogical arguments. The fourth is to study
the usefulness of compiled arguments to produce simple and accurate explana-
tions. Finally, we plan to study the notion of compiled arguments between other
types of argument relations such as attacks and supports.

44
L. Amgoud and V. David
References
1. Amgoud, L., Prade, H.: Using arguments for making and explaining decisions.
Artif. Intell. 173, 413–436 (2009)
2. Zhong, Q., Fan, X., Luo, X., Toni, F.: An explainable multi-attribute decision
model based on argumentation. Expert Syst. Appl. 117, 42–61 (2019)
3. Governatori, G., Maher, M., Antoniou, G., Billington, D.: Argumentation seman-
tics for defeasible logic. J. Log. Comput. 14(5), 675–702 (2004)
4. Garc´ıa, A., Simari, G.: Defeasible logic programming: an argumentative approach.
Theory Pract. Logic Program. 4(1–2), 95–138 (2004)
5. Sycara, K.: Persuasive argumentation in negotiation. Theor. Decis. 28, 203–242
(1990)
6. Hadidi, N., Dimopoulos, Y., Moraitis, P.: Argumentative alternating oﬀers. In:
Proceedings of the International Conference on Autonomous Agents and Multi-
agent Systems (AAMAS 2010), pp. 441–448 (2010)
7. Besnard, P., Hunter, A.: A logic-based theory of deductive arguments. Artif. Intell.
128(1–2), 203–235 (2001)
8. Amgoud, L., Besnard, P.: Logical limits of abstract argumentation frameworks. J.
Appl. Non-Classical Logics 23(3), 229–267 (2013)
9. Vesic, S.: Identifying the class of maxi-consistent operators in argumentation. J.
Artif. Intell. Res. 47, 71–93 (2013)
10. Amgoud, L., Ben-Naim, J.: Axiomatic foundations of acceptability semantics. In:
Proceedings of the Fifteenth International Conference on Principles of Knowledge
Representation and Reasoning KR, pp. 2–11 (2016)
11. Amgoud, L., David, V.: Measuring similarity between logical arguments. In: Pro-
ceedings of the Sixteenth International Conference on Principles of Knowledge
Representation and Reasoning KR, pp. 98–107 (2018)
12. Amgoud, L., Bonzon, E., Delobelle, J., Doder, D., Konieczny, S., Maudet, N.:
Gradual semantics accounting for similarity between arguments. In: Proceedings of
the Sixteenth International Conference on Principles of Knowledge Representation
and Reasoning KR, pp. 88–97 (2018)
13. Amgoud, L., David, V.: An adjustment function for dealing with similarities. In
Prakken, H., Bistarelli, S., Santini, F., Taticchi, C., (eds.) Computational Models
of Argument - Proceedings of COMMA 2020, Perugia, Italy, 4–11 September 2020.
Volume 326 of Frontiers in Artiﬁcial Intelligence and Applications, pp. 79–90. IOS
Press (2020)
14. Amgoud, L., David, V.: A general setting for gradual semantics dealing with sim-
ilarity. In: 35th AAAI Conference en Artiﬁcial Intelligence (AAAI 2021), Virtual
Conference, United States, AAAI: Association for the Advancement of Artiﬁcial
Intelligence. AAAI Press, February 2021
15. Amgoud, L., David, V., Doder, D.: Similarity measures between arguments revis-
ited. In: Kern-Isberner, G., Ognjanovi´c, Z. (eds.) ECSQARU 2019. LNCS (LNAI),
vol. 11726, pp. 3–13. Springer, Cham (2019). https://doi.org/10.1007/978-3-030-
29765-7 1
16. Lang, J., Liberatore, P., Marquis, P.: Propositional independence-formula-variable
independence and forgetting. J. Artif. Intell. Res. 18, 391–443 (2003)
17. Amgoud, L., Besnard, P., Vesic, S.: Equivalence in logic-based argumentation. J.
Appl. Non-Classical Logics 24(3), 181–208 (2014)
18. Jaccard, P.: Nouvelles recherches sur la distributions ﬂorale. Bulletin de la Soci´et´e
Vaudoise des Sciences Naturelles 37, 223–270 (1901)
19. Russell, S., Norvig, P.: Artiﬁcial Intelligence: A Modern Approach (1995)

Necessary and Suﬃcient Explanations for
Argumentation-Based Conclusions
AnneMarie Borg1(B)
and Floris Bex1,2
1 Department of Information and Computing Sciences, Utrecht University,
Utrecht, The Netherlands
{a.borg,f.j.bex}@uu.nl
2 Tilburg Institute for Law, Technology, and Society, Tilburg University,
Tilburg, The Netherlands
Abstract. In this paper, we discuss necessary and suﬃcient explana-
tions – the question whether and why a certain argument or claim can be
accepted (or not) – for abstract and structured argumentation. Given a
framework with which explanations for argumentation-based conclusions
can be derived, we study necessity and suﬃciency: what (sets of) argu-
ments are necessary or suﬃcient for the (non-)acceptance of an argument
or claim? We will show that necessary and suﬃcient explanations can be
strictly smaller than minimal explanations, while still providing all the
reasons for a conclusion and we discuss their usefulness in a real-life
application.
Keywords: Computational argumentation · Structured
argumentation · Explainable artiﬁcial intelligence
1
Introduction
In recent years, explainable AI (XAI) has received much attention, mostly
directed at new techniques for explaining decisions of (subsymbolic) machine
learning algorithms [18]. However, explanations traditionally also play an impor-
tant role in (symbolic) knowledge-based systems [10]. Computational argumen-
tation is one research area in symbolic AI that is frequently mentioned in relation
to XAI. For example, arguments can be used to provide reasons for or against
decisions [1,10,15]. The focus can also be on the argumentation itself, where it is
explained whether and why a certain argument or claim can be accepted under
certain semantics for computational argumentation [7–9,19]. It is the latter type
of explanations that is the subject of this paper.
Two central concepts in computational argumentation are abstract argumen-
tation frameworks [6] – sets of arguments and the attack relations between
them – and structured argumentation frameworks [3] – where arguments are con-
structed from a knowledge base and a set of rules and the attack relation is based
This research has been partly funded by the Dutch Ministry of Justice and the Dutch
National Police.
c
⃝Springer Nature Switzerland AG 2021
J. Vejnarov´a and N. Wilson (Eds.): ECSQARU 2021, LNAI 12897, pp. 45–58, 2021.
https://doi.org/10.1007/978-3-030-86772-0_4

46
A. Borg and F. Bex
on the individual elements in the arguments. The explanations framework that is
introduced in [5] is designed to provide explanations for the (non-)acceptance of
arguments and the claim of an argument in case of a structured setting. However,
like the other existing works on explanations for argumentation-based conclu-
sions, the framework does not account for ﬁndings from the social sciences on
human explanations [15].
One of the important characteristics of explanations provided by humans is
that they select the explanation from a possible inﬁnite set of explanations [15].
In this paper we look at how to select minimal,1 necessary and suﬃcient expla-
nations for the (non-)acceptance of an argument. To this end we will introduce
variations to the basic framework from [5] that will provide necessary or suﬃcient
explanations for both abstract and structured argumentation (i.e., ASPIC+[17]).
Intuitively, a necessary explanation contains the arguments that one has to
accept in order to accept the considered argument and a suﬃcient explanation
contains the arguments that, when accepted, guarantee the acceptance of the
considered argument. We will show that such explanations exist in most cases
and how these relate to the basic explanations from [5] as well as to minimal
explanations as introduced in [7]. Moreover, we will discuss a real-life application
from the Dutch National Police, where the necessary and suﬃcient explanations
will reduce the size of the provided explanations in a meaningful way.
The paper is structured as follows. We start with a short overview of related
work and present the preliminaries on abstract and structured argumentation
and the basic explanations from [5]. Then, in Sect. 4 we introduce necessary and
suﬃcient explanations and study how these relate to the better known minimal
explanations. In Sect. 5 we show how a real-life application beneﬁts from these
explanations and we conclude in Sect. 6.
2
Related Work
We are interested in local explanations for computational argumentation: expla-
nations for a speciﬁc argument or claim. We work here with the framework
from [5] for several reasons. Often, explanations are only deﬁned for a spe-
ciﬁc semantics [7,8] and can usually only be applied to abstract argumenta-
tion [8,11,19],2 while the framework from [5] can be applied on top of any argu-
mentation setting (structured or abstract) that results in a Dung-style argumen-
tation framework. Furthermore, when this setting is a structured one based on a
knowledge base and set of rules (like ASPIC+ or logic-based argumentation [3]),
the explanations can be further adjusted (something which is not considered at
all in the literature). To the best of our knowledge, this is the ﬁrst approach to
explanations for formal argumentation in which necessary and suﬃcient expla-
nations are considered and integrated into a real-life application.
1 Interpreting [15]’s simplicity as minimality.
2 These explanations do not account for the sub-argument relation in structured argu-
mentation. For example, in structured argumentation one cannot remove speciﬁc
arguments or attacks without inﬂuencing other arguments/attacks.

Necessary and Suﬃcient Explanations for Argumentation-Based Conclusions
47
3
Preliminaries
An abstract argumentation framework (AF) [6] is a pair AF = ⟨Args, Att⟩, where
Args is a set of arguments and Att ⊆Args × Args is an attack relation on these
arguments. An AF can be viewed as a directed graph, in which the nodes rep-
resent arguments and the arrows represent attacks between arguments.
Fig. 1. Graphical representation of the AF AF 1.
Example 1. Figure 1 represents AF1 = ⟨Args1, Att1⟩where Args1 = {A, B, C, D,
E, F} and Att1 = {(B, A), (C, B), (C, D), (D, C), (E, B), (E, F), (F, E)}.
Given an AF, Dung-style semantics [6] can be applied to it, to determine what
combinations of arguments (called extensions) can collectively be accepted.
Deﬁnition 1. Let AF = ⟨Args, Att⟩be an argumentation framework, S ⊆Args
a set of arguments and let A ∈Args. Then: S attacks A if there is an A′ ∈S such
that (A′, A) ∈Att, let S+ denote the set of arguments attacked by S; S defends A
if S attacks every attacker of A; S is conﬂict-free if there are no A1, A2 ∈S such
that (A1, A2) ∈Att; and S is an admissible extension (Adm) if it is conﬂict-free
and it defends all of its elements.
An admissible extension that contains all the arguments that it defends is a
complete extension (Cmp). The grounded extension (Grd) is the minimal (w.r.t.
⊆) complete extension; A preferred extension (Prf) is a maximal (w.r.t. ⊆) com-
plete extension; and A semi-stable extension (Sstb) is a complete extension for
which S ∪S+ is ⊆-maximal. Sem(AF) denotes the set of all the extensions of
AF under the semantics Sem ∈{Adm, Cmp, Grd, Prf, Sstb}.
In what follows we will consider an argument accepted if it is part of at least
one extension and non-accepted if it is not part of at least one extension.3
Deﬁnition 2. Where AF = ⟨Args, Att⟩is an AF, Sem a semantics such that
Sem(AF) ̸= ∅, it is said that A ∈Args is:
– accepted if A ∈ Sem(AF): there is some Sem-extension that contains A;
– non-accepted if A /∈ Sem(AF): A is not part of at least one Sem-extension.
Example 2. For AF1 we have that Grd(AF1) = {∅} and there are four preferred
and semi-stable extensions: {A, C, E}, {A, C, F}, {A, D, E} and {B, D, F}.
Therefore, all arguments from Args1 are accepted and non-accepted for Sem ∈
{Prf, Sstb}.
3 In [5], four acceptance strategies are considered, the other two are not relevant for
our study on necessity and suﬃciency and are therefore not introduced here.

48
A. Borg and F. Bex
3.1
ASPIC+
For our discussion on explanations for structured settings we take ASPIC+ [17]
which allows for two types of premises – axioms that cannot be questioned and
ordinary premises that can be questioned – and two types of rules – strict rules
that cannot be questioned and defeasible ones. We choose ASPIC+ since it allows
to vary the form of the explanations in many ways (see Sect. 3.2 and [5]).
An ASPIC+ setting starts from an argumentation system (AS = ⟨L, R, n⟩),
which contains a logical language L closed under negation (¬), a set of rules
R = Rs ∪Rd consisting of strict (Rs) and defeasible (Rd) rules and a naming
convention n for defeasible rules. Arguments are constructed in an argumentation
setting from a knowledge base K ⊆L which consists of two disjoint subsets
K = Kp ∪Kn: the set of axioms (Kn) and the set of ordinary premises (Kp).
Deﬁnition 3. An argument A on the basis of a knowledge base K in an argu-
mentation system ⟨L, R, n⟩is:
1. φ if φ ∈K, where Prem(A) = Sub(A) = {φ}, Conc(A) = φ, Rules(A) = ∅and
TopRule(A) = undeﬁned;
2. A1, . . . , An ⇝ψ, where ⇝∈{→, ⇒}, if A1, . . . , An are arguments such that
there exists a rule Conc(A1), . . . , Conc(An) ⇝ψ in Rs if ⇝= →and in Rd
if ⇝= ⇒.
Prem(A) = Prem(A1) ∪. . . ∪Prem(An); Conc(A) = ψ; Sub(A) = Sub(A1) ∪
. . .∪Sub(An)∪{A}; Rules(A) = Rules(A1)∪. . .∪Rules(An)∪{Conc(A1), . . . ,
Conc(An) ⇝ψ}; DefRules(A) = {r ∈Rd | r ∈Rules(A)}; and TopRule(A) =
Conc(A1), . . . , Conc(An) ⇝ψ.
The above notation can be generalized to sets. For example, where S is a set of
arguments Prem(S) = {Prem(A) | A ∈S} and Conc(S) = {Conc(A) | A ∈S}.
Attacks on an argument are based on the rules and premises applied in the
construction of that argument.
Deﬁnition 4. Let A and B be two arguments, we denote ψ = −φ if ψ = ¬φ or
φ = ¬ψ. A attacks an argument B iﬀA undercuts, rebuts or undermines B:
– A undercuts B (on B′) iﬀConc(A) = −n(r) for some B′ ∈Sub(B) such that
B′’s top rule r is defeasible, it denies a rule;
– A rebuts B (on B′) iﬀConc(A) = −φ for some B′ ∈Sub(B) of the form
B′′
1 , . . . , B′′
n ⇒φ, it denies a conclusion;
– A undermines B (on φ) iﬀConc(A) = −φ for some φ ∈Prem(B) \ Kn, it
denies a premise.
Argumentation theories and their corresponding Dung-style argumentation
frameworks can now be deﬁned.
Deﬁnition 5. An argumentation theory is a pair AT = ⟨AS, K⟩, where AS is
an argumentation system and K is a knowledge base.
From an argumentation theory AT the corresponding AF can be derived such
that AF(AT) = ⟨Args, Att⟩, where Args is the set of arguments constructed from
AT and (A, B) ∈Att iﬀA, B ∈Args and A attacks B as deﬁned in Deﬁnition 4.

Necessary and Suﬃcient Explanations for Argumentation-Based Conclusions
49
Example 3. Let AS1 = ⟨L1, R1, n⟩where the rules in R1 are such that, with
K1 = K1
n = {r, s, t, v} the following arguments can be derived:4
A : s, t
d1
⇒u
B : p, ¬q
d2
⇒¬n(d1)
C : r, s
d3
⇒q
D : v
d4
⇒¬q
E : r, t
d5
⇒¬p
F : v
d6
⇒p.
The graphical representation of the corresponding argumentation framework
AF(AT1) with AT1 = ⟨AS1, K1⟩is the graph from Fig. 1.
Dung-style semantics (Deﬁnition 1) can be applied in the same way as they
are applied to abstract argumentation frameworks (recall Example 2). In addi-
tion to (non-)acceptance of arguments, in a structured setting we can also con-
sider (non-)acceptance of formulas:
Deﬁnition 6. Let AF(AT) = ⟨Args, Att⟩be an AF, based on AT, let Sem be a
semantics such that Sem(AF) ̸= ∅and let φ ∈L. Then φ is:
– accepted: if φ ∈ Concs(Sem(AF(AT))), that is: there is some argument
with conclusion φ that is accepted;
– non-accepted: if φ /∈ Concs(Sem(AF(AT))), that is: there is some Sem-
extension without an argument with conclusion φ.
Example 4. As was the case for arguments (recall Example 2), all formulas in
{p, ¬p, q, ¬q, r, s, t, u, v} are accepted and {p, ¬p, q, ¬q, u} are also non-accepted.
3.2
Basic Explanations
In [5] four types of explanations for abstract and structured argumentation were
introduced. These explanations are deﬁned in terms of two functions: D, which
determines the arguments that are in the explanation and F, which determines
what elements of these arguments the explanation presents. For the basic expla-
nations in this paper, we instantiate D with the following functions, let A ∈Args
and E ∈Prf(AF) for some AF AF = ⟨Args, Att⟩:5
– Defending(A) = {B ∈Args | B defends A} denotes the set of arguments
that defend A and Defending(A, E) = Defending(A) ∩E denotes the set of
arguments that defend A in E.
– NoDefAgainst(A, E) = {B ∈Args | B attacks A and E does not defend A
against B} denotes the set of all attackers of A that are not defended by E.
The explanations are deﬁned for arguments and formulas.
4 We ignore the arguments based on the elements from K1, since these neither attack
nor are attacked by any argument.
5 We write that B ∈Args defends A ∈Args if it attacks an attacker of A or it defends
an argument that defends A.

50
A. Borg and F. Bex
Deﬁnition 7. Let AF = ⟨Args, Att⟩be an AF and suppose that A ∈Args [resp.
φ ∈L] is accepted w.r.t. Sem. Then:
SemAcc(A) = {Defending(A, E) | E ∈Sem(AF) and A ∈E}.
SemAcc(φ) = {F(Defending(A, E)) | E ∈Sem(AF) such that A ∈E and
Conc(A) = φ} .
An acceptance explanation, for an argument or formula, contains all the argu-
ments that defend the argument (for that formula) in an extension. If it is an
explanation for a formula, the function F can be applied to it.
Deﬁnition 8. Let AF = ⟨Args, Att⟩be an AF and suppose that A ∈Args [resp.
φ ∈L] is non-accepted w.r.t. Sem. Then:
SemNotAcc(A) =

E∈Sem(AF) and A/∈E
NoDefAgainst(A, E).
SemNotAcc(φ) =

A∈Args and Conc(A)=φ

E∈Sem(AF) and A/∈E
F(NoDefAgainst(A, E)).
A non-acceptance explanation contains all the arguments that attack the argu-
ment [resp. an argument for the formula] and to which no defense exists in some
Sem-extension. For a formula F can be applied again.
The function F can be instantiated in diﬀerent ways. We recall here some of
the variations introduced in [5].
– F = id, where id(S) = S. Then explanations are sets of arguments.
– F = Prem. Then explanations only contain the premises of arguments (i.e.,
knowledge base elements).
– F = AntTop, where AntTop(A) = ⟨TopRule(A), Ant(TopRule(A))⟩. Then
explanations contain the last applied rule and its antecedents.
– F = SubConc, where SubConc(A) = {Conc(B) | B ∈Sub(A), Conc(B) /∈
K∪{Conc(A)}}. Then the explanation contains the sub-conclusions that were
derived in the construction of the argument.
Example 5. Consider the AF AF(AT1) from Examples 1 and 3. We have that:
– PrfAcc(A) ∈{{C}, {E}, {C, E}} and PrfAcc(B) = {D, F};
– PrfNotAcc(A) = {B, D, F} and PrfNotAcc(B) = {C, E}.
If
we
take
F
=
Prem,
then:
PrfAcc(u)
∈
{{r, s}, {r, t}, {r, s, t}},
PrfAcc(¬n(d1)) = PrfNotAcc(u) = {v} and PrfNotAcc(¬n(d1)) = {r, s, t}.
A conclusion derived from an argumentation system can have many causes
and therefore many explanations. When humans derive the same conclusion and
are asked to explain that conclusion they are able to select the explanation from
all the possible explanations. In the social sciences a large amount of possible
selection criteria that humans might apply have been investigated, see [15] for
an overview. In this paper we focus on necessity and suﬃciency.

Necessary and Suﬃcient Explanations for Argumentation-Based Conclusions
51
4
Necessity and Suﬃciency
Necessity and suﬃciency in the context of philosophy and cognitive science are
discussed in, for example, [13,14,20]. Intuitively, an event Γ is suﬃcient for Δ,
if no other causes are required for Δ to happen, while Γ is necessary for Δ,
if in order for Δ to happen, Γ has to happen as well. In the context of logical
implication (denoted by →), one could model suﬃciency by Γ →Δ and necessity
by Δ →Γ [12].
In the next sections we formulate these logical notions in our argumentation
setting. We will assume that the arguments on which the explanation for an
argument A is based are relevant for A: B ∈Args [resp. S ⊆Args] is relevant for
A if B (in)directly attacks or defends A (i.e., there is a path from B to A) and
does not attack itself [resp. for each C ∈S, C is relevant for A].
4.1
Necessity and Suﬃciency for Acceptance
In the context of argumentation, a set of accepted arguments is suﬃcient if it
guarantees, independent of the status of other arguments, that the considered
argument is accepted, while an accepted argument is necessary if it is impossible
to accept the considered argument without it.
Deﬁnition 9. Let AF = ⟨Args, Att⟩be an AF and let A ∈Args be accepted
(w.r.t. some Sem). Then:
– S ⊆Args is suﬃcient for the acceptance of A if S is relevant for A, S is
conﬂict-free and S defends A against all its attackers;
– B ∈Args is necessary for the acceptance of A if B is relevant for A and if
B /∈E for some E ∈Adm(AF), then A /∈E.
We denote by Suﬀ(A) = {S ⊆Args | S is suﬃcient for the acceptance of A}
the set of all suﬃcient sets of arguments for the acceptance of A and by
Nec(A) = {B ∈Args | B is necessary for the acceptance of A} the set of all
necessary arguments for the acceptance of A.
Example 6. In AF1 both {C} and {E} are suﬃcient for the acceptance of A but
neither is necessary, while for B, {D, F} is suﬃcient and D and F are necessary.
Necessary and suﬃcient explanations are now deﬁned by replacing Defending
in the basic explanations from Sect. 3.2 with Nec resp. Suﬀ.
Deﬁnition 10. Let AF = ⟨Args, Att⟩be an AF and let A ∈Args [resp. φ ∈L]
be accepted. Then suﬃcient explanations are deﬁned by:
– Acc(A) ∈Suﬀ(A);
– Acc(φ) ∈{F(Suﬀ(A)) | A ∈Args and Conc(A) = φ}.
Necessary explanations are deﬁned by:
– Acc(A) = Nec(A);

52
A. Borg and F. Bex
– Acc(φ) = {F(Suﬀ(A)) | A ∈Args and Conc(A) = φ}.
Example 7. For AF(AT1) we have that suﬃcient explanations are Acc(A) ∈
{{C}, {E}, {C, E}, {C, F}, {D, E}}, Acc(B) = {D, F}, Acc(u) ∈{{r, s}, {r, t},
{r, s, t}} and Acc(¬n(d1)) = {v}. Moreover, necessary explanations are Acc(A) =
∅, Acc(B) = {D, F}, Acc(u) = {r} and Acc(¬n(d1)) = {v}.
Next we show that the sets in Suﬀ(A) are admissible and contain all the
needed arguments. Additionally, we look at conditions under which Suﬀand
Nec are empty, as well as the relation between Suﬀand Nec. These last results
provide the motivation for the necessary formula acceptance explanation.6
Proposition 1. Let AF = ⟨Args, Att⟩be an AF and let A ∈Args be accepted
w.r.t. some Sem ∈{Adm, Cmp, Grd, Prf, Sstb}. Then:
1. For all S ∈Suﬀ(A), {S, S ∪{A}} ⊆Adm(AF);
2. Suﬀ(A) = ∅iﬀthere is no B ∈Args such that (B, A) ∈Att.
3. Nec(A) = ∅iﬀthere is no B ∈Args such that (B, A) ∈Att or  Suﬀ(A) = ∅.
4. Nec(A) ⊆ Suﬀ(A).
The next proposition relates the introduced notions of necessity and suﬃ-
ciency with Defending and therefore with the basic explanations from Sect. 3.2.
Proposition 2. Let AF = ⟨Args, Att⟩be an AF and let A ∈Args be accepted
w.r.t. Sem ∈{Adm, Cmp, Grd, Prf, Stb}. Then:
– for all E ∈Sem(AF) such that A ∈E, Defending(A, E) ∈Suﬀ(A);
– 
E∈Sem(AF) and A∈E Defending(A, E) = Nec(A).
4.2
Necessity and Suﬃciency for Non-acceptance
When looking at the non-acceptance of an argument A, the acceptance of any of
its direct attackers is a suﬃcient explanation. However, other arguments (e.g.,
some of the indirect attackers) might be suﬃcient as well. An argument is nec-
essary for the non-acceptance of A, when it is relevant and A is accepted in
the argumentation framework without it. In what follows we will assume that
(A, A) /∈Att, since otherwise A itself is the reason for its non-acceptance.
We need the following deﬁnition for our notion of suﬃciency.
Deﬁnition 11. Let AF = ⟨Args, Att⟩be an AF and let A, B ∈Args such that A
indirectly attacks B, via C1, . . . , Cn ∈Args, i.e., (A, C1), (C1, C2), . . . , (Cn, B) ∈
Att. It is said that the attack from A on B is uncontested if there is no D ∈Args
such that (D, C2i) ∈Att for i ∈{1, . . . , n
2 }. It is contested otherwise, in which
case it is said that the attack from A is contested in C2i.
The need for the above deﬁnition is illustrated in the next example:
6 Full proofs of our results can be found in the online technical appendix at: https://
nationaal-politielab.sites.uu.nl/necessary-suﬃcient-explanations-proofs/.

Necessary and Suﬃcient Explanations for Argumentation-Based Conclusions
53
Example 8. In AF1, the indirect attacks from D and F on A are contested: the
attack from D is contested in B, since (E, B) ∈Att and the attack from F is
also contested in B since (C, B) ∈Att. It is therefore possible that A and D or
F are part of the same extension (recall Example 2).
For the deﬁnition of necessity for non-acceptance we deﬁne subframeworks,
which are needed because an argument might be non-accepted since it is attacked
by an accepted or by another non-accepted argument.7
Deﬁnition 12. Let AF = ⟨Args, Att⟩be an AF and let A ∈Args. Then AF↓A =
⟨Args \ {A}, Att ∩(Args \ {A} × Args \ {A})⟩denotes the AF based on AF but
without A.
Since indirect attacks might be suﬃcient for not accepting an argument, but
they also might be contested, the deﬁnition of suﬃciency for non-acceptance is
deﬁned inductively.
Deﬁnition 13. Let AF = ⟨Args, Att⟩be an AF and let A ∈Args be non-accepted
(w.r.t. Sem). Then:
– S ⊆Args is suﬃcient for the non-acceptance of A if S is relevant for A and
there is a B ∈S such that:
• (B, A) ∈Att; or
• B indirectly attacks A and that attack is uncontested; or
• B indirectly attacks A and for every argument C in which the attack from
B on A is contested and every D ∈Args such that (D, C) ∈Att, there is
an S′ ⊆S that is suﬃcient for the non-acceptance of D.
– B ∈Args is necessary for the non-acceptance of A if B is relevant for A and
A is accepted w.r.t. Sem in AF↓B.
We denote by SuﬀNot(A) = {S ⊆Args | S is suﬃcient for the non-acceptance
of A} the set of all suﬃcient sets of arguments for the non-acceptance of A and
by NecNot(A) = {B ∈Args | B is necessary for the non-acceptance of A} the
set of all necessary arguments for the non-acceptance of A.
Example 9. For AF1 from Example 1 we have that B is both necessary and
suﬃcient for the non-acceptance of A. Moreover, while D and F are neither
suﬃcient for the non-acceptance of A, {D, F} is. For the non-acceptance of B
we have that C and E are suﬃcient, but neither is necessary.
Deﬁnition 14. Let AF = ⟨Args, Att⟩be an AF and let A ∈Args [resp. φ ∈L]
be non-accepted. Then suﬃcient explanations are deﬁned by:
– NotAcc(A) ∈SuﬀNot(A);
– NotAcc(φ) ∈{F(SuﬀNot(A)) | A ∈Args and Conc(A) = φ}.
7 In terms of labeling semantics (see e.g., [2]) an argument is non-accepted if it is out
(i.e., attacked by an in argument) or undecided.

54
A. Borg and F. Bex
Necessary explanations are deﬁned by:
– NotAcc(A) = NecNot(A);
– NotAcc(φ) =  {F(SuﬀNot(A)) | A ∈Args and Conc(A) = φ}.
Example 10. For AF1
we have, for suﬃciency NotAcc(A)
∈
{{B}, {D,
F}, {B, D, F}}, NotAcc(B)
∈
{{C}, {E}, {C, E}}, NotAcc(u)
=
{v} and
NotAcc(¬n(d1)) ∈{{r, s}, {r, t}, {r, s, t}} and for necessity NotAcc(B) = ∅and
NotAcc(u) = {v}.
The next propositions are the non-acceptance counterparts of Propositions 1
and 2. First some basic properties of suﬃciency and necessity for non-acceptance.
Proposition 3. Let AF = ⟨Args, Att⟩be an AF and let A ∈Args be non-
accepted w.r.t. Sem ∈{Adm, Cmp, Grd, Prf, Sstb}. Then: SuﬀNot(A) ̸= ∅; and
NecNot(A) = ∅implies that there are at least two direct attackers of A.
Now we show how NoDefAgainst (and hence the basic explanations from
Sect. 3.2) is related to our notions of suﬃciency and necessity for non-acceptance.
Proposition 4. Let AF = ⟨Args, Att⟩be an AF and let A ∈Args be an argu-
ment that is not accepted w.r.t. Sem ∈{Cmp, Grd, Prf, Sstb}. Then:
– for all E ∈Sem(AF) such that A /∈E, NoDefAgainst(A, E) ∈SuﬀNot(A);
– NecNot(A) ⊆
E∈Sem(AF) and A/∈E NoDefAgainst(A, E).
4.3
Necessity, Suﬃciency and Minimality
In this paper we have introduced necessity and suﬃciency to reduce the size of
an explanation. More common in the literature is to place a minimality condition
on the explanation [7,8]. In this section we show that our notions of necessity
and suﬃciency result in explanations that do not contain more arguments than
the minimal explanations from [7]. To this end we introduce, for ⪯∈{⊆, ≤}:
– MinDefending⪯(A, E) = {S ∈Defending(A, E) | ∄S′ ∈Defending(A, E) such
that S′ ⪯S} denotes the ⪯-minimal Defending(A, E) sets.8
– MinNotDefAgainst⪯(A, E) = {S ∈NoDefAgainst(A, E) | there is no S′ ∈
NoDefAgainst(A, E) such that S′ ⪯S}, denotes the set with all ⪯-minimal
NoDefAgainst(A, E) sets.
– MinSuﬀ⪯(A) = {S ∈Suﬀ(A) | ∄S′ ∈Suﬀ(A) such that S′ ⪯S}, denotes the
set of all ⪯-minimally suﬃcient sets for the acceptance of A.
– MinSuﬀNot⪯(A) = {S ∈SuﬀNot(A) | ∄S′ ∈SuﬀNot(A) such that S′ ⪯S},
denotes the set of all ⪯-minimally SuﬀNot sets for the non-acceptance of A.
The next example shows that suﬃcient explanations can be smaller than
minimal basic explanations.
8 In view of the result in [5], these sets correspond to the minimal (⪯= ≤and where
S ≤S′ denotes |S| ≤|S′|) and compact (⪯= ⊆) explanations from [7].

Necessary and Suﬃcient Explanations for Argumentation-Based Conclusions
55
Example 11. Let AF2 = ⟨Args2, Att2⟩, shown in Fig. 2. Here we have that
Prf(AF2) = {{A, B}, {C, D}} and that PrfAcc(B) = {A, B}, PrfAcc(D) = {C,
D}, PrfNotAcc(B) = {C, D} and PrfNotAcc(D) = {A, B}. These are the expla-
nations for B and D, whether as deﬁned in Sect. 3.2 or with Defending [resp.
NoDefAgainst] replaced by MinDefending [resp. MinNotDefAgainst].
Fig. 2. Graphical representation of AF 2.
When looking at minimally suﬃcient sets instead, we have that Acc(B) =
{A} and NotAcc(D) ∈{{A}, {B}}. To see that these explanations are still mean-
ingful, note that A defends B against all of its attackers and as soon as A is
accepted under complete semantics, B will be accepted as well. Thus, the mini-
mally suﬃcient explanations for B and D are ≤- and ⊆-smaller than the minimal
basic explanations for B and D, but still meaningful.
That minimally suﬃcient explanations can be smaller than minimal expla-
nations is formalized in the next propositions.
Proposition 5. Let AF = ⟨Args, Att⟩be an AF, let A ∈Args be accepted w.r.t.
Sem ∈{Adm, Cmp, Grd, Prf, Sstb} and let ⪯∈{⊆, ≤}. Then:
– for all E ∈Sem(AF) and all S ∈MinDefending⪯(A, E) there is some S′ ∈
MinSuﬀ⪯(A) such that S′ ⪯S;
– for all E ∈Adm(AF) and all S ∈MinSuﬀ⪯(A) also S ∈MinDefending⪯(A, E);
– for all E ∈Sem(AF) and all S ∈MinDefending⪯(A, E), Nec(A) ⊆S.
Proposition 6. Let AF = ⟨Args, Att⟩be an AF and let A ∈Args be not accepted
w.r.t. Sem ∈{Cmp, Grd, Prf, Sstb}. Then:
– for all E ∈Sem(AF) and all S ∈MinNotDefAgainst⪯(A, E), there is some
S′ ∈MinSuﬀNot⪯(A) such that S′ ⪯S;
– for all E ∈Sem(AF) and all S ∈MinNotDefAgainst⪯(A, E), NecNot(A) ⊆S.
5
Applying Necessity and Suﬃciency
At the Dutch National Police several argumentation-based applications have
been implemented [4]. These applications are aimed at assisting the police at
working through high volume tasks, leaving more time for tasks that require
human attention. In this section we illustrate how necessity and suﬃciency can
be applied in the online trade fraud application from [16].
Consider the following language L3: the complainant delivered (cd), the coun-
terparty delivered (cpd); the received product seems fake (fake); a package is

56
A. Borg and F. Bex
expected (pex); the complainant waited before ﬁling the complaint (wait); the
received packages is indeed fake (recfake); the delivery may still arrive (deco); it
is a case of fraud (f ); and their negations. Based on Dutch Criminal Law (i.e.,
Article 326) we can derive the following arguments:
A1 : cpd
A2 : ¬cpd
A3 : fake
A4 : ¬fake
A5 : pex
A6 : ¬pex
A7 : wait
A8 : ¬wait
A9 : cd
A10 : ¬cd
B1 : A1, A3 ⇒recfake
B2 : A2, A6 ⇒¬deco
B3 : A2, A5, A7 ⇒¬deco
B4 : A5, A8 ⇒deco
C1 : A9, B1 ⇒f
C2 : A2, A9, B2 ⇒f
C3 : A2, A9, B3 ⇒f
C4 : A9, B4 ⇒¬f
C5 : A4, A9 ⇒¬f
C6 : A10 ⇒¬f.
The above arguments are only a small subset of the possible arguments in the
actual application, yet this framework already results in 30 preferred and semi-
stable extensions. We can therefore not provide a detailed formal analysis. How-
ever, we can already show the usefulness of necessary and suﬃcient explanations.
The necessary explanation for the acceptance of f is cd, while for the accep-
tance of ¬f the necessary explanation is empty. The reason for this is that, by
Article 326, the complainant must have delivered (e.g., sent the goods or money)
before it is a case of fraud but ¬f can be accepted for a variety of reasons. In the
basic explanations it is not possible to derive this explanation, yet it can be the
sole reason for not accepting f. Moreover, minimal suﬃcient explanations for the
acceptance of ¬f when cd and F = Prem are {cd, pex, ¬wait} and {cd, ¬fake},
these are both ⊂and <-smaller than any basic explanation for the acceptance
of ¬f, while still providing the main reasons for the acceptance of ¬f.
Therefore, with necessary and suﬃcient explanations, we can provide com-
pact explanations that only contain the core reasons for a conclusion, something
which is not possible with the (minimal) explanations from the basic framework.
6
Conclusion
We have discussed how the explanations from the basic framework introduced
in [5] can be adjusted to account for ﬁndings from the social sciences on necessary
and suﬃcient explanations [13,14,20]. To this end we have introduced necessary
and suﬃcient sets of arguments for the (non-)acceptance of an argument and
formula and integrated these into the explanations deﬁnition. The result is a
meaningful reduction in the size of an explanation, which almost always exists.
Moreover, we have shown that our necessary and suﬃcient explanations can be
smaller than the minimal explanations from [7] and reduce the explanations to
the core reasons of (not) accepting a conclusion in a real-life application.
To the best of our knowledge this is the ﬁrst investigation into necessary
and suﬃcient sets for (non-)acceptance of arguments, especially in the context
of integrating ﬁndings from the social sciences (e.g., [12]) into (explanations for)
argumentation-based conclusions and into a real-life application. In future work
we plan to investigate how to integrate further ﬁndings, such as contrastiveness
and other selection mechanisms.

Necessary and Suﬃcient Explanations for Argumentation-Based Conclusions
57
References
1. Atkinson, K., et al.: Towards artiﬁcial argumentation. AI Mag. 38(3), 25–36 (2017)
2. Baroni, P., Caminada, M., Giacomin, M.: Abstract argumentation frameworks and
their semantics. In: Baroni, P., Gabay, D., Giacomin, M., van der Torre, L. (eds.)
Handbook of Formal Argumentation, pp. 159–236. College Publications (2018)
3. Besnard, P., et al.: Introduction to structured argumentation. Argum. Comput.
5(1), 1–4 (2014)
4. Bex, F., Testerink, B., Peters, J.: AI for online criminal complaints: from natural
dialogues to structured scenarios. In: Workshop Proceedings of Artiﬁcial Intelli-
gence for Justice at ECAI 2016, pp. 22–29 (2016)
5. Borg, A., Bex, F.: A basic framework for explanations in argumentation. IEEE
Intell. Syst. 36(2), 25–35 (2021)
6. Dung, P.M.: On the acceptability of arguments and its fundamental role in non-
monotonic reasoning, logic programming and n-person games. Artif. Intell. 77(2),
321–357 (1995)
7. Fan, X., Toni, F.: On computing explanations in argumentation. In: Bonet, B.,
Koenig, S. (eds.) Proceedings of the 29th AAAI Conference on Artiﬁcial Intelli-
gence (AAAI 2015), pp. 1496–1502. AAAI Press (2015)
8. Fan, X., Toni, F.: On explanations for non-acceptable arguments. In: Black, E.,
Modgil, S., Oren, N. (eds.) TAFA 2015. LNCS (LNAI), vol. 9524, pp. 112–127.
Springer, Cham (2015). https://doi.org/10.1007/978-3-319-28460-6 7
9. Garc´ıa, A., Ches˜nevar, C., Rotstein, N., Simari, G.: Formalizing dialectical expla-
nation support for argument-based reasoning in knowledge-based systems. Expert
Syst. Appl. 40(8), 3233–3247 (2013)
10. Lacave, C., Diez, F.J.: A review of explanation methods for heuristic expert sys-
tems. Knowl. Eng. Rev. 19(2), 133–146 (2004)
11. Liao, B., van der Torre, L.: Explanation semantics for abstract argumentation.
In: Prakken, H., Bistarelli, S., Santini, F., Taticchi, C. (eds.) Proceedings of the
8th International Conference on Computational Models of Argument (COMMA
2020). Frontiers in Artiﬁcial Intelligence and Applications, vol. 326, pp. 271–282.
IOS Press (2020)
12. Lin, F.: On strongest necessary and weakest suﬃcient conditions. Artif. Intell.
128(1), 143–159 (2001)
13. Lipton, P.: Contrastive explanation. R. Inst. Philos. Suppl. 27, 247–266 (1990)
14. Lombrozo, T.: Causal-explanatory pluralism: how intentions, functions, and mech-
anisms inﬂuence causal ascriptions. Cogn. Psychol. 61(4), 303–332 (2010)
15. Miller, T.: Explanation in artiﬁcial intelligence: insights from the social sciences.
Artif. Intell. 267, 1–38 (2019)
16. Odekerken, D., Borg, A., Bex, F.: Estimating stability for eﬃcient argument-based
inquiry. In: Prakken, H., Bistarelli, S., Santini, F., Taticchi, C. (eds.) Proceedings of
the 8th International Conference on Computational Models of Argument (COMMA
2020). Frontiers in Artiﬁcial Intelligence and Applications, vol. 326, pp. 307–318.
IOS Press (2020)
17. Prakken, H.: An abstract framework for argumentation with structured arguments.
Argum. Comput. 1(2), 93–124 (2010)
18. Samek, W., Wiegand, T., M¨uller, K.R.: Explainable artiﬁcial intelligence: under-
standing, visualizing and interpreting deep learning models. arXiv preprint
arXiv:1708.08296 (2017)

58
A. Borg and F. Bex
19. Saribatur, Z., Wallner, J., Woltran, S.: Explaining non-acceptability in abstract
argumentation. In: Proceedings of the 24th European Conference on Artiﬁcial Intel-
ligence (ECAI 2020). Frontiers in Artiﬁcial Intelligence and Applications, vol. 325,
pp. 881–888. IOS Press (2020)
20. Woodward, J.: Sensitive and insensitive causation. Philos. Rev. 115(1), 1–50 (2006)

Addressing Popular Concerns Regarding
COVID-19 Vaccination with Natural
Language Argumentation Dialogues
Lisa Chalaguine(B) and Anthony Hunter(B)
Department of Computer Science, University College London, London, UK
ucablc3@ucl.ac.uk, anthony.hunter@ucl.ac.uk
Abstract. Chatbots have the potential of being used as dialogical argu-
mentation systems for behaviour change applications. They thereby oﬀer
a cost-eﬀective and scalable alternative to in-person consultations with
health professionals that users could engage in from the comfort of their
own home. During events like the global COVID-19 pandemic, it is even
more important than usual that people are well informed and make con-
scious decisions that beneﬁt themselves. Getting a COVID-19 vaccine is
a prime example of a behaviour that beneﬁts the individual, as well as
society as a whole. In this paper, we present a chatbot that engages in
dialogues with users who do not want to get vaccinated, with the goal
to persuade them to change their stance and get a vaccine. The chatbot
is equipped with a small repository of arguments that it uses to counter
user arguments on why the user is reluctant to get a vaccine. We evaluate
our chatbot in a study with participants.
Keywords: Chatbots · Argumentative persuasion systems ·
Computational persuasion · Natural language argumentation ·
Knowledge base construction
1
Introduction
During events like the global COVID-19 pandemic, it is even more important
than usual that people are well informed and make conscious decisions that ben-
eﬁt themselves and society. One such example is the willingness to get a vaccine.
Vaccines have historically proven to be highly successful and cost-eﬀective public
health tools for disease prevention [19]. But the eﬀectiveness of a vaccine in con-
trolling the spread of COVID-19 depends on the willingness to get vaccinated in
the general population. A suﬃciently high vaccine coverage may generate herd
immunity, which will protect everyone, including those particularly susceptible
to the virus [11]. However, a barrier to reaching herd immunity is the preva-
lence of people who refuse or are hesitant to take vaccines [14,20]. For example,
the most recent numbers from YouGov surveys on vaccine hesitancy from late
March 2021 show that whereas the numbers in the UK are quite high (around
c
⃝Springer Nature Switzerland AG 2021
J. Vejnarov´a and N. Wilson (Eds.): ECSQARU 2021, LNAI 12897, pp. 59–73, 2021.
https://doi.org/10.1007/978-3-030-86772-0_5

60
L. Chalaguine and A. Hunter
86%), the numbers in, for example, neighbouring France are much lower, at only
49%. In the USA they are a bit higher, at around 59% [1]. Interviewing all those
people who refuse to vaccinate in person and trying to convince them to get the
vaccine, would be an impossible task.
This problem can be tackled as an argumentation problem: Arguments can be
used to provide information and overturn misconceptions [13]. They are an essen-
tial part of sensible discussions on controversial and problematic topics. However,
despite an increasing body of literature on computational models of argument,
there is still a lack of practical applications. Conversational agents, also known as
chatbots, have the potential of being used as dialogical argumentation systems
for behaviour change applications by applying computational models of argu-
ment. A chatbot could engage in an argumentative dialogue with people over
the internet from the comfort and safety of their own home, trying to persuade
them to get a COVID-19 vaccine. Chatbots could thereby oﬀer a cost-eﬀective
and scalable alternative to in-person consultations with health professionals. In
order to represent arguments and concerns in the chatbot’s knowledge base, we
introduce a variant of an argument graph called a concern-argument graph.
The problem with controversial topics like the COVID-19 vaccine is that
people block out information they disagree with: through creating social media
echo chambers, reading partisan news, or only surrounding themselves with
like-minded people. A recent paper [16] found that people select more belief-
consistent information and perceive belief-conﬁrming information as more cred-
ible, useful, and convincing when searching for online health information, also
known as conﬁrmation bias. Therefore, a dialogue with someone (or something
- like a chatbot) could expose people to new information and potentially have a
positive eﬀect on their decision-making process.
However, diﬀerent people worry about diﬀerent things and hence arguments
for not getting a COVID-19 vaccine will vary in a population. One person might
be worried about the potential side eﬀects of a newly developed vaccine, whereas
someone else might think that he or she does not need a vaccine because they are
young and healthy. A chatbot could address those diﬀerent concerns by providing
counterarguments tailored to the diﬀerent user arguments and during the course
of an argumentative dialogue, try to persuade the user to change their stance
about getting vaccinated.
Whilst this seems like an obvious idea, there is a lack of a general framework
for using concerns in making strategic choices of move in dialogical argumenta-
tion systems. A concern is not the same as a value in value-based argumentation
frameworks [4,5], since values are used for ignoring attacks by counterarguments
where the value of the attacking argument is lower ranked than the attacked one
[12]. In our case, however, the chatbot’s counterargument addresses the same
concern as the user’s argument, instead of presenting an argument that raises
an opposing value.
In this paper, we present a chatbot that engages in persuasive dialogues with
users who are reluctant to get a COVID-19 vaccine. We show that given a novel
domain, like the COVID-19 pandemic and the associated vaccine development, it

Addressing Popular Concerns Regarding COVID-19 Vaccination
61
requires a relatively small repository of counterarguments to address the major-
ity of possible arguments people might have for not getting the vaccine.
The rest of the paper is structured as follows: Sect. 2 gives some background
theory on concerns and the use of argument graphs to construct a chatbot knowl-
edge base; Sect. 3 gives the aim of the paper and the hypotheses; Sect. 4 describes
the chatbot architecture that was used for the experiments; Sect. 5 describes the
experiments that were conducted with the chatbot, Sect. 6 presents the results,
and in Sect. 7 we discuss and conclude our ﬁndings.
2
Conceptualising Argumentation
2.1
Using Arguments and Concerns for a Persuasive Chatbot
In order for the chatbot to be able to engage in persuasive dialogues, it needs to
be equipped with arguments from both perspectives. It needs arguments for (not)
engaging in a certain behaviour, that could potentially be given by the user, and
arguments that attack the user’s arguments (counterarguments). Furthermore,
the chatbot should also be able to identify the concerns of the user, a concern
being a matter of interest or importance to the user. We have shown in a previous
study unrelated to healthcare [8], that arguments and counterarguments can be
crowdsourced, represented as a graph, and used as the chatbot’s knowledge base.
Further, we have shown that the chatbot can automatically identify the concerns
of the user that she raises in her arguments during the chat, in order for the
chatbot to present counterarguments that address the user’s concerns.
Taking the user’s concerns into account when presenting an argument is
important. The chatbot might present a perfectly valid argument that the user
might not even disagree with. However, the chatbot’s argument may have no
impact on her stance, if the argument does not address her concerns. Whereas,
if the chatbot presents a counterargument that addresses her concern, the user
is more likely to be convinced. In order to choose a suitable counterargument in
a given dialogue, our chatbot tries to identify the concern of the user argument
and counter with an argument that addresses the user’s concern.
To illustrate how concerns arise in argumentation, and how they can be
harnessed to improve persuasion, consider a person who is reluctant to the idea
of getting a COVID-19 vaccine due to the short time it took to be developed
compared to other vaccines. The chatbot (that argues for getting a COVID-19
vaccine) could choose one of the following arguments:
– Option 1: A vaccine is the only safe way to create herd immunity which is
necessary to stop the virus from spreading. This will protect us all from getting
the virus, as well as those who, for some reason, cannot get the vaccine.
– Option 2: Yes, the vaccine was developed fast. That’s because a lot of funding
and research priority is currently directed at developing COVID-19 vaccines
around the world. As a result, the process is being sped up, allowing for more
clinical trials to take place in a shorter period of time.

62
L. Chalaguine and A. Hunter
Both arguments are perfectly reasonable. However, option 2 addresses the
concern regarding the short time frame by acknowledging it and providing an
argument on why the user should not worry about it, whereas option 1 does not,
even though it provides a valid argument for getting a COVID-19 vaccine.
2.2
Using an Argument Graph as Chatbot Knowledge Base
Argument graphs as proposed by Dung [10] are directed graphs where each node
denotes an argument and each arc denotes one argument attacking another. Such
graphs provide a useful representation to study attack and support relationships
of a given set of arguments.
In our previous studies we have used chatbots to persuade people to cycle
more, instead of driving or using public transport [12]; to reduce meat consump-
tion [9], and to change their stance on UK university fees [8]. The arguments were
either compiled and edited by hand by the researchers [12] or crowdsourced [8,9]
and stored as directed graphs in the chatbot’s knowledge base. In [7] we described
a method for automatically acquiring a large argument graph via crowdsourcing.
In this study, we used a hybrid approach where we crowdsourced the argu-
ments that people have for not taking a COVID-19 vaccine and hand-crafted
the counterarguments for the chatbot ourselves to avoid including invalid or
emotionally-loaded arguments in the argument graph. Crowdsourcing arguments
provides insight into the reasons for people’s behaviour, and an indication of
what sort of free natural language input the chatbot needs to handle.
We refrained from crowdsourcing counterarguments because COVID-19 is a
serious health issue with global impacts. It is a new domain and research on this
disease contains many uncertainties, and scientists still do not know many crucial
aspects regarding the virus and its transmission characteristics. Moreover, it is
the ﬁrst time that humans are being injected with vaccines based on mRNA
technology. All these form a highly uncertain information landscape and we
believe that a carefully curated knowledge base is better than a crowdsourced
one in such a critical communication framework.
3
Hypotheses
In this paper, we present a chatbot that utilises a set of arguments for taking
a COVID-19 vaccine as a knowledge base. The chatbot uses concerns to make
strategic choices of moves in order to engage in argumentative dialogues with
users to persuade them to get the vaccine.
Given this setting, we want to address the following three questions: Firstly,
whether a small and shallow argument graph is enough to counter the majority
of arguments people might have for not getting the vaccine and thereby create
persuasive dialogues. Secondly, whether it is possible by only identifying the
concern of a user argument to give a suitable counterargument. And ﬁnally,
whether an interactive chatbot is more persuasive than a static web page that
presents arguments for getting the vaccine. We summarise these points in the
following three hypotheses:

Addressing Popular Concerns Regarding COVID-19 Vaccination
63
H1 Given a novel domain (i.e. a domain which is relatively new to the user of
the chatbot and for which their knowledge and opinions might be limited), a
small set of arguments (between 30–50 arguments) can be used to represent
most of the possible arguments that a set of normal users would know and
appropriate counterarguments, and can be utilised by a chatbot to create
persuasive dialogues, meaning that the stance of the user changes after the
chat.
H2 Given a novel domain, the arguments that address the same concern are
suﬃciently similar to allow for the provision of suitable counterarguments
just by identifying the concern of the arguments.
H3 An argumentative dialogue with an interactive chatbot has a higher per-
suasive eﬀect than presenting the same arguments on a static web page for
people to read.
Further, we were also interested in whether during the chats new concerns
could be identiﬁed which were not raised in the crowdsourced arguments that
were used to construct the chatbot’s knowledge base.
We would like to note that the authors of this paper are neither psycholo-
gists, nor health professionals and that in this work we are (1) not taking any
personality traits of the user attributes into account in order to evaluate what
sort of argument might be more eﬀective for this particular person, (2) do not
compare the persuasiveness of the chatbot’s arguments to other potential argu-
ments, and (3) do not incorporate any other methods of persuasion apart from
argumentation. Our aim is to present a prototype chatbot that can be used to
convince people to get a COVID-19 vaccine using argumentation, and leave the
aforementioned issues for future work.
In the remainder of this paper, we describe the design of our chatbot that
was used for the argumentative dialogues and explain the experiments conducted
with the chatbot in order to test our hypotheses.
4
Chatbot Design
In this section, we describe the acquisition of arguments used to construct the
chatbot’s knowledge base, and the concern classiﬁer, used by the chatbot to
identify the concerns of the incoming user arguments.
4.1
Knowledge Base Construction
To construct the chatbot’s knowledge base which consists of a concern-argument
graph, which we deﬁne below, we recruited 100 participants via Proliﬁc1, which
is an online recruiting platform for scientiﬁc research studies, and asked them
to provide three arguments against getting a COVID-19 vaccine. We identiﬁed
7 concerns that were raised by the majority of the 300 crowdsourced arguments
by inspecting the most common, meaningful words, namely: short-term side
1 https://proliﬁc.co.

64
L. Chalaguine and A. Hunter
eﬀects of the vaccine, long-term side eﬀects of the vaccine, its fast devel-
opment, the mutation of the virus, the safety of the vaccine, comparison of
COVID-19 to the ﬂu and downplaying its danger, and young people believing
they do not need a vaccine.
The arguments given by users for each of the identiﬁed concerns were quite
similar as the following example demonstrates:
– I will not get a COVID-19 vaccine because of its potential side eﬀects.
– It’s a new vaccine, so we don’t yet know what the side eﬀects are.
– The vaccine may have a lot of side eﬀects that could be more dangerous than
COVID-19 itself.
The arguments regarding side eﬀects are very similar, and could all be coun-
tered with the same argument, for example, that there is high scrutiny over the
research on those particular vaccines and nobody would allow giving it to the
public if it was unsafe. Whereas, drafting individual counterarguments would
make the graph unnecessarily big and might result in the inclusion of many
similar counterarguments. We, therefore, structured the knowledge base as a
concern-argument graph which we deﬁne as follows:
Deﬁnition 1. A concern-argument graph is an acyclic graph (N, E, L)
where N is a set of nodes, E is a set of edges, and L is a labelling function
such that
– N can be partitioned into a set of arguments A and a set of concerns C (i.e.
N = A ∪C and A ∩C = ∅);
– the root of the tree, denoted ρ, is an argument in A;
– for each argument α ∈A\{ρ}, the labelling function assigns a set of concerns
to α (i.e. L(α) ⊆C);
– E is the smallest set satisfying the following conditions:
• for each concern σ ∈C, there is an edge (σ, ρ) ∈E
• for each argument α ∈A \ {ρ}, if σ ∈L(α), there is an edge (α, σ)
So a concern-argument graph can be regarded as a compacted version of a
three-level acyclic argument graph (i.e. an argument graph with an argument at
the root, counterarguments to the root argument, and counter-counterarguments
to the counterarguments). Furthermore, in the concern-argument graph, instead
of counterarguments, we use concerns as place-holders for a set of similar argu-
ments raising that concern. This information about concerns we will use in the
dialogue strategy.
The resulting graph is, therefore, much smaller and shallower than the tra-
ditional argument graph that includes both arguments and counterarguments.
Figure 1 shows an example of such a graph, where the goal argument is being
attacked by two concerns and each concern is being attacked by one counterar-
gument. One could, of course, extend the concern-argument graph and include
more concerns that attack speciﬁc counterarguments in level 4, and counterar-
guments that attack these in level 5. But we aimed to investigate whether a

Addressing Popular Concerns Regarding COVID-19 Vaccination
65
simple 3-layer model would be suﬃcient in this particular domain where people
are more likely to present a new argument, instead of countering the chatbot’s
counterargument.
Fig. 1. Part of the chatbot’s knowledge base with concerns a and b representing clusters
of arguments in level 1 (concerned about insuﬃcient testing of the vaccine and possible
side eﬀects respectively) attacking the chatbot’s goal argument, and arguments in level
2 addressing the concerns.
The counterarguments for level 3 in the concern-argument graph were care-
fully researched using reliable sources. Some arguments included links to the
NHS website that lists potential short-term side eﬀects from the vaccine2, or
videos that explain how the vaccine was developed so quickly without compro-
mising its safety3. As already stated above, we did not individually evaluate the
persuasiveness of the arguments, as this is out of the scope of this study.
The chatbot also needed default arguments for getting a COVID-19 vaccine
that it could use in case the concern of the user argument could not be identiﬁed.
These arguments are therefore not counterarguments in the traditional sense as
they do not directly counter the user argument but instead “change topic” and
present a new, important issue in the debate. We also added phrases like “Ok
but”, “Have you considered that” and “Nevertheless” to the beginning of the
default arguments to indicate that a deviation from the topic occurs. This way
the dialogue would resemble argumentation as it would happen between two
people: if two human agents engage in an argumentative dialogue, just because
one presents an argument the other cannot counter, the dialogue does not nec-
essarily end at that point. The other agent might switch topics and present a
new argument he or she believes in, without referencing and directly counter-
ing the previous argument. Two out of the three default arguments stated the
importance of the vaccine in order to reach herd immunity.
2 https://www.nhs.uk/conditions/coronavirus-covid-19/coronavirus-vaccination/
coronavirus-vaccine/.
3 All arguments were fact-checked by a medical doctor trainee and a machine learning
consultant who works in the pharmaceutical industry.

66
L. Chalaguine and A. Hunter
Our initial concern-argument graph consisted of 7 concerns each with 2–
4 counterarguments, with 19 leaves in total in the concern-argument graph.
Additionally, the chatbot was equipped with three default arguments it could
use in case no concern could be identiﬁed.
4.2
Understanding the User Input
The initial move is the root argument presented by the chatbot, then the user
gives a counterargument which is analysed to determine the concern. The chatbot
identiﬁed the concern of the user argument using a multinomial logistic regression
and a binary feature representation of the arguments (one-hot encoded vectors)4.
For the initial classiﬁer, the crowdsourced arguments were used for training. If
the prediction was over 40% in conﬁdence, the argument was labelled with the
identiﬁed concern which was used by the chatbot to pick a leaf argument from
the concern-argument graph that is a counterargument to that concern. If no
concern cannot be identiﬁed, or all the leaves of the identiﬁed concern had been
used, a default argument was presented to the user.
5
Experiments
The purpose of the chatbot was to test all three of our hypotheses. Prior to
recruiting participants for the study, we ran a survey where we asked people
to choose from a scale of 1–5 whether they would get a COVID-19 vaccine.
The options were very unlikely, somewhat unlikely, neither likely nor unlikely,
somewhat likely and very likely. We recruited 300 participants from those that
chose very unlikely, somewhat unlikely and neither likely nor unlikely, i.e. those
that had a negative or neutral stance. 240 participants chatted with the chatbot
and 60 participants were presented a static web page.
Before the chat, the users were directed to a Microsoft Form and asked again
how likely they would get a COVID-19 vaccine. After submitting their answer
they were redirected to a web page where they could begin the chat. The chat-
bot was composed of a front-end we coded in Javascript and a Python back-end
using the Flask web server library. The chatbot started the chat by instructing
the user that they could end the chat anytime by sending the word “quit” and
then asking why the user would not get a COVID-19 vaccine, once it became
available to him/her. The user then presented his/her ﬁrst argument. The chat-
bot replied with either a counterargument from the concern-argument graph or
a default argument, depending on whether it could identify the concern of the
user argument. The counterarguments were stored in a Python dictionary with
the concerns as the keys and the list of counterarguments that addressed that
concern as the values. If the concern could be identiﬁed, the chatbot replied
4 Due to the small amount of data and the use of one-hot encoded vectors, there was
no considerable diﬀerence when evaluating diﬀerent classiﬁers. More sophisticated
methods, like pre-trained language models, could not be used due to the lack of data
that could be used for pretraining on the given topic.

Addressing Popular Concerns Regarding COVID-19 Vaccination
67
with the ﬁrst counterargument in the list. If the message of the user was less
than 7 words in length and contained a negation, the chatbot queried Why?
or Why not? to force the user to expand. This process was repeated with each
subsequent argument given by the user. The chatbot would end the chat as soon
as all default arguments were used up and no concern could be identiﬁed, or
all counterarguments that addressed the concern were also used up. At the end
of the chat the chatbot presented the user with a link that redirected them to
another Microsoft Form where they were asked a series of questions:
1. Did you feel understood by the chatbot? (Yes/No/Sometimes)
2. Did you feel that the chatbot’s arguments were relevant? (Yes/No/Some of
them)
3. Do you feel like all your concerns were addressed? (The majority of
them/None of them/Some of them)
4. How likely would you get a COVID-19 vaccine, once one becomes available
to you? (Very unlikely - very likely)
Questions 1–3 were used to test our second hypothesis and judge the rele-
vance, length and quality of the chats, and question 4 was to test our ﬁrst and
third hypotheses and compare the stances of the participants before and after the
chat with the chatbot in order to judge persuasiveness. In order to test our third
hypothesis, 60 out of the 3005 participants did not chat with the chatbot but
instead were presented the chatbot’s 10 most commonly used counterarguments
in persuasive chats on a static web page.
Table 1. Breakdown of the 240 participants’ stance for getting a COVID-19 vaccine
before and after chatting with the chatbot.
Very unlikely Somewhat unlikely Neither likely nor unlikely Somewhat likely Very likely
Before 30%
42%
28%
0%
0%
After
22.5%
37.5%
29%
9.5%
1.5%
In order to test whether new concerns can be identiﬁed during the chats that
were not identiﬁed in the crowdsourced arguments, we analysed the chats after
every batch of 60 participants (we also recruited participants in batches of 60).
By inspecting common, meaningful words we could identify new concerns after
each batch of 60 participants and re-train the classiﬁer with enough examples of
the new concern, and add suitable counterarguments to the concern-argument
graph. We only added a new concern to the chatbot’s concern-argument graph
if we could automatically identify at least 10 arguments that addressed that
concern. The training set was also updated with more training examples to an
5 Due to limited funding we did not want to split the participants in half but rather
collect more data in form of chatlogs, since a web page does not provide data for
further research.

68
L. Chalaguine and A. Hunter
already existing concern. For example, in the crowdsourced arguments, many
people used the word mutation whereas in the chats the word strain was preva-
lent.
This way 8 additional concerns could be identiﬁed: death, that the vaccine
does not prevent you from getting and spreading COVID-19, people claiming
they already had COVID-19, that COVID-19 has a too high of a survival rate
to be worried about it, that the vaccine may impact fertility, that the vaccine
might not be eﬀective, that the ingredients of the vaccine are unknown, and
that herd immunity can be created naturally by catching the virus and hence
no vaccine is needed.
6
Evaluation of the Chatbot
Table 2. Percentage of the 240 participants who changed their stance after chatting
with the chatbot.
Negative to neutral Neutral to positive Negative to positive Total
9%
7.5%
3.5%
20%
Table 1 shows the stance of the 240 participants who chatted with the chatbot,
before and after chatting with the chatbot. We divided the change in stance
into 3 categories: a change from negative to neutral (from very unlikely/ some-
what unlikely to neither likely nor unlikely); a change from neutral to positive
(from neither likely nor unlikely to somewhat likely/very likely); and a change
from negative to positive (from very unlikely/somewhat unlikely to somewhat
likely/very likely). We do not consider a change from very unlikely to somewhat
unlikely. Table 2 shows the percentage of the 240 participants who changed their
stance by engaging in an argumentative dialogue with the chatbot. 20% of the
participants (48 out of 240) had a positive change in stance. This veriﬁes our
ﬁrst hypothesis - that a small, shallow concern-argument graph can be utilised
by a chatbot to create persuasive dialogues.
Given that the chatbot did not use natural language generation and was
not able to address novel arguments or expand on existing ones by giving more
information, and solely relied on correct concern classiﬁcation, the given results
are promising. The length of the chats were on average 12 alternating turns.
This means that the chatbot, on average, gave 6 arguments, 3 of which were
default arguments and 3 from the graph. Table 3 shows the results for the ﬁrst
three questions. 35% of the participants felt understood by the chatbot and
further 41% felt sometimes understood. 32% perceived the chatbot’s arguments
as relevant and further 55% perceived them as sometimes relevant. 23% felt that
the majority of their concerns were addressed and further 54% felt that some of
their concerns were addressed. This supports our second hypothesis that only

Addressing Popular Concerns Regarding COVID-19 Vaccination
69
by identifying the concern of an argument, suitable counterarguments can be
presented and that the resulting argumentation dialogues are of satisfactory
length and quality. An example of a chat can be seen in Fig. 2. All chatlogs, the
data for the concern-argument graph, the concerns and their descriptions, and
the code for the chatbot can be found on github [2].
As mentioned in the previous section, we recruited the participants in batches
of 60. To evaluate our third hypothesis, the chatbot’s persuasive eﬀect compared
to a static web page, we compared the results of the 60 participants who were
presented with the static web page with a batch of 60 participants who chatted
with the chatbot with similar starting distributions of their stance. The fourth
(and last) batch of participants had a similar distribution as the batch that was
recruited to read the arguments on a static web page. The starting distributions
are shown in Table 4 and the change of stance for both groups are shown in
Table 5.
We used a Chi-Square test to compare the number of participants who
changed their stance after chatting with the chatbot with the number of partic-
ipants who changed their stance after reading the 10 most common arguments
used by the chatbot (this means that the participants who only saw the static
web page, on average saw 4 more arguments than those who chatted with the
chatbot). The results were statistically signiﬁcant with a p-value of .023 at p <
.05. The results, therefore, support our hypothesis that an interactive chatbot is
more persuasive than a static web page.
Table 3. Answers to the ﬁrst three questions by the 240 participants who chatted with
the chatbot.
Felt understood
Relevance
Concern addressed
Yes
Sometimes No
Yes
Some No
Majority Some None
35% 41%
24% 32% 55%
13% 23%
54%
23%
Table 4. Breakdown of stance for getting a COVID-19 vaccine of the group of 60
participants before chatting with the chatbot, and the group of 60 participants who
was presented with a static web page.
Very unlikely Somewhat unlikely Neither likely nor unlikely
Chatbot
60%
33%
7%
Web page 52%
28%
20%
Table 5. Change of stance for the group of 60 participants who chatted with the
chatbot, and the group of 60 participants who was presented with a static web page.
Negative to neutral Neutral to positive Negative to positive Total (no of participants)
Chatbot
12%
5%
2%
18% (11)
Web Page
5%
0%
0%
5% (3)

70
L. Chalaguine and A. Hunter
The newly identiﬁed concerns mentioned in the previous section were also
raised by some arguments that were collected during the initial argument collec-
tion described in Sect. 4.1. So claiming that new concerns could be identiﬁed dur-
ing the chats would be incorrect. Given a large enough sample of crowdsourced
arguments, more concerns could have been identiﬁed and included (together with
appropriate counterarguments). in the concern-argument graph that was used
as the chatbot’s knowledge base.
Side eﬀects (short and long term) are by far the most popular concern, with
45% of user arguments given during the chats (where a concern could be iden-
tiﬁed) raising it. This is coherent with previous studies which analysed vaccine
hesitancy in France [21], the US [15] and the EU [17]. The second most prevalent
concern was about the safety of the vaccine in general (28%), and in the third
place, the vaccines fast development and young people believing they do not
need one (both 10%).
7
Discussion and Conclusion
The aim of this paper was to present a prototype chatbot that can engage in per-
suasive dialogues with people who are opposed to the COVID-19 vaccine using
computational models of argument. Our contribution in this paper is threefold.
Firstly, we have shown that for a new domain, where there exists a lot of uncer-
tainty, a small argument graph can be used to represent most of the possible
arguments in this domain. This argument graph can be utilised by a chatbot
to create persuasive dialogues, and we presented a method how to acquire and
structure such a graph in form of a concern-argument graph. In our previous work
[8] the chatbot’s knowledge base consisted of an argument graph that included
both arguments and counterarguments. The chatbot matched the incoming user
argument with a similar argument in the graph (target argument) using cosine
similarity of the vector representations of the two arguments (the vectors were
created using GloVe word embeddings [18]). The graph was therefore much big-
ger (containing over 1200 arguments) than the one presented in this paper.
Secondly, we have demonstrated that no sophisticated natural language
understanding of the user arguments is needed in order to provide suitable coun-
terarguments that address the majority of the concerns of the users. And thirdly,
we have shown that an interactive chatbot has a higher persuasive eﬀect than a
static web page.
Further, we have shown that in this domain a concern-argument graph (a
three-level acyclic graph) where after the initial move, the chatbot only picks a
leaf at every turn or uses a default argument, is enough to generate persuasive
dialogues. Using a modest concern-argument graph, as described in this paper,
and to not constraint the chatbot to use a larger argument graph that may
involve long paths, has two main beneﬁts: ﬁrstly, the graph can be constructed
with less data than for a larger argument graph; secondly, this allows the chatbot
to counter user arguments that are not direct counterarguments to the previously
given chatbot argument. This is important, as during the chats people often

Addressing Popular Concerns Regarding COVID-19 Vaccination
71
Fig. 2. Example chat between a participant from the fourth batch. Chatbot arguments
are in the dark boxes and user arguments in the light boxes. Default arguments are
indicated with an *. The chat begins in the left column and continues in the right
one. The participant indicated that he or she was somewhat unlikely to get vaccinated
before the chat, but changes his/her stance to neither likely nor unlikely after the chat.
ignored the chatbot’s counterargument but instead gave a new argument on
why not to take the vaccine.
It would, of course, be desirable to use more sophisticated natural language
processing methods to process the user’s input. For that, however, much more
data is needed which currently is not available. The chatbot, hence, can only
reply to well-phrased arguments that raise common concerns that it can iden-
tify. These sort of replies contributed to only 50% of the users’ replies. Other
types of responses included novel arguments, statements like “I don’t care or “I
will take my chances”, emotional accusations (about the government not caring
or the chatbot being stupid) and questions about the vaccine. In future work,
argumentation will, therefore, only be one component of the chatbot, paired
with the ability to answer questions and provide information about COVID-19

72
L. Chalaguine and A. Hunter
vaccines and their associated risks, and a conversational component that can
address user statements and emotional responses.
We also want to experiment with diﬀerent argumentation frameworks and
dialogue strategies. A reasonable extension to the current framework would be
bipolar argumentation [3,6]. In Fig. 2, the fourth argument that the chatbot
presents addresses the concern of long term side eﬀects. The user replies with
an argument that again raises long term side eﬀects. Hence, the ﬁfth argument
by the chatbot also addresses that concern. Argument 5 can, therefore, be seen
as a supporting argument to argument 4. A potential dialogue strategy for the
chatbot could be to use arguments that support the previously given argument
by the chatbot, if a concern cannot be identiﬁed, instead of giving default argu-
ments. For example, after the user said that he is still unsure, the chatbot could
have provided a supporting argument to argument 6 and present the user with
another argument that addresses the fast development of the vaccine.
To conclude this paper, we want to emphasise the advantage of using a chat-
bot for such a task: a chatbot is able to address millions of people at the same
time in the comfort of their own home and collect a vast amount of data in a
very short time. Our method of analysing the incoming user arguments scales
easily and allows obtaining many arguments from diﬀerent people. The more
data comes in, the easier it gets to identify patterns, discover new concerns, and
acquire arguments that address these concerns and update the chatbot’s concern-
argument graph accordingly. This allows us to identify common misconceptions,
address the lack of information, and potentially even fake news.
Acknowledgements. This study was partially supported by Noah Castelo from the
Alberta School of Business.
References
1. https://yougov.co.uk/topics/international/articles-reports/2021/01/12/covid-19-
willingness-be-vaccinated
2. https://github.com/lisanka93/covid-19 vacc bot
3. Amgoud, L., Cayrol, C., Lagasquie-Schiex, M.C., Livet, P.: On bipolarity in argu-
mentation frameworks. Int. J. Intell. Syst. 23(10), 1062–1093 (2008)
4. Bench-Capon, T., Atkinson, K.: Abstract argumentation and values. In: Simari,
G., Rahwan, I. (eds.) Argumentation in Artiﬁcial Intelligence, pp. 45–64. Springer,
Boston (2009). https://doi.org/10.1007/978-0-387-98197-0 3
5. Bench-Capon, T.J.: Persuasion in practical argument using value-based argumen-
tation frameworks. J. Log. Comput. 13(3), 429–448 (2003)
6. Cayrol, C., Lagasquie-Schiex, M.C.: On the acceptability of arguments in bipolar
argumentation frameworks. In: Godo, L. (ed.) ECSQARU 2005. LNCS (LNAI),
vol. 3571, pp. 378–389. Springer, Heidelberg (2005). https://doi.org/10.1007/
11518655 33
7. Chalaguine,
L.A.,
Hunter,
A.:
Knowledge
acquisition
and
corpus
for
argumentation-based chatbots. In: Proceedings of the 3rd Workshop on Advances
in Argumentation in Artiﬁcial Intelligence, pp. 1–14. CEUR (2019)

Addressing Popular Concerns Regarding COVID-19 Vaccination
73
8. Chalaguine, L.A., Hunter, A.: A persuasive chatbot using a crowd-sourced argu-
ment graph and concerns. In: Proceedings of Computational Models of Argument
(COMMA), pp. 9–20. IOS Press (2020)
9. Chalaguine, L.A., Hunter, A., Hamilton, F.L., Potts, H.W.W.: Impact of argument
type and concerns in argumentation with a chatbot. In: Proceedings of the 31st
International Conference on Tools with Artiﬁcial Intelligence, pp. 1557–1562. IEEE
(2019)
10. Dung, P.: On the acceptability of arguments and its fundamental role in non-
monotonic reasoning, logic programming and n-person games. Artif. Intell. 77(2),
321–357 (1995)
11. Fines, P., Eames, K., Heymann, D.: “Herd immunity”: a rough guide. Clin. Infect.
Dis. 52(7), 911–916 (2011)
12. Hadoux, E., Hunter, A.: Comfort or safety? Gathering and using the concerns of
a participant for better persuasion. Argument Comput. 10(2), 113–147 (2019)
13. Hunter, A.: Towards a framework for computational persuasion with applications
in behaviour change. Argument Comput. 1, 15–40 (2018)
14. MacDonald, N.: Vaccine hesitancy: deﬁnition, scope and determinants. Vaccine
33(34), 4161–4164 (2015)
15. Marco-Franco, J.E., Guadalajara-Olmeda, N., de Juli´an, S.G., Vivas-Consuelo, D.:
Covid-19 healthcare planning: predicting mortality and the role of the herd immu-
nity barrier in the general population. Sustainability 12(13), 5228 (2020)
16. Meppelink, C.S., Smit, E.G., Fransen, M.L., Diviani, N.: “I was right about vacci-
nation”: conﬁrmation bias and health literacy in online health information seeking.
J. Health Commun. 24(2), 129–140 (2019)
17. Neumann-B¨ohme, S., et al.: Once we have it, will we use it? A European survey
on willingness to be vaccinated against COVID-19. Eur. J. Health Econ. 21(7),
977–982 (2020). https://doi.org/10.1007/s10198-020-01208-6
18. Pennington, J., Socher, R., Manning, C.D.: GloVe: global vectors for word repre-
sentation. In: Empirical Methods in Natural Language Processing (EMNLP), pp.
1532–1543. ACL (2014)
19. R´emy, V., Largeron, N., Quilici, S., Carroll, S.: The economic value of vaccination:
why prevention is wealth. J. Market Access Health Policy 3(1), 29284 (2015)
20. Sherman, S.M., et al.: COVID-19 vaccination intention in the UK: Results from the
COVID-19 vaccination acceptability study (CoVAccs), a nationally representative
cross-sectional survey. medRxiv (2020)
21. Ward, J.K., Alleaume, C., Peretti-Watel, P.: The French public’s attitudes to a
future Covid-19 vaccine: the politicization of a public health issue (2020). https://
doi.org/10.31235/osf.io/xphe9

Argument Strength in Probabilistic
Argumentation Using
Conﬁrmation Theory
Anthony Hunter(B)
Department of Computer Science, University College London, London, UK
anthony.hunter@ucl.ac.uk
Abstract. It is common for people to remark that a particular argu-
ment is a strong (or weak) argument. Having a handle on the relative
strengths of arguments can help in deciding on which arguments to con-
sider, and on which to present to others in a discussion. In computational
models of argument, there is a need for a deeper understanding of argu-
ment strength. Our approach in this paper is to draw on conﬁrmation
theory for quantifying argument strength, and harness this in a frame-
work based on probabilistic argumentation. We show how we can calcu-
late strength based on the structure of the argument involving defeasible
rules. The insights appear transferable to a variety of other structured
argumentation systems.
Keywords: Argument strength · Probabilistic argumentation ·
Deductive argumentation · Defeasible logic
1
Introduction
In real-world argumentation, it is common for arguments to be considered in
terms of their strength. Yet in computational models of argument, we lack
formalisms that adequately measure strength in arguments. Some variants of
abstract argumentation touch on the notion of strength such as rankings (e.g.
[1,2,4,6,21]), and probabilities (e.g. [11,15,18,20,30,34]). However, these do not
capture a notion of argument strength in terms of the quality of the contents of
the premises and/or claim, rather they either assume that some kind of strength
value is given for each argument and/or they calculate strength in terms of
attacking and supporting arguments.
Using logical (i.e. structured) arguments allows the quality of the contents
of the premises and claim to be directly considered. Some proposals assume
strength is an input to the system (e.g. [8]). Others assess the strength of an
argument in terms of the belief in it, often in terms of belief in the premises
and claim (e.g. [14,15,29]), and in terms of the conditional probability of the
claim given the premises of the argument (e.g. [16,28,36]). So these draw on
uncertainty in argumentation to quantify the strength. But as we shall see, these
only give us an incomplete picture of the strength of an argument.
c
⃝Springer Nature Switzerland AG 2021
J. Vejnarov´a and N. Wilson (Eds.): ECSQARU 2021, LNAI 12897, pp. 74–88, 2021.
https://doi.org/10.1007/978-3-030-86772-0_6

Argument Strength in Probabilistic Argumentation
75
The use of defeasible logic is well-established in argumentation (see for exam-
ple [13,27,33]), and key approaches to structured argumentation incorporate var-
ious kinds of defeasible rules [3]. There are also some proposals for probabilistic
quantiﬁcation of uncertainty in argumentation systems based on defeasible logic
(e.g. [11,31,32]), but quantifying notions of strength have not been systemati-
cally considered in these formalisms.
To formalize argument strength for defeasible logic, we draw on measures
from conﬁrmation theory. These were originally proposed to determine the degree
to which scientiﬁc evidence supports a hypothesis. They can capture how uncer-
tainty associated with premises can impact a claim. To use these, we adapt the
epistemic approach to probabilistic argumentation [15,18,34].
The proposal in this paper could be used in various ways. However, to illus-
trate and motivate, we focus on the following audience scenario: Someone
presents us with an argument graph and the knowledgebase from which it has
been constructed. The knowledge represents patterns that normally hold in the
world (e.g., if it is bird, then it is capable of ﬂying). As the audience, we are
at liberty to identify a probability distribution over the possible worlds (in the
following sections we make this precise) that represent our beliefs about the
propositions in the language, and then we can use this probability distribution
to analyze the strength of the arguments presented to us.
2
Defeasible Logic
We assume a ﬁnite set of atoms A. We form a set of literals L(A) = A∪{¬φ |
φ ∈A}. A defeasible rule is of the form ψ1∧. . .∧ψn →φ where ψ1, . . . , ψn, φ ∈
L(A). For a rule ρ of the form ψ1 ∧. . . ∧ψn →φ, let Tail(ρ) = {ψ1, . . . , ψn} and
Head(ρ) = φ. The set of rules is R(A) and the set of formulae is F(A) =
L(A) ∪R(A). A knowledgebase is a subset of F(A).
Example 1. Consider b for bird, p for penguin, and f for capable of ﬂying. Then
Δ = {b, p, b →f, p →¬f} ⊆F(A) is a knowledgebase.
Next, we present a variant of defeasible logic, incorporating ex falso quodlibet,
to build arguments, and a semantics to analyze arguments.
Deﬁnition 1. Let Δ be a knowledgebase and φ, ψ ∈L(A). The consequence
relation, denoted ⊢, is deﬁned as follows: (1) Δ ⊢φ if φ ∈Δ; (2) Δ ⊢φ if
there is a ψ1 ∧. . .∧ψn →φ ∈Δ and Δ ⊢ψ1 and . . . and Δ ⊢ψn; and (3) Δ ⊢φ
if Δ ⊢ψ and Δ ⊢¬ψ. Let Closure(Δ) = {φ ∈A | Δ ⊢φ}.
Example 2. For Δ = {b, b →f}, Closure(Δ) = {b, f}.
The Tarskian properties (widely regarded as requirements for a logic) are
satisﬁed (though for reﬂexivity, it is restricted to literals): (Reﬂexivity) (Δ ∩
L(A)) ⊆Closure(Δ); (Monotonicity) Closure(Δ) ⊆Closure(Δ′) if Δ ⊆Δ′; and
(Idempotency) Closure(Δ) ⊆Closure(Closure(Δ)).

76
A. Hunter
A model is an assignment of true or false to the literals of the language.
We represent each model by a subset of A. The set of models of the language,
denoted M(A), is the power set of A. For m ∈M(A), φ ∈A, the satisfaction
relation, denoted |=, is: (1) m |= φ iﬀφ ∈m; And (2) m |= ¬φ iﬀφ ̸∈m. We
deﬁne the models for a set of formulae using the following ﬁxpoint function.
Deﬁnition 2. For Δ ⊆F(A), and i ∈N, the inference operators, denoted
Infi, are deﬁned as: Inf1(Δ) = Δ ∩L(A) and Infi+1(Δ) = Infi(Δ) ∪{Head(ρ) |
ρ ∈Δ ∩R(A) and for all ψ ∈Tail(ρ), ψ ∈Infi(Δ)}. Let Infer(Δ) = Infk(Δ)
where k is the smallest value s.t. Infk(Δ) = Infk+1(Δ).
Deﬁnition 3. The satisfying models for Δ ⊆F(A), is Models(Δ) = {m ∈
M(A) | m |= φ for all φ ∈Infer(Δ)}.
Example 3. For Δ = {b,¬o, b →f, p →¬f}, where A = {b, p, f, o}, Infer(Δ) =
{b, ¬o, f}, and Models(Δ) = {{b, f}, {b, p, f}}.
Deﬁnition 4. For Δ ⊆F(A), φ ∈L(A), the entailment relation holds,
denoted Δ |= φ, iﬀModels(Δ) ⊆Models(φ).
Example 4. For knowledgebase Δ = {b, p, b →f}, Δ |= f.
A knowledgebase is consistent iﬀit does not imply an atom and its negation.
So Example 1 (respectively Example 4) is inconsistent (respectively consistent).
Obviously, Δ is consistent iﬀModels(Δ) ̸= ∅.
Proposition 1. For Δ ⊆F(A), φ ∈L(A), Δ ⊢φ iﬀΔ |= φ.
When Δ is consistent, this correctness result can be shown via the notion of a
proof tree where φ is at the root, each leaf is a literal in Δ, and each non-leaf node
φ′ is such that there is a rule ψ1 ∧. . .∧ψn →φ′ ∈Δ and each ψi ∈{ψ1, . . . , ψn}
is a child of φ′. So Δ ⊢φ holds iﬀthere is such a tree where each branch is
ﬁnite. We can use the same tree to consider entailment. So Δ |= φ holds iﬀthe
leaves and inferences are satisﬁed by all the models of Δ, and hence all these
models satisfy φ. When Δ is inconsistent, the consequence relation entails any
literal (Proof rule 3), and similarly the entailment relation for defeasible logic
is trivializable in the sense that any literal is entailed by inconsistency. This is
because when Δ is inconsistent, Models(Δ) = ∅, and therefore for any β ∈L,
Models(Δ) ⊆Models(β).
3
Probabilistic Argumentation
In this section, we adapt the epistemic approach to probabilistic argumentation
for use with defeasible logic. To present models, we use a signature, denoted S,
which is the atoms of the language L given in a sequence ⟨a1, . . . , an⟩, and then
each model m ∈M(A) is a binary number b1. . .bn where for each digit bi, if bi
is 1, then ai is true in m, and if bi is 0, then ai is false in m. For example, for S
= ⟨a, b, c⟩, M(A) is {111, 110, 101, 100, 011, 010, 001, 000}. So for m = 101, a is
true, b is false, and c is true.

Argument Strength in Probabilistic Argumentation
77
Deﬁnition 5. A probability distribution P over M(A) is a function P :
M(A) →[0, 1] s.t. 
m∈M(A) P(m) = 1.
Deﬁnition 6. The probability of literal φ ∈L(A) w.r.t. probability distribu-
tion P is P(φ) = 
m∈M(A) s.t. m|=φ P(m).
Next, for an argument: (1) the premises imply the claim; (2) the premises
are consistent; and (3) the premises are minimal for entailing the claim.
Deﬁnition 7. For Φ ⊆F(A), and α ∈L(A), ⟨Φ, α⟩is an argument iﬀ(1)
Φ ⊢α; (2) Φ is consistent; and (3) there is no Φ′ ⊂Φ such that Φ′ ⊢α.
Example 5. For Δ = {b, p, b →f, p →¬f}, the arguments are ⟨{b, b →f}, f⟩,
⟨{p, p →¬f}, ¬f⟩, ⟨{b}, b⟩, and ⟨{p}, p⟩.
A reﬂexive argument is of the form A = ⟨{α}, α⟩. For argument A = ⟨Φ, α⟩,
Support(A) returns Φ, Claim(A) returns α, Facts(A) returns the literals in Φ,
Rules(A) returns the rules in Φ, and Frame(A) returns Facts(A) ∪{Claim(A)}.
The probability of an argument being acceptable is based on the facts in the
premises and the claim of the argument. So returning to the audience scenario,
if the audience has been presented with an argument graph, they can use their
probability distribution to calculate the probability of each argument.
Deﬁnition 8. The probability of argument A being acceptable is denoted
P(A), where P(A) = 
m∈Models(Frame(A)) P(m).
Example 6. Continuing Example 1, with probability distribution P below, and
signature S = ⟨b, p, f⟩, P(⟨{b, b →f}, f⟩) = 0.95 and P(⟨{p, p →¬f}, ¬f⟩) =
0.01.
⟨b, p, f⟩110
101
100
P
0.01 0.95 0.04
Deﬁnition 9. For φ, φ′ ∈L(A), let φ ≡φ′ denote Models(φ) = Models(φ′) and
let ¬¬φ = φ. For arguments A and B, A is a direct undercut of B if there
is φ ∈Support(B) s.t. Claim(A) ≡¬φ. A is a rebuttal of B if Claim(A) ≡
¬Claim(B). A attacks B iﬀA is a direct undercut or A is a rebuttal of B.
The following coherence property holds because the sum of belief in comple-
mentary literals is less than or equal to 1.
Proposition 2. For probability distribution P, if B attacks A, then P(A) +
P(B) ≤1.

78
A. Hunter
We use the usual notion of an argument graph, where each node is an argu-
ment, and each arc denotes an attack by one argument on another [10]. For
a knowledgebase, a complete argument graph contains all arguments and
attacks (e.g. Fig. 1). However, we are not using the argument graph to deter-
mine which arguments are acceptable. Rather, we use a probability distribution
to determine the acceptable arguments as explained below. So the role of the
argument graph is to provide a presentation of the arguments.
Fig. 1. The following is a complete argument graph (excluding reﬂexive arguments)
from Δ = {b, p, b ∧n →f, p →¬f, p →¬n} where b is bird, p is penguin, f is capable
of ﬂying, and n is a normality atom. Argument A3 has the claim that negates n, and
so represents an attack on the use of the rule with n as condition in A1.
When P(A) > 0.5, then the argument is believed to be acceptable, whereas
when P(A) ≤0.5, then it is not believed to be acceptable. The epistemic
extension for a graph G, denoted Extension(P, G), is the set of arguments that
are believed to be acceptable (i.e. A ∈Extension(P, G) iﬀA is in G and
P(A) > 0.5). For example, for graph G in Fig. 1, with P(A1) = 0.95, and
P(A2) = P(A3) = 0.15, Extension(P, G) = {A1}. As shown in [15], for any
probability distribution P and graph G, Extension(P, G) is conﬂict-free.
The epistemic approach provides a ﬁner-grained assessment of an argument
graph than given by the deﬁnitions for Dung’s extensions. By adopting con-
straints on the distribution, the epistemic approach subsumes Dung’s deﬁnitions
[18,26,34]. The epistemic approach also provides alternatives to Dung’s app-
roach. For instance, we may wish to represent disbelief in arguments even when
they are unattacked [25].
4
Modelling Normality
Defeasible rules are normally correct, but sometimes are incorrect, and so we
need to attack them. To address this, we use normality atoms as illustrated in
Fig. 1. We assume the set of atoms A is partitioned into normality atoms,
denoted N, and ordinary atoms, denoted Q. So A = Q ∪N and Q ∩N = ∅.
We read the normality atom in the condition as saying that the context for
applying the rule is normal. If there are reasons to believe that it is not a normal
context, then a counterargument attacks this assumption of normality.
We will use the following normality modelling convention: Each rule has
at most one normality atom as a condition, and if a normality atom appears as
condition in a rule, it is unique to that rule. No other rule in the knowledge-
base has the same normality atom as a condition. However, multiple rules in
the knowledgebase can have the same negated normality atom as head. This

Argument Strength in Probabilistic Argumentation
79
convention helps us to specify an appropriate probability distribution over a set
of atoms that includes normality atoms. We quantify the probability of a nor-
mality atom in terms of the unique rule that contains it as an antecedent: For a
defeasible rule β1 ∧. . . ∧βn ∧γ →α with normality atom γ, the probability of
γ is P(β1 ∧. . . ∧βn ∧α).
Example 7. Consider probability distribution P, with the rules in Δ being b ∧
n1 →f and p ∧n2 →¬f. So P satisﬁes our constraints that P(n1) = P(b ∧f)
and P(n2) = P(p ∧¬f).
⟨b, p, f, n1, n2⟩11001 10110 10000
P
0.01
0.95
0.04
The use of normality propositions to disable rules is analogous to the use of
abnormality predicates in formalisms such as circumscription [22]. Furthermore,
we can use normality atoms to capture the speciﬁcity principle where a more
speciﬁc rule is preferred over a less speciﬁc rule.
5
Argument Strength
Given an argument A and probability distribution P, we let SP (A) be an assign-
ment in the [−1, 1] interval to denote the strength of A. If SP (A) ≤0, then the
support of the argument does not provide a good reason for the claim. As SP (A)
rises above zero, then the support of the argument gives an increasingly good
reason for the claim.
In the rest of this paper, we will focus on arguments that involve relation-
ships between observations. We will assume that all the atoms, apart from the
normality atoms, are observations. These are atoms that can ultimately be veri-
ﬁed as true or false, though at any speciﬁc time, there may be uncertainty about
which observations are true or false. Examples of observations include b = bird,
d = duck, p = penguin, e = eagle, and f = ﬂy-thing. Given an argument A, the
evidence in A, denoted Ev(A), is the set of observations in the support of A that
are not normality atoms. (i.e. Ev(A) = Facts(A)∩L(Q)). So when we investigate
strength, we want to quantify some aspect of how believing the evidence in the
premises supports the claim.
There are various ways of deﬁning SP . To clarify some of the issues consider
the simple situation of an argument A of the form ⟨{b, b →a}, a⟩. For this,
consider the four models for signature ⟨a, b⟩. Mass on 11 indicates positive cor-
relation between a and b, and mass on 10 and 01 indicates negative correlation.
On this basis, the conditional probabilities P(a|b) and P(b|a) indicate positive
correlation, and the conditional probabilities P(a|¬b), P(¬a|b), P(b|¬a), and
P(¬b|a), indicate negative correlation.

80
A. Hunter
So there are multiple dimensions to connecting the evidence and claim. We
will draw some of these out in the following seven properties that capture desir-
able, though not mandatory, features of argument strength. For a set of literals
Γ, ∧Γ is the conjunction of the literals. We extend the deﬁnition for the proba-
bility of a literal so that if λ is a Boolean combination of literals in L(A), then
P(λ) is the sum of the probability of the models that classically satisfy λ. Also
P(λ|λ′) is P(λ ∧λ′)/P(λ′), and if P(λ) = 0, then we let P(λ|λ′) = 0.
(X1) If Claim(A) ≡∧Ev(A) and P(Claim(A)) > 0, then S(A) < 1
(X2) If ∧Ev(A) ≡∧Ev(B) and Claim(A) = Claim(B), then S(A) = S(B)
(X3) If S(A) > 0, then P(Claim(A)| ∧Ev(A)) > 0
(X4) If P(Claim(A)| ∧Ev(A)) = P(Claim(A), then S(A) = 0
(X5) If P(∧Ev(A)|Claim(A)) = P(∧Ev(A)|¬Claim(A)), then S(A) = 0
(X6) If P is a uniform probability distribution, then S(A) = 0
(X7) If P(Claim(A)| ∧Ev(A)) = P(Claim(A)|¬ ∧Ev(A)), then S(A) = 0
We explain these properties as follows: (X1) If the evidence and claim are
logically equivalent, and there is non-zero belief in the claim, then the argument
is not providing a good reason for the claim because the reason is just the claim
reiterated, and hence the strength is below 1; (X2) If two arguments are logically
equivalent with respect to support and claim, then they provide equivalent rea-
sons for equivalent claims, and so they are equally strong; (X3) Positive strength
requires the probability of the claim conditional on the evidence to be non-zero;
(X4) If the probability of the claim conditional on the evidence equals the prob-
ability of the claim, then the argument has zero strength as the premises are
not giving a useful reason for the claim; (X5) If the probability of the evidence
conditional on the claim equals the probability of the evidence conditional on
the negation of the claim, then the argument has zero strength; (X6) If there is a
uniform distribution, there is no material relationship between the evidence and
claim and so the argument has zero strength; And (X7) If the probability of the
claim conditional on the evidence equals the probability of the claim conditional
on the negation of the evidence, then the argument has zero strength.
The ﬁrst strength function we consider is the plausibility strength function
which is the probability of the claim conditional on the evidence.
Deﬁnition 10. The plausibility of an argument A w.r.t. probability distribu-
tion P, denoted Sp
P , is P(Claim(A)| ∧Ev(A)).
Example 8. Continuing Example 7, the plausibility of argument A = ⟨{b, b ∧
n1 →f}, f⟩is Sp
P (A) = 0.95.
Proposition 3. The plausibility measure Sp
P satisﬁes X2 and X3, but it does
not satisfy X1, X4, X5, X6, or X7.
Proof. For counterexamples for X4, X5 and X7, consider the signature ⟨a, b⟩with
argument ⟨{b, b →a}, a⟩. (X1) Assume Claim(A) ≡∧Ev(A) and P(Claim(A)) >
0. So P(Claim(A) ∧Ev(A))/P(Claim(A)) = 1. So Sp
P (A) ̸< 1. (X2 and X3)

Argument Strength in Probabilistic Argumentation
81
Direct from deﬁnition. (X4) Let P(11) = 0.2, P(10) = 0.3, P(01) = 0.2, and
P(00) = 0.3. (X5) Let P(11) = 0.2, P(10) = 0.2, P(01) = 0.3, and P(00) = 0.3.
(X6) If P is uniform, then P(Claim(A) ∧Ev(A))|P(∧Ev(A)) ̸= 0. So Sp
P (A) ̸= 0.
(X7) Let P(11) = 0.2, P(10) = 0.3, P(01) = 0.2, and P(00) = 0.3.
The failure of the X1, X4, X5, X6, and X7 properties suggests that there
may be useful alternatives to the plausibility for measuring the strength of an
argument. We are not suggesting that there is a single measure that tells us
everything we need to know about the strength of a probabilistic argument
based on defeasible rules, but we do expect that diﬀerent measures can tell us
diﬀerent useful things about the strength of arguments.
6
Conﬁrmation Theory
For an alternative perspective on the strength of an argument, we turn to con-
ﬁrmation measures. Originally, conﬁrmation measures were developed in the
philosophy of science to investigate the development of scientiﬁc hypotheses [9].
The aim of a conﬁrmation measure C(E, H) is to capture the degree to which
evidence E supports hypothesis H. Conﬁrmation measures have been proposed
as a measure of argument strength in [24] but only in the restricted context of
an argument that is a conditional probability statement. For our purposes, we
assume that the evidence is a set of literals Δ and the hypothesis is a literal φ.
We review some well-known conﬁrmation measures next.
Deﬁnition 11. For Δ ⊆L(A), φ ∈L(A), and probability distribution P, the
Cd
P [5], Cs
P [7], and Ck
P [19] conﬁrmation measures are deﬁned as follows.
– Cd
P (Δ, φ) = P(φ| ∧Δ) −P(φ)
– Cs
P (Δ, φ) = P(φ| ∧Δ) −P(φ|¬(∧Δ))
– Ck
P (Δ, φ) = P (∧Δ|φ)−P (∧Δ|¬φ)
P (∧Δ|φ)+P (∧Δ|¬φ) when P (∧Δ|φ)+P (∧Δ|¬φ)>0 and 0 otherwise.
We explain these measures as follows: (Cd
P ) the increase in belief in the claim
that can be attributed to believing the evidence to be true, i.e. for it to be
positive, P(φ) < P(φ | ∧Δ) holds; (Cs
P ) the diﬀerence in belief in the claim
conditioned on the evidence being true and belief in the claim conditioned on
the evidence being untrue, i.e. for it to be positive, P(φ | ¬(∧Δ)) < P(φ |
∧Δ) holds; and (Ck
P ) the diﬀerence in belief in the evidence conditioned on
the claim being true and belief in the evidence conditioned on the claim being
untrue, normalized by the maximum range for the value, i.e. for it to be positive,
P(∧Δ | ¬φ) < P(∧Δ | φ) holds.
Example 9. Consider S = ⟨a, b⟩with the following the distribution (left) and
strength measures (right) for Δ = {b} and φ = a. Here, the conditional prob-
ability gives a quite high score, whereas the conﬁrmation measures give lower
scores, reﬂecting the mass assigned to the models 10 and 01.
⟨a, b⟩
11
10
01
00
P
0.5
0.1
0.2
0.2
P(.) Sp
P
Sd
P
Ss
P
Sk
P
A
0.5
0.71
0.11
0.38
0.25

82
A. Hunter
We can harness conﬁrmation theory for argumentation (where arguments
concern relationships between observations) as below, where the greater the
value, the stronger the argument. In order to focus on the evidence, we consider
the ordinary facts (i.e. observations) in the support of the argument.
Deﬁnition 12. The conﬁrmation strength of argument A w.r.t. probability
distribution P and conﬁrmation measure Cx
P , for x ∈{d, s, k}, denoted Sx
P (A),
is Sx
P (A) = Cx
P (Ev(A), Claim(A)).
Example 10. Consider the following probability distribution (left), with signa-
ture ⟨d, p, f⟩, for an insectarium, where d is dragonﬂy, p is pollinator, and f is
ﬂying insect.
⟨d, p, f⟩
101
011
P
0.2
0.8
P(.)
Sp
P
Sd
P
Ss
P
Sk
P
A1
0.00
0.00
−0.80
−1.00
−1.00
A2 0.20
1.00
0.80
1.00
1.00
For arguments A1 = ⟨{d, d →p}, p⟩and A2 = ⟨{d, d →¬p}, ¬p⟩, the values
for argument strength are given in the table on the right above where P(.)
is the probability of the argument being acceptable. Since dragonﬂies are not
pollinators (as shown by the probability distribution), A1 has low scores, and
A2 has high scores, for conﬁrmation strength.
Example 11. Consider the probability distribution P, with signature ⟨b, p, f, n⟩,
where b is bird, p is penguin, f is capable of ﬂying, and N = {n}, for a zoo with
a large aviary.
⟨b, p, f, n⟩1100 1011 1000 0000
P
0.01
0.75
0.04
0.2
P(.) Sp
P
Sd
P
Ss
P
Sk
P
A1 0.75 0.94 0.19 0.94 0.67
A2 0.01 1.00 0.75 0.76 1.00
A3 0.01 1.00 0.75 0.76 1.00
For the arguments A1 = ⟨b, b ∧n →f}, f⟩, A2 = ⟨p, p →¬f}, ¬f⟩, and A3 =
⟨{p, p →¬n}, ¬n⟩, the strengths are given in the table on the above right. So A1
has a high probability of acceptability, and some good scores for conﬁrmation
strength, and A2 and A3 have a low probability of acceptability but are quite
strong arguments.
Proposition 4. The table captures satisfaction ✓, or non-satisfaction ×, of the
X1 to X7 properties.
X1 X2 X3 X4 X5 X6 X7
Sd
P
✓
✓
✓
✓
✓
✓
✓
Ss
P
×
✓
✓
×
✓
✓
✓
Sk
P
×
✓
✓
×
✓
✓
✓

Argument Strength in Probabilistic Argumentation
83
Proof. For counterexamples, consider the signature ⟨a, b⟩with ⟨{a, a →a}, a⟩
for X1, and ⟨{b, b →a}, a⟩for X4 and X7. (Sd
P ) (X1) Assume ∧Cn(Ev(A)) =
∧Cn(Claim(A)) = φ. So S(A) = P(φ|φ) −P(φ) = 1 −P(φ). So if P(φ) > 0, then
S(A) < 1. (X2) Direct from deﬁnition. (X3) If Sd
P (A) > 0, then P(Claim(A)| ∧
Ev(A))−P(Claim(A)) > 0. So P(Claim(A)|∧Ev(A)) > 0. (X4) Direct from deﬁni-
tion. (X5) From the assumption P(∧Ev(A)|Claim(A)) = P(∧Ev(A)|¬Claim(A)),
we can show P(Claim(A)| ∧Ev(A)) = P(Claim(A)). (X6) If P is uniform, then
P(Claim(A)| ∧Ev(A)) = 0.5 and P(Claim(A)) = 0.5. So Sd
P (A) = 0. (X7) From
the assumption P(Claim(A)| ∧Ev(A)) = P(Claim(A)|¬ ∧Ev(A)) we can show
P(Claim(A)| ∧Ev(A)) = P(Claim(A)). (Ss
P ) (X1) Let P(11) = 0.5 and P(00) =
0.5. (X2) Direct from deﬁnition. (X3) If Ss
P (A) > 0, then P(Claim(A)|∧Ev(A))−
P(Claim(A)|¬ ∧Ev(A)) > 0. So P(Claim(A)| ∧Ev(A)) > 0. (X4) Let P(11) = 1.
(X5) Follows from the fact that P(Claim(A)| ∧Ev(A)) = P(Claim(A)|¬ ∧Ev(A))
holds iﬀP(∧Ev(A)|Claim(A)) = P(∧Ev(A)|¬Claim(A)) holds. (X6) If P is uni-
form, then P(Claim(A)| ∧Ev(A)) = 0.5 and P(Claim(A)|¬ ∧Ev(A)) = 0.5. So
Ss
P (A) = 0. (X7) Direct from deﬁnition. (Sk
P ) (X1) Let P(11) = 0.5 and P(00) =
0.5. (X2) Direct from defn. (X3) If Sp
P (A) > 0, then P(Claim(A)|∧Ev(A)) > 0. So
P(Ev(A)|Claim(A)) > 0. (X4) Let P(11) = 1. (X5) Direct from deﬁnition. (X6)
If P is uniform, then P(∧Ev(A) ∧Claim(A)) = P(∧Ev(A))/2 and P(∧Ev(A) ∧
¬Claim(A)) = P(∧Ev(A))/2. Hence P(∧Ev(A)|Claim(A)) = ((P(∧Ev(A))/2)/0.5
= P(∧Ev(A)) and P(∧Ev(A)|¬Claim(A)) = ((P(∧Ev(A))/2)/0.5 = P(∧Ev(A)).
So P (∧Ev(A)|Claim(A))−P (∧Ev(A)|¬Claim(A)
P (∧Ev(A)|Claim(A))+P (∧Ev(A)|¬Claim(A) = P (∧Ev(A))−P (∧Ev(A))
P (∧Ev(A))+P (∧Ev(A)) = 0 (X7) Same as
for Ss
P X5.
Recall that the plausibility measure Sp
P only satisﬁes X2 and X3. So Sd
P , Ss
P
and Sk
P oﬀer potentially valuable alternatives to Sp
P as they satisfy most/all of
the properties. Note, four of these properties concern what zero strength means,
and so in future work, we will consider further properties to explore what positive
and negative strength mean, and to diﬀerentiate the measures.
7
Multiple Defeasible Rules
We now consider the question of the strength of an argument with multiple
defeasible rules in the premises. To illustrate some of our concerns, consider the
following arguments where b denotes bird, w denotes has wings, y denotes yellow,
f denotes capable of ﬂying.
– A1 = ⟨{b, b →f}, f⟩
– A2 = ⟨{b, b →w, w →f}, f⟩
– A3 = ⟨{b, b →y, y →f}, f⟩
Intuitively, A1 is a reasonably strong argument since most birds have the capa-
bility to ﬂy. But does A2 have the same strength as A1 since it starts from the
same fact (i.e. bird) or is it stronger because it makes the intermediate point
concerning having wings? And does A3 have the same strength as A1 or is it

84
A. Hunter
weaker because it makes the intermediate point (i.e. being yellow) that is irrel-
evant (and unlikely to be correct)? Assuming the probability distribution over
the atoms b and f is the same for each argument, then the strength of each
argument is the same since it is based on b and f. However, taking the rules into
account, we might expect the following: (A2) A strong conﬁrmation by birds for
has wings, and by has wings for capable of ﬂying; and (A3) A weak conﬁrmation
by birds for yellow, and a weak conﬁrmation by yellow for capable of ﬂying. To
capture this, we consider how the assessment of the strength of an argument can
depend on its intermediate steps.
Deﬁnition 13. Argument B is an intermediate of argument A iﬀRules(B) ⊆
Rules(A). Let Intermediates(A) = {B | B is an intermediate of A}.
Example 12. For A1 = ⟨{b, b →w, w →f}, f⟩, the intermediates are A1, B1 =
⟨{b, b →w}, w⟩, and B2 = ⟨{w, w →f}, f⟩,
If B is a strict intermediate of A (i.e. Rules(B) ⊂Rules(A)), and Claim(B) ̸=
Claim(A), then there is defeasible rule β1 ∧. . . ∧βn →φ ∈Support(A) where
Claim(B) ∈{β1, . . . , βn} (e.g. B1 in Example 12). This is because arguments
are minimal, and so if the claim of the intermediate diﬀers from that of the
argument, then it is also a condition in a defeasible rule. Also, if B is a strict
intermediate of A, it is not necessarily the case that Support(B) ⊂Support(A)
(e.g. B2 in Example 12). In order to consider the intermediates in the derivation
of a claim from its premises, we use the following deﬁnition that judges not just
the argument but also the intermediates.
Deﬁnition 14. An argument A is compositionally strong with respect to
strength measure SP and threshold τ ∈[−1, 1] iﬀfor all B ∈Intermediates(A),
SP (B) ≥τ.
We now return to the arguments A2 and A3 from the introduction of this
section, and analyze them in the following two examples.
Example 13. For A2 = ⟨{b, b →w, w →f}, f⟩, with the following probability dis-
tribution for a zoo, where S = ⟨b, w, f⟩, then A2 and its strict intermediates have
high strength.
⟨b, w, f⟩111 110 001 000
P
0.09 0.01 0.02 0.88
Sp
P
Sd
P
Ss
P
Sk
P
⟨{b, b →w, w →f}, f⟩0.90 0.79 0.88 0.97
⟨{b, b →w}, w⟩
1.00 0.90 1.00 1.00
⟨{w, w →f}, f⟩
0.90 0.79 0.88 0.97
Example 14. For A3 = ⟨{b, b →y, y →f}, f⟩, with the following probability dis-
tribution for a zoo, where S = ⟨b, y, f⟩, then A3 has the same high strength as
A2 in the previous example (because the marginals involving b and f are the
same) but low strength for the strict intermediates.
⟨b, y, f⟩111 101 100 010 001 000
P
0.01 0.08 0.01 0.02 0.02 0.86
Sp
P
Sd
P
Ss
P
Sk
P
⟨{b, b →y, y →f}, f⟩0.90 0.79 0.88 0.97
⟨{b, b →y}, y⟩
0.10 0.07 0.08 0.56
⟨{y, y →f}, f⟩
0.33 0.22 0.23 0.60

Argument Strength in Probabilistic Argumentation
85
In the same way that we consider the strength of an argument, we can con-
sider the strength of a defeasible rule.
Deﬁnition 15. The strength of rule ψ1 ∧. . . ∧ψn →φ is Sx
P (A) where A =
⟨{ψ1, . . . , ψn, ψ1 ∧. . . ∧ψn →φ}, φ⟩and P is a probability distribution and
x ∈{p, d, s, k}.
The following result says that if we want compositionally strong arguments,
then we only need to consider strong defeasible rules.
Proposition 5. If argument A is compositionally strong w.r.t. strength measure
Sx
P and threshold τ, then the strength of any rule ρ ∈Support(A) is greater than
or equal to τ.
Proof. An argument A is compositionally strong with respect to strength mea-
sure Sx
P and threshold τ ∈[−1, 1] iﬀfor all B ∈Intermediates(A), Sx
P (B) ≥τ. So
for all B ∈Intermediates(A) of the form B = ⟨{ψ1, . . . , ψn, ψ1∧. . .∧ψn →φ}, φ⟩,
Sx
P (B) ≥τ. So for all rules ρ = ψ1 ∧. . . ∧ψn →φ ∈Support(A), the strength of
ρ is greater than or equal to τ.
The above result means that if we do select strong defeasible rules for our
knowledgebase, then we do not risk missing strong arguments. In other words,
by rejecting weak defeasible rules, rules that are not going to lead to strong
arguments are eliminated from the knowledgebase.
8
Discussion
Returning to the audience scenario given in the introduction, the conﬁrmation
measures give us diﬀerent ways to judge the arguments, and decide which we
regard as strong arguments. For those arguments that we identify as weak, we
may argue with the person who provided the arguments in order to question
those arguments. Possibly, they may provide supporting arguments to back-up
the arguments under question, and if we are convinced by those supporting
arguments, we have the option to then update our probability distribution. The
protocol for this, the criteria for being convinced by the supporting arguments,
and the method for updating the probability distribution, are beyond the scope
of this paper, but this expanded scenario indicates how being able to analyze
the strength of the arguments presented by others is potentially useful as part
of an argumentation process.
Another application is in a persuasion scenario (for a review of persua-
sion see [17]). Assume we have a knowledgebase from which we can construct
arguments. In a dialogue with another agent, we may want to select the best
arguments to present in order to maximize the likelihood that they will be per-
suaded. For this, we can construct a probability distribution that reﬂects what
we think the other agent believes about the world. Then using that probability
distribution, we could select the stronger arguments to present.

86
A. Hunter
A third example of an application is an analytical scenario. If we have
acquired knowledge (perhaps from multiple sources), we may want to analyze
the quality of the arguments generated from that knowledge. We can construct
multiple probability distributions in order to investigate the arguments. Each
probability distribution could reﬂect a possible modelling of the world, and so
the change in strength for speciﬁc arguments could be investigated. Robustness
could be investigated by identifying how extreme the modelling would be for
arguments to be substantially weakened or strengthened. We leave the framework
for undertaking robustness analysis to future work.
So there are potential applications for measuring argument strength, but it
is an insuﬃciently understood notion in the literature on computational models
of arguments. To address this, we consider a very simple defeasible logic with
a clear semantics. This is so that we can get a clear understanding of the key
concepts. We could have used an existing proposal for argumentation, but then
the underlying issues we wanted to explore would be less clear in a more com-
plex framework (e.g. defeasible logic programming [12]). Nonetheless, we believe
this paper provides insights relevant for other argumentation systems, and so
in future work, we will adapt existing proposals for structured argumentation
systems (e.g. [12,23,35]) to quantify strength in a probabilistic context.
References
1. Amgoud, L., Ben-Naim, J.: Axiomatic foundations of acceptability semantics. In:
Proceedings of KR 2016, pp. 2–11. AAAI Press (2016)
2. Baroni, P., Rago, A., Toni, F.: From ﬁne-grained properties to broad principles
for gradual argumentation: a principled spectrum. Int. J. Approximate Reasoning
105, 252–286 (2019)
3. Hunter, A., et al.: Introduction to structured argumentation. Argument Comput.
5(1), 1–4 (2014)
4. Bonzon, E., Delobelle, J., Konieczny, S., Maudet, N.: A comparative study of
ranking-based semantics for abstract argumentation. In: Proceedings of AAAI 2016
(2016)
5. Carnap, R.: Logical Foundations of Probability, 2nd edn. University of Chicago
Press (1962)
6. Cayrol, C., Lagasquie-Schiex, M.: Graduality in argumentation. J. Artif. Intell.
Res. 23, 245–297 (2005)
7. Christensen, D.: Measuring conﬁrmation. J. Philos. 96, 437–461 (1999)
8. Cohen, A., Gottifredi, S., Tamargo, L., Garc´ıa, A., Simari, G.: An informant-
based approach to argument strength in defeasible logic programming. Argument
Comput. 12(1), 115–147 (2021)
9. Crupi, V.: Conﬁrmation, Spring 2020 edn. In: The Stanford Encyclopedia of Phi-
losophy. Metaphysics Research Lab, Stanford University (2020)
10. Dung, P.: On the acceptability of arguments and its fundamental role in non-
monotonic reasoning, logic programming and n-person games. Artif. Intell. 77(2),
321–358 (1995)
11. Dung, P., Thang, P.: Towards (probabilistic) argumentation for jury-based dispute
resolution. In: Proceedings of COMMA 2010, pp. 171–182. IOS Press (2010)

Argument Strength in Probabilistic Argumentation
87
12. Garcia, A., Simari, G.R.: Defeasible logic programming: an argumentative app-
roach. Theor. Pract. Logic Program. 4(1–2), 95–138 (2004)
13. Governatori, G., Maher, M., Antoniou, G., Billington, D.: Argumentation seman-
tics for defeasible logic. J. Logic Comput. 14(5), 675–702 (2004)
14. Haenni, R.: Modeling uncertainty with propositional assumption-based systems.
In: Hunter, A., Parsons, S. (eds.) Applications of Uncertainty Formalisms. LNCS
(LNAI), vol. 1455, pp. 446–470. Springer, Heidelberg (1998). https://doi.org/10.
1007/3-540-49426-X 21
15. Hunter, A.: A probabilistic approach to modelling uncertain logical arguments. Int.
J. Approximate Reasoning 54(1), 47–81 (2013)
16. Hunter, A.: Generating instantiated argument graphs from probabilistic informa-
tion. In: Proceedings of ECAI 2020. IOS Press (2020)
17. Hunter, A., Chalaguine, L., Czernuszenko, T., Hadoux, E., Polberg, S.: Towards
computational persuasion via natural language argumentation dialogues. In:
Benzm¨uller, C., Stuckenschmidt, H. (eds.) KI 2019. LNCS (LNAI), vol. 11793,
pp. 18–33. Springer, Cham (2019). https://doi.org/10.1007/978-3-030-30179-8 2
18. Hunter, A., Thimm, M.: Probabilistic reasoning with abstract argumentation
frameworks. J. Artif. Intell. Res. 59, 565–611 (2017)
19. Kemeny, J., Oppenheim, P.: Degrees of factual support. Philos. Sci. 19, 307–324
(1952)
20. Li, H., Oren, N., Norman, T.J.: Probabilistic argumentation frameworks. In: Mod-
gil, S., Oren, N., Toni, F. (eds.) TAFA 2011. LNCS (LNAI), vol. 7132, pp. 1–16.
Springer, Heidelberg (2012). https://doi.org/10.1007/978-3-642-29184-5 1
21. Matt, P.-A., Toni, F.: A game-theoretic measure of argument strength for abstract
argumentation. In: H¨olldobler, S., Lutz, C., Wansing, H. (eds.) JELIA 2008. LNCS
(LNAI), vol. 5293, pp. 285–297. Springer, Heidelberg (2008). https://doi.org/10.
1007/978-3-540-87803-2 24
22. McCarthy, J.: Circumscription: a form of non-monotonic reasoning. Artif. Intell.
28(1), 89–116 (1980)
23. Modgil, S., Prakken, H.: The ASPIC+ framework for structured argumentation: a
tutorial. Argument Comput. 5, 31–62 (2014)
24. Pfeifer, N.: On Argument strength. In: Zenker, F. (eds.) Bayesian Argumentation.
Synthese Library. Studies in Epistemology, Logic, Methodology, and Philosophy of
Science, vol 362. Springer, Dordrecht (2013). https://doi.org/10.1007/978-94-007-
5357-0 10
25. Polberg, S., Hunter, A.: Empirical evaluation of abstract argumentation: support-
ing the need for bipolar and probabilistic approaches. Int. J. Approx. Reason. 93,
487–543 (2018)
26. Polberg, S., Hunter, A., Thimm, M.: Belief in attacks in epistemic probabilistic
argumentation. In: Moral, S., Pivert, O., S´anchez, D., Mar´ın, N. (eds.) SUM 2017.
LNCS (LNAI), vol. 10564, pp. 223–236. Springer, Cham (2017). https://doi.org/
10.1007/978-3-319-67582-4 16
27. Pollock, J.: Defeasible reasoning. Cogn. Sci. 11(4), 481–518 (1987)
28. Pollock, J.: Cognitve Carpentry. MIT Press (1995)
29. Prakken, H.: Probabilistic strength of arguments with structure. In: Proceedings
of KR 2018, pp. 158–167. AAAI Press (2018)
30. Riveret, R., Governatori, G.: On learning attacks in probabilistic abstract argu-
mentation. In: Proceedings of the AAMAS 2016, pp. 653–661 (2016)
31. Riveret, R., Rotolo, A., Sartor, G., Prakken, H., Roth, B.: Success chances in
argument games: a probabilistic approach to legal disputes. In: Proceedings of
JURIX 2007, pp. 99–108. IOS Press (2007)

88
A. Hunter
32. Shakarian, P., et al.: Belief revision in structured probabilistic argumentation -
model and application to cyber security. Ann. Math. Artif. Intell. 78(3–4), 259–
301 (2016)
33. Simari, G., Loui, R.: A mathematical treatment of defeasible reasoning and its
implementation. Artif. Intell. 53(2–3), 125–157 (1992)
34. Thimm, M.: A probabilistic semantics for abstract argumentation. In: Proceedings
of ECAI 2012 (2012)
35. Toni, F.: A tutorial on assumption-based argumentation. Argument Comput. 5(1),
89–117 (2014)
36. Verheij, B.: Arguments and their strength: revisiting Pollock’s anti-probabilistic
starting points. In: Proceedings of COMMA 2014. IOS Press (2014)

The Degrees of Monotony-Dilemma in
Abstract Argumentation
Timotheus Kampik1(B) and Dov Gabbay2,3,4
1 Ume˚a University, Ume˚a, Sweden
tkampik@cs.umu.se
2 University of Luxembourg, Esch-sur-Alzette, Luxembourg
dov.gabbay@kcl.ac.uk
3 King’s College London, London, UK
4 Bar Ilan University, Ramat Gan, Israel
Abstract. In this paper, we introduce the notion of the degree of
monotony to abstract argumentation, a well-established method for
drawing inferences in face of conﬂicts in non-monotonic reasoning.
Roughly speaking, the degree of monotony allows us, given an abstract
argumentation semantics and an abstract argumentation framework to
be as monotonic as possible, when iteratively drawing inferences and
expanding the argumentation framework. However, we also show that
when expanding an argumentation framework several times using so-
called normal expansions, an agent may, at any given step, select a con-
clusion that has the highest degree of monotony w.r.t. the previous con-
clusion (considering the constraints of the semantics), but end up with a
conclusion that has a suboptimal degree of monotony w.r.t. one or sev-
eral conclusions that precede the previous conclusion. We formalize this
observation as the degrees of monotony-dilemma.
Keywords: Abstract argumentation · Non-monotonic reasoning ·
Argumentation dynamics
1
Introduction
When reasoning in dynamic environments, we expect that an intelligent agent
can draw inferences in a non-monotonic manner, i.e. that the agent is able to
reject previously inferred statements on the basis of newly acquired beliefs. In
research on formal methods of reasoning, the design and analysis of inference
approaches that systematically relax monotony of entailment has been an active
ﬁeld of research since several decades, in particular since the emergence of relaxed
monotony principles like restricted [11] and rational monotony [13,15]. More
recently, similar principles [4,7,12] have been introduced to formal argumenta-
tion approaches, which constitute a particularly vibrant research area related to
the study of knowledge representation and reasoning [2]. In this paper, we pro-
vide a new perspective on relaxed monotony in the context of formal argumen-
tation that we base on a measure of monotony, instead of on Boolean conditions.
c
⃝Springer Nature Switzerland AG 2021
J. Vejnarov´a and N. Wilson (Eds.): ECSQARU 2021, LNAI 12897, pp. 89–102, 2021.
https://doi.org/10.1007/978-3-030-86772-0_7

90
T. Kampik and D. Gabbay
Assuming that we have an agent that iteratively infers possible conclusions (sets
of atoms) from a belief base, selects a conclusion, and then expands the belief
base to again infer possible conclusions (and so on), the agent measures the
degree of monotony of the conclusions she can infer w.r.t. a previous conclusion
she selected, and then commits to an inference result that implies a maximal
degree of monotony, i.e. to a conclusion that rejects as few of the previously
inferred statements as possible. This can, however, lead to a dilemma: if the
belief base is expanded several times, and the agent draws inferences after all
expansions, selecting a conclusion that is as monotonic as possible w.r.t. one
of the previously inferred conclusions may lead to the selection of a “subop-
timally” monotonic conclusion w.r.t. another previous inference result, even if
our agent always selects the conclusion that is maximally monotonic (considering
constraints imposed by the inference function) w.r.t. the immediately preceding
inference result. Indeed, this paper shows that this dilemma occurs in the con-
text of several argumentation semantics that Dung deﬁnes in his seminal paper
on abstract argumentation [10].
Let us introduce the following informal example to further motivate the
research this paper presents.
Example 1. We have an agent A1 that draws inferences from a belief base BB.
Over time, the agent acquires new beliefs and adds them to the belief base,
i.e. in our example, we have the belief bases BB1, BB2 = BB1 ∪BB′
1, and
BB3 = BB2 ∪BB′
2. Any newly added belief may conﬂict with any other belief;
our agent’s inference function concls resolves conﬂicts at run-time and draws
new inferences every time the agent updates the belief base. In addition, we
have two agents A2 and A3 that observe the inferences that agent A1 draws at
diﬀerent steps.
– A2 observes A1’s inferences from BB1 and BB3.
– A3 observes A1’s inferences from BB2 and BB3.
At any point, A1’s belief base may contain conﬂicting beliefs and hence allows
for a range of alternative conclusions (sets of atoms), e.g.:
1. concls(B1) = {C1,1} = {{a, b}};
2. concls(B2) = {C2,1, C2,2} = {{a, b, c, d, e}, {c, d, e, f}};
3. concls(B3) = {C3,1, C3,2} = {{a, b, g}, {c, d, e, f}}.
A1’s inference function is non-monotonic: a belief that has been inferred as true
may be rejected by a future inference after a belief update. Still, to ensure some
degree of consistency in her decision process, A1 attempts to be as monotonic
as possible, using a measure that determines the degree of monotony of a newly
inferred conclusion C′ w.r.t. a previously inferred conclusion C. We denote the
degree of monotony of C′ w.r.t. C by degmon(C′, C), and deﬁne degmon(C′, C) =
|C ∩C′|
|C|
if |C| ̸= 0; 1, otherwise. With this, A1 proceeds as follows:
– degmon(C2,1, C1,1) = 1 and degmon(C2,2, C1,1) = 0: after having inferred C1,1
and having expanded the belief base from BB1 to BB2, A1 chooses to infer

The Degrees of Monotony-Dilemma in Abstract Argumentation
91
C2,1 from BB2, because it ensures a higher degree of monotony w.r.t. C1,1
than inferring C2,2.
– degmon(C3,1, C2,1) = 2
5 and degmon(C3,2, C2,1) = 3
5; according to these mea-
sures, A1 should choose to infer C3,2, which means A3 observes that A1’s
inference is as monotonic as possible.
– However, degmon(C3,1, C1,1) = 1 and degmon(C3,2, C1,1) = 0; according to
these measures, A1 should choose to infer C3,1, which means A2 observes
that A1’s inference is as monotonic as possible. We say that A1 is in a degrees
of monotony-dilemma, because she can maximize the degree of monotony
merely w.r.t. either C2,1 or C1,1.
The reader may consider this scenario a (simpliﬁed) model of the follow-
ing example use-case. Our agents represent diﬀerent IT systems in a complex
enterprise system landscape1. A1 is the so-called leading system, and provides
business insights to A2 and A3, which these two systems then use to update their
own conﬁgurations. From a practical perspective, we can realistically claim that
A2 and A3 may observe A1’s inferences with diﬀerent frequencies; for exam-
ple, A2 could be a legacy system, whose synchronization with A1 runs as an
overnight batch job, whereas A3 is a modern system that listens to updates
from A1, approximately in real-time. Every time A2 or A3 observe that A1 has
reversed a previously inferred statement, this implies invasive changes to the
systems’ conﬁgurations and operations. For example, active cases might need to
get re-routed or even require manual (human) intervention. Hence, for both A2
and A3, the inferences the systems observe should be as monotonic as possible.
However, because of the diﬀerences in observation frequencies, an inference that
is maximally monotonic w.r.t. the last update that A2 has observed may not be
maximally monotonic w.r.t. the last update that A3 has observed.
We formalize the notion of a degree of monotony for abstract argumenta-
tion [10], an approach for drawing inferences from conﬂicting beliefs (or: argu-
ments) that has gained tremendous attention in the symbolic artiﬁcial intelli-
gence community. Then, we show that for most2 of Dung’s original argumenta-
tion semantics (inference functions), the degrees of monotony-dilemma in fact
occurs as a problem and present an initial mitigation approach.
The rest of this paper is organized as follows. Section 2 provides the theoreti-
cal preliminaries of abstract argumentation. Then, Sect. 3 introduces the notion
of the degree of monotony to abstract argumentation, based on which Sect. 4
formally deﬁnes and analyzes the degrees of monotony-dilemma, and also out-
lines a mitigation approach. Subsequently, Sect. 5 discusses the ﬁndings in the
light of related and potential future research, before Sect. 6 concludes the paper.
1 For an introduction to academic literature on the topic, see Lankhorst [14] or
Dumas et al. [9], for example.
2 Precisely, the problem occurs for three of the four semantics. The exception is
grounded semantics, which always infers exactly one extension and is typically
regarded as “too skeptical” for many use-cases.

92
T. Kampik and D. Gabbay
2
Theoretical Preliminaries
The central model of this paper is an (abstract) argumentation framework, as
introduced by Dung [10].
Deﬁnition 1 (Argumentation Framework [10]). An argumentation frame-
work AF is a tuple (AR, AT), such that AR is a set of elements (called argu-
ments) and AT ⊆AR × AR (called attacks).
We assume that the set of arguments in an argumentation framework is
ﬁnite. Given an argumentation framework AF = (AR, AT), and a, b ∈AR, we
say that “a attacks b” iﬀ(a, b) ∈AT. For S ⊆AR, we say that “S attacks a”
iﬀ∃(c, a) ∈AT, such that c ∈S. We say that “S defends a” iﬀ∀d ∈AR, such
that d attacks a, it holds true that S attacks d. To model dynamics in abstract
argumentation, Baumann and Brewka introduce the notion of argumentation
framework expansions [4].
Deﬁnition 2 (Argumentation Framework Expansions [4]).
Let AF =
(AR, AT) and AF ′ = (AR′, AT ′) be argumentation frameworks.
– AF ′ is an expansion of AF (denoted by AF ⪯AF ′) iﬀAR ⊆AR′ and
AT ⊆AT ′.
– AF ′ is a normal expansion of AF (denoted by AF ⪯N AF ′) iﬀAF ⪯AF ′
and (AR × AR) ∩(AT ′ \ AT) = {}.
– AF ′ is a weak expansion of AF (denoted by AF ⪯W AF ′) iﬀAF ⪯N AF ′
and ∀a ∈AR, AR′ \ AR does not attack a.
In words, an expansion of an argumentation framework AF adds new argu-
ments and attacks to AF, without removing “old” arguments or attacks, a nor-
mal expansion of AF is an expansion of AF that only adds attacks involving at
least one new argument, and a weak expansion of AF is a normal expansion of
AF in which no new argument attacks an “old” argument. Let us provide an
example that illustrates the notions of expansions and normal expansions.
Example 2. Consider the following argumentation frameworks: i) AF
=
(AR, AT) = ({a, b}, {(a, b)}); ii) AF ′ = (AR′, AT ′) = ({a, b, c}, {(a, b), (b, c),
(c, a)}); iii) AF ′′ = (AR′′, AT ′′) = ({a, b, c}, {(a, b), (b, c), (c, a), (b, a)}). AF ′
is a normal expansion of AF (AF ⪯N AF ′): AR ⊆AR′, AT ⊆AT ′, and
Fig. 1. AF ⪯N AF ′ and AF ′ ⪯AF ′′, but AF ̸⪯N AF ′′ and AF ′ ̸⪯N AF ′′.

The Degrees of Monotony-Dilemma in Abstract Argumentation
93
(AR × AR) ∩(AT ′ \ AT) = {}. This implies that AF ′ is an expansion of AF
(AF ⪯AF ′). In contrast, AF ′ ⪯AF ′′, but AF ′ ̸⪯N AF ′′: AR′ ⊆AR′′ and
AT ′ ⊆AT ′′, but (AR′ × AR′) ∩(AT ′′ \ AT ′) ̸= {}. Also, AF ⪯AF ′′, but
AF ̸⪯N AF ′′. In this example, we do not have any weak expansions. Counter-
factually, if AF ′ (which is a normal expansion of AF) did not feature the attack
(c, a), it would be a weak expansion of AF. Figure 1 depicts the example’s argu-
mentation frameworks.
Let us now deﬁne the notions of conﬂict-free and admissible sets of argu-
ments, which play a crucial role in abstract argumentation.
Deﬁnition 3 (Conﬂict-Free
and
Admissible
Sets [10]). Let AF
=
(AR, AT) be an argumentation framework. A set S ⊆AR:
– is conﬂict-free iﬀ∄a, b ∈S such that a attacks b;
– is admissible iﬀS is conﬂict-free and ∀a ∈S, S defends a.
In his seminal paper that introduces abstract argumentation, Dung deﬁnes
four argumentation semantics, functions that take an argumentation framework
and return a set of extensions, where each extension is a set of arguments. We
say that an argumentation semantics is universally deﬁned iﬀit yields at least
one extension for every argumentation framework.
Deﬁnition 4 (Stable, Complete, Preferred, and Grounded Semantics
and Extensions [10]). Let AF = (AR, AT) be an argumentation framework.
A set S ⊆AR is a:
– stable extension of AF iﬀS is admissible and S attacks each argument that
does not belong to S. Stable semantics σstable(AF) denotes all stable exten-
sions of AF;
– complete extension of AF iﬀS is admissible and each argument that is
defended by S belongs to S. Complete semantics σcomplete(AF) denotes all
complete extensions of AF;
– preferred extension of AF iﬀS is a maximal (w.r.t. set inclusion) admis-
sible subset of AR. Preferred semantics σpreferred(AF) denotes all preferred
extensions of AF;
– grounded extension of AF iﬀS is the minimal (w.r.t. set inclusion) complete
extension of AF. Grounded semantics σgrounded(AF) denotes all grounded
extensions of AF.
Note that i) for some argumentation frameworks there does not exist a sta-
ble extension (consider the argumentation framework AF = ({a, b}, {(a, a)}); ii)
each preferred extension is also a complete extension [1]; iii) for every argumen-
tation framework, there exists exactly one grounded extension [3]. Because this
work deals with the selection of extensions from the set of extensions a semantics
returns, we focus our analysis on complete and preferred semantics, i.e. the two
original Dung-semantics that are universally deﬁned [3] but may return more
than one extension. Let us note that the four semantics all rely on the notion of

94
T. Kampik and D. Gabbay
admissibility. Over the past decades, additional abstract argumentation seman-
tics families have been designed that are not admissible set-based, most notably
naive set-based semantics [17] and weak admissible set-based semantics [5]. How-
ever, for the sake of conciseness, we do not consider these semantics families in
our analysis. Finally, let us provide the deﬁnition of the weak cautious monotony
argumentation principle, which describes argumentation semantics that satisfy
a notion of relaxed monotony.
Deﬁnition 5 (Weak
Cautious
Monotony
in
Abstract
Argumenta-
tion [12]).
Let σ be an argumentation semantics. σ satisﬁes weak cautious
monotony iﬀfor every two argumentation frameworks AF = (AR, AT) and
AF ′ = (AR′, AT ′), such that AF ⪯N AF ′, and ∀E ∈σ(AF), it holds true that
if {(a, b) | (a, b) ∈AT ′, a ∈AR′ \ AR, b ∈E} = {} then ∃E′ ∈σ(AF ′) such that
E ⊆E′.
3
Degrees of Monotony
This section formalizes the notion of the degree of monotony in the context of
abstract argumentation and provides a brief theoretical analysis on how normally
expanding an argumentation framework may aﬀect the degree of monotony of
an extension inferred from the argumentation framework expansion w.r.t. an
extension inferred from the original argumentation framework.
Deﬁnition 6 (Degree of Monotony).
Let E and E′ be two ﬁnite sets
of arguments. We deﬁne the degree of monotony of E′ w.r.t. E, denoted by
degmon(E′, E), as follows:
degmon(E′, E) =

1
if|E| = 0;
|E′∩E|
|E|
otherwise.
The degree of monotony tells us to what extent we violate monotony/are
non-monotonic in a dynamic inference scenario, in which we draw conclusions
from a changing knowledge base. Having such a measure can be useful because
in practice, knowledge bases typically change with time, and the “more non-
monotonic” (colloquially speaking) we are, the more careful we need to be when
assessing the consequences of the changed inference, for example when updat-
ing documents and IT system conﬁgurations to accommodate the change. The
normalization (division by |E|) can have practical advantages as it facilitates
benchmarking. Let us highlight that potentially, argumentation variants that
extend abstract argumentation could assign a weight to each argument in an
argumentation framework, and this weight could then inform the costs of violat-
ing monotony, roughly speaking. However, for the sake of conciseness, we abstain
from introducing such a formal argumentation framework variant here. Let us
prove the inﬁmum and supremum of the degree of monotony.
Proposition 1. Given two ﬁnite sets of arguments E and E′, the inﬁmum of
degmon(E′, E) is 0 and the supremum of degmon(E′, E) is 1.

The Degrees of Monotony-Dilemma in Abstract Argumentation
95
Proof. Let us ﬁrst prove the inﬁmum. By deﬁnition of the degree of monotony
(Deﬁnition 6), degmon(E′, E) is minimal iﬀ|E′ ∩E| = 0 and |E| > 0. Given E
and E′, such that |E| > 0 and |E′ ∩E| = 0, it holds that degmon(E′, E) = 0.
Let us now prove the supremum. By deﬁnition of the degree of monotony
(Deﬁnition 6), degmon(E′, E) is maximal iﬀE′ ∩E = E. Given E and E′, such
that E′ ∩E = E, it holds that degmon(E′, E) = 1.
⊓⊔
We deﬁne a notion that allows us to select the monotony-maximizing exten-
sions w.r.t. a previously inferred set of arguments E from the extensions an
argumentation semantics infers from an argumentation framework.
Deﬁnition 7 (Monotony-Maximizing Extensions). Let AF = (AR, AT)
be an argumentation framework. Let σ be a universally deﬁned argumentation
semantics, and let S be a ﬁnite set of arguments. We deﬁne the monotony-
maximizing σ-extensions of AF w.r.t. S, denoted by Extsmon(S, AF, σ), as
{E|E ∈σ(AF), ∄E′ ∈σ(AF), such that degmon(E, S) < degmon(E′, S)}.
Let us introduce an example to illustrate the notions of the degree of
monotony and monotony maximizing extensions in the context of an argumen-
tation framework and one of its normal expansions.
Example 3. Let us consider the following argumentation frameworks (AF and
AF ′, such that AF ⪯N AF ′):
– AF = (AR, AT) = ({a, b, c, d}, {(a, b), (b, a), (b, c), (d, c)}).
– AF ′ = (AR′, AT ′) = ({a, b, c, d, e}, {(a, b), (b, a), (b, c), (d, c), (a, e), (e, d)}).
Let us consider complete, preferred, and grounded semantics3.
– Complete semantics: σcomplete(AF) = {{d}, {a, d}, {b, d}} and σcomplete(AF ′)
=
{{}, {a, d}, {b, e}}.
Given
Ec
=
{d}, Ec
∈
σcomplete(AF),
we have Extsmon(Ec, AF ′, σcomplete) = {{a, d}} and degmon({a, d}, Ec) = 1.
– Preferred semantics: σpreferred(AF) = {{a, d}, {b, d}} and σpreferred(AF ′) =
{{a, d}, {b, e}}.
Given
Ep
=
{b, d}, Ep
∈
σpreferred(AF),
we
have
Extsmon(Ep, AF ′, σpreferred) = {{a, d}, {b, e}} and degmon({a, d}, Ep) =
degmon({b, e}, Ep) = 1
2.
– Grounded semantics: σgrounded(AF) = {{d}} and σgrounded(AF ′) = {{}}.
Given Eg = {d}, Eg ∈σgrounded(AF), we have Extsmon(Eg, AF ′, σgrounded) =
{{}} and degmon({}, Eg) = 0.
3 We do not consider stable semantics for the sake of conciseness, and because stable
semantics is not universally deﬁned.

96
T. Kampik and D. Gabbay
Figure 2 depicts the example’s argumentation frameworks.
Fig. 2. Example argumentation frameworks to illustrate degrees of monotony.
Intuitively (and roughly speaking), the maximal degree of monotony may be
0 even in case of a small expansion of a large initial argumentation framework
from which we infer a large set of arguments.
Proposition 2.
Let σx be an argumentation semantics, such that x ∈
{stable, complete, preferred, grounded}, and let n ∈N, n > 1. There exist
two argumentation frameworks AF = (AR, AT), AF ′ = (AR′, AT ′), such that
|AR| = n + 1, AF ⪯N AF ′, |AR′ \ AR| = 1, |AT ′ \ AT| = 1, and ∃E ∈σ(AF),
such that |E| = n and ∀E′ ∈Extsmon(E, AF ′, σx), degmon(E′, E) = 0.
Proof. Let us consider the following argumentation framework constructions:
– AF
=
(AR, AT), such that AR
=
{a1, . . . , an, an+1} and AT
=
{(an+1, ai)|1 ≤i < n} ∪{(an, an+1)};
– AF ′ = (AR′, AT ′), such that AR′ = AR ∪{b}, b ̸∈AR and AT ′ = AT ∪
{(b, an)}.
Note that AF ⪯N AF ′ and |AR′ \ AR| = 1, |AT ′ \ AT| = 1. We observe that
σx(AF) = {E}, such that E = {a1, . . . , an}, |E| = n and σx(AF ′) = {{b, an+1}}.
Hence, ∀E such that E ∈σx(AF), ∀E′ ∈Extsmon(E, AF ′, σx), degmon(E′, E) =
0. This proves the proposition4.
⊓⊔
In contrast, if a normal expansion of an argumentation framework does not
add any attacks to a speciﬁc extension E that has been inferred from the origi-
nal argumentation framework, using complete, preferred, or grounded semantics,
then we can infer an extension E′ from the expansion such that the degree of
monotony of E′ w.r.t. E is 1. To prove this, let us ﬁrst introduce the notion of
(normal) E-weak expansions. Note that this deﬁnition draws upon the theoret-
ical preliminaries provided by Deﬁnitions 2 and 5.
Deﬁnition 8 ((Normal) E-Weak Expansions). Let AF = (AR, AT) and
AF ′ = (AR′, AT ′) be argumentation frameworks and let E ⊆AR.
4 Note that the fact that stable semantics is not universally deﬁned is not relevant for
this proof, as we prove the existence of a phenomenon, roughly speaking.

The Degrees of Monotony-Dilemma in Abstract Argumentation
97
– AF ′ is an E-weak expansion of AF (denoted by AF ⪯W (E) AF ′) iﬀAF ⪯
AF ′ and {(a, b) | (a, b) ∈AT ′, a ∈AR′ \ AR, b ∈E} = {}.
– AF ′ is an E-weak normal expansion of AF (denoted by AF ⪯NW (E) AF ′)
iﬀAF ⪯N AF ′ and {(a, b) | (a, b) ∈AT ′, a ∈AR′ \ AR, b ∈E} = {}.
Note that colloquially speaking, we can say that cautious monotony (Deﬁni-
tion 5) is monotony in the face of normal E-weak expansions.
Now, we can introduce and prove the proposition.
Proposition 3.
Let σx be an argumentation semantics, such that x ∈
{complete, preferred, grounded}. For every argumentation framework AF =
(AR, AT), ∀E ∈σx(AF) and for every argumentation AF ′ = (AR′, AT ′), such
that AF ⪯NW (E) AF ′, ∃E′ ∈σx(AF ′) such that degmon(E′, E) = 1.
Proof. Let us provide a proof by contradiction. Let us assume that ∃E ∈σx(AF),
such that ∄E′ ∈σx(AF ′) and degmon(E′, E) = 1.
1. By deﬁnition of the degree of monotony (Deﬁnition 6), the following statement
holds true:
∀E ∈σ(AF),
if ∄E′ ∈σ(AF ′), such that degmon(E′, E) = 1
then ∄E′ ∈σ(AF ′), such that E ⊆E′
2. Because AF ⪯N AF ′, by deﬁnition of σx (Deﬁnition 4, σx is universally
deﬁned and each σx-extension contains all arguments it defends; consider
that each preferred/grounded extension also is a complete extension), the
following statement holds true:
∀E ∈σ(AF),
if ∄E′ ∈σ(AF ′), such that E ⊆E′
then ∃a ∈E, such that a is attacked by AR′ \ AR
3. From 1. and 2., it follows that the following statement holds true:
∀E ∈σ(AF),
if ∄E′ ∈σ(AF ′), such that degmon(E′, E) = 1
then ∃a ∈E, such that a is attacked by AR′ \ AR
4. From 3. it follows that ∀E
∈
σx(AF) if ∄E′
∈
σ(AF ′) such that
degmon(E′, E) ̸= 1 then AF ̸⪯NW (E) AF ′. Contradiction! This proves the
proposition.
⊓⊔
Let us claim this proof entails the proof that complete, grounded, and pre-
ferred semantics satisfy cautious monotony and note that there exist argu-
mentation semantics for which an analogous proposition does not hold true,
consider e.g., stable semantics and the argumentation frameworks AF
=
({a}, {}), AF ′ = ({a, b}, {(b, b)}). This claim can be extended to other semantics
like stage semantics [17], by considering the argumentation frameworks AF =
({a, b, c}, {(a, b), (b, c), (c, a)}), AF ′ = ({a, b, c, d}, {(a, b), (b, c), (c, a), (d, a)}).

98
T. Kampik and D. Gabbay
4
The “Degrees of Monotony”-Dilemma
This section formally introduces the notion of the degrees of monotony-dilemma
and deﬁnes a possible mitigation approach.
Deﬁnition 9 (The Degrees of Monotony-Dilemma).
Let σ be an argu-
mentation semantics. σ is aﬀected by the degrees of monotony-dilemma iﬀthere
exist argumentation frameworks AF = (AR, AT), AF ′ = (AR′, AT ′), and
AF ′′ = (AR′′, AT ′′), such that AF ⪯N AF ′, AF ′ ⪯N AF ′′ and the following
statement does not hold true:
∀E ∈σ(AF), ∀E′ ∈σ(AF ′), such that E′ ∈Extsmon(E, AF ′, σ),
∃E′′ ∈σ(AF ′′), such that E′′ ∈Extsmon(E′, AF ′′, σ)
and E′′ ∈Extsmon(E, AF ′′, σ)
Let us note that we constrain the deﬁnition to normal expansions because
arguments can be defeated instead of removed to keep track of history and the
addition of new attacks between existing arguments can be emulated by adding
new arguments as well (for an analogy, think of immutable variables in some
programming languages). Moreover, the constraint allows us to show that the
degrees of monotony-dilemma occurs even under these somewhat strict con-
ditions. Complete and preferred semantics (the two universally deﬁned Dung-
semantics that may return more than one extension for a given argumentation
framework) are aﬀected by the degrees of monotony-dilemma.
Proposition 4 (Complete and Preferred Semantics are Aﬀected by the
Degrees of Monotony-Dilemma).
Let σx be an argumentation semantics,
such that x ∈{complete, preferred}. σx is aﬀected by the degrees of monotony-
dilemma.
Proof. We provide a proof by counter-example. Let us consider the following
argumentation frameworks: i) AF = (AR, AT) = ({a, b, c}, {(a, b), (b, a), (b, c)});
ii) AF ′ = (AR′, AT ′) = ({a, b, c, d, e, f}, {(a, b), (b, a), (b, c)}); iii) AF ′′ =
(AR′′, AT ′′) = ({a, b, c, d, e, f, g}, {(a, b), (b, a), (b, c), (b, g), (g, d), (g, e), (g, f)}).
Let us observe that AF ⪯N AF ′ and AF ′ ⪯N AF ′′ and that: i) σpreferred(AF) =
{{b}, {a, c}} and σcomplete(AF) = σpreferred(AF) ∪{{}}; ii) σpreferred(AF ′) =
{{b, d, e, f}, {a, c, d, e, f}} and σcomplete(AF ′) = σpreferred(AF ′) ∪{{d, e, f}}; iii)
σpreferred(AF ′′) = {{b, d, e, f}, {a, c, g}} and σcomplete(AF ′′) = σpreferred(AF ′′) ∪
{{}}.
Let us consider E
=
{a, c}, E
∈
σx(AF). Extsmon(E, AF ′, σx)
=
{{a, c, d, e, f}}. Given E′
= {a, c, d, e, f}, E′
∈Extsmon(E, AF ′, σx), we
have Extsmon(E′, AF ′′, σx) = {{b, d, e, f}}. However, Extsmon(E, AF ′′, σx) =
{{a, c, g}}. Hence, we can conclude that given E = {a, c}, the following state-
ment does not hold true:
∀E′ ∈σx(AF ′), such that E′ ∈Extsmon(E, AF ′, σx), ∃E′′ ∈σx(AF ′′),
such that E′′ ∈Extsmon(E′, AF ′′, σx) and E′′ ∈Extsmon(E, AF ′′, σx)

The Degrees of Monotony-Dilemma in Abstract Argumentation
99
By deﬁnition of the degrees of monotony-dilemma (Deﬁnition 9), this proves the
proposition.
⊓⊔
Fig. 3. Complete and preferred semantics are aﬀected by the degrees of monotony-
dilemma.
Figure 3 illustrates the argumentation frameworks that are used in the proof
of Proposition 4. The proposition highlights that the degrees of monotony-
dilemma that we have colloquially described in the introduction does indeed
occur in the context of abstract argumentation. Let us observe that grounded
semantics is not aﬀected by the degrees of monotony-dilemma, as it is uni-
versally uniquely deﬁned [3]5. In contrast, given an argumentation frame-
work and its normal expansion, stable semantics may yield an extension for
the original framework, but return no extension for the expansion (consider
AF = ({a}, {}), AF ′ = ({a, b}, {(b, b)})); i.e., even the more basic problem
of maximally monotonic extension selection cannot be guaranteed for stable
semantics.
To mitigate the degrees of monotony-dilemma, we can, for instance, compute
a weighted average of several degrees of monotony, considering the results of
past inferences. Let us ﬁrst deﬁne the notion of the weighted average degree of
monotony.
Deﬁnition 10 (Weighted Average Degree of Monotony). Let E′ be a
ﬁnite set of arguments and let ES = ⟨E0, . . . , En⟩be a ﬁnite sequence of ﬁnite
sets of arguments. We deﬁne the weighted average degree of monotony (denoted
by degγ,wmon(E′, ES)) as follows:
degγ,wmon(E′, ES) =
n
i=0 γ(n−i)degmon(E′, Ei)
|n + 1|
,
where 0 < γ ≤1.
5 However, grounded semantics is typically considered a simplistic approach to
abstract argumentation, roughly speaking because it is too skeptical.

100
T. Kampik and D. Gabbay
Note that in the deﬁnition, γ can be considered the discount factor, and a
smaller γ gives a higher importance to recent inferences (relative to less recent
inferences), roughly speaking.
Now, we can deﬁne the mitigation approach.
Deﬁnition 11 (Weighted Average Monotony-Maximizing Extensions).
Let AF = (AR, AT) be an argumentation framework, let σ be an argumentation
semantics, and let ES = ⟨E0, . . . , En⟩be a ﬁnite sequence of ﬁnite sets of argu-
ments. We deﬁne the weighted average monotony-maximizing σ-extensions of
AF w.r.t. ES, denoted by Extsγ,wmon(ES, AF, σ) as {E|E ∈σ(AF), ∄E′ ∈
σ(AF), such that degγ,wmon(E, ES) < degγ,wmon(E′, ES)}, where 0 < γ ≤1.
Let us illustrate this approach using an example.
Example 4. We consider preferred semantics and the argumentation frameworks
depicted by Fig. 3. We start by “selecting” E0 = {a, b}, E0 ∈σpreferred(AF).
Then, we have Extsmon(E, AF ′, σpreferred) = Extsγ,wmon(⟨E⟩, AF ′, σpreferred) =
{E1}, such that E1 = {a, c, d, e, f}. We assume γ = 0.9. Now, we want to select
an extension from σpreferred(AF ′′), considering E0 and E1. σpreferred(AF ′′) =
{{a, c, g}, {b, d, e, f}}.
We
have
degγ,wmon({a, c, g}, ⟨E0, E1⟩)}
=
13
20
and
degγ,wmon({b, d, e, f}, ⟨E0, E1⟩)} =
3
10. Hence, we select {a, c, g}.
5
Discussion
This paper contributes to the continuation of a long line of research on sys-
tematically relaxing monotony, see for example the works by Gabbay [11] and
Makinson [15] that introduce the restricted monotony and rational monotony
principles. In the context of formal argumentation research, this paper adds to
the body of research on dynamics in formal argumentation (see [8] for a sur-
vey), and in particular relies on work by Baumman and Brewka that presents
monotony results in abstract argumentation [4]. Let us highlight that the work
presented in this paper is diﬀerent from (but has a similar motivation to) our
recent results that show how to ensure consistent preferences over powersets
of arguments in the context of normal expansions and naive set-based seman-
tics [12], taking principles of economic rationality as a starting point (Dung’s
admissible set-based semantics violate the corresponding argumentation princi-
ple). The results this paper provides raise the following questions.
1. Can we specify more bounds to the degree of monotony when expanding a
belief base (argumentation framework), given some constraints to the nature
of the expansion and the inference function (argumentation semantics)?
2. What additional approaches can mitigate the degrees of monotony-dilemma
and according to which criteria can these approaches be formally analyzed?
3. What roles can degrees of monotony play in formal argumentation variants
that extend abstract argumentation, such as value-based argumentation [6]?

The Degrees of Monotony-Dilemma in Abstract Argumentation
101
Future research can set out to answer these questions for abstract argumenta-
tion, other formal argumentation variants, and indeed for any method of non-
monotonic reasoning in the face of uncertainty. In the context of abstract argu-
mentation, the analysis provided in this paper can, for example, be extended
to other argumentation semantics (see [1] for an overview, and also see the
weak admissible set-based semantics family as recently introduced by Baumann
et al. [5]), and consider additional constraints, e.g. other notions of expan-
sions [4], constraints to the structure of an argumentation framework, or formal
argumentation principles [16] that constrain semantics behavior.
6
Conclusion
In this paper, we have deﬁned the notion of the degree of monotony to abstract
argumentation and then introduced the degrees of monotony-dilemma, which
can occur when repeatedly ﬁrst determining the extensions of an argumentation
framework and then normally expanding the argumentation framework. The
degrees of monotony-dilemma highlights that selecting extensions from every
argumentation framework that maximize the degree of monotony w.r.t. an exten-
sion that has been inferred by the immediate predecessor may lead to a subopti-
mal degree of monotony of any extension inferred from an argumentation frame-
work and any “indirect” predecessor. With this, the ﬁndings shed new light on
the challenge of drawing “consistent” inferences from an expanding base of con-
ﬂicting beliefs, and provide an increment to the point of departure for work on
relaxed notions of monotony in formal argumentation and beyond.
Acknowledgments. We thank the anonymous reviewers for their thoughtful and
useful feedback. This work was partially supported by the Wallenberg AI, Autonomous
Systems and Software Program (WASP) funded by the Knut and Alice Wallenberg
Foundation.
References
1. Baroni, P., Caminada, M., Giacomin, M.: Abstract argumentation frameworks and
their semantics, chap. 4. In: Baroni, P., Gabbay, D., Massimiliano, G., van der
Torre, L. (eds.) Handbook of Formal Argumentation. College Publications, pp.
159–236. College Publications (2018)
2. Baroni, P., Gabbay, D.M., Giacomin, M., van der Torre, L.: Handbook of Formal
Argumentation. College Publications (2018)
3. Baumann, R.: On the nature of argumentation semantics: Existence and unique-
ness, expressibility, and replaceability. J. Appl. Logics 4(8), 2779–2886 (2017)
4. Baumann, R., Brewka, G.: Expanding argumentation frameworks: enforcing and
monotonicity results. COMMA 10, 75–86 (2010)
5. Baumann, R., Brewka, G., Ulbricht, M.: Revisiting the foundations of abstract
argumentation-semantics based on weak admissibility and weak defense. In: AAAI,
pp. 2742–2749 (2020)
6. Bench-Capon, T.J.: Persuasion in practical argument using value-based argumen-
tation frameworks. J. Log. Comput. 13(3), 429–448 (2003)

102
T. Kampik and D. Gabbay
7. ˇCyras, K., Toni, F.: Non-monotonic inference properties for assumption-based
argumentation. In: Black, E., Modgil, S., Oren, N. (eds.) TAFA 2015. LNCS
(LNAI), vol. 9524, pp. 92–111. Springer, Cham (2015). https://doi.org/10.1007/
978-3-319-28460-6 6
8. Doutre, S., Mailly, J.G.: Constraints and changes: a survey of abstract argumen-
tation dynamics. Argument Comput. 9, 223–248 (2018). https://doi.org/10.3233/
AAC-180425
9. Dumas, M., Rosa, M.L., Mendling, J., Reijers, H.: Fundamentals of Business Pro-
cess Management (2018)
10. Dung, P.M.: On the acceptability of arguments and its fundamental role in non-
monotonic reasoning, logic programming and n-person games. Artif. Intell. 77(2),
321–357 (1995)
11. Gabbay, D.M.: Theoretical foundations for non-monotonic reasoning in expert sys-
tems. In: Apt, K.R. (ed.) Logics and Models of Concurrent Systems. NATO ASI
Series, pp. 439–457. Springer, Heidelberg (1985). https://doi.org/10.1007/978-3-
642-82453-1 15
12. Kampik, T., Nieves, J.C.: Abstract argumentation and the rational man. J. Logic
Comput. 31(2), 654–699 (2021). https://doi.org/10.1093/logcom/exab003
13. Kraus, S., Lehmann, D., Magidor, M.: Nonmonotonic reasoning, preferential mod-
els and cumulative logics. Artif. Intell. 44(1), 167–207 (1990). https://doi.org/10.
1016/0004-3702(90)90101-5,
https://www.sciencedirect.com/science/article/pii/
0004370290901015
14. Lankhorst, M.: Enterprise Architecture at Work: Modelling, Communication and
Analysis. The Enterprise Engineering Series, Springer, Heidelberg (2017). https://
doi.org/10.1007/978-3-662-53933-0
15. Makinson, D.: General theory of cumulative inference. In: Reinfrank, M., de Kleer,
J., Ginsberg, M.L., Sandewall, E. (eds.) NMR 1988. LNCS, vol. 346, pp. 1–18.
Springer, Heidelberg (1989). https://doi.org/10.1007/3-540-50701-9 16
16. van der Torre, L., Vesic, S.: The principle-based approach to abstract argumenta-
tion semantics. IfCoLog J. Logics Appl. 4(8), 1–44 (2017)
17. Verheij, B.: Two approaches to dialectical argumentation: admissible sets and argu-
mentation stages. Proc. NAIC 96, 357–368 (1996)

Constrained Incomplete Argumentation
Frameworks
Jean-Guy Mailly(B)
LIPADE, University of Paris, Paris, France
jean-guy.mailly@u-paris.fr
Abstract. Operations like belief change or merging have been adapted
to the context of abstract argumentation. However, these operations may
require to express some uncertainty or some disjunction in the result,
which is not representable in classical AFs. For this reason, some of
these works require a set of AFs or a set of extensions as the outcome of
the operation, somehow to represent a disjunction of AFs or extensions.
In parallel, the notion of Incomplete AFs (IAFs) has been developed
recently. It corresponds to AFs where the existence of some arguments
or attacks may be uncertain. Each IAF can be associated with a set
of classical AFs called completions, that correspond to diﬀerent ways
of “resolving the uncertainty”. While these IAFs could be good candi-
dates for a compact representation of a “disjunction” of AFs, we prove
that this model is not expressive enough. Then we introduce Constrained
IAFs, that include a propositional formula allowing to select the set of
completions used for reasoning. We prove that this model is expressive
enough for representing any set of AFs, or any set of extensions. More-
over, we show that the complexity of credulous and skeptical reasoning
is the same as in the case of IAFs. Finally, we show that CIAFs can be
used to model a new form of extension enforcement.
Keywords: Abstract argumentation · Uncertainty · Extension
enforcement
1
Introduction
Representing uncertainty and reasoning with uncertain information is of utmost
importance in artiﬁcial intelligence. Indeed, there are many reasons that may
lead an intelligent agent to face uncertainty or impossibility to choose between
alternatives. For instance, she can receive information from diﬀerent sources,
which can have diﬀerent degrees of reliability. This information can be incompat-
ible with her previous knowledge, or with information provided by other sources.
This kind of problem can be formalized as belief change operations (“How to
incorporate a new piece of information to my knowledge if it is not logically con-
sistent?”) [1,21,22] or belief merging (“How to give a coherent representation of
several agent’s knowledge even if they are globally inconsistent?”) [23]. In this
kind of application, a simple way to deal with the uncertainty of the result is
c
⃝Springer Nature Switzerland AG 2021
J. Vejnarov´a and N. Wilson (Eds.): ECSQARU 2021, LNAI 12897, pp. 103–116, 2021.
https://doi.org/10.1007/978-3-030-86772-0_8

104
J.-G. Mailly
the logical disjunction: if the result of revising an agent’s knowledge is “I am not
sure whether a is true or b is true.”, then it can be expressed with a∨b. However,
there are formalisms where this kind of simple representation of undecidedness
cannot be done. For instance, in abstract argumentation frameworks (AFs) [18],
either there is certainly an attack between two arguments, or there is certainly
no attack between them. But an agent cannot express something like “I am not
sure whether a attacks b or not.” AFs have been extended in this direction:
Partial AFs (PAFs) [9] allow to represent uncertain attacks. Later, Incomplete
AFs (IAFs) [5,6] have been proposed, as a generalization of PAFs where also
arguments can be uncertain. Reasoning with a PAF or an IAF is possible thanks
to a set of completions, that are classical AFs that correspond to the diﬀer-
ent possible worlds encoded in the uncertain information. While this framework
allows to express uncertainty in abstract argumentation in a rich way, there are
still situations that cannot be modeled. Consider, e.g., that an agent faces the
information “Either a attacks b, or b attacks a, but I am not sure which one is
true.”. There is no way to represent this information with an IAF. However, this
may be necessary in some situations. For instance, several adaptations of belief
change [11,13] or merging [9,15] to abstract argumentation lead to results that
can contain such an uncertainty over the result, impossible to be represented by
a single AF. So, these works propose to represent the “disjunction” in the result
as a set of AFs, or even as a set of extensions (and it is also known that not
every set of extensions can be represented by a single AF [19]).
In this paper, we deﬁne a generalization of IAFs, that adds a constraint to
it. The constraint in a Constrained IAF (CIAF) is a propositional formula that
allows to specify which subset of the completions of the IAF should be used
for reasoning. We show that this framework is more expressive than IAFs, in
the sense that any set of AFs can be the set of completions of a CIAF. Also,
any set of extensions can be obtained from (the completions of) a CIAF. We
prove that, despite being more expressive than IAFs, the complexity of credu-
lous and skeptical reasoning does not increase compared to IAFs, under various
classical semantics. Interestingly, we also identify a relation between our CIAFs
and extension enforcement [3]. This operation consists in modifying an AF s.t.
a given set of arguments becomes part of an extension. Classical enforcement
operators are based on expansions, i.e. addition of arguments and attacks s.t.
the attack relation between former arguments remain unchanged. Theoretical
results show under which conditions enforcement is possible under expansions.
However, these results may suppose the possibility to perform unnatural expan-
sions, like adding a new argument that attacks all the undesired arguments. In
a real dialogue, such an “ultimate attacker”, that defeats every unwanted argu-
ment, is not likely to exist. We show that completions of a CIAF can be used
to model the set of expansions that are available to an agent, and then enforce-
ment is possible iﬀthe desired set of arguments is credulously accepted w.r.t.
the CIAF.
The paper is organized as follows. Section 2 describes background notions
of abstract argumentation. Our ﬁrst contributions are presented in Sect. 3: the

Constrained Incomplete Argumentation Frameworks
105
deﬁnition of CIAFs, the properties of the framework regarding its expressivity,
and ﬁnally the computational complexity of credulous and skeptical acceptance.
Then in Sect. 4, we show how CIAFs can be used to model scenarios of extension
enforcement. We discuss related work in Sect. 5, and ﬁnally Sect. 6 concludes the
paper and highlights some topics of interest for future research.1
2
Background
2.1
Dung’s Abstract Argumentation
Abstract argumentation was introduced in [18], where arguments are abstract
entities whose origin or internal structure are ignored. The acceptance of argu-
ments is purely deﬁned from the relations between them.
Deﬁnition 1 (Abstract AF). An abstract argumentation framework (AF) is
a directed graph F = ⟨A, R⟩, where A is a set of arguments, and R ⊆A × A is
an attack relation.
We say that a attacks b when (a, b) ∈R. If (b, c) ∈R also holds, then a
defends c against b. Attack and defense can be adapted to sets of arguments:
S ⊆A attacks (respectively defends) an argument b ∈A if ∃a ∈S that attacks
(respectively defends) b.
Example 1. Let F = ⟨A, R⟩be the AF depicted in Fig. 1, with A = {a, b, c, d, e}
and R = {(b, a), (c, a), (c, d), (d, b), (d, c), (e, a)}. Each arrow represents an attack.
d defends a against both b and c, since these are attackers of a that are, in turn,
both attacked by d.
Fig. 1. The AF F
Diﬀerent semantics have been introduced to evaluate the acceptability of
arguments [18], relying on two basic concepts: conﬂict-freeness and admissibility.
Deﬁnition 2 (Conﬂict-Freeness and Admissibility). Given F = ⟨A, R⟩, a
set of arguments S ⊆A is:
– conﬂict-free iﬀ∀a, b ∈S, (a, b) ̸∈R;
– admissible iﬀit is conﬂict-free, and defends each a ∈S against its attackers.
1 Proofs are omitted for space reasons.

106
J.-G. Mailly
We use cf(F) and ad(F) for denoting the sets of conﬂict-free and admissible
sets of an argumentation framework F. The intuition behind these principles
is that a set of arguments may be accepted only if it is internally consistent
(conﬂict-freeness) and able to defend itself against potential threats (admissibil-
ity). The semantics proposed by [18] can be deﬁned as follows.
Deﬁnition 3 (Extension Semantics). Given F = ⟨A, R⟩, an admissible set
S ⊆A is:
– a complete extension iﬀit contains every argument that it defends;
– a preferred extension iﬀit is a ⊆-maximal complete extension;
– the unique grounded extension iﬀit is the ⊆-minimal complete extension;
– a stable extension iﬀit attacks every argument in A \ S.
The sets of extensions of an AF F, for these semantics, are denoted (respec-
tively) co(F), pr(F), gr(F) and st(F). Based on these semantics, we can deﬁne
the status of any (set of) argument(s), namely skeptically accepted (belonging
to each σ-extension), credulously accepted (belonging to some σ-extension) and
rejected (belonging to no σ-extension). Given an AF F and a semantics σ, we
use (respectively) skσ(F), crσ(F) and rejσ(F) to denote these sets of arguments.
Example 2. We consider again F given in Fig. 1. Its extensions for the diﬀerent
semantics, as well as the sets of accepted arguments, are given in Table 1.
Table 1. Extensions and accepted arguments of F for σ ∈{co, pr, gr, st}
σ
σ(F)
crσ(F)
skσ(F) rejσ(F)
co {e}, {d, e}, {b, c, e} {b, c, d, e} {e}
{a}
pr {d, e}, {b, c, e}
{b, c, d, e} {e}
{a}
gr {e}
{e}
{e}
{a, b, c, d}
st
{d, e}, {b, c, e}
{b, c, d, e} {e}
{a}
For more details about argumentation semantics, we refer the interested
reader to [2,18].
2.2
Incomplete AFs
Now, we describe Incomplete Argumentation Frameworks [5,6,9].
Deﬁnition 4 (Incomplete AF). An Incomplete Argumentation Framework
(IAF) is a tuple I = ⟨A, A?, R, R?⟩, where A and A? are disjoint sets of argu-
ments, and R, R? ⊆(A ∪A?) × (A ∪A?) are disjoint sets of attacks.

Constrained Incomplete Argumentation Frameworks
107
Elements from A and R are certain arguments and attacks, i.e. the agent is
sure that they appear in the framework. On the opposite, A? and R? represent
uncertain arguments and attacks. For each of them, there is a doubt about their
actual existence.
Example 3. Let us consider I = ⟨A, A?, R, R?⟩given in Fig. 2. We use plain nodes
and arrows to represent certain arguments and attacks, i.e. A = {a, b, c, d, e} and
R = {(b, a), (c, a), (d, b), (d, c)}. Uncertain arguments are represented as dashed
square nodes (i.e. A? = {f}) and uncertain attacks are represented as dotted
arrows (i.e. R? = {(e, a), (f, d)}).
Fig. 2. The IAF I
The notion of completion in abstract argumentation was ﬁrst deﬁned in [9]
for Partial AFs (i.e. IAFs with A? = ∅), and then adapted to IAFs. Intuitively,
a completion is a classical AF which describes a situation of the world coherent
with the uncertain information encoded in the IAF.
Deﬁnition 5 (Completion of an IAF).
Given I = ⟨A, A?, R, R?⟩, a com-
pletion of I is F = ⟨A′, R′⟩, s.t. A ⊆A′ ⊆A ∪A? and R|A′ ⊆R′ ⊆R|A′ ∪R?
|A′,
where R|A′ = R ∩(A′ × A′) (and similarly for R?
|A′).
The set of completions of an IAF I is denoted comp(I).
Example 4. We consider again the IAF from Fig. 2. Its set of completions is
described at Fig. 3.
Fig. 3. The completions of I

108
J.-G. Mailly
Concerning the question of compact representation of a set of AFs by means
of an incomplete AF, the following example proves that some sets of AFs (even
simple ones) cannot be represented by an IAF.
Example 5. Suppose that the result of revising an AF [11] is the set F = {F1 =
⟨{a, b}, {(b, a)}⟩, F2 = ⟨{a, c}, {(c, a)}⟩}. The question is to determine whether
this set can be compactly represented by a single IAF. Towards a contradiction,
suppose that there is an IAF I = ⟨A, A?, R, R?⟩s.t. comp(I) = F. Since a
belongs to both F1 and F2, it must belong to the certain arguments A. On the
contrary, the uncertain arguments are A? = {b, c}, each of them belongs to some
(but not all) completions. A = {a} and A? = {b, c} imply the existence of some
completions that only contain a, and some completions that contain the three
arguments a, b, c. This is not the case. So I does not exist.
3
Constrained IAFs
Now we introduce the Constrained Incomplete Argumentation Frameworks, that
generalize IAFs by adding a constraint on the set of possible completions.
3.1
Constraints on Completions
Intuitively, for a given I, a constrained version of it is a pair ⟨I, C⟩where
C ⊆comp(I). Then, reasoning on ⟨I, C⟩requires to use only C instead of the
full set of completions of I. For instance, let us consider applications where the
uncertainty about the world is encoded as a set of AFs or a set of extensions
(for instance, revision [11,13] or merging of argumentation frameworks [9,15]).
This set of AFs or extensions may not be encodable in a single IAF, while
being representable with a single Constrained IAF. But rather than deﬁning
the constraint with a set of completions, we deﬁne a logical language to express
information on the structure of an AF, i.e. a propositional language s.t. the
models of a formula correspond to AFs, inspired by [10] for selecting extensions.
This is a more compact representation of the constraint.
Deﬁnition 6 (Constraint). Given A a set of arguments, we deﬁne the set of
propositional atoms PropA = ArgA ∪AttA where ArgA = {arga | a ∈A} and
AttA = {atta,b | (a, b) ∈A × A}. Then, LA is the propositional language built
from PropA with classical connectives {¬, ∨, ∧}.
The satisfaction of a constraint by an AF is deﬁned as follows.
Deﬁnition 7 (Constraint Satisfaction). Given A a set of arguments, and
φ ∈LA a formula, the set of models of φ is denoted mod(φ). An AF F = ⟨A′, R⟩
with A′ ⊆A and R ⊆A′ × A′ satisﬁes φ iﬀthere is a model ω ∈mod(φ) s.t.
A′ = {a ∈A | ω(arga) = ⊤}, and R = {(a, b) ∈A × A | ω(atta,b) = ⊤)}.

Constrained Incomplete Argumentation Frameworks
109
3.2
Deﬁnition and Expressivity of CIAFs
Deﬁnition 8 (Constrained IAF). A Constrained Incomplete Argumentation
Framework (CIAF) is a tuple C = ⟨A, A?, R, R?, φ⟩, where ⟨A, A?, R, R?⟩is an
IAF, and φ ∈LA∪A? is a constraint.
The constraint φ is used to select a subset of the completions of the IAF
IC = ⟨A, A?, R, R?⟩. The completions of a CIAF are then deﬁned as follows.
Deﬁnition 9 (Completions of a CIAF). Given C = ⟨A, A?, R, R?, φ⟩a
CIAF, we deﬁne its set of completions by comp(C) = {c ∈comp(IC) |
c satisﬁes φ} where IC = ⟨A, A?, R, R?⟩.
Example 6. Let C = ⟨A, A?, R, R?, φ⟩be a CIAF s.t. IC = ⟨A, A?, R, R?⟩is the
IAF from Fig. 2, and φ = atte,a ∧argf. Recall that the completions of IC are
given in Fig. 3. Only two of them satisfy φ, namely F5 (Fig. 3e) and F6 (Fig. 3f).
So comp(C) = {F5, F6}.
Let us mention that, in order to be meaningful, the constraint φ must satisfy
some conditions. Indeed, there must be at least one model of φ s.t. arga is true
for each a ∈A, atta,b is true for each (a, b) ∈R, and atta,b is false for each
(a, b) ∈((A ∪A′) × (A ∪A′)) \ (R ∪R?). Otherwise, comp(C) is trivially empty.
More generally, a CIAF C is over-constrained when comp(C) = ∅.
Now, we focus on the expressivity of CIAFs, i.e. given a set of AFs (or a
set of extensions), is there a CIAF s.t. its completions (or the extensions of
its completions) correspond to the given set? We show that, in both cases, the
answer is yes.
Representing a Set of AFs First, we deﬁne a particular formula, that is only
satisﬁed by one given AF.
Deﬁnition 10. Given A a set of arguments, and F = ⟨A′, R⟩with A′ ⊆A, and
R ⊆A′ × A′, we deﬁne ψF ∈LA as
ψF = (

a∈A′
arga) ∧(

a∈A\A′
¬arga) ∧(

(a,b)∈R
atta,b) ∧(

(a,b)∈(A×A)\R
¬atta,b)
Proposition 1. Let F = {F1 = ⟨A1, R1⟩, . . . , Fn = ⟨An, Rn⟩} be a set of AFs.
There is a CIAF C = ⟨A, A?, R, R?, φ⟩s.t. comp(C) = F.
Intuitively, a simple CIAF that does the job consists of all the arguments
and attacks from F deﬁned as uncertain, and then φ is the disjunction of the ψF
formulas, for F ∈F. Let us exemplify this result.
Let us exemplify this result.
Example 7. We continue Example 5. For F = {F1 = ⟨{a, b}, {(b, a)}⟩, F2 =
⟨{a, c}, {(c, a)}⟩}, we deﬁne C = ⟨A, A?, R, R?, φ, ⟩, with A = ∅, A? = {a, b, c},
R = ∅, R? = {(b, a), (c, a)}, φ = ψF1 ∨ψF2, where
ψF1 = arga ∧argb ∧¬argc ∧attb,a ∧(

(x,y)∈({a,b,c}×{a,b,c})\{(b,a)}
¬attx,y)

110
J.-G. Mailly
and
ψF2 = arga ∧¬argb ∧argc ∧attc,a ∧(

(x,y)∈({a,b,c}×{a,b,c})\{(c,a)}
¬attx,y)
We have comp(C) = F.
Representing a Set of Extensions Now, we focus on the expressibility of a set of
extensions with a CIAF.
Proposition 2. Let E = {E1, . . . , En} be a set of extensions, and σ
∈
{co, pr, st, gr}. There is a CIAF C = ⟨A, A?, R, R?, φ⟩s.t. 
c∈comp(C) σ(c) = E.
Of course, the construction described in Example 7 only shows the existence
of a CIAF that satisﬁes the expected property. This does not mean that this
given CIAF is the best way to represent the set of AFs (or extensions). A ﬁrst
possible simpliﬁcation consists in choosing A = n
i=1 Ai and A? = n
i=1 Ai \ A.
This natural simpliﬁcation means that an argument that appears in every AF
must be considered as certain. A similar reasoning can be made with the attacks,
and the constraint can also be simpliﬁed. In a context of belief revision [11,13]
or belief merging [9,15], it is important to ensure that the resulting CIAF is as
close as possible to the initial AF(s). This question is out of the scope of this
paper, and is kept for future research.
3.3
Complexity Issues
Observation 1. Verifying whether an AF (or a completion) satisﬁes a con-
straint φ is a polynomial task: the correspondence between an AF and an inter-
pretation ω described in Deﬁnition 7 can be done polynomially, as well as check-
ing whether ω satisﬁes φ.
This means that, given a CIAF C = ⟨A, A?, R, R?, φ⟩, guessing a completion
of C is equivalent to guessing a set of arguments A ⊆A′ ⊆A?, a set of attacks
R|A′ ⊆A′ ⊆R?
|A′, and verifying (in polynomial time) whether ⟨A′, R′⟩satisﬁes
φ. This will be useful in the proofs of complexity results.
Now we study the complexity of credulous and skeptical reasoning. More
speciﬁcally, given C = ⟨A, A?, R, R?, φ⟩a CIAF, a ∈A, and σ a semantics,
Cred-σ is a ∈
F∈comp(C)

S∈σ(F) S?
Skep-σ is a ∈
F∈comp(C)

S∈σ(F) S?
These problems correspond to possible credulous acceptance (PCA) and necessary
skeptical acceptance (NSA) for IAFs [5]. We prove that complexity does not
increase from IAFs to CIAFs.
Proposition 3. The following hold:
1. For σ ∈{ad, st, co, gr, pr}, Cred-σ is NP-complete.

Constrained Incomplete Argumentation Frameworks
111
2. For σ ∈{st, co, gr}, Skep-σ is coNP-complete.
3. Skep-pr is ΠP
2 -complete.
Concerning skeptical reasoning, the problem is trivial under σ = ad, as usual,
since ∅is admissible in any AF, there is no skeptically accepted argument in any
completion.
Finally, let us mention that credulous and skeptical acceptance can be gener-
alized to sets of arguments. These versions consists, respectively, in determining
whether a given set of arguments S satisﬁes S ⊆
F∈comp(C)

S∈σ(F) S, or
S ⊆
F∈comp(C)

S∈σ(F) S. The generalized versions keep the same complexity.
4
CIAFs and Extension Enforcement
4.1
Expansion-Based Enforcement
Now we introduce the notions of AF expansion and extension enforcement [3].
Deﬁnition 11. Let F = ⟨A, R⟩be an AF. An expansion of F is an AF F′ =
⟨A ∪A′, R ∪R′⟩s.t. A′ ̸= ∅and A ∩A′ = ∅. An expansion is called normal if
∀(a, b) ∈R′, a ∈A′ or b ∈A′. Moreover, a normal expansion is strong (resp.
weak) if ∀(a, b) ∈R′, a ̸∈A (resp. b ̸∈A).
In words, an expansion adds some arguments, and possibly attacks. In the
case of a normal expansion, the only added attacks concern at least one new
arguments, i.e. the attacks between the former arguments are not modiﬁed.
Finally, a normal expansion is strong (resp. weak) if it adds only strong (resp.
weak) arguments, i.e. arguments that are not attacked by (resp. do not attack)
the former arguments. The fact that F′ is an expansion of F is denoted F ⪯E F′
(and normal, strong, weak expansions are denoted by ⪯N, ⪯S, ⪯W ).
Deﬁnition 12. Given F = ⟨A, R⟩, a set of arguments S ⊆A, and a semantics
σ, the AF F′ is a normal (resp. strong, weak) σ-enforcement of S in F iﬀF′ is
a normal (resp. strong, weak) expansion of F, and ∃E ∈σ(F′) s.t. S ⊆E.
Deﬁnition 12 only considers “non-strict” enforcement, i.e. the desired set of argu-
ments must be included in an extension of the new AF. Strict enforcement is
deﬁned in a similar manner, but the desired set of arguments must exactly cor-
respond to an extension.
Some (im)possibility results for these operations have been presented in [3].
However, some results rely on examples that are not representative of realistic
argument-based dialogues. The following example is inspired by [3, Theorem 4].
Example 8. Let F = ⟨A, R⟩be the AF given in Fig. 1. Recall that its stable
extensions are st(F) = {{d, e}, {b, c, e}}. Now let S = {a, d} be the set of argu-
ments to be enforced. We can deﬁne the (strong) expansion F′ = ⟨A∪{x}, R∪R′⟩
where x is a fresh argument, and R′ = {(x, y) | y ∈A\S}. F′ is shown at Fig. 4.
st(F′) = {{x, a, d}}, thus it is a strong enforcement of S in F.

112
J.-G. Mailly
Fig. 4. The expansion F ′ enforces S = {a, d}
Example 8 illustrates the (theoretical) possibility to enforce any (conﬂict-
free) set of arguments if strong (or normal) expansions are permitted. However,
in an application context like dialogue (e.g. argument-based negotiation [17] or
persuasion [7]), the existence of an “ultimate” attacker like x, that defeats all
the undesired arguments, is unlikely.
4.2
Enforcement as Credulous Acceptability in CIAFs
To handle the problem highlighted by Example 8, we propose to take into account
the set of arguments and attacks that an agent has at her disposal for partici-
pating to the debate. This means that we parameterize the expansion operation
by the set of possible expanded AFs resulting of using some of the available
arguments and attacks.
Deﬁnition 13. Given F = ⟨A, R⟩an AF, A a set of available arguments s.t.
A ∩A = ∅, and R ⊆((A ∪A) × (A ∪A)) \ (A × A), we say that F′ = ⟨A′, R′⟩
is an A-R-parameterized expansion of F (denoted by F ⪯A,R F′) iﬀF ⪯E F′,
A ⊆A′ ⊆A ∪A and R′ = (R ∪R) ∩(A′ × A′).
We use ⪯A,R
N
(resp. ⪯A,R
S
, ⪯A,R
W
) to denote A-R-parameterized normal (resp.
strong, weak) expansions, i.e. A-R-parameterized expansions where F′ is (addi-
tionally) normal (resp. strong, weak). This deﬁnition allows to take into account
the arguments and attacks that are actually known by an agent that participates
in a debate. We can show that a set of arguments that can be enforced with an
arbitrary (strong) expansion (like in Example 8) may not be enforceable with
parameterized expansions.
Example 9. We continue Example 8. Suppose that the available arguments and
attacks are A = {f, g} and R = {(f, c), (g, b)}. Figure 5 depicts the agent’s
possible actions: say nothing (i.e. keep the initial AF, Fig. 5a), say “f attacks c”
(Fig. 5b), say “g attacks b” (Fig. 5c), or both (Fig. 5d). In all the possible cases,
S = {a, d} is not enforced, since a is never defended against e.
What we call here the “possible actions” of the agent can actually be seen
as the set of completions of a CIAF, and the possibility of enforcing a set of
arguments corresponds to the credulous acceptance of this set w.r.t. the CIAF.
Deﬁnition 14. Given F an AF, A a set of arguments, R a set of attacks, and
X ∈{E, N, S, W} denoting the type of expansion, we deﬁne F = {F} ∪{F′ |
F ⪯A,R
X
F′}. Then, CA,R
F,X is a CIAF s.t. comp(CA,R
F,X) = F.

Constrained Incomplete Argumentation Frameworks
113
Fig. 5. The agent’s possible actions
The existence of CA,R
F,X is guaranteed by Proposition 1. The construction illus-
trated by Example 7 provides a suitable CA,R
F,X. However, other CIAFs can be
deﬁned, for instance all the arguments and attacks from the initial F can be
deﬁned as certain elements. The following proposition states that CIAFs can be
used as a computational tool for determining the possibility of enforcement.
Proposition 4. Given an AF F = ⟨A, R⟩, a set of arguments S ⊆A, X ∈
{E, N, S, W}, A a set of arguments and R a set of attacks, and a semantics σ,
S can be σ-enforced in F by means of a A-R-parameterized X-expansion iﬀS
is credulously accepted in CA,R
F,X w.r.t. σ.
Observe that this result holds for non-strict enforcement, as given in Deﬁni-
tion 12. Strict enforcement requires, instead, the notion of extension veriﬁcation
[20] for CIAFs, i.e. we must check that the set of arguments S is actually an
extension of one of the completions.
5
Related Work
We have described the main existing work on IAFs. Let us also mention [20],
which deﬁnes an alternative notion of extension (compared to the one from
[6]). This does not have an impact on the work presented here, since we focus on
argument acceptance; our deﬁnitions are consistent with the ones in [5]. However,
as mentioned previously, this will be useful for characterizing strict extension
enforcement with CIAF-based reasoning.
Besides IAFs, our contribution is related to other previous works. Using
propositional formulas as constraints in an argumentation framework has been
originally proposed in [10], which deﬁnes Constrained Argumentation Frame-
works. In this setting, the propositional formula is a constraint on arguments
that is used for selecting the best extensions. Intuitively, we use here the con-
straint in CIAFs in a similar way, but for selecting completions of a IAF instead
of selecting the extensions of a (classical) AF.
We have shown how to represent any set of extensions with a single CIAF.
The question of representing sets of extensions has already arisen in classical AFs.
This corresponds to the notion of realizability in the literature [4,19], i.e. given a
set of extensions E and a semantics σ, is there an AF F s.t. σ(F) = E. Existing
results show that it is not possible in general for most classical semantics. The
non-realizability of some sets of extensions is the reason why some operations

114
J.-G. Mailly
(like belief revision or merging) cannot be easily adapted to AFs, as mentioned
in the introduction. With Proposition 2, we continue this line of research, by
proving the realizability of any set of extensions by means of CIAFs.
Regarding extension enforcement, it has been proven that (non-strict)
enforcement is NP-complete [28] for another type of authorized change:
argument-ﬁxed enforcement [12], where the set of arguments cannot be mod-
iﬁed, but all the attacks (or non-attacks) can be questioned. Although this is
out of the scope of this paper, we believe that this kind of enforcement can also
be captured by the CIAF setting, which will allow to deﬁne a parameterized
version of argument-ﬁxed enforcement. The parameters A and R are also remi-
niscent of the “control part” of Control AFs [16,24,25], that allows to enforce a
set of arguments in presence of uncertainty.
Constraints that express dependencies between arguments of an ADF [8] in
a dynamic context have been studied in [27]. While there is some similarity
between these constraints and the ones deﬁned here, both studies have diﬀerent
purposes. Indeed, [27] does not focus on uncertain environment as we do here,
but only on dynamic scenarios. Connections with enforcement based on A-R-
parameterized expansions will be studied.
6
Conclusion
We have deﬁned Constrained Incomplete Argumentation Frameworks (or CIAFs,
for short) that generalize IAFs by adding a constraint over the set of completions.
This new framework increases the expressivity of IAFs without a gap in complex-
ity, and paves the way for the deﬁnition of revision or merging operators for AFs
that return a CIAF, i.e. a more compact result than a (potentially exponentially
large) set of AFs or extensions. However, the CIAF that we have exhibited here
to prove the representability of any set of AFs or extensions may not be a suit-
able solution in scenarios like belief revision or belief merging, where the notion
of minimal change is important. We will study how to generate a CIAF that
is optimal in such contexts. Knowledge compilation [14] is an interesting way
for providing a succinct equivalent propositional constraint such that relevant
reasoning tasks are polynomially doable. Other interesting research tracks are
the study of complexity for other decision problems (e.g. extension veriﬁcation,
or possible skeptical and necessary credulous acceptance), and the implemen-
tation of eﬃcient algorithms (e.g. based on Boolean encoding, in the line of
[26]). We will also study how to encode other extension enforcement operators
as CIAF-based reasoning.
Acknowledgements. The author warmly thanks Antonio Yuste-Ginel for the inter-
esting discussion that lead to this work, as well as the reviewers that provided valuable
feedback.

Constrained Incomplete Argumentation Frameworks
115
References
1. Alchourr´on, C.E., G¨ardenfors, P., Makinson, D.: On the logic of theory change:
partial meet contraction and revision functions. J. Symb. Log. 50(2), 510–530
(1985)
2. Baroni, P., Caminada, M., Giacomin, M.: Abstract argumentation frameworks and
their semantics. In: Baroni, P., Gabbay, D., Giacomin, M., van der Torre, L. (eds.)
Handbook of Formal Argumentation, pp. 159–236. College Publications (2018)
3. Baumann, R., Brewka, G.: Expanding argumentation frameworks: enforcing and
monotonicity results. In: Proceedings of the COMMA 2010, vol. 216, pp. 75–86
(2010)
4. Baumann, R., Dvor´ak, W., Linsbichler, T., Strass, H., Woltran, S.: Compact argu-
mentation frameworks. In: Proceedings of the ECAI 2014, vol. 263, pp. 69–74
(2014)
5. Baumeister, D., Neugebauer, D., Rothe, J.: Credulous and skeptical acceptance in
incomplete argumentation frameworks. In: Proceedings of the COMMA 2018, pp.
181–192 (2018)
6. Baumeister, D., Neugebauer, D., Rothe, J., Schadrack, H.: Veriﬁcation in incom-
plete argumentation frameworks. Artif. Intell. 264, 1–26 (2018)
7. Bonzon, E., Delobelle, J., Konieczny, S., Maudet, N.: A parametrized ranking-
based semantics for persuasion. In: Proceedings of the SUM 2017, pp. 237–251
(2017)
8. Brewka, G., Strass, H., Ellmauthaler, S., Wallner, J.P., Woltran, S.: Abstract
dialectical frameworks revisited. In: Rossi, F. (ed.) Proceedings of the IJCAI 2013,
pp. 803–809 (2013)
9. Coste-Marquis, S., Devred, C., Konieczny, S., Lagasquie-Schiex, M., Marquis, P.:
On the merging of dung’s argumentation systems. Artif. Intell. 171(10–15), 730–
753 (2007)
10. Coste-Marquis, S., Devred, C., Marquis, P.: Constrained argumentation frame-
works. In: Proceedings of the KR 2006, pp. 112–122 (2006)
11. Coste-Marquis, S., Konieczny, S., Mailly, J.G., Marquis, P.: On the revision of
argumentation systems: minimal change of arguments statuses. In: Proceedings of
the KR 2014 (2014)
12. Coste-Marquis, S., Konieczny, S., Mailly, J.G., Marquis, P.: Extension enforce-
ment in abstract argumentation as an optimization problem. In: Proceedings of
the IJCAI 2015, pp. 2876–2882 (2015)
13. Coste-Marquis, S., Konieczny, S., Mailly, J.G., Marquis, P.: A translation-based
approach for revision of argumentation frameworks. In: Proceedings of the JELIA
2014, pp. 77–85 (2014)
14. Darwiche, A., Marquis, P.: A knowledge compilation map. J. Artif. Intell. Res. 17,
229–264 (2002)
15. Delobelle, J., Haret, A., Konieczny, S., Mailly, J.G., Rossit, J., Woltran, S.: Merging
of abstract argumentation frameworks. In: Proceedings of the KR 2016, pp. 33–42
(2016)
16. Dimopoulos, Y., Mailly, J.G., Moraitis, P.: Control argumentation frameworks. In:
Proceedings of the AAAI 2018, pp. 4678–4685 (2018)
17. Dimopoulos, Y., Mailly, J.G., Moraitis, P.: Argumentation-based negotiation with
incomplete opponent proﬁles. In: Proceedings of the AAMAS 2019, pp. 1252–1260
(2019)

116
J.-G. Mailly
18. Dung, P.M.: On the acceptability of arguments and its fundamental role in non-
monotonic reasoning, logic programming and n-person games. Artif. Intell. 77,
321–357 (1995)
19. Dunne, P.E., Dvor´ak, W., Linsbichler, T., Woltran, S.: Characteristics of multiple
viewpoints in abstract argumentation. Artif. Intell. 228, 153–178 (2015)
20. Fazzinga, B., Flesca, S., Furfaro, F.: Revisiting the notion of extension over incom-
plete abstract argumentation frameworks. In: Proceedings of the IJCAI 2020, pp.
1712–1718 (2020)
21. Katsuno, H., Mendelzon, A.O.: On the diﬀerence between updating a knowledge
base and revising it. In: Proceedings of the KR 1991, pp. 387–394 (1991)
22. Katsuno, H., Mendelzon, A.O.: Propositional knowledge base revision and minimal
change. Artif. Intell. 52(3), 263–294 (1992)
23. Konieczny, S., P´erez, R.P.: Merging information under constraints: a logical frame-
work. J. Log. Comput. 12(5), 773–808 (2002)
24. Mailly, J.G.: Possible controllability of control argumentation frameworks. In: Pro-
ceedings of COMMA 2020, vol. 326, pp. 283–294 (2020)
25. Niskanen, A., Neugebauer, D., J¨arvisalo, M.: Controllability of control argumen-
tation frameworks. In: Proceedings of the IJCAI 2020, pp. 1855–1861 (2020)
26. Niskanen, A., Neugebauer, D., J¨arvisalo, M., Rothe, J.: Deciding acceptance in
incomplete argumentation frameworks. In: Proceedings of the AAAI 2020, pp.
2942–2949 (2020)
27. Wallner, J.P.: Structural constraints for dynamic operators in abstract argumen-
tation. Argument Comput. 11(1–2), 151–190 (2020)
28. Wallner, J.P., Niskanen, A., J¨arvisalo, M.: Complexity results and algorithms for
extension enforcement in abstract argumentation. J. Artif. Intell. Res. 60, 1–40
(2017)

Complexity of Nonemptiness in Control
Argumentation Frameworks
Daniel Neugebauer1, J¨org Rothe1
, and Kenneth Skiba2(B)
1 Institut f¨ur Informatik, Heinrich-Heine-Universit¨at, D¨usseldorf, Germany
2 Institute for Web Science and Technologies, University of Koblenz-Landau,
Koblenz, Germany
Kennethskiba@uni-koblenz.de
Abstract. Control argumentation frameworks (CAFs) [4] extend the standard
model of argumentation framework (AF) due to Dung [6] in a way that takes
unquantiﬁed uncertainty regarding arguments and attacks into account. Comple-
menting recent work by Skiba et al. [21] for incomplete argumentation frame-
works, we study the (nonempty) existence problem for CAFs and fully analyze
its computational complexity for the most common semantics.
1
Introduction
Argumentation theory is a key research ﬁeld within artiﬁcial intelligence. In his semi-
nal early work, Dung [6] introduced the central model of abstract argumentation frame-
work, where— neglecting the actual contents of arguments—a discussion is represented
as a directed graph (AF) whose vertices are the arguments and whose directed edges
represent the attacks among arguments.
Later work established various extensions of this basic model so as to overcome
its shortcomings, especially regarding its inability to handle structural uncertainty and
incomplete information about the existence of arguments or attacks. One such model—
which is central to this paper—proposes control argumentation frameworks (CAF) [4],
which feature uncertain arguments and attacks and are intended speciﬁcally to model
strategic scenarios from the point of view of an agent. The model divides the uncertain
elements of an argumentation into a control part and an uncertain part, where the exis-
tence of elements in the control part is controlled by the agent, and the existence of ele-
ments in the uncertain part is not. This distinguishes CAFs from the related incomplete
argumentation frameworks [2], which also feature uncertain arguments and attacks, but
which do not specify such a sub-division.
Acceptability of arguments in AFs can be determined using one of several different
semantics, where a set of arguments that satisﬁes a semantics is called an extension. To
model various central reasoning tasks in argumentation frameworks formally, a number
of decision problems have been introduced and studied. The veriﬁcation problem asks
whether a given set of arguments is an extension; the credulous acceptance and skep-
tical acceptance problems ask whether a given argument is contained in at least one
extension or in all extensions, respectively; and the even more fundamental problem of
existence asks whether an extension exists in the ﬁrst place.
c⃝Springer Nature Switzerland AG 2021
J. Vejnarov´a and N. Wilson (Eds.): ECSQARU 2021, LNAI 12897, pp. 117–129, 2021.
https://doi.org/10.1007/978-3-030-86772-0_9

118
D. Neugebauer et al.
In control argumentation frameworks, the problem of controllability generalizes the
credulous and skeptical acceptance problems and asks whether there exists a selection
of arguments in the control part (i.e., a control conﬁguration) such that for all selections
of elements in the uncertain part (i.e., all completions), a given target set of arguments is
a subset of an extension (or of all extensions). This problem was analyzed by Dimopou-
los et al. [4] and Niskanen et al. [17].
Extending their work, we tackle natural generalizations of the (nonempty) existence
problem for CAFs, which—even though being very fundamental—can be a very hard
problem for some semantics. Recall that all other reasoning problems, such as the veri-
ﬁcation, acceptability, or controllability problems, are based on the existence of exten-
sions. Therefore, it is decisive to know the computational complexity of the (nonempty)
existence problem ﬁrst of all. This enables us to conclude whether the hardness of a vari-
ant of veriﬁcation, acceptability, or controllability comes from the problem itself or is
simply an artifact of the underlying (nonempty) existence problem. Thus an analysis of
(nonempty) existence problems for control argumentation frameworks is sorely missing
and crucially needed.
The existence problem can be trivially solved whenever a semantics guarantees
existence of at least one extension, and this is the case for all semantics that allow
the empty set to be an extension. This gives rise to consider the nonempty existence
(or, nonemptiness, for short) problem, which only accepts nonempty extensions. The
existence and nonemptiness problems coincide for semantics that do not allow empty
extensions, so we henceforth will focus on the nonemptiness problem only. For CAFs,
we deﬁne (nonempty) existence problems analogously to the controllability problem
studied by Dimopoulos et al. [4] and Niskanen et al. [17]: Does there exist a control
conﬁguration such that for all completions, there exists a nonempty set of arguments
satisfying a given semantics?
We provide a full analysis of the computational complexity of these generalized
nonempty existence problems with respect to the most common Dung semantics. The
relationship between the controllability and the nonempty existence problem is the fol-
lowing: A “yes”-instance for controllability is also a “yes”-instance of the nonempty
existence problem, and a “no”-instance for nonempty existence is also a “no”-instance
of the controllability problem. Table 1 on page 11 in the conclusions gives an overview
of our complexity results for this problem regarding the considered semantics, and com-
pares them with those of Skiba et al. [21] for the analogous problems in the related
incomplete argumentation frameworks and of Chv´atal [3], Modgil and Caminada [16],
and Dimopoulos and Torres [5] in standard argumentation frameworks.
This paper is structured as follows. In Sect. 2, we describe the formal models of
abstract AFs and its extension to CAFs, and we present a formal deﬁnition of the
nonemptiness problem in this setting. This is followed by a full analysis of the com-
putational complexity of nonemptiness for CAFs in Sect. 3. In Sect. 4, we summarize
our results and point out some related work in this ﬁeld.

Complexity of Nonemptiness in Control Argumentation Frameworks
119
2
Preliminaries
First, we brieﬂy present the model of (abstract) argumentation framework that is due
to Dung [6]. We then describe the extension of this model to control argumentation
frameworks, as introduced by Dimopoulos et al. [4].
Fig. 1. Argumentation framework for Example 1
2.1
Abstract Argumentation Frameworks
For a ﬁnite set A of arguments and a set R of attacks between them, we call the pair
⟨A,R ⟩with R ⊆A × A an argumentation framework (AF). Whenever (a,b) ∈R ,
argument a attacks argument b. An argument a is said to be acceptable with respect
to a set S ⊆A if for all arguments b ∈A attacking a, there is an argument c ∈S
attacking b; in this case, a is defended by c against its attacker b.
An argumentation framework ⟨A,R ⟩can be depicted by a directed graph with ver-
tex set A and edge set R .
Example 1. Figure 1 shows the graph representing AF = ⟨A,R ⟩with A = {a,b,c,d,e}
and R = {(a,b),(b,e),(c,a),(c,c),(d,a),(d,c),(d,e),(e,d)}.
For argumentation frameworks, Dung [6] also introduced the notion of semantics.
A set of arguments satisfying the properties of a given semantics is called an extension.
We will use the following semantics.
Deﬁnition 1. Let AF = ⟨A,R ⟩be an argumentation framework. A set S ⊆A is said to
be conﬂict-free (CF) if no arguments in S attack each other. Further, a conﬂict-free set
S is admissible (AD) if all arguments in S are acceptable with respect to S; complete
(CP) if S is closed under defense (i.e., S contains all arguments that are acceptable with
respect to S); grounded (GR) if S is the least complete set with respect to set inclusion;
preferred (PR) if it is a set-maximal admissible set; and stable (ST) if every b ∈A \ S
is attacked by at least one argument a ∈S.
Every stable extension is preferred, every preferred extension is complete, and every
complete extension is admissible. There always is a conﬂict-free, admissible, preferred,
complete, and grounded extension in every argumentation framework (it is possible that
this is just the empty set, though). For the stable semantics, by contrast, existence cannot
be taken for granted.

120
D. Neugebauer et al.
Example 2. Continuing Example 1, the conﬂict-free sets in our argumentation frame-
work are {a,e}, {b,d}, and all their subsets. The only two nonempty admissible sets are
{d} (since d defends itself against e’s attack) and {b,d} (as a’s attack on b is defended
by d, and e’s attack on d is defended by b and d). However, no other nonempty, conﬂict-
free set can defend all its arguments: No member of {a} or {a,e} defends a against c’s
attack; {e} lacks a defending attack against b’s attack; and {b} cannot be defended
against a’s attack. Among {d} and {b,d}, only the latter is complete, since d defends
b against a’s attack. Further, the empty set is complete, since there are no unattacked
arguments—i.e., there are no arguments that are defended by the empty set. Since /0
is complete, it is also the (unique) grounded extension. On the other hand, the com-
plete extension {b,d} is the only preferred (i.e., maximal admissible) extension. Since
d attacks each of a, c, and e, the preferred extension {b,d} is also a stable extension
(and since there are no other preferred extensions, it is the only stable extension).
We are interested in the existence problem restricted to nonempty extensions (the
nonemptiness problem, for short) for the above semantics in control argumentation
frameworks. In standard argumentation frameworks, the existence problem was ﬁrst
studied by Dimopoulos and Torres [5] and later on by Dunne and Wooldridge [8]. For
each semantics s ∈{CF,AD,CP,GR,PR,ST}, deﬁne the following problem:
s-NONEMPTINESS (s-NE)
Given:
An argumentation framework ⟨A,R ⟩.
Question: Does there exist a nonempty set S ⊆A satisfying the conditions speci-
ﬁed by semantics s?
2.2
Control Argumentation Frameworks
Next, we formally deﬁne control argumentation frameworks due to Dimopoulos
et al. [4], while adapting some notation from Niskanen et al. [17].
Deﬁnition 2. A control argumentation framework (CAF) is a triple (F ,C,U) consist-
ing of a ﬁxed part F of elements known to exist, a control part C of elements that may
or may not exist (and whose existence is controlled by the agent), and an uncertain part
U of elements that may or may not exist (and whose existence is not controlled by the
agent):
F = ⟨AF,RF⟩with RF ⊆(AF ∪AU)×(AF ∪AU),
C = ⟨AC,RC⟩with RC ⊆(AC ×(AF ∪AC ∪AU))∪((AF ∪AC ∪AU)×AC),
U = ⟨AU,RU ∪R ↔
U ⟩with RU,R ↔
U ⊆(AF ∪AU)×(AF ∪AU),
where AF, AC, and AU are pairwise disjoint sets of arguments, and RF, RC, RU, and
R ↔
U
are pairwise disjoint sets of attacks. While the existence of attacks in RU is not
known at all, attacks in R ↔
U are known to exist, but their direction is unknown.
For elements in the control part, we always look for only (at least) one conﬁguration
to achieve the given goal, while for elements in the uncertain part, we always require all

Complexity of Nonemptiness in Control Argumentation Frameworks
121
their completions to achieve that goal. Also, the control part always “moves ﬁrst,” i.e.,
a conﬁguration of the control part must be chosen before knowing how the uncertain
part will be completed.
Example 3. Consider a control argumentation framework with the argument sets AF =
{a,c,d},AC = {b},AU = {e} and the attack relations RF = {(c,a),(d,a), (d,c)},
RC = {(a,b),(b,e)}, RU = {(c,c)}, and R↔
U = {(d,e),(e,d)}. Its graph representation
is given in Fig. 2, where we follow the style of display introduced in the original work
on CAFs by Dimopoulos et al. [4].
Fig. 2. Control argumentation framework for Example 3, where control arguments are drawn as
solid rectangles, uncertain arguments as dashed rectangles, attacks from RC are drawn as bold-
faced arrows, uncertain attacks from RU are drawn as dotted arrows, and uncertain attacks from
R ↔
U are drawn as doubleheaded dashed arrows.
Conﬁgurations of the control part are formally captured by the notion of control
conﬁgurations, which are used to represent a move of the controlling agent. Here, the
arguments from AC that the agent wants to use are included in the control conﬁguration,
and the arguments that the agent does not want to include are discarded.
Deﬁnition 3. A subset Aconf ⊆AC is called a control conﬁguration. Aconf induces a
CAF (F ,⟨Aconf,Rconf⟩,U) with Rconf =RC|(AF∪AU∪Aconf), where the corresponding con-
trol move has been made (we deﬁne R |A = R ∩(A ×A)). When the CAF is known from
context, we may sometimes use the control conﬁguration Aconf to represent its induced
CAF.
Now, assuming that a control conﬁguration has been chosen, the remaining uncer-
tainty of the uncertain part is resolved via a completion.
Deﬁnition 4. A completion of a CAF (F ,⟨Aconf,Rconf⟩,U) is an argumentation frame-
work AF∗= (A∗,R ∗) with
AF ∪Aconf ⊆A∗⊆AF ∪Aconf ∪AU,
(RF ∪Rconf)|A∗⊆R ∗⊆(RF ∪Rconf ∪RU ∪R ↔
U )|A∗,
and it is required that (a,b) ∈R ∗or (b,a) ∈R ∗(or both) for all (a,b) ∈R ↔
U .
Example 4. We continue Example 3. Using the control conﬁguration {b} and the com-
pletion that includes the uncertain argument e and all uncertain attacks (c,c), (d,e), and
(e,d) produces the AF of Example 1, with the properties mentioned in Example 2.

122
D. Neugebauer et al.
2.3
Some Background on Complexity Theory
We will study the computational complexity of the nonemptiness problem for CAFs to
be deﬁned in the next section. We assume that the reader is familiar with the needed
background on complexity theory, such as the complexity classes of the polynomial
hierarchy [15,22], the zeroth level of which is P, the ﬁrst level of which consists of NP
and coNP, the second level of which consists of Σp
2 = NPNP and Πp
2 = coNPNP, and
the third level of which consists of Σp
3 = NPNPNP and Πp
2 = coNPNPNP. We also assume
familiarity with the concepts of hardness and completeness based on polynomial-time
many-one reducibility. For more background on computational complexity, we refer the
reader to, e.g., the textbooks by Papadimitriou [18] and Rothe [20].
3
Complexity of Nonemptiness in CAFs
For CAFs, we generalize the nonemptiness problem following the pattern of the control-
lability problem [17]. The generalized problem s-CONTROL-NONEMPTINESS wraps
the s-NE problem for standard argumentation frameworks with an outer existential
quantiﬁer over control conﬁgurations followed by an inner universal quantiﬁer over
completions. Using the notation from Deﬁnitions 2 and 3, we deﬁne this problem as
follows for any semantics s.
s-CONTROL-NONEMPTINESS (s-CONNE)
Given:
A control argumentation framework (F ,C,U).
Question: Does there exist a control conﬁguration Aconf ⊆AC such that every
completion AF∗= (A∗,R ∗) of (F ,⟨Aconf,Rconf⟩,U) has a nonempty
s extension?
We now determine the computational complexity of s-CONNE for each semantics
s ∈{CF, GR,AD,CP,ST,PR}. First, we extend a result of Skiba et al. [21] for incom-
plete argumentation frameworks to CAFs and show that CF-CONNE can be solved in
polynomial time.
Proposition 1. CF-CONNE is in P.
Proof. Let CAF = (F ,C,U) be a given control argumentation framework. Consider
the control conﬁguration Amax
conf = AC of CAF that includes every control argument in C.
If the CAF induced by Amax
conf has an argument a for which the self-attack (a,a) does
not exist in any of the attack relations, then clearly {a} is a nonempty conﬂict-free set
in every completion, so CAF ∈CF-CONNE. On the other hand, if the CAF induced
by Amax
conf has no such argument without a self-attack, then there cannot be a nonempty
conﬂict-free set in any of its completions. Further, no control conﬁguration other than
Amax
conf can change this fact, since these cannot provide the existence of such an argument.
Therefore, it holds that CAF ̸∈CF-CONNE.
The control conﬁguration Amax
conf and its induced CAF can be created in polynomial
time, and checking whether there exists an argument without a self-attack in any of the
attack relations can be done in polynomial time, too.
⊓⊔

Complexity of Nonemptiness in Control Argumentation Frameworks
123
Next, we provide a result analogous to that of Skiba et al. [21] for incomplete argu-
mentation frameworks (IAFs) by showing that GR-CONNE can also be solved in poly-
nomial time.
Proposition 2. GR-CONNE is in P.
Proof. For a GR-CONNE instance (F ,C,U) with C = ⟨AC,RC⟩, consider the control
conﬁgurations A /0
conf = /0 and Aa
conf = {a} for each a ∈AC. For each of these |AC| + 1
control conﬁgurations, GR-CONNE can be solved by the following algorithm. Recall
that a nonempty grounded extension exists in a completion if and only if the completion
has an unattacked argument.
Consider a sequence AFi of completions for control conﬁguration Ax
conf that is con-
structed as follows. AF0 is the maximal completion that includes all uncertain elements
U and all selected control elements C x
conf:
AF0 = ⟨AF ∪Ax
conf ∪AU,RF ∪R x
conf ∪RU ∪{(a,b),(b,a) | (a,b) ∈R ↔
U }⟩.
Initially, let i = 1. Deﬁne AFi to be AFi−1 but with all arguments removed that originally
were in AU and are unattacked in AFi−1, and with all attacks incident to these arguments
removed as well. Repeat this step (incrementing i) until AFi = AFi−1 for some i. Then
AF∗= AFi is our so-called critical completion.
If AF∗has no unattacked arguments, it also has no nonempty grounded extension.
If, on the other hand, AF∗has an unattacked argument y, then every completion must
have an unattacked argument y: In every completion, either y itself is unattacked, or
else the completion must include some arguments u ⊆AU attacking y. The only can-
didates are removed by the algorithm. These uncertain arguments can only be attacked
by other uncertain arguments. Also, at least one of the uncertain arguments needs to
be not attacked by any other argument (deﬁnite or uncertain); otherwise, the algorithm
would not delete such an argument in the ﬁrst iteration. Hence, one argument must
be unattacked in any given completion. Note that excluding any additional uncertain
attacks from completions can only make it more likely for unattacked arguments to
exist in the completion, so we do not need to consider this separately in the algorithm.
Note further that if AF∗has a nonempty grounded extension then it is allowed to answer
“yes” for the given GR-CONNE instance.
Clearly, when we get a “yes” answer for at least one of the control conﬁgurations,
this allows a “yes” answer for the given instance of the GR-CONNE problem, since we
have found a control conﬁguration such that for all completions, a nonempty grounded
extension exists. On the other hand, if for all control conﬁgurations A /0
conf and Aa
conf
with a ∈AC, the answer is “no,” we can deduce a “no” answer for the given instance
of GR-CONNE, so we know that none of the control conﬁgurations A /0
conf or Aa
conf can
ensure the existence of an unattacked argument for all its completions. The only other
control conﬁguration left to be considered are those where multiple control arguments
are added simultaneously, and it is clear that these cannot create any new unattacked
arguments, since no new arguments are introduced, but only new attacks are possibly
included.
It is easy to see that this algorithm runs in polynomial time.
⊓⊔

124
D. Neugebauer et al.
Finally, in order to show hardness of the remaining cases of s-CONNE, we provide
in Deﬁnition 5 a translation from Σ3SAT instances to instances of s-CONNE, where
Σ3SAT is the canonical problem for the complexity class Σp
3 in the polynomial hierarchy
which asks, for a 3-CNF formula ϕ over a set X ⊎Y ⊎Z of propositional variables,1
whether (∃τX)(∀τY)(∃τZ)[ϕ[τX,τY,τZ] = true], where τχ denotes a truth assignment
over variables in a set χ, and ϕ[τχ] is the truth value that ϕ evaluates to under τχ.
Deﬁnition 5. Given a Σ3SAT instance (ϕ,X,Y,Z) with ϕ = 
i ci and ci = 
j αi, j for
each clause ci, where αi, j are the literals in clause ci, a control argumentation frame-
work CAF = (F ,C,U) representing (ϕ,X,Y,Z) is deﬁned by its ﬁxed part F :
AF = {¯xj | xj ∈X}∪{¯yk | yk ∈Y}∪{zℓ, ¯zℓ| zℓ∈Z}∪{ci | ci in ϕ}∪{ϕ,d},
RF = {(yk, ¯yk) | yk ∈Y}∪{(¯zℓ,zℓ),(zℓ, ¯zℓ) | zℓ∈Z}∪{(¯xj,ci) | ¬xj in ci}
∪{(yk,ci) | yk in ci}∪{(¯yk,ci) | ¬yk in ci}
∪{(zℓ,ci) | zℓin ci}∪{(¯zℓ,ci) | ¬zℓin ci}
∪{(ci,ϕ) | ci ∈ϕ}∪{(d,a) | a ∈{¯xj,yk, ¯yk,zℓ, ¯zℓ,cl}}∪{(ϕ,d),(d,d)};
its control part C:
AC = {xj | xj ∈X},
RC = {(xj, ¯xj),(d,xj) | xj ∈X}∪{(xj,ci) | xj in ci};
and its uncertain part U:
AU = {yk | yk ∈Y},
RU = R ↔
U = /0.
We call arguments ci clause arguments and arguments representing literals αi, j literal
arguments. A clause argument ci can be interpreted as “clause ci is unsatisﬁed.”
Every assignment τX on X corresponds to a control conﬁguration AτX
conf for CAF that
includes a control argument xj if and only if τX(xj) = true. Further, given an assign-
ment τX on X and the CAF induced by the control conﬁguration AτX
conf , every assignment
τY on Y corresponds to a completion AFτX,τY of the CAF induced by the control con-
ﬁguration, where an uncertain argument yk is included in the completion if and only if
τY(yk) = true. For full assignments τX, τY, and τZ, we denote by AτX,τY [τX,τY,τZ] the
corresponding set of arguments in the completion AFτX,τY :
AτX,τY [τX,τY,τZ] ={xj | τX(xj) = true}∪{¯xj | τX(xj) = false}∪
{yk | τY(yk) = true}∪{¯yk | τY(yk) = false}∪
{zℓ| τZ(zℓ) = true}∪{¯zℓ| τZ(zℓ) = false}.
Example 5. Consider the Σ3SAT instance ϕ = c1 ∧c2 = (x1 ∨¬y1) ∧(y1 ∨¬z1), with
X = {x1}, Y = {y1}, and Z = {z1}. It is a “yes”-instance of Σ3SAT, since ϕ[τX,τY,τZ] =
1 ⊎denotes the disjoint union of sets.

Complexity of Nonemptiness in Control Argumentation Frameworks
125
Fig. 3. CAFs created for the Σ3SAT instances from Example 5 using the translation from Deﬁ-
nition 5. The left CAF uses ϕ = c1 ∧c2 = (x1 ∨¬y1) ∧(y1 ∨¬z1) and is a “yes”-instance of s-
CONNE for all s ∈{AD,ST,CP,PR}, whereas the right CAF uses ϕ′ = c′
1 ∧c′
2 = (x1 ∨¬z1)∧
(y1) and is a “no”-instance of s-CONNE for all s ∈{AD,ST,CP,PR}.
true for the assignment τX on X with τX(x1) = true, for any assignment τY on Y,
and for the assignment τZ on Z with τZ(z1) = false. The translation from Deﬁni-
tion 5 produces the CAF representation visualized in Fig. 3 (left), where we display
control arguments as rectangles, uncertain arguments as dashed rectangles, and ele-
ments of RC as boldfaced arrows. This CAF is a “yes”-instance of s-CONNE for all
s ∈{AD,ST,CP,PR}, since for the control conﬁguration {x1} and for every one of
its completions, the set {x1, ˆy1, ¯z1,ϕ} is nonempty and stable (and thus also admissible,
complete, and preferred), where ˆy1 = y1 if τY(y1) = true and ˆy1 = ¯y1 if τY(y1) = false.
Now consider a modiﬁed Σ3SAT instance ϕ′ = c′
1 ∧c′
2 = (x1 ∨¬z1) ∧(y1), where
¬y1 in the ﬁrst clause is replaced by ¬z1 and “∨¬z1” is removed from the sec-
ond clause. This instance is a “no”-instance of Σ3SAT, since for each assignment
τX on X, the assignment τY on Y with τY(y1) = false, and for each assignment τZ
on Z, the second clause is unsatisﬁed and thus ϕ′[τX,τY,τZ] = false. The correspond-
ing CAF is displayed in Fig. 3 (right). Irrespective of the control conﬁguration used,
the completion that excludes argument y1 has no literal argument that can defend ϕ′
against the clause argument c′
2, so this CAF is a “no”-instance of s-CONNE for all
s ∈{AD,ST,CP,PR}.
Next, we observe that the properties shown by Skiba et al. [21] (see their Lem-
mas 12 and 13) to hold for completions of IAFs created via their Deﬁnition 19 do hold
for completions of CAFs created via Deﬁnition 5 as well. This result follows directly
from [21, Lemma 12]: Given a Σ3SAT instance (ϕ,X,Y,Z) and letting X′ = X ∪Y and
Y ′ = Z, (ϕ,X′,Y ′) is a Π2SAT instance that has an IAF representation IAF via [21, Def-
inition 19]. The completion AFτX,τY considered in the current lemma is the same argu-
mentation framework as the completion AFτX′ of IAF, and the result of [21, Lemma 12]
for AFτX′ applies here.
The ﬁrst lemma states that the argument ϕ has to be in any nonempty extension
satisfying the semantics s ∈{AD,CP,GR,PR,ST}. Otherwise, the argument d is not
attacked and has to be in every potential admissible set, which is impossible as this
would conﬂict with the conﬂict-freeness of every set, as d attacks itself, so d cannot be
part of any conﬂict-free set.

126
D. Neugebauer et al.
Lemma 1. Given a Σ3SAT instance (ϕ,X,Y,Z), let CAF be the control argumentation
framework created for it according to Deﬁnition 5. Let τX and τY be full assignments
on X and Y, respectively. In the completion AFτX,τY , the argument ϕ has to be in every
nonempty extension that satisﬁes any of the semantics s ∈{AD,CP,GR,PR,ST}.
Similarly, Lemma 13 by Skiba et al. [21] directly provides the following result for
CAFs, which states that if there is a truth assignment for the Σ3SAT instance, there is
an extension satisfying the semantics s ∈{AD,CP,GR,PR,ST}.
Lemma 2. Let (ϕ,X,Y,Z) be a Σ3SAT instance and let τX, τY, and τZ be assignments
on X, Y, and Z, respectively. Let CAF be the control argumentation framework created
by Deﬁnition 5 for (ϕ,X,Y,Z). Let AFτX,τY be its completion corresponding to τX and
τY and let AτX,τY [τX,τY,τZ] be the set of literal arguments corresponding to the total
assignment. If ϕ[τX,τY,τZ] = true, then AFτX,τY has an extension AτX,τY [τX,τY,τZ] ∪
{ϕ} that is admissible, complete, preferred, and stable.
We are now ready to show Σp
3-completeness of s-CONNE for the remaining seman-
tics s ∈{AD,CP,ST,PR}.
Theorem 1. s-CONNE is Σp
3-complete for each s ∈{AD,CP,ST,PR}.
Proof. To show membership of the problems s-CONNE in Σp
3, we look at their quan-
tiﬁer representation. For each semantics s ∈{AD,CP,ST,PR}, the problem can be
written as the set of all CAF (using the notation from Deﬁnitions 2 and 3) such that
(∃Aconf ⊆AC)(∀completion AF∗)[AF∗∈s-NE], and since s-NE is NP-complete for
these semantics s [3,5], this provides a Σp
3 upper bound of s-CONNE. Note that all
quantiﬁers in this representation are polynomially length-bounded in the size of the
encoding of the given CAF.
To prove Σp
3-hardness, let (ϕ,X,Y,Z) be a Σ3SAT instance. Let CAF = (F ,C,U)
be the control argumentation framework created for (ϕ,X,Y,Z) via Deﬁnition 5. We
will show that for any s ∈{AD,CP,PR,ST}, (ϕ,X,Y,Z) ∈Σ3SATif and only if CAF ∈
s-CONNE.
From left to right, assume that (ϕ,X,Y,Z) ∈Σ3SAT, i.e., there exists an assignment
τX on X such that for every assignment τY on Y, there exists an assignment τZ on Z such
that ϕ[τX,τY,τZ] is true. We then know from Lemma 2 that, in the completion AFτX,τY
of CAF, the set AτX,τY [τX,τY,τZ]∪{ϕ} is a nonempty extension that is admissible, com-
plete, stable, and preferred. Therefore, CAF ∈s-CONNE for s ∈{AD,CP,PR,ST}.
From right to left, assume that, for any s ∈{AD,CP,PR,ST}, CAF ∈s-CONNE
holds, i.e., there exists a control conﬁguration such that in all completions, there exists
a nonempty s extension E. Every nonempty s extension has to contain ϕ; otherwise, d
would not be attacked and thus has to be in every potentially admissible set, as it attacks
every other argument. However, d also attacks itself, so every extension containing d
would not be conﬂict-free and therefore not admissible. Therefore, no s extension E
can contain any clause argument ci; otherwise, we would create a set that is not conﬂict-
free. Since we have a control conﬁguration all completions of which have a nonempty
s extension, these extensions contain a literal argument v ∈X ∪Y ∪Z that attacks ci,
for each ci. It is obvious that E can only contain v or ¯v and not both. Hence, the exten-
sion contains ϕ and several literal arguments, which together are attacking every clause

Complexity of Nonemptiness in Control Argumentation Frameworks
127
Table 1. Summary of complexity results for s-CONNE for various semantics s, in comparison
with the known complexity results for s-NE, s-POSNE, and s-NECNE. C-c denotes “C-complete”
for a complexity class C. New results for s-CONNE are marked by the corresponding theorem or
proposition, and results due to previous work are displayed in grey. Results for s-NE in standard
argumentation frameworks are marked by the respective reference (where the result marked by ∗
is straightforward and not formally proven).
s
s-NE
s-POSNE
s-NECNE
s-CONNE
CF
in P
∗
in P
[21] in P
[21] in P
(Proposition 1)
GR in P
[16] in P
[21] in P
[21] in P
(Proposition 2)
AD NP-c [5]
NP-c [21] Πp
2-c [21] Σp
3-c (Theorem 1)
CP
NP-c [5]
NP-c [21] Πp
2-c [21] Σp
3-c (Theorem 1)
PR
NP-c [5]
NP-c [21] Πp
2-c [21] Σp
3-c (Theorem 1)
ST
NP-c [3]
NP-c [21] Πp
2-c [21] Σp
3-c (Theorem 1)
argument. This extension is already seen to be admissible, complete, and preferred, but
regarding stability we may have some literals vj ∈X ∪Y ∪Z for which neither argument
vj nor ¯vj is in E. Therefore, either vj or ¯vj has to be in E. Consequently, we can deﬁne
a total assignment based on the s extension depending on whether the literal arguments
are in or out of E: If a literal argument is in E, we set the corresponding literal to true;
otherwise we set its negation to true. This assignment must be a satisfying assignment,
since every clause argument is attacked by E and, accordingly, every clause is satisﬁed
by the assignment.
⊓⊔
Finally, let us consider a few special cases of control nonemptiness. The literature
on control argumentation frameworks also considers simpliﬁed CAFs, which are CAFs
where the sets of uncertain elements AU, RU, and R ↔
U
are empty. Clearly, the prob-
lem s-CONNE for simpliﬁed CAFs is equivalent to s-POSNE for argument-incomplete
argumentation frameworks as deﬁned by Skiba et al. [21], so their complexity results
for s-POSNE carry over.
Recently, Mailly [14] introduced the idea of possible controllability, related to the
concept of possible problem variants for incomplete argumentation frameworks. A cor-
responding problem possible control nonemptiness can be deﬁned, which asks, for a
given CAF, whether there exists a control conﬁguration such that there is a comple-
tion in which a nonempty extension exists. This problem is clearly in NP via the three
collapsing existential quantiﬁers, and NP hardness is directly inherited from s-NE for
s ∈{AD,CP,PR,ST}. For s ∈{CF,GR}, the complexity of this variant of nonempti-
ness remains an open problem.
4
Conclusion
The contribution of this work is the deﬁnition of variants of the nonemptiness problem
for control argumentation frameworks and a characterization of their computational
complexity. Table 1 summarizes our complexity results, also providing a comparison

128
D. Neugebauer et al.
with the corresponding complexity results of Chv´atal [3], Modgil and Caminada [16],
and Dimopoulos and Torres [5] for the analogous problems in standard argumentation
frameworks and of Skiba et al. [21] in incomplete argumentation frameworks.
Comparing the complexity of “necessary nonemptiness” in incomplete argumenta-
tion frameworks with that of nonemptiness in control argumentation frameworks, there
is a complexity jump from Πp
2-hardness to Σp
3-hardness, indicating a signiﬁcant dif-
ference between the two formalisms. Interestingly, the Σp
3-completeness that we deter-
mined for s-CONNE is the same as the complexity of credulous controllability for the
same four semantics s ∈{AD,CP,PR,ST} [17]. Credulous controllability is the prob-
lem of deciding whether there is a control conﬁguration, such that for all completions, a
given target set of arguments is a subset of at least one s extension. This is a reﬁnement
of s-CONNE, since s-CONNE only requires the existence of at least one nonempty s
extension. The coinciding complexities indicate that the hardness of credulous control-
lability in control argumentation frameworks lies predominantly in making sure that
an s extension exists in the ﬁrst place, while the additional requirement of enforcing
that the given target set of arguments is contained in that extension does not add to the
asymptotical complexity. In contrast, for the grounded semantics, credulous controlla-
bility is Σp
2-hard [17], while GR-CONNE is in P.
In future work, it would be interesting to extend the complexity analysis of
nonemptiness in CAFs to further semantics. Control argumentation frameworks model
argumentation by agents strategically. Relatedly, Maher [12] (see also [13]) introduced
a model of strategic argumentation by simulating a game. However, his model does not
allow any uncertainty regarding the attacks between arguments. Further, CAFs have
some similarities to AF expansion due to Baumann and Brewka [1], where new argu-
ments and new attacks incident to at least one new argument may be added. Opposed to
CAFs, the set of new arguments and attacks is not ﬁxed, and new attacks among existing
arguments are not allowed. One might also modify the model of control argumentation
frameworks by adding weights to the uncertain elements or by following a probabilistic
approach (see, e.g., the work of Dunne et al. [7], Li et al. [11], Fazzinga et al. [9], and
Gaignier et al. [10]). Such reﬁnements of the model of CAFs may lead to new chal-
lenges in future work. One could also consider the approach of Riveret et al. [19] who
combine probabilistic argumentation with reinforcement learning techniques.
Acknowledgments. This work was supported by Deutsche Forschungsgemeinschaft under
grants KE 1413/11-1, RO 1202/14-2, and RO 1202/21-1 and by the NRW project “Online Partic-
ipation.”
References
1. Baumann, R., Brewka, G.: Expanding argumentation frameworks: enforcing and monotonic-
ity results. In: Proceedings of the 3rd International Conference on Computational Models of
Argument, pp. 75–86. IOS Press (September 2010)
2. Baumeister, D., Neugebauer, D., Rothe, J., Schadrack, H.: Veriﬁcation in incomplete argu-
mentation frameworks. Artif. Intell. 264, 1–26 (2018)
3. Chv´atal, V.: On the computational complexity of ﬁnding a kernel. Technical report CRM-
300, Centre de Recherches Math´ematiques, Universit´e de Montr´eal (1973)

Complexity of Nonemptiness in Control Argumentation Frameworks
129
4. Dimopoulos, Y., Mailly, J., Moraitis, P.: Control argumentation frameworks. In: Proceed-
ings of the 32nd AAAI Conference on Artiﬁcial Intelligence, pp. 4678–4685. AAAI Press
(February 2018)
5. Dimopoulos, Y., Torres, A.: Graph theoretical structures in logic programs and default theo-
ries. Theoret. Comput. Sci. 170(1), 209–244 (1996)
6. Dung, P.: On the acceptability of arguments and its fundamental role in nonmonotonic rea-
soning, logic programming and n-person games. Artif. Intell. 77(2), 321–357 (1995)
7. Dunne, P., Hunter, A., McBurney, P., Parsons, S., Wooldridge, M.: Weighted argument sys-
tems: Basic deﬁnitions, algorithms, and complexity results. Artif. Intell. 175(2), 457–486
(2011)
8. Dunne, P., Wooldridge, M.: Complexity of abstract argumentation, chap. 5. In: Rahwan,
I., Simari, G. (eds.) Argumentation in Artiﬁcial Intelligence, pp. 85–104. Springer, Boston
(2009). https://doi.org/10.1007/978-0-387-98197-0 5
9. Fazzinga, B., Flesca, S., Parisi, F.: On the complexity of probabilistic abstract argumentation.
In: Proceedings of the 23rd International Joint Conference on Artiﬁcial Intelligence, pp. 898–
904. AAAI Press/IJCAI (August 2013)
10. Gaignier, F., Dimopoulos, Y., Mailly, J.G., Moraitis, P.: Probabilistic control argumentation
frameworks. In: Proceedings of the 19th International Conference on Autonomous Agents
and Multiagent Systems, pp. 519–527. IFAAMAS (2021)
11. Li, H., Oren, N., Norman, T.J.: Probabilistic argumentation frameworks. In: Modgil, S., Oren,
N., Toni, F. (eds.) TAFA 2011. LNCS (LNAI), vol. 7132, pp. 1–16. Springer, Heidelberg
(2012). https://doi.org/10.1007/978-3-642-29184-5 1
12. Maher, M.: Resistance to corruption of strategic argumentation. In: Proceedings of the 30th
AAAI Conference on Artiﬁcial Intelligence, pp. 1030–1036. AAAI Press (February 2016)
13. Maher, M.J.: Resistance to corruption of general strategic argumentation. In: Baldoni, M.,
Chopra, A.K., Son, T.C., Hirayama, K., Torroni, P. (eds.) PRIMA 2016. LNCS (LNAI), vol.
9862, pp. 61–75. Springer, Cham (2016). https://doi.org/10.1007/978-3-319-44832-9 4
14. Mailly, J.G.: Possible controllability of control argumentation frameworks. In: Proceedings
of the 8th International Conference on Computational Models of Argument, vol. 326, pp.
283–294 (2020)
15. Meyer, A., Stockmeyer, L.: The equivalence problem for regular expressions with squaring
requires exponential space. In: Proceedings of the 13th IEEE Symposium on Switching and
Automata Theory, pp. 125–129. IEEE Computer Society Press (1972)
16. Modgil, S., Caminada, M.: Proof theories and algorithms for abstract argumentation frame-
works, chap. 6. In: Rahwan, I., Simari, G. (eds.) Argumentation in Artiﬁcial Intelligence, pp.
105–129. Springer, Boston (2009). https://doi.org/10.1007/978-0-387-98197-0 6
17. Niskanen, A., Neugebauer, D., J¨arvisalo, M.: Controllability of control argumentation frame-
works. In: Proceedings of the 29th International Joint Conference on Artiﬁcial Intelligence,
pp. 1855–1861. ijcai.org (July 2020)
18. Papadimitriou, C.: Computational Complexity, 2nd edn. Addison-Wesley (1995)
19. Riveret, R., Gao, Y., Governatori, G., Rotolo, A., Pitt, J., Sartor, G.: A probabilistic argu-
mentation framework for reinforcement learning agents - towards a mentalistic approach to
agent proﬁles. J. Autonomous Agents Multi-agent Syst. 33(1–2), 216–274 (2019)
20. Rothe, J.: Complexity Theory and Cryptology. An Introduction to Cryptocomplexity. EATCS
Texts in Theoretical Computer Science. Springer, Heidelberg (2005). https://doi.org/10.
1007/3-540-28520-2
21. Skiba, K., Neugebauer, D., Rothe, J.: Complexity of possible and necessary existence prob-
lems in abstract argumentation. In: Proceedings of the 24th European Conference on Artiﬁ-
cial Intelligence. Frontiers in Artiﬁcial Intelligence and Applications, vol. 325, pp. 897–904.
IOS Press (August/September 2020)
22. Stockmeyer, L.: The polynomial-time hierarchy. Theoret. Comput. Sci. 3(1), 1–22 (1976)

Generalizing Complete Semantics
to Bipolar Argumentation Frameworks
Nico Potyka(B)
University of Stuttgart, Stuttgart, Germany
nico.potyka@ipvs.uni-stuttgart.de
https://www.ipvs.uni-stuttgart.de/de/institut/team/Potyka-00001/
Abstract. Computational models of argumentation are an interesting
tool to represent decision processes. Bipolar abstract argumentation
studies the question of which arguments a rational agent can accept given
attack and support relationships between them. We present a generaliza-
tion of the fundamental complete semantics from attack-only graphs to
bipolar graphs. As opposed to previous semantics, the new bi-complete
semantics treats attacks and supports symmetrically and directly leads
to natural generalizations of grounded, preferred, semi-stable and stable
semantics. We show, in particular, that bipolar graphs are strictly more
expressive than attack-only graphs under bi-complete semantics. That
is, the meaning of support edges cannot be encoded by attack edges like
in deductive support frameworks.
Keywords: Abstract argumentation · Bipolar argumentation ·
Argumentation semantics
1
Introduction
Computational argumentation provides models for reasoning based on pro and
contra arguments. Given a set of arguments that may mutually attack or support
each other, argumentation answers the question of which arguments can reason-
ably be accepted. Abstract argumentation [12] abstracts from the content of the
arguments and focuses merely on their relationships. While this abstraction goes
too far for some domains, abstract argumentation frameworks have been applied
successfully in diverse areas like computational persuasion [6,14,15], recommender
systems [25] or review aggregation [11]. Even though such applications often proﬁt
from a quantitative interpretation based on numerical degrees of acceptance, clas-
sical models remain interesting in their own right and as a basis for non-classical
models. For example, [16] recently proposed probability distributions over classi-
cal argumentation graphs to learn user models from data. Classical argumentation
frameworks can also often be turned into Markov networks that allow for uncertain
reasoning over arguments [22].
Our focus here is on classical bipolar argumentation frameworks that consider
abstract arguments and attack and support relations between them [3,7,10,18].
c
⃝Springer Nature Switzerland AG 2021
J. Vejnarov´a and N. Wilson (Eds.): ECSQARU 2021, LNAI 12897, pp. 130–143, 2021.
https://doi.org/10.1007/978-3-030-86772-0_10

Generalizing Complete Semantics to Bipolar Argumentation Frameworks
131
It is interesting to note that supports are often not considered as direct coun-
terparts of attacks, but rather as meta-relations with a special meaning. For
example, bipolar argumentation with deductive support uses support relations
only to derive indirect attacks between arguments [10]. In this way, the evaluation
of bipolar graphs can be reduced to the better studied evaluation of attack-only
graphs. However, this approach favors attack relationships as we will discuss
later. Another example is bipolar argumentation with evidential support [18].
Here, nothing can be accepted unless directly or indirectly supported by special
prima-facie arguments. Furthermore, attacks can only be successful if they are
supported. In this approach, support can be seen as the dominating relationship.
Our focus here is on bipolar argumentation frameworks that attempt to treat
attacks and supports equally. Recently, some novel bipolar semantics have been
introduced in [22,23] that are inspired by the idea of deductive supports from
[10], but try to balance the power of attacks and supports. The s-deductive and
m-deductive semantics from [23] generalize stable semantics from attack-only
graphs to bipolar graphs. However, while they cannot label arguments undecided
in attack-only graphs like stable semantics, they can label arguments undecided
in bipolar graphs. Furthermore, since the stable semantics is a reﬁnement of
the more fundamental complete semantics, one may wonder, is there a natu-
ral generalization of complete semantics to bipolar graphs? We present such a
generalization in Sect. 3 that we call bi-complete semantics. It is strictly more
expressive than the deductive support framework from [10] in the sense that the
meaning of bipolar graphs under bi-complete semantics cannot be encoded in an
attack-only graph under most reasonable semantics. The bi-complete semantics
can be reﬁned in diﬀerent ways analogous to reﬁnements of complete semantics.
2
Background
We begin with an overview of the relevant background. A Dung-style (ﬁnite)
abstract argumentation framework (AAF) is a tuple (A, Att), where A is a ﬁnite
set of (abstract) arguments and Att ⊆A × A is the attack relation [12]. For
our purposes, an argument is simply an entity that can be accepted or rejected.
If (A, B) ∈Att, we say that A attacks B. With a slight abuse of notation, we
let Att(A) = {B | (B, A) ∈Att} denote the set of attackers of A. Semantics of
argumentation frameworks can be deﬁned in terms of extensions or labellings
in an equivalent way [9]. We will use labellings here. A labelling is a function
L : A →{in, out, und} that assigns to each argument a label. We say that an
argument A is accepted if L(A) = in and that A is rejected if L(A) = out.
Following [9], we call a labelling.
Complete: if L satisﬁes
1. L(A) = in if and only if L(B) = out for all B ∈Att(A).
2. L(A) = out if and only if L(B) = in for some B ∈Att(A).
For a labelling L and a label lab ∈{in, out, und}, we let lab(L) = {A ∈A |
L(A) = lab} denote those arguments that receive the label lab. We say that a
complete labelling is

132
N. Potyka
Fig. 1. BAF (A, Att, Sup) with A = {A, B, C}, Att = {(B, A)}, Sup = {(C, A)}.
Grounded: if in(L) is minimal with respect to ⊆.
Preferred: if in(L) is maximal with respect to ⊆.
Semi-stable: if und(L) is minimal with respect to ⊆.
Stable: if und(L) = ∅.
In many applications it is useful to consider not only attack relations, but
also support relations. A bipolar argumentation framework (BAF) is a tuple
(A, Att, Sup), where A is a ﬁnite set of arguments, Att ⊆A × A is the attack
relation as before and Sup ⊆A × A is the support relation [10]. We also assume
that Att ∩Sup = ∅. We let again Sup(A) = {B | (B, A) ∈Sup} denote the set
of supporters of A. Graphically, we denote attack relations by solid edges and
support relations by dashed edges. Figure 1 shows an example BAF with one
attack and one support. For example, in the context of a recommender system,
the arguments could be associated with the following meaning:
A: Movie m should be recommended because the user generally likes movies
from this genre.
B: M should not be recommended because the user generally does not like movies
with actor a.
C: M should be recommended because the user generally likes movies written
by writer w.
Diﬀerent semantics have been considered for bipolar graphs. The idea of
deductive and necessary support is to reduce the semantics of BAFs to AAF
semantics. The intuitive idea of deductive support [7,10] is that if an argument
is accepted, every argument that it supports must be accepted as well. The dual
idea, if an argument is accepted, all its supporters must be accepted as well, is
referred to as necessary support [10]. Since deductive support relations can be
translated to necessary support relations by just reversing their direction, we
will just look at the former here. A deeper discussion of both and other notions
of support can be found in [10]. The deductive support frameworks gives formal
meaning to support relations by translating the given BAF to an AAF with
new attacks [10]. These new attacks correspond to indirect attacks that result
from the interplay between attack and support relations. The following indirect
attacks have been considered for this purpose in [10]:
Supported Attack from A to B: there is a sequence of arguments A1, . . . , An
such that A1 = A, An = B, (Ai, Ai+1) ∈Sup for 1 ≤i ≤n −2 and
(An−1, An) ∈Att.
Mediated Attack from A to B: there is a sequence of arguments A1, . . . , An
such that A1 = B, An = A, (Ai, Ai+1) ∈Sup for 1 ≤i ≤n −2 and
(An, An−1) ∈Att.

Generalizing Complete Semantics to Bipolar Argumentation Frameworks
133
Intuitively, there is a supported attack from A to B iﬀA directly or indirectly
supports an attacker of B. There is a mediated attack from A to B iﬀA attacks
an argument that is directly or indirectly supported by B. The Dung framework
associated with (A, Att, Sup) is then deﬁned as the Dung framework (A, Att ∪
Atts ∪Attm), where Atts and Attm contain additional attacks that correspond
to supported and mediated attacks in (A, Att, Sup) [10]. While this is an elegant
way to extend AAF semantics to BAFs, the resulting semantics does not treat
attack and support equally. For example, in Fig. 1, it seems that A could as
well be accepted as rejected. However, the translation takes only account of the
mediated attack from B to C and ignores the fact that A has a supporter. Hence,
the only complete labelling of the associated Dung framework labels B in and
both A and C out. The disparity between attack and support becomes more
prevalent when we keep our attacker B, but add n supporters Ci of A. Then B
will still be accepted and A and all of its supporters Ci will be rejected even
when A is supported by thousands of undisputed arguments. In the context of
a recommender system, this would mean that an item cannot be recommended
anymore if there is a single negative aspect about the item.
In order to overcome this limitation, some alternatives have been presented
recently that focus on equal treatment of attack and support. Deductive labellings
are deﬁned by ﬁrst weakening the conditions of complete labellings, and then
adding a symmetrical support condition for every attack condition [22]. While
the resulting semantics treats attacks and supports equally and respects sup-
ported and mediated attacks, it can be very indecisive. S-deductive and m-
deductive labellings overcome the problem by introducing a slight asymmetry
by favoring labelling arguments in, but still treat attacks and supports equally
[23]. The m-deductive labellings also take the number of attackers and supporters
into account to decide the state of an argument based on the state of its attackers
and supporters. In this way, they can be more decisive than s-deductive labellings
[23]. Both s-deductive and m-deductive labellings generalize stable labellings for
AAFs to BAFs in the following sense: in a BAF (A, Att, Sup) with empty sup-
port relation Sup = ∅, the s-deductive and m-deductive labellings are exactly
the stable labellings of the corresponding AAF (A, Att) and vice versa. This is
a nice property to have since it shows consistency with a well-established AAF
semantics. However, s-deductive labellings are perhaps not the most natural gen-
eralization of stable labellings since they allow labelling arguments undecided in
bipolar graphs. Furthermore, since stable labellings are derived from complete
labellings in AAFs, it is natural to ask, is there a generalization from complete
labellings to BAFs? Such a generalized semantics could then be extended to
grounded, preferred, semi-stable and stable variants like in the AAF setting.
3
Bi-Complete Semantics
Motivated by the previous discussion, we now present a generalization of com-
plete semantics from AAFs to BAFs. Given a labelling L, we say that the

134
N. Potyka
Fig. 2. Supporting attack cycles.
Table 1. Bi-complete labellings for
the BAF in Fig. 2.
Labelling
A
B
C
D
L1
und
und
und
und
L2
in
out
in
out
L3
out
in
out
in
attackers of an argument dominate its supporters if |{B ∈Att(A) | L(B) =
in}| > |{B ∈Sup(A) | L(B) ̸= out}|. That is, for every supporter that is not
out, there is an attacker that is in and there is at least one additional attacker that
is in. Intuitively, every non-rejected pro-argument is balanced out by an accepted
counterargument and there is an additional counterargument that breaks a poten-
tial tie. Symmetrically, the supporters of an argument dominate its attackers if
|{B ∈Sup(A) | L(B) = in}| > |{B ∈Att(A) | L(B) ̸= out}|. Given a BAF
(A, Att, Sup), we call a labelling L : A →{in, out, und}
Bi-complete: if L satisﬁes
1. L(A) = in if and only if L(B) = out for all B ∈Att(A) or A’s supporters
dominate its attackers.
2. L(A) = out if and only if A’s attackers dominate its supporters.
In line with complete semantics, if all attackers of A are labelled out, A must
be labelled in. However, A can also be labelled in when an attacker is in pro-
vided that the supporters dominate the attackers. In fact, A must be labelled
in whenever the supporters dominate the attackers. Similarly, A can and must
be labelled out when its attackers dominate its supporters. The reader may
notice that a bi-complete labelling is not conﬂict-free in the AAF sense. That
is, both an argument and one of its attackers can be accepted under bi-complete
semantics in the bipolar setting. However, as we will show later in Proposition
1, bi-complete labellings correspond to complete labellings when there are no
support edges. Therefore, they do respect conﬂict-freeness in the AAF setting. I
would argue that conﬂict-freeness is not necessarily desirable in the bipolar set-
ting. This is because people tend to consider both pro and contra arguments and
not only contra and contra-contra arguments. Therefore, a pro argument should
be able to compensate for a contra argument in my opinion. Let us illustrate the
behaviour of the bi-complete semantics with some examples.
Example 1. Let us compute the bi-complete labellings for the BAF in Fig. 2.
Since B and C have no attackers, they must be labelled in. Since there is no
domination between attackers and supporters, A must be labelled undecided.
Hence, there is only a single bi-complete labelling.

Generalizing Complete Semantics to Bipolar Argumentation Frameworks
135
Fig. 3. Dominating attackers (left) and supporters (right).
Example 2. In the previous example, we had only one bi-complete labelling. The
BAF in Fig. 2 shows an example with multiple bi-complete labellings. To begin
with, all arguments can be labelled undecided. Another labelling labels A and C
in and B and D out. The out-state of B and D is now justiﬁed by the in-state
of A and C and their in-state is justiﬁed by the fact that B and D are out.
Symmetrically, another labelling labels A and C out and B and D in. Table 1
summarizes all bi-complete labellings.
Example 3. The BAF in Fig. 3 shows two BAFs with dominating attackers and
supporters. In both BAFs, B, C and D must be labelled in since they are
unattacked. For the BAF on the left, A must be labelled out because the attack-
ers dominate the supporters. Symmetrically, A must be labelled in for the BAF
on the right. For both BAFs, there is only a single bi-complete labelling.
3.1
Expressiveness
We now look at the expressiveness of bipolar frameworks under bi-complete
labellings. To begin with, we note that the bi-complete semantics generalizes
the complete semantics in the sense of the following deﬁnition.
Deﬁnition 1. Let S be a labelling-semantics for AAFs and let S′ be a labelling-
semantics for BAFs. We say that S′ generalizes S to BAFs if
1. for every BAF (A, Att, Sup) with empty support relation Sup = ∅, every S′-
labelling is a S-labelling for the AAF (A, Att) and
2. for every AAF (A, Att), every S-labelling is a S′-labelling for the BAF
(A, Att, ∅).
Intuitively, if a BAF semantics generalizes an AAF semantics, bipolar frame-
works under the BAF semantics are at least as expressive as AAFs under the
AAF semantics.
Proposition 1. The bi-complete semantics generalizes the complete semantics
to bipolar graphs.
Proof. First assume that L is a bi-complete labelling for (A, Att, ∅). Since there
are no supporters, we have L(A) = in if and only if all attackers are out. We have
L(A) = out if and only if the attackers dominate the supporters. Since there are

136
N. Potyka
no supporters this is the case if and only if at least one attacker is in. Hence, L
is a complete labelling for (A, Att).
Now assume that L is a complete labelling for (A, Att). L(A) = in if and
only if all attackers are out. Since there are no supporters, the case that the
supporters dominate the attackers cannot occur and condition 1 of bi-complete
labellings is satisﬁed as well. L(A) = out if and only if at least one attacker
is in, which is the case if and only if the attackers dominate the supporters as
explained above. Hence, L is also a bi-complete labelling for (A, Att, ∅).
⊓⊔
Obviously BAFs with deductive supports as deﬁned in [10] generalize diverse
AAF semantics since they just translate a given BAF to the associated AAF.
However, one may argue that the deductive support BAFs do not add any expres-
siveness to Dung frameworks. Since every BAF is translated into an AAF, it
cannot capture anything that an AAF could not capture already. Of course, it
may provide a more natural model, but conceptually, the support edges are just
syntactic sugar without any additional expressive power. A natural question is
therefore, do BAFs with bi-complete semantics provide more expressiveness? In
order to answer this question, we ﬁrst deﬁne more precisely what we mean by
being more expressive.
Deﬁnition 2. A BAF labelling semantics S is more expressive than an AAF
labelling semantics S′ if there is a BAF (A, Att, Sup) such that there is no AAF
(A, Att′) such that the S-labellings for (A, Att, Sup) are exactly the S′-labellings
for (A, Att′).
Hence, we say that a BAF semantics is more expressive than an AAF semantics
if it is not possible to ﬁnd for every BAF a corresponding AAF with the same set
of arguments, but potentially diﬀerent attacks, such that the labellings for the
BAF are exactly the labellings for the AAF. Intuitively, this just means that the
meaning of the support edges cannot be encoded by attack relations already. To
put it another way, we could say that a BAF semantics is not more expressive
than an AAF semantics if there was a general translation function that maps
every BAF to an AAF with the same labellings. If the BAF semantics S is more
expressive than the AAF labelling semantics S′, it is impossible to deﬁne such
a translation. The deductive support framework from [10] is clearly not more
expressive than AAFs because for every BAF, the associated Dung framework
is an AAF with the exact same labellings. In contrast, bi-complete semantics
are more expressive than most reasonable AAF semantics as we explain in the
following proposition.
Proposition 2. Let S be an AAF semantics that satisﬁes
Isolated Acceptance: L(A) = in whenever Att(A) = ∅.
Hard Attack: L(A) = out whenever L(B) = in for some B ∈Att(A).
Then the bi-complete semantics is more expressive than S.
Proof. Note that the condition in Deﬁnition 2 is an existential statement. There-
fore, it suﬃces to give an example to prove the claim. We use the BAF in Fig. 4

Generalizing Complete Semantics to Bipolar Argumentation Frameworks
137
Fig. 4. BAF under bi-complete seman-
tics
that
cannot
be
expressed
as
an AAF under AAF semantics that
respect Isolated Acceptance and Hard
Attack.
Table 2. Bi-complete labellings for the
BAF in Fig. 4.
Labelling
A
B
C
D
E
F
L1
und
und
und
und
in
und
L2
und
und
in
out
in
und
L3
und
und
out
in
in
und
L4
in
out
und
und
in
und
L5
in
out
in
out
in
out
L6
in
out
out
in
in
und
L7
out
in
und
und
in
und
L8
out
in
in
out
in
und
L9
out
in
out
in
in
in
as an example. We have to show that there is no AAF ({A, B, C, D, E, F}, Att)
such that the S-labellings are the ones shown in Table 2. To do so, let us think
about candidates for the attack relations Att. Since L9 labels F in together with
B, D and E, none of them can be an attacker of F because of the Hard Attack
property. For the same reason, F cannot be self-attacking. Hence, the only pos-
sible attackers of F are A and C. However, A cannot attack F because L4 labels
A in and F undecided and C cannot attack F because L2 labels C in and F
undecided.
Hence, we must have Att(F) = ∅. But then Isolated Acceptance implies that
F must be accepted by every S-labelling.
Hence, there can be no attack relation Att such that the S-labellings of
({A, B, C, D, E, F}, Att) correspond to the bi-complete labellings of the BAF in
Fig. 4.
⊓⊔
We make two assumptions about the AAF semantics. The assumptions are
very weak and are, in particular, satisﬁed by complete semantics and its reﬁne-
ments. Isolated Acceptance demands that all isolated arguments are accepted.
This is a special case of the suﬃcient in-condition of complete semantics (if
there are no attackers, all attackers are trivially out). Hard Attack corresponds
to the suﬃcient out-condition of complete semantics. Note, in particular, that
every semantics that satisﬁes Hard Attack also satisﬁes conﬂict-freeness. To talk
about the relationships between diﬀerent semantics, we introduce some addi-
tional terminology.
Deﬁnition 3. A BAF labelling semantics S is strictly more expressive than an
AAF labelling semantics S′ if
1. S generalizes S′ to BAFs and
2. S is more expressive than S′.

138
N. Potyka
The following corollary uses this terminology to make some explicit statements
about the relationships between the bi-complete semantics and the AAF seman-
tics that we discussed previously.
Corollary 1. 1. The bi-complete semantics is more expressive than the com-
plete, grounded, preferred, semi-stable and stable semantics.
2. The bi-complete semantics is strictly more expressive than the complete
semantics.
Proof. 1) follows from Proposition 2 by observing that all ﬁve semantics satisfy
Isolated Acceptance and Hard Attack. 2) follows from Proposition 1 and 2.
⊓⊔
3.2
Reﬁnements
We now look at reﬁnements of bi-complete labellings that are deﬁned analo-
gously to reﬁnements of complete labellings for AAFs. We say that a bi-complete
labelling is
Bi-Grounded: if in(L) is minimal with respect to ⊆.
Bi-Preferred: if in(L) is maximal with respect to ⊆.
Bi-Semi-stable: if und(L) is minimal with respect to ⊆.
Bi-Stable: if und(L) = ∅.
We ﬁrst note that these labellings generalize the corresponding AAF-labellings
to BAFs in the sense of Deﬁnition 1.
Corollary 2. 1. The bi-grounded semantics generalizes the grounded semantics
to BAFs.
2. The bi-preferred semantics generalizes the preferred semantics to BAFs.
3. The bi-semi-stable semantics generalizes the semi-stable semantics to BAFs.
4. The bi-stable semantics generalizes the stable semantics to BAFs.
Proof. We know from Proposition 1 that the complete labellings in AAFs cor-
respond to the bi-complete labellings in BAFs. Hence, the complete labellings
that minimize/maximize/exclude a particular label in an AAF are exactly the
bi-complete labellings that minimize/maximize/exclude this label in a BAF. ⊓⊔
Stable labellings for AAFs typically do not exist in graphs with odd cycles. The
BAF in Fig. 2 demonstrates that, in the bipolar setting, bi-stable labellings do
not necessarily exist even in tree-structured bipolar graphs. In this example, the
bi-grounded, bi-prefered and bi-semi-stable labellings coincide with the unique
bi-complete labelling, of course.
The BAF in Fig. 2 is a more interesting example. Its bi-complete labellings
are shown in Table 1. L1 is the bi-grounded, while L2 and L3 are the bi-preferred,
bi-semi-stable and bi-stable labellings.
The bi-complete labellings of the BAF in Fig. 4 are shown in Table 2. L1 is
the bi-grounded labelling and L9 is the bi-preferred labelling. L5 and L9 are the
bi-semi-stable and bi-stable labellings.

Generalizing Complete Semantics to Bipolar Argumentation Frameworks
139
The reﬁnements of bi-complete semantics may potentially loose expressive-
ness. It seems not even obvious that they are still more expressive than their
direct counterparts in AAFs. However, as we show in the following proposition,
bi-semi-stable and bi-stable semantics still provide a similar expressiveness as
bi-complete semantics.
Proposition 3. Let S be an AAF semantics that satisﬁes
Isolated Acceptance: L(A) = in whenever Att(A) = ∅.
Hard Attack: L(A) = out whenever L(B) = in for some B ∈Att(A).
Then the bi-semi-stable and bi-stable semantics are more expressive than S.
Fig. 5. BAF under bi-semi-stable/bi-stable semantics that cannot be expressed as an
AAF under AAF semantics that respect Isolated Acceptance and Hard Attack.
Table 3. Bi-semi-stable/bi-stable labellings for the BAF in Fig. 5.
Labelling A
B
C
D
E
F
G
L1
in
out in
out out in
in
L2
out in
in
out out in
in
L3
in
out out in
out in
in
L4
out in
out in
out in
in
L5
out in
out in
in
out in
L6
in
out in
out in
out out
Proof. Consider the BAF in Fig. 5. The bi-semi-stable/bi-stable labellings are
shown in Table 3. We show that there is no AAF ({A, B, C, D, E, F, G}, Att)
such that the S-labellings coincide with the bi-semi-stable/bi-stable labellings of
the BAF. To do so, we ﬁrst show that G must be isolated. Since G is in together
with A, C and F in L1, they cannot be attackers for otherwise the Hard Attack
property would be violated. Similarly, L4 shows that B and D cannot be attack-
ers and L5 that E cannot be an attacker. Since G is in in several labellings, there
can be no self-attack either. Hence, G must be isolated. But then G must be
labelled in because of the Isolated Acceptance property. Therefore, L6 is not an
S-labelling.
⊓⊔

140
N. Potyka
Together with Corollary 2, we get, in particular, the following result.
Corollary 3. 1. The bi-semi-stable semantics is strictly more expressive than
the semi-stable semantics.
2. The bi-stable semantics is strictly more expressive than the stable semantics.
Fig. 6. Example BAF from [3].
4
Related Work
Several other bipolar argumentation approaches have been discussed in the past.
We gave a brief overview of deductive support [10] and evidential support [18] in
the introduction. A more elaborate discussion of both approaches can be found
in [10]. Several chapters in the handbook of formal argumentation deal with
other design ideas for bipolar semantics [4]. A more recent discussion and anal-
ysis of bipolar approaches can be found in [24,27]. The authors in [13] recently
considered the idea of monotonic support. That is, adding a supporter should
never downgrade the acceptance status of an argument and should potentially
allow to upgrade it. Intuitively, bi-complete semantics should satisfy this notion
of monotonicity as well, but I have not checked the formal deﬁnition. It may be
interesting to consider a symmetric notion of monotonicity for attack and check
if these notions are satisﬁed by bipolar frameworks with balanced attacks and
supports.
Here, we built up on the ideas in [22] and [23] and focussed on bipolar argu-
mentation with balanced attacks and supports. As opposed to these works, we
designed our new semantics by reﬁning complete semantics for bipolar graphs.
In this way, we obtain natural generalizations of grounded, preferred, semi-stable
and stable semantics. Our bi-stable semantics here diﬀer, in particular, from the
s-deductive and m-deductive semantics in [23] that generalize stable semantics
as well. The bi-stable semantics remains closer to the original stable semantics
in that it does not allow labelling arguments undecided even in bipolar graphs.
Instead of deﬁning new bipolar semantics by extending AAF labellings, one
may try to extend AAF extensions. This idea has indeed been studied in [3].
The authors consider a reﬁnement of admissible sets that demands that the set
is conﬂict-free, defends all its elements and is closed under the support relation.
Figure 6 shows an example from [3]. The only admissible set is {D, E, F} [3]. To
see this, note that C cannot be defended against the attack by E. Therefore,
it cannot be accepted. Since accepting A or B would entail accepting C, they

Generalizing Complete Semantics to Bipolar Argumentation Frameworks
141
cannot be accepted either. As this example shows, attack relations are again
stronger than support relations under this semantics because C is necessarily
rejected even though it is both attacked and supported.
Bipolar argumentation also plays a prominent role in gradual argumentation
frameworks as discussed in [1–3,5,20,26]. Roughly speaking, gradual argumen-
tation frameworks compute a numerical strength value for every argument iter-
atively. This is done based on an initial base score and the current strength of
attackers and supporters. If this procedure converges, a well-deﬁned degree of
acceptance can be assigned to every argument. Since arguments are evaluated
numerically and the solution is uniquely deﬁned as the outcome of the itera-
tive procedure, the idea of balanced attacks and supports is easier to formalize.
Roughly speaking, an attack’s negative eﬀect on the base score should equal a
support’s positive eﬀect, see [21] Deﬁnition 5.1 and Proposition 5.2 for a more
precise deﬁnition. A discussion of the relevance of bipolar argumentation in the
context of probabilistic argumentation can be found in [19].
While pure AAFs can be suﬃcient for interesting applications, support edges
are vital for many recent applications of argumentation frameworks. For exam-
ple, in review aggregation [11], it is important that both positive and negative
aspects of a review are taken into account. In fake news detection [17], claims in
an article can be contradicted or supported by other sources. In product recom-
mendation [25], there can be features that make the product more (support) or
less (attack) interesting for a user. For these reasons, recent work on modeling
users’ beliefs using probability distributions over classical AAFs [16] could per-
haps be improved by replacing AAFs with BAFs. As we saw, the BAF semantics
considered here are indeed strictly more expressive than their AAF counterparts.
Even though we generalized AAF semantics to BAFs, our goal is not to design
a most general framework or to compete with such generalizations. For exam-
ple, Abstract Dialectical Frameworks (ADFs) [8] generalize AAFs by abstracting
away from the nature of edges between arguments and concrete semantics. This
is done by considering abstract acceptance conditions that deﬁne when an argu-
ment can be accepted given the state of its parents. It may be possible to capture
our semantics by appropriately deﬁned acceptance conditions in ADFs as well.
5
Discussion
Motivated by the observation that the s-deductive and m-deductive semantics
from [23] generalize stable semantics from attack-only graphs to bipolar graphs,
we proposed a more elementary generalization of complete semantics called
the bi-complete semantics. It yields natural generalizations of reﬁnements of
complete semantics like preferred and stable semantics. In particular, bi-stable
semantics generalize stable semantics in a stricter sense than the semantics from
[23] because they do not allow labelling arguments undecided in bipolar graphs.
As opposed to deductive support frameworks [10], BAFs under bi-complete, bi-
semi-stable and bi-stable semantics cannot be encoded by AAFs under most
reasonable AAF semantics. That is, support edges add expressive power over

142
N. Potyka
attack-only frameworks under these semantics. In particular, attacks and sup-
ports have approximately the same power and can cancel their eﬀects in a sym-
metrical manner.
Acknowledgements. This work was supported by the DFG through the projects
EVOWIPE (STA572/15-1) and COFFEE (STA572/15-2).
References
1. Amgoud, L., Ben-Naim, J.: Evaluation of arguments in weighted bipolar graphs.
In: Antonucci, A., Cholvy, L., Papini, O. (eds.) ECSQARU 2017. LNCS (LNAI),
vol. 10369, pp. 25–35. Springer, Cham (2017). https://doi.org/10.1007/978-3-319-
61581-3 3
2. Amgoud, L., Ben-Naim, J., Doder, D., Vesic, S.: Acceptability semantics for
weighted argumentation frameworks. In: International Joint Conference on Artiﬁ-
cial Intelligence (IJCAI) (2017)
3. Amgoud, L., Cayrol, C., Lagasquie-Schiex, M.C., Livet, P.: On bipolarity in argu-
mentation frameworks. Int. J. Intell. Syst. 23(10), 1062–1093 (2008)
4. Baroni, P., Gabbay, D.M., Giacomin, M., van der Torre, L.: Handbook of Formal
Argumentation. College Publications, London (2018)
5. Baroni, P., Romano, M., Toni, F., Aurisicchio, M., Bertanza, G.: Automatic eval-
uation of design alternatives with quantitative argumentation. Argum. Comput.
6(1), 24–49 (2015)
6. Black, E., Coles, A., Hampson, C.: Optimal simple strategies for persuasion. In:
European Conference on Artiﬁcial Intelligence (ECAI), pp. 1736–1737 (2016)
7. Boella, G., Gabbay, D.M., van der Torre, L., Villata, S.: Support in abstract argu-
mentation. In: International Conference on Computational Models of Argument
(COMMA), pp. 40–51. Frontiers in Artiﬁcial Intelligence and Applications, IOS
Press (2010)
8. Brewka, G., Woltran, S.: Abstract dialectical frameworks. In: International Confer-
ence on Principles of Knowledge Representation and Reasoning (KR), pp. 102–111
(2010)
9. Caminada, M.W., Gabbay, D.M.: A logical account of formal argumentation. Stu-
dia Logica 93(2–3), 109 (2009)
10. Cayrol, C., Lagasquie-Schiex, M.C.: Bipolarity in argumentation graphs: towards
a better understanding. Int. J. Approx. Reason. 54(7), 876–899 (2013)
11. Cocarascu, O., Rago, A., Toni, F.: Extracting dialogical explanations for review
aggregations with argumentative dialogical agents. In: International Conference on
Autonomous Agents and MultiAgent Systems (AAMAS), pp. 1261–1269 (2019)
12. Dung, P.M.: On the acceptability of arguments and its fundamental role in non-
monotonic reasoning, logic programming and n-person games. Artif. Intell. 77(2),
321–357 (1995)
13. Gargouri, A., Konieczny, S., Marquis, P., Vesic, S.: On a notion of monotonic
support for bipolar argumentation frameworks. In: International Conference on
Autonomous Agents and MultiAgent Systems (AAMAS) (2020)
14. Hadoux, E., Hunter, A.: Learning and updating user models for subpopulations in
persuasive argumentation using beta distribution. In: International Conference on
Autonomous Agents and MultiAgent Systems (AAMAS), vol. 17, pp. 1141–1149.
Association for Computing Machinery (ACM) (2018)

Generalizing Complete Semantics to Bipolar Argumentation Frameworks
143
15. Hunter, A.: Modelling the persuadee in asymmetric argumentation dialogues for
persuasion. In: International Joint Conference on Artiﬁcial Intelligence (IJCAI),
pp. 3055–3061 (2015)
16. Hunter, A., Noor, K.: Aggregation of perspectives using the constellations app-
roach to probabilistic argumentation. In: AAAI Conference on Artiﬁcial Intelli-
gence (AAAI), pp. 2846–2853 (2020)
17. Kotonya, N., Toni, F.: Gradual argumentation evaluation for stance aggregation in
automated fake news detection. In: Proceedings of the 6th Workshop on Argument
Mining, pp. 156–166 (2019)
18. Oren, N., Norman, T.J.: Semantics for evidence-based argumentation. In: Interna-
tional Conference on Computational Models of Argument (COMMA), pp. 276–284.
IOS Press (2008)
19. Polberg, S., Hunter, A.: Empirical evaluation of abstract argumentation: support-
ing the need for bipolar and probabilistic approaches. Int. J. Approx. Reason. 93,
487–543 (2018)
20. Potyka, N.: Continuous dynamical systems for weighted bipolar argumentation. In:
International Conference on Principles of Knowledge Representation and Reason-
ing (KR), pp. 148–157 (2018)
21. Potyka, N.: Extending modular semantics for bipolar weighted argumentation.
In: International Conference on Autonomous Agents and MultiAgent Systems
(AAMAS), pp. 1722–1730 (2019)
22. Potyka, N.: Abstract argumentation with markov networks. In: European Confer-
ence on Artiﬁcial Intelligence (ECAI), pp. 865–872 (2020)
23. Potyka, N.: Bipolar abstract argumentation with dual attacks and supports. In:
International Conference on Principles of Knowledge Representation and Reason-
ing (KR), pp. 677–686 (2020)
24. Prakken, H.: On validating theories of abstract argumentation frameworks: The
case of bipolar argumentation frameworks. In: International Conference on Com-
putational Models of Argument (COMMA), pp. 21–30 (2020)
25. Rago, A., Cocarascu, O., Toni, F.: Argumentation-based recommendations: Fan-
tastic explanations and how to ﬁnd them. In: International Joint Conference on
Artiﬁcial Intelligence (IJCAI), pp. 1949–1955 (2018)
26. Rago, A., Toni, F., Aurisicchio, M., Baroni, P.: Discontinuity-free decision support
with quantitative argumentation debates. In: International Conference on Princi-
ples of Knowledge Representation and Reasoning (KR), pp. 63–73 (2016)
27. Yu, L., Markovich, R., van der Torre, L.: Interpretations of support among argu-
ments. In: Legal Knowledge and Information Systems - JURIX 2020. Frontiers in
Artiﬁcial Intelligence and Applications, vol. 334, pp. 194–203. IOS Press (2020)

Philosophical Reﬂections on Argument
Strength and Gradual Acceptability
Henry Prakken1,2(B)
1 Department of Information and Computing Sciences, Utrecht University,
Utrecht, The Netherlands
h.prakken@uu.nl
2 Faculty of Law, University of Groningen, Groningen, The Netherlands
Abstract. This paper proposes a classiﬁcation of three aspects of argu-
ment strength based on philosophical insights, in particular Aristotle’s
distinction between logic, dialectic and rhetoric. It is then argued that
when developing or evaluating gradual accounts of argument strength
it is essential to be explicit about which aspect of argument strength
is modelled and about the adopted interpretation of the arguments and
their relations in abstract or bipolar argumentation frameworks. The
underlying aim is to encourage a principled development and evaluation
of (principles for) gradual argumentation semantics.
Keywords: Computational argument · Argument strength ·
Graduality
1
Introduction
A recent trend in the formal study of argumentation is the development of grad-
ual notions of argument acceptability. These notions are proposed as alternatives
to extension-based notions that are deﬁned on top of the theory of abstract [16] or
bipolar [13] argumentation frameworks. The gradual notions are often motivated
by a discontent with the fact that extension-based notions of acceptability only
allow for rather coarse distinctions between degrees of acceptability. The current
developments arguably go back to [12] and really took oﬀwith publications like
[29] and [1] (although largely ignored is that Pollock [35] already proposed a for-
malisation of gradual acceptability). In this body of work, the gradual nature of
argumentation can have various sources: diﬀerent base strengths of arguments,
diﬀerent sets or numbers of attackers and/or supporters, and attack or support
relations that hold to varying degrees.
Although the new developments are very interesting and the formal achieve-
ments have been impressive, there are also reasons to take a step back. To start
with, there is a need to reﬂect on which notions or aspects of argument accept-
ability, or argument strength, are modelled, and why proposed semantics or pro-
posed sets of principles for those semantics are good. What is needed is a concep-
tual or philosophical underpinning of the formal ideas and constructs. Further-
more, almost all work builds on abstract or bipolar argumentation frameworks and
c
⃝Springer Nature Switzerland AG 2021
J. Vejnarov´a and N. Wilson (Eds.): ECSQARU 2021, LNAI 12897, pp. 144–158, 2021.
https://doi.org/10.1007/978-3-030-86772-0_11

Philosophical Reﬂections on Argument Strength and Gradual Acceptability
145
thus does not give explicit formal accounts of the nature of arguments and their
relations, while yet this may be relevant when evaluating the formal proposals.
Moreover, the arguments in abstract and bipolar frameworks are increasingly not
seen as genuine arguments in the sense of inferential structures (as in Dung’s sem-
inal paper and much initial follow-up work) but as statements that can be true or
false. In this paper I will argue that some proposed (principles for) gradual seman-
tics, while making sense when arguments are interpreted as statements, make less
sense when arguments are regarded as inferential structures and hence cannot be
regarded as general accounts of argumentation semantics.
More generally, in this paper I aim to make two contributions. First, I will
propose a classiﬁcation of three aspects of argument strength based on philo-
sophical insights, in particular Aristotle’s distinction between logic, dialectic and
rhetoric. I then argue that when developing or evaluating gradual accounts of
argument strength it is essential to be explicit about which aspect of argument
strength is modelled and about the adopted interpretation of the arguments and
their relations. The underlying hope is that this paper will encourage a princi-
pled and focused development and evaluation of (principles for) gradual argu-
mentation semantics. The discussion will largely proceed in terms of examples of
principles and semantics, since given the fast growing literature a comprehensive
discussion and analysis is outside the scope of this paper.
The paper is organised as follows. First in Sect. 2 I informally sketch the
assumed formal background, and then in Sect. 3 I present the three aspects of
logical, dialectical and rhetorical strength. In Sect. 4 I discuss why it is important
to be explicit about which of these aspects is modelled and about the nature of
arguments and their relations. Finally, in Sect. 5 I conclude.
2
Background
In this section I informally summarise the formal background assumed in this
paper. The aim of my paper is not to carry out formal investigations but to
oﬀer a conceptual framework that can guide the development and evaluation of
speciﬁc formal proposals. I will therefore use as little formal notation as possible
and assume that the reader is familiar with the basics of formal argumenta-
tion, in particular of the theory of abstract [5] and bipolar [13] argumentation
frameworks and of the main structured accounts of argumentation [27].
2.1
Arguments as Statement or as Inferential Structures
In this subsection I explain in more detail the recent trend to view arguments
as statements and how this is relevant for the evaluation of gradual argumen-
tation semantics. Abstract argumentation frameworks as introduced by Dung
[16] consist of a set of arguments with a binary relation of attack. Nothing is
assumed about the structure of the arguments and the nature of the attack rela-
tions. Bipolar frameworks add to abstract argumentation frameworks a binary
support relation between arguments, sometimes but not always assumed to be

146
H. Prakken
disjoint from the attack relation. Various semantics have been proposed for eval-
uating the arguments in an abstract or bipolar argumentation framework. For
present purposes their details do not matter.
As noted in the introduction, the arguments in abstract or bipolar frameworks
are in applications increasingly not seen as genuine arguments in the sense of
inferential structures but as statements that can be true or false. For evaluating
(principles for) gradual argumentation semantics it is crucial how the nodes in an
argument graph are interpreted, so the adopted interpretation should be made
explicit. Sometimes this is indeed done, e.g. by Baroni et al. [7] and Rago et al.
[39], who present their model as a formalisation of the IBIS model [28]. Then
the formalism can be evaluated on its adequacy for what it was explicitly meant
for. However, often the ‘statement’ interpretation of the nodes in argumentation
frameworks is not made explicit but has to be inferred from the informal text and
from the examples that are given. For instance, in [42] an example is discussed
with the following (and some other) arguments:
S1: We should buy an SUV; it’s the right choice for us
C:
SUVs are very safe, safety is very important to us
where C supports S1. This support relation in fact expresses an argument with
premises SUVs are safe and Safety is important to us and the conclusion We
should buy an SUV, which readers familiar with the theory of argument schemes
will recognise as an instance of the argument scheme from good consequences. In
a formalisation where arguments are inferential structures this argument would
appear as a single node in the argument graph, while here it is spread out over
a subgraph of a bipolar argumentation framework with nodes C and S1.
The diﬀerence between the inferential and statement interpretations of the
nodes in an abstract or bipolar argumentation framework is important for the
design and choice of abstract formalisms. While bipolar argumentation frame-
works model support as a relation between arguments, when support expresses
some kind of inference, it can also be modelled inside arguments, at least if argu-
ments are interpreted as inferential structures. Then this can be done by using
abstract argumentation frameworks in combination with a theory of the struc-
ture of arguments and the nature of attack. There are quite a few such theo-
ries, dating back to the seminal work of Pollock [34]. All these theories allow for
support relations that are not between but inside arguments, namely as inferen-
tial relations between (sets of) statements in some logical language. Examples
are assumption-based argumentation [44], Defeasible Logic Programming [19] and
ASPIC +[37]. While this approach is possible when arguments are interpreted as
inferential structures, this is diﬀerent when they are interpreted as statements;
then support relations between arguments (viewed as statements) are needed, as
in bipolar argumentation frameworks. Relations between arguments then become
what philosophers call reasons [24,34]. In this approach, arguments-as-inferential-
structures are not nodes in but subgraphs of the argument graph, as illustrated by
the above example. Moreover, since in general statements are supported by sets of

Philosophical Reﬂections on Argument Strength and Gradual Acceptability
147
statements, we need support relations from sets of arguments to arguments, which
in ‘standard’ bipolar frameworks cannot be expressed. So how the arguments in
abstract and bipolar frameworks are interpreted greatly matters for what abstract
formalism is needed.
2.2
Basic Concepts of Argument Structure and Relations
I next informally sketch what I mean in this paper by arguments and their
relations, trying to remain as close as possible to the formal (e.g. [27]) and infor-
mal (e.g. [18]) literature on argument structure. Basic arguments have a set of
premises and a conclusion (statements that can be true or false) and an infer-
ence from the premises to the conclusion licensed by an inference rule. Basic
arguments can be combined into complex arguments by letting their conclusion
be among the premises of another argument. Arguments can be informally repre-
sented as directed acyclic hypergraphs, in a way similar to the usual visualisation
methods in argumentation theory [40], with the nodes corresponding to premises
or intermediate or ﬁnal conclusions and the links from sets of nodes to nodes
corresponding to inferences (see e.g. Figs. 1 and 2 below).
Both premises and inference rules can be attackable or non-attackable, so
arguments can also be attackable or non-attackable. I will call attackable and
non-attackable inference rules defeasible, respectively, deductive. An attackable
argument can be attacked in three ways: it can be undermined by an argument of
which the conclusion is incompatible with one of its premises, it can be rebutted
by an argument of which the conclusion is incompatible with an intermediate
or its conclusion, and it can be undercut by an argument of which the conclu-
sion says that some inference rule applied in the attacked argument does not
apply. Attacks can be allowed or not allowed depending on further constraints
on these informal deﬁnitions. Finally, allowed attacks can succeed or not suc-
ceed as defeats. Henceforth, when I say that argument A attacks argument B I
assume that the attack from A on B is allowed, while when I say that argument
A defeats argument B I assume that the attack from A on B succeeds as defeat.
This informal sketch could be regarded as an abstraction of the ASPIC +
framework. Depending on the precise formal deﬁnitions of arguments, of incom-
patibility of statements and of the constraints on allowed attacks and defeats,
one variant or another of this framework can be obtained, or a variant of some
related approach like assumption-based argumentation or defeasible logic pro-
gramming. For present purposes the precise design choices and the diﬀerences
between these formal frameworks do not matter.
3
Logical, Dialectical and Rhetorical Argument Strength
In classifying aspects of argument strength it is natural to take Aristotle’s famous
distinction between logic, dialectic and rhetoric as starting point. Very brieﬂy,
logic concerns the validity of arguments given their form, dialectic is the art of
testing ideas through critical discussion and rhetoric deals with the principles of

148
H. Prakken
eﬀective persuasion [17, Section 1.4]. Accordingly, I distinguish between logical,
dialectical and rhetorical argument strength.
Logical argument strength divides into two aspects: inferential and contextual
argument strength.
Inferential argument strength is about how well the premises support the con-
clusion if we only look at the arguments premises, inferences and conclusion(s).
Example criteria for argument strength are that arguments with only deduc-
tive inferences are stronger than arguments with defeasible inferences, or that
arguments with only non-attackable premises are stronger than arguments with
attackable premises. Such criteria can be reﬁned by combining them (for exam-
ple, ﬁrst looking at the type of inference and then for arguments with equally
strong inferences looking at the types of premises), by deﬁning preference rela-
tions on inference rules and/or premises, or by deﬁning notions of strength (for
example, probabilities) on inference rules and/or premises.
Contextual argument strength is about how well the conclusion of an argument
is supported if we look at the context of all relevant arguments. Formal frame-
works like Dung’s theory of abstract argumentation frameworks, assumption-
based argumentation, ASPIC + and defeasible logic programming formalise this
kind of argument strength. The reader might wonder why this is not called dialec-
tical strength, since after all, determining an argument’s contextual strength as
deﬁned here involves the comparison of argument and counterargument. Yet this
is not truly dialectical, since the just-mentioned formalisms do not model prin-
ciples of critical discussion but just deﬁne mathematical consequence notions on
the basis of a given body of information; likewise [20,30]. This even holds for
the argument games proposed as proof theories for extension-based semantics
[32]: these apply to a given framework of arguments and their relations, while
principles of discussion allow for introduction of new arguments during a dispute.
Dialectical argument strength looks at how well defended an argument is in the
context of an ongoing or terminated critical discussion. This context can be
regulated by formal or informal principles of fair and eﬀective disputes. Informal
examples are legal procedures or rules of order for meetings. Formal examples are
the many dialogue systems for argumentation proposed in philosophy and AI,
e.g. [4,20,31,36,46]. Dialectical strength has both static and dynamic aspects. A
static aspect is given by the outcome of a critical discussion: has the argument
been successfully defended in the discussion? Dynamic aspects concern how well-
defended or challengeable an argument is in a given state of the discussion. In [49]
the latter is formulated as “a function of the (un)availability of permissible move
sequences originating at the present dialogue stage, and ending in a discussant?s
role-speciﬁed goal being achieved”.
To illustrate the idea of dialectical strength, I next suggest some possible
criteria for determining dialectical argument strength, without claiming to be
exhaustive. First, one might regard an argument as dialectically weaker the more
attacks on it are allowed in the current state. Among other things, this may

Philosophical Reﬂections on Argument Strength and Gradual Acceptability
149
imply that arguments are dialectically weaker the more attackable premises or
inference rules they have. This idea is motivated by an underlying principle that
many decision makers are aware of, namely, to justify one’s decisions as sparsely
as possible, in order to minimize the chance of successful appeal.
One might also look at how many attacks an argument has survived in a given
state. For instance, if arguments are generated by argument schemes [47], one
might regard an argument as dialectically stronger the more critical questions
have been asked and successfully answered.
Yet another aspect of dialectical strength is to what extent it is possible to
change the current contextual strength of the argument by moving to a new
state. This aspect is arguably formalised by formal work on the dynamics of
argumentation, in particular on so-called preservation, realisability and enforce-
ment properties [8,15]. Preservation is about the extent to which the current
contextual status of arguments is preserved under change, while realisability
and enforcement concern the extent to which particular outcomes can or will be
obtained by changing the current state.
Rhetorical argument strength looks at how capable an argument is to persuade
other participants in a discussion or an audience. Persuasiveness essentially is a
psychological notion; although principles of persuasion may be formalised, their
validation as principles of successful persuasion is ultimately psychological (as
acknowledged in [26] and done in e.g. [23]). One way to formulate criteria for per-
suasiveness is in terms of agreement with shared background information or with
information in a model of the other discussants or the audience (cf. [25]), possi-
bly reﬁned with probability distributions on what can be in these models. Such
opponent models could also be used in game-theoretic investigations of optimal
debate strategies [41]. Another way is to formulate heuristics about what is gen-
erally known or expected to be persuasive, such as the argumentation techniques
of Perelman [33] or the argument schemes of Walton [45], or a technique such as
procatalepsis, modelled by Bonzon et al. [11], which is the attempt of a speaker
to strengthen their argument by dealing with possible counter-arguments before
their audience can raise them.
Argument strength is multi-faceted. The classiﬁcation proposed in this section
shows that argument strength is a multi-faceted notion. Not only can we distin-
guish between logical, dialectical and rhetorical argument strength but each of
these aspects of strength involves multiple criteria, which sometimes reinforce
but sometimes oppose each other, and which have to be combined to provide an
overall assessment of an argument’s logical, dialectical or rhetorical strength. In
fact, deﬁning such overall notions seems a daunting task, and it may be better
to focus on just one criterion or a small set of criteria for aspects of argument
strength. This also holds for combining logical, dialectical and rhetorical strength
into one overall notion for argument strength. Since the three aspects of argu-
ment strength serve diﬀerent purposes, it may not be good to combine them into
an overall notion. Another reason for this is that dialectical strength may presup-
pose contextual strength, since one aspect of dialectical strength is the extent to

150
H. Prakken
which an argument’s logical strength may be changed in the course of a dispute.
In any case, even if logical, dialectical and rhetorical strength are combined into
an overall notion of strength, they should ﬁrst be separately deﬁned, in order to
make their combination a principled one.
4
Evaluating Semantics and Principles
In this section I discuss how the above classiﬁcation into three aspects of argu-
ment strength, together with the interpretation of the nodes and links in abstract
or bipolar argumentation frameworks, is relevant for developing and evaluating
gradual accounts of argument strength. A complication here is that several recent
accounts rely on the distinction between the base and overall score of an argu-
ment, but it is not obvious whether notions of argument strength can always
be suitably deﬁned in this format. For example, such accounts presuppose that
the base and overall scores are of the same sort and can therefore be compared
but this does not have to be the case. Consider, for example, gradual deﬁnitions
of contextual strength in terms of extension-based [10] or labelling-based [48]
semantics. A very simple deﬁnition of overall strength would be that (given a
grounded labelling) being in is better than being undecided, which is better than
being out. It is not obvious what the base score of arguments would be in such
an approach. For these reasons I will below mainly discuss accounts with no
distinction between base and overall scores.
4.1
Be Explicit About Which Aspects of Argument Strength Are
Modelled
It is important to be explicit about which aspects of argument strength are
modelled (as in [11], who explicitly model two aspects of persuasiveness). The
aspects serve diﬀerent purposes, so principles or deﬁnitions that are good for one
aspect may not be good for another aspect. Consider, for example, two arguments
A and B where A defeasibly infers q from p while B ﬁrst defeasibly infers r
from p and then defeasibly infers q from r. Consider a deﬁnition of dialectical
strength capturing that having fewer attackable elements is dialectically better
and a deﬁnition of rhetorical strength that captures that a larger overlap of
an argument’s elements with the audience’s beliefs is rhetorically better. Even
without formalising these notions it is obvious that argument A is dialectically
stronger than argument B, since A has one attackable element less than B.
However, if the audience accepts that p defeasibly implies r and that r defeasibly
implies q but not that p defeasibly implies q, then B is rhetorically stronger than
A since it shares some elements with the background theory while A does not.
This illustrates that while sparsely justifying one’s claims or decisions may be
dialectically good, it may at the same time make an argument less persuasive.
Another example is the phenomenon of procatalepsis, modelled in [11], which
is the attempt to strengthen an argument by dealing with possible counter-
arguments before the audience can raise them. As modelled by Bonzon et al.,

Philosophical Reﬂections on Argument Strength and Gradual Acceptability
151
procatalepsis makes an argument rhetorically stronger when combined with an
attacker and an attacker of that attacker than when presented alone. Bonzon
et al. prove that procatalepsis is inconsistent with the principle of ‘void prece-
dence’ [1], according to which an argument that has no attackers is more accept-
able than an argument that has attackers, even if these attackers are counter-
attacked, and which is a key element of many current gradual argumentation
semantics [9]. It is implied by Basic Idea 7 of [6] that a strictly larger set of
attackers determine a lower strength. One interpretation of the void precedence
principle is as an aspect of dialectical strength, in particular, as capturing that
having fewer ways to attack an argument makes it dialectically less challenge-
able. On this interpretation, examples of procatalepsis are other cases where
an argument can be rhetorically stronger but dialectically weaker than another
argument.
Being explicit about which aspects of argument strength are modelled is not
only important when formulating theories of argument strength but also when
evaluating applications of computational argumentation. For instance, according
to [43] the Debater system was evaluated by twenty human annotators who had
to indicate to what extent they agreed with the statement ‘The ﬁrst speaker is
exemplifying a decent performance in this debate’. It is unclear which aspects
the annotators had in mind when answering this question or even whether all
annotators looked at the same aspects and applied the same criteria.
4.2
Be Explicit About the Interpretation of Arguments and Their
Relations
It is important to be explicit about whether the arguments in an abstract or
bipolar framework are regarded as statements or as inferential structures. One
reason is that trade-oﬀs between attacks and supports, or even to regard support-
ers as strengthening the supported argument, may make sense in the statement
interpretation but may not make sense in the inferential-structure interpreta-
tion. When the arguments are regarded as statements, then attack and sup-
port relations between arguments can hardly be interpreted as anything else
but expressing reasons for or against the statement. For example, in a decision-
making application [7,39] attack and support relations are reasons to adopt or
not adopt a given decision option. Then it makes sense to consider trade-oﬀs
between supporting and attacking arguments, as, for example, captured in the
(Strict) Franklin, Weakening and Strengthening principles proposed in [3]. The
same holds for Basic Idea 8 of [6], according to which (everything else being
equal) a strictly larger set of supporters determines a higher strength. In the
statement interpretation of arguments it makes sense to say that (in the absence
of attackers) a statement for which there are reasons to believe or accept it is
more acceptable than a statement for which there are no such reasons.
However, when arguments are regarded as inferential structures, then mul-
tiple interpretations of the support relation are possible and their diﬀerences
matter. In the context of the ASPIC + framework, Cohen et al. [14] deﬁne four
kinds of support (visualised in Fig. 1).
Subargument support corresponds to

152
H. Prakken
Fig. 1. Cohen et al.’s (2018) four kinds of support in ASPIC +.
the ASPIC + proper-subargument relation. Informally, every argument B corre-
sponding to subgraph of an argument A (viewed as a hypergraph) that is also
an argument (so takes all its premises from the premises of A) is a subargument
of A. A subargument of an argument A is not separate from A but is contained
in A as part of it, as visualised in the top row of Fig. 1. Clearly, when support
corresponds to (proper) subargument support, it makes no sense to consider
trade-oﬀs between attacks and support or to regard supporters as strengthening
the supported argument. Logically, the number of supports of an argument is
then just a measure of its inferential complexity while dialectically, having more
supporters may make an argument more vulnerable to attack and thus weaker.
For conclusion support (argument A supports B whenever they have the
same conclusion) these ideas may make more sense but for premise support
they are again questionable. An argument A premise-supports an argument B
iﬀthe ﬁnal conclusion of A is equal to a premise of B (intermediate support is a
variant of premise support in which not a premise but an intermediate conclusion
is supported; I will therefore not discuss it separately). Premise support may be

Philosophical Reﬂections on Argument Strength and Gradual Acceptability
153
useful in debate contexts, where there usually is no global knowledge base from
which the debaters construct their arguments. Then if one debate participant
presents an argument for a premise of another participant’s argument, it may
be less natural to say that the supporting argument is part of the supported
argument as captured in the subargument relation. Instead, the arguments may
be said to remain separate, as depicted in the third row of Fig. 1.
Consider an example from [38], in which John argues “Nixon was a paciﬁst
since he was a Quaker and Quakers are usually paciﬁsts” (argument A). Now sup-
pose Mary supports John’s argument by saying “Nixon regularly attended service
in a Quaker church, people who are regularly seen in a Quaker church usually are
a Quaker, so Nixon was a Quaker” (argument B). Consider contextual strength.
Can we say that Mary’s supporting argument makes John’s argument contextually
stronger? If so, then a successful attack on Mary’s argument should intuitively also
weaken John’s argument. Suppose that Bob attacks Mary’s argument by arguing
that Nixon only attended service in a Quaker church to please his wife, who was a
Quaker (argument C). It can be argued that this does not knock down John’s argu-
ment, since why should John be blamed for Mary’s ﬂawed attempt to support his
argument? However, it is still unsatisfactory that there is no logical relation at all
between attacking a supporter and the status of a supported argument. If support
means anything at all, then surely attacking a supporter should have some eﬀect
on the status of an argument supported by it (note that for subargument support
this is automatic). The solution adopted in [38] is that whether C’s attack on B
also weakens A is conditional on what the audience accepts as given. If the audi-
ence accepts that Nixon was a quaker without further support, then argument C
has no eﬀect on the acceptability of A, while if the audience wants further support
for this premise, then argument C reduces A’s acceptability. This approach implies
that whether trade-oﬀs between premise-supporters and attackers should be con-
sidered, or whether a premise-supporter can strengthen the supported argument
at all, cannot be determined in general but depends on the context, in particular
on the audience’s beliefs.
While this is one approach, there may also be reasons to always consider
trade-oﬀs between premise-supporters and attackers and to regard a premise-
supporter as, everything else being equal, strengthening the supported argument.
However, even then the nature of the arguments and their relations matters.
Consider Fig. 2, with two bipolar frameworks in the top row and two instanti-
ations of these frameworks in the bottom row (in BAF1 and BAF2 the dashed
arguments depict support relations between arguments). According to the prin-
ciple that, everything else being equal, having more supporters is better (e.g.
the Cardinality Preference axiom of [2]), argument C on the top right is better
supported than argument A on the top left since C has two premise-supporters
while A has just one. However, as shown in the bottom row, all of A’s premises
(namely, q) are supported while only one of C’s two premises is supported, so
dialectically and perhaps also rhetorically A might just as well be regarded as
better supported than C. Or imagine that D does not premise-support C on u
but on v: then both A and C have all their premises supported, so there seems

154
H. Prakken
Fig. 2. Is having more supporters better?
no reason to prefer C over A. Consider next Basic Idea 8 of [6], which says that
a strictly larger set of supporters (w.r.t. to set inclusion) determines a higher
strength, and suppose that argument A has no premise supporters while argu-
ment C only has supporters for premise u. Since any set includes the empty set,
Basic Idea 8 implies that C is stronger than A. It is not obvious why this should
be, given that they both have the same number of unsupported premises. This
becomes even less obvious if C is changed to have more than two premises, of
which only one is supported. Concluding, even in applications in which it makes
sense to regard premise-supporters as, everything else being equal, strengthening
the supported argument, it is important to take the structure of arguments and
the nature of their relations into account.
So far I have illustrated the importance of being explicit about whether argu-
ments are statements or inferential structures, and in the latter case of being
explicit about the structure of arguments and the nature of the support relation.
I next illustrate the importance of being explicit about whether an argument is
attackable or not (a distinction made in some of the main structured approaches
to argumentation, such as assumption-based argumentation, defeasible logic pro-
gramming and ASPIC +). Consider the Cardinality Precedence principle that
having fewer attackers makes an argument stronger [1,9] and consider AF1 with
A being attacked by B and AF2 with C being attacked by D and E (Fig. 3; in
the AFs in Figures 3 and 4 the solid arrows depict attack relations). According
to Cardinality Precedence argument A is stronger than argument C. However, if
B is not attackable while D and E are attackable, then it is not obvious why this
should be the case. For example, from the point of view of dialectical strength C
is arguably dialectically stronger than A since C can still be made in by adding
new arguments and attacks while for A this cannot happen.
Another example of why the distinction between attackable and non-
attackable arguments matters concerns the principle that having more defenders
makes an argument stronger. Consider the AFs displayed in Fig. 4.
Accord-
ing to the gradual semantics of [21,22], A2 is justiﬁed to a higher degree than

Philosophical Reﬂections on Argument Strength and Gradual Acceptability
155
Fig. 3. Is having fewer attackers better?
Fig. 4. Is having more defenders better?
A1, since A2 has two defenders (C2 and D2) while A1 has only one defender
(C1). However, if C1 is unattackable while C2 and D2 are attackable then it
is not obvious why this has to be so, whatever aspect of argument strength is
modelled.
5
Conclusion
In this paper I proposed a classiﬁcation of three aspects of argument strength,
namely, logical, dialectical and rhetorical strength. I then showed with several
examples that when developing or evaluating gradual accounts of argument
strength it is essential to be explicit about which aspect of argument strength
is modelled, since some principles or semantics maybe suitable for one aspect
but not for another. Likewise, I showed that it is important to be explicit about
the adopted interpretation of the arguments and their relations in abstract or
bipolar argumentation frameworks. For example, it matters whether the argu-
ments are interpreted as statements or as inferential structures, how support is
deﬁned, and how the structure of arguments is deﬁned.
The underlying aim of this paper was to encourage a principled development
and evaluation of (principles for) gradual argumentation semantics. As such,
this was just an initial attempt. More comprehensive formal investigations should
yield more systematic insights into the purposes for which (principles for) gradual
semantics are suitable and into the assumptions on which they depend. This
paper has aimed to lay the conceptual foundations for such investigations.
References
1. Amgoud, L., Ben-Naim, J.: Ranking-based semantics for argumentation frame-
works. In: Liu, W., Subrahmanian, V.S., Wijsen, J. (eds.) SUM 2013. LNCS
(LNAI), vol. 8078, pp. 134–147. Springer, Heidelberg (2013). https://doi.org/10.
1007/978-3-642-40381-1 11

156
H. Prakken
2. Amgoud, L., Ben-Naim, J.: Evaluation of arguments from support relations: axioms
and semantics. In: Proceedings of the 25th International Joint Conference on Arti-
ﬁcial Intelligence (IJCAI-16), pp. 900–906 (2016)
3. Amgoud, L., Ben-Naim, J.: Weighted bipolar argument graphs: axioms and seman-
tics. In: Proceedings of the 27th International Joint Conference on Artiﬁcial Intel-
ligence (IJCAI-18), pp. 5194–5198 (2018)
4. Atkinson, K., Bench-Capon, T., McBurney, P.: A dialogue game protocol for multi-
agent argument over proposals for action. J. Auton. Agents Multi-Agent Syst. 11,
153–171 (2005)
5. Baroni, P., Caminada, M., Giacomin, M.: An introduction to argumentation
semantics. Knowl. Eng. Rev. 26, 365–410 (2011)
6. Baroni, P., Rago, A., Toni, F.: How many properties do we need for gradual argu-
mentation? In: Proceedings of the 32nd AAAI Conference on Artiﬁcial Intelligence
(AAAI 2018), pp. 1736–1743 (2018)
7. Baroni, P., Romano, M., Toni, F., Aurisicchio, M., Bertanza, G.: Automatic evalu-
ation of design alternatives with quantitative argumentation. Argum. Comput. 6,
24–49 (2015)
8. Baumann, R.: What does it take to enforce an argument? Minimal change in
abstract argumentation. In: Proceedings of the 20th European Conference on Arti-
ﬁcial Intelligence, pp. 127–132 (2012)
9. Bonzon, E., Delobelle, J., Konieczny, S., Maudet, N.: A comparative study of
ranking-based semantics for abstract argumentation. In: Proceedings of the 30st
AAAI Conference on Artiﬁcial Intelligence (AAAI 2016), pp. 914–920 (2016)
10. Bonzon, E., Delobelle, J., Konieczny, S., Maudet, N.: Combining extension-based
semantics and ranking-based semantics for abstract argumentation. In: Principles
of Knowledge Representation and Reasoning: Proceedings of the Sixteenth Inter-
national Conference, pp. 118–127. AAAI Press (2018)
11. Bonzon, E., Delobelle, J., Konieczny, S., Maudet, N.: A parametrized ranking-
based semantics compatible with persuasion principles. Argum. Comput. 12, 49–85
(2021)
12. Cayrol, C., Lagasquie-Schiex, M.C.: Graduality in argumentation. J. Artif. Intell.
Res. 23, 245–297 (2005)
13. Cayrol, C., Lagasquie-Schiex, M.C.: Bipolar abstract argumentation systems. In:
Rahwan, I., Simari, G. (eds.) Argumentation in Artiﬁcial Intelligence, pp. 65–84.
Springer, Berlin (2009). https://doi.org/10.1007/978-0-387-98197-0 4
14. Cohen, A., Parsons, S., Sklar, E., McBurney, P.: A characterization of types of sup-
port between structured arguments and their relationship with support in abstract
argumentation. Int. J. Approx. Reason. 94, 76–104 (2018)
15. Doutre, S., Mailly, J.G.: Constraints and changes: a survey of abstract argumen-
tation dynamics. Argum. Comput. 9, 223–248 (2018)
16. Dung, P.: On the acceptability of arguments and its fundamental role in nonmono-
tonic reasoning, logic programming, and n-person games. Artif. Intell. 77, 321–357
(1995)
17. van Eemeren, F.H., Garssen, B., Krabbe, E.C.W., Snoeck Henkemans, A.F., Ver-
heij, B., Wagemans, J.H.M.: Handbook of Argumentation Theory. Springer, Dor-
drecht (2014). https://doi.org/10.1007/978-90-481-9473-5
18. Freeman, J.: Dialectics and the Macrostructure of Arguments. A Theory of Argu-
ment Structure. Fors/de Gruyter, Berlin-New York (1991)
19. Garcia, A., Simari, G.: Defeasible logic programming: an argumentative approach.
Theory Pract. Logic Program. 4, 95–138 (2004)

Philosophical Reﬂections on Argument Strength and Gradual Acceptability
157
20. Gordon, T.: The pleadings game: an exercise in computational dialectics. Artif.
Intell. Law 2, 239–292 (1994)
21. Grossi, D., Modgil, S.: On the graded acceptability of arguments. In: Proceedings
of the 24th International Joint Conference on Artiﬁcial Intelligence, pp. 868–874
(2015)
22. Grossi, D., Modgil, S.: On the graded acceptability of arguments in abstract and
instantiated argumentation. Artif. Intell. 275, 138–173 (2019)
23. Hadoux, E., Hunter, A.: Comfort or safety? Gathering and using the concerns of
a participant for better persuasion. Argum. Comput. 10, 113–147 (2019)
24. Horty, J.: Reasons as Defaults. Oxford University Press, Oxford (2012)
25. Hunter, A.: Making arguments more believable. In: Proceedings of the 19th
National Conference on Artiﬁcial Intelligence, pp. 6269–274 (2004)
26. Hunter, A.: Towards a framework for computational persuasion with applications
in behaviour change. Argum. Comput. 9, 15–40 (2018)
27. Hunter, A. (ed.): Argument and Computation, vol. 5 (2014). Special issue with
Tutorials on Structured Argumentation
28. Kunz, W., Rittel, H.: Issues as elements of information systems. Working Paper
No. 131, Institute of Urban and Regional Development, University of California,
Berkeley, California (1970)
29. Leite, J., Martins: Social abstract argumentation. In: Proceedings of the 22nd
International Joint Conference on Artiﬁcial Intelligence (IJCAI-11), pp. 2287–2292
(2011)
30. Loui, R.: Process and policy: resource-bounded non-demonstrative reasoning. Com-
put. Intell. 14, 1–38 (1998)
31. Mackenzie, J.: Question-begging in non-cumulative systems. J. Philos. Logic 8,
117–133 (1979)
32. Modgil, S., Caminada, M.: Proof theories and algorithms for abstract argumen-
tation frameworks. In: Rahwan, I., Simari, G. (eds.) Argumentation in Artiﬁcial
Intelligence, pp. 105–129. Springer, Berlin (2009). https://doi.org/10.1007/978-0-
387-98197-0 6
33. Perelman, C., Olbrechts-Tyteca, L.: The New Rhetoric. A Treatise on Argumen-
tation. University of Notre Dame Press, Notre Dame (1969)
34. Pollock, J.: Defeasible reasoning. Cogn. Sci. 11, 481–518 (1987)
35. Pollock, J.: Defeasible reasoning with variable degrees of justiﬁcation. Artif. Intell.
133, 233–282 (2002)
36. Prakken, H.: Coherence and ﬂexibility in dialogue games for argumentation. J.
Logic Comput. 15, 1009–1040 (2005)
37. Prakken, H.: An abstract framework for argumentation with structured arguments.
Argum. Comput. 1, 93–124 (2010)
38. Prakken, H.: Modelling support relations between arguments in debates. In:
Ches˜nevar, C., Falappa, M.A., et al. (eds.) Argumentation-based Proofs of Endear-
ment. Essays in Honor of Guillermo R. Simari on the Occasion of his 70th Birthday,
pp. 349–365. College Publications, London (2018)
39. Rago, A., Toni, F., Aurisicchio, M., Baroni, P.: Discontinuity-free decision support
with quantitative argumentation debates. In: Principles of Knowledge Represen-
tation and Reasoning: Proceedings of the Fifteenth International Conference, pp.
63–72. AAAI Press (2016)
40. Reed, C., Walton, D., Macagno, F.: Argument diagramming in logic, law and
artiﬁcial intelligence. Knowl. Eng. Rev. 22, 87–109 (2007)

158
H. Prakken
41. Riveret, R., Prakken, H., Rotolo, A., Sartor, G.: Heuristics in argumentation: a
game-theoretical investigation. In: Besnard, P., Doutre, S., Hunter, A. (eds.) Com-
putational Models of Argument. Proceedings of COMMA 2008, pp. 324–335. IOS
Press, Amsterdam etc (2008)
42. Rosenfeld, A., Kraus, S.: Providing arguments in discussions based on the predic-
tion of human argumentative behavior. In: Proceedings of the 29th AAAI Confer-
ence on Artiﬁcial Intelligence (AAAI 2015), pp. 1320–1327 (2015)
43. Slonim, N., Bilu, Y., Alzate, C.: An autonomous debating system. Nature 591,
397–384 (2021)
44. Toni, F.: A tutorial on assumption-based argumentation. Argum. Comput. 5, 89–
117 (2014)
45. Walton, D.: Argumentation Schemes for Presumptive Reasoning. Lawrence Erl-
baum Associates, Mahwah (1996)
46. Walton, D., Krabbe, E.: Commitment in Dialogue. Basic Concepts of Interpersonal
Reasoning. State University of New York Press, Albany (1995)
47. Walton, D., Reed, C., Macagno, F.: Argumentation Schemes. Cambridge University
Press, Cambridge (2008)
48. Wu, Y., Caminada, M.: A labelling-based justiﬁcation status of arguments. Stud.
Logic 3, 12–29 (2010)
49. Zenker, F., Debowska-Kozlowska, K., Godden, D., Selinger, M., Wells, S.: Five
approaches to argument strength: probabilistic, dialectical, structural, empirical,
and computational. In: Proceedings of the 3rd European Conference on Argumen-
tation, pp. 653–674. College Publications, London (2020)

An Abstract Argumentation and Logic
Programming Comparison Based on 5-Valued
Labellings
Samy S´a(B)
and Jo˜ao Alcˆantara
Universidade Federal do Cear´a, Fortaleza, Brazil
samy@ufc.br, jnando@lia.ufc.br
Abstract. Abstract argumentation and logic programming are two formalisms
of non-monotonic reasoning that share many similarities. Previous studies con-
templating connections between the two formalisms provided back and forth
translations from one to the other and found they correspond in multiple dif-
ferent semantics, but not all. In this work, we propose a new set of ﬁve argument
labels to revisit the semantic correspondences between abstract argumentation
and logic programming. By doing so, we shed light on why the two formalisms
are not absolutely equivalent. Our investigation lead to the speciﬁcation of the
novel least-stable semantics for abstract argumentation which corresponds to the
L-stable semantics of logic programming.
Keywords: Abstract argumentation · Logic programming · Argument
labellings
1
Introduction
Logic Programming (LP) and Abstract Argumentation Frameworks are two different
formalisms widely used for the representation of knowledge and reasoning. Abstract
Argumentation (AA) was itself inspired by logic programming in its origins, which
naturally led to several studies concerning connections between them [5,6,8,11,14,16].
One of the main approaches to observe those connections is based on the comparison
of the different semantics proposed for each formalism. To that end, the ﬁrst questions
were raised and answered in [6], the work that originally introduced abstract argumen-
tation: a translation from a logic program into an abstract argumentation framework
was proposed and used to prove that the stable models (resp. the well-founded model)
of a logic program correspond to the stable extensions (resp. the grounded extension) of
its corresponding abstract argumentation framework. Other advances were made when
[16] observed the equivalence between the complete semantics for abstract argumenta-
tion and the p-stable semantics for logic programs. Those particular semantics gener-
alise many others in their respective formalisms, wielding a plethora of results gathered
in [5] and recently expanded in [4]. One particular equivalence formerly expected to
hold, however, could not be achieved, namely the correspondence between the semi-
stable semantics from abstract argumentation [3] and the L-stable semantics from logic
c
⃝Springer Nature Switzerland AG 2021
J. Vejnarov´a and N. Wilson (Eds.): ECSQARU 2021, LNAI 12897, pp. 159–172, 2021.
https://doi.org/10.1007/978-3-030-86772-0_12

160
S. S´a and J. Alcˆantara
programming [9]. The work we are about to present is largely motivated by that non-
correspondence result.
In our efforts to understand why the semi-stable and L-stable semantics do not cor-
respond to one another, we detected the fault is always related to some arguments whose
attackees coincide; in their redundancy, one or more of those arguments would be irrel-
evant to the evaluation of those arguments they mutually attack. We were also able to
identify that sink1 arguments play a special role in pinpointing the culprits. To achieve
our goals for this work, we follow the presentation of [5] while revising some key deﬁ-
nitions: (i) the instantiation of abstract argumentation frameworks from logic programs
is revised to include special arguments we call default arguments (which are always
sinks) and (ii) we revise the deﬁnition of complete argument labellings [2] to use a dif-
ferent set of labels. Our revision of complete labellings follows [1,10] to contemplate
partial labellings and different categories of undecidedness. The resulting labellings,
here on called 5-valued labellings, can isolate any arguments causing semi-stable and
L-stable semantics to differ and are shown to preserve the main results in [5].
Based on those results, we introduce a novel semantics for abstract argumentation
called the least-stable (or L-stable) semantics, which we show is equivalent to the L-
stable LP semantics. The new AA semantics closes the previous gap preventing the
claim that AA is actually equivalent to LP. Our results add further to the literature con-
cerning semantics of non-monotonic reasoning formalisms, potentially allowing us to
import proof procedures and implementations from formal argumentation to logic pro-
gramming and vice-versa. Among other implications of our work, we set precedence
to the proposal of new argumentation semantics based on partial labellings. We also
establish that sink arguments have a role in the semantic evaluation of abstract argu-
mentation frameworks, which may also help us identify new interesting AA semantics.
Moreover, we show that if we restrict our attention to sink arguments, the standard
3-valued labellings are enough to capture the logic programming L-stable semantics.
2
Preliminaries
2.1
Abstract Argumentation
Abstract argumentation was introduced in [7] and most of the deﬁnitions in this section
are based on that work. Concerning new deﬁnitions, we will compute the set of sink
arguments of an abstract argumentation framework and use them to propose argumen-
tation semantics in the form of 5-valued labellings, a core trait of our approach.
Deﬁnition 1. An abstract argumentation framework is a pair (Ar, att) where Ar is a
ﬁnite set of arguments and att ⊆Ar × Ar.
We may refer to AA frameworks simply as argumentation frameworks or AF’s.
Deﬁnition 2. Given an argumentation framework AF = (Ar, att), the set of its sink
arguments is given by SINKSAF = {A ∈Ar | ∀B, (A, B) /∈att}.
1 In graph theory, a sink node is one from which no edges emerge.

AA vs LP in 5-Valued Labellings
161
The traditional approach of [7] to semantics involves the identiﬁcation of sets of
arguments (called extensions) based on the concept of admissibility (see [7]). An alter-
native formalization of AA semantics was introduced in [2] where argument labellings
are used instead of extensions. The argument labellings of [2] are functions attribut-
ing to each argument a single label from {in, out, undec}. In this work, we specialise
the labels out, undec into two different labels each based on the sink arguments they
attack. Our labelling concept follows the lead of [1], where the authors explore alterna-
tive sets of labels and special cases of undecidedness in argumentation semantics. We
borrow some of the labels discussed in their work, namely the “I don’t care” (idc) label
(originally proposed in [10]) and the “I don’t know” (idk) label ([1]) and we introduce
a new label of our own, here dubbed “It doesn’t matter if it’s out” (ido). Hence, in
our work, given an argumentation framework AF = (Ar, att), an argument labelling
Al will be a function Al : Ar →{in, out, ido, idk, idc}. We will start deﬁning
complete labellings, since the other relevant semantics are special cases of them.
Deﬁnition 3. An argument labelling is called a complete argument labelling iff for each
A ∈Ar it holds that
– Al(A) = in iff all B that attacks A has Al(B) ∈{out, ido}.
– Al(A) = out iff at least one B that attacks A has Al(B) = in and A attacks no
sink C with Al(C) = idk.
– Al(A) = ido iff at least one B that attacks A has Al(B) = in and A attacks at
least one sink C with Al(C) = idk
– Al(A) = idk iff at least one B that attacks A has Al(B) ∈{out, ido}, no B that
attacks A has Al(B) = in, and every sink C attacked by A has Al(C) = idk.
– Al(A) = idc iff at least one B that attacks A has Al(B) ∈{out, ido}, no B that
attacksA hasAl(B) = in, and at least one sink C attacked by A has Al(C) ̸= idk.
As one can observe, the sink arguments inﬂuence the evaluation of arguments in
our deﬁnition. For instance, the only difference between labelling an argument as out
or ido is whether that argument attacks any idk-labelled sinks. As such, out and ido
are merely specialisations of the out label from the standard 3-valued labellings of [2].
The same holds for idk and idc, as specialisations of undec from [2]. Intuitively, idk
replaces undec in most cases; idc (resp. ido) is used when the status of a classically
undec (resp. out) argument is irrelevant to the status of its attackee. In a way, the labels
ido, idc allow us to sometimes evaluate one extra argument (using ido) or one less
argument (using idc) as undecided. This is enough to ensure that our 5-valued complete
labellings from Deﬁnition 3 are one to one related to the 3-valued complete labellings
of [2] and preserve the complete semantics for abstract argumentation frameworks. The
same holds for the grounded, preferred, stable and semi-stable semantics, as they are all
particular cases of the complete semantics. Another straightforward result is
Proposition 1. Let (Ar, att) be an AF and Al be one of its complete argument
labellings. For every sink argument C ∈SINKSAF, it holds Al(C) ∈{in, out, idk}.
Given an argument labelling Al, we write v(Al) = {A | Al(A) = v} for v ∈
{in, out, ido, idk, idc}. An argument labelling provides a partition of Ar, so we
might as well present Al as a tuple (in(Al), out(Al), ido(Al), idk(Al), idc(Al)).

162
S. S´a and J. Alcˆantara
The next deﬁnition adapts from well-known results concerning the 3-valued argu-
ment labellings of [2,5]. Our adaptation maps the 3-valued label undec to idk ∪idc,
the specialised labels we created from the standard undec.
Deﬁnition 4. Let AL be the set of all complete argument labellings of AF. Then
– Al ∈AL is grounded if ∄Al′ ∈AL such that in(Al′) ⊊in(Al).
– Al ∈AL is preferred if ∄Al′ ∈AL such that in(Al′) ⊋in(Al).
– Al ∈AL is stable if idk(Al) ∪idc(Al) = ∅.
– Al ∈AL is semi-stable if ∄Al′ ∈AL such that idk(Al′) ∪idc(Al′) ⊊idk(Al) ∪
idc(Al).
The following example should help the reader to further understand these concepts.
Example 1. The argumentation framework AF below has complete labellings Al1 =
({ } , { } , { } , Ar, { }), Al2 = ({A2, A8} , {A3, A7} , {A4}, {A1, A5, A6, A9}, { }),
Al3 = ({A3, A7, A9}, {A2, A5, A8}, { }, {A1, A4, A6}, { }). The grounded labelling
of AF is Al1 and its preferred labellings are Al2, Al3. AF has no stable labellings, but
has two semi-stable labellings, namely Al2, Al3. For contrast, the corresponding argu-
ment labellings in Caminada’s 3-valued approach are Al′
1 = ({ } , { } , Ar), Al′
2 =
({A2, A8} , {A3, A7, A4} , {A1, A5, A6, A9}), Al′
3 = ({A3, A7, A9} , {A2, A5, A8} ,
{A1, A4, A6}). We highlight the difference between Al2 and Al′
2 due to ido(Al2) ̸= ∅.
A2
A3
A4
A1
A5
A7
A6
A8
A9
It should be noticed that sink arguments were only attributed labels in, out, idk, as
previously stated in Proposition 1.
2.2
Logic Programs and Semantics
In the current paper, we account for propositional normal logic programs2, which we
call logic programs or simply programs from now on. We will follow the presentation
of logic programs and their semantics as done in [5], which, in turn, is based on [13].
Deﬁnition 5. A rule r is an expression
r : c ←a1, . . . , an, not b1, . . . , not bm
(m, n ≥0) where c, each ai (1 ≤i ≤n) and each bj (1 ≤j ≤m) are atoms and
not represents negation as failure. A literal is either an atom a (positive literal) or a
2 These are logic programs whose rules may contain weak but not strong negation and where
the head of each rule is a single atom.

AA vs LP in 5-Valued Labellings
163
negated atom not a (negative literal). Given a rule r as above, c is called the head of r,
which we denote head(r), and a1, . . . , an, not b1, . . . , not bm is called the body of r,
denoted body(r). Further, we divide body(r) in two sets body+(r) = {a1, . . . , an} and
body−(r) = {not b1, . . . , not bm}. A logic program (or simply a program or LP) P is
then deﬁned as a ﬁnite set of rules. If every r ∈P has body−(r) = ∅, then P is positive.
The Herbrand Base of P is the set HBP of all atoms appearing in the program.
Deﬁnition 6. A 3-valued Herbrand Interpretation I of a logic program P is a pair
⟨T, F⟩with T, F ⊆HBP and T ∩F = ∅. The atoms in T are said to be true, the atoms
in F are said to be false and the atoms in HBP \ (T ∪F) are said to be undeﬁned.
Deﬁnition 7. A 3-valued Herbrand interpretation I of a logic program P is called a
model if I(head(r)) ≥min({I(l) | l ∈body(r)}) for each r ∈P, according to the
order true > undeﬁned > false.
Let I be a 3-valued Herbrand Interpretation of P, the reduct of P with respect to I
(written P/I) can be obtained replacing each occurrence of a naf-literal not c in P
“true” if c ∈F, by “false” if c ∈T, or by “undefined” otherwise. In this context,
“true”, “false” and “undefined” are special atoms not occurring in P which are
necessarily evaluated according to their naming conventions. This procedure ensures
P/I has no instances of negative literals. As consequence, P/I has a unique least 3-
valued model [13] hereby denoted ΨP (I) = ⟨TΨ, FΨ⟩3 with minimal TΨ and maximal
FΨ (w.r.t. set inclusion) such that, for every a ∈HBP :
– a ∈TΨ if there is a rule r′ ∈P/I with head(r′) = a and body+(r′) ⊆TΨ;
– a ∈FΨ if every rule r′ ∈P/I with head(r′) = a has body+(r′) ∩FΨ ̸= ∅;
Deﬁnition 8. Let I = ⟨T, F⟩be a 3-valued Herbrand Interpretation of program P.
– I is a partial stable (or P-stable) model of P iff ΨP (I) = I.
– T is a well-founded model of P iff I is a P-stable model of P where T is minimal
(w.r.t. set inclusion) among all P-stable models of P.
– T is a regular model of P iff I is a P-stable model of P where T is maximal (w.r.t.
set inclusion) among all P-stable models of P.
– T is a stable model of P iff I is a P-stable model of P where T ∪F = HBP .
– T is an L-stable model of P iff I is a P-stable model of P where T ∪F is maximal
(w.r.t. set inclusion) among all P-stable models of P.
While some of the above deﬁnitions are not standard in logic programming litera-
ture, their equivalence is proved in [5]. This approach is favored in our work due to the
structural similarities to Deﬁnition 4, simplifying the proof of some results.
The following example should help the reader to understand the above concepts.
Example 2. Consider the following logic program P:
r1 : c ←not c r4 : c ←not c, not a
r2 : a ←not b r5 : g ←not g, not b
r3 : b ←not a
3 The above deﬁnition consists of a least ﬁx-point of the immediate consequence operator Ψ
deﬁned in [13], which is guaranteed to exist and be unique for positive programs.

164
S. S´a and J. Alcˆantara
This program has three P-stable models: I1 = ⟨{ }, { }⟩, I2 = ⟨{a}, {b}⟩, I3 =
⟨{b}, {a, g}⟩. P has { } for its well-founded model (always unique), {a}, {b} for its
regular models, no stable models and only {b} as an L-stable model.
3
From Logic Programs to Argumentation Frameworks
In this section, we review the procedure used to instantiate argumentation frameworks
based on logic programs from [5] with slight updates to each step. Our updates are
required to include special arguments we call default arguments, which are necessarily
sinks, in the result. By design, they will be the only sinks in the output of our procedure.
3.1
Step 1: AF Instantiation
Given a normal logic program P, we recursively instantiate the following arguments:
Deﬁnition 9. Let P be a logic program.
– If r : c ←not b1, . . . , not bm is a rule in P then r is also an argument (say A) with
Conc(A) = c, Rules(A) = {r}, Vul(A) = {b1, . . . , bm} and Sub(A) = {A}.
– If r : c ←a1, . . . , an, not b1, . . . , not bm is a rule in P and for each ai ∈body+(r)
there is an argument Ai with Conc(Ai) = ai and r /∈Rules(Ai) then
c ←(A1), . . . , (An), not b1, . . . , not bm
is an argument (say A) with Conc(A) = c, Rules(A) =  Rules(Ai) ∪{r},
Vul(A) =  Vul(Ai) ∪{b1, . . . , bm} and Sub(A) =  Sub(Ai) ∪{A}.
– For each different head(r) where r ∈P, there is a default argument (say not A)
such that Conc(not A) = not head(r), Rules(not A) = ∅, Vul(not A) =
{head(r)} and Sub(not A) = {not A}.
Above, Conc(A), Rules(A), Vul(A) and Sub(A) respectively refer to what we
call the conclusion, rules, vulnerabilities and subarguments of A. Our deﬁnition is very
similar to the one in [5], which consists of only the ﬁrst two bullet points above. The
difference is the third bullet point, which is required to instantiate the default arguments.
Example 3. Based on the logic program P from Example 2 we can instantiate: Ai = ri,
1 ≤i ≤5, based on the ﬁrst bullet point of Deﬁnition 9; A6 = not c, A7 = not a,
A8 = not b, A9 = not g, based on the third bullet point of Deﬁnition 9. No arguments
are instantiated following the second bullet point because all r ∈P have body+(r) = ∅.
To exemplify the components of some of those arguments, A1 has Conc(A1) = c,
Rules(A1) = {r1}, Vul(A1) = {c} and Sub(A1) = {A1}. A6 has Conc(A6) =
not c, Rules(A6) = ∅, Vul(A6) = {c} and Sub(A6) = {A6}.
The next step is to determine the attack relation of the argumentation framework:
an argument attacks another iff its conclusion is a vulnerability of the latter.
Deﬁnition 10. Let A and B be arguments4. We say A attacks B iff Conc(A) ∈Vul(B).
4 According to Deﬁnition 9.

AA vs LP in 5-Valued Labellings
165
Example 4. The argument A1 from Example 3 consists of r : c ←not c and has
Conc(A1) = c and Vul(A1) = {c}; since Conc(A1) ∈Vul(A1), A1 attacks itself. A6,
in comparison, attacks no arguments, but is also attacked by A1.
Deﬁnition 11. The argumentation framework AFP associated with a program P is
AFP = (Ar P , attP ), where Ar P is the set of all arguments from P (Deﬁnition 9)
and attP = {(A, B) ∈Ar × Ar | Conc(A) ∈Vul(B)} (Deﬁnition 10).
Given AFP , the computation of SINKSAFP retrieves its set of default arguments.
Example 5. Following the names of arguments used in Example 3, the AFP correspond-
ing to P from Example 2 is precisely the framework depicted in Example 1.
3.2
Step 2: Applying Argumentation Semantics
Once we have AFP , we will be interested in its semantics from the standpoint of abstract
argumentation, which are given by Deﬁnition 3.
3.3
Step 3: Computing Conclusion Labellings
The goal of this step is to translate the 5-valued argument labellings obtained in the
previous step to sets of conclusions comparable to the models of the original program,
which are in turn 3-valued. For this reason, conclusions will be labeled exclusively
within the elements of {in, out, undec}. In what follows, we will refer to the necessary
sets of labels as V5 = {in, out, ido, idk, idc} and V3 = {in, out, undec}.
Deﬁnition 12. Let P be a program. A conclusion labelling is a map Cl : HBP →V3.
As in [5], we follow the approach of [15]: for each conclusion, we map the labels
of arguments that yield it (taken from V5) to a label in V3. The label attributed to each
a ∈HBP is then the best one5 according to the total order in > undec > out.
Deﬁnition 13. The mapping σ:V5→V3 is such that σ(in) = in, σ(out) = σ(ido) =
out, σ(idk) = σ(idc) = undec.
Deﬁnition 14. Let AFP = (Ar P , attP ) be the AF associated with a program P and Al
be an argument labelling of AFP . We say that Cl is the associated conclusion labelling
of Al iff Cl is a conclusion labelling such that for each c ∈HBP it holds that Cl(c) =
max({σ(Al(A)) | Conc(A) = c} ∪{out}) where in > undec > out.
Given a conclusion labelling Cl, we write in(Cl) to denote {c | Cl(c) = in},
out(Cl) for {c | Cl(c) = out} and undec(Cl) for {c | Cl(c) = undec}. Because
a conclusion labelling deﬁnes a partition of HBP into sets of in/out/undec-labelled
conclusions, we may sometimes write Cl as the triple (in(Cl), out(Cl), undec(Cl)).
5 In case there is no argument for a particular conclusion, it will be simply labelled out.

166
S. S´a and J. Alcˆantara
Example 6. Recall the complete argument labellings of AF in Example 1 are: Al1 =
({ } , { } , { } , Ar, { }), Al2 = ({A2, A8} , {A3, A7} , {A4} , {A1, A5, A6, A9}, { }),
Al3
= ({A3, A7, A9}, {A2, A5, A8}, { }, {A1, A4, A6}, { }). Furthermore, remem-
ber Conc(A1) = Conc(A4) = c (Example 3). Observe that Al2(A4) = ido
and Al2(A1) = idk. Given that σ(ido) = out and σ(idk) = undec, we will
ﬁnd Cl2(c) = undec in the corresponding conclusion labelling Cl2. Following
Deﬁnition 14, we produce the conclusion labellings Cl1 = ({} , {} , {a, b, c, g}),
Cl2 = ({a} , {b} , {c, g}), and Cl3 = ({b} , {a, g} , {c}) respectively associated to
Al1, Al2, Al3. Please observe that ⟨in(Cli), out(Cli)⟩(1 ≤i ≤3) are precisely the
p-stable models of P (Example 2).
4
Semantic Correspondences
In this section, we overview results concerning semantic relations between abstract
argumentation and logic programming. We show that our approach based on 5-valued
labellings preserves the original results and provides us a new correspondence.
First, we must formally deﬁne functions to map argument labellings to conclusion
labellings and vice-versa. Our intention is to later prove they are each other’s inverse.
The function AL2CL below simply follows Deﬁnition 14, while CL2AL is introduced
anew. In the latter, we will ﬁnd that an argument is out or ido in the resulting argu-
ment labelling if some of its vulnerabilities are in in the original conclusion labelling.
Deciding if that argument is out or ido will depend solely on whether it attacks an
undec-labelled sink. The case for idk and idc arguments is similarly based on undec
and the status of the sink arguments they attack.
Deﬁnition 15. Let P be a program and AFP be its associated argumentation frame-
work. Let AL be the set of all argument labellings of AFP and let CL be the set of all
conclusion labellings of AFP .
– We deﬁne a function AL2CL: AL →CL such that for each Al ∈AL, AL2CL(Al) is
the associated conclusion labelling of Al (Deﬁnition 14).
– We deﬁne a function CL2AL : CL →AL such that for each Cl ∈CL and each
A ∈Ar P it holds that:
• CL2AL(Cl)(A) = in iff Vul(A) ⊆out(Cl)
• CL2AL(Cl)(A) = out iff Vul(A) ∩in(Cl) ̸= ∅and
{Conc(B) | B ∈SINKSAFP and (A, B) ∈att} ∩undec(Cl) = ∅
• CL2AL(Cl)(A) = ido iff Vul(A) ∩in(Cl) ̸= ∅and
{Conc(B) | B ∈SINKSAFP and (A, B) ∈att} ∩undec(Cl) ̸= ∅
• CL2AL(Cl)(A) = idk iff Vul(A) ∩in(Cl) = ∅, while Vul(A) \ out(Cl) ̸= ∅
and Conc(B) | B ∈SINKSAFP and (A, B) ∈att ⊆undec(Cl)
• CL2AL(Cl)(A) = idc iff Vul(A) ∩in(Cl) = ∅, while Vul(A) \ out(Cl) ̸= ∅
and {Conc(B) | B ∈SINKSAFP and (A, B) ∈att} \ undec(Cl) ̸= ∅
Theorem 1. When restricted to complete argument labellings and complete conclusion
labellings, the functions AL2CL and CL2AL are bijections and each other’s inverse.

AA vs LP in 5-Valued Labellings
167
We are now ready to present some key results of our work. In what follows, let P be
a logic program and AFP be its associated argumentation framework. Further, assume
Cl = (in(Cl), out(Cl), undec(Cl)) is an arbitrary conclusion labelling of AFP . The
results below are based on our σ-mapping from 5-valued labellings to the standard
in/out/undec-labellings from [2] and theorems from [5] connecting the standard
labellings to logic programming models. In summary, the following theorems assure
our 5-valued labellings preserve all the results from [5].
Theorem 2. Let Al be a complete argument labelling of AFP . Then AL2CL(Al) = Cl
iff ⟨in(Cl), out(Cl)⟩is a p-stable model of P.
As our deﬁnition of complete argument labelling corresponds to the deﬁnition of
[2] and the characterization of in is equivalent in both settings, we can borrow the
following result from [5]:
Lemma 1. Let P be a program and AFP = (Ar, att) be its associated argumen-
tation framework. Let Al1 and Al2 be complete argument labellings of AFP , and
Cl1 = AL2CL(Al1) and Cl2 = AL2CL(Al2). It holds that
1. in(Al1) ⊆in(Al2) iff in(Cl1) ⊆in(Cl2),
2. in(Al1) = in(Al2) iff in(Cl1) = in(Cl2), and
3. in(Al1) ⊊in(Al2) iff in(Cl1) ⊊in(Cl2).
From Theorem 2 and Lemma 1, we obtain:
Theorem 3. Let Al be the grounded argument labelling of AFP . Then AL2CL(Al) = Cl
iff in(Cl) is the well-founded model of P.
Theorem 4. Let Al be a preferred argument labelling of AFP . Then AL2CL(Al) = Cl
iff in(Cl) is a regular model of P.
In addition, we know idk, idc = ∅in a complete argument labelling Al iff
undec = ∅in AL2CL(Al) = Cl (Deﬁnition 14). Thus,
Theorem 5. Let Al be a stable argument labelling of AFP . Then AL2CL(Al) = Cl iff
in(Cl) is a stable model of P.
Following the queue of the theorems above, we can as well consider the conclusion
labellings where in(Cl) is an L-stable model of P. The natural candidate argument
labellings to correspond to those conclusion labellings would be the semi-stable argu-
ment labellings, since both semi-stable and L-stable semantics respectively minimize
undecided arguments and undecided atoms. Unfortunately, in the same way as in [5],
we ﬁnd there are cases where the semi-stable semantics does not exactly correspond
to the L-stable semantics. Our running example is enough to show that: as one can
observe from the computed labellings in Example 6, the framework from Example 1
has two semi-stable argument labellings (hence two semi-stable conclusion labellings),
namely Al2, Al3, but Cl3 is the sole complete conclusion labelling for which in(Cl)
is an L-stable model of P. In other words, P has a single L-stable model, but AFP
has two semi-stable argument labellings. Differently from previous works, however,
our labellings isolated a possible culprit to that difference in Example 6, for we have
Al2(A4) = ido. But why may it be a culprit? Because if A4 was instead labelled idk
in Al2, only Al3 would be semi-stable, ﬁxing the expected correspondence.

168
S. S´a and J. Alcˆantara
5
The L-Stable Semantics for Abstract Argumentation
Frameworks
The discussion about the semi-stable and L-stable semantics by the end of Sect. 4 led
us to the suggestion that changing the labels of some particular arguments could lead
those semantics to coincide. Unfortunately, such changes are far from trivial, as chang-
ing the label of an argument is likely to affect the labels of others. Fortunatelly, there
are other parts of the three-step procedure from Sect. 3 that can be easily adapted to
achieve a similar result: the conception of a new abstract argumentation semantics that
corresponds perfectly to the L-stable semantics from logic programming.
Deﬁnition 16. Let AL be the set of all complete argument labellings of AF. Then Al is
a least-stable (or L-stable) argument labelling of AF if Al ∈AL and ∄Al′ ∈AL such
that idk(Al′) ∪ido(Al′) ⊊idk(Al) ∪ido(Al).
Lemma 2. Let Al be a complete labelling of an argumentation framework and Cl =
AL2CL(Al). Then ∃A ∈idk(Al) ∪ido(Al) with Conc(A) = c iff Cl(c) = undec.
Theorem 6. Let Al be a least-stable argument labelling of AFP . Then AL2CL(Al) = Cl
iff in(Cl) is an L-stable model of P.
This correspondence ensures the L-stable semantic for argumentation frameworks
has similar properties to the semi-stable semantics.
Corollary 1. Every AF has at least one L-stable labelling.
Corollary 2. If Al is astablelabellingof AFP , then Al is an L-stable labelling of AFP .
Corollary 3. If AFP has at least one stable labelling, then Al is an L-stable labelling
of AFP iff Al is a semi-stable labelling of AFP iff Al is a stable labelling of AFP .
However, differently than semi-stable, the L-stable AA semantic captures the L-
stable semantics for LP.
6
On the Role of Sink Arguments
We have afﬁrmed sink arguments together with 5-valued labellings are able to isolate
the possible culprits for the difference between the semi-stable and L-stable semantics.
Now we resume this discussion to provide a more detailed account on the role played
by sink arguments in argumentation semantics. In particular, we will emphasize the
following two points:
– When considering only the sink arguments obtained in our translation (Deﬁnition 9),
3-valued labellings sufﬁce to capture the equivalence between logic programming
semantics and abstract argumentation semantics (including L-stable).
– The sink arguments together with the 5-valued labellings offer a more ﬁne-grained
view of the notion of undecidedness when compared with 3-valued labellings.

AA vs LP in 5-Valued Labellings
169
The sink arguments can be employed to obtain the conclusion labellings associated
with complete argument labellings:
Theorem 7. Let P be a logic program, AFP = (Ar P , attP ) be its associated argu-
mentation framework and Al be a complete 5-valued argument labelling of AFP . We
deﬁne Cl : HBP →V3:
Cl(c) =

σ(Al(A)) if ∃A ∈Ar P such that Conc(A) = not c
out
otherwise
in which in = out, out = in and undec = undec. Then AL2CL(Al) = Cl and
⟨in(Cl), out(Cl)⟩is a p-stable model of P.
From Deﬁnition 9, there is at most one argument A in AFP such that Conc(A) =
not c for each c ∈HBP . Therefore, the function Cl above is well-deﬁned.
Theorem 8. Let P be a program, AFP = (Ar P , attP ) be its associated argumentation
framework, Al be a 5-valued argument labelling of AFP and AL3(Al) : SINKSAFP →
{in, out, undec}, in which for every A ∈SINKSAFP , we have AL3(Al)(A) =
σ(Al(A)). Let also AL be the set of all complete argument labellings of AFP . Then
– Al is the grounded argument labelling of AFP iff Al ∈AL and ∄Al′ ∈AL such
that out(AL3(Al′)) ⊊out(AL3(Al)).
– Al is a preferred argument labelling of AFP iff Al ∈AL and ∄Al′ ∈AL such that
out(AL3(Al′)) ⊋out(AL3(Al)).
– Al is a stable argument labelling of AFP iff Al ∈AL and undec(AL3(Al)) = ∅.
– Al is an L-stable argument labelling of AFP iff Al ∈AL and ∄Al′ ∈AL such that
undec(AL3(Al′)) ⊊undec(AL3(Al)).
Theorem 8 highlights a distinction between the semi-stable and L-stable semantics
over an argumentation framework AFP obtained from a logic program P: the semi-
stable argument labellings of AFP are those complete argument labellings with minimal
set of undec-labelled arguments, whereas the L-stable argument labellings of AFP are
those complete argument labellings with minimal set of undec-labelled sinks. There-
fore, the labels assigned to the sink arguments of AFP sufﬁce to know if a labelling is
grounded, preferred, stable and L-stable, but they may not sufﬁce to know if it is semi-
stable. Revisiting Example 6, we have both Al2 and Al3 are complete labellings with
minimal undec arguments (semi-stable, see Example 1); in contrast, Al3 is the only
complete labelling with minimal undec sinks (L-stable, see Example 2).
One may then wonder it the 5-valued labelling semantics we introduced in this
paper are of some interest on their own. First, observe that the 5 labels are a neces-
sary condition to deﬁne the L-stable argumentation semantics (complete labellings with
minimal (ido ∪idk)-valued arguments). Further, although one can mimic the L-stable
logic programming semantics with 3-valued labellings, it will work as expected just for
argumentation frameworks obtained from logic programs through Deﬁnition 11; in an
arbitrary argumentation framework (one that does not account for default arguments),
unlike our deﬁnition of L-stable semantics based on 5 labels, this adaptation of the
3-valued approach to capture L-stable will not produce meaningful results.

170
S. S´a and J. Alcˆantara
Besides, we emphasize the role played by sink arguments in our 5-valued labellings:
in an argumentation framework AF without sink arguments, its complete 3-valued
labellings coincide precisely with its 5-valued counterpart by simply replacing undec
for idk in the respective labellings. This means that the semi-stable and L-stable argu-
mentation semantics collapse into each other for those frameworks! What is more,
owing to the 5-valued labellings, we can deﬁne new semantics with interesting prop-
erties: for instance, consider a new argumentation semantics (let us call it “I-stable”)
deﬁned by the complete labellings with minimal idk arguments. In an argumentation
framework without sink arguments, it will coincide with both semi-stable and L-stable
argumentation semantics and yet it will possibly produce distinct results for some argu-
mentation frameworks. As such, these semantics offer three different views of unde-
cidedness in which the difference between them is positively determined by the sink
arguments.
7
Conclusion
This work was largely motivated by the curious result of [5] stating the semi-stable
semantics from abstract argumentation is not equivalent to the L-stable semantics from
logic programming. Both semantics are based on the minimization of undecided argu-
ments and atoms in their respective domains, share very similar properties, and yet
are different. Our efforts to understand the differences between the semantics led us to
consider the instantiation of special arguments in logic programming we called default
arguments and the specialization of the standard 3-valued argument labellings from [2]
into more expressive 5-valued partial labellings (as previously considered in [1]). To
accomplish our goals, we considered the labels in, out, ido, idk, idc and introduced a
corresponding deﬁnition of complete labellings revising the meanings of out, idk, idc
and introducing the new label ido, here dubbed “It doesn’t matter if it’s out”. Fol-
lowing the revised concepts, we showed our approach preserves well-known results
in the semantic comparison of abstract argumentation and logic programming. While
we did not try to resolve the semi-stable-L-stable equivalence problem, we shed light
into it, as our 5-valued labellings can isolate the possible culprit arguments causing
the differences based on the labels ido, idc. Instead, we introduced a new semantics
for abstract argumentation frameworks: the least-stable (or L-stable) semantics. What
is more, we proved our newly established semantics to be equivalent to the logic pro-
gramming L-stable semantics for normal programs and their corresponding instantiated
argumentation frameworks. The non-equivalence result proved in [5] was based on a
particular procedure for the instantiation of argumentation frameworks from logic pro-
grams. The 5-valued labellings, the sink arguments and our revision of their procedure
are the key changes to obtain an argumentation-based semantics corresponding to L-
stable. Indeed, we show that if we restrict our attention to sink arguments, the standard
3-valued labelling is enough to characterize the logic programming semantics, includ-
ing L-stable.
It is worth noticing that partial labellings were also employed by [12] in an algo-
rithm for enumerating preferred extensions of an argumentation framework. As we do,
they also consider ﬁve labels, namely in, out, undec, blank, and must-out labels. In

AA vs LP in 5-Valued Labellings
171
their algorithm, all arguments of the framework are initially blank and must-out is
used at run time to ﬂag arguments that are candidate to be labelled out.
Natural ramiﬁcations of this work include an in-depth investigation of properties
of the least-stable argumentation semantics. Our newly deﬁned 5-valued complete
labellings may also lead to the proposal of other interesting semantics as we try to max-
imize or minimize different combinations of the labels at our disposal. We also intend
to investigate whether sink arguments play other relevant roles in the characterization
or computation of argumentation semantics.
References
1. Baroni, P., Giacomin, M., Liao, B.: I don’t care, i don’t know ... i know too much! on incom-
pleteness and undecidedness in abstract argumentation. In: Eiter, T., Strass, H., Truszczy´nski,
M., Woltran, S. (eds.) Advances in Knowledge Representation, Logic Programming, and
Abstract Argumentation. LNCS (LNAI), vol. 9060, pp. 265–280. Springer, Cham (2015).
https://doi.org/10.1007/978-3-319-14726-0 18
2. Caminada, M.: On the issue of reinstatement in argumentation. In: Fisher, M., van der
Hoek, W., Konev, B., Lisitsa, A. (eds.) JELIA 2006. LNCS (LNAI), vol. 4160, pp. 111–123.
Springer, Heidelberg (2006). https://doi.org/10.1007/11853886 11
3. Caminada, M.: Semi-stable semantics. In: Dunne, P.E., Bench-Capon, T.J.M. (eds.) Com-
putational Models of Argument: Proceedings of COMMA 2006, September 11–12, 2006,
Liverpool, UK. Frontiers in Artiﬁcial Intelligence and Applications, vol. 144, pp. 121–130.
IOS Press (2006). http://www.booksonline.iospress.nl/Content/View.aspx?piid=1932
4. Caminada, M., Harikrishnan, S., S´a, S.: Comparing logic programming and formal argumen-
tation; the case of ideal and eager semantics. In: Argument and Computation Pre-press, pp.
1–28 (2021). https://doi.org/10.3233/AAC-200528
5. Caminada, M., S´a, S., Alcˆantara, J., Dvoˇr´ak, W.: On the equivalence between logic program-
ming semantics and argumentation semantics. Int. J. Approx. Reason. 58, 87–111 (2015)
6. Dung, P.: An argumentation procedure for disjunctive logic programs. J. Logic Program. 24,
151–177 (1995)
7. Dung, P.: On the acceptability of arguments and its fundamental role in nonmonotonic rea-
soning, logic programming and n-person games. Artif. Intell. 77, 321–357 (1995)
8. Dvoˇr´ak, W., Gaggl, S.A., Wallner, J.P., Woltran, S.: Making use of advances in answer-set
programming for abstract argumentation systems. In: Tompits, H., Abreu, S., Oetsch, J.,
P¨uhrer, J., Seipel, D., Umeda, M., Wolf, A. (eds.) INAP/WLP -2011. LNCS (LNAI), vol.
7773, pp. 114–133. Springer, Heidelberg (2013). https://doi.org/10.1007/978-3-642-41524-
1 7
9. Eiter, T., Leone, N., Sacc´a, D.: On the partial semantics for disjunctive deductive databases.
Ann. Math. Artif. Intell. 19(1–2), 59–96 (1997)
10. Jakobovits, H., Vermeir, D.: Robust semantics for argumentation frameworks. J. Logic Com-
put. 9(2), 215–261 (1999)
11. Nieves, J.C., Cort´es, U., Osorio, M.: Preferred extensions as stable models. Theory Pract.
Logic Program. 8(4), 527–543 (2008)
12. Nofal, S., Atkinson, K., Dunne, P.E.: Algorithms for decision problems in argument systems
under preferred semantics. Artif. Intell. 207, 23–51 (2014)
13. Przymusinski, T.: The well-founded semantics coincides with the three-valued stable seman-
tics. Fundamenta Informaticae 13(4), 445–463 (1990)

172
S. S´a and J. Alcˆantara
14. Toni, F., Sergot, M.: Argumentation and answer set programming. In: Balduccini, M., Son,
T.C. (eds.) Logic Programming, Knowledge Representation, and Nonmonotonic Reasoning.
LNCS (LNAI), vol. 6565, pp. 164–180. Springer, Heidelberg (2011). https://doi.org/10.1007/
978-3-642-20832-4 11
15. Wu, Y., Caminada, M.: A labelling-based justiﬁcation status of arguments. Stud. Logic 3(4),
12–29 (2010)
16. Wu, Y., Caminada, M., Gabbay, D.M.: Complete extensions in argumentation coincide with
3-valued stable models in logic programming. Studia Logica 93(1–2), 383–403 (2009)

Assumption-Based Argumentation
Is Logic Programming with Projection
Samy S´a(B)
and Jo˜ao Alcˆantara
Universidade Federal do Cear´a, Fortaleza, Brazil
samy@ufc.br , jnando@lia.ufc.br
Abstract. We revisit the semantic relations between Assumption-Based
Argumentation (ABA) and Logic Programming (LP) based on the recent
development of model-based semantics for ABA frameworks. This eﬀort
is motivated by the close resemblance between the computation of com-
plete ABA models and the computation of Przymuzinski’s partial stable
models for logic programs. As we show these concepts coincide ipsis
litteris, multiple results about the diﬀerent ABA semantics (preferred,
grounded, stable, semi-stable, ideal, eager) and corresponding LP seman-
tics (regular, well-founded, stable, L-stable, ideal, eager) follow. Our app-
roach also introduces a new translation from ABA frameworks to logic
programs that has better properties than the one available in the lit-
eratue, including lower computational complexity. The combination of
our new translation and model-based ABA semantics is the key to all of
our results. It is also known that the more traditional assumption exten-
sion and labelling-based semantics for ABA can be obtained from ABA
models using an operation called tuple projection, so it follows from our
results that ABA is LP with projection.
Keywords: Logic Programming · Assumption-Based Argumentation ·
Argumentation semantics
1
Introduction
In this work, we provide new perspective on the semantic relations between
Assumption-Based Argumentation (ABA) [2,8,15] and Logic Programming (LP)
[10]. Both are rule-based formalisms of nonmonotonic reasoning sharing very
similar syntax and whose semantics may be understood through the application
of 3-valued interpretations. Previous research [2,6,14] provided ways for map-
ping logic programs into ABA frameworks and vice-versa and found that ABA
and LP are pairwise equivalent under almost every semantics, with a notable
exception concerning their respective semantics minimizing undecidedness [6]:
the L-stable semantics for logic programs [9] and the semi-stable semantics for
ABA frameworks [4]. Here, we follow this trend of recent works on expressive
power of argumentation and nonmonotonic reasoning formalisms [1,4–6,13,16],
bringing some new mechanics and intuitions to this type of comparative research.
c
⃝Springer Nature Switzerland AG 2021
J. Vejnarov´a and N. Wilson (Eds.): ECSQARU 2021, LNAI 12897, pp. 173–186, 2021.
https://doi.org/10.1007/978-3-030-86772-0_13

174
S. S´a and J. Alcˆantara
For starters, we propose that the diﬀerence between ABA and LP is that given
a theory in any desired logics, once adapted to each case,
– LP semantics evaluates what sentences are true in the models of the theory;
while
– ABA semantics evaluates what sentences are false in the models of the theory.
This idea is largely motivated by the available translations between ABA
and LP in the literature [2,6] and can be illustrated by an example even without
formally introducing the relevant concepts. Towards that end, consider the logic
program P 1 with four rules below2:
P : a ←not b c ←not c
b ←not a d ←b, c
The language of P comprises four sentences, namely a, b, c, d. The semantics
of LP are based on models, each one proposing which sentences from the pro-
gram’s language may be simultaneously true. In each case, the evaluation of the
derived sentences not a, not b, not c, not d will be obtained as a byproduct of
those models. In particular, we highlight that (traditionally) only a, b, c, d will
appear in the models of P.
Following [2], P would be translated into the ABA framework ABA(P) = F3
below.
F : a ←β c ←γ
α = a β = b
b ←α d ←b, c γ = c δ = d
Notice how the rules in F (depicted in the ﬁrst two columns) mirror the rules
of P, while the operator −captures the semantics of not .
The language of F is a bit diﬀerent from the language of P, but not by
much. It consists of eight sentences, namely a, b, c, d, α, β, γ, δ, where α, β, γ, δ
respectively model not a, not b, not c, not d as native elements in F’s language.
Among the sentences in the language of F, the ones we can assume to be true
by default (α, β, γ, δ in our example) are called assumptions. In this format,
each assumption χ must have a unique contrary (obtained via the operator −)
which consists of a non-assumption capable of denying χ. In our example, a is
the contrary of α (which corresponds to not a), b is the contrary of β (which
corresponds to not b), and so on. The semantics of ABA are based on assumption
extensions, which are sets of assumptions consistent with the set of rules. That
is, in the semantics of F (traditionally), only α, β, γ, δ will appear. Then, since
the assumptions of ABA(P) correspond to the negative literals in P, we can
retrieve from the assumption extensions what sentences are false in the models
of P.
One could expect these perspectives to be equivalent, but evidence [4–6,13]
suggests this may not always be the case. More speciﬁcally, in the case of ABA
1 This program is extracted from [6].
2 not denotes negation as failure [7].
3 At this time, to avoid the necessity of formal concepts, only the core syntactic ele-
ments of F are shown.

ABA is LP with Projection
175
and LP, these concepts can only be guaranteed to coincide if there is bijection
between defeasible sentences in the input knowledge base and their contraries [6].
While this property necessarily holds for logic programs, it does not necessarily
hold for ABA frameworks. This interferes with the ability for some LP seman-
tics to capture some ABA semantics, despite of their similarities. This suggests
that ABA is somehow more expressive than LP, but the results so far may be
limited by a missing property in the translation from ABA frameworks into logic
programs in which they are based, which is not injective. For instance, if one
follows [6] to translate F into a logic program, the result would be P. However,
this would also be the result if the input if F′ below, which is basically F, but
removing the assumption δ from its language.
F′ : a ←β c ←γ
α = a β = b
b ←α d ←b, c γ = c
While the diﬀerence may seem small, F, F′ have signiﬁcantly diﬀerent seman-
tics. The diﬀerence is enough so that the semi-stable assumption extensions of
F′ do not correspond to the L-stable models of P, while the semi-stable assump-
tion extensions of F do, and yet, F, F′ are both translated to P. Then perhaps,
this undesirable behavior of the translation from [6] is what causes ABA to seem
more expressive than LP. Be it or not, whether ABA and LP are equivalent is
still an open problem heavily related to the concept of undecidability.
In this work, we will contribute in several ways to show how LP captures
ABA and what causes the diﬀerences observed in previous works. First, we pro-
pose a new translation from ABA frameworks to logic programs satisfying some
desirable properties that the one by [6] does not. For instance, their translation
only works for ABA frameworks under strict syntactic assumptions, such as that
(i) each assumption has a unique contrary that may not be an assumption and
(ii) assumptions may not occur in the conclusions of rules. Our translation is
introduced under the same syntactic assumptions to facilitate proofs and com-
parison, but here they are not strict: ours may be easily adapted for backwards
compatibility to the original work on ABA [2], where, amongst other possibil-
ities, assumptions may have multiple contraries (including other assumptions)
and sentences may be contraries to multiple assumptions. In fact, we can even
accommodate ABA frameworks where assumptions may be in the conclusions
of rules. All of this is possible because we map assumptions from the ABA
framework to a proper subset of positive literals in the corresponding program,
instead of mapping them to negative literals (with not). As we do that, our
translation ensures that the language (assumptions + non-assumptions) of an
ABA framework matches the language of its corresponding logic program, which
is a stronger relation than an arbitrary one-to-one correspondence between sen-
tences. Another consequence of this strategy is that the contrary operator −in
the input ABA framework can be modeled using not in a straightforward way.
This means that ABA frameworks such as F, F′ in our discussion above will nec-
essarily have diﬀerent results in our translation. In fact, any changes to an ABA
framework will ﬁnd corresponding changes in the resulting logic program. Our

176
S. S´a and J. Alcˆantara
approach is further motivated by the recent proposal of model-based semantics
for ABA [13], where all sentences (instead of just assumptions) in the language of
an ABA framework are evaluated. Using our translation, we ﬁnd that the models
of an ABA framework are not only in one-to-one correspondence with the models
of the resulting logic program, but they coincide precisely for all particular cases
of the complete ABA semantics, including semi-stable. Therefore our result are
stronger than the previous ones. It conﬁrms that ABA frameworks are inherently
logic programs with somewhat diﬀerent syntax and a simple twist on its seman-
tics. That slight twist is characterized by an operation called tuple projection4,
where only a subset of sentences in the language of the input are extracted from
pre-computed models. Fundamentally, it consists of a restriction to the language
of the models. Our translation also beneﬁts of lower computational complexity
and is necessarily optimal: intuitively it consists of just reading the ABA frame-
work input once. Immediately, our results allow the computation of semantics
for any ABA frameworks by means of logic programming interpreters using a
simple parser to perform the tuple projection.
The rest of the paper develops as follows. In Sect. 2, we will introduce the
concepts of model-based ABA semantics and LP semantics, focusing on the com-
putation and properties of the complete (resp. p-stable) models for ABA (resp.
LP). In Sect. 3, we will introduce our novel translation from ABA frameworks
to logic programs and explore some properties. The inverse translation is intro-
duced in Sect. 4 and is used to characterize the class of ABA programs, which
is semantically equivalent to ABA frameworks. In Sect. 5 we gather the conclu-
sions of previous sections and organize those ideas to motivate our main results.
Sect. 6 concludes the paper with a summary of our contributions, ramiﬁcations
of our results and future works.
2
Formal Preliminaries
We deﬁne a set of truth values V = {t, u, f} whose elements stand respectively
for true, undecided, and false. We establish a total preorder > over V such that
t > u > f regarding how much truth is semantically attributed to each value.
We also deﬁne that −t = f, −f = t, and −u = u.
Based on V and given a set S of propositional symbols, we say that
I = ⟨T, F, U⟩is an interpretation of S iﬀ{T, F, U} is a partition of S in sets
of (respectively) true, false and undecided propositions. Just alike, I can be
understood as the function I : S →V such that (i) I(a) = t iﬀa ∈T; (ii)
I(a) = f iﬀa ∈F; and (iii) I(a) = u iﬀa ∈U. Given two interpretations
I = ⟨T, F, U⟩, J = ⟨T ′, F ′, U ′⟩, we say I ⪯J5 if T ⊆T ′ and F ′ ⊆F. An
interpretation I of S is a least interpretation of S iﬀI is ⪯-least.
4 In relational algebra, this is equivalent to selection, but the common name for this
operation in logic programming literature is projection.
5 The interpretation order ⪯is based on >.

ABA is LP with Projection
177
2.1
Normal Logic Programs
In the current paper, we account for propositional normal logic programs6, which
we call logic programs or simply programs from now on.
Deﬁnition 1. A rule r is an expression r : c ←a1, . . . , an, not b1, . . . , not bm
(n ≥0, m ≥0) where c, each ai (1 ≤i ≤n) and each bj (1 ≤j ≤m) are atoms
and not represents negation as failure. Let r be a rule, we write hd(r) to denote
its head (the atom c), bd+(r) to denote the set {a1, . . . , an} and bd−(r) to denote
the set {not b1, . . . , not bm}. We refer to the body of r as bd(r) = bd+(r)∪bd−(r).
A logic program P is then deﬁned as a ﬁnite set of rules. If every r ∈P has
bd−(r) = ∅, we say that P is a positive program. The Herbrand Base of a
program P is the set HBP of all atoms occurring in P.
Given a program P, we say interchangeably that an interpretation I of HBP
is an interpretation of P. In that case, we refer to the correlate function ˆI
[11,12] as the interpretation of HBP ∪{not a | a ∈HBP } where ˆI(a) = I(a)
and ˆI(not a) = −ˆI(a) for every a ∈HBP . Below, whenever implicit, assume a
generic program P.
Deﬁnition 2. An interpretation I = ⟨T, F, U⟩of a program HBP is a 3-
valued model (or simply a model) of P iﬀfor every rule r ∈P, ˆI(hd(r)) ≥
min{ ˆI(l) | l ∈bd(r) }.
The 3-valued stable models [12] of a program P can be obtained by a process
called program division. The ﬁrst step of this process eliminates rules that do
not have their bodies satisﬁed by I; the second step eliminates from the bodies
of the remaining rules all those negative literals that are necessarily true in I;
the last step gets rid of the remaining negative literals, replacing them by a new
literal u.
Deﬁnition 3. Let I be an interpretation of the program P. The reduct P/I is
the program D3(P, I) computed as:
1. D1(P, I) = P \ {r | not bi ∈bd−(r) for some bi ∈T}
2. D2(P, I) = {hd(r) ←bd(r) \ {not bi | bi ∈F} | r ∈D1(P, I)}
3. D3(P, I) = {r ∈D2(P, I) | bd−(r) = ∅} ∪{hd(r) ←bd+(r) ∪{u} | r ∈
D2(P, I) and bd−(r) ̸= ∅}.
In the above procedure, u is an atom not in HBP which is undecided in all
interpretations of P (a constant). It should be noticed that P/I will always be
positive [11], which ensures that P/I has a unique least model denoted ΨP (I) =
⟨TΨ, FΨ, UΨ⟩7 with minimal TΨ and maximal FΨ (w.r.t. set inclusion) such that,
for every a ∈HBP , (i) a ∈TΨ if there is a rule r′ ∈P/I with hd(r′) = a
and bd+(r′) ⊆TΨ; and (ii) a ∈FΨ if every rule r′ ∈P/I with hd(r′) = a has
bd+(r′) ∩FΨ ̸= ∅.
6 Logic programs whose rules may contain weak but not strong negation, and the head
of each rule is an atom [10].
7 ΨP (I) is a least ﬁx-point of the immediate consequences operator Ψ of [11], which
is guaranteed to exist and be unique for positive programs.

178
S. S´a and J. Alcˆantara
Deﬁnition 4. An interpretation I of the program P is a 3-valued stable (alt.
p-stable) model of P iﬀΨP (I) = I.
2.2
Assumption-Based Argumentation
For our convenience, we will introduce ABA frameworks from the perspective of
model-based semantics [13]. This approach exploits the identical structure shared
by inference rules (ABA) and program rules (LP), allowing the computation of
the complete semantics for ABA by a process that mimics the computation of
p-stable models from [11].
Deﬁnition 5. An ABA framework is a tuple ⟨L, R, A,¯⟩where (L, R) is a
deductive system8, with language L and set of inference rules R, A ⊆L9 whose
elements are referred as assumptions, and ¯ is a total mapping from A into
L \ A10, where ¯α is called the contrary of α.
Given an ABA framework F, we say interchangeably that an interpretation
I of L is an interpretation of F. For every inference rule C ←L1, . . . , Ln in R,
we deﬁne hd(r) = C and bd(r) = {L1, . . . , Ln}, so r can be written as hd(r) ←
bd(r). For our current purposes, we will restrict our attention to ﬂat ABA-
frameworks, which means the heads of inference rules cannot be assumptions
[8]. In what follows, whenever implicit, assume a generic ﬂat ABA framework
F = ⟨L, R, A,¯⟩.
Deﬁnition 6. An interpretation I of L is (3-valued) model of F iﬀ(i) ∀r ∈R,
I(hd(r)) ≥min{ I(l) | l ∈bd(r) } and (ii) ∀a ∈A, I(a) = −I(¯a).
The concept of framework reduction [13] mimics the program division of [11].
Deﬁnition 7. Let I be an interpretation of F. The reduct F/I is the ABA
framework R3(F, I) where each 1 ≤i ≤3, Ri(F, I) = ⟨Li, Ri, Ai,¯i⟩s.t. that:
1. R1 = R \ {r | ∃a ∈A ∩bd(r) s.t. ¯a ∈T}, A1 = A \ F, L1 = L \ (A ∩F);
2. R2 =

R1 ∪

hd(r) ←bd(r) \ {a ∈A | ¯a ∈F}
 r ∈R1
\ {r ∈R1 | ∃a ∈
A ∩bd(r) s.t. ¯a ∈F}, A2 = A1 \ T, L2 = L1 \ (A1 ∩T);
3. R3 = {r ∈R2 | bd(r) ∩A = ∅} ∪{hd(r) ←(bd(r) \ A) ∪{u} | r ∈
R2 and bd(r) ∩A ̸= ∅}, A3 = A2 \ U = ∅, L3 = (L2 \ (A ∩U)) ∪{u} =
(L \ A) ∪{u};
4. ¯ai : Ai →Li is such that ¯ai = ¯a for each a ∈Ai.
8 The concept of deductive systems used in ABA is fully detailed in [2].
9 Traditionally, A is required to be non-empty. We opt to relax that condition to favor
a simpler presentation of our later concepts.
10 In most ABA works, the codomain of ¯ is L, but the contraries of assumptions are
implicitly assumed to be non-assumptions.

ABA is LP with Projection
179
In the above, u is an atom not in L which is undecided in all interpretations
of L (a constant).
Notice the reduct F/I has no assumptions, allowing us to disregard the
contrariness relation in operations over F/I, which ensures that F/I = ⟨(L \
A)∪{u}, R3, ∅, ∅⟩has a unique least model ΨF(I) = ⟨TΨ, FΨ, UΨ⟩with minimal
TΨ and maximal FΨ (w.r.t. set inclusion) such that, for all l ∈L \ A, (i) l ∈TΨ
iﬀthere is r ∈R3 with hd(r) = l and bd(r) ⊆TΨ; and (ii) l ∈FΨ iﬀevery
r ∈R3 with hd(r) = l has bd(r) ∩FΨ ̸= ∅[13]. From ΨF(I), we can obtain a
corresponding least model of F denoted M(F/I) such that (iii) M(F/I)(l) =
ΨF(I)(l) for all l ∈L \ A and (iv) M(F/I)(l) = −M(F/I)(¯l) for all l ∈A [13].
Deﬁnition 8. An interpretation I of L is a complete model of F iﬀM(F/I) =
I.
Example 1. Let F = ⟨L, R, A,¯⟩, s.t. L = {a, b, c, α, β, γ}, R = {a ←β, b ←
α, b ←γ, c ←β, c ←γ}, A = {α, β, γ}, and ¯α = a, ¯β = b, ¯γ = c. The complete
models of F are I1 = ⟨{ }, { }, {a, b, c, α, β, γ}⟩, I2 = ⟨{a, c, β}, {b, α, γ}, { }⟩,
and I3 = ⟨{b, α}, {a, β}, {c, γ}⟩.
3
From ABA to LP
In this section, we will present a translation from any given ABA framework F
to a corresponding logic program preserving the semantics of F.
Deﬁnition 9. The
program
corresponding
to
an
ABA
framework
F
is
ABAf2LP(F) = R ∪{a ←not ¯a | a ∈A}.
Example 2. Recall F
from Example 1. The program associated to F
is
ABAf2LP(F) = {a ←β, b ←α, b ←γ, c ←β, c ←γ, α ←not a, β ←
not b, γ ←not c}. Below, we show R to the left and ABAf2LP(F) = P for
comparison and make use of superscripts F,P for contrast. Recall that F has
three complete models we named I1, I2, and I3 (Example 1). Just alike, I1, I2,
I3 are the p-stable models of ABAf2LP(F).
rF
1 : a ←β
rF
2 : b ←α
rF
3 : b ←γ
rF
4 : c ←β
rF
5 : c ←γ
rP
1 : a ←β rP
6 : α ←not a
rP
2 : b ←α rP
7 : β ←not b
rP
3 : b ←γ rP
8 : γ ←not c
rP
4 : c ←β
rP
5 : c ←γ
From the above, any ABAf2LP(F) will be clearly divided in two distinguishing
sets of rules. The ﬁrst set is obtained by importing R from F and (therefore)
has no negative literals. The second set is produced based on the contrariness
relation of F and each of its rules has an empty positive body. This second set
concentrates all the occurrences of negative literals in ABAf2LP(F). This is not by
chance. In fact, this rather convenient eﬀect stems from the similarities of ABA
and LP and is merely captured by our deﬁnition. Furthermore, provided that F
is ﬂat, the literals of ABAf2LP(F) that map what originally were assumptions in
F can only appear in the heads of rules in the second set, never the ﬁrst.

180
S. S´a and J. Alcˆantara
Lemma 1. Let F be an ABA framework, then I is an interpretation of F iﬀI
is an interpretation of ABAf2LP(F).
Lemma 2. If I is a model of F, then I is a model of ABAf2LP(F).
The converse of Lemma 2 does not hold for every LP model because, as
an example, ABAf2LP(F) from Example 2 admits models such as I where
I(a) = t, but I(α) ̸= f, while every model I′ of F must enforce that
I′(α) = −I′(a). Observe that for those r ∈ABAf2LP(F) ∩R, ABA mod-
els have the same requirement as with logic programming models, i.e., we
need that I(hd(r)) ≥min{ I(l) | l ∈bd(r) }. Then any LP models where
ˆI(hd(r)) ≯min{ ˆI(l) | l ∈bd(r) } for each r ∈ABAf2LP(F) \ R are also ABA
models.
Lemma 3. If I is a model of ABAf2LP(F) such that ˆI(a) = ˆI(not ¯a) for all
r : a ←not ¯a ∈ABAf2LP(F) \ R, then I is an ABA model of F.
Even though a program ABAf2LP(F) corresponding to F admits some models
that F does not, that does not prevent the complete models of F to coincide
with the p-stable models of ABAf2LP(F). This is one of our main original results.
Theorem 1. Let F be an ABA framework, then I is a complete model of F iﬀ
I is a p-stable model of ABAf2LP(F).
Proof. Let I be an interpretation of F, then, by Lemma 1, I is an interpretation
of ABAf2LP(F) = PF (for short). We follow by comparing F/I and PF/I step
by step. Let M(F/I) = ⟨TM, FM, UM⟩be the least model of F retrieved from
F/I and ΨPF (I) = ⟨TΨ, FΨ, UΨ⟩be the least model of PF/I. We will refer to
the intermediate reducts produced in the division procedures proposed for logic
programs (Deﬁnition 3) and ABA frameworks (Deﬁnition 7).
First, we will ﬁrst show that if I is a model of F, then ΨP (I) = M(F/I).
We can assume that I is a model of F, PF, because either hypothesis that I is
a complete model of F or that I is a p-stable model of PF ensure as much.
Let r ∈PF, we have two possibilities: r ∈PF ∩R or r ∈PF \ R. Recall from
Deﬁnition 9, that (a) hd(r) ∈L \ A and bd−(r) = ∅for all r ∈PF ∩R while (b)
hd(r) ∈A and bd(r) = {not hd(r)} for all r ∈PF \ R, which means bd+(r) = ∅
and |bd−(r)| = 1 for r ∈PF \ R. Further, take note that the program division
(Deﬁnition 3) can only remove or change those r ∈PF where bd−(r) ̸= ∅, so
R ⊆PF/I. For that reason, we will consider the evaluation of bd(r) for r ∈R.
We have four possibilities:
(1) Suppose bd(r) ∩A = ∅. From that, we obtain that r ∈F/I, which means
that M(F/I)(hd(r)) ≥min{ M(F/I)(l) | l ∈bd(r) }. Further, we obtain that
r ∈P/I and ΨP (I)(hd(r)) ≥min{ 
ΨP (I)(l) | l ∈bd(r) }.
(2) Suppose bd(r)∩A∩F ̸= ∅⇔∃a ∈bd(r)∩A s.t. a ∈F ⇔∃a ∈bd(r)∩A s.t.
¯a ∈T ⇔r /∈R1 and a ←not ¯a /∈D1(PF, I) and {r ∈R | a ∈bd(r)} ∩R1 = ∅.
Since r /∈R3, we can only state that M(F/I)(hd(r)) ≥f.
Because r ∈PF ∩R, we know r ∈PF/I. However, since a ←not ¯a is the
only r′ ∈PF s.t. hd(r′) = a, we have {r′ ∈PF/I | hd(r′) = a} = ∅. Then

ABA is LP with Projection
181
a ∈FΨ, which means 
ΨP (I)(a) = f and min{ 
ΨP (I)(l) | l ∈bd(r) } = f. Then
ΨP (I)(hd(r)) ≥f.
Then, for all r ∈R s.t. bd(r) ∩A ∩F ̸= ∅, we will have M(F/I)(hd(r)) ≥f
and ΨP (I)(hd(r)) ≥f
Cases (3) and (4) account implicitly for the possibilities concerning bd(r) ∩
A∩T, as the second step of framework reduction and program division will only
remove those l ∈bd(r) ∩A ∩T from each r in F and PF.
(3) Suppose bd(r) ∩A ∩F = ∅and bd(r) ∩A ∩U = ∅. Let rT
A = {a ∈bd(r) |
a ∈A ∩T}, then r will be replaced by r′ : hd(r) ←bd(r) \ rT
A in R2. Then
M(F/I)(hd(r)) ≥min{ M(F/I)(l) | l ∈bd(r) \ rT
A }.
Given that ¯a ∈F for each a ∈rT
A, each a ←not ¯a ∈PF where a ∈rT
A will
be replaced by a ←in D2(PF, I). Recall that for each a ∈rT
A, a ←not ¯a is the
only r′ ∈PF with hd(r′) = a. Then the only r′ ∈D2(PF, I) s.t. hd(r′) = a is
a ←. Because body−(r′) = ∅, we have r′ ∈PF/I, which means 
ΨP (I)(a) = t for
every a ∈rT
A. Then ΨP (I)(hd(r)) ≥min{ 
ΨP (I)(l) | l ∈bd(r) \ rT
A }.
Therefore, for all r ∈R s.t. bd(r) ∩A ∩F = ∅and bd(r) ∩A ∩U = ∅, we will
have M(F/I)(hd(r)) ≥min{ M(F/I)(l) | l ∈bd(r) \ rT
A } and ΨP (I)(hd(r)) ≥
min{ 
ΨP (I)(l) | l ∈bd(r) \ rT
A }.
(4) The last case, where bd(r) ∩A ∩F = ∅, but bd(r) ∩A ∩U ̸= ∅, has a
proof analogous to that of (3).
From (1)–(4), ΨP (I) and M(F/I) impose the same constraints for the eval-
uation of the heads of each r ∈R. Given that ΨP (I) and M(F/I) are least
models, we ﬁnd that ΨP (I)(hd(r)) = M(F/I)(hd(r)) for all r ∈R. Then
ΨP (I)(a) = M(F/I)(a) for all a ∈L \ A.
Now we need to prove that ΨP (I)(a) = M(F/I)(a) for all a ∈A. Given
that a ←not ¯a is the only rule in PF with hd(r) = a, we have, for all a ∈
A, ΨP (I)(a) = 
ΨP (I)(a) = 
ΨP (I)(not ¯a) = −
ΨP (I)(¯a) = −M(F/I)(¯a) =
−M(F/I)(¯a) = M(F/I)(a), i.e., ΨP (I)(a) = M(F/I)(a). Then ΨP (I)(l) =
M(F/I)(l) for all l ∈L.
Finally, I is a complete ABA model of F ⇔M(F/I) = I ⇔ΨP (I) = I ⇔
I is a p-stable model of PF.
□
An intuition for the proof of this theorem can be obtained by comparing
the division procedures for ABA and LP based on the same interpretation. This
result ensures that every ﬂat ABA framework has a corresponding LP whose p-
stable models are precisely the same as the ABA framework’s complete models.
At this point, we can state:
Corollary 1. Flat-ABA under model-based semantics is a fragment of NLP.
We will complement these results with theorems in the next couple of sec-
tions to conclude that ﬂat-ABA is actually equivalent to NLP equipped with an
operation called tuple projection.

182
S. S´a and J. Alcˆantara
4
From LP to ABA
Previously, we showed that every ABA framework has a corresponding logic
program that fully preserves its original semantics. In this section, we explore
a translation from logic programs to ABA frameworks in an attempt to reverse
Deﬁnition 9. The new proposed translation can only operate on some programs
which we identify as ABA programs. Later on, we will expand the results of this
section.
Ideally, we would like for every program P to have a corresponding ABA
framework FP such that ABAf2LP(FP ) = P. If possible, we would like that
mapping to be the inverse of ABAf2LP. We will start by ensuring that is the
case for those programs akin to the results produced by ABAf2LP, which we
denominate as ABA-programs.
Deﬁnition 10. Given a normal logic program P, we say P is an ABA program
iﬀ(i) for all r ∈P, if bd−(r) ̸= ∅, then |bd(r)| = 1; and (ii) for all r, r′ ∈P, if
bd−(r) ̸= ∅and bd−(r′) ̸= ∅, then hd(r) ̸= hd(r′).
Deﬁnition 11. The ABA framework corresponding to an ABA program P is
LP2ABAf(P) = ⟨HBP , RP , AP ,¯P ⟩such that
– RP = {r ∈P | bd−(r) = ∅},
– AP = {hd(r) | r ∈P and bd−(r) ̸= ∅}, and
– ¯P is such that ¯aP = l for each a ∈AP , where a ←not l is the only r ∈P
with hd(r) = a.
Example 3. From Example 2, retrieve F and ABAf2LP(F) = P. One can verify
that P is an ABA program and that LP2ABAf(P) = F.
We can state the following.
Lemma 4. Given an ABA framework F, it holds that LP2ABAf(ABAf2LP(F)) =
F.
Lemma 5. Given an ABA-program P, it holds that ABAf2LP(LP2ABAf(P)) = P.
Theorem 2. When restricted to the set of all ﬂat ABA frameworks and the set
of all ABA-programs, ABAf2LP and LP2ABAf are both bijective and each other’s
inverse.
Proof. Let S be the set of all ﬂat ABA frameworks and T be the set of all ABA-
programs. It follows from Deﬁnition 11 that LP2ABAf : S →T is injective. Then,
|S| ≤|T| (via Lemma 4) and |S| ≥|T| (via Lemma 5). Therefore, |S| = |T|
and LP2ABAf is bijective. Then ABAf2LP : T →S is bijective and LP2ABAf−1 =
ABAf2LP.
□
Example 4. Take F, F′ from Sect. 1. The program corresponding to F is
ABAf2LP(F):
a ←β c ←γ
α ←not a γ ←not c
b ←α d ←b, c β ←not b δ ←not d
Just like the diﬀerence between F, F′ is that δ is not in the language of F′, the
program ABAf2LP(F′) corresponding F′ is ABAf2LP(F) \ {δ ←not d}.

ABA is LP with Projection
183
Theorem 2 ensures the p-stable models of an ABA program coincide with
the complete models of its corresponding ABA framework (and vice-versa).
Corollary 2. Let P be an ABA program, then I is a p-stable model of P iﬀI
is a complete model of LP2ABAf(P).
The result of Corollary 2 is automatically extended to other core semantics of
ABA and LP due to their isomorphic hierarchies. For instance, the well-founded
model of a program P is the p-stable model of P where T is minimal w.r.t. set
inclusion, whereas the grounded model of an ABA framework F is the complete
model of F where T is minimal w.r.t. set inclusion. Given the mirrored conditions
on those deﬁnitions, I is the well-founded model of P iﬀI is the grounded model
of LP2ABAf(P). Similarly, the regular (resp. stable, L-stable, ideal) models of a
program P are the p-stable models of P where T is maximal (resp. where U = ∅,
U is minimal, T is maximally contained in each regular model of P), whereas
the preferred (resp. stable, semi-stable, ideal) models of an ABA program F are
the complete models of F where T is maximal (resp. U = ∅, U is minimal, T
is maximally contained in each preferred model of F), which means that I is a
regular (resp. stable, L-stable, ideal) model of P iﬀI is the preferred (resp. stable,
semi-stable, ideal) model of LP2ABAf(P). While further adaptations are required,
a similar result can be obtained for the eager semantics, which was proposed for
ABA frameworks in [4] via assumption extensions and more recently for LP in
[3].
5
Summary and Main Result
The results we introduced up to this point have some interesting ramiﬁcations
about ﬂat-ABA, normal LP, ABA programs, and their variations using param-
eterized projection. For instance, we are now equipped to conclude and contex-
tualize that
Proposition 1. Flat-ABA under model-based semantics is equivalent to ABA
Programs.
Proof. Follows from Theorem 1 + Theorem 2.
□
This result is rather surprising: while the results of previous works [2,6]
suggested ﬂat-ABA could be more expressive than normal LP, we found that a
fragment of normal LP suﬃces to capture ﬂat-ABA to the point where
1. there is a bijection between all ﬂat-ABA frameworks and all ABA programs
(Theorem 2) such that
2. their semantics coincide precisely (Theorem 1, Corollary 2).
Again, we highlight that the coincidence of semantics is a lot stronger than
the usual correspondences obtained using translations between ABA assumption
labellings and LP models. Our approach ﬁnds the exact same semantic results

184
S. S´a and J. Alcˆantara
given a corresponding logic program and ABA framework. It should be noticed
there is a diﬀerence of perspective, since our results are based on the model
semantics of ABA instead of assumption labellings. Fortunately, given an ABA
framework F, the assumption labellings of F in each semantics can be retrieved
from the models of F for the same semantics using a tuple projection11 param-
eterized by A [13]. Therefore,
Proposition 2. Flat-ABA under assumption labelling semantics is equivalent
to ABA programs with the projection σA.
Proof. Follows from Proposition 1 combined with Theorem 5 from [13].
□
This is the main conclusion of our work: while it reinforces that ﬂat-ABA is
not equivalent to normal LP, it clearly shows their diﬀerence may be character-
ized by the rather simple operation of tuple projection. The result is relatively
straightforward from Proposition 1, for it ensures that applying the same tuple
projection over the models of corresponding ABA frameworks and logic programs
in any given semantics will render identical results.
6
Conclusion
In this work, we revisited the semantic connection between Assumption-Based
Argumentation and Logic Programming in the light of model-based semantics
for ABA [13]. Diﬀerent from previous works on ABA semantics, model-based
semantics explicitly evaluates sentences that are not assumptions. Based on the
presentation and results of [13], we established a pair of translations ABAf2LP and
LP2ABAf that are bijective and each others inverse from the set of all ABA frame-
works to a class of logic programs we called ABA-programs. These translations
were motivated by the original speciﬁcation of ABA frameworks in [2], where
assumptions may have multiple contraries (including other assumptions) and
sentences may be contraries to multiple assumptions. This means that mapping
the assumptions of an ABA framework to negative literals in a corresponding
program is not trivial as done in the translation of [6]. For instance, their transla-
tion does not capture the semantics of ABA frameworks having an assumption as
the contrary of another assumption, at least not directly. Instead, it is necessary
to convert the ABA framework into another where the contrary of each assump-
tion is necessarily a non-assumption, then apply their translation. We recognize
there is a similar condition on our translation from logic programs to ABA frame-
works, which requires an ABA-program to work, but this is not a drawback, since
our focus is the translation from ABA frameworks into logic programs. Compar-
atively, our approach has an advantage compared to [6]: our translations ensure
that ABAf2LP(LP2ABAf(P)) = P and LP2ABAf(ABAf2LP(F)) = F, while their
translations only satisfy the ﬁrst.
11 Let S be a set and T = ⟨S1, S2, . . . , Sk⟩be a tuple of sets. The projection of elements
of S from T is σS(T) = ⟨S1 ∩S, S2 ∩S, . . . , Sk ∩S⟩.

ABA is LP with Projection
185
The semantic coincidences we identiﬁed concerning ABA models and LP
models (Theorem 2) extend also for the particular cases of the complete ABA
semantics and the p-stable LP semantics. This holds trivially as a corollary for
each semantics, since the complete ABA models of a framework match precisely
the p-stable models of its corresponding program. In particular, the semi-stable
models of a framework F coincide with the L-stable models of ABAf2LP(F), but
in some cases, as in [6], it is possible that the semi-stable labellings (that only
evaluate the assumptions in F) do not correspondence to the L-stable models of
ABAf2LP(F). This diﬀerence was also found when mapping semi-stable models
of an ABA framework to its semi-stable assumption labellings in [13], which
means that both the semi-stable and L-stable semantics are sensitive to tuple
projection. When comparing the ABA semi-stable and LP L-stable semantics,
we conjecture that their diﬀerence concerns whether we minimize undecidedness
before or after performing the projection σA. To motivate this idea, observe that
F′ (from Sect. 1) has three complete models: I1 = ⟨{ }, { }, {a, b, c, d, α, β, γ}⟩,
I2
= ⟨{a, β}, {b, α}, {c, d, γ}⟩, and I3
= ⟨{b, α}, {a, d, β}, {c, γ}⟩. Amongst
them12, only I3 has a minimal set of undecided sentences. However, if we com-
pute the assumption labellings corresponding to I1, I2, I3 using σ{α,β,γ} and
compare again, we will have σ{α,β,γ}(I1) = ⟨{ }, { }, {α, β, γ}⟩, σ{α,β,γ}(I2) =
⟨{β}, {α}, {γ}⟩, and σ{α,β,γ}(I3) = ⟨{α}, {β}, {γ}⟩. Now, both σ{α,β,γ}(I2) and
σ{α,β,γ}(I3) have minimal sets of undecided sentences. To ﬁnd the semi-stable
ABA model semantics (which coincide to the L-stable LP model semantics) we
minimized the sets of undecided sentences before applying projection. To ﬁnd
the semi-stable assumption labellings of F, on the other hand, we applied pro-
jection and only then we selected those models with minimal sets of undecided
sentences. Interestingly, only those semantics minimizing undecidedness seem to
be sensitive to this operation. Our results suggest that, despite its simplicity, the
tuple projection operator σ is instrumental to relate instances of non-monotonic
formalisms, given the example concerning ABA and LP.
In future works, we intend to study the projection operation in context to ﬁnd
whether its properties could motivate the proposal of new ABA and LP seman-
tics. Another potentially fruitful line of investigation regards checking whether
the projection operation may capture semantic diﬀerences between other for-
malisms.
References
1. Alcˆantara, J.F.L., S´a, S., Guadarrama, J.C.A.: On the equivalence between
abstract dialectical frameworks and logic programs. Theory Pract. Log. Program.
19(5–6), 941–956 (2019)
2. Bondarenko, A., Dung, P., Kowalski, R., Toni, F.: An abstract, argumentation-
theoretic approach to default reasoning. AI 93, 63–101 (1997)
12 Given some F (resp. P), a complete (resp. p-stable) model I = ⟨T, F, U⟩of F is a
semi-stable (resp. L-stable) model of F (resp. P) iﬀI has minimal {l ∈L | I(l) = u}
amongst all complete (resp. p-stable) models of F (resp. P).

186
S. S´a and J. Alcˆantara
3. Caminada, M., Harikrishnan, S., S´a, S.: Comparing logic programming and formal
argumentation; the case of ideal and eager semantics. Argum. Comput. (Preprint),
1–28 (2020)
4. Caminada, M., S´a, S., Alcˆantara, J., Dvor´ak, W.: On the diﬀerence between
assumption-based argumentation and abstract argumentation. IfCoLog J. Log-
ics Appl. 2, 15–34 (2015). http://www.collegepublications.co.uk/journals/ifcolog/?
00003
5. Caminada, M., S´a, S., Alcˆantara, J., Dvor´ak, W.: On the equivalence between logic
programming semantics and argumentation semantics. Int. J. Approx. Reason. 58,
87–111 (2015)
6. Caminada, M., Schulz, C.: On the equivalence between assumption-based argu-
mentation and logic programming. J. Artif. Intell. Res. 60, 779–825 (2017)
7. Clark, K.L.: Negation as failure. In: Logic and Data Bases, pp. 293–322. Springer,
Heidelberg (1978). https://doi.org/10.1007/978-1-4684-3384-5 11
8. Dung, P., Kowalski, R., Toni, F.: Assumption-based argumentation. In: Simari, G.,
Rahwan, I. (eds.) Argumentation in Artiﬁcial Intelligence, pp. 199–218. Springer,
US (2009). https://doi.org/10.1007/978-0-387-98197-0 10
9. Eiter, T., Leone, N., Sacca, D.: On the partial semantics for disjunctive deductive
databases. Ann. Math. Artif. Intell. 19(1–2), 59–96 (1997)
10. Lloyd, J.W.: Foundations of Logic Programming; (2nd extended ed.). Springer-
Verlag, New York Inc., New York (1987). https://doi.org/10.1007/978-3-642-
83189-8
11. Przymusinski, T.: The well-founded semantics coincides with the three-valued sta-
ble semantics. Fundamenta Informaticae 13(4), 445–463 (1990)
12. Przymusinski, T.C.: Stable semantics for disjunctive programs. New Gener. Com-
put. 9, 401–424 (1991)
13. S´a, S., Alcˆantara, J.: Interpretations and models for assumption-based argumenta-
tion. In: Proceedings of the 34rd Annual ACM Symposium on Applied Computing.
SAC ’19, ACM, New York, NY, USA (2019). https://doi.org/10.1145/3297280.
329739
14. Schulz, C., Toni, F.: Complete assumption labellings. In: Computational Models of
Argument - Proceedings of COMMA 2014, Atholl Palace Hotel, Scottish Highlands,
UK, 9–12 September 2014, pp. 405–412 (2014). https://doi.org/10.3233/978-1-
61499-436-7-405
15. Toni, F.: A tutorial on assumption-based argumentation. Argum. Comput. 5(1),
89–117 (2014)
16. Wu, Y., Caminada, M., Gabbay, D.: Complete extensions in argumentation coin-
cide with 3-valued stable models in logic programming. Studia Logica 93(1–2),
383–403 (2009). Special issue: new ideas in argumentation theory

A Paraconsistent Approach to Deal
with Epistemic Inconsistencies
in Argumentation
Rafael Silva(B)
and Jo˜ao Alcˆantara
Department of Computer Science, Federal University of Cear´a, Fortaleza, Brazil
{rafaels,jnando}@lia.ufc.br
Abstract. We introduce in ASPIC + languages an interrogation mark
? as a plausibility operator to enhance any defeasible conclusion do not
have the same status as an irrefutable one. The resulting framework,
dubbed ASPIC ?, is tailored to make a distinction between strong incon-
sistencies and weak inconsistencies. The aim is to avoid the former and to
tolerate the latter. This means the extensions obtained from the ASPIC ?
framework are free of strong conﬂicts, but tolerant to weak conﬂicts.
As a collateral eﬀect, the application of the Ex Falso principle can be
prevented in ASPIC ? when the last rule employed in the construction
of those arguments with symmetric conﬂicting conclusions is defeasible.
We then show ASPIC ? preserves current results on the satisfaction of
consistency and logical closure properties.
Keywords: Argumentation · Paraconsistency · Conﬂict-tolerance
1
Introduction
As noticed in [1], contradictions can be considered under the mantle of many
points of views: as a consequence of the only correct description of a contradictory
world, as a temporary state of our knowledge, as the outcome of a particular
language which we have chosen to describe the world, as the result of conﬂicting
observational criteria, as the superposition of world-views, or as the result from
the best theories available at a given moment. Indeed, in [2], it is argued that
inconsistency is a natural companion to defeasible methods of reasoning and
that paraconsistency (the property of a logic admitting non-trivial inconsistent
theories) should play a role in the formalisation of these methods.
Given that much current work on structured argumentation [3–5] combines
strict and defeasible inference rules, unexpected results can arise when two argu-
ments based on defeasible rules have contradictory conclusions. This is partic-
ularly critical (see [6]) if the strict inference rules include the Ex Falso prin-
ciple (that an inconsistent set implies anything), because for any formula φ,
This research was partly ﬁnanced by FUNCAP.
c
⃝Springer Nature Switzerland AG 2021
J. Vejnarov´a and N. Wilson (Eds.): ECSQARU 2021, LNAI 12897, pp. 187–200, 2021.
https://doi.org/10.1007/978-3-030-86772-0_14

188
R. Silva and J. Alcˆantara
an argument concluding ¬φ can be constructed from these two arguments. As
consequence, any other argument is potentially under threat!
In order to solve this problem for ASPIC +, Wu [7] requires that for each
argument the set of conclusions of all its sub-arguments are classically consistent.
She shows that this solution works for a restricted version of ASPIC + without
preferences, but gives counterexamples to the consistency postulates for the case
with preferences. Another approach was taken in [6] by Grooters and Prakken,
in which they replace classical logic as the source for strict rules by the (weaker)
paraconsistent logic presented in [8] to invalidate the Ex Falso principle as a
valid strict inference rule. Then, they also showed under which conditions, this
version of ASPIC + satisﬁes the consistency and closure postulates of [9].
In this paper, we exploit how to avoid the application of the Ex Falso prin-
ciple in ASPIC + [4] when those arguments with inconsistent conclusions are
based on defeasible rules. In the Inconsistent Default Logic [2], they introduced
an interrogation mark ? as a plausibility operator to enhance any defeasible
conclusion do not have the same status as an irrefutable one, obtained from
deduction. Inspired by these ideas, we will impose any defeasible rule will be of
the form φ1, . . . , φn ⇒φ?. Our intention is the conclusion of φ? will not neces-
sarily prevent the conclusion of ¬φ?; it is required an argument with conclusion
¬φ, which can only be constructed from a strict rule, to attack the conclusion
¬φ? of an argument. Thus, to produce a (let us call) strong conﬂict between the
conclusions of the arguments, at least in one of them, the conclusion should be
a ?-free formula obtained via a strict rule.
In our argumentation framework, dubbed ASPIC ?, we distinguish between
strong inconsistencies as in {φ, ¬φ} (or {φ?, ¬φ}) from weak inconsistencies as
in {φ?, ¬φ?}: the ﬁrst should be avoided; the second can be tolerated. This
means the (weak) conﬂict between φ? and ¬φ? can be accommodated in the
same extension. Hence, the extensions in ASPIC ? will be free of strong conﬂicts,
but tolerant to weak conﬂicts. In addition, as we isolate strong conﬂicts from
the weak ones, we can easily adapt the proofs in [9] to show the rationality
postulates described there hold also for ASPIC ?.
One could argue why we need to change ASPIC +: can the same objectives
be achieved by changing the logical language used in ASPIC + in a suitable way?
Roughly speaking, ASPIC ? is ASPIC + with the requisite the conclusion of any
defeasible rule should be an ?-suﬃxed formula and the base logic tolerant to weak
inconsistencies. Thus a mere change in the base logic is not enough to impose
defeasible and irrefutable conclusions should have diﬀerent epistemic status.
The rest of the paper is organised as follows: in Sect. 2, ASPIC ? framework is
presented. Then, we introduce the corresponding argumentation framework with
two kinds of attacks (strong and weak) and its semantics. Section 3 is focused
on proving the satisfaction of the rationality postulates described in [9]. In the
next section, we resort to examples to discuss the role played by ? operator and
how it can be employed to avoid unexpected extensions. Finally, we summarise
our contributions and themes for future developments.

A Paraconsistent Approach for Epistemic Inconsistencies in Argumentation
189
2
The ASPIC ? Framework
An Abstract Argumentation Framework AF [10] is a pair (A, D) in which A
is a set of arguments and D ⊆A × A is a relation of defeat. An argument
A defeats B if (A, B) ∈D. The ASPIC + framework [4,11] gives structure to
the arguments and defeat relation in an AF. In this section, we introduce in
ASPIC + languages an interrogation mark ? as a plausibility operator to enhance
defeasible conclusions do not have the same status as those irrefutable. The
resulting framework, ASPIC ?, is tailored to distinguish strong inconsistencies
from weak inconsistencies. The aim is to avoid the former and to tolerate the
latter. We start by deﬁning the argumentation systems speciﬁed by ASPIC ?:
Deﬁnition 1 (Argumentation System). An argumentation system is a tuple
AS = (L,−, R, n), in which
– L = L∗∪L? is a logical language with a unary negation symbol ¬ and a unary
plausibility symbol ? such that
• L∗is a ?-free logical language with a unary negation symbol.
• L? = {φ? | φ ∈L∗}.
– −is a function from L to 2L, such that
• ϕ is a contrary of ψ if ϕ ∈ψ, ψ ̸∈ϕ;
• ϕ is a contradictory of ψ (denoted by ϕ = −ψ), if ϕ ∈ψ, ψ ∈ϕ;
– R = Rs ∪Rd is a set of strict (Rs) and defeasible (Rd) inference rules of the
form φ1, . . . , φn →φ and φ1, . . . , φn ⇒ψ? respectively (in which φ1, . . . , φn, φ
are meta-variables ranging over wﬀin L and ψ is a meta-variable ranging over
wﬀin L∗), and Rs ∩Rd = ∅.
– n is a partial function such that n : Rd −→L.
For any formula φ ∈L∗, we say ψ ∈−φ if ψ = ¬φ or ψ = ¬φ? or φ = ¬ψ or
(φ = ¬γ and ψ = γ?); we say ψ ∈−φ? if ψ = ¬φ or φ = ¬ψ.
Note for any φ ∈L∗, φ and φ? are contradictories of ¬φ; whilst, only φ
is a contradictory of ¬φ?. This means φ? is not a contradictory of ¬φ?. A set
as {φ, −φ} (or {φ?, −φ?}) is intended to represent a strong inconsistency, and
{φ?, ¬φ?} is intended to represent a weak inconsistency. We will refer to these
two kinds of inconsistencies (strong and weak) as epistemic inconsistencies or
simply inconsistencies.
It is also required a knowledge base to provide premises for the arguments.
Deﬁnition 2 (Knowledge Base). A knowledge base in an argumentation sys-
tem AS = (L,−, R, n) is a set K ⊆L consisting of two disjoint subsets Kn (the
axioms) and Kp (the ordinary premises).
Axioms are certain knowledge and cannot be attacked, whilst, ordinary
premises are uncertain and can be attacked. Now we can deﬁne an argumen-
tation theory:

190
R. Silva and J. Alcˆantara
Deﬁnition 3. An argumentation theory (AS, K) is a pair in which AS is an
argumentation system and K is a knowledge base in AS.
In ASPIC ?, arguments are are constructed recursively from an argumenta-
tion theory by the successive application of construction rules:
Deﬁnition 4 (Argument). An argument A on the basis of an argumentation
theory (AS, K) and an argumentation system (L,−, R, n) is
1. φ if φ ∈K with Prem(A) = {φ}, Conc(A) = φ, Sub(A) = {φ}, DefR(A) = ∅,
Rules(A) = ∅, TopRule(A) = undeﬁned.
2. A1, . . . , An →ψ if A1, . . . , An are arguments s. t. there is a strict rule
Conc(A1), . . . , Conc(An) →ψ ∈Rs; Prem(A) = Prem(A1) ∪· · · ∪Prem(An);
Conc(A) = ψ; Sub(A) = Sub(A1) ∪· · · ∪Sub(An) ∪{A}; Rules(A) =
Rules(A1)∪· · ·∪Rules(An)∪{Conc(A1), . . . , Conc(An) →ψ}; TopRule(A) =
Conc(A1), . . . , Conc(An) →ψ.
3. A1, . . . , An ⇒ψ? if A1, . . . , An are arguments such that there exists a defea-
sible rule Conc(A1), . . . , Conc(An) ⇒ψ? ∈Rd; Prem(A) = Prem(A1) ∪
· · · ∪Prem(An); Conc(A) = ψ?; Sub(A) = Sub(A1) ∪· · · ∪Sub(An) ∪{A};
Rules(A) = Rules(A1) ∪· · · ∪Rules(An) ∪{Conc(A1), . . . , Conc(An) ⇒ψ?};
TopRule(A) = Conc(A1), . . . , Conc(An) ⇒ψ.
For any argument A we deﬁne Premn(A) = Prem(A) ∩Kn; Premp(A) =
Prem(A) ∩Kp;
DefR(A)
=
{r ∈Rd | r ∈Rules(A)}
and
StR(A)
=
{r ∈Rs | r ∈Rules(A)}.
Example 1. Consider the argumentation system AS = (L,−, R, n), in which
– L = L∗∪L? with L∗= {a, b, f, w, ¬a, ¬b, ¬f, ¬w, ∼a, ∼b, ∼f, ∼w, ∼¬a, ∼
¬b, ∼¬f, ∼¬w}. The symbols ¬ and ∼respectively denote strong and weak
negation.
– For any φ ∈L∗and any ψ ∈L, (1) φ ∈ψ iﬀ(a) ψ = ¬φ or ψ = ¬φ? or
φ = ¬ψ or (φ = ¬γ and ψ = γ?); or (b) ψ =∼φ or (ψ =∼φ?. (2) φ? ∈ψ iﬀ
(a) ψ = ¬φ or φ = ¬ψ; or (b) ψ =∼φ.
– Rs = {¬f →¬w; b →a} and Rd = {a ⇒¬f?; b, ∼¬w ⇒w?; ¬f? ⇒¬w?}.
Let K be the knowledge base such that Kn = ∅and Kp = {b, ∼¬w}. The
arguments deﬁned on the basis of K and AS are A1 = [b], A2 = [∼¬w], A3 =
[A1 →a], A4 = [A3 ⇒¬f?], A5 = [A1, A2 ⇒w?] and A6 = [A4 ⇒¬w?].
An argument A is for φ if Conc(A) = φ; it is strict if DefR(A) = ∅; defeasible if
DefR(A) ̸= ∅; ﬁrm if Prem(A) ⊆Kn; plausible if Prem(A)∩Kp ̸= ∅. An argument
is fallible if it is defeasible or plausible and infallible otherwise. We write S ⊢φ
if there is a strict argument for φ with all premises taken from S, and S |∼φ if
there is a defeasible argument for φ with all premises taken from S.
In the sequel, we introduce the c-consistent (contradictory consistent) sets:

A Paraconsistent Approach for Epistemic Inconsistencies in Argumentation
191
Deﬁnition 5 (c-consistency). A set S ⊆L is c-consistent if for no φ it is the
case that S ⊢φ and S ⊢−φ. Otherwise S is c-inconsistent. We say S ⊆L is
minimally c-inconsistent iﬀS is c-inconsistent and ∀S′ ⊂S, S′ is c-consistent.
An argument A is c-consistent iﬀPrem(A) is c-consistent.
The notion of c-inconsistency is what we mean by strong inconsistency.
2.1
Attacks and Defeats
In ASPIC ? arguments are related to each other by attacks (as in ASPIC +) and
by weak attacks:
Deﬁnition 6 (Attacks). Consider the arguments A and B. We say A attacks
B iﬀA undercuts, undermines and rebuts B, in which
– A undercuts B (on B′) iﬀConc(A) ∈n(r) for some B′ ∈Sub(B) such that
B′’s top rule r is defeasible.
– A undermines B (on φ) iﬀConc(A) ∈φ and φ ∈Premp(B). In such a case,
A contrary-undermines B iﬀConc(A) is a contrary of φ.
– A rebuts B (on B′) iﬀConc(A) ∈φ? for some B′ ∈Sub(B) of the form
B′′
1 , . . . , B′′
n ⇒φ?. In such a case, A contrary-rebuts B iﬀConc(A) is a con-
trary of φ?.
We say A weakly attacks B iﬀA weakly undermines or weakly rebuts B, in which
– A weakly undermines B (on φ? (resp. ¬φ?)) iﬀConc(A) = ¬φ? (resp.
Conc(A) = φ?) for an ordinary premise φ? (resp. ¬φ?) of B.
– A weakly rebuts B (on B′) iﬀConc(A) = ¬φ? (resp. Conc(A) = φ?) for some
B′ ∈Sub(B) of the form B′′
1 , . . . , B′′
n ⇒φ? (resp. B′′
1 , . . . , B′′
n ⇒¬φ?).
Example 2. Recalling Example 1, we have A5 weakly rebuts A6 and A6 weakly
rebuts A5. Besides, A6 contrary-undermines A2 and A5 on ∼¬w. If in addition,
one had the argument A7 = [A4 →¬w?], then A7 (like A6) would weakly rebut
A5 on A5; however, A7 (unlike A6) would not be weakly rebutted by A5.
Deﬁnition 7. A (c-)structured argumentation framework ((c-)SAF) deﬁned by
an argumentation theory AT is a tuple ⟨A, C, C′, ⪯⟩, in which
– In a SAF (resp. c-SAF), A is the set of all arguments (resp. c-consistent
arguments) constructed from K in ASPIC+ satisfying Deﬁnition 4;
– (X, Y ) ∈C iﬀX attacks Y and (X, Y ) ∈C′ iﬀX weakly attacks Y ;
– ⪯is a preference ordering on A.
It is clear a c-SAF is a SAF in which all arguments are required to have a
c-consistent set of premises. Next, we deﬁne the corresponding defeat relation:
Deﬁnition 8 (Defeat). Let A, B ∈A and A attacks B. If A undercut,
contrary-rebut or contrary-undermine attacks B on B′ then A is said to
preference-independent attack B on B′; otherwise A is said to preference-
dependent attack B on B′. A defeats B iﬀfor some B′ either A preference-
independent attacks B on B′ or A preference-dependent attacks B on B′ and
A ̸≺B′.

192
R. Silva and J. Alcˆantara
2.2
Abstract Argumentation Frameworks with Two Kinds of
Defeats
As (c-)SAFs have two kinds of attacks, the associated abstract argumentation
frameworks have to couple not only with attacks, but also with weak attacks.
Deﬁnition 9 (Argumentation frameworks with two kinds of attacks).
An abstract argumentation framework with two kinds of attacks (AF 2) cor-
responding to a c-SAF = ⟨A, C, C′, ⪯⟩is a tuple (A, D, D′) such that D =
{(X, Y ) ∈C | X defeats Y } and D′ = {(X, Y ) ∈C′ | X ̸≺Y }.
Example 3 (Example 2 continued).
Let ⪯= {(A6, A2)} (i.e., A6 ≺A2) be a
preference ordering on A = {A1, A2, A3, A4, A5, A6}. In the c-SAF (A, C, C′, ⪯)
deﬁned by AT, we have C = {(A6, A2)} and C′ = {(A5, A6), (A6, A5)}. As
(A6, A2) is a preference independent attack, we obtain D = C and D′ = C′.
Traditional approaches to argumentation semantics ensure conﬂicting are not
tolerated in the same set, which is said to be conﬂict free. In ASPIC ?, the aim
is to avoid strong conﬂicts, which are carried over by the defeat relation D, and
to tolerate weak conﬂicts, which are carried over by the relation D′.
Deﬁnition 10 (Compatible sets). Let (A, D, D′) be an AF 2 and S ⊆A. A
set S is compatible if ∀A ∈S, there is no B ∈S such that (B, A) ∈D.
Compatible sets do not contain the arguments A and B if A attacks B (or vice
versa); however they can accommodate weak attacks between their members.
This means in Example 3, the set {A5, A6} is compatible, but {A2, A6} is not.
Now we are entitled to do extend AF semantics to deal with compatible sets:
Deﬁnition 11 (Semantics). Let (A, D, D′) be an AF 2 and S ⊆A be a com-
patible set of arguments. Then X ∈A is acceptable with respect to S iﬀ
– ∀Y ∈A such that (Y, X) ∈D : ∃Z ∈S such that (Z, Y ) ∈D and
– ∀Y ∈A such that (Y, X) ∈D′ : ∃Z ∈S such that (Z, Y ) ∈D ∪D′.
1) S is an admissible set iﬀS is compatible and X ∈S implies X is acceptable
w.r.t. S; 2) S is a complete extension iﬀS is admissible and if X ∈A is
acceptable w.r.t. S then X ∈S; 3) S is a preferred extension iﬀit is a set
inclusion maximal complete extension; 4) S is the grounded extension iﬀit is
the set inclusion minimal complete extension; 5) S is a stable extension iﬀS is
complete extension and ∀Y ̸∈S, ∃X ∈S s.t. (X, Y ) ∈D ∪D′. 6) S is a semi-
stable extension iﬀit is a complete extension such that there is no complete
extension S1 such that S ∪S+ ⊂S1 ∪S+
1 .
Notice for an argument X to be acceptable w.r.t. S, if (Y, X) ∈D, there
should exist an argument Z ∈S such that (Z, Y ) ∈D, i.e., a weak defeat as
(Z, Y ) ∈D′ is not robust enough to defend a defeat as (Y, X) ∈D. Otherwise,
X defends a weak defeat (Y, X) ∈D′ if ∃Z ∈S such that Z (weak) defeats Y .

A Paraconsistent Approach for Epistemic Inconsistencies in Argumentation
193
Example 4 (Example 3 continued).
Regarding the AF 2 constructed in Example 3, we obtain the following results:
– Complete Extensions: {A1, A3, A4}, {A1, A3, A4, A5}, {A1, A3, A4, A6}, {A1,
A3, A4, A5, A6};
– Grounded Extension: {A1, A3, A4};
– Preferred Extension: {A1, A3, A4, A5, A6}
– Stable/Semi-stable Extensions: {A1, A3, A4, A6}, {A1, A3, A4, A5, A6}
3
Rationality Postulates
Caminada and Amgoud [9] proposed four postulates to constraint on any exten-
sion of an argumentation framework corresponding to an argumentation theory.
It is shown in [4,11] under which conditions these postulates hold in ASPIC +.
In the sequel, we show ASPIC ? under the same conditions also satisﬁes all of
them in Theorems 1,2, 3 and 4. Before let us characterise some notions:
Deﬁnition 12. Consider the argumention system (L,−, R, n). For any S ⊆
L, let the closure of S under strict rules, denoted ClRs(S), be the smallest set
containing S and the consequent of any strict rule in Rs whose antecedents are
in ClRs(S). Then, 1) a set S ⊆L is directly consistent iﬀ̸ ∃ψ, ϕ ∈S such that
ψ ∈ϕ; 2) indirectly consistent iﬀClRs(S) is directly consistent.
We proceed by constraining our attention to well deﬁned (c-)SAFs:
Deﬁnition 13 (Well deﬁned c-SAF). Let AT = (AS, K) be an argumentation
theory, where AS = (L,−, R, n). We say that AT is
– closed under contraposition iﬀfor all S ⊆L, s ∈S and φ, if S ⊢φ, then
S\{s} ∪{−φ} ⊢−s.
– closed under transposition iﬀif φ1, . . . , φn →ψ ∈Rs, then for i = 1 . . . n,
φ1, . . . , φi−1, −ψ, φi+1, . . . , φn →−φi ∈Rs;
– axiom consistent iﬀClRs(Kn) is consistent.
– c-classical iﬀfor any minimal c-inconsistent S ⊆L and for any ϕ ∈S, it
holds that S\{ϕ} ⊢−ϕ (i.e., amongst all arguments deﬁned there exists a
strict argument with conclusion −ϕ with all premises taken from S\{ϕ}).
– well formed if whenever ϕ is a contrary of ψ then ψ ̸∈Kn and ψ is not the
consequent of a strict rule.
A c-SAF is well deﬁned if it is deﬁned by an AT that is c-classical, axiom
consistent, well formed and closed under contraposition or closed under transpo-
sition. A SAF is well deﬁned if it is deﬁned by an AT that is axiom consistent,
well formed and closed under contraposition or closed under transposition.
To prove our results, we resort to the maximal fallible sub-arguments of B:
Deﬁnition 14. The set M(B) of the maximal fallible sub-arguments of B is
deﬁned such that for any B′ ∈Sub(B), B′ ∈M(B) iﬀ:

194
R. Silva and J. Alcˆantara
1. B′’s top rule is defeasible or B′ is an ordinary premise, and;
2. there is no B′′ ∈Sub(B) s.t. B′′ ̸= B and B′ ∈Sub(B′′), and B′′ satisﬁes 1.
The maximal fallible sub-arguments of B are those with the ‘last’ defeasible
inferences in B or else (if B is strict) they are B’s ordinary premises. We also
refer to an argument as a strict continuation of a set S of arguments if it is
obtainable by extending S only with strict rules:
Deﬁnition 15 (Strict continuations of arguments). For any set of argu-
ments S = {A1, . . . , An}, the argument A is a strict continuation of S iﬀ
– Premp(A) = n
i=1 Premp(Ai);
– DefR(A) = n
i=1 DefR(Ai);
– StR(A) ⊇n
i=1 StR(Ai) and Premn(A) ⊇n
i=1 Premn(Ai).
In addition, ⪯should satisfy properties that one might expect to hold of
orderings over arguments composed from fallible and infallible elements.
Deﬁnition 16 (Reasonable Argument Ordering). An argument ordering
⪯is reasonable iﬀ
1. (a) ∀A, B, if A is strict and ﬁrm and B is plausible or defeasible, then B ≺A;
(b) ∀A, B, if B is strict and ﬁrm then B ̸≺A;
(c) ∀A, A′, B such that A′ is a strict continuation of {A}, if A ̸≺B then
A′ ̸≺B, and if B ̸≺A then B ̸≺A′.
2. Let {C1, . . . , Cn} be a ﬁnite subset of A, and for i = 1 . . . n, let C+\i be some
strict continuation of {C1, . . . , Ci−1, Ci+1, . . . , Cn}. Then it is not the case
that: ∀i, C+\i ≺Ci.
From now on we will assume any (c-)SAF (A, C, C′, ⪯) is well deﬁned and ⪯is
reasonable to prove ASPIC ? satisﬁes the postulates of [9]. For space restrictions,
the simplest proofs of some intermediary results have been omitted.
Proposition 1. Let (A, C, C′, ⪯) be a (c-)SAF and (A, D, D′) be the correspond-
ing AF 2, A and B are in A such that A and B have contradictory conclusions,
B is plausible or defeasible with a strict top rule, and assume Prem(A)∪Prem(B)
is c-consistent if A and B are deﬁned as in Deﬁnition 5. Then
1. For all B′ ∈M(B), there exists a strict continuation A+
B′ of (M(B)\{B′}) ∪
M(A) such that A+
B′ rebuts or undermines B on B′.
2. If B ≺A, and ⪯is reasonable, then for some B′ ∈M(B), (A+
B′, B) ∈D.
The following lemma provide us with some intermediary results required to
show that our approach satisﬁes the rationality postulates of [9]:
Lemma 1. Let (A, D, D′) be a AF 2 and A ∈A.
1. If A is acceptable w.r.t. S ⊆A then A is acceptable w.r.t. any superset of S.

A Paraconsistent Approach for Epistemic Inconsistencies in Argumentation
195
2. If (A, B) ∈D, then (A, B′) ∈D for some B′ ∈Sub(B), and if (A, B′) ∈D,
B′ ∈Sub(B), then (A, B) ∈D.
3. If (A, B) ∈D′, then (A, B′) ∈D′ for some B′ ∈Sub(B), and if (A, B′) ∈D′,
B′ ∈Sub(B), then (A, B) ∈D′.
4. If A is acceptable w.r.t. S ⊆A, A′ ∈Sub(A), then A′ is acceptable w.r.t. S.
The next result comes from the fact that if B defeats a strict continuation
A of {A1, . . . , An}, then B defeats A on some Ai ∈{A1, . . . , An}. This lemma
is employed to prove Theorem 2 and Lemma 4.
Lemma 2. Let (A, D, D′) be a AF 2 corresponding to a (c-)SAF (A, C, C′, ⪯).
Let A ∈A be a strict continuation of {A1, . . . , An} ⊆A, and for i = 1 . . . n, Ai
is acceptable w.r.t. E ⊆A. Then A is acceptable w.r.t. E.
The next lemma is employed to prove Proposition 3.
Lemma 3. Let A be acceptable w.r.t. an admissible extension S ⊆A of an AF 2
(A, D, D′) corresponding to a (c-)SAF (A, C, C′, ⪯). Then ∀B ∈S ∪{A}, neither
(A, B) ∈D nor (B, A) ∈D.
For the following proposition, recall that by assumption, any c-SAF is well
deﬁned and so satisﬁes c-classicality (Deﬁnition 13).
Proposition 2. Let C = (A, C, C′, ⪯) be a c-SAF. If A1, . . . , An are acceptable
w.r.t. some compatible E ⊆A, then n
i=1 Prem(Ai) is c-consistent.
The next proposition shows that if an argument A is acceptable w.r.t. to an
set S of arguments, then S ∪{A} is compatible.
Proposition 3. Let A ∈A be acceptable w.r.t. an admissible extension S of
an AF 2 (A, D, D′) corresponding to a well deﬁned (c-)SAF (A, C, C′, ⪯). Then,
S′ = S ∪{A} is compatible.
Theorems 1, 2, 3, and 4 show ASPIC ? satisﬁes the postulates of [9]:
Theorem 1 (Sub-argument closure). Let △= (A, C, C, ⪯) be a (c-)SAF
and E a complete extension of △. Then ∀A ∈E : if A′ ∈Sub(A) then A′ ∈E.
Proof. According to Lemma 1 (item 3), A′ is acceptable w.r.t. E, and E ∪{A′}
is compatible (Proposition 3). As E is a complete extension, A′ ∈E.
Theorem 2 (Closure under strict rules). Let △= (A, C, C′, ⪯) be a
(c-)SAF and E a complete extension of △. Then {Conc(A) | A ∈E} =
ClRs({Conc(A) | A ∈E}).
Proof. We will show for any strict continuation X of {A | A ∈E}, X ∈E. Any
such X is acceptable with relation to E (Lemma 2), and E ∪{X} is compatible
(Proposition 3). Then X ∈E as E is a complete extension. If △is a c-SAF,
Proposition 2 guarantees Prem(X) is c-consistent.

196
R. Silva and J. Alcˆantara
The following lemma is employed to prove Theorem 3.
Lemma 4. Let △= (A, C, C′, ⪯) be a (c-)SAF, E an admissible extension of
△, and ∃Y ∈E such that Y is defeasible or plausible and has a strict top rule.
Then ∀X ∈E, it holds Conc(X) ̸∈Conc(Y ).
Proof. By absurd, suppose ∃X ∈E s.t. Conc(X) ∈Conc(Y ). Given Y is defeasi-
ble or plausible and has a strict top rule, by the well-formed assumption Conc(X)
and Conc(Y ) must be contradictory. If △is a c-SAF, and X, Y ∈E, X, Y are
acceptable w.r.t. E, and so Prem(A) ∪Prem(B) is c-consistent (Proposition 2).
Now, we consider two cases: 1) If Y ̸≺X, then (Y, X) ∈D: an absurd as E is
compatible. 2) If Y ≺X, then from Proposition 1 (item 1) for all Y ′ ∈M(Y ),
there is a strict continuation X+
Y ′ of M(Y )\{Y ′} ∪M(X) s.t. (X+
Y ′, Y ) ∈C, and
from Proposition 1 (item 2) (X+
Y ′, Y ) ∈D. By Lemma 2 X+
Y ′ is acceptable w.r.t.
E, and by Proposition 3, E ∪{X+
Y ′} is compatible, contradicting (X+
Y ′, Y ) ∈D.
Theorem 3 (Direct consistency). Let △= (A, C, C′, ⪯) be a (c-)SAF and
E an admissible extension of △. Then {Conc(A) | A ∈E} is consistent.
Proof. Suppose by absurd A, B ∈E, and Conc(A) ∈Conc(B) (i.e., E is incon-
sistent (Deﬁnition 12)).
– If A is ﬁrm and strict, and 1) B is strict and ﬁrm. It is an absurd as it contra-
dicts the assumption of axiom consistency (Deﬁnition 13). 2) B is plausible
or defeasible, and B is an ordinary premise or has a defeasible top rule. We
have (A, B) ∈D, which is an absurd since E is compatible. Note B cannot
have a strict top rule according to Lemma 4.
– A is plausible or defeasible, and
• B is strict and ﬁrm. Under the well-formed assumption (Deﬁnition 13)
Conc(A) cannot be a contrary of Conc(B). Thus, Conc(A) and Conc(B)
are a contradictory of each other, and A is an ordinary premise or has a
defeasible top rule; in both cases (B, A) ∈D, contradicting E is compat-
ible. By Lemma 4, A cannot have a strict top rule.
• if B is plausible or defeasible and B is an ordinary premise or has a defea-
sible top rule. Then (A, B) ∈D if Conc(A) is the contrary of Conc(B);
otherwise, Conc(A) and Conc(B) are contradictory and as consequence,
(A, B) ∈D or (B, A) ∈D. In both cases, it is an absurd as E is compat-
ible. Again by Lemma 4, B cannot have a strict top rule.
Theorem 4 (Indirect consistency). Let △= (A, C, ⪯) be a (c-)SAF and E
an complete extension of △. Then ClRS({Conc(A) | A ∈E}) is consistent.
Proof. It follows from Theorems 2 and 3.

A Paraconsistent Approach for Epistemic Inconsistencies in Argumentation
197
4
Related Work and Discussion
Over the years, diverse proposals have been presented to handle paraconsis-
tent argumentation or conﬂict-tolerant semantics [6,12–19]. In [12,13], Arieli
extended both the extension-based labelling-based approaches to admit conﬂict-
tolerant semantics. In particular, he resorted to a four-valued labelling: in,
out, both, and none. As usual, an argument is labelled in/out if it is accept-
able/unacceptable. An argument A is labelled both when there exists both sup-
portive and opposing evidence A, and A is labelled none when there is not enough
evidence supporting or against A. In [14–16], Arieli proposed the sequent based
argumentation framework to accommodate diﬀerent types of languages, includ-
ing paraconsistent logics. In [17,18], Ben-Nain and Amgoud defended ranking
semantics for paraconsistent argumentation, in which arguments are ranked from
the most to the less acceptable according to the number of attacks to them.
In this paper, we contribute to the debate by showing that not all conﬂicts
have the same nature. In ASPIC ?, we introduced the plausibility operator ?
to distinguish strong inconsistencies from those weak. Whereas the resulting
extensions are free of strong conﬂicts, they are tolerant to weak conﬂicts. We
illustrate the rationale behind it in the example below:
Example 5. The following example is adapted from [20]: let AS be the argumen-
tation system of Example 1, in which a = “it is an animal”, b = “it is a bird”,
f = “it ﬂies” and w = “it has wings”. The rules of Rd are “if it is an animal
and assuming that it is not winged, then it normally cannot ﬂy”; and “if it is a
bird, then it normally has wings” (Rd = {a, ∼f?, ∼w ⇒¬f?; b, ∼¬w? ⇒w?}).
Recall Rs is constituted by the rules “if it is a bird, then it is an animal” and
“if it does not ﬂy, then it has not wings”, i.e., Rs = {b →a; ¬f →¬w}.
The knowledge base K is given by Kn = ∅and Kp = {b, ∼f?, ∼w, ∼¬w?}.
The resulting arguments are A1 = [b], A2 = [∼f?], A3 = [∼w], A4 = [∼¬w?],
A5 = [A1 →a], A6 = [A5, A2, A3 ⇒¬f?] and A7 = [A1, A4 ⇒w?]. We
know A7 preference-independent attacks both A6 and A3, and they are the only
available attacks. Thus in the corresponding AF 2 = ({A1, . . . , A7} , D, D′), we
have D = {(A7, A6), (A7, A3)} and D′ = ∅. Its unique complete extension is
{A1, A2, A4, A5, A7}, and from A7, we conclude it is plausible it has wings.
In the defeasible rule a, ∼f?, ∼w ⇒¬f?, note the idea of normal/usual is
enhanced by ∼f?. This means an argument whose conclusion is f is demanded
to attack it. By its turn, the exception of this rule is expressed by ∼w. Thus
an argument with a less demanding attack (one with a plausible conclusion w?)
suﬃces. In contradistinction, owing to the defeasible rule b, ∼¬w? ⇒w?, in
order to preference-independent attack an argument for w? (A7), we have to
construct an argument for ¬w. This is not possible here, because an argument
for ¬f? (A6) is not robust enough to construct an argument for ¬w.
Motivated by [20], one can argue any complete extension in which it is an
animal, but a wingless bird and unable to ﬂy is anomalous. Furthermore, if the
defeasible rules were as in ASPIC + [4] (Rd = {a, ∼f, ∼w ⇒¬f; b, ∼¬w ⇒w})
and Kp = {b, ∼f, ∼w, ∼¬w}; instead of A2, A4, A6 and A7, we would have

198
R. Silva and J. Alcˆantara
respectively
A′
2
=
[∼
f],
A′
4
=
[∼
¬w],
A′
6
=
[A5, A′
2, A3
⇒
¬f]
and
A′
7
=
[A1, A′
4
⇒
w].
We
would
also
have
A′
8
=
[A′
6
→
¬w] and AF
=
({A1, A′
2, A3, A′
4, A5, A′
6, A′
7, A′
8}, D) with D
=
{(A′
7, A′
8), (A′
7, A′
6), (A′
7, A3), (A′
8, A′
7), (A′
8, A′
4)}. Its complete extensions are
{A1, A′
2, A5}, {A1, A′
2, A′
4, A5, A′
7} and the anomalous {A1, A′
2, A3, A5, A′
6, A′
8},
in which it is an animal (A5), but a wingless bird (A′
8) and unable to ﬂy (A′
6).
Note one can remove extraneous elements from the example and include a pref-
erence relation over the arguments to avoid this unintended extension. Our point
here, however, is our proposal is more informative and ﬂexible to deal with these
subtleties inherent to knowledge representation.
In [6,19], Grooters and Prakken approached the trivialisation problem in
ASPIC + in the presence of inconsistent conclusions. We illustrate this problem
with the following example in ASPIC +:
Example 6 [6]. Let Rd = {p ⇒q; r ⇒¬q; t ⇒s}, Kp = ∅and Kn = {p, r, t},
while Rs consists of all propositionally valid inferences. The corresponding AF
includes the arguments A1 = [p], A2 = [A1 ⇒q], B1 = [r], B2 = [B1 ⇒¬q],
C = [A2, B2 →¬s], D1 = [t] and D2 = [D1 ⇒s]. We have C defeats D2 if
C ̸≺D2. This is problematic as s can be any formula. Hence, any defeasible
argument unrelated to A2 or B2 can, depending on ⪯, be defeated by C owing
to the explosiveness of classical logic as the source for Rs. In [6,19], to solve such
a problem, classical logic was replaced by the paraconsistent logic W [8].
In ASPIC ?, we circumvent this problem by preventing the defeasible rules
of producing strong inconsistencies as their conclusions are all ?-suﬃxed. Thus,
Rd above would be {p ⇒q?; r ⇒¬q?; t ⇒s?} in ASPIC ?. Let Rs be the set of
all propositionally valid inferences of a monotonic paraconsistent logic as LEI
[2], which do not have an explosive behaviour for weak inconsistencies. Then,
the problematic argument C is no more obtained and D2 is no more defeated.
5
Conclusion and Future Works
Inconsistency may be present in knowledge bases for many reasons, some of
which are evolving information, and merging information from multiple sources.
In the literature, argumentation is frequently referred to as a natural approach
to dealing with inconsistency. In this work, we presented ASPIC ? by introducing
in ASPIC + [4] languages an interrogation mark ? as an operator to enhance any
defeasible conclusion do not have the same status than an irrefutable one.
As in [2], we make a distinction between the strong contradiction as in {φ, ¬φ}
(or {φ?, ¬φ}) and the weak contradiction as in {φ?, ¬φ?}. We avoid the former
and tolerate the latter. This means the extensions generated from the ASPIC ?
framework will be strong conﬂict-free, but weak conﬂict-tolerant. In the sequel,
we also showed the rationality postulates described in [9] hold also for ASPIC ?.
Future developments encompass to investigate which monotonic paraconsis-
tent logics can be employed as source of strict rules to prevent trivialisation

A Paraconsistent Approach for Epistemic Inconsistencies in Argumentation
199
in ASPIC ? when two defeasible arguments with inconsistent conclusions are
available while preserving the rationality postulates of [9]. Another venture is
to explore how ASPIC ? improves computational models of arguments to distin-
guish strong inconsistencies from those weak, and in which applications ASPIC ?
may enable some improvements when compared to other approaches such as
ASPIC +. We also intend to study the relation between ASPIC ? and the works
presented in [21–23].
References
1. Carnielli, W., Marcos, J.: A taxonomy of c-systems. In: Paraconsistency, pp. 24–
117. CRC Press, Boca Raton (2002)
2. Pequeno, T., Buchsbaum, A.: The logic of epistemic inconsistency. In: Proceedings
of the Second International Conference on Principles of Knowledge Representation
and Reasoning, pp. 453–460 (1991)
3. Gorogiannis, N., Hunter, A.: Instantiating abstract argumentation with classical
logic arguments: postulates and properties. Artif. Intell. 175(9–10), 1479–1497
(2011)
4. Modgil, S., Prakken, H.: A general account of argumentation with preferences.
Artif. Intell. 195, 361–397 (2013)
5. Caminada, M., Modgil, S., Oren, N.: Preferences and unrestricted rebut. Compu-
tational Models of Argument (2014)
6. Grooters, D., Prakken, H.: Combining paraconsistent logic with argumentation. In
COMMA, pp. 301–312 (2014)
7. Wu, Y.: Between argument and conclusion-argument-based approaches to discus-
sion, inference and uncertainty. PhD thesis, University of Luxembourg (2012)
8. Rescher, N., Manor, R.: On inference from inconsistent premisses. Theory Decis.
1(2), 179–217 (1970)
9. Caminada, M., Amgoud, L.: On the evaluation of argumentation formalisms. Artif.
Intell. 171(5–6), 286–310 (2007)
10. Dung, P.: On the acceptability of arguments and its fundamental role in non-
monotonic reasoning, logic programming and n-person games. Artif. Intell. 77(2),
321–357 (1995)
11. Prakken, H.: An abstract framework for argumentation with structured arguments.
Argum. Comput. 1(2), 93–124 (2010)
12. Arieli, O.: Conﬂict-tolerant semantics for argumentation frameworks. In: del Cerro,
L.F., Herzig, A., Mengin, J. (eds.) JELIA 2012. LNCS (LNAI), vol. 7519, pp. 28–40.
Springer, Heidelberg (2012). https://doi.org/10.1007/978-3-642-33353-8 3
13. Arieli, O.: Conﬂict-free and conﬂict-tolerant semantics for constrained argumenta-
tion frameworks. J. Appl. Logic 13(4), 582–604 (2015)
14. Arieli, O., Straßer, C.: Sequent-based logical argumentation. Argum. Comput. 6(1),
73–99 (2015)
15. Arieli, O., Straßer, C.: Logical argumentation by dynamic proof systems. Theor.
Comput. Sci. 781, 63–91 (2019)
16. Borg, A., Straßer, C., Arieli, O.: A generalized proof-theoretic approach to logical
argumentation based on hypersequents. Studia Logica 109(1), 167–238 (2021)
17. Amgoud, L., Ben-Naim, J.: Ranking-based semantics for argumentation frame-
works. In: Liu, W., Subrahmanian, V.S., Wijsen, J. (eds.) SUM 2013. LNCS
(LNAI), vol. 8078, pp. 134–147. Springer, Heidelberg (2013). https://doi.org/10.
1007/978-3-642-40381-1 11

200
R. Silva and J. Alcˆantara
18. Ben-Naim, J.: Argumentation-based paraconsistent logics. In: Hernandez, N.,
J¨aschke, R., Croitoru, M. (eds.) ICCS 2014. LNCS (LNAI), vol. 8577, pp. 19–24.
Springer, Cham (2014). https://doi.org/10.1007/978-3-319-08389-6 2
19. Grooters, D., Prakken, H.: Two aspects of relevance in structured argumentation:
minimality and paraconsistency. J. Artif. Intell. Res. 56, 197–245 (2016)
20. Morris, P.H.: The anomalous extension problem in default reasoning. Artif. Intell.
35(3), 383–399 (1988)
21. Dung, P., Thang, P., Son, T.C.: On structured argumentation with conditional
preferences. In: Proceedings of the AAAI Conference on Artiﬁcial Intelligence, vol.
33, pp. 2792–2800 (2019)
22. Dung, P., Thang, P.: Fundamental properties of attack relations in structured
argumentation with priorities. Artif. Intell. 255, 1–42 (2018)
23. Dung, P.: An axiomatic analysis of structured argumentation with priorities. Artif.
Intell. 231, 107–150 (2016)

Gradual Semantics for Weighted Bipolar
SETAFs
Bruno Yun1
and Srdjan Vesic2(B)
1 University of Aberdeen, Aberdeen, Scotland
bruno.yun@abdn.ac.uk
2 CNRS, Univ. Artois, CRIL – Centre de Recherche en Informatique de Lens,
62300 Lens, France
vesic@cril.fr
Abstract. Gradual semantics are now well-studied in the computa-
tional argumentation literature. In this paper, we argue that gradual
semantics that can handle both bipolarity (e.g. attacks and supports)
and sets of attacking arguments (i.e. several arguments together attack-
ing an argument) might be useful in some contexts. We deﬁne the formal
framework and properties for such semantics. We proceed by adapting,
studying and implementing three well-known semantics from the bipolar
gradual literature to this new framework.
Keywords: Argumentation · Gradual semantics
1
Introduction
Recently, gradual argumentation semantics have drawn the attention of many
scholars [1,2,6,8,11,14,16,17]. Some of the existing approaches for gradual
semantics allow for intrinsic weights of arguments [2], some for intrinsic weights
of attacks [4], some for sets of attacking arguments [22], and some for bipolarity,
i.e. two types of interactions between argument are allowed: attacks and sup-
ports [1,3,7,17]. One might need all of those features (weights on arguments
and attacks, bipolarity and sets of attacking/supporting arguments) to model
some situations in an intuitive manner, as illustrated by Fig. 1, which is inspired
from the example by Prakken [18]. We do not claim that our example cannot be
modelled by simpler frameworks, but we believe that our framework allows to
represent it in a natural way.
First, the intrinsic weights of the arguments might represent the conﬁdence
level of the source or the plausibility of the argument. For instance, the argument
“hot” might have a greater intrinsic weight if the outside temperature is 40◦
Celsius than if the outside temperature is 30◦.
Srdjan Vesic was supported by “Responsible AI” ANR Chair in Artiﬁcial Intelligence,
https://ia-responsable.eu/.
c
⃝Springer Nature Switzerland AG 2021
J. Vejnarov´a and N. Wilson (Eds.): ECSQARU 2021, LNAI 12897, pp. 201–214, 2021.
https://doi.org/10.1007/978-3-030-86772-0_15

202
B. Yun and S. Vesic
Fig. 1. Representation of the jogging example. The solid red lines represent sets of
attacking arguments whereas the dashed lines represent sets of supporting arguments.
(Color ﬁgure online)
Second, the intrinsic weights on attacks represent how much arguments attack
another argument. For instance, hot weather may be a good reason for cancelling
a jogging whereas the rain might be less important. In Fig. 1, this is represented
by having a greater intrinsic weight on the attack coming from the argument
“hot” (0.8) than “rain” (0.5).
Lastly, an agent might have diﬀerent inclinations toward how the weather
aﬀects their jogging routine. For the sake of the example, let us model three
possibilities. The simplest case is the one where both the hot weather and the
rain are arguments against going for a jogging but there is no interaction between
them (left part of Fig. 1). The second case is the one where both the hot weather
and the rain are still arguments against going for a jogging but the combination
of the hot weather and the rain is a desirable reason to go for a jogging (center
part of Fig. 1). The third case is the one where the two factors (the hot weather
and the rain) may interact together and represent a further reason against going
for a jogging (right part of Fig. 1).
This latter case is modelled using an argumentation framework, known in the
literature, under the name of set of attacking arguments framework (SETAF).
Please note that to model all three possibilities, we also need supports (in addi-
tion to attacks) to model positive and negative eﬀects of temperature and precip-
itation on jogging. The reader is invited to observe the intricate link with the idea
of argument accrual, which is well studied in the literature [5,9,10,12,18,19,21].
Note however, that arguments accrual was not studied for gradual semantics nor
was adapted to sets of attacking argument frameworks.
The contributions of this paper are: (1) a new argumentation framework
with sets of attacking/supporting arguments with intrinsic weights on relations,
(2) the deﬁnition of properties for this framework and (3) the deﬁnition and
characterisation of bipolar gradual semantics tailored for this framework.
The paper is structured as follows. We start by presenting the background
notions and a list of desirable properties that a gradual semantics for weighted
bipolar SETAFs should satisfy. We then show some links between those proper-
ties (e.g. some properties imply others). We continue by adapting three seman-
tics from the literature (namely, Euler-based, DF-Quad and Sigmoid damped
max-based semantics) to our framework and execute a formal study of their

Gradual Semantics for Weighted Bipolar SETAFs
203
properties. We also provide a JAVA implementation for several semantics and
extend the existing ASPARTIX format to the case of weighted bipolar SETAFs.
2
Formal Setting
We ﬁrst extend the argumentation framework proposed by [15] with sets of
supporting arguments which allow arguments to jointly support an argument.
Deﬁnition 1. A weighted bipolar SETAF (WBSETAF) is AS = ⟨A, P, w⟩,
where A is a ﬁnite set of arguments, P ⊆(2A \ ∅) × A × {att, supp} is a set
of collective interactions (each interaction being either an attack or a support),
and w is a function from A ∪P to [0, 1].
The set of all WBSETAFs is denoted by Xsetaf. The set of non-maximal
WBSETAFs is deﬁned as X <1
setaf = {⟨A, P, w⟩∈Xsetaf | for every b ∈A, w(b) <
1} and the set of non-minimal WBSETAFs is deﬁned as X >0
setaf = {⟨A, P, w⟩∈
Xsetaf |
for every b ∈A, w(b) > 0}. (X, j, att) ∈P means that there is an
attack from X ∈(2A \ ∅) to j ∈A whereas (X, j, supp) ∈P means that there is
a support from X to j. Please note that in our formal setting, we can have both
an attack and a support from the same set of arguments toward an argument
but all the properties in this paper still hold in the restricted case, where there is
only one type of interaction (support or attack) between a set of arguments and
an argument. If a ∈A, we denote by w(a) the intrinsic weight of argument a rep-
resenting its trustworthiness or the certainty degree of the argument’s premises.
For p ∈P, we denote by w(p) the intrinsic weight of the attack/support p. The
set of attackers of a ∈A is Att(a) = {X ∈2A | (X, a, att) ∈P}. Likewise, the
set of supports of a ∈A is Supp(a) = {X ∈2A | (X, a, supp) ∈P}. The union
of two WBSETAFs AS = ⟨A, P, w⟩and AS′ = ⟨A′, P′, w′⟩s.t. A ∩A′ = ∅is
AS ⊕AS′ = ⟨A ∪A′, P ∪P′, w′′⟩, where ∀a ∈A ∪A′ ∪P ∪P′, w′′(a) = w(a), if
a ∈A ∪P and w′′(a) = w′(a), if a ∈A′ ∪P′.
A path is a sequence of sets of arguments that are linked with either sets of
attacking or supporting arguments.
Deﬁnition 2 (Path). A sequence ⟨X0, X1, . . . , Xn⟩is called a path in AS =
⟨A, P, w⟩iﬀ∀i ∈{0, . . . , n −1}, ∃x ∈Xi+1 s.t. (Xi, x, l) ∈P, where l ∈
{att, supp}.
A path is signiﬁcant w.r.t. a WBSETAF AS = ⟨A, P, w⟩if it is, roughly
speaking, composed of attacks/supports having strictly positive weights.
Deﬁnition 3 (Signiﬁcant path). A path ⟨X0, X1, . . . , Xn⟩in AS = ⟨A, P, w⟩
is signiﬁcant if and only if ∀i ∈{0, . . . , n −1}, ∃x ∈Xi+1 s.t. (Xi, x, l) ∈P and
w((Xi, x, l)) > 0, where l ∈{att, supp}.
We can now deﬁne the notion of cycle.
Deﬁnition 4 (Cycle). A path ⟨X0, X1, . . . , Xn⟩is called a cycle in AS =
⟨A, P, w⟩iﬀ∃x ∈X0 s.t. (Xn, x, l) ∈P, where l ∈{att, supp}.

204
B. Yun and S. Vesic
A semantics is a function assigning an overall strength from [0, 1] to each
argument of the WBSETAF.
Deﬁnition 5 (Semantics). Let AS = ⟨A, P, w⟩be a weighed bipolar SETAF.
A semantics is a function σ transforming any AS ∈Xsetaf into a function
σAS : A →[0, 1].
When AS is clear from the context, we simply write σ(a) instead of σAS(a).
Deﬁnition 6 (Isomorphism). Let AS = ⟨A, P, w⟩and AS′ = ⟨A′, P′, w′⟩
be two WBSETAFs. We say that f is an isomorphism from A to A′ w.r.t.
AS and AS′ iﬀall the following conditions are satisﬁed: (1) ∀X ⊆A, a ∈A,
z ∈{att, supp}, (X, a, z) ∈P iﬀ(f(X), f(a), z) ∈P′, (2) ∀X ⊆A, a ∈A and
z ∈{att, supp} s.t. (X, a, z) ∈P, w((X, a, z)) = w′((f(X), f(a), z)) and (3)
∀a ∈A, w(a) = w′(f(a))
3
Desirable Properties
We now extend the desirable properties that a semantics should satisfy [1] for
the more general case of WBSETAFs. In each set of attacking (resp. support-
ing) arguments, all the components are necessary. Thus, we follow an important
intuition from the literature [22] (reﬂected in our new properties) which is that
the strength of a set of arguments should be equal to the strength of the weak-
est argument. Of course, this is not the only intuition and other aggregation
functions can be used without loss of generality.
The ﬁrst property states that the overall strength returned by the semantics
should only be computed based on the structural elements.
Property 1 (Anonymity). A semantics σ satisﬁes anonymity iﬀfor any two
WBSETAFs AS = ⟨A, P, w⟩and AS′ = ⟨A′, P′, w′⟩, for any isomorphism f
from A to A′ w.r.t. AS and AS′, it holds that ∀a ∈A, σAS(a) = σAS′(f(a)).
The independence property states that overall strength of an argument should
not be aﬀected by any other arguments that are not connected to it.
Property 2 (Independence). A semantics σ satisﬁes independence iﬀfor any two
WBSETAFs AS = ⟨A, P, w⟩and AS′ = ⟨A′, P′, w′⟩s.t. A ∩A′ = ∅, it holds
that ∀a ∈A, σAS(a) = σAS⊕AS′(a).
The directionality property states that the overall strength of an argument
should depend only on the signiﬁcant paths directed to it.
Property 3 (Directionality). A semantics σ satisﬁes directionality iﬀfor any two
WBSETAFs AS = ⟨A, P, w⟩and AS′ = ⟨A, P′, w′⟩s.t. all the following condi-
tions are satisﬁed:
– ∀y ∈A ∪P, w(y) = w′(y),

Gradual Semantics for Weighted Bipolar SETAFs
205
– ∀x, b ∈A, ∀X ⊆A, if P′ = P ∪{(X, b, z)}, where z ∈{att, supp} and there
is no X′ ⊆A s.t. b ∈X′ and there is a signiﬁcant path from X′ to x,
then σAS(x) = σAS′(x).
The equivalence property states that the overall strength of an argument
depends only on the overall strength of its direct attackers and supporters and
the weight of the corresponding relations.
Property 4 (Equivalence). A semantics σ satisﬁes equivalence iﬀfor every
WBSETAF AS = ⟨A, P, w⟩and ∀a, b ∈A, if all the following conditions are
satisﬁed:
– w(a) = w(b),
– there exists a bijection f between Att(a) and Att(b) s.t. ∀X ∈Att(a),
min
x∈X σAS(x) =
min
x′∈f(X) σAS(x′) and w((X, a, att)) = w((f(X), b, att)),
– there exists a bijection f between Supp(a) and Supp(b) s.t. ∀X ∈Supp(a),
min
x∈X σAS(x) =
min
x′∈f(X) σAS(x′) and w((X, a, supp)) = w((f(X), b, supp)),
then σAS(a) = σAS(b).
The reinforcement property states that an argument becomes stronger if the
quality of its attackers is reduced or the quality of its supporters is increased.
Property 5 (Reinforcement). A semantics σ satisﬁes reinforcement iﬀfor every
WBSETAF AS = ⟨A, P, w⟩, ∀C, C′ ⊆2A, ∀a, b ∈A and ∀X1, X2, Y1, Y2 ⊆A
s.t. Xi, Yi /∈C ∪C′, if all the following conditions are satisﬁed:
– w(a) = w(b), Att(a) = C ∪{X1} and Att(b) = C ∪{Y1},
– Supp(a) = C′ ∪{X2} and Supp(b) = C′ ∪{Y2},
–
min
x1∈X1 σAS(x1) ≤min
y1∈Y1 σAS(y1),
– w((X1, a, att)) ≤w((Y1, a, att)),
–
min
x2∈X2 σAS(x2) ≥min
y2∈Y2 σAS(y2),
– w((X2, a, supp)) ≥w((Y2, a, supp)),
then σAS(a) ≥σAS(b).
The stability property states that if an argument is neither attacked nor
supported, then its overall strength should be equal to its intrinsic weight.
Property 6 (Stability). A semantics σ satisﬁes stability iﬀ, for every WBSETAF
AS = ⟨A, P, w⟩, for every argument a ∈A, if Att(a) = Supp(a) = ∅, then
σAS(a) = w(a).
In the original work of [1], it is assumed that the neutral overall strength
is the minimum possible value, which is zero. The intuition is that the “worth-
less” arguments, i.e. those with the strength zero, do not contain any valuable
information and thus should not impact the other arguments. In their initial

206
B. Yun and S. Vesic
work, [13] generalised the existing approaches by allowing for the neutral overall
strength to be any value. In the later paper [14], the authors changed the neutral
overall strength to be zero but left the overall strength value to be any interval.
In our paper, we choose the overall strength of all semantics to be in [0, 1] to
align with the literature standards. As a result, the neutral overall strength is
chosen in the aforementioned interval.
The neutrality property states that the overall strength of an argument
should not be aﬀected by sets of attacking/supporting arguments containing
arguments with an overall strength equal to the neutral overall strength.
Property 7 (Neutrality). A semantics σ satisﬁes neutrality iﬀ, there exists a
unique α ∈[0, 1] s.t. both of the following conditions are satisﬁed:
– there exists a WBSETAF AS = ⟨A, P, w⟩and a ∈A s.t. σAS(a) = α,
– for every WBSETAF AS = ⟨A, P, w⟩, for every argument a, b ∈A s.t. w(a) =
w(b), X ⊆A s.t. Att(a) ⊆Att(b), Supp(a) ⊆Supp(b) and Att(a) ∪Supp(a) ∪
{X} = Att(b) ∪Supp(b), if min
x∈X σAS(x) = α or w((X, b, z)) = 0, where z ∈
{att, supp},
then σAS(a) = σAS(b).
The ﬁrst item of the neutrality property is necessary because if the semantics
σ never assigns an overall strength β ∈[0, 1] to an argument then neutrality is
trivially satisﬁed by putting α = β (under the condition that whenever the
intrinsic weight of an attack/support is equal to zero, it does not aﬀect the
overall strength of its target). If σ satisﬁes the neutrality property, α is called
the neutral overall strength.
The monotonicity property states that if two arguments a and b have the
same intrinsic weight and a is “less attacked” and “more supported” than b,
then a should have a higher overall strength than b.
Property 8 (Monotonicity). A semantics σ satisﬁes monotonicity iﬀ, for every
WBSETAF AS = ⟨A, P, w⟩, ∀a, b ∈A s.t. w(a) = w(b), Att(a) ⊆Att(b) and
∀X ∈Att(a), w((X, a, att)) ≤w((X, b, att)), Supp(b) ⊆Supp(a) and ∀X ∈
Supp(b), w((X, b, supp)) ≤w((X, a, supp)), then σAS(a) ≥σAS(b).
The resilience property states that if the intrinsic weight of an argument is
not 0 nor 1 then its overall strength cannot be 0 nor 1.
Property 9 (Resilience).
A
semantics
σ
satisﬁes
resilience
iﬀ,
for
every
WBSETAF AS = ⟨A, P, w⟩, ∀a ∈A, if 0 < w(a) < 1 then 0 < σAS(a) < 1.
Franklin states that an attacker and a supporter of equal strength should
counter-balance each other. Thus, the overall strength should remain unchanged.
Property 10 (Franklin). A semantics σ satisﬁes Franklin iﬀ, for every WBSETAF
AS = ⟨A, P, w⟩, ∀a, b ∈A and X, Y ⊆A, if all of the following conditions are
satisﬁed:

Gradual Semantics for Weighted Bipolar SETAFs
207
– w(a) = w(b), w((X, a, att)) = w((Y, a, supp)),
– Att(a) = Att(b) ∪{X}, Supp(a) = Supp(b) ∪{Y },
– min
x∈X σAS(x) = min
y∈Y σAS(y),
then σAS(a) = σAS(b).
The strengthening property states that if an argument receives more supports
than attacks, then its overall strength should increase. Roughly speaking, there
are three cases: (1) there are more supporters than attackers, (2) the weakest
strength of a set attack is strictly weaker than the weakest strength of the corre-
sponding supporter, (3) the intrinsic weight of one attack is strictly weaker than
the corresponding support.
Property 11 (Strengthening). A semantics σ satisﬁes strengthening iﬀ, for every
WBSETAF AS = ⟨A, P, w⟩, ∀a ∈A, if w(a) < 1 and there exists an
injective function f from Att(a) to Supp(a) s.t. ∀X ∈Att(a), min
x∈X σAS(x) ≤
min
x′∈f(X) σAS(x′), ∀X ∈Att(a), w((X, a, att)) ≤w((f(X), a, supp)) and at least
one of the following conditions is satisﬁed:
– there exists X′ ∈Supp(a)\{f(X) | X ∈Att(a)} such that w((X′, a, supp)) >
0 and min
x′∈X′ σAS(x′) > 0,
– there exists X ∈Att(a) such that min
x∈X σAS(x) <
min
x′∈f(X) σAS(x′)
and
w((f(X), a, supp)) > 0, or
– there exists X ∈Att(a) such that w((X, a, att)) < w((f(X), a, supp)) and
min
x′∈f(X) σAS(x′) > 0,
then σAS(a) > w(a).
Similarly, the weakening property states that if an argument receives less
supports than attacks, then its overall strength should decrease.
Property 12 (Weakening). A semantics σ satisﬁes weakening iﬀ, for every
WBSETAF AS = ⟨A, P, w⟩, ∀a ∈A, if 0 < w(a) and there exists an injec-
tive function f from Supp(a) to Att(a) s.t. ∀X ∈Supp(a), min
x∈X σAS(x) ≤
min
x′∈f(X) σAS(x′), ∀X ∈Supp(a), w((X, a, supp)) ≤w((f(X), a, att)) and at least
one of the following conditions is satisﬁed:
– there exists X′ ∈Att(a)\{f(X) | X ∈Supp(a)} such that w((X′, a, att)) > 0
and min
x′∈X′ σAS(x′) > 0,
– there exists X ∈Supp(a) such that min
x∈X σAS(x) <
min
x′∈f(X) σAS(x′) and
w((f(X), a, att)) > 0, or
– there exists X ∈Supp(a) such that w((X, a, supp)) < w((f(X), a, att)) and
min
x′∈f(X) σAS(x′) > 0,
then σAS(a) < w(a).

208
B. Yun and S. Vesic
The next property states that adding an attack to an argument should
decrease its overall strength whereas adding a support should increase it. The
idea of accumulation is similar to that of monotonicity except that monotonicity
is deﬁned on two arguments of the same graph whereas accumulation is deﬁned
on the “same” argument in the “copy” of the graph. Note that in the next
section, we identify the conditions under which monotonicity and accumulation
are equivalent (Corollary 1).
Property 13 (Accumulation). A semantics σ satisﬁes accumulation iﬀ, for every
two WBSETAFs AS = ⟨A, P, w⟩and AS′ = ⟨A, P′, w′⟩, ∀a ∈A, ∀X ∈2A, if
∀p ∈A ∪P, we have w(p) = w′(p), and P′ = P ∪{(X, a, z)} then (1) σAS′(a) ≤
σAS(a) if z = att and (2) σAS′(a) ≥σAS(a) if z = supp.
4
Links Between Properties
We now study the link between the properties that we previously deﬁned.
Proposition 1. Let σ be a semantics that satisﬁes independence, directionality,
stability, accumulation. Then, σ satisﬁes monotonicity.
Proof. Let AS = ⟨A, P, w⟩and a, b ∈A s.t. the conditions of monotonicity are
satisﬁed. The intuition of the next construction is the following. We will deﬁne
a “copy” AS′ of AS. For each argument x, we create its copy x′. Furthermore,
we keep only the attacks from the direct attackers of a and b and only the
supports from the direct supporters of a and b. All the other attacks and supports
are forgotten. This is done to allow for the application of directionality when
comparing AS′ and AS1 (which will be deﬁned later in the proof).
Let us deﬁne AS′ = ⟨A′, P′, w′⟩s.t. there exists a bijection f : Att(b) ∪
Supp(a) ∪{a, b} →A′ deﬁned as ∀x, f(x) = x′ and:
– A′ = {a′, b′}∪{x′
i | xi ∈X and X ∈Att(b)}∪{y′
i | yi ∈Y and Y ∈Supp(a)}.
– P′ = {(f(X), f(a), att) such that (X, a, att) ∈P} ∪{(f(X), f(b), att) such
that (X, b, att) ∈P} ∪{(f(Y ), f(a), supp) such that (Y, a, supp) ∈P} ∪
{(f(Y ), f(b), supp) such that (Y, b, supp) ∈P}.
– Let us deﬁne w′ as follows: w′(f(a)) = w(a), w′(f(b)) = w(b), ∀x ∈A′ \
{a′, b′}, w′(x) = σAS(x) and ∀(T, t, z) ∈Att(b) ∪Supp(a), let
w′((f(T), f(t), z)) = w((T, t, z)).
From stability, we know that ∀t′ ∈A′ \ {a′, b′}, we have σAS′(t′) = w′(t′) =
σAS(t). By using the independence and equivalence properties, we have that
σAS(a) = σAS′(a′) and σAS(b) = σAS′(b′). Let us use the notation Att(a′) =
{X′
1, . . . , X′
n}, Att(b′) = Att(a′)∪{X′
n+1, . . . , X′
m}, Supp(b′) = {Y ′
1, . . . , Y ′
k} and
Supp(a′) = Supp(b′) ∪{Y ′
k+1, . . . , Y ′
l }.
Let us now remove attacks from Att(b′) \ Att(a′) to b′ and supports from
Supp(a′) \ Supp(b′) to a′ using the following procedures.

Gradual Semantics for Weighted Bipolar SETAFs
209
– Let AS1 = ⟨A′, P1, w1⟩with P1 = P′ \ {(X′
m, b′, att)} and ∀p ∈A′ ∪
P1, w1(p) = w′(p). By accumulation, we have that σAS1(b′) ≥σAS′(b′). More-
over, by directionality, ∀t ∈A′, σAS1(t) = σAS′(t). We repeat this procedure
m −n times to remove all the attacks from Att(b′) \ Att(a′) to b′. Let us call
the resulting graph ASm−n.
– Let us deﬁne ASm−n+1 = ⟨A′, Pm−n+1, wm−n+1⟩with Pm−n+1 = Pm−n \
{(Y ′
l , a′, supp)} and ∀p ∈A′∪Pm−n+1, wm−n+1(p) = w′(p). By accumulation,
we have that σASm−n+1(a′) ≤σASm−n(a′). Moreover, by directionality, ∀t ∈
A′, σASm−n(t) = σASm−n+1(t). We repeat this procedure l−k times to remove
all the supports from Supp(a′)\Supp(b′) to a′. Let us call the resulting graph
AS∗.
By equivalence, we have that σAS∗(a) = σAS∗(b). Thus:
σAS′(b′) ≤σAS1(b′) ≤· · · ≤σAS∗(b′) =
σAS∗(a′) ≤· · · ≤σAS1(a′) ≤σAS′(a′)
Finally, since we have that σAS(b) = σAS′(b′) and σAS(a) = σAS′(a′), we
conclude that σAS(a) ≥σAS(b).
Regarding the previous result, the proposition holds for an arbitrary seman-
tics; note also that if σ is a semantics that is deﬁned only on acyclic graphs, we
can show that σ satisﬁes monotonocity (on the class of acyclic graphs).
Proposition 2. Let σ be a semantics that satisﬁes anonymity, independence,
directionality, equivalence and monotonicity on the class of acyclic graphs. Then,
σ satisﬁes accumulation on the class of acyclic graphs.
Proposition 1 also holds for a semantics σ deﬁned on acyclic graphs only.
Hence, from that fact and Proposition 2, we conclude that accumulation and
monotonicity are equivalent given anonymity, independence, directionality, sta-
bility and equivalence. We formally state this in the next corollary.
Corollary 1. Let σ be a semantics that satisﬁes anonymity, independence,
directionality, stability and equivalence on the class of acyclic graphs. Then σ
satisﬁes accumulation on the class of acyclic graphs iﬀit satisﬁes monotonicity
on the class of acyclic graphs.
5
Gradual Semantics
As mentioned before, in each set of attacking (resp. supporting) arguments, all
the components are necessary. Thus, removing one argument from the set would
make the attack (resp. support) void. In this section, we generalise the semantics
from the literature by considering the force of the set of attacking (resp. support-
ing) arguments to be the force of the weakest argument of the set. For simplicity,
we also incorporate the intrinsic weight of attacks by means of the multiplica-
tion operator but the whole approach can be generalised with other aggregating

210
B. Yun and S. Vesic
methods. Moreover, the ﬁrst two semantics (Euler-based and DF-Quad) do not
always converge in the class of cyclic WBSETAFs. For this reason, as usual in
the literature [1,20], we consider that the class of all acyclic WBSETAFs is still
expressive enough to deserve attention. In the next deﬁnitions, we generalise the
existing Euler-based [1] and DF-Quad semantics [20] for the new framework.
Deﬁnition 7 (Euler-based semantics). Let AS = ⟨A, P, w⟩∈Xsetaf be an
acyclic WBSETAF. ∀a ∈A, we have:
σEB
AS (a) = 1 −
1 −w(a)2
1 + w(a)eE(a)
where E(a) = Y −Z with Y =

X∈Supp(a)
min
x∈X σEB
AS (x) · w((X, a, supp)) and Z =

X∈Att(a)
min
x∈X σEB
AS (x) · w((X, a, att)).
Table 1. Satisfaction of the properties by the gradual semantics on acyclic graphs
Xsetaf
X >0
setaf
X <1
setaf
σEB σDF
σSD σEB σDF
σSD σEB σDF
σSD
Anonymity
✓
✓
✓
✓
✓
✓
✓
✓
✓
Independence
✓
✓
✓
✓
✓
✓
✓
✓
✓
Directionality
✓
✓
✓
✓
✓
✓
✓
✓
✓
Equivalence
✓
✓
✓
✓
✓
✓
✓
✓
✓
Reinforcement ✓
✓
✓
✓
✓
✓
✓
✓
✓
Stability
✓
✓
✓
✓
✓
✓
✓
✓
✓
Neutrality
✓
✓
✗
✗
✓
✗
✓
✓
✗
Monotonicity
✓
✓
✗
✓
✓
✗
✓
✓
✗
Resilience
✓
✗
✓
✓
✗
✓
✓
✓
✓
Franklin
✓
✗
✗
✓
✗
✗
✓
✗
✗
Strengthening
✗
✗
✗
✓
✗
✗
✗
✓
✗
Weakening
✗
✗
✗
✗
✗
✗
✓
✓
✗
Accumulation
✓
✓
✗
✓
✓
✗
✓
✓
✗
Deﬁnition 8 (DF-Quad semantics). Let AS = ⟨A, P, w⟩∈Xsetaf be an
acyclic WBSETAF. ∀a ∈A, we have: if vs(a) = va(a), σDF
AS (a) = w(a); else
σDF
AS (a) = w(a) + (0.5 +
vs(a)−va(a)
2·|vs(a)−va(a)| −w(a)) · |vs(a) −va(a)|,
where va(a) = 1 −

X∈Att(a)

1 −min
x∈X σDF
AS (x) · w((X, a, att))

and vs(a) =
1−

X∈Supp(a)

1 −min
x∈X σDF
AS (x) · w((X, a, supp))

. Please note that if Att(a) = ∅
then va = 0. Similarly, vs = 0 if Supp(a) = ∅.

Gradual Semantics for Weighted Bipolar SETAFs
211
We also generalise the Sigmoid damped max-based gradual semantics [14] for
this framework. For presentation purposes, we changed the original deﬁnition to
match the other semantics by translating the range of the semantics from [−1, 1]
to [0, 1]. We also changed the “top” operator from the original semantics to
“max” because we wanted to take all the arguments into account for determining
the overall strength. Note that the sets of attacking (resp. supporting) arguments
with a minimum element having a score strictly inferior to 0.5 will increase (resp.
decrease) the score of the attacked (resp. supported) argument. This eﬀect is an
intrinsic aspect of the original Sigmoid damped max-based semantics, which was
not introduced by the generalisation we propose.
Deﬁnition 9 (Sigmoid
damped
max-based
semantics).
Given
the
WBSETAF AS = ⟨A, P, w⟩∈Xsetaf and δ > 2. ∀a ∈A, we have σSD
AS (a) =
f

Y (a)
δ
+ f −1(w(a))

, where
Y (a) =
max
X∈Supp(a)

min
x∈X f −1(σSD
AS (x)) · w((X, a, supp))

−
max
X∈Att(a)

min
x∈X f −1(σSD
AS (x)) · w((X, a, att))

and f(x) = tanh(x)+1
2
.
In Table 1, we show the satisfaction of the properties by the aforementioned
gradual semantics on the three classes Xsetaf, X <1
setaf and X >0
setaf. Due to space
limitations, we do not show all the proofs in the paper although all the combi-
nations were diligently proved. We now provide a brief overview of the property
satisfaction on the general class Xsetaf.
The Euler-based semantics satisﬁes the greatest number of properties. The
only two non-satisﬁed properties are strengthening and weakening. Note that
the only case when an argument cannot be weakened (resp. strengthened) is the
case of an argument that has the maximal (resp. minimal) intrinsic weight. It
seems to us that in several reasonable application contexts, this behaviour of the
Euler-based semantics can be seen as rational.
The DF-Quad semantics violates strengthening and weakening as well as
Franklin and resilience. Again, this is not necessarily a fatal problem, since in
some contexts it might not be a good idea to be able to cancel one positive
argument with one negative argument of the same strength.
Finally, the Sigmoid damped max-based semantics violates even more prop-
erties, which is (we believe) linked to its simple idea to take into account only
the strongest attacker and the strongest supporter and to ignore the others. Note
that Sigmoid damped max-based semantics does not satisfy neutrality, even if
we use the generalised version of the proposition that allows for any number to
be the neutral overall strength. Similar to the original version by Mossakowski
and Neuhaus [13], we could restrict our version of Sigmoid damped max-based
semantics to arguments with intrinsic strengths above 0.5 only, which would
lead to satisfying neutrality. Notice that every property is satisﬁed by Sigmoid
damped max-based semantics on the class Xsetaf iﬀit is satisﬁed on X >0
setaf and
on X <1
setaf. The reason is that the Sigmoid damped max-based semantics is only
deﬁned in the interval (0, 1).

212
B. Yun and S. Vesic
Interestingly, we also showed that the new DF-Quad semantics does not
satisfy Franklin (see Example 1) and that the Euler-based semantics does not
satisfy strengthening on the class of non-maximal graphs even in the acyclic case
(see Example 2). Since neither of our examples uses set attacks, we conclude that
the original versions of those two semantics do not satisfy the aforementioned
properties, contrary to what was suggested by Amgoud and Ben-Naim [1].
Example 1. Let AS = ⟨A, P, w⟩s.t. A = {a, b, x1, y1, z}, P = {({z}, b, att),
({z}, a, att), ({y1}, a, supp), ({x1}, a, att)}, w(a) = w(b) = 0.5, w(x1) = w(y1) =
0.7, w(z) = 0.6 and w(({x1}, a, att)) = w(({z}, b, att)) = w(({z}, a, att)) =
w(({y1}, a, supp)) = 1, represented in Fig. 1. We have that vs(a) = 1−(1−0.7) =
0.7, va(a) = 1 −0.3 · 0.4 = 0.88, vs(b) = 0 and va(b) = 1 −(1 −0.6) = 0.6. As a
result, σDF
AS (a) = 1+(−0.5)·0.18 = 0.91 whereas σDF
AS (b) = 1+(−0.5)·0.6 = 0.7
(Fig. 2)
Fig. 2. Counter-example of the satisfaction of Franklin by DF-Quad.
Example 2. Let AS = ⟨A, P, w⟩s.t. A = {a, b}, P = {({b}, a, supp)}, w(a) =
0, w(b) = 0.5 and w(({b}, a, supp)) = 1. Then, σEB
AS (a) = w(a) = 0.
6
Discussion
In this paper, we studied gradual semantics for WBSETAFs. This is the ﬁrst
study of the properties that a gradual semantics should satisfy in the framework
that allows for bipolar interactions (i.e. both attacks and supports), weights (on
both arguments and attacks) and SETAFs (the possibility for several arguments
to jointly attack or support an argument). We generalised twelve properties from
the literature [1,14] and introduced a new one called “accumulation”. We proved
some links between the properties, and generalised three semantics from the lit-
erature to be applicable with WBSETAFs. We conducted a formal evaluation of
those semantics against the properties on three classes of argumentation graphs.
We also provided an implementation of the three new semantics available
at https://github.com/AnonymousConfsSubmissions/WBSETAFs. Please note
that this implementation is based on an extension of the ASPARTIX format for
WBSETAFs.

Gradual Semantics for Weighted Bipolar SETAFs
213
References
1. Amgoud, L., Ben-Naim, J.: Weighted bipolar argumentation graphs: axioms and
semantics. In: Proceedings of the Twenty-Seventh International Joint Conference
on Artiﬁcial Intelligence, IJCAI 2018 (2018)
2. Amgoud, L., Ben-Naim, J., Doder, D., Vesic, S.: Acceptability semantics for
weighted argumentation frameworks. In: Proceedings of the Twenty-Sixth Inter-
national Joint Conference on Artiﬁcial Intelligence, IJCAI 2017, pp. 56–62 (2017)
3. Amgoud, L., Cayrol, C., Lagasquie-Schiex, M.-C., Livet, P.: On bipolarity in argu-
mentation frameworks. Int. J. Intell. Syst. 23(10), 1062–1093 (2008)
4. Amgoud, L., Doder, D.: Gradual semantics accounting for varied-strength attacks.
In: Proceedings of the 18th International Conference on Autonomous Agents and
MultiAgent Systems, AAMAS 2019, pp. 1270–1278 (2019)
5. Bench-Capon, T.J.M., Prakken, H.: Justifying actions by accruing arguments.
Comput. Models Argument Proc. COMMA 2006, 247–258 (2006)
6. Bonzon, E., Delobelle, J., Konieczny, S., Maudet, N.: A comparative study of
ranking-based semantics for abstract argumentation. In: Proceedings of the Thir-
tieth AAAI Conference on Artiﬁcial Intelligence, pp. 914–920 (2016)
7. Cayrol, C., Lagasquie-Schiex, M.-C.: On the acceptability of arguments in bipolar
argumentation frameworks. In: Symbolic and Quantitative Approaches to Reason-
ing with Uncertainty, 8th European Conference, ECSQARU 2005, Proceedings,
pp. 378–389 (2005)
8. Leite, J., Martins, J.: Social abstract argumentation. In: IJCAI 2011, Proceedings
of the 22nd International Joint Conference on Artiﬁcial Intelligence, pp. 2287–2292
(2011)
9. Lucero, M.J.G., Ches˜nevar, C.I., Simari, G.R.: Modelling argument accrual in pos-
sibilistic defeasible logic programming. In: Symbolic and Quantitative Approaches
to Reasoning with Uncertainty, 10th European Conference, ECSQARU 2009, pp.
131–143 (2009)
10. Lucero, M.J.G., Ches˜nevar, C.I., Simari, G.R.: On the accrual of arguments in
defeasible logic programming. In: Proceedings of the 21st International Joint Con-
ference on Artiﬁcial Intelligence, IJCAI 2009, pp. 804–809 (2009)
11. Matt, P.-A., Toni, F.: A game-theoretic measure of argument strength for abstract
argumentation. In: Logics in Artiﬁcial Intelligence, 11th European Conference,
JELIA 2008, Proceedings, pp. 285–297 (2008)
12. Modgil, S., Bench-Capon, T.J.M.: Integrating dialectical and accrual modes of
argumentation. In: Computational Models of Argument: Proceedings of COMMA
2010, 8–10 September, 2010, pp. 335–346 (2010)
13. Mossakowski, T., Neuhaus, F.: Bipolar Weighted Argumentation Graphs. CoRR,
abs/1611.08572 (2016)
14. Mossakowski, T., Neuhaus, F.: Modular Semantics and Characteristics for Bipolar
Weighted Argumentation Graphs. CoRR, abs/1807.06685 (2018)
15. Nielsen, Søren Holbech., Parsons, Simon: A generalization of dung’s abstract frame-
work for argumentation: arguing with sets of attacking arguments. In: Maudet,
Nicolas, Parsons, Simon, Rahwan, Iyad (eds.) ArgMAS 2006. LNCS (LNAI), vol.
4766, pp. 54–73. Springer, Heidelberg (2007). https://doi.org/10.1007/978-3-540-
75526-5 4
16. C. d. C. Pereira, A. Tettamanzi, and S. Villata. Changing One’s Mind: Erase or
Rewind? In Proceedings of the 22nd International Joint Conference on Artiﬁcial
Intelligence, IJCAI 2011, pages 164–171, 2011

214
B. Yun and S. Vesic
17. Potyka, N.: Continuous dynamical systems for weighted bipolar argumentation.
In: Principles of Knowledge Representation and Reasoning: Proceedings of the
Sixteenth International Conference, KR 2018, pp. 148–157 (2018)
18. Prakken, H.: A study of accrual of arguments, with applications to evidential rea-
soning. Proceedings of the Tenth International Conference on Artiﬁcial Intelligence
and Law, ICAIL 2005, 85–94 (2005)
19. Prakken, H.: Modelling accrual of arguments in ASPIC+. In: Proceedings of the
Seventeenth International Conference on Artiﬁcial Intelligence and Law, ICAIL
2019, pp. 103–112 (2019)
20. Rago, A., Toni, F., Aurisicchio, M., Baroni, P.: Discontinuity-free decision support
with quantitative argumentation debates. In: Principles of Knowledge Represen-
tation and Reasoning: Proceedings of the Fifteenth International Conference, KR
2016, pp. 63–73 (2016)
21. Verheij, B.: Accrual of arguments in defeasible argumentation, pp. 217–224. In
Dutch/German Workshop on Nonmonotonic Reasoning. Proceedings of the Second
Workshop, Delft University of Technology, Universiteit Utrecht (1995)
22. Yun, B., Vesic, S., Croitoru, M.: Ranking-based semantics for sets of attacking argu-
ments. In: The Thirty-Fourth AAAI Conference on Artiﬁcial Intelligence, AAAI
2020, pp. 3033–3040 (2020)

Bayesian Networks and Graphical
Models

Multi-task Transfer Learning for Bayesian
Network Structures
Sarah Benikhlef1(B), Philippe Leray1
, Guillaume Raschia1
,
Montassar Ben Messaoud2, and Fayrouz Sakly2
1 LS2N, UMR CNRS 6004, University of Nantes, Nantes, France
{sarah.benikhlef,philippe.leray,guillaume.raschia}@ls2n.fr
2 LARODEC, ISG Sousse, Sousse, Tunisia
Abstract. We consider the interest of leveraging information between
related tasks for learning Bayesian network structures. We propose a new
algorithm called Multi-Task Max-Min Hill Climbing (MT-MMHC) that
combines ideas from transfer learning, multi-task learning, constraint-
based and search-and-score techniques. This approach consists in two
main phases. The ﬁrst one identiﬁes the most similar tasks and uses their
similarity to learn their corresponding undirected graphs. The second
one directs the edges with a Greedy Search combined with a Branch-and-
Bound algorithm. Empirical evaluation shows that MT-MMHC can yield
better results than learning the structures individually or than the state-
of-the-Art MT-GS algorithm in terms of structure learning accuracy and
computational time.
Keywords: Bayesian networks · Structure learning · Multi-task
learning · Transfer learning
1
Introduction
Learning reliable models from small datasets is diﬃcult, therefore transfer learn-
ing (also known as domain adaptation [21]) can enhance the robustness of the
discovered models by leveraging data from related tasks [12]. Transfer learning
is a well-studied area. It has been successfully employed in a variety of machine
learning ﬁelds, focusing mainly on neural networks [13,21].
Multi-Task (MT) learning, also referred to as parallel transfer learning, is a
learning paradigm that aims to leverage useful information contained in related
tasks to help improve the generalization of all the tasks [20]. In order to consid-
erably increase learning tasks performance, MT learning can be combined with
other learning mechanisms [20].
Inductive transfer and MT learning are closely related. They both intend to
leverage knowledge among similar problems [10]. The distinction between these
two approaches lies in the transfer technique. In Transfer Learning (TL), the
information is transferred from the source to the target task. Ultimately, the
goal is to improve the performance of the target task with the support of source
c
⃝Springer Nature Switzerland AG 2021
J. Vejnarov´a and N. Wilson (Eds.): ECSQARU 2021, LNAI 12897, pp. 217–228, 2021.
https://doi.org/10.1007/978-3-030-86772-0_16

218
S. Benikhlef et al.
tasks by aﬀording additional information. We note here that the target task
has a more signiﬁcant role than source tasks. On the contrary, the tasks in MT
learning are considered equally [17].
Bayesian Networks (BNs) have proven to be an eﬃcient tool to capture condi-
tional dependencies and independencies between random variables. They give an
eﬀective way to represent the structure of real-world applications and determine
the eﬀect of many observations on an outcome. They are frequently employed in
decision support systems and machine learning applications, where it is gener-
ally assumed to have suﬃcient data from which a reliable model can be learned.
However, in some ﬁelds, as manufacturing or medicine [7], data can be rare and
usually gathered from diﬀerent but closely related problems. In this situation,
existing solutions have been proposed for transfer and multi-task learning of
Bayesian networks [8,10,11]. These approaches are mainly adaptations of basic
constraint-based or score-based BN structure learning algorithms.
In this paper, we propose an extension to multi-task learning of an eﬃcient
BN structure discovery algorithm called Max-Min Hill-Climbing (MMHC) [18]
that takes advantage of both constraint based and score-based algorithms.
Our three main contributions are: (1) MT-MMHC, a hybrid transfer learning
algorithm that can learn multiple BN structures simultaneously by inducing
information between similar tasks to improve the performance of the constructed
networks; (2) one procedure to generate MT benchmarks from any reference
model, by controlling the similarity between tasks; and (3) one experimental
validation of MT-MMHC by using such benchmarks compared to Single Task
(ST) structure learning, but also to MT-GS reference algorithm (Multi Task
Greedy Search).
Section 2 introduces background information and relevant related work about
BN structure learning. Section 3 describes our MT-MMHC approach. Section 4 is
dedicated to the generation of MT benchmarks and to the empirical evaluation
of our proposition. Finally, Sect. 5 gives a general conclusion and several research
perspectives.
2
Bayesian Network Multi-task Structure Learning
A Bayesian Network (BN) B =< G, Θ > represents the joint probability distri-
bution of a set of n random variables {X1, X2, . . . , Xn} [4]. It is characterized
by a Directed Acyclic Graph (DAG) G and a set Θ of Conditional Probability
Tables (CPTs), also called parameters. The graph or structure G is given by
a pair (V, E) where V is the set of nodes in the graph and E the set of edges
between them. Each node corresponds to a random variable and each edge rep-
resents a direct dependency between the two variables that it connects. A strong
property of BNs is that this representation is easy to interpret and can help in
visualizing dependencies between variables.
BN Single-Task (ST) learning aims at discovering the structure of the network
G and estimating the parameters Θ of the model from one dataset D.
In Multi-Task (MT) learning, we consider k tasks corresponding to k datasets
D = {D1, D2, . . . , Dk} from which we learn k corresponding BN graphs G =

Multi-task Transfer Learning for Bayesian Network Structures
219
{G1, G2, ..., Gk} and the associated parameters. The objective is to learn all
the models simultaneously as a multi-task problem while leveraging information
between the tasks. It is worth to notice here that Azzimonti et al. [1] propose
an alternative BN learning approach where one common BN structure is learned
from related data sets.
MT parameter learning, when G is known, have been processed in the litera-
ture [4,8]. For instance, Luis et al. [8] present a method of inductive transfer to
determine the CPTs using aggregation functions from several sources.
In this paper, we are mainly interested in Bayesian network MT structure
learning. In a single task context, discovering the structure of a Bayesian network
can be seen as a problem of selecting a probabilistic model that ﬁts and explains
a given dataset [4]. Three main approaches are generally adopted [4] : (1) search
and score based methods exploring the space of structures; (2) constraint-based
methods using conditional independencies; and (3) hybrid methods combining
the two previous ones.
Search and Score Approaches. One of the most widely known methods of
learning Bayesian network structures is the use of search and score techniques [4].
These approaches perform an exact or a heuristic search within the space of
network structures and evaluate the best candidate structure to ﬁt the data
using a given scoring metric. Generally speaking, these algorithms require: (i) a
search space of allowable states of the problem, each state represents a Bayesian
network structure; (ii) a scoring function to evaluate a state and see how well it
matches the data; (iii) a mechanism to explore this space in an exact way when
the dimension of the search space is limited, or in a heuristic way.
Greedy Search (GS) [14], for example, is a ST structure learning algorithm
that starts from some initial structure and explores the DAG space by selecting
at each iteration the neighbor graph with the highest score. The search stops
when the current structure has a better score than all its neighbors, and may be
repeated several times starting from diﬀerent initial states to avoid local maxima.
A neighborhood of a structure G is commonly deﬁned as all the DAGs obtained
by removing or reversing an existing edge in G, or by adding a new one. For
states evaluation, several scores have been settled in the literature such as AIC,
BIC or BDeu [3] or more recent ones such as qNML [15].
In [10], Niculescu-Mizil and Caruana extend this score-based search in a MT
context. Let us denote this algorithm MT-GS. They consider a conﬁguration of
k structures {G1, G2, . . . , Gk} and then follow a greedy search procedure in the
space of DAGs to ﬁnd the k graphs that ﬁt the best the k respective datasets.
The neighborhood of a conﬁguration is deﬁned as the set of all conﬁgurations
obtained by considering all possible subsets of the graphs and applying an add,
remove or reverse operation to the same edge in each graph of each subset. The
score to maximise is the posterior probability of a conﬁguration given the data
deﬁned in Eq. 1.

220
S. Benikhlef et al.
P(G|D) = P(G1, . . . , Gk|D1, . . . , Dk) ∝P(G)
k

a=1
P(Da|Ga)
(1)
They propose two diﬀerent priors (i) an edit prior which considers the mini-
mum number of updates needed to make an edge similar in every structure and
(ii) a paired prior that considers the diﬀerences among each pair of structures
as deﬁned in Eq. 2.
P(G) = Zδ,k

1≤a≤k
P(Ga)
1
1+(k−1)δ

1≤a<b≤k
(1 −δ)
d(Ga,Gb)
k−1
(2)
where δ ∈[0, 1] is a parameter that penalizes every diﬀerence between the mod-
els’ structure when calculating the prior, Zδ,k is a normalization constant and
d(Ga, Gb) is the number of edges in the symmetric diﬀerence between Ga and
Gb. Thus, the scoring function takes into account data from all the tasks and
leverages information between them.
As the search space can get large for large k and n (the number of variables
in datasets), they provide a computational optimization based on a Branch-and-
Bound strategy to ﬁnd at each step the best conﬁguration in the neighborhood
of the current one. To this end, they deﬁne a partial conﬁguration of order
ℓ, Cℓ= (G1, . . . , Gℓ), as a conﬁguration where only the ﬁrst ℓstructures are
speciﬁed and the other k −ℓstructures are not. This exploration goes through
a search tree of depth k to reach the best scoring conﬁguration. At each level
ℓ< k, only the score of the partial conﬁguration Cℓ= (G1, . . . , Gℓ) is computed
and compared to the current best score. The score of a partial conﬁguration is
deﬁned as an upper bound to the scores of all complete conﬁgurations C that
match it. By this way, each sub-tree rooted at a partial conﬁguration whose score
is lower than the current best score can be pruned.
Constraint-Based Approaches. Algorithms following this approach test con-
ditional independencies between variables in the data, and progressively identify
the graph that describes these dependencies and independencies discovered in
the data [4].
In a transfer learning context, Jia et al. [5] address the problem of constraint-
based learning with inductive transfer. Luis et al. [8] propose a constraint-based
structure learning for BNs in a MT setting. The general outline of the algo-
rithm is inspired from the (single task) PC algorithm [16]. It starts with a fully
connected undirected graph, and measures the association between variables to
decide if an edge should be removed from the graph or not. The major diﬀerence
is the way the independence tests are evaluated. It is replaced by a linear combi-
nation of independence measures from the target task with the closest auxiliary
task, where closeness is determined by the combination of two metrics: a global
similarity Sg deﬁned in Eq. 3 and a local similarity Sl deﬁned in Eq. 4.
The global similarity measure Sgab computes the number of common depen-
dencies and independencies between every possible pair of variables (X, Y ) in

Multi-task Transfer Learning for Bayesian Network Structures
221
task a and task b (i.e. in their corresponding datasets Da and Db)[8].
Sgab =

X<Y
1(Ia(X, Y ) −Ib(X, Y ))
(3)
where Ia(X, Y ) and Ib(X, Y ) are respectively the result of an independence test
between variables X and Y performed on datasets Da and Db.
The local similarity measure Slab(X, Y |S) compares independencies between
two variables X and Y given a subset of variables S [8].
Slab(X, Y |S) =

1,
if Ia(X, Y |S) = Ib(X, Y |S).
0.5,
otherwise.
(4)
where Ia(X, Y |S) and Ib(X, Y |S) are respectively the result of the conditional
independence test between variables X and Y given S performed on datasets Da
and Db. Based on the two previous metrics, Luis et al. [8] deﬁne the combined
similarity measure Scab(X, Y |S) as:
Scab(X, Y |S) = Sgab × Slab(X, Y |S)
(5)
For a given task a, the conﬁdence measure αa estimates the conﬁdence of the
independence test between X and Y given the conditioning set S and is deﬁned
as:
αa(X, Y |S) = 1 −log Na
2Na
× T
(6)
where T = |X| × |Y | × |S|, with |x| is the cardinality of x, and where Na is the
size of the dataset Da.
Finally, the combined independence function Ia, that computes the indepen-
dence test between X and Y given S in task a with inductive transfer learning,
is a linear weighted combination of the independence measures in a and in its
most similar task b∗(with respect to the combined similarity Sc) :
Ica(X, Y |S) = αa(X, Y |S) × sgn(Ia(X, Y |S))
+ αb∗(X, Y |S) × Scab∗(X, Y |S) × sgn(Ib∗(X, Y |S))
(7)
where sgn(I) is +1 if X and Y are independent given S and −1 otherwise.
Hybrid Approaches. These approaches combine the features of the constraint-
based and the score-based algorithms. To our knowledge, they are only proposed
so far for single task learning. Generally, these algorithms start by implementing
a constraint-based strategy to reduce the space of candidate DAGs, then they
perform a score-based strategy to ﬁnd an optimal DAG in the restricted space.
In this context, we can cite the Max-Min Hill-Climbing (MMHC) algorithm
proposed in [18]. MMHC is a hybrid approach for single task Bayesian structure
learning that ﬁrst identiﬁes the skeleton of the graph with a constraint-based
method (named MMPC for Max-Min Parent Children) then it selects and directs
the interesting edges using a search-and-score procedure.

222
S. Benikhlef et al.
The MMPC algorithm uses an association metric A(X, Y |S) such as Mutual
information or χ2 to estimate the strength of the dependency between X and
Y given S and it performs a conditional independence test I(X, Y |S) from this
metric. The algorithm progressively identiﬁes for each variable X a set of can-
didate parents and children CPC(X) (without distinction between parent or
child).
Throughout the edge direction assignment step, a greedy search is performed
to determine the DAG that best ﬁts the data. The important diﬀerence from
standard greedy search is that the search space is constrained by the fact that
candidate edges must be consistent with the CPCs discovered by MMPC.
3
The MT-MMHC Algorithm
The BN structure learning algorithms mentioned in Sect. 2 mainly perform trans-
fer learning in single task (ST) scenario with constraint based approaches [8]
or multi-task consideration with search-and-score methods [10]. In our con-
tribution, we propose a hybrid approach for multi-task problems that we call
MT-MMHC. The purpose is to learn k BN structures from k similar problems
simultaneously, combining beneﬁts from constraint-based algorithms, score-and-
search based algorithms, TL and MT learning techniques.
3.1
Overall Process of MT-MMHC
The main idea is to extend the MMHC algorithm to the MT scenario, as shown
in Fig. 1. As its ST counterpart, the procedure starts with a constraint-based
phase to identify the CPC sets associated to each task. It performs a local
search technique ensured by the MMPC algorithm (refer to the grey boxes in
Fig. 1). For transfer learning adaptation, we propose in Sect. 3.2 a combined
association metric. In the second phase, for edge orientation, we apply the MT
greedy search algorithm proposed in [10] adapted to our context by constraining
it to the discovered CPCs as described in Sect. 3.3.
3.2
The Combined Association Measure
The ﬁrst phase of the MT-MMHC algorithm that we denote MT-MMPC, con-
sists in k parallel MMPC with a new combined association measure, to identify
the upper bound CPC of the skeleton of each model (refer to the grey boxes in
Fig. 1).
Inspired by the work in [8], we propose in Eq. 8 this new association metric
taking into account the MT setting.
Aca(X, Y |S) = αa(X, Y |S)Aa(X, Y |S) + αb∗(X, Y |S)Scab∗(X, Y |S)Ab∗(X, Y |S)
αa(X, Y |S) + αb∗(X, Y |S)Scab∗(X, Y |S)
(8)
where Aa(X, Y |S) is the usual association measure between two variables X
and Y given a subset S from the dataset Da and Ab∗(X, Y |S) expresses the

Multi-task Transfer Learning for Bayesian Network Structures
223
Fig. 1. Overall process of MT-MMHC.
association measure between the two same variables X and Y given the subset
S from dataset Db∗of the closest task b∗determined by the combined similarity
measure Sc.
This combined association measure allows us to transfer information between
similar tasks while computing independence tests. We propose using Aca instead
of the combined independence function Ia deﬁned in Eq. 7 for the following
reason: as a convex combination of two association measurements, this value
can also be interpreted as an association measurement. Ia is the non convex
linear combination of two signs of independence tests which is only used for its
sign whereas the MMPC algorithm requires also the information provided by
the strength of the association between variables.
3.3
MT Greedy Search with CPC Constraints
MT-MMPC, the constraint-based phase of MT-MMHC, outputs a CPC set
{CPC1, CPC2, . . . , CPCk} for each task.
In the second phase of MT-MMHC, as inspired by the single task MMHC,
we propose to apply the MT-GS algorithm (cf. Fig. 1) described in Sect. 2. The
main diﬀerence lies in the input of the algorithm and how we accordingly bound
the search space by using the information provided by the CPCs.
The neighborhood of a conﬁguration {G1, G2, . . . , Gk} is generated by apply-
ing for each pair of nodes in each possible subset of graphs an edit operation
(add, remove, reverse or leave unchanged an edge). We adapt the generation of

224
S. Benikhlef et al.
Fig. 2. A procedure to generate multi-task benchmarks from a reference BN.
Table 1. Description of the BNs used in our experimentation.
Name
Nodes Arcs Max in-degree Description
Asia
8
8
2
Used for patient chest clinic diagnosis
given symptoms and risk factors
Alarm 37
46
4
Medical diagnosis for monitoring
intensive care patients
this neighborhood by allowing the addition of the edge X →Y in a graph Ga
only if X ∈CPCa(Y ).
By using this deﬁnition of a neighborhood, we can perform a MT greedy
search procedure while keeping all the properties of the greedy search in the
context of MT with the constraints provided by the result of MT-MMPC. Hence,
we can reduce the search space and decrease the computational cost of the greedy
search. As the neighborhood size of a conﬁguration {G1, G2, ..., Gk} can get large
for large k and n, MT-MMHC could then be more scalable than MT-GS.
4
Experiments
In this section, we present an empirical evaluation of MT-MMHC as a compar-
ative study with state-of-the-art single task BNs structure learning algorithms
GS and MMHC, and also with the MT-GS algorithm.
4.1
Experimental Protocol
Benchmark and Data Generation. When benchmarks for BN structure
learning in single task context are quite popular with reference models such as
ASIA [6] or ALARM [2] (see Table 1), there are no benchmarks for evaluating
MT algorithms. We propose here one simple procedure that takes as an input
one of the reference models used for ST learning, and generates a set of k MT
reference models by controlling the similarity between each model.
As a ﬁrst version of this procedure, we generate k similar networks by apply-
ing one random walk between each model (by applying randomly ℓusual oper-
ators: add, remove or reverse edge) and recomputing randomly the parameters
(see Fig. 2).

Multi-task Transfer Learning for Bayesian Network Structures
225
We can then generate one dataset for each model by applying the usual
forward sampling algorithm, with a potentially diﬀerent number of observations
that we call data size Na for each task a.
In our work, we generated three series of experiments, with small datasets
size (Na ∈[500, 1000[), medium size (Na ∈[1000, 5000[) and large size (Na ∈
[5000, 10000[) for k = 5 diﬀerent tasks generated with ℓ= 1.
Algorithms. We have implemented several algorithms in PILGRIM1, our C++
library dedicated to probabilistic graphical models. We propose to compare MT-
MMHC (described in Sect. 3), MT-GS (cf. Sect. 2) and the independent running
of k single task structure learning algorithms kST-GS and kST-MMHC. In our
experiments, we used mutual information as a measure of association, with α =
5% for the independence tests, and BIC as an approximation of each marginal
likelihood P(Da|Ga) in Eq. 1 and δ = 1e −7 as a penalty in Eq. 2.
Evaluation Metrics. We measure performances in terms of run time and
Structural Hamming Distance (SHD) between the true structures (from which
we generated sampling data) and the learned ones, and more exactly the dis-
tance between the essential graphs as proposed in [18]. For each experiment, we
propose the mean and standard deviation of SHD over 10 runs x 5 tasks, and
the mean and standard deviation of the execution time over the 10 runs.
4.2
Empirical Results
MT-MMHC Versus kST-GS and kST-MMHC. Figure 3 (top) presents
performances in terms of SHD (the lower the better) for the three approaches
MT-MMHC, kST-GS and kST-MMHC with respect to three categories of data
size, and MT benchmarks generated from ASIA and ALARM networks.
The trends in the performances are as expected: learned networks are more
accurate with larger datasets, and kST-MMHC performs better than kST-GS
except for small datasets. It is worth to note that MT-MMHC is able to ﬁnd
better networks than the single task approaches.
Figure 3 (bottom) shows the average execution time to learn ﬁve tasks BNs.
Single task MMHC is the fastest approach in all experiments, but we can notice
that MT-MMHC is much faster than ST greedy search in a medium network
like Alarm thanks to the space reduction strategy. For the small network Asia,
MT-MMHC is the slowest but still runs in quite reasonable time (from 2.5 to
17.5 s on average). For instance, in the case of the largest data slice from ASIA,
MT-MMHC is 17% slower but 40% more accurate than kST-GS.
MT-MMHC Versus MT-GS. Figure 4 presents performances in terms of
SHD and execution time for MT-MMHC and MT-GS with respect to the sizes
of datasets, for MT benchmarks generated from the ALARM network.
1 https://pilgrim.univ-nantes.fr/.

226
S. Benikhlef et al.
Fig. 3. SHD (top) and run time (bottom) with respect to data size for MT-MMHC,
kST-GS and kST-MMHC (MT benchmarks are generated respectively from the ASIA
and ALARM networks).
Fig. 4. SHD (left) and run time (right) with respect to datasets size for MT-MMHC
and MT-GS for MT benchmarks generated from the ALARM network
For small datasets, MT-GS is slightly better than MT-MMHC with a much
higher execution time. For medium and larger datasets MT-MMHC performs
better in terms of quality of the learned model and runs with an aﬀordable time
cost.

Multi-task Transfer Learning for Bayesian Network Structures
227
5
Conclusion
In this paper, we propose an extension of an eﬃcient BN structure discovery
algorithm MMHC to a multi-task context. Our algorithm MT-MMHC is the ﬁrst
hybrid transfer learning algorithm. MT-MMHC can learn multiple BN structures
simultaneously by inducing information between similar tasks with the help of
a new combined association measure.
In order to validate our approach, we have also proposed one procedure to
generate MT benchmarks from any reference model, by controlling the similarity
between tasks.
The results of our experiments show that it is more beneﬁcial to learn related
tasks simultaneously than considering them individually and, both our combined
association measure and the use of the CPC discovered by MT-MMPC help in
ﬁnding accurate models with an aﬀordable time cost. In these experiments, MT-
MMHC was able to learn better models than kST-GS, kST-MMHC and MT-GS
in medium to large MT benchmarks.
This work is the ﬁrst step of our research, with several perspectives. In a
very short term, we intend to perform larger experiments with other datasets to
consolidate the interest of our proposition. We are also planning to work on addi-
tional procedures to generate MT benchmarks, for instance by using “longer”
random walks, or by creating tasks that don’t necessarily have all variables in
common.
Finally, our objective is to combine such MT structure learning algorithms
with diﬀerential privacy techniques (already used for BN ST learning in [19] for
instance) in order to propose one general framework to Federated Learning of
Bayesian networks, i.e. collaborative Structure and Parameter Learning with pri-
vacy considerations and no shared training data. Our interest is also to perform
this BN federated learning for the development of BN-based medical assistants
such as Medical Companion [9].
Acknowledgment. This work was supported by ANR AIby4 (Artiﬁcial Intelligence
by Humans, for Humans) PhD program (ANR-20-THIA-0011) and Atlanstic2020
regional program.
References
1. Azzimonti, L., Corani, G., Scutari, M.: Structure learning from related data sets
with a hierarchical bayesian score. In: Jaeger, M., Nielsen, T.D. (eds.) Proceedings
of the 10th International Conference on Probabilistic Graphical Models. Proceed-
ings of Machine Learning Research, vol. 138, pp. 5–16. PMLR (2020)
2. Beinlich, I.A., Suermondt, H.J., Chavez, R.M., Cooper, G.F.: The ALARM mon-
itoring system: A case study with two probabilistic inference techniques for belief
networks. In: Hunter, J., Cookson, J., Wyatt, J. (eds.) AIME 89, Second Euro-
pean Conference on Artiﬁcial Intelligence in Medicine. Lecture Notes in Medical
Informatics, vol. 38, pp. 247–256. Springer (1989)
3. Carvalho, A.M.: Scoring functions for learning Bayesian networks. Technical report
54/2009 Apr 2009, INESC-ID (2009)

228
S. Benikhlef et al.
4. Daly, R., Shen, Q., Aitken, S.: Learning Bayesian networks: approaches and issues.
Knowl. Eng. Rev. 26(2), 99–157 (2011)
5. Jia, H., Wu, Z., Chen, J., Chen, B., Yao, S.: Causal discovery with bayesian net-
works inductive transfer. In: Liu, W., Giunchiglia, F., Yang, B. (eds.) KSEM 2018.
LNCS (LNAI), vol. 11061, pp. 351–361. Springer, Cham (2018). https://doi.org/
10.1007/978-3-319-99365-2 31
6. Lauritzen, S.L., Spiegelhalter, D.J.: Local computations with probabilities on
graphical structures and their application to expert systems. J. Roy. Stat. Soc.
Series B (Methodological) 50(2), 157–224 (1988)
7. L´opez-Cruz, P.L., Larra˜naga, P., DeFelipe, J., Bielza, C.: Bayesian network model-
ing of the consensus between experts: An application to neuron classiﬁcation. Int.
J. Approximate Reasoning 55(1), 3–22 (2014)
8. Luis, R., Sucar, L.E., Morales, E.F.: Inductive transfer for learning Bayesian net-
works. Mach. Learn. 79(1), 227–255 (2010)
9. Mouchabac, S., Leray, P., Adrien, V., Gollier-Briant, F., Bonnot, O.: Beyond big
data in behavioral psychiatry, the place of Bayesian network. example from a pre-
clinical trial of an innovative smartphone application to prevent suicide relapse. J.
Med. Internet Res. 16/03/2021:24560, (in press) (2021)
10. Niculescu-Mizil, A., Caruana, R.: Inductive transfer for Bayesian network structure
learning. In: Meila, M., Shen, X. (eds.) Proceedings of the Eleventh International
Conference on Artiﬁcial Intelligence and Statistics. Proceedings of Machine Learn-
ing Research, vol. 2, pp. 339–346. PMLR, San Juan, Puerto Rico (21–24 Mar 2007)
11. Oyen, D., Lane, T.: Leveraging domain knowledge in multitask Bayesian network
structure learning. In: Proceedings of the AAAI Conference on AI 26(1) (2012)
12. Oyen, D., Lane, T.: Bayesian discovery of multiple Bayesian networks via transfer
learning. In: IEEE International Conference on Data Mining (2013)
13. Salaken, S.M., Khosravi, A., Nguyen, T., Nahavandi, S.: Extreme learning machine
based transfer learning algorithms. Neurocomput. 267(C), 516–524 (2017)
14. Scutari, M., Vitolo, C., Tucker, A.: Learning Bayesian networks from big data
with greedy search: computational complexity and eﬃcient implementation. Stat.
Comput. 29(5), 1095–1108 (2019)
15. Silander, T., Lepp¨a-Aho, J., J¨a¨asaari, E., Roos, T.: Quotient normalized maximum
likelihood criterion for learning Bayesian network structures. In: International Con-
ference on Artiﬁcial Intelligence and Statistics, pp. 948–957. PMLR (2018)
16. Spirtes, P., Glymour, C., Scheines, R.: Causation, Prediction, and Search, 2nd edn.
MIT press (2000)
17. Thung, K.H., Wee, C.Y.: A brief review on multi-task learning. Multimed. Tools
Appl. 77(22), 29705–29725 (2018)
18. Tsamardinos, I., Brown, L.E., Aliferis, C.F.: The max-min hill-climbing bayesian
network structure learning algorithm. Mach. Learn. 65(1), 31–78 (2006)
19. Zhang, J., Cormode, G., Procopiuc, C.M., Srivastava, D., Xiao, X.: PrivBayes:
Private data release via Bayesian networks. ACM Trans. Database Syst. 42(4)
(2017)
20. Zhang, Y., Yang, Q.: A survey on multi-task learning. IEEE Transactions on
Knowledge and Data Engineering, pp. 1–20 (2021)
21. Zhou, Y., Hospedales, T.M., Fenton, N.: When and where to transfer for Bayesian
network parameter learning. Expert Syst. Appl. 55, 361–373 (2016)

Persuasive Contrastive Explanations
for Bayesian Networks
Tara Koopman and Silja Renooij(B)
Department of Information and Computing Sciences, Utrecht University,
Utrecht, The Netherlands
s.renooij@uu.nl
Abstract. Explanation in Artiﬁcial Intelligence is often focused on pro-
viding reasons for why a model under consideration and its outcome are
correct. Recently, research in explainable machine learning has initiated
a shift in focus on including so-called counterfactual explanations. In this
paper we propose to combine both types of explanation in the context
of explaining Bayesian networks. To this end we introduce persuasive
contrastive explanations that aim to provide an answer to the question
Why outcome t instead of t′? posed by a user. In addition, we propose an
algorithm for computing persuasive contrastive explanations. Both our
deﬁnition of persuasive contrastive explanation and the proposed algo-
rithm can be employed beyond the current scope of Bayesian networks.
Keywords: Explainable AI · Counterfactuals · Bayesian networks
1
Introduction
Explanation of Bayesian networks has been a topic of interest ever since their
introduction [15,19]. Four categories of explanation method are distinguished,
depending on the focus of explanation: 1) explanation of evidence; 2) explanation
of reasoning; 3) explanation of the model itself, and 4) explanation of decisions [5,
11]. The last category is a recent addition to cover methods that address the
question of whether or not the user can make an informed enough decision.
In the explanation of reasoning category, methods typically aim to provide
justiﬁcation for the obtained outcomes and the underlying inference process [11].
Approaches include those that extract reasoning chains from the Bayesian net-
work, measure the impact of evidence, or identify supporting and conﬂicting
evidence [10–12,20,22,25]. Explanation of reasoning could also address the expla-
nation of outcomes not obtained [11]. In fact, upon encountering an unexpected
event, people tend to request contrastive explanations that answer the ques-
tion Why outcome t instead of t′? [14]. Such contrastive explanations were
recently adopted to explain black-box machine learning (ML) models with high-
dimensional feature spaces [23] by using counterfactuals that capture the change
in input required to change the outcome from t to t′. The use of counterfactuals
The original version of this chapter was revised: The errors in Algorithm 1 (page 235)
were corrected to include intentionally blank lines in the algorithm. The correction to
this chapter is available at https://doi.org/10.1007/978-3-030-86772-0 49
c
⃝Springer Nature Switzerland AG 2021, corrected publication 2021
J. Vejnarov´a and N. Wilson (Eds.): ECSQARU 2021, LNAI 12897, pp. 229–242, 2021.
https://doi.org/10.1007/978-3-030-86772-0_17

230
T. Koopman and S. Renooij
for the purpose of explanation is popular in ML research, although the deﬁnition
of what a counterfactual explanation entails varies greatly [21].
A recently identiﬁed challenge for ML research is that of unifying coun-
terfactual explanations with more “traditional explainable AI” that focuses on
justifying the original outcome [21]. This, however, is not a challenge speciﬁc to
ML models only. In this paper we propose the persuasive contrastive explanation
in the context of Bayesian networks. This explanation provides an answer to the
question Why t instead of t′? posed by a user, where t is the outcome predicted
as most likely by the Bayesian network and t′ is the output expected (or desired)
by the user. We will provide a contrastive explanation based upon an interpreta-
tion of counterfactuals by Wachter et al. [24]. Counterfactuals, however, do not
serve to justify outcome t. To provide a more complete answer to the Why. . . ?
question, we will try to persuade the user into believing that in fact t is the
correct outcome. To this end, we propose to include the evidence that suﬃces
to conclude t in the explanation as well. After presenting some properties of our
explanations, we propose an algorithm for their computation.
This paper is organised as follows. In Sect. 2 we introduce our new type of
explanation and some of its properties. In Sect. 3 we present a search structure
for explanations that is exploited by the algorithm detailed in Sect. 4. We review
more related work in Sect. 5 and conclude the paper in Sect. 6.
2
Persuasive Contrastive Explanations
In this section we propose our new type of explanation in the context of Bayesian
networks. A Bayesian network (BN) represents a joint probability distribution
Pr over a set of discrete random variables [8]. We denote such variables V by
capital letters and use Ω(V ) to represent their domain. We write v as shorthand
for a value assignment V = v, v ∈Ω(V ). (Sub)sets of variables are denoted by
bold-face capital letters V and their joint value combinations, or conﬁgurations,
by bold-face small letters v; Ω(V) is taken to represent the domain of all conﬁg-
urations of V. We write v′ ⊆v to denote that v′ is a conﬁguration of V′ ⊆V
that is consistent with v; we call v′ a sub-conﬁguration of v. Moreover, for a
given conﬁguration v, we write v to indicate a conﬁguration for V in which
every Vi ∈V takes on a value from Ω(Vi) that is diﬀerent from its value in v.
Note that v is unique only if all variables in V are binary-valued.
We are interested in the probabilities Pr(T | e) that can be computed from
the Bayesian network for a target variable T ∈V and evidence e for a set of
variables E ⊆V \ {T}. We assume that the most likely value of T given e, i.e.
arg maxt∗∈Ω(T ){Pr(t∗| e)} = arg maxt∗∈Ω(T ){Pr(t∗e)}, is conveyed to the user
as the network’s output. We refer to this as the mode of T given e, written
⊤(T |e). We now deﬁne the explanation context used throughout the paper.
Deﬁnition 1. An explanation context is a tuple

e, t, t′ 
where t = ⊤(T |e)
and t ̸= t′ ∈Ω(T).1
1 In case ⊤(T |e) is not unique, we assume all modes are output to the user and that
t′ is not among these.

Persuasive Contrastive Explanations
231
The explanation context describes the context for answering the question Why t
instead of t′?. We will answer this question with a contrastive explanation that
combines a suﬃcient explanation for t with a counterfactual explanation for t′.
Deﬁnition 2. Consider explanation context

e, t, t′ 
. A persuasive contrastive
explanation is any pair [s, c] where s ∈Ω(S), c ∈Ω(C), S, C ⊆E, and
• s ⊆e is a suﬃcient explanation for t, i.e. ⊤(T |se′) = t for all e′ ∈Ω(E′),
E′ = E \ S, and there is no s′ ⊂s for which this property holds; and
• c ⊆e is a counterfactual explanation for t′, i.e. ⊤(T |e′c) = t′ for e′ ⊆e,
e′ ∈Ω(E′), E′ = E \ C, and there is no c′ ⊂c for which this property holds.
Since e is taken to represent observations in the real world, it is common practice
to assume that Pr(e) > 0. For computing the modes in the above deﬁnition
it is required that Pr(se′) > 0 and Pr(e′c) > 0. As it does not make sense
for explanations to include impossible combinations of observations, any zero-
probability conﬁguration for E can be disregarded.
The suﬃcient explanation explains how the evidence relates to the outcome of
the network by giving the user a sub-conﬁguration of the evidence that results
in the same outcome, regardless of which values are observed for the remain-
ing evidence variables. The suﬃcient explanation generalizes the PI-explanation
that was introduced for explaining naive Bayesian classiﬁers with binary-valued
target variables [16], and more recently referred to as suﬃcient reason when
used in explaining Bayesian network classiﬁers (BNCs) with both binary-valued
target and evidence variables [4]. The word ‘counterfactual’ has various interpre-
tations; in our case, a counterfactual explanation details how the evidence should
be diﬀerent to result in the outcome expected by the user. Our deﬁnition is a
formalisation of the one by Wachter et al. [24], tailored to our speciﬁc context.
We will call a set S with which a suﬃcient explanation is associated a suf-
ﬁcient set; a counterfactual set is deﬁned analogously. Suﬃcient sets and coun-
terfactual sets have a number of properties that we will exploit to enable their
computation. All properties assume an explanation context

e, t, t′ 
. The ﬁrst
property addresses the extent to which suﬃcient explanations are unique and
follows directly from Deﬁnition 2.
Proposition 1. A set S ⊆E has at most one associated suﬃcient explanation
s. Sets S, S′ ⊂E for which neither S ⊂S′ nor S′ ⊂S can both be suﬃcient sets.
The next property addresses the relation between the type of evidence variables
(binary or non-binary) and counterfactual explanations.
Proposition 2. A counterfactual set C ⊆E can have multiple associated
counterfactual explanations c, unless all variables in C are binary-valued. Sets
C, C′ ⊂E with C ⊂C′ can both be counterfactual sets, unless all variables in
C are binary-valued.
Proof. If all variables in C are binary-valued then c ⊆e is unique; in this
case, by Deﬁnition 2, no subset of C can be a counterfactual set. Otherwise,

232
T. Koopman and S. Renooij
e is not unique and there exist multiple conﬁgurations c ⊆e for C. Consider
two such conﬁgurations c1 ̸= c2. Then both can adhere to Deﬁnition 2 and
be counterfactual explanations, but this is not necessary. Assume that c1 is a
counterfactual explanation and that c2 is not, and consider a conﬁguration c′ ⊃
c2 for a set C′ ⊃C. Then c′ ̸⊃c1 so c′ could be a counterfactual explanation,
in which case both C and C′ would be counterfactual sets.
⊓⊔
We conclude that a given explanation context can be associated with multi-
ple persuasive contrastive explanations. Finally we establish a relation between
suﬃcient sets and counterfactual sets.
Proposition 3. Consider a set S ⊆E, and let C = E \ S. If S is a suﬃcient
set, or is a superset of a suﬃcient set, then C cannot be a counterfactual set.
Proof. If S is a suﬃcient set with suﬃcient explanation s ⊆e then by Deﬁni-
tion 3, ⊤(T |sc) = t for all c ∈Ω(C). Since t′ ̸= t, no c can be a counterfactual
explanation and hence C is not a counterfactual set. Now let S′ ⊂S be a suﬃ-
cient set with suﬃcient explanation s′ and consider conﬁguration s = s′d ⊆e for
d ∈S\S′. Then dc ∈Ω(C′), where C′ = E\S′ and c ∈Ω(C). Since S′ is a suﬃ-
cient set, ⊤(T |s′c′) = t for all c′ ∈Ω(C′). As a result, ⊤(T |s′dc) = ⊤(T |sc) = t
for all c ∈Ω(C). Hence, C is not a counterfactual set.
⊓⊔
3
Explanation Lattice
To ﬁnd all suﬃcient and counterfactual explanations for a given explanation
context

e, t, t′ 
, we can typically do better than naively looping through all
possible conﬁgurations e for E and computing all distributions Pr(T | e). In order
to exploit the properties from Propositions 1–3 in our search for explanations,
we propose to organise the search space using an annotated lattice.
Deﬁnition 3. Consider context

e, t, t′ 
and lattice L = (P(E), ⊆), for power-
set P(E) of E. An explanation lattice for this context is the lattice L in which
each lattice element S ⊆E is annotated with the tuple (s, MS, lS) such that
– s ⊆e is the conﬁguration of S consistent with e;
– if S = E then MS = {(∅, t)}; otherwise, MS = {(c, t∗) | t∗= ⊤(T |sc), with
c ∈Ω(C), C = E \ S and c ⊆e};
– lS ∈{true, exp, oth}, where lS = true if t∗= t for all (c, t∗) ∈MS, lS = exp
if t∗= t′ for all (c, t∗) ∈MS, and lS = oth, otherwise.
The elements of lattice L are all subsets of E and hence represent potential
suﬃcient sets S with associated suﬃcient explanation s. For each lattice element
S, the set C = E \ S is a potential counterfactual set. To determine if c is a
counterfactual explanation, we need to know the corresponding outcome; these
pairs are stored in M. Label l summarises whether or not all sc conﬁgurations
associated with a lattice element result in the same outcome, with true indicating
that this is always the originally predicted outcome t and exp indicating that this

Persuasive Contrastive Explanations
233
Fig. 1. A partially annotated explanation lattice for the evidence in the Child network:
elements S ⊆E = {H, O, C, X} are annotated with label lS. Numbers between brackets
indicate the fraction of modes actually computed. See Example 1 for further details.
is always the expected output t′. Note that if all variables in E are binary-valued
then each MS contains a single pair (c, t∗). If the target variable T is binary-
valued, then lS = oth can only occur with non-binary evidence variables. Figure 1
shows a partially annotated lattice, which is further explained in Sect. 4.3.
For a lattice element S we will use the term ancestors to refer to all supersets
of S in the lattice, and parents to refer to the supersets of size |S|+1; the parent
set will be denoted S⇑. Similarly, the term descendants is used to refer to all
subsets of S in the lattice, and a child is a subset of size |S| −1; the set of all
children will be denoted S⇓. The lattice now provides all information necessary
for determining whether or not a lattice element represents a suﬃcient set.
Lemma 1. Consider context

e, t, t′ 
and explanation lattice L with lattice ele-
ment S ⊆E. Then for any possible conﬁguration e′ ∈Ω(E′) for E′ = E \ S,
output ⊤(T |se′) is available from the annotation of S or one of its ancestors.
Proof. From Deﬁnition 3 we have that MS contains ⊤(T |se′) for all e′ ∈Ω(E′)
with e′ ⊆e. Now consider a conﬁguration de+ ∈Ω(E′) where d ⊆e and
e+ ⊆e for some set E+. Then S+ = E \ E+ is a superset of S, annotated
with s+ = sd and outcome ⊤(T |sde+). We conclude that the outcomes for all
remaining e′ ∈Ω(E′) are found in the annotations of all supersets S+ of S,
which are exactly the ancestors of S.
⊓⊔
The exact way to establish a suﬃcient set from the lattice is given by the fol-
lowing proposition.
Proposition 4. Consider context

e, t, t′ 
and lattice element S ⊆E in expla-
nation lattice L. Set S is a suﬃcient set iﬀall of the following hold:
1. S and each of its ancestors S+ is annotated with label lS = lS+ = true, and

234
T. Koopman and S. Renooij
2. for each child S−∈S⇓, either lS−̸= true, or S−has an ancestor S+ with
label lS+ ̸= true.
Proof. From Deﬁnition 3 and Lemma 1 we have that the ﬁrst property holds iﬀ
⊤(T |se′) = t for any possible conﬁguration e′ ∈Ω(E′) for E′ = E\S. Therefore,
by Deﬁnition 2 S is a suﬃcient set, unless there exists a subset S−⊂S, i.e.
a lattice descendant, that adheres to the ﬁrst property. This case is covered
by the second property. Consider a child S−∈S⇓of S in the lattice. Then
lS−̸= true iﬀthere exists a conﬁguration e+ ∈Ω(E+) for E+ = E \ S−such
that ⊤(T |s−e+) ̸= t. Hence neither S−, nor any of its descendants, can represent
a suﬃcient set. Now suppose that lS−= true then S−can only be suﬃcient if
all its ancestors S+ have lS+ = true. If there exists an ancestor with lS+ ̸= true,
then none of the descendants of S+, which include S−and its descendants, can
represent a suﬃcient set.
⊓⊔
The next proposition provides the means for determining counterfactual expla-
nations from the lattice.
Proposition 5. Consider context

e, t, t′ 
and lattice element S ⊆E in expla-
nation lattice L. Conﬁguration c for set C = E\S is a counterfactual explanation
for t′ iﬀ(c, t′) ∈MS and for none of the ancestors S+ of S there exists a c′ ⊂c
with (c′, t′) ∈MS+.
Proof. From Deﬁnition 3 we have that (c, t′) ∈MS iﬀ⊤(T |sc) = t′. Therefore,
by Deﬁnition 2, c is a counterfactual explanation for t′, unless there exists a
c′ ⊂c that also results in outcome t′. Such a c′ can only be found for a set
C′ = E \ S+ where S+ is a superset of S and hence a lattice ancestor.
⊓⊔
4
Computing Suﬃcient and Counterfactual Explanations
We will use a breadth ﬁrst search on the explanation lattice to return the suﬃ-
cient and counterfactual explanations. During the search, the lattice is annotated
dynamically in order to minimize the number of mode computations, since not
all lattice elements need necessarily be visited. As a result, lS = ∅as long as a
lattice element has not been processed during search, and the modes in MS are
unknown (unkn) until actually computed. We will ﬁrst present two algorithms
for separately computing the two types of explanation. We will then illustrate
the combined search and discuss further optimisations.
4.1
Searching the Lattice for Suﬃcient Explanations
The breadth-ﬁrst search for suﬃcient explanations (bfs-sfx) is described in
pseudo code in Algorithm 1. Since suﬃcient explanations are set-minimal, it
may seem most optimal to start the search at the bottom of the explanation
lattice. However, to decide whether a lattice element represents a suﬃcient set,
we require the labels of its lattice ancestors (see Proposition 4). Hence we start

Persuasive Contrastive Explanations
235
Algorithm 1: bfs-sfx for computing suﬃcient explanations.
Input
: BN B, context

e, t, t′ 
and explanation lattice L
Output: Set S with all suﬃcient explanations
1 SQ ←E; PotS ←∅
2 while SQ not empty do
3
S ←Dequeue(SQ);
4
if lS = ∅and ∀S+ ∈S⇑: lS+ = true then
5
ComputeModesAndLabel(L, S);
6
if lS = true then
7
PotS ←PotS ∪{S};
8
for all S−∈S⇓do Enqueue(SQ, S−);
9
10
end
11
end
12 end
13 S ←{s ⊆e | S ∈PotS, s ∈Ω(S), ∀S−∈S⇓: lS−̸= true};
14
15 return S
the search at the top of the explanation lattice. We add an unvisited set S to
the set PotS of potential suﬃcient sets if all its lattice parents (if any) are in
PotS and ⊤(T |sc) = t for all conﬁgurations c for C = E \ S with c ⊆e. Line
5 of the algorithm (ComputeModesAndLabel) serves for computing modes from
the Bayesian network and for recording them, together with the summarising
label, in the explanation lattice. For a given explanation context, the algorithm
returns suﬃcient explanations for all suﬃcient sets that adhere to the properties
stated in Proposition 4.
Proposition 6. Consider context

e, t, t′ 
and explanation lattice L. Algo-
rithm 1 returns all suﬃcient explanations for t.
Proof. Queue SQ is initially ﬁlled with the top lattice element S = E, which is
subsequently processed since it wasn’t visited and does not have any parents.
ComputeModesAndLabel gives ME = {(∅, t)} and lE = true. Since all modes
associated with this lattice element equal t, it is added to the set PotS of potential
suﬃcient sets and its lattice children are enqueued in SQ. Subsequently, labels are
only computed for lattice elements S for which all parents are potential suﬃcient
sets, and only if lS = true will its children be enqueued in SQ. As a result, a
lattice element is in PotS iﬀit adheres to the ﬁrst property in Proposition 4.
The algorithm now returns (line 14) only explanations for the sets in PotS for
which the children S−in the lattice are labelled lS−= exp or lS−= oth, or for
which lS−= ∅. The former two cases clearly adhere to the second property from
Proposition 4. If lS−= ∅, we have not computed the modes and label for S−, so
we could still have that in fact lS−should be true. However, during search the
label for a set S−remains undetermined if it has a parent that is not in PotS;
in that case it has an ancestor S+ with label lS+ = exp or lS+ = oth. So with

236
T. Koopman and S. Renooij
Algorithm 2: bfs-cfx for computing counterfactual explanations.
Input
: BN B, context

e, t, t′ 
, explanation lattice L
Output: Set C with all counterfactual explanations
1 CQ ←E; C ←∅
2 while CQ not empty do
3
S ←Dequeue(CQ);
4
if lS = ∅and ∀S+ ∈S⇑: lS+ ̸= exp then
5
potc ←{c | (c, unkn) ∈MS, ¬∃c′ ∈C : c′ ⊂c};
6
if potc ̸= ∅then
7
ComputeModesAndLabel(L, S, potc);
8
if lS ̸= true then
9
C ←C ∪{c ∈potc | (c, t′) ∈MS};
10
end
11
if lS ̸= exp then
12
for all S−∈S⇓do Enqueue(CQ, S−);
13
end
14
end
15
end
16 end
17 return C
lS−= ∅, S−also adheres to property 2 of Proposition 4. Hence set S contains
all suﬃcient explanations for t.
⊓⊔
4.2
Searching the Lattice for Counterfactual Explanations
The breadth-ﬁrst search for counterfactual explanations (bfs-cfx) is described
in pseudo code in Algorithm 2. The search again starts at the top of the explana-
tion lattice, processing an unvisited set S if it can potentially have counterfactual
explanations associated with it. Whereas the extent of the search for suﬃcient
explanations is independent of variable type (binary vs non-binary), the search
for counterfactual explanations can become quite more extensive for non-binary
variables. For a given explanation context, Algorithm 2 returns all counterfactual
explanations that adhere to the properties stated in Proposition 5.
Proposition 7. Consider context

e, t, t′ 
and explanation lattice L. Algo-
rithm 2 returns all counterfactual explanations for t′.
Proof. First note that if a set S ⊆E is a potential suﬃcient set (PotS) according
to Algorithm 1, then set C = E \ S cannot be a counterfactual set (see Proposi-
tion 3). Therefore, only if we encounter a set S with lS ̸= true can C possibly be a
counterfactual set. Since the search starts at the top of the lattice, any potential
counterfactual set is encountered earlier in the search than any of its supersets.
Therefore, the ﬁrst c with (c, t′) ∈MS found is in fact a counterfactual explana-
tion and added to the set C of counterfactual explanations. As result, no c′ ⊃c
can be a counterfactual explanation (see Proposition 5) and set potc prevents

Persuasive Contrastive Explanations
237
such c′ from being added to C (line 5). If potc is empty then all conﬁgurations
c associated with the current lattice element S are already covered by C so nei-
ther S nor its descendants can have an associated true counterfactual set C. If
potc is non-empty, then the computation of modes and the resulting label can
be restricted to conﬁgurations in potc (see ComputeModesAndLabel’s optional
argument). Now any c ∈potc with (c, t′) ∈MS is a counterfactual explanation.
If there also exists a c ∈potc with (c, t∗) ∈MS such that t∗̸= t′, then this c
could be part of a counterfactual explanation associated with a superset of C in
one of the descendants of S. Hence the children of S are enqueued in CQ. If such
a child has a parent S+ for which lS+ = exp then all possible conﬁgurations for
C−= E \ S+ are counterfactual explanations or are covered by counterfactual
explanations in ancestors; hence, the child is not processed further. We conclude
that once queue CQ is empty, set C contains all counterfactual explanations. ⊓⊔
4.3
Combining the Search for Explanations
Both algorithms bfs-sfx and bfs-cfx do a breadth ﬁrst search through the
explanation lattice and can easily be combined to compute both types of expla-
nation in a single search. Recall that Algorithm 1 goes through the lattice until it
encounters lattice elements that cannot be suﬃcient sets. It is not until this point
that the search for counterfactual explanations needs to start (Proposition 3).
Rather than starting bfs-cfx at the top of the lattice, we could therefore have
Algorithm 1 initialize set C and queue CQ. The following additions to Algo-
rithm 1 serve for checking for counterfactual explanations upon encountering
an element S that cannot be suﬃcient, adding those to C and, if necessary,
enqueueing the children of S in queue CQ:
▷line 1, add: CQ ←∅; C ←∅
▷for lS ̸= true, ﬁll in blank line 9 with:
9
else C ←C ∪{c | (c, t′) ∈MS};
9a

if lS ̸= exp then
9b

|
for all S−∈S⇓do Enqueue(CQ, S−);
9c

end
If all variables are binary-valued then any lattice element S has only a single
associated (c, t∗) ∈MS; if t∗= t′ and this c is added to C, then none of the
descendants of S can contain counterfactual explanations. In this case, therefore,
the above adaptation leaves queue CQ empty. As a result, once all suﬃcient
sets are found, set C contains all counterfactual explanations and we are done.
An example of computing suﬃcient and counterfactual explanations from the
well-known Asia network2 with only binary-valued variables is given in the ﬁrst
author’s MSc. thesis [9]. In case the target variable and/or the evidence variables
are non-binary, queue CQ will probably be non-empty and the search can now
fully focus on ﬁnding any remaining counterfactual explanations by continuing
2 All mentioned networks are available from https://www.bnlearn.com/bnrepository/.

238
T. Koopman and S. Renooij
the search with the already partially ﬁlled queue CQ and set C. That is, we add
the following to Algorithm 1:
▷ﬁll in blank line 14 with: GetRemainingCounterfactuals(L, C,CQ), which
executes lines 2–16 of bfs-cfx;
▷line 15, add C.
We will use bfs-sfx-cfx to refer to Algorithm 1 with the four above changes to
include the computation of both types of explanation. We now illustrate their
computation with an example.
Example 1. We consider the Child Bayesian network [18] with 6-valued target
variable Disease(D) and four of its evidence variables: LVH Report (H) with 2
values, Lower Body O2 (O) with 3 values, CO2 Report (C) with 2 values and
X-ray Report (X) with 5 values. We enter the evidence e ≡‘H = yes ∧O =
5-12 ∧C = < 7.5 ∧X = Oligaemic’. We ﬁnd ⊤(D|e) = PAIVS, whereas the
user instead expected outcome TGA ∈Ω(D). Figure 1 now shows the elements
S ⊆E in the explanation lattice for context

e, PAIVS, TGA

. In addition, the
ﬁgure shows the labels lS computed for each element and, between brackets, the
number of computed modes versus the total number of associated conﬁgurations.
Starting at the top of the lattice, bfs-sfx-cfx ﬁrst searches for potential
suﬃcient sets. After computing modes for HOCX, HOC, HCX, HOX, OCX,
and HX (in total: 11), the algorithm has found all suﬃcient sets, resulting in
a single suﬃcient explanation: S = {‘H = yes ∧X = Oligaemic’}. In the
process, set C is initialised to C = {‘X =Plethoric’}; had all variables been
binary-valued then we would have been done. Instead, queue CQ is initialised
with CQ= [HO, HC, OC, OX, CX] so the search for counterfactuals continues,
ﬁnally resulting in four counterfactual explanations: C = {‘X = Plethoric’,
‘X = Normal ∧H = no’, ‘X = Grd Glass ∧H = no’, ‘H = no ∧O = < 5
∧X = Asy/Patchy’}. The persuasive contrastive explanations for PAIVS are
now given by all four pairs [s, c] such that s ∈S and c ∈C.
⊓⊔
We note that in the example we ultimately computed modes for 39 out of the 60
represented evidence conﬁgurations. The search for counterfactual explanations
continued all the way to the bottom of the lattice: since the target variable has
a large state-space, the majority of elements is labelled with oth, indicating that
possibly another counterfactual explanation is to be found. Two of the labels
in Fig. 1 have an exclamation mark (for HC and H). Here the computed labels
are in fact diﬀerent from what they should be according to Deﬁnition 3. These
labels should be oth, since both modes PAIVS and TGA are found. However, the
conﬁgurations that would result in mode TGA in both cases are excluded from
potc due to ‘X = Plethoric’ ∈C, leaving their modes unkn. As a result, the
computed labels are based only on conﬁgurations that result in mode PAIVS.
Note that this does not aﬀect the outcome or correctness of the algorithm.
4.4
Complexity and Further Optimisations
The search for suﬃcient and counterfactual explanations is aborted as soon as
all explanations are guaranteed to be found. In worst case, however, bfs-sfx-

Persuasive Contrastive Explanations
239
cfx will visit and process all of the 2|E| lattice elements. In processing a lattice
element S, at most 
Ci∈E\S(|Ω(Ci)|−1) modes are computed. For Bayesian net-
works, these computations can be time-consuming, since in general probabilistic
inference in Bayesian networks is NP-hard [3], even if we prune computationally
irrelevant variables from the network [2]. The overall computational burden can
be reduced in at least two ways:
– We can do Bayesian network inference using so-called saturated junction
trees: a C-saturated junction tree will allow for computing all probabilities
Pr(TCs) through eﬃcient local operations only [8].
– We can further reduce the number of mode computations by exploiting mono-
tonicity properties in the domain, such as monotonicity in distribution or in
mode [6]; this eﬀectively serves for pruning the explanation lattice.
Assuming an ordering on the values of each variable, inducing a partial order on
conﬁgurations, the Bayesian network is for example monotonic in mode if higher
values of e result in a higher mode. In such a case, if we have observed the highest
value e for some individual evidence variable Ei, then lower values for Ei will
never result in a higher mode. If t′ > t, then we can disregard the values of Ei
in our search for counterfactuals. Exploiting such properties can greatly reduce
the number of conﬁgurations bfs-sfx-cfx needs to consider. Further details on
how monotonicity can be exploited to this end, including example computations
on an adapted version of the Insurance network, can again be found in [9].
5
Related Work
As discussed in Sect. 2, our notion of suﬃcient explanation is similar to the
concept of PI-explanation introduced for explaining Bayesian network classi-
ﬁers. In the related research the Bayesian networks are assumed to be restricted
in topology (naive vs general) [13,16] and/or restricted in variable type (binary
vs non-binary) [4,16]. The algorithms used for computing the PI-explanations
all assume that the Bayesian network is used purely as a classiﬁer and rely on
transforming the classiﬁer into a tractable model, such as an Ordered (Binary)
Decision Diagram (O(B)DD) [4,16], an Extended Linear Classiﬁer (ELC) [13],
or a representation in First Order Logic [7]. Some transformations either apply
only to naive Bayesian networks [13] or require NP-hard compilations [16].
In the past year, several papers have introduced a concept of counterfactual
explanation for Bayesian network classiﬁers, all using diﬀerent deﬁnitions. A
common denominator in these deﬁnitions is that they determine counterfactual
explanations from PI-explanations or vice-versa. Examples include taking the
evidence that is common to all PI-explanations (critical inﬂuences) together with
taking the combined non-critical evidence from the PI-explanations (potential
inﬂuences) [1], or using PI-explanations to explain for which changes in evidence
the current mode will be left unchanged (‘even-if-because’) [4]. Ignatiev et al. [7]
prove a formal relationship between PI-explanations and counterfactual explana-
tions for ML models. Their deﬁnition of counterfactual explanation is also based

240
T. Koopman and S. Renooij
on Wachter et al. [24], but assumes binary-valued evidence variables. In contrast,
our counterfactual explanation is deﬁned for discrete variables in general and dif-
ferent counterfactual explanations can include the same evidence variables with
diﬀerent counterfactual values. Moreover, PI-explanations do not provide any
information about our counterfactual explanations other than excluding some
conﬁgurations as possible counterfactuals.
6
Conclusions and Further Research
In this paper we introduced persuasive contrastive explanations for Bayesian
networks, detailed an algorithm for their computation and proved its correct-
ness. The new type of explanation combines a suﬃcient explanation for the
current most likely outcome of the network with a counterfactual explanation
that explains the changes in evidence that would result in the outcome expected
by the user. Suﬃcient explanations were introduced before as PI-explanations
and eﬃcient algorithms for their computation exist for special cases. Counter-
factual explanations such as we deﬁne have, to the best of our knowledge, not
been used in this context before. We have demonstrated that for special cases
the counterfactual explanations are available as soon as the search for suﬃcient
explanations ﬁnishes; in general the search for counterfactuals then starts.
Our deﬁnitions and basic algorithm are in essence model-agnostic, albeit
that the required modes are computed from the Bayesian network. The modes,
however, could represent the output predicted by other types of model over
the same variables, since we do not exploit properties speciﬁc to the Bayesian
network. We can therefore employ the same concepts and algorithm for other
types of underlying model, as long as the number of diﬀerent conﬁgurations for
a typical set of evidence variables is limited enough to process.
Since Bayesian network classiﬁers of arbitrary topology allowing non-binary
evidence and target variables can now be compiled into a tractable ODD [17],
it is worth investigating the suitability of ODDs for more eﬃciently computing
persuasive contrastive explanations in Bayesian network classiﬁers. When many
diﬀerent explanations are found, it is necessary to make a selection to be pre-
sented to the user. Such a selection can for example be based on the cardinality
of the explanation. A beneﬁt of directly using Bayesian networks rather than
compiled structures such as ODDs is that the computed probabilities can also
be exploited for selecting explanations to present to the user. In future we aim
to further study the use of probabilistic information for explanation selection.
In addition, we aim to further exploit the direct use of a Bayesian network by
introducing intermediate variables into the explanation.
Acknowledgements. This research was partially funded by the Hybrid Intelligence
Center, a 10-year programme funded by the Dutch Ministry of Education, Culture and
Science through the Netherlands Organisation for Scientiﬁc Research, https://hybrid-
intelligence-centre.nl. We would like to thank the anonymous reviewers for their useful
and inspiring comments.

Persuasive Contrastive Explanations
241
References
1. Albini, E., Rago, A., Baroni, P., Toni, F.: Relation-based counterfactual explana-
tions for Bayesian network classiﬁers. In: Bessiere, C. (ed.) Proceedings of the 29th
International Joint Conference on Artiﬁcial Intelligence, pp. 451–457 (2020)
2. Baker, M., Boult, T.E.: Pruning Bayesian networks for eﬃcient computation. In:
Bonissone, P., Henrion, M., Kanal, L., Lemmer, J. (eds.) Uncertainty in Artiﬁcial
Intelligence 6. Elsevier Science, Amsterdam (1991)
3. Cooper, G.F.: The computational complexity of probabilistic inference using
Bayesian belief networks. Artif. Intell. 42, 393–405 (1990)
4. Darwiche, A., Hirth, A.: On the reasons behind decisions. In: De Giacomo, G.,
Catala, A., Dilkina, B., Milano, M., Barro, S., Bugar´ın, A., Lang, J. (eds.) Pro-
ceedings of 24th European Conference on Artiﬁcial Intelligence, pp. 712–720. IOS
Press (2020)
5. Derks, I.P., de Waal, A.: A taxonomy of explainable Bayesian networks. In: Gerber,
A. (ed.) Artiﬁcial Intelligence Research, pp. 220–235. Springer, Cham (2020)
6. van der Gaag, L.C., Bodlaender, H.L., Feelders, A.: Monotonicity in Bayesian
networks. In: Chickering, M., Halpern, J. (eds.) Proceedings of the 20th Conference
on Uncertainty in Artiﬁcial Intelligence, pp. 569–576 (2004)
7. Ignatiev, A., Narodytska, N., Asher, N., Marques-Silva, J.: From contrastive to
abductive explanations and back again. In: Baldoni, M., Bandini, S. (eds.) AIxIA
2020. LNCS (LNAI), vol. 12414, pp. 335–355. Springer, Cham (2021). https://doi.
org/10.1007/978-3-030-77091-4 21
8. Jensen, F.V., Nielsen, T.D.: Bayesian networks and decision graphs. Springer Sci-
ence & Business Media, 2 edn. (2007)
9. Koopman, T.: Computing Contrastive, Counterfactual Explanations for Bayesian
Networks. Master’s thesis, Universiteit Utrecht, The Netherlands (2020). https://
dspace.library.uu.nl/handle/1874/398728
10. Kyrimi, E., Marsh, W.: A progressive explanation of inference in ‘hybrid’ Bayesian
networks for supporting clinical decision making. In: Antonucci, A., Corani, G., de
Campos, C.P. (eds.) Proceedings of the 8th Conference on Probabilistic Graphical
Models, pp. 275–286 (2016)
11. Lacave, C., D´ıez, F.J.: A review of explanation methods for Bayesian networks.
Knowl. Eng. Rev. 17(2), 107–127 (2002)
12. van Leersum, J.: Explaining the reasoning of Bayesian networks with intermedi-
ate nodes and clusters. Master’s thesis, Utrecht University (2015). http://dspace.
library.uu.nl/handle/1874/313520
13. Marques-Silva, J., Gerspacher, T., Cooper, M., Ignatiev, A., Narodytska, N.:
Explaining naive Bayes and other linear classiﬁers with polynomial time and delay.
In: Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M.F., Lin, H. (eds.) Advances
in Neural Information Processing Systems, vol. 33, pp. 20590–20600. Curran Asso-
ciates, Inc. (2020)
14. Miller, T.: Explanation in artiﬁcial intelligence: insights from the social sciences.
Artif. Intell. 267, 1–38 (2019)
15. Pearl, J.: Probabilistic Reasoning in Intelligent Systems: Networks of Plausible
Inference. Morgan Kaufmann Publishers (1988)
16. Shih, A., Choi, A., Darwiche, A.: A symbolic approach to explaining Bayesian
network classiﬁers. In: Proceedings of the 27th International Joint Conference on
Artiﬁcial Intelligence, pp. 5103–5111 (2018)

242
T. Koopman and S. Renooij
17. Shih, A., Choi, A., Darwiche, A.: Compiling Bayesian network classiﬁers into deci-
sion graphs. In: Proceedings of the 33rd AAAI Conference on Artiﬁcial Intelligence,
pp. 7966–7974 (2019)
18. Spiegelhalter, D.J., Dawid, A.P., Lauritzen, S.L., G.Cowell, R.: Bayesian analysis
in expert systems. Stat. Sci. 8(3), 219–247 (1993)
19. Suermondt, H.J.: Explanation in Bayesian belief networks. Ph.D. thesis, Stanford
University (1992)
20. Timmer, S.T., Meyer, J.-J.C., Prakken, H., Renooij, S., Verheij, B.: Explaining
Bayesian networks using argumentation. In: Destercke, S., Denoeux, T. (eds.)
ECSQARU 2015. LNCS (LNAI), vol. 9161, pp. 83–92. Springer, Cham (2015).
https://doi.org/10.1007/978-3-319-20807-7 8
21. Verma, S., Dickerson, J.P., Hines, K.E.: Counterfactual explanations for machine
learning: A review. ArXiv abs/2010.10596 (2020)
22. Vlek, C., Prakken, H., Renooij, S., Verheij, B.: A method for explaining Bayesian
networks for legal evidence with scenarios. Artif. Intell. Law 24(3), 285–324 (2016)
23. van der Waa, J., Robeer, M., van Diggelen, J., Brinkhuis, M., Neerincx, M.: Con-
trastive explanations with local foil trees. In: Proceedings of the Workshop on
Human Interpretability in Machine Learning, pp. 41–47 (2018)
24. Wachter, S., Mittelstadt, B., Russell, C.: Counterfactual explanations without
opening the black box: automated decisions and the GPDR. Harvard J. Law Tech-
nol. 31, 841 (2017)
25. Yap, G.E., Tan, A.H., Pang, H.H.: Explaining inferences in Bayesian networks.
Appl. Intell. 29(3), 263–278 (2008)

Explainable AI Using MAP-Independence
Johan Kwisthout(B)
Donders Institute for Brain, Cognition, and Behaviour, Radboud University,
Nijmegen, The Netherlands
j.kwisthout@donders.ru.nl
http://www.socsci.ru.nl/johank/
Abstract. In decision support systems the motivation and justiﬁcation
of the system’s diagnosis or classiﬁcation is crucial for the acceptance of
the system by the human user. In Bayesian networks a diagnosis or classi-
ﬁcation is typically formalized as the computation of the most probable
joint value assignment to the hypothesis variables, given the observed
values of the evidence variables (generally known as the MAP problem).
While solving the MAP problem gives the most probable explanation of
the evidence, the computation is a black box as far as the human user
is concerned and it does not give additional insights that allow the user
to appreciate and accept the decision. For example, a user might want
to know to what extent a variable was relevant for the explanation. In
this paper we introduce a new concept, MAP-independence, which tries
to formally capture this notion of relevance, and explore its role towards
a justiﬁcation of an inference to the best explanation.
Keywords: Bayesian networks · Most probable explanations ·
Relevance · Explainable AI · Computational complexity
1
Introduction
With the availability of petabytes of data and the emergence of ‘deep’ learning
as an AI technique to ﬁnd statistical regularities in these large quantities of data,
artiﬁcial intelligence in general and machine learning in particular has arguably
entered a new phase since its emergence in the 1950s. Deep learning aims to build
hierarchical models representing the data, with every new layer in the hierarchy
representing ever more abstract information; for example, from individual pixels
to lines and curves, to geometric patterns, to features, to categories. Superﬁcially
this might be related to how the human visual cortex interprets visual stimuli
and seeks to classify a picture to be that of a cat, rather than of a dog.
When describing in what sense a cat is diﬀerent from a dog, humans may
use features and categories that we agreed upon to be deﬁning features of cats
and dogs, such as whiskers, location and form of the ears, the nose, etc. The
deep learning method, however, does not adhere to features we humans ﬁnd
to be good descriptors; it bases its decisions where and how to ‘carve nature’s
joints’ solely on basis of the statistics of the data. Hence, it might very well
c
⃝Springer Nature Switzerland AG 2021
J. Vejnarov´a and N. Wilson (Eds.): ECSQARU 2021, LNAI 12897, pp. 243–254, 2021.
https://doi.org/10.1007/978-3-030-86772-0_18

244
J. Kwisthout
be that the curvature of the spine (or some other apparently ‘random’) feature
happens to be the statistically most important factor to distinguish cats from
dogs. This imposes huge challenges when the machine learning algorithm is asked
to motivate its classiﬁcation to a human user. The sub-ﬁeld of explainable AI
has recently emerged to study how to align statistical machine learning with
informative user-based motivations or explanations. Explainable AI, however,
is not limited to deep neural network applications. Any AI application where
trustworthiness is important beneﬁts from justiﬁcation and transparency of its
internal process [6], and this includes decision support systems that are based on
Bayesian networks, which is the focus of this paper. In these systems typically
one is interested in the hypothesis that best explains the available evidence; for
example in a medical case, the infection that is most probable given a set of
health complaints and test ﬁndings.
Note that ‘explainability’ in explainable AI is in principle a triadic relation-
ship between what needs to be explained, the explanation, and the user who
seeks the explanation [15]. An explanation will be more satisfying (‘lovelier’, in
Peter Lipton’s [11] terms) if it allows the user to gain more understanding about
the phenomenon to be explained. In this paper we speciﬁcally try to improve
the user’s understanding of a speciﬁc decision by explicating the relevant infor-
mation that contributed to said decision. In some way, in deciding what the best
explanation is for a set of observations, the process of marginalizing out the vari-
ables that are neither observed nor hypothesis variables, makes the process more
opaque: some of these variables have a bigger impact (i.e., are more relevant) on
the eventual decision than others, and this information is lost in the process. For
example, the absence of a speciﬁc test result (i.e., a variable we marginalize out
in the MAP computation) may lead to a diﬀerent explanation of the available
evidence compared to when a negative (or positive) test result were present. In
this situation, this variable is more relevant to the eventual explanation than
if the best explanation would be the same, irrelevant of whether the test result
was positive, negative, or missing. Our approach in this paper is to motivate a
decision by showing which of these variables were relevant in this sense towards
arriving at this decision.
This perspective has roots in Pearl’s early work on conditional independence
[14]. Pearl suggests that human reasoning is in principle based on conditional
independence: The organizational structure of human memory is such that it
allows for easily retrieving context-dependent relevant information. For example
(from [14, p.3]): The color of my friend’s car is normally not related to the color
of my neighbour’s car. However, when my friend tells me she almost mistook my
neighbour’s car for her own, this information suddenly becomes relevant for me
to understand, and for her to explain, this mistake. That is, the color of both
cars is independent but becomes conditionally dependent on the evidence1.
1 Graphically one can see this as a so-called common-eﬀect structure, where C1 and
C2 are variables that represent my car’s, respectively my neighbour’s car’s, color;
both variables have a directed edge towards the variable M that indicates whether
my friend misidentiﬁed the cars or not. When M is unobserved, C1 and C2 are
independent, but they become conditionally dependent on observation of M.

Explainable AI Using MAP-Independence
245
In this paper we will argue that Pearl’s proposal to model context-dependent
(ir)relevance as conditional (in)dependence is in fact too strict. It generally leads
to too many variables that are considered to be relevant: for some it is likely the
case that, while they may not be conditionally independent on the hypothesized
explanation given the evidence, they do not contribute to understanding why
some explanation h is better than the alternatives. That means, for explana-
tory purposes their role is limited. In the remainder of this paper we will build
on Pearl’s work, yet provide a stronger notion of context-dependent relevance
and irrelevance of variables relative to explanations of observations. Our goal
is to advance explainable AI in the context of Bayesian networks by formaliz-
ing the problem of justiﬁcation of an explanation (i.e., given an AI-generated
explanation, advance the user’s understanding why this explanation is preferred
over others) into a computational problem that captures some aspects of this
justiﬁcation; in particular, by opening up the ‘marginalization black box’ and
show which variables contributed to this decision. We show that this problem is
intractable in the general case, but also give ﬁxed-parameter tractability results
that show what constraints are needed to render it tractable.
To summarize, we are interested in the potential applicability of this new
concept for motivation and justiﬁcation of MAP explanations, with a focus on
its theoretical properties. The remainder of this paper is structured as follows.
In the next section we oﬀer some preliminary background on Bayesian networks
and computational complexity and share our conventions with respect to nota-
tion with the reader. In Sect. 3 we introduce so-called MAP-independence as an
alternative to conditional independence and elaborate on the potential of these
computational problems for justifying explanations in Bayesian networks. In
Sect. 4 we introduce a formal computational problem based on this notion, and
give complexity proofs and ﬁxed-parameter tractability results for this problem.
We conclude in Sect. 5.
2
Preliminaries and Notation
In this section we give some preliminaries and introduce the notational conven-
tions we use throughout this paper. The reader is referred to textbooks like [2]
for more background.
A Bayesian network B = (GB, Pr) is a probabilistic graphical model that suc-
cinctly represents a joint probability distribution Pr(V) = n
i=1 Pr(Vi | π(Vi))
over a set of discrete random variables V. B is deﬁned by a directed acyclic graph
GB = (V, A), where V represents the stochastic variables and A models the
conditional (in)dependencies between them, and a set of parameter probabilities
Pr in the form of conditional probability tables (CPTs). In our notation π(Vi)
denotes the set of parents of a node Vi in GB. We use upper case to indicate
variables, lower case to indicate a speciﬁc value of a variable, and boldface to
indicate sets of variables respectively joint value assignments to such a set. Ω(Vi)
denotes the set of value assignments to Vi, with Ω(Va) denoting the set of joint
value assignment to the set Va.

246
J. Kwisthout
One of the key computational problems in Bayesian networks is the prob-
lem to ﬁnd the most probable explanation for a set of observations, i.e., a joint
value assignment to a designated set of variables (the explanation set) that has
maximum posterior probability given the observed variables (the joint value
assignment to the evidence set) in the network. If the network includes variables
that are neither observed nor to be explained (referred to as intermediate vari-
ables) this problem is typically referred to as MAP. We use the following formal
deﬁnition:
MAP
Instance: A Bayesian network B = (GB, Pr), where V(GB) is
partitioned into a set of evidence nodes E with a joint value assignment
e, a set of intermediate nodes I, and an explanation set H.
Output: A joint value assignment h∗to H such that for all joint value
assignments h′ to H, Pr(h∗| e) ≥Pr(h′ | e).
We assume that the reader is familiar with standard notions in computational
complexity theory, notably the classes P and NP, NP-hardness, and polynomial
time (many-one) reductions. The class PP is the class of decision problems that
can be decided by a probabilistic Turing machine in polynomial time; that is,
where yes-instances are accepted with probability strictly larger than 1/2 and
no-instances are accepted with probability no more than 1/2. A problem in PP
might be accepted with probability 1/2+ϵ where ϵ may depend exponentially on
the input size n. Hence, it may take exponential time to increase the probability
of acceptance (by repetition of the computation and taking a majority decision)
close to 1. PP is a powerful class; we know for example that NP ⊆PP and the
inclusion is assumed to be strict. The canonical PP-complete decision problem
is Majsat: given a Boolean formula φ, does the majority of truth assignments
to its variables satisfy φ?
In computational complexity theory, so-called oracles are theoretical con-
structs that increase the power of a speciﬁc Turing machine. An oracle (e.g.,
an oracle for PP-complete problems) can be seen as a ‘magic sub-routine’ that
answers class membership queries (e.g., in PP) in a single time step. In this
paper we are speciﬁcally interested in classes deﬁned by non-deterministic Turing
machines with access to a PP-oracle. Such a machine is very powerful, and like-
wise problems that are complete for the corresponding complexity classes NPPP
(such as MAP) and co-NPPP (such as Monotonicity) are highly intractable
[4,13].
Finally we wish to introduce the concept of ﬁxed-parameter tractability to the
reader. Oftentimes, NP-hard computational problems can be rendered tractable
when the set of instances is constrained to instances where the value of some (set
of) problem parameter(s) is bounded. Formally a parameterized problem k −Π
is called ﬁxed-parameter tractable for a parameter k if an instance i of Π can
be decided in time O(p(|i|)f(k)) for a polynomial p and an arbitrary function f.

Explainable AI Using MAP-Independence
247
3
MAP-Independence
The topic of relevance in Bayesian networks has been studied from several angles:
while [19] aimed to reduce the number of variables in the explanation set to the
relevant ones, and [12] studied the relevance of evidence variables for sensitivity
analysis, in [10] the approach was to reduce the number of intermediate variables
that aﬀect an inference to the best explanation. In the current paper we take a
similar approach, but here we focus on the application of this notion of relevance
in explainable AI, rather than to construct a heuristic approach towards the
computationally expensive MAP problem. Here the problem of interest is not
so much to ﬁnd the most probable explanation (viz., the joint value assignment
to a set of hypothesis variables given observations in the network), but rather
to motivate what information did or did not contribute to a given explanation.
That is, rather than providing the ‘trivial’ explanation “h∗is the best2 expla-
nation for e, since argmaxhPr(H = h | e) = argmaxh

i∈Ω(I) Pr(H = h, i | e) =
h∗” our goal is to partition the set I into variables I+ that are relevant to estab-
lishing the best explanation and variables I−that are irrelevant. One straight-
forward approach, motivated by [14], would be to include variables in I+ if they
are conditionally dependent on H given e, and in I−when they are conditionally
independent from H given e and to motivate the sets I+ and I−in terms of a
set of independence relations. This is particularly useful when inclusion in I+
is triggered by the presence of an observation, such as in Pearl’s example where
‘color of my friend’s car’ and ‘color of my neighbour’s car’ become dependent on
each other once we learn that my friend confused both cars.
We argue that this way of partitioning intermediate variables into relevant
and irrelevant ones, however useful, might not be the full story with respect
to explanation. There is a sense in which a variable has an explanatory role in
motivating a conclusion that goes beyond conditional (in)dependence. Take for
example the small binary network in Fig. 1. Assume that we want to motivate
the best explanation for A given the evidence C = c, i.e., we want to motivate the
outcome of argmaxaPr(A = a | C = c) = argmaxa

B,D Pr(A = a, B, D | C =
c) in terms of variables that contribute to this explanation. Now, obviously D
is not relevant, as it is d-separated from A given C. But the roles of B is less
obvious. This node is obviously not conditionally independent from A given C.
Whether B plays an explanatory role in general in the outcome of the MAP
query is dependent on whether argmaxa

D Pr(A = a, B = b, D | C = c) =
argmaxa

D Pr(A = a, B = ¯b, D | C = c). If both are equal (B’s value, were it
observed, would have been irrelevant to the MAP query) than B arguably has no
explanatory role. If both are unequal than the fact that B is unobserved may in
fact be crucial for the explanation. For example, if B represents a variable that
2 In this paper we do not touch the question whether ‘best’ is to be identiﬁed with
‘most probable’. The interested reader is referred to the vast literature on inference
to the best explanation such as [11], and more in particular to some of our ear-
lier work [9] that discusses the trade-oﬀbetween probability and informativeness of
explanations.

248
J. Kwisthout
encodes a ‘default’ versus ‘fault’ condition (with Pr(b) > Pr(¯b)) the absence of a
fault (i.e., B is unobserved) can lead to a diﬀerent MAP explanation than the
observation that B takes its default value; e.g., if Pr(b) = 0.6, Pr(a | c, b) = 0.6,
and Pr(a | c,¯b) = 0.3 we have that Pr(a | c) = 0.48 yet Pr(a | c, b) = 0.6 so the
MAP explanation changes from ¯a to a on the observation of the default value b.
This information is lost in the marginalization step but it helps motivate why ¯a
is the best explanation of c.
Fig. 1. An example small network. Note that the explanatory role of B in motivating
the best explanation of A given an observation for C is context-dependent and may be
diﬀerent for diﬀerent observations for C, as well as for diﬀerent conditional probability
distributions; hence it cannot be read oﬀthe graph alone.
Thus, the relevance of B for the explanation of A may need a diﬀerent
(and broader) notion of independence, as also suggested by [10]. Indeed, in
this example, variable D is irrelevant for explaining A as D is conditionally
independent from A given C; yet, we could also argue that B is irrelevant
for explaining A if its value, were it observed, could not inﬂuence the explana-
tion for A. We introduce the term MAP-independence for this (uni-directional)
relationship; we say that A is MAP-independent from B given C = c when
∀b∈Ω(B)argmaxa∗Pr(A = a∗, B = b | C = c) = a for a speciﬁc value assignment
a ∈Ω(A).
3.1
MAP-independence for Justiﬁcation and Decision Support
AI-based clinical decision support systems for diagnosis and treatment have been
proposed since the 1970s, with MYCIN [16] as the canonical example. Whereas
original systems were largely an eﬀort to demonstrate the promise of AI tech-
niques (i.e., isolated, diﬃcult to maintain or generalize, of mostly academic inter-
est, etc.), current systems have developed into systems that are integrated with
the medical workﬂow, in particular aligned with electronic health records [17].
However, several challenges that were already identiﬁed with MYCIN still remain
present in decision support systems: they are diﬃcult to maintain and adapt to
new insights, the justiﬁcation of the system’s advice does not match typical rea-
soning patterns of the user, and there is little justiﬁcation of the soundness (or
acknowledgement of uncertainty and ignorance) of the advice.

Explainable AI Using MAP-Independence
249
The concept of MAP-independence in Bayesian networks may help overcome
some of these shortcomings, particularly the justiﬁcation of an inference to the
most probable explanation. For an unobserved variable I we have that the MAP
explanation h∗is MAP-independent of I given the evidence if the explanation
would not have been diﬀerent had I be observed to some value, and MAP-
dependent if this is not the case. An explication of how I may impact or fail to
impact the most probable explanation of the evidence will both help motivate
the system’s advice as well as oﬀer guidance in further decisions (e.g., to gather
additional evidence [3,5] to make the MAP explanation more robust).
4
Formal Problem Deﬁnition and Results
The computational problem of interest is to decide upon the set I+, the relevant
variables that contribute to establishing the best explanation h∗given the evi-
dence e. In order to establish I+ we need to decide the following sub-problem:
given R ⊆I: is h∗MAP-independent from R given e? We formalize this problem
as below.
MAP-independence
Instance: A Bayesian network B = (GB, Pr), where V(G) is partitioned
into a set of evidence nodes E with a joint value assignment e, a
non-empty explanation set H with a joint value assignment h∗, a
non-empty set of nodes R for which we want to decide
MAP-independence relative to H, and a set of intermediate nodes I.
Question: Is ∀r∈Ω(R)argmaxHPr(H, R = r | e) = h∗?
Observe that the complement problem MAP-dependence is deﬁned similarly
with yes- and no-answers reversed.
4.1
Computational Complexity
We will show in this sub-section that an appropriate decision variant of MAP-
independence is co-NPPP-complete and thus resides in the same complexity
class as the Monotonicity problem [4]. Note that the deﬁnition of MAP-
independence in the previous sub-section had an explanation given in the input
and assumed that R is nonempty. The reason therefore is that if we would allow
R = ∅and leave out the explanation, the problem has MAP as a degenerate
special case. As this somewhat obfuscates the computational complexity of the
core of the problem (i.e., determining the relevance of the set R for the actual
explanation) we force R to be non-empty and h∗to be provided in the input.
Note that we did not explicitly require that h∗= argmaxh Pr(H = h, e); this
is neither necessary (for the hardness proof) nor desired (as it would eﬀectively
make MAP-independence a so-called promise problem).
Another complication is that, while MAP-independence is already deﬁned
as a decision problem, part of the problem deﬁnition requires comparing MAP

250
J. Kwisthout
explanations, and while the MAP problem has a decision variant that is NPPP-
complete, the functional variant is FPNPPP-complete [7]. We therefore introduce
the following decision variant3,4 which is in the line with the traditional decision
variant of Partial MAP:
MAP-independence-D
Instance: A Bayesian network B = (GB, Pr), where V(G) is partitioned
into a set of evidence nodes E with a joint value assignment e, a
non-empty explanation set H with a joint value assignment h∗, a
non-empty set of nodes R for which we want to decide
MAP-independence relative to H, and a set of intermediate nodes I;
rational number s.
Question: Is, for each joint value assignment r to R, Pr(h∗, r, e) > s?
For the hardness proof we reduce from the canonical satisﬁability co-NPPP-
complete variant A-Majsat [18] deﬁned as follows:
A-Majsat
Instance: A Boolean formula φ with n variables {x1, xn}, partitioned
into the sets A = {x1, xk} and M = {xk+1, xn} for some k ≤n.
Question: Does, for every truth instantiation xa to A, the majority of
truth instantiations xm to M satisfy φ?
As a running example for our reduction we use the formula φex = ¬(x1 ∧
x2) ∨(x3 ∨x4), with A = {x1, x2} and M = {x3, x4}. This is a yes-instance
of A-Majsat: for each truth assignment to A, at least three out of four truth
assignments to M satisfy φ.
We construct a Bayesian network Bφ from a given Boolean formula φ with
n variables. For each propositional variable xi in φ, a binary stochastic variable
3 Note that as a decision variant of MAP-independence there is still a slight caveat,
as the probability of Pr(h∗, r, e) can be diﬀerent for each joint value assignment r,
implying that the ‘generic’ threshold s can either be too strict (h is still the MAP
explanation although the test fails) or too loose (there is another explanation h′
which is the MAP explanation although the test passes). As the number of joint
value assignments |Ω(R)| can be exponential in the size of the network (and thus we
cannot include individual thresholds si in the input of the decision problem without
blowing up the input size), this nonetheless appears to be the closest decision problem
variant that still captures the crucial aspects of MAP-independence.
4 One of the reviewers asked whether a ‘suitable’ (neither too loose nor too strict)
threshold s can (or is guaranteed) to exist. A trivial example with two unconnected
binary nodes H (the MAP variable, with Pr(H = h) = 0.51) and uniformly dis-
tributed R, and a threshold s = 0.5 suﬃces to show that indeed there exists net-
works where a suitable s exists. On the other hand it is also easy to construct
a small network where no suitable s exists; take a ternary node H instead and
deﬁne Pr(H = h1 | r) = 0.55, Pr(H = h2 | r) = 0.45, Pr(H = h3 | r) = 0,
Pr(H = h1 | ¯r) = 0.45, Pr(H = h2 | ¯r) = 0.4, and Pr(H = h3 | r) = 0.15. While h1
is the MAP for each value of R, a threshold of 0.44 is too loose while a threshold of
0.45 is too strict.

Explainable AI Using MAP-Independence
251
Xi is added to Bφ, with possible values T and F and a uniform probability
distribution. For each logical operator in φ, an additional binary variable in Bφ
is introduced, whose parents are the variables that correspond to the input of
the operator, and whose conditional probability table is equal to the truth table
of that operator. For example, the value T of a stochastic variable mimicking
the and-operator would have a conditional probability of 1 if and only if both its
parents have the value T, and 0 otherwise. The top-level operator in φ is denoted
as Vφ. In Fig. 2 the network Bφ is shown for the formula ¬(x1 ∧x2) ∨(x3 ∨x4).
Fig. 2. The network Bφ created from the A-Majsat instance (φ, {x1, x2}, {x3, x4}) per
the description above.
Theorem 1. MAP-independence is co-NPPP-complete.
Proof. To prove membership in co-NPPP, we give a falsiﬁcation algorithm for no-
answers to MAP-independence-D instances, given access to an oracle for the
PP-complete [1] Inference problem. Let (B, E, e, H, h∗, R, I, s) be an instance
of MAP-independence-D. We non-deterministically guess a joint value assign-
ment ¯r, and use the Inference oracle to verify that Pr(h∗,¯r, e) ≤s, which by
deﬁnition implies that H is not MAP-independent from R given E = e.
To prove hardness, we reduce from A-Majsat. Let (φ, XA, XM) be an
instance of A-Majsat and let Bφ be the Bayesian network created from φ
as per the procedure described above. We set R = XA, H = {Vφ}, E = ∅,
I = V(GB) \ R ∪{Vφ}, h∗= {T}, and s = 2−|R|−1.

252
J. Kwisthout
=⇒Assume that (φ, XA, XM) is a yes-instance of A-Majsat, i.e., for
every truth assignment to XA, the majority of truth assignments to
XM satisﬁes φ. Then, by the construction of Bφ, we have 
r Pr(Vφ =
T, r) > 1/2 and so, as the variables in R are all uniformly distributed,
Pr(Vφ = T, r) > 2−|R|−1 for every joint value assignment r to R, and
so this is a yes-instance of MAP-independence-D.
⇐= Assume that (B, ∅, ∅, Vφ, T, R, I, 2−|R|−1) is a yes-instance of MAP-
independence-D. Given the construction this implies that for all
joint value assignments r it holds that Pr(Vφ = T, r) > 2−|R|−1. But
this implies that for all truth assignments to XA, the majority of
truth assignments to XM satisﬁes φ, hence, this is a yes-instance of
A-Majsat.
Observe that the construction of Bφ takes time, polynomial in the size of φ, which
concludes our proof. Furthermore, the result holds in the absence of evidence.
Corollary 1. MAP-dependence is NPPP-complete.
4.2
Algorithm and Algorithmic Complexity
To decide whether a MAP explanation h∗is MAP-independent from a set of
variables R given evidence e, the straightforward algorithm below shows that
the run-time of this algorithm is O(Ω(R)) = O(2|R|) times the time needed for
each MAP computation.
Algorithm 1: Straightforward MAP-independence algorithm
Input: Bayesian network partitioned in E = e, H = h∗, R, and I.
Output: yes if H is MAP-independent from R given e, no if otherwise.
foreach r ∈Ω(R) do
if argmaxHPr(H, R = r | e) ̸= h∗then
return no;
end
end
return yes;
This implies that, given known results on ﬁxed-parameter tractability [8]
and eﬃcient approximation [10,13] of MAP, the size of the set against which
we want to establish MAP independence is the crucial source of complexity if
MAP can be computed or approximated feasibly. The following ﬁxed-parameter
tractability results can be derived:
Corollary 2. Let c = maxW ∈V (G) Ω(W), q = Pr(h∗), and let tw be the tree-
width of B. Then p-MAP-independence is ﬁxed-parameter tractable for p =
{|H|, |R|, tw, c}, p = {|H|, |R|, |I|, c}, p = {q, |R|, tw, c}, and p = {q, |R|, |I|, c}.

Explainable AI Using MAP-Independence
253
5
Conclusion and Future Work
In this paper we introduced MAP-independence as a formal notion, relevant for
decision support and justiﬁcation of decisions. In a sense, MAP-independence is
a relaxation of conditional independence, suggested by Pearl [14] to be a scaﬀold
for human context-dependent reasoning. We suggest that MAP-independence
may be a useful notion to further explicate the variables that are relevant for the
establishment of a particular MAP explanation. Establishing whether the MAP
explanation is MAP-independent from a set of variables given the evidence (and
so, whether these variables are relevant for justifying the MAP explanation) is a
computationally intractable problem; however, for a speciﬁc variable of interest
I (or a small set of these variables together) the problem is tractable whenever
MAP can be computed tractably; in practice, this may suﬃce for usability in
typical decision support systems.
There are many related problems of interest that one can identify, but which
will be delegated to future work. For example, if the set of relevant variables is
large, one might be interested in deciding whether observing one variable can
bring down this set (by more than one, obviously). Another related problem
would be to decide upon the observations that are relevant for the MAP expla-
nation (i.e., had we not observed E ∈E or had we observed a diﬀerent value,
would that change the MAP explanation?). This would extend previous work
[12] where the relevance of E for computing a posterior probability (conditioned
on E) was established. Finally, in order to test its practical usage, the formal
concept introduced in this paper should be put to the empirical test in an actual
decision support system to establish whether the justiﬁcations supported by
the notion of MAP-independence actually help understand and appreciate the
system’s advise.
Acknowledgments. The author is grateful for the valuable feedback received from
Nils Donselaar on an earlier version of this paper, and acknowledges the highly con-
structive and thorough comments from the anonymous reviewers that helped improve
this paper.
References
1. Cooper, G.F.: The computational complexity of probabilistic inference using
bayesian belief networks. Artif. Intell. 42(2–3), 393–405 (1990)
2. Darwiche, A.: Modeling and Reasoning with Bayesian Networks. Cambridge Uni-
versity Press (2009)
3. van der Gaag, L., Bodlaender, H.: On stopping evidence gathering for diagnos-
tic Bayesian networks. In: European Conference on Symbolic and Quantitative
Approaches to Reasoning and Uncertainty, pp. 170–181 (2011)
4. van der Gaag, L., Bodlaender, H., Feelders, A.: Monotonicity in Bayesian networks.
In: Chickering, M., Halpern, J. (eds.) Proceedings of the Twentieth Conference on
Uncertainty in Artiﬁcial Intelligence, pp. 569–576. AUAI press, Arlington (2004)
5. van der Gaag, L., Wessels, M.: Selective evidence gathering for diagnostic belief
networks. AISB Q. 86, 23–34 (1993)

254
J. Kwisthout
6. Gunning, D., Steﬁk, M., Choi, J., Miller, T., Stumpf, S., Yang, G.: XAI–explainable
artiﬁcial intelligence. Science Robotics 4(37) (2019)
7. Kwisthout, J.: Complexity results for enumerating MPE and partial MAP. In:
Jaeger, M., Nielsen, T. (eds.) Proceedings of the Fourth European Workshop on
Probabilistic Graphical Models, pp. 161–168 (2008)
8. Kwisthout, J.: Most probable explanations in bayesian networks: complexity and
tractability. Int. J. Approximate Reasoning 52(9), 1452–1469 (2011)
9. Kwisthout, J.: Most inforbable explanations: ﬁnding explanations in bayesian net-
works that are both probable and informative. In: van der Gaag, L.C. (ed.)
ECSQARU 2013. LNCS (LNAI), vol. 7958, pp. 328–339. Springer, Heidelberg
(2013). https://doi.org/10.1007/978-3-642-39091-3 28
10. Kwisthout, J.: Tree-width and the computational complexity of MAP approxima-
tions in Bayesian networks. J. Artif. Intell. Res. 53, 699–720 (2015)
11. Lipton, P.: Inference to the Best Explanation. Routledge, London (1991)
12. Meekes, M., Renooij, S., van der Gaag, L.C.: Relevance of evidence in bayesian
networks. In: Destercke, S., Denoeux, T. (eds.) ECSQARU 2015. LNCS (LNAI),
vol. 9161, pp. 366–375. Springer, Cham (2015). https://doi.org/10.1007/978-3-319-
20807-7 33
13. Park, J.D., Darwiche, A.: Complexity results and approximation settings for MAP
explanations. J. Artif. Intell. Res. 21, 101–133 (2004)
14. Pearl, J., Paz, A.: GRAPHOIDS: a graph-based logic for reasoning about relevance
relations. Technical report R-53-L, UCLA Computer Science Department (1987)
15. Ras, G., van Gerven, M., Haselager, P.: Explanation methods in deep learning:
users, values, concerns and challenges. In: Escalante, H.J., et al. (eds.) Explainable
and Interpretable Models in Computer Vision and Machine Learning. TSSCML,
pp. 19–36. Springer, Cham (2018). https://doi.org/10.1007/978-3-319-98131-4 2
16. Shortliﬀe, E., Buchanan, B.: A model of inexact reasoning in medicine. Math.
Biosci. 379, 233–262 (1975)
17. Sutton, R., Pincock, D., Baumgart, D., Sadowski, D., Fedorak, R., Kroeker, K.:
An overview of clinical decision support systems: beneﬁts, risks, and strategies for
success. NPJ Digital Med. 3(1), 1–10 (2020)
18. Wagner, K.W.: The complexity of combinatorial problems with succinct input
representation. Acta Informatica 23(3), 325–356 (1986)
19. Yuan, C., Lim, H., Lu, T.: Most relevant explanation in Bayesian networks. J.
Artif. Intell. Res. 42(1), 309–352 (2011)

Bayesian Networks for the Test Score
Prediction: A Case Study on a Math
Graduation Exam
Martin Plajner1 and Jiˇr´ı Vomlel1,2(B)
1 Institute of Information Theory and Automation, Czech Academy of Sciences,
Pod Vod´arenskou vˇeˇz´ı 4, Prague 8, Czechia
vomlel@utia.cas.cz
2 Faculty of Management, University of Economics, Prague, Jaroˇsovsk´a 1117/II,
377 01 Jindˇrich˚uv Hradec, Czechia
Abstract. In this paper we study the problem of student knowledge
level estimation. We use probabilistic models learned from collected data
to model the tested students. We propose and compare experimentally
several diﬀerent Bayesian network models for the score prediction of stu-
dent’s knowledge. The proposed scoring algorithm provides not only the
expected value of the total score but the whole probability distribution
of the total score. This means that conﬁdence intervals of predicted total
score can be provided along the expected value. The key that enabled
eﬃcient computations with the studied models is a newly proposed infer-
ence algorithm based on the CP tensor decomposition, which is used for
the computation of the score distribution. The proposed algorithm is two
orders of magnitude faster than a state of the art method. We report
results of experimental comparisons on a large dataset from the Czech
National Graduation Exam in Mathematics. In this evaluation the best
performing model is an IRT model with one continuous normally dis-
tributed skill variable related to all items by the graded response models.
The second best is a multidimensional IRT model with an expert struc-
ture of items-skills relations and a covariance matrix for the skills. This
model has a higher improvement with larger training sets and seems to
be the model of choice if a suﬃciently large training dataset is available.
Keywords: Bayesian networks · Educational testing · Score
prediction · Eﬃcient probabilistic inference · Multidimensional IRT ·
CP tensor decomposition
1
Introduction
In this paper we study the problem of student knowledge level estimation. For
this problem we use probabilistic models learned from collected data. When a
model is applied to testing a particular student the model is updated by items
answered by the tested student so far and the updated model is used to select the
c
⃝Springer Nature Switzerland AG 2021
J. Vejnarov´a and N. Wilson (Eds.): ECSQARU 2021, LNAI 12897, pp. 255–267, 2021.
https://doi.org/10.1007/978-3-030-86772-0_19

256
M. Plajner and J. Vomlel
next item to be answered by the tested student. This concept is often referred
as computerized adaptive testing (CAT), see, e.g. [11,20].
In their seminal work [2] Almond and Mislevy pointed out that the students
can be modelled well using Bayesian networks [9,10,14]. This idea was further
elaborated in [1,19]. All probabilistic models studied in this paper can be con-
sidered Bayesian networks (BNs) in a broader sense.
The paper is organized as follows. In Sect. 2 we describe probabilistic models
that can be used to model student knowledge. In Sect. 3 we explain how the
probabilistic inference with these models can be performed. Section 4 represents
the main theoretical contribution of the paper, which is an eﬃcient method for
the score computation based on the CP tensor decomposition. In Sect. 5, which
presents the main experimental contribution of the paper, we report results of
experimental comparisons using a large dataset from the Czech National Grad-
uation Exam in Mathematics. The results are summarized in Sect. 6.
2
Models
We will model the problem using models sharing four groups of variables:
– Skills (in some models called Factors), denoted S1, . . . , Sm are unobserved
(hidden) variables used to specify student abilities (skills). They will be either
binary variables, discrete ordinal variables, or continuous variables depending
on the model used.
– Items (also called Questions or Tasks), denoted X1, . . . , Xn will be discrete
ordinal variables whose states correspond to the number of points received
for the answer. We will assume the states (points) are from a set of integers
{0, 1, . . . , z −1}. When testing a particular student we will estimate their
probability distributions using the collected evidence.
– Answered items are a subset of copies of items X1, . . . , Xn. An answered item
is observed for the currently tested student. Answered item corresponding to
an item Xi, i ∈{1, . . . , n} is denoted X′
i. Conditional probability of each X′
i
given the skills is by deﬁnition the same as for their corresponding item Xi.
The answered items are used to propagate collected evidence to the model1.
– The total score node, denoted Y , which represents the total sum of scores from
all questions2. The values of Y are all possible total sums of items’ values of
X1, . . . , Xn. For example, if all items are binary (taking values 0 and 1) then
Y has n + 1 values (0, 1, . . . , n).
1 For each answered item X′
i its corresponding item Xi is generally still uncertain.
Items Xi represent certain type of a question or a task that can be repeated and
the result need not to be the same for the same student, i.e., we estimate student’s
skills and admit mistakes even if he/she has required skills and vice versa.
2 The score we estimate is not the score the tested student gets at the end of the
current test but in another test of the same type. Of course, the model can also
provide the estimate of the test score in the current test – in this case, another score
node, say Y ′, would have nodes X′
i as its parents instead. In this case all methods
would necessarily have the zero score prediction error at the end of the test.

Bayesian Networks for the Test Score Prediction
257
The general structure of all models will be similar albeit they will diﬀer by the
number of skill variables, by the type of skill variables, and by a possible direct
dependence among skills represented in the model. In Fig. 1 and Fig. 2 we give
examples of two types of considered models.
The BN model in Fig. 1 has three independent skills. These skills inﬂuence
scores of items. It is assumed that for each item a domain expert decided which
skills are relevant. In the model of Fig. 1 skill S1 inﬂuences item X1, skill S2
inﬂuences both X2 and X3 items, and skill S3 also inﬂuences both items X2 and
X3 but also item X4. The items X1, X2, X3, and X4 are never observed while
items X′
1, X′
2, X′
3, and X′
4 represent observed items and are included in the
model only if they are observed. If they are not observed then they are omitted
from the model since they represent barren variables.
Fig. 1. An expert BN model with independent skills
Another BN model is presented in Fig. 2. This diﬀers from the BN model
in Fig. 1 in two aspects. First, in this models all items are related to all skills.
Second, the skills are dependent. Typically, this model would be learned from
collected data without using expert knowledge.
The models discussed above can be combined. For example, expert knowledge
of relations of items to skills from the BN model of Fig. 1 can be used to reduce
the number of edges in the BN model of Fig. 2.
In this paper we will compare following models:
– An expert model with independent skills (Fig. 1 type). Skills are represented
by ordinal variables with three states. Items are related to their parent skills
by general conditional probability tables restricted by monotonicity condi-
tions [5]. The model parameters will be learned using restricted gradient
method [15]. This model will be referred as rgrad. We should note that other

258
M. Plajner and J. Vomlel
Fig. 2. A BN model with dependent skills related to all items
methods for learning conditional probability tables satisfying monotonicity
conditions could be also used, e.g. the isotonic regression EM by Masegosa
et al. [12]. We decided to report results of the restricted gradient method in
this paper since it provided best results in preliminary experiments [16].
– A model with one continuous skill variable related to all items. The skill
variable is assumed to have Gaussian distributions and items are related to
the skill node by the graded response models (GRMs) [17]. This model belongs
to the well-known family of IRT models and, thus, it will be referred as irt.
– An expert model with the same structure as rgrad but the skill variables are
continuous, each having the Gaussian distributions and the items are related
to skills by GRMs as in IRT. This model can be considered as an example
of a multidimensional IRT [8] with the zero covariance among skills (often
called factors in IRT). Therefore it will be referred as mirt-cov0.
– An expert model with the same structure as rgrad with continous skill vari-
ables which diﬀers from mirt-cov0 only by including relation between skills
represented by multidimensional Gaussian distribution with a covariance
matrix allowing nonzero non-diagonal elements. This model will be referred
as mirt-cov1.
– The last group of models has the structure of model from Fig. 2, i.e., all
items are related to all skills and the skills are dependent. It is a straightfor-
ward generalization of irt to more skills (factors). Skills are assumed to have
multidimensional Gaussian distribution with the covariance matrix allowing
nonzero non-diagonal elements. Depending on the number of factors these
models will be referred as mirt-2F, mirt-3F, and mirt-4F, respectively.
The model parameters of the IRT and the multidimensional IRT models will be
learned using algorithms implemented in the R mirt package [4].

Bayesian Networks for the Test Score Prediction
259
The joint probability distribution of these models is deﬁned by their structure
represented by directed acyclic graph and their conditional probability distribu-
tions of each node given its parents as:
P(Y |X1, . . . , Xn) ·
n

i=1
P(Xi|pa(Xi)) ·
n

i=1
P(X′
i|pa(X′
i)) ·
m

j=1
P(Sj|pa(Sj)) .
3
Probabilistic Inference
Models introduced in Sect. 2 will be used to estimate the probability distribution
of the total score given the items answered so far. We will use symbol I to denote
the set of indexes of already answered items X′
i. To simplify notation we will use
symbol e to denote the evidence collected so far, i.e., e = {X′
i = x′
i}i∈I.
The probability distribution of the total score given evidence e is (in case of
discrete skill nodes) computed as:
P(Y |e) =

X1,...,Xn

S1,...,Sm
P(Y, X1, . . . , Xn, S1, . . . , Sm|e) .
For the continuous skill nodes the integrals are used instead of the summation.
Using the general structure of the model this can be rewritten as:
P(Y |e) =

X1,...,Xn

S1,...,Sm
P(Y |X1, . . . , Xn) · Q(X1, . . . , Xn, S1, . . . , Sm|e)
where
Q(X1, . . . , Xn, S1, . . . , Sm|e) =
n

i=1
P(Xi|pa(Xi)) · R(S1, . . . , Sm|e)
R(S1, . . . , Sm|e) ∝

i∈I
P(X′
i = x′
i|pa(X′
i)) ·
m

j=1
P(Sj|pa(Sj))
and the conditional probability distribution
P(Y = y|X1 = x1, . . . , Xn = xn) =

1 if y = n
i=1 xi
0 otherwise
is deterministic and represents the distribution of the total sum of items’ values.
In all considered models the values of items are dependent through the skills.
Therefore we cannot sum out the skills for each item independently and then
compute the probability distribution of the total sum. On the other hand, if a
skill conﬁguration is ﬁxed, say it is a vector s = (s1, . . . , sm), then items become
independent. This means that for each conﬁguration of skills we can write
Q(X1, . . . , Xn, S1 = s1, . . . , Sm = sm|e) =
n

i=1
P(Xi|s) · R(s|e) .

260
M. Plajner and J. Vomlel
Using the above formula and denoting by x the vector (x1, . . . , xn), by X =
×n
i=1Xi the set of all conﬁgurations of x, and by S the set of all conﬁgurations
of all skills we get:
P(Y |e) =

s∈S

x∈X
P(Y |X1 = x1, . . . , Xn = xn) ·
n

i=1
P(Xi = xi|s)

· R(s|e) .
In case of continuous skill nodes we will approximate the integrals over skills
by a suﬃciently large ﬁnite set of skill conﬁgurations s = (s1, . . . , sm) chosen so
that they cover well the space of skill values. For this purpose, we will use Halton
sequences [6] in m dimensions to generate points in the Cartesian product S of
skills’ state spaces3. Although these sequences are deterministic, they are of low
discrepancy, i.e. they cover well the space S. Thus, we can write:
P(Y |e) =

s∈S
P(Y |s) · R(s|e) , where
(1)
P(Y |s) =

x∈X
P(Y |X1 = x1, . . . , Xn = xn) ·
n

i=1
P(Xi = xi|s) .
(2)
Now, the only remaining obstacle for eﬃcient inference is the conditional dis-
tribution P(Y |X1 = x1, . . . , Xn = xn), which can be very large since, typically,
the models contain tens to hundreds of items. An eﬃcient transformation of this
distribution will be the topic of the next section.
4
Eﬃcient Inference Method for the Score Computation
In this section we present a computationally eﬃcient method that will be used
to compute the probability distribution of the total score. It is based on the CP
tensor decomposition [3,7] and can be also viewed as an application of Discrete
Fourier Transformation (DFT). As we will show latter it is several orders of
magnitude faster than a standard probabilistic inference approach based on,
so called, parent divorcing [13]. The proposed method is especially useful for
discrete random variables that have a large number of parents. This is the case
of our case study since the total score variable has 37 discrete valued parents.
We assume each of n items takes values (points) from the set {0, 1, . . . , z−1}.
Therefore the maximum possible total score is n(z −1) and the total number
of states of Y is4 k = 1 + n(z −1). In the derivation of our algorithm we will
exploit Theorem 3 and its proof given in [18]. From this result it follows that for
all y, x1, . . . , xn we can write5:
3 In order to use the same formula for both discrete and continuous skills we will use
S to denote the set of skill conﬁgurations also in the continuous case.
4 Please, note we include also the state 0.
5 Please, note that the upper index m of αm
j , j = 1, . . . , k. represents the power of αj.

Bayesian Networks for the Test Score Prediction
261
P(Y = y|X1 = x1, . . . , Xn = xn)
=
k

b=1
αx1
b · . . . · αxn
b
· βb,y
=
k

b=1
αx1+...+xn
b
· βb,y ,
(3)
where α1, . . . , αk are pairwise distinct real or complex numbers and values of
βb,y are solutions of the system of linear equations
Y = AX ,
(4)
where Y is the k×k identity matrix, A is the k×k Vandermonde matrix deﬁned
by a vector α = (α1, . . . , αk) as
A =

αj
i
	k,k−1
i=1,j=0
=
⎛
⎜
⎜
⎝
α0
1
α0
2
. . .
α0
k
α1
1
α1
2
. . .
α1
k
. . .
α(k−1)
1
α(k−1)
2
. . . α(k−1)
k
⎞
⎟
⎟
⎠,
and X is the matrix deﬁned as
X = {βb,y}k,k−1
b=1,y=0
=
⎛
⎜
⎜
⎝
β1,0
β1,1 . . . β1,k−1
β2,0
β2,1 . . . β1,k−1
. . .
βk,0
βk,1 . . . βk,k−1
⎞
⎟
⎟
⎠.
Using the standard notation for probability tables we can write the Eq. (3) in a
compact form as
P(Y |X1, . . . , Xn) =

B
P(Y, B) ·
n

i=1
P(Xi, B) ,
(5)
where for all values b, xi, y of variables B, Xi, Y , respectively, it holds:
P(Y = y, B = b) = βb,y
and
P(Xi = xi, B = b) = αxi
b
.
To achieve a good numerical stability for large dimensional problems (i.e., with
high values of n) it is wise to use complex valued coeﬃcients α1, . . . , αk. We
deﬁne them as the complex numbers from the unit circle (roots of unity) in the
space of complex numbers, i.e., for j = 1, . . . , k:
αj = exp

j · 2πi
k

,
(6)
where i is the imaginary unit, satisfying the equation i2 = −1. This brings also
a very nice beneﬁt since, in this case, the solution of the system of linear Eq. (4)
is well-known:
X = 1
k · AT ,
where AT denotes the transpose of A and AT denotes the matrix with complex
conjugated entries of AT .

262
M. Plajner and J. Vomlel
Remark 1. Vandermonde matrix A with entries speciﬁed by Eq. (6) is actually
the Discrete Fourier Transformation (DFT) matrix.
Now we are ready to specify the ﬁnal step of the probabilistic inference
discussed in Sect. 3. In formula (2) we replace the conditional probability distri-
bution of Y by the expression speciﬁed by formula (5)
P(Y |s) =

x∈X

b∈{1,...,k}
P(Y, B = b) ·
n

i=1
P(Xi = xi, B = b) ·
n

i=1
P(Xi = xi|s)
=

b∈{1,...,k}
P(Y, B = b) ·
n

i=1

xi∈Xi
P(Xi = xi, B = b) · P(Xi = xi|s)
=

b∈{1,...,k}
βb,y ·
n

i=1

xi∈Xi
αxi
b · P(Xi = xi|s) .
Fig. 3. Scoring algorithm for a given skill conﬁguration
In Fig. 3 we present6 the algorithm for the total score computation for a given
skill conﬁguration s. The algorithm input is speciﬁed in z ×n matrix7 P deﬁned
by distributions P(Xi|s) so that the element of jth row and ith column of matrix
P corresponds to the value P(Xi = j|s) of distribution P(Xi|s). The output y
of the algorithm is P(Y |s) – the probability distribution of the total score for
the given s.
6 We we use the notation of R, in which we implemented the algorithm and which we
believe is self-explanatory.
7 The number of rows z is the maximum item value computed over all items plus one.
If a value is impossible then the corresponding matrix entry is set to zero.

Bayesian Networks for the Test Score Prediction
263
Proposition 1. The algorithm presented in Fig. 3 computes the probability dis-
tribution of the total score P(Y |s) in O(nz) time.
Proof. The proof follows from the discussion above.
To estimate the total score of a particular student the scoring algorithm
given a skill conﬁguration s is repeated for each conﬁguration s of student’s
skills, which can be thousands or more. Therefore, the complexity of the scor-
ing algorithm represents an important issue. The complexity of the presented
algorithm is linear with respect to the number of possible total scores.
In Fig. 4 we compare the computational CPU time of the presented scoring
algorithm based on the CP tensor decomposition with the state of the art method
– the parent divorcing method [13]. In the ﬁgure the horizontal axis corresponds
to the number of items that are parents of the score node. The items corresponds
to items from the model of the Czech National Graduation Exam in Mathematics
described in Sect. 5. They are added one by one in the ascending order. Most
items have two states only, but some of them have more than two states (the
maximum is four). The vertical axis corresponds to the total CPU time of one
thousand of actual computations of the score for the given number of items8.
We can see that the scoring algorithm based on the CP tensor decomposition is
two orders of magnitude faster than the parent divorcing method.
number of parents
inference time [s]
5
10
15
20
25
30
35
0.1
1.0
10.0
100.0
parent divorcing
CP tensor decomposition
Fig. 4. Comparisons of the computational time of the scoring algorithms
5
Experimental Model Comparisons
In this section we report results of experimental comparisons performed using a
large dataset from the Czech National Graduation Exam in Mathematics. This
8 Please, note the vertical axis has a logarithmic scale.

264
M. Plajner and J. Vomlel
dataset contains answers from more than 20,000 students who took this test in
the year 2015. We randomly selected training subsets of diﬀerent sizes and a
testing subset consisting of answers of 100 students9. The test contains 37 items,
most of them scored with either 0 or 1 point, the highest number of points for
one item is 3. The expert model contains 8 skill nodes and is described in [15].
Model comparisons are presented in Fig. 5. We evaluate the score prediction
quality during adaptive tests performed for each of 100 students from the testing
set. The criteria used for evaluation was the average absolute diﬀerence of the
expected score computed by each model and the true score. The items were
selected for each student using an adaptive criteria. To get comparable results
we used the same sequence of items for all models. The sequence was established
using the rgrad model with the expected information gain criteria10.
From the plots we can clearly see that the best performing algorithm is irt
for both sizes of the training dataset11. The second best performing algorithm
was mirt-COV1. This is the algorithm that uses the expert structure for items-
skills relations and includes relations between skills. It is important to note that
the improvement with larger training set is larger for mirt-COV1 than for irt.
Generally, more complex models seem to require more training data than irt to
achieve better performance. Also, expert models that do not represent depen-
dence between skills (mirt-COV0 and rgrad) have worse performance than the
corresponding expert model representing this dependence (mirt-COV1). This
indicates that the current expert model rgrad could be probably improved by
including such dependencies explicitly in the model. Finally, from models with
more than one skill those that have the expert structure for items-skills relations
perform better than those that have not.
Since the scoring algorithm provides not only the expected value of the total
score but the whole probability distribution of the total score it can be used to
provide conﬁdence intervals of predicted total score as well. In Fig. 6 we present
an example of 95% conﬁdence interval for one student from the testing set using
the irt model learned from 160 students’ records from training data.
6
Discussion
In this paper we presented an experimental comparison of diﬀerent Bayesian
network models for score prediction during an adaptive test of student’s knowl-
edge. The key that enabled eﬃcient computation was a new inference algorithm
used for computation of the score distribution. We are not aware of any other
9 Of course, the selection process ensured that the datasets were disjoint.
10 Albeit the item sequences might slightly diﬀer for diﬀerent models we do not expect
this has a signiﬁcant impact on models’ performance. The score prediction is inde-
pendent of the order of items in the sequence if the items are the same.
11 All claims about algorithms’ performance were veriﬁed using the Wilcoxon rank test
with signiﬁcance level 0.01. Actually, the probability of the alternative hypothesis
was always lower than 10−15.

Bayesian Networks for the Test Score Prediction
265
0
10
20
30
0
2
4
6
8
10
number of answered questions
average absolute score prediction error
rgrad
irt
mirt−COV0
mirt−COV1
mirt−2F
mirt−3F
mirt−4F
0
10
20
30
0
2
4
6
8
10
number of answered questions
average absolute score prediction error
rgrad
irt
mirt−COV0
mirt−COV1
mirt−2F
mirt−3F
mirt−4F
Fig. 5. Model comparisons for the training set consisting of 160 and 640 students,
respectively.

266
M. Plajner and J. Vomlel
0
10
20
30
0
10
20
30
40
50
number of answered questions
expected score
expected score
true score
95% confidence interval
Fig. 6. Expected total score and 95% conﬁdence interval for one student from the
testing set.
work that would compare several diverse student models as those discussed in
this paper on a real data.
The models presented in this paper were tested solely with respect to their
ability to predict the total score. It should be noted that another important task
in educational testing is skill analysis. For example, the estimation of the level
of student’s skills plays an important role in intelligent tutoring systems. Only
some of the tested models contain skills with a comprehensible interpretation. In
rgrad, mirt-cov0, and mirt-cov1 experts created the model structures with such
an interpretation in mind. On the other hand, skills in models mirt-2F, mirt-3F,
and mirt-4F are truly hidden variables and do not have any clear interpretation.
In the speciﬁc case of the irt model the skill node can be interpreted as a general
ability to answer/solve the modeled items.
Since the data used in our study do not provide information about pres-
ence/absence of skills of a student we cannot test directly models’ skill prediction
quality. However, we can assume that skill and total score prediction quality are
related. We expect that best score prediction models with expert-identiﬁed skills
can be used also as good skill analysis models.
Acknowledgements. We would like to thank anonymous reviewers for their com-
ments and suggestions that helped us to improve the paper. This work was supported
by the Czech Science Foundation, Project No. 19-04579S.
References
1. Almond, R.G., Mislevy, R.J., Steinberg, L.S., Yan, D., Williamson, D.M.: The
future of Bayesian networks in educational assessment. In: Bayesian Networks in
Educational Assessment. SSBS, pp. 583–599. Springer, New York (2015). https://
doi.org/10.1007/978-1-4939-2125-6 16

Bayesian Networks for the Test Score Prediction
267
2. Almond, R.G., Mislevy, R.J.: Graphical models and computerized adaptive testing.
Appl. Psychol. Meas. 23(3), 223–237 (1999)
3. Carroll, J.D., Chang, J.J.: Analysis of individual diﬀerences in multidimensional
scaling via an n-way generalization of Eckart-Young decomposition. Psychometrika
35, 283–319 (1970)
4. Chalmers, R.P.: mirt: a multidimensional item response theory package for the R
environment. J. Statist. Softw. 48(6), 1–29 (2012). https://doi.org/10.18637/jss.
v048.i06
5. van der Gaag, L.C., Bodlaender, H.L., Feelders, A.J.: Monotonicity in Bayesian
networks. In: Proceedings of the Twentieth Conference on Uncertainty in Artiﬁcial
Intelligence, pp. 569–576. AUAI Press (2004)
6. Halton, J.H.: Algorithm 247: radical-inverse quasi-random point sequence. Com-
mun. ACM 7(12), 701–702 (1964). https://doi.org/10.1145/355588.365104
7. Harshman, R.A.: Foundations of the PARAFAC procedure: models and conditions
for an “explanatory” multi-mode factor analysis. UCLA Working Pap. Phonetics
16, 1–84 (1970)
8. Hartig, J., H¨ohler, J.: Multidimensional IRT models for the assessment of compe-
tencies. Stud. Educ. Eval. 35(2), 57–63 (2009). https://doi.org/10.1016/j.stueduc.
2009.10.002
9. Jensen, F.V.: Bayesian Networks and Decision Graphs. Springer, New York (2001).
https://doi.org/10.1007/978-1-4757-3502-4
10. Lauritzen, S.L., Spiegelhalter, D.J.: Local computations with probabilities on
graphical structures and their application to expert systems (with discussion). J.
Roy. Stat. Soc. B 50, 157–224 (1988)
11. Jeevan, M., Dhingra, A., Hanmandlu, M., Panigrahi, B.K.: Robust speaker veri-
ﬁcation using GFCC based i-vectors. In: Lobiyal, D.K., Mohapatra, D.P., Nagar,
A., Sahoo, M.N. (eds.) Proceedings of the International Conference on Signal, Net-
works, Computing, and Systems. LNEE, vol. 395, pp. 85–91. Springer, New Delhi
(2017). https://doi.org/10.1007/978-81-322-3592-7 9
12. Masegosa, A.R., Feelders, A.J., van der Gaag, L.C.: Learning from incomplete data
in Bayesian networks with qualitative inﬂuences. Int. J. Approximate Reasoning
69, 18–34 (2016). https://doi.org/10.1016/j.ijar.2015.11.004
13. Olesen, K.G., et al.: A Munin network for the median nerve - a case study
on loops. Appl. Artif. Intell. 3(2–3), 385–403 (1989). https://doi.org/10.1080/
08839518908949933
14. Pearl, J.: Probabilistic reasoning in intelligent systems: networks of plausible infer-
ence. Morgan Kaufmann Publishers Inc., San Francisco (1988)
15. Plajner, M., Vomlel, J.: Learning bipartite Bayesian networks under monotonic-
ity restrictions. Int. J. Gen. Syst. 49(1), 88–111 (2020). https://doi.org/10.1080/
03081079.2019.1692004
16. Plajner, M., Vomlel, J.: Monotonicity in practice of adaptive testing (2020).
https://arxiv.org/abs/2009.06981
17. Samejima, F.: Estimation of latent ability using a response pattern of graded scores.
Psychometrika 34, 1–97 (1969). https://doi.org/10.1007/BF03372160
18. Savicky, P., Vomlel, J.: Exploiting tensor rank-one decomposition in probabilistic
inference. Kybernetika 43(5), 747–764 (2007)
19. Vomlel, J.: Bayesian networks in educational testing. Int. J. Uncertainty Fuzziness
Knowl.-Based Syst. 12(supp01), 83–100 (2004)
20. Wainer, H., Dorans, N.J.: Computerized Adaptive Testing: A Primer. Routledge
(1990)

Fine-Tuning the Odds in Bayesian
Networks
Bahare Salmani(B)
and Joost-Pieter Katoen(B)
RWTH Aachen University, Aachen, Germany
{salmani,katoen}@cs.rwth-aachen.de
Abstract. This paper proposes new analysis techniques for Bayes net-
works in which conditional probability tables (CPTs) may contain sym-
bolic variables. The key idea is to exploit scalable and powerful tech-
niques for synthesis problems in parametric Markov chains. Our tech-
niques are applicable to arbitrarily many, possibly dependent, parame-
ters that may occur in multiple CPTs. This lifts the severe restrictions on
parameters, e.g., by restricting the number of parametrized CPTs to one
or two, or by avoiding parameter dependencies between several CPTs,
in existing works for parametric Bayes networks (pBNs). We describe
how our techniques can be used for various pBN synthesis problems
studied in the literature such as computing sensitivity functions (and
values), simple and diﬀerence parameter tuning, ratio parameter tuning,
and minimal change tuning. Experiments on several benchmarks show
that our prototypical tool built on top of the probabilistic model checker
Storm can handle several hundreds of parameters.
1
Introduction
Parametric Bayesian Networks. We consider Bayesian networks (BNs) whose
conditional probability tables (CPTs) contain symbolic parameters such as x1,
2·x2
1, and x1+x2 with 0 < x1, x2 < 1. Parametric probabilistic graphical models
received a lot of attention, see e.g., [7,9–11,13,14,17–19,26,34,37,40,42]. Sensi-
tivity analysis determines the eﬀect of the parameter values in the CPTs on the
decisions drawn from the parametric BN (pBN), e.g., whether Pr(H=h | E=e) >
q for a given q ∈Q ∩[0, 1]. It amounts to establishing a function expressing an
output probability in terms of the xi parameters under study. Parameter syn-
thesis on pBNs deals with instantiating or altering the parameters such that the
resulting BN satisﬁes some constraint of interest. For pBNs, synthesis mostly
amounts to parameter tuning: ﬁnd the minimal change on the parameters such
that some constraint, e.g., Pr(H=h | E=e) > q holds [15,40]. As sensitivity
analysis and parameter synthesis are computationally hard in general [37,38,40],
many techniques restrict the number of parameters per CPT (n-way for small
n [13,17,29]), permit parameter dependencies in several CPTs (single CPT [14]),
or consider speciﬁc structures such as join trees [37] and require all parameters
to occur in the same clique of the junction tree.
This work is funded by the ERC AdG Project FRAPPANT (Grant Nr. 787914).
c
⃝Springer Nature Switzerland AG 2021
J. Vejnarová and N. Wilson (Eds.): ECSQARU 2021, LNAI 12897, pp. 268–283, 2021.
https://doi.org/10.1007/978-3-030-86772-0_20

Fine-Tuning the Odds in Bayesian Networks
269
Parametric Markov Chains.
Quite independently, analysis techniques for
Markov chains (MCs) have been developed in the formal veriﬁcation commu-
nity in the last two decades [21,23,24,27,30,31,39,41]; for a recent overview
see [35]. Parametric MCs (pMCs) are MCs in which transitions are labelled with
multivariate polynomials over a ﬁxed set of parameters. Substitution of these
variables by concrete values induces a probability distribution over the state
space of the MC. Whereas early works focused on computing a rational func-
tion over the parameters expressing the reachability probability of a given target
state, in the last years signiﬁcant progress has been made to check whether there
exists a parameter valuation inducing a MC that satisﬁes a given objective, or to
partition the parameter space—the space of all possible parameter values—into
“good” and “bad” w.r.t. a given objective, e.g., is the probability to reach some
states below (or above) a given threshold q? The complexity of various pMC
synthesis problems is studied in [1,36].
This paper aims to extend the spectrum of parameter synthesis techniques for
parametric BNs, i.e., BNs in which arbitrary many CPTs may contain symbolic
probabilities, with state-of-the-art and recently developed techniques for paramet-
ric MCs. Consider the BN adapted from [22] depicted below. The probability of
a cow being pregnant given both tests are negative is about 0.45. Assume the
farmer wants to replace both tests such that this false-positive error is below 0.2.
Figure 1 (left) indicates the corresponding pBN while (right) shows the synthe-
sized values of the parameters p and q (the false-negative probabilities for the
new urine and blood tests) using pMC techniques [41] such that the farmer’s
constraint is satisﬁed (green) or not (red).
Pregnancy
Blood Test
Urine Test
Pregnancy
no
yes
0.13
0.87
Urine Test
Pregnancy neg
pos
no
0.893 0.107
yes
0.36
0.64
Blood Test
Pregnancy neg
pos
no
0.894 0.106
yes
0.27
0.73
Fig. 1. (left) Parametric CPTs and (right) the parameter space split into safe and
unsafe regions for the constraint Pr(P = yes | U = neg and B = neg) ≤0.2. (Color
ﬁgure online)

270
B. Salmani and J.-P. Katoen
Let us highlight a few issues: we can treat parameter space partitionings that
go beyond rectangular shapes, and we support multiple, possibly dependent,
parameters (not illustrated in our example). Thanks to approximate techniques
such as parameter lifting [41], the entire parameter space can be split into safe,
unsafe, and—by some approximation factor—unknown regions. This provides
useful information: if it is not possible to ﬁnd urine and blood tests of a certain
quality, are alternatives ﬁne too? Parameter tuning [13] for pBNs ﬁnds parame-
ter values that are at a minimal distance to the original values in the BN. For our
example, the BN sensitivity tool SamIam suggests changing the false-negative
probability of the urine test (p) from 0.36 to 0.110456; or changing the false-
negative probability of the blood test (q) from 0.27 to 0.082842.1 Interestingly,
if the parameters occur in multiple CPTs, the constraint can be satisﬁed with
a smaller deviation from the original parameter values. Other recent work [5]
focuses on obtaining symbolic functions for pBN-related problems such as sen-
sitivity analysis. Note that pBNs are similar to constrained BNs [6] that focus
on logical semantics rather than synthesis algorithms and tools as we do.
We treat the theoretical foundations of exploiting pMC techniques for various
synthesis questions on pBNs, present a prototypical tool that is built on top of the
probabilistic model checker Storm [25] and the pMC analysis tool Prophesy [24],
and provide experimental results that reveal:
– pMC techniques are competitive to most common functions with the pBN
tools SamIam and Bayesserver2.
– pMC techniques are well-applicable to general pBNs, in particular by allowing
parameter dependencies.
– Approximate parameter space partitioning is eﬀective for parameter tuning
e.g., ratio, diﬀerence, and minimal change problems.
Further proofs and details of this paper can be found in [44].
2
Parametric Bayesian Networks
This section deﬁnes parametric Bayesian networks (BNs)3 and deﬁnes the sensi-
tivity analysis and parameter tuning tasks from the literature that we consider.
Parameters. Let X = {x1, . . . , xn} be a ﬁnite set of real-valued parameters and
Q(X) denote the set of multivariate polynomials over X with rational coeﬃ-
cients. A parameter instantiation is a function u: X →R. A polynomial f can
be interpreted as a function f : Rn →R where f(u) is obtained by substitution,
i.e., in f(u) each occurrence of xi in f is replaced by u(xi). To make clear where
substitution occurs, we write f[u] instead of f(u) from now on. We assume that
1 When it comes to multiple parameter suggestions, SamIam suggests p = 0.120097 or
q = 0.089892.
2 We do not consider tools such as [48] for sensitivity analysis of Credal networks.
3 Our notion of parametric BN should not be confused with existing parametric notions
that consider variations of the structure, i.e., the topology of the BN.

Fine-Tuning the Odds in Bayesian Networks
271
all parameters are bounded, i.e., lbi ≤u(xi) ≤ubi for each parameter xi. The
parameter space of X is the set of all possible parameter values, the hyper-
rectangle spanned by the intervals [lbi, ubi]. A subset R of the parameter space
is called a region.
Parametric Bayes Networks. A parametric BN is a BN in which entries in the
conditional probability tables (CPTs) are polynomials over the parameters in X.
Deﬁnition 1. A parametric BN (pBN) B is a tuple (V, E, X, Θ) with:
– V = {v1, . . . , vm} is a set of discrete random variables with dom(vi) = Dvi
– G = (V, E) with E ⊆V × V is a directed acyclic graph on V
– X = {x1, . . . , xn} is a ﬁnite set of real-valued parameters
– Θ is a set of parametric conditional probability tables Θ = { Θvi | vi ∈V }
Θvi :
⎛
⎝

p∈parents(vi)
Dp
⎞
⎠→Q(X)|Dvi|.
Let B[u] be obtained by replacing every parameter xi in B by its value u(xi). A
parameter instantiation u is well-formed for the pBN B if B[u] is a BN, i.e., for
every vi ∈V and parent evaluation par, Θvi(par)[u] yields a probability distri-
bution over Dvi. In the sequel, we assume u to be well-formed. A pBN deﬁnes a
parametric joint probability distribution over V .
pBN Subclasses. We deﬁne some sub-classes of pBNs that are used in existing sen-
sitivity analysis techniques and tools. They constrain the number of parameters,
the number of CPTs (and the number of rows in a CPT) containing parameters.
Let B = (V, E, X, Θ) be a pBN, c(xi) the number of CPTs in B in which xi
occurs and r(xi) the number of CPT rows in which xi occurs.
– B ∈p1c1r1 iﬀB contains one parameter x1 and x1 only occurs in a single row
of a single CPT, i.e., X = {x1}, c(x1) = r(x1) = 1.
– B ∈p2c≤2r1 iﬀB involves two parameters occurring only in two rows of two
(or one) CPTs, i.e., X = {x1, x2}, c(xi) ∈{1, 2}, r(xi) = 1 for i = 1, 2.
– B ∈p∗c1r1 iﬀB allows multiple distinct parameters, provided each parameter
occurs in a single row of a single CPT, i.e., r(xi) = 1, c(xi) = 1 for each xi
and all the parameters occur in the same CPT.
The class p1c1r1 is used in one-way, p2c≤2r1 in two-way sensitivity analy-
sis [13,17,29] and p∗c1r1in single CPT [14].
Parameter Synthesis Problems in pBN. We deﬁne some synthesis problems for
pBNs by their corresponding decision problems [38]. Let Pr denote the paramet-
ric joint distribution function induced by pBN B = (V, E, X, Θ) and Pr[u] the
joint probability distribution of B[u] at well-formed instantiation u. Let E ⊆V
be the evidence, H ⊆V the hypothesis and q ∈Q ∩[0, 1] a threshold.
Parameter Tuning. Find an instantiation u s.t. Pr[u](H = h | E = e) ≥q.

272
B. Salmani and J.-P. Katoen
Hypothesis Ratio Parameter Tuning. Find an instantiation u s.t.
Pr[u](H = h′ | E = e)
Pr[u](H = h | E = e)
≥q
i.e.,
Pr[u](H = h′, E = e)
Pr[u](H = h, E = e)
≥q,
where h and h′ are joint variable evaluations for the hypothesis H.
Hypothesis Diﬀerence Parameter Tuning. Find an instantiation u s.t.
Pr[u](H = h | E = e) −Pr[u](H = h′ | E = e) ≥q,
where h and h′ are joint variable evaluations for the hypothesis H.
Minimal Change Parameter Tuning. For a given parameter instantiation
u0 and ϵ ∈Q>0, ﬁnd an instantiation u s.t.
d(Pr[u], Pr[u0]) ≤ϵ,
where d is a distance notion on probability distributions, see [15].
Computing Sensitivity Function and Sensitivity Value. For the evidence
E = e and the hypothesis H = h, compute the sensitivity function:
fPr(H=h | E=e) = Pr(H = h | E = e).
This is a rational function over X, i.e., a fraction g/h with g, h ∈Q(X).
The diﬀerence and ratio problems can analogously be deﬁned for evidences. The
evidence tuning problems are deﬁned for values e and e′ for E, given a ﬁxed
value h for H.
3
Parametric Markov Chains
A parametric Markov chain is a Markov chain in which the transitions are
labelled with polynomials over the set X of parameters. These polynomials are
intended to describe a parametric probability distribution over the pMC states.
Deﬁnition 2. A parametric Markov chain (pMC) M is a tuple (Σ, σI, X, P)
where Σ is a ﬁnite set of states with initial state σI ∈Σ, X is as before, and
P : Σ × Σ →Q(X) is the transition probability function.
For pMC M = (Σ, σI, X, P) and well-formed parameter instantiation u on X,
M[u] is the discrete-time Markov chain (Σ, σI, X, P[u]) where P[u] is a probabil-
ity distribution over Σ. We only consider well-formed parameter instantiations.
Reachability Probabilities. Let D be an MC. Let Paths(σ) denote the set of all
inﬁnite paths in D starting from σ, i.e., inﬁnite sequences of the form σ1σ2σ3 . . .
with σ1 = σ and P(σi, σi+1) > 0. A probability measure PrD is deﬁned on mea-
surable sets of inﬁnite paths using a standard cylinder construction; for details,
see, e.g., [2, Ch. 10]. For T ⊆Σ and σ ∈Σ, let
Pr
D (♦T) = Pr
D { σ1σ2σ3 . . . ∈Paths(σ) | ∃i. σi ∈T }
(1)

Fine-Tuning the Odds in Bayesian Networks
273
denote the probability to eventually reach some state in T from σ. For pMC M,
PrM(♦T) is a function with PrM(♦T)[u] = PrD(♦T) where D = M[u], see [23].
Parameter Synthesis Problems on pMCs. We consider the following synthesis
problems on pMCs. Let M = (Σ, σI, X, P) be a pMC and λ ∈Q ∩[0, 1] a
threshold, ∼a binary comparison operator, e.g., < or ≥, and T ⊆Σ.
Feasibility Problem. Find a parameter instantiation u s.t. PrM[u](♦T) ∼λ.
Synthesis Problem. Partition a region R into Ra and Rr s.t.
Pr
M[u](♦T) ∼λ for all u ∈Ra
and
Pr
M[u](♦T) ̸∼λ for all u ∈Rr.
Ra is called an accepting region and Rr a rejecting region.
Approximate Synthesis Problem. Partition a region R into an accepting
region Ra, rejecting region Rr, and unknown region Ru, such that Ra ∪Rr
covers at least c% of R. Additionally, Ra,Rr, and Ru should be ﬁnite unions
of rectangular regions.
Veriﬁcation Problem. Check whether region R is accepting, rejecting, or
inconsistent, i.e., neither accepting nor rejecting.
Computing
Reachability
Functions.
Compute
the
rational
function
PrM(♦T).
Algorithms for pMC Synthesis Problems. Several approaches have been devel-
oped to compute the reachability function PrM(♦T). This includes state elim-
ination [23], fraction-free Gaussian elimination [1] and decomposition [28,33].
The reachability function can grow exponentially in the number of parameters,
even for acyclic pMCs [1]. Feasibility is a computationally hard problem: ﬁnd-
ing parameter values for a pMC that satisfy a reachability objective is ETR-
complete4 [36]. Feasibility has been tackled using sampling search methods such
as PSO5 and Markov Chain Monte Carlo [16] and solving a non-linear opti-
mization problem [4]. State-of-the-art approaches [20,21] iteratively simplify the
NLP6 encoding around a point to guide the search. The approximate synthesis
problem checking is best tackled with parameter lifting [41]. The parameter lift-
ing algorithm (PLA) ﬁrst drops all dependencies between parameters in a pMC.
It then transforms the pMC into a Markov decision process to get upper and
lower bounds for the given objective.
4
Analysing Parametric BNs Using pMC Techniques
The key of our approach to tackle various synthesis problems on pBNs is to
exploit pMC techniques. To that end, we transform a pBN into a pMC. We ﬁrst
present a recipe that is applicable to all inference queries on pBNs, and then
detail a transformation that is tailored to the evidence in an inference query.
4 Existential Theory of the Reals. ETR problems are between NP and PSPACE, and
ETR-hard problems are as hard as ﬁnding the roots of a multi-variate polynomial.
5 Particle swarm optimization.
6 Nonlinear programming.

274
B. Salmani and J.-P. Katoen
A pBN2pMC Transformation. This is inspired by our mapping of BNs onto tree-
shaped MCs [43]. Here, we propose an alternative transformation that yields
more succinct (p)MCs, as it only keeps track of a subset of (p)BN variables at
each “level” of the (p)MC. These are the so-called open variables whose valuations
are necessary to determine the subsequent transitions. Intuitively, the variable
vi is open if it has already been processed and at least one of its children has
not. Below, we use ∗if the value of variable v ∈V is don’t care, i.e., either not
yet determined or not needed any more.
Deﬁnition 3. Let G = (V, E) be a DAG with topological order ϱ = (v1, . . . , vm)
on V . Let vi ∈V be open at level7 j iﬀi <ϱ j and ∃vk ∈children(vi). k >ϱ j.
Let openϱ(j) denote the set of open variables at level j under ϱ.
Deﬁnition 4. For pBN B = (V, E, X, Θ) with topological order ϱ = (v1, . . . , vm)
on V and dom(vi) = Dvi, let the pMC Mϱ
B = (Σ, σI, X, P) where:
– Σ = m
j=1
m
i=1 {vi} × Tj(Dvi) with Tj(Dvi) =

Dvi
if i = j or vi ∈openϱ(j),
{∗}
otherwise
– σI = V × {∗} is the initial state, and
– P is the transition probability function deﬁned for f ∈Q(X) and function
tj(di) =

di
if vi ∈openϱ(j)
∗
otherwise
by the following inference rules:
Θv1(d1) = f
σI
f→

(v1, d1), (v2, ∗), . . . , (vm, ∗)
	
(2)
σ =

(v1, ti−1(d1)), . . . , (vi−2, ti−1(di−2)), (vi−1, di−1), (vi, ∗), . . . , (vm, ∗)
	
,
σ |= parents(vi),
Θvi(parents(vi))(di) = f
σ
f
−→σ′ =

(v1, ti(d1)), . . . , (vi−1, ti(di−1)), (vi, di), (vi+1, ∗), . . . , (vm, ∗)
	
(3)
σF =

(v1, ∗), . . . , (vm−1, ∗), (vm, dm)
	
1
−→σF
(4)
The states in pMC Mϱ
B are tuples of pairs (vi, di) where vi is a random vari-
able in B and di ∈dom(vi) ∪{ ∗} is the current value of vi. Note that (vi, di)
encodes vi = di. In the initial state all variables are don’t care. The (parametric)
function P speciﬁes the probability of evolving between pMC states, which is
determined by parametric CPT entries of the pBN B. Rule (2) deﬁnes the outgo-
ing transitions of the initial state σI. Let d1 ∈Dv1 and f=Θv1(d1) be v1’s CPT
entry for d1. σI evolves—with the parametric probability function f—to a state
in which all variables are ∗except that v1=d1. Let σF =((v1, ∗), · · · , (vm, dm))
7 Levels are deﬁnable on Mϱ
B as Deﬁnition 4 does not impose any backward transition.

Fine-Tuning the Odds in Bayesian Networks
275
be the ﬁnal state of Mϱ
B. Since vm has no children, openϱ(m)=∅. It also holds
that openϱ(m−1)−{vm}=∅as vm−1 can not have any other unprocessed children
than vm. Thus all variables, except for vm, have been reset to ∗in σF , see rule
(4). Rule (3) speciﬁes the transitions from the states that are neither initial nor
ﬁnal. Let σ be the state in which v1, · · · , vi−1 have been already processed, i.e.,
have been assigned a value in the path from σI to σ; some may have been reset
to ∗afterwards. Functions tj(di) ensures that only variables with no unprocessed
children are reset. Let parents(vi) be a joint variable evaluation for vi’s parents.
The second premise in rule (3) conﬁrms that σ is consistent with parents(vi). Let
f=Θ(parents(vi))(di) be vi’s CPT entry for parents(vi) and vi=di. Function f
accordingly determines the (parametric) transition probability from σ to σ′, i.e.
a state with vi=di and the other variables determined by ti(dj). Intuitively, for
j < i, σ′ forgets vj if vi has been vj’s only unprocessed child.
Example. Figure 2 (left) indicates the pMC for the pregnancy test pBN and the
topological ordering (P, U, B) by deﬁnition 4. The node names are abbreviated
by the ﬁrst letter and the “don’t care” evaluations are omitted.
Fig. 2. The generated pMCs for the pregnancy test example based on (left) pBN2pMC
and (right) evidence-guided pBN2pMC transformation.
The following result relates (conditional) inference in pBNs to (conditional)
reachability objectives in pMCs.
Proposition 1. Let B be a pBN. Let E, H ⊆V and ϱ be a topological ordering
on V . Then, for the pMC Mϱ
B we have:
Pr
B (E) = 1 −Pr
Mϱ
B
(♦¬E)
and
Pr
B (H | E) =
1 −Pr
Mϱ
B
(♦(¬H ∨¬E))
1 −Pr
Mϱ
B
(♦¬E)
,
where the latter equality requires PrB(¬E) < 1.

276
B. Salmani and J.-P. Katoen
This result directly enables to use techniques for feasibility checking on pMCs
to pBNs, and the use of techniques to compute reachability functions on pMCs
to computing sensitivity functions on pBNs.
An Evidence-Tailored pBN2pMC Transformation. The above transformation is
agnostic from the inference query. We now construct a pMC Mϱ
B,E tailored
to a given evidence E ⊆V . This transformation is inspired by a transfor-
mation on MCs [3] for computing conditional reachability probabilities. Let
B = (V, E, X, Θ) be a pBN and ϱ be a topological order on V . Let evidence
E = (v1 = d1) ∧· · · ∧(vk = dk), such that v1 <ϱ . . . <ϱ vk. We construct
pMC Mϱ
B,E by the following two amendments on the pMC Mϱ
B as deﬁned in
Deﬁnition 4:
1. Let Σ¬E = {σ ∈E | ∃i. σ |= (vi = ¬di)} be the set of states violating E. We
redirect all incoming transitions of σ ∈Σ¬E to the initial state σI.
2. For vj ̸∈E with vj <ϱ vk, we propagate the values of vj until the level k. In
other words, we pretend vk+1 is the child of vj, so we keep vj open.
Let us formalize this. Let Σ′ and P ′ be deﬁned analogously to Σ and P in
Deﬁnition 4, except that the deﬁnition of open set changes as described above,
aﬀecting the deﬁnitions of Tj(vi) and tj(di). Then, let Mϱ
B,E = (ΣE, σI, X, PE),
where
ΣE = Σ′ \ Σ¬E
and
PE(σ, σ′) =
 
σ′̸|=E
P ′(σ, σ′)
if σ′ ∈Σ¬E
P ′(σ, σ′)
otherwise.
Example. Fig. 2 (right) indicates the evidence-guided pMC generated for our
running example, the ordering (P, U, B), and the evidence U=neg ∧B=neg.
Proposition 2. For the evidence-tailored MC Mϱ
B,E of pBN B, we have:
Pr
B (H | E) = 1 −Pr
Mϱ
B,E
(♦¬H).
(5)
This result facilitates using pMC techniques for pBN parameter tuning.
Ratio and Diﬀerence Parameter Tuning by Parameter Lifting. The ratio problem
on pBN B corresponds to ﬁnding an instantiation u in the pMC Mϱ
B,E s.t.
Pr
Mϱ
B,E
[u](♦T) ≥q · Pr
Mϱ
B,E
[u](♦G),
(6)
where Pr(♦T) stands for 1 −PrMϱ
B,E[u](♦(H = ¬h′ ∨E = ¬e)) and Pr(♦G)
abbreviates 1 −PrMϱ
B,E[u](♦(H = ¬h ∨E = ¬e)). This problem can be solved
by using PLA: let region R ⊆Rn
≥0. We perform PLA for reaching G and reaching
T on MB, respectively. This gives upper (UBT , UBG) and lower bounds (LBT ,
LBG) for the probabilities on the left- and the right-hand side of (6). Then:
– If LBT ≥q · UBG, the region R is accepting for the ratio property.

Fine-Tuning the Odds in Bayesian Networks
277
– If LBT ≤q · UBG, the region R is rejecting for the ratio property.
– Otherwise, reﬁne the region R.
For diﬀerence parameter tuning, we adopt the above recipe by replacing (6) by:
Pr
Mϱ
B,E
[u](♦T) ≥q + Pr
Mϱ
B,E
[u](♦G).
(7)
5
Experiments
Our pBN Analysis Tool. We developed a prototypical tool on top of the
tools Storm [25] and Prophesy [24], see Fig. 3. Storm is a probabilistic model
checker that dominated the last (and only) two model-checking competitions,
see https://qcomp.org/; Prophesy is an eﬃcient tool for pMC synthesis. Our
tool deploys pMC parameter synthesis techniques to analyze pBNs. It includes
both pBN2pMC transformations where pBNs are provided in an extended bif
format. The pMCs are either encoded in Jani [8] or in the explicit drn format.
It is also possible to transform non-parametric BNs into MCs and parameterize
the MC. Storm is used to compute the sensitivity function and for parameter
tuning using PLA. Prophesy is exploited for feasibility checking: ﬁnd a param-
eter instance satisfying an inference query. Our tool-chain supports p∗c∗r∗, the
general pBNs class. As baseline we used two synthesis tools for parametric BNs:
SamIam and Bayesserver.
Fig. 3. Our prototypical tool-chain for synthesis problems on pBNs
SamIam. SamIam8 is a commonly used tool for the sensitivity analysis for pBNs,
developed at Darwiche’s group at UCLA. It allows the speciﬁcation of condi-
tional, hypothesis ratio, and hypothesis diﬀerence constraints on pBNs. SamIam
then attempts to identify minimal parameter changes that are necessary to sat-
isfy these constraints. It supports the pBN classes p1c1r1 and p∗c1r1.
Bayesserver. Bayeserver9 is a commercial tool that oﬀers sensitivity analysis and
parameter tuning of pBNs. For sensitivity analysis, it computes the sensitivity
8 http://reasoning.cs.ucla.edu/samiam.
9 https://www.bayesserver.com.

278
B. Salmani and J.-P. Katoen
function and sensitivity value. It also performs minimal-change parameter tun-
ing for conditional, hypothesis ratio, and hypothesis diﬀerence constraints. It
supports the classes p1c1r1 and p2c≤2r1 for sensitivity analysis and the class
p1c1r1 for parameter tuning. Table 1 lists the functionalities of all tools.
Table 1. Overview of the capabilities of the pBN synthesis tools considered.
SamIam Bayesserver Storm-prophesy
Computing sensitivity function ✗
p≤2c≤2r1
p∗c∗r∗
Computing sensitivity value
✗
p≤2c≤2r1
p1c∗r∗
Simple parameter tuning
p∗c1r1
p1c1r1
p∗c∗r∗
Diﬀerence parameter tuning
p∗c1r1
p1c1r1
p∗c∗r∗
Ratio parameter tuning
p∗c1r1
p1c1r1
p∗c∗r∗
Minimal change tuning
p∗c1r1
p1c1r1
p∗c∗r∗
Experimental Set-up. We took benchmarks from [45] and conducted all our exper-
iments on a 2.3 GHz Intel Core i5 processor with 16 GB of RAM. We focused
on questions such as:
1. What is the scalability for computing sensitivity functions on pBNs?
2. What is the practical scalability for feasibility checking?
3. To what extent is PLA applicable to parameter tuning for pBNs?
Computing Sensitivity Function. We performed a series of experiments for com-
puting pBN sensitivity functions using our tool-chain for the p∗c∗r1 class.
Figure 4 summarizes the results. The x−axis (log scale) indicates the pBN bench-
marks and the y−axis denotes the timing in seconds. The numbers on the bars
indicate the number of parameters in the solution functions, which is related to
the number of relevant parameters identiﬁed for the given query. We observe that
Fig. 4. Storm’s performance for calculating prior sensitivity functions of pBNs. (Unfor-
tunately, a comparison with the other tools was not possible, as SamIam does not
explicitly oﬀers sensitivity function computation and Bayesserver sensitivity analysis
is limited to 1 or 2 parameters, see Table 1.)

Fine-Tuning the Odds in Bayesian Networks
279
Storm scales up to 380 parameters for very large networks such as hailfinder.
The blue bars represent regular computations, while the orange bars indicate the
impact of bisimulation minimization, a built-in reduction technique in Storm.
Feasibility Checking. Our tool exploits Prophesy to ﬁnd a parameter instanti-
ation u of pBN B such that the BN B[u] satisﬁes the given inference query.
We have performed a set of experiments for the class p∗c∗r1. Figure 5 (log-log
scale) illustrates the results; the x-axis indicates the number of parameters in
the pBN and the y-axis the time for feasibility checking (in seconds). Each line
corresponds to a pBN and the points on the lines represent single experiments.
We inserted the parameters in the predecessors of the query nodes (i.e., in H) to
maximize their relevance. We also imposed queries over multiple nodes at once
to push the boundaries. We used convex optimization (QCQP10) (left plot) and
PSO (right plot). Prophesy was able to handle up to 853 parameters.
Fig. 5. Feasibility checking on pBN benchmarks by (left) QCQP and (right) PSO.
Fig. 6. PLA results on the alarm pBN (p2c3r26) for the constraint Pr(venttube = 0 |
ventlung = 0) > 0.6 with a 99% parameter space coverage. (Color ﬁgure online)
Approximate Parameter Synthesis on pBNs: Tuning the Parameters and More.
Experiments on the pBN benchmarks using PLA aimed at (a) the classes p1c1r1
10 Quadratically-constrained quadratic programming.

280
B. Salmani and J.-P. Katoen
and p∗c1r1 to validate them against SamIam and Bayesserver, and (b) the class
p∗c∗r∗to investigate the potential of PLA for general pBNs. Figure 6 visualizes
the results for the alarm pBN with 2 parameters occurring in 26 rows of 3 CPTs,
i.e., a pBN with parameter dependencies. The parameter x was used in the CPT
entries e1, · · · , ek only when the probability values of those entries coincided
in the original BN. As seen in the ﬁgure, PLA can partition the entire n-way
parameter space. The minimal-change parameter values can be extracted from
the PLA results, where the precision depends on the PLA approximation factor.
6
Conclusion
This paper exploited tools and techniques for parameter synthesis on Markov
chains to synthesis problems on parametric Bayesian networks. Prototypical tool
support for pBN analysis on top of existing pMC synthesis tools has been real-
ized. Our experiments indicate that pMC techniques can scale sensitivity anal-
ysis and parameter tuning tasks on pBNs. The experiments reveal the poten-
tial of parameter lifting [41] for partitioning the parameter space of pBNs. Most
importantly, the proposed techniques are applicable to general pBNs—no restric-
tions are imposed on the number or occurrence of parameters—and may involve
parameter dependencies. Future work include ﬁnding optimal parameter set-
tings [47], exploiting monotonicity checking [46] and to extend the current work
to (parametric) dynamic, Gaussian [12], and recursive BNs [32].
Acknowledgement. We thank Robin Drahovsky for his contributions on transform-
ing pMCs into pBNs, Caroline Jabs for her implementation eﬀorts, and Sebastian
Junges, Tim Quatmann, and Matthias Volk for discussions. We also thank Arthur
Choi for his support.
References
1. Baier, C., Hensel, C., Hutschenreiter, L., Junges, S., Katoen, J., Klein, J.: Para-
metric Markov chains: PCTL complexity and fraction-free Gaussian elimination.
Inf. Comput. 272, 104504 (2020)
2. Baier, C., Katoen, J.: Principles of Model Checking. MIT Press (2008)
3. Baier, C., Klein, J., Klüppelholz, S., Märcker, S.: Computing conditional probabil-
ities in Markovian models eﬃciently. In: Ábrahám, E., Havelund, K. (eds.) TACAS
2014. LNCS, vol. 8413, pp. 515–530. Springer, Heidelberg (2014). https://doi.org/
10.1007/978-3-642-54862-8_43
4. Bartocci, E., Grosu, R., Katsaros, P., Ramakrishnan, C.R., Smolka, S.A.: Model
repair for probabilistic systems. In: Abdulla, P.A., Leino, K.R.M. (eds.) TACAS
2011. LNCS, vol. 6605, pp. 326–340. Springer, Heidelberg (2011). https://doi.org/
10.1007/978-3-642-19835-9_30
5. Bartocci, E., Kovács, L., Stankovič, M.: Analysis of Bayesian networks via prob-
solvable loops. In: Pun, V.K.I., Stolz, V., Simao, A. (eds.) ICTAC 2020. LNCS,
vol. 12545, pp. 221–241. Springer, Cham (2020). https://doi.org/10.1007/978-3-
030-64276-1_12

Fine-Tuning the Odds in Bayesian Networks
281
6. Beaumont, P., Huth, M.: Constrained Bayesian Networks: Theory, Optimization,
and Applications. CoRR https://arxiv.org/abs/1705.05326 (2017)
7. Bolt, J.H., van der Gaag, L.C.: Balanced tuning of multi-dimensional Bayesian
network classiﬁers. In: Destercke, S., Denoeux, T. (eds.) ECSQARU 2015. LNCS
(LNAI), vol. 9161, pp. 210–220. Springer, Cham (2015). https://doi.org/10.1007/
978-3-319-20807-7_19
8. Budde, C.E., Dehnert, C., Hahn, E.M., Hartmanns, A., Junges, S., Turrini, A.:
JANI: quantitative model and tool interaction. In: Legay, A., Margaria, T. (eds.)
TACAS 2017. LNCS, vol. 10206, pp. 151–168. Springer, Heidelberg (2017). https://
doi.org/10.1007/978-3-662-54580-5_9
9. Castillo, E., Gutiérrez, J.M., Hadi, A.S.: Parametric structure of probabilities in
Bayesian networks. In: Froidevaux, C., Kohlas, J. (eds.) ECSQARU 1995. LNCS,
vol. 946, pp. 89–98. Springer, Heidelberg (1995). https://doi.org/10.1007/3-540-
60112-0_11
10. Castillo, E.F., Gutiérrez, J.M., Hadi, A.S.: Goal oriented symbolic propagation in
Bayesian networks. In: AAAI/IAAI, vol. 2. pp. 1263–1268. AAAI Press/The MIT
Press (1996)
11. Castillo, E.F., Gutiérrez, J.M., Hadi, A.S.: Sensitivity analysis in discrete Bayesian
networks. IEEE Trans. Syst. Man Cybern. Part A 27(4), 412–423 (1997)
12. Castillo, E.F., Gutiérrez, J.M., Hadi, A.S., Solares, C.: Symbolic propagation and
sensitivity analysis in Gaussian Bayesian networks with application to damage
assessment. Artif. Intell. Eng. 11(2), 173–181 (1997)
13. Chan, H., Darwiche, A.: When do numbers really matter? J. Artif. Intell. Res. 17,
265–287 (2002)
14. Chan, H., Darwiche, A.: Sensitivity analysis in Bayesian networks: from single to
multiple parameters. In: UAI, pp. 67–75. AUAI Press (2004)
15. Chan, H., Darwiche, A.: A distance measure for bounding probabilistic belief
change. Int. J. Approx. Reason. 38(2), 149–174 (2005)
16. Chen, T., Hahn, E.M., Han, T., Kwiatkowska, M.Z., Qu, H., Zhang, L.: Model
repair for Markov decision processes. In: TASE. IEEE (2013)
17. Coupé, V.M.H., van der Gaag, L.C.: Properties of sensitivity analysis of Bayesian
belief networks. Ann. Math. Artif. Intell. 36(4), 323–356 (2002)
18. Coupe, V.M., van der Gaag, L.C.: Practicable sensitivity analysis of Bayesian
belief networks, vol. 1998. Utrecht University, Information and Computing Sci-
ences (1998)
19. Coupé, V.M., van der Gaag, L.C., Habbema, J.D.F.: Sensitivity analysis: an aid
for belief-network quantiﬁcation. Knowl. Eng. Rev. 15(3), 215–232 (2000)
20. Cubuktepe, M., et al.: Sequential convex programming for the eﬃcient veriﬁcation
of parametric MDPs. In: Legay, A., Margaria, T. (eds.) TACAS 2017. LNCS, vol.
10206, pp. 133–150. Springer, Heidelberg (2017). https://doi.org/10.1007/978-3-
662-54580-5_8
21. Cubuktepe, M., Jansen, N., Junges, S., Katoen, J.-P., Topcu, U.: Synthesis in
pMDPs: a tale of 1001 parameters. In: Lahiri, S.K., Wang, C. (eds.) ATVA 2018.
LNCS, vol. 11138, pp. 160–176. Springer, Cham (2018). https://doi.org/10.1007/
978-3-030-01090-4_10
22. Darwiche, A.: Modeling and Reasoning with Bayesian Networks. Cambridge Uni-
versity Press (2009)
23. Daws, C.: Symbolic and parametric model checking of discrete-time markov chains.
In: Liu, Z., Araki, K. (eds.) ICTAC 2004. LNCS, vol. 3407, pp. 280–294. Springer,
Heidelberg (2005). https://doi.org/10.1007/978-3-540-31862-0_21

282
B. Salmani and J.-P. Katoen
24. Dehnert, C., et al.: PROPhESY: a probabilistic parameter synthesis tool. In: Kroen-
ing, D., Păsăreanu, C.S. (eds.) CAV 2015. LNCS, vol. 9206, pp. 214–231. Springer,
Cham (2015). https://doi.org/10.1007/978-3-319-21690-4_13
25. Dehnert, C., Junges, S., Katoen, J.-P., Volk, M.: A storm is coming: a modern prob-
abilistic model checker. In: Majumdar, R., Kunčak, V. (eds.) CAV 2017. LNCS,
vol. 10427, pp. 592–600. Springer, Cham (2017). https://doi.org/10.1007/978-3-
319-63390-9_31
26. Druzdzel, M.J., van der Gaag, L.C.: Building probabilistic networks: “where do the
numbers come from?”. IEEE Trans. Knowl. Data Eng. 12(4), 481–486 (2000)
27. Fang, X., Calinescu, R., Gerasimou, S., Alhwikem, F.: Fast parametric model check-
ing through model fragmentation. In: ICSE, pp. 835–846. IEEE (2021)
28. Fang, X., Calinescu, R., Gerasimou, S., Alhwikem, F.: Fast parametric model check-
ing through model fragmentation. CoRR https://arxiv.org/abs/2102.01490 (2021)
29. van der Gaag, L.C., Renooij, S., Coupé, V.M.H.: Sensitivity analysis of probabilistic
networks. In: Lucas, P., Gámez, J.A., Salmerón, A. (eds.) Advances in Probabilis-
tic Graphical Models. SFSC, vol. 213, pp. 103–124. Springer, Heidelberg (2007).
https://doi.org/10.1007/978-3-540-68996-6_5
30. Gainer, P., Hahn, E.M., Schewe, S.: Accelerated model checking of parametric
markov chains. In: Lahiri, S.K., Wang, C. (eds.) ATVA 2018. LNCS, vol. 11138, pp.
300–316. Springer, Cham (2018). https://doi.org/10.1007/978-3-030-01090-4_18
31. Hahn, E.M., Hermanns, H., Zhang, L.: Probabilistic reachability for parametric
Markov models. Int. J. Softw. Tools Technol. Transf. 13(1), 3–19 (2011)
32. Jaeger, M.: Complex probabilistic modeling with recursive relational Bayesian net-
works. Ann. Math. Artif. Intell. 32(1–4), 179–220 (2001)
33. Jansen, N., et al.: Accelerating parametric probabilistic veriﬁcation. In: Norman,
G., Sanders, W. (eds.) QEST 2014. LNCS, vol. 8657, pp. 404–420. Springer, Cham
(2014). https://doi.org/10.1007/978-3-319-10696-0_31
34. Jensen, F.V.: Gradient descent training of Bayesian networks. In: Hunter, A., Par-
sons, S. (eds.) ECSQARU 1999. LNCS (LNAI), vol. 1638, pp. 190–200. Springer,
Heidelberg (1999). https://doi.org/10.1007/3-540-48747-6_18
35. Junges, S., et al.: Parameter synthesis for Markov models. CoRR https://arxiv.
org/abs/1903.07993 (2019)
36. Junges, S., Katoen, J.P., Pérez, G.A., Winkler, T.: The complexity of reachability
in parametric Markov decision processes. J. Comput. Syst. Sci. 119, 183–210 (2021)
37. Kjærulﬀ, U., van der Gaag, L.C.: Making sensitivity analysis computationally eﬃ-
cient. In: UAI, pp. 317–325. Morgan Kaufmann (2000)
38. Kwisthout, J., van der Gaag, L.C.: The computational complexity of sensitivity
analysis and parameter tuning. In: UAI, pp. 349–356. AUAI Press (2008)
39. Lanotte, R., Maggiolo-Schettini, A., Troina, A.: Parametric probabilistic transition
systems for system design and analysis. Formal Aspects Comput. 19(1), 93–109
(2007)
40. Laskey, K.B.: Sensitivity analysis for probability assessments in Bayesian networks.
IEEE Trans. Syst. Man Cybern. 25(6), 901–909 (1995)
41. Quatmann, T., Dehnert, C., Jansen, N., Junges, S., Katoen, J.-P.: Parameter syn-
thesis for Markov models: faster than ever. In: Artho, C., Legay, A., Peled, D.
(eds.) ATVA 2016. LNCS, vol. 9938, pp. 50–67. Springer, Cham (2016). https://
doi.org/10.1007/978-3-319-46520-3_4
42. Renooij, S.: Co-variation for sensitivity analysis in Bayesian networks: properties,
consequences and alternatives. Int. J. Approx. Reason. 55(4), 1022–1042 (2014)

Fine-Tuning the Odds in Bayesian Networks
283
43. Salmani, B., Katoen, J.-P.: Bayesian inference by symbolic model checking. In:
Gribaudo, M., Jansen, D.N., Remke, A. (eds.) QEST 2020. LNCS, vol. 12289, pp.
115–133. Springer, Cham (2020). https://doi.org/10.1007/978-3-030-59854-9_9
44. Salmani, B., Katoen, J.: Fine-tuning the odds in Bayesian networks. CoRR https://
arxiv.org/abs/2105.14371 (2021)
45. Scutari, M.: Bayesian network repository. https://www.bnlearn.com. Accessed
2019
46. Spel, J., Junges, S., Katoen, J.-P.: Are parametric Markov chains monotonic? In:
Chen, Y.-F., Cheng, C.-H., Esparza, J. (eds.) ATVA 2019. LNCS, vol. 11781, pp.
479–496. Springer, Cham (2019). https://doi.org/10.1007/978-3-030-31784-3_28
47. Spel, J., Junges, S., Katoen, J.-P.: Finding provably optimal Markov chains. In:
TACAS 2021. LNCS, vol. 12651, pp. 173–190. Springer, Cham (2021). https://doi.
org/10.1007/978-3-030-72016-2_10
48. Tolo, S., Patelli, E., Beer, M.: An open toolbox for the reduction, inference compu-
tation and sensitivity analysis of Credal networks. Adv. Eng. Softw. 115, 126–148
(2018)

Cautious Classiﬁcation with Data Missing
Not at Random Using Generative
Random Forests
Julissa Villanueva Llerena1(B)
, Denis Deratani Mau´a1
,
and Alessandro Antonucci2
1 Institute of Mathematics and Statistics, Universidade de S˜ao Paulo,
S˜ao Paulo, Brazil
{jgville,ddm}@ime.usp.br
2 Dalle Molle Institute for Artiﬁcial Intelligence, Lugano, Switzerland
alessandro@idsia.ch
Abstract. Missing data present a challenge for most machine learn-
ing approaches. When a generative probabilistic model of the data is
available, an eﬀective approach is to marginalize missing values out.
Probabilistic circuits are expressive generative models that allow for eﬃ-
cient exact inference. However, data is often missing not at random, and
marginalization can lead to overconﬁdent and wrong conclusions. In this
work, we develop an eﬃcient algorithm for assessing the robustness of
classiﬁcations made by probabilistic circuits to imputations of the non-
ignorable portion of missing data at prediction time. We show that our
algorithm is exact when the model satisﬁes certain constraints, which is
the case for the recent proposed Generative Random Forests, that equip
Random Forest Classiﬁers with a full probabilistic model of the data. We
also show how to extend our approach to handle non-ignorable missing
data at training time.
Keywords: Probabilistic circuits · Generative random forests ·
Missing data · Conservative inference rule
1
Introduction
This work presents a new tractable algorithm for analyzing the eﬀect of all
potential imputations of non-ignorable missing values to a probabilistic classi-
ﬁer’s response.
Missing data present a challenge in many machine learning tasks. The stan-
dard approach to inference with such data is to either impute or marginalize
out the missing values [3,7]. The latter option requires a complete statistical
model of features and target variables, and eﬃcient inference routines. Recently,
Correia, Peharz and de Campos [5] proposed Generative Random Forest (GeFs),
Supported by CAPES Finance 001, CNPQ grant #304012/2019-0.
c
⃝Springer Nature Switzerland AG 2021
J. Vejnarov´a and N. Wilson (Eds.): ECSQARU 2021, LNAI 12897, pp. 284–298, 2021.
https://doi.org/10.1007/978-3-030-86772-0_21

Cautious Classiﬁcation for Data MNAR Using GeFs
285
which extend standard random forest classiﬁers into complete statistical models
with tractable marginalization of missing values. GeFs have shown superior per-
formance to imputation approaches and other ad hoc heuristics used for random
forests in classiﬁcation tasks under missing data [5].
GeFs are actually part of a larger class of tractable probabilistic models,
called Probabilistic Circuits, that allow for linear time marginalization [4,11].
Sum-Product Networks [21], Probabilistic Sentential Decision Diagrams [9] and
Cutset Networks [22] are other notable examples of Probabilistic Circuits. These
models have obtained impressive results in several machine learning tasks due to
their ability to eﬃciently represent and manipulate intricate multidimensional
distributions [20,21,24–26,31].
Imputation and marginalization are either theoretically supported by a miss-
ing at random assumption (MAR), that roughly considers that the probability
of the missing values does not depend on the variables with missing values them-
selves [23]. This is not always a sensible choice [13]. For instance, in personalized
recommendation, users have a strong bias towards rating items which they either
strongly like or strongly dislike [14]. Automatically constructed knowledge-based
systems oﬀer another example, as they are most often populated exclusively with
“positive” facts involving only a small fraction of the true facts [27]. In such cases,
called non-ignorable missing data or MNAR (i.e., missing not at random), imput-
ing or averaging over completions can lead to biased and inconsistent estimates
and ultimately hurt performance. Importantly, it is not possible to statistically
test whether or not the MAR assumption is satisﬁed, nor to learn from data the
incompleteness process responsible for the missing values [17,23].
In the presence of MNAR data, marginalization can still be used as a heuris-
tic, at the risk of inducing excessive bias. To analyze such potential bias, we follow
[30] and propose to quantify the eﬀect in a probabilistic classiﬁer’s decision to
all possible imputations of the MNAR data. The challenge that we overcome
here is doing so in a computationally tractable way, making use of the machin-
ery of Probabilistic Circuits. Such an analysis has been previously applied to
traditional probabilistic models such as Bayesian networks [1], where it suﬀers
from intractability of inference. In fact, one can show that for Bayesian networks
the task is equivalent to performing marginal inference in credal networks [2],
a task whose theoretical and practical complexity far exceeds that of marginal
inference in Bayesian networks [15].
In this work, we devise a polynomial-time procedure to quantify the eﬀect
of diﬀerent imputations of the missing values of features in the classiﬁcation
of a target discrete variable at prediction time. This is important because while
training data can often be curated and missingness mechanisms investigated, the
same is generally not true for missing data at prediction time. We assume the
classiﬁer is represented as a Probabilistic Circuit, and focus on the case of GeFs
(although the algorithm we present is slightly more general). The procedure can
be used to determine the set of maximal values for the target variable given
an observation with data (assumed) MNAR, that is, to decide which values
are the most probable classiﬁcation under some imputation. We also discuss

286
J. V. Llerena et al.
how to enable conservative inferences with non-ignorable data at learning time.
Experiments show that our algorithm obtains reliable conclusions often more
accurate than criteria that ignores or marginalizes missing variables.
2
Probabilistic Circuits and Generative Random Forests
We start by establishing some notation and terminology. We denote random
variables by upper-case letters (e.g., Xi, X), and their values by lower case (e.g.,
xi, x). Sets of random variables are written in boldface (e.g., X), as well as
their realization (e.g., x). In this work we assume that random variables take
on a ﬁnite number of values, denoted as val(X) for random variable X. We
associate every discrete random variable X with a set of indicator functions
{[[X = x]] : x ∈val(X)}, where the notation [[X = x]] describes the function that
returns 1 if X takes value x and 0 otherwise.
A Probabilistic Circuit (PC) M over a set of categorical random variables
X is a rooted weighted acyclic directed graph whose leaves are associated with
indicator functions [[Xi = xi]] of variables in X, and the internal nodes are
associated to either sum or product operations. The arcs i →j leaving sum
node i are associated with non-negative weights wij. We write Mi to denote the
sub-PC rooted at node i . The scope of a PC is the set of random variables
associated with the indicator variables at the leaves, and the scope of a node is
the scope of the respective sub-PC. A PC represents a joint distribution of X by
PM(x) = M(x)/(
x′ M(x′)). The value M(x), called the evaluation of the circuit
at x, is deﬁned inductively in the size of the circuit as: M(x) = [[Xi = xi]](xi) if
M is a leaf node [[Xi = xi]]; M(x) = 
j wijMj(x) if M is a circuit rooted at a sum
node i with children j; and M(x) = 
j Mj(x) if M is a circuit rooted at a product
node with children j. For example, the evaluation of the PC on the right-hand
side of Fig. 1 at X = 4, Y = 1 and Z = 1 is M(x) = 0.6 × 0.7 × 0.6 = 0.252.
To ensure that marginal inference is computed in linear time in the size of
the circuit, it suﬃces that the circuit satisﬁes the properties of smoothness and
decomposability [9,21]. Smoothness states that the scopes of any two children
of a sum node are identical.1 Decomposability states that the scopes of any two
children of a product node are disjoint. For the rest of this paper, we assume
that PCs are smooth and decomposable. The tractability of more complex proba-
bilistic queries rely on additional properties. One such property is determinism:
each sum node has at most one child that evaluates to non-zero at any (com-
plete) realization of its scope.2 Determinism ensures that maximum likelihood
estimates for the weights can be obtained in closed-form under complete data; it
also enables ﬁnding the most probable realization in linear time [18], a NP-hard
task in non-deterministic PCs. The property is also necessary (but not suﬃ-
cient) for advanced operations such as encoding constraints, computing entropy
or KL-divergence, and producing expected predictions with respect to a com-
patible regressor [7–9].
1 Smoothness is also called completeness in the context of Sum-Product Networks.
2 Determinism is also called selectivity in the context of Sum-Product Networks.

Cautious Classiﬁcation for Data MNAR Using GeFs
287
Fig. 1. A Decision Tree for classifying Y based on the values of X and Z (left) and its
Generative Decision Tree extension (right).
Consider a Decision Tree mapping features X to a target variable Y . Gener-
ative Decision Trees (GeDT) are deterministic, decomposable and smooth PCs
built from such a Decision Tree and data by converting each decision node into a
sum node and each leaf into a sub-PC whose support is the partition induced by
the corresponding path of the Decision Tree. The sub-PCs at the leaves can be
learned with any structure learning algorithm for PCs or take simple forms such
as fully factorized distributions. A particularly convenient form for the sub-PCs
is to encode a distribution that factorizes as P(X|γ)P(Y |γ), where γ denotes
the corresponding partition of the feature space. Such class-factorized GeDTs
produce the same classiﬁcations under complete data as the original Decision
Tree [5]. As that property will be crucial to ensure exactness of the procedure
we develop later, we generalize it to arbitrary PCs as follows. We say that a PC
is class-factorized with respect to a target variable Y if for any leaf whose scope
is {Y }, its parents also have scope {Y }. Intuitively, in a class-factorized PC the
leaves with indicators for variable Y cannot be used to select between diﬀerent
sub-PCs as in the Proof of Theorem 1 we show later.
The structure of a GeDT can be modiﬁed by “pulling the indicators up” to
speed up computations, that is, by adding product nodes and indicator leaves
that encode the partitions of the decision tree nodes. While this procedure vio-
lates decomposability, it renders a PC whose evaluations produce identical result
(and marginal inference is therefore still linear). Hereafter, we will refer to GeDT
as the PC obtained from a Decision Tree after such an operation. Figure 1 shows
a decision tree (on the left) and a class-factorized GeDT extension (on the right)
obtained by pulling up indicators. The numbers inside each node in the decision
tree indicate the percentage of the training data instances that fall in the cor-
responding partition out of those instances that are in the partition deﬁned by
the parent node. Those values (transformed in probabilities) are used to deﬁne
the weight of the respective outgoing arc in the GeDT.

288
J. V. Llerena et al.
GeDTs allow for proper and eﬃcient treatment of missing at random data
by computing P(Y |o), the conditional probability of the target variable given
the observed features o (which might be a subset of all features). While Decision
Trees are consistent (Bayes optimal) estimators only when used in fully observed
data, GeDTs are consistent also for missing at random features [5]. A Generative
Random Forest (GeF) is the structure obtained as a mixture model where each
component is the GeDT corresponding to a Decision Tree in a Random Forest
Classiﬁer. Marginal inference also takes therefore linear time in GeFs.
3
Tractable Conservative Inference
Consider a PC M(X), possibly learned from some (complete or MAR incomplete)
dataset of realizations of variables X. Suppose we are interested in using our
model to predict the value of a target variable Y given a conﬁguration x of the
variables such that some of its values are missing. Let o denote the observed
part of x and u denote a possible completion for the unobserved values. Under
the missing at random hypothesis, this is best performed by computing
arg max
y
P(y|o) = arg max
y
M(y, o)/M(o) ,
(1)
where M(y, o) = 
u M(y, o, u) is the marginal value of the circuit at y and o,
which can be obtained in linear time as discussed, and M(o) = 
y M(y, o).
When the missingness process is non-ignorable, the inference in (1) can lead
to erroneous and unreliable conclusions. As an example, consider a Boolean
target Y and Boolean features O and U. Say we observe O = o, and assume
that the value of U guides the prediction being P(Y =1|o, u1) = 0.7 and P(Y =
1|o, u2) = 0.4, and that P(U) is uniform. Also, the observation of U is missing
due to the following MNAR process: when U is missing, value u1 is nine times
less likely than u2. The Bayes-optimal classiﬁcation is thus Y = 0 as P(Y =
1|o, U =⋆) = 0.7 × 0.1 + 0.4 × 0.9 = 0.43 < 0.5, yet the marginal classiﬁcation is
Y = 1 as P(Y =1|o) = 0.55 > 0.5.
Unlike the example above, we rarely have access to the missingness process.
We can instead estimate the robustness of a classiﬁcation Y = y′ under non-
ignorable missing data with respect to an alternative classiﬁcation Y = y′′ by
δM,o(y′, y′′) = min
u [M(y′, o, u) −M(y′′, o, u)] .
(2)
A decision analyst might want to suspend the classiﬁcation on the basis of
the value in (2), thus producing more conservative conclusions. For example,
if δM,o(y′, y′′) > 0, then any imputation of the values of u still leads to a classiﬁ-
cation y′ that is more probable (as far as the model estimates) than y′′; we thus
say that y′ dominates y′′. Note that δ can be a function of P(y|o) and P(o),
that is, it can be instance speciﬁc and account for other sources of information.
The conservative inference rule (CIR) prescribes that the only conclusion
supported by non-ignorable missing data is to return the set of non-dominated
values [30]:

Cautious Classiﬁcation for Data MNAR Using GeFs
289

y : max
y′ δM,o(y′, y) ≤0

.
(3)
This is akin to classiﬁcation with a rejection option, but possibly more informa-
tive (and arguably more principled).
Even though evaluation takes linear time in PCs, a brute-force approach to
computing (2) requires evaluating M(y, o, u) for each u. This is unfeasible when
the number of possible completions is high. The next result shows that computing
such a value is coNP-hard even in deterministic PCs, ruling out the existence of
an eﬃcient exact procedure (under common assumptions of complexity theory).
Theorem 1. Given a smooth, decomposable and deterministic PC M over ran-
dom variables Y , O and U, target values y′ and y′′, a partial observation o, and
a (rational) threshold ρ, deciding if δM,o(y′, y′′) > ρ is coNP-complete.
The proof is in the appendix.
We now provide a linear-time algorithm for computing δM,o(y′, y′′) in tree-
shaped deterministic PCs that satisfy class-factorization, which include class-
factorized GeDTs. For the sake of readability, we drop the dependence on o in
the following. The algorithm can be described straightforwardly by a collection
of recursive equations depending on the type of node at which it operates. The
recursive formulation also provides a proof of its correctness under the above
assumptions.
Sum Nodes. If M is rooted at a sum node with children M1, . . . , Mn and weights
w1, . . . , wn, then the algorithm computes:
δM(y′, y′′) =
n
min
i=1 wi min
u [Mi(y′, o, u) −Mi(y′′, o, u)] =
n
min
i=1 wiδMi(y′, y′′) .
(4)
The correctness of the operation follows from the determinism of the circuit
and the class-factorization property. The former ensures that for any realization
(y, x) at most one sub-PC Mi evaluates to a nonnegative value Mi(y, x) > 0. The
latter ensures that either M encodes a distribution over Y (i.e., its scope is the
singleton {Y }) or the nonnegative child for Mi(y′, x) and Mi(y′′, x) is the same.
Product Nodes. If instead M is a product node with children M1, . . . , Mn such that
Y is in the scope of M1 (and no other), then the algorithm computes:
δM(y′, y′′) = min
u1 [M1(y′, o1, u1) −M1(y′′, o1, u1)]


	
=δM1(y′,y′′)
n

i=2
optuiMi(oi, ui) ,
(5)
where oi (resp., ui) denotes the projection of o (resp., u) into the scope of Mi,
and
opt =

max
if δM1(y′, y′′) > 0 ,
min
if δM1(y′, y′′) ≤0 .

290
J. V. Llerena et al.
The ﬁrst term denotes the recursive computation on the sub-PC M1. The remain-
ing terms optuiMi(oi, ui) deﬁne an optimization of the conﬁgurations ui for the
sub-PC Mi; this can be performed in linear time in deterministic PCs by bottom-
up traversal, replacing sums with maximizations/minimizations [18,19].
Leaves. Finally, if M is a leaf node representing an indicator variable then the
algorithm computes:
δM(y′, y′′) =
⎧
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎩
1
if M is [[Y = y′]],
−1
if M is [[Y = y′′]],
1
if M is consistent with o or u,
0
otherwise.
(6)
We thus obtain the following result.
Theorem 2. The algorithm obtained by Eqs. (4), (5) and
(6) computes
δM,o(y′, y′′) in class-factorized tree-shaped deterministic PCs in linear time.
For non-deterministic networks, the equation for sum nodes is no longer valid,
as in such circuits
min
u
n

i=1
wi[Mi(y′, o, u) −Mi(y′′, o, u)] ̸= min
u wi
n
min
i=1 [Mi(y′, o, u) −Mi(y′′, o, u)] .
The equations for products and leaves remain valid for non-deterministic circuits.
Thus, we can use our algorithm as an eﬀective heuristic for non-deterministic
PCs or that violate class-factorization. This is the case for instance when we
have a partially ignorable missingness process and we marginalize part of the
missing variables by judging their missingness to satisfy MAR. Then, even for
deterministic PCs, the algorithm described is not guaranteed to provide the
correct outcome if some variables are marginalized. Yet our experiments show
that it provides an eﬀective heuristic, supporting the reasoning above.
4
Non-ignorable Training Data
In the previous section we considered PCs learned from complete (or MAR
incomplete) datasets, while restricting the presence of MNAR data to predic-
tion time. Yet, we might also have non-ignorable missingness in the training
dataset and, following the ideas outlined by [28], apply the same conservative
treatment to the learning of the PC. As a ﬁrst step, in this work we consider
that the structure of the circuit (i.e., its directed graph) is either speciﬁed in a
data-free fashion (e.g., using region graphs or random structures), or learned by
standard algorithms using only the complete portion of the data set [20,21]. The
latter is a sensible choice when the missing values do not signiﬁcantly alter the
(context-speciﬁc) independences in the data, but can aﬀect the quantiﬁcation of
the weights in a signiﬁcant form.

Cautious Classiﬁcation for Data MNAR Using GeFs
291
Say that a PC structure is uniformly deterministic, if any quantiﬁcation
assigning positive weights leads to a deterministic PC. Thus assume that we
have a ﬁxed uniformly deterministic PC structure that we need to quantify
using incomplete data. For complete data, the maximum likelihood estimates of
the weights associated to a sum node can be obtained as
wij =
Nij + α

j Nij + α ,
where Nij counts the number of instances of the dataset for which the sub-PC
Mj contributed to the PC value and α is a smoothing factor to counter the eﬀect
of small sample sizes.3 Clearly, each possible completion of the non-ignorable
missing values induces a diﬀerent quantiﬁcation of the weights using the formula
above. This leads to the speciﬁcation of a credal PC, whose weights are not
speciﬁed by sharp numerical values, but only required to satisfy a ﬁnite number
of linear constraints [16]. Standard inference in such models is therefore intended
as the computation of the lower and upper bounds of the query with respect to
all the possible speciﬁcations of the weights consistent with the constraints. A
simple strategy is to obtain interval-valued weights:
0 ≤wij ≤wij ≤wij ≤1,

i→j
wij = 1 .
(7)
For deterministic PCs, the lower bound wij can be approximated as the count Nij
over instances which have no missing values for the scope of the corresponding
node, and the upper bound wij is obtained by assuming that all completions
will satisfy Mj > 0 and hence contribute to the corresponding count Nij. Note
that those bounds are loose as they ignore the dependencies among diﬀerent
parameters. An alternative technique to learn non-deterministic credal PCs in
the presence of incomplete data was recently proposed by [10]. We could also
resort to their approach for a CIR-based training of credal PCs. We leave as
future work adapting their results to learning deterministic credal PCs.
Assessing the robustness with interval-valued credal PCs amounts to com-
puting:
δM(y′, y′′) = min
w min
u [Mw(y′, o, u) −Mw(y′′, o, u)] ,
(8)
where the notation Mw denotes a PC quantiﬁed by weights w.
The algorithm for deciding dominance can be easily adapted for handling
class-factorized deterministic tree-shaped credal PCs. If M is rooted at a sum node
with children M1, . . . , Mn and weights w1, . . . , wn, then the algorithm computes
δM(y′, y′′) =
n
min
i=1 wi min
w,u[Mi(y′, o, u) −Mi(y′′, o, u)] ,
(9)
3 By contributing to the PC value, we mean that there is path from the root to Mj
where each node evaluates to a positive value for the given instance.

292
J. V. Llerena et al.
where wi = wi if the inner minimization is positive, and wi = wi if the inner
minimization is negative. Similarly, if M is a product node with children M1, . . . , Mn
such that Y is in the scope of M1 (and no other), then the algorithm computes:
δM(y′, y′′) = min
w1,u1[M1(y′, o1, u1) −M1(y′′, o1, u1)]


	
=δM1(y′,y′′)
n

i=2
opti Mi(oi, ui) ,
(10)
where
opti =

maxwi,ui
if δM1(y′, y′′) > 0 ,
minwi,ui
if δM1(y′, y′′) ≤0 .
The sub-problems opti Mi(oi, ui) can be computed in linear time by the algorithm
described in [12]. The equations for the leaves remain unchanged. We have that:
Theorem 3. The algorithm obtained by Eqs. (9), (10) and
(6) computes
δM,o(y′, y′′) in class-factorized tree-shaped deterministic credal PCs in linear time.
5
Experiments
We empirically evaluate the ability of our proposed methods in assessing the
robustness of classiﬁcations to non-ignorable missing feature values, by means
of the index δ. To this end, we learn class-factorized GeFs from six well-known
complete binary datasets for density estimation [6], using the algorithm in [5].
The characteristics of the datasets are in Table 1. Missing test values are simu-
lated using a mix of MAR, MCAR and MNAR mechanisms. The average number
of (MAR, MCAR and MNAR) missing values per instance is denoted as AvM,
and the average number of MNAR values per instance is denoted as AvMNAR.
Table 1. Datasets characteristics.
Dataset
# Test
AvM AvMNAR # Train
Model
Variables Instances
Instances Size
Audio
100
3,000
4.1
1.9
15,000
3,858
Dna
180
1,186
5.5
2.2
1,600
1,038
Msnbc
17
5,624
1.6
0.5
291,326
2,816
Mushrooms 112
5,624
7.7
3.4
2,000
1,764
Netﬂix
100
3,000
6.7
3.0
15,000
3,524
Nltcs
16
3,236
1.4
0.4
16,181
568
In Table 2 we report relevant performance metrics of our CIR predictions.
The last column (Acc) shows the accuracy of classiﬁcations made by marginal-
izing all missing test values. Columns RAcc and ¬RAcc report the classiﬁcation

Cautious Classiﬁcation for Data MNAR Using GeFs
293
accuracy on the portions of instances that are robust and non-robust, respec-
tively. A test instance is robust if the CIR inference (Eq. 3) returns only one
non-dominated class value. For the rows tagged “marg”, we marginalize MAR
variables and optimize over the MNAR variables. For the other rows, we optimize
over all missing values. Column %R shows the percentage of robust instances.
By comparing RAcc, Acc and ¬RAcc, we observe the ability of CIR in discrimi-
nating between the easy-to-classify instances, corresponding to the robust ones,
and the harder ones (non-robust instances), for which a set of classes is returned.
Similar conclusions can be reached by inspecting the SAcc (Set Accuracy) col-
umn, which measures the percentage of (set-valued) classiﬁcations that contain
the true class. Finally, the informative character of marginal classiﬁcations are
captured by the discounted accuracy (DAcc), which penalizes “imprecise” clas-
siﬁcations by weighting correct set-valued classiﬁcations by the reciprocal of
their size (see [29] for more details and motivation about the metric). A DAcc
value higher than the corresponding Acc denotes that the classiﬁer issues pre-
dictions that are on average more accurate than random classiﬁcations, hence
being informative despite the false MAR assumption.
Table 2. Set accuracy (SAcc), Discounted accuracy (DAcc), percentage of robust
instances (%R), and classiﬁcation accuracy on robust (RAcc), non-robust (¬RAcc)
and overall (Acc) instances when marginalizing missing values at prediction time.
Dataset
CIR
Marginalization
SAcc DAcc %R
RAcc ¬RAcc Acc
Audio
0.879 0.707
65.6 0.807
0.679
0.763
Audio (marg)
0.863 0.708
69.1 0.798
0.686
0.763
Dna
0.899 0.799
80.0 0.880
0.511
0.806
Dna (marg)
0.858 0.801
88.6 0.846
0.496
0.806
Msnbc
0.978 0.956
95.5 0.978
0.925
0.976
Msnbc (marg)
0.979 0.932
90.6 0.978
0.956
0.976
Mushrooms
1.000 0.991
98.2 1.000
1.000
1.000
Mushrooms (marg) 1.000 0.991
98.2 1.000
1.000
1.000
Netﬂix
0.894 0.662
53.6 0.771
0.652
0.716
Netﬂix (marg)
0.873 0.665
58.3 0.760
0.655
0.716
Nltcs
0.980 0.912
86.4 0.977
0.856
0.961
Nltcs (marg)
0.975 0.906
86.2 0.972
0.888
0.961
To analyze the approach on a more realistic missingness scenario, we learn
GeFs from a binarized version of the complete version of the Jester dataset.4
This is an complete dataset of user ratings on 10 items (variables), divided into
4 http://eigentaste.berkeley.edu/dataset.

294
J. V. Llerena et al.
17,467 training instances (users) and 7,486 test instances. We build a binary
classiﬁcation task by predicting, for each user/instance, the rating of a distin-
guished item given the other items ratings. We fabricate MNAR values in both
training and test sets by independently omitting a positive rating with either
low probability (p = 0.05) or high probability (p = 0.5). This simulates observed
behaviour of users providing ratings in such systems [14]. Table 3 shows that
learning an imprecise model relay better accuracy than the precise version that
ignore missing values. Note that when learning a credal PC, we might produce
set-valued classiﬁcations even when we marginalize (MAR) the missing test val-
ues. Figure 2 shows that for both missingness levels the measure in (2) can be
used to detect easy-to-classify instances for the precise classiﬁer that assumes
MAR. Similar patterns are achieved in terms of (modiﬁed) discounted accuracy
in Fig. 3, where this approach is combined with a rejection option.
Table 3. Performance of models learned from Jester with two diﬀerent missingness
proportions p in the training and test set. Imprecise models are obtained as in Sect. 4,
precise models are obtained after removal of instances with missing values.
Model
Inference p
Model + Inference Precise + MAR
SAcc DAcc %R
RAcc ¬RAcc Acc
Imprecise MAR
0.05 0.689 0.664
95.1
0.596
0.540
0.593
Imprecise CIR
0.05 0.716 0.661
88.9
0.600
0.540
0.593
Precise
CIR
0.05 0.644 0.602
91.5
0.598
0.536
0.593
Imprecise MAR
0.5
0.753 0.597
68.7
0.639
0.435
0.575
Imprecise CIR
0.5
0.847 0.597
50.0
0.693
0.457
0.575
Precise
CIR
0.5
0.827 0.578
50.3
0.657
0.493
0.575
−5
0
5
10
15
20
25
·10−4
0.6
0.7
0.8
ϵ
Robust Accuracy
20
40
60
80
100
Robust Instances (%)
−15
−10
−5
0
5
·10−4
0.65
0.7
0.75
0.8
ϵ
Robust Accuracy
20
40
60
80
100
Robust Instances (%)
RAcc
%R
Fig. 2. Robust accuracy (RAcc) of the precise classiﬁer (MAR) for the Jester dataset
with low (p = 0.05, left) and high (p = 0.5, right) missingness levels. Condition
δM,o(y, ¬y) > ϵ, where δM,o is deﬁned as in Eq. (2) and y is the class returned by
the classiﬁer, is used to decide robustness. We also display %R by threshold ϵ.

Cautious Classiﬁcation for Data MNAR Using GeFs
295
−2
0
2
4
6
8
·10−3
0.5
0.6
0.7
0.8
ϵ
Discounted Accuracy
0
20
40
60
80
100
Robust Instances (%)
−2
−1
0
1
2
3
·10−3
0.5
0.6
0.7
0.8
ϵ
Discounted Accuracy
0
20
40
60
80
100
Robust Inst. (%)
ρ = 0.5
ρ = 0.6
ρ = 0.8
%R
Fig. 3. Modiﬁed discounted accuracy of the (imprecise) classiﬁer for the Jester dataset
with low (p = 0.05, left) and high (p = 0.5, right) missingness levels. Robustness is
decided as in Fig. 2 for diﬀerent values of the threshold ϵ. A value ρ is used instead of
0.5 to score imprecise classiﬁcations, which regulates preference for model uncertainty
against aleatory uncertainty (see [29]).
6
Conclusion
We developed an exact polynomial-time method for the conservative treatment
of non-ignorable missing data using probabilistic circuits. Experiments with real-
istic data demonstrated that the approach is eﬀective in discriminating instances
which are sensitive to the missingness process from those that are not. Our app-
roach to handling missing data at training time led us to consider credal circuits,
which extend standard probabilistic circuits by locally associating sets of prob-
abilities to sum nodes. Such extensions retain many of the tractable properties
of probabilistic circuits, oﬀering an interesting and more cautious alternative
to marginal inference. We left as future work the treatment of other types of
missing data (e.g., coarse and unreliable observations).
Proof of Theorem 1
Membership in coNP is trivial: given a conﬁguration u we can compute M(y′, o, u)
and M(y′′, o, u) in linear time and decide the sign of its diﬀerence in constant time.
Hence we have a polynomial certiﬁcate that the problem is not in the language.
We show hardness by reduction from the subset sum problem: Given positive
integers z1, . . . , zn, decide
∃u ∈{0, 1}n :

i∈[n]
viui = 1 ,
where vi =
2zi

i∈[n] zi
.
(11)
+
×
×
[[Y = y′]]
P1(u)
P2(u)
[[Y = y′′]]
a
b

296
J. V. Llerena et al.
To solve that problem, build a tree-shaped deterministic PC as shown above,
where Ui are binary variables, P1(u) = 
i e−2viui and P2(u) = 
i e−viui. Note
that the PC is not class-factorized. Use the PC to compute:
δ(y′, y′′) = min
u

a exp

−2

i
viui

−b exp

−

i
viui

.
If we call x := exp(−
i viui), the above expression is the minimum for positive
x of f(x) := ax2 −bx. Function f is a strictly convex function minimized at
x = b/(2a). Selecting a and b such that b/(2a) = e−1 makes the minimum occur
at 
i viui = 1. Thus, there is a solution to (11) if and only if δ(y′, y′′) ≤−ae−2.
This proof is not quite valid because the distributions P1(u) and P2(u) use
non-rational numbers. However, we can use the same strategy as used to prove
Theorem 5 in [16] and exploit the rational gap between yes and no instances
of the original problem to encode a rational approximation of P1 and P2 of
polynomial size.
⊓⊔
References
1. Antonucci, A., Piatti, A.: Modeling unreliable observations in Bayesian networks by
credal networks. In: Proceedings of the Third International Conference on Scalable
Uncertainty Management (SUM), pp. 28–39 (2009)
2. Antonucci, A., Zaﬀalon, M.: Decision-theoretic speciﬁcation of credal networks: a
uniﬁed language for uncertain modeling with sets of Bayesian networks. Int. J.
Approximate Reasoning 49(2), 345–361 (2008)
3. Azur, M.J., Stuart, E.A., Frangakis, C., Leaf, P.J.: Multiple imputation by chained
equations: what is it and how does it work? Int. J. Methods Psychiatr. Res. 20,
40–49 (2011)
4. Choi, Y., Vergari, A., Van den Broeck, G.: Probabilistic circuits: a unifying frame-
work for tractable probabilistic models (2020)
5. Correia, A.H.C., Peharz, R., de Campos, C.P.: Joints in random forests. In:
Advances in Neural Information Processing Systems 33 (NeurIPS) (2020)
6. Davis, J., Domingos, P.: Bottom-up learning of Markov network structure. In:
Proceedings of the 27th International Conference on Machine Learning (ICML),
pp. 271–280 (2010)
7. Khosravi, P., Choi, Y., Liang, Y., Vergari, A., Van den Broeck, G.: On tractable
computation of expected predictions. In: Advances in Neural Information Process-
ing Systems 32 (NeurIPS) (2019)
8. Khosravi, P., Liang, Y., Choi, Y., Van den Broeck, G.: What to expect of classiﬁers?
Reasoning about logistic regression with missing features. In: Proceedings of the
28th International Joint Conference on Artiﬁcial Intelligence (IJCAI) (2019)
9. Kisa, D., Van den Broeck, G., Choi, A., Darwiche, A.: Probabilistic sentential deci-
sion diagrams. In: Proceedings of the 14th International Conference on Principles
of Knowledge Representation and Reasoning (PKDD), pp. 1–10 (2014)
10. Levray, A., Belle, V.: Learning credal sum-product networks. In: Proceedings of
the 2nd Conference on Automated Knowledge Base Construction (2020)
11. Liang, Y., Van den Broeck, G.: Learning logistic circuits. In: Proceedings of the
33rd Conference on Artiﬁcial Intelligence (AAAI) (2019)

Cautious Classiﬁcation for Data MNAR Using GeFs
297
12. Llerena, J.V., Mau´a, D.D.: Eﬃcient algorithms for robustness analysis of maxi-
mum a posteriori inference in selective sum-product networks. Int. J. Approximate
Reasoning 126, 158–180 (2020)
13. Manski, C.F.: Partial identiﬁcation with missing data: concepts and ﬁndings. Int.
J. Approximate Reasoning 39(2–3), 151–165 (2005)
14. Marlin, B.M., Zemel, R.S., Roweis, S.T., Slaney, M.: Recommender systems: miss-
ing data and statistical model estimation. In: Proceedings of the 22nd International
Joint Conference in Artiﬁcial Intelligence (IJCAI) (2011)
15. Mau´a, D.D., De Campos, C.P., Benavoli, A., Antonucci, A.: Probabilistic inference
in credal networks: new complexity results. J. Artif. Intell. Res. 50, 603–637 (2014)
16. Mau´a, D.D., Conaty, D., Cozman, F.G., Poppenhaeger, K., de Campos, C.P.:
Robustifying sum-product networks. Int. J. Approximate Reasoning 101, 163–180
(2018)
17. Mohan, K., Pearl, J., Tian, J.: Graphical models for inference with missing data.
In: Proceedings of Advances in Neural Information Processing Systems (NeurIPS),
pp. 1277–1285 (2013)
18. Peharz, R., Gens, R., Domingos, P.: Learning selective sum-product networks. In:
Proceedings of the Workshop on Learning Tractable Probabilistic Models (2014)
19. Peharz, R., Gens, R., Pernkopf, F., Domingos, P.: On the latent variable interpre-
tation in sum-product networks. IEEE Trans. Pattern Anal. Mach. Intell. 39(10),
2030–2044 (2017)
20. Peharz, R., et al.: Random sum-product networks: a simple and eﬀective approach
to probabilistic deep learning. In: Proceedings of The 35th Uncertainty in Artiﬁcial
Intelligence Conference (UAI) (2020)
21. Poon, H., Domingos, P.: Sum-product networks: a new deep architecture. In: Pro-
ceedings of the 27th Conference on Uncertainty in Artiﬁcial Intelligence (UAI), pp.
337–346 (2011)
22. Rahman, T., Kothalkar, P., Gogate, V.: Cutset networks: a simple, tractable, and
scalable approach for improving the accuracy of Chow-Liu trees. In: Calders, T.,
Esposito, F., H¨ullermeier, E., Meo, R. (eds.) Proceedings of the Joint European
Conference on Machine Learning and Knowledge Discovery in Databases (ECML-
PKDD), pp. 630–645 (2014)
23. Rubin, D.B.: Inference and missing data. Biometrika 63(3), 581–592 (1976)
24. Shao, X., Alejandro Molina, A.V., Stelzner, K., Peharz, R., Liebig, T., Kersting,
K.: Conditional sum-product networks: imposing structure on deep probabilistic
architectures. In: Proceedings of the 10th International Conference on Probabilistic
Graphical Models (PGM) (2020)
25. Shen, Y., Choi, A., Darwiche, A.: A tractable probabilistic model for subset selec-
tion. In: Proceedings of the 33rd Conference on Uncertainty in Artiﬁcial Intelligence
(UAI) (2017)
26. Shen, Y., Goyanka, A., Darwiche, A., Choi, A.: Structured Bayesian networks:
from inference to learning with routes. In: Proceedings of the Thirty-Third AAAI
Conference on Artiﬁcial Intelligence (AAAI) (2019)
27. Shin, J., Wu, S., Wang, F., Sa, C.D., Zhang, C., R´e, C.: Incremental knowledge
base construction using deepdive. In: Proceedings of the VLDB Endowment (2015)
28. Zaﬀalon, M.: Conservative rules for predictive inference with incomplete data. In:
Proceedings of the 4th International Symposium on Imprecise Probabilities and
Their Applications (ISIPTA), pp. 406–415 (2005)
29. Zaﬀalon, M., Corani, G., Mau´a, D.: Evaluating credal classiﬁers by utility-
discounted predictive accuracy. Int. J. Approximate Reasoning 53(8), 1282–1301
(2012)

298
J. V. Llerena et al.
30. Zaﬀalon, M., Miranda, E.: Conservative inference rule for uncertain reasoning
under incompleteness. J. Artif. Intell. Res. 34, 757–821 (2009)
31. Zheng, K., Pronobis, A., Rao, R.P.N.: Learning graph-structured sum-product net-
works for probabilistic semantic maps. In: Proceedings of the 32nd AAAI Confer-
ence on Artiﬁcial Intelligence (AAAI) (2018)

Belief Functions

Scoring Rules for Belief Functions and
Imprecise Probabilities: A Comparison
Esther Anna Corsi1(B)
, Tommaso Flaminio2
, and Hykel Hosni1
1 Department of Philosophy, University of Milan,
Via Festa del Perdono 7, 20122 Milano, Italy
{esther.corsi,hykel.hosni}@unimi.it
2 Artiﬁcial Intelligence Research Institute (IIIA – CSIC),
Campus UAB, 08193 Bellaterra, Spain
tommaso@iiia.csic.es
Abstract. This paper investigates de Finetti’s coherence as an opera-
tional foundation for a wide range of non-additive uncertainty measures
and focuses, in particular, on Belief functions and Lower probabilities. In
a companion paper we identify a number of non-limiting circumstances
under which Dutch Book criteria for Belief functions and Lower proba-
bility are undistinguishable, which is surprising given that Lower prob-
abilities are known to exist which do no satisfy the axioms of Belief
functions. The main contribution of this paper consists in putting for-
ward a comparison between a criterion based on the Brier scoring rule for
Belief Functions and the scoring rule introduced in 2012 by Seidenfeld,
Schervish and Kadane for Imprecise probabilities. Through this compar-
ison we show that scoring rules allow us to distinguish coherence-wise
between Belief functions and Imprecise probabilities.
Keywords: Scoring rules · Belief functions · Lower probabilities ·
Imprecise probabilities · Coherence
1
Introduction
In a companion paper [4], we observe that in any ﬁnite boolean algebra with at
least three atoms there exist books deﬁned over rich-enough sets of events that do
not distinguish assignments extendible to Belief functions from those extendible
to Lower probabilities. Given that all Belief functions are Lower probabilities,
but the converse does not hold, our previous ﬁnding is somewhat puzzling. In
this work we extend the comparison between those two well-known non-additive
measures of uncertainty to the alternative, but logically equivalent (in probabil-
ity), criterion of coherence based on the Brier scoring rule.
To do so, we introduce a new scoring rule in Deﬁnition 6 and (i) show that it
characterises assignments that are extendible to Belief functions and (ii) compare
it to the scoring rule for imprecise probabilities introduced by Seidenfeld et al. in
[20]. This latter allows us to make a distinction between coherent Belief functions
on the one hand, and coherent Lower probabilities on the other hand.
c
⃝Springer Nature Switzerland AG 2021
J. Vejnarov´a and N. Wilson (Eds.): ECSQARU 2021, LNAI 12897, pp. 301–313, 2021.
https://doi.org/10.1007/978-3-030-86772-0_22

302
E. A. Corsi et al.
The paper is organised as follows. In Sect. 2, we recall the Dutch Book and
the proper scoring rule coherence criteria for probability functions. Early gen-
eralisations of probabilistic coherence have been proposed by [15,22]. In Sect. 3,
we give the required background on uncertainty measures, a geometric view on
coherence and extendibility, and a brief outline of the key results obtained in [4].
In Sect. 4, we introduce a scoring rule that characterises assignments extendible
to Belief functions. In Sect. 5, we relate our work to [20], draw some conclusions
and outline the research questions opened up by the present investigation.
2
Dutch Books and Proper Scoring Rules
Bruno de Finetti proves in Chap. 3 of [7] two criteria for the coherent assessment
of uncertainty. The ﬁrst is based on the no-existence of a Dutch book and it
is deﬁned in terms of a two-player zero-sum game. Suppose that ψ1, . . . , ψn are
elements of the set of sentences built recursively from a ﬁnite set of propositional
variables as usual, which are interpreted (see, e.g. [17]) as the events of interest
to a bookmaker B. Suppose further that this interest materialises with the pub-
lication of a book β : ψ1 →β1, . . . , ψn →βn where for i = 1, . . . , n, βi ∈[0, 1]. A
gambler G then chooses real-valued stakes σ1, . . . , σn and for i = 1, . . . , n, pays
σiβi to B. G will then receive back σiv(ψi), where v(ψi) = 1, if ψi is true, and
v(ψi) = 0 otherwise. Thus, G’s payoﬀis n
i=1 σi(v(ψi) −βi) and B’s payoﬀis
n
i=1 σi(βi −v(ψi)). The book published by B is coherent if there is no choice
of (possibly negative) stakes which G can make, exposing B to a sure loss. More
precisely, for every σ1, . . . , σn ∈R there is a valuation v such that,
n

i=1
σi(βi −v(ψi)) ≥0.
(1)
The second criterion is framed in terms of the individual decision of a fore-
caster F, who is asked to assess a value βi to each event ψi (a forecast). F knows
that they will suﬀer a penalty Li (L stands for loss) proportional to the square of
the euclidean distance between the realized value of ψi and the chosen value βi.
It is assumed that the forecaster’s objective is to minimize their loss. A notable
example of a loss function just introduced is the Brier scoring rule
Li(ψi, βi) = (∥v(ψi) −βi∥2)2.
(2)
De Finetti shows that minimising the expectation of loss under the brier rule is
equivalent to avoiding sure loss in the Dutch Book setting. However he argues
that it is preferable as it neutralises some potential shortcomings arising from
the strategic aspects of the betting game. Even if they are formally deﬁned in
the same way, in the context of the Dutch Book, any assignment on a set of
events is referred to in what follows as a book, while in the framework of scoring
rules, it is referred to as a forecast.

Scoring Rules for Belief Functions and Imprecise Probabilities
303
Deﬁnition 1 (Proper Scoring Rule Criterion).
Let Ψ = {ψ1, . . . , ψn} be
a set of events and, for i = 1, . . . , n, let βi ∈[0, 1] be the value the forecaster
F assigns to each ψi. The forecast β : ψ1 →β1, . . . , ψn →βn is coherent if
there is no distinct forecast β′ whose Brier score uniformly dominates β, i.e. if
L(Ψ, β) = 
i(∥v(ψi) −βi∥2)2, that is to say there is no β′ s.t. L(Ψ, β′) < L(Ψ, β)
for every valuation v.
De Finetti in ([7], Sects. 3.3–3.4) shows the equivalence between the Dutch
Book and the Proper Scoring Rule criteria for deﬁning (probabilistic) coherence.
Therefore the following proposition holds.
Proposition 1. Let Ψ = {ψ1, . . . , ψn} be a set of events and let β : ψi →βi
for i = 1, . . . , n a forecast over Ψ. The forecast β is coherent iﬀit extends to a
probability measure over the algebra of events.
Example 1. Let us consider the set of events Ψ = {α, ¬α} and the relative
forecast β : α →0.8, ¬α →0.6. The penalty that F will suﬀer depends on
the realizations of the events. If v(α) = 1 (and v(¬α) = 0), then L(Ψ, β) =
(1 −0.8)2 + (0 −0.6)2 = 0.4. If v(α) = 0 (and v(¬α) = 1), then L(Ψ, β) =
(0−0.8)2 +(1−0.6)2 = 0.8. In the geometric interpretation of Fig. 1, the penalty
relative to the ﬁrst realization of the event α is the square of the euclidean dis-
tance between (1, 0) and the point β = (0.8, 0.6), while the penalty relative to
the second one is the square of the euclidean distance between β and (0, 1). Let
us consider the projection of β onto the simplex of probabilities. This projection
(also referred to as de Finetti’s projection) identiﬁes the point β′ = (0.6, 0.4).
Since β′ is closer to each endpoint of the simplex of probabilities, its Brier scor-
ing rule is smaller than the Brier scoring rule relative to β, i.e. β′ dominates β
and β is incoherent.
Fig. 1. Example of an incoherent forecast.
In geometrical terms, β is coherent if and only if it cannot be moved in
such a way to reduce the distance from the set of all possible points. This is a

304
E. A. Corsi et al.
characterization of a convex hull. Note that this is not to say that any point
inside a convex hull shares the same distance from all its vertices, but that no
matter how we move the point, it will necessarily get closer to some points of
the convex hull and farther from some others. In particular, this happens if we
consider the vertices of the convex hull. Instead, if we consider a point lying
outside the convex hull, then we can always move it in such a way as to reduce
its distance from all the point of the convex hull.
Through Deﬁnition 1, we are not minimizing the sum of the squared euclidean
distance between β and the vertices of the convex hull of probabilities (property
hold by the centroid), but we minimize every single distance between β and all
the points of the convex hull. As stated in [7], (footnote 18), if we move the
point β to another position β∗, its distance from a generic point P increases or
decreases depending on whether P is on the same side as β or β∗with respect
to the hyperplane that bisects the segment ββ∗orthogonally. Two cases must
be considered then:
1. If β is not in the convex hull PΨ (the convex hull of the probabilities over
Ψ), there exists a hyperplane separating it from PΨ. Moving β to β∗, its
orthogonal projection into such hyperplane, diminishes its distance from all
points P ∈PΨ.
2. If β belongs to the convex hull PΨ, then to whatever point β∗we move β,
it always follows that for some point P ∈PΨ, the distance increases. If we
construct the bisecting orthogonal hyperplane relative to ββ∗and P is on the
same side as β∗, the point β would be distinguished from the convex hull of
PΨ, but this is contrary to the hypothesis.
3
Background on Uncertainty Measures and Their
Geometric Interpretation
We shall assume the reader to be familiar with basic notions and results of
(ﬁnitely additive) probability theory. In particular, since we will only consider
measures on ﬁnite boolean algebras, we shall often identify a probability measure
P on an algebra A with the distribution p obtained by restricting P on the
atoms of A. As for the other uncertainty measures we will deal with in the
following sections, it is convenient to recall some basic deﬁnitions and results
from [1,9,14,16].
We only consider ﬁnite, and hence atomic, boolean algebras as the domain
for uncertainty measures. Boolean algebras are understood as described in the
signature {∧, ∨, ¬, ⊥, ⊤} and their elements are denoted by lower-case Greek
letters with possible subscripts. In particular, the atoms of an algebra will be
indicated as α1, α2, . . ..
Deﬁnition 2 (Belief function). A Belief function B on an algebra A is a
[0, 1]-valued map satisfying:

Scoring Rules for Belief Functions and Imprecise Probabilities
305
(B1) B(⊤) = 1, B(⊥) = 0;
(B2) B (n
i=1 ψi) ≥n
i=1

{J⊆{1,...,n}:|J|=i}(−1)i+1B

j∈J ψj

for n = 1, 2, 3, . . ..
Belief functions on boolean algebras can be characterised in terms of mass func-
tions as follows. Let A be any ﬁnite boolean algebra with atoms α1, . . . , αt. A
mass function is a map m that assigns to each subset X of atoms, a real number
such that m(∅) = 0 and 
X m(X) = 1. Given a mass function m, the map
B(ψ) =

X⊆{αi|αi≤ψ}
m(X)
is a Belief function and every Belief function on A can be deﬁned in this way.
Deﬁnition 3 (Lower probability). A Lower probability P on an algebra A
is a monotone [0, 1]-valued map satisfying:
(L1) P(⊤) = 1, P(⊥) = 0;
(L2) For all natural numbers n, m, k and all ψ1, . . . , ψn, if {{ψ1, . . . , ψn}} is
an (m, k)-cover of (ϕ, ⊤)1, then k + mP(ϕ) ≥n
i=1 P(ψi).
Although the deﬁnition above does not make clear why those measures are called
Lower probabilities, [1, Theorem 1] characterises them as follows: Let P : A →
[0, 1] be a Lower probability and denote with M(P) the following set:
M(P) = {P : A →[0, 1] | P is a probability function and ∀ψ ∈A, P(ψ) ≤P(ψ)}.
Then, for all ψ ∈A,
P(ψ) = min{P(ψ) | P ∈M(P)}.
Lower probabilities are more general than Belief functions. The following
result characterises the Lower probabilities that are Belief functions.
Remark 1. A Lower probability P on an algebra A is a Belief function iﬀP
satisﬁes (B2), namely
P
 n
	
i=1
ψi

≥
n

i=1

{J⊆{1,...,n}:|J|=i}
(−1)i+1P
⎛
⎝
j∈J
ψj
⎞
⎠
(3)
for all n = 1, 2, . . ..
1 An element ϕ of a boolean algebra A is said to be covered m times by a multiset
{{ψ1, . . . , ψn}} of elements of A if every homomorphisms of A to {0, 1} that maps ϕ
to 1, also maps to 1 at least m propositions from ψ1, . . . , ψn as well. An (m, k)-cover
of (ϕ, ⊤) is a multiset {{ψ1, . . . , ψn}} that covers ⊤k times and covers ϕ n+k times.

306
E. A. Corsi et al.
The geometric approach we consider is similar to Paris’s [17] and is related
to [5,6].
Let Ψ = {ψ1, . . . , ψn} be a ﬁnite set of events (i.e., elements of a ﬁnite
boolean algebra A). Let us denote by V = {v1, . . . , vt} the ﬁnite set of all possible
homomorphisms of A to the boolean chain on the two-element set {0, 1}. For
every j = 1, . . . , t, call ej the binary vector
ej = (vj(ψ1), . . . , vj(ψn)) ∈{0, 1}n.
(4)
Given this basic construction, we can characterise in geometric terms the extend-
ability problem for assignments on Ψ to ﬁnitely additive probability measures
and Belief functions. The additional notions we need are the euclidean convex
hull co(X) of a subset X ⊆Rt (which reduces to co(X) in case X is ﬁnite) and
the less common tropical convex hull co∧,+(X) of X (see [8]).
Deﬁnition 4 (Tropical Hull).
Let x1, . . . , xt ∈[0, 1]n. The tropical hull of
the xj’s is the subset co∧,+(x1, . . . , xt) of all points y of [0, 1]n for which there
exist parameters λ1, . . . , λt ∈[0, 1] such that t
j=1 λj = 0 and
y =
t
j=1
λj + xj.
The symbol ∧stands for the minimum and + for the ordinary addition in the
tropical semiring (R, ∧, +). Given λ ∈[0, 1] and x ∈[0, 1]n, λ + x = (λ +
x1, . . . , λ + xn) and the  operator is deﬁned component-wise.
For e1, . . . , et being deﬁned as above from the formulas ψi’s in Ψ, let us
consider the following sets:
1. PΨ = co(e1, . . . , et);
2. BΨ = co(co∧,+(e1, . . . , et)), where, in this case, being co∧,+(e1, . . . , et) usu-
ally uncountable, co denotes the topological closure of the Euclidean convex
hull co.
Theorem 1 ([7,10,12]). Let Ψ = {ψ1, . . . , ψn} be a ﬁnite set of events and let
β : Ψ →[0, 1] be a assignment. Then,
1. β extends to a probability measure iﬀ(β(ψ1), . . . , β(ψn)) ∈PΨ;
2. β extends to a Belief function iﬀ(β(ψ1), . . . , β(ψn)) ∈BΨ.
In general, PΨ is strictly included in BΨ (i.e., PΨ ⊂BΨ) and this is
expected because Belief functions are strictly more general than probabilities
measures. In [4], we ask whether avoiding sure loss is suﬃcient to distinguish
Lower probabilities from Belief functions. In other words, we ask whether the
above strict inclusion is matched by a detectable diﬀerence in coherence, and
ﬁnd that the answer is negative.
We denote with LΨ the set of all assignments β on Ψ that extend to a Lower
probability P. The sets of events that do not distinguish Belief functions from
Lower probabilities are referred to as adequate. The main result of our previous
work reads as follows.

Scoring Rules for Belief Functions and Imprecise Probabilities
307
Theorem 2. For every algebra A with at least three atoms there exists an ade-
quate subset Ψ of A, i.e., Ψ is such that BΨ = LΨ.
We now introduce a scoring rule that characterises assignments extendible
to Belief functions. In addition, we investigate adequate sets of events from the
perspective of this rule.
4
Coherence for Belief Functions
Let us begin by recalling Jaﬀray’s extension of de Finetti’s Dutch Book to Belief
functions [15]. The key idea of his framework is that if an event ψ occurs, then
every non-contradictory event which logically follows from ψ, also occurs.
Let Ψ = {ψ1, . . . , ψn} be a set of events and β : ψ1 →β1, . . . , ψn →βn a book
published by the bookmaker B. If the gambler G places real stakes σ1, . . . , σn
on ψ1, . . . , ψn at the betting odds written in β, then G pays B for each ψi the
amount σiβi and gains the amount σiCψ(ψi). The function Cψ(ψi) is deﬁned
as follows, where |=cl denotes the consequence relation of classical propositional
logic.
Cψ(ψi) =

1
if |=cl ψ →ψi,
0
otherwise.
The total balance for B is
n

i=1
σi(βi −Cψ(ψi)).
Deﬁnition 5 (Coherence Under Partially Resolved Uncertainty). Let
Ψ = {ψ1, . . . , ψn} be a set of events β a book over Ψ. The book β is coherent
under partially resolved uncertainty if there is no choice of stakes which G can
make, exposing B to a sure loss, i.e. it is not the case that, for every ﬁxed non-
contradictory event ψ, n
i=1 σi(βi −Cψ(ψi)) < 0.
This notion of coherence characterises Dempster-Shafer Belief functions [21]
in the sense that a book β on Ψ is coherent under partially resolved uncertainty
if and only if it extends to a Belief function on the algebra of events, [19].
A criterion based on a Brier-like scoring rule can be put forward for Belief
functions as follows. From Deﬁnition 1 above it follows that a forecast β is
dominated by another forecast β′ if the squared euclidean distance between β
and the vertices of PΨ is greater than the squared euclidean distance between β′
and the same points. Thus, we can move β to reduce the distance from all points
of PΨ, vertices included. Hence, we can extend the deﬁnition of dominating
forecast to Belief functions. To do this we consider the vertices of the polytope
BΨ, rather than those of PΨ. This distance can be recovered from the Cψ
function used by Jeﬀray recalled above:
L(ψi, βi) = (∥Cψ(ψi) −βi∥2)2.
(5)

308
E. A. Corsi et al.
Remark 2. As pointed out by two anonymous referees, the forthcoming [18] pro-
vides a characterisation of scoring rules for Belief functions yielding a result
which is essentially equivalent to Theorem 3 below. Whilst we take this as wel-
come news to the eﬀect that scoring-rule based coherence is of interest beyond
the well-known probabilistic case, the present work diﬀers substantially in its
motivating question from [18]. In particular, whilst this latter reviews several
candidate notions for ‘coherent’ Belief functions, our present concern lies in
comparing Belief functions and Lower probabilities against the natural formula-
tion provided by Deﬁnition 6. Indeed, as we point out in Subsect. 5.2, the notion
captured by this Deﬁnition allows us to tell coherence-wise Belief functions from
Lower probability, a distinction not permitted by Dutch-Book coherence [4].
Deﬁnition 6 (Scoring Rule for Belief functions).
Let Ψ = {ψ1, . . . , ψn}
be a set of events and, for i = 1, . . . , n, let βi ∈[0, 1] be the value the forecaster
F assigns to each ψi. The forecast β : ψ1 →β1, . . . , ψn →βn is coherent if
there is no distinct forecast β′ whose Brier score uniformly dominates β, i.e. if
L(Ψ, β) = 
i(∥Cψ(ψi) −βi∥2)2, then there is no β′ s.t. L(Ψ, β′) < L(Ψ, β) for
every non-contradictory event ψ and every possible value of Cψ(ψi).
As shown in the following Proposition, BΨ can be characterised by using the
Cψ function. We denote by Cψ(Ψ) the vector (Cψ(ψ1), . . . , Cψ(ψn)).
Proposition 2. Let A be a boolean algebra, Ψ = {ψ1, . . . , ψn} be a ﬁnite set of
events over A and β : Ψ →[0, 1] be an assignment over Ψ. Then
BΨ = co(Cψ(Ψ)| ψ is a non-contradictory event in A).
Proof. Since both BΨ and co(Cψ(Ψ)| ψ is a non-contradictory event in A)
are convex hulls, to show that they coincide, it is suﬃcient to show that they
share the same vertices. In particular, recall that the vectors ej are deﬁned as
ej = (vj(ψ1), . . . , vj(ψn)) where vj is an homomorphisms of A to the boolean
chain on the two-element set {0, 1}. Thus, ej identiﬁes in Rn the same point of
Cαj(Ψ) = (Cαj(ψ1), . . . , Cαj(ψn)) where αj is an atom of A. The other vertices
of BΨ are recovered from the ej with j = 1, . . . , t by taking the point-wise min-
imum of every subset of ej’s. Since the entailment relation to which Cψ refers
to is the consequence relation of classical propositional logic, for any proposi-
tional formula ψi and ψj we have |=cl (ψi ∨ψj) →ψi iﬀ|=cl ψi →ψi and
|=cl ψj →ψi. Thus, Cαi∨αj(ψi) = 1 iﬀCαi(ψi) = 1 and Cαj(ψi) = 1, i.e.
min{Cαi(ψi), Cαj(ψi)} = 1. Therefore, the vertices of BΨ obtained as ei ∧ej
with i, j ∈{1, . . . , t} identify the same point of Cαi∨αj(Ψ) and, in general,

j∈J⊆{1,...,t}
ej = CψJ(Ψ),
where, for all J ⊆{1, . . . , t}, ψJ = 
j∈J⊆{1,...,t} αj.
Theorem 3. Let A be a boolean algebra, and let Ψ = {ψ1, . . . , ψn} be a ﬁnite
set of elements of A. A forecast β deﬁned over Ψ is coherent if and only if it
can be extended to a Belief function on A.

Scoring Rules for Belief Functions and Imprecise Probabilities
309
Proof. (⇒) If the forecast β is coherent, then there is no rival forecast β′ deﬁned
over the same set of events s.t. L(Ψ, β′) < L(Ψ, β) for every non-contradictory
event ψ and every possible value of Cψ(ψi). By Proposition 2, the vertices of
BΨ coincide with all possible values of CΨ. Thus, β ∈BΨ and by Theorem 1 it
can be extended to a Belief function over A.
(⇐) If the forecast β can be extended to a Belief function over A, then
by Theorem 1, β ∈BΨ and by Proposition 2, β ∈co(Cψ(ψi)| ψ is a non-
contradictory event and i = 1, . . . , n). Therefore, no β′ reduces the (euclidean)
distance from all possible point of that convex hull (vertices included), which
implies that there is no β′ s.t. L(Ψ, β′) < L(Ψ, β) for any possible value of Cψ.
Example 2. Let A be the boolean algebra of 8 elements and 3 atoms {α1, α2, α3}
and consider the non-trivial set of events Ψ = {ψ1, ψ2, ψ3} ⊂A where ψ1 =
α1 ∨α2, ψ2 = α2 ∨α3 and ψ3 = α1 ∨α3. In Table 1 we compute the Cψ(ψi)
function for every non-contradictory event ψ.
Table 1. Values of Cψ(ψi) over the boolean algebra A and the set of events Ψ
ψ
Cψ(ψ1) Cψ(ψ2) Cψ(ψ3)
α1
1
0
1
α2
1
1
0
α3
0
1
1
α1 ∨α2
1
0
0
α2 ∨α3
0
1
0
α1 ∨α3
0
0
1
α1 ∨α2 ∨α3 0
0
0
Let us consider the forecasts β1 : ψ1 →3/8, ψ2 →3/8, ψ3 →3/8 and
β2 : ψ1 →7/8, ψ2 →7/8, ψ3 →7/8. If we compute the convex hull consider-
ing the points identiﬁed by Cψ(Ψi), we can verify that β1 belongs to it while β2
does not. Since the convex hull generated by Cψ coincides with BΨ, β1 extends
to a Belief function and β2 does not.
5
Comparing Scoring Rules for Belief Functions and
Imprecise Probabilities
5.1
Scoring Rule for Imprecise Probabilities
Seidenfeld et al. introduce in [20] the following scoring rule for imprecise proba-
bilities. Let us consider the event ψ and a Lower and Upper probability forecast
(p, q) for the event ψ, that is to say, let us assume that p, q ∈[0, 1] are values
assigned to ψ by a Lower probability and its dual Upper probability, respectively.
The Brier-style IP scoring rule is hence deﬁned as follows.

310
E. A. Corsi et al.
L(ψ, (p, q)) =

(1 −q)2
if v(ψ) = 1,
p2
if v(ψ) = 0.
Extending the concept of dominance between forecasts from the deﬁnitions
on probabilities and Belief functions, we say that a forecast F = {(pi, qi)| i =
1, . . . , n} over the events Ψ = {ψ1, . . . , ψn} strictly dominates another forecast
F′ = {(p′
i, q′
i)| i = 1, . . . , n} if L(Ψ, F) < L(Ψ, F′). The penalty for a set of
forecasts is the sum of the individual penalty scores.
Since the forecast F0 = {(pi = 0, qi = 1)| i = 1, . . . , n} dominates any
other forecast, coherence for imprecise probabilities based on scoring rule risks
to trivialise if we do not further elaborate it. For this reason, we need to consider
an additional restriction on the class of rival forecasts and an index of relative
imprecision between forecasts.
This is the reason why, for each forecast F = {(pi, qi)| i = 1, . . . , n}, Seiden-
feld et al., construct a scoring set SF deﬁned as follows:
SF = {(q1, p2, . . . , pn), (p1, q2, . . . , pn) . . . , (p1, . . . , pn−1, qn)}.
Thus, a forecast F is at least as determinate as a forecast F′ if co(SF) is iso-
morphic under rigid movements of a subset of co(SF′).
Finally, the class M of rival forecasts considered is the ε-contamination
class.2 The scoring criterion for imprecise probabilities is then deﬁned as
follows.
Deﬁnition 7 (Scoring Rule for Imprecise probabilities).
Let A be a
boolean algebra and let F be a forecast over the atoms of the algebra. F is IP-
coherent with respect to M if there is no dominating forecast F′ from M such
that S′
F is at least as determinate as SF.
The following theorem characterises coherent forecasts.
Theorem 4. Let A be a boolean algebra and let F be a forecast over the atoms
Ψ = {α1, . . . , αt} of the algebra A. Then, SF lies entirely within the probability
simplex PΨ iﬀF matches an ε-contamination model and it is IP-coherent with
respect to M.
It is now worth to point out that, for what concerns the extendability problem,
an immediate consequence of the theorem above is that a forecast F = {(pi, qi) |
i = 1, . . . , t} on the atoms α1, . . . , αt is IP-coherent with respect to M if and
only if for all i = 1, . . . , t, pi = min{P(αi) | P ∈co(SF)} and qi = max{P(αi) |
P ∈co(SF)} where Ψ = {α1, . . . , αt}. In addition, as we will see in the following
Section, it can be that a forecast F is not coherent as for Deﬁnition 7 and still
being extendable to an Upper and Lower probability.
2 For what concerns the present paper, we do not need to elaborate more on the pre-
viously given notions and deﬁnitions, and we invite the interested reader to consult
[20] for the relative literature on this class of forecasts.

Scoring Rules for Belief Functions and Imprecise Probabilities
311
5.2
Distinguishing Belief Functions and Imprecise Probabilities
Through Scoring Rules
As illustrated in [4], given a ﬁnite boolean algebra A with at least three atoms
we can identify an adequate set of events and assignments βi that do not satisfy
equation (3) and still cannot distinguish Belief functions from Lower probabili-
ties. In particular, the main result of [4] is based on the following.
Remark 3. [4, Example 2] Let us consider an algebra A with atoms α1, . . . , αt
(t ≥3) and probability distributions p1(α1) = q, p1(α2) = 1 −q, p1(αi) = 0 for
all i ̸= 1, 2; p2(α2) = q, p2(α3) = 1−q, p2(αi) = 0 for all i ̸= 2, 3; p3(α1) = 1−q,
p3(α3) = q, p3(αi) = 0 for all i ̸= 1, 3 where q is any value 1/3 < q ≤1/2. Let us
consider, as events, the co-atoms of A: ψ1 = α1 ∨α2, ψ2 = α2 ∨α3, ψ3 = α1 ∨α3
and the assignment β : ψi →q for every i = 1, 2, 3. The Lower probability P
derived by the probability distributions pj extends β. Furthermore, it not diﬃcult
to show that P is not a Belief function as it fails to prove (3) of Remark 1.
However, the same assignment β is coherent in the sense of Deﬁnition 5 and in
fact there is a Belief function of A that extends it. Thus, for that set of events, it
is impossible to distinguish assignments that are coherent in the sense of Belief
functions, from those that are coherent in the sense of Lower probabilities.
Now, let us move to analyse what happens if we take into account coherence
through proper scoring rules. First, it is easy to see that the assignment displayed
in Remark 3 is coherent according to Deﬁnition 6. However, if we consider the fore-
cast F = {(0, 1−q), (0, 1−q), (0, 1−q)} relative to the same probability distribu-
tion over the atoms of A and construct the scoring set SF = {(1 −q, 0, 0), (0, 1 −
q, 0), (0, 0, 1−q)}, then it does not lie in PΨ ′ where Ψ ′ is the set of the three atoms
considered. In fact, since 1/3 < q ≤1/2, then 1/2 ≤1 −q < 2/3 and co(SF) ̸⊆
PΨ ′ = co((1, 0, 0), (0, 1, 0), (0, 0, 1)). Thus, by Theorem 4, the forecast F is not IP-
coherent with respect to the ε-contamination model class even though the assign-
ments βP : α1 →0, α2 →0, α3 →0 and βP : α1 →1 −q, α2 →1 −q, α3 →1 −q
can be extended to Lower and Upper probabilities, respectively.
Let us conclude with a brief discussion on why scoring rule-based coherence
seems to be stronger (in the sense that allows to distinguish more) than avoiding
sure loss. Indeed, although this argument needs further and deeper insights that
will be addressed in our future work, it seems clear that such an asymmetry
might be caused by the information that these criteria encode in their formal-
ization. In particular, although scoring rule-based criterion for Belief functions
(Deﬁnition 6) relies on the sole information carried by the events ψ1, . . . , ψn on
which (coherent) forecasts are deﬁned, the same criterion for imprecise prob-
abilities (Deﬁnition 7) needs a forecast F to be deﬁned on the atoms and on
the co-atoms of A. In this setting, in fact, a forecast assigns values of Lower
and Upper probability to each atoms and hence, in algebraic terms, it needs
both atoms and co-atoms of A to be deﬁned: if qi = P(αi) = 1 −P(¬αi)
being ¬α = 
j̸=i αj a co-atom of A. We believe that the observed asymmetry
between the Dutch Book and scoring rule-based criteria, especially for imprecise
probabilities, lies in this additional information that the latter requires.

312
E. A. Corsi et al.
6
Conclusion
We investigated particular sets of events that do not distinguish Belief functions
from Lower probabilities from the point of view of Dutch Books, but which are
distinguishable through scoring rules. We introduced a loss function through
which we characterise assignments deﬁned over a generic subset of events and
that can be extended to a Belief function. Considering an already existing scoring
rule deﬁned for imprecise probabilities, we observed that the same assignments
from which we can recover adequate sets of events, through coherence, can dis-
tinguish Lower probabilities from Belief functions. As pointed out at the end of
the previous section, this result opens up new lines of research. In particular,
given the tight connection between the scoring rule criterion and the geometric
interpretation of the extendibility criterion, we could ﬁrst characterise geomet-
rically the sets LΨ and UΨ of the assignments deﬁned over Ψ and that can be
extended to Lower and Upper probabilities. Then, following the same process
used in Sect. 4 for the Belief functions, deﬁne additional scoring rules through
which it is possible to characterise assignments deﬁned over generic sets of events
and that can be extended to Upper or Lower probabilities. We could also inves-
tigate how to deﬁne a scoring rule that characterises assignments extendible to
necessity measures.
The comparison reported in this paper are limited to the unconditional case.
A general logical framework to investigate conditional probability functions has
recently been introduced in [11]. Combining this with the conditional approach
to Duch Book coherence put forward in [2,3] on the one hand, and the approach
based on scoring rules of [13] on the other hand, we aim to extend our compari-
son to conditional, non-additive measures of uncertainty. With regards to Belief
functions the very recent [18] obtains related results from a distinct angle, as
brieﬂy noted in Remark 2 above. Further work will elucidate potential mutual
relations between the two independent approaches.
Acknowledgments. We are grateful to the anonymous referees for their thor-
ough comments on the draft and to the ECSQARU Programme Committee
for their very competent handling of the paper. Corsi and Hosni acknowledge
funding by the Department of Philosophy “Piero Martinetti” of the University of
Milan under the Project “Departments of Excellence 2018–2022” awarded by the
Ministry of Education, University and Research (MIUR). Flaminio acknowledges
partial support by the Spanish project PID2019-111544GB-C21 and by the Span-
ish Ram´on y Cajal research program RYC-2016-19799. Hosni also acknowledges
funding from the Deutsche Forschungsgemeinschaft (DFG, grant LA 4093/3-1).
References
1. Anger, B., Lembcke, J.: Inﬁnitely subadditive capacities as upper envelopes of
measures. Zeitschrift f¨ur Wahrscheinlichkeitstheorie und verwandte Gebiete 68(3),
403–414 (1985)

Scoring Rules for Belief Functions and Imprecise Probabilities
313
2. Coletti, G., Petturiti, D., Vantaggi, B.: A Dutch Book coherence condition for
conditional completely alternating choquet expectations. Bollettino dell’Unione
Matematica Italiana 13(4), 585–593 (2020)
3. Coletti, G., Scozzafava, R.: Probabilistic logic in a coherent setting, vol. 15.
Springer (2002). https://doi.org/10.1007/978-94-010-0474-9
4. Corsi, E.A., Flaminio, T., Hosni, H.: When belief functions and lower probabilities
are indistinguishable. In: ISIPTA 2021 - Proceedings of Machine Learning Research
(147), pp. 83–89 (2021)
5. Cuzzolin, F.: The geometry of consonant belief functions: simplicial complexes of
necessity measures. Fuzzy Sets Syst. 161(10), 1459–1479 (2012)
6. Cuzzolin, F.: The geometry of uncertainty. Springer (2017). https://doi.org/10.
1007/978-3-030-63153-6
7. De Finetti, B.: Theory of probability, vol. 1. John Wiley & Sons (1974)
8. Develin, M., Sturmfels, B.: Tropical convexity. arXiv preprint math/0308254 (2003)
9. Dubois, D., Prade, H.: Possibility Theory: Approach to Computerized Processing
of Uncertainty. Plenum Press, New York (1988)
10. Flaminio, T., Godo, L.: A note on the convex structure of uncertainty measures on
MV-algebras. In: Synergies of Soft Computing and Statistics for Intelligent Data
Analysis, pp. 73–81. Springer (2013). https://doi.org/10.1007/978-3-642-33042-1 9
11. Flaminio, T., Godo, L., Hosni, H.: Boolean algebras of conditionals, probability
and logic. Artif. Intelli. 286, 103347 (2020)
12. Flaminio, T., Godo, L., Marchioni, E.: Geometrical aspects of possibility measures
on ﬁnite domain MV-clans. Soft Comput. 16(11), 1863–1873 (2012)
13. Gilio, A., Sanﬁlippo, G.: Coherent conditional probabilities and proper scoring
rules. In: Proceedings of ISIPTA, vol. 11, pp. 189–198. Citeseer (2011)
14. Halpern, J.Y.: Reasoning about uncertainty. MIT press (2017)
15. Jaﬀray, J.Y.: Coherent bets under partially resolving uncertainty and belief func-
tions. Theor. Decis. 26(2), 99–105 (1989)
16. Miranda, E.: A survey of the theory of coherent lower previsions. Int. J. Approxi-
mate Reasoning 48(2), 628–658 (2008)
17. Paris, J.B.: A note on the Dutch book method. ISIPTA. 1, 301–306 (2001)
18. Petturiti, D., Vantaggi, B.: How to assess coherent beliefs: a comparison of dif-
ferent notions of coherence in Dempster-Shafer theory of evidence. arXiv preprint
arXiv:2105.10546 (2021)
19. Regoli, G.: Rational comparisons and numerical representations. Decision Theory
and Decision Analysis: Trends and Challenges, pp. 113–126 (1994)
20. Seidenfeld, T., Schervish, M.J., Kadane, J.B.: Forecasting with imprecise proba-
bilities. Int. J. Approximate Reasoning 53(8), 1248–1261 (2012)
21. Shafer, G.: A mathematical theory of evidence, vol. 42. Princeton University Press
(1976)
22. Walley, P.: Statistical reasoning with imprecise probabilities. Chapman & Hall
(1991)

Comparison of Shades and Hiddenness
of Conﬂict
Milan Daniel1
and V´aclav Kratochv´ıl2(B)
1 Institute of Computer Sciences, Czech Academy of Sciences, Prague, Czechia
milan.daniel@cs.cas.cz
2 Institute of Information Theory and Automation, Czech Academy of Sciences,
Prague, Czechia
velorex@utia.cas.cz
Abstract. Conﬂict, dissonance, inconsistency, entropy. There are many
notions related to one phenomenon. When working with uncertainty,
there can be diﬀerent sources of information, and often they are in some
level of mutual disagreement. When working with belief functions, one of
the approaches how to measure conﬂict is closely connected with a belief
mass assigned by the non-normalized conjunctive rule to the empty set.
Recently, we have observed and presented cases where a conﬂict of belief
functions is hidden (there is a zero mass assigned to the empty set by
the non-normalized conjunctive rule). Above that, we distinguish several
degrees of such a hiddeness. In parallel, Pichon et al. introduced a new
family of conﬂict measures of diﬀerent strengths, the so-called shades of
conﬂict. In this paper, we compare both approaches not only from the
theoretical point of view but also by examples.
Keywords: Belief function · Conﬂict · Hidden conﬂict ·
N-consistency · Shades of conﬂict · Auto-conﬂict
1
Introduction
When combining belief functions (BFs) by the conjunctive rules of combination,
some conﬂicts often appear (they are assigned either to ∅by the non-normalized
conjunctive rule
∩
⃝or distributed among other belief masses by normalization
in Dempster’s rule of combination ⊕). A combination of conﬂicting BFs may
be complicated and interpretation of their conﬂicts is often questionable in real
applications.
The sum of all multiples of conﬂicting belief masses (denoted by m ∩
⃝(∅))
was interpreted as a conﬂict between BFs in the classical Shafer’s approach [14].
Nevertheless, examples of mutually non-conﬂicting BFs with high m ∩
⃝(∅) were
observed as early as in the 1990s [1]. Diﬀerent approaches to understand and
This work was supported by the institutional support RVO: 67985807 (ﬁrst author)
and grant GA ˇCR 19-04579S (second author).
c
⃝Springer Nature Switzerland AG 2021
J. Vejnarov´a and N. Wilson (Eds.): ECSQARU 2021, LNAI 12897, pp. 314–327, 2021.
https://doi.org/10.1007/978-3-030-86772-0_23

Comparison of Shades and Hiddenness of Conﬂict
315
cover the nature of conﬂicts of BFs resulted in numerous papers; for references
see, e.g., [9,11,13].
The value m ∩
⃝(∅) is frequently higher than an expected value of conﬂict even
for partially conﬂicting BFs, This is explained in [3] where two quantitatively
diﬀerent parts of conﬂicts are distinguished: an internal conﬂict of an individ-
ual BF (due to its internal inconsistency), and an actual conﬂict between two
diﬀerent BFs (due to a conﬂict of represented evidence).
On the other hand, even though zero-sum of all multiples of conﬂicting belief
masses m ∩
⃝(∅) is usually considered as non-conﬂictness of BFs. When analysing
properties of conﬂict between belief functions based on their non-conﬂicting parts
[4,5], a positive value of a conﬂict was observed even in a situation when m ∩
⃝(∅)
equals zero.
We have investigated and presented this in [8,9] where diﬀerent degrees
of hidden conﬂicts were introduced when m ∩
⃝(∅) = 0. The phenomenon may
appear even inside individual BFs as a hidden internal conﬂict (hidden auto-
conﬂict) [6,9]. Related diﬀerent degrees of non-conﬂictness have been presented
in [7].
In parallel to our research, Pichon et al. [13] introduced an entire family of
measures of conﬂict based on a consistency of results of conjunctive combination
of BFs in question. There are diﬀerent degrees/shades of consistency and of
related conﬂicts.
Both of the approaches come from diﬀerent directions, see [9] and [13], thus
they present diﬀerent, nevertheless analogous results of diﬀerent strength. Let us
sketch the approaches in Sects. 3 and 4 and then compare them both on examples
in Sect. 5.1 and also theoretically in Sect. 5.2.
2
Basic Notions
In this section, we will recall some basic notations needed in this paper.
Assume a ﬁnite frame of discernment Ω. In the case of |Ω| = n, we will
highlight this fact using a subscript as Ωn and we assume Ωn = {ω1, ω2, . . . , ωn}.
P(Ω) = {X|X ⊆Ω} is a power-set of Ω. P(Ω) is often denoted also by 2Ω, e.g.,
in [13].
A basic belief assignment (bba) is a mapping m : P(Ω) −→[0, 1] such that

A⊆Ω
m(A) = 1;
the values of the bba are called basic belief masses (bbm). m(∅) = 0 is usually
assumed. We sometimes speak about m as of a mass function.
There are other equivalent representations of m: A belief function (BF) is a
mapping Bel : P(Ω) −→[0, 1], Bel(A) = 
∅̸=X⊆A m(X). A plausibility function
Pl : P(Ω) −→[0, 1], Pl(A) = 
∅̸=A∩X,X⊆Ω m(X). Because there is a unique
correspondence among m and corresponding Bel and Pl, we often speak about
m as of a belief function.

316
M. Daniel and V. Kratochv´ıl
Let X ⊆Ω and Bel be a belief function deﬁned on Ω by bba m. If m(X) > 0
for X ⊆Ω, we say X is a focal element of Bel. The set of focal elements is
denoted by F. We say that a focal element X ∈F is proper if X ̸= Ω. If all
focal elements are singletons (|X| = 1, X ∈F), then we speak about a Bayesian
belief function (BBF); in fact, it is a probability distribution on Ω. If there are
only focal elements such that |X| = 1 or |X| = n we speak about a quasi-
Bayesian BF (qBBF). In the case of m(Ω) = 1 we speak about the vacuous
BF (VBF) and about a non-vacuous BF otherwise. If all focal elements have a
non-empty intersection, we speak about a consistent BF.
Dempster’s (normalized conjunctive) rule of combination ⊕:
(m1 ⊕m2)(A) =

X∩Y =A;X,Y ⊆Ω
Km1(X)m2(Y )
for A ̸= ∅, where K =
1
1−κ,
κ =

X∩Y =∅;X,Y ⊆Ω
m1(X)m2(Y ),
(1)
and (m1⊕m2)(∅) = 0, see
[14]. Putting K = 1 and (m1 ∩
⃝m2)(∅) = κ we obtain
the non-normalized conjunctive rule of combination
∩
⃝, see e.g. [15]:
(m1 ∩
⃝m2)(A) =

X∩Y =A;X,Y ⊆Ω
m1(X)m2(Y )
for any A ⊆Ω. κ > 0 is usually considered to represent a conﬂict of respective
belief functions. Following the notation from [13], focal elements of m1 ∩
⃝m2 are
denoted as F12. To simplify formulas, we often use
∩⃝3
1m = m ∩
⃝m ∩
⃝m, and
also
∩⃝k
1(m1 ∩
⃝m2) = (m1 ∩
⃝m2) ∩
⃝. . .
∩
⃝(m1 ∩
⃝m2), where (m1 ∩
⃝m2) is repeated
k-times.
3
Hidden Conﬂict
The notion of a hidden conﬂict was introduced in [8,9] based on an interest-
ing observation during the analysis of the conﬂicting measure based on non-
conﬂicting parts (for deﬁnitions1 see [4,5,9]). Speciﬁcally, a positive value of
this measure has been observed while the classical measure of conﬂict indicated
nothing (i.e. mass assigned to the empty set by the conjunctive rule was zero).
Above that, several degrees of hiddeness can be distinguished. The deﬁnition
follows:
1 In this comparison, we present just the notions necessary for understanding of both
the compared approaches, for more detail including motivations see [9] and [13]. For
irrelevance of dependence/independence of belief sources for conﬂicts see also [9].

Comparison of Shades and Hiddenness of Conﬂict
317
Deﬁnition 1. Assume two BFs Beli, Belii deﬁned by mi and mii such that for
some k>0 (∩⃝k
1(mi∩
⃝mii))(∅) = 0. If there further holds (∩⃝k+1
1
(mi∩
⃝mii))(∅) > 0
we say that there is a conﬂict of BFs mi and mii hidden in the k-th degree
(hidden conﬂict of k-th degree). If there is already ( ∩⃝k+1
1
(mi ∩
⃝mii))(∅) =
(mi ∩
⃝mii)(∅) > 0 for k = 0 then there is a conﬂict of respective BFs which
is not hidden or we can say that it is conﬂict hidden in degree zero.
Note that the existence of conﬂict does not correspond to any particular
assignment of mass values to focal elements, but the structure of focal elements
of individual belief functions. That is also why we specify just the structure of
focal elements in the following examples, usually using a ﬁgure. For illustration,
see Fig. 1.
In the following examples, we would like to illustrate how the hidden con-
ﬂict is revealed. Note that because of the commutativity of
∩
⃝, we can rewrite

∩⃝3
1(mi ∩
⃝mii)

into

∩⃝3
1(mi) ∩
⃝∩⃝3
1(mii)

, etc. Once a positive mass is assigned
to the empty set, it cannot be removed by
∩
⃝. To simplify the following exam-
ples, let us highlight the ﬁrst occurrence of a positive mass on an empty set by
dropping some BFs from respective formulas according to Deﬁnition 1.
Introductory Example.
Let us assume Bel′, Bel′′ on Ω3 where F′ =
{{ω1, ω2}, {ω1, ω3}} and F′′ = {{ω2, ω3}} (see Fig. 1). Then (m′ ∩
⃝m′′)(∅) = 0.
But (m′ ∩
⃝m′
∩
⃝m′′)(∅) > 0 (as highlighted in Fig. 1 where conﬂicting focal ele-
ments are drawn in red), which implies ( ∩⃝2
1(m′ ∩
⃝m′′))(∅) > 0 as well. Thus,
there is a conﬂict hidden in the 1-st degree. For detail including numeric belief
masses see [9].
∩
=
∩
∩
=
and
∅
Fig. 1. Arising of a hidden conﬂict: focal elements of m′, m′, m′′; m′ ∩
⃝m′, m′′; and of
m′ ∩
⃝m′ ∩
⃝m′′. (Color ﬁgure online)
For a conﬂict hidden in a higher degree, see the following Little Angel
example.
Little Angel Example. Let us have two BFs Beli and Belii on Ω5: Fi =
{{ω1, ω2, ω5}, {ω1, ω2, ω3, ω4}, {ω1, ω3, ω4, ω5}}, Fii = {{ω2, ω3, ω4, ω5}}, i.e.,
|Fi| = 3 while |Fii| = 1. Respective structures can be seen in Fig. 2 where
sets of focal elements of individual BFs mi (3×) and mii (1×) are depicted
in its ﬁrst row (ω1 is on the top with ωis clock-wise enumerated). Again,
there is (mi ∩
⃝mii)(∅) = 0 (there is no empty intersection of any X ∈Fi
with Y ∈Fii). Moreover, ( ∩⃝2
1(mi ∩
⃝mii))(∅) = 0 in this example. Finally,

318
M. Daniel and V. Kratochv´ıl
( ∩⃝3
1(mi ∩
⃝mii))(∅) > 0. Following the second line of Fig. 2, the empty set emerges
as the intersection of focal elements drawn by red color, i.e., it appears already
in mi ∩
⃝mi ∩
⃝mi ∩
⃝mii.
Fig. 2.
Little
Angel
example:
focal
elements
of
mi, mi, mi, mii;
∩
⃝3
1mi, mii;
( ∩
⃝3
1mi) ∩
⃝mii (Color ﬁgure online)
Let us note that we can assume independent beliefs mi and mii as originally
in [8] and for low k also couples or k-tuples of same valued (identical) BFs from
independent sources. Nevertheless, we are not interested in
∩⃝k
1m in general for
any k > 1 here. We are interested only in whether its value for the empty set is
zero or positive. This is needed for recognition whether the original pair of BFs
has some hidden conﬂict or for proof that respective BFs are non-conﬂicting.
Therefore no source independence assumption is needed in this case. For more
details see [9].
Two major theorems specify an upper bound on the maximal degree of hidden
conﬂict. For their proofs, see [9].
Theorem 1. Hidden conﬂict of non-vacuous BFs on Ωn, n > 1 is always of a
degree less or equal to n −2; i.e., the condition
( ∩⃝n−1
1
(mi ∩
⃝mii))(∅) = 0
(2)
always means full non-conﬂictness of respective BFs and there is no hidden con-
ﬂict.
Theorem 2. (i)
Assume two non-vacuous BFs Beli, Belii given by mi, mii
on Ωn. Then
( ∩⃝c
1(mi ∩
⃝mii))(∅) = 0
means full non-conﬂictness of respective BFs for c = min(ci, cii)+sgn(|ci−cii|),
where ci, cii are the maximal cardinalities of proper focal elements of mi, mii.
(ii) For any two non-vacuous quasi-Bayesian BFs mi, mii on any frame of dis-
cernment Ωn the condition (mi ∩
⃝mii)(∅) = 0 always means full non-conﬂictness
of the BFs.
(iii) For any BF mi and any quasi-Bayesian BF mii the condition
( ∩⃝2
1(mi ∩
⃝mii))(∅) = 0 always means full non-conﬂictness of the BFs.

Comparison of Shades and Hiddenness of Conﬂict
319
4
Shades of Conﬂict
There is a large family of shades of conﬂicts recently deﬁned by Pichon et al.
in [13] which is based on dual family of consistency measures. Let us start with
unique total inconsistency of the empty mass function m∅, deﬁned by the expres-
sion m(∅) = 1 and on the other hand by deﬁnition of a family of (total) N-
consistencies of belief functions. To do this we need several technical notions
from [13].
Let us suppose a belief function Bel given by mass function (bba) m. Let
us denote its set of focal elements (focal sets) by F where |F| is its cardinality
(originally denoted by F by [13] ). Pichon et al. [13] similarly to Destercke and
Burger [11] denote a union of all focal elements by D2. I.e., D = 
X∈F X. M
simply denotes the set of all mass functions (bbas).
Deﬁnition 2. (N-consistency). A mass function m is said to be consistent of
order N (N-consistent for short), with 1 ≤N ≤|F|, iﬀits focal sets are N-wise
consistent, i.e., ∀F′ ⊆F s.t. |F′| = N, we have 
A∈F′ A ̸= ∅.
In addition, let ΦN denote the measure from M to [0, 1] such that, for 1 ≤
N ≤|F|, and all m ∈M, ΦN(m) := 1 −( ∩⃝N
1 m)(∅).
Note that Pichon et al. [13] use notation mN for ( ∩⃝N
1 m) = m ∩
⃝m ∩
⃝. . .
∩
⃝m
(N copies of m are combined). Similarly, Martin et al. [12] call the belief mass
assigned by this combination to the empty set as the auto-conﬂict of m: aN(m) =
( ∩⃝N
1 m)(∅).3 Let us further note that the auto-conﬂict does not work with the
whole
∩⃝N
1 m, but with its value for the empty set only. Thus, any assumption of
independence (in the sense of belief sources) is not necessary. In general, Pichon
et al. [13] consider belief functions from independent sources for their conﬂict
measures.
The family of N-consistencies represents diﬀerent strengths of consistency
from the weakest 1-consistency to the strongest |F|-consistency. And as it is
referred to, in [13], it covers three previously deﬁned notions of consistency:
We can easily see that a mass function is 1-consistent iﬀintersections

A∈F′ A are nonempty for all sets F′ of focal elements of cardinality 1 = |F′|,
thus simply iﬀall focal elements are nonempty. This weakest consistency is sat-
isﬁed by all normalized BFs (m(∅) = 0), thus by all BFs in classical Shafer’s
approach; this is called probabilistic consistency by Destercke and Burger in [11].
For N = |F′| = 2 we obtain ∀X,Y ∈FX ∩Y ̸= ∅, i.e., Yager’s pairwise consistency
[17]. And ﬁnally, for N = |F| we obtain the strongest consistency 
A∈F A ̸= ∅
which corresponds to logical consistency from [10,11]. It also coincides with the
consistency notion from Sect. 2. As |F| ≤|P(Ω)| = 2n −1 there are up to 2n −1
2 Note, that this union is called a core of a belief function and denoted by C by Shafer
in [14] ; on the other hand Cuzzolin uses a completely diﬀerent (conjunctive) core
deﬁned as C = 
A∈F A in [2].
3 More precisely, Martin et al.do not use
∩
⃝, but ⊕sign, which we use as Shafer in
his normalized approach [14] for Demspter’s rule, where always m(∅) = 0, (mi ⊕
mj)(∅) = 0 and (k
1 m)(∅) = 0.

320
M. Daniel and V. Kratochv´ıl
consistencies of diﬀerent strength. These grades of consistency are referred to as
shades of consistency in [13].
In accordance with unique total inconsistency and shades of consistency,
Pichon et al. [13] consider unique total conﬂict of mass functions from [11]: m1
and m2 are totally conﬂicting when D1 ∩D2 = ∅, and they deﬁne a family
of grades/shades of non-conﬂictness as N-consistency of mass function m12 =
m1 ∩
⃝m2 with the set of focal elements F12. And further, they deﬁne a related
family of measures of conﬂict.
Deﬁnition 3. (Order of Non-conﬂictness)
m1 and m2 are said to be non-
conﬂicting of order N (N-non-conﬂicting for short), with 1 ≤N ≤|F12|, iﬀ
∀Ai ∈F12, i = 1, . . . , N, we have N
i=1 Ai ̸= ∅.
Analogously to N-consistency, also N-non-conﬂict subsumes several previous
deﬁnitions: m1 and m2 are 1-non-conﬂicting iﬀall A ∈F12 are nonempty, i.e.
iﬀX ∩Y ̸= ∅for any X ∈F1, Y ∈F2, thus iﬀ
X∩Y =∅m1(X)m2(Y ) = 0,
i.e., non-conﬂict by Destercke & Burger [11] as well as by Shafer [14]. 2-non-
conﬂict (of order 2) corresponds to Yager’s deﬁnition of pair-wise consistency
and |F12|-non-conﬂict is strong non-conﬂict : 
A∈F1∪F2 A ̸= ∅from [11].
Deﬁnition 4. (Measures of conﬂict κN)
Measure of conﬂict κN of belief
masses m1 and m2 is the measure κN(m1, m2) : M × M −→[0, 1] deﬁned
by κN(m1, m2) = 1 −ΦN(m1 ∩
⃝m2), for 1 ≤N ≤|F12|.
The
above
deﬁnition
subsumes
previous
measure
of
conﬂict
κm:
κ1(m1, m2) = 1 −Φ1(m1 ∩
⃝m2) = 1 −(1 −m12(∅)) = m12(∅)) = κm(m1, m2)
[11,13]. It is the classical measure conﬂict as it is used in Smets’ TBM [15,16]
and its logarithm, more precisely log(
1
1−κm ), deﬁnes the weight of conﬂict in
Shafer’s [14]. The deﬁnition does not subsume κπ = 1 −Φπ as corresponding
measures Φπ and Φ|F12| are not equal, see Remark 1 in [13], where Φπ is max
contour (see Φpl in [11]).
Similarly, shades of consistency are deﬁned up to 2n −1 shades where κ|F12|
is the strongest and κ1 is the weakest one from the family. Above that, it always
holds κN−1(m1, m2) ≤κN(m1, m2).
5
Comparison of both Approaches
In the previous sections, we have presented two diﬀerent approaches to conﬂict.
Despite that the conjunctive combination of a belief function with itself appears
in both of them, there are diﬀerent motivations for it. In the hidden conﬂict, it
has been just a simple idea that a combination of two non-conﬂicting BFs should
remain non-conﬂicting with any of the input belief functions, and the idea was
repeated resulting in a repetitive combination of the respective belief functions.
The second approach—the so-called shades of conﬂict—is based on the idea
of k combinations of m12 (i.e. of m ∩
⃝) into
∩⃝k
1m12, (i.e., based on auto-conﬂict
of the k-th degree), to make all the intersections of k focal elements for 0 < k ≤
|F12|. Because N-consistency of m12 should be checked.

Comparison of Shades and Hiddenness of Conﬂict
321
Let us compare these approaches now. We can immediately note, that the
number of degrees of hidden conﬂict is limited by |Ωn| = n in general and further
by the cardinality of the biggest proper focal elements. On the other hand, there
is no speciﬁed upper bound for shades so far. Thus, there are shades up to 2n −1
(up to the maximal theoretical number of focal elements on Ωn).
5.1
Comparison by Examples
Let us take a look at both approaches by several examples. We will start with
the examples from Sect. 3.
Introductory and Little Angel Examples. There is a hidden conﬂict of
the ﬁrst degree in the Introductory example. There are two focal elements of
m1 ∩
⃝m2, see Fig. 3 (i). Theoretically, two shades of conﬂict are deﬁned. m′ and
m′′ are 1-non-conﬂicting, but they have positive 2-conﬂict κ2(m′, m′′) > 0. Sim-
ilarly, there is a hidden conﬂict of the second degree in Little Angel example.
There are three focal elements of m ∩
⃝= mi ∩
⃝mii, see Fig. 4 (i), thus there
are three shades of conﬂict—three measures of conﬂict κ1, κ2, κ3: κ1(mi, mii) =
m ∩
⃝(∅) = 0 = ( ∩⃝2
1m ∩
⃝)(∅) = κ2(mi, mii) while κ3(mi, mii) = ( ∩⃝3
1m ∩
⃝)(∅) >
0. In both the examples, degree of hidden conﬂict corresponds with the num-
ber of shades. Nevertheless in general a number of shades can be greater. To
illustrate that, see the following modiﬁcation of the above mentioned examples.
Extension of the Introductory Example. Let us modify the Introductory
example by adding Ω3 as a focal element4 to m′′. Then we obtain m′ ∩
⃝m++
with four focal elements (as depicted in Fig. 3 (ii)). Note that it has four shades
of conﬂict and four measures of conﬂict. Going further we can analogously add
focal element of cardinality 3 to m′ to obtain m+ ∩
⃝m++ with six focal elements
– i.e. with six shades of conﬂict and six measures of conﬂict. See Fig. 3 (iii).
Continuing further, we can add the singleton {ω1} to m++, then we obtain
m+ ∩
⃝m∗∗with seven focal elements, thus with seven shades of conﬂict and seven
measures of conﬂict (Fig. 3 (iv)). There are seven conﬂict measures5 κ1, κ2, ...κ7:
κ1(m+, m∗∗) = m ∩
⃝(∅) = (m+ ∩
⃝m∗∗)(∅) = 0 < κ2(m+, m∗∗) = ( ∩⃝2
1m ∩
⃝)(∅) <
... < κ7(m+, m∗∗) = ( ∩⃝7
1m ∩
⃝)(∅) = m7
∩
⃝(∅).
Focal elements of BFs are enough for checking N-non-conﬂictness and
zero/positive values of conﬂict measures. Analogously, it is enough to check
degrees of hidden conﬂicts. Note that there is just a hidden conﬂict of the ﬁrst
degree in all these extensions of the Introductory example from Figs. 3 (ii)-(iv).
4 There are focal elements m′, m′′ substituted by some of m+, m++, m∗, m∗∗in the fol-
lowing extensions; analogously focal elements mi, mii are substituted by mxi and/or
mxii later in extensions of the Little Angel example.
5 Note, that m ∩
⃝denotes here always the result of non-normalized conjunctive com-
bination
∩
⃝of a couple of bbas corresponding to the example extension in question,
thus it varies and we can see its precise deﬁnition from the context.

322
M. Daniel and V. Kratochv´ıl
We have reached seven measures of conﬂict, i.e., the maximum number 23−1
for belief functions on Ω3 according to deﬁnitions from the previous section. We
already cannot increase the number of conﬂict measures, but we can increase a
diﬀerence between degree of hidden conﬂict and number of shades by decreasing
the degree of hidden conﬂict, e.g., as it follows. Les us add singleton {ω1} also
to m+, hence we obtain un-hidden conﬂict of m∗and m∗∗, i.e., hidden in degree
zero, see Fig. 3(v). In this case we have seven measures 0 < κ1(m∗, m∗∗) <
κ2(m∗, m∗∗) < ... < κ7(m∗, m∗∗). As the auto-conﬂict ak(m∗∩
⃝m∗∗) is positive
and increases already from its ﬁrst degree a1(m∗∩
⃝m∗∗).
Concerning Fig. 3 and all the subsequent ones, focal elements appearing
already in the previous row are drawn in gray.
Fig. 3. Focal elements of couple of BFs on Ω3 and their
∩
⃝combination. (i) m′, m′′
and m′ ∩
⃝m′′, N ≤2, (ii) m′ ∩
⃝m++, N ≤4, (iii) m+ ∩
⃝m++, N ≤6, (iv) m+ ∩
⃝m∗,
N ≤7, (v) m∗∩
⃝m∗, N ≤7; N shades, hidden conﬂict of the ﬁrst degree in (i)–(iv),
unhidden conﬂict in (v).
Extension of the Little Angel Example. Let us, similarly to the previous
case, add focal element of cardinality 5 to mii, see Fig. 4 (ii). Then we obtain
six focal elements of mi ∩
⃝mxii, thus six conﬂict measures κN. Adding the same
focal element also to mi, see Fig. 4 (iii), we obtain even 8 focal element, thus
eight measures of conﬂict, where κ1(mxi, mxii) = m ∩
⃝(∅) = (mxi ∩
⃝mxii)(∅) =
0 = ( ∩⃝2
1m ∩
⃝)(∅) = κ2(mxi, mxii)< κ3(mxi, mxii) = ( ∩⃝3
1m ∩
⃝)(∅) < ... <

Comparison of Shades and Hiddenness of Conﬂict
323
κ8(mxi, mxii) = ( ∩⃝8
1m ∩
⃝)(∅).6 The degree of hidden conﬂict is kept 2 in both
of these extensions.
There are many other such extensions of the example with diﬀerent size up to
8 of family of conﬂict measures. Analogously to the Introductory Example there
are also extensions decreasing the hiddeness of conﬂict and further increasing
number7 of conﬂict measures.
Fig. 4. Focal elements of couple of BFs on Ω5 and their
∩
⃝combination. (i) mi, mii
and mi ∩
⃝mii, N ≤3, (ii) mi ∩
⃝mxii, N ≤6, (iii) mxi ∩
⃝mxii, N ≤8; N shades, always
hidden conﬂict of the second degree.
Non-conﬂicting Example. We have observed that the size of family of mea-
sures κN oversizes the degree of hiddeness of conﬂict in numerous examples. Let
us look also at some non-conﬂicting cases now. Let us take Little Angel exam-
ple but instead of mii consider bba miii with four focal elements {ω1, ω2, ω5},
{ω1, ω3, ω4}, {ω1, ω2, ω3}, {ω1, ω4, ω5} as depicted in Fig. 5.
Combining non-conﬂicting mi and miii we obtain mi ∩
⃝miii with nine focal
elements, see Fig. 5 (i), thus nine conﬂict measures κ1, ..., κ9. Adding Ω5 as focal
6 Let us notice, that the example from Fig. 4 (iii) is, in fact, discounting of that from
4 (i), hence conﬂict decreasing while the number of deﬁned shades of conﬂict is
increased from 3 to 8; and analogously 3 (iii) is discounting of 3 (i) with four more
shades.
7 There are interesting open issues: what is max number of shades of conﬂict related
to a hidden conﬂict for this example; in general on Ω5, and in full generality on Ωn?
And analogously, a number of shades related to a hidden conﬂict of the 2-nd degree
and k-th degree?.

324
M. Daniel and V. Kratochv´ıl
element to miii we obtain mi ∩
⃝mxiii with 11 f.e., both four-elements from mi
are added, see Fig. 5 (ii); and adding Ω5 also to mi we obtain mxi ∩
⃝mxiii with
12 f.e., where Ω5 is newly added, see Fig. 5 (iii). Hence we obtain dozen conﬂict
measures κN(mxi, mxiii) = ( ∩⃝N
1 (mxi ∩
⃝mxiii))(∅) = ( ∩⃝N
1 m ∩
⃝)(∅), such that
κ1(mxi, mxiii) = 0 = κ2(mxi, mxiii) = ... = κ12(mxi, mxiii).
5.2
A Theoretic Comparison
According to Sect. 3, there are up to n −2 degrees of hiddeness conﬂicts for any
couple of BFs on Ωn. This number is further decreased to c −1 by Theorem 2.
There are N-consistencies deﬁned by Deﬁnition 2 for N = 1, ..., |F|, where num-
ber of focal elements is limited only by the power set of the frame of discernment
thus 2n −1 for Ωn. Further, there are up to 2n −1 degrees of non-conﬂictness
deﬁned by Deﬁnition 3 and up to 2n −1 measures of conﬂict κN in general.
Fig. 5. Focal elements of couple of non-conﬂicting BFs on Ω5 and their
∩
⃝combination.
(i) mi, miii and mi ∩
⃝miii, N ≤9, (ii) mi ∩
⃝mxiii, N ≤11, (iii) mxi ∩
⃝mxiii, N ≤12;
N shades, no conﬂict of any degree.

Comparison of Shades and Hiddenness of Conﬂict
325
This number is decreased for a given pair of BFs m1, m2 by |F12|, i.e., number
of focal elements of m12 = m1 ∩
⃝m2.8
As it has been illustrated by above examples, even the decreased number of
κN is often greater than the degree of hidden conﬂict of particular BFs.
Observation 1. The degree of hidden conﬂict of m1 and m2 is equal to number
of zero κN values for the pair of BFs, i.e., it is equal to max degree of N-
consistency of m1 ∩
⃝m2 and max degree of N-non-conﬂictness of the couple m1
and m2. For unhidden conﬂict, thus 0 degree of hidden conﬂict there is no N-
consistency, for non-conﬂicting BFs there is |F12|-consistency and all κN equal
to zero.
The results on non-conﬂictness are stronger in the hidden conﬂict approach.
Using the above comparison of the approaches we can strengthen the results on
N-conﬂictness:
Theorem 3. n −1-non-conﬂicting BFs m1 and m2 on any Ωn are |F12|-non-
conﬂicting, i.e., strongly non-conﬂicting.
Proof. For maximal n−2 degree of hidden conﬂict (Theorem 1) there is n−2-non-
conﬂictness, hence N-non-conﬂictness for N > n −2 already means no hidden
conﬂict thus strong non-conﬂictness.
Theorem 4. c-non-conﬂicting BFs m1 and m2 on any Ωn such that c =
min(ci, cii) + sgn(|ci −cii|), where ci, cii are the maximal cardinalities of proper
focal elements of m1, m2 are |F12|-non-conﬂicting, i.e., strongly non-conﬂicting.
Proof. Analogously, c −1 is max degree of hidden conﬂict (Theorem 2 ), there
is c −1-non-conﬂictness, hence N-non-conﬂictness for N > c −1 already means
no hidden conﬂict thus strong non-conﬂictness.
On the other side, there are some cases for large frames of discernment and
big minimal focal elements, where number of focal elements of m1 ∩
⃝m2 may be
less than c from Theorem 2. Thus Pichon el al.’s results [13] are stronger in
such cases and we can strengthen results on hidden conﬂicts for such cases as it
follows:
Theorem 5. A hidden conﬂict of any non-vacuous BFs m1 and m2 on any Ωn
has always a degree less than or equal to |F12| −1.
Proof. The assertion follows the fact that ( ∩⃝|F12|
1
(m1 ∩
⃝m2))(∅) = 0 already
means full/strong non-conﬂictness.
8 More correctly, there are 2n −1 measures κN for BFs on Ωn, where only |F12| values
are deﬁned as N-conﬂict of corresponding BFs m1 and m2 by Pichon et al. [13].
Nevertheless, on the other hand, a degree of auto-conﬂict is not limited, thus there
exist values of κN(m1, m2) for all N ≤2n −1 regardless the deﬁnitions from Sect. 4,
originally from [13] : κN(m1, m2) = 1 −ΦN(m1 ∩
⃝m2) = 1 −(1 −(m1 ∩
⃝m2)N(∅)) =
( ∩
⃝N
1 (m1 ∩
⃝m2))(∅) = aN(m1 ∩
⃝m2); in fact there exist values for any ﬁnite N and
limit is either 0 for totally non-conﬂicting BFs or 1 if there is any kind of conﬂict.

326
M. Daniel and V. Kratochv´ıl
Having the above presented strengthening of previous results, there arises a
new question: whether there is some upper bound of degrees of hidden conﬂicts
and shades of conﬂict following some condition about small focal elements?
6
Conclusion
We compared two approaches to diﬀerent degrees of conﬂictness of belief func-
tions. Starting points and motivations of these approaches are quite diﬀerent,
nevertheless both approaches use conjunctive combination to combine beliefs in
question, and they compute belief mass assigned to empty set by repeated com-
bination of resulting BF with itself, called auto-conﬂict by Martin et al. Both
these approaches are compared not only from the theoretical point of view but
also using both conﬂicting and non-conﬂicting examples.
Our comparison has even fruitfully improved both approaches. New theorems
introducing bounds of the number of diﬀerent N-non-conﬂictness have been
presented altogether with maximum degree theorem. Above that some cases
where one can create lower bound degree of hidden conﬂict has been observed.
From the application point of view, the presented results can improve decision
making whether belief functions in question are really non-conﬂicting or whether
some level of conﬂict between them is hidden there.
Besides, there are several open issues for further research and development of
the topic like e.g., description of classes of non-conﬂicting BFs or investigation
of further practical usefulness of hidden conﬂicts and their degrees.
References
1. Almond, R.G.: Graphical belief modeling, 1st edn. CRC Press Inc, Boca Raton
(1995). https://www.routledge.com/Graphical-Belief-Modeling/Almond/p/book/
9780412066610
2. Cuzzolin, F.: On consistent approximations of belief functions in the mass space.
In: Liu, W. (ed.) ECSQARU 2011. LNCS (LNAI), vol. 6717, pp. 287–298. Springer,
Heidelberg (2011). https://doi.org/10.1007/978-3-642-22152-1 25
3. Daniel, M.: Conﬂicts within and between belief functions. In: H¨ullermeier, E.,
Kruse, R., Hoﬀmann, F. (eds.) IPMU 2010. LNCS (LNAI), vol. 6178, pp. 696–705.
Springer, Heidelberg (2010). https://doi.org/10.1007/978-3-642-14049-5 71
4. Daniel, M.: Non-conﬂicting and conﬂicting parts of belief functions. In: 7th Inter-
national Symposium on Imprecise Probability: Theories and Applications (ISIPTA
2011), pp. 149–158. SIPTA, Innsbruck (2011). https://www.sipta.org/isipta11/
proceedings/041.html
5. Daniel, M.: Conﬂict between belief functions: a new measure based on their non-
conﬂicting parts. In: Cuzzolin, F. (ed.) BELIEF 2014. LNCS (LNAI), vol. 8764, pp.
321–330. Springer, Cham (2014). https://doi.org/10.1007/978-3-319-11191-9 35
6. Daniel, M., Kratochv´ıl, V.: Hidden auto-conﬂict in the theory of belief functions.
In: Proceedings of the 20th Czech-Japan Seminar on Data Analysis and Decision
Making under Uncertainty (CJS 2017), pp. 34–45 (2017). http://hdl.handle.net/
11104/0276743

Comparison of Shades and Hiddenness of Conﬂict
327
7. Daniel, M., Kratochv´ıl, V.: Belief functions and degrees of non-conﬂictness. In:
Kern-Isberner, G., Ognjanovi´c, Z. (eds.) ECSQARU 2019. LNCS (LNAI), vol.
11726, pp. 125–136. Springer, Cham (2019). https://doi.org/10.1007/978-3-030-
29765-7 11
8. Daniel, M., Kratochv´ıl, V.: On hidden conﬂicts of belief functions. In: 11th Confer-
ence of the European Society for Fuzzy Logic and Technology (EUSFLAT 2019).
Atlantis Press (2019). https://doi.org/10.2991/eusﬂat-19.2019.70
9. Daniel, M., Kratochv´ıl, V.: Hidden conﬂicts of belief functions. Int. J. Comput.
Intell. Syst. 14, 438–452 (2020). https://doi.org/10.2991/ijcis.d.201008.001
10. Destercke, S., Dubois, D.: Idempotent conjunctive combination of belief functions:
extending the minimum rule of possibility theory. Inf. Sci. 181(18), 3925–3945
(2011). https://doi.org/10.1016/j.ins.2011.05.007
11. Destercke, S., Burger, T.: Toward an axiomatic deﬁnition of conﬂict between belief
functions. IEEE Trans. Cybern. 43(2), 585–596 (2013). https://doi.org/10.1109/
TSMCB.2012.2212703
12. Martin, A., Jousselme, A.L., Osswald, C.: Conﬂict measure for the discounting
operation on belief functions. In: 11th International Conference on Information
Fusion. IEEE (2008). https://ieeexplore.ieee.org/document/4632320
13. Pichon, F., Jousselme, A.L., Abdallah, N.B.: Several shades of conﬂict. Fuzzy Sets
Syst. 366, 63–84 (2019). https://doi.org/10.1016/j.fss.2019.01.014
14. Shafer, G.: A mathematical theory of evidence. Princeton University Press
Princeton (1976). https://press.princeton.edu/books/paperback/9780691100425/
a-mathematical-theory-of-evidence
15. Smets, P.: Decision making in the TBM: the necessity of the pignistic transfor-
mation. Int. J. Approximate Reasoning 38(2), 133–147 (2005). https://doi.org/10.
1016/j.ijar.2004.05.003
16. Smets, P., Kennes, R.: The transferable belief model. Artif. Intell. 66(2), 191–234
(1994). https://doi.org/10.1016/0004-3702(94)90026-4
17. Yager, R.R.: On considerations of credibility of evidence. Int. J. Approximate Rea-
soning 7, 45–72 (1992). https://doi.org/10.1016/0888-613X(92)90024-T

Games of Incomplete Information:
A Framework Based on Belief Functions
H´el`ene Fargier1, ´Erik Martin-Dorel2, and Pierre Pomeret-Coquot2(B)
1 CNRS, IRIT, Toulouse, France
helene.fargier@irit.fr
2 University Paul Sabatier, IRIT, University of Toulouse, Toulouse, France
{erik.martin-dorel,pierre.pomeret}@irit.fr
Abstract. This paper proposes a model for incomplete games where
the knowledge of the players is represented by a Dempster-Shafer belief
function. Beyond an extension of the classical deﬁnitions, it shows such
a game can be transformed into an equivalent hypergraphical complete
game (without uncertainty), thus generalizing Howson and Rosenthal’s
theorem to the framework of belief functions and to any number of play-
ers. The complexity of this transformation is ﬁnally studied and shown
to be polynomial in the degree of k-additivity of the mass function.
Keywords: Game theory · Incomplete games · Belief functions
1
Introduction
Game theory [21,25] proposes a powerful framework to capture decision prob-
lems involving several agents. In non-cooperative games of complete information,
the players do not coordinate their actions but each of them knows everything
about the game: the players, their available actions and all their utilities. This
assumption of complete knowledge cannot always be satisﬁed. In the real world
indeed, players are not so well informed and have only limited knowledge about
the game. This is why Bayesian games of incomplete information have been pro-
posed [16]. Nevertheless, the Bayesian hypothesis is strong, and requires a good
knowledge of the environment. For instance, in case of ignorance, the Bayesian
way is to suppose equiprobability, but this can lead to a model that does not ﬁt
with the agents’ behavior (e.g. see Ellsberg’s paradox [9]).
In the present paper, we propose a new kind of game of incomplete informa-
tion, which we call credal game. Agents have a partial knowledge, represented by
a Dempster-Shafer belief function [5,30], and cardinal utilities, but do not neces-
sarily make the equiprobability assumption. The underlying decision rule is the
Choquet integral based on the Bel measure [4], in order to capture the agents’
aversion for ambiguity [12,13,22]. We then follow the line deﬁned by Howson
and Rosenthal [17] who have shown that any 2-player Bayesian game can be
Pierre Pomeret-Coquot and Helene Fargier have beneﬁted from the AI Interdisciplinary
Institute ANITI. ANITI is funded by the French “Investing for the Future – PIA3”
program under the Grant agreement n◦ANR-19-PI3A-0004.
c
⃝Springer Nature Switzerland AG 2021
J. Vejnarov´a and N. Wilson (Eds.): ECSQARU 2021, LNAI 12897, pp. 328–341, 2021.
https://doi.org/10.1007/978-3-030-86772-0_24

Games of Incomplete Information: A Framework Based on Belief Functions
329
transformed into a complete knowledge polymatrix game [33]. In this paper, we
show that such a transformation is possible for credal games, and for any num-
ber of agents, producing a hypergraphical game [26]. An important consequence
of this result is that the algorithmics developed for hypergraphical games [3,32]
can be reused for the search of Nash equilibria in credal games.
2
Background and Motivations
To illustrate and motivate our work, we will use the following example inspired
by the murder of Mr. Jones [31], where the suspects are Peter, Paul and Mary.
Example 1 (Peter, Quentin and Rose).
Two agents, named Agent 1 and
Agent 2, are independently looking for a business association, with either Peter
(P), Quentin (Q), or Rose (R). The point is that a crime has been committed,
for which these three people are suspected. Several testimonies, not very reliable,
allowed to estimate that there is 50% of chance that the culprit is a man (P or
Q), and 50% of chance that it is a woman (R).
As to the interest of the associations, making the deal with an innocent leads
to a payoﬀof $6k (to be shared between the people making the deal), while
associating with a guilty person produces no payoﬀ($0k). Moreover, Agent 1
is investigating about P and will knows whether he is guilty or not. Similarly,
Agent 2 will knows whether R is guilty before making the decision.
The Bayesian approach is not relevant here. Indeed, if Agent 1 learns that
P is innocent, the probability of guilt should become 1/2 for Q and 1/2 for
R. However, in a purely Bayesian view, equiprobability would be applied and
the prior probability of guilt would be 1/4 for P and 1/4 for Q. Then, after
conditioning, Agent 1 would get a probability of 1/3 for Q and 2/3 for R.
2.1
Dempster-Shafer’s Theory of Evidence
Let us ﬁrst look at the epistemic aspect of the problem. The prior knowlege is
simply that P({P, Q}) = P({M}) = 1
2, and nothing more. The kind of knowl-
edge at work here is well captured in Dempster-Shafer’s theory of evidence, that
does not restrict probability assignments to elements of the frame of discernment:
Deﬁnition 1 (Mass function). A mass function for a frame of discernment
Ω (or “bpa” for basic probability assignment) is a function m : 2Ω →[0, 1] such
that m(∅) = 0 and 
A⊆Ω m(A) = 1.
A set with a nonzero mass is called a focal element and the set of focal
elements is denoted Sm. Two dual measures on 2Ω derive from m:
Bel(A) =

B∈Sm,B⊆A
m(B)
and
Pl(A) =

B∈Sm,B∩A̸=∅
m(B).
Bel(A) (resp. Pl(A)) estimates to what extent A is implied by (resp. is compat-
ible with) the knowledge captured by m.

330
H. Fargier et al.
Probabilities are belief functions where the focal elements are then singletons
– where m is “1-additive”. k-additivity is more generally deﬁned as follows:
Deﬁnition 2 (k-additivity). A mass function is k-additive if all its focal ele-
ments are at most of size k, i.e., ∀B ∈Sm, |B| ≤k.
Even if the agents share the same prior knowledge, as in our example, they
may acquire diﬀerent pieces of information and thus have a diﬀerent posterior
knowledge. In the example, Agent 1 eventually learns whether P is guilty or not
while Agent 2 will acquire information about R. Each agent thus revises his/her
knowledge on the basis on the information C she learns (e.g. R for Agent 2).
States where C is false are considered as impossible by the agent, so she modiﬁes
the initial belief function in such a way that Pl( ¯C) = 0: the conditioning at work
here is Dempster’s rule [5] (see also [1,7] for more details about the conditioning).
Deﬁnition 3 (Dempster conditioning). For any nonempty A, C ⊆Ω,
m|C(A) := KC ·

B∈Sm,C∩B=A
m(B),
where KC = 1/ 
B∈Sm,B∩C̸=∅m(B) is a normalization factor.
2.2
Decision Making with Belief Functions
Let us now consider belief functions in a (single-agent) decision making context.
Following Savage’s modelling of decision making under uncertainty [28], a deci-
sion (or “action”) is a function a : Ω →X where Ω is the set of possible states,
as previously, and X is the set of possible outcomes. The preferences of an agent
are represented by an utility function u : X →R. When the knowledge about Ω
is captured by a belief function, the discrete Choquet integral [4] based on the
Bel measure is classically advocated because of its ability to capture the agents’
aversion for ambiguity [12,13,22]:
Deﬁnition 4 (Discrete Choquet integral). Let Λ(a) = {λ1 ≤· · · ≤λ|Λ(a)|}
be the set of utility values reached by an action a, labelled by increasing order,
and Eλi(a) = {ω | u(a(ω)) ≥λi} denote the set of worlds for which the utility
of action a is at least λi. The discrete Choquet utility value (or CEU) of a is :
CEU(a) = λ1 +
|Λ(a)|

i=2
(λi −λi−1) × Bel(Eλi(a)).
The CEU has a simple expression based on the mass function:
CEU(a) =

B∈Sm
m(B) × min
ω∈B u(a(ω)).

Games of Incomplete Information: A Framework Based on Belief Functions
331
2.3
Game Theory
A simultaneous game of complete information models a situation where several
agents make a decision (the term “action” is rather used in game theory) without
coordination with the other agents – the ﬁnal utility of each agent depending on
the actions chosen by all agents.
Deﬁnition 5 (Complete game). A simultaneous game of complete informa-
tion (also called complete game) is a tuple G =

N, (Ai)i∈N, (ui)i∈N

where:
– N = {1, . . . , n} is a ﬁnite set of agents (or “players”),
– Ai is the set of actions of Agent i; A := 
i∈N Ai contains all the possible
combinations of actions or “proﬁles”,
– ui : A →R is the utility function of Agent i.
A mixed strategy for player i is a probability distribution on Ai. The strategy is
said to be pure when only one action receives a non-zero probability.
A pure (resp. mixed) joint strategy (or strategy proﬁle) is a vector p =
(p1, . . . , pn) which speciﬁes a pure (resp. mixed) strategy for each player.
A mixed strategy is classically interpreted as distributions the players use to
randomly choose among available actions in order to avoid being predictable –
this is especially useful in repeated games. An alternative view is to consider that
each pi represents the knowledge that the other agents have about i’s decision.
In the following, we will use the following notations: for any vector v =
(v1, . . . , vn) in some product domain V = 
i∈N Vi and for any e ⊆N, ve is the
restriction of v to e and Ve = 
i∈e Vi. By abuse of notation, we write vi for v{i}.
For any i, −i denotes the set N \ {i}, i.e. v−i = (v1, . . . , vi−1, vi+1, . . . , vn) ∈
V−i = 
j̸=i Vj. Thus, v−i is the restriction of v to all players but i. Finally, “.”
denotes the concatenation, e.g., v′
i.v−i = (v1, . . . , vi−1, v′
i, vi+1, . . . , vn). Hence
a = ai.a−i belongs to A and given two proﬁles a, a′ ∈A, a′
i.a−i denotes the
proﬁle a where ai is replaced with a′
i.
Because the strategies can be randomized, the global utility for a player of
a joint mixed strategy p is deﬁned as the expected utility of ui according to the
probability distribution it induces over A (Obviously, when the strategy is pure,
EUi is equal to the utility value given by ui):
Deﬁnition 6 (Utility of a strategy). Given a joint strategy p in a complete
game

N, (Ai)i∈N, (ui)i∈N

, the expected utility of player i is deﬁned by:
EUi(p) =

a∈A

i∈N
pi(ai)
	
× ui(a).
Among the proﬁles of interest, Nash equilibria are emphasized, i.e., proﬁles in
which no player can increase his/her utility by changing his/her own strategy.
Deﬁnition 7 (Nash equilibrium [25]). A strategy proﬁle p is a Nash equilib-
rium iﬀfor all i ∈N, there exists no p′
i such that ui(p′
i.p−i) > ui(p).
A pure strategy proﬁle p is a Nash equilibrium iﬀfor all i ∈N, there exists
no pure strategy p′
i such that ui(p′
i.p−i) > ui(p).

332
H. Fargier et al.
When the utility functions are described explicitly, G is said to be in stan-
dard normal form (SNF). SNF representations become spatially costly when the
number of players increases (O(nαn) for a game with n players and α actions
per player). More succinct forms have been proposed, that suit cases where util-
ity functions can be decomposed as a sum of smaller utility functions – namely
hypergraphical games [26] and polymatrix games [33].
Deﬁnition 8 (Hypergraphical game).
A hypergraphical game is a tuple
G =

N, E, (Ai)i∈N, (ue
i)e∈E,i∈e

where N is a set of players, E = {e1, . . . em}
is a multiset of subsets of N ((N, E) is an hypergraph) and for each e ∈E,

e, (Ai)i∈e, (ue
i)i∈e

is a classical standard normal form game. The global utility
of Agent i is the sum of i’s local utilities: ui(a) = 
e∈E ue
i(ae).
Polymatrix games are hypergraphical games with 2-player local games.
This framework assumes that each player knows everything about the game:
the players, the actions available to each player, all their utilities for each combi-
nation of actions, etc. The assumption of complete knowledge cannot always be
satisﬁed. In the real world indeed, players have only a limited knowledge about
the outcomes of their strategies – the ﬁnal outcomes may depend on an ill-known
event (in our example, the payoﬀfor making the deal with one of P, Q, or R
depends on whether they are guilty or innocent).
Harsanyi [16] proposed games of incomplete information as a way to capture
such situations (see also [24], for more details). A game of incomplete informa-
tion can be ﬁrst understood as a set of possible classical games (of complete
information) – one for each possible world ω ∈Ω. Players don’t know exactly
which world is the real one, but may have some knowledge about it. Just before
playing, each player i will receive some information τi(ω∗) about the real world
ω∗. τi maps any world to an element θi of a set Θi called the set of “types” of
Agent i. After having observed τi(ω∗), Agent i knows more about the real game,
but several games may still be plausible. The player then conditions his/her
knowledge on τi(ω∗) and decides which action to play. Notice that the diﬀerent
agents may receive diﬀerent pieces of information and thus have a diﬀerent pos-
terior knowledge. The question is then, for each player, to determine a strategy
(either an action, or a probabilistic strategy) for each of his/her possible types.
Harsanyi has shown that such games can be described on the space of types
Θ = Θ1×· · ·×Θn (the underlying worlds are omitted). The idea of Harsanyi when
deﬁning types is that this concept can encapsulate every piece of information
agents may have access to. It includes the agent-observable world status, but
also their beliefs on other agents and their introspective mental states.
3
Credal Games
Bayesian games are games of incomplete information where prior knowledge is
captured by a probability measure. To capture problems where the Bayesian
assumption is not obeyed (as in our motivating example), we propose in this
paper the framework of credal games:

Games of Incomplete Information: A Framework Based on Belief Functions
333
Deﬁnition 9 (Credal game). A credal game G is deﬁned as a tuple composed
of

N, (Ai)i∈N, (Θi)i∈N, (ui)i∈N, m

where:
– N = {1, . . . , n} is a ﬁnite set of players,
– Ai is the set of actions of player i; A = 
i∈N Ai denotes the set of all action
proﬁles,
– Θi is the set of types of player i; let Θ = 
i∈N θi,
– m : 2Θ →[0, 1] is the mass function describing the common prior knowledge,
– ui : A × Θ →R is the utility function of Agent i.
G is said to be in standard normal form iﬀthe utility functions ui and the
mass function m are given in extenso.
Following Harsyani’s approach of incomplete games, we consider the “ex
interim” setting where each player plans a strategy for each of the types he/she
can receive, ideally a strategy which is a best response to that of the other
players. We thus adopt the deﬁnition of strategy proposed by Harsyani’s in the
general context of incomplete games:
Deﬁnition 10 (Pure and mixed strategies [16]). A pure (resp. mixed) strat-
egy for player i in a credal game is a function ρi which maps each “type” θi ∈Θi
to an action of (resp. a probability over) Ai.
A pure (resp. mixed) joint strategy is a vector p = (p1, . . . , pn) which speciﬁes
a pure (resp. mixed) strategy for each player.
ρ(θ) =

ρ1(θ1), . . . , ρn(θn)

denotes the proﬁle which will be played if the
conﬁguration of types is θ.
Let us ﬁrst consider pure strategies. In the ex interim approach of incomplete
games, Agent i may consider the strategy ρ−i planned by the other players for
each of their types. When receiving her type θi, he/she revises his/her knowledge
– in a credal game, his/her posterior knowledge over the joint type conﬁguration
is m|θi. According to the deﬁnition of the Choquet expected utility, the utility
of a pure strategy proﬁle for Agent i of type θi, shall thus be deﬁned as:
Deﬁnition 11 (Choquet Expected Utility of a pure strategy proﬁle).
The utility of a pure strategy proﬁle ρ = (ρ1, . . . , ρn), for Agent i of type θi, is:
CEU(i,θi)(ρ) = 
B∈Sm|θi m|θi(B) × minθ′∈B ui(ρ(θ′), θ′).
Let us now consider mixed strategies. For each conﬁguration θ, ρ(θ) deﬁnes
a probability distribution over A: the probability that a is played when agents
have types θ is equal to 
i∈N ρi(θi)(ai). If we now consider all the θ’s, we get
a bpa mρ over A × Θ. To any focal element B ∈Sm and any pure strategy
proﬁle σ : Θ →A corresponds an element σ.B := {(σ(θ), θ) | θ ∈B} of mass
mρ(σ.B) = m(B) × 
θ∈B

i∈N ρi(θi)(σi(θi)) – σ.B is focal iﬀB is focal and σ
is possible according to ρ. Finally, Agent i recieving type θi conditions his/her
knowledge which becomes mρ
|θi. Hence the following deﬁnition of the utility of a
mixed strategy proﬁle:

334
H. Fargier et al.
Deﬁnition 12 (Choquet Expected Utility of a mixed strategy proﬁle).
The utility of a mixed strategy proﬁle ρ = (ρ1, . . . , ρn), for player i of type θi,
is:
CEU(i,θi)(ρ) =

σ.B∈Smρ
|θi
⎛
⎝m|θi(B) ×

θ′∈B

j∈N
ρj(θ′
j)(σj(θ′
j))
⎞
⎠× min
θ′∈B ui(σ(θ′), θ′)
Obviously, Deﬁnition 12 leads to Deﬁnition 11 when ρ is a pure strategy pro-
ﬁle. Now, recall that a strategy proﬁle is a Nash equilibrium if no player can
improve unilaterally his/her utility. This concept straightforwardly extends to
credal games:
Deﬁnition 13 (Nash equilibrium). A mixed (resp. pure) strategy proﬁle ρ is
a Nash equilibrium for CEU iﬀ, whatever i, there exists no mixed (resp. pure)
strategy ρ′
i such that for any θi, CEU(i,θi)(ρ′
i.ρ−i) > CEU(i,θi)(ρ).
Example. Our running example (see Example 1) is captured by the credal game
G =

N, (Ai)i∈N(Θi)i∈N, (ui)i∈N, m

where:
– N = {1, 2};
– A1 = {P1, Q1, R1}, A2 = {P2, Q2, R2} (each agent chooses an associate).
– Θ1 = {P, ¯P}, Θ2 = {R, ¯R} (Agent 1 investigates on Peter, Agent 2 investigates
on Rose).
– m : 2Θ →[0, 1] has two focal elements: m

{(¯P, R)}

= 1/2 (the murderer is a
woman, thus necessarily Rose – in this case Agent 1 will learn ¯P and Agent 2
will learn R) and m

{(P, ¯R), (¯P, ¯R)}

= 1/2 (the murderer is a man: Agent 2
necessarily learns ¯R but Agent 1 can learn either ¯P – which happens when
Quentin is the murderer – or P – Peter is the murderer).
– Making a deal with a murderer has a utility value of 0, making a deal with
an innocent leads to a utility of 6
2 = 3, unless the other agent approaches the
same associate, in which case each agent receives 6
3 = 2. The utility functions
are summarized below (Table 1). Null values (in gray) are given for the case
where θ = (P, R) (both R and P are guilty) which is not a possible world.
Let ρ be the pure strategy where Agent 1 makes the deal with R when learning
that P is guilty and with P otherwise, and Agent 2 joins Q when learning that R
is guilty and R otherwise: ρ1(P) = R1, ρ1(¯P) = P1, ρ2(R) = Q2 and ρ2(¯R) = R2.
– Consider Agent 1 receiving type P: the conditioned bpa, m|P, has only one
focal element {(P, ¯R)}, KP = 1/ 1
2 and m|P({(P, ¯R)}) = 1. In short, Agent 1
knows that P is guilty and R is not. In the only possible conﬁguration, (P, ¯R),
ρ prescribes ρ1(P) = R1 for Agent 1 and ρ2(¯R) = R2 for Agent 2. Then
CEU(1,P)(ρ) = m|P

{(P, ¯R)}

× u1

(R1, R2), (P, ¯R)

= 1 × 2 = 2.
– Consider now Agent 1 receiving ¯P: his/her revised knowledge, m|¯P, has two
focal elements, {(¯P, R)} and {(¯P.¯R)} (each with probability 1
2, thus K¯P = 1).
The strategy prescribes ρ(¯P) = P1 for Agent 1, who doesn’t know whether
Agent 2 learns R (and plays ρ(R) = Q2) or ¯R (and plays ρ(¯R) = R2). Hence
CEU(1,¯P)(ρ) = 1
2 × u1

(P1, R2), (¯P, ¯R)

+ 1
2 × u1

(P1, Q2), (¯P, R)

= 3.

Games of Incomplete Information: A Framework Based on Belief Functions
335
Table 1. Example 1: Utility matrices for each conﬁguration of the types
θ2 = ¯R
θ2 = R
P2
Q2
R2
P2
Q2
R2
θ1 = P
P1
(0, 0) (0, 3) (0, 3)
P1
(0, 0) (0, 0) (0, 0)
Q1 (3, 0) (2, 2) (3, 3)
Q1 (0, 0) (0, 0) (0, 0)
R1
(3, 0) (3, 3) (2, 2)
R1 (0, 0) (0, 0) (0, 0)
P2
Q2
R2
P2
Q2
R2
θ1 = ¯P
P1
(2, 2) (3, 0) (3, 3)
P1
(2, 2) (3, 3) (3, 0)
Q1 (0, 3) (0, 0) (0, 3)
Q1 (3, 3) (2, 2) (3, 0)
R1
(3, 3) (3, 0) (2, 2)
R1 (0, 3) (0, 3) (0, 0)
– Similarly, the bpa of Agent 2 receiving R, m|R, has only one focal element,
{(¯P, R)} (thus KR = 1/ 1
2) in which ρ prescribes P1 for Agent 1 and Q2 for
Agent 2. Then CEU(2,R)(ρ) = 1 × u2

(P1, Q2), (¯P, R)

= 1 × 3 = 3.
– Finally, the bpa of Agent 2 receiving ¯R, m|¯R, has one focal element,
{(¯P.¯R), (P.¯R)} and K¯R = 1/ 1
2. Agent 2 does not know whether Agent 1 receives
¯P or P. Since ρ prescribes Agent 1 to play P1 in the ﬁrst case, R1 in the sec-
ond one, and prescribes Agent 2 to play R2 in both cases, CEU(2,¯R)(ρ) =
1 × min

u2

(P1, R2), (¯P, ¯R)

, u2

(R1, R2), (P, ¯R)

= 1 × min(3, 2) = 2.
In this strategy, Agent 1 does not give the best possible response to Agent 2’s
strategy: when learning that P is guilty, he/she plays R1 while knowing that in
this case Agent 2 learns ¯R and thus plays R2. Let Agent 1 modify his/her strategy
and play Q1 when learning P – hence the strategy ρ′:
ρ′
1(P) = Q1, ρ′
1(¯P) = P1, ρ′
2(R) = Q2, ρ′
2(¯R) = R2
– CEU(1,P)(ρ′) = KP × u1

(Q1, R1), (P, ¯R)

= 1 × 3 = 3,
– CEU(1,¯P)(ρ′) = K¯P × u1

(P1, R2), (¯P, ¯R)

+ K¯P × u1

(P1, Q2), (¯P, R)

= 3,
– CEU(2,R)(ρ′) = KR × u2

(P1, R2), (¯P, R)

= 1 × 3 = 3,
– CEU(2,¯R)(ρ′) = K¯R × min

u2

(P1, R2), (¯P, ¯R)

, u2

(Q1, R2), (P, ¯R)

= 3.
It can be checked in ρ′ each player has his/her maximal possible utility ($3k)
– no player has an incentive to change: ρ′ is a pure Nash equilibrium.
4
From Credal Games to Complete Games
One of the most prominent results about Bayesian games is Howson’s and Rosen-
thal’s theorem [17]: any 2-player Bayesian game can be transformed into a (com-
plete information) polymatrix game equivalent to the original one. This result
is important from the computational point of view since it provides 2-player
Bayesian games with practical resolution tools: to solve a 2-player Bayesian
games, it is enough to use this theorem and to solve the resulting polymatrix
game by one of algorithms proposed for such games [3,32]. In the sequel, we
generalize this theorem to credal games and extend it to any number of players.

336
H. Fargier et al.
4.1
The Direct Transform
A ﬁrst idea is to deﬁne from a credal game G, an hypergraphical game ˜G, the
vertices (players) of which are pairs (i, θi) with action set Ai – to each pure
strategy ρ of G corresponds a unique pure strategy ˜ρ of ˜G and conversely – we
call ˜ρ the Selten transform of ρ:1,2
Deﬁnition 14 (Selten transform of a pure strategy). For any pure strategy
ρ of G, the Selten transform of ρ is the vector ˜ρ deﬁned by ˜ρ(i,θi) = ρi(θi).
The local games correspond to the focal elements of m. Roughly, (i, θi) plays
in the local game corresponding to the focal element B if the type θi is plausible
for B – technically, if there exists θ′ ∈B such that θ′
i = θi. In this local game,
(i, θi) obtains a local utility K|θi · m(B) · minθ′∈B,θ′
i=θi ui(ρ(θ′), θ′).
Given a proﬁle of actions ˜ρ, and a player (i, θi), the hypergraphical game
sums these local utilities over all the focal elements for which θi is plausible.
Hence the global utility for (i, θi) is equal to the CEU of the joint ρ.
One may note that two pairs (i, θi) and (i, θ′
i) may play in the same local
game – this happens when θ and θ′ belong to the same focal set. In this case, the
utility of (i, θ′
i) does not depend on the action played by (i, θi) and conversely.
For any focal element B of m, let Players(B) := {(i, θi) | θ ∈B, i ∈N} –
Players(B) denotes the future players involved in the local game corresponding
to B. Let ˜E be the multiset ˜E := [Players(B) | B ∈Sm]. The elements e of
˜E and the focal elements in Sm are in bijection and we denote Be the focal
element of m which leads to e. These notations allow us to propose a ﬁrst, direct
generalization to credal games of the Howson’s and Rosenthal’s transform:
Deﬁnition 15 (Direct transform of a credal game). The direct transform
of a credal game G =

N, (Ai, Θi, ui)i∈N, m

is the hypergraphical game ˜G =
 ˜N, ˜E, ( ˜A(i,θi))(i,θi)∈˜
N, (˜ue
(i,θi))e∈˜
E,(i,θi)∈e

where:
– ˜N = {(i, θi) | i ∈N, θi ∈Θi},
– For each i ∈N, θi ∈Θi, ˜A(i,θi) = Ai,
˜A = 
i∈N,θi∈Θi ˜A(i,θi) denotes the set of all the pure strategy proﬁles in ˜G.
– ˜E = [Players(B) | B ∈Sm],
– For each e ∈˜E, (i, θi) ∈e and ˜ρ ∈˜A,
˜ue
(i,θi)(˜ρe) = K|θi · m(Be) · minθ′∈Be,θ′
i=θi ui

ρ(θ′), θ′
.
It is easy to show that the CEU value of a pure strategy ρ in G and the
global utility of ˜ρ in ˜G are equal, whatever is the couple (i, θi) considered.
Proposition 1. 3 Let G be a credal game and ˜G its direct transform. For any
pure strategy ρ of G, it holds that CEU(i,θi)(ρ) = ˜u(i,θ)(˜ρ).
1 Named after Selten, who proposed a similar deﬁnition for Bayesian games [16].
2 We could use the notation ρ for both, but the pure strategy proﬁles of the credal
game are vectors of functions ρi : Θi →Ai while the pure strategy proﬁles of ˜G are
vectors in 
i∈N,θi∈Θi Ai. So, we keep the two notations ˜ρ and ρ.
3 The proofs are omitted for the sake of brevity and can be found at [11].

Games of Incomplete Information: A Framework Based on Belief Functions
337
Let us extend the Selten transform to mixed strategy proﬁles ρ of G: each
˜ρ(i,θi) = ρi(θi) is then a probability distribution over Ai, and ˜ρ is then a vector
of such distributions.
Corollary 1. Let G be a credal game and ˜G its direct transform. For any mixed
strategy proﬁle ρ of G, it holds that CEU(i,θi)(ρ) = ˜u(i,θ)(˜ρ).
When m is a probability distribution and G is a 2-player game, we get at most
|Θ| local games, each involving two players (i, θi) and (j, θj): ˜G is a polymatrix
game, and Howson and Rosenthal’s Theorem is recovered. More generally, we
get:
Theorem 1 (Generalized Howson-Rosenthal Theorem). For any credal
game G, there exists an hypergraphical game ˜G such that ρ is a pure (resp. mixed)
Nash equilibrium of G iﬀ˜ρ is a pure (resp. mixed) Nash equilibrium of ˜G.
Example 2. Let us deﬁne the direct transform of the credal game G corre-
sponding to our running example. The set of players is: ˜N = {(1, P), (1, ¯P),
(2, R), (2, ¯R)}. The set of actions are ˜A(i,θi) = {Pi, Qi, Ri}.
Because m has two focal elements B1 = {(¯P, R)} and B2 = {(P, ¯R), (¯P, ¯R)} each
with probability 1
2, ˜G involves two local games. The set of players involved are
respectively e1 = {(1, ¯P), (2, R)} and e2 = {(1, ¯P), (1, P), (2, ¯R)}. ˜G’s hypergraph
is drawn on Fig. 1.
Fig. 1. G’s direct transform. Gray circles denote vertices (players; one shade per agent),
white boxes denote hyperedges (local games; linked to the players involved).
Player (2, ¯R) plays only in e2, we have for instance:
˜ue2
(2,¯R)(R1, P1, R2) = K¯R · m(B2) × min

u2

(R1, R2), (P, ¯R)
, u2

(P1, R2), (¯P, ¯R)
= 2.
For player (1, ¯P), which plays in both local games, we have for instance:
˜ue1
(1,¯P)(P1, P2) = K¯P · m(B1) × u1

(P1, P2), (¯P, R)

= 0.5 × 2 = 1
˜ue2
(1,¯P)( , P1, Q2) = K¯P · m(B2) × u1

(P1, Q2), (¯P, ¯R)

= 0.5 × 3 = 1.5.
The Selten transform of the Nash equilibrium ρ′ described in the previous section
is ˜ρ′((1, ¯P)) = P1, ˜ρ′((1, P)) = Q1, ˜ρ′((2, ¯R)) = R1, ˜ρ′((2, R)) = Q2. It is easy to
check that: ˜u(1,¯P)(˜ρ′) = ˜ue1
(1,¯P)

(P1, Q2)

+ ˜ue2
(1,¯P)

(Q1, P1, R2)

= CEU(1,¯P)(ρ′).
Notice that in the sum, one part of the utility of (1, ¯P) comes from the local
game e1 (i.e., from B1) and the other part comes from e2 (i.e., from B2).

338
H. Fargier et al.
As to the complexity of the transform, let α (resp. β) be the maximum
number of actions (resp. types) per player in G and k the degree of additivity
of m. G contains n utility tables of size (αβ)n and the size of the description of
m is bounded by k · n · |Sm|. So, Size(G) is in O

n(αβ)n + kn · |Sm|

.
˜G contains |Sm| local games. Each of them involves at most kn players (i, θi) –
the size of their SNF representation is thus at most knαkn – hence a spatial cost
for the representation of ˜G in O(|Sm| · knαkn). Notice now that since m is k-
additive, |Sm| < βkn. So, Size( ˜G) is bounded by kn(αβ)kn. In short, we get:
Proposition 2. The direct transform of a credal game G has a temporal com-
plexity in O

|Sm| · kn2αknβ

, also bounded by k2 · Size(G)k+1 and a spatial
complexity in O

|Sm| · knαkn
, also bounded by k · Size(G)k.
So, the degree of additivity of the bpa is the main factor of complexity.
Hopefully, low degrees of additivity can be assumed – it has indeed been shown
[15,19] that such low values (typically, k ≤3) allow the description of many cases
of interest. In such situations, the transform is quadratic or, at worst, cubic.
4.2
The Conditioned Transform
Now, when for each focal element B and each θi, only a few types are compat-
ible for the other players, one shall use a more sophisticated transform. In the
following, we propose to condition each focal element B for which θi is plausible
– we thus get a subset of B for which a local game is created. This transform
leads to smaller local games as soon as this subset involves less players than B.
Formally, for any B ∈Sm, i ∈N and θi ∈Θi, let B|θi = {θ′ ∈B | θ′
i = θi}.
We thus deﬁne the conditioned transform of G as follows:
Deﬁnition 16 (The conditioned transform of a credal game). The condi-
tioned transform of a credal game

N, (Ai)i∈N, (Θi)i∈N, (ui)i∈N, m

is the hyper-
graphical game ˜G =
 ˜N, ˜E, ( ˜A(i,θi))(i,θi)∈˜
N, (˜ue
(i,θi))e∈˜
E,(i,θi)∈e

where:
– ˜N = {(i, θi) | i ∈N, θi ∈Θi}.
– For each i ∈N, θi ∈Θi, ˜A(i,θi) = Ai.
˜A = 
i∈N,θi∈Θi ˜A(i,θi) denotes the set of all the pure strategy proﬁles in ˜G.
– ˜E = [Players(B|θi) | B ∈Sm, i ∈N, θi ∈Θi].
– For each e ∈˜E, (i, θi) ∈e, ˜ρe ∈˜Ae, ˜ue
(i,θi)(˜ρe) = m|θi(Be) · min
θ′∈Be ui(ρe(θ), θ).
The hypergraph of the conditioned transform our running example is drawn
on Fig. 2. And ﬁnally, we can show that:
Proposition 3. Let G be a credal game and ˜G its conditioned transform. For
any pure or mixed strategy ρ of G, it holds that (i) CEU(i,θi)(ρ) = ˜u(i,θ)(˜ρ) and
(ii) ρ is a Nash equilibrium of G iﬀ˜ρ is a Nash equilibrium of ˜G.

Games of Incomplete Information: A Framework Based on Belief Functions
339
Fig. 2. G’s conditioned transform. Gray circles are vertices (players; one color per
agent), white boxes are hyperedges (local games; linked to the involved players).
Proposition 4. The conditioned transform of a credal game G has a temporal
complexity in O

|Sm| · kn2αknβ

which is bounded by O

k2 · Size(G)k+1
and a
spatial complexity in O

|  Sm|θi| · knαkn
which is bounded by O

k · Size(G)k
.
So, the conditioned transform has the same worst case spatial complexity
than the direct one. In practice, the size of the transform depends on the struc-
ture of the mass function. Typically, if a focal element B involves only one θi for
Agent i, both transforms contain the same local game Players(B) (as B = B|θi),
but the conditioned one may produce many more local games. If on the contrary
many types are compatible with a focal B for any agent, the local game produced
by the direct transform is needlessly large. For instance, from a 2-player credal
game where both agents have types a, b, . . . and considering the focal element
{(a, a), (b, b), . . . }, the conditioned transform produces several 2-player games
while the direct one has one single local game linked to all vertices (i, θi).
5
Conclusion
This article provides two main contributions. On the one hand, we deﬁne a
model for games of incomplete information based of belief functions. On the
other hand, we introduce two transformations that make it possible to build an
hypergraphical game (of complete information) equivalent to the initial credal
game, generalizing Howson–Rosenthal’s theorem. As a result, the algorithmic
tools developed for hypergraphical games [3,32] can be used to solve credal
games.
This work opens several research directions. First, we shall let the model use
other decision rules, e.g. Jaﬀray’s [6,18], which generalize Hurwicz’s approach to
belief functions, or Gilboa and Schmeider’s multiple prior expected utility [14,
29]4. Beyond belief functions, we aim at extending the model to other Choquet
capacities and encompass other decision principles, based e.g. on rank-dependent
utility [27], probability intervals [2] or on some neighborhood models such as the
PPM model [20]. Finally, we shall also study how belief function based extensions
4 Notice that in the latter approach, the belief function is understood as the lower
bound of an imprecise probability – under this interpretation, the conditioning at
work must rather be Fagin-Halpern’s [10].

340
H. Fargier et al.
of mixed strategies [8,23] extend to credal games. In this extended framework,
the power of representation belief functions will be used not only to capture the
uncertainty about the game, but also as a way to describe the agents’ knowledge
about the others’ strategies. Finally, we like to formalize those results with the
Coq proof assistant in order to build, with other in-progress results, a modular
formal library on incomplete games and decision theory.
References
1. Augustin, T., Schollmeyer, G.: Comment: on focusing, soft and strong revision of
choquet capacities and their role in statistics. Stat. Sci. 36(2), 205–209 (2021)
2. Campos, L.M.D., Huete, J.F., Moral, S.: Probability intervals: a tool for uncertain
reasoning. Int. J. Uncertain. Fuzziness Knowl. Based Syst. 2(2), 167–196 (1994)
3. Chapman, A., Farinelli, A., de Cote, E.M., Rogers, A., Jennings, N.: A distributed
algorithm for optimising over pure strategy nash equilibria. In: Proceedings AAAI
(2010)
4. Choquet, G.: Theory of capacities. Ann. de l’institut Four. 5, 131–295 (1954)
5. Dempster, A.P.: Upper and lower probabilities induced by a multivalued mapping.
Ann. Math. Stat. 38, 325–339 (1967)
6. Denoeux, T., Shenoy, P.P.: An interval-valued utility theory for decision making
with dempster-shafer belief functions. Int. J. Approx. Reason. 124, 194–216 (2020)
7. Dubois, D., Denoeux, T.: Conditioning in dempster-shafer theory: prediction vs.
revision. In: Proceedings BELIEF, vol. 164, pp. 385–392. Springer, Heidelberg
(2012). https://doi.org/10.1007/978-3-642-29461-7 45
8. Eichberger, J., Kelsey, D.: Non-Additive Beliefs and Game Theory. Tilburg Uni-
versity, Center for Economic Research, Technical report (1994)
9. Ellsberg, D.: Risk, ambiguity, and the savage axioms. Q. J. Econ. 75, 643–669
(1961)
10. Fagin, R., Halpern, J.Y.: A new approach to updating beliefs. In: Proceedings UAI,
pp. 347–374 (1990)
11. Fargier, H., Martin-Dorel, E., Pomeret-Coquot, P.: Games of Incomplete Informa-
tion: a Framework Based on Belief Functions – version with proofs. https://www.
irit.fr/∼Erik.Martin-Dorel/ecrits/2021 credal-games ecsqaru extended.pdf
or in
HAL
12. Ghirardato, P., Le Breton, M.: Choquet rationality. J. Econ. Theory 90(2), 277–
285 (2000)
13. Gilboa, I.: Expected utility with purely subjective non-additive probabilities. J.
Math. Econ. 16(1), 65–88 (1987)
14. Gilboa, I., Schmeidler, D.: Maxmin expected utility with non-unique prior. J. Math.
Econ. 18(2), 141–153 (1989)
15. Grabisch, M.: Upper approximation of non-additive measures by k-additive
measures-the case of belief functions. In: ISIPTA, pp. 158–164 (1999)
16. Harsanyi, J.C.: Games with incomplete information played by “bayesian” players,
I-III part I. The basic model. Manag. Sci. 14(3), 159–182 (1967)
17. Howson, J.T., Jr., Rosenthal, R.W.: Bayesian equilibria of ﬁnite two-person games
with incomplete information. Manag. Sci. 21(3), 313–315 (1974)
18. Jaﬀray, J.Y.: Linear utility theory for belief functions. Oper. Res. Lett. 8(2), 107–
112 (1989)

Games of Incomplete Information: A Framework Based on Belief Functions
341
19. Miranda, P., Grabisch, M., Gil, P.: Dominance of capacities by k-additive belief
functions. Eur. J. Oper. Res. 175(2), 912–930 (2006)
20. Montes, I., Miranda, E., Destercke, S.: Unifying neighbourhood and distortion
models: Part II - new models and synthesis. Int. J. Gen. Syst. 49(6), 636–674
(2020)
21. Morgenstern, O., Von Neumann, J.: Theory of Games and Economic Behavior.
Princeton University Press (1953)
22. Mukerji, S.: Understanding the nonadditive probability decision model. Econ. The-
ory 9(1), 23–46 (1997)
23. Mukerji, S., Shin, H.S.: Equilibrium departures from common knowledge in games
with non-additive expected utility. Adv. Theor. Econ. 2(1), 1011 (2002)
24. Myerson, R.B.: Game Theory. Harvard University Press, Cambridge (2013)
25. Nash, J.: Non-cooperative games. Ann. Math., 286–295 (1951)
26. Papadimitriou, C.H., Roughgarden, T.: Computing correlated equilibria in multi-
player games. J. ACM 55(3), 1–29 (2008)
27. Quiggin, J.: A Theory of Anticipated Utility. J. Econ. Behav. Organ. 3(4), 323–343
(1982)
28. Savage, L.J.: The Foundations of Statistics. Wiley, Hoboken (1954)
29. Schmeidler, D.: Integral representation without additivity. Proc. Am. Math. Soc.
97(2), 255–261 (1986)
30. Shafer, G.: A Mathematical Theory of Evidence. Princeton University Press,
Princeton (1976)
31. Smets, P., Kennes, R.: The transferable belief model. Artif. Intell. 66(2), 191–234
(1994)
32. Wahbi, M., Brown, K.N.: A distributed asynchronous solver for nash equilibria in
hypergraphical games. Front. Artif. Intell. Appl. 285, 1291–1299 (2016)
33. Yanovskaya, E.: Equilibrium points in polymatrix games. Lithuanian Math. J. 8,
381–384 (1968)

Uncertainty-Aware Resampling Method
for Imbalanced Classiﬁcation Using
Evidence Theory
Fares Grina1,2(B), Zied Elouedi1, and Eric Lef`evre2
1 Institut Sup´erieur de Gestion de Tunis, LARODEC,
Universit´e de Tunis, Tunis, Tunisia
fares.grina@isg.u-tunis.tn, zied.elouedi@gmx.fr
2 Univ. Artois, UR 3926, Laboratoire de G´enie Informatique et d’Automatique de
l’Artois (LGI2A), 62400 B´ethune, France
eric.lefevre@univ-artois.fr
Abstract. Class imbalance is a common issue in many real world classi-
ﬁcation problems. It refers to situations where the number of observations
in the training dataset signiﬁcantly diﬀers for each class. Ignoring this
issue will make it more challenging for classiﬁers to properly learn data
characteristics, which results in poor performance. Many strategies have
been proposed to deal with this issue. The most common one is tackling
the imbalance at the preprocessing level, by re-sampling the training set.
However, imbalanced classiﬁcation can be aﬀected by other data factors,
such as uncertainty, i.e., ambiguous samples and noise. In this paper, we
propose an uncertainty-aware hybrid re-sampling technique based on the
theory of evidence to tackle imbalanced binary datasets in the presence
of aleatoric uncertainty. A soft evidential structure is assigned to each
object in the training set, which is later used to clean the dataset out
of overlapping and noisy majority samples, and then selectively generate
synthetic minority objects using a modiﬁed SMOTE algorithm. Experi-
mental results on benchmark imbalanced datasets have shown signiﬁcant
improvement over popular re-sampling techniques.
Keywords: Resampling · Imbalanced datasets · Evidence theory ·
Data uncertainty
1
Introduction
Imbalanced classiﬁcation is an active research topic in machine learning and
data mining. It is a scenario in which class sizes are not equal making the class
distribution imbalanced. Data imbalance exist in many real-life domains such
as fraudulent credit card detection [25], medical diagnosis [5], drug discovery
[18], etc. For instance, in imbalanced binary datasets, the class with the high-
est number of instances is referred to as the majority class, while the minority
class is deﬁned as the one with the fewest examples. In most cases, the minority
c
⃝Springer Nature Switzerland AG 2021
J. Vejnarov´a and N. Wilson (Eds.): ECSQARU 2021, LNAI 12897, pp. 342–353, 2021.
https://doi.org/10.1007/978-3-030-86772-0_25

Evidential Undersampling for Imbalanced Datasets
343
class is more relevant than the majority one [7]. As an example, failing to detect
intrusions in a company’s network may result in huge ﬁnancial losses. A variety
of variables may cause the class imbalance, such as the domain’s nature (e.g.
rare disease) or data collection factors (e.g. storage). Additionally, most clas-
siﬁer algorithms (such as decision trees, k-nearest neighbors, neural networks,
etc.) were designed with the presumption that training datasets have an even
distribution, which reduces greatly their eﬃciency [13].
Many methods have been proposed over the years to cope with imbalanced
datasets. Data resampling is one of the most eﬃcient strategies for dealing with
class imbalance [10]. This approach aims at ﬁxing the uneven class distribution
at the preprocessing level by re-balancing the training dataset. Being algorithm-
independent, resampling is versatile and could be applied with any selected clas-
siﬁer.
In addition, recent ﬁndings show that class imbalance is not an issue in and of
itself, but rather gets ampliﬁed by other data diﬃculties. Data uncertainty (can
also be referred to as aleatoric uncertainty) refers to the imperfections present
in the data. This type of uncertainty can include class overlapping and noise,
which were proven to worsen the class imbalance issue [13].
To improve performance on imbalanced and uncertain binary datasets, we
suggest an Uncertainty-Aware Hybrid reSampling (UAHS) method based on Evi-
dence Theory, which was recently used for imbalanced classiﬁcation [11,12]. After
creating soft evidential labels for each object, our method eﬃciently selects the
majority instances to remove in an undersampling phase ﬁrst, and the minority
objects to focus on in the oversampling procedure lastly. The considered eviden-
tial label is appropriate for our goal, since it includes membership values towards
single classes, in addition to a belief mass assigned to meta-classes (ambigu-
ous region). This versatility allows us to create precise rules for the process
of selecting undesirable samples in the undersampling phase, and intelligently
select minority instances to generate new synthetic objects. It is important to
note that our proposal is a hybrid resampling method, meaning that it performs
both undersampling and oversampling, unlike [11] and [12] which are respectively
pure oversampling and undersampling approaches.
The remainder of this paper will be divided as follows. First, related work for
resampling methods is presented in Sect. 2. Evidence Theory is recalled in Sect. 3.
Section 4 details each step of our idea. Experimental evaluation is discussed in
Sect. 5. Our paper ends with a conclusion and an outlook on future work in
Sect. 6.
2
Related Work
Data resampling is one of the most common approaches for dealing with imbal-
anced classiﬁcation [13]. In fact, data resampling deals with class imbalance at
the preprocessing level by changing the class distribution of the training set. As
a result, it alleviates the eﬀects of distribution skewness of the learning process.
These methods can be further categorized into three groups, namely:

344
F. Grina et al.
– Oversampling: These techniques introduce new minority synthetic samples
to re-balance the dataset. The most straightforward method is random over-
sampling (ROS), which consists of selecting minority observations in the orig-
inal data set and simply replicating them. Although it appears to be tech-
nically eﬀective since the class balance is adjusted, it can lead to overﬁtting
[16]. To cope with overﬁtting, the Synthetic Minority Oversampling Technique
(SMOTE) was suggested in [6]. Unlike ROS, SMOTE generates new synthetic
samples by interpolating among several minority objects that are close to
each other. However, many studies [9,30] have shown SMOTE’s drawbacks
which involve potential ampliﬁcation of noise, overlap already present in the
data. SMOTE’s improvement include Borderline-SMOTE [14], which identi-
ﬁes borderline minority class examples to generate new samples. Clustering-
based oversampling techniques were also proposed [9,23] to smartly select the
regions where to generate new points.
– Undersampling: These approaches create a subset of the original dataset
by removing some majority class instances. Like random oversampling, the
naive undersampling technique is to randomly remove majority objects, which
may potentially remove meaningful information from the dataset. Therefore,
other techniques have been suggested to smartly remove unwanted majority
class instances. Commonly, traditional ﬁltering techniques have been used to
perform undersampling. For example, Neighborhood Cleaning Rule (NCL)
discards majority class instances using the Edited Nearest Neighbors (ENN)
introduced in [37]. Similarly, Tomek Links (TL) [15] is occasionally used as
an undersampling method. Clustering has also been used for undersampling
in a number of occasions [26,34], to optimize the selection process of majority
instances to eliminate.
– Hybrid: This strategy combines both oversampling and undersampling in
order to re-balance the dataset. Typically, SMOTE is paired with an under-
sampling procedure to ﬁx its drawbacks. For instance, SMOTE-ENN and
SMOTE-TL were suggested in [3] to combine SMOTE with ENN and TL
respectively. SMOTE-RSB* [29] is a method which combines SMOTE for
oversampling with the Rough Set Theory [27] as a cleaning technique.
In SMOTE-IPF [17], SMOTE is ﬁrstly executed, and then the Iterative-
Partitioning Filter (IPF) [17] is performed to remove noisy original examples,
and those introduced by SMOTE. Authors in [20] suggested a combination
of a SMOTE-like algorithm with a cleaning procedure to reduce the eﬀects
of overlapping. Similarly, the class overlap issue is touched upon in [35] com-
bining a soft clustering method with Borderline-SMOTE.
3
Theory of Evidence
The theory of evidence [8,31,33], also referred to as Dempster-Shafer theory
(DST) or belief function theory, is a ﬂexible and well-founded framework for the
representation and combination of uncertain knowledge. The frame of discern-
ment deﬁnes a ﬁnite set of M exclusive possible events, e.g., possible labels for

Evidential Undersampling for Imbalanced Datasets
345
an object to classify, and is denoted as follows:
Ω = {ω1, ω2, ..., ωM}
(1)
A basic belief assignment (bba) denotes the amount of belief stated by a
source of evidence, committed to 2Ω, i.e., all subsets of the frame including
the whole frame itself. Precisely, a bba is represented by a mapping function
m : 2Ω →[0, 1] such that:

A∈2Ω
m(A) = 1
(2)
m(∅) = 0
(3)
Each mass m(A) quantiﬁes the amount of belief allocated to an event A of Ω.
A bba is unnormalized when m(∅) > 0, and must be normalized under a closed-
world assumption [32]. A focal element is a subset A ⊆Ω where m(A) ̸= 0.
The Plausibility function is another representation of knowledge deﬁned by
Shafer [31] as follows:
Pl(A) =

B∩A̸=∅
m(B),
∀A ∈2Ω
(4)
Pl(A) represents the total possible support for A and its subsets.
4
Uncertainty-Aware Hybrid Re-Sampling Method
(UAHS)
To tackle binary imbalanced datasets, we propose an Uncertainty-Aware Hybrid
re-sampling method (UAHS). Observations are ﬁrstly assigned soft evidential
labels (bbas) using the credal classiﬁcation rule (CCR) introduced in [22].
CCR uses the centers of each class and meta-class as pieces of evidence for
each example’s membership, instead of using nearest neighbors as it has been
employed in [11]. Unlike [11], our case deals with the soft labeling of both major-
ity and minority classes. Thus, an evidential nearest neighbor-based approach
might produce biased memberships towards the majority class, since the latter
usually have a much higher density than the minority one.
The computed bba is later used for cleaning unwanted majority objects and
selectively generating synthetic minority instances.
Each step is detailed in the following subsections.
4.1
Creating Soft Labels
UAHS proceeds by determining the centers of each class and meta-class (the over-
lapping region), then creating a bba based on the distance between the majority
sample and each class center.
The class centers are simply computed by the mean value of the training set
in the corresponding class. For the meta-class U, representing the overlapping

346
F. Grina et al.
region, the center is deﬁned by the barycenter of the involved class centers as
follows:
CU = 1
|U|

ωi∈U
Ci
(5)
where ωi are the classes involved in U, Ci is the corresponding center and U
represents the meta-class.
Once the centers are created, the evidential soft label of each example is rep-
resented by a bba over the frame of discernment Ω = {ω0, ω1, ω2} where ω1 and
ω2 represent respectively the majority and the minority class. The proposition
ω0 is included in the frame of discernment explicitly to represent the outlier
class.
Let xs be a sample belonging to the training set. Each class center represents
a piece of evidence to the evidential membership of the sample. The mass values
in regard to the class memberships of xs should depend on d(xs, C), i.e., the
distance between xs and the respective class center. The greater the distance, the
lower the mass value. Consequently, the closer xs is to a speciﬁc class center, the
more likely it belongs to the corresponding class. Hence, the initial unnormalized
masses should be represented by decreasing distance based functions. We use the
Mahalanobis distance [24], in this work, as recommended by [22] in order to deal
with anisotropic datasets.
The unnormalized masses are calculated accordingly:
ˆm({ωi}) = e−d(xs,Ci),
i ∈{1, 2}
(6)
ˆm(U) = e−γ λ d(xs,CU),
U = {ω1, ω2}
(7)
ˆm({ω0}) = et
(8)
where λ = β 2α. A value of α = 1 is ﬁxed as recommended to obtain good results
on average, and β is a parameter such that 0 < β < 1. It is used to tune the
number of objects committed to the overlapping region. The value of γ is equal
to the ratio between the maximum distance of xs to the centers in U and the
minimum distance. It is used to measure the degree of distinguishability among
the majority and minority classes. The smaller γ indicates a poor distinguisha-
bility degree between the classes of U for xs. The outlier class ω0 is taken into
account in order to deal with objects far from both classes, and its mass value
is calculated according to an outlier threshold t.
As performed in [22], the unnormalized belief masses are ﬁnally normalized
as follows:
m(A) =
ˆm(A)

B⊆Ω ˆm(B)
(9)
As a result, a bba is created to formally represent each object’s soft label.
4.2
Cleaning the Majority Class
As a result of bba creation, each majority object will have masses in 4 focal
elements namely: m({ω1}) for the majority class, m({ω2}) for the minority class,

Evidential Undersampling for Imbalanced Datasets
347
m(U) for the overlapping region U, and m({ω0}) for the outlier class. These
masses are used to remove problematic samples from the majority class. There
are diﬀerent types of unwanted samples which could be removed namely:
– Overlapping: Ambiguous samples are usually located in regions where there
is strong overlap between classes as seen in Fig. 1a. This situation could cor-
respond to what is called “conﬂict” in Evidence Theory. In our framework,
this type of examples will have a high mass value in m(U). Thus, majority
instances whose bba has the maximum mass committed to m(U) are consid-
ered as part of an overlapping region, and are automatically discarded. To
avoid excessive elimination and allow tuning, it is also possible to tune the
parameter β. The higher value of β will result in fewer objects committed to
the overlapping region. As for majority instances whose highest mass is not
committed to m(U) (i.e. not in overlapping regions), the observation is neces-
sarily committed to one of the singletons in Ω ({ω1}, {ω2}, or {ω0}). In this
situation, we make use of the plausibility function deﬁned in Eq. (4) to make
a decision of acceptance or rejection. Each majority instance xs is aﬀected to
the class with the maximum plausibility Plmax = maxω∈ΩPl({ω}).
– Label noise: Normally, majority observations should have the maximum
plausibility committed to m({ω1}) which measures the membership value
towards the majority class. By contrast, having Plmax committed to m({ω2})
signify that they are located in a minority location, as illustrated in Fig. 1c.
Consequently, these objects are eliminated from the dataset.
– Outlier: The ﬁnal scenario occurs when the sample in question is located
in a region far from both classes, as shown in Fig. 1b. In our framework,
this is characterized by the state of ignorance and could be discarded in the
undersampling procedure. Hence, majority objects whose maximum plausi-
bility Plmax committed to m({ω0}) are considered as outliers and removed
from the dataset.
Fig. 1. Illustrations describing the diﬀerent data diﬃculty factors that could worsen
class imbalance. Green and red colors respectively represent the majority class and the
minority one. (Color ﬁgure online)

348
F. Grina et al.
4.3
Applying Selective Minority Oversampling
Once the cleaning procedure (undersampling) is performed, we execute the over-
sampling phase. The created bbas at the ﬁrst step are further exploited in this
stage, to intelligently create new synthetic minority samples.
Similarly to the cleaning step, the minority objects are categorized into three
possible diﬃculties: overlapping, label noise, or outlier. The object is considered
as “safe” if it does not belong to any of the three types. Thus, it does not
need to participate in the oversampling phase. The same goes for both label
noise and outlier, since using these types to create synthetic data could result in
overgeneralization, which is a major drawback for oversampling [14]. Although,
samples belonging to overlapping signify that they are located at the borders
of the minority class. Hence, those are the samples that we are most uncertain
about, and should focus on in the oversampling phase. Much like other popular
oversampling methods such as BorderlineSMOTE [14], our goal is to emphasize
the borders of the minority class in order to further improve its visibility.
More formally, let P denote the set of minority examples whose highest mass
is committed to m(U) (the set of ambiguous minority objects). We ﬁrstly com-
pute the k minority nearest neighbors for each object in P. In this step, we
generate |P| ∗s synthetic minority points, where s is a value between 1 and k.
In other words, for each minority instance in P, we randomly select s samples
from its k minority nearest neighbors. Finally, s new synthetic points are gener-
ated anywhere on the lines joining the samples in P and its s randomly selected
neighbors:
−−→
new = −→a + w ∗(−→b −−→a )
(10)
where −→a is the sample in P, −→b is a selected minority neighbor, and w represents
a random value between 0 and 1. This procedure is repeated for each sample in
P, similarly to the SMOTE algorithm (more details in [6]).
Fig. 2. An imbalanced binary example showing the behavior of our proposed algorithm
at each step.
Furthermore, we tested our resampling method on a two-dimensional imbal-
anced dataset in order to showcase the behavior of the algorithm at each step
(see Fig. 2).

Evidential Undersampling for Imbalanced Datasets
349
5
Experimental Study
In this section, we will describe ﬁrstly the setup of the conducted experiments in
Subsect. 5.1. Lastly, we will present the results and discuss them in Subsect. 5.2.
5.1
Setup
Datasets. For the purpose of this evaluation, we selected binary imbalanced
datasets from the keel repository [1]. The datasets are further detailed in Table 1.
The imbalance ratio was calculated as IR = #majority
#minority . The variations of the dif-
ferent parameters (IR, features, and size) allowed for experimenting in diﬀerent
real world settings.
Table 1. Description of the imbalanced datasets selected from the KEEL repository.
Datasets
Imbalance ratios (IR) Features Samples
wisconsin
1.860
9.000
683.000
glass0
2.060
9.000
214.000
vehicle3
2.990
18.000
846.000
ecoli1
3.360
7.000
336.000
yeast3
8.100
8.000
1484.000
page-blocks0
8.790
10.000
5472.000
ecoli-0-6-7 vs 3-5
9.090
7.000
222.000
yeast-0-3-5-9 vs 7-8
9.120
8.000
506.000
ecoli-0-2-6-7 vs 3-5
9.180
7.000
224.000
ecoli-0-1-4-7 vs 2-3-5-6 10.590
7.000
336.000
glass4
15.460
9.000
214.000
yeast-2 vs 8
23.100
8.000
482.000
yeast5
32.730
8.000
1484.000
kr-vs-k-zero vs eight
53.070
6.000
1460.000
abalone-20 vs 8-9-10
72.690
8.000
1916.000
Baseline Classiﬁer. As baseline, we use the decision tree classiﬁer, more specif-
ically CART [4]. The implementation provided in the scikit-learn machine learn-
ing python library [28] was used, with the default parameters.
Metrics and Evaluation Strategy. To appropriately assess the methods in
imbalanced scenarios, we use the G-Mean (GM) [2]. The GM is a popular mea-
sure for evaluating classiﬁers in imbalanced settings. It is calculated as the geo-
metric mean of sensitivity and speciﬁcity:
G-Mean =

sensitivity × specificity
(11)

350
F. Grina et al.
In order to ensure the fairness of the observed results, we adopt a 10-fold strat-
iﬁed cross validation to eliminate inconsistencies. The dataset is split into 10
parts taking into account the class distribution, 90% of which is the training set,
the rest is the test set, and the average of G-mean is taken as the ﬁnal result. It
is worth noting that at each fold, resampling was performed only on the training
set.
To better evaluate the signiﬁcance of the results, statistical analysis was run
using the Wilcoxon’s signed rank tests [36] for the signiﬁcance level of α = 0.05.
Reference Methods and Parameters. We compared our proposed method
(UAHS) against 4 well known re-sampling methods, in addition to Baseline (BL).
The compared methods are SMOTE [6], SMOTE-IPF [30], SMOTE-ENN [3],
and SMOTE-TL [3]. The SMOTE-IPF implementation in Smote-variants [19]
was used. For the rest of the methods, the implementations provided by the
python toolbox imbalanced-learn [21] were applied.
The following parameters were considered for our proposed method UAHS:
α was set to 1 as recommended in [22], the tuning parameter t for m({ω0})
was ﬁxed to 2 to obtain good results in average, and we tested three diﬀerent
values for β in {0.3, 0.5, 0.7} and selected the most performing one for each
dataset, since the amount of class overlap diﬀers in each case. For the other
reference methods, we used the recommended parameters in their respective
original papers.
5.2
Results Discussion
Results on 15 binary imbalanced datasets are shown in Table 2. The best G-
Mean value is marked in bold. Our proposed method UAHS achieved the top
performances across 10 out of 15 datasets. It showed clear improvement over
the compared resampling methods and baseline across complex datasets, espe-
cially with high imbalance degree and class overlapping. Furthermore, UAHS
performed signiﬁcantly better in cases where there is a high number of bor-
derline instance. This conﬁrms that our proposal succeeded at emphasizing on
the visibility of the minority class, and improving its borders by cleaning the
uncertain samples present in the overlapping and noisy regions of the majority
class.
The results for Wilcoxon’s pairwise test are shown in Table 3. R+ represents
the sum of ranks in favor of UAHS, R−, the sum of ranks in favor of the reference
methods, and exact p-values are calculated for each comparison. All comparisons
can be considered as statistically signiﬁcant under a level of 5% since all p-
values are lower than the threshold 0.05. This reveals statistically signiﬁcant
improvements by our method against SMOTE, SMOTE-IPF, SMOTE-ENN,
SMOTE-TL, and baseline.

Evidential Undersampling for Imbalanced Datasets
351
Table 2. G-Mean results for KEEL datasets using CART.
Datasets
BL
SMOTE SMOTE-IPF SMOTE-ENN SMOTE-TL UAHS
wisconsin
0.934
0.928
0.924
0.950
0.932
0.964
glass0
0.737
0.789
0.772
0.763
0.782
0.786
vehicle3
0.689
0.660
0.682
0.727
0.679
0.673
ecoli1
0.816
0.849
0.856
0.871
0.836
0.883
yeast3
0.791
0.834
0.854
0.878
0.849
0.921
page-blocks0
0.904
0.928
0.920
0.930
0.923
0.958
ecoli-0-6-7 vs 3-5
0.764
0.721
0.769
0.772
0.782
0.790
yeast-0-3-5-9 vs 7-8
0.532
0.620
0.606
0.677
0.582
0.613
ecoli-0-2-6-7 vs 3-5
0.762
0.783
0.781
0.751
0.780
0.761
ecoli-0-1-4-7 vs 2-3-5-6
0.735
0.797
0.878
0.857
0.861
0.835
glass4
0.735
0.637
0.706
0.658
0.706
0.838
yeast-2 vs 8
0.547
0.666
0.664
0.722
0.734
0.737
yeast5
0.840
0.830
0.812
0.912
0.855
0.932
kr-vs-k-zero vs eight
0.976
1.000
1.000
0.999
1.000
1.000
abalone-20 vs 8-9-10
0.477
0.695
0.602
0.679
0.587
0.743
Table 3. Wilcoxon’s signed ranks test results comparing the G-Mean scores for CART.
Comparisons
R+
R−
p-value
UAHS vs BL
117.0 3.0
0.000153
UAHS vs SMOTE
98.0
22.0 0.002364
UAHS vs SMOTE-IPF
95.0
25.0 0.004187
UAHS vs SMOTE-ENN 94.0
26.0 0.027679
UAHS vs SMOTE-TL
83.5
36.5 0.004377
6
Conclusion
Solutions for imbalanced datasets are increasingly being applied to critical real
world domains. In order to deal with such scenarios, the proposed methods
should also handle the uncertainty in the data. In this work, we propose an
Uncertainty-Aware Hybrid resampling which combines undersampling and over-
sampling phases to eﬃciently re-balance binary datasets. We use an evidential
structure to represent soft labels for each sample in the dataset. These repre-
sentations are later used to remove majority samples which are ambiguous and
noisy, and to select minority observations at the borders to generate new minor-
ity points.
For future work, we plan to further optimize our method using heuristic meth-
ods in order to approximate the amount of instances which should be cleaned,
and the number of instances to generate.

352
F. Grina et al.
References
1. Alcala-Fdez, J., et al.: Keel data-mining software tool: data set repository, integra-
tion of algorithms and experimental analysis framework. J. Multiple-Valued Logic
Soft Comput. 17, 255–287 (2010)
2. Barandela, R., Valdovinos, R.M., S´anchez, J.S.: New applications of ensembles of
classiﬁers. Pattern Anal. Appl. 6(3), 245–256 (2003)
3. Batista, G., Prati, R., Monard, M.C.: A study of the behavior of several methods
for balancing machine learning training data. SIGKDD Explor. 6, 20–29 (2004)
4. Breiman, L., Friedman, J., Stone, C.J., Olshen, R.A.: Classiﬁcation and Regression
Trees. CRC Press, Boca Raton (1984)
5. Bridge, J., et al.: Introducing the gev activation function for highly unbalanced
data to develop covid-19 diagnostic models. IEEE J. Biomed. Health Inf. 24(10),
2776–2786 (2020)
6. Chawla, N.V., Bowyer, K.W., Hall, L.O., Kegelmeyer, W.P.: Smote: synthetic
minority over-sampling technique. J. Artif. Intell. Res. 16, 321–357 (2002)
7. Chawla, N.V., Japkowicz, N., Drive, P.: Editorial: special issue on learning from
imbalanced data sets. ACM SIGKDD Explor. Newsl. 6(1), 1–6 (2004)
8. Dempster, A.P.: A generalization of bayesian inference. J. Roy. Stat. Soc. Ser. B
(Methodol.) 30(2), 205–232 (1968)
9. Douzas, G., Bacao, F., Last, F.: Improving imbalanced learning through a heuristic
oversampling method based on k-means and SMOTE. Inf. Sci. 465, 1–20 (2018)
10. Feng, Y., Zhou, M., Tong, X.: Imbalanced classiﬁcation: an objective-oriented
review. arXiv preprint arXiv:2002.04592 (2020)
11. Grina, F., Elouedi, Z., Lefevre, E.: A preprocessing approach for class-imbalanced
data using SMOTE and belief function theory. In: Analide, C., Novais, P., Cama-
cho, D., Yin, H. (eds.) IDEAL 2020. LNCS, vol. 12490, pp. 3–11. Springer, Cham
(2020). https://doi.org/10.1007/978-3-030-62365-4 1
12. Grina, F., Elouedi, Z., Lefevre, E.: Evidential undersampling approach for imbal-
anced datasets with class-overlapping and noise. In: The 18th International Confer-
ence on Modeling Decisions for Artiﬁcial Intelligence. Springer, Heidelberg (2021)
13. Haixiang, G., Yijing, L., Shang, J., Mingyun, G., Yuanyue, H., Bing, G.: Learn-
ing from class-imbalanced data: review of methods and applications. Expert Syst.
Appl. 73, 220–239 (2017)
14. Han, H., Wang, W.-Y., Mao, B.-H.: Borderline-SMOTE: a new over-sampling
method in imbalanced data sets learning. In: Huang, D.-S., Zhang, X.-P., Huang,
G.-B. (eds.) ICIC 2005. LNCS, vol. 3644, pp. 878–887. Springer, Heidelberg (2005).
https://doi.org/10.1007/11538059 91
15. Ivan, T.: Two modiﬁcations of CNN. IEEE Trans. Syst. Man Commun. SMC 6,
769–772 (1976)
16. Japkowicz, N.: Class imbalances: are we focusing on the right issue. In: Workshop
on Learning from Imbalanced Data Sets II, vol. 1723, p. 63 (2003)
17. Khoshgoftaar, T.M., Rebours, P.: Improving software quality prediction by noise
ﬁltering techniques. J. Comput. Sci. Technol. 22(3), 387–396 (2007)
18. Korkmaz, S.: Deep learning-based imbalanced data classiﬁcation for drug discovery.
J. Chem. Inf. Model. 60(9), 4180–4190 (2020)
19. Kov´acs, G.: Smote-variants: a python implementation of 85 minority oversampling
techniques. Neurocomputing 366, 352–354 (2019)
20. Koziarski, M., Wo´zniak, M., Krawczyk, B.: Combined cleaning and resampling
algorithm for multi-class imbalanced data with label noise. Knowl.-Based Syst.
204, 106223 (2020)

Evidential Undersampling for Imbalanced Datasets
353
21. Lemaˆıtre, G., Nogueira, F., Aridas, C.K.: Imbalanced-learn: a python toolbox to
tackle the curse of imbalanced datasets in machine learning. J. Mach. Learn. Res.
18(1), 559–563 (2017)
22. Liu, Z.G., Pan, Q., Dezert, J., Mercier, G.: Credal classiﬁcation rule for uncertain
data based on belief functions. Pattern Recogn. 47(7), 2532–2541 (2014)
23. Ma, L., Fan, S.: CURE-SMOTE algorithm and hybrid algorithm for feature selec-
tion and parameter optimization based on random forests. BMC Bioinf 18(1), 1–18
(2017)
24. Mahalanobis, P.C.: On the generalized distance in statistics, vol. 2, pp. 49–55.
National Institute of Science of India (1936)
25. Makki, S., Assaghir, Z., Taher, Y., Haque, R., Hacid, M.S., Zeineddine, H.: An
experimental study with imbalanced classiﬁcation approaches for credit card fraud
detection. IEEE Access 7, 93010–93022 (2019)
26. Ofek, N., Rokach, L., Stern, R., Shabtai, A.: Fast-CBUS: a fast clustering-based
undersampling method for addressing the class imbalance problem. Neurocomput-
ing 243, 88–102 (2017)
27. Pawlak, Z.: Rough sets. Int. J. Comput. Inf. Sci. 11(5), 341–356 (1982)
28. Pedregosa, F., et al.: Scikit-learn: machine learning in python. J. Mach. Learn.
Res. 12, 2825–2830 (2011)
29. Ramentol, E., Caballero, Y., Bello, R., Herrera, F.: SMOTE-RSB *: a hybrid pre-
processing approach based on oversampling and undersampling for high imbalanced
data-sets using SMOTE and rough sets theory. Knowl. Inf. Syst. 33(2), 245–265
(2012)
30. S´aez, J.A., Luengo, J., Stefanowski, J., Herrera, F.: SMOTE-IPF: addressing
the noisy and borderline examples problem in imbalanced classiﬁcation by a re-
sampling method with ﬁltering. Inf. Sci. 291(C), 184–203 (2015)
31. Shafer, G.: A Mathematical Theory of Evidence, vol. 42. Princeton University
Press, Princeton (1976)
32. Smets, P.: The nature of the unnormalized beliefs encountered in the transferable
belief model. In: Uncertainty in Artiﬁcial Intelligence, pp. 292–297. Elsevier (1992)
33. Smets, P.: The transferable belief model for quantiﬁed belief representation.
In: Smets, P. (ed.) Quantiﬁed Representation of Uncertainty and Imprecision.
HDRUMS, vol. 1, pp. 267–301. Springer, Dordrecht (1998). https://doi.org/10.
1007/978-94-017-1735-9 9
34. Tsai, C.F., Lin, W.C., Hu, Y.H., Yao, G.T.: Under-sampling class imbalanced
datasets by combining clustering analysis and instance selection. Inf. Sci. 477,
47–54 (2019)
35. Vuttipittayamongkol, P., Elyan, E.: Improved overlap-based undersampling for
imbalanced dataset classiﬁcation with application to epilepsy and parkinson’s dis-
ease. Int. J. Neural Syst. 30(08), 2050043 (2020)
36. Wilcoxon, F.: Individual comparisons by ranking methods. In: Breakthroughs in
Statistics, pp. 196–202. Springer, Heidelberg (1992). https://doi.org/10.1007/978-
1-4612-4380-9 16
37. Wilson, D.L.: Asymptotic properties of nearest neighbor rules using edited data.
IEEE Trans. Syst. Man Cybern. 3, 408–421 (1972)

Approximations of Belief Functions Using
Compositional Models
Radim Jiroušek1,2
and Václav Kratochvíl1,2(B)
1 The Czech Academy of Sciences, Institute of Information Theory and Automation,
Prague, Czech Republic
{radim,velorex}@utia.cas.cz
2 Faculty of Management, Prague University of Economics and Business,
Jindřichův Hradec, Czech Republic
Abstract. For applications to practical problems, the paper proposes
to use the approximations of belief functions, which simplify their depen-
dence structure. Using an analogy with probability distributions, we rep-
resent these approximations in the form of compositional models. As no
theoretical apparatus similar to probabilistic information theory exists
for belief functions, the problems arise not only in connection with the
design of algorithms seeking the optimal approximations but even in con-
nection with a criterion comparing two diﬀerent approximations. With
this respect, the application of the analogy with probability theory fails.
Therefore, the paper suggests the employment of simple heuristics easily
applicable to real-life problems.
Keywords: Compositional models · Entropy of belief functions ·
Decomposable entropy · Heuristics · Optimality criterion
1
Motivation
Consider a large set of discrete random variables W with a joint probability dis-
tribution π. For an arbitrary partition {U1, U2, . . . , Uk} of W, one can decompose
the joint distribution π using the chain rule as follow:
π“π(U1)π(U2|U1) . . . π(Uk|(U1 Y . . . Y Uk´1))
“
k

i“1
π(Ui|(U1 Y . . . Y Ui´1)).
(1)
Notice that in the product formula of Eq. (1) (which will be often used through-
out this paper), for i “ 1, π(Ui|(U1 Y . . . Y Ui´1)) is just the marginal π(U1).
For i “ 2, π(U2|U1) is the conditional probability table for U2 given U1, etc. In
large models (|W| is large), it is rarely the case that the conditional marginal of
Ui depends on all variables in U1 Y . . . Y Ui´1. This fact was exploited by Perez
Supported by the Czech Science Foundation – Grant No. 19-06569S.
c
⃝Springer Nature Switzerland AG 2021
J. Vejnarová and N. Wilson (Eds.): ECSQARU 2021, LNAI 12897, pp. 354–366, 2021.
https://doi.org/10.1007/978-3-030-86772-0_26

Approximations of Belief Functions
355
[10], who suggested using an ε-admissible approximation by simpliﬁcation of
the dependence structure1 to overcome the computational-complexity problems
accompanying the application of multidimensional probability distributions. His
basic idea is as follows. Substitute each set (U1 Y . . . Y Ui´1) in Eq. (1) by its
smaller subset Ti such that the conditional probability distribution π(Ui|Ti) is
almost the same as π(Ui|(U1 Y. . .YUi´1)). The non-similarity of probability dis-
tributions π and κ deﬁned on Ω can be measured using Kullback-Leibler (KL)
divergence [9] deﬁned as follows2
KL(π∥κ) “

xPΩ:κ(x)ą0
π(x) log
π(x)
κ(x)

.
(2)
Let κ “ k
i“1 π(Ui|Ti). If KL(π∥κ) ď ε, then κ is called an ε-admissible approx-
imation of π.
Now, consider a diﬀerent problem. Let {V1, V2, . . . , Vk} be a set of subsets of
W (generally not disjoint) such that k
i“1 Vi “ W. Given a set of low-dimensional
distributions {κi(Vi)}i“1,...,k, a question is whether there exists a multidimen-
sional distribution for W such that all κi’s are its marginals. If there exists a
distribution π such that all κi are its marginals, Perez [10] found an answer to a
related question: What is the best ε-admissible approximation of π (in the sense
of the smallest ε) that can be assembled from {κi(Vi)}i“1,...,k? Thus, Perez was
looking for a permutation (j1, j2, . . . , jk) of indices (1, 2, . . . , k), which minimizes
KL

π∥k
i“1 κji(Vji \ Tji|Tji)
	
, where Tji “ Vji X (Vj1 Y . . . Y Vji´1). For this,
he showed [10] that (H denotes the Shannon entropy [12])
KL

π∥
k

i“1
κji(Vji \ Tji|Tji)

“ ´H(π)`
k

i“1

H(κji(Vji))´H(κji(Tji))
	
, (3)
which equals H(k
i“1 κji(Vji \ Tji|Tji)) ´ H(π) in case that all κi are marginals
of both π and k
i“1 κji(Vji \ Tji|Tji). Thus, regardless of whether distribution
π is known or not, he proved that its best approximation (that simpliﬁes the
dependence structure), which can be set up from {κi(Vi)}i“1,...,k, is that which
minimizes k
i“1

H(κji(Vji)) ´ H(κji(Tji))
	
. If one considers only the approx-
imations k
i“1 κji(Vji \ Tji|Tji) having all κi for its marginals, then the best
approximation minimizes its Shannon entropy H(k
i“1 κji(Vji \ Tji|Tji)) (which
corresponds with the intuition that it maximizes its information content).
1 The notion reﬂects the fact that the considered approximation extends the set of con-
ditional independence relations holding for the probability distribution in question
[15].
2 Eq. (2) deﬁnes the KL divergence if κ dominates π, i.e., if for all x P Ω, for which
κ(x) “ 0, π(x) is also 0. Otherwise, the KL divergence is deﬁned to be `∞.

356
R. Jiroušek and V. Kratochvíl
2
Belief Functions
As in Sect. 1, let W denote a set of variables with ﬁnite number of states. For
X P W, Let ΩX denote the set of states of variable X. A basic assignment for
variables U Ď W (or equivalently basic assignment on the Cartesian product
ΩU “×XPU ΩX) is a mapping mU : 2ΩU →[0, 1], such that 
aĎΩU mU(a) “ 1
and mU(H) “ 0.
Consider a basic assignment mU. If the set of the corresponding variables is
clear from the context, we omit the subscript U. Thus, we say that a is a focal
element of m if m(a) ą 0.
For basic assignment mV, we often consider its marginal basic assignment
m↓U
V
for U Ď V. A similar notation is used also for projections: for a P ΩV,
symbol a↓U denotes the element of ΩU, which is obtained from a by omitting
the values of variables in V \ U. For a Ď ΩV,
a↓U “ {a↓U : a P a}.
Thus, the marginal m↓U
V
of basic assignment mV for U is deﬁned as follows:
m↓U
V (b) “

aĎΩV: a↓U“b
mV(a).
for all b Ď ΩU.
The projection of sets enables us to deﬁne a join of two sets. Consider two
arbitrary sets U and V of variables (they may be disjoint or overlapping, or one
may be a subset of the other). Consider two sets a Ď ΩU and b Ď ΩV. Their
join is deﬁned as
a Ź◁b “ {c P ΩUYV : c↓U P a & c↓V P b}.
Notice that if U and V are disjoint, then a Ź◁b “ a × b, if U “ V, then
a Ź◁b “ a X b, and, in general, for c Ď ΩUYV, c is a subset of c↓U Ź◁c↓V,
which may be a proper subset.
A basic assignment m can equivalently be deﬁned by the corresponding belief
function, or by plausibility function, or by commonality function [11] as follows:
Belm(a) “

bĎΩ: bĎa
m(b),
Plm(a) “

bĎΩ: bXa̸“H
m(b),
Qm(a) “

bĎΩ: bĚa
m(b).
These representations are equivalent in the sense that when one of these functions
is given, it is always possible to compute the others uniquely. For example:
Plm(a) “ 1 ´ Belm(Ω \ a),

Approximations of Belief Functions
357
m(a) “

bĎa
(´1)|a\b|Belm(b),
m(a) “

bP2Ω: bĚa
(´1)|b\a|Qm(b).
(4)
After normalizing the plausibility function for singleton subsets, one gets for
each a Ď Ω
λm(a) “

bPa Plm({b})

bPΩ Plm({b})
(5)
a probability function on Ω. λm is called a plausibility transform of basic assign-
ment m [1]. There is a number of other probabilistic transforms of a mass assign-
ment m described in literature (e.g., [2]) but in this text we need only the so-
called pignistic transform [13,14] deﬁned as follows:
πm(a) “

aPa

bĎΩ:aPb
m(b)
|b| .
(6)
To construct multidimensional models from low-dimensional building blocks,
we need some operators connecting two low-dimensional basic assignments into
one more-dimensional. One possibility is the classical Dempster’s combination
rule, which is used to combine distinct belief functions. Consider two basic assign-
ments mU and mV for arbitrary sets of variables U and V. Dempster’s combina-
tion rule is deﬁned for each c Ď ΩUYV as follows:
(mU ‘ mV)(c) “
1
1 ´ K

aĎΩU,bĎΩV:aŹ◁b“c
mU(a) · mV(b),
(7)
where
K “

aĎΩU,bĎΩV:aŹ◁b“H
mU(a) · mV(b),
(8)
which can be interpreted as the amount of conﬂict between mU and mV. If
K “ 1, then we say that the basic assignments mU and mV are in total conﬂict
and their Dempster’s combination is undeﬁned.
3
Compositional Models
Dempster’s rule of combination may be equivalently expressed using the corre-
sponding commonality functions QmU and QmV [11]
QmU‘mV(c) “

1
1 ´ K

QmU (c↓U) · QmV(c↓V),
where K is the same as deﬁned in Eq. (8). Let us stress that it was designed
to combine independent sources of information. When combining the sources
of uncertain information, which may not be distinct, we have to ensure that no
information is double-counted. For this, an operator of composition was designed.

358
R. Jiroušek and V. Kratochvíl
Deﬁnition 1. Consider two arbitrary basic assignments mU, mV, and their
commonality functions QmU and QmV. Their composition is a basic assign-
ment mU Ź mV, the corresponding commonality function of which is given by
the composition of their commonality functions deﬁned for each c Ď ΩUYV by
the following expression:
(QmU Ź QmV)(c) “
⎧
⎨
⎩
1
L
QmU (c↓U)·QmV (c↓V)
Qm↓UXV
V
(c↓UXV)
if
Qm↓UXV
V
(c↓UXV) ą 0,
0
otherwise,
(9)
where the normalization constant
L “

dĎΩUYV:Qm↓UXV
V
(c↓UXV)ą0
(´1)|d|`1 QmU (c↓U) · QmV(c↓V)
Qm↓UXV
V
(c↓UXV)
.
If L “ 0 then mU and mV are in total conﬂict and the composition is undeﬁned.
Remark. Deﬁnition 1 is taken from [4], where the reader can ﬁnd the moti-
vation not repeated in this paper. Unfortunately, there is no explicit formula
for computing the composition of two basic assignments. However, there is a
way to avoid the necessity to transform the ﬁrst argument into its commonal-
ity representation. When computing the composition of two basic assignments
we transform the second argument mV into QmV, and compute the correspond-
ing conditional commonality function QmV\U|VXU “ QmV/Qm↓UXV
V
. Then, using
Eq. (4), we compute the corresponding conditional basic assignment mV\U|VXU.
Eventually
mU Ź mV “ mU ‘ mV\U|VXU.
Thus, the computations of composition are limited by the dimensionality of the
second argument because, as a rule, the representation of the corresponding com-
monality function requires the space for 2(2|V|) values regardless of the number
of focal elements of mV.
In the following assertion, we brieﬂy describe the main properties of the
composition operator. These properties are proved for Shenoy’s valuation-based
systems (VBS) in [4], from which it follows that all of them hold for belief
functions also. Notice that Properties 2 and 3 of the following assertion prove
the fact that the introduced operator of composition avoids double-counting the
information about the variables U X V. Namely, we can see that the operator
disregards the information about these variables that is contained in the second
argument.
Proposition 1. For arbitrary basic assignments mU1, mU2, mU3 the following
statements hold, if the respective expressions are deﬁned.
1. (Domain): mU1 Ź mU2 is a basic assignment for variables U1 Y U2.
2. (Composition preserves ﬁrst marginal): (mU1 Ź mU2)↓U1 “ mU1.
3. (Reduction:) If U2 Ď U1 then, mU1 Ź mU2 “ mU1.

Approximations of Belief Functions
359
4. (Non-commutativity): In general, mU1 Ź mU2 ̸“ mU2 Ź mU1.
5. (Commutativity under consistency): If mU1 and mU2 are consistent, i.e.,
m↓U1XU2
U1
“ m↓U1XU2
U2
, then mU1 Ź mU2 “ mU2 Ź mU1.
6. (Non-associativity): In general, (mU1 Ź mU2) Ź mU3 ̸“ mU1 Ź (mU2 Ź mU3).
7. (Associativity under RIP): If U1 ⊃(U2 X U3), or, U2 ⊃(U1 X U3) then,
(mU1 Ź mU2) Ź mU3 “ mU1 Ź (mU2 Ź mU3).
By a belief function compositional model we understand a basic assignment
m1 Ź · · · Ź mn obtained by a multiple application of the composition operator.
Let us emphasize that if not speciﬁed otherwise by parentheses, the operators
are always performed from left to right, i.e.,
m1 Ź m2 Ź m3 Ź . . . Ź mn “ (. . . ((m1 Ź m2) Ź m3) Ź . . . Ź mn´1) Ź mn.
Consider a (ﬁnite) system W of small subsets of the considered variables W.
The vague assumption that U P W is small is accepted to avoid the compu-
tational problems connected with computations with the corresponding basic
assignments. Thus, we assume that for each U P W we have (or we can easily
get) a basic assignment mU. Moreover, we assume that these basic assignments,
as well as the corresponding commonality functions QmU , can eﬀectively be rep-
resented in computer memory.
Using an analogy with Perez’ approximations of probability distributions, we
are looking for the best approximation simplifying the dependence structure of
some basic assignment m, the marginals of which for sets from W are at our
disposal. In other words, we are looking for a sequence of sets (Ui)i“1,...,n from
W such that the model mU1 ŹmU2 Ź· · ·ŹmUn approximates the unknown basic
assignment m best. To simplify notation, we denote mi “ mUi. Therefore we
will speak about a model m1 Ź m2 Ź . . . Ź mn, in which basic assignment mi is
deﬁned for variables Ui, and the corresponding commonality function is Qi.
The considered compositional model is a |U1 Y . . . Y Un|-dimensional basic
assignment. It is said to be perfect if all mi are marginals of m1 Ź m2 Ź . . . Ź
mn. Thus, perfect models reﬂect all the information represented by the low-
dimensional basic assignments from which they are composed. So, it is not sur-
prising that the optimal approximation simplifying the dependence structure
will be, as a rule, a perfect model.
If a model is not perfect, it can always be perfectized using the following
assertion (proved in [4]).
Proposition 2 (perfectization procedure). For any compositional model
m1 Ź m2 Ź . . . Ź mn, the model ¯m1 Ź ¯m2 Ź . . . Ź ¯mn deﬁned
¯m1 “ m1,
¯m2 “ ¯m↓U2XU1
1
Ź m2,
...
¯mn “ ¯m↓UnX(U1Y...YUn´1)
n
Ź mn,
is perfect, and m1 Ź m2 Ź . . . Ź mn “ ¯m1 Ź ¯m2 Ź . . . Ź ¯mn.

360
R. Jiroušek and V. Kratochvíl
The procedure applies to any compositional model, nevertheless, its compu-
tational eﬃciency is guaranteed only for decomposable models introduced below.
As a rule, a perfect model can equivalently be represented by several permuta-
tions of low-dimensional basic assignments. In [4], the following two important
assertions are proved.
Proposition 3 (on perfect models). Consider a perfect model m1Ź. . .Źmn,
and a permutation of its indices i1, i2, . . . , in such that mi1 Ź mi2 Ź . . . Ź min is
also perfect. Then,
m1 Ź m2 Ź . . . Ź mn “ mi1 Ź mi2 Ź . . . Ź min.
Compositional model m1 Ź m2 Ź . . . Ź mn is said to be decomposable if
the sequence U1, U2, . . . , Un of the corresponding basic assignments meets the
so called running intersection property (RIP): ∀i “ 2, . . . , n
∃j (1 ď j < i) :
Ui X (U1 Y . . . Y Ui´1) Ď Uj.
Proposition 4 (on consistent decomposable models). Consider a decom-
posable model m1 Ź m2 Ź . . . Ź mn. The model is perfect if and only if
basic assignments m1, m2, . . . , mn are pairwise consistent, i.e., ∀{i, j}
⊂
{1, 2, . . . , n}, m↓UiXUj
i
“ m↓UiXUj
j
.
4
Entropy
In this paper, we primarily consider Shenoy’s entropy introduced in [6]. It is
deﬁned
HS(mV) “

aĎΩV
(´1)|a|QmV(a) log(QmV(a))
(10)
using the commonality function of basic assignment mV (no formula based on a
basic assignment is known). This function is not always non-negative. However,
its merit is that it is the only deﬁnition of belief function entropy that satisﬁes an
additivity property in the sense that HS(mX ‘ mY |X) “ HS(mX) ` HS(mY |X)
(here, mX is a basic assignment for X, and mY |X is a conditional basic assign-
ment for Y given X such that its marginal for X is vacuous). This additivity,
which is one of the fundamental properties in probabilistic information theory,
makes the computation of the entropy for perfect compositional models of very
high dimensions possible. Namely, the conditional entropy should be computed
according to the following formula (U and T are disjoint sets of variables):
HS(mU|T ) “

aĎΩUYT
(´1)|a|Qm(UYT )(a) log(QmU|T (a)),
(11)
where QmU|T (a) “ Qm(UYT )(a)/Qm↓T
(UYT )(a↓T ) for all a Ď ΩUYT . Note that for
T “ H, HS(mU|T ) “ HS(mU), and that the deﬁnition of conditional entropy
in Eq. (11) is analogous to Shannon’s deﬁnition of conditional entropy of proba-
bilistic conditionals [12].

Approximations of Belief Functions
361
Thus, for arbitrary U and V, entropy HS of a composition of two consistent
mU and mV (i.e., m↓UXV
U
“ m↓UXV
V
) can be computed as a sum of HS(mU) and
the respective conditional entropy computed from mV
HS(mU Ź mV) “ HS(mU) ` HS(mV\U|VXU).
(12)
5
Example
In this example, we consider an 8-dimensional basic assignment m for binary
variables S, T, U, V, W, X, Y, Z. Let us start studying the approximations of m
assembled from its marginals. We consider the approximations that are analogous
to Perez’ probabilistic approximations simplifying the dependence structure. For
this purpose, consider the ﬁve marginals described in Table 1.
Table 1. Five low-dimensional basic assignments
Basic assignments Number of focal elements HS
m{S,T,U}
9
0.1951790
m{T,U,V }
9
0.1644314
m{V,W,X}
10
0.1562828
m{W,Y }
5
0.0702895
m{X,Z}
5
0.1385793
Checking the validity of RIP, one can easily verify that compositional model
m{S,T,U} Ź m{T,U,V } Ź m{V,W,X} Ź m{W,Y } Ź m{X,Z} is decomposable. Due to
Proposition 4, it means that this model is perfect, and therefore it contains all the
information from all the given low-dimensional basic assignments. Therefore, we
are sure that this compositional model is optimal among those approximations
that can be assembled from the marginals from Table 1.
In addition to this optimal one, let us consider four other approximations
deﬁned by the permutations not satisfying RIP. Thus, in this example, we com-
pare the following ﬁve compositional models:
M1 : m{S,T,U} Ź m{T,U,V } Ź m{V,W,X} Ź m{W,Y } Ź m{X,Z},
M2 : m{S,T,U} Ź m{V,W,X} Ź m{W,Y } Ź m{X,Z} Ź m{T,U,V },
M3 : m{S,T,U} Ź m{W,Y } Ź m{X,Z} Ź m{T,U,V } Ź m{V,W,X},
M4 : m{W,Y } Ź m{X,Z} Ź m{V,W,X} Ź m{T,U,V } Ź m{S,T,U},
M5 : m{W,Y } Ź m{S,T,U} Ź m{X,Z} Ź m{V,W,X} Ź m{T,U,V }.

362
R. Jiroušek and V. Kratochvíl
To eﬃciently compute their entropy HS, we modify the expressions deﬁning the
considered models using the properties from Proposition 1 receiving
M2 : m{S,T,U} Ź m{V,W,X} Ź m{W,Y } Ź m{X,Z},
M3 : m{S,T,U} Ź m{W,Y } Ź m{X,Z} Ź m{T,U,V },
M4 :

m{W,Y } Ź m{X,Z}

Ź m{V,W,X} Ź m{T,U,V } Ź m{S,T,U},
M5 :

m{W,Y } Ź m{X,Z}

Ź m{V,W,X} Ź m{S,T,U}.
Notice, these models are decomposable (for this we have to consider that models
M4 and M5 start with a four-dimensional basic assignment in parentheses),
and therefore they can easily be perfectized using Proposition 2. Therefore, the
values of HS presented in the ﬁrst row of Table 2 can be computed by a successive
application of Formula (12). Using the analogy with the probabilistic paradigm
(introduced in Sect. 1) we expect that the lower the entropy, the better the
model. Thus, the values of HS from Table 2 suggest the following preferences of
models:
M2 ≈M5 ≻M1 ≈M4 ≻M3,
which is not what we would like to see because we are sure that model M1 is
the best one. Thus, entropy HS cannot be recommended as an ultimate criterion
determining, which of the compared approximations is better. In general, it is
not an easy task to say, which of two compositional models approximates better
a given basic assignment, and ﬁnding a corresponding criterion function remains
an open problem. In the next section, we consider three functions and study
whether they can heuristically be used for this purpose.
Table 2. Comparison of compositional models Mi based on HS, HA, HP
M1
M2
M3
M4
M5
HS
0.5346689
0.5324691
0.537525
0.5346689
0.5324691
HA 11.21685
11.13043
11.27071
11.23775
11.14948
HP
10.23799
10.33313
10.28555
10.24451
10.3397
Let us repeat that we can recognize the optimal solution only in very special
situations. Namely, when the considered approximation is a perfect composi-
tional model constructed from all the considered basic assignments {mU}UPW.
Then, the approximation reﬂects all the information from the system of the
considered low-dimensional basic assignments, and therefore it is optimal.
This fact was already employed in [3], in which we compared simple heuristic
(“hill-climbing”) algorithms that were controlled by HS and other two functions
proposed as entropy for belief functions. In short, in addition to HS, we consid-
ered the entropy suggested in [5].
HA(mV) “

aĎΩV
m(a) log(|a|)`H(λm) “

aĎΩV
m(a) log(|a|)´

aPΩV
λ(a) log(λ(a)),

Approximations of Belief Functions
363
and its modiﬁcation (inspired by [7])
HP (mV) “

aĎΩV
m(a) log(|a|)`H(πm) “

aĎΩV
m(a) log(|a|)´

aPΩV
π(a) log(π(a))
(recall that H denotes Shannon entropy, and λm and πm are the respective plau-
sibility and pignistic transforms deﬁned by Formulas (5) and (6), respectively).
The values of HA and HP for models M1 – M5 are in Table 2. Looking at their
values, one can see that HP detects the optimal model suggesting the preferences
M1 ≻M4 ≻M3 ≻M2 ≻M5.
Note that this observation is also in agreement with results published in [3].
Nevertheless, in contrast to HS, neither HA nor HP is additive, and therefore
one cannot compute their values for compositional models of practical size. This
is why in the next section, we propose and test heuristics applicable to real size
problems.
6
Comparison of Heuristics on Random Models
In the example in Sect. 5, we mentioned three functions HS, HA and HP pro-
posed to serve as entropy for belief functions. The great advantage of HS is
its additivity expressed in Formula (12), which is the property holding also for
Shannon entropy. It enables us to compute HS for perfect models of very high
dimensions as a sum
HS(m1 Ź . . . Ź mn) “
n

i“1
HS(mUi\ ˆUi|UiX ˆUi),
(13)
where ˆUi “ U1 Y U2 Y . . . Y Ui´1, and mUi\ ˆUi|UiX ˆUi is computed from mi. No
analogous formulas for the computations of the other two entropies HA and HP
exist. Not being able to compute their values for models of higher dimensions,
we performed computational experiments with a derived heuristic function HA
(and analogously also HP )
HA(m1 Ź . . . Ź mn) “
n

i“1

HA(m↓Ui
i
) ´ HA(m↓UiX ¯Ui
i
)
	
.
Using the codes developed in R-studio, we randomly generated 110 perfect
decomposable compositional models for 26 variables3. Realizing random simple
swaps on the order of the basic assignments deﬁning the decomposable mod-
els, we damaged the running intersection property. In this way, likewise in the
3 To generate a decomposable model, ﬁrst, we generate a sequence of sets of variables
satisfying running intersection property. Then we generated random basic assign-
ments for given sets of variables and run the perfectization procedure as described
in Proposition 2.

364
R. Jiroušek and V. Kratochvíl
example presented in the previous section, we got for each randomly generated
decomposable model 19 non-decomposable models. For each model from such
a 20-tuple, we computed three values: HS, and HA, HP . By HS we denote the
value computed according to Formula (13). Notice that HS “ HS only for perfect
models. Repeat that this equality is guaranteed, due to the pairwise consistency
of randomly generated low-dimensional basic assignments, only for the decom-
posable model from each of the considered 20-tuple of compositional models.
Table 3. Results from random experiments
HS
HA HP
Minimum achieved for the decomposable model
13
110 110
Minimum achieved only for the decomposable model 12
107 107
In Table 3, we depict how many times the respective heuristic functions
achieved their minimum for the decomposable models. From this, one can see
that both HA and HP detected all the decomposable models as optimal. Only for
3 out of all 110 of these experiments, a non-decomposable model was found, for
which the value of HA (and also HP ) was the same as that for the decomposable
model.
Realize, that in total we generated 2,200 compositional models. In Fig. 1, the
reader can see how the values of the considered heuristics for non-decomposable
models diﬀer from those for the respective decomposable model. The histograms
(a), (b) and (c) describe the behavior of values of heuristics HA, HP and HS
for all 19 × 110 “ 2, 090 non-decomposable models. Notice that while from his-
tograms (a) and (b) we see that all the diﬀerences were non-negative, histogram
(c) shows that values of HS for non-decomposable models were both higher and
lower than the corresponding values for the respective decomposable models.
Though the results achieved with HA and HP are rather promising, neither
of these heuristics guarantees the detection of the optimal model with certainty.
Have a look at Table 4 containing values of HA and HP for models from Sect. 5.
From this, one can see that not only entropy HA (as shown in Table 2), but also
the heuristic HA does not achieve its minimum for the optimal model.
Table 4. Comparison of compositional models Mi based on HA, HP
M1
M2
M3
M4
M5
HA 11.20454 11.12313 11.26799 11.22912 11.14770
HP
10.25123 10.33647 10.29524 10.26100 10.34624
Let us ﬁnish this section by mentioning that these results fully correspond
with the results presented in [3] describing the experiments with heuristic model
learning procedures.

Approximations of Belief Functions
365
1
10
100
0.0
0.5
1.0
1.5
2.0
2.5
HA: Difference from decomposable model
Count (log scale)
1
10
100
0.0
0.5
1.0
1.5
2.0
2.5
HP: Difference from decomposable model
Count (log scale)
1
10
100
−0.4
−0.2
0.0
0.2
HS: Difference from decomposable model
Count (log scale)
Fig. 1. .
7
Conclusions
Because of their high computational complexity, one cannot make an inference
with multidimensional belief functions. Therefore, we suggest using their approx-
imations. In this paper, we studied the approximations called approximations
simplifying the dependence structure. As illustrated with an example, the open
problem is not only to ﬁnd an optimal approximation but even the problem
of recognizing, which of two approximations is better. Inspired by an analogy
with probability theory, we studied the possibility of using information-theoretic
characteristics4 to evaluate the quality of an approximation. Based on the results
from random experiments, we suggest heuristic functions denoted by HA and
HP , for this purpose.
Acknowledgment. The authors wish to acknowledge that the ﬁnal version of the
paper reﬂects long discussions with Prakash P. Shenoy.
4 Most of the characteristics suggested in [8] cannot be used because of their high com-
putational complexity. As said above, only HS can be computed for high-dimensional
models due to its additivity.

366
R. Jiroušek and V. Kratochvíl
References
1. Cobb, B.R., Shenoy, P.P.: On the plausibility transformation method for trans-
lating belief function models to probability models. Int. J. Approximate Reason.
41(3), 314–340 (2006)
2. Cuzzolin, F.: On the relative belief transform. Int. J. Approximate Reason. 53(5),
786–804 (2012)
3. Jiroušek, R., Kratochvíl, V., Shenoy, P.P.: Entropy-based learning of compositional
models from data. Submitted to Belief 2021 (2021)
4. Jiroušek, R., Shenoy, P.P.: Compositional models in valuation-based systems. Int.
J. Approximate Reason. 55(1), 277–293 (2014)
5. Jiroušek, R., Shenoy, P.P.: A new deﬁnition of entropy of belief functions in the
Dempster-Shafer theory. Int. J. Approximate Reason. 92(1), 49–65 (2018)
6. Jiroušek, R., Shenoy, P.P.: On properties of a new decomposable entropy of
Dempster-Shafer belief functions. Int. J. Approximate Reason. 119(4), 260–279
(2020)
7. Jousselme, A.-L., Liu, C., Grenier, D., Bossé, É.: Measuring ambiguity in the evi-
dence theory. IEEE Trans. Syst. Man Cybern. Part A Syst. Hum. 36(5), 890–903
(2006)
8. Klir, G.J.: Generalized information theory. Fuzzy Sets Syst. 40(1), 127–142 (1991)
9. Kullback, S., Leibler, R.A.: On information and suﬃciency. Ann. Math. Stat. 22,
76–86 (1951)
10. Perez, A.: ε-admissible simpliﬁcations of the dependence structure of a set of ran-
dom variables. Kybernetika 13(6), 439–449 (1977)
11. Shafer, G.: A Mathematical Theory of Evidence. Princeton University Press (1976)
12. Shannon, C.E.: A mathematical theory of communication. Bell Syst. Tech. J.
27(379–423), 623–656 (1948)
13. Smets, P.: Constructing the pignistic probability function in a context of uncer-
tainty. In: Henrion, M., Shachter, R., Kanal, L.N., Lemmer, J.F. (eds.) Uncertainty
in Artiﬁcial Intelligence, vol. 5, pp. 29–40. Elsevier (1990)
14. Smets, P., Kennes, R.: The transferable belief model. Artif. Intell. 66(2), 191–234
(1994)
15. Studený, M.: Formal properties of conditional independence in diﬀerent calculi of
AI. In: Clarke, M., Kruse, R., Moral, S. (eds.) ECSQARU 1993. LNCS, vol. 747,
pp. 341–348. Springer, Heidelberg (1993). https://doi.org/10.1007/BFb0028219

Dempster-Shafer Approximations
and Probabilistic Bounds in Statistical
Matching
Davide Petturiti1(B)
and Barbara Vantaggi2
1 Dip. Economia, University of Perugia, Perugia, Italy
davide.petturiti@unipg.it
2 Dip. MEMOTEF, “La Sapienza” University of Rome, Rome, Italy
barbara.vantaggi@uniroma1.it
Abstract. Many economic applications require to integrate information
coming from diﬀerent data sources. In this work we consider a speciﬁc
integration problem called statistical matching, referring to probabilistic
distributions of Y |X, Z|X and X, where X, Y, Z are categorical (possibly
multi-dimensional) random variables. Here, we restrict to the case of no
logical relations among random variables X, Y, Z. The non-uniqueness
of the conditional distribution of (Y, Z)|X suggests to deal with sets of
probabilities. For that we consider diﬀerent strategies to get a conditional
belief function for (Y, Z)|X that approximates the initial assessment in
a reasonable way. In turn, such conditional belief function, together with
the marginal probability distribution of X, gives rise to a joint belief
function for the distribution of V = (X, Y, Z).
Keywords: Statistical matching · Belief functions · Inference
1
Introduction
In the era of big data the integration of knowledge coming from several sepa-
rate data bases is a problem of primary interest. Many economic applications
provide situations in which the data sets have some variables in common as well
as some variables recorded only in one data base. Some distinguished examples
are marketing research [13] and microsimulation modeling [19–21,26,31]. In par-
ticular, in this work we deal with the so called statistical matching problem for
categorical variables, where there are two or more diﬀerent sources with some
overlapping variables while some variables are collected only in one source. Thus,
data are missing by design since they have been already collected separately, and
the gathering of joint data on variables collected separately would be expensive
and time-consuming.
This problem has been studied in a coherent probability setting in [27] by
showing that when no logical constraints among the variables are present, then
the assignment is coherent, but there is more than one probability compatible
with the given assignment.
c
⃝Springer Nature Switzerland AG 2021
J. Vejnarov´a and N. Wilson (Eds.): ECSQARU 2021, LNAI 12897, pp. 367–380, 2021.
https://doi.org/10.1007/978-3-030-86772-0_27

368
D. Petturiti and B. Vantaggi
Traditionally, to cope with this problem by avoiding to manage a class of
distributions, assumptions that are strong enough to assure a unique compatible
distribution are used. For instance, the available data are assumed to satisfy con-
ditional independence among the variables collected separately given the com-
mon variables. Nevertheless, it turns out that, it is too restrictive to consider just
one of the compatible distributions (as already noted for the statistical matching
problem in [7,9,12,22,27] and for the missing data problem in [3,15,24]).
As suggested in [6,27] we could consider the envelopes of the compatible prob-
abilities. In [6] it has been proved that the lower envelope is not 2-monotone and
the upper envelope is not 2-alternating, therefore the computation of probability
bounds on new conditional events must be solved through linear programming.
Here, we consider just two data sources, namely A and B, each characterized
by (possibly multi-dimensional) categorical random variables (X, Y ) and (X, Z).
We assume to know the probabilistic distributions of Y |X, Z|X and X and
restrict to the case of no logical relations among random variables X, Y, Z. The
non-uniqueness of the conditional distribution of (Y, Z)|X suggests to deal with
sets of probabilities. A ﬁrst aim of the paper is to approximate the probabilistic
bounds for (Y, Z)|X arising from statistical matching in order to provide an
approximation of the lower envelope of the joint distribution of V = (X, Y, Z)
by means of a belief function. This problem can be faced by looking to inner and
outer approximations. The main peculiarity of inner approximation resides on
the fact that we determine a subset of the core of the lower envelope given by the
bounds estimated from data only, by taking a subset of compatible probabilities.
However, we prove here that the belief functions that are inner approximations
reduce to probability measures, so this approach reduces to the choice of any
probability distribution built from copulas, as shown in Subsection 4.1.
Then, we face the problem by looking for an outer approximation of the
probabilistic bounds for the distribution of (Y, Z)|X. The main peculiarity of
outer approximation resides on the fact that we complete the core of the lower
envelope by adding further probability distributions. Obviously there are diﬀer-
ent outer approximations as already mentioned for other problems in [16–18],
and the choice of the outer approximation depends on the distance used for this
aim. In analogy with [17], we provide a method to get the outer approximations
based on linear programming for the L1 and L∞distance and based on quadratic
programming for the L2 distance.
Then, we study ϵ-contaminated models that are obtained from the ϵ-conta-
mination [11] of a probability measure in the core of (Y, Z)|X for a ﬁxed x. In
particular we show that such models are neither inner nor outer approximations.
Finally, we show that the global assessment arising from the approximation
procedure is a coherent lower conditional probability in the sense of Williams [30].
Further, relying on results given in [1], we show that the least committal coherent
lower conditional probability extension of the approximated assessment can be
computed, in many cases, just through a generalized Bayesian conditioning rule
[1,29].

Dempster-Shafer Approximations and Probabilistic Bounds
369
2
Preliminaries
Let A be a ﬁnite Boolean algebra of events, where ∅and Ω stand for the impossi-
ble and sure events. Further, ∨, ∧, (·)c denote the disjunction, conjunction, and
contrary operations, while ⊆, = stand for implication and double implication.
Moreover, A0 stands for A0 = A \ {∅}.
A function ϕ : A →[0, 1] such that ϕ(∅) = 0 and ϕ(Ω) = 1 is said to be a:
probability measure: if ϕ(A∨B) = ϕ(A)+ϕ(B), for all A, B ∈A with A∧B = ∅;
belief function: if, for every k ≥2 and every E1, . . . , Ek ∈A,
ϕ
 k
i=1
Ei

≥

∅̸=I⊆{1,...,k}
(−1)|I|+1ϕ

i∈I
Ei

;
2-monotone capacity: if ϕ(A ∨B) ≥ϕ(A) + ϕ(B) −ϕ(A ∧B), for all A, B ∈A;
(coherent) lower probability: if there exists a closed (in the product topology) set
P of probability measures on A such that, for all A ∈A,
ϕ(A) = min
P ∈P P(A).
In particular, a lower probability [28,29] is normally denoted as P and P is
taken equal to the set
P = {P : P is a probability measure on A, P ≥P},
said core [10] or credal set [14]. It turns out that 2-monotone capacities are
particular lower probabilities, belief functions are particular 2-monotone capac-
ities, and probability measures are particular belief functions. Further, every
function ϕ has an associated dual function ψ deﬁned, for every A ∈A, as
ψ(A) = 1 −ϕ(Ac).
In the rest of the paper we will be mainly concerned with belief functions,
whose theory has been essentially developed by Dempster and Shafer in [4,25]. A
belief function is usually denoted as Bel and its dual, said plausibility function,
as Pl. Every belief function is completely characterized by its M¨obius inverse
m : A →[0, 1], also said basic probability assignment, which is a non-negative
function with m(∅) = 0 and summing up to 1 such that, for all A ∈A, Bel(A) =

B⊆A
B∈A
m(B).
In the rest of the paper, we refer to the notion of conditional probability due
to Dubins [8]. A function P : A × A0 →[0, 1] is a full conditional probability on
A if it satisﬁes:
(i) P(E|H) = P(E ∧H|H), for every E ∈A and H ∈A0;
(ii) P(·|H) is a probability measure on A, for every H ∈A0;
(iii) P(E∧F|H) = P(E|H)·P(F|E∧H), for every H, E∧H ∈A0 and E, F ∈A.

370
D. Petturiti and B. Vantaggi
More generally, given a non-empty G ⊆A × A0, a function P : G →[0, 1]
is a coherent conditional probability (see [2]) if there exists a full conditional
probability Q on A such that P(E|H) = Q(E|H), for all E|H ∈G.
This allows to introduce the notion of coherent lower conditional probability
due to Williams [30] as a function P : G →[0, 1] such that there exists a closed
(in the product topology) set P of full conditional probabilities on A such that
P(E|H) = min
Q∈P Q(E|H), for all E|H ∈G.
All notions introduced so far adapt in a straightforward way if we take a ﬁeld
of sets in place of A.
3
Statistical Matching
Consider two or more diﬀerent sources with some overlapping variables and some
variables collected only in one source. For example let A and B be two sources
and let X represent the common variables, Y denotes the variables collected
only in A, and Z those only in B. Thus, the data consist of two samples, one on
(X, Y ) and the other one on (X, Z). In this context data are missing by design
since they have been already collected separately.
Traditionally, to cope with these problems, the available data are combined
with assumptions, as conditional independence between Y and Z given X, strong
enough to assure a unique compatible distribution. Actually, since there are many
distributions on (X, Y, Z) compatible with the available partial information on
(X, Y ) and (X, Z), it is too restrictive to consider just one of the compatible
distributions (as already noted in [6,7,12,22,27]).
The available information allows to estimate probability distributions PY |X
from A and PZ|X from B and the marginal probability PX from both sources. For
instance, assuming that units in A and B are i.i.d. observations from probabil-
ity distributions P(X,Y ) and P(X,Z), respectively, and assuming that (X, Y ) has
range X × Y and (X, Z) has range X × Z, the ML estimates of PY |X, PZ|X, PX
can be obtained from the corresponding observed frequencies. In detail, we set
ˆPY |X(y|x) = nA
xy
nA
x
,
ˆPZ|X(z|x) = nB
xz
nB
x
,
ˆPX(x) = nA
x + nB
x
nA + nB ,
where nA
xy is the number of units in A with categories (x, y), nB
xz is the number
of units in B with categories (x, z), nA
x and nA
x are the number of units with
category x in A and B, respectively, and nA and nB are the total number of
units in A and B.
Let X, Y, Z be categorical random variables, ranging in the ﬁnite sets X, Y, Z.
Such random variables determine the Boolean algebras B(X), B(Y ), B(Z) gen-
erated by the partitions of the sure event
{(X = x) : x ∈X},
{(Y = y) : y ∈Y},
and
{(Z = z) : z ∈Z}.
Let V
= (X, Y, Z) and assume no logical relations are present between
the random variables X, Y, Z, that is B(V ) = B(X) ⊗B(Y ) ⊗B(Z) having

Dempster-Shafer Approximations and Probabilistic Bounds
371
B(X), B(Y ), B(Z) as Boolean sub-algebras. This implies that the vector V has
range V = X × Y × Z and a probability measure P : B(V ) →[0, 1] induces a
probability distribution PV : 2V →[0, 1] for V by setting, for every A ∈2V,
PV (A) = P(V ∈A).
Notice that B(V ) is a Boolean algebra isomorphic to 2V, thus P can be actually
identiﬁed with PV .
Consider the assessment
{PY |X(y|x), PZ|X(z|x), PX(x) : x ∈X, y ∈Y, z ∈Z}.
(1)
The statistical matching problem in this case amounts to ﬁnding a probability
distribution PV for V consistent with (1), and this reduces to a problem of
coherence of the given assessment.
The coherence of assessment (1) is assured (see [27]) whenever there is no
logical constraint between the variables X, Y, Z.
Theorem 1. Let X, Y, Z be three ﬁnite random variables such that B(V ) =
B(X) ⊗B(Y ) ⊗B(Z), then the assessment (1) is coherent.
Remark 1. In other terms, Theorem 1 states that there exists a full conditional
probability Q on B(V ) such that, for all x ∈X, y ∈Y, z ∈Z, it holds that
Q(Y = y|X = x) = PY |X(y|x), Q(Z = z|X = x) = PZ|X(z|x), and Q(X = x) =
PX(x), where Q(·|Ω) is identiﬁed with Q(·), as usual.
The assessment (1) gives rise to a closed and convex set of probability distri-
butions PV for V deﬁned on 2V. The lower and upper envelopes P V = min PV
and P V = max PV , where minima and maxima are pointwise on 2V, have been
characterized in [6].
In analogy, for every x ∈X, the sub-assessment
{PY |X(y|x), PZ|X(z|x) : y ∈Y, z ∈Z}
(2)
determines a closed and convex set of probability distributions P(Y,Z)|(X=x) for
(Y, Z)|(X = x) deﬁned on 2Y×Z. The set P(Y,Z)|(X=x) induces the lower and
upper envelopes
P (Y,Z)|(X=x) = min P(Y,Z)|(X=x)
and
P (Y,Z)|(X=x) = max P(Y,Z)|(X=x),
where minima and maxima are pointwise on 2Y×Z.
Actually, for all A ∈2Y and B ∈2Z, since B(V ) = B(X)⊗B(Y )⊗B(Z), and,
so, B(Y, Z) = B(Y ) ⊗B(Z) is isomorphic to 2Y×Z, from the Frech´et-Hoeﬀding
inequality, the lower bound on (A × Z) ∩(Y × B) = A × B is
P (Y,Z)|(X=x)(A × B) = max{0, PY |X(A|x) + PZ|X(B|x) −1},
where PY |X(A|x) = 
y∈A PY |X(y|x) and PZ|X(B|x) = 
z∈B PZ|X(z|x). More-
over, from Theorem 1 in [23], for all H ∈2Y×Z, it follows that
P (Y,Z)|(X=x)(H) =
max
A×B⊆H
A∈2Y,B∈2Z
max{0, PY |X(A|x) + PZ|X(B|x) −1}.
(3)

372
D. Petturiti and B. Vantaggi
It turns out that P V and P (Y,Z)|(X=x) are generally not 2-monotone capac-
ities (see [6]). Actually, in Proposition 2 in the aforementioned paper it has
been proved that P V is 2-monotone only in the case that at least a distribution
between PY |X and PZ|X is degenerate, for all x ∈X with PX(x) > 0.
For completeness we provide the following example that will be developed in
the rest of the paper.
Example 1. Let X, Y, Z be dichotomic random variables ranging in X = Y =
Z = {0, 1} and consider the conditional distributions,
PY |X(y|0) =
 1
3 if y = 0,
2
3 if y = 1,
and
PZ|X(z|0) =
 1
2 if z = 0,
1
2 if z = 1.
Denote
Y × Z = {(0, 0)
	 
 
=a1
, (0, 1)
	 
 
=a2
, (1, 0)
	 
 
=a3
, (1, 1)
	 
 
=a4
}
and let Ai = {ai}, Aij = {ai, aj}, Aijk = {ai, aj, ak} and A1234 = Y × Z.
We have that P(Y,Z)|(X=0) = {αP1 + (1 −α)P2 : α ∈[0, 1]}, where the
probability distributions P1, P2 and the lower envelope P (Y,Z)|(X=0) are
2Y×Z
∅A1 A2 A3 A4 A12 A13 A14 A23 A24 A34 A123 A124 A134 A234 A1234
P1
0 0
2
6
3
6
1
6
2
6
3
6
1
6
5
6
3
6
4
6
5
6
3
6
4
6
1
1
P2
0
2
6
0
1
6
3
6
2
6
3
6
5
6
1
6
3
6
4
6
3
6
5
6
1
4
6
1
P (Y,Z)|(X=0) 0 0
0
1
6
1
6
2
6
3
6
1
6
1
6
3
6
4
6
3
6
3
6
4
6
4
6
1
It is easy to see that P (Y,Z)|(X=0) is not 2-monotone since
P (Y,Z)|(X=0)(A123) = 3
6 < 5
6 = P (Y,Z)|(X=0)(A12) + P (Y,Z)|(X=0)(A13)
−P (Y,Z)|(X=0)(A1).
♦
4
Dempster-Shafer Inner and Outer Approximations
Our aim is to obtain a belief function Bel(Y,Z)|(X=x) approximating in some
reasonable sense P (Y,Z)|(X=x). This allows to deﬁne
Bel(Y,Z)|X(A|x) = Bel(Y,Z)|(X=x)(A),
(4)
Pl(Y,Z)|X(A|x) = Pl(Y,Z)|(X=x)(A),
(5)
for every A ∈2Y×Z and x ∈X, with Pl(Y,Z)|(X=x)(A) = 1−Bel(Y,Z)|(X=x)(Ac),
where the complement is intended in Y × Z.
For every B ∈2V and x ∈X let
[B]x = {(y, z) ∈Y × Z : (x, y, z) ∈B},
(6)

Dempster-Shafer Approximations and Probabilistic Bounds
373
where possibly [B]x = ∅. It is easily seen that, every B ∈2V can be expressed as
B =

x∈X
({x} × [B]x)
in which a Cartesian product is set to ∅if one of the factors is equal to ∅.
Then we deﬁne, for every B ∈2V
BelV (B) =

x∈X
Bel(Y,Z)|X([B]x|x)PX(x),
(7)
PlV (B) =

x∈X
Pl(Y,Z)|X([B]x|x)PX(x).
(8)
Proposition 1. BelV and PlV deﬁned as in (7) and (8) are dual belief and
plausibility functions on 2V. Moreover, for every A ∈2Y×Z and x ∈X it holds
that
(a) BelV ({x} × Y × Z) = PlV ({x} × Y × Z) = PX(x),
(b) Bel(Y,Z)|X(A|x) = BelV ({x}×A)
PX(x)
and Pl(Y,Z)|X(A|x) = P lV ({x}×A)
PX(x)
, provided
that PX(x) > 0.
Proof. It is suﬃcient to prove that BelV is a belief function on 2V and that PlV
is the dual capacity of BelV .
Since [∅]x = ∅and [V]x = Y × Z, we have that
BelV (∅) =

x∈X
Bel(Y,Z)|X(∅|x)PX(x) = 0,
BelV (V) =

x∈X
Bel(Y,Z)|X(Y × Z|x)PX(x) = 1.
For every A, B ∈2V with A ⊆B, we have that [A]x ⊆[B]x for every
x ∈X, and this implies Bel(Y,Z)|X([A]x|x) ≤Bel(Y,Z)|X([B]x|x). Hence, by
the monotonicity of the expectation operator with respect to PX, it follows that
BelV (A) ≤BelV (B). This implies that BelV is a normalized capacity on 2V.
For every k ≥2, every E1, . . . , Ek ∈2V, and every ∅̸= I ⊆{1, . . . , k}, since
for every x ∈X it holds that

i∈I
Ei

x
=

i∈I
[Ei]x
and

i∈I
Ei

x
=

i∈I
[Ei]x
we have that
Bel(Y,Z)|X
 k
i=1
[Ei]x
 x

≥

∅̸=I⊆{1,...,k}
(−1)|I|+1Bel(Y,Z)|X
 
i∈I
[Ei]x
 x

.
Hence, by the monotonicity and the linearity of the expectation operator with
respect to PX, it follows that
BelV
 k
i=1
Ei

≥

∅̸=I⊆{1,...,k}
(−1)|I|+1BelV

i∈I
Ei

.

374
D. Petturiti and B. Vantaggi
Further, for every B ∈2V, since ([B]x)c = [Bc]x, where the complement in
the ﬁrst member is taken with respect to Y × Z, while the complement in the
second member is taken with respect to V, we have that
BelV (B) =

x∈X
Bel(Y,Z)|X([B]x|x)PX(x)
=

x∈X

1 −Pl(Y,Z)|X([B]c
x|x)

PX(x)
= 1 −

x∈X
Pl(Y,Z)|X([B]c
x|x)PX(x)
= 1 −

x∈X
Pl(Y,Z)|X([Bc]x|x)PX(x)
= 1 −PlV (Bc).
Statements (a) and (b) follow since, for every x, x′ ∈X and A ∈2Y×Z,
[{x} × A]x′ is equal to A if x = x′ and to ∅otherwise.
□
Below we explore some strategies to arrive to a Bel(Y,Z)|X. We stress that
Bel(Y,Z)|X is a collection of conditional belief functions on 2Y×Z for x ∈X.
Their combination through Dempster’s rule (see [4,25]) does not seem suitable
both from a semantic point of view and since it would result in a belief function
on 2Y×Z disregarding PX.
4.1
Inner Approximating Conditional Belief Functions
For all x ∈X, the most intuitive way for deﬁning Bel(Y,Z)|(X=x) is to require
that Bel(Y,Z)|(X=x) ≥P (Y,Z)|(X=x), pointwise on 2Y×Z. In such a way we get an
inner approximating conditional belief function.
The following theorem shows that, for all x ∈X, every inner approximating
conditional belief function is actually additive and belongs to P(Y,Z)|(X=x).
Theorem 2. For all x ∈X, every belief function Bel(Y,Z)|(X=x) on 2Y×Z sat-
isfying
– Bel(Y,Z)|(X=x)({y} × Z) ≥PY |X(y|x), for all y ∈Y,
– Bel(Y,Z)|(X=x)(Y × {z}) ≥PZ|X(z|x), for all z ∈Z,
is additive and belongs to P(Y,Z)|(X=x).
Proof. Since every Bel(Y,Z)|(X=x) is completely characterized by its M¨obius
inverse m : 2Y×Z →[0, 1], we need to ﬁnd an m solving the system
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩

B⊆{y}×Z
m(B) ≥PY |X(y|x), for all y ∈Y,

B⊆Y×{z}
m(B) ≥PZ|X(z|x), for all z ∈Z,

B⊆Y×Z
m(B) = 1,
m(B) ≥0,
for all B ⊆Y × Z,
m(∅) = 0.

Dempster-Shafer Approximations and Probabilistic Bounds
375
Summing memberwise the inequalities related to y ∈Y and those related to
z ∈Z, we get

y∈Y

B⊆{y}×Z
m(B) ≥1
and

z∈Z

B⊆Y×{z}
m(B) ≥1.
The two inequalities above, together with

B⊆Y×Z
m(B) = 1 imply that m can
be positive only on singletons, therefore Bel(Y,Z)|(X=x) is additive (see [25]).
Further, since P (Y,Z)|(X=x)({y} × Z) = PY |X(y|x) and P (Y,Z)|(X=x)(Y × {z}) =
PZ|X(z|x), it follows that Bel(Y,Z)|(X=x) ∈P(Y,Z)|(X=x).
□
The previous theorem also implies that the only belief functions having a core
contained in P(Y,Z)|(X=x) are actually additive and coincide with the elements
of P(Y,Z)|(X=x).
4.2
Outer Approximating Conditional Belief Functions
Since only trivial inner approximations are possible, another possibility for deﬁn-
ing Bel(Y,Z)|(X=x) is to require that P (Y,Z)|(X=x) ≥Bel(Y,Z)|(X=x), pointwise on
2Y×Z. In such a way we get an outer approximating conditional belief function.
The problem of ﬁnding an outer approximation of a lower probability through
a belief function has been investigated by a series of papers [16–18]. In the
cited papers, the outer approximation is found by minimizing a suitable distance
deﬁned, in our context, over the set of lower probabilities on 2Y×Z. In detail,
the three most investigated distances are deﬁned, for every P, Q as:
– d1(P, Q) =

A⊆Y×Z
|P(A) −Q(A)|,
– d2(P, Q) =

A⊆Y×Z
(P(A) −Q(A))2,
– d∞(P, Q) =
max
A⊆Y×Z |P(A) −Q(A)|.
Notice that d1, d2, and d∞can be seen as restrictions of distances induced by
the L1, L2, and L∞norms on the space [0, 1]2Y×X .
In our context, for a chosen distance d, the problem is to ﬁnd a M¨obius inverse
m : 2Y×Z →[0, 1] for the belief function Bel(Y,Z)|(X=x), solving the optimization
problem
minimize d(P (Y,Z)|(X=x), Bel(Y,Z)|(X=x))
subject to:
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩

B⊆A
m(B) ≤P (Y,Z)|(X=x)(A), for all A ⊆Y × Z,

B⊆Y×Z
m(B) = 1,
m(B) ≥0,
for all B ⊆Y × Z,
m(∅) = 0.
(9)

376
D. Petturiti and B. Vantaggi
In
particular,
due
to
the
outer
approximation
constraint
between
P (Y,Z)|(X=x) and Bel(Y,Z)|(X=x), we can remove the absolute value in d1 and
d∞. Therefore, using d1 problem (9) reduces to a linear programming problem,
while using d2 problem (9) reduces to a quadratic programming problem. Finally,
using d∞problem (9) reduces to the optimization of a convex piecewise linear
(aﬃne) function with linear constraints that, in turn, can be transformed to a
linear programming problem, by adding an extra scalar variable.
As highlighted in [17], the main disadvantage connected to distance d1 is the
non-uniqueness of the found solution, while d2 always leads to a unique solution.
Further, both d1 and d2 guarantee that the found solution is undominated, i.e.,
there is no other belief function pointwise comprised between P (Y,Z)|(X=x) and
the found solution (see [17]). On the other hand, using d∞we generally lose both
uniqueness and undomination.
Example 2. Take X, Y, Z, and PY |X(y|0) and PZ|X(z|0) as in Example 1. Solving
problem (9) using d1 we get inﬁnitely many undominated solutions, two of which
are
2Y×Z
∅A1 A2 A3 A4 A12 A13 A14 A23 A24 A34 A123 A124 A134 A234 A1234
P (Y,Z)|(X=0)
0 0
0
1
6
1
6
2
6
3
6
1
6
1
6
3
6
4
6
3
6
3
6
4
6
4
6
1
m1
0 0
0
1
6
1
6
0
2
6
0
0
2
6
0
0
0
0
0
0
Bel1
(Y,Z)|(X=0) 0 0
0
1
6
1
6
0
3
6
1
6
1
6
3
6
2
6
1
6
3
6
4
6
4
6
1
m2
0 0
0
1
6
1
6
1
6
1
6
0
0
1
6
1
6
0
0
0
0
0
Bel2
(Y,Z)|(X=0) 0 0
0
1
6
1
6
1
6
2
6
1
6
1
6
2
6
3
6
3
6
3
6
4
6
4
6
1
whose d1 distance from P (Y,Z)|(X=0) is equal to 4
6.
Using d2 we get a unique undominated solution that coincides with the belief
function Bel2
(Y,Z)|(X=0) also minimizing d1, whose d2 distance from P (Y,Z)|(X=0)
is equal to 1
9.
Finally, we have that Bel2
(Y,Z)|(X=0) turns out to minimize also d∞, and its d∞
distance from P (Y,Z)|(X=0) is equal to 1
6. In this case it holds that Bel2
(Y,Z)|(X=0)
is the unique solution minimizing d∞and is also undominated.
♦
Besides minimizing a distance, other approaches are available, like minimizing
a measure of nonspeciﬁcity (or imprecision) as done in [5]. Independently of the
objective function, problem (9) has an exponential number of constraints and
unknowns thus it is a computationally hard problem in general. However, the
number of unknowns can be reduced limiting to k-additive belief functions [10].
4.3
ϵ-Contamination Models
Previous subsections showed that to avoid triviality, only outer approximations
of P (Y,Z)|(X=x) make sense, as no non-additive belief function inner approximat-
ing P (Y,Z)|(X=x) exists.
Another method to get a belief function connected to P(Y,Z)|(X=x) is to choose
a reference P ∗∈P(Y,Z)|(X=x), as that determined by the independence copula

Dempster-Shafer Approximations and Probabilistic Bounds
377
between PY |X(·|x) and PZ|X(·|x), and then build an ϵ-contamination model [11].
This amounts to choose an ϵ ∈(0, 1) and build the class of probability distribu-
tions on 2Y×Z deﬁned as
PP ∗,ϵ
(Y,Z)|(X=x) = {Q = (1 −ϵ)P ∗+ ϵP : P is a probability on 2Y×Z}.
The lower envelope of PP ∗,ϵ
(Y,Z)|(X=x) is the belief function BelP ∗,ϵ
(Y,Z)|(X=x)
deﬁned, for every A ∈2Y×Z as
BelP ∗,ϵ
(Y,Z)|(X=x)(A) =

(1 −ϵ)P ∗(A) if A ̸= Y × Z,
1
otherwise.
(10)
For 0 < ϵ < ϵ′ < 1, we have BelP ∗,ϵ′
(Y,Z)|(X=x) ≤BelP ∗,ϵ
(Y,Z)|(X=x) pointwise on
2Y×Z. Since BelP ∗,ϵ
(Y,Z)|(X=x) is non-additive for all ϵ ∈(0, 1), we automatically
get from Theorem 2 that BelP ∗,ϵ
(Y,Z)|(X=x) cannot be an inner approximation of
P (Y,Z)|(X=x).
The following example shows that, in general, BelP ∗,ϵ
(Y,Z)|(X=x) is neither an
outer approximation of P (Y,Z)|(X=x).
Example 3. Take X, Y, Z, and PY |X(y|0) and PZ|X(z|0) as in Example 1. Let
P ∗be the element of P(Y,Z)|(X=0) determined by the independence copula
between PY |X(·|0) and PZ|X(·|0). For A1 = {(0, 0)}, since P ∗(A1) =
1
6 and
P (Y,Z)|(X=0)(A1) = 0, there is no ϵ ∈(0, 1) such that BelP ∗,ϵ
(Y,Z)|(X=0)(A1) ≤
P (Y,Z)|(X=0)(A1). Actually, it is possible to show that for all P ∗∈P(Y,Z)|(X=0)
and all ϵ ∈(0, 1), the resulting BelP ∗,ϵ
(Y,Z)|(X=0) is not an outer approximation of
P (Y,Z)|(X=0).
♦
5
Dempster-Shafer Approximation and Inference
The main advantage in approximating the probabilistic assessment (2) with
the conditional belief assessment Bel(Y,Z)|X is that the global Dempster-Shafer
assessment {BelV , Bel(Y,Z)|X} can be viewed as a lower conditional probability
assessment on the set of conditional events
G = {(V ∈B)|Ω : B ∈2V} ∪{((Y, Z) ∈A)|(X = x) : A ∈2Y×Z, x ∈X}.
In particular, the resulting assessment on G turns out to be coherent in the
sense of Williams [30] since there exists a closed set Q of full conditional proba-
bilities on B(V ) whose lower envelope Q = min Q, where minima are pointwise
on B(V ) × B(V )0, is such that, for all B ∈2V, A ∈2Y×Z, and x ∈X, it holds
that
Q(V ∈B) = BelV (B)
and
Q((Y, Z) ∈A|X = x) = Bel(Y,Z)|X(A|x),

378
D. Petturiti and B. Vantaggi
where we identify Q(·|Ω) with Q(·), as usual. This claim is an immediate conse-
quence of Proposition 1.
In turn, the assessment on G can be extended on the entire B(V ) × B(V )0
searching for the least committal coherent lower conditional probability. This
amounts to taking Q equal to the set of all full conditional probabilities on B(V )
compatible with {BelV , Bel(Y,Z)|X}.
By results in [1], we have that, in many cases, the computation of the least
committal Q can be simply reduced to a generalized version of the Bayesian
conditioning rule [28]. In particular, for all E|H ∈B(V ) × B(V )0, we have that
Q(E|H) =
Q(E ∧H)
Q(E ∧H) + Q(Ec ∧H),
(11)
provided that Q(E ∧H) + Q(Ec ∧H) > 0, where Q is the upper envelope of Q.
For instance, for all A ∈2X and non-empty B ∈2Y×Z, we can compute
Q(X ∈A|(Y, Z) ∈B) =
Q((X∈A)∧((Y,Z)∈B))
Q((X∈A)∧((Y,Z)∈B))+Q((X∈Ac)∧((Y,Z)∈B))
=
BelV ((A×Y×Z)∩(X×B))
BelV ((A×Y×Z)∩(X×B))+P lV ((Ac×Y×Z)∩(X×B)),
(12)
provided that BelV ((A×Y ×Z)∩(X ×B))+PlV ((Ac ×Y ×Z)∩(X ×B)) > 0.
Example 4. Take X, Y, Z, and PY |X(y|0) and PZ|X(z|0) as in Example 1. Con-
sider the outer approximation Bel2
(Y,X)|(X=0) computed in Example 2 by min-
imizing distance d2, which is also optimal according to d1 and d∞. Suppose
PY |X(y|1) = PY |X(y|0) and PZ|X(z|1) = PZ|X(z|0), therefore we can take
Bel2
(Y,X)|(X=1) = Bel2
(Y,X)|(X=0), obtaining Bel(Y,Z)|X. Let PX(0) = PX(1) = 1
2.
Simple computations show that
Q(X = 1|Y = 1, Z = 1) =
BelV ({(1, 1, 1)})
BelV ({(1, 1, 1)}) + PlV ({(0, 1, 1)}) =
1
12
1
12 + 3
12
= 1
4.
♦
6
Conclusions and Future Works
In this paper we have addressed the statistical matching problem for two data
sources, assuming no logical relations between the variables X, Y, Z. Due to
the relevance of avoiding assumptions imposing a unique probability measure,
we proposed strategies in order to approximate the initial assessment inside
Dempster-Shafer theory. We showed that one of the advantages of such approx-
imation with respect to dealing with the original coherent lower conditional
probability is an easier way to compute the least committal extension on new
events, in many cases. The aim of future research is threefold: (i) to generalize
the problem to the case of logical relations among the random variables X, Y, Z;
(ii) to consider more than two data sources; (iii) to carry out an experimental
analysis.

Dempster-Shafer Approximations and Probabilistic Bounds
379
References
1. Coletti, G., Petturiti, D., Vantaggi, B.: Conditional belief functions as lower
envelopes of conditional probabilities in a ﬁnite setting. Inf. Sci. 339, 64–84 (2016)
2. Coletti, G., Scozzafava, R.: Probabilistic Logic in a Coherent Setting, Trends in
Logic, vol. 15. Kluwer Academic Publisher, Dordrecht/Boston/London (2002)
3. de Cooman, G., Zaﬀalon, M.: Updating beliefs with incomplete observations. Artif.
Intell. 159, 75–125 (2004)
4. Dempster, A.: Upper and lower probabilities induced by a multivalued mapping.
Ann. Math. Stat. 38(2), 325–339 (1967)
5. Denœux, T.: Constructing belief functions from sample data using multinomial
conﬁdence regions. Int. J. Approximate Reasoning 42(3), 228–252 (2006)
6. Di Zio, M., Vantaggi, B.: Partial identiﬁcation in statistical matching with mis-
classiﬁcation. Int. J. Approximate Reasoning 82, 227–241 (2017)
7. D’Orazio, M., Di Zio, M., Scanu, M.: Statistical Matching: Theory and Practice.
Wiley (2006)
8. Dubins, L.: Finitely additive conditional probabilities, conglomerability and disin-
tegrations. Ann. Probab. 3(1), 89–99 (1975)
9. Endres, E., Fink, P., Augustin, T.: Imprecise imputation: a nonparametric micro
approach reﬂecting the natural uncertainty of statistical matching with categorical
data. J. Oﬃcial Stat. 35(3), 599–624 (2019)
10. Grabisch, M.: Set Functions, Games and Capacities in Decision Making. Theory
and Decision Library C. Springer International Publishing (2016)
11. Huber, P.J.: Robust Statistics. Wiley, New York (1981)
12. Kadane, J.B.: Some statistical problems in merging data ﬁles. J. Oﬃcial Stat. 17,
423–433 (2001)
13. Kamakura, W.A., Wedel, M.: Statistical data fusion for cross-tabulation. J. Mark.
Res. 34, 485–498 (1997)
14. Levi, I.: The Enterprise of Knowledge. MIT Press, Cambridge (1980)
15. Manski, C.: Identiﬁcation Problems in the Social Sciences. Harvard University
Press, Cambridge (1995)
16. Miranda, E., Montes, I., Vicig, P.: On the selection of an optimal outer approxi-
mation of a coherent lower probability. Fuzzy Sets and Systems (2021)
17. Montes, I., Miranda, E., Vicig, P.: 2-monotone outer approximations of coherent
lower probabilities. Int. J. Approximate Reasoning 101, 181–205 (2018)
18. Montes, I., Miranda, E., Vicig, P.: Outer approximating coherent lower probabili-
ties with belief functions. Int. J. Approximate Reasoning 110, 1–30 (2019)
19. Okner, B.: Constructing a new microdata base from existing microdata sets: The
1966 merge ﬁle. Ann. Econ. Soc. Meas. 1, 325–362 (1972)
20. Okner, B.: Data matching and merging: an overview. Ann. Econ. Soc. Meas. 3,
347–352 (1974)
21. Paass, G.: Statistical match: evaluation of existing procedures and improvements
by using additional information. In: Orcutt, G., Quinke, H. (eds.) Microanalytic
Simulation Models to Support Social and Financial Policy, vol. 1, pp. 401–422.
Elsevier Science (1986)
22. Rubin, D.: Statistical matching using ﬁle concatenation with adjusted weights and
multiple imputations. J. Bus. Econ. Stat. 2, 87–94 (1986)
23. R¨uschendorf, L.: Fr´echet-Bounds and Their Applications, Mathematics and Its
Applications, vol. 67, pp. 151–187. Springer, Netherlands (1991)
24. Schafer, J.: Analysis of incomplete multivariate data. Chapman & Hall (1997)

380
D. Petturiti and B. Vantaggi
25. Shafer, G.: A Mathematical Theory of Evidence. Princeton University Press,
Princeton (1976)
26. Sziv´os, P., Rudas, T., T´oth, I.: A tax-beneﬁt microsimulation model for hungary. In:
Workshop on Microsimulation in the New Millennium: Challenges and Innovations
(1998)
27. Vantaggi, B.: Statistical matching of multiple sources: a look through coherence.
Int. J. Approximate Reasoning 49, 701–711 (2008)
28. Walley, P.: Coherent lower (and upper) probabilities. Department of Statistics,
University of Warwick, Technical report (1981)
29. Walley, P.: Statistical Reasoning with Imprecise Probabilities. Chapman and Hall,
London (1991)
30. Williams, P.M.: Note on conditional previsions (1975), unpublished report of School
of Mathematical and Physical Science, University of Sussex (Published in Interna-
tional Journal of Approximate Reasoning, 44, 366–383 (2007))
31. Wolfson, M., Gribble, S., Bordt, M., Murphy, B., Rowe, G.: The social policy
simulation database and model: an example of survey and administrative data
integration. Surv. Curr. Bus. 69, 36–41 (1989)

The Vehicle Routing Problem with Time
Windows and Evidential Service
and Travel Times: A Recourse Model
Tekwa Tedjini(B), Sohaib Aﬁﬁ, Fr´ed´eric Pichon, and Eric Lef`evre
Univ. Artois, UR 3926, Laboratoire de Genie Informatique et d’Automatique de
l’Artois (LGI2A), 62400 B´ethune, France
{tekwa.tedjini,sohaib.afifi,frederic.pichon,eric.lefevre}@univ-artois.fr
Abstract. This paper addresses a variant of the vehicle routing problem
with time windows where service and travel times are modeled within the
framework of belief function theory. This theory is general as it oﬀers to
model several facets of information imperfection, including uncertainty
and imprecision. An extension of stochastic programming with recourse
is used to tackle the problem. This approach aims to regain the feasibility
of the routes that missed one or more of the customer time windows due
to the uncertain nature of the problem. A memetic algorithm is devised
to solve the problem on an adaptation of literature instances.
Keywords: Vehicle routing · Time windows · Recourse · Belief
function · Memetic algorithm
1
Introduction
The Vehicle Routing Problem with Time Windows (VRPTW) [9] is one of the most
studied variants of vehicle routing problems (VRP). VRPTW routes are subject to
time and capacity restrictions. Customers must be served within their time win-
dows, vehicles have to return to the depot before its closure and their capacity
must be respected. The main objective is to optimize the operating costs including
the cost of vehicles and the overall traversed distances. The VRPTW is NP-hard.
Indeed, even ﬁnding a feasible solution for the problem is itself NP-complete in the
strong sense [15].
Usually, external factors like weather condition or unexpected road accidents
aﬀect one or more of the input parameters such as service and travel times, thus,
the planned routes must account for the possible variations of those parameters.
Accordingly, most of the research papers handled this issue using probability the-
ory giving rise to the Stochastic VRPTW (SVRPTW) [11,18]. SVRPTW models
are either tackled in a Chance-Constrained Programming (CCP) fashion [3,4,11],
where the probabilities (chances) of time windows violations are below a given
threshold, or in a Stochastic Programming with Recourse (SPR) one [5,11,20]. In
SPR models, routes are ﬁrst planned, then, when actual service and travel times are
c
⃝Springer Nature Switzerland AG 2021
J. Vejnarov´a and N. Wilson (Eds.): ECSQARU 2021, LNAI 12897, pp. 381–395, 2021.
https://doi.org/10.1007/978-3-030-86772-0_28

382
T. Tedjini et al.
revealed, corrections (so-called recourse actions) are performed on routes that are
subject to time windows violations in order to regain their feasibility. For instance,
when a vehicle arrives late at a customer’s location, the service is dropped and a
new visit will be rescheduled to serve him. Each correction induces a penalty cost
that must be added to the routing costs. Both of the CCP and SPR approaches
have their advantages and drawbacks, they can either be combined as in [6], or
applied separately depending on the needs of the decision maker.
Set theory has also been used to handle the uncertainty in time parameters,
yielding the Robust VRPTW (RVRPTW) [1,8,12]. This model assumes that
uncertain parameters belong to a predeﬁned set, i.e., parameters are known impre-
cisely, and provides solutions that are immunized against imprecision. This is com-
monly done by optimizing the routing costs considering extreme scenarios for ser-
vice and travel times. Nevertheless, solutions tend to be overly conservative.
Recently, more general uncertainty reasoning frameworks have emerged in the
VRP’s literature to serve as complementary tools to the existent set/probabilistic
approaches, among others the theory of belief function also known as evidence the-
ory [16]. Beyond imprecision or (probabilistic) uncertainty, this theory oﬀers to
represent problem parameters that are aﬀected by more subtle forms of informa-
tion imperfection arising from partial lack of knowledge. In the context of VRPs,
belief function theory was used, for the ﬁrst time, to model the uncertainty on
customers demands in the Capacitated VRP (CVRP) variant [7]. Extensions of
the CCP and the SPR approaches were proposed yielding the Belief-Constrained
Programming and the Belief Programming with Recourse approaches. In this
paper, we follow this line of work by tackling the VRPTW with uncertain ser-
vice and travel times represented within the belief function framework. A Belief-
Constrained Programming version for the same problem was already proposed in
[19]. Herein, a belief programming with recourse approach is devised. We assume
that time windows are hard, i.e., no early or late services are allowed. The recourse
policy is based on skipping service at the failures locations to regain routes feasi-
bility. This problem is very challenging from a computational perspective, to our
knowledge, even in the case of stochastic programming, few papers handled the
SPR version of VRPTW with stochastic service and/or travel times and hard time
windows [5,6,20]. Most of the works focused either on total or partial soft time win-
dows due the diﬃculty of the problem. We also design a memetic algorithm [13] to
solve an adaptation of literature instances.
The paper is organized as follows. Section 2 deﬁnes the VRPTW and recalls
the fundamentals of belief function theory. In Sect. 3 we describe the proposed
recourse approach as well as some particular cases of the model. Section 4 is
dedicated to the experimental results. Section 5 concludes the paper.
2
Deﬁnitions and Notations
This section provides a brief background on the necessary tools required for our
developments.

VRPTW with Evidential Service and Travel Times: A Recourse Model
383
2.1
Problem Formulation
Given a ﬂeet of K vehicles of the same capacity Q and cost M (M is a large
value), the VRPTW is deﬁned on a graph G = (V, E), where V = {0, 1, . . . , n} is
the node set and E = {(i, j) : i ̸= j, i, j ∈V } is the arc set. The depot is denoted
by 0 and Vc = {1, . . . , n} is the set of customers. Each node i has a demand
qi ≤Q, a service time si and a time interval (window) [ei, li] in which service
can start, q0 = s0 = 0 and [e0, l0] = [0, l0] is the time horizon for the depot.
Time windows are considered to be hard, that is if a vehicle k arrives at node j
earlier than ej, the service is postponed until the opening of the time window,
and arrivals after lj are forbidden. A non negative distance dij and travel time
tij are associated with every arc (i, j) ∈E. We assume that distances satisfy
the triangle inequalities. A solution of VRPTW is composed of multiple routes,
each one must respect the following constraints: 1) Each customer is only served
once; 2) A vehicle k leaves each visited customer and travels toward the next one
on the route and each route starts and ends at the depot; 3) The total demand
on any route does not exceed Q; 4) Finally, the service at each customer must
start within his time window and vehicles must return to the depot before l0.
A formal description of those constraints can be found in [9]. The objective (1)
minimizes the overall operating costs:
min

M.

k∈K

j∈Vc
x0jk +

k∈K

(i,j)∈E
dijxijk

(1)
where xijk is the decision variable. It is equal to 1 if vehicle k traverses arc
(i, j) and 0 otherwise. In (1), due to M being large, the cost of vehicles is ﬁrst
minimized then the overall traversed distance is reduced. This means that a
solution with fewer routes but higher distance is preferred over another one with
more routes but lower distance.
2.2
Belief Function Theory
Belief function theory was ﬁrst introduced by Shafer [16]. In this theory, the
available knowledge about a variable x deﬁned on a ﬁnite set X, known as the
frame of discernment, is represented by a mass function mX : 2X →[0, 1] s.t.

A⊆X mX (A) = 1 and mX (∅) = 0. mX (A) quantiﬁes the part of our belief
that x ∈A without providing any further information about x ∈A′ ⊂A. Each
subset A ⊆X such that mX (A) > 0 is called focal element of mX . If all the
focal elements A ⊆X of mX are singletons (|A| = 1) then mX is called Bayesian
and it is equivalent to a probability measure. If mX has a unique focal element
A ⊆X, i.e., mX (A) = 1, mX is said to be categorical and it corresponds to
a set. In the remainder of this paper, a variable x whose true value is known
in the form of a mass function will be referred to as evidential variable. The
notion of expected value, in probability theory, of a function f : X →R related
to a probability mass function pX is extended, in belief function theory, to the
notions of lower (E(f, mX )) and upper (E(f, mX )) expected values of f related
to a mass function mX as follows [2]:

384
T. Tedjini et al.
E(f, mX ) =

A⊆X
mX (A) min
x∈A f(x),
(2)
E(f, mX ) =

A⊆X
mX (A) max
x∈A f(x).
(3)
3
A Recourse Model for the VRPTW with Evidential
Times
In this section, we present the VRPTW with evidential service and travel times
under the recourse programming approach. Our model is inspired from the work
in [7] and adapted to account for uncertainty in both service and travel times, the
time windows requirements as well as a diﬀerent recourse policy. We will start
by formalizing the problem and by showing how to incorporate and compute the
expected cost of recourse actions. Then, we will present an eﬃcient method to
compute these costs in presence of particular evidential information about service
and travel times. Finally, we will discuss some special cases of the problem.
3.1
Formalization
Consider a solution to the VRPTW where time windows constraints are relaxed.
Let R = {1, . . . , i, . . . , n, n + 1 = 0} be a route from this solution. For the
sake of simplicity we suppose, without loss of generality, that the ith visit on
R corresponds to customer i of the problem. To account for the time windows
constraints, one must check the feasibility of R at each of its visits: If the time
needed for a vehicle to arrive at visit i (customer or depot) meets its correspond-
ing time window [ei, li] then the time window constraint is veriﬁed for i. In case
where the vehicle arrives later than li, a failure occurs and i can not be served,
so the vehicle skips the service of i and travels directly toward visit i + 1 the
immediate successor of i on R. An exclusive visit will be rescheduled later to
serve i. We adopt this policy since the actual values of service and travel times
are only revealed once the vehicle arrives at the visit’s location, thus failures can
not be predicted. This is equivalent to the strategy used in [6] for the stochas-
tic case. Formally, let fi be a binary variable describing the failure situation at
visit i. fi equals 1 if the vehicle arrives later than li otherwise it is equal to 0.
Arrival times are computed recursively by cumulating service and travel times
up to visit i with possible truncations induced by waiting times. Formally, let
ai, i = 1, . . . , n + 1 be the arrival time at visit i. ai is a function of the set
T01 × S1 × . . . × Si−1 × Ti−1i of all service and travel times up to i, where Si
(resp. Ti−1i) is the set on which the service time si (resp. travel time ti−1i) is
deﬁned:
ai =

vi−1 + ti−1i
if i ∈{2, . . . , n + 1},
t01
if i = 1.
(4)

VRPTW with Evidential Service and Travel Times: A Recourse Model
385
where vi is the departure time from visit i which is expressed by:
vi =

max{ei, ai} + si
if ai ≤li,
ai
if ai > li.
(5)
The failure situation along R is represented by a unique vector f
=
(f1, . . . , fn+1) ∈F = {0, 1}n+1 in presence of a certain and precise (deter-
ministic) knowledge about service and travel times. This vector is a function of
all service and travel times on R:
g : T01 × S1 × . . . × Sn × Tnn+1 →F
(t01, s1, . . . , sn, tnn+1) →f = (f1, . . . , fn+1).
(6)
Each rescheduled visit induces a penalty that must be added to the routing
cost of R. A penalty is an extra cost that measures, for instance, customers
dissatisfaction or simply the cost of dedicating exclusive vehicles to serve them.
Note that it is possible to deﬁne more complex recourse policies, nonetheless,
we chose this simple policy to somehow manage the tractability of the problem
given the complexity of belief functions. Let f = (f1, . . . , fn+1) ∈F be a failure
vector and let p be the function deﬁned from F to R+, representing the penalty
induced by f. This penalty is composed of the cost of using an extra vehicle to
serve visit i, whose service has been skipped, plus the total distance of a round
trip from i to the depot:
p(f) =
n+1

i=1
fi × (M + 2d0i).
(7)
The cost C(R) of a route R in presence of a deterministic failure vector f is
composed of the routing cost C(R) plus the penalty p(f) induced by f:
C(R) = C(R) + p(f)
= M +

(i,j)∈E(R)
dij +
n+1

i=1
fi × (M + 2d0i).
(8)
When service and travel times are evidential, i.e., knowledge about these
variables is represented by mass functions, the failure situation is no longer
deterministic but evidential and it is represented by a mass function mF as will
be detailed in Sect. 3.2. In this case, the cost C(R) of a route R consists of the
routing cost part C(R) to which is added the (lower or upper) expected value
of p related to mF as recalled in Sect. 2.2. In this work, we assume a pessimistic
attitude toward the possible failures that may occur on route R, and therefore
choose to use the upper expectation (3). We denote this upper expected value
by E(p, mF):
E(p, mF) =

F ⊆F
mF(F) × max
f∈F p(f).
(9)

386
T. Tedjini et al.
The cost C(R) of a route R is then expressed as:
C(R) = C(R) + E(p, mF)
= M +

(i,j)∈E(R)
dij + E(p, mF).
(10)
Consequently, the overall cost C(S) of a solution S = {R1, . . . , RK} of the
VRPTW with evidential service and travel times under the recourse approach,
is nothing but the sum of all the expected costs of its K routes.
C(S) =

k∈K
C(Rk).
(11)
3.2
Evidential Failures
In this section, we start by describing how evidential knowledge about service and
travel times induces evidential failures that are represented by a mass function.
Then, we present an eﬃcient method to compute this mass function under some
particular assumptions.
Let us ﬁrst consider the route R deﬁned in Sect. 3.1 and suppose that
service si and travel tij times are evidential and represented, respectively,
by mass functions mSi
si and mTij
tij deﬁned on the sets Si and Tij, such that
|Si| ≤S ∈N∗
+ and |Tij| ≤T ∈N∗
+. Assume that service and travel time
variables are independent (this assumption is not required but only stated here
to simplify the explanation of our method), hence, the joint mass function
mT01×S1×...×Sn×Tnn+1 of service and travel times on R is equal to the product of
independent mass functions mSi
si and mTij
tij , for all i ∈Vc(R) and (i, j) ∈E(R).
When mT01×S1×...×Sn×Tnn+1 has a unique focal set T ⊆T01×S1×. . .×Sn×Tnn+1
such that mT01×S1×...×Sn×Tnn+1(T) = 1, knowledge about the failure situation is
imprecise, i.e., all we know is that it belongs to a set of possible failures denoted
by F, which is the image of the set T by function g deﬁned in (6). F can be
described as:
F = g(T) =

(t01,s1,...,sn,tnn+1)∈T
g(t01, s1, . . . , sn, tnn+1).
(12)
In general, when mT01×S1×...×Sn×Tnn+1 has at most c ∈N∗
+ focal sets, knowl-
edge about the failure situation is both imprecise and uncertain, and can be
represented by the mass function mF deﬁned as:
mF(F) =

g(T )=F
mT01×S1×...×Sn×Tnn+1(T), ∀F ∈F.
(13)
To compute mF deﬁned in (13), g(T) is evaluated for each focal set T ⊆
T01 × S1 × . . . × Sn × Tnn+1 of the mass function mT01×S1×...×Sn×Tnn+1. In the
worst case, T can have up to Sn × T n+1 element (|T| = |Si||V (R)| × |Tij||E(R)|

VRPTW with Evidential Service and Travel Times: A Recourse Model
387
≤S|V (R)| × T |E(R)| ≤Sn × T n+1), thus evaluating (13) requires a worst case
time complexity of O([Sn × T n+1] × c) which is intractable. However, following
a similar tree-based approach to that of [7, Section 3.2.3] described in the next
paragraph, if service and travel time variables are modeled by intervals, i.e.,
si ∈σi = [σi, σi] ⊆Si and tij ∈θij = [θij, θij] ⊆Tij and T = θ01 ×
σ1 . . . × σn × θnn+1 is the Cartesian product of interval service and travel
times along R, one can demonstrate that only the bounds of the intervals σi, i ∈
V (R) and θij, (i, j) ∈E(R) has to be evaluated rather than each element of Si
and Tij. Subsequently, the complexity of evaluating (13) drops to O(22n+1 × c).
Furthermore, this complexity can also be decreased to O(2n × c) if travel times
tij are considered as singletons rather than intervals, that is the marginalization
of mT01×S1×...×Sn×Tnn+1 onto Tij, for any travel time variable tij is a Bayesian
mass function where θij = {˜θij}. As a consequence T = {˜θ01} × σ1 . . . ×
σn × {˜θnn+1}. A similar result holds when service times are singletons, that is
σi = {˜σi} and T = θ01 × {˜σ1} . . . × {˜σn} × θnn+1, in which case the overall
complexity becomes O(2n+1 × c).
Consider, in the following, a categorical mass function mT01×S1×...×Sn×Tnn+1
having a unique focal set T = θ01 × σ1 . . . × σn × θnn+1, that is, si ∈
σi, ∀i ∈Vc and tij ∈θij, ∀(i, j) ∈E. Note that arrival times (resp. departure
times) are imprecise in this case: ai ∈αi = [αi, αi] (resp. vi ∈υi = [υi, υi]),
where bounds αi and αi (resp. υi and υi) are obtained by applying the recursive
reasoning described below. As mentioned before, in this case, the failure situation
that can be encountered by a vehicle is described by a set F deﬁned by (12).
In the following, we adapt the evaluation procedure in [7, Section 3.2.3] to our
problem in order to eﬃciently compute F. Suppose that the vehicle operating
on R travels from visit i −1 to visit i. The arrival time ai at visit i induces the
following three cases:
1. If αi ≤li: there will be no failure at visit i, hence fi = 0 and vi ∈
[max{ei, αi} + σi, max{ei, αi} + σi].
2. If αi > li: the failure situation is “precise” and service at visit i is skipped
i.e., fi = 1 and vi ∈[αi, αi].
3. if αi ≤li < αi: the failure situation at visit i is “imprecise”, so we account
for both possibilities:
(a) When the true arrival time belongs to [αi, li], there will be no failure at
visit i, i.e., fi = 0 and vi ∈[max{ei, αi} + σi, li + σi].
(b) When the true arrival time belongs to ]li, αi], service at visit i is skipped,
i.e., fi = 1 and vi ∈]li, αi].
This reasoning is applied for all the visits of R, starting from the ﬁrst cus-
tomer up to the depot, to get all the failure situations related to the focal set
T. This latter procedure is sketched in Algorithm 1 and its execution induces
a tree of n + 1 levels. Each level i represents the potential failure situations
that a vehicle encounters when reaching visit i. Each node of a level i contains
information about the failure component fi as well as the arrival and depar-
ture time intervals. The evaluation is extended from a level to another one, and

388
T. Tedjini et al.
stops after evaluating level n + 1 which corresponds to the depot. To guarantee
the feasibility of single customer routes, we assume that there is no particu-
lar reason for a vehicle to arrive late at the ﬁrst visit of any route R, thus
f1 = 0. Consider a branch from the tree induced by Algorithm 1. The concate-
nation of the binary failure variables fi from the root up to level n + 1 induces
a failure vector f = (f1, . . . , fn+1) describing one possible failure situation on
R. The set of all the vectors f obtained from all the branches of the recourse
tree, denoted by F, expresses all the failures that can occur on R. Using a
similar proof to that of [7, Proposition 6], we can show that the set F veriﬁes
F = g(θ01×σ1×. . .×σn×θnn+1). Note that in case mT01×S1×...×Sn×Tnn+1
has multiple focal sets, then given (13) the previous reasoning is applied for each
one of them as demonstrated in the following example:
Algorithm 1. Failures tree-based Evaluation (FE)
Require: Visit i ∈{1, . . . , n + 1}, service time [σi, σi], travel time [θij, θij].
Ensure: A tree describing all failures on R.
1: i = 1;
2: [α1, α1] = [θ01, θ01];
3: f1 = 0;
4: [υ1, υ1] = [max{e1, α1} + σ1, max{e1, α1} + σ1];
5: Create the root node of T ([α1, α1], [υ1, υ1], f1); i + +;
6: while (i > 1 and i ≤n + 1) do
7:
[αi, αi] = [υi−1 + θi−1i, υi−1 + θi−1i];
8:
if αi ≤li then
9:
f left
i
= 0;
10:
[υleft
i
, υleft
i
] = [max{ei, αi} + σi, max{ei, αi} + σi];
11:
T left = FE([αi, αi], [υleft
i
, υleft
i
], f left
i
, i + +);
12:
Attach T left as a left branch of T;
13:
else if αi > li then
14:
f right
i
= 1;
15:
[υright
i
, υright
i
] = [αi, αi];
16:
T right = FE([αi, αi], [υright
i
, υright
i
], f right
i
, i + +);
17:
Attach T right as a right branch of T;
18:
else
19:
[αi, αi] = [υi−1 + θi−1i, li];
20:
f left
i
= 0;
21:
[υleft
i
, υleft
i
] = [max{ei, αi} + σi, li + σi];
22:
T left = FE([αi, αi], [υleft
i
, υleft
i
], f left
i
, i + +);
23:
Attach T left as a left branch of T;
24:
[αi, αi] =]li, υi−1 + θi−1i];
25:
f right
i
= 1;
26:
[υright
i
, υright
i
] =]li, αi];
27:
T right = FE([αi, αi], [υright
i
, υright
i
], f right
i
, i + +);
28:
Attach T right as a right branch of T;
29:
end if
30: end while

VRPTW with Evidential Service and Travel Times: A Recourse Model
389
Example 1. Let us illustrate Algorithm 1 by considering a route R
=
{1, 2, 3, 0}, where time windows are [e0, l0] = [0, 200], [e1, l1] = [5, 30], [e2, l2] =
[40, 95], [e3, l3] = [80, 120]. The available information about service and travel
times is represented by the joint mass function: mT01×S1×T12×S2×T23×S3×T30
denoted by m for simpliﬁcation:
m([15, 20] × [20, 25] × [40, 50] × [30, 35] × [10, 20] × [15, 20] × [10, 15]) = 0.6,
m([15, 20] × [20, 25] × [25, 30] × [20, 30] × [10, 15] × [15, 20] × [10, 15]) = 0.4.
(14)
F = {(0, 0, 0, 0), (0, 1, 0, 0), (0, 0, 1, 0), (0, 0, 0, 1), (0, 1, 1, 0), (0, 1, 0 , 1), (0, 0, 1, 1),
(0, 1, 1, 1)} is the set of all possible failure situations on R, such that f1 = 0.
The ﬁrst focal set induces the tree illustrated in Fig. 1 and the second one
induces the tree depicted in Fig. 2. Take for instance the tree in Fig. 1, it has
two branches representing two possible failure situations on R when informa-
tion about service and travel times is given by the ﬁrst focal set of m. The
concatenation of the binary variables fi from the root to the leaves starting
from the left branch yields the subset F1 = {(f1, f2, f 1
3 , f 1
0 ), (f1, f2, f 2
3 , f 2
0 )} =
{(0, 0, 0, 0), (0, 0, 1, 0)}, which means that either all the customers of R will be
successfully served or that a failure will occur at customer 3 and the latter can not
be served. Similarly, the tree in Fig. 2 yields the subset F2 = {(f1, f2, f3, f0)} =
{(0, 0, 0, 0)} with respect to the second focal set of m, which means that no
failure will occur on R. Formally, using equation (13) with g being the function
given in (12) and mT01×S1×...×Sn×Tnn+1 deﬁned such as in (14), our knowledge
about the entire failure situation on R can be expressed via the mass function
mF deﬁned by:
mF(F1) = mF({(0, 0, 0, 0), (0, 0, 1, 0)}) = 0.6,
mF(F2) = mF({(0, 0, 0, 0)}) = 0.4.
(15)
Fig. 1. The failure tree related to the ﬁrst focal set of m.

390
T. Tedjini et al.
Fig. 2. The failure tree related to the second focal set of m.
3.3
Particular Cases
The recourse model degenerates into well known problems when the mass func-
tion mSn×T |E| representing knowledge about all service and travel time variables
of the problem has a special form. Speciﬁcally, if mSn×T |E| has a unique focal
set, minimizing the upper expected cost function (10) is equivalent to minimiz-
ing the recourse cost under the worst case scenario. Thus, our model reduces to
a robust approach. Note that it is also possible to replace the upper expected
penalty (9) by the lower expected one using Eq. (2) in case the decision maker
is more interested in optimistic solutions. Another interesting particular case
of the proposed model is when mSn×T |E| is Bayesian, i.e., its focal sets are no
longer sets but singletons. In this case, the lower and the upper expected costs
of the optimal solution reduce to the classical expectation in probability theory.
As a consequence, our model is equivalent to a SVRPTW with recourses. It is
important to mention that the two aforementioned models (i.e., the robust and
stochastic ones) are conceptually diﬀerent and it is not possible, at least trivially,
to convert one into another. The following example illustrates this remark.
Example 2. Consider a VRPTW instance where time windows are: [e0, l0] =
[0, 250], [e1, l1] = [5, 20], [e2, l2] = [40, 95], [e3, l3] = [80, 130] and Q = ∞, vehicles
unit cost is M = 1000 and distances are given by: d01 = d10 = 10, d02 = d20 = 10,
d03 = d30 = 15, d12 = d21 = 20, d13 = d31 = 15 and d23 = d32 = 20. Service
and travel times are imprecise and represented by the following categorical mass
function:
mT (

{60}×{20}×{20}×{10}×{20}×{15}×{25}×{30}×{15, 20}
	
) = 1 (16)
with T = S1 × S2 × S3 × T01 × T02 × T03 × T12 × T13 × T23.
We denote by S⋆
m the optimal solution to this instance and by C
∗
m its cost.
The set of feasible solutions related to this instance is given in Table 1. The
solution S5 is composed of a unique route serving customers 1, 2 and 3 with a
possible failure at customer 3 (highlighted in bold). From Table 1, we can deduce
that the optimal solution (highlighted with ⋆) is S⋆
m = S3 with a corresponding
cost C
⋆
m = 2060.

VRPTW with Evidential Service and Travel Times: A Recourse Model
391
Consider now another representation where service and travel times are ran-
dom and expressed by a probability distribution that is compatible with mT .
Denote by S⋆
p its related optimal solution and by C⋆
p its cost.
pT (60, 20, 20, 10, 20, 15, 25, 30, 15) = pT (60, 20, 20, 10, 20, 15, 25, 30, 20) = 1/2.
(17)
The set of feasible solutions related to pT is given in Table 2. The optimal solution
is S⋆
p = S5 with a cost of C
⋆
p = 1580. We clearly have S⋆
m ̸= S⋆
p and C
⋆
m ̸= C
⋆
p.
Table 1. The set of feasible solutions
related to the mass function mT .
Solution R1
R2
R3
Cost
S1
{0, 1, 0}
{0, 2, 0} {0, 3, 0} 3070
S2
{0, 1, 2, 0}
{0, 3, 0} –
2070
S⋆
3
{0, 1, 3, 0}
{0, 2, 0} –
2060
S4
{0, 2, 3, 0}
{0, 1, 0} –
2065
S5
{0, 1, 2, 3, 0} –
–
2095
Table 2. The set of feasible solutions
related to the probability pT .
Solution R1
R2
R3
Cost
S1
{0, 1, 0}
{0, 2, 0} {0, 3, 0} 3070
S2
{0, 1, 2, 0}
{0, 3, 0} –
2070
S3
{0, 1, 3, 0}
{0, 2, 0} –
2060
S4
{0, 2, 3, 0}
{0, 1, 0} –
2065
S⋆
5
{0, 1, 2, 3, 0} –
–
1580
Note that this remark also holds with respect to our evidential model. Indeed,
one can build similar examples showing that it is not possible, at least trivially,
to convert the evidential service and travel times into imprecise or probabilistic
ones while preserving the same solutions.
4
Experimental Results
To solve the VRPTW with evidential service and travel times under the recourse
approach, we propose to use a Memetic Algorithm (MA) [13]. This choice is moti-
vated by MA’s attractive results for VRPs and their variants [6,10,14]. Speciﬁ-
cally, the MA operates on an initial set (population) of individuals or solutions
that evolves by undergoing a series of modiﬁcations using some recombination
and mutation tools, to create a new population, similarly as in the natural evo-
lution process of individuals. The new population is then enhanced by exploring
the neighborhoods of its individuals using the so-called local search (LS) proce-
dures. This hybridization between population evolution and local search helps
to escape local optima and provide good-quality solutions. Formally, the MA
runs with a population POP of m solutions. POP is ﬁrst initialized and sorted
increasingly according to the cost of its solutions. At each iteration, two selected
parents P1 and P2 are crossed. The resulting oﬀspring child has a probability
pm to be mutated and a probability pls to be enhanced by a LS procedure. child
will join POP only if he improves it. The process is repeated until a stopping
condition is met, such as the number of iterations without improvement of POP.
Note that POP’s size must be constant over the iterations. We refer the reader to
[6,10,14] for further information about selection, crossover and mutation tools.
The pseudo-code of the MA is presented in Algorithm 2.

392
T. Tedjini et al.
Algorithm 2. MA structure
Require: POP a population of size m.
Ensure: SP OP [0], the best solution to the problem.
1: Initialize POP ;
2: Evaluate each individual in POP and keep POP sorted;
3: while (! stopping condition) do
4:
select two parents P1 and P2;
5:
child ←cross(P1, P2);
6:
if (p ∼U(0, 1) ≤pm) then
7:
child ←mutate(child);
8:
else if (p ∼U(0, 1) ≤pls) then
9:
child ←LS(child);
10:
end if
11:
if (C(Schild) ≤C(SP OP [m−1])) then
12:
if (∄i |C(SP OP [i]) = C(Schild)) then
13:
remove POP[m −1] from POP;
14:
update stopping condition;
15:
else
16:
update stopping condition;
17:
end if
18:
insert or replace child in POP;
19:
else
20:
update stopping condition;
21:
end if
22: end while
We have adapted Solomon’s instances [17] to account for the evidential nature
of service and travel times. These instances are composed of three sets: 25, 50 and
100 indicating the number of customers. Each set has 56 instances divided into
six categories C1, R1, RC1, C2, R2 and RC2. Customers’ positions in categories
R1 and R2 are randomly generated meanwhile in C1 and C2, positions are
clustered. Categories RC1 and RC2 gather both random and clustered positions.
Another classiﬁcation can be established according to the time windows nature.
Categories C1, R1 and RC1 (type 1) have tight time windows while categories
C2, R2 and RC2 (type 2) have larger time windows. We kept the same data as
in the original instances, except for service and travel times which are adapted
to the belief function framework as follows:
mSi([sdet
i
, sdet
i
]) = 0.8, mSi([sdet
i
, sdet
i
+ σi]) = 0.2
(18)
where sdet
i
is the (deterministic) value of si in Solomon’s instances and σi a
random parameter generated from the interval [1, 10], and:
mTij([tdet
ij , tdet
ij ]) = 0.8, mTij([tdet
ij , tdet
ij + θij]) = 0.2
(19)
with tdet
ij
the deterministic value of tij and θij a random parameter generated
from the interval [1, 20]. The vehicle cost M is set to 1000 as in [6]. This large
value prioritizes the optimization of the number of vehicles followed by the total

VRPTW with Evidential Service and Travel Times: A Recourse Model
393
distance and the penalties. The MA performs 15 tests for each instance of the
problem and stops after n2 iterations without improvement, with n being the
number of customers per instance. The population size is set to n. The LS pro-
cedure runs for n iterations with a probability of 0.2 whereas, the probability of
mutation is ﬁxed to 0.01. Explicit details about the initialization algorithm, the
LS procedure as well as the parameters setting are not provided here because of
the space limit. Table 3 displays average results per category for both instances
of 50 and 100 customers. We recorded, for each category, the average number
of vehicles #V , the average traversed distance Dist, the average total penalties
Pen as well as the average execution time (CPU(s)) in seconds. The conducted
experiments show that the overall costs are quite acceptable given the execution
time. We notice that instances of type 1 have higher costs, either in terms of #V ,
Dist or Pen, than those of type 2 (we clearly see that in classes R1 and RC1).
This increase is natural and justiﬁed by the fact that these particular instances
have tighter time windows, hence solutions tend to require many vehicles and
they are also subject to many failures. Another remark can be established regard-
ing the execution time. We note that CPU time is quite pronounced in type 2
instances, speciﬁcally, when the number of customers is large, this is related to
the fact that this class has larger time windows, consequently, the search space
is larger and requires further exploration than its counterpart of type 1.
Table 3. Average results per category
Category 50 customers
100 customers
#V
Dist
Pen
CPU(s) #V
Dist
Pen
CPU(s)
C1
6.16
490.16
9.49
6.11
12.04 1128.81
5.06
134.10
C2
2.78
436.78
1.79
16.99
4.67
759.42
0.00
326.26
R1
11.88 1164.98 751.71
15.13
22.31 2054.62 1578.82
252.28
R2
3.24
784.21
66.96
78.88
5.80 1246.85
165.32 2000.22
RC1
10.41 1080.51 601.26
13.21
21.55 2216.88 1063.47
233.30
RC2
3.70
770.51
91.90 112.40
6.58 1449.87
188.59 1431.39
5
Conclusions
We proposed a recourse approach for the VRPTW with evidential service and
travel times. The model is inspired from [7] and adapted to take account of
the uncertainty on both service and travel times as well as the additional hard
requirements on time windows. A skipping-based policy was used to recover the
feasibility of routes. Our model reduces to robust and stochastic cases when
the mass functions of service and travel times are, respectively, categorical and
Bayesian and allows, beyond sole imprecision or probabilistic uncertainty, to
model more complex information that are aﬀected by other forms of imper-
fections. We devised a memetic algorithm to solve an adaptation of litera-
ture instances. Combining both the Belief-Constrained Programming and the

394
T. Tedjini et al.
recourse approaches in one model as in SVRPTW [6] is an interesting and chal-
lenging perspective.
References
1. Agra, A., Christiansen, M., Figueiredo, R., Hvattum, K.L., Poss, M., Requejo, C.:
The robust vehicle routing problem with time windows. Comput. Oper. Res. 40,
856–866 (2013)
2. Denoeux, T.: Analysis of evidence-theoretic decision rules for pattern classiﬁcation.
Patt. Recogn. 30(7), 1095–1107 (1997)
3. Ehmke, J.F., Campbell, A.M., Urban, T.L.: Ensuring service levels in routing prob-
lems with time windows and stochastic travel times. Eur. J. Oper. Res. 240(2),
539–550 (2015)
4. Errico,F.,Desaulniers,G.,Gendreau,M.,Rei,W.,Rousseau,L.-M.:Thevehiclerout-
ing problem with hard time windows and stochastic service times. EURO J. Transp.
Logistics 7(3), 223–251 (2016). https://doi.org/10.1007/s13676-016-0101-4
5. Errico, F., Desaulniers, G., Gendreau, M., Rei, W., Rousseau, L.: A priori opti-
mization with recourse for the vehicle routing problem with hard time windows
and stochastic service times. Eur. J. Oper. Res. 249(1), 55–66 (2016)
6. Gutierrez, A., Dieulle, L., Labadie, N., Velasco, N.: A multi-population algorithm
to solve the VRP with stochastic service and travel times. Comput. Ind. Eng. 125,
144–156 (2018)
7. Helal, N., Pichon, F., Porumbel, D., Mercier, D., Lefevre, E.: The capacitated
vehicle routing problem with evidential demands. Int. J. Approx. Reason. 95, 124–
151 (2018)
8. Hu, C., Lu, J., Liu, X., Zhang, G.: Robust vehicle routing problem with hard
time windows under demand and travel time uncertainty. Comput. Oper. Res. 94,
139–153 (2018)
9. Kallehauge, B.: Formulations and exact algorithms for the vehicle routing problem
with time windows. Comput. Oper. Res. 35(7), 2307–2330 (2008)
10. Labadi, N., Prins, C., Reghioui, M.: A memetic algorithm for the vehicle routing
problem with time windows. Rairo Oper. Res. 42, 415–431 (2008)
11. Li, X., Tian, P., Leung, S.C.: Vehicle routing problems with time windows and
stochastic travel and service times: models and algorithm. Int. J. Prod. Econ.
125(1), 137–145 (2010)
12. Lu, D., Gzara, F.: The robust vehicle routing problem with time windows: solution
by branch and price and cut. Eur. J. Oper. Res. 275(3), 925–938 (2019)
13. Moscato, P., Cotta, C.: A gentle introduction to memetic algorithms. In: Glover, F.,
Kochenberger, G. (eds.) Handbook of Metaheuristics, ISOR, vol. 57, pp. 105–144.
Springer, Boston, MA (2003). https://doi.org/10.1007/0-306-48056-5 5
14. Prins, C.: A simple and eﬀective evolutionary algorithm for the vehicle routing
problem. Comput. Oper. Res. 31(12), 1985–2002 (2004)
15. Savelsbergh, M.: Local search in routing problems with time windows. Ann. Oper.
Res. 4(1), 295–305 (1985)
16. Shafer, G.: A Mathematical Theory of Evidence. Princeton University Press,
Princeton (1976)
17. Solomon, M.: Algorithms for the vehicle routing and scheduling problem with time
window constraints. Oper. Res. 35(2), 254–265 (1987)

VRPTW with Evidential Service and Travel Times: A Recourse Model
395
18. Ta¸s, D., Dellaert, N., van Woensel, T., de Kok, T.: Vehicle routing problem with
stochastic travel times including soft time windows and service costs. Comput.
Oper. Res. 40(1), 214–224 (2013)
19. Tedjini, T., Aﬁﬁ, S., Pichon, F., Lefevre, E.: A belief-constrained programming
model for the VRPTW with evidential service and travel times. In: Proceedings of
28es rencontres francophones sur la Logique Floue et ses Applications, pp. 217–224.
Al`es, France (2019)
20. Wang, X., Regan, A.C.: Assignment models for local truckload trucking prob-
lems with stochastic service times and time window constraints. Transp. Res. Rec.
1771(1), 61–68 (2001)

Imprecise Probability

A New Score for Adaptive Tests in
Bayesian and Credal Networks
Alessandro Antonucci(B), Francesca Mangili, Claudio Bonesana,
and Giorgia Adorni
Istituto Dalle Molle di Studi sull’Intelligenza Artiﬁciale, Lugano, Switzerland
{alessandro,francesca,claudio.bonesana,giorgia.adorni}@idsia.ch
Abstract. A test is adaptive when the sequence and number of ques-
tions is dynamically tuned on the basis of the estimated skills of the taker.
Graphical models, such as Bayesian networks, are used for adaptive tests
as they allow to model the uncertainty about the questions and the skills
in an explainable fashion, especially when coping with multiple skills. A
better elicitation of the uncertainty in the question/skills relations can be
achieved by interval probabilities. This turns the model into a credal net-
work, thus increasing the inferential complexity of the queries required to
select questions. This is especially the case for the information-theoretic
quantities used as scores to drive the adaptive mechanism. We present
an alternative family of scores, based on the mode of the posterior proba-
bilities, and hence easier to explain. This makes considerably simpler the
evaluation in the credal case, without signiﬁcantly aﬀecting the quality
of the adaptive process. Numerical tests on synthetic and real-world data
are used to support this claim.
Keywords: Adaptive tests · Information theory · Credal networks ·
Bayesian networks · Index of qualitative variation
1
Introduction
A test or an exam can be naturally intended as a measurement process, with the
questions acting as sensors measuring the skills of the test taker in a particular
discipline. Such measurement is typically imperfect with the skills modeled as
latent variables whose actual values cannot be revealed in a perfectly reliable
way. The role of the questions, whose answers are regarded instead as mani-
fest variables, is to reduce the uncertainty about the latent skills. Following this
perspective, probabilistic models are an obvious framework to describe tests.
Consider for instance the example in Fig. 1, where a Bayesian network evaluates
the probability that the test taker knows how to multiply integers. In such frame-
work making the test adaptive, i.e., picking a next question on the basis of the
G. Adorni—We thank the Swiss National Science Foundation (grant no. 187246) for
support
c
⃝Springer Nature Switzerland AG 2021
J. Vejnarov´a and N. Wilson (Eds.): ECSQARU 2021, LNAI 12897, pp. 399–412, 2021.
https://doi.org/10.1007/978-3-030-86772-0_29

400
A. Antonucci et al.
current knowledge level of the test taker is also very natural. The information
gain for the available questions might be used to select the question leading to
the more informative results (e.g., according to Table 1, Q1 is more informative
than Q2 no matter what the answer is). This might also be done before the
answer on the basis of expectations over the possible alternatives.
A critical point when coping with such approaches is to provide a realistic
assessment for the probabilistic parameters associated with the modeling of the
relations between the questions and the skills. Having to provide sharp numerical
values for these probabilities might be diﬃcult. As the skill is a latent quantity,
complete data are not available for a statistical learning and a direct elicitation
should be provided by experts (e.g., a teacher). Yet, it might be not obvious to
express such a domain knowledge by single numbers and a more robust elicita-
tion, such as a probability interval (e.g., P(Q1 = 1|S1 = 1) ∈[0.85, 0.95]), might
add realism and robustness to the modeling process [15]. With such generalized
assessments of the parameters a Bayesian network simply becomes a credal net-
work [22]. The counterpart of such increased realism is the higher computational
complexity of inference in credal networks [21]. This is an issue especially when
coping with information-theoretic measures such as the information gain, whose
computation in credal networks might lead to complex non-linear optimization
tasks [19].
The goal of this paper is to investigate the potential of alternatives to the
information-theoretic scores driving the question selection in adaptive tests based
on directed graphical models, no matter whether these are Bayesian or credal
networks. In particular, we consider a family of scores based on the (expected)
mode of the posterior distributions over the skills. We show that, when cop-
ing with credal networks, the computation of these scores can be reduced to
a sequence of linear programming task. Moreover, we show that these scores
beneﬁt of better explainability properties, thus allowing for a more transparent
process in the question selection.
Fig. 1. A Bayesian network over Boolean variables modeling a simple test to evaluate
integer multiplication skill. Probabilities of correct answers are also depicted.
The paper is organized as follows. A critical discussion about the existing
work in this area is in Sect. 2. The necessary background material is reviewed in
Sect. 3. The adaptive testing concepts are introduced in Sect. 4 and specialized
to graphical models in 5. The technical part of the paper is in Sect. 6, where the
new scores are discussed and specialized to the credal case, while the experiments
are in Sect. 7. Conclusions and outlooks are in Sect. 8.

A New Score for Adaptive Tests in Bayesian and Credal Networks
401
Table 1. Posterior probabilities of the skill after one or two questions in the test
based on the Bayesian network in Fig. 1. A uniform prior over the skill is considered.
Probabilities are regarded as grades and sorted from the lowest one. Bounds obtained
with a perturbation ϵ = ±0.05 of all the input parameters are also reported.
Q1 Q2 P(S = 1|q1, q2) P(S = 1|q1, q2) P(S = 1|q1, q2)
0
0
0.087
0.028
0.187
0
−
0.125
0.052
0.220
0
1
0.176
0.092
0.256
−
0
0.400
0.306
0.506
−
1
0.600
0.599
0.603
1
0
0.667
0.626
0.708
1
−
0.750
0.748
0.757
1
1
0.818
0.784
0.852
2
Related Work
Tests are modeled as a process relating latent and manifest variables since the
classical item response theory (IRT), that has been widely used even to imple-
ment adaptive sequences [14]. Despite its success related to the ease of implemen-
tation and inference, IRT might be inadequate when coping with multiple latent
skills, especially when these are dependent. This moved researchers towards the
area of probabilistic graphical models [17], as practical tools to implement IRT
in more complex setups [2]. Eventually, Bayesian networks have been identi-
ﬁed as a suitable formalism to model tests, even behind the IRT framework
[25], this being especially the case for adaptive models [26] and coached solving
[12]. In order to cope with latent skills, some authors successfully adopted EM
approaches to these models [23], this also involving the extreme situation of no
ground truth information about the answers [6]. As an alternative approach to
the same issue, some authors considered relaxations of the Bayesian formalism,
such as fuzzy models [7] and imprecise probabilities [19]. The latter is the direc-
tion we consider here, but trying to overcome the computational limitations of
that approach when coping with information-theoretic scores. This has some
analogy with the approach in [11], that is focused on the Bayesian case only, but
whose score, based on the same-decision problem, appears hard to be extended
to the imprecise framework without increasing the computational complexity.
3
Background on Bayesian and Credal Networks
We denote variables by Latin uppercase letters, while using lowercase for their
generic values, and calligraphic for the set of their possible values. Thus, v ∈V
is a possible value of V . Here we only consider discrete variables.1
1 IRT uses instead continuous skills. Yet, with probabilistic models, discrete skills do
not prevent evaluations to range over continuous domains. E.g., see Table 1, where
the grade corresponds to a (continuous) probability.

402
A. Antonucci et al.
3.1
Bayesian Networks
A probability mass function (PMF) over V is denoted as P(V ), while P(v) is
the probability assigned to state v. Given a function f of V , its expectation with
respect to P(V ) is EP (f) := 
v∈V P(v)f(v). The expectation of −logb[P(V )]
is called entropy and denoted also as H(V ). In particular we assume b := |V| to
have the maximum of the entropy, achieved for uniform PMFs, equal to one.
Given a joint PMF P(U, V ), the marginal PMF P(V ) is obtained by sum-
ming out the other variable, i.e., P(v) = 
u∈U P(u, v). Conditional PMFs such
as P(U|v) are similarly obtained by Bayes’s rule, i.e., P(u|v) = P(u, v)/P(v)
provided that P(v) > 0. The notation P(U|V ) := {P(U|v)}v∈V is used for
such a conditional probability table (CPT). The entropy of a conditional PMF
is deﬁned as in the unconditional case and denoted as H(U|v). The condi-
tional entropy is a weighted average of entropies of the conditional PMFs, i.e.,
H(U|V ) := 
v∈V H(U|v)P(v). If P(u, v) = P(u)P(v) for each u ∈U and v ∈V,
variables U and V are independent. Conditional formulations are also considered.
We assume the set of variables V := (V1, . . . , Vr) to be in one-to-one corre-
spondence with a directed acyclic graph G. For each V ∈V , the parents of V ,
i.e., the predecessors of V in G, are denoted as PaV . The graph G together with
the collection of CPTs {P(V |PaV )}V ∈V provides a Bayesian network (BN) spec-
iﬁcation [17]. Under the Markov condition, i.e., every variable is conditionally
independent of its non-descendants non-parents given its parents, a BN com-
pactly deﬁnes a joint PMF P(V ) that factorizes as P(v) = 
V ∈V P(v|paV ).
Inference, intended as the computation of the posterior PMF of a single
(queried) variable given some evidence about other variables, is in general NP-
hard, but exact and approximate schemes are available (see [17] for details).
3.2
Credal Sets and Credal Networks
A set of PMFs over V is denoted as K(V ) and called credal set (CS). Expec-
tations based on CSs are the bounds of the PMF expectations with respect
to the CS. Thus E[f] := infP (V )∈K(V ) E[f] and similarly for the supremum E.
Expectations of events are in particular called lower and upper probabilities and
denoted as P and P. Notation K(U|v) is used for a set of conditional CSs, while
K(U|V ) := {K(U|v)}v∈V is a credal CPT (CCPT).
Analogously to a BN, a credal network (CN) is speciﬁed by graph G together
with a family of CCPTs {K(V |PaV )}V ∈V [13]. A CN deﬁnes a joint CS K(V )
corresponding to all the joint PMFs induced by BNs whose CPTs are consistent
with the CN CCPTs.
For CNs, we intend inference as the computation of the lower and upper
posterior probabilities. The task generalizes BN inference being therefore NP-
hard, see [21] for a deeper characterization. Yet, exact and approximate schemes
are also available to practically compute inferences [4,5,16].

A New Score for Adaptive Tests in Bayesian and Credal Networks
403
4
Testing Algorithms
A typical test aims at evaluating the knowledge level of a test taker σ on the basis
of her answers to a number of questions. Let Q denote a repository of questions
available to the instructor. The order and the number of questions picked from
Q to be asked to σ might not be deﬁned in advance. We call testing algorithm
(TA) a procedure taking care of the selection of the sequence of questions asked
to the test taker, and deciding when the test stops. Algorithm 1 depicts a general
TA scheme, with e denoting the array of the answers collected from taker σ.
Algorithm 1. General TA: given the proﬁle σ and repository Q, an evaluation
based on answers e is returned.
1: e ←∅
2: while not Stopping(e) do
3:
Q∗←Pick(Q, e)
4:
q∗←Answer(Q∗, σ)
5:
e ←e ∪{Q∗= q∗}
6:
Q ←Q \ {Q∗}
7: end while
8: return Evaluate(e)
The Boolean function Stopping decides whether the test should end, this
choice being possibly based on the previous answers in e. Trivial stopping
rules might be based on the number of questions asked to the test taker
(Stopping(e) = 1 if and only if |e| > n) or on the number of correct answers
provided that a maximum number of questions is not exceeded. Function Pick
selects instead the question to be asked to the student from the repository Q. A
TA is called adaptive when this function takes into account the previous answers
e. Trivial non-adaptive strategies might consist in randomly picking an element
of Q or following a ﬁxed order. The function Answer is simply collecting (or simu-
lating) the answer of test taker σ to a particular question Q. In our assumptions,
this answer is independent of the previous answers to other questions.2
Finally, Evaluate is a function returning the overall judgment of the test
(e.g., a numerical grade or a pass/fail Boolean) on the basis of all the answers
collected after the test termination. Trivial examples of such functions are the
percentage of correct answers or a Boolean that is true when a suﬃcient number
of correct answers has been provided. Note also that in our assumptions the TA
is exchangeable, i.e., the stopping rule, the question ﬁnder and the evaluation
function are invariant with respect to permutations in e [24]. In other words,
2 Generalized setups where the quality of the student answer is aﬀected by the previous
answers will be discussed at the end of the paper. This might include a fatigue
model negatively aﬀecting the quality of the answers when many questions have
been already answered as well as the presence of revealing questions that might
improve the quality of other answers [18].

404
A. Antonucci et al.
the same next question, the same evaluation and the same stopping decision is
produced for any two students who provided the same list of answers in two
diﬀerent orders.
A TA is supposed to achieve reliable evaluation of taker σ from the answers
e. As each answer is individually assumed to improve such quality, asking all the
questions, no matter the order because of the exchangeability assumption, is an
obvious choice. Yet, this might be impractical (e.g., because of time limitations)
or just provide an unnecessary burden to the test taker. The goal of a good TA
is therefore to trade oﬀthe evaluation accuracy and the number of questions.3
5
Adaptive Testing in Bayesian and Credal Networks
The general TA setup in Algorithm 1 can be easily specialized to BNs as fol-
lows. First, we identify the proﬁle σ of the test taker with the actual states of a
number of latent discrete variables, called skills. Let S = {Sj}m
j=1 denote these
skill variables, and sσ the actual values of the skills for the taker. Skills are
typically ordinal variables, whose states correspond to increasing knowledge lev-
els. Questions in Q are still described as manifest variables whose actual values
are returned by the Answer function. This is achieved by a (possibly stochas-
tic) function of the actual proﬁle sσ. This reﬂects the taker perspective, while
the teacher has clearly no access to sσ. As a remark, note that we might often
coarsen the set of possible values Q for each Q ∈Q: for instance, a multiple
choice question with three options might have a single right answer, the two
other answers being indistinguishable from the evaluation point of view.4
A joint PMF over the skills S and the questions Q is supposed to be avail-
able. In particular we assume this to correspond to a BN whose graph has the
questions as leaf nodes. Thus, for each Q ∈Q, PaQ ⊆S and we call PaQ the
scope of question Q. Note that this assumption about the graph is simply reﬂect-
ing a statement about the conditional independence between (the answer to) a
question and all the other skills and questions given scope of the question. This
basically means that the answers to other questions are not directly aﬀecting
the answer to a particular question.5
As the available data are typically incomplete because of the latent nature
of the skills, dedicated learning strategies, such as various forms of constrained
EM should be considered to train a BN from data. We refer the reader to the
various contributions of Plajner and Vomlel in this ﬁeld (e.g., [23]) for a complete
discussion of that approach. Here we assume the BN quantiﬁcation available.
3 In some generalized setups, other elements such as a serendipity in choice in order
to avoid tedious sequences of questions might be also considered [8].
4 The case of abstention to an answer and the consequent problem of modeling the
incompleteness is a topic we do not consider here for the sake of conciseness. Yet,
general approaches based on the ideas in [20] could be easily adopted.
5 Moving to other setups would not be really critical because of the separation prop-
erties of observed nodes in Bayesian and credal networks, see for instance [3,9].

A New Score for Adaptive Tests in Bayesian and Credal Networks
405
In such a BN framework, Stopping(e) might be naturally based on an eval-
uation of the posterior PMF P(S|e), this being also the case for Evaluate.
Regarding the question selection, Pick might be similarly based on the (pos-
terior) CPT P(S|Q, e), whose values for the diﬀerent answers to Q might be
weighted by the marginal P(Q|e). More speciﬁcally, entropies and conditional
entropies are considered by Algorithm 2, while the evaluation is based on a
conditional expectation for a given utility function.
Algorithm 2. Information Theoretic TA in a BN over the questions Q and
the skills S: given the student proﬁle sσ, the algorithm returns an evaluation
corresponding to the expectation of an evaluation function f with respect to the
posterior for the skills given the answers e.
1: e = ∅
2: while H(S|e) > H∗do
3:
Q∗←arg maxQ∈Q [H(S|e) −H(S|Q, e)]
4:
q∗←Answer(Q∗, sσ)
5:
e ←e ∪{Q∗= q∗}
6:
Q ←Q \ {Q∗}
7: end while
8: return EP (S |e)[f(S)]
When no data are available for the BN training, elicitation techniques should
be considered instead. As already discussed, CNs might oﬀer a better formalism
to capture domain knowledge, especially by providing interval-valued probabili-
ties instead of sharp values. If this is the case, a CN version of Algorithm 2 can
be considered. To achieve that, in line 3, a score taking into account the fact that
the entropies in a CN are not anymore precisely speciﬁed should be adopted.
Similar considerations apply to the evaluation function in line 8.
The price of such increased realism in the elicitation is the higher complexity
characterizing inferences based on CNs. The work in [19] oﬀers a critical dis-
cussion of those issues, that are only partially addressed by heuristic techniques
used there to approximate the upper bounds of conditional entropies. In the next
section we consider an alternative approach to cope with CNs and adaptive TAs
based on diﬀerent scores used to select the questions.
6
A New Score for Testing Algorithms
Following [27], we can regard the PMF entropy (and its conditional version)
used by Algorithm 2 as an example of an index of qualitative variation (IQV).
An IQV is just a normalized number that takes value zero for degenerate PMFs,
one on uniform ones, being independent on the number of possible states (and
samples for empirical models). The closer to uniform is the PMF, the higher is
the index and vice versa.

406
A. Antonucci et al.
In order to bypass the computational issues related to its application with
CNs, we want to consider alternative IQVs to replace entropy in Algorithm 2.
Wilkox deviation from the mode (DM) appears a sensible option. Given a PMF
P(V ), this corresponds to:
M(V ) := 1 −

v∈V
maxv′∈V P(v′) −P(v)
|V| −1
.
(1)
It is a trivial exercise to check that this is a proper IQV, with the same unimodal
behavior of the entropy. In terms of explainability, being a linear function of the
modal probability, the numerical value of the DM oﬀers a more transparent
interpretation than the entropy. From a computational point of view, for both
marginal and conditional PMFs, both the entropy and the DM can be directly
obtained from the probabilities of the singletons.
The situation is diﬀerent when computing the bounds of these quantities
with respect to a CS. For the upper bound, by simple algebra, we obtain:
M(V ) :=
max
P (V )∈K(V ) M(V ) = 1 −minP (V )∈K(V ) maxv′∈V P(v′)
(|V| −1)/|V|
.
(2)
As we assume CSs deﬁned by a ﬁnite number of linear constraints, such a min-
imax objective function can be easily reduced to a linear programming task by
adding an auxiliary variable corresponding to the maximum and the constraints
modeling the fact that this variable is the maximum. The situation is even sim-
pler for the lower bound M(V ), which reduces to a maximax corresponding to
the identiﬁcation of the singleton state with the highest upper probability. Opti-
mizing entropy requires instead a non-trivial, but convex, optimization. See for
instance [1] for an iterative procedure to ﬁnd the maximum when coping with
CSs deﬁned by probability intervals. The situation is even more critical for the
minimization, that has been proved to be NP-hard in [28].
The optimization becomes more challenging for conditional entropies, as
these are mixtures of entropies of conditional distributions based on imprecise
weights. Consequently, in [19], only an inner approximation for the upper bound
have been derived. The situation is diﬀerent for conditional DMs. The following
result oﬀers a feasible approach in a simpliﬁed setup, to be later extended to the
general case.
Theorem 1. Under the setup of Sect. 5, consider a CN with a single skill S and
a single question Q, that is a child of S. Let K(S) and K(Q|S) be the CCPTs of
such CN. Let also Q = {q1, . . . , qn} and S = {s1, . . . , sm}. The upper conditional
DM, i.e.,
M(S|Q) := 1 −
min
P (S)∈K(S)
P (Q|S)∈K(Q|S)
n

i=1

max
j∈{1,...,m} P(sj|qi)

P(qi) ,
(3)
where the denominator in (2) was omitted for the sake of brevity, is such that:
M(S|Q) := 1 −
min
ˆj1,...,ˆjn∈{1,...,m}
Ω(ˆj1, . . . ,ˆjn) ,
(4)

A New Score for Adaptive Tests in Bayesian and Credal Networks
407
where Ω(ˆj1, . . . ,ˆjn) is the solution of the following linear programming task.
min

i
xiˆji
s.t.

ij
xij = 1
(5)
xij ≥0
∀i, j
(6)

k
xkj ≥P(sj)
∀j
(7)

k
xkj ≤P(sj)
∀j
(8)
P(qi|sj)

k
xkj ≤xij
∀i, j
(9)
P(qi|sj)

k
xkj ≥xij
∀i, j
(10)
xiˆji ≥xij
∀i, j ̸= ˆji
(11)
Assignments of (ˆj1, . . . ,ˆjn) such that the corresponding linear programming
task is unfeasible are just removed from the minimization in Eq. (4). Note that
the bounds on the sums over the indexes and on the universal quantiﬁers are
also omitted for the sake of brevity.
Proof. Equation (3) rewrites as:
M(S|Q) = 1 −
min
P (S)∈K(S)
P (Q|S)∈K(Q|S)
n

i=1

max
j=1,...,m P(sj)P(qi|sj)

.
(12)
We deﬁne the variables of such constrained optimization as xij := P(sj)·P(qi|sj)
for each i ∈{1, . . . , n} and j ∈{1, . . . , m}. The CCPT constraints can be easily
reformulated with respect to such new variables by noticing that xij = P(sj, qi),
and hence P(sj) = 
i xij and P(qi|sj) = xij/(
k xkj). Consequently, the inter-
val constraints on P(S) correspond to the linear constraints in Eqs. (7) and (8).
Similarly, for P(Q|S), we obtain:
P(qi|sj) ≤
xij

k xkj
≤P(qi|sj) ,
(13)
that easily gives the linear constraints in Eqs. (9) and (10) (as we cope with
strictly positive probabilities of the skills, the denominator in Eq. (13) cannot be
zero). The non-negativity of the probabilities corresponds to Eq. (6), while Eq.
(5) gives the normalization of P(S, Q) and the normalization of P(Q|S) is by
construction. Equation (12) rewrites therefore as:
M(S|Q) = 1 −
min
{xij}∈Γ

i
max
j
xij ,
(14)

408
A. Antonucci et al.
where Γ denotes the linear constraints in Eqs. (5)–(10). If ˆji := arg maxj xij,
Eq. (14) rewrites as:
M(S|Q) = 1 −
min
{xij}∈Γ ′

i
xiˆji ,
(15)
where Γ ′ are the constraints in Γ with the additional (linear) constraints in Eq.
(11) implementing the deﬁnition of ˆj. The minimization in the right-hand side
of Eq. (15) is not a linear programming task, as the values of the indexes ˆji
are potentially diﬀerent for diﬀerent assignments of the optimization variables
consistent with the constraints in Γ. Yet, we might address such optimization as
a brute-force task with respect to all the possible assignation of the indexes ˆji.
This is exactly what is done by Eq. (4) where all the mn possible assignations are
considered. Finally, regarding the feasibility of the constraints, as the constraints
in Eq. (14) are feasible by construction, there is at least a value of (ˆj1, . . . ,ˆjn),
i.e., the one corresponding to the optimum, for which also the constraints in Eq.
(15) are feasible.
⊓⊔
An analogous result holds for the computation of M(S|Q). In that case a
maximum should replace the minimum in both Eq. (3) and in the linear pro-
gramming tasks. The overall complexity is O(mn) with n := |Q|. This means
quadratic complexity for any test where only the diﬀerence between a wrong
and a right answer is considered from an elicitation perspective, and tractable
computations provided that the number of distinct answers to the same question
is bounded by a small constant. Coping with multiple questions becomes trivial
by means of the results in [3], that allows to merge multiple observed children
into a single one. Finally, the case of multiple skills might be similarly considered
by using the marginal bounds of the single skills in Eqs. (7) and (8).
7
Experiments
We validate the ideas outlined in the previous section in order to check whether or
not the DM can be used for TAs as a sensible alternative to information-theoretic
scores such as the entropy. In the BN context, this is achieved by computing the
necessary posterior probabilities, while Theorem 1 is used instead for CNs.
7.1
Single-Skill Experiments on Synthetic Data
For a very ﬁrst validation of our approach, we consider a simple setup made of a
single Boolean skill S and a repository with 18 Boolean questions based on nine
diﬀerent parametrizations (two questions for each parametrization). In such a
BN, the CPT of a question can be parametrized by two numbers. E.g., in the
example in Fig. 1, we used the probabilities of correctly answering the question
given that the skill is present or not, i.e., P(Q = 1|S = 1) and P(Q = 1|S = 0).

A New Score for Adaptive Tests in Bayesian and Credal Networks
409
A more interpretable parametrization can be obtained as follows:
δ := 1 −1
2[P(Q = 1| S = 1) + P(Q = 1| S = 0)] ,
(16)
κ := P(Q = 1| S = 1) −P(Q = 1| S = 0) .
(17)
Note that P(Q = 1|S = 1) > P(Q = 1|S = 0) is an obvious rationality constraint
for questions, otherwise having the skill would make it less likely to answer prop-
erly to a question. Both parameters are therefore non-negative. The parameter
δ, corresponding to the average probability of a wrong answer over the diﬀerent
skill values, can be regarded as a normalized index of the question diﬃculty. E.g.,
in Fig. 1, Q1 (δ = 0.4) is less diﬃcult than Q2 (δ = 0.5). The parameter κ can be
instead regarded as a descriptor of the diﬀerence of the conditional PMFs asso-
ciated with the diﬀerent skill values. In the most extreme case κ = 1, the CPT
P(Q|S) is diagonal implementing an identity mapping between the skill and the
question. We therefore regard κ as a indicator of the discriminative power of
the question. In our tests, for the BN quantiﬁcation, we consider the nine pos-
sible parametrizations corresponding to (δ, γ) ∈[0.4, 0.5, 0.6]2. For P(S) we use
instead a uniform PMF. For the CN approach we perturb all the BN parameters
with ϵ = ±0.05, thus obtaining a CN quantiﬁcation. A group of 1024 simulated
students, half of them having S = 0 and half with S = 1 is used for simulations.
The student answers are sampled from the CPT of the asked question on the
basis of the student proﬁle. Figure 2 (left) depicts the accuracy of the BN and
CN approaches based on both the entropy and the DM scores. To force credal
models to give a single output, decisions are based on the mid-point between
the lower and the upper probability, while lower entropies are used. Pure credal
approaches returning multiple options will be considered in a future work. We
notably see all the adaptive approaches outperforming a non-adaptive, random,
choice of the questions. To better investigate the strong overlap between these
trajectories, in Fig. 2 (right) we compute the Brier score and we observe a strong
similarity between DM and entropy approaches in both the Bayesian and the
credal case, with the credal slightly outperforming the Bayesian approach.
Fig. 2. Accuracy (left) and Brier distance (right) of TAs for a single-skill BN/CN

410
A. Antonucci et al.
7.2
Multi-skill Experiments on Real Data
For a validation on real data, we consider an online German language placement
test (see also [19]). Four diﬀerent Boolean skills associated with diﬀerent abilities
(vocabulary, communication, listening and reading) are considered and modeled
by a chain-shaped graph, for which BN and CN quantiﬁcation are already avail-
able. A repository of 64 Boolean questions, 16 for each skill, with four diﬀer-
ent levels of diﬃculty and discriminative power, have been used. Experiments
have been achieved by means of the CREMA library for credal networks [16].6
The Java code used for the simulations is available together with the Python
scripts used to analyze the results and the model speciﬁcations.7 Performances
are evaluated as for the previous model, the only diﬀerence being that here the
accuracy is aggregated by average over the separate accuracies for the four skills.
The results (Fig. 3) are analogous to those for the single-skill case: entropy-based
and mode-based scores are providing similar results, with the credal approach
typically leading to more accurate evaluations.
0
10
20
30
40
50
60
0
0.2
0.4
0.6
0.8
1
Number of questions
Aggregated Accuracy
Credal Entropy
Credal Mode
Random
Bayesian Entropy
Bayesian Mode
Fig. 3. Aggregated accuracy for a multi-skill TA
8
Outlooks and Conclusions
A new score for adaptive testing in Bayesian and credal networks has been
proposed. Our proposal is based on indexes of qualitative variation, being in
particular focused on the modal probability for their explainability features. An
algorithm to evaluate this quantity in the credal case is derived. Our experi-
ments show that moving to these scores does not really aﬀect the quality of the
selection process. Besides a deeper experimental validation, a necessary future
work consists in the derivation of simpler elicitation strategies for these mod-
els in order to promote their application to real-world testing environments. To
achieve that we also intend to embed these new scores in a software we recently
developed for the practical implementation of web-based adaptive tests [10].8
6 https://github.com/IDSIA/crema.
7 https://github.com/IDSIA/crema-adaptive.
8 https://github.com/IDSIA/adapquest.

A New Score for Adaptive Tests in Bayesian and Credal Networks
411
References
1. Abellan, J., Moral, S.: Maximum of entropy for credal sets. Int. J. Uncertainty
Fuzziness Knowl.-Based Syst. 11(05), 587–597 (2003)
2. Almond, R.G., Mislevy, R.J.: Graphical models and computerized adaptive testing.
Appl. Psychol. Meas. 23(3), 223–237 (1999)
3. Antonucci, A., Piatti, A.: Modeling unreliable observations in Bayesian networks
by credal networks. In: Godo, L., Pugliese, A. (eds.) SUM 2009. LNCS (LNAI),
vol. 5785, pp. 28–39. Springer, Heidelberg (2009). https://doi.org/10.1007/978-3-
642-04388-8 4
4. Antonucci, A., de Campos, C.P., Huber, D., Zaﬀalon, M.: Approximating credal
network inferences by linear programming. In: van der Gaag, L.C. (ed.) ECSQARU
2013. LNCS (LNAI), vol. 7958, pp. 13–24. Springer, Heidelberg (2013). https://
doi.org/10.1007/978-3-642-39091-3 2
5. Antonucci, A., de Campos, C.P., Huber, D., Zaﬀalon, M.: Approximate credal
network updating by linear programming with applications to decision making.
Int. J. Approximate Reasoning 58, 25–38 (2015)
6. Bachrach, Y., Graepel, T., Minka, T., Guiver, J.: How to grade a test without
knowing the answers–a Bayesian graphical model for adaptive crowdsourcing and
aptitude testing. arXiv preprint arXiv:1206.6386 (2012)
7. Badaracco, M., Mart´ınez, L.: A fuzzy linguistic algorithm for adaptive test in
intelligent tutoring system based on competences. Expert Syst. Appl. 40(8), 3073–
3086 (2013)
8. Badran, M.E.K., Abdo, J.B., Al Jurdi, W., Demerjian, J.: Adaptive serendipity
for recommender systems: Let it ﬁnd you. In: ICAART (2), pp. 739–745 (2019)
9. Bolt, J.H., De Bock, J., Renooij, S.: Exploiting Bayesian network sensitivity func-
tions for inference in credal networks. In: Proceedings of the Twenty-Second Euro-
pean Conference on Artiﬁcial Intelligence (ECAI), vol. 285, pp. 646–654. IOS Press
(2016)
10. Bonesana, C., Mangili, F., Antonucci, A.: ADAPQUEST: a software for web-based
adaptive questionnaires based on Bayesian networks. In: IJCAI 2021 Workshop
Artiﬁcial Intelligence for Education (2021)
11. Chen, S.J., Choi, A., Darwiche, A.: Computer adaptive testing using the same-
decision probability. In: BMA@ UAI, pp. 34–43 (2015)
12. Conati, C., Gertner, A.S., VanLehn, K., Druzdzel, M.J.: On-line student modeling
for coached problem solving using Bayesian networks. In: Jameson, A., Paris, C.,
Tasso, C. (eds.) User Modeling. ICMS, vol. 383, pp. 231–242. Springer, Vienna
(1997). https://doi.org/10.1007/978-3-7091-2670-7 24
13. Cozman, F.G.: Credal networks. Artif. Intell. 120(2), 199–233 (2000)
14. Embretson, S.E., Reise, S.P.: Item Response Theory. Psychology Press, Hove (2013)
15. H´ajek, A., Smithson, M.: Rationality and indeterminate probabilities. Synthese
187(1), 33–48 (2012)
16. Huber, D., Caba˜nas, R., Antonucci, A., Zaﬀalon, M.: CREMA: a Java library
for credal network inference. In: Jaeger, M., Nielsen, T. (eds.) Proceedings of the
10th International Conference on Probabilistic Graphical Models (PGM 2020).
Proceedings of Machine Learning Research, PMLR, Aalborg, Denmark (2020)
17. Koller, D., Friedman, N.: Probabilistic Graphical Models: Principles and Tech-
niques. MIT Press, Cambridge (2009)
18. Laitusis, C.C., Morgan, D.L., Bridgeman, B., Zanna, J., Stone, E.: Examination
of fatigue eﬀects from extended-time accommodations on the SAT reasoning test.
ETS Research Report Series 2007(2), i–13 (2007)

412
A. Antonucci et al.
19. Mangili, F., Bonesana, C., Antonucci, A.: Reliable knowledge-based adaptive tests
by credal networks. In: Antonucci, A., Cholvy, L., Papini, O. (eds.) ECSQARU
2017. LNCS (LNAI), vol. 10369, pp. 282–291. Springer, Cham (2017). https://doi.
org/10.1007/978-3-319-61581-3 26
20. Marchetti, S., Antonucci, A.: Reliable uncertain evidence modeling in Bayesian
networks by credal networks. In: Brawner, K.W., Rus, V. (eds.) Proceedings of the
Thirty-First International Florida Artiﬁcial Intelligence Research Society Confer-
ence (FLAIRS-31), pp. 513–518. AAAI Press, Melbourne, Florida, USA (2018)
21. Mau´a, D.D., De Campos, C.P., Benavoli, A., Antonucci, A.: Probabilistic inference
in credal networks: new complexity results. J. Artif. Intell. Res. 50, 603–637 (2014)
22. Piatti, A., Antonucci, A., Zaﬀalon, M.: Building knowledge-based expert systems
by credal networks: a tutorial. In: Baswell, A. (ed.) Advances in Mathematics
Research, vol. 11, chap. 2. Nova Science Publishers, New York (2010)
23. Plajner, M., Vomlel, J.: Monotonicity in practice of adaptive testing. arXiv preprint
arXiv:2009.06981 (2020)
24. Sawatzky, R., Ratner, P.A., Kopec, J.A., Wu, A.D., Zumbo, B.D.: The accuracy
of computerized adaptive testing in heterogeneous populations: a mixture item-
response theory analysis. PLoS ONE 11(3), e0150563 (2016)
25. Vomlel, J.: Bayesian networks in educational testing. Int. J. Uncertain. Fuzziness
Knowl.-Based Syst. 12(supp01), 83–100 (2004)
26. Vomlel, J.: Building adaptive tests using Bayesian networks. Kybernetika 40(3),
333–348 (2004)
27. Wilcox, A.R.: Indices of qualitative variation and political measurement. Western
Political Q. 26(2), 325–343 (1973)
28. Xiang, G., Kosheleva, O., Klir, G.J.: Estimating information amount under inter-
val uncertainty: algorithmic solvability and computational complexity. Technical
report 158, Departmental Technical Reports (CS) (2006)

Multi-label Chaining with Imprecise
Probabilities
Yonatan Carlos Carranza Alarc´on
and S´ebastien Destercke(B)
Sorbonne Universit´es, Universit´e Technologique de Compi`egne, CNRS,
UMR 7253 - Heudiasyc, 57 Avenue de Landshut, Compi`egne, France
{yonatan-carlos.carranza-alarcon,sebastien.destercke}@hds.utc.fr
Abstract. We present two diﬀerent strategies to extend the classical
multi-label chaining approach to handle imprecise probability estimates.
These estimates use convex sets of distributions (or credal sets) in order
to describe our uncertainty rather than a precise one. The main reasons
one could have for using such estimations are (1) to make cautious pre-
dictions (or no decision at all) when a high uncertainty is detected in the
chaining and (2) to make better precise predictions by avoiding biases
caused in early decisions in the chaining. We adapt both strategies to the
case of the naive credal classiﬁer, showing that this adaptations are com-
putationally eﬃcient. Our experimental results on missing labels, which
investigate how reliable these predictions are in both approaches, indi-
cate that our approaches produce relevant cautiousness on those hard-
to-predict instances where the precise models fail.
Keywords: Imprecise probabilities · Multi-label · Classiﬁer chains
Multi-label classiﬁcation (MLC) is a generalization of traditional classiﬁca-
tion (with a single label), as well as a special case of multi-task learning. This
approach is increasingly required in diﬀerent research ﬁelds, such as the clas-
siﬁcation of proteins in bioinformatics [17], text classiﬁcation in information
retrieval [10], object recognition in computer vision [3], and so on.
A classical issue in multi-label learning techniques is how to integrate the
possible dependencies between labels while keeping the inference task tractable.
Indeed, while decomposition techniques [10,17] such as Binary relevance or Cal-
ibrated ranking allow to speed up both the learning and inference tasks, they
roughly ignore the label dependencies, while using a fully speciﬁed model such
as in pr obabilistic trees [6] requires, at worst, to scan all possible predictions
(whose quantity grows exponentially in the number of labels). A popular tech-
nique, known as chaining [15] to solve this issue, at least for the inference task, is
to use heuristic predictions: they consists in using, incrementally, the predictions
made on previous labels as additional conditional features to help better predict
the rel of a current label, rather than using the prediction probabilities.
To the best of our knowledge, there are only a few works on multi-label clas-
siﬁcation producing cautious predictions, such as the reject option [14], partial
c
⃝Springer Nature Switzerland AG 2021
J. Vejnarov´a and N. Wilson (Eds.): ECSQARU 2021, LNAI 12897, pp. 413–426, 2021.
https://doi.org/10.1007/978-3-030-86772-0_30

414
Y. C. C. Alarc´on and S. Destercke
predictions [1,8] or abstaining labels [18], but none of these have studied this
issue in chaining (or classiﬁer-chains approach).
In this paper, we consider the problem of extending chaining to the imprecise
probabilistic case, and propose two diﬀerent extensions, as some predictive prob-
abilities are too imprecise to use predicted labels, henceforth called abstained
labels, in the chaining. The ﬁrst extension treats the abstained labels in a robust
way, exploring all possible conditional situation in order not to propagate early
uncertain decisions, whereas the latter marginalizes the probabilistic model over
those labels, ignoring them in the predictive model.
Section 1 introduces the notations that we use for the multi-label setting, and
gives the necessary reminders about making inferences with convex sets of prob-
abilities. In Sect. 2, we recall the classical classiﬁer-chains approach and then we
present our extended approaches based on imprecise probabilities. Section 3 then
shows that in the case of the naive credal classiﬁer (NCC) [21], those strategies
can be performed in polynomial time.
Finally, in Sect. 4, we perform a set of experiments on real data sets, which
are perturbed with missing labels, in order to investigate how cautious (when
we abstain on labels diﬃcult to predict) is our approach. In order to adhere
to the page limit, all proofs and supplementary experimental results have been
relegated to the appendix of an online extended version [4].
1
Problem Setting
In the multi-label problem, an instance x of an input space X =Rp is no longer
associated with a single label mk of an output space K={m1, . . . , mm}, as in the
traditional classiﬁcation problem, but with a subset of labels Λx ⊆K often called
the set of relevant labels while its complement K\Λx is considered as irrelevant
for x. Let Y ={0, 1}m be a m-dimensional binary space and y =(y1, . . . , ym) ∈
Y be any element of Y such that yi =1 if and only if mi ∈Λx.
From a decision theoretic approach (DTA), the goal of the multi-label prob-
lem is the same as usual classiﬁcation. Given a probability distribution ˆP ﬁtting
a set of i.i.d. observations D = {(xi, yi)|i = 1, . . . , N} issued from a (true) the-
oretical probability distribution P : X × Y →[0, 1], DTA aims to minimize the
risk of getting missclassiﬁcation with respect to a speciﬁed loss function ℓ(·, ·):
Rℓ(Y, h(X)) = min
h
EˆP [ℓ(Y, h(X))] .
(1)
where h:X →Y is a m-dimensional vector function. If ℓ(·, ·) is deﬁned instance-
wise, that is ℓ: Y×Y →R, the solution of Equation (1) is obtained by minimizing
the conditional expected risk (cf. [7, Eq. 3] and [9, Eq. 2.21])
h(x) = arg min
y∈Y
EˆPY |x [ℓ(Y, y)] = arg min
y∈Y

y ′∈Y
ˆP(Y =y′|X =x)ℓ(y′, y)
(2)

Multi-label Chaining with Imprecise Probabilities
415
or, equivalently, by picking the undominated elements of the order relation1 ⪰
over Y ×Y for which y1 ⪰y2 (y1 is preferred to/dominates y2) iﬀ
EˆPY |x

ℓ(y2, ·) −ℓ(y1, ·)

= EˆPY |x

ℓ(y2, ·)

−EˆPY |x

ℓ(y1, ·)

≥0.
(3)
This amounts to saying that exchanging y2 for y1 would incur a non-negative
expected loss (which is not desirable).
In this paper, we are also interested in making set-valued predictions when
uncertainty is too high (e.g. due to insuﬃcient evidence to include or discard
a label as relevant, see Example 1). The set-valued prediction will here be
described as a partial binary vector y∗∈Y where Y = {0, 1, ∗}m is the new
output space with a new element ∗representing the abstention. For instance, a
partial prediction y∗=(∗, 1, 0) corresponds to two plausible binary vector solu-
tions {(0, 1, 0), (1, 1, 0)} ⊆Y . To obtain such predictions, we will use imprecise
probabilities as a well-founded framework.
1.1
Notions About Imprecise Probabilities
Imprecise probabilities consist in representing our uncertainty by a convex set of
probability distributions PX [2,19] (i.e. a credal set [12]), deﬁned over a space
X rather than by a precise probability measure PX [16]. Given such a set of
distributions PX and any measurable event A ⊆X , we can deﬁne the notions
of lower and upper probabilities as:
P X(A) =
inf
P ∈P X P(A)
and
P X(A) =
sup
P ∈P X
P(A)
(4)
where P X(A) = P X(A) only when we have suﬃcient information about event
A. The lower probability is dual to the upper [2], in the sense that P X(A) =
1−P X(Ac) where Ac is the complement of A. Many authors [19,21] have argued
that when information is lacking or imprecise, considering credal sets as our
model of information better describes our actual uncertainty.
However, such an approach comes with extra challenges in the learning and
inference step, especially in combinatorial domains. In this paper, we will con-
sider making a chain of binary inferences, each inference inﬂuenced by the pre-
vious one. If we consider Y = {0, 1} as the output space and Y as a univariate
random variable on Y, a standard way to take a decision with abstention given
a credal set P on Y is
ˆy =
⎧
⎪
⎨
⎪
⎩
1
if P x(Y =1) > 0.5,
0
if P x(Y =1) < 0.5,
∗
if 0.5 ∈
	
P x(Y =1), P x(Y =1)

 .
(5)
The next example illustrates such notions on a multi-label example
Example 1. We consider an output space of two labels K = {m1, m2}, a single
binary feature x1 and Table 1 with imprecise estimations of ˆP(Y1, Y2|X1).
1 A complete, transitive, and asymmetric binary relation.

416
Y. C. C. Alarc´on and S. Destercke
Table 1. Estimated conditional probability distributions ˆP(Y1, Y2|X1) ∈ˆ
PY1,Y2|X1.
y1 y2 x1
ˆ
PY1,Y2|X1=0 y1 y2 x1
ˆ
PY1,Y2|X1=1
0
0
0
[0.4,0.7]
0
0
1
0.00
0
1
0
[0.3,0.6]
0
1
1
0.00
1
0
0
0.00
1
0
1
[0.6,0.8]
1
1
0
0.00
1
1
1
[0.2,0.4]
Based on the probabilities of Table 1, we have that ˆP0(y1 = 0) := ˆP(y1 =
0|x1 = 0) = 1 and ˆP0(y2 = 0) ∈[0.4, 0.7], therefore not knowing whether ˆP0(y2 =
0) > 0.5. This could lead us to propose as a prediction ˆy∗=(0, ∗). On the con-
trary, the imprecision on the right hand-side is such that ˆP1(y2 =0)∈[0.6, 0.8],
leading to the precise prediction ˆy∗=(1, 0).
2
Multilabel Chaining with Imprecise Probabilities
Solving (2) can already be computationally prohibitive in the precise case [6],
which is why heuristic to approximate inferences done on the full joint models
such as the chain model have been proposed. This section recalls its basics and
presents our proposed extension. To do so, we will need a couple notations: we
will denote by I subsets of label indices and by [[j]] = {1, . . . , j} the set of the
ﬁrst j integers. Given a prediction made in the j ﬁrst labels, we will denote by
1. (relevant labels) I j
R ⊆[[j]] the indices of the labels predicted as relevant
among the j ﬁrst, i.e. ∀i ∈I j
R, yi = 1,
2. (irrelevant labels) I j
I ⊆[[j]], I j
I ∩I j
R = ∅the indices of the labels predicted
as irrelevant among the j ﬁrst, i.e. ∀i ∈I j
I , yi = 0, and
3. (abstained labels) I j
A = [[j]]\(I j
R ∪I j
I ) the indices of the labels on which we
abstained among the j ﬁrst, i.e. ∀i ∈I j
A, yi = {0, 1} := ∗,
and of course I j =I j
A ∪I j
R ∪I j
I =[[j]]. Besides, for the sake of simplicity, we
will use the notation
P [[j−1]]
x
(Yj =1) := P(Yj =1|YI j−1= ˆyI j−1, X = x),
(6)
where ˆyI j−1 is a (j −1)-dimensional vector that contains the previously inferred
precise and/or abstained values of labels having indices I j−1.
2.1
Precise Probabilistic Chaining
Classiﬁer chains is a well-known approach exploiting dependencies among labels
by ﬁtting at each step of the chain (see Fig. 1) a new classiﬁer model hj :
X × {0, 1}j−1 →{0, 1} predicting the relevance of the jth label. This classi-
ﬁer combines the original input space attribute and all previous predictions in

Multi-label Chaining with Imprecise Probabilities
417
the chain in order to create a new input space X ∗
j−1 = X × {0, 1}j−1, j ∈N>0.
In brief, we consider a chain h = (h1, . . . , hm) of binary classiﬁers resulting in
the full prediction ˆy obtained by solving each single classiﬁer as follows
ˆyj := hj(x) = arg max
y∈{0,1}
P [[j−1]]
x
(Yj =y).
(7)
The classical multi-label chaining then works as follows:
1. Random label ordering. We randomly pick an order between labels and
assume that the indices are relabelled in an increasing order.
2. Prediction jth label. For a label yj and the previous predictions on labels
y1, . . . , yj−1 and let I j−1
R
, I j−1
I
⊆[[j −1]] be set of indices of relevant and
irrelevant labels with I j−1
R
∩I j−1
I
=∅. Then, the prediction of ˆyj (or hj(x))
for a new instance x is
ˆyj =

1
if Px(Yj = 1|YI j−1
R
= 1, YI j−1
I
= 0) = P [[j−1]]
x
(Yj =1) ≥0.5
0
if Px(Yj = 1|YI j−1
R
= 1, YI j−1
I
= 0) = P [[j−1]]
x
(Yj =1) < 0.5
(8)
Figure 1 summarizes the procedure presented above, as well as the obtained
predictions for a speciﬁc case (in bold red predicted labels and probabilities).
Fig. 1. Precise chaining (Color ﬁgure online)
Figure 1 shows that using this heuristic can lead to strong biases, as two
diﬀerent orderings of the same joint model can lead to shift from one prediction to
its opposite. Intuitively, adding some robustness and cautiousness in the process
could help to avoid unwarranted biases.
In what follows, we propose two diﬀerent extensions of precise chaining based
on imprecise probability estimates, in which the ﬁnal prediction belongs to the
output space Y of partial bianry vectors.
2.2
Imprecise Probabilistic Chaining
We now consider that the estimates P [[j−1]]
x
(Yj = 1) can become imprecise, that
is, we now have [P [[j−1]]
x
](Yj = yj) := [P [[j−1]]
x
(Yj = yj), P
[[j−1]]
x
(Yj = yj)]. The basic
idea of using such estimates is that in the chaining, we should be cautious when

418
Y. C. C. Alarc´on and S. Destercke
the classiﬁer is unsure about the most probable prediction. In this section, we
describe two diﬀerent strategies (or extensions) in a general way, and we will
propose an adaptation of those strategies to the NCC in the next section.
Let us ﬁrst formulate the generic procedure to calculate the probability bound
of the jth label,
1. Random label ordering. As in the precise case.
2. Prediction jth label. For a given label yj, we assume we have (possily
imprecise) predictions for y1, . . . , yj−1 such that I j−1
A
are the indices of labels
on which we abstained {∗} so far, and I j−1
R
and I j−1
I
remain the indices of
relevant and irrelevant labels, such that I j−1
A
∪I j−1
R
∪I j−1
I
=I j−1. Then,
we calculate [P [[j−1]]
x
](Yj =1) in order to predict the label ˆyj as
ˆyj =
⎧
⎪
⎨
⎪
⎩
1
if P [[j−1]]
x
(Yj = 1) > 0.5,
0
if P
[[j−1]]
x
(Yj = 1) < 0.5,
∗
if 0.5 ∈[P [[j−1]]
x
(Yj = 1), P
[[j−1]]
x
(Yj = 1)],
(9)
where this last equation is a slight variation of Eq. (5) by using the new input
space X ∗
j−1.
We propose the following two diﬀerent extensions of how to calculate
[P [[j−1]]
x
](Yj = 1) at each inference step of the imprecise chaining.
Imprecise Branching. The ﬁrst strategy treats unsure predictions in a robust
way, considering all possible branchings in the chaining as soon as there is an
abstained label. Thus, the estimation of [P [[j−1]]
x
(Yj = 1), P
[[j−1]]
x
(Yj = 1)] (for
Yj = 0, it directly obtains as P [[j−1]]
x
(Yj = 0) = 1 −P
[[j−1]]
x
(Yj = 1), and similarly
for the upper bound) comes down to computing
P [[j−1]]
x
(Yj = 1)=
min
y∈{0,1}|I j−1
A | P x(Yj = 1|YI j−1
R
= 1, YI j−1
I
= 0, YI j−1
A
= y),
P
[[j−1]]
x
(Yj = 1)=
max
y∈{0,1}|I j−1
A | P x(Yj = 1|YI j−1
R
= 1, YI j−1
I
= 0, YI j−1
A
= y).
(IB)
So we consider all possible replacements of variables for which we have abstained
so far. This corresponds to a very robust version of the chaining, where every
possible path is explored. It will propagate imprecision along the tree, and may
produce quite imprecise evaluations, especially if we abstain on the ﬁrst labels.
Illustrations providing some intuition about this strategy can be seen in
Fig. 2(b) where we have abstained on labels (Y2, Y4) and we want to compute
lower and upper probability bounds of the label Y5 = 1.

Multi-label Chaining with Imprecise Probabilities
419
Fig. 2. Imprecise branching strategy
In Fig. 2(a), we will consider the previous example (see Fig. 1) in order
to study in detail how we should calculate probability bounds [P [[j−1]]
x
(Yj =
1), P
[[j−1]]
x
(Yj = 1)]. For the sake of simplicity, we assume that probabilities about
Y2 are precise and that the probability bounds for Y1 = 1 is ˆP [[0]]
x
(Y1 = 1) ∈
[0.45, 0.70]. Taking all possible paths (bold on Fig. 2(a)) for which Y2 = 1, we
get
P [[1]]
x
(Y2 = 1) =
min
y1∈{0,1} Px(Y2 = 1|Y1 = y1) = min(0.1, 0.6) = 0.1,
P
[[1]]
x
(Y2 = 1) =
max
y1∈{0,1} Px(Y2 = 1|Y1 = y1) = max(0.1, 0.6) = 0.6,
which means that in this case we would abstain on both labels, i.e. (ˆy1, ˆy2)=(∗, ∗).
Marginalization. The second strategy simply ignores unsure predictions in the
chaining. Its interest is that it will not propagate imprecision in the tree. Thus, we
begin by presenting the general formulation (which will after lead to the formula-
tion without unsureness) which takes into account unsure predicted labels condi-
tionally, so the estimation of probability bounds [P [[j−1]]
x
(Yj = 1), P
[[j−1]]
x
(Yj = 1)]
comes down to computing
P [[j−1]]
x
(Yj =1)=P x(Yj = 1|YI j−1
R
=1, YI j−1
I
=0, YI j−1
A
={0, 1}|I j−1
A
|),
P
[[j−1]]
x
(Yj =1)=P x(Yj = 1|YI j−1
R
=1, YI j−1
I
=0, YI j−1
A
={0, 1}|I j−1
A
|),
(MAR)
where I j−1
A
= {i1, . . . , ik} denotes the set of indices of abstained labels and the
last conditional term of probability bounds can be deﬁned as

YI j−1
A
= {0, 1}|I j−1
A
|
:=(Yi1 = 0 ∪Yi1 = 1) ∩· · · ∩(Yik = 0 ∪Yik = 1). (10)
The MAR formulation can be reduced by using Bayes’s theorem in conjunc-
tion with the law of total probability. That is, for instance, given abstained labels
(Y1 = ∗, Y3 = ∗) and the precise prediction (Y2 = 1), inferring Y4 = 1 comes
down to computing Px(Y4 =1|(Y1 =0∪Y1 =1), Y2 =1, (Y3 =0∪Y3 =1)) as follows

420
Y. C. C. Alarc´on and S. Destercke

y3,y1∈{0,1}2
Px(Y4 =1, Y1 =y1, Y2 =1, Y3 =y3)

y3,y1∈{0,1}2
Px(Y1 =y1, Y2 =1, Y3 =y3)
= Px(Y4 =1, Y2 =1)
Px(Y2 =1)
=Px(Y4 =1|Y2 =1),
An illustration providing some intuition about this last example can be seen in
Fig. 3(a), in which we draw the possible path to infer the label Y4 (considering
a third branch in the chain to represent abstained labels).
The results of the last example can easily be generalized, and hence, MAR
comes down to calculating the new formulation called (MAR*)
P [[j−1]]
x
(Yj = 1) = minP ∈P ∗Px(Yj = 1|YI j−1
R
= 1, YI j−1
I
= 0),
(11)
P
[[j−1]]
x
(Yj = 1) = maxP ∈P ∗Px(Yj = 1|YI j−1
R
= 1, YI j−1
I
= 0).
(12)
where P∗is simply the set of joint probability distributions described by the
imprecise probabilistic tree (we refer to [5] for a detailed analysis). (MAR) comes
down to restrict the conditional chain model to consider only those labels on
which we have not abstained (for which we have suﬃcient reliable information).
In general, both (IB) and (MAR) can lead to burdensome computations. In the
next section, we propose to adapt their principle to the NCC, showing that this
can be done eﬃciently.
Remark 1. Note that another strategy that would be computationally eﬃcient
would be to simply considers all precise chaining paths consistent with local
intervals, pruning dominated branches. However, this would also mean that each
explored branch would have all previous predicted labels as conditioning features,
thus still being impacted by the bias of picking those predictions.
3
Imprecise Chaining with NCC
NCC extends the classical naive Bayes classiﬁer (NBC) on a set of distribu-
tions. NCC preserves the assumption of feature independence made by NBC, and
relies on the Imprecise Dirichlet model (IDM) [20] to estimate class-conditional
imprecise probabilities, whose imprecision level is controlled through a hyper-
parameter s ∈R. Therefore, the class-conditional probability bounds evaluated
for Yj = 1 (Yj = 0 can be directly calculated using duality) can be calculated as
follows2
P(Yj =1|X=x,YI j−1= ˆyI j−1)=

1+ P(Yj =0)P 0(X=x)P 0(YI j−1= ˆyI j−1)
P(Yj =1)P 1(X=x)P 1(YI j−1= ˆyI j−1)
−1
, (13)
P(Yj =1|X=x,YI j−1= ˆyI j−1)=

1+ P(Yj = 0)P 0(X=x)P 0(YI j−1= ˆyI j−1)
P(Yj =1)P 1(X = x)P 1(YI j−1= ˆyI j−1)
−1
. (14)
2 For reviewer convenience, details are given to the extended version submitted to
ArXiv https://arxiv.org/abs/2107.07443 .

Multi-label Chaining with Imprecise Probabilities
421
where conditional upper probabilities of [P 1, P 1] and [P 0, P 0] are deﬁned as
P a(X=x):=
p

i=1
P(Xi =xi|Yj =a) and P a(YI j−1=yI j−1):=
j−1

k=1
P(Yk = ˆyk|Yj =a),
(15)
where a ∈{0, 1}. Conditional lower probabilities are obtained similarly. Using
Eqs. (13) and (14), we now propose eﬃcient procedures to solve the aforemen-
tioned strategies.
3.1
Imprecise Branching
In the speciﬁc case where we use the NCC, we can eﬃciently reduce the opti-
mization problems of Eqs. (IB), as expressed in the proposition below.
Proposition 1. Optimisation problems of the imprecise branching (IB) can
be reduced by using probability bounds obtained from the NCC, namely Equa-
tions (13) and (14), as follows
P [[j−1]]
x
(Yj =1)∝max
y ∈{0,1}|I j−1
A
|
P 0(YI j−1
A
=y)
P 1(YI j−1
A
=y) and P
[[j−1]]
x
(Yj =1)∝min
y ∈{0,1}|I j−1
A
|
P 0(YI j−1
A
=y)
P 1(YI j−1
A
=y).
Besides, applying the equations derived from the imprecise Dirichlet model, we
have that the values of abstained labels for which the previous optimisation prob-
lems are solved are, respectively
ˆyI j−1
A :=arg max
y ∈{0,1}
I j−1
A


yi∈y
n(yi|yj =0)+s
n(yi|yj =1)
and
ˆyI j−1
A := arg min
y ∈{0,1}
I j−1
A


yi∈y
n(yi|yj =0)
n(yi|yj =1)+s
(16)
where I j−1
A
is the set of indices of the (j −1)th ﬁrst predicted abstained labels,
n(·) is a count function that counts the number of occurrences of the event yi|yj
and n(yi|yj =1) is always strictly positive.
Proposition 1 says that it is not necessary to know the original input features
X and neither the (j −1)th ﬁrst precise predicted labels, in order to get the
lower and upper probability bound of Equations (IB). However, it is necessary
to keep track of the estimates made on all abstained labels, which is consistent
with the fact that we want to capture the optimal lower and upper bounds of
the conditional probability over all possible paths on which we have abstained.
Proposition 1 allows us to propose an algorithm below that can calculate
Eqs. (16) linearly in the number of abstained labels.
Proposition 2. The chain of labels ˆyI j−1
A
and ˆyI j−1
A
can be obtained in a time
complexity of O(|I j−1
A
|).
The following proposition provides the time complexity of the inference step
of the imprecise branching strategy, jointly with the NCC and previous results.
Proposition 3. The global time complexity of the imprecise branching strat-
egy in the worst-case is O(m2) and in the best-case is O(m).

422
Y. C. C. Alarc´on and S. Destercke
3.2
Marginalization
When the NCC is considered, nothing needs to be optimized in the marginal-
ization strategy, thanks to the assumption of independence applied between the
binary conditional models of the chain.
We recall that the marginalization strategy needs to compute the conditional
models described in Eqs. (MAR). These latter can be solved by simply ignoring
the abstained labels in Eqs. (13) and (14) of the NCC. We thus focus on adapting
Eq. (14) (Eq. (13) can be treated similarly), in order to show that the abstained
labels can be removed of the conditioning and to get the expression presented
in Eq. (12). Based on Eq. (14), we can only focus on the conditional upper
probability on labels, namely Eq. (15), and rewrite it as follows:
P 0(YI j−1 = ˆyI j−1) := P 0(YI j−1
∗
= ˆyI j−1
∗
, YI j−1
A
= {0, 1}|I j−1
A
|)
(17)
where I j−1
∗
= I j−1
R
∪I j−1
I
is the set of indices of relevant and irrelevant
inferred labels, and the right side of last equation can be stated as
max
P ∈

P Yk|Yj ,P Ya|Yj

k∈I j−1
∗
,a∈I j−1
A

k∈I j−1
∗
P(Yk = ˆyk|Yj =0)

a∈I j−1
A
P(Ya = 0 ∪Ya = 1|Yj =0).
Thanks to the product assumption in the NCC, each term can be treated sepa-
rately, making it possible to decouple the multiplication in two parts;
max
P ∈P Yk|Yj
k∈I j−1
∗

k∈I j−1
∗
P(Yk = ˆyk|Yj =0) ×
max
P ∈P Ya|Yj
a∈I j−1
A

a∈I j−1
A
P(Ya = 0 ∪Ya = 1|Yj =0),
where P(Ya=0∪Ya=1|Yj=0)=1, and hence the second part becomes 1. Replacing
this result in Eq. (17), and then this latter in Eq. (14), we get
P
[[j−1]]
x
(Yj = 1) = maxP ∈P Yj |YI j−1
R
,YI j−1
I
Px(Yj = 1|YI j−1
R
= 1, YI j−1
I
= 0).
Therefore, at each inference step, we can apply Eqs. (13) and (14) on the reduced
new formulation of the marginalization strategy (MAR*). An illustration pro-
viding some intuition about this reduction can be seen in Fig. 3.
Fig. 3. Marginalization strategy applied to NCC for four labels {Y1, Y2, Y3, Y4}

Multi-label Chaining with Imprecise Probabilities
423
4
Experiments
In this section, we perform experiments3 on 6 data sets issued from the MULAN
repository4 (c.f. Table 2), following a 10×10 cross-validation procedure.
Table 2. Multi-label data sets summary
Data set
#Domain #Features #Labels #Instances #Cardinality #Density
Emotions Music
72
6
593
1.90
0.31
Scene
Image
294
6
2407
1.07
0.18
Yeast
Biology
103
14
2417
4.23
0.30
cal500
Music
68
174
502
26.04
0.15
Medical
Text
1449
45
978
1.25
0.03
Enron
Text
1001
53
1702
3.38
0.06
Evaluation and Setting. The usual metrics used in multi-label problems are
not adapted at all when we infer set-valued predictions. Thus, we consider appro-
priate to use the set-accuracy (SA) and completeness (CP) [8, §4.1], as follows
SA(ˆy, y) = 1(y∈ˆy)
and
CP(ˆy, y) = |Q|
m ,
where ˆy is the partial binary prediction (i.e. the set of all possibles binary vectors)
and Q denote the set of non-abstained labels. When predicting complete vectors,
then CP = 1 and SA equals the 0/1 loss function and when predicting the
empty vector, i.e. all labels ˆyi = ∗, then CP =0 and by convention SA =1. The
reason for using SA is that chaining is used as an approximation of the optimal
prediction for a 0/1 loss function.
Imprecise Classiﬁer. As mentioned in Sect. 3, we will use the naive credal
classiﬁer (NCC). Note that NCC needs discretized input spaces, so we dis-
cretize data sets to z = 6 intervals (except for Medical and Enron data sets).
Besides, we restrict the values of the hyper-parameter of the imprecision to
s ∈{0.0, 0.5, . . . , 4.5, 5.5} (when s = 0.0, NCC becomes the precise classiﬁer
NBC). At higher values of s, the NCC model will make mostly vacuous predic-
tions (i.e. abstain in all labels ∀i, Yi =∗) for the data sets we consider here.
Missing Labels. To simulate missingness during the training step, we uniformly
pick at random a percentage of labels Yj,i (the jth label of the ith instance),
which are then removed from the training data used to ﬁt the conditional models
in the chain. In this paper, we set up ﬁve diﬀerent percentages of missingness:
{0, 20, 40, 60, 80}%.
3 Implemented in Python, see https://github.com/sdestercke/classiﬁp.
4 http://mulan.sourceforge.net/datasets.html.

424
Y. C. C. Alarc´on and S. Destercke
4.1
Experimental Results
Figure 4, we provide the results for 3 data sets, showing set-accuracy and com-
pleteness measures in average5(%) obtained by ﬁtting the NCC model for dif-
ferent percentages of missing labels, respectively, applied to the data sets of
Table 2 and using the imprecise branching strategy (trends were similar for the
marginalization strategies, not displayed due to lack of space)6.
Set−accuracy
Completeness
0
20
40
60
80
40%
60%
80%
100%
0%
25%
50%
75%
100%
% Missing
Imprecision
0.0
0.5
1.5
2.5
3.5
4.5
5.5
Set−accuracy
Completeness
0
20
40
60
80
0%
20%
40%
60%
25%
50%
75%
100%
% Missing
Imprecision
0.0
0.5
1.5
2.5
3.5
4.5
5.5
Set−accuracy
Completeness
0
20
40
60
80
25%
50%
75%
100%
0%
25%
50%
75%
100%
% Missing
Imprecision
0.0
0.5
1.5
2.5
3.5
4.5
5.5
Fig. 4. Missing labels - Imprecise Branching evolution of the average (%) set-
accuracy (top) and completeness (down) for each level of imprecision (a curve for each
one) and a discretization z = 6, with respect to the percentage of missing labels.
The results show that when the percentage of missing labels increases, the set-
accuracy (SA) increases (regardless of the amount of imprecision s we inject) as
we abstain more and more (as completeness decreases). This means that the more
imprecise we get, the more accurate are those predictions we retain. However, a
high amount of imprecision is sometimes required to include the ground-truth
solution within the set-valued prediction (this may be due to the very restrictive
0/1 loss metric). For instance, with s=5.5 and 40% of missingness, we get a
>65% of set-accuracy versus a<50% of completeness, in the Emotions data set.
Overall, the results are those that we expect and are also suﬃcient to show
that cautious inferences with probability sets may provide additional beneﬁts
when dealing with missing labels.
5 The conﬁdence intervals obtained on the experimental results are very small and we
therefore prefer not to display them in the ﬁgures in order not to overcharge them.
6 The supplementary results can be found in the online extend version [4].

Multi-label Chaining with Imprecise Probabilities
425
5
Conclusions
In this paper, we have proposed two new strategies to adapt the classical chain-
ing multi-label problem to the case of handling imprecise probability estimates.
Such strategies come with daunting challenges to obtain cautious and reliable
predictions, and have been successfully resolved using the NCC model.
While the NCC makes easy to solve the strategies thanks to its assumptions,
the same restrictive assumptions may also be the reason why the initial accuracy
is rather low (especially for Yeast). Indeed, it seems reasonable to think that
the independence assumptions somehow limit the beneﬁts of including label
dependencies information through the chaining. It seems therefore essential, in
future works, to investigate other classiﬁers as well as to solve optimisation issues
in a general or approximative way.
Another open issue is how we can use or extend the existing heuristics of
probabilistic classiﬁer approaches on our proposal strategies, such as epsilon-
approximate inference, A∗and beam search methods [11,13].
Acknowledgments. We wish to thank the reviewers of the ECSQARU conference,
especially one of them who was thorough, and made us rethink a lot of aspects (of
which only some are visible in the revised version).
References
1. Antonucci, A., Corani, G.: The multilabel Naive credal classiﬁer. Int. J. Approxi-
mate Reasoning 83, 320–336 (2017)
2. Augustin, T., Coolen, F.P., de Cooman, G., Troﬀaes, M.C.: Introduction to Impre-
cise Probabilities. John Wiley & Sons, Hoboken (2014)
3. Boutell, M.R., Luo, J., Shen, X., Brown, C.M.: Learning multi-label scene classiﬁ-
cation. Pattern Recogn. 37(9), 1757–1771 (2004)
4. Alarc´on, Y.C.C., Destercke, S.: Multi-label chaining with imprecise probabilities
(2021). https://arxiv.org/abs/2107.07443
5. De Cooman, G., Hermans, F.: Imprecise probability trees: bridging two theories of
imprecise probability. Artif. Intell. 172(11), 1400–1427 (2008)
6. Dembczynski, K., Cheng, W., H¨ullermeier, E.: Bayes optimal multilabel classiﬁca-
tion via probabilistic classiﬁer chains. In: ICML (2010)
7. Dembczy´nski, K., Waegeman, W., Cheng, W., H¨ullermeier, E.: On label depen-
dence and loss minimization in multi-label classiﬁcation. Mach. Learn. 5–45 (2012).
https://doi.org/10.1007/s10994-012-5285-8
8. Destercke, S.: Multilabel prediction with probability sets: the hamming loss case.
In: Laurent, A., Strauss, O., Bouchon-Meunier, B., Yager, R.R. (eds.) IPMU 2014.
CCIS, vol. 443, pp. 496–505. Springer, Cham (2014). https://doi.org/10.1007/978-
3-319-08855-6 50
9. Friedman, J., Hastie, T., Tibshirani, R.: The Elements of Statistical Learning.
Springer, New York Inc (2001). https://doi.org/10.1007/978-0-387-84858-7
10. F¨urnkranz, J., H¨ullermeier, E., Menc´ıa, E.L., Brinker, K.: Multilabel classiﬁcation
via calibrated label ranking. Mach. Learn. 73(2), 133–153 (2008)

426
Y. C. C. Alarc´on and S. Destercke
11. Kumar, A., Vembu, S., Menon, A.K., Elkan, C.: Beam search algorithms for multi-
label learning. Mach. Learn. 92(1), 65–89 (2013). https://doi.org/10.1007/s10994-
013-5371-6
12. Levi, I.: The enterprise of knowledge: an essay on knowledge, credal probability,
and chance. MIT Press, Cambridge (1983)
13. Mena, D., Monta˜n´es, E., Quevedo, J.R., del Coz, J.J.: An overview of inference
methods in probabilistic classiﬁer chains for multilabel classiﬁcation. Wiley Inter-
disc. Rev. Data Mining Knowl. Disc. 6(6), 215–230 (2016)
14. Pillai, I., Fumera, G., Roli, F.: Multi-label classiﬁcation with a reject option. Pat-
tern Recogn. 46(8), 2256–2266 (2013)
15. Read, J., Pfahringer, B., Holmes, G., Frank, E.: Classiﬁer chains for multi-label
classiﬁcation. Mach. Learn. 85(3), 333–359 (2011)
16. Taylor, S.J.: Introduction to Measure and Integration. CUP Archive (1973)
17. Tsoumakas, G., Katakis, I.: Multi-label classiﬁcation: an overview. Int. J. Data
Warehousing Mining (IJDWM) 3(3), 1–13 (2007)
18. Vu-Linh Nguyen, E.H.: Reliable multilabel classiﬁcation: prediction with partial
abstention. In: Thirty-Fourth AAAI Conference on Artiﬁcial Intelligence (2019)
19. Walley, P.: Statistical reasoning with imprecise Probabilities. Chapman and Hall,
London (1991)
20. Walley, P.: Inferences from multinomial data: learning about a bag of marbles. J.
Roy. Stat. Soc.: Ser. B (Methodol.) 58(1), 3–34 (1996)
21. Zaﬀalon, M.: The Naive credal classiﬁer. J. Stat. Plann. Infer. 105(1), 5–21 (2002)

Centroids of Credal Sets:
A Comparative Study
Enrique Miranda(B)
and Ignacio Montes
Department of Statistics and Operations Research,
University of Oviedo, Oviedo, Spain
{mirandaenrique,imontes}@uniovi.es
Abstract. We compare a number of diﬀerent notions of centroid of a
credal set: the Shapley value, that arises in the context of game the-
ory; the average of the extreme points; the incenter with respect to the
total variation distance between probability measures; and the limit of
a procedure of uniform contraction. We show that these four centers do
not coincide in general, give some suﬃcient conditions for their equality,
and analyse their axiomatic properties. Finally, we discuss brieﬂy how
to deﬁne a notion of centrality measure.
Keywords: Game solutions · Credal sets · 2-monotonicity ·
Probability intervals
1
Introduction
The elicitation of a probability measure within a convex set can be interesting
in a wide variety of contexts: we may consider for instance the core of a game in
coalitional game theory [9], and look for a game solution that provides a way to
divide the wealth between the players; we could also consider transformations
from imprecise to precise probabilities, as has been done for instance in the
context of possibility theory [10]; we might also consider inner approximations
of a credal set, so as to look for a more informative model that is compatible
with the existing information [16]; or we might also connect the problem with
that of transforming second order models with ﬁrst order ones [24].
In this paper, we examine several possibilities for determining a probability
measure that can be considered as the center of the credal set. After introducing
some preliminary notions in Sect. 2, in Sect. 3 we discuss the Shapley value, the
average of the extreme points, the incenter of the credal set with respect to the
total variation distance and the limit of the contractions of the credal set. These
four centroids are compared in Sect. 4, by showing that they need not coincide
in general and establishing suﬃcient conditions for their equality. In Sect. 5, we
compare the centroids in terms of a number of axiomatic properties; and in Sect. 6
Supported by project PGC2018-098623-B-I00. We thank Arthur Van Camp and the
anonymous reviewers for some helpful comments and discussion.
c
⃝Springer Nature Switzerland AG 2021
J. Vejnarov´a and N. Wilson (Eds.): ECSQARU 2021, LNAI 12897, pp. 427–441, 2021.
https://doi.org/10.1007/978-3-030-86772-0_31

428
E. Miranda and I. Montes
we discuss how these deﬁnitions may lead to a notion of centrality measure. Some
additional comments are given in Sect. 7. Due to space limitations, proofs have
been omitted.
2
Preliminary Concepts
Consider a ﬁnite possibility space X = {x1, . . . , xn}, and denote by P(X ) the
set of all probability measures on X . A closed and convex subset M of P(X )
is called a credal set. Taking lower and upper envelopes on events, it determines
a coherent lower and upper probability:
P(A) = min
P ∈M P(A),
P(A) = max
P ∈M P(A)
∀A ⊆X .
A lower probability can be equivalently represented using its M¨obius inverse:
m(A) =

B⊆A
(−1)|A\B|P(B)
∀A ⊆X ,
because P(A) = 
B⊆A m(B) for every A ⊆X .
More generally, we can consider gambles, which are functions f : X →R; the
set of all gambles on X shall be denoted by L . If for simplicity we use the same
symbol P to denote the expectation operator with respect to the probability
measure P, then for each gamble f ∈L , the credal set M determines a coherent
lower and upper prevision:
P(f) = min
P ∈M P(f),
P(f) = max
P ∈M P(f)
∀f ∈L .
In general, the credal set associated with a coherent lower prevision is not equiv-
alent to that of the coherent lower probability that is its restriction to events.
A suﬃcient condition for their equality is that P satisﬁes the property of 2-
monotonicity [5,22]:
P(f ∨g) + P(f ∧g) ≥P(f) + P(g)
∀f, g ∈L .
In the case of events, 2-monotonicity means that
P(A ∪B) + P(A ∩B) ≥P(A) + P(B)
∀A, B ⊆X .
3
Center Points of a Credal Set
Let us introduce the notions of centroid of a credal set we shall compare in this
paper.

Centroids of Credal Sets: A Comparative Study
429
3.1
The Shapley Value
One of the most popular notions of centroid of a credal set is the Shapley value. It
was introduced by Shapley [18,19] in the framework of coalitional game theory,
as a ‘fair’ procedure to distribute some wealth between the players. Later on, it
was rediscovered in the context of non-additive probabilities [7] and popularised
by Smets as the pignistic transformation of a belief function [21].
Deﬁnition 1. Given a credal set M with associated lower probability P, its
Shapley value is deﬁned as the probability measure associated with the following
distribution:
ΦM
1 ({x}) =

x/∈A
|A|!(n −|A| −1)!
n!

P(A ∪{x}) −P(A)

∀x ∈X .
(1)
It was proven by Smets [20] that, when P is a belief function, ΦM
1
can be
equivalently computed as
ΦM
1 ({x}) =

x∈A
m(A)
|A|
∀x ∈X .
More generally, when P is 2-monotone, it follows from [19] that the extreme
points of M (P), the set of dominating probability measures, are given by {Pσ |
σ ∈Sn}, where Sn denotes the set of permutations of {1, . . . , n}, and given
σ ∈Sn, Pσ is determined by the equations
Pσ

{xσ(1), . . . , xσ(i)}

= P

{xσ(1), . . . , xσ(i)}

∀i = 1, . . . , n;
then the Shapley value can be computed as
ΦM
1 ({x}) = 1
n!

σ∈Sn
Pσ({x})
∀x ∈X .
(2)
In fact, the above results can be extended to arbitrary lower probabilities:
Proposition 1. Let P : P(X ) →[0, 1] be a lower probability with M¨obius
inverse m, and let ΦM
1
be given by Eq. (1). Using the notation Pσ in Eq. (2), it
holds that:
ΦM
1 ({x}) =

x∈A
m(A)
|A|
=

σ∈Sn
Pσ({x})
n!
∀x ∈X .
While the Shapley value seems like a reasonable choice as a central point, it has
one important drawback: it is only guaranteed to belong to the credal set (i.e.,
we can only assure that ΦM
1
≥P) when the lower probability P of the credal
set is 2-monotone.
More generally, when the lower probability P of the credal set is not 2-
monotone, we can only assure that ΦM
1
dominates P for small cardinalities
(n ≤4), as showed by Baroni and Vicig [2, Prop. 5]. We refer to [12] for a study
of the consistency between the Shapley value and the lower probability.

430
E. Miranda and I. Montes
3.2
Vertex Centroid
The second possibility we consider in this paper is the average of the extreme
points of the credal set:
Deﬁnition 2. Let M be a credal set with a ﬁnite number of extreme points
{P1, . . . , Pk}. The vertex centroid [8] is deﬁned as the average of the extreme
points:
ΦM
2
=
k
i=1 Pi
k
.
While the deﬁnition above is only applicable in the case of polytopes, it is worth
remarking that these include in particular the credal sets M (P) associated with
a lower probability P that is coherent [25], and therefore also in the particular
cases of 2-monotonicity. Nevertheless, it is not always applicable for arbitrary
credal sets, that are associated with coherent lower previsions.
The above centroid is sometimes referred to as the ‘center of gravity’ of
the credal set; however, strictly speaking the center of gravity corresponds to
the expectation of the set over a uniform probability distribution, and not only
over its extreme points. It is not diﬃcult to show that this other notion does not
necessarily produce the same centroid as Deﬁnition 2. While the center of gravity
has the advantage of being applicable over general credal sets, and not only on
polytopes, it also has the drawback of being computationally more expensive
(see for example [8]).
It follows from Deﬁnition 2 that ΦM
2
always belongs to the credal set M ;
considering the comments in the previous section, this implies that it need not
coincide with Shapley value when the lower envelope P of M is not 2-monotone.
As we shall see later on, they need not coincide even under 2-monotonicity:
while it has been proven in [19] that the extreme points of M (P) are indeed
{Pσ | σ ∈Sn}, a key diﬀerence is that in the computation of Shapley value
in Eq. (2) we are allowing for repetitions of the same extreme point, while in
Deﬁnition 2 we do not.
3.3
Incenter
Our third approach consists in considering the incenter of the credal set, which is
the element or elements P ∈M for which we can include the largest ball centered
on P in the interior of the credal set. This notion requires the speciﬁcation of
a distance between probability measures from which the balls are deﬁned. We
consider here the total variation distance [11], which is the one associated with
the supremum norm:
dT V (P, Q) = max
A⊆X |P(A) −Q(A)|.
Our choice of this distance is due to the fact that a ball with the total variation
distance is always a polytope, unlike some other distances such as the Euclidean
distance. To ease the notation, we will simply denote dT V by d.

Centroids of Credal Sets: A Comparative Study
431
Deﬁnition 3. Let M be a credal set. The supremum radius of M is deﬁned as
αI = sup

α | ∃P0 ∈M such that int

Bα
d (P0)

⊆int(M )

.
(3)
Then any P ∈M such that int

BαI
d (P)

⊆int(M ) is called an incenter of the
credal set M . The set of all such P is denoted ΨM
3 .
The reason why we are requiring the inclusion int

BαI
d (P0)

⊆int(M ) in the
above deﬁnition is that otherwise we could obtain the counter-intuitive result
that an incenter belongs to the boundary of M , which we ﬁnd incompatible
with the underlying idea of centrality (as in ‘deepest inside the credal set’) we
consider in this paper.
Two natural questions related to the incenter is whether (i) it exists, and (ii)
it is unique. Our next result provides an answer to the ﬁrst question.
Proposition 2. Consider a credal set M . Then the value αI in Eq. (3) is a
maximum. As a consequence, the set ΨM
3
is always non-empty, and any of its
elements belongs to int(M ).
Concerning the second question, the set ΦM
3
may have more than one element,
as we show next:
Example 1. Consider the possibility space X = {x1, x2, x3}, and the credal set
M determined by P({x1}) ∈[0.5, 0.8], P({x2}) ∈[0.1, 0.25] and P({x3}) ∈
[0.1, 0.35]. It holds that αI = 0.075 and that the incenter is not unique:
ΨM
3
coincides with the convex combinations of Q1 = (0.575, 0.175, 0.25) and
Q2 = (0.65, 0.175, 0.175). Hence, there are inﬁnite incenters. Figure 1 shows
the graphical representation of M as well as the balls BαI
d (Q1), BαI
d (Q2) and
BαI
d (Qβ) for β = 0.5. ♦
Fig. 1. Credal set M in Example 1, the incenters Q1 and Q2 with the ball they induce
(left hand-side ﬁgure) and Qβ for β = 0.5 with the ball it induces (right hand-side
ﬁgure).

432
E. Miranda and I. Montes
Related to this approach, we may alternatively have considered the circumcenter
of the credal set, which is the element or elements for which the smallest ball
centered on them includes the credal set. That is, we may consider
αC = inf

α | ∃P0 ∈M such that Bα
d (P0) ⊇M
	
,
and then let Ψ
′M
3
be the set of those P such that BαC
d (P) ⊇M . However, this
approach possesses the two drawbacks we have discussed so far: not only it need
not lead to a unique solution, but also [1] it may produce values that are outside
the credal set.
In this sense, and as suggested by a reviewer, one possibility would be to
consider, for each P ∈M , the value αP = inf

α | Bα
d (P) ⊇M
	
, and then call
P ∈M a circumcenter of M if it minimises the value αP . In this manner we
would guarantee that the solution obtained belongs to the credal set, although
it may be an element of the boundary. The study of this approach is left as an
open problem.
3.4
Contraction Centroid
Our fourth approach is motivated by the lack of uniqueness of the incenter.
Consider a credal set M determined by a ﬁnite number of constraints. This
means that there are two (disjoint) sets of gambles L > and L = such that the
lower and upper previsions P, P associated with M satisfy P(f) = P(f) for any
f ∈L = and P(f) < P(f) for any f ∈L >, and that the credal set can be
expressed as:
M =

P ∈P(X ) | P(f) = P(f) ∀f ∈L =,
P(f) ≥P(f) ∀f ∈L >	
.
(4)
Note that we can assume without loss of generality that {IA | A ⊆X } ⊆
L = ∪L >; in that case when L > is empty we obtain that P(A) = P(A) for
any A ⊆X , and thus that M has only one element.
The idea of this fourth approach is to contract M in a uniform manner as long
as we can, and then proceed iteratively reducing the cardinality of L >. More
speciﬁcally, we increase the value of the lower prevision in a constant amount α
in all the gambles f ∈L >. Our next proposition gives further insight into this
idea:
Proposition 3. Let M be a credal set determined by a ﬁnite number of gambles
L =, L > by means of Eq. (4). For a given α > 0, let:
Mα =

P ∈P(X ) | P(f) = P(f) ∀f ∈L =,
P(f) ≥P(f) + α ∀f ∈L >	
.
(5)
Then the Λ = {α | Mα ̸= ∅} is non-empty and has a maximum value αS =
max Λ. Moreover, there is some f ∈L > such that P(f) is constant for any
P ∈MαS.
The above proposition also tells us that when we get to the set MαS the size of
L > decreases; we can therefore iterate the procedure, now starting with MαS,

Centroids of Credal Sets: A Comparative Study
433
and after a ﬁnite number of steps we end up with a precise credal set: one formed
by a single probability measure that we shall call the contraction centroid. This
means that after a ﬁnite number of steps we obtain the values α1
S, . . . , αl
S and
the chain of nested credal sets:
M ⊃Mα1
S ⊃. . . ⊃Mαl
S =

ΦM
4
	
.
(6)
Example 2. Consider again the credal set from Example 1. Then L = is empty
and L > contains six gambles, that coincide with the indicator functions of the
non-trivial events. Let us see that α1
S = max Λ = 0.075. On the one hand, Mα1
S
is non-empty because the probability P = (0.625, 0.175, 0.2) belongs to Mα1
S.
On the other hand, if we increase the lower probability in a quantity α, to keep
coherence it should happen that:
1 ≥

P({x2}) + α

+

P({x1, x3}) + α

= 0.85 + 2α,
so α ≤0.075. Therefore, α1
S = 0.075, and this gives rise to the following credal
set:
Mα1
S =

P ∈P(X ) | P(A) ≥P(A) + α1
S ∀A ̸= ∅, X
	
.
If we denote by P 1 its associated lower probability, it is given by:
P 1({x1}) = 0.575,
P 1({x2}) = 0.175,
P 1({x3}) = 0.175,
P 1({x1, x2}) = 0.75,
P 1({x1, x3}) = 0.825,
P 1({x2, x3}) = 0.35.
In this second step, L =
1
=

I{x2}, I{x1,x3}
	
and L >
1
=

I{x1}, I{x3},
I{x1,x2}, I{x2,x3}
	
, i.e., there are two events whose probability is now ﬁxed.
Iterating the procedure, we obtain α2
S = 0.0375 and Mα2
S =

ΦM
4
	
, where
ΦM
4
= (0.6125, 0.175, 0.2125). Figure 2 shows Mα1
S (in blue) and Mα2
S (in red).
♦
Fig. 2. Graphical representation of the credal sets Mα1
S in blue and Mα2
S in red. (Color
ﬁgure online)

434
E. Miranda and I. Montes
It is worth mentioning that the lower envelope of the credal set Mα in Eq. (5)
does not necessarily coincide with P + α. While by construction it dominates
this lower probability, they may not agree on some events because P + α need
not be coherent. The lower envelope of Mα corresponds to the natural extension
[23] of P + α.
4
Relationships Between the Centroids
In our previous section we have introduced four diﬀerent notions of the center of
a credal set. Let us begin by showing that these four notions are indeed diﬀerent:
Example 3. Consider the credal set in Example 1; there, we gave the set of
incenters ΨM
3 , while the contraction centroid ΦM
4
was given in Example 2. The
extreme points of the credal set M are given by:
x1
x2
x3
P1 0.55 0.1 0.35
P2
0.5
0.15 0.35
P3
0.5
0.25 0.25
x1
x2
x3
P4 0.65 0.25 0.1
P5
0.8
0.1 0.1
It follows that the average of the extreme points and the Shapley value are given
by ΦM
2
= (0.6, 0.17, 0.23) and ΦM
1
= (0.63, 0.1583, 0.2083). We conclude, taking
into account also Examples 1 and 2, that the four approaches lead to diﬀerent
results.
♦
While the four approaches do not lead to the same solution in general, in the
following subsections we give some suﬃcient conditions for their equality.
4.1
Probability Intervals
A probability interval [4] is an uncertainty model that gives lower and upper
bounds to the probability of the singletons: I =

[li, ui] | li ≤ui ∀i = 1, . . . , n
	
.
It determines a credal set given by:
M (I ) =

P ∈P(X ) | li ≤P({xi}) ≤ui ∀i = 1, . . . , n
	
.
Taking lower and upper envelopes of M (I ) we obtain a lower and upper pre-
vision, and the probability interval is called coherent when P({xi}) = li and
P({xi}) = ui for every i = 1, . . . , n; in that case, the values of P for events can
be easily computed using the results in [4]. When the credal set is determined
by a coherent probability interval, we can give an explicit formula for the value
αS in the contraction method.
Proposition 4. Let M be a credal set determined by a coherent probability
interval I = {[li, ui] | ∀i = 1, . . . , n}. Let I > =

i ∈{1, . . . , n} | li < ui
	
and
I = =

i ∈{1, . . . , n} | li = ui
	
. Then:

Centroids of Credal Sets: A Comparative Study
435
1. The value αS = max Λ is given by:
αS = min

1
|I >|

1 −
n

i=1
li

,
1
|I >|
 n

i=1
ui −1

, 1
2 min
i∈I >(ui −li)

.
2. If αS =
1
|I >|

1 −
i=1,...,n li

or αS =
1
|I >|
 
i=1,...,n ui −1

, then MαS =

ΦM (I )
4
	
.
From Proposition 4 we can also deduce an explicit formula for the value αS for
the Linear Vacuous (LV) and the Pari Mutuel Model (PMM), which constitute
particular instances of distortion models [14,15] and nearly linear models [3]. The
PMM [13,17,23] is determined by the coherent lower probability P P MM(A) =
max

(1+δ)P0(A)−δ, 0
	
for any A ⊆X , where P0 ∈P(X ) is a given probability
measure and δ > 0. Similarly, the LV [23] is deﬁned by the coherent lower
probability given by P LV (A) = (1−δ)P0(A), for any A ⊂X and P LV (X ) = 1,
where P0 ∈P(X ) and δ ∈(0, 1). Both the PMM and the LV are instances of
probability intervals, where:
IP MM =

max{(1 + δ)P0({xi}) −δ, 0}, min{(1 + δ)P0({xi}), 1}
  i = 1, . . . , n

,
ILV =

(1 −δ)P0({xi}), min{(1 −δ)P0({xi}) + δ, 1}
  i = 1, . . . , n

.
Thus we can apply Proposition 4 for computing the value αS. In fact, when both
P0 and the lower probability only take the values 0 and 1 for the impossible
and sure events, respectively, the computation of αS can be simpliﬁed and the
procedure of contracting the credal set ﬁnishes in only one step.
Corollary 1. Consider a credal set M associated with either a PMM P P MM
or a LV P LV determined by P0 ∈P(X ) and the distortion parameter δ. Assume
that P0(A), P P MM(A) and P LV (A) belong to (0, 1) for any A ̸= ∅, X . Then:
1. For the PMM, αS = max Λ =
δ
n and Mα =

ΦM
4
	
where ΦM
4 ({xi}) =
(1 + δ)P0({xi}) −δ
n for any i = 1, . . . , n.
2. For the LV, αS = max Λ =
δ
n and Mα =

ΦM
4
	
where ΦM
4 ({xi}) = (1 −
δ)P0({xi}) + δ
n for any i = 1, . . . , n.
3. In both cases, there is a unique incenter (i.e., ΨM
3
= {ΦM
3 }) and ΦM
1
=
ΦM
2
= ΦM
3
= ΦM
4 .
In this respect, it is worth remarking that (i) the good behaviour of these two
distortion models is in line of other desirable properties they possess, as discussed
in [6,14,15]; and (ii) the centroid of the LV and PMM models does not coincide
with P0, because the distortion is not done uniformly in all directions of the
simplex. This was already shown in [12, Sect. 4.3] for the particular case of the
Shapley value.

436
E. Miranda and I. Montes
4.2
Connections Between the Incenter and the Contraction
Centroid
Next we prove a connection between contraction centroid and the set of incenters.
Proposition 5. Let M be a credal set included in int(P(X )) with L = = ∅.
For any α ≤αS, if P0 ∈Mα, then Bα
d (P0) ⊆M . When M is determined by
its restriction to events, then the converse also holds. As a consequence, in that
case MαS = ΨM
3 .
From this result we deduce that, when M is associated with a coherent lower
probability, the credal set MαS obtained contracting the initial credal set coin-
cides with the set of incenters. However, when the credal set is associated with
a lower prevision, this equivalence does not hold in general, as the next example
shows.
Example 4. Consider X = {x1, x2}, the gamble f given by f(x1) = 1 and
f(x2) = 0.9 and consider the credal set M given by:
M = {P ∈P(X ) | 0.91 ≤P(f) ≤0.99}.
Equivalently, this credal set is determined by the extreme points P1 = (0.1, 0.9)
and P2 = (0.9, 0.1). Consider now P0 = (0.2, 0.8) and α = 0.015. Then, the ball
Bα
d (P0) has two extreme points, Q1 = (0.185, 0.815) and Q2 = (0.215, 0.785), so
Bα
d (P0) ⊆M . However, P0(f) = 0.92 /∈[0.925, 0.975] = [P(f) + α, P(f) −α]. ♦
5
Properties of the Centroids
Next we compare the diﬀerent centroids in terms of the axiomatic properties they
satisfy. In this respect, it is worth recalling that Shapley value was characterised
in the context of coalitional game theory as the unique probability distribution
satisfying the following axioms:
Eﬃciency: n
i=1 ΦM ({xi}) = 1.
Symmetry: P

A ∪{xi}

= P

A ∪{xj}

for any A ⊆X \ {xi, xj} implies that
ΦM ({xi}) = ΦM ({xj}).
Linearity: ΦM (λ1P 1+λ2P 2) = λ1ΦM (P 1) + λ2ΦM (P 2) for any λ1, λ2 ∈R and
every P 1, P 2.
Null player: P

A∪{xi}

= P(A) for any A ⊆X \{xi} implies ΦM ({xi}) = 0.
Let us study these properties for the other centroids considered in this paper.
Note that in the case of the incenter, they should be required to any ΦM
3
∈ΨM
3 .
In this respect, since in the framework of this paper any center of a credal
set shall be a probability measure, the eﬃciency property is trivially satisﬁed.
With respect to the other properties, it is not diﬃcult to establish the following:
Proposition 6. ΦM
2 , ΦM
4
and any ΦM
3
∈ΨM
3
satisfy the symmetry and null-
player properties, but none of them satisﬁes linearity.

Centroids of Credal Sets: A Comparative Study
437
Next we consider other desirable properties of a centroid.
Deﬁnition 4. Let ΦM be a centroid of a credal set M . We say that it satisﬁes:
• Consistency if ΦM ∈M .
• Continuity if for any ε > 0, there exists δ > 0 such that d(P 1, P 2) :=
maxA⊆X |P 1(A) −P 2(A)| < δ implies d

ΦM (P 1), ΦM (P 2)
< ε.
• Ignorance preservation if M = P(X ) implies that ΦM is the uniform distri-
bution.
When dealing with the incenter, the previous properties should be slightly
rewritten due to its lack of uniqueness: the incenter satisﬁes consistency when
ΨM
3
⊆M ; it satisﬁes ignorance preservation if M = P(X ) implies that the
only element of ΨM
3
is the uniform distribution; and it satisﬁes continuity when
for any ε > 0 there exists some δ > 0 such that d(P 1, P 2) < δ implies that
d(Q1, Q2) < ε, where Q1 and Q2 are the lower envelopes of Ψ
M (P 1)
3
and Ψ
M (P 2)
3
.
Proposition 7. 1. ΦM
1
satisﬁes continuity and ignorance preservation, but it
does not satisfy consistency.
2. ΦM
2
satisﬁes consistency and ignorance preservation, but not continuity.
3. ΨM
3 , ΦM
4
satisfy consistency, continuity and ignorance preservation.
The next table summarises the results from this section:
Shapley Average of Incenter Contraction
value
extremes
(dT V )
center
Eﬃciency
YES
YES
YES
YES
Symmetry
YES
YES
YES
YES
Linearity
YES
NO
NO
NO
Null player
YES
YES
YES
YES
Consistency
NO
YES
YES
YES
Continuity
YES
NO
YES
YES
Ignorance preservation YES
YES
YES
YES
Finally, it is worth remarking on the ability of these centroids to distinguish
between lower previsions and lower probabilities: in general, a credal set M
determines, by means of lower envelopes, a lower prevision P on a gambles and a
lower probability by considering its restriction to events. However, the credal set
M ′ determined by the latter, given by M ′ := {P | P(A) ≥P(A) ∀A ⊆X } is
in general a superset of the original credal set M . It would be desirable then that
the centroids of M and M ′ do not necessarily coincide, since they correspond
to diﬀerent credal sets. In this respect, it is not diﬃcult to show that ΨM
3 , ΦM
4
are capable of distinguishing between lower previsions and lower probabilities,
and so does ΦM
2
(with the restriction that it is only applicable on polytopes).
On the other hand, the Shapley value is only deﬁned via the lower probability,
and so it does not distinguish between lower probabilities and lower previsions.

438
E. Miranda and I. Montes
6
Centrality Measures
More generally, instead of determining which element of the credal set can be
considered its center, we may deﬁne a centrality measure, that allows us to
quantify how deep in the interior of a credal set an element is. Consider for
instance the same credal set as in Example 1, depicted in Fig. 3. Intuitively, given
the probability measures Q1 = (0.75, 0.125, 0.125) and Q2 = (0.65, 0.15, 0.2),
emphasised in red in Fig. 3, Q2 should have a greater centrality degree than Q1.
Fig. 3. Graphical representation of the credal set M in Example 1 (Color ﬁgure online)
This simple example suggests the following deﬁnition of centrality measure.
Deﬁnition 5. Given a credal set M , a centrality measure is a function ϕ :
P(X ) →[0, 1] satisfying the following properties:
CM1 ϕ(P) = 0 for every P /∈M .
CM2 If P ∈ext(M ), then ϕ(P) = 0.
CM3 There exists a unique P0 ∈M satisfying ϕ(P0) = 1. Such P0 is called
central point in M with respect to ϕ.
CM4 Consider P ∈ext(M ), P0 the probability given in the previous item and
λ, β ∈[0, 1] such that λ ≥β. Given P1 = λP + (1 −λ)P0 and P2 = βP + (1 −
β)P0, it holds that ϕ(P1) ≤ϕ(P2).
The idea underlying these properties is the following: CM1 tells us that an
element outside the credal set should have degree of centrality zero; from CM2,
the same should hold for the extreme points of the credal set; CM3 means that
there is a unique probability P0 with degree of centrality 1; ﬁnally, property
CM4 represents the idea that the closer a probability is to P0, the greater its
degree of centrality.
A centrality measure ϕ allows to deﬁne a chain of credal sets {Mα}α∈[0,1],
where Mα is formed by the probabilities with centrality degree of at least α:
Mα = {P ∈M | ϕ(P) ≥α}
∀α ∈[0, 1].

Centroids of Credal Sets: A Comparative Study
439
We next discuss two possible strategies for deﬁning a centrality measure. The
former consists in considering a centroid of the credal set, and to measure the
distance with respect to it. It requires then to specify both the centroid and
the distance. Out of the options considered in the previous section, we would
reject ΦM
1
due to the lack of consistency and ΨM
3
because of non-uniqueness.
With respect to the distance, we will be considering here the total variation,
although it would also be possible to consider other options such as the L1 or
the Euclidean distances.
In this sense, if we let ΦM be this centroid and take β = min

d

ΦM , Pi

:
Pi ∈ext(M )

, then we can deﬁne
ϕ1(P) = 1 −min

d

P, ΦM 
β
, 1

.
(7)
A second approach would consist in considering directly a chain {Mα}α∈[0,1]
of convex credal sets such that M0 := M , M1 is a singleton determining the
central point ΦM and where Mα is included in the interior of M for any α > 0,
and letting
ϕ2(P) = sup

α ∈[0, 1] | P ∈Mα

.
(8)
The chain {Mα}α∈[0,1] of credal sets could be deﬁned, for example, as:
Mα = CH

(1 −α)P + αΦM | P ∈ext(M )
	
∀α ∈[0, 1].
Let us show that both approaches lead to a centrality measure.
Proposition 8. Let M be a credal set, and let ϕ1, ϕ2 be given by Eqs. (7)
and (8). Then ϕ1 and ϕ2 satisfy conditions (CM1)–(CM4).
It is also possible to deﬁne a centrality measure by considering the chain of credal
sets from Eq. (6). For this, note that for each P ∈M there is j ∈{1, . . . , l} such
that P ∈Mαj−1
S
\ Mαj
S. Also, there is α ∈Λj−1 such that P ∈

Mj−1

α, but
P /∈

Mj−1

α+ε for any ε > 0. Then we let:
ϕ3(P) = α1
S + . . . + αj−1
S
+ α
α1
S + . . . + αl
S
.
(9)
Proposition 9. The function ϕ3 deﬁned in Eq. (9) satisﬁes conditions (CM1)–
(CM4).
7
Conclusions
In this paper, we have analysed four diﬀerent deﬁnitions of centroid for a credal
set. These could serve as a representative of the credal set or as a game solution

440
E. Miranda and I. Montes
when the credal set is interpreted as the core of a cooperative game. We have
analysed their diﬀerences, the connection between them as well as their axiomatic
properties. Also, we have seen that the problem could be tackled by deﬁning a
centrality measure whose unique modal point is interpreted as the centroid.
While the above results give some overview of the properties of the centroids
of a credal set, there is still much work to be done in order to have a full picture
of this problem. On the one hand, we could consider other possibilities in the
context of game solutions, such as the Banzhaf value, or other alternatives to the
total variation distance, such as the Euclidean distance or the Kullback-Leibler
divergence; secondly, it would be interesting to obtain further conditions for the
equality between some of these centroids; and ﬁnally, a deeper study of centrality
measures and their axiomatic properties would be of interest. We intend to tackle
these problems in the near future.
References
1. Bader, U., Gelander, T., Monod, N.: A ﬁxed point theorem for L1 spaces. Invent.
Math. 189, 143–148 (2012)
2. Baroni, P., Vicig, P.: An uncertainty interchange format with imprecise probabili-
ties. Int. J. Approximate Reasoning 40, 147–180 (2005)
3. Corsato, C., Pelessoni, R., Vicig, P.: Nearly-Linear uncertainty measures. Int. J.
Approximate Reasoning 114, 1–28 (2019)
4. de Campos, L.M., Huete, J.F., Moral, S.: Probability intervals: a tool for uncertain
reasoning. Internat. J. Uncertain. Fuzziness Knowl.-Based Syst. 2, 167–196 (1994)
5. de Cooman, G., Troﬀaes, M.C.M., Miranda, E.: n-Monotone exact functionals. J.
Math. Anal. Appl. 347, 143–156 (2008)
6. Destercke, S., Montes, I., Miranda, E.: Processing multiple distortion models: a
comparative study. In: Proceedings of the 12th ISIPTA Conference. PMLR 147,
126–135 (2021)
7. Dubois, D., Prade, H.: Fuzzy Sets and Systems. Theory and Applications. Aca-
demic Press, New York (1980)
8. Elbassioni, K., Tiway, H.R.: Complexity of approximating the vertex centroid of a
polyhedron. Theoret. Comput. Sci. 421, 56–61 (2012)
9. Grabisch, M.: Set Functions, Games and Capacities in Decision Making. Springer
(2016). https://doi.org/10.1007/978-3-319-30690-2
10. Klir, G.J., Parviz, B.: Probability-possibility transformations: a comparison. Int.
J. Gen. Syst. 21, 291–310 (1992)
11. Levin, D.A., Peres, Y., Wilmer, E.L.: Markov Chains and Mixing Times. American
Mathematical Society (2009)
12. Miranda, E., Montes, I.: Shapley and Banzhaf values as probability transforma-
tions. Internat. J. Uncertain. Fuzziness Knowl.-Based Syst. 26(6), 917–947 (2018)
13. Montes, I., Miranda, E., Destercke, S.: Pari-mutuel probabilities as an uncertainty
model. Inf. Sci. 481, 550–573 (2019)
14. Montes, I., Miranda, E., Destercke, S.: Unifying neighbourhood and distortion
models: part I- New results on old models. Int. J. Gen. Syst. 49(6), 602–635 (2020)
15. Montes, I., Miranda, E., Destercke, S.: Unifying neighbourhood and distortion
models: part II- New models and synthesis. Int. J. Gen. Syst. 49(6), 636–674
(2020)

Centroids of Credal Sets: A Comparative Study
441
16. Montes, I., Miranda, E., Vicig, P.: 2-monotone outer approximations of coherent
lower probabilities. Int. J. Approximate Reasoning 101, 181–205 (2018)
17. Pelessoni, R., Vicig, P., Zaﬀalon, M.: Inference and risk measurement with the
Pari-mutuel model. Int. J. Approximate Reasoning 51, 1145–1158 (2010)
18. Shapley, L.S.: A value for n-person games. Ann. Math. Stud. 28, 307–317 (1953)
19. Shapley, L.S.: Cores of convex games. Internat. J. Game Theory 1, 11–26 (1971)
20. Smets, P.: Decision making in the TBM: the necessity of the Pignistic transforma-
tion. Int. J. Approximate Reasoning 38, 133–147 (2005)
21. Smets, P., Kennes, R.: The transferable belief model. Artif. Intell. 66(2), 191–234
(1994)
22. Walley, P.: Coherent lower (and upper) probabilities. Statistics Research Report
22, University of Warwick, Coventry (1981)
23. Walley, P.: Statistical Reasoning with Imprecise Probabilities. Chapman and Hall,
London (1991)
24. Walley, P.: Statistical inferences based on a second-order possibility distribution.
Int. J. Gen. Syst. 26, 337–383 (1997)
25. Wallner, A.: Extreme points of coherent probabilities in ﬁnite spaces. Int. J.
Approximate Reasoning 44(3), 339–357 (2007)

The Smallest Probability Interval
a Sequence Is Random for: A Study
for Six Types of Randomness
Floris Persiau, Jasper De Bock(B), and Gert de Cooman(B)
Foundations Lab for imprecise probabilities, Ghent University, Ghent, Belgium
{floris.persiau,jasper.debock,gert.decooman}@ugent.be
Abstract. There are many randomness notions. On the classical
account, many of them are about whether a given inﬁnite binary sequence
is random for some given probability. If so, this probability turns out to
be the same for all these notions, so comparing them amounts to ﬁnding
out for which of them a given sequence is random. This changes com-
pletely when we consider randomness with respect to probability inter-
vals, because here, a sequence is always random for at least one interval,
so the question is not if, but rather for which intervals, a sequence is
random. We show that for many randomness notions, every sequence
has a smallest interval it is (almost) random for. We study such smallest
intervals and use them to compare a number of randomness notions. We
establish conditions under which such smallest intervals coincide, and
provide examples where they do not.
Keywords: Probability intervals · Martin-L¨of randomness ·
Computable randomness · Schnorr randomness · Church randomness
1
Introduction
The ﬁeld of algorithmic randomness studies what it means for an inﬁnite binary
sequence, such as ω = 0100110100 . . . , to be random for an uncertainty model.
Classically, this uncertainty model is often a single (precise) probability p ∈[0, 1].
Some of the best studied precise randomness notions are Martin-L¨of randomness,
computable randomness, Schnorr randomness and Church randomness. They are
increasingly weaker; for example, if a sequence ω is Martin-L¨of random for a
probability p, then it is also computably random, Schnorr random and Church
random for p. Meanwhile, these notions do not coincide; it is for example possible
that a path ω is Church random but not computably random for 1/2. From a
traditional perspective, this is how we can typically diﬀerentiate between various
randomness notions [1,2,6,10].
As shown by De Cooman and De Bock [3–5], these traditional randomness
notions can be generalised by allowing for imprecise-probabilistic uncertainty
models, such as closed probability intervals I ⊆[0, 1]. These more general ran-
domness notions, and their corresponding properties, allow for more detail to
c
⃝Springer Nature Switzerland AG 2021
J. Vejnarov´a and N. Wilson (Eds.): ECSQARU 2021, LNAI 12897, pp. 442–454, 2021.
https://doi.org/10.1007/978-3-030-86772-0_32

The Smallest Probability Interval a Sequence Is Random for
443
arise in their comparison. Indeed, every inﬁnite binary sequence ω is for exam-
ple random for at least one closed probability interval. And for the imprecise
generalisations of many of the aforementioned precise randomness notions, we
will see that for every (or sometimes many) ω, there is some smallest probabil-
ity interval, be it precise or imprecise, that ω is (almost) random for—we will
explain the modiﬁer ‘almost’ further on. It is these smallest probability intervals
we will use to compare a number of diﬀerent randomness notions.
We will focus on the following three questions: (i) when is there a well-deﬁned
smallest probability interval for which an inﬁnite binary sequence ω is (almost)
random; (ii) are there alternative expressions for these smallest intervals; and
(iii) for a given sequence ω, how do these smallest intervals compare for diﬀerent
randomness notions? Thus, by looking from an imprecise perspective, we are
able to do more than merely conﬁrm the known diﬀerences between several
randomness notions. Deﬁning randomness for closed probability intervals also
lets us explore to what extent existing randomness notions are diﬀerent, in the
sense that we can compare the smallest probability intervals for which an inﬁnite
binary sequence is random. Surprisingly, we will see that there is a large and
interesting set of inﬁnite sequences ω for which the smallest interval that ω is
(almost) random for is the same for several randomness notions.
Our contribution is structured as follows. In Sect. 2, we introduce (im)pre-
cise uncertainty models for inﬁnite binary sequences, and introduce a generic
deﬁnition of randomness that allows us to formally deﬁne what it means for a
sequence to have a smallest interval it is (almost) random for. In Sect. 3, we pro-
vide the mathematical background on supermartingales that we need in order
to introduce a number—six in all—of diﬀerent randomness notions in Sect. 4:
(weak) Martin-L¨of randomness, computable randomness, Schnorr randomness,
and (weak) Church randomness. In the subsequent sections, we tackle our three
main questions. We study the existence of the smallest intervals an inﬁnite binary
sequence ω is (almost) random for in Sect. 5. In Sects. 6 and 7, we provide alter-
native expressions for such smallest intervals and compare them; we show that
these smallest intervals coincide under certain conditions, and provide examples
where they do not. To adhere to the page limit, the proofs of all novel results—
that is, the ones without citation—are omitted. They are available in Appendix
B of an extended on-line version [8].
2
Forecasting Systems and Randomness
Consider an inﬁnite sequence of binary variables X1, . . . , Xn, . . . , where every
variable Xn takes values in the binary sample space X := {0, 1}, generically
denoted by xn. We are interested in the corresponding inﬁnite outcome sequences
(x1, . . . , xn, . . . ), and, in particular, in their possible randomness. We denote such
a sequence generically by ω and call it a path. All such paths are collected in the
set Ω := X N.1 For any path ω = (x1, . . . , xn, . . . ) ∈Ω, we let ω1:n := (x1, . . . , xn)
1 N denotes the natural numbers and N0 := N ∪{0} denotes the non-negative integers
(A real x ∈R is called negative, positive, non-negative and non-positive, respectively,
if x < 0, x > 0, x ≥0 and x ≤0).

444
F. Persiau et al.
and ωn := xn for all n ∈N. For n = 0, the empty sequence ω1:0 := ω0 := () is
called the initial situation and is denoted by □. For any n ∈N0, a ﬁnite outcome
sequence (x1, . . . , xn) ∈X n is called a situation, also generically denoted by s,
and its length is then denoted by |s| := n. All situations are collected in the
set S := 
n∈N0 X n. For any s = (x1, . . . , xn) ∈S and x ∈X, we use sx to denote
the concatenation (x1, . . . , xn, x).
The randomness of a path ω ∈Ω is always deﬁned with respect to an uncer-
tainty model. Classically, this uncertainty model is a real number p ∈[0, 1],
interpreted as the probability that Xn equals 1, for any n ∈N. As explained in
the Introduction, we can generalise this by considering a closed probability inter-
val I ⊆[0, 1] instead. These uncertainty models will be called interval forecasts,
and we collect all such closed intervals in the set I. Another generalisation of the
classical case consists in allowing for non-stationary probabilities that depend
on s or |s|. Each of these generalisations can themselves be seen as a special
case of an even more general approach, which consists in providing every situ-
ation s ∈S with a (possibly diﬀerent) interval forecast in I, denoted by ϕ(s).
This interval forecast ϕ(s) ∈I then describes the uncertainty about the a priori
unknown outcome of X|s|+1, given that the situation s has been observed. We
call such general uncertainty models forecasting systems.
Deﬁnition 1. A forecasting system is a map ϕ: S →I that associates with
every situation s ∈S an interval forecast ϕ(s) ∈I. We denote the set of all
forecasting systems by Φ.
With any forecasting system ϕ ∈Φ, we associate two real processes ϕ and ϕ,
deﬁned by ϕ(s) := min ϕ(s) and ϕ(s) := max ϕ(s) for all s ∈S. A forecasting
system ϕ ∈Φ is called precise if ϕ = ϕ. A forecasting system ϕ ∈Φ is called
stationary if there is an interval forecast I ∈I such that ϕ(s) = I for all s ∈S;
for ease of notation, we will then denote this forecasting system simply by I.
The case of a single probability p corresponds to a stationary forecasting system
with I = {p}. A forecasting system ϕ ∈Φ is called temporal if its interval
forecasts ϕ(s) only depend on the situations s ∈S through their length |s|,
meaning that ϕ(s) = ϕ(t) for any two situations s, t ∈S that have the same
length |s| = |t|.
In some of our results, we will consider forecasting systems that are com-
putable. To follow the argumentation and understand our results, the following
intuitive description will suﬃce: a forecasting system ϕ ∈Φ is computable if
there is some ﬁnite algorithm that, for every s ∈S and any n ∈N0, can compute
the real numbers ϕ(s) and ϕ(s) with a precision of 2−n. For a formal deﬁnition
of computability, which we use in our proofs, we refer the reader to Appendix A
of the extended on-line version, which contains these proofs [8].
So what does it mean for a path ω ∈Ω to be random for a forecasting
system ϕ ∈Φ? Since there are many diﬀerent deﬁnitions of randomness, and
since we intend to compare them, we now introduce a general abstract deﬁnition
and a number of potential properties of such randomness notions that will, as it
turns out, allow us to do so.

The Smallest Probability Interval a Sequence Is Random for
445
Deﬁnition 2. A notion of randomness R associates with every forecasting sys-
tem ϕ ∈Φ a set of paths ΩR(ϕ). A path ω ∈Ω is called R-random for ϕ if
ω ∈ΩR(ϕ).
All of the randomness notions that we will be considering further on, satisfy
additional properties. The ﬁrst one is a monotonicity property, which we can
describe generically as follows. If a path ω ∈Ω is R-random for a forecasting
system ϕ ∈Φ, it is also R-random for any forecasting system ϕ′ ∈Φ that
is less precise, meaning that ϕ(s) ⊆ϕ′(s) for all s ∈S. Consequently, this
monotonicity property requires that the more precise a forecasting system is,
the fewer R-random paths it ought to have.
Property 1. For any two forecasting systems ϕ, ϕ′ ∈Φ such that ϕ ⊆ϕ′, it holds
that ΩR(ϕ) ⊆ΩR(ϕ′).
Furthermore, it will also prove useful to consider the property that every
path ω ∈Ω is R-random for the (maximally imprecise) vacuous forecasting
system ϕv ∈Φ, deﬁned by ϕv(s) := [0, 1] for all s ∈S.
Property 2. ΩR([0, 1]) = Ω.
Thus, if Properties 1 and 2 hold, every path ω ∈Ω will in particular be R-random
for at least one interval forecast—the forecast I = [0, 1]—and if a path ω ∈Ω is
R-random for an interval forecast I ∈I, then it will also be R-random for any
interval forecast I′ ∈I for which I ⊆I′. It is therefore natural to wonder whether
every path ω ∈Ω has some smallest interval forecast I such that ω ∈ΩR(I).
In order to allow us to formulate an answer to this question, we consider the
sets IR(ω) that for a given path ω ∈Ω contain all interval forecasts I ∈I that ω
is R-random for. If there is such a smallest interval forecast, then it is necessarily
given by
IR(ω) :=

IR(ω) =

I∈IR(ω)
I.
As we will see, for some randomness notions R, IR(ω) will indeed be the small-
est interval forecast that ω is random for. Consequently, for these notions, and
for every ω ∈Ω, the set IR(ω) is completely characterised by the interval fore-
cast IR(ω), in the sense that ω will be R-random for an interval forecast I ∈I
if and only if IR(ω) ⊆I.
In general, however, this need not be the case. For example, consider the
situation depicted in Fig. 1. It could very well be that for some randomness
notion R: that satisﬁes Properties 1 and 2, there is a path ω∗∈Ω that is
R-random for all interval forecasts of the form [p, 1] and [0, q], with p < 1/3
and 2/3 ≤q, but for no others. Then clearly, IR(ω∗) = [1/3, 2/3], but ω∗is not
R-random for IR(ω∗).
In addition, it need not even be guaranteed that the intersection IR(ω) is
non-empty. To guarantee that it will be, and as an imprecise counterpart of the
law of large numbers, it suﬃces to consider the additional property that if a
path ω ∈Ω is R-random for an interval forecast I ∈I, then this I should imply
the following bounds on the relative frequency of ones along ω.

446
F. Persiau et al.
Fig. 1. The green intervals correspond to interval forecasts for which ω∗is R-random,
whereas the red intervals correspond to interval forecasts that ω∗is not R-random for.
(Color ﬁgure online)
Property 3. For all interval forecasts I ∈I and all paths ω ∈ΩR(I), it holds
that min I ≤lim infn→∞1
n
n
k=1 ωk ≤lim supn→∞
1
n
n
k=1 ωk ≤max I.
Properties 1–3 hold for all randomness notions R that we will consider, whence
also, IR(ω) ̸= ∅. We repeat that for some of these notions, IR(ω) will be the
smallest interval forecast that ω ∈Ω is R-random for. If not, we will sometimes
still be able to show that IR(ω) is the smallest interval forecast that ω is almost
R-random for.
Deﬁnition 3. A path ω ∈Ω is called almost R-random for an interval fore-
cast I ∈I if it is R-random for any interval forecast I′ ∈I of the form
I′ = [min I −ϵ1, max I + ϵ2] ∩[0, 1], with ϵ1, ϵ2 > 0.
If a path ω ∈Ω is almost R-random for the interval forecast IR(ω), then IR(ω)
almost completely characterises the set IR(ω): the only case where we cannot
immediately decide whether a path ω is R-random for an interval forecast I ∈I
or not, occurs when min I = min IR(ω) or max I = max IR(ω). Moreover, if
Property 1 holds, then as our terminology suggests, ω ∈Ω is almost R-random
for every interval forecast I ∈I it is random for.
In the remainder of this contribution, we intend to study the smallest interval
forecasts a path is (almost) random for, for several notions of randomness. In
the next section, we start by introducing the mathematical machinery needed
to introduce some of these notions, and in particular, the martingale-theoretic
approach to randomness, which makes extensive use of the concept of betting.
Generally speaking, a path ω ∈Ω is then considered to be random for a fore-
casting system ϕ ∈Φ if a subject can adopt no implementable betting strategy
that is allowed by ϕ and makes him arbitrarily rich along ω. This approach will
enable us to introduce the notions of Martin-L¨of randomness, weak Martin-L¨of
randomness, computable randomness and Schnorr randomness, which diﬀer only
in what is meant by ‘implementable’ and in the way a subject should not be able
to get arbitrarily rich [6].

The Smallest Probability Interval a Sequence Is Random for
447
3
A Martingale-Theoretic Approach—Betting Strategies
Consider the following betting game involving an inﬁnite sequence of binary
variables X1, . . . , Xn, . . . There are three players: Forecaster, Sceptic and Reality.
Forecaster starts by specifying a forecasting system ϕ ∈Φ. For every situ-
ation s ∈S, the corresponding interval forecast ϕ(s) expresses for every gam-
ble f : X →R whether or not Forecaster allows Sceptic to select f; the set of
all gambles is denoted by L(X). A gamble g ∈L(X) is oﬀered by Forecaster to
Sceptic if its expectation Ep(g) := pg(1) + (1 −p)g(0) is non-positive for every
probability p ∈I, or equivalently, if maxp∈I Ep(g) ≤0.
After Forecaster has speciﬁed a forecasting system ϕ ∈Φ, Sceptic selects
a betting strategy that speciﬁes for every situation s ∈S an allowable gam-
ble fs ∈L(X) for the corresponding interval forecast ϕ(s) ∈I, meaning that
maxp∈ϕ(s) Ep(fs) ≤0.
The betting game now unfolds as Reality reveals the successive elements
ωn ∈X of a path ω ∈Ω. In particular, at every time instant n ∈N0, the
following actions have been and are completed: Reality has already revealed the
situation ω1:n, Sceptic engages in a gamble fω1:n ∈L(X) that is speciﬁed by
his betting strategy, Reality reveals the next outcome ωn+1 ∈X, and Sceptic
receives a (possibly negative) reward fω1:n(ωn+1). We furthermore assume that
Sceptic starts with initial unit capital, so his running capital at every time instant
n ∈N0 equals 1+n−1
k=0 fω1:k(ωk+1). We also don’t allow Sceptic to borrow. This
means that he is only allowed to adopt betting strategies that, regardless of the
path that Reality reveals, will guarantee that his running capital never becomes
negative.
In order to formalise Sceptic’s betting strategies, we will introduce the notion
of test supermartingales. We start by considering a real process F : S →R; it is
called positive if F(s) > 0 for all s ∈S and non-negative if F(s) ≥0 for all s ∈S.
A real process F is called temporal if F(s) only depends on the situation s ∈S
through its length |s|, meaning that F(s) = F(t) for any two s, t ∈S such that
|s| = |t|. A real process S is called a selection process if S(s) ∈{0, 1} for all s ∈S.
With any real process F, we can associate a gamble process ΔF : S →L(X),
deﬁned by ΔF(s)(x) := F(s x) −F(s) for all s ∈S and x ∈X, and we call it
the process diﬀerence for F. If F is positive, then we can also consider another
gamble process DF : S →L(X), deﬁned by DF (s)(x) := F (s x)/F (s) for all s ∈S
and x ∈X, which we call the multiplier process for F. And vice versa, with
every non-negative real gamble process D: S →L(X), we can associate a non-
negative real process D⊚: S →R deﬁned by D⊚(s) := n−1
k=0 D(x1:k)(xk+1) for
all s = (x1, . . . , xn) ∈S, and we then say that D⊚is generated by D.
When given a forecasting system ϕ ∈Φ, we call a real process M a super-
martingale for ϕ if for every s ∈S, ΔM(s) is an allowable gamble for the corre-
sponding interval forecast ϕ(s), meaning that maxp∈ϕ(s) Ep(ΔM(s)) ≤0. More-
over, a supermartingale T is called a test supermartingale if it is non-negative
and T(□) := 1. We collect all test supermartingales for ϕ in the set T(ϕ).
It is easy to see that every test supermartingale T corresponds to an allowed

448
F. Persiau et al.
betting strategy for Sceptic that starts with unit capital and avoids borrow-
ing. Indeed, for every situation s = (x1, . . . , xn) ∈S, T speciﬁes an allowable
gamble ΔT(s) for the interval forecast ϕ(s) ∈I, and Sceptic’s running capital
1+n−1
k=0 ΔT(x1:k)(xk+1) equals T(s) and is therefore non-negative, and equals 1
in □.
We recall from Sect. 2 that martingale-theoretic randomness notions diﬀer in
the nature of the implementable betting strategies that are available to Scep-
tic. More formally, we will consider three diﬀerent types of implementable test
supermartingales: computable ones, lower semicomputable ones, and test super-
martingales generated by lower semicomputable multiplier processes. A test
supermartingale T ∈T(ϕ) is called computable if there is some ﬁnite algorithm
that, for every s ∈S and any n ∈N0, can compute the real number T(s) with
a precision of 2−n. A test supermartingale T ∈T(ϕ) is called lower semicom-
putable if there is some ﬁnite algorithm that, for every s ∈S, can compute an
increasing sequence (qn)n∈N0 of rational numbers that approaches the real num-
ber T(s) from below—but without knowing, for any given n, how good the lower
bound qn is. Similarly, a real multiplier process D is called lower semicomputable
if there is some ﬁnite algorithm that, for every s ∈S and x ∈X, can compute
an increasing sequence (qn)n∈N0 of rational numbers that approaches the real
number D(s)(x) from below. For more details, we refer the reader to Appendix
A of the extended on-line version [8].
4
Several Notions of (Imprecise) Randomness
At this point, we have introduced the necessary mathematical machinery to
deﬁne our diﬀerent randomness notions. We start by introducing four martingale-
theoretic ones: Martin-L¨of (ML) randomness, weak Martin-L¨of (wML) random-
ness, computable (C) randomness and Schnorr (S) randomness. Generally speak-
ing, for these notions, a path ω ∈Ω is random for a forecasting system ϕ ∈Φ
if Sceptic has no implementable allowed betting strategy that makes him arbi-
trarily rich along ω. We stress again that these randomness notions diﬀer in
how Sceptic’s betting strategies are implementable, and in how he should not
be able to become arbitrarily rich along a path ω ∈Ω. With these types of
restrictions in mind, we introduce the following sets of implementable allowed
betting strategies.
TML(ϕ)
all lower semicomputable test supermartingales for ϕ
TwML(ϕ)
all test supermartingales for ϕ generated by lower
semicomputable multiplier processes
TC(ϕ), TS(ϕ)
all computable test supermartingales for ϕ
For a path ω to be Martin-L¨of, weak Martin-L¨of or computably random, we
require that Sceptic’s running capital should never be unbounded on ω for any
implementable allowed betting strategy; that is, no test supermartingale T ∈
TR(ϕ) should be unbounded on ω, meaning that lim supn→∞T(ω1:n) = ∞.

The Smallest Probability Interval a Sequence Is Random for
449
Deﬁnition 4 ([5]). For any R ∈{ML, wML, C}, a path ω ∈Ω is R-random for
a forecasting system ϕ ∈Φ if no test supermartingale T ∈TR(ϕ) is unbounded
on ω.
For Schnorr randomness, we require instead that Sceptic’s running capital
should not be computably unbounded on ω for any implementable allowed betting
strategy. More formally, we require that no test supermartingale T ∈TS(ϕ)
should be computably unbounded on ω. That T is computably unbounded on ω
means that lim supn→∞[T(ω1:n) −τ(n)] ≥0 for some real map τ : N0 →R≥0
that is
(i) computable;
(ii) non-decreasing, so τ(n + 1) ≥τ(n) for all n ∈N0;
(iii) unbounded, so limn→∞τ(n) = ∞.2
Since such a real growth function τ is unbounded, it expresses a (computable)
lower bound for the ‘rate’ at which T increases to inﬁnity along ω. Clearly, if
T ∈TS(ϕ) is computably unbounded on ω ∈Ω, then it is also unbounded on ω.
Deﬁnition 5 ([5]). A path ω ∈Ω is S-random for a forecasting system ϕ ∈Φ
if no test supermartingale T ∈TS(ϕ) is computably unbounded on ω.
De Cooman and De Bock have proved that these four martingale-theoretic
randomness notions satisfy Properties 1 and 2 [5, Propositions 9,10,17,18]. To
describe the relations between these martingale-theoretic imprecise-probabilistic
randomness notions, we consider the sets ΩR(ϕ), with R ∈{ML, wML, C, S};
they satisfy the following inclusions [5, Section 6].
ΩML(ϕ) ⊆ΩwML(ϕ) ⊆ΩC(ϕ) ⊆ΩS(ϕ).
Thus, if a path ω ∈Ω is Martin-L¨of random for a forecasting system ϕ ∈
Φ, then it is also weakly Martin-L¨of, computably and Schnorr random for ϕ.
Consequently, for every forecasting system ϕ ∈Φ, there are at most as many
paths that are Martin-L¨of random as there are weakly Martin-L¨of, computably
or Schnorr random paths. We therefore call Martin-L¨of randomness stronger
than weak Martin-L¨of, computable, or Schnorr randomness. And so, mutatis
mutandis, for the other randomness notions.
We also consider two other imprecise-probabilistic randomness notions, which
have a more frequentist ﬂavour: Church randomness (CH) and weak Church
randomness (wCH). Their deﬁnition makes use of yet another (but simpler) type
of implementable real processes; a selection process S is called recursive if there
is a ﬁnite algorithm that, for every s ∈S, outputs the binary digit S(s) ∈{0, 1}.
2 Since τ is non-decreasing, it being unbounded is equivalent to limn→∞τ(n) = ∞.

450
F. Persiau et al.
Deﬁnition 6 ([5]).
A path ω ∈Ω is CH-random (wCH-random) for a fore-
casting system ϕ ∈Φ if for every recursive (temporal) selection process S for
which limn→∞
n−1
k=0 S(ω1:k) = ∞, it holds that
lim inf
n→∞
n−1
k=0 S(ω1:k)[ωk+1 −ϕ(ω1:k)]
n−1
k=0 S(ω1:k)
≥0
and
lim sup
n→∞
n−1
k=0 S(ω1:k)[ωk+1 −ϕ(ω1:k)]
n−1
k=0 S(ω1:k)
≤0.
For a stationary forecasting system I ∈I, the conditions in these deﬁnitions
simplify to the perhaps more intuitive requirement that
min I ≤lim inf
n→∞
n−1
k=0 S(ω1:k)ωk+1
n−1
k=0 S(ω1:k)
≤lim sup
n→∞
n−1
k=0 S(ω1:k)ωk+1
n−1
k=0 S(ω1:k)
≤max I.
It is easy to see that these two randomness notions also satisfy Properties 1
and 2. Since the notion of weak Church randomness considers fewer selection
processes than Church randomness does, it is clear that if a path ω
∈
Ω is
Church random for a forecasting system ϕ ∈Φ, then it is also weakly Church
random for ϕ. Hence, ΩCH(ϕ) ⊆ΩwCH(ϕ). For computable forecasting systems,
we can also relate these two ‘frequentist ﬂavoured’ notions with the martingale-
theoretic notions considered before [5, Sections 6 and 7]: for every computable
forecasting system ϕ ∈Φ,
ΩML(ϕ) ⊆ΩwML(ϕ) ⊆ΩC(ϕ)
⊆
ΩCH(ϕ)
⊆
⊆
ΩS(ϕ)
⊆
ΩwCH(ϕ).
(1)
5
Smallest Interval Forecasts and Randomness
From now on, we will focus on stationary forecasting systems and investigate
the diﬀerences and similarities between the six randomness notions we consider.
We start by studying if there is a smallest interval forecast for which a path
is (almost) random. To this end, we ﬁrst compare the sets IR(ω), with R ∈
{ML, wML, C, S, CH, wCH}. They satisfy similar relations as the sets ΩR(ϕ)—
but without a need for computability assumptions.
Proposition 1 ([5, Sect. 8]). For every path ω ∈Ω, it holds that
IML(ω) ⊆IwML(ω) ⊆IC(ω)
⊆
ICH(ω)
⊆
⊆
IS(ω)
⊆
IwCH(ω).
Similarly to before, if a path ω ∈Ω is Martin-L¨of random for an interval fore-
cast I ∈I, then it is also weakly Martin-L¨of, computably, Schnorr and (weakly)

The Smallest Probability Interval a Sequence Is Random for
451
Church random for I. Observe that for our weakest notion of randomness, Deﬁ-
nition 6—with S = 1—guarantees that all interval forecasts I ∈IwCH(ω) satisfy
Property 3, and therefore, by Proposition 1, all six randomness notions that we
are considering here satisfy Property 3. Since the sets IR(ω) are also non-empty
by Property 2, the interval forecasts IR(ω) are well-deﬁned and non-empty for
all R ∈{ML, wML, C, S, CH, wCH}. Moreover, since the sets IR(ω) satisfy the
relations in Proposition 1, their intersections IR(ω) satisfy the following inverse
relations.
Corollary 1. For every path ω ∈Ω, it holds that
IwCH(ω)
⊆
ICH(ω)
⊆
⊆
IS(ω)
⊆
IC(ω) ⊆IwML(ω) ⊆IML(ω).
For Church and weak Church randomness, it holds that every path ω ∈Ω is
in fact Church and weakly Church random, respectively, for the interval fore-
casts ICH(ω) and IwCH(ω).
Proposition 2. Consider any R ∈{CH, wCH} and any path ω ∈Ω. Then
IR(ω) is the smallest interval forecast that ω is R-random for.
A similar result need not hold for the other four types of randomness we
are considering here. As an illustrative example, consider the non-stationary but
temporal precise forecasting system ϕ∼1/2 deﬁned, for all s ∈S, by
ϕ∼1/2(s) := 1
2 + (−1)|s|δ(|s|), with δ(n) := e−
1
n+1

e
1
n+1 −1 for all n ∈N0.
It has been proved that if a path ω ∈Ω is computably random for ϕ∼1/2, then
ω is Church random and almost computably random for the stationary precise
model 1/2, whilst not being computably random for 1/2 [3].
While in general IR(ω) may not be the smallest interval forecast that a
path ω ∈Ω is R-random for, De Cooman and De Bock have eﬀectively proved
that for R ∈{wML, C, S}, every path ω ∈Ω is almost R-random for IR(ω),
essentially because the corresponding sets IR(ω) are then closed under ﬁnite
intersections.
Proposition 3. ([5, Sect. 8]). Consider any R
∈
{wML, C, S} and any
path ω ∈Ω. Then IR(ω) is the smallest interval forecast for which ω is almost
R-random.
It should be noted that there is no mention of Martin-L¨of randomness in
Propositions 2 and 3. Indeed, it is an open problem whether every path ω ∈Ω is
(almost) ML-random for the interval forecast IML(ω). We can however provide a
partial answer by focusing on paths ω ∈Ω that are ML-random for a computable
precise forecasting system ϕ ∈Φ.
Proposition 4. If a path ω ∈Ω is ML-random for a computable precise fore-
casting system ϕ ∈Φ, then IML(ω) is the smallest interval forecast for which ω
is almost ML-random.

452
F. Persiau et al.
6
What Do Smallest Interval Forecasts Look Like?
Having established conditions under which IR(ω) is the smallest interval forecast
ω is (almost) random for, we now set out to ﬁnd an alternative expression for
this interval forecast. Forecasting systems will play a vital role in this part of
the story; for every path ω ∈Ω and every forecasting system ϕ ∈Φ, we consider
the interval forecast Iϕ(ω) deﬁned by
Iϕ(ω) :=

lim inf
n→∞ϕ(ω1:n), lim sup
n→∞ϕ(ω1:n)

.
When we restrict our attention to computable forecasting systems ϕ ∈Φ, and
if we assume that a path ω ∈Ω is R-random for such a forecasting system ϕ,
with R ∈{ML, wML, C, S, CH, wCH}, then the forecasting system ϕ imposes
outer bounds on the interval forecast IR(ω) in the following sense.
Proposition 5. For any R ∈{ML, wML, C, S, CH, wCH} and any path ω ∈Ω
that is R-random for a computable forecasting system ϕ ∈Φ: IR(ω) ⊆Iϕ(ω).
If we only consider computable precise forecasting systems ϕ ∈Φ and assume
that a path ω ∈Ω is R-random for ϕ, with R ∈{ML, wML, C, CH}, then the
forecasting system ϕ completely characterises the interval forecast IR(ω).
Theorem 1. For any R ∈{ML, wML, C, CH} and any path ω ∈Ω that is R-
random for a computable precise forecasting system ϕ ∈Φ: IR(ω) = Iϕ(ω).
When the computable precise forecasting systems ϕ ∈Φ are also temporal,
this result applies to Schnorr and weak Church randomness as well.
Theorem 2. For any R ∈{ML, wML, C, S, CH, wCH} and any path ω ∈Ω
that is R-random for a computable precise temporal forecasting system ϕ ∈Φ:
IR(ω) = Iϕ(ω).
7
When Do These Smallest Interval Forecasts Coincide?
Finally, we put an old question into a new perspective: to what extent are ran-
domness notions diﬀerent? We take an ‘imprecise’ perspective here, by comparing
the smallest interval forecasts for which a path ω ∈Ω is (almost) R-random,
with R ∈{ML, wML, C, S, CH, wCH}. As we will see, it follows from our previ-
ous exposition that there are quite some paths for which these smallest interval
forecasts coincide.
Let us start by considering a path ω ∈Ω that is ML-random for some com-
putable precise forecasting system ϕ ∈Φ; similar results hold when focusing on
weaker notions of randomness. We know from Eq. (1) that ω is then also wML-,
C- and CH-random for ϕ. By invoking Propositions 2, 3 and 4, we infer that
IR(ω) is the smallest interval forecast that ω is (almost) R-random for, for any
R ∈{ML, wML, C, CH}. Moreover, by Theorem 1, these smallest interval fore-
casts all equal Iϕ(ω) and therefore coincide, i.e., IML(ω) = IwML(ω) = IC(ω) =
ICH(ω) = Iϕ(ω).

The Smallest Probability Interval a Sequence Is Random for
453
By only looking at temporal computable precise forecasting systems ϕ ∈Φ,
we can even strengthen these conclusions. For example, using a similar argument
as before—but using Theorem 2 instead of 1—we see that if ω is ML-random
for such a forecasting system ϕ, then the smallest interval forecasts IR(ω) for
which ω is (almost) R-random coincide for all six randomness notions that we
consider.
Looking at these results, the question arises whether there are paths ω ∈Ω
for which the various interval forecasts IR(ω) do not coincide. It turns out
that such paths do exist. We start by showing that the smallest interval fore-
casts IC(ω) and IS(ω) for which a path ω ∈Ω is respectively almost C- and
almost S-random do not always coincide; this result is mainly a reinterpretation
of a result in [5,10].
Proposition 6. There is a path ω ∈Ω such that IS(ω) = 1/2 ∈[1/2, 1] ⊆IC(ω).
We are also able to show that there is a path ω ∈Ω such that IC(ω) = 1/2 is
the smallest interval forecast it is almost C-random for, whereas ω is not almost
ML-random for 1/2; for this result, we have drawn inspiration from [9].
Proposition 7. For every δ ∈(0, 1/2), there is a path ω ∈Ω such that IC(ω) =
1/2 and I /∈IML(ω) for any I ∈I such that I ⊆[1/2 −δ, 1/2 + δ].
Clearly, the path ω ∈Ω in Proposition 7 cannot be Martin-L¨of random for a
precise computable forecasting system ϕ ∈Φ, because otherwise, the interval
forecasts IC(ω) and IML(ω) would coincide by Eq. (1) and Theorem 1, and ω
would therefore be almost Martin-L¨of random for 1/2 by Proposition 4, contra-
dicting the result. So the path ω in this result is an example of a path for which
we do not know whether there is a smallest interval forecast that ω is almost
Martin-L¨of random for. However, if there is such a smallest interval forecast,
then Proposition 7 shows it is deﬁnitely not equal to 1/2; due to Corollary 1, it
must then strictly include 1/2.
8
Conclusions and Future Work
We’ve come to the conclusion that various (non-stationary) precise-probabilistic
randomness notions in the literature are, in some respects, not that diﬀerent; if
a path is random for a computable precise (temporal) forecasting system, then
the smallest interval forecast for which it is (almost) random coincides for sev-
eral randomness notions. The computability condition on the precise forecasting
system is important for this result, but we don’t think it is that big a restriction.
After all, computable forecasting systems are those that can be computed by a
ﬁnite algorithm up to any desired precision, and therefore, they are arguably the
only ones that are of practical relevance.
An important concept that made several of our results possible was that
of almost randomness, a notion that is closely related to randomness but is—
slightly—easier to satisfy. In our future work, we would like to take a closer look
at the diﬀerence between these two notions. In particular, the present discussion,

454
F. Persiau et al.
together with our work in [7], makes us wonder to what extent the distinction
between them is relevant in a more practical context.
We also plan to continue investigating the open question whether there is for
every path some smallest interval forecast for which it is (almost) Martin-L¨of
random. Finally, there is still quite some work to do in ﬁnding out whether the
randomness notions we consider here are all diﬀerent from a stationary imprecise-
probabilistic perspective, in the sense that there are paths for which the smallest
interval forecasts for which they are (almost) random do not coincide.
Acknowledgments. Floris Persiau’s research was supported by FWO (Research
Foundation-Flanders), project number 11H5521N.
References
1. Ambos-Spies, K., Kucera, A.: Randomness in computability theory. Contemporary
Math. 257, 1–14 (2000)
2. Bienvenu, L., Shafer, G., Shen, A.: On the history of martingales in the study of
randomness. Electron. J. Hist. Probab. Stat. 5, 1–40 (2009)
3. De Cooman, G., De Bock, J.: Computable randomness is inherently imprecise.
In: Proceedings of the Tenth International Symposium on Imprecise Probability:
Theories and Applications. Proceedings of Machine Learning Research, vol. 62, pp.
133–144 (2017)
4. De Cooman, G., De Bock, J.: Randomness and imprecision: a discussion of
recent results. In: Proceedings of the Twelfth International Symposium on Impre-
cise Probability: Theories and Applications. Proceedings of Machine Learning
Research, vol. 147, pp. 110–121 (2021)
5. De Cooman, G., De Bock, J.: Randomness is inherently imprecise. Int. J.
Approximate Reasoning (2021). https://www.sciencedirect.com/science/article/
pii/S0888613X21000992
6. Downey, R.G., Hirschfeldt, D.R.: Algorithmic Randomness and Complexity.
Springer, New York (2010). https://doi.org/10.1007/978-0-387-68441-3
7. Persiau, F., De Bock, J., De Cooman, G.: A remarkable equivalence between
non-stationary precise and stationary imprecise uncertainty models in computable
randomness. In: Proceedings of the Twelfth International Symposium on Impre-
cise Probability: Theories and Applications. Proceedings of Machine Learning
Research, vol. 147, pp. 244–253 (2021)
8. Persiau, F., De Bock, J., De Cooman, G.: The smallest probability interval a
sequence is random for: a study for six types of randomness (2021). https://arxiv.
org/abs/2107.07808, extended online version
9. Schnorr, C.P.: A uniﬁed approach to the deﬁnition of random sequences. Math.
Syst. Theory 5, 246–258 (1971)
10. Wang, Y.: Randomness and Complexity. PhD thesis, Ruprecht Karl University of
Heidelberg (1996)

Inconsistency Handling and Preferences

Merging Epistemic States
and Manipulation
Am´ılcar Mata D´ıaz1 and Ram´on Pino P´erez2(B)
1 El Valle del Esp´ıritu Santo, Nueva Esparta, Venezuela
2 Universit´e d’Artois, CRIL-CNRS, Lens, France
pinoperez@cril.fr
Abstract. In this work we study the problem of manipulation in the
framework of merging complex epistemic states. We adopt the techniques
concerning representation and impossibility in belief merging of complex
epistemic states proposed by Mata D´ıaz and Pino P´erez in 2017. We
introduce here the notion of belief lifting, aiming to capture the pref-
erences of an agent over formulas. This allows us to deﬁne a general
notion of manipulability. We prove that, provided some rational prop-
erties, a merging operator is either manipulable, with respect to any
well-behaved belief lifting, or it admits an Arrovian dictator. We also
study the behaviour of some concrete belief merging operators, showing
that some of them are manipulable. Moreover, we prove that strategy-
proofness cannot be characterized in terms of Arrovian dictators.
Keywords: Belief merging · Epistemic states · Impossibility ·
Manipulation · Strategy-proofness · Arrovian dictator
1
Introduction
Merging information is an important issue which appears naturally in many
domains: medical diagnosis, decision making, policy planning, aggregation of
data bases, etc. The goal is to extract a coherent and relevant piece of informa-
tion, from information given by several sources which could be in conﬂict. A logic
based model of belief merging has been proposed in [18,20]. In that framework
the agents’ basic piece of information is encoded in propositional logic and the
group information is a bag of propositional formulas (a proﬁle).
In many situations, more complex representations are necessary. For instance
Darwiche and Pearl [6] have shown this necessity for revision operators. In the
case of merging, we have illustrated in [24] the usefulness of considering more
complex representations of information through relevant examples.
In the present work we adopt the framework of belief merging complex epis-
temic states introduced in [23] and improved in [24]. One interesting aspect of
this framework is that it allows for a formulation and a better understanding of
common problems of belief merging and social choice theory [1,17,29]. Actually,
c
⃝Springer Nature Switzerland AG 2021
J. Vejnarov´a and N. Wilson (Eds.): ECSQARU 2021, LNAI 12897, pp. 457–470, 2021.
https://doi.org/10.1007/978-3-030-86772-0_33

458
A. Mata D´ıaz and R. Pino P´erez
similarities between belief merging and social choice theory have been pointed
out in [19] (see also [8,12]).
Let us recall the central questions in social choice theory: given a set of
alternatives and a set of voters with their preferences over the alternatives, how
to select the best alternatives for the group and in what measure a method for
selecting the alternatives is good. One measure of the goodness of a social choice
function is the fact that it satisﬁes some reasonable criteria stated by Arrow [1]
(non-imposition, the Pareto condition, independence of irrelevant alternatives
and absence of dictators). In his work, Arrow showed that, if a social choice
function satisﬁes the ﬁrst three criteria, then it admits a dictator. This is his
famous impossibility theorem.
Exploiting the similarities between social choice functions and belief merging
operators, we have translated [24] the Arrovian criteria to the logical framework
of belief merging and showed that this type of operators meets the majority of
those criteria, stating also an impossibility result which generalizes the Arrow’s
Theorem (cf. Theorem 2 below).
Another interesting topic addressed in social choice theory concerns the
manipulability1 of electoral processes [2,7,10,13,14,16,22,27,30]. Manipulation
occurs when a voter expresses preferences which are not his real ones, in order to
obtain a more suitable result for him. Manipulability situations are also present
in belief merging: an agent might “lie” in order to obtain a result that ﬁts better
with his beliefs. This has been studied in belief merging in [5,9,25,26].
An inherent issue in the study on manipulation is to measure how suitable the
outcome of a merging process is for an agent. Everaere et al. [9] stated a quanti-
tative manner to do so, introducing the notion of indexes of satisfaction. In [25]
is introduced a qualitative procedure to measure such suitability using belief lift-
ings. A belief lifting extends preferences over interpretations to preferences over
formulas, in a similar way in which preferences over objects is extended to prefer-
ences over sets of objects [3,11,22]. Liftings have been considered in the study of
strategy-proofness in social choice theory [2,7,13,16,22,30,31] and in the study of
some logical frameworks in order to state preferences over formulas [15,32].
In this work we continue the study of strategy-proofness in the extended
framework of belief merging of epistemic states, using belief liftings as a tool
for establishing preferences over information. We have to note two important
features of our work. First, that the view of epistemic states considered here
is more general than the propositional view, although it preserves the main
logical aspects. This abstract view of epistemic states, introduced by Benfer-
hat et al. [4], is indeed a formalization of the concept established by Darwiche
and Pearl. Second, we enrich the set of rational postulates by introducing new
social postulates inspired on the classical Arrow’s criteria, stating then some
general strategy-proofness results, which show a dichotomy between manipula-
bility (with respect to a belief lifting) and the existence of an Arrovian dictator.
It is worth mentioning that in [25] there is a strategy-proofness result, which
is similar to ours but it involves powerful agents with a weaker behaviour than
Arrovian dictators.
1 The absence of manipulability is usually called strategy-proofness.

Merging Epistemic States and Manipulation
459
This work is organized as follows: in Sect. 2 we present some special orders
that will be used throughout the paper and the notions of belief liftings. In
Sect. 3 we present brieﬂy the concept of merging operators of epistemic states
as well as the social postulates and an impossibility theorem stated in [24].
This impossibility theorem is a basic tool for obtaining the main result of this
work. In Sect. 4 we introduce other new social postulates and we establish the
main results of this paper about strategy-proofness. In Sect. 5 we analyze the
behavior of some ES basic merging operators in order to illustrate manipulability
situations. Finally, in Sect. 6 we make some concluding remarks.
2
Preliminaries
A preorder over a set A is a binary relation ⪰over A which is reﬂexive and
transitive. Its associated strict relation, ≻, and indiﬀerence relation, ≃, are given
as follows: x ≻y iﬀx ⪰y & y ̸⪰x and x ≃y iﬀx ⪰y & y ⪰x.
A total preorder over a set A is a preorder which is total. Given a subset C of
A, we say that c in C is a maximal element of C, with respect to a total preorder
⪰, if c ⪰x, for all x in C. The set of maximal elements of C with respect to ⪰
will be denoted by max(C, ⪰). We will write max(⪰) instead of max(A, ⪰) to
denote the set of maximal elements of the whole set A with respect to the total
preorder ⪰. P(A) denotes the set of all total preorders over a set A.
An example of a total preorder, which is often used along this paper, is the lex-
icographical combination of two total preorders, ⪰1 and ⪰2, denoted ⪰lex(⪰1,⪰2):
x ⪰lex(⪰1,⪰2) y iﬀ

x ≻1 y, or
x ≃1 y
& x ⪰2 y
For any pair of total preorders ⪰1, ⪰2 over a set A, its lexicographical com-
bination, ⪰lex(⪰1,⪰2), is also a total preorder over A for which the following
holds:
max(⪰lex(⪰1,⪰2)) = max(max(⪰1), ⪰2)
(1)
A more complex example of a total preorder is the precise-leximax order intro-
duced by Leal and Pino P´erez [22]. Given a nonempty set A with n elements,
and a total preorder ⪰over A, we consider the set (A)⪰formed by all the
tuples of size less or equal to n, whose inputs are not repeated elements of
A, ordered in decreasing manner by ⪰. Thus, given a total preorder ⪰over
A, we deﬁne the precise-leximax order ⪰plm
⪰
as follows, for every pair of tuples
−→x = (x1, x2, . . . , xk) and −→y = (y1, y2, . . . , ym) in (A)⪰:
−→x ⪰plm
⪰
−→y iﬀ

k ≤m and xi ≃yi, for all i ≤k, or
there is j ≤min{k, m} s.t. xi ≃yi, for all i < j, and xj ≻yj
A precise-leximax order is actually a total preorder that discriminates the
ordered chains by considering the lexicographical order and privileging proper
initial segments.

460
A. Mata D´ıaz and R. Pino P´erez
The set of non contradictory propositional formulas built over a ﬁnite set
P of atomic propositions will be denoted LP while WP is its associated set of
interpretations (models). If ϕ is a formula in LP, we denote by [[ϕ]] the set of
its models. If ϕi is a formula in LP, for each i in a ﬁnite set of indexes I, then
we denote by  ϕi the conjunction of all the formulas ϕi. If M is a nonempty
subset of WP, ϕM denotes a formula whose set of models is exactly M.
Extending an ordering over models to an ordering over propositions can be
performed, via the semantics, in a similar manner as the extension of an ordering
over a set A to an ordering over subsets of A [3]. We call this type of processes
belief liftings. More precisely, a belief lifting is a mapping ⪰→⊒⪰that associates
a preorder ⊒⪰over LP to any total preorder ⪰over WP, for which the following
hold: ϕw ⊒⪰ϕw′ iﬀw ⪰w′, and if ϕ ≡ψ then ϕ ≃ψ, with respect to ⊒⪰.
Given a preorder ⊒WP
⪰
over subsets of WP, it is possible to establish an
order over LP, ⊒LP
⪰, as follows: ϕ ⊒LP
⪰
ϕ′ iﬀ[[ϕ]] ⊒WP
⪰
[[ϕ′]]. Thus, through
liftings over sets, as the Kelly lifting [16] and the precise leximax lifting2 [22], it
is possible to deﬁne two belief liftings using this technique:
Kelly Belief Lifting: ϕ ⊒K
⪰ψ iﬀfor all w |= ϕ and for all w′ |= ψ; w ⪰w′.
Precise-Leximax Belief Lifting: ϕ ⊒plm
⪰
ψ iﬀfor all −→x in [[ϕ]]⪰containing
all the models of ϕ, there exists −→y in [[ψ]]⪰containing all the models of ψ such
that −→x ⪰plm
⪰
−→y .
Barber`a et al. [3] characterized many natural liftings through their properties.
Among them, there is a pair of basic properties, which were stated by G¨ardenfors
[13]. We call them the G¨ardenfors properties, which in our setting are:
G1: If w ≻w′ then ϕw,w′ ⊐⪰ϕw′ and G2: If w ≻w′ then ϕw ⊐⪰ϕw,w′
These properties have a very natural interpretation: G1 expresses that good
company improves the group; G2 says that bad company worsens the group.
From now on, a G-belief lifting is a belief lifting that satisﬁes both instances
of the G¨ardenfors properties. It is not hard to see that the Kelly belief lifting and
the precise-leximax belief lifting are actually two instances of G-belief liftings.
3
Epistemic State Merging Operators
In this section we establish the concept of Epistemic State merging operators (ES
merging operators for short) and give some features of their social behaviour.
First of all, we need the notions of epistemic state, agents and epistemic proﬁles.
An epistemic space is a triple (E, B, LP), where E is a nonempty set, whose
elements are called epistemic states, LP is a set of non contradictory formulas and
B is a surjection from E into LP. For every E in E, B(E) should be interpreted
as the belief base or the most entrenched belief of E.
The set of agents is a preﬁxed well ordered set (S, <). A society of agents is
a ﬁnite and nonempty set N = {i1, i2, . . . , in} of S, whose elements are disposed
increasingly. A partition of N is a ﬁnite family {N1, . . . , Nk} of pairwise disjoint
sets such that their union is N.
2 The Precise leximax lifting is in fact a variant of the leximax lifting [3].

Merging Epistemic States and Manipulation
461
Now, we ﬁx an epistemic space (E, B, LP) and a set of agents (S, <). Given
a ﬁnite society of agents N, an N-proﬁle (also called epistemic proﬁle) is a tuple
Φ = (Ei1, . . . , Ein) ordered increasingly by the elements in N. Sometimes we will
write Φ(i) in order to represent the epistemic state Ei in Φ. If N is a singleton,
suppose N = {i}, by abuse, Ei will denote the N-proﬁle (Ei), and we call it
single proﬁle. The set of all the epistemic proﬁles is denoted P(S, E).
Let N = {i1, . . . , in} and M = {j1, . . . , jm} be two societies of agents, and
consider the epistemic proﬁles Φ = (Ei1, . . . , Ein) and Φ′ = (E′
j1, . . . , E′
jm). We
denote Φ ≡Φ′, if n = m and Eik = Ejk, for k = 1, . . . , n. By abuse and being
clear from the context, we write respectively Ei = Ej and Ei ̸= Ej instead of
Ei ≡Ej and Ei ̸≡Ej. When N and M are disjoint, we deﬁne a new (N ∪M)-
proﬁle, Φ ⊔Φ′, as follows: (Φ ⊔Φ′
(i) is Φ(i) if i ∈N, otherwise it is Φ′(i). If
M ⊆N, ΦM denotes the M-proﬁle obtained by the restriction of Φ to M.
From now on, Φ and Φ′ denote the proﬁles (Ei1, . . . , Ein) and (E′
i1, . . . , E′
in),
respectively. Additionally, Φ[E∗/i] is the epistemic proﬁle obtained from Φ by
replacing Ei with E∗, that is: Φ[E∗/i](j) is Φ(j) if j ̸= i, otherwise it is E∗.
An operator of epistemic states (also called an ES operator for short) is a
function of the form ∇: P(S, E) × E −→E, where ∇(Φ, E) is the result of
combining the epistemic states in Φ under an integrity constraint E.
Now we present the rationality postulates of merging in the framework of
epistemic states. Most of them are adapted from IC merging postulates proposed
by Konieczny and Pino P´erez [18]. The postulates, in terms of epistemic states,
were proposed and widely studied in [23,24].
(ESF1) B

∇(Φ, E)

⊢B(E).
(ESF2) B

∇(Φ, E)

≡B

∇(Φ′, E′)
, whenever Φ ≡Φ′ and B(E) ≡B(E′).
(ESF3) B

∇(Φ, E′)

∧B(E′′) ⊢B

∇(Φ, E)
, whenever B(E) ≡B(E′) ∧B(E′′).
(ESF4) B

∇(Φ, E)

⊢B

∇(Φ, E′)

∧B(E′′), whenever B(E) ≡B(E′)∧B(E′′) and
B

∇(Φ, E′)

∧B(E′′) ̸⊢⊥.
(ESF5) If Ej ̸= Ek, then there exists E∗in E s.t. B

∇(Ej, E∗)

̸≡B

∇(Ek, E∗)
.
(ESF6) B

∇(Φ, E)

≡
i∈N B(Ei) ∧B(E), whenever 
i∈N B(Ei) ∧B(E) ̸⊢⊥.
(ESF7) B

∇(ΦN1, E)

∧B

∇(ΦN2, E)

⊢B

∇(Φ, E)

.
(ESF8) B

∇(Φ, E)

⊢B

∇(ΦN1, E)

∧B

∇(ΦN2, E)
, whenever B

∇(ΦN1, E)

∧B

∇(ΦN2, E)

̸⊢⊥.
The ﬁrst four postulates, (ESF1)–(ESF4), are the minimal requirements
of rationality that ES operators have to satisfy. They allow to state an impor-
tant class of ES combination operators, namely the ES basic merging operators:
those operators that satisfy them. The last four postulates describe mainly the
relationships between the results of merging as a whole society and the results
of merging in its subsocieties (cf. [24] for more details). They allow to introduce
an important subclass of ES basic merging operators, namely, ES merging oper-
ators: those operators that satisfy (ESF1)–(ESF8). In Sect. 5 we present some
examples of this type of merging operators.
Now we give some semantic aspects of merging processes, which are deter-
mined by orderings over interpretations. An assignment is a map Φ →⪰Φ which

462
A. Mata D´ıaz and R. Pino P´erez
associates a total preorder ⪰Φ over WP to every epistemic proﬁle Φ, such that
⪰Φ=⪰Φ′, whenever Φ ≡Φ′. An assignment Φ →⪰Φ is called a faithful assignment
if the following properties hold:
1. If Ej ̸= Ek, then ⪰Ej̸=⪰Ek.
2. If 
i∈N B(Ei) ̸⊢⊥, then [[
i∈N B(Ei)]] = max(⪰Φ).
3. If w ⪰ΦN1 w′ and w ⪰ΦN2 w′ then w ⪰Φ w′.
4. If w ⪰ΦN1 w′ and w ≻ΦN2 w′, then w ≻Φ w′.
The intended meaning of a faithful assignment is coding semantically the
group preferences. Indeed, from 2 follows that the most entrenched preferences
of an agent represent his entrenched beliefs. More precisely, for every single
proﬁle E, we have straightforwardly from 2 that the following equality, which is
called the max condition, holds: [[B(E)]] = max(⪰E).
The next result collects the semantic characterizations (at the level of belief
bases) of ES basic merging operators and ES merging operators, showed in [24].
Theorem 1. For every ES operator ∇, the following assertions hold:
(i) ∇is an ES basic merging operator iﬀthere is a unique assignment Φ →⪰Φ
such that:
[[B

∇(Φ, E)

]] = max

[[B(E)]], ⪰Φ

(B-Rep)
(ii) ∇is an ES merging operator iﬀthere exists a unique faithful assignment
Φ →⪰Φ satisfying (B-Rep).
Now, we present some properties of the merging processes, which capture the
following social principles appearing in the seminal work of Arrow [1]: Standard
domain, Pareto condition, independence of irrelevant alternatives and existence
of a dictator. These properties were introduced and deeply studied in [24]. In
order to give a formulation of these principles in logical terms, using epistemic
states, from now on we suppose that P has at least two propositional variables.
Thus, there are at least four interpretations available in WP.
(ESF-SD) For every agent i in S and every triple w, w′, w′′ in WP, the following
conditions hold:
(i) There exists Ei s.t. B

∇(Ei, Ew,w′)

≡ϕw,w′ and B

∇(Ei, Ew′,w′′)

≡ϕw′.
(ii) There exists Ei s.t. B

∇(Ei, Ew,w′)

≡ϕw and B

∇(Ei, Ew′,w′′)

≡ϕw′,w′′.
(iii) There exists Ei s.t. B

∇(Ei, Ew,w′)

≡ϕw and B

∇(Ei, Ew′,w′′)

≡ϕw′.
where [[B(Ew,w′)]] = {w, w′} and [[B(Ew′,w′′)]] = {w′, w′′}.
(ESF-P) For all i in N, if B

∇(Ei, E)

∧B(E′) ⊢⊥and  B

∇(Ei, E)

̸⊢⊥,
then B

∇(Φ, E)

∧B(E′) ⊢⊥.
(ESF-I) B

∇(Φ, E)

≡B

∇(Φ′, E)
, whenever B

∇(Ei, E′)

≡B

∇(E′
i, E′)
, for
every i in N and every epistemic state E′ in E, with B(E′) ⊢B(E).
(ESF-D) For every ﬁnite society N in F∗(S) there exists an agent d in N s.t.
B

∇(Φ, E)

⊢B

∇(Ed, E)
, for all N-proﬁle Φ and every restriction E in E.

Merging Epistemic States and Manipulation
463
(ESF-SD) is called the standard domain condition. This property states
some “richness” in the set of results of a merging process. Indeed, if an ES basic
merging operator satisﬁes it, then the following holds for its associated assign-
ment: for any agent i in S, any triple of interpretations w, w′, w′′ in WP, and
any total preorder ⪰over {w, w′, w′′} (except the ﬂat order), there is an i-proﬁle
Ei such that ⪰is ⪰Ei↾{w,w′,w′′} (the restriction of ⪰Ei to {w, w′, w′′}). (ESF-
P) is referred to as the Pareto condition. This property expresses that, if all
the agents reject a piece of information, and all the agents have some consen-
sus, such information will be rejected in the result of merging. (ESF-I) is the
independence condition. This captures the following principle: the merging pro-
cess depends only on how the restrictions in the individual epistemic states are
related. (ESF-D) evokes the existence of a dictator, there is an agent (an Arro-
vian dictator) that imposes his will. This is the postulate that good operators
should avoid. Operators satisfying that postulate are called dictatorial operators.
Any ES merging operator satisﬁes the Pareto Condition. This is because
(ESF7) and (ESF8) entails (ESF-P). Moreover, every dictatorial operator
also satisﬁes the Pareto condition [24].
Now we present an Arrovian impossibility result, which is similar to that
stated by Sen for social choice functions [17,28]. This will be very useful later.
Theorem 2 (Impossibility theorem [24]). If an ES basic merging operator
satisﬁes (ESF-SD), (ESF-P) and (ESF-I), then (ESF-D) also holds.
4
On Manipulability for ES Merging Operators
In this section we deal with another interesting issue which is present in the
framework of logic-based merging, namely, strategy-proofness of merging infor-
mation processes. In order to address this issue we present other two natural
social properties: Non-imposition and stability.
Non-imposition expresses the fact that, if a complete information entails the
beliefs of the integrity constraint, then it can be the belief of one possible output
of the merging process. More precisely:
(ESF-NI) For every ﬁnite society N and every model w of B(E), there exists
an N-proﬁle Φ such that B

∇(Φ, E)

≡ϕw.
We say that an ES operator is non-imposed if it satisﬁes (ESF-NI). It is
not hard to see that any ES merging operator is non-imposed, as the next result
reveals. We leave its proof to the reader.
Proposition 1. If (ESF6) holds, then (ESF-NI) also holds.
Stability captures the fact that if an information is accepted by the whole
group under a restriction and also by the revision of an epistemic state under
the same restriction, it will remain accepted if an agent – whose revised beliefs
with the restriction of the merging process coincides with the belief of such
restriction – changes his mind by the epistemic state which accepted such an
information. Formally:

464
A. Mata D´ıaz and R. Pino P´erez
(ESF-S) If ϕ ⊢B

∇(Φ, E)

∧B

∇(E∗, E)
 and B(∇(Ei, E)) ≡B(E), then
ϕ ⊢B(∇(Φ[E∗/i], E)).
We say that an ES operator is stable if (ESF-S) holds and it is called unstable
if that property does not hold. In Sect. 5 we present some concrete examples of
stable and unstable operators. Indeed, every ES merging operator is stable:
Proposition 2. If (ESF1), (ESF7) and (ESF8) hold, then (ESF-S) holds.
Proof. We suppose ϕ ⊢B(∇(Φ, E)) ∧B(∇(E∗, E)) and B(∇(Ei, E)) ≡B(E).
If Φ is a single proﬁle or ϕ ⊢⊥, then the result is straightforward.
Now, we assume that the N-proﬁle Φ has at least two inputs, ϕ ̸⊢⊥, and
consider M = N \ {i}. On one hand, since B(∇(Ei, E)) ≡B(E), from (ESF1)
we get B(∇(ΦM, E)) ∧B(∇(Ei, E)) ≡B(∇(ΦM, E)). On the other hand, from
(ESF7) and (ESF8) we obtain B(∇(ΦM, E)) ∧B(∇(Ei, E)) ≡B(∇(Φ, E)).
Thus, B(∇(ΦM, E)) ≡B(∇(Φ, E)). Now, since ϕ ⊢B(∇(Φ, E)) ∧B(∇(E∗, E)),
we have ϕ ⊢B(∇(ΦM, E)) ∧B(∇(E∗, E)). From this, applying (ESF7) and
(ESF8) again, we get ϕ ⊢B(∇(Φ[E∗/i], E)), as desired.
⊓⊔
Now, we present our notion of manipulability. An ES basic merging operator
is manipulable by an agent if such an agent, knowing the restriction of the
merging process and the information that will be expressed by the other agents,
changes his mind in order to obtain a result that “ﬁts” better his true beliefs.
Deﬁnition 1. An ES basic merging operator ∇is said to be manipulable if there
exist a belief lifting ⪰→⊒⪰, a ﬁnite society of agents N in F∗(S), an N-proﬁle
Φ, an agent i in N and a couple of epistemic states E, E∗in E, such that:
B

∇(Φ[E∗/i], E)

⊐⪰Ei B

∇(Φ, E)

In this case, we say that ∇is manipulable with respect to the belief lifting
⪰→⊒⪰, while the tuple formed by N, Φ, i, E, E∗is called a manipulability
situation for ∇(with respect to ⪰→⊒⪰). If such a manipulability situation exists,
we say that ∇admits a manipulability situation (with respect to ⪰→⊒⪰).
We say that ∇is absolutely manipulable (resp. absolutely G-manipulable) if
it is manipulable with respect to any belief lifting (resp. to any G-belief lifting).
If ∇does not admit a manipulability situation with respect to a preﬁxed belief
lifting, we say that ∇is strategy-proof with respect to that belief lifting.
Strategy-proofness entails some rational properties when other good proper-
ties are involved. This can be seen through the following result.
Proposition 3. For every ES basic merging operator ∇, which is strategy-proof
with respect to a G-belief lifting, the following properties hold:
(i) If ∇satisﬁes (ESF-NI), then it also satisﬁes (ESF-P).
(ii) ∇satisﬁes (ESF-S) iﬀit also satisﬁes (ESF-I).

Merging Epistemic States and Manipulation
465
Proof. To show (i), suppose that (ESF-P) does not hold. Thus, there exist a
ﬁnite society N, an N-proﬁle Φ and a pair of epistemic states E, E′ in E such
that, for every agent i, B

∇(Ei, E)

∧B(E′) ⊢⊥,  B

∇(Ei, E)

̸⊢⊥, but
B

∇(Φ, E)

∧B(E′) ̸⊢⊥. Consider a couple of interpretations w, w′ in WP such
that w |=  B

∇(Ei, E)

and w′ |= B

∇(Φ, E)

∧B(E′). For simplicity, due to
(ESF1), (ESF3) and (ESF4), we may suppose3 [[B(E)]] = {w, w′}.
By (ESF-NI), consider a N-proﬁle Φ∗such that B(∇(Φ∗, E)) ≡ϕw, and
note that B

∇(Φ, E)

̸≡ϕw. Deﬁne the sequence Φ = Φ0, Φ1, . . . , Φn = Φ∗as
follows: Φt = Φt−1[E∗
it/it], for t > 0. Let Φk be the ﬁrst in the sequence s.t.
B

∇(Φk, E)

≡ϕw. Put r = k −1 and i = ik. Then, Φk = Φr[E∗
i/i] and,
B

∇(Φr, E)

≡ϕw′ or B

∇(Φr, E)

≡ϕw,w′
Now, since w |= B(∇(Ei, E)) and w′ ̸|= B(∇(Ei, E)), from (B-Rep) we get
w ≻Ei w′. Thus, ϕw ⊐⪰Ei ϕw′ and from G2 we get ϕw ⊐⪰Ei ϕw,w′. Therefore,
B

∇(Φr[E∗
i/i], E)

⊐⪰Ei B

∇(Φr, E)

, a contradiction.
To show the only if part of (ii), we assume that (ESF-S) holds and, towards
a contradiction, suppose that (ESF-I) does not hold. Then, we consider a couple
of epistemic proﬁles Φ = (Ei1, . . . , Ein), Φ∗= (E∗
i1, . . . , E∗
in) and E in E such
that B(∇(Ei, E′)) ≡B(∇(E∗
i , E′)), for every E′ in E with B(E′) ⊢B(E), but
B(∇(Φ, E)) ̸≡B(∇(Φ∗, E)).
Suppose B(∇(Φ, E)) ̸⊢B(∇(Φ∗, E)) (the other case is analogous) and con-
sider a pair w, w′ in WP such that w |= B(∇(Φ, E)) ∧¬B(∇(Φ∗, E)) and
w′ |= B(∇(Φ∗, E)). Due to (ESF1), (ESF3) and (ESF4), we may suppose
[[B(E)]] = {w, w′}. Thus, we get B(∇(Φ, E)) ̸≡ϕw′ and B(∇(Φ∗, E)) ≡ϕw′.
Let Φk be the ﬁrst of Φ = Φ0, Φ1, . . . , Φn = Φ∗s.t. B(∇(Φk, E)) ≡ϕw′,
where Φt = Φt−1[E∗
it/it], for t > 0. Then, if r = k −1 and i = ik, we obtain
Φk = Φr[E∗
i/i], while ϕw ⊢B(∇(Φr, E)) and B(∇(Φr[E∗
i/i], E)) ≡ϕw′, that is,
ϕw ̸⊢B(∇(Φr[E∗
i/i], E)). From this and (ESF-S), we get the next two cases:
ϕw ̸⊢B(∇(Φr, E)) ∧B(∇(E∗
i , E)): In this case, since ϕw ⊢B(∇(Φr, E)), we
have ϕw ̸⊢B(∇(E∗
i , E)). Then, since B(∇(E∗
i , E)) ≡B(∇(Ei, E)), from
(B-Rep), we get w′ ≻Ei w. Hence, ϕw′ ⊐⪰Ei ϕw and, by G2, ϕw′ ⊐⪰Ei ϕw,w′.
Thus, B(∇(Φr[E∗
i/i], E)) ⊐⪰Ei B(∇(Φr, E)), contradicting the no manipulation.
B(∇(Ei, E)) ̸≡B(E): In this case, by the assumption, B(∇(E∗
i , E)) ̸≡B(E).
Thus, w ̸|= B(∇(E∗
i , E)) or w′ ̸|= B(∇(E∗
i , E)). If w ̸|= B(∇(E∗
i , E)), with
a similar argument to the previous one, B(∇(Φr[E∗
i/i], E)) ⊐⪰Ei B(∇(Φr, E)),
contradicting the no manipulation. Finally, suppose that w′ ̸|= B(∇(E∗
i , E))
and note that, by (B-Rep), w ≻E∗
i w′. Then, ϕw ⊐⪰E∗
i ϕw′, and from G1 it fol-
lows ϕw,w′ ⊐⪰E∗
i ϕw′. Now, since Φr = Φk[Ei/i], we get B(∇(Φk[Ei/i], E)) ⊐⪰E∗
i
B(∇(Φk, E)), contradicting the no manipulation.
To show the if part of (ii), we assume that (ESF-I) holds, and towards a
contradiction, we suppose that (ESF-S) does not. Consider thus a ﬁnite society
3 Rigorously, we must consider E′′ in E such that [[B(E′′)]] = {w, w′}, then apply
(ESF1), (ESF3) and (ESF4) and continue with the proof by switching E and E′′.

466
A. Mata D´ıaz and R. Pino P´erez
N, an agent i in N, an N-proﬁle Φ, a couple E, E∗in E and ϕ in LP such that
ϕ ⊢B(∇(Φ, E))∧B(∇(E∗, E)), B(∇(Ei, E)) ≡B(E) and ϕ ̸⊢B(∇(Φ[E∗/i], E)).
We consider a pair of models w, w′ in WP
such that w
|=
ϕ ∧
¬B(∇(Φ[E∗/i], E)) and w′ |= B(∇(Φ[E∗/i], E)). Again, by (ESF1), (ESF3) and
(ESF4), we may suppose [[B(E)]] = {w, w′}. Thus, since w |= B(∇(E∗, E)), by
(B-Rep) we get the following two cases:
w ≃E∗w′: In this case, it is not hard to see that B(∇(Ei, E′)) ≡B(∇(E∗, E′)),
for every E′ in E, with B(E′) ⊢B(E). Therefore, from (ESF-I) we obtain
B(∇(Φ, E)) ≡B(∇(Φ[E∗/i], E)), a contradiction.
w ≻E∗w′: In this case, assume Φ∗= Φ[E∗/i] and note that Φ∗[Ei/i] = Φ. Thus,
from (ESF1) we obtain B(∇(Φ∗, E)) ≡ϕw′, while B(∇(Φ∗[Ei/i], E)) ≡ϕw or
B(∇(Φ∗[Ei/i], E)) ≡ϕw,w′. Now, since w ≻E∗w′, we have ϕw ⊐⪰E∗ϕw′ and,
by G1, ϕw,w′ ⊐⪰E∗ϕw′. Therefore, for any of the two possible outcomes of
B(∇(Φ∗[Ei/i], E)), we get B(∇(Φ∗[Ei/i], E)) ⊐⪰E∗B(∇(Φ∗, E)).
In any case, we have obtained the manipulability of ∇, a contradiction.
⊓⊔
We are now able to state our main strategy-proofness result. This is a
straightforward corollary of Theorem 2 and Proposition 3, which establishes a
dichotomy between absolute G-manipulability and the existence of an Arrovian
dictator.
Theorem 3. Any ES basic merging operator satisfying (ESF-SD), (ESF-NI)
and (ESF-S) is absolutely G-manipulable or satisﬁes (ESF-D).
In particular, if a non-imposed, stable and strategy-proof (with respect to a
G-belief lifting) ES basic merging operator satisﬁes the standard domain condi-
tion, then it admits an Arrovian dictator. The converse of this assertion fails.
Indeed, the Σ-projective operator (see Sect. 5) is an example of a non-imposed,
stable ES basic merging operator satisfying the standard domain condition,
which is dictatorial, but it is absolutely manipulable. Moreover, the projective
operator is a non-imposed, stable and dictatorial basic merging operator that
satisﬁes the standard domain condition, which is strategy-proof with respect
to the Kelly’s lifting, but it is manipulable with respect to the precise-leximax
lifting (see Sect. 5). Thus, Theorem 3 relies heavily on the lifting chosen.
The next result is a corollary of Propositions 1 and 2 and Theorem 3. It
shows that any non-dictatorial ES merging operator satisfying standard domain
is absolutely G-manipulable.
Theorem 4. Every ES merging operator satisfying (ESF-SD) is absolutely G-
manipulable or satisﬁes (ESF-D).
5
Concrete Examples of Merging Operators
In this section, we are going to exhibit some speciﬁc examples of ES merging
operators in order to illustrate manipulability situations. Such operators were
introduced in [24]. In order to do so, we assume that the epistemic states are given

Merging Epistemic States and Manipulation
467
by total preorders over interpretations [that is, E = P(WP)] and the function
B is such that [[B(⪰)]] = max(⪰) for any epistemic state ⪰. In terms of this
concrete representation of information, due to (1) and (B-Rep), we can build an
ES basic merging operator through a given basic assignment Φ →⪰Φ as follows:
∇(Φ, ⪰) =⪰lex(⪰,⪰Φ)
(2)
First, we present two well known instances of ES basic merging operators
that are classical in the study of belief merging [9,18–21,24–26]: the sum and
max operators. They are deﬁned using the sum and max functions.
Sum: (x1, · · · , xn) =  xi and Max: max(x1, . . . , xn) = max{x1, x2, . . . ,
xn}.
These functions allow us to introduce in a natural way the sum and the
max assignments. They associate a total preorder ⪰F
Φ over WP to any epistemic
proﬁle Φ = (⪰i1, ⪰i2, . . . , ⪰in), as follows:
w ⪰F
Φ w′ iﬀF(ri1(w), . . . , rin(w)) ≥F(ri1(w′), . . . , rin(w′))
(3)
where F denotes sum or max and ri is the natural ranking function4 of ⪰i.
Thus, from (2) and (3), it is possible to build the sum operator, ∇Σ, and the
max operator, ∇max. It is not hard to see that sum is a non-imposed and stable
ES merging operator, while max is a non-imposed ES basic merging operator
which is unstable. Moreover, they are non-dictatorial operators that satisfy the
standard domain condition, which are absolutely manipulable.
Now, we present two operators, which were introduced in [24]: the projective
and the Σ-projective. From now on, given a ﬁnite society N, d denotes max(N).
The projective operator, ∇π, is totally determined by projections over epis-
temic proﬁles: the output of a merging process completely depends on the beliefs
of the agent d and the integrity constraints:
Projective Operator: ∇π(Φ, ⪰) =⪰lex(⪰,⪰d)
The projective operator is actually an ES basic merging operator that satisﬁes
(ESF-SD), (ESF-P), (ESF-I) and (ESF-D) [24]. It is also easy to see that
the projective operator satisﬁes (ESF-NI) and it is strategy-proof with respect
to Kelly’s belief lifting. Thus, from Proposition 3 we get that (ESF-S) also
holds. Moreover, ∇π is manipulable through the precise-leximax belief lifting:
assume an epistemic proﬁle Φ, a pair w, w′ in WP and a pair ⪰, ⪰∗in E, such
that max(⪰) = max(⪰d) = {w, w′} and max(⪰∗) = {w}. From (B-Rep) we
get B(∇π(Φ, ⪰)) ≡ϕw,w′ and B(∇π(Φ[⪰∗/d], ⪰)) ≡ϕw. Now, since w ≃d w′,
we get (w) ⪰plm
⪰d (w, w′) and (w) ⪰plm
⪰d (w′, w). Thus, ϕw ⊐plm
⪰d ϕw,w′, that is,
B(∇π(Φ[⪰∗/d], ⪰)) ⊐plm
⪰d B(∇π(Φ, ⪰)). This shows the manipulability of ∇π.
The Σ-projective operator5, ∇Σπ, is deﬁned using sum and projections:
Σ-projective Operator: ∇Σπ(Φ, ⪰) =⪰lex(⪰,⪰Σπ
Φ
); where ⪰Σπ
Φ =⪰lex(⪰d,⪰Σ
Φ ).
4 Given by ri(x) = max{n ∈N : ∃x0, . . . , xn s.t. xk+1 ≻i xk and xn = x}.
5 The Σ-projective operator is exactly the Σ-pseudoprojective operator stated in [24].

468
A. Mata D´ıaz and R. Pino P´erez
The Σ-projective operator is structure preserving6 and it satisﬁes (ESF-
SD), (ESF-P) and (ESF-D) but (ESF-I) does not hold [24]. Furthermore, it
is not hard to see that it also satisﬁes (ESF-NI) and (ESF-S).
Fig. 1. Manipulability situation for the Σ-projective operator.
The Σ-projective operator is absolutely manipulable. To show that, we sup-
pose WP = {w1, w2, w3, w4} and consider the following situation7: the epis-
temic proﬁle Φ represented in Fig. 1(a), the epistemic state ⪰∗given in Fig. 1(b)
and a restriction ⪰in E such that [[B(⪰)]] = {w1, w2}. Simple calculations
lead us to w2 ≻Σ
Φ w1 and w1 ≻Σ
Φ[⪰∗/1] w2. Thus, since w1 ≃3 w2, we obtain
w2 ≻Σπ
Φ
w1 and w1 ≻Σπ
Φ[⪰∗/1] w2 and, by (B-Rep), B(∇Σπ(Φ, ⪰)) ≡ϕw2 and
B(∇Σπ(Φ[⪰∗/1], ⪰)) ≡ϕw1. Then, since w1 ≻Σπ
⪰1 w2, for any belief lifting ⪰→⊒⪰
we get B(∇Σπ(Φ[⪰∗/1], ⪰)) ⊐⪰Σπ
⪰1 B(∇Σπ(Φ, ⪰)), as desired.
6
Concluding Remarks
We have stated a general result of manipulability for ES basic merging operators
that shows a dichotomy between absolute G-manipulation and the existence of
a dictator when some rational properties are involved. One interesting feature
of our approach is that it allows to instantiate the strategy-proofness result
to diﬀerent representations of epistemic states: ordinal conditional functions,
rational relations, and of course total preorders. However, with the representation
of epistemic states as formulas, our results do not hold because it is impossible
to have the standard domain condition in the presence of a good representation
of beliefs, namely, the max condition [24]. This fact reveals the necessity of using
complex epistemic states if we want properties like standard domain to hold.
Exploiting the dichotomy obtained from these results, it is possible to deter-
mine when an ES merging operator is absolutely G-manipulable: exactly when
it satisﬁes the standard domain condition and does not admit Arrovian dictators
(cf. Theorem 4). This technique leads us to determine that ES merging operators
with good behaviour, like sum, are absolutely G-manipulable.
6 Its assignment is the identity in single proﬁles.
7 We may also use this situation to show the absolute manipulation of sum and max.

Merging Epistemic States and Manipulation
469
Although our results also reveal that, under rational properties, strategy-
proofness entails the existence of a dictator, it is surprising to ﬁnd a dictatorial
operator which is absolutely manipulable, namely the Σ-projective operator.
This fact shows that strategy-proofness cannot be characterized in terms of
Arrovian dictators.
References
1. Arrow, K.: Social Choice and Individual Values, 1st edn. Wiley, New York (1951)
2. Barber`a, S.: Manipulation of social decision functions. J. Econom. Theory 15(2),
266–278 (1977)
3. Barber`a, S., Bossert, W., Pattanaik, P.K.: Ranking sets of objects, Handbook of
Utility Theory, vol. 2, chap. 17, pp. 893–978. Kluwer Publisher (2004)
4. Benferhat, S., Konieczny, S., Papini, O., Pino P´erez, R.: Iterated revision by epis-
temic states: Axioms, semantics and syntax. In: Proceedings of the 14th European
Conference on Artiﬁcial Intelligence, pp. 13–17. ECAI 2000. IOS Press, NLD (2000)
5. Chopra, S., Ghose, A.K., Meyer, T.A.: Social choice theory, belief merging, and
strategy-proofness. Inf. Fusion 7(1), 61–79 (2006)
6. Darwiche, A., Pearl, J.: On the logic of iterated belief revision. Artif. Intell. 89,
1–29 (1997)
7. Duggan, J., Schwartz, T.: Strategic manipulability without resoluteness or shared
beliefs: Gibbard-Satterthwaite generalized. Soc. Choice Welfare 17(1), 85–93
(2000)
8. Eckert, D., Pigozzi, G.: Belief merging, judgment aggregation and some links with
social choice theory. In: Delgrande, J., Lang, J., Rott, H., Tallon, J.M. (eds.)
Belief Change in Rational Agents: Perspectives from Artiﬁcial Intelligence, Philos-
ophy, and Economics. No. 05321 in Dagstuhl Seminar Proceedings, Internationales
Begegnungs- und Forschungszentrum f¨ur Informatik (IBFI), Schloss Dagstuhl, Ger-
many, Dagstuhl, Germany (2005)
9. Everaere, P., Konieczny, S., Marquis, P.: The strategy-proofness landscape of merg-
ing. J. Artif. Intell. Res. (JAIR) 28, 49–105 (2007)
10. Feldman, A.: Strongly nonmanipulable multi-valued collective choice rules. Public
Choice 35(3), 503–509 (1980)
11. de Finetti, B.: La pr´evision: Ses lois logiques, ses sources subjectives. Annales de
l’Institut Henri Poincar´e 17, 1–68 (1937)
12. Gabbay, D., Pigozzi, G., Rodrigues, O.: Belief revision, belief merging and voting.
In: Proceedings of the Seventh Conference on Logic and the Foundations of Games
and Decision Theory (LOFT06), pp. 71–78 (2006)
13. G¨ardenfors, P.: Manipulation of social choice functions. J. Econ. Theory 13(2),
217–228 (1976)
14. Gibbard, A.: Manipulation of voting schemes: a general result. Econometrica 41(4),
587–601 (1973)
15. Halpern, J.Y.: Deﬁning relative likelihood in partially-ordered preferential struc-
tures. J. Artif. Intell. Res. 7, 1–24 (1997)
16. Kelly, J.S.: Strategy-proofness and social choice functions without singlevaluedness.
Econometrica 45(2), 439–446 (1977)
17. Kelly, J.S.: Social Choice Theory: An Introduction. Springer-Verlag, Berlin (1988).
https://doi.org/10.1007/978-3-662-09925-4

470
A. Mata D´ıaz and R. Pino P´erez
18. Konieczny, S., Pino P´erez, R.: Merging information under constraints: a logical
framework. J. Log. Comput. 12(5), 773–808 (2002)
19. Konieczny, S., Pino P´erez, R.: Propositional belief base merging or how to merge
beliefs/goals coming from several sources and some links with social choice theory.
Eur. J. Oper. Res. 160(3), 785–802 (2005)
20. Konieczny, S., Pino P´erez, R.: Logic based merging. J. Philos. Logic 40(2), 239–270
(2011)
21. Konieczny, S., Lang, J., Marquis, P.: DA2 merging operators. Artif. Intell. 157(1–
2), 49–79 (2004), nonmonotonic Reasoning
22. Leal, J., Pino P´erez, R.: A weak version of Barber`a-Kelly’s theorem. Revista
Colombiana de Matem´aticas 51, 173–194 (2017)
23. Mata D´ıaz, A., Pino P´erez, R.: Logic-based fusion of complex epistemic states. In:
ECSQARU, pp. 398–409 (2011)
24. Mata D´ıaz, A., Pino P´erez, R.: Impossibility in belief merging. Artif. Intell. 251,
1–34 (2017)
25. Mata D´ıaz, A., Pino P´erez, R.: Epistemic states, fusion and strategy-proofness.
In: Ferm´e, E., Villata, S. (eds.) Proceedings of the 17th International Workshop
on Non-Monotonic Reasoning, NMR-2018, 27–29 October 2018, Tempe, Arizona
USA, pp. 176–185 (2018)
26. Mata D´ıaz, A., Pino P´erez, R.: Manipulability in logic-based fusion of belief
bases: indexes vs. liftings. Revista Iberica de Sistemas e Tecnologias de Informa˜cao
(RISTI) 2019(E20), 490–503 (2019)
27. Satterthwaite, M.A.: Strategy-proofness and Arrow’s conditions: existence and cor-
respondence theorems for voting procedures and social welfare functions. J. Econ.
Theory 10(2), 187–217 (1975)
28. Sen, A.: Quasi-transitivity, rational choice and collective decisions. Rev. Econ.
Stud. 36(3), 381–393 (1969)
29. Suzumura, K.: Introduction. In: Arrow, K., Sen, A.K., Suzumura, K. (eds.) Hand-
book of Social Choice and Welfare, Volume 1 (Handbooks in Economics), pp. 1–32.
North-Holland (2002)
30. Taylor, A.D.: The manipulability of voting systems. Am. Math. Mon. 109(4), 321–
337 (2002)
31. Taylor, A.D.: Social Choice and the Mathematics of Manipulation. Cambridge
University Press, Cambridge (2005)
32. van Benthem, J., Girard, P., Roy, O.: Everything else being equal: a modal logic
for ceteris paribus preferences. J. Philos. Log. 38, 83–125 (2009)

Explanation with the Winter Value:
Eﬃcient Computation for Hierarchical
Choquet Integrals
Christophe Labreuche1,2(B)
1 Thales Research and Technology, Palaiseau, France
christophe.labreuche@thalesgroup.com
2 SINCLAIR AI Lab, Palaiseau, France
Abstract. Multi-Criteria Decision Aiding arises in many industrial
applications where the user needs an explanation of the recommenda-
tion. We consider in particular an explanation taking the form of a con-
tribution level assigned to each variable. Decision models are often hier-
archical, and the inﬂuence is computed by the Winter value, which is an
extension of the Shapley value on trees. The contribution of the paper
is to propose an exact algorithm to compute eﬃciently the Winter val-
ues for a very general class of decision models known as the Choquet
integral. The main idea of our algorithm is to prune the combinatorial
structure on which the Winter value is computed, based on upper and
lower bounds of the utility on subtrees. Extensive simulations show that
this new algorithm provides very signiﬁcant computation gains compared
to the state of the art.
Keywords: Explanable AI · Winter value · Choquet integral
1
Introduction
The ability to explain AI algorithms is key in many domains [1]. We are inter-
ested in explaining decisions depending on several criteria. Multi-Criteria Deci-
sion Aiding (MCDA) aims at helping a decision maker to select one alternative
among several on the basis of multiple and conﬂicting criteria [9]. A very versa-
tile MCDA model is the Hierarchical Choquet Integral (HCI) model [3,23]. This
latter is composed of a set of Choquet integrals organized in a hierarchical way,
where the hierarchy comes from domain knowledge and eases the interpretability
of the model. The Choquet integral generalizes the weighted sum and is able to
capture various forms of interaction among criteria [7].
Example 1. The supervision of a metro line requires monitoring the satisfaction
of passengers on a daily basis. It is measured from several quality criteria: P
(Punctuality of the trains w.r.t. timetable), CT (number of Cancelled Trains),
R (Regularity: mean time between two successive trains) and TTT (Train Travel
Time: average journey time inside a train). These criteria are assessed against two
c
⃝Springer Nature Switzerland AG 2021
J. Vejnarov´a and N. Wilson (Eds.): ECSQARU 2021, LNAI 12897, pp. 471–485, 2021.
https://doi.org/10.1007/978-3-030-86772-0_34

472
C. Labreuche
types of hours (PH: Peak Hour; OPH: Oﬀ-Peak Hour), and three line segments in
the metro line (M1, M2 and M3). At the end, there are 4×3×2 = 24 elementary
criteria.
The criteria are organized in a hierarchical way – see Fig. 1. The top node
PQoS represents the Passengers’ Quality of Service. The second level is a decom-
position regarding the hour (PH vs. OPH). The next level separates the eval-
uation on each segment. For each hour type and segment, the four criteria
(P,. . .,TTT) are also organized hierarchically. P and CT are related to the
Planned Schedule (PS), whereas R and TTT are related to the Overall Train
Travel (OTT).
■
Fig. 1. Hierarchy of criteria for the supervision operator. The organization of the four
elementary criteria (P,. . .,TTT) is only shown for node PH-M1 for readability reasons.
But they are also present of the others nodes, in place of “· · · ”.
In Ex. 1, the supervision operator wishes to know whether the current situ-
ation is preferred (according to the PQoS utility) to a situation of the past. As
an explanation, the operator needs to understand which nodes in the tree are at
the origin of this preference. This is obtained by computing an index measuring
the inﬂuence of each node in the tree, on the preference between two options.
This inﬂuence index can be computed to a leaf node like PH-M1-P, or to an
intermediate node like PH-M1, as all nodes in the tree make sense to the user.
There are many connections with Feature Attribution (FA)1 in Machine Learn-
ing. Computing the level of contribution of a feature in a classiﬁcation black-box
model or that of a criterion in a MCDA model is indeed similar.
The Shapley value is one of the leading concepts for FA [29]. Unlike our
situation, feature attribution only computes the inﬂuence of leaves in a model.
It has been argued that the Shapley value is not appropriate on trees [16], when
we are interested in knowing the contribution level of not only the leaves but
also other nodes. A speciﬁc value for trees – called the Winter value – has been
deﬁned [16,30]. The Winter value takes the form of a recursive call of the Shapley
1 Feature Attribution consists in assessing the level of contribution of each feature in
the prediction model for a particular instance [13,26].

Winter Value for Hierarchical Choquet Integrals
473
value at several nodes in the hierarchy. Our inﬂuence index applied to any node
in a tree is thus obtained by the Winter value [16].
The main drawback of the Shapley and the Winter values is that their expres-
sions contain an exponential number of terms in the number of inputs. Several
approaches have been proposed to approximate the computation of the Shapley
value [5,21]. The drawback of these methods is that it is hard to have accurate
and reliable bounds of the error made. We explore another avenue in this paper.
We ﬁrst note that the Winter value (and also the Shapley value) can be written
as an average added-value of a given attribute over a combinatorial structure.
Taking inspiration from Branch & Bound (BB) algorithms, we develop a method
that prunes the exploration of the combinatorial structure when computing the
average. An advantage of our algorithm compared to the existing ones is that
it is exact. We develop this idea when the aggregation functions are Choquet
integrals. A major diﬃculty is to handle the hierarchy of aggregation models.
The aim of this paper is to propose an eﬃcient algorithm to compute the
Winter value on hierarchical Choquet integrals. Section 2 gives the reminders on
MCDA, the Choquet integral and the Winter value. Section 3 presents a novel
algorithm computing the Winter value, which prunes the underlying combina-
torial structure. Our algorithm is not a simple adaptation of BB as these latters
compute the optimal solution in a combinatorial structure whereas we need to
compute the average value of a quantity over all leaves of the tree. Finally, for a
general class of Choquet integrals (namely the 2-additive model), we experimen-
tally demonstrate in Sect. 4 the eﬃciency of our algorithm on a large number of
randomly generated trees and models.
2
Background
2.1
Multi-attribute Preference Model
We are interested in representing the preferences of a decision maker regarding
how to compare several alternatives from a ﬁnite set A of attributes. Attribute
j ∈A is depicted by a set Xj, and alternatives are elements of XA, where
XB := ×j∈BXj for any B ⊆A. Preferences are assumed to be modeled by a
numerical utility u : XA →R. For x ∈XA and B ⊆A, xB denotes the restriction
of x on B.
In order to be interpretable, model u is organized in a hierarchical way. We
consider a rooted tree T whose leaves are attributes A, and characterized by a
set N of nodes (with A ⊂N), a root node r ∈N\A, and the list of children
C(j) for every node j, with C(j) ̸= ∅iﬀj ∈N\A. Let D(j) be the set of leaves
which are descendants of j. For instance, D(r) = A and D(j) = {j} for every
j ∈A.
Following the tree structure, model u can be decomposed into a utility at each
node j ∈N given the value of the alternative on its descendent leaves, i.e. uj :
XD(j) →R. For j ∈N\A, uj results from the aggregation of its children C(j) by
an aggregation function Fj : RC(j) →R, i.e. uj(xD(j)) = Fj((uh(xD(h)))h∈C(j)).

474
C. Labreuche
At the end, for x ∈XA, the overall utility is u(x) = ur(x), where r is the root
of the tree.
2.2
Aggregation Models
There exist many classes of aggregation functions. The Choquet integral is a
powerful model able to capture interaction among criteria and thus to represent
real-life decision strategies. It takes the form for e ∈RC(j) [12]:
Fj(eC(j)) =

S⊆C(j)
η(S) ∧h∈S eh,
(1)
where ∧is the minimum function, and η : 2C(j) →R are the M¨obius coeﬃcients
with η(∅) = 0. We refer the reader to [12] for the list of properties of the Choquet
integral.
The drawback of this model is that it contains an exponential number of
parameters. The k-additive Choquet integral restricts this complexity by enforc-
ing that η(S) = 0 for all |S| > k. Value k = 2 is a good compromise between the
versatility of the model and a reasonable number of unknowns.
Example 2 (Ex. 1 cont.). The aggregation functions are given by (For the
sake of conciseness, we keep only the last letters in the name of the nodes):
FPQoS(uPH, uOPH) = (uPH +uPH ∧uOPH)/2, FPH(uM1, uM2, uM3) = uM1 ∧uM2 ∧
uM3, FM1(uPS, uPS) = (uPS+uOTT)/2, and FPS(uCT, uP) = (uCT+uCT∧uP)/2.
The weights in these models are learnt from training examples provided by an
expert [3,11].
■
2.3
Shapley and Winter Values
In Cooperative Game Theory, A is now the set of players, and we denote by G(A)
the set of set functions on A (called Transferable Utility (TU) games) v : 2A →R
satisfying v(∅) = 0, where v(S) (with S ⊆A) is the wealth produced by players
S when they form a coalition. The Shapley value is a fair share of the global
wealth v(A) produced by all players together, among themselves [28]. It gives
ShA
i (v) to player i:
ShA
i (v) = SA
i (δiv),
(2)
where δiv ∈G(A\{i}) is deﬁned by δiv(S) := v(S∪{i})−v(S), and SA
i is deﬁned
on a game v′ ∈G(A\{i}) by SA
i (v′) = 
S⊆A\{i} Cs
a v′(S), with Cs
a := s!(a−s−1)!
a!
,
and the notation that sets are denoted by capital letters (e.g. A, S) and their
cardinality by the lower case letter (e.g. a, s).
When the set of players is organized in a hierarchical way with a nested
coalition structure, it has been argued that the Shapley value is not consistent
with the hierarchy, as the Shapley value at a coalition is not necessarily equal to
the sum of the Shapley values of its members [16,25,30]. The Winter value has
been deﬁned to overcome this diﬃculty [30]. When computing the value of node

Winter Value for Hierarchical Choquet Integrals
475
i ∈A, the tree is restricted to the nodes at a distance at most 1 from the path
from the root r to node i. For node i =PH-M1-P in Example 1, the path from r
to i is in black in Fig. 1, and the restricted tree is composed of the grey or black
nodes. The path from the root of the tree to node i is denoted by (p0, p1, . . . , pq)
where p0 = r and pq = i. For m ∈{1, . . . , q}, we set Nm = C(pm−1) and
N ′
m = Nm\{pm}. The Winter value of node i is then a recursive application of
SNm
pm at all levels m = 1, . . . , q [16,18,30]:
Wini(v) = SN1
p1 ◦SN2
p2 ◦· · · ◦SNq
pq (δiv)
(3)
=

S1⊆N′
1,...,Sq⊆N′q

q

m=1
Csm
nm

×(v(Sq
1 ∪{i}) −v(Sq
1))
(4)
where Sq
k := ∪q
m=kSm and N ′q
k := ∪q
m=kN ′
m for k ∈{1, . . . , q}.
2.4
Explanation of Hierarchical MCDA
We are interested in an explanation of why an option y ∈XA is preferred to
another option x ∈XA (i.e. u(y) ≥u(x)), taking the form of an inﬂuence
index, denoted by Ii(x, y), computed at each node i ∈A. The idea is to consider
compound alternatives [ySx]A mixing the values of x and y, where [ySx]T ∈XT
is equal to yj for j ∈S ∩T and to xj if j ∈T\S. Then we deﬁne the contribution
of node i based on its added-value u([y{i}t]A)−u([x{i}t]A) for t taking all possible
values [ySx]A\{i}, S ⊆A\{i} [16]. The inﬂuence of node i is then the Winter
value of a particular game [16]:
Ii(x, y) = Wini(vA
u ),
(5)
where vT
u ∈G(T) is deﬁned by
vT
u (S) = u([ySx]T ) −u(xT )
(6)
Index (5) is characterized by 5 axioms: Null Attribute, Restricted Equal Treat-
ment, Additivity, Generalized Eﬃciency and Consistency with Restricted Tree
[16].
3
Computation of the Explanation for Hierarchical
Choquet Integrals
We present in this section a new algorithm to compute the inﬂuence index (5)
of a given criterion i ∈A when all aggregation functions Fj are general Choquet
integrals. We ﬁrst give some pruning and recursive properties on the inﬂuence
index and then we describe the exact algorithm.

476
C. Labreuche
3.1
Pruning and Recursive Properties
For the computation of the Winter value Ii(x, y) w.r.t. i ∈A, we only consider
a restricted tree. For the sake of simplicity of the notation, we assume that the
original tree is the restricted tree characterized by p1, . . . , pq, N1, . . . , Nq. Hence
the leaves are A = D(p0) = N ′q
1∪{i}. In order to avoid cumbersome notation, we
assume that the utility functions at the leaves are the identity function. Hence
Xj = R for all j ∈A. For an option z ∈XA, we denote by zj (for j ∈N\A) the
utility uj(zD(j)).
Example 3 (Ex. 2 cont.). We will illustrate our approach on the inﬂuence of
criterion PH-M1-P when comparing two alternatives x and y given in Table 1.
For the sake of concision, we keep only the last letters in the labels of the
nodes. For instance, PH-M1-P is simply noted P. In the restricted tree, A =
{P,CT,OTT,M2,M3,OPH}. In Table 1, the values of x and y on the leaves A are
in white, while their values on the other nodes are in grey (obtained by using
the aggregation functions).
■
We set zj = min(xj, yj) and zj = max(xj, yj) for all j ∈N. In the recursive
application of (3), we will ﬁx at some point the subsets at levels from 1 to a value
h, yielding option χ = [ySx]N′h
1 with S ⊆N ′h
1. Now for z ∈XA , we introduce
alternative zS,h taking the ﬁxed value χ at levels lower or equal to h, and the
value of z for levels strictly larger than h. For l ∈{1, . . . , q} and j ∈A ∩Nl, we
have
zS,h
j
=
⎧
⎨
⎩
yj
if l ≤h and j ∈S
xj
if l ≤h and j ̸∈S
zj
if l > h
For j ∈N\A, we set zS,h
j
= uj(zS,h
D(j)). For z = z (resp. z = z), zS,h
j
is the best
(resp. worst) possible value at node j when we ﬁx the values to χ at the nodes
having a depth lower or equal to h.
Example 4 (Ex. 3 cont.). Vector z{M2},3 (resp. z{M2},3), takes the value of z
(resp. z) for leaves of levels at least 4 (i.e. P, CT). The elements in black in
Table 1 are the leaves of level at most 3. They are ﬁxed to y for leaf in S = {M2}
and to x for the other leaves OTT, M3, OPH. Finally the components in grey are
the ones that are deduced from the application of the aggregation functions.
In z{M2,OTT},3 and z{M2,OTT},3, we take value 0.65 of y on OTT.
■
Instead of directly applying the nested sum of (4), we will use the recursive
formulae (3). We ﬁrst apply SN1
p1 and ﬁx thus a subset S1 ⊆N ′
1. We analyze,
as in a BB algorithm whether this ﬁxed subset S1 implies some pruning or
simpliﬁcation on the remaining term SN2
p2 ◦· · · ◦SNq
pq (δiv). If it is not the case,
we apply SN2
p2 and ﬁx thus a subset S2 ⊆N ′
2. And so on.
In order to support this recursive process, we denote by Jm,h
i
(f, S) (with
0 ≤m ≤h ≤q) the inﬂuence of node i w.r.t. a function f : XD(pm) →R deﬁned

Winter Value for Hierarchical Choquet Integrals
477
Table 1. Values of x, y, z, z, zS,h, zS,h for S = {M2}, {M2, OTT}, and h = 3.
PQoS
OPH
PH
M1
M2
M3
PS
OTT
P
CT
Tree
Level
0 (p0)
1 (p1)
2 (p2)
3 (p3)
4 (p4)
4
3
2
2
1
y
0.25
0.4
0.625
0.6
0.7
0.6
0.65
0.4
0.7
0.1
x
0.05
0.1
0.175
0.15
0.1
0.2
0.2
0.1
0.8
0
z
0.25
0.4
0.625
0.6
0.7
0.6
0.65
0.4
0.8
0.1
z
0.05
0.1
0.175
0.15
0.1
0.2
0.2
0.1
0.7
0
z{M2},3
0.2
0.4
0.4
0.6
0.7
0.6
0.2
0.4
0.8
0
z{M2},3
0.088
0.175
0.175
0.15
0.1
0.2
0.2
0.4
0.8
0
z{M2,OTT},3 0.2
0.4
0.625
0.6
0.7
0.6
0.65
0.4
0.8
0
z{M2,OTT},3 0.2
0.4
0.4
0.15
0.1
0.2
0.65
0.4
0.8
0
at node pm (with D(pm) = N ′q
m+1 ∪{i}) when subsets Sm+1, . . . , Sh are ﬁxed
(with S = Sh
m+1):
Jm,h
i
(f, S) = SNh+1
ph+1 ◦· · · ◦SNq
pq (δiv)
(7)
where v ∈G(D(ph)) is given by v(T) = vD(pm)
f
(T ∪S), and vD(pm)
f
is deﬁned by
(6).
The next lemma initiates the recursion.
Lemma 1.
Ii(x, y) = J0,0
i
(up0, ∅)
(8)
Example 5 (Ex. 4 cont.). IP(x, y) = J0,0
P (uPQoS, ∅).
■
As for Ii, operator Jm,h
i
(f, S) is linear in its function f. The next result is
then obtained by using the linearity (1) of the Choquet integral.
Lemma 2. For m ≤h < q
Jm,h
i
(upm, S) =η({pm+1}) Jm,h
i
(upm+1, S)
(9)
+

T ⊆Nm+1:pm+1∈T,|T |>1
η(T) Jm,h
i
(∧j∈T uj, S)
Example 6 (Ex. 5 cont.). As uPQoS
=
(uPH + uPH ∧uOPH)/2, we have
J0,0
P (uPQoS, ∅) = 1
2J0,0
P (uPH, ∅) + 1
2J0,0
P (uPH ∧uOPH, ∅).
■

478
C. Labreuche
As most of the terms in (9) correspond to a min function, we derive
eﬃcient pruning strategies for the min. For instance, the computation of
Jm,h
i
(upm+1 ∧uj, S) requires to explore uj
∈{xj, yj}, ui ∈{xi, yi} and
Sm+2 ⊆N ′
m+2, . . . , Sq ⊆N ′
q. If uj is always lower than upm+1, whatever the
choice of uj, ui, Sm+2, . . . , Sq, then upm+1 is drowned by uj so that there is no
inﬂuence of changing xi to yi. Hence Jm,h
i
(upm+1 ∧uj, S) = 0. We generalize this
example in the next result.
Lemma 3. Let T ⊆Nm+1 with pm+1 ∈T. If ∃j ∈T\{pm+1} such that
zS,h
pm+1 ≥zS,h
j
then
Jm,h
i
(∧j∈T uj, S) = 0
(10)
Example 7 (Ex. 6 cont.). As {xPH, yPH} ≥{xOPH, yOPH}, we have z∅,0
PH = 0.1 ≥
z∅,0
OPH = 0.1. Thus by Lemma 3, J0,0
P (uPH ∧uOPH, ∅) = 0. Hence J0,0
P (uPQoS, ∅) =
1
2J0,0
P (uPH, ∅).
■
Term J0,0
P (uPH, ∅) computes the inﬂuence of a function deﬁned on the top
node PQoS; but its argument is a function depending only on the ﬁrst variables
uPH and not on the other criteria N ′
1. The following lemma shows that we can
skip the sum SNh+1
ph+1 over S1 ⊆N ′
1.
Lemma 4. For m ≤h < q
Jm,h
i
(upm+1, S) = Jm+1,(m+1)∨h
i
(upm+1, S ∩D(pm+1))
(11)
Example 8 (Ex. 7 cont.). Hence J0,0
P (uPH, ∅) = J1,1
P (uPH, ∅).
■
The ﬁrst operation in Jm,h
i
(∧j∈T uj, S) is the sum over Sh+1 ⊆N ′
h+1 appear-
ing in SNh+1
ph+1 . Due to the speciﬁcity of the min operator, we can in general
reduce the computation complexity by removing some elements of N ′
h+1 in the
sum. More speciﬁcally, any element j ∈N ′
h+1 belongs to one of the following
cases:
– Case 1 (set N ¬
h+1): The value on j does not inﬂuence δiv in (7) and thus j
can be removed from the sum;
– Case 2 (set N x
h+1): Term δiv in the sum is zero when j takes value y. Thus
there remains only the case where the value on j is equal to x in the sum;
– Case 3 (set N y
h+1): Likewise there remains only the value of j equal to y in
the sum;
– Case 4 (set N xy
h+1): j can take values x or y.

Winter Value for Hierarchical Choquet Integrals
479
At the end, the elements in set N ¬
h+1 completely disappear from the expression,
and the sum over N ′
h+1 is restricted to only a sum over N xy
h+1. Sets N x
h+1 and
N y
h+1 constraint the value that j can take. Lastly, the support of ∧j∈T uj can
be reduced. The support of the min may vary for each subset 
S and is given by
T S,h ⊆T.
Lemma 5. Let T ⊆Nm+1 with pm+1 ∈T. We have
Jm,h
i
(∧j∈T uj, S)
=

S′⊆Nxy
h+1
C
s′+ny
h+1
nx
h+1+ny
h+1+nxy
h+1Jm,h+1
i

∧j∈T S∪S′,h+1 uj, S ∪S′ ∪N y
h+1

(12)
where
– T S,h = {j ∈T : zS,h
j
< zS,h
pm+1}
– N ¬
h+1 =

j ∈N ′
h+1 : yj = xj or j ̸∈T S,h
if h = m, and N ¬
h+1 = {j ∈N ′
h+1 :
yj = xj} else
– N x
h+1 = {j ∈N ′
h+1\N ¬
h+1 : yj ≤zph+1}
– N y
h+1 = {j ∈N ′
h+1\N ¬
h+1 : xj ≤zph+1}
– N xy
h+1 = {j ∈N ′
h+1\N ¬
h+1 : zj > zph+1}
We ﬁrst illustrate the case where the sum over N xy
h+1 is composed of only one
term, and one element from T is removed in the min.
Example 9 (Ex. 8 cont.). Sum at level 2: J1,1
P (uPH, ∅) = J1,1
P (uM1∧uM2∧uM3, ∅).
Criterion M3 is removed from the min (i.e. T = {M1, M2, M3} but T ∅,1 =
{M1, M2}) as z∅,1
M3 = zM3 = 0.7 ≥z∅,1
M1 = zM1 = 0.625. Intuitively, M3 is removed
as the min can never be reached by M3. Hence N ¬
2 = {M3}. Moreover N x
2 = ∅;
N y
2 = {M2} as xM2 = 0.1 ≤zM1 = 0.175; N xy
2
= ∅. Hence there is only one term
in the sum: IP(x, y) = 1
2J1,2
P (uM1 ∧uM2, {M2}).
■
We now develop the computation of J1,2
P (uM1 ∧uM2, {M2}).
Example 10 (Ex. 9 cont.). Sum at level 3: We have T {M2},2 = {M1, M2} as
z{M2},2
M2
= yM2 = 0.4 < z{M2},2
M1
= zM1 = 0.625. Moreover N ¬
3 = ∅; N x
3 = ∅;
N y
3 = ∅; N xy
3
= {OTT} as zOTT = 0.2 > zPS = 0.15. As N xy
3
̸= ∅, there are thus
two terms in the sum. The support of the min for these two terms is T {M2},3 =
{M1} as we remove M2 from T {M2},2 (because z{M2},3
M2
= 0.4 ≥z{M2},3
M1
= 0.4),
and T {M2,OTT},3 = {M1, M2} as z{M2,OTT},3
M2
= 0.4 < z{M2,OTT},3
M1
= 0.625.
Hence IP(x, y) = 1
2J1,2
P (uM1 ∧uM2, {M2}) = 1
4J1,3
P (uM1, {M2}) + 1
4J1,3
P (uM1 ∧
uM2, {M2, OTT}).
■
We use the previous results to further prune the computation.

480
C. Labreuche
Example 11 (Ex. 10 cont.). We have J1,3
P (uM1 ∧uM2, {M2, OTT}) = 0 by
Lemma 3 as z{M2,OTT},3
M1
= 0.4 ≥z{M2,OTT},3
M2
= 0.4. Hence IP(x, y) =
1
4J1,3
P (uM1, {M2}).
By Lemma 4, J1,3
P (uM1, {M2}) = J2,3
P (uM1, ∅). By Lemma 2, J2,3
P (uM1, ∅) =
1
2J2,3
P (uPS, ∅) as uM1 =
uPS+uOTT
2
. By Lemma 4, J2,3
P (uPS, ∅) = J3,3
P (uPS, ∅).
By Lemma 2, J3,3
P (uPS, ∅) = 1
2J3,3
P (uP ∧uCT, ∅). Hence IP(x, y) =
1
16J3,3
P (uP ∧
uCT, ∅).
Sum at level 4 with Lemma 5: T ∅,3 = {P, CT} as z∅,3
CT = zCT = 0.2 < z∅,3
P
=
zP = 0.7. Moreover N ¬
3 = ∅; N x
3 = ∅; N y
3 = ∅; N xy
3
= {CT} as zCT = 0.2 >
zP = 0.1. Hence IP(x, y) =
1
32J3,4
P (uP ∧uCT, ∅) + 1
32J3,4
P (uP ∧uCT, {CT}).
■
The last result describes the formula when we reach leaf i at the ﬁnal level q.
Lemma 6.
Jm,q
i
(v, S) = v

[yS∪{i}x]D(pm)

−v

[ySx]D(pm)

Example 12 (Ex. 11 cont.). By Lemma 6, IP(x, y) = 1/32

yP∧xCT−xP∧xCT

+
1/32

yP ∧yCT −xP ∧yCT

= 0.1/32 + 0.5/32 = 0.6/32.
■
Finally in the example, the computation of IP(x, y) requires only the com-
putation of two diﬀerences (the other terms being zero). By contrast, the mere
application of (5) and (4) requires the computation of 21 × 22 × 21 × 21 = 32
diﬀerences.
3.2
Algorithm for Hierarchical Choquet Integrals
We introduce Algorithms 1 and 2 deﬁning functions ISin and IMin respectively,
where ISin(i, m, h, S) returns the inﬂuence of node i on upk and IMin(i, T, m, h, S)
returns the inﬂuence of node i on ∧j∈T uj. The next result shows that index
Ii(x, y) is the result of ISin(i, 0, 0, ∅).
Function ISin(i, m, h, S):
1 If m = q and h = q then
2
return ui(yi) −ui(xi)
3 Else
4
r ←0
5
For T ⊆N ′
m+1
6
If pm+1 ∈T and η(T) ̸= 0 then
7
If |T| = 1 then
8
r ←r + η(T) × ISin(i, m + 1, (m + 1) ∨h, S ∩D(pm+1))
10
Else
11
r ←r + η(T) × IMin(i, T, m, h, S)
12
return r
Algorithm 1: Function ISin (Sin stands for singleton).

Winter Value for Hierarchical Choquet Integrals
481
Function IMin(i, T, m, h, S):
1 If ∃l ∈T \ {pm+1} s.t. zS,h
pm+1 ≥zS,h
j
then
2
return 0
3 ElseIf h = q then
4
return ∧j∈T uj

[yS∪{i}x]D(pm)

−∧j∈T uj

[ySx]D(pm)

5 Else
6
r ←0
7
For S′ ⊆N xy
h+1
8
r ←r + C
s′+ny
h+1
nx
h+1+ny
h+1+nxy
h+1 × IMin(i, T S∪S′,h+1, m, h + 1, S ∪S′ ∪N y
h+1)
9
return r
Algorithm 2: Function IMin for ∧j∈T uj, with T ⊆Nm+1.
Theorem 1. Ii(x, y) is given by ISin(i, 0, 0, ∅).
The application of ISin(i, 0, 0, ∅) is denoted by WINT-P, where P stands for
Pruned. In order to assess the eﬃciency of this algorithm, we compare it in the
next section to the mere application of (5) and (4) – denoted by WINT.
4
Experimental Analysis of the Computation Time
We are interested in this section on comparing the computation time of WINT-P
compared to WINT. In the general case, we cannot obtain interesting bounds
between these two algorithms. In the best case, WINT-P can prune the tree at
the highest level and requires only a few operations while WINT is exponen-
tial in the number of criteria. In the worst case, there might be no pruning in
WINT-P, so that the whole tree N ′
1 × · · · × N ′
q needs to be explored, as for
WINT. However, there are extra-computation operations in WINT-P for testing
whether the pruning conditions apply. As a consequence, we need an experimen-
tal comparison of the computation time of WINT-P and WINT, from randomly
generated instances, which is the aim of this section. We restrict ourselves to
2-additive Choquet integrals. We denote by CWINT the complexity of algorithm
WINT, and by CWINTP the complexity of our new algorithm WINT-P.
We randomly generate more than 100 000 trees, with |A| ranging from 2 to
100, the depth between 1 and 6, and the number of children between 2 and 6.
For each tree, we randomly generate two instances x and y. We also randomly
generate 2-additive Choquet integrals at each aggregation node.
Figure 2 shows the value of R =
CWINT
CWINTP . Ratio R is on average increasing
with |A|. We ﬁrst analyze the cases where WINT-P is more time consuming than
WINT, i.e. R < 1. This occurs in 0.2% of instances when |A| ≤20, in 0.05% of
instances when 21 < |A| ≤30, in 0.009% when 31 < |A| ≤50, and never occurs
when |A| > 50. In the worst cases, ratio R is never below 0.33 whatever |A|, is
never below 0.5 when |A| > 20, and is never below 1.2 when |A| > 50. Moreover,
ratio R is of order 100 for |A| > 50. Lastly, the computation time CWINTBB is
acceptable as it is below 1s in most instances.

482
C. Labreuche
Fig. 2. Boxplots of log10(R) where the mth graph (m ∈{1, . . . , 10}) considers only the
instances with |A| ∈

10(m −1) + 1, 10m

.
Fig. 3. PDP of each variable vs. ratio R (Y -axis). The variable (in the X-axis) is the
number of criteria |A| for Fig. a, the number of aggregation nodes |N\A| for Fig. b,
the tree depth for Fig. c, the maximal value of |C(i)| for Fig. d, the interactionRate
for Fig. e and the sign for Fig. f.
In the experiments, we also analyze how the computation gain R depends on
the following six variables: the number of criteria |A|, the number of aggregation
nodes |N\A|, the tree depth, the maximal value of |C(i)|, the interactionRate
and the sign. Figure 3 shows the Partial Dependent Plots (PDP) of each of these
variables vs. ratio R.
Figure 3.a shows that there is a very large improvement of computation time
when the attributes number is beyond 40. According to Fig. 3.b, the improve-
ment is the largest when the number of aggregation nodes is around 30–40. When
the number of aggregation nodes is very large (above 60), the average number
of children is small and thus the pruning strategy is less eﬃcient. The average
improvement rate grows exponentially with the depth of the tree and the num-
ber of children – see Fig. 3.c and 3.d. As expected, we see that there are more
improvements for smaller values of the rate interactionRate of non-zero M¨obius

Winter Value for Hierarchical Choquet Integrals
483
coeﬃcients – see Fig. 3.e. Above 0.5 there is no signiﬁcant diﬀerence. Finally,
the sign of the interaction does not really inﬂuence the performance (Fig. 3.f).
It is an interesting feature of our approach not to rely on a special convexity or
concavity of the model, unlike some existing works [10].
5
Related Works
The problem of assessing the contribution level of variables has received a lot
of attention in many domains and can be split into two classes. In the ﬁrst
one, there is no dataset. A typical example is Global Sensitivity Analysis (GSA)
aiming at allocating the uncertainty in the output of a model on the diﬀerent
sources of uncertainty in the model input [27]. The Shapley values have been
recently used in GSA as an alternative to the Sobol’ indices to better account
for dependent inputs [24]. In MCDA, the compellingness is based on the local
weight of variables [4,15]. However, it returns counter-intuitive behaviors as it
fails to satisfy some important properties [16].
The second class uses a dataset [26]. The Shapley value has recently become
popular in Machine Learning (ML) for feature attribution [8,19,20,22,29]. What
distinguishes these methods is the game on which the Shapley value is com-
puted. The exponential number of terms in the expression of the Shapley value
implies that its brute-force computation takes unreasonable long time even for
20 inputs. Several methods have been developed to reduce its computational
time. The Shapley value can be rewritten as an average of the contribution of
a variable over all possible permutations of the set of variables. This value can
be approximated by sampling the set of permutations [5,21] (see [8,29] in the
context of ML). The second approach is based on the fact that the Shapley value
is the unique solution of a convex optimization problem under linear constraints
[19]. Lastly the Shapley value is approximated by restricting coalitions to the
neighbor features in the graph of conditional dependencies among features [6].
6
Conclusion
The Winter value, which is an extension of the Shapley value on trees, is used
to identify which attributes mostly contribute to the decision. A bottleneck for
the practical usage of these two values is their exponential number of terms.
While most of existing works propose approximate algorithms, the originality
of our approach is to deﬁne an exact algorithm that prunes the combinatorial
structure of the Winter value. We consider the case of a hierarchical MCDA
model composed of Choquet integrals. We ﬁrst give some recursive properties
of the inﬂuence, and then derive an exact algorithm where the combinatorial
structure is pruned based on upper and lower bounds on subtrees. Extensive
experiments on randomly generated instances show a signiﬁcant improvement of
the computation time over the current algorithm.
Our approach can be extended in several directions. First, we could address
a wider classes of models, and in particular fuzzy decision trees for which the

484
C. Labreuche
min and max operators can be used as the fuzzy conjunction and disjunction
operators. Another extension could be to consider general DAG (Directed Acyclic
Graphs) rather than trees.
Acknowledgments. This paper is supported by the European Union’s Horizon
2020 research and innovation programme under grant agreement No 825619. AI4EU
Project.(2 https://www.ai4europe.eu/).
References
1. Arrieta, A.B., et al.: Explainable artiﬁcial intelligence (XAI): concepts, taxonomies,
opportunities and challenges toward responsible AI. Inf. Fusion 58, 82–115 (2020)
2. Belahc`ene, K., Labreuche, C., Maudet, N., Mousseau, V., Ouerdane, W.: Compar-
ing options with argument schemes powered by cancellation. In: Proceedings of
the Twenty-Eight International Joint Conference on Artiﬁcial Intelligence (IJCAI
2019), Macao, China, pp. 1537–1543, August 2019
3. Bresson, R., Cohen, J., H¨ullermeier, E., Labreuche, C., Sebag, M.: Neural represen-
tation and learning of hierarchical 2-additive Choquet integrals. In: Proceedings of
the Twenty-Eight International Joint Conference on Artiﬁcial Intelligence (IJCAI
2020), Yokohoma, Japan, pp. 1984–1991 (2020)
4. Carenini, G., Moore, J.D.: Generating and evaluating evaluative arguments. Artif.
Intell. 170, 925–952 (2006)
5. Castro, J., G´omez, D., Tejada, J.: Polynomial calculation of the Shapley value
based on sampling. Comput. Oper. Res. 36, 1726–1730 (2009)
6. Chen, J., Song, L., Wainwright, M., Jordan, M.: L-Shapley and C-Shapley: eﬃcient
model interpretation for structured data. arXiv preprint arXiv:1808.02610 (2018)
7. Choquet, G.: Theory of capacities. Annales de l’Institut Fourier 5, 131–295 (1953)
8. Datta, A., Sen, S., Zick, Y.: Algorithmic transparency via quantitative input inﬂu-
ence: theory and experiments with learning systems. In: IEEE Symposium on Secu-
rity and Privacy, San Jose, CA, USA, May 2016
9. Figueira, J., Greco, S., Ehrgott, M., (eds.) Multiple Criteria Decision Analysis:
State of the Art Surveys, 2nd edition. Kluwer Acad. Publ. (2016)
10. Galand, L., Lesca, J., Perny, P.: Dominance rules for the Choquet integral in multi-
objective dynamic programming. In: 23rd International Joint Conference on Arti-
ﬁcial Intelligence (IJCAI 2013), Beijing, China, pp. 538–544, August 2013
11. Grabisch, M., Kojadinovic, I., Meyer, P.: A review of capacity identiﬁcation meth-
ods for Choquet integral based multi-attribute utility theory – applications of the
Kappalab R package. Eur. J. Oper. Res. 186, 766–785 (2008)
12. Grabisch, M., Labreuche, C.: A decade of application of the Choquet and Sugeno
integrals in multi-criteria decision aid. Ann. Oper. Res. 175, 247–286 (2010)
13. Guidotti, R., Monreale, A., Ruggieri, S., Turini, F., Giannotti, F., Pedreschi, D.:
A survey of methods for explaining black box models. ACM Comput. Surv. 51(6),
1–42 (2018)
14. Herlocker, J.L., Konstan, J.A., Riedl, J.: Explaining collaborative ﬁltering recom-
mendations. In: CSCW, pp. 241–250 (2000)
15. Klein, D.A.: Decision Analytic Intelligent Systems: Automated Explanation and
Knowledge Acquisition. Lawrence Erlbaum Associates (1994)

Winter Value for Hierarchical Choquet Integrals
485
16. Labreuche, C., Fossier, S.: Explaining multi-criteria decision aiding models with
an extended shapley value. In: Proceedings of the Twenty-Seventh International
Joint Conference on Artiﬁcial Intelligence (IJCAI 2018), Stockholm, Sweden, pp.
331–339, July 2018
17. Labreuche, C.: A general framework for explaining the results of a multi-attribute
preference model. Artif. Intell. 175, 1410–1448 (2011)
18. Labreuche, C.: Explaining hierarchical multi-linear models. In: Proceedings of the
13th International Conference on Scalable Uncertainty Management (SUM 2019),
Compi`egne, France, December 2019
19. Lundberg, S., Lee, S.I.: A uniﬁed approach to interpreting model predictions. In:
Guyon, I., et al. (eds.) 31st Conference on Neural Information Processing Systems
(NIPS 2017), Long Beach, CA, USA, pp. 4768–4777 (2017)
20. Lundberg, S., Enrion, G., Lee, S.I.: Consistent individualized feature attribution
for tree ensembles. arXiv preprint arXiv:1802.03888 (2018)
21. Maleki, S., Tran-Thanh, L., Hines, G., Rahwan, T., Rogers, A.: Bounding the
estimation error of sampling-based Shapley value approximation. arXiv:1306.4265
(2013)
22. Merrick, L., Taly, A.: The explanation game: explaining machine learning models
with cooperative game theory. arXiv preprint arXiv:1909.08128 (2018)
23. Ovchinnikov, S.: Max-min representation of piecewise linear functions. Contrib.
Algebra Geom. 43, 297–327 (2002)
24. Owen, A.B.: Sobol’ indices and Shapley value. SIAM/ASA J. Uncertain. Quant.
2, 245–251 (2014)
25. Owen, G.: Values of games with a priori unions. In: Moeschlin, O., Hein, R., (ed.),
Essays in Mathematical Economics and Game Theory, pp. 76–88. Springer, Hei-
delberg (1977). https://doi.org/10.1007/978-3-642-45494-3 7
26. Ribeiro, M.T., Singh, S., Guestrin, C.: “Why Should I Trust You?”: explaining the
predictions of any classiﬁer. In: KDD 2016 Proceedings of the 22nd ACM SIGKDD
International Conference on Knowledge Discovery and Data Mining, San Francisco,
California, USA, pp. 1135–1144 (2016)
27. Saltelli, A., et al.: Global Sensitivity Analysis: The Primer. Wiley, New York (2008)
28. Shapley, L.S.: A value for n-person games. In: Kuhn, H.W., Tucker, A.W. (eds.)
Contributions to the Theory of Games, vol. II, number 28 in Annals of Mathematics
Studies, pp. 307–317. Princeton University Press (1953)
29. ˇStrumbelj, E., Kononenko, I.: An eﬃcient explanation of individual classiﬁcations
using game theory. J. Mach. Learn. Res. 11, 1–18 (2010)
30. Winter, E.: A value for cooperative games with levels structure of cooperation. Int.
J. Game Theory 18, 227–240 (1989)
31. Zhong, Q., Fan, X., Toni, F., Luo, X.: Explaining best decisions via argumentation.
In: Proceedings of the European Conference on Social Intelligence (ECSI-2014),
Barcelona, Spain, pp. 224–237, November 2014

Inducing Inference Relations
from Inconsistency Measures
Xiaolong Liu1,2(B)
, Philippe Besnard2(B), and Sylvie Doutre2(B)
1 Department of Philosophy, Institute of Logic and Cognition,
Sun Yat-sen University, Guangzhou, China
liuxlong6@mail2.sysu.edu.cn
2 IRIT-CNRS, University of Toulouse, Toulouse, France
{xiaolong.liu,philippe.besnard,sylvie.doutre}@irit.fr
Abstract. This article deﬁnes a family of inference relations which aim
at reasoning with inconsistent knowledge bases. These inference relations
are deﬁned out of inconsistency measures. We check under which con-
ditions the new inference relations satisfy a series of properties for non-
monotonic reasoning. We also show that they are paraconsistent when
the corresponding inconsistency measures satisfy some rationality postu-
lates. Besides, we show some dependencies and incompatibilities among
some rationality postulates.
Keywords: Inconsistency measures · Inference relations · Rationality
1
Introduction
Inconsistency measures are intended to provide a measure of how inconsistent a
knowledge base is. Informally speaking, an inconsistency measure is a function
which assigns a non-negative real value to a knowledge base with the mean-
ing that larger values indicate a larger inconsistency. As pointed out in Bryson
Brown’s article [3], an application of inconsistency measures is to induce para-
consistent inference relations, which are able to draw valuable information from
inconsistent knowledge bases. Although many inconsistency measures have been
proposed so far, there are few inference relations based on them. A drawback
of such existing inference relations proposed in [3] is that they are restricted
to particular inconsistency measures. It would be desirable to ﬁnd an approach
that works for a broader set of inconsistency measures of the existing literature.
To the best of our knowledge, no such general approach exists yet.
It has long been known that a paraconsistent logic, such as LP [11], can
be used to deﬁne an inconsistency measure [4,6,10]. In this article, we inves-
tigate the other way around: How to deﬁne a paraconsistent inference relation
out of a given inconsistency measure. For a discussion on relationships between
inconsistency measures and paraconsistent consequence, see [3]. Our idea is to
investigate what happens, under an inconsistency measure I, when supplement-
ing a knowledge base with the negation of a formula. If I would assign a greater
c
⃝Springer Nature Switzerland AG 2021
J. Vejnarov´a and N. Wilson (Eds.): ECSQARU 2021, LNAI 12897, pp. 486–498, 2021.
https://doi.org/10.1007/978-3-030-86772-0_35

Inducing Inference Relations from Inconsistency Measures
487
inconsistency degree to the knowledge base supplemented with the negation of
the formula than to the knowledge base itself, then it would mean somehow that
the knowledge base infers the formula. Regarding such an expected behaviour
also as a suﬃcient condition, a notion of inference induced from I appears.
A number of rationality postulates for inconsistency measures have been pro-
posed (e.g., [5,9,14]). A survey is provided in [15]. In this article, when an
inconsistency measure satisﬁes some rationality postulates, we will explore which
properties are satisﬁed by the inference relation induced from this measure.
Paper Outline. Section 2 introduces the new inference relations induced from
inconsistency measures. Section 3 (resp. Section 4) investigates structural prop-
erties (resp. KLM properties) enjoyed by these relations. Section 5 deﬁnes para-
consistency and investigates under which conditions the new inference relations
are paraconsistent. Section 6 shows dependencies and incompatibility for some
rationality postulates. Section 7 concludes and points out future work.
2
Inference Relations from Inconsistency Measures
Before introducing the new inference relations, we present some basic notions.
We consider a countable set of propositional atoms A. We use a, b, c, . . . to denote
atoms. The resulting logical language, using the connectives ¬, ∨, ∧, and, →, is
denoted L. We use Greek letters, ϕ, ψ, . . . to denote formulas of L.
We write K for the set of knowledge bases over L (where a knowledge base
K is a ﬁnite set of formulas of L, in symbols: K ⊆ﬁn L). A minimal inconsistent
subset of a knowledge base K is an inconsistent K′ ⊆K such that all proper
subsets of K′ are consistent. The set of minimal inconsistent subsets of K is
denoted by MI(K). The set of atoms occurring in K is denoted by Atoms(K).
An atomic injective substitution for K is an injection σ: Atoms(K) →A. Then
σ(ϕ) is the formula resulting from ϕ by replacing a ∈Atoms(K) with σ(a)
simultaneously. We extend σ to an injective substitution for K = {ϕ1, . . . , ϕm}
with σ(K) = {σ(ϕ1), . . . , σ(ϕm)}.
Moreover, we write |= for the consequence relation of classical logic. As we
write ⊥for falsum, the notation K |= ⊥means that K is inconsistent. Logical
equivalence is denoted ≡, i.e., ϕ ≡ψ means that ϕ |= ψ and ψ |= ϕ hold.
K ≡K′ stands for K |=  K′ and K′ |=  K. Lastly, a formula ϕ is free for
K iﬀfor every consistent subset K′ of K, K′ ∪{ϕ} ̸|= ⊥. A formula ϕ is safe
for K iﬀAtoms(ϕ) ∩Atoms(K) = ∅and ϕ is free for K.
Deﬁnition 1 (Inconsistency measure).
A function I : K →R∞
≥0 is an
inconsistency measure [2] if it satisﬁes the following two conditions:
Consistency Null: I(K) = 0 iﬀK ̸|= ⊥.
Variant Equality: If σ is an injective substitution then I(K) = I(σ(K)).
For example, the drastic inconsistency measure Id is a trivial measure that
distinguish only between consistent and inconsistent sets of formulas, formally,

488
X. Liu et al.
Id(K) = 0 if K is consistent and 1 if K is inconsistent. IMI counts the number
of minimal inconsistent subsets, formally, IMI(K) = |MI(K)|.
We are now in the position to deﬁne an inference relation induced from an
inconsistency measure, according to the intuition introduced in Sect. 1. That is,
we are about to deﬁne K ⊩ϕ as I(K ∪{¬ϕ}) > I(K). However, we slightly
amend this in the actual deﬁnition below because we must take care of the special
case ¬ϕ ∈K (which obviously gives I(K ∪{¬ϕ}) = I(K) no matter what).
Deﬁnition 2 (Induced inference relations). Let I be an inconsistency mea-
sure. For any K ∈K, the inference relation ⊩induced from I is given by
K ⊩ϕ iﬀI(K ∪{¬ϕ}) > I(K\{¬ϕ}).
Lemma 1. Let I be an inconsistency measure.
K ⊩ϕ iﬀ

I(K ∪{¬ϕ}) > I(K)
if ¬ϕ ̸∈K
I(K) > I(K\{¬ϕ})
if ¬ϕ ∈K
Lemma 2. K ⊩ϕ iﬀK\{¬ϕ} ⊩ϕ.
It is desirable that for any consistent knowledge base, ⊩gives the same
consequences as classical logic. The following theorem guarantees this.
Theorem 1. Let I be an inconsistency measure. For all consistent K ∈K,
K ⊩ϕ iﬀK |= ϕ.
Proof. Take a consistent knowledge base K. (⇒) Suppose K ⊩ϕ. By Deﬁni-
tion 2, I(K ∪{¬ϕ}) > I(K\{¬ϕ}). It is the case that I(K\{¬ϕ}) = 0, due
to Consistency Null and the consistency of K\{¬ϕ}. Hence I(K ∪{¬ϕ}) > 0.
By Consistency Null, I(K ∪{¬ϕ}) > 0 iﬀK ∪{¬ϕ} is inconsistent. There-
fore, K |= ϕ. (⇐) Suppose K |= ϕ. Then K ∪{¬ϕ} is inconsistent. Apply-
ing Consistency Null yields I(K ∪{¬ϕ}) > 0 and I(K\{¬ϕ}) = 0. Hence,
I(K ∪{¬ϕ}) > I(K\{¬ϕ}). That is, K ⊩ϕ by Deﬁnition 2.
Before proceeding, we must pay special attention to a rationality postulate
called Free-Formula Independence [2], which can be written in two ways:
Set Union version: If ϕ is free for K then I(K ∪{ϕ}) = I(K).
Set Diﬀerence version: If ϕ is free for K then I(K) = I(K\{ϕ}).
Lemma 3. The two versions of Free-formula Independence are equivalent.
Proof. The ﬁrst case is ϕ ∈K. We start to prove that set union version entails
set diﬀerence version. Assume that ϕ is free for K. Then ϕ is also free for K\{ϕ}.
By set union version, I((K\{ϕ})∪{ϕ}) = I(K\{ϕ}). That is, I(K) = I(K\{ϕ})
due to the assumption ϕ ∈K. The converse is obvious as I(K ∪{ϕ}) = I(K)
always holds due to the assumption ϕ ∈K.
The second case is ϕ /∈K. It is obvious that set union version entails set
diﬀerence version, since I(K) = I(K\{ϕ}) always holds due to the assumption

Inducing Inference Relations from Inconsistency Measures
489
ϕ /∈K. To prove the converse, we assume that ϕ is free for K. It suﬃces to
show that ϕ is also free for K ∪{ϕ}. We assume for the contradiction that ϕ
is not free for K ∪{ϕ}. Then there exists a consistent subset K1 of K ∪{ϕ}
such that K1 ∪{ϕ} |= ⊥by the deﬁnition of free formulas. If ϕ ∈K1, then we
have K1 |= ⊥which contradicts the consistency of K1. If ϕ /∈K1 (which entails
K1 ⊆K), then the assumption K1 ∪{ϕ} |= ⊥contradicts the assumption that
ϕ is free for K. To sum up, ϕ is free for K ∪{ϕ}. Immediately, by set diﬀerence
version, I(K ∪{ϕ}) = I((K ∪{ϕ})\{ϕ}). That is, I(K ∪{ϕ}) = I(K).
Lemma 4. Let I satisfy Free-formula Independence. The following items hold:
(i) If ¬ϕ is free for K then K ̸⊩ϕ. (ii) If ϕ is free for K then K ̸⊩¬ϕ.
Proof. (i) If ¬ϕ ̸∈K, apply Free-formula Independence, set union version, to
get I(K ∪{¬ϕ}) = I(K) and Lemma 1 to obtain K ̸⊩ϕ. If ¬ϕ ∈K, apply
the set diﬀerence version to get I(K) = I(K\{¬ϕ}) and Lemma 1 to obtain
K ̸⊩ϕ. Then, apply Lemma 3. (ii) Since ϕ is free for K, ¬¬ϕ is free for K. The
remaining proof is similar to (i).
3
Structural Properties Checking
Tarski [13] and Scott [12] worked out the idea of a logic as a closure operator,
i.e., an operator over sets of formulas which satisﬁes the so-called Extension,
Idempotence, and Isotony. Since we have formulated the induced consequence
as a relation ⊩, they can be expressed, assuming compactness, as follows.
Reﬂexivity K ⊩ϕ for any formula ϕ ∈K.
Monotony If K ⊩ϕ then K ∪{ψ} ⊩ϕ.
Cut If K ⊩ϕ and K ∪{ϕ} ⊩ψ then K ⊩ψ.
We now investigate the question of which properties of I guarantee ⊩to
satisfy Reﬂexivity, Cut, and Monotony.1 We will use the following postulates of
the literature [1,15].
Penalty: For ϕ ̸∈K, if ϕ is not free for K then I(K ∪{ϕ}) > I(K).
Monotony: If K ⊆K′ then I(K) ≤I(K′).
Rewriting: If ψ is a prenormal form2 of ϕ then I(K ∪{ϕ}) = I(K ∪{ψ}).
Proposition 1. Let I satisfy Penalty. Then, K ⊩ϕ for all consistent ϕ ∈K
(i.e., ⊩satisﬁes Reﬂexivity except for inconsistent formulas).
1 Using Monotony, it is clear that Cut implies the ﬁnite general form: If K ⊩ϕi for
i = 1, . . . , n and K ∪{ϕ1, . . . , ϕn} ⊩ψ then K ⊩ψ. Similarly, Monotony gives the
ﬁnite general form: If K ⊩ϕ then K ∪{ψ1, . . . , ψn} ⊩ϕ.
2 ψ is a prenormal form of ϕ if ψ results from ϕ by applying one or more of following
principles: commutativity, associativity and distribution for ∧and ∨, De Morgan
laws, double negation equivalence.

490
X. Liu et al.
Proof. Consider a consistent formula ϕ of K. Thus, ¬ϕ is not free for K due to
{ϕ} being a consistent subset of K such that supplementing it with ¬ϕ gives an
inconsistent set: {ϕ, ¬ϕ} |= ⊥. Assume ﬁrst ¬ϕ ̸∈K. By Penalty, I(K∪{¬ϕ}) >
I(K). Then, K ⊩ϕ ensues by Lemma 1. Assume ¬ϕ ∈K instead. Since ϕ is
in K and ϕ ̸|= ⊥, it happens that {ϕ} is a consistent subset of K\{¬ϕ} (in
view of the trivial fact ϕ ̸= ¬ϕ) such that supplementing it with ¬ϕ gives an
inconsistent set. That is, ¬ϕ is not free for K\{¬ϕ}. Also, ¬ϕ is not in K\{¬ϕ}.
Then, Penalty applies to give I((K\{¬ϕ}) ∪{¬ϕ}) > I(K\{¬ϕ}). By Lemma
1, K\{¬ϕ} ⊩ϕ. Applying Lemma 2, K ⊩ϕ.
Please observe that for an inconsistent ϕ in K, having K ⊩ϕ would amount
to I(K ∪{¬ϕ}) > I(K), that is, supplementing K with a tautology ¬ϕ would
increase the inconsistency degree: No inconsistency measure can be expected to
do that. In other words, Proposition 1 is the most that can be obtained towards
⊩satisfying Reﬂexivity. The following example is an illustration of this.
Example 1. Consider the inconsistency measure IMI satisfying Penalty, and an
inconsistent knowledge base K = {a, ¬a, a ∧¬a}. Let a be a consistent element
of K. It is easy to see that K ⊩a, since 2 = IMI(K) > IMI(K\{¬a}) = 1.
However, for the inconsistent element a ∧¬a of K, we have K ̸⊩a ∧¬a due to
IMI(K ∪{¬(a ∧¬a)}) = IMI(K) = 2.
We now turn to Monotony. To start with, Proposition 2 (proved by applying
Lemma 2 twice) states that Monotony succeeds when ψ = ¬ϕ. A more substan-
tial result is oﬀered by Proposition 3.
Proposition 2. If K ⊩ϕ then K ∪{¬ϕ} ⊩ϕ.
Proposition 3. Let I satisfy Free-formula Independence. Let ψ be free for K ∪
{¬ϕ}. If K ⊩ϕ then K ∪{ψ} ⊩ϕ.
Proof. (i) First, consider the case ¬ϕ ̸∈K. Assume that ψ is free for K ∪{¬ϕ}.
In view of Free-formula Independence, I(K ∪{¬ϕ} ∪{ψ}) = I(K ∪{¬ϕ}). Also
by Free-formula Independence, I(K) = I(K ∪{ψ}) because ψ being free for
K ∪{¬ϕ} entails ψ being free for K. However, Lemma 1 gives I(K ∪{¬ϕ}) >
I(K) due to K ⊩ϕ and ¬ϕ ̸∈K. Hence, I(K∪{ψ}∪{¬ϕ}) > I(K∪{ψ}). Should
ψ = ¬ϕ, a contradiction I(K ∪{ψ}) > I(K ∪{ψ}) would arise. Hence, ψ ̸= ¬ϕ.
This together with ¬ϕ ̸∈K give ¬ϕ ̸∈K ∪{ψ}. By Lemma 1, K ∪{ψ} ⊩ϕ.
(ii) Second, consider the case ¬ϕ ∈K. Then K = K ∪{¬ϕ}. Also, K ⊩ϕ
gives K\{¬ϕ} ⊩ϕ according to Lemma 2. Now, ¬ϕ ̸∈K\{¬ϕ}. Moreover, ψ
is free for (K\{¬ϕ}) ∪{¬ϕ} because ψ is free for K. (i) can be applied, giving
(K\{¬ϕ}) ∪{ψ} ⊩ϕ. For ψ ̸= ¬ϕ, Lemma 1 then gives I((K\{¬ϕ}) ∪{ψ} ∪
{¬ϕ}) > I((K\{¬ϕ}) ∪{ψ}). That is, I(K ∪{ψ} ∪{¬ϕ}) > I((K\{¬ϕ}) ∪
{ψ}). Therefore I(K ∪{ψ} ∪{¬ϕ}) > I((K ∪{ψ})\{¬ϕ}) due to ψ ̸= ¬ϕ. Per
Deﬁnition 2, K ∪{ψ} ⊩ϕ. The remaining case ψ = ¬ϕ is obvious since ¬ϕ ∈K.
Example 2. This example shows that Proposition 3 is the most that can be
obtained towards ⊩satisfying Monotony. Consider the drastic measure Id sat-
isfying Free-Formula Independence. Let K = {a, ¬a}, ϕ = a, and ψ = b ∧¬b.

Inducing Inference Relations from Inconsistency Measures
491
Then ψ is not free for K ∪{¬ϕ}. Besides, we have K ⊩ϕ. However, K ∪{ψ} ̸⊩ϕ
since Id(K ∪{ψ} ∪{¬ϕ}) = Id((K ∪{ψ})\{¬ϕ}) = 1.
Since Monotony seems to fail on a general count, we can look at a weaken-
ing known as Cautious Monotony [8]. To show under which conditions Cautious
Monotony and Cut are satisﬁed, we propose, from a technical point of view,
two postulates, Denial Elimination and Denial Preservation which are basi-
cally the converse of each other. Denial Preservation states that if a formula
increases the inconsistency of a knowledge base, then for its superset with the
same inconsistency, we may wish that the negation of the formula increases the
same inconsistency to the knowledge base and the superset.
Cautious Monotony. If K ⊩ϕ and K ⊩ψ then K ∪{ψ} ⊩ϕ.
Denial Elimination: Let K and ϕ be such that I(K ∪{ϕ}) > I(K). For all
K′ ⊇K, if I(K′ ∪{¬ϕ}) = I(K ∪{¬ϕ}) then I(K′) = I(K).
Denial Preservation: Let K and ϕ be such that I(K ∪{ϕ}) > I(K). For all
K′ ⊇K, if I(K′) = I(K) then I(K′ ∪{¬ϕ}) = I(K ∪{¬ϕ}).
Proposition 4. If I satisﬁes Denial Elimination, Monotony, and Rewriting,
then ⊩satisﬁes Cautious Monotony restricted to the case ¬ϕ ̸∈K and ¬ψ ̸∈K.
Proof. Let K ⊩ϕ and K ⊩ψ. Assume ¬ϕ ̸∈K. Firstly, consider ¬ϕ = ψ.
K ⊩ϕ gives I(K ∪{¬ϕ}) > I(K) by Lemma 1. That is, I(K ∪{ψ} ∪{¬ϕ}) >
I((K ∪{ψ})\{¬ϕ}). Hence, K ∪{ψ} ⊩ϕ by Deﬁnition 2. Secondly, consider
¬ϕ ̸= ψ. Then ¬ϕ /∈K ∪{ψ}. Assume further ¬ψ /∈K. Applying Lemma 1 to
K ⊩ψ yields I(K ∪{¬ψ}) > I(K). Denial Elimination can be applied so that
I(K ∪{¬ψ}) ̸= I(K) entails I(K ∪{¬ϕ} ∪{¬¬ψ}) ̸= I(K ∪{¬¬ψ}). Then
by Rewriting, we have I(K ∪{¬ϕ} ∪{ψ}) ̸= I(K ∪{ψ}). Due to Monotony,
I(K ∪{¬ϕ} ∪{ψ}) > I(K ∪{ψ}). Eventually, K ∪{ψ} ⊩ϕ by Lemma 1.
Proposition 5. If I satisﬁes Denial Preservation, Monotony, and Rewriting,
then ⊩satisﬁes Cut restricted to the case ¬ϕ ̸∈K and ¬ψ ̸∈K.
Proof. The proof is similar to the converse of the proof of Proposition 4.
4
KLM Properties Checking
Kraus, Lehman and Magidor [7] introduced a series of possible properties for
inference relations. In this section, we explore under which conditions the fol-
lowing KLM properties can be satisﬁed. More speciﬁcally, when an inconsistency
measures satisﬁes some postulate(s), we show that the induced inference relation
satisﬁes some KLM properties restricted to compulsory extra condition(s).
Right Weakening. If K ⊩ϕ and ϕ |= ψ then K ⊩ψ.
Left Logical Equivalence. If K ≡K′ and K ⊩ϕ, then K′ ⊩ϕ.

492
X. Liu et al.
AND. If K ⊩ϕ and K ⊩ψ then K ⊩ϕ ∧ψ.
Rational Monotony. If K ⊩ϕ and K ̸⊩¬ψ then K ∪{ψ} ⊩ϕ.
Contraposition. If ϕ ⊩ψ then ¬ψ ⊩¬ϕ.
OR. If K ∪{ϕ} ⊩ρ and K ∪{ψ} ⊩ρ then K ∪{ϕ ∨ψ} ⊩ρ.
Below are the rationality postulates [1,15] we will consider in this section.
Note that Antinomy Increase and Mutual Increase are new rationality postu-
lates. The former states that adding an inconsistent formula would increase the
inconsistency degree. The latter states that when adding a formula ϕ increases
the inconsistency amount of an inconsistent knowledge base K, we would expect
that adding K also increases the inconsistency amount of ϕ.
Dominance: If ϕ /∈K, ϕ ̸|= ⊥and ϕ |= ψ then I(K ∪{ϕ}) ≥I(K ∪{ψ}).
Disjunct Minimality: min(I(K ∪{ϕ}), I(K ∪{ψ})) ≤I(K ∪{ϕ ∨ψ}).
Disjunct Maximality: I(K ∪{ϕ ∨ψ}) ≤max(I(K ∪{ϕ}), I(K ∪{ψ})).
Antinomy Increase: Let |= ¬ϕ. If ϕ ̸∈K then I(K ∪{ϕ}) > I(K).
Mutual Increase: If K |= ⊥and I(K ∪{ϕ}) > I(K), then I(K ∪{ϕ}) > I({ϕ}).
Proposition 6. Let I satisfy Dominance. Then ⊩satisﬁes Right Weakening
restricted to the case ¬ψ ̸|= ⊥, ¬ψ /∈K and ¬ϕ /∈K.
Proof. Assume that K ⊩ϕ and ϕ |= ψ. Then ¬ψ |= ¬ϕ. By Dominance, we
have I(K ∪{¬ψ}) ≥I(K ∪{¬ϕ}). We also have I(K ∪{¬ϕ}) > I(K) by
Lemma 1. Immediately, I(K ∪{¬ψ}) > I(K). That is, K ⊩ψ by Lemma 1.
Example 3. The extra condition ¬ϕ ̸∈K of Proposition 6 is compulsory. Actu-
ally, consider Id satisfying Dominance. Let K = {a, ¬a}, ϕ = a and ψ = a∨b. It
follows that K ⊩ϕ and ϕ |= ψ. But K ̸⊩ψ since Id(K ∪{¬ψ}) = Id(K) = 1.
Due to failure of the general form for Right Weakening, a special case is of
interest: Is every tautology a consequence according to ⊩?
Proposition 7. If I satisﬁes Antinomy Increase, K ⊩ϕ for every tautology ϕ.
Proof. Take a tautology ϕ. Thus, ¬¬ϕ is also a tautology. Assume ¬ϕ /∈K. By
Antinomy Increase, I(K ∪{¬ϕ}) > I(K). Hence, K ⊩ϕ by Lemma 1. Assume
¬ϕ ∈K. Then, ¬ϕ /∈K\{¬ϕ}. Therefore, I((K\{¬ϕ}) ∪{¬ϕ}) > I(K\{¬ϕ})
by Antinomy Increase. That is I(K) > I(K\{¬ϕ}). So K ⊩ϕ by Lemma 1.
Using the previous proposition, we can get closer to Right Weakening.
Corollary 1. Let I satisfy Dominance and Antinomy Increase. Let ¬ψ /∈K
and ¬ϕ /∈K. If K ⊩ϕ and ϕ |= ψ then K ⊩ψ.
Proposition 8. Let K ̸|= ⊥. Then ⊩satisﬁes Left Logical Equivalence.
Proof. Apply Theorem 1 twice.
Example 4. The extra condition K ̸|= ⊥is necessary. Actually, consider IMI,
and two inconsistent knowledge bases K = {a, ¬a} and K′ = {b, ¬b}. It is
obvious that K ≡K′ and K ⊩a. However, K′ ̸⊩a as I(K′∪{¬a}) = I(K′) = 1.

Inducing Inference Relations from Inconsistency Measures
493
Proposition 9. Let I satisfy Disjunct Minimality, Rewriting and Monotony.
Then ⊩satisﬁes AND restricted to the case ¬ϕ ̸∈K and ¬ψ ̸∈K.
Proof. Assume that K ⊩ϕ and K ⊩ψ. Let ¬ϕ ̸∈K and ¬ψ ̸∈K. Then
I(K ∪{¬ϕ}) > I(K) and I(K ∪{¬ψ}) > I(K) by Lemma 1. Applying Disjunct
Minimality gives I(K ∪{¬ϕ ∨¬ψ}) ≥min(I(K ∪{¬ϕ}), I(K ∪{¬ψ})). Hence,
I(K∪{¬ϕ∨¬ψ}) > I(K). Applying Rewriting yields I(K∪{¬(ϕ∧ψ)}) > I(K).
Besides, applying Monotony gives I(K) ≥I(K\{¬(ϕ∧ψ)}). Therefore, it is the
case that I(K ∪{¬(ϕ ∧ψ)}) > I(K\{¬(ϕ ∧ψ)}). By Deﬁnition 2, K ⊩ϕ ∧ψ.
Example 5. Consider the drastic inconsistency measure Id satisfying Disjunct
Minimality, Rewriting and Monotony. Let K = {a, ¬a} and ϕ = ψ = a. Then
¬ψ and ¬ϕ are formulas occurring in K. Besides, K ⊩ϕ and K ⊩ψ. But we
have K ̸⊩ϕ ∧ψ due to Id(K ∪{¬(ϕ ∧ψ)}) = Id(K\{¬(ϕ ∧ψ)}) = 1.
The converse of the second item of Lemma 4 is untrue because I(K ∪{ψ}) =
I(K) is possible for ψ not free for K (see for instance the drastic inconsistency
measure). However, by Monotony, it is possible to use K ̸⊩¬ψ to infer I(K ∪
{ψ}) = I(K). That is the interest of the next lemma.
Lemma 5. Let I satisfy Monotony and Rewriting. Let K\{¬ϕ} ̸⊩¬ψ. If K ⊩ϕ
then K ∪{ψ} ⊩ϕ.
Proof. First, consider ¬ϕ ̸∈K. The assumption becomes K ̸⊩¬ψ. Since the
codomain of I is linearly ordered, Lemma 1 gives I(K ∪{¬¬ψ}) ≤I(K). Using
Rewriting, I(K ∪{ψ}) ≤I(K). Applying Monotony gives I(K ∪{ψ}) = I(K).
However, Lemma 1 gives I(K ∪{¬ϕ}) > I(K) due to K ⊩ϕ and ¬ϕ ̸∈K.
Therefore, I(K ∪{¬ϕ}) > I(K ∪{ψ}). By Monotony, I(K ∪{¬ϕ} ∪{ψ}) ≥
I(K ∪{¬ϕ}) hence I(K ∪{¬ϕ} ∪{ψ}) > I(K ∪{ψ}). Should ψ = ¬ϕ, a
contradiction I(K ∪{ψ}) > I(K ∪{ψ}) would arise. Hence, ψ ̸= ¬ϕ. This
together with ¬ϕ ̸∈K give ¬ϕ ̸∈K ∪{ψ}. By Lemma 1, K ∪{ψ} ⊩ϕ ensues.
Consider now ¬ϕ ∈K. Since K\{¬ϕ} ̸⊩¬ψ, in view of Lemma 1, we
have I((K\{¬ϕ}) ∪{¬¬ψ}) ≤I(K\{¬ϕ}). However, I(K\{¬ϕ}) < I(K) by
Lemma 1 due to K ⊩ϕ. Also, Monotony requires I(K) ≤I(K ∪{ψ}). Combin-
ing, I((K\{¬ϕ}) ∪{¬¬ψ}) < I(K ∪{ψ}) ensues. By Rewriting, I((K\{¬ϕ}) ∪
{ψ}) < I(K ∪{ψ}). Should ψ = ¬ϕ, a contradiction I(K ∪{ψ}) < I(K ∪{ψ})
would arise. Hence, ψ ̸= ¬ϕ. Thus, I((K ∪{ψ})\{¬ϕ}) < I(K ∪{ψ}). Applying
Lemma 1, this gives K ∪{ψ} ⊩ϕ.
Proposition 10. Let I satisfy Monotony and Rewriting. Then ⊩satisﬁes
Rational Monotony restricted to the case ¬ϕ ̸∈K.
Proof. Apply Lemma 5 for ¬ϕ ̸∈K in order to obtain that if K ⊩ϕ and K ̸⊩¬ψ
then K ∪{ψ} ⊩ϕ.
Example 6. The extra condition ¬ϕ /∈K in Proposition 10 is compulsory. Actu-
ally, consider Id which satisﬁes Monotony and Rewriting. Let K = {a, ¬a, ¬b},
ϕ = a and ψ = b. Trivially, ¬ϕ ∈K. Also, K ⊩ϕ since 1 = Id(K) >
Id(K\{¬ϕ}) = 0. On the other hand, K ̸⊩¬ψ since Id(K ∪{¬¬ψ}) = Id(K) =
1. However, K ∪{ψ} ̸⊩ϕ because Id(K ∪{ψ}) = Id((K ∪{ψ})\{¬ϕ}) = 1.

494
X. Liu et al.
Proposition 11. Let I satisfy Rewriting and Mutual Increase. Then ⊩satisﬁes
Contraposition restricted to the case ϕ |= ⊥, ¬ψ ̸= ϕ and ψ ̸= ¬ϕ.
Proof. Assume ϕ ⊩ψ. Then I({ϕ} ∪{¬ψ}) > I({ϕ}) by Lemma 1. Applying
Mutual Increase to {ϕ} and {¬ψ} yields I({ϕ} ∪{¬ψ}) > I({¬ψ}). In view of
Rewriting, I({¬¬ϕ} ∪{¬ψ}) > I({¬ψ}). Hence, ¬ψ ⊩¬ϕ by Lemma 1.
Example 7. The extra condition ¬ψ ̸= ϕ is compulsory. Actually, consider Id
satisfying Rewriting and Mutual Increase. Let ¬ψ = ϕ = ¬(a ∨¬a). Then
ψ = a ∨¬a. ϕ ⊩ψ since Id({ϕ} ∪{¬ψ}) = 1 > 0 = Id({ϕ})\{¬ψ}). But
¬ψ ̸⊩¬ϕ since Id({¬ψ} ∪{¬¬ϕ}) = Id({¬ψ}\{¬¬ϕ}) = 1.
A general result for OR is hard to obtain, since conditions satisfying OR
are very demanding. Assessing inconsistency of a disjunction is a delicate mat-
ter. There is a lack of measures for assessing inconsistency of a disjunction in
the existing literature except Disjunct Minimality and Disjunct Maximality [1].
However, even if an inconsistency measure satisﬁes these two postulates, it is
not clear whether OR is satisﬁed by the induced inference relation.
Assume that K ∪{ϕ} ⊩ρ and K ∪{ψ} ⊩ρ. Then by Deﬁnition 2, I(K ∪
{ϕ}∪{¬ρ}) > I((K ∪{ϕ})\{¬ρ}) and I(K ∪{ψ}∪{¬ρ}) > I((K ∪{ψ})\{¬ρ}).
But K ∪{ϕ ∨ψ} ⊩ρ is not necessarily entailed by these conditions. Let us
analyze this by considering two cases. Firstly, consider the case min(I(K ∪{ϕ}∪
{¬ρ}), I(K ∪{ψ}∪{¬ρ})) = I(K ∪{ϕ}∪{¬ρ}). Then I(K ∪{ϕ∨ψ}∪{¬ρ}) ≥
I(K ∪{ϕ} ∪{¬ρ}) by Disjunct Minimality. In such a case, we are not able to
compare I(K ∪{ϕ∨ψ}∪{ρ}) and I((K ∪{ϕ∨ψ})\{ρ}) since the relative order
between I((K ∪{ϕ})\{¬ρ}) and I((K ∪{ϕ})\{¬ρ}) is not clear. Both I((K ∪
{ϕ})\{¬ρ}) ≤I((K ∪{ϕ})\{¬ρ}) and I((K ∪{ϕ})\{¬ρ}) ≥I((K ∪{ϕ})\{¬ρ})
are possible. If I((K ∪{ϕ})\{¬ρ}) ≤I((K ∪{ϕ})\{¬ρ}), we have no idea which
one is smaller.
However, it is possible to compare the two items when I((K ∪{ϕ})\{¬ρ}) ≥
I((K ∪{ϕ})\{¬ρ}). Let us consider a simple case where ¬ρ ̸= ϕ, ¬ρ ̸= ψ and
¬ρ ̸= ϕ ∨ψ. Then I((K ∪{ϕ})\{¬ρ}) ≥I((K ∪{ϕ})\{¬ρ}) is equivalent to
I((K\{¬ρ}) ∪{ϕ}) ≥I((K\{¬ρ}) ∪{ψ}). Applying Disjunct Maximality gives
I((K\{¬ρ}) ∪{ϕ}) ≥I((K\{¬ρ}) ∪{ϕ ∨ψ}), namely, I((K ∪{ϕ})\{¬ρ}) ≥
I((K ∪{ϕ ∨ψ})\{¬ρ}). Thus, I(K ∪{ϕ ∨ψ}\{¬ρ}) > I((K ∪{ϕ ∨ψ})\{¬ρ}).
By Deﬁnition 2, K ∪{ϕ ∨ψ} ⊩ρ. The analysis for the case where min(I(K ∪
{ϕ}∪{¬ρ}), I(K ∪{ψ}∪{¬ρ})) = I(K ∪{ψ}∪{¬ρ}) is similar to the ﬁrst case.
5
Paraconsistency Checking
This section investigates which inconsistency measures induce a paraconsistent
inference relation. The following deﬁnition formally describes paraconsistency.
Deﬁnition 3 (Paraconsistency). An inference relation ⊩is paraconsistent
if for all inconsistent K ∈K, there exists a formula ϕ such that K ̸⊩ϕ.

Inducing Inference Relations from Inconsistency Measures
495
An inconsistency measure I is called relative [2] if it satisﬁes Normalization
(i.e., 0 ≤I(K) ≤1) and either Free-Formula Reduction or Relative Separability
(or both).
Free-Formula Reduction: For ϕ ̸∈K, if ϕ is free for K and I(K) ̸= 0 then
I(K ∪{ϕ}) < I(K).
Relative Separability: If I(K1) ⪅I(K2) and Atoms(K1) ∩Atoms(K2) = ∅then
I(K1) ⪅I(K1 ∪K2) ⪅I(K2) where either ⪅is < in every instance or ⪅is
= in every instance.
Proposition 12. If I is an inconsistency measure satisfying (i) Free-Formula
Reduction or (ii) Relative Separability, then ⊩is paraconsistent.
Proof. (i) Take an inconsistent K ∈K and an atom a /∈Atoms(K). Then ¬a is
free for K and ¬a /∈K. Besides, I(K) ̸= 0 by Consistency Null. So applying Free-
Formula Reduction to K gives I(K ∪{¬a}) < I(K). Thus, K ̸⊩ϕ by Lemma 1.
Hence, ⊩is paraconsistent by Deﬁnition 3. (ii) Take an inconsistent K ∈K
and an atom a /∈Atoms(K). Then I(K) > 0 and I({¬a}) = 0 by Consistency
Null. Hence, we have I({¬a}) < I(K). Besides, Atoms({¬a}) ∩Atoms(K) = ∅.
Applying Relative Separability gives I({¬a}) < I(K∪{¬a}) < I(K). Therefore,
K ̸⊩a by Lemma 1. Thus, ⊩is paraconsistent by Deﬁnition 3.
Theorem 2. For every relative inconsistency measure I, the inference relation
⊩induced from I is paraconsistent.
Next we show that the inference relation ⊩induced from I is paraconsis-
tent if I satisﬁes some rationality postulate(s). Note that we propose a postu-
late Tautology Non-Increase which states that any tautology does not increase
inconsistency degree.
Safe-Formula Independence: If ϕ is safe for K then I(K ∪{ϕ}) = I(K).
Tautology Independence: If ϕ is a tautology then I(K ∪{ϕ}) = I(K).
MI-separability: If MI(K1 ∪K2) = MI(K1) ∪MI(K2) and MI(K1) ∩MI(K2) = ∅
then I(K1 ∪K2) = I(K1) + I(K2).
Tautology Non-Increase: If ϕ is a tautology then I(K ∪{ϕ}) ≤I(K).
Proposition 13. If I is an inconsistency measure satisfying (i) Free-formula
Independence, (ii) Safe-Formula Independence, (iii) Tautology Independence or
(iv) MI-separability, then ⊩is paraconsistent.
Proof. (i) Take an inconsistent K ∈K and an atom a /∈Atoms(K). Then apply-
ing Free-Formula Independence to K gives I(K ∪{¬a}) = I(K), as ¬a is free for
K. Hence, K ̸⊩ϕ by Lemma 1. (ii) Obvious. (iii) Take an inconsistent K ∈K,
and an inconsistent formula ϕ such that no atom in ϕ occur in K. Then apply-
ing Tautology Independence gives I(K ∪{¬ϕ}) = I(K), as ¬ϕ is a tautology.
Trivially, ¬ϕ /∈K. Hence K ̸⊩ϕ by Lemma 1. (iv) Take an inconsistent K ∈K
and an atom a /∈Atoms(K). Then MI(K ∪{¬a}) = MI(K) since ¬a does not
participate in inconsistency. Besides, we have MI({¬a}) = ∅. Hence, we have

496
X. Liu et al.
MI(K ∪{¬a}) = MI(K) ∪MI({¬a}) and MI(K) ∩MI({¬a}) = ∅. Therefore,
I(K ∪{¬a}) = I(K) + I({¬a}) holds by applying MI-separability to K and
{¬a}. Thus, I(K ∪{¬a}) = I(K) due to I({¬a}) = 0. So K ̸⊩a by Lemma 1.
Lemma 6. If I satisﬁes Tautology Non-Increase, K ̸⊩ϕ for all inconsistent ϕ.
Proof. Let ϕ is be an inconsistent formula. Then ¬ϕ is a tautology. Assume ¬ϕ ̸∈
K. According to Tautology Non-Increase, I(K ∪{ϕ}) ≤I(K). Using Lemma 1,
K ̸⊩ϕ. Otherwise, assume ¬ϕ ∈K. Applying Tautology Non-Increase, we have
I((K\{¬ϕ}) ∪{¬ϕ}) ≤I(K\{¬ϕ}). By Lemma 1, K ̸⊩ϕ.
It is easy to see from Lemma 6 that if I satisﬁes Tautology Non-Increase
then ⊩is paraconsistent.
6
Dependencies and Incompatibilities
Some postulates imply some other postulates, whereas others are incompatible.
This section presents results in this sense.
Lemma 7. The following statements hold: (i) MI-separability entails Antinomy
Increase and Tautology Independence. (ii) Penalty entails Antinomy Increase.
(iii) Tautology Independence entails Tautology Non-Increase. (iv) Penalty and
Monotony entails Mutual Increase.
Proof. (i) We ﬁrst show that MI-separability entails Antinomy Increase. Let
ϕ ̸∈K such that ϕ
|=
⊥. Then, MI(K ∪{ϕ}) = MI(K) ∪MI({ϕ}) and
MI(K) ∩MI({ϕ}) = ∅. By MI-separability, I(K ∪{ϕ}) = I(K) + I({ϕ}). How-
ever, Consistency Null guarantees I({ϕ}) > 0. Then, I(K ∪{ϕ}) > I(K). We
turn to show that MI-separability entails Tautology Independence. Let ϕ be a
tautology. Then, MI(K ∪{ϕ}) = MI(K) ∪MI({ϕ}) and MI(K) ∩MI({ϕ}) = ∅.
MI-separability then gives I(K ∪{ϕ}) = I(K) + I({ϕ}). Consistency Null
ensures I({ϕ}) = 0. Thus, I(K ∪{ϕ}) = I(K). (ii) Consider ϕ ̸∈K such
that ϕ |= ⊥. Clearly, ϕ is not free for K because the empty set is a con-
sistent subset K but ∅∪{ϕ} |= ⊥. Applying Penalty, I(K ∪{ϕ}) > I(K).
(iii) Obvious. (iv) Assume K |= ⊥and I(K ∪{ϕ}) > I(K). Then the latter
assumption gives ϕ /∈K. Let us take a formula ψ that belongs to a mini-
mal inconsistent subset of K. Hence, ψ is not free for (K\{ψ}) ∪{ϕ}. Apply-
ing Penalty yields I(K ∪{ϕ}) > I((K\{ψ}) ∪{ϕ}). In view of Monotony,
I((K\{ψ}) ∪{ϕ}) ≥I({ϕ}). Immediately, I(K ∪{ϕ}) > I({ϕ}).
Accordingly, if I satisﬁes Penalty then K ⊩ϕ for all consistent ϕ ∈K (by
Proposition 1) and all tautological ϕ (by Proposition 7).
The results given in the previous sections do not generally add up in the sense
that combining rationality postulates for a given inconsistency measure may
make the induced inference inference fails to enjoy some property. There are two
main reasons. First, some possible properties of the induced inference relation are
exclusive of each other. Second, some rationality postulates are incompatible. For

Inducing Inference Relations from Inconsistency Measures
497
Table 1. Satisﬁable Properties of the Inference Relation ⊩, depending on Suﬃcient
Postulates of the Inconsistency Measure I and Compulsory Extra Conditions, with
Reference to the Result
Properties of ⊩
Suﬃcient postulates of
I
Extra conditions
Results
Reﬂexivity
Penalty
Consistent formulas
Proposition 1
Monotony
Free-formula
independence
ψ is free for K ∪{¬ϕ}
Proposition 3
Cut
Denial preservation,
monotony, rewriting
¬ϕ ̸∈K, ¬ψ ̸∈K
Proposition 5
Cautious monotony Denial elimination,
monotony, rewriting
¬ϕ ̸∈K, ¬ψ ̸∈K
Proposition 4
Right weakening
Dominance, antinomy
increase
¬ϕ /∈K, ¬ψ /∈K
Corrolary 1
AND
Disjunct minimality,
rewriting, monotony
¬ϕ ̸∈K, ¬ψ ̸∈K
Proposition 9
Rational monotony Monotony, rewriting
¬ϕ ̸∈K
Proposition 10
Contraposition
Rewriting, mutual
increase
ϕ |= ⊥, ¬ψ ̸= ϕ,
¬ψ ̸= ¬¬ϕ
Proposition 11
instance, a consequence of Lemma 6 is that Tautology Non-Increase makes AND
to fail whenever Reﬂexivity holds for all consistent formulas (let K ⊇{ϕ, ¬ϕ}
for some consistent non-tautological ϕ). Then we have the following proposition.
Proposition 14. If I satisﬁes Penalty and Tautology Non-Increase, then ⊩fails
to satisfy AND.
7
Conclusion and Future Work
In order to reason with inconsistent knowledge bases, we have proposed a new
family of inference relations induced from inconsistency measures. It is worth
mentioning that the new inference relations have the same consequences as clas-
sical logic when premises are consistent. We have shown that these inference
relations satisfy some important nonmonotonic reasoning properties (restricted
to some extra requirements) when the corresponding inconsistency measures sat-
isfy some rationality postulates (see Table 1). Furthermore, we have shown that
the new inference relations are paraconsistent when the corresponding inconsis-
tency measures satisfy some rationality postulates; importantly, every relative
inconsistency measures can induce a paraconsistent inference relation. Lastly, we
have presented some dependencies and incompatibilities among postulates.
This article opens up directions for future research. Firstly, we will continue
to investigate whether the remaining KLM properties (i.e. Equivalence, MPC,

498
X. Liu et al.
Transitivity, EHD, Loop) can be satisﬁed. We will also keep on investigating
dependencies and incompatibilities between postulates. Secondly, we will con-
sider an extension of the approach to inﬁnite knowledge bases. An idea in this
sense would be that the inference relation ⊩induced from an inconsistency mea-
sure I, considering an inﬁnite knowledge base X, would be such that: X ⊩ϕ iﬀ
I(Y ∪{¬ϕ}) > I(Y \{¬ϕ}) for some ﬁnite Y ⊆X.
References
1. Besnard, P.: Revisiting postulates for inconsistency measures. In: Ferm´e, E., Leite,
J. (eds.) JELIA 2014. LNCS (LNAI), vol. 8761, pp. 383–396. Springer, Cham
(2014). https://doi.org/10.1007/978-3-319-11558-0 27
2. Besnard, P., Grant, J.: Relative inconsistency measures. Artif. Intell. 280, 103231
(2020)
3. Brown, B.: Inconsistency measures and paraconsistent consequence. In: Grant, J.,
Martinez, M.V. (eds.) Measuring Inconsistency in Information, Studies in Logic,
vol. 73, pp. 219–233. College Publications (2018)
4. Grant, J., Hunter, A.: Measuring consistency gain and information loss in stepwise
inconsistency resolution. In: Liu, W. (ed.) ECSQARU 2011. LNCS (LNAI), vol.
6717, pp. 362–373. Springer, Heidelberg (2011). https://doi.org/10.1007/978-3-
642-22152-1 31
5. Hunter, A., Konieczny, S.: On the measure of conﬂicts: shapley inconsistency val-
ues. Artif. Intell. 174(14), 1007–1026 (2010)
6. Konieczny, S., Lang, J., Marquis, P.: Quantifying information and contradiction in
propositional logic through test actions. In: Gottlob, G., Walsh, T. (eds.) Proceed-
ings of the 18th International Joint Conference on Artiﬁcial Intelligence (IJCAI
2003), 9–15 August , 2003, Acapulco, Mexico, pp. 106–111. Morgan Kaufmann
(2003)
7. Kraus, S., Lehmann, D., Magidor, M.: Nonmonotonic reasoning, preferential mod-
els and cumulative logics. Artif. Intell. 44(1–2), 167–207 (1990)
8. Makinson, D.: General patterns in nonmonotonic reasoning. In: Gabbay, D.M.,
Hogger, C.J., Robinson, J.A. (eds.) Handbook of Logic in Artiﬁcial Intelligence
and Logic Programming, vol. 3, pp. 35–110. Clarendon Press, Oxford (1994)
9. Mu, K., Liu, W., Jin, Z.: A general framework for measuring inconsistency through
minimal inconsistent sets. J. Knowl. Inf. Syst. 27(1), 85–114 (2011)
10. Oller, C.: Measuring coherence using LP-models. J. Appl. Logic 2(4), 451–455
(2004)
11. Priest, G.: The logic of paradox. J. Philos. Logic 8(1), 219–241 (1979)
12. Scott, D.: Completeness and axiomatizability in many-valued logic. In: Henkin,
L.A. (ed.) Tarski Symposium, Proceedings of Symposia in Pure Mathematics,
Providence, RI, USA, vol. 25, pp. 411–436. American Mathematical Society (1974)
13. Tarski, A.: On some Fundamental Concepts of Metamathematics. In: Logic, Seman-
tics, Metamathematics, 2nd edn, pp. 30–37. Hackett, Indianapolis (1983). Trans-
lated by J. H. Woodger, edited by John Corcoran
14. Thimm, M.: Measuring inconsistency in probabilistic knowledge bases. In: Bilmes,
J.A., Ng, A.Y. (eds.) 25th Conference on Uncertainty in Artiﬁcial Intelligence (UAI
2009), 18–21 June 2009, Montreal, QC, Canada, pp. 530–537. AUAI Press (2009)
15. Thimm, M.: On the evaluation of inconsistency measures. In: Grant, J., Martinez,
M.V. (eds.) Measuring Inconsistency in Information, Studies in Logic, vol. 73, pp.
19–60. College Publications (2018)

The Degree of Conﬂict Between Formulas
in an Inconsistent Knowledge Base
Kedian Mu(B)
School of Mathematical Sciences, Peking University, Beijing, P.R. China
mukedian@math.pku.edu.cn
Abstract. Measuring inconsistency in a knowledge base has been
increasingly recognized as a good starting point to better understand
the inconsistency of that base. Most approaches to measuring inconsis-
tency for knowledge bases focus on either the degree of inconsistency of
a whole knowledge base or the responsibility of each formula of a knowl-
edge base for the inconsistency in that base or both. In this paper, we
propose an approach to measuring the degree of conﬂict between formu-
las, which allows us to have a more clear picture on the relation between
formulas involved in inconsistency. Then we show that this measure can
be explained well in the framework of Halpern-Pearl’s causal model.
Keywords: Inconsistency · Propositional knowledge bases · Degree of
conﬂict · Causality
1
Introduction
Inconsistency handling is one of the most important issues in knowledge-based
applications of artiﬁcial intelligence. It has been increasingly recognized that
measuring inconsistency provides a promising starting point to understand
inconsistency better and to facilitate the inconsistency handling for knowledge
bases [11].
A variety of inconsistency measures for a knowledge base (a ﬁnite set of logi-
cal formulas) have been proposed so far. These measures may be grouped into two
categories, i.e., base-level measures and formula-level measures [8]. Roughly speak-
ing, base-level measures for a knowledge base aim to characterize the inconsistency
from a perspective of the whole knowledge base. Such measures are often used to
describe the degree of inconsistency [8] or the signiﬁcance of inconsistency [4] of
the whole base. In contrast, formula-level measures aim to characterize the role
of an individual formula in causing the inconsistency in a knowledge base. Such
measures are often used to describe the contribution made by each formula to the
inconsistency [6] or the degree of responsibility of each formula for the inconsis-
tency [14] in a knowledge base. However, we also need to know whether there is
a conﬂictive relation between two formulas and to what extent they contradict
each other in some scenarios, especially when we choose formulas to be modiﬁed
in order to restore the consistency of a knowledge base. For example, consider a
c
⃝Springer Nature Switzerland AG 2021
J. Vejnarov´a and N. Wilson (Eds.): ECSQARU 2021, LNAI 12897, pp. 499–510, 2021.
https://doi.org/10.1007/978-3-030-86772-0_36

500
K. Mu
knowledge base {a, ¬a, ¬a ∨b, ¬b, c}. Obviously, a and ¬a contradict each other
directly, whilst neither ¬a ∨b nor ¬b contradicts a directly. But a is in conﬂict
with a combination of ¬a ∨b and ¬b. In contrast, c is not involved in any conﬂict
with a or combinations of a and some other formulas. Neither the formula-level
inconsistency measure nor the base-level inconsistency measure describes such a
conﬂictive relation between two formulas explicitly.
Minimal inconsistent subsets of a knowledge base provide a natural character-
ization of inconsistency arising in that base, especially for scenarios of syntax-
based inconsistency handling [1,3,7,9,10,12–15]. Here a minimal inconsistent
subset of a knowledge base refers to an inconsistent subset without an incon-
sistent proper subset. Moreover, there is a conﬂictive relation among formulas
involved in a minimal inconsistent subset of a knowledge base in intuition. Then
it is natural to characterize conﬂictive relations between formulas based on min-
imal inconsistent subsets.
In this paper, we focus on conﬂictive relations between formulas in a given
inconsistent knowledge base based on minimal inconsistent subsets of that base.
Moreover, we propose an approach to evaluating the degree of conﬂict between
any two formulas. Then we give an intuitive explanation of this evaluation in the
framework of Halpern-Pearl’s causal model [5] from a perspective of causality.
The rest of this paper is organized as follows. In Sect. 2, we give very brief
introductions to inconsistent knowledge bases and Halpern-Pearl’s causal model,
respectively. Then we propose an approach to measuring the degree of conﬂict
between formulas in an inconsistent knowledge base in Sect. 3. In Sect. 4, we pro-
vide an intuitive explanation of this measurement in the framework of Halpern-
Pearl’s causal model. Finally, we conclude this paper in Sect. 5.
2
Preliminaries
We use a ﬁnite propositional language in this paper. Let P be a ﬁnite set of
propositional symbols (atoms or variables) and L a propositional language built
from P under connectives {¬, ∧, ∨, →}. We use a, b, c, · · · to denote the propo-
sitional atoms, and α, β, γ, · · · to denote the distinct propositional formulas.
A knowledge base K is a ﬁnite set of propositional formulas. Just for simplic-
ity, we assume that each knowledge base is non-empty. For example, both {a}
and {a, ¬a ∨b, c} are knowledge bases.
K is inconsistent if there is a formula α such that K ⊢α and K ⊢¬α, where
⊢is the classical consequence relation. We abbreviate α ∧¬α as ⊥when there
is no confusion. Then we use K ⊢⊥(resp. K ̸⊢⊥) to denote that a knowledge
base K is inconsistent (resp. consistent). In particular, given a formula α, we
say that α is consistent if {α} is consistent. Just for simplicity of discussion,
we assume that each formula of a knowledge base is consistent. In addition, we
use K ∪{γ} to denote that we enlarge the knowledge base K by adding a new
formula γ ̸∈K to K.
An inconsistent subset K′ of K is called a minimal inconsistent subset (or
minimal unsatisﬁable subset) of K if no proper subset of K′ is inconsistent. We
use MI(K) to denote the set of all the minimal inconsistent subsets of K, i.e.,

The Degree of Conﬂict Between Formulas
501
MI(K) = {K′ ⊆K|K′ ⊢⊥and ∀K′′ ⊂K′, K′′ ̸⊢⊥}.
In syntax-based application domains, MI(K) could be considered as a char-
acterization of the inconsistency in K in the sense that one needs to remove
only one formula from each minimal inconsistent subset to resolve the inconsis-
tency [16].
Let Sub(K) be a set of some subsets of K, we abbreviate

K′∈Sub(K)
K′ as
 Sub(K). For example,  MI(K) is the abbreviation of

M∈MI(K)
M. It denotes
the set of formulas involved in minimal inconsistent subsets.
A formula in K is called a free formula if this formula does not belong to
any minimal inconsistent subset of K [6]. That is, free formulas have nothing to
do with any minimal inconsistent subset of K. We use FREE(K) to denote the
set of free formulas of K, i.e.,
FREE(K) = {α ∈K|∀M ∈MI(K), α ̸∈M}.
Evidently, K = ( MI(K)) ∪FREE(K).
Now we use the following example to illustrate these notions.
Example 1. Consider K1 = {a, ¬a, ¬a ∨b, ¬b, c, d, ¬d}. Then
MI(K1) = {M1, M2, M3},
FREE(K1) = {c},
where
M1 = {a, ¬a}, M2 = {a, ¬a ∨b, ¬b}, M3 = {d, ¬d}.
Next we give very brief introductions to Halpern and Pearl’s causal model [5]
and Chockler and Halpern’s notion of responsibility [2], respectively. For more
material, please see Sect. 2 and Sect. 3 in [2].
We start with the signature of a causal model. A signature is a tuple
S = ⟨U, V, R⟩, where U is a ﬁnite set of exogenous variables, whose values
are determined by factors outside a causal model, V is a ﬁnite set of endogenous
variables, whose values are ultimately determined by the exogenous variables,
and R associates with every variable Y ∈U ∪V a ﬁnite nonempty set R(Y ) of
possible values for Y [2,5].
A causal model over signature S is a tuple M = ⟨S, F⟩, where F asso-
ciates with every endogenous variable X ∈V a function FX such that FX :
((×U∈UR(U)) × (×Y ∈V\{X}R(Y ))) →R(X) [2,5].
We use ⃗X and ⃗x to denote a (possibly empty) vector of variables in V and
values for the variables in ⃗X, respectively. We use ⃗X ←⃗x to denote the case of
setting the values of the variables in ⃗X to ⃗x. We use ⃗u to denote a setting for
the variables in U. Here we call ⃗u a context [2,5].
Given ⃗X ←⃗x, a new causal model denoted M ⃗
X←⃗x over the signature
S ⃗
X = ⟨U, V −⃗X, R|V−⃗
X⟩, is deﬁned as M ⃗
X←⃗x = ⟨S ⃗
X, F ⃗
X←⃗x⟩, where F ⃗
X←⃗x
Y
is obtained from FY by setting the values of the variables in ⃗X to ⃗x [2,5].

502
K. Mu
Given a signature S = ⟨U, V, R⟩, a primitive event is a formula of the form
X = x, where X ∈V and x ∈R(X) [2,5]. In general, for ⃗X = (X1, X2, · · · , Xn)
and ⃗x = (x1, x2, · · · , xn), we abbreviate (X1 = x1)∧(X2 = x2)∧· · ·∧(Xn = xn)
as ⃗X = ⃗x.
A basic causal formula deﬁned in [2,5] is in the form of [⃗Y ←⃗y]ϕ, where ϕ
is a Boolean combination of primitive events. As explained in [2,5], [⃗Y ←⃗y]ϕ
means that ϕ holds in the counterfactual world that would arise if ⃗Y is set to ⃗y.
A causal formula is a Boolean combination of basic causal formulas [2,5]. We
use (M, ⃗u) |= ϕ to denote that a causal formula ϕ is true in causal model M
given a context ⃗u. Given a recursive model M, (M, ⃗u) |= [⃗Y ←⃗y](X = x) if the
value of X is x in the unique vector of values for the endogenous variables that
simultaneously satisﬁes all equations F
⃗Y ←⃗y
Z
, Z ∈V −Y under the setting ⃗u of
U [2,5]. Note that this deﬁnition can be extended to arbitrary causal formulas
in the usual way.
Deﬁnition 1 (Cause [5]). We say that ⃗X = ⃗x is a cause of ϕ in (M, ⃗u) if the
following three conditions hold:
AC1. (M, ⃗u) |= ( ⃗X = ⃗x) ∧ϕ.
AC2. There exists a partition (⃗Z, ⃗W) of V with ⃗X ⊆⃗Z and some setting (⃗x′, ⃗w′)
of the variables in ( ⃗X, ⃗W) such that if (M, ⃗u) |= Z = z∗for Z ∈⃗Z, then
(a) (M, ⃗u) |= [ ⃗X ←⃗x′, ⃗W ←⃗w′]¬ϕ. That is, changing ( ⃗X, ⃗W) from (⃗x, ⃗w)
to (⃗x′, ⃗w′) changes ϕ from true to false.
(b) (M, ⃗u) |= [ ⃗X ←⃗x, ⃗W ←⃗w′, ⃗Z′ ←⃗z∗]ϕ for all subsets ⃗Z′ of ⃗Z −⃗X. That
is, setting ⃗W to ⃗w′ should have no eﬀect on ϕ as long as ⃗X has the value
⃗x, even if all the variables in an arbitrary subset of ⃗Z are set to their
original values in the context ⃗u.
AC3. ( ⃗X = ⃗x) is minimal, that is, no subset of ⃗X satisﬁes AC2.
Based on Halpern-Pearl’s causal model above, the notion of responsibility
presented by Chockler and Halpern is given as follows:
Deﬁnition 2 (Degree of Responsibility [2]). The degree of responsibility of
X = x for ϕ in (M, ⃗u), denoted dr((M, ⃗u), (X = x), ϕ), is 0 if X = x is not a
cause of ϕ in (M, ⃗u); it is
1
k+1 if X = x is a cause of ϕ in (M, ⃗u) and there
exists a partition (⃗Z, ⃗W) and setting x′, ⃗w′ for which AC2 holds such that (a) k
variables in ⃗W have diﬀerent values in ⃗w′ than they do in the context ⃗u and (b)
there is no partition (⃗Z′, ⃗W ′) and setting x′′, ⃗w′′ satisfying AC2 such that only
k′ < k variables have diﬀerent values in ⃗w′′ than they do in the context ⃗u.
As explained in [2], the degree of responsibility of X = x for ϕ in (M, ⃗u)
captures the minimal number of changes that have to be made in ⃗u in order to
make ϕ counterfactually depend on X.

The Degree of Conﬂict Between Formulas
503
3
Degree of Conﬂict Between Formulas
In this section, we consider the degree of conﬂict between formulas in an incon-
sistent knowledge base. We start with a notion of α-minimal inconsistent subset.
Deﬁnition 3. Let K be an inconsistent knowledge base and α ∈K a formula of
K. A minimal inconsistent subset M of K is called an α-minimal inconsistent
subset of K if α ∈M.
We use MI(K|α) to denote the set of all α-minimal inconsistent subsets of
K. Evidently, MI(K|α) = ∅if and only if α ∈FREE(K).
Deﬁnition 4. Given a knowledge base K and α, β two distinct formulas of K,
we say that there exists a conﬂictive relation between α and β, denoted α ▷◁β,
if ∃M ∈MI(K) s.t. {α, β} ⊆M.
We can get the following observations on the conﬂictive relation. Firstly, note
that α ▷◁β if and only if there exists at least one minimal inconsistent subset
containing both α and β. Then the conﬂictive relation is symmetrical, that is,
β ▷◁α if and only if α ▷◁β. Secondly, there is no conﬂictive relation between
free formulas, moreover, there is no conﬂictive relation between a free formula
and any formula involved in minimal inconsistent subsets.
We use the following example to illustrate the notion of conﬂictive relation.
Example 2. Consider K1 again. Then
MI(K1|a) = {M1, M2},
MI(K1|¬a) = {M1},
MI(K1|¬a ∨b) = MI(K1|¬b) = {M2},
MI(K1|d) = MI(K1|¬d) = {M3},
MI(K1|c) = ∅.
So, we can ﬁnd the following conﬂictive relations between formulas:
a ▷◁¬a, a ▷◁¬a ∨b, a ▷◁¬b,
¬a ∨b ▷◁¬b,
d ▷◁¬d.
Given two formulas α, β ∈K, we abbreviate MI(K|α)∩MI(K|β) as MI(K|α∩
β). Evidently, α ▷◁β if and only if MI(K|α ∩β) ̸= ∅. Now we are ready to deﬁne
the degree of conﬂict between formulas.
Deﬁnition 5. Let K be a knowledge base and α, β two distinct formulas of
K. Then the degree of conﬂict between α and β in K, denoted C(α, β|K), is
deﬁned as
C(α, β|K) =

max
M∈MI(K|α∩β)
1
|M|−1, if α ▷◁β,
0,
otherwise.
Evidently, we can get the following observations on the degree of conﬂict
between formulas:

504
K. Mu
(1) 0 ≤C(α, β|K) ≤1.
(2) C(α, β|K) = C(β, α|K).
(3) C(α, β|K) = 0 for all β ∈K\{α} if α ∈FREE(K).
The ﬁrst two observations together state that C is a bounded symmetric
function. The third states that any free formula is not in conﬂict with other
formulas.
The following proposition provides characterizations of the upper and the
lower bounds of the degree of conﬂict between formulas.
Proposition 1. Given a knowledge base K and α, β two distinct formulas of
K, then
1. C(α, β|K) = 1 if and only if {α, β} ∈MI(K).
2. C(α, β|K) = 0 if and only if MI(K|α) ∩MI(K|β) = ∅.
Proof. This is a direct consequence of the deﬁnition of C(α, β|K).
□
Now we use the following example to illustrate the degree of conﬂict between
formulas.
Example 3. Consider K1 = {a, ¬a, ¬a ∨b, ¬b, c, d, ¬d} again. Then
C(a, ¬a|K1) = 1,
C(d, ¬d|K1) = 1.
C(a, ¬a ∨b|K1) = C(a, ¬b|K1) = C(¬a ∨b, ¬b|K1) = 1
2,
C(a, d|K1) = C(a, ¬d|K1) = C(a, c|K1) = 0.
This coincides with the intuition that a and ¬a contradict each other directly,
while a and ¬b (resp. ¬a ∨b) are involved in the inconsistency given ¬a ∨b
(resp. ¬b).
Note that given a minimal inconsistent subset M ∈MI(K), C(α, β|K) > 0
for all α ∈M and β ∈M\{α}. But it does not always hold that C(α, β|K) =
C(α, γ|K) for β, γ ∈M\{α} given α ∈M. To illustrate this, consider
K2 = {a, a →b ∧c, ¬b, c →d, ¬d}.
Then
MI(K2) = {M4 = {a, a →b ∧c, ¬b}, M5 = {a, a →b ∧c, c →d, ¬d}} .
Consider M5, then we ﬁnd that
C(a, a →b ∧c|K2) = 1
2 > 1
3 = C(a, c →d|K2) = C(a, ¬d|K2).
However, the following proposition shows that all the other formulas in the
smallest α-minimal inconsistent subset have the same degree of conﬂict with α.
Proposition 2.
Let K be an inconsistent knowledge base and α ∈ MI(K) a
formula of K. Then
C(α, β|K) = C(α, γ|K)
for all β, γ ∈M ∗\{α}, where M ∗∈MI(K|α) s.t. |M ∗| ≤|M| for all M ∈
MI(K|α).

The Degree of Conﬂict Between Formulas
505
Proof. Let K be an inconsistent knowledge base and α ∈ MI(K), then {α} ⊆
M ∗∈MI(K|α). Moreover, for all β ∈M ∗\{α},
C(α, β|K) =
1
|M ∗| −1.
So,
C(α, β|K) = C(α, γ|K)
for all β, γ ∈M ∗\{α}.
□
The following proposition shows that the measure satisﬁes some intuitive
properties adapted from the most used properties presented by Hunter and
Konieczny for characterizing inconsistency measures [6–8].
Proposition 3. Let K be a knowledge base. Then
– Consistency: C(α, β|K) = 0 for all α, β ∈K if and only if K is consistent.
– Free Formula Independence: C(α, β|K) = C(α, β|K ∪{γ}) for all α, β ∈K
if γ ∈FREE(K ∪{γ}).
– Monotonicity: C(α, β|K) ≤C(α, β|K ∪{γ}).
– Dominance-1: For all three distinct formulas α, β, γ ∈K, C(α, β|K) ≤
C(α, γ|K) if γ ⊢β.
– Dominance-2: For any two formulas α, β ∈K, C(α, β|K∪{γ2}) ≤C(α, β|K∪
{γ1}) if γ1 ⊢γ2.
Proof. Let K be a knowledge base.
– Consistency. ∀α, β ∈K, C(α, β|K) = 0. ⇐⇒MI(K) = ∅.⇐⇒K is consistent.
– Free Formula Independence. If γ is a free formula of K ∪{γ}, then MI(K ∪
{γ}) = MI(K). So, ∀α ∈K, MI(K|α) = MI(K ∪{γ}|α). Therefore,
C(α, β|K) = C(α, β|K ∪{γ}).
– Monotonicity. Note that ∀γ ̸∈K, MI(K) ⊆MI(K ∪{γ}). So, ∀α ∈K,
MI(K|α) ⊆MI(K ∪{γ}|α). Therefore,
C(α, β|K) ≤C(α, β|K ∪{γ}).
– Dominance-1. Let α, β, and γ be three distinct formulas of K. If γ ⊢β, then
• if MI(K|α ∩β) = ∅, then C(α, β|K) = 0 ≤C(α, γ|K).
• if MI(K|α ∩β) ̸= ∅, then ∀M ∈MI(K|α ∩β), ∃M ′ ∈MI(K|α ∩γ) such
that
M ′ ⊆(M ∪{γ}) \{β}.
Therefore,
C(α, β|K) ≤C(α, γ|K).
– Dominance-2: Consider two formulas γ1, γ2 ̸∈K such that γ1 ⊢γ2.
• If MI(K ∪{γ2}) = ∅, then for any two distinct formulas α, β ∈K,
C(α, β|K ∪{γ2}) = 0 ≤C(α, β|K ∪{γ1}).

506
K. Mu
• If MI(K∪{γ2}) ̸= ∅but MI(K∪{γ2}) = MI(K), then γ2 is a free formula
of K ∪{γ2}, then
C(α, β|K ∪{γ2}) = C(α, β|K).
Furthermore, according to the property of Monotonicity, we have
C(α, β|K) ≤C(α, β|K ∪{γ1}).
So,
C(α, β|K ∪{γ2}) ≤C(α, β|K ∪{γ1}).
• If MI(K∪{γ2}) ̸= ∅but MI(K∪{γ2}) ̸= MI(K), then ∃M ∈MI(K∪{γ2})
such that γ2 ∈M. So, ∃M ′ ∈MI(K ∪{γ1}) such that
M ′ ⊆(M ∪{γ1}) \{γ2}.
So,
min
M∈MI(K∪{γ1}|α∩β) |M| ≤
min
M ′∈MI(K∪{γ2}|α∩β) |M ′|.
Therefore,
C(α, β|K ∪{γ2}) ≤C(α, β|K ∪{γ1}).
□
Roughly speaking, the property of Consistency says that only within a con-
sistent knowledge base, any two formulas are not in conﬂict with each other.
The property of Free Formula Independence says that adding a free formula to
a knowledge base cannot aﬀect the degree of conﬂict between any two formulas
in that base. The property of Monotonicity states that the degree of conﬂict
between any two formulas of a knowledge base cannot decrease when we enlarge
that base. The property of Dominance-1 states that if a formula γ is logically
stronger than another formula β, then the degree of conﬂict between β with
any other formula cannot exceed that between γ with that formula. In contrast,
the property of Dominance-2 states that adding logically stronger formulas to a
knowledge base may lift the degree of conﬂict between formulas in that base. In
summary, the two properties of dominance imply that logically stronger formulas
may bring more conﬂicts.
The following proposition describes the relation between the smallest size of
minimal inconsistent subsets of an inconsistent knowledge base and the degree
of conﬂict between formulas of that base.
Proposition 4. Let K be an inconsistent knowledge base. Then
min
M∈MI(K) |M| =
1
max
α,β∈K C(α, β|K) + 1.
Proof. This is a direct consequence of the deﬁnition of C(α, β|K).
□.

The Degree of Conﬂict Between Formulas
507
4
The Causality-Based Explanation
In this section, we provide a causality-based explanation for the degree of conﬂict
between formulas in the framework of Halpern-Pearl’s model.
Given an inconsistent knowledge base K, we construct the following causal
model for the conﬂictive relations between formulas in K:
– we associate every formula α ∈K with a binary variable Xα, whose value is
1 if α is considered and 0 otherwise. We use ⃗X to denote the vector of all the
variables corresponding to formulas.
– we associate every formula variable Xα a binary exogenous Uα. Moreover, we
assume that the value of Xα depends on only the value of Uα. We use UK
and ⃗U to denote the set and the vector of all the exogenous variables.
– we associate every pair (α, β) of formulas of K with a binary variable Y(α,β),
whose value is 1 if α conﬂicts with β and 0 otherwise. We use ⃗Y to denote
the vector of all the variables of this type corresponding to formulas.
– we associate with every α-minimal inconsistent subset M ∈MI(K|α) a binary
variable SM|α, whose value is 1 if all the formulas of M\{α} are considered
and 0 otherwise. We use ⃗S to denote the vector of all the variables of this
type.
Let VK = {Xα|α ∈K} ∪{Y(α,β)|α, β ∈K} ∪{SM|α|M ∈MI(K|α), α ∈K}.
Then RK(V ) = {0, 1} for each V ∈VK.
Futhermore, we deﬁne the following functions to characterize the dependence
relation between these variables:
– FXα(⃗U, ⃗X −Xα, ⃗Y , ⃗S) = Uα (Xα = Uα for short) for every formula α ∈K.
– FSM|α(⃗U, ⃗X, ⃗Y , ⃗S −SM|α) =

β∈M\{α}
Xβ (SM|α =

β∈M\{α}
Xβ for short) for
every M ∈MI(K|α).
– FY(α,β)(⃗U, ⃗X, ⃗Y −Y(α,β), ⃗S) =

M∈MI(K|α∩β)
SM|α (Y(α,β) =

M∈MI(K|α∩β)
SM|α
for short) for every variable Y(α,β), where  is the Boolean addition.
Roughly speaking, the function FXα represents the assumption that the value
of variable Xα is determined by only the value of the corresponding exogenous
variable Uα. The function FSM|α describes the fact that all the other formu-
las in the α-minimal inconsistent subset M together involve α in the minimal
inconsistent subset M. That is, removing any other formula from the α-minimal
inconsistent subset M can break M. The function FY(α,β) describes the fact that
α will not be free from conﬂict with β if at least one α ∩β-minimal inconsistent
subset keeps unchanged. We use FK to denote the set of all the functions above
for K, i.e.,
FK = {FXα|α ∈K} ∪{FY(α,β)|α, β ∈K} ∪{FSM|α|M ∈MI(K|α), α ∈K}.
Then the causal model for the conﬂict relation between formulas of K,
denoted MK, is deﬁned as MK = ⟨SK, FK⟩, where SK = ⟨UK, VK, RK⟩. We use

508
K. Mu
the context ⃗u(α,β) = (0, 0, . . . , 0, 1, 0) (⃗u(α,β) = (⃗0, 1, 0) for short) to represent
the case where Uα = 1, Uβ = 0 and Uγ = 0 for all γ ∈K\{α, β}, i.e., only α is
considered.
We use the following example to illustrate the causal model for conﬂictive
relations between formulas.
Example 4. Consider K1 = {a, ¬a, ¬a ∨b, ¬b, c, d, ¬d} again. Here we only
consider the conﬂictive relations between a and other formulas. Note that
MI(K1|a) = {M1, M2}, where M1 = {a, ¬a} and M2 = {a, ¬a ∨b, ¬b}. Then
– SM1|a = X¬a and SM2|a = X¬a∨b × X¬b.
– Y(a,¬a) = SM1|a and Y(a,¬a∨b) = Y(a,¬b) = SM2|a.
Given a context ⃗u(a,¬b) = (⃗0, 1, 0) , then (MK1, ⃗u(a,¬b)) |= (Y(a,¬b) = 0).
Furthermore, consider counterfactual worlds arising from X¬a∨b ←1 and
X¬b ←1then
(MK1, ⃗u(a,¬b)) |= [X¬a∨b ←1, X¬b ←1](Y(a,¬b) = 1).
This implies that a conﬂicts with ¬b given ¬a ∨b.
Now we are ready to characterize the degree of conﬂict between formulas in
the causal model.
Proposition 5. Given an inconsistent knowledge base K and two distinct for-
mulas α, β ∈K such that α ▷◁β. Then
C(α, β|K) = dr((MK, ⃗u(α,β)), Xβ = 0, Y(α,β) = 0).
Proof. Let α, β ∈K such that α ▷◁β, then ∃M ∈MI(K|α ∩β). Let MW =
M\{α, β} and ⃗W be the vector of variables corresponding to formulas of MW .
Further, let ⃗Z = V −⃗W, then Xα, Xβ ∈⃗Z. Note that
(MK, ⃗u(α,β)) |= Xβ = 0 ∧Y(α,β) = 0.
Moreover,
(MK, ⃗u(α,β)) |= [Xβ ←1, ⃗W ←⃗1]Y(α,β) = 1.
and
(MK, ⃗u(α,β)) |= [Xβ ←0, ⃗W ←⃗1, ⃗Z′ ←⃗0]Y(α,β) = 0
for all subset ⃗Z′ of ⃗Z −(Xα, Xβ).
So, Xβ = 0 is a cause of Y(α,β) = 0 in the causal model (MK, ⃗u(α,β)).
Further, consider M ∗∈MI(K|α ∩β). Let ⃗W ∗be the vector of variables
corresponding to formulas of M ∗
W , then
dr((MK, ⃗u(α,β)), Xβ = 0, Y(α,β) = 0) =
1
1 + |⃗Z∗|
=
1
|M ∗| −1.

The Degree of Conﬂict Between Formulas
509
By Proposition 2, C(α, β|K) =
1
|M ∗|−1. So,
C(α, β|K) = dr((MK, ⃗u(α,β)), Xβ = 0, Y(α,β) = 0).
□
This proposition shows that the degree of conﬂict between two formulas is
exactly a special kind of the Chockler and Halpern’s degree of responsibility [2]
in Halpern and Pearl’s causal model. Such a linkage provides an intuitive expla-
nation for the degree of conﬂict C(α, β|K) between two formulas α and β from
the perspective of causality.
On the other hand, according to Halpern-Pearl’s causal, taking into account
the other formulas in a α ∩β-minimal inconsistent subset exactly obtains a
contingency where α conﬂicts with β. Moreover, the more the formulas involved
in such a contingency, the smaller the degree of conﬂict between α and β is.
5
Conclusion
Measuring inconsistency for knowledge bases has been paid much attention
recently. A growing number of techniques for measuring inconsistency have been
proposed in a variety of applications so far. Most approaches to measuring incon-
sistency for a knowledge base focus on either the degree of inconsistency of the
whole knowledge base or the degree of responsibility (or contribution) of an
individual formula for the inconsistency of the knowledge base or both.
In this paper, we provide a new perspective to look inside the inconsistency in
a knowledge base by characterizing the problem of conﬂictive relations between
formulas within that knowledge base. We have proposed an approach to mea-
suring the degree of conﬂict between any two distinct formulas of a knowledge
base based on minimal inconsistent subsets of that base. Then we provided an
intuitive explanation for such an approach from the perspective of causality. We
have shown that the degree of conﬂict presented in this paper is exactly a spe-
cial kind of Chockler and Halpern’s responsibility introduced in the context of
Halpern-Pearl’s causal model.
It has been argued that taking constraints on inconsistency handling into
account can make inconsistency measuring techniques more applicable and useful
in practical applications [15]. Then it is advisable to incorporate constraints in
measuring the degree of conﬂict between formulas in future.
Acknowledgements. This work was partly supported by the National Natural Sci-
ence Foundation of China under Grant No. 61572002, No. 61690201, and No. 61732001.

510
K. Mu
References
1. Bona, G.D., Grant, J., Hunter, A., Konieczny, S.: Towards a uniﬁed framework
for syntactic inconsistency measures. In: Proceedings of the Thirty-Second AAAI
Conference on Artiﬁcial Intelligence, (AAAI-18), the 30th innovative Applications
of Artiﬁcial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational
Advances in Artiﬁcial Intelligence (EAAI-18), New Orleans, Louisiana, USA, 2–7
February 2018, pp. 1803–1810 (2018)
2. Chockler, H., Halpern, J.Y.: Responsibility and blame: a structural-model app-
roach. J. Artif. Intell. Res. 22, 93–115 (2004)
3. Grant, J., Hunter, A.: Measuring consistency gain and information loss in stepwise
inconsistency resolution. In: Liu, W. (ed.) ECSQARU 2011. LNCS (LNAI), vol.
6717, pp. 362–373. Springer, Heidelberg (2011). https://doi.org/10.1007/978-3-
642-22152-1 31
4. Grant, J., Hunter, A.: Analysing inconsistent information using distance-based
measures. Int. J. Approx. Reasoning 89, 3–26 (2017)
5. Halpern, J.Y., Pearl, J.: Causes and explanations: a structural-model approach.
part i: causes. Br. J. Philos. Sci. 56(4), 843–887 (2005)
6. Hunter, A., Konieczny, S.: Shapley inconsistency values. In: Doherty, P., Mylopou-
los, J., Welty, C. (eds.) Principles of Knowledge Representation and Reasoning:
Proceedings of the 10th International Conference (KR06), pp. 249–259. AAAI Press
(2006)
7. Hunter, A., Konieczny, S.: Measuring inconsistency through minimal inconsistent
sets. In: Brewka, G., Lang, J. (eds.) Principles of Knowledge Representation and
Reasoning: Proceedings of the Eleventh International Conference (KR08), pp. 358–
366. AAAI Press (2008)
8. Hunter, A., Konieczny, S.: On the measure of conﬂicts: shapley inconsistency val-
ues. Artif. Intell. 174(14), 1007–1026 (2010)
9. Jabbour, S., Ma, Y., Raddaoui, B.: Inconsistency measurement thanks to MUS
decomposition. In: Bazzan, A.L.C., Huhns, M.N., Lomuscio, A., Scerri, P. (eds.)
International Conference on Autonomous Agents and Multi-Agent Systems,
AAMAS 2014, Paris, France, 5–9 May 2014, pp. 877–884. IFAAMAS/ACM (2014)
10. Jabbour, S., Ma, Y., Raddaoui, B., Sais, L., Salhi, Y.: A MIS partition based
framework for measuring inconsistency. In: Baral, C., Delgrande, J.P., Wolter, F.
(eds.) Principles of Knowledge Representation and Reasoning: Proceedings of the
Fifteenth International Conference, KR 2016, Cape Town, South Africa, 25–29
April 2016, pp. 84–93. AAAI Press (2016)
11. Liu, W., Mu, K.: Introduction to the special issue on theories of inconsistency
measures and their applications. Int. J. Approx. Reasoning 89, 1–2 (2017)
12. Mu, K., Jin, Z., Lu, R., Liu, W.: Measuring inconsistency in requirements speciﬁ-
cations. In: Godo, L. (ed.) ECSQARU 2005. LNCS (LNAI), vol. 3571, pp. 440–451.
Springer, Heidelberg (2005). https://doi.org/10.1007/11518655 38
13. Mu, K., Liu, W., Jin, Z., Bell, D.: A syntax-based approach to measuring the
degree of inconsistency for belief bases. Int. J. Approx. Reasoning 52(7), 978–999
(2011)
14. Mu, K.: Responsibility for inconsistency. Int. J. Approx. Reasoning 61, 43–60
(2015)
15. Mu, K.: Measuring inconsistency with constraints for propositional knowledge
bases. Artif. Intell. 259, 52–90 (2018)
16. Reiter, R.: A theory of diagnosis from ﬁrst principles. Artif. Intell. 32(1), 57–95
(1987)

Possibility Theory and Fuzzy
Approaches

Representation of Explanations
of Possibilistic Inference Decisions
Isma¨ıl Baaj1,2(B), Jean-Philippe Poli1, Wassila Ouerdane3,
and Nicolas Maudet2
1 Universit´e Paris-Saclay, CEA, List, 91120 Palaiseau, France
{ismail.baaj,jean-philippe.poli}@cea.fr
2 LIP6, Sorbonne Universit´e, Paris, France
{ismail.baaj,nicolas.maudet}@lip6.fr
3 MICS, CentraleSup´elec, Universit´e Paris-Saclay, Gif sur Yvette, France
wassila.ouerdane@centralesupelec.fr
Abstract. In this paper, we study how to explain to end-users the infer-
ence results of possibilistic rule-based systems. We formulate a necessary
and suﬃcient condition for justifying by a relevant subset of rule premises
the possibility degree of each output attribute value. We apply functions
to reduce the selected premises, in order to form two kinds of explana-
tions: the justiﬁcation and the unexpectedness of the possibility degree
of an output attribute value. The justiﬁcation is composed of possibilis-
tic expressions that are suﬃcient to justify the possibility degree of the
output attribute value. The unexpectedness is a set of possible or certain
possibilistic expressions, which are not involved in the determination of
the considered inference result although there may appear to be a poten-
tial incompatibility between them and the considered inference result.
We then deﬁne a representation of explanations of possibilistic infer-
ence decisions that relies on conceptual graphs and may be the input
of natural language generation systems. Our extracted justiﬁcation
and unexpectedness are represented by nested conceptual graphs. All our
constructions are illustrated with an example of a possibilistic rule-based
system that controls the blood sugar level of a patient with type 1 dia-
betes.
Keywords: Explainable artiﬁcial intelligence · Possibility theory ·
Rule-based system · Conceptual graphs
1
Introduction
Possibility Theory is a well-known framework for the handling of incomplete or
imprecise information [7,8] that models the uncertainty by two dual measures
called possibility and necessity. These measures allow to distinguish between
what is possible without being certain at all and what is certain to some extent.
The possibilistic handling of rule-based systems [10,12] has led to the emergence
c
⃝Springer Nature Switzerland AG 2021
J. Vejnarov´a and N. Wilson (Eds.): ECSQARU 2021, LNAI 12897, pp. 513–527, 2021.
https://doi.org/10.1007/978-3-030-86772-0_37

514
I. Baaj et al.
of possibilistic rule-based systems used for medical diagnostics [4] e.g., DIA-
BETO [16]. For safety-critical applications such as medicine, the generation of
explanations of the decisions made by AI systems is now a legitimate demand, in
view of the recent adoption of laws that reinforce users’ rights e.g., GDPR [13].
Recently, an emphasis was put on possibilistic rule-based systems [9], where the
authors highlighted the approach of [11] to develop the explanatory capabili-
ties of these systems. In [11], Farreny and Prade propose to perform a sensitiv-
ity analysis by using a min-max equations system. By an example, the authors
suggest that it is possible to justify an inference result by some rule premises.
They also give a natural language explanation of an inference result of their
example. In fact, their approach aims at generating explanations of possibilistic
inference decisions, which have to be expressed in natural language for end-users.
To generate them with Natural Language Generation techniques [14], authors of
[2] propose to deﬁne a representation of the explanations of inference decisions.
In this paper, we elaborate the explanatory capabilities of possibilistic rule-based
systems. Our purpose is twofold. First, we study how to select rule premises
justifying their inference results. Then, we deﬁne a graphical representation of
explanations constructed from the selected premises. We ﬁrst remind the infer-
ence mechanism of a possibilistic rule-based system, introduce useful notations
and give an example of such a system, which will be used to illustrate all the
constructions of the paper (Sect. 2). The inference result of a possibilistic rule-
based system is an output possibility distribution, which assigns to each output
attribute value a possibility degree. In Sect. 3, we give a necessary and suﬃcient
condition to justify by rule premises the possibility degree of any output attribute
value. Under this condition, we extract a corresponding subset of premises.
In Sect. 4, we deﬁne four premise reduction functions and apply them to the sub-
set of premises of Sect. 3. This leads us to form two kinds of explanations: the
justiﬁcation and the unexpectedness of the considered output attribute value.
The justiﬁcation is formed by reducing the selected premises to the structure
responsible for their possibility or necessity degree. It uses two premise reduction
functions. The unexpectedness is a set of possible or certain possibilistic expres-
sions related to the considered inference result in the following sense: although
there may appear to be a potential incompatibility between each of the possi-
bilistic expressions and the considered inference result, they are not involved in
the determination of the inference result. They are extracted by applying the
two other premise reduction functions.
We then propose to represent explanations by conceptual graphs, which provide
a natural way to represent knowledge by concepts and n-ary relations (Sect. 5).
Using our justiﬁcation and unexpectedness, we represent explanations by nested
conceptual graphs. Finally, in Sect. 6, we conclude with some perspectives.
2
Background
In this section, following [9], we remind the inference mechanism of possibilistic
rule-based systems. Some notations which will be useful in the rest of the paper
are introduced. We also give an example of a possibilistic rule-based system.

Representation of Explanations of Possibilistic Inference Decisions
515
We consider a set of n parallel if-then possibilistic rules R1, R2, · · · , Rn, where
each Ri is of the form: “if pi then qi” and has its uncertainty propagation matrix

π(qi|pi)
π(qi|¬pi)
π(¬qi|pi) π(¬qi|¬pi)

=

1 si
ri 1

. The premise pi of the rule Ri is of the form
pi = pi
1∧pi
2∧· · ·∧pi
k, where each pi
j is a proposition: “ai
j(x) ∈P i
j”. The attribute
ai
j is applied to an item x, where its information is represented by a possibility
distribution πai
j(x) : Dai
j →[0, 1] deﬁned on its domain Dai
j, which is supposed
to be normalized i.e., ∃u ∈Dai
j such that πai
j(x)(u) = 1. The possibility degree
of pi
j and that of its negation are computed using the possibility measure Π by
π(pi
j) = Π(P i
j) = supu∈P i
j πai
j(x)(u) and π(¬pi
j) = Π(P i
j) = supu∈P i
j πai
j(x)(u)
respectively, where P i
j ⊆Dai
j and P i
j is its complement. As πai
j(x) is normalized,
we have max(π(pi
j), π(¬pi
j)) = 1. The necessity degree of pi
j is deﬁned with the
necessity measure N by n(pi
j) = N(P i
j) = 1 −π(¬pi
j) = infu∈P i
j (1 −πai
j(x)(u)).
The possibility degree of pi is π(pi) = mink
j=1 π(pi
j) and that of its negation is
π(¬pi) = maxk
j=1 π(¬pi
j). These formulas π(pi) and π(¬pi) preserve the normal-
ization i.e., max(π(pi), π(¬pi)) = 1 and are respectively noted λi and ρi. The
necessity degree of pi is n(pi) = 1−π(¬pi) = mink
j=1(1−π(¬pi
j)) = mink
j=1 n(pi
j).
The degrees λi and ρi allow to have the following interpretations of pi:
• π(pi) = λi estimates to what extent pi is possible,
• n(pi) = 1 −ρi estimates to what extent pi is certain.
The conclusion qi of Ri is of the form “b(x) ∈Qi”, where Qi ⊆Db. The possibil-
ity degrees of qi and ¬qi are respectively noted αi and βi. They are deﬁned by

π(qi)
π(¬qi)

=

1 si
ri 1

□max
min

λi
ρi

where the operator □max
min uses min as the product
and max as the addition. The normalization max(π(pi), π(¬pi)) = 1 implies:
αi = max(si, λi) and βi = max(ri, ρi).
The possibility distribution of the output attribute b associated to Ri is deﬁned
for any u ∈Db by π∗i
b(x)(u) =

αi
if u ∈Qi
βi
if u ∈Qi
. Finally, with n rules, the output
possibility distribution is deﬁned by a min-based conjunctive combination:
π∗
b(x)(u) = min(π∗1
b(x)(u), π∗2
b(x)(u), · · · , π∗n
b(x)(u)).
(1)
We introduce some additional notations. For a set output attribute value u ∈Db,
the computation of its possibility degree is given by:
π∗
b(x)(u) = min(γ1, γ2, · · · , γn),
(2)
where γi = π∗i
b(x)(u) = max(ti, θi) with (ti, θi) =

(si, λi)
if γi = αi
(ri, ρi)
if γi = βi
.
(3)

516
I. Baaj et al.
The relation (2) is a more convenient formulation of (1). According to (3), for
each i = 1, 2, · · · , n, we remark that ti denotes a parameter (si or ri) of the rule
Ri and θi denotes either the possibility degree λi of the premise pi of the rule
Ri or the possibility degree ρi of its negation.
For a premise of a possibilistic rule, the information given by its possibility and
necessity degrees can be represented by the following triplet:
Notation 1. For a premise p, the triplet (p, sem, d) denotes either (p, P, π(p))
or (p, C, n(p)), where sem ∈{P, C} (P for possible, C for certain) is the semantics
attached to the degree d ∈{π(p), n(p)}.
We introduce the following triplets according to the γ1, γ2, · · · , γn appearing in
the relation (2). For i = 1, 2, · · · , n, we set:
(pi, semi, di) =
(pi, P, λi)
if γi = αi
(pi, C, 1 −ρi) if γi = βi .
(4)
Example 1. Possibilistic
rule-based
systems
have
been
used
in
medecine
e.g., DIABETO [4] enables an improvement in the dietetics of diabetic patients
[16]. We propose a possibilistic rule-based system for controlling the blood sugar
level of a patient with type 1 diabetes (Table 1), according to some factors [3]:
Table 1. Rule base for the control of the blood sugar level.
activity (act)
current-bloodsugar (cbs)
future-bloodsugar (fbs)
R1
dinner, drink-coﬀee, lunch
medium, high
high
R2
long-sleep, sport, walking
low, medium
low
R3
alcohol-consumption, breakfast
low, medium
low, medium
The premises p1, p2 and p3 of the possibilistic rules R1, R2 and R3 are built
using two input attributes: activity (act) and current-bloodsugar (cbs). The con-
clusions of the rules use the output attribute future-bloodsugar (fbs). We have
Dact = {alcohol-consumption, breakfast, dinner, drink-coﬀee, long-sleep, lunch,
sport, walking} and Dcbs = Dfbs = {low, medium, high}. As parameters of the
rules, we take s1 = 1, s2 = 0.7, s3 = 1 and r1 = r2 = r3 = 0. The three rules
are certain [10] because we have π(qi | pi) = 1 and ri = π(¬qi | pi) = 0.
In our example, we assume that πact(x)(drink-coﬀee) = 1, πcbs(x)(medium) = 1
and πcbs(x)(low) = 0.3, while the others elements of the domains of the input
attributes have a possibility degree equal to zero. The obtained output possibility
distribution is: ⟨low : 0.3, medium : 0.3, high : 1⟩.
3
Justifying Inference Results
Farreny and Prade’s approach [11] focuses on two explanatory purposes for an
output attribute value u ∈Db, which can be formulated as two questions:

Representation of Explanations of Possibilistic Inference Decisions
517
(i) How to get π∗
b(x)(u) strictly greater or lower than a given τ ∈[0, 1]?
(ii) What are the degrees of the premises justifying π∗
b(x)(u) = τ?
For these two questions, the parameters of the rules si and ri are set.
Regarding (i), the authors of [11] give a suﬃcient condition to obtain π∗
b(x)(u) > τ
for a particular pair (u,τ) of their example. Taking advantage of the notations (2)
and (3) we note that π∗
b(x)(u) ranges between ω = min(t1, t2, · · · , tn) and 1. Fol-
lowing this, a necessary and suﬃcient condition to obtain π∗
b(x)(u) > τ according
to the degrees of premises can be easily stated:
∀i ∈{j ∈{1, 2, · · · , n} | tj ≤τ} we have θi > τ.
And similarly, π∗
b(x)(u) < τ with ω < τ ≤1 will hold if and only if ∃i ∈
{j ∈{1, 2, · · · , n} | tj < τ} such that θi < τ. With these assumptions on
θ1, θ2, · · · , θn, we can give suitable conditions on the possibility distributions
of the input attributes.
Regarding (ii), [11] claim that one can directly read the possibility degrees of
the premises involved in the computation of the possibility degree of an output
attribute value. Their claim is sustained by a particular output attribute value
u of their example. In what follows, we elaborate on this question.
3.1
Justifying the Possibility Degree π∗
b(x)(u) = τ
We give a necessary and suﬃcient condition that allows us to justify π∗
b(x)(u) = τ
by degrees of premises. This allows to extract the subset of premises whose
degrees are involved in the computation of π∗
b(x)(u). To study how the possibility
degree π∗
b(x)(u) = τ with ω ≤τ ≤1 is obtained, we introduce the following two
sets JP and JR in order to compare the parameters t1, t2, · · · tn of the rules
to the degrees θ1, θ2, · · · , θn of the premises in the relation (2). Intuitively, JP
(resp. JR) collect indices where θi is greater (resp. lower) than ti: in other words,
the γi related to the rule Ri can be explained by a degree of the premise (resp.
by a parameter of the rule):
JP = {i ∈{1, 2, · · · , n} | ti ≤θi} and JR = {i ∈{1, 2, · · · , n} | ti ≥θi}.
We have {1, 2, · · · , n} = JP ∪JR but JP or JR may be empty. We take:
cθ = min
i∈JP θi and ct = min
i∈JR ti (with the convention min
∅
= 1).
(5)
For a given output attribute value, if JP ̸= ∅(resp. JR ̸= ∅), cθ (resp. ct) is the
lowest possibility degree justiﬁable by premises (resp. by the parameters of the
rules). By using the properties of the min function, we establish:
Proposition 1.
τ = min(cθ, ct).
(6)

518
I. Baaj et al.
When we can’t explain by degrees of premises. As the degrees θ1, θ2, · · · , θn of the
premises are computed using the possibility distributions of the input attributes,
we may have JP = ∅. In that case, cθ = 1, JR = {1, 2, · · · , n} and:
π∗
b(x)(u) = ct = min(t1, t2, · · · , tn).
(7)
Clearly, it appears that π∗
b(x)(u) is independent from θ1, θ2, · · · , θn and we cannot
justify π∗
b(x)(u) = τ by degrees of premises. For this reason, we suppose in the
following that JP ̸= ∅.
For a set value u ∈Db, we remind that the triplets (pi, semi, di) are deﬁned in (4)
according to (2) and (3). To the non-empty set JP we associate the following set:
Jb(x)(u) = {(pi, semi, di) | i ∈JP and θi = τ}.
(8)
Then, using (5), the equality (6) and the deﬁnition of Jb(x)(u), one can check
directly that we have:
Proposition 2. Jb(x)(u) ̸= ∅⇐⇒π∗
b(x)(u) = cθ.
This means that when Jb(x)(u) ̸= ∅, the set Jb(x)(u) is formed by the premises
justifying π∗
b(x)(u) = τ, because if τ = cθ, π∗
b(x)(u) is the minimum of some
precise degrees θi of premises pi. However, if Jb(x)(u) = ∅, we have τ = ct < cθ
and then τ is the minimum of some parameters si or ri. In this case, there is no
way for deducing τ from θ1, θ2, · · · , θn and therefore from the premises.
Example 2. For our blood sugar level control system (Sect. 2), according to the
relation (2), we set πfbs(x)(high) = min(γ1, γ2, γ3) with γ1 = α1 = max(s1, λ1),
γ2 = β2 = max(r2, ρ2) and γ3 = β3 = max(r3, ρ3). We have λ1 = ρ2 = ρ3 = 1.
To apply (6), we compute JP = {1, 2, 3} and JR = {1}. The possibility degree
τ = 1 of the output attribute value high can be both justiﬁed by degrees of
premises (λ1, ρ2 and ρ3) or a parameter of the rule R1. For rules R2 and R3,
justiﬁcations in terms of premises only can be given. As cθ = 1 = πfbs(x)(high),
using (8), the following triplets are selected:
Jfbs(x)(high) = {(p1, P, 1), (p2, C, 0), (p3, C, 0)}.
Now assume that r1 > 0.3. For u = low, (7) holds and the corresponding set JP
is empty: no justiﬁcation in terms of premises could be given in that case.
4
Justiﬁcation and Unexpectedness
In [1], a method that reduces a premise of a fuzzy if-then rule to the structure
responsible for its activation degree has been deﬁned. In this section, analogously
to the fuzzy case, we deﬁne four functions Rπ, Rn, Cπ and Cn that reduce a
compounded premise with respect to a threshold η > 0. The threshold η is set
according to what is modelled by the possibilistic rule-base for the following
purpose: if a possibility (resp. necessity) degree is higher than the threshold, it

Representation of Explanations of Possibilistic Inference Decisions
519
intuitively means that the information it models is relevantly possible (resp. cer-
tain). In order to deﬁne these reduction functions for premises, we ﬁrst introduce
two auxiliary functions Pπ and Pn that are deﬁned for propositions.
Let a be an attribute with a normalized possibility distribution πa(x) on its
domain Da and a proposition p of the form “a(x) ∈P”, where P ⊆Da. We
introduce the following two subsets of Da:
Pπ = {v ∈P | πa(x)(v) = Π(P)} and Pn = P ∪{v ∈P | 1 −πa(x)(v) > N(P)}.
The proposition related to Pπ (resp. Pn) is noted pπ (resp. pn). Let us notice that:
Proposition 3. Pn = {v ∈P | 1 −πa(x)(v) = N(P)}.
This result is a consequence of N(P) = infv∈P (1−πa(x)(v)). For the proposition
p with its set P, Pπ reduces P if π(p) ≥η and Pn reduces P if n(p) ≥η:
Pπ(p) =

pπ
if π(p) ≥η
p
if π(p) < η and Pn(p) =

pn
if n(p) ≥η
p
if n(p) < η .
We notice that π(Pπ(p)) = π(p) and n(Pn(p)) = n(p).
In what follows, we deﬁne the four reduction functions and show how we apply
them to a triplet (p, sem, d) (Notation 1). We apply Rπ and Rn to the triplets
of Jb(x)(u), see (8), to form the justiﬁcation of π∗
b(x)(u). Similarly, we apply Cπ
and Cn to the same triplets to extract the unexpectedness of π∗
b(x)(u).
4.1
Extracting Justiﬁcations: Rπ and Rn Functions
Let p = p1 ∧p2 ∧· · ·∧pk be a compounded premise, where pj for j = 1, 2, · · · , k,
is a proposition of the form “aj(x) ∈Pj” with Pj ⊆Daj. The function Rπ (resp.
Rn) returns the structure responsible for π(p) (resp. n(p)), which is the con-
junction of propositions Pπ(pj) (resp. Pn(pj)) that make p relevantly possible
(resp. certain) or not.
The reduction function Rπ extends Pπ in the following sense:
Rπ(p) =
k
j=1 Pπ(pj)
if π(p) ≥η

pj∈{ps|π(ps)<η for s=1,··· ,k} pj
if π(p) < η .
Similarly, the reduction function Rn extends Pn in the following sense:
Rn(p) =
k
j=1 Pn(pj)
if n(p) ≥η

pj∈{ps|n(ps)<η for s=1,··· ,k} pj
if n(p) < η .
We notice that π(Rπ(p)) = π(p) and n(Rn(p)) = n(p).

520
I. Baaj et al.
4.2
Extracting Unexpectedness: Cπ and Cn Functions
Intuitively, with respect to the threshold η, for a compounded premise p =
p1 ∧p2 ∧· · · ∧pk that is not relevantly possible (resp. certain), Cπ (resp. Cn)
returns a conjunction of propositions, called an unexpectedness, which is not
involved in the determination of π(p) (resp. n(p)), although relevantly possi-
ble (resp. certain).
When π(p) < η and Aπ
p = {pj | π(pj) ≥η for j = 1, · · · , k} ̸= ∅, the function Cπ
returns the conjunction of the propositions Pπ(pj) such that π(pj) ≥η:
Cπ(p) =

pj∈Aπ
p
Pπ(pj).
If π(p) < η, each proposition pj composing p, is either used in Rπ(p) or in Cπ(p),
according to its possibility degree π(pj).
Similarly, when n(p) < η and An
p = {pj | n(pj) ≥η for j = 1, · · · , k} ̸= ∅, Cn
returns the conjunction of the propositions Pn(pj) such that n(pj) ≥η:
Cn(p) =

pj∈An
p
Pn(pj).
If n(p) < η, each proposition pj composing p, is either used in Rn(p) or in Cn(p),
according to its necessity degree n(pj).
4.3
Justiﬁcation and Unexpectedness of π∗
b(x)(u)
To apply in an appropriate way the reduction functions Rπ and Rn to the
premise p of a triplet (p, sem, d), see Notation (1), we introduce the function SR :
SR (p, sem, d) =

(Rπ(p), sem, d)
if sem = P
(Rn(p), sem, d)
if sem = C .
Similarly, to apply Cπ and Cn, we introduce the function SC :
SC (p, sem, d) =

(Cπ(p), sem, π(Cπ(p)))
if sem = P, d < η and Aπ
p ̸= ∅
(Cn(p), sem, n(Cn(p)))
if sem = C, d < η and An
p ̸= ∅.
The justiﬁcation of π∗
b(x)(u) is formed by applying SR to the triplets of Jb(x)(u),
see (8):
Justiﬁcationb(x)(u) = {SR (p, sem, d) | (p, sem, d) ∈Jb(x)(u)}.
(9)
The possibilistic expressions in the triplets of (9) are suﬃcient to justify “b(x) is
u at a possibility degree π∗
b(x)(u)”. By using SC , we obtain the unexpectedness
of π∗
b(x)(u) i.e., possible or certain possibilistic expressions, which may appear to
be incompatible with π∗
b(x)(u) while not being involved in its determination:

Representation of Explanations of Possibilistic Inference Decisions
521
Unexpectednessb(x)(u) = {SC (p, sem, d) | (p, sem, d) ∈Jb(x)(u)}.
(10)
The purpose of an unexpectedness X is to be able to formulate statements such
as “even if X, b(x) is u at a possibility degree π∗
b(x)(u)”. It is in the same vein
as the “even-if-because” statements studied in [6].
Example 3. For our blood sugar level control system (Sect. 2), we take η = 0.1
and obtain the justiﬁcation of πfbs(x)(high) and its unexpectedness:
• Justiﬁcationfbs(x)(high) = {(Rπ(p1), P, 1), (Rn(p2), C, 0), (Rn(p3), C, 0)}.
•
Unexpectednessfbs(x)(high) = {(Cn(p2), C, 1)}.
For the premise of the rule R1, Rπ returns the conjunction of “act(x) ∈{drink-
coﬀee}” and “cbs(x) ∈{medium}”. By applying Rn to the premise of R2, we
obtain the proposition “act(x) ∈{long-sleep, sport, walking}”. For the premise
of R3, Rn returns “act(x) ∈{alcohol-consumption, breakfast}”. For the premise
of R2 and that of R3, Cn returns for both “cbs(x) ∈{low, medium}”.
5
Representing Explanations of Possibilistic Inference
Decisions
In this section, we represent graphically two explanations: the justiﬁcation and
the unexpectedness of π∗
b(x)(u), (see (9) and (10)) in terms of conceptual graphs.
The resulting conceptual graphs are visual representations of the outcomes of
several analytical operations performed on the rule base that constitute explana-
tions. Conceptual graphs are multi-graphs composed of concept nodes represent-
ing entities and relation nodes representing relationships between these entities.
They were introduced by Sowa [15] and enriched by Chein and Mugnier [5]. We
rely on the work of [5] for our deﬁnitions.
In the following, a possibilistic conceptual graph is deﬁned as a conceptual graph
where each concept node is gifted with a degree and a semantics. For each repre-
sentation, we ﬁrst specify its input which we call an explanation query. We asso-
ciate an explicit explanation query to the justiﬁcation of π∗
b(x)(u) and another one
to its unexpectedness. Each explanation query gives rise to a vocabulary, which is
a simple ontology from which we deﬁne possibilistic conceptual graphs represent-
ing statements and a conceptual graph representing the structure of the expla-
nation. One statement is called an observed phenomenon and represents the pos-
sibility degree π∗
b(x)(u). Depending on the chosen explanation query, the other
statements represent either the justiﬁcation of π∗
b(x)(u) or its unexpectedness.
Each representation is obtained by nesting the possibilistic conceptual graphs
representing the statements in the conceptual graph representing the structure.
5.1
Explanation Query
To describe the vocabularies of the two explanations, we introduce the notion of
explanation query:

522
I. Baaj et al.
Deﬁnition 1. An explanation query is formed by a triplet E
= (T , b, u)
such that:
• T = {(p, sem, d)} is a ﬁnite set of triplets (Notation 1),
• b is an attribute of domain Db with a possibility distribution π∗
b(x) : Db →
[0, 1],
• u ∈Db is an attribute value for which the justiﬁcation or the unexpectedness
of its possibility degree π∗
b(x)(u) is requested.
Let us set an explanation query E = (T , b, u), where m = card(T ) ≥1 for which
we adopt the following notations:
Notation 2 We index the triplets of T as follows:
T = {v(1), v(2), · · · , v(m)}
;
v(i) = (p(i), sem(i), d(i)).
For each triplet v(i) = (p(i), sem(i), d(i)) ∈T , we set a decomposition p(i) =
p(i)
1 ∧p(i)
2 ∧· · · ∧p(i)
ki where for each j = 1, 2, · · · , ki we have:
• p(i)
j
is the proposition “a(i)
j (x) ∈P (i)
j ”, where a(i)
j
is an attribute with a nor-
malized possibility distribution πa(i)
j
: Da(i)
j
→[0, 1], P (i)
j
⊆Da(i)
j
and x is
an item.
• A(i) = {a(i)
1 , a(i)
2 , · · · , a(i)
ki } with card(A(i)) = ki,
• S(i) = {P (i)
1 , P (i)
2 , · · · , P (i)
ki } with card(S(i)) = ki.
We take the disjoint unions: A = 
1≤i≤m A(i) and S = 
1≤i≤m S(i). These
disjoint unions will allow us to deﬁne an application δ: S →A verifying δ(P (i)
j ) =
a(i)
j
and are necessary because the domains of two distinct attributes a(i)
j
and
a(i′)
j′
with i ̸= i′ may have a non-empty intersection. Therefore, the sets P (i)
j
and
P (i′)
j′
of the two propositions p(i)
j
and p(i′)
j′
may be equal.
For our explanations, we take the following explanation queries using the
justiﬁcation and the unexpectedness of an output attribute value u, see (9)
and (10):
EJ = (Justiﬁcationb(x)(u), b, u)
and
EU = (Unexpectednessb(x)(u), b, u).
(11)
5.2
Vocabulary Construction
Let VE = (TC, TR, I, δ, σ) be the vocabulary associated to the explanation query
E = (T , b, u), where TC is the set of concept types, TR is the set of relation
symbols, I is the set of individual markers, δ : I →TC is an individual typing
function and a relation symbol signature σ, which gives for each relation symbol
of TR the concept type of each of its arguments [5]. In VE, the attribute b
and the attributes in A are concept types. The set {u} is an individual marker
representing the attribute value u. The sets in S are individual markers. To any
triplet v(i), we associate a relation symbol inferredv(i) of arity ki +1. Therefore,
a conceptual graph based on VE may contain:

Representation of Explanations of Possibilistic Inference Decisions
523
– a concept node of type b and individual marker {u},
– a concept node of type a(i)
j
and individual marker P (i)
j , which gives a repre-
sentation of the proposition p(i)
j ,
– a relation node of type inferredv(i), which will be linked by multi-edges to the
concept node of type b and the concept nodes representing the propositions
p(i)
1 , p(i)
2 , · · · , p(i)
ki .
Additionally, for structuring the explanations, VE includes: two concept types:
Phenomenon and e, a relation symbol t, and m + 1 individual markers that are
named Statements. In a conceptual graph based on VE, we may ﬁnd a concept
node of type Phenomenon and marker Statement0, m concept nodes of type e
and marker Statementi for i = 1, 2, · · · , m and a relation node of type t, which
will be linked by multi-edges to the m + 1 concept nodes that we just described.
We explicitly deﬁne VE as follows:
• TC = {b} ∪A ∪{e} ∪{Phenomenon} with card(TC) = 3 + 	m
i=1 ki.
• TR = {inferredv(i)|v(i) ∈T } ∪{t} with card(TR) = m + 1 and such that
arity(inferredv(i)) = ki + 1 and arity(t) = m + 1.
• I = {{u}} ∪S ∪{Statement0, Statement1, · · · , Statementm} with card(I) =
m + 2 + 	m
i=1 ki.
• δ : I →TC such that:
– {u} →b
;
P (i)
j
→a(i)
j
– Statement0 →Phenomenon
;
Statementi →e for i = 1, 2, · · · , m.
• The signature map σ is given by:
– σ(inferredv(i)) = (b, a(i)
1 , a(i)
2 , · · · , a(i)
ki ) for v(i) ∈T
– σ(t) = (Phenomenon, e, e, · · · , e).
In the vocabulary VEJassociated to the explanation query EJ, see (11), the con-
cept type e is noted “Justiﬁcation” and the relation symbol t is noted “isJus-
tiﬁedBy”. In VEU of EU, see (11), we respectively note them “Unexpectedness”
and “evenIf”.
5.3
Possibilistic Conceptual Graphs
We introduce possibilistic conceptual graphs that extend basic conceptual graphs
(BG) [5] by adding two additional ﬁelds to the labels of concept nodes:
Deﬁnition 2. A possibilistic conceptual graph (PCG) is a BG G = (C, R, E, l),
where C is the concept nodes set, R the relation nodes set, E is the multi-edges
set and the label function l is extended by allowing a degree and a semantics in
the label of any concept node c ∈C:
l(c) = (type(c) : marker(c)|semc, dc)
The deﬁnition of a star BG [5] i.e., a BG restricted to a relation node and its
neighbors, is naturally extended as a star PCG.

524
I. Baaj et al.
5.4
Conceptual Graphs Based on the Vocabulary VE
Given an explanation query E = (T , b, u) (deﬁnition 1), let us specify, in a PCG
G = (C, R, E, l) built on the vocabulary VE, the deﬁnition of the labels of the
following concept nodes:
• for a concept node c ∈C such that type(c) = b and marker(c) = {u}, we put:
semc = P and dc = π∗
b(x)(u).
(12)
• for a concept node c ∈C such that type(c) = a(i)
j
and marker(c) = P (i)
j , we
take:
semc = sem(i) and dc =

π(p(i)
j )
if sem(i) = P
n(p(i)
j )
if sem(i) = C .
(13)
For the other concept nodes, we specify neither a degree nor a semantics.
On the vocabulary VE, let us deﬁne m+1 PCG D, N1, N2, · · · , Nm and a BG R:
Deﬁnition 3. D is deﬁned as the PCG reduced to one concept node with label
(b : {u} | P, π∗
b(x)(u)). It is a graphical representation of a statement, which
describes an observed phenomenon.
Deﬁnition 4. Each Ni is the star PCG where the unique relation node ri is of
type inferredv(i) with v(i) ∈T . The graph Ni contains ki + 1 concept nodes:
c(i)
b , c(i)
a1 , c(i)
a2 , · · · , c(i)
aki of type b, a(i)
1 , a(i)
2 , · · · , a(i)
ki and marker {u}, P (i)
1 , P (i)
2 , · · · ,
P (i)
ki , as in (12), (13). The multi-edges are labeled (ri, 0, c(i)
b ) and (ri, j, c(i)
aj ) for
j = 1, 2, · · · , ki. Each Ni represents graphically either a statement justifying
the phenomenon represented by D or an unexpectedness statement. The link
between the phenomenon and the justiﬁcation statement or the unexpectedness
statement is represented by a relation node of type inferredv(i).
Deﬁnition 5. The graph R is the star BG where the unique relation node r
is of type t and the m + 1 concept nodes are noted c0, c1, · · · , cm, where c0
is of type “Phenomenon” and c1, c2, · · · , cm are of type e. Their individual
markers are respectively Statement0, Statement1, · · · , Statementm. The multi-
edges are labeled (r, j, cj) for j = 0, 1, · · · , m. R structures the explanation by
representing the link between the observed phenomenon D and the statements
N1, N2, · · · , Nm.
In Fig. 1a and 1b we give examples of Ni and R respectively, with m = 3, e =
“Justiﬁcation” and t = “isJustiﬁedBy”.
5.5
Representation of Explanations
We deﬁne the representation of an explanation as a nested conceptual graph G
deﬁned by its associated tree as in [5], which is denoted Tree(G) = (VT , UT , lT ).
For our representation, the PCG D, N1, N2, · · · , Nm are nested in the concept
nodes of R:

Representation of Explanations of Possibilistic Inference Decisions
525
Fig. 1. Examples of graphs. Nodes with a rectangular shape are concept nodes and
those with an oval shape are relation nodes.
Deﬁnition 6. Tree(G) = (VT , UT , lT ) is given by:
• VT = {R, D, N1, N2, · · · , Nm} is the set of nodes,
• UT = {(R, D), (R, N1), (R, N2), · · · , (R, Nm)} is the set of edges and the node
R is the root of Tree(G),
• the labels of the edges are given by lT (R, D) = (R, c0, D) and lT (R, Ni) =
(R, ci, Ni) for i = 1, 2, · · · , m.
Taking the explanation queries EJ and EU (11), we get by Deﬁnition 6, two nested
conceptual graphs that represent explanations of possibilistic inference decisions.
Example 4. We represent an explanation (Fig. 2) of a decision of our blood sugar
control system (Sect. 2). It is a justiﬁcation of πfbs(x)(high) = 1 built using
EJ = (Justiﬁcationfbs(x)(high), fbs, high) that could be in natural language: “It
is possible that the patient’s blood sugar level will become high. In fact, his activity
is drinking coﬀee and his current blood sugar level is medium. In addition, it is
assessed as not certain that he chose sport, walking, sleeping, eating breakfast or
drinking alcohol as an activity.” Its unexpectedness can also be represented.
Fig. 2. Representation of an explanation.
6
Conclusion
In this paper, we introduced a method to justify by a subset of rule premises
the possibility degree of an output attribute value obtained by the inference of

526
I. Baaj et al.
a possibilistic rule-based system. We used it to represent two kinds of expla-
nations of possibilistic inference decisions. Natural Language Generation sys-
tems may use our representation to produce natural language explanations.
Question-answering applications may also rely on our representation, as the con-
ceptual graphs framework provides a mechanism for querying. This may lead to
the development of more general extraction justiﬁcation methods. Moreover,
the representation of explanation may be adapted for the case of a cascade [9]
and fuzzy systems.
References
1. Baaj, I., Poli, J.P.: Natural language generation of explanations of fuzzy infer-
ence decisions. In: 2019 IEEE International Conference on Fuzzy Systems (FUZZ-
IEEE), pp. 1–6. IEEE (2019)
2. Baaj, I., Poli, J.P., Ouerdane, W.: Some insights towards a uniﬁed semantic rep-
resentation of explanation for explainable artiﬁcial intelligence. In: Proceedings of
the 1st Workshop on Interactive Natural Language Technology for Explainable
Artiﬁcial Intelligence (NL4XAI 2019), pp. 14–19 (2019)
3. Brown, A., Close, K.L.: Bright spots & landmines: the diabetes guide I wish some-
one had handed me. diaTribe Foundation (2017)
4. Buisson, J.C., Farreny, H., Prade, H.: The development of a medical expert system
and the treatment of imprecision in the framework of possibility theory. Inf. Sci.
37(1–3), 211–226 (1985)
5. Chein, M., Mugnier, M.L.: Graph-Based Knowledge Representation: Computa-
tional Foundations of Conceptual Graphs. Springer, London (2008). https://doi.
org/10.1007/978-1-84800-286-9
6. Darwiche, A., Hirth, A.: On the reasons behind decisions. In: Giacomo, G.D.,
Catal´a, A., Dilkina, B., Milano, M., Barro, S., Bugar´ın, A., Lang, J. (eds.) ECAI
2020–24th European Conference on Artiﬁcial Intelligence, 29 August–8 September
2020, Santiago de Compostela, Spain, 29 August–8 September 2020 - Including
10th Conference on Prestigious Applications of Artiﬁcial Intelligence (PAIS 2020).
Frontiers in Artiﬁcial Intelligence and Applications, vol. 325, pp. 712–720. IOS
Press (2020)
7. Dubois, D., Prade, H.: Possibility Theory: An Approach to Computerized Process-
ing of Uncertainty. Plenum Press, New York (1988)
8. Dubois, D., Prade, H.: Possibility theory and its applications: where do we stand?
In: Handbook of Computational Intelligence (2015)
9. Dubois, D., Prade, H.: From possibilistic rule-based systems to machine learning
- a discussion paper. In: Davis, J., Tabia, K. (eds.) SUM 2020. LNCS (LNAI),
vol. 12322, pp. 35–51. Springer, Cham (2020). https://doi.org/10.1007/978-3-030-
58449-8 3
10. Farreny, H., Prade, H.: Default and inexact reasoning with possibility degrees.
IEEE Trans. Syst. Man Cybern. 16(2), 270–276 (1986)
11. Farreny, H., Prade, H.: Positive and Negative Explanations of Uncertain Reasoning
in the Framework of Possibility Theory, pp. 319–333. Wiley, USA (1992)
12. Farreny, H., Prade, H., Wyss, E.: Approximate reasoning in a rule-based expert
system using possibility theory: a case study. In: IFIP Congress, pp. 407–414 (1986)

Representation of Explanations of Possibilistic Inference Decisions
527
13. General Data Protection Regulation: Regulation EU 2016/679 of the European
parliament and of the council of 27 April 2016. Oﬃcial Journal of the European
Union (2016)
14. Reiter, E., Dale, R.: Building applied natural language generation systems. Nat.
Lang. Eng. 3(1), 57–87 (1997)
15. Sowa, J.F.: Conceptual graphs for a data base interface. IBM J. Res. Develop.
20(4), 336–357 (1976)
16. Turnin, M.C.G., et al.: Telematic expert system diabeto: new tool for diet self-
monitoring for diabetic patients. Diabetes Care 15(2), 204–212 (1992)

Towards a Tesseract of Sugeno Integrals
Didier Dubois1, Henri Prade1(B), and Agn`es Rico2
1 IRIT, CNRS & Universit´e Paul Sabatier, 118 route de Narbonne, Cedex 09,
Toulouse, France
{didier.dubois,henri.prade}@irit.fr
2 ERIC, Universit´e Claude Bernard-Lyon 1, 43 bd du 11 novembre,
69100 Villeurbanne, France
agnes.rico@univ-lyon1.fr
Abstract. Structures of opposition, such as the hexagon and diﬀerent
cubes, derived from the square of opposition of ancient logic, have for
more than a decade shown their interest in the analysis of various frame-
works for the representation and processing of information (possibly per-
vaded with uncertainty). The use of a renewed and less constrained vision
of the structures of opposition leads in this article to consider a general
cube and a hypercube of opposition applicable to binary or gradual set-
tings, which is here exempliﬁed on Sugeno integrals and related integrals
1
Introduction
The square of opposition dates back to Aristotle. It had been closely associated
to the history of logic until the XIXth century so long as logic was mainly relying
on the study of syllogisms. A renewal of interest for the square of opposition was
initiated in the 1950’s by R. Blanch´e [3,4] who completed it into a hexagon
relating notions present in many conceptual structures. It was rediscovered by
J.-Y. B´eziau [1,2].
This renewed interest led to a ﬂowering of works on structures of oppositions.
Diﬀerent cubes of opposition have been proposed in particular by A. Moretti [21]
and by Dubois and Prade [8], agreeing respectively with earlier proposals by H.
Reichenbach [30] on the one hand and by J. N. Keynes [20] and W. E. Johnson
[19] on the other hand; see [14]. Moreover it has been also shown that structures
of opposition can apply not only to binary items, but also to notions that are a
matter of degree [5].
Structures of opposition rely on the interplay of diﬀerent forms of negation.
Their merits are twofold, beyond their own algebraic interest: they are shared by
many representation settings, and, for each setting, they may draw attention to
neglected operators. The JK cube of opposition [14] can be encountered in very
diﬀerent knowledge representation formalisms, such as ﬁrst order logic, modal
logic, possibility theory in its all-or-nothing version, formal concept analysis,
rough set theory and abstract argumentation. A gradual extension of this struc-
ture ﬁts with several quantitative settings such as possibility theory (including
c
⃝Springer Nature Switzerland AG 2021
J. Vejnarov´a and N. Wilson (Eds.): ECSQARU 2021, LNAI 12897, pp. 528–542, 2021.
https://doi.org/10.1007/978-3-030-86772-0_38

Towards a Tesseract of Sugeno Integrals
529
the handling of fuzzy events), belief function theory, weighted multiple criteria
aggregations, Choquet and Sugeno integrals, fuzzy rough sets, and fuzzy rela-
tions [5,9,12,13]. Imprecise probability assessments can be organized in a square
of opposition [28].
Besides, a modern reading of the square of opposition, less constrained than
the classical one, has been proposed by Westerst˚ahl [31], motivated by the rep-
resentation of various types of quantiﬁers. This kind of reading has been more
recently applied to fuzzy quantiﬁers [22,23] and to ﬁrst-order logic [25] in more
general structures of opposition. The aim of this paper is to further explore
modern structures of opposition.
The paper is organized into two main parts. Section 2 is dedicated to the pre-
sentation of classical and modern structures of opposition, starting with squares,
and then moving to cubes and hypercubes (also called tesseracts). Section 3 illus-
trates these structures on Sugeno integrals and related operators. However a few
examples of particular cases can already be found in Section 2. This leads to a
more encompassing view of these integrals and their particular cases.
2
Structures of Opposition
The traditional square of opposition [27] is built with universally and existen-
tially quantiﬁed statements in the following way. Consider a statement (A) of
the form “all A’s are B’s”, which is negated by the statement (O) “at least one
A is not a B”, together with the statement (E) “no A is a B”, which is clearly
in even stronger opposition (than O) to the ﬁrst statement (A). These three
statements, together with the negation of the last statement, namely (I) “at
least one A is a B” can be displayed on a square whose vertices are traditionally
denoted by the letters A, I (AﬀIrmative half: from Latin “AﬀIrmo”) and E, O
(nEgative half: from Latin “nEgO”), as pictured in Fig. 1 (where B stands for
“not B”).
Fig. 1. Traditional square of opposition

530
D. Dubois et al.
2.1
Logical View of the Classical Square
As can be checked, noticeable relations hold in the square:
– (i) A and O (resp. E and I) are the negation of each other (linked by dotted
diagonals in Fig. 1);
– (ii) A entails I, and E entails O (it is assumed that there is at least one A
thus avoiding existential import problems) (linked by thin lines with arrows
in Fig. 1);
– (iii) A and E cannot be true together, but both may be false (linked by an
edge in bold black line in Fig. 1) ;
– (iv) I and O cannot be false together, but both may be true (linked by a
double line edge in Fig. 1).
So we can formally state:
Deﬁnition 1. In a square of opposition AEOI, the following holds:
a The diagonal link between A and O, which represents the symmetrical relation
of contradiction, A ≡¬O. Similarly, E ≡¬I.
b The vertical arrows represent implication relations A →I and E →O.
c The link between A and E represents the symmetrical relation of contrariety,
and corresponds to mutual exclusion, namely ¬A ∨¬E should hold.
d The link between I and O represents the symmetrical relation of subcontra-
riety. It is a disjunction, namely I ∨O holds.
This leaves us with three options for deﬁning a formal square of opposition with
independent conditions [5]: i) either we can regard (a) and (b), or ii) (a) and
(c), or iii) (a) and (d), as the basic requirements. Actually, we can express the
content of a square of opposition in propositional logic using two propositional
atoms (say A and E) together with the axiom ¬A∨¬E, expressing their mutual
exclusion (and deﬁning I ≡¬E and O ≡¬A).
In a graded square of opposition [5], each corner A, E, O, I is associated
respectively to degrees α, ϵ, o, ι in [0, 1].
These degrees obey the constraints
(a)–(d). Using Lukasiewicz conjunction, they take the form:
(i) α = 1 −o; ϵ = 1 −ι;
(ii) α ≤ι and ϵ ≤o;
(iii) α + ϵ ≤1;
(iv) ι + o ≥1,
thus clearly generalizing the Boolean case. An illustration is given in Figure 2,
where we can recognize, respectively in the expressions of A, E, O, I, necessity
degrees N(X), N(X), and possibility degrees Π(X), Π(X). Here X has mem-
bership degrees x1, ..., xn (and X is deﬁned by 1 −x1, ..., 1 −xn) , while ∧, ∨,
s ⇒t stand for min, max and max(1−s, t) (Dienes implication). The expressions
in Fig. 2 can also be thought of as weighted min and weighted max aggregations
(and are also particular cases of Sugeno integrals, as further discussed in Sect. 3).

Towards a Tesseract of Sugeno Integrals
531
Fig. 2. Square of qualitative aggregations/possibility theory
2.2
Modern Square of Opposition
D. Westerst˚ahl [31] has proposed a “modern” reading of the square of opposition
AEOI (see also [23]). This reading is quite diﬀerent from the classical one where
logical constraints are supposed to hold between the vertices of the square (see
Deﬁnition 1). In the modern reading, A and E, as I and O are exchanged through
an involutive internal negation, and A and I (resp. E and O) are dual of each
other, where duality is obtained by composing the internal negation with the
(involutive) external negation (assumed to hold between diagonal vertices). In
contrast with the classical square, the deﬁnition of an internal negation requires
the assumption of some formal expression to be transformed via the application
of external and internal negations. Here we use a two place relation or operator
R, linking formal expressions A and B, say R(A, B). Then internal negations
may take the forms R(f(A), B), R(A, g(B)), or R(f(A), g(B)), where f and g are
involutive. Moreover A and f(A) (resp. B and g(B)) should be “in opposition”
in some sense. The modern square of opposition can be deﬁned as follows.
Deﬁnition 2. In a modern square of opposition AEOI, the following holds:
a The diagonal link between A and O, E and I is an external negation.
b The link between A and E, I and O is an internal negation.
c Vertical arrows represent a duality relation deﬁned as the composition of
internal negation and external negation.
Note that A and I (resp. E and O) play the same role and the duality may
correspond to what we have named semi-duality elsewhere [16]. As already said,
we have three possible inner negations which lead to three possible modern
squares of opposition. One is displayed in Fig. 3 (the internal negation is on the
right argument).

532
D. Dubois et al.
Fig. 3. A modern square of opposition
The case with internal negation of R(A, B) deﬁned as R(f(A), g(B)) with
f = g = ¬ corresponds to a Klein group of four logical transformations of a
given logical statement φ = f(p, q) : (i) the identity I(φ) = φ, (ii) the negation
N(φ) = ¬φ, (iii) the reciprocation R(φ) = f(¬p, ¬q), (iv) the correlation C(φ) =
¬f(¬p, ¬q), identiﬁed by Piaget [26]. See also [17]. These transformations are at
work during the acquisition of cognitive capabilities progressively mastered by
children and pre-adolescents [24].
As can be checked, the square of Fig. 1 can also be seen as a modern square
where R(A, B) stands for A ⊆B, then A is A ⊆B, E is A ⊆B (corresponding
to R(A, g(B))), I is A ̸⊆B i.e., A ∩B ̸= ∅and O is A ∩B ̸= ∅). This is the case
for many classical squares. This is true as well for the graded square of Fig. 2
where the external and internal negations are the complementation to 1, as can
be easily checked. But there exist modern squares of opposition that are not
classical squares. Let us show an example:
Consider R(A, B) = A > B with E : A > 1−B, O : A ≤B and I : A ≤1−B,
where A and B are numbers. There is no implication relation between A: A > B
and I : A ≤1 −B (neither A →I, nor I →A).
Another example can be given in the graded case: Starting with A : N(F),
take E : N(ant(F)) where ant(F) is the antonym of F deﬁned by central sym-
metry on the referential. Then if F is large enough, F and ant(F) may overlap
and the condition (iii) α + ϵ ≤1 may fail to hold.
2.3
Classical and Modern Cubes of Opposition
More than one century ago, two logicians, W. E. Johnson [19] and J. N. Keynes
[20], in their discussion of syllogisms, introduced an octagon of opposition. It
was rediscovered in [8,12,14], as a cube of opposition, and called JK-cube. In
such a cube the front facet and the back facet are squares of opposition.
Deﬁnition 3. Let A, I, E, O, a, i, e, o be propositional variables. In a JK-
cube of opposition AEOIaieo, the following holds:
– Front and back facets: AEOI and aeoi are squares of opposition (in the sense
of Deﬁnition 1).

Towards a Tesseract of Sugeno Integrals
533
– Side facets represent entailment relations:
(l) ¬A ∨i; ¬a ∨I; ¬e ∨O; ¬E ∨o.
– Top and bottom facets:
(m) a and E are contraries: ¬a ∨¬E; the same for A and e: ¬A ∨¬e;
(n) i and O are sub-contraries, i.e., i ∨O; the same for I and o, i.e., I ∨o.
Fig. 4. JK cube of opposition
The relations between vertices in the JK cube are shown in Fig. 4 with the
same conventions as in Fig. 1. Note that the back facet is upside down with
respect to the front facet (i and o are at the top, a and e at the bottom) for a
better visualization of the diﬀerent links between the vertices.
The conditions that deﬁne the JK cube of opposition are redundant [5,12]:
– the properties on the front and back facets associated to the properties on
the side facets entail the properties on the diagonal planes between front and
back facets;
– the properties on the front and back facets associated to the properties on
one of the diagonal planes entail the properties on the side facets and the
other diagonal plane.
An example of JK cube (in the sense of Deﬁnition 3 and Fig. 4) is easily
obtained with the inclusion relation ⊆(and its negation) between two sets A,
and B (and their complements), as in Fig. 5. This cube clearly generalizes the
square of Figure 1 (e.g., i corresponds to “at least one an element outside A is
outside B”).

534
D. Dubois et al.
Fig. 5. A JK cube of opposition for inclusion
In the “classical” cube of Fig. 5, the following negations are at work: i) an
external one: ¬(A ⊆B) = A ̸⊆B, and three internal ones: ii) A ⊆B; iii)
A ⊆B; iv) A ⊆B. Six modern squares can be identiﬁed, induced by these
internal negations: AEOI and aeoi are associated to the ﬁrst internal negation;
AeiO and IoaE are associated to the second internal negation; IieE and AaOo
are associated to the third one. This is also a particular instance of a modern
cube of opposition which we present now.
Indeed, more generally, with the general notations of the previous Subsect. 2.2
we get the cube of Fig. 6 where i) dotted lines correspond to the external nega-
tion; ii) double lines to the f-based internal negation; iii) bold lines to the g-
based internal negation; iv) thin lines to f-based or g-based dualities. Note that
in Fig. 6 the back facet is no longer upside down for convenience.
Fig. 6. A modern cube of opposition
The JK cube has been generalized into a graded cube [5,14]. All the rela-
tions of Deﬁnition 3 are now extended in the same way as in the graded square.
Gradual versions of the JK cubes of opposition may be found in many settings

Towards a Tesseract of Sugeno Integrals
535
including possibility theory, Shafer’s belief function theory, qualitative weighted
multiple criteria aggregation, possibilistic Sugeno integrals, and Choquet inte-
grals; see [14] for references, but not for imprecise probabilities at large, because
the notion of Shafer commonality function is absent. The example of qualitative
aggregation is given in Fig. 7. Its front facet is the square of Fig. 2. This cube
can be also viewed as an instance of the modern cube of Fig. 6. Indeed, as in
the graded square of Fig. 2, the external and internal negations f and g here are
the complementation to 1, as can be easily seen. The use of Dienes implication
(s ⇒t = max(1 −s, t)) ensures that the external negation is at work in the
diagonals of the front and back facets. As indicated in Fig. 7, the expressions
correspond to the 4 set functions of possibility theory Π, N, Δ, ∇for the fuzzy
events X and X [12]. Δ and ∇are decreasing set functions (in the wide sense),
called strong (guaranteed) possibility and weak necessity respectively, while Π
and N are increasing set functions (in the wide sense).
Fig. 7. Cube of qualitative aggregations
2.4
The Tesseract
We have seen that an external negation together with an internal negation (g in
Fig. 3) leads to a modern square of opposition. Adding an internal negation (f in
Fig. 6), we obtain a modern cube of opposition. This process can be continued,
and the use of a third internal negation, say an involutive function h, will lead to
a hypercube, or 4-cube, also called tesseract, with 16 vertices. Such a structure of
opposition has been used in [22,23] in the discussion of summarizing statements
based on fuzzy quantiﬁers. More recently, a similar structure was independently
obtained for organizing particular ﬁrst order logic expressions [25].
One may think of several ways for introducing such an h, starting with an
expression of the form R(A, B)1. At least three options may be considered, lead-
ing to diﬀerent tesseracts:
1 Obviously, dealing with a ternary expression R(A, B, C), and h applying to argument
C would lead to a natural and straightforward extension.

536
D. Dubois et al.
– h, as g, applies to B. For instance, in the example of Fig. 7, h(X) = ant(X),
where ant(X) is the antonym of X deﬁned by central symmetry on the refer-
ential of X. Thus, if we have a strict partition, small −medium −large,
small −medium = large, ant(small −medium) = medium −large, and
ant(small −medium) = small. This easily extends with fuzzy sets. Then
the tesseract is made of two cubes that mirror each other: the cube of Fig. 6,
and the cube obtained by applying h to the second argument of the expression
of each vertice of the ﬁrst cube.
– h, as f, applies to A. This is very similar to the previous case. In the example
of Fig. 7, one might deﬁne ant(π), although this has not yet been considered
in possibility theory.
– h applies to R. This means that h(R) would be an involutive transform of R.
In the example of Fig. 7, this would mean exchanging the pair (∧, ⇒) with
another pair of conjunction - implication, such as, e.g., (∧G, →G) where →G
is G¨odel implication (s →G t = 1 if s ≤t and s →G t = t otherwise), while ∧G
is the associated conjunction (for a good interplay with the external negation:
s ∧G t = t if s > 1 −t and s ∧G t = 0 otherwise). This is not just a formal
game, since the 16 expressions thus obtained do make sense in possibility
theory [12].
The tesseracts that can be obtained are pictured in Fig. 8 where the exponent h
on the names of the vertices of the outside cube refers to any of the three above
transformations of the inside cube. Note that the inside cube is the modern cube
of Fig. 6.
Fig. 8. A modern tesseract

Towards a Tesseract of Sugeno Integrals
537
A diﬀerent road has been taken in [15], which does not involve any h. Indeed,
in the modern cube of Fig. 6, we have two involutions f and g at work. Assume
now that f ̸= g and both f and g can be applied to the ﬁrst and the second
argument of R. Then we move from 8 to 16 syntactically distinct expressions
(since on one may each argument in R(A, B), one may apply f, g, g(f) or the
identity). Thus we can build the cube in Fig. 9, not to be confused with the
modern cube of opposition of Fig. 6. Mind that the semantics of the links are
diﬀerent, although bold lines, double lines, dotted lines, vertical lines, horizontal
lines correspond each to a type of transformation, as can be checked. Then
applying f to the second argument on this cube, one obtains another cube which,
with the ﬁrst one, displays the 16 distinct expressions. The result is a tesseract
very diﬀerent from the one of Figure 8, the internal cube of which being the
one of Fig. 9. In this tesseract, the external negation plays no role. We shall not
further consider this cube and this tesseract since they are not genuine modern
structures of opposition built from an external negation and internal involutions.
Moreover, the illustration of this construct with Sugeno integrals in [15] leads to
a number of trivial operators either equal to 1 or to 0 (as can be checked).
Fig. 9. A pseudo-modern cube of opposition with operators f and g
3
Cube of Sugeno Integral
In this section we investigate if it is possible to extend the modern square of
Fig. 3 and the modern cube of Fig. 6 to qualitative aggregation operators that
are more general than the ones exhibited in Fig. 7, namely Sugeno integrals and
related integrals. Obviously, once a cube is obtained, a tesseract in the sense of
Fig. 8 could be derived. The section starts with a refresher on these integrals.
3.1
Sugeno Integrals and Related Integrals
Sugeno integrals are commonly used as qualitative aggregation functions in mul-
tiple criteria decision making [7]. They are also used to model fuzzy quantiﬁers
[18], because they indeed generalise standard quantiﬁers.

538
D. Dubois et al.
We consider a universe of discourse C supposed to be ﬁnite (C can be viewed as
a set of criteria), a scale L which is a bounded totally ordered set with a bottom
denoted by 0 and a top denoted by 1. Moreover L is supposed to be equipped
with an involutive negation denoted by ‘1−’. Sugeno integral is deﬁned using a
fuzzy measure μ : 2C →L (a set function increasing in the wide sense, such that
μ(∅) = 0 and μ(C) = 1, also called capacity). The conjugate fuzzy measure μc
of the fuzzy measure μ is deﬁned by μc = 1 −μ( · ). The Sugeno integral of an
alternative (to be evaluated) x = (x1, · · · , xn) with respect to μ (which describes
the importance of subsets of criteria) is:
Sμ(x) =

A
μ(A) ∧xinf(A) =

A
μ(A) ∨xsup(A)=

A
μc(A) ⇒xsup(A)
where A denotes the complement of A,
xinf(A) = 
i∈A xi and xsup(A) =

i∈A xi.and ⇒is Dienes implication.
Note that when x is the characteristic function of an ordinary subset X of
C, Sugeno integral coincides with the capacity:
Sμ(x) = μ(X)
Let us also recall an important notion and a representation result regarding
qualitative capacities. They are of interest not only for weighting subsets of
criteria, but also as a general setting for modeling uncertainty:
– a qualitative capacity [6] μ can be deﬁned from its M¨obius transform μ#:
μ#(E) = μ(E) if μ(E) > μ(E\{i}), ∀i ∈E; μ#(E) = 0 otherwise, and then
μ(X) = maxE⊆X μ#(E). This may be viewed as a qualitative counterpart of
the expression of a belief function in terms of its mass function (replacing the
sum by max). Here μ# is strictly increasing on the set of its focal elements
F ={E|μ#(E)>0}.
– any qualitative capacity μ in a ﬁnite setting can be represented as the ﬁnite
conjunction of possibility measures (or equivalently the ﬁnite disjunction of
necessity measures) [10].
Moreover, there exist residuated variants of Sugeno integrals [11] where the
sup-min and the inf-max forms no longer coincide, as it is the case in the above
expressions for Sugeno integrals. In these variants, the pair (∧, ⇒) is replaced by
a pair (⊗, →⊗) where →⊗is a residuated implication associated with ⊗, such
as the pair (∧G, →G) already mentioned. Then only an inequality holds between
the sup-min and the inf-max forms:
S⊗
μ (x) =

A
(μ(A) ⊗xinf(A)) ≥S→⊗
μ
(x) =

A
(μc(A) →⊗xsup(A)).
Besides, a Sugeno desintegral (see [11] for more details) is a decreasing func-
tion deﬁned using an anti-fuzzy measure (or anti-capacity) ν which is a decreas-
ing set function in the wide sense such that ν(∅) = 1 and ν(C) = 0. More

Towards a Tesseract of Sugeno Integrals
539
precisely, the Sugeno desintegral of x = (x1, · · · , xn) with respect to the anti-
capacity ν is deﬁned as:
S−
ν (x) =

A⊆C
ν(A) ∧(1 −x)inf(A) =

A⊆C
νc(A) ⇒(1 −x)sup(A).
It is worth noticing that a Sugeno desintegral appears to be deﬁned applying
two negations on the Sugeno integral, one for the fuzzy measure and the second
one for the alternative x since S−
ν (x) = S1−νc(1 −x) with νc(A) = 1 −ν(A).
3.2
In Search of a Modern Cube of Sugeno Integrals
We now examine how the diﬀerent structures of opposition apply to Sugeno
integrals. We start with the square. It is easy to check that we obtain the modern
square of Fig. 10.
Fig. 10. Modern square of Sugeno integral
This square is a particular case of the square presented in [18] where Sugeno
integrals are generalized by qualitative integrals. The modern square, which is
less constrained, presents a clear advantage here with respect to the classical
square. Indeed, following the graded extension of the classical view [9], we have to
restrict ourselves to the pessimistic and optimistic parts of a capacity μ, respec-
tively deﬁned by μ⋆(A) = min(μ(A), μc(A)) and μ⋆(A) = max(μ(A), μc(A)) in
order to guarantee the entailments of the classical view between Sμ⋆(x) and
Sμ⋆(x). The interest of the modern view here is to allow for any Sugeno integral,
since simple duality replaces entailment.
Let us consider the cube now. Interestingly enough, a classical graded cube
of opposition has been obtained for Choquet integrals [13] from the graded cube
of opposition of belief function theory [9]. Indeed the Choquet integral with
respect to a quantitative capacity μ is Cμ(x) = ΣA⊆Cmμ(A)xinf(A) where mμ
denotes the M¨obius transform of μ, and a belief function μ with mass function
mμ coincides with the Choquet integral for classical subsets of C.
As we are going to see, ﬁnding a modern cube of opposition is less straight-
forward for Sugeno integrals. Indeed if we try to mimic the change of expression

540
D. Dubois et al.
from N(X) = n
i=1 πi ⇒xi to Δ(X) = n
i=1(1−πi) ⇒(1−xi) (see Fig. 7), start-
ing from Sμ(x) = 
A μc(A) ⇒xsup(A) one would obtain an expression having
the form 
A(1 −μ)c(A) ⇒(1 −x)sup(A) = 
A μ(A) ⇒(1 −x)sup(A). Although
it somewhat resembles the desintegral expression S−
ν (x) = 
A⊆C νc(A) ⇒
(1 −x)sup(A), μ is a capacity and νc is an anti-capacity. So it does not work.
However, we can write the cube of qualitative aggregations of Fig. 7 with
particular Sugeno integrals, where the set functions are Δ(A) = mini∈A πi, and
∇(A) = 1 −Δ(A) = maxi∈A 1 −πi; see Fig. 11. So a modern cube makes sense
for at least some Sugeno integrals.
Fig. 11. Cube of qualitative aggregations as Sugeno integrals
It has been observed [29] that classical squares of opposition can be com-
bined using conjunction for A and E and disjunctions for I and O; this extends
to graded cubes [12] and to modern squares where no diﬀerence is made between
top and bottom vertices. Thus thanks to the cube of Fig. 7 and the representa-
tion result recalled in Subsect. 3.1 (second hyphen), we obtain a modern cube
for capacities (see Fig. 12). Since Sugeno integrals can be expressed as the min-
imum of possibilistic integrals [9] based on the Πj’s in the cube of Fig. 12, it
Fig. 12. A modern cube of opposition for capacity μ(X) = minj Πj(X)

Towards a Tesseract of Sugeno Integrals
541
is straightforward to jointly extend this cube and the one of Fig. 7 and form
another modern cube of opposition for Sugeno integrals.
4
Concluding Remarks
The paper has reviewed two diﬀerent types of structures of opposition, respec-
tively referred to as ‘classical’ and ‘modern’. The general forms of the modern
square, cube, and tesseract of opposition have been provided. Generally speak-
ing, the modern square and its cube and tesseract extensions oﬀer a rich frame-
work for studying involutive transformations between expressions. We have illus-
trated the process on Sugeno integrals. The tesseract in [25] involves formulas
such that ∀x(A(x) →∀y(B(y) →¬R(x, y))); its graded extension would lead
to a kind of double Sugeno integrals. It is clear that the approach is applicable
to many other representation settings for uncertainty modeling or multicriteria
aggregation especially (since Sugeno integrals include possibilistic measures of
fuzzy events as particular cases). The general investigation of what is precisely
obtained in the diﬀerent settings is a topic for further research.
References
1. B´eziau, J.Y.: New light on the square of oppositions and its nameless corner.
Logical Invest. 10, 218–233 (2003)
2. B´eziau, J.-Y.: The power of the hexagon. Logica Universalis 6(1–2), 1–43 (2012)
3. Blanch´e, R.: Sur l’opposition des concepts. Theoria 19, 89–130 (1953)
4. Blanch´e, R.: Structures Intellectuelles. Essai sur l’Organisation Syst´ematique des
Concepts. Librairie philosophique J. Vrin, Paris (1966)
5. Ciucci, D., Dubois, D., Prade, H.: Structures of opposition induced by relations.
The Boolean and the gradual cases. Ann. Maths Artif. Intel. 76, 351–373 (2016)
6. Dubois, D., Faux, F., Prade, H., Rico, A.: Qualitative capacities and their infor-
mational comparison. In: Proceedings 12th Conference of the European Society for
Fuzzy Logic and Technology (EUSFLAT 2021), 19–24 September 2021, Bratislava
(2021)
7. Dubois, D., Marichal, J.-L., Prade, H., Roubens, M., Sabbadin, R.: The use of the
discrete Sugeno integral in decision-making: a survey. Int. J. Uncert. Fuzz. Knowl.
Syst. 9(5), 539–561 (2001)
8. Dubois, D., Prade, H.: From Blanch´e’s hexagonal organization of concepts to formal
concept analysis and possibility theory. Logica Univers. 6, 149–169 (2012)
9. Dubois, D., Prade, H., Rico, A.: The cube of opposition: a structure underly-
ing many knowledge representation formalisms. In: Proceedings 24th International
Joint Conference on Artiﬁcial Intelligence (IJCAI 2015), Buenos Aires, AAAI
Press, pp. 2933–2939 (2015)
10. Dubois, D., Prade, H., Rico, A.: Representing qualitative capacities as families of
possibility measures. Int. J. Approximate Reasoning 58, 3–24 (2015)
11. Dubois, D., Prade, H., Rico, A.: Residuated variants of sugeno integrals: towards
new weighting schemes for qualitative aggregation methods. Inf. Sci. 329, 765–781
(2016)

542
D. Dubois et al.
12. Dubois, D., Prade, H., Rico, A.: Graded cubes of opposition and possibility theory
with fuzzy events. Int. J. Approximate Reasoning 84, 168–185 (2017)
13. Dubois, D., Prade, H., Rico, A.: Organizing families of aggregation operators into
a cube of opposition. In: Kacprzyk, J., Filev, D., Beliakov, G. (eds.) Granular, Soft
and Fuzzy Approaches for Intelligent Systems. SFSC, vol. 344, pp. 27–45. Springer,
Cham (2017). https://doi.org/10.1007/978-3-319-40314-4 2
14. Dubois, D., Prade, H., Rico, A.: Structures of opposition and comparisons: Boolean
and gradual cases. Logica Univ. 14, 115–149 (2020)
15. Dubois, D., Prade, H., Rico, A.: Le tesseract de l’int´egrale de Sugeno. In: Actes
29`emes Rencontres Francophones sur la Logique Floue et ses Applications (LFA
2020), S`ete, 15–16 October, C´epadu`es (2020)
16. Dubois, D., Prade, H., Rico, A., Teheux, B.: Generalized qualitative Sugeno inte-
grals. Inf. Sci. 415, 429–445 (2017)
17. Gottschalk, W.H.: The theory of quaternality. J. Symb. Logic. 18, 193–196 (1953)
18. Holˇcapek, M., Rico, A.: A note on the links between diﬀerent qualitative integrals.
In: Fuzz-IEEE (2020)
19. Johnson, W.E.: Logic. Cambridge University Press, Part I (1921)
20. Keynes, J.N.: Studies and Exercises in Formal Logic, 3rd edn. MacMillan (1894)
21. Moretti, A.: The geometry of standard deontic logic. Logica Univ. 3, 19–57 (2009)
22. Moyse, G.: R´esum´es linguistiques de donn´ees num´eriques: interpr´etabilit´e et
p´eriodicit´e de s´eries. Th`ese Univ, Paris (2016)
23. Moyse, G., Lesot, M.-J., Bouchon-Meunier, B.: Oppositions in fuzzy linguistic sum-
maries. Int. Conf. Fuzzy System (Fuzz-IEEE 2015), Istanbul (2015)
24. Murray, F.B. (ed.): Critical Features of Piaget’s Theory of the Development of
Thought. University of Delaware Press, Newark (1972)
25. Nilsson, J.F.: A cube of opposition for predicate logic. Logica Univ. 14, 103–114
(2020)
26. Piaget, J.: Trait´e de logique. Essai de logistique op´eratoire, Armand Colin (1949)
27. Parsons, T.: The traditional square of opposition. In: The Stanford Encyclopedia
of Philosophy (2008)
28. Pfeifer, N., Sanﬁlippo, G.: Probabilistic squares and hexagons of opposition under
coherence. Int. J. Approximate Reasoning 88, 282–294 (2017)
29. Pizzi, C.: Contingency logics and modal squares of opposition. In: Beziau, J.Y.,
Gan-Krzywoszynska, K. (eds.) Handbook of Abstracts of the 3rd World Congress
on the Square of Opposition, Beirut, 26–30 June, pp. 29–30 (2012)
30. Reichenbach, H.: The syllogism revised. Philos. Sci. 19(1), 1–16 (1952)
31. Westerst˚ahl, D.: Classical vs. modern squares of opposition, and beyond. In:
Payette, G., B´eziau, J.-Y. (eds.) The Square of Opposition. A General Framework
for Cognition, Peter Lang, pp. 195–229 (2012)

Canonical Extension of Possibility
Measures to Boolean Algebras
of Conditionals
Tommaso Flaminio
, Lluis Godo(B)
, and Sara Ugolini
IIIA - CSIC, 08193 Bellaterra, Spain
{tommaso,godo,sara}@iiia.csic.es
Abstract. In this paper we study conditional possibility measures
within the algebraic setting of Boolean algebras of conditional events.
More precisely, we focus on the possibilistic version of the Strong Con-
ditional Event Problem, introduced for probabilities by Goodman and
Nguyen, and solved in ﬁnitary terms in a recent paper by introducing
the so-called Boolean algebras of conditionals. Our main result shows
that every possibility measure on a ﬁnite Boolean algebra can be canon-
ically extended to an unconditional possibility measure on the resulting
Boolean algebra of conditionals, in such a way that the canonical exten-
sion and the conditional possibility, determined in usual terms by any
continuous t-norm, coincide on every basic conditional expression.
Keywords: Conditional possibility · Boolean algebras of conditionals ·
Strong conditional event problem
1
Introduction
The issue of conditioning in the framework of possibility theory has been dis-
cussed at large in the literature, starting from the pioneering work by Hisdal
[15] and then followed by Dubois, Prade and colleagues (see e.g. [1,9–11]), de
Cooman and Walley [8,17] and Coletti and colleagues [2–6] among others.
Comparing to the case of probability, where there is a common agreement
on taking the conditional probability P(a | b) as the ratio P(a ∧b)/P(b) (in
case P(b) > 0), a conditional possibility Π(a | b) has mainly been deﬁned in the
literature in (at least) two diﬀerent ways: in an ordinal setting, the min-based
conditioning sets Πmin(a | b) = Π(a∧b) if Π(b) > Π(a∧b) and 1 otherwise; and
in a numerical setting, the product-based conditioning deﬁnes Πprod(a | b) as
Π(a∧b)/Π(b) if Π(b) > 0 and 1 otherwise. More in general, if ∗is a continuous t-
norm and ⇒∗is its residuum, one can deﬁne Π+
∗(a | b) = Π(b) ⇒∗Π(a∧b). Such
Π+
∗(a | b) is in fact the greatest solution x of the equation Π(a ∧b) = x ∗Π(b).
Notice that the two deﬁnitions above correspond to the choice ∗= minimum
t-norm and ∗= product t-norm. However, as also discussed in the literature,
see e.g. [10], not every choice of ∗is compatible with the satisfaction of some
c
⃝Springer Nature Switzerland AG 2021
J. Vejnarov´a and N. Wilson (Eds.): ECSQARU 2021, LNAI 12897, pp. 543–556, 2021.
https://doi.org/10.1007/978-3-030-86772-0_39

544
T. Flaminio et al.
rationality postulates for conditional possibility that are counterparts to Cox’s
axioms for conditional probability, and moreover the deﬁnition of Π+
∗itself has
some problems as well, and has to be slightly modiﬁed to another expression
that we will denote with Π∗.
In this paper we deal with conditional possibility measures within the alge-
braic setting of Boolean algebras of conditionals [13], a recently proposed setting
for measure-free conditionals endowed with the structure of a Boolean algebra.
More precisely, we focus on a possibilistic version of the Strong Conditional Event
Problem, considered for probabilities by Goodman and Nguyen [14], and solved
in ﬁnitary terms in [13] with the help of those structures. Indeed, Boolean alge-
bras of conditional events are introduced with the aim of playing for conditional
probability a similar role Boolean algebras of plain events play for probability
theory. To do so, the authors try to identify which properties of conditional
probability depend on the algebraic features of conditional events and which
properties are intrinsic to the nature of the measure itself.
Clearly, a similar analysis can be done for the notion of conditional possibil-
ity, with the advantage that we can now study the suitability of Boolean algebras
of conditionals as a common algebraic framework for conditional events also in
relation to conditional possibility, in the sense of checking whether conditional
possibilities can be regarded as plain possibility measures on algebras of con-
ditional events. Our main result shows that this is actually the case for a very
general notion of conditional possibility. Namely, if A is any ﬁnite Boolean alge-
bra and Π is a possibility measure on A, then we show that, for any continuous
t-norm ∗, Π can be always extended to a (unconditional) possibility measure μΠ
on the Boolean algebra of conditionals C(A) built over A, in such a way that,
for every basic conditional object (a | b) ∈C(A), μΠ(‘(a | b)’) coincides with the
conditional possibility Π∗(a | b).
This paper is structured as follows. Next Sect. 2 is dedicated to recall basic
notions and results on conditional possibility theory, while Boolean algebras
of conditionals will be brieﬂy recalled in Sect. 3. Our main result, namely the
solution of the possibilistic version of the Strong Conditional Event Problem,
will be proved in Sect. 4. Finally, in Sect. 5, we will conclude and present future
work on this subject.
2
Possibility and Conditional Possibility Measures
In this section we will recall basic notions and results about (conditional) possi-
bility theory [9,18]. We will assume the reader to be familiar with the algebraic
setting of Boolean algebras. For every ﬁnite Boolean algebra A, we will hence-
forth denote by at(A) the ﬁnite set of its atoms.
A possibility measure on a Boolean algebra A is a mapping Π : A →[0, 1]
satisfying the following properties:
(Π1) Π(⊤) = 1,
(Π2) Π(⊥) = 0,
(Π3) Π(
i∈I ai) = sup{Π(ai) | i ∈I}

Canonical Extension of Possibility Measures to Boolean Algebras
545
If A is ﬁnite, which will be our setting in this paper, then (Π3) can just be
replaced by
(Π3)∗Π(a ∨b) = max(Π(a), Π(b))
In the ﬁnite setting, possibility measures are completely determined by their
corresponding (normalized) possibility distributions on the atoms of the algebra.
Namely, Π : A →[0, 1] is a possibility measure iﬀthere is a mapping π : at(A) →
[0, 1] with max{π(α) : α ∈at(A)} = 1 such that
Π(a) = max{π(α) : α ∈at(A), α ≤a}.
Clearly, in such a case π(α) = Π(α) for each α ∈at(A).
When we come to deﬁne conditional possibility measures there have been
several proposals in the literature, see e.g. a summary of them in [17]. Never-
theless, Π(a | b) has been traditionally chosen as the greatest solution of the
equation [10,12]:
Π(a ∧b) = x ∗Π(b)
(1)
where either ∗= min or ∗= product. This leads to these two deﬁnitions:
Πmin(a | b) =

Π(a ∧b), if Π(b) > Π(a ∧b)
1,
otherwise
Πprod(a | b) =

Π(a ∧b)/Π(b), if Π(b) > 0
1,
otherwise
Πmin(· | ·) has been known as the qualitative conditioning of Π while Πprod(· | ·)
has been known as the quantitative conditioning of Π.
Actually, in a ﬁnite setting, both expressions satisfy very intuitive properties
a conditional possibility Π(. | .) should enjoy. For instance, for any a, b ∈A and
any α, γ ∈at(A) we have:
P1. Π(· | b) is a possibility measure on A, if Π(b) > 0
P2. Π(a | b) = Π(a ∧b | b), and hence Π(a | b) = 0 if a ∧b = ⊥and Π(b) > 0
P3. If Π(α) < Π(γ) then Π(α | b) < Π(γ | b), if α, γ ≤b
P4. If Π(b) = 1 then Π(a | b) = Π(a ∧b)
P5. If Π(a) = 0 and Π(b) > 0 then Π(a | b) = 0
Notice however that in case the Boolean algebra A is inﬁnite, Πmin may fail to
satisfy P1, that is, Πmin(· | b) might fail to be a possibility measure in a strict
sense, even if Π(b) > 0, see [7].
Actually, if ∗is a continuous t-norm, properties P2-P4 keep holding for the
conditional possibility measure Π+
∗(· | ·) deﬁned as the maximal solution for x
in Eq. (1), in other words, by the conditional possibility deﬁned as
Π+
∗(a | b) = Π(b) ⇒∗Π(a ∧b),
(2)

546
T. Flaminio et al.
where ⇒∗is the residuum of ∗, that is the binary operation deﬁned as x ⇒∗y =
max{z ∈[0, 1] | x ∗z ≤y}. If ∗is without zero-divisors,1 then Π+
∗(· | ·) further
satisﬁes P1 and P5 as well. For every continuous t-norm ∗, the pair (∗, ⇒∗)
is known as a residuated pair. When ∗= min or ∗= product, we obtain the
above deﬁnitions of Πmin and Πprod as particular cases. Note that if ∗has zero
divisors, P1 fails because Π+
∗(⊥| b) = Π(b) ⇒∗0 is not guaranteed to be 0
when Π(b) > 0 [7,10]. For the same reason P5 can fail as well. For instance,
if ∗is Lukasiewicz t-norm, Π(b) ⇒∗0 = 1 −Π(b) > 0 when Π(b) < 1. To
avoid this situation and other issues related to the conditioning by an event of
measure zero, in [4] the authors considered a modiﬁed expression for Π+
∗, called
TDP -conditional possibility, on which we base the following deﬁnition.
Deﬁnition 1. Given a possibility measure Π on a Boolean algebra A and a
continuous t-norm ∗, we deﬁne the mapping Π∗: A × A′ →[0, 1], where A′ =
A \ {⊥}, as follows:
Π∗(a | b) =

Π(b) ⇒∗Π(a ∧b), if a ∧b ̸= ⊥
0,
otherwise.
(3)
In this way, Π∗(a | b) = 0 whenever a ∧b = ⊥, even if Π(b) = 0 (while
Π+
∗(a | b) = 1 in that case). For any continuous t-norm ∗, Π∗keeps satisfying
P2-P4, and if ∗has no zero-divisors then P1 and P5 as well. Moreover, for any
continuous t-norm ∗, Π∗also satisﬁes both P1 and P2 when Π(b) = 0.
However, if ∗is a left-continuous t-norm, to start with, (1) might not have a
greatest solution. Indeed, if we deﬁne Π∗as in (3) for a left-continuous t-norm
∗, we obtain the following characterisation.
Proposition 1. If ∗is a left-continuous t-norm, the following are equivalent:
(i) ∗is continuous.
(ii) Π∗(a | d) ∗Π∗(b | a ∧d) = Π∗(a ∧b | d), for all possibility measures Π on
any Boolean algebra A, and for all a, b, c ∈A.
(iii) Π(a ∧b) = Π(a) ∗Π∗(b | a), for all possibility measures Π on any Boolean
algebra A, and for all a, b, c ∈A.
Proof. That (i) implies (ii) is proved in [4, Prop. 1]. Nevertheless we show
here a more compact proof by using the fact that ∗is continuous iﬀ,
together with its residuum ⇒∗, it satisﬁes the divisibility equation x ∗(x ⇒∗
y) = min(x, y), see e.g. [16]. Notice ﬁrst that if a ∧b ∧d = ⊥, then
Π∗(a | d) ∗Π∗(b | a ∧d) = Π∗(a | d) ∗0 = 0 = Π∗(a ∧b | d), by deﬁnition
of Π∗. Suppose now that a ∧b ∧d ̸= ⊥(and thus also a ∧d ̸= ⊥), then
1 A t-norm ∗has no zero-divisors when x ∗y = 0 implies either x = 0 or y = 0, see
e.g. [16].

Canonical Extension of Possibility Measures to Boolean Algebras
547
Π∗(a | d) ∗Π∗(b | a ∧d) = Π∗(a | d) ∗(Π(a ∧d) ⇒∗Π(a ∧b ∧d))
= Π∗(a | d) ∗((Π(d) ∗Π∗(a | d)) ⇒∗Π(a ∧b ∧d))
= Π∗(a | d) ∗(Π∗(a | d) ⇒∗(Π(d) ⇒∗Π(a ∧b ∧d)))
= min(Π∗(a | d), Π(d) ⇒∗Π(a ∧b ∧d))
= min(Π∗(a | d), Π∗(a ∧b | d))
= Π∗(a ∧b | d).
That (ii) implies (iii) can be easily seen by setting d = ⊤. We now show that
(iii) implies (i) by contraposition. Suppose that ∗is not continuous, this means
that divisibility does not hold and then there exist x, y ∈[0, 1] such that y < x
and x ∗(x ⇒∗y) < y. Then we can ﬁnd a Boolean algebra A with a possibility
measure Π where Π(a)∗Π∗(b | a) < Π(a∧b). Take A to be the Boolean algebra
generated by two elements a and b. The atoms of A are conjunctions of literals
of the generators. We can then deﬁne a measure Π by setting Π(a ∧b) = y,
Π(a∧¬b) = x, Π(¬a∧¬b) = Π(¬a∧b) = 1. Then Π(a) = max(x, y) = x. Thus,
Π(a) ∗Π∗(b | a) = Π(a) ∗(Π(a) ⇒∗Π(a ∧b)) = x ∗(x ⇒∗y) < y = Π(a ∧b),
so (iii) would not hold and the proof is complete.
□
Thus, continuity of the t-norm ∗is equivalent to requiring both usual Bayes’
Theorem (iii) and the generalized version of it (ii).
The problem of conditioning by an event with zero measure (already men-
tioned regarding Deﬁnition 1) and other related problems led Coletti and col-
leagues, see e.g. [2,5,6], to put forward an axiomatic approach to the deﬁnition
of conditional possibility, similar to the case of conditional probability, where
the notion of conditional possibility is primitive and not derived from a (uncon-
ditional) possibility. The following deﬁnition is basically from [2].
Deﬁnition 2. Given a t-norm ∗, a ∗-conditional possibility2 measure on A is
a binary mapping Π(· | ·) : A × A′ →[0, 1], where A′ = A \ {⊥}, satisfying the
following conditions:
(CΠ1) Π(a | b) = Π(a ∧b | b), for all a ∈A, b ∈A′
(CΠ2) Π(· | b) is a possibility measure for each b ∈A′
(CΠ3) Π(a ∧b | c) = Π(b | a ∧c) ∗Π(a | c), for all a, b, c ∈A such that
a ∧c ∈A′.
Regarding (CΠ2), note that if Π(b | ⊤) = 0 then the possibility measure Πb(·) =
Π(· | b) is such that Πb(a) = 0 if a ∧b = ⊥and Πb(a) = 1 otherwise.
The requirement of the operation ∗in (CΠ3) to be a t-norm is very natural
and has been discussed and justiﬁed by several authors, specially in relation
to the simpler, particular case Π(a ∧b | ⊤) = Π(b | a) ∗Π(a | ⊤), see e.g.
[2,5,10]. Moreover, although they might be a bit weaker, arguments diﬀerent
from the ones discussed above to further require ∗to be continuous and without
zero-divisors can also be put forward here:
2 Called T-conditional possibility in [5,6].

548
T. Flaminio et al.
– no zero-divisors: in the case b ≤a ≤c and (CΠ3) yields Π(b | c) =
Π(b | a)∗Π(a | c), then if ∗has zero-divisors, it can happen that Π(b | c) = 0
(which can be read as “(b | c) is not possible”), while Π(b | a) and Π(a | c)
are both positive, which is unintuitive.
– continuity: roughly speaking, if we require that small changes in Π(b | a ∧c)
and Π(a | c) imply small changes in Π(a ∧b | c).
Note that when c = ⊤, (CΠ3) simpliﬁes to:
Π(a ∧b | ⊤) = Π(b | a) ∗Π(a | ⊤),
so that Π(a | b) is a solution of Eq. (1) once we identify Π(.) with Π(. | ⊤).
Moreover, for every (unconditional) Π on A, it turns out that Π∗(as deﬁned in
Deﬁnition 1) is the greatest ∗-conditional possibility Π on A × A′ agreeing with
Π on A, that is, such that Π(. | ⊤) = Π(.).
Proposition 2 (cf. [4]). For every continuous t-norm ∗and for every possi-
bility measure Π on A, let C∗(Π) denote the set of ∗-conditional possibilities
agreeing with Π on A. Then Π∗= max{C∗(Π)}, i.e. Π∗∈C∗(Π) and Π∗≥Π
for every Π ∈C∗(Π).
Proof. That Π∗is a ∗-conditional possibility is shown in [4, Prop.1] and that
Π∗(. | ⊤) = Π(.) is clear by deﬁnition. Suppose Π is a ∗-conditional possibility
agreeing with Π, and let a, b ∈A such that a ∧b ̸= ⊥(otherwise Π(a | b) =
Π∗(a | b) = 0). Then we have Π(a ∧b) = Π(a ∧b | ⊤) = Π(b | a) ∗Π(a | ⊤) =
Π(b | a) ∗Π(a), and hence Π(b | a) ≤(Π(a) ⇒∗Π(a ∧b)) = Π∗(b | a).
3
A Brief Recap on Boolean Algebras of Conditionals
In this section we recall basic notions and results from [13] where, for any Boolean
algebra A = (A, ∧, ∨, ¬, ⊥, ⊤), a Boolean algebra of conditionals, denoted C(A),
is built. Intuitively, a Boolean algebra of conditionals over A allows basic con-
ditionals, i.e. objects of the form (a | b) for a ∈A and b ∈A′ = A \ {⊥}, to
be freely combined with the usual Boolean operations up to certain extent, but
always satisfying the following requirements:
(R1) For every b ∈A′, the conditional (b | b) will be the top element of C(A),
while (¬b | b) will be the bottom;
(R2) Given b ∈A′, the set of conditionals A | b = {(a | b) : a ∈A} will be the
domain of a Boolean subalgebra of C(A), and in particular when b = ⊤, this
subalgebra will be isomorphic to A;
(R3) In a conditional (a | b) we can replace the consequent a by a ∧b, that is,
the conditionals (a | b) and (a ∧b | b) represent the same element of C(A);
(R4) For all a ∈A and all b, c ∈A′, if a ≤b ≤c, then the result of conjunctively
combining the conditionals (a | b) and (b | c) must yield the conditional (a | c).
R4 encodes a sort of restricted chaining of conditionals and it is inspired by the
chain rule of conditional probabilities: P(a | b) · P(b | c) = P(a | c) whenever
a ≤b ≤c.

Canonical Extension of Possibility Measures to Boolean Algebras
549
In mathematical terms, the formal construction of the algebra of condi-
tionals C(A) is done as follows. One ﬁrst considers the free Boolean algebra
Free(A
|
A)
=
(Free(A
|
A), ⊓, ⊔, ∼, ⊥∗, ⊤∗) generated by the set
A | A = {(a | b) : a ∈A, b ∈A′}. Then, in order to accommodate the
requirements R1-R4 above, one considers the smallest congruence relation ≡C
on Free(A | A) satisfying:
(C1) (b | b) ≡C ⊤∗, for all b ∈A′;
(C2) (a1 | b) ⊓(a2 | b) ≡C (a1 ∧a2 | b), for all a1, a2 ∈A, b ∈A′;
(C3) ∼(a | b) ≡C (¬a | b), for all a ∈A, b ∈A′;
(C4) (a ∧b | b) ≡C (a | b), for all a ∈A, b ∈A′;
(C5) (a | b) ⊓(b | c) ≡C (a | c), for all a ∈A, b, c ∈A′ such that a ≤b ≤c.
Note that (C1)–(C5) faithfully account for the requirements R1-R4 where, in
particular, (C2) and (C3) account for R2. Finally, the algebra C(A) is deﬁned
as follows.
Deﬁnition 3. For every Boolean algebra A, the Boolean algebra of conditionals
of A is the quotient structure C(A) = Free(A | A)/≡C.
To distinguish the operations of A from those of C(A), the following signature
is adopted:
C(A) = (C(A), ⊓, ⊔, ∼, ⊥C, ⊤C).
Since C(A) is a quotient of Free(A | A), elements of C(A) are equivalence classes,
but without danger of confusion, one can henceforth identify classes [t]≡C with
one of its representative elements, in particular, by t itself. For instance, using
this notation convention, the following equalities, which correspond to (C1)–(C5)
above, hold in any Boolean algebra of conditionals C(A), for all a, a′ ∈A and
b, c ∈A′:
1. (b | b) = ⊤C;
2. (a | b) ⊓(c | b) = (a ∧c | b);
3. ∼(a | b) = (¬a | b);
4. (a ∧b | b) = (a | b);
5. if a ≤b ≤c, then (a | b) ⊓(b | c) = (a | c).
A basic observation is that if A is ﬁnite, C(A) is ﬁnite as well, and hence
atomic. Therefore, in the ﬁnite case, it is of crucial importance, for uncertainty
measures on C(A) like probabilities or possibilities, to identify the atoms of C(A),
since these measures are fully determined by their values on the atoms of the
algebra they are deﬁned upon.
If A is a Boolean algebra with n atoms, i.e. |at(A)| = n, it is shown in
[13] that the atoms of C(A) are in one-to-one correspondence with sequences
α = ⟨α1, . . . , αn−1⟩of n−1 pairwise diﬀerent atoms of A, each of these sequences
giving rise to an atom ωα deﬁned as the following conjunction of n −1 basic
conditionals:
ωα = (α1 | ⊤) ⊓(α2 | ¬α1) ⊓. . . ⊓(αn−1 | ¬α1 ∧. . . ∧¬αn−2),
(4)

550
T. Flaminio et al.
It is then clear that |at(C(A))| = n!.
Now, assume at(A) = {α1, . . . , αn} and let us consider a basic conditional
of the form (α1 | b) for α1 ≤b. Let us denote the set of atoms below (α1 | b) by
at≤(α1 | b) = {ωγ ∈at(C(A)) | ωγ ≤(α1 | b)}. For every j = 1, . . . , n, deﬁne
Sj = {ωγ ∈at≤(α1 | b) | γ = ⟨γ1, . . . , γn−1⟩with γ1 = αj}.
In other words, for all j, Sj denotes the set of all those atoms below (α1 | b) of
the form ωγ where γ is the sequence ⟨γ1, . . . , γn−1⟩and γ1 = αj. In [13, Lemma
5.1] it is proved the following result.
Lemma 1. The set {Sj}j=1...,n is a partition of at≤(α1 | b), i.e. at≤(α1 | b) =
∪j=1,nSj, and Si ∩Si = ∅whenever i ̸= j. Moreover, for j ≥2, Sj = ∅if αj ≤b.
Now, let us describe the sets Sj’s. For every sequence ⟨α1, . . . , αi⟩of pairwise
diﬀerent atoms of A with i ≤n −1, let us consider the following set of atoms:
α1, . . . , αi = {ωγ ∈at(A) | γ = ⟨α1, . . . , αi, σi+1, . . . , σn−1}.
In other words, α1, . . . , αi stands for the subset of at(C(A)) whose elements are
in one-one correspondence with those sequences γ having ⟨α1, . . . , αi⟩as initial
segment. The next proposition recalls how the sets Sj are obtained from the sets
α1, . . . , αi.
Proposition 3 ([13, Proposition 5.3]). Let A be a ﬁnite Boolean algebra with
atoms α1, . . . , αn and let b ∈A such that ¬b = β1 ∨. . . ∨βk and let βk = αj.
For every t = 2, . . . , k −1, denote by Pt the set of permutations p : {1, . . . , t} →
{1, . . . , t}. Then,
Sj = αj, α1 ∪
k
i=1
αj, βi, α1 ∪
k−1

t=2

p∈Pt
αj, βp(1) . . . , βp(t)α1.
4
The Strong Conditional Event Problem for Possibility
Measures
Our aim in this section is to solve the following possibilistic version of the Good-
man and Nguyen’s strong conditional event problem:
Possibilistic Strong Conditional Event Problem: Given a possibility
measure Π on a ﬁnite Boolean algebra A, ﬁnd an extension of Π to a
possibility measure μΠ on the Boolean algebra of conditionals C(A) such
that, for any (a | b) ∈C(A), μΠ(a | b) coincides with the ∗-conditional
possibility Π∗(a | b) for some suitable continuous t-norm ∗.
Since the main results we need to prove next in order to provide a solution
for this problem are quite general, in what follows, unless stated otherwise, ∗
will denote an arbitrary t-norm and ⇒a residuated implication, so that we will

Canonical Extension of Possibility Measures to Boolean Algebras
551
not assume, in general, that ⇒is the residuum of ∗. A pair (∗, ⇒) of this kind
will be simply called a t-norm implication-pair, or simply a ti-pair.
Given a possibility measure Π on a ﬁnite Boolean algebra A with n atoms
and a ti-pair (∗, ⇒), we start by deﬁning a mapping μΠ : at(C(A)) →[0, 1] as
follows: for every atom of the Boolean algebra of conditionals C(A) as in Eq. (4),
we deﬁne
μΠ(ωα) = Π(α1) ∗(Π(¬α1) ⇒Π(α2)) ∗. . . ∗(Π(
n−2
j=1 ¬αi) ⇒Π(αn−1)).
For the sake of a lighter and shorter notation, we will henceforth write
Π(a)
Π(b)≻
in place of Π(b) ⇒Π(a), so that the above expression becomes
μΠ(ωα) = Π(α1) ∗
Π(α2)
Π(¬α1)≻∗. . . ∗
Π(αn−1)
Π(n−2
j=1 ¬αi)≻.
(5)
Lemma 2. For any ti-pair (∗, ⇒), μΠ is a normalized possibility distribution
on at(C(A)).
Proof. We have to show that μΠ takes value 1 for some atom of C(A). First of
all note that (5) can be equivalently expressed as
μΠ(ωα) = Π(α1) ∗
Π(α2)
Π(α2∨α3∨...∨αn)≻∗. . . ∗
Π(αn−1)
Π(αn−1∨αn)≻.
Since Π is a possibility measure on A, we can rank its atoms at(A) according
to the values taken by Π, so that we can assume at(A) = {β1, β2, . . . , βn} with
1 = Π(β1) ≥Π(β2) ≥. . . ≥Π(βn). In this way, for 1 ≤i ≤n −1, Π(βi) =
maxn
j=i Π(βj) = Π(βi ∨βi+1 ∨. . . ∨βn), and hence
Π(βi)
Π(βi∨βi+1∨...∨βn)≻= 1. Let
β to be the sequence β = ⟨β1, β2, . . . , βn−1⟩. Then, the corresponding atom ωβ
of C(A) is such that μΠ(ωβ) = Π(β1) ∗
Π(β2)
Π(β2∨β3∨...)≻∗. . . ∗
Π(βn−1)
Π(βn−1∨βn)≻= 1. □
In light of the above lemma, we extend μΠ to a possibility measure on C(A),
denoted by the same symbol μΠ : C(A) →[0, 1], by deﬁning, for every t ∈C(A),
μΠ(t) = max{μΠ(ωα) | ωα ∈at(C(A)) and ωα ≤t}.
Then, so deﬁned, μΠ is indeed a possibility measure on C(A).
Deﬁnition 4. For every possibility measure Π on a Boolean algebra A and for
every ti-pair (∗, ⇒), the possibility measure μΠ : C(A) →[0, 1] is called the
(∗, ⇒)-canonical extension of Π.
Example 1. Let A be a Boolean algebra with 3 atoms (say α1, α2, α3) and
8 elements, and let Π be the possibility distribution Π(α1) = 0.7, Π(α2) =
1, Π(α3) = 0.2. The corresponding algebra of conditionals C(A) has 3! = 6
atoms and 64 elements. In particular, the atoms are the following compound
conditionals:

552
T. Flaminio et al.
ω1 = (α1 | ⊤) ⊓(α2 | ¬α1), ω2 = (α1 | ⊤) ⊓(α3 | ¬α1), ω3 = (α2 | ⊤) ⊓(α1 | ¬α2)
ω4 = (α2 | ⊤) ⊓(α3 | ¬α2), ω5 = (α3 | ⊤) ⊓(α1 | ¬α3), ω6 = (α3 | ⊤) ⊓(α2 | ¬α3)
Now, let us consider the basic conditionals t1 = (α1 | ¬α3) = (α1 | ¬α1 ∨α2)
and t2 = (α2 ∨α3 | ⊤). It is not hard to check that t1 = ω1 ⊔ω2 ⊔ω5; t2 =
ω3 ⊔ω4 ⊔ω5 ⊔ω6; t1 ⊓t2 = ω5; ¬t1 = ω3 ⊔ω4 ⊔ω6.
Taking ∗= product and ⇒its residuum, the extension μΠ of Π to C(A) is
determined by the following distribution on its atoms given by (5):
μΠ(ω1) = 0.7 · 1 = 0.7, μP (ω2) = 0.7 · (0.2/1) = 0.14,
μP (ω3) = 1 · (0.7/0.7) = 1, μP (ω4) = 1 · (0.2/0.7) = 2/7,
μP (ω5) = 0.2 · 0.7 = 0.14, μP (ω6) = 0.2 · 1 = 0.2.
Then we can compute the possibility degree of any other (basic or compound)
conditional. For instance:
μΠ(t1) = μΠ(α1 | ¬α1 ∨α2) = max{μΠ(ω1), μΠ(ω2), μΠ(ω5)} = 0.7
μΠ(t2) = μΠ(α2 ∨α3 | ⊤) = max{μΠ(ω3), μΠ(ω4), μΠ(ω5), μΠ(ω6)} = 1
μΠ(t1 ⊓t2) = μΠ(ω5) = 0.2
μΠ(¬t1) = max{μΠ(ω3), μΠ(ω4), μΠ(ω6)} = 1.
⊓⊔
For what follows, recall the sets Sj and α1, . . . , αi as deﬁned in Sect. 3.
Lemma 3. Let αi1, . . . , αit ∈at(A). Then
μΠ(αi1, . . . , αit) = Π(αi1) ∗
Π(αi2)
Π(¬αi1)≻∗· · · ∗
Π(αit)
Π(¬αi1∧¬αi2∧...∧¬αit−1)≻.
Proof. Let γ1, . . . , γl be the atoms of A diﬀerent from αi1, . . . , αit. Thus, ωδ ∈
αi1, . . . , αit iﬀδ = ⟨αi1, . . . , αit, σ1, . . . , σl−1⟩where σ = ⟨σ1, . . . , σl−1⟩is any
string which is obtained by permuting l −1 elements in {γ1, . . . , γl}.
Therefore, if Ψ = ¬αi1 ∧. . . ∧¬αit(= γ1 ∨. . . ∨γl), by letting
- K = Π(αi1) ∗
Π(αi2)
Π(¬αi1)≻∗. . . ∗
Π(αit)
Π(¬αi1∧¬αi2∧...∧¬αit−1)≻,
- H = 
σ
Π(σ1)
Π(Ψ)≻∗
Π(σ2)
Π(Ψ∧¬σ1)≻∗. . . ∗
Π(σl−1)
Π(Ψ∧¬σ1∧...∧¬σn−2)≻,
it is clear that μΠ(αi1, . . . , αit) = K ∗H. We now prove H = 1 by induction
on the number l of atoms diﬀerent from αi1, . . . , αit, i.e. l = n −t:
(Case 0) The basic case is for l = 2. In this case H =
Π(γ1)
Π(Ψ)≻∨
Π(γ2)
Π(Ψ)≻=
Π(γ1∨γ2)
Π(Ψ) ≻. Thus, the claim trivially follows because Ψ = γ1 ∨γ2.
(Case l) For any j = 1, . . . , l consider the strings σ such that σ1 = γj. Therefore,
H =
 Π(γ1)
Π(Ψ)≻∗
σ:σ1=γ1(
Π(σ2)
Π(Ψ∧¬γ1)≻∗. . . ∗
Π(σl−1)
Π(Ψ∧¬σ1∧...∧¬σl−2)≻)

∨. . .
. . . ∨
 Π(γl)
Π(Ψ)≻∗
σ:σ1=γl(
Π(σ2)
Π(Ψ∧¬γ1)≻∗. . . ∗
Π(σl−1)
Π(Ψ∧¬σ1∧...∧¬σl−2)≻)

.
By
inductive
hypothesis,
each
term

σ:σ1=γj(
Π(σ2)
Π(Ψ∧¬γ1)≻
∗
. . . ∗
Π(σl−1)
Π(Ψ∧¬σ1∧...∧¬σl−2)≻) equals 1. Thus, H = l
j=1
Π(γj)
Π(Ψ)≻=
Π(l
j=1 γj)
Π(Ψ)
≻= 1 since
Ψ = l
j=1 γj.
□

Canonical Extension of Possibility Measures to Boolean Algebras
553
As we did in the last part of Sect. 3, let us focus on a basic conditional of
the form (α1 | b) for b ≥α1. Then, the next result immediately follows from
Lemma 3 above together with the observation that S1 = α1.
Lemma 4. Let A be a ﬁnite Boolean algebra with atoms α1, . . . , αn, let b ∈A be
such that b ≥α1. Then, for every ti-pair (∗, ⇒), the (∗, ⇒)-canonical extension
μΠ of a possibility measure Π of A satisﬁes, μΠ(S1) = 
ωγ∈S1 μΠ(ωγ) = Π(α1).
Now we prove our main result that will directly lead to a ﬁnitary solution for the
Possibilistic Strong Conditional Event Problem stated at the beginning of this
section. In what follows, for every basic conditional (a | b), we write μΠ(a | b)
instead of μΠ(‘(a | b)’).
Theorem 1. For every possibility measure Π : A →[0, 1] and for every ti-pair
(∗, ⇒), the (∗, ⇒)-canonical extension μΠ : C(A) →[0, 1] of Π to C(A) is such
that, for every basic conditional (a | b),
μΠ(a | b) =
0,
if a ∧b = ⊥
Π(b) ⇒Π(a ∧b), otherwise.
Proof. Let Π be given as in the hypothesis, and let μΠ be deﬁned on C(A)
as in (5). By deﬁnition and Lemma 2, μΠ is a normalised possibility mea-
sure. Moreover, recall that (a | b) = ⊥C if a ∧b = ⊥, whence, by deﬁnition,
μΠ(a | b) = 0. Thus, it is left to prove the claim for those (a | b) with a ∧b > ⊥.
To this end, notice that each basic conditional (a | b) equals (
αi≤a αi | b) =

αi≤a(αi | b). Since μP is maxitive, let us prove the claim for conditionals of the
form (α | b) for α ∈at(A) and α ≤b. Without loss of generality we will assume
α = α1. By Lemma 1, {Sj}j=1,...,n is a partition of at≤(α1 | b). Thus,
μΠ(α1 | b) =
	n
j=1 μΠ(Sj) = μΠ(S1) ∨
	n
j=2 μΠ(Sj).
Also by Lemma 1, Sj = ∅, and thus μΠ(Sj) = 0, for all those j such that αj ≤b.
Therefore, by Lemma 4 one has,
μΠ(α1 | b) =
	n
j=1 μΠ(Sj) = Π(α1) ∨
	
j:αj≤¬b μΠ(Sj).
From this equation it already follows μΠ(α1 | b) ≥Π(α1). We have to prove
that μΠ(α1 | b) =
Π(α1)
Π(b) ≻.
In what follows we assume ¬b = β1 ∨. . . ∨βk, where Π(β1) ≤. . . ≤Π(βk).
The proof is divided in three steps:
(1) μΠ(α1 | b) ≤
Π(α1)
Π(b) ≻. If ω ≤(α1 | β), then it must be of the form
ω = (β1 | ⊤)∧(β2 | ¬β1)∧. . .∧(βi | ¬β1∧. . .∧¬βi−1)∧(α1 | ¬β1∧. . .∧¬βi)∧. . .
where β1, . . . , βi ≤¬b, or equivalently, ¬β1 . . . ¬βi ≥b. Then we have ω ≤(α1 |
¬β1 ∧. . . ∧¬βi), and hence
μΠ(ω) ≤
Π(α1)
Π(¬β1∧...∧¬βi)≻≤
Π(α1)
Π(b) ≻.

554
T. Flaminio et al.
Therefore, μΠ(α1 | b) = 
j:αj≤¬b μΠ(Sj) ≤
Π(α1)
Π(b) ≻.
(2) Π(b) < 1 implies μΠ(α1 | b) ≥
Π(α1)
Π(b) ≻. In this case, Π(¬b) = 1, and thus
Π(βk) = 1. Let ib = max{j ∈{1, ..., k} | Π(βib) < Π(b)}, and consider the
subset S = βk, βk−1, . . . , βib+1, α1 of atoms of C(A). Proposition 3 shows that
S ⊆Sk, whence, in particular, S ⊆
j:αj≤¬b Sj. Then, by Lemma 3, we have:
μΠ(S) = Π(βk) ∗
Π(βk−1)
Π(¬βk)≻∗. . . ∗
Π(βib+1)
Π(¬βk∧...∧¬βib+2)≻∗
Π(α1)
Π(¬βk∧...∧¬βib+1)≻.
Notice that, for every i ∈{ib + 1, . . . , k}, Π(¬βk ∧¬βk−1 . . . ∧¬βi) = Π(βi−1 ∨
. . .∨β1 ∨b) = Π(βi−1)∨Π(b) = Π(βi−1), and moreover Π(¬βk ∧. . .∧¬βib+1) =
Π(βib ∨. . . ∨β1 ∨b) = Π(b). Therefore we have:
μΠ(S) = Π(βk) ∗
Π(βk−1)
Π(βk−1)≻∗. . . ∗
Π(βib+1)
Π(βib+1)≻∗
Π(α1)
Π(b) ≻= 1 ∗. . . ∗1 ∗
Π(α1)
Π(b) ≻
and hence, μΠ(α1 | b) ≥μΠ(S) =
Π(α1)
Π(b) ≻.
(3) Π(b) = 1 implies μΠ(α1 | b) ≥
Π(α1)
Π(b) ≻. As above observed, μΠ(α1 | b) ≥
Π(α1) =
Π(α1)
Π(b) ≻.
Therefore, (1), (2) and (3) imply μΠ(α1 | b) =
Π(α1)
Π(b) ≻.
□
Let us observe that the above result holds under very general assumptions
about the operations ∗and ⇒under which μΠ may fail to be a ∗-conditional
possibility in the sense of Deﬁnition 2. However, when we restrict to the case
where ∗is a continuous t-norm and (∗, ⇒∗) is a residuated pair, by Proposition 2
μΠ does provide a solution to the possibilistic strong conditional event problem.
Corollary 1. If Π is a possibility measure on a ﬁnite Boolean algebra A, ∗is
a continuous t-norm and ⇒∗its residuum, then the (∗, ⇒∗)-canonical extension
μΠ of Π to C(A) restricted to basic conditionals yields a ∗-conditional possibility,
in the sense that μΠ(a | b) = Π∗(a | b) for every basic conditional (a | b) ∈C(A).
5
Conclusions and Future Work
In this paper we have shown that, in a ﬁnite setting, Boolean algebras of condi-
tionals are suitable structures to accommodate conditional possibility measures,
in a similar (but not completely analogous) way to that of conditional proba-
bilities in [13]. We back up this claim by proving the Strong Conditional Event
Problem for conditional possibility: for ∗being a continuous t-norm without zero-
divisors, any possibility measure Π in a Boolean algebra of events A extends
to a possibility measure μΠ on the full algebra of conditional events C(A) such
that its restriction to basic conditionals is the ∗-conditional possibility Π∗(. | .).
This result indeed holds under very general assumptions. This suggests that the
structure of a Boolean Algebra of Conditionals leaves room to accommodate
notions of conditional possibility other than Π∗(. | .). In particular, we plan to

Canonical Extension of Possibility Measures to Boolean Algebras
555
study when and how a ∗-conditional possibility on A×A′, regarded as a partially
speciﬁed (unconditional) possibility on C(A), can be extended to any compound
conditional in the full algebra C(A).
Following the lines of [13], we will also investigate under which conditions a
(plain) possibility measure Π on C(A) satisﬁes the axioms of a ∗-conditional pos-
sibility as in Deﬁnition 2 when restricted to basic conditionals. More precisely we
will consider those possibilities satisfying Π((a | b)∧(b | c)) = Π(a | b)∗Π(b | c),
for a ≤b ≤c and ∗being a continuous t-norm. These measures, in analogy with
[13], can be called ∗-separable. Furthermore, we want to explore to what extent
the results can be generalised when replacing the real unit interval [0, 1] by more
general scales for possibility measures, like G¨odel or strict BL-algebras. Finally,
we also plan to study the use of Boolean algebras of conditionals in relation to
conditioning more general uncertainty models such as belief functions.
Acknowledgments. The authors are grateful to the anonymous reviewers for their
comments and suggestions. Flaminio acknowledges partial support by the Spanish
Ram´on y Cajal research program RYC-2016- 19799. Godo is indebted to Karim Tabia
for helpful discussions on conditional possibility and acknowledges support by the
Spanish project ISINC (PID2019-111544GB-C21). Ugolini receives funding from the
European Union’s Horizon 2020 research and innovation programme under the Marie
Sklodowska-Curie grant agreement No 890616 (H2020-MSCA-IF-2019).
References
1. Benferhat, S., Dubois, D., Prade, H.: Expressing independence in a possibilistic
framework and its application to default reasoning. In: Cohn, A.G. (ed.) Proceed-
ings of the 11th European Conference in Artiﬁcial Intelligence (ECAI 1994), pp.
150–154. Wiley, New York (1994)
2. Bouchon-Meunier, B., Coletti, G., Marsala, C.: Conditional possibility and neces-
sity. In: Bouchon-Meunier, B., et al. (eds.) Technologies for Constructing Intelligent
Systems. STUDFUZZ, vol. 90, pp. 59–71. Springe, Heidelberg (2002). https://doi.
org/10.1007/978-3-7908-1796-6 5. (Selected papers from IPMU 2000)
3. Coletti, G., Petturiti, D.: Finitely maxitive T-conditional possibility theory: coher-
ence and extension. Int. J. Approximate Reasoning 71, 64–88 (2016)
4. Coletti, G., Petturiti, D., Vantaggi, B.: Independence in possibility theory under
diﬀerent triangular norms. In: van der Gaag, L.C. (ed.) ECSQARU 2013. LNCS
(LNAI), vol. 7958, pp. 133–144. Springer, Heidelberg (2013). https://doi.org/10.
1007/978-3-642-39091-3 12
5. Coletti, G., Vantaggi, B.: Comparative models ruled by possibility and necessity:
a conditional world. Int. J. Approximate Reasoning 45, 341–363 (2007)
6. Coletti, G., Vantaggi, B.: T-conditional possibilities: coherence and inference.
Fuzzy Sets Syst. 160, 306–324 (2009)
7. De Baets, B., Tsiporkova, E., Mesiar, R.: Conditioning in possibility theory with
strict order norms. Fuzzy Sets Syst. 106, 221–229 (1999)
8. De Cooman, G.: Possibility theory II: conditional possibility. Int. J. Gen. Syst.
25(4), 325–351 (1997)
9. Dubois, D., Prade, H.: Possibility Theory. Plenum, New York (1988)

556
T. Flaminio et al.
10. Dubois, D., Prade, H.: The logical view of conditioning and its application to
possibility and evidence theories. Int. J. Approximate Reasoning 4, 23–46 (1990)
11. Dubois, D., Prade, H.: Bayesian conditioning in possibility theory. Fuzzy Sets Syst.
92, 223–240 (1997)
12. Dubois, D., Prade, H.: Possibility theory: qualitative and quantitative aspects. In:
Smets, Ph. (ed.) Handbook on Defeasible Reasoning and Uncertainty Management
Systems, vol. 1, pp. 169–226. Kluwer Academic, Dordrecht (1988)
13. Flaminio, T., Godo, L., Hosni, H.: Boolean algebras of conditionals, probability
and logic. Artif. Intell. 286, 103347 (2020)
14. Goodman, I.R., Nguyen, H.T.: A theory of conditional information for probabilistic
inference in intelligent systems: II. Product space approach. Inf. Sci. 76, 13–42
(1994)
15. Hisdal, E.: Conditional possibilities: independence and non-interaction. Fuzzy Sets
Syst. 1, 283–297 (1978)
16. Klement, E.P., Mesiar, R., Pap, E.: Triangular Norms. TREN, vol. 8. Springer,
Dordrecht (2000). https://doi.org/10.1007/978-94-015-9540-7
17. Walley, P., de Cooman, G.: Coherence of rules for deﬁning conditional possibility.
Int. J. Approximate Reasoning 21, 63–107 (1999)
18. Zadeh, L.A.: Fuzzy sets as a basis for a theory of possibility. Fuzzy Sets Syst. 1,
3–28 (1978)

On the KLM Properties of a Fuzzy DL
with Typicality
Laura Giordano(B)
DISIT - Universit`a del Piemonte Orientale, Alessandria, Italy
laura.giordano@uniupo.it
Abstract. The paper investigates the properties of a fuzzy logic of typicality.
The extension of fuzzy logic with a typicality operator was proposed in recent
work to deﬁne a fuzzy multipreference semantics for Multilayer Perceptrons, by
regarding the deep neural network as a conditional knowledge base. In this paper,
we study its properties. First, a monotonic extension of a fuzzy ALC with typi-
cality is considered (called ALCFT) and a reformulation the KLM properties of
a preferential consequence relation for this logic is devised. Most of the proper-
ties are satisﬁed, depending on the reformulation and on the fuzzy combination
functions considered. We then strengthen ALCFT with a closure construction by
introducing a notion of faithful model of a weighted knowledge base, which gen-
eralizes the notion of coherent model of a conditional knowledge base previously
introduced, and we study its properties.
Keywords: Knowledge representation · Description logics · Preferential
semantics · Fuzzy logics
1
Introduction
Preferential approaches have been used to provide axiomatic foundations of non-
monoto- nic and common sense reasoning [4,22,35,38,39,42–44]. They have been
extended to description logics (DLs), as well as to the ﬁrst order case [3], to deal with
inheritance with exceptions in ontologies, by allowing for non-strict forms of inclu-
sions, called typicality or defeasible inclusions, with different preferential semantics
[14,30,31] and closure constructions [7,16,17,19,28,32,45].
In previous work [24], a concept-wise multipreference semantics for weighted
knowledge bases has been proposed to account for preferences with respect to differ-
ent concepts, by allowing, for a concept C, a set of typicality inclusions of the form
T(C) ⊑D (meaning “the typical C’s are D’s” or “normally C’s are D’s”) with pos-
itive or negative weights. The concept-wise multipreference semantics has been ﬁrst
introduced as a semantics for ranked DL knowledge bases [25] and extended in [24]
to weighted knowledge bases in the two-valued and fuzzy case, based on a different
semantic closure construction, still in the spirit of Lehmann’s lexicographic closure [39]
and Kern-Isberner’s c-representations [35,36], but exploiting multiple preferences asso-
ciated to concepts. A related semantics with multiple preferences has been proposed in
the ﬁrst-order logic setting by Delgrande and Rantsaudis [23], and an extension of DLs
c
⃝Springer Nature Switzerland AG 2021
J. Vejnarov´a and N. Wilson (Eds.): ECSQARU 2021, LNAI 12897, pp. 557–571, 2021.
https://doi.org/10.1007/978-3-030-86772-0_40

558
L. Giordano
with defeasible role quantiﬁers and defeasible role inclusions has been developed by
Britz and Varzinczak [13,15], by associating multiple preferences to roles.
The concept-wise multipreference semantics has been proved to have some desired
properties from the knowledge representation point of view in the two-valued case [25].
In particular, it satisﬁes the KLM postulates of a preferential consequence relation. The
properties of entailment in a fuzzy DL with typicality have not been studied so far. In
this paper, a monotonic extension of a fuzzy ALC with typicality is considered (called
ALCFT) and the KLM properties of a preferential consequence relation are reformu-
lated for this logic. Most of the postulates are satisﬁed, depending on the reformulation
and on the chosen fuzzy combination functions.
The closure construction developed in [24] to deﬁne the models of a weighted defea-
sible knowledge base in the fuzzy case is reconsidered, by introducing a notion of faith-
ful model of a weighted (fuzzy) knowledge base which is weaker than the notion of
coherent fuzzy multipreference model in [24]. This allows us to capture the larger class
of monotone non-decreasing activation functions in Multilayer Perceptrons (MLPs)
[33], based on the idea that, in a deep neural network, synaptic connections can be
regarded as weighted conditionals. The paper discusses the properties of faithful multi-
preference entailment for weighted conditionals in a fuzzy DL.
2
The Description Logic ALC and Fuzzy ALC
In this section we recall the syntax and semantics of the description logic ALC [1] and
of its fuzzy extension [41].
Let NC be a set of concept names, NR a set of role names and NI a set of individual
names. The set of ALC concepts (or, simply, concepts) can be deﬁned inductively:
– A ∈NC, ⊤and ⊥are concepts;
– if C and D are concepts, and r ∈NR, then C ⊓D, C ⊔D, ¬C, ∀r.C, ∃r.C are
concepts.
A knowledge base (KB) K is a pair (T , A), where T is a TBox and A is an ABox.
The TBox T is a set of concept inclusions (or subsumptions) C ⊑D, where C, D are
concepts. The ABox A is a set of assertions of the form C(a) and r(a, b) where C is a
concept, a and b are individual names in NI and r a role name in NR.
An ALC interpretation is deﬁned as a pair I = ⟨Δ, ·I⟩where: Δ is a domain—a set
whose elements are denoted by x, y, z, . . . —and ·I is an extension function that maps
each concept name C ∈NC to a set CI ⊆Δ, each role name r ∈NR to a binary
relation rI ⊆Δ × Δ, and each individual name a ∈NI to an element aI ∈Δ. It is
extended to complex concepts as follows:
⊤I = Δ,
⊥I = ∅,
(¬C)I = Δ\CI,
(∃r.C)I = {x ∈Δ | ∃y.(x, y) ∈rI and y ∈CI},
(C ⊓D)I = CI ∩DI,
(∀r.C)I = {x ∈Δ | ∀y.(x, y) ∈rI ⇒y ∈CI},
(C ⊔D)I = CI ∪DI.
The notion of satisﬁability of a KB in an interpretation and the notion of entailment
are deﬁned as follows:
Deﬁnition 1 (Satisﬁability and entailment).
Given an ALC interpretation I =
⟨Δ, ·I⟩:

On the KLM Properties of a Fuzzy DL with Typicality
559
– I satisﬁes an inclusion C ⊑D if CI ⊆DI;
– I satisﬁes an assertion C(a) (resp., r(a, b)) if aI ∈CI (resp., (aI, bI) ∈rI).
Given a KB K = (T , A), an interpretation I satisﬁes T (resp. A) if I satisﬁes all
inclusions in T (resp. all assertions in A); I is a model of K if I satisﬁes T and A.
A subsumption F = C ⊑D (resp., an assertion C(a), r(a, b)), is entailed by K,
written K |= F, if for all models I =⟨Δ, ·I⟩of K, I satisﬁes F.
Given a knowledge base K, the subsumption problem is the problem of deciding
whether an inclusion C ⊑D is entailed by K.
Fuzzy description logics have been widely studied in the literature for representing
vagueness in DLs [5,9,41,46,47], based on the idea that concepts and roles can be
interpreted as fuzzy sets. Formulas in Mathematical Fuzzy Logic [21] have a degree of
truth in an interpretation rather than being true or false; similarly, axioms in a fuzzy DL
have a degree of truth, usually in the interval [0, 1]. In the following we shortly recall
the semantics of a fuzzy extension of ALC referring to the survey by Lukasiewicz
and Straccia [41]. We limit our consideration to a few features of a fuzzy DL and, in
particular, we omit considering datatypes.
A fuzzy interpretation for ALC is a pair I = ⟨Δ, ·I⟩where: Δ is a non-empty
domain and ·I is fuzzy interpretation function that assigns to each concept name A ∈
NC a function AI : Δ →[0, 1], to each role name r ∈NR a function rI : Δ × Δ →
[0, 1], and to each individual name a ∈NI an element aI ∈Δ. A domain element
x ∈Δ belongs to the extension of A to some degree in [0, 1], i.e., AI is a fuzzy set.
The interpretation function ·I is extended to complex concepts as follows:
⊤I(x) = 1,
⊥I(x) = 0,
(¬C)I(x) = ⊖CI(x),
(∃r.C)I(x) = supy∈Δ rI(x, y) ⊗CI(y),
(C ⊔D)I(x) = CI(x) ⊕DI(x)
(∀r.C)I(x) = infy∈Δ rI(x, y) ▷CI(y),
(C ⊓D)I(x) = CI(x) ⊗DI(x)
where x ∈Δ and ⊗, ⊕, ▷and ⊖are arbitrary but ﬁxed t-norm, s-norm, implication
function, and negation function, chosen among the combination functions of various
fuzzy logics (we refer to [41] for details). For instance, in both Zadeh and G¨odel logics
a ⊗b = min{a, b}, a ⊕b = max{a, b}. In Zadeh logic a ▷b = max{1 −a, b} and
⊖a = 1 −a. In G¨odel logic a ▷b = 1 if a ≤b and b otherwise; ⊖a = 1 if a = 0 and 0
otherwise.
The interpretation function ·I is also extended to non-fuzzy axioms (i.e., to strict
inclusions and assertions of an ALC knowledge base) as follows:
(C ⊑D)I = infx∈ΔCI(x)▷DI(x),
(C(a))I = CI(aI),
R(a, b))I = RI(aI, bI).
A fuzzy ALC knowledge base K is a pair (T , A) where T is a fuzzy TBox and A a
fuzzy ABox. A fuzzy TBox is a set of fuzzy concept inclusions of the form C ⊑D θ n,
where C ⊑D is an ALC concept inclusion axiom, θ ∈{≥, ≤, >, <} and n ∈[0, 1].
A fuzzy ABox A is a set of fuzzy assertions of the form C(a)θn or r(a, b)θn, where C
is an ALC concept, r ∈NR, a, b ∈NI, θ ∈{≥, ≤, >, <} and n ∈[0, 1]. Following
Bobillo and Straccia [5], we assume that fuzzy interpretations are witnessed, i.e., the sup
and inf are attained at some point of the involved domain. The notions of satisﬁability
of a KB in a fuzzy interpretation and of entailment are deﬁned in the natural way.
Deﬁnition 2 (Satisﬁability and entailment for fuzzy KBs). A fuzzy interpretation I
satisﬁes a fuzzy ALC axiom E (denoted I |= E), as follows, for θ ∈{≥, ≤, >, <}:

560
L. Giordano
– I satisﬁes a fuzzy ALC inclusion axiom C ⊑D θ n if (C ⊑D)Iθ n;
– I satisﬁes a fuzzy ALC assertion C(a) θ n if CI(aI)θ n;
– I satisﬁes a fuzzy ALC assertion r(a, b) θ n if rI(aI, bI)θ n.
Given a fuzzy KB K = (T , A), a fuzzy interpretation I satisﬁes T (resp. A) if I satisﬁes
all fuzzy inclusions in T (resp. all fuzzy assertions in A). A fuzzy interpretation I is a
model of K if I satisﬁes T and A. A fuzzy axiom E is entailed by a fuzzy knowledge
base K, written K |= E, if for all models I =⟨Δ, ·I⟩of K, I satisﬁes E.
3
Fuzzy ALC with Typicality: ALCFT
In this section, we extend fuzzy ALC with typicality concepts of the form T(C), for
C a concept in fuzzy ALC. The idea is similar to the extension of ALC with typicality
[31], but transposed to the fuzzy case. The extension allows for the deﬁnition of fuzzy
typicality inclusions of the form T(C) ⊑D θ n, meaning that typical C-elements are
D-elements with a degree greater than n. A typicality inclusion T(C) ⊑D, as in the
two-valued case, stands for a KLM conditional implication C |∼D [38,39], but now it
has an associated degree.
We call ALCFT the extension of fuzzy ALC with typicality. As in the two-valued
case, such as in SROIQPT, a preferential extension of SROIQ with typicality [27],
or in the propositional typicality logic, PTL [8] the typicality concept may be allowed to
freely occur within inclusions and assertions, while the nesting of the typicality operator
is not allowed.
We have to deﬁne the semantics of ALCFT. Observe that, in a fuzzy ALC inter-
pretation I = ⟨Δ, ·I⟩, the degree of membership CI(x) of the domain elements x in a
concept C, induces a preference relation <C on Δ, as follows:
x <C y iff CI(x) > CI(y)
(1)
Each <C has the properties of preference relations in KLM-style ranked interpretations
[39], that is, <C is a modular and well-founded strict partial order. Let us recall that,
<C is well-founded if there is no inﬁnite descending chain x1 <C x0, x2 <C x1,
x3 <C x2, . . . of domain elements; <C is modular if, for all x, y, z ∈Δ, x <C y
implies (x <C z or z <C y). Well-foundedness holds for the induced preference <C
deﬁned by condition (1) under the assumption that fuzzy interpretations are witnessed
[5] (see Sect. 2) or that Δ is ﬁnite. In the following, we will assume Δ to be ﬁnite.
While each preference relation <C has the properties of a preference relation in
KLM rational interpretations [39] (also called ranked interpretations), here there are
multiple preferences and, therefore, fuzzy interpretations can be regarded as multipref-
erential interpretations, which have also been studied in the two-valued case [23,25,28].
Each preference relation <C captures the relative typicality of domain elements wrt
concept C and may then be used to identify the typical C-elements. We will regard
typical C-elements as the domain elements x that are preferred with respect to relation
<C among those such that CI(x) ̸= 0.
Let CI
>0 be the crisp set containing all domain elements x such that CI(x) > 0,
that is, CI
>0 = {x ∈Δ | CI(x) > 0}. One can provide a (two-valued) interpretation of

On the KLM Properties of a Fuzzy DL with Typicality
561
typicality concepts T(C) in a fuzzy interpretation I, by letting:
(T(C))I(x) =

1 if x ∈min<C(CI
>0)
0 otherwise
(2)
where min<(S) = {u : u ∈S and ∄z ∈S s.t. z < u}. When (T(C))I(x) = 1, we
say that x is a typical C-element in I.
Note that, if CI(x) > 0 for some x ∈Δ, min<C(CI
>0) is non-empty. This gener-
alizes the property that, in the crisp case, CI ̸= ∅implies (T(C))I ̸= ∅.
Deﬁnition 3 (ALCFT interpretation). An ALCFT interpretation I = ⟨Δ, ·I⟩is fuzzy
ALC interpretation, equipped with the valuation of typicality concepts given by condi-
tion (2) above.
The fuzzy interpretation I = ⟨Δ, ·I⟩implicitly deﬁnes a multipreference interpre-
tation, where any concept C is associated to a preference relation <C. This is different
from the two-valued multipreference semantics in [25], where only a subset of distin-
guished concepts have an associated preference, and a notion of global preference < is
introduced to deﬁne the interpretation of the typicality concept T(C), for an arbitrary
C. Here, we do not need to introduce a notion of global preference. The interpretation
of any ALC concept C is deﬁned compositionally from the interpretation of atomic
concepts, and the preference relation <C associated to C is deﬁned from CI.
The notions of satisﬁability in ALCFT, model of an ALCFT knowledge base, and
ALCFT entailment can be deﬁned in a similar way as in fuzzy ALC (see Sect. 2). In
particular, given an ALCFT knowledge base K, an inclusion T(C) ⊑D θn (with
θ ∈{≥, ≤, >, <} and n ∈[0, 1]) is entailed from K in ALCFT (written K |=ALCFT
T(C) ⊑D) if T(C) ⊑D θn is satisﬁed in all ALCFT models I of K. For instance,
the fuzzy inclusion axiom ⟨T(C) ⊑D ≥n⟩is satisﬁed in a fuzzy interpretation
I = ⟨Δ, ·I⟩if infx∈Δ(T(C))I(x) ▷DI(x) ≥n holds, which can be evaluated based
on the combination functions of some speciﬁc fuzzy logic.
4
KLM Properties of ALCFT
In this section we aim at investigating the properties of typicality in ALCFT and, in
particular, verifying whether the KLM postulates of a preferential consequence rela-
tion [38,39] are satisﬁed in ALCFT. The satisﬁability of KLM postulates of rational
or preferential consequence relations [38,39] has been studied for ALC with defeasible
inclusions and typicality inclusions in the two-valued case [14,31]. The KLM postu-
lates of a preferential consequence relation (i.e., reﬂexivity, left logical equivalence,
right weakening, and, or, cautious monotonicity) can be reformulated for ALC with
typicality, by considering that a typicality inclusion T(C) ⊑D stands for a conditional
C|∼D in KLM preferential logics, as follows:
(REFL) T(C) ⊑C
(LLE) If |= A ≡B and T(A) ⊑C, then T(B) ⊑C
(RW) If |= C ⊑D and T(A) ⊑C, then T(A) ⊑D
(AND) If T(A) ⊑C and T(A) ⊑D, then T(A) ⊑C ⊓D
(OR) If T(A) ⊑C and T(B) ⊑C, then T(A ⊔B) ⊑C
(CM) If T(A) ⊑D and T(A) ⊑C, then T(A ⊓D) ⊑C

562
L. Giordano
For ALC, |= A ≡B is interpreted as equivalence of concepts A and B in the underlying
description logic ALC (i.e., AI = BI in all ALC interpretations I), while |= C ⊑D
is interpreted as validity of the inclusion C ⊑D in ALC (i.e., AI ⊆BI for all ALC
interpretations I).
How can these postulates be reformulated in the fuzzy case? First, we can interpret
|= C ⊑D as the requirement that the fuzzy inclusion C ⊑D ≥1 is valid in fuzzy
ALC (that is, C ⊑D ≥1 is satisﬁed in all fuzzy ALC interpretations), and |= A ≡B
as the requirement that the fuzzy inclusions A ⊑B ≥1 and B ⊑A ≥1 are valid in
fuzzy ALC. For the typicality inclusions, we have some options. We might interpret an
inclusion T(A) ⊑C as the fuzzy inclusion T(A) ⊑C ≥1, or as the fuzzy inclusion
T(A) ⊑C > 0.
The fuzzy inclusion axiom T(A) ⊑C ≥1, is rather strong, as it requires that all
typical A-elements belong to C with membership degree 1. On the other hand, T(A) ⊑
C > 0 is a weak condition, as it requires that all typical A-elements belong to C with
some membership degree greater that 0. We will see that both options fail to satisfy one
of the postulates. With the ﬁrst option, the postulates can be reformulated as follows:
(REFL′) T(C) ⊑C ≥1
(LLE′) If |= A ≡B and T(A) ⊑C ≥1, then T(B) ⊑C ≥1
(RW ′) If |= C ⊑D and T(A) ⊑C ≥1, then T(A) ⊑D ≥1
(AND′) If T(A) ⊑C ≥1 and T(A) ⊑D ≥1, then T(A) ⊑C ⊓D ≥1
(OR′) If T(A) ⊑C ≥1 and T(B) ⊑C ≥1, then T(A ⊔B) ⊑C ≥1
(CM ′) If T(A) ⊑D ≥1 and T(A) ⊑C ≥1, then T(A ⊓D) ⊑C ≥1
We can prove that, in the well-known Zadeh logic and G¨odel logic, the postulates above,
with the exception of reﬂexivity (REFL′), are satisﬁed in all ALCFT interpretations.
Proposition 1. In Zadeh logic and in G¨odel logic any ALCFT interpretation I =
⟨Δ, ·I⟩satisﬁes postulates (LLE′), (RW ′), (AND′), (OR′) and (CM ′).
Proof. Let I = ⟨Δ, ·I⟩be an ALCFT interpretation in Zadeh logic, or in G¨odel logic.
Let us prove, as an example, that I satisﬁes postulate (LLE′).
Assume that axioms A ⊑B ≥1, B ⊑A ≥1 are valid in fuzzy ALC and that
T(A) ⊑C ≥1 is satisﬁed in I. We prove that T(B) ⊑C ≥1 is satisﬁed in I, that is
(T(B) ⊑C)I ≥1.
From the validity of A ⊑B ≥1 and B ⊑A ≥1, infx∈ΔAI(x) ▷BI(x) ≥1 and
infx∈ΔBI(x) ▷AI(x) ≥1 in both Zadeh logic and in G¨odel logic. Hence,
for all x ∈Δ, AI(x) ▷BI(x) ≥1 and BI(x) ▷AI(x) ≥1
(3)
In G¨odel logic, this implies that: for all x ∈Δ, AI(x) ≤BI(x) and BI(x) ≤AI(x),
i.e., AI(x) = BI(x). Therefore, the preference relations <A and <B must be the
same and AI
>0 = BI
>0. Hence, T(A)I(x) = T(B)I(x) for all x ∈Δ, and from
(T(A) ⊑C)I ≥1, it follows that (T(B) ⊑C)I ≥1, that is, T(B) ⊑C ≥1 is
satisﬁed in I.
In Zadeh logic, (3) implies that, for all x ∈Δ, max{1 −AI(x), BI(x)} ≥1 and
max{1−BI(x), AI(x)} ≥1 must hold, which implies that either AI(x) = BI(x) = 0

On the KLM Properties of a Fuzzy DL with Typicality
563
or AI(x) = BI(x) = 1. It follows that, for all x ∈Δ, either (T(A))I(x) =
(T(B))I(x) = 0 or (T(A))I(x) = (T(B))I(x) = 1. Hence, for all x ∈Δ, it
holds that (T(A))I(x) = (T(B))I(x), and from (T(A) ⊑C)I ≥1, it follows that
T(B) ⊑C ≥1 is satisﬁed in I.
For the other postulates the proof is similar and is omitted for lack of space.
⊓⊔
The meaning of the postulate (REFL′) T(C) ⊑C ≥1 is that the typical C-elements
must have a degree of membership in C equal to 1, which may not be the case in an
interpretation I, when there is no domain element x such that CI(x) = 1. Product
and Lukasiewicz logics fail to satisfy postulates (REFL′) and (OR′), but they can be
proven to satisfy all the other postulates.
The following corollary is a consequence of Proposition 1.
Corollary 1. In Zadeh logic and in G¨odel logic, ALCFT entailment from a given
knowledge base K satisﬁes postulates (LLE′), (RW ′), (AND′), (OR′) and (CM ′).
For instance, for (AND′), if T(A) ⊑C ≥1 and T(A) ⊑D ≥1 are entailed from a
knowledge base K in ALCFT, then they are satisﬁed in all the models I of K. Hence,
by Proposition 1, T(A) ⊑C ⊓D ≥1 is as well satisﬁed in all the ALCFT models
of K, i.e., it is entailed by K. As reﬂexivity is not satisﬁed, the notion of ALCFT
entailment from a given knowledge base K does not deﬁne a preferential consequence
relation, under the proposed formulation of the postulates. It is easy to see that ALCFT
entailment does not satisfy the Rational Monotonicity postulate.
Let us consider the alternative formulation of the postulates in the fuzzy case,
obtained by interpreting the typicality inclusion T(A) ⊑C as the fuzzy inclusion
axiom T(A) ⊑C > 0, that is:
(REFL′′) T(C) ⊑C > 0
(LLE′′) If |= A ≡B and T(A) ⊑C > 0, then T(B) ⊑C > 0
(RW ′′) If |= C ⊑D and T(A) ⊑C > 0, then T(A) ⊑D > 0
(AND′′) If T(A) ⊑C > 0 and T(A) ⊑D > 0, then T(A) ⊑C ⊓D > 0
(OR′′) If T(A) ⊑C > 0 and T(B) ⊑C > 0, then T(A ⊔B) ⊑C > 0
(CM ′′) If T(A) ⊑D > 0 and T(A) ⊑C > 0, then T(A ⊓D) ⊑C > 0
With this formulation of the postulates, it can be proven that in Zadeh logic and
G¨odel logic all postulates except for Cautious Monotonicity (CM ′′) are satisﬁed in
all ALCFT interpretations. Under this formulation, reﬂexivity (REFL′′) is satisﬁed
in all ALCFT interpretations I, as it requires that all typical C-elements in I have a
degree of membership in C higher than 0, which holds from the deﬁnition of (T(C))I.
Proposition 2. In Zadeh logic and in G¨odel logic, any ALCFT interpretation I satis-
ﬁes postulates (REFL”), (LLE”), (RW”), (AND”) and (OR”).
Cautious Monotonicity is too strong in the formulation (CM ′′). From the hypoth-
esis that T(A) ⊑D > 0 is entailed from K, we know that, in all ALCFT models I
of K, the typical A-elements have some degree of membership n in D. However, the
degree n may be small and not enough to conclude that typical A ⊓D-elements are as

564
L. Giordano
well typical A-elements (which is needed to conclude that T(A ⊓D) ⊑C > 0 is satis-
ﬁed in I, given that T(A) ⊑C > 0 is satisﬁed in I). A weaker alternative formulation
of Cautious Monotonicity can be obtained by strengthening the antecedent of (CM ′′)
as follows:
(CM ∗) If T(A) ⊑D ≥1 and T(A) ⊑C > 0, then T(A ⊓D) ⊑C > 0
This postulate is satisﬁed by ALCFT entailment in Zadeh logic and G¨odel logic. We
can then prove that:
Corollary 2. In Zadeh logic and in G¨odel logic, ALCFT entailment from a knowledge
base K satisﬁes postulates (REFL”), (LLE”), (RW”), (AND”), (OR”) and (CM∗).
As in the two-valued case, the typicality operator T introduced in ALCFT is non-
monotonic in the following sense: for a given knowledge base K, from K |=ALCFT
C ⊑D ≥1 we cannot conclude that K |=ALCFT T(C) ⊑T(D) ≥1. Nevertheless,
the logic ALCFT is monotonic, that is, for two ALCFT knowledge bases K and K′,
if K ⊆K′, and K |=ALCFT E then K′ |=ALCFT E. ALCFT is a fuzzy relative of the
monotonic logic ALC + T [31].
Although most of the postulates of a preferential consequence relation hold in
ALCFT, this typicality extension of fuzzy ALC is rather weak, as it happens in the
two-valued case for ALC + T, for the rational extension of ALC in [14] and for pref-
erential and rational entailment in KLM approach [39]. In particular, ALCFT does not
allow to deal with irrelevance. From the fact that birds normally ﬂy, one would like to
be able to conclude that normally yellow birds ﬂy (being the color irrelevant to ﬂying).
As in KLM framework, in the two-valued case, this has led to the deﬁnition of non-
monotonic defeasible DLs [7,16–18,28,32], which exploit some closure construction
(such as the rational closure [39] and the lexicographic closure [40]) or some notion of
minimal entailment [6]. In the next section we strengthen ALCFT based on a closure
construction similar to the one in [24], but exploiting a weaker notion of coherence, and
we discuss its properties.
5
Strengthening ALCFT: A Closure Construction
To overcome the weakness of rational closure (as well as of preferential entailment),
Lehmann has introduced the lexicographic closure of a conditional knowledge base
[40] which strengthens the rational closure by allowing further inferences. From the
semantic point of view, in the propositional case, a preference relation is deﬁned on the
set of propositional interpretations, so that the interpretations satisfying conditionals
with higher rank are preferred to the interpretations satisfying conditionals with lower
rank and, in case of contradictory defaults with the same rank, interpretations satisfy-
ing more defaults with that rank are preferred. The ranks of conditionals used by the
lexicographic closure construction are those computed by the rational closure construc-
tion [39] and capture speciﬁcity: the higher is the rank, the more speciﬁc is the default.
In other cases, the ranks may be part of the knowledge base speciﬁcation, such as for
ranked knowledge bases in Brewka’s framework of basic preference descriptions [12],
or might be learned from empirical data.

On the KLM Properties of a Fuzzy DL with Typicality
565
In this section, we consider weighted (fuzzy) knowledge bases, where typicality
inclusions are associated to weights, and develop a (semantic) closure construction to
strengthen ALCFT entailment, which leads to a generalization of the notion of fuzzy
coherent multipreference model in [24]. The construction is related to the deﬁnition of
Kern-Isberner’s c-representations [35,36] which also include penalty points for falsiﬁed
conditionals.
A weighted ALCFT knowledge base K, over a set C = {C1, . . . , Ck} of dis-
tinguished ALC concepts, is a tuple ⟨Tf, TC1, . . . , TCk, Af⟩, where Tf is a set of
fuzzy ALCFT inclusion axiom, Af is a set of fuzzy ALCFT assertions and TCi =
{(di
h, wi
h)} is a set of all weighted typicality inclusions di
h = T(Ci) ⊑Di,h for Ci,
indexed by h, where each inclusion di
h has weight wi
h, a real number. As in [24], the
typicality operator is assumed to occur only on the left hand side of a weighted typ-
icality inclusion, and we call distinguished concepts those concepts Ci occurring on
the l.h.s. of some typicality inclusion T(Ci) ⊑D. Arbitrary ALCFT inclusions and
assertions may belong to Tf and Af.
Example 1. Consider the weighted knowledge base K
=
⟨Tf, TBird, TP enguin,
TCanary, Af⟩, over the set of distinguished concepts C = {Bird, Penguin, Canary},
with empty ABox and with Tf containing, for instance, the inclusions:
Y ellow ⊓Black ⊑⊥≥11
Y ellow ⊓Red ⊑⊥≥1
Black ⊓Red ⊑⊥≥1
The weighted TBox TBird contains the following weighted defeasible inclusions:
(d1) T(Bird) ⊑Fly, +20
(d2) T(Bird) ⊑∃has Wings.⊤, +50
(d3) T(Bird) ⊑∃has Feather.⊤, +50;
TP enguin and TCanary contain, respectively, the following defeasible inclusions:
(d4) T(Penguin) ⊑Bird, +100
(d7) T(Canary) ⊑Bird, +100
(d5) T(Penguin) ⊑Fly, - 70
(d8) T(Canary) ⊑Y ellow, +30
(d6) T(Penguin) ⊑Black, +50;
(d9) T(Canary) ⊑Red, +20
The meaning is that a bird normally has wings, has feathers and ﬂies, but having
wings and feather (both with weight 50) for a bird is more plausible than ﬂying (weight
20), although ﬂying is regarded as being plausible. For a penguin, ﬂying is not plau-
sible (inclusion (d5) has negative weight -70), while being a bird and being black are
plausible properties of prototypical penguins, and (d4) and (d6) have positive weights
(100 and 50, respectively). Similar considerations can be done for concept Canary.
Given Reddy who is red, has wings, has feather and ﬂies (all with degree 1) and Opus
who has wings and feather (with degree 1), is black with degree 0.8 and does not ﬂy
(FlyI(opus) = 0), considering the weights of defeasible inclusions, we may expect
Reddy to be more typical than Opus as a bird, but less typical than Opus as a penguin.
We deﬁne the semantics of a weighted knowledge base trough a semantic closure
construction, similar in spirit to Lehmann’s lexicographic closure [40], but more related
to c-representations and, additionally, based on multiple preferences. The construction
1 This
is
a
strong
requirement
which,
e.g.,
in
G¨odel
logic,
holds
in
I
only
if
(Y ellow ⊓Black)I(x) = 0, ∀x
∈
Δ. This suggests to be cautious when combining
fuzzy inclusions and defeasible inclusions in the same KB (e.g., Penguin ⊑Bird ≥1
would be too strong and conﬂicting with our interpretation of degree of membership as degree
of typicality).

566
L. Giordano
allows a subset of the ALCFT interpretations to be selected, the interpretations whose
induced preference relations <Ci, for the distinguished concepts Ci, faithfully represent
the defeasible part of the knowledge base K.
Let TCi = {(di
h, wi
h)} be the set of weighted typicality inclusions di
h = T(Ci) ⊑
Di,h associated to the distinguished concept Ci, and let I = ⟨Δ, ·I⟩be a fuzzy ALCFT
interpretation. In the two-valued case, we would associate to each domain element
x ∈Δ and each distinguished concept Ci, a weight Wi(x) of x wrt Ci in I, by summing
the weights of the defeasible inclusions satisﬁed by x. However, as I is a fuzzy interpre-
tation, we do not only distinguish between the typicality inclusions satisﬁed or falsiﬁed
by x; we also need to consider, for all inclusions T(Ci) ⊑Di,h ∈TCi, the degree of
membership of x in Di,h. Furthermore, in comparing the weight of domain elements
with respect to <Ci, we give higher preference to the domain elements belonging to
Ci (with a degree greater than 0), with respect to those not belonging to Ci (having
membership degree 0).
For each domain element x ∈Δ and distinguished concept Ci, the weight Wi(x)
of x wrt Ci in the ALCFT interpretation I = ⟨Δ, ·I⟩is deﬁned as follows:
Wi(x) =

h wi
h DI
i,h(x) if CI
i (x) > 0
−∞
otherwise
(4)
where −∞is added at the bottom of all real values.
The value of Wi(x) is −∞when x is not a C-element (i.e., CI
i (x) = 0). Otherwise,
CI
i (x) > 0 and the higher is the sum Wi(x), the more typical is the element x relative
to concept Ci. How much x satisﬁes a typicality property T(Ci) ⊑Di,h depends on
the value of DI
i,h(x) ∈[0, 1], which is weighted by wi
h in the sum. In the two-valued
case, DI
i,h(x) ∈{0, 1}, and Wi(x) is the sum of the weights of the typicality inclusions
for C satisﬁed by x, if x is a C-element, and is −∞, otherwise.
Example 2. Let us consider again Example 1. Let I be an ALCFT interpretation such
that FlyI(reddy) = (∃has Wings.⊤)I(reddy) = (∃has Feather.⊤)I(reddy) = 1
and RedI(reddy) = 1, i.e., Reddy ﬂies, has wings and feather and is red (and
BlackI(reddy) = 0). Suppose further that FlyI(opus) = 0 and (∃has Wings.⊤)I
(opus)
=
(∃has
Feather.⊤)I(opus)
=
1 and BlackI(opus)
=
0.8, i.e.,
Opus does not ﬂy, has wings and feather, and is black with degree 0.8. Considering
the weights of typicality inclusions for Bird, WBird(reddy) = 20+ 50 + 50 =
120 and WBird(opus)
=
0 + 50+ 50
=
100. This suggests that reddy
should be more typical as a bird than opus. On the other hand, if we suppose
BirdI(reddy) = 1 and BirdI(opus) = 0.8, then WP enguin(reddy) = 100 −70 +
0 = 30 and WP enguin(opus) = 0.8 × 100−0 + 0.8 × 50 = 120. This suggests that
reddy should be less typical as a penguin than opus.
We have seen in Sect. 3 that each fuzzy interpretation I induces a preference relation
for each concept and, in particular, it induces a preference <Ci for each distinguished
concept Ci. We further require that, if x <Ci y, then x must be more typical than y
wrt Ci, that is, the weight Wi(x) of x wrt Ci should be higher than the weight Wi(y)
of y wrt Ci (and x should satisfy more properties or more plausible properties of typ-
ical Ci-elements with respect to y). This leads to the following deﬁnition of a fuzzy
multipreference model of a weighted a ALCFT knowledge base.

On the KLM Properties of a Fuzzy DL with Typicality
567
Deﬁnition 4 (Faithful (fuzzy) multipreference model of K). Let K = ⟨Tf, TC1, . . . ,
TCk, Af⟩be a weighted ALCFT knowledge base over C. A faithful (fuzzy) multipref-
erence model (fm-model) of K is a fuzzy ALCFT interpretation I = ⟨Δ, ·I⟩s.t.:
– I satisﬁes the fuzzy inclusions in Tf and the fuzzy assertions in Af;
– for all Ci ∈C, the preference <Ci is faithful toTCi, that is:
x <Ci y ⇒Wi(x) > Wi(y)
(5)
Example 3. Referring to Example 2 above, where BirdI(reddy) = 1, BirdI(opus) =
0.8, let us further assume that PenguinI(reddy) = 0.2 and PenguinI(opus) = 0.8.
Clearly, reddy <Bird opus and opus <P enguin reddy. For the interpretation I
to be faithful, it is necessary that the conditions WBird(reddy) > WBird(opus) and
WP enguin(opus) > WP enguin(reddy) hold; which is true, as seen in Example 2. On
the contrary, if it were PenguinI(reddy) = 0.9, the interpretation I would not be
faithful.
Notice that, requiring that the converse of condition (5) also holds, gives the equiv-
alence x <Ci y iff Wi(x) > Wi(y), a stronger condition which would make the notion
of faithful multipreference model of K above coincide with the notion of coherent fuzzy
multipreference model of K introduced in [24]. Here, we have considered the weaker
notion of faithfulness and a larger class of fuzzy multipreference models of a weighted
knowledge base, compared to the class of coherent models. This allows a larger class of
monotone non-decreasing activation functions in neural network models to be captured
(for space limitation, for this result, we refer to [26], Sec. 7).
The notion of faithful multipreference entailment (fm-entailment) from a weighted
ALCFT knowledge base K can be deﬁned in the obvious way.
Deﬁnition 5 (fm-entailment). A fuzzy axiom E is fm-entailed from a fuzzy weighted
knowledge base K (K |=fm E) if, for all fm-models I = ⟨Δ, ·I⟩of K, I satisﬁes E.
From Proposition 2, the next corollary follows as a simple consequence.
Corollary 3. In Zadeh logic and in G¨odel logic, fm-entailment from a given knowledge
base K satisﬁes postulates (REFL”), (LLE”), (RW”), (AND”), (OR”) and (CM∗).
To conclude the paper let us infomally describe how fuzzy multipreference entailment
deals with irrelevance and avoids inheritance blocking, properties which have been con-
sidered as desiderata for preferential logics of defeasible reasoning [36,48].
For “irrelevance”, we have already considered an example: if typical birds ﬂy, we
would like to conclude that of typical yellow birds ﬂy, as the property of being yellow
is irrelevant with respect to ﬂying. Observe, that in Example 2, we can conclude that
Reddy is more typical than Opus as a bird (reddy <Bird Opus), as Opus does not ﬂy,
while Reddy ﬂies. The relative typicality of Reddy and Opus wrt Bird does not depend
on their color, and we would obtain the same relative preferences if reddy were yellow
rather than red. A formal proof of the irrelevance property would require the domain Δ
to be large enough to contain some typical bird which is yellow, requiring, as usual in
the two-valued case [32], a restriction to some canonical models.

568
L. Giordano
The fuzzy multipreference entailment is not subject to the problem called by Pearl
the “blockage of property inheritance” problem [44], and by Benferhat et al. the
“drowning problem” [4]. This problem affects rational closure and system Z [44], as
well as rational closure reﬁnements. Roughly speaking, the problem is that property
inheritance from classes to subclasses is not guaranteed. If a subclass is exceptional
with respect to a superclass for a given property, it does not inherit from that superclass
any other property. For instance, referring to the typicality inclusions in Example 2, in
the rational closure, typical penguins would not inherit the property of typical birds of
having wings, being exceptional to birds concerning ﬂying. On the contrary, in fuzzy
multipreference models, considering again Example 2, the degree of membership of a
domain element x in concept Bird, i.e., BirdI(x), is used to determine the weight of
x wrt Penguin (as the weight of typicality inclusion (d4) is positive. The higher is
the value of BirdI(x), the higher the value of WP enguin(x). Hence, provided the rel-
evant properties of penguins (such as non-ﬂying) remain unaltered, the more typical is
x as a bird, the more typical is x as a Penguin. Notice also that the weight WBird(x)
of a domain element x wrt Bird is related to the interpretation of Bird in I by the
faithfulness condition.
6
Conclusions
In this paper we have studied the properties of an extension of fuzzy ALC with typical-
ity, ALCFT. We have considered some alternative reformulation of the KLM postulates
of a preferential consequence relation for ALCFT, showing that most of these postu-
lates are satisﬁed, depending on the formulation considered and on the fuzzy logic com-
bination functions. We have considered a (semantic) closure construction to strengthen
ALCFT, by deﬁning a notion of faithful (fuzzy) multipreference model of a weighted
knowledge base. Faithful models of a conditional (weighted) knowledge base are a
more general class of models with respect to the coherent fuzzy multipreference models
considered in [24] to provide a semantic interpretation of multilayer perceptrons. This
allows us to capture the larger class of monotone non-decreasing activation functions in
multilayer perceptrons (a result which is not included in the paper due to space limita-
tions and for which we refer to [26]). The paper studies the KLM properties of faithful
multipreference entailment and discusses how the multipreference approach allows to
deal with irrelevance and avoids inheritance blocking.
For MLPs, the proposed semantics allows the input-output behavior of a deep net-
work (considered after training) to be captured by a fuzzy multipreference interpretation
built over a set of input stimuli, through a simple construction which exploits the activ-
ity level of neurons for the stimuli. Each unit h of N can be associated to a concept
name Ch and, for a given domain Δ of input stimuli, the activation value of unit h for a
stimulus x is interpreted as the degree of membership of x in concept Ch. The resulting
fm-interpretation can be used for verifying properties of the network by model check-
ing and it can be proven [24] to be a model of the conditional knowledge base KN
obtained, from the network N, by mapping synaptic connections to weighted condi-
tionals. This opens to the possibility of combining empirical knowledge and symbolic
knowledge in the form of DL axioms and motivates the study of the properties of this
multipreference extension of fuzzy DLs.

On the KLM Properties of a Fuzzy DL with Typicality
569
Undecidability results for fuzzy description logics with general inclusion axioms
[2,10,20] have motivated restricting the logics to ﬁnitely valued semantics [11], and
also motivate the investigation of decidable approximations of fm-entailment. An issue
is whether alternative (non-crisp) deﬁnitions of the typicality operator could be adopted,
inspired to fuzzy set based models of linguistic hedges [34]. Another issue is whether
the multipreference semantics can provide a semantic interpretation to other neural net-
work models, besides MLPs and Self-Organising Maps [37], for which a (two-valued)
multipreference semantics and a fuzzy semantics have been investigated in [29].
Acknowledgement. We thank the anonymous referees for their helpful comments and sugges-
tions. This research is partially supported by INDAM-GNCS Project 2020.
References
1. Baader, F., Calvanese, D., McGuinness, D., Nardi, D., Patel-Schneider, P.: The Description
Logic Handbook - Theory, Implementation, and Applications, 2nd edn. Cambridge Univer-
sity Press, Cambridge (2007)
2. Baader, F., Pe˜naloza, R.: Are fuzzy description logics with general concept inclusion axioms
decidable? In: FUZZ-IEEE 2011, IEEE International Conference on Fuzzy Systems, Taipei,
Taiwan, 27–30 June 2011, Proceedings, pp. 1735–1742. IEEE (2011)
3. Beierle, C., Falke, T., Kutsch, S., Kern-Isberner, G.: System ZFO: default reasoning with
system z-like ranking functions for unary ﬁrst-order conditional knowledge bases. Int. J.
Approx. Reason. 90, 120–143 (2017)
4. Benferhat, S., Dubois, D., Prade, H.: Possibilistic logic: from nonmonotonicity to logic pro-
gramming. In: Symbolic and Quantitative Approaches to Reasoning and Uncertainty, Euro-
pean Conference, ECSQARU 1993, Granada, Spain, 8–10 November 1993, Proceedings, pp.
17–24 (1993)
5. Bobillo, F., Straccia, U.: Reasoning within fuzzy OWL 2 EL revisited. Fuzzy Sets Syst. 351,
1–40 (2018)
6. Bonatti, P.A., Lutz, C., Wolter, F.: The complexity of circumscription in DLs. J. Artif. Intell.
Res. (JAIR) 35, 717–773 (2009)
7. Bonatti, P.A., Sauro, L.: On the logical properties of the nonmonotonic description logic
DLN. Artif. Intell. 248, 85–111 (2017)
8. Booth, R., Casini, G., Meyer, T., Varzinczak, I.: On rational entailment for propositional
typicality logic. Artif. Intell. 277, 103178 (2019)
9. Borgwardt, S., Distel, F., Pe˜naloza, R.: The limits of decidability in fuzzy description logics
with general concept inclusions. Artif. Intell. 218, 23–55 (2015)
10. Borgwardt, S., Pe˜naloza, R.: Undecidability of fuzzy description logics. In: Brewka, G.,
Eiter, T., McIlraith, S.A. (eds.) Principles of Knowledge Representation and Reasoning:
Proceedings of the Thirteenth International Conference, KR 2012, Rome, Italy, 10–14 June
2012. AAAI Press (2012)
11. Borgwardt, S., Pe˜naloza, R.: The complexity of lattice-based fuzzy description logics. J. Data
Semant. 2(1), 1–19 (2013)
12. Brewka, G.: A rank based description language for qualitative preferences. In: Proceedings
of the 16th Eureopean Conference on Artiﬁcial Intelligence, ECAI 2004, Valencia, Spain,
22–27 August 2004, pp. 303–307 (2004)
13. Britz, A., Varzinczak, I.: Contextual rational closure for defeasible ALC (extended abstract).
In: Proceedings of the 32nd International Workshop on Description Logics, Oslo, Norway,
18–21 June 2019 (2019)

570
L. Giordano
14. Britz, K., Heidema, J., Meyer, T.: Semantic preferential subsumption. In: Brewka, G., Lang,
J. (eds.) Principles of Knowledge Representation and Reasoning: Proceedings of the 11th
International Conference (KR 2008), Sidney, Australia, pp. 476–484. AAAI Press, Septem-
ber 2008
15. Britz, K., Varzinczak, I.J.: Rationality and context in defeasible subsumption. In: Proceed-
ings of the 10th International Symposium on Foundation of Information and Knowledge
Systems, FoIKS 2018, Budapest, 14–18 May 2018, pp. 114–132 (2018)
16. Casini, G., Meyer, T., Varzinczak, I.J., Moodley, K.: Nonmonotonic reasoning in description
logics: rational closure for the ABox. In: 26th International Workshop on Description Logics
(DL 2013). CEUR Workshop Proceedings, vol. 1014, pp. 600–615 (2013)
17. Casini, G., Straccia, U.: Rational closure for defeasible description logics. In: Janhunen, T.,
Niemel¨a, I. (eds.) JELIA 2010. LNCS (LNAI), vol. 6341, pp. 77–90. Springer, Heidelberg
(2010). https://doi.org/10.1007/978-3-642-15675-5 9
18. Casini, G., Straccia, U.: Lexicographic closure for defeasible description logics. In: Proceed-
ings of Australasian Ontology Workshop, vol. 969, pp. 28–39 (2012)
19. Casini, G., Straccia, U., Meyer, T.: A polynomial time subsumption algorithm for nominal
safe elo⊥under rational closure. Inf. Sci. 501, 588–620 (2019)
20. Cerami, M., Straccia, U.: On the undecidability of fuzzy description logics with GCIs with
Lukasiewicz t-norm. CoRR abs/1107.4212 (2011). http://arxiv.org/abs/1107.4212
21. Cintula, P., H´ajek, P., Noguera, C. (eds.): Handbook of Mathematical Fuzzy Logic, vol. 37–
38. College Publications (2011)
22. Delgrande, J.: A ﬁrst-order conditional logic for prototypical properties. Artif. Intell. 33(1),
105–130 (1987)
23. Delgrande, J., Rantsoudis, C.: A preference-based approach for representing defaults in ﬁrst-
order logic. In: 18th International Workshop on Non-monotonic Reasoning, NMR 2020,
Workshop Notes, 12th–14th September 2020 (2020)
24. Giordano, L., Theseider Dupr´e, D.: Weighted defeasible knowledge bases and a multipref-
erence semantics for a deep neural network model. In: Faber, W., Friedrich, G., Gebser, M.,
Morak, M. (eds.) JELIA 2021. LNCS (LNAI), vol. 12678, pp. 225–242. Springer, Cham
(2021). https://doi.org/10.1007/978-3-030-75775-5 16
25. Giordano, L., Dupr´e, D.T.: An ASP approach for reasoning in a concept-aware multiprefer-
ential lightweight DL. Theory Pract. Log. Program. 20(5), 751–766 (2020)
26. Giordano, L., Dupr´e, D.T.: Weighted defeasible knowledge bases and a multipreference
semantics for a deep neural network model. CoRR abs/2012.13421 (2020). https://arxiv.org/
abs/2012.13421v2
27. Giordano, L., Gliozzi, V.: Encoding a preferential extension of the description logic
SROIQ into SROIQ. In: Esposito, F., Pivert, O., Hacid, M.-S., Ra´s, Z.W., Ferilli, S.
(eds.) ISMIS 2015. LNCS (LNAI), vol. 9384, pp. 248–258. Springer, Cham (2015). https://
doi.org/10.1007/978-3-319-25252-0 27
28. Giordano, L., Gliozzi, V.: A reconstruction of multipreference closure. Artif. Intell. 290
(2021). https://doi.org/10.1016/j.artint.2020.103398
29. Giordano, L., Gliozzi, V., Dupr´e, D.T.: On a plausible concept-wise multipreference seman-
tics and its relations with self-organising maps. In: Calimeri, F., Perri, S., Zumpano, E. (eds.)
CILC 2020, Rende, Italy, 13–15 October 2020. CEUR Workshop Proceedings, vol. 2710, pp.
127–140 (2020). an extended version in CoRR abs/2103.06854, https://arxiv.org/abs/2103.
06854
30. Giordano, L., Gliozzi, V., Olivetti, N., Pozzato, G.L.: Preferential description logics. In:
Dershowitz, N., Voronkov, A. (eds.) LPAR 2007. LNCS (LNAI), vol. 4790, pp. 257–272.
Springer, Heidelberg (2007). https://doi.org/10.1007/978-3-540-75560-9 20
31. Giordano, L., Gliozzi, V., Olivetti, N., Pozzato, G.L.: ALC+T: a preferential extension of
description logics. Fund. Inform. 96, 1–32 (2009)

On the KLM Properties of a Fuzzy DL with Typicality
571
32. Giordano, L., Gliozzi, V., Olivetti, N., Pozzato, G.L.: Semantic characterization of rational
closure: from propositional logic to description logics. Artif. Intell. 226, 1–33 (2015)
33. Haykin, S.: Neural Networks - A Comprehensive Foundation. Pearson (1999)
34. Huynh, V., Ho, T.B., Nakamori, Y.: A parametric representation of linguistic hedges in
Zadeh’s fuzzy logic. Int. J. Approx. Reason. 30(3), 203–223 (2002)
35. Kern-Isberner, G. (ed.): Conditionals in Nonmonotonic Reasoning and Belief Revision.
LNCS (LNAI), vol. 2087. Springer, Heidelberg (2001). https://doi.org/10.1007/3-540-
44600-1
36. Kern-Isberner, G., Eichhorn, C.: Structural inference from conditional knowledge bases.
Stud. Logica. 102(4), 751–769 (2014)
37. Kohonen, T., Schroeder, M., Huang, T. (eds.): Self-Organizing Maps, 3rd edn. Springer
Series in Information Sciences. Springer, Heidelberg (2001). https://doi.org/10.1007/978-
3-642-56927-2
38. Kraus, S., Lehmann, D., Magidor, M.: Nonmonotonic reasoning, preferential models and
cumulative logics. Artif. Intell. 44(1–2), 167–207 (1990)
39. Lehmann, D., Magidor, M.: What does a conditional knowledge base entail? Artif. Intell.
55(1), 1–60 (1992). https://doi.org/10.1016/0004-3702(92)90041-U
40. Lehmann, D.J.: Another perspective on default reasoning. Ann. Math. Artif. Intell. 15(1),
61–82 (1995)
41. Lukasiewicz, T., Straccia, U.: Description logic programs under probabilistic uncertainty and
fuzzy vagueness. Int. J. Approx. Reason. 50(6), 837–853 (2009)
42. Makinson, D.: General theory of cumulative inference. In: Non-monotonic Reasoning, 2nd
International Workshop, Grassau, FRG, 13–15 June 1988, Proceedings, pp. 1–18 (1988)
43. Pearl, J.: Probabilistic Reasoning in Intelligent Systems Networks of Plausible Inference.
Morgan Kaufmann, Burlington (1988)
44. Pearl, J.: System Z: a natural ordering of defaults with tractable applications to nonmonotonic
reasoning. In: Proceedings of the 3rd Conference on Theoretical Aspects of Reasoning about
Knowledge (TARK 1990), Paciﬁc Grove, CA, USA, March 1990, pp. 121–135. Morgan
Kaufmann (1990)
45. Pensel, M., Turhan, A.: Reasoning in the defeasible description logic EL⊥- computing
standard inferences under rational and relevant semantics. Int. J. Approx. Reasoning 103,
28–70 (2018)
46. Stoilos, G., Stamou, G.B., Tzouvaras, V., Pan, J.Z., Horrocks, I.: Fuzzy OWL: uncertainty
and the semantic web. In: Proceedings of the OWLED*05 Workshop on OWL: Experiences
and Directions, Galway, Ireland, 11–12 November 2005. CEUR Workshop Proceedings, vol.
188. CEUR-WS.org (2005). http://ceur-ws.org/Vol-188/sub16.pdf
47. Straccia, U.: Towards a fuzzy description logic for the semantic web (preliminary report). In:
G´omez-P´erez, A., Euzenat, J. (eds.) ESWC 2005. LNCS, vol. 3532, pp. 167–181. Springer,
Heidelberg (2005). https://doi.org/10.1007/11431053 12
48. Weydert, E.: System JLZ - rational default reasoning by minimal ranking constructions. J.
Appl. Log. 1(3–4), 273–308 (2003)

Probability Logics

Trust Evidence Logic
Alessandro Aldini1(B), Gianluca Curzi2, Pierluigi Graziani1,
and Mirko Tagliaferri1
1 University of Urbino Carlo Bo, Urbino, Italy
{alessandro.aldini,pierluigi.graziani,mirko.tagliaferri}@uniurb.it
2 University of Birmingham, Birmingham, UK
g.curzi@bham.ac.uk
Abstract. We investigate the application of a modal language `a la
Hennessy-Milner to the speciﬁc domain of evidence-based trust estima-
tions. In particular, we refer to a context-aware notion of computational
trust joining in a quantitative setting both assessment of subjective opin-
ions and third-party recommendations. Moreover, for a comprehensive
analysis of the proposed logics, we oﬀer an axiomatization and provide
soundness and completeness results.
Keywords: Probabilistic modal logic · Trust · Completeness
1
Introduction
Trust fosters cooperation [14,27] and it does so without requiring complex and
expensive infrastructures [33]. Moreover, trust allows this cooperation to emerge
in systems characterized by uncertainty [4], by decreasing the complexity of social
environments [25] and, thus, promoting actions even when data (or time) is not
suﬃcient to perform a thorough analysis of the possible outcomes of said actions.
Those facts suggest that trust is an extremely important factor in environments
where social interactions take place. Henceforth, the relation between trust and
online environments has been object of study [20,30]. Online environments have
become important parts of our daily life: we book hotels through websites, we
socialize with other people through social media, etc. Basically everything that
we used to do in the physical world until thirty years ago, can now, potentially,
be done online. Moreover, the increased possibility to interact online has become
important also for technologies (e.g., smart homes sensors), which can exchange
data and create huge networks of complementary services, given birth to what
is now called the Internet of Things (IoT) [11].
Since this shift from interactions in the physical world to interactions in
online environments gradually increased over time, formal approaches have been
proposed to explore trust from a logical perspective, which help computer scien-
tists develop better trust infrastructures that are inserted in the design of various
interacting scenarios [1,2,7,23,29,31].
c
⃝Springer Nature Switzerland AG 2021
J. Vejnarov´a and N. Wilson (Eds.): ECSQARU 2021, LNAI 12897, pp. 575–589, 2021.
https://doi.org/10.1007/978-3-030-86772-0_41

576
A. Aldini et al.
The aim of this paper is to show that by using classical ingredients of modal
languages, it is possible to deﬁne a logical framework in order to build and
analyze context-aware trust relations based on possessed evidence. The resulting
language, called Trust Evidence Logic (TEL), includes modalities, inspired by
logics `a la Hennessy-Milner [17], used to express estimations about trust towards
logical formulas.
In Sect. 2, motives that inspired our approach are provided, hinting at alter-
native solutions and connections between them. In Sect. 3, the syntax and seman-
tics of TEL are introduced and model theoretic results are given. In Sect. 4, TEL
is interpreted to study a context-aware computational notion of trust based on
evidence. In Sect. 5, soundness and completeness results for TEL are proved,
and in Sect. 6, conclusions and future works follow.
2
Background and Motivations
A prominent conception of trust is that of Gambetta [14]. According to his deﬁni-
tion of trust, when Alice (the trustor) trusts Bob (the trustee), Alice subjectively
attributed a suﬃciently high probability to the possibility that Bob will act in
a beneﬁcial way towards her. In this sense, what seems relevant for the presence
of trust is that Alice is able to have enough information to form a subjective
evaluation on the probability that Bob will act in a certain way. The kinds of
information that Alice will seek are context dependant and might change in dif-
ferent scenarios and/or time frames, i.e., when and in which circumstances Alice
is evaluating Bob’s actions.
One of the main solutions to implement such form of trust in online environ-
ments is to employ reputation models [18,19]. The main advantage of reputation
models is that they are well-structured to deal with indirect evidence of behavior
and they provide a solid base for trust evaluations. Each user gives a score based
on his/her personal evaluation and, from there, the reputation model computes
a uniﬁed value that could be provided to the trustor to form his/her subjective
evaluation. Given that reputation models only need to manipulate data pro-
vided directly by users, they are easy to implement in online environments and
thus are widely employed as evidence-bases for trust evaluations in computer
science. Assuming a reputation model where the users can only provide Boolean
evaluations for certain behaviors, e.g., the behavior is present or is absent, it
would be easy to represent such reputation model through the use of graded
modal logics (GML) [12,13,15]. In GML, the modal operator ♦nφ speciﬁes that
in strictly more than n accessible states of the system φ holds. Hence, by inter-
preting states as agents and φ as the evaluated behavior, the modal operator
can be adapted to decide whether a given number of evaluations are present and

Trust Evidence Logic
577
from there, provide an estimation of trust1. Taking the intuition behind GML
as a starting point, two important elements can be added in order to improve
the expressiveness. The ﬁrst element, call it a, is a parameter indicating the con-
text of evaluation. In this case, an evaluation is not a general assessment, but
strictly depends on a speciﬁc scenario that indicates in which circumstances the
evaluation is made. The second element is a numeric parameter indicating the
expertise of the evaluator, i.e., it establishes how much weight the trustor places
on the evaluations of other agents. In this sense, not all evaluations are judged
equally, but some will be more relevant than others. Those two additions produce
a language (TEL) that encompasses some features from both the probabilistic
version of Hennessy-Milner logic (HML), see for instance [22], and the Prob-
abilistic Computation Tree Logic (PCTL) [16]. In particular, TEL introduces
modal formulas with shape ⟨a⟩≥
pφ, where p denotes the evaluation threshold used
to govern trust-based decisions, and taken as a rational (to keep the language
countable) from the interval [0, 1].
Following both [22] and [16], the semantics of TEL is deﬁned on top of proba-
bilistic labelled state-transition systems, which generalize Kripke semantics and
allow for a straightforward logical characterization of bisimulation. Moreover,
we extend soundness and completeness results of modal languages with graded
modalities to our setting.
3
Syntax and Semantics of TEL
We start introducing the language of Trust Evidence Logic (TEL).
Deﬁnition 1 (Language of TEL). Let At be a countable set of propositional
atoms ranging over α, β, γ, . . ., and let A be a countable set of labels ranging
over a, b, c, . . .. The language LTEL is generated by the following grammar:
φ ::= ⊤| α | ¬φ | φ ∨φ | ⟨a⟩
≥
pφ
(1)
where α ∈At, a ∈A, and p ∈Q[0,1]. The elements of LTEL are called formulas,
and range over φ, ψ. As usual, we deﬁne ⊥≜¬⊤, φ ∧ψ ≜¬(¬φ ∨¬ψ) and
φ →ψ ≜¬φ ∨ψ. Moreover, we deﬁne the following modal operators for all
a ∈A and all p ∈Q[0,1]:
⟨a⟩
>
pφ ≜¬⟨a⟩
≥
1−p¬φ
[a]
≤
pφ ≜⟨a⟩
≥
1−pφ
[a]
<
pφ ≜⟨a⟩
>
1−pφ
(2)
also illustrated in Fig. 1, and we set ⟨a⟩=
pφ ≜⟨a⟩≥
pφ ∧¬⟨a⟩>
pφ.
1 An alternative approach is to employ majority logics [26]. Such logics are well suited
to deal with dynamic scenarios where the number of evaluations is not ﬁxed. The
advantage of employing majority logics instead of GML is given by the fact that
majority logics only specify that the (strict) majority of evaluations must be positive,
without specifying a given number of those. On the contrary, in GML, this number
must always be speciﬁed.

578
A. Aldini et al.
Deﬁnition 2 (PLSTS). A Probabilistic Labelled State-Transition System is a
tuple M = (S, At, A, {Da}a∈A, v), where S is a non-empty countable set of states,
At is the countable set of state labels, A is the countable set of transition labels,
v is a valuation function v : S →℘(At), and {Da}a∈A is a family of probabilistic
transition functions of the form Da : S × S →[0, 1] satisfying the following
condition:
∀s ∈S :

t∈S
Da(s, t) = 1.
(3)
If X ⊆S, we let Da(s, X) denote 
t∈X Da(s, t).
Fig. 1. Dualities between modalities, where ¬ is the logical negation, and ∼is the
involutive function on Q[0,1] deﬁned by ∼p = 1 −p.
PLSTSs generalize Kripke models. It is then natural to look for a suitable
notion of “frame” in the above extended setting. This will allow us to deﬁne
classes of frames and to provide axiomatic characterizations for them, as in
standard modal logic.
Deﬁnition 3 (Frames). Let M = (S, At, A, {Da}a∈A, v) be a PLSTS. The frame
of M, written FM, is a pair (S, {Ra}a∈A) where each Ra is called accessibility
relation and is deﬁned for any s, s′ ∈S as:
(s, s′) ∈Ra
iﬀ
Da(s, s′) > 0.
In this case we also say that M is based on FM. Frames range over F. We deﬁne
K, T , B and K4 as, respectively, the class of all frames, the class of all reﬂexive
frames, the class of all symmetric frames, and the class of all transitive frames.
Moreover, S4 and S5 denote the class of all frames whose relation is, respectively,
a preorder and an equivalence.
Remark 1. Since the total mass of each distribution Da(s, ) is equal to 1, the
accessibility relation Ra is serial, i.e. ∀s ∈S. ∃s′ ∈S. (s, s′) ∈Ra. This means
that every frame is serial.
In the semantics of TEL, formulas will be interpreted over PLSTSs. In partic-
ular, ⟨a⟩≥
pφ will be true at a state s if the sum of the weights that the distribution
Da(s, ) associates with the states in which φ is true is ≥p.

Trust Evidence Logic
579
Deﬁnition 4 (Truth). Let φ ∈LTEL and M = (S, At, A, {Da}a∈A, v) be a
PLSTS. We inductively deﬁne the notion of φ being satisﬁed (or true) at state
s ∈S in M, written s |=M φ, as follows:
(a) s |=M ⊤iﬀtrue;
(b) s |=M α iﬀα ∈v(s), where α ∈At;
(c) s |=M ¬φ iﬀs ̸|=M φ;
(d) s |=M φ ∨ψ iﬀs |=M φ or s |=M ψ;
(e) s |=M ⟨a⟩≥
pφ iﬀDa(s, Sφ) ≥p, where p ∈Q[0,1] and:
Sφ ≜{s′ ∈S | s′ |=M φ}.
(4)
In this case, we also say that φ is satisﬁable in M. We say that φ is true in
M, written |=M φ, when s |=M φ holds for every s ∈S; we say that φ is true,
written |= φ, when |=M φ holds for every PLSTS M. A set of formulas Γ is
satisﬁed (or true) at state s ∈S
in M, written s |=M Γ, if s |=M φ for any
φ ∈Γ. In this case we also say that Γ is satisﬁable in M. We say that Γ is true
in M, written |=M Γ, when s |=M Γ holds for any φ ∈Γ and any s ∈S.
Deﬁnition 5 (Validity). Let φ ∈LTEL and let F be a frame. We say that φ is
valid at a state s ∈S
in F, written s |=F φ, if s |=M φ for every PLSTS M
based on F (i.e. FM = F). We say that φ is valid in F, written |=F φ, if φ is valid
in F at any state s ∈S. Finally, we say that a formula φ is valid on a class of
frames F if it is valid in every frame F ∈F. The notion of validity can be easily
extended to sets of formulas.
The following are straightforward properties of Deﬁnition 4.
Proposition 1. Let M = (S, At, A, {Da}a∈A, v) be a PLSTS. For every s ∈S,
a ∈A, and for every φ, ψ ∈LTEL:
1. Da(s, S¬φ) = 1 −Da(s, Sφ);
2. Da(s, Sφ∨ψ) + Da(s, Sφ∧ψ) ≥Da(s, Sφ) + Da(s, Sψ).
Using Deﬁnition 4 and Proposition 1, we can assign truth-conditions to the
alternative modalities introduced in (2):
Proposition 2. Let φ ∈LTEL and M = (S, At, A, {Da}a∈A, v) be a PLSTS.
Then, for all s ∈S:
– s |=M [a]▷◁
pφ iﬀDa(s, S¬φ) ▷◁p, where p ∈Q[0,1] and ▷◁∈{<, ≤};
– s |=M ⟨a⟩▷◁
pφ iﬀDa(s, Sφ) ▷◁p, where p ∈Q[0,1] and ▷◁∈{>, =}.
where Sφ is as in (4).
As TEL shares features with both PCTL and probabilistic HML, it inherits
some of their semantic properties. A remarkable example is the logical charac-
terization of bisimulation, whose standard deﬁnition is reported below.

580
A. Aldini et al.
Deﬁnition 6 (Bisimulation). Let M = (S, At, A, {Da}a∈A, v) be a PLSTS. An
equivalence relation B over S is a bisimulation if and only if whenever (s, t) ∈B
it holds that v(s) = v(t) and ∀a ∈A, ∀C ∈S/B:

s′∈C
Da(s, s′) =

t′∈C
Da(t, t′).
As usual, we say that two states s and t in S are bisimilar, denoted s ∼t,
if there exists a bisimulation B on S such that sBt; two states s and t in S are
logically equivalent, denoted s ≡t, if and only if they satisfy exactly the same
formulas of LTEL.
By adapting to PLSTSs the results in [6] we obtain the following theorem:2
Theorem 3 (Logical characterization of bisimulation).
For any PLSTS, ∼
coincides with ≡.
4
Modeling Trust in TEL
Since their introduction, the logical frameworks induced by the probabilistic
extensions of modal logics like HML and CTL, have been employed to compare
and study the properties of probabilistic systems. In such a setting, each state of
the underlying semantic model represents a system conﬁguration, characterized
by a set of atomic predicates expressing the statements that hold in the state.
On the other hand, each transition is enriched with a label expressing the action
that is executed through the transition and with probabilistic information used
to determine quantitatively the behavior associated to the action. In this view,
model checking algorithms are employed to verify properties of probabilistic
systems in terms of satisﬁability of modal logic formulas [5,21].
In this section, we show how to provide a speciﬁc interpretation of PLSTSs
in order to reason about context-aware evidence-based trust.
In the formal setting of PLSTSs, we assume that every state represents an
agent of a social network of connected agents. The atomic propositions in At
labeling the state associated with an agent represent the evidences that the
agent believes to be true. The information associated with a connection from
agent s to agent s′ enables the trust estimation towards formulas, based on
such evidence. More precisely, the transition label a ∈A represents the context
in which trust is estimated, while Da(s, s′) represents the normalized level of
expertise of agent s′ as perceived by agent s with respect to the given context a.
Two further considerations about our interpretation are necessary. Firstly,
we are in a classical logical setting, thus for any formula φ and agent s, either s
states that φ or it states that ¬φ. So every agent is forced to have a belief about
φ. This is not a limitation for reputation models, where all involved agents have
2 In [10] it is shown that disjunction can be discarded (and in [8] that, as an alter-
native to disjunction, conjunction can be discarded), without changing the logical
characterization result, which still holds when we move from rational numbers to
real numbers.

Trust Evidence Logic
581
already expressed their belief about φ. Nonetheless, the probabilistic transition
functions can be used to rule out agents’ beliefs, by assigning weight 0 to them.
In practice, Da(s, s′) = 0 expresses that agent s does not consider agent s′ for
trust estimations about a, for various possible reasons, e.g., s′ is not accessible
to s, or else s is not interested in the beliefs of s′.
Secondly, notice that (3) imposes that the sum of the evaluations for the
expertise of the diﬀerent agents as perceived by any agent s is equal to 1. This
suggests that s will always have a claque of agents that he evaluates as experts
with respect to the given context a. However, the function Da could be reﬂexive,
allowing an agent to take into consideration also his-own expertise. In a limiting
scenario there might be contexts in which the only relevant expertise is the one
of the evaluating agent s, i.e., Da(s, s) = 1.
Then, the modal operator ⟨a⟩≥
pφ expresses that φ is subject to a trust esti-
mation in the context of label a, so that the evaluation of such a modal formula
for a given agent says whether the agent trusts φ or not with respect to a given
trustworthiness threshold p. Given this interpretation of PLSTSs, we can deﬁne
the notions of trust and distrust.
Deﬁnition 7 (Trust and distrust). For each φ ∈LTEL, a ∈A, and p ∈Q[0,1],
we deﬁne trust as T a
p φ to stand for the modality ⟨a⟩>
pφ and distrust as Da
pφ to
stand for the modality [a]<
1−p¬φ. Similarly, we deﬁne weak trust (wT a
p φ) as ⟨a⟩≥
pφ
and weak distrust (wDa
pφ) as [a]
≤
1−p¬φ.
Notice that, by (2), trusting φ with respect to a and p corresponds to dis-
trusting ¬φ with respect to a and p. To emphasize the nature of the duality
between trust and distrust, we point out that for every p ∈Q[0.5,1], it holds
that T a
p φ ∧Da
pφ is not satisﬁable for any state of every PLSTS, i.e., a state-
ment cannot be trusted and distrusted at the same time with respect to a given
context and a threshold expressing at least weak majority. However, uncertainty
is admissible in the case of lower thresholds, which could make both trust and
distrust satisﬁable. The same result holds for the weak notions provided that
p ∈Q]0.5,1]; indeed p = 0.5 admits the satisﬁability of wT a
p φ ∧wDa
pφ.
Fig. 2. PLSTS example, where G := Gianluca, P := Pierluigi, M := Mirko, A :=
Alessandro, c := restaurants, b := cybersecurity, φ := Ciacci is renowned for its gas-
tronomy, ψ := Ciacci oﬀers the best value for money in the whole of Urbino, θ := TLS
1.3 is secure. Let G |= ¬θ, P |= φ ∧ψ, M |= φ ∧¬ψ, and A |= ¬φ ∧¬ψ ∧θ.
On the other hand, we have that T a
p φ ∨Da
pφ is not true. Indeed, as a coun-
terexample consider a PLSTS graphically represented as follows:

582
A. Aldini et al.
s
s′ φ
s′′ ¬φ
a
0.5
a
0.5
where φ holds in s′ but not in s′′. We have that neither s |= T a
1
2 φ nor s |= Da
1
2 φ
hold. The special case p = 0.5 illustrated for the weak notions reveals that the
counterexample above does not work for weak trust/distrust.
We now show a toy example illustrating how this general setting works to
model a notion of context-aware evidence-based trust.
Example 1. Figure 2 shows a PLSTS. Here, the labels c and b are contexts,
which identify a speciﬁc situation where trust of an agent towards a given state-
ment is considered.
For instance, concerning the label c, we have that G |= wT c
0.75φ and
G |= Dc
0.5ψ. We can then say that G weakly trusts φ with respect to context c
and threshold 0.75 because the sum of the weights associated with the agents
accessible through c for which φ holds overtakes the trustworthiness threshold
0.75. Similarly, G distrusts ψ with respect to context c and threshold 0.5 because
the sum of the weights associated with the agents accessible through c for which
¬ψ holds strictly overtakes the threshold 0.5. Notice that G, in the context c,
is not taking into account his own belief about φ, e.g., because he is completely
ignorant about φ.
Concerning the label b, we recall that G |= ¬θ; however it holds that
G |= wT b
0.75θ. This means that G weakly trusts θ with respect to context b
and threshold 0.75 even if G states that ¬θ, because the weight G assigns to A is
far greater than the weight G assigns to himself. The fact that G evaluates A’s
belief more than his-own in context b emphasizes that in our framework s |=M φ
must be read as agent s believes that φ holds. However, beliefs are subjective
and could be subverted by other agents’ beliefs.
It is interesting to notice the trust-theoretical meaning of bisimulation, i.e.,
what does it imply for two agents to be bisimilar in the setting of trust? Thanks
to Theorem 3, two bisimilar agents trust the same formulas. More in-depth,
recalling the two conditions of Deﬁnition 6, two agents s and t belonging to the
same class of equivalence share the same evidence (v(s) = v(t)) and, for each
context, express the same trustworthiness towards every class of agents induced
by bisimulation. Reasoning at the level of classes implies two facts. First, s and
t could even interact with diﬀerent agents and be, anyway, bisimilar. Second,
as bisimulation induces aggregation of equivalent states, every agent may treat
each class of agents as a unique entity whose belief is evaluated to some degree.
5
Soundness and Completeness
This section is about soundness and completeness for various trust evidence nor-
mal modal logics (TENML), the “normal” modal logics for TEL. In particular,

Trust Evidence Logic
583
we shall focus on the systems TEK, TET, TEB, TEK4, TES4 and TES5, which can
be seen as quantitative and “serial” extensions of the standard modal logics K,
T, B, K4, S4 and S5, respectively (Proposition 6). Each such TENML is proven
both sound and complete with respect to all those PLSTSs that are based on
a speciﬁc class of frames. This requires to establish a version of the canonical
model theorem for TEL (Theorem 9), the main result of this section. Finally,
we introduce a counterexample to compactness and we use it to infer the failure
of strong completeness in our framework.
The proof of the canonical model theorem follows the lines of [9], adapting the
techniques to a quantitative setting, while the counterexample to compactness
is taken from [32].
5.1
Soundness
We start deﬁning the notion of trust evidence normal modal logic.
Deﬁnition 8 (TENML). A trust evidence normal modal logic (TENML) is a set
Λ ⊆LTEL containing (i) all propositional tautologies, (ii) all the substitution
instances of the following axiom schemata, for φ, ψ ∈LTEL, a ∈A and p, q ∈
Q[0,1]:
1. ⟨a⟩>
pφ →⟨a⟩≥
pφ ;
2. ⟨a⟩≥
pφ →⟨a⟩>
qφ
with q < p;
3. ⟨a⟩
≥
0φ
4. ⟨a⟩
≥
1(φ →ψ) →⟨a⟩≥
pφ →⟨a⟩≥
pψ;
5. ⟨a⟩
≥
1¬(φ ∧ψ) →(⟨a⟩≥
pφ ∧⟨a⟩≥
qψ) →⟨a⟩
≥
p+q(φ ∨ψ)
with p + q ≤1;
and (iii) closed under modus ponens (MP) and necessitation (NECa with a ∈A):
φ
⟨a⟩
≥
1φ
NECa
The above axioms state fundamental properties about Da(s, Sφ) (see (4)).
Axiom 1 and Axiom 2 formalize properties about inequalities. Axiom 3 and its
dual formulation [a]
≤
1φ (see (2)) state that Da(s, Sφ) is always a rational in [0, 1].
Axiom 4 allows modalities to distribute over implication, and it can be seen as
a generalization of (K), i.e. □(φ →ψ) →□φ →□ψ. Finally, Axiom 5 describes
compositionality for ∨, which holds whenever both disjuncts are “incompatible”.
We shall write ⊢Λ φ, or simply ⊢φ (when no confusion arises), when φ ∈Λ. If
Γ is a set of formulas, we write Γ ⊢Λ φ if either ⊢Λ φ or there are ψ1, . . . , ψn ∈Γ
such that ⊢Λ (ψ1∧. . .∧ψn) →φ. Finally, we say that Γ is Λ-consistent, or simply
consistent (when no confusion arises), when Γ ̸⊢Λ⊥. A set of formulas Γ is
maximal Λ-consistent, or simply maximal consistent (when no confusion arises),
if Γ is Λ-consistent and any Γ ′ properly containing Γ is not Λ-consistent. We
shall often refer to Γ as a mc-set, and the set of all mc-sets is MAX .
The following proposition states some basic properties about Λ.

584
A. Aldini et al.
Proposition 4. Let φ, ψ ∈LTEL and p, q ∈Q[0,1]:
1. If ⊢φ →ψ then ⊢⟨a⟩▷◁
pφ →⟨a⟩▷◁
pψ, where ▷◁∈{≥, >, =};
2. If ⊢φ ↔ψ then ⊢⟨a⟩▷◁
pφ ↔⟨a⟩▷◁
pψ, where ▷◁∈{≥, >, =};
3. If q < p then ⊢⟨a⟩≥
pφ →⟨a⟩≥
qφ;
4. ⊢⟨a⟩=
0(φ ∧ψ) →(⟨a⟩=
pφ ∧⟨a⟩=
qψ) →⟨a⟩=
p+q(φ ∨ψ).
As usual, there is a smallest TENML Λ containing a given set of formulas
Γ, called the TENML axiomatized by Γ. When Γ = ∅, Λ will be written TEK.
In analogy with standard modal logic, we consider the following axioms:
(T+) = ⟨a⟩
≥
1φ →φ
(B+) = φ →⟨a⟩
≥
1⟨a⟩
>
0φ
(4+) = ⟨a⟩
≥
1φ →⟨a⟩
≥
1⟨a⟩
≥
1φ
We shall work with extensions of TEK that combine the above axioms. In par-
ticular, TET, TEB, TEK4 are obtained by adding to TEK, respectively, (T+),
(B+), and (4+). Moreover, TES4 extends TEK with both (T+) and (4+), and
TES5 extends TES4 with (B+).
The following theorem shows a series of soundness results relating the above
deﬁned TENLMs and classes of frames. In particular, TEK turns out to be sound
w.r.t. all PLSTSs.
Theorem 5 (Soundness for various TENLMs). For any φ ∈LTEL:
⊢TEK φ implies |=K φ;
⊢TET φ implies |=T φ;
⊢TEB φ implies |=B φ;
⊢TEK4 φ implies |=K4 φ;
⊢TES4 φ implies |=S4 φ; ⊢TES5 φ implies |=S5 φ.
In Remark 1 we stressed that any frame is serial by construction. This condi-
tion is formalized by Axiom 2, which can be seen as a quantitative generalization
of Axiom (D), i.e. □φ →♦φ. To see this, let us ﬁx A = {∗} in (1), and let us
assume that the parameters p, q are taken from {0, 1}. It is not diﬃcult to see
that, by setting □φ ≜⟨∗⟩
≥
1φ, Axioms 1–5 deﬁne exactly the serial normal modal
logic D. In particular, Axiom 2 collapses to (D). More formally, we have:
Proposition 6. TEK is a conservative extension of the normal modal logic D.
This observation can be also found in [12], where a variant of Axiom 2 is
considered. Let us ﬁnally notice that we could avoid seriality by deﬁning each
Da(s, ) as a sub-distribution, i.e. a distribution whose mass can be smaller
than 1. However, turning the equality in (3) into an inequality would break the
equation in Proposition 1.1 and, as a consequence, the logical dualities in Fig. 1.
5.2
Completeness
In this subsection we sketch the proof of the canonical model theorem for TEL,
from which we infer a series of completeness results for TEK, TET, TEB, TEK4,
TES4 and TES5. The canonical model for TEL can be obtained by adapting the
one for graded modal logics (GML) [9] to a quantitative setting.

Trust Evidence Logic
585
Deﬁnition 9 (Canonical model). Given any consistent TENML Λ, we deﬁne
its canonical model MΛ = ⟨SΛ, AtΛ, AΛ, {DΛ
a }a∈AΛ, vΛ⟩as follows:
– SΛ is the set MAX of all maximally consistent sets;
– AtΛ, AΛ are the sets of state and transition labels of LTEL;
– for any a ∈AΛ and for any Γ, Δ ∈SΛ, we set:
DΛ
a (Γ, Δ) = min{p ∈Q[0,1] | ⟨a⟩
=
pφ ∈Γ, φ ∈Δ};
(5)
– for any Γ ∈SΛ, vΛ(Γ) = {α ∈AtΛ | α ∈Γ}.
In [9], De Caro showed a key property relating the canonical model for GML
to graded modalities ♦n (n ∈N): given a state of this model, i.e. a maximally
consistent set Γ, ♦nφ ∈Γ iﬀthe number of mc-sets containing φ that are
“accessible” from Γ is strictly greater than n. In a similar way, by essentially
transplanting De Caro’s proof techniques into TEL, we obtain the following:
Lemma 7. Let Γ0 ∈SΛ, φ ∈LTEL, a ∈AΛ, and let p ∈Q[0,1]. Then:
DΛ
a (Γ0, {Γ ∈SΛ | φ ∈Γ}) ≥p ⇐⇒⟨a⟩
≥
pφ ∈Γ0
Before stating the Truth Lemma we show that the construction of MΛ yields
a PLSTS. This fact follows by proving that the functions DΛ
a (Γ, ) are actually
probabilistic distributions, and can be established by means of two fundamental
properties of the maximally consistent sets:
1. For any Γ ∈SΛ and φ ∈LTEL there exists exactly one p ∈Q[0,1] such that
⟨a⟩=
pφ ∈Γ;
2. Let Γ1, . . . , Γh ∈SΛ (h ≥2) be distinct mc-sets. Then there exist ψ1, . . . .ψh ∈
LTEL such that ψi ∈Γj iﬀi = j and ⊢
1≤i,j≤h
s.t. i̸=j
¬(ψi ∧ψj).
Point 1 crucially relies on the quantitative features of PLSTSs, as it requires
the density property of Q, while point 2 can be seen as a “separation result” for
various modal logics (see [9]). In particular, if Γ0 ∈SΛ and φ ∈LTEL, point 1
implies ⟨a⟩≥
pφ, ⟨a⟩
≥
1−p¬φ ∈Γ0, for some p ∈Q[0,1]. By applying Lemma 7, we can
easily conclude that the mass of DΛ
a (Γ0, ) is greater than (or equal to) 1. Now,
suppose towards contradiction that DΛ
a (Γ0, ) has mass > 1. W.l.o.g. DΛ
a (Γ0, )
can be taken with ﬁnite support {Γ1, . . . , Γh}, where h ≥2 by Lemma 7, so that
there exist ψ1, . . . , ψh as in point 2. By point 1 there exists pi such that ⟨a⟩=
piψi ∈
Γ0 (1 ≤i ≤h), and by the fact that ψ1, . . . , ψh are mutually incompatible, using
Proposition 4.4 we obtain ⟨a⟩=
p1+...+ph( 
1≤i≤h ψi) ∈Γ0 with p1 + . . . + ph ≤1.
By deﬁnition we have DΛ
a (Γ0, Γi) ≤pi, which contradicts the assumptions.
Thanks to Lemma 7, we can easily establish the Truth Lemma by induction
on formulas.
Lemma 8 (Truth Lemma). For any φ ∈LTEL and Γ ∈SΛ:
Γ |=MΛ φ
iﬀ
φ ∈Γ

586
A. Aldini et al.
Given the Truth Lemma, the canonical model theorem follows using a stan-
dard argument.
Theorem 9 (Canonical model). Let Λ be a TENML. Then, for any φ ∈LTEL:
φ ∈Λ
iﬀ
φ is true in MΛ.
To show that a TENML Λ is complete with respect to a class of frames C
it suﬃces to check that its canonical model MΛ is based on a frame in C. This
allows us to infer in a fairly simple way the following completeness results:
Corollary 10 (Completeness for various TENMLs). For any φ ∈LTEL:
|=K φ implies ⊢TEK φ;
|=T φ implies ⊢TET φ;
|=B φ implies ⊢TEB φ;
|=K4 φ implies ⊢TEK4 φ;
|=S4 φ implies ⊢TES4 φ;
|=S5 φ implies ⊢TES5 φ.
Unfortunately, the compactness property does not hold for TENMLs. To see
this, let α ∈At and consider the following inﬁnite set of formulas:
Γα = {¬⟨a⟩
=
pα | p ∈Q[0,1]}
(6)
Clearly, any set Γp ≜Γα \ {¬⟨a⟩=
pα} is satisﬁable (and so any ﬁnite subset of
Γα is satisﬁable) but Γα is not. Now, by a standard argument, a TENML Λ is
strongly complete with respect to a class of frames C iﬀany Λ-consistent set of
formulas Δ is satisﬁable on some F ∈C. This means that the failure of strong
completeness follows by showing that Γα is Λ-consistent.
Let us ﬁnally remark that the results of this section still hold when the set
Q[0,1] is extended with a (recursively enumerable) set R ⊆[0, 1] in both the
language LTEL (see Deﬁnition 1) and its semantics (see Deﬁnition 2). In this
case, following [12], we require two closure conditions for R:
– quasi-closure under addition: ∀r, r′ ∈R. (r + r′ ≤1 ⇒r + r′ ∈R);
– closure under complements: ∀r ∈[0, 1]. (r ∈R ⇒1 −r ∈R).
6
Conclusion and Future Works
In this paper we have shown that a natural way of building a formal framework
for trust representation relies on probabilistic modal logics, where the modali-
ties are simply inherited from the probabilistic variants of HML and CTL. With
respect to these frameworks, we adopt a completely diﬀerent interpretation of
the underlying semantic model, which allows us to model networks of agents,
contexts of evaluation, beliefs of agents about the statements, and personal
judgments about the expertise of agents. All together, these ingredients form
the base for context-aware evidence-based trust estimations. For future work,

Trust Evidence Logic
587
it is worth investigating properties of trust that emerged in alternative formal
frameworks, such as [3,23,24]. An example is to relax the additivity constraint
on Da, moving to a sub-additive version, which would allow to model scenarios
in which uncertainty plays an important role.
As far as the theoretical foundations of TEL are concerned, we have extended
soundness and completeness results of graded modal logics to our setting. A
possible future work in this direction could be to investigate decidability of TEK,
and to ﬁnd a suitable sequent-calculus based presentation for it. Moreover, we
could explore techniques to recover compactness and strong completeness. A
ﬁrst idea is to introduce ﬁniteness constraints in the axiomatic systems able to
limit the use of probabilistic graded modalities, as in [12]. A more intriguing
approach comes from the coalgebraic treatment of modal logic, where coherence
conditions between syntax and semantics can be introduced to guarantee these
missing meta-properties [28]. A ﬁnal expansion we could explore is the possibility
of moving from a classical Boolean setting to a Many-Valued Logic, in order to
explicitly model the uncertainty of an agent about the truth of a proposition.
Acknowledgements. This work was supported by a UKRI Future Leaders Fellow-
ship, ‘Structure vs Invariants in Proofs’, project reference MR/S035540/1, and by the
Italian Ministry of Education, University and Research through the PRIN 2017 project
“The Manifest Image and the Scientiﬁc Image” prot. 2017ZNWW7F 004.
References
1. Aldini, A: A formal framework for modeling trust and reputation in collective
adaptive systems. In: Electronic Proceedings in Theoretical Computer Science,
EPTCS, vol. 217, pp. 19–30 (2016)
2. Aldini, A.: Design and veriﬁcation of trusted collective adaptive systems. Trans.
Model. Comput. Simul. (TOMACS) 28(2), 1–27 (2018)
3. Aldini, A., Tagliaferri, M.: Logics to reason formally about trust computation and
manipulation. In: Saracino, A., Mori, P. (eds.) ETAA 2019. LNCS, vol. 11967, pp.
1–15. Springer, Cham (2020). https://doi.org/10.1007/978-3-030-39749-4 1
4. Baier, A.: Trust and antitrust. Ethics 96(2), 231–260 (1986)
5. Baier, C., de Alfaro, L., Forejt, V., Kwiatkowska, M.: Model checking probabilistic
systems. In: Handbook of Model Checking, pp. 963–999. Springer, Cham (2018).
https://doi.org/10.1007/978-3-319-10575-8 28
6. Baier, C., Katoen, J.-P., Hermanns, H., Wolf, V.: Comparative branching-time
semantics for Markov chains. Inf. Comput. 200(2), 149–214 (2005)
7. Becker, M.Y., Russo, A., Sultana, N.: Foundations of logic-based trust manage-
ment. In: 2012 IEEE Symposium on Security and Privacy, pp. 161–175 (2012)
8. Bernardo, M., Miculan, M.: Disjunctive probabilistic modal logic is enough for
bisimilarity on reactive probabilistic systems. In: Bil`o, V., Caruso, A. (eds.) 17th
Italian Conference on Theoretical Computer Science, vol. 1720 of CEUR Workshop
Proceedings, pp. 203–220. CEUR-WS.org (2016)
9. De Caro, F.: Graded modalities, II (canonical models). Studia Logica 47(1), 1–10
(1988)
10. Desharnais, J., Edalat, A., Panangaden, P.: Bisimulation for labelled Markov pro-
cesses. Inf. Comput. 179(2), 163–193 (2002)

588
A. Aldini et al.
11. Evans, D.: The Internet of Things: how the next evolution of the Internet is chang-
ing everything. CISCO white paper (2011)
12. Fattorosi-Barnaba, M., Amati, G.: Modal operators with probabilistic interpreta-
tions. I. Studia Logica 46(4), 383–393 (1987)
13. Fine, K.: In so many possible worlds. Notre Dame J. Formal Logic 13(4), 516–520
(1972)
14. Gambetta, D.: Trust: Making and Breaking Cooperative Relations. Blackwell,
Hoboken (1988)
15. Globe, L.S.: Grades of modality. Logique et Analyse 13(51), 323–334 (1970)
16. Hansson, H., Jonsson, B.: A logic for reasoning about time and reliability. Formal
Aspects Comput. 6(5), 512–535 (1994)
17. Hennessy, M., Milner, R.: On observing nondeterminism and concurrency. In: de
Bakker, J., van Leeuwen, J. (eds.) ICALP 1980. LNCS, vol. 85, pp. 299–309.
Springer, Heidelberg (1980). https://doi.org/10.1007/3-540-10003-2 79
18. Jøsang, A.: Trust and reputation systems. In: Aldini, A., Gorrieri, R. (eds.) FOSAD
2006-2007. LNCS, vol. 4677, pp. 209–245. Springer, Heidelberg (2007). https://doi.
org/10.1007/978-3-540-74810-6 8
19. Jøsang, A., Ismail, R.: The beta reputation system. In: Proceedings of the 15th
Bled Electronic Commerce Conference, pp. 1–14 (2002)
20. Keymolen, E.: Trust on the Line: A Philosophical Exploration of Trust in the
Networked Era. Wolf Publishers, Nijmegen (2016)
21. Kwiatkowska, M., Norman, G., Parker, D.: PRISM 4.0: veriﬁcation of probabilistic
real-time systems. In: Gopalakrishnan, G., Qadeer, S. (eds.) CAV 2011. LNCS,
vol. 6806, pp. 585–591. Springer, Heidelberg (2011). https://doi.org/10.1007/978-
3-642-22110-1 47
22. Larsen, K.G., Skou, A.: Bisimulation through probabilistic testing. In: 16th Annual
ACM Symposium on Principles of Programming Languages, pp. 344–352. ACM
Press (1989)
23. Leturc, C., Bonnet, G.: A normal modal logic for trust in the sincerity. In: Interna-
tional Foundation for Autonomous Agents and Multiagent Systems, AAMAS’18,
pp. 175–183 (2018)
24. Liu, F., Lorini, E.: Reasoning about belief, evidence and trust in a multi-agent
setting. In: An, B., Bazzan, A., Leite, J., Villata, S., van der Torre, L. (eds.) PRIMA
2017. LNCS (LNAI), vol. 10621, pp. 71–89. Springer, Cham (2017). https://doi.
org/10.1007/978-3-319-69131-2 5
25. Luhmann, N.: Trust and Power. John Wiley and Sons Inc., Hoboken (1979)
26. Pacuit, E., Salame, S.: Majority logic. In: Dubois, D., Welty, C.A., Williams, M.-
A. (eds.) Principles of Knowledge Representation and Reasoning: 9th International
Conference (KR2004), pp. 598–605. AAAI Press (2004)
27. Putnam, R.: Making Democracy Work. Princeton University Press, Princeton
(1993)
28. Schr¨oder, L., Pattinson, D.: Strong completeness of coalgebraic modal logics. In:
Albers, S., Marion, J.-Y. (eds.) 26th International Symposium on Theoretical
Aspects of Computer Science (STACS 2009), pp. 673–684. Leibniz International
Proceedings in Informatics (2009)
29. Singh, M.P.: Trust as dependence: a logical approach. In: 10th International Con-
ference on Autonomous Agents and Multiagent Systems, AAMAS ’11, vol. 2, pp.
863–870 (2011)
30. Sonya, G.K., Schratt-Bitter, S.: Trust in online social networks: a multifaceted
perspective. Forum Social Econ. 44(1), 48–68 (2013)

Trust Evidence Logic
589
31. Tagliaferri, M., Aldini, A.: From knowledge to trust: a logical framework for pre-
trust computations. In: Gal-Oz, N., Lewis, P.R. (eds.) IFIPTM 2018. IAICT,
vol. 528, pp. 107–123. Springer, Cham (2018). https://doi.org/10.1007/978-3-319-
95276-5 8
32. van der Hoek, W.: Some considerations on the logic PfD. In: Voronkov, A. (ed.)
First Russian Conference on Logic Programming, vol. 592 of LNCS, pp. 474–485.
Springer, Heidelberg (1991)
33. Williamson, O.: Calculativeness, trust, and economic organization. J. Law Econ.
36(2), 453–486 (1993)

Generalized Rules of Probabilistic
Independence
Janneke H. Bolt1,2(B) and Linda C. van der Gaag2
1 Department of Information and Computing Sciences, Utrecht University,
Utrecht, Netherlands
j.h.bolt@uu.nl
2 Department of Mathematics and Computer Science,
Eindhoven University of Technology, Eindhoven, Netherlands
{j.h.bolt,l.c.v.d.gaag}@tue.nl
Abstract. Probabilistic independence, as a fundamental concept of
probability, enables probabilistic inference to become computationally
feasible for increasing numbers of variables. By adding ﬁve more rules to
an existing sound, yet incomplete, system of rules of independence, Stu-
den´y completed it for the class of structural semi-graphoid independence
relations over four variables. In this paper, we generalize Studen´y’s rules
to larger numbers of variables. We thereby contribute enhanced insights
in the structural properties of probabilistic independence. In addition,
we are further closing in on the class of probabilistic independence rela-
tions, as the class of relations closed under the generalized rules is a
proper subclass of the class closed under the previously existing rules.
Keywords: Probabilistic independence · Rules of independence ·
Semi-graphoid independence relations · Structural semi-graphoid
relations
1
Introduction
Probabilistic independence is a subject of intensive studies from both a mathe-
matics and a computing-science perspective [1,2,10]. Pearl and his co-workers
were among the ﬁrst to formalize properties of independence in a system of
qualitative rules [2], which characterizes the class of so-called semi-graphoid
independence relations. Although the semi-graphoid rules of independence are
probabilistically sound, they are not complete for probabilistic independence, as
was shown by Studen´y [4]. While the independences of any discrete multivariate
probability distribution adhere to the semi-graphoid rules, a set of indepen-
dence statements that is closed under these rules, may lack statements that are
probabilistically implied. As a consequence, the semi-graphoid rules allow inde-
pendence relations for which there are no matching probability distributions.
For proving incompleteness of Pearl’s system of rules, Studen´y formulated a
new rule for probabilistically implied independence using a proof construct based
c
⃝Springer Nature Switzerland AG 2021
J. Vejnarov´a and N. Wilson (Eds.): ECSQARU 2021, LNAI 12897, pp. 590–602, 2021.
https://doi.org/10.1007/978-3-030-86772-0_42

Generalized Rules of Probabilistic Independence
591
on the concept of multiinformation. He further deﬁned the class of structural
semi-graphoid independence relations as the class of independence relations that
are closed under all rules that can be found through such a construct [6–9], and
presented a set of ﬁve rules that completes the existing rule system for structural
relations involving four variables. For an unlimited number of variables, no ﬁnite
rule system can fully characterize the class of structural semi-graphoid relations,
which implies that there is also no ﬁnite complete set of rules for the class of
probabilistic independence relations [5].
In this paper we generalize Studen´y’s rules of independence to larger num-
bers of variables. By doing so, we uncover combinatorial structures in rules of
probabilistic independence and enable further investigation of such structures.
We moreover arrive at an enhanced description of the class of structural semi-
graphoid independence relations and thereby close in on the class of probabilistic
independence relations, as the class of relations closed under the generalized rules
is a proper subclass of the class closed under the previously existing rule system.
2
Preliminaries
We review sets of independence rules and the classes of relations they govern.
2.1
Semi-graphoid Independence Relations
We consider a ﬁnite, non-empty set V of discrete random variables and use
(possibly indexed) capital letters A, B, C, . . . to denote subsets of V . We will
use concatenation to denote set union and will further abbreviate the union of
sets in our ﬁgures by concatenating their indices, that is, we write A123B12 for
A1A2A3B1B2. A triplet over V now is a statement of the form ⟨A, B | C ⟩, where
A, B, C ⊆V are pairwise disjoint sets with A, B ̸= ∅. A triplet ⟨A, B | C ⟩is
taken to state that the sets of variables A and B are independent given the
conditioning set C. Any set of triplets over V is called an independence relation.
Pearl introduced the class of so-called semi-graphoid independence relations
by formulating four rules of independence [2], which are summarized by [3]:
A1: ⟨A, B | C ⟩↔⟨B, A | C ⟩
A2: ⟨A, BC | D⟩↔⟨A, B | CD⟩∧⟨A, C | D⟩
These two rules are schemata in which the arguments A, B, C, D are to be instan-
tiated to mutually disjoint subsets of V upon application. The rules A1, A2 are
called the semi-graphoid rules of independence, and any independence relation
that is closed under these rules is coined a semi-graphoid independence relation;
in the sequel, we will use A = {A1, A2} to denote the system of semi-graphoid
rules. The set A constitutes a sound inferential system for independence rela-
tive to the class of discrete multivariate probability distributions. The two rules
in A do not constitute a complete system for independence in such probability

592
J. H. Bolt and L. C. van der Gaag
distributions, however. The system’s incompleteness was shown by Studen´y [4],
who formulated the following additional rule:
A3 : ⟨A, B | CD⟩∧⟨A, B | ∅⟩∧⟨C, D | A⟩∧⟨C, D | B⟩↔
⟨A, B | C⟩∧⟨A, B | D⟩∧⟨C, D | AB⟩∧⟨C, D | ∅⟩
For constructing A3 and other new rules of independence, he built on the notion
of multiinformation, which we will review in the next section.
2.2
Multiinformation and Rules of Independence
A well-known measure for the amount of information shared by two (sets of)
variables A, B given a third (set of) variables C in the context of a discrete
multivariate probability distribution Pr, is the conditional mutual information
(see for example [12]), which is deﬁned as:
I(A; B | C || Pr) =

abc
Pr(abc) · log
Pr(ab | c)
Pr(a | c) · Pr(b | c)
where abc ranges over all possible value combinations for the variables in ABC
with Pr(a | c), Pr(b | c) ̸= 0. We have that I(A; B | C || Pr) ≥0 for any A, B, C
and Pr, and note that the mutual-information measure is related to independence
through the following property: I(A; B | C || Pr) = 0 iﬀthe triplet ⟨A, B | C⟩is a
valid independence statement in Pr. In the sequel, we omit Pr from the notation
as long as no ambiguity arises and take the sets A, B, C to be mutually disjoint.
For studying rules of independence, Studen´y exploited the notion of multiin-
formation [4,11], which is a function M: 2V →[0, ∞) over all subsets of variables
V with I(A; B | C) = M(ABC)+M(C)−M(AC)−M(BC); for details, we refer
to [11]. The multiinformation function thereby has the following properties:
• M(ABC) + M(C) −M(AC) −M(BC) ≥0;
• M(ABC) + M(C) −M(AC) −M(BC) = 0 iﬀ⟨A, B | C⟩.
The relation between the conditional mutual-information measure and the notion
of multiinformation now enables elegant soundness proofs for rules of indepen-
dence: a rule is sound if all its multiinformation terms ‘cancel out’, that is, if the
multiinformation terms of its set of premise triplets equal the multiinformation
terms of its set of consequent triplets. In the sequel, we will refer to this type of
proof as a multiinformation proof construct.
2.3
Structural Semi-graphoid Independence Relations
Building on the multiinformation concept, Studen´y introduced the class of struc-
tural semi-graphoid independence relations [6–9] where, roughly stated, a struc-
tural semi-graphoid relation is an independence relation that is closed under all

Generalized Rules of Probabilistic Independence
593
possible rules whose soundness derives from a multiinformation proof construct.
In addition to rule A3 stated above, Studen´y formulated four more rules for
probabilistic independence found through such proof constructs [6]. We state
these rules here in their original form, with their original numbering:
A4 : ⟨A, B | CD⟩∧⟨A, D | B⟩∧⟨C, D | A⟩∧⟨B, C | ∅⟩
↔
⟨A, B | D⟩∧⟨A, D | BC⟩∧⟨C, D | ∅⟩∧⟨B, C | A⟩
A5 : ⟨A, C | D⟩∧⟨B, D | C⟩∧⟨B, C | A⟩∧⟨A, D | B⟩
↔
⟨A, C | B⟩∧⟨B, D | A⟩∧⟨B, C | D⟩∧⟨A, D | C⟩
A6 : ⟨A, B | C⟩∧⟨A, C | D⟩∧⟨A, D | B⟩
↔
⟨A, B | D⟩∧⟨A, C | B⟩∧⟨A, D | C⟩
A7 : ⟨A, B | CD⟩∧⟨C, D | AB⟩∧⟨A, C | ∅⟩∧⟨B, D | ∅⟩
↔
⟨A, B | ∅⟩∧⟨C, D | ∅⟩∧⟨A, C | BD⟩∧⟨B, D | AC⟩
In the sequel, we will use S to denote the rule system {A1, . . . , A7}. We note that,
while all triplets in the rules A1, A2 respectively, share a ﬁxed same conditioning
(sub-)set of variables, the rules A3, . . . , A7 do not. To equally accommodate
additional conditioning variables, the latter ﬁve rules can each be enhanced by
adding an extra set of variables X to the conditioning parts of its triplets. In
phrasing the generalizations of these rules, we will omit such additional sets to
conform to the literature.
Any structural semi-graphoid independence relation is closed under the rule
system S by deﬁnition. As any semi-graphoid relation is closed under the system
of rules A, and A does not imply the additional rules in S, we have that the class
of structural semi-graphoid independence relations is a proper subclass of the
class of semi-graphoid relations. The system S was shown to fully characterize
the class of structural semi-graphoid relations over at most four variables [6],
that is, the system is both sound and complete for this class. Although sound,
the system is not complete for the class of structural independence relations
over more than four variables. Studen´y proved in fact that there is no ﬁnite
axiomatization of probabilistic independence [5,11], by providing the following
rule for all n ≥2 (reformulated):
⟨A, B0 | B1⟩∧. . . ∧⟨A, Bn−1 | Bn⟩∧⟨A, Bn | B0⟩
↔
(1)
⟨A, B0 | Bn⟩∧⟨A, B1 | B0⟩∧. . . ∧⟨A, Bn | Bn−1⟩
We note that this rule is a generalization of rule A6 in the system S. From
the structure of this rule, it is readily seen that any complete system of rules
for a ﬁxed number of k variables, will not be complete for k + 1 variables. The
hierarchy of independence relations is depicted in Fig. 1(a).

594
J. H. Bolt and L. C. van der Gaag
Fig. 1. The existing hierarchy of semi-graphoid, structural semi-graphoid, and proba-
bilistic independence relations (a) and the hierarchy extended with our results (b).
3
Generalizing Inference Rules
For the class of structural semi-graphoid relations, the new rules A3–A7 of inde-
pendence have been formulated, involving four sets of variables each (disregard-
ing any ﬁxed additional conditioning context in all triplets). We now reconsider
these rules and generalize them to larger numbers of variable sets. By doing so,
we arrive at an enhanced characterization of the class of structural semi-graphoid
relations and at further insights in the combinatorial structures of independence
rules. Because of space restrictions, we cannot detail full soundness proofs for
the new rules; instead, we provide brief proof sketches in the appendix.
We begin with formulating our generalization of rule A5. The idea underlying
its generalization is analogous to the idea of rule A6. Informally stated, the rule
takes a sequence of sets of variables and moves it (up to symmetry) over the
three argument positions of the triplets involved.
Proposition 1. Let A0, . . . , An, n ≥2, be non-empty, mutually disjoint sets of
variables. Then,

i∈{0,...,n}
⟨Ai, Aµ(i+1) | Aµ(i+2)⟩
↔

i∈{0,...,n}
⟨Ai, Aµ(i+1) | Aµ(i−1)⟩
with μ(x) := x mod (n + 1).
The property stated in the proposition is taken as the independence rule G5.
Note that in the case of n = 2, the rule reduces to a tautology. G5 further
embeds rule A5 as a special case with n = 3, as is seen by setting A0 ←C,
A1 ←A, A2 ←D and A3 ←B. As an example of the generalization, rule G5 is
now detailed for n = 4:
⟨A0, A1 | A2⟩∧⟨A1, A2 | A3⟩∧⟨A2, A3 | A4⟩∧⟨A3, A4 | A0⟩∧⟨A4, A0 | A1⟩↔
⟨A0, A1 | A4⟩∧⟨A1, A2 | A0⟩∧⟨A2, A3 | A1⟩∧⟨A3, A4 | A2⟩∧⟨A4, A0 | A3⟩
Note that the rule’s consequent cannot be derived from its premise using the
existing rule system S considered thus far. For n + 1 sets of variables, rule G5

Generalized Rules of Probabilistic Independence
595
Fig. 2. The combinatorial structure of rule G5, for n ≥2.
includes n + 1 premise triplets and n + 1 consequent triplets. When proceeding
from n+1 to n+2 sets of variables therefore, both the number of premise triplets
and the number of consequent triplets increase by one. Throughout this section,
the combinatorial structures of the generalized rules are illustrated by schematic
visualizations. The structure of rule G5 is shown in Fig. 2. Each labeled line
X
Z
Y in the ﬁgure depicts a triplet ⟨X, Y | Z⟩, with the line’s label cor-
responding with the triplet’s conditioning set. The left-hand side of the ﬁgure
summarizes the joint premise of the rule and the right-hand side its joint conse-
quent. The ﬁgure is readily seen to reﬂect the rule’s sliding structure.
Before addressing our generalization of rule A6 from S, we recall that Studen´y
already generalized this rule from pertaining to four sets of variables, to an
arbitrary number of variable sets; we recall this rule as Eq. (1) from Sect. 2.3.
Where Eq. (1) included just a single variable set Bi in a triplet’s conditioning
part, our generalization has conditioning parts including multiple such sets per
triplet. More speciﬁcally, the rule takes a sequence of variable sets B0, . . . , Bn,
just like rule G5 above, and moves it sliding over a triplet, yet now over just
its second and third argument positions, with the ﬁrst argument ﬁxed to an
unrelated set A. The number of sets that are included in the conditioning parts
of the rule’s triplets, is governed by a parameter k.
Proposition 2. Let A, B0, . . . , Bn, n ≥0, be non-empty, mutually disjoint sets
of variables. Then, for all k ∈[0, n],

i∈{0,...,n}
⟨A, Bi | Bk
i+⟩
↔

i∈{0,...,n}
⟨A, Bi | Bk
i−⟩
where Bk
i+ = Bµ(i+1) · · · Bµ(i+k), Bk
i−= Bµ(i−k) · · · Bµ(i−1), taking Bk
i+, Bk
i−:=
∅for k = 0, and where μ(x) := x mod (n + 1) as before.
The property stated in the proposition is taken as the independence rule G6.
Note that for k = 0, k = n, n = 0 and n = 1 the rule reduces to a tautology. G6
further embeds rule A6 as a special case with n = 2, k = 1, as is seen by setting
B0 ←B, B1 ←C and B2 ←D. Equation (1) is embedded as the cases with
k = 1 and any n ≥2. As an example of the generalization, rule G6 is detailed
for n = 3 and k = 2 below:
⟨A, B0 | B1B2⟩∧⟨A, B1 | B2B3⟩∧⟨A, B2 | B3B0⟩∧⟨A, B3 | B0B1⟩↔
⟨A, B0 | B2B3⟩∧⟨A, B1 | B3B0⟩∧⟨A, B2 | B0B1⟩∧⟨A, B3 | B1B2⟩

596
J. H. Bolt and L. C. van der Gaag
Fig. 3. The combinatorial structure of rule G6, for n ≥2 and k = 2.
We observe that the rule’s joint consequent cannot be derived from its joint
premise using the original rules from the system S. For n + 2 sets of variables,
rule G6 includes n + 1 premise triplets and n + 1 consequent triplets. When
proceeding from n+2 to n+3 variable sets therefore, both the number of premise
triplets and the number of consequent triplets increase by one; the parameter k
does not aﬀect the numbers of triplets involved. The combinatorial structure of
rule G6 is shown in Fig. 3. The ﬁgure highlights the rule’s star-shaped structure
that originates from the ﬁxed set A as the ﬁrst argument of all triplets involved.
The idea underlying our generalization of rule A4 from S, is somewhat less
intuitive. Informally phrased, the generalized rule takes two ﬁxed sets of variables
A, B, and associates these sets with an ordered sequence C of sets Ci. Each Ci
occurs as the second argument in two triplets, one with A for its ﬁrst argument
and one with B for its ﬁrst argument. The two subsequences that remain after
removing Ci from C are each positioned in the conditioning part of one of these
triplets. In the case of an odd index i, moreover, the triplet involving A has the
conditioning part extended with the set B, and vice versa. In the case of an even
index, the conditioning parts of the triplets are not extended.
Proposition 3. Let A, B, C1, . . . , Cn, with n ≥2 even, be non-empty, mutually
disjoint sets of variables. Then,

i∈{1,3,...,n−1}
[ ⟨A, Ci | Ci−B ⟩∧⟨B, Ci | Ci+ A ⟩] ∧

i∈{2,4,...,n}
[ ⟨A, Ci | Ci−⟩∧⟨B, Ci | Ci+⟩]
↔

i∈{1,3,...,n−1}
[ ⟨A, Ci | Ci+ B ⟩∧⟨B, Ci | Ci−A ⟩] ∧

i∈{2,4,...,n}
[ ⟨A, Ci | Ci+⟩∧⟨B, Ci | Ci−⟩]
where Ci−= C1 · · · Ci−1 and Ci+ = Ci+1 · · · Cn, taking Cj · · · Cj−1 := ∅.
The property stated in the proposition is taken as the independence rule G4.
Note that G4 embeds rule A4 as a special case with n = 2, as is seen by setting

Generalized Rules of Probabilistic Independence
597
Fig. 4. The combinatorial structure of rule G4, for n = 6.
A ←D, B ←B, C1 ←A and C2 ←C, where each set from G4 is assigned a
set from rule A4. As an example of the generalization, rule G4 is now detailed
for n = 4:
⟨A, C1 | B⟩∧⟨A, C3 | C1C2B⟩∧⟨B, C1 | C2C3C4A⟩∧⟨B, C3 | C4A⟩∧
⟨A, C2 | C1⟩∧⟨A, C4 | C1C2C3⟩
∧⟨B, C2 | C3C4⟩∧⟨B, C4 | ∅⟩
↔
⟨A, C1 | C2C3C4B⟩∧⟨A, C3 | C4B⟩∧⟨B, C1 | A⟩∧⟨B, C3 | C1C2A⟩∧
⟨A, C2 | C3C4⟩∧⟨A, C4 | ∅⟩∧⟨B, C2 | C1⟩∧⟨B, C4 | C1C2C3⟩
Note that the rule’s consequent is not derivable from its premise using the orig-
inal rules in S. For n + 2 sets of variables, rule G4 includes 2 · n premise triplets
and the same number of consequent triplets. When proceeding from n + 2 to
(n + 2) + 2 variable sets therefore, both the number of premise triplets and the
number of consequent triplets increase by four. The rule’s combinatorial struc-
ture is shown in Fig. 4, for n = 6. The rule has a bipartitely linked structure,
originating from the ﬁxed sets A and B in the ﬁrst argument positions per triplet
linking to the exact same sets from the sequence C. We further note that the
lines from A and from B to the same set Ci swap labels (replacing A by B and
vice versa) between the premise and consequent parts of the rule.
As the above generalization, our generalization of rule A3 takes two ﬁxed
sets of variables A, B. Instead of being associated with a single sequence of sets,
A and B are now associated with two separate sequences C and D, respectively.
Each set Ci occurs as the second argument in two triplets, both with A as the
ﬁrst argument. The two remaining subsequences of C after removing Ci, are each
positioned in the conditioning part of one of these triplets, supplemented with
a subsequence of D. A similar pattern is seen with the set B and its associated
sequence D. The triplet pairs with A and those with B as their ﬁrst arguments,

598
J. H. Bolt and L. C. van der Gaag
Fig. 5. The combinatorial structure of rule G3, for n = 3.
are again related through the inclusion of B in the conditioning part of one of
the triplet pairs with A, and vice versa.
Proposition 4. Let A, B, Ci, Di, i = 1, . . . , n, n ≥1, be non-empty, mutually
disjoint sets of variables, and let C = C1 · · · Cn and D = D1 · · · Dn. Then,

i∈{1,...,n}
[ ⟨A, Ci | Ci−Di++⟩∧⟨A, Ci | Ci+ Di−−B ⟩∧
⟨B, Di | Ci++ Di−A ⟩∧⟨B, Di | Ci−−Di+⟩]
↔

i∈{1,...,n}
[ ⟨A, Ci | Ci−Di++ B ⟩∧⟨A, Ci | Ci+ Di−−⟩∧
⟨B, Di | Ci++ Di−⟩∧⟨B, Di | Ci−−Di+ A ⟩]
where Zi−= Z1 · · · Zi−1, Zi+ = Zi+1 · · · Zn, Zi−−= Z1 · · · Zn−i+1 and Zi++ =
Zn−i+2 · · · Zn, taking Zj · · · Zj−1 := ∅, for Z = C, D.
The property stated in the proposition is taken as the independence rule G3.
Note that G3 embeds rule A3 as a special case with n = 1, as is seen by setting
A ←A, B ←C, C1 ←B and D1 ←D, where each set from G3 is assigned a
set from rule A3. As an example of the generalization, rule G3 is now detailed
for n = 2:
⟨A, C1 | ∅⟩∧⟨A, C1 | C2D1D2B⟩∧⟨B, D1 | A⟩∧⟨B, D1 | C1C2D2⟩∧
⟨A, C2 | C1D2⟩∧⟨A, C2 | D1B⟩∧⟨B, D2 | C2D1A⟩∧⟨B, D2 | C1⟩
↔
⟨A, C1 | B⟩∧⟨A, C1 | C2D1D2⟩∧⟨B, D1 | ∅⟩∧⟨B, D1 | C1C2D2A⟩∧
⟨A, C2 | C1D2B⟩∧⟨A, C2 | D1⟩∧⟨B, D2 | C2D1⟩∧⟨B, D2 | C1A⟩
We note that it is not possible to derive the rule’s consequent from its premise
by means of the rule system S. For 2·n+2 sets of variables, rule G3 includes 4·n
premise triplets and the same number of consequent triplets. When proceeding
from 2·n+2 to 2·(n+1)+2 variable sets therefore, both the number of premise
triplets and the number of consequent triplets increase by four. The combinato-
rial structure of the rule is illustrated in Fig. 5, for n = 3. This structure consists

Generalized Rules of Probabilistic Independence
599
Fig. 6. The combinatorial structure of rule G7, for n = 7.
of double-edged stars with the cardinalities of the conditioning sets of the paired
triplets adding up to 2 · n.
To conclude this section, we present our generalization of rule A7 from S.
The generalized rule involves a ﬁxed sequence with an even number of sets of
variables Ai. Each set with an even index i is paired with each set with an odd
index i in the ﬁrst two argument positions of a triplet. The conditioning part of
such a triplet includes either the intervening variables sets of the sequence or all
remaining variable sets.
Proposition 5. Let A0, . . . , An, with n ≥1 an odd number, be non-empty,
mutually disjoint sets of variables. Then,

i ∈{0, 2, . . . , n −1},
k ∈{1, 3, . . . , n}
⟨Ai, Aµ(i+k) | Ai+⟩
↔

i ∈{0, 2, . . . , n −1},
k ∈{1, 3, . . . , n}
⟨Ai, Aµ(i−k) | Ai−⟩
where Ai+ = Aµ(i+1) · · · Aµ(i+k−1) and Ai−= Aµ(i−k+1) · · · Aµ(i−1), taking
Ai+, Ai−:= ∅for k = 1, and where μ(x) := x mod (n + 1), as before.
The property stated in the proposition is taken as the independence rule G7.
Note that for n = 1 the rule reduces to a tautology. G7 further embeds rule A7
as a special case with n = 3, as is seen by setting A0 ←A, A1 ←C, A2 ←D
and A3 ←B. As an example of the generalization, rule G7 is now detailed for
n = 5:

600
J. H. Bolt and L. C. van der Gaag
⟨A0, A1 | ∅⟩∧⟨A0, A3 | A1A2⟩∧⟨A0, A5 | A1A2A3A4⟩∧
⟨A2, A3 | ∅⟩∧⟨A2, A5 | A3A4⟩∧⟨A2, A1 | A3A4A5A0⟩∧
⟨A4, A5 | ∅⟩∧⟨A4, A1 | A5A0⟩∧⟨A4, A3 | A5A0A1A2⟩
↔
⟨A0, A5 | ∅⟩∧⟨A0, A3 | A4A5⟩∧⟨A0, A1 | A2A3A4A5⟩∧
⟨A2, A1 | ∅⟩∧⟨A2, A5 | A0A1⟩∧⟨A2, A3 | A4A5A0A1⟩∧
⟨A4, A3 | ∅⟩∧⟨A4, A1 | A2A3⟩∧⟨A4, A5 | A0A1A2A3⟩
We observe that it is not possible to derive the rule’s consequent from its premise
using the rule system S considered thus far. For n + 1 sets of variables, rule
G7 includes ((n + 1)/2)2 premise triplets and the same number of consequent
triplets. When proceeding from n+1 to n+3 sets of variables therefore, both the
number of premise triplets and the number of consequent triplets are increased
by n+2. The combinatorial structure of the rule is illustrated in Fig. 6 for n = 7.
The conditioning sets of a premise triplet are found on the outermost circle, in
between the sets of its ﬁrst two arguments, going clockwise starting from its ﬁrst
argument. For a consequent triplet a similar observation holds, now however the
conditioning sets are found going counterclockwise.
We now deﬁne G = A ∪{G3, . . . , G7} for the new system of independence
rules. Compared to the system S, system G further restricts the number of
relations closed under all known independence rules and thereby constitutes
an enhanced characterisation of the class of structural semi-graphoid relations.
Figure 1(b) positions the new system G into the hierarchy of independence rela-
tions.
4
Conclusions and Future Research
With existing rule systems for probabilistic independence being sound yet incom-
plete, we presented generalizations of the ﬁve rules for independence formulated
by Studen´y [6], to larger numbers of (sets of) variables. With these generalized
rules we uncovered combinatorial structures of probabilistic independence, and
thereby enable their further study. We also closed in on the class of probabilis-
tic independence relations, as the class of relations closed under the generalized
rules is a proper subclass of the class closed under the previously existing rule
system. By further exploiting multiinformation proof constructs, we hope to
uncover in future research further combinatorial structures of independence and
to formulate new generalized rules for probabilistic independence.
Acknowledgment. We would like to thank Milan Studen´y for his quick answers to
our questions related to his work.
Appendix: Sketches of Proofs
All propositions stated in Sect. 3 are proven through multiinformation proof
constructs. We ﬁnd that the multiinformation equations of the four rules G3–
G6 share the property that each term occurs exactly twice. In these equations,

Generalized Rules of Probabilistic Independence
601
moreover, any multiinformation term originating from a premise triplet has a
matching term, that occurs either in the rule’s consequent with the same sign or
in another premise triplet with the opposite sign. All terms therefore cancel out
of the equation. For the ﬁfth rule G7 the same observation holds, except for the
terms M(∅) and M(A0 · · · An). These terms may occur more than twice, how-
ever both terms occur equally many times in the multiinformation expressions
of the rule’s premise and of the rule’s consequent and thus cancel out as well. In
the proof sketches per rule, we restrict ourselves to indicating how each multiin-
formation term from an arbitrarily chosen premise triplet is canceled out from
the multiinformation equation. Upon doing so, we abbreviate the multiinforma-
tion terms M(XY Z), M(Z), M(XZ) and M(Y Z) of a triplet θ = ⟨X, Y | Z⟩as
MIθ, MIIθ, MIIIθ and MIVθ, respectively.
Proposition 1. Let θ = ⟨Aj, Aµ(j+1) | Aµ(j+2)⟩be a premise triplet of rule
G5, for some index i = j. Then, the triplet θ′ = ⟨Aµ(j+1), Aµ(j+2) | Aj⟩, with
i = μ(j + 1), in the rule’s consequent, has MIθ′ = MIθ and MIVθ′ = MIIIθ. The
consequent triplet θ′′ = ⟨Aµ(j+3), Aµ(j+4) | Aµ(j+2)⟩, with i = μ(j + 3), further
has MIIθ′′ = MIIθ, and the consequent triplet θ′′′ = ⟨Aµ(j+2), Aµ(j+3) |Aµ(j+1)⟩,
with i = μ(j + 2), to conclude, has MIIIθ′′′ = MIVθ.
□
Proposition 2. Let θ = ⟨A, Bj | Bk
j+⟩be a premise triplet of rule G6, for
some index i = j and some k. Then, the consequent triplet θ′ = ⟨A, Bµ(j+k) |
Bk
µ(j+k)−⟩, with i = μ(j + k), has MIθ′ = MIθ and MIVθ′ = MIVθ. The conse-
quent triplet θ′′ = ⟨A, Bµ(j+k+1) | Bk
µ(j+k+1)−⟩, with i = μ(j + k + 1), further
has MIIθ′′ = MIIθ and MIIIθ′′ = MIIIθ.
□
Proposition 3. Let θ = ⟨A, Cj | C1 · · · Cj−1 B⟩be a premise triplet of rule G4,
for some index i = j. Then, the consequent triplet θ′ = ⟨B, Cj | C1 · · · Cj−1 A⟩,
with i = j, has MIθ′ = MIθ and MIIIθ′ = MIIIθ. The consequent triplet θ′′ =
⟨B, Cj+1 | C1 . . . Cj⟩, with i = j + 1, has MIIIθ′′ = MIVθ. For j > 1, the
consequent triplet θ′′′ = ⟨B, Cj−1 | C1 . . . Cj−2⟩, with i = j −1, has MIθ′′′ =
MIIθ. For j = 1, the term MIIθ is canceled out of the multiinformation equation
by the term MIII, with the opposite sign, of the premise triplet ⟨B, Cn | ∅⟩.
Similar arguments apply to the premise triplets of another form than θ.
□
Proposition 4. Let θ = ⟨A, Cj | C1 · · · Cj−1Dn−j+2 · · · Dn⟩be a premise triplet
of G3, for some index i = j. Then, the consequent triplet θ′ = ⟨B, Dn−j+1 |
C1 · · · CjDn−j+2 · · · DnA⟩, with i = n −j + 1, has MIIθ′ = MIθ. The premise
triplet θ′′ = ⟨B, Dn−j+1 | C1 · · · CjDn−j+2 · · · Dn⟩, with i = n −j + 1, further
has MIIθ′′ = −MIVθ. For all j > 1, the premise triplet θ′′′ = ⟨B, Dn−j+2 |
C1 · · · Cj−1Dn−j+3 · · · Dn⟩, with i = n −j + 2, has MIVθ′′′ = −MIIθ and the
consequent triplet θ′′′′ = ⟨B, Dn−j+2 | C1 · · · Cj−1Dn−j+3 · · · DnA⟩, also with
i = n −j + 2, has MIVθ′′′′ = MIIIθ. For j = 1 to conclude, the term MIIθ is
matched by the MII term of the consequent triplet ⟨B, D1 | ∅⟩and the term
MIIIθ is canceled out by the term MII of opposite sign of the premise triplet
⟨B, D1 | A⟩, with i = 1. Similar arguments apply to premise triplets of other
form than θ.
□

602
J. H. Bolt and L. C. van der Gaag
Proposition 5. Let θ = ⟨Aj, Aµ(j+h) | Aµ(j+1) · · · Aµ(j+h−1)⟩be a premise
triplet of G7, for some indices i = j, k = h. The term MIIIθ is can-
celed out from the multiinformation equation by the term MIIIθ′ of the con-
sequent triplet θ′ = ⟨Aµ(j+h−1), Aµ(j−1) | Aj · · · Aµ(j+h−2)⟩, with k = h,
i = μ(j + h −1). The term MIVθ is canceled out by MIVθ′′ of the con-
sequent triplet θ′′ = ⟨Aµ(j+h+1), Aµ(j+1) | Aµ(j+2) · · · Aµ(j+h)⟩, with k = h,
i = μ(j + h + 1). For all k ≤n −2, the term MIθ is canceled out by the term
MIIθ′′′ of the consequent triplet θ′′′ = ⟨Aµ(j+h+1), Aµ(j−1) | Aµ(j) · · · Aµ(j+h)⟩,
with k = h + 2, i = μ(j + h + 1); for k = n, the MI terms of all premise
and consequent triplets have MI· = M(A0 · · · An) and hence match. For all
k ≥3, the term MIIθ is canceled out by the term MIθ′′′′ of the consequent
triplet θ′′′′ = ⟨Aµ(j+h−1), Aµ(j+1) | Aµ(j+2) · · · Aµ(j+h−2)⟩, with k = h −2,
i = μ(j + h −1); for k = 1, the MII terms of all premise and consequent triplets
have MII = M(∅) and thus match.
□
References
1. Dawid, A.: Conditional independence in statistical theory. J. R. Stat. Soc. B 41,
1–31 (1979)
2. Pearl, J.: Probabilistic Reasoning in Intelligent Systems: Networks of Plausible
Inference. Morgan Kaufmann (1988)
3. Studen´y, M.: Attempts at axiomatic description of conditional independence.
Kybernetika 25, 72–79 (1989)
4. Studen´y, M.: Multiinformation and the problem of characterization of conditional
independence relations. Probl. Control Inf. Theor. 18, 01 (1989)
5. Studen´y, M.: Conditional independence relations have no ﬁnite complete character-
ization. In: Information Theory, Statistical Decision Functions and Random Pro-
cesses. Transactions of the 11th Prague Conference, vol. B, pp. 377–396. Kluwer,
Dordrecht (1992)
6. Studen´y, M.: Structural semigraphoids. Int. J. Gen. Syst. 22, 207–217 (1994)
7. Studen´y, M.: Description of structures of stochastic conditional independence by
means of faces and imsets. 1st part: introduction and basic concepts. Int. J. Gen.
Syst. 23, 123–137 (1995)
8. Studen´y, M.: Description of structures of stochastic conditional independence by
means of faces and imsets. 2rd part: basic theory. Int. J. Gen. Syst. 23, 201–219
(1995)
9. Studen´y, M.: Description of structures of stochastic conditional independence by
means of faces and imsets. 3rd part: examples of use and appendices. Int. J. Gen
Syst 23, 323–341 (1995)
10. Studen´y, M.: Probabilistic Conditional Independence Structures. Springer, London
(2005). https://doi.org/10.1007/b138557
11. Studen´y, M., Vejnarov´a, J.: The multiinformation function as a tool for measuring
stochastic dependence. In: Learning in Graphical Models, pp. 261–298. Kluwer
Academic Publishers (1998)
12. Yeung, R.: A First Course in Information Theory. Kluwer Academic Publishers,
Dordrecht (2002)

Algebras of Sets and Coherent
Sets of Gambles
Arianna Casanova1(B)
, Juerg Kohlas2
, and Marco Zaﬀalon1
1 Istituto Dalle Molle di Studi sull’Intelligenza Artiﬁciale (IDSIA),
Lugano, Switzerland
{arianna,zaffalon}@idsia.ch
2 Department of Informatics (DIUF), University of Fribourg, Fribourg, Switzerland
juerg.kohlas@unifr.ch
Abstract. In a recent work we have shown how to construct an infor-
mation algebra of coherent sets of gambles deﬁned on general possibility
spaces. Here we analyze the connection of such an algebra with the set
algebra of sets of its atoms and the set algebra of subsets of the possibility
space on which gambles are deﬁned. Set algebras are particularly impor-
tant information algebras since they are their prototypical structures.
Furthermore, they are the algebraic counterparts of classical proposi-
tional logic. As a consequence, this paper also details how propositional
logic is naturally embedded into the theory of imprecise probabilities.
Keywords: Desirability · Information algebras · Order theory ·
Imprecise probabilities · Coherence
1
Introduction and Overview
While analysing the compatibility problem of coherent sets of gambles, Miranda
and Zaﬀalon [14] have recently remarked that their main results could be
obtained also using the theory of information algebras [7]. This observation
has been taken up and deepened in some of our recent works [13,18]: we have
shown that the founding properties of desirability can in fact be abstracted into
properties of information algebras. Stated diﬀerently, desirability makes up an
information algebra of coherent sets of gambles.
Information algebras are algebraic structures composed by ‘pieces of infor-
mation’ that can be manipulated by operations of combination, to aggregate
them, and extraction, to extract information regarding a speciﬁc question. From
the point of view of information algebras, sets of gambles deﬁned on a possibil-
ity space Ω are pieces of information about Ω. It is well known that coherent
sets of gambles are ordered by inclusion and, in this order, there are maximal
elements [4]. In the language of information algebras such elements are called
atoms. In particular, any coherent set of gambles is contained in a maximal set
(an atom) and it is the intersection (meet) of all the atoms it is contained in. An
information algebra with these properties is called atomistic. Atomistic informa-
tion algebras have the universal property of being embedded in a set algebra,
c
⃝Springer Nature Switzerland AG 2021
J. Vejnarov´a and N. Wilson (Eds.): ECSQARU 2021, LNAI 12897, pp. 603–615, 2021.
https://doi.org/10.1007/978-3-030-86772-0_43

604
A. Casanova et al.
which is an information algebra whose elements are sets. This is an important
representation theorem for information algebras, since set algebras are a spe-
cial kind of algebras based on the usual set operations. Set algebras are a kind
of prototype information algebras, as ﬁelds of sets are prototypes of Boolean
algebras, it matters to know that the information algebra of coherent sets of
gambles is also of this kind, although this is not evident at the ﬁrst sight. This
result is similar to the well-known representation theorems for Boolean algebras
and Priestley duality for distributive lattices, see [1]. Similar general represen-
tation theorems for a special type of general information algebras (commutative
ones) have been presented in [7,12]. It is conjectured (but remains to be proved)
that these results carry over to the more general type of information algebras
proposed in [8] and to which the algebra of coherent sets of gambles belongs.
Conversely, any such set algebra of subsets of Ω is embedded in the algebra of
coherent sets of gambles deﬁned on Ω. These links between set algebras and
the algebra of coherent sets of gambles are the main topic of the present work.
Since set algebras are algebraic counterparts of classical propositional logic, the
results of this paper detail as well how the latter is formally part of the theory of
imprecise probabilities [17]. We refer also to [3] for another aspect of this issue.
After recalling the main concepts introduced in our previous work in Sects. 2,
3 and 4, in Sect. 5 we establish some results about atoms needed for the subse-
quent sections. In Sect. 6 we deﬁne the concept of embedding for the information
algebras of interest and ﬁnally, in Sect. 7, we show the links between set algebras
(of subsets of Ω and of sets of atoms) and the algebra of coherent sets of gambles.
2
Desirability
Consider a set Ω of possible worlds. A gamble over this set is a bounded function
f : Ω →R. It is interpreted as an uncertain reward in a linear utility scale. A
subject might desire a gamble or not, depending on the information they have
about the experiment whose possible outcomes are the elements of Ω. We denote
the set of all gambles on Ω by L(Ω), or more simply by L, when there is no
possible ambiguity. We also introduce L+(Ω) := {f ∈L(Ω) : f ≥0, f ̸= 0}, or
simply L+ when no ambiguity is possible, the set of non-negative non-vanishing
gambles. These gambles should always be desired, since they may increase the
wealth with no risk of decreasing it. As a consequence of the linearity of our
utility scale, we assume also that a subject disposed to accept the transactions
represented by f and g, is disposed to accept also λf + μg with λ, μ ≥0 not
both equal to 0. More generally speaking, we consider the notion of a coherent
set of gambles [17]:
Deﬁnition 1 (Coherent set of gambles). We say that a subset D of L is a
coherent set of gambles if and only if D satisﬁes the following properties:
D1. L+ ⊆D [Accepting Partial Gains];
D2. 0 /∈D [Avoiding Status Quo];
D3. f, g ∈D ⇒f + g ∈D [Additivity];

Algebras of Sets and Coherent Sets of Gambles
605
D4. f ∈D, λ > 0 ⇒λf ∈D [Positive Homogeneity].
So, D is a convex cone. Let us denote with C(Ω), or simply with C, the family
of coherent sets of gambles on Ω. This leads to the concept of natural extension.
Deﬁnition 2 (Natural extension for gambles). Given a set K ⊆L, we call
E(K) := posi(K ∪L+), where posi(K′) :=
r
j=1 λjfj : fj ∈K′, λj > 0, r ≥1

,
for every set K′ ⊆L, its natural extension.
E(K) of a set of gambles K is coherent if and only if 0 ̸∈E(K).
In [18] we showed that Φ(Ω) := C(Ω) ∪{L(Ω)}, or simply Φ when there is
no possible ambiguity, is a complete lattice under inclusion [1], where meet is
intersection and join is deﬁned for any family of sets {Di}i∈I ∈Φ as

i∈I
Di :=
 
D ∈Φ :

i∈I
Di ⊆D
	
.
Note that, if the family of coherent sets Di has no upper bound in C, then its join
is simply L. Moreover, we deﬁned the following closure operator [1] on subsets
of gambles K ⊆L
C(K) :=

{D ∈Φ : K ⊆D}.
(1)
It is possible to notice that C(K) = E(K) if 0 ̸∈E(K), that is if E(K) is coherent,
otherwise C(K) = L and we may have E(K) ̸= C(K).
The most informative cases of coherent sets of gambles, i.e., coherent sets
that are not proper subsets of other coherent sets, are called maximal. The
following proposition provides a characterisation of such maximal elements [4,
Proposition 2].
Proposition 1 (Maximal coherent set of gambles). A coherent set of gam-
bles D is maximal if and only if (∀f ∈L \ {0}) f /∈D ⇒−f ∈D.
We shall denote maximal sets with M to diﬀerentiate them from the general case
of coherent sets. These sets play an important role with respect to information
algebras (see Sect. 5). Another important class is that of strictly desirable sets
of gambles [17].1
Deﬁnition 3 (Strictly desirable set of gambles). A coherent set of gambles
D is said to be strictly desirable if and only if it satisﬁes (∀f ∈D \ L+)(∃δ >
0) f −δ ∈D.
For strictly desirable sets, we shall employ the notation D+.
1 Strictly desirable sets of gambles are important since they are in a one-to-one corre-
spondence with coherent lower previsions, a generalization of the usual expectation
operator on gambles. Given a coherent lower prevision P(·), D+ := {f ∈L : P(f) >
0} ∪L+ is a strictly desirable set of gambles [17, Sect. 3.8.1].

606
A. Casanova et al.
3
Stucture of Questions and Possibilities
In this section we review the main results about the structure of Ω [8,9,18].
With reference to our previous work [18], we recall that coherent sets of gambles
are understood as pieces of information describing beliefs about the elements in
Ω. Beliefs may be originally expressed relative to diﬀerent questions or variables
that we identify by families of equivalence relations ≡x on Ω for x in some
index set Q [5,8,9].2 A question x ∈Q has the same answer in possible worlds
ω ∈Ω and ω′ ∈Ω, if ω ≡x ω′. There is a partial order between questions
capturing granularity: question y is ﬁner than question x if ω ≡y ω′ implies
ω ≡x ω′. This can be expressed equivalently considering partitions Px, Py of Ω
whose blocks are respectively, the equivalence classes [ω]x, [ω]y of the equivalence
relations ≡x, ≡y, representing possible answers to x and y. Then ω ≡y ω′
implying ω ≡x ω′, means that any block [ω]y of partition Py is contained in
some block [ω]x of partition Px. If this is the case, we say that: x ≤y or
Px ≤Py.3 Partitions Part(Ω) of any set Ω, form a lattice under this order [6].
In particular, the partition sup{Px, Py} := Px ∨Py of two partitions Px, Py is,
in this order, the partition obtained as the non-empty intersections of blocks
of Px with blocks of Py. It can be equivalently expressed also as Px∨y. The
deﬁnition of the meet Px ∧Py, or equivalently Px∧y, is somewhat involved [6].
We usually assume that the set of questions Q analyzed, considered together
with their associated partitions denoted with Q := {Px : x ∈Q}, is a join-
sub-semilattice of (Part(Ω), ≤) [1]. In particular, we assume often that the top
partition in Part(Ω), i.e. P⊤(where the blocks are singleton sets {ω} for ω ∈Ω),
belongs to Q. A gamble f on Ω is called x-measurable, iﬀfor all ω ≡x ω′ we have
f(ω) = f(ω′), that is, if f is constant on every block of Px. It could then also be
considered as a function (a gamble) on the set of blocks of Px. We denote with
Lx(Ω), or more simply with Lx when no ambiguity is possible, the set of all
x-measurable gambles. We recall also the logical independence and conditional
logical independence relation between partitions [8,9].
Deﬁnition 4 (Independent Partitions). For a ﬁnite set of partitions
P1, . . . , Pn ∈Part(Ω), n ≥2, let us deﬁne
R(P1, . . . , Pn) := {(B1, . . . , Bn) : Bi ∈Pi, ∩n
i=1Bi ̸= ∅}.
We call the partitions independent, if R(P1, . . . , Pn) = P1 × · · · × Pn.
Deﬁnition 5 (Conditionally Independent Partitions). Consider a ﬁnite
set of partitions P1, . . . Pn ∈Part(Ω), and a block B of a partition P (contained
or not in the list P1, . . . , Pn), then deﬁne for n ≥1,
RB(P1, . . . , Pn) := {(B1, . . . , Bn) : Bi ∈Pi, ∩n
i=1Bi ∩B ̸= ∅}.
2 This view generalises the most often used multivariate model, where questions are
represented by families of variables and theirs domains [7,8].
3 In the literature usually the inverse order between partitions is considered. However,
this order better corresponds to our natural order of questions by granularity.

Algebras of Sets and Coherent Sets of Gambles
607
We call P1, . . . , Pn conditionally independent given P, if for all blocks B of P,
RB(P1, . . . , Pn) = RB(P1) × · · · × RB(Pn).
This relation holds if and only if Bi ∩B ̸= ∅for all i = 1, . . . , n, implies B1 ∩
. . .∩Bn ∩B ̸= ∅. In this case we write ⊥{P1, . . . , Pn}|P or, for n = 2, P1⊥P2|P.
Px⊥Py|Pz can be indicated also with x⊥y|z. We may also say that P1⊥P2|P
if and only if ω ≡P ω′ implies the existence of an element ω′′ ∈Ω such that
ω ≡P1∨P ω′′ and ω′ ≡P2∨P ω′′. The three-place relation P1⊥P2|P, in particular,
satisﬁes the following properties:
Theorem 1. Given P, P′, P1, P2 ∈Part(Ω), we have:
C1 P1⊥P2|P2;
C2 P1⊥P2|P implies P2⊥P1|P;
C3 P1⊥P2|P and P′ ≤P2 imply P1⊥P′|P;
C4 P1⊥P2|P implies P1⊥P2 ∨P|P.
From these properties it follows also: Px⊥Py|Pz
⇐⇒Px∨z⊥Py∨z|Pz. A join
semilattice (Q, ≤) together with a relation x⊥y|z with x, y, z ∈Q, (Q, ≤, ⊥),
satisfying conditions C1 to C4 is called a quasi-separoid (or also q-separoid) [8],
a retract of the concept of separoid [1].
4
Information Algebra of Coherent Sets of Gambles
In [18] we showed that Φ(Ω) with the following operations:
1. Combination: D1 · D2 := C(D1 ∪D2),4
2. Extraction: ϵx(D) := C(D ∩Lx) for x ∈Q,
where (Q, ≤, ⊥) is a q-separoid of questions on Ω in which x⊥y|z with x, y, z ∈
Q is the conditional independence relation introduced on them (see Sect. 3),
is a domain-free information algebra that we call the domain-free information
algebra of coherent sets of gambles. Combination captures aggregation of pieces
of belief, and extraction, which for multivariate models leads to marginalisation
in the equivalent labeled view of information algebras [7,13], describes ﬁltering
the part of information relative to a question x ∈Q. Information algebras are
particular valuation algebras as deﬁned by [15] but with idempotent combination.
Domain-free versions of valuation algebras have been proposed by Shafer [19].
Idempotency of combination has important consequences, such as the possibility
to deﬁne an information order, atoms, approximation, and more [7,8]. It also
oﬀers—the subject of the present paper—important connections to set algebras.
Here we remind the characterizing properties of the domain-free informa-
tion algebra Φ together with the q-separoid (Q, ≤, ⊥) and with a family E of
extraction operators ϵx : Φ →Φ for x ∈Q:
4 Notice that combination of sets in Φ coincides with their join in the lattice (Φ, ⊆),
as deﬁned in Sect. 2.

608
A. Casanova et al.
1. Semigroup: (Φ, ·) is a commutative semigroup with a null element 0 = L(Ω)
and a unit 1 = L+(Ω).
2. Quasi-Separoid: (Q, ≤, ⊥) is a quasi-separoid.
3. Existential Quantiﬁer: For any x ∈Q, D1, D2, D ∈Φ:
(a) ϵx(0) = 0,
(b) ϵx(D) · D = D,
(c) ϵx(ϵx(D1) · D2) = ϵx(D1) · ϵx(D2).
4. Extraction: For any x, y, z ∈Q, D ∈Φ, such that x∨z⊥y∨z|z and ϵx(D) = D,
we have: ϵy∨z(D) = ϵy∨z(ϵz(D)).
5. Support: For any D ∈Φ there is an x ∈Q so that ϵx(D) = D, i.e. a support
of D [18], and for all y ≥x, y ∈Q, ϵy(D) = D.
When we need to specify the constructing elements of the information algebra, we
can refer to it with the tuple (Φ, Q, ≤, ⊥, ·, 0, 1, E) or equivalently with (Φ, Q, ≤
, ⊥, ·, 0, 1, E), where (Q, ≤, ⊥) is the q-separoid of partitions equivalent to (Q, ≤
, ⊥).5 When we do not need this degree of accuracy, we can refer to it simply as Φ.
Similar considerations can be made for other domain-free information algebras.
Notice that, in particular, (Φ, ·) is an idempotent, commutative semigroup. So,
a partial order on Φ is deﬁned by D1 ≤D2 if D1 ·D2 = D2. Then D1 ≤D2 if and
only if D1 ⊆D2. This order is called an information order [18]. This deﬁnition
entails the following facts: ϵx(D) ≤D for every D ∈Φ, x ∈Q; given D1, D2 ∈Φ,
if D1 ≤D2, then ϵx(D1) ≤ϵx(D2) for every x ∈Q [7].
5
Atoms and Maximal Coherent Sets of Gambles
Atoms are a well-known concept in (domain-free) information algebras. In our
previous work [18], we showed that maximal coherent sets M are atoms of Φ.
We denote them with At(Φ). Moreover, for every D ∈Φ, we deﬁne At(D) :=
{M ∈At(Φ) : D ⊆M}. We remind here some elementary properties of atoms
for the speciﬁc case of At(Φ) [7]. Given M, M1, M2 ∈At(Φ) and D ∈Φ:
1. M · D = M or M · D = 0;
2. either D ≤M or M · D = 0;
3. either M1 = M2 or M1 · M2 = 0.
Φ is in particular atomic [7], i.e. for any D ̸= 0 the set At(D) is not empty,
and atomistic, i.e. for any D ̸= 0, D = 
 At(D). It is a general result of atom-
istic information algebras that the subalgebras ϵx(Φ) := {ϵx(D), D ∈Φ} are
also atomistic [12]. Moreover, in [18], we showed that At(ϵx(Φ)) = ϵx(At(Φ)) =
{ϵx(M) : M ∈At(Φ)} for any x ∈Q and, therefore, we call ϵx(M) for M ∈At(Φ)
and x ∈Q local atoms for x. 6 Local atoms Mx = ϵx(M) for x ∈Q induce a par-
tition Atx of At(Φ) with blocks At(Mx), see Lemma 1 below. If M and M ′ belong
5 We can use the same notation E for the extraction operators in the two signatures
of the information algebra. Indeed, we can indicate ϵx also as ϵPx, where Px ∈Q is
the partition associated to x ∈Q.
6 Since local atoms for x are atoms of the subalgebra ϵx(Φ), they respect in particular
all the elementary properties of atoms restricted to elements of ϵx(Φ) [7].

Algebras of Sets and Coherent Sets of Gambles
609
to the same block, we say that M ≡x M ′. Let us indicate with PartQ(At(Φ)) :=
{Atx : x ∈Q} the set of these partitions. PartQ(At(Φ)) is in particular a subset
of Part(At(Φ)), the set of all the partitions of At(Φ). On Part(At(φ)) we can
introduce a partial order ≤, respect to which Part(At(Φ)) is a lattice, and a con-
ditional independence relation At1⊥At2|At3, with At1, At2, At3 ∈Part(At(Φ))
analogous to the ones introduced in Sect. 3. Their introduction, indeed, is inde-
pendent of the set on which partitions are deﬁned [6]. It can be shown moreover
that (Part(At(Φ)), ≤, ⊥) is a quasi-separoid [8, Theorem 2.6]. We claim now that
partitions Atx ∈PartQ(At(Φ)) mirror the partitions Px ∈Q. Before stating this
main result, we need the following lemma.
Lemma 1. Let us consider M, M ′ ∈At(Φ) and x ∈Q. Then
M ≡x M ′ ⇐⇒ϵx(M) = ϵx(M ′) ⇐⇒M, M ′ ∈At(ϵx(M)) = At(ϵx(M ′)).
Hence, Atx ≤Aty if and only if At(ϵx(M)) ⊇At(ϵy(M)) for every M ∈At(Φ)
and x, y ∈Q.
Proof. If M ≡x M ′, there exists a local atom Mx such that M, M ′ ∈At(Mx).
Therefore, M, M ′ ≥Mx and ϵx(M), ϵx(M ′) ≥Mx [18, Lemma 15, item 3]. How-
ever, ϵx(M), ϵx(M ′) and Mx are all local atoms, hence ϵx(M) = ϵx(M ′) = Mx.
The converse is obvious. For the second part, let us suppose At(ϵy(M)) ⊆
At(ϵx(M)) for every M ∈At(Φ) with x, y ∈Q, and consider M ′, M ′′ ∈At(Φ)
such that M ′ ≡y M ′′. Then M ′, M ′′ ∈At(ϵy(M ′)) and so M ′, M ′′ ∈At(ϵx(M ′)),
which implies M ′ ≡x M ′′. Vice versa, consider Atx ≤Aty and M ′ ∈At(ϵy(M))
for some M, M ′ ∈At(Φ). Then, M ≡y M ′, hence M ≡x M ′ and so M ′ ∈
At(ϵx(M)).
Now we can state the main result of this section.
Theorem 2. The map Px →Atx, from (Q, ≤, ⊥) to (PartQ(At(Φ)), ≤, ⊥), pre-
serves order, that is Px ≤Py implies Atx ≤Aty, and conditional independence
relations, that is Px⊥Py|Pz implies Atx⊥Aty|Atz.
Proof. If x ≤y, then ϵx(M) ≤ϵy(M) for any atom M ∈At(Φ) [18, Lemma 15,
item 4]. Therefore, At(ϵx(M)) ⊇At(ϵy(M)) for any M ∈At(Φ), hence, by
Lemma 1, Atx ≤Aty. For the second part, ﬁrst of all notice that properties C2
and C3 of Theorem 1 are valid also for the conditional independence relation
deﬁned on Part(At(Φ)), restricted to PartQ(At(Φ)). Then, recall that x⊥y|z if
and only if x∨z⊥y ∨z|z. Consider then local atoms Mx∨z, My∨z and Mz so that
At(Mx∨z) ∩At(Mz) ̸= ∅,
At(My∨z) ∩At(Mz) ̸= ∅.
Hence, there is an atom M ′ ∈At(Mx∨z)∩At(Mz) and an atom M ′′ ∈At(My∨z)∩
At(Mz). Therefore, Mx∨z = ϵx∨z(M ′), My∨z = ϵy∨z(M ′′) and Mz = ϵz(M ′) =
ϵz(M ′′). Now, thanks to the Existential Quantiﬁer axiom, we have:
ϵz(Mx∨z · My∨z · Mz) = ϵz(Mx∨z · My∨z) · Mz = ϵz(ϵx∨z(M ′) · ϵy∨z(M ′′)) · Mz

610
A. Casanova et al.
Thanks to [18, Theorem 16] and [18, Lemma 15, item 3, item 6], we obtain
ϵz(ϵx∨z(M ′) · ϵy∨z(M ′′)) · Mz = ϵz(M ′) · ϵz(M ′′) · Mz ̸= 0.
Therefore Mx∨z · My∨z · Mz ̸= 0 [18, Lemma 15, item 2] and hence, since
the algebra is atomic, there is an atom M ′′′ ∈At(Mx∨z · My∨z · Mz). Then
Mx∨z, My∨z, Mz ≤M ′′′, whence M ′′′ ∈At(Mx∨z) ∩At(My∨z) ∩At(Mz) and
so Atx∨z⊥Aty∨z|Atz. From C2 and C3 of Theorem 1 and the ﬁrst part of this
theorem then, it follows that Atx⊥Aty|Atz.
6
Information Algebras Homomorphisms
We are interested in homomorphisms between domain-free information algebras.
A domain-free information algebra is a two-sorted structure consisting of an
idempotent commutative semigroup with a null and a unit element and a q-
separoid, where the two sorts are linked by the extraction operators. Therefore
an homomorphism should preserve operations and relations of both structures.
This is achieved by the following deﬁnition.
Deﬁnition 6 (Domain-free information algebras homomorphism). Let
(Ψ, Q, ≤, ⊥, ·, 0, 1, E) and (Ψ ′, Q′, ≤′, ⊥′, ·′, 0′, 1′, E′) be two domain-free informa-
tion algebras, where E := {ϵx : x ∈Q} and E′ := {ϵ′
x′ : x′ ∈Q′} are respectively
the families of the extraction operators of the two structures. Consider a pair of
maps f : Ψ →Ψ ′ and g : Q →Q′, with the associated map h : E →E′ deﬁned
by h(ϵx) = ϵ′
g(x). Then the pair (f, g) is an information algebras homomorphism
between Ψ and Ψ ′ if:
1. f(ψ · φ) = f(ψ) ·′ f(φ), for every ψ, φ ∈Ψ;
2. f(0) = 0′ and f(1) = 1′;
3. x ≤y implies g(x) ≤′ g(y) and x⊥y|z implies g(x)⊥′g(y)|g(z), for all x, y, z ∈
Q;
4. f(ϵx(ψ)) = ϵ′
g(x)(f(ψ)), for all ψ ∈Ψ and ϵx ∈E.
We can give an analogous deﬁnition of homomorphism considering partitions
equivalent to questions. We use the same notation for both the signatures.
If f and g are one-to-one, the pair (f, g) is an information algebras embedding
and Ψ is said to be embedded into Ψ ′; if they are bijective, (f, g) is an information
algebras isomorphism and the two structures are called isomorphic.
7
Set Algebras
Archetypes of information algebras are so-called set algebras, where the ele-
ments are subsets of some universe, combination is intersection, and extraction
is related to so-called saturation operators. We ﬁrst specify the set algebra of
subsets of Ω and show then that this algebra may be embedded into the infor-
mation algebra of coherent sets. Conversely, we show that the algebra Φ may

Algebras of Sets and Coherent Sets of Gambles
611
itself be embedded into a set algebra of its atoms, so is, in some precise sense,
itself a set algebra. This is a general result for atomistic information algebras
[7,12].
To any partition Px of Ω there corresponds a saturation operator deﬁned for
any subset S ⊆Ω by
σx(S) := {ω ∈Ω : (∃ω′ ∈S) ω ≡x ω′}.
(2)
It corresponds to the union of the elements of blocks of Px that have not empty
intersection with S. The following are well-known properties of saturation oper-
ators. They can be shown analogously to the similar results in [8,18].
Lemma 2. For all S, T ⊆Ω and any partition Px of Ω:
1. σx(∅) = ∅,
2. S ⊆σx(S),
3. σx(σx(S) ∩T) = σx(S) ∩σx(T),
4. σx(σx(S)) = σx(S),
5. S ⊆T ⇒σx(S) ⊆σx(T),
6. σx(σx(S) ∩σx(T)) = σx(S) ∩σx(T).
Note that the ﬁrst three items of this theorem imply that σx is an exis-
tential quantiﬁer relative to intersection as combination. This is a ﬁrst step to
construct a domain-free information algebra of subsets of Ω. Now, consider the
q-separoid (Q, ≤, ⊥) sort of the information algebra (Φ, Q, ≤, ⊥, ·, 0, 1, E) con-
sidered in Sect. 4. We need the support axiom to be satisﬁed. If P⊤∈Q, the
set of partitions equivalent to questions in Q, then σ⊤(S) = S for all S ⊆Ω.
Otherwise, we must limit ourselves to the subsets of Ω for which there exists a
support x ∈Q. We call these sets saturated with respect to some x ∈Q, and
we indicate them with PQ(Ω) or more simply with PQ when no ambiguity is
possible. Clearly, if the top partition belongs to Q, PQ(Ω) = P(Ω), the power
set of Ω. So in what follows we can refer more generally to sets in PQ(Ω). Note
that in particular Ω, ∅∈PQ(Ω). At this point the support axiom is satisﬁed
for every element of PQ(Ω). Indeed, if x ≤y with x, y ∈Q, then ω ≡y ω′
implies ω ≡x ω′, so that σy(S) ⊆σx(S). Then, if x is a support of S, we have
S ⊆σy(S) ⊆σx(S) = S, hence σy(S) = S. Moreover (PQ(Ω), ∩) is a commuta-
tive semigroup with the empty set as the null element and Ω as the unit. Indeed,
the only property we need to prove, is that PQ(Ω) is closed under intersection.
So, let us consider S and T, two subsets of Ω with support x ∈Q and y ∈Q
respectively. They have also both supports x ∨y that belongs to Q, because
(Q, ≤) is a join semilattice. Therefore, thanks to Lemma 2, we have
σx∨y(S ∩T) = σx∨y(σx∨y(S) ∩σx∨y(T)) = σx∨y(S) ∩σx∨y(T) = S ∩T.
So, PQ(Ω) is closed under intersection. Lemma 2 moreover, implies that it is
closed also under saturation. It remains only to verify the extraction property
to conclude that PQ forms a domain-free information algebra. We show it in the
next result.

612
A. Casanova et al.
Theorem 3. Given x, y, z ∈Q, suppose x∨z⊥y∨z|z. Then, for any S ∈PQ(Ω),
σy∨z(σx(S)) = σy∨z(σz(σx(S))).
Proof. From σz(σx(S)) ⊇σx(S) we obtain σy∨z(σz(σx(S)) ⊇σy∨z(σx(S)). Con-
sider therefore an element ω ∈σy∨z(σz(σx(S))). Then there are elements μ, μ′
and ω′ so that ω ≡y∨z μ ≡z μ′ ≡x ω′ and ω′ ∈S. This means that ω, μ belong
to some block By∨z of partition Py∨z, μ, μ′ to some block Bz of partition Pz
and μ′, ω′ to some block Bx of partition Px. It follows that Bx ∩Bz ̸= ∅and
By∨z ∩Bz ̸= ∅. Then x ∨z⊥y ∨z|z implies, thanks to properties of a quasi-
separoid, that x⊥y ∨z|z. Therefore, we have Bx ∩By∨z ∩Bz ̸= ∅, and in partic-
ular, Bx ∩By∨z ̸= ∅. So there is a λ ∈Bx ∩By∨z such that ω ≡y∨z λ ≡x ω′ ∈S,
hence ω ∈σy∨z(σx(S)). So we have σy∨z(σx(S)) = σy∨z(σz(σx(S))).
So, we have the domain-free information algebra (PQ(Ω), Q, ≤, ⊥, ∩, ∅,
Ω, {σx : x ∈Q}). It is in particular an algebra of sets, with intersection as
combination and saturation as extraction. Such type of information algebra will
be called set algebra. In particular, we claim that this set algebra of subsets of Ω
can be embedded into the information algebra of coherent sets of gambles Φ(Ω).
Indeed, for any set S ∈PQ(Ω), deﬁne
DS := {f ∈L(Ω) : inf
ω∈S f(ω) > 0} ∪L+(Ω).
If S ̸= ∅, this is clearly a coherent set, otherwise it corresponds to L(Ω). The next
theorem shows that the map f : S →DS together with the identity map g, and
the associated map h : ϵx →σx, form an information algebras homomorphism
between PQ(Ω) and Φ(Ω). In this case the q-separoid considered in the two
algebras is the same and g = id, therefore item 3 in the deﬁnition of a domain-
free information algebras homomorphism is trivially satisﬁed.
Theorem 4. Let S, T ∈PQ(Ω) and x ∈Q. Then
1. DS · DT = DS∩T ,
2. D∅= L(Ω), DΩ = L+(Ω),
3. ϵx(DS) = Dσx(S).
Proof. 1. Note that DS = L+ or DT = L+ if and only if S = Ω or T = Ω.
Clearly in this case we have immediately the result. The same is true if DS = L
or DT = L, which is equivalent to having S = ∅or T = ∅. Now suppose
DS, DT ̸= L+ and DS, DT ̸= L. If S ∩T = ∅, then DS∩T = L(Ω). Consider
f ∈DS and g ∈DT . Since S and T are disjoint, we have ˜f ∈DS and ˜g ∈DT ,
where ˜f, ˜g are deﬁned in the following way for every ω ∈Ω:
˜f(ω) :=
⎧
⎨
⎩
f(ω)
for ω ∈S,
−g(ω) for ω ∈T,
0
for ω ∈(S ∪T)c,
˜g(ω) :=
⎧
⎨
⎩
−f(ω) for ω ∈S,
g(ω)
for ω ∈T,
0
for ω ∈(S ∪T)c.
However, ˜f +˜g = 0 ∈E(DS∪DT ), hence DS·DT := C(DS∪DT ) = L(Ω) = DS∩T .
Assume then that S ∩T ̸= ∅. Note that DS ∪DT ⊆E(DS ∪DT ) ⊆DS∩T so that

Algebras of Sets and Coherent Sets of Gambles
613
E(DS ∪DT ) is coherent and DS · DT = E(DS ∪DT ) ⊆DS∩T . Consider then a
gamble f ∈DS∩T . Select a δ > 0 and deﬁne two functions
f1(ω) :=
⎧
⎪
⎪
⎨
⎪
⎪
⎩
1/2f(ω) for ω ∈(S ∩T),
δ
for ω ∈S \ T,
f(ω) −δ for ω ∈T \ S,
1/2f(ω) for ω ∈(S ∪T)c,
f2(ω) :=
⎧
⎪
⎪
⎨
⎪
⎪
⎩
1/2f(ω) for ω ∈(S ∩T),
f(ω) −δ for ω ∈S \ T,
δ
for ω ∈T \ S,
1/2f(ω) for ω ∈(S ∪T)c,
for every ω ∈Ω. Then f = f1 + f2 and f1 ∈DS, f2 ∈DT . Therefore f ∈
E(DS ∪DT ) = DS · DT , hence DS · DT = DS∩T .
2. Both have been noted above.
3. If S is empty, then ϵx(D∅) = L(Ω) so that item 3 holds in this case. Hence,
assume S ̸= ∅. Then we have that DS is coherent, and therefore:
ϵx(DS) := C(DS ∩Lx) = E(DS ∩Lx) := posi(L+(Ω) ∪(DS ∩Lx)).
Consider a gamble f ∈DS ∩Lx. If f ∈L+(Ω) ∩Lx then clearly f ∈Dσx(S).
Otherwise, infS f > 0 and f is x-measurable. If ω ≡x ω′ for some ω′ ∈S
and ω ∈Ω, then f(ω) = f(ω′). Therefore infσx(S) f = infS f > 0, hence f ∈
Dσx(S). Then C(DS ∩Lx) ⊆C(Dσx(S)) = Dσx(S). Conversely, consider a gamble
f ∈Dσx(S). Dσx(S) is a strictly desirable set of gambles.7 Hence, if f ∈Dσx(S),
f ∈L+(Ω) or there is δ > 0 such that f −δ ∈Dσx(S). If f ∈L+(Ω), then
f ∈ϵx(DS). Otherwise, let us deﬁne for every ω ∈Ω, g(ω) := infω′≡xω f(ω′)−δ.
If ω ∈S, then g(ω) > 0 since infσx(S)(f −δ) > 0. So we have infS g ≥0 and g
is x-measurable. However, infS(g + δ) = infS g + δ > 0 hence (g + δ) ∈DS ∩Lx
and f ≥g + δ. Therefore f ∈E(DS ∩Lx) = C(DS ∩Lx) =: ϵx(DS).
Item 3. guarantees that, if S ∈PQ(Ω), then there exists an x ∈Q such that
ϵx(DS) = DS. Moreover, f and g are one-to-one,8so PQ is embedded into Φ.
Regarding instead the relation between Φ and a set algebra of sets of its
atoms, consider the set algebra:
(P(At(Φ)), Part(At(Φ)), ≤, ⊥, ∩, ∅, At(Φ), Σ),
(3)
where P(At(Φ)) is the power set of At(Φ), which clearly leads to the commuta-
tive semigroup (P(At(Φ)), ∩) with ∅as the null element and At(Φ) as the unit
element, (Part(At(Φ)), ≤, ⊥) is the q-separoid introduced in Sect. 5 and Σ is the
set of all saturation operators associated with any partition At ∈Part(At(Φ)),
deﬁned similarly to (2). It is clearly a set algebra. In fact, the Semigroup, the
Quasi-Separoid and the Support axioms are clearly satisﬁed. The Existential
Quantiﬁer axiom follows from Lemma 2 applied to saturation operators in Σ and
subsets of At(Φ) and the Extraction axiom follows from Theorem 3 applied again
to Σ and elements in P(At(Φ)). We claim moreover that (Φ, Q, ≤, ⊥, ·, 0, 1, E)
can be embedded into (P(At(Φ)), Part(At(Φ)), ≤, ⊥, ∩, ∅, At(Φ), Σ). Consider
the maps f : Φ →P(At(Φ)) and g : Q →PartQ(At(Φ)) deﬁned as f(D) = At(D)
7 P(f) := infS(f) for every f ∈L with S ̸= ∅, is a coherent lower prevision [17].
8 Indeed g is clearly one-to-one and regarding f, if DS = DT it means that S = T.

614
A. Casanova et al.
and g(Px) = Atx, where Atx ∈PartQ(At(Φ)) is the partition deﬁned by the
equivalence relation between atoms M ≡x M ′ if and only if ϵx(M) = ϵx(M ′)
with M, M ′ ∈At(Φ). As noted before, associated with the partition Atx,
there is the saturation operator σx, deﬁned for any subset S ⊆At(Φ) by
σx(S) := {M ∈At(Φ) :
(∃M ′ ∈S) M ≡x M ′}. Thus, we have the map h
deﬁned as h(ϵx) = σx. The next result, together with Theorem 2, shows that
the pair of maps (f, g) is an information algebra homomorphism between Φ and
P(At(Φ)).
Theorem 5. For any element D1, D2 and D of Φ and all x ∈Q,
1. At(D1 · D2) = At(D1) ∩At(D2),
2. At(L(Ω)) = ∅, At(L+(Ω)) = At(Φ),
3. At(ϵx(D)) = σx(At(D)).
Proof. Item 2 is obvious. If there is a an atom M ∈At(D1 · D2), then M ≥D1 ·
D2 ≥D1, D2 and thus M ∈At(D1)∩At(D2). Conversely, if M ∈At(D1)∩At(D2),
then D1, D2 ≤M, hence D1 · D2 ≤M and M ∈At(D1 · D2). Furthermore, if
ϵx(D) = 0, then D = 0 and At(D) = ∅, hence σx(∅) = ∅and vice versa [18,
Lemma 15, item 2]. Assume therefore At(D) ̸= ∅and consider M ∈σx(At(D)).
There is then a M ′ ∈At(D) so that ϵx(M) = ϵx(M ′). But D ≤M ′, hence
ϵx(D) ≤ϵx(M ′) = ϵx(M) ≤M. Thus M ∈At(ϵx(D)). Conversely consider
M ∈At(ϵx(D)). We claim that ϵx(M)·D ̸= 0. Because otherwise 0 = ϵx(ϵx(M)·
D) = ϵx(M) · ϵx(D) = ϵx(M · ϵx(D)), by Existential Quantiﬁer axiom, which is
not possible since ϵx(D) ≤M. So there is an M ′ ∈At(ϵx(M) · D) so that D ≤
ϵx(M) · D ≤M ′. We conclude that M ′ ∈At(D). Furthermore, ϵx(ϵx(M) · D) =
ϵx(M) · ϵx(D) ≤ϵx(M ′). It follows that ϵx(M ′) · ϵx(M) · ϵx(D) ̸= 0 and therefore
ϵx(M) · ϵx(M ′) ̸= 0. But they are both local atoms, hence ϵx(M) = ϵx(M ′),
which together with M ′ ∈At(D) tells us that M ∈σx(At(D)).
Φ is atomistic, then f is one-to-one. Further, g is also one-to-one since ϵx ̸= ϵy
implies Atx ̸= Aty. Therefore Φ is embedded into P(At(Φ)) and we can say that
it is in fact a set algebra.
8
Conclusions
This paper presents an extension of our work on information algebras related
to gambles on a possibility set that is not necessarily multivariate [18]. In the
multivariate case, we showed also that lower previsions and strictly desirable sets
of gambles form isomorphic information algebras [18]. It can be expected to be
valid also in this case. Since then convex credal sets are sets of atoms associated
with a lower prevision [17,18], we claim that the algebra of lower previsions,
hence of strictly desirable sets of gambles, can be embedded into the set algebra
of credal sets. This is left for future work, along with other aspects such as the
question of conditioning and the equivalent expression of the results shown in
this paper for the labeled view of information algebras, better suited for compu-
tational purposes [7,18].

Algebras of Sets and Coherent Sets of Gambles
615
Acknowledgements. We would like to thank the anonymous Referees for a number
of useful remarks.
References
1. Davey, B.A., Priestley, H.A.: Introduction to Lattices and Order. Cambridge Uni-
versity Press (1990)
2. Dawid, A.P.: Separoids: a mathematical framework for conditional independence
and irrelevance. Ann. Math. Artif. Intell. 32(1–4), 335–372 (2001). https://doi.
org/10.1023/A:1016734104787
3. De Cooman, G.: Belief models: an order-theoretic investigation. Ann. Math. Artif.
Intell. 45(1), 5–34 (2005). https://doi.org/10.1007/s10472-005-9006-x
4. De Cooman, G., Quaeghebeur, E.: Exchangeability and sets of desirable gambles.
Int. J. Approx. Reason. 53, 363–395 (2012)
5. Shafer, G.: A Mathematical Theory of Evidence, vol. 42. Princeton University
Press (1976)
6. Gr¨atzer, G.: General Lattice Theory. Birkh¨auser Verlag (2003)
7. Kohlas, J.: Information Algebras: Generic Structures for Inference. Springer, Lon-
don (2003). https://doi.org/10.1007/978-1-4471-0009-6
8. Kohlas, J.: Algebras of information. A new and extended axiomatic foundation
(2017). https://arxiv.org/abs/1701.02658
9. Kohlas, J., Monney, P.A.: A Mathematical Theory of Hints. An Approach to the
Dempster-Shafer Theory of Evidence. Lecture Notes in Economics and Mathemat-
ical Systems, vol. 425. Springer, Heidelberg (1995). https://doi.org/10.1007/978-
3-662-01674-9
10. Kohlas, J., Schmid, J.: Research notes: an algebraic theory of information. Tech-
nical report 06–03, Department of Informatics, University of Fribourg (2013)
11. Kohlas, J., Schmid, J.: An algebraic theory of information: an introduction and
survey. Information 5(2), 219–254 (2014)
12. Kohlas, J., Schmid, J.: Commutative information algebras and their representation
theory (2020)
13. Casanova, A., Kohlas, J., Zaﬀalon, M.: Information algebras in the theory of impre-
cise probabilities (2021). https://arxiv.org/abs/2102.13368
14. Miranda, E., Zaﬀalon, M.: Compatibility, desirability, and the running intersection
property. Artif. Intell. 283, 103247 (2020)
15. Shafer, G., Shenoy, P.P.: Axioms for probability and belief function propagation.
In: Shafer, G., Pearl, J. (eds.) Readings in Uncertain Reasoning. Morgan Kaufmann
Publishers Inc., San Mateo (1990)
16. Troﬀaes, M.C.M., De Cooman, G.: Lower Previsions. John Wiley & Sons, Hoboken
(2014)
17. Walley, P.: Statistical Reasoning with Imprecise Probabilties. Chapman and Hall
(1991)
18. Kohlas, J., Casanova, A., Zaﬀalon, M.: Information algebras of coherent sets of
gambles in general possibility spaces. In: International Symposium on Imprecise
Probabilities: Theories and Applications, PMLR 2021. https://people.idsia.ch/
∼arianna.casanova/papers/InfoAlgebrasCoherentSetsGS.pdf
19. Shafer, G.: An axiomatic study of computation in hypertrees. School of Business,
University of Kansas. Working Paper (1991)

A Probabilistic Deontic Logic
Vincent de Wit, Dragan Doder(B), and John Jules Meyer
Department of Information and Computing Sciences, Utrecht University,
Utrecht, The Netherlands
{d.doder,J.J.C.Meyer}@uu.nl
Abstract. In this article, we introduce a logic for reasoning about prob-
ability of normative statements. We present its syntax and semantics,
describe the corresponding class of models, provide an axiomatization
for this logic and prove that the axiomatization is sound and complete.
We also prove that our logic is decidable.
Keywords: Monadic deontic logic · Normative reasoning ·
Probabilistic logic · Completeness · Decidability
1
Introduction
The seminal work of von Wright from 1951 [14] initiated a systematic study
on formalization of normative reasoning in terms of deontic logic. The latter
is a branch of modal logics that deals with obligation, permission and related
normative concepts. A plethora of deontic logics have been developed for various
application domains like legal reasoning, argumentation theory and normative
multi-agent systems [1,7].
Some recent work also studied learning behavioral norms from data [11,13]. In
[11], the authors pointed out that human norms are context-speciﬁc and laced
with uncertainty, which poses challenges to their representation, learning and
communication. They gave an example of a learner that might conclude from
observations that talking is prohibited in a library setting, while another learner
might conclude the opposite when seeing people talking at the checkout counter.
They represented uncertainty about norms using deontic operators, equipped
with probabilistic boundaries that capture the subjective degree of certainty.
In this paper, we study uncertain norms form a logical point of view. We use
probabilistic logic [3–6,12] to represent uncertainty, and we present the proof-
theoretical and model-theoretical approach to a logic which allows reasoning
about uncertain normative statements. We take two well studied logics, monadic
deontic logic (MDL) [9] and probabilistic logic of Fagin, Halpern and Magido
(FHM) [4], as the starting points, and combine them in a rich formalism that
generalizes each of them. The resulting language makes it possible to adequately
model diﬀerent degrees of belief in norms; for example, we can express statements
like “the probability that one is obliged to be quiet is at least 0.9”.
c
⃝Springer Nature Switzerland AG 2021
J. Vejnarov´a and N. Wilson (Eds.): ECSQARU 2021, LNAI 12897, pp. 616–628, 2021.
https://doi.org/10.1007/978-3-030-86772-0_44

A Probabilistic Deontic Logic
617
The semantics for our logic consists of speciﬁc Kripke-like structures, where
each model contains a probability space whose sample space is the set of states,
and with each state carrying enough information to evaluate a deontic formula.
We consider so-called measurable models, which allow us to assign a probability
value to every deontic statement.
The main result of this article is a sound and complete axiomatization for
our logic. Like any other real-valued probabilistic logic, it is not compact, so any
ﬁnitary axiomatic system would fail to be strongly complete (“every consistent
set of formulas has a model”) [6]. We prove weak completeness (“every consistent
formula has a model”) combining and modifying completeness techniques for
MDL and FHM. We also show that our logic is decidable, combining the method
of ﬁltration and a reduction to a system of inequalities.
The rest of the paper is organised as follows: In Sect. 2 the proposed syn-
tax and semantics of the logic will be presented together with other needed
deﬁnitions. In Sect. 3 the axiomatization of the logic is given, followed by the
soundness and completeness proof in Sect. 4. In Sect. 5 we show that our logic is
decidable. Lastly, in Sect. 6 a conclusion is given together with future research
topics.
2
Syntax and Semantics
In this section, we present the syntax and semantics of our probabilistic deontic
logic. This logic, that we call PDL, contains two types of formulas: standard
deontic formulas of MDS, and probabilistic formulas. Let Q denote the set of
rational numbers.
Deﬁnition 1 (Formula). Let P be a set of atomic propositions. The language L
of probabilistic monadic deontic logic is generated by the following two sentences
of BNF (Backus Naur Form):
[Ldeontic] φ ::= p | ¬φ | (φ ∧φ) | Oφ
p ∈P
[Lprob−d] f ::= a1w(φ1) + · · · + anw(φn) ≥α | ¬f | (f ∧f)
ai, α ∈Q
The set of all formulas L is Ldeontic ∪Lprob−d. We denote the elements of L
with θ and θ’, possibly with subscripts.
The construct Oφ is read as “It is obligatory that φ”, while w(φ) stands for
“probability of φ”. An expression of the form a1w(φ1) + · · · + anw(φn) is called
term. We denote terms with x and t, possibly with subscripts. The propositional
connectives, ∨, →and ↔, are introduced as abbreviations, in the usual way.
We abbreviate θ ∧¬θ with ⊥, and ¬⊥with ⊤. We also use abbreviations to
deﬁne other types of inequalities; for example, w(φ) ≥w(φ′) is an abbreviation
for w(φ) −w(φ′) ≥0, w(φ) = α for w(φ) ≥α and −w(φ) ≥−α, w(φ) < α for
¬w(φ) ≥α.
Example 1. Following our informal example from the introduction about behav-
ioral norms in a library, the fact that a person has become fairly certain that it

618
V. de Wit et al.
is normal to be quiet might be expressed by the probabilistic statement “the prob-
ability that one is obliged to be quiet is at least 0.9”. This sentence could be
formalized using the introduced language as
w(Oq) ≥0.9.
Note that we do not allow mixing of the formulas from Ldeontic and Lprob−d.
For example, O(p ∨q) ∧w(Oq) ≥0.9 is not a formula of our language. Before
we introduce the semantics of PDL we will introduce Monadic Deontic Logic
models.
Deﬁnition 2 (Relational model). A relational model D is a tuple D =
(W, R, V ) where:
– W is a (non-empty) set of states (also called “possible worlds”); W is called
the universe of the model.
– R ⊆W × W
is a binary relation over W. It is understood as a relation of
deontic alternativeness: sRt (or, alternatively, (s, t) ∈R ) says that t is an
ideal alternative to s, or that t is a “good” successor of s. The ﬁrst one is
“good” in the sense that it complies with all the obligations true in the second
one. Furthermore, R is subject to the following constraint:
(∀s ∈W)(∃t ∈W)(sRt)
(seriality)
This means that the model does not have a dead end, a state with no good
successor.
– V : P →2W is a valuation function assigning to each atom p a set V (p) ⊆W
(intuitively the set of states at which p is true.)
Next, we deﬁne the satisﬁability of a formula in a model. This deﬁnition is
in accordance with standard satisﬁability relation of MDL.
Deﬁnition 3 (Satisfaction in MDL). Let D = (W, R, V ) be a relational deon-
tic model, and let w ∈W. We deﬁne the satisﬁability of a deontic formula
φ ∈Ldeontic in the state w, denoted by D, w |=MDL φ, recursively as follows:
– D, w |=MDL p iﬀws ∈Vs(p).
– D, w |=MDL ¬φ iﬀD, s ̸|=MDL φ.
– D, w |=MDL φ ∧ψ iﬀD, s |=MDL φ and D, s |=MDL ψ.
– D, w |=MDL Oφ iﬀfor all u ∈Ws, if wRu then D, u |=MDL φ.
Now we introduce the semantics of PDL.
Deﬁnition 4 (Model). A probabilistic deontic model is a tuple M
=
⟨S, X , μ, τ⟩, where
– S is a non-empty set of states
– X is a σ-algebra of subsets of S
– μ : X →[0, 1] is a probability measure, i.e.,

A Probabilistic Deontic Logic
619
• μ(X) ≥0 for all X ∈X
• μ(S) = 1
• μ(∞
i=1 Xi) = ∞
i=1 μ(Xi), if the Xi’s are pairwise disjoint members of
X
– τ associates with each state s in S a tuple containing a monadic deontic model
and one of its worlds, i.e., τ(s) = (Ds, ws), where:
• Ds = (Ws, Rs, Vs) is a relational model of monadic deontic logic as deﬁned
in Deﬁnition 3.
• ws ∈Ws is a world ws in Ws of model Ds.
Let us illustrate this deﬁnition.
Example 1 (continued) Assume a ﬁnite set of atomic propositions {p, q}.
Let us consider the model M = ⟨S, X , μ, τ⟩, where
– S = {s, s′, s′′, s′′′}
– X
is the set of all subsets of S
– μ is characterized by: μ({s}) = 0.5, μ({s′}) = μ({s′′}) = 0.2, μ({s′′′}) = 0.1
(other values follow from the properties of probability measures)
– τ is a mapping which assigns to the state s, Ds = (Ws, Rs, Vs) and ws such
that
• Ws = {w1, w2, w3, w4}
• Rs = {(w1, w2), (w1, w3), (w2, w2), (w2, w3), (w3, w2), (w3, w3), (w4, w2),
(w4, w3), (w4, w4)}
• Vs(p) = {w1, w3}, Vs(q) = {w2, w3}
• ws = w1
Note that the domain of τ is always the whole set S, but in this example
we only explicitly specify τ(s) for illustration purposes.
This model is depicted in Fig. 1. The circle on the right contains the four states
of the model, which are measured by
μ. Each of the states is equipped with a
standard pointed model of MDL. In this picture, only one of them is shown, the
one that corresponds to s. It is represented within the circle on the left. Note
that the arrows depict the “good” alternative relation R. If we assume that
q
stands for “quiet”, like in the previous example, in all good successors of
w1
the proposition
q holds. Note that, according to Deﬁnition 3 , this means that
in w1 people are obliged to be quiet in the library.
For a model M = ⟨S, X , μ, τ⟩and a formula φ ∈Ldeontic, let ∥φ∥M denote
the set of states that satisfy φ, i.e., ∥φ∥M = {s ∈S | Ds, ws |=MDL φ}. We
omit the subscript M from ∥φ∥M when it is clear from context. The following
deﬁnition introduces an important class of probabilistic deontic models, so-called
measurable models.
Deﬁnition 5 (Measurable model). A probabilistic deontic model is measur-
able if
∥φ∥M ∈X
for every φ ∈Ldeontic.

620
V. de Wit et al.
Fig. 1. Model M = ⟨S, X , μ, τ⟩.
In this paper, we focus on measurable structures, and we prove completeness
and decidability results for this class of structures.
Deﬁnition 6 (Satisfaction). Let M = ⟨S, X , μ, τ⟩be a measurable probabilis-
tic deontic model. We deﬁne the satisﬁability relation |= recursively as follows:
• M |= φ iﬀfor all s ∈S, Ds, ws |=MDL φ
• M |= a1w(φ1) + · · · + akw(φk) ≥α iﬀa1μ(∥φ1∥) + · · · + akμ(∥φk∥) ≥α.
• M |= ¬f iﬀM ̸|= f
• M |= f ∧g iﬀM |= f and M |= g.
Example 1 (continued) Continuing the previous example, it is now also pos-
sible to speak about the probability of the obligation to be quiet in a library.
First, according to Deﬁnition 3 it holds that
Ds, ws |=MDL Oq. Furthermore,
assume that
τ
is deﬁned in the way such that
Ds′, ws′ |=MDL Oq and
Ds′′, ws′′ |=MDL Oq, but Ds′′′, ws′′′ ̸|=MDL Oq. Then μ(∥Oq∥) = μ({s, s′, s′′}) =
0.5 + 0.2 + 0.2 = 0.9 . According to Deﬁnition 6, M |= w(Oq) ≥0.9.
Note that, according to Deﬁnition 6, a deontic formula is true in a model iﬀit
holds in every state of the model. This is a consequence of our design choice that
those formulas represent certain deontic knowledge, while probabilistic formulas
express uncertainty about norms. At the end of this section, we deﬁne some
standard semantical notions.
Deﬁnition 7 (Semantical consequence). Given a set Γ of formulas, a for-
mula θ is a semantical consequence of Γ (notation: Γ |= θ) whenever, all the
states of the model have, if M, s |= θ′ for all θ′ ∈Γ, then M, s |= θ.
Deﬁnition 8 (Validity). A formula θ is valid (notations: |= θ) whenever for
M = ⟨S, X , μ, τ⟩and every s ∈S: M, s |= θ holds.

A Probabilistic Deontic Logic
621
3
Axiomatization
The following axiomatization contains 13 axioms and 3 inference rules. It com-
bines the axioms of proof system D of monadic deontic logic [9] with the axioms
of probabilistic logic. The axioms for reasoning about linear inequalities are taken
form [4].
The Axiomatic System: AXPDL
Tautologies and Modus Ponens
Taut. All instances of propositional tautologies.
MP. From θ and θ →θ′ infer θ′.
Reasoning with O:
O-K. O(φ →ψ) →(Oφ →Oψ)
O-D. Oφ →Pφ
O-Nec. From φ infer Oφ.
Reasoning About Linear Inequalities:
I1. x ≥x (identity)
I2. (a1x1 + ... + akxk ≥c) ↔(a1x1 + ... + akxk + 0xk+1 ≥c) (adding and
deleting 0 terms)
I3. (a1x1 + ... + akxk ≥c) →(aj1xj1 + ... + ajkxjk ≥c), if j1, ..., jk is a
permutation of 1, ..., k (permutation)
I4. (a1x1 + ... + akxk ≥c) ∧(a′
1x1 + ... + a′
kxk ≥c′) →((a1 + a′
1)x1 + ... +
(ak + a′
k)xk ≥(c + c′)) (addition of coeﬃcients)
I5. (a1x1 + ... + akxk ≥c) ↔(da1x1 + ... + dakxk ≥dc) if d > 0 (multipli-
cation of non-zero coeﬃcients)
I6. (t ≥c) ∨(t ≤c) if t is a term (dichotomy)
I7. (t ≥c) →(t > d) if t is a term and c > d (monotonicity).
Reasoning About Probabilities:
W1. w(φ) ≥0 (nonnegativity).
W2. w(φ ∨ψ) = w(φ) + w(ψ), if ¬(φ ∧ψ) is an instance of a classical propo-
sitional tautology (ﬁnite additivity).
W3. w(⊤) = 1
P-Dis. From φ ↔ψ infer w(φ) = w(ψ) (probabilistic distributivity).
The axiom Taut allows all Ldeontic-instances and Lprob−d-instances of propo-
sitional tautologies. For example, w(Oq) ≥0.9 ∨¬w(Oq) ≥0.9 is an instance of
Taut, but w(Oq) ≥0.9∨¬w(Oq) ≥1 is not. Note that Modus Ponens (MP) can
be applied to both types of formulas, but only if θ and θ′ are both from Ldeontic
or both from Lprob−d. O-Nec is a deontic variant of necessitation rule. P-Dis is
an inference rule which states that two equivalent deontic formulas must have
the same probability values.

622
V. de Wit et al.
Deﬁnition 9 (Syntactical consequence). A derivation of θ is a ﬁnite
sequence θ1, . . . , θm of formulas such that θm = θ, and every θi is either an
instance of an axiom, or it is obtained by the application of an inference rule
to formulas in the sequence that appear before θi. If there is a derivation of θ,
we say that θ is a theorem and write ⊢θ. We also say that θ is derivable from
a set of formulas Γ, and write Γ ⊢θ, if there is a ﬁnite sequence θ1, . . . , θm of
formulas such that θm = θ, and every θi is either a theorem, a member of Γ, or
the result of an application of MP. or P-Nec. to formulas in the sequence that
appear before θi.
Note that this deﬁnition restricts the application of O-Nec. to theorems only.
This is a standard restriction for modal necessitations, which enables one to
prove Deduction theorem using induction on the length of the inference. Also,
note that only deontic formulas can participate in a proof of another deontic
formula, thus derivations of deontic formulas in our logic coincide with their
derivations in MDL.
Deﬁnition 10 (Consistency). A set Γ is consistent if Γ ̸⊢⊥, and inconsistent
otherwise.
Now we prove some basic consequences of AXPDL. The ﬁrst one is probabilis-
tic variant of necessitation. It captures the semantical property that a deontic
formula represents certain knowledge, and therefore it must have probability
value 1. The third part of the lemma shows that a form of additivity proposed
as an axiom in [4] is provable in AXPDL.
Lemma 1. The following rules are derivable from our axiomatization:
1. From φ infer w(φ) = 1
2. ⊢w(⊥) = 0
3. ⊢w(φ ∧ψ) + w(φ ∧¬ψ) = w(φ).
Proof.
1. Let us assume that a formula φ is derived. Then, using propositional reasoning
(Taut and MP), one can infer φ ↔⊤. Consequently, w(φ) = w(⊤) follows
from the rule P-Dis. Since we have that w(⊤) = 1 (by W3), we can employ
the axioms for reasoning about inequalities to infer w(φ) = 1.
2. Then to show that w(⊥) = 0 using ﬁnite additivity (W2) w(⊤∨¬⊤) =
w(⊤) + w(¬⊤) = 1 and so w(¬⊤) = 1 −w(⊤). Since w(⊤) = 1 and ¬⊤↔⊥
we can derive w(⊥) = 0.
3. To derive additivity we begin with the propositional tautology, ¬((φ ∧ψ) ∧
(φ∧¬ψ)) then the following equation is given by W2 w(φ∧ψ)+w(φ∧¬ψ) =
w((φ ∧ψ) ∨(φ ∧¬ψ)). The disjunction (φ ∧ψ) ∨(φ ∧¬ψ) can be rewritten
to, φ ∧(ψ ∨¬ψ) which is equivalent to φ. From φ ↔(φ ∧ψ) ∨(φ ∧¬ψ), using
P-Dis, we obtain w(φ) = w(φ ∧ψ) + w(φ ∧¬ψ).

A Probabilistic Deontic Logic
623
4
Soundness and Completeness
In this section, we prove that our logic is sound and complete with respect to
the class of measurable models, combining and adapting the approaches from
[2,4].
Theorem 1 (Soundness & Completeness). The axiom system AXPDL is
sound and complete with respect to the class of measurable probabilistic deontic
models. i.e., ⊢θ iﬀ|= θ.
Proof. The proof of soundness is straightforward. To prove completeness, we
need to show that every consistent formula θ is satisﬁed in a measurable model.
Since we have two types of formulas, we distinguish two cases.
If θ ∈Ldeontic we write θ as φ. Since φ is consistent and monadic deontic
logic is complete [9], we know that there is a MDL model (W, R, V ) and w ∈W
such that (W, R, V ), w |= φ. Then, for any probabilistic deontic model M with
only one state s and τ(s) = ((W, R, V ), w) we have M, s |= φ, and therefore
M |= φ (since s is the only state); so the formula is satisﬁable.
When θ ∈Lprob−d we write θ as f, and assuming consistency of, f we need
to prove that it is satisﬁable. First notice that f can be equivalently rewritten
as a formula in disjunctive normal form,
f ↔g1 ∨· · · ∨gn
this means that satisﬁability of f can be proven by showing that one of the
disjuncts gi of the disjunctive normal form of f is satisﬁable. Note that every
disjunct is of the form
gi =
r
j=1
(

k
aj,kw(φj,k) ≥cj) ∧
r+s

j=r+1
¬(

k
aj,kw(φj,k) ≥cj)
In order to show that gi is satisﬁable we will substitute each weight term w(φj,k)
by a sum of weight terms that take as arguments formulas from the set Δ that will
be constructed below. For any formula θ, let us denote the set of subformulas
of θ by Sub(θ). Then, for considered, gi we introduce the set of all deontic
subformulas SubDL(gi) = Sub(gi) ∩Ldeontic. We create the set Δ as the set of
all possible formulas that are conjunctions of formulas from SubDL(gi) ∪{¬e |
e ∈SubDL(gi)}, such that for every e either e or ¬e is taken as a conjunct (but
not both). Then we can prove the following two claims about the set Δ:
• The conjunction of any two diﬀerent formulas δk and δl from Δ is inconsistent:
⊢¬(δk∧δl). This is the case because for each pair of δ’s at least one subformula
e ∈Sub(φ) such that δk ∧δl ⊢e ∧¬e and e ∧¬e ⊢⊥. If there is no such, e
then by construction δk = δl.
• The disjunction of all δ’s in Δ is a tautology: ⊢
δ∈Δ δ. Indeed, it is clear
from the way the set Δ is constructed, that the disjunction of all formulas is
an instance of a propositional tautology.

624
V. de Wit et al.
As noted earlier, we will substitute each term of each weight formula of gi by a
sum of weight terms. This can be done by using the just introduced set Δ and
the set Φ, which we deﬁne as the set containing all deontic formulas φj,k that
occur in the weight terms of gi. In order to get all the relevant δ’s to represent
a weight term, we construct for each φ ∈Φ the set Δφ = {δ ∈Δ | δ ⊢φ} which
contains all δ’s that imply φ. Then we can derive the following equivalence:
⊢φ ↔

δ∈Δφ
δ.
From the rule P-Dis we obtain
⊢w(φ) = w(

δ∈Δφ
δ).
Since any two elements of Δ are inconsistent, from W2 and axioms about inequal-
ities we obtain ⊢w(
δ∈Δφ δ) = 
δ∈Δφ w(δ). Consequently, we have
⊢w(φ) =

δ∈Δφ
w(δ).
Note that some of the formulas δ’s might be inconsistent (for example, a
formula from Δ might be a conjunction in which both Op and F(p ∧q) appear
as conjuncts). For an inconsistent formula δ, we have ⊢δ ↔⊥and, consequently
⊢w(δ) = 0, by the inference rule P-Dis. This can provably ﬁlter out the incon-
sistent δ’s from each weight formula, using the axioms about linear inequalities.
Thus, without any loss of generality, we can assume in the rest of the proof that
all the formulas from Δ are consistent1.
Lets us consider a new formula f ′, created by substituting each term of each
weight formula of gi:
f ′ =
⎛
⎝
r
j=1
(

k
aj,k

δ∈Δφj,k
w(δ) ≥cj)
⎞
⎠∧
⎛
⎝
r+s

j=r+1
¬(

k
aj,k

δ∈Δφj,k
w(δ) ≥cj)
⎞
⎠
Then we will construct f ′′ by adding to f ′: a non-negativity constraint and an
equality that binds the total probability weight of δ’s to 1. In other words, f ′′ is
the conjunction of the following formulas:
1 We might introduce Δc and Δc
φ as the sets of all consistent formulas from Δ and
Δφ, respectively, but since we will still have ⊢w(φ) = 
δ∈Δc
φ w(δ), we prefer not to
burden the notation with the superscripts in the rest of the proof, and we assume
that we do not have inconsistent formulas in Δ.

A Probabilistic Deontic Logic
625

δ∈Δ
w(δ) = 1
∀δ ∈Δ
w(δ) ≥0
∀l ∈{1, . . . , r}

k
al,k

δ∈Δφl,k
w(δ) ≥cl
∀l ∈{r + 1, . . . , r + s}

k
al,k

δ∈Δφl,k
w(δ) < cl
Since the weights can be attributed independently while respecting the system
of equations, the formula f ′′ is satisﬁable if the following system of equations is
solvable. With I = {1, . . . , |Δ|}:
|Δ|

i=1
xi = 1
∀i ∈I
xi ≥0
∀l ∈{1, . . . , r}

k
al,k
|Δφl,k |

i=1
xi ≥cl
∀l ∈{r + 1, . . . , r + s}

k
al,k
|Δφl,k |

i=1
xi < cl
Each δ can be identiﬁed as a state in the universe of the probability structure.
Since MDL is complete, each state in the probability structure corresponds with
a pointed deontic model’s state via the identiﬁcation function τ. Furthermore,
w() abides to the rules of probability measures due to the axiom system. This
means that to prove satisﬁability, only a probability measure should be found
that corresponds with the representation f ′. By adding the constraints to the
representation, we can ﬁnd a probability measure by solving the system of linear
inequalities f ′′ using the axioms for reasoning with inequalities I1-I7. We took f
in the beginning of the proof to be a consistent formula and f is either satisﬁable
or unsatisﬁable. When the system can be shown to be satisﬁable we have proven
completeness, satisﬁability of f is proven when satisﬁability of f ′′ is shown. This
is the case because if f ′′ is satisﬁable then so is f ′ which means gi is satisﬁable
and if gi is satisﬁable then f is satisﬁable. Assume f ′′ is unsatisﬁable then ¬f ′′ is
provable from the axioms I1-I7. As just explained f’s satisﬁability is equivalent
to that of f ′′. Then ¬f is provable, which means that f is inconsistent. This is
a contradiction, and therefore we have to reject that f ′′ is unsatisﬁable and to
conclude that f is satisﬁable.

626
V. de Wit et al.
5
Decidability
In this section, we prove that our logic is decidable. First, let us recall the
satisﬁability problem: given a formula θ, we want to determine if there exists a
model M such that M |= θ.
Theorem 2 (Decidability). Satisﬁability problem for PDL is decidable.
Proof. (Sketch). Since we have two types of formulas, we will consider two cases.
First, let us assume that θ ∈Ldeontic. We start with the well-known result
that the problem of whether a formula from Ldeontic is satisﬁable in a standard
monadic deontic model is decidable. It is suﬃcient to show that each θ ∈Ldeontic
is satisﬁable in a monadic deontic model iﬀit is satisﬁable under our semantics.
First, if (W ′, R′, V ′), w′ |= θ for some deontic model (W ′, R′, V ′) and w′ ∈W ′, let
us construct the model M = ⟨S, X , μ, τ⟩, with S = {s}, X = {∅, S}, μ(S) = 1
and τ(s) = ((W ′, R′, V ′), w′). Since (W ′, R′, V ′), w′ |= θ, then M, s |= θ. From
the fact that s is the unique state of M, we conclude that M |= θ. On the other
hand, if θ is not satisﬁable in standard monadic deontic logic, then for every
M = ⟨S, X , μ, τ⟩and s ∈S we will have M, s ̸|= θ, so M ̸|= θ.
Now, let us consider the case θ ∈Lprob−d. In the proof, we use the method of
ﬁltration [2,8], and reduction to ﬁnite systems of inequalities. We only provide a
sketch of the proof, since we use similar ideas as in our completeness proof. We
will also use notation introduced in the proof of completeness. In the ﬁrst part
of the proof, we show that a formula is satisﬁable iﬀit is satisﬁable in a model
with a ﬁnite number of (1) states and (2) worlds.
(1) First we show that if θ ∈Lprob−d is satisﬁable, then it is satisﬁable
in a model with a ﬁnite set of states, whose size is at most 2|SubDL(θ)| (where
SubDL(θ) is the set of deontic subformulas of θ, as deﬁned in the proof of The-
orem 1). Let M = ⟨S, X , μ, τ⟩be a model such that M |= θ. Let us deﬁne by
∼the equivalence relation over S × S in the following way: s ∼s′ iﬀfor every
φ ∈SubDL(θ), M, s |= φ iﬀs′ |= φ. Then the corresponding quotient set S/∼is
ﬁnite and |S/∼| ≤2|SubDL(θ)|. Note that every Ci belongs to X , since it corre-
sponds to a formula δi of Δ (from the proof of Theorem 1), i.e., Ci = ∥δi∥. Next,
for every equivalence class, Ci we choose one element and denote it si. Then we
consider the model M ′ = ⟨S′, X ′, μ′, τ ′⟩, where:
• S′ = {si | Ci ∈S/∼},
• X ′ is the power set of S′,
• μ′({si}) = μ(Ci) such that si ∈Ci and for any X
⊆S′, μ′(X) =

si∈X μ′({si}),
• τ ′(si) = τ(si).
Then it is straightforward to verify that M ′ |= θ. Moreover, note that, by deﬁ-
nition of M ′, for every si ∈S there is δi ∈Δ such that M ′, si |= δi, and that for
every sj ̸= si we have M ′, sj ̸|= δi. We therefore say that δi is the characteristic
formula of si.

A Probabilistic Deontic Logic
627
(2) Even if S′ is ﬁnite, some sets of worlds attached to a state might be
inﬁnite. Now we will modify τ ′, in order to ensure that every W(si) is ﬁnite, and
of the size which is bounded by a number that depends on the size of θ. In this
part of the proof we refer to the ﬁltration method used to prove completeness
of MDL [2], which shows that if a deontic formula φ is satisﬁable, that it is
satisﬁed in a world of a model D(ψ) = (W, R, V ) where the size of W is at
most exponential wrt. the size of the set of subformulas of φ. Then we can
replace τ ′ with a function τ ′′ which assigns to each si one such D(δi) and the
corresponding world, where δi is the characteristic formula of si. We also assume
that each V (si) is restricted to the propositional letters from SubDL(θ). Finally,
let M ′′ = ⟨S′, X ′, μ′, τ ′′⟩It is easy to check that for every φ ∈SubDL(θ) and
si ∈S′, M ′, si |= φ iﬀM ′′, si |= φ. Therefore, M ′′ |= θ.
From the steps (1) and (2) it follows that in order to check if a formula
θ ∈Lprob−d is satisﬁable, it is enough to check if it is satisﬁed in a model
M = ⟨S, X , μ, τ⟩in which S and each Ws (for every s ∈S) are of ﬁnite size,
bounded from above by a ﬁxed number depending on the size of |SubDL(θ)|.
Then there are ﬁnitely many options for the choice of S and τ (i.e., (Ds, ws),
for every s ∈S), and our procedure can check in ﬁnite time whether there is
a probability measure μ for some of them, such that θ holds in the model. We
guess S and τ and check whether we can assign probability values to the states
from S, using translation to a system of linear inequalities, in the same way as we
have done in the proof of Theorem 1. This ﬁnishes the proof, since the problem
of checking whether a linear system of inequalities has a solution is decidable.
6
Conclusion
In this article, we introduced the probabilistic deontic logic PDL, a logic in
which we can reason about the probability of deontic statements. We proposed
a language that extends both monadic deontic logic and probability logic from
[4]. We axiomatized that language and proved soundness and completeness with
respect to corresponding semantics. We also proved that our logic is decidable.
To the best of our knowledge, we are the ﬁrst to propose a logic for reasoning
about probabilistic uncertainty about norms. It is worth mentioning that there is
a recent knowledge representation framework about probabilistic uncertainty in
deontic reasoning obtained by merging deontic argumentation and probabilistic
argumentation frameworks [10].
Our logic PDL used MDL as the underlying framework, we used this logic
simply because it is one of the most studied deontic logics. On the other hand,
MDL is also criticized because of some issues, like representation of contrary-to-
duty obligations. It is important to point out that the axiomatization technique
developed in this work can be also applied if we replace MDL with, for exam-
ple, dyadic deontic logic, simply by changing the set of deontic axioms and
the function τ in the deﬁnition of model, which would lead to a more expres-
sive framework for reasoning about uncertain norms. Another avenue for future
research is to extend the language by allowing conditional probabilities. In such

628
V. de Wit et al.
a logic, it would be possible to express that one uncertain norm becomes more
certain if another norm is accepted or learned.
References
1. Boella, G., van der Torre, L.W.N., Verhagen, H.: Introduction to normative multi-
agent systems. Comput. Math. Organ. Theor. 12(2–3), 71–79 (2006). https://doi.
org/10.1007/s10588-006-9537-7
2. Chellas, B.F.: Modal Logic: An Introduction. Cambridge University Press (1980).
https://doi.org/10.1017/CBO9780511621192
3. Fagin, R., Halpern, J.Y.: Reasoning about knowledge and probability. J. ACM
41(2), 340–367 (1994)
4. Fagin, R., Halpern, J.Y., Megiddo, N.: A logic for reasoning about probabilities.
Inf. Computat. 87(1), 78–128 (1990)
5. Frisch, A., Haddawy, P.: Anytime deduction for probabilistic logic. Artif. Intell.
69, 93–122 (1994)
6. van der Hoek, W.: Some considerations on the logic pfd. J. Appl. Non Class. Logics
7(3), 287–307 (1997)
7. Horty, J.F.: Agency and Deontic Logic. Oxford University Press (2001)
8. Hughes, G.E., Cresswell, M.J.: A Companion to Modal Logic. Methuen London,
New York (1984)
9. Parent, X., Van Der Torre, L.: Introduction to Deontic Logic and Normative Sys-
tems. Texts in Logic and Reasoning. College Publications (2018). https://books.
google.nl/books?id=IyUYwQEACAAJ
10. Riveret, R., Oren, N., Sartor, G.: A probabilistic deontic argumentation framework.
Int. J. Approximate Reason. 126, 249–271 (2020)
11. Sarathy, V., Scheutz, M., Malle, B.F.: Learning behavioral norms in uncertain
and changing contexts. In: 2017 8th IEEE International Conference on Cognitive
Infocommunications (CogInfoCom), pp. 000301–000306 (2017). https://doi.org/
10.1109/CogInfoCom.2017.8268261
12. Savic, N., Doder, D., Ognjanovic, Z.: Logics with lower and upper probability
operators. Int. J. Approx. Reason. 88, 148–168 (2017)
13. Tomic, S., Pecora, F., Saﬃotti, A.: Learning normative behaviors through abstrac-
tion. In: Giacomo, G.D. (eds.) 24th European Conference on Artiﬁcial Intelligence,
ECAI 2020, 29 August–8 September 2020, Santiago de Compostela, Spain, Includ-
ing 10th Conference on Prestigious Applications of Artiﬁcial Intelligence (PAIS
2020). Frontiers in Artiﬁcial Intelligence and Applications, vol. 325, pp. 1547–1554.
IOS Press (2020). https://doi.org/10.3233/FAIA200263
14. von Wrigth, G.H.: I. Deontic logic. Mind LX(237), 1–15 (1951). https://doi.org/
10.1093/mind/LX.237.1

Iterated Conditionals and
Characterization of P-Entailment
Angelo Gilio1 and Giuseppe Sanﬁlippo2(B)
1 Department SBAI, University of Rome “La Sapienza”, Rome, Italy
angelo.gilio@sbai.uniroma1.it
2 Department of Mathematics and Computer Science,
University of Palermo, Palermo, Italy
giuseppe.sanfilippo@unipa.it
Abstract. In this paper we deepen, in the setting of coherence, some
results obtained in recent papers on the notion of p-entailment of Adams
and its relationship with conjoined and iterated conditionals. We recall
that conjoined and iterated conditionals are suitably deﬁned in the
framework of conditional random quantities. Given a family F of n con-
ditional events {E1|H1, . . . , En|Hn} we denote by C(F) “ (E1|H1) ^
· · · ^ (En|Hn) the conjunction of the conditional events in F. We intro-
duce the iterated conditional C(F2)|C(F1), where F1 and F2 are two
ﬁnite families of conditional events, by showing that the prevision of
C(F2) ^ C(F1) is the product of the prevision of C(F2)|C(F1) and the
prevision of C(F1). Likewise the well known equality (A ^ H)|H “ A|H,
we show that (C(F2) ^ C(F1))|C(F1) “ C(F2)|C(F1). Then, we consider
the case F1 “ F2 “ F and we verify for the prevision µ of C(F)|C(F)
that the unique coherent assessment is µ “ 1 and, as a consequence,
C(F)|C(F) coincides with the constant 1. Finally, by assuming F p-
consistent, we deepen some previous characterizations of p-entailment
by showing that F p-entails a conditional event En`1|Hn`1 if and only
if the iterated conditional (En`1|Hn`1) | C(F) is constant and equal to
1. We illustrate this characterization by an example related with weak
transitivity.
Keywords: Coherence · Conditional events · Conditional random
quantities · Conditional previsions · Conjoined conditionals · Iterated
conditionals · Probabilistic entailment
1
Introduction
The study of logical operations among conditional events has been considered
in many papers (see, e.g., [2,11,12,16,21,37,39,42,43]). In a pioneering paper,
written in 1935, de Finetti [20] proposed a three-valued logic for conditional
A. Gilio and G. Sanﬁlippo—Both authors contributed equally to the article and are
listed alphabetically.
A. Gilio—Retired.
c
⃝Springer Nature Switzerland AG 2021
J. Vejnarov´a and N. Wilson (Eds.): ECSQARU 2021, LNAI 12897, pp. 629–643, 2021.
https://doi.org/10.1007/978-3-030-86772-0_45

630
A. Gilio and G. Sanﬁlippo
events. Many often, conjunctions and disjunctions have been deﬁned as suitable
conditional events (see, e.g., [1,4–7,37]). However, in this way classical proba-
bilistic properties are lost. For instance, the lower and upper probability bounds
for the conjunction are no more the Fr´echet-Hoeﬀding bounds [45]. A more gen-
eral approach to conjunction has been given in [39,42] and, in the setting of
coherence, in [27,28,31–33], where also the notion of iterated conditional has
been studied. In these papers the notions of compound and iterated condition-
als are deﬁned as suitable conditional random quantities with a ﬁnite number
of possible values in the interval [0, 1]. The main relevance of our approach
is theoretical: indeed, all the basic probabilistic properties are preserved (for
a synthesis see [36]). For instance, De Morgan’s Laws are satisﬁed [33] and the
Fr´echet-Hoeﬀding bounds for the conjunction of conditional events still hold [35].
A suitable notion of conditional constituent can be introduced, with properties
analogous to the case of unconditional events; moreover, a generalized inclusion-
exclusion formula for the disjunction of conditional events is valid [34]. We also
recall that the Lewis’ triviality results [41] are avoided because in our theory the
Import-Export Principle is not valid (see [31,46,48]). For some applications of
compound and iterated conditionals see, e.g., [24,26,46–48]. More speciﬁcally,
by exploiting iterated conditionals, the probabilistic modus ponens has been
generalized to conditional events [47]; one-premise and two-premise centering
inferences, related to the notion of centering used in Lewis’ logic [40], has been
examined in [24,48]. In [46] several (generalized) iterated conditionals have been
considered, in order to properly formalize diﬀerent kinds of latent information;
in particular, some intuitive probabilistic assessments discussed in [15] have been
explained, by making explicit some background information.
An interesting aspect which could be possibly investigated concerns the rela-
tionship of our notions of compound and iterated conditionals with other topics
of research, such as belief and plausibility functions, data fusion, inductive rea-
soning, and fuzzy logic [8–10,13,17–19,38,44,49]. For instance, by recalling [10],
an application of our notion of conjunction could be given by interpreting the
membership function of the cartesian product of fuzzy sets as the prevision of
conjoined conditionals.
By exploiting conjunction a characterization of the probabilistic entailment
of Adams [1] for conditionals has been given in [33]. Moreover, by exploit-
ing iterated conditionals, the p-entailment of E3|H3 from a p-consistent family
F “ {E1|H1, E2|H2} has been characterized by the property that the iterated
conditional (E3|H3)|((E1|H1) ^ (E2|H2)) is constant and coincides with 1 [26].
In this paper, based on a general notion of iterated conditional, we extend this
characterization of p-entailment by considering the case where F is a family of
n conditional events.
The paper is organized as follows. After recalling in Sect. 2 some preliminary
notions and results, in Sect. 3 we introduce the iterated conditional C(F2)|C(F1),
where C(F1) and C(F2) are the conjunctions of the conditional events in two ﬁnite
families F1 and F2. We show that (C(F2) ^ C(F1))|C(F1) “ C(F2)|C(F1) and
P[C(F2) ^ C(F1)] “ P[C(F2)|C(F1)]P[C(F1)]. Then, we prove that C(F)|C(F)
is constant and coincides with 1. In Sect. 3, by assuming F p-consistent, we
characterize the p-entailment of En`1|Hn`1 from F by the property that the

Iterated Conditionals and Characterization of P-Entailment
631
iterated conditional (En`1|Hn`1)|C(F) is constant and coincides with 1. We also
illustrate this characterization by an example related with weak transitivity.
2
Preliminary Notions and Results
An event A is a two-valued logical entity which is either true, or false. We use
the same symbol to refer to an event and its indicator. We denote by Ω the sure
event and by H the impossible one. We denote by A ^ B (resp., A _ B), or
simply by AB, the conjunction (resp., disjunction) of A and B. By ¯A we denote
the negation of A. We simply write A Ď B to denote that A logically implies
B. Given two events A and H, with H ̸“ H, the conditional event A|H is a
three-valued logical entity which is true, or false, or void, according to whether
AH is true, or ¯AH is true, or ¯H is true, respectively. The negation A|H of A|H
is deﬁned as ¯A|H.
In the betting framework, to assess P(A|H) “ x amounts to say that, for
every real number s, you are willing to pay an amount s x and to receive s, or 0,
or s x, according to whether AH is true, or ¯AH is true, or ¯H is true (bet called
oﬀ), respectively. Hence, for the random gain G “ sH(A´x), the possible values
are s(1 ´ x), or ´s x, or 0, according to whether AH is true, or ¯AH is true, or
¯H is true, respectively. We denote by X a random quantity, that is an uncertain
real quantity, which has a well determined but unknown value. We assume that
X has a ﬁnite set of possible values. Given any event H ̸“ H, agreeing to the
betting metaphor, if you assess that the prevision of “X conditional on H” (or
short: “X given H”), P(X|H), is equal to μ, this means that for any given real
number s you are willing to pay an amount sμ and to receive sX, or sμ, according
to whether H is true, or false (bet called oﬀ), respectively. In particular, when
X is (the indicator of) an event A, then P(X|H) “ P(A|H). Given a conditional
event A|H with P(A|H) “ x, the indicator of A|H, denoted by the same symbol,
is
A|H “ AH ` x ¯H “ AH ` x(1 ´ H) “
⎧
⎨
⎩
1, if AH is true,
0, if ¯AH is true,
x, if ¯H is true.
(1)
Notice that, denoting by P the prevision, it holds that P(AH ` x ¯H) “ xP(H) `
xP( ¯H) “ x. The third value of the random quantity A|H (subjectively) depends
on the assessed probability P(A|H) “ x. When H Ď A (i.e., AH “ H), it holds
that P(A|H) “ 1; then, for the indicator A|H it holds that
A|H “ AH ` x ¯H “ H ` ¯H “ 1, (when H Ď A).
(2)
Likewise, if AH “ H, it holds that P(A|H) “ 0; then
A|H “ 0 ` 0 ¯H “ 0, (when AH “ H).
For the indicator of the negation of A|H it holds that ¯A|H “ 1 ´ A|H. Given a
random quantity X and an event H ̸“ H, with a prevision assessment P(X|H) “
μ, in our approach, likewise formula (1), the conditional random quantity X|H

632
A. Gilio and G. Sanﬁlippo
is deﬁned as X|H “ XH `μ ¯H. Notice that P(XH `μ ¯H) “ P(XH)`μP( ¯H) “
μP(H) ` μP( ¯H) “ μ. For a discussion on this extended notion of a conditional
random quantity and on the notion of coherence of a prevision assessment see,
e.g., [31,34,46]. In betting terms coherence means that in any ﬁnite combination
of n bets, it cannot happen that, after discarding the cases where the bet is called
oﬀ, the values of the random gain are all positive, or all negative (Dutch Book).
Remark 1. Given a conditional random quantity X|H and a prevision assess-
ment P(X|H) “ μ, if conditionally on H being true X is constant, say X “ c,
then by coherence μ “ c.
Probabilistic Consistency and Probabilistic Entailment. We recall the notions of
p-consistency and p-entailment of Adams [1] formulated for conditional events in
the setting of coherence in [30] (see also [3,23,29]). For a discussion on deduction
from uncertain premises and p-validity, under coherence, see [14].
Deﬁnition 1. Let Fn “ {Ei|Hi ,
i “ 1, . . . , n} be a family of n condi-
tional events. Then, Fn is p-consistent if and only if the probability assessment
(p1, p2, . . . , pn) “ (1, 1, . . . , 1) on Fn is coherent.
Deﬁnition 2. A p-consistent family Fn “ {Ei|Hi , i “ 1, . . . , n} p-entails a
conditional event E|H (denoted by Fn ñp E|H) if and only if for any coherent
probability assessment (p1, . . . , pn, z) on Fn Y {E|H} it holds that: if p1 “ · · · “
pn “ 1, then z “ 1.
The inference from Fn to E|H is p-valid if and only if Fn ñp E|H [1].
Logical operations among conditional events. We recall below the notion of con-
junction of two conditional events [31].
Deﬁnition 3. Given any pair of conditional events E1|H1 and E2|H2, with
P(E1|H1) “ x1 and P(E2|H2) “ x2, their conjunction (E1|H1) ^ (E2|H2) is
the conditional random quantity deﬁned as
(E1|H1) ^ (E2|H2) “ (E1H1E2H2 ` x1 ¯H1E2H2 ` x2 ¯H2E1H1)|(H1 _ H2)
“
⎧
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎩
1,
if E1H1E2H2 is true,
0,
if ¯E1H1 _ ¯E2H2 is true,
x1, if ¯H1E2H2 is true,
x2, if ¯H2E1H1 is true,
x12, if ¯H1 ¯H2 is true,
(3)
where x12
“
P[(E1|H1) ^ (E2|H2)]
“
P[(E1H1E2H2 ` x1 ¯H1E2H2 `
x2 ¯H2E1H1)|(H1 _ H2)].
In betting terms, the prevision x12 represents the amount you agree to pay,
with the proviso that you will receive the quantity E1H1E2H2 ` x1 ¯H1E2H2 `
x2 ¯H2E1H1, or you will receive back the quantity x12, according to whether
H1 _H2 is true, or ¯H1 ¯H2 is true. Notice that, diﬀerently from conditional events
which are three-valued objects, the conjunction (E1|H1) ^ (E2|H2) is no longer
a three-valued object, but a ﬁve-valued object with values in [0, 1]. We recall
below the notion of conjunction of n conditional events.

Iterated Conditionals and Characterization of P-Entailment
633
Deﬁnition 4. Let n conditional events E1|H1, . . . , En|Hn be given. For each
non-empty strict subset S of {1, . . . , n}, let xS be a prevision assessment on

iPS(Ei|Hi). Then, the conjunction (E1|H1) ^ · · · ^ (En|Hn) is the conditional
random quantity C1···n deﬁned as
C1···n “ [n
i“1 EiHi ` 
H̸“S⊂{1,2...,n} xS(
iPS ¯Hi) ^ (
i/P S EiHi)]|(n
i“1 Hi)
“
⎧
⎪
⎪
⎨
⎪
⎪
⎩
1,
if n
i“1 EiHi is true,
0,
if n
i“1 ¯EiHi is true,
xS,
if (
iPS ¯Hi) ^ (
i/P S EiHi) is true, H ̸“ S ⊂{1, 2 . . . , n},
x1···n, if n
i“1 ¯Hi is true,
(4)
where
x1···n “ x{1,...,n} “ P(C1···n)
“ P[(n
i“1 EiHi ` 
H̸“S⊂{1,2...,n} xS(
iPS ¯Hi) ^ (
i/PS EiHi))|(n
i“1 Hi)].
(5)
For n “ 1 we obtain C1 “ E1|H1. In Deﬁnition 4 each possible value xS of
C1···n, H ̸“ S ⊂{1, . . . , n}, is evaluated when deﬁning (in a previous step) the
conjunction CS “ 
iPS(Ei|Hi). Then, after the conditional prevision x1···n is
evaluated, C1···n is completely speciﬁed. Of course, we require coherence for the
prevision assessment (xS, H ̸“ S Ď {1, . . . , n}), so that C1···n P [0, 1]. In the
framework of the betting scheme, x1···n is the amount that you agree to pay
with the proviso that you will receive:
- the amount 1, if all conditional events are true;
- the amount 0, if at least one of the conditional events is false;
- the amount xS equal to the prevision of the conjunction of that conditional
events which are void, otherwise. In particular you receive back x1···n when all
conditional events are void.
As we can see from (4), the conjunction C1···n is (in general) a (2n ` 1)-valued
object because the number of nonempty subsets S, and hence the number of
possible values xS, is 2n ´1. We recall a result which shows that the prevision of
the conjunction on n conditional events satisﬁes the Fr`echet-Hoeﬀding bounds
[33, Theorem13].
Theorem 1. Let n conditional events E1|H1, . . . , En|Hn be given, with xi “
P(Ei|Hi), i “ 1, . . . , n and x1···n “ P(C1···n). Then
max{x1 ` · · · ` xn ´ n ` 1, 0} ď x1···n ď min{x1, . . . , xn}.
In [35, Theorem 10] we have shown, under logical independence, the sharpness
of the Fr`echet-Hoeﬀding bounds.
Remark 2. Given a ﬁnite family F of conditional events, their conjunction is
also denoted by C(F). We recall that in [33], given two ﬁnite families of con-
ditional events F1 and F2, the object C(F1) ^ C(F2) is deﬁned as C(F1 Y F2).
Then, conjunction satisﬁes the commutativity and associativity properties [33,
Propositions 1 and 2]. Moreover, the operation of conjunction satisﬁes the mono-
tonicity property [33, Theorem7], that is C1···n`1 ď C1···n. Then,
C(F1 Y F2) ď C(F1), C(F1 Y F2) ď C(F2).
(6)

634
A. Gilio and G. Sanﬁlippo
Iterated Conditioning. We now recall the notion of iterated conditional given in
[28]. Such notion has the structure |⃝“  ^ ⃝` P(|⃝)¯
⃝, where P denotes
the prevision, which reduces to formula (1) when  “ A and ⃝“ H.
Deﬁnition 5 (Iterated conditioning). Given any pair of conditional events
E1|H1 and E2|H2, with E1H1 ̸“ H, the iterated conditional (E2|H2)|(E1|H1) is
deﬁned as the conditional random quantity
(E2|H2)|(E1|H1) “ (E2|H2) ^ (E1|H1) ` μ ¯E1|H1,
(7)
where μ “ P[(E2|H2)|(E1|H1)].
Remark 3. Notice that we assumed that E1H1 ̸“ H to give a nontrivial meaning
to the notion of iterated conditional. Indeed, if E1H1 were equal to H, then
E1|H1 “ (E2|H2) ^ (E1|H1) “ 0 and ¯E1|H1 “ 1, from which it would follow
(E2|H2)|(E1|H1) “ (E2|H2)|0 “ (E2|H2) ^ (E1|H1) ` μ ¯E1|H1 “ μ; that is,
(E2|H2)|(E1|H1) would coincide with the (indeterminate) value μ. Similarly to
the case of a conditional event E|H, which is of no interest when H “ H, the
iterated conditional (E2|H2)|(E1|H1) is not considered in our approach when
E1H1 “ H.
Deﬁnition 5 has been generalized in [33] to the case where the antecedent is the
conjunction of more than two conditional events.
Deﬁnition 6. Let be given n`1 conditional events E1|H1, . . . , En`1|Hn`1, with
(E1|H1) ^ · · · ^ (En|Hn) ̸“ 0. We denote by (En`1|Hn`1)|((E1|H1) ^ · · · ^
(En|Hn)) “ (En`1|Hn`1)|C1···n the random quantity
(E1|H1) ^ · · · ^ (En`1|Hn`1) ` μ (1 ´ (E1|H1) ^ · · · ^ (En|Hn)) “
“ C1···n`1 ` μ (1 ´ C1···n),
where μ “ P[(En`1|Hn`1)|C1···n].
We observe that, based on the betting metaphor, the quantity μ is the amount
to be paid in order to receive the amount C1···n`1 ` μ (1 ´ C1···n). Deﬁnition
6 generalizes the notion of iterated conditional (E2|H2)|(E1|H1) given in pre-
vious papers (see, e.g., [27,28,31]). We also observe that, deﬁning P(C1···n) “
x1···n and P(C1···n`1) “ x1···n`1, by the linearity of prevision it holds that
μ “ x1···n`1 ` μ (1 ´ x1···n); then, x1···n`1 “ μ x1···n, that is P(C1···n`1) “
P[(En`1|Hn`1)|C1···n]P(C1···n).
Characterization of p-Consistency and p-Entailment. We recall a characteriza-
tion of p-consistency of a family F in terms of the coherence of the prevision
assessment P[C(F)] “ 1 [33, Theorem 17].
Theorem 2. A family of n conditional events F “ {E1|H1, . . . , En|Hn} is p-
consistent if and only if the prevision assessment P[C(F)] “ 1 is coherent.
We recall a characterization of p-entailment in terms of suitable conjunctions.
[33, Theorem 18].

Iterated Conditionals and Characterization of P-Entailment
635
Theorem 3. Let be given a p-consistent family of n conditional events F “
{E1|H1, . . . , En|Hn} and a further conditional event En`1|Hn`1. Then, the fol-
lowing assertions are equivalent:
(i) F p-entails En`1|Hn`1;
(ii) the conjunction (E1|H1) ^ · · · ^ (En|Hn) ^ (En`1|Hn`1) coincides with the
conjunction (E1|H1) ^ · · · ^ (En|Hn);
(iii) the inequality (E1|H1) ^ · · · ^ (En|Hn) ď En`1|Hn`1 is satisﬁed.
We recall a result where it is shown that the p-entailment of a conditional event
E3|H3 from a p-consistent family F “ {E1|H1, E2|H2} is equivalent to condition
(E3|H3)|((E1|H1) ^ (E2|H2)) “ 1 [26, Theorem 8].
Theorem 4. Let three conditional events E1|H1, E2|H2, and E3|H3 be given,
where {E1|H1, E2|H2} is p-consistent. Then, {E1|H1, E2|H2} p-entails E3|H3 if
and only if (E3|H3)|((E1|H1) ^ (E2|H2)) “ 1.
Theorem 4 will be generalized in Sect. 4.
3
A General Notion of Iterated Conditional
Let a family F “ {E1|H1, . . . , En|Hn} of n conditional events be given. More-
over, let M “ (xS : H ̸“ S Ď {1, . . . , n}) be a coherent prevision assess-
ment on the family {CS : H ̸“ S Ď {1, . . . , n}}, where CS “ 
iPS(Ei|Hi) and
xS “ P(CS). Denoting by Λ the set of possible values of C(F), it holds that
Λ Ď“ {1, 0, xS : H ̸“ S Ď {1, . . . , n}}. We observe that if xS “ 0 for some
S, then from (6), it holds that xS′ “ 0 for every S′ such that S ⊂S′. The
conjunction C(F) is constant and coincides with 0 when Λ “ {0}, in which
case we write C(F) “ 0. This happens when E1H1 · · · EnHn “ H and for each
H ̸“ S Ď {1, . . . , n} such that (
iPS ¯Hi)^(
i/PS EiHi) ̸“ H it holds that xS “ 0.
For instance, when E1H1 · · · EnHn “ H and x1 “ · · · “ xn “ 0, it holds that
xS “ 0 for every S; then, Λ “ {0} and C(F) coincides with the constant 0. We
give below a generalization of Deﬁnition 6.
Deﬁnition 7. Let F1 and F2 be two ﬁnite families of conditional events, with
C(F1) ̸“ 0. We denote by C(F2)|C(F1) the random quantity deﬁned as
C(F2)|C(F1) “ C(F2) ^ C(F1) ` μ(1 ´ C(F1)) “ C(F1 Y F2) ` μ(1 ´ C(F1)),
where μ “ P[C(F2)|C(F1)].
We
observe
that
Deﬁnition
7
reduces
to
Deﬁnition
6
when
F1
“
{E1|H1, . . . , En|Hn} and F2 “ {En`1|Hn`1}. We also remark that by linearity of
prevision it holds that μ “ P[C(F2)|C(F1)] “ P[C(F2)^C(F1)]`μ(1´P[C(F1)]),
that is
P[C(F2) ^ C(F1)] “ P[C(F2)|C(F1)]P[C(F1)].
(8)
Formula (8) generalizes the well known relation: P(AH) “ P(A|H)P(H) (com-
pound probability theorem). In the following result we obtain an equivalent rep-
resentation of C(F2)|C(F1).

636
A. Gilio and G. Sanﬁlippo
Theorem 5. Let F1 and F2 be two ﬁnite families of conditional events, with
C(F1) ̸“ 0. It holds that
C(F2)|C(F1) “ C(F1 Y F2)|C(F1) “ (C(F2) ^ C(F1))|C(F1).
(9)
Proof. We set μ′ “ P[C(F2)|C(F1)] and μ′′ “ P[C(F1 Y F2)|C(F1)]. Then,
C(F2)|C(F1) “ C(F1 Y F2) ` μ′ (1 ´ C(F1))
and
C(F1YF2)|C(F1) “ C(F1YF2YF1)`μ′′ (1´C(F1)) “ C(F1YF2)`μ′′ (1´C(F1)).
In order to prove (9) it is enough to verify that μ′ “ μ′′. We observe that
C(F2)|C(F1) ´ C(F1 Y F2)|C(F1) “ (μ′ ´ μ′′)(1 ´ C(F1)),
where μ′ ´ μ′′ “ P[C(F2)|C(F1) ´ C(F1 Y F2)|C(F1)]. Moreover, by setting F1 “
{E1|H1, . . . , En|Hn}, it holds that
(μ′ ´ μ′′)(1 ´ C(F1)) “
⎧
⎨
⎩
0, if C(F1) “ 1,
μ′ ´ μ′′, if C(F1) “ 0,
(μ′ ´ μ′′)(1 ´ xS), if 0 ă C(F1) “ xS ă 1,
where H ̸“ S Ď {1, . . . , n}. Within the betting framework, μ′´μ′′ is the amount
to be paid in order to receive the random amount (μ′ ´μ′′)(1´C(F1)). Then, as
a necessary condition of coherence, μ′ ´μ′′ must be a linear convex combination
of the possible values of (μ′ ´ μ′′)(1 ´ C(F1)) associated with the cases where
the bet is not called oﬀ, that is the cases where you do not receive back the paid
amount μ′ ´ μ′′. In other words, (as a necessary condition of coherence) μ′ ´ μ′′
must belong to the convex hull of the set {0, (μ′ ´μ′′)(1´xS) : 0 ă xS ă 1, H ̸“
S Ď {1, . . . , n}}. We observe that max{0, |μ′ ´μ′′|(1´xS)} ď |μ′ ´μ′′|, where as
xS P (0, 1) the equality holds if and only if μ′ ´μ′′ “ 0. Then, μ′ ´μ′′ belongs to
convex hull of the set {0, (μ′ ´ μ′′)(1 ´ xS) : 0 ă xS ă 1, H ̸“ S Ď {1, . . . , n}} if
and only if μ′ ´μ′′ “ 0, that is μ′ “ μ′′. Thus, C(F2)|C(F1) “ C(F1 YF2)|C(F1).
Finally, by recalling Remark 2, as C(F1 Y F2) “ C(F1) ^ C(F2), it follows that
C(F2)|C(F1) “ (C(F1) ^ C(F2))|C(F1).
⊓⊔
In particular, given any family F “ {E1|H1, . . . , En|Hn}, with C(F) ̸“ 0, and
any conditional event E|H, from (9) it follows that
C(F Y {E|H})|C(F) “ (E|H)|C(F).
(10)
Moreover, as F Y F “ F, it holds that
C(F)|C(F) “ C(F) ` μ(1 ´ C(F)),
(11)
where μ “ P[C(F)|C(F)]. In the next theorem we show that C(F)|C(F) “ μ “ 1.

Iterated Conditionals and Characterization of P-Entailment
637
Theorem 6. Let F “ {E1|H1, . . . , En|Hn} be a family of n conditional events,
with C(F) not equal to the constant 0. Then, C(F)|C(F) coincides with the con-
stant 1.
Proof. We set C0 “ ¯H1 · · · ¯Hn. We observe that when C0 is true, the value of
C(F) is x1···n and the value of C(F)|C(F) is x1···n ` μ(1 ´ x1···n). By linearity of
prevision, from (11) it holds that μ “ x1···n ` μ(1 ´ x1···n).
We denote by K the set of constituents Ch’s generated by F such that
Ch Ď H1 _ · · · _ Hn, that is Ch ̸“ C0. Then, we consider the partition
{K1, K0, K∗} of K as deﬁned below.
K1 “ {Ch : if Ch is true, then the value of C(F) is 1},
K0 “ {Ch : if Ch is true, then the value of C(F) is 0},
K∗“ {Ch : if Ch is true, then the value of C(F) is positive and less than 1}.
Notice that the set K1 also includes the constituents Ch’s such that C(F) “ xS,
with xS “ 1. The set K0 also includes the constituents Ch’s such that C(F) “ xS,
with xS “ 0. For each Ch P K∗, it holds that
Ch “ (
	
iPS
¯Hi) ^ (
	
i/PS
EiHi), for a suitable H ̸“ S Ď {1, . . . , n};
moreover, if Ch is true, then C(F) “ xS, with 0 ă xS ă 1. We observe that
K1YK∗̸“ H, because we assumed that C(F) does not coincide with the constant
0. Moreover, the value of C(F)|C(F) associated with a constituent Ch is 1, or
μ, or belongs to the set {xS ` μ(1 ´ xS) : 0 ă xS ă 1, H ̸“ S Ď {1, . . . , n}},
according to whether Ch P K1, or Ch P K0, or Ch P K∗, respectively.
By linearity of prevision, based on (11), it holds that
μ “ P[C(F)|C(F)] “ P[C(F)] ` μP[(1 ´ C(F)] “ x1···n ` μ(1 ´ x1···n) ,
from which it follows that μ x1···n “ x1···n.
We distinguish two cases: (a) x1···n ą 0; (b) x1···n “ 0.
(a). As x1···n ą 0, it holds that μ “ 1 and hence xS ` μ(1 ´ xS) “ 1, for every
S; therefore C(F)|C(F) coincides with the constant 1.
(b). In this case x1···n “ 0. If we bet on C(F)|C(F), we agree to pay its prevision
μ by receiving C(F)|C(F) “ C(F) ` μ(1 ´ C(F)), with the bet called oﬀwhen
you receive back the paid amount μ (whatever be μ). This happens when it is
true a constituent Ch P K0 Y {C0}, in which case the value of C(F) is 0, so that
C(F)`μ(1´C(F)) “ μ. Denoting by Γ the set of possible values of C (F)|C (F),
it holds that
C(F)|C(F) P Γ Ď V “ {1, μ, xS ` μ(1 ´ xS) : 0 ă xS ă 1, H ̸“ S Ď {1, . . . , n}}.
Then, as a necessary condition of coherence, μ must belong to the convex hull
of the set
Γ ∗“ Γ \ {μ} Ď V∗“ V \ {μ} “ {1, xS ` μ(1 ´ xS) : 0 ă xS ă 1, H ̸“ S Ď {1, . . . , n}}.
Notice that Γ ∗is the set of values of C(F)|C(F) associated with the constituents
Ch’s which belong to the nonempty set K1 YK∗. Moreover, if μ does not belong

638
A. Gilio and G. Sanﬁlippo
to the convex hull of V∗, then μ does not belong to the convex hull of Γ ∗. In
order μ be coherent it must belong to the convex hull of Γ ∗, that is it must be a
linear convex combination of the set of values in Γ ∗. We distinguish three cases:
(i) μ “ 1; (ii) μ ă 1; (iii) μ ą 1. In the case (i) it holds that xS `μ(1´xS) “ 1,
for every S; then Γ ∗“ V∗“ {1}, hence the prevision assessment μ “ 1 is
trivially coherent and C(F)|C(F) “ 1. In the case (ii) it holds that μ ă min V∗
and hence μ doesn’t belong to the convex hull of V∗; then μ doesn’t belong to
the convex hull of Γ ∗, that is the prevision assessment μ ă 1 is not coherent.
In the case (iii) it holds that μ ą max V∗and hence μ doesn’t belong to the
convex hull of V∗; then, μ doesn’t belong to the convex hull of Γ ∗, that is
the prevision assessment μ ą 1 is not coherent. Therefore, the unique coherent
prevision assessment on C(F)|C(F) is μ “ 1 and hence
C(F)|C(F) “ C(F) ` μ(1 ´ C(F)) “ C(F) ` 1 ´ C(F) “ 1 .
⊓⊔
In the next section we generalize Theorem 4, by characterizing the p-validity
of the inference from a premise set F “ {E1|H1, . . . , En|Hn} to the conclusion
En`1|Hn`1.
4
Characterization of P-Entailment in Terms of Iterated
Conditionals
We recall that, given a family F of n conditional events {E1|H1, . . . , En|Hn}, we
also denote by C1···n the conjunction C(F). Moreover, given a further conditional
event En`1|Hn`1 we denote by C1···n`1 the conjunction C(F Y{En`1|Hn`1}). In
other words,
C1···n “ (E1|H1) ^ · · · ^ (En|Hn), C1···n`1 “ (E1|H1) ^ · · · ^ (En`1|Hn`1).
We set P(C1···n) “ x1···n and P(C1···n`1) “ x1···n`1. Let us consider the iterated
conditional (En`1|Hn`1)|C1···n, which by Deﬁnition 7, is given by
(En`1|Hn`1)|C1···n “ C1···n`1 ` μ(1 ´ C1···n),
where μ “ P[(En`1|Hn`1)|C1···n]. In the next result, by assuming F p-consistent,
the p-entailment of En`1|Hn`1 from F is characterized in terms of the iterated
conditional (En`1|Hn`1)|C1···n.
Theorem 7. A p-consistent family F p-entails En`1|Hn`1 if and only if the
iterated conditional (En`1|Hn`1)|C1···n is equal to 1.
Proof. First of all we observe that, by Theorem 2, as F is p-consistent, the
assessment P(C1···n) “ 1 is coherent and hence C1···n ̸“ 0, so that the iterated
conditional (En`1|Hn`1)|C1···n makes sense. We consider the following assertions:
(i) F p-entails En`1|Hn`1; (ii) C1···n`1 “ C1···n; (iii) (En`1|Hn`1)|C1···n “ 1.

Iterated Conditionals and Characterization of P-Entailment
639
By Theorem 3, the conditions (i) and (ii) are equivalent, thus (i)
“ñ
(ii).
Then, in order to prove the theorem it is enough to verify that
(ii)
“ñ
(iii)
“ñ
(i) .
(ii)
“ñ
(iii). By Theorem 5 it holds that (En`1|Hn`1)|C1···n “ C1···n`1|C1···n.
Moreover, as C1···n`1 “ C1···n, it holds that C1···n`1|C1···n “ C1···n|C1···n, which
by Theorem 6 is constant and coincides with 1. Thus, (En`1|Hn`1)|C1···n “
C1···n|C1···n “ 1 , that is (iii) is satisﬁed.
(iii)
“ñ
(i). As the condition (iii) is satisﬁed, that is (En`1|Hn`1)|C1···n “ 1,
the unique coherent prevision assessment μ on (En`1|Hn`1)|C1···n is
μ “ 1 “ P[C1···n`1 ` μ(1 ´ C1···n)] “ x1···n`1 ` 1 ´ x1···n ,
from which it follows x1···n`1 “ x1···n. We observe that, as F is p-consistent, it
is coherent to assess x1 “ · · · “ xn “ 1. Moreover, by recalling Theorem 1, it
holds that: max{x1 ` · · · ` xn ´ n ` 1, 0} ď x1···n ď min{x1, . . . , xn}.
Then, when x1 “ · · · “ xn “ 1 it follows that x1···n “ 1 “ x1···n`1 and hence
xn`1 “ 1. Thus, F p-entails En`1|Hn`1, that is the condition (i) is satisﬁed. ⊓⊔
We recall that the Transitivity rule is not p-valid, that is {C|B, B|A} does not
p-entail C|A. In [25, Theorem 5] it has been shown that
P(C|B) “ 1, P(B|A) “ 1, P(A|(A _ B)) ą 0 ñ P(C|A) “ 1,
which is a weaker version of transitivity. This kind of Weak Transitivity has been
also obtained in [22] in the setting of preferential relations. In the next example,
in order to illustrate Theorems 5, 6 and 7, we consider two aspects: (i) a p-valid
version of Weak Transitivity, where the constraint P(A|(A_B)) ą 0 is replaced
by P(A|(A_B)) “ 1, by showing that (C|A)|((C|B)^(B|A)^(A|(A_B))) “ 1;
(ii) the non p-validity of the Transitivity rule, by showing that the iterated
conditional (C|A)|((C|B) ^ (B|A)) does not coincide with the constant 1.
Example 1. (i)-Weak
Transitivity.
We
consider
the
premise
set
F
“
{C|B, B|A, A|(A _ B)} and the conclusion C|A, where the events A, B, C are
logically independent. By Deﬁnition 4
C(F) “ (C|B) ^ (B|A) ^ (A|(A _ B)) “
⎧
⎨
⎩
1, if ABC is true,
0, if AB ¯C _ A ¯B _ ¯AB is true,
z, if ¯A ¯B is true,
“ ABC|(A _ B),
where z “ P[C(F)] “ P(ABC|(A _ B)). As C(F) “ ABC|(A _ B) Ď C|A, it
follows that C(F) ^ (C|A) “ C(F) and by Theorem 3, {C|B, B|A, A|(A _ B)}
p-entails C|A. Then, by Theorem 7, (C|A)|((C|B) ^ (B|A) ^ (A|(A _ B))) is
constant and coincides with 1. Indeed, by recalling Theorems 5 and 6, it holds
that
(C|A)|((C|B) ^ (B|A) ^ (A|(A _ B))) “ (C|A)|C(F)
“ ((C|A) ^ C(F))|C(F) “ C(F)|C(F) “ 1.

640
A. Gilio and G. Sanﬁlippo
(ii)-Transitivity. We recall that Transitivity is not p-valid, that is the premise set
{C|B, B|A} does not p-entail the conclusion C|A. Then the iterated conditional
(C|A)|((C|B) ^ (B|A)) does not coincide with 1, as we show below. We set
P(B|A) “ x, P(BC|A) “ y, P[(C|B)^(B|A)] “ u, P[(C|B)^(B|A)^(C|A)] “
w, then by Deﬁnition 3 we obtain that
(C|B) ^ (B|A) “ (ABC ` x ¯ABC)|(A _ B) “ ABC ` x ¯ABC ` u ¯A ¯B,
and
(C|B) ^ (B|A) ^ (C|A) “ (C|B) ^ (BC|A) “ (ABC ` y ¯ABC)|(A _ B) “
“ ABC ` y ¯ABC ` w ¯A ¯B.
Deﬁning P[(C|A)|((C|B) ^ (B|A))] “ ν, by linearity of prevision it holds that
ν “ w ` ν(1 ´ u) and hence
(C|A)|((C|B) ^ (B|A)) “ (C|B) ^ (B|A) ^ (C|A) ` ν(1 ´ (C|B) ^ (B|A))
“
⎧
⎪
⎪
⎨
⎪
⎪
⎩
1,
if ABC is true,
ν,
if A ¯B _ B ¯C is true,
y ` ν(1 ´ x), if ¯ABC is true,
ν,
if ¯A ¯B is true.
We observe that in general y ` ν(1 ´ x) ̸“ 1, for instance when (x, y) “ (1, 0) it
holds that y ` ν(1 ´ x) “ 0. Thus, in agreement with Theorem 7, the iterated
conditional (C|A)|((C|B) ^ (B|A)) does not coincide with the constant 1.
Notice that the p-validity of other inference rules, with a two-premise set
{E1|H1, E2|H2} and a conclusion E3|H3, has been examined in [26] by checking
whether the condition (E3|H3)|((E1|H1) ^ (E2|H2)) “ 1 is satisﬁed.
5
Conclusions
In this paper, we generalized the notion of iterated conditional by introduc-
ing the random object C(F2)|C(F1). We showed that P[C(F2) ^ C(F1)] “
P[C(F2)|C(F1)]P[C(F1)] and that (C(F2) ^ C(F1))|C(F1) “ C(F2)|C(F1). Then,
we veriﬁed that the iterated conditional C(F)|C(F) is constant and coincides
with 1. Moreover, under p-consistency of F, we characterized the p-entailment
of En`1|Hn`1 from F by the property that the iterated conditional where the
antecedent is the conjunction C(F) and the consequent is (En`1|Hn`1) coin-
cides with the constant 1. In other words, F p-entails (En`1|Hn`1) if and only
if (En`1|Hn`1)|C(F) “ 1. We have also illustrated this characterization by an
example related with weak transitivity. We observe that a particular case of
this characterization is obtained when we consider a (p-consistent) family of n
unconditional events F “ {E1, . . . , En} and a further event En`1. In this case
F p-entails En`1 if and only if E1 · · · En Ď En`1, which also amounts to the
property that the conditional event En`1|E1 · · · En coincides with 1.
Acknowledgements. We thank the four anonymous reviewers for their useful com-
ments and suggestions. G. Sanﬁlippo has been partially supported by the INdAM–
GNAMPA Project 2020 Grant U-UFMBAZ-2020-000819.

Iterated Conditionals and Characterization of P-Entailment
641
References
1. Adams, E.W.: The Logic of Conditionals. Reidel, Dordrecht (1975)
2. Benferhat, S., Dubois, D., Prade, H.: Nonmonotonic reasoning, conditional objects
and possibility theory. Artif. Intell. 92, 259–276 (1997). https://doi.org/10.1016/
S0004-3702(97)00012-X
3. Biazzo, V., Gilio, A., Lukasiewicz, T., Sanﬁlippo, G.: Probabilistic logic under
coherence: complexity and algorithms. Ann. Math. Artif. Intell. 45(1–2), 35–81
(2005). https://doi.org/10.1007/s10472-005-9005-y
4. Calabrese, P.: An algebraic synthesis of the foundations of logic and probability.
Inf. Sci. 42(3), 187–237 (1987). https://doi.org/10.1016/0020-0255(87)90023-5
5. Calabrese, P.: Logic and Conditional Probability: A Synthesis. College Publications
(2017)
6. Ciucci, D., Dubois, D.: Relationships between connectives in three-valued logics.
In: Greco, S., Bouchon-Meunier, B., Coletti, G., Fedrizzi, M., Matarazzo, B., Yager,
R.R. (eds.) IPMU 2012. CCIS, vol. 297, pp. 633–642. Springer, Heidelberg (2012).
https://doi.org/10.1007/978-3-642-31709-5 64
7. Ciucci, D., Dubois, D.: A map of dependencies among three-valued logics. Inf. Sci.
250, 162–177 (2013). https://doi.org/10.1016/j.ins.2013.06.040
8. Coletti, G., Petturiti, D., Vantaggi, B.: Fuzzy memberships as likelihood functions
in a possibilistic framework. Int. J. Approximate Reasoning 88, 547–566 (2017).
https://doi.org/10.1016/j.ijar.2016.11.017
9. Coletti, G., Petturiti, D., Vantaggi, B.: A Dutch book coherence condition for
conditional completely alternating Choquet expectations. Bollettino dell’Unione
Matematica Italiana 13(4), 585–593 (2020). https://doi.org/10.1007/s40574-020-
00251-8
10. Coletti, G., Scozzafava, R.: Conditional probability, fuzzy sets, and possibility: a
unifying view. Fuzzy Sets Syst. 144, 227–249 (2004)
11. Coletti, G., Scozzafava, R., Vantaggi, B.: Coherent conditional probability, fuzzy
inclusion and default rules. In: Yager, R., Abbasov, A.M., Reformat, M.Z., Shah-
bazova, S.N. (eds.) Soft Computing: State of the Art Theory and Novel Applica-
tions, pp. 193–208. Springer, Heidelberg (2013)
12. Coletti, G., Scozzafava, R., Vantaggi, B.: Possibilistic and probabilistic logic under
coherence: Default reasoning and System P. Mathematica Slovaca 65(4), 863–890
(2015). https://doi.org/10.1515/ms-2015-0060
13. Coletti, G., Vantaggi, B.: Coherent conditional plausibility: a tool for handling
fuzziness and uncertainty under partial information. In: Collan, M., Kacprzyk,
J. (eds.) Soft Computing Applications for Group Decision-making and Consensus
Modeling. SFSC, vol. 357, pp. 129–152. Springer, Cham (2018). https://doi.org/
10.1007/978-3-319-60207-3 9
14. Cruz, N.: Deduction from uncertain premises? In: Elqayam, S., Douven, I., Evans,
J.S.B.T., Cruz, N. (eds.) Logic and Uncertainty in the Human Mind: A Tribute
to David E. Over, pp. 27–41. Routledge, Oxon (2020). https://doi.org/10.4324/
9781315111902-3
15. Douven, I., Dietz, R.: A puzzle about Stalnaker’s hypothesis. Topoi, pp. 31–37
(2011). https://doi.org/10.1007/s11245-010-9082-3
16. Douven, I., Elqayam, S., Singmann, H., van Wijnbergen-Huitink, J.: Conditionals
and inferential connections: toward a new semantics. Thinking Reasoning, pp. 1–41
(2019). https://doi.org/10.1080/13546783.2019.1619623

642
A. Gilio and G. Sanﬁlippo
17. Dubois, D., Faux, F., Prade, H.: Prejudice in uncertain information merging: push-
ing the fusion paradigm of evidence theory further. Int. J. Approximate Reasoning
121, 1–22 (2020). https://doi.org/10.1016/j.ijar.2020.02.012
18. Dubois, D., Liu, W., Ma, J., Prade, H.: The basic principles of uncertain infor-
mation fusion. An organised review of merging rules in diﬀerent representation
frameworks. Inf. Fusion 32, 12–39 (2016). https://doi.org/10.1016/j.inﬀus.2016.
02.006
19. Dujmovi´c, J.J., Legind Larsen, H.: Generalized conjunction/disjunction. Int. J.
Approximate Reasoning 46(3), 423–446 (2007). https://doi.org/10.1016/j.ijar.
2006.12.011, special Section: Aggregation Operators
20. de Finetti, B.: La logique de la probabilit´e. In: Actes du Congr`es International de
Philosophie Scientiﬁque, Paris, 1935, pp. IV 1-IV 9 (1936)
21. Flaminio, T., Godo, L., Hosni, H.: Boolean algebras of conditionals, probability
and logic. Artif. Intell. 286, 103347 (2020). https://doi.org/10.1016/j.artint.2020.
103347
22. Freund, M., Lehmann, D., Morris, P.: Rationality, transitivity, and contraposition.
Artif. Intell. 52(2), 191–203 (1991)
23. Gilio, A.: Probabilistic reasoning under coherence in System P. Annals Math. Artif.
Intell. 34, 5–34 (2002). https://doi.org/10.1023/A:101442261
24. Gilio, A., Over, D.E., Pfeifer, N., Sanﬁlippo, G.: Centering and Compound Condi-
tionals Under Coherence. In: Ferraro, M.B., Giordani, P., Vantaggi, B., Gagolewski,
M., Gil, M.´A., Grzegorzewski, P., Hryniewicz, O. (eds.) Soft Methods for Data
Science. AISC, vol. 456, pp. 253–260. Springer, Cham (2017). https://doi.org/10.
1007/978-3-319-42972-4 32
25. Gilio, A., Pfeifer, N., Sanﬁlippo, G.: Transitivity in coherence-based probability
logic. J. Appl. Logic 14, 46–64 (2016). https://doi.org/10.1016/j.jal.2015.09.012
26. Gilio, A., Pfeifer, N., Sanﬁlippo, G.: Probabilistic entailment and iterated condi-
tionals. In: Elqayam, S., Douven, I., Evans, J.S.B.T., Cruz, N. (eds.) Logic and
Uncertainty in the Human Mind: a Tribute to David E. Over, pp. 71–101. Rout-
ledge, Oxon (2020). https://doi.org/10.4324/9781315111902-6
27. Gilio, A., Sanﬁlippo, G.: Conditional random quantities and iterated conditioning
in the setting of coherence. In: van der Gaag, L.C. (ed.) ECSQARU 2013. LNCS
(LNAI), vol. 7958, pp. 218–229. Springer, Heidelberg (2013). https://doi.org/10.
1007/978-3-642-39091-3 19
28. Gilio, A., Sanﬁlippo, G.: Conjunction, disjunction and iterated conditioning of con-
ditional events. In: Kruse, R., Berthold, M., Moewes, C., Gil, M., Grzegorzewski,
P., Hryniewicz, O. (eds.) Synergies of Soft Computing and Statistics for Intelligent
Data Analysis. AISC, vol. 190, pp. 399–407. Springer, Heidelberg (2013). https://
doi.org/10.1007/978-3-642-33042-1 43
29. Gilio, A., Sanﬁlippo, G.: Probabilistic entailment in the setting of coherence: the
role of quasi conjunction and inclusion relation. Int. J. Approximate Reasoning
54(4), 513–525 (2013). https://doi.org/10.1016/j.ijar.2012.11.001
30. Gilio, A., Sanﬁlippo, G.: Quasi conjunction, quasi disjunction, t-norms and t-
conorms: probabilistic aspects. Inf. Sci. 245, 146–167 (2013). https://doi.org/10.
1016/j.ins.2013.03.019
31. Gilio, A., Sanﬁlippo, G.: Conditional random quantities and compounds of con-
ditionals. Studia Logica 102(4), 709–729 (2013). https://doi.org/10.1007/s11225-
013-9511-6
32. Gilio, A., Sanﬁlippo, G.: Conjunction and disjunction among conditional events. In:
Benferhat, S., Tabia, K., Ali, M. (eds.) IEA/AIE 2017. LNCS (LNAI), vol. 10351,
pp. 85–96. Springer, Cham (2017). https://doi.org/10.1007/978-3-319-60045-1 11

Iterated Conditionals and Characterization of P-Entailment
643
33. Gilio, A., Sanﬁlippo, G.: Generalized logical operations among conditional events.
Appl. Intell. 49(1), 79–102 (2018). https://doi.org/10.1007/s10489-018-1229-8
34. Gilio, A., Sanﬁlippo, G.: Algebraic aspects and coherence conditions for conjoined
and disjoined conditionals. Int. J. Approximate Reasoning 126, 98–123 (2020).
https://doi.org/10.1016/j.ijar.2020.08.004
35. Gilio, A., Sanﬁlippo, G.: Compound conditionals, Fr´echet-Hoeﬀding bounds, and
Frank t-norms. Int. J. Approximate Reasoning 136, 168–200 (2021). https://doi.
org/10.1016/j.ijar.2021.06.006
36. Gilio, A., Sanﬁlippo, G.: On compound and iterated conditionals. Argumenta 6
2(2021), 241–266 (2021). https://doi.org/10.14275/2465-2334/202112.gil
37. Goodman, I.R., Nguyen, H.T., Walker, E.A.: Conditional Inference and Logic for
Intelligent Systems: A Theory of Measure-Free Conditioning. North-Holland (1991)
38. Grabisch, M., Marichal, J., Mesiar, R., Pap, E.: Aggregation functions. Cambridge
University Press (2009)
39. Kaufmann, S.: Conditionals right and left: probabilities for the whole family. J.
Philosophical Logic 38, 1–53 (2009). https://doi.org/10.1007/s10992-008-9088-0
40. Lewis, D.: Counterfactuals. Blackwell, Oxford (1973)
41. Lewis, D.: Probabilities of conditionals and conditional probabilities. Philos. Rev.
85(3), 297–315 (1976)
42. McGee, V.: Conditional probabilities and compounds of conditionals. Philos. Rev.
98(4), 485–541 (1989). https://doi.org/10.2307/2185116
43. Nguyen, H.T., Walker, E.A.: A history and introduction to the algebra of condi-
tional events and probability logic. IEEE Trans. Syst. Man Cybernetics 24(12),
1671–1675 (1994). https://doi.org/10.1109/21.328924
44. Petturiti, D., Vantaggi, B.: Modeling agent’s conditional preferences under objec-
tive ambiguity in dempster-shafer theory. Int. J. Approximate Reasoning 119,
151–176 (2020). https://doi.org/10.1016/j.ijar.2019.12.019
45. Sanﬁlippo, G.: Lower and upper probability bounds for some conjunctions of two
conditional events. In: Ciucci, D., Pasi, G., Vantaggi, B. (eds.) SUM 2018. LNCS
(LNAI), vol. 11142, pp. 260–275. Springer, Cham (2018). https://doi.org/10.1007/
978-3-030-00461-3 18
46. Sanﬁlippo, G., Gilio, A., Over, D., Pfeifer, N.: Probabilities of conditionals and
previsions of iterated conditionals. Int. J. Approximate Reasoning 121, 150–173
(2020). https://doi.org/10.1016/j.ijar.2020.03.001
47. Sanﬁlippo, G., Pfeifer, N., Gilio, A.: Generalized probabilistic modus ponens. In:
Antonucci, A., Cholvy, L., Papini, O. (eds.) ECSQARU 2017. LNCS (LNAI), vol.
10369, pp. 480–490. Springer, Cham (2017). https://doi.org/10.1007/978-3-319-
61581-3 43
48. Sanﬁlippo, G., Pfeifer, N., Over, D., Gilio, A.: Probabilistic inferences from con-
joined to iterated conditionals. Int. J. Approximate Reasoning 93(Supplement C),
103–118 (2018). https://doi.org/10.1016/j.ijar.2017.10.027
49. Sezgin, M., Kern-Isberner, G., Rott, H.: Inductive reasoning with diﬀerence-making
conditionals. In: 18th International Workshop on Non-Monotonic Reasoning (NMR
2020 Workshop Notes), pp. 83–92 (2020)

A Triple Uniqueness of the Maximum
Entropy Approach
J¨urgen Landes(B)
Munich Center for Mathematical Philosophy, LMU Munich, Munich, Germany
juergen landes@yahoo.de
To be uncertain is to be
uncomfortable, but to be certain
is to be ridiculous.
Chinese proverb
Abstract. Inductive logic is concerned with assigning probabilities to
sentences given probabilistic constraints. The Maximum Entropy App-
roach to inductive logic I here consider assigns probabilities to all sen-
tences of a ﬁrst order predicate logic. This assignment is built on an appli-
cation of the Maximum Entropy Principle, which requires that probabil-
ities for uncertain inference have maximal Shannon Entropy. This paper
puts forward two diﬀerent modiﬁed applications of this principle to ﬁrst
order predicate logic and shows that the original and the two modiﬁed
applications agree in many cases. A third promising modiﬁcation is stud-
ied and rejected.
Keywords: Inductive logic · Maximum entropy · First order predicate
language · Uncertain inference · Objective Bayesianism
1
Introduction
Inductive logic is a formal approach to model rational uncertain inferences. It
seeks to analyse the degree to which premisses entail putative conclusions. Given
uncertain premisses ϕ1, . . . , ϕk with attached uncertainties X1, . . . , Xk an induc-
tive logic provides means to attach uncertainty Y to a conclusion ψ, where the
Xi and Y are non-empty subsets of the unit interval. An inductive logic can be
represented as
ϕX1
1 , . . . , ϕXk
k
|≈ψY ,
(1)
where |≈denotes an inductive entailment relation [11]. Much work has gone into
the development and exploration of inductive logics, see, e.g., [4,9,12,16,29–
31,41].
I gratefully acknowledge funding from the Deutsche Forschungsgemeinschaft (DFG,
German Research Foundation) - 432308570 and 405961989.
c
⃝Springer Nature Switzerland AG 2021
J. Vejnarov´a and N. Wilson (Eds.): ECSQARU 2021, LNAI 12897, pp. 644–656, 2021.
https://doi.org/10.1007/978-3-030-86772-0_46

A Triple Uniqueness of the Maximum Entropy Approach
645
The main early proponent of inductive logic was Rudolf Carnap [2,3]. Nowa-
days, the spirit of his approach today continues in the Pure Inductive Logic
approach [14,17,21–24,40]. In this paper, I however consider uncertain inference
within the maximum entropy framework, which goes back to Edwin Jaynes [15],
who put forward a Maximum Entropy Principle governing rational uncertain
inference.
Maximum Entropy Principle. Rational agents ought to use a probability
function consistent with the evidence for drawing uncertain inferences. In case
there is more than one such probability function, a rational agent ought to use
probability functions with maximal entropy.
In case only a single probability function is consistent with the evidence, the
Maximum Entropy Principle is uncontroversial. Its strength (and sometimes con-
troversial nature) is rooted in applications with multiple prima facie reasonable
probability functions for probabilistic inference. This principle is at the heart of
objective Bayesianism.
If the underlying domain is ﬁnite, then applying the Maximum Entropy Prin-
ciple for inductive entailment is straight-forward and well-understood due to the
seminal work of Alena Vencovsk´a & JeﬀParis [32–34,36–39]. Matters change
dramatically for inﬁnite domains. Naively replacing the sum by an integral in
the deﬁnition of Shannon Entropy produces a great number of probability func-
tions with inﬁnite entropy. But then there is no way to pick a probability function
with maximal entropy out of a set in which all functions have inﬁnite entropy.
There are two diﬀerent suggestions for inductive logic on an inﬁnite ﬁrst order
predicate logic explicating the Maximum Entropy Principle. The Entropy Limit
Approach [1,35] and the Maximum Entropy Approach [28,45–48]. It has been
conjectured, that both approaches agree in cases in which the former approach
is-well deﬁned [48, p. 191]. This conjecture has been shown to hold in a number
of cases of evidence bases with relatively low quantiﬁer-complexity [19,25,44].
This paper introduces modiﬁcations of the Maximum Entropy Approach and
studies their relationships. I next properly introduce this approach along with
some notation and the modiﬁcations. I then proceed to investigate their rela-
tionships. My main result is Theorem 1; it proves that the two suggested mod-
iﬁcations agree with the original Maximum Entropy Approach expounded by
Jon Williamson for convex optimisation problems, if at least one of these three
approaches yields a unique probability function for inference on the underlying
ﬁrst order predicate language.
In Sect. 4, I study a third modiﬁcation of Williamson’s Maximum Entropy
Approach, which I reject due to the absurd probabilities it delivers for inductive
inference, see Proposition 6. In Sect. 5, I put forward some concluding remarks
and consider avenues for future research.
2
The Maximum Entropy Approach and Two
Modiﬁcations
The formal framework and notation is adapted from [25].

646
J. Landes
A ﬁxed ﬁrst order predicate language L is given. It consists of countably many
constant symbols t1, t2, . . . exhausting the universe (for every element in the
universe there is at least one constant symbol representing it) and ﬁnitely many
relation symbols, U1, . . . , Un. In particular, note that the language does not
contain a symbol for equality nor does it contain function symbols. The atomic
sentences are sentences of the form Uiti1 . . . tik, where k is the arity of the relation
Ui; the atomic sentences are denoted by a1, a2, . . .. They are by construction
ordered in such a way that atomic sentences involving only constants among
t1, . . . , tn occur before those atomic sentences that also involve tn+1. The set of
sentences of L is denoted by SL.
The ﬁnite sublanguages Ln of L are those languages, which only contain the
ﬁrst n constant symbols t1, . . . , tn and the same relation symbols as L. Denote
the sentences of Ln by SLn.
The contingent conjunctions of maximal length of the form ±a1∧. . .∧±arn ∈
SLn are called the n-states. Let Ωn be the set of n-states for each n ∈N with
|Ωn| = 2rn.
Deﬁnition 1 (Probabilities on Predicate Languages). A probability func-
tion P on L is a function P : SL −→R≥0 such that:
P1: If τ is a tautology, i.e., |= τ, then P(τ) = 1.
P2: If θ and ϕ are mutually exclusive, i.e., |= ¬(θ ∧ϕ), then P(θ ∨ϕ) = P(θ) +
P(ϕ).
P3: P (∃xθ(x)) = supm P (m
i=1 θ(ti)).
A probability function on Ln is deﬁned similarly (the supremum in P3 is
dropped and m is equal to n).
P denotes the set of all probability functions on L.
A probability function P ∈P is determined by the values it gives to the
quantiﬁer-free sentences, this result is known as Gaifman’s Theorem [8]. P3 is
sometimes called Gaifman Condition [40, p. 11]. The Gaifman Condition is jus-
tiﬁed by the assumption that the constants exhaust the universe. Consequently,
a probability function is determined by the values it gives to the n-states, for
each n [33, p. 171].
It is thus sensible to measure the entropy of a probability function P ∈P via
n-states with varying n.
Deﬁnition 2 (n-entropy). The n-entropy of a probability function P ∈P is
deﬁned as:
Hn(P) : = −

ω∈Ωn
P(ω) log P(ω) .
The usual conventions are 0 log 0 := 0 and log denoting the natural logarithm.
The second convention is inconsequential for current purposes.

A Triple Uniqueness of the Maximum Entropy Approach
647
Hn(·) is a strictly concave function.
The key idea is to combine the n-entropies deﬁned on ﬁnite sublanguages Ln
into an overall notion of comparative entropy comparing probability functions
P and Q deﬁned on the entire ﬁrst order language.
So far, the literature has only studied such inductive logics with respect to
the ﬁrst binary relation in the following deﬁnition.
Deﬁnition 3 (Comparative Notions of Entropy). That a probability func-
tion P ∈P has greater (or equal) entropy than a probability function Q ∈P
could be deﬁned in the following three ways.
1. If and only if there is some natural number N such that for all n ≥N it holds
that Hn(P) > Hn(Q), denoted by P ≻Q.
2. If and only if there is some natural number N such that for all n ≥N it holds
that Hn(P) ≥Hn(Q) and there are inﬁnitely many n such that Hn(P) >
Hn(Q), denoted by P]Q.
3. If and only if there is some natural number N such that for all n ≥N it holds
that Hn(P) ≥Hn(Q), denoted by P)Q.
The lower two deﬁnitions are alternative ways in which one could explicate the
intuitive idea of comparative entropy, which have never been studied before.
Prima facie, all three deﬁnitions appear reasonable.
Before I can deﬁne notions of maximal entropy with respect to the given
premisses, I need to specify the set of probability functions over which entropy
is to be maximised.
Deﬁnition 4 (Region of Feasible Probability Functions). The set of prob-
ability functions consistent with all premisses, P(ϕi) ∈Xi for all i, is denoted
by E and deﬁned as
E := {P ∈P : P(ϕi) ∈Xi for all 1 ≤i ≤k} .
In order to simplify the notation, I do not display the dependence of E on the
premisses.
In this paper, I only consider a ﬁxed set of premisses, ϕX1
1 , . . . , ϕXk
k . There is
hence no need to complicate notation by writing EϕX1
1
,...,ϕ
Xk
k
or similar.
Deﬁnition 5 (Set of Maximum Entropy Functions). The set of probability
functions on L with maximal entropy in E relative to a notion of comparative
entropy > deﬁned on P × P can then be deﬁned as
maxent> E : ={P ∈E : there is no Q ∈E \ {P} with Q > P} .
(2)
The Maximum Entropy Principle now compels agents to use probabilities
in maxent> E for drawing uncertain inferences as described by the scheme for
inductive logic in (1). The induced inductive logics are described in the following
deﬁnition.

648
J. Landes
Deﬁnition 6 (Maximum Entropy Inductive Logics). An inductive logic
with respect to > is induced by attaching uncertainty Y>(ψ) ⊆[0, 1] to the sen-
tences ψ of L via
Y>(ψ) := {r ∈[0, 1] | there exists P ∈maxent> E with P(ψ) = r} .
In case there are two or more diﬀerent probability functions in maxent> E, there
are some sentences of ψ of L to which multiple diﬀerent probabilities attach.
The Maximum Entropy Approach arises using ≻for comparing entropies of
probability functions in P [28,45–48].
Remark 1 (Comparisons to the Entropy Limit Approach). It is known that the
Entropy Limit Approach is invariant under arbitrary permutations of the con-
stant symbols [25, Footnote 2]. The Entropy Limit Approach however suﬀers
from a ﬁnite model problem, a premiss sentence formalising the existence of an
inﬁnite order does not have a ﬁnite model [42, Sect. 4.1] and hence the entropy
limit is undeﬁned.
The three Maximum Entropy Approaches deﬁned in Deﬁnition 6 are invariant
under ﬁnite permutations of constant symbols, the three notions of comparative
entropy only depend on the limiting behaviour of Hn and n-entropy is invariant
under permutation of the ﬁrst n constant symbols (Deﬁnition 3). Whether these
Maximum Entropy Approaches are invariant under inﬁnite permutations is still
to be determined. Since these approaches do not make use of ﬁnite models,
they are immune to the ﬁnite model problem. See [19,25] for more detailed
comparisons and further background.
Remark 2 (Maximum Entropy Functions). Determining maximum entropy func-
tions is an often diﬃcult endeavour. In concrete applications, it is often easier
to determine the entropy limit ﬁrst and then show that the entropy limit also
has maximal entropy. See [25] for an overview of the cases in which a maximum
entropy function exists and is unique.
Trivially, if the equivocator function, P= ∈P, which for all n assigns all
n-states the same probability of 1/|Ωn|,1 is in E, then {P=} = maxent≻E.
In a forthcoming paper [26], we show that if there is only a single premiss ϕ
such that 0 < P=(ϕ) < 1, then the maximum entropy function is obtained from
(Jeﬀrey) updating the equivocator function. For the premiss ϕc with 0 ≤c ≤1
it holds that {cP=(·|ϕN)+(1−c)P=(·|¬ϕN)} = maxent≻E, where N is maximal
such that tN ∈ϕ and ϕN is deﬁned as the disjunction of N-states ωN such that
P=(ϕ ∧ωN) > 0.
Cases with multiple uncertain premisses are, in general, still poorly under-
stood.
1 Note that the equivocator function is the unique probability function in P which is
uniform over all Ωn, P=(ωn) =
1
|Ωn| for all n and all ωn ∈Ωn. The name for this
function is derived from the fact that it is maximally equivocal. The function has
also been given other names. In Pure Inductive Logic it is known as the completely
independent probability function and is often denoted by c∞in reference to the role
it plays in Carnap’s famous continuum of inductive methods [3].

A Triple Uniqueness of the Maximum Entropy Approach
649
In the next section, I study (the relationships of) these binary relations and
the arising inductive logics. Particular attention is paid to the case of a unique
probability function for inference, | maxent> E| = 1. These cases are of partic-
ular interest, since they deliver well-deﬁned (unique) probabilities for inductive
inference.
3
Maximal (Modiﬁed) Entropy
I ﬁrst consider two notions of reﬁnement relating these three binary relations.
Deﬁnition 7 (Strong Reﬁnement). > is called a strong reﬁnement of ≫, if
and only if the following hold
– > is a reﬁnement of ≫, for all P, Q ∈P it holds that P ≫Q entails P > Q,
– for all R, P, Q ∈P it holds that, if R ≫P and P > Q are both true, then
R ≫Q and R ̸= Q.
Deﬁnition 8 (Centric Reﬁnement). A reﬁnement > of ≫is called centric,
if and only if for all diﬀerent R, P ∈P with R > P it holds that (R+P)/2 ≫P.
The name centric has been chosen to emphasise that the centre between R and
P is greater than P.
Clearly, not all binary relations possess strong reﬁnements; not all binary
relations possess centric reﬁnements.
Proposition 1 (Strong and Centric Reﬁnements). ] is a strong and centric
reﬁnement of ≻. ) is a strong and centric reﬁnement of ] and of ≻.
Proof. For ease of comparison, I now display the three notions of comparative
entropy line by line. The ﬁrst line deﬁnes P ≻Q, the second line P]Q and the
third line P)Q. The second conjunct in the ﬁrst deﬁnition is superﬂuous as is
the second conjunct in the third deﬁnition:
Hn(P) ≤Hn(Q) not inﬁnitely often & Hn(P) > Hn(Q) inﬁnitely often
Hn(P) < Hn(Q) not inﬁnitely often & Hn(P) > Hn(Q) inﬁnitely often
Hn(P) < Hn(Q) not inﬁnitely often & Hn(P) ≥Hn(Q) inﬁnitely often .
By thusly spelling out both comparative notions of entropy one easily observes
that P ≻Q entails P]Q, and that P]Q entails P)Q. This establishes the reﬁne-
ment relationships.
Strong Reﬁnements. Next note that, if R ≻Q or if R]Q, then R ̸= Q.
] is a strong reﬁnement of ≻: Let R ≻P and P]Q. Then R ̸= Q. Furthermore,
Hn(R) ≤Hn(Q) is true for at most ﬁnitely many n, since from some N onwards
P has always greater or equal n-entropy than Q. So, R ≻Q.
) is a strong reﬁnement of ]: Let R]P and P)Q. Then R ̸= Q. From some N
onwards P has always greater or equal n-entropy than Q. There are also inﬁnitely
many n ∈N such that Hn(R) > Hn(P). So, R]Q.

650
J. Landes
) is a strong reﬁnement of ≻: Let R ≫P and P)Q. Then R ̸= Q. From some
N onwards P has always greater or equal n-entropy than Q. From some N ′
onwards R has always greater n-entropy than P. Hence, Hn(R) ≤Hn(Q) can
only be the case for ﬁnitely many n ∈N. So, R ≻Q.
Centric Reﬁnement. First, note that diﬀerent probability functions disagree
on some quantiﬁer free sentence ϕ ∈LN (Gaifman’s Theorem [8]). Since
ϕ ∈Ln+N for all n ≥1, these probability functions also disagree on all more
expressive sub-languages Ln+N.
] is a centric reﬁnement of ≻: Fix arbitrary probability functions R, P deﬁned
on L with R]P. R ̸= P. From the concavity of the function Hn it follows that
Hn( R+P
2
) > Hn(P), whenever Hn(R) ≥Hn(P). By deﬁnition of ], there are
only ﬁnitely many n for which Hn(R) ≥Hn(P) fails to hold. Hence, R+P
2
≻P
by deﬁnition of ≻.
) is a centric reﬁnement of ≻: Fix arbitrary probability functions R, P deﬁned
on L with R)P. Note that R may be equal to P. From the concavity of the
function Hn it follows that Hn( R+P
2
) > Hn(P), whenever Hn(R) ≥Hn(P). By
deﬁnition of ), there are only ﬁnitely many n for which Hn(R) ≥Hn(P) fails to
hold. Hence, R+P
2
≻P by deﬁnition of ≻.
) is a centric reﬁnement of ]: Fix arbitrary probability functions R, P deﬁned on
L with R)P. Note that R may be equal to P. Since R+P
2
≻P (see above case)
and since ] is a reﬁnement of ≻, it holds that R+P
2
]P.
⊓⊔
Remark 3 (Properties of Comparative Entropies). If Hn(P) = Hn(Q) for all even
n and Hn(P) > Hn(Q) for all odd n, then P]Q and P ⊁Q. Hence, ] is a proper
reﬁnement of ≻.
For P = Q it holds that P)Q and Q)P. Hence, ) is a proper reﬁnement of ]
and thus a proper reﬁnement of ≻.
] is transitive, irreﬂexive, acyclic and asymmetric. ) is transitive, reﬂexive and
has non-trivial cycles, e.g., for all probability functions P, Q with zero-entropy,
Hn(P) = 0 for all n ∈N, it holds that P)Q.
I now turn to entropy maximisation and the induced inductive logics.
Proposition 2 (Downwards Uniqueness). Let > be a strong reﬁnement of
≫. If maxent≫E = {Q}, then {Q} = maxent≫E = maxent> E.
In case the inductive logic induced by ≫as a notion of a comparative entropy
provides a unique probability function for rational inference, so does the induc-
tive logic induced by >.
Proof. Note at ﬁrst that since > is a reﬁnement of ≫it holds that
maxent> E ⊆maxent≫E .
(3)
Maximal elements according to ≫may not be maximal according to > and all
maximal elements according to > are also maximal according to ≫.

A Triple Uniqueness of the Maximum Entropy Approach
651
Assume for the purpose of deriving a contradiction that Q /∈maxent> E.
Then, there has to exist a P ∈E \ {Q} such that P > Q but P ≫Q fails to
hold ({Q} = maxent≫E holds by assumption).
However, since {Q} = maxent≫E and Q /∈maxent> E hold, there has to
exist some R ∈E \ {P} such that R ≫P, P cannot have maximal ≫-entropy.
We hence have R ≫P and P > Q. Since > is a strong reﬁnement of ≫, we
obtain R ≫Q and R ̸= Q. Since R ∈E it follows from the deﬁnition of maxent≫
that Q /∈maxent≫E. Contradiction. So, Q ∈maxent> E.
Since {Q} = maxent≫E
(3)
⊇maxent> E ∋Q, it follows that maxent> E =
{Q}.
⊓⊔
The converse is also true for convex E and centric reﬁnements.
Proposition 3 (Upwards Uniqueness). If E is convex, > is a centric reﬁne-
ment of ≫and maxent> E = {Q}, then {Q} = maxent≫E = maxent> E.
Proof. Assume for contradiction that there exists a feasible probability function
P ∈E\{Q} such that P is not ≫-dominated by the probability functions in E but
>-dominated by some R ∈E \ {P}, R > P. Now deﬁne S = 1
2(P + R) and note
that S ∈E (convexity) and that S, P, R are pairwise diﬀerent, |{S, P, R}| = 3.
Since > is a centric reﬁnement of ≫, we conclude that S ≫P, which con-
tradicts that P ∈maxent≫E and P ̸= Q. So, only Q can be in maxent≫E.
Since Q ∈maxent> E and maxent> E
(3)
⊆maxent≫E it follows that {Q} =
maxent≫E.
⊓⊔
Theorem 1 (Triple
Uniqueness). If E is convex and at least one of
maxent) E, maxent] E or maxent≻E is a singleton, then
maxent E) = maxent] E = maxent≻E .
Proof. Simply apply the above three propositions.
⊓⊔
It is known that maxent≻E is a singleton, in case of a certain Σ1 premiss [44],
a class of Π1 premisses [25] and a class of constraints for unary languages [42]. As
remarked above, we show in a forthcoming paper [26], that for a single premiss
ϕc with 0 < P=(ϕ) < 1 and 0 ≤c ≤1 there exists a unique maximum entropy
function in maxent≻E, which is obtained from suitably (Jeﬀrey) updating the
equivocator.
Premiss sentences ϕ with P=(ϕ) = 0 and cases with multiple uncertain pre-
misses – on the other hand – are still poorly understood.
4
Modiﬁcation Number 3
The Maximum Entropy Approach, in its original formulation, fails to provide
probabilities for uncertain inference for certain evidence bases of quantiﬁer com-
plexity Σ2 [43, § 2.2]. For example, for the single certain premiss ϕ := ∃x∀yUxy

652
J. Landes
every probability function P consistent with the evidence must assign ϕ proba-
bility one, P ∈E entails P(ϕ) = 1. There must hence be, at least, one constant
tk witnessing the existence of all these y, P(∀yUtky) > 0. Ceteris paribus, the
k −1-entropy of such functions increases the greater the k such that tk is a
witness. Now suppose for contradiction that there exists a maximum entropy
function P ∈E with tk as the ﬁrst witness. Now construct a probability Q by
postponing the witness by one, tk+1 is the ﬁrst witness of the premiss according
to Q. Q can be constructed such that Hn(Q) ≥Hn(P) for all n. One can then
show that R := Q+P
2
has strictly greater n-entropy than P for all r ≥n + 1
(concavity of Hn). It is hence the case that for all P ∈E there exists a R ∈E
such that R ≻P. maxent≻E is hence empty, see the forthcoming [26] for more
details and a proof.
Theorem 1 shows that inductive logics induced by ) and ] also do not
produce a unique probability for uncertain inference for the certain premiss
ϕ = ∃x∀yUxy.
From the perspective of this paper, we see that the inductive logic of the
standard Maximum Entropy Approach fails to deliver well-deﬁned probabilities
for inference for the certain premiss ϕ = ∃x∀yUxy, because the relation ≻
holds for too many pairs of probability functions. Proceeding in the spirit of this
paper, it seems sensible to deﬁne a modiﬁed inductive logic induced by a notion
of comparative entropy, }, which holds for fewer pairs of probability functions.
That is, } is reﬁned by ≻.
Closest to the spirit of Deﬁnition 3 is to deﬁne P}Q as follows.
Deﬁnition 9 (Modiﬁcation Number 3). P}Q, if and only if Hn(P) >
Hn(Q) for all n ∈N.
Clearly, the other three notions of comparative entropy are reﬁnements of }.
Proposition 4 (Comparison of } vs. ≻, ],)). Neither of the three binary
relations ≻, ], ) is a strong reﬁnement and neither is a centric reﬁnement of }.
Proof. Consider three pairwise diﬀerent probability functions P, Q, R with i)
Hn(P) > Hn(Q) for all n , ii) Hn(P )
Hn(Q) ≈1, iii) H1(Q) = H1(R)−δ for large δ > 0
and iv) Hn(Q) > Hn(R) for all n ≥1.
Then P}Q and Q ≻R, Q]R, Q)R all hold. Now note that H1(P) < H1(R)
and thus P}R fails to hold. Hence, none of ≻, ], ) is a strong reﬁnement of }.
Finally, observe that
Q+R
2
}R fails to hold. Hence, none of ≻, ], ) is a centric
reﬁnement of }.
⊓⊔
The aim here is to deﬁne a diﬀerent inductive logic producing well-deﬁned
probabilities for the premiss sentence ϕ = ∃x∀yRxy (and other premiss sen-
tences). Theorem 1 shows that a diﬀerent logic can only arise, if none of the
other three notions of comparative entropy is a strong and centric reﬁnement
of }. Proposition 4 shows that neither of these notions is a strong and centric
reﬁnement. It his hence in principle possible that } deﬁnes a novel inductive
logic.

A Triple Uniqueness of the Maximum Entropy Approach
653
The following proposition shows that this not only possible in principle by
providing a case in which the induced inductive logics do come apart.
Proposition 5 (Inductive Logics). The binary relation } induces a diﬀerent
inductive logic than ≻, ], ).
Proof. Let U be the only and unary relation symbol of L. Suppose there is
no evidence, then all probability functions are consistent with the empty set of
premisses, E = P. Then every P ∈P with P(Ut1) = P(¬Ut1) = 0.5 has maximal
1-entropy. Hence, all such P are members of maxent} E. For □∈{≻, ], )} it holds
that maxent□E = {P=}. So, maxent□E is a proper subset of maxent} E:
maxent□E = {P=} ⊂{P ∈P : P(Ut1) = P(¬Ut1)} ⊂maxent} E .
⊓⊔
This proof leads to the following more general observation:
Proposition 6 (Finite Sublanguages). If there exists an n ∈N and a P ∈E
such that Hn(P) = max{Hn(Q) : Q ∈E}, then P ∈maxent} E.
This strong focus on single sublanguages Ln makes maxent} unsuitable as an
inductive logic for inﬁnite predicate languages, as the following example demon-
strates.
Example 1 (Absurdity of Modiﬁcation 3). Consider the case in which the pre-
misses jointly determine the probabilities on L1. For example, the given lan-
guage L contains two relation symbols: a unary relation symbol U1 and a
binary relation symbol U2. The premisses are U2t1t1 holds with certainty and
P(U1t1) = 10%. Then every probability function P ∈P that satisﬁes these
two premisses has a 1-entropy of H1(P) = −0.9 · log(0.9) −0.1 · log(0.1). So,
H1(P) = max{H1(Q) : Q ∈E}. This means that every feasible probability
function (of which there are many) is a maximum entropy function – regardless
of how entropic (or not) probabilities are assigned to the other sublanguages Ln
for n ≥2:
maxent} E = E .
5
Conclusions
Maximum entropy inductive logic on inﬁnite domains lacks a paradigm approach.
The Entropy Limit Approach, the Maximum Entropy Approach as well as the
here studied modiﬁed Maximum Entropy Approaches induce the same unique
inductive logic in a number of natural cases (Theorem 1 and [25,42,44]). This
points towards a uniﬁed picture of maximum entropy inductive logics – in spite
of the number of possible ways to deﬁne such inductive logics.

654
J. Landes
This uniqueness is particularly noteworthy in light of a string of results sug-
gesting and comparing diﬀerent notions of entropy (maximisation), which lead
to diﬀerent maximum entropy functions [5–7,10,13,18,20,27].
The Maximum Entropy Approach fails to provide probabilities for uncertain
inference for some evidence bases of quantiﬁer complexity Σ2 [43, § 2.2]. In these
cases, for all P ∈E there exists a Q ∈E such that Q ≻P and maxent E is hence
empty [26]. One way to sensibly deﬁne an inductive logic could be to consider
a binary relation which is reﬁned by ≻. Unfortunately, the most obvious way to
deﬁne such an inductive logic produces absurd results (Proposition 6). Finding
a way to sensibly deﬁne a (maximum entropy) inductive logic properly dealing
with such cases must be left to further study.
Further avenues for future research suggest themselves. Firstly, the ques-
tion arises whether the ﬁrst two here suggested modiﬁcations and the original
Maximum Entropy Approach agree more widely or whether they come apart in
important cases. If they do provide diﬀerent probabilities for inductive inference,
which of them is to be preferred and why? Secondly, are there further prima facie
plausible ways to modify the Maximum Entropy Approach? Thirdly, are there
other modiﬁcations of the Entropy Limit Approach? If so, how do they look like
and what are the implications for the induced inductive logics? Fourthly, what
is the status of the entropy limit conjecture [48, p. 191], the conjecture that
the Entropy Limit Approach and the Maximum Entropy Approach agree under
the assumption that the former is well-deﬁned, in light of these modiﬁcations?
Finally, cases with multiple uncertain premisses remain poorly understood and
pose a challenge to be tackled.
Acknowledgements. Many thanks to JeﬀParis, Soroush Raﬁee Rad, Alena Ven-
covsk´a and Jon Williamson for continued collaboration on maximum entropy inference.
I’m also indebted to anonymous referees who helped me improve this paper.
References
1. Barnett, O., Paris, J.B.: Maximum entropy inference with quantiﬁed knowledge.
Logic J. IGPL 16(1), 85–98 (2008). https://doi.org/10.1093/jigpal/jzm028
2. Carnap, R.: The two concepts of probability: the problem of probability. Philos.
Phenomenological Res. 5(4), 513–532 (1945). https://doi.org/10.2307/2102817
3. Carnap, R.: The Continuum of Inductive Methods. Chicago University of Chicago
Press, Chicago (1952)
4. Crupi, V.: Inductive logic. J. Philos. Logic 44(6), 641–650 (2015). https://doi.org/
10.1007/s10992-015-9348-8
5. Crupi, V., Nelson, J., Meder, B., Cevolani, G., Tentori, K.: Generalized information
theory meets human cognition: introducing a uniﬁed framework to model uncer-
tainty and information search. Cogn. Sci. 42, 1410–1456 (2018). https://doi.org/
10.1111/cogs.12613
6. Csisz´ar, I.: Axiomatic characterizations of information measures. Entropy 10(3),
261–273 (2008). https://doi.org/10.3390/e10030261
7. Cui, H., Liu, Q., Zhang, J., Kang, B.: An improved deng entropy and its application
in pattern recognition. IEEE Access 7, 18284–18292 (2019). https://doi.org/10.
1109/access.2019.2896286

A Triple Uniqueness of the Maximum Entropy Approach
655
8. Gaifman, H.: Concerning measures in ﬁrst order calculi. Isr. J. Math. 2(1), 1–18
(1964). https://doi.org/10.1007/BF02759729
9. Groves, T.: Lakatos’s criticism of Carnapian inductive logic was mistaken. J. Appl.
Logic 14, 3–21 (2016). https://doi.org/10.1016/j.jal.2015.09.014
10. Gr¨unwald, P.D., Dawid, A.P.: Game theory, maximum entropy, minimum discrep-
ancy and robust Bayesian decision theory. Ann. Stat. 32(4), 1367–1433 (2004).
https://doi.org/10.1214/009053604000000553
11. Haenni, R., Romeijn, J.W., Wheeler, G., Williamson, J.: Probabilistic Argumen-
tation, Synthese Library, vol. 350. Springer, Dordrecht (2011). https://doi.org/10.
1007/978-94-007-0008-6 3
12. Halpern, J.Y., Koller, D.: Representation dependence in probabilistic inference. J.
Artif. Intell. Res. 21, 319–356 (2004). https://doi.org/10.1613/jair.1292
13. Hanel, R., Thurner, S., Gell-Mann, M.: Generalized entropies and the transfor-
mation group of superstatistics. Proc. Nat. Acad. Sci. 108(16), 6390–6394 (2011).
https://doi.org/10.1073/pnas.1103539108
14. Howarth, E., Paris, J.B.: Pure inductive logic with functions. J. Symbolic Logic,
1–22 (2019). https://doi.org/10.1017/jsl.2017.49
15. Jaynes, E.T.: Probability Theory: The Logic of Science. Cambridge University
Press, Cambridge (2003)
16. Kließ, M.S., Paris, J.B.: Second order inductive logic and Wilmers’ principle. J.
Appl. Logic 12(4), 462–476 (2014). https://doi.org/10.1016/j.jal.2014.07.002
17. Landes, J.: The Principle of Spectrum Exchangeability within Inductive Logic.
Ph.D. thesis, Manchester Institute for Mathematical Sciences (2009). https://
jlandes.ﬁles.wordpress.com/2015/10/phdthesis.pdf
18. Landes, J.: Probabilism, entropies and strictly proper scoring rules. Int. J. Approx-
imate Reason. 63, 1–21 (2015). https://doi.org/10.1016/j.ijar.2015.05.007
19. Landes, J.: The entropy-limit (Conjecture) for Σ2-premisses. Stud. Logica. 109,
423–442 (2021). https://doi.org/10.1007/s11225-020-09912-3
20. Landes, J., Masterton, G.: Invariant equivocation. Erkenntnis 82, 141–167 (2017).
https://doi.org/10.1007/s10670-016-9810-1
21. Landes, J., Paris, J., Vencovsk´a, A.: Language invariance and spectrum exchange-
ability in inductive logic. In: Mellouli, K. (ed.) ECSQARU 2007. LNCS (LNAI),
vol. 4724, pp. 151–160. Springer, Heidelberg (2007). https://doi.org/10.1007/978-
3-540-75256-1 16
22. Landes, J., Paris, J.B., Vencovsk´a, A.: Some aspects of polyadic inductive logic.
Stud. Logica 90(1), 3–16 (2008). https://doi.org/10.1007/s11225-008-9140-7
23. Landes, J., Paris, J.B., Vencovsk´a, A.: Representation theorems for probability
functions satisfying spectrum exchangeability in inductive logic. Int. J. Approxi-
mate Reason. 51(1), 35–55 (2009). https://doi.org/10.1016/j.ijar.2009.07.001
24. Landes, J., Paris, J.B., Vencovsk´a, A.: A survey of some recent results on spectrum
exchangeability in polyadic inductive logic. Synthese 181, 19–47 (2011). https://
doi.org/10.1007/s11229-009-9711-9
25. Landes, J., Raﬁee Rad, S., Williamson, J.: Towards the entropy-limit conjecture.
Ann. Pure Appl. Logic 172, 102870 (2021). https://doi.org/10.1016/j.apal.2020.
102870
26. Landes, J., Raﬁee Rad, S., Williamson, J.: Determining maximal entropy functions
for objective Bayesian inductive logic (2022). Manuscript
27. Landes, J., Williamson, J.: Objective Bayesianism and the maximum entropy prin-
ciple. Entropy 15(9), 3528–3591 (2013). https://doi.org/10.3390/e15093528
28. Landes, J., Williamson, J.: Justifying objective Bayesianism on predicate lan-
guages. Entropy 17(4), 2459–2543 (2015). https://doi.org/10.3390/e17042459

656
J. Landes
29. Niiniluoto, I.: The development of the Hintikka program. In: Gabbay, D.M., Hart-
mann, S., Woods, J. (eds.) Handbook of the History of Logic, pp. 311–356. Elsevier,
Kidlington (2011). https://doi.org/10.1016/b978-0-444-52936-7.50009-4
30. Ognjanovi´c, Z., Raˇskovi´c, M., Markovi´c, Z.: Probability Logics. Springer, Cham
(2016). https://doi.org/10.1007/978-3-319-47012-2
31. Ortner, R., Leitgeb, H.: Mechanizing induction. In: Gabbay, D.M., Hartmann, S.,
Woods, J. (eds.) Handbook of the History of Logic, pp. 719–772. Elsevier (2011).
https://doi.org/10.1016/b978-0-444-52936-7.50018-5
32. Paris, J.B.: Common sense and maximum entropy. Synthese 117, 75–93 (1998).
https://doi.org/10.1023/A:1005081609010
33. Paris, J.B.: The Uncertain Reasoner’s Companion: A Mathematical Perspective,
2 edn. Cambridge Tracts in Theoretical Computer Science, vol. 39. Cambridge
University Press, Cambridge (2006)
34. Paris, J.B.: What you see is what you get. Entropy 16(11), 6186–6194 (2014).
https://doi.org/10.3390/e16116186
35. Paris, J.B., Rad, S.R.: A note on the least informative model of a theory. In:
Ferreira, F., L¨owe, B., Mayordomo, E., Mendes Gomes, L. (eds.) CiE 2010. LNCS,
vol. 6158, pp. 342–351. Springer, Heidelberg (2010). https://doi.org/10.1007/978-
3-642-13962-8 38
36. Paris, J.B., Vencovsk´a, A.: On the applicability of maximum entropy to inexact
reasoning. Int. J. Approximate Reason. 3(1), 1–34 (1989). https://doi.org/10.1016/
0888-613X(89)90012-1
37. Paris, J.B., Vencovsk´a, A.: A note on the inevitability of maximum entropy.
Int. J. Approximate Reason. 4(3), 183–223 (1990). https://doi.org/10.1016/0888-
613X(90)90020-3
38. Paris, J.B., Vencovsk´a, A.: In defense of the maximum entropy inference process.
Int. J. Approximate Reason. 17(1), 77–103 (1997). https://doi.org/10.1016/S0888-
613X(97)00014-5
39. Paris, J.B., Vencovsk´a, A.: Common sense and stochastic independence. In: Cor-
ﬁeld, D., Williamson, J. (eds.) Foundations of Bayesianism, pp. 203–240. Kluwer,
Dordrecht (2001)
40. Paris, J.B., Vencovsk´a, A.: Pure Inductive Logic. Cambridge University Press,
Cambridge (2015)
41. Paris, J.B., Vencovsk´a, A.: Six problems in pure inductive logic. J. Philos. Logic
(2019). https://doi.org/10.1007/s10992-018-9492-z
42. Raﬁee Rad, S.: Inference processes for ﬁrst order probabilistic languages. Ph.D. the-
sis, Manchester Institute for Mathematical Sciences (2009). http://www.raﬁeerad.
org/manthe.pdf
43. Raﬁee Rad, S.: Equivocation axiom on ﬁrst order languages. Stud. Logica. 105(1),
121–152 (2017). https://doi.org/10.1007/s11225-016-9684-x
44. Raﬁee Rad, S.: Maximum entropy models for Σ1 sentences. J. Logics Appl.
5(1),
287–300
(2018).
http://www.collegepublications.co.uk/admin/download.
php?ID=ifcolog00021
45. Williamson, J.: Objective Bayesian probabilistic logic. J. Algorithms 63(4), 167–
183 (2008). https://doi.org/10.1016/j.jalgor.2008.07.001
46. Williamson, J.: Objective Bayesianism with predicate languages. Synthese 163(3),
341–356 (2008). https://doi.org/10.1007/s11229-007-9298-y
47. Williamson, J.: In Defence of Objective Bayesianism. Oxford University Press,
Oxford (2010)
48. Williamson, J.: Lectures on Inductive Logic. Oxford University Press, Oxford
(2017)

A Logic and Computation for Popper’s
Conditional Probabilities
Shota Motoura(B)
Data Science Research Laboratories, NEC Corporation, Kawasaki, Kanagawa, Japan
motoura@nec.com
Abstract. A Popper function is one that gives a conditional probability of
propositional formulae. This paper gives a ﬁnite set of axioms that deﬁnes
the set of Popper functions in many-sorted monadic second-order logic,
and proves the decidability of the validity problem for a practically impor-
tant ﬁrst-order fragment of the second-order language with respect to a
set of Popper functions. Upon these logical foundations, we propose, with
results on their time complexity, two algorithms that compute the range
of values that designated conditional probabilities can take under given
constraints.
Keywords: Logic · Conditional probability · Popper function ·
Deﬁnability · Decidability · Computation · Complexity.
1
Introduction
The purpose of this paper is to propose, on the basis of theoretical foundations,
algorithms that directly compute the range of values that designated conditional
probabilities can take under given constraints.
Motivating Example. There are many constraints which can be written in terms
of conditional probabilities of propositional formulae. Examples are:
– P(5-yearSurvival|LungCancer) = 32.7%;
– P(5-yearSurvival|Cancer ∧Male) = 59.1%;
– P(LungCancer|Smoker) = 1.6P(LungCancer|¬Smoker), etc..
Our expected output of the algorithms is, for example:
32.5% ≤P(¬5-yearSurvival|LungCancer ∧¬Smoker ∧Male) ≤56.2%
which means that the probability that one will die within ﬁve years when a
person is a male non-smoker and diagnosed with lung cancer is greater than or
equal to 32.5% and less than or equal to 56.2%.1
1 The numbers are just randomly chosen for the sake of a concrete example.
c
⃝Springer Nature Switzerland AG 2021
J. Vejnarov´a and N. Wilson (Eds.): ECSQARU 2021, LNAI 12897, pp. 657–671, 2021.
https://doi.org/10.1007/978-3-030-86772-0_47

658
S. Motoura
Related Work. Regarding unconditional probabilities, Hailperin [17] gave the
upper and lower bounds of the probability P(ϕ) of a designated propositional
formula ϕ, under constraints in the form of a lower and a upper bounds ai ≤
P(ψi) ≤bi on ﬁnitely many formulae ψi. This result is generalised in [14], where
the decidability is proved for a Boolean combination of inequalities between
polynomial of unconditional probabilities. On the semantic side, several types of
possible-world semantics have been proposed and their complete axiomatisations
are given [6,14,19]. Papers [14,19] also refer indirect reasoning about conditional
probability, that is, reducing it to reasoning about unconditional probability.
On the syntactic side, Boriˇci´c [9] and [8] propose sequent calculus and natural
deduction, respectively, for the probabilistic semantics.2
As for direct reasoning about conditional probability, a problem is what value
P(A|B) should take where the probability of the conditioning part P(B) is equal
to 0 (cf. [18]). Adams in [1,2] deﬁned P(A|B) to be 1 following ‘a conventional
stipulation’ [3], while Bacchus in [6] deﬁned it to be 0. Concerning this problem,
Popper, among others, treats a conditional probability as a primitive notion and
proposed, in [27], a kind of function that assigns a conditional probability to
each pair of formulae together with a set of axioms that the function should
obey. Such a function is now called a Popper function.3 In this paper, we take
the Popper functions to give concrete examples.4
About the reasoning task such as the motivating example above, Gilio [16]
and Wagner [31], show how non-strict upper and lower bounds of a conditional
probability propagate along formal systems, called System P and modus tollens,
respectively. Another related work is Ikodinovi´c and Ognjanovi´c [21]. It proves
the completeness and the decidability for a logic with operator CP≥s(A|B)
expressing P(A|B) ≥s, although operator CP≥s(A|B) is not deﬁned where
B is a propositional contradiction.
Our Contributions. In this paper, we:
1. give a ﬁnite set of axioms in many-sorted monadic second-order logic that
deﬁnes the set of Popper functions, including their domain and codomain;
2. prove the decidability of the validity problem for a practically important ﬁrst-
order fragment of the second-order language with respect to the set of Popper
functions;
3. propose two algorithms that compute the range of the values that designated
conditional probabilities can take under given constraints.
2 In this sequent calculus, the probability is assigned to a sequent but it can be proved
equal to that of the corresponding material implication.
3 A Popper function has a salient characterisation as the standard part of a condi-
tional probability of hyperreal-valued probabilities [24]. This result is extended to
the representation theorem by non-Archimedean probabilities in [10]. Thanks to an
anonymous referee for pointing this out.
4 However, our theorems and algorithms may apply to other cases, including those of
P(A|B) = 1 or 0 for P(B) = 0.

A Logic and Computation for Popper’s Conditional Probabilities
659
Organisation. In Sect. 2, we recall the deﬁnition of a Popper function. Section 3
gives the deﬁnability result for many-sorted monadic second-order logic for Pop-
per functions. In Sect. 4, we prove that the validity problem with respect to the
set of Popper functions is decidable for an important ﬁrst-order fragment of this
second-order language, and, in Sect. 5, we propose two algorithms that compute
the range of values that designated conditional probabilities can take and show
results on their complexity.
2
Popper Functions
Let us begin with recalling the deﬁnition of Popper function, which gives a kind
of conditional probability of each pair of propositional formulae.
We ﬁrst specify the set of formulae on which Popper functions are deﬁned.
Throughout the paper, we ﬁx a nonempty and ﬁnite set A of proposition letters.
Deﬁnition 1. The propositional formulae are deﬁned by the rule ϕ :: = a | ¬ϕ |
ϕ∧ϕ, where a ranges over A . The set of propositional formulae are denoted by L .
Since a Popper function depends on syntax, we explicitly deﬁne logical con-
nectives ϕ ∨ψ, ϕ →ψ and ϕ ↔ψ to be ¬(¬ϕ ∧¬ψ), ¬(ϕ ∧¬ψ) and
(ϕ →ψ) ∧(ψ →ϕ), respectively.
On this propositional language, a Popper function is deﬁned as below:
Deﬁnition 2 (Popper Functions [27]). A Popper function on L is a function
P : L ×L →R that satisﬁes the following conditions (notation: P(ϕ|ψ) instead
of P(ϕ, ψ)):
P1 for any ϕ and ψ in L , there exist ϕ′ and ψ′ in L such that P(ϕ|ψ) ̸=
P(ϕ′|ψ′);
P2 for any ϕ and ψ in L , P(ϕ|χ) = P(ψ|χ) for all χ in L implies P(γ|ϕ) =
P(γ|ψ) for all γ in L ;
P3 for any ϕ and ψ in L , P(ϕ|ϕ) = P(ψ|ψ);
P4 for any ϕ, ψ and χ in L , P(ϕ ∧ψ|χ) ≤P(ϕ|χ);
P5 for any ϕ, ψ and χ in L , P(ϕ ∧ψ|χ) = P(ϕ|ψ ∧χ)P(ψ|χ);
P6 for any ϕ and ψ in L , P(ϕ|ψ)+P(¬ϕ|ψ) = P(ψ|ψ) unless P(χ|ψ) = P(ψ|ψ)
for all χ in L .
We denote the set of all Popper functions by P.
3
Deﬁnability
A Popper function is a special case of a function from the product of two copies
of a formula algebra to the ﬁeld of real numbers, where the formula algebra
(L , F¬, F∧) is deﬁned by F¬(ϕ) := ¬ϕ and F∧(ϕ, ψ) := ϕ ∧ψ for any ϕ and ψ
in L . Therefore, as we shall see, a Popper function can be seen as a structure
for many-sorted monadic second-order logic with the sorts {form, real} for the
propositional formulae and the real numbers. In this section, we prove that the
set of Popper functions for A is ﬁnitely axiomatisable in many-sorted monadic
second-order logic.

660
S. Motoura
3.1
Many-Sorted Monadic Second-Order Logic for Popper
Functions
We ﬁrst specify the many-sorted monadic second-order logic which we shall use.
(cf. [15,26]) The language of our logic is given as follows:
Deﬁnition 3 (Alphabet). The alphabet Σ = (S , F, R, V1, V2) of our lan-
guage consists of:
1. a set of sorts S = {form, real};
2. a set F of function symbols: a : form for a ∈A , f¬ : form →form, f∧:
form×form →form, 0 : real, 1 : real, + : real×real →real, · : real×real →real,
p : form × form →real
3. a set of a relation symbol R = {≤: real × real};
4. the union V1 of two disjoint inﬁnite sets of ﬁrst-order variables for each of
form and real: V 1
form = {x : form, . . .} and V 1
real = {y : real, . . .};
5. the union of two disjoint inﬁnite sets of monadic second-order variables for
each of form and real: V 2
form = {X : form, . . .} and V 2
real = {Y : real, . . .}.
In addition to them, we use =σ as the logical symbol for equality for sort σ ∈
{form, real}, that is, =σ is interpreted in the standard manner.
Deﬁnition 4 (Formulae)
1. The terms are deﬁned by the following rule: t
:
σ :: =
x
:
σ
|
f(t1 : σ1, . . . , tn : σn) : σ, where x : σ ranges over V 1
σ for σ ∈{form, real}
and f : σ1 × · · · × σn →σ over F.
2. The atomic formulae are deﬁned by: α :: = t : σ =σ s : σ | t : real ≤s : real,
where t : σ and s : σ range over the set of terms, and t : real and s : real over
the real-terms.
3. The formulae are deﬁned by: ϕ :: = α | ¬ϕ | ϕ ∧ϕ | (∃x : σ)ϕ | (∃X : σ)ϕ,
where α ranges over the set of atomic formulae, and x : σ and X : σ over V 1
σ
and V 2
σ for σ ∈{form, real}, respectively.
We denote by L 2
PF the set of formulae deﬁned above.
We write ¬x and x ∧y for f∧(x, y) and f¬(x), respectively, when there is no
confusion. In addition, we suppress the sorts σ in terms and formulae when
they are clear from the context. We deﬁne (∀x)ϕ to be ¬(∃x)¬ϕ as usual and
use (∃x1 . . . xn) and (∀x1 . . . xn) instead of (∃x1) · · · (∃xn) and (∀x1) · · · (∀xn),
respectively, and s ̸= t instead of ¬s = t.
The clauses in the deﬁnition of a Popper function (Deﬁnition 2) can be
directly translated into this language:
Example 1. The following set of axioms for Popper functions is denoted by PFL:
P1 (∀xy)(∃x′y′)p(x|y) ̸= p(x′|y′)
P2 (∀xy)(((∀z)p(x|z) = p(y|z)) →((∀w)p(w|x) = p(w|y)))
P3 (∀xy)p(x|x) = p(y|y)

A Logic and Computation for Popper’s Conditional Probabilities
661
P4 (∀xyz)p(x∧y|z) ≤p(x|z)
P5 (∀xyz)p(x∧y)|z) = p(x|y∧z) · p(y|z)
P6 (∀xy)((¬(∀z)p(z|y) = p(y|y)) →(p(x|y) + p(¬x|y) = p(y|y))).
⊣
The structures for this language are given by:
Deﬁnition 5 (Σ-structures)
– A Σ-structure M = (Mform, Mreal, pM) is given as follows:
• Mform = (S, f M
¬ , f M
∧) and Mreal = (R, 0M, 1M, +M, ·M);
• pM : S × S →R.
We call Mform and Mreal structures for form and real, respectively.
– An assignment μ
=
(μform, μreal) assigns the following for each σ
∈
{form, real}:
• to each ﬁrst-order variable x : σ an element μσ(x) ∈Mσ;
• to each monadic second-order variable X : σ a subset μσ(X) ⊆Mσ.
In what follows, we identify a Σ-structure, (Mform, Mreal, pM), and a function,
pM : Mform × Mform →Mreal. For example, a Popper function P : L × L →R
is identiﬁed with the Σ-structure((L , F¬, F∧), (R, 0, 1, +, ·), P).
The interpretation is deﬁned as usual:
Deﬁnition 6 (Interpretation of Terms). Given a Σ-structure M and an
assignment μ, the terms are inductively interpreted as:
– aM[μ] = aM for any a ∈A ;
– cM[μ] = cM for each c = {0, 1};
– (x : σ)M[μ] = μσ(x : σ) for x : σ ∈V 1
σ with σ ∈{form, real};
– f(t1, . . . tn)M[μ] = f M(t1M[μ], . . . , tnM[μ]) for f = f¬, f∧, +, ·, p;
– (X : σ)M[μ] = μσ(X : σ) for X : σ ∈V 2
σ with σ ∈{form, real}.
Deﬁnition 7 (Satisfaction Relation). Let M be a Σ-structure and μ an
assignment. For every formula ϕ in L 2
PF, the statement that ϕ is satisﬁed by
μ in M (notation: M |= ϕ[μ]) is inductively deﬁned by the following clauses:
M |= (t : σ = s : σ)[μ]
iﬀtM[μ] = sM[μ]
M |= (t : real ≤s : real)[μ] iﬀtM[μ] ≤sM[μ]
M |= X(t)[μ]
iﬀtM[μ] ∈μ(X)
M |= ¬ϕ[μ]
iﬀM ̸|= ϕ[μ]
M |= ϕ ∧ψ[μ]
iﬀM |= ϕ[μ] and M |= ψ[μ]
M |= (∃x : σ)ϕ[μ]
iﬀ
there is an element e of Mσ s.t. M |= ϕ[μ(e/x)]
M |= (∃X : σ)ϕ[μ]
iﬀ
there is a subset S of Mσ s.t. M |= ϕ[μ(S/X)]
Here, σ is form and real, and μ(e/x) (respectively, μ(S/X)) is the same assign-
ment as μ except that μ(e/x) assigns e to x (respectively, μ(S/X) assigns S to
X).
If ϕ is satisﬁed by μ in M for any assignment μ, we say that M satisﬁes ϕ or that
M is a model of ϕ and use the notation M |= ϕ. These notions and notations
are naturally extended to the cases where a class C of models and/or a set Φ
of formulae are considered, namely, C |= Φ, C |= ϕ and M |= Φ. In particular,
for the set P of Popper functions, we say that a formula ϕ is valid or P-valid
if P |= ϕ.

662
S. Motoura
3.2
Deﬁnability in Second-Order Logic
As we have seen, a Popper function can be identiﬁed with a structure of the
second-order logic deﬁned above. We now prove that the set of Popper functions
P : L × L →R is deﬁnable by ﬁnitely many axioms written in L 2
PF. As a
Popper function consists of three parts, the formula algebra L , the ﬁeld R of
real numbers and the function itself P, we conﬁrm the deﬁnability of the domain
L and the codomain R and then prove that of the set of Popper functions.
Firstly, the set of axioms given below deﬁnes the formula algebra (L , F¬, F∧):
Deﬁnition 8. We denote the set of the following axioms by FL:
F1 (∀xx′)¬x = ¬x′ →x = x′
F2 (∀xyx′y′)x∧y = x′∧y′ →(x = x′) ∧(y = y′)
F3 (∀xyz)¬x ̸= y∧z
F4 (∀xy)a ̸= x∧y for any a ∈A
F5 (∀y)a ̸= ¬x for any a ∈A
F6 (∀X)[[(
a∈A X(a))∧(∀x)(X(x) →X(¬x))∧(∀xy)(X(x)∧X(y) →X(x∧y))]
→(∀x)X(x)]
This is a straightforward extension of Peano’s deﬁnition of the set of natural
numbers (cf. [13]). Note that (
a∈A X(a)) in axiom F6 is legitimate as A is
assumed to be ﬁnite.
Theorem 1. FL uniquely deﬁnes the formula algebra generated from A up to
isomorphism.
Proof. This can be proved by straightforwardly extending the ordinary proof for
the case of Peano’s deﬁnition of natural numbers (cf. [29]).
⊓⊔
Combining Theorem 1 above and the well-known fact that there is a ﬁnite
set RFL of axioms that deﬁnes the ﬁeld of reals to be the Dedekind-complete
ordered ﬁeld, we can readily see that the union FL∪RFL∪PFL deﬁnes the Popper
functions:
Theorem 2. Let p : Mform × Mform →Mreal be a Σ-structure. Then, p satisﬁes
FL ∪RFL ∪PFL if and only if p is isomorphic to a Popper function in the sense
that there exist a Popper function P and a pair of isomorphisms f : Mform →
(L , F¬, F∧) and g : Mreal →R satisfying P◦(f × f) = g◦p, where f × f is
the mapping from Mform × Mform to (L , F¬, F∧) × (L , F¬, F∧) such that (f ×
f)(x, y) = (f(x), f(y)) for any (x, y) in Mform × Mform.
Proof. We sketch the proof. Since FL and RFL deﬁne the formula algebra and
the ﬁeld of reals uniquely up to isomorphism, there are f : Mform →(L , F¬, F∧)
and g : Mreal →R. By their bijectivity and homomorphism conditions, we see
that P = g ◦p ◦(f −1 × f −1) is a Popper function and that it satisﬁes that
P ◦(f × f) = g ◦p.
⊓⊔

A Logic and Computation for Popper’s Conditional Probabilities
663
4
Decidability
In this section, we consider the ﬁrst-order fragment LPF of L 2
PF. Accordingly,
we consider an assignment μ = (μform, μreal) restricted to the sets V 1
form and V 1
real
of the ﬁrst-order variables, i.e. μform : V 1
form →L and μreal : V 1
real →R.
The problem of determining whether a given formula ϕ is P-valid is called
the P-validity problem. In this section, we prove the decidability of the P-
validity problem for a speciﬁc set of sentence which we shall call Popper sentence.
A Popper sentence is one in LPF where form terms appear only in the scopes of
the occurrences of function symbol p. More precisely:
Deﬁnition 9. The Popper formulae are deﬁned by the following rules:
1. the terms of form are inductively deﬁned from the variables x : form in V 1
form
and the constant symbols a in A by using function symbols f¬ and f∧;
2. the terms of real are inductively deﬁned from the variables x : real in V 1
real,
constant symbols 0 and 1, and p(t|s) with form terms t and s by −and ·;
3. the formulae are deﬁned by:
ϕ ::= t : real = s : real | t : real ≤s : real | ¬ϕ | ϕ ∧ϕ | (∃x : real)ϕ | (∃x : form)ϕ
We denote the set of Popper formulae by PF. A Popper formula is called a
Popper sentence if it does not contain any free variable, and the set of Popper
sentences is denoted by PS.
For example, P1-P6 in PFL (Example 1) are all Popper sentences. A Popper sen-
tence is of practical importance as it can express a relation between conditional
probabilities.
In our decision procedure, we ﬁrst reduce, through three translations, the P-
validity problem of a given Popper sentence to the validity problem of a formula
of the ﬁrst-order logic of the ﬁeld of real numbers.
4.1
Translation 1
The ﬁrst translation is based on the following fact:
Theorem 3. Let P be a Popper function. If ϕ ↔ϕ′ and ψ ↔ψ′ are tautologies
in classical propositional logic, then we have P(ϕ|ψ) = P(ϕ′|ψ′).
Proof. We only sketch the proof. We ﬁrst prove that, for any propositional for-
mula χ, if a sequent ϕ1, . . . , ϕn→ψ1, . . . , ψm is provable in propositional sequent
calculus PK (cf. [11]), then we have P(¬(ϕ1∧· · ·∧ϕn∧¬ψ1∧· · ·∧¬ψm)|χ) = 1, by
the induction on the construction of the PK-proof. Using this fact and elemen-
tary results on Popper functions (cf. APPENDIX *v of [27]), we see that ϕ →ψ
implies P(ϕ|χ) ≤P(ψ|χ) for all χ and thus ϕ ↔ψ implies P(ϕ|χ) = P(ψ|χ) for
all χ. We then obtain the theorem by P2.
⊓⊔
This leads to the notion of a ‘Popper function’ whose domain is a
Lindenbaum-Tarski algebra:

664
S. Motoura
Deﬁnition 10 (cf. [12]). The Lindenbaum-Tarski algebra L over the set A of
proposition letters is the structure (L, −, ·), where
1. L is the set {[ϕ] | ϕ ∈L } of equivalence classes for the classical equivalence
relation ↔;
2. −is a unary operation on L deﬁned by −[ϕ] = [¬ϕ];
3. · is a binary operation on L deﬁned by [ϕ] · [ψ] = [ϕ ∧ψ].
Deﬁnition 11 (Quotient Popper Functions). A quotient Popper function
is a function Q : L × L →R such that:
1. for any x, y ∈L, there exist x′, y′ ∈L such that Q(x|y) ̸= Q(x′|y′);
2. for any x, y ∈L, Q(x|z) = Q(y|z) for all z ∈L implies Q(w|x) = Q(w|y) for
all w ∈L;
3. for any x, y ∈L, Q(x|x) = Q(y|y);
4. for any x, y, z ∈L, Q(x · y|z) ≤Q(x|z);
5. for any x, y, z ∈L, Q(x · y|z) = Q(x|y · z)Q(y|z);
6. for any x, y ∈L, Q(x|y) + Q(−x|y) = Q(y|y) unless Q(z|y) = Q(y|y) for all
z ∈L.
We denote, by Q, the set of all quotient Popper functions.
The target language of the ﬁrst translation is one for the quotient Popper
functions. It is the same as LPF, except for the form constant symbols:
Deﬁnition 12. Language LQPF is many-sorted ﬁrst-order language deﬁned by
the alphabet (S , F ′, R, V1) together with logical symbols =σ with σ ∈{form, real}
for equality, where F ′ consists of f¬, f∧, 0, 1, −, ·, p and constant symbols
z : form for all z ∈L.
The notion of a structure and the interpretation of terms and formulae are
deﬁned in an analogous way to those for the ﬁrst-order fragment of L 2
PF. A
quotient Popper function is a structure for LQPF. A quotient Popper formula
and a quotient Popper sentence are a formula and a sentence in LQPF whose
occurrences of form terms only appear in the scopes of the occurrences of p. We
denote the set of quotient Popper formulae by QPF.
Deﬁnition 13 (Translation 1). We give a translation T1 : PF →QPF by
induction on the construction of a Popper formula (in the induction, we deﬁne
the translation of a term too): T1(a) = [a] for a ∈A and T1(x) = x for x ∈V1;
for the other symbols, T1 translates a term and a formula homomorphically.
This translation T1 preserves the validity in the following sense:
Proposition 4. Let ϕ be a Popper sentence. Then, we have that P |= ϕ iﬀ
Q |= T1(ϕ).
Proof. This can be proved by using the facts that, for any Popper function P,
the function ¯P deﬁned by ¯P([ϕ]|[ψ]) = P(ϕ|ψ) is a quotient Popper function
and that, for any quotient Popper function Q, there is a Popper function P such
that Q = ¯P.
⊓⊔

A Logic and Computation for Popper’s Conditional Probabilities
665
4.2
Translation 2
The second translation eliminates the occurrences of form quantiﬁers:
Deﬁnition 14 (Translation 2). We deﬁne a translation T2 : QPF →QPF
by: T2(t) = t for any term t; T2((∃x : form)ϕ) = 
z∈L T2(ϕ)[z/x] for any formula
ϕ; for the other logical connectives, T2 translates a formula homomorphically.
This translation also preserves the validity:
Proposition 5. For any quotient Popper sentence ϕ, it holds that Q |= ϕ iﬀ
Q |= T2(ϕ).
4.3
Translation 3
Now we give the deﬁnition of the ﬁnal translation into a ﬁrst-order language LR
for the ﬁeld of real numbers, i.e. a language for real closed ﬁelds.
Deﬁnition 15. The ﬁrst-order language LR is constructed from the variables
{V(x|y) | x, y ∈L} and V 1
real by function symbols −and · and relation symbols ≤
and =σ.
Deﬁnition 16. For a variable-free form term t in LQPF, its corresponding ele-
ment L(t) in L is inductively deﬁned by:
1. L(z) = z for any form constant symbol z in LQPF;
2. L(¬t) = −L(t) for any variable-free form term ¬t in LQPF;
3. L(t ∧s) = L(t) · L(s) for any variable-free form terms t ∧s in LQPF.
Deﬁnition 17 (Translation
3). The translation T3
:
form-variable-free
QPF →LR is inductively deﬁned by the clause that T1(p(t|s)) = V(L(t)|L(s)).
For the other symbols, T3 maps a term and a formula homomorphically.
This translation again preserves the validity as follows:
Proposition 6. Let ϕ be a quotient Popper formula which does not contain any
form variables and η = (ηform, ηreal) an assignment with ηform : V 1
form →L and
ηreal : V 1
real →R. Then, we have:
Q |= ϕ[η] iﬀR |= (∀V(x|y))(x,y)∈L2(T3 ◦T2 ◦T1(PFL) →T3(ϕ))[ηreal].
Proof. We sketch the proof. For the left-to-right direction, assume the negation
of the right-hand side to prove the contraposition. Then, there is a function F :
L×L →R such that R |= T3◦T2◦T1(PFL)∧¬T3(ϕ)[ηreal(F(x, y)/V(x|y))(x,y)∈L2].
The ﬁrst conjunct implies that F is a quotient Popper function; and the right
one does F ̸|= ϕ[η]. For the other direction, from the right-hand side, we see
that R |= T3 ◦T2 ◦T1(PFL) →T3(ϕ)[ηreal(Q(x|y)/V(x|y))(x,y)∈L2] for any quotient
Popper function Q : L × L →R. Since Q is a quotient Popper function, the
antecedent is true. Therefore, we obtain R |= T3(ϕ)[ηreal(Q(x|y)/V(x|y))(x,y)∈L2]
and thus Q |= ϕ[η].
⊓⊔

666
S. Motoura
Using the three translations, we obtain the decidability:
Theorem 7. The P-validity problem of any Popper sentence is decidable.
Proof. Let ϕ be a Popper sentence. Then, by Propositions 4, 5 and 6, we have
that P |= ϕ iﬀR |= (∀V(x|y))(x,y)∈L(T3 ◦T2 ◦T1(PFL) →T3◦T2 ◦T1(ϕ)). Since
the validity problem for the logic of the ﬁeld of real numbers, i.e. the logic of
real closed ﬁelds, is decidable [30] and since T1, T2 and T3 are computable,5 the
P-validity problem of ϕ is decidable.
⊓⊔
5
Computing Algorithms
In this section, we propose two algorithms which compute the region of the
values that designated conditional probabilities can take under given constraints:
the ﬁrst algorithm outputs a Boolean combination of equations and inequalities
that determines the region; the second one evaluates the region from inside and
outside with arbitrary accuracy. We also give results on their computational
complexity. In what follows, we assume that the language LPFL is extended so
that it has one constant symbol for each real number.6
5.1
Algorithms
Algorithm 1. The ﬁrst algorithm proceeds as follows:
Algorithm 1: Equations and inequalities
input : (i) probabilities p(ϕ1|ψ1), . . . , p(ϕn|ψn) in syntax that we would
like to compute such that at least ϕi ↔ϕi′ or ψi ↔ψi′ is not a
tautology for any pair i ̸= i′
(ii) Popper sentences χ1, . . . , χm that express constraints
output: a Boolean combination of equations and inequalities that
determines the range
{(P(ϕ1|ψ1), . . . , P(ϕn|ψn)) ∈Rn | P |= χj for j = 1, . . . , m}
X ←−χ1 ∧· · · ∧χm;
L′ ←−L2 −{([ϕi], [ψi]) ∈L2 | i = 1, . . . , n};
Φ ←−(∃V(x|y))(x,y)∈L′(T3 ◦T2 ◦T1(PFL) ∧T3 ◦T2 ◦T1(X));
return QER(Φ);
In the algorithm above, QER denotes a quantiﬁer elimination (QE) algorithm in
real closed ﬁelds (cf. [5,7,25]).
The theorem behind the algorithm is as below:
5 For translation T1, we use the fact that, for any propositional formula x in L ,
its corresponding equivalence class [x] in the Lindenbaum-Tarski algebra L can be
calculated.
6 This assumption relieves the range of formulae applicable to the results on complexity
in Sect. 5.2 (see also Theorem 11, infra). All results in Sect. 5 also hold in the original
language.

A Logic and Computation for Popper’s Conditional Probabilities
667
Theorem 8. Suppose the settings of Algorithm 1. Then, the following are
equivalent:
1. (V([ϕ1]|[ψ1]), . . . , V([ϕn]|[ψn])) = (r1, . . . , rn) ∈Rn is a solution of the system of
equations and inequalities QER(Φ);
2. there is a Popper function P with P(ϕi|ψi) = ri for i = 1, . . . , n that satisﬁes
all the constraints χ1, . . . , χm.
Algorithm 2. Algorithm 1 does not necessarily output a human-readable solu-
tion such as 1/3 ≤p(a|b) ≤1/2 and, in the worst case, it contains an equation or
an inequalities that cannot be solved algebraically. To deal with such cases, we
here propose another algorithm that evaluates the range from inside and outside
with arbitrary accuracy:
Algorithm 2: Evaluation algorithm
input : (i) probabilities p(ϕ1|ψ1), . . . , p(ϕn|ψn) in syntax that we would
like to compute such that at least ϕi ↔ϕi′ or ψi ↔ψi′ is not a
tautology for any pair i ̸= i′
(ii) Popper sentences χ1, . . . , χm that express constraints
(iii) a non-zero precision parameter N ∈N
output: the assignment that assigns, to each n-dimensional hypercube
1/N on a side in [0, 1]n, one of ‘Inside’, ‘Boundary’ and ‘Outside’
of the range
{(P(ϕ1|ψ1), . . . , P(ϕn|ψn)) ∈Rn | P |= χj for j = 1, . . . , m}
H ←−the set of N n-many hypercubes 1/N on a side obtained by dividing
[0, 1]n;
foreach hypercube h = [k1/N, (k1 + 1)/N] × · · · × [kn/N, (kn + 1)/N] ∈H
do
X ←−χ1 ∧· · · ∧χm;
L′ ←−L2 −{([ϕi], [ψi]) ∈L2 | i = 1, . . . , n};
Φ ←−(∃V(x|y))(x,y)∈L′(T3 ◦T2 ◦T1(PFL) ∧T3 ◦T2 ◦T1(X));
Φ1 ←−(∀V([ϕi]|[ψi]))n
i=1(n
i=1(ki/N ≤V([ϕi]|[ψi]) ≤(ki + 1)/N) →Φ);
Φ2 ←−(∃V([ϕi]|[ψi]))n
i=1(n
i=1(ki/N ≤V([ϕi]|[ψi]) ≤(ki + 1)/N) ∧Φ);
if QER(Φ1) = ⊤then zh ←−‘Inside’;
else if QER(Φ2) = ⊤then zh ←−‘Boundary’;
else zh ←−‘Outside’;
return {zh}h∈H;
Here, ki/N ≤V([ϕi]|[ψi]) ≤(ki + 1)/N is an abbreviation of ki/N ≤V([ϕi]|[ψi]) ∧
V([ϕi]|[ψi]) ≤(ki + 1)/N.
This algorithm determines, for each hypercube h, whether it is inside,
outside or on the boundary of the range of the values that probabilities
(P(ϕ1|ψ1), . . . , P(ϕn|ψn)) can take. Intuitively, Φ1 (respectively, Φ2) says that,
for any (respectively, some) point r in a hypercube h, there is a Popper function
P with (P(ϕ1|ψ1), . . . , P(ϕn|ψn)) = r that satisﬁes the constraints χ1, . . . , χm.
More precisely, the following holds:

668
S. Motoura
Theorem 9. Suppose the settings of Algorithm 2. Then, the following are
equivalent:
1. R |= Φ1 (respectively, R |= Φ2);
2. for any (respectively, some) (r1, . . . , rn)
∈
[k1/N, (k1 + 1)/N] × · · · ×
[kn/N, (kn + 1)/N], there exists a Popper function P with P(ϕi|ψi) = ri
for i = 1, . . . , n that satisﬁes all the constraints χ1, . . . , χm.
5.2
Complexity of Quantiﬁer Elimination in the Algorithms
We next give results on the computational complexity of Algorithms 1 and 2.
The primary parts of the algorithms are QER on input Φ in Algorithm 1, and on
Φ1 and Φ2 in Algorithm 2. We assess their time complexities.
There have been proposed several QE algorithms (cf. [5]). We, in this paper,
apply the following results given in [7]:
Theorem 10 (cf. Theorems 1.3.1 and 1.3.2 in [7]). Let P1, . . . , Ps be s
polynomials each of degree at most d, in k + l variables, with coeﬃcients in a
real closed ﬁeld R and
Ψ = (QwX[w]) · · · (Q1X[1])F(P1, . . . , Ps)
be a ﬁrst-order formula where Qi ∈{∀, ∃}, Qi ̸= Qi+1, Y = (Y1, . . . , Yl) is a
block of l free variables, X[i] is a block of ki variables with Σ1≤i≤wki = k, and
F(P1, . . . , Ps) is a quantiﬁer-free Boolean formula with atomic predicates of the
form
sign(Pi(Y, X[w], . . . , X[1])) = σ
with σ ∈{0, 1, −1}, where the sign, sign(a), of an element a ∈R is 0 if a = 0,
1 if a > 0 and −1 if a < 0. Then,
1. there is an algorithm that computes quantiﬁer-free formula equivalent to Ψ,
using s(l+1)  (ki+1)d(l+1)  O(ki) arithmetic operations in D;
2. when l = 0, there is an algorithm to decide the truth of Ψ that uses
s
 (ki+1)d
 O(ki) arithmetic operations in D.
Here, D denotes the smallest subring of R containing the coeﬃcients of
P1, . . . , Ps.
Using this theorem, the time complexities of QER on input Φ, Φ1 and Φ2 are
assessed as follows:
Theorem 11. Let
p(ϕ1|ψ1), . . . , p(ϕn|ψn)
be
probabilities
in
syntax
and
χ1, . . . , χm be the Popper sentence that does not contain real quantiﬁers.7 For
each χj, let ej denote the number of its form quantiﬁers, sj the number of
its polynomials, dj the maximum total degree of its polynomials. Then, for
Ψ = T3 ◦T2 ◦T1(PFL) ∧T3 ◦T2 ◦T1(m
j=1 χj),
7 This condition does not restrict the range of constraints χ1, . . . , χn in practice, since
our language now contains the constant symbols for all real numbers.

A Logic and Computation for Popper’s Conditional Probabilities
669
1. the number of its polynomials is bounded by s = |L|4 + 5|L|3 + 2|L| +
m
j=1(|L|ejsj);
2. the maximum degree of its polynomials is bounded by d = max{2, d1, . . . , dm}.
Hence, the time complexity, in terms of arithmetic operations, of the algorithm,
on input Φ in Algorithm 1 is s(n+1)(|L|2−n+1)d(n+1)O(|L|2−n); and both of those
on input Φ1 and Φ2 in Algorithm 2 are s(|L|2−n+1)(n+1)dO(|L|2−n)O(n).
Proof. We give here a proof sketch. For (1), it can be easily seen that the number
of polynomials in T3◦T2◦T1(PFL) is |L|4+5|L|3+2|L|. In addition, each T3◦T2◦
T1(χi) has |L|eisi polynomials at most, since each form quantiﬁer gives |L|-many
copies of its scope when it is translated by T2. Adding up, we see that the number
of polynomials in Ψ is bounded by s = |L|4 + 5|L|3 + 2|L| + m
j=1(|L|eisi). For
(2), the maximum number of degrees of polynomials in T3 ◦T2 ◦T1(PFL) is 2
and the maximum degree of the polynomials in T3 ◦T2 ◦T1(χj) is bounded by
the maximum total degree of the polynomials. Therefore, since T1, T2 and T3 do
not increase the total degree of a polynomial in χj, the maximum degree of the
polynomials in Ψ is bounded by max{2, d1, . . . , dn}.
To assess the complexities, we apply Theorem 10. For Algorithm 1, the num-
ber of free real variables in Ψ is |{V(s|t)
| s, t ∈L}| = |L|2 and thus that
of Φ is n. Note also the number of polynomials input to the QE algorithms
becomes at most 2s by transforming an atomic formula in the form of P ≤Q
into sign(P −Q) = −1∨sign(P −Q) = 0 to adjust the syntax with the maximal
degree of polynomials preserved; however, this multiplier 2 is incorporated into
dO(n). For algorithm 2, input formulae Φ1 and Φ2 can readily be transformed
into prenex normal form and again we apply Theorem 10.
⊓⊔
6
Conclusion and Future Work
6.1
Conclusion
In this paper, we have proposed a ﬁnite set of axioms that deﬁnes the set of Pop-
per functions in many-sorted monadic second-order logic. We have also proved
the decidability of the validity problem for an important ﬁrst-order fragment of
this second-order language, which we call Popper sentence, with respect to the
set of Popper functions. We ﬁnally proposed two algorithms that compute the
range of values that designated conditional probabilities can take under given
constraints expressed by ﬁnitely many Popper sentences, and assessed the time
complexities of their primary subalgorithms.
6.2
Future Work
Theoretically, this study is the ﬁrst step of our project of systematising the
relationship between probability-related algorithms, including Markov logic net-
works [28], Bayesian logic programming [22,23] and abductive reasoning [4,20].
Our algorithm shall be used as a baseline in our project.

670
S. Motoura
Practically, we shall address the following issues:
1. we shall relieve the restriction that the set A of proposition letters is speciﬁed
since we cannot assume in practice that all involved proposition letters are
speciﬁed;
2. we shall reduce the computational complexities of the proposed algorithms
so that they can be used in practice.
Acknowledgements. The author would like to thank sincerely Takashi Maruyama,
Takeshi Tsukada, Shun’ichi Yokoyama, Eiji Yumoto and Itaru Hosomi for discussion
and many helpful comments. Thanks are also due to all anonymous referees who gave
valuable comments on earlier versions of this article.
References
1. Adams, E.: The logic of conditionals. Inquiry 8(1–4), 166–197 (1965). https://doi.
org/10.1080/00201746508601430
2. Adams, E.W.: Probability and the logic of conditionals. In: Hintikka, J., Sup-
pes, P. (eds.) Aspects of Inductive Logic, Studies in Logic and the Founda-
tions of Mathematics, vol. 43, pp. 265–316. Elsevier (1966). https://doi.org/
10.1016/S0049-237X(08)71673-2, https://www.sciencedirect.com/science/article/
pii/S0049237X08716732
3. Adams, E.W.: The Logic of Conditionals: An Application of Probability to Deduc-
tive Logic, Synthese Library, vol. 86. Springer, Netherlands (1975). https://doi.
org/10.1007/978-94-015-7622-2
4. Aliseda, A.: The logic of abduction: an introduction. In: Magnani, L., Bertolotti,
T. (eds.) Springer Handbook of Model-Based Science. SH, pp. 219–230. Springer,
Cham (2017). https://doi.org/10.1007/978-3-319-30526-4 10
5. Anai, K., Yokoyama, K.: Algorithms of Quantiﬁer Elimination and their Applica-
tion: Optimization by Symbolic and Algebraic Methods [in Japanese]. University
of Tokyo Press (2011)
6. Bacchus, F.: Representing and Reasoning with Probabilistic Knowledge: A Logical
Approach to Probabilities. The MIT Press, Cambridge (1990)
7. Basu, S., Pollack, R., Roy, M.F.: On the combinatorial and algebraic complexity of
quantiﬁer elimination. J. ACM 43(6), 1002–1045 (1996). https://doi.org/10.1145/
235809.235813
8. Boriˇci´c,
M.:
Inference
rules
for
probability
logic.
PUBLICATIONS
DE
L’INSTITUT MATH´EMATIQUE 100(114), 77–86 (2016). https://doi.org/10.
2298/PIM1614077B
9. Boriˇci´c, M.: Suppes-style sequent calculus for probability logic. J. Logic Comput.
27(4), 1157–1168 (10 2015). https://doi.org/10.1093/logcom/exv068
10. Brickhill, H., Horsten, L.: Triangulating non-archimedean probability. Rev. Sym-
bolic Logic 11(3), 519–546 (2018). https://doi.org/10.1017/S1755020318000060
11. Buss, S.R.: An introduction to proof theory. In: Buss, S.R. (ed.) Handbook of Proof
Theory, vol. 137, pp. 1–78. Elsevier, Amsterdam (1998)
12. Davey, B.A., Priestley, H.A.: Introduction to Lattices and Order, 2nd edn. Cam-
bridge University Press (2002). https://doi.org/10.1017/CBO9780511809088
13. Enderton, H.B.: A Mathematical Introduction to Logic, 2nd edn. Academic Press
(2000)

A Logic and Computation for Popper’s Conditional Probabilities
671
14. Fagin, R., Halpern, J.Y., Megiddo, N.: A logic for reasoning about probabilities.
Inf. Comput. 87(1), 78–128 (1990)
15. Gallier, J.H.: Logic for Computer Science: Foundations of Automatic Theorem
Proving, 2nd edn. Dover Publications (2015)
16. Gilio, A.: Precise propagation of upper and lower probability bounds in system p.
arXiv preprint math/0003046 (2000)
17. Hailperin, T.: Best possible inequalities for the probability of a logical function of
events. Am. Math. Monthly 72(4), 343–359 (1965)
18. H´ajek, A.: Probability, logic, and probability logic. In: Goble, L. (ed.) The Blackwell
Guide to Philosophical Logic, chap. 16, pp. 362–384. Blackwell Publishing (2001)
19. Halpern, J.Y.: Reasoning About Uncertainty, 2nd edn. MIT Press (2017)
20. Hobbs, J.R., Stickel, M.E., Appelt, D.E., Martin, P.: Interpretation as abduction.
Artif. Intell. 63(1), 69–142 (1993)
21. Ikodinovi´c, N., Ognjanovi´c, Z.: A logic with coherent conditional probabilities. In:
Godo, L. (ed.) ECSQARU 2005. LNCS (LNAI), vol. 3571, pp. 726–736. Springer,
Heidelberg (2005). https://doi.org/10.1007/11518655 61
22. Kersting, K., De Raedt, L.: Bayesian logic programs. arXiv preprint cs.AI/0111058
(2001). https://arxiv.org/abs/cs/0111058
23. Kersting, K., De Raedt, L.: Bayesian logic programming: theory and tool. In:
Getoor, L., Taskar, B. (eds.) Introduction to Statistical Relational Learning, pp.
291–321. MIT Press (2007)
24. McGee, V.: Learning the impossible. In: Eells, E., Brian, S. (eds.) Probability
and Conditionals: Belief Revision and Rational Decision, pp. 179–199. Cambridge
University Press (1994)
25. Mishra, B.: Algorithmic Algebra. Monographs in Computer Science. Springer, New
York (1993). https://doi.org/10.1007/978-1-4612-4344-1
26. Mueller, E.T.: Commonsense reasoning: An Event Calculus Based Approach, 2nd
edn. Morgan Kaufmann (2014)
27. Popper, K.: The Logic of Scientiﬁc Discovery, 2nd edn. Routledge (2002)
28. Richardson, M., Domingos, P.: Markov logic networks. Mach. Learn. 62(1), 107–
136 (2006). https://doi.org/10.1007/s10994-006-5833-1
29. Tabata, H.: On second-order Peano arithmetic [in Japanese]. Tottori University
Journal of the Faculty of Education and Regional Sciences 4(1), 37–84 (2002)
30. Tarski, A.: A Decision Method for Elementary Algebra and Geometry: Prepared
for Publication with the Assistance of J.C.C. McKinsey. RAND Corporation, Santa
Monica, CA (1951)
31. Wagner, C.G.: Modus tollens probabilized. Brit. J. Phil. Sci. 55(4), 747–753 (2004).
https://doi.org/10.1093/bjps/55.4.747

Interpreting Connexive Principles
in Coherence-Based Probability Logic
Niki Pfeifer1(B) and Giuseppe Sanﬁlippo2(B)
1 Department of Philosophy, University of Regensburg, Regensburg, Germany
niki.pfeifer@ur.de
2 Department of Mathematics and Computer Science, University of Palermo,
Palermo, Italy
giuseppe.sanfilippo@unipa.it
Abstract. We present probabilistic approaches to check the validity of
selected connexive principles within the setting of coherence. Connexive
logics emerged from the intuition that conditionals of the form If ∼A, then
A, should not hold, since the conditional’s antecedent ∼A contradicts its
consequent A. Our approach covers this intuition by observing that for
an event A the only coherent probability assessment on the conditional
event A| ¯A is p(A| ¯A) = 0. Moreover, connexive logics aim to capture the
intuition that conditionals should express some “connection” between the
antecedent and the consequent or, in terms of inferences, validity should
require some connection between the premise set and the conclusion. This
intuition is covered by a number of principles, a selection of which we
analyze in our contribution. We present two approaches to connexivity
within coherence-based probability logic. Speciﬁcally, we analyze connec-
tions between antecedents and consequents ﬁrstly, in terms of probabilis-
tic constraints on conditional events (in the sense of defaults, or negated
defaults) and secondly, in terms of constraints on compounds of condition-
als and iterated conditionals. After developing diﬀerent notions of nega-
tions and notions of validity, we analyze the following connexive princi-
ples within both approaches: Aristotle’s Theses, Aristotle’s Second The-
sis, Abelard’s First Principle and selected versions of Boethius’ Theses.
We conclude by remarking that coherence-based probability logic oﬀers a
rich language to investigate the validity of various connexive principles.
Keywords: Aristotle’s Theses · Coherence · Compounds of
conditionals · Conditional events · Conditional random quantities ·
Connexive logic · Iterated conditionals · Probabilistic constraints
Both authors contributed equally to the article and are listed alphabetically.
N. Pfeifer is supported by the BMBF project 01UL1906X.
G. Sanﬁlippo is a member of the GNAMPA Research Group and partially supported
by the INdAM–GNAMPA Project 2020 Grant U-UFMBAZ-2020-000819.
c
⃝Springer Nature Switzerland AG 2021
J. Vejnarov´a and N. Wilson (Eds.): ECSQARU 2021, LNAI 12897, pp. 672–687, 2021.
https://doi.org/10.1007/978-3-030-86772-0_48

Interpreting Connexive Principles in Coherence-Based Probability Logic
673
1
Introduction
We present probabilistic approaches to check the validity of selected connexive
principles within the setting of coherence. Connexive logics emerged from the
intuition that conditionals of the form if not-A, then A, denoted by ∼A →A,
should not hold, since the conditional’s antecedent not-A contradicts its con-
sequent A. Indeed, experimental psychological data show that people believe
that sentences of the form if not-A, then A are false (e.g., [39,40]), which sup-
ports the psychological plausibility of this intuition. Connexive principles were
developed to rule out such self-contradictory conditionals (for overviews, see
e.g.,[36,53]). Many of these principles can be traced back to antiquity or the
middle ages, which is reﬂected by the names of these principles, for example,
Aristotle’s Thesis or Abelard’s First Principle (see Table 1). In classical logic,
however, Aristotle’s Thesis, i.e. ∼(∼A →A), is not a theorem since the corre-
sponding material conditional is contingent because ∼(∼∼A ∨A) is logically
equivalent to ∼A (which is not necessarily true). Moreover, connexive logics
aim to capture the intuition that conditionals should express some “connection”
between the antecedent and the consequent or, in terms of inferences, validity
should require some connection between the premise set and the conclusion.
Table 1. Selected connexive principles (see also [53]).
Name
Abbreviation Connexive principle
Aristotle’s Thesis
(AT)
∼(∼A →A)
Aristotle’s Thesis′
(AT′)
∼(A →∼A)
Abelard’s First Principle
(AB)
∼((A →B) ∧(A →∼B))
Aristotle’s Second Thesis
(AS)
∼((A →B) ∧(∼A →B))
Boethius’ Thesis
(BT)
(A →B) →∼(A →∼B)
Boethius’ Thesis′
(BT′)
(A →∼B) →∼(A →B)
Reversed Boethius’ Thesis
(RBT)
∼(A →∼B) →(A →B)
Reversed Boethius’ Thesis′ (RBT′)
∼(A →B) →(A →∼B)
Boethius Variation 3
(B3)
(A →B) →∼(∼A →B)
Boethius Variation 4
(B4)
(∼A →B) →∼(A →B)
The connexive intuition that conditionals of the form if not-A, then A should
not hold is covered in subjective probability theory. Speciﬁcally, we cover this
intuition by the observation that for any event A, with A ̸= ∅, the only coherent
assessment on the conditional event A|A is p(A|A) = 0.
The aim of our contribution is to investigate selected connexive principles
within the framework of coherence-based probability logic. The coherence app-
roach to (subjective) probability was originated by Bruno de Finetti (see, e.g.,
[11,12]) and has been generalised to the conditional probability and to previsions

674
N. Pfeifer and G. Sanﬁlippo
of conditional random quantities (see, e.g., [2,3,6,9,21,24,29,33,49,52]). In the
present framework, we present two approaches to connexivity within coherence-
based probability logic. In the ﬁrst approach we analyze connections between
antecedents and consequents in terms of probabilistic constraints on conditional
events (in the sense of defaults or negated defaults [16,44–46]). In the second
approach, based the recently developed more general framework of compounds
of conditionals and iterated conditionals [18,19,22,24,27], we deﬁne these con-
nections in terms of constraints on suitable conditional random quantities. After
developing diﬀerent notions of negations and notions of validity, we analyze the
connexive principles given in Table 1 within both approaches.
The coherence principle plays a key role in probabilistic reasoning and allows
for probabilistic inferences of a further conditional event (the conclusion) from
any coherent probabilistic assessment on an arbitrary family of conditional events
(the premises). Moreover, coherence is a more general approach to conditional
probabilities compared to approaches which requires positive probability for the
conditioning events. In standard approaches to probability the conditional prob-
ability p(C|A) is deﬁned by the ratio p(A ∧C)/p(A), which requires positive
probability of the conditioning event, p(A) > 0. However, in the framework of
coherence, conditional probability p(C|A), as a degree of belief, is a primitive
notion and it is properly deﬁned even if the conditioning event has probabil-
ity zero, i.e., p(A) = 0. Analogously, within coherence, previsions of conditional
random quantities, are primitive and properly deﬁned even if the conditioning
event has probability zero. Therefore, coherence is a more general approach to
conditional probabilities compared to approaches which requires positive proba-
bility for the conditioning events. The only requirement is that the conditioning
event must be logically possible. Thus, although p(C|A) is well deﬁned even if
p(A) = 0, it is undeﬁned if A ≡∅(where ∅denotes a logical contradiction). This
is in line with the reading that Boethius and Aristotle thought that principles
like (BT) and (AT), respectively, hold only when the conditional’s antecedent is
possible (see [34] who argues that the “ancient logicians most likely meant their
theses as applicable only to ‘normal’ conditionals with antecedents which are
not self-contradictory”; p. 16). It is also in line with the Ramsey test, which is
expressed in his famous footnote: “If two people are arguing ‘If A will C?’ and
are both in doubt as to A, they are adding A hypothetically to their stock of
knowledge and arguing on that basis about C; so that in a sense ‘If A, C’ and
‘If A, C’ are contradictories. We can say they are ﬁxing their degrees of belief
in C given A. If A turns out false, these degrees of belief are rendered void” [48,
p. 155, we adjusted the notation]. The quantitative interpretation of the Ram-
sey test became a cornerstone of the conditional probability interpretation of
conditionals. Adding a contradiction to your stock of knowledge does not make
sense (as, traditionally, knowledge implies truth). Moreover, Ramsey’s thought
that conditionals with contradicting consequents C and C contradict each other
coincides with the underlying intuition of (AB).

Interpreting Connexive Principles in Coherence-Based Probability Logic
675
2
Preliminary Notions and Results
Given two events A and H, with H ̸= ∅(where ∅denotes the impossible event),
the conditional event A|H (read: A given H) is deﬁned as a three-valued logical
entity which is true if AH (i.e., A ∧H) is true, false if AH is true, and void if
H is false. We observe that A|H assumes the logical value (true or false) of A,
when H is true, and it is void, otherwise. There is a long history of how to deal
with negations (see, e.g., [30]). In our context, the negation of the conditional
event “A given H”, denoted by A|H, is the conditional event A|H, that is “the
negation of A” given H. We use the inner negation to preserve for conditional
events the usual property of negating unconditional events: p(A) = 1 −p(A). In
the subjective approach to probability based on the betting scheme, a conditional
probability assessment p(A|H) = x means that, for every real number s, you are
willing to pay an amount s · x and to receive s, or 0, or s · x (money back),
according to whether AH is true, or ¯AH is true, or ¯H is true (bet called oﬀ),
respectively. The random gain, which is the diﬀerence between the (random)
amount that you receive and the amount that you pay, is G = (sAH + 0 ¯AH +
sx ¯H) −sx = sAH + sx(1 −H) −sx = sH(A −x).
Given a probability function p deﬁned on an arbitrary family K of conditional
events, consider a ﬁnite subfamily F = {A1|H1, . . . , An|Hn} ⊆K and the vector
P = (x1, . . . , xn), where xi = p(Ai|Hi) is the assessed probability for the condi-
tional event Ai|Hi, i = 1, . . . , n. With the pair (F, P) we associate the random
gain G = n
i=1 siHi(Ai −xi). We denote by GHn the set of values of G restricted
to Hn = H1 ∨· · · ∨Hn, i.e., the set of values of G when Hn is true. Then, we
recall below the notion of coherence in the context of the betting scheme.
Deﬁnition 1. The function p deﬁned on K is coherent if and only if, ∀n ≥1,
∀s1, . . . , sn, ∀F = {A1|H1, . . . , An|Hn} ⊆K, it holds that: min GHn
≤0 ≤
max GHn.
In betting terms, the coherence of conditional probability assessments means
that in any ﬁnite combination of n bets, after discarding the case where all the
bets are called oﬀ, the values of the random gain are neither all positive nor all
negative (i.e., no Dutch Book). In particular, coherence of x = p(A|H) is deﬁned
by the condition min GH ≤0 ≤max GH, ∀s, where GH is the set of values of G
restricted to H (that is when the bet is not called oﬀ). Depending on the logical
relations between A and H (with H ̸= ∅), the set Π of all coherent conditional
probability assessments x = p(A|H) is:
Π =
⎧
⎨
⎩
[0, 1], if ∅̸= AH ̸= H,
{0}, if AH = ∅,
{1}, if AH = H.
(1)
In numerical terms, once x = p(A|H) is assessed by the betting scheme, the
indicator of A|H, denoted by the same symbol, is deﬁned as 1, or 0, or x,
according to whether AH is true, or ¯AH is true, or ¯H is true. Then, by setting
p(A|H) = x,

676
N. Pfeifer and G. Sanﬁlippo
A|H = AH + xH =
⎧
⎨
⎩
1, if AH is true,
0, if AH is true,
x, if H is true.
(2)
Note that since the three-valued numerical entity A|H is deﬁned by the betting
scheme once the value x = p(A|H) is assessed, the deﬁnition of (the indicator
of) A|H is not circular. The third value of the random quantity A|H (subjec-
tively) depends on the assessed probability p(A|H) = x. Moreover, the value
x coincides with the corresponding conditional prevision, denoted by P(A|H),
because P(A|H) = P(AH + xH) = p(AH) + xp(H) = p(A|H)p(H) + xp(H) =
xp(H) + xp(H) = x.
In the special case where AH = H, it follows by (1) that x = 1 is the only
coherent assessment for p(A|H); then, for the indicator A|H it holds that
A|H = AH + xH = H + H = 1,
if AH = H.
(3)
In particular (3) holds when A = Ω (i.e., the sure event), since Ω ∧H = H and
hence Ω|H = H|H = 1. Likewise, if AH = ∅, it follows by (1) that x = 0 is the
only coherent assessment for p(A|H); then,
A|H = 0 + 0H = 0,
if AH = ∅.
(4)
In particular (4) holds when A = ∅, since ∅∧H = ∅and hence ∅|H = 0. We
observe that conditionally on H be true, for the (indicator of the) negation it
holds that A|H = A = 1 −A = 1 −A|H. Conditionally on H be false, by
coherence, it holds that A|H = p(A|H) = 1 −p(A|H) = 1 −A|H. Thus, in all
cases it holds that
A|H = A|H = (1 −A)|H = 1 −A|H.
(5)
We denote by X a random quantity, with a ﬁnite set of possible values. Given
any event H ̸= ∅, agreeing to the betting metaphor, if you assess the prevision
P(X|H) = μ means that for any given real number s you are willing to pay an
amount sμ and to receive sX, or sμ, according to whether H is true, or false
(bet called oﬀ), respectively. In particular, when X is (the indicator of) an event
A, then P(X|H) = P(A|H). The notion of coherence can be generalized to the
case of prevision assessments on a family of conditional random quantities (see,
e.g., [25,51]). Given a random quantity X and an event H ̸= ∅, with prevision
P(X|H) = μ, likewise formula (2) for the indicator of a conditional event, an
extended notion of a conditional random quantity, denoted by the same symbol
X|H, is deﬁned as follows X|H = XH + μH. We recall now the notion of con-
junction of two (or more) conditional events within the framework of conditional
random quantities in the setting of coherence ([19,22,24,26], for alternative
approaches see also, e.g., [31,37]). Given a coherent probability assessment (x, y)
on {A|H, B|K}, we consider the random quantity AHBK + xHBK + yKAH
and we set P[(AHBK + xHBK + yKAH)|(H ∨K)] = z. Then we deﬁne the
conjunction (A|H) ∧(B|K) as follows:

Interpreting Connexive Principles in Coherence-Based Probability Logic
677
Deﬁnition 2. Given a coherent prevision assessment p(A|H) = x, p(B|K) = y,
and P[(AHBK+xHBK+yKAH)|(H∨K)] = z, the conjunction (A|H)∧(B|K)
is the conditional random quantity deﬁned as
(A|H) ∧(B|K) = (AHBK + xHBK + yKAH)|(H ∨K)
=
⎧
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎩
1, if AHBK is true,
0, if AH ∨BK is true,
x, if HBK is true,
y, if AHK is true,
z, if H K is true.
(6)
Of course, P[(A|H)∧(B|K)] = z. Coherence requires that the Fr´echet-Hoeﬀding
bounds for prevision of the conjunction are preserved [22], i.e., max{x+y−1, 0} ≤
z ≤min{x, y}, like in the case of unconditional events. Other preserved prop-
erties are listed in [27]. We notice that if conjunctions of conditional events are
deﬁned as suitable conditional events (see, e.g., [1,5,7,8,28]), classical proba-
bilistic properties are not preserved. In particular, the lower and upper prob-
ability bounds for the conjunction do not coincide with the above mentioned
Fr´echet-Hoeﬀding bounds [50]. Here, diﬀerently from conditional events which
are three-valued objects, the conjunction (A|H) ∧(B|K) is not any longer a
three-valued object, but a ﬁve-valued object with values in [0, 1]. We observe
that (A|H) ∧(A|H) = A|H and (A|H) ∧(B|K) = (B|K) ∧(A|H). Moreover, if
H = K, then (A|H) ∧(B|H) = AB|H.
For comparison with other approaches, like [13], see [25, Section 9].
In analogy to formula (2), where the indicator of a conditional event “A given
H” is deﬁned as A|H = A ∧H + p(A|H)H, the iterated conditional “B|K given
A|H” is deﬁned as follows (see, e.g., [18,19,22]):
Deﬁnition 3 (Iterated conditioning). Given any pair of conditional events
A|H and B|K, with AH ̸= ∅, the iterated conditional (B|K)|(A|H) is deﬁned as
the conditional random quantity (B|K)|(A|H) = (A|H) ∧(B|K) + μA|H, where
μ = P[(B|K)|(A|H)].
Notice that we assume AH ̸= ∅to avoid trivial cases of iterated condition-
als. Speciﬁcally, similar as in the three-valued notion of a conditional event
A|H where the antecedent H must not be impossible (i.e., H must not coin-
cide with the constant 0), the iterated conditional (B|K)|(A|H) requires that
the antecedent A|H must not be constant and equal to 0 (this happens when
AH ̸= ∅). Furthermore, we recall that the compound prevision theorem is pre-
served, that is P[(A|H) ∧(B|K)] = P[(B|K)|(A|H)]p(A|H).
3
Approach 1: Connexive Principles and Default
Reasoning
In order to validate the connexive principles we interpret a conditional A →C
by the default A |∼C, where A and C are two events (with A ̸= ∅). A default

678
N. Pfeifer and G. Sanﬁlippo
A |∼C can be read as C is a plausible consequence of A and is interpreted by the
probability constraint p(C|A) = 1 [16].1 The conditional A →∼C is interpreted
by the default A |∼C. Likewise, ∼A →C is interpreted by A |∼C. A negated
conditional ∼(A →C) is interpreted by the negated default ∼(A |∼C) (it is
not the case that: C is a plausible consequence of A; also denoted by A |∼/ C
in [14,32]) that is p(C|A) ̸= 1, which corresponds to the wide scope negation of
negating conditionals [16].
The conjunction of two conditionals, denoted by (A →B) ∧(C →D),
is interpreted by the sequence of their associated defaults (A |∼B, C |∼D),
which represents in probabilistic terms the constraint (p(B|A) = 1, p(D|C) =
1), that is (p(B|A), p(D|C)) = (1, 1). Then, the negation of the conjunction
of two conditionals, denoted by ∼((A →B) ∧(C →D)) (i.e., in terms of
defaults ∼(A |∼B)∧(C |∼D)) is interpreted by the negation of the probabilistic
constraint (p(B|A), p(D|C)) = (1, 1), that is (p(B|A), p(D|C)) ̸= (1, 1). Table 2
summarizes the interpretations.
We now introduce the deﬁnition of validity for non-iterated connexive prin-
ciples (e.g., (AT), (AT′), (AB)).
Deﬁnition 4. We say that a non-iterated connexive principle is valid if and only
if the probabilistic constraint associated with the connexive principle is satisﬁed
by every coherent assessment on the involved conditional events.
In the next paragraphs we check the validity in terms of Deﬁnition 4 of the
non-iterated connexive principles in Table 1.
Aristotle’s Thesis (AT): ∼(∼A →A). We assume that A ̸= ∅and we interpret
the principle ∼(∼A →A) by the negated default ∼(A |∼A) with the following
associated probabilistic constraint: p(A|A) ̸= 1. We observe that p(A|A) = 0
is the unique precise coherent assessment on A|A. Then, (AT) is valid because
every coherent precise assessment p(A|A) is such that p(A|A) ̸= 1.
Aristotle’s Thesis ′ (AT ′): ∼(A →∼A). Like (AT), (AT)′ can be validated.
Abelard’s Thesis (AB): ∼((A →B) ∧(A →∼B)). We assume that A ̸= ∅.
The structure of this principle is formalized by ∼((A |∼B) ∧(A |∼B)) which
expresses the constraint (p(B|A), p(B|A)) ̸= (1, 1). We recall that coherence
requires p(B|A)+p(B|A) = 1. Then, (AT) is valid because each coherent assess-
ment on (B|A, B|A) is necessarily of the form (x, 1 −x), with x ∈[0, 1], which
of course satisﬁes (p(B|A), p(B|A)) ̸= (1, 1).
1
According to ε-semantics (see, e.g., [1,38]) a default A |∼C is interpreted by
p(C|A) ≥1 −ε, with ε > 0 and p(A) > 0. Gilio introduced a coherence-based
probability semantics for defaults by also allowing ε and p(A) to be zero [15]. In this
context, defaults in terms of probability 1 can be used to give a alternative deﬁnition
of p-entailment which preserve the usual non-monotonic inference rules like those
of System P [4,15,20,21], see also [9,10]. For the psychological plausibility of the
coherence-based semantics of non-monotonic reasoning, see, e.g., [41–43,47].

Interpreting Connexive Principles in Coherence-Based Probability Logic
679
Aristotle’s Second Thesis (AS): ∼((A →B) ∧(∼A →B)). We assume that
A ̸= ∅and A ̸= ∅. The structure of this principle is formalized by ∼((A |∼
B) ∧(A |∼B)) which expresses the constraint (p(B|A), p(B|A)) ̸= (1, 1). We
recall that, given two logically independent events A and B every assessment
(x, y) ∈[0, 1]2 on (B|A, B|A) is coherent. In particular, (p(B|A), p(B|A)) =
(1, 1) is a coherent assessment which does not satisfy the probabilistic constraint
(p(B|A), p(B|A)) ̸= (1, 1). Thus, (AS) is not valid.
Concerning iterated connexive principles (e.g. (BT), (BT′)), we interpret the
main connective (→) as the implication (⇒) from the probabilistic constraint
on the antecedent to the probabilistic constraint on the conclusion. Then, for
instance, the iterated conditional (A →B) →(C →D) is interpreted by the
implication A |∼B ⇒C |∼D, that is p(B|A) = 1 ⇒p(D|C) = 1. We now deﬁne
validity for iterated connexive principles.
Deﬁnition 5. An iterated connexive principle ⃝⇒□is valid if and only if
the probabilistic constraint of the conclusion □is satisﬁed by every coherent
extension to the conclusion from any coherent probability assessment satisfying
the constraint of the premise ⃝.
Table 2. Probabilistic interpretations of logical operation on conditionals in terms of
defaults or negated defaults.
Conditional object
Default
Probabilistic interpretation
A →C
A |∼C
p(C|A) = 1
∼(A →C)
∼(A |∼C)
p(C|A) ̸= 1
(A →B) ∧(C →D)
(A |∼B, C |∼D)
(p(B|A), p(D|C)) = (1, 1)
∼((A →B) ∧(C →D)) ∼(A |∼B, C |∼D) (p(B|A), p(D|C)) ̸= (1, 1)
(A →B) →(C →D)
A |∼B ⇒C |∼D
p(B|A) = 1 ⇒p(D|C) = 1
We check the validity in terms of Deﬁnition 5 of the iterated connexive prin-
ciples in Table 1.
Boethius’ Thesis (BT): (A →B) →∼(A →∼B). We assume that A ̸= ∅. This
is interpreted by the implication A |∼B ⇒∼(A |∼B), that is p(B|A) = 1 ⇒
p(B|A) ̸= 1. We observe that, by setting p(B|A) = 1, p(B|A) = 1−p(B|A) = 0 is
the unique coherent extension to B|A. Then, as p(B|A) = 1 ⇒p(B|A) = 0 ̸= 1,
the iterated connexive principle (BT) is valid.
Boethius’ Thesis ′ (BT ′): (A →∼B) →∼(A →B). Like (BT), it can be shown
that (BT′) is valid too.
Reversed Boethius’ Thesis (RBT): ∼(A →∼B) →(A →B). We assume that
A ̸= ∅. This is interpreted by ∼(A |∼B) ⇒A |∼B, that is p(B|A) ̸= 1 ⇒
p(B|A) = 1. We observe that, by setting p(B|A) = x it holds that p(B|A) = 1−x
is the unique coherent extension to B|A. In particular by choosing x ∈]0, 1[, it
holds that p(B|A) ̸= 1 and p(B|A) ̸= 1. Thus, p(B|A) ̸= 1 ⇏p(B|A) = 1 and
hence (RBT) is not valid.

680
N. Pfeifer and G. Sanﬁlippo
Reversed Boethius’ Thesis ′ (RBT ′): ∼(A →B) →(A →∼B). Like (RBT), it
can be shown that (RBT′) is not valid too.
Boethius Variation (B3): (A →B) →∼(∼A →B). We assume that A ̸= ∅
and A ̸= ∅. This is interpreted by A |∼B ⇒∼(A |∼B), that is p(B|A) = 1 ⇒
p(B|A) ̸= 1. We observe that, by setting p(B|A) = 1, any value p(B|A) ∈[0, 1]
is a coherent extension to B|A, because the assessment (1, y) on (B|A, B|A) is
coherent for every y ∈[0, 1]. In particular the assessment (1, 1) on (B|A, B|A) is
coherent. Therefore, as p(B|A) = 1 ⇏p(B|A) ̸= 1, (B3) is not valid.
Boethius Variation (B4): (∼A →B) →∼(A →B). Like (B3), it can be shown
that (B4) is not valid too.
We summarize the results of this section in Table 3.
Table 3. Connexive principles in the framework of defaults and probabilistic con-
straints (Approach 1).
Name
Connexive principle
Default
Probabilistic constraint
Validity
(AT)
∼(∼A →A)
∼(A |∼A)
p(A|A) ̸= 1
yes
(AT’)
∼(A →∼A)
∼(A |∼A)
p(A|A) ̸= 1
yes
(AB)
∼((A →B) ∧(A →∼B)) ∼(A |∼B, A |∼B)
(p(B|A), p(B|A)) ̸= (1, 1)
yes
(AS)
∼((A →B) ∧(∼A →B)) ∼(A |∼B, A |∼B)
(p(B|A), p(B|A)) ̸= (1, 1)
no
(BT)
(A →B) →∼(A →∼B)
A |∼B ⇒∼(A |∼B)
p(B|A) = 1 ⇒p(B|A) ̸= 1 yes
(BT’)
(A →∼B) →∼(A →B)
A |∼B ⇒∼(A |∼B)
p(B|A) = 1 ⇒p(B|A) ̸= 1 yes
(RBT)
∼(A →∼B) →(A →B)
∼(A |∼B) ⇒A |∼B
p(B|A) ̸= 1 ⇒p(B|A) = 1 no
(RBT’) ∼(A →B) →(A →∼B)
∼(A |∼B) ⇒A |∼B
p(B|A) ̸= 1 ⇒p(B|A) = 1 no
(B3)
(A →B) →∼(∼A →B)
A |∼B ⇒∼(A |∼B)
p(B|A) = 1 ⇒p(B|A) ̸= 1 no
(B4)
(∼A →B) →∼(A →B) A |∼B ⇒∼(A |∼B)
p(B|A) = 1 ⇒p(B|A) ̸= 1 no
4
Approach 2: Connexive Principles and Compounds
of Conditionals
In this section we analyze connexive principles within the theory of logical oper-
ations among conditional events. Speciﬁcally, we analyze connections between
antecedents and consequents in terms of constraints on compounds of condi-
tionals and iterated conditionals. In this second approach, a basic conditional
A →C is interpreted as a conditional event C|A (instead of a probabilistic con-
straint on conditional events) which is a three-valued object: C|A ∈{1, 0, x},
where x = p(C|A). The negation ∼(A →C) is interpreted by C|A (which
is the narrow scope negation of negating conditionals). Then, ∼(A →∼C)
amounts to C|A which coincides with C|A. We recall that logical operations
among conditional events do not yield a conditional event, rather they yield
conditional random quantities with more than three possible values (see, e.g.,

Interpreting Connexive Principles in Coherence-Based Probability Logic
681
[22]). Then, we interpret the results of the logical operations in the connex-
ive principles by suitable conditional random quantities. In particular, the con-
junction (A →B) ∧(C →D) (resp., ∼((A →B) ∧(C →D))) is inter-
preted by (B|A)∧(D|C) (resp., by (B|A) ∧(D|C)), and the iterated conditional
(A →B) →(C →D) is interpreted by (D|C)|(B|A). Moreover, we deﬁne
validity of connexive principles within Approach 2.
Deﬁnition 6. A connexive principle is valid if and only if the associated con-
ditional random quantity is constant and equal to 1.
We now check the validity of the connexive principles in Table 1 according to
Deﬁnition 6.
Aristotle’s Thesis (AT): ∼(∼A →A). We interpret the principle ∼(∼A →A)
by the negation of the conditional event A|A, that is by A|A, where A ̸= ∅.
Then, based on Eqs. (5) and (3), it follows that A|A = 1 −A|A = A|A = 1.
Therefore, (AT) is valid because the conditional random quantity A|A, which
also coincides with the conditional event A|A, is constant and equal to 1.
Aristotle’s Thesis ′ (AT ′): ∼(A →∼A). We interpret the principle ∼(A →∼A)
by the negation of the conditional event A|A, that is by A|A, where A ̸= ∅. Like
in (AT), it holds that A|A = 1 −A|A = A|A = 1, which validates (AT ′). Notice
that, (AT ′) also follows from (AT) when A is replaced by A (of course A = A).
Abelard’s Thesis (AB): ∼((A →B)∧(A →∼B)). The structure of this principle
is formalized by the conditional random quantity (B|A) ∧(B|A), where A ̸= ∅.
We observe that (B|A) ∧(B|A) = (B ∧B)|A = ∅|A. Then, (B|A) ∧(B|A) =
∅|A = ∅|A = Ω|A = 1. Therefore, (AB) is valid.
Aristotle’s Second Thesis (AS): ∼((A →B)∧(∼A →B)). The structure of this
principle is formalized by the random quantity (B|A) ∧(B|A), where A ̸= ∅and
A ̸= ∅. By setting p(B|A) = x and p(B|A) = y, it follows that [19,23]
(B|A) ∧(B|A) = (B|A) · (B|A) =
⎧
⎨
⎩
0, if AB ∨AB is true,
y, if AB is true,
x, if AB is true.
Then, (B|A) ∧(B|A) =1 −(B|A) ∧(B|A) = 1 −(yAB + xAB), which is not
constant and can therefore not necessarily be equal to 1. In particular, by
choosing the coherent assessment x = y = 1, it follows that (B|A) ∧(B|A) =
1 −AB −AB = 1 −B = B, which is not necessarily equal to 1 as it could
be either 1 or 0, according to whether B is true or false, respectively. There-
fore, (AS) is not valid. Moreover, by setting P[(B|A) ∧(B|A)] = μ, it holds that
μ = y p(AB)+x p(AB) =y p(B|A)p(A)+x p(B|A)p(A) =xy p(A)+xy p(A) = xy.
Then, P[(B|A) ∧(B|A)] = 1−xy. We also observe that, in the special case where
x = y = 0, it follows that (B|A) ∧(B|A) = 1.

682
N. Pfeifer and G. Sanﬁlippo
Boethius’ Thesis (BT): (A →B) →∼(A →∼B). This principle is formalized
by the iterated conditional (B|A)|(B|A), with AB ̸= ∅. We recall that (B|A) =
B|A. Then (B|A)|(B|A) = (B|A)|(B|A). Moreover, by setting p(B|A) = x and
P[(B|A)|(B|A)] = μ, it holds that
(B|A)|(B|A) = (B|A) ∧(B|A) + μ(1 −B|A) = (B|A) + μ(1 −B|A) =
=
⎧
⎨
⎩
1,
if AB is true,
μ,
if AB, is true,
x + μ(1 −x), if A is true.
By the linearity of prevision it holds that μ = x + μ(1 −x). Then,
(B|A)|(B|A) = (B|A) + μ(1 −B|A) =
1, if AB is true,
μ, if A ∨B is true.
Then, by coherence it must be that μ = 1 and hence ([22, Remark 2], see also
[27, Section 3.2])
(B|A)|(B|A) = 1.
(7)
Therefore (B|A)|(B|A) is constant and equal to 1 and hence (BT) is valid.
Boethius’ Thesis ′ (BT ′): (A →∼B) →∼(A →B). This principle is formal-
ized by the iterated conditional (B|A)|(B|A), where AB ̸= ∅. By observing that
B|A = B|A, it follows that (B|A)|(B|A) = (B|A)|(B|A) which is constant and
equal to 1 because of (7). Therefore, (BT′) is valid.
Reversed Boethius’ Thesis (RBT): ∼(A →∼B) →(A →B). This princi-
ple is formalized by the iterated conditional (B|A)|(B|A), where AB ̸= ∅. As
(B|A) = B|A, it follows from (7) that (B|A)|(B|A) = (B|A)|(B|A) = 1. There-
fore, (RBT) is valid.
Reversed Boethius’ Thesis
′ (RBT ′): ∼(A →B) →(A →∼B). This princi-
ple is formalized by the iterated conditional (B|A)|(B|A), where AB ̸= ∅. As
(B|A) = B|A, it follows from (7) that (B|A)|(B|A) = (B|A)|(B|A) = 1. There-
fore (RBT′) is validated.
Boethius Variation (B3): (A →B) →∼(∼A →B). This principle is formal-
ized by the iterated conditional (B|A)|(B|A), where AB ̸= ∅. We observe that
(B|A)|(B|A) = (B|A)|(B|A), because B|A = B|A. By setting p(B|A) = x,
p(B|A) = y, and P[(B|A)|(B|A)] = μ, it holds that
(B|A)|(B|A) = (B|A) ∧(B|A) + μ(1 −B|A) =
⎧
⎪
⎪
⎨
⎪
⎪
⎩
y,
if AB is true,
μ,
if AB is true,
μ(1 −x),
if AB is true,
x + μ(1 −x), if AB is true,
which is not constant and can therefore not necessarily be equal to 1. For exam-
ple, if we choose the coherent assessment x = y = 1, it follows that
(B|A)|(B|A) = (B|A) ∧(B|A) + μ(1 −B|A) =
⎧
⎪
⎪
⎨
⎪
⎪
⎩
1, if AB is true,
μ, if AB is true,
0, if AB is true,
1, if AB is true,

Interpreting Connexive Principles in Coherence-Based Probability Logic
683
which is not constant and equal to 1. Therefore, (B3) is not valid.
Boethius Variation (B4): (∼A →B) →∼(A →B). This principle is for-
malized by the iterated conditional (B|A)|(B|A), where AB ̸= ∅. We observe
that (B|A)|(B|A) is not constant and not necessarily equal to 1 because it is
equivalent to (B3) when A is replaced by A. Therefore, (B4) is not valid.
Connexive principles and their interpretation in terms of compounds of con-
ditionals or iterated conditionals are illustrated in Table 4.
Table 4. Connexive principles in the framework of compounds of conditionals and
iterated conditionals (Approach 2). Value denotes whether the conditional random
quantity is constant and equal to 1.
Name
Connexive principle
Interpretation
Value Validity
(AT)
∼(∼A →A)
A|A
= 1
yes
(AT′)
∼(A →∼A)
A|A
= 1
yes
(AB)
∼((A →B) ∧(A →∼B)) (B|A) ∧(B|A) = 1
yes
(AS)
∼((A →B) ∧(∼A →B)) (B|A) ∧(B|A) ̸= 1
no
(BT)
(A →B) →∼(A →∼B)
(B|A)|(B|A)
= 1
yes
(BT′)
(A →∼B) →∼(A →B)
(B|A)|(B|A)
= 1
yes
(RBT)
∼(A →∼B) →(A →B)
(B|A)|(B|A)
= 1
yes
(RBT′) ∼(A →B) →(A →∼B)
(B|A)|(B|A)
= 1
yes
(B3)
(A →B) →∼(∼A →B)
(B|A)|(B|A)
̸= 1
no
(B4)
(∼A →B) →∼(A →B) (B|A)|(B|A)
̸= 1
no
5
Concluding Remarks
We presented two approaches to investigate connexive principles. Connexivity
is interpreted in terms of probabilistic constraints on conditional events (in the
sense of defaults, or negated defaults) in Approach 1. Within this approach we
showed that the connexive principles (AT), (AT′), (AB), (BT), and (BT′) are
valid, whereas (AS), (RBT), (RBT′), (B3), and (B4) are not valid (see Table 3).
In Approach 2 connexivity is interpreted by constraints on compounds of condi-
tionals and iterated conditionals. Here, we demonstrated that, like in Approach 1,
(AT), (AT′), (AB), (BT), and (BT′) are valid, whereas (AS), (B3), and (B4) are
not valid. Contrary to Approach 1, (RBT) and (RBT′) are valid in Approach 2
(see Table 4).
Approach 1 is characterized by employing concepts from coherence-based
probability theory and probabilistic interpretations of defaults and negated
defaults. Conditionals, interpreted as defaults, are negated by the wide scope
negation. We gave two notions of validity, namely for non-iterated and iter-
ated connexive principles, respectively. Approach 2 allows for dealing with log-
ical operations on conditional events and avoids (see, e.g., [51]) the well known

684
N. Pfeifer and G. Sanﬁlippo
Lewis’ triviality results (see, e.g., [35]). It therefore oﬀers a more uniﬁed app-
roach to connexive principles, which is reﬂected by a unique deﬁnition of validity
for both, iterated and non-iterated connexive principles. Moreover, Approach 2
negates conditionals by the narrow scope negation. Thus, validity depends on
how conditionals and negation are deﬁned.
One might wonder why neither of the two approaches validates all connexive
principles. Of course, we have shown by proofs why, for instance, (AS) is not
valid in both approaches. Apart from the insight obtained from our proofs, this
is not surprising since also not all connexive principles are valid in all systems
of connexive logic. Moreover, some rules which are valid in classical logic (e.g.,
transitivity, contraposition, and premise strengthening) are not valid in proba-
bility logic, while, for example, the rules of the basic nonmonotonic System P
are valid within coherence-based probability logic [9,15–17,24].
We have shown that coherence-based probability logic oﬀers a rich language
to investigate the validity of various connexive principles. Future work will be
devoted to investigations on other intuitively plausible logical principles con-
tained in alternative and non-classical logics.
Acknowledgment. Thanks to four anonymous reviewers for useful comments.
References
1. Adams, E.W.: The logic of conditionals. An application of probability to deduction.
Reidel, Dordrecht (1975)
2. Berti, P., Regazzini, E., Rigo, P.: Well calibrated, coherent forecasting sys-
tems. Theory Probability Appl. 42(1), 82–102 (1998). https://doi.org/10.1137/
S0040585X97975988
3. Biazzo, V., Gilio, A.: A generalization of the fundamental theorem of de Finetti
for imprecise conditional probability assessments. Int. J. Approximate Reasoning
24(2–3), 251–272 (2000)
4. Biazzo, V., Gilio, A., Lukasiewicz, T., Sanﬁlippo, G.: Probabilistic logic under
coherence, model-theoretic probabilistic logic, and default reasoning in System P.
J. Appl. Non-Classical Logics 12(2), 189–213 (2002)
5. Calabrese, P.: Logic and Conditional Probability: A Synthesis. College Publications
(2017)
6. Capotorti, A., Lad, F., Sanﬁlippo, G.: Reassessing accuracy rates of median deci-
sions. Am. Stat. 61(2), 132–138 (2007)
7. Ciucci, D., Dubois, D.: Relationships between connectives in three-valued logics.
In: Greco, S., Bouchon-Meunier, B., Coletti, G., Fedrizzi, M., Matarazzo, B., Yager,
R.R. (eds.) IPMU 2012. CCIS, vol. 297, pp. 633–642. Springer, Heidelberg (2012).
https://doi.org/10.1007/978-3-642-31709-5 64
8. Ciucci, D., Dubois, D.: A map of dependencies among three-valued logics. Inf. Sci.
250, 162–177 (2013). https://doi.org/10.1016/j.ins.2013.06.040
9. Coletti, G., Scozzafava, R.: Probabilistic Logic in a Coherent Setting. Kluwer,
Dordrecht (2002)
10. Coletti, G., Scozzafava, R., Vantaggi, B.: Possibilistic and probabilistic logic under
coherence: Default reasoning and System P. Math. Slovaca 65(4), 863–890 (2015)

Interpreting Connexive Principles in Coherence-Based Probability Logic
685
11. de Finetti, B.: Sul signiﬁcato soggettivo della probabilit´a. Fundam. Math. 17, 298–
329 (1931)
12. de Finetti, B.: Theory of Probability, vol. 1, 2. Wiley, Chichester (1970/1974)
13. Flaminio, T., Godo, L., Hosni, H.: Boolean algebras of conditionals, probability
and logic. Artif. Intell. 286, 103347 (2020). https://doi.org/10.1016/j.artint.2020.
103347
14. Freund, M., Lehmann, D., Morris, P.: Rationality, transitivity, and contraposition.
Artif. Intell. 52(2), 191–203 (1991)
15. Gilio, A.: Probabilistic reasoning under coherence in System P. Ann. Math. Artif.
Intell. 34, 5–34 (2002)
16. Gilio, A., Pfeifer, N., Sanﬁlippo, G.: Transitivity in coherence-based probability
logic. J. Appl. Log. 14, 46–64 (2016). https://doi.org/10.1016/j.jal.2015.09.012
17. Gilio, A., Pfeifer, N., Sanﬁlippo, G.: Probabilistic entailment and iterated condi-
tionals. In: Elqayam, S., Douven, I., Evans, J.S.B.T., Cruz, N. (eds.) Logic and
Uncertainty in the Human Mind: A Tribute to David E. Over, pp. 71–101. Rout-
ledge, Oxon (2020). https://doi.org/10.4324/9781315111902-6
18. Gilio, A., Sanﬁlippo, G.: Conditional random quantities and iterated conditioning
in the setting of coherence. In: van der Gaag, L.C. (ed.) ECSQARU 2013. LNCS
(LNAI), vol. 7958, pp. 218–229. Springer, Heidelberg (2013). https://doi.org/10.
1007/978-3-642-39091-3 19
19. Gilio, A., Sanﬁlippo, G.: Conjunction, disjunction and iterated conditioning of
conditional events. In: Kruse, R., Berthold, M.R., Moewes, C., Gil, M.A., Grze-
gorzewski, P., Hryniewicz, O. (eds.) Synergies of Soft Computing and Statistics
for Intelligent Data Analysis, AISC, vol. 190, pp. 399–407. Springer, Heidelberg
(2013). https://doi.org/10.1007/978-3-642-33042-1 43
20. Gilio, A., Sanﬁlippo, G.: Probabilistic entailment in the setting of coherence: the
role of quasi conjunction and inclusion relation. Int. J. Approximate Reasoning
54(4), 513–525 (2013). https://doi.org/10.1016/j.ijar.2012.11.001
21. Gilio, A., Sanﬁlippo, G.: Quasi conjunction, quasi disjunction, t-norms and t-
conorms: Probabilistic aspects. Inf. Sci. 245, 146–167 (2013). https://doi.org/10.
1016/j.ins.2013.03.019
22. Gilio, A., Sanﬁlippo, G.: Conditional random quantities and compounds of con-
ditionals. Stud. Logica. 102(4), 709–729 (2014). https://doi.org/10.1007/s11225-
013-9511-6
23. Gilio, A., Sanﬁlippo, G.: Conjunction of conditional events and t-norms. In: Kern-
Isberner, G., Ognjanovi´c, Z. (eds.) ECSQARU 2019. LNCS (LNAI), vol. 11726, pp.
199–211. Springer, Cham (2019). https://doi.org/10.1007/978-3-030-29765-7 17
24. Gilio, A., Sanﬁlippo, G.: Generalized logical operations among conditional events.
Appl. Intell. 49(1), 79–102 (2019). https://doi.org/10.1007/s10489-018-1229-8
25. Gilio, A., Sanﬁlippo, G.: Algebraic aspects and coherence conditions for conjoined
and disjoined conditionals. Int. J. Approximate Reasoning 126, 98–123 (2020).
https://doi.org/10.1016/j.ijar.2020.08.004
26. Gilio, A., Sanﬁlippo, G.: Compound conditionals, Fr´echet-Hoeﬀding bounds, and
Frank t-norms. Int. J. Approximate Reasoning 136, 168–200 (2021). https://doi.
org/10.1016/j.ijar.2021.06.006
27. Gilio, A., Sanﬁlippo, G.: On compound and iterated conditionals. Argumenta 6(2),
241–266 (2021). https://doi.org/10.14275/2465-2334/202112.gil
28. Goodman, I.R., Nguyen, H.T., Walker, E.A.: Conditional Inference and Logic for
Intelligent Systems: A Theory of Measure-Free Conditioning. North-Holland (1991)
29. Holzer, S.: On coherence and conditional prevision. Bollettino dell’Unione Matem-
atica Italiana 4(6), 441–460 (1985)

686
N. Pfeifer and G. Sanﬁlippo
30. Horn, L.R.: A Natural History of Negation. CSLI Publications, Stanford (2001)
31. Kaufmann, S.: Conditionals right and left: probabilities for the whole family. J.
Philos. Log. 38, 1–53 (2009)
32. Kraus, S., Lehmann, D., Magidor, M.: Nonmonotonic reasoning, preferential mod-
els and cumulative logics. Artif. Intell. 44, 167–207 (1990)
33. Lad, F.: Operational Subjective Statistical Methods: A Mathematical, Philosoph-
ical, and Historical Introduction. Wiley, New York (1996)
34. Lenzen, W.: A critical examination of the historical origins of connexive logic. Hist.
Philos. Logic 41(1), 16–35 (2020)
35. Lewis, D.: Probabilities of conditionals and conditional probabilities. Philos. Rev.
85, 297–315 (1976)
36. McCall, S.: A history of connexivity. In: Gabbay, D.M., Pelletier, F.J., Woods, J.
(eds.) Handbook of the History of Logic, vol. 11 (Logic: A History of its Central
Concepts). Elsevier, Amsterdam (2012)
37. McGee, V.: Conditional probabilities and compounds of conditionals. Philos. Rev.
98, 485–541 (1989)
38. Pearl, J.: Probabilistic semantics for nonmonotonic reasoning: a survey. In: Shafer,
G., Pearl, J. (eds.) Readings in Uncertain Reasoning, pp. 699–711. Morgan Kauf-
mann, San Mateo (1990)
39. Pfeifer, N.: Experiments on Aristotle’s Thesis: towards an experimental philosophy
of conditionals. Monist 95(2), 223–240 (2012)
40. Pfeifer, N.: Probability logic. In: Knauﬀ, M., Spohn, W. (eds.) Handbook of Ratio-
nality. MIT Press, Cambridge (in press)
41. Pfeifer, N., Kleiter, G.D.: Coherence and nonmonotonicity in human reasoning.
Synthese 146(1–2), 93–109 (2005)
42. Pfeifer, N., Kleiter, G.D.: Framing human inference by coherence based probability
logic. J. Appl. Log. 7(2), 206–217 (2009)
43. Pfeifer, N., Kleiter, G.D.: The conditional in mental probability logic. In: Oaksford,
M., Chater, N. (eds.) Cognition and Conditionals: Probability and Logic in Human
Thought, pp. 153–173. Oxford University Press, Oxford (2010)
44. Pfeifer, N., Sanﬁlippo, G.: Probabilistic squares and hexagons of opposition under
coherence. Int. J. Approximate Reasoning 88, 282–294 (2017). https://doi.org/10.
1016/j.ijar.2017.05.014
45. Pfeifer, N., Sanﬁlippo, G.: Probabilistic semantics for categorical syllogisms of
Figure II. In: Ciucci, D., Pasi, G., Vantaggi, B. (eds.) SUM 2018. LNCS (LNAI),
vol. 11142, pp. 196–211. Springer, Cham (2018). https://doi.org/10.1007/978-3-
030-00461-3 14
46. Pfeifer, N., Sanﬁlippo, G.: Probability propagation in selected Aristotelian syl-
logisms. In: Kern-Isberner, G., Ognjanovi´c, Z. (eds.) ECSQARU 2019. LNCS
(LNAI), vol. 11726, pp. 419–431. Springer, Cham (2019). https://doi.org/10.1007/
978-3-030-29765-7 35
47. Pfeifer, N., Tulkki, L.: Conditionals, counterfactuals, and rational reasoning. An
experimental study on basic principles. Minds Mach. 27(1), 119–165 (2017)
48. Ramsey, F.P.: General propositions and causality (1929). In: Mellor, D.H. (ed.)
Philosophical Papers by F. P. Ramsey, pp. 145–163. Cambridge University Press,
Cambridge (1929/1994)
49. Regazzini, E.: Finitely additive conditional probabilities. Rendiconti del Seminario
Matematico e Fisico di Milano 55, 69–89 (1985)

Interpreting Connexive Principles in Coherence-Based Probability Logic
687
50. Sanﬁlippo, G.: Lower and upper probability bounds for some conjunctions of two
conditional events. In: Ciucci, D., Pasi, G., Vantaggi, B. (eds.) SUM 2018. LNCS
(LNAI), vol. 11142, pp. 260–275. Springer, Cham (2018). https://doi.org/10.1007/
978-3-030-00461-3 18
51. Sanﬁlippo, G., Gilio, A., Over, D.E., Pfeifer, N.: Probabilities of conditionals and
previsions of iterated conditionals. Int. J. Approximate Reasoning 121, 150–173
(2020). https://doi.org/10.1007/978-3-030-00461-3 14
52. Walley, P., Pelessoni, R., Vicig, P.: Direct algorithms for checking consistency and
making inferences from conditional probability assessments. J. Stat. Plann. Infer-
ence 126(1), 119–151 (2004)
53. Wansing, H.: Connexive logic. In: Zalta, E.N. (ed.) The Stanford Encyclopedia
of Philosophy. Spring 2020 edn. (2020). https://plato.stanford.edu/entries/logic-
connexive/

Correction to: Persuasive Contrastive
Explanations for Bayesian Networks
Tara Koopman and Silja Renooij
Correction to:
Chapter “Persuasive Contrastive Explanations for Bayesian
Networks” in: J. Vejnarová and N. Wilson (Eds.): Symbolic
and Quantitative Approaches to Reasoning with Uncertainty,
LNAI 12897, https://doi.org/10.1007/978-3-030-86772-0_17
In the originally published article, the pseudocode in Algorithm 1 does not include the
lines that were left intentionally blank. This has been corrected to include 2 lines (line 9
and 14), which are intentionally left blank in the algorithm for the addition of new code
to these lines.
The updated version of this chapter can be found at
https://doi.org/10.1007/978-3-030-86772-0_17
© Springer Nature Switzerland AG 2021
J. Vejnarová and N. Wilson (Eds.): ECSQARU 2021, LNAI 12897, p. C1, 2021.
https://doi.org/10.1007/978-3-030-86772-0_49

Author Index
Adorni, Giorgia
399
Afantenos, Stergos
3
Aﬁﬁ, Sohaib
381
Alarcón, Yonatan Carlos Carranza
413
Alcântara, João
159, 173, 187
Aldini, Alessandro
575
Amgoud, Leila
19, 32
Antonucci, Alessandro
284, 399
Baaj, Ismaïl
513
Benikhlef, Sarah
217
Besnard, Philippe
486
Bex, Floris
45
Bolt, Janneke H.
590
Bonesana, Claudio
399
Borg, AnneMarie
45
Casanova, Arianna
603
Chalaguine, Lisa
59
Corsi, Esther Anna
301
Curzi, Gianluca
575
Daniel, Milan
314
David, Victor
32
De Bock, Jasper
442
de Cooman, Gert
442
de Wit, Vincent
616
Destercke, Sébastien
413
Doder, Dragan
616
Doutre, Sylvie
486
Dubois, Didier
528
Elouedi, Zied
342
Fargier, Hélène
328
Flaminio, Tommaso
301, 543
Gabbay, Dov
89
Gilio, Angelo
629
Giordano, Laura
557
Godo, Lluis
543
Graziani, Pierluigi
575
Grina, Fares
342
Hosni, Hykel
301
Hunter, Anthony
59, 74
Jiroušek, Radim
354
Kampik, Timotheus
89
Katoen, Joost-Pieter
268
Kohlas, Juerg
603
Koopman, Tara
229
Kratochvíl, Václav
314, 354
Kunze, Tarek
3
Kwisthout, Johan
243
Labreuche, Christophe
471
Landes, Jürgen
644
Lefèvre, Eric
342, 381
Leray, Philippe
217
Lim, Suryani
3
Liu, Xiaolong
486
Llerena, Julissa Villanueva
284
Mailly, Jean-Guy
103
Mangili, Francesca
399
Martin-Dorel, Érik
328
Mata Díaz, Amílcar
457
Mauá, Denis Deratani
284
Maudet, Nicolas
513
Messaoud, Montassar Ben
217
Meyer, John Jules
616
Miranda, Enrique
427
Montes, Ignacio
427
Motoura, Shota
657
Mu, Kedian
499
Neugebauer, Daniel
117
Ouerdane, Wassila
513
Persiau, Floris
442
Petturiti, Davide
367
Pfeifer, Niki
672
Pichon, Frédéric
381
Pino Pérez, Ramón
457
Plajner, Martin
255

690
Author Index
Poli, Jean-Philippe
513
Pomeret-Coquot, Pierre
328
Potyka, Nico
130
Prade, Henri
3, 528
Prakken, Henry
144
Raschia, Guillaume
217
Renooij, Silja
229
Richard, Gilles
3
Rico, Agnès
528
Rothe, Jörg
117
Sá, Samy
159, 173
Sakly, Fayrouz
217
Salmani, Bahare
268
Sanﬁlippo, Giuseppe
629, 672
Silva, Rafael
187
Skiba, Kenneth
117
Tagliaferri, Mirko
575
Tedjini, Tekwa
381
Ugolini, Sara
543
van der Gaag, Linda C.
590
Vantaggi, Barbara
367
Vesic, Srdjan
201
Vomlel, Jiˇrí
255
Yun, Bruno
201
Zaffalon, Marco
603

