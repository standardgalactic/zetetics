Monge, Bregman and Occam: Interpretable Optimal Transport in
High-Dimensions with Feature-Sparse Maps
Marco Cuturi 1 Michal Klein 1 Pierre Ablin 1
Abstract
Optimal transport (OT) theory focuses, among
all maps T : Rd →Rd that can morph a prob-
ability measure onto another, on those that are
the “thriftiest”, i.e. such that the averaged cost
c(x, T(x)) between x and its image T(x) be
as small as possible. Many computational ap-
proaches have been proposed to estimate such
Monge maps when c is the ℓ2
2 distance, e.g., us-
ing entropic maps (Pooladian and Niles-Weed,
2021), or neural networks (Makkuva et al., 2020;
Korotin et al., 2020). We propose a new model
for transport maps, built on a family of transla-
tion invariant costs c(x, y) := h(x −y), where
h := 1
2∥· ∥2
2 + τ and τ is a regularizer. We pro-
pose a generalization of the entropic map suitable
for h, and highlight a surprising link tying it with
the Bregman centroids of the divergence Dh gen-
erated by h, and the proximal operator of τ. We
show that choosing a sparsity-inducing norm for τ
results in maps that apply Occam’s razor to trans-
port, in the sense that the displacement vectors
∆(x) := T(x) −x they induce are sparse, with
a sparsity pattern that varies depending on x. We
showcase the ability of our method to estimate
meaningful OT maps for high-dimensional single-
cell transcription data, in the 34000-d space of
gene counts for cells, without using dimensional-
ity reduction, thus retaining the ability to interpret
all displacements at the gene level.
1. Introduction
A fundamental task in machine learning is learning how to
transfer observations from a source to a target probability
measure. For such problems, optimal transport (OT) (San-
tambrogio, 2015) has emerged as a powerful toolbox that
can improve performance and guide theory in various set-
tings. For instance, the computational approaches advo-
cated in OT have been used to transfer knowledge across
1Apple. {cuturi,michalk,p ablin}@apple.com
datasets in domain adaptation tasks (Courty et al., 2016;
2017), train generative models (Montavon et al., 2016; Ar-
jovsky et al., 2017; Genevay et al., 2018; Salimans et al.,
2018), and realign datasets in natural sciences (Janati et al.,
2019; Schiebinger et al., 2019).
High-dimensional Transport. OT ﬁnds its most straight-
forward and intuitive use-cases in low-dimensional geomet-
ric domains (grids and meshes, graphs, etc...). This work
focuses on the more challenging problem of using it on
distributions in Rd, with d ≫1. In Rd, the ground cost
c(x, y) between observations x, y is often the ℓ2 metric or
its square ℓ2
2. However, when used on large-d data samples,
that choice is rarely meaningful. This is due to the curse-of-
dimensionality associated with OT estimation (Dudley et al.,
1966; Weed and Bach, 2019) and the fact that the Euclidean
distance loses its discriminative power as dimension grows.
To mitigate this, practitioners rely on dimensionality reduc-
tion, either in two steps, before running OT solvers, using,
e.g., PCA, a VAE, or a sliced-Wasserstein approach (Rabin
et al., 2012; Bonneel et al., 2015); or jointly, by estimating
both a projection and transport, e.g., on hyperplanes (Niles-
Weed and Rigollet, 2022; Paty and Cuturi, 2019; Lin et al.,
2020; Huang et al., 2021; Lin et al., 2021), lines (Deshpande
et al., 2019; Kolouri et al., 2019), trees (Le et al., 2019) or
more advanced featurizers (Salimans et al., 2018). However,
an obvious drawback of these approaches is that transport
maps estimated in reduced dimensions are hard to interpret
in the original space (Muzellec and Cuturi, 2019).
Contributions. To target high d regimes, we introduce
a radically different approach. We use the sparsity tool-
box (Hastie et al., 2015; Bach et al., 2012) to build OT maps
that are, adaptively to input x, drastically simpler:
• We introduce a generalized entropic map (Pooladian
and Niles-Weed, 2021) for translation invariant costs
c(x, y) := h(x −y), where h is strongly convex. That
entropic map Th,ε is deﬁned almost everywhere (a.e.), and
we show that it induces displacements ∆(x) := T(x)−x
that can be cast as Bregman centroids, relative to the
Bregman divergence generated by h.
• When h is an elastic-type regularizer, the sum of a
strongly-convex term ℓ2
2 and a sparsifying norm τ, we
show that such centroids are obtained using the proximal
arXiv:2302.04065v1  [stat.ML]  8 Feb 2023

Monge, Bregman and Occam
h(z) = 1
2∥z∥2
2
h(z) = 1
2∥z∥2
2 + 1∥z∥1
h(z) = 1
2∥z∥2
2 + 0.5τstvs(z)
h(z) = 1
2∥z∥2
2 + 10∥z∥2
ovk
h(z) = 1
2∥z∥2
2 + 2∥z∥1
h(z) = 1
2∥z∥2
2 + 2τstvs(z)
h(z) = 1
2∥z∥2
2 + 100∥z∥2
ovk
Base measure xi
Target measure yj
Points x
Entropic map Th,ε(x)
Figure 1. Plots of entropic map estimators Th,ε, as deﬁned in Prop. 4.2, to map a 2D measure supported on (xi) onto that supported on
(yj), for various costs h. The displacements ∆(x) = Th,ε(x) −x of unseen points are displayed as arrows. From left to right: standard
ℓ2
2 norm, Elastic ℓ1, STVS, and k-support costs (k = 1). For each proposed cost, the regularization γ is small on the top row and high on
the bottom. Displacements are not sparse for the ℓ2
2 cost but become increasingly so as γ grows, with a support that varies with input x.
Note that Elastic ℓ1 and STVS tend to censor displacements as γ grows, to the extent that they become null. In contrast, the k-support
cost encourages sparsity but enforces displacements with at least k non-zero values. See also Figure 2 for aggregate results.
operator of τ. This induces sparse displacements ∆(x),
with a sparsity pattern that depends on x, controlled by the
regularization strength set for τ. To our knowledge, our
formulation is the ﬁrst in the computational OT literature
that can produce features-wise sparse OT maps.
• We apply our method to single-cell transcription data us-
ing two different sparsity-inducing proximal operators.
We show that this approach succeeds in recovering mean-
ingful maps in extremely high-dimension.
Not the Usual Sparsity found in Computational OT. Let
us emphasize that the sparsity studied in this work is un-
related, and, in fact, orthogonal, to the many references to
sparsity found in the computational OT literature. Such
references arise when computing an OT plan from n to m
points, resulting in large n × m optimal coupling matrices.
Such matrices are sparse when any point in the source mea-
sure is only associated to one or a few points in the target
measure. Such sparsity acts at the level of samples, and is
usually a direct consequence of linear programming dual-
ity (Peyr´e and Cuturi, 2019, Proposition 3.4). It can be also
encouraged with regularization (Courty et al., 2016; Dessein
et al., 2018; Blondel et al., 2018) or constraints (Liu et al.,
2022). By contrast, sparsity in this work only occurs relative
to the features of the displacement vector ∆(x) ∈Rd, when
moving a given x, i.e., ∥∆(x)∥0 ≪d. Note, ﬁnally, that we
do not use coupling matrices in this paper.
Links to OT Theory with Degenerate Costs. Starting
with the seminal work by Sudakov (1979), who proved the
existence of Monge maps for the original Monge problem,
studying non-strongly convex costs with gradient disconti-
nuities (Santambrogio, 2015, §3) has been behind many key
theoretical developments (Ambrosio and Pratelli, 2003; Am-
brosio et al., 2004; Evans and Gangbo, 1999; Trudinger and
Wang, 2001; Carlier et al., 2010; Bianchini and Bardelloni,
2014). While these works have few practical implications,
because they focus on the existence of Monge maps, con-
structed by stitching together OT maps deﬁned pointwise,
they did, however, guide our work in the sense that they
shed light on the difﬁculties that arise from “ﬂat” norms
such as ℓ1. This has guided our focus in this work on elastic-
type norms, which allow controlling the amount of sparsity
through regularization strength, by analogy with the Lasso
tradeoff where an ℓ2
2 loss is paired with an ℓ1 regularizer.
2. Background
2.1. The Monge Problem.
Consider a translation-invariant cost function c(x, y) :=
h(x −y), where h : Rd →R. The Monge problem (1781)
consists of ﬁnding, among all maps T : Rd →Rd that
push-forward a measure µ ∈P(Rd) onto ν ∈P(Rd), the
map which minimizes the average length (as measured by
h) of its displacements:
T ⋆:= arg inf
T ♯µ=ν
Z
Rd h(x −T(x)) dµ .
(1)
From Dual Potentials to Optimal Maps.
Problem (1)
is notoriously difﬁcult to solve directly since the set of

Monge, Bregman and Occam
h(z) = 1
2∥z∥2
2
h(z) = 1
2∥z∥2
2 + 1∥z∥1
h(z) = 1
2∥z∥2
2 + 0.5τstvs(z)
h(z) = 1
2∥z∥2
2 + 10∥z∥2
ovk
h(z) = 1
2∥z∥2
2 + 2∥z∥1
h(z) = 1
2∥z∥2
2 + 2τstvs(z)
h(z) = 1
2∥z∥2
2 + 100∥z∥2
ovk
Base measure xi
Target measure yj
Entropic map distribution (Th,ε(xi))
Figure 2. Follow-up to Figure 1, where a fresh sample of points from the base measure is transported using the various entropic map
estimators Th,ε that were considered. Paired with Figure 1, this plot shows the tradeoff, controlled by γ, between the sparsity of
displacements and the ability to recover the target measure (as γ increases, ultimately, the map no longer moves points.). An interesting
feature of the ∥· ∥ovk norm resides in its ability, no matter what γ, to enforce at least one displacement (here k = 1).
admissible maps T is not even convex. The deﬁning feature
of OT theory to obtain an optimal push-forward solution
T ⋆is to cast Problem (1) as a linear optimization problem:
relax the requirement that x is mapped onto a single point
T(x), to optimize instead over the space of couplings of
µ, ν, namely on the set Π(µ, ν) of probability distributions
in P(Rd × Rd) with marginals µ, ν:
P ⋆:= arg inf
P ∈Π(µ,ν)
ZZ
Rd×Rd c dP .
(2)
If T ⋆is optimal for Equation 1, then (Id, T ⋆)♯µ is trivially
an optimal coupling. To recover a map T ⋆from a coupling
P ⋆requires considering the dual to (2):
f ⋆, g⋆∈arg sup
f,g:Rd→R
f⊕g≤c
Z
Rd f dµ +
Z
Rd g dν ,
(3)
where ∀x, y we write (f ⊕g)(x, y) := f(x) + g(y).
Leaving aside how such couplings and dual potentials can
be approximated from data (this will be discussed in the next
section), suppose that we have access to an optimal dual pair
(f ⋆, g⋆). By a standard duality argument (Santambrogio,
2015, §1.3), if a pair (x0, y0) lies in the support of P ⋆,
supp(P ⋆), the constraint for dual variables is saturated, i.e.,
f ⋆(x0) + g⋆(y0) = h(x0 −y0) ,
Additionally, by a so-called c-concavity argument one has:
g⋆(y0) = inf
x h(x −y0) −f ⋆(x).
Assuming f ⋆is differentiable at x0, combining these two
results yields perhaps the most pivotal result in OT theory:
(x0, y0) ∈supp(P ⋆) ⇔∇f ⋆(x0) ∈∂h(x0 −y0) , (4)
where ∂h denotes the subdifferential of h, see, e.g. (Carlier
et al., 2010). Let h∗be the convex conjugate of h,
h∗(y) := sup
x∈Rd⟨x, y⟩−h(x).
Depending on h, two cases arise in the literature:
• If h is differentiable everywhere, strictly convex, one has:
∇f ⋆(x0) = ∇h(x0 −y0) .
Thanks to the identity ∇h∗= (∇h)−1, one can uniquely
characterize the only point y0 to which x0 is associated in
the optimal coupling P ⋆as x0 −∇h∗(∇f ⋆(x0)). More
generally one recovers therefore for any x in supp(µ):
T ⋆(x) = x −∇h∗◦∇f ⋆(x) .
(5)
The Brenier theorem (1991) is a particular case of that
result, which states that when h =
1
2∥· ∥2
2, we have
T(x) = x −∇f ⋆(x0), since in that case ∇h = ∇h∗=
(∇h)−1 = Id, see (Santambrogio, 2015, Theo. 1.22).
• If h is “only” convex, then one recovers the sub-
differential inclusion y0 ∈x0 + ∂h∗(∇f ⋆(x0)) (Am-
brosio et al., 2004)(Santambrogio, 2015, §3).
In summary, given an optimal dual solution f ⋆to Prob-
lem (3), one can use differential (or sub-differential) calcu-
lus to deﬁne an optimal transport map, in the sense that it
deﬁnes (uniquely or as a multi-valued map) where the mass
of a point x should land.

Monge, Bregman and Occam
−5
0
5
z
0
2
4
∥z∥1
τstvs(z)
−5
0
5
z
−5
0
5
prox∥z∥1(z)
proxτstvs(z)
Figure 3. The STVS regularizer is not convex, but its proximal
operator is well-deﬁned and tends to shrink values less than the
usual soft-thresholding operator. For instance, its values near
{−5, 5} are close to the identity line.
2.2. Bregman Centroids
We suppose in this section that h is strongly convex, in
which case its convex conjugate is differentiable everywhere
and gradient smooth. The generalized Bregman divergence
(or B-function) generated by h (Telgarsky and Dasgupta,
2012; Kiwiel, 1997) is,
Dh(x|y) = h(x) −h(y) −
sup
w∈∂h(y)
⟨w, x −y⟩.
Consider a family of k points z1, . . . , zm ∈Rd with weights
p1, . . . , pm > 0 summing to 1. A point in the set
arg min
z∈Rd
X
j
p jDh(z, zj) ,
is called a Bregman centroid (Nielsen and Nock, 2009, Theo.
3.2). Assuming h is differentiable at each zj, one has that
this point is uniquely deﬁned as:
Ch
 (zj)j, (p j)i

:= ∇h⋆


m
X
j=1
p j∇h(zi)

.
(6)
2.3. Sparsity-Inducing Penalties
To form relevant functions h, we will exploit the following
sparsity-inducing functions: the ℓ1 and ∥·∥ovk norms, and a
handcrafted penalty that mimics the thresholding properties
of ℓ1 but with less shrinkage.
• For a vector z ∈Rd, ∥z∥p := (Pd
i=1 |zi|p)1/p. We write
ℓ2 and ℓ2
2 for ∥· ∥2 and ∥· ∥2
2 respectively.
• We write ℓ1 for ∥·∥1. Its proximal operator proxγℓ1(z) =
STγ(z) = (1−γ/|z|)+⊙z is called the soft-thresholding
operator.
• Schreck et al. (2015) propose the soft-thresholding opera-
tor with vanishing shrinkage (STVS),
τstvs(z) = γ21T
d

σ(z) + 1
2 −1
2e−2σ(z)
≥0 ,
(7)
with σ(z) := asinh

z
2γ

, and where all operations are
element-wise. τstvs is a non-convex regularizer, non-
negative thanks to (our) addition of + 1
2, to recover a non-
negative quantity that cancels if and only if z = 0. Schreck
et al. show that the proximity operator proxτstvs, written
STVS for short, decreases the shrinkage (see Figure 3)
observed with soft-thresholding:
STVSγ(z) =
 1 −γ2/|z|2
+ ⊙z .
(8)
The Hessian of τstvs is a diagonal matrix with values
1
2|z|/
p
z2 + γ2−1
2 and is therefore lower-bounded (with
positive-deﬁnite order) by −1
2Id.
• Let Gk be the set of all subsets of size k within {1, . . . , d}.
Argyriou et al. (2012) introduces the k-overlap norm:
∥z∥ovk = min{
X
I∈Gk
∥vI∥2 | supp(vI) ⊂I,
X
I∈Gk
vI = z} .
For any vector z in Rd, we write z↓for the vector com-
posed with all entries of z sorted in a decreasing order.
This formula can be evaluated as follows to exhibit a ℓ1/ℓ2
norm split between the d variables in a vector:
∥z∥2
ovk =
k−r−1
X
i=1
(|z|↓
i )2 +
 
d
X
i=k−r
|z|↓
i
!2
/(r + 1)
where r ≤k −1 is the unique integer such that
|z|↓
k−r ≤
d
X
i=k−r
|z|↓
i < |z|↓
k−r−1.
Its proximal operator is too complex to be recalled here
but given in (Argyriou et al., 2012, Algo. 1), running in
O(d(log d + k)) operations.
Note that both ℓ1 and τstvs are separable—their proximal
operators act element-wise—but it is not the case of ∥· ∥ovk.
3. Generalized Entropic-Bregman Maps
Generalized Entropic Potential.
When h is the ℓ2
2 cost,
and when µ and ν can be accessed through samples, i.e.,
ˆµn = 1
n
P
i δxi, ˆνm =
1
m
P
j δyj, a convenient estimator
for f ⋆and subsequently T ⋆is the entropic map (Pooladian
and Niles-Weed, 2021; Rigollet and Stromme, 2022). We
generalize these estimators for arbitrary costs h. Similar to
the original approach, our construction starts by solving a
dual entropy-regularized OT problem. Let ε > 0 and write
Kij = [exp(−h(xi −yj)/ε)]ij the kernel matrix induced
by cost h. Deﬁne (up to a constant):
f ⋆, g⋆= arg max
f∈Rn,g∈Rm⟨f, 1n
n ⟩+ ⟨g, 1m
m ⟩−ε⟨e
f
ε , Ke
g
ε ⟩.
(9)

Monge, Bregman and Occam
Problem (9) is the regularized OT problem in dual
form (Peyr´e and Cuturi, 2019, Prop. 4.4), an unconstrained
concave optimization problem that can be solved with the
Sinkhorn algorithm (Cuturi, 2013). Once such optimal vec-
tors are computed, estimators fε, gε of the optimal dual
functions f ⋆, g⋆of Equation 3 can be recovered by extend-
ing these discrete solutions to unseen points x, y,
fε(x) = minε([h(x −yj) −g⋆
j]j) ,
(10)
gε(y) = minε([h(xi −y) −f ⋆
i ]i) ,
(11)
where for a vector u or arbitrary size s we deﬁne the log-
sum-exp operator as minε(u) := −ε log( 1
s1T
s e−u/ε).
Generalized Entropic Maps.
Using the blueprint given
in Equation 4, we use the gradient of these dual potential
estimates to formulate maps. Such maps are only properly
deﬁned on a subset of Rd deﬁned as follows:
Ωˆνm(h) := {x | ∀j ≤m, ∇h(x −yj) exists.} ⊂Rd.
(12)
However, because a convex function is a.e. differentiable,
Ωˆνm(h) has measure 1 in Rd. With this, ∇fε is properly
deﬁned for x in Ωˆνm(h), as:
∇fε(x) =
m
X
j=1
p j(x)∇h(x −yj) ,
(13)
using the x-varying Gibbs distribution in the m-simplex:
p j(x) :=
exp
 −
 h(x −yj) −g⋆
j

/ε

Pm
k=1 exp (−(h(x −yk) −g⋆
k) /ε) .
(14)
One can check that if h = 1
2ℓ2
2, Equation 13 simpliﬁes to
the usual estimator (Pooladian and Niles-Weed, 2021):
T2,ε(x) := x −∇fε(x) =
m
X
j=1
p j(x)yj .
(15)
We can now introduce the main object of interest of this
paper, starting back from Equation 5, to provide a suitable
generalization for entropic maps of elastic-type:
Deﬁnition 3.1. The entropic map estimator for h evaluated
at x ∈Ωˆνm(h) is x −∇h∗◦∇fε(x). This simpliﬁes to:
Th,ε(x) := x −Ch((x −yj)j, (p j(x))j)
(16)
Bregman Centroids vs. Wc Gradient ﬂow
To displace
points, a simple approach consists of following Wc gradient
ﬂows, as proposed, for instance, in (Cuturi and Doucet,
2014) using a primal formulation Equation 2. In practice,
this can also be implemented by relying on variations in
dual potentials ∇fε, as advocated in Feydy et al. (2019, §4).
xi
yj
xi
yj
V
DLSsNAFJ3Ud31VXboZbAQ3lkTwsR04VLBPqAp4WZ60w6dTMLMRCjFH3Djr7hxoYhb9+78G6c1C209MHA45947954oE1wbz/tySnPzC4tLyvl1bX1jc3K1nZD
p7liWGepSFUrAo2CS6wbgS2MoWQRAKb0eBi7DfvUGmeylszLCTQE/ymDMwVgornsZ9t3DLmqG0lA3kBAJoHEY3IHCTHORSjesVL2aNwGdJX5BqTAdVj5DL
opyxM7kgnQu17memMQBnOBN6Xg1xjBmwAPWxbKiFB3RlNrmn+1bp0jhV9tmVJurvjhEkWg+TyFYmYPp62huL/3nt3MRnRGXW5Qsp+P4lxQk9JxNLTLFTIjh
pYAU9zuSlkfFDBjAyzbEPzpk2dJ46jmn9SOb46q5ydFHMtkl+yRA+KTU3JOrsg1qRNGHsgTeSGvzqPz7Lw57z+lJafo2SF/4Hx8A8Elmrs=</latexit>Dh-descent rf"
V
DLSgNBEJz1GeNr1aMg4ngKewGjB4DXjxGMA9IQuid9CZDZmeXmdlACLl58Ve8eFDEq7/gzb9x8jhoYsFAUdXd01BIrg2nvftrK1vbG5tZ3ayu3v7B4fu0XFN
x6liWGWxiFUjAI2CS6wabgQ2EoUQBQLrweB26teHqDSP5YMZJdiOoCd5yBkYK3Xcsy5qhtLQfEtCICGndYQFCai1jmO27OK3gz0FXiL0iOLFDpuF+tbszSyI
5kArRu+l5i2mNQhjOBk2wr1ZgAG0APm5ZKiFC3x7M7JvTCKl0axso+u9JM/d0xhkjrURTYyghMXy97U/E/r5ma8KY95jJDUo2/yhMBTUxnYZCu1whM2JkCTDF7
a6U9UEBMza6rA3BXz5ldSKBb9UuLov5sqlRwZckrOySXxyTUpkztSIVXCyCN5Jq/kzXlyXpx352NeuYsek7IHzifP2qXmP8=</latexit>descent rf"
Figure 4. Difference between Wc gradient descents, minimizing
loss directly in direction −λ∇fε, or Bregman descent as described
in Equation 17 when h = 1
2ℓ2
2 + ℓ1, see Theorem 4.1 for details
on computing ∇h∗. Six steps are plotted with stepsize λ = 1
4.
This approach arises from the approximation of Wc(ˆµn, ˆνm)
using the dual objective Equation 3,
Sh,ε

1
n
P
i δxi, 1
m
P
j δyj

= 1
n
P
i fε(xi)+ 1
m
P
j gε(yj) ,
differentiated using the Danskin theorem. As a result, any
point x in µ is then pushed away from ∇fε to decrease that
distance. This translates to a gradient descent scheme:
x ←x −λ∇fε(x)
Our analysis suggests that the descent must happen relative
to Dh, to use, instead, a Bregman update (here ¯λ = 1 −λ):
x ←∇h∗ ¯λ∇h(x) + λ∇h(x −∇h∗◦∇fε(x))

(17)
Naturally, these two approaches are exactly equivalent as
h = 1
2ℓ2
2 but result in very different trajectories for other
functions h as shown in Figure 4.
4. Structured Monge Displacements
We introduce in this section cost functions h that we call of
elastic-type, namely functions with a ℓ2
2 term in addition to
another function τ. When τ is sparsity-inducing (minimized
on sparse vectors, with kinks) and has a proximal operator
in closed form, we show that the displacements induced by
this function h are feature-sparse.
4.1. Elastic-type Costs
By reference to (Zou and Hastie, 2005), we call h of elastic-
type if it is strongly convex and can be written as
h(z) := 1
2∥z∥2 + τ(z) .
(18)
where τ : Rd →R is a function whose proximal operator is
well-deﬁned. Since OT algorithms are invariant to a posi-
tive rescaling of the cost c, our elastic-type costs subsume,
without loss of generality, all strongly-convex translation
invariant costs with convex τ. They do also include useful
cases arising when τ is not (e.g., τstvs).

Monge, Bregman and Occam
Proposition 4.1. For h as in (18) and x ∈Ωˆνm(τ) one has:
Th,ε(x) := x−proxτ

x −
m
X
j=1
p j(x)
 yj + ∇τ(x −yj)



(19)
Proof. The result follows from ∇h∗= proxτ . Indeed:
h∗(w) = sup
z wT z −1
2∥z∥2 −τ(z)
= −inf
z −wT z + 1
2∥z∥2 + τ(z)
= 1
2∥w∥2 −inf
z
1
2∥z −w∥2 + τ(z).
Differentiating on both sides and using Danskin’s lemma,
we get the desired result by developing ∇h and taking ad-
vantage of the fact that the weights p j(x) sum to 1.
4.2. Sparsity-Inducing Functions τ
We discuss in this section the three choices we introduced
in §2 for proximal operators and their practical implications
in the context of our generalized entropic maps.
1-Norm ℓ1.
As a ﬁrst example, we consider τ(z) = γ∥z∥1
in Equation 18. The associated proximal operator is the soft-
thresholding operator proxτ(·) = ST(·, γ) mentioned in
the introduction. We also have ∇h(z) = z + γ sign(z) for
z with no 0 coordinate. Plugging this in Equation 5, we ﬁnd
that the Monge map Tγℓ1,ε(x) is equal to
x −STγ

x −
m
X
j=1
p j(x)
 yj + γ sign(x −yj)


,
where the p j(x) are evaluated at x using Equation 14.
Applying the transport consists in an element-wise oper-
ation on x: for each of its features t ≤d, one substracts
STγ
Pm
j=1 p j(x)∇h(xt −yj
t)

. The only interaction be-
tween coordinates comes from the weights p j(x).
The soft-thresholding operator sparsiﬁes the displacement.
Indeed, when for a given x and a feature t ≤d one has
|xt −
m
X
j=1
p j(x)yj
t + γ sign(xt −yj
t)| ≤γ,
then there is no change on that feature : [Tγℓ1,ε(x)]t = xt.
That mechanism works to produce, locally, sparse displace-
ments on certain coordinates.
Another interesting phe-
nomenon happens when x is too far from the yj’s on some
coordinates, in which case the transport defaults back to
a ℓ2
2 average of the target points yj (with weights that are,
however, inﬂuenced by the γℓ1 regularization):
Proposition 4.2. If x is such that xt ≥maxj yj
t or xt ≤
minj yj
t then Tγℓ1,ε(x)t = P
j p j(x)yj
t.
Proof. For instance, assume xt ≥maxj yj
t. Then, for
all j, we have sign(xt −yj
t) = 1, and as a consequence
Pm
j=1 p j(x)∇h(x −yj)t = xt −P
j p j(x)yj
t + γ. This
quantity is greater than γ, so applying the soft-thresholding
gives STγ(Pm
j=1 p j(x)∇h(x−yj)t) = xt−P
j p j(x)yj
t,
which gives the advertised result. Similar reasoning gives
the same result when xt ≤minj yj
t.
Interestingly, this property depends on γ only through the
p j(x)’s, and the condition that xt ≥maxj yj
t or xt ≤
minj yj
t does not depend on γ at all.
Vanishing Shrinkage STVS:
The ℓ1 term added to form
the elastic net has a well-documented drawback, notably
for regression: on top of having a sparsifying effect on the
displacement, it also shrinks values. This is clear from the
soft-thresholding formula, where a coordinate greater than γ
is reduced by γ. This effect can lead to some “shortening” of
displacement lengths in the entropic maps. We use the Soft-
Thresholding with Vanishing Shrinkage (STVS) proposed
by Schreck et al. (2015) to overcome this problem. The
cost function is given by Equation 7, and its prox in Equa-
tion 8. When |z| is large, we have proxτstvs(z) = z + o(1),
which means that the shrinkage indeed vanishes. Interest-
ingly, even though the cost τstvs is non-convex, it still has a
proximal operator, and 1
2∥· ∥2 + τstvs is 1
2-strongly convex.
k-Overlap: τ = ∥· ∥ovk.
The k-overlap norm offers the
distinctive feature that its proximal operator selects any-
where between d (small γ) and k (large γ) non-zero vari-
ables, see Figure 1. Applying this proximal operator is,
however, signiﬁcantly more complex, because it is not sepa-
rable across coordinates and requires d(k+log d) operations,
instantiating a k × d −k matrix to select two integers r, l
(r ≤k ≤l) at each evaluation. We were able to use it for
moderate problem sizes but these costs became prohibitive
on larger scale datasets, where d is a few tens of thousands.
5. Experiments
We start this experimental study with two synthetic tasks.
For classic costs such as ℓ2
2, several examples of ground-
truth optimal maps are known. Unfortunately, we do not
know yet how to propose ground-truth h-optimal maps, nor
dual potentials, when h has the general structure considered
in this work. As a result, we study two synthetic problems,
where the sparsity pattern of a ground truth transport is ei-
ther constant across x or split across two areas. We follow
with an application to single-cell genomics, where the mod-
eling assumption that a treatment has a sparse effect on gene

Monge, Bregman and Occam
ℓ1
τstvs
∥· ∥2
ovk, k = 5
∥· ∥2
ovk, k = 7
ℓ2
2 cost
10−1 101
0.05
0.06
NMSE
d = 8, s = 5
10−1 101
0.04
0.05
d = 20, s = 5
10−1 101
0.025
0.050
d = 100, s = 5
10−1 101
Regularization γ
10−5
10−3
Support error
10−1 101
Regularization γ
10−3
10−2
10−1 101
Regularization γ
10−2
10−1
Figure 5. Synthetic experiment: ability of the estimated Th,ε to
recover a ground-truth transport which displaces s coordinates in
dimension d, with n = 1000 samples. We compare the different
costs h proposed in the paper with the classical ℓ2
2 cost. We identify
3 regimes. Left: when s ≃d, the ℓ2
2 cost is already good, and the
proposed costs barely improve over it in terms of MSE. Middle:
when d is moderately larger than s, all the proposed costs improve
over the ℓ2
2 cost, and the optimal regularization for ℓ1 and τstvs
are ﬁnite. Right: When d ≫s, the proposed methods vastly
improve over the ℓ2
2 cost. The optimal regularization for ℓ1 and
τstvs is inﬁnite even for the MSE. In terms of support error, larger
regularization always leads to better results.
activation (across 34k genes) is plausible. In terms of im-
plementaiton, the entire pipeline described in § 3 and 4 rests
on running the Sinkhorn algorithm ﬁrst, with an appropriate
cost, and than differentiating the resulting potentials. This
can be carried out in a few lines of code using a parame-
terized TICost, fed into the Sinkhorn solver, to output
a DualPotentials object in OTT-JAX1(Cuturi et al.,
2022). We a class of regularized translation invariant cost
functions, specifying both regularizers τ and their proximal
operators. We call such costs RegTICost.
5.1. Synthetic experiments.
Constant sparsity-pattern.
We measure the ability of
our method to recover a sparse transport map using a set-
ting inspired by (Pooladian and Niles-Weed, 2021). Here
µ = U[0,1]d. For an integer s < d, we set ν = T ⋆
s ♯µ, where
the map T ⋆
s acts on coordinates independently with the for-
mula T ⋆
s (x) = [exp(x1), . . . , exp(xs), xs+1, . . . , xd]: it
only changes the ﬁrst s coordinates of the vector, and cor-
responds to a sparse displacement when s ≪d. Note that
this sparse transport plan is much simpler than the maps our
model can handle since, for this synthetic example, the spar-
sity pattern is ﬁxed across samples. Note also that while it
might be possible to detect that only the ﬁrst s components
have high variability using a 2-step pre-processing approach,
1https://github.com/ott-jax/ott
101
102
103
Dimension d
10−2
10−1
NMSE
ℓ2
2 cost
ℓ1
τstvs
∥· ∥2
ovk, k = 5
∥· ∥2
ovk, k = 7
Figure 6. Scaling
with
dimension: The number
of
samples
is
ﬁxed
to n = 100, and the
sparsity to s = 2. For
each dimension, we do
a grid search over γ and
retain the one with the
lowest MSE.
or an adaptive, robust transport approach (Paty and Cuturi,
2019), our goal is to detect that support in a one-shot, thanks
to our choice of h. We generate n = 1, 000 i.i.d. samples xi
from µ, and yj from ν independently; the samples yj are ob-
tained by ﬁrst generating fresh i.i.d. samples ˜xj from µ and
then pushing them : yj := T ⋆
s (˜xj). We use our three costs
to compute Th,ε from these samples, and measure our ability
to recover T ⋆
s from Th,ε using a normalized MSE deﬁned as
1
nd
Pn
i=1 ∥T ⋆
s (xi)−Th,ε(xi)∥2. We also measure how well
our method identiﬁes the correct support: for each sample,
we compute the support error as Pd
i=s+1 ∆2
i / Pd
i=1 ∆2
i
with ∆the displacement Th,ε(x) −x. This quantity is be-
tween 0 and 1 and cancels if and only if the displacement
happens only on the correct coordinates. We then average
this quantity overall the xi. Figure 5 displays the results as
d varies and s is ﬁxed. Here, τstvs performs better than ℓ1.
x-dependent sparsity-pattern.
To illustrate the abil-
ity of our method to recover transport maps whose
sparsity pattern is adaptive, depending depending on
the input x, we extend the previous setting as fol-
lows.
To compute Fs(x), we compute ﬁrst the norms
of two coordinate groups of x:
n1
= Ps
i=1 x2
i and
n2 = P2s
i=s+1 x2
i . Second, we displace the coordinate
group with the largest norm: if n1 > n2, Fs(x) =
[exp(x1), . . . , exp(xs), xs+1, . . . , xd], otherwise Fs(x) =
[x1, . . . , xs, exp(xs+1), . . . , exp(x2s), x2s+1, . . . , xd].
Obviously, the displacement pattern depends on x. Figure 6
shows the NMSE with different costs when the dimension d
increases while s and n are ﬁxed. As expected, we observe
a much better scaling for our costs than for the standard ℓ2
2
cost, indicating that sparsity-inducing costs mitigate the
curse of dimensionality.
5.2. Single-Cell RNA-seq data.
We validate our approach on the single-cell RNA sequenc-
ing perturbation data from (Srivatsan et al., 2020). After
removing cells with less than 200 expressed genes and genes
expressed in less than 20 cells, the data consists of 579, 483
cells and 34, 636 genes. In addition, the raw counts have
been normalized and log(x+1) scaled. We select the 5 drugs
(Belinostat, Dacinostat, Givinostat, Hesperadin, and Quisi-
nostat) out of 188 drug perturbations that are highlighted in

Monge, Bregman and Occam
50
100
1
2ℓ2
2 estim.
60
80
100
120
Elastic γℓ1 estim.
ℓ2
2 Sink. div., PCA space
0.50
0.75
1.00
1
2ℓ2
2 estim.
0.4
0.6
0.8
1.0
R2, dosage: 10nM
0.0
0.5
1.0
1
2ℓ2
2 estim.
0.0
0.5
1.0
R2, dosage: 10µM
0.25
0.50
1
2ℓ2
2 estim.
0.2
0.4
0.6
RBO marker genes
10−1
6 × 10−2
2 × 10−1
3 × 10−1
4 × 10−1
2−3
20
Regularization γ
101
102
% non-zero in disp.
Hesperadin, MCF7
γℓ1
γ-τstvs
ℓ2
2
2−3
20
Regularization γ
2300
2350
2400
2450
2500
ℓ2
2 Sink. div., gene space
Hesperadin, MCF7
2−3
20
Regularization γ
101
102
% non-zero in disp.
Givinostat, K562
2−3
20
Regularization γ
2800
2850
2900
2950
3000
ℓ2
2 Sink. div., gene space
Givinostat, K562
Figure 7. Top row: performance, for all 15 experiments, of the elastic-γℓ1 estimator vs. the 1
2ℓ2
2 entropic map. We consider 6 values
for γ. Each of the 15 × 6 crosses denotes the mean, over 10 random 80%,/20% splits of that cell line/drug experiment, of a quantity
of interest. To facilitate reading, rather than reporting the γ value, we report the average percentage of non-zero displacements (using
np.isclose(.,0), equivalently thresholding values below 10−8) across all displaced points in that fold (yellow means 40% dense
displacements, dark-blue displacements only happen on ≈5% of genes). While all these maps are estimated in full genes space (≈34k),
we provide a simpliﬁed measure of their ability to reconstruct the measures by computing a 1
2ℓ2
2-Sinkhorn divergence in PCA space.
This picture shows that one can sparsify signiﬁcantly 1
2ℓ2
2 maps and still get a similar reconstruction error. Next, we picture separately
the R2 (see text body for details) computed on marker genes on low (10nM) and high (10µM) dosages of the drug. For low dosages,
inducing sparsity in displacements seems to help, whereas this may no longer be the case when the effect of perturbations becomes
large. Finally, the RBO metric shows that sparsity does help to select marker genes based only on map estimation. Bottom row: Close up
on Hesperadin/MCF7 and Givinostat/K562 experiments. For each, we quantify the sparsifying effect w.r.t γ, as well as 1
2ℓ2
2-Sinkhorn
divergence in full gene space.
Table 1. Per cell line, sample sizes of control + drug perturbation.
Cont.
Dac.
Giv.
Bel.
Hes.
Quis.
A549
3274
558
703
669
436
475
K562
3346
388
589
656
624
339
MCF7
6346
1562
1805
1684
882
1520
the original data (Srivatsan et al., 2020) as showing a strong
effect. We consider 3 human cancer cell lines (A549, K562,
MCF7) to each of which is applied each of the 5 drugs. We
use our four methods to learn an OT map from control to
perturbed cells in each of these 3 × 5 scenarios. For each
cell line/drug pair, we split the data into 10 non-overlapping
80%/20% train/test splits, keeping the test fold to produce
our metrics.
Methods. We ran experiments in two settings, using the
whole 34, 000-d gene space and subsetting to the top 5k
highly variable genes using SCANPY (Wolf et al., 2018).
We consider entropic map estimators with the following cost
functions and pre-processing approaches: 1
2ℓ2
2 cost; the 1
2ℓ2
2
cost on 50-d PCA space (PCA directions are recomputed
on each train fold); Elastic with γℓ1; Elastic with γ-τsvts
cost. We vary γ for these two methods. We did not use the
∥· ∥ovk norm because of memory challenges when handling
such a high-dimensional dataset. For the non-PCA-based
approaches, we can also measure their performance in PCA
space by projecting their high-dimensional predictions onto
the 50-d space. The ε regularization parameter for all these
approaches is set for each cost and experiment to 10% of
the mean value of the cost matrix between the train folds of
control and treated cells, respectively.
Evaluation. We evaluate methods using these metrics:
• the ℓ2
2-Sinkhorn divergence (using ε to be 10% of the
mean of pairwise ℓ2
2 cost matrix of treated cells) between
transferred points (from test fold of control) and test points
(from perturbed state); lower is better.
• Ranked biased overlap (Webber et al., 2010) with p = 0.9,

Monge, Bregman and Occam
between the 50 perturbation marker genes as computed
on all data with SCANPY, and the following per-gene
statistic, computed using a map as follows: average (on
fold) expression of (predicted) perturbed cells from orig-
inal control cells (this tracks changes in log-expression
before/after predicted treatment); higher is better.
• Coefﬁcient of determination (R2) between the average
ground-truth / predicted gene expression on the 50 pertur-
bation markers (Lotfollahi et al., 2019); higher is better.
These results are summarized in Figure 7, across various
costs, perturbations and hyperparameter choices.
Conclusion. We consider structured translation-invariant
ground costs h for transport problems. After forming an
entropic potential with such costs, we plugged it in Bre-
nier’s approach to construct a generalized entropic map. We
highlighted a surprising connection between that map and
the Bregman centroids associated with the divergence gen-
erated by h, resulting in a more natural approach to gradient
ﬂows deﬁned by Wc, illustrated in a simple example. By
selecting costs h of elastic type (a sum of ℓ2
2 and a sparsi-
fying term), we show that our maps mechanically exhibit
sparsity, in the sense that they have the ability to only impact
adaptively x on a subset of coordinates. We have proposed
two simple generative models where this property helps
estimation and applied this approach to high-dimensional
single-cell datasets where we show, at a purely mechanical
level, that we can recover meaningful maps. Many natural
extensions of our work arise, starting with more informative
sparsity-inducing norms (e.g., group lasso), and a more gen-
eral approach leveraging the Bregman geometry for more
ambitious Wc problems, such as barycenters.
References
Luigi Ambrosio and Aldo Pratelli. Existence and stability
results in the l1 theory of optimal transportation. Optimal
Transportation and Applications: Lectures given at the
CIME Summer School, held in Martina Franca, Italy,
September 2-8, 2001, pages 123–160, 2003.
Luigi Ambrosio, Bernd Kirchheim, and Aldo Pratelli. Ex-
istence of optimal transport maps for crystalline norms.
Duke Mathematical Journal, 125(2):207–241, 2004.
Andreas Argyriou, Rina Foygel, and Nathan Srebro. Sparse
prediction with the k-support norm. In F. Pereira, C.J.
Burges, L. Bottou, and K.Q. Weinberger, editors, Ad-
vances in Neural Information Processing Systems, vol-
ume 25. Curran Associates, Inc., 2012.
Martin Arjovsky, Soumith Chintala, and L´eon Bottou.
Wasserstein gan. arXiv preprint arXiv:1701.07875, 2017.
Francis Bach, Rodolphe Jenatton, Julien Mairal, Guillaume
Obozinski, et al. Optimization with sparsity-inducing
penalties. Foundations and Trends® in Machine Learn-
ing, 4(1):1–106, 2012.
Stefano Bianchini and Mauro Bardelloni. The decomposi-
tion of optimal transportation problems with convex cost.
arXiv preprint arXiv:1409.0515, 2014.
Mathieu Blondel, Vivien Seguy, and Antoine Rolet. Smooth
and sparse optimal transport. In International conference
on artiﬁcial intelligence and statistics, pages 880–889.
PMLR, 2018.
Nicolas Bonneel, Julien Rabin, Gabriel Peyr´e, and
Hanspeter Pﬁster. Sliced and radon wasserstein barycen-
ters of measures. Journal of Mathematical Imaging and
Vision, 51:22–45, 2015.
Yann Brenier. Polar factorization and monotone rearrange-
ment of vector-valued functions.
Comm. Pure Appl.
Math., 44(4):375–417, 1991.
ISSN 0010-3640.
doi:
10.1002/cpa.3160440402. URL https://doi.org/
10.1002/cpa.3160440402.
Guillaume Carlier, Luigi De Pascale, and Filippo Santam-
brogio. A strategy for non-strictly convex transport costs
and the example of ∥x −y∥p in r2. Communications in
Mathematical Sciences, 8(4):931–941, 2010.
Nicolas Courty, R´emi Flamary, Devis Tuia, and Alain Rako-
tomamonjy. Optimal transport for domain adaptation.
IEEE transactions on pattern analysis and machine intel-
ligence, 39(9):1853–1865, 2016.
Nicolas Courty, R´emi Flamary, Amaury Habrard, and Alain
Rakotomamonjy. Joint distribution optimal transportation
for domain adaptation. Advances in Neural Information
Processing Systems, 30, 2017.
Marco Cuturi. Sinkhorn distances: Lightspeed computation
of optimal transport. In Advances in neural information
processing systems, pages 2292–2300, 2013.
Marco Cuturi and Arnaud Doucet.
Fast computation
of wasserstein barycenters.
In Eric P. Xing and
Tony Jebara, editors, Proceedings of the 31st Interna-
tional Conference on Machine Learning, volume 32
of Proceedings of Machine Learning Research, pages
685–693, Bejing, China, 22–24 Jun 2014. PMLR.
URL https://proceedings.mlr.press/v32/
cuturi14.html.
Marco Cuturi, Laetitia Meng-Papaxanthos, Yingtao Tian,
Charlotte Bunne, Geoff Davis, and Olivier Teboul. Op-
timal transport tools (ott): A jax toolbox for all things
wasserstein. arXiv preprint arXiv:2201.12324, 2022.
Ishan Deshpande, Yuan-Ting Hu, Ruoyu Sun, Ayis Pyrros,
Nasir Siddiqui, Sanmi Koyejo, Zhizhen Zhao, David

Monge, Bregman and Occam
Forsyth, and Alexander G Schwing. Max-sliced wasser-
stein distance and its use for gans. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pages 10648–10656, 2019.
Arnaud Dessein, Nicolas Papadakis, and Jean-Luc Rouas.
Regularized optimal transport and the rot mover’s dis-
tance. The Journal of Machine Learning Research, 19(1):
590–642, 2018.
Richard Mansﬁeld Dudley et al.
Weak convergence of
probabilities on nonseparable metric spaces and empir-
ical measures on euclidean spaces. Illinois Journal of
Mathematics, 10(1):109–126, 1966.
Lawrence C Evans and Wilfrid Gangbo. Differential equa-
tions methods for the Monge-Kantorovich mass transfer
problem. American Mathematical Soc., 1999.
Jean Feydy, Thibault S´ejourn´e, Franc¸ois-Xavier Vialard,
Shun-ichi Amari, Alain Trouv´e, and Gabriel Peyr´e. In-
terpolating between optimal transport and mmd using
sinkhorn divergences. In The 22nd International Con-
ference on Artiﬁcial Intelligence and Statistics, pages
2681–2690. PMLR, 2019.
Aude Genevay, Gabriel Peyr´e, and Marco Cuturi. Learning
generative models with Sinkhorn divergences. In Pro-
ceedings of the 21st International Conference on Artiﬁcial
Intelligence and Statistics, pages 1608–1617, 2018.
Trevor Hastie, Robert Tibshirani, and Martin Wainwright.
Statistical learning with sparsity. Monographs on statis-
tics and applied probability, 143:143, 2015.
Minhui Huang, Shiqian Ma, and Lifeng Lai. A rieman-
nian block coordinate descent method for computing the
projection robust wasserstein distance. In Marina Meila
and Tong Zhang, editors, Proceedings of the 38th Inter-
national Conference on Machine Learning, volume 139
of Proceedings of Machine Learning Research, pages
4446–4455. PMLR, 18–24 Jul 2021.
Hicham Janati, Marco Cuturi, and Alexandre Gramfort.
Wasserstein regularization for sparse multi-task regres-
sion. In The 22nd International Conference on Artiﬁ-
cial Intelligence and Statistics, pages 1407–1416. PMLR,
2019.
Krzysztof C Kiwiel. Proximal minimization methods with
generalized bregman functions. SIAM journal on control
and optimization, 35(4):1142–1168, 1997.
Soheil Kolouri, Kimia Nadjahi, Umut Simsekli, Roland
Badeau, and Gustavo Rohde. Generalized sliced wasser-
stein distances. Advances in neural information process-
ing systems, 32, 2019.
Alexander Korotin, Vage Egiazarian, Arip Asadulaev,
Alexander Saﬁn, and Evgeny Burnaev. Wasserstein-2
generative networks.
In International Conference on
Learning Representations, 2020.
Tam Le, Makoto Yamada, Kenji Fukumizu, and Marco
Cuturi.
Tree-sliced variants of wasserstein distances.
Advances in neural information processing systems, 32,
2019.
Tianyi Lin, Chenyou Fan, Nhat Ho, Marco Cuturi, and
Michael Jordan. Projection robust wasserstein distance
and riemannian optimization. Advances in neural infor-
mation processing systems, 33:9383–9397, 2020.
Tianyi Lin, Zeyu Zheng, Elynn Chen, Marco Cuturi, and
Michael I Jordan. On projection robust optimal trans-
port: Sample complexity and model misspeciﬁcation. In
International Conference on Artiﬁcial Intelligence and
Statistics, pages 262–270. PMLR, 2021.
Tianlin Liu, Joan Puigcerver, and Mathieu Blondel.
Sparsity-constrained optimal transport. arXiv preprint
arXiv:2209.15466, 2022.
Mohammad Lotfollahi, F. Alexander Wolf, and Fabian J.
Theis. scgen predicts single-cell perturbation responses.
Nature Methods, 16(8):715–721, Aug 2019. ISSN 1548-
7105. doi: 10.1038/s41592-019-0494-8. URL https:
//doi.org/10.1038/s41592-019-0494-8.
Ashok Makkuva, Amirhossein Taghvaei, Sewoong Oh, and
Jason Lee. Optimal transport mapping via input convex
neural networks. In International Conference on Machine
Learning, pages 6672–6681. PMLR, 2020.
Gaspard Monge. M´emoire sur la th´eorie des d´eblais et des
remblais. Histoire de l’Acad´emie Royale des Sciences,
pages 666–704, 1781.
Gr´egoire Montavon, Klaus-Robert M¨uller, and Marco Cu-
turi. Wasserstein training of restricted boltzmann ma-
chines. In D. Lee, M. Sugiyama, U. Luxburg, I. Guyon,
and R. Garnett, editors, Advances in Neural Information
Processing Systems, volume 29. Curran Associates, Inc.,
2016.
Boris Muzellec and Marco Cuturi. Subspace detours: Build-
ing transport plans that are optimal on subspace projec-
tions. In H. Wallach, H. Larochelle, A. Beygelzimer,
F. d'Alch´e-Buc, E. Fox, and R. Garnett, editors, Advances
in Neural Information Processing Systems, volume 32.
Curran Associates, Inc., 2019.
Frank Nielsen and Richard Nock. Sided and symmetrized
bregman centroids. IEEE Transactions on Information
Theory, 55(6):2882–2904, 2009. doi: 10.1109/TIT.2009.
2018176.

Monge, Bregman and Occam
Jonathan Niles-Weed and Philippe Rigollet. Estimation
of wasserstein distances in the spiked transport model.
Bernoulli, 28(4):2663–2688, 2022.
Franc¸ois-Pierre Paty and Marco Cuturi. Subspace robust
wasserstein distances. arXiv preprint arXiv:1901.08949,
2019.
Gabriel Peyr´e and Marco Cuturi. Computational optimal
transport. Foundations and Trends® in Machine Learn-
ing, 11(5-6):355–607, 2019.
Gabriel Peyr´e and Marco Cuturi. Computational optimal
transport. Foundations and Trends in Machine Learning,
11(5-6), 2019. ISSN 1935-8245.
Aram-Alexandre Pooladian and Jonathan Niles-Weed. En-
tropic estimation of optimal transport maps.
arXiv
preprint arXiv:2109.12004, 2021.
Julien Rabin, Gabriel Peyr´e, Julie Delon, and Marc Bernot.
Wasserstein barycenter and its application to texture mix-
ing. In Scale Space and Variational Methods in Computer
Vision: Third International Conference, SSVM 2011, Ein-
Gedi, Israel, May 29–June 2, 2011, Revised Selected
Papers 3, pages 435–446. Springer, 2012.
Philippe Rigollet and Austin J Stromme. On the sample
complexity of entropic optimal transport. arXiv preprint
arXiv:2206.13472, 2022.
Tim Salimans, Han Zhang, Alec Radford, and Dimitris
Metaxas. Improving GANs using optimal transport. In
International Conference on Learning Representations,
2018.
Filippo Santambrogio. Optimal transport for applied math-
ematicians. Springer, 2015.
Geoffrey Schiebinger, Jian Shu, Marcin Tabaka, Brian
Cleary, Vidya Subramanian, Aryeh Solomon, Joshua
Gould, Siyan Liu, Stacie Lin, Peter Berube, et al. Optimal-
transport analysis of single-cell gene expression identiﬁes
developmental trajectories in reprogramming. Cell, 176
(4):928–943, 2019.
Amandine Schreck, Gersende Fort, Sylvain Le Corff, and
Eric Moulines. A shrinkage-thresholding metropolis ad-
justed langevin algorithm for bayesian variable selection.
IEEE Journal of Selected Topics in Signal Processing, 10
(2):366–375, 2015.
Richard Sinkhorn. A relationship between arbitrary posi-
tive matrices and doubly stochastic matrices. Ann. Math.
Statist., 35:876–879, 1964.
Sanjay R. Srivatsan, Jos´e L. McFaline-Figueroa, Vi-
jay Ramani, Lauren Saunders, Junyue Cao, Jonathan
Packer, Hannah A. Pliner, Dana L. Jackson, Riza M.
Daza, Lena Christiansen, Fan Zhang, Frank Steemers,
Jay Shendure, and Cole Trapnell.
Massively mul-
tiplex chemical transcriptomics at single-cell resolu-
tion. Science, 367(6473):45–51, 2020. doi: 10.1126/
science.aax6234.
URL https://www.science.
org/doi/abs/10.1126/science.aax6234.
Vladimir N Sudakov. Geometric problems in the theory of
inﬁnite-dimensional probability distributions. Number
141. American Mathematical Soc., 1979.
Matus Telgarsky and Sanjoy Dasgupta. Agglomerative breg-
man clustering. In Proceedings of the 29th International
Conference on Machine Learning, ICML 2012, Edin-
burgh, Scotland, UK, June 26 - July 1, 2012. icml.cc /
Omnipress, 2012.
Neil S Trudinger and Xu-Jia Wang. On the monge mass
transfer problem.
Calculus of Variations and Partial
Differential Equations, 13(1):19–31, 2001.
William Webber, Alistair Moffat, and Justin Zobel. A simi-
larity measure for indeﬁnite rankings. ACM Transactions
on Information Systems (TOIS), 28(4):1–38, 2010.
Jonathan Weed and Francis Bach. Sharp asymptotic and
ﬁnite-sample rates of convergence of empirical measures
in wasserstein distance. Bernoulli, 25(4A):2620–2648,
2019.
F Alexander Wolf, Philipp Angerer, and Fabian J Theis.
Scanpy: large-scale single-cell gene expression data anal-
ysis. Genome biology, 19(1):1–5, 2018.
Hui Zou and Trevor Hastie. Regularization and variable
selection via the elastic net. Journal of the royal statistical
society: series B (statistical methodology), 67(2):301–
320, 2005.

