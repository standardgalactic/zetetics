Consistency driven Sequential Transformers Attention Model
for Partially Observable Scenes
Samrudhdhi B. Rangreja, Chetan L. Srinidhib, James J. Clarka
aMcGill University, Canada. bSunnybrook Research Institute, University of Toronto, Canada.
samrudhdhi.rangrej@mail.mcgill.ca, chetan.srinidhi@utoronto.ca, james.clark1@mcgill.ca
Abstract
Most hard attention models initially observe a complete
scene to locate and sense informative glimpses, and pre-
dict class-label of a scene based on glimpses. However, in
many applications (e.g., aerial imaging), observing an en-
tire scene is not always feasible due to the limited time and
resources available for acquisition. In this paper, we de-
velop a Sequential Transformers Attention Model (STAM)
that only partially observes a complete image and pre-
dicts informative glimpse locations solely based on past
glimpses. We design our agent using DeiT-distilled [44]
and train it with a one-step actor-critic algorithm. Further-
more, to improve classification performance, we introduce
a novel training objective, which enforces consistency be-
tween the class distribution predicted by a teacher model
from a complete image and the class distribution predicted
by our agent using glimpses. When the agent senses only
4% of the total image area, the inclusion of the proposed
consistency loss in our training objective yields 3% and 8%
higher accuracy on ImageNet and fMoW datasets, respec-
tively. Moreover, our agent outperforms previous state-of-
the-art by observing nearly 27% and 42% fewer pixels in
glimpses on ImageNet and fMoW.
1. Introduction
High-performing image classification models such as Ef-
ficientNet [42], ResNet [15] and Vision Transformers (ViT)
[13] assume that a complete scene (or image) is available for
recognition. However, in many practical scenarios, a com-
plete image is not always available at once. For instance,
an autonomous agent may acquire an image only partially
and through a series of narrow observations. The reasons
may include a small field of view, high acquisition cost, lim-
ited time for acquisition, or limited bandwidth between the
sensor and the computational unit. Often, an agent would
have partially acquired an image, and the system must per-
form recognition based on incomplete information. Models
trained on complete images prove inefficient while classify-
Figure 1. Schematic diagram of Sequential Transformers Atten-
tion Model (STAM). We divide an image (X) into equally-sized
non-overlapping glimpses. STAM sequentially observes informa-
tive glimpses (gt) from an image. While never observing an image
entirely, STAM predicts the class-label of an image (y) based on
glimpses. At each t, our agent encodes past glimpses and their
locations (g0:t, l0:t) into a Markov state st. It uses state st to pre-
dict class distribution p(yt|st) and attention policy π(lt+1|st). We
sample the next glimpse location lt+1 from π(lt+1|st).
ing incomplete images. For example, the accuracy of DeiT-
Small [44] drops by around 10% when 50% of the image re-
gions are unavailable [25]. Moreover, they cannot perform
autonomous sensing.
Many developed autonomous agents that acquire a series
of most informative sub-regions from a scene to perform
classification from partial observations [5,14,24,27]. Most
existing scalable approaches [14, 27, 45] initially glance at
an entire scene to locate the informative sub-regions. How-
ever, in practice, glancing at an entire scene is not always
feasible. Examples include time-sensitive rescue operations
using aerial imagery, an autonomous car driving in a new
territory, and a medical expert probing a tissue to find ab-
normalities. We develop an autonomous agent that predicts
locations of the most informative regions, called glimpses,
without observing the entire scene initially. Starting from
observing a glimpse at a random location, the autonomous
agent decides which location to attend next solely based on
the partial observations made so far.
We design our autonomous agent using a transformer ar-
chitecture [13, 44, 47] and call it Sequential Transformers
Attention Model (STAM). Transformers efficiently model
long-range dependencies and are ideal for aggregating in-
formation from distant glimpses. At any given time, our
2518

agent predicts an optimal location for the next glimpse
and class-label of an image based on the glimpses col-
lected so far. As glimpse acquisition is a discrete and non-
differentiable process, we train our agent using reinforce-
ment learning (RL). Further, we propose an additional train-
ing objective where the agent is required to predict a class
distribution from a set of glimpses consistent with the class
distribution predicted from a complete image. To do so, we
employ a teacher transformers model to predict the class
distribution from a complete image and our agent (a student
model) tries to reproduce this distribution using partial ob-
servations. We perform experiments on two large-scale real-
world datasets, namely, ImageNet [34] and fMoW [10].
Our main contributions are as follows.
• We develop a transformers-based RL agent called
STAM, which actively senses glimpses from a scene
and predicts class-label based on partial observations.
Instead of locating informative glimpses by observ-
ing an entire image, our agent sequentially predicts the
next most informative glimpse location based on past
glimpses.
• We propose a consistency-based training objective,
where the agent must predict a class distribution con-
sistent with the complete image using only partial ob-
servations. With only 4% of the total image area ob-
served, our proposed objective yields ∼3% and ∼8%
gain in accuracy on ImageNet and fMoW, respectively.
• Our agent that never observes a complete image out-
performs previous methods that initially glance at an
entire image to locate informative glimpses. It starts
exceeding the previous state-of-the-art while sensing
27% and 42% fewer pixels in glimpses on ImageNet
and fMoW, respectively.
2. Related Works
Hard Attention. As opposed to soft attention [52], which
attends to all regions of an image but with a varying de-
gree of importance, hard attention [24] sequentially attends
to only the most informative subregions in an image. Hard
attention was first introduced by Minh et al. [24] and later
studied by many others. Different techniques are used for
hard attention such as expectation maximization [30], ma-
jority voting [1], wake-sleep algorithm [6], sampling from
self-attention or certainty maps [37,38], and Bayesian opti-
mal experiment design [29]. The most successful hard atten-
tion models learn to sample glimpses using policy gradient
RL algorithms [5,14,24,27,48,52].
Most previous hard attention models initially glance at
a complete image to locate the most informative glimpses.
For instance, Xu et al. [52] and Saccader [14] analyze the
complete image at original resolution; whereas DRAM [6],
TNet [27] and GFNet [48] observes the complete image
at low resolution. Furthermore, TNet [27] and GFNet [48]
uses the low resolution gist of an image to predict the class-
label. In contrast, our model does not look at the entire im-
age at low resolution or otherwise. We predict the attention-
worthy glimpse locations and the class of the complete im-
age solely based on partial observations. From this perspec-
tive, RAM [24], which also operates under partial observ-
ability, is the closest related approach. While RAM could
not scale beyond MNIST dataset [21], our approach scales
to large-scale real-world datasets.
Patch Selection. Many approaches observe an entire image
to select all informative sub-regions at once. For example,
region proposal networks [33], top-K patch selection [2,11],
multiple-instance learning [18], attention sampling [19] and
PatchDrop [45]. Unlike these approaches, our model does
not observe the entire image, and it predicts the location of
informative sub-regions sequentially. Among vision trans-
formers, methods such as PS-ViT [54], Dynamic-ViT [31]
and IA-RED2 [26] start with observing a complete image
and progressively (re-)sample most discriminative patches
from each successive transformer block. In contrast, we
sample and input only informative patches to the transform-
ers. Moreover, our model is sequential in nature, sensing
only one additional patch at each step.
Consistency Learning. The idea of consistency learning
was initially proposed by Sajjadi et al. [36] and has become
an important component in many recent semi-supervised
learning (SSL) algorithms [8,20,39,50]. Consistency learn-
ing acts as a regularizer that enforces the model output
to be invariant to the perturbations in the input image
[8, 36, 39, 50] or hidden state [7, 20] or model parameters
[17, 40]. The consistency is achieved by using the predic-
tions made from one perturbation as pseudo-targets for the
predictions made from another perturbation.
Another closely related idea in SSL is the pseudo-
labeling [4, 22, 28], where a trained model, known as
‘teacher model’, is used to generate soft (continuous dis-
tributions) or hard (one-hot distributions) pseudo-labels for
the unlabeled data. These pseudo-labels are later used as
targets while training a student model with unlabeled exam-
ples [9, 51]. This approach is closely linked to Knowledge
Distillation [9,16], where the student is trained to reproduce
the teacher output.
In this work, we develop a consistency training objective
based on these concepts. We train our agent to be invariant
to a specific type of input perturbations, i.e., the partial and
the complete observations. Furthermore, we use a teacher
model to produce soft pseudo-labels from a complete image
and use them as targets while training our agent (a student
model) using partial observations.
2519

Step 1:                   Step 2:                  Step 3:
Figure 2. An overview of our Sequential Transformers Attention Model (STAM). The STAM consists of a core T , classifiers G and
D, an actor A, and a critic C (only used during training). We discuss the working principles of these modules in Section 3.1, except for
the critic C which we discuss in Section 4. We update model parameters T times per batch using the objectives shown in the right and
discussed in Section 4. Each training iteration consists of three steps: Step 1 (green path): Given a complete image X, the teacher model
predicts a soft pseudo-label q(y|X). Step 2 (blue path): Given glimpses g0:t, the core T predicts features f g
t and f d
t . The classifiers G
and D predict class distributions pg(yt|f g
t ) and pd(yt|f d
t ) from features f g
t and f d
t , respectively. Given a state st = [f g
t ; f d
t ], the critic C
predicts value V (st) and the actor A predicts attention policy π(lt+1|st). The actor predicts logits π′((i, j)|st) for all unobserved glimpse
locations (i, j) in a conditionally independent manner and applies softmax to the logits resulting in π(lt+1|st). Step 3 (orange path): A
glimpse gt+1 at lt+1 ∼π(lt+1|st) is sensed. Using the glimpses g0:t+1, the ensemble class distribution p(yt+1|st+1) and value V (st+1)
are computed following the same path as Step 2. The model parameters are updated using the gradients from Step 2. In practice, Step 1 is
performed once per batch at t = 0, whereas, Steps 2-3 are performed T times per batch.
3. Sequential Transformers Attention Model
(STAM)
Given an unobserved scene X, the agent actively cap-
tures a series of non-overlapping glimpses and, while never
observing X completely, it predicts the class-label y of X
based on glimpses. A schematic diagram of our agent is
shown in Figure 1. At time t, the agent senses a glimpse gt at
location lt from an image X. Using the glimpses observed
up to time t, our agent predicts: i) yt, an approximation of
label y, and ii) lt+1, the location of the next glimpse.
We model the sequential attention mechanism of our
agent as a Partially Observable Markov Decision Process
(POMDP). In the POMDP, our agent encodes a history
of partial observations, {(gt′, lt′)| t′ ∈{0, . . . , t}}, in a
Markov state st and maps it to: i) a class distribution
p(yt|st) and ii) an attention policy π(lt+1|st) – a distribu-
tion over candidate glimpse locations for t + 1.
3.1. Model of Our Agent
We build our agent using DeiT-distilled [44], referred to
as DeiTDin the rest of the paper. Briefly, DeiTDis a type
of ViT trained using knowledge distillation. The transform-
ers in DeiTDtransform an input sequence of a class token,
a distillation token, and patch tokens (linear projections of
the image patches added to the positional embeddings) to
an output sequence; with the outputs corresponding to the
class and the distillation tokens, the two classifiers predict
the ground truth and the teacher’s prediction, respectively.
In our approach, we adapt the DeiTDto predict labels from
glimpses and use the distillation token to impose consis-
tency. Figure 2 shows the model of our agent. Our agent is
composed of the following components.
Sensor. We consider a sensor that captures non-overlapping
glimpses from a scene. To model this sensor, we divide the
image X into N ×N equally sized non-overlapping blocks,
X = {X(i, j)| i, j ∈{1, . . . , N}}. Given a location lt =
(i, j), a sensor senses a glimpse gt = X(i, j), as shown in
Figure 1.
Core (T ). At time t, we extract M × M patches from
each glimpse observed up to t, forming a set of t × M ×
M patches. We feed these patches, the positional embed-
dings, the class token, and the distillation token to the
DeiTDmodel. The positional embedding represents the po-
sition of a patch in an image. We derive the position of a
patch from the location of a parent glimpse in an image.
2520

Algorithm 1 Inference using STAM
1: Initialize l0 randomly;
2: for t ∈{0, . . . , T −1} do
3:
Sample gt at lt from an image
\triangleright Sensor
4:
f g
t , f d
t = T (g0:t, l0:t); st = [f g
t ; f d
t ]
\triangleright Core
5:
pg(yt|·) = G(f g
t );
pd(yt|·) = D(f d
t )
\triangleright Classifiers
6:
π′(l′|·) = A(st, l′), ∀l′ ∈{{1, .., N}2 −l0:t} \triangleright Actor
7:
yt = argmax(pg(yt|·) + pd(yt|·))
8:
lt+1 = argmax(π′(l′|·))
9: end for
Among the outputs of the final transformer block, let us de-
fine the ones corresponding to the class token as f g
t and the
distillation token as f d
t . We then form a Markov state st by
concatenating f g
t and f d
t , which an actor module will later
use to predict an attention policy.
Classifiers (G and D). As in DeiTD, we use two linear
classifiers to predict two class distributions pg(yt|f g
t ) and
pd(yt|f d
t ) from f g
t and f d
t , respectively. We treat the pre-
dicted distributions independently during training and av-
erage them to form an ensemble distribution during infer-
ence [44]:
  p(y_t| s _
t)=\frac {
1 } { 2}(p_g( y
_ t|f^g_t) + p_d(y_t|f^d_t)). \label {eq:ensemble}
(1)
Actor (A). An actor MLP predicts attention policy
π(lt+1|st). The distribution π(lt+1|st) is computed by ap-
plying softmax over logits {π′((i, j)|st)}, where (i, j)s are
unobserved glimpse locations. An actor predicts π′((i, j)|·)
for each (i, j) in a conditionally independent manner [3,43].
For any (i, j), the actor accepts a concatenation of glimpse
location embeddings e(i, j) and a Markov state st, and out-
puts π′((i, j)|st). Here, e(i, j)s are learnable embeddings
initialized by interpolating positional embeddings of a pre-
trained DeiTD. We use lt+1 ∼π(lt+1|st) during training
and lt+1 = argmax(π(lt+1|st)) during inference.
We provide the inference steps in Algorithm 1.
4. Training Objectives
We train the parameters of the core (θT ), the classifiers
(θG and θD) and the actor (θA) using the training objec-
tives discussed next. Figure 2 illustrates the training steps
of our model, and Algorithm 1 in the supplementary mate-
rial presents the corresponding pseudocode.
4.1. Learning Classification
Our agent predicts two class distributions based on input
glimpses, namely, pg and pd; where, pg is an estimation of
the ground truth class distribution associated with a com-
plete image and pd is an approximation of the class distri-
bution predicted by a teacher model from a complete image.
We learn pg and pd using the following two objectives:
Supervised Loss. As our goal is to predict y from partial
observations, we learn the parameters {θT , θG} by minimiz-
ing a cross-entropy between pg(yt|st) and δ(y|X) given by
  \m a t
h
cal {L }_{sup} = - \sum \delta (y|X) \log (p_g(y_t|s_t)), \label {eq:lce}
(2)
where, δ(y|X) is a delta distribution indicating the ground
truth label of a complete image.
Consistency Loss. To improve the performance of our
agent, we enforce that the predictions made from the
glimpses are consistent with the predictions made from a
complete image. Furthermore, the above predictions should
also be the same irrespective of the number and loca-
tion of the glimpses observed so far. Ideally, for each t,
we require our agent to produce pd(yt|st) that minimize
KL[pd(yt|st)||p(y|X)]; where p(y|X) is the agent’s predic-
tion after observing all glimpses from an image.
The direct optimization of the above KL divergence is
difficult as the target p(y|X) keeps shifting during train-
ing. To circumvent this issue, we rely on a separate teacher
model to provide a stable target. Our teacher model predicts
the class distribution q(y|X) from a complete image; where
q(y|X) is commonly referred to as soft pseudo-label for X
in the literature [16,51]. The resultant consistency objective
to train {θT , θD} is given by
  \mathc a l {L}_{consist} = \textbf {KL}[p_d(y_t|s_t)||q(y|X)]. \label {eq:lconsistency}
(3)
4.2. Learning Attention Policy
We consider attention to be a POMDP. After observ-
ing a glimpse at location lt+1 ∼π(lt+1|st), we award our
agent a reward Rt+1 indicating the utility of the observed
glimpse. Our training objective is to learn π(lt+1|st) that
maximizes the sum of future rewards, also known as re-
turn, Gt = PT
t′=t+1(R′
t). A majority of the existing works
[5, 14, 24, 27] use REINFORCE algorithm [49] to learn an
attention policy. These methods run an agent for t = 0 to
T −1 steps to achieve R1 to RT and compute G0 to GT −1.
At the end, the parameters of the agent are updated once to
maximize the returns. Due to the quadratic complexity of
the transformers, running our agent for T steps and updat-
ing the parameters just once at the end is expensive. Instead,
we adopt one-step actor-critic algorithm [41] to update the
parameters at each time step.
Critic loss. To train our agent using the one-step actor-critic
algorithm, we introduce a critic MLP (C) with parameters
υ. A critic learns a value function V (st) that estimates the
expected return given the current state of the agent, i.e.,
Eπ[Gt]. As Eπ[Gt] = Eπ[Rt+1 + Gt+1], V (st) should be
equal to Eπ[Rt+1 + V (st+1)]. Hence, the critic parameters
υ are learned by minimizing the difference between the two
quantities. In practice, we estimate the expectation with re-
spect to π using a single Monte-Carlo sample, yielding
  \math c al {L}_ { criti c }  = ||V(s_t) - (R_{t+1}+V(s_{t+1}))||. \label {eq:lcritic}
(4)
2521

We run our agent for one additional time step to compute
V (st+1). Note that the quantity (Rt+1 + V (st+1)) acts as a
target and does not contribute to the parameter update. We
use the critic MLP only during training and discard it once
the training is over.
Actor loss. The goal of an agent is to learn a policy that
achieves the maximum return. When the agent achieves
lower than the expected return by sensing a glimpse at loca-
tion lt+1, π(lt+1|st) must reduce proportional to the deficit.
In other words, π(lt+1|st) must reduce by the factor of
(V (st) −(Rt+1 + V (st+1)); where V (st) is an estimation
of the expected return for st, and (Rt+1 + V (st+1)) is the
estimation of the expected return following glimpse at lt+1.
We optimize the parameters {θT , θA} by minimizing
  \mat h cal {L}_{actor} =  \lo g  (\pi  ( l_{t+1}|s_t)) (V(s_t) - (R_{t+1}+V(s_{t+1}))). \label {eq:lactor}
(5)
Note that (V (st) −(Rt+1 + V (st+1))) acts as a scaling
factor and does not contribute to the parameter update.
Reward. We use a reward that incentivizes the agent to pre-
dict yt that is consistent with the label predicted by the
teacher model based on a complete image. Our reward is
  R _t = -\textbf {KL}[p(y_t|s_t)||q(y|X)], \label {eq:reward}
(6)
where p(yt|st) is computed using Equation 1. We expect the
accuracy of the predictions made from a complete image to
provide an upper bound for the accuracy of the predictions
made from partial observations. The above reward encour-
ages the agent to reach for the upper bound.
Our overall final training objective is as follows.
  \
mathca l  {L} = \f r ac {1}{ 2 }(\mathcal {L}_{sup} + \mathcal {L}_{consist}) + (\mathcal {L}_{actor} + \mathcal {L}_{critic}) \label {eq:lfinal}
(7)
5. Experiment Setup
Datasets. We experiment with two large-scale real-world
datasets, namely, ImageNet [34] and fMoW [10]. ImageNet
consists of natural images from 1000 categories. It includes
∼1.3M training images and 50K validation images. We re-
size the images to size 224×224. The fMoW contains satel-
lite images from 62 categories. It holds ∼0.36M, ∼53K, and
∼64K images for training, validation, and test, respectively.
We crop the images based on the bounding boxes provided
with the dataset and resize the cropped images to 224×224.
Unless stated otherwise, we implement and optimize STAM
with the same default setting on both datasets.
Implementation.1
We
divide
the
images
into
non-
overlapping glimpses of size 32 × 32, yielding a 7 × 7 grid
1Our
code
is
available
at:
https : / / github . com /
samrudhdhirangrej / STAM - Sequential - Transformers -
Attention-Model
of glimpses. As required by DeiTD, we further divide each
glimpse into four non-overlapping patches of size 16 × 16.
We use DeiTD-Small architecture for our agent unless
stated otherwise. The actor and the critic MLPs are of the
form {3×{FC-BN-ReLU}-FC} with hidden dimensions of
2048 and 512, respectively. We initialize the core and the
classifiers using a pretrained DeiTD2 and initialize the ac-
tor and the critic at random. We normalize the logits π′(·)
with l2 norm for training stability and multiply them with
τ before applying a softmax. We stop the gradients from
the critic to our agent. We normalize the rewards to have
zero mean and unit variance in each training iteration. The
magnitude of the value V (·) varies from one time-step to the
next, as it approximates the expected sum of future rewards.
To learn V (·) of varying magnitude, we apply PopArt-style
normalization [46] to the predicted values.
We use DeiTDand DeiT as teacher models for ImageNet
and fMoW, respectively. For the ImageNet teacher model,
we use publicly available weights. The teacher model for
fMoW is first initialized with the DeiT model pretrained on
ImageNet, followed by fine-tuning on the fMoW dataset for
100 epochs using the default hyperparameter setting from
[44] with an additional vertical flip augmentation.
Optimization. Our agent runs for T = 21 time-steps per
image, capturing one glimpse at a time. We update the
model parameters T times per batch, once after each time
step. To account for T updates per batch, we allow the agent
to see only 1/T th of the data during one epoch. We train
our agents with the batch size (B) of 4096 for 200 epochs
on ImageNet and B of 600 for 400 epochs on fMoW. The
hyperparameter τ is increased linearly from 1 to 4 for the
first 100 epochs and fixed to 4 for the remaining training.
We augment the training images using the Rand-
Augment scheme [12] and follow the same setting as Tou-
vron et al. [44]. Additionally, for fMoW, we also use ran-
dom vertical flip augmentation. We train our agents using
an AdamW optimizer [23] with a weight decay of 0.05. We
adapt a cosine learning schedule with an initial learning rate
of lrbase×B/512 and a minimum learning rate of 1e-6. The
base learning rate lrbase is set to 1e-3 for the critic module.
For the remaining modules, lrbase is set to 1e-6 for Ima-
geNet and 1e-5 for fMoW. We train our agents on four V100
GPUs in less than a day, using 32GB memory per GPU for
ImageNet and 16GB for fMoW.
6. Results
6.1. Comparison with Baseline Attention Policies
We compare the policy learned by our agent with three
baseline policies, namely, the Random, the Plus, and the
Spiral. The Random agent selects the next glimpse ran-
2https://github.com/facebookresearch/deit
2522

0 2 4 6 8 10 12 14 16 18 20
Time t
20
40
60
80
Accuracy (%)
Random
Plus
Spiral
STAM (Ours)
4
5
6
7
8
9
50
55
60
65
(a)
0 2 4 6 8 10 12 14 16 18 20
Time t
30
40
50
60
70
Accuracy (%)
Random
Plus
Spiral
STAM (Ours)
(b)
Figure 3. Baseline comparison of various attention policies. (a)
ImageNet; (b) fMoW. The Random selects glimpses in random
order. The Plus and the Spiral select glimpses in the order shown in
Figure 4 (c). Starting from a random glimpse location, our STAM
uses an RL agent to predict next glimpse location. Results for the
Random and STAM are presented as mean±5×std computed from
ten independent runs.
domly from a set of unobserved glimpses. In contrast, the
Plus and the Spiral agents account for the object-centric
nature of vision datasets and select glimpses in the order
shown in Figure 4 (c). For a fair comparison, all baseline
agents begin with the first glimpse at a random location.
The model architecture of the baseline agents is similar to
our proposed agent, except that the baseline agents do not
have an actor module. We train the baseline agents follow-
ing the same procedure as our agent using the losses from
Equation 2 and 3.
Results are shown in Figure 3. Among the three base-
lines, the Spiral and the Plus agents outperform the Random
agent. For t ≥8, the Plus achieves higher accuracy than
the Spiral on ImageNet, whereas, on fMoW, the Spiral out-
performs the Plus. This inconsistent behavior is mainly due
to the different orientations of objects in the two datasets.
While the objects are mainly aligned vertically or horizon-
tally in ImageNet, the landmarks in fMoW have no spe-
cific orientation. Finally, our agent outperforms all base-
line agents across the two datasets both at initial (t < 8)
and later (t ≥8) time-steps. At t = 8, it achieves 1.8%
higher accuracy on ImageNet and 2.3% higher accuracy on
fMoW than the top-performing baselines for the respective
datasets.
6.2. Analysis of Consistency Loss
To quantify the gain achieved with a consistency loss
from Equation 3, we compare our agents trained with and
without this loss. For a fair comparison, when training our
agent without the consistency loss, we evaluate the cross-
entropy loss between the ensemble distribution p(yt|st)
from Equation 1 and the ground truth. The remaining train-
ing setup is the same for both agents.
0 2 4 6 8 10 12 14 16 18 20
Time t
0.0
0.5
1.0
1.5
2.0
2.5
3.0
Gain in Accuracy (%)
Lconsist with hard
pseudo-labels
Lconsist with soft
pseudo-labels (Ours)
(a)
0 2 4 6 8 10 12 14 16 18 20
Time t
0
1
2
3
4
5
6
7
8
Gain in Accuracy (%)
Lconsist with hard
pseudo-labels
Lconsist with soft
pseudo-labels (Ours)
(b)
(c)
Figure 4. The gain in the accuracy of STAM when trained by in-
cluding the consistency loss (with soft vs hard pseudo-labels) over
excluding it from the training objectives. (a) ImageNet; (b) fMoW.
Results are presented as mean ± std computed using ten different
runs. (c) Given an image, (top) the Spiral and (bottom) the Plus
baselines select glimpses in the order shown.
In Figure 4 (blue curve), we display the difference in the
accuracy of STAM when trained by including and excluding
the consistency loss in the training objectives (Equation 7).
With only two glimpses (i.e., one random and one selected
by the agent at t = 1), the proposed consistency loss results
in significant improvement in accuracy with a gain of ∼3%
on ImageNet and ∼8% on fMoW datasets.
To benchmark the improvement offered by our pro-
posed consistency loss, we study the effect of an alterna-
tive consistency loss using hard pseudo-labels, Lconsist =
−P δ(ˆy|X) log(pd(yt|st)) where ˆy are the hard pseudo-
labels predicted by the teacher model from a complete im-
age, i.e., ˆy = argmax(q(y|X)), and δ(ˆy|X) is a delta dis-
tribution. The results are shown in Figure 4 (purple curve).
An agent trained using a consistency loss with hard pseudo-
labels achieves a gain of ∼1.5% on ImageNet and ∼3.5%
on fMoW for the first two glimpses.
Overall, the consistency loss improves the performance
of STAM. The gain in accuracy with consistency loss
evaluated using soft pseudo-labels is greater than the hard
pseudo-labels. In the supplementary material (Section 1.1),
we demonstrate that the consistency loss also improves
the performance of the Random, the Plus, and the Spiral
agents. There we observe that the gain in performance is
higher for STAM over the baseline agents.
Additional results in the supplementary material (SM).
In Section 1.2 of SM, we show that when the area observed
in an image is < 20%, STAM achieves higher accuracy
with smaller glimpses than the larger ones. Also, in Section
1.3 of SM, we show that an increase in model capacity can
improve the performance of STAM. In Section 1.4 of SM,
we show that longer training on ImageNet also improves
STAM’s performance further.
2523

(a)
(b)
Figure 5. Visualization of glimpses selected by STAM on example images from t = 0 to 15. (a) ImageNet; (b) fMoW. Complete images
are shown for reference only. STAM does not observe a complete image. Zoom-in to observe details.
6.3. Glimpse Visualization
In Figure 5, we display the glimpses selected by STAM
and the predicted labels on example images from ImageNet
and fMoW. In the ImageNet example, STAM locates and
identifies the umbrella at t = 14. In the fMoW example,
STAM locates the transmission line and identifies the pow-
erplant at t = 8. Note that, while not observing a complete
image, STAM predicts the location of informative glimpses
solely based on past glimpses. For more examples, refer to
Figures 5-6 in the supplementary material.
In Figure 6, we display the histograms of glimpse loca-
tions selected by our agent with increasing t. At t = 0, the
agent observes a glimpse at a random location, and at t = 1,
the agent learns to observe mainly a glimpse centered on
an image, perhaps due to the object-centric nature of the
dataset. For the subsequent glimpses, the agent prefers to
attend vertically and horizontally centered glimpses in Im-
ageNet. While for fMoW, it attends to glimpses with mini-
mum distance from the center. Note in ImageNet the object
of interest frequently appears at the center and is aligned
vertically or horizontally; whereas in fMoW, the object of
interest appears at the center but with no specific orienta-
tion. With time, the agent attends to different locations away
from the center based on the content observed through pre-
vious glimpses as shown in Figure 5.
(a)
(b)
Figure 6. Histograms of glimpse locations sensed by STAM on (a)
ImageNet and (b) fMoW. The first, second, and third rows of each
panel display histograms for t = 0 to 6, 7 to 13, and 14 to 20. At
t = 0, STAM observes a glimpse at a random location. At t > 0,
STAM senses glimpses at the locations predicted by an RL agent.
6.4. State-of-the-Art Comparison
A fair comparison between our method and the pre-
vious works is challenging due to the following reasons.
Most previous works observe an entire image, occasionally
at low resolution, to locate the most informative glimpses
[5,14,27,45,48]. Furthermore, if they observe an entire im-
age at low resolution, they optionally use the image along
with the glimpses to predict the class-label [27, 45, 48]. In
contrast, our agent operates only under partial observability.
We present the state-of-the-art comparison in Table 1 and
indicate which method uses an entire image and for what
reasons. As different methods use different glimpse sizes,
we compare them based on the number of pixels sensed per
image for classification. If the method senses a complete
image but does not use it for classification [5,14], we do not
2524

ImageNet
fMoW
Method
Note
Is complete image used for
#pixels for
Accuracy
#pixels for
Accuracy
Attention?
Classification?
classification
(%)
classification
(%)
DRAM [5]
results from [27]
Yes
No
47.4K
67.50
—
—
GFNet [48]
Yes
Yes
46.1K
75.93
—
—
Saccader [14]
results from [27]
Yes
No
35.6K
70.31
—
—
TNet [27]
Yes
Yes
35.6K
74.62
—†
—
STN [32,45]
developed in [45] based on [32]
Yes
Yes
28.2K
71.40
22.0K
64.8
PatchDrop [45]
Yes
Yes
27.9K
76.00
19.4K
68.3
STAM (DeiTD-Small (default))
o
⋆equivalent
No
No
20.5K(t = 19)
76.35
11.3K(t = 10)
68.8
STAM (DeiTD-Base)
accuracy to [45]
No
No
14.3K(t = 13)
76.13
—
—
STAM (DeiTD-Small (default))
o
⋄equivalent
No
No
27.7K(t = 26)
78.25
19.5K(t = 18)
71.5
STAM (DeiTD-Base)
sensing to [45]
No
No
27.7K(t = 26)
80.78
—
—
Table 1. State-of-the-Art comparison. We report the number of pixels sensed per image for classification and the resultant accuracy. If a
method uses a low-resolution gist of a complete image for classification, we include the pixels of the gist in the above count. Our results
are presented as an average computed over ten runs. We present our results at two different time steps, (⋆first two rows) when the accuracy
achieved by our method is equivalent to PatchDrop, and (⋄last two rows) when the number of pixels sensed by our agent for classification
is equivalent to PatchDrop. †TNet [27] use 896 × 896 resolution images; hence, we do not include their results in the above comparison.
include the pixels of the complete image in the above count.
To achieve the same level of accuracy as PatchDrop [45]
(the best performing method), our default agent observes
7.4K fewer pixels per image from ImageNet and 8.1K fewer
pixels per image from fMoW. Moreover, while observing a
similar number of pixels as PatchDrop, our default agent
achieves 2.25% higher accuracy on ImageNet and 3.2%
higher accuracy on fMoW. We also train our agent with
DeiTD-Base as the core module on ImageNet. This agent
requires 13.6K fewer pixels per image to achieve the same
accuracy as PatchDrop, and achieves 4.78% higher accu-
racy than PatchDrop while sensing the same number of pix-
els per image. We emphasize that our agent does not ob-
serve a complete image to locate informative glimpses or to
perform classification, whereas PatchDrop does.
6.5. Early Termination
In practice, we can save time and resources by termi-
nating sensing when the agent confidently concludes a class
for the image. To this end, we devise a simple mechanism to
decide when to stop sensing. Let us define a scoring func-
tion based on the probability of the predicted class, St =
max(p(yt|st)). The agent stops sensing more glimpses if St
is greater than threshold γ. In Figure 7, we show the average
number of glimpses observed per image vs. the accuracy of
our agent for γ varying from 0 to 1. Remarkably, with γ =
0.5, STAM suspends sensing on ImageNet after observing
on an average 7.5 glimpses per image and achieves 66.47%
accuracy. Similarly, on fMoW with γ = 0.5, STAM sus-
pends sensing after observing on an average 8.7 glimpses
per image and achieves 65.71% accuracy.
7. Discussion and Conclusions
We introduced a novel Sequential Transformers Atten-
tion Model (STAM) that progressively observes a scene
only partially using glimpses to predict its label. It pre-
0
20
40
Avg number of glimpses
20
40
60
80
Accuracy (%)
=0.0
=0.1
=0.2
=0.3
=0.4
=0.5
=0.6
=0.7
=0.8
=0.9
=1.0
(a)
0
20
40
Avg number of glimpses
30
40
50
60
70
Accuracy (%)
=0.0
=0.1
=0.2
=0.3
=0.4
=0.5
=0.6
=0.7 =0.8
=0.9
=1.0
(b)
Figure 7. Average number of glimpses observed per image vs
the accuracy achieved by STAM on (a) ImageNet and (b) fMoW
datasets with early termination scheme. STAM suspends sensing
once the probability of predicted class is higher than threshold γ.
dicts future informative glimpse locations solely based on
past glimpses. STAM is applicable in scenarios where a
complete image is not observable due to reasons including
a small field of view or limited time and resources, e.g.,
aerial imaging. We trained STAM using a one-step actor-
critic algorithm; and proposed a novel consistency training
objective which further improves its accuracy by 3% on Im-
ageNet and 8% on fMoW with only two glimpses.
While never sensing a complete image, our agent out-
performs the previous state-of-the-art [45] that observes an
entire image by sensing nearly 27% and 42% fewer pixels in
glimpses per image on ImageNet and fMoW, respectively.
Finally, to save the inference time and resources, we de-
vise a simple scheme to terminate sensing when STAM has
predicted a label with sufficient confidence. With a confi-
dence score > 0.5, STAM correctly classifies nearly 65%
images on both datasets by observing on an average < 9
glimpses per image (i.e., < 18% of the total image area).
However, STAM is limited by its quadratic computational
cost. One way to overcome this limitation is to replace
DeiTDwith sparse transformers [26, 31]. Lastly, note that
while we focus on static images only, STAM could also
potentially be applied to dynamic scenes for early recog-
nition [35,53].
2525

References
[1] Bogdan Alexe, Nicolas Heess, Yee W Teh, and Vittorio Fer-
rari. Searching for objects driven by context. In Advances
in Neural Information Processing Systems, pages 881–889,
2012. 2
[2] Baptiste Angles, Simon Kornblith, Shahram Izadi, An-
drea Tagliasacchi, and Kwang Moo Yi.
Mist: Multi-
ple instance spatial transformer network.
arXiv preprint
arXiv:1811.10725, 2018. 2
[3] Ivan Anokhin, Kirill Demochkin, Taras Khakhulin, Gleb
Sterkin, Victor Lempitsky, and Denis Korzhenkov. Image
generators with conditionally-independent pixel synthesis.
In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pages 14278–14287, 2021. 4
[4] Eric Arazo, Diego Ortego, Paul Albert, Noel E O’Connor,
and Kevin McGuinness. Pseudo-labeling and confirmation
bias in deep semi-supervised learning. In 2020 International
Joint Conference on Neural Networks (IJCNN), pages 1–8,
2020. 2
[5] Jimmy Ba, Volodymyr Mnih, and Koray Kavukcuoglu. Mul-
tiple object recognition with visual attention. In ICLR, 2015.
1, 2, 4, 7, 8
[6] Jimmy Ba, Russ R Salakhutdinov, Roger B Grosse, and
Brendan J Frey.
Learning wake-sleep recurrent attention
models. In Advances in Neural Information Processing Sys-
tems, pages 2593–2601, 2015. 2
[7] Philip Bachman, Ouais Alsharif, and Doina Precup. Learn-
ing with pseudo-ensembles. Advances in Neural Information
Processing Systems, 27:3365–3373, 2014. 2
[8] David Berthelot, Nicholas Carlini, Ekin D Cubuk, Alex
Kurakin, Kihyuk Sohn, Han Zhang, and Colin Raffel.
Remixmatch: Semi-supervised learning with distribution
alignment and augmentation anchoring.
arXiv preprint
arXiv:1911.09785, 2019. 2
[9] Lucas Beyer, Xiaohua Zhai, Am´elie Royer, Larisa Markeeva,
Rohan Anil, and Alexander Kolesnikov. Knowledge distilla-
tion: A good teacher is patient and consistent. arXiv preprint
arXiv:2106.05237, 2021. 2
[10] Gordon Christie, Neil Fendley, James Wilson, and Ryan
Mukherjee. Functional map of the world. In Proceedings
of the IEEE Conference on Computer Vision and Pattern
Recognition, pages 6172–6180, 2018. 2, 5
[11] Jean-Baptiste Cordonnier, Aravindh Mahendran, Alexey
Dosovitskiy,
Dirk
Weissenborn,
Jakob
Uszkoreit,
and
Thomas Unterthiner. Differentiable patch selection for im-
age recognition. arXiv preprint arXiv:2104.03059, 2021. 2
[12] Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V
Le. Randaugment: Practical automated data augmentation
with a reduced search space. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition
Workshops, pages 702–703, 2020. 5
[13] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-
vain Gelly, et al. An image is worth 16x16 words: Trans-
formers for image recognition at scale.
arXiv preprint
arXiv:2010.11929, 2020. 1
[14] Gamaleldin Elsayed, Simon Kornblith, and Quoc V Le. Sac-
cader: improving accuracy of hard attention models for vi-
sion. In Advances in Neural Information Processing Systems,
pages 702–714, 2019. 1, 2, 4, 7, 8
[15] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In Proceed-
ings of the IEEE Conference on Computer Vision and Pattern
Recognition, pages 770–778, 2016. 1
[16] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean.
Distill-
ing the knowledge in a neural network.
arXiv preprint
arXiv:1503.02531, 2015. 2, 4
[17] Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q
Weinberger. Deep networks with stochastic depth. In Euro-
pean Conference on Computer Vision, pages 646–661, 2016.
2
[18] Maximilian
Ilse,
Jakub
Tomczak,
and
Max
Welling.
Attention-based deep multiple instance learning. In Interna-
tional Conference on Machine Learning, pages 2127–2136.
PMLR, 2018. 2
[19] Angelos Katharopoulos and Francois Fleuret.
Process-
ing megapixel images with deep attention-sampling mod-
els. In International Conference on Machine Learning, pages
3282–3291, 2019. 2
[20] Samuli Laine and Timo Aila. Temporal ensembling for semi-
supervised learning. arXiv preprint arXiv:1610.02242, 2016.
2
[21] Yann LeCun, L´eon Bottou, Yoshua Bengio, and Patrick
Haffner. Gradient-based learning applied to document recog-
nition. Proceedings of the IEEE, 86(11):2278–2324, 1998. 2
[22] Dong-Hyun Lee et al. Pseudo-label: The simple and effi-
cient semi-supervised learning method for deep neural net-
works. In Workshop on Challenges in Representation Learn-
ing, ICML, volume 3, page 896, 2013. 2
[23] Ilya Loshchilov and Frank Hutter.
Decoupled weight de-
cay regularization. In International Conference on Learning
Representations, 2018. 5
[24] Volodymyr Mnih, Nicolas Heess, Alex Graves, et al. Re-
current models of visual attention. In Advances in Neural
Information Processing Systems, pages 2204–2212, 2014. 1,
2, 4
[25] Muzammal Naseer, Kanchana Ranasinghe, Salman Khan,
Munawar Hayat, Fahad Shahbaz Khan, and Ming-Hsuan
Yang. Intriguing properties of vision transformers. arXiv
preprint arXiv:2105.10497, 2021. 1
[26] Bowen Pan, Yifan Jiang, Rameswar Panda, Zhangyang
Wang,
Rogerio
Feris,
and
Aude
Oliva.
Ia-red
2:
Interpretability-aware redundancy reduction for vision trans-
formers. arXiv preprint arXiv:2106.12620, 2021. 2, 8
[27] Athanasios Papadopoulos, Paweł Korus, and Nasir Memon.
Hard-attention for scalable image classification.
arXiv
preprint arXiv:2102.10212, 2021. 1, 2, 4, 7, 8
[28] Hieu Pham, Zihang Dai, Qizhe Xie, and Quoc V Le. Meta
pseudo labels. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pages 11557–
11568, 2021. 2
[29] Samrudhdhi B. Rangrej and James J. Clark. A probabilistic
hard attention model for sequentially observed scenes. arXiv
preprint arXiv:2111.07534, 2021. 2
2526

[30] Marc’Aurelio Ranzato. On learning where to look. arXiv
preprint arXiv:1405.5488, 2014. 2
[31] Yongming Rao, Wenliang Zhao, Benlin Liu, Jiwen Lu, Jie
Zhou, and Cho-Jui Hsieh. Dynamicvit: Efficient vision trans-
formers with dynamic token sparsification. arXiv preprint
arXiv:2106.02034, 2021. 2, 8
[32] Adria Recasens, Petr Kellnhofer, Simon Stent, Wojciech Ma-
tusik, and Antonio Torralba. Learning to zoom: a saliency-
based sampling layer for neural networks. In Proceedings
of the European Conference on Computer Vision (ECCV),
pages 51–66, 2018. 8
[33] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.
Faster r-cnn: towards real-time object detection with re-
gion proposal networks.
In Proceedings of the 28th In-
ternational Conference on Neural Information Processing
Systems-Volume 1, pages 91–99, 2015. 2
[34] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-
jeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,
Aditya Khosla, Michael Bernstein, et al. Imagenet large scale
visual recognition challenge. International Journal of Com-
puter Vision, 115(3):211–252, 2015. 2, 5
[35] Michael S Ryoo. Human activity prediction: Early recog-
nition of ongoing activities from streaming videos. In 2011
International Conference on Computer Vision, pages 1036–
1043. IEEE, 2011. 8
[36] Mehdi Sajjadi, Mehran Javanmardi, and Tolga Tasdizen.
Regularization with stochastic transformations and perturba-
tions for deep semi-supervised learning. Advances in neural
information processing systems, 29:1163–1171, 2016. 2
[37] Soroush Seifi, Abhishek Jha, and Tinne Tuytelaars. Glimpse-
attend-and-explore: Self-attention for active visual explo-
ration. In Proceedings of the IEEE International Conference
on Computer Vision, pages 16137–16146, 2021. 2
[38] Soroush Seifi and Tinne Tuytelaars. Attend and segment:
Attention guided active semantic segmentation.
In Com-
puter Vision–ECCV: 16th European Conference, Glasgow,
UK, Proceedings, Part XXV 16, pages 305–321, 2020. 2
[39] Kihyuk Sohn, David Berthelot, Chun-Liang Li, Zizhao
Zhang, Nicholas Carlini, Ekin D Cubuk, Alex Kurakin, Han
Zhang, and Colin Raffel.
Fixmatch: Simplifying semi-
supervised learning with consistency and confidence. arXiv
preprint arXiv:2001.07685, 2020. 2
[40] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya
Sutskever, and Ruslan Salakhutdinov. Dropout: a simple way
to prevent neural networks from overfitting. The journal of
machine learning research, 15(1):1929–1958, 2014. 2
[41] Richard Stuart Sutton. Temporal credit assignment in rein-
forcement learning. PhD thesis, University of Massachusetts
Amherst, 1984. 4
[42] Mingxing Tan and Quoc Le. Efficientnet: Rethinking model
scaling for convolutional neural networks. In International
Conference on Machine Learning, pages 6105–6114, 2019.
1
[43] Matthew Tancik, Pratul P Srinivasan, Ben Mildenhall, Sara
Fridovich-Keil, Nithin Raghavan, Utkarsh Singhal, Ravi Ra-
mamoorthi, Jonathan T Barron, and Ren Ng. Fourier features
let networks learn high frequency functions in low dimen-
sional domains. arXiv preprint arXiv:2006.10739, 2020. 4
[44] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco
Massa, Alexandre Sablayrolles, and Herv´e J´egou. Training
data-efficient image transformers & distillation through at-
tention. In International Conference on Machine Learning,
pages 10347–10357, 2021. 1, 3, 4, 5
[45] Burak Uzkent and Stefano Ermon. Learning when and where
to zoom with deep reinforcement learning. In Proceedings
of the IEEE Conference on Computer Vision and Pattern
Recognition, pages 12345–12354, 2020. 1, 2, 7, 8
[46] Hado P van Hasselt, Arthur Guez, Matteo Hessel, Volodymyr
Mnih, and David Silver. Learning values across many orders
of magnitude. Advances in Neural Information Processing
Systems, 29:4287–4295, 2016. 5
[47] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-
reit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia
Polosukhin. Attention is all you need. In Advances in neural
information processing systems, pages 5998–6008, 2017. 1
[48] Yulin Wang, Kangchen Lv, Rui Huang, Shiji Song, Le Yang,
and Gao Huang.
Glance and focus: a dynamic approach
to reducing spatial redundancy in image classification. Ad-
vances in Neural Information Processing Systems, 33, 2020.
2, 7, 8
[49] Ronald J Williams. Simple statistical gradient-following al-
gorithms for connectionist reinforcement learning. Machine
Learning, 8(3-4):229–256, 1992. 4
[50] Qizhe Xie, Zihang Dai, Eduard Hovy, Minh-Thang Luong,
and Quoc V Le. Unsupervised data augmentation for consis-
tency training. arXiv preprint arXiv:1904.12848, 2019. 2
[51] Qizhe Xie, Minh-Thang Luong, Eduard Hovy, and Quoc V
Le. Self-training with noisy student improves imagenet clas-
sification. In Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition, pages 10687–10698,
2020. 2, 4
[52] Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron
Courville, Ruslan Salakhudinov, Rich Zemel, and Yoshua
Bengio. Show, attend and tell: Neural image caption gen-
eration with visual attention. In International Conference on
Machine Learning, pages 2048–2057, 2015. 2
[53] Serena Yeung, Olga Russakovsky, Greg Mori, and Li Fei-
Fei.
End-to-end learning of action detection from frame
glimpses in videos. In Proceedings of the IEEE conference
on computer vision and pattern recognition, pages 2678–
2687, 2016. 8
[54] Xiaoyu Yue, Shuyang Sun, Zhanghui Kuang, Meng Wei,
Philip HS Torr, Wayne Zhang, and Dahua Lin. Vision trans-
former with progressive sampling.
In Proceedings of the
IEEE International Conference on Computer Vision, pages
387–396, 2021. 2
2527

