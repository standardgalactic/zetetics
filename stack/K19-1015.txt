Proceedings of the 23rd Conference on Computational Natural Language Learning, pages 152–162
Hong Kong, China, November 3-4, 2019. c⃝2019 Association for Computational Linguistics
152
Learning to Represent Bilingual Dictionaries
Muhao Chen1∗, Yingtao Tian2∗, Haochen Chen2,
Kai-Wei Chang1, Steven Skiena2 & Carlo Zaniolo1
1University of California, Los Angeles, CA, USA
2The State University of New York, Stony Brook, NY, USA
muhaochen@ucla.edu; {kwchang, zaniolo}@cs.ucla.edu;
{yittian, haocchen, skiena}@cs.stonybrook.edu
Abstract
Bilingual word embeddings have been widely
used to capture the correspondence of lexi-
cal semantics in different human languages.
However, the cross-lingual correspondence
between sentences and words is less studied,
despite that this correspondence can signiﬁ-
cantly beneﬁt many applications such as cross-
lingual semantic search and textual inference.
To bridge this gap, we propose a neural em-
bedding model that leverages bilingual dictio-
naries1. The proposed model is trained to map
the lexical deﬁnitions to the cross-lingual tar-
get words, for which we explore with differ-
ent sentence encoding techniques. To enhance
the learning process on limited resources, our
model adopts several critical learning strate-
gies, including multi-task learning on differ-
ent bridges of languages, and joint learning of
the dictionary model with a bilingual word em-
bedding model. We conduct experiments on
two new tasks.
In the cross-lingual reverse
dictionary retrieval task, we demonstrate that
our model is capable of comprehending bilin-
gual concepts based on descriptions, and the
proposed learning strategies are effective. In
the bilingual paraphrase identiﬁcation task, we
show that our model effectively associates sen-
tences in different languages via a shared em-
bedding space, and outperforms existing ap-
proaches in identifying bilingual paraphrases.
1
Introduction
Cross-lingual semantic representation learning has
attracted signiﬁcant attention recently. Various ap-
proaches have been proposed to align words of
different languages in a shared embedding space
(Ruder et al., 2017). By offering task-invariant se-
∗Both authors contributed equally to this work.
1We refer the term dictionary to its common meaning, i.e.
lexical deﬁnitions of words. Note that this is different from
some papers on bilingual settings that refer dictionaries to
seed lexicons for one-to-one word mappings.
mantic transfers, these approaches critically sup-
port many cross-lingual NLP tasks including neu-
ral machine translations (NMT) (Devlin et al.,
2014), bilingual document classiﬁcation (Zhou
et al., 2016), knowledge alignment (Chen et al.,
2018b) and entity linking (Upadhyay et al., 2018).
While many existing approaches have been pro-
posed to associate lexical semantics between lan-
guages (Chandar et al., 2014; Gouws et al., 2015;
Luong et al., 2015a), modeling the correspon-
dence between lexical and sentential semantics
across different languages is still an unresolved
challenge.
We argue that learning to represent
such cross-lingual and multi-granular correspon-
dence is well desired and natural for multiple rea-
sons. One reason is that, learning word-to-word
correspondence has a natural limitation, consider-
ing that many words do not have direct transla-
tions in another language. For example, schaden-
freude in German, which means a feeling of joy
that comes from knowing the troubles of other
people, has no proper English counterpart word.
To appropriately learn the representations of such
words in bilingual embeddings, we need to capture
their meanings based on the deﬁnitions.
Besides, modeling such correspondence is also
highly beneﬁcial to many application scenarios.
One example is cross-lingual semantic search of
concepts (Hill et al., 2016), where the lexemes
or concepts are retrieved based on sentential de-
scriptions (see Fig. 1). Others include discourse
relation detection in bilingual dialogue utterances
(Jiang et al., 2018), multilingual text summariza-
tion (Nenkova et al., 2012), and educational ap-
plications for foreign language learners. Finally,
it is natural in foreign language learning that a
human learns foreign words by looking up their
meanings in the native language (Hulstijn et al.,
1996). Therefore, learning such correspondence
essentially mimics human learning behaviors.

153
A male descendent in relation to his parents. EN
Tout être humain du sexe masculin considéré
par rapport à son père et à sa mère, ou à un des 
deux seulement.                                                    FR
FilsFR
Cross-lingual Reverse 
Dictionary Retrieval
Cross-lingual Paraphrases
SonEN
Figure 1: An example illustrating the two cross-lingual
tasks.
The cross-lingual reverse dictionary retrieval ﬁnds
cross-lingual target words based on descriptions. In terms
of cross-lingual paraphrases, the French sentence (which
means any male being considered in relation to his father
and mother, or only one of them) describes the same meaning
as the English sentence, but has much more content details.
However, realizing such a representation learn-
ing model is a non-trivial task, inasmuch as it re-
quires a comprehensive learning process to effec-
tively compose the semantics of arbitrary-length
sentences in one language, and associate that with
single words in another language. Consequently,
this objective also demands high-quality cross-
lingual alignment that bridges between single and
sequences of words. Such alignment information
is generally not available in the parallel and seed-
lexicon that are utilized by bilingual word embed-
dings (Ruder et al., 2017).
To incorporate the representations of bilingual
lexical and sentential semantics, we propose an
approach to capture the mapping from the deﬁni-
tions to the corresponding foreign words by lever-
aging bilingual dictionaries The proposed model
BilDRL
(Bilingual Dictionary Representation
Learning) ﬁrst constructs a word embedding
space with pre-trained bilingual word embed-
dings.
Based on cross-lingual word deﬁnitions,
a sentence encoder is trained to realize the map-
ping from literal descriptions to target words in
the bilingual word embedding space, for which
we investigate with multiple encoding techniques.
To enhance cross-lingual learning on limited re-
sources, BilDRL conducts multi-task learning on
different directions of a language pair.
More-
over, BilDRL enforces a joint learning strategy
of bilingual word embeddings and the sentence
encoder, which seeks to gradually adjust the em-
bedding space to better suit the representation of
cross-lingual word deﬁnitions.
To show the applicability of BilDRL, we con-
duct experiments on two useful cross-lingual tasks
(see Fig. 1). (i) Cross-lingual reverse dictionary
retrieval seeks to retrieve words or concepts given
descriptions in another language. This task is use-
ful to help users ﬁnd foreign words based on the
notions or descriptions, and is especially beneﬁ-
cial to users such as translators, foreigner language
learners and technical writers using non-native
languages.
We show that BilDRL
achieves
promising results on this task, while bilingual
multi-task learning and joint learning dramatically
enhance the performance.
(ii) Bilingual para-
phrase identiﬁcation asks whether two sentences
in different languages essentially express the same
meaning, which is critical to question answering
or dialogue systems that apprehend multilingual
utterances (Bannard and Callison-Burch, 2005).
This task is challenging, as it requires a model to
comprehend cross-lingual paraphrases that are in-
consistent in grammar, content details and word
orders. BilDRL maps sentences to the lexicon
embedding space. This process reduces the prob-
lem to evaluate the similarity of lexicon embed-
dings, which can be easily solved by a simple clas-
siﬁer. BilDRL performs well with even a small
amount of data, and signiﬁcantly outperforms pre-
vious approaches.
2
Related Work
We discuss two lines of relevant work.
Bilingual word embeddings. Various approaches
have been proposed for training bilingual word
embeddings. These approaches span in two fami-
lies: off-line mappings and joint training.
The off-line mapping based approach ﬁxes the
structures of pre-trained monolingual embeddings,
and induces bilingual projections based on seed
lexicons (Mikolov et al., 2013a). Some variants
of this approach improve the quality of projec-
tions by adding constraints such as orthogonality
of transforms, normalization and mean centering
of embeddings (Xing et al., 2015; Artetxe et al.,
2016; Vuli´c et al., 2016). Others adopt canonical
correlation analysis to map separate monolingual
embeddings to a shared embedding space (Faruqui
and Dyer, 2014; Doval et al., 2018).
Unlike off-line mappings, joint training mod-
els simultaneously update word embeddings and
cross-lingual alignment.
In doing so, such ap-
proaches generally capture more precise cross-
lingual semantic transfer (Ruder et al., 2017;
Upadhyay et al., 2018). While a few such mod-
els still maintain separated embedding spaces for
each language (Artetxe et al., 2017), more of them
maintain a uniﬁed space for both languages. The

154
cross-lingual semantic transfer by these models is
captured from parallel corpora with sentential or
document-level alignment, using techniques such
as bilingual bag-of-words distances (BilBOWA)
(Gouws et al., 2015), Skip-Gram (Coulmance
et al., 2015) and sparse tensor factorization (Vyas
and Carpuat, 2016).
Neural sentence modeling.
Neural sentence
models seek to capture phrasal or sentential se-
mantics from word sequences. They often adopt
encoding techniques such as recurrent neural en-
coders (RNN) (Kiros et al., 2015), convolutional
encoders (CNN) (Chen et al., 2018a), and atten-
tive encoders (Rockt¨aschel et al., 2016) to repre-
sent the composed semantics of a sentence as an
embedding vector.
Recent works have focused
on apprehending pairwise correspondence of sen-
tential semantics by adopting multiple neural sen-
tence models in one learning architecture, includ-
ing Siamese models for detecting discourse rela-
tions of sentences (Sha et al., 2016), and sequence-
to-sequence models for tasks like style transfer
(Shen et al., 2017), text summarization (Chopra
et al., 2016) and translation (Wu et al., 2016).
On the other hand, fewer efforts have been put
to characterizing the associations between senten-
tial and lexical semantics. Hill et al. (2016) and
Ji et al. (2017) learn off-line mappings between
monolingual descriptions and lexemes to capture
such associations. Eisner et al. (2016) adopt a sim-
ilar approach to capture emojis based on descrip-
tions. At the best of our knowledge, there has been
no previous approach to learn to discover the cor-
respondence of sentential and lexical semantics in
a multilingual scenario. This is exactly the focus
of our work, in which the proposed strategies of
multi-task learning and joint learning are critical
to the corresponding learning process under lim-
ited resources. Utilizing such correspondence, our
approach also sheds light on addressing discourse
relation detection in a multilingual scenario.
3
Modeling Bilingual Dictionaries
We hereby begin our modeling with the formaliza-
tion of bilingual dictionaries. We use L to denote
the set of languages. For a language l ∈L, Vl de-
notes its vocabulary, where for each word w ∈Vl,
bold-faced w ∈Rk denotes its embedding vec-
tor. A li-lj bilingual dictionary D(li, lj) (or simply
Dij) contains dictionary entries (wi, Sj
w) ∈Dij,
in which wi ∈Vli, and Sj
w = wj
1 . . . wj
n (wj
· ∈
Vlj) is a cross-lingual deﬁnition that describes the
word wi with a sequence of words in language
lj.
For example, a French-English dictionary
D(Fr, En) could include a French word app´etite
accompanied by its English deﬁnition desire for,
or relish of food or drink. Note that, for a word
wi, multiple deﬁnitions in lj may coexist.
BilDRL is constructed and improved through
three stages, as depicted in Fig. 2. A sentence en-
coder is ﬁrst used to learn from a bilingual dic-
tionary the association between words and deﬁni-
tions. Then in a pre-trained bilingual word em-
bedding space, multi-task learning is conducted on
both directions of a language pair. Lastly, joint
learning with word embeddings is enforced to si-
multaneously adjust the embedding space during
the training of the dictionary model, which further
enhances the cross-lingual learning process.
It is noteworthy that, NMT (Wu et al., 2016)
is considered as an ostensibly relevant method to
ours. NMT does not apply to our problem setting
bacause it has major differences from our work in
those perspectives: (i) In terms of data modali-
ties, NMT has to bridge between corpora of the
same granularity, i.e.
either between sentences
or between lexicons. This is unlike BilDRL that
captures multi-granular correspondence of seman-
tics across different modalities, i.e. sentences and
words; (ii) As for learning strategies, NMT relies
on an encoder-decoder architecture using end-to-
end training (Luong et al., 2015b), while BilDRL
employs joint learning of a dictionary-based sen-
tence encoder and a bilingual embedding space.
3.1
Encoders for Lexical Deﬁnitions
BilDRL models a dictionary using a neural sen-
tence encoder E(S), which composes the mean-
ing of the sentence into a latent vector representa-
tion. We hereby introduce this model component,
which is designed to be a GRU encoder with self-
attention. Besides that, we also investigate other
widely-used neural sequence encoders.
3.1.1
Attentive GRU Encoder
The GRU encoder is a computationally efﬁcient
alternative of the LSTM (Cho et al., 2014). Each
unit consists of a reset gate rt and an update gate zt
to track the state of the sequence. Given the vector
representation wt of an incoming item wt, GRU
updates the hidden state h(1)
t
as a linear combina-
tion of the previous state h(1)
t−1 and the candidate

155
Embedding Encoder Average
Description English
Target French
* a male descendent in
relation to his parents
^ fils
Sentence English
Sentence French
* the  cat  sat  on  the  mat
^ le  chat  s’est assis sur   le   tapis
Word English
Classifier
✓Neighbor English
✘Other English
Word French
✓Neighbor French
✘Other French
* mat
^ tapis
!"#,%&
'(
Ω"#,%&
*
!"#
+,
!%&
+,
MTL Dictionary Model
Bilingual BilBOWA Model
English/French Monolingual Skip-Gram Model
Joint Objective Function = !"#,%&
'(
+ ./ !"#
+, + !%&
+,
+ .0Ω"#,%&
1
The same color indicates shared parameters.
Legend
Output
*
^
*
^
*
^
# the  cat  sat  on  the
$ le  chat  s’est assis sur  le
#
$
Figure 2: Joint learning architecture of BilDRL.
state ˜h(1)
t
of the new item wt as below.
h(1)
t
= zt ⊙˜h(1)
t
+ (1 −zt) ⊙h(1)
t−1.
The update gate zt balances between the infor-
mation of the previous sequence and the new item,
where Mz and Nz are two weight matrices, bz is
a bias vector, and σ is the sigmoid function.
zt = σ

Mzxt + Nzh(1)
t−1 + bz

.
The candidate state ˜h(1)
t
is calculated similarly
to those in a traditional recurrent unit as below.
The reset gate rt thereof controls how much infor-
mation of the past sequence should contribute to
the candidate state.
˜h(1)
t
= tanh

Mswt + rt ⊙(Nsh(1)
t−1) + bs

rt = σ

Mrwt + Nrh(1)
t−1 + br

.
While a GRU encoder can stack multiple of the
above GRU layers, without an attention mecha-
nism, the last state h(1)
S
of the last layer represents
the overall meaning of the encoded sentence S.
The self-attention mechanism (Conneau et al.,
2017) seeks to highlight the important units in an
input sentence when capturing its overall meaning,
which is calculated as below:
ut = tanh

Mah(1)
t
+ ba

at =
exp
 u⊤
t uS

P
wm∈S exp (u⊤
muS)
h(2)
t
= |S|atut.
ut is the intermediary representation of GRU out-
put h(1)
t , and uS = tanh(Mah(1)
S
+ ba) is that
of the last GRU output h(1)
S . uS can be seen as
a high-level representation of the input sequence.
By measuring the similarity of each ut with uS,
the normalized attention weight at, which high-
lights an input that contributes signiﬁcantly to the
overall meaning, is produced through a softmax.
Note that a scalar |S| is multiplied along with at
to ut, so as to keep the weighted representation
h(2)
t
from losing the scale of h(1)
t . The sentence
encoding is calculated as the average of the last
attention layer E(1)(S) =
1
|S|
P|S|
t=1 ath(2)
t .
3.1.2
Other Encoders
We also experiment with other widely used neural
sentence modeling techniques2, which are how-
ever outperformed by the attentive GRU in our
tasks. These techniques include the vanilla GRU,
CNN (Kalchbrenner et al., 2014), and linear bag-
of-words (BOW) (Hill et al., 2016). We brieﬂy in-
troduce the later two techniques in the following.
A convolutional encoder applies a kernel Mc ∈
Rh×k to produce the latent representation h(3)
t
=
tanh(Mcwt:t+h−1 + bc) from each h-gram of the
input vector sequence wt:t+h−1, for which h is the
kernel size and bc is a bias vector. A sequence of
latent vectors H(3) = [h(3)
1 , h(3)
2 , ..., h(3)
|S|−h+1] is
produced from the input, where each latent vector
leverages the signiﬁcant local semantic features
from each h-gram.
Following convention (Liu
et al., 2017), we apply dynamic max-pooling to
extract robust features from the convolution out-
puts, and use the mean-pooling results of the last
layer to represent the sentential semantics.
The Linear bag-of-words (BOW) encoder (Ji
et al., 2017; Hill et al., 2016) is realized by the
2Note that recent advances in monolingual contextualized
embeddings like multilingual ELMo (Peters et al., 2018; Che
et al., 2018) and M-BERT (Pires et al., 2019; Devlin et al.,
2018) can also be supported to represent sentences for our
setting. We leave them as future work, as they require non-
trivial adaption to both multilingual settings and joint train-
ing, and extensive pre-training on external corpora.

156
sum of projected word embeddings of the input
sentence, i.e. E(2)(S) = P|S|
t=1 Mbwt.
3.2
Basic Learning Objective
The objective of learning the dictionary model is
to map the encodings of cross-lingual word deﬁni-
tions to the target word embeddings. This is real-
ized by minimizing the following L2 loss,
LST
ij =
1
|Dij|
X
(wi,Sj
w)∈Dij
Eij(Sj
w) −wi2
2
in which Eij is the dictionary model that maps
from descriptions in lj to words in li.
The above deﬁnes the basic model variants
of BilDRL that learns on a single dictionary.
For word representations in the learning process,
BilDRL initializes the embedding space using
pre-trained word embeddings. Note that, without
adopting the joint learning strategy in Section 3.4,
the learning process does not update word embed-
dings that are used to represent the deﬁnitions and
target words. While other forms of loss such as
cosine proximity (Hill et al., 2016) and hinge loss
(Ji et al., 2017) may also be used in the learning
process, we ﬁnd that L2 loss consistently leads to
better performance in our experiments.
3.3
Bilingual Multi-task Learning
In cases where entries in a bilingual dictionary
are not amply provided, learning the above bilin-
gual dictionary on one ordered language pair may
fall short in insufﬁciency of alignment informa-
tion. One practical solution is to conduct a bilin-
gual multi-task learning process. In detail, given
a language pair (li, lj), we learn the dictionary
model Eij on both dictionaries Dij and Dji with
shared parameters. Correspondingly, we rewrite
the previous learning objective function as below,
in which D = Dij ∪Dji.
LMT
ij
=
1
|D|
X
(w,Sw)∈D
∥Eij(Sw) −w∥2
2 .
This strategy non-trivially requests the same
dictionary model to represent semantic transfer in
two directions of the language pair. To fulﬁll such
a request, we initialize the embedding space using
the BilBOWA embeddings (Gouws et al., 2015),
which provide a uniﬁed embedding space that re-
solves both monolingual and cross-lingual seman-
tic relatedness of words. In practice, we ﬁnd this
simple multi-task strategy to bring signiﬁcant im-
provement to our cross-lingual tasks. Note that,
besides BilBOWA, other joint-training bilingual
embeddings in a uniﬁed space (Doval et al., 2018)
can also support this strategy, for which we leave
the comparison to future work.
3.4
Joint Learning Objective
While above learning strategies are based on a
ﬁxed embedding space, we lastly propose a joint
learning strategy.
During the training process,
this strategy simultaneously updates the embed-
ding space based on both the dictionary model and
the bilingual word embedding model. The learn-
ing is through asynchronous minimization of the
following joint objective function,
J = LMT
ij
+ λ1(LSG
i
+ LSG
j ) + λ2ΩA
ij,
where λ1 and λ2 are two positive hyperparameters.
LSG
i
and LSG
j
are the original Skip-Gram losses
(Mikolov et al., 2013b) to separately obtain word
embeddings on monolingual corpora of li and lj.
ΩA
ij, termed as below, is the alignment loss to min-
imize bag-of-words distances for aligned sentence
pairs (Si, Sj) in parallel corpora Cij.
ΩA
ij =
1
|Cij|
X
(Si,Sj)∈Cij
dS(Si, Sj)
dS(Si, Sj) =

1
|Si|
X
wim∈Si
wi
m −
1
|Sj|
X
wj
n∈Sj
wj
n

2
2
The joint learning process adapts the embed-
ding space to better suit the dictionary model,
which is shown to further enhance the cross-
lingual learning of BilDRL.
3.5
Training
To initialize the embedding space,
we pre-
trained BilBOWA on the parallel corpora Eu-
roparl v7 (Koehn, 2005) and monolingual cor-
pora of tokenized Wikipedia dump (Al-Rfou et al.,
2013). For models without joint learning, we use
AMSGrad (Reddi et al., 2018) to optimize the pa-
rameters. Each model without bilingual multi-task
learning thereof, is trained on batched samples
from each individual dictionary. Multi-task learn-
ing models are trained on batched samples from
two dictionaries.
Within each batch, entries of
different directions of languages can be mixed to-
gether. For joint learning, we conduct an efﬁcient

157
multi-threaded asynchronous training (Mnih et al.,
2016) of AMSGrad. In detail, after initializing the
embedding space based on pre-trained BilBOWA,
parameter updating based on the four components
of J occurs across four worker threads.
Two
monolingual threads select batches of monolin-
gual contexts from the Wikipedia dump of two lan-
guages for Skip-Gram, one alignment thread ran-
domly samples parallel sentences from Europarl,
and one dictionary thread extracts samples of en-
tries for a bilingual multi-task dictionary model.
Each thread makes a batched update to model pa-
rameters asynchronously for each term of J. The
asynchronous training of all threads keeps going
until the dictionary thread ﬁnishes its epochs.
4
Experiments
We present experiments on two multilingual tasks:
the cross-lingual reverse dictionary retrieval task
and the bilingual paraphrase identiﬁcation task.
4.1
Datasets
The experiment of cross-lingual reverse dictio-
nary retrieval is conducted on a trilingual dataset
Wikt3l.
This dataset is extracted from Wik-
tionary3, which is one of the largest freely avail-
able multilingual dictionary resources on the Web.
Wikt3l contains dictionary entries of language
pairs (English, French) and (English, Spanish),
which form En-Fr, Fr-En, En-Es and Es-En dic-
tionaries on four bridges of languages in total.
Two types of cross-lingual deﬁnitions are ex-
tracted from Wiktionary: (i) cross-lingual deﬁni-
tions provided under the Translations sections of
Wiktionary pages; (ii) monolingual deﬁnitions for
words that are linked to a cross-lingual counterpart
with a inter-language link4 of Wiktionary. We ex-
clude all the deﬁnitions of stop words in construct-
ing the dataset, and list the statistics in Table 1.
Since existing datasets for paraphrase identi-
ﬁcation are merely monolingual, we contribute
with another dataset WBP3l for cross-lingual
sentential paraphrase identiﬁcation. This dataset
contains 6,000 pairs of bilingual sentence pairs
respectively for En-Fr and En-Es settings. Within
each bilingual setting, positive cases are formed
as pairs of descriptions aligned by inter-language
links, which exclude the word descriptions in
3https://www.wiktionary.org/
4An inter-language link matches the entries of counterpart
words between language versions of Wiktionary.
Dictionary
En-Fr Fr-En En-Es Es-En
#Target words 15,666 16,857 8,004 16,986
#Deﬁnitions 50,412 58,808 20,930 56,610
Table 1: Statistics of the bilingual dictionary dataset Wikt3l.
Positive Examples
En:Being remote in space.
Fr:Se trouvant `a une grande distance.
En:The interdisciplinary science that applies theories and
methods of the physical sciences to questions of biology.
Es:Ciencia que emplea y desarrolla las teorias y m´etodos de
la f´ısica en la investigaci´on de los sistemas biolgicos.
Negative Examples
En:A person who secedes or supports secession from a
political union.
Fr:Contrˆole politique exerc´e par une grande puissance sur
une contre inf´eod´ee.
En:The fear of closed, tight places.
Es:P´erdida o disminuci´on considerables de la memoria.
Table 2: Examples of bilingual paraphrases from WBP3l.
Wikt3l for training BilDRL. To generate negative
examples, given a source word, we ﬁrst ﬁnd its
15 nearest neighbors in the embedding space.
Within the nearest neighbors, we use ConceptNet
(Speer et al., 2017) to ﬁlter out the synonyms of
the source word, so as to prevent from generating
false negative cases.
Then we randomly pick
one word from the ﬁltered neighbors and pair
its
cross-lingual
deﬁnition
with
the
English
deﬁnition of the source word to create a negative
case.
This process ensures that each negative
case is endowed with limited dissimilarity of
sentence meanings, which makes the decision
more challenging.
For each language setting,
we randomly select 70% for training, 5% for
validation, and the rest 25% for testing.
Note
that each language setting of this dataset thereof,
matches with the quantity and partitioning of
sentence pairs in the widely-used Microsoft
Research
Paraphrase
Corpus
benchmark
for
monolingual paraphrase identiﬁcation (Yin et al.,
2016; Das and Smith, 2009). Several examples
from the dataset are shown in Table 2.
The
datasets and the processing scripts are available
at
https://github.com/muhaochen/
bilingual_dictionaries.
4.2
Cross-lingual Reverse Dictionary
Retrieval
The objective of this task is to enable cross-lingual
semantic retrieval of words based on descriptions.
Besides comparing variants of BilDRL that adopt
different sentence encoders and learning strate-

158
Languages
En-Fr
Fr-En
En-Es
Es-En
Metric
P@1 P@10 MRR P@1 P@10 MRR P@1 P@10 MRR P@1 P@10 MRR
BOW
0.8
3.4
0.011
0.4
2.2
0.006
0.4
2.4
0.007
0.4
2.6
0.007
CNN
6.0
12.4
0.070
6.4
14.8
0.072
3.8
7.2
0.045
7.0
16.8
0.088
GRU
35.6
46.0
0.380
38.8
49.8
0.410
47.8
59.0
0.496
57.6
67.2
0.604
ATT
38.8
47.4
0.411
39.8
50.2
0.425
51.6
59.2
0.534
60.4
68.4
0.629
GRU-mono 21.8
33.2
0.242
27.8
37.0
0.297
34.4
41.2
0.358
36.8
47.2
0.392
ATT-mono
22.8
33.6
0.249
27.4
39.0
0.298
34.6
42.2
0.358
39.4
48.6
0.414
GRU-MTL 43.4
49.2
0.452
44.4
52.8
0.467
50.4
60.0
0.530
63.6
71.8
0.659
ATT-MTL
46.8
56.6
0.487
47.6
56.6
0.497
55.8
62.2
0.575
66.4
75.0
0.687
ATT-joint
63.6
69.4 0.654 68.2
75.4 0.706 69.0
72.8 0.704 78.6
83.4 0.803
Table 3: Cross-lingual reverse dictionary retrieval results by BilDRL variants. We report P@1, P@10, and MRR on four
groups of models: (i) basic dictionary models that adopt four different encoding techniques (BOW, CNN, GRU and ATT); (ii)
models with the two best encoding techniques that enforce the monolingual retrieval approach by Hill et al. (2016) (GRU-
mono and ATT-mono); (iii) models adopting bilingual multi-task learning (GRU-MTL and ATT-MTL); (iv) joint learning that
employs the best dictionary model of ATT-MTL (ATT-joint).
gies, we also compare with the monolingual re-
trieval approach proposed by Hill et al. (2016).
Instead of directly associating cross-lingual word
deﬁnitions, this approach learns deﬁnition-to-
word mappings in a monolingual scenario. When
it applies to the multilingual setting, given a lexical
deﬁnition, it ﬁrst retrieves the corresponding word
in the source language. Then, it looks for seman-
tically related words in the target language using
bilingual word embeddings. As discussed in Sec-
tion 3, NMT does not apply to this task due that it
cannot capture the multi-granular correspondence
between a sentence and a word.
Evaluation Protocol. Before training the mod-
els, we randomly select 500 word deﬁnitions from
each dictionary respectively as test cases, and
exclude these deﬁnitions from the training data.
Each of the basic BilDRL variants are trained
on one bilingual dictionary. The monolingual re-
trieval models are trained to ﬁt the target words
in the original languages of the word deﬁnitions,
which are also provided in Wiktionary. BilDRL
variants with multi-task or joint learning use both
dictionaries of the same language pair. In the test
phase, for each test case (wi, Sj
w) ∈Dij, the
prediction performs a kNN search from the deﬁ-
nition encoding Eij(Sj
w), and record the rank of
wi within the vocabulary of li. We limit the vo-
cabularies to all words that appear in the Wikt3l
dataset, which involve around 45k English words,
44k French words and 36k Spanish words. To pre-
vent the surface information of the target word
from appearing in the deﬁnition, we have also
masked out any translation of the target word oc-
curring in the deﬁnition using a wildcard token
<concept>. We aggregate three metrics on test
cases: the accuracy P@1 (%), the proportion of
ranks no larger than 10 P@10 (%), and mean re-
ciprocal rank MRR.
We pre-train BilBOWA based on the original
conﬁguration by Gouws et al. (2015) and obtain
50-dimensional initialization of bilingual word
embedding spaces respectively for the English-
French and English-Spanish settings. For CNN,
GRU, and attentive GRU (ATT) encoders, we
stack ﬁve of each corresponding encoding layers
with hidden-sizes of 200, and two afﬁne layers are
applied to the ﬁnal output for dimension reduction.
This encoder architecture consistently represents
the best performance through our tuning. Through
comprehensive hyperparameter tuning, we ﬁx the
learning rate α to 0.0005, the exponential decay
rates of AMSGrad β1 and β2 to 0.9 and 0.999, co-
efﬁcients λ1 and λ2 to both 0.1, and batch size
to 64. Kernel-size and pooling-size are both set
to 2 for CNN. Word deﬁnitions are zero-padded
(short ones) or truncated (long ones) to the se-
quence length of 15, since most deﬁnitions (over
92%) are within 15 words in the dataset. Training
is limited to 1,000 epochs for all models as well as
the dictionary thread of asynchronous joint learn-
ing, in which all models are able to converge.
Results. Results are reported in Table 3 in four
groups. The ﬁrst group compares four different
encoding techniques for the basic dictionary mod-
els. GRU thereof consistently outperforms CNN
and BOW, since the latter two fail to capture the
important sequential information for descriptions.
ATT that weighs among the hidden states has no-
table improvements over GRU. While we equip
the two better encoding techniques with the mono-
lingual retrieval approach (GRU-mono and ATT-
mono), we ﬁnd that the way of learning the dic-
tionary models towards monolingual targets and

159
retrieving cross-lingual related words incurs more
impreciseness to the task. For models of the third
group that conduct multi-task learning in two di-
rections of a language pair, the results show sig-
niﬁcant enhancement of performance in both di-
rections.
For the ﬁnal group of results, we in-
corporate the best variant of multi-task models
into the joint learning architecture, which leads
to compelling improvement of the task on all set-
tings. This demonstrates that properly adapting
the word embeddings in joint with the bilingual
dictionary model efﬁcaciously constructs the em-
bedding space that suits better the representation
of both bilingual lexical and sentential semantics.
In general, this experiment has identiﬁed the
proper encoding techniques of the dictionary
model. The proposed strategies of multi-task and
joint learning effectively contribute to the precise
characterization of the cross-lingual correspon-
dence of lexical and sentential semantics, which
have led to very promising capability of cross-
lingual reverse dictionary retrieval.
4.3
Bilingual Paraphrase Identiﬁcation
The bilingual paraphrase identiﬁcation problem5
is a binary classiﬁcation task with the goal to de-
cide whether two sentences in different languages
express the same meanings. BilDRL provides an
effective solution by transferring sentential mean-
ings to word-level representations and learning a
simple classiﬁer.
We evaluate three variants of
BilDRL on this task using WBP3l: the multi-task
BilDRL with GRU encoders (BilDRL-GRU-
MTL), the multi-task BilDRL with attentive GRU
encoders (BilDRL-ATT-MTL), and the joint
learning BilDRL with with attentive GRU en-
coders (BilDRL-ATT-joint). We compare against
several baselines of neural sentence pair mod-
els that are proposed for monolingual paraphrase
identiﬁcation.
These models include Siamese
structures of CNN (BiCNN) (Yin and Sch¨utze,
2015), RNN (BiLSTM) (Mueller and Thyagara-
jan, 2016), attentive CNN (ABCNN) (Yin et al.,
2016), attentive GRU (BiATT) (Rockt¨aschel et al.,
2016), and BOW (BiBOW). To support the rea-
soning of cross-lingual semantics, we provide the
baselines with the same BilBOWA embeddings.
5Paraphrases have similar meanings, but can largely dif-
fer in content details and word orders. Hence, they are essen-
tially different from translations. We have found that even the
well-recognized Google NMT frequently caused distortions
to short sentence meanings, and led to results that were close
to random guess by the baseline classiﬁers after translation.
Languages
En&Fr
En&Es
Metrics
Acc.
F1
Acc.
F1
BiBOW
54.93 0.622 56.27 0.623
BiCNN
54.33 0.625 53.80 0.611
ABCNN
56.73 0.644 58.83 0.655
BiLSTM
59.60 0.662 57.60 0.637
BiATT
61.47 0.699 61.27 0.689
BilDRL-GRU-MTL 64.80 0.732 63.33 0.722
BilDRL-ATT-MTL 65.27 0.735 66.07 0.735
BilDRL-ATT-joint
68.53 0.785 67.13 0.759
Table 4: Accuracy and F1-scores of bilingual paraphrase
identiﬁcation.
For BilDRL, the results by three model
variants are reported: BilDRL-GRU-MTL and BilDRL-
ATT-MTL are models with bilingual multi-task learning, and
BilDRL-ATT-joint is the best ATT-based dictionary model
variant deployed with both multi-task and joint learning.
Evaluation protocol.
BilDRL transfers each
sentence into a vector in the word embedding
space. Then, for each sentence pair in the train
set, a Multi-layer Perceptron (MLP) with a binary
softmax loss is trained on the subtraction of two
vectors as a downstream classiﬁer. Baseline mod-
els are trained end-to-end, each of which directly
uses a parallel pair of encoders with shared param-
eters and an MLP that is stacked to the subtraction
of two sentence vectors. Note that some works use
concatenation (Yin and Sch¨utze, 2015) or Manhat-
tan distances (Mueller and Thyagarajan, 2016) of
sentence vectors instead of their subtraction (Jiang
et al., 2018), which we ﬁnd to be less effective on
small amount of data.
We apply the conﬁgurations of the sentence en-
coders from the last experiment to corresponding
baselines, so as to show the performance under
controlled variables. Training of a classiﬁer is ter-
minated by early-stopping based on the validation
set. Following convention (Hu et al., 2014; Yin
et al., 2016), we report the accuracy and F1 scores.
Results. This task is challenging due to the hetero-
geneity of cross-lingual paraphrases and limited-
ness of learning resources. The results in Table 4
show that all the baselines, where BiATT con-
sistently outperforms the others, merely reaches
slightly over 60% of accuracy on both En-Fr and
En-Es settings. We believe that it comes down to
the fact that sentences of different languages are
often drastically heterogenous in both lexical se-
mantics and the sentence grammar that governs
the composition of words. Hence, it is not sur-
prising that previous neural sentence pair models,
which capture the semantic relation of bilingual
sentences directly from all participating words,
fall short at the multilingual task. BilDRL, how-

160
ever, effectively leverages the correspondence of
lexical and sentential semantics to simplify the
task to an easier entailment task in the lexicon
space, for which the multi-task learning BilDRL-
ATT-MTL outperforms the best baseline respec-
tively by 3.80% and 4.80% of accuracy in both
language settings, while BilDRL-ATT-joint, em-
ploying the joint learning, further improves the
task by another satisfying 3.26% and 1.06% of ac-
curacy. Both also show notable increment in F1.
5
Conclusion and Future Work
In this paper, we propose a neural embedding
model BilDRL that captures the correspondence
of cross-lingual lexical and sentential semantics.
We experiment with multiple forms of neural
models and identify the best technique.
The
two learning strategies, bilingual multi-task learn-
ing and joint learning, are effective at enhancing
the cross-lingual learning with limited resources,
and also achieve promising performance on cross-
lingual reverse dictionary retrieval and bilingual
paraphrase identiﬁcation tasks by associating lex-
ical and sentential semantics.
An important di-
rection of future work is to explore whether the
word-sentence alignment can improve bilingual
word embeddings. Applying BilDRL to bilingual
question answering and semantic search systems
is another important direction.
6
Acknowledgement
We thank the anonymous reviewers for their in-
sightful comments. This work was supported in
part by National Science Foundation Grant IIS-
1760523.
References
Rami Al-Rfou, Bryan Perozzi, and Steven Skiena.
2013. Plyglot: Distributed word representations for
multilingual nlp.
In The SIGNLL Conference on
Computational Natural Language Learning.
Mikel Artetxe, Gorka Labaka, and Eneko Agirre. 2016.
Learning principled bilingual mappings of word em-
beddings while preserving monolingual invariance.
In Proceedings of the 2016 Conference on Empiri-
cal Methods in Natural Language Processing, pages
2289–2294.
Mikel Artetxe, Gorka Labaka, and Eneko Agirre. 2017.
Learning bilingual word embeddings with (almost)
no bilingual data. In Proceedings of the 55th Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers), volume 1, pages
451–462.
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora.
In Pro-
ceedings of the 43rd Annual Meeting on Association
for Computational Linguistics, pages 597–604. As-
sociation for Computational Linguistics.
Sarath Chandar, Stanislas Lauly, Hugo Larochelle,
Mitesh Khapra, Balaraman Ravindran, Vikas C
Raykar, and Amrita Saha. 2014.
An autoencoder
approach to learning bilingual word representations.
In Advances in Neural Information Processing Sys-
tems, pages 1853–1861.
Wanxiang Che, Yijia Liu, Yuxuan Wang, Bo Zheng,
and Ting Liu. 2018.
Towards better UD parsing:
Deep contextualized word embeddings, ensemble,
and treebank concatenation. In CoNLL Shared Task,
pages 55–64. ACL.
Muhao Chen, Chang Ping Meng, Gang Huang, and
Carlo Zaniolo. 2018a. Neural article pair modeling
for wikipedia sub-article machine. In Proceedings
of European Conference of Machine Learning.
Muhao Chen, Yingtao Tian, Kai-Wei Chang, Steven
Skiena, and Carlo Zaniolo. 2018b. Co-training em-
beddings of knowledge graphs and entity descrip-
tions for cross-lingual entity alignment. In Proceed-
ings of the 27th International Joint Conference on
Artiﬁcial Intelligence.
Kyunghyun Cho, Bart van Merrienboer, Caglar Gul-
cehre, Dzmitry Bahdanau, Fethi Bougares, Holger
Schwenk, and Yoshua Bengio. 2014.
Learning
phrase representations using rnn encoder–decoder
for statistical machine translation. In Proceedings of
the 2014 Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP), pages 1724–
1734.
Sumit Chopra, Michael Auli, and Alexander M Rush.
2016. Abstractive sentence summarization with at-
tentive recurrent neural networks. In Proceedings of
the 2016 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies, pages 93–98.
Alexis Conneau, Douwe Kiela, Holger Schwenk, Lo¨ıc
Barrault, and Antoine Bordes. 2017.
Supervised
learning of universal sentence representations from
natural language inference data. In Proceedings of
the 2017 Conference on Empirical Methods in Nat-
ural Language Processing, pages 670–680.
Jocelyn Coulmance,
Jean-Marc Marty,
Guillaume
Wenzek, and Amine Benhalloum. 2015.
Trans-
gram, fast cross-lingual word-embeddings. In Pro-
ceedings of the 2015 Conference on Empirical Meth-
ods in Natural Language Processing, pages 1109–
1113.

161
Dipanjan Das and Noah A Smith. 2009. Paraphrase
identiﬁcation as probabilistic quasi-synchronous
recognition.
In Proceedings of the Joint Confer-
ence of the 47th Annual Meeting of the ACL and the
4th International Joint Conference on Natural Lan-
guage Processing, pages 468–476. Association for
Computational Linguistics.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2018. Bert: Pre-training of deep
bidirectional transformers for language understand-
ing. arXiv preprint arXiv:1810.04805.
Jacob Devlin, Rabih Zbib, Zhongqiang Huang, Thomas
Lamar, Richard Schwartz, and John Makhoul. 2014.
Fast and robust neural network joint models for sta-
tistical machine translation. In Proceedings of the
52nd Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), vol-
ume 1, pages 1370–1380.
Yerai Doval, Jose Camacho-Collados, Luis Espinosa
Anke, and Steven Schockaert. 2018.
Improving
cross-lingual word embeddings by meeting in the
middle. In Proceedings of the 2018 Conference on
Empirical Methods in Natural Language Process-
ing, pages 294–304.
Ben Eisner, Tim Rockt¨aschel, Isabelle Augenstein,
Matko
Boˇsnjak,
and
Sebastian
Riedel.
2016.
emoji2vec:
Learning emoji representations from
their description. arXiv preprint arXiv:1609.08359.
Manaal Faruqui and Chris Dyer. 2014. Improving vec-
tor space word representations using multilingual
correlation. In Proceedings of the 14th Conference
of the European Chapter of the Association for Com-
putational Linguistics, pages 462–471.
Stephan Gouws, Yoshua Bengio, and Greg Corrado.
2015. Bilbowa: Fast bilingual distributed represen-
tations without word alignments.
In Proceedings
of the 32nd International Conference on Machine
Learning, pages 748–756.
Felix Hill, KyungHyun Cho, Anna Korhonen, and
Yoshua Bengio. 2016.
Learning to understand
phrases by embedding the dictionary. Transactions
of the Association for Computational Linguistics,
4:17–30.
Baotian Hu, Zhengdong Lu, Hang Li, and Qingcai
Chen. 2014. Convolutional neural network architec-
tures for matching natural language sentences. In
Advances in neural information processing systems,
pages 2042–2050.
Jan H Hulstijn, Merel Hollander, and Tine Grei-
danus. 1996. Incidental vocabulary learning by ad-
vanced foreign language students: The inﬂuence of
marginal glosses, dictionary use, and reoccurrence
of unknown words. The modern language journal,
80(3):327–339.
Guoliang Ji, Kang Liu, Shizhu He, Jun Zhao, et al.
2017.
Distant supervision for relation extraction
with sentence-level attention and entity descriptions.
In Proceedings of the AAAI International Confer-
ence on Artiﬁcial Intelligence, pages 3060–3066.
Jyun-Yu Jiang, Francine Chen, Yan-Ying Chen, and
Wei Wang. 2018.
Learning to disentangle inter-
leaved conversational threads with a siamese hierar-
chical network and similarity ranking. In Proceed-
ings of the 2018 Conference of the North American
Chapter of the Association for Computational Lin-
guistics: Human Language Technologies, Volume 1
(Long Papers), volume 1, pages 1812–1822.
Nal Kalchbrenner, Edward Grefenstette, Phil Blunsom,
Dimitri Kartsaklis, Nal Kalchbrenner, Mehrnoosh
Sadrzadeh, Nal Kalchbrenner, Phil Blunsom, Nal
Kalchbrenner, and Phil Blunsom. 2014. A convo-
lutional neural network for modelling sentences. In
Proceedings of the 52nd Annual Meeting of the As-
sociation for Computational Linguistics, pages 212–
217. Association for Computational Linguistics.
Ryan Kiros, Yukun Zhu, Ruslan R Salakhutdinov,
Richard Zemel, Raquel Urtasun, Antonio Torralba,
and Sanja Fidler. 2015. Skip-thought vectors. In
Advances in neural information processing systems,
pages 3294–3302.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In MT summit, vol-
ume 5, pages 79–86.
Jingzhou Liu, Wei-Cheng Chang, Yuexin Wu, and
Yiming Yang. 2017.
Deep learning for extreme
multi-label text classiﬁcation. In Proceedings of the
40th International ACM SIGIR Conference on Re-
search and Development in Information Retrieval,
pages 115–124. ACM.
Thang Luong, Hieu Pham, and Christopher D Man-
ning. 2015a.
Bilingual word representations with
monolingual quality in mind. In Proceedings of the
1st Workshop on Vector Space Modeling for Natural
Language Processing, pages 151–159.
Thang Luong, Hieu Pham, and Christopher D Man-
ning. 2015b.
Effective approaches to attention-
based neural machine translation. In Proceedings of
the 2015 Conference on Empirical Methods in Nat-
ural Language Processing, pages 1412–1421.
Tomas Mikolov, Quoc V Le, and Ilya Sutskever. 2013a.
Exploiting similarities among languages for ma-
chine translation. arXiv preprint arXiv:1309.4168.
Tomas Mikolov, Ilya Sutskever, et al. 2013b.
Dis-
tributed representations of words and phrases and
their compositionality. In Advances in Neural In-
formation Processing Systems.
Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi
Mirza, Alex Graves, Timothy Lillicrap, Tim Harley,
David Silver, and Koray Kavukcuoglu. 2016. Asyn-
chronous methods for deep reinforcement learning.

162
In International Conference on Machine Learning,
pages 1928–1937.
Jonas Mueller and Aditya Thyagarajan. 2016. Siamese
recurrent architectures for learning sentence similar-
ity. In Proceedings of the AAAI Conference on Arti-
ﬁcial Intelligence, pages 2786–2792.
Ani Nenkova, Kathleen McKeown, et al. 2012. A sur-
vey of text summarization techniques.
In Mining
text data, pages 43–76. Springer.
Matthew Peters, Mark Neumann, Mohit Iyyer, Matt
Gardner, Christopher Clark, Kenton Lee, and Luke
Zettlemoyer. 2018. Deep contextualized word repre-
sentations. In Proceedings of the 2018 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies, Volume 1 (Long Papers), volume 1,
pages 2227–2237.
Telmo Pires, Eva Schlinger, and Dan Garrette. 2019.
How multilingual is multilingual bert? In ACL.
Sashank J Reddi, Satyen Kale, and Sanjiv Kumar.
2018. On the convergence of adam and beyond. In
International Conference on Learning Representa-
tions.
Tim Rockt¨aschel, Edward Grefenstette, Karl Moritz
Hermann, Tom´aˇs Koˇcisk`y, and Phil Blunsom. 2016.
Reasoning about entailment with neural attention.
In International Conference on Learning Represen-
tations.
Sebastian Ruder, Ivan Vuli´c, and Anders Søgaard.
2017. A survey of cross-lingual word embedding
models. Journal of Artiﬁcial Intelligence Research.
Lei Sha, Baobao Chang, et al. 2016.
Reading and
thinking: Re-read lstm unit for textual entailment
recognition.
In Proceedings of the International
Conference on Computational Linguistics.
Tianxiao Shen, Tao Lei, Regina Barzilay, and Tommi
Jaakkola. 2017. Style transfer from non-parallel text
by cross-alignment. In Advances in Neural Informa-
tion Processing Systems, pages 6833–6844.
Robert Speer, Joshua Chin, and Catherine Havasi.
2017. Conceptnet 5.5: An open multilingual graph
of general knowledge. In Thirty-First AAAI Confer-
ence on Artiﬁcial Intelligence.
Shyam Upadhyay, Nitish Gupta, and Dan Roth. 2018.
Joint multilingual supervision for cross-lingual en-
tity linking. In Proceedings of the 2018 Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 2486–2495.
Ivan Vuli´c, Anna Korhonen, et al. 2016. On the role
of seed lexicons in learning bilingual word embed-
dings. In Proceedings of the 54th Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 1: Long Papers), volume 1, pages 247–257.
Yogarshi Vyas and Marine Carpuat. 2016.
Sparse
bilingual word representations for cross-lingual lex-
ical entailment. In Proceedings of the 2016 Confer-
ence of the North American Chapter of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies, pages 1187–1197.
Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V
Le,
Mohammad Norouzi,
Wolfgang Macherey,
Maxim Krikun,
Yuan Cao,
Qin Gao,
Klaus
Macherey, et al. 2016.
Google’s neural ma-
chine translation system: Bridging the gap between
human and machine translation.
arXiv preprint
arXiv:1609.08144.
Chao Xing, Dong Wang, Chao Liu, and Yiye Lin. 2015.
Normalized word embedding and orthogonal trans-
form for bilingual word translation.
In Proceed-
ings of the 2015 Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
1006–1011.
Wenpeng Yin and Hinrich Sch¨utze. 2015.
Convolu-
tional neural network for paraphrase identiﬁcation.
In Proceedings of the 2015 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies.
Wenpeng Yin, Hinrich Sch¨utze, et al. 2016. Abcnn:
Attention-based convolutional neural network for
modeling sentence pairs. Transactions of the Asso-
ciation for Computational Linguistics, 4(1).
Xinjie Zhou, Xiaojun Wan, and Jianguo Xiao. 2016.
Cross-lingual sentiment classiﬁcation with bilingual
document representation learning. In Proceedings
of the 54th Annual Meeting of the Association for
Computational Linguistics (Volume 1:
Long Pa-
pers), volume 1, pages 1403–1412.

