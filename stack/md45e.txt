Vidi Experiment-2018 (#17692)
Created: 12/11/2018 04:09 AM (PT)
Public:    12/12/2018 12:35 AM (PT)
Author(s)
Müge Simsek (Utrecht University) - M.Simsek@uu.nl
Arnout van de Rijt (Utrecht University) - arnoutvanderijt@gmail.com
Mathijs de Vaan (Haas School of Business, University of California Berkeley) - mdevaan@haas.berkeley.edu
1) Have any data been collected for this study already?
No, no data have been collected for this study yet.
2) What's the main question being asked or hypothesis being tested in this study?
The main research question of the study is: “If NWO panel members in the Vidi competition in Social Sciences and Humanities domain only read the CV and
a summary of the proposal, will they select the same applicants in the preselection as when they get to read the CV and complete proposal?”
The hypotheses that we wish to test in this study are:
1) On a scale from 0 to 10, where 0 is random agreement and 10 is perfect agreement, the agreement between the rankings provided by the shadow
panelists following the regular procedure and the rankings provided by the shadow panelists following the alternative procedure will exceed a score of 5.
Random agreement indicates no association between the rankings. Perfect agreement is defined as the agreement that would be achieved by shuffling
panelists, while holding procedure constant.
2) Reduced information on the proposals introduces more uncertainty about the quality of the proposals and thus more disagreement in panelist ratings
are expected.
3) CVs (academic credentials) are expected to gain more prominence in the ratings given by panelists who receive proposals for consideration in abridged
form compared to those given by panelists who get to assess regular forms of proposals.
4) The ratings provided by the shadow panelists following the alternative procedure will be more likely to be influenced by the private information that the
panelists have about applicants. In other words, not-merit-related factors, such as shared contacts and gender, are expected to contribute more to the
ratings.
3) Describe the key dependent variable(s) specifying how they will be measured.
The key dependent variable is the overall rating given to an application by a panelist. The overall rating ranges from 1 to 9 and is a weighted average of
three other ratings --one of the quality of the researcher (weighted 0.4), one of the quality, innovative character, and academic impact of the proposed
research (weighted 0.4), and one of the knowledge utilization (weighted 0.2).
4) How many and which conditions will participants be assigned to?
Shadow panelists are randomly assigned to condition 1 (regular evaluation procedure including a CV and a complete proposal) or condition 2 (alternative
evaluation procedure including a CV and a summary of the proposal). Overall, applications are reviewed in 3 conditions; regular evaluation procedure by
NWO panelists, regular evaluation procedure by shadow panelists, and alternative evaluation procedure by shadow panelists.
5) Specify exactly which analyses you will conduct to examine the main question/hypothesis.
For hypothesis 1, non-inferiority tests will be used. For hypothesis 2, we will use a non-parametric test to compare the agreement between the rankings
given by shadow panelists in condition 1 and the agreement between the rankings given by shadow panelists in condition 2.To test hypotheses 3 and 4, we
will run cross-classified multilevel regression models to predict the influence that the alternative evaluation procedure has on the effects of academic
credentials and the private information which panelists have about applicants on the ratings given by shadow panelists.
6) Describe exactly how outliers will be defined and handled, and your precise rule(s) for excluding observations.
We will look for extreme values on the ratings to define outliers and will perform sensitivity analyses with and without outliers.
7) How many observations will be collected or what will determine sample size? No need to justify decision, but be precise about exactly how the
number will be determined.
The sample size is dependent on the number of applications received for SSH-Vidi competition 2018 as well as the experimental design (e.g., the number of
shadow panelists, random assignment of the panelists to the conditions, and assignment of applications to the panelists). The latter is determined by NWO.
Accordingly, within shadow panels, applications may receive two ratings from two panelists in different conditions, two panelists in condition 1 or two
panelists in condition 2. Each application is also reviewed by 2 regular NWO panelists. 182 applications are received by NWO, therefore, 728 ratings in total.
8) Anything else you would like to pre-register? (e.g., secondary analyses, variables collected for exploratory purposes, unusual analyses planned?)
Available at https://aspredicted.org/md45e.pdf 
(Permanently archived at http://web.archive.org/web/*/https://aspredicted.org/md45e.pdf)
Version of AsPredicted Questions: 2.00

This study will have policy implications restricted to the main research question posed in the study. 
If we find empirical support for H1 then it means than the alternative evaluation procedure is non-inferior to the regular evaluation procedure. Both
procedures yield a selection of similar applicants, with the alternative procedure being more efficacious. 
If we find no support for H1 then the alternative evaluation procedure is not as good as the regular evaluation procedure. Replacing the regular evaluation
procedure with the alternative procedure will result in a different selection of applicants jeopardizing the quality of the preselection process.
Available at https://aspredicted.org/md45e.pdf 
(Permanently archived at http://web.archive.org/web/*/https://aspredicted.org/md45e.pdf)
Version of AsPredicted Questions: 2.00

