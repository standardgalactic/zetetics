Preprint
LORAMOE: REVOLUTIONIZING MIXTURE
OF EX-
PERTS FOR MAINTAINING WORLD KNOWLEDGE IN
LANGUAGE MODEL ALIGNMENT
Shihan Dou1∗, Enyu Zhou1∗, Yan Liu1, Songyang Gao1, Jun Zhao1, Wei Shen1,
Yuhao Zhou1, Zhiheng Xi1, Xiao Wang1, Xiaoran Fan1, Shiliang Pu2, Jiang Zhu2,
Rui Zheng1, Tao Gui1†, Qi Zhang1†, Xuanjing Huang1
1 NLP Group, Fudan University
2 Hikvision Inc
shdou21@m.fudan.edu.cn, eyzhou23@m.fudan.edu.cn
{rzheng20, tgui, qz}@fudan.edu.cn
ABSTRACT
Supervised fine-tuning (SFT) is a crucial step for large language models (LLMs),
enabling them to align with human instructions and enhance their capabilities in
downstream tasks. When the models are required to align with a broader range
of downstream tasks, or there is a desire to notably improve the performance on
a specific task, a substantial increase in fine-tuning data often emerges as the so-
lution. However, we find that large-scale increases in instruction data can disrupt
the world knowledge previously stored in the LLMs, i.e., world knowledge for-
getting. In this paper, we introduce LoRAMoE to address the above challenge.
The LoRAMoE is a plugin version of Mixture of Experts (MoE). The plugin form
ensures the integrity of world knowledge by freezing the backbone model during
the training phase. We then propose the use of localized balancing constraints to
coordinate parts of experts for task utilization, meanwhile enabling other experts
to fully leverage the world knowledge stored in the models. Experimental results
demonstrate that LoRAMoE can reasonably coordinate experts based on data type
during inference, and even dramatically increasing instruction data does not result
in knowledge forgetting. Moreover, LoRAMoE provides additional benefits for
the performance of downstream tasks, indicating the potential of our approach for
multi-task learning.
1
INTRODUCTION
Large Language Models (LLMs) (Touvron et al., 2023; Muennighoff et al., 2022) have demonstrated
remarkable capabilities in a variety of tasks. Supervised fine-tuning (SFT) of the models to align
them with human instructions is a crucial step in unleashing their full potential (Chung et al., 2022;
Ouyang et al., 2022). Although some works (Zhou et al., 2023; Cao et al., 2023) have indicated that
models can follow human instruction well with a little fine-tuning data, increasing the amount of
data is a straightforward solution when the variety of tasks expands, or when enhanced performance
on a specific task is required (as shown in the left of Figure 1).
However, the large-scale increase in fine-tuning data brings new challenges. Specifically, we ob-
serve a notable decline in performance on the Closed-Book Question Answering (CBQA) dataset
(e.g., TriviaQA (Han et al., 2019), Natural Questions (Kwiatkowski et al., 2019)) when there is a
substantial increase in the amount of fine-tuning data, as shown in the blue group of lines on the right
of Figure 1. We hypothesize that this significant decline in performance may be related to the col-
lapse of world knowledge (Touvron et al., 2023) (i.e., parametric knowledge Neeman et al. (2022))
previously learned and stored in the pre-trained models (Petroni et al., 2019; Yu et al., 2023). This
hypothesis is demonstrated in two steps. First, we verify that the CBQA dataset relies on the world
∗Equal contribution.
† Corresponding author.
1
arXiv:2312.09979v2  [cs.CL]  18 Dec 2023

Preprint
0
100K
3M
Number of Training Samples
20
30
40
50
60
70
80
90
Performance
RTE
MultiRC-ppl
Race-middle
Race-high
ReCoRD
Xsum
5M
Number of Training Samples
0
1M
2M
3M
4M
10
20
30
40
50
Performance
4.0
4.5
5.0
5.5
6.0
Parameters
×10
5
TriviaQA
Filtered TriviaQA
HotpotQA
Filtered NQ
Parameters
Figure 1: (Left) When the number of fine-tuning data increases from 100,000 to 3 million, the
performance of many tasks is significantly improved. (Right) With the amount of instruction data
increasing, fine-tuning (training for an epoch using the same set of hyperparameters) can continually
change model parameters (shown as the red line), resulting in a decline in performance on the
benchmarks that measure world knowledge. The details of training implementation can be seen in
Section 2.1.
knowledge stored in the models for making inferences. Second, we demonstrate that the substantial
decrease in performance on the CBQA dataset is attributed to the fact that large-scale fine-tuning
can markedly change the model’s parameters (as shown in the right of Figure 1), leading to the de-
struction of world knowledge (i.e., knowledge forgetting). Overall, in vanilla supervised fine-tuning,
there is a contradiction between simultaneously improving performance on downstream tasks and
maintaining world knowledge of LLMs.
Findings: Vanilla SFT with massive data can lead to forgetting parametric knowledge stored in
LLMs.
An ideal solution is to delineate a specific region within the model dedicated to storing world knowl-
edge, akin to how the hippocampus in the human brain is specialized for memory (Treves & Rolls,
1994; Voss et al., 2017). Such a structure allows the parameters in this part to be frozen and thus
protected during the fine-tuning process, thereby keeping world knowledge from being disrupted.
However, due to the black-box nature of large language models (Sun et al., 2022; Kaddour et al.,
2023), identifying regions of the world knowledge of LLMs is highly challenging (Feng et al., 2023).
Instead of identifying these regions, another solution is to keep all the parameters of the model by
plugin-based fine-tuning. Specifically, it fine-tunes the model by freezing all the parameters of the
backbone model and binding parameter changes to an additional network. With a complete backup
of the backbone model’s parameters available, the world knowledge stored in it is technically recov-
erable.
However, fine-tuning with a single plugin is similar in form to direct fine-tuning (Ding et al., 2022;
He et al., 2021b). Consequently, the issue of knowledge forgetting persists. Mixture of Experts
(MoE) (Jacobs et al., 1991) is an architecture that introduces multiple experts, where data with dif-
ferent characteristics are routed to the corresponding experts for customized processing (Du et al.,
2022; Shazeer et al., 2016). Drawing on this idea, we hope to introduce multiple plugins as ex-
perts, allowing a part of them the opportunity to access the backup, while another part can perform
downstream tasks.
In this paper, we propose LoRAMoE, to alleviate world knowledge forgetting and simultaneously
enhance the LLMs’ capabilities of solving downstream tasks. LoRAMoE is a plugin version of
MoE. It changes the model’s architecture by adding multiple parallel plugins as experts (i.e. LoRA
Hu et al. (2021)) in each feed-forward layer and connecting them with routers. We then propose the
2

Preprint
use of localized balancing constraint to split the experts in each LoRAMoE layer into distinct groups.
Specifically, one group is dedicated to downstream tasks, while the other focuses on aligning world
knowledge within the backbone model with human instructions to alleviate knowledge forgetting.
Additionally, localized balancing constraint also balances the importance of all experts within the
same expert group, which prevents only a few experts in the same group from being valued by
the routers. It enables several experts to collaborate, improving capabilities of solving downstream
tasks.
Experiment results show that LoRAMoE can effectively keep the world knowledge in language
models from being disrupted by the large-scale fine-tuning. Further, we confirm the effectiveness of
LoRAMoE on capability localization at an interpretable level by visualizing the expert weight for
tasks. Our observations reveal when finishing world knowledge benchmarks, the router pays more
attention to the output of experts specifically handling these tasks. Conversely, for other downstream
tasks, the router focuses on experts from another group. LoRAMoE effectively resolves the conflict
by fostering collaboration among experts. Besides, the experiment result shows that learning on
various downstream tasks also benefits from our method, implying the prospect of our method in
multi-task learning.
Our contributions can be summarized as follows:
1. We find that significantly increasing the amount of supervised fine-tuning data can severely
impair the world knowledge inside the LLMs, due to its great modification to their param-
eters. This indicates that maintaining the world knowledge inside the LLMs is in conflict
with the large-scale addition of downstream fine-tuning data.
2. We introduce a new trainable plugin for LLMs, LoRAMoE, which is similar in architecture
to the Mixture of Experts. LoRAMoE can automatically route different types of data to
respective experts during the SFT phase without interfacing with the original parameter of
LLMs. LoRAMoE employs localized balancing constraint in training, enhancing expert
group specialization and internal balance. It partitions experts into two groups to learn
tasks and align world knowledge with human instructions, reducing knowledge forgetting.
3. We demonstrate the effectiveness of LoRAMoE through extensive experiments. We can
maintain a stable knowledge in the model when scaling up fine-tuning data, during which
performance in other tasks has also seen considerable improvement. Our method is further
evidenced by the visualization of expert utilization.
2
CONFLICT BETWEEN EXPANDING FINE-TUNING DATA AND RETENTION OF
WORLD KNOWLEDGE IN LLMS
In this section, we conduct SFT tasks on the LLM with vast and varied datasets. We find that the
world knowledge inside the LLM is severely compromised during the expansion of SFT.
2.1
IMPLEMENTATION
Datasets.
To understand the impact of large-scale SFT on the world knowledge of the LLM,
we construct a large-scale dataset that includes a variety of tasks. Namely, there are seven tasks,
Closed-Book Question Answering (CBQA), coreference resolution, Natural Language Inference
(NLI), abstract summarization, multi-lingual translation, reading comprehension, and text classifi-
cation. We augmented the training datasets to 5 million through data augmentation methods. More
details about our composition of fine-tuning data can be seen in the Appendix A.1.
Base Model.
We utilize LLaMA-2-7B (Touvron et al., 2023) as the base model, considering it
stands out as one of the most notable and widely used open-source LLMs in current academia.
Evaluation.
For evaluation of the world knowledge, we use CBQA as a key benchmark of
the model’s world knowledge, with reference to the previous work (Touvron et al., 2023; Petroni
et al., 2019). Notably, considering previous work that has noted train-test overlap in CBQA datasets
(Lewis et al., 2020), we elaborately select parts of the CBQA dataset without train-test overlap for
our testing set, namely Filtered NQ and Filtered TriviaQA, to analyze the world knowledge of mod-
3

Preprint
WSC
Winogrande
Flores
Xsum
Race-middle
Race-highRTE
ReCoRD
AX-g
MultiRC-ppl
Filtered NQ
TriviaQA
HotpotQA NQ
Filtered TriviaQA
Task
0
20
40
60
80
Performance
1000
3 Million
5 Million
Data scale
Downstream tasks
World knowledge benchmarks
Figure 2: Performance on the various tasks after expanding the amount of fine-tuning data. For
most of the downstream tasks (e.g., NLI and summarization), with the expansion of training data,
performance on these tasks remains stable after improvement. Whereas, for the world knowledge
benchmark, a significant decline can be witnessed after a large amount of instruction data.
els better. For evaluating the performance on other downstream tasks, we utilize the opencompass1
framework to run the evaluation process on the aforementioned tasks.
2.2
THE EXPANSION OF FINE-TUNING DATA LEADS TO THE KNOWLEDGE FORGETTING
INSIDE THE LLMS
0.0
0.5
1.0
1.5
2.0
2.5
Number of Training Samples
1e5
0
5
10
15
20
25
30
35
40
45
Performance
Filtered TriviaQA
Filtered NQ
HotpotQA
Figure 3:
Performance on world knowledge
benchmarks after training on CBQA solely. Its
performance rises greatly after training with very
few samples and remains relatively stable there-
after.
During the expansion of fine-tuning data, we
observed a diverging trend in the performance
across two types of tasks, as can be seen in Fig-
ure 2:
On some tasks, such as summarization, NLI,
machine translation, etc., the performance fine-
tuned model initially increased magnificently
and stabilized at a promising level.
How-
ever, there was a catastrophic decline in the
model’s performance on the benchmark mea-
suring knowledge capability(e.g.
TriviaQA,
NQ, Hotpot QA), even much lower than the
baseline. Notably, with the training data ex-
panding, a contiguous decline can be witnessed.
Besides, in the filtered test set, the collapse hap-
pens earlier on the filtered test set than the orig-
inal one.
Further, we dissect the reasons behind the de-
cline of the performance in world knowledge
benchmarks with the expansion of fine-tuning data.
The performance on world knowledge benchmarks highly relies on the knowledge and skills
learned during pre-training phase.
1https://opencompass.org.cn/
4

Preprint
To investigate the relationship between world knowledge benchmarks and the knowledge embedded
in pre-trained models, we conduct fine-tuning solely on the CBQA dataset with 250k samples and
run evaluation on the test sets without train-test overlap.
The results in Figure 3 shows that: the performance on these benchmarks can be significantly en-
hanced through naive training, however, the first one percent of the training process (approximately
1000 samples) contributes to most of the boost, and further increases in the training sample do not
actually improve the performance to any great extent.
Actually, this phenomenon is reasonable. In the early stages of fine-tuning, the model quickly learns
to follow human instructions through training data and align the world knowledge already stored in
the model with the instructions (Zhou et al., 2023), thereby increasing the performance of CBQA.
However, due to the limited overlap between the training and testing data, the knowledge in the test
set is difficult to incorporate during training, so more samples do not improve performance.
Therefore, the model’s capability for completing the world knowledge benchmark is highly depen-
dent on the knowledge and skills acquired during pre-training phase. Given this, it is naturally as-
sumed that the markedly diminished performance of the model on CBQA stems from the disruption
of knowledge stored in the LLM due to large-scale instruction tuning.
SFT process with large-scale instruction data disrupts the stored knowledge in LLMs, thus
leading to knowledge forgetting.
To verify this hypothesis, we employed two distinct datasets for sequential fine-tuning of the model.
Initially, the model was fine-tuned using instruction data excluding the CBQA segment. Subse-
quently, we further fine-tuned the model with the CBQA dataset that had been segregated before.
The experimental results are presented in Table 1, where we can observe a notable degradation in
the knowledge capabilities of the fine-tuned model and its performance was inferior compared to the
original LLM. This indicates that the world knowledge within the model was compromised during
the first stage of fine-tuning, resulting in the model’s inability to forge the alignment between human
instructions and the already disrupted knowledge in the subsequent stage of fine-tuning.
Tasks
Baseline
SFT solely
on CBQA
Two-stage
Fine-tuning
TriviaQA
33.5
36.22
13.7
NQ
7.8
12.8
3.6
HotpotQA
11.2
16.1
7.1
Table 1: From left to right are the performance
of LlaMA-2-7B, the model fine-tuned solely on
CBQA datasets and the model first fine-tuned on 3
million instruction datasets excluding CBQA then
continue-trained on CBQA datasets. After con-
tinuing to fine-tune on CBQA dataset, the model
after a large-scale SFT still fails to improve its
knowledge-answering ability and falls far below
the baseline.
Further, we find that there is a massive change
in the LLM’s parameter during the expansion of
fine-tuning data, as can be seen in the right part
of Figure 1. As the previous research has doc-
umented that models store knowledge within
their parameters during the pre-training pro-
cess (Petroni et al., 2019; Roberts et al., 2020;
AlKhamissi et al., 2022), this further indi-
cates the destruction of the knowledge stored in
the parameters during large-scale fine-tuning,
which results in the knowledge forgetting.
Overall, enhancing the instruction following
capability and facilitating the performance on
various downstream tasks of LLM through
large-scale vanilla SFT inherently conflicts at
the parameter level with retaining the world
knowledge stored in the model.
3
METHODOLOGY
Enhancing the language model’s capabilities in various tasks while retaining its world knowledge
is crucial for applying them across various downstream tasks. However, as previously mentioned,
these two objectives conflict at the parameter level with traditional methods. To achieve this goal,
we propose LoRAMoE, an LLM adapter employing the Mixture of Experts (MoE) approach, de-
signed to partition different capabilities within distinct sections of LoRA (Low-Rank Adaptation).
In this section, we initially provide a concise overview of the traditional MoE and LoRA method-
ologies. Subsequently, we delve into how LoRAMoE ingeniously amalgamates the methodologies
5

Preprint
of MoE and LoRA. This integration effectively leverages the strengths of both methods and capably
addresses the conflict issue outlined in section 2.
3.1
PRELIMINARIES
3.1.1
MIXTURE OF EXPERTS
The Mixture of Experts significantly scales up model parameters without correspondingly increasing
computational efforts. For transformers-based large language models, MoE supplants the conven-
tional feed-forward neural network layer in each transformer block with an MoE layer (Shazeer
et al., 2016; Fedus et al., 2021; Lepikhin et al., 2020). This MoE layer is composed of N para-
metrically identical and independent feed-forward neural networks {Ei}N
i=1 as the experts, coupled
with a gating function G(·) as the router. The router is used to model the probability distribution
that governs the weights of outputs from these expert networks. Formally, for the output h of the
attention layer in any given block, the output y of the MoE layer can be mathematically represented
as follows:
y =
N
X
i=1
G(h)iEi(h)
(1)
where Ei(h) and G(h)i denote the output and the corresponding weight of i-th expert in the MoE
layer, respectively. The router G(·) can be written as follows:
G(·) = Softmax(hWg)
(2)
where Wg is the trainable weight matrix for router G(·).
The language model based on MoE architecture facilitates the acquisition of varied competencies by
its experts and their focus on distinct capabilities, achieved through the mechanism of routing (Du
et al., 2022; Riquelme et al., 2021; Bao et al., 2022). Our work draws upon this MoE paradigm and
we fine-tuned the experts to strategically distribute competencies, aiming to address the knowledge-
forgetting issue. To clarify, we freeze the parameters of the backbones and substitute the feed-
forward neural network layer of experts with LoRA to enhance the efficiency of the training process.
3.1.2
LOW-RANK ADAPTION
LoRA (Low-Rank Adapter) has been demonstrated to be an effective and efficient way to adapt
Pre-trained Models to specific tasks (Hu et al., 2021). Formally, for a pre-trained matrix W (e.g.,
attention-k) with the W0 ∈Rdin×dout, LoRA update the W with a low-rank decomposition:
W + ∆W = W + BA, A ∈Rdin×r, B ∈Rr×dout
(3)
where r represents LoRA rank. The forward process with LoRA is as follows, during which the W
is frozen:
h = Wx + α
r BAx
(4)
where α represents a scaling factor that adjusts the magnitude of the changes on the original W
made by LoRA modules. It is worth noting that, the forward process of LoRA is different from
LoRAMoE. In the latter, LoRA is used as the expert network within the MoE architecture, primarily
to facilitate accelerated training and optimization.
3.2
LORAMOE
3.2.1
ARCHITECTURE
Our goal is to tackle the challenge of conflict in the expansion of instruction data with the mainte-
nance of world knowledge inside LLM during the fine-tuning phase. The left of Figure 4 illustrates
the forward process of the standard MoE architecture. In the MoE-based architecture, the router
function focuses on different experts according to the instruction data categories, allowing them to
divide their labor to complete the forward process (Jacobs et al., 1991).
Taking inspiration from that discovery, as shown on the left side of Figure 4, we attempt to extend the
pre-trained language model architecture into an MoE-like architecture to tackle the aforementioned
6

Preprint
Embedding
Transformer
Block
Transformer
Block
Output
Input
Embedding
Input
Multi-Head attention
Add & Norm
Add & Norm
Embedding
Router
Multi-Head attention
Add & Norm
Add & Norm
FFN
FFN
Router
MoE
LoRAMoE
Input
FFN
Figure 4: The architecture of LoRAMoE, compared with classic MoE. LoRAMoE utilizes multiple
LoRAs as adaptable experts and a router to gate them in the FFN layer of every transformer block.
During the training process, only the experts and the router are optimized.
conflict, at the same time retaining its original strong capabilities of language model. Specifically,
we freeze the parameters of the backbone model, allowing experts within LoRAMoE the opportunity
to leverage the existing world knowledge in the base model. For the feed-forward neural network
layer in each transformers block, we define several parallel trainable experts connected by the router.
Simultaneously we replace the fully-connected layer of the expert with a low-rank form to improve
training and inference efficiency.
Formally, for the traditional transformers architecture, the forward propagation process of each de-
coder block can be simplified as follows:
f(x) = x + fFNN(x)
(5)
where fFFN(·) stands for the feed-forward neural network block and x denotes the input of the FFN
block. The matrix operation of the linear layer in the FNN block can be expressed as:
o = Wx = W0x + ∆Wx
(6)
where W0 ∈Rdin×dout represents the parameter matrix and ∆W ∈Rdin×dout denotes the parameter
update in the training phase. o is the output of the linear layer with dimension dout.
To address the conflict issue delineated in Section 2 during the fine-tuning phase, we adopted a
strategy distinct from optimizing a single matrix with the entire instruction dataset. Specifically, we
substituted the linear layer with the MoE architecture. This modification permits the experts within
the MoE layer to collaborate and learn the updated matrix ∆W. Consider the MoE layer comprising
N experts and the set of experts denoted as {Ei}N
i=1, the forward process of the MoE layer can be
mathematically expressed as follows:
o = W0x + ∆Wx = W0x +
N
X
i=1
G(x)iEi(x)
(7)
where Ei(·) and G(·) = Softmax(xWg) represent the i-th expert and the router in the MoE layer,
respectively. The Wg is the trainable parameter matrix of route function. Through this approach,
the experts and routing function work in tandem during the training phase, enabling the experts to
develop varied capabilities and efficiently handle diverse types of tasks.
Concurrently, Low-Rank Adaptation has been proven to be both effective and efficient in the fine-
tuning of pre-trained language models (Wang et al., 2023; Liu et al., 2022; Pan et al., 2022). To
7

Preprint
0
1000
2000
3000
4000
Step
0
1
2
3
4
5
6
Coefficient of Variation
Original
Smoothed
Figure 5: The coefficient of variation for the
experts of the unconstrained LoRAMoE pro-
gressively escalates and sustains at a high level
(i.e., approximately three, similar to the phe-
nomenon observed at Shazeer et al. (2016)),
signifying the prolonged predominance of a
limited number of experts.
Localized Balanced 
Experts
Localized Balanced 
Experts
Imbalanced Experts Groups with Different Capabilities
Figure 6: The experts are segregated into two cat-
egories: one of which concentrates on learning
massive tasks, while another focuses on aligning
the world knowledge inside the LLM and human
instruction. The routing mechanism assigns simi-
lar importance to the experts within a group, and
it selectively activates the group more specialized
for the given data type.
enhance the efficiency and resource conservation of the fine-tuning process, we adapt the parameter
matrix of the experts in the MoE layer into a low-rank format. Specifically, the matrix ∆WE ∈
Rdin×dout of single expert E(·) in the LoRAMoE layer can be written as follows:
∆WE = BA
(8)
where A ∈Rdin×r, B ∈Rr×dout, and the rank r ≪min(din, dout). LoRA contributes to a signif-
icant reduction in the parameters of the experts network, thereby enhancing efficiency and saving
costs during the fine-tuning process. Overall, the forward process of the modified linear layer (i.e.,
LoRAMoE Layer) in the feed-forward neural network layer can be represented as:
o = W0x + γ
N
X
i=1
G(x)iEi(x) = W0x + α
r
N
X
i=1
ωi · BiAix
(9)
where ωi denotes the attention weight of i-th expert in the LoRAMoE layer and α is the constant
hyper-parameter, approximately equivalent to the learning rate (Hu et al., 2021).
3.2.2
MIXED DISTRIBUTION DILEMMAS FOR EXPERT BALANCING
When fine-tuning MoE without any constraints, the router mechanism often converges to a state
in which a small number of experts receive a disproportionately large share of preferences by the
router, as depicted in Figure 5. This imbalance among experts presents a challenge to correct,
as experts that receive greater routing weights in the early stages of training undergo more rapid
optimization, thereby garnering increased preferences from the router. A similar phenomenon has
been documented in the work presented in Shazeer et al. (2016) and Fedus et al. (2021).
A conventional solution for balancing experts utilization involves employing the coefficient of vari-
ation of the experts’ importance as the loss function, aimed at equalizing the significance of each
expert (Shazeer et al., 2016). This solution assumes that the distribution of training samples for
optimising MoE is a single distribution, which inherently eliminates the necessity of considering
the diverse origins of data distribution. Specifically, this traditional approach simplifies the model-
ing process by assuming homogeneity in data sources that often do not align with fine-tuning data
containing both factual knowledge QA and other downstream tasks. Therefore, such simplifica-
tion can lead to significant biases, particularly when encountering datasets with varied distributional
characteristics.
Traditional balancing constraints, which aim to allocate a uniform distribution of training samples
across all experts, can lead to inaccurate parameter estimation. This is because such constraints
8

Preprint
do not account for the intrinsic differences in data representation and importance across various
categories. Recognizing the disparate nature of data distributions, LoRAMoE strategically assigns
data to experts, not uniformly, but based on the observed imbalances. This allocation is governed by
a set of weights that are calibrated to reflect the varying significance and representation of different
data categories within the overall dataset.
Such a specialized allocation method is pivotal in addressing the challenges posed by uneven data
distributions. By tailoring the distribution of training samples to each expert based on the inherent
disparities in the data, LoRAMoE facilitates a more accurate and representative parameter estima-
tion. This nuanced approach to data distribution allows for a more effective fitting of the model
to diverse data subsets, significantly enhancing the model’s predictive accuracy and generalization
capability. This strategy is particularly effective in scenarios where data imbalance could otherwise
lead to skewed learning and generalization errors, ensuring that each data category is appropriately
represented and modeled within the overall system.
To illustrate the concept with a simplified model, let’s assume our training data is sampled from a
mixture of two Gaussian distributions. The means (µ1, µ2) and variances (σ2
1, σ2
2) of these distri-
butions are implicit. The proportion of training data from each distribution is denoted as p1andP2
where p1 + p2 = 1, without loss of generality, we assume that p1 ≤p2. When a MoE model fits
the proposed distribution with balanced weights m, the likelihood of the model given the data can
be expressed as:
L(X) =
Y
x∈X1

m × N

x; µ
′
1, σ
′2
1

+ (1 −m) × N

x; µ
′
2, σ
′2
2

×
Y
x∈X2

m × N

x; µ
′
1, σ
′2
1

+ (1 −m) × N

x; µ
′
2, σ
′2
2

,
(10)
where Card(X1) : Card(X2) = p1 : p2. Using N1(x) and N2(x) for N

x; µ
′
1, σ
′2
1

and
N

x; µ
′
2, σ
′2
2

,
The optimal mean value for µ
′
1 satisfies the following conditions, whose value is 0 when the fitted
distribution is in the same family of mixed distributions N(θ, p1) as the sampling distribution:
∂log L(X)
∂µ′
1
=
X
x∈X1∪X2
∂
∂µ′
1
log (m × N1(x) + (1 −m) × N2(x))
=
X
x∈X1∪X2
(x −µ′
1)
σ′2
1

×
m × N1(x)
m × N1(x) + (1 −m) × N2(x),
(11)
In equation 10, we can replace part of the summation with the empirical estimate of the mean of the
input x. For an ideal routing network, there must exist a distribution Ni such that the data allocated
to this distribution is independently and identically distributed with one of the peaks in the sampling
distribution. Let’s assume this distribution to be N2. In this case, if m ≥p1, then the fitting result
for distribution µ1′ will be µ′
1 = (p1µ1 + (m −p1)µ2)/m. Based on the chain rule of differential
derivation, we end up with:
d log L
dm
= ∂log L
∂µ′
1
× dµ′
1
dm
=
 
X
x∈X1∪X2
(x −µ′
1)
σ′2
1

×
m × N1(x)
m × N1(x) + (1 −m) × N2(x)
!
× p1(µ2 −µ1)
m2
≤0
(12)
The inverse result can be derived similarly. Therefore, the best training error is achieved only when
the mixing coefficient m of the prior distribution is consistent with the actual sampling distribution
weight p1. In the next section, we will introduce a localized balancing constraint algorithm, which
uses adaptive balancing coefficients to explore the optimal data distribution mixing coefficient.
9

Preprint
3.2.3
LOCALIZED BALANCING CONSTRAINT
Considering the aforementioned dilemmas, we introduce a strategic approach as depicted in Fig-
ure 6. Unlike previous efforts that solely focused on distributing the learning data among all experts
evenly, LoRAMoE can methodically segregate the experts into two distinct groups, one of which
concentrates on learning massive tasks, while another focuses on aligning the world knowledge
with instructions (i.e. invoking the world knowledge in the base model to take respond to the human
instructions). While all experts within the same group are balanced, this balance is not uniformly
applied across groups.
Formally, we define the importance matrix Q of LoRAMoE layer and Qn,m denotes the sum of
router values of the n-th expert for the m-th training sample in a batch, which can be represented as
follows:
Qn,m =
Tm
X
j=1
G(xj)i =
exp(ωj
i /τ)
PN
k=1 exp(ωj
i /τ)
(13)
where N and Tm denote the number of experts and the number of tokens of m-th training sample,
respectively. xj is the hidden input of the j-th token to the LoRAMoE layer. We then define the
coefficient matrix I with the same size of Q, corresponding to the importance matrix Q. In,m
denotes the importance coefficient of Qn,m, which can be written as follows:
In,m =

1 + δ,
Typee(n) = Types(m).
1 −δ,
Typee(n) ̸= Types(m).1
(14)
where δ ∈[0, 1] controls the degree of imbalance between experts groups. Typee(n) and Types(m)
are pre-defined target type of n-th expert and the task type of m-th training sample in a batch, re-
spectively. We categorize the instruction data into two distinct groups: CBQA and other downstream
task data. The CBQA serves to enable expert groups to align human instructions with knowledge,
whereas the remaining data is allocated to task-focused experts to boost task performance.
Specifically, suppose that Ii,k and Ij,k denote the importance coefficient of the i-th and j-th expert
for the k-th sample, respectively. Two experts intentionally assigned to the same panel hold equiva-
lent values at corresponding positions in the coefficient matrix (i.e., Ii,k = Ij,k). This implies that
their importance is assigned equal weight. On the contrary, two experts in distinct groups possess
divergent values at corresponding positions in their coefficient matrix (i.e., Ii,k ̸= Ij,k).
We define the localized balancing constraint loss Llbc to quantify the dispersion of the weighted
importance matrix Z = I ◦Q, which can be mathematically represented as:
Llbc = σ2(Z)
µ(Z)
(15)
where σ2(Z) and µ(Z) represent the variance and mean of Z, respectively. Specifically, for a specific
sample, given that experts on the same panel possess identical values in the coefficient matrix I, the
reduction of the optimized loss Llbc results in a progressive equalization of their importance. On the
other hand, experts engaged in distinct panels are assigned differing values in I, thus, the convergent
state of the loss essentially equates to attaining a soft-weighted balance among these different groups
with the I scaling the Q. In this state, the router intensifies its focus on the particular expert group
according to the type of task, yet refrains from fully obscuring other groups, thereby preserving the
capacity for generalization.
Ultimately, optimizing Llbc reduction achieves a balance in the importance of experts within the
same group, while also ensuring different groups concentrate on different capabilities. The overall
loss of LoRAMoE can be represented as follows:
Ltotal = L + βLlbc
(16)
where L is the next-token prediction loss of large language models and Llbc is the localized balanc-
ing constraint loss for all LoRAMoE layers. β controls the strength of localized balancing constraint.
During the fine-tuning phase, we freeze the base model and the trainable parameters are those of
the experts and routers within the LoRAMoE layers. LoRAMoE significantly conserves resources
compared to fine-tuning the full range of parameters of models. In the inference process, the router
autonomously determines and assigns output weights to all experts, thereby obviating the need for
pre-specified data types.
10

Preprint
Task
baseline
SFT solely on
CBQA
SFT
LoRA
LoRAMoE
LoRAMoE
(with Llbc)
WSC
65.4
-
76.0
65.4
71.2
70.2
winogrande
61.7
-
71.2
64.3
66.3
69.6
Flores
0.1
-
24.3
26.6
26.4
25.9
Xsum
19.7
-
34.7
34.5
34.8
33.2
Race-middle
30.5
-
89.1
78.8
84.5
90.0
Race-high
30.4
-
86.1
75.3
80.6
86.5
RTE
52.7
-
88.1
77.3
80.9
87.4
ReCoRD
29.4
-
84.8
83.2
84.3
85.9
AX-g
52.0
-
84.8
76.1
81.7
87.1
multiRC
44.0
-
86.7
81.4
87.3
87.9
TriviaQA
52.2
57.8
51.1
47.8
55.3
58.1
NQ
18.5
28.6
24.5
16.2
23.8
28.0
Filtered TriviaQA
33.5
36.2
21.6
33.4
38.5
35.4
Filtered NQ
7.8
12.8
7.3
11.6
13.4
12.0
hotpot QA
11.2
16.1
13.4
10.7
14.4
16.1
Table 2: Results of LoRAMoE. Contrary to direct full fine-tuning and the use of LoRA-tuning that
exhibits reduced performance on world knowledge benchmarks after training, our approach ensures
simultaneous growth of both world knowledge benchmarks and other downstream tasks.
4
EXPERIMENTS
4.1
EXPERIMENT SETUP
In this section, we introduce the training implementation for LoRAMoE. We only replace the linear
layer in the feed-forward neural network of LLM with the LoRAMoE layer. We initialize each
LoRAMoE layer with six experts, of which three experts are dedicated to addressing downstream
tasks, and the other three are responsible for aligning world knowledge in the base model with the
human instructions. The hyperparameters for control constraint strength β and degree of imbalance
δ are both set to 0.1. For the experts based on low-rank adapter, the α, and r are set to 32 and
four, respectively. The dropout is 0.05, and the learning rate for both the experts and router in the
LoRAMoE layer is 2e −4. The training dataset is the 3 million set the same as the one described
in Section 2.1. We freeze the parameters of the base model, rendering only the experts and router in
LoRAMoE trainable. The global batch size is set to 64, and all our experiments were conducted on
32-card A100 80G.
4.2
RESULTS
Table 2 displays the performance of LoRAMoE with 3 million training samples and compares this
result with the outcomes of directly applying SFT to the model or utilizing LoRA tuning. We report
the results on the same test set as discussed in Section 2.1. The results show that the language
model with LoRAMoE gets good performance on both world knowledge benchmarks and others,
indicating the effectiveness of avoiding knowledge forgetting while improving various tasks.
For world knowledge benchmarks, first of all, the catastrophic collapse observed in Section 2 did not
occur. On the contrary, the model with the LoRAMoE plugin even outperforms the one fine-tuned
solely with the CBQA dataset, such as on TriviaQA and HotpotQA. Besides, compared to vanilla
SFT using the same amount of data, LoRAMoE achieves up to a 64% improvement in world knowl-
edge benchmarks, with an average increase of 35%. By comparing the performance of LoRAMoE
with and without the implementation of the local balance expert loss (the last two columns in Table
2), we observed that Llbc can facilitate the performance on the world knowledge benchmarks.
LoRA tuning is intuitively a method involving smaller degrees of parameter modification than
vanilla SFT (He et al., 2021a). However, it is observed that the forgetting of world knowledge
still occurs. Its performance in CBQA is generally lower than the baseline, and there is an aver-
age decrease of 22% compared to direct single-task tuning. Multiple parallel experts are helpful in
11

Preprint
achieving excellent results in balancing world knowledge retention and the scaling up of fine-tuning
data.
For other downstream tasks, LoRAMoE is capable of achieving performance close to or even sur-
passing that of direct SFT, For instance, in all reading comprehension tasks (i.e., Race, ReCoRD,
multiRC), LoRAMoE achieved superior performance. Besides, Llbc improves outcomes for Lo-
RAMoE in the vast majority of tasks, both world knowledge benchmarks and others. Notably, for
reading comprehension, NLI, and the original CBQA dataset, the benefits of this method were quite
substantial, up to 11.8%. This indicates capability partitioning in the expert group benefits the per-
formance. Further, LoRAMoE holds significant promise for multi-task learning.
4.3
VISUALIZING THE EXPERTS UTILIZATION
Experts group 1
Experts group 2
HotpotQA
Filtered NQ
WSC
Filtered TriviaQA
Flores
Race-high
ReCoRD
Figure 7: Visualization of routers’ weight on dif-
ferent types of data, where Group 1 refers to the
experts dedicated to aligning the world knowledge
in the base model with the human instruction and
Group 2 refers to the experts focus on downstream
tasks. The utilization rate of the group of experts
diverged significantly across tasks.
To confirm the effectiveness of LoRAMoE in
specializing the grouped experts, we visualize
their weight assigned by the router when en-
countered with data from downstream tasks and
knowledge benchmarks respectively, as illus-
trated in Figure 7.
There is a distinct contrast in the utilization
of the two expert groups when dealing with
world knowledge benchmarks and other down-
stream tasks. This suggests that the routers can
automatically allocate specific tasks to experts
with corresponding abilities during the infer-
ence phase. Specifically, the expert group re-
quested to prioritize aligning parametric knowl-
edge with human instructions is greatly em-
ployed in world knowledge benchmarks (e.g.,
TriviaQA, Natural Question, and HotpotQA),
underscoring their vital role in preventing world
knowledge forgetting. This corresponds to the
fact we state in Section 2 that supervised fine-
tuning boosts the model’s capabilities in these tasks by associating pre-stored world knowledge in
the model with human instructions. On the other hand, experts assigned to focus on enhancing
performance in downstream tasks are given increased prominence when encountering these tasks.
Through this visualized result, we find that some downstream tasks still require another group of
experts. It is reasonable. For example, in reading comprehension tasks, the knowledge learned
by the model during pre-training can better assist in making factual judgments. This phenomenon
is even more pronounced in language-based tasks. In the WSC task (Levesque et al., 2012), the
router allocates an average of about 45% of its attention to the expert group responsible for world
knowledge.
5
RELATED WORK
Parameter-Efficient Fine-tuning.
With the significant rise in the number of parameters in lan-
guage models, parameter-efficient fine-tuning (PEFT) (He et al., 2021a) has become an important
research trend. It can consume less resources while fine-tuning the large language models. Sev-
eral methods achieve more efficient fine-tuning of the language model, including prompt LoRA (Hu
et al., 2021), adapters (Houlsby et al., 2019), and prompt learning (Lester et al., 2021). PEFT based
on Lora-rank adapters (Hu et al., 2021) is widely used, which introduce two trainable low-rank
matrices for each fully connected layer, to achieve significant savings in training resources without
adding additional inference computation cost. In this paper, We introduce low-rank to the expert
networks of LoRAMoE to reduce fine-tuning consumption, which increases the number of experts
to improve performance while not significantly increasing computational resources.
Mixture-of-Experts.
Mixture-of-Experts (MoE) (Jacobs et al., 1991) modifies the feed-forward
neural network layer to sparsely activated experts, which significantly enlarges the model without
12

Preprint
remarkably increasing the computational cost. The exploration of MoE has attracted more and more
attention in recent years, including the early sample-level MoE (Jacobs et al., 1991) up to the token-
level MoE (Shazeer et al., 2016; Lepikhin et al., 2020; Du et al., 2022; Riquelme et al., 2021; Fedus
et al., 2021; Xue et al., 2022) that have become mainstream nowadays. Meanwhile some researchers
(Zhou et al., 2022; Chi et al., 2022) aim to investigate the router selection problem in MoE. However,
the vast majority of the works are trying to significantly increase the model parameters and reduce
the computational cost. Differently, Chen et al. (2023) explores the advantages of MoE-based lan-
guage modeling in continuous learning. Our approach is significantly different. We use a MoE-like
structure to address the parametric knowledge retention issue in LLMs, rather than significantly
expanding the count of model parameters.
Multi-LoRA Architecture. Some researchers have also proposed the utilization of multiple LoRAs
to improve the performance of the models or to be more advantageous in some aspects. Huang et al.
(2023) proposed LoraHub which trains several LoRAs and chooses different LoRA combinations
based on data types during the inferencing phase. MOELoRA (Liu et al., 2023) fine-tuned language
models through the incorporation of MoE structures, thereby boosting their efficacy in multitasking
within the medical domain. However, these methods take the data type as the input of the router
during training, which necessitates prior knowledge of the data type during inference, i.e., choosing
the combination of LoRAs based on the data type. Zadouri et al. (2023) introduced Mixture of
Vectors and Mixture of LoRA to reduce the resource consumption of fine-tuning the large language
models and improve the performance on unseen tasks. Sheng et al. (2023) proposed S-LoRA, a
system that can serve thousands of LoRA adapters from a single machine. These methods are
very different from LoRAMoE. We explored for the first time that the expansion of instruction data
seriously damages the knowledge of the pre-trained language models during the fine-tuning phase.
We further propose to address conflicts through the MoE structure and to utilize the LoRA to reduce
resource consumption. Simultaneously LoRAMoE is also an end-to-end approach that does not
require a priori knowledge of data types for inference.
6
CONCLUSION
In this paper, we delve into the consequences of increasing data quantity on the instruction task
during the supervised fine-tuning stage. We uncover that an extensive expansion in data quantity can
critically impair the world knowledge of LLMs, consequently causing world knowledge forgetting.
To address this conflict, we then introduce LoRAMoE, a new MoE-based trainable LLM plugin
that separates networks concentrate on different capabilities with localized balancing constraints.
Extensive experiments have shown that LoRAMoE is adept at resolving this conflict. It preserves
the world knowledge inside the LLMs, while simultaneously enhancing the performance of other
downstream tasks within the instruction data during the fine-tuning process.
REFERENCES
Badr AlKhamissi, Millicent Li, Asli Celikyilmaz, Mona Diab, and Marjan Ghazvininejad. A review
on language models as knowledge bases. arXiv preprint arXiv:2204.06031, 2022.
Hangbo Bao, Wenhui Wang, Li Dong, Qiang Liu, Owais Khan Mohammed, Kriti Aggarwal, Sub-
hojit Som, Songhao Piao, and Furu Wei.
Vlmo: Unified vision-language pre-training with
mixture-of-modality-experts. Advances in Neural Information Processing Systems, 35:32897–
32912, 2022.
Yihan Cao, Yanbin Kang, and Lichao Sun. Instruction mining: High-quality instruction data selec-
tion for large language models. arXiv preprint arXiv:2307.06290, 2023.
Wuyang Chen, Yanqi Zhou, Nan Du, Yanping Huang, James Laudon, Zhifeng Chen, and Claire Cui.
Lifelong language pretraining with distribution-specialized experts. In International Conference
on Machine Learning, pp. 5383–5395. PMLR, 2023.
Zewen Chi, Li Dong, Shaohan Huang, Damai Dai, Shuming Ma, Barun Patra, Saksham Singhal,
Payal Bajaj, Xia Song, Xian-Ling Mao, et al. On the representation collapse of sparse mixture of
experts. Advances in Neural Information Processing Systems, 35:34600–34613, 2022.
13

Preprint
Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li,
Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned lan-
guage models. arXiv preprint arXiv:2210.11416, 2022.
Ning Ding, Yujia Qin, Guang Yang, Fuchao Wei, Zonghan Yang, Yusheng Su, Shengding Hu, Yulin
Chen, Chi-Min Chan, Weize Chen, et al. Delta tuning: A comprehensive study of parameter
efficient methods for pre-trained language models. arXiv preprint arXiv:2203.06904, 2022.
Nan Du, Yanping Huang, Andrew M Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim
Krikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat, et al. Glam: Efficient scaling of language
models with mixture-of-experts. In International Conference on Machine Learning, pp. 5547–
5569. PMLR, 2022.
William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter
models with simple and efficient sparsity. arXiv: Learning,arXiv: Learning, Jan 2021.
Zhangyin Feng, Weitao Ma, Weijiang Yu, Lei Huang, Haotian Wang, Qianglong Chen, Weihua
Peng, Xiaocheng Feng, Bing Qin, et al. Trends in integration of knowledge and large language
models: A survey and taxonomy of methods, benchmarks, and applications.
arXiv preprint
arXiv:2311.05876, 2023.
Francisco Guzm´an, Peng-Jen Chen, Myle Ott, Juan Pino, Guillaume Lample, Philipp Koehn,
Vishrav Chaudhary, and Marc’Aurelio Ranzato. The flores evaluation datasets for low-resource
machine translation: Nepali-english and sinhala-english. arXiv preprint arXiv:1902.01382, 2019.
Moonsu Han, Minki Kang, Hyunwoo Jung, and Sung Ju Hwang. Episodic memory reader: Learning
what to remember for question answering from streaming data. arXiv preprint arXiv:1903.06164,
2019.
Junxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-Kirkpatrick, and Graham Neubig. Towards a
unified view of parameter-efficient transfer learning. Cornell University - arXiv,Cornell Univer-
sity - arXiv, Oct 2021a.
Ruidan He, Linlin Liu, Hai Ye, Qingyu Tan, Bosheng Ding, Liying Cheng, Jia-Wei Low, Lidong
Bing, and Luo Si. On the effectiveness of adapter-based tuning for pretrained language model
adaptation. arXiv preprint arXiv:2106.03164, 2021b.
Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, An-
drea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for nlp.
In International Conference on Machine Learning, pp. 2790–2799. PMLR, 2019.
Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,
and Weizhu Chen.
Lora: Low-rank adaptation of large language models.
arXiv preprint
arXiv:2106.09685, 2021.
Chengsong Huang, Qian Liu, Bill Yuchen Lin, Tianyu Pang, Chao Du, and Min Lin. Lorahub: Effi-
cient cross-task generalization via dynamic lora composition. arXiv preprint arXiv:2307.13269,
2023.
Robert A Jacobs, Michael I Jordan, Steven J Nowlan, and Geoffrey E Hinton. Adaptive mixtures of
local experts. Neural computation, 3(1):79–87, 1991.
Jean Kaddour, Joshua Harris, Maximilian Mozes, Herbie Bradley, Roberta Raileanu, and
Robert McHardy.
Challenges and applications of large language models.
arXiv preprint
arXiv:2307.10169, 2023.
Daniel Khashabi, Snigdha Chaturvedi, Michael Roth, Shyam Upadhyay, and Dan Roth.
Look-
ing beyond the surface: A challenge set for reading comprehension over multiple sentences. In
Proceedings of the 2018 Conference of the North American Chapter of the Association for Com-
putational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pp. 252–262,
2018.
14

Preprint
Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris
Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. Natural questions: a
benchmark for question answering research. Transactions of the Association for Computational
Linguistics, 7:453–466, 2019.
Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. Race: Large-scale reading
comprehension dataset from examinations. arXiv preprint arXiv:1704.04683, 2017.
Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang,
Maxim Krikun, Noam Shazeer, and Zhifeng Chen. Gshard: Scaling giant models with conditional
computation and automatic sharding. In International Conference on Learning Representations,
2020.
Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt
tuning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Pro-
cessing, pp. 3045–3059, 2021.
Hector Levesque, Ernest Davis, and Leora Morgenstern. The winograd schema challenge. In Thir-
teenth international conference on the principles of knowledge representation and reasoning,
2012.
Patrick Lewis, Pontus Stenetorp, and Sebastian Riedel. Question and answer test-train overlap in
open-domain question answering datasets. arXiv preprint arXiv:2008.02637, 2020.
Haokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao Huang, Mohit Bansal, and
Colin A Raffel. Few-shot parameter-efficient fine-tuning is better and cheaper than in-context
learning. Advances in Neural Information Processing Systems, 35:1950–1965, 2022.
Qidong Liu, Xian Wu, Xiangyu Zhao, Yuanshao Zhu, Derong Xu, Feng Tian, and Yefeng Zheng.
Moelora: An moe-based parameter efficient fine-tuning method for multi-task medical applica-
tions. arXiv preprint arXiv:2310.18339, 2023.
Xiaodong Liu, Yu Wang, Jianshu Ji, Hao Cheng, Xueyun Zhu, Emmanuel Awa, Pengcheng He,
Weizhu Chen, Hoifung Poon, Guihong Cao, et al. The microsoft toolkit of multi-task deep neural
networks for natural language understanding. arXiv preprint arXiv:2002.07972, 2020.
Niklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman, Teven Le
Scao, M Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hailey Schoelkopf, et al. Crosslingual gen-
eralization through multitask finetuning. arXiv preprint arXiv:2211.01786, 2022.
Shashi Narayan, Shay B Cohen, and Mirella Lapata.
Don’t give me the details, just the sum-
mary! topic-aware convolutional neural networks for extreme summarization. arXiv preprint
arXiv:1808.08745, 2018.
Ella Neeman, Roee Aharoni, Or Honovich, Leshem Choshen, Idan Szpektor, and Omri Abend.
Disentqa: Disentangling parametric and contextual knowledge with counterfactual question an-
swering. arXiv preprint arXiv:2211.05655, 2022.
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong
Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow
instructions with human feedback.
Advances in Neural Information Processing Systems, 35:
27730–27744, 2022.
Junting Pan, Ziyi Lin, Xiatian Zhu, Jing Shao, and Hongsheng Li. St-adapter: Parameter-efficient
image-to-video transfer learning. Advances in Neural Information Processing Systems, 35:26462–
26477, 2022.
Fabio Petroni, Tim Rockt¨aschel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander H Miller,
and Sebastian Riedel. Language models as knowledge bases? arXiv preprint arXiv:1909.01066,
2019.
Peng Qi, Xiaowen Lin, Leo Mehr, Zijian Wang, and Christopher D Manning. Answering complex
open-domain questions through iterative query generation.
arXiv preprint arXiv:1910.07000,
2019.
15

Preprint
Carlos Riquelme, Joan Puigcerver, Basil Mustafa, Maxim Neumann, Rodolphe Jenatton, Andr´e
Susano Pinto, Daniel Keysers, and Neil Houlsby. Scaling vision with sparse mixture of experts.
Advances in Neural Information Processing Systems, 34:8583–8595, 2021.
Adam Roberts, Colin Raffel, and Noam Shazeer. How much knowledge can you pack into the
parameters of a language model? arXiv preprint arXiv:2002.08910, 2020.
Amrita Saha, Rahul Aralikatte, Mitesh M Khapra, and Karthik Sankaranarayanan.
Duorc: To-
wards complex language understanding with paraphrased reading comprehension. arXiv preprint
arXiv:1804.07927, 2018.
Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adver-
sarial winograd schema challenge at scale. Communications of the ACM, 64(9):99–106, 2021.
Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and
Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. In
International Conference on Learning Representations, 2016.
Ying Sheng, Shiyi Cao, Dacheng Li, Coleman Hooper, Nicholas Lee, Shuo Yang, Christopher Chou,
Banghua Zhu, Lianmin Zheng, Kurt Keutzer, et al. S-lora: Serving thousands of concurrent lora
adapters. arXiv preprint arXiv:2311.03285, 2023.
Tianxiang Sun, Yunfan Shao, Hong Qian, Xuanjing Huang, and Xipeng Qiu. Black-box tuning
for language-model-as-a-service. In International Conference on Machine Learning, pp. 20841–
20855. PMLR, 2022.
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Niko-
lay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open founda-
tion and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.
Alessandro Treves and Edmund T Rolls. Computational analysis of the role of the hippocampus in
memory. Hippocampus, 4(3):374–391, 1994.
Joel L Voss, Donna J Bridge, Neal J Cohen, and John A Walker. A closer look at the hippocampus
and memory. Trends in cognitive sciences, 21(8):577–588, 2017.
Xiao Wang, Tianze Chen, Qiming Ge, Han Xia, Rong Bao, Rui Zheng, Qi Zhang, Tao Gui, and
Xuanjing Huang. Orthogonal subspace learning for language model continual learning. arXiv
preprint arXiv:2310.14152, 2023.
Zhiguo Wang, Wael Hamza, and Radu Florian. Bilateral multi-perspective matching for natural
language sentences. arXiv preprint arXiv:1702.03814, 2017.
Fuzhao Xue, Ziji Shi, Futao Wei, Yuxuan Lou, Yong Liu, and Yang You. Go wider instead of deeper.
Proceedings of the AAAI Conference on Artificial Intelligence, pp. 8779–8787, Jul 2022. doi:
10.1609/aaai.v36i8.20858. URL http://dx.doi.org/10.1609/aaai.v36i8.20858.
Jifan Yu, Xiaozhi Wang, Shangqing Tu, Shulin Cao, Daniel Zhang-Li, Xin Lv, Hao Peng, Zijun
Yao, Xiaohan Zhang, Hanming Li, et al. Kola: Carefully benchmarking world knowledge of
large language models. arXiv preprint arXiv:2306.09296, 2023.
Ted Zadouri, Ahmet ¨Ust¨un, Arash Ahmadian, Beyza Ermis¸, Acyr Locatelli, and Sara Hooker. Push-
ing mixture of experts to the limit: Extremely parameter efficient moe for instruction tuning.
arXiv preprint arXiv:2309.05444, 2023.
Sheng Zhang, Xiaodong Liu, Jingjing Liu, Jianfeng Gao, Kevin Duh, and Benjamin Van Durme.
Record: Bridging the gap between human and machine commonsense reading comprehension.
arXiv preprint arXiv:1810.12885, 2018.
Xiang Zhang, Junbo Zhao, and Yann LeCun. Character-level convolutional networks for text clas-
sification. Advances in neural information processing systems, 28, 2015.
Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat,
Ping Yu, Lili Yu, et al. Lima: Less is more for alignment. arXiv preprint arXiv:2305.11206, 2023.
16

Preprint
Yanqi Zhou, Tao Lei, Hanxiao Liu, Nan Du, Yanping Huang, Vincent Zhao, Andrew M Dai, Quoc V
Le, James Laudon, et al. Mixture-of-experts with expert choice routing. Advances in Neural
Information Processing Systems, 35:7103–7114, 2022.
A
APPENDIX
A.1
DETAILS ABOUT FINE-TUNING DATASETS
Table 3 shows the specific tasks covered by the 3-million-sample dataset we used in Section 2 and
the amount of data for each task. The five million fine-tuning data we use includes three million
versions and their variants from data augmentation strategies. The 1-million-sample version is the
subset of the original 3-million-sample dataset.
Task Name
# train
# test
Task Type
TriviaQA (Han et al., 2019)
78785
254
closed-book QA
Natural Questions (Kwiatkowski
et al., 2019)
104071
357
closed-book QA
Hotpot QA (Qi et al., 2019)
72798
5622
closed-book QA
WSC (Levesque et al., 2012)
554
146
coreference resolution
WinoGrande (Sakaguchi et al.,
2021)
40398
1767
coreference resolution
Flores (Guzm´an et al., 2019)
0
1600
machine translation
WMT 2
500000
-
machine translation
RTE 3
2490
3000
NLI
ReCoRD (Zhang et al., 2018)
100730
10000
reading comprehension
AX-g 4
0
356
NLI
multiRC (Khashabi et al., 2018)
27243
9693
reading comprehension
anli r1/r2/r3 (Liu et al., 2020)
162874
-
NLI
qqp (Wang et al., 2017)
363846
-
NLI
Xsum (Narayan et al., 2018)
204045
11334
single-document
summarization
Race (Lai et al., 2017)
87866
4934
reading comprehension
duorc-selfRC (Saha et al., 2018)
60721
-
reading comprehension
AG-news (Zhang et al., 2015)
120000
-
topic classification
yelp review (Zhang et al., 2015)
650000
-
sentiment classification
openai tldr 5
232188
-
summarization
Table 3: Details about the tasks in our fine-tuning dataset. ”-” means we do not use the test set of
this dataset for evaluation.
17

