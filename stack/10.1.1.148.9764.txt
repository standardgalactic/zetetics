Strange Attractors in Nonlinear Time-delay
Neural Systems
Shyan-Shiou Chen ∗Chang-Yuan Cheng †
Abstract—In this paper, we provide a geometric
construction of a transversal homoclinic orbit for a
nonlinear model neuron with time delays.
The ex-
istence of chaos for time-delay systems in a high-
dimensional space is theoretically conﬁrmed with sev-
eral conditions.
These units may be used as basic
elements for networks with higher-order information
processing capabilities.
Keywords: Nonlinear Time-delay Systems, neural net-
works, Marotto’s chaos, transversal homoclinic orbits
1
Introduction
Recently, there has been an increasing interest in the de-
layed equations: both diﬀerential and diﬀerence. Typ-
ically, for non-delayed discrete dynamical systems, it
is assumed that all causes have an instantaneous ef-
fect. So far, the hypothesis is provided for a great num-
ber of systems which may be described by the diﬀeren-
tial/diﬀerence equations. However, it should be possible
that a meaningful time-scale characteristic of a system is
smaller than a time delay so that a simply model can-
not characterize all information or changes.
Hence, a
model with time delay not only possesses more reason-
able and extensible meaning for a living system but also
oﬀers new modeling possibilities such as optics [1], and
biological systems [2], etc. It means that time delays can
be viewed as intrinsic properties of the nervous system.
Consequentially, the study of neural systems with time
delays is very meaningful.
In 1998, Rabinovich and Abarbanel [3] argued that chaos
itself is not in charge of the function of neural assem-
blies, but the resting state is at the edge of instability
or beyond it.
They thought that neural systems were
transfered/forced to diﬀerent working states by chaotic
oscillations. That is, a behavior (a stable state) always
starts from a resting state (a chaotic state). For exam-
ple, the self-organizing phenomenon [4] can be observed
on a sensorimotor coordination task. Several years ago, a
Nagumo-Sato-based neural network with chaotic behav-
iors was initiated by Aihara et. al. [5] in order to charac-
∗Department of Mathematics, National Taiwan Normal Uni-
versity,
Taipei,
Taiwan
&
Corresponding
Author:
shyansh-
iou.chen@gmail.com
†Department of Applied Mathematics, National PingTung Uni-
versity of Education, Taiwan
terize a number of sophisticated dynamics observed in a
biological neural system. Their behaviors include stable
ﬁxed points, periodic oscillations, and chaos which are
diﬀerent from behaviors of static neural network models.
In 1995, Chen and Aihara [6] proposed that transiently
chaotic neural network (TCNN) with the increase of self-
coupling from a suﬃciently large negative to zero can
mimic adaptable behaviors and successfully applied to
optimization problems. The features of TCNN simulta-
neously contain two parts: stability and chaos [7, 8, 9, 10].
Therefore, TCNN can be exploited to support the view of
Rabinovich and Abarbanel. Recently, Wu and Zhang [11]
initiated a system of two diﬀerence equations coupled
through an excitatory feedback with an integer delay and
conﬁrmed the system possesses larger capacity for asso-
ciative memory than one with no delay item. It mainly
deals with the coexistence of multiple stable patterns such
as multiple equilibria and periodic orbits, lying at the ba-
sis of the mechanism for associative content-addressable
memory storage and retrieval in neural networks, pop-
ulation models, and ecological models, etc. It has been
shown that time delay provides an eﬀective mechanism
for a network to store and retrieve periodic patterns, and
biologically these delays arise due to axonal conduction
time, distances of interneurons and the ﬁnite switching
speeds of ampliﬁers. For the time delay system, a chaotic
behavior near the origin was conﬁrmed by Huang and
Zou [12].
By these two results, it is possible to prove
that neural networks with time delays exist both stabil-
ity and chaos.
Here, we review some developments and modiﬁcations
about discrete-type chaos.
Initially, a chaotic phe-
nomenon was numerically found in the Lorenz’s re-
search [13] on weather prediction in 1963. Until 1975,
the mathematical deﬁnition of chaos was initiated by Li
and Yorke [14] for one-dimensional continuous maps, and
the criterion of existence of chaos is “period three imply
chaos” as follows. Let f : I →I be a continuous map
of the compact interval I of the real line R into itself. If
f has a periodic point of period three, then f exhibits
chaotic behavior. After three years, the above result is
generalized by Marotto [15]. He proposed the deﬁnition
of the “snap-back repeller” and proved that “snap-back
repellers imply chaos” in a multi-dimensional space. The
type is specially called “Marotto’s chaos” or chaos in the
Proceedings of the World Congress on Engineering 2009 Vol II
WCE 2009, July 1 - 3, 2009, London, U.K.
ISBN:978-988-18210-1-0
WCE 2009

sense of Marotto. Later on, Marotto [16] gave some com-
ments about changes, corrections and updates from many
researchers. As an extension of the Li and York theorem
on the one-dimensional maps, Marotto [15] established a
signiﬁcant theorem in conﬁrming the chaotic dynamics
for multi-dimensional systems. With presence of the so-
called “snap-back repeller”, the phase space possesses a
topological structure which includes inﬁnitely many pe-
riodic points and a scrambled set. Very erratic behaviors
of the system then occur, including the lack of global
stability for solutions, and the existence of an uncount-
able collection of orbits which do not eventually approach
any periodic points. Up to now, the theorem is the best
one for analyzing chaos in multi-dimensional maps. The
detailed description of Marotto’s chaos is presented in
Appendix.
In the paper, we propose a simple, modiﬁed model as
the following coupled system with two neurons and time
delays τ1, τ2:
x(n) =µ1x(n −1) + µ2x(n −τ1) + ω11gε(x(n −τ1))
+ ω12gε(y(n −τ2)) + ν1,
(1)
y(n) =µ1y(n −1) + µ2y(n −τ2) + ω22gε(y(n −τ2))
+ ω21gε(x(n −τ1)) + ν2,
(2)
where n (∈N) is the iteration time; τi (∈Z) are time-
delayed parameters larger than one; µi between 0 and
1 are the damping factors of the nerve membrane ; ωii
is the self-feedback connection weight; ωij is the connec-
tion weight from neuron i to neuron j; ε (ε > 0) is the
steepness parameter of the neuronal activation function;
νi is an input bias of neuron i; ρ (−1 < ρ < 0) is a neg-
ative parameter. Herein, i = 1, 2. A saturated output
function gε : R →R satisﬁes the following conditions:
gε(x) =

2 +
x
ε + 1
 −
x
ε −1

 .
4.
(3)
A traditional neural network is extended to one with de-
lay times. In case of τ1 = τ2 = 1, the system (1)-(2) with
Eq. (3) can be viewed as a cellular neural network [17].
We aim to provide the parameter conditions for the exis-
tence of chaos in the time delay system.
The remainder of the paper is organized as follows. A
discrete neural network with one neuron and a time de-
lay is investigated in Section 2.
Herein, some compli-
cated conditions for the existence of chaos in the sense of
Marotto are presented. In Section 3, we proof the exis-
tence of chaos for a delayed discrete neural network with
two neurons.
The generic conditions for the existence
of high-dimensional chaotic phenomena will be described
without constrain of parameters as the system (1)-(2).
Finally, we will draw conclusions in Section 4.
2
A delayed discrete neural network with
one neuron
Let us consider the following simple model of a delayed
discrete neural network (DDNN) with one neuron and a
time delay τ:
x(n) =µ1x(n −1) + µ2x(n −τ) + ω
h
gε(x(n −τ)) + ρ
i
+ ν,
(4)
where |ν| ≤h, −1 < ρ < 0, and a saturated output
function gε is deﬁned as Eq. (3) whose graph is shown in
Figure 1.
y
x
1
-ε
ε
Figure 1: A saturated output function gε for ε > 0.
For a ﬁxed ε > 0, the real line R can be represented as the
union of three disjointed regions Ωℓ= {x ∈R|x < −ε},
Ωm = {x ∈R||x| ≤ε} and Ωr = {x ∈R|x > ε} based
on the piecewise feature of the saturation function gε.
Notably, the neural network (4) is smooth on the three
disjointed intervals. To provide suﬃcient conditions of
ﬁxed points for DDNN (4), the following conditions are
given.
(H1a) 1−µ1−µ2
ω
ε > 0, | 1
2ε| > | 1−µ1−µ2
ω
|,
1−µ1−µ2
ω
ε +
| h
ω | −ρ < 1, 1−µ1−µ2
ω
(−ε) −| h
ω | −ρ > 0.
(H1b)
1−µ1−µ2
ω
ε
<
0,
1−µ1−µ2
ω
ε + | h
ω | −ρ
<
1,
1−µ1−µ2
ω
(−ε) −| h
ω | −ρ > 0.
Condition (H1a) conﬁrms the existence of 3 ﬁxed points
for the family of one-dimensional DDNNs (4). Condition
(H1b) assures that there exists at least one ﬁxed point
for the family of one-dimensional DTDNNs (4). The rea-
sons to give these conditions are shown in the following
lemmas.
Lemma 2.1 Suppose that (H1a) holds. Then, for each
|ν| < h, there exist three points φ0,ℓ∈Ωℓ, φ0,m ∈Ωm and
φ0,r ∈Ωr such that (1−µ1−µ2)φ0,⋆= ω
h
gε(φ0,⋆)+ρ
i
+ν,
where ⋆= “ℓ”, “m”, or “r”.
proof: Assume that there is a ﬁxed point x for a neural
network (4). The equation
x = µ1x + µ2x + ω
h
gε(x) + ρ
i
+ ν
(5)
is hold. Therefore, we can reformulate Eq. (5) as a couple
of equations
y = [(1 −µ1 −µ2)x −ν]/ω −ρ,
(6)
y = gε(x).
Proceedings of the World Congress on Engineering 2009 Vol II
WCE 2009, July 1 - 3, 2009, London, U.K.
ISBN:978-988-18210-1-0
WCE 2009

y
L
x
1
v
−ε
ε
Figure 2: Diagram of ﬁnding a solution of a couple of
equations. Let Lν(x) = [(1 −µ1 −µ2)x −ν]/ω −ρ with
the positive slope 1−µ1−µ2
ω
. A solution exists if the inter-
section between the dotted line and gε is nonempty.
Obviously, Eq. (6) is a linear function of x. These two
equations can be illustrated in the same x-y plane as Fig-
ure 2.
Let ε > 0. Consider ω > 0. If h ≥|ν|, [(1 −µ1 −µ2)ε +
h]/ω−ρ < 1 and [−(1−µ1−µ2)ε−h]/ω−ρ > 0, then [(1−
µ1−µ2)ε−ν]/ω−ρ < 1 and [−(1−µ1−µ2)ε−ν]/ω−ρ > 0.
Hence, there exist three ﬁxed points. Consider ω < 0. If
h > |ν|, [(1 −µ1 −µ2)ε −h]/ω −ρ < 1 and [−(1 −µ1 −
µ2)ε + h]/ω −ρ > 0, then [(1 −µ1 −µ2)ε −ν]/ω −ρ < 1
and [−(1 −µ1 −µ2)ε −ν]/ω −ρ > 0. Hence, there exist
three ﬁxed points. The argument can be supported by
Figure 2 (a). Consequently, all conditions in (H1a) imply
the existence of three ﬁxed points.
Lemma 2.1 shows that, for each diﬀerent parameter ν
with |ν| < h, these three ﬁxed points φ0,ℓ(ν), φ0,m(ν) and
φ0,r(ν) in the interior areas of the corresponding regions
Ωℓ, Ωm and Ωr can be thought as the functions of ν if
six parameters µ1, µ2, ω, ρ, ε and h are compatible with
Condition (H1a). With the property, we can extend the
existence of ﬁxed points for 1-d neural networks to one
for neural networks in high dimensional space in the next
section.
Note that the ﬁrst [resp.
second] superscript
in the symbol φ0,⋆represents a ﬁxed point [resp.
the
position of a ﬁxed point at Ωr, Ωm or Ωℓ].
Lemma 2.2 Suppose that Condition (H1b) holds. Then,
for each |ν| < h, there exists at least one point φ0,m ∈Ωm
such that (1 −µ1 −µ2)φ0,m = ω
h
gε(φ0,m) + ρ
i
+ ν.
Proof: The proof resembles one in Lemma 2.1.
More illustrations for Lemma 2.2 are as follows. Assume
µ2 + ω
2ε < −1 and µ1 = 0. If 0 < µ2 < 1, there exists only
one ﬁxed point φ0,m ∈Ωm. If µ2 > 1, there exists three
ﬁxed points φ0,ℓ∈Ωℓ, φ0,m ∈Ωm and φ0,r ∈Ωr. The
reason of these two conditions is very clear by Figure 3.
Next, to construct a trajectory for a pre-image of one-
dimensional DDNN (4), the following conditions are
given.
(ε,b)
(a ,-ε)
µ2
µ
ω/(2ε)
+
2
µ2
x
y
y = x
ε
-ε
Figure 3: Illustration for Lemma (2.2). Assume that µ2+
ω/(2ε) < −1 and there exist a ﬁxed point in Ωm.
If
0 < u2 < 1, there is no interaction in Ωℓand Ωr.
If
µ2 > 1, there exist a ﬁxed point for every region.
(H2a1) ε > 0, µ2+ ω
2ε > 1, µ2 < −1, µ2ε+ω(1+
ρ) −h > [−ε −ω(1 + ρ) −h]/µ2.
(H2a2) ε > 0, µ2 + ω
2ε > 1, µ2 < −1, µ2(−ε) +
ωρ + h < (ε −ωρ + h)/µ2.
(H2b1) ε > 0, µ2 + ω
2ε < −1, µ2 > 0, (−ε−ωρ−
h)/µ2 > µ2ε+ω(1+ρ)+h, µ2(−ε)+ωρ−h > ε.
(H2b2) ε > 0, µ2+ ω
2ε < −1, µ2 > 0, −µ2ε+ωρ−
h > [ε−ω(1+ρ)+h]/µ2, −ε > µ2ε+ω(1+ρ)+h.
All of conditions (H2a1)-(H2a2) [or brieﬂy (H2a)],
(H2b1)-(H2b2) [or brieﬂy (H2b)] assure the existence of
the pre-image of some types of points for the neural net-
works (4) for a suitably small value µ1, respectively. Fur-
thermore, if there exists a ﬁxed point, then the conditions
can assure the existence of its pre-image point.
For precise explanation for the existence of a snap-back
repeller of 1-d neural network with delayed time 3, we
rewrite the 1-d model as the following equivalent discrete
dynamical system on R3:
Φ(n + 1) = T(Φ(n)),
(7)
where Φ(n) = (φ3(n), φ2(n), φ1(n)), and T : R3 →R3 is
given by
T
0
@
φ3(n)
φ2(n)
φ1(n)
1
A =
0
B
@
φ2(n)
φ1(n)
µ1φ1(n) + µ2φ3(n) + ω
“
gε(φ3(n)) + ρ
”
+ ν
1
C
A .
Note that the subscript i for elements φi (i = 1, 2, 3) of
Φ can be thought as time delays.
For the simpliﬁcation of notation, we deﬁne a map h from
N ∪{0} to {m, r} as follows:
h(z) =
 r
if z ∈{4, 5, 6},
m
otherwise.
(8)
The map is exploited to describe the chosen location of
the pre-image of the neural network (4). The domain of
h means the pre-image number and its range describes
its corresponding and speciﬁed position.
Notably, the
number of the preimage for the neural network (4) may
Proceedings of the World Congress on Engineering 2009 Vol II
WCE 2009, July 1 - 3, 2009, London, U.K.
ISBN:978-988-18210-1-0
WCE 2009

be larger than one. Therefore, a notation φn,h(n) means
that the point is the nth preimage of φ0,h(0) under the
action of the neural network (4) and belongs to the set of
interior points in Ωh(n).
To provide the expanding feature, we give the following
conditions.
(H3a) µ2 + ω
2ε > 1 + |µ1|.
(H3b) µ2 + ω
2ε < −(1 + |µ1|).
Absolute values of eigenvalues of linear part for the 3-d
dynamical system (7) are larger than one if either Con-
dition (H3a) or Condition (H3b) holds.
Lemma 2.3 Suppose
either
(H1a)-(H3a)
or
(H1b)-
(H3b) hold. There exists a sequence {φn,h(n) ∈R|n ∈
N ∪{0}} which approaches to a ﬁxed point φ0,h(0) as
n →∞. Hence, for neural network (7), the constructed
orbit is a transversal homoclinic orbit and the ﬁxed point
φ0,h(0) is a snap-back repeller.
proof: Let us consider the neural network (7).
Since
(H1a) holds, it follows from Lemma (2.1) that there
exists a ﬁxed point Φ0 = (φ0,h(0), φ0,h(0), φ0,h(0)) for
Eq. (7), where φ0,h(0) ∈Ω0
h(0).
Therefore, there ex-
ists eµ1
1 > 0 such that φ0,h(0) −µ1φ0,h(0) ∈Ω0
h(0) for
all µ1 ∈eU1 where eU1 = [−eµ1
1, eµ1
1].
By (H2a), there
exists a point φ1,h(1) ∈Ω0
h(1) which satisﬁes φ0,h(0) −
µ1φ0,h(0)
= µ2φ1,h(1) + ω[gε(φ1,h(1)) + ρ] + ν.
Let
φ2,h(2) = φ1,h(1), Φ−1 = (φ1,h(1), φ0,h(0), φ0,h(0)) and
Φ−2 = (φ2,h(2), φ1,h(1), φ0,h(0)). Note that T(Φ−1) = Φ0
and T(Φ−2) = Φ−1. For an integer n > 2, we deﬁne
Φ−n = (φn,h(n), φn−1,h(n−1), φn−2,h(n−2))
(9)
whose components satisfy the following equation:
φn−3,h(n−3)−µ1φn−2,h(n−2) = µ2φn,h(n)+ω[gε(φn,h(n))+ρ]+ν.
For k = 1, · · · , n −1, assume a positive value eµk
1
and φk,h(k) ∈Ω0
h(k) exists with µ1 ∈∩n−1
k=1 eUk where
eUk = [−µk
1, µk
1].
By induction, there exists a suitable
eµn
1 > 0 such that φn,h(n) ∈Ω0
h(n) exists if µ1 ∈∩n
k=1 eUk
where eUk = [−eµk
1, eµk
1].
The linear part DT of T at
Φ = (φ1, φ2, φ3) is


0
1
0
0
0
1
µ2 + ωg
′(φ3)
0
µ1

.
Note that φk,h(k)
∈Ω0
m as k
≥6.
Hence, µ2 +
ωg
′(φk,h(k) = µ2 + ω
2ε. By (H3a) and Lemma (4.1), ab-
solute values of eigenvalues of DT are larger than one as
φ3 ∈Ωm. By Chen [18], there exists a norm |·| in R3 such
that |DT(Φ)ω| ≥s|ω| for all ω ∈R3 and φ3 ∈Ωm for
some s = 1+ρ > 1. Hence, there exists a suﬃciently small
radius d > 0 such that if Φ
′ belongs to a d-ball neighbor-
hood of Φ0, then |T−1(Φ0)−T−1(Φ
′)| < 1
s|Φ0−Φ
′|. That
is, the ﬁxed point Φ0 is stable and attractive for the map
T−1. That is, if a point φN,h(N) locates in the basin of
Φ0 for the map T−1, then φn,h(n) approaches φ0,h(0) for
n ≥N. Actually, the sequence {φn,h(n) ∈R|n ∈N∪{0}}
is a transversal homoclinic orbit of φ0,h(0) for the neural
networks, Eq. (4).
3
A delayed discrete neural network with
two neurons
Let us start with a DDNN with two neurons as follows:
x1(n)
=
µ1x1(n −1) + µ2x1(n −τ1) + ω11[gε(x1(n −τ1)) −a1]
+ω12gε(x2(n −τ2)) + ν1,
(10)
x2(n)
=
µ1x2(n −1) + µ2x2(n −τ3) + ω22[gε(x2(n −τ3)) −a2]
+ω21gε(x1(n −τ4)) + ν2,
(11)
where, for simpliﬁcation, we let τ1 = τ2 = τ3 = τ4 =
τ ∈N, a1 = a2 = −ρ, ω11 = ω22 = ω, n ∈N, and
gε : R →R is a saturated output function as Eq. (3).
Besides, the neural network (10)-(11) can be transformed
as the following equivalent discrete map on R2τ:
Φ(n + 1) = T(Φ(n)),
(12)
where Φ = (Φ1, Φ2) with Φ1 = (φ1
τ, · · · , φ1
1), Φ2 =
(φ2
τ, · · · , φ2
1), and T : R2τ →R2τ is given by
T
0
B
B
B
B
B
B
B
B
B
B
B
B
B
@
φ1
τ
...
φ1
2
φ1
1
φ2
τ
...
φ2
2
φ2
1
1
C
C
C
C
C
C
C
C
C
C
C
C
C
A
=
0
B
B
B
B
B
B
B
B
B
B
B
B
B
@
φ1
τ−1
...
φ1
1
µ1φ1
1 + µ2φ1
τ + ω[gε(φ1
τ) + ρ] + ω12gε(φ2
τ) + ν1
φ2
τ−1
...
φ2
1
µ1φ2
1 + µ2φ2
τ + ω[gε(φ2
τ) + ρ] + ω21gε(φ1
τ) + ν2
1
C
C
C
C
C
C
C
C
C
C
C
C
C
A
,
where T = (T 1
τ , · · · , T 1
1 , T 2
τ , · · · , T 2
1 ).
Theorem 3.1 If the parameters satisfy (H1a) [resp.
(H1b)], then there exists one ﬁxed point in Ωj1···jn [resp.
Ωm···m] for the neural network (10)-(11).
Proof: Consider a ﬁxed region Ωj1···jn for certain ji = “ℓ”
or “m” or “r” (i = 1, · · · , n).
Let (eξ1, · · · , eξn) be any
point in Rn. Then, by Lemma 2.1, there exist ξℓ
i ∈Ωℓ,
ξm
i ∈Ωm, ξr
i ∈Ωr such that
ξ∗
i = µ1ξ∗
i + µ2ξ∗
i + ω[gε(ξ∗
i ) + ρ] +
n
X
j=1,j̸=i
ωijgε(eξj) + νi,
(13)
where “ ∗” =“ℓ”, “m”, “r” for each i. Restated, each
ξ∗
i is a ﬁxed point of the one-dimensional map ξi 7→
µ1ξi + µ2ξi + ω[gε(ξi) + ρ] + Pn
j=1,j̸=i ωijgε(eξj) + νi. Let
H : Ωj1···jn 7→Ωj1···jn be deﬁned by H(eξ1, · · · , eξn) =
Proceedings of the World Congress on Engineering 2009 Vol II
WCE 2009, July 1 - 3, 2009, London, U.K.
ISBN:978-988-18210-1-0
WCE 2009

(ξj1
1 , · · · , ξjn
n ). We want to show that there exists a ﬁxed
point for H. Deﬁne G : Rn × Rn →Rn by
Gi(ex, x) = xi −µ1xi −µ2xi −ω[gε(xi)+ρ]−
n
X
j=1,j̸=i
ωijgε(exj)−νi,
where G = (G1, · · · , Gn), ex = (ex1, · · · , exn), i =
1, · · · , n. Notably, G(ex, H(ex)) = 0 for every ex ∈Ωj1···jn,
by Eq. (13). Now,
∂G
∂x = diag[χ1, · · · , χn],
where χi = 1−µ1 −µ2 −ωg
′
ε(xi), i = 1, · · · , n. Note that
g
′
ε = 0 in regions Ωr and Ωℓ, g
′
ε =
1
2ε in the interior region
Ωm. For x = (x1, · · · , xn) ∈Ωj1···jn, we have χi(xi) =
1−µ1 −µ2 −ω
2ε or 1−µ1 −µ2 for each i whose values are
nonzero due to the condition 0 < (1−µ1−µ2)/ω < 1/(2ε).
Hence, the Jacobian of G with respect to x is nonzero.
It follows from the implicit function theorem that H is
a C1 function on the region Ωj1···jn. Furthermore, there
exists one ﬁxed point x of H in Ωj1···jn, which is also a
ﬁxed point of T. In fact, there exists only one ﬁxed point
in each Ωj1···jn. Consequently, there are 3n ﬁxed points
of T in Rn.
Theorem 3.2 If the parameters satisfy either (H1a-
H3a) or (H1b-H3b), then there exists a snap-back repeller
in the interior Ωm···m for the neural network (10)-(11).
Proof: For simpliﬁcation, τ = 3 is given.
Neural net-
works (10)-(11) becomes
x1(n) = µ1x1(n −1) + µ2x1(n −3) + ω[gε(x1(n −3)) + ρ]
+ ω12gε(x2(n −3)) + ν1,
(14)
x2(n) = µ1x2(n −1) + µ2x2(n −3) + ω[gε(x2(n −3)) + ρ]
+ ω21gε(x1(n −3)) + ν2.
(15)
By Theorem 3.1, we know that there exists a ﬁxed point
(φ0,m
1
, φ0,m
2
) ∈Ωmm for the full couple of neural net-
works (14) and (15). Let (ξ
′
1, ξ
′
2) be any point in Ωrr. If
these parameters (µ1, µ2, ω, ε, ρ, ν), i = 1, 2, satisfy Con-
dition (H2a), there exists a point (ξ1, ξ2) ∈Ωrr such that
µ2ξ1 + ω[gε(ξ1) −a01] + ω12gε(ξ
′
2) + a1 = (1 −µ1)φ0,m
1
,
µ2ξ2 + ω[gε(ξ2) −a02] + ω21gε(ξ
′
1) + a2 = (1 −µ1)φ0,m
2
.
Let H : Ωj1j2
7→Ωj1j2 be deﬁned by H(eξ1, eξ2) =
(ξj1
1 , ξj2
n ).
We want to show that there exists a ﬁxed
point for H.
Deﬁne a map G : R2 × R2 →R2 with
G = (G1, G2) by
G1(x
′, x) = (1 −µ1)φ0,m
1
−µ2x1 −ω[gε(x1) −a01] −ω12gε(x
′
2) −a1,
G2(x
′, x) = (1 −µ1)φ0,m
2
−µ2x2 −ω[gε(x2) −a02] −ω21gε(x
′
1) −a2,
where x = (x1, x2) and x
′ = (x
′
1, x
′
2). Then
(∂G/∂x)(x
′, x) =
 −µ2 −ω/2ε
0
0
−µ2 −ω/2ε

.
The Jacobian of G with respect to x is nonzero, pro-
vided that Condition (H3a) holds. It follows from the
implicit function theorem that H is a C1 function on the
region Ωj1j2. Furthermore, there exists one ﬁxed point
ex of H in Ωj1j2 by Brouwer’s ﬁxed point theorem. By
the implicit function theorem, there exists a point ex =
(φ1,h(1)
1
, φ1,h(1)
2
) ∈Ωrr such that G(ex, ex) = 0. Denoting
Φ−1 = (Φ−1
1 , Φ−1
2 ) with Φ−1
1
= (φ1,h(1)
1
, φ0,h(0)
1
, φ0,h(0)
1
)
and Φ−1
2
=
(φ1,h(1)
2
, φ0,h(0)
2
, φ0,h(0)
2
),
it follows that
T(Φ−1)
=
Φ0.
By the same certiﬁcation,
it is
easy to construct a series {Φ−i|i = 2, 3, 4, · · · } where
Φ−k
1
=

φk,h(k)
1
, φk−1,h(k−1)
1
, φk−2,h(k−2)
1

and Φ−k
2
=

φk,h(k)
2
, φk−1,h(k−1)
2
, φk−2,h(k−2)
2

with an integer k ≥2.
Recall that h is deﬁned as Eq. (8). Furthermore, the Ja-
cobian matrix of T−1 in the region Ωm,··· ,m is the inverse
of the matrix
DT (Φ) =
0
B
B
B
B
B
B
@
0
1
0
0
0
0
0
0
1
0
0
0
µ2 + ωg
′
ε(φ1
3)
0
µ1
β2g
′
ε(φ2
3)
0
0
0
0
0
0
1
0
0
0
0
0
0
1
β2g
′
ε(φ1
3)
0
0
µ2 + ωg
′
ε(φ2
3)
0
µ1
1
C
C
C
C
C
C
A
.
The absolute values of all of its eigenvalues are larger
than one by Gerschgorin’s theorem for suﬃciently small
β2. Therefore, the series approaches the ﬁxed point in the
region Ωm,··· ,m under the action of T. In other words, a
homoclinic orbit for the neural network (10)-(11) is con-
structed. This ﬁxed point Φ0 is a snap-back repeller.
4
Conclusion
In the paper, we successfully develop a constructive
method to explore the existence of a transversal homo-
clinic orbit for the system (1)-(2) with Eq. (3). The theo-
rems based on the constructive method are theoretically
supported by applying Marotto’s theorem and have given
suﬃcient conditions for the existence of both ﬁxed points
and their homoclinic orbits. The simple system with a
delay item can demonstrate very complicated behavior
near the origin. The analysis has indicated that, as mul-
tiple ﬁxed points coexist, their homoclinic orbits position
themselves in a tangle. In addition, the number of ﬁxed
points can grow exponentially in the number of neurons.
This scenario has revealed the complication of the dy-
namics for the system. It is believed that more dynamical
features other than snap-back repellers can be explored
along this line of investigation. It is expected that the
present work has great potential in many applications.
Appendix A: Marotto’s Theorem
Consider a dynamical system: x 7→F(x), xk ∈Rn and
F is in C1(Rn, Rn) or piecewise C1. Suppose x is a ﬁxed
point of F with all eigenvalues of DF(x) exceeding 1 in
magnitude, and suppose there exists a point x0 ̸= x in a
repelling neighborhood of x, such that Fm(x0) = x and
det(DFm(x0)) ̸= 0, for some 1 < m ∈N.
Then x is
Proceedings of the World Congress on Engineering 2009 Vol II
WCE 2009, July 1 - 3, 2009, London, U.K.
ISBN:978-988-18210-1-0
WCE 2009

called a snap-back repeller [16] of F. If F has a snap-back
repeller, then the system of F is chaotic in the following
sense: (i) There exists a positive integer m0 such that
for each integer p ≥m0, F has p-periodic points. (ii)
There exists a scrambled set, that is, an uncountable set
L containing no periodic points such that the following
pertains: (a) F(L) ⊂L; (b) for every y ∈L and any
periodic point x of F,
lim sup
m→∞||Fm(y) −Fm(x)|| > 0;
(c) for every x, y ∈L with x ̸= y,
lim sup
m→∞||Fm(y) −Fm(x)|| > 0;
(3) There exists an uncountable subset L0 of L such that
for every x, y ∈L0,
lim inf
m→∞||Fm(y) −Fm(x)|| = 0.
Appendix B
Let A = (aij) be a k×k matrix where aj,j+1 = 1, ak,1 = a
and ak,k = b; otherwise, ai,j = 0.
The characteristic
polynomial of A is λk −bλk−1 −a.
Lemma 4.1 Consider
λk −bλk−1 −a = 0,
(16)
where a, b are real numbers and k ≥2 is an integer. If
(a) a > 1, |b| < a −1, or (b) a < −1, |b| < −a −1, then
all of λ’s solutions have absolute values larger than 1.
References
[1] Giacomelli, G., Meucci, R., Politi, A., Arecchi, F. T.,
“Defects and spacelike properties of delayed dynam-
ical systems,” Phys. Rev. Lett., V73, N8, pp. 1099–
11 002, 1994.
[2] Mackey, M. C., Glass, L., “Oscillation and chaos
in physiological control systems,” Science, V197,
N4300, pp. 287–289, 1977.
[3] Rabinovich, M. I., Abarbanel, H. D. I., “The role of
chaos in neural systems,” Neurosience, V87, N1, pp.
5–14, 1998.
[4] Chen, S. S., Chen, L. F., Wu, Y. T., Wu, Y. Z.,
Lee, P. L., Yeh, T. C., Hsieh, J. C., “Detection of
synchronization between chaotic signals: an adap-
tive similarity-based approach,” Physical Review E,
V76, N6, pp. 066 208–1, 2007.
[5] Aihara, K., Takabe, T., Toyoda, M., “Chaotic neural
networks,” Phys. Lett. A, V144, N6, pp. 333–340,
1990.
[6] Chen, L., Aihara, K., “Chaotic simulated anneal-
ing by neural network model with transient chaos,”
Neural Networks, V8, N6, pp. 915–930, 1995.
[7] ——, “Chaos and asymptotical stability in discrete-
time neural networks,” Phys. D, V104, pp. 286–325,
1997.
[8] Chen, S. S., Shih, C. W., “Transversal homoclinic or-
bits in a transiently chaotic neural network,” Chaos,
V12, N3, pp. 654–671, 2002.
[9] ——, “Asymptotic behaviors in a transiently chaotic
neural network,” Discrete and continuous dynamical
systems, V10, N3, pp. 805–826, 2004.
[10] ——,
“Transiently
chaotic
neural
networks
with
piecewise
linear
output
functions,”
Chaos
Solitons
Fractals,
vol.
accepted,
p.
doi:10.1016/j.chaos.2007.01.103, 2007.
[11] Wu, J., Zhang, R. Y., “A simple delayed neural net-
work for associative memory with large capacity,”
Disc. Contin. Dynam. Syst. Ser. B, V4, N3, pp. 853–
865, 2004.
[12] Huang, Y., Zou, X., “Co-existence of chaos and sta-
ble periodic orbits in a simple discrete neural net-
work,” J. Nonlinear Sc., V15, pp. 291–303, 2005.
[13] Lorenz, E., “Deterministic nonperiodic ﬂow,” J. of
Atmospheric Science, V20, pp. 130–141, 1963.
[14] Li, T., Yorke J., “Periodic three implies chaos,” Am
Math Monthly, V82, pp. 985–992, 1975.
[15] Marotto, F. R., “Snap-back repellers imply chaos in
Rn,” J. Math. Anal. Appl., V63, pp. 199–223, 1978.
[16] ——, “On redeﬁning a snap-back repeller,” Chaos
Solitons Fractals, V25, pp. 25–28, 2005.
[17] Chua, L. O., Yang, L., “Cellular neural networks:
Theory,” IEEE Trans. Circuits Sys. I, V35, pp.
1257–1272, 1988.
[18] Chen, G., Hsu, S. B., Zhou, J., “Snapback repellers
as a cause of chaotic vibration of the wave equation
with a van der pol boundary condition and energy
injection at the middle of the span,” J. Math. Phys.,
V39, N12, pp. 6459–6489, 1998.
Proceedings of the World Congress on Engineering 2009 Vol II
WCE 2009, July 1 - 3, 2009, London, U.K.
ISBN:978-988-18210-1-0
WCE 2009

