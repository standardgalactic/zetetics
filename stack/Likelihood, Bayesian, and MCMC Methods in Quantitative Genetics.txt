Likelihood, Bayesian,
and MCMC Methods
in Quantitative Genetics
Daniel Sorensen
Daniel Gianola
Springer

Statistics for Biology and Health
Series Editors
K. Dietz, M. Gail, K. Krickeberg, J. Samet, A. Tsiatis

Daniel Sorensen
Daniel Gianola
Likelihood, Bayesian,
and MCMC Methods
in Quantitative Genetics

Daniel Sorensen
Daniel Gianola
Department of Animal Breeding
Department of Animal Science
and Genetics
Department of Dairy Science
Danish Institute of Agricultural Sciences
Department of Biostatistics and Medical
DK-8830 Tjele
Informatics
Denmark
University of Wisconsin-Madison
sorensen@inet.uni2.dk
Madison, WI 53706
USA
gianola@calshp.cals.wisc.edu
Series Editors
K. Dietz
M. Gail
K. Krickeberg
Institut fu¨r Medizinische Biometrie
National Cancer Institute
Le Chatelet
Universita¨t Tu¨bingen
Rockville, MD 20892
F-63270 Manglieu
Westbahnhofstrasse 55
USA
FRANCE
D-72070 Tu¨bingen
GERMANY
J. Samet
A. Tsiatis
School of Public Health
Department of Statistics
Department of Epidemiology
North Carolina State University
Johns Hopkins University
Raleigh, NC 27695
615 Wolfe Street
USA
Baltimore, MD 21205-2103
USA
Library of Congress Cataloging-in-Publication Data
Sorensen, Daniel.
Likelihood, Bayesian and MCMC methods in quantitative genetics / Daniel Sorensen,
Daniel Gianola.
p. cm. — (Statistics for biology and health)
Includes bibliographical references and index.
ISBN 0-387-95440-6 (alk. paper)
1. Genetics—Statistical methods.
2. Monte Carlo method.
3. Markov processes.
4. Bayesian statistical decision theory.
I. Gianola, Daniel, 1947–
II. Title.
III. Series.
QH438.4.S73 S675 2002
575.6′07′27—dc21
2002019555
ISBN 0-387-954406
Printed on acid-free paper.
2002 Springer-Verlag New York, Inc.
All rights reserved. This work may not be translated or copied in whole or in part without the
written permission of the publisher (Springer-Verlag New York, Inc., 175 Fifth Avenue, New York,
NY 10010, USA), except for brief excerpts in connection with reviews or scholarly analysis. Use
in connection with any form of information storage and retrieval, electronic adaptation, computer
software, or by similar or dissimilar methodology now known or hereafter developed is forbidden.
The use in this publication of trade names, trademarks, service marks and similar terms, even if
they are not identified as such, is not to be taken as an expression of opinion as to whether or not
they are subject to proprietary rights.
Printed in the United States of America.
9 8 7 6 5 4 3 2 1
SPIN 10866246
www.springer-ny.com
Springer-Verlag
New York Berlin Heidelberg
A member of BertelsmannSpringer Science+Business Media GmbH

Preface
Statistical genetics results from the merger of genetics and statistics into a
coherent quantitative theory for predicting and interpreting genetic data.
Based on this theory, statistical geneticists developed techniques that made
notable contributions to animal and plant breeding practices in the second
half of the last century as well as to advances in human genetics.
There has been an enormous research impetus in statistical genetics over
the last 10 years. Arguably, this was stimulated by major breakthroughs
in molecular genetics, by the advent of automatic data-recording devices,
and by the possibility of applying computer-intensive statistical methods
to large bodies of data with relative ease. Data from molecular biology
and biosensors are characterized by their massive volume. Often, intricate
distributions need to be invoked for appropriate modeling. Data-reduction
techniques are needed for accounting for the involved nature of these data
and for extracting meaningful information from the observations. Statisti-
cal genetics plays a major role in this process through the development,
implementation, and validation of probability models for inference. Many
of these models can be daunting and, often, cannot be ﬁtted via standard
methods. Fortunately, advances in computing power and computer-based
inference methods are making the task increasingly feasible, especially in
connection with likelihood and Bayesian inference.
Two important breakthroughs in computational statistics have been the
bootstrap and Markov chain Monte Carlo (MCMC) methods. In this book
we focus on the latter. MCMC was introduced into the statistical literature
in the late 1980s and early 1990s, and incorporation and adaptation of the
methods to the needs of quantitative genetic analysis was relatively rapid,

vi
particularly in animal breeding. Also, MCMC is having a major impact in
applied statistics (especially from a Bayesian perspective), opening the way
for posing models with an enormous amount of ﬂexibility. With MCMC,
it is possible to arrive at better descriptions of the perceived underlying
structures of the data at hand, free from the strictures of standard methods
of statistical analysis.
The objective of this book is to present the main ideas underlying like-
lihood and Bayesian inference and MCMC methods in a manner that is
accessible to numerate biologists, giving step-by-step derivations and fully
worked-out examples. Most of these examples are from quantitative genet-
ics and, although not exclusively, we focus on normal or generalized linear
models.
Most students and researchers in agriculture, biology, and medicine lack
the background needed for understanding the foundations of modern bio-
metrical techniques. This book has been written with this particular read-
ership in mind. A number of excellent books describing MCMC methods
have become available in recent years. However, the main ideas are pre-
sented typically in a technically demanding style, as these books have been
written by and addressed to statisticians. The statistician often has the
mathematical background needed to “ﬁll in the blanks”. What is tedious
detail to a statistician, so that it can be omitted from a derivation, can
cause considerable consternation to a reader with a diﬀerent background.
In particular, biologists need a careful motivation of each model from a
subject matter perspective, plus a detailed treatment of all the algebraic
steps needed to carry out the analysis. Cavalier statements such as “it fol-
lows immediately”, or “it is easy to show”, are encountered frequently in
the statistical literature and cause much frustration to biological scientists,
even to numerate ones. For this reason, we oﬀer considerably more detail in
the developments than what may be warranted for a more mathematically
apt audience. We do not apologize for this, and hope that this approach
will be viewed sympathetically by the scientiﬁc community to which we
belong. Nevertheless, some mathematical and statistical prerequisites are
needed in order to be able to extract maximum beneﬁt from the mate-
rial presented in this book. These include a beginning course in diﬀerential
and integral calculus, an exposure to elementary linear algebra (preferably
with a statistical bent), an understanding of probability theory and of the
concepts of statistical inference, and a solid grounding in the applications
of mixed eﬀects linear models. Most students of quantitative genetics and
animal breeding acquire this preparation during the ﬁrst two years of their
graduate education, so we do not feel that the requirements are especially
stringent. Some applied statisticians reading this book may be caught by
the quantitative genetics jargon. However, we attempt to relate biological
to statistical parameters and we trust that the meaning will become clear
from the context.

vii
The book is organized into four parts. Part I (Chapters 1 and 2) presents
a review of probability and distribution theory. Random variables and their
distributions are introduced and illustrated. This is followed by a discus-
sion on functions of random variables. Applied and theoretical statisticians
can skip this part of the book safely, although they may ﬁnd some of the
examples interesting.
The ﬁrst part lays the background needed for introducing methods of
inference, which is the subject of the seven chapters in Part II. Chapters
3 and 4 cover the classical theory of likelihood inference. Properties of
the maximum likelihood estimator and tests of hypotheses based on the
Neyman–Pearson theory are discussed. An eﬀort has been made to derive,
in considerable detail, many of the important asymptotic results and sev-
eral examples are given. The problems encountered in likelihood inference
under the presence of nuisance parameters are discussed and illustrated.
Chapter 4 ends with a presentation of models for which the likelihood does
not have a closed form. Bayesian inference is the subject of chapters 5-8
in Part II . Chapter 5 provides the essential ingredients of the Bayesian
approach. This is followed by a chapter covering in fair detail the analysis
of the linear model. Chapter 7 discusses the role of the prior distribution
in Bayesian analysis. After a short tour of Bayesian asymptotics, the con-
cepts of statistical information and entropy are introduced. This is followed
by a presentation of Bayesian analysis using prior distributions conveying
vague prior knowledge, perhaps the most contentious topic of the Bayesian
paradigm. The chapter ends with an overview of a technically diﬃcult topic
called reference analysis. Chapter 8 deals brieﬂy with hypothesis testing
from a Bayesian perspective. Chapter 9, the ﬁnal one of this second part,
provides an introduction to the expectation–maximization (EM) algorithm,
a topic which has had far-reaching inﬂuences in the statistical genetics lit-
erature. This algorithm is extremely versatile, and is so inextricable from
the statistical structure of a likelihood or Bayesian problem that we opted
to include it in this part of the book.
The ﬁrst two parts of the book described above provide the basis for
positing probability models. The implementation and validation of models
via MCMC requires some insight on the subtleties on which this technique is
based. This is presented in Part III, whose intent is to explain this remark-
able computational tool, within the constraints imposed by the authors’
limited mathematics. After an introduction to discrete Markov chains in
Chapter 10, the MCMC procedures are discussed in a detailed manner in
Chapter 11. An inquisitive reader should be able to follow the derivation of
the acceptance probability of various versions of the celebrated Metropolis–
Hastings algorithm, including reversible jump. An overview of methods for
analyzing MCMC output is the subject of Chapter 12.
Part IV gives a presentation of some of the models that are being used
in quantitative genetics at present. The treatment is mostly Bayesian and
the models are implemented via MCMC. The classical Gaussian mixed

viii
model for single- and multiple-trait analyses is described in Chapter 13.
Extensions are given for robust analyses using t distributions. The Bayesian
MCMC implementation of this robust analysis requires minor changes in
a code previously developed for analyzing Gaussian models, illustrating
the remarkable versatility of the MCMC techniques. Chapter 14 discusses
analyses involving ordered categorical traits based on the threshold model
of Sewall Wright. This chapter also includes a Bayesian MCMC descrip-
tion of a model for joint analysis of categorical and Gaussian responses.
Chapter 15 deals with models for the analysis of longitudinal data, and
the book concludes with Chapter 16, which introduces segregation analysis
and models for the detection of quantitative trait loci.
Although this book can be used as a text, it cannot claim such status
fully. A textbook requires carefully chosen exercises, and probably a more
linear development than the one presented here. Hence, these elements will
need to be provided by the instructor, should this book be considered for
classroom use. We have decided not to discuss software issues, although
some reasonably powerful public domain programs are already available.
The picture in this area is changing too rapidly, and we felt that many of
our views or recommendations in this respect would probably be rendered
obsolete at the time of publication.
The book evolved from cooperation between the two authors with col-
leagues from Denmark and Wisconsin leading to a series of papers in which
the ﬁrst applications in animal breeding of Bayesian hierarchical models
computed via MCMC methods were reported. Subsequently, we were in-
vited to teach or coteach courses in Likelihood and Bayesian MCMC analy-
sis at Ames (USA), Armidale (Australia), Buenos Aires (Argentina), Edin-
burgh (Scotland), Guelph (Canada), Jokioinen (Finland), Li`ege (Belgium),
Lleida (Spain), Madison (USA), Madrid (Spain), Milan (Italy), Montecillo
(Mexico), Piracicaba (Brazil), Ribeirao Preto (Brazil), Toulouse (France),
Uppsala (Sweden), Valencia (Spain), and Vi¸cosa (Brazil). While in the
course of these teaching experiences, we thought it would be useful to amal-
gamate some of our ideas in book form. What we hope you will read is the
result of several iterations, starting from a monograph written by Daniel
Sorensen and entitled “Gibbs Sampling in Quantitative Genetics”. This was
published ﬁrst in 1996 as Internal Report No. 82 by the Danish Institute
of Agricultural Sciences (DIAS).
Colleagues, friends, and loved ones have contributed in a variety of
ways toward the making of this book. Carlos Becerril, Agust´ın Blasco,
Rohan Fernando, Bernt Guldbrandtsen (who also made endless contribu-
tions with LaTeX related problems), Larry Schaeﬀer, and Bruce Walsh
worked through a large part of the manuscript. Speciﬁc chapters were read
by Anders Holst Andersen, Jos´e Miguel Bernardo, Yu-mei Chang, Miguel
P´erez Enciso, Davorka Gulisija, Shyh-Forng Guo, Mark Henryon, Bjorg
Heringstad, Just Jensen, Inge Riis Korsgaard, Mogens Sandø Lund, Nuala

ix
Sheehan, Mikko Sillanp¨a¨a, Miguel Angel Toro, and Rasmus Waagepetersen.
We acknowledge their valuable suggestions and corrections. However, we
are solely responsible for the mistakes that evaded scrutiny, as no book
is entirely free of errors. Some of the mistakes ﬁnd a place in the book by
what one may mercifully call random accidents. Other mistakes may reﬂect
incomplete knowledge of the topic on our side. We would be grateful if we
could be made aware of these errors.
We wish to thank colleagues at the Department of Animal Breeding and
Genetics, DIAS, and at the Departments of Animal Sciences and of Dairy
Science of the University of Wisconsin-Madison for providing an intellectu-
ally stimulating and socially pleasant atmosphere. We are in special debt
to Bernt Bech Andersen for much support and encouragement, and for
providing a rare commodity: intellectual space.
We acknowledge John Kimmel from Springer-Verlag for encouragement
and patience. Tony Orrantia, also from Springer-Verlag, is thanked for his
sharp professional editing.
DG wishes to thank Arthur B. Chapman for his inﬂuential mentoring and
for his views on the ethics of science, the late Charles R. Henderson for his
pioneering work in linear models in animal breeding, and my colleagues and
friends Jean-Louis Foulley, Rohan Fernando, and Sotan Im, from whom I
learned much. DG had to ﬁt the book into a rather hectic schedule of
lecturing and research, both at home and overseas. This took much time
away from Graciela, Magdalena, and Daniel Santiago, but they always gave
me love, support and encouragement. I also wish to thank Gorgias and
Alondra (my parents), the late Tatu, Morocha, and H´ector, and Chiquita,
Mumu, Arturo, and Cecilia for their love.
This book was written “at work”, at home, in airports and in hotels,
on week-ends and on vacation. Irrespective of place, DS received consistent
support from Maiken, Jon, and Elsebeth. They accepted that I was unavail-
able, and put up with moments of frustration (often in good spirit) when
things did not work out. I was inﬂuenced by and am in debt to my early
teachers in Reading and Edinburgh, especially Robert Curnow, Bill Hill,
and Alan Robertson. Brian Kennedy introduced me to mixed linear model
theory while I was a post-doc in Davis, California, and later in Guelph. I
have learned much from him. To my parents I owe unveiling for me at an
early age, that part of life that thrives on the top of trees, in worlds of
reason and poetry, where it ﬁnds its space and achieves its splendor.

This page intentionally left blank

Contents
Preface
v
I
Review of Probability and Distribution Theory
1
1
Probability and Random Variables
3
1.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . .
3
1.2
Univariate Discrete Distributions . . . . . . . . . . . . . . .
4
1.2.1
The Bernoulli and Binomial Distributions . . . . . .
7
1.2.2
The Poisson Distribution
. . . . . . . . . . . . . . .
10
1.2.3
Binomial Distribution: Normal Approximation
. . .
12
1.3
Univariate Continuous Distributions . . . . . . . . . . . . .
13
1.3.1
The Uniform, Beta, Gamma, Normal,
and Student-t Distributions . . . . . . . . . . . . . .
18
1.4
Multivariate Probability Distributions . . . . . . . . . . . .
29
1.4.1
The Multinomial Distribution . . . . . . . . . . . . .
37
1.4.2
The Dirichlet Distribution . . . . . . . . . . . . . . .
40
1.4.3
The d-Dimensional Uniform Distribution
. . . . . .
40
1.4.4
The Multivariate Normal Distribution . . . . . . . .
41
1.4.5
The Chi-square Distribution . . . . . . . . . . . . . .
53
1.4.6
The Wishart and Inverse Wishart Distributions . . .
55
1.4.7
The Multivariate-t Distribution . . . . . . . . . . . .
60
1.5
Distributions with Constrained Sample Space . . . . . . . .
62
1.6
Iterated Expectations
. . . . . . . . . . . . . . . . . . . . .
67

xii
Contents
2
Functions of Random Variables
77
2.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . .
77
2.2
Functions of a Single Random Variable . . . . . . . . . . . .
78
2.2.1
Discrete Random Variables . . . . . . . . . . . . . .
78
2.2.2
Continuous Random Variables
. . . . . . . . . . . .
79
2.2.3
Approximating the Mean and Variance
. . . . . . .
89
2.2.4
Delta Method . . . . . . . . . . . . . . . . . . . . . .
93
2.3
Functions of Several Random Variables . . . . . . . . . . . .
95
2.3.1
Linear Transformations
. . . . . . . . . . . . . . . .
111
2.3.2
Approximating the Mean and Covariance Matrix . .
114
II
Methods of Inference
117
3
An Introduction to Likelihood Inference
119
3.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . .
119
3.2
The Likelihood Function . . . . . . . . . . . . . . . . . . . .
120
3.3
The Maximum Likelihood Estimator . . . . . . . . . . . . .
122
3.4
Likelihood Inference in a Gaussian Model
. . . . . . . . . .
125
3.5
Fisher’s Information Measure . . . . . . . . . . . . . . . . .
128
3.5.1
Single Parameter Case . . . . . . . . . . . . . . . . .
128
3.5.2
Alternative Representation of Information . . . . . .
131
3.5.3
Mean and Variance of the Score Function . . . . . .
134
3.5.4
Multiparameter Case . . . . . . . . . . . . . . . . . .
135
3.5.5
Cram´er–Rao Lower Bound
. . . . . . . . . . . . . .
138
3.6
Suﬃciency . . . . . . . . . . . . . . . . . . . . . . . . . . . .
142
3.7
Asymptotic Properties: Single Parameter Models . . . . . .
143
3.7.1
Probability of the Data Given the Parameter . . . .
144
3.7.2
Consistency . . . . . . . . . . . . . . . . . . . . . . .
146
3.7.3
Asymptotic Normality and Eﬃciency . . . . . . . . .
147
3.8
Asymptotic Properties: Multiparameter Models . . . . . . .
152
3.9
Functional Invariance
. . . . . . . . . . . . . . . . . . . . .
153
3.9.1
Illustration of Functional Invariance
. . . . . . . . .
153
3.9.2
Invariance in a Single Parameter Model . . . . . . .
157
3.9.3
Invariance in a Multiparameter Model . . . . . . . .
159
4
Further Topics in Likelihood Inference
161
4.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . .
161
4.2
Computation of Maximum Likelihood Estimates
. . . . . .
162
4.3
Evaluation of Hypotheses
. . . . . . . . . . . . . . . . . . .
166
4.3.1
Likelihood Ratio Tests . . . . . . . . . . . . . . . . .
166
4.3.2
Conﬁdence Regions . . . . . . . . . . . . . . . . . . .
177
4.3.3
Wald’s Test . . . . . . . . . . . . . . . . . . . . . . .
179
4.3.4
Score Test . . . . . . . . . . . . . . . . . . . . . . . .
179
4.4
Nuisance Parameters . . . . . . . . . . . . . . . . . . . . . .
181

Contents
xiii
4.4.1
Loss of Eﬃciency Due to Nuisance Parameters
. . .
182
4.4.2
Marginal Likelihoods . . . . . . . . . . . . . . . . . .
182
4.4.3
Proﬁle Likelihoods . . . . . . . . . . . . . . . . . . .
186
4.5
Analysis of a Multinomial Distribution . . . . . . . . . . . .
190
4.5.1
Amount of Information per Observation . . . . . . .
199
4.6
Analysis of Linear Logistic Models . . . . . . . . . . . . . .
202
4.6.1
The Logistic Distribution . . . . . . . . . . . . . . .
204
4.6.2
Likelihood Function under Bernoulli Sampling
. . .
204
4.6.3
Mixed Eﬀects Linear Logistic Model . . . . . . . . .
208
5
An Introduction to Bayesian Inference
211
5.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . .
211
5.2
Bayes Theorem: Discrete Case . . . . . . . . . . . . . . . . .
214
5.3
Bayes Theorem: Continuous Case . . . . . . . . . . . . . . .
223
5.4
Posterior Distributions . . . . . . . . . . . . . . . . . . . . .
235
5.5
Bayesian Updating . . . . . . . . . . . . . . . . . . . . . . .
249
5.6
Features of Posterior Distributions . . . . . . . . . . . . . .
257
5.6.1
Posterior Probabilities . . . . . . . . . . . . . . . . .
258
5.6.2
Posterior Quantiles . . . . . . . . . . . . . . . . . . .
262
5.6.3
Posterior Modes
. . . . . . . . . . . . . . . . . . . .
264
5.6.4
Posterior Mean Vector and Covariance Matrix
. . .
280
6
Bayesian Analysis of Linear Models
287
6.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . .
287
6.2
The Linear Regression Model . . . . . . . . . . . . . . . . .
287
6.2.1
Inference under Uniform Improper Priors
. . . . . .
288
6.2.2
Inference under Conjugate Priors . . . . . . . . . . .
297
6.2.3
Orthogonal Parameterization of the Model
. . . . .
307
6.3
The Mixed Linear Model . . . . . . . . . . . . . . . . . . . .
313
6.3.1
Bayesian View of the Mixed Eﬀects Model . . . . . .
313
6.3.2
Joint and Conditional Posterior Distributions . . . .
317
6.3.3
Marginal Distribution of Variance Components . . .
322
6.3.4
Marginal Distribution of Location Parameters . . . .
323
7
The Prior Distribution and Bayesian Analysis
327
7.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . .
327
7.2
An Illustration of the Eﬀect of Priors on Inferences . . . . .
328
7.3
A Rapid Tour of Bayesian Asymptotics
. . . . . . . . . . .
330
7.3.1
Discrete Parameter . . . . . . . . . . . . . . . . . . .
330
7.3.2
Continuous Parameter . . . . . . . . . . . . . . . . .
331
7.4
Statistical Information and Entropy
. . . . . . . . . . . . .
334
7.4.1
Information . . . . . . . . . . . . . . . . . . . . . . .
334
7.4.2
Entropy of a Discrete Distribution . . . . . . . . . .
337
7.4.3
Entropy of a Joint and Conditional Distribution
. .
340
7.4.4
Entropy of a Continuous Distribution
. . . . . . . .
341

xiv
Contents
7.4.5
Information about a Parameter . . . . . . . . . . . .
346
7.4.6
Fisher’s Information Revisited
. . . . . . . . . . . .
351
7.4.7
Prior and Posterior Discrepancy
. . . . . . . . . . .
353
7.5
Priors Conveying Little Information
. . . . . . . . . . . . .
356
7.5.1
The Uniform Prior . . . . . . . . . . . . . . . . . . .
356
7.5.2
Other Vague Priors . . . . . . . . . . . . . . . . . . .
358
7.5.3
Maximum Entropy Prior Distributions . . . . . . . .
367
7.5.4
Reference Prior Distributions . . . . . . . . . . . . .
379
8
Bayesian Assessment of Hypotheses and Models
399
8.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . .
399
8.2
Bayes Factors . . . . . . . . . . . . . . . . . . . . . . . . . .
400
8.2.1
Deﬁnition . . . . . . . . . . . . . . . . . . . . . . . .
400
8.2.2
Interpretation . . . . . . . . . . . . . . . . . . . . . .
402
8.2.3
The Bayes Factor and Hypothesis Testing . . . . . .
403
8.2.4
Inﬂuence of the Prior Distribution . . . . . . . . . .
412
8.2.5
Nested Models
. . . . . . . . . . . . . . . . . . . . .
414
8.2.6
Approximations to the Bayes Factor . . . . . . . . .
418
8.2.7
Partial and Intrinsic Bayes Factors . . . . . . . . . .
422
8.3
Estimating the Marginal Likelihood
. . . . . . . . . . . . .
424
8.4
Goodness of Fit and Model Complexity
. . . . . . . . . . .
429
8.5
Goodness of Fit and Predictive Ability of a Model
. . . . .
433
8.5.1
Analysis of Residuals . . . . . . . . . . . . . . . . . .
434
8.5.2
Predictive Ability and Predictive Cross-Validation .
436
8.6
Bayesian Model Averaging . . . . . . . . . . . . . . . . . . .
439
8.6.1
General . . . . . . . . . . . . . . . . . . . . . . . . .
439
8.6.2
Deﬁnitions
. . . . . . . . . . . . . . . . . . . . . . .
440
8.6.3
Predictive Ability of BMA . . . . . . . . . . . . . . .
441
9
Approximate Inference Via the EM Algorithm
443
9.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . .
443
9.2
Complete and Incomplete Data . . . . . . . . . . . . . . . .
444
9.3
The EM Algorithm . . . . . . . . . . . . . . . . . . . . . . .
445
9.3.1
Form of the Algorithm . . . . . . . . . . . . . . . . .
445
9.3.2
Derivation . . . . . . . . . . . . . . . . . . . . . . . .
445
9.4
Monotonic Increase of ln p (θ|y) . . . . . . . . . . . . . . . .
447
9.5
The Missing Information Principle . . . . . . . . . . . . . .
448
9.5.1
Complete, Observed and Missing Information . . . .
448
9.5.2
Rate of Convergence of the EM Algorithm . . . . . .
449
9.6
EM Theory for Exponential Families . . . . . . . . . . . . .
451
9.7
Standard Errors and Posterior Standard Deviations . . . . .
452
9.7.1
The Method of Louis . . . . . . . . . . . . . . . . . .
453
9.7.2
Supplemented EM Algorithm (SEM) . . . . . . . . .
454
9.7.3
The Method of Oakes
. . . . . . . . . . . . . . . . .
457
9.8
Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . .
458

Contents
xv
III
Markov Chain Monte Carlo Methods
475
10 An Overview of Discrete Markov Chains
477
10.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . .
477
10.2 Deﬁnitions . . . . . . . . . . . . . . . . . . . . . . . . . . . .
478
10.3 State of the System after n-Steps . . . . . . . . . . . . . . .
479
10.4 Long-Term Behavior of the Markov Chain . . . . . . . . . .
481
10.5 Stationary Distribution
. . . . . . . . . . . . . . . . . . . .
481
10.6 Aperiodicity and Irreducibility
. . . . . . . . . . . . . . . .
483
10.7 Reversible Markov Chains . . . . . . . . . . . . . . . . . . .
487
10.8 Limiting Behavior
. . . . . . . . . . . . . . . . . . . . . . .
492
11 Markov Chain Monte Carlo
497
11.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . .
497
11.2 Preliminaries
. . . . . . . . . . . . . . . . . . . . . . . . . .
498
11.2.1 Notation . . . . . . . . . . . . . . . . . . . . . . . . .
498
11.2.2 Transition Kernels . . . . . . . . . . . . . . . . . . .
499
11.2.3 Varying Dimensionality
. . . . . . . . . . . . . . . .
499
11.3 An Overview of Markov Chain Monte Carlo . . . . . . . . .
500
11.4 The Metropolis–Hastings Algorithm
. . . . . . . . . . . . .
502
11.4.1 An Informal Derivation
. . . . . . . . . . . . . . . .
502
11.4.2 A More Formal Derivation . . . . . . . . . . . . . . .
504
11.5 The Gibbs Sampler . . . . . . . . . . . . . . . . . . . . . . .
509
11.5.1 Fully Conditional Posterior Distributions
. . . . . .
510
11.5.2 The Gibbs Sampling Algorithm . . . . . . . . . . . .
510
11.6 Langevin–Hastings Algorithm . . . . . . . . . . . . . . . . .
517
11.7 Reversible Jump MCMC . . . . . . . . . . . . . . . . . . . .
517
11.7.1 The Invariant Distribution . . . . . . . . . . . . . . .
518
11.7.2 Generating the Proposal . . . . . . . . . . . . . . . .
519
11.7.3 Specifying the Reversibility Condition . . . . . . . .
520
11.7.4 Derivation of the Acceptance Probability
. . . . . .
522
11.7.5 Deterministic Proposals . . . . . . . . . . . . . . . .
523
11.7.6 Generating Proposals via the Identity Mapping . . .
525
11.8 Data Augmentation
. . . . . . . . . . . . . . . . . . . . . .
532
12 Implementation and Analysis of MCMC Samples
539
12.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . .
539
12.2 A Single Long Chain or Several Short Chains?
. . . . . . .
540
12.3 Convergence Issues . . . . . . . . . . . . . . . . . . . . . . .
541
12.3.1 Eﬀect of Posterior Correlation on Convergence
. . .
541
12.3.2 Monitoring Convergence . . . . . . . . . . . . . . . .
547
12.4 Inferences from the MCMC Output . . . . . . . . . . . . . .
550
12.4.1 Estimators of Posterior Quantities . . . . . . . . . .
550
12.4.2 Monte Carlo Variance . . . . . . . . . . . . . . . . .
553
12.5 Sensitivity Analysis . . . . . . . . . . . . . . . . . . . . . . .
556

xvi
Contents
IV
Applications in Quantitative Genetics
561
13 Gaussian and Thick-Tailed Linear Models
563
13.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . .
563
13.2 The Univariate Linear Additive Genetic Model . . . . . . .
564
13.2.1 A Gibbs Sampling Algorithm . . . . . . . . . . . . .
566
13.3 Additive Genetic Model with Maternal Eﬀects
. . . . . . .
570
13.3.1 Fully Conditional Posterior Distributions
. . . . . .
575
13.4 The Multivariate Linear Additive Genetic Model . . . . . .
576
13.4.1 Fully Conditional Posterior Distributions
. . . . . .
580
13.5 A Blocked Gibbs Sampler for Gaussian Linear Models . . .
584
13.6 Linear Models with Thick-Tailed Distributions
. . . . . . .
588
13.6.1 Motivation
. . . . . . . . . . . . . . . . . . . . . . .
588
13.6.2 A Student-t Mixed Eﬀects Model . . . . . . . . . . .
595
13.6.3 Model with Clustered Random Eﬀects . . . . . . . .
600
13.7 Parameterizations and the Gibbs Sampler . . . . . . . . . .
602
14 Threshold Models for Categorical Responses
605
14.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . .
605
14.2 Analysis of a Single Polychotomous Trait
. . . . . . . . . .
607
14.2.1 Sampling Model
. . . . . . . . . . . . . . . . . . . .
607
14.2.2 Prior Distribution and Joint Posterior Density
. . .
608
14.2.3 Fully Conditional Posterior Distributions
. . . . . .
611
14.2.4 The Gibbs Sampler . . . . . . . . . . . . . . . . . . .
615
14.3 Analysis of a Categorical and a Gaussian Trait
. . . . . . .
615
14.3.1 Sampling Model
. . . . . . . . . . . . . . . . . . . .
616
14.3.2 Prior Distribution and Joint Posterior Density
. . .
617
14.3.3 Fully Conditional Posterior Distributions
. . . . . .
619
14.3.4 The Gibbs Sampler . . . . . . . . . . . . . . . . . . .
625
14.3.5 Implementation with Binary Traits . . . . . . . . . .
626
15 Bayesian Analysis of Longitudinal Data
627
15.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . .
627
15.2 Hierarchical or Multistage Models
. . . . . . . . . . . . . .
628
15.2.1 First Stage
. . . . . . . . . . . . . . . . . . . . . . .
629
15.2.2 Second Stage . . . . . . . . . . . . . . . . . . . . . .
634
15.2.3 Third Stage . . . . . . . . . . . . . . . . . . . . . . .
639
15.2.4 Joint Posterior Distribution . . . . . . . . . . . . . .
641
15.3 Two-Step Approximate Bayesian Analysis . . . . . . . . . .
642
15.3.1 Estimating Location Parameters
. . . . . . . . . . .
643
15.3.2 Estimating Dispersion Parameters
. . . . . . . . . .
650
15.3.3 Special Case: Linear First Stage
. . . . . . . . . . .
652
15.4 Computation via Markov Chain Monte Carlo . . . . . . . .
653
15.4.1 Fully Conditional Posterior Distributions
. . . . . .
655
15.5 Analysis with Thick-Tailed Distributions . . . . . . . . . . .
664

Contents
xvii
15.5.1 First- and Second-Stage Models . . . . . . . . . . . .
665
15.5.2 Fully Conditional Posterior Distributions
. . . . . .
666
16 Segregation and Quantitative Trait Loci Analysis
671
16.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . .
671
16.2 Segregation Analysis Models . . . . . . . . . . . . . . . . . .
672
16.2.1 Notation and Model . . . . . . . . . . . . . . . . . .
672
16.2.2 Fully Conditional Posterior Distributions
. . . . . .
675
16.2.3 Some Implementation Issues . . . . . . . . . . . . . .
677
16.3 QTL Models
. . . . . . . . . . . . . . . . . . . . . . . . . .
679
16.3.1 Models with a Single QTL . . . . . . . . . . . . . . .
680
16.3.2 Models with an Arbitrary Number of QTL
. . . . .
690
References
701
List of Citations
727
Subject Index
733

This page intentionally left blank

Part I
Review of Probability and
Distribution Theory
1

This page intentionally left blank

1
Uncertainty, Random Variables, and
Probability Distributions
1.1
Introduction
Suppose there is a data set consisting of observations on body weight taken
on beef animals and that there are questions of scientiﬁc or practical impor-
tance to be answered from these data. The questions, for example, might
be:
(1) Which are potential factors to be included in a statistical model for
analysis of the records?
(2) What is the amount of additive genetic variance in the reference beef
cattle population?
Answers to these questions are to be obtained from a ﬁnite amount of
information contained in the data at hand. Because information is ﬁnite,
there will be uncertainty associated with the outcome of the analyses. In
this book, we are concerned with learning about the state of nature from
experimental or observational data, and with measuring the degree of un-
certainty at diﬀerent stages of the learning process using probability mod-
els. This uncertainty is expressed via probabilities; arguably, probability
can be viewed as the mathematics of uncertainty.
This chapter reviews probability models encountered often in quantita-
tive genetic analysis, and that are used subsequently in various parts of
this book. The treatment is descriptive and informal. It is assumed that
the reader will have had an introductory course in probability theory or
in mathematical statistics at the level of, for example, Hogg and Craig
(1995), or Mood et al. (1974). The concept of a random variable is deﬁned

4
1. Probability and Random Variables
and this is followed by an overview of univariate discrete and continuous
distributions. Multivariate distributions are presented in the last section of
the chapter, with considerable emphasis on the multivariate normal distri-
bution, because of the important role it plays in statistical genetic analysis.
For additional information, readers should consult treatises such as Johnson
and Kotz (1969, 1970a,b, 1972).
1.2
Univariate Discrete Distributions
We start by providing a succinct and intuitive introduction to the concepts
of random variable and probability which is adequate for our purposes.
Informally, a locus is a location in the genome where a gene resides. The
word gene has become a rather elusive concept over recent years (see Keller,
2000, for a discussion). Here, we refer to the gene as the Mendelian “factor”
transmitted from parent to oﬀspring. Consider a locus at which there are
two possible variants or alleles: A1 and A2. Envisage the random experiment
consisting of generating genotypes from alleles A1 and A2. There are four
possible outcomes for this experiment, represented by the four possible
genotypes: A1A1, A1A2, A2A1, and A2A2. Any particular outcome cannot
be known in advance; thus the use of the word “random”. The set of all
possible outcomes of the experiment constitutes its sample space, denoted
by Ω. “Possible” means “conceptually possible” and often compromises
between some perception of physical reality and mathematical convenience.
In this simple case, the sample space consists of the four possible genotypes;
that is, Ω= {ω1, ω2, ω3, ω4}. Each possible outcome ω is a sample point in
Ω. Here, ω1 = A1A1, ω2 = A1A2, ω3 = A2A1, and ω4 = A2A2.
Typically one is interested not so much on the experiment and on the
totality of its outcomes, but on some consequences of the experiment. For
example, imagine that individuals carrying allele A2 suﬀer from a measur-
able disease. Let X (ω) be the function of the sample point ω, deﬁned as
follows: X (ω) = 0 if ω = ω1, X (ω) = 1 if ω = ω2, ω3 or ω4, where 0 and
1 indicate absence or presence of disease, respectively.
The same gene could have an additive eﬀect on another trait, such that
allele A2 confers an increase in some measurement. A diﬀerent function
Y (ω) of ω could represent this situation deﬁned as follows: Y (ω) = 0 if ω =
ω1, Y (ω) = 1/2 if ω = ω2, ω3, Y (ω) = 1 if ω = ω4. These two functions
map the sample space to the real line R. That is, the functions X (ω) and
Y (ω), with domain Ω, make some real number correspond to each outcome
of the experiment. The functions X (ω) and Y (ω) are known as random
variables. Some authors prefer the term random quantities (i.e., De Finetti,
1975a; Bernardo and Smith, 1994), because the function in question is not
a variable, but a deterministic mapping from Ωto R. However, in this book,
the usual term, random variable, will be adopted.

1.2 Univariate Discrete Distributions
5
When attention focuses at the level of the random variable, this allows
in some sense, to ignore the (often abstract) sample space Ω. One can
deﬁne a new (concrete) sample space S in R, to be the range of possible
values that the random variable can take, so that no further mapping is
required. In this case the random variable becomes the identity mapping,
and can be thought of as the variable that describes the result of the random
experiment. In the case of X (ω), the sample space is S = {0, 1}, whereas
in the case of Y (ω) the sample space is S =

0, 1
2, 1

. The random variable
X (ω) will be written simply as X.
To distinguish between a random variable and its realized values, upper
case letters are often used for the former whereas lower case letters are
employed for the latter. This notation becomes awkward when bold face
letters denote matrices (upper case) or vectors (lower case), so this con-
vention is not followed consistently in this book. However, it will be clear
from the context whether one is discussing a random variable or its realized
values.
The sample spaces of the random variables X and Y consist of a count-
able number of values. Such spaces are called discrete and X and Y are
discrete random variables.
The random variable X (whether discrete or continuous) in turn induces
a probability PX on S. This is a function that to each event (subset) G ⊆S,
assigns a number in the closed interval [0, 1], deﬁned by
PX(G) = Pr({ω ∈Ω: X(ω) ∈G}) = Pr (X ∈G) .
(1.1)
The probability PX is called the distribution of X, which for a discrete
random variable, is completely determined by specifying Pr (X = x) for all
x.
The mathematical model sketched above is quite divorced from the intu-
itive understanding(s) of the concept of probability. The interpretation of
probability of the event G usually adopted in this book, is that it measures
the subjective degree of plausibility or belief in G, conditional on infor-
mation currently available to the subject. An alternative interpretation
of probability is as the long run frequency of occurrence of G with inde-
pendent replications of the random experiment carried out under identical
conditions.
By deﬁnition, a probability is a number between 0 and 1; both 0 and 1
are included as valid numbers. A probability cannot be negative or larger
than 1. Thus,
0 ≤Pr (X = x) ≤1, for all x ∈S.
Suppose that the sample space S = {x1, x2, . . .} is countable. The prob-
ability function (or probability mass function, abbreviated p.m.f.) of a dis-
crete random variable X is deﬁned by
p (x) =

Pr (X = x) , if x ∈S,
0, otherwise.

6
1. Probability and Random Variables
In distribution theory, the values that X can take are often denoted mass
points, and p (x) is the probability mass associated with the mass point x.
The distribution (1.1) reduces to the p.m.f. when the subset G becomes
a mass point at X = x. The support is the set of values of x for which
the probability function is non-zero. The probability function gives a full
description of the randomness or uncertainty associated with X. It can be
used to calculate the probability that X takes certain values, or that it falls
in a given region.
To illustrate, suppose that an individual is the progeny from crossing
randomly two lines, and that each line consists of genotypes A1A2 only.
Let the random variable X now represent the number of A2 alleles in the
progeny from this cross. From Mendel’s laws of inheritance, the probability
distribution of X is
Genotype
A2A2
A2A1
A1A1
x
2
1
0
Pr (X = x)
1
4
1
2
1
4
The probability that the number of A2 alleles in a randomly drawn in-
dividual is 2, say, is
Pr({ω ∈Ω: X(ω) = 2}) = 1
4,
abbreviated hereinafter as
Pr (X = 2) = p (2) = 1
4.
(1.2)
In the above example, we have
Pr (X = 0) = 1
4; Pr (X = 1) = 1
2; Pr (X = 2) = 1
4,
for each of the three values that X can take. Similarly,
Pr (X = 1 or X = 2) = 1
2 + 1
4 = 3
4.
To every random variable X there is an associated function called the
cumulative distribution function (c.d.f.) of X. It is denoted by F (x) and
is deﬁned by
F (x) = Pr (X ≤x) , for all x.
(1.3)
The c.d.f. is also a special case of (1.1), when G is the half-open interval
(−∞, x]. The notation emphasizes that the c.d.f. is a function of x. For the
example above, for x = 1,
F (1) = p (0) + p (1) = 1
4 + 1
2 = 3
4.
(1.4)

1.2 Univariate Discrete Distributions
7
Since F (·) is deﬁned for all values of x, not just those in S = {0, 1, 2}, one
can compute for x = 1.5 and for x = 1.99 say, F (1.5) = 1
4 + 1
2 = 3
4 and
F (1.99) = 1
4 + 1
2 = 3
4, respectively. However, for x = 2, F (2) = 1. Thus,
F (x) is a step-function. In this example, the entire c.d.f. is
F (x)
=
0, for x < 0,
F (x)
=
1
4, for 0 ≤x < 1,
F (x)
=
3
4, for 1 ≤x < 2,
F (x)
=
1, for x ≥2.
In general, one writes
F (x) =

t≤x
p (t) ,
(1.5)
where t is a “dummy” variable as used in calculus. Another notation that
will be used in this book is
F (x) =

t
I (t ≤x) p (t) ,
(1.6)
where I (t ≤x) is the indicator function that takes the value 1 if the argu-
ment is satisﬁed, in this case, t ≤x, and zero otherwise.
The distribution function F (x) has the following properties:
• If x1 ≤x2, then F (x1) ≤F (x2) .
• Pr (x1 < X ≤x2) = Pr (X ≤x2) −Pr (X ≤x1) = F (x2) −F (x1).
• limx→∞F (x) = 1 and limx→−∞F (x) = 0.
Example 1.1
Point mass
Consider a point mass at i (such that the total probability mass is concen-
trated on i). Then,
F (x) =

0,
x < i,
1,
x ≥i.
This property that F is only 0 or 1 is a characteristic of point masses.
■
In the following section, some of the most widely used discrete distribu-
tions in genetics are discussed. These are the Bernoulli, binomial and the
Poisson distributions.
1.2.1
The Bernoulli and Binomial Distributions
A random variable X that has a Bernoulli probability distribution can take
either 1 or 0 as possible values (more generally, it can have two modalities

8
1. Probability and Random Variables
only) with probabilities θ and 1 −θ, respectively. The quantity θ is called
a parameter of the distribution of X and, in practice, this may be known
or unknown. The p.m.f. of X is
Pr (X = x|θ) = Br (x|θ)

θx (1 −θ)1−x ,
for x = 0, 1,
0,
otherwise,
(1.7)
where 0 ≤θ ≤1 and Br is an abbreviation for Bernoulli.
In general, the notation Br (θ), say, will be used to specify a distri-
bution, in this case the Bernoulli distribution with parameter θ, whereas
Br (x|θ) refers to the corresponding p.m.f. of X. The notation for the p.m.f.
Pr (X = x|θ) is used to stress the dependence on the parameter θ.
Often a random variable X having a Bernoulli p.m.f. is referred to as say-
ing that X has a Bernoulli distribution. This phraseology will be adopted
in this book for other random variables (discrete or continuous) having a
named p.m.f. or probability density function (abbreviated p.d.f. and deﬁned
in Section 1.3).
Suppose the frequency of allele A in a population is 0.34 (the gene fre-
quency is given by the number of A alleles found, divided by twice the
number of individuals in a diploid organism). Then one may postulate that
a randomly drawn allele is a Bernoulli random variable with parameter
θ = 0.34. If θ is known, one can specify the probability distribution of X
exactly. In this case, there will be uncertainty about the values that X can
take, but not about the value of θ. If θ is unknown, there are two (inti-
mately related) consequences. First, the Bernoulli random variables in the
sequence X1, X2, . . . are no longer independent (the concept of indepen-
dence is discussed below and in Section 1.4). Second, there will be an extra
source of uncertainty associated with X, and this will require introducing
an additional probability distribution for θ in order to calculate the proba-
bility distribution of X correctly (accounting for the uncertainty about θ).
This is a central problem in statistical inference, as will be seen later in this
book. Typically, (1.7) cannot be speciﬁed free of error about the parameter,
because θ must be inferred from previous considerations, or from a ﬁnite
set of data at hand. Clearly, it is somewhat disturbing to use an estimated
value of θ and then proceed as if (1.7) were the “true” distribution. In this
book, procedures for taking such “errors” into account will be described.
The mean of the Bernoulli distribution is obtained as follows:
E (X|θ) = 0 × Pr (X = 0|θ) + 1 × Pr (X = 1|θ) = θ,
(1.8)
where E (·) denotes expected or average value. Similarly,
E

X2|θ

= 02 × Pr (X = 0|θ) + 12 × Pr (X = 1|θ) = θ.
Then, by deﬁnition of the variance of a random variable, abbreviated as
V ar hereinafter,
V ar (X|θ) = E

X2|θ

−E2 (X|θ) = θ −θ2 = θ (1 −θ) .
(1.9)

1.2 Univariate Discrete Distributions
9
A random variable X has a binomial distribution (the term process will
be used interchangeably) if its p.m.f. has the form
Pr (X = x|θ, n) = Bi (x|θ, n) =




n
x

θx (1 −θ)n−x ,
x = 0, 1, ..., n,
0,
otherwise,
(1.10)
where 0 ≤θ ≤1 and n ≥1. Here the parameters of the process are θ and
n, and inferences about X are conditional on these parameters. In a se-
quence of n identical and independent Bernoulli trials, each with “success”
probability θ, deﬁne the random variables Y1, Y2, . . . , Yn as follows:
Yi =

1,
with probability θ,
0,
with probability 1 −θ.
Then the random variable X = n
i=1 Yi has the binomial distribution
Bi (θ, n). The mean and variance of X are readily seen to be equal to nθ
and to nθ (1 −θ), respectively.
The binomial distribution thus stems from consideration of n mutually
independent Bernoulli random variables all having the same parameter θ.
Mutual independence means that knowledge of the value of some subset
of the n Bernoulli random variables does not alter the state of uncertainty
about the remaining ones. For example, in the two-variable case, if two
coins are tossed by two diﬀerent individuals, having observed the outcome
of one of the tosses will not modify the state of uncertainty about the
second toss.
Example 1.2
Number of females in a sibship of size 3
Let the random variable X denote the total number of females observed in
three randomly chosen births, so n = 3 and the value of X ranges from 0
to 3. Assume that θ, the probability of a female birth, is known and equal
to θ = 0.49. Then
Pr (X = 0|θ, n)
=
Bi (0|0.49, 3) = 0.1327,
Pr (X = 1|θ, n)
=
Bi (1|0.49, 3) = 0.3823,
Pr (X = 2|θ, n)
=
Bi (2|0.49, 3) = 0.3674,
Pr (X = 3|θ, n)
=
Bi (3|0.49, 3) = 0.1176.
The events are mutually exclusive and exhaustive because, for example, if
X = 0, then all other values of the random variable are excluded. Hence,
the probability that X takes at least one of these four values is equal to 1.
Further, the probability that X is equal to 0 or 2 is
Pr (X = 0 or X = 2|θ, n) = Pr (X = 0|θ, n) + Pr (X = 2|θ, n)
= 0.1327 + 0.3674 = 0.5001.

10
1. Probability and Random Variables
When events are disjoint (mutually exclusive), the probability that either
one or another event of interest occurs is given by the sum of their elemen-
tary probabilities, as in the above case.
■
Example 1.3
Simulating a binomial random variable
A simple way of simulating a binomial random variable X ∼Bi (θ, n) is
to generate n independent standard uniforms and to set X equal to the
number of uniform variates that are less than or equal to θ (Gelman et al.,
1995).
■
1.2.2
The Poisson Distribution
When θ is small and n is large, an approximation to Bi (x|θ, n) is obtained
as follows. Let nθ = λ be a new parameter. Expanding the binomial coef-
ﬁcient in (1.10) one obtains, for X = k,
Bi (k|θ, n) = n (n −1) ... (n −k + 1)
k!
θk (1 −θ)n−k
= n
n
n −1
n
· · · n −k + 1
n
× λk
k!

1 −λ
n
n 
1 −λ
n
−k
.
(1.11)
Suppose that n →∞and θ →0, such that nθ = λ remains constant.
Taking limits, when n →∞,
lim
n→∞Bi (k|θ, n) = lim
n→∞

n −1
n
· · · n −k + 1
n

1 −λ
n
−k
×λk
k! lim
n→∞

1 −λ
n
n
= λk
k! lim
n→∞

1 −λ
n
n
.
(1.12)
Recall the binomial expansion
(a + b)n =
n

x=0

n
x

bxan−x.
Put a = 1 and b = −λ/n. Then

1 −λ
n
n
=
n

x=0

n
x
 
−λ
n
x
=
n

x=0
n
n
(n −1)
n
· · · (n −x + 1)
n
(−λ)x
x!
,

1.2 Univariate Discrete Distributions
11
so
lim
n→∞

1 −λ
n
n
=
∞

x=0
(−λ)x
x!
.
(1.13)
A Taylor series expansion of exp (V ) about 0 yields
exp (V ) = 1 + V + V 2
2! + V 3
3! + · · ·
so if the series has an inﬁnite number of terms, this is expressible as
exp (V ) =
∞

x=0
V x
x! .
Making use of this in (1.13), with V = −λ, leads directly to
lim
n→∞

1 −λ
n
n
= exp (−λ) .
Finally, employing this in (1.12) gives the result
lim
n→∞Bi (k|θ, n) = λk exp (−λ)
k!
.
(1.14)
This gives an approximation to the binomial probabilities for situations
where n is “very large” and the probability θ is “very small”. This may
be useful, for example, for calculating the probability of ﬁnding a certain
number of mutants in a population when the mutation rate is low.
In fact, approximation (1.14) plays an important role in distribution the-
ory in statistics: a random variable X is said to have a Poisson distribution
with parameter λ (this being strictly positive, by deﬁnition) if its proba-
bility function is
Pr (X = x|λ) = Po (x|λ) =

λx exp (−λ) /x!,
for x = 0, 1, 2, . . . , ∞,
0,
otherwise,
(1.15)
where Po is an abbreviation for Poisson. Here, X is a discrete random
variable, and its sample space is countably inﬁnite. This distribution could
feature in models for quantitative trait loci (QTL) detection, and may
be suitable, for example, for describing the occurrence of family sizes in
animals with potentially large sibships, such as insects or ﬁsh. It has been
used in animal breeding in the analysis of litter size in pigs and sheep.
Example 1.4
Exposure of mice to carcinogens
In order to illustrate the relationship between the binomial and the Poisson
distributions, suppose there is a 2% chance that a mouse exposed to a
certain carcinogen will develop cancer. What is the probability that two

12
1. Probability and Random Variables
or more mice will develop cancer within a group of 60 experimental mice
exposed to the causative agent? Let the random variable X describe the
number of mice that develop cancer, and suppose that the sampling model
Bi (x|0.02, 60) is tenable. Using (1.10), the probability that two or more
mice will have a carcinoma is computed to be
Pr (X ≥2|θ, n) = 1 −Pr (X ≤1|θ, n)
= 1 −Pr (X = 0|θ, n) −Pr (X = 1|θ, n)
= 0.3381.
However, given that θ = 0.02 and n = 60 seem (in a rather arbitrary
manner) reasonably small and large, respectively, the above probability is
approximated using the Poisson distribution Po (1.2), with its parameter
arrived at as λ = 0.02 × 60 = 1.2. The probability that k mice will develop
cancer is given by
Pr (X = k|λ = 1.2) = e−1.2 (1.2)k
k!
.
Therefore,
Pr (X ≥2|λ = 1.2) = 1 −Pr (X ≤1|λ = 1.2)
= 1 −Pr (X = 0|λ = 1.2) −Pr (X = 1|λ = 1.2)
= 1 −1 + 1.2
e1.2
= 0.3374,
so the approximation is satisfactory, at least in this case.
■
1.2.3
Binomial Distribution: Normal Approximation
Another useful approximation to the binomial distribution is based on the
central limit theorem, to be discussed later in this chapter. Let X be a
binomial variable, deﬁned as X = n
i=1 Ui, where the Ui’s are independent
and identically distributed (for short, i.i.d.) Br (θ) random variables. Then,
as given in (1.8) and in (1.9), E (Ui) = θ and V ar (Ui) = θ (1 −θ). There-
fore, the algebra of expectations yields E (X) = n
i=1 E (Ui) = nθ and
V ar (X) = n
i=1 V ar (Ui) = nθ (1 −θ). The formula for the variance is
a consequence of the independence assumption made about the Bernoulli
variates, so the variance of a sum is the sum of the variances. Now, the
random variable
Z =
X −nθ
[nθ (1 −θ)]1/2
(1.16)
has expectation and variance equal to 0 and 1, respectively. By virtue of
the central limit theorem, for large n, Z is approximately distributed as an
N (0, 1) random variable, which stands for a standard, normally distributed

1.3 Univariate Continuous Distributions
13
random variable (it is assumed that the reader is somewhat familiar with
the normal or Gaussian distribution, but it will be discussed subsequently).
If X ∼Bi (θ, n), given ﬁxed numbers x1 and x2, x1 < x2, as n →∞, it
follows, from the approximation above, that
Pr (x1 < X ≤x2) = Pr

x1 −nθ
[nθ (1 −θ)]1/2 <
X −nθ
[nθ (1 −θ)]1/2 ≤
x2 −nθ
[nθ (1 −θ)]1/2

= Pr (z1 < Z ≤z2) = Pr (Z ≤z2) −Pr (Z ≤z1)
≈Φ (z2) −Φ (z1) ,
(1.17)
where zi = (xi −nθ)/ [nθ (1 −θ)]1/2, and Φ (·) is the c.d.f. of a standard
normal random variable.
For the mouse example, the probability that two or more mice will de-
velop cancer can be approximated as
Pr (X ≥2)
=
Pr

X −nθ
[nθ (1 −θ)]1/2 ≥
2 −nθ
[nθ (1 −θ)]1/2

=
Pr (Z ≥0.7377)
=
1 −Pr (Z < 0.7377) ≈0.230.
Here the normal approximation is not adequate. In general, this approxi-
mation does not work well, unless min [nθ, n (1 −θ)] ≥5, and the behavior
is more erratic when θ is near the boundaries. In the example, nθ is equal
to 60 (0.02) = 1.2. A better approximation is obtained using the continu-
ity correction (e.g., Casella and Berger, 1990). Instead of approximating
Pr (X ≥2), one approximates Pr (X ≥2 −0.5); this yields 0.391, which is
a little closer to the exact result 0.338 obtained before.
For a recent discussion about the complexities associated with this ap-
proximation in the context of calculation of the conﬁdence interval for a
proportion, the reader is referred to Brown et al. (2001) and to Henderson
and Meyer (2001).
1.3
Univariate Continuous Distributions
The variable Z, deﬁned in (1.16) is an example of a continuous random
variable, that is, one taking any of an inﬁnite number of values. In the
case of Z these values exist in the real line, R; the sample space is here
continuous. More precisely, a random variable is (absolutely) continuous if
its c.d.f. F (x), deﬁned in (1.18), is continuous at every real number x.
The probability that the continuous random variable will assume any
particular value is zero because the possible values that X can take are not
countable. A typical example of a random variable that can be regarded

14
1. Probability and Random Variables
as continuous is the body weight of an animal. In practice, however, mea-
surements are taken on a discrete scale only, with the degree of discreteness
depending on the resolution of the instrument employed for recording. How-
ever, much simplicity can be gained by treating body weight as if it were
continuous.
In the discrete case, as given in (1.5), the distribution function is deﬁned
in terms of a sum. On the other hand, the c.d.f. of a continuous random
variable X on the real line is given by the integral
F (x) = Pr (X ≤x) =
x

−∞
p (t) dt,
for −∞< x < ∞,
(1.18)
where p (·) is the p.d.f. of this random variable. If dt is an inﬁnitesimally
small increment in the variable, so that the density is continuous and fairly
constant between t and t + dt, the probability that the random variable
takes a value in the interval between t and t + dt is nearly p (t) dt (Bulmer,
1979). If p (t) > p (u) then values of X are more likely to be seen close to t
than values of X close to u.
As a little technical aside, the p.d.f. is not uniquely deﬁned. This is
because one can always change the value of a function at a ﬁnite number of
points, without changing the integral of the function over the interval. This
lack of uniqueness does not pose problems for probability calculations but
can lead to subtle complications in certain statistical applications. However,
if the distribution function is diﬀerentiable then a natural choice for p (x)
is
d
dxF (x) = p (x) .
(1.19)
Any probability statement about the random variable X can be deduced
from either F (x) or p (x); the p.d.f. encapsulates all the necessary infor-
mation needed to make statements of uncertainty about X. This does not
mean that X is characterized by its c.d.f.. Two diﬀerent random variables
can have the same c.d.f. or p.d.f.. As a trivial example, consider the Gaus-
sian random variable X with mean zero and variance 1. Then the random
variable −X is also Gaussian with the same mean and variance.
Properties of the p.d.f. are:
• p (x) ⩾0 for all x.
•
 ∞
−∞p (x) dx = 1.
(Any positive function p satisfying
 ∞
−∞p (x) dx = 1 may be termed a
p.d.f.).
• for x ∈(a, b), Pr (a < X < b) =
 b
a p (x) dx = F (b) −F (a) .

1.3 Univariate Continuous Distributions
15
(Because of the lack of uniqueness mentioned above, it is possible to
ﬁnd another density function p∗, such that
 b
a p (x) dx =
 b
a p∗(x) dx,
for all a and b but for which p (x) and p∗(x) diﬀer at a ﬁnite number
of points).
By virtue of the continuity of X,
Pr (a < X < b) = Pr (a < X ≤b)
= Pr (a ≤X < b) = Pr (a ≤X ≤b) ,
(1.20)
so < and ≤can be used indiscriminately in this case.
• The probability that X takes a particular value is,
Pr (X = x) = lim
ε→0
a+ε

a−ε
p (x) dx = 0.
• As noted above, the p.d.f. describes the probability that X takes a
value in a neighborhood of a point x. Thus if ∆is small and p is
continuous,
Pr (x ≤X ≤x + ∆) =
x+∆

x
p (x) dx ∼= ∆p (x) .
(1.21)
Consider the continuous random variable X that has density p (x) with
support on the real line, R. Let A be some interval in R. Notationally, we
say that A ⊆R and, using the concept of the indicator function introduced
in (1.6), the probability that the random variable X belongs in A can be
written
Pr (X ∈A)
=

A
p (x) dx
=

I (x ∈A) p (x) dx
=
E [I (x ∈A)] .
(1.22)
In this book we shall often encounter integrals of the form

g (x) p (x) dx.
This is the expectation of g with respect to a probability on R. It will be
referred to as “integrating g over the distribution of X”. We may also use,
loosely, “integrating g, or taking the expectation of g, with respect to p”.

16
1. Probability and Random Variables
Often it suﬃces to identify the p.d.f. of a random variable X only up to
proportionality; this is particularly convenient in the context of a Bayesian
analysis, as will be seen repeatedly in this book. A function f (x), such
that p (x) ∝f (x), is called the kernel of p (x). That is, if it is known that
p (x) = Cf (x), the constant C can be determined using the fact that a
p.d.f. integrates to 1. Thus, for a random variable X taking any value on
the real line
 ∞
−∞
p (x) dx = C
 ∞
−∞
f (x) dx = 1
so
C =
1
 ∞
−∞f (x) dx.
Equivalently,
p (x) =
f (x)
 ∞
−∞f (x) dx.
(1.23)
If the random variable takes values within the interval (lower, upper), say,
the integration limits above must be changed accordingly.
Example 1.5
Kernel and integration constant of a normal distribution
Suppose X has a normal distribution with mean µ and variance σ2. Then
its density function is
p

x|µ, σ2
=

2πσ2−1
2 exp

−(x −µ)2
2σ2

.
The kernel is
f (x) = exp

−(x −µ)2
2σ2

and the constant of integration is
C =
1
 ∞
−∞exp

−(x−µ)2
2σ2

dx
=

2πσ2−1
2 .
(1.24)
To prove (1.24), denote the integral in the denominator as I. Using the
substitution u = (x −µ)/σ, since dx = σ du, I can be written
I = σ
 ∞
−∞
exp

−u2
2

du.
Now write the square of I as a double integral
I2
=
σ2
 ∞
−∞
exp

−u2
2

du
 ∞
−∞
exp

−v2
2

dv
=
σ2
 ∞
−∞
 ∞
−∞
exp

−

u2 + v2
2

du dv

1.3 Univariate Continuous Distributions
17
and make a transformation to polar coordinates
u
=
r cos (θ) ,
v
=
r sin (θ) .
If the angle θ is expressed in radians, then it can take values between 0
and 2π. The variable r takes values between 0 and ∞. From calculus, the
absolute value of the two-dimensional Jacobian of the transformation is the
absolute value of the determinant
det
∂(u, v)
∂(r, θ)

=
det

∂u/∂r
∂u/∂θ
∂v/∂r
∂v/∂θ

=
det

cos (θ)
−r sin (θ)
sin (θ)
r cos (θ)

=
r

cos2 (θ) + sin2 (θ)

= r.
Using this, together with the result
u2 + v2 = r2 
cos2 (θ) + sin2 (θ)

= r2
one obtains
I2
=
σ2
 ∞
0
 2π
0
r exp

−r2
2

dθdr
=
σ2
 ∞
0
r exp

−r2
2

dr
 2π
0
dθ
=
2πσ2
 ∞
0
r exp

−r2
2

dr.
The integral in the preceding expression can be evaluated from the family
of gamma integrals (Box and Tiao, 1973). For a > 0 and p > 0:
 ∞
0
rp−1 exp

−ar2
dr = 1
2a−p/2Γ
p
2

,
(1.25)
where Γ (·) is the gamma function (which will be discussed later). Setting
p = 2 and a = 1
2, it follows that
 ∞
0
r exp

−r2
2

dr = 1
2 × 2 × Γ (1) = 1,
because Γ (1) = 0! = 1. Therefore, I2 = 2πσ2, and, ﬁnally,
C =
1
 ∞
0
exp

−(x−µ)2
2σ2

dx
= 1
I =
1
√
2πσ2
yielding the desired result.
■

18
1. Probability and Random Variables
1.3.1
The Uniform, Beta, Gamma, Normal,
and Student-t Distributions
Uniform Distribution
A random variable X has a uniform distribution over the interval [a, b] (in
view of (1.20) we shall allow ourselves to be not quite consistent throughout
this book when deﬁning the support of distributions of continuous random
variables whose sample space is truncated) if its p.d.f. is
p (x|a, b) = Un (x|a, b) =

1
b−a,
for a ≤x ≤b,
0,
otherwise.
(1.26)
The c.d.f. is
F (x|a, b) =



0,
if x < a,
 x
a Un (x|a, b) dx =
1
b−a
 x
a dx = x−a
b−a ,
for a ≤x ≤b,
1,
for x > b.
The average value of X is found by multiplying the value x times its “in-
ﬁnitesimal probability” p (x) dx, and then summing over all possible such
values. This corresponds to the integral
E (X|a, b) =
b

a
x
1
b −adx = a + b
2
.
(1.27)
Likewise the average value of X2 is
E

X2|a, b

=
b

a
x2
1
b −adx = b3 −a3
3 (b −a) = (a + b)2 −ab
3
.
Then the variance can be found to be, after algebra,
V ar (X|a, b)
=
E

X2|a, b

−E2 (X|a, b)
=
(a + b)2 −ab
3
−

a + b
2
2
= (b −a)2
12
.
(1.28)
Example 1.6
Simulation of discrete random variables
Suppose one wishes to generate a discrete random variable with p.m.f.
Pr (X = xi) = pi,
i = 1, 2, 3, 4,
with 4
i=1 pi = 1. To accomplish this, generate a random number U from
a uniform distribution Un (0, 1) and set
X =







x1,
if U < p1,
x2,
if p1 ≤U < p1 + p2,
x3,
if p1 + p2 ≤U < p1 + p2 + p3,
x4,
if p1 + p2 + p3 ≤U < 1.

1.3 Univariate Continuous Distributions
19
To conﬁrm that X has the desired distribution, ﬁrst note that p (u) = 1.
Then
Pr (X = x3)
=
Pr (p1 + p2 ≤U < p1 + p2 + p3)
=
Pr (U ≤p1 + p2 + p3) −Pr (U ≤p1 + p2)
=
 p1+p2+p3
0
du −
 p1+p2
0
du = p3.
This method of generating the random variable is known as the inverse
transform method. For continuous random variables, the general algorithm
is as follows. If one wishes to generate a random variable that has c.d.f. F,
then:
(a) ﬁrst generate U ∼Un (0, 1); and
(b) then set X = F −1
X (U);
and X has the desired c.d.f. FX (·). To see this, note that
Pr (X ≤x)
=
Pr

F −1
X (U) ≤x

=
Pr (U ≤FX (x))
=
 FX(x)
0
du = FX (x) .
The equality in the second line follows because FX (x) is a monotone
function.
■
Simulating Uniform Random Numbers
The uniform distribution plays an important role in the generation of pseu-
dorandom numbers with a computer. Ripley (1987) and Ross (1997) discuss
how uniform random “deviates” can be produced in an electronic com-
puter; Fishman (1973) presents a readable discussion. A summary of the
basic ideas follows.
The most common algorithm for generating “random” numbers actually
produces a nonrandom sequence of values. Each number depends on the
previous one, which implies that all numbers are determined by the initial
value in the sequence. However, if a generator is designed appropriately (in
some sense which is beyond the scope of this book), the numbers appear
as “independent” draws from a uniform distribution. A popular method is
known as the “linear congruential generator”. It has the form
Zi ≡aZi−1 + c mod (m) ,
(1.29)
where the “seed” Z0 is an integer 0 ≤Z0 < m, the multiplier a is an
integer 1 < a < m, c is an integer increment (0 ≤c < m), and modulus m
is an integer 0 < m. The modulus notation means that Zi is the remainder
obtained when aZi−1 +c is divided by m. This remainder can be expressed

20
1. Probability and Random Variables
as
Zi = aZi−1 + c −
aZi−1 + c
m

m,
where
aZi−1 + c
m

= largest integer in the quantity,
note that aZi−1 + c is integer. For example, if the quantity within brackets
is 12
5 , the largest integer is 2; if the quantity is 1
6, the largest integer is 0.
The arithmetic guarantees that each element in the sequence {Zi} is an
integer in the interval [0, m −1] . Then, when {Zi} displays “suﬃciently
random” behavior, a pseudorandom sequence in the unit interval can be
obtained as
Xi = Zi
m .
Since Zi < m, the sequence can contain at most m distinct numbers and,
as soon as a number is repeated, the entire sequence repeats itself. Thus,
m should be as large as possible to ensure a large quantity of distinct
numbers. The parameters a, c and Z0 must be selected so that as many
as possible of the m numbers occur in a cycle. It can be shown (Cohen,
1986) that the sequence {Zi} is periodic: there exists a P ≤m such that
Zi = Zi+P for all i ≥0. A large P (therefore, a large m) is desirable
for achieving apparent randomness. A generator is said to have full period
when P = m; however, a full period does not guarantee that a generator
is adequate. For example, old generators, such as RANDU (formerly used
in the IBM Scientiﬁc Subroutine Package; Cohen, 1986) had a = 65, 539,
c = 0 and m = 231; it had a maximal period of 229, and yet failed some
tests. On the other hand, the full-period LCG generator (Cohen, 1986) has
a = 69, 069, c = 1 and m = P = 232 and has failed tests as well. Marsaglia
and Zaman (1993) describe “Kiss”, a generator that has displayed excellent
performance from a statistical point of view (Robert, 1994).
The dependency between Zi and Z0 can be deduced as follows. Consider
(1.29) and write
Z1
≡
aZ0 + c mod (m) ,
Z2
≡
aZ1 + c mod (m) ,
Z3
≡
aZ2 + c mod (m) .
Expressing Z3 in terms of Z2, and this in terms of Z1, one obtains the
identity
Z3 ≡a3Z0 + (a2 + a + 1)c mod (m)
and, more generally
Zn ≡anZ0 + (an−1 + an−2 + ... + a + 1)c mod (m) .

1.3 Univariate Continuous Distributions
21
Note that
(an−1 + an−2 + ... + a + 1)(a −1) = an −1.
Hence
Zn ≡anZ0 + an −1
(a −1)c mod (m)
(1.30)
and, consequently,
Xn ≡anX0 +
an −1
(a −1)mc mod (m)
where X0 = Z0/m.
Example 1.7
A hypothetical random number generator
In order to illustrate, an example from Fishman (1973) will be modiﬁed
slightly. Consider a hypothetical generator with a = 3, c = 0, Z0 = 4, and
m = 7. The pseudo-random uniform deviates, given in the table below, can
be computed with (1.29) or with (1.30):
i
Zi = aZi−1 + c

aZi−1+c
m

m
Xi = Zi
m
0
4
4/7 = 0.5714
1
3 × 4 −
 3×4
7
!
× 7 = 12 −1 × 7 = 5
5/7 = 0.7143
2
3 × 5 −
 3×5
7
!
× 7 = 15 −2 × 7 = 1
1/7 = 0.1429
3
3 × 5 −
 3×5
7
!
× 7 = 15 −2 × 7 = 1
3/7 = 0.4286
4
3 × 3 −
 3×3
7
!
× 7 = 9 −1 × 7 = 2
2/7 = 0.2857
5
3 × 2 −
 3×2
7
!
× 7 = 6 −0 × 7 = 6
6/7 = 0.8571
6
3 × 6 −
 3×6
7
!
× 7 = 18 −2 × 7 = 4
4/7 = 0.5714
The sequence repeats itself after six steps. If the parameters stay as before
and m = 8, a string of 4s is obtained, illustrating a very poor generator of
pseudo-random numbers.
■
Beta Distribution
The beta distribution can be used as a model for random variables that
take values between zero and one, such as probabilities, a gene frequency, or
the coeﬃcient of heritability. A random variable X has a beta distribution
if its p.d.f. has the form
p (x|a, b) = Be (x|a, b) =

Cxa−1 (1 −x)b−1 ,
for x ∈[0, 1] ,
0,
otherwise,
(1.31)

22
1. Probability and Random Variables
where a > 0 and b > 0 are parameters of this distribution. The value of
the integration constant is C = Γ (a + b) /Γ (a) Γ (b), where
Γ (α) =
∞

0
xα−1e−x dx,
α > 0,
(1.32)
and Γ (·) is the gamma function, as seen in (1.25). Note that, if α = 1,
Γ (1) =
∞

0
e−x dx = −e−x""∞
0 = 1.
Suppose α > 1. The integral in (1.32) can be evaluated by parts. Recall
the basic calculus result

u dv = uv −

v du.
Put u = xα−1 and dv = e−x; then du = (α −1) xα−2 and v = −e−x. The
integral sought is then
Γ (α) =
∞

0
xα−1e−x dx = −xα−1e−x""∞
0 +
∞

0
(α −1) xα−2e−x dx
= (α −1)
∞

0
xα−2e−x dx = (α −1) Γ (α −1)
(1.33)
with the preceding following from the deﬁnition of the gamma function in
(1.32). Thus, provided α is integer and greater than one,
Γ (α) = (α −1) (α −2) . . . 3 × 2 × 1 × Γ (1) = (α −1)!
(1.34)
Consider now the beta density in (1.31), and write it explicitly as
p (x|a, b) = Γ (a + b)
Γ (a) Γ (b)xa−1 (1 −x)b−1 .
(1.35)
The expected value and variance of the beta distribution can be deduced
by using the fact that the constant of integration is such that
C−1 =
1

0
xa−1 (1 −x)b−1 dx = Γ (a) Γ (b)
Γ (a + b) .
(1.36)

1.3 Univariate Continuous Distributions
23
The mean value of a beta distributed random variable is then
E (X|a, b) = C
 1
0
x

xa−1 (1 −x)b−1
dx
= C
 1
0
xa+1−1 (1 −x)b−1 dx
= C Γ (a + 1) Γ (b)
Γ (a + b + 1)
= Γ (a + b)
Γ (a) Γ (b)
Γ (a + 1) Γ (b)
Γ (a + b + 1)
=
Γ (a + b) aΓ (a)
Γ (a) (a + b) Γ (a + b) =
a
(a + b).
(1.37)
Using a similar development, it can be established that
V ar (X|a, b)
=
E

X2|a, b

−E2 (X|a, b)
=
ab
(a + b)2 (a + b + 1)
.
(1.38)
Example 1.8
Variation in gene frequencies
Consider the following, patterned after Wright (1968). Suppose n alleles are
sampled from some population, and that the outcome of each draw is either
A or a. In the absence of correlation between outcomes, the probability
distribution of X, the number of A alleles observed in the sample of size
n, can be calculated using the binomial distribution (1.10); the probability
θ corresponds to the frequency of allele A in the population. Wright refers
to this set of n draws as “clusters”. If this sampling scheme were to be
repeated a large number of times, an excess of clusters consisting largely or
entirely of A or of a would indicate correlation between outcomes resulting,
for example, from family aggregation. The distribution of correlated draws
can be derived by assuming that θ varies according to some distribution.
Because θ varies between 0 and 1, the beta distribution is convenient here.
The parameterization of Wright is adopted subsequently. Let
c = a + b
and
θ =
a
a + b
be the mean of the distribution. The beta density (1.35) can then be written
as
p

θ|θ, c

=
Γ (c)
Γ

θc

Γ

c

1 −θ
θcθ−1 (1 −θ)c(1−θ)−1 .
(1.39)

24
1. Probability and Random Variables
If gene frequencies vary at random in the clusters according to this beta
distribution, the between-cluster variance of gene frequencies is given by
V ar

θ|θ, c

= θ

1 −θ

1 + c
.
This variability can accommodate “extra-binomial” variation and account
for the possible correlations between alleles that have been drawn. This
example will be elaborated further later.
■
Gamma, Exponential, and Chi-square Distributions
The gamma distribution arises in quantitative genetic studies of variance
components and of heritability through Bayesian methods. For example,
Wang et al. (1994) analyzed a selection experiment for increased proliﬁ-
cacy in pigs in which the initial state of uncertainty about genetic and
environmental variance components was represented by “inverted” gamma
distributions (these are random variables whose reciprocals are distributed
according to gamma distributions). A random variable X has a gamma
distribution Ga (a, b) if its p.d.f. has the form
p (x|a, b) = Ga (x|a, b) = Cxa−1 exp [−bx] ,
x > 0,
(1.40)
where C = ba/Γ (a); the “shape” parameter a and the “inverse scale”
parameter b are positive. A special case of interest arises when a = 1; then
X is said to have an exponential distribution with parameter b and density
p (x|b) = b exp (−bx) ,
x ≥0.
(1.41)
The exponential distribution has been employed for modeling survival or
length of productive life in livestock (Famula, 1981).
Another special case of interest is when a = v/2 and b = 1/2; then X is
said to possess a central chi-square distribution with positive parameter v
(often known as degrees of freedom) and density
p (x|v) = Cx(v/2)−1 exp (−x/2) ,
for v > 0 and x > 0,
(1.42)
where the integration constant is
C = (1/2)v/2
Γ (v/2) .
(1.43)
The chi-square process will be revisited in a section where quadratic forms
on normal variables are discussed.

1.3 Univariate Continuous Distributions
25
The kth (k = 1, 2, ...) moment from the origin of the gamma distribution
is, by deﬁnition,
E

Xk|a, b

=
∞

0
xk ba
Γ (a)xa−1 exp (−bx) dx
=
ba
Γ (a)
∞

0
xk+a−1 exp (−bx) dx.
Let X = Y 2, so dx = 2y dy. Then
E

Xk|a, b

= 2ba
Γ (a)
∞

0
y2(k+a)−1 exp

−by2
dy.
Using the gamma integral in (1.25) with p = 2(k + a),
E

Xk|a, b

=
2ba
Γ (a)
 ∞
0
y
2(k+a)−1 exp

−by2
dy
=
ba
Γ (a)b−2(k+a)
2
Γ

2(k + a)
2

.
(1.44)
The mean of the distribution follows, by putting k = 1,
E (X|a, b) = Γ (a + 1)
bΓ (a)
= a
b
(1.45)
because Γ (a + 1) = aΓ (a) , as seen in (1.33). Similarly,
E

X2|a, b

= Γ (a + 2)
b2Γ (a)
= a (a + 1)
b2
.
The variance of the distribution is thus,
V ar (X|a, b) = a
b2 .
(1.46)
The coeﬃcient of variation (ratio between the standard deviation and the
mean of the distribution) is equal to 1/√a, so it depends on only one of
the parameters.
The Normal and Student-t Distributions
The normal distribution, without doubt, is the most important random pro-
cess in statistical genetics. It has played a central role in model development
(e.g., the inﬁnitesimal model of inheritance; for a thorough discussion, see

26
1. Probability and Random Variables
Bulmer, 1980); selection theory (Pearson, 1903; Gianola et al., 1989), esti-
mation of dispersion parameters (Henderson, 1953; Patterson and Thomp-
son, 1971; Searle et al., 1992); prediction of genetic values (Henderson, 1963,
1973, 1975), inference about response to selection (Sorensen et al., 1994),
and evaluation of hypotheses (Edwards, 1992; Rao, 1973). The density of
a normally distributed random variable X with mean µ and variance σ2,
i.e., N

x|µ, σ2
, has been presented already, and is
p

x|µ, σ2
=
1
σ
√
2π exp

−(x −µ)2
2σ2

,
for −∞< x < ∞,
(1.47)
with −∞< µ < ∞and σ > 0 being the parameters and their corre-
sponding spaces. Since p (µ + ε) = p (µ −ε), it is seen that the distri-
bution is symmetric about µ, so the mean and median are equal, with
E

X|µ, σ2
= µ. The parameter σ is the standard deviation of X. The
density has a maximum at x = µ (so the mean is equal to the mode and to
the median) and, at this value, p (µ) =

σ
√
2π
−1. Furthermore, the den-
sity has inﬂection points (where the curve changes from concave to convex)
at µ ± σ.
In order to derive the mean and variance, use is made of what is called
the moment generating function. Let X be a random variable having some
distribution, and consider a Taylor series expansion of exp (tX) about X =
0, where t is a dummy variable. Then
exp (tX)
=
1 + {t exp (tX)}X=0 X +

t2 exp (tX)

X=0
X2
2! + · · ·
· · · +

tk exp (tX)

X=0
Xk
k! +
=
1 + tX + t2X2
2!
+ · · · + tkXk
k!
+ · · ·
The moment generating function is the average value of exp (tX):
E [exp (tX)] = 1 + tE (X) + t2E

X2
2!
+ · · · + tkE

Xk
k!
+ · · ·
(1.48)
which is a linear combination of all moments of the distribution. Diﬀerenti-
ation of (1.48) with respect to t, and setting t = 0, yields E(X). Likewise,
diﬀerentiation of (1.48) twice with respect to the dummy variable, and set-
ting t = 0, gives E

X2
; in general, k diﬀerentiations (followed by putting
t = 0) will produce E

Xk
. With this technique, moments can be found
for most distributions, provided the moment generating function exists.

1.3 Univariate Continuous Distributions
27
Example 1.9
Finding the mean and variance of a normal process
For the normal distribution, the moment generating function is
M (t) =
∞

−∞
exp (tx)
1
σ
√
2π exp

−(x −µ)2
2σ2

dx.
(1.49)
The exponents in the integral can be combined, by adding and subtracting
terms and by completing a square, as follows:
tx −(x −µ)2
2σ2
= tx −

 x2
2σ2 + µ2
2σ2 −2µx
2σ2

+

µt + σ2t2
2

−

µt + σ2t2
2

=

µt + σ2t2
2

+ 2σ2tx
2σ2
−

 x2
2σ2 + µ2
2σ2 −2µx
2σ2

−

2σ2µt
2σ2
+ σ4t2
2σ2

=

µt + σ2t2
2

−1
2σ2

x2 + µ2 + σ4t2 −2µx −2σ2tx −2σ2µt

=

µt + σ2t2
2

−

x −

µ + σ2t
2
2σ2
.
Employing this in (1.49), one obtains
M (t)
=
exp

µt + σ2t2
2

σ
√
2π
∞

−∞
exp
#
−

x −

µ + σ2t
2
2σ2
$
dx
=
exp

µt + σ2t2
2

(1.50)
as the integral evaluates to σ
√
2π, the reciprocal of the integration constant
of the normal distribution. The moments of the distribution can now be
obtained by successive diﬀerentiation of M(t) with respect to t, and then
evaluating the result at t = 0. For example, the ﬁrst moment is
E (X)
=
M ′ (0) =
 d
dt exp

µt + σ2t2
2

t=0
=

µ + σ2t

exp

µt + σ2t2
2

t=0
= µ,

28
1. Probability and Random Variables
yielding the mean of the distribution. Similarly, the second moment is
E

X2
=
M ′′ (0) =

σ2 +

µ + σ2t
2
exp

µt + σ2t2
2
%
t=0
=
σ2 + µ2.
It follows that the variance of the distribution is E

X2
−E2 (X) = σ2,
as anticipated.
■
The normal and Student-t distributions are intimately related to each
other. If a random variable X has a t

µ, σ2, ν

distribution, its density
function is
p

x|µ, σ2, ν

= Γ [(ν + 1) /2]
Γ (ν/2) √νπσ

1 + (x −µ)2
νσ2
−(ν+1)/2
.
(1.51)
It can be seen that this distribution is symmetric about µ, its mean. Two
additional positive parameters here are: the degrees of freedom ν and the
scale of the distribution σ. The variance of a t process is
V ar

X|µ, σ2, ν

=
ν
(ν −2)σ2.
Thus, when ν is 2 or less the variance is inﬁnite. When the degrees of
freedom parameter goes to inﬁnity, the process tends toward a normal
N

µ, σ2
one; when ν = 1 the distribution is called Cauchy (whose mean
and higher moments do not exist).
Derivation of density (1.51) requires consideration of the distribution of
pairs of random variables. As shown in more detail later on in this chapter,
if given µ and S2
i , Xi is normally distributed
Xi|µ, S2
i ∼N

µ, S2
i

,
(1.52)
and if S2
i follows a scaled inverted chi-square distribution with parameters
ν and σ2:
S2
i |ν, σ2 ∼νσ2χ−2
ν ,
(1.53)
then the density
p

xi|µ, σ2, ν

=

p

xi|µ, S2
i

p

S2
i |ν, σ2
dS2
i
(1.54)
is that of the t

µ, σ2, ν

distribution (1.51). The t distribution can therefore
be interpreted as a mixture of normal distributions with a common mean
and a variance that varies according to a scaled inverted chi-square.
This way of viewing the t distribution leads to a straightforward method
for drawing Monte Carlo samples. First draw S2∗from (1.53); second, draw
x∗
i from

Xi|µ, S∗2
. Then x∗
i is a draw from (1.54) and repeating this
procedure generates an i.i.d. sample from (1.54). This is known as the
method of composition (Tanner, 1996).

1.4 Multivariate Probability Distributions
29
1.4
Multivariate Probability Distributions
Results presented for a single random variable generalize in a fairly direct
way to multivariate situations, that is, to settings in which it is desired
to assign probabilities to events involving several random variables jointly.
For example, consider the joint distribution of genotypes at two loci for a
quantitative trait. A question of interest might be if genotype Bb, say, at
locus B appears more frequently in association with genotype aa at locus
A than with either genotype AA or Aa. Here one can let X be a random
variable denoting genotype at locus A, and Y denote the genotype at locus
B. The joint distribution of X and Y is called a bivariate distribution.
Whenever the random process involves two or more random variables, one
speaks of multivariate probability distributions.
We start this section by introducing the concept of independence. Let
(X1, X2, . . . , Xn) be a random vector whose elements Xi (i = 1, 2, . . . , n)
are one-dimensional random variables. Then
X1, X2, . . . , Xn
are called mutually independent if, for every (x1, x2, . . . , xn),
p (x1, x2, . . . , xn) = p (x1) p (x2) . . . p (xn) .
(1.55)
The term p (xi) is a marginal probability density. It is obtained by inte-
grating (adding if the random variable is discrete) over the distribution of
the remaining (n −1) random variables
p (xi) =

p (x1, . . . , xi−1, xi, xi+1, . . . , xn) dx1 . . . dxi−1 dxi+1 . . . dxn,
(1.56)
where the integral is understood to be of dimension (n −1). Mutual inde-
pendence implies pairwise independence; however, pairwise independence
does not imply mutual independence.
A generalization of the concept of i.i.d. random variables is that of ex-
changeable random variables, an idea due to De Finetti (1975b). The ran-
dom variables X1, X2, . . . , Xn are exchangeable, if any permutation of any
subset of size k of the random variables, for any k ≤n, has the same dis-
tribution. That is, for exchangeable random variables, any permutation of
the indexes will lead to the same joint probability density. In practice, in a
modeling context, the idea of exchangeability is associated with ignorance
or symmetry: the less one knows about a problem, the more conﬁdently one
can make claims of exchangeability. For example, in rolling a die on which
no information is available, one is prepared to assign equal probabilities
to all six outcomes. Then the probability of obtaining a 1 and a 5, in two
independent throws, should be the same as that of obtaining any other two
possible outcomes. Note that random variables that are not independent

30
1. Probability and Random Variables
can be exchangeable. For example, if X1, X2, . . . , Xn are i.i.d. as Bernoulli
Br (θ), then, given n
i=1 Xi = t, X1, X2, . . . , Xn are exchangeable, but not
independent.
For simplicity, the remainder of this section motivates developments us-
ing a bivariate situation. Analogously to the univariate case, the joint c.d.f.
of two random variables X and Y is deﬁned to be
F (x, y) = Pr (X ≤x, Y ≤y) ,
(1.57)
where Pr (X ≤x, Y ≤y) means “probability that X takes a value smaller
than or equal to x and Y takes a value smaller than or equal to y”. If
the two random variables are discrete, their joint probability distribution
is given by
Pr (X = x, Y = y) = p (x, y) .
(1.58)
When the two random variables are continuous, a density function must
be introduced, as in the univariate case, because the probability that X = x
and Y = y is 0. The joint p.d.f. of random variables (X, Y ), that take value
in R2, is, by deﬁnition,
p (x, y) = ∂2F (x, y)
∂x∂y
(1.59)
provided the distribution function F (·) is diﬀerentiable (in this case at
least twice). The joint p.d.f. p (x, y) is deﬁned for all (x, y) in R2. The joint
probability that (X, Y ) belong to a subset A of R2 is given by the two-fold
integral
Pr [(X, Y ) ∈A]
=
 
A
p (x, y) dx dy
=
 
I [(x, y) ∈A] p (x, y) dx dy.
For example, if A = (a1, b1) × (a2, b2), then the integral above is
Pr [(X, Y ) ∈A]
=
 
I [x ∈(a1, b1) , y ∈(a2, b2)] p (x, y) dx dy
=
 b1
a1
 b2
a2
p (x, y) dx dy.
Conditional distributions play an important role when making inferences
about nonobservable quantities given information on observable quantities.
The conditional probability distribution of X given Y = y is the function
of X given by
p (x|y) = p (x, y)
p (y) ,
p (y) > 0.
(1.60)

1.4 Multivariate Probability Distributions
31
In the case p (y) = 0, the convention is that p (x|y) = p (x). Expression
(1.60) implies that
p (x, y) = p (x|y) p (y)
regardless of whether p (y) > 0. In the case of continuous random variables,
notice that (1.60) integrates to 1:

p (x|y) dx
=

p (x, y) dx
p (y)
=
p (y)
p (y) = 1.
Because p (x|y) is a function of X at the ﬁxed value Y = y, one can also
write
p (x|y) ∝p (x, y)
(1.61)
as the denominator does not depend on x. This result holds both for dis-
crete and continuous random variables, and highlights the fact that a joint
distribution must be the starting point in a multivariate analysis. Again,
when X and Y are statistically independent, p(x, y) = p(x)p(y) and
p (x|y) = p (x) p (y)
p (y)
= p (x) .
(1.62)
In the case of n mutually independent random variables X1, X2, . . . , Xn,
the conditional distribution of any subset of the coordinates, given the value
of the rest, is the same as the marginal distribution of the subset.
In the discrete two-dimensional case, the marginal distribution of X is
given by
Pr (X = x) = p (x) =

y
p (x, y) =

y
p (x|y) p (y) .
(1.63)
Above, the sum is over all possible values of Y , each weighted by its
marginal probability Pr (Y = y). For instance, if one were interested in
ﬁnding the marginal distribution of genotypes at locus A from the joint
distribution of genotypes at the two loci, application of (1.63) yields
Pr(X = x) = p(x|BB)p(BB) + p(x|Bb)p(Bb) + p(x|bb)p(bb)
where x = AA, Aa, or aa. This gives the total probability that X = x
(Aa, say) as the sum of the probabilities of the three ways in which this
can occur, i.e., when X = Aa jointly with Y being either BB, Bb, or bb.
Further, the conditional probability that Y = BB given that X = Aa can

32
1. Probability and Random Variables
be written, making use of (1.60), as
Pr(Y = BB|X = Aa)
=
p(Aa|BB)p(BB)
p(Aa|BB)p(BB) + p(Aa|Bb)p(Bb) + p(Aa|bb)p(bb)
=
p(BB|Aa)p(Aa)
p(Aa|BB)p(BB) + p(Aa|Bb)p(Bb) + p(Aa|bb)p(bb).
In the continuous case, the counterpart of (1.63) is the two-dimensional
analogue of (1.56), and is obtained by integrating over the distribution of
Y :
p (x) =

p (x, y) dy =

p (x|y) p (y) dy.
(1.64)
Results (1.60) and (1.62) are fairly general, and apply to either scalar or
vector variates. The random variables can be either discrete or continuous,
or one can be discrete while the other continuous. For example, suppose Y
is the genotype at a molecularly marked locus (discrete random variable)
and X is the breeding value at a QTL for growth rate in pigs (continuous).
If the observation that Y = y alters our knowledge about the distribution
of breeding values, then one can exploit this stochastic dependence in a
marker-assisted selection program for faster growth. Otherwise, if knowl-
edge of the marked locus does not contribute information about growth
rate, we would be in the situation depicted by (1.62).
Often, a joint density can be identiﬁed only up to proportionality. As in
the univariate case, one can write p(x, y) = Cf(x, y), where C is a constant
(i.e., it does not depend on X and Y ), and f(x, y) is known as the kernel
of the joint density. Hence,
p (x, y) =
f (x, y)
 
f (x, y) dx dy .
(1.65)
It follows that
C−1 =
 
f (x, y) dx dy.
(1.66)
Before reviewing standard multivariate distributions, a number of exam-
ples are discussed to illustrate some of the ideas presented so far.
Example 1.10
Aitken’s integral and the bivariate normal distribution
A useful result in multivariate normal theory is Aitken’s integral (e.g.,
Searle, 1971). Here assume that the random variables X and Y possess
what is called a bivariate normal distribution, to be discussed in more
detail below. Such a process can be represented as

X
Y

∼N


µX
µY

,

σ2
X
σXY
σXY
σ2
Y

,

1.4 Multivariate Probability Distributions
33
where µX = E (X), µY = E (Y ) , and
V =

σ2
X
σXY
σXY
σ2
Y

is the 2×2 positive-deﬁnite variance–covariance matrix of the distribution.
The joint density of X and Y is
p(x, y|µX, µY , V) =
1
2π |V|
1
2
× exp

−1
2

X −µX,
Y −µY
′ V−1

X −µX
Y −µY

.
(1.67)
The kernel of the density is the expression exp [·], and Aitken’s integral is
 
exp

−1
2

X −µX,
Y −µY
′ V−1

X −µX
Y −µY

dx dy
=
2π |V|
1
2
which follows because p(x, y|µX, µY , V) integrates to 1. Then C−1 = 2π |V|
1
2 ,
and the bivariate normal density can be represented as
p(x, y|µX, µY , V) =
exp

−1
2

X −µX,
Y −µY
′ V−1

X −µX
Y −µY

 
exp

−1
2

X −µX,
Y −µY
′ V−1

X −µX
Y −µY

dx dy
.
■
Example 1.11
A discrete bivariate distribution
This example is elaborated after Casella and Berger (1990). Mastitis is
a serious disease of the mammary gland in milk producing animals, and
it has important economic consequences in dairy farming. Suppose that
in a breed of sheep, the disease appears in three mutually exclusive and
exhaustive modalities: absent, subclinical, or clinical. Let Y be a random
variable denoting the disease status. Also, let X be another random variable
taking two values only, X = 1 or X = 2, representing ewes that are born as
singles or twins, respectively. From knowledge of the population, it is known
that the bivariate random vector (X, Y ) has a joint probability distribution
given by:
Y = 1
Y = 2
Y = 3
p (x)
X = 1
1/10
2/10
2/10
5/10
X = 2
1/10
1/10
3/10
5/10
p (y)
2/10
3/10
5/10

34
1. Probability and Random Variables
The entries in this table give the joint probabilities of the six possible
events. The element in the ﬁrst row and column represents
Pr (X = 1, Y = 1) = 1/10.
The last column gives the marginal probability distribution of X, and the
last row presents the marginal probability distribution of Y . For example,
the marginal probability of an animal born as a single (X = 1) is obtained
as
Pr (X = 1) = Pr (X = 1, Y = 1) + Pr (X = 1, Y = 2)
+ Pr (X = 1, Y = 3) = 5
10.
This can also be obtained from
Pr (X = 1) =
i=3

i=1
Pr (X = 1|Y = yi) Pr (Y = yi)
=

1
2
2
10

+

2
3
3
10

+

2
5
5
10

= 5
10.
The random variables X and Y are not independent because
p (x, y) ̸= p (x) p (y) .
For example,
p (1, 3) = 1
5 ̸= 1
2
1
2 = Pr (X = 1) Pr (Y = 3) .
Note that there are some (X, Y ) values for which their joint probability is
equal to the product of their marginals. For example,
p (1, 1) = 1
10 = 1
2
1
5 = Pr (X = 1) Pr (Y = 1) .
However, this does not ensure independence, as seen from the case of p(1, 3).
All values must be checked.
■
Example 1.12
Two independent continuous random variables
Suppose the following function is a suitable density arising in the descrip-
tion of uncertainty about the pair of continuous random variables X and
Y
p (x, y) =

6xy2,
0 < x < 1, 0 < y < 1,
0,
otherwise.
The joint sample space can be viewed as a square of unit length. We check
ﬁrst whether this function is suitable as a joint p.d.f., by integrating it over

1.4 Multivariate Probability Distributions
35
the entire sample space
∞

−∞
∞

−∞
p (x, y) dx dy
=
1

0
1

0
6xy2dx dy
=
1

0
3y2dy = y3""1
0
=
1.
It is not essential that the function integrates to 1; the only requirement
is that the integral must be ﬁnite. For example, if it integrates to K, say,
then the joint p.d.f. would be 6xy2/K. When a density function does not
integrate to a ﬁnite value, the corresponding distribution is said to be im-
proper. Improper distributions arise in certain forms of Bayesian analysis,
as will be seen later on. Employing (1.64), the marginal density of Y is
obtained by integrating the joint density p(x, y) over the distribution of X
p (y) =
1

0
p (x, y) dx =
1

0
6xy2 dx = 6y2 x2
2
""""
1
0
= 3y2.
It can be veriﬁed that the resulting density integrates to 1. We say that X
has been “integrated out” of the joint density; the resulting marginal p.d.f.
of Y is, therefore, not a function of X. This is important in a Bayesian
context, where “nuisance parameters” are eliminated by integration. For
example, if X were a nuisance parameter in a Bayesian model, the marginal
density of the parameter of interest (Y in this case) would not depend on
X. Likewise, the marginal density of X is
p (x) =
1

0
p (x, y) dy =
1

0
6xy2 dy = 2x,
and this integrates to 1 as well. The density function of the conditional
distribution of Y given X is, using (1.60),
p (y|x) = p (x, y)
p (x)
= 6xy2
2x
= 3y2, for 0 < y < 1.
Similarly, the conditional density of X given Y is
p (x|y) = p (x, y)
p (y)
= 6xy2
3y2 = 2x.
Hence, the conditional distribution [Y |X] does not depend on X, and the
conditional distribution [X|Y ] does not depend on Y (in general, the nota-
tion [A|B, C] is employed to refer to the conditional distribution of random

36
1. Probability and Random Variables
variable A given random variables B and C). This is because X and Y are
independent random variables: their joint density is obtained by multipli-
cation of the marginal densities
p (x, y) = p (x) p (y) = 2x3y2 = 6xy2.
Because of independence, p (y|x) = p (y) and p (x|y) = p (x), as already
seen.
We can calculate the regression curve, or regression function, of Y on X,
which is denoted by E (Y |X = x). This function, using the assumptions
made in the preceding example, is
E (Y |X = x) =
1

0
yp (y|x) dy =
1

0
y3y2 dy = 3
4.
(1.68)
This is a constant because Y is independent of X, so E(Y |x) = E(Y ) here.
The uncertainty about the distribution of Y , once we know that X = x,
can be assessed through the variance of the distribution [Y |X], that is, by
V ar (Y |X = x). By deﬁnition
V ar (Y |X = x) = E

Y 2|x

−E2 (Y |x) .
For the example above, having computed E(Y |x) = E(Y ) = 3/4, we need
now
E

Y 2|x

=
1

0
y2 p (y|x) dy =
1

0
y2 p (y) dy = E

Y 2
= 3
5
so
V ar (Y |X = x) = 3/5 −[3/4]2 = 3/80,
which is equal to the variance of the distribution of Y in this case. As
expected (because of the independence noted), the variance does not de-
pend on X. Here, it is said that the dispersion is homoscedastic or constant
throughout the regression line.
■
Example 1.13
A continuous bivariate distribution
Two random variables have as joint p.d.f.
p (x, y) =

x + y,
0 ≤x ≤1, 0 ≤y ≤1,
0,
otherwise.
The marginal densities of X and Y are
p (x)
=
1

0
p (x, y) dy = x + 1
2,
p (y)
=
y + 1
2.

1.4 Multivariate Probability Distributions
37
The probability that X is between 1/2 and 1 can be found to be equal to
1

1
2
(x + 1/ 2) dx = 5/8.
The probability that X is between 1/2 and 1 and that Y is between 0 and
1/2 is
1

1
2
1
2

0
(x + y) dx dy = 1/4.
The variables X and Y are not independent, as p (x, y) ̸= p (x) p (y). The
conditional density of Y given x is
p (y|x) = p (y, x)
p (x)
= y + x
x + 1
2
.
The regression function of Y on X is
E (Y |X = x) =
1

0
yp (y|x) dy =
1

0
y (y + x)
x + 1
2
dy =
3x + 2
3 (1 + 2x),
which is a decreasing, nonlinear function of X. The variance of the condi-
tional distribution [Y |X] can be found to be equal to
V ar (Y |X = x) =

1 + 6x + 6x2

18 (1 + 2x)2 ,
0 ≤x ≤1.
Here the conditional distribution is not homoscedastic. For example, for
x = 0 and x = 1, the conditional variance is equal to 1/18 and 13/162,
respectively.
■
1.4.1
The Multinomial Distribution
The binomial distribution is generalized to the multinomial distribution as
follows. Let C1, C2, . . . , Ck represent k mutually exclusive and exhaustive
classes or outcomes. Imagine an experiment consisting of n independent
trials and, in each trial, only one of these k distinct outcomes is possible.
The probability of occurrence of outcome i is pi (i = 1, 2, . . . , k) on every
trial. Let Xi be a random variable corresponding to the number (counts)
of times that the ith outcome occurred in the n trials. When k = 2, this
is a binomial experiment and X1 counts the number of “successes” and
X2 = n −X1 counts the number of “failures” in the n independent trials.

38
1. Probability and Random Variables
The p.m.f. of the random vector (X1, X2, . . . , Xk) is
Pr (X1 = x1, X2 = x2, . . . , Xk = xk|X1 + X2 + · · · + Xk = n)
=
n!
x1! x2! . . . xk!px1
1 px2
2 . . . pxk
k ,
(1.69)
with k
i=1 xi = n, k
i=1 pi = 1. Therefore an alternative way of writing
(1.69) is
Pr (X1 = x1, X2 = x2, . . . , Xk = xk|X1 + X2 + · · · + Xk = n)
= n!px1
1 px2
2 . . . (1 −p1 −· · · −pk−1)(n−x1−···−xk−1)
x1!x2! . . . (n −x1 −· · · −xk−1)!
.
Observe that the k random variables (X1, X2, . . . , Xk) are not independent;
any k −1 of them determines the kth. The mean and (co)variance of this
distribution are
E (Xi) = npi,
V ar (Xi) = npi (1 −pi) ,
Cov (Xi, Xj) = −npipj,
i ̸= j.
The n trials can be divided into two classes: Xi counts belonging to out-
come i, and the remaining events corresponding to “non-i”. The marginal
distribution of the random variable Xi is binomial with p.m.f.
Bi (xi|pi, n) .
(1.70)
Further, given Xi, the vector X−i = (X1, . . . , Xi−1, Xi+1, . . . , Xk) is
multinomially distributed
X−i|Xi ∼Mu (q1, . . . , qi−1, qi+1, . . . , qk, n −Xi) ,
(1.71)
where qj = pj/(1 −pi).
Let
X−i,−j = (X1, . . . , Xi−1, Xi+1, . . . , Xj−1, Xj+1, . . . , Xk) .
Then the p.m.f. of the distribution [X−i,−j|Xi, Xj] is,
Mu (x−i,−j|r1, . . . , ri−1, ri+1, . . . , rj−1, rj+1, . . . , rk, n −Xi −Xj) (1.72)
where rs = ps/ (1 −pi −pj).
As in the binomial case, a generalization of (1.16) exists. As n →∞, the
vector of observed responses X will tend to a multivariate normal distri-
bution (to be discussed below) with mean vector
E (X) = {npi} ,
i = 1, . . . , k,
(1.73)

1.4 Multivariate Probability Distributions
39
and covariance matrix
V =n


p1 (1 −p1)
−p1p2
· · ·
−p1pk
−p2p1
p2 (1 −p2)
· · ·
−p2pk
...
...
...
...
−pkp1
−pkp2
· · ·
pk (1 −pk)

.
(1.74)
Summing the elements of any row of (1.74), the ith, say, yields
npi (1 −p1 −p2 −· · · −pk) = 0.
Hence V has rank k −1. A generalized inverse of V can readily be shown
(Stuart and Ord, 1991) to be equal to
V−= 1
n
 ,V−1
0
0
0

where ,V−1 is a (k −1) × (k −1) matrix with diagonal elements (1/pi) +
(1/pk) (i = 1, 2, . . . , k −1) and oﬀ-diagonals 1/pk. Therefore the exponent
of the limiting singular multivariate normal distribution has the following
form:
(x−np)′ V−(x−np) ,
(1.75)
where p′ = (p1, . . . , pk). For large n, the random variable deﬁned in (1.75)
has a central chi-square distribution with tr (V−V) = (k −1) degrees of
freedom (Searle, 1971). This gives the basis for the usual test of goodness
of ﬁt.
Example 1.14
Generating samples from the multinomial distribution
A general procedure for simulating multivariate random variables is based
on the composition or conditional distribution method (Devroye, 1986).
Let
X = (X1, . . . , Xd)
denote a d-dimensional random vector with p.d.f. f (x). Then,
f (x) = f (x1) f (x2|x1) f (x3|x1, x2) . . . f (xd|x1, . . . , xd−1) .
If the marginal distributions and all the conditional distributions are known,
this method allows reducing a multivariate generation problem to d uni-
variate generations.
As an example, consider generating a multinomial random variable
(X1, X2, X3, X4) ∼Mu (p1, p2, p3, p4, n)
with 4
i=1 pi = 1 and 4
i=1 Xi = n. Using (1.70), (1.71), and (1.72), one
proceeds as follows. Generate
X1 ∼Bi (p1, n) ,

40
1. Probability and Random Variables
X2|X1 = x1 ∼Bi

p2
1 −p1
, n −x1

,
X3|X1 = x1, X2 = x2 ∼Bi

p3
1 −p1 −p2
, n −x1 −x2

.
Set
X4 = n −x1 −x2 −x3.
The vector (x1, x2, x3, x4) is a realized value from X. If at any time in the
simulation n = 0, use the convention that a Bi (p, 0) random variable is
identically zero (Gelman et al., 1995).
■
1.4.2
The Dirichlet Distribution
The Dirichlet distribution is a multivariate generalization of the beta distri-
bution. For example, as discussed in Chapter 11, Example 11.7, the Dirich-
let is a natural model for the distribution of gene frequencies at a locus
with more than two alleles. The random vector
X = (X1, X2, . . . , Xk) ,
X1, X2, . . . , Xk ≥0, k
i=1 Xi = 1, follows the Dirichlet distribution of
dimension k, with parameters
α = (α1, α2, . . . , αk) ,
αj > 0,
if its probability density Dik (x|α) is
p (x|α) = Γ (α1 + α2 + · · · + αk)
Γ (α1) Γ (α2) . . . Γ (αk)
k
-
i=1
xαi−1
i
.
(1.76)
A simple and eﬃcient way to sample from (1.76) is ﬁrst to draw k inde-
pendent gamma random variables with scale parameter αi and unit scale:
Ga (yi|αi, 1). Then form the ratios
xi =
yi
k
j=1 yi
,
i = 1, . . . , k.
The vector (x1, x2, . . . , xk) is a realized value from (1.76).
1.4.3
The d-Dimensional Uniform Distribution
A d × 1 random vector x is uniformly distributed on [0, 1]d if
p (x) =

1,
if x ∈[0, 1]d ,
0,
otherwise.
From (1.56), each scalar element of x is distributed as [0, 1]. Often, mul-
tidimensional uniform distributions are assigned as prior distributions to
some of the parameters of a Bayesian model.

1.4 Multivariate Probability Distributions
41
1.4.4
The Multivariate Normal Distribution
In this subsection, the multivariate normal distribution is introduced, fol-
lowed by a presentation of the marginal and conditional distributions in-
duced by this process, of its moment generating function, of the distribution
of linear combinations of normal variates, and by a simple derivation of the
central limit theorem. The subsection concludes with examples that illus-
trate applications of the multivariate normal distribution in quantitative
genetics.
Density, Mean Vector, and Variance–Covariance Matrix
Let y denote a random vector of dimension n (the notational distinction
between a random variable and its realized value is omitted here). This
vector is said to have an n-dimensional multivariate normal distribution if
its p.d.f. is
p (y|m, V) = |2πV|−1/2 exp

−1
2 (y −m)′ V−1 (y −m)

(1.77)
where
m =E(y|m, V) =

yp (y|m, V) dy
(1.78)
is the mean vector, and
V
=
E[(y −m)(y −m)′] =

(y −m)(y −m)′p (y|m, V) dy
=

yy′p (y|m, V) dy −mm′
(1.79)
is the variance–covariance matrix of the distribution, assumed to be non-
singular. All integrals are n-dimensional, and taken over Rn, the entire
n-dimensional space. The notation dy is used for dy1dy2 . . . dyn. From the
density in (1.77) it follows that

exp

−1
2 (y −m)′ V−1 (y −m)

dy = |2πV|1/2 .
This is a generalization of Aitken’s integral seen in Example 1.10
Marginal and Conditional Distributions
Partition the vector of random variables as y = [y′
1, y′
2]′, with the corre-
sponding partitions of m and V being
m
=
[m′
1, m′
2]′
V
=

V11
V12
V21
V22

.

42
1. Probability and Random Variables
For any arbitrary partition, it can be shown that all marginal distributions
are normal. For example, the marginal density of y1 is
p (y1|m1, V11) =
∞

−∞
p (y1, y2|m, V) dy2
= (2π)−n1
2 |V11|
1
2 exp

−1
2 (y1−m′
1)′ V−1
11 (y1−m1)

,
(1.80)
where n1 is the order of y1. The conditional distribution
[y1|y2, m, V]
is normal as well, with density
p (y1|y2, m, V) = (2π)−n1
2 |V ar (y1|y2)|−1
2
× exp

−1
2 [y1 −E (y1|y2)]′ V ar−1 (y1|y2) [y1 −E (y1|y2)]
%
.
(1.81)
This holds for any partition of y, irrespective of whether the components
are scalars or vectors. The mean vector and covariance matrix of this con-
ditional distribution are
E (y1|y2, m, V) = m1 + V12V−1
22 (y2−m2)
(1.82)
and
V ar (y1|y2, m, V) = V11 −V12V−1
22 V21,
(1.83)
respectively. The variance–covariance matrix does not depend on y2, so this
conditional distribution is homoscedastic. This is an important feature of
the multivariate normal distribution. Another important fact to be noticed
is that marginal and conditional normality is arrived at assuming bivariate
normality as point of departure. However, marginal normality does not
imply joint normality.
A suﬃcient condition for independence in the multivariate normal dis-
tribution is that the variance–covariance matrix is diagonal. For example,
suppose that
V = Iσ2,
where σ2 is a positive scalar, and that E (yi) = m (i = 1, 2, ..., n). Then
(y −m)′ V−1 (y −m) = 1
σ2
n

i=1
(yi −m)2
and
|2πV|−1
2 =
""2πIσ2""−1
2 =

2πσ2−n
2 |I| =

2πσ2−n
2 .

1.4 Multivariate Probability Distributions
43
Using this in (1.77) yields
p (y|m, V) =
1
(2πσ2)
n
2 exp

−
n
i=1
(yi −m)2
2σ2

=
n
-
i=1
p

yi|m, σ2
which is the product of the densities of n i.i.d. normal variates, each with
mean m and variance σ2.
If the matrix V is singular, then y is said to have a singular or degenerate
normal distribution. If the rank of V is k < n, the singular density can be
written as
(2π)−k/2
(λ1λ2 . . . λk) 1/2 exp

−1
2 (y −m)′ V−(y −m)

,
where V−is a generalized inverse of V and λ1, λ2, . . . , λk are the nonzero
eigenvalues of V. Details can be found, for example, in Searle (1971), Rao
(1973), Anderson (1984), and Mardia et al. (1979).
Moment Generating Function
Let y ∼N (m, V) . The formula for the univariate case in (1.49) extends to
the multivariate normal situation, after similar algebra (Searle, 1971), to
M (t) = E [exp (t′y)] =
1
|2πV|1/2
×
∞

−∞
exp [t′y] exp

−1
2 (y −m)′ V−1 (y −m)

dy
= exp

t′m+t′Vt
2

,
(1.84)
where t is a dummy vector of order n. Diﬀerentiation of the moment gen-
erating function with respect to t yields
∂M (t)
∂t
= exp

t′m+t′Vt
2

(m + Vt)
and putting t = 0 gives directly E (y) = m. An additional diﬀerentiation
gives
∂2M (t)
∂t ∂t′
= exp

t′m + t′Vt
2
 
V+ (m + Vt) (m + Vt)′
which, for t = 0, leads to E (yy′) = V + mm′. The covariance matrix is
then E (yy′) −E (y) E (y′) = V.

44
1. Probability and Random Variables
Linear Functions of Normally Distributed Random Variables
Another important property of the multivariate Gaussian distribution is
that a linear transformation of a normally distributed vector is normal
as well. Let x = α + β′y be a random variable resulting from a linear
combination of the multivariate normal vector y, where α is a known scalar
and β is a vector, also known. The moment generating function of x is then
E

exp

α + β′y

t

= exp (αt) E

exp

tβ′y

= exp (αt) M (t∗) ,
where t∗= tβ. Making use of (1.84) in the preceding expression gives
E

exp

α + β′y

t

=
exp (αt) exp

t∗′m + t∗′Vt∗
2

=
exp

t

α + β′m

+ t2 
β′Vβ

2

.
This is the moment generating function of a normal random variable with
mean α + β′m and variance β′Vβ. The property is important in quanti-
tative genetics under additive inheritance: here, the additive genetic value
of an oﬀspring is equal to the average value of the parents, plus a residual.
If all these terms follow a multivariate normal distribution, it follows that
the additive value in the progeny is normally distributed as well.
Central Limit Theorem
As stated, the normal distribution has played a central role in statistics
and quantitative genetics. An example is the so-called inﬁnitesimal model
(Fisher, 1918; Bulmer, 1971). Here, y in (1.77) represents a vector of ad-
ditive genetic values and V is a function of additive genetic relationships
between subjects, or of twice the coeﬃcients of coancestry (Mal´ecot, 1969).
This matrix of coancestries enters when quantifying the process of genetic
drift, in estimation of genetic variance–covariance components, and in in-
ferences about additive genetic values and functions thereof in animal and
plant breeding.
The inﬁnitesimal model posits that the additive genetic value for a quan-
titative trait is the result of the sum of values at each of an inﬁnite number
loci. If the population in question is in joint equilibrium at all loci as a
result of random mating without selection over many generations, the con-
tributions from the diﬀerent loci will be statistically independent from each
other. This will be true even under the presence of linkage, since linkage
slows down the approach to equilibrium but does not change the equilib-
rium ultimately attained. The celebrated central limit theorem leads to the
result that the additive genetic value follows a normal distribution, approx-
imately, irrespective of the distribution of eﬀects at individual loci. Here,
borrowing from Bulmer (1979), it is shown that this is so.

1.4 Multivariate Probability Distributions
45
Let Y = n
i=1 Yi be the additive genetic value of an individual, and let
Yi be the value at locus i. The mean and variance at locus i are µi and σ2
i ,
respectively, so that E (Y ) = n
i=1 µi = µ and V ar (Y ) = n
i=1 σ2
i = σ2,
say, as the eﬀects at diﬀerent loci are mutually independent of each other.
Consider the moment generating function of the standardized variate Z =
(Y −µ) /σ:
MZ (t) = E [exp (tZ)] = E
#
exp
 n

i=1
tZi
$
,
where Zi = (Yi −µi) /σ and 
i Zi = Z. Further,
MZ (t) =

exp
 n

i=1
tZi

p (z1, z2, ..., zn) dz
=
n
-
i=1
E

exp
t (Yi −µi)
σ
%
=
n
-
i=1
Mi

 t
σ

,
(1.85)
where Mi (t) is the moment generating function of (Yi −µi) . The preceding
follows because the n random variables Yi are mutually independent. Using
(1.48), one can write
Mi

 t
σ

= E
t (Yi −µi)
σ

= 1 + tE (Yi −µi)
σ
+ t2E (Yi −µi)2
2σ2
+ · · ·
· · · + tkE (Yi −µi)k
k!σk
+ · · ·
≈1 + t2σ2
i
2σ2 + · · · .
(1.86)
This is so, because for large n, third- and higher-order moments (from the
mean) for small individual loci eﬀects are small, relative to σ3, σ4, etc., so
the corresponding terms can be ignored. Then, employing (1.86) in (1.85)
gives
log [MZ (t)] =
n

i=1
log

Mi

 t
σ

≈
n

i=1
log

1 + t2σ2
i
2σ2

.
(1.87)
Now consider an expansion about 0 of the function
log (1 + x) = log (1) + x −x2 + 2x3 + · · · ≈x.
The approximation results from the fact that for values of x near 0, higher-
order terms can be neglected. Using this in (1.87) gives
MZ (t) ≈exp
n

i=1
t2σ2
i
2σ2 = exp

t2
2

(1.88)

46
1. Probability and Random Variables
which is the moment generating function of a normally distributed variable
with mean 0 and variance 1; this can be veriﬁed by inspection of (1.50).
Thus, approximately, Z ∼N (0, 1) and, since Y = µ + Zσ is a linear
combination of Z, it follows that Y ∼

µ, σ2
is approximately normal as
well. (A little more formally, it is the c.d.f. of Z that converges to the c.d.f.
of the N (0, 1), as n →∞. This is known as convergence in distribution).
As pointed out by Bulmer (1979), the central limit theorem explains
why many observed distributions “look” normal. To the extent that ran-
dom variation is the result of a large number of independent factors acting
additively, each making a small contribution to the total variation, the
resulting distribution should be close to a normal one.
There are extensions of the central limit theorem for variables that are
independent but not identically distributed, and for dependent random
variables. The latter is particularly relevant for the study of time series and
Markov processes. A good starting point is the book of Lehmann (1999).
Example 1.15
A tetravariate normal distribution
Consider two genetically related individuals and suppose that a measure-
ment, e.g., stature, is taken on each of them. Assume that the inﬁnitesi-
mal additive genetic model described above operates. The expected genetic
value of a child, given the genetic values of the parents, is equal to the av-
erage of the genetic values of the father and mother. Also, assume that
the population from which the two individuals are drawn randomly, is ho-
mogeneous in every possible respect. Let the model for the measurements
be (we relax the distinction between random variables and their realized
values unless the setting requires maintaining it)
y1
=
a1 + e1,
y2
=
a2 + e2.
Further, suppose that E (ai) = E (ei) = 0 (i = 1, 2), so that E (yi) = 0,
and that Cov (ai, ei) = 0, implying that V ar (yi) = σ2
a + σ2
e, where σ2
a
is the additive genetic variance and σ2
e is the residual or environmental
variance. Also, let Cov (e1, e2) = 0; thus, Cov (y1, y2) = Cov (a1, a2) =
r12σ2
a where r12 is the additive genetic relationship between individuals 1
and 2. The additive relationship is equal to twice the probability that a
randomly chosen allele drawn from a locus from individual 1 is identical
by descent (i.e., it is a biochemical copy from a common ancestor) to a
randomly chosen allele taken from the same locus of individual 2 (Mal´ecot,
1969). For example, the average additive genetic relationship between two
full-sibs is equal to 1/2, whereas that between an individual and itself, in
the absence of inbreeding, is equal to 1. Under multivariate normality, if
pairs of such individuals are drawn at random from the population, the
joint distribution of measurements and of additive genetic values can be

1.4 Multivariate Probability Distributions
47
represented as


y1
y2
a1
a2

∼N






0
0
0
0

,


σ2
a + σ2
e
r12σ2
a
σ2
a
r12σ2
a
r12σ2
a
σ2
a + σ2
e
r12σ2
a
σ2
a
σ2
a
r12σ2
a
σ2
a
r12σ2
a
r12σ2
a
σ2
a
r12σ2
a
σ2
a





.
In what follows, use is made of results (1.77) through (1.83). First, the
mean and variance of the conditional distribution [y1|a1] is derived, with
the dependence on the parameters suppressed in the notation. Because of
the assumption of joint normality, this distribution must be normal as well.
The mean and variance are given by
E (y1|a1)
=
0 + σ2
a

σ2
a
−1 (a1 −0) = a1,
V ar (y1|a1)
=
σ2
a + σ2
e −σ2
a

σ2
a
−1 σ2
a = σ2
e,
and these are calculated with (1.82) and (1.83), respectively. The expected
value and variance of this conditional distribution can also be deduced
directly from the model, and also hold in the absence of normality, provided
that the covariance between the additive genetic value and the residual
deviation of the same individual is null. This can be veriﬁed simply by
ﬁxing the additive genetic value, and then taking the expectation and the
variance under this assumption.
Consider now the conditional distribution [y1|a1, a2]. Intuitively, it is clear
that this must be the same as [y1|a1] because, given the additive genetic
value of individual 1, knowledge of the genetic value of individual 2 should
not provide any additional information about the stature of individual 1.
Having ﬁxed a1, the only remaining term in the model is the residual e1,
and this is independent of a2. Anyhow, the distribution [y1|a1, a2] must be
normal, and application of (1.82) and (1.83) yields
E (y1|a1, a2) =

σ2
a
r12σ2
a
 
σ2
a
r12σ2
a
r12σ2
a
σ2
a
−1 
a1
a2

= a1
and
V ar (y1|a1, a2) = σ2
a + σ2
e
−
 σ2
a
r12σ2
a
 
σ2
a
r12σ2
a
r12σ2
a
σ2
a
−1 
σ2
a
rσ2
a

= σ2
e.
Because the mean and variance are suﬃcient to identify fully the desired
normal distribution, it follows that [y1|a1, a2] = [y1|a1] , as expected. Fur-
ther, given the genotypic values, the observations are conditionally inde-
pendent. Thus, the joint density of the two measurements, given the two
genetic values, can be written as
p (y1, y2|a1, a2)
=
p (y1|a1, a2) p (y2|a1, a2)
=
p (y1|a1) p (y2|a2) .

48
1. Probability and Random Variables
Exploiting situations of conditional independence is a key issue in the
Bayesian analysis of hierarchical models. This is discussed in Chapter 6.
We turn attention now to the distribution [y1|a2]. Again, this conditional
probability distribution is normal, with mean
E (y1|a2) = 0 + r12σ2
a

σ2
a
−1 a2 = r12a2
and variance:
V ar (y1|a2)
=
σ2
a + σ2
e −r12σ2
a

σ2
a
−1 r12σ2
a
=
σ2
a

1 −r2
12

+ σ2
e.
The variance of this distribution is smaller than that of the marginal dis-
tribution of y1, but is larger than the variance of [y1|a1] ; this is because r12
is larger than 0, although it cannot exceed 1. Conversely, the distribution
[a2|y1] has mean and variance
E (a2|y1) =
r12σ2
a
σ2a + σ2e
y1,
V ar (a2|y1) = σ2
a

1 −r2
12
σ2
a
σ2a + σ2e

= σ2
a

1 −r2
12h2
,
where h2 = σ2
a
4 
σ2
a + σ2
e

is called “heritability” in quantitative genetics.
This parameter measures the fraction by which measurable diﬀerences be-
tween parents for a given trait are expected to be recovered in the next
generation, under the supposition of additive Mendelian inheritance. The
preceding mean and variance would be the parameters of the normal dis-
tribution used to infer the unobservable genetic value of individual 2 using
a measurement (y1) on related individual 1. Letting r12 = 1, one obtains
the formulas needed to make inferences about the genetic value of animal
1 using his measurement or phenotypic value (remember that in genetics,
“phenotype” means whatever can be observed in an individual, e.g., a blood
group, or the testosterone level of a bull at a speciﬁed age).
Another conditional distribution of interest might be [y1|y2]. This pro-
cess can be useful in a situation where, for example, one wishes to make
probability statements about the phenotype of individual 1 based on a mea-
surement obtained on relative 2. This distribution is normal with mean
E (y1|y2)
=
0 + r12σ2
a

σ2
a + σ2
e
−1 y2
=
r12h2y2
and variance
V ar (y1|y2)
=
σ2
a + σ2
e −r12σ2
a

σ2
a + σ2
e
−1 r12σ2
a
=

σ2
a + σ2
e
 
1 −r2
12h4
.

1.4 Multivariate Probability Distributions
49
It is seen that even for individuals that are as closely related as full-
sibs (r12 = 1/2), and for heritability values as large as 1/2, knowledge
of y2 produces a reduction in variance of only 1/16, so not much knowledge
about y1 is gained in this situation.
■
Example 1.16
Decomposing the joint distribution of additive genetic
values
Consider the following genealogy (also called pedigree in animal breeding):
Individual
Father
Mother
1
−
−
2
−
−
3
1
2
4
1
2
5
1
3
6
4
3
7
5
6
Let ai (i = 1, 2, . . . , 7) be the additive genetic value of individual i. Assume
a model of gene transmission that allows writing the regression of the addi-
tive genetic value of a child (ao) on the additive genetic values of its father
(af) and of its mother (am) as
ao = 1
2af + 1
2am + eo,
where eo, often known as the Mendelian sampling term, is distributed in-
dependently of similar terms in any other ancestors. The joint distribution
of the additive genetic values in the pedigree can be factorized as follows:
p (a1, a2, . . . , a7) = p (a7|a1, a2 . . . , a6) p (a1, a2 . . . , a6)
= p (a7|a5, a6) p (a1, a2 . . . , a6) .
The equality in the second line follows, because under the Mendelian inher-
itance model, given the additive genetic values of its parents, the additive
genetic value a7 is conditionally independent of the additive genetic values
of non-descendants of 7. Similarly p (a1, a2 . . . , a6) can be factorized as
p (a1, a2 . . . , a6) = p (a6|a1, a2, . . . , a5) p (a1, a2, . . . , a5)
= p (a6|a3, a4) p (a1, a2, . . . , a5) .
Continuing in this way with the remaining terms, we obtain ﬁnally
p (a1, a2, . . . , a7) = p (a7|a5, a6) p (a6|a3, a4) p (a5|a1, a3)
p (a4|a1, a2) p (a3|a1, a2) p (a1) p (a2) .
This is an important decomposition that will be encountered again in (16.1)
from Chapter 16 in the context of segregation analysis, where genetic values

50
1. Probability and Random Variables
are modeled as discrete random variables. Incidentally, note that oﬀspring
are conditionally independent, given their parents, and therefore
p (a5, a6|a1, a3, a4) = p (a5|a1, a3) p (a6|a3, a4) .
However, given a1, a3, a4, and a7 (the child of 5 and 6), then a5 and a6 are
no longer conditionally independent; they are correlated negatively.
■
Example 1.17
A multivariate normal sampling model
Imagine there is a data set consisting of two measurements (traits) taken
on each of n subjects. For each trait, a model having the following form is
adopted:
yj = Xjβj + Zjaj + ej,
j = 1, 2,
(1.89)
where βj and aj are, formally, location vectors containing the eﬀects of
factors aﬀecting variation of the responses, and Xj and Zj are known in-
cidence matrices relating these parameters to the data vectors yj, each of
order n. The term ej is a residual representing random variation about
Xjβj+Zjaj. For example, (1.89) could represent a mixed eﬀects model for
quantitative genetic analysis (Henderson, 1973) in which case β1 (β2) and
a1 (a2) would be ﬁxed and random eﬀects, respectively, on trait 1 (trait 2),
respectively. The records on traits 1 and 2 for individual i can be put in a
two-dimensional vector y∗
i = [yi1, yi2]′ , (i = 1, 2, ..., n). These n vectors are
assumed to be conditionally independent, given the location vectors, and
assumed to follow a bivariate normal distribution. The joint density of all
data y∗= [y∗′
1 , y∗′
2 , ..., y∗′
n ]′ is then expressible as
p (y∗|β, a, R0)
∝|R0|−n
2 exp

−1
2
n

i=1
(yi1 −mi1, yi2 −mi2) R−1
0

yi1 −mi1
yi2 −mi2

,
(1.90)
where mi1 = x′
i1β1 + z′
i1a1, mi2 = x′
i2β2 + z′
i2a2, and x′
i1, x′
i2, z′
i1, z′
i2 are
the ith rows of the incidence matrices X1, X2, Z1, Z2, respectively. Here
the dispersion structure will be taken to be
V ar

yi1|β1, a1
yi2|β2, a2

=
 r11
r12
r21
r22

= R0 ∀i,
(1.91)
so R0 = {rij} is the variance–covariance matrix between the two traits
measured on the same individual. Let
Se =


n
i=1
(yi1 −mi1)2
n
i=1
(yi1 −mi1) (yi2 −mi2)
n
i=1
(yi1 −mi1) (yi2 −mi2)
n
i=1
(yi2 −mi2)2

(1.92)

1.4 Multivariate Probability Distributions
51
be a matrix of sums of squares and cross-products of residuals. Then
n

i=1
[yi1 −mi1, yi2 −mi2] R−1
0

yi1 −mi1
yi2 −mi2

=
tr
# n

i=1
[yi1 −mi1, yi2 −mi2] R−1
0

yi1 −mi1
yi2 −mi2
$
=
tr

R−1
0 Se

,
where tr (·) means “trace” (sum of diagonal elements) of a matrix (Searle,
1982). Matrices can be commuted cyclically (preserving comformability)
under the tr operator. With this notation, the joint probability density of
all data is
p (y∗|β, a, R0) = (2π)−n
2 |R0|−n
2 exp

−1
2 tr

R−1
0 Se

,
(1.93)
which is a useful representation in multivariate analysis (Anderson, 1984).■
Example 1.18
Computing conditional multivariate normal distributions
Conditional distributions are important in prediction of genetic values us-
ing mixed eﬀects models (Henderson, 1963, 1973; Searle et al., 1992). These
distributions also play a key role in a Gibbs sampling-based Bayesian anal-
ysis. The algorithm will be discussed in detail later, and at this point it
suﬃces to state that its implementation requires constructing all possible
conditional distributions from a joint distribution of interest. The example
here will illustrate how some conditional distributions that arise in connec-
tion with a mixed eﬀects linear model can be computed under Gaussian
assumptions.
Consider the linear model
y = Xβ + Za + e,
(1.94)
where the random vectors a and e have a joint multivariate normal distri-
bution with null mean vector and covariance matrix
V ar

a
e

=

Aσ2
a
0
0
Iσ2
e

.
Above, X and Z are known incidence matrices, A is a known matrix (e.g.,
of additive relationships between individuals), and σ2
a and σ2
e are variance
components. Then E (y|β) = Xβ and V ar

y|β,σ2
a, σ2
e

= ZAZ′σ2
a + Iσ2
e.
Let
θ′
=

β′, a′′ ,
W = [X, Z] ,
Σ
=

0
0
0
A−1k

and C = W′W + Σ,

52
1. Probability and Random Variables
where k = σ2
e/σ2
a. The linear set of equations
C 5θ = W′y = r
has solution 5θ = C−1r, assuming the inverse of the coeﬃcient matrix C
exists. This system is called Henderson’s mixed model equations. It will
be shown later on, that in a certain Bayesian setting (Lindley and Smith,
1972; Gianola and Fernando, 1986) the posterior distribution of θ when σ2
a
and σ2
e are known, is multivariate normal with parameters
θ|σ2
a, σ2
e, y ∼N

5θ, C−1σ2
e

.
(1.95)
Now, partition θ =

θ′
1, θ′
2
′ arbitrarily, so θ1 can be a scalar or a vector.
What then is the distribution

θ1|θ2, σ2
a, σ2
e, y

? From multivariate nor-
mal theory, because the posterior distribution

θ|σ2
a, σ2
e, y

is normal with
parameters (1.95), it follows that the desired conditional posterior distribu-
tion must be normal as well. A useful way of arriving at the parameters of
this conditional distribution is presented here. Given the above partition,
one can write the joint posterior distribution of θ as

θ1
θ2
"""" σ2
a, σ2
e, y

∼N

5θ1
5θ2

,

C11
C12
C21
C22

σ2
e

,
(1.96)
where

C11
C12
C21
C22
−1
=

C11
C12
C21
C22

= C−1.
Now deﬁne r′ = [r′
1, r′
2], such that the partition is consistent with that of
θ. Using (1.82), the expected value of the distribution

θ1|θ2, σ2
a, σ2
e, y

is
E

θ1|θ2, σ2
a, σ2
e, y

= 5θ1 + C12 
C22−1 
θ2 −5θ2

.
(1.97)
From the mixed model equations, after inverting C, one has
5θ1
=
C11r1 + C12r2,
5θ2
=
C21r1 + C22r2.
Employing these expressions for 5θ1 and 5θ2 in (1.97) above, we get:
E

θ1|θ2, σ2
a, σ2
e, y

= C11r1 + C12r2 + C12 
C22−1
×

θ2 −C21r1−C22r2

=

C11 −C12 
C22−1 C21
r1
+C12 
C22−1 θ2
= C−1
11

r1 + C11C12 
C22−1 θ2

= C−1
11 (r1 −C12θ2) .
(1.98)

1.4 Multivariate Probability Distributions
53
In the above derivation, use is made of the standard matrix algebra result
for partitioned inverses (e.g., Searle, 1982):
C11 −C12 
C22−1 C21 = C−1
11
and
C11C12 
C22−1 = −C12.
The variance of the conditional posterior distribution

θ1|θ2, σ2
a, σ2
e, y

is
V ar

θ1|θ2, σ2
a, σ2
e, y

=

C11 −C12 
C22−1 C21
σ2
e
=
C−1
11 σ2
e.
(1.99)
Therefore, one can write
θ1|θ2, σ2
a, σ2
e, y ∼N

C−1
11 (r1−C12θ2) , C−1
11 σ2
e

.
(1.100)
Results (1.98) and (1.99) are useful in the implementation of a Gibbs sam-
pler in a hierarchical or mixed eﬀects linear model. Even if θ has a large
number of elements and C is, therefore, a very large matrix (diﬃcult to
invert by brute force methods), the mean and variance of

θ1|θ2, σ2
a, σ2
e, y

involve the inverse of a matrix having order equal to the number of elements
in θ1. For example, if θ1 is a scalar, only the reciprocal of the appropriate
scalar element is needed.
■
1.4.5
Quadratic Forms on Normal Variables:
the Chi-square Distribution
Let the random vector y have the distribution
y ∼N (m, V)
and consider the random variable y′Qy. This is called a quadratic form on
vector y; the matrix of constants Q can be taken to be symmetric, without
loss of generality. Then, provided that QV is idempotent one has that
y′Qy ∼χ2

rank (Q) , 1
2m′Qm

.
(1.101)
(Matrix A is idempotent if AA = A. If A is idempotent, rank (A) = tr A).
Expression (1.101) means that the quadratic form y′Qy has a noncentral
chi-square distribution with integer degrees of freedom equal to rank(Q) ,
and where 1
2m′Qm is the noncentrality parameter (see Searle, 1971). Other
authors (e.g., Stuart and Ord, 1987) deﬁne the noncentrality parameter as
m′Qm. If the non-centrality parameter is null, with a suﬃcient condition
for this being m = 0, then y′Qy is said to have a central chi-square distri-
bution.

54
1. Probability and Random Variables
The mean value of the distribution (1.101) is
E (y′Qy) = m′Qm + tr (QV)
(1.102)
and the variance can be shown to be equal to
V ar (y′Qy) = 4m′QVQm + 2tr (QV)2 .
(1.103)
In the special case when m = 0, then E (y′Qy) = tr (QV) and
V ar (y′Qy) = 2 tr (QV)2 = 2 tr (QV) .
Typically,
rank (Q) < rank (V) ,
so
tr (QV) = rank (QV) = rank (Q) ,
this resulting from the idempotency of QV. Then, for m = 0,
E (y′Qy) = rank (Q)
and
V ar (y′Qy) = 2 rank (Q) ,
so the mean and variance of the distribution are given by the number of
degrees of freedom, and by twice the degrees of freedom, respectively.
Example 1.19
Distribution of estimates of the variance of a normal
distribution
In Chapter 3, it will be shown that for the sampling model
y ∼N

Xβ, Iσ2
e

,
the maximum likelihood (ML) estimator of the variance σ2
e is given by
6
σ2e
=

y −X5β
′ 
y −X5β

n
=
y′Qy
n
,
where 5β = (X′X)−1 X′y is the ordinary least-squares estimator of β,
Q =[I −X (X′X)−1 X′] is an idempotent n × n matrix, and n is the order
of y. Here it is assumed that the matrix X has full column rank equal to
p, the order of β. Now
6
σ2e
=
σ2
e
n
y′Qy
σ2e
= σ2
e
n y′Q

 I
σ2e

Qy
=
σ2
e
n y∗′

 I
σ2e

y∗.

1.4 Multivariate Probability Distributions
55
Hence, 6
σ2e is a multiple of the quadratic form y∗′ 
Iσ−2
e

y∗, where y∗= Qy.
It will be veriﬁed that this new quadratic form has a chi-square distribution.
First note that y∗is normal by virtue of being a linear combination of y,
with
E (y∗) = QE (y) = QXβ = Xβ −X (X′X)−1 X′Xβ = 0
and
V ar (y∗) = Qσ2
e.
Further

 I
σ2e

V ar (y∗) =

 I
σ2e

Qσ2
e = Q
is idempotent. Hence,
y∗′

 I
σ2e

y∗= y′Qy
σ2e
has a central chi-square distribution (since E (y∗) = 0) with degrees of
freedom equal to
rank

 Q
σ2e

=
rank (Q) = tr (Q) = tr

I −X (X′X)−1 X′
=
n −tr

X (X′X)−1 X′
=

n −tr (X′X)−1 X′X

= n −p.
It follows that
6
σ2e
=
σ2
e
n y∗′

 I
σ2e

y∗
=
σ2
e
n χ2
n−p,
so the ML estimator of σ2
e is distributed as a scaled chi-square random
variable.
■
1.4.6
The Wishart and Inverse Wishart Distributions
Multivariate analysis is important in quantitative and evolutionary genet-
ics. For example, in plant and animal breeding, selection for multiple at-
tributes, e.g., yield and quality of wheat or growth rate and feed eﬃciency
in beef cattle, is the rule rather than the exception. This requires knowl-
edge of genetic variances and covariances between traits. Correlation and
covariance matrices are also of interest in evolution because, for example,
a genetic variance–covariance matrix contains information about possible

56
1. Probability and Random Variables
bottlenecks under natural selection (Lynch and Walsh, 1998), or about
the evolutionary trajectory of several traits (Roﬀ, 1997). The Wishart and
inverse Wishart distributions appear in connection with inference about
covariance matrices involving attributes that follow a multivariate normal
distribution, and play an important role in multivariate analysis (Mardia
et al., 1979; Anderson, 1984). Deriving these distributions is technically
involved, so only a few features are sketched here.
Suppose there is data on p traits or attributes, each expressed as a de-
viation from their respective expectations, on each of n individuals. The
p-dimensional vector of traits is assumed to follow the p-variate Np (0, Σ)
distribution, where Σ is a positive-deﬁnite variance–covariance matrix be-
tween attributes. Let Y(n×p) be a data matrix containing in row i, say, the
p measurements taken on individual i. Often Y(n×p) can be assumed to
represent a collection of n independent draws from this normal sampling
model. Form now M = Y′Y, a random p × p matrix of sums of squares
and crossproducts. Given the normality assumption, the symmetric matrix
M is said to have a Wishart distribution of order p with scale matrix Σ
and degrees of freedom parameter equal to n. We write M ∼Wp (Σ, n)
to denote this random process. The Wishart distribution is a matrix gen-
eralization of the chi-squared distribution, as we shall see later, and it
involves p (p + 1) /2 random quantities: the p distinct sums of squares and
the p (p −1) /2 sums of products.
The p.d.f. of the Wishart distribution is
p (M|Σ, n) =
|M|(n−p−1)/2 exp

−1
2 tr

Σ−1M

2np/2πp(p−1)/4 |Σ|n/2
p7
i=1
Γ
 1
2 (n + 1 −i)

∝|M|
(n−p−1)/2
exp

−1
2 tr

Σ−1M

(1.104)
with the sample space being such that |M| > 0. A special situation is when
p = 1, in which case M and Σ are scalars; here M = n
i=1 Y 2
i is a sum
of squares, and Σ = σ2 is the variance of the normal distribution. In this
situation, the Wishart density reduces to the univariate p.d.f.
p

M|σ2, n

∝M (n−2)/2 exp

−M
2σ2

.
(1.105)
This is the kernel of a gamma density with parameters n/2 and (2σ2)−1. It
is also the density of the distribution of σ2χ2
(n), a scaled chi-square random
variable, with scale parameter σ2 and n degrees of freedom. Return to the
general case, and put M = {Mij} and Σ = {σij}. It can be shown that the
expected (matrix) value of the random matrix M with p.d.f. given in (1.104)
is E (M|Σ,n) = nΣ so, for any element of the matrix E (Mij) = nσij.

1.4 Multivariate Probability Distributions
57
The Inverse Wishart Distribution
A related distribution is that of the inverse of a matrix of sums of squares
and products of n randomly drawn vectors from Np (0, Σ). The distribu-
tion of T = M−1, for |Σ| > 0 and n ≥p, is called the inverse Wishart
distribution; it is symbolized as T ∼W −1
p
(Σ, n) or as T ∼IWp (Σ, n).
The density of an inverse Wishart matrix is
p (T|Σ, n)
=
|T|−(n+p+1)/2 exp

−1
2 tr

Σ−1T−1
2np/2πp(p−1)/4 |Σ|n/2
p7
i=1
Γ
 1
2 (n + 1 −i)

∝
|T|
−(n+p+1)/2
exp

−1
2 tr

Σ−1T−1
.
(1.106)
The expected value of T is
E (T|Σ, n) =
Σ−1
(n −p −1),
(1.107)
provided n ≥p + 2. An important special case is when T is a scalar (p =
1). For example, in a Bayesian context one may consider using the scalar
version of (1.106) to describe uncertainty about the variance σ2. Here,
T = σ2, and Σ−1 = S is now the scale parameter. Then (1.106) reduces to:
p

σ2|S, n

∝

σ2−( n
2 +1) exp

−S
2σ2

.
(1.108)
This is the kernel of the density of a scaled inverted chi-square distribution
with parameters S and n (also, of an inverted gamma process with param-
eters n/2 and S/2). An alternative representation is that of, e.g., Gelman
et al. (1995), who put S = nS∗. In a sense, S can be interpreted, Bayesianly
speaking, as a prior sum of squares, whereas S∗would play the role of a
value of σ2 that, a priori, is viewed as very likely (actually, the most likely
one when n is very large). In a Bayesian setting, n can be interpreted as a
“degree of belief” in S∗.
More generally, if the random matrix is now T = Σ, the variance–cova-
riance matrix of a normal distribution, then (1.106) will describe uncer-
tainty about it. The scale parameter would be interpretable as a matrix of
“prior sums of squares and products”. If one has a prior opinion about the
value of an unknown variance–covariance matrix Σ, and this is Σ∗, say, the
value of the scale matrix (which we denote now as S−1 to avoid confusion
with the covariance matrix that one wishes to infer, this being Σ) can be
assessed from (1.107) as S−1 = (n −p −1) Σ∗, given a value of n.
Properties of Wishart and Inverse Wishart Distributions
Some properties of Wishart and inverse Wishart distributions are summa-
rized below. The material is taken from Korsgaard et al. (1999), where

58
1. Probability and Random Variables
the results presented here are given in a more general setting. Let M be
a 2 × 2 symmetric, positive-deﬁnite random matrix having the Wishart
distribution
M|V, n ∼W2 (V,n) ,
where V is the scale matrix. Let T = M−1 so that
T|V, n ∼IW2 (V, n) .
The symmetric matrices M, V, T have elements
M =

M11
M12
M21
M22

,
V =

V11
V12
V21
V22

,
and
T =

T11
T12
T21
T22

,
respectively. Using properties of partitioned matrices one can write
M11
=

T11 −T 2
12T −1
22
−1 ,
M12
=
−T12
4
T11T22 −T 2
12

,
M22
=

T22 −T 2
12T −1
11
−1 .
Deﬁne
X1
=
M11,
X2
=
M −1
11 M12,
X3
=
M22 −M 2
12M −1
11 .
From these equalities it can be deduced that the following one-to-one rela-
tionships exist between the Xs and the Ts:
T11
=
X−1
1
+ X2
2X−1
3 ,
(1.109)
T12
=
−X2X−1
3 ,
(1.110)
T22
=
X−1
3 .
(1.111)
Then the following properties can be shown to hold:
X1 ∼W1 (V11, n) ,
(1.112)
(X2|X1 = x1) ∼N

V −1
11 V12, x−1
1 V22.1

,
(1.113)
X3 ∼W1 (V22.1, n −1) ,
(1.114)
p (x1, x2|x3) = p (x1, x2) ,
(1.115)

1.4 Multivariate Probability Distributions
59
where V22.1 = V22 −V 2
12V −1
11 . This means that X1 and X3 have univariate
Wishart (or gamma) distributions with appropriate parameters, that the
conditional distribution [X2|X1 = x1] is normal, and the joint distribution
[X1, X2] is independent of X3. All these distributions are easy to sample
from. Thus, if one wishes to draw a random matrix from an W2 (V, n)
distribution, the sampling procedure consists of:
(a) draw x from the three preceding distributions;
(b) compute T∗from x. This is a realized value from T|V, n ∼IW2 (V, n);
(c) ﬁnally, invert T∗to obtain a draw from W2 (V, n).
In a joint analysis of Gaussian and discrete data (the latter employing
a probit model) with Bayesian methods, the following problem is often
encountered. Draws of 2 × 2 covariance matrices are to be obtained from
a certain posterior distribution, subject to the restriction that one of the
variances is equal to 1 (this is the residual variance in the probit scale).
In Chapter 14 we show how to exploit the properties described above, in
order to perform a Markov chain Monte Carlo (MCMC) based Bayesian
analysis of Gaussian and binary distributed traits.
Simulation of an Inverse Wishart Distribution
An eﬃcient way of simulating a p-dimensional Wishart distribution with
n degrees of freedom and scale matrix S, Wp (S, n), is described by Odell
and Feiveson (1966). The algorithm is as follows:
• Compute the Cholesky factorization of S = L′L, such that L′ is lower
triangular.
• Construct a lower triangular matrix
T = {tij} , i = 1, 2, . . . , p, j = 1, 2, . . . , p,
with tii =
8
χ2
n+1−i, tij ∼N (0, 1), if i > j, and tij = 0 if i < j.
• Compute the product L′TT′L. This matrix is distributed as Wp (S, n).
• The matrix

L′TT′L
−1 is distributed as IWp (S, n).
As discussed later in this book, a Bayesian analysis requires posing a prior
distribution for all the parameters of the model. If a covariance matrix C
of dimension p×p, say, is assigned a priori the distribution IWp (V, n) , one
way of choosing the scale matrix V is from consideration of the expected
value of C:
E (C|V, n) =
V−1
n −p −1.
Then set V−1 = (n −p −1) ,E (C|V, n), where ,E (C|V, n) is some “rea-
sonable” value chosen on the basis of prior information.

60
1. Probability and Random Variables
1.4.7
The Multivariate-t Distribution
Density of the Distribution
Suppose a random vector has the conditional multivariate normal distribu-
tion with p.d.f.
y|µ, Σ, w ∼N

y|µ, Σ
w

,
(1.116)
where w, in turn, is a scalar random variable following a
Ga
ν
2, ν
2

process; here ν > 0 is a parameter. The density of the joint distribution of
y and w, using (1.40) and (1.77), is then
p (y, w|µ, Σ, ν) = p (y|µ, Σ, w) p (w|ν)
=
""""2π

Σ
w
""""
−1
2
exp

−1
2 (y −µ)′

Σ
w
−1
(y −µ)

× (ν/2)
ν
2
Γ (ν/2)w
ν
2 −1 exp

−νw
2

.
(1.117)
The marginal density of y is obtained by integrating the joint density above
with respect to w, yielding
p (y|µ, Σ, ν) = |2πΣ|−1
2 (ν/2)
ν
2
Γ (ν/2)
×
∞

0
w
n+ν
2
−1 exp

−w(y −µ)′ Σ−1 (y −µ) + ν
2

dw.
(1.118)
Reference to (1.40) indicates that the integrand is the kernel of the density
Ga

w|n + ν
2
, (y −µ)′ Σ−1 (y −µ) + ν
2

.
Hence, the integral in (1.118) is equal to the reciprocal of the integration
constant of the corresponding distribution, that is
∞

0
w
n+ν
2
−1 exp

−w(y −µ)′ Σ−1 (y −µ) + ν
2

dw
=
Γ
 n+ν
2


(y−µ)′Σ−1(y−µ)+ν
2
 n+ν
2 .
(1.119)

1.4 Multivariate Probability Distributions
61
Employing (1.119) in (1.118), and rearranging
p (y|µ, Σ, ν) = (ν)
ν
2 Γ
 n+ν
2

Γ
 ν
2

|πΣ|
1
2

(y −µ)′ Σ−1 (y −µ) + ν
−n+ν
2
=
Γ
 n+ν
2

Γ
 ν
2

|νπΣ|
1
2

1 + (y −µ)′ Σ−1 (y −µ)
ν
−n+ν
2
.
(1.120)
This is the density of an n-dimensional multivariate-t distribution with
mean vector µ, scale matrix Σ, and degrees of freedom parameter ν. Note
that ν can take any value in the positive part of the real line and does not
need to be an integer. When n = 1, a univariate-t distribution results, and
the density has already been given in (1.51).
It is interesting to observe that the t distribution results by averaging
an inﬁnite number of normal processes with a randomly varying covari-
ance matrix Σw−1 over a gamma (or inverse gamma, or scaled inverse
chi-square) distribution. For this reason, it is often stated in the litera-
ture that the t distribution is a mixture of an inﬁnite number of Gaussian
processes (Gelman et al., 1995).
The mean vector and covariance matrix of the multivariate-t distribution
can be deduced from (1.116) using iterated expectations (see below), as this
leads to
E (y|µ, Σ, ν) = Ew (y|µ, Σ, w) = Ew (µ) = µ
and to
V ar (y|µ, Σ, ν) = Ew [V ar (y|µ, Σ, w)] + V arw [E (y|µ, Σ, w)]
= Ew [V ar (y|µ, Σ, w)] = Ew

Σ
w

= ΣEw

 1
w

.
It is shown in Chapter 2 that the average value of the reciprocal of a gamma
random variable is given by
Ew

 1
w

=
a
b −1 =
ν
2
ν
2 −1 =
ν
ν −2
in this case. Thus
V ar (y|µ, Σ, ν) =
ν
ν −2Σ.
Marginal and Conditional Distributions
A similar development can be adopted to show that all marginal and con-
ditional distributions deriving from a multivariate-t distribution are uni-
variate or multivariate-t as well. This is so since for any arbitrary partition
of y in (1.116), say,

y1
y2
"""" µ, Σ, w ∼N


µ1
µ2

,
 Σ11
Σ12
Σ21
Σ22
 1
w

,

62
1. Probability and Random Variables
all marginal and conditional distributions are normal, with appropriate
mean vector and covariance matrix. Integration over the Ga (ν/2, ν/2)
distribution leads to the desired result directly. For example, the condi-
tional distribution of y1 given y2 is an n1-dimensional (the order of y1)
multivariate-t with mean vector
E (y1|y2, µ, Σ, ν) = µ1 + Σ12 (Σ22)−1 (y2 −µ2) ,
covariance matrix
V ar (y1|y2, µ, Σ, ν) =
ν
ν −2

Σ11 −Σ12 (Σ22)−1 Σ21

,
and degrees of freedom ν. A similar reasoning leads to the result that any
linear combination of a vector that has a multivariate-t distribution must
be multivariate- (or univariate-) t distributed also.
1.5
Distributions with Constrained Sample Space
In genetics, situations where the sampling space of random variables or
where the space of unknowns is restricted, are not uncommon. For example,
a Poisson sampling model might be sensible for describing litter size in pigs.
However, litters of size 0 may not be reported in practice! In this situation,
one may consider adopting a Poisson distribution with probabilities “nor-
malized” such that the event x = 0 is not observable. Likewise, many “selec-
tion” models have been proposed in the context of the multivariate normal
distribution and these often involve a restriction of the sampling space of
the variables entering into the problem. Perhaps the best known one is selec-
tion by truncation (Pearson, 1903; Aitken, 1934). Henderson et al. (1959),
Curnow (1961), Thompson (1973), Henderson (1975), Thompson (1976),
Robertson (1977), Bulmer (1980), and Gianola et al. (1989), among others,
have discussed the eﬀects of selection of multivariate normal variates in
a genetic context, from the point of view of either parameter estimation
(ﬁxed eﬀects; variance components) or of prediction of breeding values.
Let X be a discrete random variable with p.m.f. p (x) and let a and b be
constants lying within the support of the domain of p. Then, the doubly
truncated p.m.f. of X, given that a < X ≤b, is
Pr (X = x|a < X ≤b) = Pr (X = x, a < X ≤b)
Pr (a < X ≤b)
=
Pr (X = x)
Pr (a < X ≤b),
for a < X ≤b.
Therefore,
Pr (X = x|a < X ≤b) =
#
0,
if x ≤a or x > b,
p(x)
F (b)−F (a),
if a < x ≤b,

1.5 Distributions with Constrained Sample Space
63
where F(·) is the distribution function. In particular, given a truncation
point t, the p.m.f. of X, given that X > t (i.e., truncated below), is
Pr (X = x|X > t) =
#
0,
if x ≤t,
p(x)
1−F (t),
if x > t.
(1.121)
Example 1.20
Sibship size
As an example of a discrete truncated distribution, let s denote the number
of children in a nuclear family. Assuming that a male or a female birth are
equally likely (i.e., θ = 1/2), the probability that there will be exactly x
girls in a family of size s, assuming a binomial sampling model, is
Pr (X = x|θ, s) =

s
x
 
1
2
x 
1
2
s−x
=

s
x
 
1
2
s
,
x = 0, 1, . . . , s.
Suppose that there is interest in the distribution of the number of girls
in nuclear families of size s that have at least one girl. The corresponding
probability, using (1.121), is
Pr (x|X > 0, θ, s) =
Pr (X = x|θ, s)
1 −Pr (X = 0|θ, s)
=

s
x
  1
2
s
1 −
 1
2
s
,
x = 1, 2, . . . , s.
For example, for s = 4, the unconstrained and truncated probability dis-
tributions are
x
0
1
2
3
4
p (x|θ, s)
0.0625
0.2500
0.3750
0.2500
0.0625
p (x|X > 0, θ, s)
0
0.2667
0.4000
0.2667
0.0667
The two distributions do not diﬀer by much because Pr(X = 0|θ, s) takes
a low value in the untruncated binomial distribution.
■
Let a random vector z have a multivariate distribution with density
p (z|θ), where θ is the parameter vector. If the original sampling space of
z, Rz is constrained such that this vector is observed only in a more limited
space, Rc
z, and this happens with probability

p (z|θ) I (z ∈Rc
z) dz = P
where I (·) is an indicator function, then the density of the constrained
distribution of z is p (z|θ) /P, which can be seen to integrate to 1 over

64
1. Probability and Random Variables
the space Rc
z. This setting has been used extensively by Pearson (1903)
and Henderson (1975), among others, in genetic applications. A special
case is the truncation selection model of quantitative genetics, which is
described now for a bivariate situation. Let two random variables X and Y
have a bivariate distribution with joint p.d.f. p(x, y). Assume that selection
operates on X such that this random variable is observed only if X ≥t,
where t is a known truncation point, or threshold of selection, whereas
the sampling space of Y remains unrestricted. The density of the joint
distribution of X and Y (after selection), using the result given above, is
p (x, y|X ≥t) =
p (x, y)
∞

−∞
∞

t
p (x, y) dx dy
=
p (x, y)
∞

t
p (x) dx
= p (x, y)
P
,
(1.122)
where P is now the proportion selected. The conditional density of Y given
X (after selection) is, by deﬁnition
p (y|x, X ≥t) = p (y, x|X ≥t)
p (x|X ≥t)
= p (y, x) /P
p (x) /P
= p (y|x) .
(1.123)
It follows that this conditional distribution is the same as in the absence of
selection. This result is not unexpected because, intuitively, the conditional
distribution [Y |X] is deﬁned for any X, so whatever happens with the
sample space of X is irrelevant. An important corollary is that the form of
E(Y |X), the regression function, is unaﬀected by selection operating on X,
this being true for any distribution. However, the marginal distribution of Y
is altered by selection, unless X and Y are independent. This is illustrated
below.
Example 1.21
Parental selection
Suppose there are two unrelated parents and one oﬀspring; let their additive
genetic values be a1, a2 and a3 respectively. Under multivariate normality
the conditional distribution of the additive genetic value of a3 given a1 and
a2 is normal with mean E (a3|a1, a2) = (a1 + a2) /2 and variance σ2
a/2;
this variance can be found using formula (1.83), taking into account that
the covariance between the additive genetic values of a parent and of an
oﬀspring is σ2
a/2. This conditional distribution holds true for any pair of
parents, selected or unselected, so the expected value of the additive genetic
value of an oﬀspring, conditionally on parental breeding values, is always
equal to the average of the additive genetic values of parents. However, the
unconditional mean and variance of a3 are aﬀected by the selection process.
Letting Es denote expected value under selection, it follows, by virtue of
the theorem of double expectation (to be discussed below), that
Es (a3) = Es [E (a3|a1, a2)] = Es
a1 + a2
2

̸= E
a1 + a2
2

,

1.5 Distributions with Constrained Sample Space
65
so the mean of the additive genetic values in the oﬀspring, following selec-
tion, in general, will not be equal to the mean breeding value of parents
selected at random, unless selection is of a stabilizing form (Bulmer, 1980).
Likewise, the variance of breeding values in progeny of selected parents,
V ars, is
V ars (a3)
=
Es [V ars (a3|a1, a2)] + V ars [Es (a3|a1, a2)]
=
σ2
a
2 + V ars
a1 + a2
2

=
1
2

σ2
a + σ2
as

,
where σ2
as is the variance of the breeding values within selected parents.
Most often, the latter will not be equal to the variance in the absence of
selection. It can be lower or larger depending on the form of selection.
■
In a Bayesian analysis, inferences about a parameter vector θ are made
from the conditional distribution [θ|y], called posterior distribution in this
speciﬁc context. Parameters are treated in the Bayesian approach as ran-
dom variables (the randomness arises from the state of uncertainty about
their values) and the analysis is made conditionally on the observed data,
y. Suppose y0 and y1 are vectors of data of a selection experiment col-
lected at generations 0 and 1, respectively. Assume that individuals with
data y1 are the oﬀspring of selected parents, and that selection was based
on the available phenotypic records, y0. Further, suppose that selection is
by truncation, as before, and that the truncation point t is known. Had
there been no selection, the density of the posterior distribution (using all
data) is
p (θ|y0, y1) = p (θ, y0, y1)
p (y0, y1) .
(1.124)
However, if the joint distribution [y0, y1] is modiﬁed by selection such that
only individuals whose phenotypic records exceed t (this is informally de-
noted as y0 > t) are used as parents, the posterior distribution must be
written as
p (θ|y0, y1, y0 > t) = p (θ, y0, y1|y0 > t)
p (y0, y1|y0 > t)
= p (θ, y0, y1)
P
P
p (y0, y1)
= p (θ|y0, y1) ,
(1.125)
where
P =
 ∞
t
 ∞
−∞
p (y0, y1) dy1 dy0
is the probability of selection. The important point is that when all data
on which selection operates is included in the analysis, inferences about

66
1. Probability and Random Variables
θ can be made from the posterior density p (θ|y0, y1), as if selection had
not taken place. Including all data (records from parents, nonparents and
oﬀspring) in the analysis is necessary, to justify using the sampling model
[y0, y1|θ] as if selection had not occurred. This sampling model is needed for
constructing the posterior density (1.125). Gianola and Fernando (1986),
and more recently Sorensen et al. (2001), discuss this result in a more
general setting.
Simulation of Univariate Truncated Distributions
An eﬃcient algorithm for sampling from truncated distributions can be
found in Devroye (1986) and is as follows. Let Y be a random variable from
a normal distribution, truncated between a (lower bound) and b (upper
bound). To sample from the truncated normal TN(a,b)

µ, σ2
, where µ
and σ2 are the mean and variance before truncation:
• Simulate U from a uniform distribution Un (0, 1).
• The truncated normal is
Y = µ + σΦ−1 [p1 + U (p2 −p1)] ,
where Φ−1 (·) is the inverse c.d.f. of the normal distribution,
p1 = Φ [(a −µ) /σ]
and
p2 = Φ [(b −µ) /σ] .
The method can be generalized to any univariate distribution truncated
in the interval [a, b]. If the c.d.f. of the untruncated variate is F, then a
draw from the truncated distribution is
y = F −1 {F (a) + U [F (b) −F (a)]} .
(1.126)
The proof that (1.126) is a value from the desired truncated distribution
in the interval [a, b] is as follows
Pr (Y ≤y) = Pr

F −1 {F (a) + U [F (b) −F (a)]} ≤y

= Pr [F (a) + U [F (b) −F (a)] ≤F (y)]
= Pr

U ≤F (y) −F (a)
F (b) −F (a)

=

F (y)−F (a)
F (b)−F (a)
0
du
= F (y) −F (a)
F (b) −F (a) .

1.6 Iterated Expectations
67
1.6
Iterated Expectations
Let x and y be two random vectors with joint p.d.f. p(x, y) and let p(x|y)
denote the conditional p.d.f. of x given y (the distinction between a random
variable and its realized value is dropped in this section). Then
E (x) = Ey [E (x|y)] .
(1.127)
This also holds for an arbitrary function of x, f (x), in which case x is
replaced above by f(x). The proof of (1.127) is as follows:
E (x)
=

xp (x) dx =
 
xp (x, y) dx dy
=
 
xp (x|y) dx

p (y) dy
=

[E (x|y)] p (y) dy = Ey [E (x|y)] ,
where p(y) is the marginal density of y. Thus the mean of x can be obtained
by averaging conditional (given y) means over the marginal distribution of
y. This result is at the root of Monte Carlo methods that use conditional
distributions. For example, one can estimate E(x) by either direct drawings
from the distribution of x or, alternatively, by drawing samples from the
distribution of y and computing E(x|y) for each sample. Then weighted
averages of these conditional expectations are calculated, where the weights
are assigned according to the density that the values of y take in their dis-
tribution. As shown below, since V ar (x) ≥V ary [E (x|y)], inferences using
the conditional mean rather than direct drawings from the distribution of
x are usually more precise.
A similar result holds for the covariance
Cov (x, y′) = Ez [Cov (x, y′|z)] + Covz [E (x|z) , E (y′|z)] ,
(1.128)
where Cov(·) now indicates a covariance matrix. Using the above result,
observe that by deﬁnition of a covariance matrix (Searle, 1971),
Ez [Cov (x, y′|z)] = Ez {E (xy′|z) −[E (x|z)] [E (y′|z)]}
= Ez [E (xy′|z)] −Ez {[E (x|z)] [E (y′|z)]}
= E (xy′) −[E (x) E (y′)] −Ez {[E (x|z)] [E (y′|z)]}
+ Ez [E (x|z)] Ez [E (y′|z)]
= Cov (x, y′) −Covz [E (x|y) , E (y′|z)]
and this leads to the desired result directly. The expression for the variance
is obtained immediately, setting x = y,
V ar (x) =Ey [V ar (x|y)] + V ary [E (x|y)] .
(1.129)

68
1. Probability and Random Variables
Example 1.22
Predicting a random variable using the conditional mean
Let Y be a random variable to be predicted using some function u (X) in-
volving the known random variable X = x. For any function u (X), consider
E[(Y −u (X))2] equal to
E{[Y −u (X)]2} =
 
[y −u (x)]2 p (x, y) dxdy.
(1.130)
Expression (1.130) is minimized when u (x) = E (Y |X = x). The proof is
as follows. Let E (Y |X = x) = w (x) and write (1.130) as
E{[Y −u (X)]2} = E
9
[(Y −w (X)) + (w (X) −u (X))]2:
= E
9
[Y −w (X)]2:
+ E
9
[w (X) −u (X)]2:
+2E {[Y −w (X)] [w (X) −u (X)]} .
The expectation involving the cross-product term can be written as
E {[Y −w (X)] [w (X) −u (X)]}
= EX {EY [(Y −w (X)) (w (X) −u (X)) |X]}
= EX {(w (X) −u (X)) EY [(Y −w (X)) |X]} .
However
EY [(Y −w (X)) |X] = E (Y |X) −w (X) = 0.
Then it follows that
E{[Y −u (X)]2} = E
9
[Y −w (X)]2:
+ E
9
[w (X) −u (X)]2:
.
The ﬁrst term in the right-hand side does not involve u (X), and
E
9
[w (X) −u (X)]2:
≥0
with equality if u (X) ≡w (X) = E (Y |x). Deﬁning the “best predictor”
as that for which E[(Y −u (X))2] is a minimum, then the best choice for
u (x) is
u (x) = E (Y |X = x) .
■
Example 1.23
Variation in gene frequencies: the beta–binomial distri-
bution
Return to Example 1.8, where clusters of n alleles are drawn at random,
and where θ = Pr (allele A) . Suppose now that θ varies between clusters
according to a beta distribution, with density
p

θ|θ, c

=
Γ (c)
Γ

cθ

Γ

c

1 −θ
θcθ−1 (1 −θ)c(1−θ)−1 .

1.6 Iterated Expectations
69
Recall that, in the parameterization of Wright (1968), c = a + b and
θ =
a
a + b.
The two alternative parameterizations will be used interchangeably here.
Let X be the number of A alleles in a cluster of size n and suppose that,
given θ, its distribution is binomial. Then, X = n
i=1 Xi, where, given θ,
Xi is a Bernoulli variable with success probability θ. Hence
E (X|θ) =
n

i=1
E (Xi) = nθ
and
V ar (X|θ) =
n

i=1
V ar (Xi) = nθ (1 −θ) ,
since the Bernoulli variables are assumed to be conditionally independent.
Now, using (1.127), the mean value of X over clusters is
E (X) = E [E (X|θ)] = E (nθ) = nθ =
na
a + b.
(1.131)
Employing (1.129),
V ar (X)
=
E [V ar (X|θ)] + V ar [E (X|θ)]
=
E [nθ (1 −θ)] + V ar (nθ)
=
n

E (θ) −E2 (θ) + n V ar (θ)

.
Using the mean and variance of the beta distribution, given in (1.37) and
(1.38), respectively, one obtains after algebra,
V ar (X)
=
nab (a + b + n)
(a + b)2 (a + b + 1)
=
nθ

1 −θ
 
c + n
c + 1

.
(1.132)
If the cluster has size n = 1, the mean of the marginal distribution of X is
θ, and the variance is θ

1 −θ

.
Next, we derive the correlation between alleles within a cluster. Note that
for a cluster of size n
V ar (X)
=
V ar
 n

i=1
Xi

= n V ar (Xi) + n (n −1) Cov (Xi, Xj)
=
n V ar (Xi) [1 + (n −1) ρ]
=
nθ

1 −θ

[1 + (n −1) ρ] ,

70
1. Probability and Random Variables
where ρ is the correlation between alleles; if ρ = 0, there is no within-cluster
aggregation and sampling is purely binomial. The correlation between al-
leles is
ρ =
1
n −1

V ar (X)
nθ

1 −θ
 −1

.
The marginal distribution of X is called beta–binomial, which is discrete.
Its form can be derived by noting that if X|θ is binomial, and θ follows a
beta process, then the marginal distribution of X is given by
Pr

X = x|n, c, θ

=
1

0

n
x

θx (1 −θ)n−x
×
Γ (c)
Γ

cθ

Γ

c

1 −θ
θcθ−1 (1 −θ)c(1−θ)−1 dθ
=

n
x

Γ (c)
Γ

cθ

Γ

c

1 −θ

×
1

0
θx+cθ−1 (1 −θ)n−x+c(1−θ)−1 dθ
=

n
x

Γ (c)
Γ

cθ

Γ

c

1 −θ

×Γ

x + cθ

Γ

n −x + c

1 −θ

Γ (n + c)
.
(1.133)
The last expression results from use of the integral in (1.36). Below is
another example of the beta–binomial process.
■
Example 1.24
Deconditioning a binomial distribution having a random
parameter
As discussed before, the sum of n independent random variables Xi, each
following a Bernoulli probability distribution Br (θ), is a random variable
that has a binomial distribution. Assume now that the probability of suc-
cess, θ, is unknown, and that this random variable is assigned a beta dis-
tribution Be (a, b). Thus, conditionally on θ
Pr (Xi = xi|θ) =

Br (xi|θ) = θxi (1 −θ)1−xi ,
for xi = 0, 1,
0,
for other values of xi.
(1.134)
The density of the beta distribution for θ is
p (θ|a, b) = Be (θ|a, b) =

Cθa−1 (1 −θ)b−1 ,
for 0 ≤θi ≤1,
0,
for other values of θ,
(1.135)

1.6 Iterated Expectations
71
where a and b are the parameters of the beta distribution. Recall that the
mean and variance of the beta distribution are given, respectively, by
E (θ|a, b)
=
a
a + b,
V ar (θ|a, b)
=
ab

(a + b)2 (a + b + 1)
.
Consider the random variable Y = n
i=1 Xi, the total number of “suc-
cesses” in n trials. Given θ, the random variable Y is Bi (θ, n), and its
marginal distribution is in the form of a beta–binomial, which is generated
by the mixture
Bb (y|a, b, n) =
 1
0
Bi (y|n, θ) Be (θ|a, b) dθ,
(1.136)
as in the previous example. Any of the moments of [Y |a, b, n] can be ob-
tained from (1.136). In animal breeding, the beta–binomial model could
arise in the following context. Suppose that in a given cluster (a cow in
a given herd, say), a cow is artiﬁcially inseminated and pregnancy is reg-
istered. Pregnancy can be modeled as a Bernoulli random variable, with
unknown probability equal to θ associated with the particular cluster. Here
we obtain the mean and variance of Y , the rate of calving of the cow in n
trials, unconditionally on θ, using iterated expectations (in a more realis-
tic set up, there would be several clusters, each with its own probability of
pregnancy). The mean of the marginal distribution of Y is obtained directly
as follows:
E (Y )
=
E
 n

i=1
Xi

=

i
Eθ [E (Xi|θ)]
=

i
Eθ (θ)
=
na
a + b.
Because, given θ, the Xi are independent, the conditional variance is
V ar (Y |θ) = V ar
 n

i=1
(Xi|θ)

=
n

i=1
V ar (Xi|θ) = nθ (1 −θ) .

72
1. Probability and Random Variables
Recalling (1.129), the marginal variance of Y is
V ar (Y ) = Eθ [V ar (Y |θ)] + V arθ [E (Y |θ)] .
Since E (Y |θ) = nθ, and θ has a beta distribution,
V arθ [E (Y |θ)] = V ar (nθ) =
n2ab
(a + b)2 (a + b + 1)
.
Also, V ar (Y |θ) = nθ (1 −θ). Then
Eθ [V ar (Y |θ)] = Eθ [nθ (1 −θ)]
= n
1

0
θ (1 −θ) p (θ|a, b) dθ
= Cn
1

0
θ (1 −θ) θa−1 (1 −θ)b−1 dθ
=
nab
(a + b) (a + b + 1).
Thus
V ar (Y ) =
nab
(a + b)2
a + b + n
a + b + 1
= nE (θ|a, b) [1 −E (θ|a, b)] a + b + n
a + b + 1 .
The variance of the beta–binomial with mean probability a/ (a + b) is
greater by a factor (a + b + n) / (a + b + 1) than the binomial with the
same probability. When n = 1, there is no information available to dis-
tinguish between the beta and binomial variation, and both models have
equal variances.
■
Example 1.25
Genetic markers and the covariance between half-sibs
Suppose a male is drawn at random from a population in equilibrium (i.e.,
where mating is at random, so gene and genotypic frequencies are constant
from generation to generation, and there is no inbreeding or assortative
mating). Let the additive genetic variance of a trait of interest at an auto-
somal additive locus be Vg. Since allelic eﬀects would be independently and
identically distributed in this population, Vg is equal to twice the variance
between either paternal or maternal allelic eﬀects. Assume that this male
is randomly mated to an unknown female, also sampled at random, and
that two half-sibs are born from this mating. Let x and y designate the
values of the alleles (haplotypes) that the two half-sibs received from their

1.6 Iterated Expectations
73
father and assume that E (x) = E (y) = 0. The allelic variance is
V ar (x) = V ar (y) = 1
2Vg.
The additive genetic covariance between half-sibs, Cov (x, y), can be de-
rived as follows. Let z be a Bernoulli random variable taking the value 1 if
the paternally derived alleles from both individuals are identical by descent
(IBD), and 0 otherwise. Using (1.128),
Cov (x, y)
=
Ez [Cov (x, y|z)] + Covz [E (x|z) , E (y|z)]
=
Ez [Cov (x, y|z)] .
This is so, because E (x|z) = E (x) = 0, a constant, with the same holding
for y. In other words, the mean value of an allelic eﬀect is not altered by
knowledge of identity by descent. Taking expectations with respect to the
distribution of z yields
Cov (x, y) = Ez [Cov (x, y|z)]
= Cov (x, y|z = 0) Pr (z = 0) + Cov (x, y|z = 1) Pr (z = 1) .
(1.137)
The term Cov (x, y|z = 0) is null. This is because, if the two alleles are not
IBD, the allelic eﬀects are independently distributed; thus
Cov (x, y|z = 0) = E (xy|z = 0) = E (x) E (y) = 0.
Further
Cov (x, y|z = 1) = V ar (x) = 1
2Vg,
(1.138)
because if the two alleles are identical by descent, x ≡y. In order to
calculate the probability of IBD, suppose the father has genotype A1A2,
where subscripts 1 and 2 are labels for chromosomes 1 and 2, respectively.
Thus, in principle, alleles at position A1A2 can be identical in state. We
refer to the event “drawing allele Ai (i = 1, 2) from the ﬁrst half-sib with
value x” as “Ai in x”. Similarly, the event, “drawing allele Ai from the other
half-sib with value y” is written as “Ai in y”. The required probability is
Pr (z = 1) = ω11 Pr (A1 in x ∩A1 in y)
+ ω22 Pr (A2 in x ∩A2 in y)
+ ω12 [Pr (A1 in x ∩A2 in y) + Pr (A2 in x ∩A1 in y)]
= ω11
1
2
1
2 + ω22
1
2
1
2 + 2ω12
1
2
1
2,
(1.139)
where ωij = Pr (Ai ≡Aj|Ai in x ∩Aj in y), i, j = 1, 2, which we write
Pr (Ai ≡Aj) for short. Above, the four terms of the form
Pr (Ai in x ∩Aj in y) ,
i = 1, 2, j = 1, 2,

74
1. Probability and Random Variables
are equal to
Pr (Ai in x ∩Aj in y)
=
Pr (Ai in x) Pr (Aj in y)
=
1
2
1
2 = 1
4,
since there is a probability of 1/2 of drawing one of the two alleles from
each individual and the two drawings are independent. Expression (1.139)
results from the fact that the probability involves four mutually exclusive
and exhaustive events. Two such events pertain to the situation where the
same allele is picked on each of the individuals, and here
Pr (A1 ≡A1) = Pr (A2 ≡A2) = 1.
The other two events involve diﬀerent alleles, but it must be noted that
Pr (A1 ≡A2) may not always be zero, as these alleles might be copies of
the same allele from a common ancestor. Since we assume there is no in-
breeding, Pr (A1 ≡A2) = 0, so
Pr (z = 1) = 1
2
1
2 + 1
2
1
2 = 1
2.
(1.140)
Using (1.138) and (1.140) in (1.137), the half-sib covariance is
Cov (x, y) = 1
4Vg.
(1.141)
Imagine now that the sire is known to be heterozygote for a genetic marker
linked to the locus aﬀecting the trait in question. Let the recombination
fraction between the marker and the locus be r, and deﬁne a new random
variable M, taking the value 1 if the marker alleles are the same in both
half-sibs, and 0 otherwise. The additive genetic covariance between the two
half-sibs, conditionally on their marker information, is expressible as
Cov (x, y|M) = Ez [Cov (x, y|z, M)]
+ Covz [E (x|z, M) , E (y|z, M)]
= Ez [Cov (x, y|z, M)] ,
since the covariance between the conditional means is zero, following the
same argument as before. Now take expectations with respect to the dis-
tribution of z. When the half-sibs receive the same marker allele from the
sire (M = 1), one has
Ez [Cov (x, y|z, M = 1)]
= Cov (x, y|z = 0, M = 1) Pr (z = 0|M = 1)
+ Cov (x, y|z = 1, M = 1) Pr (z = 1|M = 1)
= Cov (x, y|z = 1, M = 1) Pr (z = 1|M = 1) .
(1.142)

1.6 Iterated Expectations
75
The conditional probability in the bottom line of (1.142) is
Pr (z = 1|M = 1) = Pr (z = 1, M = 1)
Pr (M = 1)
.
(1.143)
The numerator in (1.143) is the probability of drawing independently two
gametes from the sire that have the same marker allele and the same allele
at the locus in question. These gametes are either nonrecombinant, with
probability 1
2 (1 −r)2, or recombinant, with the corresponding probability
being 1
2r2. Therefore the numerator of (1.143) is equal to
1
2 (1 −r)2 + 1
2r2.
Since marker alleles in the two half-sibs are equal to each other one-half of
the time, Pr (M = 1) = 1
2. Therefore
Pr (z = 1|M = 1) = (1 −r)2 + r2.
The conditional covariance between the half-sibs, given that they inherited
the same marker allele from their sire, is
Cov (x, y|M = 1)
=
Cov (x, y|z = 1, M = 1)

(1 −r)2 + r2
=
(1 −r)2 + r2
2
Vg.
(1.144)
This is because, if the two alleles are identical by descent,
Cov (x, y|z = 1, M = 1) = Vg
2
in (1.142). Similar arguments lead to the following expression for the condi-
tional covariance between the half-sibs, given that they inherited diﬀerent
marker alleles from their sire
Cov (x, y|M = 0) = r (1 −r)
2
Vg.
(1.145)
As r →0, (1.144) and (1.145) tend to V g/2 and to 0, respectively. On the
other hand, when the marker provides less and less information about the
locus in question (i.e., when r →1/2), both (1.144) and (1.145) tend to
V g/4, as in (1.141), as it should.
■
In this chapter, we have discussed and illustrated the most important
univariate and multivariate distributions encountered in statistical genetics.
This is extended in Chapter 2, which discusses random processes arising
from functions of random variables.

This page intentionally left blank

2
Uncertainty about Functions
of Random Variables
2.1
Introduction
It is seldom the case that the initial parameterization of a statistical model
for genetic analysis will lead directly to inferences about all parameters of
interest. One may also wish to learn about functions of the parameters of
the model. An illustration is the use of a linear mixed model parameter-
ized in terms of variance components. The investigator may wish to make
inferences about variance ratios or functions thereof, such as intraclass cor-
relations or heritabilities. As another example, consider data from a trial
in which experimental mice are subjected to varying doses of a carcino-
genic agent. The response variable is whether a tumor has developed or
not at the end of the trial. Because of the binary nature of the response,
a Bernoulli sampling model may be adopted, and a linear structure (with
dose as an explanatory variable) may be imposed to the log of the ratio
between the probability of developing a tumor at a given dose and that
of the complementary event. This is a “generalized linear model” with a
logit link function (Nelder and Wedderburn, 1972; McCullagh and Nelder,
1989). Then, for mouse j at dose k, one could have
log
pjk
1 −pjk
= β0 + β1xk,
(2.1)
where pjk is the probability of response, xk is the dose or a function thereof
(such as the logarithm of the dose), and β0 and β1 are parameters of the
dose-response process. It may be that the latter are not the parameters
of primary interest, for example, one may wish to ﬁnd the dose at which

78
2. Functions of Random Variables
the probability of developing a tumor is 1
2 (this is usually called the LD-50
dose, with LD standing for “lethal dose”). At this probability, it is seen
that
xLD-50 = −β0
β1
.
(2.2)
In a model where the parameters are regarded as random and, therefore,
having a joint distribution, it would follow that the LD-50 is also a random
variable, by virtue of being a function of randomly varying quantities. Even
if the parameters are not “random”, in the usual sense of being drawn
randomly from a conceptual population of values (as in a random eﬀects
model), one could perhaps argue from a Bayesian perspective, as follows.
Since the LD-50 is unknown, it is an uncertain quantity and, therefore,
there is randomness which is naturally measured by probability. Here the
LD-50 would be viewed as random irrespective of whether or not the β’s are
drawn randomly from a population! Disregarding the issue of the origin of
the randomness, model (2.1) would require invoking a bivariate distribution
for β0 and β1, whereas the LD-50 in (2.2) involves a scalar distribution
resulting from a nonlinear function of β0 and β1.
In this chapter, we review some concepts of distribution theory needed
for functions of random variables, and illustrate the theory with examples.
Single random variables are considered in the ﬁrst section. Subsequently,
functions of sets of random variables are discussed.
2.2
Functions of a Single Random Variable
2.2.1
Discrete Random Variables
If X is a discrete random variable having some p.m.f. p (x), then any func-
tion of X, say Y = f (X), is also a random variable. If the inverse trans-
formation from Y to X is denoted by f −1 (Y ) = X, then the p.m.f. of the
random variable Y is
pY (y) = pX

f −1 (y)

.
(2.3)
Example 2.1
A situation involving the binomial distribution
Suppose that X has a binomial distribution, Bi (p, n). Consider the random
variable Y = f (X) = n−X. The inverse transformation is f −1 (Y ) = X =
n −Y . The probability function of Y is then
pY (y) = pX

f −1 (y)

= pX (n −y)
=

n
n −y

pn−y (1 −p)n−(n−y)
=

n
y

(1 −p)y pn−y.

2.2 Functions of a Single Random Variable
79
Therefore, Y has also a binomial distribution, but now with parameters
(1 −p) and n. The sample space of Y is the same as that of X, that is,
Y = 0, 1, ..., n.
■
2.2.2
Continuous Random Variables
Let X be a continuous random variable having sample space A (denoted
as X ∈A) and p.d.f. p (x) (so p (x) = 0 for x /∈A). Let Y be a function
of X, Y = f (X). Then, if f(·) is a monotone function and the inverse
transformation is X = f −1 (Y ), the p.d.f. of Y is given by
pY (y) = pX

f −1 (y)
 """"
d
dy f −1 (y)
""""
= pX

f −1 (y)

|J (y)| ,
y ∈f (A) ,
(2.4)
and pY (y) = 0 for y /∈f (A). In (2.4), |J (y)| is the absolute value of the
Jacobian of the transformation as a function of y. This ensures that the
density is positive throughout (the derivative of the inverse function with
respect to y may be negative at some values).
Interpretation of expression (2.4) can be facilitated recalling (1.21) which
discloses that a p.d.f. has units: probability by unit of measurement of the
random variable. A change in these units leads naturally to a change in the
density function.
An equivalent, perhaps more suggestive way of writing (2.4) is
pY (y) = pX (x)
""""
dx
dy
"""" ,
y ∈f (A) , x = f −1 (y) .
(2.5)
Result (2.4) is not intuitively obvious, and its proof is as follows (e.g.
Hoel et al., 1971). Let F and G denote the respective c.d.f.s of X and Y .
Suppose ﬁrst that Y = f (X) is strictly increasing, i.e., f (x1) < f (x2) if
x1 < x2, with x1 ∈A and x2 ∈A. Then f −1 is strictly increasing on f (A) ,
for y ∈f (A). One can write
G (y) = Pr (Y ≤y)
= Pr (f (X) ≤y)
= Pr

X ≤f −1 (y)

= F

f −1 (y)

.
Using the chain rule of diﬀerentiation yields
d
dy G (y) = d
dy F

f −1 (y)

=
d
df −1 (y)F

f −1 (y)
 df −1 (y)
dy
= p

f −1 (y)
 df −1 (y)
dy
.

80
2. Functions of Random Variables
Now
df −1 (y)
dy
=
""""
df −1 (y)
dy
""""
since f −1 is strictly increasing, so this yields (2.4). Suppose next that f is
strictly decreasing on A. Then f −1 is strictly decreasing on f (A), and for
y ∈f (A) the following holds:
G (y) = Pr (Y ≤y)
= Pr (f (X) ≤y)
= Pr

X ≥f −1 (y)

= 1 −F

f −1 (y)

.
Thus
d
dy G (y) = −d
dy F

f −1 (y)

=
d
df −1 (y)F

f −1 (y)
 
−df −1 (y)
dy

= p

f −1 (y)
 
−df −1 (y)
dy

.
Here
−df −1 (y)
dy
=
""""
df −1 (y)
dy
""""
because f −1 is strictly decreasing, thus yielding (2.4) again. Therefore, in
either case, we see that the density of Y is given by (2.4).
Example 2.2
The lognormal distribution
A variable whose logarithm is normally distributed is said to have a lognor-
mal distribution. Let X ∼N

m, σ2
, for −∞< x < ∞, and suppose one
seeks the distribution of the transformed random variable Y = f (X) =
exp (X). The inverse transformation is X = f −1 (Y ) = ln (Y ). The p.d.f.
of Y is then, using (2.4),
p (y) = p

f −1 (y)
 """"
d
dy f −1 (y)
""""
=
1
y
√
2πσ2 exp

−1
2σ2 (ln y −m)2

,
where 1/y enters from the Jacobian of the transformation. Then, Y =
exp (X) is said to have a lognormal distribution, with density as given
above. This distribution arises, for example, in quantitative genetic analysis
of the productive life of breeding animals (Ducrocq et al., 1988) and in
survival analysis (Kleinbaum, 1996).
■

2.2 Functions of a Single Random Variable
81
Example 2.3
Distribution of the inverse of a lognormal random variable
Consider now the transformation Z = 1/Y where Y is lognormal, with
density as given above. The absolute value of the Jacobian of the transfor-
mation is z−2 and, employing this in conjunction with the density above
gives
p (z) =
1
z−1√
2πσ2 exp

−1
2σ2 (−ln z −m)2

z−2
=
1
z
√
2πσ2 exp

−1
2σ2 (ln z + m)2

.
This implies that the reciprocal of a lognormal random variable is lognormal
as well.
■
Example 2.4
Transforming a uniform random variable
Let X be a random variable having a uniform distribution in the interval
[0, 1], so its p.d.f. is
p (x|0, 1) =

1,
if 0 ≤x ≤1,
0,
otherwise.
The uniform distribution was used by Bayes (1763) in an attempt to rep-
resent prior ignorance about the value of a parameter within a certain
range. This “principle of insuﬃcient reason” has been used extensively in
quantitative genetics and in other ﬁelds. The uniform distribution assigns
equal probabilities to all possible ranges of equal length within which the
random variable can fall. Thus, it is intuitively appealing to use the uni-
form process to represent (in terms of probability) lack of prior knowledge.
Often though, the uniform distribution is not a good choice for conveying
vague prior knowledge. For example, one may think that a uniform distri-
bution between −1 and 1 can be used to represent prior ignorance about
a coeﬃcient of correlation. However, Bayarri (1981) showed that the prior
distribution that should be used for such purpose is not the uniform.
Suppose that one is interested in the variable Y = f (X) = −log X. The
inverse transformation is X = f −1 (Y ) = exp (−Y ). The interval [0, 1], con-
stituting the sample space of X, maps onto the interval [0, ∞) as the sample
space for Y . The absolute value of the Jacobian of the transformation is
then
""""
d
dy f −1 (−y)
"""" =
""""
d
dy exp (−y)
"""" = exp (−y) .
The p.d.f. of Y is, therefore,
p (y) = p

f −1 (y)

exp (−y) = exp (−y) ,
for y > 0.
(2.6)
This is the density of an exponentially distributed random variable with
mean and variance equal to 1. More generally, as noted in Chapter 1, the

82
2. Functions of Random Variables
density of an exponential distribution with parameter η is
p (y|η) = η exp (−ηy) ,
for y > 0
and the mean and variance can be shown to be η−1 and η−2, respectively.
From the developments leading to (2.6), it follows that if one wishes to
simulate random variables from an exponential distribution with parameter
η the draws can be computed as
y = −ln (x)
η
,
where x is a draw from Un (0, 1) .
■
Example 2.5
From the beta to the logistic distribution
Assume that X follows the beta distribution, Be (a, b), and recall that its
support is the set of values of X in the closed interval [0, 1]. Applying the
logistic transformation yields
Y = f (X) = ln

X
1 −X

,
where Y is often referred to as a logit. Note that Y is deﬁned in the space
(−∞, ∞). The inverse transformation is
f −1 (Y ) =
exp (Y )
1 + exp (Y ).
The Jacobian of the transformation, in this case being positive for all values
of y, is
d
dy

f −1 (y)

=
exp (y)
[1 + exp (y)]2
so the p.d.f. of Y is
p (y) = C

exp (y)
1 + exp (y)
a−1 
1
1 + exp (y)
b−1
exp (y)
[1 + exp (y)]2
= C
#
[exp (y)]a
[1 + exp (y)]a+b
$
,
−∞< y < ∞,
(2.7)
where the constant is:
C = Γ (a + b)
Γ (a) Γ (b)
as given in (1.36) of the previous chapter. For appropriate values of the
parameters a and b, the density (2.7) can be shown to approximate well
that of a normally distributed random variable. This is why linear models
are often employed for describing the variation of logistically transformed

2.2 Functions of a Single Random Variable
83
variables taking values between 0 and 1 (such as probabilities) before trans-
formation. For example, Gianola and Foulley (1983) described methods for
genetic analysis of logits of probabilities, using Bayesian ideas.
■
Example 2.6
The standard logistic distribution
Consider the random variable Y from a logistic distribution with p.d.f.
p (y) =
exp (y)
[1 + exp (y)]2 .
This distribution has mean 0 and variance π2/3. There may be interest in
ﬁnding the distribution of the linear combination f (Y ) = Z = α + βY ,
where α and β are constants. Here the inverse transformation is
f −1 (Z) = β−1 (Z −α) ,
and the Jacobian of the transformation is β−1. The density of interest is
then
p (z) =
exp

z−α
β

β

1 + exp

z−α
β
2 .
The density characterizes the sech-squared distribution (Johnson and Kotz,
1970b), that has mean and variance
E (Z) = α
and
V ar (Z) = (βπ)2
3
,
respectively. If, in the transformation, one takes α = 0 and β =
√
3/π, the
density becomes
p (z) =
π exp

πz
√
3

√
3

1 + exp

πz
√
3
2
(2.8)
this being the density of a standard logistic distribution, with mean 0 and
variance 1.
■
Example 2.7
Moment generating function of the logistic distribution
The moment generating function (see Chapter 1) of a random variable
following a logistic distribution is
E [exp (tX)] =
∞

−∞
exp (tx)
exp (x)
[1 + exp (x)]2 dx.
(2.9)

84
2. Functions of Random Variables
Change variables to
Y =
exp (x)
[1 + exp (x)],
so
1 −Y =
1
[1 + exp (x)],
and note that the sampling space of Y goes from 0 to 1. The inverse trans-
formation is
X = ln

Y
1 −Y

with Jacobian
dX
dY =
1
Y (1 −Y ).
Observe now that
exp (tX) = exp

t ln

Y
1 −Y

= Y t (1 −Y )−t .
Using the preceding in (2.9) yields the moment generating function
E [exp (tX)] =
1

0
y1+t−1 (1 −y)1−t−1 dy = B (1 + t, 1 −t)
= Γ (1 + t) Γ (1 −t)
Γ (2)
= Γ (1 + t) Γ (1 −t) ,
(2.10)
where B (·) is called the beta function and Γ (·) is the gamma function,
seen in Chapter 1.
■
Example 2.8
The inverse chi-square distribution from a gamma process
As seen in Chapter 1, a gamma random variable has as p.d.f.
p (x|a, b) = Ga (x|a, b) =

Γ (a) b−a−1 xa−1 exp (−bx) ,
x > 0,
(2.11)
where a and b are strictly positive parameters. If one sets a = ν/2 and
b = 1/2, where ν is an integer, the above p.d.f. becomes that of a chi-square
random variable. The parameter ν is known as the degrees of freedom of
the distribution. The p.d.f. of a chi-square random variable is
p (x|ν) =

Γ
ν
2

2ν/2−1
x
ν
2 −1 exp

−1
2x

,
x > 0.
(2.12)
Let Y = 1/X be an “inverse chi-square” random variable. Noting that
d
dy f −1 (y) = −y−2,

2.2 Functions of a Single Random Variable
85
then the p.d.f. of the inverse chi-square distribution is
p (y|ν) =

Γ
ν
2

2ν/2−1
y−( ν
2 −1) exp

−1
2y

y−2
=

Γ
ν
2

2ν/2−1
y−( ν
2 +1) exp

−1
2y

,
y > 0.
(2.13)
■
Example 2.9
The scaled inverse chi-square distribution
A by-product of the inverse chi-square distribution plays an important
role in variance component problems. Assume that X is a random variable
following an inverse chi-square distribution. Let S be a positive nonrandom
quantity called the scale parameter, and consider the transformation Y =
f (X) = SX, with inverse X = f −1 (Y ) = Y/S. Noting that the Jacobian
is
d
dy f −1 (y) = 1
S ,
then the p.d.f. of Y , using (2.4) and (2.13), is
p (y|ν, S) =

Γ
ν
2

2ν/2−1
S( ν
2)y−( ν
2 +1) exp

−S
2y

∝y−( ν
2 +1) exp

−S
2y

,
y > 0, ν > 0, S > 0.
(2.14)
This is the density of the scaled inverse chi-square distribution. An alter-
native parameterization (employed often) is obtained by deﬁning the scale
parameter as S = νS∗. For example, in certain Bayesian variance compo-
nent problems (Lindley and Smith, 1972; Box and Tiao, 1973; Gelfand and
Smith, 1990), a scaled inverse chi-square distribution is used to represent
prior uncertainty about a variance component. Here the value of S∗may
be interpreted as a statement about the mean or mode of this prior distri-
bution of the variance component, and ν as a degree of belief in such value.
The term “degree of belief” is appealing because ν is a strictly positive,
continuous parameter, whereas “degrees of freedom” is normally employed
in connection with linear models to refer to integer quantities pertaining to
the rank of certain matrices (Searle, 1971). It will be shown later that when
ν tends to inﬁnity, the mean of the scaled inverse chi-square distribution
tends toward S∗.
The scaled inverse chi-square is a special case of the inverse gamma distri-
bution, whose density is
p (x|a, b) = Cx−(a+1) exp (−b/x) ,
x > 0, a, b > 0,
(2.15)
where C = ba/Γ (a). Note that (2.14) can be retrieved from (2.15), by
setting a = ν/2 and b = S/2.

86
2. Functions of Random Variables
Suppose one seeks the mean of the distribution with density kernel (2.14).
First, the propriety (integrability to a ﬁnite value) of the distribution will
be assessed. Consider
 ∞
0
Cx−( ν
2 +1) exp

−S
2x

dx.
(2.16)
To evaluate this integral, make the change of variable y = 1/x, so dx =
−y−2 dy. Then (2.16) is expressible as
C
 ∞
0
y( ν
2 +1) exp

−S
2 y

y−2 dy = C
 ∞
0
y( ν
2 −1) exp

−S
2 y

dy.
Recalling the result from gamma integrals employed earlier in the book (for
details, see Abramowitz and Stegun, 1972), for α > 0 and λ > 0,
 ∞
0
zα−1 exp [−λz] dz = Γ (α)
λα .
Making use of this in the expression above yields
C
 ∞
0
y( ν
2 −1) exp

−S
2 y

dy = C Γ
 ν
2

 S
2
 ν
2
since
C =

Γ
 ν
2

 S
2
 ν
2
−1
.
Hence, the distribution is proper provided S and ν are both positive. Com-
putation of the expected value of the distribution requires evaluation of
E (X|ν, S) = C
 ∞
0
x x−( ν
2 +1) exp

−S
2 x−1

dx
= C
 ∞
0
y( ν
2) exp

−S
2 y

y−2dy
= C
 ∞
0
y( ν
2 −1−1) exp

−S
2 y

dy.
Making use of the gamma integral again:
E (X|ν, S) = C Γ
 ν
2 −1

 S
2
 ν
2 −1 ,
ν
2 −1 > 0,
with the preceding condition required for the gamma integral to exist.
Finally, recalling that Γ (ν) = (ν −1) Γ (ν −1), this expression reduces to
E (X|ν, S) =
S
ν −2,
ν > 2.
(2.17)

2.2 Functions of a Single Random Variable
87
If ν ≤2, the expected value is not ﬁnite. A similar development leads to
the result that if ν ≤4, the variance of the distribution is not deﬁned.
However, the scaled inverse chi-square distribution is still proper, provided
that the degree of belief parameter is positive. Note that for S = νS∗, the
expectation in (2.17) becomes
E (X|ν, S∗) = νS∗
ν −2,
ν > 2.
Hence, E (X|ν, S∗) →S∗as ν →∞. It can also be veriﬁed that the mode
of the scaled inverted chi-square distribution is
Mode (X|ν, S∗) = νS∗
ν + 2.
■
Many-to-one Transformations
There are situations in which the transformation is not one-to-one. For
example consider Y = X2, where X is a random variable with known
distribution. Here the transformation from X to Y is clearly not one-to-
one, because both X and −X produce the same value of Y . This section
discusses how to cope with these many-to-one transformations.
To formalize the argument, let X be a continuous random variable with
p.d.f. pX (x) and let Y deﬁne the many-to-one transformation Y = f (X).
If A denotes the space where p (x) > 0, and B is the space where g (y) > 0,
then there exist points in B that correspond to more than one point in
A. However, if A can be partitioned into k sets A1, A2, . . . , Ak, such that
fi deﬁnes a one-to-one transformation of each Ai onto Bi (the Bi can be
overlapping), then the p.d.f. of Y is (i.e., Rao, 1973)
g (y) =
k

i=1
Ii (y ∈Bi) pX

f −1
i
(y)

|Ji (y)| ,
(2.18)
where the indicator function Ii is 1 if y ∈Bi and 0 otherwise, and Ji (y) =
df −1
i
(y) /dy is the Jacobian of the transformation in partition i. That
is, within each region i we work with (2.4), and then add all parts i =
1, 2, . . . , k.
Example 2.10
The square of a normal random variable: the chi-squared
distribution
Let X ∼N (0, 1), with density
pX (x) =
1
√
2π exp

−x2
2

,
−∞< x < ∞.

88
2. Functions of Random Variables
Let Y = f (X) = X2. The transformation f is not one-to-one over the
given domain. However, one can partition the domain into disjoint regions
within which the transformation is one-to-one. These two regions in the
space of X are A1 = (−∞, 0) and A2 = (0, ∞). The corresponding regions
in the space of Y are B1 = (0, ∞) and B2 = (0, ∞), which in this case are
completely overlapping. In (A1, B1), f −1
1
(Y ) = −Y 1/2, and in (A2, B2),
f −1
2
(Y ) = Y 1/2. The absolute value of the Jacobian of the transformation
in both partitions is 1
2Y −1/2. The density of Y , applying (2.18), is
g (y) = 1
2y−1/2 
pX

−y1/2
+ pX

y1/2
= (1/2)1/2
√π
y−1/2 exp

−y2
2

= (1/2)1/2
Γ (1/2) y−1/2 exp

−y2
2

,
0 < y < ∞,
where the last equality arises because Γ (1/2) = √π. Here the indicator
function Ii is not needed because the regions in the space of Y are com-
pletely overlapping. From (1.42), g (y) is the p.d.f. of a random variable
from a chi-squared distribution with one degree of freedom.
■
Example 2.11
Many-to-one transformation with partially overlapping
regions
Let X have p.d.f.
pX (x) = C exp(x),
−1 ≤x ≤2,
where C is the integration constant. Consider again the many-to-one trans-
formation Y = f (X) = X2. Deﬁne the regions in the space of X, A1 =
(−1, 0) and A2 = (0, 2), with corresponding regions in the space of Y ,
B1 = (0, 1) and B2 = (0, 4), which are partially overlapping. In (A1, B1),
f −1
1
(Y ) = −Y 1/2, and in (A2, B2), f −1
2
(Y ) = Y 1/2, as in the previous
example. The absolute value of the Jacobian of the transformation in both
partitions is again equal to Y −1/2/2. The density of Y , applying (2.18), is
g (y) = 1
2y−1/2 
pX

−y1/2
I1 (y ∈B1) + pX

y1/2
I2 (y ∈B2)

= 1
2y−1/2 [C exp (−√y) I1 (y ∈B1)] + [C exp (√y) I2 (y ∈B2)] .
This can also be expressed as
g (y) =



1
2y−1/2 
C exp

−√y

+

C exp
√y

,
0 < y ≤1,
1
2y−1/2C exp
√y

,
1 < y ≤4,
which shows the discontinuity at y = 1 more transparently.
■

2.2 Functions of a Single Random Variable
89
2.2.3
Approximating the Mean and Variance
Sometimes it is extremely diﬃcult to arrive at the distribution of functions
of random variables by analytical means. Often, one may just wish to have
a rough idea of a distribution by using an approximation to its mean and
variance. In this subsection and the following one, two widely employed
and useful methods for approximating the mean and variance of the dis-
tribution of a function of a random variable are presented, and examples
are given to illustrate. The approximation in this subsection is arguably
based on a mathematical, rather than statistical argument, and has been
used extensively in quantitative genetics, specially for obtaining standard
errors of estimates of heritabilities and genetic correlations (e.g., Becker,
1984; Dempster and Lerner, 1950), as these involve nonlinear functions of
estimates of variance and covariance components. As discussed later in this
book, more powerful computer-based weaponry is presently available.
Let X be a random variable and let Y = f(X) be a function of X that
is diﬀerentiable at least twice. Expanding f(X) in a Taylor series about
X = E [X] = µ gives
Y = f (X) ∼= f [µ] +
d
dX f (X)
""""
X=µ
(X −µ)
+ 1
2
d2
(dX)2 f (X)
"""""
X=µ
(X −µ)2 .
(2.19)
Taking expectations, one obtains, as a second-order approximation to E(Y )
E [Y ] ∼= f [µ] + 1
2
d2
(dX)2 f (X)
"""""
X=µ
V ar (X) .
(2.20)
Now, taking variances over approximation (2.19) and retaining only second
order terms
V ar (Y ) ∼=

d
dX f (X)
""""
X=µ
2
V ar (X) .
Sometimes, only the linear term is retained in (2.20), and one uses as a
rough guide
Y ∼


f (µ) ,

d
dX f (X)
""""
X=µ
2
V ar (X)


.
(2.21)
Example 2.12
Mean and variance of a transformed, beta-distributed
random variable
In Example 2.5, it was noted that a logistic transformation of a beta ran-
dom variable gives an approximately normally distributed process. Thus,

90
2. Functions of Random Variables
if X ∼Be (a, b), then the variable
Y = f (X) = ln [X/ (1 −X)]
is approximately distributed as N [E (Y ) , V ar (Y )]. The mean and variance
of the beta distribution are
E (X|a, b) =
a
a + b,
V ar (X|a, b) =
ab

(a + b)2 (a + b + 1)
.
(2.22)
The mean of Y , using (2.21), is
E (Y |a, b) ∼= ln

E (X)
1 −E (X)

= ln
a
b

,
and using (2.20), is
E (Y |a, b) ∼= ln
a
b

+ (a −b) (a + b)
2ab (a + b + 1).
From (2.21) the resulting variance is
V ar (Y |a, b) ∼=
(a + b)2
ab (a + b + 1).
■
Example 2.13
Genotypes in Gaussian and discrete scales: the threshold
model
Dempster and Lerner (1950) discussed the quantitative genetic analysis of a
binary character (Yo) following the ideas of Wright (1934) and of Robertson
and Lerner (1949). These authors assumed that the expression of the trait
(Yo = 0 = attribute absent, Yo = 1 = attribute present) is related to an
underlying, unobservable normal process, and that gene substitutions take
place at this level. Let
Y = µ + G + E
be the Gaussian variable, where µ is the mean of Y and G and E are
random terms representing the genetic and environmental eﬀects on Y ,
respectively. Suppose that G ∼N(0, VG), E ∼N(0, VE) have indepen-
dent distributions. Hence, the marginal distribution of the latent variable
is Y ∼N(µ, VG+VE). Dempster and Lerner (1950) deﬁned “genotype in the
observable scale” as the conditional probability of observing the attribute,
given the genotype in the latent scale, that is
Pr (Yo = 1|G) = Pr(Y > t|G),

2.2 Functions of a Single Random Variable
91
where t is a threshold (assume, subsequently, that t = 0). In other words,
the attribute is observed if the value of the latent variable exceeds the
threshold. The “outward” genotype can be expressed as
G0 = Pr (Yo = 1|G) = Pr(Y −µ −G > t −µ −G|G)
= 1 −Pr

Z ≤−µ −G
√VE

= Φ

µ + G
√VE

,
where Z ∼N(0, 1). A ﬁrst-order approximation of the outward genotype
about 0, the mean of the distribution of G, is
G0 ∼= Φ

µ
√VE

+ φ

µ
√VE

G
√VE
.
The genetic variance in the outward scale is, approximately,
V ar (G0) = V ar

Φ

µ + G
√VE

∼= φ2

µ
√VE
 VG
VE
.
The heritability (ratio between genetic and total variance) in the underlying
scale is
h2 =
VG
VG + VE
so “heritability in the outward scale” is approximately:
h2
o = V ar (G0)
V ar (Yo)
∼=
φ2

µ
√VE

VG/VE
V ar (Yo)
.
Now
V ar (Yo) = E

Y 2
o

−E2 (Yo)
= 02 × Pr(Y < t) + 12 Pr (Y ≥t) −[Pr (Y ≥t)]2
= Pr (Y ≥t) [1 −Pr (Y ≥t)]
= Φ

−µ
√VG + VE
 
1 −Φ

−µ
√VG + VE

=

1 −Φ

µ
√VG + VE

Φ

µ
√VG + VE

,
so
h2
o =
φ2

µ
√VE

VG/VE

1 −Φ

µ
√VG + VE

Φ

µ
√VG + VE
.

92
2. Functions of Random Variables
Since the latent variable cannot be observed, one can take the residual
standard deviation in the underlying scale to be equal to 1, so all terms are
expressed in units of √VE. In this scale
h2
o =
φ2 (µ) h2/

1 −h2

1 −Φ

µ
√VG + 1

Φ

µ
√VG + 1
.
Alternatively, one could take VG + VE = 1, so VG = h2, heritability in the
latent scale. Here
h2
o =
φ2

µ
√
1 −h2

h2/

1 −h2
[1 −Φ (µ)] Φ (µ)
.
The two expressions for h2
o do not coincide with what was given by Dempster
and Lerner (1950). The reason is that these authors used a diﬀerent linear
approximation to the genotype in the discrete scale.
Consider now a second-order approximation for the genotype in the out-
ward scale. Here
G0 = Φ

µ + G
√VE

∼= Φ

µ
√VE

+ φ

µ
√VE

G
√VE
−µ
VE
φ

µ
√VE

G2.
The genetic variance in the outward scale in this case is
V ar (G0) ∼= φ2

µ
√VE
 VG
VE
+
 µ
VE
φ

µ
√VE
2
V ar(G2)
+ 2φ2

µ
√VE

µ
(VE)
3
2 Cov(G, G2).
Using the moment generating function of the normal distribution or, di-
rectly, results for the variance of quadratic forms on normal variates (and
for the covariance between a linear and a quadratic form, (Searle, 1971)),
it can be established that
V ar

G2
= 2 (VG)2
and
Cov

G, G2
= 0.
Hence, the heritability in the outward scale resulting from the second-order
approximation is
h2
o =
#
φ2

µ
√VE

+ 2

µφ

µ
√VE
2
(VG/VE)
$
(VG/VE)

1 −Φ

µ
√VG + VE

Φ

µ
√VG + VE

.

2.2 Functions of a Single Random Variable
93
The threshold model is revisited in Chapters 4 and 14.
■
2.2.4
Delta Method
The approach based on the Taylor series described above yields approxi-
mate formulas for means and variances of functions of random variables.
However, nothing is said here about the distributional properties of the
derived statistics. A related large-sample based technique, that is more for-
mally anchored statistically, known as the delta method, does this. Borrow-
ing from Lehmann (1999), let Tn be a random variable where the subscript
expresses its dependence on sample size n. As n increases towards inﬁnity,
suppose that the sequence of c.d.f.s of √n (Tn −µ) converges to the c.d.f.
of a normal variable with mean 0 and variance σ2. This limiting behavior
is known as convergence in distribution, denoted here by
√n (Tn −µ)
D
→N

0, σ2
.
(2.23)
The delta method provides the following limiting distribution for a function
of Tn, f (Tn):
√n [f (Tn) −f (µ)]
D
→N

0, σ2 [f ′ (µ)]2
,
(2.24)
where f ′ (µ) denotes the ﬁrst derivative of f (Tn) evaluated at µ. The proof
of this result is based on a Taylor expansion of f (Tn) around f (µ):
f (Tn) = f (µ) + (Tn −µ) f ′ (µ) + op (Tn −µ) .
(2.25)
The notation op (Tn −µ) denotes a random variable of smaller order than
Tn −µ for large n, in the sense that, for ﬁxed ϵ > 0,
Pr (op (Tn −µ)/ (Tn −µ) ≤ϵ) →1
as n →∞. Therefore this last term converges in probability to 0 as n
increases toward inﬁnity. (If Xn converges in probability to X, then for
ϵ > 0,
lim
n→∞Pr (|Xn −X| ≥ϵ) = 0
and hence, for large n, Xn ≈X). Subtracting f (µ) from both sides and
multiplying by √n yields
√n [f (Tn) −f (µ)] = √n (Tn −µ) f ′ (µ) + √nop (Tn −µ) .
As n →∞, the second term in the right-hand side vanishes; therefore, the
left-hand side has the same limiting distribution as
√n (Tn −µ) f ′ (µ) .

94
2. Functions of Random Variables
Since Tn −µ is approximately normal with variance σ2/n, then f (Tn) −
f (µ) is approximately normal with variance σ2 [f ′ (µ)]2 /n and result (2.24)
follows.
When the term f ′ (µ) = 0, in (2.25), it is natural to carry the expansion
to a higher-order term, provided that the second derivative exists and that
it is not 0:
f (Tn) = f (µ) + 1
2 (Tn −µ)2 f
′′ (µ) + op (Tn −µ)2 ,
where f
′′ (µ) is the second derivative of f (Tn) evaluated at µ. The last
term tends to 0 for large n; therefore we can write, loosely,
n [f (Tn) −f (µ)] = 1
2f
′′ (µ) n (Tn −µ)2 .
(2.26)
Now from (2.23) it follows that
n (Tn −µ)2
σ2
→χ2
1.
Therefore, using (2.26),
n [f (Tn) −f (µ)] →1
2σ2f
′′ (µ) χ2
1.
(2.27)
Example 2.14
Bernoulli random variables
Let Xi (i = 1, 2, . . .) be independent Bernoulli random variables with pa-
rameter θ and let Tn = n−1 n
i=1 Xi. By the central limit theorem,
√n (Tn −θ) →N [0, θ (1 −θ)]
because E (Tn) = θ and V ar (Tn) = θ (1 −θ) /n. Imagine that one is inter-
ested in the large sample behavior of the statistic f (Tn) = Tn (1 −Tn) as
an estimate of f (θ) = θ (1 −θ). From (2.24), since f ′ (θ) = 1 −2θ,
√n [Tn (1 −Tn) −θ (1 −θ)] →N

0, θ (1 −θ) (1 −2θ)2
for θ ̸= 1/ 2. When θ = 1/ 2, f ′ (1/ 2) = 0. Then, using (2.27), for θ = 1/ 2,
since f (1/ 2) = 1/ 4 and f ′′ (1/ 2) = −2,
n

Tn (1 −Tn) −1
4

→1
2
1
4 (−2) χ2
1 = −1
4χ2
1
or, equivalently,
4n
1
4 −Tn (1 −Tn)

→χ2
1.
■

2.3 Functions of Several Random Variables
95
The delta method generalizes straightforwardly to functions of random
vectors. Let Tn = (Tn1, Tn2, . . . , Tnp)′ be asymptotically multivariate nor-
mal, with mean θ = (θ1, θ2, . . . , θp)′ and covariance matrix Σ/n. Suppose
the function f (t1, t2, . . . , tp) has nonzero diﬀerential ∆= (∆1, ∆2, . . . , ∆p)′
at θ, where
∆i = ∂f
∂ti
""""
t=θ
.
Then,
√n [f (Tn) −f (θ)]
D
→N

0, ∆′Σ∆

.
(2.28)
2.3
Transformations Involving Several Discrete
or Continuous Random Variables
Let X = (X1, X2, . . . , Xn)′ be a random vector with p.m.f. or p.d.f. equal to
pX (x), and let Y = f (X) be a one-to-one transformation. Let the sample
space of X, denoted as S ⊆Rn, be such that
Pr [(X1, X2, . . . , Xn) ∈S] = 1,
or, equivalently, that the integral of pX (x) over S is equal to 1. Deﬁne
T ⊆Rn to be the image of S under the transformation, that is, as the
values of X1, X2, . . . , Xn vary over S, the values of Y1, Y2, . . . , Yn vary over
T. Corresponding to each value of Y1, Y2, . . . , Yn in the set T there is a
unique value of X1, X2, . . . , Xn in the set S, and vice-versa. This ensures
that the inverse transformation exists and this is denoted by X =f −1 (Y).
The elements of Y = f (X) are
Y1 = f1 (X1, X2, . . . , Xn) ,
Y2 = f2 (X1, X2, . . . , Xn) ,
...
Yn = fn (X1, X2, . . . , Xn) ,
whereas X = f −1 (Y) has elements
X1 = f −1
1
(Y1, Y2, . . . , Yn) ,
X2 = f −1
2
(Y1, Y2, . . . , Yn) ,
...
Xn = f −1
n
(Y1, Y2, . . . , Yn) .
Now let S′ be a subset of S and let T ′ denote the mapping of S′ under the
transformation. The events (X1, X2, . . . , Xn) ∈S′ and (Y1, Y2, . . . , Yn) ∈T ′

96
2. Functions of Random Variables
are said to be equivalent. Hence,
Pr [(Y1, Y2, . . . , Yn) ∈T ′] = Pr [(X1, X2, . . . , Xn) ∈S′]
(2.29)
which, in the continuous case, is equal to

pX (x1, x2, . . . , xn) dx1 dx2 . . . dxn,
where the integral is multidimensional and taken over S′. In order to ﬁnd
the p.d.f. of Y, a change of variable of integration must be eﬀected in (2.29),
such that x1 = f −1
1
(y) , x2 = f −1
2
(y) , . . . , xn = f −1
n
(y). In calculus books
(e.g., Kaplan, 1993) it is shown that this change of variables results in the
following expression:
Pr [(Y1, Y2, . . . , Yn) ∈T ′]
=

pX

f −1
1
(y) , f −1
2
(y) , . . . , f −1
n
(y)

|J (y)| dy1dy2 . . . dyn,
(2.30)
where |J (y)| is the absolute value of the Jacobian of the transformation.
This implies that the p.d.f. of the vector Y is (writing from now onwards
J (y) = J)
pY (y) =

pX

f −1
1
(y) , f −1
2
(y) , . . . , f−1
n
(y)

|J| ,
y ∈T,
0,
otherwise.
(2.31)
In the multivariate situation, the Jacobian is the determinant of a matrix
of ﬁrst derivatives and is given by
J = det


∂f −1
1
(y)
∂y1
. . .
∂f −1
1
(y)
∂yn
...
...
...
∂f −1
n
(y)
∂y1
. . .
∂f −1
n
(y)
∂yn


,
(2.32)
where ∂f −1
j
(y) /∂yk is the ﬁrst partial derivative of the jth element of
f −1 (Y) with respect to the kth element of Y. The Jacobian is also denoted
J = det
∂(x1, x2, . . . , xn)
∂(y1, y2, . . . , yn)

.
(2.33)
It may be that there is only one function of interest, for example, Y1 =
f1 (X1, . . . , Xn). By deﬁning appropriate additional arbitrary functions f2,
. . . , fn, such that the transformation is one-to-one, then the joint p.d.f. of
Y can be obtained by the method above. The p.d.f. of Y1 can be derived
subsequently by integrating pY (y) over the space spanned by Y2, Y3 . . . , Yn.

2.3 Functions of Several Random Variables
97
For discrete random variables, the joint probability of Y = y, deﬁned
within the sample space T, is given directly by
Pr (Y1 = y1, Y2 = y2, . . . , Yn = yn)
= p (X1 = x1, X2 = x2, . . . , Xn = xn) ,
(2.34)
where x1 = f −1
1
(y1, y2, . . . , yn) , . . . , xn = f −1
n
(y1, y2 . . . , yn).
If Y = f (X) is a many-to-one diﬀerentiable transformation, the density
of Y can be obtained by applying (2.31) to each solution of the inverse
transformation separately, and then summing the transformed densities
for each solution. This is exactly in the same spirit as in (2.18).
The Derivative and the Jacobian as a Local Magniﬁcation of a Projection
In this section a heuristic motivation of expression (2.31) is provided. A
more rigorous treatment can be found in standard books on multivariate
calculus (e.g., Kaplan, 1993; Williamson et al., 1972).
Consider the projection of a point x of a line onto the point g (x) of
another line. The magniﬁcation at x, when x is projected onto g (x), is
the absolute value of the derivative of g at x. For example, if g (x) = 2x,
then the magniﬁcation at x = u is |g′ (u)| = 2. There is a magniﬁcation
by a factor of 2 at all points, irrespective of the value of x. This means
that a distance R = x1 −x on the original line is projected onto a distance
S = g (x1) −g (x), and that the magniﬁcation of the distance at x is given
by the absolute value of the derivative of g evaluated at x:
"""" lim
x1→x
g (x1) −g (x)
x1 −x
"""" = |g′ (x)| .
(2.35)
As x1 →x, the length S is, approximately,
length of S = |g′ (x)| length of R
or, equivalently,
dS = |g′ (x)| dR.
(2.36)
This argument extends to the multivariate case as follows, and two vari-
ables are used to illustrate. Let R be a region in the uv plane and let S be
a region in the xy plane. Consider a mapping given by the functions
x = f1 (u, v) ,
y = f2 (u, v) ,
and let S be the image of R under this mapping. As R approaches zero,
area of S
area of R →|J|

98
2. Functions of Random Variables
or, equivalently,
area of S = |J| area of R,
(2.37)
where |J| is the absolute value of the Jacobian of the transformation. The
Jacobian is equal to the determinant
J = det
∂(f1, f2)
∂(u, v)

which is a function of (u, v). The absolute value of the Jacobian is a measure
of the local magniﬁcation at the point (u, v). In the bivariate case, if area
of S is approximated by dx dy and area of R is approximated by du dv,
(2.37) can be written as
dx dy = |J| du dv.
(2.38)
Then,

S
h (x, y) dx dy =

R
h (f1 (u, v) , f2 (u, v)) |J| du dv.
(2.39)
In a trivariate case, the absolute value of the Jacobian would represent the
magniﬁcation of a volume, and so on.
Example 2.15
Transforming a multinomial distribution
This example illustrates a multivariate transformation in the discrete case
and, also, how a conditional distribution can be derived from the joint and
marginal probability distributions of the transformed variables. As seen in
Chapter 1, the multinomial distribution applies to a sampling model where
n independent draws are made from the same population. The outcome of
each draw is a realization into one of C mutually exclusive and exhaustive
classes, or categories of response. The categories can be ordered or un-
ordered. For example, in beef cattle breeding the degree of ease of calving
is often scored into four classes (C = 4), for example, “no diﬃculty”, “some
assistance is needed”, “mechanical pull”, or “caesarean section required”,
so the categories are ordered. On the other hand, unordered categories ap-
pear, for example, in genetic analyses of leg deformities in chickens. Here
the possible classes cannot be ordered in a meaningful way.
Let ni be the number of observations falling into the ith class, and let
pi be the probability that an individual observation falls in the ith class,
for i = 1, 2, 3. Then n = n1 + n2 + n3, and the joint probability function of
(n1, n2) is (the dependence on parameters p1, p2, and n is omitted)
p (n1, n2) =
n!
n1! n2! (n −n1 −n2)!pn1
1 pn2
2 (1 −p1 −p2)n−n1−n2 .
(2.40)
Suppose that one needs to ﬁnd the conditional probability distribution of
n1, given n1 + n2. That is, the conditional probability function is
p (n1|n1 + n2) = p (n1, n1 + n2)
p (n1 + n2)
.
(2.41)

2.3 Functions of Several Random Variables
99
To simplify the notation, let (n1, n2) = (X, Y ) and (n1, n1 + n2) = (U, V ) ,
and omit the parameters (p1, p2) as arguments of p (·). The probability
function of (n1, n2) = (X, Y ) is then expressible as
p (X, Y ) =
n!
X! Y ! (n −X −Y )!pX
1 pY
2 (1 −p1 −p2)n−X−Y .
(2.42)
To derive the numerator in (2.41), that is, p (n1, n1+n2) = p (U, V ), note
that the transformation (X, Y ) →(U, V ) can be written as

U
V

= f (X, Y ) =

1
0
1
1
 
X
Y

=

X
X + Y

(2.43)
with inverse transformation

X
Y

= f −1 (U, V ) =

1
0
−1
1
 
U
V

=

U
V −U

.
Therefore, from (2.34),
pU,V (U, V ) =
n!
U! (V −U)! (n −V )!pU
1 pV −U
2
(1 −p1 −p2)n−V .
(2.44)
To obtain p (n1|n1 + n2), (2.44) must be divided by p (V ). Now the random
variable V follows the binomial distribution
V ∼Bi (p1 + p2, n) .
This is so because the three classes can be regrouped into two “wider” cat-
egories, one where the counts (n1 + n2) are observed to fall, and the other
involving the third original category with counts n3. In view of the inde-
pendence of the draws, it follows that (n1 + n2) is binomially distributed.
Dividing pU,V (U, V ) in (2.44) by the marginal probability function pV (V )
we obtain
p (n1|n1 + n2) = p (U|V )
= p (U, V )
p (V )
=
V !
U! (V −U)!
pU
1 pV −U
2
(p1 + p2)V
= (n1 + n2)!
n1! n2!
pn1
1 pn2
2
(p1 + p2)n1+n2
= (n1 + n2)!
n1! n2!

p1
p1 + p2
n1 
p2
p1 + p2
n2
.
This implies that
[n1|n1 + n2] ∼Bi

p1
p1 + p2
, n1 + n2

.

100
2. Functions of Random Variables
Hence, the conditional distribution [n1|n1 + n2] has mean,
E (n1|n1 + n2) = (n1 + n2)
p1
p1 + p2
and variance
V ar (n1|n1 + n2) = (n1 + n2)
p1
p1 + p2
p2
p1 + p2
.
■
Example 2.16
Distribution of the ratio between two independent ran-
dom variables
Suppose that two random variables are i.i.d., with densities equal to p (xi) =
2xi, for i = 1, 2, with the sample space being the set of all points contained
in the interval (0, 1). Their joint p.d.f. is then
p (x1, x2) =

4x1x2,
for 0 < x1 < 1 and 0 < x2 < 1,
0,
otherwise.
(2.45)
We wish to ﬁnd the p.d.f. of the ratio Y1 = X1/X2. This is a situation
where an auxiliary variable is needed in order to make the transformation
one-to-one. To ﬁnd the p.d.f. of the ratio, the auxiliary variable is integrated
out from the joint density of all transformed variables. Let the auxiliary
variable be Y2 = X2. Then
Y1 = f1 (X1, X2) = X1/X2,
Y2 = f2 (X1, X2) = f2 (X2) = X2.
The inverse transformation is
X1 = f −1
1
(Y1, Y2) = Y1Y2,
X2 = f −1
2
(Y1, Y2) = f −1
2
(Y2) = Y2.
In view of the support of p (x1, x2), to ﬁnd the sample space of the joint
distribution [Y1, Y2] , observe that
0 < y1 < ∞,
0 < y2 < 1.
Now, from
0 < y1y2 < 1,
the following relationship also holds
0 < y2 < 1
y1
.

2.3 Functions of Several Random Variables
101
The Jacobian of the transformation is
J = det


∂f −1
1
(y1, y2)
∂y1
∂f −1
1
(y1, y2)
∂y2
∂f −1
2
(y1, y2)
∂y1
∂f −1
2
(y1, y2)
∂y2


= det


∂(y1y2)
∂y1
∂(y1y2)
∂y2
∂y2
∂y1
∂y2
∂y2


= det

y2
y1
0
1

= y2.
The p.d.f. of the vector Y = [Y1, Y2]′ is obtained as follows: in the density
p (x1, x2), replace x1 by y1y2, x2 by y2, and then multiply the result by the
absolute value of J. This leads to
p (y1, y2) =

4y1y3
2,
0 < y1 < ∞, 0 < y2 < 1, 0 < y1y2 < 1,
0,
otherwise.
(2.46)
We now check whether or not this is a proper p.d.f.: the integral of p (y1, y2)
over the sampling space induced by the transformation must be ﬁnite. We
then have
 
p (y1, y2) dy1 dy2
=
 1
y2=0
 1
y1=0
p (y1, y2) dy1dy2 +
 1/y1
y2=0
 ∞
y1=1
p (y1, y2) dy1 dy2
=
 1
y1=0
4y1

y4
2
4
""""
1
0

dy1 +
 ∞
y1=1
4y1

y4
2
4
""""
1
y1
0

dy1
= 1
2 + 1
2 = 1.
Thus, propriety is established. The marginal p.d.f. of Y1 = X1/X2 is ob-
tained by integrating the joint density with respect to Y2, yielding
p (y1) =











 1
y2=0
p (y1, y2) dy2 = 4y1

y4
2
4
""""
1
0

= y1, 0 < y1 < 1,
 1
y2=0
p (y1, y2) dy2 = 4y1

y4
2
4
""""
1
y1
0

= 1
y3
1
, 1 < y1 < ∞.
Consider now the calculation of
Pr

X1 < 1
2, X2 < 0.7

=
 0.7
x2=0

1
2
x1=0
4x1x2 dx1 dx2
= 0.1225.

102
2. Functions of Random Variables
In view of the relationship between (X1, X2) and (Y1, Y2) this joint proba-
bility can be written in terms of (Y1, Y2) as
Pr

Y1Y2 < 1
2, Y2 < 0.7

= Pr

Y1 <
1
2Y2
, Y2 < 0.7

=
 0.7
y2=0

1
2y2
y1=0
4y1y3
2 dy1 dy2
= 0.1225,
corroborating (2.29).
■
Example 2.17
Parameterization of a variance components model
Consider a Gaussian linear mixed eﬀects model for animal breeding data,
and let this model have two sets of random eﬀects with variances σ2
a and
σ2
e, respectively, both unknown. For example, σ2
a could be the additive
genetic variance, and σ2
e the environmental variance for a certain trait.
Suppose that in a Bayesian setting (where unknown parameters are treated
as random variables), the two variance components (both strictly positive)
are assigned a proper joint prior density equal to
p(σ2
a, σ2
e) = p(σ2
a)p(σ2
e),
σ2
a ≥0, σ2
e > 0,
so that there is independence between the two components, a priori. Sup-
pose, further, that there is a family structure, so that observations can be
clustered into families of half-sibs, such that the observations within a clus-
ter are equicorrelated, whereas those in diﬀerent clusters are independent.
Let the variance between half-sib clusters be σ2
s and the variance within
clusters be σ2
w. Conceivably, one may wish to parameterize the model in
terms of random eﬀects having variances σ2
s and σ2
w. From a classical per-
spective, the two models are said to be equivalent (Henderson, 1984) if the
same likelihood (see Chapter 3 for a formal deﬁnition of the concept) is
conferred by the data to values of the same parameter under each of the
models. For this equivalence to hold, a relationship is needed to establish a
link between the two sets of parameters, and this comes from genetic theory.
Under additive inheritance (Fisher, 1918) in a randomly mated population
in linkage equilibrium, one has:
1) σ2
s = 1
4σ2
a, and
2) σ2
w = 3
4σ2
a + σ2
e.
This is a one-to-one linear transformation expressible in matrix notation
as

σ2
s
σ2
w

=

1
4
0
3
4
1
 

σ2
a
σ2
e

.
Since heritability h2 = σ2
a/(σ2
a +σ2
e) necessarily takes values between 0 and
1, it follows that σ2
s/σ2
w must take a value between 0 and 1/3. In order to

2.3 Functions of Several Random Variables
103
arrive at the probabilistic beliefs in the parameterization in terms of half-
sib clusters, one must eﬀect a change of variables in the prior distribution.
It turns out that the joint prior density of σ2
s and σ2
w is:
p

σ2
s, σ2
w

= 4p

4σ2
s

p

σ2
w −3σ2
s

,
σ2
w > 0, 0 ≤σ2
s ≤σ2
w
3 .
(2.47)
This shows that σ2
s and σ2
w are not independent a priori. It follows that a
Bayesian that takes σ2
a and σ2
e as independent a priori, has diﬀerent prior
beliefs than one that assigns independent prior distributions to σ2
s and σ2
w.
Even if the likelihood in the two models confers the same strength to values
of the same parameter, the two models would not be equivalent, at least
in the Bayesian sense, unless the joint prior density in the second param-
eterization has the form given in (2.47). This illustrates that constructing
models that are probabilistically consistent (or, in a Bayesian context, that
reﬂect beliefs in a coherent manner) require careful consideration not only
of the statistics, but of the subject matter of the problem as well.
■
Example 2.18
Conditioning on a function of random variables
Suppose there are two continuously distributed random vectors x and y
having joint p.d.f. p (x, y). Here the distinction between a random variable
and its realized value is omitted. Let z = f (y) be a vector valued function
of y, and let y = f −1 (z) be the inverse transformation. We wish to ﬁnd
the p.d.f. of the conditional distribution of x given z. Assume that each of
the elements of any of the vectors can take any value in the real line. The
joint density of x and z is
pXZ (x, z) = pXY

x, f −1 (z)

|J|
where the Jacobian is
J = det


∂x
∂x′
∂x
∂z′
∂f −1 (z)
∂x′
∂f −1 (z)
∂z′

= det


I
0
0
∂f −1 (z)
∂z′


= det
∂f −1 (z)
∂z′

.
Therefore
pXZ (x, z) = p

x, f −1 (z)
 """"det
∂f −1 (z)
∂z′
"""" .
Recall that
pZ (z) = pY

f −1 (z)
 """"det
∂f −1 (z)
∂z′
"""" .

104
2. Functions of Random Variables
To obtain p (x|z), use is made of the fact that
pX|Z (x|z) = p (x, z)
p (z)
=
pXY

x, f −1 (z)
 """det

∂f −1(z)
∂z′
"""
pY (f −1 (z))
"""det

∂f −1(z)
∂z′
"""
= pX|Y

x|f −1 (z)

.
(2.48)
This is an important result: it shows that if z is a one-to-one transformation
of y, the conditional p.d.f. (or distribution) of x, given z, is the same as the
conditional p.d.f. of x, given y. Arguing intuitively, this implies that the
same inferences about parameters of this conditional distribution are drawn
irrespective of whether one bases inferences on x|y or on x|z. Suppose, in a
Bayesian context, that x is unknown and that y is an observed data vector.
Often, all information about x will be contained in a vector z having a lower
dimension than y (loosely speaking, one can refer to this as a principle of
suﬃciency, but see Chapter 3); in this case, the posterior distribution of x
based on z will lead to the same inferences about x than the corresponding
posterior distribution based on y, but with a considerable reduction in
dimensionality. For example, in Bayesian inferences about the coeﬃcient
of correlation of a bivariate normal distribution from which n pairs have
been sampled at random (so the data y are in a vector of order 2n×1), the
posterior distribution of the correlation parameter can be shown to depend
on the data only through its ML estimator, which is a scalar variable (Box
and Tiao, 1973; Bayarri, 1981; Bernardo and Smith, 1994).
To illustrate (2.48), suppose that phenotypic values for a certain trait y
are recorded on members of nuclear families consisting of an oﬀspring (y0),
a father (yf), and a mother (ym). Assume that their joint p.d.f. is the
trivariate normal process


y0
yf
ym

∼N






µ
µ
µ

,


σ2
σ2
a/2
σ2
a/2
σ2
a/2
σ2
0
σ2
a/2
0
σ2





,
where σ2 is the variance of the phenotypic values, and σ2
a is the additive
genetic variance for this trait in the population from which individuals are
sampled. The independence between parental records reﬂects the often-
made assumption that the two parents are not genetically related or mated
assortatively. The distribution [y0|yf, ym] is also normal, with expected
value,
E (y0|yf, ym) = µ + 1
2h2 (yf −µ) + 1
2h2 (ym −µ)

2.3 Functions of Several Random Variables
105
and variance
V ar (y0|yf, ym) = σ2

1 −h4
2

,
where h2 = σ2
a/σ2. Consider now the random variables
ˆaf = h2 (yf −µ)
and
ˆam = h2 (ym −µ) .
These are the means of the conditional distribution of the father’s and
mother’s additive genetic values, respectively, given their phenotypic values.
The inverse transformations are yf = h−2ˆaf + µ and ym = h−2ˆam + µ,
respectively. Suppose that one wishes to derive the distribution [y0|ˆaf, ˆam].
According to (2.48), this distribution must be the same as [y0|yf, ym], where
we replace, in the latter, yf by h−2ˆaf + µ and ym by h−2ˆam + µ. That is,
[y0|ˆaf, ˆam] is normal, with expected value:
E (y0|ˆaf, ˆam) = µ + 1
2ˆaf + 1
2ˆam
and variance as before.
■
Example 2.19
The Box–Muller transformation
The following technique was devised for generating standard normal ran-
dom variables (Box and Muller, 1958). Let U1 and U2 be independent ran-
dom variables having the same uniform distribution Un (0, 1) and construct
the transformation
X = f1 (U1, U2) = (−2 ln U1)
1
2 cos (2πU2) ,
Y = f2 (U1, U2) = (−2 ln U1)
1
2 sin (2πU2) .
It will be shown that X and Y are i.i.d. N (0, 1). The proof below makes
use of the following trigonometric relationships:
sin2 (x) + cos2 (x) = 1,
tan (x) = sin (x)
cos (x),
tan−1 (tan x) = x,
d
dx tan−1 (u) =
1
1 + u2
du
dx.
Using these relationships, the inverse transformations are obtained as
U1 = f −1
1
(X, Y ) = exp

−1
2

X2 + Y 2
,
U2 = f −1
2
(X, Y ) = (2π)−1 tan−1

 Y
X

.

106
2. Functions of Random Variables
Then, the joint p.d.f. of (X, Y ) is
pX,Y (x, y) = pU1U2

f −1
1
(X, Y ) , f −1
2
(X, Y )

|J| = |J| .
(2.49)
The Jacobian is given by
J = det


∂f −1
1
(X, Y )
∂X
∂f −1
1
(X, Y )
∂Y
∂f −1
2
(X, Y )
∂X
∂f −1
2
(X, Y )
∂Y


= det


−exp

−1
2

X2 + Y 2
X
−exp

−1
2

X2 + Y 2
Y
−Y

2π

1 + Y 2
X2

X2−1

2π

1 + Y 2
X2

X
−1


= −(2π)−1 exp

−1
2

X2 + Y 2
.
(2.50)
Using the absolute value of this in (2.49) gives
pX,Y (x, y) =
√
2π
−1
exp

−X2
2
 √
2π
−1
exp

−Y 2
2

which can be recognized as the p.d.f. of two independent standard normal
variables.
■
Example 2.20
Implied distribution of beliefs about heritability
Suppose a quantitative geneticist wishes to undertake a Bayesian analysis of
the heritability

h2
of a certain trait. Let there be two sources of variance
in the population, genetic and environmental, and let the values of the cor-
responding variance components be unknown. As pointed out earlier (and
see Chapter 5), the Bayesian approach requires the speciﬁcation of an un-
certainty distribution, the prior distribution, which is supposed to describe
beliefs about heritability before the data are observed. Let the positive ran-
dom variables Y and X denote the genetic and environmental components
of variance, respectively. Suppose that this geneticist uses proper uniform
distributions to represent vague prior knowledge about components of vari-
ance. In addition, the geneticist assumes that prior beliefs about X are
independent of those about Y , and takes as prior densities
p (x) = 1
a,
for 0 < x < a,
p (y) = 1
b ,
for 0 < y < b,
where a and b are the maximum values that X and Y , respectively, are
allowed to take. These upper bounds are perhaps established on the basis

2.3 Functions of Several Random Variables
107
of some knowledge of the population in question, or on mechanistic con-
siderations. Since X and Y are assumed to be independent, the joint prior
density of X and Y is simply the product of the two densities above. The
problem is to derive the induced prior p.d.f. of the heritability, denoted
here W and deﬁned as the ratio
W =
Y
Y + X .
It will be shown that uniform prior distributions for each of the two variance
components lead to discontinuous and perhaps sharp prior distributions of
W, depending on the values of a and b adopted. In order to make a one-
to-one transformation, deﬁne the auxiliary random variable U = Y . Thus,
the transformation can be written as
W = f1 (X, Y ) =
Y
X + Y ,
U = f2 (X, Y ) = f2 (Y ) = Y,
and the inverse transformation is
X = f −1
1
(U, W) = U (1 −W)
W
,
Y = f −1
2
(U, W) = f −1
2
(U) = U.
Then the joint p.d.f. of U and W is
p (u, w) = p

f −1
1
(u, w) , f −1
2
(u)

|J| ,
where
J = det


∂f −1
1
(u, w)
∂u
∂f −1
1
(u, w)
∂w
∂f −1
2
(u)
∂u
∂f −1
2
(u)
∂w


= det


1−w
w
−u
w2
1
0

= u
w2 .
Therefore,
p (u, w) =
u
abw2 .
(2.51)
The next step involves ﬁnding the support of this joint p.d.f. The relation-
ship U = Y implies
(i) 0 < u < b
and the relationship W = Y/ (Y + X) implies
(ii) 0 < w < 1.

108
2. Functions of Random Variables
Further, the fact that X = U (1 −W) /W implies
(iiia) 0 < u (1 −w)
w
< a,
(iiib) 0 < u (1 −w) < wa,
(iiic) 0 < u <
wa
1 −w.
From (i) and (iiic) the following inequalities must be satisﬁed:
(u < b)
and

u <
wa
1 −w

.
Since a and b are given, and u must be smaller than b, one must determine
the values of w such that wa/ (1 −w) is smaller than b. We have
wa
1 −w < b =⇒wa < b (1 −w) =⇒w <
b
b + a.
From all these relationships, the sample space of the joint distribution
[U, W], with density in (2.51), is
0 < w < 1,
0 < u <
wa
1 −w,
for 0 < w <
b
b + a,
0 < u < b,
for
b
b + a < w < 1.
Hence, p (u, w) is discontinuous at b/ (b + a). Verifying that p (u, w) in
(2.51) is a proper p.d.f. requires integrating the density over the range
of all possible values of U and W:
 
p (u, w) du dw
= 1
ab
b
a+b

w=0
wa
1−w

u=0
u
w2 du dw + 1
ab
1

w=
b
a+b
b

u=0
u
w2 du dw
= 1
ab
ab
2 + ab
2

= 1,
so propriety is established. The marginal density of heritability (W) is
p (w) =















wa
1−w

u=0
u
abw2 du,
for 0 < w <
b
a + b,
b

u=0
u
abw2 du,
for
b
a + b ≤w < 1.

2.3 Functions of Several Random Variables
109
0.2
0.4
0.6
0.8
1.0
0.5
1.0
1.5
2.0
2.5
a=1.0
b=0.25
a=1.0
b=0.5
a=1.0
b=1.0
a=1.0
b=4.0
a=1.0
b=2.0
h
2
p(h
2)
FIGURE 2.1. Prior distributions of heritability

h2 = W

implied by assuming
uniform prior distributions of the variance components, for diﬀerent bounds a
and b.
This yields
p (w) =



a
2b(1−w)2 ,
for 0 < w <
b
a+b,
b
2aw2 ,
for
b
a+b ≤w < 1.
(2.52)
Figure 2.1 shows the implied prior distribution of heritability (W), given
the assumed prior distributions for the variance components, for diﬀerent
values of a and b. Five diﬀerent prior densities are depicted. It can be
seen that independent, proper, uniform distributions, for each of the two
variance components, induce a spiked prior density for heritability, with a
degree of sharpness or asymmetry that depends on the bounds a and b. ■
Example 2.21
Revisited implied distribution of beliefs about heritability
Suppose now that the additive genetic (Y ) and the environmental (X) com-
ponents of variance follow independent inverse gamma (or scaled inverse
chi-square) distributions, a priori. It follows immediately that their recip-
rocals G = 1/Y and T = 1/X possess independent gamma distributions.
Note that heritability can be written as
W =
Y
X + Y =
T
T + G.
Once again, the geneticist wishes to derive the implied prior distribution
of W. The prior gamma densities of T and G are
p (t) =
bat
t
Γ (at)tat−1 exp (−btt) ,
t, bt, at > 0,
p (g) =
bag
g
Γ (ag)gag−1 exp (−bgg) ,
g, bg, ag > 0,

110
2. Functions of Random Variables
where bt, at, bg, and ag are parameters of the appropriate distributions. In
view of the independence between T and G, their joint density is
p (t, g) = bat
t bag
g tat−1gag−1
Γ (at) Γ (ag)
exp (−btt −bgg) .
In order to arrive at the marginal distribution of W, make the one-to-one
transformation
W = f1 (T, G) =
T
T + G,
0 < W < 1,
U = f2 (T, G) = T + G,
U > 0,
where U is an auxiliary random variable. The inverse transformation is
T = f −1
1
(U, W) = UW,
G = f −1
2
(U, W) = U (1 −W) .
The joint p.d.f. of U and W is
pUW (u, w) = pT G

f −1
1
(U, W) , f −1
2
(U, W)

|J| ,
where
J = det


∂f −1
1
(u, w)
∂u
∂f −1
1
(u, w)
∂w
∂f −1
2
(u, w)
∂u
∂f −1
2
(u, w)
∂w


= det

w
u
1 −w
−u

= −u.
Therefore
pUW (u, w) =
bat
t bag
g
Γ (ae) Γ (ag)u (uw)at−1 [u (1 −w)]
ag−1
× exp [−btuw −bgu (1 −w)] .
To arrive at the desired marginal distribution, one must integrate the pre-
ceding joint density with respect to U:
p (w) =
 ∞
0
pUW (u, w) du
=
bat
t bag
g
Γ (at) Γ (ag)wat−1 (1 −w)
ag−1
×
 ∞
0
uat+ag−1 exp [−(bg + btw −bgw) u] du.

2.3 Functions of Several Random Variables
111
Now the integrand is the kernel of the density of a gamma distribution, so
 ∞
0
uat+ag−1 exp [−(bg + btw −bgw) u] du
=

(bg + btw −bgw)at+ag
Γ (at + ag)
−1
.
Hence, the prior density of heritability is
p (w) = Γ (at + ag) bat
t bag
g
Γ (at) Γ (ag)
wat−1 (1 −w)
ag−1
(bg + btw −bgw)−at−ag .
(2.53)
The corresponding distribution does not have an easily recognizable form.
Consider now the special case where bg = bt; then (2.53) reduces to
p (w) = Γ (at + ag) wat−1 (1 −w)ag−1
Γ (at) Γ (ag)
.
(2.54)
This is the density of a beta distribution with parameters at, ag. In the hy-
pothetical Bayesian analysis of variance components, the parameters of the
gamma distribution assigned to the reciprocal of the variance components,
would be equal to those of the inverse gamma (or scaled inverse chi-square)
process assigned to the variance components. A typical speciﬁcation would
be at = νt/2, ag = νg/2, bt = νtSt/2, and bg = νgSg/2. Here, ν and S are
parameters of the scaled inverse chi-square distributions associated with
the variance components. The implied prior density of heritability would
be the beta form given by (2.54), provided that νtSt = νgSg; otherwise,
the implied prior density of heritability would be as in (2.53).
■
2.3.1
Linear Transformations
A special case is when the transformation is linear, as in (2.43). Let x be a
random vector possessing p.d.f. p (x), and let y = f (x) = Ax be a one-to-
one linear transformation, so that the matrix of constants A is nonsingular.
The inverse transformation is, therefore
x = f −1 (y) = A−1y.
From (2.31), the p.d.f. of Y is given by
pY (y) = pX

f −1 (y)

|J|
= pX

f −1 (y)
 """"det

∂f −1 (y)
∂y′
"""" .
Now, the matrix of derivatives is
∂f −1 (y)
∂y′
= ∂A−1y
∂y′
= A−1.

112
2. Functions of Random Variables
Hence
pY (y) = pX

f −1 (y)
 ""det

A−1""
= pX

f −1 (y)
 """"
1
det (A)
"""" .
(2.55)
Example 2.22
Samples from the multivariate normal distribution
Imagine one wishes to obtain realizations from y ∼N(m, V). The starting
point consists of drawing a vector of independent standard normal deviates,
x, from a N(0, I) distribution having the same order (n, say) as y. Then
write x ∼N(0, I), so
p (x) =
1
(2π)
n
2 exp

−x′x
2

.
Because V is a nonsingular variance–covariance matrix and, therefore, pos-
itive deﬁnite, it can be expressed as V = L′L, where L′ is a nonsingular,
lower triangular matrix; this is called the Cholesky decomposition. Then
V−1 = L−1 (L′)−1 and, further, the following relationships can be estab-
lished
""V−1"" =
""L−1""2 ⇒
""L−1"" = |V|−1
2 .
Now, the linear transformation
y = f(x) = m + L′x
has the desired distribution, by being a linear combination of the normal
vector x. To verify this, note that the inverse transformation is
x = f −1 (y) = (L′)−1 (y −m)
with the absolute value of the Jacobian of the transformation being
""""det

∂f −1 (y)
∂y′
"""" =
"""det (L′)−1""" =
"""det (V)−1
2
""" .
In the usual notation employed for the Gaussian model, we write
det (V)−1
2 = |V|−1
2 .
Then, applying (2.55), and recalling that the initial distribution is that of
n independent standard normal variables, gives, as p.d.f. of Y,
p (y) = (2π)−n
2 |V|−1
2 exp

−1
2 (y −m)′ L−1 (L′)−1 (y −m)

which proves the result.
■

2.3 Functions of Several Random Variables
113
Example 2.23
Bivariate normal distribution with null means
Consider the centered (null means) bivariate normal density function:
p (x, y) = C exp

−
1
2 (1 −ρ2)

 x2
σ2
X
−2ρxy
σXσY
+ y2
σ2
Y

,
(2.56)
where σ2
X = V ar(X), σ2
Y = V ar(Y ), ρ is the coeﬃcient of correlation
between X and Y, and C is the integration constant. Hereinafter, we will
retain only the kernel of this density. If ρ = 0, the joint density can be
factorized as
p (x, y) = p (x) p (y) ,
where
p (x) ∝exp

−x2
2σ2
X

,
p (y) ∝exp

−y2
2σ2
Y

.
This veriﬁes that a null value of the correlation is a suﬃcient condition for
independence in the bivariate normal distribution (as discussed in Chapter
1, a suﬃcient condition for mutual independence in the multivariate distri-
bution is that the variance–covariance matrix has a diagonal form).
Returning to the general case of nonnull correlation, now let U and V be
random variables arising through the linear transformation of X and Y

U
V

=


X
σx −Y
σy
X
σx + Y
σy


=
 σ−1
x
−σ−1
y
σ−1
x
σ−1
y
 
X
Y

= A

X
Y

,
where A is the coeﬃcient matrix preceding the [X, Y ]′ vector. Hence, U is
a contrast between the standardized X and Y variables, whereas V is their
sum. Since U and V are linear combinations of normal random variables,
they must also follow a joint bivariate normal distribution, and are also
normal at the margin. It can be veriﬁed that the covariance between U and
V is equal to 0, so these two variates are independent. We proceed to verify
this formally. The inverse transformation is
 X
Y

=
 σ−1
x
−σ−1
y
σ−1
x
σ−1
y
−1  U
V

= 1
2

σx
σx
−σy
σy
  U
V

.
Therefore,
X = f −1
1
(U, V ) = σx (U + V )
2
,
Y = f −1
2
(U, V ) = σy (V −U)
2
.

114
2. Functions of Random Variables
The absolute value of the Jacobian of the transformation is
|J| =
""""
1
det A
"""" = σxσy
2
.
Using (2.55) the joint p.d.f. of U and V is
p (u, v) = p

f −1
1
(u, v) , f −1
2
(u, v)
 σxσy
2
∝exp
#
−
1
2 (1 −ρ2)

u2 + v2 + 2uv

4
−2ρ

v2 −u2
4
+

u2 + v2 −2uv

4
$
∝exp

−
1
4 (1 −ρ2)

u2 + v2 −ρ

v2 −u2
which can be factorized as
p (u, v) = C exp

−
1
4 (1 −ρ2)u2 (1 + ρ)

× exp

−
1
4 (1 −ρ2)v2 (1 −ρ)

.
Hence,
p (u, v) ∝p (u) p (v)
which shows that U and V are independent.
■
2.3.2
Approximating the Mean and Covariance Matrix
Let x be a random vector having mean m and covariance matrix V, and
suppose one is interested in a scalar valued function, Y = f(x), of the
vector x. Assume that this function admits at least up to second-order
partial diﬀerentiability with respect to x. Put
 ∂
∂x′ , f (x)
""""
x=m

= b′
and

∂2
∂x∂x′ f (x)
""""
x=m

= B.
Expanding f(x) in a second-order Taylor series about m gives
f (x) ∼= f (m) + b′ (x −m)
+1
2 (x −m)′ B (x −m) .
(2.57)

2.3 Functions of Several Random Variables
115
Taking expectations and variances, one obtains the following approxima-
tions to E(Y ) and V ar (Y ):
(1) First order:
E (Y ) = E [f (x)] ∼= f [m] ,
(2.58)
V ar (Y ) = V ar [f (x)] ∼= b′Vb.
(2.59)
(2) Second order:
E (Y ) = E [f (x)] ∼= f (m) + 1
2 tr [BV] ,
(2.60)
V ar (Y ) = V ar [f (x)] ∼= b′Vb
+1
4 V ar [tr (SxB)] + Cov

b′ (x −m) , (x −m)′ B (x −m)

,
(2.61)
where
Sx = (x −m) (x −m)′
is a matrix. The feasibility of evaluating the variance of the quadratic form
in (2.61) depends on the distribution of the vector x (Searle, 1971; Rao,
1973). For example, if its distribution is normal, the covariance term van-
ishes because the third moments from the mean are null.
Example 2.24
Approximate mean and variance of a ratio
Let x′ = [x1, x2], E (x)′ = m′ = [m1, m2], and
V ar (x) = V =

σ2
1
σ12
σ12
σ2
2

.
Consider the function
Y = f (x) = X1
X2
.
Then, from (2.58), the linear approximation gives as mean
E (Y ) = E

X1
X2

∼= m1
m2
.
The vector of ﬁrst derivatives is
b = ∂f (x)
∂x
""""
x=m
=


∂Y
∂X1
∂Y
∂X2


X1=m1
X2=m2
=


1
m2
−m1
m2
2

.

116
2. Functions of Random Variables
Then, from (2.59)

∂
∂X′ f (X)
""""
x=m

V

∂
∂Xf (X)
""""
X=m

=

1
m2
−m1
m2
2
 
σ2
1
σ12
σ12
σ2
2
 

1
m2
−m1
m2
2


and, ﬁnally,
V ar

X1
X2

∼=

m1
m2
2 
 σ2
1
m2
1
+ σ2
2
m2
2
−2 σ12
m1m2

=

m1
m2
2 
C2
1 + C2
2 −2ρC1C2

,
where ρ is the coeﬃcient of correlation and C1 and C2 are the corresponding
coeﬃcients of variation.
■
At this point, we have completed a review of the basic elements of dis-
tribution theory needed by geneticists to begin the study of parametric
methods of inference. In the next chapters, a discussion of two important
methods for drawing inferences is presented. Chapters 3 and 4 give a de-
scription of the principles of likelihood-based inference, whereas Chapters
5, 6, 7, and 8 present an overview of the Bayesian approach to statistical
analysis.

Part II
Methods of Inference
117

This page intentionally left blank

3
An Introduction to
Likelihood Inference
3.1
Introduction
A well-established problem in statistics, especially in what today is called
the classical approach, is the estimation of a single parameter or an en-
semble of parameters from a set of observations. Data are used to make
statements about a statistical model proposed to describe aspects of the
state of nature. This model is characterized in terms of parameters that
have a “true” value. The description of uncertainty, typically, is in terms
of a distribution that assigns probabilities or densities to the values that
all random variables in the model, including the observations, can take. In
general, the values of the parameters of this distribution are unknown and
must be inferred from observable quantities or data. In genetics, the observ-
ables can consist of phenotypic measurements for quantitative or discrete
traits, and/or information on molecular polymorphisms.
In this chapter, an important method of statistical inference called es-
timation by maximum likelihood (ML) is discussed. There is an extensive
literature on this method, originally due to Fisher (1922) and now ﬁrmly
entrenched in statistics. A historical account can be found in Edwards
(1974). A readable introduction is given by King (1989), and a discussion
of related concepts, from the point of view of animal breeding, is in Blasco
(2001). Edwards (1992), one of the strong adherents to the use of likelihood
inference in statistics, starts his book as follows:
“Likelihood is the central concept in statistical inference.
Not only does it lead to inferential techniques in its own right,

120
3. An Introduction to Likelihood Inference
but it is as fundamental to the repeated-sampling theories of
estimation advanced by the “classical” statisticians as it is to
the probabilistic reasoning advanced by the Bayesian. It is the
key to an understanding of ﬁducial probability, and the con-
cept without which no serious discussion of the philosophy of
statistical inference can begin.”
Although likelihood is indeed a central concept in statistical inference,
it is not free from controversy. In fact, all methods of inference are the
subject of some form of controversy. Mal´ecot (1947) gives an interesting
critique of approaches based on likelihood, and Bernardo and Smith (1994)
discuss it from a Bayesian perspective, another approach to inference in
which parameters are viewed as random variables, with the randomness
stemming from subjective uncertainty. It will be shown in Chapter 5 how
likelihood enters into the Bayesian paradigm.
This chapter is organized as follows. The likelihood function and the
ML estimator are presented, including a measure of the information about
the parameters contained in the data. First-order asymptotic theory of
likelihood inference is covered subsequently in some detail. This gives the
basis for the appeal of the method, at least from a frequentist point of view.
The chapter ends with a discussion of the functional invariance of the ML
estimator.
3.2
The Likelihood Function
Assume that the observed data y (scalar, vector, or matrix) is the outcome
of a stochastic model (i.e., a random process), that can be characterized
in terms of a p.d.f. p (y|θ) indexed by a parameter(s) θ taking values in
the interior of a parameter space Ω. (Hereinafter, unless it is clear from
the context, we use boldface for the parameters, to allow for the possibility
that θ may be a vector). For example, in a bivariate normal distribution,
the parameter space of the correlation coeﬃcient includes all real numbers
from −1 to 1. This gives an illustration of a bounded parameter space. On
the other hand, in a regression model with p coeﬃcients, each of which can
take any value in the real line, the parameter space is the p-dimensional
hypervolume Rp. There are situations where the parameter space may be
constrained, due to restrictions imposed by the model, or due to mechanis-
tic considerations. It was seen previously that, in a random eﬀects model
where the total variation is partitioned into between and within half-sib
family components, purely additive genetic action imposes the constraint
that the variance within families must be at least three times as large
as the variance between families. Without this constraint, the statistical
model would not be consistent with the biological system it is supposed
to describe. Hence, taking these constraints into account is important in

3.2 The Likelihood Function
121
likelihood-based inference. This is because one is interested in ﬁnding the
maximum values the parameters can take inside their allowable space, given
the data, so constrained maximization techniques must be employed.
Given the probability model and the parameter θ, the joint probability
density (or distribution, if the random variable under study is discrete) of
the observations, p (y|θ), is a function of y. This describes the plausibility
of the diﬀerent values y can take in its sampling space, at a given value
of θ. The likelihood function or, just likelihood, is based on an “inversion”
of the preceding concept. By deﬁnition, the likelihood is any function of
θ that is proportional to p (y|θ) ; it is denoted as L(θ|y) or L(θ). Thus,
the likelihood is a mathematical function of the parameter for ﬁxed data,
whereas the p.d.f. is viewed as varying with y at ﬁxed values of θ. There-
fore, the likelihood is not a probability density or a probability function, so
the diﬀerent values θ takes in the likelihood cannot be interpreted in the
usual probabilistic sense. Further, because the true value of a parameter is
ﬁxed, one cannot apply the probability calculus to a likelihood function,
at least in principle. Blasco (2001) pointed out that Fisher proposed the
likelihood function as a rational measure of degree of belief but without
sharing the properties of probability. The adherents of the likelihood ap-
proach to inference view the entire likelihood as a complete description of
the information about θ contained in the data, given the model.
Now, by deﬁnition
L(θ|y) = k(y)p (y|θ) ∝p (y|θ) ,
(3.1)
where k(y) is a function that does not depend on θ, but may depend on
the data. For θ = θ∗, the value L(θ∗|y) is called the likelihood of θ∗. It
is apparent that a likelihood, by construction, must be positive, because
any density (or probability) is positive for any θ in the allowable space.
While a probability takes values between 0 and 1, a likelihood evaluated
at a given point has no speciﬁc meaning. On the other hand, it is mean-
ingful to compare the ratio of likelihoods from the same data. To be more
speciﬁc, consider a one-to-one transformation f (y) = z. For example, the
transformation could represent diﬀerent scales of measurement associated
with y and z, centimeters and meters respectively, say. Then the likelihood
function based on z is
L(θ|z) = L(θ|y)
""""
∂y
∂z
"""" .
Then for two possible values of the parameter, θ = θ∗and θ = θ∗∗, the
likelihood ratio is the relevant quantity to study
L(θ∗|y)
L(θ∗∗|y),

122
3. An Introduction to Likelihood Inference
as opposed to the diﬀerence
[L(θ∗|y) −L(θ∗∗|y)]
""""
∂y
∂z
"""" .
The latter is arbitrarily aﬀected by the particular transformation used, in
the above example, by the arbitrary scale of measurement.
The ratio of the likelihoods can be interpreted as a measure of support
brought up by the data set for one value of θ relative to the other. It
must be emphasized that likelihood values obtained from diﬀerent data
sets cannot be compared.
3.3
The Maximum Likelihood Estimator
Suppose for the moment that the random variable Y is discrete taking
values y with probabilities depending on a parameter θ:
Pr (Y = y|θ) = p (y|θ) .
(3.2)
We said that the likelihood of θ, L (θ|y), is proportional to (3.2), and the
value of θ that maximizes the likelihood L (θ|y) is the ML estimate of θ; it
is denoted by 5θ. The ML estimate 5θ can be viewed as the most likely value
of θ given the data, in the following sense. If θ1 and θ2 are two possible
values for θ, and if Pr (Y = y|θ1) > Pr (Y = y|θ2), then the probability of
observing what was actually observed, i.e., Y = y, is greater when the true
value of θ is θ1. Therefore it is more likely that θ = θ1 than θ = θ2. The
ML estimate provides the best explanation for observing the data point
y, under the probability model (3.2). This does not mean that 5θ is the
value of θ that maximizes the probability of observing the data y. This
role is played by the true value of the parameter θ, in the sense discussed
in Subsection 3.7.1.
The above interpretation can be adapted to the case where Y is continu-
ously distributed, provided we view p (y|θ) as the probability that Y takes
values in a small set containing y.
Often, rather than working with the likelihood it is more convenient to
work with the logarithm of the likelihood function. This log-likelihood is
denoted l(θ|y). The maximizer of the likelihood is also the maximizer of
the log-likelihood. The value of the ML estimate of θ must be inside the
parameter space and must be a global maximum, in the sense that any
other value of the parameter would produce a smaller likelihood.
Fisher (1922) suggested that it would be meaningful to rescale all likeli-
hood values relative to the maximum value it can take for a speciﬁc data
set. For example, if the maximizer of L(θ|y) is 5θ, one could rescale values
as
r(θ) = L(θ|y)
L(5θ|y)
,
(3.3)

3.3 The Maximum Likelihood Estimator
123
so that rescaled values are between 0 and 1.
Beyond providing a means of viewing uncertainty in terms of a relative
fall in likelihood, it turns out that the maximizer of the likelihood func-
tion plays an important role in statistical inference. If 5θ is used as a point
estimator of θ, conceptual repeated sampling over the distribution of y
generates a distribution of 5θ values, one corresponding to each realization
of y. In fact, the sampling distribution of 5θ has interesting coverage proba-
bilities in relation to the true value of θ. This distribution will be discussed
in a later section.
If the p.d.f. is everywhere continuous and there are no corner solutions,
the ML estimator, if it exists, can be found by diﬀerentiating the log-
likelihood with respect to θ, setting all partial derivatives equal to zero,
and solving the resulting equations for θ. It is a joint maximization with
respect to all elements in θ that must be achieved. If this parameter has
p elements, then there are p simultaneous equations to be solved, one for
each of its components. The vector of ﬁrst-order partial derivatives of the
log-likelihood with respect to each of the elements of θ is called the gradient
or score, and is often denoted as S (θ|y) or as l′ (θ|y).
There are some potential diﬃculties inherent to inferences based on like-
lihood. First, there is the issue of constructing the likelihood. For example,
consider generalized mixed eﬀects linear models (Nelder and Wedderburn,
1972; McCullagh and Nelder, 1989). These are models that can include
several sets of random eﬀects (u) and where the response variable (y) may
not be normal. The starting point in building such models is the speciﬁ-
cation of the marginal distribution of u, followed by an assumption about
the form of the conditional distribution of y given u. This produces the
joint distribution [y, u|θ] . In order to form the likelihood as in (3.1), one
must obtain the marginal p.d.f. of the data. This requires evaluating the
integral:
L(θ|y) ∝p (y|θ) =

p (y|u, θ) p (u|θ) du,
(3.4)
where p (y|u, θ) is the density of the conditional distribution of the data
given the random eﬀects, and p (u|θ) is the marginal density of the random
eﬀects. The integration in (3.4) seldom leads to a likelihood function that is
expressible in closed form. This creates diﬃculties in evaluating likelihoods
and in ﬁnding the ML estimator of θ.
A second issue is that it is often not straightforward to locate the global
maximum of a likelihood, especially in high-dimensional models (those hav-
ing many parameters). This is particularly troublesome in animal breeding
because of the potentially large number of parameters a model can have. For
example, consider a 17-variate analysis of type traits in dairy cattle, such
as scores on body condition and feet and legs, and suppose that the same
Gaussian linear model with two random factors is entertained for each trait.
These factors could be the breeding value of a cow plus a random residual.

124
3. An Introduction to Likelihood Inference
Here there are two covariance matrices, each having 153 potentially distinct
elements to be estimated, unless these matrices are structured as a function
of a smaller number of parameters. If, additionally, the number of location
parameters or ﬁxed eﬀects is f, the order of θ would be f + 306. Typi-
cally, the order of f is in the thousands when ﬁeld records are employed in
the analysis. In this situation, encountered often in practice, it would be
extremely diﬃcult to verify that a global maximum has been found. The
zeros of the ﬁrst derivatives only locate extreme points in the interior of
the domain of a function. If extrema occur on the boundaries or corners,
the ﬁrst derivative may not be zero at that point. First derivatives can also
be null at local minima or maxima, or at inﬂection points. To ensure that
a maximum (local or global) has been found, the second derivative of the
log-likelihood with respect to the parameter must be negative, and this is
relatively easy to verify in a single parameter model. If θ is a vector, the
conditions for a maximum are:
(i) The vector of partial derivatives of l(θ|y) with respect to θ must be
null.
(ii) The symmetric matrix of mixed second-order partial derivatives of
l(θ|y) with respect to all parameters:
∂2l(θ|y)
∂θ ∂θ′
(3.5)
must be negative deﬁnite. A symmetric matrix is negative deﬁnite if all its
eigenvalues are negative. In the dairy cattle example given above, f + 306
eigenvalues would need to be computed to check this. Further, even if all
eigenroots of (3.5) are negative, there is no assurance that the stationary
point would be a global maximum.
The form of a likelihood function depends on the sampling model hypoth-
esized for the observations. Maximum likelihood is a parametric method, so
the distributional assumptions made are central. However, there is a certain
automatism in the calculations required: ﬁnd ﬁrst and second derivatives;
if iterative methods are employed, iterate until convergence; verify that
a stationary point has been reached and attempt to ascertain that it is
a supremum. However, the chores of ﬁnding the ML estimator of θ, in a
model where normality is assumed, diﬀer from those in a model where, say,
Student-t distributions with unknown degrees of freedom are used.
Another important issue is the potential susceptibility of the ML esti-
mator of θ to small changes in the data. A slightly diﬀerent sample of
observations can produce very diﬀerent estimates when the likelihood is
ﬂat in the neighborhood of the maximum. In theory, it is advisable to ex-
plore the likelihood as much as possible. However this can be complicated,
if not impossible, in multidimensional models. In the dairy cattle breeding
example, how would one represent a likelihood in f + 306 dimensions?
One of the main diﬃculties of ML is encountered in multiparameter
situations. The problem is caused by the existence of nuisance parame-

3.4 Likelihood Inference in a Gaussian Model
125
ters. For example, suppose that the multivariate normal sampling model
y ∼N (Xβ, V) is proposed, where β is an unknown location vector to
be estimated, and V is a variance–covariance matrix, also unknown. This
matrix is needed for a correct speciﬁcation of the model but it may not be
of primary interest. The ML procedure, at least in its original form, treats
all parameters identically, and does not account well for the possible loss
of information incurred in estimating V. An approach for dealing with this
shortcoming, at least in some models, will be discussed later.
A related problem is that of inferences about random eﬀects in linear
models (Henderson, 1963, 1973; Searle et al., 1992). In the notation em-
ployed in connection with (3.4), suppose that u is a vector of genetic eﬀects,
and that one wishes to make statements about the conditional distribution
[u|y, θ], with unknown θ (e.g., θ may include β and V). Intuitively, one
may wish to use the approximation [u|y, θ =5θ], where 5θ is the ML estimate
of θ. However, this does not take into account the fact that there is an error
of estimation associated with 5θ, and it is not obvious why one should use
5θ as opposed to any other point estimate of θ. Arguably, likelihood infer-
ence was developed for assessing the plausibility of values of θ, and not for
making statements about unobservable random variables from conditional
distributions.
3.4
Likelihood Inference in a Gaussian Model
Suppose a single data point y = 10 is drawn from a normal distribution
with unknown mean m and known variance σ2 = 3. The likelihood resulting
from this single observation follows from the corresponding normal p.d.f.
L

m|σ2 = 3, y = 10

=
1
√
6π exp

−(10 −m)2
6

.
(3.6)
The function k(y) in (3.1) is 1/
√
6π, and the only part that matters (from
a likelihood inference viewpoint) is the exponential function in (3.6). This
is interpreted as a function of m for a given value of σ2 and of the observed
data point y = 10. A plot of the likelihood function using k(y) = 1/
√
6π is
in Figure 3.1.
The likelihood is symmetric about m = 10, its maximizer, as shown in
Figure 3.1. Likelihoods are not probability functions or density functions
and do not necessarily yield a ﬁnite value (e.g., 1) when integrated with
respect to θ. In this example, however, if (3.6) is integrated with respect
to m (which would be meaningless in a likelihood setting because m is not
stochastic), the value of the integral is precisely equal to one. At any rate,
the main issues in likelihood inference are the shape of the function and
the relative heights.

126
3. An Introduction to Likelihood Inference
5
10
15
20 m
L
FIGURE 3.1. Likelihood of m based on a sample of size 1.
Suppose now that four additional independent random samples are drawn
from the same distribution and that the new data set consists of the 5 × 1
vector
y′ = [y1 = 10, y2 = 8, y3 = 12, y4 = 9, y5 = 11] .
The likelihood is now built from the joint p.d.f. of the observations. This,
by virtue of the independence assumptions, is the product of ﬁve normal
densities each with mean m and variance σ2 = 3. For this example, the
likelihood of m is
L

m|σ2 = 3, y

=
1
√
6π
5 exp

−(10 −m)2 + · · · + (11 −m)2
6

.
(3.7)
This can be written as
L

m|σ2 = 3, y

=
√
6π
−5
exp

−(10 −10)2 + · · · + (11 −10)2
6

× exp

−5 (10 −m)2
6

∝k (y) exp

−5 (10 −m)2
6

,
where k(y) =
√
6π
−5 exp
9
−[(10 −10)2 + · · · + (11 −10)2]/6
:
. The prod-
uct of the two exp(·) functions arises because the term in the exponent in
(3.7) can be written as
5

i=1
(yi −m)2 =
5

i=1
(yi −y)2 + 5 (y −m)2 ,
(3.8)
where
y =
n
i=1
yi
5
.

3.4 Likelihood Inference in a Gaussian Model
127
5
10
15
20 m
L
FIGURE 3.2. Likelihood of m based on a sample of size 5.
The relevant part is that containing m. It follows that
L

m|σ2 = 3, y

∝exp

−5 (y −m)2
6

.
(3.9)
The relative values of the likelihood are the same irrespective of whether
(3.7) or (3.9) are used. The plot of the likelihood is shown in Figure 3.2.
Note that the likelihood is much sharper than that presented in Figure
3.1. The curvature of the likelihood at its maximum is larger with the larger
sample. This is because the likelihood of m based on a sample of size 5 has
more information about m than likelihood (3.6), a concept to be deﬁned
formally later in this chapter. In both data sets, the maximum value of the
likelihood is obtained when m = 10, so the ML estimate of this parameter
is 5m = 10 in the two situations. In practice, this would rarely happen,
but we have chosen the situation deliberately to compare the sharpness of
two likelihood functions having the same maximum. For example, if the
observed value in the ﬁrst data set had been y = 7.6, this would have been
the corresponding ML estimate of m.
With a sample of n independent observations, it follows from (3.8) that
the log-likelihood can be expressed as
l

m|σ2, y

= constant −
1
2σ2
 n

i=1
(yi −y)2 + n (y −m)2

.
(3.10)
Maximizing (3.10) with respect to m is equivalent to minimizing the sum
of squared deviations n
i=1 (yi −m)2 , so there is a relationship with the
least-squares criterion in this case. Taking the derivative of (3.10) with
respect to m gives the linear form in m :
dl

m|σ2, y

dm
= n (y −m)
σ2
.
(3.11)

128
3. An Introduction to Likelihood Inference
Setting (3.11) to zero and solving for m gives, as ML of m,
5m =
n
i=1
yi
n
= y.
(3.12)
Consequently, the ML estimator of the expected value of a normal distribu-
tion with known variance is the arithmetic mean of the observations (the
estimator would have been the same if the variance had been unknown,
as shown later). It is also the least-squares estimator, but this coincides
with the ML estimator of m only when normality is assumed, as in this
example. Diﬀerentiating (3.11) with respect to m gives −n/ σ2. Because
this derivative is negative, it follows that 5m is a maximum.
Suppose, temporarily, that m is a random variable in the real line, and
that σ2 is an unknown parameter. Now integrate (3.7) with respect to m
to obtain, for a sample of size n,

L

m|σ2, y

dm
=
1
(2πσ2) (n−1)/2 √n
exp

−1
2σ2
n

i=1
(yi −y)2

.
(3.13)
which, given the data, is a function of σ2 only. (The limits of integration are
±∞, but this is generally omitted unless the context dictates otherwise).
Expression (3.13) is called an integrated or marginal likelihood, and has
the same form as the so-called restricted likelihood, proposed by Patterson
and Thompson (1971). In fact, anticipating a subject to be discussed later
in this book, when σ2 is an unknown parameter, maximization of (3.13)
with respect to σ2 yields
6
σ2 =
n
i=1
(yi −y)2
n −1
,
which is the restricted maximum likelihood estimator (REML) of σ2.
3.5
Fisher’s Information Measure
3.5.1
Single Parameter Case
Suppose the random vector y has a distribution indexed by a single pa-
rameter θ, and let the corresponding density function be p (y|θ). Fisher’s
information (also known as Fisher’s expected information, or expected in-
formation) about θ, represented as I(θ), is deﬁned to be
I(θ) = Ey

 dl
dθ
2
,
(3.14)

3.5 Fisher’s Information Measure
129
where the notation emphasizes that expectations are taken with respect to
the marginal distribution of y. Hereinafter, l = l (θ) = l (θ|y) will be used
indistinctly for the log-likelihood of θ. This measure of information must
be positive because it involves an average of positive random variables, the
square of the ﬁrst derivatives of the log-likelihood with respect to θ.
If the elements of y are drawn independently of each other, the log-
likelihood is expressible as
l = constant + ln p (y|θ) = constant +
n

i=1
ln p(yi|θ).
(3.15)
Hence, the amount of information in a sample of size n can be written as
I(θ) =
n

i=1
Ii(θ),
(3.16)
where
Ii(θ) = E

d ln p(yi|θ)
dθ
2
and the expectation is taken over the marginal distribution of yi. If the ob-
servations have independent and identical distributions, the result in (3.16)
indicates that the amount of information in a sample of size n is exactly
n times larger than that contained in a single draw from the distribution.
This is because Ii(θ) is constant from observation to observation in this
case.
Information accrues additively, irrespective of whether the observations
are distributed independently or not. In the general case, it can be veriﬁed
that, for a sample of size n,
I(θ) = I1(θ) + I2.1(θ) + ... + Ii.i−1,i−2,...,1(θ) + ... + In−1.n−2,...,1(θ)
+ In.n−1,n−2,...,1(θ),
(3.17)
where Ii.i−1,i−2,...,1(θ) is the information contributed by a sample from the
distribution with density p(yi|yi−1, yi−2, ..., y2, y1, θ). If the random vari-
ables are independent, the amount of information about θ is larger than
otherwise.
Example 3.1
Information in a sample of size two
Consider a sample of size 2 consisting of draws from independently dis-
tributed random variables X and Y with p.d.f. p (x|θ) and g (y|θ). From

130
3. An Introduction to Likelihood Inference
(3.14), the information about θ in the sample is
I (θ) = E
 d
dθ ln p (x|θ) + d
dθ ln g (y|θ)
2
= E
 d
dθ ln p (x|θ)
2
+ E
 d
dθ ln g (y|θ)
2
+2 E
 d
dθ ln p (x|θ)

E
 d
dθ ln g (y|θ)

.
(3.18)
The last line follows because X and Y are independent, and therefore, the
expectation of the product is equal to the product of the expectations. As
will be shown below, the terms in the third line are equal to zero. Letting
IX (θ) = E[ d
dθ ln p (x|θ)]2, (3.18) shows that the information in the sample
is given by the sum of the information contributed by each sample point
I (θ) = IX (θ) + IY (θ) .
■
Example 3.2
Information under independent sampling from a normal
distribution
In the normal model with likelihood (3.10), the ﬁrst derivative is given in
(3.11). Upon squaring it, the term (y −m)2 is encountered and this has
expectation σ2/n. Hence
I(θ) = n2
σ4
σ2
n = n
σ2 ,
so the information is proportional to sample size.
■
Example 3.3
Information with correlated draws from a normal distri-
bution
Two lactation milk yields are available from the same cow. Their distribu-
tion is identical, having the same unknown mean µ and known variance σ2,
but there is a known correlation ρ between records, induced by factors that
are common to all lactations of a cow. Here a bivariate normal sampling
model may be appropriate, in which case the joint density of the two yields
is
p(y|µ, σ2, ρ) =
1
2π
>
(1 −ρ2) σ4 exp

−
1
2 (1 −ρ2) σ2 Q
%
,
where
Q =

(y1 −µ)2 + (y2 −µ)2 −2ρ (y1 −µ) (y2 −µ)

.
With µ being the only unknown parameter, the log-likelihood, apart from
a constant is
l

µ|σ2, ρ, y

= −(y1 −µ)2 + (y2 −µ)2 −2ρ (y1 −µ) (y2 −µ)
2 (1 −ρ2) σ2
.

3.5 Fisher’s Information Measure
131
After taking derivatives with respect to µ, squaring the result and taking
expectations, the information about µ can be shown to be equal to
I(µ) =
2
(1 + ρ)σ2
and this is smaller than the information under independence in Example
3.2 by a factor (1 + ρ)−1. When the correlation is perfect, the information
is equal to that obtained from a single sample drawn from N(µ, σ2).
■
The deﬁnition of information given in (3.14) involves conceptual repeated
sampling, as the expectation operator indicates averaging with respect to
the distribution [y|θ] or, equivalently, over all possible values that the ran-
dom vector y can take at a ﬁxed, albeit unknown, value of θ. This suggests
that a more precise term is expected information, to make a distinction with
the observed information resulting from a single realization of the random
process. Hence, the observed information is (dl/dθ)2. The observed infor-
mation is a function both of y and θ, whereas the expected information is
a function of θ only.
The observed information represents the curvature (see Example 3.4 be-
low) of the observed log-likelihood for the given data y, whereas the ex-
pected information is an average curvature over realizations of y. In this
sense, the observed information is to be preferred (Efron and Hinkley, 1978).
Typically, because the true value of the parameter is unknown, expected
and observed information are approximated by replacing θ with the max-
imum likelihood estimate. The observed information evaluated at 5θ repre-
sents the curvature at 5θ; a large curvature is associated with a strong peak,
intuitively indicating less uncertainty about θ. In Example 3.2 the expected
information was found to be n/σ2, whereas the observed information is the
square of (3.11). Note that the expected information does not depend on
the unknown m, whereas the observed information involves the two pa-
rameters of the model

m, σ2
plus the sample average y, which is the
ML estimator in this case. Often, the expected information is algebraically
simpler than the observed information.
A word of warning is particularly apposite here. Although one speaks of
information about a parameter contained in a data point, the concept is
mainly justiﬁed by the fact that, in many cases, under regularity conditions
to be speciﬁed below, the inverse of the information is the smallest asymp-
totic variance obtainable by an estimator. In other words, the concept of
information has an asymptotic justiﬁcation and is meaningful provided reg-
ularity conditions are satisﬁed (Lehmann, 1999).
3.5.2
Alternative Representation of Information
Recall that l = ln p(y|θ), and let l′ and l′′ denote the ﬁrst and second
derivatives of the log-likelihood with respect to θ. Here it is shown that

132
3. An Introduction to Likelihood Inference
Fisher’s expected information can also be expressed as
I (θ) = −Ey (l′′)
= −

l′′p(y|θ) dy
(3.19)
and, thus,
I (θ) = Ey (l′)2
= Ey (−l′′) .
Since p(y|θ) is a density function, it follows that

p(y|θ) dy = 1.
Therefore,
d
dθ

p(y|θ) dy = 0.
If the derivative can be taken inside the integral sign, it follows that
0 =

d
dθp(y|θ) dy
=

l′p(y|θ) dy
= Ey (l′) .
(3.20)
Thus, the expected value of the score is equal to zero. If a second diﬀeren-
tiation also can be taken under the integral sign, then we have
0 =

d
dθl′p(y|θ) dy
=

l′′p(y|θ) dy+

(l′)2 p(y|θ) dy
leading directly to (3.19).
Example 3.4
Curvature as a measure of information
Another way of visualizing information derives from the deﬁnition of cur-
vature at a given point of the log-likelihood function l (θ). The curvature
at the point θ is (Stein, 1977)
c (θ) =
l′′ (θ)

1 + l′ (θ)2 3
2 .

3.5 Fisher’s Information Measure
133
Since l′ 
5θ

= 0, the curvature at the maximum of the function is given by
the second derivative
c

5θ

= l′′ 
5θ

.
When the curvature is large, the sample of data points clearly toward the
value 5θ. On the other hand, if the curvature is small, there is ambiguity since
a range of values of θ leads to almost the same value of the likelihood.
■
Example 3.5
A quadratic approximation to the log-likelihood
Consider a second order Taylor series expansion of l (θ) = ln L (θ|y) around
5θ
l (θ) ≈l

5θ

+ l′ 
5θ
 
θ −5θ

+ 1
2l′′ 
5θ
 
θ −5θ
2
= l

5θ

−1
2J

5θ
 
θ −5θ
2
,
where J

5θ

= −l′′ 
5θ

is the observed information. Therefore,
L (θ|y)
L

5θ|y
 ≈exp

−1
2J

5θ
 
θ −5θ
2
.
(3.21)
It is important to note that in (3.21), θ is a ﬁxed parameter, and 5θ varies
in conceptual replications.
In Subsection 3.7.3, it will be shown that under regularity conditions, a
slight variant of the following asymptotic approximation can be established
5θ ∼N

θ, J

5θ
−1
,
which means that, approximately,
p

5θ

≈(2π)−1
2
"""J

5θ
"""
1
2 exp

−1
2J

5θ
 
θ −5θ
2
.
(3.22)
Again, this is an approximation to the sampling density of 5θ, with θ ﬁxed.
Using (3.21) in (3.22), we obtain
p

5θ

≈(2π)−1
2
"""J

5θ
"""
1
2 L (θ|y)
L

5θ|y
,
(3.23)
which is a more accurate approximation than (3.22). A slightly modiﬁed
version of this formula due to Barndorﬀ-Nielsen (1983) is so precise that
Efron (1998) refers to it as the “magic formula”.
■

134
3. An Introduction to Likelihood Inference
3.5.3
Mean and Variance of the Score Function
The mean and variance of the score function are needed to establish some
properties of the ML estimator, as discussed later. The score function in a
single parameter model is S (θ|y) = l′. As seen in connection with (3.11),
the score is a function of the data, so it must be a random variable. Usually,
the distribution of the score is unknown, although in (3.11) it possesses a
normal distribution, by virtue of being a linear function of y, a normally
distributed vector. In (3.20), it was shown that the score has zero expec-
tation. Here we show that the variance of the score is equal to Fisher’s
expected information. Thus
l′ = S (θ|y) ∼[0, I (θ)] .
(3.24)
The variance of the score, by deﬁnition, is
V ar(l′) = E (l′)2 −E2 (l′) = E (l′)2 = I (θ)
(3.25)
which follows from the deﬁnition of information as given in (3.14) and
(3.19). Note, from the derivation in (3.20), that it is assumed that the
derivative can be moved inside of the integral sign.
Example 3.6
Mean and variance of the score function in the normal
distribution
When sampling n observations independently from a normal distribution
with mean µ and variance σ2, the score, as given in (3.11), is
dl

µ|σ2, y

dµ
= n (y −µ)
σ2
.
Consequently the score is a function of the data, as stated before. For any
particular sample, it can be positive or negative, depending on the value
of y −µ. However, over an inﬁnite number of samples drawn from the
distribution of y, its average value is
Ey
n (y −µ)
σ2

= n
σ2 Ey [(y −µ)] = 0
and it has variance
V ar
n (y −µ)
σ2

= n
σ2 .
This is precisely the information about µ, as found in Example 3.2. This
completes the veriﬁcation that the distribution of the score has parameters
as in (3.24). In addition, this distribution is normal in the present example,
as pointed out previously.
■

3.5 Fisher’s Information Measure
135
3.5.4
Multiparameter Case
The results given above generalize to a vector of parameters θ in a straight-
forward manner. The expected information matrix is deﬁned to be:
I (θ) = Ey

 ∂l
∂θ
 
 ∂l
∂θ
′
= −Ey

∂2l
∂θ ∂θ′

,
(3.26)
where l = ln (θ|y) is now a function of a vector-valued parameter, ∂l/∂θ
is a vector of ﬁrst partial derivatives of the log-likelihood with respect to
each of the elements of θ, and ∂2l/∂θ ∂θ′ is the matrix (whose dimension
is equal to the number of elements in θ) of second derivatives of the log-
likelihood with respect to the parameters. The equivalent of (3.24) is that
now there is a score vector S (θ|y) having the multivariate distribution
S (θ|y) ∼[0, I (θ)] .
(3.27)
The concept of information is more subtle in the multiparameter than in
the single parameter case, unless the information matrix is diagonal. In
general, in a multiparameter model, the ith diagonal element of I (θ) cannot
be interpreted literally as information about the ith element of θ. Often,
reference needs to be made to the entire information matrix.
To illustrate, consider the sampling model y ∼N (Xβ, V) where β is
unknown, X is a known matrix of explanatory variables and V is a non-
singular variance–covariance matrix, assumed known. The likelihood func-
tion is expressible as
L (β|V, y) ∝exp

−1
2 (y −Xβ)′ V−1 (y −Xβ)

.
(3.28)
The score vector is
S (β|y) = ∂

−1
2 (y −Xβ)′ V−1 (y −Xβ)

∂β
= X′V−1 (y −Xβ) .
(3.29)
The observed information matrix is equal to the expected information ma-
trix in this case, because the second derivatives do not involve y. Here
I (β) = E

−
∂2l
∂β ∂β′

= E

−∂S (β|y)
∂β′

= E

X′V−1X

= X′V−1X.
(3.30)
From (3.26) and (3.29), the information matrix can also be calculated as
I (β) = E

 ∂l
∂β
 
 ∂l
∂β
′
= E

X′V−1 (y −Xβ) (y −Xβ)′ V−1X

= X′V−1E

(y −Xβ) (y −Xβ)′
V−1X
= X′V−1VV−1X = X′V−1X.

136
3. An Introduction to Likelihood Inference
The preceding result follows because the variance–covariance matrix of y,
by deﬁnition, is V = E

(y −Xβ) (y −Xβ)′
. Also, note from (3.29) that
E [S (β|y)] = X′V−1E (y −Xβ) = 0
and that
V ar [S (β|y)] = X′V−1 [V ar (y −Xβ)] V−1X′
= X′V−1 V ar (y) V−1X′ = X′V−1X.
This veriﬁes that the score has a distribution with mean 0 and a variance–
covariance matrix equal to the expected information. Here, this distribution
is multivariate normal, with dimension equal to the number of elements in
β. This is because the score vector in (3.29) is a linear transformation of
the data vector y.
Example 3.7
Linear regression
In a simple linear regression model, it is postulated that the observations are
linked to an intercept β0 and to a slope parameter β1 via the relationship
yi = β0 + β1xi + ei,
where xi (i = 1, 2, ..., n) are known values of an explanatory variable and
ei ∼N

0, σ2
is a residual. The distributions of the n residuals are as-
sumed to be mutually independent. The parameter vector is [β0, β1, σ2].
The likelihood function can be written as
L

β0, β1, σ2|y

∝

σ2−n
2 exp

−1
2σ2
n

i=1
(yi −β0 −β1xi)2

and the corresponding log-likelihood, apart from an additive constant, is
l

β0, β1, σ2|y

= −n
2 ln

σ2
−
1
2σ2
n

i=1
(yi −β0 −β1xi)2 .
The score vector is given by


∂l
∂β0
∂l
∂β1
∂l
∂σ2

=


1
σ2
n

i=1
(yi −β0 −β1xi)
1
σ2
n

i=1
xi (yi −β0 −β1xi)
−n
2σ2 +
1
2σ4
n

i=1
(yi −β0 −β1xi)2


.
Setting the score vector to zero and solving simultaneously for the unknown
parameters gives explicit solutions to the ML equations. The ML estimators

3.5 Fisher’s Information Measure
137
are
5σ2 =
n
i=1
(yi −5β0 −5β1xi)2
n
,
where 5β0 and 5β1 are the solutions to the matrix equation


n
n

i=1
xi
n

i=1
xi
n

i=1
x2
i



5β0
5β1

=


n

i=1
yi
n

i=1
xiyi

.
Explicitly,
5β0 = y −5β1x
and
5β1 =
n
i=1
xiyi −1
n
n
i=1
xi
n
i=1
yi
n
i=1
x2
i −1
n

 n
i=1
xi
2 .
The ﬁrst two columns of the 3 × 3 matrix of negative second derivatives of
the log-likelihood with respect to the parameters, or observed information
matrix, are


σ−2n
σ−2
n

i=1
xi
σ−2
n

i=1
xi
σ−2
n

i=1
x2
i
σ−4
n

i=1
(yi −β0 −β1xi)
σ−4
n

i=1
xi (yi −β0 −β1xi)


and the last column


σ−4
n

i=1
(yi −β0 −β1xi)
σ−4
n

i=1
xi (yi −β0 −β1xi)
−

2σ4−1 n + σ−6
n

i=1
(yi −β0 −β1xi)2


.
It is easy to verify that the expected value of each of the elements of the
score vector is null. For example,
E

 ∂l
∂β1

= 1
σ2
n

i=1
xiE (yi −β0 −β1xi) = 0

138
3. An Introduction to Likelihood Inference
and
E

 ∂l
∂σ2

= −n
2σ2 +
1
2σ4
n

i=1
E (yi −β0 −β1xi)2
= −n
2σ2 +
1
2σ4 nσ2 = 0.
Further, the expected information matrix is given by
I (θ) = σ−2


n
n

i=1
xi
0
n

i=1
xi
n

i=1
x2
i
0
0
0

2σ2−1 n


.
Note that the elements (1, 2) and (2, 1) of this matrix are not null. This
illustrates that in a multiparameter model it is often more sensible to speak
about joint information on a set of parameters, rather than about informa-
tion on individual parameters themselves. Also, observe that the elements
(1, 3) , (3, 1) , (2, 3) , and (3, 2) are null. This has a connection with the
distribution of the ML estimator of θ, as discussed later.
■
3.5.5
Cram´er–Rao Lower Bound
The concept of expected information can be used to determine a lower
bound for the variance of an estimator of the scalar parameter θ. Let 5θ =
T (y) be a function of a sample y drawn from the distribution [y|θ] ; this
function, usually called a statistic, is employed for estimating θ. Also, let
E

5θ

= m (θ) ,
where m (θ) is a function of θ; for example, m can be the identity operator.
If m (θ) = θ, the estimator is said to be unbiased for θ. The Cram´er–Rao
inequality (e.g., Casella and Berger, 1990) states that
V ar

5θ

≥[m′ (θ)]2
I (θ)
,
(3.31)
where m′ (θ) = dm(θ)/dθ, assuming m (θ) is diﬀerentiable. If the variance
of 5θ attains the right-hand side of (3.31), then the estimator is said to have
minimum variance.
A particular case is when 5θ is unbiased, that is, when E(5θ) = θ. Here, the
derivative of m (θ) = θ with respect to θ is equal to 1. Then the Cram´er–
Rao lower bound for the variance of an unbiased estimator is
V ar

5θ

≥[I (θ)]−1 .
(3.32)

3.5 Fisher’s Information Measure
139
This states that an unbiased estimator cannot have a variance that is lower
than the inverse of Fisher’s information measure. If
V ar

5θ

= [I (θ)]−1 ,
then 5θ is said to be a minimum variance unbiased estimator.
In general, as in (3.31), the lower bound depends on the expectation of
the estimator and on the distribution of the observations, because I (θ)
depends on p (y|θ) . In order to prove (3.31), use is made of the Cauchy–
Schwarz inequality (Stuart and Ord, 1991), as given below.
Cauchy–Schwarz Inequality
Let u and v be any two random variables and let c be any constant. Then
(u −cv)2 is positive or null, and so is its expectation. Hence
E (u −cv)2 = E

u2
+ c2E

v2
−2cE (uv) ≥0.
In this expression, arbitrarily choose
c = E (uv)
E (v2)
to obtain
E

u2
+ E2 (uv)
E (v2) −2E2 (uv)
E (v2) ≥0.
This leads directly to the Cauchy–Schwarz inequality
E2 (uv) ≤E

u2
E

v2
.
(3.33)
If the random variables are expressed as deviations from their expectations,
the above implies that
Cov2 (u, v) ≤V ar (u) V ar (v) .
(3.34)
In addition, if these deviations are measured in standard deviation units of
the corresponding variate, (3.34) implies that
Corr2 (u, v) ≤1,
(3.35)
where Corr(·) denotes the coeﬃcient of correlation.
■

140
3. An Introduction to Likelihood Inference
Now let u = 5θ, and v = l′, the score function. It has been established
already that E (l′) = 0 and V ar (l′) = I (θ) . Hence
Cov

5θ, l′
=
 
5θ d ln p (y|θ)
dθ

p (y|θ) dy
=

5θ dp (y|θ)
dθ
dy
= d
dθ

5θ p (y|θ) dy
= d
dθE

5θ

= d
dθm (θ) = m′ (θ) .
(3.36)
Applying the Cauchy–Schwarz inequality, as in (3.34),
Cov2 
5θ, l′
= [m′ (θ)]2 ≤V ar

5θ

I (θ) .
Rearrangement of this expression leads directly to the Cram´er–Rao lower
bound given in (3.31). Note that the proof requires interchange of inte-
gration and diﬀerentiation, as seen in connection with (3.36). There are
situations in which it is not possible to do this; typically, when the limits
of integration depend on the parameter. An example is provided by the
uniform distribution
X ∼Un (0, θ) =

1
θ,
for 0 ≤x ≤θ,
0,
otherwise.
Integration with respect to p (x|θ) is over the range 0 ≤x ≤θ, which
includes θ.
Example 3.8
Cram´er–Rao lower bound in the linear regression model
Return to the linear regression Example 3.7, and consider ﬁnding the
Cram´er–Rao lower bound for an estimator of the variance. It was seen that
I

σ2
= n/2σ4. Hence, any unbiased estimator of the variance (5v, say)
must be such that
V ar (5v) ≥2σ4
n .
The ML estimator of σ2 for this example can be written as
6
σ2 =
n
i=1

yi −5β0 −5β1xi
2
n
=
n
i=1

yi −5β0 −5β1xi
2
σ2
σ2
n
= χ2
n−2
σ2
n ,

3.5 Fisher’s Information Measure
141
because the residual sum of squares
n

i=1

yi −5β0 −5β1xi
2
∼χ2
n−2σ2
has a scaled chi-square distribution with n −2 degrees of freedom. The
expectation of the estimator is
E

5σ2
= σ2 n −2
n
,
so the estimator has a downward bias. It can readily be veriﬁed that the
variance of the ML estimator of σ2 satisﬁes the Cram´er–Rao lower bound
given by (3.31) and equal to 2σ4 (n −2) /n2. An unbiased estimator is
,σ2 = 5σ2
n
n −2 = χ2
n−2
σ2
n −2,
with variance
V ar

,σ2
= 2σ4
n −2.
Hence, this unbiased estimator does not attain the Cram´er–Rao lower
bound for 5v given above.
■
This example suggests that it would be useful to ﬁnd a condition under
which an unbiased estimator 5θ reaches the lower bound. Suppose that the
score can be written as a linear function of 5θ as follows:
l′ = a (θ)

5θ −θ

,
(3.37)
where a (θ) is some constant that does not involve the observations. Then,
Cov

5θ, l′
=

5θ a (θ)

5θ −θ

p (y|θ) dy
= a (θ) V ar

5θ

,
and
V ar (l′) = I (θ) = a2 (θ) V ar

5θ

.
The Cram´er–Rao inequality in (3.31), in view of (3.37), can be written as
Cov2 
5θ, l′
= a2 (θ) V ar2 
5θ

≤V ar

5θ

I (θ)
= V ar

5θ

a2 (θ) V ar

5θ

.
The inequality becomes an equality, so the Cram´er–Rao lower bound is
attained automatically for an unbiased estimator 5θ provided the score can

142
3. An Introduction to Likelihood Inference
be written as in (3.37). For example, in a normal sampling process with
unknown mean and known variance, the score is, as given in (3.11),
dl

µ|σ2, y

dµ
= n (y −µ)
σ2
.
Because this has the required form, with θ = µ, a (θ) = n/σ2 and 5θ = y, it
follows that the sample mean is the minimum variance unbiased estimator
of µ.
In a multiparameter situation, the condition for an unbiased estimator
to attain the lower bound is written as
l′ = A (θ)

5θ −θ

,
(3.38)
where l is the score vector and A (θ) is a matrix that may be a function
of the parameters, but not of the data. For example, under the sampling
model y ∼N (Xβ, V) , with known variance–covariance matrix, it was seen
in (3.29) that
l′ = S (β|y) = X′V−1 (y −Xβ) ,
and if

X′V−1X
−1 exists, the score can be written as
S (β|y) = X′V−1X

X′V−1X
−1 X′V−1y −β

,
which is in the form of (3.38), with θ = β, 5θ =

X′V−1X
−1 X′V−1y,
and A (θ) =

X′V−1X

. Hence,

X′V−1X
−1 X′V−1y is the minimum
variance unbiased estimator of β.
3.6
Suﬃciency
The concept of suﬃciency was developed by Fisher in the early 1920s
(Fisher, 1920, 1922). Let y represent data from the sampling model p (y|θ).
An estimator T (y) of a parameter θ is said to be suﬃcient if the condi-
tional distribution of the data y, given T (y) , is independent of θ. This
implies that T (y) contains as much information about θ as the data it-
self. Obviously, when T (y) = y, then T (y) is suﬃcient. But this is not
very useful because the idea of suﬃciency is to reduce dimensionality of y
without losing information about θ.
The deﬁnition of suﬃciency is model dependent. That is, if T (y) is suf-
ﬁcient for θ under a certain probability model, it may not be so under
another probability model.
Given a model, ML estimators are suﬃcient. To see this, assume that the
likelihood can be factorized into a term that depends on θ and a second

3.7 Asymptotic Properties: Single Parameter Models
143
term that does not
L (θ|y) ∝p (y|θ)
= g (T (y) |θ) h (y) ,
(3.39)
where the equality must hold for all y and θ. The solution of the equation
dL (θ|y)
dθ
= 0
is the same as that of equation
dg (T (y) |θ)
dθ
= 0.
Therefore, the ML estimator must be a function of the suﬃcient statistic
T (y), if the latter exists. Similarly, if
T1 (y) , T2 (y) , . . . , Tr (y)
are jointly suﬃcient statistics in a model with parameters
θ = [θ1, θ2, . . . , θr]′ ,
then the ML estimators of these parameters are functions only of T1 (y) ,
T2 (y) , . . . , Tr (y) and are, thus, jointly suﬃcient themselves.
Suﬃcient statistics are not unique. If a statistic T is suﬃcient for a pa-
rameter θ, then a one-to-one transformation function of T is also suﬃcient.
Example 3.9
A sample from a normal distribution with known variance
The log-likelihood based on a sample from a normal distribution with un-
known mean and known variance can be put, from (3.10), as
l

µ|σ2, y

= constant −n (y −µ)2
2σ2
−

i (yi −y)2
2σ2
.
Clearly, the ﬁrst term is in the form g (T (y) |µ), and the second term is
independent of µ. In this example, the suﬃcient statistic for µ is T (y) = y,
which is the ML estimator of this parameter.
■
3.7
Asymptotic Properties:
Single Parameter Models
An important reason why ML estimation is often advocated in the statis-
tical literature is because the method possesses some properties that are
deemed desirable, in some sense. In general, these properties can be shown

144
3. An Introduction to Likelihood Inference
to hold only when, given n independent draws from the same distribution,
sample size increases beyond all bounds, i.e., when n →∞. Such properties,
termed asymptotic, are consistency, eﬃciency and asymptotic normality. A
mathematically rigorous discussion of these properties is beyond the scope
of this book. However, because application of ML estimation typically re-
quires recourse to asymptotic properties, some results based on the classi-
cal, ﬁrst-order asymptotic theory are presented, to enhance understanding
of this method of estimation.
3.7.1
Probability of the Data Given the
True Value of the Parameter
Suppose n observations are drawn independently from the distribution
[y|θ0], where θ0 is the true value of the parameter θ, at some point of
the parameter space. The joint density of the observations under this dis-
tribution is then
p (y|θ0) =
n
-
i=1
p (yi|θ0) .
Likewise, let
p (y|θ) =
n
-
i=1
p (yi|θ)
be the joint density under any other value of the parameter. Then, under
certain conditions described in, e.g., Lehmann and Casella (1998),
lim
n→∞Pr
# n
-
i=1
p (yi|θ0) >
n
-
i=1
p (yi|θ)
$
= 1.
(3.40)
This result can be interpreted in the following manner: as the sample gets
larger, the density (or probability if the random variable is discrete) of the
observations at θ0 exceeds that at any other value of θ with high probability.
In order to prove the above, consider the statement
n7
i=1
p (yi|θ)
n7
i=1
p (yi|θ0)
< 1.
This is equivalent to
ln
 p (y|θ)
p (y|θ0)

=
n

i=1
ln
 p (yi|θ)
p (yi|θ0)

< 0
and to
1
n
n

i=1
ln
 p (yi|θ)
p (yi|θ0)

< 0.
(3.41)

3.7 Asymptotic Properties: Single Parameter Models
145
Then, by the law of large numbers,
lim
n→∞
#
1
n
n

i=1
ln
 p (yi|θ)
p (yi|θ0)
$
= E

ln
 p (y|θ)
p (y|θ0)
%
.
In an inﬁnitely large sample the condition in (3.41) would become
E

ln
 p (y|θ)
p (y|θ0)
%
< 0.
(3.42)
By Jensen’s inequality (see below):
E

ln
 p (y|θ)
p (y|θ0)
%
< ln E
 p (y|θ)
p (y|θ0)

.
(3.43)
Now
E
 p (y|θ)
p (y|θ0)

=

p (y|θ)
p (y|θ0)p (y|θ0) dy
=

p (y|θ) dy = 1.
so
ln E
 p (y|θ)
p (y|θ0)

= 0.
(3.44)
In view of (3.43), with the right-hand side as in (3.44), inequality (3.42)
follows, thus proving the statement in (3.40). As pointed out by Lehmann
and Casella (1998), (3.40) suggests that if in large samples the ML esti-
mator of θ were close to θ0, it would constitute a reasonable estimator,
generating the observed data with near maximum probability.
Jensen’s Inequality
Jensen’s inequality (Rao, 1973; Casella and Berger, 1990) states that if X
is a random variable, and g(X) is a concave function (“holds water”), then
g [E(X)] ≤E [g(X)] .
(3.45)
If g(X) is convex (“spills water”)
E [g(X)] ≤g [E(X)] .
(3.46)
A function is said to be convex if its second derivative with respect to
the variable is negative or null throughout. For example, ln (X) is convex.
Under convexity, the function lies below all its tangent lines (Casella and
Berger, 1990). Hence, it must be true that
g(X) ≤l(X) = a + bX,

146
3. An Introduction to Likelihood Inference
where l(X) is a line tangent to g(X) at X. Taking expectations of the
inequality
E [g(X)] ≤E [l(X)] = l [E (X)] = g [E (X)] ,
thus proving (3.46). The ﬁrst equality arises because the expectation of a
linear function is equal to the linear function of the expectation, and the
second, because l is the tangent at E (X).
A similar argument can be employed for a concave upward function
(“holds water”); in this case the function lies above the tangent lines.
■
3.7.2
Consistency
Suppose that 5θn is an estimator of the parameter θ based on random vari-
ables Y1, Y2, . . . , Yn. If, as n increases, the sampling distribution of 5θn be-
comes more and more concentrated around θ, then 5θn is said to be consis-
tent. More formally, the sequence of estimators {5θn} is consistent if {5θn}
converges in probability to the constant θ. That is,
Pr
"""5θn −θ
""" < ϵ

→1
as n →∞
for each ϵ > 0 and each θ. Although convergence refers to a sequence of
estimators, one writes informally that 5θn is a consistent estimator of θ.
The consistency property of the ML estimator (sketched below) states
that, as n →∞, the solution to the ML equation l′ (θ) = 0 has a root
5θn = f(y) tending to the true value θ0 with probability 1. For this to
hold, the likelihood must be diﬀerentiable with respect to θ ∈Ω, and
the observations must be i.i.d.. Following Lehmann and Casella (1998),
where more technical detail is given, suppose that a is small enough so
that (θ0 −a, θ0 + a) ∈Ω, and consider the event
Sn = {[p (y|θ0) > p (y|θ0 −a)] ∩[p (y|θ0) > p (y|θ0 + a)]}
= {[l (θ0) > l (θ0 −a)] ∩[l (θ0) > l (θ0 + a)]} .
By virtue of (3.40), it must be true that, as n →∞, then Pr (Sn) →1. This
implies that within the interval considered there exists a value θ0 −a <
5θn < θ0 + a at which l (θ) has a local maximum. Hence, the ﬁrst-order
condition l′ 
5θn

= 0 would be satisﬁed. It follows that for any a > 0
suﬃciently small, it must be true that
Pr
"""5θn −θ0
""" < a

→1.
(3.47)
An alternative motivation follows from (3.40). Note that
lim
n→∞Pr
#
ln
n
-
i=1
p (yi|θ0) > ln
n
-
i=1
p (yi|θ)
$
= 1

3.7 Asymptotic Properties: Single Parameter Models
147
can be restated as
lim
n→∞Pr {l (θ0|y) > l (θ|y)} = 1.
However, by deﬁnition of the ML estimator, it must be true that
l

5θn|y

≧l (θ0|y) .
Together the two preceding expressions imply that as n →∞, l

5θn|y

must take the value l (θ0|y) . This means that
lim
n→∞Pr

5θn = θ0

= 1,
which shows the consistency of 5θn.
Feng and McCulloch (1996) have extended these results by proving con-
sistency of the ML estimator when the true parameter value is on the
boundary of the parameter space.
3.7.3
Asymptotic Normality and Eﬃciency
Lehmann and Casella (1998) give a formal statement of the asymptotic
properties of the ML estimator 5θn of the parameter θ ∈Ω, where Ωis the
parameter space. These properties are attained subject to the following
regularity conditions:
1. The parameter space Ωis an open interval (not necessarily ﬁnite).
This guarantees that θ lies inside Ωand not on the boundaries. Unless
this condition is satisﬁed, a generally valid Taylor expansion of (5θ−θ)
is not possible. If Ωis a closed interval (such as 0 ≤h2 ≤1), the
theory holds for values of the parameters that do not include the
boundary.
2. The support of the p.d.f. of the data does not depend on θ; that is,
the set
A = {y : p (y|θ) > 0}
is independent of θ. The data are Y = (Y1, . . . , Yn) and the Yi are
i.i.d. with p.d.f. p (y|θ) or with p.m.f. Pr (Y = y|θ) = p (y|θ).
3. The distributions of the observations are distinct; that is,
F (y|θ1) = F (y|θ2)
implies θ1 = θ2. In other words, the parameter must be identiﬁable.
In linear models, this condition is typically referred to as estimability
(Searle, 1971).

148
3. An Introduction to Likelihood Inference
4. The density p (y|θ) is three times diﬀerentiable with respect to θ and
the third derivative is continuous in θ.
5. The integral

p (y|θ) dy can be diﬀerentiated three times under the
integral sign. This condition implies E [l′ (θ|y)] = 0 and that
E [−l′′ (θ|y)] = E [l′ (θ|y)]2 = I (θ) .
6. The Fisher information satisﬁes 0 < I (θ) < ∞.
7. There exists a function M (y) (whose expectation is ﬁnite) such that
third derivatives are bounded as follows:
"""""
d3
(dθ)3 [ln p (y|θ)]
""""" ≤M (y)
for all y in A and for θ near θ0, where θ0 is the true value of the
parameter.
Under the above conditions, Lehmann and Casella (1998) prove that 5θn
is asymptotically (n →∞) normal
√n

5θn −θ0
 D
→N

0,
1
I1 (θ0)

(3.48)
where I1 (θ0) = I (θ0) /n is the amount of information about θ contained
in a sample of size 1 and I (θ0) is the information about θ contained in
the data. The notation “
D
→” means convergence in distribution; it was
encountered in Chapter 2, Section 2.2.4. See Casella and Berger (1990)
and Lehmann (1999) for a careful development of this concept. As pointed
out in Chapter 2, (3.48) means that the sequence of random variables
√n

5θn −θ0

, as n →∞, has a sequence of cumulative distribution func-
tions (c.d.f.) that converges to the c.d.f. of an N

0,
1
I1(θ0)

random variable.
Expression (3.48) is often interpreted as:
5θn ∼N (θ0, I (θ0)) .
(3.49)
The basic elements of the proof are elaborated below.
Let Y1, Y2, ..., Yn be i.i.d. random variables from a distribution having
true parameter value θ0. The maximized log-likelihood is then l

5θn

, and
expanding the corresponding score function about θ0 yields
l′ 
5θn

≈l′ (θ0) + l′′ (θ0)

5θn −θ0

+ 1
2l′′′ (θ0)

5θn −θ0
2
.
(3.50)

3.7 Asymptotic Properties: Single Parameter Models
149
The ﬁrst, second, and third derivatives are functions of both θ0 and y.
Assuming that 5θn is the maximizer of l (θ) , then l′ 
5θn

= 0. Expression
(3.50) can be rearranged, after multiplying both sides by √n, as
√n

5θn −θ0

=
√n l′ (θ0)
−l′′ (θ0) −1
2l′′′ (θ0)

5θn −θ0
.
Dividing the numerator and denominator by n yields
√n

5θn −θ0

=
n−1/2 l′ (θ0)
−n−1l′′ (θ0) −(2n)−1 l′′′ (θ0)

5θn −θ0
.
(3.51)
Now we examine the limiting behavior of the terms in (3.51), as n →∞,
considering the three terms in the numerator and denominator of (3.51)
separately.
(1) First, note that
1
√nl′ (θ0) = √n 1
n
 d
dθ [ln p (y|θ)]
%
θ=θ0
= √n 1
n
#
d
dθ
n

i=1
ln [p (yi|θ)]
$
θ=θ0
= √n 1
n
n

i=1
p′ (yi|θ0)
p (yi|θ0) .
(3.52)
By being expressible as a sum of independent random variables, this func-
tion should have an asymptotically normal distribution (central limit the-
orem). Its expectation is
E
 1
√nl′ (θ0)

= √n 1
n
n

i=1
E
p′ (yi|θ0)
p (yi|θ0)

= 0
(3.53)
this being so because
E
p′ (yi|θ0)
p (yi|θ0)

=

p′ (yi|θ0) dyi = d
dθ

p (yi|θ0) dyi = 0
under the condition that diﬀerentiation and integration are interchange-
able, as noted earlier. Also, from (3.25),
V ar
 1
√nl′ (θ0)

= 1
nV ar [l′ (θ0)]
= 1
nI (θ0) = nI1 (θ0)
n
= I1 (θ0) .
(3.54)

150
3. An Introduction to Likelihood Inference
From (3.53), (3.54), and the central limit theorem, it follows that
1
√nl′ (θ0)
D
→N [0, I1 (θ0)] .
(3.55)
(2) The second term of interest in (3.51) is
−1
nl′′ (θ0) = −1
n
n

i=1
 d
dθ
p′ (yi|θ)
p (yi|θ)
%
θ=θ0
= 1
n
n

i=1
[p′ (yi|θ0)]2 −p (yi|θ0) p′′ (yi|θ0)
p2 (yi|θ0)
= 1
n
n

i=1
[p′ (yi|θ0)]2
p2 (yi|θ0) −1
n
n

i=1
p′′ (yi|θ0)
p (yi|θ0) .
(3.56)
As n →∞,
1
n
n

i=1
[p′ (yi|θ0)]2
p2 (yi|θ0)
→E
#
[p′ (yi|θ0)]2
p2 (yi|θ0)
$
= E
 d
dθ ln p (yi|θ)
2
θ=θ0
= I1 (θ0)
and
1
n
n

i=1
p′′ (yi|θ0)
p (yi|θ0) →E
p′′ (yi|θ0)
p (yi|θ0)

=

p′′ (yi|θ0) dyi
=
d2
dθ

p (yi|θ) dyi

θ=θ0
= 0.
Hence, in probability, we have that (3.56)
−1
nl′′ (θ0) →I1 (θ) .
(3.57)
(3) The third term in (3.51) is
−1
2nl′′′ (θ0)

5θn −θ0

.
Note that
1
nl′′′ (θ0) = 1
n
n

i=1
#
d3
(dθ)3 [ln p (yi|θ)]
$
θ=θ0
.
Based on the regularity condition 7, suppose that the third derivatives are
bounded as follows:
"""""
d3
(dθ)3 [ln p (y|θ)]
""""" ≤M (y)

3.7 Asymptotic Properties: Single Parameter Models
151
for all y in A and for θ near θ0. Then
""""
1
nl′′′ (θ0)
"""" ≤1
n
n

i=1
"""""
#
d3
(dθ)3 [ln p (yi|θ)]
$
θ=θ0
"""""
≤1
n
n

i=1
M (yi) .
As n →∞,
""n−1l′′′ (θ0)
"" must be smaller than or equal to E [M (y)] , which
in condition 7 above, is assumed to be ﬁnite. Hence, as sample size goes to
inﬁnity, the term
−1
2nl′′′ (θ0)

5θn −θ0

→0,
(3.58)
this being so because 5θn →θ0 and the third derivatives are bounded in the
preceding sense.
Considering (3.55), collecting (3.57) and (3.58), then the expression of
interest in (3.51), that is,
√n

5θn −θ0

=
n−1/2l′ (θ0)
−n−1l′′ (θ0) −(2n)−1 l′′′ (θ0)

5θn −θ0

behaves in the limit as
n−1/2l′ (θ0)
I1 (θ0)
∼N

0,
1
I1 (θ0)

.
(3.59)
This indicates that if
√n

5θn −θ0

∼N

0,
1
I1 (θ0)

(3.60)
then, as n →∞, the ML estimator can be written as the random variable
5θn = θ0 +
1
>
nI1 (θ0)
N (0, 1)) ,
(3.61)
where nI1 (θ0) = I (θ0). Hence, as n →∞:
(1) E

5θn

= θ0, so the ML estimator is asymptotically unbiased.
(2) V ar

5θn

= [nI1 (θ0)]−1 = [I (θ0)]−1, so it reaches the Cram´er-Rao
lower bound, i.e., it has minimum variance among asymptotically unbiased
estimators of θ. When this limit is reached, the estimator is said to be
eﬃcient.
(3) Informally stated, as in (3.49), it is said that the ML estimator has the
asymptotic distribution 5θn ∽N

θ0, I−1 (θ0)

. This facilitates inferences,
although these are valid strictly for a sample having inﬁnite size.
(4) As seen earlier, 5θn is consistent, reaching the true value θ0 when n →∞.

152
3. An Introduction to Likelihood Inference
3.8
Asymptotic Properties:
Multiparameter Models
Consider now the situation where the distribution of the observations is in-
dexed by parameters θ. This vector can have distinct components, some of
primary interest and others incidental, often referred to as nuisance param-
eters. The asymptotic properties of the ML estimator extend rather nat-
urally, and constitute multidimensional counterparts of (3.60) and (3.61).
Establishing these properties is technically more involved than in the single
parameter case (Lehmann and Casella, 1998), so only an informal account
will be given here. Similar to (3.50), the score vector l′ 
5θn

evaluated at
5θn, the ML estimator based on n i.i.d. samples, can be expanded about the
true parameter value θ0 as
l′ 
5θn

≈l′ (θ0) +
∂2l (θ)
∂θ ∂θ′

θ=θ0

5θn −θ0

.
Derivatives higher than second-order are ignored in the above expansion.
The score vector must be null when evaluated at a stationary point, so
rearranging and multiplying both sides by √n yields
√n

5θn −θ0

≈

−1
n
∂2l (θ)
∂θ ∂θ′
−1
θ=θ0
1
√nl′ (θ0) .
(3.62)
The random vector:
1
√nl′ (θ0) =
1
√n
n

i=1
l′
i (θ0)
D
→N [0, I1 (θ0)]
(3.63)
by the central limit theorem, because it involves the sum of many i.i.d.
random score vectors l′
i (θ0) . Likewise, as n →∞, the random matrix

−1
n
∂2l (θ)
∂θ ∂θ′
−1
θ=θ0
=
#
1
n
n

i=1

−∂2li (θ)
∂θ ∂θ′
$−1
θ=θ0
→I−1
1
(θ0) .
Hence, because n−1/2l′ (θ0) converges in distribution to an N [0, I1 (θ)]
process and

−1
n
∂2l (θ)
∂θ ∂θ′
−1
θ=θ0
converges to the constant I−1
1
(θ0) , the multivariate version of Slutsky’s
theorem (Casella and Berger, 1990) yields

−1
n
∂2l (θ)
∂θ ∂θ′
−1
θ=θ0
1
√nl′ (θ0)
D
→I−1
1
(θ0) N [0, I1 (θ)] .
(3.64)

3.9 Functional Invariance
153
Using this in (3.62), it follows that
√n

5θn −θ0
 D
→N

0, I−1
1
(θ0)

.
(3.65)
Hence, in large samples, the ML estimator can be written (informally) as
5θn ∼N

θ0, I−1 (θ0)

,
(3.66)
so the ML estimator is asymptotically unbiased, eﬃcient, and multivariate
normal. Distribution (3.66) gives the basis for the solution of many infer-
ential problems in genetics via large sample theory. Note, however, that
the asymptotic distribution depends on the unknown θ0. In practice, one
proceeds by using the approximate distribution 5θn ∼N[5θn, I−1(5θn)]. For
a large sample, this may be accurate enough, in view of the consistency
property of 5θn. It should be clear, however, that this approximation does
not take into account the error associated with the estimation of θ.
3.9
Functional Invariance of Maximum
Likelihood Estimators
The property of functional invariance states that if 5θ is the ML estimator
of θ, then the ML estimator of the function f (θ) is f(5θ). The property is
motivated using the linear regression model of Example 3.7, and a more
formal presentation is given subsequently.
3.9.1
Illustration of Functional Invariance
In Example 3.7 the ML estimators of β0 and β1 were found to be 5β0 and
5β1, respectively. If the function β0/β1 exists, then its ML estimator is:
5β0
5β1
=

y −5β1x
 
n
i=1
x2
i −1
n

 n
i=1
xi
2
n
i=1
xiyi −1
n
n
i=1
xi
n
i=1
yi
.
We proceed to verify that this is indeed the case. The log-likelihood, as-
suming independent sampling from a normal distribution, apart from an
additive constant, is
l

β0, β1, σ2|y

= −n
2 ln

σ2
−
1
2σ2
n

i=1
(yi −β0 −β1xi)2 .

154
3. An Introduction to Likelihood Inference
Deﬁne η = β0/β1, supposing the slope coeﬃcient cannot be null. The
sampling scheme remains unchanged, but the model is formulated now in
terms of the one-to-one reparameterization

β0
β1

→

η
β1

.
This means that it is possible to go from one parameterization to another
in a unique manner. For example, a reparameterization to β0 and β2
1 would
not be one-to-one because β1 and −β1 yield the same value of β2
1. The log-
likelihoods under the alternative parameterizations are related according
to
l

β0, β1, σ2|y

= l

η, β1, σ2|y

= −n
2 ln

σ2
−
1
2σ2
n

i=1
(yi −ηβ1 −β1xi)2 .
The score vector under the new parameterization is


∂l/∂η
∂l/∂β1
∂l/∂σ2

=


β1
σ2
n

i=1
(yi −ηβ1 −β1xi)
1
σ2
n

i=1
(yi −ηβ1 −β1xi) (η + xi)
−n
2σ2 +
1
2σ4
n

i=1
(yi −ηβ1 −β1xi)2


.
Setting its three elements simultaneously to zero, and solving for the un-
known parameters gives the ML estimators. From the ﬁrst equation, one
obtains directly
5η =

y −5β1x

5β1
.
The second equation leads to
5η
n

i=1

yi −y −5β1 (xi −x)

+
n

i=1

xiyi −5η5β1xi −5β1x2
i

=
n

i=1

xiyi −

y −5β1x

xi −5β1x2
i

= 0,
which, when solved for 5β1, gives as solution
5β1 =
n
i=1
xiyi −1
n
n
i=1
xi
n
i=1
yi
n

i=1
x2
i −1
n

 n
i=1
xi
2 .

3.9 Functional Invariance
155
This is identical to the ML estimator found under the initial parameter-
ization, as shown in Example 3.7. The third equation leads to the ML
estimator of σ2
5σ2 = 1
n
n
i=1

yi −5η5β1 −5β1xi
2
= 1
n
n
i=1

yi −5β0 −5β1xi
2
as found before. Finally, from the reparameterized model, one can estimate
β0 as 5β0 = 5η5β1.
It can be veriﬁed that the ML estimators of β0 and β1 are unbiased. For
example,
E

5β1

= E





n
i=1
xiyi −1
n
n
i=1
xi
n
i=1
yi
n
i=1
x2
i −1
n

 n
i=1
xi
2





=
E

 n
i=1
xiyi −1
n
n
i=1
xi
n
i=1
yi

n
i=1
x2
i −1
n

 n
i=1
xi
2
=
β0
n
i=1
xi + β1
n
i=1
x2
i −1
n
n
i=1
xi

nβ0 + β1
n
i=1
xi

n
i=1
x2
i −1
n

 n
i=1
xi
2
= β1,
and the same is true for β0. This should not be construed as an indication
that ML estimators are always unbiased; it was seen in Example 3.8 that
the ML estimator of σ2 has a downward bias. It is not obvious how to
obtain an unbiased estimator of η = β0/β1, but the functional invariance
property of the ML estimator leads directly to ML estimation of this ratio.
The estimator is biased, as shown below.
Example 3.10
Bias of a ratio of estimators
Suppose we wish to evaluate, in the linear regression model,
E
 5β0
5β1

.
Following Goodman and Hartley (1958) and Raj (1968), one can write
Cov
 5β0
5β1
, 5β1

= E

5β0

−E
 5β0
5β1

E

5β1

= β0 −E
 5β0
5β1

β1.

156
3. An Introduction to Likelihood Inference
Rearranging,
E
 5β0
5β1

= β0
β1
−
Cov
 β0
β1 , 5β1

β1
,
which shows that unless Cov

5β0/5β1, 5β1

is null, the ML estimator of β0/β1
is biased. This bias, E

5β0/5β1

−β0/β1, expressed in units of standard
deviation, is
Bias

5β0/5β1

?
V ar

5β0/5β1
 = −
Corr

5β0/5β1, 5β1
 ?
V ar

5β1

β1
= −Corr

5β0/5β1, 5β1

CV

5β1

,
where CV (·) denotes coeﬃcient of variation. Hence
"""Bias

5β0/5β1
"""
?
V ar

5β0/5β1
 ≤CV

5β1

which gives an upper bound for the absolute value of the bias in units
of standard deviation. From the form of the ML estimator of β1 given in
Example 3.7, it can be deduced that
V ar

5β1

=
σ2
n
i=1
x2
i −1
n

 n
i=1
xi
2 .
Thus,
CV

5β1

= σ
β
@
A
A
A
B
1
n
i=1
x2
i −1
n

 n
i=1
xi
2 .
This indicates that the absolute standardized bias of the ratio estimator
can be reduced by increasing the dispersion of the values of the explanatory
variable x, as measured by
n

i=1
x2
i −1
n

 n
i=1
xi
2
.
■

3.9 Functional Invariance
157
3.9.2
Invariance in a Single Parameter Model
In the preceding section, it was seen that for a one-to-one reparameteriza-
tion, the same results are obtained irrespective of whether the likelihood
function is maximized with respect to θ or η = f (θ). In the presentation
below it is assumed that the transformation is one-to-one, in the sense that
θ = f −1 (η). However, the principle of invariance can be extended to hold
for any transformation (Mood et al., 1974; Cox and Hinkley, 1974).
The explanation of the principle of invariance given here is based on
the fact that two alternative parameterizations must generate the same
probability distribution of y. Hence
p (y|θ, θ ∈Ω) = p

y|f −1 (η) ,
η ∈Ω∗
= p (y|η, η ∈Ω∗) = p [y|f (θ) ,
θ ∈Ω] ,
(3.67)
where Ω∗is the parameter space of η. A similar relationship must then
hold for the log-likelihoods
l (θ|y) = l

f −1 (η) |y

= l (η|y) = l [f (θ) |y] .
(3.68)
Then, if 5η is the maximizer of l (η|y) , it must be true that 5η = f

5θ

,
because l (θ|y) and l (η|y) have the same maximum and the relationship is
one-to-one. Under the new parameterization, the score can be written as
dl (η|y)
dη
= dl (θ|y)
dη
= dl (θ|y)
dθ
dθ
dη .
(3.69)
Setting this equation to zero leads to 5η, the ML estimator of η. In order
to evaluate the observed information about η contained in the sample,
diﬀerentiation of (3.69) with respect to η yields
d2l (η|y)
(dη)2
= d
dη
dl (θ|y)
dθ
dθ
dη

=
 d
dη
dl (θ|y)
dθ
% dθ
dη +
dl (θ|y)
dθ
 d2θ
(dη)2
=
#
d2l (θ|y)
(dθ)2

dθ
dη
$
dθ
dη +
dl (θ|y)
dθ
 d2θ
(dη)2
=

d2l (θ|y)
(dθ)2

dθ
dη
2
+ dl (θ|y)
dθ
d2θ
(dη)2

.
(3.70)

158
3. An Introduction to Likelihood Inference
The expected information about the new parameter η contained in the
sample is then
I (η) = E

d2l (θ|y)
(dθ)2
 
dθ
dη
2
+ E
dl (θ|y)
dθ
 d2θ
(dη)2
= E

d2l (θ|y)
(dθ)2
 
dθ
dη
2
= I (θ)

dθ
dη
2
= I

f −1 (η)
 df −1 (η)
dη
2
.
(3.71)
This being so because E [dl (θ|y) /dθ] = 0, as seen in (3.20). Now, using
the general result
dx
dy =
1
dy/dx,
the (asymptotic) variance of the ML estimator of the transformed param-
eter is
V ar (η) = [I (η)]−1
= [I (θ)]−1
dη
dθ
2
= V ar (θ)
dη
dθ
2
(3.72)
which would be evaluated at θ = 5θ. Expression (3.72) can give erroneous
results when f is not monotone, as shown in the following example.
Example 3.11
A binomially distributed random variable
For a binomially distributed random variable X (number of “successes” in
n trials), the likelihood is
L (θ|x, n) ∝θx (1 −θ)n−x
and the log-likelihood, ignoring an additive constant, is
l (θ|x, n) = x ln θ + (n −x) ln (1 −θ) .
Setting the ﬁrst diﬀerential of the log-likelihood with respect to θ equal to
zero and solving for θ yields the closed-form ML estimator
5θ = x
n.
Since, by deﬁnition, X results from the sum of n independent Bernoulli
trials, each with variance θ (1 −θ), the variance of 5θ is
V ar

5θ

= θ (1 −θ)
n
.

3.9 Functional Invariance
159
Imagine that interest focuses on the odds ratio η = f (θ) = θ/(1−θ). Since
dη
dθ =
1
(1 −θ)2 ,
using (3.72), we obtain
V ar (5η) =
5θ
n

1 −5θ
3 .
In this case, the transformation f is one-to-one. Suppose instead that there
is interest in the quantity ω = g (θ) = θ (1 −θ). Now,
dω
dθ = 1 −2θ.
Then (3.72) yields, for the asymptotic variance of the ML of ω,
V ar (5ω) =
5θ

1 −5θ

n

1 −25θ
2
,
which underestimates the variance rather drastically if 5θ = 1/2. The source
of this problem is that g is not one-to-one. A way around this problem was
discussed in Section 2.2.4 of Chapter 2.
■
3.9.3
Invariance in a Multiparameter Model
Let the distribution of the random vector y be indexed by a parameter
θ having more than one element. Consider the one-to-one transformation
η = f (θ) such that the inverse function θ = f −1 (η) exists, and suppose
that the likelihood is diﬀerentiable with respect to η at least twice. The
relationship between likelihoods given in (3.68) carries directly to the mul-
tiparameter situation. The score vector in the reparameterized model, after
equation (3.69), is
∂l (η|y)
∂η
= ∂θ′
∂η
∂l (θ|y)
∂θ
,
(3.73)
where ∂θ′/∂η = {∂θj/∂ηi} is a matrix of order p × p, p is the number of
elements in θ, and subscripts i and j refer to row and column, respectively.
Thus, the jth row of ∂θ′/∂η looks as follows:
∂θ1
∂ηj
, ∂θ2
∂ηj
, . . . , ∂θp
∂ηj

.
The expected information matrix, following (3.71), is
I (η) = ∂θ′
∂η I (θ) ∂θ
∂η′ = ∂

f −1 (η)
′
∂η
I

f −1 (η)
 ∂

f −1 (η)

∂η′
,
(3.74)

160
3. An Introduction to Likelihood Inference
and the expression equivalent to (3.72) is
V ar (5η) = ∂η
∂θ′
""""
θ=θ
V ar

5θ
 ∂η′
∂θ
""""
θ=θ
.
(3.75)
Example 3.12
A reparameterized linear regression model
The matrix in (3.74) is illustrated using the linear regression model. The
original parameterization was in terms of θ′ = [β0, β1, σ2]. The new pa-
rameterization consists of the vector
η =


η1
η2
η3

=


β0/β1
β1
σ2


with inverse
θ = f −1 (η) =


η1η2
η2
η3

.
The 3 × 3 matrix deﬁned after (3.73) would be
∂θ′
∂η =


∂θ1/∂η1
∂θ2/∂η1
∂θ3/∂η1
∂θ1/∂η2
∂θ2/∂η2
∂θ3/∂η2
∂θ1/∂η3
∂θ2/∂η3
∂θ3/∂η3

=


η2
0
0
η1
1
0
0
0
1

.
(3.76)
Diﬀerentiating the log-likelihood twice under the new parameterization,
multiplying by −1, and taking expectations, yields the information matrix
I (η) = 1
σ2


nη2
2
η2

nη1 +
n

i=1
xi

0
η2

nη1 +
n

i=1
xi

n

i=1
(η1 + xi)2
0
0
0
n
2η3


.
It can be veriﬁed that the same result is obtained from the matrix product
given in (3.74), employing the matrix (3.76).
■

4
Further Topics in Likelihood Inference
4.1
Introduction
This chapter continues the discussion on likelihood inference. First, two
commonly used numerical methods for obtaining ML estimates when the
likelihood equations do not have a closed form or are diﬃcult to solve are
introduced. (A third method, the Expectation–Maximization algorithm,
often referred to as the EM-algorithm, is discussed in Chapter 9). This is
followed by a discussion of the traditional test of hypotheses entrenched in
the Neyman–Pearson theory. The classical ﬁrst-order asymptotic distribu-
tion of the likelihood ratio is derived assuming that the standard regularity
conditions are satisﬁed, and examples of likelihood ratio tests are given.
The presence of so-called nuisance parameters has been one of the major
challenges facing the likelihood paradigm. How does one make inferences
about the parameters of interest in the presence of nuisance parameters
without overstating precision? Many diﬀerent approaches have been sug-
gested for dealing with this problem and some of these are brieﬂy discussed
and illustrated here. The chapter ends with examples involving the multi-
nomial model and the analysis of ordered categorically distributed data.

162
4. Further Topics in Likelihood Inference
4.2
Computation of Maximum
Likelihood Estimates
As noted earlier, the ML equations (the score vector)
l′ (θ) = 0
may not have an explicit solution, so numerical methods must be used
to solve them. Maximization of a likelihood function (or minimization of
any induced objective function) can be viewed as a nonlinear optimization
problem. A plethora of algorithms can be used for this purpose. Treatises in
numerical analysis, such as Dahlquist and Bj¨orck (1974), Dennis and Schn-
abel (1983), and Hager (1988) can be consulted. Here, a sketch is presented
of two widely used algorithms employed in connection with likelihood in-
ference: the Newton–Raphson method and Fisher’s scoring procedure.
The Newton–Raphson Procedure
Consider expanding the score vector about a trial value θ[t], where t =
0, 1, 2, ... denotes an iterate number. A linear approximation to the score
gives
l′ (θ) ≈l′ 
θ[t]
+
∂2l (θ)
∂θ ∂θ′

θ=θ[t]

θ −θ[t]
.
In the vicinity of a stationary point, it must be true that l′ (θ) ≈0. Using
this in the preceding expression, and solving for θ at each step t, gives the
sequence of updated values
θ[t+1] = θ[t] +

−∂2l (θ)
∂θ ∂θ′
−1
θ=θ[t] l′ 
θ[t]
,
t = 0, 1, 2, ... .
(4.1)
The diﬀerence θ[t+1] −θ[t] is called a correction. The iterative process is
continued until the correction is null, corresponding to the situation where
l′(θ[t]) = 0. This procedure is known as the Newton–Raphson algorithm;
Fisher’s scoring method uses I (θ) instead of the observed information ma-
trix −∂2l (θ) /∂θ ∂θ′. Typically, the method of scoring requires fewer cal-
culations, because many expressions vanish or simplify in the process of
taking expectations. However, it may converge at a slower rate. The two
methods may not converge at all and, even if they do, there is no assurance
that a global maximum would be located. This is not surprising, as the
methods search for stationarity without reference to the possible existence
of multiple maxima. This is a potential problem in models having many
parameters, and it is expected to occur more frequently when sample sizes
are small, as the likelihood may have several “peaks and valleys”.

4.2 Computation of Maximum Likelihood Estimates
163
Quadratic Convergence Property
The Newton–Raphson procedure has the property of converging quadrat-
ically to a stationary point. Dahlquist and Bj¨orck (1974) provide a proof
for the single parameter case, and this is presented in more detail here.
Let the root of l′ (θ) = 0 be 5θ. Hence, l′′(5θ) cannot be a null matrix and
l′′ (θ) cannot be null for all θ near (in some sense) 5θ. Let the error of the
trial values at iterates t and t + 1 be ϵ[t] = θ[t] −5θ and ϵ[t+1] = θ[t+1] −5θ,
respectively. A second-order Taylor series expansion of the score evaluated
at 5θ (which must be equal to zero, because 5θ is a root), about iterate value
θ[t], gives
l′ 
5θ

≈l′ 
θ[t]
+ l′′ 
θ[t] 
5θ −θ[t]
+ 1
2l′′′ 
θ[t] 
5θ −θ[t]2
.
Because l′′ 
θ[t]
is not null, one can write
5θ −θ[t] +
l′ 
θ[t]
l′′

θ[t] =
−1
2l′′′ 
θ[t] 
5θ −θ[t]2
l′′

θ[t]
.
(4.2)
Now, because of the form of the Newton–Raphson iteration in (4.1), ex-
pression (4.2) is equivalent to
5θ −θ[t+1] =
−1
2l′′′ 
θ[t] 
5θ −θ[t]2
l′′

θ[t]
.
Writing the above in terms of the errors about the root, one obtains
ϵ[t+1] =
l′′′ 
θ[t]
2l′′

θ[t]

ϵ[t]2
.
(4.3)
This indicates that the error at iterate t is proportional to the square of the
error at the preceding iteration, so the method is said to be quadratically
convergent. Now
lim
θ[t]→θ

ϵ[t+1]

ϵ[t]2

= lim
θ[t]→θ


l′′′ 
θ[t]
2l′′

θ[t]

=
l′′′ 
5θ

2l′′

5θ
.
(4.4)
Also,
lim
θ[t]→θ
"""""
ϵ[t+1]

ϵ[t]2
"""""

= lim
θ[t]→θ
""ϵ[t+1]""

ϵ[t]2

= 1
2
"""l′′′ 
5θ
"""
"""l′′

5θ
"""
= C

164
4. Further Topics in Likelihood Inference
and C, called the “asymptotic error constant”, will be nonnull whenever
l′′′ 
5θ

̸= 0. For a sample of size n, the second derivative of the log-
likelihood, with respect to θ, is equal to
l′′ (θ) =
n

i=1
d2
(dθ)2 ln p (yi|θ)
and, for large n, this converges in probability to −I (θ) = −nI1 (θ) . Hence,
C goes to zero as the sample size increases, provided the third derivatives
are bounded; recall that this was an assumption made when proving con-
sistency of the ML estimator. This suggests a relatively faster convergence
rate at larger values of n, given that the algorithm converges, as assumed
here.
The Newton–Raphson algorithm always converges to a root of the ML
equation provided that the starting value θ[0] is suﬃciently close to such a
root (Dahlquist and Bj¨orck, 1974). Using (4.3),
5θ −θ[1] =

5θ −θ[0]



5θ −θ[0] l′′′ 
θ[0]
2l′′

θ[0]

.
Thus, if
""""""

5θ −θ[0] l′′′ 
θ[0]
2l′′

θ[0]
""""""
< 1
the next approximation is closer to the root than the starting value. Let
1
2
""""
l′′′ (θ)
l′′ (θ)
"""" ≤m
for all θ ∈I
with m > 0 and where I is a region near 5θ, and suppose that
"""

5θ −θ[0]
m
""" =
"""ϵ[0]m
""" < 1.
Then, from (4.3),
"""ϵ[t+1]""" =
""""""
l′′′ 
θ[t]
2l′′

θ[t]
""""""

ϵ[t]2
≤m

ϵ[t]2
so
"""mϵ[t+1]""" ≤

mϵ[t]2
,

4.2 Computation of Maximum Likelihood Estimates
165
and
"""ϵ[t+1]""" ≤1
m

mϵ[t]2
= 1
m
"""mϵ[t]"""
2
≤1
m

mϵ[t−1]22
= 1
m
"""mϵ[t−1]"""
22
≤1
m

mϵ[t−2]23
= 1
m
"""mϵ[t−2]"""
23
≤1
m

mϵ[0]2t+1
.
(4.5)
Therefore, as t →∞, then
""ϵ[t+1]"" →0, and the algorithm converges toward
5θ. Similarly, let θ[0] = 5θ+ϵ[0], θ[1] = 5θ+ϵ[1], . . . , θ[t] = 5θ+ϵ[t] be the sequence
of iterates produced by the Newton–Raphson algorithm. If θ[0] belongs to
the region I, this is equivalent to the statement

5θ −
"""ϵ[0]""" ,5θ +
"""ϵ[0]"""

∈I.
(4.6)
The next iterate, θ[1], must fall in the set
I1 =

5θ −
"""ϵ[1]""" ,5θ +
"""ϵ[1]"""

.
(4.7)
Now, by (4.5),
5θ −
"""ϵ[1]""" ≥5θ −1
m

mϵ[0]2
= 5θ −m
"""ϵ[0]"""
2
= 5θ −
"""mϵ[0]"""
"""ϵ[0]""" .
By assumption,
""mϵ[0]"" < 1, so it must be true that
5θ −
"""ϵ[1]""" ≥5θ −
"""ϵ[0]""" .
(4.8)
Similarly,
5θ +
"""ϵ[1]""" ≤5θ + 1
m

mϵ[0]2
= 5θ +
"""mϵ[0]"""
"""ϵ[0]"""
so
5θ +
"""ϵ[1]""" ≤5θ +
"""ϵ[0]""" .
(4.9)
From (4.8) and (4.9)
5θ −
"""ϵ[0]""" ≤5θ −
"""ϵ[1]""" < 5θ +
"""ϵ[1]""" ≤5θ +
"""ϵ[0]""" .
Then the region of values I1 of the ﬁrst iterate is such that I1 ⊂I. Using
this argument repeatedly leads to
It ⊂It−1 ⊂It−2 ⊂· · · ⊂I2 ⊂I1 ⊂I.
(4.10)
This indicates that all iterates stay within the initial region, and that as
t →∞, I∞should contain a single value, 5θ, proving convergence of the
iterative scheme (provided iteration starts in the vicinity of the root).

166
4. Further Topics in Likelihood Inference
4.3
Evaluation of Hypotheses
A frequently encountered problem is the need for evaluating or testing a hy-
pothesis about the state of a biological system. In genetics, for example, in
a large randomly mated population, in the absence of mutation, migration,
or selection, genotypic frequencies are expected to remain constant gener-
ation after generation. This is called the Hardy–Weinberg equilibrium law
(i.e., Crow and Kimura, 1970) and it may be of interest to test if this hy-
pothesis holds, given a body of data. A discussion of how such tests can
be constructed from a frequentist point of view based on a ML analysis
is presented below. The presentation is introductory, and only classical,
ﬁrst-order asymptotic results are discussed. This ﬁeld has been undergoing
rapid developments. The reader is referred to books such as Barndorﬀ-
Nielsen and Cox (1994) and Severini (2000) for an account of the modern
approach to likelihood inference.
4.3.1
Likelihood Ratio Tests
The use of ratios between likelihoods obtained under diﬀerent models or hy-
potheses is widespread in genetics. The theoretical basis of tests constructed
from likelihood ratios was developed by Neyman and Pearson (1928). The
asymptotic distribution of twice the logarithm of a maximum likelihood
ratio statistic was derived by Wilks (1938). Cox and Hinkley (1974) and
Stuart and Ord (1991) give a heuristic account of this classical theory, and
this is followed closely here. In order to pose the problem in a suﬃciently
general framework, partition the parameter vector θ ∈Θ as
θ =

θ1
θ2

,
where θ1 is an r×1 vector of parameters involved in the hypothesis, having
at least one element, and θ2 is an s×1 vector of supplementary or nuisance
parameters, with zero or more components. Let p = r+s. Some hypotheses
to be contrasted can be formulated as
H0 : θ1 = θ10
versus
H1 : θ1 ̸= θ10,
where θ10 is the value of the parameter under the null hypothesis. It is
important here to note the nesting structure of θ: in one of the hypotheses
or models, elements of θ take a ﬁxed value, but are free to vary in the
other.
The likelihood ratio test is based on the fact that the likelihood max-
imized under H1 must be at least as large as that under H0. This is so

4.3 Evaluation of Hypotheses
167
because there is always the possibility that there may be a higher likeli-
hood at values of the parameters violating the restrictions imposed by the
null hypothesis. Let
5θ =

5θ1
5θ2

be the unrestricted ML estimator, and let
,θ =
 θ10
,θ2

be the estimator under H0. In general, ,θ2, which is the ML estimator of
θ2, for ﬁxed θ1 = θ10, will not coincide with 5θ2. The ratio of maximized
likelihoods is
LR =
L

θ10, ,θ2|y

L

5θ1, 5θ2|y
 ,
0 ≤LR ≤1.
(4.11)
Values of LR close to 1 suggest plausibility of H0, because the restriction
imposed by the hypothesis does not lower the likelihood in an appreciable
manner. Since LR is a function of y, it is a random variable having some
sampling distribution. Now, because large values of LR suggest that H0
holds, one would reject the null hypothesis whenever the LR is below a
critical threshold t. For a test with rejection rate (under H0) α, the value
of the threshold is given by the solution to the integral equation (Stuart
and Ord, 1991)
 tα
0
p (LR) dLR = α,
(4.12)
where p (LR) is the density of the distribution of the likelihood ratio un-
der H0. In general, this distribution is unrecognizable and must be ap-
proximated. However, there are instances in which this distribution can be
identiﬁed. Examples of the two situations follow.
Before embarking on these examples, we mention brieﬂy an alternative,
“pure likelihoodist”, approach to inference. Arguments in favor of this ap-
proach can be found in Edwards (1992) and in Royall (1997). Rather than
maximizing the likelihood and studying the distribution of the ML esti-
mator in conceptual replications, adherents to this school draw inferences
from the likelihood or from the likelihood ratio only, with the data ﬁxed.
The justiﬁcation is to be found in what Hacking (1965) termed the law of
likelihood:
“If one hypothesis, H1, implies that a random variable X
takes the value x with probability f1 (x), while another hypoth-
esis, H2, implies that the probability is f2 (x), then the observa-
tion X = x is evidence supporting H1 over H2 if f1 (x) > f2 (x),

168
4. Further Topics in Likelihood Inference
and the likelihood ratio f1 (x) /f2 (x) measures the strength of
that evidence.”
Likelihood ratios close to 1 represent weak evidence, and extreme ratios
represent strong evidence. The problem is ﬁnding a benchmark value k that
would give support to H1 over H2 analogous to the traditional p values
equal to 0.05 and 0.01. Values of k = 8 (representing “fairly strong”) and
k = 32 (representing “strong”) have been proposed and Royall (1997) gives
a rationale for these choices. We will not pursue this subject further. Instead
the reader is referred to the works mentioned above, where arguments in
favor of this approach can be found.
Example 4.1
Likelihood ratio in a Gaussian linear model
Suppose that a vector of observations is drawn from the multivariate dis-
tribution
y ∼N

Xβ, Vσ2
,
where β (p × 1) and σ2 are unknown parameters and V is a known ma-
trix. A hypothesis of interest may be H0: β = α, so σ2 is the incidental
parameter here. The likelihood under H1 is
L

β,σ2|y

= (2π)−N
2 ""Vσ2""−1
2 exp

−1
2σ2 (y −Xβ)′ V−1 (y −Xβ)

,
where N is the order of y. In the absence of any restriction, the ML esti-
mators can be found to be
5β =

X′V−1X
−1 X′V−1y
and
6
σ2 = 1
N

y −X5β
′
V−1 
y −X5β

The likelihood under H0 is
L

σ2|β = α, y

= (2π)−N
2 ""Vσ2""−1
2 exp

−1
2σ2 (y −Xα)′ V−1 (y −Xα)

and the corresponding ML estimator of σ2 is
55σ
2 = 1
N (y −Xα)′ V−1 (y −Xα) .
Note that
(y −Xα)′ V−1 (y −Xα)
=

y −X5β + X

5β −α
′
V−1 
y −X5β + X

5β −α

=

y −X5β
′
V−1 
y −X5β

+

5β −α
′ 
X′V−1X
 
5β −α

= N6
σ2 +

5β −α
′ 
X′V−1X
 
5β −α

.

4.3 Evaluation of Hypotheses
169
Hence
55σ
2 = 6
σ2 +

5β −α
′ 
X′V−1X
 
5β −α

N
.
Using (4.11) the ratio of maximized likelihoods is then
LR =
L

55σ
2|β = α, y

L

5β,6
σ2|y

=

1 +

5β −α
′ 
X′V−1X
 
5β −α

N6
σ2


−N
2
.
Now, under the null hypothesis (Searle, 1971),

5β −α
′ 
X′V−1X
 
5β −α

σ2
∼χ2
p
and, further, this random variable can be shown to be distributed indepen-
dently of
N6
σ2
σ2
∼χ2
N−p.
Using these results, the likelihood ratio is expressible as
LR =

1 +
σ2χ2
p
σ2χ2
N−p
−N
2
=

1 +
p
χ2
p
p
(N −p)
χ2
N−p
N−p


−N
2
.
In addition (Searle, 1971),
Fp,N−p =
χ2
p/p
χ2
N−p/(N −p)
deﬁnes an F-distributed random variable with p and n −p degrees of free-
dom. Hence
LR =

1 +
p
(N −p)Fp,N−p
−N
2
.
Thus, we see that the LR decreases monotonically as F increases, so
Pr (LR ≦tα) = Pr (F ≧Fα,p,N−p) ,
where Fα,p,N−p is a critical value deﬁned by
 Fα,p,N−p
0
p (Fp,N−p) dF = 1 −α.
Here the distribution of the LR statistic is known exactly.
■

170
4. Further Topics in Likelihood Inference
Example 4.2
The Behrens–Fisher problem
Suppose samples are drawn from two populations having distinct variances
σ2
1 and σ2
2. The sampling model is

y1
y2

∼N


X1β1
X2β2

,

I1σ2
1
0
0
I2σ2
2

.
A hypothesis of interest may be: H0: β1 = β2 = β. Under H1 : β1 ̸= β2
the likelihood is
L

β1, β2, σ2
1, σ2
2|y

∝
2
-
i=1

σ2
i
−Ni
2 exp

−1
2σ2
i
(yi−Xiβi)′ (yi−Xiβi)

,
where Ni is the order of the data vector yi. The maximizers of the unre-
stricted likelihood are
5βi = (X′
iXi)−1 Xi
′yi,
i = 1, 2,
6
σ2 = 1
Ni

yi −Xi5β
′ 
yi −Xi5β

,
i = 1, 2,
and the maximized likelihood is
L

5β1, 5β2, 6
σ2
1, 6
σ2
2|y

∝
2
-
i=1
6
σ2
i
−Ni
2
exp

−Ni
2

.
Under H0:
L

β, σ2
1, σ2
2|y

∝
2
-
i=1

σ2
i
−Ni
2 exp

−1
2σ2
i
(yi −Xiβ)′ (yi −Xiβ)

.
After diﬀerentiation of the log-likelihood with respect to the parameters,
setting the resulting equations to zero gives the system
55σ
2
i = 1
Ni

yi −Xi55β
′ 
yi −Xi55β

,
i = 1, 2,
55β =
 2

i=1
X′
iXi
55σ
2
i
−1  2

i=1
Xi′yi
55σ
2
i

which is not explicit in 55β, so it must be solved iteratively. The likelihood
ratio is
LR =
2
-
i=1
55σ
2
i
6
σ2
−Ni
2
=
2
-
i=1



yi −Xi55β
′ 
yi −Xi55β


yi −Xi5βi
′ 
yi −Xi5βi



−Ni
2
.
Because 55β cannot be written explicitly, the distribution of the LR is diﬃcult
or impossible to arrive at without using approximations.
■

4.3 Evaluation of Hypotheses
171
Approximating the Distribution of the Likelihood Ratio
The derivation of the asymptotic distribution of the likelihood ratio test
presented below is based on ﬁrst-order asymptotic results where Taylor
expansions play a central role. Here it is important to respect the conditions
for the diﬀerentiability of functions and the rate of convergence of the
terms that are ignored relative to the rate of convergence of those that are
kept. The reader is referred to Lehmann and Casella (1998) for a careful
treatment of this subject.
In (4.11) the likelihood ratio was deﬁned as
LR =
L

θ10, ,θ2|y

L

5θ1, 5θ2|y
 .
(4.13)
Minus twice the log-likelihood ratio (sometimes called the deviance) is equal
to
−2 ln LR = −2

l

θ10, ,θ2|y

−l

5θ1, 5θ2|y

= 2

l

5θ1, 5θ2|y

−l

θ10, ,θ2|y

.
(4.14)
In these expressions, ,θ2 is the ML estimator of θ2 under H0 and

5θ1, 5θ2

are the ML estimators of θ under H1.
The asymptotic properties of (4.14) are derived as follows. First, ex-
pand the log-likelihood of θ, l (θ1, θ2|y), in a Taylor series about the ML
estimates

5θ1, 5θ2

. Since ﬁrst derivatives evaluated at the ML estimates

5θ1, 5θ2

are zero, this yields
l (θ1, θ2|y) ≈l

5θ1, 5θ2|y

+1
2

θ1 −5θ1
′ ∂2l (θ1, θ2|y)
∂θ1 ∂θ′
1
""""
θ=θ

θ1 −5θ1

+1
2

θ2 −5θ2
′ ∂2l (θ1, θ2|y)
∂θ2 ∂θ′
2
""""
θ=θ

θ2 −5θ2

+

θ1 −5θ1
′ ∂2l (θ1, θ2|y)
∂θ1 ∂θ′
2
""""
θ=θ

θ2 −5θ2

.
(4.15)
Deﬁne
−∂2l (θi, θj|y)
∂θi ∂θ′
j
"""""
θ=θ
= Jij

5θ

,
(4.16)
which is the ijth block of the observed information matrix J

5θ

. (This
should not be confused with a similar symbol used to deﬁne the Jacobian

172
4. Further Topics in Likelihood Inference
in (2.4) and (2.30)). We will need

J11 
5θ

J12 
5θ

J21 
5θ

J22 
5θ


=

J11

5θ

J12

5θ

J21

5θ

J22

5θ



−1
=

J

5θ
−1
,
which is an expression for the asymptotic covariance matrix of 5θ. Substi-
tuting (4.16) in (4.15) gives
l (θ1, θ2|y) ≈l

5θ1, 5θ2|y

−1
2

5θ1 −θ1
′
J11

5θ
 
5θ1 −θ1

−1
2

5θ2 −θ2
′
J22

5θ
 
5θ2 −θ2

−

5θ1 −θ1
′
J12

5θ
 
5θ2 −θ2

.
(4.17)
Under H1, (4.17) evaluated at 5θ is equal to l

5θ1, 5θ2|y

.
The log-likelihood (4.17) under H0, which is the log-likelihood for θ2,
with θ1 ﬁxed at θ10, is equal to
l (θ10, θ2|y) ≈l

5θ1, 5θ2|y

−1
2

5θ1 −θ10
′
J11

5θ
 
5θ1 −θ10

−1
2

5θ2 −θ2
′
J22

5θ
 
5θ2 −θ2

−

5θ1 −θ10
′
J12

5θ
 
5θ2 −θ2

.
(4.18)
It is easy to obtain ,θ2, the ML estimator of θ2, given θ1 = θ10, from (4.18).
Taking partial derivatives with respect to θ2, setting these equal to zero
and solving for θ2 yields
,θ2 = 5θ2 +

J22

5θ
−1
J21

5θ
 
5θ1 −θ10

,
(4.19)
whose asymptotic covariance matrix is equal to
V ar

,θ2

= J22 
5θ

−J21 
5θ
 
J11 
5θ
−1
J12 
5θ

=

J22

5θ
−1
.
After some algebra, the log-likelihood (4.18), evaluated at θ2 = ,θ2, can be
shown to be equal to
l

θ10, ,θ2|y

≈l

5θ1, 5θ2|y

−1
2

5θ1 −θ10
′ 
J11

5θ

−J12

5θ
 
J22

5θ
−1
J21

5θ

×

5θ1 −θ10

,
(4.20)

4.3 Evaluation of Hypotheses
173
which is not a function of θ2. Recalling that
J11

5θ

−J12

5θ
 
J22

5θ
−1
J21

5θ

=

J11 
5θ
−1
,
(4.20) can be written
l

θ10, ,θ2|y

≈l

5θ1, 5θ2|y

−1
2

5θ1 −θ10
′ 
J11 
5θ
−1 
5θ1 −θ10

.
(4.21)
It follows that −2 ln LR has the form
2

l

5θ1, 5θ2|y

−l

θ10, ,θ2|y

≈

5θ1 −θ10
′ 
J11 
5θ
−1 
5θ1 −θ10

= √n

5θ1 −θ10
′ 
nJ11 
5θ
−1 √n

5θ1 −θ10

.
(4.22)
As n →∞, √n

5θ1 −θ10

converges to N

0, I11 (θ)

and nJ11 
5θ

to
I11 (θ), the covariance matrix of the limiting marginal distribution of

5θ1 −θ10

.
Therefore, as n →∞, −2 ln LR converges to a chi-square distribution, with
noncentrality parameter equal to zero under H0, and with r = dim (θ1)
degrees of freedom
−2 ln (LR) |H0 ∼χ2
r.
(4.23)
Note that this limiting chi-square distribution does not involve the nuisance
parameter θ2.
The test criterion decreases monotonically as the LR increases. Thus, if
χ2
r exceeds a certain critical value, this corresponds to a signiﬁcant lowering
of the likelihood under H0 and, thus, to rejection. If H1 holds
−2 ln (LR) |H1 ∼χ2
r,λ
(4.24)
where λ = [θ1 −θ10]′ 
J11 
5θ
−1
[θ1 −θ10] is a noncentrality parameter
(this is clearly null when θ1 = θ10). When s > 0 (the number of nuisance
parameters) the computation of LR requires two maximizations: one under
H0 and another under H1.
The developments in this section make it clear that a comparison be-
tween two models by means of the likelihood ratio test assumes a common
parameter θ that is allowed to take speciﬁc values under either model.
The test does not make sense otherwise. There must be a nested structure
whereby the reduced model is embedded under the unrestricted model. If
this requirement is not satisﬁed, the asymptotic theory described does not

174
4. Further Topics in Likelihood Inference
hold. A modiﬁcation of the likelihood ratio criterion for dealing with tests
involving nonnested models has been proposed by Cox (1961, 1962); more
recent and important generalizations can be found in Vuong (1989) and Lo
et al. (2001).
The asymptotic results presented above assume asymptotic normality
of the ML estimator. Recently, Fan et al. (2000) provided a proof of the
asymptotic chi-square distribution of the log-likelihood ratio statistic, for
cases where the ML estimator is not asymptotically normal.
As a ﬁnal warning, with many nuisance parameters, notably when their
number is large relative to the number of observations, use of the above
theory to discriminate between models can give misleading results.
Power of the Likelihood Ratio Test
When designing experiments, it is important to assess the power of the test.
This is the probability of rejecting H0, given that H1 is true or, equivalently,
of accepting that the parameter value is θ1, instead of θ10. For example, a
genetic experiment may be carried out to evaluate linkage between a genetic
marker and a QTL. A design question could be: How many individuals
need to be scored such that the hypothesis that the recombination rate
is, say, equal to 10% (θ10) , is rejected with probability P? Under H0, the
noncentrality parameter is 0, and the probability of rejection for a test of
size α would be computed using (4.12) and (4.23) as
Pr (rejecting H0|H0) =
 ∞
tα
p

χ2
1

dχ2
1 = α,
(4.25)
where tα is a critical value of a central chi-square distribution on one degree
of freedom. If H1 holds, the power would be computed as
Pr (rejecting H0|H1) =
 ∞
tα
p

χ2
1,λ

dχ2
1,λ.
(4.26)
For the linkage experiment considered here, the noncentrality parameter
would be λ = (θ −0.10)2 I (θ) and the calculation can be carried out for
any desired θ.
Example 4.3
Likelihood ratio test of Hardy–Weinberg proportions
Consider a segregating locus with two alleles: B and b. Suppose there are
three observable phenotypes corresponding to individuals with genotypes
BB, Bb, and bb. A random sample is drawn from a population. The data
consist of the vector y′ = [yBB, yBb, ybb], where yBB, yBb, and ybb are the
observed number of individuals with genotype BB, Bb, and bb, respectively,
in a sample of size n = yBB + yBb + ybb. Let the unknown probabilities of
drawing a BB, Bb, or bb individual be θBB, θBb, and θbb, respectively. If
the sample size is ﬁxed by design, and individuals are drawn independently
and with replacement, it may be reasonable to compute the probability of

4.3 Evaluation of Hypotheses
175
observing y using the multinomial distribution. This, of course, would ig-
nore knowledge about family aggregation that may exist in the population.
For example, if some parents are both bb, their progeny must be bb, nec-
essarily, so the random sampling model discussed here would not take this
into account. Under multinomial sampling, the probability of observing y
is
p (yBB, yBb, ybb) =
y!
yBB! yBb! ybb! (θBB)yBB (θBb)yBb (θbb)ybb
so the likelihood is
L (θBB, θBb, θbb) ∝(θBB)yBB (θBb)yBb (θbb)ybb .
The model imposes the parametric restriction θBB +θBb +θbb = 1 so there
are only two “free” parameters governing the distribution. Also, ybb =
n −yBB −yBb, say. The likelihood is then reexpressible as
L (θBB, θBb) ∝θyBB
BB θyBb
Bb (1 −θBB −θBb)n−yBB−yBb .
Diﬀerentiation of the log-likelihood with respect to the two θ′s, and setting
the derivatives to zero, gives the relationships
5θBB =
yBB
n −yBB −yBb

1 −5θBB −5θBb

,
5θBb =
yBb
n −yBB −yBb

1 −5θBB −5θBb

.
Summing these two equations yields
5θBB + 5θBb =
yBB + yBb
n −yBB −yBb

1 −5θBB −5θBb

.
Because of the parametric relationship (probabilities of all possible disjoint
events sum to one), the invariance property of the ML estimates yields
5θbb = 1 −5θBB −5θBb. Using this above
1 −5θbb =
yBB + yBb
n −yBB −yBb
5θbb,
from which: 5θbb = ybb/n. Similarly, it can be established that 5θBB = yBB/n
and 5θBb = yBb/n. In the absence of restrictions (other than those imposed
by probability theory) on the values of the parameters, i.e., under H1, the
maximized likelihood is
L

5θBB,5θBb,5θbb

∝
yBB
n
yBB yBb
n
yBb ybb
n
ybb .
A genetic hypothesis (denoted as H0 here) is that the population is in
Hardy–Weinberg equilibrium (Crow and Kimura, 1970; Weir, 1996). Under

176
4. Further Topics in Likelihood Inference
H0, the genotypic distribution is expected to obey the parametric relation-
ship
θBB =

θBB + 1
2θBb
2 = θ2
B,
θBb = 2θB (1 −θB) ,
θbb = (1 −θB)2 ,
where θB = θBB + θBb/2 is called the frequency of allele B in the popula-
tion. This is the total probability of drawing an allele B at this locus, that
is, the sum of:
a) the probability of drawing an individual with genotype BB (θBB) times
the probability of obtaining a B from BB, which is a certain event; plus
b) the probability of drawing a Bb (θBb) , times the probability of obtaining
B from a heterozygote
 1
2

.
Hence, under Hardy–Weinberg equilibrium the probability distribution of
the observations is governed by a single parameter, θB. The likelihood un-
der H0 is
L (θB) ∝

θ2
B
yBB [θB (1 −θB)]yBb 
(1 −θB)2ybb .
Diﬀerentiation of the log-likelihood with respect to θB and setting to zero
gives an explicit solution that can be expressed as a function of the ML
estimators under H1:
55θB = 2yBB + yBb
2y
= 5θBB + 1
2
5θBb.
Because of the parametric relationships, under H0 the ML estimators of
the probabilities of genotypes in the population (or genotypic frequencies,
in the population genetics literature) are:
55θBB = 55θ
2
B =

5θBB + 1
25θBb
2
,
55θBb = 2

5θBB + 1
25θBb
 
5θbb + 1
25θBb

,
55θbb =

5θbb + 1
25θBb
2
.

4.3 Evaluation of Hypotheses
177
Using (4.23), the statistic for the likelihood ratio test of the hypothesis that
the population is in Hardy–Weinberg equilibrium, is
−2 ln (LR) = −2 ln








5θBB + 1
25θBb
2
5θBB


yBB
×


2

5θBB + 1
25θBb
 
5θbb + 1
25θBb

5θBb


yBb 


5θbb + 1
25θBb
2
5θbb


yBb 






.
Asymptotically, this has a central χ2
1 distribution. In the general setting
described in Section 4.3, the parameter vector θ has r + s identiﬁable pa-
rameters under H1; here r+s = 2 instead of 3 because there is a redundant
parameter. Under H0 there is a single estimable parameter. This provides
the basis for the single degree of freedom of the distribution of the LR
statistic.
It is possible to move from the unrestricted to the restricted model by
setting a single function of parameters to zero. For example, letting
d = θBb −2

θBB + 1
2θBb
 
θbb + 1
2θBb

it follows that a test of the Hardy–Weinberg equilibrium is equivalent to
a test of the hypothesis H0: d = 0. Hence, the models under the two hy-
potheses diﬀer by a single parameter which, upon setting to 0, can produce
the null model as a “nested” version of the unrestricted model.
■
4.3.2
Conﬁdence Regions
The asymptotic distribution of ML estimators enables one to obtain inter-
val inferences about the “true” value of the parameters. These inferences
are expressed in terms of conﬁdence regions. There is a close relationship
between the techniques needed here and those described for the evaluation
of hypotheses. Hence, construction of conﬁdence regions is dealt with only
brieﬂy.
Before we do so, a comment about the interpretation of conﬁdence inter-
vals is in order. Conﬁdence intervals are deﬁned in terms of the distribution
of the random variable y, the data, and as such the conﬁdence interval is
also a random variable. Given a realization of y, the conﬁdence interval
will either contain the true value of the parameter or not. If the conﬁdence
region is 1 −α (see below), then under repeated sampling, 100 (1 −α) %
of the intervals will contain the true value of the parameter. Thus the con-
ﬁdence interval is not a probability statement about the parameter but
about the random interval.

178
4. Further Topics in Likelihood Inference
Given that a model or a hypothesis is true, the score has, asymptotically,
a normal distribution with null mean and covariance matrix I (θ) . Hence,
a conﬁdence region of size 1−α can be constructed from the property that
Pr

l′ (θ)′ I−1 (θ) l′ (θ) ≤tα

= 1 −α,
(4.27)
where tα is the critical value of a χ2
p random variable. Values of θ outside
the region are viewed as being unlikely, but not in a probabilistic sense,
because values of the parameters cannot be assigned probabilities. Note
that if the statement involves a single parameter, the conﬁdence region can
be formed from a standard normal distribution, this being so because the
random variable
[l′ (θ)]2
I (θ)
∼χ2
1
so
l′ (θ)
>
I (θ)
∼N (0, 1) .
Another way of constructing conﬁdence regions is based on the asymp-
totic distribution of the ML estimator, as given in (3.66) of the previous
chapter. The conﬁdence region stems from the fact that, asymptotically,
the distribution under θ = θ0 is

5θ −θ0
′
I (θ0)

5θ −θ0

∼χ2
p.
(4.28)
In the single parameter case, the region is deﬁned by the appropriate α/2
percentiles of a N (0, 1) distribution. This is so because
5θ −θ0
>
I−1 (θ0)
=

5θ −θ0
 >
I (θ0) ∼N (0, 1) .
Similarly, a conﬁdence region can be developed from the asymptotic dis-
tribution of the likelihood ratio as given in (4.23). A conﬁdence region
here is a set of values of θ in the neighborhood of the maximum (Cox
and Snell, 1989). With a single parameter, the asymptotic distribution
of the likelihood ratio statistic −2 ln (LR) is χ2
1 under H0. Noting that
a χ2
1 random variable can be generated by squaring a standard normal
deviate, an equivalent form of generating a conﬁdence region is to use
>
−2 ln (LR) ∼N (0, 1) if the ML estimator is larger than the null value
θ0, and −
>
−2 ln (LR) ∼N (0, 1) otherwise. This is because if x and −x
are realized values from a N (0, 1) process, these two generate the same
realized value from a χ2
1 distribution, in a two-to-one mapping.
In passing, we mention that the “pure likelihoodist” computes conﬁdence
regions based on quantiles derived from the likelihood ratio directly, with-
out invoking its distribution over replications of the data. A tutorial on the
topic can be found in Meeker and Escobar (1995).
This section ﬁnishes with a brief description of two tests that are also
based on classical, ﬁrst-order asymptotic likelihood theory.

4.3 Evaluation of Hypotheses
179
4.3.3
Wald’s Test
As stated in the previous chapter, in large samples, the distribution of the
ML estimator can be written (informally) as 5θ ∼N

θ0, I−1 (θ0)

, where
θ0 is the true value of the parameter θ. Then, asymptotically, the quadratic
form
λW =

5θ1 −θ10
′ 
I11 
5θ
−1 
5θ1 −θ10

∼χ2
r,
(4.29)
where 5θ1 is the ML estimator of θ1 under H1, and χ2
r is a central chi-square
random variable with r degrees of freedom (the number of elements in θ1).
Result (4.29) is an immediate application of (1.101). If λW is “too large”,
the hypothesis is rejected. Approximate conﬁdence regions can be readily
constructed from (4.29).
In (4.29), I11 
5θ

is the top left element of the inverse of Fisher’s expected
information matrix. That is
I

5θ

= −E

∂2l
∂θ ∂θ′

θ=θ
=

I11

5θ

I12

5θ

I21

5θ

I22

5θ


,
(4.30)
and

I11

5θ

I12

5θ

I21

5θ

I22

5θ



−1
=

I11 
5θ

I12 
5θ

I21 
5θ

I22 
5θ


.
Therefore,

I11 
5θ
−1
= I11

5θ

−I12

5θ
 
I22

5θ
−1
I21

5θ

.
Rather than using the inverse of Fisher’s information, other estimators
of I11 (θ), which are consistent under H0, can be used. Thus, an alterna-
tive form of Wald’s statistic uses the inverse of the observed information,
J11 
5θ

, in (4.29), which retrieves (4.22). This makes it clear that the Wald
statistic is based on a quadratic approximation to −2 ln LR.
Due to the relationship between a chi-squared variable and a normal
variable, for scalar θ, the 100 (1 −α) % conﬁdence interval based on the
Wald statistic is given by the well known formula
5θ ± zα/2I−1/2 
5θ

.
4.3.4
Score Test
The score test (or Lagrange multiplier test, as called by econometricians) is
also based on a quadratic approximation to the log-likelihood ratio. It was

180
4. Further Topics in Likelihood Inference
proposed by Rao (1947) and uses the asymptotic properties of the score
(3.63). The score statistic is
l′
θ1

θ10, ,θ2
′
I11 
θ10, ,θ2

l′
θ1

θ10, ,θ2

,
(4.31)
which, in view of (3.63) and of (1.101), follows a χ2
r distribution. The null
hypothesis is rejected for large values of (4.31). In (4.31),
l′
θ1

θ10, ,θ2

= ∂l (θ1, θ2|y)
∂θ1
""""θ1=θ10
θ2=θ2
.
Contrary to the likelihood ratio test and the Wald test, (4.31) requires only
one maximization (to compute the ML estimate under H0, ,θ2). Approxi-
mate conﬁdence regions can also be constructed from (4.31).
The three tests described here (likelihood ratio, Wald’s, and score test)
are asymptotically equivalent and are all ﬁrst-order approximations. Which
test to use depends on the situation (Lehmann, 1999), although Meeker and
Escobar (1995) and Pawitan (2000) argue in favor of the likelihood ratio.
There are two reasons. First, the Wald and score statistics are convenient
only if the log-likelihood is well-approximated by a quadratic function.
Second, the Wald statistic has an important disadvantage relative to the
likelihood ratio test: it is not transformation invariant. However, asymp-
totically, when the likelihood is a quadratic function of the parameter, the
tests are equivalent. To illustrate this in the case of a single parameter,
write the scaled log-likelihood (i.e., the logarithm of (3.21)) as
l (θ) = k

θ −5θ
2
.
Therefore,
dl (θ)
dθ
= 2k

θ −5θ

and
d2l (θ)
d2θ
= −I (θ) = 2k.
Assume that under the null hypothesis, H0 : θ = θ0 and the alternative
hypothesis is H1 : θ ̸= θ0. Then the test based on the likelihood ratio is
2

l

5θ

−l (θ0)

= 2

k

5θ −5θ
2
−k

θ0 −5θ
2
= −2k

θ0 −5θ
2
.
The Wald test is obtained from (4.29):

5θ −θ0
2
I−1 (θ)θ=θ = −2k

θ0 −5θ
2
.

4.4 Nuisance Parameters
181
The score test is obtained from (4.31):
[dl (θ0) /dθ]2
I (θ0)
=

2k

θ0 −5θ
2
−2k
= −2k

θ0 −5θ
2
.
4.4
Nuisance Parameters
Recall that a statistical model typically includes parameters of primary in-
ferential interest (denoted as θ1 here), plus additional parameters (θ2) that
are necessary to index completely the distributions of all random variables
entering into a probability model. The additional parameters are called
nuisance parameters. As pointed out by Edwards (1992), one would wish
to make statements about the values of θ1 without reference to the values
of θ2. If θ2 were known, there would be no diﬃculty, as one would write the
likelihood of θ1, with θ2 replaced by its true value. In the absence of such
knowledge, a possibility would be to infer θ1 at each of a series of possible
values of θ2. This is unsatisfactory, because it does not give guidance about
the plausibility of each of the values of the nuisance parameters. A more
appealing option is to estimate θ1 and θ2 jointly, as if both were of pri-
mary interest, using the machinery for analysis of likelihoods developed so
far. Unfortunately, when making inferences about θ1, this modus operandi
does not take into account the fact that part of the information contained
in the data must be used to estimate θ2. For example, in a nonlinear re-
gression model, the vector of parameters of the expectation function may
be of primary interest, with the residual variance playing the role of a nui-
sance parameter. Another example is that of a model with two means and
two variances, where the inferential interest centers on, say, σ2/µ1, with
the remaining parameters acting as nuisances. In any case, it is not ob-
vious how to deal with nuisance parameters in likelihood-based inference
and many solutions have been proposed. This area has been undergoing
rapid development (Kalbﬂeisch and Sprott, 1970, 1973; Barndorﬀ-Nielsen,
1986, 1991; Cox and Reid, 1987; McCullagh and Nelder, 1989; Efron, 1993;
Barndorﬀ-Nielsen and Cox, 1994; Severini, 1998). Useful reviews can be
found in Reid (1995) and Reid (2000). The recent book of Severini (2000)
is a good starting point to study modern likelihood methods. Here we only
discuss the use of marginal and proﬁle likelihoods.
The subject is introduced below with an example that illustrates the loss
of eﬃciency in the estimation of parameters of interest due to the presence
of nuisance parameters. The material is taken from Lehmann (1999).

182
4. Further Topics in Likelihood Inference
4.4.1
Loss of Eﬃciency Due to Nuisance Parameters
Consider a model depending on p parameters θ with elements
θ1, . . . , θp.
Let I (θ) denote the information matrix, with typical element Iij (θ) and
with inverse [I (θ)]−1 with typical element Iij (θ). Assuming that the nec-
essary regularity conditions hold, then from (3.65),
√n

5θ1 −θ1

, . . . , √n

5θk −θp

has a joint multivariate distribution with mean (0, 0, . . . , 0)′ and covariance
matrix [I (θ)]−1. In particular,
√n

5θj −θj

→N

0, Ijj (θ)

,
where Ijj (θ) is the element in row j and column j of [I (θ)]−1. On the
other hand, if θ1, . . . , θj−1, θj+1, . . . , θp are known, from (3.55),
√n

5θj −θj

→N

0, [Ijj (θ)]−1
.
The loss in eﬃciency due to the presence of nuisance parameters can be
studied via the relationship between Ijj (θ) and [Ijj (θ)]−1. Consider the
case p = 2 and j = 1. The inverse of the 2 × 2 matrix I (θ) is
[I (θ)]−1 =

I22 (θ) /∆
−I12 (θ) /∆
−I12 (θ) /∆
I11 (θ) /∆

,
where ∆= I11 (θ) I22 (θ) −[I12 (θ)]2. Since I (θ) is positive deﬁnite, ∆is
positive. This implies that I11 (θ) I22 (θ) ≥∆which is equivalent to
I11 (θ) = I22 (θ)
∆
≥[I11 (θ)]−1 ,
(4.32)
with equality holding when I12 (θ) = 0. The conclusion is that, even in an
asymptotic scenario, unless the estimators are asymptotically independent,
the asymptotic variance of the estimator of the parameter of interest is
larger in models containing unknown nuisance parameters.
4.4.2
Marginal Likelihoods
The marginal likelihood approach is based on the construction of a likeli-
hood for the parameters of interest from a “suitably chosen subset of the
data vector” (McCullagh and Nelder, 1989). It is desirable to choose this
subset to be as large as possible, to minimize any loss of information. As

4.4 Nuisance Parameters
183
indicated by McCullagh and Nelder (1989), the method does not always
work satisfactorily because general rules do not seem to exist. Even in cases
where it appears to work acceptably (e.g., elimination of location parame-
ters in a Gaussian linear model), it is diﬃcult to evaluate whether or not
there is a loss of information in the process of the eliminating parameters.
This is illustrated below.
Consider the sampling model studied in Example 4.1, and suppose the
parameter of primary interest is θ1 = σ2 whereas the nuisance parameter
is the location vector θ2 = β. A likelihood that does not involve β can be
constructed from a vector of ﬁtted residuals, also called “error” contrasts,
as suggested by Patterson and Thompson (1971). Let
w = y −X

X′V−1X
−1 X′V−1y
=

I −X

X′V−1X
−1 X′V−1
y
= My
(4.33)
for M =[I −X

X′V−1X
−1 X′V−1] of order N×N. If y is normal, w must
be normal by virtue of it being a linear combination of normal variables.
Further
E (w) = Xβ −X

X′V−1X
−1 X′V−1Xβ = 0
(4.34)
and:
V ar (w) = MVM′σ2.
(4.35)
Observe that
rank [V ar (w)] ≤rank [M]
and that M is an idempotent matrix. Using properties of idempotent ma-
trices plus cyclical commutation under the trace operator (Searle, 1982)
rank [M] = tr [M] = tr

I −X

X′V−1X
−1 X′V−1
= N −tr

X′V−1X
−1 X′V−1X

= N −p,
where p is the order of β. Hence, V ar (w) has deﬁcient rank and
w ∼SN

0, MVM′σ2
,
(4.36)
where SN denotes a singular normal distribution having a covariance ma-
trix of rank N −p (Searle, 1971). This means that p of the elements of
w are redundant and can be expressed as a linear function of the N −p
linearly independent combinations. Put
w =

wL
wR

=
 ML
MR

y =
 MLy
MRy

,

184
4. Further Topics in Likelihood Inference
where wL (wR) stands for a linearly independent (redundant) part of w,
and ML (MR) denotes the corresponding partition of M.
Consider now the distribution of wL. From (4.34) and (4.35), this distri-
bution wL must be normal with a nonsingular covariance matrix
wL ∼N

0, MLVM′
Lσ2
.
(4.37)
This distribution depends on σ2 only, so it is “free” of the nuisance param-
eter β. Hence, a likelihood function devoid of β can be constructed based
on wL. This marginal likelihood can be written as
L

σ2
∝
""MLVM′
Lσ2""−1
2 exp

−1
2σ2 w′
L (MLVML)−1 wL
%
∝

σ2−N−p
2
exp

−1
2σ2 y′M′
L

MLVM′
L
−1 MLy
%
.
(4.38)
A result in linear algebra (Searle et al., 1992) states that if MLX = 0 and
V is positive deﬁnite, two conditions met here, then
M′
L

MLVM′
L
−1 ML = V−1M
and this holds for any ML having full row rank. Using the preceding in
(4.38), the marginal log-likelihood function is
l

σ2
= −N −p
2
ln

σ2
−
1
2σ2 y′V−1My
= −N −p
2
ln

σ2
−
1
2σ2

y′V−1y−5β
′X′V−1y

= −N −p
2
ln

σ2
−
1
2σ2

y −X5β
′
V−1 
y −X5β

,
(4.39)
where 5β =

X′V−1X
−1 X′V−1y. Maximization of (4.39) with respect to
σ2 gives, as the marginal ML estimator,
,σ2 =

y −X5β
′
V−1 
y −X5β

N −p
=
Sy
N −p.
(4.40)
Writing
,σ2 = Sy
σ2
σ2
N −p,
and noting that Sy/σ2 ∼χ2
N−p, it follows that the marginal ML estimator
has a scaled chi-square distribution, with mean and variance
E

,σ2
=
σ2
N −p (N −p) = σ2,
(4.41)
V ar

,σ2
=

σ2
N −p
2
2 (N −p) =
2σ4
N −p.
(4.42)

4.4 Nuisance Parameters
185
This should be contrasted with the ML estimator:
6
σ2 = Sy
N
(4.43)
that also has a scaled chi-square distribution with mean and variance
E

6
σ2

= σ2 N −p
N
,
(4.44)
V ar

6
σ2

= 2σ4
N .
(4.45)
The marginal ML estimator ,σ2 is unbiased, whereas 6
σ2 has a downward
bias. This bias can be severe if p is large relative to N. On the other hand,
the ML estimator is more precise (lower variance) than the estimator based
on a marginal likelihood, suggesting that some information is lost in the
process of eliminating the nuisance parameter β. This can be checked by
computing the information about σ2 contained in the marginal likelihood
(4.38). Diﬀerentiating the marginal log-likelihood twice with respect to σ2,
and multiplying by −1, gives the observed information
−N −p
2σ4
+ Sy
σ6 .
The expected information is
IM

σ2
= −N −p
2σ4
+ (N −p) σ2
σ6
= N −p
2σ4 .
(4.46)
Using a similar procedure, the information from the full likelihood can be
found to be equal to
I

σ2
= N
2σ4 .
(4.47)
A comparison between (4.46) and (4.47) indicates that there is a loss of
information in the process of eliminating the nuisance parameter, at least
in the situation considered here. The loss of information can be serious in
a model where p/N is large. The corresponding asymptotic distributions
are then ,σ2 ∼N

0, 2σ4/(N −p)

and 6
σ2 ∼N

0, 2σ4/N

. This fact would
seem to favor the ML estimator. However, for very large N (relative to p)
the two distributions are expected to diﬀer by little.
These asymptotic distributions do not give guidance on how to choose
between the estimators when samples are ﬁnite. Here, it was shown that
the two ML estimators have distributions that are multiples of chi-square
random variables but with diﬀerent means and variances. Consider a com-
parison based on the mean squared error criterion. The mean squared error

186
4. Further Topics in Likelihood Inference
of an estimator 5θ is:
E

5θ −θ
2
= E

5θ −E

5θ

+ E

5θ

−θ
2
= E

5θ −E

5θ
2
+

E

5θ

−θ
2
= V ar

5θ

+ Bias2 
5θ

,
(4.48)
where Bias

5θ

= E

5θ

−θ gives the expected deviation of the average
of the realized value of the estimators from the true parameter value in
a process of conceptual repeated sampling. The marginal ML estimator
has null bias (see equation 4.41) so its mean squared error is equal to
its variance, as given in (4.42). Using expressions (4.44) and (4.45), the
corresponding mean squared error of the ML estimator is
E

6
σ2 −σ2
=
2σ4
N −p

1 −k + k2 N −p
2

,
where k = p/N. From this, conditions can be found under which one of the
two estimators is “better” than the other in terms of mean squared error.
For example, if p = 1, that is, if the model has a single location parameter,
the ML estimator has a smaller mean squared error than the estimator
based on marginal likelihood throughout the parameter space. For this
particular model and loss function (mean squared error), the marginal ML
estimator is said to be inadmissible, because it is known that a better
estimator exists for all values of σ2. Unfortunately, these calculations are
not possible in more complicated models, for example, Gaussian mixed
eﬀects models with unknown variance components. In general, the choice
of the likelihood to be maximized is not obvious.
The idea of using a subset of the data (error contrasts) to make inferences
about variance components in a linear model was suggested by Patterson
and Thompson (1971). These authors used the term restricted likelihood
instead of marginal likelihood, and called the resulting estimates REML for
short. It is unclear how this idea can be generalized to other parameters
of a linear or nonlinear model. The Bayesian approach, on the other hand,
provides a completely general form of elimination of nuisance parameters.
This will be discussed in the following chapter.
4.4.3
Proﬁle Likelihoods
When a model has nuisance parameters, it is possible to deﬁne a likelihood
that can be used almost invariably, but not without pitfalls. It is known
as a proﬁle likelihood. In order to introduce the concept, consider a model
with parameters (θ1, θ2), where θ2 is regarded as a vector of nuisance

4.4 Nuisance Parameters
187
parameters. Observe that ML estimation equations must satisfy
∂l (θ1, θ2|y)
∂θ1
= 0,
∂l (θ1, θ2|y)
∂θ2
= 0.
The ﬁrst equation deﬁnes the ML estimator of θ1 at a ﬁxed value of the
nuisance parameter, and vice-versa for the second equation. Let 5θ2|1 be the
partial ML estimator of the nuisance parameter obtained from the second
equation, that is, with θ1 ﬁxed. This “partial” estimator depends only on
the data and θ1. The proﬁle log-likelihood of θ1 is obtained by replacing
θ2 by 5θ2|1 in the likelihood function, and is deﬁned as
lP

θ1, 5θ2|1|y

.
(4.49)
Note that (4.49) has the same form as the numerator of the likelihood ratio
(4.11), which can also be viewed as a proﬁle likelihood ratio. Expression
(4.49) is a function of θ1 only, and its maximizer must be such that
∂lP

θ1, 5θ2|1|y

∂θ1
= 0.
It follows that the maximizer of the proﬁle likelihood must be identical to
the ML estimator of θ1 obtained from l (θ1, θ2|y) , this being so because
the ﬁrst of the two ML estimating equations given above is satisﬁed by
5θ2|1.
In Subsection 4.3.1, the nuisance parameter θ2 was estimated condition-
ally on a given value of θ1. This conditional estimator, labelled ,θ2, was
replaced in l (θ1, θ2|y) and the latter evaluated at θ1 = θ10. This led to
expression (4.21) for l

θ10, ,θ2|y

. If instead, θ1 is left free to vary, one
obtains
lP

θ1, ,θ2|y

≈constant −1
2

5θ1 −θ1
′ 
J11 
5θ
−1 
5θ1 −θ1

, (4.50)
which is an asymptotic ﬁrst-order approximation to the proﬁle log-likelihood.
Then the standard results that led to (3.66) hold here also, and we can write
(informally),
5θ1 ∼N

θ1, I11 
5θ

.
(4.51)
In view of (4.32), this shows that the asymptotic variance in the presence
of nuisance parameters is larger than when these are absent or assumed
known, and the proﬁle likelihood accounts for this extra uncertainty.
However, a word of caution is necessary. Although it would seem that
the ML machinery can be applied in a straightforward manner using a

188
4. Further Topics in Likelihood Inference
proﬁle likelihood, this is not always the case. The proﬁle likelihood is not
proportional to the density function of a random variable. In large sam-
ples, replacing θ2 by its ML estimate has relatively little consequences for
inferences involving θ1. However, if the dimension of θ2 is large relative
to sample size, a situation in which it would be diﬃcult to argue asymp-
totically, the proﬁle log-likelihood can be misleading when interpreted as a
log-likelihood function (McCullagh and Nelder, 1989; Cox and Snell, 1989).
Example 4.4
Proﬁle likelihoods in a linear model
Let the joint distribution of the observations be
y ∼N

X1β1 + X2β2, Vσ2
where β1, β2, and σ2 are the unknown parameters and V is a known
matrix. The likelihood function is
L

β1, β2,σ2|y

= (2π)−N
2 ""Vσ2""−1
2
× exp

−1
2σ2 (y −X1β1 −X2β2)′ V−1 (y −X1β1 −X2β2)

.
It will be shown here how diﬀerent proﬁle likelihoods are constructed.
(a) Suppose that the nuisance parameter is σ2 and that inferences are
sought about the location vectors β1 and β2. The partial ML estimator of
σ2 is
−→
σ 2=(y −X1β1 −X2β2)′ V−1 (y −X1β1 −X2β2)
N
.
Replacing this in the likelihood gives the proﬁle likelihood of β1 and β2:
LP (β1, β2|y) ∝
−→
σ 2−N
2 exp

−N
2

∝
−→
σ 2−N
2
and this does not depend on the nuisance parameter σ2. The corresponding
proﬁle log-likelihood, ignoring the constant, is
lP (β1, β2|y) = −N
2 ln

(y −X1β1 −X2β2)′ V−1 (y −X1β1 −X2β2)

.
Setting the ﬁrst derivatives with respect to the unknowns to zero and re-
arranging gives as solution

5β1
5β2

=
 X′
1V−1X1
X′
1V−1X2
X′
2V−1X1
X′
2V−1X2
−1  X′
1V−1y
X′
2V−1y

,
retrieving the ML estimator, as expected. In this case the proﬁle likelihood
has the same asymptotic properties as the usual likelihood, this being so
because −→
σ 2 is a consistent estimator of σ2. This can be veriﬁed by noting

4.4 Nuisance Parameters
189
that V can be decomposed as V = LL′ with nonsingular L. Hence, for
y∗= (L)−1 y, then
y∗∼N

(L)−1 (X1β1 + X2β2) , Iσ2
,
so the transformed observations are independent. Putting (L)−1 Xi = X∗
i ,
and letting x∗′
ij denote the jth row of X∗
i , it turns out that the partial ML
estimator can be written as
−→
σ 2 = 1
N
N

j=1

y∗
j −x∗′
1jβ1 −x∗′
2jβ2
2 .
It follows that as N →∞, then
−→
σ 2→E

y∗
j −x∗′
1jβ1 −x∗′
2jβ2
2
= σ2,
because of the law of large numbers.
(b) Let β1 now be of primary interest, with β2 and σ2 acting as nuisance
parameters. The partial ML estimators of β2 and σ2 are
←−
β 2 =

X′
2V−1X2
−1 X′
2V−1 (y −X1β1)
and
←−
σ 2 =

y −X1β1 −X2
←−
β 2
′
V−1 
y −X1β1 −X2
←−
β 2

N
.
Note that ←−
β 2 ∼N[β2,

X′
2V−1X2
−1 σ2] is the ML estimator of β2 applied
to the “corrected” data y −X1β1, so it must converge in probability to β2.
Hence, ←−
σ 2 must be consistent as well. The proﬁle likelihood for this case
is
LP (β1|, y) ∝
←−
σ 2−N
2
× exp

−
1
2←−
σ 2

y −X1β1 −X2
←−
β 2
′
V−1 
y −X1β1 −X2
←−
β 2

.
(c) The parameter of interest now is σ2. The partial ML estimators of
the nuisance parameters are 5β1 and 5β2, as with full ML. This is so because
solving the full ML equations for β1 and β2 does not require knowledge of
σ2. The proﬁle likelihood is then
LP

σ2|y

∝

σ2−N
2
× exp

−1
2σ2

y −X15β1 −X25β2
′
V−1 
y −X15β1 −X25β2

.

190
4. Further Topics in Likelihood Inference
It can be shown readily that the maximizer of this proﬁle likelihood is
identical to the ML estimator of σ2, as it should be. The identity between
the ML and the partial ML estimators of the nuisance parameters β1 and
β2 indicates that these must be consistent, so the proﬁle likelihood can be
used as a full likelihood to obtain inferences about σ2. For example, the
information measure based on the proﬁle likelihood is
IP

σ2
= E

−∂2 ln LP

σ2|y

(∂σ2)2

= −N
2σ4 +
E

y −X15β1 −X25β2
′
V−1 
y −X15β1 −X25β2

σ6
= −N
2σ4 + (N −p) σ2
σ6
= N
2σ4

1 −2p
N

.
The expectation taken above can be evaluated using (4.41). Note that this
measure indicates that account is taken of the information lost in elim-
inating the nuisance parameters. Observe also that there is information,
provided that p < N/2. It is of interest to compare the information in the
proﬁle likelihood with the information resulting from the marginal likeli-
hood, as given in (4.46) and reexpressible as
IM

σ2
= N
2σ4

1 −p
N

.
If one accepts the marginal likelihood as the “correct” way of accounting
for nuisance parameters, this example illustrates that proﬁle likelihoods do
not always do so in the same manner.
■
4.5
Analysis of a Multinomial Distribution
The multinomial sampling process was applied previously in connection
with the evaluation of the hypothesis that a population is in Hardy–Wein-
berg equilibrium; see Example 4.3. Inferences about the parameters of a
multinomial distribution will be dealt with here in a more general manner.
The objective is to illustrate the application of ML methods to problems
other than those arising in the classical linear model.
The data are counts ni (i = 1, 2, ..., T) in each of T mutually exclusive
and exhaustive classes. The draws are made independently of each other
from the same distribution, the only restriction being that the total num-
ber of draws, n = T
i=1 ni, is ﬁxed by sampling. The unknown probability
that an observation falls in class i is denoted as θi. These unknown pa-
rameters are subject to the natural restriction that T
i=1 θi = 1, because
the probability that an observation falls in at least one class must be equal

4.5 Analysis of a Multinomial Distribution
191
to 1, and this observation cannot fall in two or more classes. As seen in
Chapter 1, under multinomial sampling the joint probability function of
the observations is
Pr (n1, n2, ..., nT |θ1, θ2, ..., θT ) = n!
T
-
i=1
θni
i
ni! .
Given n, there are only T −1 independent n′
is so this is a multivariate
distribution of dimension T −1, having T −1 free parameters θi. When
T = 2, the multinomial distribution yields the binomial distribution as a
particular case. In order to implement the ML machinery, the means and
variances of the n′
is, as well as their covariances, are needed. These are:
E (ni) = nθi,
V ar (ni) = nθi (1 −θi) ,
and
Cov (ni, nj) = −nθiθj.
The likelihood function is
L (θ1, θ2, ..., θT |n1, n2, ..., nT ) ∝
T
-
i=1
θni
i .
(4.52)
Because there are only T −1 free parameters and independent counts,
θT = 1 −T −1
i=1 θi and nT = n −T −1
i=1 ni. Employing this in the likeli-
hood function and taking logs, the log-likelihood is, apart from an additive
constant
l (θ1, θ2, ..., θT −1|n1, n2, ..., nT )
=
T −1

i=1
ni ln (θi) +

n −
T −1

i=1
ni

ln

1 −
T −1

i=1
θi

.
(4.53)
Diﬀerentiating with respect to θi gives the score
∂l (θ1, θ2, ..., θT −1|n1, n2, ..., nT )
∂θi
= ni
θi
−

n −T −1
i=1 ni


1 −T −1
i=1 θi

= ni
θi
−nT
θT
,
i = 1, 2, ..., T −1.
(4.54)
Setting this to 0 gives
5θi = ni
nT
5θT ,
i = 1, 2, ..., T −1.

192
4. Further Topics in Likelihood Inference
Because the ML estimates must be constrained to reside in the interior of
the parameter space, it must be that T
i=1 5θi = 1. Summing the above over
the T ML estimates of probabilities gives
5θT = nT
n .
Hence,
5θi = ni
n ,
i = 1, 2, ..., T,
(4.55)
so the proportions of observations falling in class i gives the ML estimates of
the corresponding probability directly. It can be veriﬁed that the estimator
is unbiased because
E

5θi

= E
ni
n

= nθi
n
= θi.
Diﬀerentiating (4.54) again with respect to θi (taking into account the fact
that the last parameter is a function of the T −1 probabilities for the
preceding categories):
∂2l (θ1, θ2, ..., θT −1|n1, n2, ..., nT )
(∂θi)2
= −ni
θ2
i
−
nT

1 −
T −1

i=1
θi
2 ,
i = 1, 2, ..., T −1.
and
∂2l (θ1, θ2, ..., θT −1|n1, n2, ..., nT )
∂θi ∂θj
= −
nT

1 −
T −1

i=1
θi
2 ,
i, j = 1, 2, ..., T −1, i ̸= j.
The second derivatives multiplied by −1 give the elements of the informa-
tion matrix about the free parameters of the model. Taking expectations
yields Fisher’s information matrix, with elements
I (i, i) = n

 1
θi
+ 1
θT

,
i = 1, 2, ..., T −1,
(4.56)
and:
I (i, j) = n
θT
,
i, j = 1, 2, ..., T −1, i ̸= j.
(4.57)
The information is proportional to sample size. The inverse of the infor-
mation matrix gives the asymptotic variance covariance matrix of the ML
estimates. This inverse can be shown to have elements
v (i, i) = 1
nθi (1 −θi) ,
i = 1, 2, ..., T −1,
(4.58)
v (i, j) = −1
nθiθj,
i, j = 1, 2, ..., T −1, i ̸= j.
(4.59)

4.5 Analysis of a Multinomial Distribution
193
These coincide with the exact variances and covariances, as given earlier.
Hence, the ML estimator attains the Cram´er–Rao lower bound, and is a
minimum variance unbiased estimator in this case. The correlation between
parameter estimates is
Corr

5θi,5θj

= −
C
θiθj
(1 −θi) (1 −θj).
When T = 2, the multinomial model reduces to binomial sampling. The
ML estimator of the probability of response in the ﬁrst class, say, is the
proportion of observations falling in this category. The sampling variance
of the estimate is
V ar

5θ

= θ (1 −θ)
n
.
In all cases, because the θ′s are not known, parameter values must be
replaced by the ML estimates to obtain an approximation to the asymptotic
distribution. The goodness of this approximation is expected to improve as
n increases.
Example 4.5
Analysis of a trichotomy
Suppose the data consist of n = 100 observations collected at random
from a homogeneous populations of experimental chickens. Each bird is
scored for the presence or absence of leg deformities and, within deformed
individuals, there are two modalities. Hence, T = 3. Suppose the outcome
of the experiment is n1 = 20, n2 = 35, and n3 = 45. The objective is to
obtain ML estimates of the prevalence of each of the three modalities, and
of a non-linear function of the associated probabilities. Here we work with
θ1 and θ2 as free parameters, because θ3 is redundant. From (4.53) the
log-likelihood, apart from a constant, is
l (θ1, θ2|n1, n2, n3) = 20 ln (θ1) + 35 ln (θ2) + 45 ln (1 −θ1 −θ2) .
Using (4.55) the ML estimates are 5θ1 = .20, 5θ2 = .35, and 5θ3 = 1 −5θ1−
5θ2 = 1 −.20 −.35 = .45. From (4.58), the asymptotic standard deviation
of the estimate (equal to the standard deviation of the exact sampling
distribution in this case) of 5θ1 can be estimated as
?
(.20) (.80)
100
= 0.04.
Based on the ML estimator, a conﬁdence region based on two standard
deviations is .20 ± .08. The interval (0.12 −0.28) indicates that inferences
about the true value of θ1 are not very sharp in a sample of this size. How
sharp a conﬁdence region should be depends on the problem in question.
A similar calculation can be carried out for the probabilities of falling into

194
4. Further Topics in Likelihood Inference
any of the other two classes.
Suppose now that inferences are sought using the logit transform
wi = ln
θi
1 −θi
with inverse
θi =
exp (wi)
1 + exp (wi).
The logit is interpretable as a log-odds ratio, that is, the ratio between the
probabilities of “response” and of “nonresponse” measured on a logarithmic
scale. The logit is positive when the probability of “response” is larger than
the probability of the complementary event, and negative otherwise. While
the parameter space of θi is bounded between 0 and 1, that for the logit is
−∞< wi < ∞. The ML estimator of the logit is
5wi = ln
5θi
1 −5θi
.
Here,
5w1 = ln .20
.80 = −1.3863,
5w2 = ln .35
.65 = −0.6190,
5w3 = ln .45
.55 = −0.2007.
The variance of the asymptotic distribution of 5wi can be deduced from
the variance–covariance matrix of the asymptotic distribution of the ML
estimates of the probabilities. Because this is a multi-parameter problem,
one needs to form the information matrix about the logits. Using (3.74),
and working with two “free” logits only
I


w1
w2

=


dθ1
dw1
dθ2
dw1
dθ1
dw2
dθ2
dw2

I


θ1
θ2



dθ1
dw1
dθ1
dw2
dθ2
dw1
dθ2
dw2


=


dθ1
dw1
0
0
dθ2
dw2

I

 θ1
θ2



dθ1
dw1
0
0
dθ2
dw2

.
The asymptotic variance of the ML estimates of the logits is then:
V ar
 5w1
5w2

=


dθ1
dw1
0
0
dθ2
dw2


−1
I−1


θ1
θ2



dθ1
dw1
0
0
dθ2
dw2


−1
.

4.5 Analysis of a Multinomial Distribution
195
The derivatives are:
dθi
dwi
=
exp (wi)
1 + exp (wi)

1 −
exp (wi)
1 + exp (wi)

= θi (1 −θi) ,
so, using the asymptotic covariance matrix of the ML estimates of the
probabilities, with elements as in expressions (4.58) and (4.59),
V ar

5w1
5w2

= 1
n


1
θ1 (1 −θ1)
−
1
(1 −θ1) (1 −θ2)
−
1
(1 −θ1) (1 −θ2)
1
θ2 (1 −θ2)

.
The asymptotic correlation between the ML estimates of the logits is then:
Corr ( 5w1, 5w2) = −
C
θ1θ2
(1 −θ1) (1 −θ2).
In this example, the estimated variance–covariance matrix of the ML esti-
mates of the logits is
D
V ar

5w1
5w2

=
1
100


1
.16
−1
.07
−1
.07
1
.2275


=

.0625
−.1429
−.1429
.0440

.
■
Example 4.6
Estimation of allele frequencies with inbreeding
This example is adapted from Weir (1996). From population genetics the-
ory, it is known that if a population is in Hardy–Weinberg equilibrium,
and inbreeding is practiced, this causes a reduction in the frequency of
heterozygotes and a corresponding increase in homozygosity. Consider a
locus with segregating alleles A and a, so that three genotypes are pos-
sible: AA, Aa, and aa. As seen previously, if θ is the frequency of allele
A, with Hardy–Weinberg equilibrium, the frequency of the three diﬀerent
genotypes is Pr (AA) = θ2, Pr (Aa) = 2θ (1 −θ), and Pr (aa) = (1 −θ)2 .
With inbreeding, the genotypic distribution changes to
Pr (AA) = θ2 + θ (1 −θ) f,
Pr (Aa) = 2θ (1 −θ) (1 −f) ,
Pr (aa) = (1 −θ)2 + θ (1 −θ) f,
where f is the inbreeding coeﬃcient or, equivalently, the fractional reduc-
tion in heterozygosity. Suppose that nAA, nAa, and naa individuals having

196
4. Further Topics in Likelihood Inference
genotypes AA, Aa, and aa, respectively, are observed, and that the total
number of observations is ﬁxed by the sampling scheme. The unknown pa-
rameters are the allele frequency and the coeﬃcient of inbreeding. If the
individuals are sampled at random from the same conceptual population, a
multinomial sampling model would be reasonable. The likelihood function
can be expressed as
L (θ, f) ∝θnAA+nAa [θ + (1 −θ) f]nAA [(1 −f)]nAa
× [(1 −θ) + θf]naa (1 −θ)nAa+naa
where 1 −θ is the allelic frequency of a. The score vector has two ele-
ments, ∂l/∂θ and ∂l/∂f where, as usual, l is the log-likelihood function.
The elements are
∂l
∂θ = nAA + nAa
θ
+ nAA (1 −f)
θ + (1 −θ) f −naa (1 −f)
(1 −θ) + θf
−nAa + naa
(1 −θ)
and
∂l
∂f = nAA (1 −θ)
θ + (1 −θ) f −
nAa
(1 −f) +
naaθ
(1 −θ) + θf .
Setting the gradient to zero leads to a nonlinear system in θ and f that
does not have an explicit solution. Hence, second derivatives are needed not
only to complete the ML analysis, but to construct an iterative procedure
for obtaining the estimates. The second derivatives are
∂2l
(∂θ)2 = −nAA + nAa
θ2
−
nAA (1 −f)2
[θ + (1 −θ) f]2 −
naa (1 −f)2
[(1 −θ) + θf]2
−nAa + naa
(1 −θ)2 ,
∂2l
(∂f)2 = −nAA (1 −θ)2
[θ + (1 −θ) f]2 −
nAa
(1 −f)2 −
naaθ2
[(1 −θ) + θf]2 ,
and
∂2l
∂θ∂f = −
nAA
θ + (1 −θ) f −nAA (1 −θ) (1 −f)
[θ + (1 −θ) f]2
+
naa
(1 −θ) + θf
+ naa (1 −f) θ
[(1 −θ) + θf]2
= −
nAA
[θ + (1 −θ) f]2 +
naa
[(1 −θ) + θf]2 .

4.5 Analysis of a Multinomial Distribution
197
Genotype
Phenotype
Observed counts
Frequency
AA
A
nA
p2
A
AO
A
2pApO
AB
AB
nAB
2pApB
BB
B
nB
p2
B
BO
B
2pBpO
OO
O
nO
p2
O
TABLE 4.1. Frequency of genotypes and phenotypes of ABO blood group data.
From the ﬁrst and second derivatives, the Newton–Raphson algorithm can
be formed as presented in (4.1). The expected second derivatives are ob-
tained by replacing nAA, nAa, and naa by their expectations. For example
E (nAa) = 2θ (1 −θ) (1 −f) (nAA + nAa + naa) .
To illustrate, suppose that nAA = 100, nAa = 200, and naa = 200. The ML
estimates are 5θ = .40 and 5f = .1667, after round-oﬀ. From the Newton–
Raphson algorithm, the observed information matrix can be estimated as
5Io (θ, f) =


−∂2l
(∂θ)2
−∂2l
∂θ∂f
−∂2l
∂θ∂f
−∂2l
(∂f)2


θ=θ
f= 
f
.
This can be used in lieu of the expected information matrix to estimate the
asymptotic variance–covariance matrix of the estimates. In this example
5I−1
o
(θ, f) =
1
1000

.28001
.02777
.02777
1.98688

.
The asymptotic correlation between ML estimates is approximately .04.■
Example 4.7
ABO blood groups
Consider the following blood group data in Table 4.1. With three alleles,
A, B, and O there are six genotypes but only four phenotypic classes can
be observed. The expected frequency of each genotype in the last column
is derived assuming Hardy–Weinberg equilibrium. The problem at hand is
to infer pA, pB and pO, the frequency of alleles A, B, and O, respectively,
subject to the constraint pA + pB + pO = 1.
The observed data is y′ = (nA, nAB, nB, nO). The log-likelihood is given
by:
l (pA, pB|y) ∝nA ln [pA (2 −pA −2pB)] + nAB ln [2pApB]
+ nB ln [pB (2 −pB −2pA)] + 2nO ln [(1 −pA −pB)] .
(4.60)

198
4. Further Topics in Likelihood Inference
Diﬀerentiating with respect to pA and pB yields the nonlinear system of
equations
∂l (pA, pB|y)
∂pA
= nAB
pA
+ nA (2 −2pA −2pB)
pA (2 −pA −2pB) −
2nB
2 −2pA −pB
−
2nO
1 −pA −pB
,
(4.61)
∂l (pA, pB|y)
∂pB
= nAB
pB
+ nB (2 −2pA −2pB)
pB (2 −2pA −pB) −
2nA
2 −pA −2pB
−
2nO
1 −pA −pB
.
(4.62)
A solution can be obtained using Newton–Raphson. This requires the fol-
lowing second derivatives
∂2l (pA, pB|y)
(∂pA)2
= nA (2 −2pA −2pB)
pA (pA + 2pB −2)2 +
2nA
pA (pA + 2pB −2)
−nA (2pA + 2pB −2)
p2
A (pA + 2pB −2) −nAB
p2
A
−
2nO
(pA + pB −1)2
−
4nB
(2pA + pB −2)2 ,
∂2l (pA, pB|y)
(∂pB)2
= nB (2 −2pA −2pB)
pB (2pA + pB −2)2 +
2nB
pB (2pA + pB −2)
−nB (2pA + 2pB −2)
p2
B (2pA + pB −2) −nAB
p2
B
−
2nO
(pA + pB −1)2
−
4nA
(pA + 2pB −2)2 ,
∂2l (pA, pB|y)
∂pA∂pB
= −
2nA
(2 −pA −2pB)2 −
2nB
(2 −2pA −pB)2
−
2nO
(1 −pA −pB)2 .
Suppose the data are nA = 725, nAB = 72, nB = 258, nO = 1073. Using
these expressions in (4.1) yields, at convergence, the ML estimates: 5pA =
0.2091 and 5pB = 0.0808. The observed information matrix evaluated at the
ML estimates is
I (5pA, 5pB|y) =

23, 215.5
5, 031.14
5, 031.14
56, 009.4

,

4.5 Analysis of a Multinomial Distribution
199
resulting in an estimate of the asymptotic covariance matrix equal to:
V ar (5pA, 5pB|y) = [I (5pA, 5pB|y)]−1 = 10−6

43.930
−3.946
−3.946
18.209

.
■
4.5.1
Amount of Information per Observation
An important problem in experimental genetics is the evaluation of designs
for estimation of genetic parameters. In this context, a relevant question is
the measurement of the average amount of information about a parameter
per experimental unit included in the trial. It will be shown here how to
assess this when the entity of interest is yet another parameter aﬀecting
the probabilities that govern the multinomial distribution. Subsequently,
two examples are given where the target parameter is the recombination
fraction between loci.
Under multinomial sampling, the likelihood is given in (4.52). Suppose
now that the θ′s depend on some scalar parameter α. To emphasize this
dependency, the probabilities are denoted as θi (α) . The likelihood is then
viewed as a function of α, and the score function is
dl (α)
dα
=
T

i=1
ni
θi (α)
dθi (α)
dα
.
(4.63)
The amount of information about α contained in the sample is denoted as:
In (α) = E

−d2l (α)
(dα)2

= −E
# T

i=1

−
ni
θ2
i (α)

dθi (α)
dα
2
+
ni
θi (α)
d2θi (α)
(dα)2
$
=
# T

i=1

1
θi (α)

dθi (α)
dα
2
−d2θi (α)
(dα)2
$
E

ni
θi (α)

.
(4.64)
Now, E [ni/θi (α)] = nθi (α) /θi (α) = n. Using this in the preceding ex-
pression, distributing the sum, and noting that
T

i=1
d2θi (α)
(dα)2
=
d2
(dα)2
T

i=1
θi (α) =
d2
(dα)2 1 = 0,
one arrives at
In (α) = n
T

i=1
1
θi (α)

dθi (α)
dα
2
.
(4.65)

200
4. Further Topics in Likelihood Inference
Hence, the expected amount of information per observation is
I1 (α) =
T

i=1
1
θi (α)

dθi (α)
dα
2
.
(4.66)
Example 4.8
Estimating the recombination rate between two loci from
matings involving coupling heterozygotes
This example is patterned after Rao (1973). Consider two loci, each with
two alleles. Let the alleles be A and a at the ﬁrst locus, and B and b at the
second locus. Suppose that individuals that are heterozygous at both loci
are available, and that such heterozygotes originate from a cross between
AABB and aabb parents. In this case, individuals are called coupling het-
erozygotes, and their genotype is indicated as AB/ab. This means that they
came from AB and ab gametes only. Coupling heterozygotes are crossed
inter se, and the objective is to estimate the probability of recombination
(or recombination rate) between the two loci, denoted here as α. The dis-
tribution of gametes produced by coupling heterozygotes can be deduced
by considering that recombinant gametes arise with probability α, whereas
the complementary event (lack of recombination) has probability 1 −α.
Then the gametic distribution is
Pr (AB) = Pr (ab) = 1 −α
2
, Pr (Ab) = Pr (aB) = α
2 .
The random union of these gametes produces 16 possible genotypes, i.e.,
AABB, ..., aabb, nine of which are distinguishable at the genotypic level,
assuming that maternal or paternal origin of the chromosome cannot be
traced. For example, AB/Ab cannot be distinguished from Ab/AB. The fo-
cus of inference is the parameter α and to evaluate the eﬃciency of this and
of another design in terms of expected information per observation. The
data are genotypic counts scored in the progeny from the appropriate mat-
ings. Table 4.2 provides the distribution of genotypes in matings between
coupling heterozygotes, as well as the expected amount of information per
observation (for each genotype) calculated with formula (4.66).
Using (4.66), the expected amount of information per observation is ob-
tained by summing elements in the third column of Table 4.2, yielding
I1 (α) = 4 + 8
 1
2 −α
2
α (1 −α) +
(4α −2)2
(4α2 −4α + 2).
For example, if the loci are in diﬀerent chromosomes, α = 1/2 and I1 (α) =
4. As the loci become more closely linked, this measure of information
increases and tends to inﬁnity as α →0. From Table 4.2, the log-likelihood

4.5 Analysis of a Multinomial Distribution
201
Genotype
θi (α)
I[i]
1 (α)
Counts
AABB
1
4 (1 −α)2
1
n1
AaBb
 1
2 −α + α2
(4α −2)2 4
4α2 −4α + 2

n2
AABb
1
2α (1 −α)
2
 1
2 −α
2 4
α −α2
n3
AaBB
1
2α (1 −α)
2
 1
2 −α
2 4
α −α2
n4
aabb
1
4 (1 −α)2
1
n5
Aabb
1
2α (1 −α)
2
 1
2 −α
2 4
α −α2
n6
aaBb
1
2α (1 −α)
2
 1
2 −α
2 4
α −α2
n7
AAbb
1
4α2
1
n8
aaBB
1
4α2
1
n9
TABLE 4.2. Probability distribution of genotypes in progeny from crosses be-
tween coupling heterozygotes and contribution of each genotype to expected in-
formation per observation (third column).
of the parameter α can be deduced to be
l (α) = (n1 + n5) ln

(1 −α)2
4

+ (n3 + n4 + n6 + n7) ln
α (1 −α)
2

+ (n8 + n9) ln

α2
4

+ n2
#
(1 −α)2
2
+ α2
2
$
.
Given data, the log-likelihood can be maximized numerically. Suppose that,
out of n = 65 descendants from matings involving coupling heterozy-
gote parents, the genotypic count recovered is such that n1 + n5 = 30,
n3 + n4 + n6 + n7 = 8, n8 + n9 = 9, and n2 = 18. The ML estimate
of the recombination rate is, in this case, 5α = .2205. Using second deriva-
tives, an approximation to the asymptotic standard error based on observed
information is
8
D
V ar (5α) = .0412. The expected information per observa-
tion is estimated as I1 (5α) = 8.58743. Thus, with n = 65, (4.65) gives
I65 (5α) = (65) (8.58743) = 558.1830, as an estimate of the expected infor-
mation about the recombination rate parameter. The corresponding esti-
mated asymptotic standard error is
√
558.1830−1 = .0423. There is good
agreement between the standard errors based on observed and expected
information.
■
Example 4.9
Estimating the recombination rate between two loci from
a backcross
An alternative genetic design for estimating α consists of crossing the cou-
pling heterozygotes AB/ab to ab/ab (i.e., one of the parental genotypes).
This is called a backcross. The nonrecombinant types appearing in the

202
4. Further Topics in Likelihood Inference
 0.2 
 0.4 
 0.6 
 0.8 
1.0
 0.1 
 0.2 
 0.3 
 0.4 
 0.5 
α 
I1(α)/I1(α)
B
FIGURE 4.1. Plot of IB
1 (α) /I1 (α) as a function of recombination α.
progeny with a total probability of 1 −α, are AB/ab and ab/ab. The re-
combinant types (with probability α) are Ab/ab and aB/ab. Using (4.66),
the expected information per observation obtained from this design is
IB
1 (α) =
T

i=1
1
θi (α)

dθi (α)
dα
2
= 2
1
(1−α)
2

−1
2
2
+ 2 1
α
2

1
2
2
=
1
α (1 −α),
where the superscript B denotes the backcross design. If the loci are in
diﬀerent chromosomes IB
1 (α) = 4 and, when α goes to zero, then IB
1 (α) →
∞, as in the preceding design. A plot of IB
1 (α) /I1 (α) as a function of α
is given in Figure 4.1. It is seen that a design involving matings between
coupling heterozygotes is always more eﬃcient, although the designs diﬀer
little for values of α near 1/2.
■
4.6
Analysis of Linear Logistic Models
Response variables that are categorical in expression often arise in genetic
analysis. For example, Wright (1934) analyzed the inheritance of the num-
ber of digits in guinea pigs and proposed relating the expression of this
variable to an underlying normal process. It would be at this level where
gene substitutions operate. This model, known as the threshold model, was
elaborated further in quantitative genetics by Dempster and Lerner (1950),
Falconer (1965), and Gianola (1982), among others. Here, the special case
of binary response variables is considered. Attention is given to the de-
velopment of ML machinery appropriate for some situations of interest in
quantitative genetic applications.
The point of departure of a likelihood analysis is the probability mass
function of the data, given the parameters, which is developed below. Sup-

4.6 Analysis of Linear Logistic Models
203
pose there is an underlying or latent unobservable variable, l, often called
liability, and that the categories of response (two such categories assumed
here) result from the value of l relative to a ﬁxed threshold t. (The same
letter l, is used for liability and for log-likelihood, although the latter is
indexed by the parameters of the model. The common use of l should
therefore not lead to ambiguities). Let the dichotomy be “survival” versus
“death”, say. If l < t then the individual survives and the binary variable
takes the value Y = 1. If l ≥t the individual dies and Y = 0. Denote the
liability associated with datum i as li, and suppose that the underlying
variate is related to an unknown parameter vector β (of order p × 1) in
terms of the linear model
li = x′
iβ + ei,
i = 1, 2, ..., N,
(4.67)
where x′
i is the ith row of the known, nonstochastic N × p matrix of ex-
planatory variables X and ei is a random residual with p.d.f. p (ei). Assume
that the residuals are independent and identically distributed. The proba-
bility of survival of individual i (which is the p.m.f. of the random variable
Yi) is
Pr (Yi = 1|β) = Pr (li < t|β) = Pr (li −x′
iβ < t −x′
iβ|β)
= Pr (ei < t −x′
iβ|β) =
 t−x′
iβ
−∞
p (ei) dei
= 1 −
 ∞
t−x′
iβ
p (ei) dei = 1 −
 x′
iβ−t
−∞
p (ei) dei.
(4.68)
The last equality above requires that ei is symmetrically distributed around
0. The liabilities cannot be observed, and a convenient origin is to set the
value of the threshold to 0. Hence, the scale is one of deviations from the
threshold. This constraint makes the likelihood model identiﬁable and the
Hessian becomes negative deﬁnite.
It is interesting to note that, although in the underlying scale li changes
with xi at a constant rate, this is not so at the level of the probabilities.
This is veriﬁed by noting that
∂li
∂xi
= β,
whereas from expression (4.68)
∂Pr (y = 1|β)
∂xi
=
∂
∂xi

1 −
 x′
iβ
−∞
p (ei) dei

= −
∂
∂x′
iβ
 x′
iβ
−∞
p (ei) dei

∂x′
iβ
∂xi
= −p (x′
iβ) β.
(4.69)

204
4. Further Topics in Likelihood Inference
The change is not constant and depends on the value of the explanatory
vector xi.
4.6.1
The Logistic Distribution
In the analysis of binary responses (Cox and Snell, 1989), two distributions
are often assigned to the residuals. A natural candidate is the normal dis-
tribution, as in linear models. However, because the underlying variable
cannot be observed, the unit of measurement is set to be equal to the stan-
dard deviation, so a N (0, 1) residual distribution is adopted. This leads
to the probit model, and parameter estimates must be interpreted as de-
viations from the threshold in units of standard deviation. Alternatively,
a logistic distribution can be adopted, because it leads to somewhat sim-
pler algebraic expressions. The parameters are also in standard deviation
units. It should be understood, however, that if mechanistic considerations
dictate a normal distribution, this should be preferred.
Consider a random variable Z having, as density function,
p(z) =
exp (z)
[1 + exp (z)]2 ,
−∞< z < ∞.
(4.70)
Then Z has a logistic distribution with E (Z) = 0 and V ar (Z) = π2/3.
Hence, for a constant k,
Pr (Z < k) =
 k
−∞
p (z) dz =
 k
−∞
exp (z)
[1 + exp (z)]2 dz
=
exp (k)
[1 + exp (k)].
(4.71)
If the residual distribution in (4.67) is logistic, the probability of survival
in (4.68) is
Pr (Y = 1|β) = 1 −
 x′
iβ
−∞
p (ei) dei = 1 −
exp (x′
iβ)
[1 + exp (x′
iβ)]
= [1 + exp (x′
iβ)]−1 = pi (β) ,
(4.72)
and the probability of death is
Pr (Y = 0|β) =
exp (x′
iβ)
[1 + exp (x′
iβ)] = 1 −pi (β) .
(4.73)
4.6.2
Likelihood Function under Bernoulli Sampling
The data consist of binary responses on N subjects and inferential interest
is on β, the location vector of the underlying distribution. Each of the (0, 1)

4.6 Analysis of Linear Logistic Models
205
outcomes can be viewed as a Bernoulli trial with probability
Pr (Yi = yi|β) =

1
1 + exp (x′
iβ)
yi 
exp (x′
iβ)
1 + exp (x′
iβ)
1−yi
,
(4.74)
with Yi = 1 if the individual survives, and Yi = 0 otherwise. If, given β,
the N responses are mutually independent, the probability of observing the
data y is
p (y|β) =
N
-
i=1

1
1 + exp (x′
iβ)
yi 
exp (x′
iβ)
1 + exp (x′
iβ)
1−yi
.
(4.75)
This is the likelihood function when viewed as a function of β. The resulting
log-likelihood can be written as
l (β|y) =
N

i=1
{(1 −yi) x′
iβ −ln [1 + exp (x′
iβ)]} .
(4.76)
The score vector is:
l′ (β|y) =
N

i=1
∂
∂β {(1 −yi) x′
iβ −ln [1 + exp (x′
iβ)]}
=
N

i=1
(1 −yi) xi −
exp (x′
iβ)
1 + exp (x′
iβ)xi
=
N

i=1
{1 −yi −[1 −pi (β)]} xi
= −
N

i=1
[yi −pi (β)] xi.
(4.77)
Now let the N × 1 vector of probabilities of survival for the N individuals
be p (β) , and observe that
N

i=1
[yi −pi (β)] xi = {x1 [y1 −p1 (β)] , ..., xN [yN −pN (β)]}
= X′ [y −p (β)] .
The vector y −p (β) consists of deviations of the observations from their
expectations, that is, residuals in the discrete scale. Using this representa-
tion in (4.77) it can be seen that the ﬁrst-order condition for a maximum
is satisﬁed if
X′p

5β

= X′y
(4.78)

206
4. Further Topics in Likelihood Inference
where p

5β

is the vector of probabilities of survival for the N individuals
evaluated at the ML estimator 5β, if this exists. The estimating equations
(4.78) are not explicit in 5β and must be solved iteratively. The Newton–
Raphson algorithm requires second derivatives, and an additional diﬀeren-
tiation of the log-likelihood with respect to the parameters gives
l′′ (β|y) = ∂2l (β|y)
∂β ∂β′
=
∂
∂β′
#
−
N

i=1
[yi −pi (β)] xi
$
=
N

i=1
xi
∂
∂β′ pi (β) .
(4.79)
Now,
∂
∂β′ pi (β) =
∂
∂β′ [1 + exp (x′
iβ)]−1
= −[1 + exp (x′
iβ)]−2 exp (x′
iβ) x′
i
= −pi (β) [1 −pi (β)] x′
i.
Using this in (4.79)
l′′ (β|y) = −
N

i=1
xipi (β) [1 −pi (β)] x′
i = −X′D (β) X,
(4.80)
where D (β) = {pi (β) [1 −pi (β)]} is an N × N diagonal matrix. Because
the second derivatives do not depend on the observations, the expected
information is equal to the observed information in this case. Hence, the
Newton–Raphson and the scoring algorithms are identical. From (4.1) the
iteration can be represented as

X′D

β[t]
X

β[t+1] =

X′D

β[t]
X

β[t] + X′v

β[t]
,
(4.81)
where the vector v

β[t]
= p

β[t]
−y. Now let
y∗
β[t]
= Xβ[t] + D−1 
β[t]
v

β[t]
be a pseudo-data vector evaluated at iteration [t]. Then the system (4.81)
can be written as

X′D

β[t]
X

β[t+1] = X′D

β[t]
y∗
β[t]
.
(4.82)
This is an iterative reweighted least-squares system where the matrix of
weights
D

β[t]
=
9
pi

β[t] 
1 −pi

β[t]:

4.6 Analysis of Linear Logistic Models
207
Calf
Birth weight
Score
Calf
Birth weight
Score
1
40
1
7
47
0
2
40
1
8
47
0
3
40
0
9
47
1
4
43
0
10
50
0
5
43
1
11
50
0
6
43
0
12
50
0
TABLE 4.3. Hypothetical data on birth weight and calving scores taken on 12
calves.
is the reciprocal of the variance of the logit
ln
pi (β)
1 −pi (β)
evaluated at β = β[t]; this was shown in Example 4.5. The Newton–Raphson
algorithm is iterated until the change in successive rounds is negligible. If
convergence is to a global maximum 5β, then this is the ML estimate. The
asymptotic variance covariance matrix is estimated as
D
V ar

5β

=

X′D

5β

X
−1
.
(4.83)
Example 4.10
Birth weight and calving diﬃculty
Suppose that each of 12 genetically unrelated cows of the same breed gives
birth to a calf. These are weighed at birth, and a score is assigned to
indicate if there were birth diﬃculties. The scoring system is: 1 if calving
is normal and 0 otherwise. The objective of the analysis is to assess if there
is a relationship between birth weight and birth diﬃculty. A logit model is
used where the underlying variate is expressed as
li = α + βxi+ei,
i = 1, 2, ..., 12,
and where xi is the birth weight in kilograms. The hypothetical data are
in Table 4.3.
Using (4.76) the log-likelihood is
l (α, β|y) ∝(α + β 40) −3 ln [1 + exp (α + β 40)]
+ 2 (α + β 43) −3 ln [1 + exp (α + β 43)]
+ 2 (α + β 47) −3 ln [1 + exp (α + β 47)]
+ 3 (α + β 50) −3 ln [1 + exp (α + β 50)] .
The Newton–Raphson algorithm gives 5α = −12.81 and 5β = 0.305. The
estimated asymptotic variance–covariance matrix is
V ar
 5α
5β

=

82.476
−1.881
−1.881
.043

.

208
4. Further Topics in Likelihood Inference
The asymptotic standard error of 5β is
√
.043 = .2074. A conﬁdence region
of size 95% is approximately (−.101, .711) . The interval includes the value
0, so this data cannot refute the hypothesis that birth weight does not aﬀect
the probability of a diﬃcult calving. Because α is a nuisance parameter, and
it is not obvious how a marginal likelihood can be constructed for β, the
proﬁle likelihood L

β, 55α (β)

is calculated where 55α (β) is the partial ML
estimator at a ﬁxed value of β. Numerically, this is done by making a grid
of values of β, ﬁnding the partial ML estimator of α corresponding to each
value of β, and then computing the value of the resulting log-likelihood.
Values of 55α (β) and of the proﬁle log-likelihood at selected values of β are
shown in Table 4.4. The value β = .305, the ML estimate, is the maximizer
of the proﬁle likelihood, as discussed in Section 4.4.3.
■
4.6.3
Mixed Eﬀects Linear Logistic Model
Assume that the underlying variable is modelled now as:
li = x′
iβ + z′
ia+ei,
i = 1, 2, ..., N,
(4.84)
where a ∼[0, G (φ)] is a vector of random eﬀects having some distribu-
tion (typically, multivariate normality is assumed in quantitative genetics)
with covariance matrix G (φ) . In turn, this covariance matrix may de-
pend on unknown parameters φ, which may be variance and covariance
components, for example, for traits that are subject to maternal genetic
inﬂuences (Willham, 1963). The vector z′
i is a row incidence vector that
plays the same role as x′
i. The residual ei has a logistic distribution, as
before. The probability of survival for the ith individual, given β and a,
after setting the threshold to 0, is now
Pr (Yi = 1|β, a) = Pr (li < t|β, a) = 1 −
 x′
iβ+z′
ia
−∞
p (ei) dei
=

1 + exp

x′
iβ + z′
ia
−1 = pi (β, a) ,
(4.85)
and the conditional probability of death is Pr (Yi = 0|β, a) = 1 −pi (β, a) .
Under Bernoulli sampling the conditional probability of observing the data
β
55α (β)
L

β, 55α (β)

.100
−3.78275
−6.82957
.200
−8.21124
−6.38209
.300
−12.5950
−6.24025
.305
−12.8135
−6.23996
.400
−16.9390
−6.33550
.500
−21.2498
−6.60396
TABLE 4.4. Proﬁle likelihood for β.

4.6 Analysis of Linear Logistic Models
209
obtained is then:
p (y|β, a) =
N
-
i=1
[pi (β, a)]yi [1 −pi (β, a)]1−yi .
In order to form the likelihood function, the marginal probability distribu-
tion of the observations is needed. Thus, the joint distribution [y, a|β, φ]
must be integrated over a to obtain
p (y|β, φ) =

N
-
i=1
[pi (β, a)]yi [1 −pi (β, a)]1−yi p (a|φ) da,
(4.86)
where p (a|φ) is the density of the joint distribution of the random eﬀects.
This integral cannot be expressed in closed form, and must be evaluated by
numerical procedures, such as Gaussian quadrature. As shown below, this
is feasible only when the random eﬀects are independent (which is seldom
the case in genetic applications), because then the problem reduces to one
of evaluating unidimensional integrals. An alternative is to use Monte Carlo
integration procedures, such as the Gibbs sampler. This will be discussed
in subsequent chapters.
Let the model for liability be
lij = x′
ijβ+ai+eij,
i = 1, 2, ..., S, j = 1, 2, ..., ni.
Thus, there are S random eﬀects and ni observations are associated with
random eﬀect ai. Suppose the random eﬀects are i.i.d. with distribution
ai ∼N (0, φ) . Then the joint density of the vector of random eﬀects is
p (a|φ) =
S
-
i=1
p (ai|φ) .
Using this, (4.86) can be written as
p (y|β, φ) =
S
-
i=1

ni
-
j=1
[pij (β,ai)]yij [1 −pij (β,ai)]1−yij p (ai|φ) dai
(4.87)
indicating that in (4.87), S single dimension integrals need to be evaluated,
instead of a multivariate integral of order S, as pointed out in connection
with (4.86). However, the integral is not explicit. This illustrates that there
are computational challenges in connection with ML estimation. The same
is true of Bayesian methods, which are introduced in Chapter 5.

This page intentionally left blank

5
An Introduction to
Bayesian Inference
5.1
Introduction
The potential appeal of Bayesian techniques for statistical analysis of ge-
netic data will be motivated using examples from the ﬁeld of animal breed-
ing. Here, two types of data are encountered most often. First, there are
observations obtained from animal production and disease recording pro-
grams; for example, birth and weaning weights in beef cattle breeding
schemes and udder disease data in dairy cattle breeding. These are called
“ﬁeld” records, which are collected directly on the farms where animals
are located. Second, there are data from genetic selection experiments con-
ducted under fairly controlled conditions. For example, there may be lines
of mice selected for increased ovulation rate, and lines in which there is
random selection, serving as “controls”. Field records are usually available
in massive amounts, whereas experimental information is often limited.
Suppose milk yield ﬁeld records have been taken on a sample of cows,
and that one wishes to infer the amount of additive genetic variance in
the population, perhaps using a mixed eﬀects linear model. Often, use may
be made of a large part of an entire national data base. In this situa-
tion, the corresponding full, marginal, or proﬁle likelihood functions are
expected to be sharp, leading to fairly precise estimates of additive genetic
variance. Naturally, this is highly dependent on the model adopted for anal-
ysis. That is, if an elaborate multivariate structure with a large number of
nuisance parameters is ﬁtted, it is not necessarily true that all proﬁle or
marginal likelihoods will be sharp. However, in many instances, the amount

212
5. An Introduction to Bayesian Inference
of information is so large that genetic parameters are well-estimated, with
asymptotic theory working handsomely. Here, it is reasonable to expect
that inferences drawn from alternative statistical methods will seldom lead
to qualitatively diﬀerent conclusions.
Consider now inferences drawn using data from designed experiments,
where estimates tend to be far from precise. A common outcome is that
experimental results are ambiguous, and probably consistent with several
alternative explanations of the state of nature. This is because the scarce
amount of resources available dictates a small eﬀective population size of
the experimental lines. In this situation, there can be much uncertainty
about the parameters to be inferred remaining after analysis. Here, one
would need to adopt methods capable of conveying accurately the limited
precision of the estimates obtained. Arguably, inference assuming samples
of an inﬁnite size (asymptotic theory) should not be expected to be satis-
factory.
How would many quantitative geneticists address the following question:
“How much genetic change has taken place in the course of
selection?”
Suppose that the assumption of joint normality of additive genetic ef-
fects and of phenotypic values is tenable. First, they would probably at-
tempt to estimate components of variance (or covariance) in the base pop-
ulation using either a full or a marginal likelihood; in the latter case, this
leads to REML estimates of the parameters. Second, conditionally on these
estimates, they would proceed to obtain best linear unbiased predictions
(BLUP) of the additive genetic eﬀects (treated as random) (Henderson,
1973). Theoretically, BLUP is the linear combination of the observations
that minimizes prediction error variance in the class of linear functions
whose expected value is equal to the expected value of the predictand (the
genetic eﬀects). BLUP can be derived only if the dispersion parameters
(variance and covariance components) are known, at least to proportion-
ality. However, when the latter parameters are estimated from the data at
hand (by REML, say), the resulting empirical BLUP is no longer linear or
best, and remains unbiased only under certain conditions, assuming ran-
dom sampling and absence of selection or of assortative mating (Kackar and
Harville, 1981). Quantitative geneticists often ignore this problem, and pro-
ceed to predict genetic means (these being random over conceptual repeated
sampling) for diﬀerent generations or cohorts using the empirical BLUP.
From the estimated means, measures of genetic change can be obtained.
It must be noted that if dispersion parameters are known and random
sampling holds, the estimated means have a jointly Gaussian distribution,
with known mean vector and variance–covariance matrix. However, one
could argue that if genetic variances were known, there would not be any
need for conducting the experiment! Further, if the REML estimates are

5.1 Introduction
213
used in lieu of the true values, the BLUP analysis ignores their error of
estimation. Here, one could invoke asymptotic theory and hope that the
data are informative enough, such that reality can be approximated well
using limit arguments. The resulting “BLUP” of a generation mean has an
unknown distribution, so how does one construct tests of the hypothesis
“selection is eﬀective” in such a situation? Also, complications caused by
nonrandom sampling mechanisms are encountered, as parents of a subse-
quent generation are not chosen at random. Unless selection is ignorable,
in some sense, inferences are liable to be distorted. In short, this is an ex-
ample of a situation where the animal breeding paradigms for parameter
estimation (maximum likelihood, asymptotic theory) and for prediction of
random variables (BLUP), used together, are incapable of providing a com-
pletely satisfactory answer to one of the most important questions that can
be posed in applied quantitative genetics. Should one use the conditional
distribution of the random eﬀects, given the data, ignoring selection and
the error of estimation of the parameters for inferring genetic change?
If one wishes to know what to expect, at least in the frequentist sense
of hypothetical repeated sampling, the only answer would seem to reside
in simulating all conceivable selection schemes and designs, for all possible
values of the parameters. Clearly, this is not feasible. Simulations would
need to be sensibly restricted to experimental settings and to parameter
values that are likely to reﬂect reality. This implies that at least something
must be known about the state of nature, before experimentation. However,
there is always uncertainty, ranging from mild to large.
An alternative is to adopt the Bayesian point of view. Under this setting,
all unknown quantities in a statistical system are treated as random vari-
ables, reﬂecting (typically) subjective uncertainty measured by a probabil-
ity distribution. The unknowns may include parameters (e.g., heritability or
the inbreeding coeﬃcient), random eﬀects (e.g., the additive genetic value
of an artiﬁcial insemination bull), data that are yet to be observed (e.g.,
the mean of the oﬀspring of a pair of parents that will be measured under
certain conditions), the sampling distribution adopted for the data gener-
ating process (e.g., given the parameters, the observations may have either
a Gaussian or a multivariate-t distribution), or the entire model itself, en-
gulﬁng all assumptions made. Here, there may be a number of competing
probability models, of varying dimension. In the Bayesian approach, the
idea is to combine what is known about the statistical ensemble before the
data are observed (this knowledge is represented in terms of a prior proba-
bility distribution) with the information coming from the data, to obtain a
posterior distribution, from which inferences are made using the standard
probability calculus techniques presented in Chapters 1 and 2.
The inferences to be drawn depend on the question posed. Sometimes,
one may seek a marginal posterior distribution, whereas in other instances,
joint or conditional posterior distributions of subsets of variables may be
targets in the analysis. Since any unknown quantity, irrespective of whether

214
5. An Introduction to Bayesian Inference
it is a model, a parameter, or a future data point, is treated symmetrically,
the answer is always found in the same manner, that is, by arriving at
the corresponding posterior distribution via probability theory. The results
of a Bayesian analysis can be presented by displaying the entire posterior
distribution (or density function), or just some posterior summaries, such
as the mean, median, variance, or percentiles. The results are interpreted
probabilistically. For example, if one wishes to infer the mean (µ) of a
distribution, one would say “the posterior probability that µ is between a
and b is so much”. This illustrates how diﬀerent the Bayesian construct is
from the frequentist-likelihood paradigms.
An overview of the basic elements of the Bayesian approach to inference
will be presented in this chapter. The treatment begins with a description
of Bayes theorem and of its consequences. Subsequently, joint, marginal,
and conditional posterior distributions are introduced, including a presen-
tation of the Bayesian manner of handling nuisance parameters. For the
sake of clarity, the Bayesian probability models considered to illustrate de-
velopments emphasize linear speciﬁcations for Gaussian observations; lin-
ear models are well-known by quantitative geneticists. The presentation
is introductory, and a much deeper coverage can be found, for example,
in Zellner (1971), Box and Tiao (1973), Lee (1989), Bernardo and Smith
(1994), O’Hagan (1994), Gelman et al. (1995), and Leonard and Hsu (1999).
Additional topics in Bayesian analysis are discussed in Chapters 6, 7, and
8.
5.2
Bayes Theorem: Discrete Case
Suppose a scientist has M disjoint hypotheses (H1, H2, . . . , HM) about
some mechanism, these being mutually exclusive and, at least temporar-
ily, exhaustive. The latter is an important consideration because at any
point in time, one cannot formulate all possible hypotheses. Rather, the
set H1, H2, . . . , HM constitutes the collection of all hypotheses that can
be formulated by this scientist now, in the light of existing knowledge
(Mal´ecot, 1947). Additional, competing, hypotheses would surely emerge,
as more knowledge is acquired. Obviously, the “true” hypothesis cannot be
observed, but there may be some inclination by the scientist toward ac-
cepting one in the set as being more likely than the others. In other words,
there may be more certainty that one such hypothesis is true, relative to
the alternatives. In Bayesian analysis, this uncertainty is expressed in terms
of probabilities. In such a context, it is reasonable to speak of a random
variable H taking one of M mutually exclusive and exhaustive states.

5.2 Bayes Theorem: Discrete Case
215
Let p(Hi) be the prior probability assigned by this scientist to the event
“hypothesis Hi is true”, with
0 ≤p(Hi) ≤1,
i = 1, 2, ..., M,
p(Hi ∩Hj) = 0,
i ̸= j,
M

i=1
p(Hi) = 1.
Then p(Hi) (i = 1, 2, . . . , M) gives the prior probability distribution of
the competing hypotheses. The prior distribution may be elicited either on
subjective grounds, on mechanistic considerations, on evidence available so
far, or using a combination of these three approaches to assess beliefs.
The Bayesian approach provides a description of how existing knowl-
edge is modiﬁed by experience. Now let there be N observable eﬀects
E1, E2, . . . , EN. Given that hypothesis Hi holds, one expects to observe
eﬀects with conditional probabilities
0 ≤p(Ej|Hi) ≤1,
i = 1, 2, . . . , M, j = 1, 2, . . . , N,
p(Ej ∩Ej′|Hi) = 0,
j ̸= j′,
N

j=1
p(Ej|Hi) = 1.
Thus, p(Ej|Hi) gives the conditional probability of the eﬀects observed
under hypothesis Hi, with these eﬀects being disjoint, that is, Ej and
Ej′ cannot be observed simultaneously. For example, under a one-locus
Mendelian model (the hypothesis), an individual cannot be homozygote
and heterozygote at the same time. This conditional distribution represents
the probabilities of eﬀects to be observed if experimentation proceeded un-
der the conditions imposed by the hypothesis. Again, under Mendelian
inheritance, random mating, no migration or mutation and a large pop-
ulation, one would expect the probabilities of observing AA, Aa, and aa
individuals to be those resulting from the Hardy–Weinberg equilibrium.
Then let E be a random variable taking one of the states Ej (j =
1, 2, . . . , N), and let H and E have the joint distribution
p(H = Hi, E = Ej) = p(Ej|Hi)p(Hi).
The conditional probability that hypothesis Hi holds, given that eﬀects Ej
are observed, is then
p(Hi|Ej) = p(H = Hi, E = Ej)
p(Ej)
= p(Ej|Hi)p(Hi)
p(Ej)
,
(5.1)

216
5. An Introduction to Bayesian Inference
where p(Ej) is the marginal or total probability of observing eﬀect Ej, that
is, the probability of observing Ej over all possible hypotheses
p(Ej) =
M

i=1
p(Ej|Hi)p(Hi) = EH [p(Ej|Hi)] ,
(5.2)
where EH (·) indicates an expectation taken with respect to the prior dis-
tribution of the hypotheses. Using (5.2) in (5.1)
p(Hi|Ej)
=
p(Ej|Hi)p(Hi)
EH [p(Ej|Hi)]
(5.3)
∝
p(Ej|Hi)p(Hi).
(5.4)
This standard result of conditional probability is also known as Bayes the-
orem when applied to the speciﬁc problem of inferring “causes” from “ef-
fects”, and it is also called “inverse probability”. It states that the proba-
bility of a cause or hypothesis Hi, given evidence Ej, is proportional to the
product of the prior probability assigned to the hypothesis, p(Hi), times the
conditional probability of observing the eﬀect Ej under hypothesis Hi. The
distribution in (5.3) or (5.4) is called the posterior probability distribution,
with the denominator in (5.3) acting as normalizing constant.
Expression (5.4) illustrates a concept called “Bayesian learning”. This
is the process by which a prior opinion (with the associated uncertainty
stated by the prior distribution) is modiﬁed by evidence E (generated with
uncertainty under a sampling model characterized by a distribution with
probabilities p(Ej|Hi)), to become a posterior opinion, this having a distri-
bution with probabilities p(Hi|Ej). Suppose now that additional evidence
E′
j′ accrues. Using Bayes theorem in (5.3) or (5.4):
p(Hi|E′
j′, Ej)
=
p(E′
j′, Ej|Hi)p(Hi)
EH[p(E′
j′, Ej|Hi)]
∝
p(E′
j′, Ej|Hi)p(Hi)
∝
p(E′
j′|Ej, Hi)p(Ej|Hi)p(Hi)
∝
p(E′
j′|Ej, Hi)p(Hi|Ej).
(5.5)
The preceding indicates that the posterior distribution after evidence Ej
conveys the prior opinion before E′
j′ is observed. It also describes how
opinions are revised sequentially or, equivalently, how knowledge is modiﬁed
by evidence. If, given Hi, E′
j′ is conditionally independent of Ej, then
p(E′
j′|Ej, Hi)p(Ej|Hi) = p(E′
j′|Hi)p(Ej|Hi).
More generally, for S pieces of evidence, assuming conditional indepen-
dence,
p(Hi|ES
jS, ES−1
jS−1, ..., E2
j2, E1
j1) ∝
S
-
k=1
p(Ek
jk|Hi)p(Hi),
(5.6)

5.2 Bayes Theorem: Discrete Case
217
where Ek
jk denotes the evidence in datum k, with j = 1, 2, ..., N indicating
the diﬀerent values that E can take at any step k of the process of accumu-
lating information. Letting E be the entire evidence, (5.6) can be written
as
p(Hi|E) ∝exp
# S

k=1
log

p(Ek
jk|Hi)

+ log p(Hi)
$
∝exp

SL + log p(Hi)

∝exp

SL

1 + log p(Hi)
S L
%
,
(5.7)
where
L = 1
S
S

k=1
log

p(Ek
jk|Hi)

is the average log-probability of observing Ek
jk under hypothesis Hi. Now,
letting S →∞, and provided that p(Hi) > 0, it can be seen that the expo-
nent in expression (5.7) tends toward SL, indicating that the contribution
of the prior to the posterior is of order 1/S. This implies that the evidence
tends to overwhelm the prior as more and more information accumulates
so, for large S,
p(Hi|E) ∝
S
-
k=1
p(Ek
jk|Hi).
(5.8)
Following O’Hagan (1994), note from (5.1) that evidence E will increase
the probability of a hypothesis H only if
p(E|H) > p(E),
where subscripts are ignored, for simplicity. Now, from (5.2) and denoting
as H the event “H not true”, with p(H) being the corresponding probabil-
ity, one can write
p(E)
=
p(E|H)p(H) + p(E|H)p(H)
=
p(E|H)

1 −p(H)

+ p(E|H)p(H).
Rearranging
p(E|H) −p(E) =

p(E|H) −p(E|H)

p(H).
This indicates that evidence E will increase the probability of a hypothesis
if and only if p(E|H) > p(E|H), that is, if the chances of observing E
are larger under H than under any of the competing hypothesis. If this
is the case, it is said that E confers a higher likelihood to H than to H;
thus, p(E|H) is called the likelihood of H. This is exactly the concept

218
5. An Introduction to Bayesian Inference
of likelihood function discussed in Chapter 3, that is, the probability (or
density) of the observations viewed as a function of the parameters (the
hypotheses play the role of the parameter values in this discussion). Thus,
the maximum likelihood estimator is the function of the data conferring
the highest likelihood to a particular value of the parameter.
The controversy in statistics about the use of Bayes theorem in science
centers on that the prior distribution is often based on subjective, if not
arbitrary (or convenient), elicitation. In response to this criticism, Savage
(1972) wrote:
“It has been countered, I believe, that if experience system-
atically leads people with opinions originally diﬀerent to hold a
common opinion, then that common opinion, and it only, is the
proper subject of scientiﬁc probability theory. There are two
inaccuracies in this argument. In the ﬁrst place, the conclusion
of the personalistic view is not that evidence brings holders of
diﬀerent opinions to the same opinions, but rather to similar
opinions. In the second place, it is typically true of any obser-
vational program, however extensive but prescribed in advance,
that there exist pairs of opinions, neither of which can be called
extreme in any precisely deﬁned sense, but which cannot be
expected, either by their holders or any other person, to be
brought into close agreement after the observational program.”
Furthermore, Box (1980) argued as follows:
“In the past, the need for probabilities expressing prior be-
liefs has often been thought of, not as a necessity for all scientiﬁc
inference, but rather as a feature peculiar to Bayesian inference.
This seems to come from the curious idea that an outright as-
sumption does not count as a prior belief...I believe that it is
impossible logically to distinguish between model assumptions
and the prior distribution of the parameters.”
The probability distribution of the hypotheses cannot be construed as
a frequency distribution, that is, as a random process generated as if hy-
potheses were drawn at random, with replacement, from an urn. If this were
the case, one could refute or corroborate the prior distribution by draw-
ing a huge number of independent samples from the said urn. Technically,
however, there would always be ambiguity, unless the number of samples
is inﬁnite! Instead, the prior probabilities in Bayesian inference must be
interpreted as relative degrees of belief about the state of nature, before
any experimentation or observation takes place.
The debate about the alternative methods of inference has a long his-
tory with eloquent arguments from both camps; we do not feel much can
be added here. A balanced comparative overview of methods of inference

5.2 Bayes Theorem: Discrete Case
219
is presented by Barnett (1999). A partisan, in-depth presentation of the
Bayesian approach is given in Bernardo and Smith (1994) and O’Hagan
(1994). A philosophical discussion of the concept(s) of probability can be
found in Popper (1972, 1982), who holds a strong, anti-inductive and anti-
subjective position. Indeed, Popper (1982) is concerned with the inﬂuence
that subjective probability has had on quantum mechanics. Popper cites
Heisenberg (1958), who writes:
“The conception of objective reality ... has thus evaporated
... into the transparent clarity of a mathematics that represents
no longer the behavior of particles but rather our knowledge of
this behavior”.
For a detailed and focused philosophical critique of the Bayesian ap-
proach to inference, the reader can consult the books of Howson and Urbach
(1989) and Earman (1992).
When a prior distribution is “universally agreed upon”, or when it is
based on mechanistic considerations, then Bayes theorem is accepted as a
basis for inference without reservation. Two examples of the latter situation
follow.
Example 5.1
Incidence of a rare disease
Consider a rare disease whose frequency in the population is 1 in 5, 000
(i.e., 0.0002). A test for detecting the disease is available and it has a false
positive rate of 0.05 and a false negative rate of 0.01. A person is taken
at random from the population and the test gives a positive result. What
is the probability that the person is diseased? Let θ represent a random
variable taking the value 1 if the person is diseased and 0 otherwise. Let
Y be a random variable that takes the value 1 if the test is positive, and
0 if the test is negative. The data are here represented by the single value
Y = 1. The prior probability (before the test result is available) that a
randomly chosen individual is diseased is
Pr (θ = 1) = 0.0002.
Based on the false positive and false negative rates, we can write
Pr (Y = 1|θ = 0)
=
0.05,
Pr (Y = 0|θ = 0) = 0.95,
Pr (Y = 0|θ = 1)
=
0.01,
Pr (Y = 1|θ = 1) = 0.99.
Applying Bayes theorem, the posterior probability that the individual is
diseased (after having observed Y = 1) is given by
Pr (θ = 1|Y = 1)
=
Pr (θ = 1) Pr (Y = 1|θ = 1)
Pr (θ = 1) Pr (Y = 1|θ = 1) + Pr (θ = 0) Pr (Y = 1|θ = 0)
=
(0.0002) (0.99)
(0.0002) (0.99) + (0.9998) (0.05) ≈0.0039.

220
5. An Introduction to Bayesian Inference
Thus, a positive test, has raised the probability that the individual has
the disease from 0.0002 (the a priori probability before the test result is
available) to 0.0039 (the posterior probability after observing Y = 1).
■
Example 5.2
Inheritance of hemophilia
The following is adapted from Gelman et al. (1995). Hemophilia is a ge-
netic disease in humans. The locus responsible for its expression is located
in the sex chromosomes (these are denoted as XX in women, and XY in
men). The condition is observed in women only in double recessive individ-
uals (aa), and in men that are carriers of the a allele in the X-chromosome.
Suppose there is a nonhemophiliac woman whose father and mother are not
aﬀected by the disease, but her brother is known to be hemophiliac. This
implies that her nonhemophiliac mother must be heterozygote, a carrier of
a. What is the probability that the propositus woman is also a carrier of
the gene? Let θ be a random variable taking one of two mutually exclusive
and exhaustive values (playing the role of the hypotheses in the preceding
section). Either θ = 1 if the woman is a carrier, or θ = 0 otherwise. Since
it is known that the mother of the woman must be a carrier (this consti-
tutes part of the system within which probabilities are assigned), the prior
distribution of θ is
Pr (θ = 1) = Pr (θ = 0) = 1
2.
In the absence of additional information, it is not possible to make a very
sharp probability assignment. Suppose now that the woman has two sons,
none of which is aﬀected. Let Yi be a binary random variable taking the
value 0 if son i is not aﬀected, or 1 if he has the disease; thus, the values
of Y1 and Y2 constitute the evidence E. Given that θ = 1, the probability
of the observed data is
Pr (Y1 = 0, Y2 = 0|θ = 1)
=
Pr (Y1 = 0|θ = 1) Pr (Y2 = 0|θ = 1) =

1
2
2
= 1
4.
This follows because:
a) the observations are assumed to be independent, conditionally on θ and
b) if the woman is a carrier (θ = 1), there is a 50% probability that she
will not transmit the allele.
On the other hand, if she is not a carrier (θ = 0):
Pr (Y1 = 0, Y2 = 0|θ = 0)
=
Pr (Y1 = 0|θ = 0) Pr (Y2 = 0|θ = 0) = 1 × 1 = 1,
this being so because it is impossible for a son to have the disease unless the
mother is a carrier (ignoring mutation). Hence, the data confer four times

5.2 Bayes Theorem: Discrete Case
221
more likelihood to the hypothesis that the mother is not a carrier. Using
the information that none of the sons is diseased, the posterior distribution
of θ is then
Pr (θ = 1|Y1 = 0, Y2 = 0) = Pr (θ = 1) Pr (Y1 = 0, Y2 = 0|θ = 1)
Pr (Y1 = 0, Y2 = 0)
=
Pr (θ = 1) Pr (Y1 = 0, Y2 = 0|θ = 1)
1
i=0
Pr (θ = i) Pr (Y1 = 0, Y2 = 0|θ = i)
=
1
2
1
4
1
21 + 1
2
1
4
= 1
5
and
Pr (θ = 0|Y1 = 0, Y2 = 0) = 1 −1
5 = 4
5.
A sharper probability assignment can be made now, and the combination
of prior information with the evidence suggests that the mother is probably
not a carrier. The latter possibility cannot be ruled out, however, as there
is a 20% probability that the mother is heterozygote. The posterior odds
in favor of the hypothesis that the mother is not a carrier is given by the
ratio
Pr (θ = 0|Y1 = 0, Y2 = 0)
Pr (θ = 1|Y1 = 0, Y2 = 0) = Pr (Y1 = 0, Y2 = 0|θ = 0)
Pr (Y1 = 0, Y2 = 0|θ = 1)
Pr (θ = 0)
Pr (θ = 1)
= 1
1
4
1
2
1
2
= 4,
where the ratio
Pr (θ = 0)
Pr (θ = 1) = 1
is called the prior odds in favor of the hypothesis. Further,
B01 = Pr (Y1 = 0, Y2 = 0|θ = 0)
Pr (Y1 = 0, Y2 = 0|θ = 1) = 4
is called the Bayes factor, that is, the factor by which the prior odds about
the hypotheses are modiﬁed by the evidence and converted into posterior
odds (a more thorough discussion of Bayes factors will be presented in
Chapter 8). In this example, the odds in favor of the hypothesis that θ = 0
relative to θ = 1 increase by a factor of 4 after observing two sons that are
not aﬀected by the disease. Suppose that the woman suspected of being a
carrier has n children. The posterior distribution of θ can be represented
as
Pr (θ = i|y) =
Pr (θ = i) Pr (y|θ = i)
Pr (θ = i) Pr (y|θ = i) + Pr (θ ̸= i) Pr (y|θ ̸= i),
i = 0, 1,

222
5. An Introduction to Bayesian Inference
where y = [Y1, Y2, ..., Yn]′. Partition the data as y = [y′
A, y′
B]′ , with yA be-
ing the records on presence or absence of the disease for the ﬁrst m progeny,
and with yB containing data on the last n −m children. The posterior dis-
tribution is
Pr (θ = i|y) =
Pr (θ = i) p (yA|θ = i) p (yB|yA, θ = i)
1
i=0
Pr (θ = i) p (yA|θ = i) p (yB|yA, θ = i)
.
Dividing the numerator and denominator by the marginal probability of
observing yA, that is, by p (yA) gives
Pr (θ = i|y) =
Pr (θ = i) p (yA|θ = i)
p (yA)
p (yB|yA, θ = i)
1
i=0
Pr (θ = i) p (yA|θ = i)
p (yA)
p (yB|yA, θ = i)
.
Note, however, that
Pr (θ = i) p (yA|θ = i)
p (yA)
= Pr (θ = i|yA)
is the posterior probability after observing yA, which acts as a prior before
observing yB. Then, it follows that
Pr (θ = i|y) =
Pr (θ = i|yA) p (yB|yA, θ = i)
1
i=0
Pr (θ = i|yA) p (yB|yA, θ = i)
illustrating the “memory” property of Bayes theorem. If the observations
are conditionally independent, as assumed in this example, then
p (yB|yA, θ = i) = p (yB|θ = i) .
Suppose now that the woman has a third, unaﬀected, son. The prior dis-
tribution now assigns probabilities of 4
5 and 1
5 to the events “not being a
carrier” and “carrying the allele”, respectively. The posterior probability
of the woman being a carrier, after observing a third, unaﬀected child, is
Pr (θ = 1|Y1 = 0, Y2 = 0, Y3 = 0)
=
1
5 Pr (Y3 = 0|θ = 1)
1
5 Pr (Y3 = 0|θ = 1) + 4
5 Pr (Y3 = 0|θ = 0)
=
1
5
1
2
1
5
1
2 + 4
51 = 1
9.

5.3 Bayes Theorem: Continuous Case
223
The same result is obtained starting from the prior before observing any
children
Pr (θ = 1|Y1 = 0, Y2 = 0, Y3 = 0) =
1
2.
 1
2
3
1
2.
 1
2
3 + 1
2. (1)3
= 1
9.
■
5.3
Bayes Theorem: Continuous Case
In a somewhat narrower setting, consider now the situation where the role
of the evidence E is played by a vector of observations y, with the hypoth-
esis H replaced by a vector of unknowns θ. The latter will be generally re-
ferred to as the “parameter” vector, although θ can include random eﬀects
(in the usual frequentist sense), missing data, censored observations, etc.
It will be assumed that θ and y are continuous valued, and that a certain
probability model M posits the joint distribution [θ, y|M] . For example,
M may postulate that this distribution is jointly Gaussian, whereas model
M ′ supposes a multivariate-t distribution. Alternatively, M and M ′ could
represent two alternative explanatory structures in a regression model. At
this point it will be assumed that there is complete certainty about model
M holding, although this may not be so, in which case one encounters the
important problem of Bayesian model selection. We will return to this later
on but now, with the understanding that developments are conditional on
model M, the dependency on the model will be abandoned in the notation.
The joint density of θ and y can be written as
h (θ, y) = g (θ) f (y|θ) = m(y)p (θ|y) ,
(5.9)
where
• g (θ) is the marginal density of θ:
g (θ) =

h (θ, y) dy =

p (θ|y) m(y) dy =Ey [p (θ|y)] ,
The corresponding distribution describes the plausibility of values
that θ takes in a parameter space Θ, unconditionally on the ob-
servations y. This is the density of the prior distribution of θ, which
provides a summary of nonsample information about the parameters.
Values of θ outside of Θ have null density. For example, a reason-
able Bayesian model would assign null density to values of a genetic
correlation outside of the [−1, 1] boundaries, or to negative values of

224
5. An Introduction to Bayesian Inference
a variance component. In the preceding, as well as in all subsequent
developments, it will be assumed that the required integrals exist,
unless stated otherwise.
• f (y|θ) is the density of values that y takes at a given, albeit unknown,
value of θ. It represents the likelihood that evidence y confers to θ;
the part of f (y|θ) that varies with θ is called the likelihood function
or, simply, the likelihood of θ. The set of possible values that y can
take is given by ℜy, the sampling space of y. The integration above
is implicitly over this sampling space. For example, if the data vector
contains some truncated random variables, the sampling space would
be given by the boundaries within which these variables are allowed
to vary.
• m(y) is the density of the marginal distribution of the observations.
In the Bayesian context, this does not depend on θ, since
m (y) =

h (θ, y) dθ =

f (y|θ) g (θ) dθ = Eθ [f (y|θ)] ,
where the integration is over the sample space of θ, Θ. This implies
that m (y) is the average, taken over the prior distribution of θ, of
all possible likelihood values that the evidence y would confer to θ.
• p (θ|y) is the density of the posterior distribution of θ, [θ|y] , provid-
ing a summary of the information about θ contained in both y and
in the prior distribution [θ] . From (5.9), the posterior density can be
written as
p (θ|y) = g (θ) f (y|θ)
m(y)
∝g (θ) f (y|θ)
(5.10)
as one is interested in variation with respect to θ only. Further, let
L (θ|y) be the part of f (y|θ) varying with θ, or likelihood function.
Thus, an alternative representation of the posterior density is
p (θ|y) ∝g (θ) L (θ|y) ,
(5.11)
or
p (θ|y) =
g (θ) L (θ|y)

g (θ) L (θ|y) dθ .
(5.12)
A special case of Bayesian analysis is encountered when the prior
density g (θ) is uniform over the entire parameter space; in other
words, g (θ) is proportional to a constant. This is called a “ﬂat prior”.
The constant cancels in the numerator and denominator of (5.12), and
the posterior becomes
p (θ|y) =
L (θ|y)

L (θ|y) dθ ∝L (θ|y) .

5.3 Bayes Theorem: Continuous Case
225
Hence, the posterior is proportional solely to the likelihood, and it
exists only if the integral of the likelihood over θ is ﬁnite. Otherwise,
the posterior density is said to be improper. Note that if the required
integral converges, the integration constant of the posterior is the
reciprocal of the integrated likelihood. Also, if the posterior distri-
bution exists, the mode of the posterior density is identical to the
ML estimator. This suggests that this estimator is a feature of any
(existing) posterior distribution constructed from an initial position
where the observer is indiﬀerent to any of the possible values of θ,
when this is represented by the uniform distribution.
Example 5.3
Inferring additive genetic eﬀects
Let an observation for a quantitative trait be y. Suppose that a reasonable
model for representing a phenotypic value is
y = µ + a + e,
where µ is a known constant, a is the additive genetic eﬀect of the individ-
ual, and e is an environmental deviation. Let a ∼N(0, va) and e ∼N(0, ve)
be independently distributed, where va and ve are the additive genetic
and environmental variances, respectively, both being assumed known here.
Thus, y must also be normal. It follows that the conditional distribution of
y given a is the normal process
[y|µ, a, va, ve] ∼N (µ + a, ve) .
Suppose the problem is inferring a from y. Here, θ = a. The prior density
of a is normal, with the parameters given above. The posterior distribution
of interest is then:
[a|y, µ, va, ve]
which is identical to the conditional distribution of a given y in a frequentist
sense. If a and y are jointly normal, as is the case here, it follows directly
that the posterior distribution must be normal as well, with mean
E (a|y, µ, va, ve)
=
E (a) + Cov(a, y) V ar−1 (y) (y −µ)
=
va
va + ve
(y −µ) = h2 (y −µ) ,
where h2 is the heritability of the trait. The posterior variance is
V ar (a|y, µ, va, ve) = va −
v2
a
va + ve
= va

1 −h2
so there will always be some uncertainty about the genetic value, given the
phenotypic value, unless the genotype has complete penetrance, that is,

226
5. An Introduction to Bayesian Inference
when there is no environmental variance. In view of the mean and variance
of the posterior distribution, the corresponding density is
p (a|y, µ, va, ve) =
1
>
2πva (1 −h2)
exp
#
−

a −h2 (y −µ)
2
2va (1 −h2)
$
.
Note that the prior opinion has been modiﬁed by the evidence provided by
observation of the phenotypic value. For example, in the prior distribution
the modal value of a is 0, whereas it is h2 (y −µ) , a posteriori. Further, in
the prior distribution, the probability that a > 0 is 1/2. In the posterior
distribution, this value is
Pr (a > 0|y, µ, va, ve) = 1 −Pr (a ≤0|y, µ, va, ve)
= 1 −Φ

−h2 (y −µ)
>
va (1 −h2)

= Φ

h2 (y −µ)
>
va (1 −h2)

.
The marginal density of the observation is
p (y|µ, va, ve) =
1
>
2π (va + ve)
exp

−(y −µ)2
2 (va + ve)

.
This follows because the model states that the phenotypic value is a linear
combination of two normally distributed random variables. The expression
is identical to the marginal density of the observations in a frequentist
setting, because our Bayesian model does not postulate uncertainty about
µ, va and ve. Otherwise, the marginal density given above would need to
be deconditioned over the prior distribution of µ, va, and ve.
■
Example 5.4
Inferring additive genetic eﬀects from repeated measures
Let the setting be as in the preceding example. Suppose now that n in-
dependent measurements are taken on the individual, and that µ is an
unknown quantity, with prior distribution µ ∼N (µ0, v0) , where the hy-
perparameters (parameters of the prior distribution) are known. The model
for the ith measure is then
yi = µ + a + ei.
(5.13)

5.3 Bayes Theorem: Continuous Case
227
If µ and a are assumed to be independent, a priori, the joint posterior
density is
p (µ, a|y1, y2, ..., yn, µ0, v0, va, ve)
∝
n
-
i=1
p(yi|µ, a, ve)p (a|va) p (µ|µ0, v0)
∝
n
-
i=1
exp

−(yi −µ −a)2
2ve

exp

−a2
2va

exp

−(µ −µ0)2
2v0

∝exp

−
n
i=1
(yi −µ −a)2
2ve
−a2
2va

exp

−(µ −µ0)2
2v0

.
(5.14)
Now
n
i=1
(yi −µ −a)2
ve
+ a2
va
=
n
i=1
[a + µ −yi]2
ve
+ a2
va
=
n
i=1
[a −(y −µ) −(yi −y)]2
ve
+ a2
va
=
n [a −(y −µ)]2 +
n
i=1
(yi −y)2
ve
+ a2
va
,
where y is the average of the n records; recall that n
i=1 (yi −y) = 0. Using
this in the joint posterior (5.14):
p (µ, a|y, µ0, v0, va, ve)
∝exp

−1
2
#
n [a −(y −µ)]2
ve
+ a2
va
$
exp

−(µ −µ0)2
2v0

(5.15)
as expressions not involving µ or a get absorbed in the integration constant.
Now, there is an identity for combining quadratic forms (Box and Tiao,
1973) stating that
M(z −m)2 + B(z −b)2 = (M + B)(z −c)2 +
MB
M + B (m −b)2
(5.16)
with
c = (M + B)−1 (Mm + Bb) .
(5.17)

228
5. An Introduction to Bayesian Inference
Now put
m
=
y −µ,
b
=
0,
M
=
n
ve
,
B
=
1
va
,
z
=
a.
Hence, by analogy,
n [a −(y −µ)]2
ve
+ a2
va
=

 n
ve
+ 1
va
 #
a −

 n
ve
+ 1
va
−1  n
ve
(y −µ)
$2
+
n
ve
1
va
n
ve +
1
va
(y −µ)2.
Employing this decomposition, the joint posterior density in (5.15) is ex-
pressible as
p (µ, a|y, µ0, v0, va, ve)
∝exp

−1
2

 n
ve
+ 1
va
 #
a −

 n
ve
+ 1
va
−1  n
ve
(y −µ)
$2

× exp

−n(y −µ)2
2 (nva + ve)

exp

−(µ −µ0)2
2v0

.
(5.18)
From this, one can proceed to derive a series of distributions of interest, as
given below.
Conditional (given µ) posterior density of the additive genetic eﬀect. This
is obtained by retaining the part of the joint posterior that varies with a:
p (a|µ, y, µ0, v0, va, ve)
∝exp

−1
2

 n
ve
+ 1
va
 #
a −

 n
ve
+ 1
va
−1  n
ve
(y −µ)
$2

(5.19)
This is clearly the density of a normal distribution with mean
E (a|µ, y, µ0, v0, va, ve)
=

 n
ve
+ 1
va
−1  n
ve
(y −µ)

=
va
va + ve
n
(y −µ)
=
n
n + 1−h2
h2
(y −µ)

5.3 Bayes Theorem: Continuous Case
229
and variance
V ar (a|µ, y, µ0, v0, va, ve) =

 n
ve
+ 1
va
−1
= ve

n + 1 −h2
h2
−1
.
Note that this posterior distribution depends on the data only through y.
Conditional (given a) posterior density of µ. This is arrived at in a
similar manner, that is, by retaining in (5.14) only the terms varying with
µ. One obtains
p (µ|a, y, µ0, v0, va, ve) ∝exp

−
n
i=1
(yi −µ −a)2
2ve

exp

−(µ −µ0)2
2v0

.
Here, using some of the previous results,
n
i=1
(yi −µ −a)2
ve
+ (µ −µ0)2
v0
=
n [µ −(y −a)]2 +
n
i=1
(yi −y)2
ve
+ (µ −µ0)2
v0
.
There are now two quadratic forms on µ that can be combined, employing
(5.16) and (5.17), as
n [µ −(y −a)]2
ve
+ (µ −µ0)2
v0
=

 n
ve
+ 1
v0
 #
µ −

 n
ve
+ 1
v0
−1  n
ve
(y −a) + µ0
v0
$2
+
n
ve . 1
v0
n
ve + 1
v0
(y −µ0 −a)2 .
(5.20)
Using this in the conditional density, and retaining only the terms that
vary with µ
p (µ|a, y, µ0, v0, va, ve)
∝exp

−1
2

 n
ve
+ 1
v0
 #
µ −

 n
ve
+ 1
v0
−1  n
ve
(y −a) + µ0
v0
$2
.
(5.21)

230
5. An Introduction to Bayesian Inference
It follows that the preceding density is that of a normal process with pa-
rameters
E (µ|a, y, µ0, v0, va, ve) =

 n
ve
+ 1
v0
−1  n
ve
(y −a) + µ0
v0

= µ0 +
n
n + ve
v0
(y −µ0 −a)
and
V ar (µ|a, y, µ0, v0, va, ve) =

 n
ve
+ 1
v0
−1
= ve

n + ve
v0
−1
.
Marginal posterior density of µ. This is found by integrating the joint
density (5.18) over a:
p (µ|y, µ0, v0, va, ve)
=

p (µ, a|y, µ0, v0, va, ve) da
∝exp
#
−1
2

n
(nva + ve)(y −µ)2 + (µ −µ0)2
v0
$
×

exp

−1
2

 n
ve
+ 1
va
 #
a −

 n
ve
+ 1
va
−1  n
ve
(y −µ)
$2
da.
The integral is over a normal kernel, and evaluates to
C
2π

 n
ve
+ 1
va
−1
.
Noting that the expression does not involve µ, it follows that
p (µ|y, µ0, v0, va, ve) ∝exp
#
−1
2

n
(nva + ve)(y −µ)2 + (µ −µ0)2
v0
$
.
Using (5.16) and (5.17), the two quadratic forms on µ can be combined as
n
(nva + ve)(µ −y)2 + (µ −µ0)2
v0
= 1
Vµ
(µ −µ)2 +
1

v0 + va + ve
n
 (y −µ0)2
where
µ = µ0 +
v0

v0 + va + ve
n
 (y −µ0) ,
Vµ = v0

1 −
v0
v0 + va + ve
n

.

5.3 Bayes Theorem: Continuous Case
231
Thus
p (µ|y, µ0, v0, va, ve) ∝exp

−1
2V −1
µ
(µ −µ)2

× exp

−1
2

v0 + va + ve
n
−1
(y −µ0)2

.
The second term does not depend on µ, so
p (µ|y, µ0, v0, va, ve) ∝exp

−1
2V −1
µ
(µ −µ)2

.
(5.22)
Thus the marginal posterior distribution is µ ∼N (µ, Vµ) . It is seen, again,
that this posterior distribution depends on the data through y.
Marginal posterior density of a. Note that
p (µ|a, y, µ0, v0, va, ve) = p (µ, a|y, µ0, v0, va, ve)
p (a|y, µ0, v0, va, ve) ,
so
p (a|y, µ0, v0, va, ve) = p (µ, a|y, µ0, v0, va, ve)
p (µ|a, y, µ0, v0, va, ve)
(5.23)
with the densities in the numerator and denominator given in (5.15) and
(5.21), respectively. Alternatively, an instructive representation can be ob-
tained by rewriting the joint density in (5.15). Employing (5.20), this can
be put as
p (µ, a|y, µ0, v0, va, ve)
∝exp

−1
2
#
n
ve . 1
v0
n
ve + 1
v0
(y −µ0 −a)2 + a2
va
$
× exp

−1
2

 n
ve
+ 1
v0
 #
µ −

 n
ve
+ 1
v0
−1  n
ve
(y −a) + µ0
v0
$2
.
Integrating over µ, to obtain the marginal posterior density of a, yields
p (a|y, µ0, v0, va, ve)
∝exp

−1
2
#
n
ve . 1
v0
n
ve + 1
v0
(y −µ0 −a)2 + a2
va
$
×

exp

−1
2

 n
ve
+ 1
v0
 #
µ −

 n
ve
+ 1
v0
−1  n
ve
(y −a) + µ0
v0
$2
dµ

.
(5.24)

232
5. An Introduction to Bayesian Inference
The integral involves a normal kernel and evaluates to
C
2π

 n
ve
+ 1
v0
−1
which, by not being a function of a, gets absorbed in the integration con-
stant. Further, using (5.16) and (5.17),
n
ve . 1
v0
n
ve + 1
v0
(y −µ0 −a)2 + a2
va
=
n
nv0 + ve
[a −(y −µ0)]2 + a2
va
= 1
Va
(a −a)2 +
1

v0 + va + ve
n
 (y −µ0)2 ,
(5.25)
where
a =
va
v0 + va + ve
n
(y −µ0) ,
Va = va

1 −
va
v0 + va + ve
n

.
Employing (5.25) in (5.24), and retaining only the term in a,
p (a|y, µ0, v0, va, ve) ∝exp

−1
2V −1
a
(a −a)2

.
(5.26)
In conclusion, the marginal posterior density of the additive genetic eﬀect is
normal with mean a and variance Va; it depends on the data only through
y.
Marginal distribution of the data. Finding the marginal density of the
observations, i.e., the denominator of Bayes theorem, is also of interest. As
we shall see later, the corresponding distribution plays an important role
in model assessment. First, observe that the joint density of y, µ, and a,
given the hyperparameters, is
p(y, µ, a|µ0, v0, va, ve) =
n
-
i=1
p(yi|µ, a, ve)p (a|va) p (µ|µ0, v0)
(5.27)
with all kernels in a normal form. Using results developed previously, the
forms inside of the exponents can be combined as
n
i=1
(yi −µ −a)2
ve
+ a2
va
+ (µ −µ0)2
v0

5.3 Bayes Theorem: Continuous Case
233
=
n [a −(y −µ)]2 +
n
i=1
(yi −y)2
ve
+ a2
va
+ (µ −µ0)2
v0
=
n
i=1
(yi −y)2
ve
+ (µ −µ0)2
v0
+ n [a −(y −µ)]2
ve
+ a2
va
=
n
i=1
(yi −y)2
ve
+ (µ −µ0)2
v0
+

 n
ve
+ 1
va
 #
a −

 n
ve
+ 1
va
−1  n
ve
(y −µ)
$2
+
n
ve
1
va
n
ve +
1
va
(y −µ)2.
Using the preceding, the marginal density of the observations is
p(y|µ0, v0, va, ve)
=
 
n
-
i=1
p(yi|µ, a, ve)p (a|va) p (µ|µ0, v0) da dµ
∝exp

−
n
i=1
(yi −y)2
2ve



exp

−n(y −µ)2
2 (nva + ve)

exp

−(µ −µ0)2
2v0

dµ
×

exp

−1
2

 n
ve
+ 1
va
 #
a −

 n
ve
+ 1
va
−1  n
ve
(y −µ)
$2
da.
(5.28)
The last integral evaluates to
C
2π

 n
ve
+ 1
va
−1
and since it does not involve y, it gets absorbed in the integration constant
of (5.28). Thus
p(y|µ0, v0, va, ve) ∝exp

−
n
i=1
(yi −y)2
2ve


×

exp

−n(y −µ)2
2 (nva + ve)

exp

−(µ −µ0)2
2v0

dµ.
(5.29)

234
5. An Introduction to Bayesian Inference
It was seen before that
n(µ −y)2
(nva + ve) + (µ −µ0)2
v0
= n(y −µ)2
(nva + ve) + (µ −µ0)2
v0
= (µ −µ)2
Vµ
+
(y −µ0)2

v0 + va + ve
n
.
Making use of this in (5.29), one gets, after integrating over µ,
p(y|µ0, v0, va, ve) ∝exp







−1
2


n
i=1
(yi −y)2
ve
+
(y −µ0)2

v0 + va + ve
n










.
(5.30)
This is the density of an n-dimensional distribution, that depends on the
data through y, and through the sum of squared deviations of the observa-
tions from the mean. It will be shown that this is the kernel of the density
of the n-variate normal distribution
y|µ0, v0, va, ve ∼N(1µ0, V)
(5.31)
where 1 denotes a vector of 1′s of order n, and
V = (v0 + va) J + veI
is the variance–covariance matrix of the process, where J is an n×n matrix
of 1′s. Now, the inverse of V is given (Searle et al., 1992) by
V−1 = 1
ve

I −
v0 + va
ve + n (v0 + va)J

.
Hence, the kernel of the density of the multivariate normal distribution
[y|µ0, v0, va, ve] can be put as
p (y|µ0, v0, va, ve) ∝exp

−
1
2 (y −1µ0)′ 1
ve

I−
v0 + va
ve + n (v0 + va)J
%
× (y −1µ0)] .
Now,
(y −1µ0)′ 1
ve

I −
v0 + va
ve + n (v0 + va)J

(y −1µ0)
= [y −1y −1 (µ0 −y)]′ 1
ve

I −
v0 + va
ve + n (v0 + va)J

× [y −1y −1 (µ0 −y)]

5.4 Posterior Distributions
235
= 1
ve
 n

i=1
(yi −y)2 + n (y −µ0)2 −(v0 + va) n2 (y −µ0)2
ve + n (v0 + va)

=
n
i=1
(yi −y)2
ve
+ n
ve
(y −µ0)2

1 −
n (v0 + va)
ve + n (v0 + va)

=
n
i=1
(yi −y)2
ve
+
(y −µ0)2
(v0 + va) + ve
n
.
Thus
p (y|µ0, v0, va, ve) ∝exp







−1
2


n
i=1
(yi −y)2
ve
+
(y −µ0)2

v0 + va + ve
n










and this is precisely (5.30). Hence, the marginal distribution of the ob-
servations is the normal process given in (5.31). This distribution is often
referred to as the prior predictive distribution of the observation, that is,
the stochastic process describing the probabilities with which data occur,
before any observation is made. Its density depends entirely on the pa-
rameters of the prior distributions, commonly called “hyperparameters”.
This result could have been anticipated by viewing (5.13) as a random ef-
fects model, where the two independently distributed random eﬀects have
distributions µ ∼N (µ0, v0) and a ∼N (0, va) . Since, in vector notation,
y = 1 (µ + a) + e
is a linear combination of normal variates, where e ∼N (0, Ive) , it follows
immediately that y (given µ0) must be normal, with mean vector
E (y |µ0 ) = 1µ0
and variance–covariance matrix
E (y |v0, va, ve ) = 11′ (v0 + va) + Ive
= J (v0 + va) + Ive.
■
5.4
Posterior Distributions
Consider Bayes theorem in any of the forms given in (5.10) to (5.12), and
partition the vector of all quantities subject to uncertainty as θ =

θ′
1, θ′
2
′ ,

236
5. An Introduction to Bayesian Inference
where θ1 and θ2 represent distinct, unknown features of the probability
model. For example, in a linear model, θ1 may be the location vector and
θ2 the dispersion components, i.e., the variance and covariance parameters.
The joint posterior density of all unknowns is
p (θ1, θ2|y)
=
L (θ1, θ2|y) g (θ1, θ2)
 
L (θ1, θ2|y) g (θ1, θ2) dθ1 dθ2
∝
L (θ1, θ2|y) g (θ1, θ2) ,
(5.32)
where L (θ1, θ2|y) is the likelihood function (joint density or distribution
of the observations, viewed as a function of the unknowns), and g (θ1, θ2)
is the joint prior density. The latter is typically a multivariate density
function, and elicitation may be facilitated by writing
g (θ1, θ2) = g (θ1|θ2) g (θ2) = g (θ2|θ1) g (θ1) .
Here g (θi) is the marginal prior density of θi, and g (θj|θi) for i ̸= j is
the conditional prior density of θj given θi. Hence, one can assign prior
probabilities to θ1, and then to θ2 at each of the values of θ1, or to θ2,
and then to θ1, given θ2. Irrespective of the form and order of elicitation,
one must end up with the same joint process; otherwise, there would be in-
coherence in the probabilistic ensemble. Often, it happens (sometimes for
mathematical convenience) that the two sets of parameters are assigned
independent prior distributions. However, it is uncommon that parameters
remain mutually independent, a posteriori. For this to occur, the likelihood
must factorize into independent pieces, as well, with each of the portions
corresponding to each of the sets of parameters. It will be seen later that pa-
rameters that are independent a priori can become dependent a posteriori,
even when a single data point is observed.
The marginal posterior densities of each parameter (or set of parameters)
are, by deﬁnition
p (θ1|y) =

p (θ1, θ2|y) dθ2
(5.33)
and
p (θ2|y) =

p (θ1, θ2|y) dθ1.
(5.34)
It may be necessary to carry out the marginalization to further levels in a
Bayesian analysis. For example, suppose that θ1 =

θ′
1A, θ′
1B
′ where θ1A
is a vector of additive genetic eﬀects, say, and θ1B includes some other
location parameters. Then, if one wishes to assign posterior probabilities
(inference) to additive genetic eﬀects, the marginal posterior density to be
used is
p (θ1A|y)
=
 
p (θ1, θ2|y) dθ1B dθ2
=

p (θ1|y) dθ1B.
(5.35)

5.4 Posterior Distributions
237
In any of the situations described above, the parameters that are inte-
grated out of the joint posterior density are referred to as nuisance pa-
rameters. Technically, these are components of the statistical model that
need to be considered, because the probabilistic structure adopted requires
it, but that are not of primary inferential interest. Suppose that θ1 is of
primary interest, in which case the distribution to be used for inference is
the process [θ1|y] . The corresponding marginal density can be rewritten
as
p (θ1|y)
=

p (θ1, θ2|y) dθ2
=

p (θ1|θ2, y) p (θ2|y) dθ2
(5.36)
=
Eθ2|y [p (θ1|θ2, y)] ,
(5.37)
where p(θ1|θ2, y) is the density of the conditional posterior distribution of
θ1 given θ2. Representations (5.36) and (5.37) indicate that the marginal
density of the primary parameter θ1 is the weighted average of an inﬁ-
nite number of conditional densities p(θ1|θ2, y), where the weighting or
mixing function is the marginal posterior density of the nuisance parame-
ter, p(θ2|y). The conditional posterior distribution [θ1|θ2, y] describes the
uncertainty of inferences about θ1 that can be drawn when the nuisance
parameter θ2 is known, whereas the marginal distribution [θ2|y] gives the
relative plausibility of diﬀerent values of the nuisance parameter, in the light
of any prior information and of other assumptions built into the model, and
of the evidence provided by the data (Box and Tiao, 1973).
The conditional posterior distributions can be identiﬁed (at least con-
ceptually) from the joint posterior distribution, with the latter following
directly from the assumptions, once a prior and a data generating process
are postulated. Note that the conditional posterior density of a parameter
can be expressed as
p (θ1|θ2, y) = p (θ1, θ2|y)
p (θ2|y)
.
Since, in this distribution, one is interested in variation with respect to θ1
only, the denominator enters merely as part of the integration constant.
Thus, one can write
p (θ1|θ2, y) ∝p (θ1, θ2|y)
∝L (θ1, θ2|y) p (θ1, θ2)
∝L (θ1, θ2|y) p (θ1|θ2)
∝L (θ1|θ2, y) p (θ1|θ2) .
(5.38)
Above, L (θ1|θ2, y) is the likelihood function with θ2 treated as a known
constant, rather than as a feature subject to uncertainty. The preceding de-
velopment implies that a conditional posterior distribution can (often) be

238
5. An Introduction to Bayesian Inference
identiﬁed by inspection of the joint posterior density and by retaining only
the parts that vary with the parameter(s) of interest, treating the remain-
ing parts as known. This method can be useful for identifying conditional
posterior distributions in the context of MCMC methods, as discussed in
a subsequent chapter.
Example 5.5
Posterior dependence between parameters
Suppose that a sample of size n is obtained by drawing independently
from the same normal distribution N

µ, σ2
, where the mean and variance
are both unknown. Assume that the parameters are taken as following
independent distributions, with prior densities,
p (µ|a, b) =
1
b −a,
p

σ2|c, d

=
1
d −c,
where a, b, c, d are bounds on parameter values that have been elicited some-
how. The joint posterior density is
p

µ, σ2|y, a, b, c, d

∝
n
-
i=1

σ2−1
2 exp

−(yi −µ)2
2σ2

1
(b −a) (d −c)
∝

σ2−n
2 exp

−1
2σ2
n

i=1
(yi −µ)2

∝

σ2−n
2 exp

−
n
i=1
(yi −y)2 + n (µ −y)2
2σ2


(5.39)
this being nonnull for a < µ < b and c < σ2 < d. The marginal posterior
density of σ2 is obtained by integrating over µ:
p

σ2|y, a, b, c, d

∝

σ2−n
2 exp

−1
2σ2
n

i=1
(yi −y)2

×
b

a
√
2πσ2/n
√
2πσ2/n exp

−n(µ−y)2
2σ2

dµ
∝

σ2−n−1
2
exp

−1
2σ2
n

i=1
(yi −y)2
 
Φ

b−y
σ/√n

−Φ

a−y
σ/√n

.
(5.40)
The diﬀerence between the integrals is equal to 1 if µ is allowed to take
values anywhere in the real line (i.e., if a = −∞and b = ∞). The marginal

5.4 Posterior Distributions
239
posterior density of µ is found by integrating (5.39) with respect to σ2:
p (µ|y, a, b, c, d) ∝
d

c

σ2−n
2 exp

−1
2σ2
n

i=1
(yi −µ)2

dσ2
∝
d

c

σ2−( n−2
2
+1) exp

−Sµ
σ2

dσ2,
where
Sµ = 1
2
n

i=1
(yi −µ)2 .
The integral above cannot be written in closed form. However, if one takes
the positive part of the real line as parameter space for σ2, that is, c = 0
and d = ∞, use of properties of the inverse gamma (or scaled inverse chi-
square) distribution seen in Chapter 1 yields
p (µ|y, a, b)
∝
∞

0

σ2−( n−2
2
+1) exp

−Sµ
σ2

dσ2 = Γ
 n−2
2

S
n−2
2
µ
∝
S
−n−2
2
µ
∝
 n

i=1
(yi −y)2 + n (µ −y)2
−n−2
2
.
In this density, only variation with respect to µ is of concern. Hence, one
can factor out the sum of squared deviations of the observations from the
sample mean, to obtain
p (µ|y, a, b)
∝

1 +
n (µ −y)2
n
i=1
(yi −y)2


−n−3+1
2
∝

1 + (µ −y)2
(n −3) s2
n
−n−3+1
2
,
(5.41)
where
5s2 =
1
n −3
n

i=1
(yi −y)2 .
If µ were allowed to take values anywhere in the real line, (5.41) gives
the kernel of a t-distribution with n −3 degrees of freedom, mean y, and
variance equal to
5s2 (n −3)
n (n −5) =
n
i=1
(yi −y)2
n (n −5)
.

240
5. An Introduction to Bayesian Inference
This distribution is proper if n > 3, and the variance is ﬁnite if n > 5.
However, since in the example µ takes density only in the [a, b] interval, it
turns out that the marginal posterior distribution is a truncated t-process
with density
p (µ|y, a, b) =

1 +
(µ−y)2
(n−3) s2
n
−n−3+1
2
b

a

1 + (µ −y)2
(n −3) s2
n
−n−3+1
2
dµ
.
(5.42)
Finally, note that the product of (5.42) and (5.40) does not yield (5.39),
even if a = −∞, b = ∞, c = 0, and d = ∞. Hence µ and σ2 are not
independent in the posterior distribution, even if they are so, a priori.
■
Example 5.6
Conditional posterior distribution
Revisit Example 5.5, and consider ﬁnding the two induced conditional
posterior distributions. From representation (5.39) of the joint posterior
density, one can deduce the density of

µ|y, a, b, c, d, σ2
, treating σ2 as a
constant. This yields
p

µ|y, a, b, c, d, σ2
∝exp

−n (µ −y)2
2σ2

I (a < µ < b)
which is the density of a normal distribution truncated between a and b. The
mean and variance of the untruncated distribution are y and σ2/n, respec-
tively. Similarly, the density of the conditional distribution

σ2|y, a, b, c, d, µ

is arrived at by regarding µ as a constant in the joint density, to obtain
p

σ2|y, a, b, c, d, µ

∝

σ2−( n−2
2
+1) exp

−Sµ
σ2

I

c < σ2 < d

.
This is in an inverse gamma form (truncated between c and d), and the
parameters of the distribution in the absence of truncation are (n −2)/2
and Sµ.
■
Example 5.7
Posterior distributions in a linear regression model with t
distributed errors
Suppose a response y is related to a predictor variable x according to the
linear relationship
yi = β0 + β1xi + ϵi,
i = 1, 2, ..., n,
(5.43)
where the residuals are i.i.d. as
t

0, σ2, ν

,

5.4 Posterior Distributions
241
where σ2 is the scale parameter of the t-distribution and ν are the degrees
of freedom, with the latter assumed known. Recall from Chapter 1 that an
equivalent representation of (5.43) is given by
yi = β0 + β1xi +
ei
√wi
,
(5.44)
where ei ∼N

0, σ2
and wi ∼Ga
 ν
2, ν
2

are independently distributed
random variables, i = 1, 2, ..., n. If model (5.44) is deconditioned with re-
spect to the gamma random variable, then (5.43) results. Let the parameter
vector include the unknown parameters of the model, that is, the regression
coeﬃcients and σ2, plus all the gamma weights wi. This is known as “data
augmentation” (Tanner and Wong, 1987) and is discussed in Chapter 11.
The augmented parameter vector is
θ =

β0, β1, σ2, w1, w2, ..., wn
′
and adopt as joint prior density
p

β0, β1, σ2, w1, w2, ..., wn|ν

= p (β0) p (β1) p

σ2
n
-
i=1
p (wi|ν) .
(5.45)
The joint posterior density is
p

β0, β1, σ2, w1, w2, ..., wn|y, ν

∝
n
-
i=1

p

yi|β0, β1, σ2, wi

p (wi|ν)

×p (β0) p (β1) p

σ2
.
(5.46)
Explicitly, this takes the form
p

β0, β1, σ2, w1, w2, ..., wn|y, ν

∝
n
-
i=1

σ2
wi
−1
2
w
ν
2 −1
i
exp
#
−wi
2

(yi −β0 −β1xi)2 + νσ2
σ2
$
×p (β0) p (β1) p

σ2
.
(5.47)
Conditional posterior distribution of wi given all other parameters. Note
in (5.47) that, given all other parameters, the w′s are mutually independent.
The kernel of the conditional posterior density of wi is found by collecting
terms that depend on this random variable. Thus, for i = 1, 2, ..., n,
p

wi|β0, β1, σ2, y, ν

∝w
ν+1
2
−1
i
exp

−wiSi
2

,
where
Si = (yi −β0 −β1xi)2 + νσ2
σ2
.

242
5. An Introduction to Bayesian Inference
This implies that the conditional posterior distribution of wi is the gamma
process
wi|β0, β1, σ2, y, ν ∼Ga

ν + 1
2
, ν + (yi −β0 −β1xi)2 /σ2
2

.
(5.48)
The observations enter only through data point i.
Conditional posterior distribution of β0 and β1 given all other parame-
ters. From the joint density (5.47):
p

β0, β1|σ2, w, y, ν

∝
n
-
i=1
exp

−wi
2σ2 (yi −β0 −β1xi)2
p (β0) p (β1) .
(5.49)
It is not possible to be more speciﬁc about the form of the distribution
unless the priors are stated explicitly. For example, suppose that, a priori,
β0 ∼N

α0, σ2
β0

and β1 ∼N

α1, σ2
β1

. Then,
p

β0, β1|σ2, w, y, ν, α0, σ2
β0, α1, σ2
β1

∝exp
#
−1
2σ2
 n

i=1
wi (yi −β0 −β1xi)2
+ σ2
σ2
β0
(β0 −α0)2 + σ2
σ2
β1
(β1 −α1)2
$
.
(5.50)
Now
n

i=1
wi (yi −β0 −β1xi)2 = (y −Xβ)′ W (y −Xβ) ,
(5.51)
where
β =

β0
β1

and
W
=
Diag (wi) ,
X =
 1
x 
n×2 ,
1
=
{1} ,
x = [x1, x2, ..., xn]′ .
Deﬁne the following function of the data (and of the w′s):
5β =

5β0
5β1

= (X′WX)−1 X′Wy.
(5.52)

5.4 Posterior Distributions
243
Then
(y −Xβ)′ W (y −Xβ)
=

y −X5β −Xβ + X5β
′
W

y −X5β −Xβ + X5β

=

y −X5β −X

β−5β
′
W

y −X5β −X

β −5β

=

y −X5β
′
W

y −X5β

+

β −5β
′
X′WX

β −5β

(5.53)
because the cross-product term vanishes, as a consequence of the deﬁnition
of 5β:
2

β −5β

X′W

y −X5β

= 2

β −5β
 
X′Wy −X′WX5β

= 2

β −5β

(X′Wy −X′Wy) = 0.
Employing (5.53) in (5.51), the posterior density in (5.50) becomes
p

β0, β1|σ2, w, y, ν, α0, σ2
β0, α1, σ2
β1

∝exp

−1
2σ2

y −X5β
′
W

y −X5β

+

β −5β
′
X′WX

β −5β

+ σ2 (β0 −α0)2
σ2
β0
+ σ2 (β1 −α1)2
σ2
β1
$
∝exp

−1
2σ2

β −5β
′
X′WX

β −5β

+ σ2 (β0 −α0)2
σ2
β0
+ σ2 (β1 −α1)2
σ2
β1
$
,
(5.54)
upon retaining only the terms that vary with β. Deﬁning
α =
 α0
α1

,
Λ =


σ2
σ2
β0
0
0
σ2
σ2
β1

=

λ0
0
0
λ1

the conditional posterior that concerns us is expressible as
p

β0, β1|σ2, w, y, ν, α0, σ2
β0, α1, σ2
β1

∝
exp

−1
2σ2

β −5β
′
X′WX

β −5β

+ (β −α)′ Λ (β −α)

.
(5.55)

244
5. An Introduction to Bayesian Inference
The two quadratic forms on β can be combined using an extension of
formulas (5.16) and (5.17), as given in Box and Tiao (1973),
(z −m)′M(z −m) + (z −b)′B(z −b)
=
(z −c)′(M + B)(z −c)
+ (m −b)′ M (M + B)−1 B (m −b)
(5.56)
with
c = (M + B)−1 (Mm + Bb) .
(5.57)
Employing this in the context of (5.55):

β −5β
′
X′WX

β −5β

+ (β −α)′ Λ (β −α)
=

β −β
′ (X′WX + Λ)

β −β

+(5β−α)′X′WX (X′WX + Λ)−1 Λ(5β−α)
(5.58)
with
β
=
(X′WX + Λ)−1 
X′WX5β + Λα

(5.59)
=
(X′WX + Λ)−1 (X′Wy + Λα) .
(5.60)
Using (5.58) in (5.55) and retaining only the part that varies with β:
p

β0, β1|σ2, w, y, ν, α0, σ2
β0, α1, σ2
β1

∝exp

−1
2σ2

β −β
′ (X′WX + Λ)

β −β

.
(5.61)
Thus, the posterior distribution of the regression coeﬃcients, given all other
parameters, is normal, with mean vector as in (5.59) or (5.60) and variance–
covariance matrix
V ar

β0, β1|σ2, w, y, ν, α0, σ2
β0, α1, σ2
β1

= (X′WX + Λ)−1 σ2.
(5.62)
Note that the mean of this posterior distribution is a matrix-weighted av-
erage of 5β and α, where the weights are X′WX and Λ, respectively.
Conditional posterior distribution of β1 given β0 and all other param-
eters. If β0 is known, it can be treated as an oﬀset in the model, that is,
one can write a “new” response variable
ri = yi −β0 = β1xi + ϵi.
Put r = {ri} . Using this in the joint posterior density, and treating β0 as
a constant, yields
p

β1|β0, σ2, w, y, ν, α0, σ2
β0, α1, σ2
β1

∝
exp
#
−1
2σ2
 n

i=1
wi (ri −β1xi)2 + λ1 (β1 −α1)2
$
.

5.4 Posterior Distributions
245
Using similar algebra as in (5.51)to (5.60), the conditional density can be
represented in the Gaussian form
p

β1|β0, σ2, w, y, ν, α0, σ2
β0, α1, σ2
β1

∝exp
#
−1
2σ2
 n

i=1
wi (ri −β1xi)2 + λ1 (β1 −α1)2
$
∝exp

−1
2σ2

β1 −β1.0
′ (x′Wx+λ1)

β1 −β1.0

,
(5.63)
where
β1.0
=
(x′Wx + λ1)−1 (x′Wr + λ1α1)
=
n
i=1
wixi (yi −β0) + λ1α1
n
i=1
wix2
i + λ1
and the variance of the process is
V ar

β1|β0, σ2, w, y, ν, α0, σ2
β0, α1, σ2
β1

= (x′Wx + λ1)−1 σ2
=
σ2
n
i=1
wix2
i + λ1
.
Conditional posterior distribution of β0 given β1 and all other param-
eters. The development is similar, except that the oﬀset is now β1xi, to
form the “new” response
ti = yi −β1xi = β0 + ϵi
with t = {yi −β1xi} . The resulting density is
p

β0|β1, σ2, w, y, ν, α0, σ2
β0, α1, σ2
β1

∝exp

−1
2σ2

β0 −β0.1
′ (1′W1+λ0)

β0 −β0.1

,
(5.64)
where
β0.1
=
(1′W1 + λ0)−1 (1′Wt + λ0α0)
=
n
i=1
wi (yi −β1xi) + λ0α0
n
i=1
wi + λ0

246
5. An Introduction to Bayesian Inference
and
V ar

β0|β1, σ2, w, y, ν, α0, σ2
β0, α1, σ2
β1

=
σ2
n
i=1
wi + λ0
.
Conditional posterior distribution of σ2 given all other parameters. Re-
taining terms in σ2 in the joint density of all parameters given in (5.47),
one obtains
p

σ2|β0, β1, w1, w2, ..., wn, y, ν

∝

σ2−n
2 exp

−1
2σ2
n

i=1
wi (yi −β0 −β1xi)2

p

σ2
.
(5.65)
It is not possible to go further unless an explicit statement is made about
the prior distribution of σ2. For example, suppose that the prior distribu-
tion is lognormal, that is, the logarithm of σ2 follows a Gaussian distribu-
tion with mean 0 and variance ω. Then, as seen in Chapter 2, the prior
density of σ2 takes the form
p

σ2|ω

∝

σ2−1 exp

−

log σ22
2ω

.
The conditional posterior density of σ2 would be
p

σ2|β0, β1, w, y, ν

∝

σ2−n+2
2
exp
#
−1
2σ2
 n

i=1
wi (yi −β0 −β1xi)2 + σ2 
log σ22
ω
$
,
which is not in a recognizable form. In this situation, further analytical
treatment is not feasible. On the other hand, suppose that elicitation yields
a scaled inverse chi-square distribution with parameters Q and R as a
reasonable prior. The corresponding density (see Chapter 1) is
p

σ2|Q, R

∝

σ2−( Q
2 +1) exp

−QR
2σ2

.
(5.66)
Upon using (5.66) in (5.65), the conditional posterior density of σ2 turns
out to be
p

σ2|β0, β1, w1, w2, ..., wn, y, ν

∝

σ2−( n+Q
2
+1) exp

−Q∗R∗
2σ2
%
. (5.67)
This is the density of a scaled inverse chi-square process with parameters
Q∗= n + Q and
R∗= ns2 + QR
n + Q
,

5.4 Posterior Distributions
247
with
s2 =
n
i=1
wi (yi −β0 −β1xi)2
n
.
Note that R∗is a weighted average between R (“a prior value of the vari-
ance”) and s2 (“a variance provided by the data, recalling that the w′s
are treated as observed in this conditional distribution). This is another
example of the data-prior compromise that arises in Bayesian analysis.
Conditional posterior distribution of β0 and β1 given w. Suppose the
prior density of σ2 is scaled inverse chi-squared, as in (5.66). Then, using
this in (5.47) and integrating over σ2 while keeping w ﬁxed, gives
p (β0, β1|w, y, Q, R, ν) ∝p (β0) p (β1)
∞

0

σ2−n
2
×
n
-
i=1
exp

−wi
2
(yi −β0 −β1xi)2
σ2

×

σ2−( Q
2 +1) exp

−QR
2σ2

dσ2
∝p (β0) p (β1)
∞

0

σ2−( n+Q
2
+1)
× exp







−
n
i=1
wi (yi −β0 −β1xi)2 + QR
2σ2







dσ2
∝p (β0) p (β1)
∞

0

σ2−( n+Q
2
+1)
× exp

−(y −Xβ)′ W (y −Xβ) + QR
2σ2
%
dσ2
(5.68)
after making use of (5.51). The integrand is in a scaled inverse gamma
form, so the expression becomes
p (β0, β1|w, y, Q, R, ν)
∝p (β0) p (β1)

(y −Xβ)′ W (y −Xβ) + QR
−( n+Q
2 )
.
Recall the decomposition in (5.53), that is,
(y −Xβ)′ W (y −Xβ)
=

y −X5β
′
W

y −X5β

+

β −5β
′
X′WX

β −5β

.

248
5. An Introduction to Bayesian Inference
Using this in the preceding density, rearranging and keeping only terms in
β gives
p (β0, β1|w, y, Q, R, ν)
∝p (β0) p (β1)

1 +

β −5β
′
X′WX

β −5β


y −X5β
′
W

y −X5β

+ QR


−( n−2+Q+2
2
)
∝p (β0) p (β1)

1 +

β −5β
′
X′WX

β −5β

(n −2 + Q) k2


−( n−2+Q+2
2
)
,
(5.69)
where
k2 =

y −X5β
′
W

y −X5β

+ QR
(n −2 + Q)
.
The expression in brackets in (5.69) is the kernel of the density of a bivariate
t distribution having mean vector 5β, scale parameter matrix
(X′WX)−1 k2
and n −2 + Q degrees of freedom. However, the form of the distribution
[β0, β1|w, y, Q, R, ν] will depend on the form of the priors adopted for the
regression coeﬃcients. For example, if these priors are uniform, the poste-
rior distribution is a truncated bivariate-t deﬁned inside the corresponding
boundaries.
Joint posterior density of β0, β1, and σ2, unconditionally to w. This is
obtained by integrating over w. Note that the joint density (5.47) factorizes
into n independent parts that can be integrated separately, to obtain
p

β0, β1, σ2|y, ν

∝
n
-
i=1
∞

0

σ2
wi
−1
2
×w
ν
2 −1
i
exp
#
−wi
2

(yi −β0 −β1xi)2 + νσ2
σ2
$
dwip (β0) p (β1) p

σ2
∝p (β0) p (β1) p

σ2 
σ2−n
2
n
-
i=1
∞

0
w
ν+1
2
−1
i
exp

−wiSi
2
%
dwi
recalling that
Si = (yi −β0 −β1xi)2 + νσ2
σ2
.

5.5 Bayesian Updating
249
The integrand in the preceding expression is the kernel of a gamma density
with parameters (ν + 1) /2 and Si/2, so that one obtains
p

β0, β1, σ2|y, ν

∝p (β0) p (β1) p

σ2 
σ2−n
2
×
n
-
i=1
Γ

ν + 1
2
 
Si
2
−ν+1
2
∝p (β0) p (β1) p

σ2 
σ2−n
2
×
n
-
i=1

(yi −β0 −β1xi)2 + νσ2
σ2
−ν+1
2
∝p (β0) p (β1) p

σ2
×
n
-
i=1

σ2−1
2

1 + (yi −β0 −β1xi)2
νσ2
−ν+1
2
.
(5.70)
This is precisely the joint posterior distribution of all parameters for the
linear regression model in (5.43), assuming known degrees of freedom. Ad-
ditional marginalization is not possible by analytical means, but one can
estimate lower-dimensional posterior distributions of interest using Monte
Carlo methods, as discussed later in the book.
■
5.5
Bayesian Updating
The concept of Bayesian learning in a discrete setting was discussed in
a previous section; see (5.5). This will be revisited for continuous-valued
parameters and observations. Suppose that data accrue sequentially as
y1, y2, ..., yK, and that the problem is to infer a parameter vector θ. The
posterior density of θ is
p (θ|y1, y2, ..., yK) = g (θ) p (y1, y2, . . . , yK|θ)
m (y1, y2, . . . , yK)
= g (θ) p (y1|θ) p (y2|y1, θ) ...p (yK|y1, . . . , yK−1, θ)
m (y1) m (y2|y1) . . . m (yK|y1, . . . , yK−1)
∝g (θ) p (y1|θ) p (y2|y1, θ) . . . p (yK|y1, . . . , yK−1, θ) .
(5.71)
The posterior distribution of any function h(θ) is arrived at by making a
one-to-one transformation of the parameter vector such that one of the new
variables is h (or set of variables, if h is vector-valued), and then integrating
over the remaining “dummy” variables. From (5.71) it follows that
p (θ|y1, y2, . . . , yK) ∝p (θ|y1) p (y2|y1, θ) . . . p (yK|y1, y2, . . . , yK−1, θ)
∝p (θ|y1, y2) . . . p (yK|y1, y2, . . . , yK−1, θ)
∝p (θ|y1, y2, . . . , yK−1) p (yK|y1, y2, . . . , yK−1, θ) .
(5.72)

250
5. An Introduction to Bayesian Inference
Note that the posterior distribution at stage i of learning acts as a prior for
stage i + 1, as already noted in the discrete case. This implies that a single
Bayesian analysis carried out at the end of the process will lead to the
same inferences about θ as one carried out sequentially. If data accruing at
diﬀerent stages are conditionally independent, (5.71) becomes
p (θ|y1, y2, . . . , yK) ∝g (θ) p (y1|θ) p (y2|θ) . . . p (yK|θ)
∝g (θ)
K
-
i=1
p (yi|θ) .
(5.73)
Example 5.8
Progeny test of dairy bulls
Suppose that S unrelated dairy bulls are mated to unrelated cows, leading
to ni daughters per bull. Suppose, for simplicity, that the milk production
of such daughters is measured under the same environmental conditions. A
linear model for the production of daughter j of bull i could be
yij = si + eij,
where, in the dairy cattle breeding lexicon, si is known as the “transmitting
ability” of bull i, and eij ∼N

0, σ2
e

is a residual, assumed to be indepen-
dently distributed of any si, and of any other residual. Let the average
production of daughters of bull i be yi. The model for such an average is
yi =
ni

j=1
yij
ni
= si + ei,
where ei ∼N

0, σ2
e/ni

is the average of individual residuals. We seek
to infer si, given information on the average production of the daughters.
Suppose that, a priori, each of the transmitting abilities is assigned the
distribution si ∼N

µ, σ2
s

, and that these are independent among bulls;
assume that µ, σ2
s, and σ2
e are known. The posterior distribution of all
transmitting abilities, based on averages is
p

s1, s2, . . . , sS|y, y2, . . . , yS, µ, σ2
s, σ2
e

∝
S
-
i=1
exp

−ni
2σ2e
(yi −si)2
 S
-
i=1
exp

−1
2σ2s
(si −µ)2

=
S
-
i=1
exp

−1
2σ2e

ni (si −yi)2 + σ2
e
σ2s
(si −µ)2
%
.
(5.74)
Hence, the transmitting abilities of all bulls are also independent a poste-
riori; this would not be so if bulls were genetically related, in which case a
multivariate prior would need to be elicited (a situation to be encountered

5.5 Bayesian Updating
251
in the following example). Now, the two quadratic forms in the transmitting
ability in (5.74) can be combined employing (5.16) as
ni (si −yi)2 + σ2
e
σ2s
(si −µ)2 =

ni + σ2
e
σ2s

(si −si)2 +
ni
σ2
e
σ2
s
ni + σ2
e
σ2
s
(yi −µ)2 ,
(5.75)
where:
si =

ni + σ2
e
σ2s
−1 
niyi + σ2
e
σ2s
µ

=
niyi
ni + σ2
e
σ2
s
+

1 −
ni
ni + σ2
e
σ2
s

µ
= µ +
ni
ni + σ2
e
σ2
s
(yi −µ) .
(5.76)
Using (5.75) in (5.74) and retaining only the portion that varies with si
gives as posterior density of the transmitting ability of bull i:
p

si|yi, µ, σ2
s, σ2
e

∝exp

−

ni + σ2
e
σ2
s

2σ2e
(si −si)2

.
(5.77)
Hence, the posterior distribution is normal, with mean as in (5.76) and
variance
V ar

si|yi, µ, σ2
s, σ2
e

=
σ2
e
ni + σ2
e
σ2
s
.
(5.78)
Suppose now that, for each sire, the data arrive sequentially in two “crops”
of daughters of sizes ni1 and ni2, respectively. We proceed to verify that if
the posterior distribution after crop 1 is used as prior for crop 2, one obtains
the posterior distribution with density as in (5.77). From the preceding
developments, it follows immediately that the posterior density after crop
1 is
p

si|yi1, µ, σ2
s, σ2
e

∝exp

−1
2σ2e

ni1 + σ2
e
σ2s

(si −si1)2
%
,
(5.79)
where yi1 is the average production of daughters in crop 1, and si1 is the
mean of the posterior distribution after crop 1. Using this as prior for crop
2, the posterior distribution, after observing that the average production
of the ni2 cows is yi2, is
p

si|yi1, yi2, µ, σ2
s, σ2
e

∝exp

−ni2 (yi2 −si)2
2σ2e
−

ni1 + σ2
e
σ2
s

(si −si1)2
2σ2e

.
(5.80)

252
5. An Introduction to Bayesian Inference
Combining the two quadratic forms as before
ni2 (si −yi2)2 +

ni1 + σ2
e
σ2s

(si −si1)2
=

ni1 + ni2 + σ2
e
σ2s
 
si −si
2 +
ni2

ni1 + σ2
e
σ2
s

ni1 + ni2 + σ2
e
σ2
s
(yi2 −si1)2
where
si =
ni2yi2 +

ni1 + σ2
e
σ2
s

si1
ni1 + ni2 + σ2
e
σ2
s
=
ni2yi2 +

ni1 + σ2
e
σ2
s
 
µ +
ni1
ni1+
σ2e
σ2s
(yi1 −µ)

ni1 + ni2 + σ2
e
σ2
s
=
ni1yi1 + ni2yi2 + σ2
e
σ2
s µ
ni1 + ni2 + σ2
e
σ2
s
= ni1yi1 + ni2yi2
ni1 + ni2 + σ2
e
σ2
s
+

1 −
ni1 + ni2
ni1 + ni2 + σ2
e
σ2
s

µ
= µ +
ni
ni + σ2
e
σ2
s
(yi −µ) = si,
which is identical to (5.76). Use of this in (5.80), after retaining only the
part that involves si, and noting that ni1 + ni2 = ni, gives
p

si|yi1, yi2, µ, σ2
s, σ2
e

∝exp

−

ni + σ2
e
σ2
s

(si −si)2
2σ2e

.
Thus, the posterior density is identical to (5.77), illustrating that Bayes
theorem has “memory”, and that inferences can be updated sequentially.
As a side point, note that there can be two alternative scenarios in this
hypothetical scheme. In scenario A, say, inferences are done at the end, and
all one needs for constructing the posterior (given the prior information)
is ni and yi, without knowledge of the averages of each of the two crops of
daughters. In the sequential updating setting (scenario B), one needs yi1,
yi2, ni1, and ni2. Hence, scenario B requires knowing the progeny group
sizes and the averages at each crop, i.e., more information about the data
collection process is needed. At any rate, the Bayesian analysis leads to
the same inferences. This is related to what are called “stopping rules” in

5.5 Bayesian Updating
253
sequential experimental design (O’Hagan, 1994). Suppose an experiment
is designed such that it terminates after collecting n observations. Then,
the Bayesian analysis infers the parameters using the n observations, and
inferences are the same irrespective of whether the experiment has been
designed sequentially or not. On the other hand, classical methods give
diﬀerent inferences for the same data collected either in a sequential or
nonsequential manner. For example, if a hypothesis is tested, the sequential
experiment gives a lower degree of signiﬁcance than the nonsequential one
(O’Hagan, 1994).
■
Example 5.9
Updating additive genetic eﬀects
The setting is similar to that of the preceding example. Suppose that at
stage 1 (2), measurements y1 (y2) are taken on n1 (n2) diﬀerent individuals
(so that an individual measured at any stage is not recorded at the other
stage), and that the objective is to infer their additive genetic eﬀects a1
(a2) . Suppose the following linear model holds

y1
y2

=

11µ1
12µ2

+

a1
a2

+

e1
e2

,
(5.81)
where µ1 and µ2 are known location parameters common to records col-
lected in stages 1 and 2, respectively, and

e1
e2
"""" σ2
e ∼N


0
0

,

I1
0
0
I2

σ2
e

is a vector of independently distributed residual eﬀects, where σ2
e is the
(known) residual variance; Ii is an identity matrix of order ni×ni, (i = 1, 2).
The conditional distribution of the observations, given the additive genetic
eﬀects, is
p

y1, y2|µ1, µ2, a1, a2, σ2
e

∝exp

−
2
i=1
(yi −1iµi −ai)′ (yi −1iµi −ai)
2σ2e

.
(5.82)
In the classical inﬁnitesimal model of inheritance, the additive genetic ef-
fects are assumed to follow the multivariate normal distribution (acting as
a prior in the Bayesian sense):

a1
a2
"""" σ2
a ∼N


0
0

,

A11
A12
A21
A22

σ2
a

(5.83)
where σ2
a is the additive genetic variance in the population (also assumed
known), and
A =

A11
A12
A21
A22


254
5. An Introduction to Bayesian Inference
is the matrix of additive genetic relationships between individuals, or twice
the matrix of coeﬃcients of coancestry. This matrix is assumed to have
full rank, that is, clones or identical twins are not encountered. For ex-
ample, A12 contains the additive genetic relationships between individuals
measured at stages 1 and 2. Recall that

A11 −A12A−1
22 A21

σ2
a = A1.2σ2
a
(5.84)
is the covariance matrix of the conditional distribution of a1 given a2, and
that

A22 −A21A−1
11 A12

σ2
a = A2.1σ2
a
(5.85)
is the covariance matrix of the conditional distribution of a2 given a1.
Further, A21A−1
11 is the multivariate regression of additive genetic eﬀects
of individuals measured in stage 2 on additive genetic eﬀects of individuals
measured in stage 1. In fact, one can write
a2 = E (a2|a1) + ϵ
= E (a2) + A21A−1
11 [a1 −E (a1)] + ϵ
= A21A−1
11 a1 + ϵ,
(5.86)
where ϵ is a residual distributed independently of a1, and having the dis-
tribution
ϵ ∼N

0, A2.1σ2
a

.
(5.87)
Further, using properties of inverses of partitioned matrices (Searle, 1971),
let
A−1 =

A11
A12
A21
A22

,
where
A11 =

A11 −A12A−1
22 A21
−1 = A−1
1.2,
A12 = −A11A12A−1
22 ,
A21 = −A−1
22 A21A11,
and
A22 = A−1
22 + A−1
22 A21A11A−1
22 .
Suppose that data collection at stage 2 has been completed, and that we
proceed to infer the breeding values of all individuals. The posterior density
of the additive genetic eﬀects, in view of (5.82) and (5.83), can be written
as
p

a1, a2|y1, y2, µ1, µ2, σ2
e, σ2
a

∝exp

−1
2σ2e

(a −w)′ (a −w) + σ2
e
σ2a
a′A−1a
%
,
(5.88)

5.5 Bayesian Updating
255
where
w =

y1 −11µ1
y2 −12µ2

=

w1
w2

.
Combining the two quadratic forms in (5.88), by means of (5.56) to (5.57),
(a −w)′ (a −w) + σ2
e
σ2a
a′A−1a
= (a −5a)′

I + A−1 σ2
e
σ2a

(a −5a) + w′

I + A−1 σ2
e
σ2a
−1
A−1 σ2
e
σ2a
w,
where
5a =

I + A−1 σ2
e
σ2a
−1
w =

I + A−1 σ2
e
σ2a
−1 
y1 −11µ1
y2 −12µ2

.
(5.89)
Using this in (5.88) and retaining only the part that varies with a gives as
posterior density of the additive genetic eﬀects,
p

a1, a2|y1, y2, µ1, µ2, σ2
e, σ2
a

∝exp

−1
2σ2e

(a−5a)′

I + A−1 σ2
e
σ2a

(a−5a)
%
.
(5.90)
Thus, the joint posterior of all additive genetic eﬀects is normal with mean
vector 5a and variance–covariance matrix
V ar

a1, a2|y1, y2, µ1, µ2, σ2
e, σ2
a

=

I + A−1 σ2
e
σ2a
−1
σ2
e.
(5.91)
Using (5.89) to (5.91), the density of the “ﬁrst-stage” distribution

a1|y1, µ1, σ2
e, σ2
a

is immediately found to be
p

a1|y1, µ1, σ2
e, σ2
a

∝exp

−1
2σ2e

(a1 −,a1)′

I1 + A−1
11
σ2
e
σ2a

(a1 −,a1)
%
.
(5.92)
The posterior mean at stage 1 is then
,a1 =

I1 + A−1
11
σ2
e
σ2a
−1
(y1 −11µ1) =

I1 + A−1
11
σ2
e
σ2a
−1
w1
(5.93)
and the posterior covariance is
V ar

a1|y1, µ1, σ2
e, σ2
a

=

I1 + A−1
11
σ2
e
σ2a
−1
σ2
e = ,C1.
(5.94)

256
5. An Introduction to Bayesian Inference
What can be said about all additive genetic eﬀects at stage 1? The joint
posterior at stage 1 is
p

a1, a2|y1, µ1, σ2
e, σ2
a

∝

p

y1|µ1, σ2
e

p

a1|σ2
a

p

a2|a1, σ2
a

.
Noting that the expression in brackets is the posterior after stage 1, one
can write
p

a1, a2|y1, µ1, σ2
e, σ2
a

∝p

a2|a1, σ2
a

p

a1|y1, µ1, σ2
e, σ2
a

and this is the density of a normal process because the two intervening
densities are in normal forms. Hence, the marginal distribution of a2 at
stage 1 is normal as well, with marginal density
p

a2|y1, µ1, σ2
e, σ2
a

=

p

a2|a1, σ2
a

p

a1|y1, µ1, σ2
e, σ2
a

da1.
The representation above indicates that the mean of the posterior distri-
bution of a2 at stage 1 can be found to be, making use of (5.86),
,a2 = E

a2|y1, µ1, σ2
e, σ2
a

= Ea1|y1 [E (a2|a1)]
= Ea1|y1

A21A−1
11 a1

= A21A−1
11 ,a1
= A21A−1
11

I1 + A−1
11
σ2
e
σ2a
−1
w1.
(5.95)
Note that this has the form of E (a2|a1) , but with a1 replaced by its pos-
terior expectation. Likewise,
,C2 = V ar

a2|y1, µ1, σ2
e, σ2
a

= Ea1|y1 [V ar (a2|a1)] + V ara1|y1 [E (a2|a1)]
= Ea1|y1

A22σ2
a −A21A−1
11 A12σ2
a

+ V ara1|y1

A21A−1
11 a1

= A22σ2
a −A21A−1
11 A12σ2
a + A21A−1
11 ,C1A−1
11 A12.
(5.96)
The ﬁrst term represents the variance of a2 before observing anything; the
second term is the reduction in variance that would be obtained if a1 were
known, and the third term is a penalty that results from having to infer
a1 from y1. Thus, the prior distribution for a2 to be used at stage 2 is
a normal process with mean vector (5.95) and covariance matrix (5.96).
Finally, at stage 2, the posterior density of a2 is
p

a2|y1, y2, µ1, µ2, σ2
e, σ2
a

∝p

y2|µ2, a2, σ2
e

p

a2|,a2, ,C2

∝exp

−(y2 −12µ2 −a2)′ (y2 −12µ2 −a2)
2σ2e
−(a2 −,a2)′ ,C−1
2 σ2
e (a2 −,a2)
2σ2e

.
(5.97)

5.6 Features of Posterior Distributions
257
We know that the density is in a normal form, so the quadratics on a2 can be
combined in the usual manner, to arrive at the mean vector and covariance
matrix of the distribution. Alternatively, noting that a normal distribution
is unimodal (so the mean is identical to the mode), the posterior mean at
stage 2 can be found by maximizing the logarithm of (5.97). Let
F (a2) = −
(y2 −12µ2 −a2)′ (y2 −12µ2 −a2)
2σ2e
+(a2 −,a2)′ ,C−1
2 σ2
e (a2 −,a2)
2σ2e

so
∂F (a2)
∂a2
= (−1) −2 (y2 −12µ2 −a2) + 2,C−1
2 σ2
e (a2 −,a2)
2σ2e
.
Setting to 0 and solving for a2 yields
5a2 =

I2 + ,C−1
2 σ2
e
−1 
y2 −12µ2 + ,C−1
2 σ2
e,a2

(5.98)
as mean of the posterior distribution of a2, after stages 1 and 2. This is a
matrix weighted average of ,a2 and of y2 −12µ2. The variance–covariance
matrix of the distribution is given by
V ar

a2|y1, y2, µ1, µ2, σ2
e, σ2
a

=

I2 + ,C−1
2 σ2
e
−1
σ2
e.
(5.99)
It can be veriﬁed that this is equal to the inverse of minus the matrix
of second derivatives of F (a2) with respect to a2. One can also verify
that (5.98) is identical to the a2-component of the solution to (5.89). This
requires very tedious algebra, so it is not shown here.
■
5.6
Features of Posterior Distributions
The marginal posterior distribution gives an exact, complete description of
the state of knowledge about an unknown, after having observed the data.
This unknown can be a parameter, a hypothesis, a model, or a yet to be
observed data point, and can be unidimensional or multidimensional, de-
pending on the inferential objectives. In principle, for reporting purposes,
one could present the posterior distribution, and then make a commented
tour of it, highlighting zones of relatively high density or probability, point-
ing out the existence of any multimodality, and indicating areas where the
true value of the parameter may be located. The posterior holds for sam-
ples of any size, so this gives a complete solution to the problem of ﬁnite
sample size inference. In a Bayesian report, however, due to space consider-
ations, all posterior distributions of interest cannot be presented. Instead,

258
5. An Introduction to Bayesian Inference
these are typically replaced by selected posterior summaries. Clearly, when
condensing all the information in the posterior distribution into a couple of
posterior summaries, some information about the form of the posterior is
lost. There are exceptions; for example, if the posterior distribution is nor-
mal, then a report of the mean and variance suﬃces for characterizing the
posterior process in full. Here we present some of the most widely used pos-
terior summaries, and discuss their justiﬁcation from a decision-theoretic
point of view.
5.6.1
Posterior Probabilities
A natural summary is provided by a set of probabilities that the true
parameter falls in some regions of interest. If Θ is the parameter space,
the probability that θ falls in some region ℜof Θ is
Pr (θ ∈ℜ|y) =

ℜ
p (θ|y) dθ.
(5.100)
Often, interest centers on just some of the elements of θ, say θ1, with the
remaining elements (θ2) acting as nuisance parameters. In this case, the
required probability is
Pr (θ1 ∈ℜ1|y) =

ℜ1

Θ2
p (θ2, θ1|y) dθ
=

ℜ1
p (θ1|y) dθ1,
(5.101)
where Θ2 is the parameter space of θ2. An alternative expression is
Pr (θ1 ∈ℜ1|y) =

ℜ1

Θ2
p (θ2, θ1|y) dθ
=

ℜ1

Θ2
p (θ1|θ2, y) p (θ2|y) dθ.
(5.102)
Reversing the order of integration one can write
Pr (θ1 ∈ℜ1|y) =

Θ2

ℜ1
p (θ1|θ2, y) dθ1

p (θ2|y) dθ2.
(5.103)
The term in brackets is the conditional probability that θ1 ∈ℜ1, given θ2
and y. Hence, the marginal probability can be expressed as
Pr (θ1 ∈ℜ1|y) = Eθ2|y [Pr (θ1 ∈ℜ1|θ2, y)] .
(5.104)
It follows that the posterior probability that θ1 ∈ℜ1 is the weighted av-
erage of the corresponding probabilities at each possible value of the nui-
sance parameter θ2, with the weight function being the marginal posterior

5.6 Features of Posterior Distributions
259
density p (θ2|y) . Expression (5.104) can be useful in connection with the
estimation of probabilities by Monte Carlo methods. Brieﬂy, suppose that
m samples are drawn from the posterior distribution of the nuisance pa-
rameter [θ2|y] , and that these samples are θ(1)
2 , θ(1)
2 , ..., θ(m)
2
. A consistent
estimator of the posterior probability that θ1 ∈ℜ1 is given by
6
Pr (θ1 ∈ℜ1|y) = 1
m
m

i=1
Pr

θ1 ∈ℜ1|θ(i)
2 , y

.
(5.105)
This sort of calculation can be useful when analytical integration over θ2
is not feasible or very diﬃcult. However, requirements include:
(a) it must be relatively easy to sample from [θ2|y] , and
(b) Pr

θ1 ∈ℜ1|θ(i)
2 , y

must be available in closed form, so that this con-
ditional probability can be evaluated at each draw θ(i)
2 .
An alternative consistent estimator of the integral (5.101), which is sim-
pler to compute, is
6
Pr (θ1 ∈ℜ1|y) = 1
m
m

i=1
I

θ(i)
1
∈ℜ1

,
where I (·) is the indicator function and θ(i)
1
are Monte Carlo draws from
[θ1|y]. Here one must be able to draw samples from [θ1|y].
Example 5.10
Posterior probability that the breeding value of an indi-
vidual exceeds a certain threshold
A sample of n unrelated individuals is drawn from a population of ovines
raised to produce cashmere. The problem is to compute the probability
that the breeding value (additive genetic eﬀect) of a particular individual
is larger or smaller than a certain quantity. This is similar to the setting
in Example 5.3. Suppose that a tenable model (although very naive in
practice) for describing cashmere ﬁber diameter measured in individual i
is
yi = ai + ei,
where ai is the breeding value of i for ﬁber diameter and ei is an environ-
mental eﬀect. It is known that ai has the normal distribution N

0, σ2
a

,
where σ2
a is unknown, and that it is statistically independent of ei ∼
N

0, σ2
e

, where σ2
e is also unknown. Assume that the “total” variance,
σ2
a+σ2
e, is known without error, and has been estimated from the variability
between individual measurements in a large collection of individuals. This
being the case, one can rescale the observations by dividing by
>
σ2a + σ2e,
to obtain
y∗
i = a∗
i + e∗
i
=
ai
>
σ2a + σ2e
+
ei
>
σ2a + σ2e
.

260
5. An Introduction to Bayesian Inference
Thus, a∗
i ∼N

0, h2
and e∗
i ∼N

0, 1 −h2
, where
h2 =
σ2
a
σ2a + σ2e
is the unknown heritability of cashmere ﬁber diameter, a parameter ranging
between 0 and 1. Hence, there is a single dispersion parameter, h2. The joint
posterior density of all unknowns is then
p

a∗
1, a∗
2, . . . , a∗
n, h2|y∗
∝p

h2
n
-
i=1
1
>
2π (1 −h2)
exp

−(y∗
i −a∗
i )2
2 (1 −h2)

,
(5.106)
where p

h2
is the prior density of heritability; let the prior distribution of
this parameter be uniform between 0 and 1. Combining the two quadratics
in a∗
i :
(a∗
i −y∗
i )2
(1 −h2) + a∗2
i
h2 =

1
1 −h2 + 1
h2

(a∗
i −a∗
i )2 +

1
1−h2 +
1
h2
−1
(1 −h2) h2
y∗2
i
=
1
(1 −h2) h2 (a∗
i −a∗
i )2 + y∗2
i ,
(5.107)
where
a∗
i = h2y∗2
i .
Using (5.107) in (5.106), the joint posterior is then
p

a∗
1, a∗
2, . . . , a∗
n, h2|y∗
∝

1
(1 −h2) h2
 n
2
n
-
i=1
exp

−1
2
(a∗
i −a∗
i )2
(1 −h2) h2

× exp

−1
2
n

i=1
y∗2
i

p

h2
.
(5.108)
This indicates that, given h2, breeding values have independent normal
distributions with means a∗
i and common variance

1 −h2
h2. Integrat-
ing now over the n breeding values gives, as marginal posterior density of
heritability,
p

h2|y∗
∝

1 −h2
h2−n
2 p

h2
n
-
i=1
∞

−∞
exp

−(a∗
i −a∗
i )2
2 (1 −h2) h2

da∗
i
∝

1 −h2
h2−n
2 p

h2 >
(1 −h2) h2
n
∝p

h2
.
(5.109)
Hence, the Bayesian analysis yields the prior as posterior distribution of
heritability. This is because the data do not contribute information about

5.6 Features of Posterior Distributions
261
the partition of the variance into an additive genetic and an environmental
component, even if the total or phenotypic variance is known. The situation
would be diﬀerent if some of the individuals were genetically related, but
this is beyond the scope of the example. It follows that the posterior distri-
bution of the nuisance parameter (heritability in this case) is precisely the
prior distribution of h2. Now, to infer features of the posterior distribution
of the breeding value of individual i while taking into account uncertainty
about the nuisance parameter h2, one must average over its posterior dis-
tribution. Recall that our objective is to calculate the posterior probability
that the breeding value is larger than a certain value (0, say). Using (5.103),
and taking into account that the posterior (or prior, in this case) density
of heritability is uniform
Pr (a∗
i > 0|y∗) =
1

0


∞

0
p

a∗
i |h2, y∗
da∗
i

p

h2
dh2
=
1

0

1 −Pr

a∗
i ≤0|h2, y∗
p

h2
dh2
= 1 −
1

0
Pr

a∗
i −a∗
i
>
(1 −h2) h2 ≤
−a∗
i
>
(1 −h2) h2 |h2, y∗

p

h2
dh2
= 1 −
1

0
Φ

−a∗
i
>
(1 −h2) h2

p

h2
dh2.
(5.110)
The integral in (5.110) cannot be expressed in closed form, since the in-
tegrand is an integral itself. However, it can be evaluated by Monte Carlo
methods, as follows:
(1) Draw m independent values of h2 from a U (0, 1) distribution; let each
draw be h2(j).
(2) Compute, for each draw, a∗(j)
i
= h2(j)y∗2
i .
(3) For each draw, calculate
Φ


−a∗(j)
i
8
1 −h2(j)
h2(j)

.
(4) Form the consistent estimator of the desired probability
6
Pr (a∗
i > 0|y∗) = 1 −1
m
m

j=1
Φ


−a∗(j)
i
8
1 −h2(j)
h2(j)

.
(5.111)
■

262
5. An Introduction to Bayesian Inference
5.6.2
Posterior Quantiles
Consider the scalar posterior distribution [θ|y] deﬁned in θL ≤θ ≤θU,
where θL and θU are the minimum and maximum values, respectively,
that θ can take. The α-quantile of the posterior distribution is the value q
satisfying the equation
Pr (θ ≤q|y) = α.
A quantile commonly used to characterize location of the posterior distri-
bution is the posterior median, m, such that
Pr (θ ≤q|y) = Pr (θ > q|y) = 1
2
or, in the continuous case,
m

−∞
p (θ|y) dθ =
∞

m
p (θ|y) dθ.
Quantiles arise in the construction of high-credibility sets, that is, sets
of θ values that contain the true parametric value at high probability. For
example, a credibility set of size 1−α is given by all possible values between
q1 and q2 such that
Pr (q1 ≤θ ≤q2|y) = 1 −α
(5.112)
Pr (θ ≤q1|y) = α
2 ,
Pr (θ > q2|y) = α
2 .
If α = 0.05, say, then q1 is the 2.5% quantile of the posterior distribution
and q2 is the 97.5% quantile. In principle, there can be many, perhaps inﬁ-
nite, credibility regions of size 1 −α. However, if the probability statement
(5.112) is such that all values within the interval are required to have higher
posterior density than values outside it, then (q1, q2) is called a 1−α highest
posterior density interval, or HPD for short (Box and Tiao, 1973). If such
a region exists, then an HPD region of size 1 −α deﬁnes the boundaries
unambiguously.
Using the median of a posterior distribution as a “point estimate” of θ has
a justiﬁcation on decision-theoretic grounds. To illustrate, let the parameter
vector be θ =

θ1, θ′
2
′ , where θ1 varies over the real line, and θ2 ∈ℜθ2 are
nuisance parameters. Deﬁne L

5θ1, θ1

to be a loss function, with minimum
value 0 when L

5θ1 = θ1, θ1

. Here, 5θ1 is a function involving the data and
possibly the hyperparameters. Now let the loss function have the form
L

5θ1, θ1

= a
"""5θ1 −θ1
"""
(5.113)

5.6 Features of Posterior Distributions
263
where a is a scalar constant. This implies that the loss is proportional to
the absolute “error of estimation”. The expected posterior loss is
E

L

5θ1, θ1

|y

=
∞

−∞

ℜθ2
a
"""5θ1 −θ1
""" p (θ1, θ2|y) dθ1dθ2
=
∞

−∞
a
"""5θ1 −θ1
"""



ℜθ2
p (θ1, θ2|y) dθ2

dθ1
=
∞

−∞
a
"""5θ1 −θ1
""" p (θ1|y) dθ1
(5.114)
since integration of the joint density over θ2 gives the marginal posterior
of θ1. Now
"""5θ1 −θ1
""" =
#
5θ1 −θ1,
for θ1 ≤5θ1,
θ1 −5θ1,
for 5θ1 < θ1.
Hence, following Zellner (1971),
E

L

5θ1, θ1

|y

= a


θ1

−∞

5θ1 −θ1

p (θ1|y) dθ1 +
∞

θ1

θ1 −5θ1

p (θ1|y) dθ1


∝





5θ1 Pr

θ1 ≤5θ1|y

−
θ1

−∞
θ1p (θ1|y) dθ1
+
∞

θ1
θ1p (θ1|y) dθ1 −5θ1

1 −Pr

θ1 ≤5θ1|y






.
(5.115)
We seek now the 5θ1 minimizing the expected posterior loss. Diﬀerentiating
(5.115) with respect to 5θ1, recalling that
∂
 θ1
−∞
θ1p (θ1|y) dθ1

∂5θ1
= 5θ1p

5θ1|y

and setting the derivative to 0, yields, after some terms cancel out
∂E

L

5θ1, θ1

|y

∂5θ1
= 2 Pr

θ1 ≤5θ1|y

−1 = 0.

264
5. An Introduction to Bayesian Inference
This gives the equation
Pr

θ1 ≤5θ1|y

= 1
2
which is satisﬁed by taking 5θ1 to be the median of the posterior distri-
bution. Hence, the median is optimum in the sense of minimizing the ex-
pected absolute “error of estimation”. The median is functionally invariant
under one-to-one transformation. This implies that if 5θ1 is the median of
the posterior distribution of θ, then g

5θ1

is the median of the posterior
distribution of g (θ) . Hence, g

5θ1

minimizes the expected posterior loss
E
9
L

g

5θ1

, g (θ1)

|y
:
=
∞

−∞
a
"""g

5θ1

−g (θ1)
""" p (θ1|y) dθ1.
5.6.3
Posterior Modes
The modal vector of the posterior distribution is deﬁned as
,θ = Arg max
θ
[p (θ|y)] = Arg max
θ
[c L (θ|y) g (θ)]
= Arg max
θ
{log [L (θ|y)] + log [g (θ)]} ,
(5.116)
that is, as the value of the parameter having highest density (or probability
in discrete situations). As noted in Section 5.3, if the prior distribution of θ
is uniform, then the posterior mode is identical to the maximum likelihood
estimator; this is an incidental, and not a fundamental issue in Bayesian
analysis. Since the posterior mode is interpretable as “the most likely value
of the parameter”, it is a natural candidate as a purveyor of information
about the location of the posterior distribution.
The mode has a decision-theoretic justiﬁcation as a point estimator of a
parameter, at least in the single parameter situation. Suppose that the loss
function has the following form (O’Hagan, 1994):
L

5θ1, θ1






0,
if
"""5θ1 −θ1
""" ≤b,
1,
if
"""5θ1 −θ1
""" > b,

5.6 Features of Posterior Distributions
265
for some constant b. The expected posterior loss is then
E

L

5θ1, θ1

|y

= 0 × Pr
"""5θ1 −θ1
""" ≤b|y

+ 1 × Pr
"""5θ1 −θ1
""" > b|y

= Pr
"""5θ1 −θ1
""" > b|y

= Pr

5θ1 −θ1 > b|y

+ Pr

θ1 −5θ1 > b|y

=
θ1−b

−∞
p (θ1|y) dθ1 +
∞

θ1+b
p (θ1|y) dθ1.
(5.117)
Now, take derivatives of (5.117) with respect to 5θ1 to locate a minimum
∂E

L

5θ1, θ1

|y

∂5θ1
= p

5θ1 −b|y

−p

5θ1 + b|y

.
(5.118)
Setting to zero gives, as ﬁrst-order condition,
p

5θ1 −b|y

= p

5θ1 + b|y

.
If 5θ1 is a mode of the posterior distribution, it must be true that p

5θ1|y

≥
p

5θ1 −b|y

and p

5θ1|y

≥p

5θ1 + b|y

. As b →0 the condition is sat-
isﬁed only by the mode. In the limit, the “optimal” 5θ1 is given by the
posterior mode.
In a multiparameter situation, the modal vector has as many elements as
there are parameters in the model. Here it is important to make a distinc-
tion between the components of the modal vector of the joint distribution,
and the modes of the marginal distribution of one or of a set of parame-
ters of interest, after some nuisance parameters have been integrated out.
Most often, the marginal models diﬀer from the corresponding component
of the joint modal vector. Exceptions occur, such as when the posterior is
multivariate normal or multivariate-t. In these cases, if one maximizes the
joint distribution, each of the components of the modal vector is identical
to the mode of the marginal distribution of the corresponding parameter.
With the usual notation, the mode of a joint posterior distribution is
deﬁned by the statement
 ,,θ1
,,θ2

= Arg max
θ1,θ2
{log (c) + log [L (θ1, θ2|y)] + log [g (θ1, θ2)]} .
The notation ,θ1, ,θ2 will be employed here to denote the marginal modes.
In computing joint modes, it is useful to note that
log [p (θ1, θ2|y)] = log [p (θ1|y)] + log [p (θ2|θ1, y)]
= log [p (θ2|y)] + log [p (θ1|θ2, y)] .

266
5. An Introduction to Bayesian Inference
In order to locate a maximum, the following system of equations must be
solved, simultaneously,
log [p (θ1, θ2|y)]
∂θ1
= ∂log [p (θ1|θ2, y)]
∂θ1
= 0
(5.119)
and
log [p (θ1, θ2|y)]
∂θ2
= ∂log [p (θ2|θ1, y)]
∂θ2
= 0.
(5.120)
Typically, this will deﬁne an iterative algorithm (Lindley and Smith, 1972).
Note that, in some sense, the two equations do not incorporate measures of
uncertainty about nuisance parameters provided by the marginal densities
of the parameters not involved in the diﬀerentiation. For example, in (5.119)
one constructs a system of equations as if θ2 were known. This suggests
that if a joint mode is used as a point estimator, there may be an implicit
overstatement of precision in the analysis. At least in some variance com-
ponent problems (e.g., Thompson, 1980; Harville, 1977) it has been found
that a component of a joint mode can lead to estimators of variance com-
ponents that are identically equal to zero when used with vague priors.
With multiparameter problems, the joint mode does not always take into
account fully the uncertainty about other parameters in the model that
may be acting as nuisances.
Example 5.11
Joint and marginal modes in comparisons between treat-
ment and control populations
Independent samples of sizes n1, n2, n3 are drawn at random from the
three normal populations N

µ1, σ2
, N

µ2, σ2
, and N

µ3, σ2
, respec-
tively, where the variance is common, but unknown. One of the populations
or “treatments” is a control of some sort. An attribute is measured in each
of the items sampled, and the objective is to infer the diﬀerences between
means µ1 −µ2 and, possibly, µ1 −µ3, marginally or jointly with µ1 −µ2.
Here σ2 legitimately represents a nuisance parameter in all cases, so it is
important to take into account uncertainty about dispersion in the analysis.
Also, note that there are three location parameters, but the desired infer-
ences involve either the marginal distribution of a linear combination of
two means (µ1 −µ2) , or a bivariate posterior distribution (that of µ1 −µ2
and µ1 −µ3). In these cases, although marginalization proceeds to diﬀerent
degrees, the Bayesian approach has a single (and simple) solution: trans-
form the variables as needed, and integrate nuisance parameters to ﬁnd
the target distribution. In many instances, this cannot be done analyti-
cally, but sampling methods are available, as discussed later in this book.
Fortunately, there is an analytical solution in our setting, which is devel-
oped below, step-by-step. The likelihood function of the four parameters

5.6 Features of Posterior Distributions
267
entering into the model is
p

y|µ1, µ2, µ3, σ2
∝
3
-
i=1
ni
-
j=1

σ2−ni
2 exp

−1
2σ2 (yij −µi)2

∝

σ2−n1+n2+n3
2
exp

−1
2σ2

i

j
(yij −µi)2


∝

σ2−n1+n2+n3
2
exp
#
−1
2σ2

i


j
(yij −yi)2
+ ni (µi −yi)2:
,
(5.121)
where yij is observation j in treatment i, and yi is the average value of all
observations in the treatment. The maximum likelihood estimators of the
parameters can be readily found to be yi for µi (i = 1, 2, 3), and
5σ2 =

i

j
(yij −yi)2
n1 + n2 + n3
for σ2. The ﬁnite sample distributions of the maximum likelihood estima-
tors are
yi
∼
N

µi, σ2
ni

,
i = 1, 2, 3,
5σ2
∼
σ2
n1 + n2 + n3
χ2
n1+n2+n3−3.
Put n = n1 + n2 + n3. Recall that the asymptotic distribution of 5σ2 is nor-
mal with mean σ2 and variance 2σ4/n, which diﬀers from the ﬁnite sample
distribution given above. On the other hand, the exact and asymptotic dis-
tributions of the means are identical. We now give a Bayesian structure
to the model, and adopt a bounded uniform prior between µmin and µmax
for each of the three location parameters, and a scaled inverted chi-square
distribution with parameters ν (degree of belief) and τ 2 (“prior value of
the variance”). The joint posterior density of all parameters, omitting hy-
perparameters in the notation, can be written as
p

µ1, µ2, µ3, σ2|y

∝p

y|µ1, µ2, µ3, σ2
p (µ1, µ2, µ3) p

σ2|ν, τ

∝

σ2−n+ν+2
2
exp
#
−1
2σ2

n5σ2 + ντ 2 +

i
ni (µi −yi)2
$
(5.122)

268
5. An Introduction to Bayesian Inference
deﬁned within the boundaries given above. The mode of the joint posterior
distribution is arrived at by diﬀerentiating the logarithm of (5.122) with
respect to all parameters. The maximizers can be veriﬁed to be
,,µi = yi,
for i = 1, 2, 3 and ,,σ
2 = n5σ2 + ντ 2
n + ν + 2 .
(5.123)
The marginal posterior density of σ2 is obtained by integrating (5.122) over
the µ′s. Note that the resulting expression can be written as
p

σ2|y

∝

σ2−n+ν+2
2
exp

−1
2σ2

n5σ2 + ντ 2
×
3
-
i=1
µmax

µmin
exp

−ni (µi −yi)2
2σ2

dµi
∝

σ2−(n1−1+n2−1+n3−1)+ν+2
2
exp

−1
2σ2

n5σ2 + ντ 2
×
3
-
i=1

Φ

µmax −yi
σ2/ni

−Φ

µmin −yi
σ2/ni

.
(5.124)
This density is not explicit in σ2, and ﬁnding the maximizer requires tai-
loring an iterative algorithm. Suppose that the upper and lower boundaries
of the prior distributions of the means approach minus and plus inﬁnity,
respectively. In the limit, this would give a very vague (in fact, improper)
uniform prior distribution. In this case, the diﬀerence between the normal
c.d.f.’s given above goes to 1 for each of the three populations, and the
marginal posterior density of σ2 tends to
p

σ2|y

∝

σ2−(n1−1+n2−1+n3−1)+ν+2
2
exp

−1
2σ2

n5σ2 + ντ 2
. (5.125)
This is a scaled inverted chi-square density with degree of belief parameter
ν∗= n + ν −3,
and scale parameter
τ ∗2 = n5σ2 + ντ 2
ν∗
.
The mode of this marginal distribution is
,σ2 = ν∗τ ∗2
ν∗+ 2.
(5.126)
Comparison of (5.126) with ,,σ
2 in (5.123), assuming the same vague priors
for the means as in the last situation, illustrates that the mode of a marginal

5.6 Features of Posterior Distributions
269
distribution can diﬀer from the corresponding modal component of a joint
distribution. As a side issue, note that the process of integrating out the
three unknown means from the posterior distribution results in a “loss of
three degrees of freedom”, reﬂecting the straightforward, automatic book-
keeping of information that the probability calculus makes in the Bayesian
context. These three degrees of freedom are not accounted for in the joint
maximization leading to the modal component given in (5.123). Assign
now a scale inverted chi-square process prior to σ2 but, instead, take ν =
0 as a “prior degree of belief” value. In this scenario, the prior density
degenerates to σ−2, which is improper, as the integral between 0 and ∞is
not ﬁnite. However, the marginal posterior density is still proper provided
that at least two observations are collected in at least one of the three
populations. The improper prior σ−2 appears often in Bayesian analysis. In
general, we recommend exercising utmost caution when improper priors are
assigned to parameters, because the posterior distribution may turn out to
be improper, and this is not always straightforward to check. On the other
hand, proper priors ensure that the posterior process will be a proper one.
Next, we proceed to ﬁnd the marginal posterior distribution of the three
means, after integrating σ2 out of the joint density of all parameters, given
in (5.122). When this density is viewed as a function of σ2, it can be readily
seen that it is in an inverse gamma form, whose integration constant we
know (see Chapter 1). Then the desired integral is given by the reciprocal
of the integration constant
p (µ1, µ2, µ3|y) ∝

n5σ2 + ντ 2 +

i
ni (µi −yi)2
−n+ν
2
.
Eliminating terms that do not depend on the means, and rearranging the
exponent, yields,
p (µ1, µ2, µ3|y) ∝

1 +

i
ni (µi −yi)2
n5σ2 + ντ 2


−n−3+ν+3
2
.
(5.127)
Now write

i
ni (µi −yi)2
= [(µ1 −y1) , (µ2 −y2) , (µ3 −y3)] N


µ1 −y1
µ2 −y2
µ3 −y3


= (µ −y)′ N (µ −y)

270
5. An Introduction to Bayesian Inference
where
N =


n1
0
0
0
n2
0
0
0
n3

,
and let:
c2 = n5σ2 + ντ 2
n −3 + ν .
Then (5.127) can be put as
p (µ1, µ2, µ3|y) ∝

1 + (µ −y)′ N (µ −y)
(n −3 + ν) c2
−n−3+ν+3
2
.
(5.128)
This is the density of a trivariate-t distribution with mean vector y, degrees
of freedom (n −3 + ν), and variance–covariance matrix
V ar (µ1, µ2, µ3|y) = c2N−1 (n −3 + ν)
(n −5 + ν).
(5.129)
This distribution is unimodal and symmetric, and the mode is given by y.
Hence, the mode obtained after marginalizing with respect to σ2 is iden-
tical to the µ−component of the mode of the joint posterior distribution

µ1, µ2, µ3, σ2|y

. Further, since the process is multivariate-t, it follows
that all the marginal distributions are univariate t with mean vector yi,
(n −3 + ν) degrees of freedom and variance:
V ar (µi|y) = c2 (n −3 + ν)
ni (n −5 + ν).
It is important, however, to note that the true means µi are not mutually
independent, even though they are not correlated (diagonal N) a posteriori;
this is because the joint density (multivariate-t) cannot be written as a
product of the resulting marginal densities, which are all univariate-t. It
also follows that the distribution of any linear contrast µi−µi′ is univariate-
t, with mean yi −yi′, degrees of freedom equal to (n −3 + ν) and posterior
variance
V ar (µi −µi′|y) = c2 (n −3 + ν)
(n −5 + ν)

 1
ni
+ 1
ni′

.
Another consequence is that the joint distribution of, for example, pairs of
diﬀerences between population means is also bivariate-t, with parameters
derived directly from the joint distribution having density (5.128).
■
Example 5.12
Digression on multiple comparisons
Consider now the problem of carrying out multiple comparisons between
means (Milliken and Johnson, 1992) in the setting of Example 5.11. From
a frequentist point of view, the chance of ﬁnding a diﬀerence that appears

5.6 Features of Posterior Distributions
271
to be “signiﬁcant” increases with the number of comparisons made, even
if such diﬀerences are truly null. For example, these authors note that if
an experiment involves 100 “independent” tests at a signiﬁcance level of
α = 0.05, one should expect (over repeated sampling) (0.05) 100 = 5 such
tests to be signiﬁcant, just by chance. In order to adjust for the number
of comparisons made, diﬀerent error rates must be computed. For exam-
ple, the “experimentwise error” rate is the probability of making at least
one error in an experiment in which there are no real diﬀerences between
means. A battery of statistical tests has been developed for multiple com-
parisons, where the experimentwise error rate is ﬁxed at some level, e.g.,
5%, although it is far from clear when one test ought to be preferred over
another. Further, some of these tests have not been extended to accommo-
date unequal sample sizes or the presence of covariates in the model. From
a Bayesian point of view, all this is a nonissue. First, “hypothesis testing”
is approached in a completely diﬀerent manner in Bayesian statistics, as
it will be seen later. Second, the joint posterior distribution keeps track
of any existing dependencies between parameters. Third, the probability
calculus applied to the joint posterior distribution adjusts probability vol-
umes automatically, that is, uncertainty about whatever is regarded as a
nuisance in a multiple comparison is accounted for via integration. Joint
inferences are then done from a distribution of appropriate dimension, and
probability statements are either marginal or joint, depending on the objec-
tive of the analysis. Suppose that one is interested in inferring all possible
diﬀerences between means, and that if one of such diﬀerences does not
exceed a certain threshold t1−α, e.g., the 1 −α quantile of the posterior
distribution of the diﬀerence, then the “hypothesis” that the diﬀerence is
null is accepted. With the setting as in the preceding example, all possible
diﬀerences between pairs of means can be written in matrix notation as,


µ12
µ13
µ23

=


1
−1
0
1
0
−1
0
1
−1




µ1
µ2
µ3

.
The third comparison is redundant because it can be obtained from the
diﬀerence between the second and third comparisons. Hence, it suﬃces to
work with the full-rank subset of comparisons
←→
µ =
 µ12
µ13

=
 1
−1
0
1
0
−1
 

µ1
µ2
µ3

= Lµ.
Since the joint posterior distribution of µ1, µ2, and µ3 is trivariate-t, it
follows that the posterior distribution of ←→
µ is bivariate-t (by virtue of
being a linear combination), and that the marginal distributions of µ12 and
µ13 are univariate-t. All these distributions have parameters that can be
deduced from results in the preceding example. For instance, the posterior

272
5. An Introduction to Bayesian Inference
covariance between µ12 and µ13 is
Cov (µ12, µ13|y)
= V ar (µ1|y) −Cov (µ1, µ3|y) −Cov (µ1, µ2|y) + Cov (µ2, µ3|y)
= V ar (µ1|y)
since all posterior covariances between means are null, as N is a diagonal
matrix. The posterior probability that µ12 > t1−α is
p12 =
∞

t1−α
p (µ12|y) dµ12
=
∞

t1−α





1 +
[µ12 −(y1 −y2)]2 
1
n1 +
1
n2
−1
(n −3 + ν) c2





−n−3+ν+1
2
dµ12
∞

−∞





1 +
[µ12 −(y1 −y2)]2 
1
n1 +
1
n2
−1
(n −3 + ν) c2





−n−3+ν+1
2
dµ12
which can be computed readily from tabled values of the standardized
t distribution or from some available computer routine. Using a similar
procedure one can calculate p13, the posterior probability that µ13 > t1−α.
Next, we consider the joint distribution of µ12 and µ13. This is a bivariate-t
process with density
p (µ12, µ13|y) ∝

1 +
←→
µ −Ly
′ LNL′ ←→
µ −Ly

(n −3 + ν) c2
−n−3+ν+2
2
.
The posterior probability that both µ12 and µ13 exceed the threshold t1−α
is given by
p12,13 =
∞

t1−α
∞

t1−α

1 + (←
→
µ −Ly)
′LNL′(←
→
µ −Ly)
(n−3+ν)c2
−n−3+ν+2
2
dµ12 dµ13
∞

−∞
∞

−∞

1 + (←
→
µ −Ly)
′LNL′(←
→
µ −Ly)
(n−3+ν)c2
−n−3+ν+2
2
dµ12 dµ13
.
Then the probability that at least one of the comparisons will exceed the
threshold, posterior to the data, is
Pr (µ12 > t1−α ∪µ13 > t1−α) = p12 + p13 −p12,13
(5.130)
with p12 calculated as before, p13 computed with a similar expression, and
p12,13 from the expression preceding (5.130).

5.6 Features of Posterior Distributions
273
There is an important diﬀerence with the frequentist approach. In the
Bayesian analysis, the calculations involve posterior probabilities of events.
In the classical methodology of multiple comparisons, the probabilities in-
volve sampling distributions of some estimates, with calculations conducted
as if the null hypothesis were true. In the Bayesian approach, there is no
such thing as a “null or alternative” hypothesis, except in some metaphoric
sense. The calculations indicated above would be carried out for two models
representing diﬀerent states of nature. Subsequently, the posterior odds ra-
tio, as in Example 5.2, would quantify the strength of the evidence in favor
of one of the two models. It is seen that the Bayesian probability calculus
enables posing and solving the problem in a conceptually straightforward
manner.
■
Example 5.13
Joint modes in a Gaussian linear model
Let the linear model be
y = Xβ + Zu + e,
(5.131)
where β, u, and e follow the independent distributions:
β|β0, σ2
β
∼
N

1β0, Ibσ2
β

,
u|u0, σ2
u
∼
N

1u0, Itσ2
u

,
e
∼
N

0, Inσ2
e

,
with the identity matrices having orders b, t, and n as indicated. The prior
distribution of β postulates that all elements of this vector are i.i.d., with
a common mean and variance. A similar assumption is made about the
elements of u. Above, β0 and u0 are unknown scalar parameters and σ2
β, σ2
u,
and σ2
e are unknown variance components. We shall adopt the independent
prior densities
p (β0)
=
1
β0,max −β0,min
,
p (u0)
=
1
u0,max −u0,min
,
where (min, max) refer to (upper, lower) boundaries for the appropriate
parameters. The variance components are assigned independent scaled in-
verted chi-square distributions, with densities
p

σ2
β|νβ, s2
β

∝

σ2
β
−
 νβ +2
2

exp

−
νβs2
β
2σ2
β

,
p

σ2
u|νu, s2
u

∝

σ2
u
−(
νu+2
2
) exp

−νus2
u
2σ2u

,
p

σ2
e|νe, s2
e

∝

σ2
e
−(
νe+2
2 ) exp

−νes2
e
2σ2e

,

274
5. An Introduction to Bayesian Inference
where νβ, νu, and νe are known “degree of belief” parameters, and s2
β, s2
u,
and s2
e are some prior values of the dispersion components. An alternative
representation of model (5.131) is
y = Xβ + Zu + e
= (X1) β0 + Xξβ + (Z1) u0 + Zξu + e
= mβ0 + nu0 + Xξβ + Zξu + e,
(5.132)
where m = X1 and n = Z1 are incidence vectors of appropriate order and
the new variables have distributions
ξβ|σ2
β
∼
N

0, Ibσ2
β

,
ξu|σ2
u
∼
N

0, Itσ2
u

.
Hence, the entire parameter vector is then
θ =

β0, u0, ξ′
β, ξ′
u, σ2
β, σ2
u, σ2
e
′ .
Suppose we wish to ﬁnd the joint mode of the posterior distribution of θ,
having density
p

θ|β0,max, β0,min, u0,max, u0,min, νβ, νu, νe, s2
β, s2
u, s2
e, y

∝p

y|β0, u0, ξβ, ξu, σ2
e

p

β0|β0,max, β0,min

p (u0|u0,max, u0,min)
×p

ξβ|σ2
β

p

ξu|σ2
u

×p

σ2
β|νβ, s2
β

p

σ2
u|νu, s2
u

p

σ2
e|νe, s2
e

.
(5.133)
In order to ﬁnd a stationary point, the following ﬁrst derivatives are needed,
letting L (θ|y) be the logarithm of the joint posterior density.
Gradient for β0:
∂L (θ|y)
∂β0
=
∂
∂β0

log

p

y|β0, u0, ξβ, ξu, σ2
e

+ ∂
∂β0

log

p

β0|β0,max, β0,min

= −1
2σ2e
∂
∂β0
(e′e)
= m′ 
y −mβ0 −nu0 −Xξβ −Zξu

σ2e
where
e = y −mβ0 −nu0 −Xξβ −Zξu.

5.6 Features of Posterior Distributions
275
Gradient for u0:
∂L (θ|y)
∂u0
=
∂
∂u0

log

p

y|β0, u0, ξβ, ξu, σ2
e

+ ∂
∂u0
{log [p (u0|u0,max, u0,min)]}
= −1
2σ2e
∂
∂u0
(e′e)
= n′ 
y −mβ0 −nu0 −Xξβ −Zξu

σ2e
Gradient for ξβ:
∂L (θ|y)
∂ξβ
=
∂
∂ξβ

log

p

y|β0, u0, ξβ, ξu, σ2
e

+
∂
∂ξβ

log

p

ξβ|σ2
β

= −1
2σ2e
∂
∂ξβ
(e′e) −
∂
∂ξβ

ξ′
βξβ
2σ2
β

= X′ 
y −mβ0 −nu0 −Xξβ −Zξu

σ2e
−ξβ
σ2
β
.
Gradient for ξu:
∂L (θ|y)
∂ξu
=
∂
∂ξu

log

p

y|β0, u0, ξβ, ξu, σ2
e

+ ∂
∂ξu

log p

ξu|σ2
u

=
−1
2σ2e
∂
∂ξu
(e′e) −
∂
∂ξu

ξ′
uξu
2σ2u

=
Z′ 
y −mβ0 −nu0 −Xξβ −Zξu

σ2e
−ξu
σ2u
.

276
5. An Introduction to Bayesian Inference
Gradient for σ2
e:
∂L (θ|y)
∂σ2e
=
∂
∂σ2e

log

p

y|β0, u0, ξβ, ξu, σ2
e

+ ∂
∂σ2e

log

p

σ2
e|νe, s2
e

= −n + νe + 2
2σ2e
+ e′e + νes2
e
2σ4e
.
Gradient for σ2
β:
∂L (θ|y)
∂σ2
β
=
∂
∂σ2
β

log

p

ξβ|σ2
β

+ log

p

σ2
β|νβ, s2
β

= −b + νβ + 2
2σ2
β
+
ξ′
βξβ + νβs2
β
2σ4
β
.
Gradient for σ2
u:
∂L (θ|y)
∂σ2u
=
∂
∂σ2u

log

p

ξu|σ2
u

+ log

p

σ2
u|νu, s2
u

= −t + νu + 2
2σ2u
+ ξ′
uξu + νus2
u
2σ4u
.
Setting all ﬁrst derivatives simultaneously to 0 gives a system of equa-
tions that is not explicit in the solutions. A rearrangement of the system
gives, after algebra, the following functional iteration (the superscript i in
parentheses indicates round number):


m′m
m′n
m′X
m′Z
n′m
n′n
n′X
n′Z
X′m
X′n
X′X + I

σ2
e
σ2
β
(i)
X′X
Z′m
Z′n
Z′X
Z′Z+

σ2
e
σ2
u
(i)




β0
u0
ξβ
ξu


(i)
=


m′y
n′y
X′y
Z′y


(5.134)
σ2(i+1)
e
= (e′e)(i) + νes2
e
n + νe + 2
,
σ2(i+1)
β
=

ξ′
βξβ
(i) + νβs2
β
b + νβ + 2
,
and:
σ2(i+1)
u
=

ξ′
uξu
(i) + νus2
u
t + νu + 2
.

5.6 Features of Posterior Distributions
277
The functional iteration starts by specifying starting values for σ2
e, σ2
β, and
σ2
u, and then solving (5.134), to obtain values for β0, u0, ξβ, and ξu. The
variance components are then updated with the three expressions below
(5.134). If the iteration converges, it will produce a mode of the joint pos-
terior distribution of all parameters. The iteration must be tailored such
that values of β0, u0 stay within the boundaries stated in the probability
model.
■
Example 5.14
Marginal modes in a Gaussian linear model
The setting is as in Example 5.13, but consider now ﬁnding the modal
vector of the lower-dimensional distribution

β0, u0, ξβ, ξu|β0,max, β0,min, u0,max, u0,min, νβ, νu, νe, s2
β, s2
u, s2
e, y

.
(5.135)
The integral of the joint posterior density (5.133) with respect to all three
variance components is needed to obtain the density of the desired lower-
dimensional distribution. Omitting the dependency on hyperparameters in
the notation, this integral is
p

β0, u0, ξβ, ξu|y

∝p (β0) p (u0)
∞

0
p

y|β0, u0, ξβ, ξu, σ2
e

p

σ2
e

dσ2
e
×
∞

0
p

ξβ|σ2
β

p

σ2
β

dσ2
β
∞

0
p

ξu|σ2
u

p

σ2
u

dσ2
u.
(5.136)
Each of the integrals is in a scaled inverted chi-squared form, and can
be evaluated explicitly. We now evaluate each of the integrals in (5.136).
Retaining only terms that vary with β0, u0, ξβ, and ξu, we ﬁrst get
∞

0
p

y|β0, u0, ξβ, ξu, σ2
e

p

σ2
e

dσ2
e
∝
∞

0

σ2
e
−(
n+νe+2
2
) exp

−e′e + νes2
e
2σ2e

dσ2
e
∝

e′e + νes2
e
−n+νe
2
∝

1 + e′e
νes2e
−n+νe
2
,
(5.137)
recalling that
e =

y −mβ0 −nu0 −Xξβ −Zξu

.

278
5. An Introduction to Bayesian Inference
Similarly,
∞

0
p

ξβ|σ2
β

p

σ2
β

dσ2
β
∝
∞

0

σ2
β
−
 b+νβ +2
2

exp

−
ξ′
βξβ + νβs2
β
2σ2
β

dσ2
β
∝

1 + ξ′
βξβ
νβs2
β
−
b+νβ
2
.
(5.138)
Further, using a similar algebra,
∞

0
p

ξu|σ2
u

p

σ2
u

dσ2
u ∝

1 + ξ′
uξu
νus2u
−t+νu
2
.
(5.139)
Each of the expressions in (5.137)-(5.139) is the kernel of a multivariate-t
distribution. Now, collecting the integrals and using these in (5.136) we
obtain, as posterior density, after suitable normalization,
p

β0, u0, ξβ, ξu|y

∝
37
i=1

1 + λ′
iλi
ci
−di
2
37
i=1
∞

−∞

1 + λ′
iλi
ci
−di
2
dλi
,
(5.140)
where
λ1
=
y −mβ0 −nu0 −Xξβ −Zξu,
c1 = νes2
e,
d1 = n + νe,
λ2
=
ξβ,
c2 = νβs2
β,
d2 = b + νβ,
λ3
=
ξu,
c3 = νus2
u,
d3 = t + νu.
If the λ′
is were the random variables of interest, the distribution with den-
sity (5.140) would be a truncated (in the intervals β0,max −β0,min and
u0,max−u0,min) poly-t or product multivariate-t distribution (Box and Tiao,
1973); its properties are presented in Dickey (1968). For example, this dis-
tribution is known to be asymmetric and multimodal. However, note that
λ1 involves all four random terms of interest, so the process of interest
is not poly-t. Consider now ﬁnding the mode of the marginal distribution
of concern. Diﬀerentiation of the logarithm of (5.140), L

β0, u0, ξβ, ξu|y

,
with respect to each of the four terms, yields,
∂L

β0, u0, ξβ, ξu|y

∂β0
=
d1m′e

c1 + λ′
1λ1
,

5.6 Features of Posterior Distributions
279
∂L

β0, u0, ξβ, ξu|y

∂u0
=
d1n′e

c1 + λ′
1λ1
,
∂L

β0, u0, ξβ, ξu|y

∂ξβ
=
d1X′e

c1 + λ′
1λ1
 −
d2ξβ

c2 + λ′
2λ2
,
and
∂L

β0, u0, ξβ, ξu|y

∂ξu
=
d1Z′e

c1 + λ′
1λ1
 −
d3ξu

c3 + λ′
3λ3
.
Setting all diﬀerentials simultaneously to zero gives
m′mβ0 + m′nu0 + m′Xξβ + m′Zξu = m′y,
n′mβ0 + n′nu0 + n′Xξβ + n′Zξu = n′y,
X′mβ0 + X′nu0 +

X′X + Iw1
w2

ξβ + X′Zξu = X′y,
Z′mβ0 + Z′nu0 + Z′Xξβ +

Z′Z + Iw1
w3

ξu = Z′y,
where
w1 = c1 + λ′
1λ1
d1
= e′e + νes2
e
n + νe
,
w2 = c2 + λ′
2λ2
d2
=
ξ′
βξβ + νβs2
β
b + νβ
,
and
w3 = c3 + λ′
3λ3
d3
= ξ′
uξu + νus2
u
t + νu
,
can be construed as “estimates” of variance components in a Gaussian
linear mixed eﬀects model (Henderson, 1973; Searle et al., 1992). Clearly,
the equations that need to be solved simultaneously are not explicit in the
solutions. As in Example 5.13, a functional iteration can be constructed,
which can be expressed in matrix form as


m′m
m′n
m′X
m′Z
n′m
n′n
n′X
n′Z
X′m
X′n
X′X + I w(i)
1
w(i)
2
X′X
Z′m
Z′n
Z′X
Z′Z + I w(i)
1
w(i)
3




β0
u0
ξβ
ξu


(i+1)
=


m′y
n′y
X′y
Z′y

,
(5.141)
where the w′s depend on the unknowns, and change values from iterate to
iterate. The algorithm starts by assigning starting values to the pseudo-
variances (perhaps the means of the prior distributions of the variance
components), and then calculating the ﬁrst set of w′s. These are used to
obtain revised values for the location eﬀects, and so on. The properties

280
5. An Introduction to Bayesian Inference
of the algorithm are unknown. If it converges, it will locate one of the
possibly many stationary points of the poly−t distributions. Note that
(5.141) are structurally similar to (5.134). However, the algorithm does
not involve estimating equations for the variance components, as these
have been integrated out of the joint posterior distribution. While it is
extremely diﬃcult to show that the modal vector of the joint distribution of
all parameters diﬀers from the mode of the lower-dimensional distribution
with density (5.140), this example illustrates at least that the modes have
diﬀerent forms in the two cases.
■
5.6.4
Posterior Mean Vector and Covariance Matrix
Mean Vector
The mean (mean vector) and variance (covariance matrix) of posterior
distributions have been identiﬁed in several of the highly stylized examples
discussed before. It is convenient, however, to recall the pervasive presence
of unknown nuisance parameters, so the following notation is helpful in this
respect. The mean of the posterior distribution of a vector θ1, when the
statistical model posits the presence of a nuisance parameter θ2, can be
expressed as
E (θ1|y) = Eθ2|y [E (θ1|θ2, y)] .
(5.142)
The inner expectation gives the posterior mean value of the parameter of in-
terest at speciﬁed values of the nuisance parameters, as if these were known.
The outer expectation averages over the marginal posterior distribution of
the nuisances, thus incorporating whatever uncertainty exists about their
values. Representation (5.142) is also useful in connection with sampling
methods. As seen in Chapter 1, the process of Rao–Blackwellization en-
ables one to obtain a more precise Monte Carlo estimator of E (θ1|y) by
drawing m samples from the posterior distribution of the nuisance param-
eter (whenever this is feasible), and then estimating the desired posterior
mean as
5E (θ1|y) = 1
m
m

i=1
E

θ1|θ(i)
2 , y

,
where θ(1)
2 , θ(2)
2 , . . . , θ(m)
2
are draws from [θ2|y] . The preceding estimator
is at least as precise as
55E (θ1|y) = 1
m
m

i=1
θ(i)
1 ,
where θ1 is a direct draw from the posterior distribution [θ1|y] .

5.6 Features of Posterior Distributions
281
Optimality of the Mean Vector
Use of the mean as an “estimator” of the parameter has a decision-theoretic
justiﬁcation. Suppose the loss function is quadratic, with the form
L

5θ1, θ1

=

5θ1 −θ1
′
Q

5θ1 −θ1

,
where Q is a known symmetric, positive-deﬁnite matrix. The “error” of
estimation is 5θ1 −θ1, and the form adopted above for L

5θ1, θ1

indi-
cates that the penalty accruing from estimating the parameter with error
is proportional to the square of such error. Using well-known formulas for
expected values of quadratic forms, the expected posterior loss is
Eθ1|y

L

5θ1, θ1

=

5θ1 −E (θ1|y)
′
Q

5θ1 −E (θ1|y)

+tr [Q V ar (θ1|y)] .
(5.143)
The second term does not involve 5θ1, and the ﬁrst term is nonnegative.
Hence, the expected loss is minimized by taking as “optimal estimator”:
5θ1 = E (θ1|y)
(5.144)
which is the posterior mean.
Relationship Between the Posterior Mean and the “Best” Predictor
Consider adopting a frequentist point of view, that is, let [θ1, y|θ2] be a
joint distribution having a long-run frequency interpretation, where θ2 is
a known parameter (this is a very strong assumption; in practice, such a
parameter is unknown, most often). In this setting, θ1 is an unobservable
random variable, or “random eﬀect” in the usual frequentist sense. Then,
using a logic similar to the preceding one, we will show that the frequentist
expected loss
Eθ1,y|θ2

L

5θ1, θ1

= Eθ1,y|θ2

5θ1 −θ1
′
Q

5θ1 −θ1

(5.145)
is minimized by 5θ1 = E (θ1|y, θ2), among all possible predictors. In (5.145),
expectations are taken with respect to the joint distribution [θ1, y|θ2] , with
the nuisance parameter θ2 treated as a known constant. Using the theorem

282
5. An Introduction to Bayesian Inference
of double expectation
Eθ1,y|θ2

L

5θ1, θ1

= Ey|θ2
9
Eθ1|y,θ2

L

5θ1, θ1
:
= Ey|θ2

5θ1 −E (θ1|y, θ2)
′
Q

5θ1 −E (θ1|y, θ2)

+ [Q V ar (θ1|y, θ2)]}
= Ey|θ2

5θ1 −E (θ1|y, θ2)
′
Q

5θ1 −E (θ1|y, θ2)

+Ey|θ2tr [Q V ar (θ1|y, θ2)] .
(5.146)
The second term in (5.146) does not involve 5θ1; thus, minimizing expected
(frequentist) quadratic loss is achieved by minimizing the ﬁrst term. Now,
the latter is a weighted average, where the weight function is the density
of the distribution [y|θ2] . It turns out that, if one minimizes,

5θ1 −E (θ1|y, θ2)
′
Q

5θ1 −E (θ1|y, θ2)

(5.147)
for each realization of y, this will also minimize
Ey|θ2

5θ1 −E (θ1|y, θ2)
′
Q

5θ1 −E (θ1|y, θ2)
%
and, hence, (5.146). Using the same argument as in the preceding sec-
tion, it follows that the minimizer for each y is the conditional expectation
E (θ1|y, θ2) which, superﬁcially, “looks similar” to our Bayesian E (θ1|y) .
The similarity does not stand scrutiny, however, as E (θ1|y) incorporates
the uncertainty about the nuisance parameter θ2, this is clearly not the
case for 5θ1 = E (θ1|y, θ2) .
The “best predictor” is, thus, the conditional mean of the unobservable
random eﬀects given the observations, but assuming known parameters of
the joint distribution of the data and of the random eﬀects (Henderson,
1973). This statistic was introduced in Example 1.22 of Chapter 1. In ad-
dition to minimizing the expected squared error of prediction (5.145), the
conditional mean has some frequentist properties of interest. For example,
it is “unbiased”, in some sense. This can be veriﬁed by taking the expec-
tations of the conditional mean over the distribution of the observations
Ey|θ2

5θ1

= Ey|θ2 [E (θ1|y, θ2)] = E (θ1) ,
(5.148)
recalling that the parameter θ2 is not a random variable in the frequentist
sense, so the dependence on it can be omitted in the notation. This is the
deﬁnition of unbiasedness in the frequentist context of prediction of random
eﬀects. Now, since the conditional mean is an unbiased predictor, it follows
that it minimizes prediction error variance as well. This can be seen simply

5.6 Features of Posterior Distributions
283
by putting Q = I in (5.145), and by considering a scalar element of θ1;
then, the expected loss is the prediction error variance. A third property is
Cov

5θ1, θ′
1

= V ar

5θ1

.
(5.149)
This results from the fact that
Cov

5θ1, θ′
1

= Ey,θ1|θ2

5θ1θ′
1

−Ey,θ1|θ2

5θ1

E

θ′
1

= Ey|θ2

Eθ1|y,θ2

5θ1θ′
1

−E

5θ1

E

5θ
′
1

= Ey|θ2

5θ1Eθ1|y,θ2

θ′
1

−E

5θ1

E

5θ
′
1

= E

5θ15θ
′
1

−E

5θ1

E

5θ
′
1

= V ar

5θ1

.
An immediate consequence is that the prediction error 5θ1 −θ1 and the
predictor 5θ1 have a null covariance. Additional consequences of (5.149), for
any scalar element of θ1, say θ1i, are that
Corr

5θ1i, θ1i

=
@
A
A
BV ar

5θ1i

V ar (θ1i) ,
and
V ar

5θ1i −θ1i

= V ar (θ1i)

1 −Corr2 
5θ1i, θ1i

.
In general,
V ar

5θ1 −θ1

= V ar (θ1) −V ar

5θ1

.
(5.150)
Example 5.15
Best prediction of quadratic genetic merit
Suppose we are in the setting of Example 5.3, and that the function
T = k0a + k1a2
is to be predicted. As before, a is the additive genetic value of a certain
farm animal for some trait, and k0 and k1 are known constants. This func-
tion is what is called “quadratic merit” in animal breeding (Wilton et al.,
1968), and it postulates that the breeding worth of a potential parent is
proportional to the square of its additive genetic value. The expectation of
T is
E (T)
=
k0E (a) + k1E2 (a) + k1va
=
k1va
since it is assumed that E (a) = 0. As in Example 5.3 suppose that µ, va,
and ve are known. The frequentist conditional distribution of a, given the

284
5. An Introduction to Bayesian Inference
phenotypic value y, coincides in this case with the Bayesian conditional
posterior distribution of a. As shown in Example 5.3, this distribution is
a|y, µ, va, ve ∼N

h2 (y −µ) , va

1 −h2
,
where h2 = va/ (va + ve). From results in the preceding section, the best
predictor of T is
5T
=
E (T|y, µ, va, ve) = k0E (a|y, µ, va, ve) + k1E

a2|y, µ, va, ve

=
k0h2 (y −µ) + k1

h2 (y −µ)
2 + k1va

1 −h2
,
and this can be evaluated readily. It is easy to verify that the predictor is
unbiased, as it has the same expectation as the predictand. Taking expec-
tations over the distribution of y:
EyE (T|y, µ, va, ve) = k0E (a) + k1E

E

a2|y, µ, va, ve

= k0E (a) + k1E

a2
= k1va.
Using (5.150), the prediction error variance is
V ar

5T −T

= V ar (T) −V ar

5T

.
In the preceding expression
V ar (T) = V ar

k0a + k1a2
= k2
0va + k2
1 V ar(a2) + 2k0k1 Cov(a, a2)
= k2
0va + 2k2
1v2
a
with this being so, because if a ∼N (0, va) , then V ar

a2
= 2v2
a and
Cov(a, a2) = 0 (Searle, 1971). Further, letting
5a = E (a|y, µ, va, ve) ,
and using similar arguments, it follows that
V ar

5T

= V ar
9
k0h2 (y −µ) + k1

h2 (y −µ)
2 + k1va

1 −h2:
= V ar

k05a + k15a2
= k2
0 V ar (5a) + 2k2
1 V ar2 (5a)
with
V ar (5a) = h4 (va + ve) .
Collecting terms,
V ar

5T −T

= k2
0 [va −V ar (5a)] + 2k2
1

v2
a −V ar2 (5a)

= [va −V ar (5a)]

k2
0 + 2k2
1 [va + V ar (5a)]

.
■

5.6 Features of Posterior Distributions
285
Dispersion Matrix of the Posterior Distribution
The posterior variance–covariance matrix between parameters θ1 and θ2 is
Cov (θ1, θ2|y) =
  
θ1 −θ1
 
θ2 −θ2
′ p (θ1, θ2|y) dθ1dθ2
=
 
θ1θ′
2p (θ1, θ2|y) dθ1dθ2 −θ1θ2,
(5.151)
where
θi = E (θi|y) ,
i = 1, 2.
The posterior correlation between any pair of parameters can be deduced
readily from (5.151). There are situations in which a high degree of pos-
terior intercorrelation between parameters can be modiﬁed drastically by
some suitable reparameterization. This can have an impact in the numerical
behavior of Markov chain Monte Carlo (MCMC) algorithms for sampling
from posterior distributions. We shall return to this in the MCMC part of
this book.
If the probability model includes a third vector of nuisance parameters,
say θ3, a sometimes useful representation of the variance–covariance ma-
trix, of the joint posterior distribution of θ1 and θ2 is
Cov (θ1, θ2|y) = Eθ3|y [Cov (θ1, θ2|θ3, y)]
+Covθ3|y [E (θ1|θ3, y) , E (θ2|θ3, y)] .
(5.152)
The basic principles of Bayesian analysis have been covered at this point
using relatively simple settings. In the following chapter, a detailed discus-
sion of the linear regression model and of the mixed linear model will be
presented from a Bayesian perspective.

This page intentionally left blank

6
Bayesian Analysis of Linear Models
6.1
Introduction
A review of the basic tenets of Bayesian inference was given in the pre-
ceding chapter. Here the treatment is extended by presenting the Bayesian
analysis of some standard linear models used in quantitative genetics, and
appropriate analytical solutions are derived whenever these exist. First, the
standard linear regression model is discussed from a Bayesian perspective.
Subsequently, the Bayesian mixed eﬀects model under Gaussian assump-
tions is contrasted with its frequentist counterpart. The chapter ﬁnishes
with a presentation of several marginal and conditional distributions of in-
terest. The developments presented give a necessary background for a fully
Bayesian analysis of linear models via Markov chain Monte Carlo methods,
a topic to be discussed subsequently in this book.
6.2
The Linear Regression Model
Arguably, linear regression analysis is one of the most widely used statistical
methods and its use in genetics probably dates back to Galton (1885).
For example, one of the simplest methods for estimating heritability is
based on regressing the mean value for a quantitative trait measured on
several oﬀspring from a pair of parents on the midparental mean (Falconer
and Mackay, 1996). While regression models have been discussed earlier in
this book, a more general tour of the Bayesian analysis of such models is

288
6. Bayesian Analysis of Linear Models
presented in this section. Most of the needed notation has been presented
before, so we concentrate on essentials only.
A Gaussian linear regression model, making a distinction between two
distinct sets of coeﬃcients β1, (p1 × 1) and β2, (p2 × 1) , is
y = X1β1 + X2β2 + e
=

X1
X2
 
β1
β2

+ e = Xβ + e,
(6.1)
where the error term is e ∼N

0, Iσ2
and σ2 is an unknown dispersion
parameter. The likelihood function is then
L

β1, β2, σ2|y

∝

σ2−n
2 exp

−(y −Xβ)′ (y −Xβ)
2σ2

.
(6.2)
It has been seen before that
(y −X1β1 −X2β2)′ (y −X1β1 −X2β2) = Se + Sβ,
where
Se
=

y −X15β1 −X25β2
′ 
y −X15β1 −X25β2

,
Sβ
=
 
β1 −5β1
′

β2 −5β2
′ 
C

β1 −5β1
β2 −5β2

,

5β1
5β2

= C−1

X′
1y
X′
2y

,
and
C =

X′
1X1
X′
1X2
X′
2X1
X′
2X2

.
This decomposition leads rather directly to the joint, marginal and con-
ditional posterior distributions of interest. Results from Bayesian analyses
under two diﬀerent prior speciﬁcations will be presented separately.
6.2.1
Inference under Uniform Improper Priors
Joint Posterior Density
Suppose a uniform distribution is adopted as joint prior for all elements of
the parameter vector θ =

β′
1, β′
2, σ2′. Unless such a uniform prior does
not have ﬁnite boundaries, it is improper. At ﬁrst sight improper priors
do not seem to make sense in the light of probability theory, at least as
presented so far. However, improper priors have played a role in an area
usually referred to as “objective Bayesian analysis”, an approach founded

6.2 The Linear Regression Model
289
by Jeﬀreys (1961). For some parameters and models (Box and Tiao, 1973;
Bernardo and Smith, 1994), an improper uniform distribution can be shown
to introduce as little information as possible, in some sense, beyond that
contained in the data.
A speciﬁcation based on improper uniform priors will be developed in
this section. Here, β1 and β2 are allowed to take any values in the p1- and
p2-dimensional spaces ℜp1 and ℜp2 respectively, while σ2 falls between 0
and ∞. Hence, the joint posterior density is strictly proportional to the
likelihood function. After normalization (assuming the integrals exist), the
joint density can be written as
p

β1, β2, σ2|y

=

σ2−n
2 exp

−Se+Sβ
2σ2


ℜp1

ℜp2
∞

0
(σ2)−n
2 exp

−Se+Sβ
2σ2

dβ1dβ2 dσ2
.
(6.3)
The limits of integration are omitted in the notation hereinafter.
Conditional Posterior Distributions of the Regression Coeﬃcients
These can be found directly from the joint posterior density by retaining
the part that varies with β1 and β2. One gets
p

β1, β2|σ2, y

∝exp

−Sβ
2σ2

.
It follows that the joint posterior distribution of the regression coeﬃcients,
given σ2, is

β1
β2
"""" σ2, y ∝N

5β1
5β2

,

X′
1X1
X′
1X2
X′
2X1
X′
2X2
−1
σ2

.
(6.4)
Distributions of individual elements of β are, thus, normal, with mean given
directly by the corresponding component of the mean vector, and with
variance equal to the appropriate element of the inverse matrix above,
times σ2. Posterior distributions of linear combinations of the regression
coeﬃcients are normal as well, given σ2.
If, in addition to σ2, one treats either β1 or β2 as known, the resulting
conditional posterior distributions are
β1|β2, σ2, y ∝N

,β1, (X′
1X1)−1 σ2
(6.5)
and
β2|β1, σ2, y ∝N

,β2, (X′
2X2)−1 σ2
,
(6.6)
where
,βi = (X′
iXi)−1 X′
i

y −Xjβj

,
i = 1, 2, i ̸= j.

290
6. Bayesian Analysis of Linear Models
The conditional posterior distribution of an individual regression coeﬃ-
cient, βk, given σ2 and all other regression coeﬃcients

β−k

is also normal,
with mean
,βk = x′
k

y −X−kβ−k

x′
kxk
,
(6.7)
where xk is the kth column of X, and X−k is X without column xk. The
variance is
V ar

βk|β−k, σ2, y

=
σ2
x′
kxk
.
(6.8)
Conditional Posterior Distribution of σ2
If the joint density is viewed now only as a function of σ2, with the regres-
sion parameters treated as constants, one gets
p

σ2|β1, β2, y

∝

σ2−n
2 exp

−Se + Sβ
2σ2

.
(6.9)
This is the kernel of a scaled inverse chi-square process with degree of
belief parameter equal to n −2 (this “loss” of two degrees of freedom is
a curious consequence of the uniform prior adopted), and scale parameter
(Se + Sβ) / (n −2) . It has mean and variance equal to
E

σ2|β1, β2, y

= (Se + Sβ)
n −4
and to
V ar

σ2|β1, β2, y

=
2 (Se + Sβ)2
(n −4)2 (n −6)
.
For the conditional posterior distribution of σ2 to exist, it is necessary that
n > 2. The mean of the distribution exists if n > 4, and the variance of the
process is deﬁned only if n > 6. Thus, given certain constraints on sample
size, the conditional posterior distribution is proper even if the prior is not
so.
Marginal Distributions of the Regression Coeﬃcients
Integration of the joint density over σ2 gives
p (β1, β2|y) ∝
 
σ2−( n−2
2
+1) exp

−Se + Sβ
2σ2

dσ2
∝(Se + Sβ)−( n−2
2 ) ∝

1 +
Sβ
(n −2 −p1 −p2)
Se
(n−2−p1−p2)
−k
,
(6.10)

6.2 The Linear Regression Model
291
where k = (n −2 −p1 −p2 + p1 + p2) /2. This is the kernel of a p1 + p2-
dimensional multivariate-t distribution, with mean 5β = [5β
′
1, 5β
′
2]′, degrees
of freedom n −2 −p1 −p2, and variance–covariance matrix
V ar (β1, β2|y) =
Se
(n −p1 −p2 −4)

X′
1X1
X′
1X2
X′
2X1
X′
2X2
−1
.
The distribution is proper only if n > p1 + p2 + 2.
Since the joint distribution of the regression coeﬃcients is multivariate t,
it follows that any marginal and conditional distributions of the regression
coeﬃcients (after integrating σ2 out) are either univariate or multivariate-t
(see Chapter 1). The same is true for the posterior distribution of any linear
combination of the coeﬃcients.
Marginal Distribution of σ2
Integrating the joint density with respect to the regression coeﬃcients gives
p

σ2|y

∝

σ2−n
2 exp

−Se
2σ2
  
exp

−Sβ
2σ2

dβ1dβ2.
The integrand is the kernel of the density of the multivariate normal dis-
tribution (6.4), so the integral evaluates to
(2π)
p1+p2
2
""C−1σ2""
1
2 .
Using this in the previous expression and retaining only those terms varying
with σ2, one gets
p

σ2|y

∝

σ2−(
n−p1−p2−2
2
+1) exp

−Se
2σ2

.
(6.11)
This indicates that the marginal posterior distribution of the variance is a
scaled inverted chi-square process with degree of belief parameter n −p1 −
p2 −2, mean equal to
E

σ2|y

=
Se
n −p1 −p2 −4,
(6.12)
and variance
V ar

σ2|y

=
2S2
e
(n −p1 −p2 −4)2 (n −p1 −p2 −6)
.
(6.13)
The distribution is proper provided that n > p1 + p2 + 2 = rank (X) + 2.

292
6. Bayesian Analysis of Linear Models
Posterior Distribution of the Residuals
The residuals contain information about the quality of ﬁt of the model
(Draper and Smith, 1981; Bates and Watts, 1988). Hence, an analysis of
the residuals is often indicated as a starting point for exploring the ad-
equacy of the assumptions, for example, of the functional form adopted.
From a Bayesian perspective, this is equivalent to evaluating the poste-
rior distribution of the residuals. If the residual for an observation is not
centered near zero, this can be construed as evidence that the model is
probably inconsistent with such observation (Albert and Chib, 1995). Let
the residual for datum i be
ei = yi −x′
iβ,
where x′
i is now the ith row of X. Since yi is ﬁxed in Bayesian analysis,
then this is a linear combination of the random variable x′
iβ. The latter,
a posteriori, is distributed as univariate-t on n −2 −p1 −p2 degrees of
freedom, with mean x′
i5β, and variance
V ar (x′
iβ|y) =
Sex′
iC−1xi
(n −p1 −p2 −4).
(6.14)
It follows that the posterior distribution of ei is univariate-t, also on n −
2 −p1 −p2 degrees of freedom, with mean:
E (ei|y) = yi −x′
i5β
(6.15)
and variance equal to (6.14). The topic of model ﬁt based on analysis of
residuals is taken up again in Chapter 8.
Predictive Distributions
In Bayesian analysis, a distinction needs to be made between two predictive
distributions that play diﬀerent roles. The ﬁrst one is the prior predictive
distribution, which assigns densities or probabilities to the data points be-
fore these are observed. If a Bayesian model postulates that the data are
generated according to the process [y|θ] , and if θ has a prior density in-
dexed by hyperparameter H, then the prior predictive distribution is given
by the mixture
p (y|H)
=

p (y|θ) p (θ|H) dθ
=
Eθ [p (y|θ)] ,
(6.16)
with the prior acting as mixing distribution. Hence, the prior predictive
distribution results from averaging out the sampling model [y|θ] over all
possible values that the parameter vector can take, with the relative weights

6.2 The Linear Regression Model
293
being the prior density at each value of θ. In other words, the prior predic-
tive distribution gives the total probability of observing the actual data,
unconditionally on parameter values, but given H.
The prior predictive distribution does not exist unless the prior is proper;
otherwise, the integral in (6.16) would not be deﬁned. Hence, there is no
prior predictive distribution for the Bayesian regression model with an un-
bounded ﬂat prior. When it exists, as will be the case with the second
set of priors discussed later on, the predictive distribution provides a basis
for model comparison. We shall return to this later but, for the moment,
consider the following statement:
“One ought to be inclined towards choosing a model over
a competing one if the former predicts (before data collection)
the observations that will occur with a higher probability than
the latter.”
On the other hand, the posterior predictive process is the distribution or
density of future observations, given past data, and unconditionally with
respect to parameter values. Thus, a natural application of this distribu-
tion is in forecasting problems. The distribution will exist whenever the
posterior distribution of the parameters is proper. In the context of the
linear regression model, suppose that one wishes to forecast a vector of
future observations, yf, of order nf × 1, under the assumption that these
will be generated according to model (6.1). Since yf is unobservable, it can
be included as an unknown in Bayes theorem, with this being a particu-
lar case of the technique called “data augmentation” (Tanner and Wong,
1987), discussed in Chapter 11. The joint posterior density of all unknowns
is:
p

yf, β1, β2, σ2|y

= p

yf|β1, β2, σ2, y

p

β1, β2, σ2|y

= p

yf|β1, β2, σ2
p

β1, β2, σ2|y

,
because, given the parameters, the future and current observations are
mutually independent. The distribution

yf|β1, β2, σ2
is as postulated by
the sampling model, that is,
yf|β1, β2, σ2 ∼N

Xfβ, Ifσ2
,
where Xf and If have suitable dimensions. The posterior predictive density
is then
p (yf|y) =
  
p

yf|β1, β2, σ2
p

β1, β2, σ2|y

dβ1dβ2 dσ2.
(6.17)
Thus, this distribution is a mixture of the sampling model for the future
observations using the joint posterior distribution of the parameters (based

294
6. Bayesian Analysis of Linear Models
on past observations) as a mixing process. Now note that the sampling
model for future observations implies that
yf = Xfβ + ef,
where the future errors have the distribution ef ∼N

0, Ifσ2
. The mean
of the posterior predictive distribution is then
E (yf|y) = E [E (yf|β)] = E (Xfβ) = Xf 5β,
(6.18)
where the outer expectation is taken with respect to the posterior distribu-
tion. The variance–covariance matrix of the predictive distribution, using a
similar conditioning and deconditioning (outer expectations and variances
taken over the posterior distribution of the parameters), is:
V ar (yf|y) = V ar

E

yf|β,σ2
+ E

V ar

yf|β,σ2
= Xf V ar (β|y) X′
f + IfE

σ2|y

=

XfC−1X′
f + If

Se
(n −p1 −p2 −4).
(6.19)
In order to specify completely the predictive distribution, return to (6.17)
and rewrite it as
p (yf|y) =
  
p

yf|β1, β2, σ2
×p

β1, β2|σ2, y

p

σ2|y

dβ1dβ2 dσ2.
(6.20)
The ﬁrst two densities under the integral sign are in normal forms. Now the
quadratics in the exponents involve the regression coeﬃcients and can be
put as follows (we shall not distinguish between the two sets of regressions
here)
(yf −Xfβ)′ (yf −Xfβ) +

β −5β
′
C

β −5β

=

yf −Xf 5β
′ 
yf −Xf 5βf

+

β −5βf
′
X′
fXf

β −5βf

+

β −5β
′
C

β −5β

= Sef +

β −5βf
′
X′
fXf

β −5βf

+

β −5β
′
C

β −5β

,
(6.21)
where
5βf =

X′
fXf
−1 X′
fyf
and
Sef =

yf −Xf 5β
′ 
yf −Xf 5β

.

6.2 The Linear Regression Model
295
The ﬁrst term in (6.21) does not involve the regression coeﬃcients. The
second and third terms can be combined, using (5.56) and (5.57), as

β −5βf
′
X′
fXf

β −5βf

+

β −5β
′
C

β −5β

=

β −55βf
′ 
X′
fXf + X′X
 
β −55βf

+

5βf −5β
′
X′
fXf

X′
fXf + X′X
−1 X′X

5βf −5β

.
(6.22)
Here,
55βf
=

X′
fXf + X′X
−1 
X′
fXf 5βf + X′X5β

=

X′
fXf + X′X
−1 
X′
fyf + X′y

.
Let
X′
fXf + X′X = C+.
Using (6.22) in (6.21) and, further, employing the ensuing result in density
(6.20), one gets
p (yf|y) ∝
 
σ2−
 nf +p1+p2
2

exp

−
Sef +(βf −β)
′X′
f Xf C−1
+ X′X(βf −β)
2σ2

×

exp

−

β−βf
′
C+

β−βf

2σ2

dβ


p

σ2|y

dσ2.
The last integral involves a Gaussian kernel and is equal to
(2π)
p1+p2
2
""C−1
+ σ2""
1
2 =

2πσ2 p1+p2
2
""C−1
+
""
1
2 .
Making use of this, the predictive density can be written as
p (yf|y)
∝
 
σ2−
nf
2 exp

−
Sef +

5βf −5β
′
X′
fXfC−1
+ X′X

5βf −5β

2σ2


×p

σ2|y

dσ2.
(6.23)
Writing the marginal density of σ2 explicitly, as shown in (6.11), and letting
Q+ = X′
fXfC−1
+ X′X,

296
6. Bayesian Analysis of Linear Models
the predictive density is expressible as
p (yf|y) ∝
 
σ2−
 nf +n−p1−p2−2
2
+1

× exp

−
Sef +

5βf −5β
′
Q+

5βf −5β

+ Se
2σ2

dσ2.
The integrand is the kernel of a scaled inverse chi-square (or inverted
gamma) density, and this type of integral has been encountered a num-
ber of times before. Upon integrating over σ2, one obtains
p (yf|y) ∝

Sef +

5βf −5β
′
Q+

5βf −5β

+ Se
−
 nf +n−p1−p2−2
2

.
(6.24)
After lengthy matrix manipulations (see, e.g., Zellner, 1971), the posterior
predictive density can be put in the form
p (yf|y) ∝

1 +

yf −Xf 5β
′
Pf

yf −Xf 5β

Se


−
 nf +η
2

,
(6.25)
where η = n −p1 −p2 −2, and
Pf = I −Xf

X′
fXf + X′X
−1 Xf.
This indicates that the distribution is a multivariate-t process of order nf,
having mean vector Xf 5β, η degrees of freedom, and variance–covariance
matrix
Se
n −p1 −p2 −4

I −Xf

X′
fXf + X′X
−1 Xf
−1
.
(6.26)
In order to show that (6.19) and (6.26) are equal, use can be made of the
matrix identity

A + BGB′−1 = A−1 −A−1B

B′A−1B + G−1−1 B′A−1
(6.27)
provided that the inverses involved exist. Then

XfC−1X′
f + If
−1 = If −Xf

X′
fXf + X′X
−1 X′
f,
recalling that C = X′X. This implies that, in (6.26),

I −Xf

X′
fXf + X′X
−1 Xf
−1
= XfC−1X′
f + If,
which is the matrix expression entering into (6.19). There is a much simpler
way of arriving at (6.25), which will be presented in the following section
on inference under conjugate priors.

6.2 The Linear Regression Model
297
6.2.2
Inference under Conjugate Priors
Suppose now that a scaled inverse chi-square prior is adopted for σ2 and
that a multivariate normal prior is assigned to β, as follows (again, let H
be a set of hyperparameters):
p

β, σ2|H

= p

β|σ2, Hβ

p

σ2|Hσ2
.
Here Hσ2 indexes the scaled inverse chi-square process, and Hβ is the set
of hyperparameters of the multivariate normal distribution for β, given σ2.
Write
p

σ2|Hσ2 = ν∗, s∗2
∝

σ2−( ν∗
2 +1) exp

−ν∗s∗2
2σ2

,
and
p

β|σ2, Hβ = mβ, Vβ

∝
""Vβσ2""−1
2 exp

−

β −mβ
′ V−1
β

β −mβ

2σ2

,
where mβ is the prior mean and Vβσ2 is the covariance matrix of the
conditional (given σ2) prior distribution. The joint prior density is then
p

β, σ2|H

∝

σ2−
 ν∗+p1+p2
2
+1

× exp

−

β −mβ
′ V−1
β

β −mβ

+ ν∗s∗2
2σ2

.
(6.28)
Using the likelihood function in (6.2), in conjunction with the prior given
above, yields as joint posterior density of all unknown parameters
p

β, σ2|y, H

∝

σ2−
 n+ν∗+p1+p2
2
+1

× exp

−
Sβ +

β −mβ
′ V−1
β

β −mβ

+ Se + ν∗s∗2
2σ2

.
(6.29)
Now the quadratic forms in β can be combined, using (5.56) and (5.57), as
Sβ +

β −mβ
′ V−1
β

β −mβ

=

β −5β
′
C

β −5β

+

β −mβ
′ V−1
β

β −mβ

=

β −β
′ 
C + V−1
β
−1 
β −β

+ S5
β,
where
β
=

C + V−1
β
−1 
C5β + V−1
β mβ

=

C + V−1
β
−1 
X′y + V−1
β mβ

,
(6.30)

298
6. Bayesian Analysis of Linear Models
and
S5
β =

5β−mβ
′
C

C + V−1
β

V−1
β

5β−mβ

.
Employing this in (6.29), the joint posterior density is expressible as
p

β,σ2|y, H

∝

σ2−
 n+ν∗+p1+p2
2
+1

× exp

−

β −β
′ 
C + V−1
β
 
β −β

+ S5
β + Se + ν∗s∗2
2σ2

.
(6.31)
Note that this is in the same mathematical form as the joint prior (6.28).
This property is called conjugacy, meaning that the process remains in the
same family of distributions, a posteriori, so only the parameters need to
be updated, as the posterior has the same form as the prior. This implies
that the marginal posterior distribution of σ2 must be scaled inverted chi-
square, and that the conditional posterior distribution of β, given σ2, must
be multivariate normal. We now proceed to verify that this is so.
Conditional Posterior Distribution of the Regression Coeﬃcients
This can be found via the usual procedure of examining the joint posterior
density, ﬁxing some parameters, and then letting those whose distribution
is sought, vary. If (6.31) is viewed as a function of β, it is clear that the
conditional posterior distribution of the regression coeﬃcients, given σ2, is
the multivariate normal process
β|σ2, y, H ∼N

β,

X′X + V−1
β
−1
σ2

.
(6.32)
Hence, it is veriﬁed that conditional posterior has the same form as the
conditional prior distribution, except that the mean vector and covariance
matrix have been modiﬁed as a consequence of learning from the data. Since
the stochastic process is multivariate normal, all posterior distributions of
either individual or sets of regression coeﬃcients are normal as well, given
σ2. The means and (co)variances of conditional posterior distributions of
regression coeﬃcients, given some other such coeﬃcients, can be arrived at
using the formulas given in Chapter 1.
It is instructive writing (6.30) as
β =

C + V−1
β
−1
V−1
β mβ +

C + V−1
β
−1
C5β.
Following O’Hagan (1994), now let
W
=

C + V−1
β
−1
C,
I −W
=

C + V−1
β
−1
V−1
β ,

6.2 The Linear Regression Model
299
so the posterior mean becomes
β
=
(I −W) mβ + W5β
=
mβ + W

5β −mβ

.
This representation indicates that the posterior mean is a matrix-weighted
average of the prior mean mβ and of the ML estimator 5β, with the latter
receiving more weight as W increases, in some sense. When the information
in the data is substantial relative to the prior information, so that C is much
larger than V−1
β , then
W =

C + V−1
β
−1
C →I.
The implication is that the prior mean is overwhelmed by the ML estimator,
as the former receives an eﬀective weight of 0. On the other hand, when
“prior precision” (the inverse of the covariance matrix) is large relative to
the information contributed by the data, then W →0, as Vβ has “small”
elements. In this case, the posterior mean would be expected to be very
close to the prior mean, indicating a mild modiﬁcation of prior opinions
about the value of β, after having observed the data.
Similarly, using (6.27), the posterior covariance matrix can be written as

C + V−1
β
−1
σ2 =

Vβ −Vβ

Vβ + C−1−1 Vβ

σ2.
The representation on the right-hand side illustrates that the posterior
variances are always smaller than or equal to the prior variances, at least
when the prior and posterior distributions are normal. When V−1
β
→0,
so prior information becomes increasingly diﬀuse (large elements of V−1
β ),
the posterior covariance tends to C−1σ2, which is in the same form as the
variance covariance matrix of the ML estimator; the same occurs when
the information in the data is very large relative to that contributed by
the prior distribution. Hence, we see that either when prior information
becomes relatively weaker and weaker, or when the relative contribution
of the prior to knowledge about the regression coeﬃcients is much smaller
than that made by the data, then the conditional posterior distribution
tends to
β|σ2, y ∼N

5β, (X′X)−1 σ2
.
(6.33)
Hence, in the limit, our conditional posterior distribution is centered at the
ML estimator, and the posterior covariance matrix is identical in form to
the asymptotic covariance matrix of the ML estimator. This is a particular
case of a more general result on asymptotic approximations to posterior
distributions under regularity conditions and will be discussed in Chapter
7. A technical point must be highlighted here: recall that ﬁnding the asymp-
totic covariance matrix of the ML estimator requires taking expectations

300
6. Bayesian Analysis of Linear Models
over conceptual repeated sampling. However, Fisher’s information matrix
(see Chapter 3) does not play the same role in Bayesian asymptotics; this
is because the paradigm does not involve repeated sampling over the joint
distribution [θ, y] . It will be seen that in Bayesian asymptotics, the ob-
served information plays a role similar to that of expected information in
ML estimation. In the speciﬁc case of the linear regression model discussed
here, the matrix of negative second derivatives of the log-posterior with
respect to β does not involve the observations. Thus, the observed infor-
mation is a constant and, therefore, is equal to its expected value taken
over the distribution of the data. However, this would retrieve the form of
the asymptotic variance–covariance matrix of the ML estimator only when
C overwhelms V−1
β .
Example 6.1
Ridge regression from Bayesian and frequentist points of
view
Suppose the conditional prior of the regressions has the form

β1
β2
"""" σ2, Hβ ∼N

m1
m2

,

I
σ2
β1
σ2
0
0
I
σ2
β2
σ2

σ2

,
so the two sets of coeﬃcients are independent, a priori. Then the mean of
the conditional posterior distribution of the regression coeﬃcients, using
(6.30) and (6.32), is

β1
β2

=


X′
1X1 + I σ2
σ2
β1
X′
1X2
X′
2X1
X′
2X2 + I σ2
σ2
β2


−1 

X′
1y + m1 σ2
σ2
β1
X′
1y + m2 σ2
σ2
β2

.
When there is a single set of regression coeﬃcients and when the prior
mean is a null vector, this reduces to
β = (X′X + Ik)−1 X′y,
where
k = σ2
σ2
β
is a positive scalar. In the regression literature, the linear function of the
data β is known as the “ridge regression estimator”, after Hoerl and Ken-
nard (1970). Seemingly, it did not evolve from a Bayesian argument. Now,
using the notation where the posterior mean is expressed as a weighted
average of the ML estimator and of the prior mean (null in this case), one
can write symbolically
β
=
0 + W

5β −0

=
(X′X + Ik)−1 X′X5β.

6.2 The Linear Regression Model
301
Hence, the ridge regression “shrinks” the ML estimator toward zero, with a
strength that depends on the value of k, the ratio between the two “variance
components”. From a Bayesian perspective, if σ2
β →∞, indicating a vague
prior opinion about the value of the regression coeﬃcients, then there is
little shrinkage, as k is near 0. Here the ridge estimator approaches the
ML estimator. Thus, if X′X is nearly singular, as in models with extreme
collinearity, the posterior distribution is also nearly singular when k = 0.
In this case, there would be extremely large posterior variances, as the
diagonal elements of
(X′X + Ik)−1 →∞.
On the other hand, when σ2
β →0, that is, in a situation where prior infor-
mation is very precise, then the ridge regression estimator tends toward the
prior mean, which is null. Hence, by incorporating prior information, i.e.,
by increasing the value of k, a nearly singular distribution becomes better
conditioned. However, the “improvement” in condition does not depend
on the data; rather, it is a consequence of bringing external information
into the picture. Although this has a natural interpretation in a Bayesian
context, the arguments are less transparent from a frequentist perspective.
For non-Bayesian descriptions of the ridge estimator, see Bunke (1975),
Bibby and Toutenburg (1977), and Toutenburg (1982). In particular, and
from a frequentist point of view, the ridge regression estimator has a Gaus-
sian distribution, since it is a linear combination of Gaussian observations.
Its mean value, taking expectations over the sampling model, is
Ey|β,σ2



X′X + I σ2
σ2
β
−1
X′X5β

=

X′X + I σ2
σ2
β
−1
X′Xβ
=

X′X + I σ2
σ2
β
−1 
X′X + I σ2
σ2
β
−I σ2
σ2
β

β
= β −

X′X + I σ2
σ2
β
−1
σ2
σ2
β
β.
Thus, the ridge regression estimator is biased for β, with the bias vector
being
−

X′X + I σ2
σ2
β
−1
σ2
σ2
β
β,

302
6. Bayesian Analysis of Linear Models
which goes to 0 as σ2
β goes to inﬁnity. The variance–covariance matrix of
its sampling distribution is
V ary|β,σ2



X′X + I σ2
σ2
β
−1
X′X5β


=

X′X + I σ2
σ2
β
−1
X′X

X′X + I σ2
σ2
β
−1
σ2.
What can be gained by using the biased estimator, instead of the ML
statistic? The answer resides in the possibility of attaining a smaller mean
squared error of estimation. The mean squared error matrix, that is, the
sum of the covariance matrix plus the product of the bias vector times is
transpose, is
M (β) = (X′X + Ik)−1 
ββ′k2 + X′Xσ2
(X′X + Ik)−1 .
A “global” measure of the squared error of estimation is given by the sum
of the diagonal elements of the mean squared error matrix
tr [M (β)]
=
tr

(X′X + Ik)−1 ββ′k2 (X′X + Ik)−1
+ tr

(X′X + Ik)−1 X′Xσ2 (X′X + Ik)−1
=
k2β′ (X′X + Ik)−2 β + σ2 tr

X′X (X′X + Ik)−2
,
after appropriate cyclical commutation of matrices under the trace opera-
tor. When k = 0 (no shrinkage at all), then
tr [M (β)] = σ2 tr

(X′X)−1
,
as one would expect, as then the global mean squared error is the sum of the
sampling variances of the ML estimates of individual regression coeﬃcients.
On the other hand, when k →∞, that is, when there is strong shrinkage
toward 0, then tr [M (β)] goes to β′β .
In order to illustrate, consider the case of a single parameter speciﬁcation,
for example, a model with a regression line going through the origin. Here
the model is
yi = βxi + ei.
The ridge regression estimator takes the simple form
β =
n
i=1
xiyi
n
i=1
x2
i + k
.

6.2 The Linear Regression Model
303
Its expectation and sampling variance are
Ey|β,σ2

β

=
β
1 +
k
n

i=1
x2
i
,
and
V ary|β,σ2

β

=
σ2
n
i=1
x2
i

1 +
k
n

i=1
x2
i


2 ,
respectively. Clearly, the ridge estimator (or posterior mean of β for the
prior under discussion) is less variable than the ML estimator. The mean
squared error, after rearrangement, is
M (β) =
β2k2 + σ2

 n
i=1
x2
i


 n
i=1
x2
i
2

1 +
k
n

i=1
x2
i


2 .
Viewed as a function of k, at ﬁxed β, the mean squared error approaches
σ2/
n
i=1 x2
i

when k approaches 0, and β2 when k approaches ∞. A
question of interest is whether there is a value of k making M (β) minimum.
Taking derivatives of the logarithm of M (β) with respect to k and then
solving for the “optimum” value yields
k (β) = σ2
β2 .
However, β is unknown, so the optimum k must be estimated. An intuitively
appealing procedure is given by the following iterative algorithm. Set the
functional iteration
β[i+1] =
n
i=1
xiyi
n
i=1
x2
i +
σ2

β[i]2
.
Start with β[0] equal to the ML estimator, and then iterate until values sta-
bilize. In practice, σ2 must be estimated as well, and natural candidates are
the ML or the REML estimators of the variance for this regression model.
The frequentist properties of this “empirical minimum mean squared error
estimator” are diﬃcult to evaluate analytically.
■

304
6. Bayesian Analysis of Linear Models
Conditional Posterior Distribution of σ2
If one regards the joint density (6.29) as a function of σ2, with the regression
coeﬃcients ﬁxed, then it is clear that the conditional posterior distribution
of the variance is a scaled inverse chi-square process with degree of belief
parameter equal to n + ν∗+ p1 + p2, and with mean value and variance
given by
E

σ2|β, y,H

=
(y −Xβ)′ (y −Xβ) +

β −mβ
′ V−1
β

β −mβ

+ ν∗s∗2
n + ν∗+ p1 + p2 −2
,
and
V ar

σ2|β, y,H

=
2

(y −Xβ)′ (y −Xβ) +

β −mβ
′ V−1
β

β −mβ

+ ν∗s∗22
(n + ν∗+ p1 + p2 −2)2 (n + ν∗+ p1 + p2 −4)
,
respectively.
Marginal Distribution of the Regression Coeﬃcients
Using properties of the inverse gamma distribution, the joint density in
(6.31) can be integrated over σ2 to obtain the following explicit form as
marginal density of the regression coeﬃcients
p (β|y, H) ∝
 
σ2−
 n+ν∗+p1+p2
2
+1

× exp

−

β −β
′ 
C + V−1
β
 
β −β

+ S5
β + Se + ν∗s∗2
2σ2

dσ2
∝

(β−β)
′(C+V−1
β )(β−β)+S
β+Se+ν∗s∗2
2
−n+ν∗+p1+p2
2
∝

1 +

β −β
′ 
C + V−1
β
 
β −β

(n + ν∗)

S
β+Se+ν∗s∗2
n + ν∗



−p1+p2+n+ν∗
2
.
(6.34)
This is the kernel of a multivariate-t density function of order p1 + p2. The
corresponding distribution has mean vector β, covariance matrix
V ar (β|y, H) = S5
β + Se + ν∗s∗2
n + ν∗−2

C + V−1
β
−1
,
and n+ν∗degrees of freedom. Thus, all marginal distributions of individual
or of subsets of regression coeﬃcients are either univariate or multivariate-t.

6.2 The Linear Regression Model
305
The same holds for any linear combination or any conditional distribution
of the regression coeﬃcients, given some other coeﬃcients.
As a side point, note that at ﬁrst sight there does not seem to be a “loss of
degrees of freedom”, relative to n (sample size), in the process of accounting
for uncertainty about the variance. In fact, however, there is a “hidden loss”
of p1 + p2 degrees of freedom, which is canceled by the contribution made
by the conditional prior of the regression coeﬃcients, which involves σ2; see
(6.28). If the prior for β had not involved σ2, with Vβ then being the prior
variance–covariance matrix (assumed known), then the degrees of freedom
of the marginal posterior distribution of the regression coeﬃcients would
have been n+ν∗−p1 −p2. If, in addition, the “degree of belief” parameter
of the prior distribution for σ2 had been taken to be ν∗= 0, the degrees
of freedom would have been n −p1 −p2, that is, the number of degrees of
freedom arising in a standard classical analysis of linear regression. Note,
however, that the Bayesian assignment ν∗= 0 produces the improper prior
distribution
p

σ2
∝1
σ2 ,
which is some sort of “noninformative” prior. In this case, the posterior
distribution of the regression coeﬃcients would be proper if n−p1−p2 > 0.
Marginal Distribution of σ2
Integrating the joint density (6.31) with respect to the regressions, to obtain
the marginal density of the variance, gives:
p

σ2|y, H

∝

σ2−
 n+ν∗+p1+p2
2
+1

exp

−S5
β + Se + ν∗s∗2
2σ2

×
 
exp

−

β −β
′ 
C + V−1
β
 
β −β

2σ2

dβ
∝

σ2−

n+ν∗
2
+1

exp

−S5
β + Se + ν∗s∗2
2σ2

.
(6.35)
Hence, the posterior process is a scaled inverted chi-square distribution,
with mean
E

σ2|y, H

= S5
β + Se + ν∗s∗2
n + ν∗−2
,
mode equal to
Mode

σ2|y, H

= S5
β + Se + ν∗s∗2
n + ν∗+ 2
,
and variance
V ar

σ2|y, H

=
2

S5
β + Se + ν∗s∗22
(n + ν∗−2)2 (n + ν∗−4)
.

306
6. Bayesian Analysis of Linear Models
Again, it is veriﬁed that the posterior distribution of σ2 has the same form
as the prior process, because of the conjugacy property mentioned earlier.
Bayesian learning updates the parameter ν∗in the prior distribution to
n + ν∗, posterior to the data. Similarly, the parameter s∗2 becomes
S5
β + Se + ν∗s∗2
n + ν∗
in the posterior distribution.
Posterior Distribution of the Residuals
Using a similar argument to that employed when inference under improper
uniform priors was discussed, the posterior distribution of a residual is also
univariate-t, with degrees of freedom n + ν∗, mean value equal to
E (ei|y, H) = yi −x′
iβ,
and variance
V ar (ei|y, H) = S5
β + Se + ν∗s∗2
n + ν∗−2
x′
i

C + V−1
β
−1
xi.
Predictive Distributions
In the regression model with proper priors, the two predictive distribu-
tions exist. The prior predictive distribution can be arrived at directly by
taking expectations of the model over the joint prior distribution of the pa-
rameters. First, we take expectations, given σ2, and then decondition with
respect to the variance. In order to do this, note that the observations are
a linear combination of the regression coeﬃcients (which, given σ2, have a
multivariate normal prior distribution) and of the errors, with the latter
being normal as well. Hence, it follows that, conditionally on σ2, the prior
predictive distribution is the Gaussian process
y|mβ, Vβ, σ2 ∼N

Xmβ, (XVβX′ + I) σ2
.
Since the prior distribution of σ2 is scaled inverted chi-square, decondition-
ing the normal distribution above requires evaluating the integral
p

y|mβ, Vβ, s2∗, ν∗
∝
 
σ2−n+ν∗
2
+1
× exp

−(y −Xmβ) (XVβX′ + I)−1 (y −Xmβ) + ν∗s∗2
2σ2

dσ2
∝

(y −Xmβ) (XVβX′ + I)−1 (y −Xmβ) + ν∗s∗2−n+ν∗
2
∝

1 + (y −Xmβ) (XVβX′ + I)−1 (y −Xmβ)
ν∗s∗2
−n+ν∗
2
,

6.2 The Linear Regression Model
307
after making use of the results given in Chapter 1, as seen several times
before. Hence, the prior predictive distribution is the multivariate-t process
on ν∗degrees of freedom
y|H ∼tn

Xmβ, ν∗, (XVβX′ + I) s∗2ν∗
ν∗−2

.
(6.36)
The third term in the argument of the distribution is the variance–covariance
matrix. Thus, (6.36) gives the probability distribution of the data before
observation takes place. If a model assigns low probability to the obser-
vations that actually occur, this can be taken as evidence against such a
model, so some revision would be needed. This issue will be elaborated
further in the discussion about model comparisons.
The posterior predictive distribution applies to future observations, as
generated by the model
yf = Xfβ + ef.
The argument employed for the prior predictive distribution can also be
used here. Conditionally on σ2, the model for the future observations is
a linear combination of normals, with β following the normal conditional
posterior distribution given in (6.32). The future errors have the usual
normal distribution, which is independent of the posterior distribution of β,
since the latter depends on past errors only. Then, given σ2, the predictive
distribution of future observations is the normal process
yf|y, σ,2 H ∼N

Xfβ, Xf

X′X + V−1
β
−1
X′
fσ2 + Ifσ2

.
Next, we must decondition, but this time integrating over the marginal
posterior distribution of σ2, with the corresponding density given in (6.35).
Using similar algebra to that employed for the prior predictive distribution,
one arrives at
yf|y, H ∼t

Xfβ, n + ν∗,
Xf(X′X+V−1
β )
−1X′
f(S
β+Se+ν∗s∗2)
n+ν∗−2

,
(6.37)
with this distribution being an nf-dimensional one. The predictions made
by the model can then be contrasted with future observations in some test
of predictive ability.
6.2.3
Orthogonal Parameterization of the Model
Consider again the linear model
y = Xβ + e
= X1β1 + X2β2 + e,
(6.38)

308
6. Bayesian Analysis of Linear Models
where the residuals follow the usual normal process, and suppose the β′s
have been assigned some prior distribution. It may be that there is a strong
intercorrelation between the ML estimates of β1 and β2. In Bayesian analy-
sis, this would probably be reﬂected in a high degree of correlation between
parameters in the posterior distribution, unless sharp independent priors
are adopted for each of β1 and β2. A more comprehensive discussion of
the role of the prior on inferences will be presented in the next chapter.
For the time being, it suﬃces to recall (5.8) where it was seen that as data
contribute more and more information, the inﬂuence of the prior vanishes
asymptotically. At any rate, a strong posterior inter-correlation tends to
hamper interpretation and to impair numerical behavior, as well as the per-
formance of MCMC algorithms (these techniques, however, would probably
have doubtful value in this simple linear model, since an analytical solution
exists).
An alternative is to entertain an alternative parameterization such that
the new model reproduces the same location and dispersion structure as
(6.38). Ideally, it is desirable to render the new parameters independent
from each other, either in the ML analysis (in which case uncorrelated ML
estimates are obtained), or a posteriori, in the Bayesian setting. Under the
usual normality assumption, with i.i.d. residuals, the ML estimator of β is

5β1
5β2

=

X′
1X1
X′
1X2
X′
2X1
X′
2X2
−1 
X′
1y
X′
2y

.
(6.39)
After eliminating 5β1 from the estimating equations, the ML estimator of
β2 can be written as
5β2 = (X′
2M1X2)−1 X′
2M1y,
where
M1 =

I −X1 (X′
1X1)−1 X′
1

.
Recall that this matrix is idempotent. The ﬁtted residuals are given by
y −X5β = y −X (X′X)−1 X′y
=

I −X (X′X)−1 X′
y
=

I −X (X′X)−1 X′
(Xβ + e)
=

I −X (X′X)−1 X′
e.
Hence, the distribution of the ﬁtted residuals is independent of the β pa-
rameters. The independence of the distribution of the residuals with respect
to β is a consequence of the fact that

I −X (X′X)−1 X′
X = 0,

6.2 The Linear Regression Model
309
and it is said that

I −X (X′X)−1 X′
and X are orthogonal to each other.
In the context of (6.38), it follows that
M1X1 =

I −X1 (X′
1X1)−1 X′
X1 = 0
and, consequently, that
X′
2M1X1 = X′
2

I −X1 (X′
1X1)−1 X′
1

X1 = 0.
Hence, X′
2M1 is orthogonal to X1. Then, for
W
=
[W1, W2]
=
[X1, M1X2],
one has that
W′W =

W′
1W1
W′
1W2
W′
2W1
W′
2W2

=

X′
1X1
0
0
X′
2M1X2

.
Now, if instead of (6.38), one ﬁts the model
y = W1α1 + W2α2 + e,
(6.40)
it turns out the ML estimator of the new regression coeﬃcients is
 5α1
5α2

=

W′
1W1
0
0
W′
2W2


−1 
W′
1y
W′
2y


=

(W′
1W1)−1 W′
1y
(W′
2W2)−1 W′
2y


=


(X′
1X1)−1 X′
1y
(X′
2M1X2)−1 X′
2M1y

.
(6.41)
The ﬁrst term of the vector (6.41) gives the ML estimator of β1 in a model
ignoring β2, whereas the second term gives the ML estimator of β2 in the
full model (6.38). Taking expectations of the ML estimators of the “new”
parameters, with respect to the original model (6.38),
E (5α1)
=
E

(W′
1W1)−1 W′
1y

= (X′
1X1)−1 X′
1E (y)
=
(X′
1X1)−1 X′
1 (X1β1 + X2β2)
=
(X′
1X1)−1 X′
1X1β1 + (X′
1X1)−1 X′
1X2β2
=
β1 + (X′
1X1)−1 X′
1X2β2.

310
6. Bayesian Analysis of Linear Models
This reveals the parametric relationship
α1 = β1 + (X′
1X1)−1 X′
1X2β2.
(6.42)
Further,
E (5α2) = E

(W′
2W2)−1 W′
2y

= (X′
2M1X2)−1 X′
2M1E (y)
= (X′
2M1X2)−1 X′
2M1X1β1 + (X′
2M1X2)−1 X′
2M1X2β2 = β2.
so that
α2 = β2.
(6.43)
Hence, models (6.38) and (6.40) have the same expectation, since
W1α1 + W2α2 = X1β1 + X1 (X′
1X1)−1 X′
1X2β2 + M1X2β2
= X1β1 + X1 (X′
1X1)−1 X′
1X2β2 + X2β2
−X1 (X′
1X1)−1 X′
1X2β2
= X1β1 + X2β2.
(6.44)
Thus, the two models generate the same probability distribution of the
observations, and this can be represented by the statement
y|α1, α1, σ2 ∼N

W1α1 + W2α2, Iσ2
≡y|β1, β2, σ2 ∼N

X1β1 + X2β2, Iσ2
.
Note that for model (6.40):
V ar

 5α1
5α2

=

(X′
1X1)−1 σ2
0
0
(X′
2M1X2)−1 σ2

,
whereas in model (6.38) it is fairly direct to show that:
V ar

5β1
5β2

=

(X′
1M1X1)−1
Q12
Q′
12
(X′
2M2X2)−1

σ2,
where M2 = I −X2 (X′
2X2)−1 X′
2 and
Q12 = (X′
1M1X1)−1 X′
1M1M2X2 (X′
2M2X2)−1 .
Consider now a Bayesian implementation. Model (6.38) requires assign-
ing a prior distribution to the β-coeﬃcients, whereas a prior must be as-
signed to the α′s in the alternative model (6.40). These two priors must
be probabilistically consistent, that is, uncertainty statements about the
α′s must translate into equivalent statements on the β′s, and vice-versa.

6.2 The Linear Regression Model
311
Suppose that one works with (6.40), and that the prior distribution of the
α-coeﬃcients is the normal process

α1
α2

∼N


a1
a2

,

V1
0
0
V2

σ2,
where the hyperparameters are known. Further, σ2 is assumed to follow
a scaled inverted chi-square distribution with parameters ν and s2. Using
(6.34), the density of the posterior distribution of the α-coeﬃcients is
p (α|y, H) ∝

1 + (α −α)′ 
W′W + V−1
(α −α)
(n + υ)

S
α+Se+νs2
n + υ



−p1+p2+n+ν
2
, (6.45)
where

W′W + V−1
=

W′
1W1 + V−1
1
0
0
W′
2W2 + V−1
2

,
α =
 α1
α2

=
 
W′
1W1 + V−1
1
−1 
W′
1y + V−1
1 a1


W′
2W2 + V−1
2
−1 
W′
2y + V−1
2 a2


,
and
S5
α =
2

i=1
(5αi −ai)′ W′
iWi

W′
iWi + V−1
i
−1 V−1
i
(5αi −ai) ,
with
Se = y′y −
2

i=1
5α′
iW′
iy.
The two sets of regression coeﬃcients are also uncorrelated, a posteriori,
because the variance–covariance matrix of the multivariate-t distribution
with density (6.45) is diagonal. However, α1 and α2 are not stochasti-
cally independent, even with independent priors and with an orthogonal
parameterization. This is because the process of deconditioning with re-
spect to σ2, renders the α′s mutually dependent. Recall that the density of
a multivariate-t distribution with a diagonal covariance matrix cannot be
written as the product of the intervening marginal densities.
Since, presumably, inferential interest centers on the original regression
coeﬃcients, the joint posterior density of β1 and β2 can be obtained by

312
6. Bayesian Analysis of Linear Models
eﬀecting the change of variables implicit in (6.42) and (6.43). In matrix
notation

β1
β2

=

I
−(X′
1X1)−1 X′
1X2
0
I
 
α1
α2

= Tα,
with inverse
α1
α2

=

I
(X′
1X1)−1 X′
1X2
0
I
 
β1
β2

= T−1β.
The determinant of the Jacobian matrix is
|J| = det

T−1
= 1
since the determinant of a triangular matrix is equal to the product of its
diagonal elements, in this case all being equal to 1. Then the joint p.d.f. of
β1 and β2, using the inverse transformation in conjunction with (6.45), is
p (β|y, H) ∝

1 +

β −β
′ T′−1 
W′W + V−1
T−1 
β −β

(n + υ)

S
α+Se+νs2
n + υ



−p+n+ν
2
∝





1 +

β −β
′ 
T

W′W + V−1−1 T′−1 
β −β

(n + υ)

S
α+Se+νs2
n+υ






−p+n+ν
2
,
(6.46)
where p = p1 + p2, and
β = Tα.
Hence, the posterior distribution of the original regression coeﬃcients is
also multivariate−t, with mean vector β, variance–covariance matrix:
V ar (β|y, H) = S5
α + Se + νs2
n + υ −2
T

W′W + V−1−1 T′
and n + υ degrees of freedom.
It cannot be overemphasized that if one works with the parameterization
on the β′s, and that if independent priors are assigned to the two sets
of regressions, the resulting multivariate-t distribution would be diﬀerent
from that with density (6.46). This is so because, then, the induced prior
for the α′s implies that the two sets of regressions are correlated, with prior
covariance matrix
Cov (α1, α′
2) = Cov

β1 + (X′
1X1)−1 X′
1X2β2, β′
2

= Cov

β1, β′
2

+ (X′
1X1)−1 X′
1X2 V ar (β2) .
Thus, if the β′s are taken as independently distributed a priori, this would
not be the case for the α′s. Hence, one would not obtain the same proba-
bility statements as those resulting from a model where the α′s are inde-
pendent, a priori.

6.3 The Mixed Linear Model
313
6.3
The Mixed Linear Model
The mixed linear model, or (co)variance components model (e.g., Henderson,
1953; Searle, 1971), is one that includes ﬁxed and random eﬀects entering
linearly into the conditional (given the random eﬀects) expectation of the
observations. Typically, as seen several times before, the random eﬀects and
the model residuals are assigned Gaussian distributions which depend on
components of variance or covariance. These dispersion parameters may be
regarded as known (e.g., as in the case of best linear unbiased prediction)
or unknown. The distinction between ﬁxed and random eﬀects does not
arise naturally in the Bayesian framework. However, many frequentist and
likelihood-based results can be obtained as special (and limiting) cases of
the Bayesian linear model. In this section, a linear model with two vari-
ance components will be discussed in detail from a Bayesian perspective.
Models with more than two components of variance, or multivariate linear
models where several vectors of location parameters enter into the condi-
tional expectation structure, require a somewhat more involved analytical
treatment. However, Bayesian implementations are possible through Monte
Carlo methods, as discussed in subsequent chapters. Here, use will be made
of results presented in Lindley and Smith (1972), Gianola and Fernando
(1986), and Gianola et al. (1990), as well as developments given in the
present and in the preceding chapters.
6.3.1
Bayesian View of the Mixed Eﬀects Model
Consider the linear model
y = Xβ + Zu + e,
where β, (p × 1), and u, (q × 1), are location vectors related to observa-
tions y, (n × 1), through the nonstochastic matrices X and Z, respectively.
Further, let
e|σ2
e ∼N

0, Iσ2
e

,
be a vector of independently distributed residuals. Hence, given the residual
variance, the sampling model is
y|β, u, σ2
e ∼N

Xβ + Zu, Iσ2
e

.
(6.47)
Suppose that elicitation yields the prior distribution
β|σ2
β ∼N

0, Bσ2
β

,
where B is a known, nonsingular matrix and σ2
β is a hyperparameter. In
the Bayesian setting, thus, the “ﬁxed” (in the frequentist view) vector β is
random, since it is assigned a distribution, this being a normal one in this
case. Further, take
u|Aσ2
u ∼N

0, Aσ2
u

,

314
6. Bayesian Analysis of Linear Models
where, in a quantitative genetics context, A would be a matrix of addi-
tive relationships (also assumed to be nonsingular) whenever u is a vector
of additive genetic eﬀects and σ2
u is the additive genetic variance. In the
classical mixed model, the preceding distribution, rather than viewed as
a Bayesian prior, is interpreted as one resulting from a long-run process
of sampling vectors from some conceptual population, with ﬁxed A and
σ2
u. Such a sampling process generates a distribution with null mean and
covariance matrix Aσ2
u. It follows that the location structure, Xβ + Zu,
has only one random component (u) in a frequentist setting, whereas it has
two (β and u) in the Bayesian probability model.
Assume that the prior distributions elicited for the variance compo-
nents are independent scaled inverted chi-square processes with parameters

s2
e, νe

and

s2
u, νu

for σ2
e and σ2
u, respectively. The consequences of these
Bayesian assumptions follow:
• Unconditionally to the residual variance, the sampling model, given
β and u, is multivariate-t, with density
p

y|β, u, s2
e, νe

∝

1 + (y −Xβ −Zu)′ (y −Xβ −Zu)
νes2e
−n+νe
2
.
(6.48)
• The marginal distribution of the additive eﬀects, unconditionally to
the additive genetic variance, is a multivariate-t process having den-
sity
p

u|s2
u, νu

∝

1 + u′A−1u
νus2u
−q+νu
2
.
(6.49)
• In the frequentist mixed eﬀects linear model, the marginal distribu-
tion of the observations is indexed by the “ﬁxed” parameter β and
by the variance components σ2
u and σ2
e, and this is
y|β, σ2
u,σ2
e ∼N

Xβ, ZAZ′σ2
u + Iσ2
e

.
(6.50)
In the Bayesian model, on the other hand, the marginal distribution
of the observations is obtained by integrating over all unknown pa-
rameters

θ = β, u, σ2
u, σ2
e

entering in the model. If the joint prior
of all parameters has the form
p

β, u, σ2
u, σ2
e|σ2
β, s2
u, νu, s2
e, νe

= p

β|σ2
β

p

u|σ2
u

p

σ2
u|s2
u, νu

p

σ2
e|s2
e, νe

,

6.3 The Mixed Linear Model
315
then the marginal density of the observations can be written as
p

y|σ2
β, s2
u, νu, s2
e, νe

=
   
p

y|β, u, σ2
e

p

θ|σ2
β, s2
u, νu, s2
e, νe

dθ
=
  
p

y|β, u, σ2
e

p

σ2
e|s2
e, νe

dσ2
e

×

p

u|σ2
u

p

σ2
u|s2
u, νu

dσ2
u

p

β|σ2
β

dβ du
(6.51)
with the integrals in brackets evaluating to (6.48) and (6.49), re-
spectively. Hence, the marginal density of the observations, or prior
predictive distribution, cannot be written in closed form when the
variance components are unknown.
• It is possible to go further than in (6.51) when the dispersion pa-
rameters are known. In the Bayesian model, one has to contemplate
the variability (uncertainty) of the β values introduced by the prior
distribution. In this case, the marginal density of the observations
would be
p

y|σ2
β, σ2
u, σ2
e

=
 
p

y|β, u, σ2
e

p

β|σ2
β

p

u|σ2
u

dβ du.
(6.52)
Since the three densities in the integrand are normal, the exponents
can be combined as
1
σ2e

(y −Xβ −Zu)′ (y −Xβ −Zu) + β′B−1β σ2
e
σ2
β
+u′A−1u σ2
e
σ2u

= 1
σ2e

(y −Wα)′ (y −Wα) + α′Σ−1α

,
(6.53)
where α =

β′, u′′, W =
 X
Z 
, and
Σ−1 =

B−1 σ2
e
σ2
β
0
0
A−1 σ2
e
σ2
u

.
Now let
5α =

W′W + Σ−1−1 W′y.
(6.54)

316
6. Bayesian Analysis of Linear Models
Hence,
(y −Wα)′ (y −Wα) + α′Σ−1α
=
y′y −2α′W′y + α′ 
W′W + Σ−1
α
=
y′y−2α′ 
W′W + Σ−1 5α + α′ 
W′W + Σ−1
α
+5α′ 
W′W + Σ−1 5α −5α′ 
W′W + Σ−1 5α
where the quadratic in 5α is added and subtracted in order to complete
a “square”. One can then write
y′y−5α′ 
W′W + Σ−1 5α+
(α−5α)′ 
W′W + Σ−1
(α−5α)
= y′y−5α′W′y + (α−5α)′ 
W′W + Σ−1
(α−5α) .
(6.55)
Using this in (6.53), it turns out that the integrand in (6.52) is ex-
pressible as
p

y|σ2
β, σ2
u, σ2
e

∝

exp

−1
2σ2e

y′y −5α′W′y
+ (α −5α)′ 
W′W + Σ−1
(α −5α)

dα
∝exp

−y′y −5α′W′y
2σ2e
 
exp

−
(α −α)′(W′W+Σ−1)(α −α)
2σ2
e

dα.
(6.56)
The integral in (6.56) involves the kernel of a p+q-dimensional Gaus-
sian distribution, and it evaluates to
(2π)
p+q
2
"""

W′W + Σ−1−1 σ2
e
"""
1
2 .
Since this does not involve the data, it gets absorbed in the integration
constant, yielding, as marginal density of the data,
p

y|σ2
β, σ2
u, σ2
e

=
exp

−y′y −5α′W′y
2σ2e


exp

−y′y −5α′W′y
2σ2e

dy
.
(6.57)
This can be shown to be the density of the normal process
y|σ2
β, σ2
u, σ2
e ∼N

0, XBX′σ2
β + ZAZ′σ2
u + Iσ2
e

.
A comparison of this with (6.50) reveals that the Bayesian linear
model cannot be construed as its frequentist counterpart. In the lat-
ter, the marginal distribution of the observations requires knowledge

6.3 The Mixed Linear Model
317
of the ﬁxed eﬀects and of the variances in order to compute probabil-
ities. In the Bayesian model, the “ﬁxed” eﬀects have been integrated
out (with respect to the prior), and the marginal process of the obser-
vations also depends on the known hyperparameter σ2
β. The Bayesian
model diﬀers in other important respects, as illustrated in the follow-
ing sections. For example, ﬁnding the marginal distribution of the
observations (unconditionally to all parameters), or prior predictive
distribution requires additional integration with respect to σ2
u and
σ2
e.
6.3.2
Joint and Conditional Posterior Distributions
Under the assumptions made, the joint posterior density of all parameters
is
p

β, u, σ2
u, σ2
e|y, σ2
β, s2
u, νu, s2
e, νe

∝p

y|β, u, σ2
e

p

β|σ2
β

p

u|σ2
u

×p

σ2
u|s2
u, νu

p

σ2
e|s2
e, νe

.
(6.58)
All fully conditional posterior distributions can be identiﬁed directly from
the joint posterior density, by viewing the latter as a function of the param-
eter(s) of interest, while keeping all other parameters and data y (“ELSE”)
ﬁxed. All such densities can be written in closed form, as shown below.
• The conditional posterior density of the additive genetic variance, σ2
u,
given all other parameters, is
p

σ2
u|σ2
β, s2
u, νu, s2
e, νe, ELSE

∝p

u|σ2
u

p

σ2
u|s2
u, νu

∝

σ2
u
−q+νu+2
2
exp

−u′A−1u + νus2
u
2σ2u

.
(6.59)
This is the kernel of a scaled inverted chi-square distribution with
degree of belief parameter ν′
u = q + νu, and scale parameter
s′2
u = u′A−1u + νus2
u
q + νu
.
Similarly, the fully conditional posterior density of σ2
e is
p

σ2
e|σ2
β, s2
u, νu, s2
e, νe, ELSE

∝p

y|β, u, σ2
e

p

σ2
e|s2
e, νe

∝

σ2
e
−n+νe+2
2
exp

−(y −Xβ −Zu)′ (y −Xβ −Zu) +νes2
e
2σ2e

.
(6.60)
Hence, the conditional posterior distribution of the residual variance
is scaled inverted chi-square, with parameters ν′
e = n + νe, and
s′2
e = (y −Xβ −Zu)′ (y −Xβ −Zu) + νes2
e
n + νe
.

318
6. Bayesian Analysis of Linear Models
It is easy to verify that, given all other parameters, the two variance
components are conditionally independent.
• The density of the conditional distribution of the “ﬁxed” and “ran-
dom” eﬀects, given the variance components, is
p

β, u|σ2
β, s2
u, νu, s2
e, νe, ELSE

∝p

y|β, u, σ2
e

p

β|σ2
β

p

u|σ2
u

.
Use can be made here of (6.53) and (6.55), to arrive at
p

β, u|σ2
β, s2
u, νu, s2
e, νe, ELSE

∝exp

−1
2σ2e

y′y −5α′W′y + (α −5α)′ 
W′W + Σ−1
(α −5α)
%
∝exp

−(α −5α)′ 
W′W + Σ−1
(α −5α)
2σ2e

,
(6.61)
since the term that does not depend on α is absorbed in the integra-
tion constant. It follows that the corresponding conditional posterior
distribution is the multivariate normal process, as discussed in Chap-
ter 1, Example 1.18,
β, u|y, σ2
β, s2
u, νu, s2
e, νe, σ2
u, σ2
e ∼N

5α,

W′W + Σ−1−1 σ2
e

.
(6.62)
Note that since σ2
u and σ2
e are assumed to be known, the distribution
does not depend on parameters s2
u, νu, s2
e and νe. However, these are
left in the notation for the sake of symmetry in the presentation. We
now write the mean of this distribution explicitly as

X′X + B−1 σ2
e
σ2
β
X′Z
Z′X
Z′Z + A−1 σ2
e
σ2
u


 5β
5u

=

X′y
Z′y

.
If σ2
β →∞, which represents in some sense vague prior knowledge
about β, the above system of equations becomes what is known as
Henderson’s mixed model equations (Henderson et al., 1959; Hender-
son, 1973; Searle, 1971). In this situation, it can be shown that 5β
above is the ML estimator of the ﬁxed vector in a Gaussian mixed
linear model with known variance components. Further, 5u is the best
linear unbiased predictor of u, or BLUP (this holds under a mixed
linear model even if the joint distribution of the random eﬀects and
of the observations is not Gaussian, but the dispersion parameters
must be known). Hence, both the ML estimator and the BLUP arise
in the context of special cases of a more general Bayesian setting. If
there is prior information about the “ﬁxed” eﬀects, σ2
β would be ﬁnite
and, hence, the Bayesian model would eﬀect some shrinkage toward

6.3 The Mixed Linear Model
319
the prior mean (in this case assumed to be null) of the β vector. In
fact, let ,α be the solution to the system
W′W,α = W′y.
The vector ,α can be interpreted as the ML estimator of α in a model
in which both β and u are treated as ﬁxed (or as posterior mean in a
model where both σ2
β and σ2
u go to inﬁnity). Often, ,α is not deﬁned
uniquely, as W′W may have deﬁcient rank. On the other hand, 5α
is unique, provided that either X has full-column rank, or that σ2
β is
ﬁnite. Then, the posterior mean vector
5α =

W′W + Σ−1−1 W′y
=

W′W + Σ−1−1 
W′W,α + Σ−10

can be viewed as a matrix weighted average of ,α (with this vector
being a function of the data only) and of the mean of the prior dis-
tribution of α, which is 0. The matrix weights are W′W (a measure
of the precision of inferences contributed by the data) and Σ−1(a
measure of prior precision), respectively.
• Result (6.62) implies that the marginal distributions of β and u, given
the variance components, are also multivariate normal, having mean
vectors 5β and 5u, respectively. We will derive the form of these two
densities systematically. First, note that the joint posterior density
of β and u, given the dispersion parameters, can be written as
p

β, u|y, σ2
β, σ2
u, σ2
e

∝exp

−1
2σ2e

(y −Xβ −Zu)′ (y −Xβ −Zu)
+ β′B−1β σ2
e
σ2
β
+ u′A−1u σ2
e
σ2u
$
.
(6.63)
Now put w (β) = y −Xβ and write, employing a decomposition
similar to that in (6.55),
(y −Xβ −Zu)′ (y −Xβ −Zu) + u′A−1u σ2
e
σ2u
= [w (β) −Zu]′ [w (β) −Zu] + u′A−1u σ2
e
σ2u
= w′ (β) w (β) −55u (β)′ Z′w (β)
+

55u (β) −u
′ 
Z′Z + A−1 σ2
e
σ2u
 
55u (β) −u

,
where
55u (β) =

Z′Z + A−1 σ2
e
σ2u
−1
Z′w (β) .

320
6. Bayesian Analysis of Linear Models
Making use of this in the joint density (6.63) and integrating with
respect to u, to obtain the marginal posterior density of β, given the
variance components, yields
p

β|y, σ2
β, σ2
u, σ2
e

∝exp

−1
2σ2e

w′ (β) w (β) −55u (β)′ Z′w (β) + β′B−1β σ2
e
σ2
β
%

exp

−1
2σ2e

55u (β) −u
′ 
Z′Z + A−1 σ2
e
σ2u
 
55u (β) −u
%
du
∝exp

−1
2σ2e

w′ (β) w (β) −55u (β)′ Z′w (β) + β′B−1β σ2
e
σ2
β
%
× (2π)
q
2
""""

Z′Z + A−1 σ2
e
σ2
u
−1
σ2
e
""""
1
2
∝exp

−1
2σ2e

w′ (β) w (β) −55u (β)′ Z′w (β) + β′B−1β σ2
e
σ2
β
%
.
(6.64)
Now
w′ (β) w (β) −55u (β)′ Z′w (β) + β′B−1β σ2
e
σ2
β
= (y −Xβ)′ (y −Xβ)
−(y −Xβ)′ Z

Z′Z + A−1 σ2
e
σ2u
−1
Z′ (y −Xβ) + β′B−1β σ2
e
σ2
β
= (y −Xβ)′

I −Z

Z′Z + A−1 σ2
e
σ2
u
−1
Z′

(y −Xβ)
+ β′B−1β σ2
e
σ2
β .
(6.65)
Using matrix identity (6.27), it can be shown that
I −Z

Z′Z + A−1 σ2
e
σ2
u
−1
Z′ =

ZAZ′ σ2
u
σ2
e + I
−1
= V−1.
Making use of this in (6.65), the marginal density (6.64) can be writ-
ten as
p

β|y, σ2
β, σ2
u, σ2
e

∝
9
−1
2σ2
e

(y −Xβ)′ V−1 (y −Xβ) + β′B−1β σ2
e
σ2
β
:
.
Note that, as σ2
β →∞, the kernel of the posterior density tends
toward the likelihood of β in a Gaussian linear mixed model with

6.3 The Mixed Linear Model
321
known variance components and covariance matrix as given in (6.50).
The ML estimator of β is
,β =

X′V−1X
−1 X′V−1y,
and using the standard decomposition, one obtains
p

β|y, σ2
β, σ2
u, σ2
e

∝exp

−1
2σ2
e

y −X,β
′
V−1 
y −X,β

+

β −,β
′
X′V−1X

β −,β

+ β′B−1β σ2
e
σ2
β
%
∝exp

−1
2σ2
e

β −,β
′
X′V−1X

β −,β

+ β′B−1β σ2
e
σ2
β
%
. (6.66)
The quadratic forms in β can now be combined as

β −,β
′
X′V−1X

β −,β

+ β′B−1β σ2
e
σ2
β
=

β −5β
′ 
X′V−1X + B−1 σ2
e
σ2
β
 
β −5β

+ ,β
′X′V−1X

X′V−1X + B−1 σ2
e
σ2
β
−1
B−1 σ2
e
σ2
β
,β,
(6.67)
where
5β =

X′V−1X + B−1 σ2
e
σ2
β
−1
X′V−1y.
After some matrix manipulations, it is possible to show that this is
precisely the β-component of 5α in (6.62). Using (6.67) in (6.66) and
retaining the part that varies with β yields, as density of the marginal
distribution of β (conditionally on the variance components),
p

β|y, σ2
β, σ2
u, σ2
e

∝exp

−1
2σ2e

β −5β
′ 
X′V−1X + B−1 σ2
e
σ2
β
 
β −5β
%
.
Thus, the marginal posterior density of β when the dispersion pa-
rameters are known, is the normal process
β|y, σ2
β, σ2
u, σ2
e ∼N

5β,

X′V−1X + B−1 σ2
e
σ2
β
−1
σ2
e

.
(6.68)
Again, note that the mean of the distribution is a matrix weighted
average of the ML estimator of β and of the mean of the prior distri-
bution of this vector, assumed to be null in this case.

322
6. Bayesian Analysis of Linear Models
• Using similar algebra, it can be found that the marginal posterior
density of u, given the variance components, is
u|y, σ2
β, σ2
u, σ2
e ∼N

5u,

Z′T−1Z + A−1 σ2
e
σ2
u
−1
σ2
e

,
(6.69)
with
5u =

Z′T−1Z + A−1 σ2
e
σ2
u
−1
Z′T−1y,
and
T = XBX′ σ2
β
σ2
e + I.
The mean vector is equal to the u-component of 5α in (6.62).
• Since the joint distribution of β and u is jointly Gaussian, with pa-
rameters as in (6.62), it follows that the processes

β|y, σ2
β, s2
u, νu, s2
e, νe, σ2
u, σ2
e, u

,

u|y, σ2
β, s2
u, νu, s2
e, νe, σ2
u, σ2
e, β

,
are normal as well. Making use of results in Chapter 1, one can arrive
at
β|y, σ2
β, s2
u, νu, s2
e, νe, σ2
u, σ2
e, u
∼N

5β (u) ,

X′X + B−1 σ2
e
σ2
β
−1
σ2
e

,
(6.70)
where
5β (u) =

X′X + B−1 σ2
e
σ2
β
−1
X′ (y −Zu) .
Also
u|y, σ2
β, s2
u, νu, s2
e, νe, σ2
u, σ2
e, β
∼N

5u (β) ,

Z′Z + A−1 σ2
e
σ2
u
−1
σ2
e

,
(6.71)
where
5u (β) =

Z′Z + A−1 σ2
e
σ2
u
−1
Z′ (y −Xβ) .
6.3.3
Marginal Distribution of Variance Components
The location parameters can be integrated out analytically of the joint
posterior density. First, write the density of the joint posterior distribution

6.3 The Mixed Linear Model
323
of all parameters as
p

β, u,σ2
u,σ2
e|y, σ2
β, s2
u, νu, s2
e, νe

∝p

σ2
u|s2
u, νu

p

σ2
e|s2
e, νe
 
σ2
e
−n
2 
σ2
u
−q
2
× exp
9
−1
2σ2
e

y′y −5α′
cW′y + (α −5αc)′ 
W′W + Σ−1
(α −5αc)
:
∝p

σ2
u|s2
u, νu

p

σ2
e|s2
e, νe
 
σ2
e
−n
2 
σ2
u
−q
2 exp

−y′y −5α′
cW′y
2σ2e

× exp

−(α −5αc)′ 
W′W + Σ−1
(α −5αc)
2σ2e

,
where 5αc is 5α of (6.62), with the subscript placed to emphasize the de-
pendence of this vector on the unknown variance components σ2
u and σ2
e.
The location parameters now appear in the kernel of a multivariate normal
density and can be integrated out by analytical means. After integration,
one gets
p

σ2
u, σ2
e|y, σ2
β, s2
u, νu, s2
e, νe

∝p

σ2
u|s2
u, νu

p

σ2
e|s2
e, νe
 
σ2
e
−n
2 
σ2
u
−q
2 exp

−y′y −5α′
cW′y
2σ2e

×
"""

W′W + Σ−1−1 σ2
e
"""
1
2 .
The joint density of the two variance components can be written explicitly
as
p

σ2
u, σ2
e|y, σ2
β, s2
u, νu, s2
e, νe

∝

σ2
u
−q+νu+2
2

σ2
e
−n−p−q+νe+2
2
× exp

−
y′y −5α′
cW′y + νes2
e + νus2
u
σ2
e
σ2
u
2σ2e

""
W′W + Σ−1""−1
2 .
(6.72)
It is not possible to go further in the level of marginalization. This is because
the joint distribution involves ratios between variance components, both in
the exponential expression and in the matrix Σ, as part of the determinant.
Note that the two variance components are not independent a posteriori
(even if they are so, a priori).
6.3.4
Marginal Distribution of Location Parameters
The integral of the joint posterior density (6.58), with respect to the un-
known variance components, so as to obtain the unconditional posterior

324
6. Bayesian Analysis of Linear Models
density of the location parameters, can be represented as
p

β, u|y, σ2
β, s2
u, νu, s2
e, νe

∝

p

y|β, u, σ2
e

p

σ2
e|s2
e, νe

dσ2
e

×

p

u|σ2
u

p

σ2
u|s2
u, νu

dσ2
u

p

β|σ2
β

.
Each of the two integrals is proportional to the kernel of a multivariate-t
density of appropriate order, so one can write:
p

β, u|y, σ2
β, s2
u, νu, s2
e, νe

∝

1 + (y −Xβ −Zu)′ (y −Xβ −Zu)
νes2e
−n+νe
2
×

1 + u′A−1u
νus2u
−q+νu
2
p

β|σ2
β

.
(6.73)
This density does not have a recognizable form. The ﬁrst two expressions
are multivariate-t, and their product would deﬁne a poly-t density, if it
were not for the presence of p

β|σ2
β

.
Suppose now that σ2
β goes to inﬁnity, so that, in the limit, one is in a
situation of vague prior knowledge about this location parameter. The prior
density of β becomes ﬂatter and ﬂatter and, in the limit, it is proportional
to a constant. However, this uniform process is not proper, as the integral
over β is not ﬁnite. In this case, the joint distribution of β and u is in
a poly-t form, and marginalization can be carried out one step further.
Before integration of the variance components, the joint posterior density
takes the form
p

β, u, σ2
e, σ2
u|y, s2
u, νu, s2
e, νe

∝p

y|β, u, σ2
e

p

σ2
e|s2
e, νe

p

u|σ2
u

p

σ2
u|s2
u, νu

,
since now the prior density of β is ﬂat. One can write
(y −Xβ −Zu)′ (y −Xβ −Zu) = [w (u) −Xβ]′ [w (u) −Xβ]
=

w (u) −X,β
′ 
w (u) −X,β

+

β −,β
′
X′X

β −,β

,
where
,β
=
(X′X)−1 X′w (u)
=
(X′X)−1 X′ (y −Zu)

6.3 The Mixed Linear Model
325
is the “regression” of w (u) = y−Zu on X. Using this in the joint posterior
density yields
p

β, u, σ2
e, σ2
u|y, s2
u, νu, s2
e, νe

∝

σ2
e
−n+νe+2
2
exp







−

w (u) −X,β
′ 
w (u) −X,β

+ νes2
e
2σ2e







× exp

−

β −,β
′
X′X

β −,β

2σ2e



σ2
u
−q+νu+2
2
exp

−u′A−1u + νus2
u
2σ2u

.
This expression can be integrated analytically with respect to β, to obtain
p

u,σ2
e, σ2
u|y, s2
u, νu, s2
e, νe

∝

σ2
e
−n+νe+2
2
exp

−

w (u) −X,β
′ 
w (u) −X,β

+ νes2
e
2σ2e


×
"""(X′X)−1 σ2
e
"""
1
2 
σ2
u
−q+νu+2
2
exp

−u′A−1u + νus2
u
2σ2u

∝

σ2
e
−n−p+νe+2
2
exp

−

w (u) −X,β
′ 
w (u) −X,β

+ νes2
e
2σ2e


×

σ2
u
−q+νu+2
2
exp

−u′A−1u + νus2
u
2σ2u

.
(6.74)
Note now that the two variance components appear in kernels of scaled
inverted chi-square densities and that, given u, their distributions are in-
dependent. Further, the dispersion parameters can be integrated out ana-
lytically, obtaining
p

u|y, σ2
β, s2
u, νu, s2
e, νe

∝

w (u) −X,β
′ 
w (u) −X,β

+ νes2
e
−n−p+νe
2
×

u′A−1u + νus2
u
−q+νu
2
∝

1 +

w (u) −X,β
′ 
w (u) −X,β

νes2e


−n−p+νe
2
×

1 + u′A−1u
νus2u
−q+νu
2
,

326
6. Bayesian Analysis of Linear Models
as the hyperparameters are constant, and can be factored out of the ex-
pression. Finally, note that
w (u) −X,β
=
y −Zu −X (X′X)−1 X′ (y −Zu)
=
M (y −Zu) ,
where M = I −X (X′X)−1 X′. Using this in the marginal posterior density
of u above gives
p

u|y, σ2
β, s2
u, νu, s2
e, νe

∝

1 + (y −Zu)′ M (y −Zu)
νes2e
−n−p+νe
2
×

1 + u′A−1u
νus2u
−q+νu
2
.
(6.75)
Hence, the marginal distribution of u is not in any easily recognizable form
and further marginalization, e.g., with respect to a subvector of this location
parameter, is not feasible by analytical means. However, it is possible to
draw samples from the marginal distribution of each element of u by means
of MCMC methods to be discussed in later chapters.
In summary, most of the analytical results available for a Bayesian analy-
sis of linear models under Gaussian assumptions have been presented here.
When the model contains more than one unknown variance component, it
is not possible to arrive at the fully marginal distributions of individual pa-
rameters. The analytical treatment is even more involved when the model
is nonlinear in the location parameters, or when the response variables
are not Gaussian. In the next two chapters, additional topics in Bayesian
analysis will be presented, such as the role of the prior distribution and
Bayesian tools for model comparison.

7
The Prior Distribution and
Bayesian Analysis
7.1
Introduction
In the preceding two chapters, some of the basic machinery for the develop-
ment of Bayesian probability models, with emphasis on linear speciﬁcations,
was presented. It was seen that the inferences drawn depend on the forms
of the likelihood function and of the prior distribution. A natural question
is: What is the impact of the prior on inferences? Clearly, a similar query
could be raised about the eﬀect of the likelihood function. Alternatively,
one can pose the question: How much information is contributed by the
data (or by the prior) about the quantities of interest? In this chapter, we
begin with an example that illustrates the eﬀect of the prior distribution on
inferences. Subsequently, some measures of statistical information are pre-
sented and used to quantify what is encoded in the likelihood function, and
in the prior and posterior processes. Another section describes how prior
distributions, contributing “little” information relative to that contributed
by data, can be constructed. This section includes a discussion of Jeﬀreys’
prior, of the maximum entropy principle, and of what is called reference
analysis. A step-by-step derivation of the associated reference prior, includ-
ing the situation in which nuisance parameters are present in the model, is
given at the end.

328
7. The Prior Distribution and Bayesian Analysis
n
ML estimate
Posterior mode
Posterior mean
5
0.2
0.2
0.286
10
0.2
0.2
0.250
20
0.2
0.2
0.227
40
0.2
0.2
0.214
TABLE 7.1. Eﬀect of sample size (n) on the mean and mode of the posterior
distribution of the gene frequency: uniform prior.
7.2
An Illustration of the Eﬀect
of Priors on Inferences
Suppose that one wishes to infer the frequency of a certain allele (θ) in
some homogeneous population. Further, assume that all that can be stated
a priori is that the frequency is contained in the interval [0, 1] . A random
sample of n genes is drawn from the population and x copies of the allele
of interest are observed in the sample. The ML estimator of θ is x/n, and
its sample variance is θ (1 −θ) /n (this can be estimated empirically by
replacing the unknown parameter by its ML estimate).
A reasonable Bayesian probability model consists of a uniform prior dis-
tribution in the said interval, plus a binomial sampling
model, with x
successes out of n independent trials. The prior distribution is centered at
1/2, and the posterior density of the gene frequency is
p (θ|x, n) ∝θx (1 −θ)n−x .
Hence, the posterior process is the Be (x + 1, n −x + 1) distribution, with
its mode being equal to the ML estimator, and the posterior mean being
(x + 1) / (n + 2). Table 7.1 gives a sequence of posterior distributions at
increasing sample sizes; in each of these distributions the ratio of the num-
ber of successes to sample size is kept constant at 0.2, which is the ML
estimate of the gene frequency. Note that the posterior mean gets closer
to the ML estimate as n increases; in the limit, the posterior mean tends
toward x/n, suggesting that the information from the sample overwhelms
the prior, asymptotically.
Assume now that the prior distribution is the Be (11, 11) process. The
posterior density is now
p (θ|x, n) ∝θx+11−1 (1 −θ)n+11−x−1 ,
so the corresponding distribution is Be (x + 11, n + 11 −x) , having mean
(x + 11) / (n + 22), and mode (x + 10) / (n + 20). Note that as n →∞,
both the mean and mode go towards x/n. Table 7.2 gives a sequence of
posterior distributions similar to that displayed in Table 7.1. Again, both
the posterior mean and mode move toward the ML estimator as sample size
increases, but the inﬂuence of the beta prior assigned here is more marked

7.2 An Illustration of the Eﬀect of Priors on Inferences
329
n
ML estimate
Posterior mode
Posterior mean
5
0.2
0.440
0.444
10
0.2
0.400
0.406
20
0.2
0.350
0.357
50
0.2
0.286
0.292
TABLE 7.2. Eﬀect of sample size (n) on the mean and mode of the posterior
distribution of the gene frequency: beta prior.
than that of the uniform distribution in the preceding case. The reason for
this is that the Be (11, 11) distribution is fairly sharp and assigns small
prior probability to values of θ smaller than 1
4. If n = 1000 and x = 200,
thus keeping the ML estimator at 1
5, the posterior mean and mode would be
both approximately equal to 0.207, verifying the “asymptotic domination”
of the prior by the likelihood function.
The dissipation of the inﬂuence of the prior as sample size increases was
already seen in a discrete setting, when Bayes theorem was introduced.
The result can be coined in a more general form as follows. Suppose that
n independent draws are made from the same distribution [yi|θ] , and let
the prior density be p (θ|H) , where H could be a set of hyperparameters.
The posterior density of θ is then
p (θ|y1, y2, . . . , yn, H) ∝p (θ|H)
n
-
i=1
p (yi|θ)
∝exp
# n

i=1

log [p (yi|θ)] + log [p (θ|H)]
n
$
.
Then, as n increases,
log [p (yi|θ)] + log [p (θ|H)]
n
→log [p (yi|θ)] ,
and
p (θ|y1, y2, ..., yn, H) →
exp
 n
i=1
log [p (yi|θ)]
%

exp
 n
i=1
log [p (yi|θ)]
%
dθ
,
which is the normalized likelihood, assuming the integral exists. Hence, the
contribution of the prior to the posterior becomes less and less important as
the sample size grows. This can be expressed by saying that, given enough
data, the prior is expected to have a small inﬂuence on inferences about θ.
This is examined in more detail in the following section.

330
7. The Prior Distribution and Bayesian Analysis
7.3
A Rapid Tour of Bayesian Asymptotics
Intuitively, the beliefs about a parameter θ, reﬂected in its posterior distri-
bution, should become more concentrated about the true parameter value
θ0 as the amount of information increases (loosely speaking, as the num-
ber of observations n →∞). In this section, a summary of some rele-
vant asymptotic results is presented, following Bernardo and Smith (1994)
closely. The arguments and results parallel those for asymptotic theory in
ML estimation, as given in Chapters 3 and 4. Hence, only the essentials
are presented.
7.3.1
Discrete Parameter
Suppose θ takes one of several mutually exclusive and exhaustive states, so
its prior distribution is discrete. The posterior distribution is
p (θi|y) =
p (y|θi) p (θi)

i
p (y|θi) p (θi),
where the sum is taken over all possible states of the parameter. Dividing
both numerator and denominator by the likelihood conferred by the data
to the true parameter value θ0, one gets
p (θi|y) =
p(y|θi)
p(y|θ0)p (θi)

i
p(y|θi)
p(y|θ0)p (θi)
.
Assuming the observations are independent, given the parameter, the pos-
terior is expressible as
p (θi|y) =
exp

log p(y|θi)
p(y|θ0) + log p (θi)


i
exp

log p(y|θi)
p(y|θ0) + log p (θi)

=
exp

n
j=1
log p(yj|θi)
p(yj|θ0) + log p (θi)


i
exp

n
j=1
log p(yj|θi)
p(yj|θ0) + log p (θi)
.
Now, as n →∞,
lim
n→∞
1
n
n

j=1
log p (yj|θi)
p (yj|θ0)
=

log p (yj|θi)
p (yj|θ0)p (yj|θ0) dy
=
−

log p (yj|θ0)
p (yj|θi) p (yj|θ0) dy.

7.3 A Rapid Tour of Bayesian Asymptotics
331
The integral immediately above is called the Kullback–Leibler distance or
the discrepancy between two distributions, which is shown in Section 7.4.7
to be 0 when θi = θ0 and positive otherwise. Hence, as n →∞,
1
n
n

j=1
log p (yj|θi)
p (yj|θ0) →−

log p (yj|θ0)
p (yj|θi) p (xj|θ0) dy
=

0,
for θi = θ0,
−∞,
otherwise.
Hence
lim
n→∞p (θi|y) = lim
n→∞











exp

n
j=1
log p(yj|θi)
p(yj|θ0) + log p (θi)


i
exp

n
j=1
log p(yj|θi)
p(yj|θ0) + log p (θi)












= 0,
for all θi ̸= θ0,
and is equal to 1 for θi = θ0. This indicates that, as sample size grows,
the posterior distribution becomes more and more concentrated around θ0
and that, in the limit, all probability mass is placed on the true value. It
can be shown that if one fails to assign positive prior probability to the
true state of the parameter, the limiting posterior concentrates around the
parameter value producing the smallest Kullback–Leibler discrepancy with
the true model (Bernardo and Smith, 1994).
7.3.2
Continuous Parameter
The posterior density of a parameter vector can be represented as
p (θ|y) ∝p (y|θ) p (θ) ∝exp [log p (y|θ) + log p (θ)] .
Let ,θ be the prior mode and let 5θn be the ML estimator based on a sam-
ple of size n. Taylor series expansions of the log-prior density and of the
likelihood function yield (recall that the gradient is null when evaluated at
the corresponding modal value):
log p (θ) ≈log p

,θ

−1
2

θ −,θ
′ ,H

θ −,θ

and
log p (y|θ) ≈log p

5θn

−1
2

θ −5θn
′ 5H

θ −5θn

,
where
,H = −∂2 log p (θ)
∂θ ∂θ′
""""
θ=θ

332
7. The Prior Distribution and Bayesian Analysis
and
5H = −∂2 log p (y|θ)
∂θ ∂θ′
""""
θ=θn
is the observed information matrix. These two matrices of second deriva-
tives, called Hessians, can be interpreted as the “precision” matrices of
the prior distribution and of the sampling model, respectively. Under the
usual regularity conditions, the remainder of the approximation to the log-
likelihood is small in large samples, since the likelihood is expected to be
sharp and concentrated near the true value of the parameter vector. Fur-
ther, recall that in large samples the ML estimate 5θn is expected to be
close to its true value θ0. Using the Taylor series approximations to the
prior and the likelihood, the posterior density is, roughly,
p (θ|y) ∝exp
9
log

p

5θn

p

,θ

−1
2

θ −5θn
′ 5H

θ −5θn

+

θ −,θ
′ ,H

θ −,θ
%
.
After retaining only the terms that involve θ one gets
p (θ|y) ∝exp

−1
2

θ −5θn
′ 5H

θ −5θn

+

θ −,θ
′ ,H

θ −,θ
%
.
(7.1)
The two quadratic forms on the parameter vector can be combined, via the
formulas used repeatedly in the preceding chapter, to obtain

θ −5θn
′ 5H

θ −5θn

+

θ −,θ
′ ,H

θ −,θ

=

θ −55θn
′ 
5H + ,H
 
θ −55θn

+

5θn −,θ
′ 5H

5H + ,H
−1 ,H

5θn −,θ

,
(7.2)
with
55θn =

5H + ,H
−1 
5H5θn + ,H,θ

.
(7.3)
Employing (7.2) in (7.1) and keeping only the terms involving θ gives
p (θ|y) ∝exp
#
−1
2

θ −55θn
′ 
5H + ,H
 
θ −55θn
$
.
(7.4)
Hence, under regularity conditions, the posterior distribution is asymptot-
ically normal with mean 55θn and covariance matrix

5H + ,H
−1
. We write
θ|y ∼N

55θn,

5H + ,H
−1
.
(7.5)

7.3 A Rapid Tour of Bayesian Asymptotics
333
Note that the approximation to the posterior mean is a matrix weighted
average of the prior mode and of the ML estimator, with the weights being
the corresponding precision matrices. The matrix sum 5H + ,H is a measure
of posterior precision and its inverse reﬂects the posterior variances and
covariances among elements of θ.
Another approximation can be obtained as follows. For large n, the pre-
cision matrix 5H will tend to be much larger than the prior precision matrix
,H. Then, roughly, 5H + ,H ≈5H and 55θn ≈5θn. Since the prior precision ma-
trix is “dominated” by the Hessian corresponding to the sampling model,
the ML estimator receives a much larger weight, so the mean of the ap-
proximate posterior distribution will be very close to the ML estimator.
Hence, for large n,
θ|y ∼N

5θn, 5H−1
.
(7.6)
For conditionally i.i.d. observations,
−∂2 log p (y|θ)
∂θ ∂θ′
= −
n

i=1
∂2 log p (yi|θ)
∂θ ∂θ′
= n

1
n
n

i=1
−∂2 log p (yi|θ)
∂θ ∂θ′

.
By the weak law of large numbers, the term in square brackets, which is
equal to 1
n 5H, converges in probability (symbolized “
p→”) to its expecta-
tion:
1
n
n

i=1
−∂2 log p (yi|θ)
∂θ∂θ′
p→E

−∂2 log p (y|θ)
∂θ∂θ′

= I1 (θ) ,
which is Fisher’s information measure for a sample of size 1. Hence,
5H
p→nI1 (θ) = I (θ) .
That is, the observed information matrix converges in probability to Fisher’s
information matrix. Then, an alternative approximation to the posterior
distribution is
θ|y ∼N

5θn, [nI1 (θ)]−1
.
It is important to stress that the heuristics presented above build on the
existence of regularity conditions. It is deﬁnitely not always the case that
the posterior distribution approaches normality as sample size increases.
We refer the reader to Bernardo and Smith (1994), and references therein,
for a discussion of this delicate subject.
Example 7.1
Asymptotic distribution of location parameters in a normal
model
Suppose that n observations are taken from some strain of ﬁsh and that
the sampling model for some attribute is
xi|θ1, σ2 ∼N

θ1, σ2
.

334
7. The Prior Distribution and Bayesian Analysis
The residual dispersion is known. The location parameter has an uncer-
tainty distribution according to the process θ1 ∼N (0, λ1) , where the vari-
ance is also known. Another random sample of n observations is drawn
from some other strain according to the model
yi|θ2, τ 2 ∼N

θ2, τ 2
,
with θ2 ∼N (0, λ2) ; again, λ2 is known. The joint posterior density of θ1
and θ2 is
p

θ1, θ2|σ,2 λ1, τ 2, λ2, x, y

∝
 n
-
i=1
p

xi|θ1, σ2
p (θ1|λ1)
  n
-
i=1
p

yi|θ2, τ 2
p (θ2|λ2)

.
It can be seen that θ1 and θ2 are independently distributed, a posteriori,
for any sample size. Since the prior and the posterior are normal, it follows
that the posterior distribution of each θ is normal as well. For example, the
posterior distribution of θ1 has mean
E

θ1|λ1, σ2, x

=

n + σ2
λ1
−1
nx
=

1 + σ2
nλ1
−1
x
and variance
V ar

θ1|λ1, σ2, x

=

n + σ2
λ1
−1
σ2.
As n →∞, the mean and variance tend to x and σ2/n, respectively. The
asymptotic joint posterior distribution of θ1 and θ2 can then be written as
θ1, θ2|σ2, τ 2, x, y ∼N


x
y

,

σ2
0
0
τ 2

n−1

.
The same result is arrived at by employing (7.6) since x and y are the
ML estimators of θ1 and θ2, respectively. In this example the observed and
expected information matrices are equal because the variances are assumed
to be known.
■
7.4
Statistical Information and Entropy
7.4.1
Information
A readable introduction to the concept of information theory in probability
is in Applebaum (1996), and some of the ideas are adapted to our context

7.4 Statistical Information and Entropy
335
hereinafter. Consider the following two statements:
(1) In two randomly mated, randomly selected lines of ﬁnite size derived
from the same population, mean values and gene frequencies diﬀer after 50
generations.
(2) A new mutant is found at generation 2 with frequency equal to 15%.
Arguably, the second statement conveys more information than the ﬁrst,
because it involves an event having a very low prior probability. The ﬁrst
statement should not evoke surprise, as the corresponding event is expected
in the light of well-established theory. Let E be an event and let p (E) be
its probability. Then using the above line of argument, the information
contained in the observed event, I (E) , should be a decreasing function of
its probability.
There are three conditions that an information measure must meet:
(1) It must be positive.
(2) The information from observing two events jointly must be at least as
large as that from the observation of any of the single elementary events.
For example, suppose there are two unlinked loci in a population in Hardy–
Weinberg equilibrium and that the probability of observing an AA genotype
is p (AA) , while that of observing Bb is p (Bb) . As the two events are
independent, one has that
p (AA ∩Bb) = p (AA) p (Bb) .
Since p (AA ∩Bb) ≤p (AA) and p (AA ∩Bb) ≤p (Bb) , it follows that the
information content I (AA ∩Bb) ≥I (AA) and, similarly, I (AA ∩Bb) ≥
I (Bb) .
(3) For independent events E1 and E2 (Applebaum, 1996):
I (E1 ∩E2) = I (E1) + I (E2) .
To illustrate, suppose a machine reads: “genotype at ﬁrst locus is AA”,
so the information is I (AA) , while another machine yields “genotype at
second locus is Bb”, with the information being I (Bb) . Hence, the infor-
mation is I (AA) + I (Bb) . On the other hand, if the machine reads both
genotypes simultaneously, we would not have information over and above
I (AA) + I (Bb) .
From information theory, the function satisfying the three conditions
given above must have the form
I (E) = −K loga [p (E)] ,
(7.7)
where K and a are positive constants. Since 0 ≤p (E) ≤1, it follows
that this information measure is positive, as K is positive, thus meeting
the ﬁrst condition. If the event is certain, I (E) = 0, and no information
is gained from knowing that the event took place (this would be known
beforehand). On the other hand, if the event is impossible, p (E) = 0, the

336
7. The Prior Distribution and Bayesian Analysis
information measure is not ﬁnite; this is viewed as reasonable, indicating
the impossibility of obtaining information from events that do not occur.
Second, for independent events
I (E1 ∩E2)
=
−K loga [p (E1 ∩E2)]
=
−K loga [p (E1)] −K loga [p (E2)]
=
I (E1) + I (E2) ,
satisfying the second and third conditions. Standard choices for the con-
stants are K = 1 and a = 2, and the units in which information is measured
are called “bits”. For example, suppose one crosses genotypes Aa and aa;
the oﬀspring can be either Aa or aa, with equal probability. Hence, the
information resulting from observing one of the two alternatives is:
I (Aa) = I (aa) = −log2

2−1
= 1 bit.
Example 7.2
Cross between double heterozygotes
Suppose that the cross AaBb × AaBb is made, with the two loci unlinked,
as before. We calculate the information accruing from observation of each
of the following events:
1.
progeny is aa;
2.
progeny is Bb;
3.
an oﬀspring aabb is observed;
4.
the oﬀspring is Aabb; and
5.
the outcome of the cross is AaBb.
Using Mendel’s rules:
1.
p (aa) = p (bb) = 1
4, so I (aa) = I (bb) = −log2

2−2
= 2 bits.
2.
p (Bb) = 1
2, so I (Bb) = −log2

2−1
= 1 bit.
3.
p (aabb) =
1
16, so I (aabb) = −log2

2−4
= 4 bits. Note that
I (aabb) = I (aa) + I (bb) since the two events are independent.
4.
p (Aabb) = 1
2
1
4 = 1
8 and I (Aabb) = −log2

2−3
= 3 bits.
5.
p (AaBb) = 1
4 and I (AaBb) = −log2

2−2
= 2 bits.
■
In the continuous case, heuristically, if one replaces “event” by “observed
data y”, the information measure becomes
I (y)
=
−log2 [p (y|θ)]
=
−
log p (y|θ)
log (2)

=
0.69315 −log p (y|θ) ,

7.4 Statistical Information and Entropy
337
where p (y|θ) is the density function indexed by parameter θ. Since 0.69315
is a constant, it can be dropped, as it is convenient to work with natural
logarithms (that is, any calculation of information should be increased by
0.69315 to have the “correct” number of bits). Further, for n data points
drawn independently from the same distribution, the information would be
I (y) = 0.69315 −
n

i=1
log p (yi|θ) .
(7.8)
For example, if yi ∼NIID

µ, σ2
, the information in a sample of size n
would be
I (y) = 0.69315 + n
2 log

2πσ2
+ 1
2
n

i=1

yi −µ
σ
2
,
so information is related to the “deviance” n
i=1
 yi−µ
σ
2 .
7.4.2
Entropy of a Discrete Distribution
Since the information content of an event depends on its probability (or
density), it is technically more sensible to think in terms of information
from the distribution of a random variable. It is important to keep this
in mind, because in Bayesian analysis sometimes one wishes to ﬁnd and
use a prior distribution conveying “as little information as possible” (Box
and Tiao, 1973; Zellner, 1971; Bernardo, 1979; Berger and Bernardo, 1992).
The formal development requires ﬁnding a distribution that minimizes some
information measure.
Consider one discrete random variable X having K mutually exclusive
and exhaustive states and probability distribution pi (i = 1, 2, . . . , K) (in
this section, Pr (X = xi) = p (xi), the usual notation for a p.m.f. is replaced
by pi). Since X is random, one does not know beforehand how much in-
formation will be contained in a yet-to-occur observation. Hence, I (X) is
random as well, but a property of the distribution is the mean information
H (p1, p2, ..., pK)
=
E [I (X)] = E {−log [Pr (X = xi)]}
=
−
K

i=1
pi log (pi) .
(7.9)
This is called the entropy of a distribution, with its name due to the fact
that this functional form appears in thermodynamics. In physics, entropy is
used to refer to the degree of randomness or disorder in processes. Shannon
(1948) coined the term information entropy to describe the tendency of
communication to become more and more distorted by noise. For example,
if some material is photocopied over again and in this process becomes illeg-
ible, the information is continually degraded. Thus, a distribution conveying

338
7. The Prior Distribution and Bayesian Analysis
minimum information, given some constraints that we wish this distribu-
tion to reﬂect (e.g., the probabilities must add up to 1), can be viewed as a
maximum entropy distribution. The role of entropy in Bayesian analysis in
connection with the elicitation of priors, is discussed later in this chapter.
The entropy function is not deﬁned when pi = 0, and the term pi log (pi)
is taken to be null in such a case (Applebaum, 1996). Since entropy involves
an average of numbers that must be at least 0, it follows that H (·) ≥
0, with the null value corresponding to the situation where there is no
uncertainty whatsoever (so no information is gained). It follows, then, that
a situation of “maximum entropy” is one where “a lot” of information is to
be gained from observation. Entropy is a measure of the prior uncertainty
of a random experiment associated with the probability distribution in
question or, alternatively, of the information gained when the outcome is
observed (Baldi and Brunak, 1998). If one is told about the outcome of
an event, the entropy is reduced from H (·) to 0, so this measures the
gain in information. In some sense the concept is counterintuitive (Durbin
et al., 1998) because the more randomness, the higher the entropy and
the information. The concept becomes clearer when one thinks in terms
of a reduction of entropy after some information is received. Hence, what
matters is the diﬀerence in entropy before and after observing data; this
diﬀerence can be interpreted as the informational content provided by a
set of observations. For example, one may wish to compute the diﬀerence
in entropy between the prior and posterior distributions.
Example 7.3
Entropy of a random DNA
This is from Durbin et al. (1998): if each base (A, C, G, T) occurs equiprob-
ably within a DNA sequence, the probability of a random base is 1
4. The
entropy per base is then
−
4

i=1
1
4 log2

1
4

= 2 bits.
Durbin et al. (1998) state that this can be interpreted as the number of
binary questions (with yes or no responses) needed to discover the out-
come. For example, the ﬁrst question would be: Is the basis a purine or a
pyrimidine?. If the answer is “purine”, the choice must be between A or G.
The second question is: Which is the speciﬁc base? The more uncertainty
there is, the more questions are needed to discover the outcome.
■
Example 7.4
Entropy of a conserved position
Suppose that a DNA sequence is expected to be random so that, before
observation, the entropy is 2 bits, as in the preceding example of Durbin
et al. (1998). It is observed that in a given position the frequency of A is
0.7, whereas that of G is 0.3. Using (7.9), the entropy after observation is

7.4 Statistical Information and Entropy
339
then
−
 7
10 log2
7
10 + 3
10 log2
3
10

= 0.88 bits.
The information content of the position is given by the diﬀerence in entropy
before and after observation: 2 −0.88 = 1.12 bits. The more conserved the
position, the higher its information content is.
■
Example 7.5
Sampling of genes
Let allele A have frequency p in some population, so that all other possible
alleles appear with probability 1 −p. Suppose that n alleles are drawn at
random and that x are of the A form. The process is binomial and the
entropy of the distribution of the random variable X is
H (p) = −
n

x=0

log n! px (1 −p)n−x
x! (n −x)!

n! px (1 −p)n−x
x! (n −x)!
.
If n = 1, this reduces to the entropy of a Bernoulli distribution
H (p) = −p log (p) −(1 −p) log (1 −p) .
Taking derivatives with respect to p, to ﬁnd the maximum value of the
entropy for this distribution,
dH (p)
dp
= −log (p) + log (1 −p) .
Setting to 0 and solving gives p = 1
2 as the gene frequency giving maximum
entropy, with the maximized entropy being equal to 1 bit when expressed
in a log2 base. Hence the gene frequency distribution producing maximum
entropy is that corresponding to the situation where allele A has the same
frequency as that of all the other alleles combined (but without making a
distinction between these).
Now consider the situation where there are three alleles with frequencies
p1, p2, and p3 = 1−p1 −p2. The entropy of the gene frequency distribution
is now
H (p1, p2) = −p1 log (p1) −p2 log (p2) −(1 −p1 −p2) log (1 −p1 −p2) .
To ﬁnd the maximum entropy distribution we calculate the gradients
dH (p1, p2)
dpi
= −log (pi) + log (1 −p1 −p2) ,
i = 1, 2.
Setting these derivatives to 0, one arrives at p1 = p2 = 1
3. Again, the max-
imum entropy distribution is one where the three alleles are equally likely.

340
7. The Prior Distribution and Bayesian Analysis
In general, for K states, the entropy in (7.9) has the following gradient,
after introducing the constraint that K
i=1 pi = 1,
dH (p1, p2, ..., pK−1)
dpi
= −log (pi) + log (1 −p1 −p2 −· · · −pK−1) .
After setting to 0, one obtains the solution pi = 1/K for all alleles. Hence,
the maximum entropy distribution is uniform and the maximized entropy
is equal to
H

 1
K , 1
K , ..., 1
K

= −
K

i=1
1
K log

 1
K

= log (K) .
■
The preceding example illustrates that entropy is a measure of uncer-
tainty. The entropy is null when there is complete certainty about the al-
lele to be sampled, and it is maximum when one cannot make an informed
choice about which of the allelic states is more likely. Further, for a num-
ber of states K < M, log (K) < log (M) , which implies that the entropy
(uncertainty) grows monotonically with the number of choices that can be
made.
7.4.3
Entropy of a Joint and Conditional Distribution
Suppose that the pair of random variables (X, Y ) has a joint distribution
with joint probabilities pxy, (x = 1, 2, . . . , m, y = 1, 2, . . . , n) . The entropy
of the joint distribution (Applebaum, 1996) is
H (p11, p12, . . . , pmn) = −
m

x=1
n

y=1
pxy log (pxy)
= −
m

x=1
n

y=1
pxpy|x log

pxpy|x

.
(7.10)
Further
H (p11, p12, . . . , pmn) = −
m

x=1
n

y=1
pxpy|x

log (px) + log

py|x

= −
m

x=1
px log (px)
n

y=1
py|x −
m

x=1
 n

y=1
py|x log

py|x


px
= H (px) +
m

x=1
H

py|x

px = H (px) + H

py|x

,
(7.11)

7.4 Statistical Information and Entropy
341
where px is the vector of probabilities of the marginal distribution of X and
py|x is the vector of probabilities of the conditional distribution of Y given
X. This indicates that the entropy of the joint distribution is the sum of two
components: the entropy of the distribution of X and the average H

py|x

of the conditional entropies H

py|x

, taken with respect to the distribution
of X. The second term provides a measure of the uncertainty about Y
knowing that X has been realized, but without being able to state what its
value is; on the other hand, H

py|x

measures the uncertainty when one
knows the value taken by X. Note that if X and Y are independent
H (p11, p12, . . . , pmn) = −
m

x=1
n

y=1
pxpy [log (px) + log (py)]
= −
m

x=1
px log (px)
n

y=1
py −
n

y=1
py log (py)
m

x=1
px
= H (px) + H (py) ,
(7.12)
where py is the vector of probabilities of the distribution of Y.
7.4.4
Entropy of a Continuous Distribution
The entropy of a continuous distribution (Shannon, 1948; Jaynes, 1957) is
deﬁned to be
H [p (y|θ)] = −

· · ·

log [p (y|θ)] p (y|θ) dy,
(7.13)
where p (y|θ) is the density function of the random vector y, indexed by
a parameter θ. Note that entropy is not invariant under transformation.
Suppose the random variable is scalar, and that one considers the one-to-
one change of variables z = f (y), where z increases monotonically with y.
Then
p (z|θ) = p

f −1 (z) |θ
 df −1 (z)
dz
.
The entropy of the distribution of z becomes
H [p (z|θ)] = −

log

p

f −1 (z) |θ
 df −1 (z)
dz
%
p

f −1 (z) |θ
 df −1 (z)
dz
dz
= −

log

p

f −1 (z) |θ

p

f −1 (z) |θ
 df −1 (z)
dz
dz
−

log
df −1 (z)
dz

p

f −1 (z) |θ
 df −1 (z)
dz
dz
= −

{log [p (y|θ)]} p (y|θ) dy −

log
df −1 (z)
dz

p (z|θ) dz

342
7. The Prior Distribution and Bayesian Analysis
= H [p (y|θ)] −Ez

log
df −1 (z)
dz
%
.
This indicates that the information content is not invariant even under a
one-to-one, monotonic, transformation, putting in question the usefulness
of entropy as a measure of uncertainty in the continuous case (Bernardo and
Smith, 1994). In Section 7.4.7 it is shown that a transformation invariant
measure is provided by the relative entropy.
Example 7.6
Entropy of a uniform distribution
Suppose Y is a scalar variable distributed uniformly in the interval (a, b) ,
so its density is 1/ (b −a). Using (7.13) yields
H [p (y|a, b)]
=
−
b

a

log

1
b −a

1
b −ady
=
log (b −a) .
When dealing with discrete random variables, it was pointed out earlier
that entropy is at least null. This does not always carry to the continuous
case (Applebaum, 1996). For example, note that if the diﬀerence between
the bounds b −a is < 1, then the entropy would be negative.
■
Example 7.7
Entropy of a normal distribution
Let now Y ∼N

µ, σ2
, so
H

p

y|µ, σ2
= −
 #
log

1
√
2πσ2 exp

−(y −µ)2
2σ2
$
×
1
√
2πσ2 exp

−(y −µ)2
2σ2

dy
= −log

1
√
2πσ2
 
1
√
2πσ2 exp

−(y −µ)2
2σ2

dy
+
1
2σ2

(y −µ)2
1
√
2πσ2 exp

−(y −µ)2
2σ2

dy
= 1
2

1 + log

2πσ2
.
Note that entropy increases with σ2. Consider now the standardized multi-
variate normal distribution y ∼Nn (0, R) , where R is a correlation matrix

7.4 Statistical Information and Entropy
343
having all oﬀ-diagonal elements equal to ρ. Then
H [p (y|0, R)] = −
 #
log

1
(2π)
n
2 |R|
1
2 exp −

y′R−1y
2
$
×
1
(2π)
n
2 |R|
1
2 exp −

y′R−1y
2

dy
= 1
2 [log (2π)n + log |R|] + E

y′R−1y
2

= 1
2

log (2π)n + log |R| + trR−1V ar (y)

= 1
2 [log (2π)n + log |R|+n] .
Now, using results in Searle et al. (1992),
|R| = (1 −ρ) [1 + (n −1) ρ] ,
and employing this in the preceding yields
H [p (y|0, R)] = 1
2 {log (2π)n + log (1 −ρ) + log [1 + (n −1) ρ] + n} .
When ρ = 0, the entropy is equal to
H [p (y|0, R)] = n
2 [1 + log (2π)] ,
which is n times larger than the entropy of a univariate standard normal
distribution.
■
Example 7.8
Entropy in a truncated normal model
Consider calculating the entropies of the prior, likelihood, and posterior
distributions in a model where the mean of a normal distribution, µ, is
to be inferred; the variance is known. Suppose that n observations are
collected and that these are i.i.d. as N

µ, σ2
. The p.d.f. of an observation
is
p

yj|µ, σ2
=
1
√
2πσ2 exp

−1
2σ2 (yj −µ)2

,
where j = 1, 2, . . . , n identiﬁes the data point. Assume that the prior dis-
tribution of µ is uniform between boundaries a and b. As in Example 7.6,
the entropy of the prior distribution with density p0 is
H (p0) = −

log (po) po dµ = log (b −a) .
Thus, the prior entropy increases with the distance between boundaries a
and b. This means that the prior uncertainty about the values of µ increases
as the boundaries are further apart.

344
7. The Prior Distribution and Bayesian Analysis
The entropy of the likelihood for a sample of size n is
H

p

y|µ, σ2
= −

· · ·

log

p

y|µ, σ2
p

y|µ, σ2
dy
= −

· · ·

log
#

2πσ2−n
2 exp

−1
2σ2
n

i=1
(yi −µ)2
$
p

y|µ, σ2
dy
= n
2

1 + log

2πσ2
.
Hence, the entropy of the likelihood increases as sample size increases. This
is because the joint density becomes smaller and smaller, so one gets more
“surprised”, i.e., more information accrues, as more data are observed.
The density of the posterior distribution of µ is
p

µ|σ2, a, b, y

∝exp

−n (µ −y)2
2σ2

I (a < µ < b) ,
where y is the mean of all observations and I (·) is an indicator variable
denoting the region where the parameter is allowed to take density. Hence,
the posterior distribution is a truncated normal process between a and b.
In the absence of truncation
µ|a, b, y ∼N

y, σ2/n

.
Now the normalized posterior density is
p

µ|σ2, a, b, y

=
exp

−n(µ−y)2
2σ2

b
a
exp

−n(µ−y)2
2σ2

dµ
=
exp

−n(µ−y)2
2σ2

>
2πσ2/n

Φ

b−y
σ/√n

−Φ

a−y
σ/√n
,
(7.14)
with the integration constant being
cn =
>
2πσ2/n

Φ

 b −y
σ/√n

−Φ

 a −y
σ/√n
%−1
.
(7.15)
If b = ∞, so that the posterior takes nonnull density only between a and
∞, standard results from truncation selection in quantitative genetics (e.g.,
Falconer and Mackay, 1996) give
E

µ|σ2, a, b, y

= y +
φ

a−y
σ/√n

1 −Φ

a−y
σ/√n
 σ
√n = η,

7.4 Statistical Information and Entropy
345
and
V ar

µ|σ2, a, b, y

= σ2
n


1 −
φ

a−y
σ/√n

1 −Φ

a−y
σ/√n



φ

a−y
σ/√n

1 −Φ

a−y
σ/√n
 −a −y
σ/√n




= γ,
where φ (·) and Φ (·) are the standard normal density and distribution
functions, respectively.
The entropy of the truncated normal posterior distribution is then
H

p

µ|σ2, a, b, y

= −
b

a
log

p

µ|σ2, a, b, y

p

µ|σ2, a, b, y

dµ
=
n
2σ2
b

a
(µ −y)2 p

µ|σ2, a, b, y

dµ
+ log
>
2πσ2/n

Φ

 b −y
σ/√n

−Φ

 a −y
σ/√n
%
.
The expectation to be computed can be written as
Eµ|σ2,a,b,y (µ −y)2 =
b

a
(µ −y)2 p

µ|σ2, a, b, y

dµ
=
b

a
[(µ −η) + (η −y)]2 p

µ|σ2, a, b, y

dµ
=
b

a
(µ −η)2 p

µ|σ2, a, b, y

dµ + (η −y)2 = γ + (η −y)2 ,
(7.16)
with the last term resulting because the expected value of µ −η is 0 under
the posterior distribution. Recall that η and γ are the mean and variance
of the posterior distribution, respectively. Now, using this in the last ex-
pression for entropy given above, it turns out that
H

p

µ|σ2, a, b, y

=
n

γ + (η −y)2
2σ2
+ log
>
2πσ2/n + log

Φ

 b −y
σ/√n

−Φ

 a −y
σ/√n

.
The entropy of the posterior density goes to 0 as sample size goes to inﬁnity.
Algebraically, the ﬁrst and second terms go to ∞and −∞as n →∞, so

346
7. The Prior Distribution and Bayesian Analysis
they cancel out. As n →∞, the third term goes to 0 because the posterior
distribution becomes a point mass in the limit, with all density assigned
to the “true” value of µ. In the limit, there is no longer “surprise”, as the
true value of µ is known with probability equal to 1.
■
7.4.5
Information about a Parameter
The concept of amount of information about a parameter provided by a
sample of observations was discussed earlier in the book in connection
with likelihood inference. However, the treatment presented was heuristic,
without making use of information-theoretic arguments. Important contri-
butions to the use of information theory in statistics are Fisher (1925),
Shannon (1948), and Kullback (1968). Here an elementary introduction
to the subject is provided, following closely some of the developments in
Kullback (1968).
Kullback’s Information Measure
Suppose that an observation y is made and that one wishes to evaluate two
competing models or hypotheses Hi (i = 1, 2), each having prior probability
p (Hi) . For example, one of the hypotheses could be that the observation
belongs to some distribution. The posterior probability of hypothesis 1
being true is
p (H1|y) =
p (y|H1) p (H1)
p (y|H1) p (H1) + p (y|H2) p (H2),
where p (y|Hi) is the density of the observation under hypothesis Hi and
p (Hi) is the prior probability of Hi being true. The logarithm of the ratio
of posterior probabilities, or posterior log-odds ratio, is then
log p (H1|y)
p (H2|y) = log p (H1)
p (H2) + log p (y|H1)
p (y|H2),
where the ratio of densities is often known as the Bayes factor in favor of
hypothesis 1 relative to hypothesis 2. Rearranging the preceding expression
log p (y|H1)
p (y|H2) = log p (H1|y)
p (H2|y) −log p (H1)
p (H2).
(7.17)
The diﬀerence between posterior and prior log-odds ratios was interpreted
by Kullback (1968) as the information contained in y for discrimination in
favor of H1 against H2. The diﬀerence can be either negative or positive.

7.4 Statistical Information and Entropy
347
The expected information for discrimination per observation from H1 is
I (1 : 2) =

log p (y|H1)
p (y|H2)p (y|H1) dy
=
 
log p (H1|y)
p (H2|y) −log p (H1)
p (H2)

p (y|H1) dy
=
 
log p (H1|y)
p (H2|y)

p (y|H1) dy −log p (H1)
p (H2).
(7.18)
This is the diﬀerence between the mean value (taken with respect to the
distribution [y|H1]) of the logarithm of the posterior odds ratio and of the
prior log-odds ratio. Similarly, the expected information per observation
for discrimination in favor of H2 is
I (2 : 1) =

log p (y|H2)
p (y|H1)p (y|H2) dy.
(7.19)
Kullback (1968) deﬁnes the “divergence” between hypotheses (or distribu-
tions) as
J (1 : 2) = I (1 : 2) + I (2 : 1)
=
 
log p (H1|y)
p (H2|y)

p (y|H1) dy −log p (H1)
p (H2)+
 
log p (H2|y)
p (H1|y)

p (y|H2) dy −log p (H2)
p (H1)
=
 
log p (H1|y)
p (H2|y)

p (y|H1) dy +
 
log p (H2|y)
p (H1|y)

p (y|H2) dy.
(7.20)
This measure has most of the properties of a distance (Kullback, 1968).
All preceding deﬁnitions generalize naturally to the situation where the
observation is a vector, instead of a scalar.
Example 7.9
Correlation versus independence under normality assump-
tions
Suppose there are two random variables having distributions X ∼N

0, σ2
X

and Y ∼N

0, σ2
Y

. Let H1 be the hypothesis that the variables have a
joint normal distribution with correlation ρ, whereas H2 will pose that their
distributions are independent. Now let p (x, y) be the bivariate normal den-
sity and let g (x) and h (y) be the corresponding marginal densities. Then,
using the ﬁrst representation leading to (7.18),
I (1 : 2) =
 
log
 p (x, y)
g (x) h (y)

p (x, y) dx dy
= EH1 {log [p (x, y)]} −EH1 {log [g (x)]} −EH1 {log [h (y)]} .
(7.21)

348
7. The Prior Distribution and Bayesian Analysis
Now the form of the bivariate normal density gives
log [p (x, y)] = −log (2πσXσY ) −1
2 log

1 −ρ2
−
1
2 (1 −ρ2)

 x2
σ2
X
+ y2
σ2
Y
−2ρ
xy
σXσY

.
Taking expectations under H1:
EH1 {log [p (x, y)]}
= −log (2πσXσY ) −1
2 log

1 −ρ2
−
1
2 (1 −ρ2)

2 −2ρ2
= −log (2πσXσY ) −1
2 log

1 −ρ2
−1.
(7.22)
Likewise,
EH1 {log [g (x)]}
=
EH1

−log

σX
√
2π

−x2
2σ2
X

=
−log

σX
√
2π

−1
2,
(7.23)
and
EH1 {log [h (y)]} = −log

σY
√
2π

−1
2.
(7.24)
Collecting (7.22) to (7.24) in (7.21)
I (1 : 2) = −1
2 log

1 −ρ2
.
(7.25)
Hence, the expected information per observation for discrimination is a
function of the correlation coeﬃcient. Its value ranges from 0 (when the
correlation is equal to 0) to ∞when its absolute value is 1.
Similarly,
I (2 : 1) =
 
log
g (x) h (y)
p (x, y)

g (x) h (y) dx dy
= EH2 {log [g (x)]} + EH2 {log [h (y)]} −EH2 {log [p (x, y)]} .
(7.26)
Now
EH2 {log [g (x)]} = −log

σX
√
2π

−1
2,
EH2 {log [h (y)]} = −log

σY
√
2π

−1
2,

7.4 Statistical Information and Entropy
349
and
EH2 {log [p (x, y)]} = −log (2πσXσY ) −1
2 log

1 −ρ2
−
1
2 (1 −ρ2)EH2

 x2
σ2
X
+ y2
σ2
Y
−2ρ
xy
σXσY

= −log (2πσXσy) −1
2 log

1 −ρ2
−
1
(1 −ρ2).
Hence,
I (2 : 1) = 1
2 log

1 −ρ2
+
1
(1 −ρ2) −1.
(7.27)
The mean information per observation for discrimination in favor of the
independence hypothesis ranges from 0, when the correlation is null, to ∞
when ρ = 1. The divergence measure in (7.20) is, thus,
J (1 : 2)
=
I (1 : 2) + I (2 : 1)
=
ρ2
(1 −ρ2)
(7.28)
which ranges from 0 to ∞as well.
■
Example 7.10
Comparing hypotheses
Following Kullback (1968), suppose that H2 is a set of mutually exclusive
and exhaustive hypotheses, one of which must be true, and that hypothesis
H1 is a member of such a set. We wish to calculate the information in y in
favor of H1. As stated, the problem implies necessarily that
p (H2) = p (H2|y) = 1,
this being so because the event “H2 is true” is certain, a priori, so it must
be certain a posteriori as well. Using this in (7.17) gives
log p (y|H1)
p (y|H2) = log p (H1|y) −log p (H1) .
If the observation y “proves” that H1 is true, i.e., that p (H1| y) = 1, then
the information in y about H1 is
log p (y|H1)
p (y|H2) = −log p (H1) .
This implies that if the prior probability of H1 is small, the information
resulting from its veriﬁcation is large. On the other hand, if the prior prob-
ability is large, the information is small. This is reasonable on the intuitive
grounds that much is learned if an implausible proposition is found to be
true, as noted in Subsection 7.4.1.

350
7. The Prior Distribution and Bayesian Analysis
If all possible hypotheses are H1, H2, . . . , Hn, the information in y about
Hi would be −log p (Hi) . The mean value (taken over the prior distribution
of the hypotheses) is
H = −
n

i=1
[log p (Hi)] p (Hi) ,
which is the entropy of the distribution of the hypotheses.
■
Example 7.11
Information provided by an experiment
Let θ be a parameter vector having some prior distribution with density
h (θ) . Take g (y|θ) to be the density of the data vector y under the sampling
model posed, and take g (y) as the marginal density of the observations,
that is, the average of the density of the sampling model over the prior
distribution of θ; let h (θ|y) be the resulting posterior density. Put
H1 = θ and y have a joint distribution with density f (θ, y) ,
in which case the data have something to say about the parameters, and
H2 = θ and y are independent.
Using (7.21), the expected information per observation for discrimination
in favor of H1 is
I (1 : 2) =
 
log
 f (θ, y)
h (θ) g (y)

f (θ, y) dθ dy
=
 
log
h (θ|y)
h (θ)

h (θ|y) dθ
%
g (y)dy.
(7.29)
This measure was termed “information about a parameter provided by an
experiment” by Lindley (1956). Further,
I (1 : 2) =
 
[log h (θ|y) −log h (θ)] h (θ|y) dθ
%
g (y)dy
=
 
[log h (θ|y)] h (θ|y) dθ
%
g (y)dy
−
 
[log h (θ)] h (θ|y) g (y)dθdy
=
 
[log h (θ|y)] h (θ|y) dθ
%
g (y)dy
−

[log h (θ)] h (θ)

g (y|θ)dydθ.
(7.30)

7.4 Statistical Information and Entropy
351
In the preceding expression, note that

g (y|θ)dy = 1. Further, recall that
−

log h (θ) h (θ) dθ = H (θ)
is the prior entropy, and that
−

log h (θ|y) h (θ|y) dθ = H (θ|y)
is the posterior entropy. Using these deﬁnitions in (7.30), the information
about θ in the experiment can be written as
I (1 : 2) = H (θ) −Ey [H (θ|y)] ,
(7.31)
where the second term is the average of the posterior entropy over all pos-
sible values that the data can take, should the experiment be repeated
an inﬁnite number of times. It follows that the information in an experi-
ment can be interpreted as the decrease in entropy stemming from having
observed data.
■
7.4.6
Fisher’s Information Revisited
It will be shown here that Fisher’s information measure (see Chapter 3) can
be derived employing the concepts of mean information for discrimination
and of divergence proposed by Kullback (1968). Although only the case
of a single parameter will be discussed, the developments extend to the
multiparameter situation in a straightforward manner.
Suppose that θ and θ+∆θ are neighboring points in the parameter space.
Now consider the mean information measure given in the ﬁrst line of (7.18),
and put
I (θ : θ + ∆θ) =
 
log
p (y|θ)
p (y|θ + ∆θ)

p (y|θ) dy
(7.32)
and
I (θ + ∆θ : θ) =
 
log p (y|θ + ∆θ)
p (y|θ)

p (y|θ + ∆θ) dy.
(7.33)
The divergence, as deﬁned in (7.20), is
J (θ : θ + ∆θ) = I (θ : θ + ∆θ) + I (θ + ∆θ : θ)
=
 
log
p (y|θ)
p (y|θ + ∆θ)

[p (y|θ) −p (y|θ + ∆θ)] dy.
(7.34)
Recall that Fisher’s measure of information about θ (here, we shall use
the notation Inf (θ) , to avoid confusion with the notation for mean dis-

352
7. The Prior Distribution and Bayesian Analysis
crimination) is
Inf (θ) = Ey|θ
d log p (y|θ)
dθ
2
=
 
1
p (y|θ)
dp (y|θ)
dθ
2
p (y|θ) dy.
(7.35)
Using a Taylor series, now expand the logarithm of p (y|θ + ∆θ) about θ
as follows:
log p (y|θ + ∆θ) ≈log p (y|θ) + d log p (y|θ)
dθ
∆θ
+1
2
d2 log p (y|θ)
(dθ)2
(∆θ)2 + 1
6
d3 log p (y|θ)
(dθ)3
(∆θ)3 .
Hence
log p (y|θ) −log p (y|θ + ∆θ)
≈−

d log p (y|θ)
dθ
∆θ + d2 log p (y|θ)
2 (dθ)2
(∆θ)2 + d3 log p (y|θ)
6 (dθ)3
(∆θ)3

.
(7.36)
Using this, (7.32) can now be written as
I (θ : θ + ∆θ) =
 
log
p (y|θ)
p (y|θ + ∆θ)

p (y|θ) dy
≈−∆θ
 d log p (y|θ)
dθ
p (y|θ) dy −(∆θ)2
2
 d2 log p (y|θ)
(dθ)2
p (y|θ) dy
−(∆θ)3
6
 d3 log p (y|θ)
(dθ)3
p (y|θ) dy.
Under regularity conditions, as seen in Chapter 3, the ﬁrst term in the pre-
ceding expression vanishes because the expected value of the score of the
log-likelihood is 0. The quadratic term involves the expected value of the
second derivatives of the log-likelihood, or the negative of Fisher’s informa-
tion measure. Further, the cubic term can be neglected if the expansion is
up to second-order (provided that the expected value of the third deriva-
tives is bounded). Then, up to second-order,
I (θ : θ + ∆θ)
≈
−(∆θ)2
2
 d2 log p (y|θ)
(dθ)2
p (y|θ) dy
=
(∆θ)2 Inf (θ)
2
.
(7.37)

7.4 Statistical Information and Entropy
353
Hence, the mean information for discrimination in favor of θ is propor-
tional to Fisher’s information measure and to the diﬀerence between the
neighboring values of the parameter.
Now consider the divergence in (7.34)
J (θ : θ + ∆θ) =
 
log
p (y|θ)
p (y|θ + ∆θ)

[p (y|θ) −p (y|θ + ∆θ)] dy
=
 
log p (y|θ + ∆θ)
p (y|θ)
 p (y|θ + ∆θ) −p (y|θ)
p (y|θ)

p (y|θ) dy.
(7.38)
Now
log p (y|θ + ∆θ)
p (y|θ)
= log

1 + p (y|θ + ∆θ) −p (y|θ)
p (y|θ)

≈p (y|θ + ∆θ) −p (y|θ)
p (y|θ)
,
with this result following from an expansion of log (1 + x) about x = 0.
Here the role of x is played by the relative diﬀerence between densities
at the neighboring points, which is near zero, by assumption. Using the
preceding result in (7.38)
J (θ : θ + ∆θ) ≈
 p (y|θ + ∆θ) −p (y|θ)
p (y|θ)
2
p (y|θ) dy.
(7.39)
An additional approximation results from noting that, by deﬁnition,
dp (y|θ)
dθ
= p (y|θ + ∆θ) −p (y|θ)
∆θ
,
as ∆θ →0. Making use of this in (7.39)
J (θ : θ + ∆θ) ≈
 
1
p (y|θ)
dp (y|θ)
dθ
∆θ
2
p (y|θ) dy
= (∆θ)2
 d log p (y|θ)
dθ
2
p (y|θ) dy = (∆θ)2 Inf (θ) .
(7.40)
Therefore the discrepancy is proportional to the square of the diﬀerence
between the neighboring points and to Fisher’s information measure.
7.4.7
Prior and Posterior Discrepancy
It is instructive to evaluate how much information is gained (equivalently,
how much the posterior diﬀers from the prior) as the process of Bayesian
learning proceeds. A measure of “distance” between the prior and posterior
distributions is given by the Kullback–Leibler discrepancy between the two

354
7. The Prior Distribution and Bayesian Analysis
corresponding densities. Letting p0 (θ) and p1 (θ) be the prior and posterior
densities, respectively, the discrepancy is (O’Hagan, 1994)
D (p0, p1)
=

· · ·
 
log p1
p0

p1 dθ
=
E

log p1
p0

(7.41)
=
−E

log p0
p1

,
(7.42)
with the expectation taken over the posterior distribution. Note that the
Kullback–Leibler distance can be viewed as a relative entropy. This has
two advantages over absolute entropy. First, as shown below, this relative
entropy is always at least zero, contrary to the plain entropy of a continuous
distribution; see Example 7.6 for an illustration of this problem. Second,
relative entropy is invariant under transformation (Jaynes, 1994). Note that
a change of variables from θ to λ (it suﬃces to consider the scalar situation
to see that this holds true), with p∗
1 and p∗
0 representing the new densities,
gives
D (p∗
0, p∗
1)
=
−

log

p1 dθ
dλ
p0 dθ
dλ

p1
dθ
dλdλ
=
D (p0, p1) ,
since the diﬀerentials “cancel out”.
As noted above, the discrepancy is greater than or equal to 0, being null
only when the data do not contribute any information about the parameter,
as p1 = p0 in this case. In order to show that D (·) is at least 0, recall
Jensen’s inequality (Subsubsection 3.7.1 in Chapter 3), stating that for a
convex function g,
g [E(X)] ≤E [g(X)] .
In our context, for g being the logarithmic function, this yields
log E

p1
p0

≤E

log p1
p0

.
Also,
log E

p0
p1

≤E

log p0
p1

.
Now
log E

p0
p1

= log

· · ·
 p0
p1
p1dθ

= log

· · ·

p0 dθ

= log (1) = 0,

7.4 Statistical Information and Entropy
355
provided the prior is proper, so it integrates to 1. Hence, the Kullback–
Leibler discrepancy is null or positive.
When the observations are conditionally independent and sample size is
n, the Kullback–Leibler distance can be expressed as:
D (p0, p1) =

· · ·
 
log p1
p0

p1 dθ
=

· · ·


log
cng (θ)
n7
i=1
p (yi|θ)
g (θ)

p1 dθ
= log (cn) +

· · ·
 
log
n
-
i=1
p (yi|θ)

p1 dθ
= log (cn) +
n

i=1
E [log p (yi|θ)] ,
(7.43)
where p0 = g (θ) is the prior density and cn is the integration constant of
the posterior density based on n observations. Recall that the expectation
in (7.43) is taken over the posterior distribution.
Example 7.12
Kullback–Leibler distance in a truncated normal model
Consider again the truncated normal model in Example 7.8. Then, from
(7.43),
D (p0, p1) = log (cn) −n log

2πσ2
2
−
1
2σ2
n

i=1
E (yi −µ)2 ,
(7.44)
with the expectation taken over the posterior distribution of µ. From (7.16),
in the truncated normal model, one has
E (yi −µ)2 = γ + (η −yi)2 .
Using this in (7.44), the Kullback–Leibler discrepancy is
D (p0, p1) = log (cn) −n log

2πσ2
2
−
nγ +
n
i=1
(η −yi)2
2σ2
= log (cn) −n log

2πσ2
2
−
n

γ + (η −y)2
+
n
i=1
(yi −y)2
2σ2
.
Making use of the integration constant given in (7.15), and with b = ∞, the
distance between the prior and the posterior distributions can be expressed

356
7. The Prior Distribution and Bayesian Analysis
as
D (p0, p1) = −1
2 log

2πσ2/n

−log

1 −Φ

 a −y
σ/√n

−1
2n log

2πσ2
−
1
2σ2
#
n

γ + (η −y)2
+
n

i=1
(yi −y)2
$
.
■
7.5
Priors Conveying Little Information
One of the criticisms often made of Bayesian inference is the potential ef-
fect that a possibly subjective, arbitrary or misguided prior can have on
inferences. Hence, eﬀorts have been directed at arriving at “objective pri-
ors”, by this meaning prior distributions that say “little” relative to the
contributions made by the data (Jeﬀreys, 1961; Box and Tiao, 1973; Zell-
ner, 1971). It has been seen already that the eﬀect of the prior dissipates
as sample size increases, so the problem is essentially one aﬀecting ﬁnite
sample inferences. Further, when the observations in the sample are corre-
lated or when the model involves many parameters, it is not always clear
how large the sample should be for any inﬂuences of the prior to be over-
whelmed by the data. The problem of ﬁnding objective or noninformative
priors is an extremely diﬃcult one, and consensus seems to be lacking be-
tween researchers that have worked in this area. Here we will present a
short review of some of the approaches that have been suggested. For addi-
tional detail, see Bernardo (1979), Berger and Bernardo (1992), Bernardo
and Smith (1994), O’Hagan (1994), and Leonard and Hsu (1999).
7.5.1
The Uniform Prior
The most widely used (and abused) “noninformative” prior is that based
on the Bayes-Laplace “principle of insuﬃcient reason”. This states that,
in the absence of evidence to the contrary, all possibilities should have the
same prior probability (e.g., Bernardo and Smith, 1994). For example, if θ
takes one of K possible values, the noninformative prior indicated by this
principle is the uniform distribution
 1
K , 1
K , . . . , 1
K
%
.
As noted in Example 7.5 this is also a maximum entropy distribution when
all that is known is that there are K mutually exclusive and exhaustive
states.

7.5 Priors Conveying Little Information
357
In the continuous case the counterpart is the continuous uniform dis-
tribution, but this leads to inconsistencies. Suppose that θ is assigned a
uniform prior distribution to convey lack of knowledge about the values
of this parameter. Then, the density of the distribution of a parameter
resulting from a monotone transformation λ = f (θ) is
p (λ)
=
p

f −1 (λ)
 """"
f −1 (λ)
dλ
""""
∝
""""
f −1 (λ)
dλ
"""" .
If the transformation is linear, the Jacobian is a constant, so it follows that
the density of λ is uniform as well. On the other hand, if the transformation
is nonlinear, the density varies with λ. This implies that if one claims
ignorance with respect to θ, the same cannot be said about λ. This is a
severe inconsistency.
Example 7.13
The improper uniform prior as a limiting case of the nor-
mal distribution
Let the prior be θ ∼N

µθ, σ2
θ

. If σ2
θ is very small, this implies that the
values are concentrated around µθ or, equivalently, sharp prior knowledge.
On the other hand, if there is large prior uncertainty, as measured by a
large variance, the distribution becomes dispersed. As σ2
θ increases, the
normal distribution gets ﬂatter and ﬂatter and, in the limit, it degenerates
to a uniform distribution between −∞and ∞. This is an improper dis-
tribution, as the integral of the density is not ﬁnite. However, a posterior
distribution can be proper even if the prior is improper. For example, take
yi ∼N

µ, σ2
, (i = 1, 2, . . . , n) to be a sample of i.i.d. random variables
with known variance. Adopting as prior
p (µ) ∝constant,
generates an improper distribution, unless ﬁnite boundaries are assigned
to the values of µ. However, it is easy to verify that the posterior density
is always proper, since
p (µ|y1, y2, . . . , yn) ∝exp

−n
2σ2 (µ −y)2
,
integrates to
>
2πσ2/n.
■
Example 7.14
A uniform prior distribution for heritability
Consider the following example, in the same spirit as Examples 2.20 and
2.21 of Chapter 2. Suppose that in a linear model the residual variance
is known. Heritability, h2, is deﬁned as usual, but observations have been
rescaled (by dividing by the residual standard deviation) so that one can

358
7. The Prior Distribution and Bayesian Analysis
write
h2 =
σ2
a
σ2a + σ2e
=
σ2∗
a
σ2∗
a + 1,
where σ2∗
a = σ2
a/σ2
e. Suppose that all we know, a priori, is that h2 is be-
tween 0 and 1. A uniform prior is assigned to this parameter, following
the principle of insuﬃcient reason. Hence, the prior probability that h2 is
smaller than or equal to 1
4 is 1
4, and so is the prior probability that it is
larger than or equal to 3
4.
What is the implied prior distribution of the ratio of variances σ2∗
a ?
Note that σ2∗
a = h2/

1 −h2
, so that the parameter space is (0, ∞). After
calculating the Jacobian of the transformation, the density of the prior
distribution is
p

σ2∗
a

=
1
(σ2∗
a + 1)2 .
Now, the statement “heritability is smaller than or equal to 1
5” is equivalent
to the statement that σ2∗
a
is smaller than or equal to
1
4. The resulting
probability is
Pr

σ2∗
a ≤1
4

=
1
4

0
1
(σ2∗
a + 1)2 dσ2∗
a
=
−1
σ2∗
a + 1
""""
1
4
0
= 1
5.
Also, the probability of the statement “heritability is smaller than or equal
to 2
5” is equivalent to
Pr

σ2∗
a ≤2
3

=
−1
σ2∗
a + 1
""""
∞
2
3
= 2
5.
Hence, while the prior distribution of heritability assigns equal probability
to intervals of equal length, this is not so for the induced prior distribution
of σ2∗
a . This implies that prior indiﬀerence about heritability (as reﬂected
by the uniform prior) does not translate into prior indiﬀerence about the
scaled additive genetic variance.
■
7.5.2
Other Vague Priors
Although the uniform distribution is probably the most widely employed
“vague” prior, other distributions that supposedly convey vague prior knowl-
edge have been suggested. For example, imagine one seeks to infer the
probability of success in a binomial distribution (θ) and that a Be (θ|a, b)

7.5 Priors Conveying Little Information
359
distribution is used as prior for θ. If x is the number of successes after n
independent Bernoulli trials, the posterior density of θ is
p (θ|n, x, a, b) ∝θx (1 −θ)n−x θa−1 (1 −θ)b−1 .
Since a + b can be interpreted as “the size of a prior sample”, one can take
a = b = 0, yielding an improper prior distribution. However, one obtains
as posterior density
p (θ|n, x, a = 0, b = 0) ∝θx−1 (1 −θ)n−x−1 ,
which is a Be (θ|x, n −x) density function. For this distribution to be
proper, the two parameters must be positive. Hence, if either x = 0 or
x = n, the posterior distribution is improper. For example, if θ is small,
it is not unlikely that the posterior distribution turns out to be improper
unless n is large. The preceding illustrates that an improper prior can lead
to an improper posterior, and that caution must be exercised when using
this form of prior assignment.
Example 7.15
Haldane’s analysis of mutation rate
Haldane (1948) studied the problem of inferring mutation rates in a popula-
tion. He noted that since a mutation is a rare event, the sampling distribu-
tion of the relative frequencies (number of mutants/number of individuals
scored) is skewed. Haldane considered using a prior distribution for the
mutation rate θ. If p (θ|H) is some prior density (where H denotes hyper-
parameters) and x mutants are observed out of n individuals, the typical
Bernoulli sampling model produces as posterior density of θ,
p (θ|n, x, H) =
θx (1 −θ)n−x p (θ|H)
1
0
θx (1 −θ)n−x p (θ|H) dθ
.
Observe that the uniform prior corresponds to a Be (1, 1) distribution, in
which case the posterior is always proper. Haldane noted that if the uniform
prior is adopted for θ, the posterior density is in the form
Be (θ|x + 1, n −x + 1) ,
corresponding to a Beta distribution, with mean (x + 1) / (n + 2). If the
posterior mean is used as a point estimator, the statistic is biased in the
frequentist sense, a fact found objectionable by Haldane. He argued that
assuming a uniform prior does not make sense because θ should be greater
than 10−20 and lower than 10−3; in some particular cases, θ would be as
likely to be between 10−6 and 10−7 as between 10−6 and 10−5. He suggested
the prior
p (θ) ∝
1
θ (1 −θ),

360
7. The Prior Distribution and Bayesian Analysis
which is the improper Be (0, 0) distribution. The posterior distribution of
the mutation rate is then Be (x, n −x), with mean x/n, satisfying Hal-
dane’s requirement of unbiasedness. As noted above, this posterior can
be improper, but Haldane (without elaboration) cautioned that his prior
would “work” only if x > 0 (at least one mutant is observed) and if x < n
(which would be almost certain, unless n is extremely small and by sheer
accident all individuals scored are mutant). He stated that for small x/√n
the posterior distribution would be well-approximated by a gamma process
with density
p (θ|n, x) ∝θx−1 exp (−nθ) ,
which also has mean x/n. Haldane (1948) showed then that a cube root
transformation of θ would have a nearly normal distribution. Note that
Haldane’s “vague” prior for θ implies an uniform prior for the logit trans-
formation. Let
λ = log
θ
1 −θ,
with inverse transformation
θ =
exp (λ)
1 + exp (λ)
and Jacobian
dθ
dλ =
exp (λ)
[1 + exp (λ)]2 .
Observe that the logit can take any value in the real line. Then the induced
prior density of λ is
p (λ) ∝

exp (λ)
1 + exp (λ)
−1 
1
1 + exp (λ)
−1
exp (λ)
[1 + exp (λ)]2 = 1.
This uniform distribution is improper because the integral of the density
over the entire parameter space is not ﬁnite.
■
A Single Parameter
Although Jeﬀreys (1961) proposed a class of improper priors for represent-
ing ignorance, it is not entirely transparent why these should be considered
“noninformative” in some precise sense. Zellner (1971) and Box and Tiao
(1973) provide some heuristic justiﬁcations, based on the two rules sug-
gested by Jeﬀreys:
(1) If a parameter θ can take any value in a ﬁnite range or between −∞
and ∞, it should be taken as distributed uniformly, a priori.
(2) If it takes values between 0 and ∞, then its logarithm should have a
uniform distribution.

7.5 Priors Conveying Little Information
361
The justiﬁcation for the ﬁrst rule (Zellner, 1971) is that since the range
covers the entire real line, the probability that θ takes any value drawn
from a uniform distribution is
Pr (−∞< θ < ∞) =
∞

−∞
dθ = ∞,
and Jeﬀreys interprets this as the probability of the certain event (1 be-
comes ∞!). Then the probability that it falls in any interval (a, b) is 0,
and similarly for another interval (c, d) . Since the ratio of probabilities is
indeterminate, Jeﬀreys argues that this constitutes a formal representation
of ignorance, as one cannot favor an interval when choosing over any pair
of ﬁnite intervals. If the uniform prior is bounded (a typical device used by
quantitative geneticists to avoid improper posteriors), this implies that one
knows something about the range of values and the ratios between prob-
abilities are then determinate. However, if the boundaries are stretched,
the ratio of probabilities becomes indeterminate in the limit, thus satisfy-
ing Jeﬀreys’ requirements. Concerning the second rule, note that if log θ is
taken as uniformly distributed, the prior density of θ should be proportional
to 1/θ. In this setting (Zellner, 1971), one has
∞

0
1
θdθ = ∞,
a

0
1
θdθ = ∞,
∞

a
1
θdθ = ∞.
It ∞represents certainty, then the ratio between the last two integrals is
indeterminate, leading to a representation of ignorance: one cannot pick
the interval in which θ is more likely to reside.
An important step toward the development of noninformative priors was
the introduction of invariance requirements by Jeﬀreys (1961). In a nutshell,
the central point is to recognize that ignorance about θ implies ignorance
about the monotone transformation λ = f (θ) . Hence, if the prior density
of θ is p (θ) , the requirement is that the probability contents must be
preserved, that is, p (θ) dθ = p (λ) dλ, which leads directly to the usual
formula for the density of a transformed random variable. Jeﬀreys’ idea
was to use, as prior density of θ, the square root of Fisher’s information
measure
>
I (θ) =
C
E
dl (θ|y)
dθ
2
=
@
A
A
B−E

d2l (θ|y)
(dθ)2

,
(7.45)

362
7. The Prior Distribution and Bayesian Analysis
where l (θ|y) is the log-likelihood function. Making a change of variables,
consider the prior density of λ induced by Jeﬀreys’ rule
p (λ) ∝
C
E
dl (θ|y)
dθ
2 dθ
dλ
∝
C
E
dl (f −1 (λ) |y)
dθ
dθ
dλ
2
∝
C
E
dl (f −1 (λ) |y)
dλ
2
=
>
I (λ),
recalling that the likelihood is invariant under a change in parameteriza-
tion. One can transform back to θ, and retrieve
>
I (θ) as prior distribution.
Hence, Jeﬀreys’ prior is invariant under transformation. The posterior den-
sity of θ has the form
p (θ|y)
∝
L (θ|y)
>
I (θ)
∝
L (θ|y)
C
E
dl (θ|y)
dθ
2
,
(7.46)
where L (θ|y) is the likelihood. The posterior density of λ is then
p (λ|y)
∝
L [λ|y]
C
E
dl (f −1 (λ) |y)
dθ
2 dθ
dλ
∝
L (λ|y)
>
I (λ).
(7.47)
One can now go back and forth between parameterizations, and preserve
the probability contents. Some examples of Jeﬀreys’ rule are presented
subsequently.
At ﬁrst sight, Jeﬀreys’ prior seems to depend on the data, since it involves
the likelihood function. Actually, it does not; rather, it depends on the form
of the “experiment”. Note that in the process of taking expectations, the
dependence on the observed data is removed. However, the prior depends on
how the data are to be collected. From an orthodox Bayesian point of view,
prior information should not be aﬀected by the form of the experiment or
by how much data is to be collected, but this is not the case with Jeﬀreys’
prior. Box and Tiao (1973) argue that a noninformative prior does not
necessarily represent a prior opinion and that “knowing little” is meaningful
only relative to a speciﬁc experiment. Thus, the form of a noninformative
prior would depend on the experiment, and two diﬀerent experiments would
lead to diﬀerent noninformative priors.
Example 7.16
Normal distribution with unknown mean
Let there be n samples from the same normal distribution with unknown

7.5 Priors Conveying Little Information
363
mean but known variance. The log-likelihood is
l

µ|σ2, y

= constant −n (y −µ)2
2σ2
.
The square of the score with respect to µ is

dl

µ|σ2, y

dµ
2
=
n (y −µ)
σ2
2
,
and its expectation is
I (µ) = n
σ2 .
Jeﬀreys’ prior is the square root of this expression. Since it does not involve
µ, it follows that the noninformative prior is ﬂat and improper.
■
Example 7.17
Normal distribution with unknown variance
The setting is as in the previous example, but now we seek Jeﬀreys’ prior
for the variance. The log-likelihood is now
l

σ2|µ, y

= constant −n
2 log σ2 −
n
i=1
(yi −µ)2
2σ2
.
Diﬀerentiating twice with respect to σ2 and multiplying by −1 to obtain
the observed Fisher’s information gives
−n
2σ4 +
n
i=1
(yi −µ)2
σ6
.
Taking expectations and then the square root yields as Jeﬀreys’ prior
p

σ2
∝
? n
2σ4 ∝1
σ2 ,
which is in conformity with the second rule discussed above. Since Jeﬀreys’
prior is invariant under reparameterization one obtains for the standard
deviation,
p (σ) ∝1
σ2
dσ2
dσ ∝1
σ ,
whereas for the logarithm of the standard deviation, the prior is
p [log (σ)]
∝
1
σ
d

explog(σ)
d log (σ)
∝
1
σ explog(σ) ∝1,
which is the improper uniform prior.
■

364
7. The Prior Distribution and Bayesian Analysis
Example 7.18
Exponential distribution
As in Leonard and Hsu (1999), draw a random sample of size n from an
exponential distribution and take its parameter θ to be unknown. The
likelihood function is
L (θ|y) = θn exp

−θ
n

i=1
yi

.
The second derivative of the log-likelihood with respect to θ is −n/σ2. Since
this does not depend on the observations, Jeﬀreys’ prior is
p (θ) ∝
? n
θ2 ∝1
θ.
The posterior distribution is then
p (θ|y) ∝θn−1 exp

−θ
n

i=1
yi

,
which is a Ga (n, n
i=1 yi) process.
■
Many Parameters
Consider now a multi-parameter situation. Jeﬀreys’ rule generalizes to:
p (θ) ∝
>
|I (θ)|,
(7.48)
where I (θ) is Fisher’s information matrix about the p × 1 parameter vec-
tor θ. The multiparameter rule has often been criticized in the Bayesian
literature because of “inconsistencies” (e.g., O’Hagan, 1994), or due to “in-
tuitively unappealing implications” (Bernardo and Smith, 1994). The lat-
ter objection stems from the fact that when the rule is applied to certain
problems, it does not yield results that are equivalent to their frequentist
counterparts, or because no account is taken of degrees of freedom lost. We
give an example where the rule leads to an objectionable result.
Example 7.19
Jeﬀreys’ rule in a regression model
Consider the usual linear regression model under normality assumptions.
The unknown parameters are the regression coeﬃcients β (p × 1) and the
residual variance σ2. As shown in Chapter 3, the expected information
matrix is
I

β, σ2
=


X′X
σ2
0
0
n
2σ4

.

7.5 Priors Conveying Little Information
365
According to Jeﬀreys’ rule (7.48), the joint prior density of the parameters
is
p

β, σ2
∝
""""""


X′X
σ2
0
0
n
2σ4


""""""
1
2
∝

σ2−p+2
2 .
(7.49)
When β contains a sole component (a mean, µ, say), the prior reduces
to σ−3. We now further develop this simpler situation and examine the
marginal posterior distribution of σ2. For p = 1, the joint posterior based
on a sample of size n is
p

µ, σ2|y

∝

σ2−n
2 exp


n
i=1
(yi −µ)2
2σ2



σ2−3
2
∝

σ2−n+3
2
exp


n
i=1
(yi −y)2
2σ2

exp

−n (y −µ)2
2σ2

.
(7.50)
Integration over the mean, µ, leads to the following marginal density of σ2
(after rearrangement):
p

σ2|y

∝

σ2−( n
2 +1) exp

−
n
i=1
(yi −y)2
2σ2

.
(7.51)
Change now variables to
χ2 =
n
i=1
(yi −y)2
σ2
.
In a frequentist setting, this is a central chi-square random variable on n−1
degrees of freedom. From a Bayesian point of view, we need to consider its
posterior distribution. Noting that the absolute value of the Jacobian of
the transformation is n
i=1 (yi −y)2 χ−4, the posterior density of χ2 is
p

χ2|y

∝

χ2 n
2 −1 exp

−χ2
2

,
indicating a chi-square distribution on n degrees of freedom. Since “every-
body knows” that estimating the mean consumes one degree of freedom,
it becomes apparent that Jeﬀreys’ multiparameter prior does not take into
account the loss of information incurred.

366
7. The Prior Distribution and Bayesian Analysis
On the other hand, suppose one employs Jeﬀreys’ rule for each of µ and
σ2 separately (assuming that the other parameter is known in each case),
and takes as joint prior
p

µ, σ2
∝p

µ|σ2
p

σ2|µ

∝1
σ2 .
Replacing

σ2−3
2 in (7.50) by

σ2−1 and integrating over µ yields an
expression that diﬀers from (7.51) in the exponent of the ﬁrst term, which
is instead −[(n + 1)/2]. After transforming as before, the posterior density
of χ2 is now
p

χ2|y

∝

χ2 n−1
2
−1 exp

−χ2
2

.
Here, one arrives at the “correct” posterior distribution of chi-square, i.e.,
one on n −1 degrees of freedom. There is no formal justiﬁcation for such
prior, but “it works”, at least in the sense of coinciding with the frequen-
tist treatment, which was one of Jeﬀreys’ objectives. Box and Tiao (1973)
recommend exercising “special care” when choosing noninformative priors
for location and scale parameters simultaneously.
The problems of the multiparameter rule become even more serious when
the model is even more richly parameterized. Suppose now that β contains
two means (µ1 and µ2). Using (7.49), the joint prior p

µ1, µ2, σ2
turns
out to be proportional to σ−4. The joint posterior density can be written
as
p

µ1, µ2, σ2|y

∝

σ2−n1+n2+4
2
exp


2
i=1
ni

j=1
(yij −yi)2
2σ2


×
2
-
i=1
exp

−ni (yi −µi)2
2σ2

,
where ni is the number of observations associated with mean i; the rest
of the notation is clear from the context. Integrating over the two means
yields as marginal posterior density of σ2,
p

σ2|y

∝

σ2−n1+n2+2
2
exp


2
i=1
ni

j=1
(yij −yi)2
2σ2

.
Now put
χ2 =
2
i=1
ni

j=1
(yij −yi)2
σ2
.

7.5 Priors Conveying Little Information
367
This variable, in a frequentist setting, should have a chi-squared distribu-
tion on n1 + n2 −2 degrees of freedom. Changing variables, the posterior
density of χ2 becomes
p

χ2|y

∝

χ2 n1+n2
2
−1 exp

−χ2
2

.
Again, the multiparameter rule does not take into account the fact that
several means appear as nuisance parameters: the degrees of freedom of
the chi-square distribution are left intact at n = n1 + n2. Jeﬀreys (1961),
in the context of testing location parameters, writes: “The index in the
corresponding t distribution would always be (n + 1) /2 however many true
values were estimated. This is unacceptable”. We agree.
■
7.5.3
Maximum Entropy Prior Distributions
Using the principle of maximum entropy as a means of allocating prob-
abilities in a prior distribution was suggested by Jaynes, a physicist. A
comprehensive account of the idea is in his unﬁnished book (Jaynes, 1994).
Suppose one wishes to assign a prior distribution to some unknown quan-
tity. Naturally, this prior should take into account whatever information
is available, but not more. For example, knowledge of average values (or
of other aspects of the distribution) will give a reason for favoring some
possibilities over others but, beyond this, the distribution should be as un-
committed as possible. Further, no possibilities should be ruled out, unless
dictated by prior knowledge. The information available deﬁnes constraints
that ﬁx some properties of the prior distribution, but not all of them. Jaynes
formulates the problem as follows:
“To cast it into mathematical form, the aim of avoiding un-
warranted conclusions leads us to ask whether there is some rea-
sonable numerical measure of how uniform a probability distri-
bution is, which the robot could maximize subject to constraints
which represent its available information.”
Jaynes used Shannon’s (1948) theorem, which states that the only mea-
sure of the uncertainty represented by a probability distribution is entropy.
Then he argued that a distribution maximizing entropy, subject to the con-
straints imposed by the information available, would represent the “most
honest” description of what is known about a set of propositions. He noted
that the only source of arbitrariness is the base of the logarithms employed
in entropy. However, since this operates as a multiplicative constant in the
expression for entropy, it has no eﬀect on the values of the probabilities
that maximize H; see, e.g., the form of entropy in (7.9). A derivation of
the principle, as in Jaynes (1994) and in Sivia (1996), follows.

368
7. The Prior Distribution and Bayesian Analysis
Discrete Case
Let I represent the information to be used for assigning probabilities
{p1, p2, ..., pK}
to K diﬀerent possibilities. Suppose there are n >> K small “quanta” of
probability (so that n is very large) to distribute in any way one sees ﬁt. If
the quanta are tossed completely at random (reﬂecting lack of information
in the process of allocating quanta), so that each of the options has an
equal probability 1/K of getting a quanta, the allocation would be viewed
as a fair one. At the end of the experiment, it is observed that option i has
received ni quanta, and so on, so that the experiment has generated the
probability assignment
pi = ni
n ;
i = 1, 2, . . . , K.
The probability that this particular allocation of quanta will be observed
is given by the multinomial distribution
W =
n!
n1! n2!... nK!

 1
K
n1 
 1
K
n2
...

 1
K
nK
=
n!
n1! n2!... nK!K
−
K

i=1
nK =
n!
n1! n2!... nK!K−n,
(7.52)
where n = K
i=1 ni. The experiment is repeated over and over, and the
probability assignment is examined to see if it is consistent with the in-
formation I; if not, the assignment is rejected. What is the most likely
probability distribution resulting from the experiment? In order to ﬁnd it,
use will be made of Stirling’s approximation to factorials (e.g., Abramowitz
and Stegun, 1972) for large n:
log n! ≈n log n −n.
Employing this in (7.52), one can write
1
n log W = 1
n

n log n −n −
K

i=1
(ni log ni −ni) −n log K

= log n −
K

i=1
ni
n log ni

−log K.

7.5 Priors Conveying Little Information
369
Since pi = ni/n,
1
n log W = log n −
K

i=1
(pi log npi) −log K
= constant −
K

i=1
(pi log pi)
= constant + H (p1, p2, ..., pK) .
(7.53)
Maximizing W with respect to pi is equivalent to maximizing (1/n) log W,
and the preceding expression indicates that this is achieved when the en-
tropy H (p1, p2, ..., pK) is extremized with respect to the probabilities, sub-
ject to any constraints imposed by the available information I.
Now consider ﬁnding an explicit representation of the maximum entropy
prior distribution of a discrete random variable θ with K mutually exclusive
and exhaustive states θi. The information available is that the sum of the
probabilities must be equal to 1 and assume, further, that the mean of the
prior distribution to be speciﬁed, i.e., θ = K
i=1 θipi, is taken to be known.
This knowledge must be incorporated in the maximization problem as a
Lagrangian condition. Hence, the objective function to be extremized is:
H ∗(p1, p2, . . . , pK, λ0, λ1)
= −
K

i=1
(pi log pi) + λ0
 K

i=1
pi −1

+ λ1
 K

i=1
θipi −θ

,
(7.54)
where λ0 and λ1 are Lagrange multipliers ensuring that the two information
constraints are observed. The multipliers ensure propriety and knowledge of
the mean of the prior distribution, respectively. Diﬀerentiation with respect
to the K + 2 unknowns yields
∂H ∗(p1, p2, ..., pK, λ0, λ1)
∂pi
= −log pi −1 + λ0 + λ1θi,
i = 1, 2, ..., K,
∂H ∗(p1, p2, ..., pK, λ0, λ1)
∂λ0
=
K

i=1
pi −1,
∂H ∗(p1, p2, ..., pK, λ0, λ1)
∂λ1
=
K

i=1
θipi −θ.
The diﬀerentials are set to 0 simultaneously and solved for the unknowns.
The pi equation yields
pi = exp (λ1θi) exp (λ0 −1) .

370
7. The Prior Distribution and Bayesian Analysis
Since the ﬁrst Lagrangian condition dictates that the sum of the probabil-
ities must add up to 1, the preceding must be equal to
pi =
exp (λ1θi) exp (λ0 −1)
K

i=1
exp (λ1θi) exp (λ0 −1)
=
exp (λ1θi)
K

i=1
exp (λ1θi)
,
i = 1, 2, ..., K.
(7.55)
This is called the Gibbs distribution, and the denominator is known as the
partition function (Applebaum, 1996). Expression (7.55) gives the maxi-
mum entropy prior distribution of a univariate discrete random variable
with known mean. If the mean of the distribution is left unspeciﬁed, this
is equivalent to removing the second Lagrangian condition in (7.54), or
setting λ1 to 0. Doing this in expression (7.55), one obtains pi = 1/K,
(i = 1, 2, ..., K) as maximum entropy distribution. In general, as additional
side conditions are introduced (as part of the prior information I), the sys-
tem of equations that needs to be solved is non-linear. Zellner and Highﬁeld
(1988) discuss one of the possible numerical solutions.
Continuous Case via Discretization
When θ is continuous, the technical arguments are more involved. Further,
it must be recalled that entropy is not well deﬁned in such a setting (for
an illustration, see Example 7.6). The problem now consists of ﬁnding the
prior density or distribution that maximizes the entropy
H [p (θ)] = −

(log θ) p (θ) dθ,
subject to the constraints imposed by the available information I. The in-
tegral above is a functional, that is, a function that depends on another
function, this being the density p (θ) . Hence, one must ﬁnd the function
that extremizes the integral. First, we present a solution based on dis-
cretizing the prior distribution, and a formal calculus of variations solution
is given later on.
First, return to the derivation of the principle of maximum entropy given
in (7.52) and (7.53), but assume now that the chance of a quanta falling into
a speciﬁc one of the K options is mi, instead of being equal for all options
(Sivia, 1996). Then the experiment generates the probability assignment
W ∗=
n!
n1! n2!... nK! (m1)n1 (m2)n2 ... (mk)nK ,
which is the multinomial distribution. Next, as before, put
1
n log W ∗= 1
n

log n! −
K

i=1
log ni! +
K

i=1
ni log mi

,

7.5 Priors Conveying Little Information
371
and make use of Stirling’s approximation, and of pi = ni/n, to arrive at
1
n log W ∗= log n −
K

i=1
(pi log npi) +
K

i=1
pi log mi
= −
K

i=1

pi log pi
mi

.
(7.56)
This is a generalization of (7.53), where allowance is made for the options
in the random experiment to have unequal probability. Note that (7.56) is a
relative entropy, which has the advantage (in the continuous case) of being
invariant under a change of variables, as noted earlier. Now the continuous
counterpart of (7.56) is
H ∗(θ) = −
 
log p (θ)
m (θ)

p (θ) dθ.
(7.57)
This is in the same mathematical form as Kullback’s expected information
for discrimination, and is also known as the Shannon-Jaynes entropy (Sivia,
1996). Here, m (θ) plays the role of a “reference” distribution, often taken
to be the uniform one. In this case, (7.57) reduces to the standard entropy.
Now consider a discretization approach to ﬁnding the maximum relative
entropy distribution. Suppose that θ takes appreciable density in the inter-
val (a, b) . Following Applebaum (1996), one can deﬁne a partition of (a, b)
such that:
a = θ0 < θ1 < · · · < θK−1 < θK = b.
Deﬁne the event ξj ∈(θj−1, θj) for 1 ≤j ≤K, and the discrete random
variable θ∗with K mutually exclusive and exhaustive states and probability
distribution
Pr

θ∗= ξj

=
θj

θj−1
p (θ) dθ
= F (θj) −F (θj−1) = pj,
(7.58)
where F (·) is the c.d.f.. The relative entropy of the discretized distribution
is then
H (θ∗) = −
K

j=1
pj log pj
mj
.

372
7. The Prior Distribution and Bayesian Analysis
Proceed now to maximize H (θ∗) with respect to pj, subject to the La-
grangian condition that
K

j=1
pj = 1. Then
∂H (θ∗)
∂pj
=
−1 −log pj
mj
−
∂
∂pj
λ
 K

i=1
pi −1

=
−1 −log pj
mj
−λ,
with the derivative with respect to the multiplier λ leading directly to the
condition that the sum of the probabilities must be equal to 1. Setting the
preceding diﬀerential to 0 produces
pj = mj exp [−(1 + λ)] .
(7.59)
Summing now over the K states, and assuming that the discretized “refer-
ence” distribution is proper, gives exp [−(1 + λ)] = 1, so that λ = −1. It
follows that pj = mj. Further, the distribution with probabilities mj is the
one representing complete randomness of the experiment, since the proba-
bility “quanta” are allocated completely at random, with mj = 1/n for all
j when the options are equally likely. Now, the deﬁnition of the derivative
implies that
lim
∆θ→0 pj = lim
∆θ→0
F (θj) −F (θj−1)
θj −θj−1
= F ′ (θj) = p (θ) .
Hence, in the limit, it follows that the density of the maximum entropy
distribution p (θ) is the uniform density distribution m (θ), since all the
options are of equal size and inﬁnitesimally small. Before presenting the
variational argument, two examples are given, using the discretization pro-
cedure given above. Note that in the continuous case the maximum relative
entropy distribution is not invariant with respect to the reference distribu-
tion chosen.
Example 7.20
Maximum entropy prior distribution when the mean is
known
Suppose the mean of the prior distribution is given. The setting then is as
in (7.54). However, the objective function to be optimized now involves en-
tropy relative to the reference uniform distribution. The objective function
takes the form:
H ∗(p1, p2, . . . , pK, λ0, λ1)
= −
K

i=1

pi log pi
mi

+ λ0
 K

i=1
pi −1

+ λ1
 K

i=1
θipi −θ

.

7.5 Priors Conveying Little Information
373
Setting the derivatives to 0 leads to
pi = mi exp (λ1θi) exp (λ0 −1) .
The continuous counterpart gives, as density of the maximum relative en-
tropy distribution,
p (θ) = m (θ) exp (λ1θ) exp (λ0 −1) .
If m (θ) (the reference density) is uniform, it follows that
p (θ) ∝exp (−λ∗
1θ) ,
where λ∗
1 = −λ1. If θ is a strictly positive parameter, it follows that the
maximum entropy distribution relative to a uniform measure is exponential.
Since the mean of an exponential distribution is θ = 1/λ∗
1, the λ∗
1 parameter
can be assessed readily once the prior mean is speciﬁed.
■
Example 7.21
Maximum entropy prior distribution when the mean, vari-
ance, and higher-order moments are given
Knowledge of the mean and variance of the prior distribution imposes three
constraints in the optimization procedure. The ﬁrst constraint ensures pro-
priety of the distribution, and the other two give the conditions stating
knowledge of the ﬁrst and second moments. Using the discretization pro-
cedure, the objective function to be maximized is
H∗(p1, p2, ..., pK, λ0, λ1, λ2) = −
K

i=1

pi log pi
mi

+ λ0
 K

i=1
pi −1

+ λ1
 K

i=1
θipi −θ

+ λ2
 K

i=1
θ2
i pi −θ2

,
where θ2 = K
i=1 θ2
i pi and, therefore, σ2 = θ2 −θ
2 is the variance of the
prior distribution. Diﬀerentiation with respect to pi, and setting to 0, gives
log pi
mi
= 1 −λ0 −λ1θi −λ2θ2
i .
The continuous counterpart, after solving for the maximum relative entropy
density p (θ), is
p (θ) ∝m (θ) exp

1 −λ0 −λ1θ −λ2θ2
.
(7.60)
Note that by restating the Lagrangian condition
λ0
 K

i=1
pi −1


374
7. The Prior Distribution and Bayesian Analysis
as
λ0

1 −
K

i=1
pi

,
and so on, one can put, without loss of generality,
p (θ) ∝m (θ) exp

1 + λ0 + λ1θ + λ2θ2
.
(7.61)
If the reference density m (θ) is taken to be uniform (and proper, so that
the maximum relative entropy density is guaranteed to be proper as well)
one obtains
p (θ) ∝exp

λ1θ + λ2θ2
,
and the maximum entropy density is then
p (θ) =
exp

λ1θ + λ2θ2

exp

λ1θ + λ2θ2
dθ
=
exp

1 + λ0 + λ1θ + λ2θ2

exp

1 + λ0 + λ1θ + λ2θ2
dθ.
(7.62)
If the ﬁrst M moments of the prior distribution are speciﬁed, the preceding
generalizes to
p (θ) =
exp

1 +
M

i=0
λiθi


exp

1 +
M

i=0
λiθi

dθ
.
(7.63)
■
Example 7.22
The special case of known mean and variance
Return now to a setting of known mean and variance of the prior distri-
bution and consider the Kullback–Leibler discrepancy in (7.41). Let g (θ)
denote any other density with mean θ and variance σ2 and let p (θ) be the
maximum relative entropy distribution sought. Since the discrepancy is at
least null
D [g (θ) , p (θ)] =
 
log p (θ)
g (θ)

p (θ) dθ ≥0,
which implies that
H [p (θ)] ≤−

{log [g (θ)]} p (θ) dθ,
(7.64)

7.5 Priors Conveying Little Information
375
with the equality holding if and only if p (θ) = g (θ) (Applebaum, 1996).
Now take g (θ) to be the density of the normal distribution N

θ, σ2
. Then
−

{log [g (θ)]} p (θ) dθ = −
 #
log

1
√
2πσ2 exp

−

θ −θ
2
2σ2
$
p (θ) dθ
= −log

1
√
2πσ2

+
1
2σ2
 
θ −θ
2 p (θ) dθ.
Note that the integral in the preceding expression is σ2, the variance of the
prior distribution, by deﬁnition. Then, using (7.64),
H [p (θ)] ≤−

{log [g (θ)]} p (θ) dθ = 1
2

1 + log

2πσ2
.
It follows that the entropy of the maximum entropy distribution cannot
exceed

1 + log

2πσ2
/2. Now, from Example 7.7, this quantity is pre-
cisely the entropy of a normal distribution. Hence, the maximum entropy
prior distribution when the mean and variance are given must be a normal
distribution with mean θ and variance σ2.
■
Continuous Case via Variational Arguments
As noted, the search for a maximum relative entropy distribution involves
ﬁnding the function p (θ) that minimizes the integral
Int [p (θ)] =
 
log p (θ)
m (θ)

p (θ) dθ.
(7.65)
This is called a “variational” problem and its solution requires employing
the advanced techniques of the calculus of variations (e.g., Weinstock, 1974;
Fox, 1987). Since few biologists are exposed to the basic ideas (referred to
as “standard” in some statistical texts) we provide a cursory introduction.
Subsequently, a speciﬁc result is applied to the problem of ﬁnding a contin-
uous maximum entropy distribution. We follow Weinstock (1974) closely.
In general, consider the integral
Int [y (x)] =
x2

x1
f [x, y (x) , y′ (x)] dx,
(7.66)
where y (x) is a twice diﬀerentiable function and y′ (x) is its ﬁrst derivative
with respect to x. We seek the function y (x) rendering the above integral
minimum (or maximum). The problem resides in ﬁnding such a function or,
equivalently, in arriving at conditions that the function must obey. First,
the function must satisfy the boundary conditions y (x1) = y1 and y (x2) =
y2, with x1, x2, y1, and y2 given. Now, when going from the point (x1, y1)

376
7. The Prior Distribution and Bayesian Analysis
to the point (x2, y2), one can draw a number of curves, each relating the
dependent variable to x, and with each curve deﬁning a speciﬁc integration
path. Next, deﬁne the “family” of functions
Y (x) = y (x) + εη (x) ,
(7.67)
where ε is a parameter of the family and η (x) is diﬀerentiable with η (x1) =
η (x2) = 0. This ensures that Y (x1) = y (x1) = y1 and Y (x2) = y (x2) =
y2, so that all members of the family possess the required ends. Also, note
that the form of (7.67) indicates that no matter what η (x) is chosen, the
minimizing function y (x) will be a member of the family for ε = 0. Now,
return to (7.66), and replace y and y′ by Y and Y ′, respectively. Then,
write
Int (ε) =
x2

x1
f [x, Y (x) , Y ′ (x)] dx,
(7.68)
where, for a given η (x) , the integral is a function of ε. Observe from (7.67)
that
Y ′ (x) = y′ (x) + εη′ (x) ,
so, in conjunction with (7.67), it becomes clear that setting ε = 0 is equiv-
alent to replacing Y and Y ′ by y and y′, respectively. Hence (7.68) is
minimum with respect to ε at the value ε = 0. The problem of minimizing
(7.68) then reduces to one of standard calculus with respect to ε, except
that we know in advance that the minimizing value is ε = 0! Hence, it must
be true that
d Int (ε)
dε
""""
ε=0
= 0.
Now, since the limits of integration do not involve ε:
d Int (ε)
dε
=
x2

x1
d
dεf [x, Y (x) , Y ′ (x)] dx.
(7.69)
Employing standard results for the derivative of a function of several vari-
ables (Kaplan, 1993):
d
dεf [x, Y (x) , Y ′ (x)]
=
∂f
∂Y
∂Y
∂ε + ∂f
∂Y ′
∂Y ′
∂ε
=
∂f
∂Y η (x) + ∂f
∂Y ′ η′ (x) ,
the integral in (7.69) becomes
d Int (ε)
dε
=
x2

x1
 ∂f
∂Y η (x) + ∂f
∂Y ′ η′ (x)

dx.

7.5 Priors Conveying Little Information
377
Further, since setting ε = 0 is equivalent to replacing (Y, Y ′) by (y, y′), use
of the preceding in (7.69) gives
d Int (ε)
dε
""""
ε=0
=
x2

x1
∂f
∂y η (x) dx +
x2

x1
∂f
∂y′ η′ (x) dx = 0.
(7.70)
Integrating the second term of (7.70) by parts gives
d Int (ε)
dε
""""
ε=0
=
x2

x1
∂f
∂y η (x) dx + ∂f
∂y′ η (x)
""""
x2
x1
−
x2

x1
 d
dx

 ∂f
∂y′

η (x) dx = 0.
Now recall that at the boundary points, η (x1) = η (x2) = 0, so the second
term in the preceding three-term expression vanishes. Rearranging,
d Int (ε)
dε
""""
ε=0
=
x2

x1
∂f
∂y −d
dx

 ∂f
∂y′

η (x) dx = 0.
(7.71)
This condition must hold true for all η (x) , in view of the requirements
imposed above. Hence, a condition that the function must obey in order to
extremize the integral (7.66) is that
∂f [x, y (x) , y′ (x)]
∂y
−
 d
dx
∂f [x, y (x) , y′ (x)]
∂y′
%
= 0.
(7.72)
This diﬀerential equation is called the Eulerian or Euler–Lagrange condi-
tion (Weinstock, 1974; Fox, 1987). Solving the equation for y (x) provides
the function that minimizes the integral, provided that a minimum exists.
Return now to the problem of ﬁnding the prior density p (θ) that mini-
mizes the integral (7.65) subject to the constraints imposed by the informa-
tion available. As seen before, the constraints may result from specifying
moments of the prior distribution or some features thereof; for example,
one of the constraints is that the prior must be proper. Suppose there are
M constraints having the form
E [qi (θ)] = mi,
i = 0, 1, . . . , M −1,
where q (·) denotes some function of θ. For instance, if one speciﬁes the ﬁrst
and second moments of the prior distribution, the constraints would be

p (θ) dθ
=
1 = m0,

θp (θ) dθ
=
E (θ) = m1,

θ2p (θ) dθ
=
E

θ2
= m2.

378
7. The Prior Distribution and Bayesian Analysis
In general, we seek to minimize the objective function
I [p (θ)] =
 
log p (θ)
m (θ)

p (θ) dθ +
M−1

i=0
λi

qi (θ) p (θ) dθ −mi

, (7.73)
with respect to p (·) ; as usual, the λ′s are Lagrange multipliers. We now
apply the variational argument leading to the derivative (7.70) and, for sim-
plicity, set m (θ) = 1; that is, the reference density is taken to be uniform.
Then, write:
pε (θ)
=
p (θ) + εη (θ) ,
p′
ε (θ)
=
d
dε [p (θ) + εη (θ)] = η (θ) .
Using this in (7.73), with m (θ) = 1 gives:
I [pε (θ)] =

{log [p (θ) + εη (θ)]} [p (θ) + εη (θ)] dθ+
M−1

i=0
λi

qi (θ) [p (θ) + εη (θ)] dθ −mi

.
Hence:
dI [p (θ) + εη (θ)]
dε
=

{log [p (θ) + εη (θ)]} η (θ) dθ +

η (θ) dθ
+
M−1

i=0
λi

qi (θ) η (θ) dθ.
Rearranging and setting the derivative to 0 as required by the variational
argument, yields:
dI [p (θ) + εη (θ)]
dε
""""
ε=0
=
 #
log [p (θ) + εη (θ)] +
M−1

i=0
λiqi (θ)
$
η (θ) dθ
"""""
ε=0
= 0.
Since this must be true for all integration paths, the extremizing density
satisﬁes the equation:
log [p (θ)] +
M−1

i=0
λiqi (θ) = 0.
Solving for the maximum entropy density gives
p (θ) = exp

−
M−1

i=0
λiqi (θ)

.

7.5 Priors Conveying Little Information
379
After normalization, and noting that the sign of the Lagrange multipliers
is unimportant (one can write λ∗
i = −λi), this becomes
p(θ) =
exp
M−1

i=0
λ∗
i qi(θ)


exp
M−1

i=0
λ∗
i qi(θ)

dθ
.
(7.74)
This is precisely in the form of (7.63), with the latter being a particular case
of (7.74) when the qi (θ) functions are the moments of the prior distribution.
It is important to note that in all cases the maximum entropy distribu-
tion, must be deﬁned relative to a reference distribution m (θ) ; otherwise,
the concept of entropy does not carry to the continuous case. Here, a uni-
form reference distribution has been adopted arbitrarily. This illustrates
that the concept of maximum entropy (in the continuous case) does not
help to answer completely the question of how a non-informative prior
should be constructed (Bernardo, 1979; Bernardo and Smith, 1994). On
the one hand, information is introduced (and perhaps legitimately so) via
the constraints, these stating features of the prior that are known. On the
other hand, the reference measure must be a representation of ignorance it-
self, so the problem relays to one of how to construct a truly noninformative
(in some sense) reference distribution.
7.5.4
Reference Prior Distributions
Reference analysis (Bernardo, 1979) can be viewed as a way of rendering
inferences as “objective” as possible, in the sense that the prior should
have a minimal eﬀect, relative to the data, on posterior inference. As dis-
cussed below, the notion of “minimal eﬀect” is deﬁned in a precise manner.
Statisticians often use the term reference priors rather than reference anal-
ysis. The theory is technically involved, and the area is still the subject of
much research. Hence, only a sketch of the main ideas is presented following
Bernardo and Smith (1994) closely.
Single Parameter Model
From (7.29), the amount of information about a parameter that an exper-
iment is expected to provide, can be written as
I [e, h (θ)]
=
 
log
h (θ|y)
h (θ)

h (θ|y) dθ
%
g (y)dy
=
E

log
h (θ|y)
h (θ)

h (θ|y) dθ
%
,
where the expectation is taken with respect to the marginal (in the Bayesian
sense) distribution of the observations, but conditionally on the experimen-

380
7. The Prior Distribution and Bayesian Analysis
tal design adopted, as usual. The notation I [e, h (θ)] makes explicit that
this measure of information is actually a functional of the prior density
h (θ) ; e denotes a speciﬁc experiment. Note that the information measure
is the expectation of the Kullback–Leibler distance between the posterior
and prior distributions taken over all data that can result from this ex-
periment, given a certain probability model. Hence, this expectation must
be nonnegative. Also, it can be veriﬁed readily that I [e, h (θ)] is invariant
under one-to-one transformations.
Now suppose that the experiment is replicated K times, yielding the
hypothetical data vector
y∗
K =

y′
1
y′
2
.
.
.
y′
K
′ .
The sampling model would then be
p (y∗
K|θ) =
K
-
i=1
p (yi|θ) .
The expected information from such an experiment is
I [e (K) , h (θ)] =
 
log
h (θ|y∗
K)
h (θ)

h (θ|y∗
K) dθ
%
g (y∗
K)dy∗
K.
(7.75)
If the experiment could be replicated an inﬁnite number of times, one would
be in a situation of perfect or complete information about the parameter.
Hence, the quantity
I [e (∞) , h (θ)] = lim
K→∞I [e (K) , h (θ)]
measures, in some sense, the missing information about θ expressed as a
function of the prior density h (θ) . As more information is contained in
the prior, less is expected to be gained from exhaustive data. On the other
hand, if the prior contains little information, more would be expected to
be gained from valuable experimentation. A “noninformative” prior would
then be that maximizing the missing information.
The reference prior, denoted as π (θ), is deﬁned formally to be the prior
that maximizes the missing information functional given above. If πK (θ)
denotes the prior density that maximizes I [e (K) , h (θ)] for a certain amount
of replication K, then π (θ) is the limiting value as K →∞of the sequence
of priors {πK (θ) , K = 1, 2, . . .} that ensues as replication increases. Asso-
ciated with each πK (θ) there is the corresponding posterior
πK (θ|y) ∝p (y|θ) πK (θ) ,
(7.76)
whose limit, as K →∞, is deﬁned as the reference posterior distribution
π (θ|y) = lim
K→∞πK (θ|y) .

7.5 Priors Conveying Little Information
381
The reference prior is deﬁned as any positive function π (θ), such that
π (θ|y) ∝p (y|θ) π (θ) .
The deﬁnition implies that the reference posterior distribution depends only
on the asymptotic behavior of the model since the amount of replication is
allowed to go to inﬁnity.
The technical details of the procedure for obtaining the reference prior,
that culminates in expressions (7.82) and (7.83), are given below. Consider
(7.75) and rewrite it as
I [e (K) , h (θ)] =
 
log
h (θ|y∗
K)
h (θ)

p (y∗
K|θ) dy∗
K
%
h (θ) dθ.
(7.77)
Since p(y∗
K|θ) integrates to 1, the inner integral can be rearranged as

[log h (θ|y∗
K)] p (y∗
K|θ) dy∗
K −log h (θ) .
Thus
I [e (K) , h (θ)]
=

log

exp

[log h (θ|y∗
K)] p (y∗
K|θ) dy∗
K

h (θ)

h (θ) dθ.
Deﬁne
fK (θ) = exp

[log h (θ|y∗
K)] p (y∗
K|θ) dy∗
K
%
,
(7.78)
and note that fK (θ) depends implicitly on h (θ) through the posterior
density h (θ|y∗
K) . Then one has
I [e (K) , h (θ)] =

log
fK (θ)
h (θ)

h (θ) dθ.
Imposing the constraint that the prior density integrates to 1, the prior
πK (θ) that maximizes I [e (K) , h (θ)] must be an extremal of the functional
F {h (θ)} =

log
fK (θ)
h (θ)

h (θ) dθ + λ

h (θ) dθ −1

,
(7.79)
where λ is a Lagrange multiplier.
Deﬁne
hε (θ) = h (θ) + εη (θ) ,
so
h′
ε (θ) = d
dε [h (θ) + εη (θ)] = η (θ) .

382
7. The Prior Distribution and Bayesian Analysis
Now we make use of the variational argument employed in connection with
maximum entropy priors and consider the function
F {hε (θ)} =

log
fεK (θ)
hε (θ)

hε (θ) dθ + λ

hε (θ) dθ −1

.
Hence
dF [h (θ) + εη (θ)]
dε
""""
ε=0
=
  d
dε log
fεK (θ)
hε (θ)

hε (θ)
%
dθ + λ
  d
dεhε (θ)
%
dθ = 0.
Further,
dF [h (θ) + εη (θ)]
dε
""""
ε=0
=

h′
ε (θ) log
fεK (θ)
hε (θ)

dθ
+

hε (θ) hε (θ)
fεK (θ)
f ′
εK (θ) hε (θ) −fεK (θ) h′
ε (θ)
h2ε (θ)

dθ
+λ

h′
ε (θ) dθ
=

η (θ) log
fεK (θ)
hε (θ)

dθ +
 f ′
εK (θ) hε (θ)
fεK (θ)
dθ
−

η (θ) dθ + λ

η (θ) dθ = 0.
Hence, as in Bernardo and Smith (1994)
dF [h (θ) + εη (θ)]
dε
""""
ε=0
=

{[log fK (θ)] η(θ) + h (θ)
fK (θ)f ′
K (θ)
−[log h (θ) + 1] η(θ) + λη (θ)}dθ = 0,
(7.80)
where f ′
K (θ) is the derivative of fεK (θ) with respect to ε evaluated at
ε = 0.
Given the form of fK (θ) given in (7.78),
f ′
K (θ) = d
dεfεK (θ)
""""
ε=0
= d
dε exp

[log hε (θ|y∗
K)] p (y∗
K|θ) dy∗
K
%""""
ε=0
.

7.5 Priors Conveying Little Information
383
Recalling that the posterior is proportional to the product of the prior and
of the sampling model, the preceding can be expressed as
f ′
K (θ) = d
dε exp
 
log
p(y∗
K|θ)[h(θ) + εη(θ)]

p(y∗
K|θ)[h(θ) + εη(θ)]dθ

p (y∗
K|θ) dy∗
K
%""""
ε=0
.
Carrying out the diﬀerentiation
f ′
K (θ) = fK (θ)
 d
dε

log (p (y∗
K|θ) [h (θ) + εη (θ)]) p (y∗
K|θ) dy∗
K
%""""
ε=0
−fK (θ)
 d
dε
 
log


p (y∗
K|θ) [h (θ) + εη (θ)] dθ

p (y∗
K|θ) dy∗
K
%""""
ε=0
.
Proceeding with the algebra,
f ′
K (θ) = fK (θ)

d
dε (log [h (θ) + εη (θ)]) p (y∗
K|θ) dy∗
K
%""""
ε=0
−fK (θ)
×

d
dε

log


p (y∗
K|θ) [h (θ) + εη (θ)] dθ

p (y∗
K|θ) dy∗
K
%""""
ε=0
.
Further,
f ′
K (θ) = fK (θ)

η (θ)
h (θ) + εη (θ)

p (y∗
K|θ) dy∗
K
%""""
ε=0
−fK (θ)


p(y∗
K|θ)η(θ)dθ

p(y∗
K|θ)[h(θ)+εη(θ)]dθp (y∗
K|θ) dy∗
K
%""""
ε=0
.
Evaluating appropriate terms at ε = 0, noting that

p (y∗
K|θ) dy∗
K = 1,
and that

p (y∗
K|θ) h (θ) dy∗
K = p(y∗
K), gives
f ′
K (θ) = fK (θ) η (θ)
h (θ) −fK (θ)


p(y∗
K|θ)η(θ)dθ
p(y∗
K)
p (y∗
K|θ) dy∗
K
%
.
(7.81)
Note now that hε (θ) = h (θ) + εη (θ) implies that

hε (θ) dθ =

h (θ) dθ + ε

η (θ) dθ.
Hence,

η (θ) dθ = 0 is a necessary condition for h (·) being a p.d.f.. This
follows because, then,

hε (θ) dθ =

h (θ) dθ = 1.

384
7. The Prior Distribution and Bayesian Analysis
Now, if h (·) is a proper density function, the posterior is also a proper
density. Therefore,
1
=
 p (y∗
K|θ) hε (θ) dθ
p(y∗
K)
=
 p (y∗
K|θ) h (θ) dθ
p(y∗
K)
+ ε
 p (y∗
K|θ) η (θ) dθ
p(y∗
K)
=
1 + ε
 p (y∗
K|θ) η (θ) dθ
p(y∗
K)
= 1.
Hence

p (y∗
K|θ) η (θ) dθ = 0. Using this condition in (7.81), we get
f ′
K (θ) = fK (θ) η (θ)
h (θ).
Employing this in (7.80), the condition that the extremal must satisfy is

{log [fK (θ)] + 1 −log h (θ) −1 + λ} η(θ) dθ
=

{log [fK (θ)] −log h (θ) + λ} η(θ)dθ = 0.
Since this must hold for all η (θ) , the extremizing density can be found by
solving
log [fK (θ)] −log h (θ) + λ = 0.
Upon retaining only the terms that vary with θ:
h (θ)
∝
exp {log [fK (θ)] + λ}
∝
fK (θ) .
(7.82)
Finally, in view of the form of fK (θ) given in (7.78),
h (θ) ∝exp

[log h (θ|y∗
K)] p (y∗
K|θ) dy∗
K
%
.
(7.83)
This prior h (θ), which maximized I [e (K) , h (θ)] for each K, was denoted
before πK (θ). Expression (7.83) gives an implicit solution because fK (θ)
depends on the prior through the posterior density h (θ|y∗
K) . However, as
K →∞the posterior density h (θ|y∗
K) will approach an asymptotic form
with density h∗(θ|y∗
K) say, that does not depend on the prior at all. Such
an approximation is given, for example, by the asymptotic process (7.6),
either with I (θ) or 5H as precision matrix. Let the asymptotic posterior
have density h∗(θ|y∗
K) . Then, the sequence of positive functions
h∗
K (θ) ∝exp

[log h∗(θ|y∗
K)] p (y∗
K|θ) dy∗
K
%
,
K = 1, 2, . . . ,
(7.84)

7.5 Priors Conveying Little Information
385
derived from such an asymptotic posterior, will induce a sequence of pos-
terior distributions
πK (θ|y) ∝p (y|θ) h∗
K (θ) ,
K = 1, 2, . . . ,
as in (7.76). Note that p (y|θ) is the density of the actual observations
resulting from the experiment. Then, as in (7.76), the reference posterior
distribution of θ, π (θ|y), is deﬁned as the limiting distribution resulting
from this K-fold replicated “conceptual experiment”
π (θ|y) = lim
K→∞πK (θ|y) ,
(7.85)
where the limit is understood in the information-entropy sense
lim
K→∞

log
πK (θ|y)
π (θ|y)

πK (θ|y) dθ = 0.
The reference prior is a function retrieving the reference posterior π (θ|y)
by formal use of Bayes theorem, i.e., a positive function π (θ), such that,
for all y,
π (θ|y) =
p (y|θ) π (θ)

p (y|θ) π (θ) dθ.
The limiting posterior distribution (7.85) is the same as the one that
could be obtained from the sequence of priors πK (θ) maximizing the ex-
pected information I[e (K) , h (θ)], as obtained from (7.76).
Although the construction guarantees that πK (θ) is proper for each K,
this is not so for π (θ) , the limiting reference prior, as will be shown in some
examples. Hence, only the reference posterior distribution is amenable to
probabilistic interpretation. Also, observe that each of the terms in the se-
quence is the expected value of the logarithm of the density of the asymp-
totic approximation of the posterior taken with respect to the sampling
distribution of the observations generated in the appropriate K-fold repli-
cated experiment. This deﬁnes an algorithm for arriving at the form of the
reference function h∗
K (θ) , with this leading to the reference prior when
K →∞. Reference priors are, thus, limiting forms.
Invariance under Transformation
The invariance under one-to-one transformations is shown in Bernardo
(1979). Let ξ = g (θ) be such a transformation. The reference prior of ξ
should have the form
π (ξ) ∝π

g−1 (ξ)
 """"
dg−1 (ξ)
dξ
"""" ∝π

g−1 (ξ)

|Jξ| ,

386
7. The Prior Distribution and Bayesian Analysis
where |Jξ| is the absolute value of the Jacobian of the transformation. The
sequence of functions approaching the reference prior, using (7.84), is
h∗
K (ξ)
∝
exp

[log h∗(ξ|y∗
K)] p (y∗
K|ξ) dy∗
K
%
∝
exp

[log h∗(θ|y∗
K) |Jξ|] p (y∗
K|θ) dy∗
K
%
∝
exp (log |Jξ|) exp

[log h∗(θ|y∗
K)] p (y∗
K|θ) dy∗
K
%
∝
exp

[log h∗(θ|y∗
K)] p (y∗
K|θ) dy∗
K
%
|Jξ|
∝
h∗
K (θ) |Jξ| .
Hence, the reference functions follow the usual rules for change of variables,
that is, the reference function for ξ is proportional to the product of the
reference function for θ times the absolute value of the Jacobian of the
transformation.
Reference Prior under Consistency and Asymptotic Normality
It is now shown that the reference prior has an explicit form when a consis-
tent estimator of θ exists, and when the asymptotic posterior distribution
of θ, given the hypothetical data y∗
K from a K-fold replicated experiment,
is normal. As mentioned at the beginning of this chapter, important regu-
larity conditions must be satisﬁed to justify these asymptotic assumptions.
Consider an experiment based on n observations. As before, let y∗
K be a
hypothetical data vector of order kn × 1 resulting from a K-fold replicate
of the said experiment. Also, let 5θkn be a suﬃcient statistic or estimator
(a function of y∗
K) such that, with complete certainty,
lim
K→∞
5θkn = θ.
Since 5θkn is suﬃcient for the parameter, the posterior density of θ, given
the data, is the same as the posterior density, given 5θkn. This is so because
the likelihood function can be written as the product of a function of the
data only (which gets absorbed in the integration constant), times another
part involving both θ and 5θkn. Next, let the asymptotic approximation to
the posterior density be
h∗(θ|y∗
K) = h∗
θ|5θkn

.
The counterpart of (7.84) can be written as
h∗
K (θ) ∝exp
 
log h∗
θ|5θkn

p

5θkn|θ

d5θkn
%
,

7.5 Priors Conveying Little Information
387
where p

5θkn|θ

is the density of the sampling distribution of the suﬃcient
statistic or consistent estimator. Now evaluate the asymptotic approxima-
tion at 5θkn = θ, and denote this as:
h∗
θ|5θkn
"""θkn=θ .
This should be very “close” to h∗
θ|5θkn

(in the Kullback–Leibler sense).
Hence, one can write
h∗
K (θ)
∝
exp
 
log h∗
θ|5θkn
"""θkn=θ

p

5θkn|θ

d5θkn
%
∝
exp

log h∗
θ|5θkn
"""θkn=θ
 
p

5θkn|θ

d5θkn
%
∝
h∗
θ|5θkn
"""θkn=θ .
(7.86)
This implies that h∗
K (θ) is proportional to the density of any asymptotic
approximation to the posterior in which the consistent estimator 5θkn is
replaced by the unknown parameter θ.
Suppose now that the asymptotic posterior distribution of θ is normal;
as seen in Section 7.3, the assumption is valid under regularity conditions.
Hence, for a K-fold replicated experiment, write
θ|5θkn ∼N

5θkn,

knI1

5θkn
−1
,
where I1

5θkn

is Fisher’s information measure for a single observation
evaluated at 5θkn. Then, by virtue of (7.86), the reference function must be
h∗
K (θ)
∝
N

θ,

knI1

5θkn
−1""""θkn=θ
∝
1

knI1

5θkn
−1
2 exp

−
knI1(θkn)
2

θ −5θkn
2
"""""""
θkn=θ
∝
>
I1 (θ).
(7.87)
The limit of this sequence, which is the reference prior π (θ), is also
>
I1 (θ),
the square root of Fisher’s information measure for a single observation.
Hence,
π (θ) ∝
>
I1 (θ).
It follows that in the special case of a single continuous parameter and when
the posterior distribution is asymptotically normal, the reference prior al-
gorithm yields Jeﬀreys’ invariance prior (see Section 7.5.2).

388
7. The Prior Distribution and Bayesian Analysis
Example 7.23
Reference prior for the mean of a normal distribution
The experiment consists of n samples from a normal distribution with
unknown mean µ and known variance σ2. Fisher’s information measure
for a single observation is
Ey|µ,σ2

−
d2
(dµ)2

log
1
√
2πσ2 exp

−1
2σ2 (y −µ)2
%
= 1
σ2 .
Hence, the reference prior π (µ) is a constant, since the information measure
does not involve the parameter of interest. Note that the reference prior is
improper. Although, by construction, the reference functions obtained by
maximizing the information measure I [e (K) , h (θ)] are proper for each K,
as shown in (7.79), the reference prior obtained by taking limits may be
improper.
■
Example 7.24
Reference prior for the probability of success in the bino-
mial distribution
The experiment consists of n Bernoulli trials with success probability θ.
The distribution of a single observation in a Bernoulli trial is
p (y|θ) = θy (1 −θ)1−y ,
where y takes the value 1 with probability θ or the value 0 with probability
1 −θ. The information measure from a single draw is
Ey|θ
#
−d2
(dθ)2 [y log θ + (1 −y) log (1 −θ)]
$
= Ey|θ

y
θ2 + (1 −y)
(1 −θ)2

=
1
θ (1 −θ).
The reference prior is then
π (θ) ∝
C
1
θ (1 −θ),
which is a Be
 1
2, 1
2

distribution. The reference prior is proper in this case,
and the reference posterior distribution is
π (θ|y)
∝
θy (1 −θ)n−y θ−1
2 (1 −θ)−1
2
∝
θy+ 1
2 −1 (1 −θ)n−y+ 1
2 −1 ,
where y is the number of successes observed out of n Bernoulli trials.
The posterior is then a Be

y + 1
2, n −y + 1
2

distribution, which is always
proper (even if all trials are all successes or all failures). If all observations

7.5 Priors Conveying Little Information
389
are failures, the mean of the posterior distribution is equal to 1/(2n + 2).
For example, if n = 1000 ﬂies are screened in search of a speciﬁc mutant
and all are found to be normal, the posterior mean estimate of the muta-
tion rate is 1/2002. The Bayesian estimate admits the possibility that the
population is liable to at least some mutation. On the other hand, the ML
estimator of the mutation rate would be 0 in this case.
■
Presence of a Single Nuisance Parameter
Suppose that the model has two unknown parameters, so θ = (θ1, θ2)′ .
Parameter θ1 is of primary inferential interest and θ2 acts as a nuisance
parameter. This will be called an ordered parameterization, denoted as
(θ1, θ2) . The problem is to develop a reference prior for θ1. As usual, the
joint prior density can be written as
h (θ) = h (θ1) h (θ2|θ1) ,
where h (θ2|θ1) is the density of the conditional distribution of the nuisance
parameter, given θ1. Correspondingly, the density of the θ1-reference prior
distribution is expressible as
πθ1 (θ1, θ2) = πθ1 (θ1) πθ1 (θ2|θ1) .
It is important to note that the reference prior may depend on the order
of the parameterization. That is, the θ1-reference prior distribution will be
diﬀerent, in general, from the θ2-reference prior distribution with density
πθ2 (θ2, θ1) = πθ2 (θ2) πθ2 (θ1|θ2) .
This is perplexing at ﬁrst sight, but it can be explained on the grounds
that the information-theoretic measures that are maximized involve loga-
rithmic divergences between distributions, and these depend on the speciﬁc
distributions intervening.
Note that if the conditional reference prior density π (θ2|θ1) were known,
it could be used to integrate the nuisance parameter θ2 out of the likelihood
to obtain the one-parameter model
p (y|θ1) =

p (y|θ1, θ2) π (θ2|θ1) dθ2,
from which π (θ1) can be deduced as before. Then, the reference posterior
for θ1 would be
p (θ1|y) ∝p (y|θ1) π (θ1) .
We now describe the algorithm for obtaining the marginal reference pos-
terior density of θ1. There are essentially two steps. First, the conditional
reference prior for the nuisance parameter, π (θ2|θ1) (dropping the sub-
scripts indexing π from now on), can be arrived at by applying the algo-
rithm presented before for the single parameter situation. This is because,

390
7. The Prior Distribution and Bayesian Analysis
given θ1, p (y|θ1, θ2) only depends on the nuisance parameter θ2. The ref-
erence function is now
h∗
K (θ2|θ1) ∝exp

[log h∗(θ2|θ1, y∗
K)] p(y∗
K|θ)dy∗
K
%
,
K = 1, 2, . . . ,
where h∗(θ2|θ1, y∗
K) is the density of any asymptotic approximation to the
conditional posterior distribution of θ2, given θ1, that does not depend on
the prior. The conditional reference prior, π (θ2|θ1) is obtained as the limit,
as K →∞, of the sequence of reference functions, h∗
K (θ2|θ1), as before.
The ﬁrst step of the algorithm is completed, using π (θ2|θ1) to integrate out
the nuisance parameter, thus obtaining the integrated likelihood p (y∗
K|θ1):
p (y∗
K|θ1) =

p (y∗
K|θ1, θ2) π (θ2|θ1) dθ2.
This step assumes that π (θ2|θ1) is a proper density. Recall, however, that
reference analysis can lead to improper reference priors. If this is the case,
the algorithm will not work. We return to this point brieﬂy at the end of
this section.
In the second step, the algorithm is again applied using as reference
function
h∗∗
K (θ1) ∝exp

[log h∗∗(θ1|y∗
K)] p (y∗
K|θ1) dy∗
K
%
,
K = 1, 2, . . . ,
(7.88)
where h∗∗(θ1|y∗
K) is any asymptotic approximation to the posterior distri-
bution of the parameter of interest, but constructed using the integrated
likelihood p (y∗
K|θ1). The marginal reference prior π (θ1) is obtained as the
limit, as K →∞, of the sequence of reference functions h∗∗
K (θ1). Finally,
the reference posterior of θ1 is obtained as
p (θ1|y) ∝p (y|θ1) π (θ1) ,
which completes the algorithm.
We shall now consider the regular case where joint posterior asymptotic
normality can be established. It is also assumed that the conditional refer-
ence prior is proper. The asymptotic approximation to the joint posterior
distribution of θ = (θ1, θ2)′ is written as
 θ1
θ2

|5θkn ∼N

5θ1kn
5θ2kn

,

Iθ1θ1
Iθ1θ2
Iθ1θ2
Iθ2θ2
−1"""""
θ=θ

.
Recall that Iθ2θ2 is the information about θ2 in a model, either without θ1
or when assuming that this parameter is known. Also, let

Iθ1θ1
Iθ1θ2
Iθ1θ2
Iθ2θ2
−1
=

Iθ1θ1
Iθ1θ2
Iθ1θ2
Iθ2θ2

,

7.5 Priors Conveying Little Information
391
and note than an asymptotic approximation to the marginal posterior dis-
tribution of θ1 is given by
θ1|5θkn ∼N

5θ1kn, Iθ1θ1""
θ=θ

.
(7.89)
With independent samples, the information matrix is nk times the infor-
mation from a single observation, so Iθ1θ1 is equal to Iθ1θ1
1
/nk, where Iθ1θ1
1
is the appropriate part of the inverse of Fisher’s information matrix for a
single observation. Further, Iθ1θ1
1
is typically a function of both θ1 and θ2;
hence, it is instructive to write Iθ1θ1
1
as Iθ1θ1
1
(θ1, θ2).
Employing the argument in (7.87), the reference function for arriving at
the conditional reference prior of the nuisance parameter is
h∗
K (θ2|θ1) ∝
8
I1(θ2θ2).
(7.90)
Hence, the density of the conditional reference prior for the nuisance pa-
rameter is
π (θ2|θ1) ∝
8
I1(θ2θ2).
Since we assume that this is proper, the integrated likelihood is
p (y∗
K|θ1)
=

p (y∗
K|θ1, θ2) π (θ2|θ1) dθ2
∝

p (y∗
K|θ1, θ2)
8
I1(θ2θ2) dθ2.
Consequently, the reference function needed to arrive at the reference prior
for the parameter of interest θ1 in (7.88) has the form
h∗∗
K (θ1)
∝exp

[log h∗∗(θ1|y∗
K)]

p (y∗
K|θ1, θ2)
8
I1(θ2θ2)dθ2

dy∗
K
%
. (7.91)
Rearranging the integral expression
h∗∗
K (θ1)
∝exp
 8
I1(θ2θ2)

[log h∗∗(θ1|y∗
K)] p (y∗
K|θ1, θ2) dy∗
K
%
dθ2
%
∝exp
 8
I1(θ2θ2)Ey∗
K|θ1,θ2 [log h∗∗(θ1|y∗
K)] dθ2
%
,
(7.92)
which is a function of θ1 only since θ2 is integrated out. Now, using the
asymptotic approximation (7.89) to the marginal posterior of θ1, and taking
expectations over the distribution of the observations arising in the K-
fold replicated experiment, for large K (recall that 5θ1kn is asymptotically

392
7. The Prior Distribution and Bayesian Analysis
unbiased)
Ey∗
K|θ1,θ2 [log h∗∗(θ1|y∗
K)]
= Ey∗
K|θ1,θ2


log
1
C
2π

Iθ1θ1
1
(θ1,θ2)
nk
 −
nk

θ1 −5θ1kn
2
2

Iθ1θ1
1
(θ1, θ2)



= −log
@
A
A
B2π

Iθ1θ1
1
(θ1, θ2)
nk

−1
2 ∝log

Iθ1θ1
1
(θ1, θ2)
−1
2 .
Using this in (7.92):
h∗∗
K (θ1) ∝exp
 8
I1(θ2θ2) log

Iθ1θ1
1
(θ1, θ2)
−1
2 dθ2
%
,
(7.93)
and recall that π (θ1) ∝h∗∗
K (θ1) for K →∞. Since (7.93) involves infor-
mation measures (or their inverses) for a single observation, this gives the
reference prior directly. Hence, if the conditional reference prior π (θ2|θ1) is
proper, the reference prior of the primary parameter is obtained as follows:
(1) Compute Fisher’s information measure about the nuisance parameter
from a single observation (acting as if the primary parameter were known
or absent from the model).
(2) Compute Fisher’s information matrix for a single observation for the
full, two-parameter model, and invert it (this may depend on both θ1 and
θ2).
Then proceed to evaluate the integral and the exponential function in
(7.93).
Suppose now that >I1(θ2θ2) factorizes as
8
I1(θ2θ2) ∝fθ2 (θ1) gθ2 (θ2) .
This implies that the conditional reference prior of the nuisance parameter
is
π (θ2|θ1) = agθ2 (θ2) ,
(7.94)
where a−1 =

gθ2 (θ2) dθ2. Also suppose that the inverse of Fisher’s infor-
mation measure (the θ1 part) factorizes as

Iθ1θ1
1
(θ1, θ2)
−1
2 ∝fθ1 (θ1) gθ1 (θ2) .

7.5 Priors Conveying Little Information
393
Then, if the space of the nuisance parameter does not depend on θ1, appli-
cation of these conditions in (7.93) gives
h∗∗
K (θ1) ∝exp

agθ2 (θ2) log [fθ1 (θ1) gθ1 (θ2)] dθ2
%
∝exp

log [fθ1 (θ1)]

agθ2 (θ2) dθ2 +

log [gθ1 (θ2)] agθ2 (θ2) dθ2
%
∝exp {log [fθ1 (θ1)]} ∝fθ1 (θ1) ,
(7.95)
since

agθ2 (θ2) dθ2 = 1. Combining (7.94) and (7.95) yields as θ1-reference
prior,
πθ1 (θ1, θ2) ∝fθ1 (θ1) gθ2 (θ2) .
(7.96)
This result holds irrespective of whether the conditional reference prior is
proper or not.
If the conditional reference prior of the nuisance parameter is not proper,
the technical arguments are more involved. Bernardo and Smith (1994)
indicate that the entire parameter space of θ2, say Θ2, must be broken into
increasing sequences Θ2i, possibly dependent on θ1. For each sequence, one
obtains a normalized conditional reference prior such that
πi (θ2|θ1) =
π (θ2|θ1)

Θ2i
π (θ2|θ1) dθ2
,
i = 1, 2, . . . .
An integrated likelihood is obtained for each i, and a marginal reference
prior πi (θ1) is identiﬁed using the procedures described above, to arrive
at the joint prior {πi (θ1) πi (θ2|θ1)} . The limit of this sequence yields the
desired reference prior. The strategy requires identifying suitable “cuts” of
the parameter space.
Example 7.25
Reference prior for the mean of a normal distribution:
unknown standard deviation
Let n samples be drawn from a normal distribution with mean and stan-
dard deviation µ and σ, respectively, with both parameters unknown. First
we shall derive the reference posterior distribution for the ordered param-
eterization (µ, σ) in which σ acts as a nuisance parameter. Second, the
reference analysis is carried out for the ordered parameterization (σ, µ) .
Fisher’s information measure for a single observation is formed from the
expected negative second derivatives
Ey|µ,σ
#
−
∂2
(∂µ)2 log

1
σ
√
2π exp

−1
2σ2 (y −µ)2
%$
= 1
σ2 ,
Ey|µ,σ

−∂2
∂µ∂σ log

1
σ
√
2π exp

−1
2σ2 (y −µ)2
%%
= 0,

394
7. The Prior Distribution and Bayesian Analysis
Ey|µ,σ
#
−
∂2
(∂σ)2 log

1
σ
√
2π exp

−1
2σ2 (y −µ)2
%$
= 2
σ2 .
Thus, Fisher’s information matrix for n = 1, and arranging it consistently
with the ordered parameterization (µ, σ) , is

I1(µµ)
I1(µσ)
I1(µσ)
I1(σσ)

=


1
σ2
0
0
2
σ2

.
The inverse is

Iµµ
1
Iµσ
1
Iµσ
1
Iσσ
1

=


σ2
0
0
σ2
2

.
The conditional reference prior of σ, given µ, using (7.90), is
π (σ|µ) ∝
8
I1(σσ) ∝
?
2
σ2 ∝1
σ .
Since the conditional reference prior is not proper, one encounters the tech-
nical diﬃculty mentioned earlier. However, we proceed to check whether
the conditions leading to (7.96) hold. Following Bernardo (2001), note that
8
I1(σσ) =
√
2σ−1
factorizes as
8
I1(σσ) = fσ (µ) gσ (σ) ,
where fσ (µ) =
√
2 and gσ (σ) = σ−1. Also, [Iµµ
1 ]−1
2 = σ−1 factorizes as
[Iµµ
1 ]−1
2 ∝fµ (µ) gµ (σ) ,
where fµ (µ) = 1 and gµ (σ) = σ−1. Thus (7.96) leads to the µ−reference
prior
πµ (µ, σ) = π (σ|µ) π (µ) ∝fµ (µ) gσ (σ) = 1 × σ−1 = σ−1.
Hence, the reference prior for the mean is the improper uniform prior and
the joint µ-reference prior is the reciprocal of the standard deviation. The
reference posterior distribution of µ has density
πµ (µ|y)
∝

p (y|µ, σ) πµ (µ, σ) dσ
∝

(σ)−n−1 exp
#
−1
2σ
n

i=1
(yi −µ)2
$
dσ.

7.5 Priors Conveying Little Information
395
Carrying out the integration, as seen earlier in the book, leads to a univariate-
t process with mean y, scale parameter
n
i=1
(yi −y)2
n −1
,
and n −1 degrees of freedom as reference posterior distribution.
■
Example 7.26
Reference prior for the standard deviation of a normal
distribution: unknown mean
The setting is as in Example 7.25 but we now consider the situation where
µ acts as nuisance parameter. Here, >I1(µµ) = 1/σ factorizes as fµ (µ) =
1, gµ (σ) = σ−1, and [Iσσ
1 ]−1
2 =
√
2/σ factorizes as fσ (µ) =
√
2 times
gσ (σ) = σ−1. This leads to the σ-reference prior
πσ (µ, σ) ∝fµ (µ) gσ (σ) = 1 × σ−1 = σ−1,
which is identical to the joint µ-reference prior in this case. The reference
posterior density is then
πσ (σ|y) ∝

(σ)−n−1 exp
#
−1
2σ
n

i=1
(yi −µ)2
$
dµ.
This integration leads to a scaled inverted chi-square density with parame-
ters (n −1) n
i=1 (yi −y)2 and n −1. Equivalently, the reference posterior
distribution for inferring σ is the process
Ga




n −1
2
,
(n −1)
n
i=1
(yi −y)2
2



.
■
Example 7.27
Reference prior for a standardized mean
Following Bernardo and Smith (1994) and Bernardo (2001), consider in-
ferring φ = µ/σ, a standardized mean or reciprocal of the coeﬃcient of
variation of a normal distribution, with the standard deviation σ acting as
a nuisance parameter. The sampling model is as in the two preceding exam-
ples, but parameterized in terms of φ. In order to form Fisher’s information
measure for a single observation, the required derivatives are
Ey|φ,σ
#
−
∂2
(∂φ)2 log

1
σ
√
2π exp

−1
2σ2 (y −φσ)2
%$
= 1,
Ey|φ,σ

−∂2
∂φ∂σ log

1
σ
√
2π exp

−1
2σ2 (y −φσ)2
%%
= φ
σ ,

396
7. The Prior Distribution and Bayesian Analysis
Ey|φ,σ
#
−
∂2
(∂σ)2 log

1
σ
√
2π exp

−1
2σ2 (y −φσ)2
%$
= 2 + φ2
σ2
.
The information matrix is, thus,

I1(φφ)
I1(φσ)
I1(φσ)
I1(σσ)

=


1
φ
σ
φ
σ
2 + φ2
σ2

,
with inverse

Iφφ
1
Iφσ
1
Iφσ
1
Iσσ
1

=


1 + φ2
2
−φσ
2
−φσ
2
σ2
2

.
If σ is the nuisance parameter, then
8
I1(σσ) ∝
8
2 + φ2σ−1,
factorizes into fσ (φ) =
>
2 + φ2 and gσ (σ) = σ−1. Further

Iφφ
1
−1
2 ∝

1 + φ2
2
−1
2
factorizes into fφ (φ) =

1 + φ2
2
−1
2 and gφ (σ) = 1. The φ−reference prior
is then given by
πφ (φ, σ) ∝fφ (φ) gσ (σ) ∝

1 + φ2
2
−1
2
σ−1.
■
Multiparameter Models
In a multiparameter situation, the reference posterior distribution is rela-
tive to an ordered parameterization of inferential interest. For example, if
the ordered parameter vector is:
θ =
 θ1
θ2
.
.
.
θm
′ ,
so that θm is of primary inferential interest, the corresponding reference
prior can be represented as
πθm (θ) = πθm (θm|θ1, θ2, ..., θm−1) πθm (θm−1|θ1, θ2, ..., θm−2) × · · ·
· · · × πθm (θ2|θ1) πθm (θ1) .

7.5 Priors Conveying Little Information
397
An algorithm for deriving the reference prior under asymptotic normality
(an extension of the procedure described for the two parameter situation)
is presented in Berger and Bernardo (1992) and in Bernardo and Smith
(1994). The method is quite involved algebraically and is not presented
here.
As a ﬁnal comment, the reader should be aware that there is no consen-
sus among Bayesians on this topic. For example, Lindley, in the discussion
of Bernardo (1979), remarks that “... the distributions derived by this pro-
cedure violate the likelihood principle, and therefore violate requirements
of coherence ....” Lindley is referring to the foundational inconsistency of
the method which is based on integrating over the sample space of the data.
Further, McCulloch concludes his discussion of Berger and Bernardo (1992)
with the following words: “We all use ‘noninformative’ priors and this work
is probably the most important current work in the area. I found the pa-
pers very, um ... er ... ah ..., interesting. If the authors obtain impossible
solutions it is because they are working on an impossible problem.”

This page intentionally left blank

8
Bayesian Assessment of
Hypotheses and Models
8.1
Introduction
The three preceding chapters gave an overview of how Bayesian probability
models are constructed. Once a prior distribution is elicited and the form
of the likelihood function is agreed upon, Bayesian analysis is conceptually
straightforward: nuisance parameters are eliminated via integration and
parameters of interest are inferred from their marginal posterior distribu-
tions. Further, yet-to-be-observed random variables can be predicted from
the corresponding predictive distributions. In all cases, probability is the
sole measure of uncertainty at each and everyone of the stages of Bayesian
learning.
Inferences are expected to be satisfactory if the entire probability model
(the prior, the likelihood, and all assumptions made) is a “good one”, in
some sense. In practice, however, agreement about the model to be used
is more the exception than the rule, unless there is some well-established
theory or mechanism underlying the problem. For example, a researcher
may be uncertain about which hypothesis or theory holds. Further, al-
most always, there are alternative choices about the distributional form
to be adopted or about the explanatory variables that should enter into
a regression equation, say. Hence, it is important to take into account un-
certainties about the model-building process. This is perfectly feasible in
Bayesian analysis and new concepts do not need to be introduced in this
respect. If there is a set of competing models in a certain class, each of the
models in the set can be viewed as a diﬀerent state of a random variable.

400
8. Bayesian Assessment of Hypotheses and Models
The prior distribution of this variable (the model) is updated using the
information contained in the data, to arrive at the posterior distribution of
the possible states of the model. Then inferences are drawn, either from the
most probable model, a posteriori, or from the entire posterior distribution
of the models, in a technique called Bayesian model averaging.
In this chapter, several concepts and techniques for the Bayesian eval-
uation of hypotheses and models are presented. Some of the approaches
described are well founded theoretically; others are of a more exploratory
nature. The next section deﬁnes the posterior probability of a model and
an intimately related concept: the Bayes factor. Subsequently, the issue of
“testing hypotheses” is presented from a Bayesian perspective. Approxi-
mations to the Bayes factor and extensions to the concept are suggested.
A third section presents some methods for calculating the Bayes factor,
including Monte Carlo procedures, since it is seldom the case that one can
arrive at the desired quantities by analytical methods. The fourth and ﬁfth
sections present techniques for evaluating goodness of ﬁt and the predictive
ability of a model. The ﬁnal section provides an introduction to Bayesian
model averaging, with emphasis on highlighting its theoretical appeal from
the point of view of predicting future observations.
8.2
Bayes Factors
8.2.1
Deﬁnition
Suppose there are several competing theories, hypotheses, or models about
some aspect of a biological system. For example, consider diﬀerent theories
explaining how a population evolves. These theories are mutually exclusive
and exhaustive (at least temporarily). The investigator assigns prior proba-
bility p (Hi) , (i = 1, 2, ..., K) to hypothesis, or theory i, with 
i p (Hi) = 1.
There is no limit to K and nesting requirements are not involved. After
observing data y, the posterior probability of hypothesis i is
p (Hi|y) =
p (Hi) p (y|Hi)
K

i=1
p (Hi) p (y|Hi)
,
i = 1, 2, . . . , K,
(8.1)
where p (y|Hi) is the probability of the data under hypothesis i. If all
hypotheses are equally likely a priori, which is the maximum entropy or
reference prior in the discrete case (Bernardo, 1979), then
p (Hi|y) =
p (y|Hi)
K

i=1
p (y|Hi)
.

8.2 Bayes Factors
401
The posterior odds ratio of hypothesis i relative to hypothesis j takes the
form
p (Hi|y)
p (Hj|y) = p (Hi)
p (Hj)
p (y|Hi)
p (y|Hj).
(8.2)
It follows that the posterior odds ratio is the product of the prior odds
ratio and of the ratio between the marginal probabilities of observing the
data under each of the hypotheses. The Bayes factor is deﬁned to be
Bij = p (y|Hi)
p (y|Hj) =
p(Hi|y)
p(Hj|y)
p(Hi)
p(Hj)
= posterior odds ratio
prior odds ratio
.
(8.3)
According to Kass and Raftery (1995) this terminology is apparently due
to Good (1958). A Bij > 1 means that Hi is more plausible than Hj in the
light of y. While the priors are not visible in the ratio p (y|Hi) /p (y|Hj),
because algebraically they cancel out, this does not mean that Bij in general
is not aﬀected by prior speciﬁcations. This point is discussed below.
It is instructive to contrast this approach with the one employed in stan-
dard statistical analysis. In classical hypothesis testing, a null hypothesis
H0 : θ ∈θ0 and an alternative hypothesis H1 : θ ∈θ1 are speciﬁed. The
choice between these hypotheses is driven by the distribution under H0 of
a test statistic that is a function of the data (it could be the likelihood
ratio), T (y) , and by the so-called p-value. This is deﬁned as
Pr [T (y) at least as extreme as the value observed|θ, H0] .
(8.4)
Then H0 is accepted (or rejected, in which case H1 is accepted) if the
p-value is large (small) enough, or one may just quote the p-value and
leave things there. Notice that (8.4) represents the probability of obtaining
results larger than the one actually obtained; that is, (8.4) is concerned
with events that might have occurred, but have not. Thus, the famous
quotation from Jeﬀreys (1961):
“What the use of p implies, therefore, is that a hypothesis
which may be true may be rejected because it has not predicted
observable results which have not occurred. ... On the face of
it the fact that such results have not occurred might more rea-
sonably be taken as evidence for the law, not against it.”
Often (and incorrectly), the p-value is interpreted as the probability that
H0 holds true. The interpretation in terms of probability of hypotheses,
p [H0|T (y) = t (y)], which is the Bayesian formulation of the problem, is
conceptually more straightforward than the one associated with (8.4). De-
spite its conceptual clarity, the Bayesian approach is not free from prob-
lems. Perhaps not surprisingly, these arise especially in cases when prior
information is supposed to convey vague knowledge.

402
8. Bayesian Assessment of Hypotheses and Models
8.2.2
Interpretation
The appeal of the Bayes factor as formulated in (8.3), is that it provides
a measure of whether the data have increased or decreased the odds of
Hi relative to Hj. This, however, does not mean that in general, the Bayes
factor is driven by the data only. It is only when both Hi and Hj are simple
hypotheses, that the prior inﬂuence vanishes and the Bayes factor takes the
form of a likelihood ratio. In general, however, the Bayes factor depends
on prior input, a point to which we will return.
Kass and Raftery (1995) give guidelines for interpreting the evidence
against some “null hypothesis”, H0. For example, they suggest that a Bayes
factor larger than 100 should be construed as “decisive evidence” against
the null. Note that a Bayes factor under 1 means that there is evidence in
support of H0. When working in a logarithmic scale, 2 log Bij, for example,
the values are often easier to interpret by those who are familiar with
likelihood ratio tests. It should be made clear from the onset that the
Bayes factor cannot be viewed as a statistic having an asymptotic chi-
square distribution under the null hypothesis. Again, Bij is the quantity
by which prior odds ratios are increased (or decreased) to become posterior
odd ratios.
There are many diﬀerences between the Bayes factor and the usual like-
lihood ratio statistic. First, the intervening p (y|Hi) is not the classical
likelihood, in general. Recall that the Bayesian marginal probability (or
density) of the data is arrived at by integrating the joint density of the
parameters and of the observations over all values that the parameters can
take in their allowable space. For example, if hypothesis or model Hi has
parameters θi, then for continuous data and continuous valued parameter
vector
p (y|Hi)
=

p (y|θi, Hi) p (θi|Hi) dθi
=
Eθi|Hi [p (y|θi, Hi)] .
(8.5)
The marginal density is, therefore, the expected value of all possible likeli-
hoods, where the expectation is taken with respect to the prior distribution
of the parameters. In likelihood inference, no such integration takes place
unless the “parameters” are random variables having a frequentist interpre-
tation. Since, in turn, these random variables have distributions indexed by
parameters, the classical likelihood always depends on some ﬁxed, unknown
parameters. In the Bayesian approach, on the other hand, any dependence
of the marginal distribution of the data is with respect to any hyperparam-
eters the prior distribution may have, and with respect to the form of the
model. In fact, p (y|Hi) is the prior predictive distribution and gives the
density or probability of the data calculated before observation, uncondi-
tionally with respect to parameter values.

8.2 Bayes Factors
403
A second important diﬀerence is that the Bayes factor is not explicitly
related to any critical value deﬁning a rejection region of a certain size.
For example, the usual p-values in classical hypothesis testing cannot be
interpreted as the probabilities that either the null or the alternative hy-
potheses are “true”. The p-value arises from the distribution of the test
statistic (under the null hypothesis) in conceptual replications of the ex-
periment. In contrast, the Bayes factor and the prior odds contribute di-
rectly to forming the posterior probabilities of the hypotheses. In order to
illustrate, suppose that two models are equally probable, a priori. Then a
Bayes factor B01 = 19, would indicate that the null hypothesis or model is
19 times more probable than its alternative, and that the posterior proba-
bility that the null model is true is 0.95. On the other hand, in a likelihood
ratio test, a value of the test statistic generating a p-value of 0.95 as de-
ﬁned by (8.4) cannot be construed as evidence that the null hypothesis has
a 95% chance of being true.
8.2.3
The Bayes Factor and Hypothesis Testing
Decision-Theoretic View
In Bayesian analysis, “hypothesis testing” is viewed primarily as a decision
problem (e.g., Zellner, 1971). Suppose there are two hypotheses or models:
H0 (null) and H1 (alternative). If one chooses H0 when H1 is “true”, then
a loss L10 is incurred. Similarly, when H1 is adopted when the null holds,
the loss is L01. Otherwise, there are no losses.
The posterior expectation of the decision “accept the null hypothesis” is
E (loss|accept H0, y)
=
0 × p (H0|y) + L10 p (H1|y)
=
L10 p (H1|y) .
Likewise, the expected posterior loss of the decision “accept the alternative”
is
E (loss|reject H0, y)
=
0 × p (H1|y) + L01 p (H0|y)
=
L01 p (H0|y) .
Naturally, if the expected posterior loss of accepting H0 is larger than that
of rejecting it, one would decide to reject the null hypothesis. Then the
decision rule is
if E (loss|reject H0, y) < E (loss|accept H0, y ) →reject H0.
The preceding is equivalent to
L01 p (H0|y) < L10 p (H1|y) ,

404
8. Bayesian Assessment of Hypotheses and Models
or, in terms of (8.3),
B10 = p (y|H1)
p (y|H0) > L01 p (H0)
L10 p (H1).
(8.6)
This indicates that the null hypothesis is to be rejected if the Bayes factor
(ratio of marginal likelihoods under the two hypotheses or models) for the
alternative, relative to the null, exceeds the ratio of the prior expected
losses. Note that L01 p (H0) is the expected prior loss of rejecting the null
when this is true; L10 p (H1) is the expected prior loss that results when H1
is true and one accepts H0. Then the ratio of prior to posterior expected
losses
L01 p (H0)
L10 p (H1)
plays the role of the “critical” value in classical hypothesis testing. If one
views the Bayes factor as the “test statistic”, the critical value is higher
when one expects to loose more from rejecting the null than from accepting
it. In other words, the larger the prior expected loss from rejecting the null
(when this hypothesis is true) relative to the prior expected loss of accepting
it (when H1 holds), the larger the weight of the evidence should be in favor
of the alternative, as measured by the Bayes factor.
If the losses are such that L01 = L10, it follows from (8.6) that the
decision rule is simply
B10 = p (y|H1)
p (y|H0) > p (H0)
p (H1).
This implies that if the two models or hypotheses are equiprobable a priori,
then the alternative should be chosen over the null whenever the Bayes
factor exceeds 1. Similarly, a “critical value” of 10 should be adopted if it
is believed a priori that the null hypothesis is 10 times more likely than
the alternative. In all cases, it must be noted that the “accept” or “reject”
framework depends nontrivially on the form of the loss function, and that
adopting L01 = L10 may not be realistic in many cases.
The deﬁnition in the form of (8.6) highlights the importance, in Bayesian
testing, of deﬁning non-zero a priori probabilities. This is so even though
the Bayes factor can be calculated without specifying p (H0) and p (H1).
If H0 or H1 are a priori impossible, the observations will not modify this
information.
Bayesian Comparisons
Contrasting Two Simple Hypotheses
The deﬁnition of the Bayes factor in (8.3) as a ratio of marginal densities
does not make explicit the inﬂuence of the prior distributions. With one

8.2 Bayes Factors
405
exception, Bayes factors are aﬀected by prior speciﬁcations. The excep-
tion occurs when the comparison involves two simple hypotheses. In this
case, under H0, a particular value θ0 is assigned to the parameter vector,
whereas under H1, another value θ = θ1 is posited. There is no uncer-
tainty about the value of the parameter under any of the two competing
hypotheses. Then one can express the discrete prior probability of hypoth-
esis i as p (Hi) = Pr (θ = θi), and the conditional p.d.f. for y given Hi as
p (y|Hi) = p (y|θ = θi). The Bayes factor for the alternative against the
null is then
B10 = posterior odds
prior odds
= p (y|θ = θ1)
p (y|θ = θ0).
(8.7)
In this particular situation, the Bayes factor is the odds for H1 relative
to H0 given by the data only. Expression (8.7) is a ratio of standard like-
lihoods, where the values of the parameters are completely speciﬁed. In
general, however, B10 depends on prior input. When a hypothesis is not
simple, in order to arrive at the form equivalent to (8.7), one must compute
the expectation of the likelihood of θi with respect to the prior distribu-
tion. For continuously distributed values of the vector θi and prior density
p (θi|Hi), one writes
p (y|Hi) =

p (y|θi, Hi) p (θi|Hi) dθi.
In contrast to the classical likelihood ratio frequentist test, the Bayes
factor does not impose nesting restrictions concerning the form of the
likelihood functions, as illustrated in the following example adapted from
Bernardo and Smith (1994).
Example 8.1
Two fully speciﬁed models: Poisson versus negative bino-
mial process
Two completely speciﬁed models are proposed for counts. A sample of size
n with values y1, y2, . . . , yn is drawn independently from some population.
Model P states that the distribution of the observations is Poisson with
parameter θP . Then the likelihood under this model is
p (y|θ = θP ) =
n
-
i=1
θyi
P exp (−θP )
yi!

=
θny
P
exp (nθP )
n7
i=1
yi!
.
(8.8)
Model N proposes a negative binomial distribution with parameter θP .
The corresponding likelihood is
p (y|θ = θN) =
n
-
i=1
[θN (1 −θN)yi] = θn
N (1 −θN)ny .
(8.9)

406
8. Bayesian Assessment of Hypotheses and Models
The Bayes factor for Model N relative to Model P is then
BNP =

1 −θN
θP
ny
θn
N

exp (nθP )
n7
i=1
yi!
−1 ,
and its logarithm can be expressed as
log BNP = n

y log

1 −θN
θP

+ log (θN)

+ nθP +
n

i=1
log (yi!) .
■
Simple Versus Composite Hypotheses
A second type of comparison is one where one of the models (Model 0 =
M0) postulates a given value of the parameter, whereas the other model
(Model 1 = M1) allows the unknown parameter to take freely any of its
values in the allowable parameter space. This is called a simple versus
composite test (Bernardo and Smith, 1994), and the Bayes factor in this
case takes the form
B10 = p (y|M1)
p (y|M0) =

p (y|θ,M1) p (θ|M1) dθ
p (y|θ = θ0, M0)
,
where p (θ|M1) is the density of the prior distribution of the parameter
vector under the assumptions of Model 1.
There is an interesting relationship between the posterior probability
that θ = θ0 and the Bayes factor (Berger, 1985). Denote the prior proba-
bility of models 0 and 1 as p (M0) and p (M1), respectively, with p (M0) +
p (M1) = 1. The term p (M0) can also be interpreted as the prior probability
that θ = θ0. Then, the posterior probability that θ = θ0 is
Pr (θ = θ0|y) = p (M0) p (y|θ = θ0, M0)
p (y)
.
The constant term in the denominator is given by
p (y) = p (y|θ = θ0, M0) Pr (θ = θ0|M0) p (M0)
+

p (y|θ,M1) p (θ|M1) p (M1) dθ
= p (M0) p (y|θ = θ0, M0) + p (M1)

p (y|θ,M1) p (θ|M1) dθ,

8.2 Bayes Factors
407
Genotypes
AB/ab
Ab/ab
aB/ab
ab/ab
Phenotype
AB
Ab
aB
ab
Frequency
1
2 (1 −r)
1
2r
1
2r
1
2 (1 −r)
Observed
a
b
c
d
TABLE 8.1. Genotypic distribution in oﬀspring from a backcross design.
with the equality arising in the last line because Pr (θ = θ0|M0) = 1. Sub-
stituting above yields
Pr (θ = θ0|y) =

1 + p (M1)
p (M0)B10
−1
.
It is important to mention that in evaluating a point null hypothesis
θ = θ0, say, θ0 must be assigned a positive probability a priori. The point
null hypothesis cannot be tested invoking a continuous prior distribution,
since any such prior will give θ0 prior (and therefore posterior) probability
of zero.
In contrast to the traditional likelihood ratio, the test of a parameter
value on the boundary of the parameter space using the Bayes factor does
not in principle create diﬃculties. This being so because asymptotic distri-
butions and series expansions do not come into play. Such a test is illus-
trated in the example below.
Example 8.2
A point null hypothesis: assessing linkage between two loci
The problem consists of inferring the probability of recombination r be-
tween two autosomal loci A and B, each with two alleles. The parameter r
is deﬁned in the closed interval

0, 1
2

, with the upper value corresponding
to the situation where there is no linkage. We wish to derive the poste-
rior distribution of the recombination fraction between the two loci, and to
contrast the two models that follow. The null model (M0) postulates that
segregation is independent (that is, r = 1
2), whereas the alternative model
(M1) claims that the loci are linked (that is, r < 1
2).
Let the alleles at the corresponding loci be A, a, B, and b, where A
and B are dominant alleles. Suppose that a line consisting of coupling
heterozygote individuals AB/ab is crossed to homozygotes ab/ab. Hence
four oﬀspring classes can be observed: AB/ab, Ab/ab, aB/ab, and ab/ab.
Let n = a + b + c + d be the total number of oﬀspring observed. The four
possible genotypes resulting from this cross, their phenotypes, the expected
frequencies and the observed numbers are shown in Table 8.1. The expected
relative frequencies follow from the fact that if the probability of observing a
recombinant is r, the individual can be either Ab/ab or aB/ab, with the two
classes being equally likely. A similar reasoning applies to the observation
of a non-recombinant type.

408
8. Bayesian Assessment of Hypotheses and Models
Suppose that the species under consideration has 22 pairs of autosomal
chromosomes. In the absence of prior information about loci A and B, it
may be reasonable to assume that the probability that these are located
on the same chromosome (and therefore, linked, so that r < 1
2) is
1
22. This
is so because the probability that 2 randomly picked alleles are in a given
chromosome is
 1
22
2, and there are 22 chromosomes in which this can
occur. Hence, a priori, p (M1) =
1
22 and p (M0) = 21
22; the two models are
viewed as mutually exclusive and exhaustive.
Next, we must arrive at some reasonable prior distribution for r under
M1. Here, a development by Smith (1959) is followed. First, note that the
recombination fraction takes the value r =
1
2 with prior probability 21
22.
Further, assume a uniform distribution for r otherwise (provided one can
view the values 0 < r < 1
2 as “equally likely”). Then the density of this
uniform distribution, p (r|M1) must be such that
Pr

r < 1
2

=
1
2

0
p (u|M1) du = 1
22.
Solving for the desired uniform density gives
p (r|M1) = 1
11.
Therefore the prior is the uniform process p (r|M1) =
1
11 for 0 < r < 1
2,
and the point mass 21
22 at r = 1
2. That is, Pr

r = 1
2

= p (M0) = 21
22. Note
that given M0, Pr

r = 1
2|M0

= 1.
Given the data y = (a, b, c, d)′ in Table 8.1, the conditional distribution
of the observations under linkage has the multinomial form
p (y|r)
=
n!
a!b!c!d!
1
2 (1 −r)
a 
1
2r
b 
1
2r
c 1
2 (1 −r)
d
∝

1
2
n
(1 −r)a+d rb+c.
Under no linkage
p

y|r = 1
2

=
n!
a!b!c!d!

1
4
a+b+c+d
∝

1
4
n
.
Therefore the posterior odds ratio is given by
p (M1|y)
p (M0|y) = p (M1)
p (M0)
 1
2
0 p (r|M1) p (y|r, M1) dr
p

y|r = 1
2, M0

= 1
21
1
11
 1
2
n  1
2
0 rb+c (1 −r)a+d dr
 1
4
n
,

8.2 Bayes Factors
409
where
B10 =
 1
2
0 p (r|M1) p (y|r, M1) dr
p

y|r = 1
2, M0

.
The posterior probability of linkage is
Pr

r < 1
2|y,M1

=
1
2

0
p (r|y, M1) dr,
where
p (r|y, M1) = p (r|M1) p (y|r, M1)
p (y)
,
(8.10)
whereas the posterior probability of no linkage is
Pr

r = 1
2|y, M0

= 1 −Pr

r < 1
2|y, M1

.
The denominator in (8.10) is equal to
p (y) = p (M0) p

y|r = 1
2, M0

+p (M1)

1
2
0
p (r|M1) p (y|r, M1) dr.
The integrals in these expressions can easily be evaluated numerically.
■
Example 8.3
Lindley’s paradox
This problem was brought to light initially by Lindley (1957). The data
sampling involves n independent draws from N

µ, σ2
, with σ2 known and
µ to be inferred. Model 0 corresponds to the simple or sharp hypothesis
that µ = µ0. Model 1 takes σ2 as known and µ as unknown, with its
prior distribution being N

µ1, σ2
1

; the hyperparameters are assumed to
be known. This corresponds to a classical setting in which Model 0 is the
null hypothesis µ = µ0, and Model 1 is the alternative that the parameter
can take any value other than µ = µ0.
The marginal density of the data under Model 0 is
p0

y|µ0, σ2
=

1
√
2πσ2
n
exp

−1
2σ2
n

i=1
(yi −µ0)2

=

1
√
2πσ2
n
exp

−1
2σ2
n

i=1
(yi −y)2

exp

−n
2σ2 (y −µ0)2
.
(8.11)

410
8. Bayesian Assessment of Hypotheses and Models
The marginal density under Model 1 can be written as
p1

y|µ1, σ2
1, σ2
=

p

y|µ, σ2
p

µ|µ1, σ2
1

dµ
=

1
√
2πσ2
n
exp

−1
2σ2
n

i=1
(yi −y)2

1
>
2πσ2
1

exp

−n
2σ2 (y −µ)2
exp

−(µ −µ1)2
2σ2
1

dµ.
(8.12)
The Bayes factor for Model 0 relative to Model 1 is given by the ratio
between (8.11) and (8.12)
B01 =
exp

−n
2σ2 (y −µ0)2
1
√
2πσ2
1

exp
9
−1
2

n
σ2 (µ −y)2 + (µ−µ1)2
σ2
1
:
dµ
.
(8.13)
Now the two quadratic forms on µ in the integrand can be combined in the
usual manner, leading to
n
σ2 (µ −y)2 + (µ −µ1)2
σ2
1
=

 n
σ2 + 1
σ2
1

(µ −5µ)2
+ n
σ2

 n
σ2 + 1
σ2
1
−1 1
σ2
1
(y −µ1)2 .
Above
5µ =

 n
σ2 + 1
σ2
1
−1 
ny
σ2 + µ1
σ2
1

.
Carrying out the integration, the Bayes factor becomes, after some algebra,
B01 =
@
A
A
B
σ2
1

n
σ2 +
1
σ2
1
−1
exp

−n
2σ2 (y −µ0)2
exp

−n
2σ2

n
σ2 +
1
σ2
1
−1
1
σ2
1 (y −µ1)2
.
(8.14)
Now examine what happens when the prior information becomes more and
more diﬀuse, that is, eventually σ2
1 is so large that 1/σ2
1 is near 0. The
Bayes factor is, approximately,
B01 ≈
?
σ2
1σ2
n
exp

−n
2σ2 (y −µ0)2
.
For any ﬁxed value of y, the Bayes factor goes to ∞when σ2
1 →∞, which
implies that p (Model 0|y) →1. This means that no matter what the value
of y is, the null hypothesis would tend to be favored, even for values of

8.2 Bayes Factors
411
"""(y −µ0) /
>
σ2/n
""" that are large enough to cause rejection of the null at
any, arbitrary, “signiﬁcance” level in classical testing. This result, known
as “Lindley’s paradox”, illustrates that a comparison of models in which
one of the hypothesis is “sharp” (simple), strongly depends on the form
of the prior distribution. In particular, when the distribution is improper,
the Bayes factor leads to acceptance of the null. O’Hagan (1994) concludes
that improper priors cannot be used when comparing models. However,
the problem is not avoided entirely by adopting vague uniform priors over
some large but ﬁnite range. This will be discussed later.
■
Comparing Two Composite Hypotheses
Third, the comparison may be a “composite versus composite”, that is, one
where the two models allow their respective parameters to take any values
in the corresponding spaces. Here the Bayes factor is
B10 = p (y|M1)
p (y|M0) =

p (y|θ1,M1) p1 (θ1|M1) dθ

p (y|θ0, M0) p0 (θ0|M0) dθ .
(8.15)
In general, as is the case with likelihood ratios, all constants appearing
in p (y|θi,Mi) must be included when computing B10.
Example 8.4
Marginal distributions and the Bayes factor in Poisson
and negative binomial models
The setting is as in Example 8.1, but the parameter values are allowed to
take any values in their spaces. Following Bernardo and Smith (1994), take
as prior distribution for the Poisson parameter
θP ∼Ga (aP , bP ) ,
and for the parameter of the negative binomial model adopt as prior the
Beta distribution
θN ∼Be (aN, bN) .
The a′s and b′s are known hyperparameters. The marginal distribution of
the data under the Poisson model, using (8.8) as likelihood function (sup-
pressing the dependence on hyperparameters in the notation), is obtained
as
p (y|P) =
 θny
P exp (−nθP )
n7
i=1
yi!
baP
P
Γ (aP )θaP −1
P
exp (−bP θP ) dθP
=
baP
P
Γ (aP )
n7
i=1
yi!

θny+aP −1
P
exp [−(n + bP ) θP ] dθP .
(8.16)
The integrand is the kernel of the density of the
θP ∼Ga (ny + aP , n + bP )

412
8. Bayesian Assessment of Hypotheses and Models
distribution. Hence, the marginal of interest is
p (y|P) =
Γ (aP + ny) baP
P
Γ (aP ) (n + bP )aP +ny
n7
i=1
yi!
.
(8.17)
Similarly, using (8.9), the marginal distribution of the data under the neg-
ative binomial model takes the form
p (y|N) =

θn
N (1 −θN)ny Γ (aN + bN)
Γ (aN) Γ (bN)θaN−1
N
(1 −θN)bN−1 dθN
= Γ (aN + bN)
Γ (aN) Γ (bN)

θaN+n−1
N
(1 −θN)bN+ny−1 dθN.
The integrand is the kernel of a beta density, so the integral can be evalu-
ated analytically, yielding
p (y|N) = Γ (aN + bN)
Γ (aN) Γ (bN)
Γ (aN + n) Γ (bN + ny)
Γ (aN + n + bN + ny) .
(8.18)
The Bayes factor in favor of the N model relative to the P model is given by
the ratio between (8.18) and (8.17). Note that the two marginal densities
and the Bayes factor depend only on the data and on the hyperparameters,
contrary to the ratio of likelihoods. This is because all unknown parame-
ters are integrated out in the process of ﬁnding the marginals. It cannot be
overemphasized that all integration constants must be kept when calculat-
ing the Bayes factors. In classical likelihood ratio tests, on the other hand,
only those parts of the density functions that depend on the parameters
are kept.
■
8.2.4
Inﬂuence of the Prior Distribution
From its deﬁnition, and from Example 8.4, it should be apparent that the
Bayes factor depends on the prior distributions adopted for the competing
models. The exception is when two simple hypotheses are at play. For the
Poisson versus Negative Binomial setting discussed above, Bernardo and
Smith (1994) give numerical examples illustrating that minor changes in
the values of the hyperparameters produce changes in the direction of the
Bayes factors. This dependence is illustrated with a few examples in what
follows. Before we do so, note that one can write

p (y|Mi) dy
=
 
p (y|θi, Mi) p (θi|Mi) dθidy
=

p (θi|Mi)

p (y|θi, Mi) dy

dθi
=

p (θi|Mi) dθi,

8.2 Bayes Factors
413
where the last equality follows because

p (y|θi, Mi) dy =1. The message
here is that when p (θi|Mi) is improper, so is p (y|Mi). In this case, the
Bayes factor is not well deﬁned. This is discussed further in Subsection
8.2.5 below.
Example 8.5
Inﬂuence of the bounds of a uniform prior
Let the sampling model be yi|µ ∼N (µ, 1) and let the prior distribution
adopted for µ under Model 1 be uniform over [−L, L] . Model 2 postulates
the same sampling model but the bounds are [−αL, αL] , where α is a
known, positive, real number. Suppose n independent samples are drawn,
so that the marginal density of the data under Model 2 is
p (y|α, L) =
αL

−αL

1
√
2π
n
exp

−1
2
n

i=1
(yi −µ)2

1
2αLdµ
=
1
2αL

1
√
2π
n
exp

−1
2
n

i=1
(yi −y)2

αL

−αL
exp

−n
2 (µ −y)2
dµ.
The integrand is in a normal form and can be evaluated readily, yielding
p (y|α, L) =
1
2αL

1
√
2π
n
exp

−1
2
n

i=1
(yi −y)2

×

Φ

αL −y
8
1
n

−Φ

−αL −y
8
1
n




?
2π 1
n.
The Bayes factor for Model 2 relative to Model 1 is
B21 = p (y|α, L)
p (y|1, L) =
Φ

αL−y
√
1
n

−Φ

−αL−y
√
1
n

α

Φ

L−y
√
1
n

−Φ

−L−y
√
1
n
.
This clearly shows that the Bayes factor is sensitive with respect to the
value of α. This is relevant in conjunction with the problem outlined in
Example 8.3: the diﬃculties caused by improper priors in model selection
via the Bayes factors are not solved satisfactorily by adopting, for example,
bounded uniform priors. The Bayes factor depends very strongly on the
width of the interval used.
■
Example 8.6
The Bayes factor for a simple linear model
Consider the linear model
y = β + e,

414
8. Bayesian Assessment of Hypotheses and Models
where the variance of the residual distribution, σ2, is known, so it can
be set equal to 1 without loss of generality. Model 1 posits as prior dis-
tribution β ∼N

0, Iσ2
1

, whereas for Model 2 the prior distribution is
β ∼N

0, Iσ2
2

. Since the sampling model and the priors are both normal, it
follows that the marginal distributions of the data are normal as well. The
means and variances of these distributions are arrived at directly by taking
expectations of the sampling model with respect to the appropriate prior.
One gets y|σ2, σ2
1 ∼N

0, I

σ2
1 + σ2
and y|σ2, σ2
2 ∼N

0, I

σ2
2 + σ2
for Models 1 and 2, respectively. The Bayes factor for Model 1 relative to
Model 2 is
B12
=
n7
i=1
1
	
2π(σ2
1+1) exp

−
y2
i
2(σ2
1+1)

n7
i=1
1
	
2π(σ2
2+1) exp

−
y2
i
2(σ2
2+1)

=

σ2
2 + 1
σ2
1 + 1
 n
2 exp

y′y
2(σ2
2+1)

exp

y′y
2(σ2
1+1)
.
Taking logarithms and multiplying by 2, to arrive at the same scale as the
likelihood ratio statistic, yields
2 log (B12) = n log

σ2
2 + 1
σ2
1 + 1

+ y′y

σ2
1 −σ2
2
(σ2
1 + 1) (σ2
2 + 1)

.
The ﬁrst term will contribute toward favoring Model 1 whenever σ2
2 is larger
than σ2
1, whereas the opposite occurs in the second term.
■
8.2.5
Nested Models
As seen in Chapter 3, a nested model is one that can be viewed as a special
case of a more general, larger model, and is typically obtained by ﬁxing or
“zeroing in” some parameters in the latter. Following O’Hagan (1994), let
the bigger model have parameters (θ, φ) and denote it Model 1 whereas in
the nested model ﬁx φ = φ0, with this value being usually 0. This is Model
0.
Let the prior probability of the larger model (often called the “alterna-
tive” one) be π1, and let the prior density of its parameters be p1 (θ, φ) .
The prior probability of the nested model is π0 = 1 −π1, which can be
interpreted as the prior probability that φ = φ0. This is somewhat per-
plexing at ﬁrst sight, since the probability that a continuous parameter
takes a given value is 0. However, the fact that consideration is given to
the nested model as a plausible model implies that one is assigning some
probability to the special situation that φ = φ0 holds. In the nested model,

8.2 Bayes Factors
415
the prior density of the “free parameters” is p0 (θ) = p (θ|φ = φ0) , that is,
the density of the conditional distribution of the theta parameter, given
that φ = φ0. Now, for the larger model, write
p1 (θ, φ) = p1 (θ|φ) p1 (φ) ,
where p1 (θ|φ) is the density of the conditional distribution of θ, given φ. In
practice, it is reasonable to assume that the conditional density of θ, given
φ, is continuous at φ = φ0 (O’Hagan, 1994).
In order to obtain the marginal density of the data under Model 0 one
must integrate the joint density of the observations, and of the free param-
eters (given φ = φ0) with respect to the latter, to obtain
p (y|Model 0)
=

p (y|θ, φ = φ0) p (θ|φ = φ0) dθ
=
p (y|φ = φ0) .
(8.19)
For the larger model
p (y|Model 1)
=
 
p (y|θ, φ) p (θ|φ) dθ

p1 (φ) dφ
=

p (y|φ) p1 (φ) dφ = Eφ [p (y|φ)] .
(8.20)
The expectation above is an average of the sampling model marginal densi-
ties (after integrating out θ) taken over all values of φ (other than φ0) and
with plausibility as conveyed by the prior density p1 (φ) under the larger
model. The posterior probability of the null model is then
p (Model 0|y) =
p (y|Model 0) π0
p (y|Model 0) π0 + p (y|Model 1) (1 −π0)
=
p (y|φ = φ0) π0
p (y|φ = φ0) π0 +

p (y|φ) p1 (φ) dφ (1 −π0),
(8.21)
and p (Model 1|y) = 1 −p (Model 0|y) .
Consider now the case where there is a single parameter, so that Model
0 poses φ = φ0 and Model 1 corresponds to the “alternative” hypothesis
φ ̸= φ0 (the problem then consists of one of evaluating the “sharp” null
hypothesis φ = φ0). Then (8.21) holds as well, with the only diﬀerence
being that the marginal distributions of the data are calculated directly as
p (y|Model 0) = p (y|φ = φ0) ,
and
p (y|Model 1) =

p (y|φ) p1 (φ) dφ.
(8.22)

416
8. Bayesian Assessment of Hypotheses and Models
It is instructive to study the consequences of using a vague prior distri-
bution on the Bayes factor. Following O’Hagan (1994), suppose that φ is a
scalar parameter on (−∞, ∞). Vague prior knowledge is expressed as the
limit of a uniform distribution
p1 (φ) = (2c)−1 , for −c ≤φ ≤c,
by letting c →∞, in which case, p1 (φ) →0. Then (8.22) is

p (y|φ) p1 (φ) dφ = (2c)−1
 c
−c
p (y|φ) dφ.
Often, p (y|φ) will tend to zero as φ tends to inﬁnity, such that the limit
of the integral above is ﬁnite. Then as c →∞, (8.22) tends to zero and
the Bayes factor B01 tends to inﬁnity. Thus, using a prior with very large
spread on φ in an attempt to describe vague prior knowledge, forces the
Bayes factor to favor Model 0.
Example 8.7
Normal model: known versus unknown variance
The setting will be the usual N

µ, σ2
for each of n independent observa-
tions. In the larger model, both the mean and variance are taken as un-
known. In the nested model, the variance is assumed to be known, such that
σ2 = σ2
0. As in O’Hagan (1994), it will be assumed that the conditional prior
distribution of the mean is the normal process µ|µ1, wσ2 ∼N

µ1, wσ2
,
where w is a known scalar. This implies that the variance of the prior dis-
tribution is proportional to that of the sampling model. Further, it will be
assumed that the prior distribution of σ2 is a scaled inverted chi-square
distribution with parameters ν and S2.
Under the null or nested model (known variance), the prior distribution
is then µ|µ1, wσ2
0 ∼N

µ1, wσ2
0

, and the marginal distribution of the data,
following (8.19) and making use of (8.12), is
p (y|Model 0) =

p(y|µ, σ2
0)p

µ|µ1, wσ2
0

dµ
=

1
>
2πσ2
0
n
exp

−1
2σ2
0
n

i=1
(yi −y)2

×
1
>
2πwσ2
0

exp

−n
2σ2
0
(y −µ)2

exp

−(µ −µ1)2
2wσ2
0

dµ.

8.2 Bayes Factors
417
Combining the two quadratics in µ gives
p (y|Model 0) =

1
>
2πσ2
0
n
exp

−1
2σ2
0
n

i=1
(yi −y)2

1
>
2πwσ2
0
exp

−n
2σ2
0

 n
σ2
0
+
1
wσ2
0
−1
1
wσ2
0
(y −µ1)2


exp

−1
2

 n
σ2
0
+
1
wσ2
0

(µ −5µ)2

dµ,
where 5µ has the same form as in Example 8.3. After the integration is
carried out, one gets
p (y|Model 0) =

1
>
2πσ2
0
n
exp

−1
2σ2
0
n

i=1
(yi −y)2

1
>
2πwσ2
0
exp

−1
2
n
σ2
0

 n
σ2
0
+
1
wσ2
0
−1
1
wσ2
0
(y −µ1)2
 C
2πσ2
0

n + 1
w
−1
=

1
>
2πσ2
0
n
1
√nw + 1 exp

−Qy
2σ2
0

= p

y|σ2=σ2
0

,
(8.23)
where
Qy =
n

i=1
(yi −y)2 + n

n + 1
w
−1 1
w (y −µ1)2 .
In order to obtain the marginal density of the data under Model 1, use
is made of (8.20) and of (8.23), although noting that σ2 is now a free pa-
rameter. Then, recalling that the prior distribution of σ2 is scaled inverted
chi-square
p (y|Model 1) =

p

y|σ2
p

σ2|ν, S2
dσ2
=
 
2πσ2−n
2
(nw + 1)
1
2 exp

−Qy
2σ2
 
νS2
2
 ν
2
Γ
 ν
2


σ2−( ν+2
2 ) exp

−νS2
2σ2

dσ2
=
(2π)−n
2
(nw + 1)
1
2

νS2
2
 ν
2
Γ
 ν
2

 
σ2−( n+ν+2
2
) exp

−νS2 + Qy
2σ2

dσ2
=
(2π)−n
2
(nw + 1)
1
2

νS2
2
 ν
2 Γ
 n+ν
2

Γ
 ν
2


νS2 + Qy
2
−( n+ν
2 )
.
(8.24)
In order to arrive at the last result, use is made of the gamma integrals
(see Chapter 1). The Bayes factor in favor of Model 1 relative to Model 0

418
8. Bayesian Assessment of Hypotheses and Models
is given by the ratio between (8.24) and (8.23) yielding
B10 =

νS2
2
 ν
2 Γ
 n+ν
2

(σ2
0)−n
2 exp

−Qy
2σ2
0

Γ
 ν
2


νS2 + Qy
2
−( n+ν
2 )
.
■
8.2.6
Approximations to the Bayes Factor
There is extensive literature describing various approximate criteria for
Bayesian model selection. Some have been motivated by the desire for sup-
pressing the dependence of the ﬁnal results on the prior. Ease of computa-
tion has also been an important consideration, especially in the pre-MCMC
era. Some of these methods are still a useful part of the toolkit for com-
paring models. This section introduces widely used approximations to the
Bayes factor based on asymptotic arguments. The latter are based on reg-
ularity conditions which fail when the parameter lies on a boundary of its
parameter space (Pauler et al., 1999), a restriction not encountered with
the Bayes factor.
The marginal density of the data under Model i, say, is
p (y|Mi) =

p (y|θi, Mi) p (θi|Mi) dθi,
(8.25)
where θi is the pi × 1 vector of parameters under this model. In what
follows, it will be assumed that the dimension of the parameter vector
does not increase with the number of observations or that, if this occurs,
it does so in a manner that, for n being the number of observations, pi/n
goes to 0 as n →∞. This is important for asymptotic theory to hold. In
the context of quantitative genetic applications, there are models in which
the number of parameters, e.g., the additive genetic eﬀects, increases as the
number of observations increases. For such models the approximations hold
provided that these eﬀects are ﬁrst integrated out, in which case p (y|θi, Mi)
would be an integrated likelihood. For example, suppose a Gaussian linear
model has f location parameters, n additive genetic eﬀects (one for each
individual), and two variance components. Then analytical integration of
the additive eﬀects (over their prior distribution) would need to be eﬀected
before proceeding. On the other hand, if the model is one of repeated
measures taken on subjects or clusters (such as a family of half-sibs), it is
reasonable to defend the assumption that pi/n goes to 0 asymptotically.
Using the Posterior Mode
As in Chapter 7, expand the logarithm of the integrand in (8.25) around
the posterior mode, ,θi, using a second-order Taylor series expansion, to

8.2 Bayes Factors
419
obtain (recall that the gradient vanishes at the maximum value)
log [p (y|θi, Mi) p (θi|Mi)]
≈log

p

y|,θi, Mi

p

,θi|Mi

−1
2

θi −,θi
′ 
Hθi
 
θi −,θi

,
(8.26)
where Hθi is the corresponding negative Hessian matrix. Then, using this
in (8.25),
p (y|Mi) =

exp {log [p (y|θi, Mi) p (θi|Mi)]} dθi
≈exp
9
log

p

y|,θi, Mi

p

,θi|Mi
:
×

exp

−1
2

θi −,θi
′ 
Hθi
 
θi −,θi

dθi.
The integral is in a Gaussian form (this approach to integration is called
Laplace’s method for integrals), so it can be evaluated readily. Hence
p (y|Mi) ≈p

y|,θi, Mi

p

,θi|Mi

(2π)
pi
2
"""H−1
θi
"""
1
2 ,
(8.27)
where H−1
θi is the variance–covariance matrix of the Gaussian approxima-
tion to the posterior distribution. Further
log [p (y|Mi)] ≈log

p

y|,θi, Mi

+ log

p

,θi|Mi

+ pi
2 log (2π) + 1
2 log
"""H−1
θi
"""

.
(8.28)
Twice the logarithm of the Bayes factor for Model i relative to Model j,
to express the “evidence brought up by the data” in support of Model i
relative to j in the same scale as likelihood ratio tests, is then
2 log (Bij) ≈2 log


p

y|,θi, Mi

p

y|,θj, Mj


+ 2 log
p

,θi|Mi

p

,θj|Mj

+ (pi −pj) log (2π) + log


"""H−1
θi
"""
"""H−1
θj
"""

.
(8.29)
Note that the criterion depends on the log-likelihood ratios (evaluated at
the posterior modes), on the log-prior ratios (also evaluated at the modes),
on the diﬀerence between the dimensions of the two competing models, and
on a Hessian adjustment.

420
8. Bayesian Assessment of Hypotheses and Models
Using the Maximum Likelihood Estimator
A variant to approximation (8.26) is when the expansion of the logarithm
of the product of the prior density and of the conditional distribution of
the observations (given the parameters) is about the maximum likelihood
estimator 5θi, instead of the mode of the posterior distribution (Tierney and
Kadane, 1989; O’Hagan, 1994; Kass and Raftery, 1995). Here one obtains
in (8.27),
p (y|Mi) ≈p

y|5θi, Mi

p

5θi|Mi

(2π)
pi
2
"""H−1
θi
"""
1
2 ,
(8.30)
where Hθ is the observed information matrix evaluated at the maximum
likelihood estimator. In particular, if the observations are i.i.d. one has
Hθ = nH1,θ, where H1,θ is the observed information matrix calculated
from a single observation. Then
p (y|Mi) ≈p

y|5θi, Mi

p

5θi|Mi

(2π)
pi
2 (n)−pi
2
"""H−1
1,θi
"""
1
2 .
(8.31)
The approximation to twice the logarithm of the Bayes factor becomes
2 log (Bij) ≈2 log


p

y|5θi, Mi

p

y|5θj, Mj


+ 2 log
p

5θi|Mi

p

5θj|Mj

−(pi −pj) log n
2π + log
"""H−1
1,θi
"""
"""H−1
1,θj
"""
.
(8.32)
It is important to note that even though the asymptotic approximation to
the posterior distribution (using the maximum likelihood estimator) does
not depend on the prior, the resulting approximation to the Bayes factor
does depend on the ratio of priors evaluated at the corresponding maximum
likelihood estimators. If the term on the logarithm of the prior densities
is excluded, the resulting expression is called the Bayesian information
criterion (or BIC) (Schwarz, 1978; Kass and Raftery, 1995; Leonard and
Hsu, 1999).
Suppose that the prior conveys some sort of “minimal” information rep-
resented by the distribution θi|Mi ∼N

5θi, H−1
1,θ

. This is a unit infor-
mation prior centered at the maximum likelihood estimator and having a
precision (inverse of the covariance matrix) equivalent to that brought up

8.2 Bayes Factors
421
by a sample of size n = 1. Using this in (8.31):
p (y|Mi) ≈p

y|5θi, Mi

(2π)−pi
2
"""H−1
1,θ
"""
−1
2
× exp

−1
2

5θ −5θ
′ 
H1,θ1,θ
 
5θ −5θ

(2π)
pi
2 (n)−pi
2
"""H−1
1,θ
"""
1
2
= p

y|5θi, Mi

(n)−pi
2 .
(8.33)
Hence
2 log (Bij) ≈2 log


p

y|5θi, Mi

p

y|5θj, Mj


−(pi −pj) log n.
(8.34)
This is Schwarz (1978) BIC in its most commonly presented form (Kass
and Raftery, 1995; O’Hagan, 1994). Some authors (Leonard and Hsu, 1999;
Congdon, 2001) use the term BIC to refer just to the approximated marginal
densities, e.g., the logarithm of (8.33). At any rate, note that (8.34) is twice
the maximized log-likelihood ratio, plus an adjustment that penalizes the
model with more parameters. If n = 1, there is no penalty. However, the
term (pi −pj) becomes more important as a sample size increases. When
pi > pj, (8.34) is smaller than twice the log-likelihood ratio, so the adjust-
ment favors parsimony. In contrast, classical testing based on the traditional
likelihood ratio tends to favor the more complex models. Contrary to the
traditional likelihood ratio, BIC is well deﬁned for nonnested models.
Denoting S the right hand side of (8.34) divided by 2, as sample size
n →∞, this quantity satisﬁes:
S −log Bij
log Bij
→0,
so it is consistent in this sense (Kass and Raftery, 1995). Recent extensions
of BIC can be found in Kass (1995).
A related criterion is AIC (or the Akaike’s information criterion) (Akaike,
1973), where the penalty is 2 (pi −pj) . The argument underlying the AIC is
that if two models favor the data equally well, then the more parsimonious
one should be favored. The BIC produces an even more drastic penalty,
which increases with sample size, as noted.
The diﬀerences between the likelihood ratio criterion, the BIC, and the
AIC are discussed by O’Hagan (1994) in the context of a nested model. The
larger model has parameters (θ, φ) and dimension p2, whereas the “smaller
or null” model has a parameter vector θ with p1 elements and p2 −p1
ﬁxed components φ = φ0. For a large sample size, the log-likelihood ratio
may favor the larger model, yet the penalty, (p2 −p1) log n, may be severe
enough so that the Bayes factor may end up favoring the null model.

422
8. Bayesian Assessment of Hypotheses and Models
It is instructive to examine the behavior of the approximation to the
Bayes factor under repeated sampling from the appropriate model. Con-
sider the BIC as given in (8.32), and take its expected value under the
null model, with only the likelihood ratio viewed as a random variable.
Recalling that the expected value of twice the log-likelihood ratio statistic
under the null hypothesis is equal to the diﬀerence in dimension between
the competing models, or (p2 −p1) , one gets
E [2 log (B21)] ≈2 log
p

5θ2|M2

p

5θ1|M1
 −(p2 −p1)

log n
2π −1

+ constant.
Hence, as n →∞, the expected value of the log of the Bayes factor in favor
of the larger model goes to −∞. This implies that the posterior probability
of the larger model goes to 0 when the null model is true, regardless of the
prior odds ratios as conveyed by p

5θ2|M2

/p

5θ1|M1

. Conversely, when
the larger model is true, the expected value of twice the log-likelihood ratio
statistic is approximately equal to nQ (φ, φ0) ,where Q (·) is a quadratic
form (O’Hagan, 1994). This is a consequence of the asymptotically normal
distribution of the maximum likelihood estimator (see Chapters 3 and 4).
Then, under the larger model,
E [2 log (Bij)] ≈nQ (φ, φ0)+2 log
p

5θi|Mi

p

5θj|Mj
 −(p2 −p1) log n
2π +constant.
As n →∞, the logarithm of the Bayes factor in favor of the larger model
goes to ∞, since n grows faster than log n. Consequently, the posterior
probability of the larger model goes to 1, no matter what the prior odds
are. Strictly from a classical point of view, and no matter how large n is,
the null model will be rejected with probability equal to the signiﬁcance
level even when the model is true. Hence, more stringent signiﬁcance levels
should be adopted in classical hypothesis testing when sample sizes are
large. Classical theory does not give a procedure for modifying the type-
1 error as a function of sample size, and the probability of this error is
prescribed arbitrarily. As noted by O’Hagan (1994), the Bayesian approach
gives an automatic procedure in which in a single formula, such as (8.32),
the evidence from the data, the prior odds, the model dimensionality, and
the sample size are combined automatically.
8.2.7
Partial and Intrinsic Bayes Factors
The Bayes factor is only deﬁned up to arbitrary constants when prior dis-
tributions are improper (i.e., Berger and Pericchi, 1996), as was illustrated
at the end of Subsection 8.2.5. Further, when the priors are proper, the

8.2 Bayes Factors
423
Bayes factor depends on the form of the chosen prior distribution, as seen
in connection with (8.32). This dependence does not decrease as sample
size increases, contrary to the case of estimation of parameters from poste-
rior distributions. In estimation problems and under regularity conditions,
one can obtain an asymptotic approximation centered at the maximum
likelihood estimator that does not involve the prior.
Berger and Pericchi (1996) suggested what are called intrinsic Bayes
factors, in an attempt to circumvent the dependence on the prior, and to
allow for the use of improper prior distributions, such as those based on
Jeﬀreys’ rule. Here, a brief overview of one of the several proposed types
of Bayes factors (the arithmetic intrinsic Bayes factor) is presented.
Let the data vector of order n be partitioned as
y =

y′
(1), y′
(2), . . . , y′
(L)
′
,
where y(l), (l = 1, 2, ..., L) denotes what is called the minimal training sam-
ple. This is the minimal number of observations needed for the posterior
distribution to be proper. For example, if the minimal size of the training
sample is m, there would be Cn
m diﬀerent possible training samples. The
posterior distribution based on the minimal training sample has density
p

θi|y(l), Mi

. Further, put
y =

y′
(l), y′
(−l)
′
,
where y(−l) is the data vector with y(l) removed. Then the predictive den-
sity of y(−l) under model i, conditionally on the data of the training sample
y(l), is
p

y(−l)|y(l), Mi

=

p

y(−l)|θi, y(l), Mi

p

θi|y(l), Mi

dθi.
The Bayes factor for model j relative to model i, conditionally on y(l), or
partial Bayes factor (O’Hagan, 1994) is
Bji

y(l)

= p

y(−l)|y(l), Mj

p

y(−l)|y(l), Mi
 .
(8.35)
Clearly, the partial Bayes factor depends on the choice of the training sam-
ple y(l). To eliminate this dependence, Berger and Pericchi (1996) propose
averaging Bji

y(l)

over all Cn
m = K training samples. This yields the
arithmetic intrinsic Bayes factor, deﬁned formally as
BAI
ji = 1
K
K

l=1
p

y(−l)|y(l), Mj

p

y(−l)|y(l), Mi
 .
(8.36)

424
8. Bayesian Assessment of Hypotheses and Models
This expression can be computed for any pair of models, irrespective of
whether these are nested or not. Although the procedure is appealing, some
diﬃculties arise. First, for most realistic hierarchical models it is not pos-
sible to determine in advance what the minimum sample size should be in
order for the posterior to be proper. Second, and especially in animal breed-
ing, the data sets are very large so, at best, just a few minimal training
samples could be processed in practice.
There have been several other attempts to circumvent the need to us-
ing proper priors and to restrict the dependence on the prior. These are
reviewed in O’Hagan (1994).
8.3
Estimating the Marginal Likelihood
from Monte Carlo Samples
Except in highly stylized models, the integration indicated in (8.25) is not
feasible by analytical means. An alternative is to use Monte Carlo methods.
Here we shall consider the method of importance sampling, which will
be encountered again in Chapters 12 and 15, where more details on the
technique are given. Suppose samples of θi, the parameter vector under
Model i, can be obtained from some known distribution that is relatively
easy to sample from. This distribution, having the same support as the prior
or posterior, is called the importance sampling distribution, and its density
will be denoted as g (θi) . Then since

p (θi|Mi) dθi = 1, the marginal
density of the data under Model i is expressible as
p (y|Mi)
=

p (y|θi, Mi) p (θi|Mi) dθi

p (θi|Mi) dθi
=

p (y|θi, Mi) p(θi|Mi)
g(θi) g (θi) dθi
 p(θi|Mi)
g(θi) g (θi) dθi
.
(8.37)
Various Monte Carlo sampling schemes can be derived from (8.37), depend-
ing on the importance sampling function adopted. Suppose m samples can
be obtained from the distribution with density g (θi) ; let the samples be
θ[j]
i , (j = 1, 2, . . . , m) . Then note that the denominator of (8.37) can be
written as
 p (θi|Mi)
g (θi)
g (θi) dθi = lim
m→∞

1
m
m

j=1
p

θ[j]
i |Mi

g

θ[j]
i


,

8.3 Estimating the Marginal Likelihood
425
where p

θ[j]
i |Mi

is the prior density under Model i evaluated at sampled
value j. Likewise, the numerator can be written as

p (y|θi, Mi) p (θi|Mi)
g (θi)
g (θi) dθi
= lim
m→∞

1
m
m

j=1
p

y|θ[j]
i , Mi
 p

θ[j]
i |Mi

g

θ[j]
i


,
where p

y|θ[j]
i , Mi

is the density of the sampling model evaluated at the
jth sample obtained from the importance distribution. Hence for large m,
and putting w[j]
i
= p

θ[j]
i |Mi

/g

θ[j]
i

, a consistent estimator of (8.37)
is given by the ratio
5p (y|Mi) =
m

j=1
w[j]
i p

y|θ[j]
i , Mi

m

j=1
w[j]
i
,
(8.38)
which is a weighted average of the density of the sampling distribution
evaluated at the corresponding sampled values of the parameter vector
under the appropriate model.
Sampling from the Prior
If the importance distribution is the prior, each of the weights w[j]
i
are equal
to 1, and the Monte Carlo estimator (8.38) of the marginal density at the
observed value of y becomes
5p (y|Mi) = 1
m
m

j=1
p

y|θ[j]
i , Mi

,
(8.39)
where the θ[j]
i
are draws from the prior distribution. The procedure is very
simple because the joint prior distribution of the parameters is often simple
to sample from. However, the estimator is imprecise because, typically, the
θ[j]
i
drawn from the prior are conferred little likelihood by the data. There
will be just a few draws that will have appreciable likelihood and these will
“dominate” the average (Kass and Raftery, 1995). Numerical studies can
be found in McCulloch and Rossi (1991).

426
8. Bayesian Assessment of Hypotheses and Models
Sampling from the Posterior
If the importance distribution is the posterior, then
wi =
p (θi|Mi)
p (θi|y, Mi)
=
p (θi|Mi)
p(y|θi,Mi)p(θi|Mi)
p(y|Mi)
=
p (y|Mi)
p (y|θi, Mi).
Using this in (8.38):
5p (y|Mi) =
m

j=1
p(y|Mi)
p

y|θ[j]
i ,Mi
p

y|θ[j]
i , Mi

m

j=1
p(y|Mi)
p

y|θ[j]
i ,Mi

=
m
m

j=1
1
p

y|θ[j]
i ,Mi

=

1
m
m

j=1
1
p

y|θ[j]
i , Mi



−1
.
(8.40)
This estimator, the harmonic mean of the likelihood values, was derived
by Newton and Raftery (1994), but arguing directly from Bayes theorem.
Observe that a rearrangement of the theorem leads to
p (θi|Mi)
p (y|Mi) = p (θi|y, Mi)
p (y|θi, Mi).
Then, integrating both sides with respect to θi, yields
1
p (y|Mi)

p (θi|Mi) dθi =

1
p (y|θi,Mi)p (θi|y, Mi) dθi.
Since the prior must be proper for the marginal density of the data to be
deﬁned, the integral on the left is equal to 1 leading directly to
p (y|Mi) =
1
Eθi|y,Mi [p−1 (y|θi, Mi)].
(8.41)
The Monte Carlo estimator of the reciprocal of the posterior expectation
of the reciprocal of the likelihood values is precisely (8.40). An advantage
of the harmonic mean estimator is that one does not need to know the
form of the posterior distribution. The Markov chain Monte Carlo meth-
ods presented in the next part of the book enable one to draw samples
from complex, unknown, distributions. The disadvantage, however, is its

8.3 Estimating the Marginal Likelihood
427
numerical instability. The form of (8.40) reveals that values of θi with very
small likelihood can have a strong impact on the estimator. An alternative
is to form some robust estimator of the harmonic mean (Congdon, 2001)
such as a trimmed average. Kass and Raftery (1995) state that, in spite of
the lack of stability, the estimator is accurate enough for interpretation on
a logarithmic scale.
Caution must be exercised in the actual computation of (8.40), to avoid
numerical over- or under-ﬂows. A possible strategy could be as follows. Let
v
=
1
m
m

j=1
p−1 
y|θ[j], Mi

=
1
m
n

j=1
S[j]
i ,
where S[j]
i
= p−1 
y|θ[j], Mi

, and store log S[j]
i
in a ﬁle for each sampled
value. Then, since
exp (x) = exp (x −c + c) = exp (x −c) exp c,
one can write v in the form
v = 1
m
m

j=1
exp

log S[j]
i
−c

exp c,
where c is the largest value of log S[j]
i . Taking logarithms yields
log v = log

1
m
m

j=1
exp

log S[j]
i
−c


+ c.
Hence
log [5p (y|Mi)] = −log v.
Chib’s Method
Most often, the marginal posterior distributions cannot be identiﬁed. How-
ever, there are many models where the conditional posterior distributions
can be arrived at from inspection of the joint posterior densities. Advan-
tage of this is taken in a Markov chain-based method called the Gibbs
sampler, which will be introduced in Chapter 11. Chib (1995) outlined a
procedure for estimating the marginal density of the data under a given
model when the fully conditional posterior distributions can be identiﬁed.
These distributions are deﬁned in Section 11.5.1 of Chapter 11. We will

428
8. Bayesian Assessment of Hypotheses and Models
suppress the dependency on the model in the notation, for simplicity. Sup-
pose the parameter vector is partitioned as θ =

θ′
1, θ′
2
′ . The logarithm
of the marginal density of the data can be expressed as
log p (y) = log [p (y|θ1, θ2)] + log [p (θ1, θ2)] −log [p (θ1, θ2|y)]
= log [p (y|θ1, θ2)] + log [p (θ1, θ2)] −log [p (θ2|θ1, y)] −log [p (θ1|y)] .
Suppose now that samples of θ1, θ2 have been drawn from the posterior
distribution using the Gibbs sampler. Inspection of a large number of sam-
ples permits us to calculate, e.g., the posterior mean, mode, or median for
each of the elements of the parameter vector, such that one can form, say,
the vector of posterior medians ,θ =

,θ
′
1, ,θ
′
2
′
. An estimate of the marginal
density of the data can be obtained as
log 5p (y) = log

p

y|,θ1, ,θ2

+ log

p

,θ1, ,θ2

−log

p

,θ2|,θ1, y

−log

p

,θ1|y

.
(8.42)
If the conditional density log [p (θ2|θ1, y)] is known, the third term can
be evaluated readily. The diﬃculty resides in the fact that the marginal
posterior density may not be known. However, recall that
p (θ1|y) = Eθ2|y [p (θ1|θ2, y)] .
Hence
p

,θ1|y

= Eθ2|y

p

,θ1|θ2, y

,
and an estimate of the marginal posterior density can be obtained as
5p

,θ1|y

= 1
m
m

j=1
p

,θ1|θ[j]
2 , y

,
where θ[j]
2 , (j = 1, 2, . . . , m) are samples from the marginal posterior distri-
bution of θ2 obtained with the Gibbs sampler. Then, using this in (8.42),
the estimated marginal density of the data is arrived at as
log 5p (y) = log

p

y|,θ1, ,θ2

+ log

p

,θ1, ,θ2

−log

p

,θ2|,θ1, y

−log

1
m
m

j=1
p

,θ1|θ[j]
2 , y


.
(8.43)
The procedure is then repeated for each of the models in order to calculate
the Bayes factor. However, the fully conditional posterior distribution of
one parameter given the other must be identiﬁable in each of the models.
The method can be extended from two to several parameter blocks (Chib,
1995; Han and Carlin, 2001). Additional reﬁnements of the procedure are
in Chib and Jeliazkov (2001).

8.4 Goodness of Fit and Model Complexity
429
8.4
Goodness of Fit and Model Complexity
In general, as a model becomes increasingly more complex, i.e., by increas-
ing the number of parameters, its ﬁt gets better. For example, it is well
known that if one ﬁts n regression coeﬃcients to a data set consisting of
n points, the ﬁt is perfect. As seen earlier, the AIC and BIC introduce
penalties against more highly parameterized models. A slightly diﬀerent
approach was suggested by Spiegelhalter et al. (2002), and it is based on
calculating the expected posterior deviance, i.e., a measure of ﬁt.
Consider a model with parameter vector [θ1, θ2] . For example, in a mixed
linear model θ1, (p1 × 1) may be a vector of “ﬁxed” eﬀects such as breed
or sex of animal and variance parameters, while θ2 may be a vector of ran-
dom eﬀects or missing data. Hence, in some sense, the dimension of θ1 can
be viewed as ﬁxed, as the dimension of the data vector increases, whereas
the order of θ2 may perhaps increase with n. Clearly, neither the AIC nor
the BIC can be used in models where the parameters outnumber the ob-
servations (Gelfand and Dey, 1994) unless some parameters are integrated
out. In what follows it will be assumed that θ2 can be integrated some-
how, possibly by analytical means, and that one arrives at the integrated
likelihood
p (y|θ1) =

p (y|θ1, θ2) p (θ2|θ1) dθ2.
Here p (θ2|θ1) is the density of the conditional distribution of the nuisance
parameters, given the primary model parameters. The dependency on the
model will be suppressed in the notation. Now rearrange the components
of Bayes theorem as
p (θ1|y)
p (θ1)
= p (y|θ1)
p (y) .
Taking expectations of the logarithm of both sides with respect to the
posterior distribution of θ1, one gets

log
p (θ1|y)
p (θ1)

p (θ1|y) dθ1 =

log
p (y|θ1)
p (y)

p (θ1|y) dθ1.
The left-hand side is the Kullback–Leibler distance between the posterior
and prior distributions, so it is at least null. Following Dempster (1974,
1997), Spiegelhalter et al. (2002) suggest to view (given the model) p (y)
as a standardizing term and to set p (y) = 1. This can be construed as a
“perfect predictor”, giving probability 1 to each of the observations in the
data set (prior to observation). Hence, the larger the logarithm of p (y|θ1)
(the log-likelihood), the closer the model is to “perfect prediction”. Setting
p (y) = 1 and multiplying the right-hand side of the above expression by

430
8. Bayesian Assessment of Hypotheses and Models
−2, deﬁne
D
=
−2

[log p (y|θ1)] p (θ1|y) dθ1
=
Eθ1|y [−2 log p (y|θ1)]
=
Eθ1|y [D (θ1)] ,
(8.44)
where D (θ1) = −2 log p (y|θ1) is called the deviance (a function of the
unknown parameter), and D is its expected value taken over the posterior
distribution. Note that when the deviance is evaluated at the maximum
likelihood estimator, one obtains the numerator (or denominator) of the
usual likelihood ratio statistic. Thus, one averages out the deviance crite-
rion over values whose plausibilities are dictated by the posterior distribu-
tion. The expected deviance is interpreted as a posterior summary of the
ﬁt of the model. In general, D will need to be computed using Monte Carlo
procedures for sampling from the posterior distribution: samples from the
posterior are obtained, and then one averages the log-likelihoods evaluated
at each of the draws.
Concerning model complexity (degree of parameterization), Spiegelhalter
et al. (2002) suggest using the “eﬀective number of parameters”
pD = D −D

θ1

,
(8.45)
where D

θ1

is the deviance evaluated at the posterior mean of the primary
parameter vector. In order to motivate this concept, expand the deviance
around the posterior mean θ1, to obtain
D (θ1) ≈−2 log p

y|θ1

−2
∂log p (y|θ1)
∂θ1
′
θ1=θ1

θ1 −θ1

−

θ1 −θ1
′ ∂2 log p (y|θ1)
∂θ1∂θ′
1
 
θ1 −θ1

.
(8.46)
Taking the expectation of (8.46), with respect to the posterior distribution
of the parameter vector, gives the expected deviance
D ≈−2 log p

y|θ1

+ tr

−∂2 log p (y|θ1)
∂θ1∂θ′
1

θ1=θ1
V ar (θ1|y)
= D

θ1

+ tr
#
−∂2 log p (y|θ1)
∂θ1∂θ′
1

θ1=θ1
V ar (θ1|y)
$
= D

θ1

+ tr

[I (θ1)]θ1=θ1 V ar (θ1|y)

,
(8.47)
where I (θ1) is the observed information matrix and V ar (θ1|y) is the
variance–covariance matrix of the posterior distribution. The trace adjust-
ment in (8.47) is called the “eﬀective number of parameters” and is denoted

8.4 Goodness of Fit and Model Complexity
431
pD, following (8.45). Recall from Chapter 7, that an asymptotic approxi-
mation to the posterior distribution is given by a normal process having a
covariance matrix that is equal to the inverse of the sum of the observed
information matrix (evaluated at some mode), plus the negative Hessian of
the log-prior density (evaluated at some mode); the latter will be denoted as
P (θ1)θ1=θ1 when evaluated at the posterior mean. Hence, approximately,
pD ≈tr

[I (θ1)]θ1=θ1 V ar (θ1|y)

≈tr
9
[I (θ1)]θ1=θ1

[I (θ1)]θ1=θ1 + P (θ1)θ1=θ1
−1:
.
(8.48)
Thus, the eﬀective number of parameters can be interpreted as the informa-
tion about θ1 contained in the likelihood relative to the total information
in both the likelihood and the prior. Some additional algebra yields
pD ≈tr

[I (θ1)]θ1=θ1 + P (θ1)θ1=θ1 −P (θ1)θ1=θ1

×

[I (θ1)]θ1=θ1 + P (θ1)θ1=θ1
−1:
= p1 −tr

P (θ1)θ1=θ1

[I (θ1)]θ1=θ1 + P (θ1)θ1=θ1
−1
.
(8.49)
This representation leads to the interpretation that the eﬀective number of
parameters is equal to the number of parameters in θ1, minus an adjust-
ment measuring the amount of information in the prior relative to the total
information contained in the asymptotic approximation to the posterior.
Further, Spiegelhalter et al. (2002) suggested combining the measure of
ﬁt given by D (the posterior expectation of the deviance) in (8.44) with
the eﬀective number of parameters in (8.48) or (8.49), into a deviance
information criterion (DIC). This is deﬁned as
DIC
=
D + pD
=
D

θ1

+ 2pD,
(8.50)
with the last expression resulting from (8.45). Models having a smaller
DIC should be favored, as this indicates a better ﬁt and a lower degree of
model complexity. The authors emphasize that they consider DIC to be a
preliminary device for screening alternative models.
Example 8.8
Deviance information criterion in the mixed linear model
Consider a hierarchical model with structure
y = Wθ + e,
where y|θ, R ∼N (Wθ, R) . This model has been discussed several times,
especially in Chapter 6. In animal breeding θ =

β′, u′′ is typically a vector
of “ﬁxed” and “random” eﬀects, and the corresponding known incidence
matrix is then W = [X, Z]. Suppose that the dimension of β (pβ) does not

432
8. Bayesian Assessment of Hypotheses and Models
increase with the number of observations, and that the vector u (having
order pu) contains the eﬀects of clusters, e.g., half-sib families. Hence, one
can conceptually let the number of observations per cluster go to inﬁn-
ity (or, equivalently, think that the number of observations increases more
rapidly than the number of clusters). Under these conditions, one can em-
ploy the asymptotic approximations discussed earlier. The second level of
the hierarchy poses
θ|µβ, µu, Vβ, σ2
β, Gu ∼N

 µβ
µu

,
 Vβσ2
β
0
0
Gu

.
The dispersion parameters R, Vβ, σ2
β, Gu, and the location vectors µβ and
µu are assumed known. As mentioned in Chapter 1, Example 1.18, and
shown in Chapter 6, the posterior distribution of θ is normal, with mean
vector
θ =

β
u

=

X′R−1X +
V−1
β
σ2
β
X′R−1Z
Z′R−1X
Z′R−1Z + G−1
u
−1
×

X′R−1y +
V−1
β
σ2
β µβ
Z′R−1y + G−1
u µu

,
and variance–covariance matrix
C−1 =

X′R−1X +
1
σ2
β V−1
β
X′R−1Z
Z′R−1X
Z′R−1Z + G−1
u
−1
.
The deviance is
D (θ)
=
−2 log p (y|θ, R)
=
N log (2π) + log |R| + (y −Wθ)′ R−1 (y −Wθ) .
Then,
D

θ

= N log (2π) + log |R| +

y −Wθ
′ R−1 
y −Wθ

,
and the expected deviance becomes
D = N log (2π) + log |R| +

y −Wθ
′ R−1 
y −Wθ

+tr

R−1WC−1W′
.
Employing (8.45), the eﬀective number of parameters is
pD
=
D −D

θ

=
tr

C−1W′R−1W

.

8.5 Goodness of Fit and Predictive Ability of a Model
433
For example, let R = Iσ2
e and Gu = Iσ2
u, which results in a variance com-
ponent model. Further, let σ2
β →∞, to make prior information about β
vague. Here
C−1
=

X′X
X′Z
Z′X
Z′Z + σ2
e
σ2
u I
−1
σ2
e
=

Cββ
Cβu
Cuβ
Cuu

σ2
e,
and
C−1W′R−1W =

X′X
X′Z
Z′X
Z′Z+ σ2
e
σ2
u I
−1 
X′X
X′Z
Z′X
Z′Z

= Ipβ+pu −

Cββ
Cβu
Cuβ
Cuu
 
0
0
0
σ2
e
σ2
u Ipu

.
Hence
pD
=
tr

C−1W′R−1W

=
tr

Ipβ+pu

−tr
#
Cββ
Cβu
Cuβ
Cuu
 
0
0
0
σ2
e
σ2
u Ipu
$
=
pβ + pu −σ2
e
σ2u
tr

0
Cβu
0
Cuu

=
pβ + pu −σ2
e
σ2u
tr [Cuu] .
Note that the prior information about the u vector results in that the
eﬀective number of parameters is smaller than the dimension of θ.
■
8.5
Goodness of Fit and Predictive
Ability of a Model
The posterior probability of a model and the Bayes factors can be viewed
as global measures of model relative plausibility. However, one often needs
to go further than that. For example, a model can be the most plausible
within a set of competing models and, yet, either be unable to predict the
data at hand well or to give reasonable predictions of future observations.
Here we will provide just a sketch of some of the procedures that can be
used for gauging the quality of ﬁt and predictive performance of a model.

434
8. Bayesian Assessment of Hypotheses and Models
8.5.1
Analysis of Residuals
A comprehensive account of techniques for examination of residuals is given
by Barnett and Lewis (1995). In order to illustrate some of the basic ideas,
consider, for example, a linear regression analysis. One of the most widely
used techniques for assessing ﬁt is to carry out a residual analysis (e.g.,
Draper and Smith, 1981). In the context of classical regression, one calcu-
lates the predicted value of an observation, 5y, and forms the Studentized
ﬁtted residual
y −5y
8
5σ2
e
,
where 5σ2
e is typically the unbiased estimator of the residual variance. If the
absolute value of the Studentized residual exceeds a certain critical value of
the t or normal distributions, then the observation is viewed as suspicious
and regarded as a potential outlier. This may be construed as an indication
that the model does not ﬁt well.
The Bayesian counterpart of this classical regression analysis consists of
examining the posterior distribution of the unobserved standardized quan-
tity
ri = yi −x′
iβ
>
σ2e
,
where the row vector x′
i contains known explanatory variables linking the
unknown regression vector β to yi. Using the standard normality assump-
tions with independent and identically distributed errors, the distribution
of ri under the sampling model is ri ∼N (0, 1) , provided σ2
e is known. If β
has the prior distribution β|α, Vβ ∼N (α, Vβ) , where the hyperparame-
ters are also known, one obtains as prior (or predictive) distribution of the
residual above, given σ2
e,
ri|α, σ2
e, Vβ ∼N

yi −x′
iα
>
σ2e
, x′
iVβxi
σ2e

.
The unconditional (with respect to σ2
e) prior distribution of the standard-
ized residual will depend on the prior adopted for σ2
e. Then one could carry
out an analysis of the residuals prior to proceeding with Bayesian learning
about the parameters. More commonly, however, the residual analysis will
be undertaken based on the joint posterior distribution of β and σ2
e. As
seen in Chapter 6, given σ2
e, the posterior distribution of β is the normal
process
β|α, Vβ, σ2
e, y ∼N

,β,

X′X
σ2e
+ V−1
β
−1
,
where
,β =

X′X
σ2e
+ V−1
β
−1 
X′y
σ2e
+ V−1
β α

.

8.5 Goodness of Fit and Predictive Ability of a Model
435
Further, given σ2
e, the posterior distribution of the Studentized residual will
have the form
ri|α, Vβ, σ2
e, y ∼N

yi −x′
i,β
>
σ2e
,
x′
i

X′X
σ2
e + V−1
β
−1
xi
σ2e

.
The unconditional (with respect to σ2
e) posterior distribution will depend
on the form of the marginal posterior distribution of the residual variance,
and its density is obtained as
p

ri|α, Vβ, σ2
e, y

=

p

ri|α, Vβ, σ2
e, y

p

σ2
e|y

dσ2
e,
where p

σ2
e|y

is the marginal posterior density of the residual variance.
Unless standard conjugate priors are adopted, the marginal posterior dis-
tribution of the Studentized residual cannot be arrived at in closed form.
In such a situation, one can adopt the sampling techniques described in the
third part of the book and obtain draws from the posterior distribution of
the standardized residual. This is done simply by drawing from the pos-
terior distribution of the model parameters. Then, for observation i, one
forms samples
r[j]
i
= yi −x′
iβ[j]
8
σ2[j]
e
,
j = 1, 2, . . . , m,
where β[j] and σ2[j]
e
are samples from the joint posterior distribution of the
regression vector and of the residual variance. Thus, one obtains an entire
distribution for each Studentized residual, which can be used to decide
whether or not the observation is in reasonable agreement with what the
model predicts. If the value 0 appears at high density in the posterior
distribution, this can be construed as an indication that the observation is
in conformity with the model.
This simple idea extends naturally to other models in which residuals
are well deﬁned. For example, for binary (0, 1) responses analyzed with a
probit model, Albert and Chib (1993, 1995) deﬁne the Bayesian residual
ri = yi −Φ (x′
iβ), which is real valued on the interval [yi −1, yi]. If samples
are taken from the posterior distribution of β, one can form corresponding
draws from the posterior distribution of each residual. Since Φ (x′
iβ) takes
values between 0 and 1, an observation yi = 0 will be outlying if the pos-
terior distribution of ri is concentrated towards the endpoint −1, and an
observation yi = 1 is suspect if the posterior of ri is concentrated towards
the value 1. A value of 0 appearing at high density in the posterior dis-
tribution of the residuals can be interpreted as an indication of reasonable
ﬁt. Albert and Chib (1995) propose an alternative residual deﬁned at the
level of a latent variable called the liability (see Chapter 14 for a deﬁnition
of this concept). The reader is referred to their paper for details.

436
8. Bayesian Assessment of Hypotheses and Models
8.5.2
Predictive Ability and Predictive Cross-Validation
Predictive ability and goodness of ﬁt are distinct features of a model. A
certain model may explain and predict adequately the observations used
for model building. However, it may yield poor predictions of future obser-
vations or of data points that are outside the range represented in the data
employed for model building. A number of techniques is available for gaug-
ing the predictive ability of a Bayesian model. Even though some attention
is paid to foundational issues, the approaches here are often eclectic and
explorative. They constitute an important set of tools for understanding
the predictive ability of a model.
Cross-validation methods involve constructing the posterior distribution
of the parameters but leaving some observations out. Then the predictive
distributions of the observations that have been removed are derived to
examine whether or not the actual data points fall in regions of reasonably
high density. Partition the data as y′ =

yout, y′
−out

, where yout is the
observation to be removed, and y−out is the vector of the remaining obser-
vations. The density of the posterior predictive distribution can be written
as
p (yout|y−out, M) =

p (yout|θ, y−out, M) p (θ|y−out, M) dθ,
(8.51)
where p (θ|y−out, M) is the density of the posterior distribution built from
y−out and model M. In hierarchical modeling, one typically writes the
sampling distribution of the data such that conditional independence can
be exploited. Thus, given the parameters, yout is independent of y−out, and
one can write (suppressing the notation denoting model M)
p (yout|y−out) =

p (yout|θ) p (θ|y−out) dθ.
(8.52)
Since, in general, the form of the posterior density is unknown or analyti-
cally intractable, the predictive density will be calculated via Monte Carlo
methods (Gelfand et al., 1992; Gelfand, 1996). For example, if m draws
from the posterior distribution can be made via MCMC procedures, the
form of (8.52) suggests the estimator
5p (yout|y−out) = 1
m
m

j=1
p

yout|θ[j]
,
where θ[j] is a draw from [θ|y−out] . The mean and variance of the predic-
tive distribution can also be computed by Monte Carlo procedures. Since
the expected value of the sampling model can almost always be deduced
readily, e.g., in regression E (yout|θ) = x′
outβ, the mean of the predictive
distribution can be estimated as
5E (yout|y−out) = 1
m
m

j=1
E

yout|θ[j]
.
(8.53)

8.5 Goodness of Fit and Predictive Ability of a Model
437
Similarly, a Monte Carlo estimate of the variance of the predictive distri-
bution can be obtained as
D
V ar (yout|y−out) = 5E[θ|y−out] [V ar (yout|θ)] + D
V ar [E (yout|θ)] .
(8.54)
This can be illustrated with a regression model, although in this situation
there is an analytical solution under the standard assumptions. For exam-
ple, if the regression model postulates yout|β, σ2
e ∼N

x′
outβ, σ2
e

, then
5E[θ|y−out] [V ar (yout|θ)] = 1
m
m

j=1
σ2[j]
e
,
and
D
V ar[θ|y−out] [E (yout|θ)] = D
V ar [x′
outβ]
= 1
m
m

j=1

x′
outβ[j]2
−

1
m
m

j=1
x′
outβ[j]


2
.
Subsequently, the following composite statistic can be used to evaluate the
overall predictive ability of the model (Congdon, 2001):
D2 =
n

out=1

yout −5E (yout|y−out)
8
D
V ar (yout|y−out)


2
.
(8.55)
Models having a smaller value of D2 would be viewed as having a better pre-
dictive ability. Clearly, if n is very large, the computations may be taxing,
since n posterior and predictive distributions need to be computed. Other
statistics are described in Gelfand et al. (1992) and in Gelfand (1996).
A related idea has been advocated by Gelman et al. (1996). Rather than
working with the leave-one-out method in (8.52), they propose generating
data ,y from the posterior predictive distribution with density
p (,y|y,M) =

p (,y|θ,M) p (θ|y, M) dθ.
(8.56)
One then wishes to study whether the simulated value ,y agrees with the
observed data y. Systematic diﬀerences between the simulations and the
observed data indicate potential failure of model M. Various criteria or test
quantities can be used to carry out the comparisons. Examples of these are
given in Gelfand (1996). The choice of test quantities should be driven by
the aspect of the model whose ﬁt is in question and/or by the purpose with
which the model will be used. The method of composition (introduced in
Chapter 1), can be used to obtain draws from (8.56), and can be described
as follows:

438
8. Bayesian Assessment of Hypotheses and Models
1. Draw θ from the posterior distribution p (θ|y, M). Ways of achieving
this are discussed later in this book.
2. Draw ,y from the sampling distribution p (,y|θ,M). One has now a
single realization from the joint distribution p (,y, θ|M).
3. Repeat steps 1 and 2 many times.
The set of ,y′s drawn using this algorithm constitutes samples from (8.56).
Letting h (y) be a particular test quantity, for example, the average of the
top 10 observations, one can then study whether h (y) falls in a region of
high posterior probability in the distribution [h (,y) |y,M]. This can be re-
peated for all the models under investigation. Gelman et al. (1996) propose
the calculation of Bayesian p-values, pB, for given test quantities h (y, θ).
The notation emphasizes that, in contrast with classical p-values, the test
quantity can depend on both data and parameters. Then,
pB
=
p [h (,y, θ) > h (y, θ) |y]
=
 
I [h (,y, θ) > h (y, θ)] p (,y|θ) p (θ|y) dθd,y,
(8.57)
gives the probability that the simulated data ,y is more extreme than the
observed data y, averaged over the distribution [θ|y]. A possible test quan-
tity could be
h (y, θ) =
n

i=1
[yi −E (Yi|θ)]2
V ar (Yi|θ)
and
h (,y, θ) =
n

i=1

,yi −E

,Yi|θ
2
V ar

,Yi|θ

.
These are then used for computing (8.57). A cross-validation approach can
also be implemented using this idea. An application of these techniques in
animal breeding is in Sorensen et al. (2000).
Another way of assessing global predictive ability of a set of models was
proposed by Geisser and Eddy (1979) and by Geisser (1993) via the con-
ditional predictive ordinate (CPO). The logarithm of the CPO for Model
i is
log [CPOModel i] =
n

out=1
log [p (yout|y−out, Model i)] .
Gelfand and Dey (1994) describe techniques for calculating the CPO that
avoid carrying out the n implementations of the sampling procedure de-
scribed above. Chapter 12, especially Section 12.4, discusses Monte Carlo
implementation of these quantities in more detail.

8.6 Bayesian Model Averaging
439
8.6
Bayesian Model Averaging
8.6.1
General
Consider a survival analysis of sheep or of dairy cows. The information
available may consist of covariates such as herd or ﬂock, sire, year-season
of birth, molecular markers, and last known survival status, since censoring
is pervasive. The objective of the analysis may be to assess the eﬀects of
explanatory variables, or to predict the survival time of the future progeny
of some of the sires. Hence, one searches for some reasonable survival model
(e.g., Gross and Clark, 1975; Collet, 1994) and ﬁnds that a proportional
hazards model M1 ﬁts well and that it gives sensible parameter estimates.
Then one proceeds to make predictions. However, another proportional
hazards model M2 also ﬁts well, but it gives diﬀerent estimates and pre-
dictions. Which model should be used at the end?
Now imagine a standard regression analysis in which 15 predictor vari-
ables are available, and suppose that some “best” model must be sought.
Even if second-order and cross-product terms are ignored, there would be
215 diﬀerent models. For example, suppose that the variables are Y, X1, X2.
Then, using the standard notation, there are the following four possible
models
model 1
:
Y = β0 + e,
model 2
:
Y = β0 + β1X1 + e,
model 3
:
Y = β0 + β2X2 + e,
model 4
:
Y = β0 + β1X1 + β2X2 + e.
These models may diﬀer little in relative plausibility. Again, which model
ought to be used for predictions?
A third example is that of choosing between genetic models to infer
parameters, and to predict the genetic merit of future progeny. One spec-
iﬁcation may be the classical inﬁnitesimal model. A second speciﬁcation
may be a model with a ﬁnite number of loci. If so, how many? A third
model may pose polygenic variation, plus the eﬀects of some marked QTL.
The preceding three examples illustrate that the problem of model choice
is pervasive. Typically, models are chosen in some ad-hoc manner, and
inferences are based on the model eventually chosen, as if there were no
uncertainty about it. In Bayesian analysis, however, it is possible to view
the model as an item subject to uncertainty. Then the “model random
variable” is treated as a nuisance, and the posterior distribution of the
“model random variable” is used to obtain inferences that automatically
take into account the relative plausibility of the models under consideration.
This is called Bayesian model averaging, or BMA for short. We will outline
the basic ideas, and refer the reader to Madigan and Raftery (1994), Raftery
et al. (1997), and Hoeting et al. (1999) for additional details. These authors

440
8. Bayesian Assessment of Hypotheses and Models
argue as follows: since part of the evidence must be used in the process
of model selection, ignoring the uncertainty about the model leads to an
overstatement of precision in the analysis. In turn, this can lead to declaring
“false positives”, and the analysis lacks robustness unless, by chance, one
stumbles into the “right” model. It will be shown at the end of this section
that BMA can be used to enhance the predictive ability of an analysis.
8.6.2
Deﬁnitions
Let
∆
=
parameter or future data point,
y
=
data,
M
=
{M1, M2, . . . , MK} set of models,
p (Mi)
=
prior probability of model i,
p (Mi|y)
=
posterior probability of model i.
The “usual” Bayesian approach gives, as posterior distribution (or density)
of ∆,
p (∆|y, Mi) = p (y|∆, Mi) p (∆|Mi)
p (y|Mi)
,
and the notation indicates clearly that inferences are conditional on Mi, as
if the model were known to be true for sure. In BMA, on the other hand,
the idea is to average out over the posterior distribution of the models,
leading to
p (∆|y) = p (∆and M1|y) + · · · + p (∆and MK|y)
=
K

i=1
p (∆and Mi|y) =
K

i=1
p (∆|y, Mi) p (Mi|y) .
(8.58)
The preceding expression reveals that, in BMA, the model is treated as a
nuisance parameter. Hence, the nuisance is eliminated in the usual manner,
by integration or by summing. Then the inferences about a parameter can
be viewed as a weighted average of the inferences that would be drawn if
each of the models were true, using the posterior probability of the model
as a mixing distribution.
In BMA, the posterior expectation and variance are calculated in the
usual manner. For example, let the posterior mean of ∆under model k be
E (∆|Mk, y) =

∆p (∆|Mk, y) d∆= 5∆k
Then, unconditionally with respect to the model, one obtains
E (∆|y) = EM|y [E (∆|Mk, y)] =
K

k=1
5∆k p (Mk|y) .
(8.59)

8.6 Bayesian Model Averaging
441
Similarly, one can use the variance decomposition
V ar (∆|y) = EM|y [V ar (∆|M, y)] + V arM|y [E (∆|M, y)] ,
leading to
V ar (∆|y) =
K

k=1
V ar (∆|Mk, y) p (Mk|y) +
K

k=1

5∆k
2
p (Mk|y)
−
 K

k=1
5∆k p (Mk|y)
2
.
(8.60)
The idea is straightforward, and it makes eminent sense, at least from
a Bayesian perspective. The diﬃculty resides in that there can be many
models, as in a regression equation, where there may be at least 2p (for p
being the number of covariates) models, and even more when interactions
are included. Hoeting et al. (1999) discusses some of the methods that
have been used for reducing the number of terms to be included in the
sums appearing in (8.59) and (8.60).
8.6.3
Predictive Ability of BMA
Suppose one partitions the data as
y = [y′
Build, y′
Pred]′ ,
where yBuild is the data used for model building, and yPred includes the
data points to be predicted, as in predictive cross-validation. Good (1952)
introduced the predictive logscore (PLS) which, for Model k, is
PLSk = −

y∈yPred
log p (y|Mk, yBuild)
= −

y∈yPred
log

p (y|θk, Mk, yBuild) p (θk|Mk, yBuild) dθk,
(8.61)
where θk is the parameter vector under Model k. It it desirable to have a
model with as small a PLS as possible. Under BMA
PLSBMA = −

y∈yPred
log
 K

k=1
p (y|Mk, yBuild) p (Mk|yBuild)

.
(8.62)
Suppose that the model and the data to be predicted are unknown, which
is the usual situation. Now consider the diﬀerence
PLSBMA −PLSk = −

y∈yPred
log
K

k=1
p (y|Mk, yBuild) p (Mk|yBuild)
p (y|Mk, yBuild)
.

442
8. Bayesian Assessment of Hypotheses and Models
Next, take expectations of this diﬀerence with respect to the predictive
distribution under BMA (that is, averaging over all possible models). This
distribution has density
p (yPred|yBuild) =
K

k=1
p (yPred|Mk, yBuild) p (Mk|yBuild)].
Thus,
EyPred|yBuild (PLSBMA −PLSk)
= −

y∈yPred
E


log
K

k=1
p (y|Mk, yBuild) p (Mk|yBuild)
p (y|Mk, yBuild)


.
The expected value in the right hand side, taken over the distribution
[yPred|yBuild], is the Kullback–Leibler discrepancy between the predictive
distributions of datum y under BMA and under Model k. Since the dis-
crepancy is at least 0, it follows that the right-hand side is at most null.
Hence
EyPred|yBuild (PLSBMA) ≤EyPred|yBuild (PLSk) ,
as in Madigan and Raftery (1994). This implies that under model uncer-
tainty, the predictive performance of BMA (at least in the PLS sense) is
expected to be better than that obtained under a single model, even if the
latter is the most probable one. Raftery et al. (1997) and Hoeting et al.
(1999) present several study cases supporting this theoretical result.
Typically, BMA leads to posterior distributions that are more spread
than those under a single model. This illustrates that inferences based on a
single model may give an unrealistic statement of precision; this may lead
to false positive results.
The reader is now equipped with the foundations on which Bayesian
inference rests. As stated before and especially for complex models, it is
seldom the case that exact methods of inference can be used. Fortunately,
methods for sampling from posterior distributions are available, and these
are discussed in Part III of this book.

9
Approximate Inference
Via the EM Algorithm
9.1
Introduction
The classical paradigm of maximum likelihood estimation is based on ﬁnd-
ing the supremum of the likelihood function (if it exists), and on attaching
a measure of uncertainty via Fisher’s information measure, which has an
asymptotic justiﬁcation. An overview of the classical ﬁrst-order asymptotic
ML theory was presented in Chapters 3 and 4. The Bayesian counterpart of
this large sample theory consists of using an asymptotic approximation to
the posterior distribution. The most commonly used approximation relies
on computing the posterior mode and the observed information matrix.
Computation of maximum likelihood estimates with the Newton–Raphson
or scoring algorithms was dealt with in Chapter 4, but little has been said
so far of how the calculations should proceed in the approximate Bayesian
analysis.
In this chapter, an introductory account is given of one of the most versa-
tile iterative algorithms for computing maximum likelihood and posterior
modes: the expectation–maximization, or EM algorithm. This algorithm
is conceptually simple, at least in its basic form, and brings considerable
insight into the statistical structure of a maximum likelihood or posterior
mode problem, contrary to Newton–Raphson or scoring, which are based
primarily on numerical considerations. The chapter begins with a deﬁnition
of the concepts of complete and incomplete data. The subsequent section
presents a derivation of the algorithm in its basic form. Additional sections
of the chapter discuss properties of the algorithm, the special form it takes

444
9. Approximate Inference Via the EM Algorithm
when applied to exponential families, and extensions that have been sug-
gested for recovering measures of uncertainty. The chapter concludes with
a set of examples. Many developments and extensions have become avail-
able since its introduction by Dempster et al. (1977). Several of these can
be found in the comprehensive book of McLachlan and Krishnan (1997).
9.2
Complete and Incomplete Data
The EM algorithm was given its name in a celebrated paper by Dempster
et al. (1977). As mentioned above, this is an iterative method for ﬁnding ML
estimates or posterior modes in what are called incomplete-data problems.
The inﬂuence of the EM algorithm has been far reaching, not only as a
computational tool but as a way of solving diﬃcult statistical problems. A
main reason for this impact is because it is easy to implement. The basic
idea behind the method is to transform an incomplete- into a complete-
data problem for which the required maximization is computationally more
tractable. Also, the algorithm is numerically stable: each iteration increases
the likelihood or posterior density and convergence is nearly always to a
local maximum.
The concept of missing data is fairly broad. It includes, for example,
missing data in an unbalanced layout, but it extends to observations from
truncated distributions, censored data, and latent variables. In these cases,
one can view the complete data x as consisting of the vectors (y, z), where
y is the observed data or incomplete data, and z is the missing data. More
generally, many statistical problems which at ﬁrst glance do not appear to
involve missing data can be reformulated into missing-data problems, by
judicious augmentation of the data set, with unobserved values. As such,
one can view the observations at hand and the parameters of the posed
model as data: part of these data is observed (the records) and another
part is missing (the parameters). Mixed eﬀects and hierarchical models,
and models with latent variables, such as the threshold model, are typi-
cally amenable to an EM formulation. An example is an additive genetic
model where inference may focus on θ =

β′, σ2
a, σ2
e
′, where β is a vector
of location parameters and

σ2
a, σ2
e

are variance components. Here, one
may augment the observed data y, with the missing data a, the unob-
served vector of additive genetic values. As shown later, this simpliﬁes the
computations involved in ﬁnding the ML estimates of θ, or the maximum
of p

β, σ2
a, σ2
e|y

, or mode of the posterior distribution

β, σ2
a, σ2
e|y

. On
the other hand, if one wishes to ﬁnd the mode of the distribution with den-
sity p

σ2
a, σ2
e|y

, an EM strategy is to consider (β, a) as the missing data.
Here, if improper priors are adopted for the variance components and for
the location vector β, the mode is identical to the REML estimates of

σ2
a, σ2
e

. Another example is a regression model with t-distributed errors;

9.3 The EM Algorithm
445
this can be formulated as a standard weighted least-squares problem where
the missing data are related to the “weights”.
9.3
The EM Algorithm
9.3.1
Form of the Algorithm
Suppose that the objective is to draw inferences about the d × 1 vector
θ ∈Ωusing the mode of [θ|y] as point estimator. We will use p (θ|y) to
denote a posterior density (or a likelihood function if ﬂat priors are adopted
for the parameters) where, as usual, y is the vector of observed data. Let z
represent a vector of missing data, such as missing records or unobserved
“parameters” of the model, and let its conditional, p.d.f. be p (z|y, θ). The
marginal posterior density of θ is
p (θ|y) =

p (θ, z|y) dz,
where p (θ, z|y) is the joint posterior density of θ and z. The integration
above typically leads to an expression which makes p (θ|y) diﬃcult to max-
imize, even though maximization of p (θ, z|y) ∝p (θ|z, y) with respect to
θ may be trivial if z were observed. The EM algorithm formalizes an old
idea for dealing with missing-data problems. Starting with a guessed value
for the parameter θ, carry out the following iteration:
• Replace the missing data z by their expectation given the guessed
value of the parameters and the observed data. Let this conditional
expectation be ,z.
• Maximize p (θ, z|y) with respect to θ replacing the missing data z by
their expected values. This is equivalent to maximizing p (θ|,z, y).
• Reestimate the missing values z using their conditional expectation
based on the updated θ.
• Reestimate θ and continue until convergence is reached.
9.3.2
Derivation
Consider the identity
p (θ|y) = p (θ, z|y)
p (z|y, θ).
Taking logarithms on both sides leads to
ln p (θ|y) = ln p (θ, z|y) −ln p (z|y, θ) ,
(9.1)

446
9. Approximate Inference Via the EM Algorithm
where the ﬁrst term on the right-hand side is known as the complete-
data log-likelihood (more generally, as complete-data log-posterior). Now
take expectations of both sides with respect to

z|θ[t], y

, where θ[t] is the
current guess of θ. The left-hand side of (9.1) does not depend on z, so
averaging over z, providing the integrals exist, gives
ln p (θ|y) =

ln p (θ, z|y) p

z|θ[t], y

dz−

ln p (z|y, θ) p

z|θ[t], y

dz.
(9.2)
The ﬁrst term on the right-hand side of (9.2) is a function of θ for ﬁxed
y and ﬁxed θ[t], and it is denoted as Q

θ|θ[t]
in the EM literature. The
second term is denoted H

θ|θ[t]
. Thus,
ln p (θ|y) = Q

θ|θ[t]
−H

θ|θ[t]
.
(9.3)
The EM algorithm involves working with the ﬁrst term only, Q

θ|θ[t]
,
disregarding H

θ|θ[t]
. The two steps are:
1. E-step: calculation of Q

θ|θ[t]
, that is, the expectation of the com-
plete data log-likelihood (log-posterior) with respect to the condi-
tional distribution of the missing data, given the observed data and
the current guess for θ.
2. M-step: maximization of Q

θ|θ[t]
with respect to θ, solving for θ,
and setting the result equal to θ[t+1], the new value of the parameter.
Thus, if θ[t+1] maximizes Q

θ|θ[t]
, the M-step is such that
Q

θ[t+1]|θ[t]
≥Q

θ|θ[t]
,
for all θ ∈Ω,
(9.4)
which implies that θ[t+1] is a solution to the equation
∂Q

θ|θ[t]
∂θ
= 0.
(9.5)
The two steps are repeated iteratively until convergence is reached. It is
shown below that this iterative sequence leads to a monotonic increase of
ln p (θ|y). That is,
ln p

θ[t+1]|y

⩾ln p

θ[t]|y

.
(9.6)
Since the marginal posterior density increases in each step, the EM algo-
rithm, with few exceptions, converges to a local mode.

9.4 Monotonic Increase of ln p (θ|y)
447
In many important applications, the E-step involves replacing the miss-
ing data by their conditional expectations. In some cases, computation
of the E-step as formally dictated by step 1 above, can more easily dis-
close pathologies rendering the EM algorithm inapplicable. For instance,
see Flury and Zoppe (2000) for a case where the EM is not applicable
because the log-likelihood function takes the value zero in a subset of the
parameter space, so the relevant conditional expectation is not deﬁned.
In some models the calculation of Q

θ|θ[t]
in the E-step may be dif-
ﬁcult. Wei and Tanner (1990) propose a Monte Carlo approach for overcom-
ing this diﬃculty. This consists of simulating z1, z2, . . . , zm from p

z|θ[t], y

and then forming the simulation consistent estimator
5Q

θ|θ[t]
≈1
m
m

i=1
ln p (θ, zi|y) .
9.4
Monotonic Increase of ln p (θ|y)
with Each EM Iteration
Consider a sequence of iterates θ[0], θ[1], . . . , θ[t+1]. The diﬀerence in value
of ln p (θ|y) in successive iterates is obtained from (9.3) as
ln p

θ[t+1]|y

−ln p

θ[t]|y

= Q

θ[t+1]|θ[t]
−Q

θ[t]|θ[t]
−

H

θ[t+1]|θ[t]
−H

θ[t]|θ[t]
.
(9.7)
The diﬀerence between Q (·) functions on the right-hand side is nonnegative
due to (9.4). Therefore (9.6) holds if the diﬀerence in H (·) above is non-
positive; that is, if
H

θ[t+1]|θ[t]
−H

θ[t]|θ[t]
≤0.
(9.8)
Now for any θ,
H

θ|θ[t]
−H

θ[t]|θ[t]
=

ln p (z|y, θ) p

z|θ[t], y

dz
−

ln p

z|y, θ[t]
p

z|θ[t], y

dz
=

ln

p (z|y, θ)
p

z|y, θ[t]

p

z|θ[t], y

dz
= −

ln


p

z|y, θ[t]
p (z|y, θ)

p

z|θ[t], y

dz.
(9.9)

448
9. Approximate Inference Via the EM Algorithm
The integral is the Kullback–Leibler distance between the distributions
with densities p

z|y, θ[t]
and p (z|y, θ) , which is at least 0. Hence, this
integral preceded by a negative sign is at most 0. Thus, H

θ|θ[t]
≤
H

θ[t]|θ[t]
. This establishes (9.8) and, hence, inequality (9.6). In well
behaved problems the sequence of EM iterates converges to a stationary
point which is a global maximum, in which case EM yields the unique
posterior mode or ML estimate of θ, the maximizer of ln p (θ|y).
Since
H

θ|θ[t]
≤H

θ[t]|θ[t]
for all θ, this implies that H

θ|θ[t]
has a maximum at θ = θ[t]. Hence,
∂H

θ|θ[t]
∂θ
""""""
θ=θ[t]
=
∂H

θ[t]|θ[t]
∂θ
= 0.
(9.10)
Therefore from (9.3),
∂ln p (θ|y)
∂θ
""""
θ=θ[t] =
∂Q

θ|θ[t]
∂θ
""""""
θ=θ[t]
.
(9.11)
9.5
The Missing Information Principle
9.5.1
Complete, Observed and Missing Information
Recall the identity
ln p (θ|y) = ln p (θ, z|y) −ln p (z|y, θ) .
Diﬀerentiating twice with respect to θ and multiplying by −1 gives
−∂2 ln p (θ|y)
∂θ ∂θ′
= −∂2 ln p (θ, z|y)
∂θ ∂θ′
+ ∂2 ln p (z|y, θ)
∂θ ∂θ′
.
(9.12)
Note that the left-hand side is not a function of z. Now taking expectations
of both sides over the distribution [z|y, θ] gives
−∂2 ln p (θ|y)
∂θ ∂θ′
= −
 ∂2 ln p (θ, z|y)
∂θ ∂θ′
p (z|y, θ) dz
+
 ∂2 ln p (z|y, θ)
∂θ ∂θ′
p (z|y, θ) dz.
(9.13)

9.5 The Missing Information Principle
449
Recalling the deﬁnition of the Q and H functions given in (9.2), and pro-
vided that the integral and diﬀerential operations are interchangeable, one
arrives at
−∂2 ln p (θ|y)
∂θ ∂θ′
= −∂2Q (θ|θ)
∂θ ∂θ′
−

−∂2H (θ|θ)
∂θ ∂θ′

.
(9.14)
If one calls the ﬁrst and second terms on the right-hand side as “complete”
and “missing” information, respectively, (9.14) has the following interpre-
tation (Louis, 1982):
Observed information = complete information −missing information.
This can be represented as
I (θ|y) = Ic (θ|y) −Im (θ|y) .
(9.15)
The rate of convergence of the EM algorithm is related to these matrix-
valued quantities: the larger the proportion of missing information (relative
to the complete information), the slower the rate of convergence. This can
be represented in the following manner (a formal justiﬁcation for the ex-
pression is given later on):
θ[t+1] −θ∗= [Ic (θ∗|y)]−1 Im (θ∗|y)

θ[t] −θ∗
.
(9.16)
The preceding indicates that as the EM iteration proceeds, the distance
between the iterates and θ∗(a stationary point of the likelihood func-
tion or posterior density) is a function of the rate of convergence matrix
[Ic (θ∗|y)]−1 Im (θ∗|y). In short, the larger the proportion of missing in-
formation, the slower the algorithm proceeds towards θ∗, and the form of
(9.16) suggests a linear approach towards θ∗. Note from (9.15) that the
rate of convergence matrix can be written also as
[Ic (θ∗|y)]−1 Im (θ∗|y) = Id −[Ic (θ∗|y)]−1 I (θ∗|y) ,
(9.17)
where Id is the identity matrix of dimension d, the number of elements in
θ.
9.5.2
Rate of Convergence of the EM Algorithm
A formal derivation of (9.16) is presented here. Following McLachlan and
Krishnan (1997), consider a Taylor series expansion of the score vector
∂ln p (θ|y)
∂θ
about the point θ = θ[t]. This yields
∂ln p (θ|y)
∂θ
≈
∂ln p

θ[t]|y

∂θ
−I

θ[t]|y
 
θ −θ[t]
.

450
9. Approximate Inference Via the EM Algorithm
Setting θ = θ∗, the term on the left-hand side vanishes and one obtains
after rearrangement
θ∗≈θ[t] +

I

θ[t]|y
−1 ∂ln p

θ[t]|y

∂θ
.
(9.18)
Now expand ∂Q

θ|θ[t]
/∂θ
"""
θ=θ[t+1] in a linear Taylor series about θ = θ[t]:
∂Q

θ[t+1]|θ[t]
∂θ
≈
∂Q

θ[t]|θ[t]
∂θ
+
∂2Q

θ[t]|θ[t]
∂θ∂θ′

θ[t+1] −θ[t]
.
From (9.5), the term on the left-hand side is equal to zero. Making use of
this in the preceding expression and employing the notation in (9.15), this
can be written as
∂Q

θ[t]|θ[t]
∂θ
≈Ic

θ[t]|y
 
θ[t+1] −θ[t]
which, from (9.11), is
∂ln p

θ[t]|y

∂θ
≈Ic

θ[t]|y
 
θ[t+1] −θ[t]
.
(9.19)
Substituting approximation (9.19) into (9.18) yields
θ∗−θ[t]
≈

I

θ[t]|y
−1
Ic

θ[t]|y
 
θ[t+1] −θ[t]
=

I

θ[t]|y
−1
Ic

θ[t]|y
 
θ[t+1] −θ∗+ θ∗−θ[t]
.
Therefore,

Id −

I

θ[t]|y
−1
Ic

θ[t]|y
% 
θ∗−θ[t]
≈

I

θ[t]|y
−1
Ic

θ[t]|y
 
θ[t+1] −θ∗
.
Premultiplying both sides by

Ic

θ[t]|y
−1
I

θ[t]|y

yields
θ[t+1] −θ∗≈

Id −

Ic

θ[t]|y
−1
I

θ[t]|y
% 
θ[t] −θ∗
.
In view of (9.17), this is expressible as
θ[t+1] −θ∗≈

Ic

θ[t]|y
−1
Im

θ[t]|y
 
θ[t] −θ∗
,
and for θ[t] close to θ∗this leads to the desired result (9.16) directly.

9.6 EM Theory for Exponential Families
451
9.6
EM Theory for Exponential Families
The EM algorithm has a simpler form when the complete data x = (y′, z′)′
have a distribution from the regular exponential family. The density can
be written in its canonical form as
p (x|θ) = b (x) exp

θ′t (x)

a (θ)
.
(9.20)
In this expression, the vector θ is the natural or canonical parameter in-
dexing the distribution, b (x) is a function of the complete data alone, a (θ)
is a function of the vector θ alone, and t (x) is the d×1 vector of complete-
data suﬃcient statistics. Many common distributions can be put in the
form (9.20), which is characterized by a number of nice statistical proper-
ties. In an exponential family, the statistic t (x) carries all the information
about θ contained in the data x; therefore, inferences about θ can be based
solely on t (x). In a Bayesian context this means that the posterior distri-
bution of θ given x is identical to the posterior distribution [θ|y] . The ML
or posterior mode equations (the score) take a particularly simple form. To
see this, take logarithms on both sides of (9.20):
ln p (x|θ) = ln b (x) + θ′t (x) −ln a (θ) .
(9.21)
In the context of ML estimation, the score is
∂ln p (x|θ)
∂θ
= t (x) −∂ln a (θ)
∂θ
.
(9.22)
Taking expectations of both sides over the sampling model for the complete
data with density p (x|θ), and recalling from ML theory that
E
∂ln p (x|θ)
∂θ

= 0
leads to
1
a (θ)
∂a (θ)
∂θ
=

t (x) p (x|θ) dx
=
E [t (x|θ)] .
(9.23)
This is the expected value of the vector of suﬃcient statistics, given θ.
Now from (9.22), and in the context of likelihood-based inference, the ML
estimator of θ is the solution to the equations
∂ln p (x|θ)
∂θ
= t (x) −E [t (x|θ)] = 0,
or, equivalently,
t (x) = E [t (x|θ)] .
(9.24)

452
9. Approximate Inference Via the EM Algorithm
If equations (9.24) can be solved for θ, then the solution is unique due to
the convexity property of the log-likelihood for regular exponential fami-
lies. When the equations are not solvable, the maximizer of θ lies on the
boundary of the parameter space (McLachlan and Krishnan, 1997).
The regular exponential family of distributions also leads to a simple rep-
resentation of the complete-data information matrix. Note from (9.21), that
the second derivatives of the complete-data log-likelihood do not depend
on x, and that
−∂2 ln p (x|θ)
∂θ ∂θ′
= ∂2 ln a (θ)
∂θ ∂θ′
= Ic (θ) ,
(9.25)
where Ic (θ) is Fisher’s expected information matrix.
Return now to the computation of EM to obtain the ML estimator of θ
when the complete data can be written in the form of (9.20). The E-step
is given by
Q

θ|θ[t]
=

ln p (y, z|θ) p

z|θ[t], y

dz
=

ln b (x) p

z|θ[t], y

dz + θ′

t (x) p

z|θ[t], y

dz −ln a (θ) .
(9.26)
The M-step consists of diﬀerentiating this expression with respect to θ.
Since the ﬁrst term does not depend on θ,
∂Q

θ|θ[t]
∂θ
=

t (x) p

z|θ[t], y

dz−
1
a (θ)
∂a (θ)
∂θ
= E

t (x) |θ[t], y

−E [t (x|θ)] ,
(9.27)
where the ﬁrst term on the right-hand side is the conditional expected value
of the vector of suﬃcient statistics, given the observed data and the current
value of θ. It follows from (9.27) that in the M-step, θ[t+1] is chosen by
solving the equations
E

t (x) |θ[t], y

= E [t (x|θ)] ,
(9.28)
so the form of the algorithm is quite simple.
9.7
Standard Errors and Posterior
Standard Deviations
One of the early criticisms of the EM approach was that, unlike the Newton–
Raphson and related methods, it does not automatically produce an esti-
mate of the asymptotic covariance matrix of the ML estimators of θ or some

9.7 Standard Errors and Posterior Standard Deviations
453
indication of the uncertainty in the posterior distribution of a parameter
of interest. A number of ways of computing estimates of the asymptotic
covariance matrix of the ML estimates have been suggested over the last
few years. Important contributions include those of Louis (1982), Meilijson
(1989), Meng and Rubin (1991), Lange (1995), and Oakes (1999). All meth-
ods make use of asymptotic theory. Three of these approaches will be de-
scribed here.
9.7.1
The Method of Louis
Louis (1982) showed that
∂2 ln p (θ|y)
∂θ ∂θ′
= Ez|θ,y
∂2 ln p (θ, z|y)
∂θ ∂θ′

+ V arz|θ,y
∂ln p (θ, z|y)
∂θ

,
(9.29)
which, on multiplication by −1, yields (9.15). Thus,
Ic (θ|y) = Ez|θ,y

−∂2 ln p (θ, z|y)
∂θ∂θ′

(9.30)
and
Im (θ|y) = V arz|θ,y
∂ln p (θ, z|y)
∂θ

.
(9.31)
To prove (9.29), ﬁrst note that
∂ln p (θ|y)
∂θ
=
1
p (θ|y)
∂
∂θ

p (θ, z|y) dz
=
1
p (θ|y)
 ∂ln p (θ, z|y)
∂θ
p (θ, z|y) dz
=
 ∂ln p (θ, z|y)
∂θ
p (z|θ, y) dz
= Ez|θ,y
∂ln p (θ, z|y)
∂θ

.
(9.32)
Now, take derivatives with respect to θ again
∂
∂θ
∂ln p (θ|y)
∂θ′

= ∂
∂θ
 ∂ln p (θ, z|y)
∂θ
p (z|θ, y) dz

=
 ∂2 ln p (θ, z|y)
∂θ ∂θ′
p (z|θ, y) dz+
 ∂ln p (θ, z|y)
∂θ
∂p (z|θ, y)
∂θ′
dz
= Ez|θ,y
∂2 ln p (θ, z|y)
∂θ∂θ′

+
 ∂ln p (θ, z|y)
∂θ
∂ln p (z|θ, y)
∂θ′
p (z|θ, y) dz.
(9.33)

454
9. Approximate Inference Via the EM Algorithm
The second term in (9.33) can be manipulated as follows
 ∂ln p (θ, z|y)
∂θ
∂ln p (z|θ, y)
∂θ′
p (z|θ, y) dz
=
 ∂ln p (θ, z|y)
∂θ
∂ln p (θ, z|y)
∂θ′
−∂ln p (θ|y)
∂θ′

p (z|θ, y) dz
=
 ∂ln p (θ, z|y)
∂θ
∂ln p (θ, z|y)
∂θ′
p (z|θ, y) dz
−
 ∂ln p (θ, z|y)
∂θ
∂ln p (θ|y)
∂θ′
p (z|θ, y) dz
= Ez|θ,y
∂ln p (θ, z|y)
∂θ
∂ln p (θ, z|y)
∂θ′

−
 ∂ln p (θ, z|y)
∂θ
∂ln p (θ|y)
∂θ′
p (z|θ, y) dz.
In view of (9.32), the last two lines of the preceding expression can be
written as
Ez|θ,y
∂ln p (θ, z|y)
∂θ
∂ln p (θ, z|y)
∂θ′

−
 ∂ln p (θ, z|y)
∂θ
Ez|θ,y
∂ln p (θ, z|y)
∂θ′

p (z|θ, y) dz
= Ez|θ,y
∂ln p (θ, z|y)
∂θ
∂ln p (θ, z|y)
∂θ′

−Ez|θ,y
∂ln p (θ, z|y)
∂θ

Ez|θ,y
∂ln p (θ, z|y)
∂θ′

= V arz|θ,y
∂ln p (θ, z|y)
∂θ

.
Hence, (9.33) becomes
∂2 ln p (θ|y)
∂θ∂θ′
= Ez|θ,y
∂2 ln p (θ, z|y)
∂θ∂θ′

+ V arz|θ,y
∂ln p (θ, z|y)
∂θ

,
thus establishing (9.29), and the matrix of second derivatives from which
a measure of uncertainty can be derived. Tanner (1996) suggests a Monte
Carlo approximation to (9.29) that can be used when the integrals over
p (z|θ, y) are diﬃcult to obtain analytically.
9.7.2
Supplemented EM Algorithm (SEM)
This method was proposed by Meng and Rubin (1991) who showed that
the asymptotic covariance matrix of the ML of θ (evaluated at 5θ, the ML

9.7 Standard Errors and Posterior Standard Deviations
455
estimator) is equal to

I

5θ|y
−1
=

Ic

5θ|y
−1
+

Id −∆

5θ|y
−1
∆

5θ|y
 
Ic

5θ|y
−1
.
(9.34)
Here, the term ∆

5θ|y

=

Ic

5θ|y
−1
Im

5θ|y

is the rate of conver-
gence matrix and, as before, Id is an identity matrix of dimension d × d.
The ﬁrst term on the right-hand side of (9.34) is the asymptotic covariance
matrix based on the complete-data log-likelihood and averaged over the
distribution

z|5θ, y


Ic

5θ|y
−1
=

E

−∂2 ln p (θ, z|y)
∂θ ∂θ′
"""" θ, y
""""
θ=θ
−1
=

−
∂2
∂θ ∂θ′

ln p (θ, z|y) p

z|5θ, y

dz
""""
θ=θ
−1
=

−
∂2Q

θ|5θ

∂θ ∂θ′
""""""
θ=θ


−1
.
Often, this can be computed analytically, and it is simple to calculate when
the density of the complete-data distribution is in the exponential family
form (9.20), as indicated in (9.25).
The derivation of (9.34) is as follows. From (9.15),
I

5θ|y

=
Ic

5θ|y
 
Id −

Ic

5θ|y
−1
Im

5θ|y

=
Ic

5θ|y
 
Id −∆

5θ|y

.
Inverting this expression establishes (9.34)

I

5θ|y
−1
=

Id −∆

5θ|y
−1 
Ic

5θ|y
−1
=

Id +

Id −∆

5θ|y
−1
∆

5θ|y
% 
Ic

5θ|y
−1
=

Ic

5θ|y
−1
+

Id −∆

5θ|y
−1
∆

5θ|y
 
Ic

5θ|y
−1
.
The second line arises from the matrix algebra result

A −BD−1C
−1 = A−1 + A−1B

D −CA−1B
−1 CA−1,
after setting A = B = D = Id and C = ∆

5θ|y

.

456
9. Approximate Inference Via the EM Algorithm
In order to compute the rate of convergence matrix, ∆

5θ|y

, Meng and
Rubin (1991) suggest the following approach:
• Run the EM algorithm until convergence, and ﬁnd
5θ =

5θ1,5θ2, . . . ,5θd
′
,
the maximizer of ln p (θ|y).
• Choose a starting point for θ diﬀerent from 5θ in all components. A
possible starting point is the one used for the original EM calculation.
Run the EM algorithm through t iterations.
1. INPUT: θ[t] and 5θ. Run the usual E- and M-steps to obtain
θ[t+1]. Repeat the following steps (a) and (b) for i = 1, 2, . . . , d,
to obtain a matrix R[t] with element r[t]
ij deﬁned below.
(a) Construct
θ[t] (i) =

5θ1,5θ2, . . . ,5θi−1, θ[t]
i ,5θi+1, . . . ,5θd

.
(b) Use θ[t] (i) as the input value for one EM step to obtain
θ[t+1] (i) with elements θ[t+1]
j
(i), (j = 1, 2, . . . , d). The ith
row of R[t] is obtained as
r[t]
ij =
θ[t+1]
j
(i) −5θj
θ[t]
i −5θi
,
j = 1, 2, . . . , d.
(9.35)
2. OUTPUT: θ[t+1] and
9
r[t]
ij ,
(i, j = 1, 2, . . . , d)
:
. Set t = t + 1
and GO TO 1.
When the value of an element rij no longer changes between successive
iterates, this represents a numerical estimate of the corresponding element
in ∆

5θ|y

. It is possible that diﬀerent values of t may be required for
diﬀerent rij components. Meng and Rubin (1991) discuss many relevant
implementation issues. The reader is referred to their paper for details. The
numerical estimate of ∆

5θ|y

and the expression derived analytically for
Ic

5θ|y

are then used in expression (9.34) to obtain the desired asymptotic
covariance matrix of 5θ.

9.7 Standard Errors and Posterior Standard Deviations
457
9.7.3
The Method of Oakes
Oakes (1999) provided an expression for the observed information matrix
based on the second derivatives of Q

θ|θ[t]
, the conditional expectation
of the complete-data log-likelihood given the observed data and a ﬁxed
value of θ = θ[t]. Oakes’ formula is
I (θ|y)
=
−∂2 ln p (θ|y)
∂θ ∂θ′
=
−




∂2Q

θ|θ[t]
∂θ ∂θ′
+
∂2Q

θ|θ[t]
∂θ ∂θ′[t]


""""""
θ=θ[t]

.
(9.36)
The second term on the right-hand side corresponds to the information
from the missing data, and is equal to Im (θ|y) evaluated at θ = θ[t].
In order to derive (9.36), results (3.20) and (3.26) from Chapter 3 will
be used. These results are valid for any θ, and therefore

∂
∂θ[t] ln p

z|y, θ[t]
p

z|y, θ[t]
dz
= Ez|y,θ[t]

∂
∂θ[t] ln p

z|y, θ[t]
= 0,
(9.37)
and
Ez|y,θ[t]

∂
∂θ[t] ln p

z|y, θ[t] 
∂
∂θ[t] ln p

z|y, θ[t]′
= −Ez|y,θ[t]

∂2
∂θ[t]∂θ′[t] ln p

z|y, θ[t]
.
(9.38)
In the development that follows, both θ and θ[t] are regarded as argu-
ments of Q. To prove (9.36), ﬁrst diﬀerentiate (9.3) with respect to θ; this
gives
∂ln p (θ|y)
∂θ
=
∂Q

θ|θ[t]
∂θ
−
 ∂ln p (z|y, θ)
∂θ
p

z|y, θ[t]
dz.
(9.39)
Setting θ = θ[t], the second term vanishes because of (9.37), and the score
for the observed data becomes
∂ln p (θ|y)
∂θ
=
∂Q

θ|θ[t]
∂θ
""""""
θ=θ[t]
.
(9.40)
Take now partial derivatives of (9.39) with respect to θ and θ[t]. First
note that the left hand side of (9.39) is not a function of θ[t], and that one

458
9. Approximate Inference Via the EM Algorithm
may write
∂ln p (z|y, θ)
∂θ


∂p

z|y, θ[t]
∂θ[t]


′
= ∂ln p (z|y, θ)
∂θ


∂ln p

z|y, θ[t]
∂θ[t]


′
p

z|y, θ[t]
.
Then diﬀerentiating (9.39 ﬁrst with respect to θ yields
∂2 ln p (θ|y)
∂θ ∂θ′
=
∂2Q

θ|θ[t]
∂θ ∂θ′
−
 ∂2 ln p (z|y, θ)
∂θ ∂θ′
p

z|y, θ[t]
dz
=
∂2Q

θ|θ[t]
∂θ ∂θ′
−Ez|y,θ[t]
∂2 ln p (z|y, θ)
∂θ ∂θ′

,
and, second, with respect to θ[t], yields
0 =
∂2Q

θ|θ[t]
∂θ∂θ′[t]
−
 ∂ln p (z|y, θ)
∂θ


∂p

z|y, θ[t]
∂θ[t]


′
dz
=
∂2Q

θ|θ[t]
∂θ∂θ′[t]
−Ez|y,θ[t]

 ∂
∂θ ln p

z|y, θ[t] 
∂
∂θ[t] ln p

z|y, θ[t]′
.
Substituting θ = θ[t], adding, using identity (9.38), and multiplying both
sides by −1, retrieves (9.36).
As Oakes (1999) points out, (9.36) together with (9.40), implies that
the function Q

θ|θ[t]
can be used to perform standard Newton–Raphson
maximization of the observed data likelihood.
9.8
Examples
Illustrations of the EM-related computations are presented ﬁrst with two
examples involving discrete data and a multinomial sampling model. The
third and fourth examples deal with inferences about variance components
in a Gaussian hierarchical model.
Example 9.1
A multinomial example
Example 4.8 from Chapter 4 illustrated estimation of the recombination

9.8 Examples
459
Class
Genotypic class
Frequency
1
AA/BB
α2 /4
2
Aa/Bb
(α2 + (1 −α)2) /2
3
AA/Bb
α (1 −α) /2
4
Aa/BB
α (1 −α) /2
5
aa/bb
α2 /4
6
Aa/bb
α (1 −α) /2
7
aa/Bb
α (1 −α) /2
8
AA/bb
(1 −α)2 /4
9
aa/BB
(1 −α)2 /4
TABLE 9.1. Genotypic classes and frequencies from a mating between repulsion
heterozygotes.
fraction α between loci A and B using coupling heterozygotes. Here, α is
estimated with data from repulsion heterozygotes. The gametes produced
are AB, ab, Ab, and aB with respective frequencies α/2, α/2, (1 −α) /2,
and (1 −α) /2. Random union of these gametes produces the genotypic
classes shown in Table 9.1.
Assuming complete dominance at both loci, the phenotypic classes which
are distinguishable (the observed data) are shown in Table 9.2, together
with the number of observations for each phenotype and their expected
frequencies. For example, the frequency of phenotype AB is obtained sum-
ming genotypic classes 1, 2, 3, and 4.
Maximum Likelihood Estimation
Denoting θ = α2 and n = (n1, n2, n3, n4)′, the observed data likelihood is
in the form of a multinomial distribution
p (θ|n) ∝

1
2 + 1
4θ
n1 
1
4 −1
4θ
n3 
1
4 −1
4θ
n4 
1
4θ
n2
and the observed data log-likelihood, excluding an additive constant, is
ln p (θ|n) = n1 ln (2 + θ) + (n3 + n4) ln (1 −θ) + n2 ln θ.
The score is equal to
∂ln p (θ|n)
∂θ
=
n1
2 + θ −n3 + n4
1 −θ
+ n2
θ .
Phenotype
Frequency
Number observed
AB
1/2 + α2/4
n1
ab
α2/4
n2
aB
1/4 −α2/4
n3
Ab
1/4 −α2/4
n4
TABLE 9.2. Distribution of phenotypic classes with complete dominance.

460
9. Approximate Inference Via the EM Algorithm
The observed information is
−∂2 ln p (θ|n)
∂θ2
=
n1
(2 + θ)2 + n3 + n4
(1 −θ)2 + n2
θ2 .
(9.41)
Upon setting the score to 0, one obtains a quadratic equation in θ with
one positive root, which is the ML estimator of θ. For example, for n =
(20, 12, 120, 117)′, one obtains 5θ = 0.050056 and 5α =
>
5θ = 0.22. The
observed information evaluated at 5θ is
I

5θ|n

= 5056.6997.
(9.42)
The estimate of the asymptotic variance of 5θ based on the observed infor-
mation is:

V ar

5θ

=

−∂2 ln p (θ|n)
∂θ2
""""
θ=θ
−1
= 0.000198,
(9.43)
and the estimate of the asymptotic variance of 5α is

V ar (5α) =

V ar

5θ
 
dα
dθ
2"""""
θ=θ

= 0.000986.
Note that the transformation θ = α2 is not one-to-one; therefore the ex-
pression above for

V ar (5α) can be misleading. However, it can be veriﬁed
that in this case, parameterizing the likelihood in terms of α rather than
in terms of θ, yields and estimate of the asymptotic V ar (5α) based on the
inverse of the observed information equal to 0.000988.
Computation via the EM Algorithm
The ML estimator of θ is obtained now using the EM algorithm. The choice
of missing data must be based on the form of the resulting complete-data
likelihood. Suppose that we split the ﬁrst of the four multinomial cells n1,
with associated frequency

1/2 + α2/4

into two parts, n11 and n12, with
associated frequencies 1/2 and α2/4, respectively. Thus, n1 = n11+n12 and
the missing data is z = (n11, n12)′. The complete-data likelihood is now
p (θ, z|n) ∝

1
2
n11 
1
4θ
n12 
1
4 −1
4θ
n3 
1
4 −1
4θ
n4 
1
4θ
n2
.
The complete-data log-likelihood, excluding an additive constant, is
ln p (θ, z|n) = (n12 + n2) ln θ + (n3 + n4) ln (1 −θ) ,
which is the log-likelihood of the binomial probability model
Bi (n12 + n2|θ, n12 + n2 + n3 + n4) .

9.8 Examples
461
The score is
∂ln p (θ, z|n)
∂θ
= n12 + n2
θ
−n3 + n4
1 −θ ,
which is linear in θ. If n12 were observed, the ML estimator of θ is obtained
by setting the score equal to 0. The explicit solution is
5θ =
n12 + n2
n12 + n2 + n3 + n4
.
(9.44)
Of course, 5θ cannot be obtained from (9.44) because n12 is not observed.
Instead, n12 is replaced in (9.44) by its conditional expectation, given the
observed data n and θ[t]. The E-step of the EM algorithm consists of eval-
uating
Q

θ|θ[t]
= E
9
[(n12 + n2) ln θ + (n3 + n4) ln (1 −θ)]| n, θ[t]:
=

E

n12|n, θ[t]
+ n2

ln θ + (n3 + n4) ln (1 −θ) .
(9.45)
The equality in the second line arises because, conditionally on n, the only
random variable in (9.45) is n12. The M-step consists of choosing θ[t+1] as
the solution to the equation
∂Q

θ|θ[t]
∂θ
= 0.
This yields
θ[t+1] =
E

n12|n, θ[t]
+ n2
E

n12|n, θ[t]
+ n2 + n3 + n4
,
(9.46)
which has the same form as (9.44), with n12 replaced by its conditional
expectation, E

n12|n, θ[t]
. The EM scheme calculates E

n12|n, θ[t]
and
(9.46) in an iterative fashion, until convergence is reached.
In order to obtain E

n12|n, θ[t]
= E

n12|n11 + n12, θ[t]
, recall from
Example 2.15 in Chapter 2 that
E

n12|n11 + n12, θ[t]
=
(n11 + n12) θ[t]/4
1
2 + θ[t]
4
=
n1
θ[t]
2 + θ[t] ,
because
n12|θ, n1 ∼Bi

θ/4
1
2 + θ
4
, n1

.
(9.47)

462
9. Approximate Inference Via the EM Algorithm
Iteration
θ[t]
r[t]
1
0.063241
0.03598
2
0.050530
0.03623
3
0.050073
0.03624
4
0.050056
0.03624
...
...
·
10
0.050056
·
TABLE 9.3.
Using θ[0] = 0.5 as the starting value, the results of the iterative EM se-
quence are shown in Table 9.3. After a few iterations the algorithm con-
verges to 5θ = 0.050056.
A Monte Carlo EM algorithm along the lines suggested by Wei and
Tanner (1990) can be implemented by replacing E

n12|n, θ[t]
in (9.46)
with
n12 = 1
m
m

i=1
zi
where z1, z2, . . . , zm are draws from (9.47).
Rate of Convergence and Standard Errors
The third column of Table 9.3 shows the evolution of the rate of convergence
given by (9.35), which for this one-dimensional example is
r[t] = θ[t+1] −5θ
θ[t] −5θ
.
The rate of convergence of the EM iteration is 0.03624.
Now we calculate Ic

5θ|n

and Im

5θ|n

.
Ic

5θ|n

= −
∂2Q

θ|5θ

(∂θ)2
""""""
θ=θ
.
Diﬀerentiating (9.45) twice with respect to θ gives
−
∂2Q

θ|5θ

(∂θ)2
=
E

n12|n,5θ

+ n2
θ2
+ n3 + n4
(1 −θ)2
=
n1
θ
2+θ + n2
θ2
+ n3 + n4
(1 −θ)2 .
(9.48)

9.8 Examples
463
Evaluated at θ = 5θ, (9.48) is equal to
Ic

5θ|n

= 5246.84.
(9.49)
To calculate Im

5θ|n

, use is made of (9.31). This produces
V ar
 ∂
∂θ (n12 + n2) ln θ + (n3 + n4) ln (1 −θ)
"""" n, θ

= V ar
 ∂
∂θ (n12 ln θ)| n, θ

= V ar
 n12
θ
""" n, θ

=
2n1
θ (2 + θ)2 ,
where the last line follows from the variance of the binomial distribution
(9.47). When evaluated at θ = 5θ, the expression above is equal to
Im

5θ|n

= 190.14.
(9.50)
From (9.42), (9.49), and (9.50), (9.15) can be veriﬁed:
5056.70 = 5246.84 −190.14.
Second, the rate of convergence ∆

5θ|n

is

Ic

5θ|n
−1
Im

5θ|n

= 190.14
5246.84 = 0.036238,
as obtained in the third column of Table 9.3.
The asymptotic variance based on (9.34) is given by

I

5θ|n
−1
= (5246.84)−1 +
(1 −0.03624)−1 (0.03624) (5246.84)−1
= 0.0001978,
in agreement with (9.43).
In order to obtain the asymptotic variance of 5θ based on (9.36), ﬁrst
compute
∂2Q

θ|θ[t]
∂θ∂θ[t]
=
2n1
θ

2 + θ[t]2 ,
(9.51)
and, from (9.48),
∂2Q

θ|θ[t]
(∂θ)2
= −
n1
θ
2+θ + n2
θ2
−n3 + n4
(1 −θ)2 .
(9.52)

464
9. Approximate Inference Via the EM Algorithm
When θ[t] and θ are evaluated at 5θ, the sum of these two expressions times
−1 is equal to:
−


∂2Q

θ|θ[t]
(∂θ)2
+
∂2Q

θ|θ[t]
∂θ∂θ[t]
""""""
θ[t],θ=θ


=
−(−5246.84 + 190.14)
=
5056.70,
which agrees with (9.42) and (9.41).
■
Example 9.2
Blood groups
In Example 4.7 from Chapter 4, the ML estimates of the frequency of blood
group alleles were computed via Newton–Raphson. Here the EM algorithm
is used instead. Let n = (nA, nAB, nB, nO)′ be the observed data, with
nA = 725, nAB = 72, nB = 258, nO = 1073. It is sensible to treat the
unobserved counts nAO, nAA, nBB and nBO as missing data. The resulting
complete-data vector is
nc = (nAA, nAO, nAB, nBB, nBO, nO)′ .
The complete-data log-likelihood excluding an additive constant is
ln f (pA, pB|nc) = 2nAA ln (pA) + nAO ln (2pApO) + nAB ln (2pApB)
+ 2nBB ln (pB) + nBO ln (2pBpO) + 2nO ln (pO) ,
where pO = (1 −pA −pB). The E-step consists of computing the expected
value of the complete-data log-likelihood, conditionally on the observed
counts n and on the value of the parameters at iteration t,

p[t]
A , p[t]
B

.
Explicitly, this is
Q

pA,pB|p[t]
A , p[t]
B

= E [{2nAA ln (pA) + nAO ln (2pApO) + nAB ln (2pApB)
+ 2nBB ln (pB) + nBO ln (2pBpO) + 2nO ln (pO)} |p[t]
A , p[t]
B , n

= 2,nAA ln (pA) + ,nAO ln (2pApO) + nAB ln (2pApB) + 2,nBB ln (pB)
+ ,nBO ln (2pBpO) + 2nO ln (pO) ,
(9.53)
where
,nAA
=
E

nAA|p[t]
A , p[t]
B , n

,
,nAO
=
E

nAO|p[t]
A , p[t]
B , n

,
,nBB
=
E

nBB|p[t]
A , p[t]
B , n

,
,nBO
=
E

nBO|p[t]
A , p[t]
B , n

.

9.8 Examples
465
The M-step consists of maximizing (9.53) with respect to pA and pB. This
yields the following closed-form solution for pA and pB at a round (t + 1):
p[t+1]
A
=
2,nAA + nAB + ,nAO
2 (nA + nAB + nB + nO),
(9.54)
p[t+1]
B
=
2,nBB + nAB + ,nBO
2 (nA + nAB + nB + nO).
(9.55)
The unobserved counts at iteration t are imputed via their expected values,
given n and

p[t]
A , p[t]
B

. The unobserved counts are distributed binomially
(see Example 2.15 in Chapter 2) as follows:
nAA ∼Bi

p2
A
p2
A + 2pA (1 −pA −pB), nA

,
nAO ∼Bi

2pA (1 −pA −pB)
p2
A + 2pA (1 −pA −pB), nA

,
nBB ∼Bi

p2
B
p2
B + 2pB (1 −pA −pB), nB

,
and
nBO ∼Bi

2pB (1 −pA −pB)
p2
B + 2pB (1 −pA −pB), nB

.
Hence, expectations can be computed immediately. For example,
,nAA = nA
p2[t]
A
p2[t]
A
+ 2p[t]
A

1 −p[t]
A −p[t]
B
,
and similarly for the other components of the missing data. Using some
starting values for the gene frequencies, the missing counts ,nij are imputed
and the next round of gene frequency values are computed from (9.54) and
(9.55). In the case of the present example, starting with p[0]
A = p[0]
B = 0.2
leads to 5pA = 0.209130654 and to 5pB = 0.080801008 (with 5p0 = 1−5pA−5pB)
after 9 EM iterations.
To obtain an estimate of the asymptotic variance we apply the method of
Oakes (1999). This requires obtaining the 2×2 matrix of second derivatives
evaluated at pA, p[t]
A = 5pA and at pB, p[t]
B = 5pB, which yields


∂2Q

pA,pB|p[t]
A ,p[t]
B

∂pA∂pA
∂2Q

pA,pB|p[t]
A ,p[t]
B

∂pA∂pB
∂2Q

pA,pB|p[t]
A ,p[t]
B

∂pB∂pA
∂2Q

pA,pB|p[t]
A ,p[t]
B

∂pB∂pB


"""""""pA,p[t]
A =pA
pB,p[t]
B =pB
= −

26, 349.8
5993.29
5993.29
58667.1

(9.56)

466
9. Approximate Inference Via the EM Algorithm
and


∂2Q

pA,pB|p[t]
A ,p[t]
B

∂pA∂p[t]
A
∂2Q

pA,pB|p[t]
A ,p[t]
B

∂pA∂p[t]
B
∂2Q

pA,pB|p[t]
A ,p[t]
B

∂pB∂p[t]
A
∂2Q

pA,pB|p[t]
A ,p[t]
B

∂pB∂p[t]
B


""""""""pA,p[t]
A =pA
pB,p[t]
B =pB
=

3134.28
962.148
962.148
2657.74

.
(9.57)
Using (9.36), the estimate of the observed information matrix is:
−

−26, 349.8
−5, 993.29
−5, 993.29
−58, 667.1

+

3, 134.28
962.148
962.148
2, 657.74
%
=

23, 215.5
5, 031.14
5, 031.14
56, 009.4

,
where the second term in the ﬁrst line corresponds to the negative of the
missing information. The estimate of the asymptotic variance of (5pA, 5pB)′
is:
V ar (5pA, 5pB|n) = 10−6

43.930
−3.946
−3.946
18.209

,
in agreement with the estimate obtained in Example 4.7 based on Newton–
Raphson.
■
Example 9.3
Maximum likelihood estimation in the mixed linear model
In this example the iterative EM equations for the ML estimation of ﬁxed
eﬀects and of variance components in a univariate Gaussian mixed linear
model with 2 variance components are derived. The general EM algorithm,
with its distinct E- and M-steps, and the form of EM discussed in Section
9.6, are illustrated. Both approaches, of course, yield identical results.
The model considered here was introduced in Example 1.18 in Chapter
1. The data y (vector of dimension n × 1) are assumed to be a realization
from
y|β, a, σ2
e ∼N

Xβ + Za, Iσ2
e

,
and the unobserved vector of the additive genetic values (q ×1) is assumed
to follow the multivariate normal distribution
a|Aσ2
a ∼N

0, Aσ2
a

.
The vector of ﬁxed eﬀects β has order p × 1; X and Z are known incidence
matrices, and the unknown variance components are the scalars σ2
a and
σ2
e. The matrix A is known; it describes the covariance structure of a
and depends on the additive genetic relationships among individuals in the

9.8 Examples
467
pedigree. Here the focus of inference is θ =

β′, σ2
a, σ2
e
′. The observed data
likelihood is
L (θ|y) =

p

y|β, a, σ2
e

p

a|Aσ2
a

da
∝|V|−1
2 exp

−1
2 (y −Xβ)′ V−1 (y −Xβ)

,
(9.58)
where V = ZAZ′σ2
a + Iσ2
e is the unconditional variance–covariance matrix
of the observed data y. Rather than working with (9.58), the ML estimate
of θ is obtained using the EM algorithm.
General EM Algorithm
Regarding the random eﬀects a as the missing data, the complete-data
likelihood is
L (θ, a|y) =
""Iσ2
e
""−1
2 exp

−1
2σ2e
(y −Xβ −Za)′ (y −Xβ −Za)

×
""Aσ2
a
""−1
2 exp

−1
2σ2a
a′A−1a

,
(9.59)
and the corresponding log-likelihood is
ln p (θ, a|y) = constant −n
2 ln σ2
e −q
2 ln σ2
a −
1
2σ2a
a′A−1a
−1
2σ2e
(y −Xβ −Za)′ (y −Xβ −Za) .
(9.60)
The E-step is
Q

θ|θ[t]
=

ln p (θ, a|y) p

a|θ[t], y

da
= −n
2 ln σ2
e −q
2 ln σ2
a −
1
2σ2a
Ea|θ[t],y

a′A−1a

−1
2σ2e
Ea|θ[t],y (y −Xβ −Za)′ (y −Xβ −Za) .
Let:
Ea|θ[t],y

a|θ[t], y

= ,a[t],
and
V ara|θ[t],y

a|θ[t], y

= ,V[t]
a .
Then using results for expectation of quadratic forms (Searle, 1971),
Q

θ|θ[t]
= −n
2 ln σ2
e −q
2 ln σ2
a −
1
2σ2a

,a[t]′A−1,a[t] + tr

A−1 ,V[t]
a

−1
2σ2e

y −Xβ −Z,a[t]′ 
y −Xβ −Z,a[t]
+ tr

Z′Z ,V[t]
a

.

468
9. Approximate Inference Via the EM Algorithm
The M-step consists of setting the following equations equal to zero:
∂Q

θ|θ[t]
∂β
= 1
σ2e
X′ 
y −Xβ −Z,a[t]
,
∂Q

θ|θ[t]
∂σ2a
= −q
2σ2a
+
1
2 (σ2a)2

,a[t]′A−1,a[t] + tr

A−1 ,V[t]
a

,
and
∂Q

θ|θ[t]
∂σ2e
= −n
2σ2e
+
1
2 (σ2e)2

y −Xβ −Z,a[t]′ 
y −Xβ −Z,a[t]
+ tr

Z′Z ,V[t]
a

.
Solving for θ one obtains the iterative system
β[t+1] = (X′X)−1 X′ 
y −Z,a[t]
,
(9.61)
σ2[t+1]
a
= 1
q

,a[t]′A−1,a[t] + tr

A−1 ,V[t]
a

,
(9.62)
and
σ2[t+1]
e
= 1
n

y −Xβ[t+1]−Z,a[t]′ 
y −Xβ[t+1]−Z,a[t]
+tr

Z′Z ,V[t]
a

.
(9.63)
More explicit expressions for ,a[t] and ,V[t]
a are given in a section at the end
of this example.
Exponential Family Version of EM
The complete-data likelihood (9.59) can be put in the form (9.20) up to
proportionality, by making use of the following:
x = [y′, a′]′ ,
b (x) = 1,
θ
=
(θ1, θ2, θ3)′
=
 1
σ2a
, 1
σ2e
, β
σ2e
′
,

9.8 Examples
469
a (θ)
=

σ2
a
 q
2 
σ2
e
 n
2 exp
 1
2σ2e
β′X′Xβ

=
θ
−q
2
1
θ
−n
2
2
exp
θ′
3X′Xθ3
2θ2

,
(9.64a)
t (x) =


−1
2a′A−1a
−1
2 (y −Za)′ (y −Za)
X′ (y −Za)

.
The implementation of the EM algorithm consists of solving for θ[t+1] the
system of equations deﬁned by (9.28). The right-hand side of (9.28) is the
unconditional expectation of the vector of suﬃcient statistics t (x), which
is equal to (9.23). From (9.64a),
ln a (θ) = −q
2 ln θ1 −n
2 ln θ2 +
1
2θ2
θ′
3X′Xθ3.
The required unconditional expectations are given by the following partial
derivatives
∂ln a (θ)
∂θ1
= −q
2θ1
,
∂ln a (θ)
∂θ2
= −n
2θ2
−
1
2θ2
2
θ′
3X′Xθ3,
and
∂ln a (θ)
∂θ3
= 1
θ2
X′Xθ3.
The conditional expectation of t (x) (given y and θ[t]) has the components
E
a|θ[t],y

−1
2a′A−1a|θ[t], y

= −1
2

,a[t]′A−1,a[t] + tr

A−1 ,V[t]
a

,
E
a|θ[t],y

−1
2 (y −Za)′ (y −Za) |θ[t], y

=
−1
2

y −Z,a[t]′ 
y −Z,a[t]
+ tr

Z′Z ,V[t]
a

,
E
a|θ[t],y

X′ (y −Za) |θ[t], y

= X′ 
y −Z,a[t]
.
Writing the equations ∂ln a (θ)/ ∂θ in terms of

β, σ2
a, σ2
e

, and equating
to the conditional expectations, yields
σ2[t+1]
a
= 1
q

,a[t]′A−1,a[t] + tr

A−1 ,V[t]
a

,
(9.65)

470
9. Approximate Inference Via the EM Algorithm
nσ2[t+1]
e
+ β[t+1]′X′Xβ[t+1]
=

y −Z,a[t]′ 
y −Z,a[t]
+ tr

Z′Z ,V[t]
a

,
(9.66)
and
X′Xβ[t+1] = X′ 
y −Z,a[t]
.
(9.67)
From (9.67),
β[t+1] = (X′X)−1 X′ 
y −Z,a[t]
.
(9.68)
Substituting (9.68) in (9.66), and using

y −Z,a[t]′ 
I −X (X′X)−1 X′ 
y −Z,a[t]
=

y −Xβ[t+1] −Z,a[t]′ 
y −Xβ[t+1] −Z,a[t]
,
one obtains
σ2[t+1]
e
= 1
n

y −Xβ[t+1] −Z,a[t]′ 
y −Xβ[t+1] −Z,a[t]
+ 1
ntr

Z′Z ,V[t]
a

.
The iterative system arrived at is identical to that deﬁned by (9.61), (9.62)
and (9.63).
Algebraic Notes
The starting point for the computation of the mean vector and variance–
covariance matrix of

a|θ[t], y

is the joint distribution

a, y|θ[t]
:
y
a
"""" θ[t] ∼N


Xβ
0

,

V
ZAσ2
a
AZ′σ2
a
Aσ2
a

,
where V = ZAZ′σ2
a +Iσ2
e. From properties of the multivariate normal dis-
tribution it follows that
a|θ[t], y ∼N

E

a|θ[t], y

,V ar

a|θ[t], y

,
where
E

a|θ[t], y

= AZ′V−1[t] 
y −Xβ[t]
σ2[t]
a .
Substituting in this expression
V−1[t] =
1
σ2[t]
e
I −
1
σ2[t]
e
Z

Z′Z + A−1k[t]−1
Z′,

9.8 Examples
471
gives
E

a|θ[t], y

= AZ′
 1
σ2[t]
e
I −
1
σ2[t]
e
Z

Z′Z + A−1k[t]−1
Z′
 
y −Xβ[t]
σ2[t]
a
=

A 1
k[t] −1
k[t] AZ′Z

Z′Z + A−1k[t]−1
Z′ 
y −Xβ[t]
=

Z′Z + A−1k[t]−1
Z′ 
y −Xβ[t]
,
(9.69)
where k[t] = σ2[t]
e
/σ2[t]
a . The last line follows because
A 1
k[t] −1
k[t] AZ′Z

Z′Z + A−1k[t]−1
=

Z′Z + A−1k[t]−1
.
This can be veriﬁed by post-multiplying the left-hand side by the inverse
of the right-hand side, which retrieves I.
Now we show that
V ar

a|θ[t], y

=

Z′Z + A−1k[t]−1
σ2[t]
e
.
(9.70)
Again, using properties of the multivariate normal distribution,
V ar

a|θ[t], y

= Aσ2[t]
a
−AZ′σ2[t]
a V−1[t]ZAσ2[t]
a
= Aσ2[t]
a
−

A 1
k[t] −1
k[t] AZ′Z

Z′Z + A−1k[t]−1
Z′ZAσ2[t]
a
= Aσ2[t]
a
−

Z′Z + A−1k[t]−1
Z′ZAσ2[t]
a
=

Z′Z + A−1k[t]−1
σ2[t]
e
.
The last line can be veriﬁed by premultiplying the third line by

Z′Z + A−1k[t]
,
which gives Iσ2[t]
e
.
An alternative derivation from a Bayesian perspective (which implies
assigning a ﬂat, improper prior to the location vector β) is to use as point
of departure
β
a
"""" σ2[t]
a , σ2[t]
e
, y ∼N

5β
[t]
5a[t]

,

C11
C12
C21
C22
[t]
σ2[t]
e

.
(9.71)
Using similar algebra as in Example 1.18 from Chapter 1, one can show
that the mean vector of

a|β,σ2[t]
a , σ2[t]
e
, y

is equal to

Z′Z + A−1k[t]−1
Z′ (y −Xβ) ,

472
9. Approximate Inference Via the EM Algorithm
and its covariance matrix is equal to (9.70).
■
Example 9.4
Restricted maximum likelihood estimation in the mixed lin-
ear model
The model is as in the preceding example, but now the focus of inference is
θ =

σ2
a, σ2
e

, with β viewed as a nuisance parameter. A Bayesian perspec-
tive will be adopted, and the mode of the posterior distribution with density
p

σ2
a, σ2
e|y

is chosen as point estimator. Assigning improper uniform prior
distributions to each of

σ2
a, σ2
e

and to β, then
p

σ2
a, σ2
e|y

∝

p

y|β, a, σ2
e

p

a|A, σ2
a

da dβ.
In this setting the mode of the posterior distribution of the variance compo-
nents is identical to the REML estimator, (Harville, 1974). Joint maximiza-
tion of this expression is diﬃcult. However, it is relatively easy to struc-
ture an EM algorithm, where the missing data are now z =

β′, a′′. The
complete-data posterior distribution p

σ2
a, σ2
e, z|y

is identical to (9.60)
and the E-step is now
Q

θ|θ[t]
=

ln p (θ, β, a|y) p

β, a|θ[t], y

da dβ
= −n
2 ln σ2
e −q
2 ln σ2
a −
1
2σ2a
Eβ,a|θ[t],y

a′A−1a

−1
2σ2e
Eβ,a|θ[t],y (y −Xβ −Za)′ (y −Xβ −Za) .
(9.72)
This, on using (9.71), takes the form
Q

θ|θ[t]
= −n
2 ln σ2
e −q
2 ln σ2
a
−1
2σ2a

5a[t]′A−15a + tr

A−1C22[t]
σ2[t]
e

−1
2σ2e

5e[t]′5e[t] + tr

[X, Z] C−1[t] [X, Z]′
σ2[t]
e

,
where 5e[t] =

y −X5β
[t]−Z5a[t]

and
C−1 =
 C11
C12
C21
C22

.
Since the missing data are now z =

β′, a′′, expectations in (9.72) are
taken with respect to

β, a|θ[t], y

. The M-step is
∂Q

θ|θ[t]
∂σ2a
= −q
2σ2a
+
1
2 (σ2a)2

5a[t]′A−15a + tr

A−1C22[t]
σ2[t]
e

,

9.8 Examples
473
and
∂Q

θ|θ[t]
∂σ2e
= −n
2σ2e
+
1
2 (σ2e)2

5e[t]′5e[t] + tr

[X, Z] C−1[t] [X, Z]′
σ2[t]
e

.
Setting to zero yields the iterative system
σ2[t+1]
a
= 5a[t]′A−15a + tr

A−1C22[t]
σ2[t]
e
q
,
(9.73)
σ2[t+1]
e
= 5e[t]′5e[t] + tr

[X, Z] C−1[t] [X, Z]′
σ2[t]
e
n
.
(9.74)
Contrary to ML estimation, REML estimation or inference via the posterior
mode of

σ2
a, σ2
e|y

requires inverting the entire coeﬃcient matrix C.
■
The preceding examples illustrate the versatility of the EM algorithm.
Viewed more generally, the algorithm can be interpreted as a data aug-
mentation technique (Tanner and Wong, 1987), with the imputations made
using conditional expectations. Additional examples of varying degrees of
complexity can be found in Little and Rubin (1987) and in McLachlan and
Krishnan (1997).

This page intentionally left blank

Part III
Markov Chain Monte
Carlo Methods
475

This page intentionally left blank

10
An Overview of Discrete
Markov Chains
10.1
Introduction
The theory of Markov chains governs the behavior of the Markov chain
Monte Carlo (MCMC) methods that are discussed in Chapter 11 and on-
wards. The purpose of this chapter is to present an overview of some ele-
ments of this theory so that the reader can obtain a feeling for the mecha-
nisms underlying MCMC. Feller (1970), Cox and Miller (1965), Karlin and
Taylor (1975), Meyn and Tweedie (1993), Norris (1997), or Robert and
Casella (1999) should be consulted for a more detailed and formal treat-
ment of the subject. A recent contribution to the literature, that discusses
the ﬁner theoretical and practical issues in a clear style with a view towards
MCMC applications, is Liu (2001). For the sake of simplicity, this chapter
considers only chains with ﬁnite state spaces.
The aspects of Markov chain theory discussed here are the following
ones. First, the fundamental ingredients of a Markov chain are introduced.
These consist of the initial probability distribution of the states of the
chain and the matrix of transition probabilities. The two together govern
the evolution of the Markov chain. One wishes to know if, after a number of
transitions, the Markov chain converges to some equilibrium distribution,
independently of the initial probability distribution, and if this equilibrium
distribution is unique. Two important properties for discrete, ﬁnite state
Markov chains, establish the existence of a unique equilibrium distribution:
aperiodicity and irreducibility. Another important property especially from
the point of view of MCMC is that of reversibility. As shown in Chapter

478
10. An Overview of Discrete Markov Chains
11, transition probability matrices or transition kernels (in the case of con-
tinuous Markov chains) can be constructed more or less easily using the
condition of reversibility as point of departure. The transition kernels con-
structed in this way, guarantee that aperiodic and irreducible chains have a
unique equilibrium distribution. Convergence to this distribution, however,
takes place only asymptotically. Further, the rate of convergence is an im-
portant aspect of the behavior of MCMC, a subject which is discussed and
illustrated in a closing section of this chapter. Other examples illustrating
convergence can be found in Chapter 12.
10.2
Deﬁnitions
Following Grossman and Turner (1974), suppose that a mouse is placed in a
box divided into three intercommunicating compartments labelled 1, 2, and
3. Technically the three compartments are the possible states of the system
at any time, and the set S = {1, 2, 3} is called the state space. One can de-
ﬁne the random variable “presence of the mouse in a given compartment”.
This random variable can take one of three possible values. A movement
from compartment i to j (which includes the case where i = j, indicating
no movement) is referred to as a transition of the system from the ith to
the jth state. This transition will occur with some conditional probabil-
ity p (i, j) , called the transition probability of moving from compartment
i to j. This probability is conditional on the mouse being in compartment
i. Consider a sequence of random variables deﬁning the presence of the
mouse in the compartments over a period of time. If the probability that
the mouse moves to a given compartment depends on which compartment
it ﬁnds itself immediately before the move, and not on which compartments
the mouse had visited in the past, then the sequence of random variables
deﬁnes a Markov chain. A Markov chain deals with the study of the possible
transitions between the states of a system via probabilistic methods. An
important goal is to characterize the probability distribution after n tran-
sitions. This is the distribution of the proportion of times that the mouse
is expected to be in each compartment after n transitions. It is of interest
to know whether for large n, this distribution stabilizes, irrespective of the
initial distribution. A formal deﬁnition of a Markov chain follows.
A ﬁnite state, discrete Markov chain is a sequence of random vari-
ables Xn, (n = 0, 1, 2, . . . , ) where the Xn take values in the ﬁnite set
S = {0, 1, . . . , N −1} and are called the states of the Markov chain. The
subscripts n can be interpreted as stages or time periods. If Xn = i, the
process is said to be in state i at time n. A Markov chain must satisfy the

10.3 State of the System after n-Steps
479
following Markov property
Pr (Xn = j|Xn−1 = i, Xn−2 = k, . . . , X0 = m)
= Pr (Xn = j|Xn−1 = i) = p (i, j) ,
i, j, k, . . . , m ∈S.
(10.1)
This property may be interpreted as stating that, for a Markov chain,
the conditional distribution at time n, given all the past states X0 =
m, . . . , Xn−1 = i, only depends on the immediately preceding state, Xn−1 =
i. A sequence of independent random variables is a trivial example of a
Markov chain.
The evolution of the chain is described by its transition probability (10.1).
The element p (i, j) represents the probability that the chain at time n is
in state j, given that at time n −1 it was in state i. This is a conditional
probability, where j is stochastic and i is ﬁxed. In the notation of distribu-
tion theory, this would normally be written p (j|i). However, the standard
Markov chain notation is adhered to here and in the next chapter (for
example, as in Cox and Miller, 1965). The transition probabilities can be
arranged in a matrix P = {p (i, j)}, where i is a row suﬃx and j a column
suﬃx. This is the N ×N transition probability matrix of the Markov chain.
Only transition probability matrices that are independent of time (n) are
considered here. These are known as time homogeneous chains: the prob-
ability of a transition from a given state to another depends on the two
states and not on time.
The (i + 1)th row of P is the probability distribution of the values of Xn
under the condition that Xn−1 = i. Every entry of P satisﬁes p (i, j) ≥0,
and every row of P satisﬁes 
j p (i, j) = 1.
10.3
State of the System after n-Steps
As stated above, the matrix P describes the probability of transitions of the
chain in one time period. Consider a chain with state space S = {0, 1, 2}.
The matrix of transition probabilities, of order 3 × 3 takes the form
P =


p (0, 0)
p (0, 1)
p (0, 2)
p (1, 0)
p (1, 1)
p (1, 2)
p (2, 0)
p (2, 1)
p (2, 2)

.

480
10. An Overview of Discrete Markov Chains
The probability that at stage (or time) 2 + m the random variable will be
in state 2, given that it was in stage 1 at time m can be written as
p(2) (1, 2) = Pr (X2+m = 2|Xm = 1)
=
2

j=0
Pr (X2+m = 2, X1+m = j|Xm = 1)
=
2

j=0
Pr (X1+m = j|Xm = 1) Pr (X2+m = 2|X1+m = j)
= p (1, 0) p (0, 2) + p (1, 1) p (1, 2) + p (1, 2) p (2, 2) .
(10.2)
The equality in the third line follows from the Markov property. Equation
(10.2) makes it explicit that to compute the probability of moving from
state 1 to 2 in two transitions, requires summation over the probabilities
of going through all possible intermediate states before the system reaches
state 2. The last line in (10.2) can be recognized as the product of row 2 of
P (associated with state 1) and column 3 of P (associated with state 2).
The preceding argument can be generalized to arrive at two important
results. The ﬁrst one concerns the probability of moving from i to j in n
transitions. This is given by
p(n) (i, j) = Pr (Xn = j|X0 = i) = Pr (Xn+k = j|Xk = i) .
(10.3)
The element p(n) (i, j) is the (i + 1, j + 1) entry of Pn. The second result
is described by the Chapman–Kolmogorov equations
p(m+n) (i, j)
=
Pr (Xm+n = j|X0 = i)
=
N−1

k=0
Pr (Xm+n = j, Xm = k|X0 = i)
=
N−1

k=0
Pr (Xm+n = j|Xm = k) Pr (Xm = k|X0 = i)
=
N−1

k=0
p(m) (i, k) p(n) (k, j) .
(10.4)
The matrix analogue to (10.4) is
Pm+n = PmPn.
This result relates long-term behavior to short-term behavior and describes
how Xn depends on the starting value X0.

10.4 Long-Term Behavior of the Markov Chain
481
10.4
Long-Term Behavior of the Markov Chain
Let π′(n) be the N-dimensional row vector denoting the probability distri-
bution of Xn. The ith component of π(n) is:
π(n) (i) = Pr (Xn = i) ,
i ∈S.
Clearly, when n = 0, π(0) deﬁnes the initial probability distribution of the
chain. Now,
Pr (Xn = j) =
N−1

i=0
Pr (Xn = j|Xn−1 = i) Pr (Xn−1 = i)
=
N−1

i=0
p (i, j) Pr (Xn−1 = i) , j = 0, 1, . . . , N −1.
(10.5)
The left-hand side is the (j + 1)th element of π(n), and the right-hand
side is the product of the row vector π′(n−1) with column (j + 1) of the
transition matrix P. Therefore a matrix generalization of (10.5) is given by
π′(n) = π′(n−1)P.
(10.6)
Since π′(1) = π′(0)P, π′(2) = π′(1)P = π′(0)PP = π′(0)P2, and so on, it
follows that the probability distribution of the chain at time n is given by
π′(n) = π′(0)Pn.
(10.7)
Thus, we reach the important conclusion that the random evolution of
the Markov chain is completely speciﬁed in terms of the distribution of the
initial state and the transition probability matrix P. This means that given
the initial probability distribution and the transition probability matrix, it
is possible to describe the behavior of the process at any speciﬁed time
period n.
10.5
Stationary Distribution
A question of fundamental importance is whether the chain converges to
a limiting distribution, independent of any legal starting distribution. As
shown below, this requires that Pn converges to some invariant matrix,
and that, at the limit, it has identical rows. Suppose that π is a limiting
probability vector such that, from (10.7),
π′ = lim
n→∞π′(0)Pn.

482
10. An Overview of Discrete Markov Chains
Then,
π′
=
lim
n→∞π′(0)Pn+1
=

lim
n→∞π′(0)Pn
P
=
π′P.
(10.8)
The distribution π is said to be a stationary distribution (also known as the
invariant or equilibrium distribution) if it satisﬁes (10.8). An interpretation
of (10.8) is that if a chain has reached a stage where π is the stationary
distribution, it retains it in subsequent moves. Expression (10.8) can also
be written as the system of equations
π (j) =
N−1

i=0
π (i) p (i, j) .
(10.9)
For the case of ﬁnite state Markov chains under consideration here, station-
ary distributions always exist (Grimmet and Stirzaker, 1992). The issue in
general is convergence and uniqueness. This problem is taken up at the end
of this chapter.
Example 10.1
A three-state space Markov chain
Consider a Markov chain consisting of three states, 0, 1, 2, with the following
transition probability matrix:
P
=


p (0, 0)
p (0, 1)
p (0, 2)
p (1, 0)
p (1, 1)
p (1, 2)
p (2, 0)
p (2, 1)
p (2, 2)


=


1
2
1
2
0
1
4
1
2
1
4
0
1
3
2
3

.
Row 2 say, with elements p (1, 0) = 1
4, p (1, 1) = 1
2, and p (1, 2) = 1
4 rep-
resents the probability distribution of the values of Xn+1 given Xn = 1.
Element p (1, 2) say, is the probability that the system moves from state 1
to state 2 in one transition. It can be veriﬁed that
P4 =


0.2760
0.4653
0.2587
0.2326
0.4485
0.3189
0.1725
0.4251
0.4024


and p4 (1, 2) = 0.3189 is the probability of moving from state 1 to state 2
in four transitions. If the starting probability distribution of the chain is
π′(0) =
 0
1
0 
,

10.6 Aperiodicity and Irreducibility
483
then we have, to four decimal places,
π′(1)
=

0.2500
0.5000
0.2500

,
π′(2)
=

0.2500
0.4583
0.2917

,
π′(4)
=

0.2396
0.4514
0.3090

,
π′(5)
=

0.2326
0.4485
0.3189

.
Eventually the system converges to the stationary distribution
π′ =

0.2222
0.4444
0.3333

,
(10.10)
and, for large n,
Pn =


π′
π′
π′

.
The same stationary distribution (10.10) is arrived at, regardless of the
starting value of π(0). In fact, for this example, the distribution (10.10)
is unique. This distribution can be derived from (10.8) or (10.9). Let the
stationary distribution be
π′ =

π (0)
π (1)
π (2)

,
with π (2) = 1 −π (1) −π (0). From (10.8), the system of equations to be
solved for π (0) and π (1) is
π (0)
2
+ π (1)
4
= π (0) ,
π (0)
2
+ π (1)
2
+ 1 −π (1) −π (0)
3
= π (1) .
The unique solution is π (0) = 2/9, π (1) = 4/9, and π (2) = 1 −π (1) −
π (0) = 3/9.
■
10.6
Aperiodicity and Irreducibility
In the example above the Markov chain converges to a stationary distri-
bution, and it was stated that this distribution is unique. For ﬁnite state
Markov chains, convergence and uniqueness require the chain to be aperi-
odic and irreducible.
Consider a three state Markov chain and that the only possible transi-
tions are 1 →2, 2 →3, and 3 →1. That is, the states repeat themselves
every 3 movements of the chain. If the process starts at state 2, say, this
state will be revisited in a periodic fashion at times 3, 6, 9, .... The greatest

484
10. An Overview of Discrete Markov Chains
common divisor of these integers is 3; it is then said that this state has
period equal to 3.
Formally, the period of state j is deﬁned as the greatest common divisor
of all integers n ≥1 for which p(n) (j, j) > 0. If the period of state j of
the chain is d say, it means that p(n) (j, j) = 0 whenever n is not divisible
by d, and d is the largest integer with this property. An aperiodic state
has period 1, or alternatively, a state j is aperiodic if p(n) (j, j) > 0 for all
suﬃciently large n. A suﬃcient condition for a state to have period 1 is
Pr (Xn = j|X0 = j) and Pr (Xn+1 = j|X0 = j) > 0
(10.11)
for some n ≥0 and some state j = 0, 1, . . . , N −1. Clearly, aperiodicity
holds when p (j, j) = Pr (Xn = j|Xn−1 = j) > 0 for all j.
An important property of aperiodicity is that if states j and i commu-
nicate and state j is aperiodic, then state i is also aperiodic. A chain is
aperiodic if all states have period 1. Now, if all states of a Markov chain
communicate, such that every state is reachable from every other state in
a ﬁnite number of transitions, the Markov chain is called irreducible. All
states of an irreducible chain have the same period.
More formally, irreducibility means that for every pair of states (i, j),
p(k) (i, j) = Pr (Xn+k = j|Xn = i) > 0 for some k ≥0 (the value of k may
be diﬀerent for diﬀerent i, j; more formally we should write k (i, j)).
A chain that is irreducible with period d has a transition probability ma-
trix with d eigenvalues with absolute value 1. This property has important
implications for convergence of the chain, as is illustrated in the example
below.
Finite state Markov chains that are aperiodic and irreducible have the
property that, for some n ≥0, Pn has all entries positive. Such a Markov
chain is called ergodic. In the ﬁnal section of this chapter, it is shown that
an ergodic Markov chain converges to a unique stationary distribution π,
for all legal initial probability distributions. This stationary distribution
is the unique solution to (10.8). Four cases of Markov chains that are not
ergodic are presented in the following example. The transition probability
matrices of these Markov chains are such that, for large n, Pn has non-
positive entries.
Example 10.2
Four Markov chains that are not ergodic
Consider a Markov chain with the following transition probability matrix
P =


0
1
0
0
0
0.5
0
0.5
0
0
0
0.5
0
0.5
0
0
0
0.5
0
0.5
0
0
0
1
0


.

10.6 Aperiodicity and Irreducibility
485
Taking powers of this matrix discloses quickly that Pn looks diﬀerent de-
pending on whether n is even or odd. For large n, if n is even,
Pn ≃


0.25
0
0.50
0
0.25
0
0.50
0
0.50
0
0.25
0
0.50
0
0.25
0
0.50
0
0.50
0
0.25
0
0.50
0
0.25


,
and if n is odd,
Pn ≃


0
0.50
0
0.50
0
0.25
0
0.50
0
0.25
0
0.50
0
0.50
0
0.25
0
0.50
0
0.25
0
0.50
0
0.50
0


.
The Markov chain deﬁned by this transition probability is irreducible (for
some n, p(n) (i, j) > 0 for all i, j) and periodic with period d = 2 (start-
ing in state 1 say, pn (1, 1) > 0 at times n = 2, 4, 6, 8, . . .; the greatest
common divisor of the values that n can take is 2). The unique stationary
distribution for this chain (the only solution to (10.8)) can be shown to be
π
=
lim
n→∞
1
2

π(n) + π(n+1)
=

0.125
0.25
0.25
0.25
0.125
′ .
Even though a unique stationary distribution exists, the chain does not
converge to it. The equilibrium probability at state i, π (i), rather than
representing the limit of p(n) (j, i), represents the average amount of time
that is spent in state i.
One can verify that the eigenvalues of P are −1, 0, 1, −1/
√
2, 1/
√
2; that
is, there are d = 2 eigenvalues with absolute value 1. We return to this
point at the end of the chapter.
As a second case, consider the chain with the transition probability ma-
trix
P =


1
0
0
0
0
0.5
0
0.5
0
0
0
0.5
0
0.5
0
0
0
0.5
0
0.5
0
0
0
0
1


.
(10.12)

486
10. An Overview of Discrete Markov Chains
Rather than a unique stationary distribution, the system has ﬁve stationary
distributions. Thus, for large n,
Pn ≃


1
0
0
0
0
0.75
0
0
0
0.25
0.50
0
0
0
0.50
0.25
0
0
0
0.75
0
0
0
0
1


.
The rows of Pn represent the ﬁve stationary distributions, and each of
these satisfy (10.8).
As a third case, consider the following reducible chain deﬁned by the
transition probability
P =


1
2
1
2
0
0
0
1
6
5
6
0
0
0
0
0
3
4
1
4
0
0
0
3
24
16
24
5
24
0
0
0
1
6
5
6


.
For large n
Pn ≃


0.25
0.75
0
0
0
0.25
0.75
0
0
0
0
0
0.182
0.364
0.455
0
0
0.182
0.364
0.455
0
0
0.182
0.364
0.455


.
The chain splits into two noncommunicating subchains; each of these con-
verges to an equilibrium distribution, but one cannot move from state
spaces {0, 1} to state spaces {2, 3, 4}.
The fourth case illustrates an irreducible (all the states of the chain
communicate), periodic chain, with period d = 3,
P =


0
1
0
0
0
1
1
0
0

.
The only possible transitions are from state 1, to state 2, to state 3, to
state 1, and so on. That is, the chain returns to a given state at times
n = 3, 6, 9, . . .; the greatest common divisor of these numbers is 3. The
stationary distribution obtained solving (10.8) is
π =
1
3, 1
3, 1
3
′
.
(10.13)

10.7 Reversible Markov Chains
487
However, the chain does not converge to (10.13). One can verify that P
has d = 3 eigenvalues with absolute value 1. The consequences of this will
become apparent in the last section of this chapter.
■
10.7
Reversible Markov Chains
Consider an ergodic Markov chain with state space S that converges to
an invariant distribution π. Let x ∈S denote the current state of the
system, and let y ∈S denote the state at the next step. Let p (x, y) be the
probability of a transition from x to y and let p (y, x) denote the probability
of a transition in the opposite direction. A Markov chain is said to be
reversible if it satisﬁes the condition:
π (x) p (x, y) = π (y) p (y, x) ,
for all x, y ∈S,
(10.14)
which is known as the detailed balance equation. An ergodic chain in equi-
librium, satisfying (10.14) has π as its unique stationary distribution. This
can be conﬁrmed by summing both sides of (10.14) over y to yield the
equilibrium condition (10.9):
π (x) =

y∈S
π (y) p (y, x) .
The left-hand side of (10.14) can be expressed as
π (x) p (x, y) = Pr (Xn = x) Pr (Xn+1 = y|Xn = x)
= Pr (Xn = x, Xn+1 = y) ,
for all x, y ∈S.
(10.15)
This makes explicit that detailed balance is a statement involving a joint
probability. Therefore, the reversibility condition can also be written as
Pr (Xn = x, Xn+1 = y) = Pr (Xn = y, Xn+1 = x) ,
for all x, y ∈S.
(10.16)
The reversibility condition plays an important role in the construction
of MCMC algorithms, because it is often easy to generate a Markov chain
having the desired stationary distribution using (10.14) as a point of de-
parture. This is discussed in Chapter 11, where the role of reversibility in
deriving appropriate transition kernels is highlighted. Proving that π is the
unique stationary distribution from nonreversible chains can be diﬃcult.
Below is an example where this is not the case.
Example 10.3
An irreducible nonreversible Markov chain
Consider the three-state chain on S = {0, 1, 2} with transition probability
matrix
P =


0.7
0.2
0.1
0.1
0.7
0.2
0.2
0.1
0.7

.

488
10. An Overview of Discrete Markov Chains
It is easy to verify that for large n,
Pn =


1
3
1
3
1
3
1
3
1
3
1
3
1
3
1
3
1
3

.
Therefore, the unique invariant distribution is
π =

1
3
1
3
1
3
′ .
However, the chain is not reversible. For example,
π (0) p (0, 1) = 1
3 (0.2)
̸= π (1) p (1, 0) = 1
3 (0.1) .
■
Example 10.4
A multinomial distribution
Consider the 2 × 2 table studied by Casella and George (1992):
Pr (X = 0, Y = 0) = fx,y (0, 0) = p1,
Pr (X = 1, Y = 0) = fx,y (1, 0) = p2,
Pr (X = 0, Y = 1) = fx,y (0, 1) = p3,
Pr (X = 1, Y = 1) = fx,y (1, 1) = p4,
with all pi ≥0 and 4
i=1 pi = 1. The marginal distributions of X and Y are
Bernoulli, with success probabilities (p2 + p4) and (p3 + p4), respectively,
fx (0) = p1 + p3,
fx (1) = p2 + p4,
and
fy (0) = p1 + p2,
fy (1) = p3 + p4.
The conditional distributions are easily derived. These are
Px|y
=

fx|y (0|0)
fx|y (1|0)
fx|y (0|1)
fx|y (1|1)

=


p1
p1+p2
p2
p1+p2
p3
p3+p4
p4
p3+p4



10.7 Reversible Markov Chains
489
and
Py|x
=

fy|x (0|0)
fy|x (1|0)
fy|x (0|1)
fy|x (1|1)

=


p1
p1+p3
p3
p1+p3
p2
p2+p4
p4
p2+p4

.
A number of Markov chains can be generated from this model. For example,
the product Px|yPy|x generates the transition probability matrix Px|x:
Px|x = Px|yPy|x
with elements
Pr (Xn+1 = i|Xn = j) =
1

k=0
Pr (Xn+1 = i|Yn+1 = k)
× Pr (Yn+1 = k|Xn = j) .
(10.17)
For some start value of the marginal distribution of X, this transition
probability matrix generates the sequence Xn, (n = 1, 2, . . . , ) which is a
Markov chain. From (10.17) one can readily obtain Px|x whose elements
are:
Pr (X1 = 0|X0 = 0) =
p1
p1 + p2
p1
p1 + p3
+
p3
p3 + p4
p3
p1 + p3
,
Pr (X1 = 1|X0 = 0) =
p2
p1 + p2
p1
p1 + p3
+
p4
p3 + p4
p3
p1 + p3
,
Pr (X1 = 0|X0 = 1) =
p1
p1 + p2
p2
p2 + p4
+
p3
p3 + p4
p4
p2 + p4
,
Pr (X1 = 1|X0 = 1) =
p2
p1 + p2
p2
p2 + p4
+
p4
p3 + p4
p4
p2 + p4
.
It can be conﬁrmed that
Pr (X1 = 0|X0 = 0) + Pr (X1 = 1|X0 = 0) = 1
and that
Pr (X1 = 0|X0 = 1) + Pr (X1 = 1|X0 = 1) = 1.
Since Px|x is a probability matrix and the elements of Pn
x|x are all positive,
the Markov chain has a unique stationary distribution that satisﬁes π′ =
π′Px|x. Simple calculations yield
(p1 + p3) Pr (X1 = 0|X0 = 0) + (p2 + p4) Pr (X1 = 0|X0 = 1)
= (p1 + p3)

490
10. An Overview of Discrete Markov Chains
and
(p1 + p3) Pr (X1 = 1|X0 = 0) + (p2 + p4) Pr (X1 = 1|X0 = 1)
= (p2 + p4) ,
conﬁrming that π =

p1 + p3
p2 + p4
′ is the stationary distribution of
the Markov chain.
The above Markov chain satisﬁes the reversibility condition (10.16).
Thus,
Pr (Xn = 1, Xn−1 = 0) = Pr (Xn = 1|Xn−1 = 0) Pr (Xn−1 = 0)
= Pr (Xn = 0, Xn−1 = 1)
=
p1p2
p1 + p2
+
p3p4
p3 + p4
.
From the same multinomial model, another Markov chain can be gener-
ated by the 4×4 transition probability matrix Pxnyn|xn−1yn−1 with elements
deﬁned as follows:
Pr (Xn = k, Yn = l|Xn−1 = i, Yn−1 = j)
= Pr (Xn = k|Xn−1 = i, Yn−1 = j)
× Pr (Yn = l|Xn = k, Xn−1 = i, Yn−1 = j) ,
(i, j) (k, l) ∈S.
(10.18)
Instead of using (10.18), consider generating a Markov chain using the
following transition probability:
Pr (Xn = k|Yn−1 = j) Pr (Yn = l|Xn = k) ,
(i, j) (k, l) ∈S.
(10.19)
This creates the Markov chain (Xn, Yn) , (n = 1, 2, . . .) whose state space
is S = {0, 1}2 and which consists of correlated Bernoulli random variables.
Notice that the chain is formed by a sequence of conditional distributions:
ﬁrst Xn is updated from its conditional distribution, given Yn−1, and this
is followed by updating Yn from its conditional distribution, given the value
of Xn from the previous step.
It is easy to verify that this Markov chain has a unique stationary dis-
tribution given by π′ = (p1, p2, p3, p4), that is the only solution to (10.8).
Thus, the chain formed by the sequence of conditional distributions has
the joint distribution π as its unique stationary distribution. This is an
important observation that will be elaborated further in the next chapter.

10.7 Reversible Markov Chains
491
The Markov chain deﬁned by (10.19) does not satisfy the reversibility
condition. To verify this, consider, for example,
Pr (Xn = 1, Yn = 1, Xn−1 = 0, Yn−1 = 0)
= Pr (Xn = 1, Yn = 1|Xn−1 = 0, Yn−1 = 0)
× Pr (Xn−1 = 0, Yn−1 = 0)
= Pr (Xn = 1|Yn−1 = 0) Pr (Yn = 1|Xn = 1)
× Pr (Xn−1 = 0, Yn−1 = 0)
= p1

p2
p1 + p2
p4
p2 + p4

.
(10.20)
On the other hand,
Pr (Xn = 0, Yn = 0, Xn−1 = 1, Yn−1 = 1)
= Pr (Xn = 0, Yn = 0|Xn−1 = 1, Yn−1 = 1)
× Pr (Xn−1 = 1, Yn−1 = 1)
= Pr (Xn = 0|Yn−1 = 1) Pr (Yn = 0|Xn = 0)
× Pr (Xn−1 = 1, Yn−1 = 1)
= p4

p1
p1 + p3
p3
p3 + p4

.
(10.21)
Since in general, (10.20) ̸= (10.21), the ergodic Markov chain deﬁned by
(10.19) is not reversible.
The mechanism deﬁned by the transition probability (10.19) updates the
variables (X, Y ) one at a time in a systematic manner. While (10.19) does
not generate a reversible Markov chain, notice that each individual update
represents a transition probability that deﬁnes a reversible Markov chain.
For example
Pr (Xn = 0, Yn−1 = 1)
=
Pr (Xn = 0|Yn−1 = 1) Pr (Yn−1 = 1)
=
p2
p1 + p2
(p1 + p2) = p2,
which is equal to the time reversed transition
Pr (Xn−1 = 1, Yn = 0)
=
Pr (Yn = 0|Xn−1 = 1) Pr (Xn−1 = 1)
=
p2
p2 + p4
(p2 + p4) = p2,
and this holds for all possible values of (X, Y ). As discussed in the next
chapter, the Markov chain deﬁned by (10.19) is an example of the system-
atic scan, single-site Gibbs sampler.
■

492
10. An Overview of Discrete Markov Chains
10.8
Limiting Behavior of Discrete, Finite
State-Spaces, Ergodic Markov Chains
In Section 10.6 it was stated that ergodic Markov chains converge to a
unique equilibrium distribution π that satisﬁes
π′ = π′P.
(10.22)
This implies that π′ is a left eigenvector with eigenvalue 1. (From linear
algebra, recall that if π′ is a left eigenvector of P, if it satisﬁes π′P =λπ′,
where λ is the associated eigenvalue. Setting λ = 1 retrieves (10.22), so
that an equilibrium distribution must be a left eigenvector with eigenvalue
1. On the other hand, a right eigenvector satisﬁes Pπ =λπ). Since the
evolution of the Markov chain is governed by its transition probability,
insight into the limiting behavior of the Markov chain amounts to studying
the properties of Pn as n →∞, which is the topic of this ﬁnal section.
Below, use is made of the Perron-Frobenius theorem from linear algebra
(Karlin and Taylor, 1975), which implies that ergodic matrices have one
simple eigenvalue equal to 1, and all other eigenvalues have absolute value
less than 1. It follows that π in (10.22) is the unique stationary distribu-
tion of the Markov chain. However, the proof of convergence and rate of
convergence of the ergodic Markov chain to π requires a little more work.
First suppose that matrix P, of dimension m×m, has distinct eigenvalues
λi, i = 1, 2, . . . , m. Let C = (c1, c2, . . . , cm) be the matrix whose columns
ci represent the corresponding m linearly independent right eigenvectors.
Then it is a standard result of matrix theory (Karlin and Taylor, 1975)
that
P = CDC−1,
(10.23)
where the rows of C−1 are the left eigenvectors of P and D is a diagonal
matrix with diagonal elements consisting of the eigenvalues λi. That is,
D = diag {λ1, λ2, . . . , λm} .
The square of P is given by
P2
=
CDC−1CDC−1
=
CD2C−1,
and, in general,
Pn = CDnC−1.
(10.24)
Consider (10.23). Since D is diagonal it can be written in the form:
D = λ1I1 + λ2I2 + · · · + λmIm,

10.8 Limiting Behavior
493
where Ii is a square matrix with all elements equal to zero except for the
term on the diagonal corresponding to the intersection of the ith row and
ith column, which is equal to 1. Substituting in (10.23) yields
P
=
λ1CI1C−1 + λ2CI2C−1 + · · · + λmCImC−1
=
λ1Q1 + λ2Q2 + · · · + λmQm,
(10.25)
where Qi = CIiC−1 and has the following properties:
(a)
QiQi = CIiC−1CIiC−1 = Qi,
i = 1, 2, . . . , m;
(b)
QiQj = CIiC−1CIjC−1 = CIiIjC−1 = 0,
i ̸= j.
Using these properties, it follows that (10.24) can be written as
Pn = λn
1Q1 + λn
2Q2 + · · · + λn
mQm.
(10.26)
Without loss of generality, assume that the ﬁrst term on the right-hand side
corresponds to the largest eigenvalue in absolute terms. At this point we
assume that P is aperiodic and irreducible and therefore ergodic. Here we
invoke the Perron-Frobenius theorem which allows to set in (10.26), λ1 = 1
and |λi| < 1, i > 1. Therefore, for large n, Pn converges to Q1 which is
unique; that is,
lim
n→∞Pn = Q1,
since all terms in (10.26), with the exception of the ﬁrst one, tend to 0 as
n tends to inﬁnity. The rate of convergence to Q1 depends on the value of
λi with the largest modulus, other than λ1 = 1.
We now show that Q1 = 1π′, where 1 is a column vector of 1′s of
dimension m. A little manipulation yields
Q1
=
CI1C−1
=


c11
0
. . .
0
c21
0
. . .
0
...
0
...
0
cm1
0
. . .
0




c11
c12
. . .
c1m
c21
c22
. . .
c2m
...
...
...
...
cm1
cm2
. . .
cmm

, (10.27)
where ci1 (i = 1, 2, . . . , m) is the ith element of c1 (the right eigenvector
associated with the eigenvalue equal to 1) and cji is the ith element of the
jth row of matrix C−1. The ﬁrst row of C−1 is the left eigenvector with
eigenvalue equal to 1. In connection with (10.22), it was argued that this
ﬁrst row of C−1 is equal to the stationary distribution π. Now, expanding
(10.27) yields
Q1 =


c11( c11
c12
. . .
c1m )
c21( c11
c12
. . .
c1m )
...
cm1( c11
c12
. . .
c1m )

.

494
10. An Overview of Discrete Markov Chains
Since the elements of the rows of Q1 deﬁne the stationary distribution,
they must add to 1. It follows that
c11 = c21 = · · · = cm1 = w1, say.
Hence, Q1 has identical rows ,π′ say, where ,π is some equilibrium distribu-
tion equal to
,π′ = w1

c11
c12
. . .
c1m 
= w1π′.
We know that the equilibrium distribution is the left eigenvector with eigen-
value 1, with m
j=1 c1j = 1; therefore w1 = 1 and ,π = π is the unique
equilibrium distribution.
In the development above it was assumed that P could be written in the
form (10.23), where D is diagonal. When this is not possible, one can ﬁnd
a matrix B via a Jordan decomposition that has the form
B =


1
0
. . .
0
0
...
M
0


such that Pn= CBnC−1 where Mn →0 (Cox and Miller, 1965). Then, in
the same way as in the previous case,
lim
n→∞Pn = lim
n→∞CBnC−1 = C


1
0
. . .
0
0
...
0
0

C−1 = 1π′.
Example 10.5
A three-state ergodic Markov chain
Consider the stochastic, ergodic matrix in Example 10.1. The matrix has
three distinct eigenvalues
λ1 = 1,
λ2 = 1
12

4 −
√
10

,
λ3 = 1
12

4 +
√
10

,
and the corresponding right eigenvectors are
c′
1
=
[1, 1, 1] ,
c′
2
=
1
2

1 +
√
10

, 1
4

−4 −
√
10

, 1

,
c′
3
=
1
2

1 −
√
10

, 1
4

−4 +
√
10

, 1

.
Forming the diagonal matrix D = diag {λ1, λ2, λ3} with the three eigen-
values, and forming matrix C with columns corresponding to the three

10.8 Limiting Behavior
495
eigenvectors, it can be veriﬁed readily that, up to four decimal places,
P =


1
2
1
2
0
1
4
1
2
1
4
0
1
3
2
3

= CDC−1
=


1
2.0811
−1.0811
1
−1.7906
−0.2094
1
1
1

D


0.2222
0.4444
0.3333
0.1699
−0.2925
0.1225
−0.3922
−0.1519
0.5442

.
Further, expression (10.26) is of the following form:
Pn
=
1n


0.2222
0.4444
0.3333
0.2222
0.4444
0.3333
0.2222
0.4444
0.3333


+

 1
12

4 −
√
10
n
Q2 +

 1
12

4 +
√
10
n
Q3,
which shows that the two transient terms in the second line tend rapidly
to zero as n increases.
■
Example 10.6
A periodic, irreducible Markov chain
Consider the chain introduced in Example 10.2 with transition probability
matrix
P =


0
1
0
0
0
1
2
0
1
2
0
0
0
1
2
0
1
2
0
0
0
1
2
0
1
2
0
0
0
1
0


.
This matrix has period d = 2. The ﬁve distinct eigenvalues are
λ1 = −1,
λ2 = 0,
λ3 = 1,
λ4 = −1
√
2,
λ5 =
1
√
2.
The matrix C whose columns are the corresponding right eigenvectors is
C =


1
1
1
−1
−1
−1
0
1
1
√
2
−1
√
2
1
−1
1
0
0
−1
0
1
−1
√
2
1
√
2
1
1
1
1
1


.
Forming the diagonal matrix D whose diagonal elements are the ﬁve eigen-
values λ1, . . . , λ5, one can verify result (10.23).

496
10. An Overview of Discrete Markov Chains
For this example, (10.26) takes the following form:
Pn = (−1)n


0.125
−0.25
0.25
−0.25
0.125
−0.125
0.25
−0.25
0.25
−0.125
0.125
−0.25
0.25
−0.25
0.125
−0.125
0.25
−0.25
0.25
−0.125
0.125
−0.25
0.25
−0.25
0.125


+ 1n


0.125
0.25
0.25
0.25
0.125
0.125
0.25
0.25
0.25
0.125
0.125
0.25
0.25
0.25
0.125
0.125
0.25
0.25
0.25
0.125
0.125
0.25
0.25
0.25
0.125


+

−1
√
2
n
Q4 +

 1
√
2
n
Q5,
which shows that, for large n, the eigenvectors associated with the eigenval-
ues with absolute value less than 1 become irrelevant. Also, as illustrated
before, the value of Pn varies depending on whether n is even or odd. The
unique stationary distribution for this Markov chain exists, but the chain
fails to converge to it.
■
The next chapter discusses MCMC methods. Here, one creates a Markov
chain using random number generators. This particular chain is constructed
in such a way that, eventually, the numbers drawn can be interpreted as
samples from the stationary distribution π, which is often a posterior distri-
bution. The key to the right construction process is the choice of transition
kernel which governs the behavior of the system. Many transition kernels
can be used for a particular problem, and, typically, they are all derived
by imposing the condition of reversibility. Then provided that the Markov
chain is ergodic, and only then, a reversible system has π as its unique sta-
tionary distribution. Convergence to this distribution however, takes place
asymptotically. Many factors, such as the choice of transition kernel, the
degree of correlation of the parameters of the model in their posterior distri-
butions, and the structure of the data available, can interfere with a smooth
trip towards the ﬁnal goal. This general area is discussed an illustrated in
the chapters ahead.

11
Markov Chain Monte Carlo
11.1
Introduction
Markov chain Monte Carlo (MCMC) has become a very important compu-
tational tool in Bayesian statistics, since it allows inferences to be drawn
from complex posterior distributions where analytical or numerical inte-
gration techniques cannot be applied. The idea underlying these methods
is to generate a Markov chain via iterative Monte Carlo simulation that
has, at least in an asymptotic sense, the desired posterior distribution as
its equilibrium or stationary distribution. An important paper in this area
is Tierney (1994).
The classic papers of Metropolis et al. (1953) and of Hastings (1970) gave
rise to a very general MCMC method, namely the Metropolis–Hastings al-
gorithm, of which the Gibbs sampler, which was introduced in statistics
and given its name by Geman and Geman (1984), is a special case. In
Hastings (1970), the algorithm is used for the simulation of discrete equi-
librium distributions on a space of ﬁxed dimension. In a statistical setting,
ﬁxed dimensionality implies that the number of parameters of the model is
a known value. The reversible jump MCMC algorithm introduced in Green
(1995) is a generalization of Metropolis–Hastings; it has been applied for
simulation of equilibrium distributions on spaces of varying dimension. The
number of parameters of a model is inferred via its marginal posterior dis-
tribution.
In this chapter, the focus is on these MCMC algorithms. Much of the
material is drawn from the tutorial paper of Waagepetersen and Sorensen

498
11. Markov Chain Monte Carlo
(2001). Markov chains with continuous state spaces are considered here;
that is, the random variables are assumed to be distributed continuously.
Many results for discrete state spaces discussed in Chapter 10 hold also, but
some technical modiﬁcations are needed for Markov chains with continuous
state spaces. For example, convergence to a unique stationary distribution
requires the continuous Markov chain to be irreducible and aperiodic, as for
discrete chains. In addition, a continuous Markov chains must be “Harris
recurrent”. This is a measure theoretical technicality required to avoid the
possibility of starting points from which convergence is not assured. This
property shall not be discussed here, and the reader is referred to Meyn
and Tweedie (1993), to Tierney (1994), and to Robert and Casella (1999).
This chapter is organized as follows. The following section introduces
notation and terminology, since the continuous state space situation re-
quires some additional details. Subsequently, an overview of MCMC is pre-
sented, including a description of the Metropolis–Hastings, Gibbs sampling,
Langevin-Hastings and reversible jump methods. The chapter ends with a
short description of the data augmentation strategy, a technique that may
lead to simpler computational expressions in the process of ﬁtting a prob-
ability model via MCMC.
11.2
Preliminaries
11.2.1
Notation
The notation in this chapter is the standard one found in the literature
on MCMC, and it deviates somewhat from that used in the rest of this
book. Here no notational distinction is made between scalar and vector
random variables, but this should not lead to ambiguities. In this chapter,
the probability of the event E is denoted by P (E) rather than by Pr (E).
Suppose that Z = (Z1, . . . , Zd) is a real random vector of dimension
d ≥1. Resuming the notation introduced in (1.1), we shall say that Z has
density f on Rd if the probability that Z belongs to a subset A of Rd is
given by the d-fold integral
P(Z ∈A) =
 ∞
−∞
 ∞
−∞
· · ·
 ∞
−∞
I((z1, . . . , zd) ∈A)f(z1, . . . , zd) dz1 · · · dzd,
where the indicator function I((z1, . . . , zd) ∈A) is one if (z1, . . . , zd) is in A,
and zero otherwise. If, for example, A = [a1, b1] × [a2, b2], then the integral
is
 
I(x ∈[a1, b1], y ∈[a2, b2])f(x, y) dx dy =
 b1
a1
 b2
a2
f(x, y) dx dy.

11.2 Preliminaries
499
Integrals will usually be written in abbreviated form
P(Z ∈A) =

I(z ∈A)f(z) dz =

A
f(z) dz
whenever the dimension of the integral is clear from the context.
11.2.2
Transition Kernels
In the case of Markov chains with continuous state spaces, it does not
make sense to write p (i, j) = Pr (Xn = j|Xn−1 = i), since this probability
is always null. Further, the fact that X is continuously distributed precludes
construction of transition probability matrices and transition kernels are
used instead. Assume that X is a p-dimensional stochastic vector with
density f on Rp and assume that A is a subset in Rp. Then the transition
kernel P(·, ·) speciﬁes the conditional probability that Xn+1 falls in A given
that Xn = x and will be written as
P (x, A) = P (Xn+1 ∈A|Xn = x) .
(11.1)
This notation (departing from what has been used before in the book)
is required for reasons mentioned below.
11.2.3
Varying Dimensionality
Let Y be a stochastic vector in Rq. In general, the joint probability that
X ∈A and Y ∈B will be written as
P (X ∈A, Y ∈B) =

A
P (x, B) f (x) dx
(11.2)
for all subsets A ⊆Rp and B ⊆Rq. If Y has density in Rq, then (X, Y )
has joint density h on Rp+q. In this case, (11.2) is the well-known identity
P (X ∈A, Y ∈B) =

A

B
h (x, y) dx dy.
(11.3)
However, as discussed in the sections describing single-site Metropolis–
Hastings updates and reversible jump samplers, there are situations when
the q-dimensional stochastic vector Y has density on Rq′, q′ < q. In this
case, (X, Y ) does not have density h on Rp+q and, consequently, the joint
probability P (X ∈A, Y ∈B) cannot be calculated from (11.3) and one
needs to refer to (11.2). As an example of the latter, consider computing
for the two-dimensional vector Y = (Y1, Y2)′ and the scalar variable X:
P (Y ∈B|X = x)
(11.4)

500
11. Markov Chain Monte Carlo
Suppose that given X = x, Y is deﬁned by the deterministic mapping g
(Y1, Y2) = g (x, U) = (x + U, x −U) ,
where U is a stochastic one-dimensional random variable with density q
on R. Given x, Y does not have density on R2 since it can only take
values on the line y1 = 2x −y2; that is, once y2 is known, the value of
y1 is determined completely. Then (11.4) must be computed from the one-
dimensional integral
P (Y ∈B|X = x) = P (x, B) =

I ((x + U, x −U) ∈B) q (u) du
and P (X ∈A, Y ∈B), A ⊆R, B ⊆R2, is computed from (11.2):
P (X ∈A, Y ∈B) =

A

I ((x + U, x −U) ∈B) q (u) f (x) du dx.
If B = [a1, b1] × [a2, b2], this becomes
P (X ∈A, Y ∈B)
=

A

I (x + U ∈[a1, b1] , x −U ∈[a2, b2]) q (u) f (x) du dx.
On the other hand, if [Y1, Y2, X] had density h on R3, one could use the
standard formula
P (X ∈A, Y ∈B) =

A
 b1
a1
 b2
a2
h (x, y1, y2) dx dy1 dy2.
11.3
An Overview of Markov Chain Monte Carlo
The MCMC algorithms to be described below are devices for constructing
a Markov chain that has the desired equilibrium distribution as its limit-
ing invariant distribution. In the previous chapter on Markov chains, the
transition probability matrix P was known, and one wished to character-
ize the equilibrium distribution. The nature of the problem with MCMC
methods is the opposite one: the equilibrium distribution is known (usually
up to proportionality), but the transition kernel is unknown. How does one
construct a transition kernel that generates a Markov chain with a desired
stationary distribution?
Let X denote a real stochastic vector of unknown parameters or unob-
servable variables associated with some model, and assume it has a dis-
tribution with density π on Rd. The density π could represent a posterior
density. This density has typically a complex form, such that the necessary

11.3 An Overview of Markov Chain Monte Carlo
501
expectations with respect to π cannot be evaluated analytically or by using
standard techniques for numerical integration. In particular, π may only
be known up to an unknown normalizing constant. Direct simulation of π
may be diﬃcult, but as shown below, it is usually quite easy to construct
a Markov chain whose invariant distribution has density given by π. The
Markov chain X1, X2, . . . is speciﬁed in terms of the distribution for the
initial state X1 and the transition kernel P(·, ·) which speciﬁes the condi-
tional distribution of Xi+1 given the previous state Xi. If the value of the
current state is Xi = x, then the probability that Xi+1 is in a set A ⊆Rd is
given by (11.1). If the generated Markov chain is irreducible with invariant
distribution π, it can be used for Monte Carlo estimation of various ex-
pectations E (h (X)) with respect to π (Tierney, 1994; Meyn and Tweedie,
1993). That is, for any function h on Rd with ﬁnite expectation E (h (X)),
E (h (X)) =

h (x) π (x) dx = lim
N→∞
1
N
N

i=1
h (Xi) .
(11.5)
Thus, E (h (X)) can be approximated by
E (h (X)) ≈
N

i=1
h (Xi)
N
,
(11.6)
the sample average, for some large N. Here h could be the indicator function
of a set A ⊆Rd, so that E (h (X)) equals the probability
P (X ∈A) = E (I (X ∈A)) ,
which is approximated by N
i=1 I (Xi ∈A) /N . (Convergence of the sample
average to E (h (X)) for all starting values, also requires the assumption
of Harris recurrence). Result (11.5) does not rest upon the condition of
aperiodicity, even though this is required for convergence of the Markov
chain to π.
As discussed in the previous chapter, the density π is invariant for the
Markov chain if the transition kernel P(·, ·) of the Markov chain preserves
π, i.e., if Xi ∼π implies Xi+1 ∼π or, in terms of P(·, ·) and π, if

Rd P (x, B) π (x) dx =

B
π (x) dx.
(11.7)
Both terms of this expression are equal to the marginal probability that
X ∈B, provided that π is the invariant density. In order to verify that π
is the invariant density using (11.7) is a diﬃcult task, since this involves
integration with respect to π. The infeasibility of doing this was the reason
for using MCMC in the ﬁrst place. However, choosing a transition kernel
which imposes the stronger condition of reversibility with respect to π is

502
11. Markov Chain Monte Carlo
suﬃcient to guarantee that π is invariant for the Markov chain. In the
previous chapter it was indicated that reversibility holds if (Xi, Xi+1) has
the same distribution as the time-reversed subchain (Xi+1, Xi) whenever
Xi has density π, i.e., if
P (Xi+1 ∈A, Xi ∈B) =

B
P (x, A) π (x) dx
= P (Xi+1 ∈B, Xi ∈A) =

A
P (x, B) π (x) dx
(11.8)
for subsets A, B ⊆Rd. Taking A = Rd, the integral in the ﬁrst line be-
comes

B P

x, Rd
π (x) dx =

B π (x) dx, since P

x, Rd
= 1. Therefore
the reversibility condition (11.8) implies (11.7). The Metropolis–Hastings
algorithm or, more generally, the reversible jump MCMC, oﬀers a practical
recipe for constructing a reversible Markov chain with the desired invariant
distribution by an appropriate choice of a transition kernel.
11.4
The Metropolis–Hastings Algorithm
11.4.1
An Informal Derivation
An informal and intuitive derivation of the Metropolis–Hastings algorithm
will be given ﬁrst. This is followed by a more formal approach which eases
the way into reversible jump MCMC.
Since direct sampling from the target distribution may not be possible,
the Metropolis–Hastings algorithm starts by generating candidate draws
from a so-called proposal distribution. These draws are then “corrected”
so that they behave, asymptotically, as random observations from the de-
sired equilibrium or target distribution. The Markov chain produced by
the Metropolis–Hastings algorithm at each stage is thus constructed in two
steps: a proposal step and an acceptance step. These two steps are asso-
ciated with the proposal distribution and with the acceptance probability
which are the two ingredients of the Metropolis–Hastings transition kernel.
Assume that at stage n the state of the chain is Xn = x. The random vari-
able X may be a scalar or a vector. The next state of the chain is chosen
by ﬁrst sampling a candidate point Yn+1 = y from a proposal distribution
with p.d.f. q (x, ·). The density q (x, ·) may (or may not) depend on the cur-
rent point x. If it does, q (x, y) is a conditional p.d.f. of y, given x, which
in other parts of this text has been referred to as p (y|x). For example,
q (x, ·) may be a multivariate normal distribution with mean x and ﬁxed
covariance matrix. The candidate point y is then accepted with probability
a (x, y) to be derived below. If y is accepted, then the next state becomes
Xn+1 = y. If the candidate point is rejected, the chain does not move, i.e.,
Xn+1 = x. The algorithm is extremely simple:

11.4 The Metropolis–Hastings Algorithm
503
INITIALIZE X0
DO i = 1, n
SAMPLE Y FROM q (x, ·)
SAMPLE U FROM Un (0, 1)
IF (U ≤a (X, Y )) SET Xi = y
OTHERWISE SET Xi = x
ENDDO
We turn to the informal derivation of the acceptance probability a (x, y),
drawing from the tutorial of Chib and Greenberg (1995). For a joint up-
dating Metropolis–Hastings algorithm in equilibrium, the random vector
(Xn, Yn+1), consisting of the current Markov chain state and the proposal,
has joint density g, given by
g (x, y) = q (x, y) π (x) ,
(11.9)
where π is the equilibrium density and q is the proposal density of Yn+1,
given that Xn has value x. If q satisﬁes the reversibility condition such that
q (x, y) π (x) = q (y, x) π (y)
(11.10)
for all (x, y) , then the proposal density is the correct transition kernel of
the Metropolis–Hastings chain. Most likely it will be the case that, for some
(x, y),
q (x, y) π (x) > q (y, x) π (y) ,
(11.11)
say. In order to achieve equality and thus ensuring reversibility, one can
introduce a probability a (x, y) < 1 on the left-hand side, such that tran-
sitions from x to y (x ̸= y) are made according to q (x, y) a (x, y). Setting
a (y, x) = 1 on the right-hand side yields
q (x, y) π (x) a (x, y)
=
q (y, x) π (y) a (y, x)
=
q (y, x) π (y)
from which the acceptance probability becomes
a (x, y) = π (y) q (y, x)
π (x) q (x, y).
(11.12)
If inequality (11.11) is reversed, a probability a (y, x) is introduced appro-
priately and derived as above, after setting a (x, y) = 1. The probabilities
a guarantee that detailed balance is satisﬁed. These arguments imply that
the probability of acceptance must be
a (x, y) = min

1, π (y) q (y, x)
π (x) q (x, y)

,
π (x) q (x, y) > 0.
(11.13)
Notice that if (11.10) holds, a (x, y) = 1 and the candidate draw y is
accepted. This is equivalent to sampling the candidate point from the equi-
librium distribution.

504
11. Markov Chain Monte Carlo
A key observation is that the posterior distribution π must be known up
to proportionality only, since the normalizing constant cancels in the ratio
π (y) /π(x). Finally, it is clear from (11.13) that when symmetrical proposal
densities are considered with q (y, x) = q (x, y), the acceptance probability
reduces to
a (x, y) = min

1, π (y)
π (x)

,
π (x) > 0,
which is the case originally considered by Metropolis et al. (1953).
A few comments about implementation are in order. The acceptance
probability is clearly deﬁned provided π (x) q (x, y) > 0. If the chain starts
with a value x0 such that π (x0) > 0, then π (xn) > 0 for every n, since val-
ues of the proposal Y for which π (y) = 0 lead to an acceptance probability
equal to zero, and are therefore rejected. The success of the method depends
on striking some “right” rate of acceptance. Parameterization and choice of
the proposal density play a fundamental role here. Acceptance ratios in the
neighborhood of 1 imply very similar values between previous and proposed
states and the chain will move very slowly (unless, of course, the proposal
distribution is the equilibrium distribution, leading to an acceptance ratio
of 1!). On the other hand, if the proposed displacement is too large and
falls where the posterior has no support, this will lead to a high rejection
rate. Here, the chain will remain in the same state for many iterations.
No general optimization rules are available. However, Roberts et al. (1997)
obtained optimal acceptance rates of 23.4% for high-dimensional problems
under quite general conditions. Guidance on the proper choice of proposal
densities can be found, for example, in Chib and Greenberg (1995).
11.4.2
A More Formal Derivation
A more formal derivation of the acceptance probability (11.13) is given
here; this will be useful in understanding reversible jump MCMC later
on. A distinction is made between two implementations of the Metropolis–
Hastings algorithm. These are the simultaneous and the single-site updat-
ing schemes. In the former scheme all the random variables of the model are
updated jointly, whereas in the latter, random variables are updated one at
a time. In the single-site updating strategy, and in common with reversible
jump MCMC, the proposal distribution and the target distribution have
densities on spaces of diﬀerent dimension. The informal derivation of the
previous section was based on the simultaneous updating algorithm.
Metropolis–Hastings Simultaneous Updating Algorithm
As before, let Xn denote the nthe state of a Metropolis–Hastings chain
X1, X2, . . . and let Yn+1 denote the proposal for the next state of the chain.
The random vector (Xn, Yn+1) has joint density g on R2d given by (11.9),
where π is the d-dimensional target density and q (x, ·) is the d−dimensional

11.4 The Metropolis–Hastings Algorithm
505
proposal density of Yn+1, given that X has the value x ∈Rd. In this sec-
tion the acceptance probability of the simultaneous updating Metropolis–
Hastings algorithm is derived subject to the reversibility condition
P (Xn ∈A, Xn+1 ∈B) = P (Xn ∈B, Xn+1 ∈A)
(11.14)
for all A, B ⊆Rd. The left-hand side of (11.14) can be written as
P (Xn ∈A, Xn+1 ∈B) =

A
P (Xn+1 ∈B|Xn = x) π (x) dx.
(11.15)
For any B ⊆Rd deﬁne the proposal distribution as
Q (x, B) = P (Yn+1 ∈B|Xn = x) =

I (y ∈B) q (x, y) dy
(11.16)
which is the conditional probability that Yn+1 belongs in a set B, given
that Xn = x. Also deﬁne
Qa (x, B)
=
P (Yn+1 ∈B and Yn+1 is accepted|Xn = x)
=

I (y ∈B) q (x, y) a (x, y) dy
(11.17)
as the conditional probability that Yn+1 belongs in a set B and Yn+1 is
accepted, given that Xn = x and deﬁne further
s (x) = P (Yn+1 is rejected|Xn = x)
(11.18)
as the conditional probability of rejecting the proposal, given that Xn = x.
Then the transition kernel P (Xn+1 ∈B|Xn = x) can be written as
P (Xn+1 ∈B|Xn = x) = Qa (x, B) + s (x) I (x ∈B) .
(11.19)
This is by virtue of the law of total probability, since there are two ways
in which Xn+1 ∈B. One is generating a proposal Yn+1 that belongs in B
and that this candidate is accepted; the other one is rejecting the proposal,
so that the new state Xn+1 = Xn = x, and that x ∈B. Hence (11.15)
becomes equal to
P (Xn ∈A, Xn+1 ∈B)
=

A
Qa (x, B) π (x) dx +

A
s (x) I (x ∈B) π (x) dx.
(11.20)
By virtue of symmetry, the right-hand side of (11.14) is given by
P (Xn ∈B, Xn+1 ∈A)
=

B
Qa (x′, A) π (x′) dx′ +

B
s (x′) I (x′ ∈A) π (x′) dx′.
(11.21)

506
11. Markov Chain Monte Carlo
The second term on the right-hand side of (11.20) can be written as

A
s (x) I (x ∈B) π (x) dx =

s (x) I (x ∈B ∩A) π (x) dx
(11.22)
and the second term on the right-hand side of (11.21) can similarly be
written as

B
s (x′) I (x′ ∈A) π (x′) dx′ =

s (x′) I (x′ ∈B ∩A) π (x′) dx′. (11.23)
Clearly, (11.22) = (11.23) (notice that x and x′ are dummy variables of
integration), and therefore the reversibility condition is satisﬁed if

A
Qa (x, B) π (x) dx =

B
Qa (x′, A) π (x′) dx′.
(11.24)
Using deﬁnition (11.17) for Qa (x, B), the left-hand side of (11.24) can be
written more explicitly as

A
Qa (x, B) π (x) dx =

A

I (y ∈B) q (x, y) a (x, y) π (x) dx dy
=
 
I (x ∈A, y ∈B) q (x, y) a (x, y) π (x) dx dy,
(11.25)
and, similarly, for the right-hand side of (11.24):

B
Qa (x′, A) π (x′) dx′
=
 
I (x′ ∈B, y′ ∈A) q (x′, y′) a (x′, y′) π (x′) dx′ dy′.
(11.26)
In order to write (11.25) and (11.26) explicitly as functions of the same
variables, one can make the change of variables (y = x′) and (x = y′) −see
the Note below. Since the Jacobian of the transformation is 1, the right-
hand side of (11.26) now becomes
 
I (y ∈B, x ∈A) q (y, x) a (y, x) π (y) dy dx.
Substituting in (11.24) yields the following expression for the reversibility
condition:
 
I (x ∈A, y ∈B) q (x, y) a (x, y) π (x) dx dy
=
 
I (y ∈B, x ∈A) q (y, x) a (y, x) π (y) dy dx.
Equality is satisﬁed if
q (x, y) a (x, y) π (x) = q (y, x) a (y, x) π (y) .

11.4 The Metropolis–Hastings Algorithm
507
Choosing the acceptance probability as large as possible (i.e., setting a (y, x)
equal to 1) subject to detailed balance, as suggested by Peskun (1973) yields
a (x, y) = min

1, q (y, x) π (y)
q (x, y) π (x)

.
(11.27)
Note In moving from Xn = x to Xn+1 = x′, a proposal with realized
value y is generated from q (x, ·). If the proposal is accepted, x′ = y. In
the opposite move, from Xn = x′ to Xn+1 = x, the proposal with realized
value y′ is generated from q (x′, ·). If the proposal is accepted, x = y′.
Metropolis–Hastings Single-Site Updating Algorithm
In the single-site updating algorithm, only one component of Xn ∈Rd is
updated at a time. Then, given that Xn = x, Yn+1 equals x except at
the ithe component, where xi is replaced by a random variable Zi gener-
ated from a one-dimensional proposal density qi (x, ·) that may or may not
depend on x or on a subset of x. Since
Yn+1 ∈B ⇔(x1, . . . , xi−1, Zi, xi+1, . . . , xd) ∈B,
then the probability that Yn+1 belongs in B ⊆Rd, given Xn = x, is given
by the proposal distribution
Q (x, B) = P (Yn+1 ∈B|Xn = x)
=

I ((x1, . . . , xi−1, zi, xi+1, . . . , xd) ∈B) qi (x, zi) dzi
(11.28)
which is a one-dimensional integral. Notice that target density π lives on
Rd, while the proposal density qi (x, ·) lives on R.
Consider the move from a realized value equal to
x = (x1, . . . , xi−1, xi, xi+1, . . . , xd)
and to a realized value equal to
x′ = (x1, . . . , xi−1, zi, xi+1, . . . , xd) .
Note that in the notation used here, contrary to the case in the previous
section, x′ diﬀers from x in one element only. The probability that Xn+1
belongs in B ⊆Rd, given Xn = x, is given by
P (Xn+1 ∈B|Xn = x) = Qa (x, B) + s (x) I (x ∈B) ,
(11.29)
where Qa (x, B) = P (Yn+1 ∈B and Yn+1 is accepted|Xn = x) is here equal
to
Qa (x, B) =

I (x′ ∈B) a (x, x′) qi (x, zi) dzi.
(11.30)

508
11. Markov Chain Monte Carlo
The second term in (11.29) is s (x) = P (Yn+1 is rejected|Xn = x).
The left-hand side of the reversibility equation (11.14) can be written as
follows:
P (Xn ∈A, Xn+1 ∈B)
=

A
Qa (x, B) π (x) dx +

A
s (x) I (x ∈B) π (x) dx.
Similarly, for the move in the opposite direction, the right-hand side of
(11.14) is given by
P (Xn ∈B, Xn+1 ∈A)
=

B
Qa (x′, A) π (x′) dx′ +

B
s (x′) I (x′ ∈A) π (x′) dx′,
where Xn has the realized value x′. As in the previous section, the second
terms in the right-hand side of these expressions can be shown to be equal.
Therefore, reversibility is satisﬁed if

A
Qa (x, B) π (x) dx =

B
Qa (x′, A) π (x′) dx′,
(11.31)
where
Qa (x′, A) = P (Yn+1 ∈A and Yn+1 is accepted|Xn = x′)
=

I (x ∈A) qi (x′, xi) a (x′, x) dxi.
(11.32)
Substituting (11.30) in the left-hand side of (11.31) yields

A

I (x′ ∈B) qi (x, zi) a (x, x′) π (x) dzi dx
=

Rd

R
I (x′ ∈B, x ∈A) qi (x, zi) a (x, x′) π (x) dzi dx.
(11.33)
Similarly, substituting (11.32) in the right-hand side of (11.31) yields:

Rd

R
I (x ∈A, x′ ∈B) qi (x′, xi) a (x′, x) π (x′) dx′ dxi.
(11.34)
A condition for equality of (11.33) and (11.34) is that
qi (x, zi) a (x, x′) π (x) = qi (x′, xi) a (x′, x) π (x′) .
As in (11.27), using the criterion due to Peskun (1973) yields the acceptance
probability
a (x, x′) = min

1, qi (x′, xi) π (x′)
qi (x, zi) π (x)

.
(11.35)
The arguments above also hold when the updating variable, rather than
being a scalar, is a vector and a subset of x.

11.5 The Gibbs Sampler
509
11.5
The Gibbs Sampler
The Gibbs sampler is a very popular MCMC algorithm because of its com-
putational simplicity. As shown below, it is a special case of the Metropolis–
Hastings algorithm. In order to see this, in the move from x to x′, let the
proposal be generated from
qi (x, zi) = π (zi|x−i) ,
(11.36)
where x−i is equal to x with its ith component deleted, that is, x−i =
(x1, . . . , xi−1, xi+1, . . . , xd). Similarly, in the opposite move, let the proposal
be a draw from
qi (x′, xi) = π (xi|x−i) .
(11.37)
In a Bayesian context, the right-hand sides of (11.36) and (11.37) are known
as the fully conditional posterior distributions. Since
π (zi|x−i) = π (x′) /π (x−i)
and
π (xi|x−i) = π (x) /π (x−i) ,
substituting in (11.35) yields
qi (x′, xi) π (x′)
qi (x, zi) π (x) = π (x) π (x′) π (x−i)
π (x) π (x′) π (x−i) = 1.
Therefore, a Metropolis–Hastings proposal generated from the appropri-
ate fully conditional distribution (a conditional posterior distribution in
the Bayesian setting), is accepted always. This scheme is known as Gibbs
sampling.
The transition kernel of the Gibbs sampler for the updating of all ele-
ments of x involves the product
π (z1|x2, x3, . . . , xd) π (z2|z1, x3, . . . , xd) . . . π (zd|z1, z2, . . . , zd−1) .
The transition kernel of the Gibbs sampler preserves π. To see this and for
notational simplicity, let d = 3. Then, in terms of (11.7):
P (x, B) =

I (z1, z2, z3 ∈B) π (z1|x2, x3) π (z2|z1, x3)
×π (z3|z1, z2) dz1 dz2 dz3,
where it is clear from the context that the integral is three-dimensional.
Substituting in the left-hand side of (11.7) and integrating over the distri-
bution of x1, x2, and x3 yields

P (x, B) π (x1, x2, x3) dx1 dx2 dx3
=

I (z1, z2, z3 ∈B) π (z1, z2, z3) dz1 dz2 dz3
= P (X1, X2, X3 ∈B) .

510
11. Markov Chain Monte Carlo
Equation (11.7) is also satisﬁed if the transition kernel is deﬁned with
respect to only one of the elements of x. To verify this, and in terms of
this three-dimensional example, the transition kernel for the updating of
the ﬁrst element of x is now
P (x, B) =

I (z1, x2, x3 ∈B) π (z1|x2, x3) dz1
and the left-hand side of equation (11.7) is

P (x, B) π (x) dx
=
 
I (z1, x2, x3 ∈B) π (z1|x2, x3) π (x1, x2, x3) dz1 dx1 dx2 dx3.
Integrating over the distribution of x1 yields

I (z1, x2, x3 ∈B) π (z1, x2, x3) dz1 dx2 dx3
= P (X1, X2, X3 ∈B) .
11.5.1
Fully Conditional Posterior Distributions
Consider the vector of parameters (θ1, θ2, . . . , θp) whose posterior distribu-
tion is proportional to p (θ1, . . . , θi−1, θi, θi+1, . . . , θp|y). Let
θ−i = (θ1, . . . , θi−1, θi+1, . . . , θp)
be the vector of dimension (p −r), p > r, r ≥1, which is equal to θ with
its ith component, θi, deleted, and where r is the number of elements in
θi. The fully conditional posterior distribution of θi is
p (θi|θ−i, y) =
p (θ1, . . . , θi−1, θi, θi+1, . . . , θp|y)

p (θ1, . . . , θi−1, θi, θi+1, . . . , θp|y) dθi
∝p (θ1, . . . , θi−1, θi, θi+1, . . . , θp|y) .
(11.38)
In many applications, r = 1 and parameters are updated one at a time. In
general, single-site updating leads to moves along each coordinate, whereas
updating several components in a block allows for more general moves.
Joint updating, which incorporates information on the correlation structure
among the components in the joint conditional posterior distribution, can
result in faster convergence when correlations are strong (Liu et al., 1994).
11.5.2
The Gibbs Sampling Algorithm
Consider the vector of parameters of a model (θ1, θ2, . . . , θp), with posterior
density p (θ1, θ2, . . . , θp|y), known up to proportionality. Assume that the

11.5 The Gibbs Sampler
511
user supplies “legal” starting values

θ(0)
1 , θ(0)
2 , . . . , θ(0)
p

,
in the sense that p

θ(0)
1 , θ(0)
2 , . . . , θ(0)
p |y

> 0. The implementation of the
Gibbs sampler consists of iterating through the loop:
draw θ(1)
1
from p

θ1|θ(0)
2 , . . . , θ(0)
p , y

,
draw θ(1)
2
from p

θ2|θ(1)
1 , θ(0)
3 , . . . , θ(0)
p , y

,
draw θ(1)
3
from p

θ3|θ(1)
1 , θ(1)
2 , θ(0)
4 , . . . , θ(0)
p , y

,
...
draw θ(1)
p
from p

θp|θ(1)
1 , . . . , θ(1)
p−1, y

,
draw θ(2)
1
from p

θ1|θ(1)
2 , . . . , θ(1)
p , y

,
...
and so on.
After an initial period during which samples are dependent on the starting
value (burn in period), the draws θ(i)
1 , θ(i)
2 , . . . , θ(i)
p , for suﬃciently large i,
are regarded as samples from the normalized posterior distribution with
density
p (θ1, θ2, . . . , θp|y)/

p (θ1, θ2, . . . , θp|y) dθ1 . . . dθp.
The coordinate θ(i)
j
is regarded as a draw from its marginal posterior dis-
tribution with density
p (θj|y)/

p (θj|y) dθj.
The Fully Conditional Distributions Determine the Joint Distribution
The Gibbs sampler produces draws from a joint distribution by sampling
successively from all fully conditional posterior distributions. This implies
that the form of the fully conditional distributions determines uniquely the
form of the joint distribution. The main idea is sketched below for the two-
dimensional case. The general case is known as the Hammersley–Cliﬀord
in the spatial statistics literature (Besag, 1974).
Consider the identity
p (x, y) = p (y|x) p (x) = p (x|y) p (y) .
(11.39)

512
11. Markov Chain Monte Carlo
From (11.39), it follows that
p (y) = p (y|x)
p (x|y)p (x) ∝p (y|x)
p (x|y).
(11.40)
The normalized marginal density is
p (y) =
p (y|x)/ p (x|y)

p (y|x)/ p (x|y) dy .
Substituting in (11.39) yields
p (x, y) =
p (y|x)

p (y|x)/ p (x|y) dy .
This shows that p (x, y) can be expressed in terms of the conditional distri-
butions (even though it may not be possible to write the joint distribution
explicitly). This result is based on the implicit assumption that the joint
distribution [X, Y ] exists.
Example 11.1
A single observation from a bivariate normal distribution
with known covariance matrix
As a trivial example, consider data that consist of a single observation
y = (y1, y2) from a bivariate normal distribution with unknown mean
θ = (θ1, θ2)
and known covariance matrix
V =

1
ρ
ρ
1

.
Thus,
y|θ, V ∼N (θ, V) .
One wishes to obtain draws from p (θ|y, V). Let the prior distribution for θ
be proportional to a constant, independent of θ. Then the posterior density
of θ is
p (θ|y, V)
∝
p (θ) p (y|θ, V)
∝
p (y|θ, V) ,
(11.41)
a bivariate normal distribution. The stochastic element in this distribution
is the vector θ, for ﬁxed data y and covariance matrix V. When regarded
as a function of θ, (11.41) is the density of a normal distribution with mean
y and variance V. That is
θ|y, V ∼N2 (y, V) .
(11.42)

11.5 The Gibbs Sampler
513
Implementation of the Gibbs sampler in a single-site updating scheme re-
quires drawing successively from
θ1|θ2, y, V ∼N

y1 + ρ (θ2 −y2) , 1 −ρ2
(11.43)
and from
θ2|θ1, y, V ∼N

y2 + ρ (θ1 −y1) , 1 −ρ2
.
(11.44)
After a number of rounds of iteration (the burn-in period) the system con-
verges to the stationary distribution (11.42). At this point, the samples
obtained from (11.43) are Monte Carlo draws from p (θ1|y, V) and those
from (11.44) are Monte Carlo draws from p (θ2|y, V), the densities of the
respective marginal posterior distributions. The samples (θ1, θ2) are corre-
lated draws from p (θ|y, V). This correlation among the samples slows down
convergence and increases the Monte Carlo sampling error of estimates of
features of the posterior distribution. On the other hand, joint updating
involves sampling from (11.42); in this example, the system converges in
one round. In the joint updating implementation, the pairs (θ1, θ2) are
independent draws from p (θ|y, V). This is equivalent to direct sampling
from the target distribution of interest. Obviously, one would not employ
MCMC in such a situation.
■
Example 11.2
A hierarchical Bayesian model
Consider the two-parameter Bayesian model
yi|µ, σ2 ∼N

µ, σ2
,
i = 1, . . . , n, −∞< µ < ∞, σ2 > 0,
where µ and σ2 are the unknown mean and variance, respectively. These
are assumed to be a priori independent, with prior distributions equal to
µ ∼N (0, 1)
and
σ2|S, v ∼vSχ−2
v .
That is, the mean is assigned a normal (0, 1) prior, and the variance a
scaled inverted chi-square distribution with (assumed known) parameters
S and v. Under conditional independence, the likelihood is
p

y|µ, σ2
=
n
-
i=1

2πσ2−1
2 exp

−(yi −µ)2
2σ2

=

2πσ2−n
2 exp

−
n
i=1
(yi −µ)2
2σ2

,

514
11. Markov Chain Monte Carlo
and the joint posterior density is given by
p

µ, σ2|y

∝p (µ) p

σ2
p

y|µ, σ2
∝exp

−µ2
2
 
σ2−( v
2 +1) exp

−vS
2σ2
 
σ2−n
2
× exp

−
n
i=1
(yi −µ)2
2σ2

.
(11.45)
Implementing the Gibbs sampler requires knowledge of the fully conditional
posterior distributions with densities p

µ|σ2, y

and p

σ2|µ, y

−omitting
the conditioning on hyperparameters S and v.
The derivation of the fully conditional posterior distribution of µ requires
extracting terms which are function of µ from the joint posterior density
(11.45). This leads to
p

µ|σ2, y

∝exp

−µ2
2

exp

−
n
i=1
(yi −µ)2
2σ2


= exp

−µ2
2

exp

−
n
i=1
[(yi −5µ) + (5µ −µ)]2
2σ2

,
(11.46)
where 5µ = n
i=1 yi/n. Expanding the square in the second term and noting
that

i
(yi −5µ) (5µ −µ) = (5µ −µ)

i
(yi −5µ) = 0,
expression (11.46) reduces to
p

µ|σ2, y

∝exp

−σ2µ2 + n (µ −5µ)2
2σ2

.
(11.47)
Using the identity (i.e., Box and Tiao, 1973, page 74):
A (z −a)2 + B (z −b)2 = (A + B) (z −c)2 +
AB
A + B (a −b)2
with c = (Aa + Bb) /(A+B), and associating σ2 with A, n with B, µ with
z, 0 with a and 5µ with b, then (11.47) can be rewritten as
p

µ|σ2, y

∝exp

−

σ2 + n

(µ −m)2
2σ2

,
(11.48)

11.5 The Gibbs Sampler
515
where m = n5µ/

σ2 + n

. By inspection, (11.48) is recognized as the ker-
nel of the density of a normal distribution with mean m and variance
σ2/

σ2 + n

. Therefore,
µ|σ2, y ∼N

n5µ
(σ2 + n),
σ2
(σ2 + n)

.
(11.49)
In order to derive p

σ2|µ, y

, terms including σ2 are extracted from the
joint posterior density (11.45); this leads to
p

σ2|µ, y

∝

σ2−( v
2 +1) exp

−vS
2σ2
 
σ2−n
2 exp

−
n
i=1
(yi −µ)2
2σ2


=

σ2−( v
2 +1) exp

−,v ,S
2σ2

,
where ,S = (vS + n
i=1 (yi −µ)2)/,v, and ,v = v + n. This is the kernel of
the density of a scaled inverted chi-square distribution with parameters ,v
and ,S. In brief,
σ2|µ, y ∼,v ,Sχ−2
v .
(11.50)
Generation of a sample from (11.50) requires drawing a chi-square devi-
ate with ,v degrees of freedom, inverting this value, and multiplying it by

vS + n
i=1 (yi −µ)2
. One cycle of the Gibbs sampling algorithm consists
of drawing from (11.49), computing the quantities

vS + n
i=1 (yi −µ)2
using the realized value in place of µ, and ﬁnally drawing from (11.50). At
convergence, µ(i) and σ2(i) are elements of the ith sample from the marginal
distributions [µ|y] and

σ2|y

, respectively.
■
Example 11.3
A bivariate normal model with unknown covariance matrix
Let yi = (yi1, yi2)′ , (i = 1, . . . , n), represent n independent samples from
the bivariate normal distribution with mean vector µ = (µ1, µ2) and vari-
ance deﬁned by the 2 × 2 matrix V. That is,
p (y|µ, V) = |2πV|−n
2 exp

−1
2
n

i=1
(yi −µ)′ V−1 (yi −µ)

= (2π)−n |V|−n
2 exp

−1
2tr

V−1S

,
(11.51)
where
S =



i
(yi1 −µ1)2

i
(yi1 −µ1) (yi2 −µ2)

i
(yi1 −µ1) (yi2 −µ2)

i
(yi2 −µ2)2

.

516
11. Markov Chain Monte Carlo
Take a two-dimensional uniform distribution as prior for µ, and let the
prior for V be the scaled inverted Wishart distribution with density
p (V|V0, v) ∝|V|−1
2 (v+3) exp

−1
2tr

V−1V−1
0

,
where V0 and v are hyperparameters assumed known. The joint posterior
density of the parameters of this model (suppressing the dependence on V0
and v in the notation) is
p (µ, V|y) ∝p (y|µ, V) p (V)
= (2π)−n |V|−n
2 exp

−1
2tr

V−1S

|V|−1
2 (v+3)
× exp

−1
2tr

V−1V−1
0

.
(11.52)
Extracting the terms in µ from (11.52) yields as conditional posterior den-
sity
p (µ|V, y) ∝exp

−1
2
n

i=1
(yi −µ)′ V−1 (yi −µ)

= exp

−1
2
n

i=1
[(yi −y) + (y −µ)]′ V−1 [(yi −y) + (y −µ)]

∝exp

−1
2
n

i=1
(y −µ)′ V−1 (y −µ)

= exp

−1
2n (y −µ)′ V−1 (y −µ)

,
where y′ = (y1, y2) =

n−1 n
i=1 yi1, n−1 n
i=1 yi2

. Therefore, from in-
spection, the fully conditional posterior distribution of the vector µ is
µ|V, y ∼N

y, n−1V

.
(11.53)
The fully conditional distribution of the covariance matrix is derived from
(11.52) as well. The terms in V are
p (V|µ, y) ∝|V|−n
2 exp

−1
2tr

V−1S

|V|−1
2 (v+3)
× exp

−1
2tr

V−1V−1
0

= |V|−1
2 (v+3+n) exp

−1
2tr

V−1 
S + V−1
0

,

11.6 Langevin–Hastings Algorithm
517
which is the kernel of a scaled inverted Wishart distribution with parame-
ters v + n and

S + V−1
0
−1. Thus,
V|µ, y ∼IW

S + V−1
0
−1 , v + n

.
(11.54)
■
11.6
Langevin–Hastings Algorithm
A general algorithm that, in principle, allows joint updates for the complete
parameter vector of a model (ϕ, say, of length r), is based on the Langevin–
Hastings algorithm (Besag, 1994). The idea is to generate a proposal vector
ϕ(t+1) at cycle t + 1, from a candidate generating density deﬁned by the
normal process
N

ϕ(t)+γ
2
∂
∂ϕ(t) ln p

ϕ(t)|y

, Iγ

,
(11.55)
where γ is a scalar tuning parameter, I is the r × r identity matrix and
ln p (ϕ|y) is the log-posterior density, usually known up to proportional-
ity. Note that the vector ϕ includes all the parameters of the model. For
example in a Gaussian process, this includes location and dispersion pa-
rameters. The proposal is then accepted using the Metropolis–Hastings
acceptance ratio (11.27). The use of the gradient of the log-target density
in the proposal distribution can lead to much better convergence properties
than, for example, the simple random walk Metropolis–Hastings proposal
kernel N

ϕ(t), Iγ

. The gradient in (11.55) is supposed to improve moves
toward the mode of p (ϕ|y). Further developments and improvements of
this approach were presented by Stramer and Tweedie (1998).
11.7
Reversible Jump MCMC
One motivation for reversible jump MCMC is to provide a more general
recipe than Metropolis–Hastings to simulate from posterior distributions
on spaces of varying dimension; these arise naturally when the number of
parameters of the object of inference is not ﬁxed. An example is inferences
concerning the number of quantitative trait loci (QTL) aﬀecting a trait,
using genetic markers. This number can be treated as a random variable.
The Markov chain is allowed to jump across states of diﬀerent dimension,
and each state is characterized by a particular number of QTL. The distri-
bution of the proportion of times that the chain has spent across the various
states is a Monte Carlo estimate of the marginal posterior distribution of

518
11. Markov Chain Monte Carlo
the number of QTL aﬀecting the trait. This is illustrated in Chapter 16.
Another classical example is the number of densities appearing in a mixture
distribution, a problem studied by Richardson and Green (1997).
Reversible jump though, is not limited to simulation of invariant dis-
tributions that have densities on spaces of diﬀerent dimensions. The algo-
rithm can also be used when the models posed have the same number of
parameters. An example of this situation is given at the end of this chap-
ter involving a gamma and a lognormal model. Of course, if the number
of competing models is small, other approaches may be computationally
more eﬃcient than reversible jump.
The reversible jump algorithm was introduced by Green (1995), who il-
lustrates possible applications of reversible jump with several examples.
The paper is rather technical and a reader unfamiliar with measure theory
will ﬁnd it diﬃcult to fully grasp the details. Waagepetersen and Sorensen
(2001) present a simple, self-contained derivation of reversible jump in a
tutorial style and avoiding measure theoretical details. This section is based
on the latter paper. The derivation of the acceptance probability to moves
between spaces of possibly diﬀerent dimension presented here, follows es-
sentially the same steps as for the Metropolis–Hastings acceptance proba-
bility: using the reversibility condition as the point of departure, ﬁrst, the
transition kernel is expressed in terms of a proposal distribution and an ac-
ceptance probability. Second, a change of variable is performed that allows
both sides of the detailed balance equation to be expressed in terms of the
same parameters. The change of variable is made possible by a monotone
transformation; in the case of the reversible jump, this requires that the
dimension matching condition is fulﬁlled, as explained below. Identifying
the conditions for equality between the probability of opposite moves and,
thus, for detailed balance to hold, leads to the ﬁnal step in the derivation
of the acceptance probability.
Below, a step-by-step derivation of the reversible jump acceptance prob-
ability is presented. The notation is a little involved since there is a need
to account for the fact that parameters change as the Markov chain jumps
from one model to the next. Two examples are presented at the end of
this chapter and a further application of reversible jump can be found in
Chapter 16 on QTL analysis. In one of the examples, two models with a
diﬀerent number of parameters are considered. In the other, the number
of parameters is the same in both models, illustrating the generality of the
algorithm for model choice.
11.7.1
The Invariant Distribution
Here π denotes the joint probability distribution of (M, Z), where M ∈
{1, 2, . . . , I} is a “model indicator” and Z is a real stochastic vector, pos-
sibly of varying dimension (I represents either a ﬁnite integer or ∞). The
models can be diﬀerent because of their parametric form but the number

11.7 Reversible Jump MCMC
519
of parameters do not need to diﬀer. The vector Z takes values in the set
C deﬁned as the union C = ∪I
m=1Cm of spaces Cm = Rnm, (nm > 1).
Given M = m, Z can only take values in Cm, so that π is speciﬁed by
pm = P (M = m) and densities f (·|M = m) on Cm, (m = 1, 2, . . .). Thus,
for Am ⊆Cm, the joint probability distribution of (M, Z) is
P (M = m, Z ∈Am)
=
P (M = m) P (Z ∈Am|M = m)
=
pm

Am
f (z|M = m) dz.
(11.56)
The density f (·|M = m) is denoted fm hereinafter.
If a number of competing models are posed, then pm may represent the
posterior probability of model m, and given M = m, fm is the poste-
rior density of the nm-dimensional vector Z of parameters associated with
model m. In this case
pmfm = c−1,pmh (z|m) l (y|m, z) ,
(11.57)
where ,pm is the prior probability of model m, h (z|m) is the prior density
of Z given M = m, l (y|m, z) is the likelihood of the data y given (M, Z) =
(m, z), and c is the overall (typically unknown) normalizing constant
c =
I

m=1
,pm

Cm
h (z|m) l (y|m, z) dz.
(11.58)
11.7.2
Generating the Proposal
In the joint updating Metropolis–Hastings algorithm, the candidate point
Yn+1 ∈Rd for the new state Xn+1 ∈Rd, is generated from the d-dimen-
sional proposal density q (x, ·). In the single-site updating, the d-dimen-
sional candidate point
Yn+1 = (x1, x2, . . . , xi−1, Zi, xi+1, . . . , xd)
is generated by drawing the random variable Zi from the one-dimensional
proposal density qi (x, ·). With some abuse of notation, Yn+1 above can be
written
Yn+1 = g (x1, x2, . . . , xi−1, Zi, xi+1, . . . , xd) ,
where the function g is the identity mapping.
In the context of reversible jump, each state Xi of the chain contains
two components, i.e., Xi = (Mi, Zi), where Mi is the model indicator and
where Zi is a stochastic vector in CMi. Suppose that (m, z) is the value of
the current state Xn of the Markov chain and a move to the value (m′, z′)
is considered for the next state Xn+1. A proposal Yn+1 =

Y ind
n+1, Y par
n+1

for
Xn+1 is generated as described below, where superscripts ind and par are

520
11. Markov Chain Monte Carlo
labels for the proposal of the model indicator Mn+1 and for the vector Zn+1,
respectively. With user-deﬁned probability pmm′,
I
m′=1 pmm′ = 1

, the
proposal Y ind
n+1 is set equal to m′, and given Y ind
n+1 = m′, the proposal Y par
n+1
is generated in Cm′. A very general mechanism is to construct the proposal
Y par
n+1 by applying a deterministic mapping g1mm′ to the previous value z
and to a random component U. This mechanism can be formulated as
Y par
n+1 = g1mm′ (z, U) ,
(11.59)
where U is a random vector which has density qmm′ (z, ·) on Rnmm′. The
proposal Yn+1 is ﬁnally accepted with an acceptance probability
amm′ 
z, Y par
n+1

,
which is derived below.
When considering a move from a state (m, z) to
(m′, z′) = (m′, g1mm′ (z, u)) ,
and a move in the opposite direction from (m′, z′) to
(m, z) = (m, g1m′m (z′, u′)) ,
the vectors (z, u) and (z′, u′) must be of equal dimension. That is, the
dimension matching condition
nm + nmm′ = nm′ + nm′m
(11.60)
needs to be fulﬁlled. Further, it will be assumed that there exist functions
g2mm′ and g2m′m such that the mapping gmm′, given by
(z′, u′) = gmm′ (z, u) = (g1mm′ (z, u) , g2mm′ (z, u)) ,
(11.61)
is one-to-one with
(z, u) = g−1
mm′ (z′, u′)
= gm′m (z′, u′) = (g1m′m (z′, u′) , g2m′m (z′, u′))
(11.62)
and that gmm′ is diﬀerentiable. The transformations (11.61) and (11.62) are
possible because the mapping gmm′ is one-to-one with gm′m; a necessary
condition for the existence of the one-to-one mapping is that the dimension
matching (11.60) holds.
11.7.3
Specifying the Reversibility Condition
Assuming Xn = (Mn, Zn) ∼π, the condition of reversibility is
P (Mn = m, Zn ∈Am, Mn+1 = m′, Zn+1 ∈Bm′)
= P (Mn = m′, Zn ∈Bm′, Mn+1 = m, Zn+1 ∈Am)
(11.63)

11.7 Reversible Jump MCMC
521
for all m, m′ ∈{1, 2, . . . , I}, and all subsets Am and Bm′ of Cm and Cm′,
respectively. In analogy with (11.15), the left-hand side of (11.63) is
P (Mn = m, Zn ∈Am, Mn+1 = m′, Zn+1 ∈Bm′)
=

Am
P (Mn+1 = m′, Zn+1 ∈Bm′|Xn = (m, z)) pmfm (z) dz,
(11.64)
where P (Mn+1 = m′, Zn+1 ∈Bm′|Xn = (m, z)) is the transition kernel. As
in (11.17), let
Qa
mm′ (z, Bm′)
= P

Y ind
n+1 = m′, Y par
n+1 ∈Bm′ and Yn+1 is accepted|Xn = (m, z)

be the joint probability of generating the proposal Yn+1 with Y ind
n+1 = m′
and Y par
n+1 in Bm′ and accepting the proposal, given that the current state
of the Markov chain is Xn = (m, z). Also, as in (11.18), let
sm (z) = P (Yn+1 is rejected|Xn = (m, z))
be the probability of rejecting the proposal. Then the transition kernel can
be written as
P (Mn+1 = m′, Zn+1 ∈Bm′|Xn = (m, z))
=
Qa
mm′ (z, Bm′) + sm (z) I (m = m′, z ∈Bm′) .
Substituting in (11.64), the left-hand side of (11.63) equals
pm

Am
Qa
mm′ (z, Bm′) fm (z) dz
+ pm

Am
sm (z) I (m = m′, z ∈Bm′) fm (z) dz
= pm

Am
Qa
mm′ (z, Bm′) fm (z) dz
+ pm

sm (z) I (m = m′, z ∈Am ∩Bm′) fm (z) dz,
(11.65)
where
pm

Am
Qa
mm′ (z, Bm′) fm (z) dz
= P

Mn = m, Zn ∈Am, Y ind
n+1 = m′, Y par
n+1 ∈Bm′, Yn+1 is accepted

.
By symmetry, the right-hand side of (11.63) equals
pm′

Bm′
Qa
m′m (z′, Am) fm′ (z′) dz′
+pm′

sm′ (z′) I (m = m′, z′ ∈Bm′ ∩Am) fm′ (z′) dz′.
(11.66)

522
11. Markov Chain Monte Carlo
The second terms in (11.65) and (11.66) are equal both in the case when
m ̸= m′ (in which case they are zero, because the indicator function takes
the value zero), and when m = m′ (in which case the move is within
the same model, and both expressions are identical). Therefore a suﬃcient
condition for reversibility to hold is, for all m and m′,
pm

Am
Qa
mm′ (z, Bm′) fm (z) dz
= pm′

Bm′
Qa
m′m (z′, Am) fm′ (z′) dz′.
(11.67)
11.7.4
Derivation of the Acceptance Probability
Equation (11.67) is now written more explicitly. Since,
(a) Y ind
n+1 is set equal to m′ with probability pmm′;
(b) Y par
n+1 is generated in Cm′ and belongs in Bm′ implies Y par
n+1 ∈Bm′ ⇔
g1mm′ (z, U) = z′ ∈Bm′;
(c) Yn+1 is accepted with probability amm′ (z, g1mm′ (z, U)) = amm′ (z, z′),
and U ∼qmm′ (z, ·).
It follows that
Qa
mm′ (z, Bm′) = pmm′

I (z′ ∈Bm′) amm′ (z, z′) qmm′ (z, u) du. (11.68)
The left-hand side of (11.67) is therefore
pm

Am
Qa
mm′ (z, Bm′) fm (z) dz
= pm

Am

I (z′ ∈Bm′) pmm′amm′ (z, z′) qmm′ (z, u) fm (z) dz du
= pm
 
I (z ∈Am, z′ ∈Bm′) pmm′
×amm′ (z, z′) qmm′ (z, u) fm (z) dz du,
(11.69)
and, by symmetry, the right-hand side is
pm′

Bm′
Qa
m′m (z′, Am) fm′ (z′) dz′
= pm′
 
I (z′ ∈Bm′, z ∈Am) pm′m
×am′m (z′, z) qm′m (z′, u′) fm′ (z′) dz′ du′.
(11.70)
To study the conditions that satisfy reversibility and therefore equality of
(11.69) and (11.70), both equations will now be expressed as functions of the
same variables. This is possible due to the dimension matching assumption

11.7 Reversible Jump MCMC
523
and relationships (11.61) and (11.62). Using the fact that (see equations
(2.36) and (2.38) of Chapter 2)
dz′du′ = |det (g′
mm′ (z, u))| dz du,
(11.71)
where
g′
mm′ (z, u) = ∂gmm′ (z, u)
∂(z, u)
=


∂g1mm′(z,u)
∂dz
∂g2mm′(z,u)
∂z
∂g1mm′(z,u)
∂du
∂g2mm′(z,u)
∂u

,
equation (11.70) can be written as
P (Mn = m′, Zn ∈Bm′, Mn+1 = m, Zn+1 ∈Am)
=
 
I (z′ ∈Bm′, z ∈Am) pm′mam′m (z′, z)
×qm′m (z′, u′) pm′fm′ (z′) |det (g′
mm′ (z, u))| dz du.
(11.72)
By inspection, it is clear that equality between (11.69) and (11.72) is sat-
isﬁed if
pmm′amm′ (z, z′) qmm′ (z, u) pmfm (z)
= pm′mam′m (z′, z) qm′m (z′, u′) pm′fm′ (z′) |det (g′
mm′ (z, u))| .
Choosing the acceptance probability as large as possible, subject to the
detailed balance condition as suggested by Peskun (1973), yields
amm′ (z, z′)
= min

1, pm′mqm′m (z′, u′) pm′fm′ (z′)
pmm′qmm′ (z, u) pmfm (z)
""""det

∂gmm′ (z, u)
∂(z, u)
""""

(11.73)
whenever pmm′qmm′ (z, u) pmfm (z) > 0 and where
(z′, u′) = gmm′ (z, u) .
In practice, pmm′qmm′ (z, u) pmfm (z) = 0 only happens if the Markov chain
is initialized in a state (m, z) for which pmfm (z) = 0. The acceptance
probability for a move from z′ to z is given by the inverse of (11.73).
11.7.5
Deterministic Proposals
Sometimes it may be simpler to apply deterministic proposals for a move
from Cm to Cm′, i.e., to let Yn+1 = g1mm′ (z), and still use a stochastic
proposal for the move in the opposite direction. In this case, the dimension
matching condition equals
nm = nm′ + nm′m
(11.74)

524
11. Markov Chain Monte Carlo
since nmm′ = 0. Equations (11.61) and (11.62) become
(z′, u′) = gmm′ (z) = (g1mm′ (z) , g2mm′ (z))
(11.75)
and
(z) = g−1
mm′ (z′, u′) = gm′m (z′, u′) = g1m′m (z′, u′) .
(11.76)
That is, the change from state z to state z′, deﬁned by (11.75), does not
involve the generation of a stochastic variable U; the move is deterministic.
The move in the opposite direction, deﬁned by (11.76), requires U ′; this
move is stochastic. The reversibility condition has the same form as in
(11.67), but (11.68) is now given by
Qa
mm′ (z, Bm′)
=
pmm′I (g1mm′ (z) ∈Bm′) amm′ (z, g1mm′ (z))
=
pmm′I (z′ ∈Bm′) amm′ (z, z′) .
(11.77)
Substituting (11.77) in the left-hand side of (11.67):
pm

Am
Qa
mm′ (z, Bm′) fm (z) dz
=

I (z ∈Am, z′ ∈Bm′) pmm′
×amm′ (z, z′) pmfm (z) dz.
(11.78)
With a stochastic proposal for the opposite move, the right-hand side of
(11.67) is unchanged and is given by (11.70). The equivalent to (11.71) is
now
dz′ du′ = |det (g′
mm′ (z))| dz,
(11.79)
where
g′
mm′ (z) = ∂gmm′ (z)
∂z
.
Substituting (11.79) in (11.70), and using (11.78), yields the following ex-
pression for the detailed balance equation

I (z ∈Am, z′ ∈Bm′) pmm′amm′ (z, z′) pmfm (z) dz
=

I (z′ ∈Bm′, z ∈Am) pm′mam′m (z′, z)
×qm′m (z′, u′) pm′fm′ (z′) |det (g′
mm′ (z))| dz,
(11.80)
where u′ = g2mm′ (z), a function of z. Using the same approach as before,
the acceptance probability is given now by
a (z, z′) = min

1, pm′mqm′m (z′, u′) pm′fm′ (z′)
pmm′pmfm (z)
|det (g′
mm′ (z))|

.
(11.81)

11.7 Reversible Jump MCMC
525
11.7.6
Generating Proposals via the Identity Mapping
In the development presented so far, the proposal in the move from (m, z) to
(m′, z′) is generated via the deterministic mapping (11.16). An alternative
to this approach is to let g1mm′ (z, U) be the identity mapping and to set
U = Z′, which results in
Y par
n+1 = Z′.
The random variable Z′ is generated from the density qmm′ (z, ·) on Rnm′,
which may depend on the current value z. The expression equivalent to
(11.68) is now
Qa
mm′ (z, Bm′) = pmm′

I (z′ ∈Bm′) amm′ (z, z′) qmm′ (z, z′) dz′. (11.82)
Then it is easy to show that under this strategy, the acceptance probability
is given by
amm′ (z, z′) = min

1, pm′mqm′m (z′, z) pm′fm′ (z′)
pmm′qmm′ (z, z′) pmfm (z)

,
(11.83)
which does not include a Jacobian term because of the use of the identity
mapping.
This strategy (which we label the FF strategy) was suggested by S.
Fern´andez and R. Fernando (Rohan Fernando (2001), personal commu-
nication). The form of (11.83) is similar to (11.13); however, in (11.83),
there is an extra term pm′m/ pmm′ and, further, qmm′ and qm′m are densi-
ties on Rnm′ and on Rnm, respectively. In (11.13), q (x, y) and q (y, x) are
both densities on Rd where d is the dimension of Y and X.
Example 11.4
Comparing diﬀerences between two treatments
The reversible jump algorithm is showed in detail with a trivial example.
Consider a model (M = 1) where the data are assumed to be an i.i.d.
realization from
yi|M = 1, t, σ2 ∼N

t, σ2
,
i = 1, . . . , N,
(11.84)
or from the alternative model (M = 2)
yij|M = 2, ti, σ2 ∼N

ti, σ2
,
i = 1, 2, j = 1, . . . , n,
(11.85)
where N = 2n. In (11.84), t is an overall mean and σ2 is the variance of
the distribution

yi|M = 1, t, σ2
. The sampling model (11.85) postulates
instead that there are two “treatments” t1 and t2, and that observations
have variance σ2. To complete the Bayesian structure, prior distributions
Pr (M = 1) p

t, σ2|M = 1

and
Pr (M = 2) p

t1, t2, σ2|M = 2


526
11. Markov Chain Monte Carlo
are assigned to the parameters of both sampling models. The problem
consists of discriminating between these two models. This is done here using
reversible jump MCMC, despite the fact that by an appropriate choice of
prior distributions, closed forms for the relevant posterior distributions and
for the Bayes factor for these models are available (see e.g., O’Hagan, 1994).
It is convenient to introduce the stochastic variable T:
T =

t,
M = 1,
(t1, t2) ,
M = 2.
The posterior distribution, which has the form in (11.56), can then be
written
p

M = i, T, σ2|y

∝Pr (M = i) p

T, σ2|M = i

p

y|M = i, T, σ2
.
(11.86)
First, reversible jump is implemented using stochastic proposals in both
directions. Assume that the current state of the Markov chain is (m = 1, z)
and a move to (m′ = 2, z′), where z =

t, σ2
and z′ =

t1, t2, σ2
, is
proposed with probability pmm′. This probability is chosen by the user,
subject to p11 + p12 = 1. With stochastic proposals in both directions, the
dimension-matching condition (11.60) is satisﬁed if two stochastic variables
(u = v1, v2) are generated in the move from m to m′, and one stochastic
variable (u′ = v) is generated in the move from m′ to m. In this case,
nm = 2 (associated with t, σ2), nmm′ = 2 (associated with v1, v2), nm′ = 3
(associated with t1, t2, σ2), and nm′m = 1 (associated with v). The mapping
is
(z′, u′)
=

t1, t2, σ2, v

=
gmm′ (z, u)
=
(g1mm′ (z, u) , g2mm′ (z, u)) .
A reasonable choice could be


t1
t2
σ2
v

=


1
0
1
0
1
0
0
1
0
1
0
0
0
0
1
2
1
2




t
σ2
v1
v2

.
That is, g1mm′ (z, u) =

t1, t2, σ2
=

t + v1, t + v2, σ2
and
g2mm′ (z, u) = v = 1
2 (v1 + v2) .
The absolute value of the Jacobian of the transformation in the move from
m to m′ is
""""det
∂gmm′ (z, u)
∂(z, u)
"""" =
"""""det

∂

t + v1, t + v2, σ2, 1
2 (v1 + v2)

∂(t, σ2, v1, v2)
""""" = 1.

11.7 Reversible Jump MCMC
527
If v is generated from qv (z, ·) and v1, v2 from qv1v2 (z′, ·) , the expression
for the acceptance probability (11.73) is
amm′ (z, z′)
= min

1,
pm′mqv (z, v) pm′fm′
pmm′qv1v2 (z′, v1, v2) pmfm

,
where, in terms of (11.73), the posterior distribution of

M, T, σ2
is
pm′fm′ = c−1 Pr (M = 2) p

t1, t2, σ2|M = 2

p

y|M = 2, t1, t2, σ2
(11.87)
and
pmfm = c−1 Pr (M = 1) p

t, σ2|M = 1

p

y|M = 1, t, σ2
.
(11.88)
Notice that the constant c−1 cancels in the acceptance ratio.
Second, reversible jump is implemented using a stochastic proposal in one
of the moves and a deterministic proposal in the other. As before, consider
the move from m = 1 to m′ = 2. Now the dimension-matching condition is
nm + nmm′ = nm′
because nm′m = 0. For the present example, nm = 2 (associated with
t, σ2), nmm′ = 1 (associated with u), and nm′ = 3 (associated with t1, t2,
σ2). The move from m to m′ is based on a stochastic proposal, since it
requires the generation of the random variable U from q (z, ·). The mapping
is
z′
=

t1, t2, σ2
=
gmm′ (z, u)
=
g1mm′ (z, u) .
A reasonable choice is


t1
t2
σ2

=


1
1
0
1
−1
0
0
0
1




t
u
σ2

.
(11.89)
That is, g1mm′ (z, u) =

t1, t2, σ2
=

t + u, t −u, σ2
. The absolute value
of the Jacobian of the transformation is
""""det

∂gmm′ (z, u)
∂(z, u)
"""" =
"""""det

∂

t + u, t −u, σ2
∂(t, σ2, u)
""""" = 2
and the acceptance probability for the jump from m to m′ is
amm′ (z, z′) = min

1,
pm′mpm′fm′
pmm′q (z, u) pmfm
2

,

528
11. Markov Chain Monte Carlo
where pm′fm′ is equal to (11.87) and pmfm is equal to (11.88).
The move from m′ = 3 to m = 2 is deterministic. Inverting (11.89) yields


t
u
σ2

=


1
2
1
2
0
1
2
−1
2
0
0
0
1




t1
t2
σ2

.
That is,
(z, u)
=

t, σ2, u

=
gm′m (z)
=
(g1m′m (z) , g2m′m (z)) ,
where
g1m′m (z) =

t, σ2
=

1
2 (t1 + t2) , σ2

and
g2m′m (z) = u = 1
2 (t1 −t2) .
The absolute value of the Jacobian of the transformation is
""""det

∂gm′m (z)
∂z
"""" =
"""""det

∂
 1
2 (t1 + t2) , σ2, 1
2 (t1 −t2)

∂(t1, t2, σ2)
""""" = 1
2.
The acceptance probability for the jump from m′ to m is given by
am′m (z′, z) = min

1, pmm′q (z, u) pmfm
pm′mpm′fm′
1
2

,
which is equal to the inverse of amm′ (z, z′), as expected.
Finally, the problem is approached via the FF strategy. Consider the same
move from m = 1 to m′ = 2 with now, z = t and the update including
z′ = (t1, t2) only, as σ2 is common to Models 1 and 2. Then the acceptance
probability is simply
amm′ (z, z′) = min

1,
pm′mqm′m (t) pm′fm′
pmm′qmm′ (t1, t2) pmfm

,
where qm′m (·) is a density on R (e.g., a normal density with mean and
variance (t1 + t2) /2 and σ2, respectively) and qmm′ is a density on R2 (e.g.,
a bivariate normal density with mean (t, t) and well-tuned covariance).
Extension to an unknown number of covariates (treatments) is obvious,
and requires incorporating a prior distribution to this number. In this case,
reversible jump oﬀers a recipe for computing the posterior probability for
the number of covariates in the regression model. That is, the number of
covariates is treated as a random variable, to be inferred from the data at
hand.
■

11.7 Reversible Jump MCMC
529
Example 11.5
Choosing between a gamma and a lognormal model
In this example, reversible jump is applied to obtain a MCMC-based pos-
terior probability of two models with the same number of parameters, the
gamma and the lognormal model. Such a comparison cannot be performed
via the traditional Neyman–Pearson maximum likelihood ratio test de-
scribed in Chapter 4. This is so because these models do not generate the
required nested structure. In order to perform the test within the frequen-
tist paradigm a modiﬁcation of the Neyman–Pearson maximum likelihood
ratio test is required (Cox, 1961, 1962).
A gamma distributed random variable has p.d.f.
g1 (y|α, β) =
1
Γ (α) βα yα−1 exp [−y /β ] ,
0 < y < ∞, α > 0, β > 0.
The ﬁrst two moments of this distribution are
E (y)
=
αβ,
E

y2
=
β2α (α + 1) .
A lognormally distributed random variable has p.d.f.
g2

y|µ, σ2
=
1
y
√
2πσ exp

−1
2σ2 (ln y −µ)2

,
0
<
y < ∞, −∞< µ < ∞, σ > 0.
The ﬁrst two moments of this distribution are
E (y)
=
exp

µ + σ2
2

,
E

y2
=
exp

2µ + 2σ2
.
Consider Model 1 as the gamma model, and Model 2 as the lognormal
model. Deﬁne the stochastic indicator M ∈{1, 2} for Models 1 and 2
respectively.
Let the posterior probability associated with Model 1 be
f1(α, β, M = 1|y) ∝g1(y|α, β, M = 1)h1(α, β|M = 1) Pr (M = 1) ,
(11.90)
where g1 is the gamma density, h1 is a prior for the gamma density param-
eters α and β, and Pr (M = 1) is the a priori probability of Model 1. Also,
let the posterior probability associated with Model 2 be
f2(µ, σ2, M = 2|y) ∝g2(y|µ, σ2, M = 2)h2(µ, σ2|M = 2) Pr (M = 2) ,
(11.91)
where g2 is the lognormal density, h2 is a prior for the lognormal density
parameters µ and σ2, and Pr (M = 2) is the a priori probability of Model

530
11. Markov Chain Monte Carlo
2. Suppose that the current state of the Markov chain is (m, z), m = 1,
z = (α, β), and that a move is to be made to the lognormal model, i.e.,
to a state (m′, z′), m′ = 2, z′ =

µ, σ2
. One way to propose values for
the parameters µ and σ2 might be to equate the ﬁrst- and second-order
moments under the current gamma model and the proposed lognormal
model and, subsequently, add/multiply some noise, U. More precisely, solve
exp(˜µ+ ˜σ2/2) = αβ and exp(2˜µ+2˜σ2) = β2α(α+1) with respect to ˜µ and
˜σ2, and let the proposals be µ = ˜µ+U1 and σ2 = ˜σ2U2, where U = (U1, U2)
is generated from qmm′. In this case, we have
µ
=
ln

αβ
>
1 + 1/α

+ U1,
σ2
=
ln (1 + 1/α) U2,
(11.92)
U ′
1
=
U1,
U ′
2
=
U2.
That is,
(z, u) = (α, β, u1, u2)
and
(z′, U ′) =

µ, σ2, U ′
1, U ′
2

= gmm′(α, β, U1, U2),
= g1mm′ (α, β, U1, U2) , g2mm′ (α, β, U1, U2) ,
=

log(αβ/
>
1 + 1/α) + U1, log(1 + 1/α)U2

, (U1, U2) ,
where
g1mm′ (α, β, U1, U2) =

µ, σ2
=

log(αβ/
>
1 + 1/α) + U1, log(1 + 1/α)U2

,
g2mm′ (α, β, U1, U2) = (U1, U2) .
This move requires generating the stochastic vector U = (U1, U2) from
qmm′.
In the opposite move, from (m′, z′) to (m, z), solving (11.92) for α, β, U1,
and U2 yields
α
=
1/(exp(σ2/U ′
2) −1,
β
=
exp(µ −U ′
1 + σ2/(2U ′
2))(exp(σ2/U ′
2) −1),
U1
=
U ′
1,
U2
=
U ′
2.

11.7 Reversible Jump MCMC
531
That is,
(z, U) = (α, β, U1, U2)
= gm′m(µ, σ2, U ′
1, U ′
2)
= g1m′m

µ, σ2, U ′
1, U ′
2

, g2m′m

µ, σ2, U ′
1, U ′
2

=

1/(exp(σ2/U ′
2) −1), exp(µ −U ′
1 + σ2/(2U ′
2))(exp(σ2/U ′
2) −1)

,
(U ′
1, U ′
2) ,
where U ′
1 and U ′
2 are generated from qm′m and
g1m′m

µ, σ2, U ′
1, U ′
2

=

1/(exp(σ2/U ′
2) −1), exp(µ −U ′
1 + σ2/(2U ′
2))(exp(σ2/U ′
2) −1)

,
g2m′m

µ, σ2, U ′
1, U ′
2

= (U ′
1, U ′
2) .
The moves in both directions use stochastic proposals qmm′ and qm′m.
The absolute value of the Jacobian of the transformation in the move from
(m, z) to (m′, z′), is
""""
∂gmm′ (z, u)
∂(z, u)
"""" =
""""""
∂

log(αβ/
>
1 + 1/α) + u1, log(1 + 1/α)u2, u1, u2

∂(α, β, u1, u2)
""""""
= u2 [αβ (α + 1)]−1 .
Finally, the acceptance probability for the move from (m, z) to (m′, z′) is
a (z, z′) = min

1, f2(µ, σ2, M = 2|y)pm′mqm′m
f1(α, β, M = 1|y)pmm′qmm′ u2 [αβ (α + 1)]−1

,
where the posterior distributions f1 and f2 are given by (11.90) and (11.91),
respectively.
The move from the gamma to the lognormal model and the reverse move,
were chosen here to be stochastic. In this example where the number of
parameters is the same in both models, one could have chosen deterministic
moves in both directions. In this case, the acceptance probability of moving
from z to z′ would be given by
a (z, z′) = min

1, pm′mpm′fm′ (z′)
pmm′pmfm (z) |det (g′
mm′ (z))|

.
(11.93)
In the example above, the Jacobian is equal to
""""
∂gmm′ (z)
∂z
""""
=
""""""
∂

log(αβ/
>
1 + 1/α), log(1 + 1/α)

∂(α, β)
""""""
=
[αβ (α + 1)]−1 ,

532
11. Markov Chain Monte Carlo
and the acceptance probability of moving from the gamma to the lognormal
model is
a (z, z′) = min

1, f2(µ, σ2, M = 2|y)pm′m
f1(α, β, M = 1|y)pmm′ [αβ (α + 1)]−1

.
In terms of the rate of convergence and degree of autocorrelation of the
Markov chain, it is diﬃcult a priori to determine which of the two ap-
proaches is to be preferred.
This example illustrates that the Jacobian is not an inherent component of
dimension-changing MCMC. The Jacobian arises due to the deterministic
transformation used in the proposal mechanism, and the change of variable
used when equating (11.69) and (11.70).
■
As a concluding remark on the topic, we wish to draw attention to poten-
tial diﬃculties in successful implementation of the algorithm in highly di-
mensional problems. The competing models under consideration may have
diﬀerent sets of parameters and the reversible jump machinery simply pro-
vides no guidance to generate eﬀective jump proposals. A satisfactory rate
of transdimensional jumping may require very delicate tuning. A discussion
on this topic can be found in Brooks et al. (2001).
11.8
Data Augmentation
We conclude this chapter with a topic that is particularly relevant in the
implementation of MCMC. Imagine that there is interest in obtaining the
posterior distribution of a parameter θ. Due to analytical intractability, one
chooses to approximate p (θ|y) using MCMC. Often, the fully conditional
posterior distributions p (θi|θ−i, y) do not have a standard form and the
MCMC algorithm can be diﬃcult to implement. The idea of data augmen-
tation is to augment with the so-called latent data or missing data ϕ, in
order to exploit the simplicity of the resulting conditional posterior distri-
butions p (θi|θ−i, ϕ, y). This is in the same spirit as in the EM algorithm:
by increasing the dimensionality of the problem, possibly at the expense
of extra computing time, although this is not always the case (Swendsen
and Wang, 1987), the problem is simpliﬁed algorithmically. Notice that the
focus of inference is
p (θ|y) =

p (θ|ϕ, y) p (ϕ|y) dϕ,
and this marginalization is carried out via MCMC. A key paper is Tanner
and Wong (1987).
Example 11.6
Inference from truncated data
Consider a data set (the observed data) consisting of No independent obser-

11.8 Data Augmentation
533
vations from a truncated normal distribution, where the truncation point
T is known. Out of a total of NT o original observations, each one of the No
is kept because its value is larger than T. It is also known that there are
Nm = NT o −No missing observations and the only information available
on these is that they are i.i.d., that sampling was at random, and that each
one is smaller than or equal to T.
The NT o original observations are assumed to be independently and nor-
mally distributed, with mean µ and variance σ2. The observed data are
denoted by the vector y of length No; the missing data by the vector z of
length Nm. The objective of inference is to characterize µ and σ2 with the
data available.
The p.d.f. of y is given by the product of truncated normal distributions.
The contribution to the likelihood from each element of y is
L

µ, σ2|yi > T

∝
p

yi|µ, σ2
 ∞
T p (yi|µ, σ2) dyi
=
p

yi|µ, σ2

1 −Φ

T −µ
σ
,
i = 1, . . . , No,
(11.94)
where p

·|µ, σ2
is the p.d.f. of the normal distribution and Φ (·) is the
cumulative density function of the standard normal distribution.
The contribution to the likelihood from each element of z, zj (j = 1, . . . , Nm) ,
is
L

µ, σ2|zj ≤T

∝P

zj ≤T|µ, σ2
=
 T
−∞
p

zj|µ, σ2
dzj
= Φ

T −µ
σ

.
(11.95)
By virtue of independence, the likelihood is
L

µ, σ2|y

∝
No
7
i=1
p

yi|µ, σ2

1 −Φ

T −µ
σ
No

Φ

T −µ
σ
Nm
.
(11.96)
Assuming independent uniform prior distributions for µ and σ2, the joint
posterior distribution p

µ, σ2|y

is proportional to (11.96). Implementation
of the Gibbs sampler requires drawing samples from p

µ|σ2, y

and from
p

σ2|µ, y

. It is clear from (11.96) that these fully conditional posterior
distributions do not reduce to standard form. To facilitate the problem
algorithmically one can augment with the missing data z. The complete

534
11. Markov Chain Monte Carlo
data (observed data + missing data) are denoted by x′ = (z′, y′). Thus,
the complete data vector x has i.i.d. elements each with distributional form
xi ∼N

µ, σ2
.
The observed data can be envisaged as being generated in the following
manner:
yi|µ, σ2, yi > T ∼N

µ, σ2
I (xi > T)
(11.97)
and the missing data
zi|µ, σ2, zi ≤T ∼N

µ, σ2
I (xi ≤T) ,
(11.98)
where I (x ∈A) is the indicator function, which takes the value 1 if x is
contained in the set A, and zero otherwise.
The density of the complete data is
p

x|µ, σ2
∝
NT o
-
i=1

p

xi|µ, σ2
I (xi ≤T) + p

xi|µ, σ2
I (xi > T)

.
(11.99)
The augmented posterior of the parameters takes the form
p

µ, σ2, z|y

∝p

µ, σ2, z

p

y|µ, σ2, z

= p

µ, σ2
p

y, z|µ, σ2
,
which, assuming independent uniform prior distributions for µ and σ2, is
proportional to (11.99). The Gibbs sampler run under the augmentation
scheme involves drawing from p

z|µ, σ2, y

, from p

σ2|z, µ, y

, and from
p

µ|z,σ2, y

.
To derive p

z|µ, σ2, y

, one retains in (11.99) those terms that include z.
Therefore, from (11.98) and (11.99),
p

z|µ, σ2, y

∝
Nm
-
j=1
p

xj|µ, σ2
I (xj ≤T) .
(11.100)
This has the form of a left-truncated normal distribution, with mean µ and
variance σ2, where the truncation point is T.
The fully conditional posterior distribution of σ2 is
p

σ2|z, µ, y

∝
NT o
-
i=1
p

xi|µ, σ2
∝

σ2−NT o
2
exp

−
NT o

i=1
(xi −µ)2
2σ2

.

11.8 Data Augmentation
535
This is the kernel of a scaled inverted chi-square distribution, with scale
parameter NT o
i=1 (xi −µ)2 and NT o −2 degrees of freedom. Therefore,
σ2|z, µ, y ∼
NT o

i=1
(xi −µ)2

χ−2
NT o−2.
(11.101)
Finally, the fully conditional posterior distribution of µ is
p

µ|σ2z, y

∝
NT o
-
i=1
p

xi|µ, σ2
.
This is proportional to exp

−
NT o
i=1 (xi−µ)2
2σ2

. Adding and subtracting x =
NT o
i=1 xi/NT o in the squared term yields
exp


NT o

i=1
[(xi −x) + (x −µ)]2
2σ2

.
Retaining only terms in µ one obtains
µ|σ2z, y ∼N

x, σ2
NT o

.
(11.102)
The Gibbs sampling algorithm consists of drawing repeatedly from (11.100),
from (11.101), and ﬁnally from (11.102).
■
Example 11.7
ABO blood groups
Gene frequencies of the ABO blood group data were inferred using maxi-
mum likelihood implemented via Newton-Raphson in Example 4.7 of Chap-
ter 4 and implemented via the EM algorithm in Example 9.2 of Chapter 9.
Here a Bayesian MCMC approach is implemented via data augmentation,
using the data as in Example 4.7.
Assume a Dirichlet prior with parameters (αA, αB, αO) for the gene fre-
quencies. On the basis of the data in Table 4.1 the joint posterior distribu-
tion f (pA, pB, pO|n) is proportional to

p2
A + 2pApO
nA [2pApB]nAB 
p2
B + 2pBpO
nB 
p2
O
nO
[pA]αA−1 [pB]αB−1 [pO]αO−1 ,
where n = (nA, nAB, nB, nO)′ is the observed data. It is not possible to
extract standard fully conditional posterior distributions for pA, pB, and
pO from this joint posterior. Augmenting with the missing counts
nm = (nAO, nAA, nBB, nBO)

536
11. Markov Chain Monte Carlo
yields the following augmented posterior distribution
f (nm, pA, pB, pO|n) ∝f (nm, pA, pB, pO) f (n|nm, pA, pB, pO)
= f (n, nm|pA, pB, pO) f (pA, pB, pO) .
(11.103)
The augmented posterior (11.103) has the form
[pA]2nAA [2pApO]nAO [2pApB]nAB [pB]2nBB [2pBpO]nBO
[pO]2nO [pA]αA−1 [pB]αB−1 [pO]αO−1 ,
(11.104)
which is proportional to
[pA]2nAA [pA]nAO [pA]nAB [pA]αA−1
[pB]2nBB [pB]nAB [pB]nBO [pB]αB−1
[pO]2nO [pO]nAO [pO]nBO [pO]αO−1
= [pA]2nAA+nAO+nAB+αA−1 [pB]2nBB+nAB+nBO+αB−1
[pO]2nO+nAO+nBO+αO−1 .
From this expression, the joint conditional posterior distribution
[pA, pB, pO|nm, n]
is immediately recognized as Dirichlet, with parameters a = 2nAA +nAO +
nAB + αA, b = 2nBB + nAB + nBO + αB, and c = 2nO + nAO + nBO + αO;
that is,
pA, pB, pO|nm, n ∼Di (a, b, c) .
(11.105)
To derive the fully conditional posterior distribution of nAA, ﬁrst write
nAO = nA −nAA and extract the terms in nAA from (11.104). This yields
nAA|pA, pO, nA ∼Bi

p2
A
p2
A + 2pApO
, nA

,
(11.106)
with nAO = nA −nAA. Similarly,
nBB|pB, pO, nB ∼Bi

p2
B
p2
B + 2pBpO
, nB

,
(11.107)
with nBO = nB −nBB.
The Gibbs sampling algorithm deﬁned by (11.105), (11.106), and (11.107),
was run using a chain length equal to 3500. After discarding the ﬁrst
500 samples, the mean and standard deviation of the marginal posterior
distributions were estimated from the remaining 3000 draws. Choosing
αA = αB = α0 = 2, the Monte Carlo estimate of the posterior means
are for pA = 0.20925 and for pB = 0.08102. The corresponding poste-
rior standard deviations are 0.0066312 and 0.0043504 (for p0 the posterior

11.8 Data Augmentation
537
standard deviation is 0.0073635). Choosing αA = αB = α0 = 1 (leading
to a uniform prior for the gene frequencies) yields estimates of posterior
means for pA = 0.20915 and for pB = 0.08084; the corresponding posterior
standard deviations are 0.0066315 and 0.0043533 (the posterior standard
deviation of p0 is 0.0073300).
■

This page intentionally left blank

12
Implementation and Analysis of
MCMC Samples
12.1
Introduction
The typical output of a Bayesian MCMC analysis consists of correlated
samples from the joint posterior distribution of all parameters of a single
model or of a number of models. Using these samples, the analyst may be
interested in estimating various features of the posterior distribution. These
could include quantiles or moments of marginal posterior distributions of
the parameters or of functions thereof.
Three partly related questions that could be posed in the analysis of
MCMC output are:
• Can the simulated or sampled values be considered to be draws from
the posterior distribution of interest?
• Are estimates of features of the posterior distribution precise enough?
• Have the empirical averages computed from the Monte Carlo output
converged to their expectation under the equilibrium distribution?
Other pertinent problems involve the possible impropriety of posterior
distributions (a potential danger when improper priors are employed), and
issues related to the sensitivity of an analysis using the MCMC samples.
These points are discussed in this chapter and partly in Subsection 16.2.3
of Chapter 16. First, we discuss the relative advantages and disadvantages
of conducting the MCMC analysis using one or several independent chains.
Subsequently, the eﬀects of inter-correlation between parameters on the

540
12. Implementation and Analysis of MCMC Samples
behavior of the MCMC are illustrated, and some techniques for diagnosing
convergence are presented. Another section gives an overview of estimators
of features of the posterior distribution and of their Monte Carlo precision.
The chapter concludes with a discussion of sensitivity assessment.
12.2
A Single Long Chain or Several Short Chains?
In the early 1990s when MCMC methods entered into the statistical arena,
considerable discussion centered on the best ways of running the algorithms.
Diﬀerent implementation strategies aﬀect the serial correlations between
successive samples of the same or diﬀerent parameters within a chain. These
correlations, as discussed below, inﬂuence the rate of convergence to the
stationary distribution and the sampling error of estimates of features of
this distribution. In short, the speciﬁc implementation adopted can have a
profound impact on the eﬃciency of the computations.
A strategy that was advocated in the early literature (Gelfand and Smith,
1990) but that has fallen in disuse thereafter, consists of running several
short independent chains, k say, and on saving the last sample (the mth)
from each of the chains. This is known as the multiple-chain or short-
chain method. Here, mk samples are generated but only k are kept for
the post-MCMC analysis. The method is extremely ineﬃcient because
(100 (m −1) /m) % of the samples are discarded and the value of m is at
least in the dozens. In addition, the length of the chains was often judged
to be insuﬃcient to guarantee convergence of each of the runs to the target
distribution.
Current recommendations about implementation strategies in the lit-
erature range from running either several long chains (Gelman and Ru-
bin, 1992) or a single, very long one (Geyer, 1992). Supporters of the ﬁrst
approach argue that a comparison of results from several seemingly con-
verged chains might reveal genuine diﬀerences, if the chains have not yet
approached stationarity. Those in favor of the single, long-chain implemen-
tation, believe that this method has better chances of producing samples
which properly represent the complete support of the target distribution.
In practice, one almost always uses more than a single long run, and con-
vergence is assessed graphically or via more formal tests, as discussed later.
To avoid possible inﬂuences of the starting values, the initial samples are
often discarded. This is usually referred to as the burn-in period. After
burn-in, unless correlations between adjacent samples are extremely high,
or if storage is a problem, all samples are kept for later processing. A very
useful reference where many implementation problems are discussed is the
tutorial of Kass et al. (1998).

12.3 Convergence Issues
541
12.3
Convergence Issues
It was seen in Chapters 10 and 11 that an ergodic Markov chain generated
via iterative Monte Carlo converges to its stationary distribution asymp-
totically. This means that the number of iterations of the chain must ap-
proach inﬁnity! In practice, however, one runs a chain which is long enough
in some sense, so that the values obtained can be regarded as approximate
draws from the posterior distribution of interest. The diﬃculty resides in
determining how long the chain must be. Unfortunately, there is no simple
answer to this question. Clearly, if the iterations have not proceeded long
enough, the draws may be unrepresentative of the whole support of the
target distribution and this will probably result in poor inferences.
12.3.1
Eﬀect of Posterior Correlation on Convergence
A strong intercorrelation between parameters in the posterior distribution
hampers the behavior of the MCMC scheme. We start this section with a
couple of stylized examples. The ﬁrst one illustrates how a high posterior
correlation between parameters can slow down the motion of the chain
towards its equilibrium distribution. The second presents a model in which
the two parameters of a model are not identiﬁable from the likelihood
function. If improper priors are used, this leads to a situation where even
though the conditional distributions are well deﬁned, the joint posterior
does not exist. In this case, a Gibbs sampler will lead to absurd results,
even though the “numbers” may appear sensible. When proper priors are
adopted, it is shown that the analysis produces Bayesian learning, but
the inﬂuence of the prior does not dissipate asymptotically. Further, the
sampler will move very slowly in the parameter space, because of the high
intercorrelation between the poorly identiﬁed parameters.
Example 12.1
A 2 × 2 table
Consider Example 10.4 from Chapter 10, and following O’Hagan (1994) let
p1 = p4 = p
2.
p3 = p2 = 1 −p
2
.
It can be veriﬁed that
Cov (X, Y ) = E (XY ) −E (X) E (Y )
= 1
4 (2p −1) ,
and that
V ar (X) = V ar (Y ) = 1
4.

542
12. Implementation and Analysis of MCMC Samples
Therefore, the correlation between X and Y is
ρ = 2p −1.
The transition probability matrix Px|x is
Px|x =

1 −2p (1 −p)
2p (1 −p)
2p (1 −p)
1 −2p (1 −p)

.
The matrix Px|x is ergodic provided that p > 0 and the unique solution to
equation (10.8)
π′Px|x = π′,
is
π′ =

1
2
1
2

.
Therefore π is the unique stationary distribution of the Markov chain. The
matrix Px|x has two eigenvalues, λ1 = 1 and λ2 = ρ2. The corresponding
eigenvectors are
c′
1 =

1
1

,
c′
2 =

−1
1

.
Then the rate of convergence of the Markov chain can be studied using
(10.26),
Pn
x|x = λn
1Q1 + λn
2Q2,
n = 1, 2, . . . ,
which in the present example is equal to
Pn
x|x = 1n


1
2
1
2
1
2
1
2

+

ρ2n


1
2
−1
2
−1
2
1
2


=


1
2

1 + ρ2n
1
2

1 −ρ2n
1
2

1 −ρ2n
1
2

1 + ρ2n

.
(12.1)
Recall expression (10.7) from Chapter 10,
π′(n) = π′(0)Pn
x|x,
where π(0) = (p(0), 1 −p(0))′ represents the distribution of the initial state
of the Markov chain. It is easy to verify from (12.1) that after n transitions
p(n) = 1
2

1 −ρ2n 
1 −2p(0)
.
Thus, for high values of ρ, convergence toward the equilibrium distribution
is very slow. To illustrate with an extreme case, setting ρ = 0.9998 and

12.3 Convergence Issues
543
p(0) = 0.1 leads, after one transition, to p(1) = 0.10016 and, after n = 1000
transitions, to p(1000) = 0.23188, still a long way from the equilibrium value
of 1/2. Another way of illustrating the same phenomenon is to set n = 1000
in (12.1), which gives
P1000
x|x =

0.835
0.165
0.165
0.835

.
This is far from the equilibrium value


1
2
1
2
1
2
1
2

.
■
Although correlations as large as 0.9998 are not that common, moderate
to high correlations between parameters of the model can be encountered
frequently. This induces slow mixing of the chain: successive transitions
are strongly correlated and convergence can be very slow. Even if the chain
converges to the equilibrium distribution, with poor mixing, using the time-
average over the chain under the equilibrium distribution, i.e., estimator
(11.6), will result in poor inferences.
In highly parameterized models, relatively small correlations can result
in a similar behavior of the chain, because of intercorrelations between
parameters. From a practical point of view, autocorrelation between suc-
cessive samples produces long sequences where little change is detected,
misleadingly suggesting that the chain has converged. Another consequence
of within-sequence correlation is that it leads to less precise inferences than
those obtained from the same number of independent samples. Two strate-
gies are often suggested for ameliorating slow mixing in MCMC imple-
mentations: reparameterization of the model, and sampling parameters in
blocks, rather than sampling each parameter individually. However, the two
strategies often lead to added computational complexity.
Example 12.2
Identiﬁability and impropriety of posterior distributions
This example is based on an exercise in Chapter 5 of Carlin and Louis
(1996). Suppose that data yi, (i = 1, 2, . . . , n), are independent realizations
from a normal distribution with known variance
yi|θ1, θ2 ∼N (θ1 + θ2, 1) .
(12.2)
Since the variance is known, observations have been rescaled to have a
standard deviation equal to 1. The likelihood of θ = (θ1, θ2)′ can be written

544
12. Implementation and Analysis of MCMC Samples
as
p (y|θ) ∝exp

−n
2 (θ1 + θ2 −y)2
∝exp

−1
2 (θ −µ)′ N (θ −µ)

,
(12.3)
where y = 1
n
 yi, µ′=
y
2, y
2

and
N =

n
n
n
n

.
Because the matrix N has rank equal to 1, (12.3) is the kernel of a singular
bivariate normal distribution, if viewed as a function of θ. It is clearly
impossible to obtain ML estimates of θ1 and θ2 from (12.3); on the other
hand, the ML estimator of θ1 + θ2 is y. If independent, improper uniform
prior distributions are adopted for each of the parameters, the density of
the joint posterior distribution of θ1, θ2 is
p (θ1, θ2|y) ∝exp

−n
2 (θ1 + θ2 −y)2
.
(12.4)
It can be veriﬁed readily that the densities of the conditional posterior
distributions are
θ1|θ2, y ∼N

y −θ2, 1
n

(12.5)
and
θ2|θ1, y ∼N

y −θ1, 1
n

.
(12.6)
The marginal posterior density of θ1, say, is
p (θ1|y) = p (θ1, θ2|y)
p (θ2|θ1, y)
=
c12 exp

−n
2 (θ1 + θ2 −y)2
c2|1 exp

−n
2 (θ1 + θ2 −y)2
= c12
c2|1
,
(12.7)
where c12 and c2|1 are the constants of integration associated with (12.4)
and (12.6), respectively, assuming c12 is ﬁnite. Hence, this marginal poste-
rior distribution does not depend on θ1. Clearly, the integral of (12.7) over
the real line (the sample space of θ1) does not converge, indicating that the
marginal posterior distribution is improper.
In this setting, the parameters θ1 and θ2 in (12.4) are unidentiﬁable from

12.3 Convergence Issues
545
each other: the posterior distribution carries information on their sum,
θ1 + θ2, but not on each one of them separately. Since the fully conditional
posterior distributions (12.5) and (12.6) are proper, a Gibbs sampling im-
plementation of the model is possible, yielding valid inferences about fea-
tures of the distribution [θ1 + θ2|y]. However, one could naively use the
samples from the chains generated from (12.5) and (12.6), to “infer” fea-
tures of the marginal posterior distribution of θ1 or of θ2. This would lead
to meaningless results, since these marginal distributions do not exist. This
illustrates the pitfall of using improper priors in hierarchical models. Ex-
cept in highly stylized models (such as in this example) it is very diﬃcult to
assess impropriety analytically (e.g., Hobert and Casella, 1996). All condi-
tional posterior distributions may exist even when the joint distribution is
not deﬁned. Yet, the output analysis may produce seemingly “reasonable”
results!
Now consider assigning independent normal distributions, a priori, to
each of θ1 and θ2. Take
θi ∼N

ai, b2
i

,
i = 1, 2,
such that the joint prior density is
p (θ1, θ2) ∝exp

−1
2 (θ −a)′ B (θ −a)

,
(12.8)
where θ = (θ1, θ2)′, a = (a1, a2)′, and
B =


1
b2
1
0
0
1
b2
2

.
With proper prior distributions, the problem of the lack of identiﬁability of
the parameters in the posterior distribution disappears. Now using (12.8)
and (12.3) the joint posterior density is
p (θ|y) ∝exp

−1
2

(θ −a)′ B (θ −a) + (θ −µ)′ N (θ −µ)
%
.
(12.9)
Combining these two quadratic forms using results in Box and Tiao (1973),
page 418, and keeping only the terms which are functions of θ, leads to
p (θ|y) ∝exp

−1
2

θ −θ
′ (B + N)

θ −θ

.
(12.10)

546
12. Implementation and Analysis of MCMC Samples
This is the kernel of the density of a bivariate normal distribution with
mean vector θ and variance–covariance matrix (B + N)−1, where
θ = (B + N)−1 (Ba + Nµ)
=


n + 1
b2
1
n
n
n + 1
b2
2


−1 

a1
b2
1
+ ny
a2
b2
2
+ ny

.
(12.11)
After some algebra, the posterior mean vector (12.11) can be written as:
 θ1
θ2

=

a1 + k1 (y −a1 −a2)
a2 + k2 (y −a1 −a2)

,
(12.12)
where
k1 =
b2
1
b2
1 + b2
2 + 1
n
,
and
k2 =
b2
2
b2
1 + b2
2 + 1
n
.
The posterior variance–covariance matrix is
V ar (θ|y)
=


n + 1
b2
1
n
n
n + 1
b2
2


−1
=

b2
1 (1 −k1)
−b2
1k2
−b2
1k2
b2
2 (1 −k2)

.
(12.13)
There are a number of important conclusions that can be drawn from this
exercise. First, note that the marginal posterior distribution of θi is now
proper, and that it diﬀers from the prior distribution. In this sense, there
is Bayesian learning via the data. From (12.12), it is apparent that the
inﬂuence of the data on the posterior mean depends on the values of the
prior variances, via the “regression” ki = b2
i /

b2
1 + b2
2 + 1
n

. Second, (12.13)
indicates that the variance of the marginal posterior distribution is smaller
than the prior variance. Third, (12.12) and (12.13) illustrate that as the
number of observations n →∞, the inﬂuence of the prior does not vanish
asymptotically. The posterior variance does not tend to zero, and infer-
ences always depend on the relative values of the prior variances b2
1/b2
2. For
example (for large n), if b2
1 >> b2
2, the posterior mean of θ1 will tend to
y −a2, whereas the posterior mean of θ2 will be close to a2, its prior mean.
Here, Bayesian learning occurs for θ1, but not for θ2. Finally, from (12.13),
the posterior correlation between θ1 and θ2 is given by:
Corr (θ1, θ2|y) = −
b1b2
8 1
n + b2
1
  1
n + b2
2
,
(12.14)

12.3 Convergence Issues
547
which is very close to −1 for moderately large n. From the point of view of
implementing a Gibbs sampler, this has important implications. While the
proper prior distributions of θ1 and θ2 lead to proper marginal posterior
distributions, the very high posterior correlation between θ1 and θ2 will
generate a strong serial correlation between samples of a Gibbs chain. This
will have sizable eﬀects on convergence, and will retard the movement of
the Gibbs sampler over the support of the posterior distribution. In such a
situation, the quality of posterior inferences would be impaired seriously. A
broad discussion on strategies for improving MCMC can be found in Gilks
and Roberts (1996).
■
MCMC opens the opportunity for ﬁtting complex hierarchical models to
data, and these models perhaps describe better the biological system un-
der study. However, the richness and ﬂexibility are accompanied by caveats.
For example, it may be dangerous to entertain models that are not well un-
derstood analytically. In highly complex models, there is always the pitfall
that parameters may be unidentiﬁed or very weakly identiﬁed. In contrast
with the preceding example, it may not always be possible to detect lack of
identiﬁability. Therefore, it is important to learn as much as possible about
the model, and to experiment with it step by step before launching a full
MCMC-based analysis.
12.3.2
Monitoring Convergence
A large literature on convergence diagnostics has developed during the last
decade. Useful reviews can be found in Cowles and Carlin (1996), Brooks
and Roberts (1998), Robert (1998), Robert and Casella (1999), Mengersen
et al. (1999), and references herein. In this section some of the commonly
used convergence diagnostics are described, and the reader is referred to
the above reviews for a description of other methods.
Graphical Procedures
Gelfand et al. (1990) suggested informal convergence checks based on graph-
ical techniques. They run a number of independent chains with variable
starting values and perform the analysis for each of the parameters of in-
terest. Plots of histograms are overlaid for increasing chain lengths, until
the graphs become visually indistinguishable among chains. Stability of
the results is assessed by increasing chain length. Another simple graphical
tool for studying convergence and mixing behavior of the chain is what is
called the “trace plot”. Samples of the parameter of interest, from repli-
cated chains started from overdispersed values, are plotted as a function
of iterate number. Wavy patterns typically indicate strong autocorrelations
within chains, while a zigzag suggest that the parameter moves more freely.
In highly dimensional problems, however, it is not feasible to examine the

548
12. Implementation and Analysis of MCMC Samples
time series plots of all the parameters. In this case, a selective choice is
often made, such that either the focus of inference or a high-level parame-
ter in a hierarchy (e.g., variance components in a generalized linear model)
are followed, since these tend to mix more slowly. Here, it is important to
be aware that parameters converge at diﬀerent rates. Further, a slow con-
vergence of nuisance parameters may aﬀect convergence of the parameters
of interest adversely. Also, an assessment of convergence of the marginal
distributions does not provide an exhaustive diagnostic of convergence of
the joint process.
Auto-Correlograms
Examination of lag-t autocorrelations is an easy way of monitoring the
mixing behavior of the Markov chain. For instance, one can plot the au-
tocorrelations as a function of the lag (this is called a correlogram), and
detect the lag at which the correlation “dies out”. As indicated below,
these autocorrelations are also useful to estimate the eﬀective chain length
(Sorensen et al., 1995).
Between and within-chain Variability of Sample Values
A popular quantitative convergence diagnostic was suggested by Gelman
and Rubin (1992). The method involves running m independent chains,
each of length 2n. Each of the chains starts from a diﬀerent starting point
from a distribution that is overdispersed with respect to the target distri-
bution. This is an important design feature, because it can make lack of
convergence apparent. As discussed in Gelman and Rubin (1992), satisfying
this requirement may involve considerable initial eﬀort toward eliciting a
rough estimate of the variance of the marginal posterior distribution of the
scalar of interest. This knowledge is important for generating an overdis-
persed starting sequence. Starting from points far apart will also ensure
that the complete support of the target distribution will be visited. The
ﬁrst n iterations of each chain are discarded and the last n are retained.
Each feature of interest from the target distribution is monitored sepa-
rately. The following step consists of carrying out an analysis of variance:
approximate convergence is diagnosed when the variability between chains
is not larger than that within chains. The rationale of the method is that
before convergence is reached, due to the initial over-dispersion, the vari-
ance between chains should be relatively large. On the other hand, variation
within chains would be relatively small because in the intermediate stages
of the iteration the support of the target distribution would be incompletely
represented in the simulations.
Suppose that for chain i (i = 1, 2, . . . , m), simulated values
θij,
j = 1, 2, . . . , n

12.3 Convergence Issues
549
are available from the scalar posterior distribution [θ|y]. Hence, there are
m chains each of length n. The simulated values can be organized as a one-
way layout, with m classes and n observations per class. The between-chain
mean square B and the within-chain mean square W are
B =
n
m

i=1

θi. −θ..
2
m −1
,
and
W =
m

i=1
S2
i
m
.
Here
θi. =
n
j=1
θij
n
,
θ.. =
m

i=1
θi.
m
are the within-chain sample average and the mean of the chain averages,
respectively, and
S2
i =
n
j=1

θij −θi.
2
n −1
is the estimated variance of sampled values in chain i. Let
µ =

θp (θ|y) dθ
and
σ2 =

(θ −µ)2 p (θ|y) dθ,
be the mean and variance of the target posterior distribution, respectively.
If the simulated values are drawn from [θ|y], it can be veriﬁed readily that
the following expectations hold:
E (B) = σ2,
(12.15)
and
E

S2
i

= E (W) = σ2.
(12.16)
Gelman and Rubin (1992) suggest the estimator of the posterior variance
6
σ2 = n −1
n
W + 1
nB,
(12.17)
which is clearly unbiased for σ2, provided that all draws are from the target
distribution. On the other hand, if there is at least some initial overdisper-
sion, the estimator will have an upward bias because the averages θi. would

550
12. Implementation and Analysis of MCMC Samples
vary more than if drawn from the same distribution. This suggests that if
the initial draws are not overdispersed enough, 6
σ2 can be too low, falsely di-
agnosing convergence. Gelman and Rubin (1992) suggest that convergence
of any scalar quantity of interest can be monitored by the quantity
5R =
C
6
σ2
W =
C
1 + 1
n

 B
W −1

(12.18)
which is expected to be larger than 1, and declines to 1 as n →∞. There-
fore, convergence can be evaluated by examining the proximity of 5R to 1.
Gelman and Rubin (1992) mention that values of 5R around 1.2 may be
satisfactory for most problems. In some cases, however, a higher level of
precision may be required. Although normality is not required for unbi-
asedness of 6
σ2, Gelman and Rubin (1992) state that the method works
better if the posterior distribution is nearly normal.
A number of shortcomings of the method have been raised. First, con-
structing a distribution for sampling starting values may be diﬃcult, es-
pecially in models in which multimodality is expected. Second, discarding
the ﬁrst n samples is computationally wasteful. Third, the method relies
to some extent on approximate normality of the target distribution, and
this may not always hold, specially in ﬁnite sample situations. Fourth,
the procedure will not work if the chains get “trapped” within the same
subregion of the parameter space. Finally, the convergence criterion is uni-
dimensional; hence, it gives an inadequate evaluation of convergence to
the joint distribution. Brooks and Gelman (1998) have developed a multi-
parameter version of this approach. These criticisms must be seen in the
light of the fact that none of the many methods proposed can be relied
upon unilaterally. In a review of convergence diagnosis methods, Cowles
and Carlin (1996) concluded that all procedures can fail to detect the sort
of convergence failure that they were designed to identify. Therefore, the
general recommendation is to use a combination of approaches as diagnos-
tic tools (including graphical methods) and to learn as much as possible
from the target distribution before embarking in a MCMC algorithm. As
a minimum, ensuring propriety of the posterior distribution is essential.
12.4
Inferences from the MCMC Output
12.4.1
Estimators of Posterior Quantities
The MCMC output is typically used to estimate features of the posterior
distribution, such as posterior means and medians, or the posterior vari-
ance. While the issue of convergence of the Markov chain to the target
distribution is of fundamental importance, many authors place emphasis

12.4 Inferences from the MCMC Output
551
on the properties of estimators of features of the posterior distribution.
For example, it is of interest to establish whether or not the diﬀerence be-
tween estimates of a posterior mean obtained from two or more independent
chains, can be explained by Monte Carlo sampling error. In this section,
two commonly used estimators of features of posterior distributions will
be presented. Other estimators are described in Robert and Casella (1999)
and in Chen et al. (2000), where a more formal treatment of the subject
can be found.
Ergodic Averages
Consider a single chain consisting of correlated samples θ(i) (i = 1, 2, . . . , n)
from the target distribution [θ|y]. As presented in the previous chapter
(expression (11.5)), for some function h (θ), the ergodic theorem states
that the ergodic average of the function h (θ), given by
1
n
n

i=1
h

θ(i)
(12.19)
is a consistent estimator of

h (θ) p (θ|y) dθ, provided that this integral
converges. That is,
1
n
n

i=1
h

θ(i)
→

h (θ) p (θ|y) dθ
(12.20)
as n →∞, with probability 1, if

h (θ) p (θ|y) dθ < ∞. As mentioned
above, the rate of convergence may be seriously aﬀected by slow mixing
of the chain. At any rate, (12.19) is a consistent estimator of the expected
value of h (θ) with respect to the invariant distribution [θ|y], despite any
existing autocorrelation between the simulated values θ(i) of the Markov
chain. For example, expression (12.19) is an estimator of:
• the posterior mean if h (θ) = θ;
• the posterior variance if h (θ) = [θ −E (θ|y)]2. The estimator of the
posterior variance is
1
n
n

i=1

θ(i)2 −

5E (θ|y)
2
,
where 5E (θ|y) = 1
n

i θ(i);
• the posterior probability that θ ∈A, if h (θ) = I (θ ∈A), such that
Pr (θ ∈A|y) =

I (θ ∈A) p (θ|y) dθ.

552
12. Implementation and Analysis of MCMC Samples
Here, the estimator is n
i=1 I

θ(i) ∈A

/n. As a special case, the
cumulative distribution function is estimated as
5F (t) = 1
n
n

i=1
I

θ(i) < t

.
Therefore, the estimator of the posterior probability that t1 < θ < t2
is
6
Pr (t1 < θ < t2|y) = 1
n
 n

i=1
I

t1 < θ(i) < t2

;
• the posterior predictive density:
p (z|y) =

p (z|θ, y) p (θ|y) dθ,
where, usually, the form of the problem is such that p (z|θ, y) =
p (z|θ) . In this setting, h (θ) = p (z|θ) , and the estimator of the pre-
dictive density is n
i=1 p

z|θ(i)
/n.
Rao–Blackwell Estimator
Another estimator that has been proposed in the literature is known as
the Rao-Blackwell estimator (Gelfand et al., 1990; Liu et al., 1994; Casella
and Robert, 1996), which derives its name from the Rao–Blackwell theorem.
This theorem states that conditioning an unbiased estimator on a suﬃcient
statistic will result in a uniformly better unbiased estimator.
Let the parameter vector of a model consist of two scalars, that is, θ =
(θ1, θ2)′, and suppose that interest focuses on the mean of the marginal
posterior distribution of the function h (θ1), or E [h (θ1) |y]. As mentioned
above, the ergodic average estimator is
1
n
n

i=1
h

θ(i)
1

,
(12.21)
where θ(i)
1
is a sample from [θ1|y]. The Rao–Blackwell estimator is obtained
using results discussed in Chapter 1, Section 1.6. Recall that
E [h (θ1) |y] =
 
h (θ1) p (θ1|θ2, y) dθ1

p (θ2|y) dθ2
=

Eθ1|θ2,y [h (θ1) |θ2, y] p (θ2|y) dθ2
= Eθ2|y

Eθ1|θ2,y [h (θ1) |θ2, y]

.

12.4 Inferences from the MCMC Output
553
The Rao–Blackwell estimator has the form
5E [h (θ1) |y] ≈1
n
n

i=1
Eθ1|θ2,y

h (θ1) |θ(i)
2 , y

,
(12.22)
where θ(i)
2 is a draw from the marginal posterior distribution [θ2|y]. Hence,
the estimator is an ergodic average of conditional means, and one must be
able to write these in closed form, to be able to form (12.22). Recall from
(1.129) that the variance of h (θ1|y) in (12.21) can be written as
V ar [h (θ1|y)]
= V ar {E [h (θ1) |θ2, y]} + E {V ar [h (θ1) |θ2, y]} .
Therefore, V ar {E [h (θ1) |θ2, y]} ≤V ar [h (θ1|y)] indicating that (12.22)
can improve upon (12.21) in terms of variance. While making optimal use
of the available data is a praiseworthy endeavor, the improvement of (12.22)
over (12.21) is often limited in chains that have been run long enough. This
improvement comes at the cost of having to know the closed form of the
expected value of the conditional posterior distribution [θ1|θ2, y] , and at a
loss of the simplicity with which (12.21) is calculated.
Density Estimation
Another way of estimating features of the posterior distribution of a pa-
rameter of interest is to obtain a smooth estimate of the posterior density
using the chain output, and then computing moments from this density
using numerical integration. This approach seems to be in disuse in output
analysis, in favor of the simple practice of approximating the density by
histograms, and of estimating features from posterior distributions using
(12.19) and (12.22), for example. The reader is referred to classical texts on
density estimation by Silverman (1992) and by Scott (1992) for a detailed
description of this approach.
12.4.2
Monte Carlo Variance
Deﬁnition
Here it is assumed that draws from the stationary distribution [θ|y] are
available. Because only a ﬁnite number of these draws can be obtained,
there is always sampling uncertainty associated with an estimator of fea-
tures of the target distribution, such as (12.19). This sampling variance
is known as the Monte Carlo (MC) variance of estimators of posterior
quantities. In principle, it can be made as small as desired, by taking a
suﬃciently large number of samples. This MC variance can be estimated
by running several independent chains, and then calculating the empirical,
between-chain variance of the estimates obtained for each chain. Since this

554
12. Implementation and Analysis of MCMC Samples
is often computationally expensive, one resorts to theoretical estimators of
MC variance. These estimators account for the autocorrelation among the
samples taken from the target distribution. Useful references are Ripley
(1987), Geyer (1992), and Chen et al. (2000). Here, two commonly used
estimators are described.
Consider estimating the cumulative distribution function
F (t|y) = Pr (θ < t|y)
from the MCMC output θ(1), θ(2), . . . , θ(n). The estimator is
5F (t|y) = 1
n
n

i=1
I

θ(i) < t

.
Now I

θ(i) < t

has a Bernoulli distribution with success probability F (t|y).
Thus, if the draws θ(1), θ(2), . . . , θ(n) were independent,
n

i=1
I

θ(i) < t

would have a binomial distribution with parameters (F (t|y) , n). It follows
that with independent draws, the estimator of the Monte Carlo variance of
5F (t|y) would be equal to
D
V ar

5F (t|y)

= 1
n
5F (t|y)

1 −5F (t|y)

.
(12.23)
However, (12.23) may give a very distorted picture of the true Monte Carlo
variance, depending on the pattern of the autocorrelation among the sam-
ples from the target distribution. As shown by Liu et al. (1994), this pattern
can be complex since, in a reversible chain, even-lag autocorrelations are
non-negative, while odd-lag auto-covariances need not be positive.
Geyer’s Estimator
Geyer (1992) proposed an estimator of the Monte Carlo variance of the
estimator of the mean of h (θ) based on time-series theory. The estimates
produced are larger than or equal to the true Monte Carlo variance. First,
from (12.19), deﬁne the estimator of the mean of h (θ) as
5µ = 1
n
n

i=1
h

θ(i)
.
(12.24)
Let the lag-t autocovariance of the stationary Markov chain h

θ(i)
be
γ (t) = Cov

h

θ(i)
, h

θ(i+t)
,
i = 1, 2, . . . , n.

12.4 Inferences from the MCMC Output
555
An estimator of γ (t) is
5γ (t) = 1
n
n−t

i=1
9
h

θ(i)
−5µ
 
h

θ(i+t)
−5µ
:
.
(12.25)
Priestley (1981) mentions that it has been asserted that in general, this
biased estimator with divisor n has smaller mean square error than the
unbiased estimator with divisor n −t. One of the estimators of the Monte
Carlo variance of 5µ proposed by Geyer (1992), which he calls the initial
positive sequence estimator, uses (12.25) as input and is equal to
D
V ar (5µ) = 1
n

5γ (0) + 2
i=2δ+1

i=1
5γ (i)

,
(12.26)
where δ is chosen such that it is the largest integer satisfying
5γ

2δ′
+ 5γ

2δ′ + 1

> 0,
δ′ = 0, 1, . . . , δ.
If the samples are independent,
D
V ar (5µ) = 1
n5γ (0) .
An idea of the eﬀect of the autocorrelation on the amount of informa-
tion contained in the chain for inferring features of the target distribution,
can be obtained by computing an “eﬀective chain size”. Denote this as Ψ
(Sorensen et al., 1995), where
Ψ =
5γ (0)
D
V ar (5µ)
.
When the chain consists of independent draws from the target distribution,
Ψ = n, and the eﬀective and nominal sizes of the chain are equal.
Batching
A popular method of estimating Monte Carlo variances that is easy to
implement, is known as “batching” (Hastings, 1970). It is based on the
idea that if individual draws are correlated, grouping successive draws into b
batches or groups of size m each, and computing the raw averages, will lead
to b batch means that are less strongly inter-correlated than the original
draws. This can be so provided that m is chosen appropriately. Further, the
larger the autocorrelation among samples, the larger m must be. Suppose
that a chain of total length n is divided into b batches each of size m. Let
the average of the ith batch be
xi = 1
m
m

j=1
h

θ(j)
,
i = 1, 2, . . . , b.

556
12. Implementation and Analysis of MCMC Samples
Here, h

θ(j)
is some feature of the posterior distribution evaluated at the
sampled value θ(j). The batch estimator of the variance of (12.24), assuming
that m is large enough so that the x′
is are uncorrelated, is equal to
D
V arb (5µ) =
b
i=1
(xi −5µ)2
b (b −1)
.
(12.27)
An estimate of the batch-eﬀective chain size can be obtained as
Ψb =
n
i=1

h

θ(i)
−5µ
2
(n −1) D
V arb (5µ)
.
When the samples are independent, m = 1, xi = h

θ(i)
for all i, and
Ψb = n.
If the autocorrelation among the samples of the chain is very high (>
0.95), estimator (12.26) seems to be preferred over (12.27).
MCMC algorithms converge to the target distribution asymptotically
and the samples are typically correlated. A new and exciting approach,
termed perfect sampling proposed by Propp and Wilson (1996), avoids
problems of convergence and of serial correlations, since it generates inde-
pendent draws from the target distribution. This is an area where research
is just beginning; it is not clear at the moment whether the technique can
applied in settings involving high dimensional distributions without nice
symmetry properties. A tutorial can be found in Casella et al. (2001).
12.5
Sensitivity Analysis
An important part of a Bayesian analysis is the study of how robust are
inferences to modeling assumptions, including prior and likelihood speci-
ﬁcations, or presence of outliers. Smith and Gelfand (1992) describe how
to address this question using importance sampling, a technique that was
described by Hammersley and Handscomb (1964). Geweke (1989) showed
how importance sampling can be applied in Bayesian analyses. This tech-
nique was already encountered in Chapter 8, in connection with estimation
of the marginal likelihood from the Monte Carlo samples, and is discussed
again in Chapter 15. Other relevant literature on the subject can be found
in Tanner and Wong (1987), Rubin (1987b), and Gelfand and Smith (1990).
The starting point of a Bayesian analysis is the posterior density
p1 (θ|y) = c1p1 (θ) p (y|θ) ,
where c1 is the typically unknown normalizing constant, p1 (θ) is the density
of some prior distribution assigned to the parameter, and p (y|θ) is the

12.5 Sensitivity Analysis
557
likelihood. The expectation of a function h (θ) with respect to the posterior
distribution with density p1 (θ|y) is
E1 [h (θ)] =

h (θ) p1 (θ|y) dθ.
Suppose that n draws θ(i), (i = 1, 2, . . . , n) are available from this posterior
distribution. Based on (12.19), E1 [h (θ)] can be estimated as
5E1 [h (θ)] = 1
n
n

i=1
h

θ(i)
.
(12.28)
Now, one may be interested in inferences about h (θ) , conditionally on
the same data, but using a diﬀerent set of modeling assumptions. These
could involve perturbations either of the likelihood (this could take a new
functional form, or perhaps part of the data could be omitted) or of the
prior distribution. For example, suppose that one wishes to study the con-
sequences of changing the prior speciﬁcation, such that the new posterior
becomes
p2 (θ|y) = c2p2 (θ) p (y|θ) .
(12.29)
Here, p2 (θ) is the density of the “new” prior distribution, and c2 is the
corresponding integration constant. Using the draws θ(i) generated under
the distribution with density p1 (θ|y), inferences about h (θ) under the new
posterior with density p2 (θ|y) can be obtained without having to run the
MCMC procedure again. This is done by using p1 (θ|y) as importance sam-
pling density. Thus, expectations under p2 (θ|y) can be obtained as follows
E2 [h (θ)] =

h (θ) p2(θ|y)
p1(θ|y)p1 (θ|y) dθ
 p2(θ|y)
p1(θ|y)p1 (θ|y) dθ
=

h (θ) c2p2(θ)p(y|θ)
c1p1(θ)p(y|θ)p1 (θ|y) dθ
 c2p2(θ)p(y|θ)
c1p1(θ)p(y|θ)p1 (θ|y) dθ
=

h (θ) w (θ) p1 (θ|y) dθ

w (θ) p1 (θ|y) dθ
,
(12.30)
where
w (θ) = p2 (θ)
p1 (θ).
Note that in the second line of (12.30) the ratio of constants of integra-
tion and the likelihood cancel out in the numerator and denominator. A
consistent estimator of (12.30) based on (12.19) is
5E2 [h (θ)] =
n
i=1 h

θ(i)
w

θ(i)
n
i=1 w

θ(i)
,
(12.31)

558
12. Implementation and Analysis of MCMC Samples
where θ(i), (i = 1, 2, . . . , n) are the draws from the distribution with density
p1 (θ|y). The weight function wi is equal to
w

θ(i)
=
p2

θ(i)
p1

θ(i).
If wi = 1 for all i, p2 (θ|y) = p1 (θ|y) and (12.30) is equal to (12.28).
Moments and quantiles under the new posterior distribution can be ob-
tained along the same lines, using the draws from the original posterior
distribution. For instance
D
V ar2 [h (θ)] = 5E

h2 (θ)

−

5E [h (θ)]
2
=
n
i=1
h2 
θ(i)
w

θ(i)
n
i=1
w

θ(i)
−

5E [h (θ)]
2
(12.32)
and
6
Pr2 [h (θ) < t] =
n
i=1
I

h

θ(i)
< t

w

θ(i)
n
i=1
w

θ(i)
,
(12.33)
where subscript 2 indicates that inferences are being drawn from the pos-
terior distribution with density p2 [θ|y].
Often, it can be computationally advantageous to ﬁt a particular model
elicited under a certain prior or likelihood speciﬁcation. However, the ana-
lyst may have in mind an alternative model which is less tractable compu-
tationally. The approach described above provides a powerful tool for doing
this in a rather straightforward manner. This is illustrated in the following
example.
Example 12.3
Inferences from two beta distributions
Suppose n independent draws are made from a Bernoulli distribution with
unknown probability of success θ. Let x denote the number of successes
and y the number of failures. The likelihood is
p (x|θ, n) ∝θx (1 −θ)y .
(12.34)
The experimenter wishes to perform the Bayesian analysis under two dif-
ferent sets of prior assumptions. The ﬁrst model assumes a uniform prior
distribution for θ, Un (0, 1):
p1 (θ) = 1,
0 ≤θ ≤1.
(12.35)

12.5 Sensitivity Analysis
559
Mean × 10
Variance × 102
Probability
S
x
y
Exact
IS
Exact
IS
Exact
IS
4
4
1
6.364
6.356
1.9284
1.9757
0.1742
0.1710
100
4
1
6.364
6.361
1.9284
1.9322
0.1742
0.1728
1000
4
1
6.364
6.365
1.9284
1.9286
0.1742
0.1743
4
12
3
7.143
7.134
0.9276
0.9515
0.3155
0.3004
100
12
3
7.143
7.141
0.9276
0.9283
0.3155
0.3124
1000
12
3
7.143
7.143
0.9276
0.9277
0.3155
0.3157
TABLE 12.1. Comparison betwen exact results and estimates based on impor-
tance sampling (IS). S: number of samples in thousands; x: number of successes;
y: number of failures; Probability: posterior probability that the binomial param-
eter takes a value between 0.75 and 0.85.
Under this prior, the posterior density is proportional to (12.34)
p1 (θ|x, n) ∝θx (1 −θ)y ,
(12.36)
which is recognized as the density of a beta distributed random variable
with parameters x + 1, y + 1, that is Be (θ|x + 1, y + 1). The second model
assumes the same likelihood, but the prior distribution for θ is beta, with
parameters a and b. The posterior density is now
p2 (θ|x, n) ∝θa+x−1 (1 −θ)b+y−1 ,
(12.37)
which is the density Be (θ|a + x, b + y). In this example, the form of the
posterior distribution is known under either prior, so it is straightforward
to draw inferences from (12.36) or from (12.37). To illustrate, independent
samples will be drawn from (12.36), and then importance sampling will be
used to obtain inferences based on (12.37), using the draws from (12.36).
Further, the Monte Carlo-based estimates will then be compared with ex-
act results.
The results for θ = 0.8, obtained with n = x + y = 5 or 15, are shown in
Table 12.1, for three importance sampling sample sizes. The focus of infer-
ence is on the posterior mean and variance, and on the probability that the
value of θ lies between 0.75 and 0.85. In the model that provides the basis
of inference, p2 (θ|x, n), the parameters of the Beta prior are a = b = 3.
The results in the table illustrate that the estimator is consistent: as the
number of samples increases from 4000 to 1 million, the estimates based
on (12.31), (12.32), and (12.33) converge to the true values.
When the probability to be estimated is small, a larger number of im-
portance samples must be drawn to achieve the same level of precision.
For example, the true probability that θ lies between 0.3 and 0.4, based
on p2 (θ|x, n) , is 15.68 × 10−4. Estimates obtained with sample sizes of
four thousand, one hundred thousand and one million were 12.10 × 10−4,
14.66 × 10−4, and 15.75 × 10−4, respectively.
■

560
12. Implementation and Analysis of MCMC Samples
While in this example the importance sampling approach performs sat-
isfactorily, in higher-dimensional problems the relative weights
w

θ(i)
n
i=1
w

θ(i)
may be concentrated on a small number of samples. As a consequence, the
Monte Carlo sampling error associated with estimates of posterior features
is likely to be large. A larger eﬀective sample size is required in order to
mitigate this drawback.

Part IV
Applications in
Quantitative Genetics
561

This page intentionally left blank

13
Gaussian and Thick-Tailed
Linear Models
13.1
Introduction
The fourth part of this book illustrates applications of MCMC methods in
genetic analyses, in a Bayesian context. The treatment, in parts, is rather
schematic, as the objective is to present the mechanics of MCMC sam-
pling in diﬀerent modeling scenarios. Attention is restricted to models that
appear quite often in quantitative genetics, e.g., linear speciﬁcations (uni-
variate and multivariate), binary and ordered polychotomous responses,
longitudinal trajectories, segregation analysis, and the study of QTL.
We start in this chapter with a class of models that is probably the
most common in animal breeding applications, the Gaussian model. Here
the data and other random components are assumed to follow a multi-
variate normal distribution and, further, location parameters and data are
linearly related. The model is discussed in several settings, including situ-
ations where one (univariate) or several (multivariate) response variables
are measured, and where traits may be inﬂuenced by maternal eﬀects.
Also, procedures for robust (in some sense) analysis of linear models are
discussed. The reader should be aware that in several of the applications
discussed below, other approaches may be computationally more eﬃcient
than the MCMC algorithms presented here. Further, an MCMC algorithm
can be tailored in many diﬀerent ways, and it is not claimed that the im-
plementations discussed are, necessarily, the best ones. The ﬁnal section
gives a brief discussion of the impact of the alternative parameterizations
of a linear model on the behavior of Gibbs sampling algorithms.

564
13. Gaussian and Thick-Tailed Linear Models
13.2
The Univariate Linear Additive
Genetic Model
This model was introduced in Example 1.18 of Chapter 1. Genetic aspects
of the model were described brieﬂy in Subsection 1.4.4 of the same chapter.
For analytical details, see Chapter 6.
A phenotypic record for a given trait is modeled as a linear combination
of eﬀects of some explanatory variables. It is assumed that the distribution
of data y (vector of order n) for this trait, given some parameters β, a,
and σ2
e, is the multivariate normal process
y|β, a, σ2
e ∼N

Xβ + Za, Iσ2
e

.
(13.1)
Here β is a vector of “ﬁxed” (in a frequentist sense) eﬀects of order p,
a is the vector of additive genetic values of order q, X and Z are known
incidence matrices associating β and a with y, I is an identity matrix
of order n × n, and σ2
e is the variance of this conditional distribution,
often referred to as the residual variance of the model. Genotypic values
of individuals in a pedigree result from the sum of a very large number of
independent contributions from many independently segregating loci, each
with a small eﬀect. The number of individuals in the pedigree (q) is often
larger than the number of phenotypic records (n), which implies that Z has
q −n null columns. The genetic model justiﬁes invoking the central limit
theorem, which allows us to write
a|A, σ2
a ∼N

0, Aσ2
a

.
(13.2)
Above, A is the additive genetic relationship matrix (of dimension q × q)
and σ2
a is the additive genetic variance in some conceptual or “base” pop-
ulation. From a classical point of view, the parameters of the distribution
(13.2) result from a hypothetical conceptual repeated sampling process in
which vectors of additive genetic values of order q × 1 are drawn at ran-
dom, while maintaining the pedigree constant (i.e., with A ﬁxed in every
repetition of such sampling). Hence, a is called a “random” eﬀect. From
a Bayesian perspective, on the other hand, (13.2) represents the uncer-
tainty distribution about genetic eﬀects before data are observed or, in
other words, it is the prior distribution of such eﬀects. A large value of
σ2
a implies large uncertainty. The parameters which are the focus of in-
ference are β, a, σ2
a, and σ2
e and, possibly, functions thereof, such as the
coeﬃcient of heritability σ2
a/(σ2
a + σ2
e). From a frequentist point of view,
β must be deﬁned uniquely (X must have full-column rank); otherwise,
there is an identiﬁcation problem. Hence, inferences would need to center
on linearly estimable functions of β (Searle, 1971). Hereinafter, only the
Bayesian approach is considered, where the identiﬁcation problem in the-
ory disappears whenever a proper distribution is assigned to β (Bernardo
and Smith, 1994).

13.2 The Univariate Linear Additive Genetic Model
565
To carry out a Bayesian analysis, prior distributions must be assigned to
each of β, a, σ2
a and σ2
e. A distribution which approximates the notion of
vague prior knowledge about β is the ﬂat prior
p (β) ∝constant.
(13.3)
This is an improper prior distribution, which can be made proper by as-
signing upper and lower limits to each of the elements of β. In this case,
the posterior distribution will then be deﬁned within these assigned limits.
In a Bayesian model with known variance components, use of priors (13.2)
and (13.3) yields normal marginal posterior distributions for both β and
a, with mean values equal to the ML estimator of β and to the BLUP of
a, respectively (see Chapter 6).
Two common prior speciﬁcations for the variance components are either
proper uniform distributions or scaled inverted chi-square distributions.
The corresponding densities have the forms
p

σ2
i

=
1
σ2
i max
,
0 < σ2
i < σ2
i max, i = a, e,
(13.4)
and
p

σ2
i |νi, S2
i

∝

σ2
i
−(
νi
2 +1) exp

−νiS2
i
2σ2
i

,
i = a, e.
(13.5)
In (13.4), σ2
i max is the maximum value that σ2
i is allowed to take, according
to mechanistic considerations or prior knowledge about the trait. In (13.5),
νi and S2
i are parameters of the corresponding scaled inverted chi-square
distribution. Here it is assumed that these hyperparameters (like σ2
i max)
are known; otherwise, these can be assigned prior distributions, in a hi-
erarchical manner. The prior speciﬁed by (13.5) reduces to an improper
uniform distribution by taking νi = −2 and S2
i = 0.
Assuming that β,

a, σ2
a

, and σ2
e are independent a priori , the joint
posterior density of all unknown quantities is proportional to
p

β, a, σ2
a, σ2
e|y

∝p (β) p

a|σ2
a

p

σ2
a

p

σ2
e

p

y|β, a, σ2
e

.
(13.6)
In the notation, conditioning on hyperparameters is omitted. Using (13.1),
(13.2), (13.3) and, for instance, (13.5), the joint posterior density (13.6) is
given by
p

β, a, σ2
a, σ2
e|y

∝

σ2
e
−(
n+νe
2
+1) 
σ2
a
−(
q+νa
2
+1)
× exp

−(y −Xβ −Za)′ (y −Xβ −Za) + νeS2
e
2σ2e

exp

−a′A−1a + νaS2
a
2σ2a

.
(13.7)
The joint posterior density of a model that assumes instead improper uni-
form prior distributions for the variance components, is obtained by setting

566
13. Gaussian and Thick-Tailed Linear Models
νi = −2 and S2
i = 0 in (13.7). A word of caution is in order here: when
improper priors are assigned to

β, σ2
a, σ2
e

, the posterior distribution may
not always be proper (Hobert and Casella, 1996).
13.2.1
A Gibbs Sampling Algorithm
The single-site, systematic scan Gibbs sampling algorithm described be-
low is based on the fully conditional posterior distributions of each scalar
parameter. As usual, these are deduced from the joint posterior (13.7).
However, for the location parameters a and β, the derivation is simpler if
one appeals to the more general results for mixed linear models given in
Chapter 6; see also Example 1.18 in Chapter 1. First, note that the fully
conditional posterior density of a and β is proportional to
p

β, a|σ2
a, σ2
e, y

∝p (β) p

a|σ2
a

p

y|β, a,σ2
e

.
(13.8)
Rather than manipulating this expression, we proceed as follows. As in
Example 1.18 of Chapter 1, let
Xβ + Za = Wθ,
and
Σ =

0
0
0
A−1k

,
where W =

X
Z

, θ =

β′, a′′, and k = σ2
e/σ2
a. Then, using results
presented in Chapter 6, the conditional posterior distribution of θ is
θ|σ2
a, σ2
e, y ∼N

5θ, C−1σ2
e

,
(13.9)
where C = W′W + Σ, and the posterior mean 5θ is the solution to the
linear system:
C5θ = W′y = r.
(13.10)
One way of deriving the fully conditional posterior distribution of the ith
element of θ is as follows. Let θ−i be θ, except for the ith element (θi) ,
which is removed from the entire location vector. The results that follow
hold irrespective of whether or not θi is a scalar or a vector. Based on
Example 1.18, one obtains
θi|θ−i, σ2
a, σ2
e, y ∼N

,θi, C−1
i,i σ2
e

(13.11)
where ,θi satisﬁes
Ci,i,θi = (ri −Ci,−iθ−i) .
(13.12)
For example, when θi is a scalar, Ci,i is the diagonal element of the coef-
ﬁcient matrix of the mixed model equations (13.10), ri is the ith element

13.2 The Univariate Linear Additive Genetic Model
567
of the right-hand side vector in (13.10) associated with θi, and Ci,−i is a
row vector obtained by deleting element i from the ith row of C. Notice
that drawing samples from the distribution

θi|θ−i, σ2
a, σ2
e, y

, as required
in Gibbs sampling, does not require inversion of matrices if θi is a scalar.
Expression (13.11) is quite general and applies, with appropriate minor
changes, to all fully conditional densities of location parameters in Gaus-
sian linear models.
In order to derive the fully conditional posterior distribution of the vari-
ance components, one retains only those terms in (13.7) that involve the rel-
evant variance component. Note that, given θ =

β′, a′′, the two variance
components are conditionally independent. The fully conditional posterior
distribution of σ2
a is given by
p

σ2
a|β, a, σ2
e, y

∝

σ2
a
−(
q+νa
2
+1) exp

−a′A−1a + νaS2
a
2σ2a

=

σ2
a
−(
νa
2 +1) exp

−,νa ,S2
a
2σ2a

,
(13.13)
where
,S2
a =

a′A−1a + νaS2
a

/,νa
and ,νa = q + νa. By inspection, it follows that (13.13) is in the form of
a scaled inverted chi-square density with parameters ,νa and ,S2
a. In short,
one can then write
σ2
a|β, a, σ2
e, y ∼,νa ,S2
aχ−2
νa .
(13.14)
To sample from (13.14), a draw is made from a chi-square distribution
with ,νa = q + νa degrees of freedom, and the reciprocal of this number
is multiplied by ,νa ,S2
a =

a′A−1a + νaS2
a

. The resulting quantity is a
realization from the scaled inverted chi-square process (13.14).
Similarly, collecting those terms from (13.7) that involve σ2
e only yields
p

σ2
e|β, a, σ2
a, y

∝

σ2
e
−(
n+νe
2
+1)
× exp

−(y −Xβ −Za)′ (y −Xβ −Za) + νeS2
e
2σ2e

=

σ2
e
−(
νe
2 +1) exp

−,νe ,S2
e
2σ2e

,
(13.15)
where ,νe = n + νe and
,S2
e =

(y −Xβ −Za)′ (y −Xβ −Za) + νeS2
e

/,νe.
By inspection, (13.15) is proportional to the density of the following scaled
inverted chi-square distribution with parameters ,νe and ,S2
e
σ2
e|β, a, σ2
a, y ∼,νe ,S2
eχ−2
νe .
(13.16)

568
13. Gaussian and Thick-Tailed Linear Models
ID
Father
Mother
Sex
Record
1
−
−
1
−
2
−
−
2
−
3
1
−
1
y3
4
1
2
2
y4
5
3
4
1
y5
6
1
4
2
y6
7
5
6
1
−
TABLE 13.1. A hypothetical example.
The implementation of the Gibbs sampler consists of drawing succes-
sively from (13.11) for each location parameter (or block of parameters
whenever θi is vector valued), and from the distributions (13.14) and
(13.16). The process is repeated as needed to satisfy convergence require-
ments, and to attain a reasonably small Monte Carlo error.
Example 13.1
An additive genetic model
Consider the data in Table 13.1. There are seven individuals, but only those
numbered as 3, 4, 5, and 6 have a phenotypic record. Hence, the pedigree
information available involves a total of q = 7 individuals, and n = 4. Note
that subjects 4 to 7 have known mothers and fathers, whereas the parents
of subjects 1 and 2 are unknown; for individual 3, only the father is known.
Suppose the sex of the individual is the only source of heterogeneity, other
than the subjects themselves. Hence, the vector β contains the eﬀects of
sex [S1, S2]′ on the trait, and the vector of additive genetic values of the
seven individuals is a = [a1, a2, . . . , a7]′. The variance components will be
assigned uniform distributions a priori, as in (13.4), to convey (naively) a
state of vague prior knowledge about their values. Arbitrary starting values
adopted for the Gibbs sampler are: β(0)= 0, a(0)= 0, σ2(0)
a
= 5, σ2(0)
e
= 5,
therefore, the implied starting value for the ratio of variance component is
k(0) = 1. A convenient algorithm for drawing samples from (13.11) is based
on the mixed model equations. Note that C is a 9 × 9 matrix, the order
resulting from p = 2 and q = 7. The ﬁrst seven columns of the coeﬃcient
matrix of the mixed model equations are


2.00
0.00
0.00
0.00
1.00
0.00
1.00
0.00
2.00
0.00
0.00
0.00
1.00
0.00
0.00
0.00
2.33k
0.50k
−0.66k
−0.50k
0.00k
0.00
0.00
0.50k
1.50k
0.00k
−1.00k
0.00k
1.00
0.00
−0.66k
0.00k
1 + 2.83k
0.50k
−1.00k
0.00
1.00
−0.50k
−1.00k
0.50k
1 + 3k
−1.00k
1.00
0.00
0.00k
0.00k
−1.00k
−1.00k
1 + 2.62k
0.00
1.00
−1.00k
0.00k
0.00k
−1.00k
0.62k
0.00
0.00
0.00k
0.00k
0.00k
0.00k
−1.23k


,

13.2 The Univariate Linear Additive Genetic Model
569
and the last two columns are


0.00
0.00
1.00
0.00
−1.00k
0.00k
0.00k
0.00k
0.00k
0.00k
−1.00k
0.00k
0.62k
−1.23k
1 + 2.62k
−1.23k
−1.23k
2.46k


.
The location parameters are
θ = [S1, S2, a1, . . . , a7]′ .
The single-site, systematic Gibbs sampler draws the parameters at each
iteration in the order in which they appear in θ above. At iteration 1, the
ﬁrst draw from the fully conditional distribution, with density
p

S1|S2, a1, . . . , a7, σ2
e, y

,
is obtained as:
S(1)
1 |S(0)
2 , a(0)
1 , . . . , a(0)
7 , σ2(0)
e
, σ2(0)
a
, y ∼N

5S(1)
1 , σ2(0)
e
2

,
where, in view of the starting values adopted, 5S(1)
1
= (y3 + y5) /2. Next,
draw S(1)
2
from
S(1)
2 |S(1)
1 , a(0)
1 , . . . , a(0)
7 , σ2(0)
e
, σ2(0)
a
, y ∼N

5S(1)
2 , σ2(0)
e
2

,
where 5S(1)
2
= (y4 + y6) /2. Subsequently, draw a(1)
1
from
a(1)
1 |S(1)
1 , S(1)
2 , a(0)
2 , . . . , a(0)
7 , σ2(0)
e
, σ2(0)
a
, y ∼N

5a(1)
1 ,
σ2(0)
e
2.33k(0)

,
where 5a(1)
1
= 0/2.33k(0) = 0. The process is continued systematically for
the genetic eﬀects of individuals 2, 3, 4, 5, 6, and 7. For example, additive
genetic value 6 in iteration 1, a(1)
6 , is sampled from
a(1)
6 |S(1)
1 , S(1)
2 , a(1)
1 , . . . , a(1)
5 , a(0)
7 , σ2(0)
e
, σ2(0)
a
, y ∼N

5a(1)
6 ,
σ2(0)
e
1 + 2.62k(0)

,

570
13. Gaussian and Thick-Tailed Linear Models
where
5a(1)
6
=
y6 −S(1)
2
−k(0) 
−1.00a(1)
1
−1.00a(1)
4
+ 0.62a(1)
5
−1.23a(1)
7


1 + 2.62k(0)
.
Finally, for additive genetic value 7, sample a(1)
7
from
a(1)
7 |S(1)
1 , S(1)
2 , a(1)
1 , . . . , a(1)
6 , σ2(0)
e
, σ2(0)
a
, y ∼N

5a(1)
7 ,
σ2(0)
e
1 + 2.46k(0)

,
where
5a(1)
7
=

0 −k(0) 
−1.23a(1)
5
−1.23a(1)
6

2.46k(0)
.
Having sampled all location parameters, one proceeds to drawing the two
variance components, to complete the ﬁrst iteration of the sampler. First,
the following sums of squares are computed:
SS(1)
e
=

y −Xβ(1) −Za(1)′ 
y −Xβ(1) −Za(1)
,
and
SS(1)
a
= a′(1)A−1a(1).
The ﬁrst-round sample for the additive genetic variance, σ2(1)
a
, is extracted
from
σ2
a|β, a, y ∼SS(1)
a χ−2
7−2,
and the residual variance σ2(1)
e
from
σ2
e|β, a, y ∼SS(1)
e χ−2
4−2.
Algorithmically, the second round of iteration starts by updating the ratio
of variance components k(1) = σ2(1)
e
/σ2(1)
a
, followed by updating the coeﬃ-
cient matrix of the mixed model equations. The iteration then proceeds as
sketched above.
■
13.3
Additive Genetic Model with
Maternal Eﬀects
This model was proposed by Willham (1963), and has been used widely
in animal breeding applications. Here, it is assumed that an oﬀspring’s
attribute (or phenotypic record) is aﬀected by the “usual” genetic and
environmental eﬀects, plus a contribution from its mother’s phenotype. The
latter may include both genetic and nongenetic components of maternal

13.3 Additive Genetic Model with Maternal Eﬀects
571
origin, but it will be assumed here that the maternal eﬀect is only genetic
in nature, although it acts as an environmental inﬂuence on the oﬀspring’s
record. The maternal inﬂuence is transmitted in a Mendelian manner, so
both males and females carry genes for maternal eﬀects. Naturally, genetic
diﬀerences for maternal eﬀects can be assessed only when females produce
oﬀspring.
A typical example of a trait aﬀected by maternal eﬀects is body weight
at weaning in cattle or sheep. At weaning time, variation in body weight
can be partly attributed to the calf’s genes (direct genetic eﬀects), by the
pre- and post-natal environment provided by the dam of the animal (e.g.,
amount of milk available for suckling), and to environmental eﬀects stem-
ming from sources other than the mother’s inﬂuence. Again, although the
maternal eﬀects are environmental vis-a-vis the measurement made in the
oﬀspring, part of the variation in milk yield between dams is assumed to be
additive genetic. The calf or lamb receives a sample of 50% of its mother’s
and father’s autosomal genes for milk yield. If the calf is female, and if it
becomes a mother, these genes will inﬂuence her oﬀspring’s weaning weight.
With this model, the dispersion parameters of interest are:
(1) the additive genetic variance of “direct” eﬀects aﬀecting the trait e.g.,
calf’s weaning weight;
(2) the additive genetic variance for maternal eﬀects;
(3) a possible additive genetic covariance between direct and maternal ef-
fects; and
(4) the residual variance.
As usual, interest may also focus on the posterior distribution of location
parameters or functions thereof, and on genetic parameters such as the
heritabilities of direct and maternal eﬀects, and the genetic correlation
between direct and maternal eﬀects. For applications to livestock breeding
and for variations of the model, see Van Vleck (1993); an extension of
Willham’s speciﬁcation can be found in Koerkhuis and Thompson (1997).
A Bayesian analysis of a maternal eﬀects model via the Gibbs sampler was
described by Jensen et al. (1994).
The model used here to illustrate the algorithm is as follows
y = Xβ + Zmm + Zaa + e.
(13.17)
The notation is as for model (13.1), with the only novelty being the in-
troduction of m, a vector of order q of maternal additive genetic values;
a is now the vector of order q of direct additive genetic values, and Zm
and Za are known incidence matrices associating the data to m and to a,
respectively. The model is written in a manner that allows each individual
in a pedigree to have both direct and maternal genetic eﬀects, and this
is the reason for which the order of the two vectors is q × 1; this will be
illustrated below

572
13. Gaussian and Thick-Tailed Linear Models
ID
Father
Mother
Sex
Record
1∗
−
−
2
−
2
−
−
1
−
3
−
−
2
−
4
2
1∗
1
y4
5
2
3
2
y5
6
4
5
1
y6
7
2
5
2
y7
8
6
7
1
−
TABLE 13.2. Hypothetical data structure from a maternal eﬀects model.
Example 13.2
Incidence matrices in a maternal eﬀects model
To illustrate the structure of matrices Zm and Za, we revert to the example
in Table 13.1. Notice that individual 3 does not have a known mother. It
simpliﬁes matters algorithmically, if a “phantom” mother is created. After
creating the “phantom” mother (1∗) of individual 3 (which now becomes
individual 4, as all individuals are renumbered), the data looks as in Table
13.2.
Based on the data in Table 13.2, the structure of the matrix Zm, of order
4 × 8 and which links a data point to its mother, is


1
0
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
1
0
0
0

,
and note that the columns are nonnull only for those individuals that are
mothers and that have a measured progeny (i.e., 1∗, 3, 5). Likewise, the
structure of the matrix Za, also of order 4 × 8, is


0
0
0
1
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
1
0

.
Here, the nonnull columns pertain to the individuals with measurements
(4 to 7) .
■
Proceeding with the deﬁnition of the parameters of the model, let G0
be a 2 × 2 variance–covariance matrix whose diagonal elements are σ2
m,
the maternal additive genetic variance, and σ2
a, the direct additive genetic
variance; σa,m, the additive genetic covariance between direct and mater-
nal eﬀects, is in the oﬀ-diagonal position. The genetic correlation between
maternal and direct additive genetic eﬀects is, then
ra,m = σa,m
σaσm
.

13.3 Additive Genetic Model with Maternal Eﬀects
573
The inverse of the matrix G0 is
G−1
0
=

σ2
m
σa,m
σa,m
σ2
a
−1
=

gm,m
gm,a
gm,a
ga,a

.
The covariance structure associated with the entire vector of genetic eﬀects
[m′, a′]′ is
G =V ar

m
a

= G0 ⊗A,
(13.18)
where the symbol ⊗stands, as usual, for “direct product”, and A is the
additive genetic relationship matrix.
According to the model postulated, the variance of y (in the frequentist
sense, where both β and G0 are ﬁxed parameters) is given by
V ar (y) = V ar (Xβ + Zmm + Zaa + e)
=

Zm
Za

V ar

m
a
 
Z′
m
Z′
a

+ V ar (e)
=

Zm
Za

[G0 ⊗A]

Z′
m
Z′
a

+ Iσ2
e
= ZmAZ′
mσ2
m + ZaAZ′
aσ2
a + ZaAZ′
mσa,m + ZmAZ′
aσa,m+Iσ2
e.
The scalar version of this expression is as follows. Let ya and yb represent
records of individuals a and b, respectively. Let the mothers of a and b be
c and d, respectively. Then, if Aij denotes the additive genetic relationship
between i and j:
Cov (ya, yb) = Aabσ2
a + Acdσ2
m + (Aad + Abc) σa,m
with the extra term σ2
e when a = b.
Return now to the Bayesian implementation via Gibbs sampling. As in
(13.1) it is assumed that, conditionally on all location eﬀects β, m, a, and
on the residual variance σ2
e, the data are a realization from the normal
process
y|β, m, a, σ2
e ∼N

Xβ + Zmm + Zaa, Iσ2
e

.
(13.19)
The joint prior distribution of m and a, under the assumptions of the
inﬁnitesimal model, is
 m
a
"""" A, G0 ∼N


0
0

, G0 ⊗A

.
(13.20)
On deﬁning g = [m′, a′]′, one can write the corresponding density as
p (g|A, G0) = |2πG|−1
2 exp

−1
2g′ 
G−1
0
⊗A−1
g

∝|G0|−q
2 exp

−1
2g′ 
G−1
0
⊗A−1
g

.
(13.21)

574
13. Gaussian and Thick-Tailed Linear Models
This expression is now written in the form of Example 1.17 in Chapter 1,
since this makes it easier to derive the fully conditional posterior distribu-
tion of G0. Deﬁne
Sg =

m′A−1m
m′A−1a
a′A−1m
a′A−1a

.
Then
g′ 
G−1
0
⊗A−1
g =

m′
a′  
gm,mA−1
gm,aA−1
gm,aA−1
ga,aA−1
 
m
a

= tr

G−1
0 Sg

.
Therefore (13.21) can be expressed as
p (g|A, G0) ∝|G0|−q
2 exp

−1
2 tr

G−1
0 Sg

.
(13.22)
The vector β is assigned again the uniform prior distribution, with den-
sity
p (β) ∝constant.
The residual variance is assumed to follow, a priori, a scaled inverted chi-
square distribution with density
p

σ2
e|νe, S2
e

∝

σ2
e
−(
νe
2 +1) exp

−νeS2
e
2σ2e

,
(13.23)
where νe and S2
e are hyperparameters, assumed known. Finally, the covari-
ance matrix G0 is assumed to follow, a priori, a scaled, two-dimensional,
inverse Wishart distribution having density function
p (G0|V, υ) ∝|G0|−1
2 (υ+k+1) exp

−1
2tr

G−1
0 V−1
,
(13.24)
where k, the dimension of G0, is equal to 2 here. The density (13.24) is
symbolized as IW (V, υ) , and it reduces to a two-dimensional improper
uniform distribution by setting υ = −(k + 1) and V = 0.
The joint posterior density of all parameters is given by
p

β, m, a, G0, σ2
e|y

∝p

y|β, m, a, σ2
e

p

β, m, a, G0, σ2
e

∝p

y|β, m, a, σ2
e

p (m, a|G0) p (G0|V, υ) p

σ2
e|νe, S2
e

,
(13.25)
with the second line of (13.25) arising in view of the prior assumed for
β, and because σ2
e and (m, a, G0) are assumed to be independent, a pri-
ori. This joint density (distribution) is the basis for deriving all the fully
conditional posterior densities (distributions) needed for implementing the
Gibbs sampler.

13.3 Additive Genetic Model with Maternal Eﬀects
575
13.3.1
Fully Conditional Posterior Distributions
As in the previous section, in order to derive the fully conditional posterior
distributions of β, m, and a, let
Xβ + Zmm + Zaa = Wθ,
(13.26)
where
W =

X
Zm
Za

,
and put
C = W′W + Σ,
with
Σ =


0
0
0
0
A−1km,m
A−1km,a
0
A−1km,a
A−1ka,a

.
(13.27)
In (13.27), kmm = σ2
egm,m, km,a = σ2
egm,a, and ka,a = σ2
ega,a. Then, as in
(13.11), the fully conditional posterior distribution of θi is
θi|θ−i, G0, σ2
e, y ∼N

5θi, C−1
i,i σ2
e

(13.28)
where 5θi satisﬁes
Ci,i5θi = ri −Ci,−iθ−i.
(13.29)
To derive the fully conditional posterior distribution of the residual vari-
ance note, from (13.25), that the only terms that include σ2
e are the ﬁrst
and the last ones in the joint density. Therefore,
p

σ2
e|β, m, a, G0, y

∝p

y|β, m, a, σ2
e

p

σ2
e

∝

σ2
e
−n
2 exp

−(y −Wθ)′ (y −Wθ)
2σ2e
 
σ2
e
−(
νe
2 +1) exp

−νeS2
e
2σ2e

∝

σ2
e
−(
νe
2 +1) exp

−,νe ,S2
e
2σ2e

,
(13.30)
where ,νe = νe + n and
,S2
e = (y −Wθ)′ (y −Wθ) + νeS2
e
,νe
.
Expression (13.30) is proportional to a scaled inverted chi-square density,
with ,νe degrees of freedom and scale parameter ,S2
e. Thus
σ2
e|β, m, a, G0, y ∼,νe ,S2
eχ−2
νe .
(13.31)

576
13. Gaussian and Thick-Tailed Linear Models
Finally, the fully conditional posterior distribution of G0 is obtained by
retaining those terms in (13.25) that involve G0:
p

G0|β, m, a, σ2
e, y

∝p (m, a|G0) p (G0|V, υ)
∝|G0|−q
2 exp

−1
2tr

G−1
0 Sg

|G0|−1
2 (υ+k+1) exp

−1
2tr

G−1
0 V−1
.
Collecting terms yields
p

G0|β, m, a, σ2
e, y

∝|G0|−1
2 (υ+q+k+1) exp

−1
2tr

G−1
0

Sg + V−1%
(13.32)
which can be recognized as the kernel of a 2 × 2 scaled inverted Wishart
distribution (k = 2), with degrees of freedom equal to v+q and scale matrix
Sg + V−1. Hence, we write
G0|β, m, a, σ2
e, y ∼IW2

Sg + V−1−1 , v + q

.
(13.33)
The Gibbs sampler proceeds by sampling from distribution (13.28), either
elementwise or blockwise, and from (13.31) and (13.33).
13.4
The Multivariate Linear Additive
Genetic Model
This section describes a Gibbs sampler for making Bayesian inferences
based on a multiple-trait mixed linear model. This is a model that applies
to a situation where several response variables are measured simultane-
ously on an individual. For example, Smith (1936) considered the problem
of selecting among varieties of wheat diﬀering in yield and quality traits;
Hazel (1943) applied some of the ideas to pig breeding schemes, where body
weights and scores had been collected in each of the animals. The “selec-
tion index” procedures suggested by these authors depend on knowledge of
genetic and environmental correlations between traits, and these must be
inferred using some multivariate model.
The developments presented here are circumscribed to a two-trait case,
but extension to more response variables is straightforward. An arbitrary
pattern of missing data will be assumed, and use is made of the ideas of data
augmentation to “ﬁll-in” the missing observations. This simpliﬁes the Gibbs
sampler, because the fully conditional posterior distributions of residual
covariance matrices follow standard inverse Wishart distributions, at least
for certain forms of the prior distribution. An important assumption is
that the missing data are missing at random, in the sense deﬁned by Rubin
(1976). This means that the pattern of missing data may depend on the
observed data, but not on the missing data. In this case, the missing data

13.4 The Multivariate Linear Additive Genetic Model
577
mechanism can be ignored in the formulation of the probabilistic model,
simplifying matters considerably. If, on the other hand, the data are not
missing at random, it is necessary to model the missing data mechanism,
which requires making extra assumptions, and additional parameters enter
into the model. See Rubin (1987a), Little and Rubin (1987), Gelman et al.
(1995), and Schafer (2000) for a comprehensive discussion of the issues.
Consider two traits denoted by Y and Z, and let yo and zo denote the
vectors of observed data for traits Y and Z, respectively. The individuals are
hypothetically from a polytocous (litter bearing) species, so that there may
be eﬀects common to individuals raised in the same litter. Ideally, each of
the individuals would have measurements for both traits, but this is seldom
the case, at least with animal breeding data, where some individuals will
have records for both Y and Z, whereas others will have measurements for
either Y or Z only. Suppose that the number of individuals having at least
one record is n, that is, n = nY Z + nY + nZ, where nY Z is the number
of individuals having records for each of the two traits, and nY (nZ) is
the number of subjects with records only for trait Y (Z). If there were
no missing records, the vector of “complete” data would have dimension
2n×1. Let ym and zm be the vectors of missing data, and let the complete
data vectors then be y′ = [y′
o, y′
m] and z′ = [z′
o, z′
m]. The following model
will be assumed

y
z

=

Xy
0
0
Xz
  βy
βz

+

Wy
0
0
Wz
 
ly
lz

+

Zy
0
0
Zz
 
ay
az

+

ey
ez

,
(13.34)
where βy (βz) is a vector of “ﬁxed eﬀects” aﬀecting trait Y (Z), ly (lz) is
a vector of litter eﬀects of order s (s), ay (az) is the vector of order q (q)
of additive genetic values for trait Y (Z) and ey (ez) is a vector of residual
eﬀects of order n (n) for trait Y (Z). As stated, the litter eﬀect parameters
account for the common environment aﬀecting individuals that are raised
together (often, but not always, contemporaneous full-sibs). Alternatively,
they could represent, for example, “pen” or “cage” eﬀects in laboratory
animals; if such eﬀects do not exist, they can be removed from the model.
Matrices X, W, and Z, with subscripts y and z, are known incidence arrays
relating location eﬀects for each trait to the data. This is a model that could
be used, for example, to analyze data on daily gain and feed intake in pigs.
Put now β =

β′
y, β′
z
′ , l =

l′
y, l′
z
′ , a =

a′
y, a′
z
′, with appropriate
partitions for matrices X, W and Z, such that
X =

Xy
0
0
Xz

, W =

Wy
0
0
Wz

, Z =

Zy
0
0
Zz

.
The conditional distribution of the complete data for each individual, given
the parameters, is assumed to be bivariate normal. For all individuals the

578
13. Gaussian and Thick-Tailed Linear Models
distribution can be written as
v|β, l, a, Re ∼N (Xβ + Wl + Za, R) ,
(13.35)
where v contains y and z. We shall assume that records have been sorted
by individual and trait within individual, so that v is a sequence of (Y, Z)
values for each individual. Pairs of records from diﬀerent individuals are
assumed to be conditionally independent, given the parameters, but a cor-
relation between residuals of the same individual is allowed. Hence, the
sorting is such that the residual variance–covariance matrix can be written
as R = In⊗Re, a block diagonal matrix with n submatrices of residual co-
variances Re (of order 2 × 2) and, as usual, In is the n × n identity matrix.
The residual dispersion matrix is
Re =
 σ2
e,y
σe,yz
σe,yz
σ2
e,z

,
where σ2
e,y is the residual variance for trait Y, σ2
e,z is the residual vari-
ance for trait Z, and σe,yz is the residual covariance. The residual correla-
tion σe,yz/ (σe,yσe,z) is a measure of the association between traits due to
sources other than genetic and litter (or pen) eﬀects.
Prior distributions are speciﬁed now, starting with the location eﬀects.
For the vector β, a proper uniform distribution is assigned, with density
p (β) ∝constant,
(13.36)
with possible boundaries, βmin, βmax, to ensure propriety of the joint pos-
terior distribution. The vector of litter eﬀects is assumed to have a prior
uncertainty distribution well-reﬂected by the multivariate normal process
l|Rl ∼N (0, Rl ⊗Is) ,
(13.37)
where Is is the s × s identity matrix, and Rl is the covariance matrix
Rl =
 σ2
l,y
σl,yz
σl,yz
σ2
l,z

.
Above, σ2
l,y

σ2
l,z

is the variance between litter eﬀects for trait Y (Z), and
σl,yz is a covariance component. The vector of additive genetic values is
assumed to follow, a priori, the multivariate normal distribution
a|G0, A ∼N (0, G0 ⊗A) ,
(13.38)
where A is the additive genetic relationship matrix of order q × q, and
G0 =
 σ2
a,y
σa,yz
σa,yz
σ2
a,z


13.4 The Multivariate Linear Additive Genetic Model
579
is a 2×2 matrix, whose elements are the additive genetic (co)variance com-
ponents. The genetic correlation between traits, σa,yz/ (σa,yσa,z) measures
the strength of the linear association between additive eﬀects for traits Y
and Z.
Two-dimensional scaled inverted Wishart distributions are assigned as
prior processes for each of the Re, Rl, and G0 covariance matrices, with
the respective densities being
p (Re|υe, Ve) ∝|Re|−1
2 (υe+k+1) exp

−1
2tr

R−1
e V−1
e

,
(13.39)
p (Rl|υl, Vl) ∝|Rl|−1
2 (υl+k+1) exp

−1
2tr

R−1
l
V−1
l

,
(13.40)
and
p (G0|υa, Va) ∝|G0|−1
2 (υa+k+1) exp

−1
2tr

G−1
0 V−1
a

,
(13.41)
where k = 2 in our hypothetical bivariate model. In these expressions,
υi and Vi (i = e, l, a) are hyperparameters of the distributions, which are
assumed known. As mentioned in connection with (13.24), these inverse
Wishart distributions reduce to improper uniform distributions, if υi =
−(k + 1) and Vi = 0.
The joint posterior density of all parameters (after augmentation with
the missing records), allowing for dependence of the distribution of the
litter and additive eﬀects on the corresponding covariance matrices, but
assuming prior independence otherwise, is given by
p (ym, zm, β, l, a, Re, Rl, G0|yo, zo)
∝p (ym, zm, β, l, a, Re, Rl, G0) p (yo, zo|ym, zm, β, l, a, Re)
= p (ym, yo, zm, zo|β, l, a, Re) p (l|Rl) p (Rl) p (a|G0) p (G0) p (Re) .
(13.42)
This density is deﬁned within the bounds speciﬁed in connection with the
prior (13.36). Note that the ﬁrst term in (13.42) is the conditional density
of the complete data, deﬁned in (13.35). The fully conditional posterior
distributions are derived from (13.42) by proceeding in the usual manner,
that is, ﬁxing the appropriate conditioning variables in the joint density.
Here scaled inverse Wishart distributions were chosen as prior speciﬁca-
tions for the covariance matrices. This makes implementation of the Gibbs
sampler straightforward. Clearly, other prior speciﬁcations could be chosen,
such as assigning independent prior distributions to the 3 elements of the
covariance matrices, and using a parameterization in terms of correlations.

580
13. Gaussian and Thick-Tailed Linear Models
13.4.1
Fully Conditional Posterior Distributions
Imputation of Missing Records
Since the missing records are treated as unknowns in the probability model
having density (13.42), their values must be imputed via Gibbs sampling
by eﬀecting draws from their fully conditional posterior distributions. First,
note from the joint posterior density that
p (zm|ym, β, l, a, Re, Rl, G0, yo, zo) ∝p (ym, yo, zm, zo|β, l, a, Re)
∝p (zm|β, l, a, Re, yo, zo)
∝p (zm|β, l, a, Re, yo) .
(13.43)
The preceding follows because:
(1) Missingness is at random, so the distribution of missing records depends
on the observed data, and not on the missing observations.
(2) Given β, l, a, Re, the missing observations for trait Z do not depend
on the observed records for this trait, as these have been measured on
other individuals, and observations from diﬀerent subjects are conditionally
independent.
Furthermore, because of such conditional independence,
p (zm|β, l, a, Re, yo) =
-
i∈MZ
p (zm,i|β, l, a, Re, yo,i) ,
(13.44)
where zm,i is the missing record, and yo,i is the observed record for subject
i in the set MZ of individuals with missing data for trait Z, comprising nY
members. Similarly
p (ym|zm, β, l, a, Re, Rl, G0, yo, zo) ∝
-
i∈MY
p (ym,i|β, l, a, Re, zo,i) ,
(13.45)
where now ym,i is the missing record and zo,i is the observed record for sub-
ject i in the set MY of individuals with missing data for trait Y, comprising
nZ subjects.
Consider an individual i with a record for trait Y and no record for Z.
Using (13.35), and the results from multivariate normal theory, the fully
conditional posterior distribution of zm,i is
zm,i|yo,i, β, l, a, Re ∼N

5zm,i, Vzm,i

,
(13.46)
where
5zm,i = E (zm,i|yo,i, β, l, a, Re)
= x′
mz,iβz + w′
mz,ilz+z′
mz,iaz + σe,yz
σ2e,y

yo,i −x′
oy,iβy −w′
oy,ily −z′
oy,iay

(13.47)

13.4 The Multivariate Linear Additive Genetic Model
581
and
Vzm,i = σ2
e,z

1 −
σ2
e,yz
σ2e,yσ2e,z

.
(13.48)
In these expressions, x′
mz,i is the row of Xz associating βz to zm,i, w′
mz,i is
the row of Wz associating lz to zm,i, and z′
mz,i is the row of Zz associating
az to zm,i. If, instead, individual i has an observation for trait Z and lacks a
record for Y , the fully conditional posterior distribution for ym,i is derived
in a similar manner. Thus, the draws for the missing records for Z can be
done from (13.46), or from its counterpart when Y is the missing trait.
In this computing strategy, generation of the missing data requires knowl-
edge of elements of the incidence matrices, that is, of the way that location
eﬀects enter into the missing record. Often, for some missing records, this
information may not be available. An alternative strategy, which is com-
putationally simpler, and which avoids knowledge of incidence matrices
altogether, is to generate “missing residuals” instead of missing observa-
tions, as suggested by Van Tassell and Van Vleck (1996) and Wang et al.
(1997). In this setting, the joint posterior distribution is augmented with
the “missing residuals” in lieu of the missing records. The missing residuals
are sampled from a normal distribution with mean
5emz,i = σe,yz
σ2e,y

yo −x′
oy,iβy −w′
oy,ily −z′
oy,iay

.
The variance of the fully conditional posterior distribution of the missing
residual is as in (13.48). Similar expressions apply to the “missing residuals”
for trait Y.
Location Eﬀects
The derivation of the fully conditional posterior distributions of the location
parameters β, l and a is similar to the one leading to (13.9), with a slight
modiﬁcation needed to accommodate the multiple-trait case. Let
θ =

β′, l′, a′′ ,
M =

Xy
0y
Wy
0y
Zy
0y
0z
Xz
0z
Wz
0z
Zz

,
R = Re ⊗In,
noting that this implies the sorting of individuals within trait,
Ω=


0
0
0
0
R−1
l
⊗Is
0
0
0
G−1
0
⊗A−1

,
and
C = M′R−1M + Ω.

582
13. Gaussian and Thick-Tailed Linear Models
The multiple-trait mixed model equations are
C5θ = t
(13.49)
where the right-hand side vector t is equal to
t
=
M′R−1v
=
M′R−1

y
z

.
Then the fully conditional posterior distribution of θ is
θ|Re, Rl, G0, y, z ∼N

5θ, C−1
,
(13.50)
and the multiple-trait version of (13.11) is, for any sub-vector θi of θ,
θi|θ−i, Re, Rl, G0, y, z ∼N

,θi, C−1
i,i

,
(13.51)
where ,θi satisﬁes
Ci,i,θi = ti −Ci,−iθ−i.
(13.52)
As before, when the Gibbs sampler is implemented in a scalar mode (draw-
ing from the fully conditional posterior distributions one element of θ at a
time), θi and Ci,i above are scalars and Ci,−i is a row vector. On the other
hand, when the entire location vector θ is sampled (as discussed later on),
the oﬀset Ci,−iθ−i is null, and (13.51) reproduces (13.50).
Dispersion Matrices
The fully conditional posterior distributions of the dispersion matrices are
derived next. From (13.42):
p (Re|β, l, a, Rl, G0, y, z) ∝p (y, z|β, l, a, Re) p (Re) .
(13.53)
Deﬁne
Se =
 e′
yey
e′
yez
e′
yez
e′
zez

,
where
ey = y −Xyβy −Wyly −Zyay,
and
ez = z −Xzβz −Wzlz −Zzaz.
The density of the conditional distribution of the complete data, given β,
l, a, Re, can be expressed as
p (y, z|β, l, a, Re) ∝|R|−1
2 exp

−1
2tr

R−1
e Se

= |Re|−n
2 exp

−1
2tr

R−1
e Se

.

13.4 The Multivariate Linear Additive Genetic Model
583
Combining this with prior (13.39) yields
p (Re|β, l, a, Rl, G0, y, z) ∝|Re|−1
2 (υe+n+k+1)
× exp

−1
2 tr

R−1
e

Se + V−1
e
%
.
This is the kernel of a scaled inverted Wishart distribution of order k = 2
(the number of traits in this case), with (υe + n) degrees of freedom and
scale matrix

Se + V−1
e
−1. Hence, the Gibbs sampler obtains updates for
the residual covariance matrix from
Re|β, l, a, Rl, G0, y, z ∼IW2

Se + V−1
e
−1 , υe + n

.
(13.54)
Similarly for Rl, from the joint posterior (13.42) one can deduce that
p (Rl|β, l, a, Re, G0, y, z) ∝p (l|Rl) p (Rl) .
Deﬁne
Sl =
 l′
yly
l′
ylz
l′
ylz
l′
zlz

and express the density p (l|Rl) as
p (l|Rl) ∝|R|−s
2 exp

−1
2 tr

R−1
l
Sl

.
Combining this with prior (13.40) yields
p (Rl|β, l, a, Re, G0, y, z) ∝|Rl|−1
2 (υl+s+k+1)
× exp

−1
2 tr

R−1
l

Sl + V−1
l
%
.
Therefore
Rl|β, l, a, Re, G0, y, z ∼IW2

Sl + V−1
l
−1 , υl + s

,
so the samples are also obtained from an inverted Wishart distribution.
Finally, for the genetic covariance matrix, from (13.42) one obtains
p (G0|β, l, a, Re, Rl, y, z) ∝p (a|G0) p (G0) .
(13.55)
The term p (a|G0) is
p (a|G0) ∝|G0|−q
2 exp

−1
2

ay
az
′ [G0 ⊗A]−1
 ay
az
%
.
On deﬁning
Sa =
 a′
yA−1ay
a′
yA−1az
a′
zA−1ay
a′
zA−1az

,

584
13. Gaussian and Thick-Tailed Linear Models
the density p (a|G0) can be expressed as
p (a|G0) ∝|G0|−q
2 exp

−1
2 tr

G−1
0 Sa

.
Therefore, combining this with prior (13.41) yields
p (G0|β, l, a, Re, Rl, y, z) ∝|G0|−1
2 (υa+q+k+1)
× exp

−1
2tr

G−1
0

V−1
a
+ Sa
%
,
which is recognized as the kernel of the scaled inverse Wishart distribution
G0|β, l, a, Re, Rl, y, z ∼IW2

V−1
a
+ Sa
−1 , υa + q

.
(13.56)
This process completes the speciﬁcation of all conditional posterior distri-
butions needed for running a Gibbs sampler. The order of visitation of the
distributions is dictated primarily by computational convenience. In prin-
ciple, one can follow an order similar to that described for the univariate
additive genetic model in Section 13.2, keeping in mind that imputations
for the missing records must be eﬀected before draws are made for the
location eﬀects.
13.5
A Blocked Gibbs Sampler for
Gaussian Linear Models
In the Gibbs sampler, as stated earlier, the location eﬀects can be drawn
either element-by-element or in blockwise manners. In the elementwise or
scalar version of the sampler, each location parameter is drawn successively.
A consequence of scalar sampling is that convergence may be very slow,
especially in models where the parameters in the posterior distribution are
highly intercorrelated (Hills and Smith, 1992; Roberts and Sahu, 1997).
Also, use of data augmentation in situations where there is a large fraction
of missing observations (in the broad sense of including random eﬀects or
unobserved latent variables, such as in threshold models ), can also slow
down convergence, for reasons similar to those hampering the expectation-
maximization algorithm (Dempster et al., 1977; Liu et al., 1994).
Liu (1994), Liu et al. (1994), and Roberts and Sahu (1997) compared
rates of convergence of various blocking strategies in Gibbs sampling. Liu
(1994) and Liu et al. (1994) considered a three-dimensional posterior dis-
tribution with parameters a, b, c, say, and contrasted elementwise sampling
with a blockwise sampling algorithm, called a group sampler. In the lat-
ter, two parameters (a, b) were blocked, so that the draws were made from
[a, b|c, data] and from [c|a, b, data]. In our context, a and b could correspond

13.5 A Blocked Gibbs Sampler for Gaussian Linear Models
585
to ﬁxed and random eﬀects, respectively, whereas c could correspond to
dispersion parameters. A collapsed sampler was considered as well, where
draws were made from [a, b|data] (so integration of parameter c is required
here) and from [c|a, b, data] . Liu (1994) found that the collapsed sampler
works better than the blocked sampler, with the latter performing bet-
ter than the elementwise implementation. The collapsed sampler has the
potential shortcoming that integration may not always be possible in non-
stylized models; for further discussion, see Chen et al. (2000). In the study
of Roberts and Sahu (1997), where the joint posterior distribution was mul-
tivariate normal, it was found that when all partial correlations between
parameters were positive, the blocked Gibbs sampler had a faster rate of
convergence than the piecewise algorithm. This suggests that grouping pos-
itively correlated parameters can be advantageous (e.g., members of a fam-
ily in a genetic context), although there are instances in which blocking
can make things worse.
Hence, the eﬀectiveness of a sampler may be enhanced by drawing pa-
rameters in blocks. For example, in the case of the linear additive genetic
model discussed in Section 13.2, rather than drawing each element of the
vector θ at a time, one may draw the whole vector θ in one pass. Clearly,
if the dispersion parameters were known, this is identical to sampling di-
rectly from the target posterior distribution of the location parameter with-
out creating a Markov chain. However, since the (co)variance parameters
are typically unknown, this would be equivalent to the group sampler de-
scribed above, with c being the location eﬀects, say, and a and b playing
the role of the variance components, as stated. Here a strategy presented
by Garc´ıa-Cort´es and Sorensen (1996) is described, which makes feasible
sampling the entire θ directly for some large Gaussian linear models. For
simplicity, the model is restricted to a univariate (single-trait) setting with
a sole random eﬀect other than the residual. Extension to models with
several variance components and to multivariate (multiple-trait) settings is
relatively straightforward.
Consider the same model assumptions as in Section 13.2, and adopt
proper prior distributions for a (13.2), for β ((13.3), with upper and lower
bounds), and for the two variance components σ2
i (i = a, e) as in (13.4).
Then the fully conditional posterior distributions, needed for implementing
the blocked or grouped Gibbs sampler, are
θ|σ2
a, σ2
e, y ∼N

5θ, C−1σ2
e

,
(13.57)
and
σ2
i |θ, y ∼,νi ,Siχ−2
νi ,
i = a, e,
(13.58)

586
13. Gaussian and Thick-Tailed Linear Models
where ,νi and ,Si are deﬁned in connection with (13.14) and (13.16). Recall
that the coeﬃcient matrix of the mixed model equation is
C =
 X′X
X′Z
Z′X
Z′Z + A−1k

,
where k is the ratio between the residual and the genetic components of
variance. The vector of the right-hand sides is W′y, where W =

X
Z

.
It is computationally diﬃcult to draw θ from (13.57) in a single pass when
p+q is very large; the usual calculations for extracting a multivariate normal
vector involve performing the Cholesky decomposition of the covariance
matrix, etc., and these are involved and must be repeated iteration after
iteration. The Garc´ıa-Cort´es and Sorensen procedure, instead, makes use
of the fact that 5θ = C−1W′y can be calculated rapidly using iterative
methods for solving linear systems of equations.
Deﬁne the following random vector of order (p + q) × 1:
θ∗= 5θ +

µ
a

−C−1W′z,
(13.59)
where:
• The vector 5θ is the mean of the conditional posterior distribution

θ|σ2
a, σ2
e, y

.
• µ is a p × 1 vector of “pseudo-ﬁxed” eﬀects (we will see later that
the value of µ is immaterial, so each of its elements can be set con-
veniently equal to 0).
• As before, a|A, σ2
a ∼N

0, A σ2
a

is a vector of random genetic eﬀects
and A is the usual, non-stochastic, additive relationship matrix.
• The random vector z is a vector of pseudo-observations generated
according to the process

z|µ, a, σ2
e

∼N

Xµ + Za, Iσ2
e

.
(13.60)
Since the additive genetic eﬀects are normally distributed, and the con-
ditional distribution (13.60) is normal, it follows that the process
[a, z|µ, A, σ2
a, σ2
e]
is jointly normal, with parameters

a
z
"""" µ, σ2
a, σ2
e ∼N


0
Xµ

,

Aσ2
a
AZ′σ2
a
ZAσ2
a
ZAZ′σ2
a + Iσ2
e

.
(13.61)

13.5 A Blocked Gibbs Sampler for Gaussian Linear Models
587
First, write θ∗in (13.59) as
θ∗= 5θ + C−1

C

µ
a

−W′z
%
= 5θ + C−1

X′Xµ + X′Za −X′ (Xµ + Za + e)
Z′Xµ +

Z′Z + A−1k

a −Z′ (Xµ + Za + e)

= 5θ + C−1

−X′e
A−1ka −Z′e

,
(13.62)
and observe that this is a linear combination of normal vectors, so its
distribution (given the observed data) must be normal as well. Second, note
that this vector does not depend at all on µ so there is no loss of generality
in assuming that µ= 0. Taking the expectation of θ∗over [a, z|µ, A, σ2
a, σ2
e]
one gets at once that
E (θ∗) = 5θ,
so the mean of the distribution of θ∗is identical to the mean of the condi-
tional posterior distribution of θ. Third, taking variances and covariances
of representation (13.62) over

a, z|µ, A, σ2
a, σ2
e

yields, since both 5θ and
C−1 are ﬁxed,
V ar (θ∗) = C−1V ar

−X′e
A−1ka −Z′e

C−1
= C−1
 X′X
X′Z
Z′X
Z′Z + A−1k

C−1σ2
e = C−1σ2
e,
which is identical to the variance–covariance matrix of the target condi-
tional posterior distribution. Hence, the distribution of θ∗is precisely that
of the posterior process (13.57). The three results together imply that sam-
ples from the conditional posterior distribution of the location eﬀects can
be obtained by generating θ∗draws using (13.59). It is convenient to rear-
range θ∗(after setting µ = 0, in view of considerations above) as
θ∗=

0
a

+ 5θ −C−1W′z
=

0
a

+ C−1W′ (y −z)
=
 0
a

+
 X′X
X′Z
Z′X
Z′Z + A−1α
−1 
X′ (y −z)
Z′ (y −z)

.
(13.63)
To summarize, the algorithm for carrying out the fully blocked imple-
mentation of the Gibbs sampler is
1. Provide starting values for σ2
a and σ2
e.

588
13. Gaussian and Thick-Tailed Linear Models
2. Generate a∗from N

0, Aσ2
a

.
3. Generate z∗from N

Za∗, Iσ2
e

.
4. Calculate y −z∗.
5. Compute
θ =

β
a

=

0
a∗

+
 X′X
X′Z
Z′X
Z′Z + G−1
−1 
X′ (y −z∗)
Z′ (y −z∗)

,
where

β′, a′′ is a draw from

θ|σ2
a, σ2
e, y

.
6. Compute ,Si, (i = a, e).
7. Sample variance components from (13.58), and update the coeﬃcient
matrix of the mixed model equations.
8. Return to step 2 and continue with this loop, until the end of the
chain.
Many diﬀerent iterative algorithms that do not require inversion of C
are available for solving the linear system
 X′X
X′Z
Z′X
Z′Z + A−1k

s =

X′ (y −z∗)
Z′ (y −z∗)

.
The choice of method to apply depends on the dimension of C and on
whether or not one wishes to pay attention to memory requirements or to
computing time.
Another useful approach for sampling parameters in blocks, which is not
restricted to Gaussian models, is based on the Langevin–Hastings algo-
rithm; this was brieﬂy described in Section 11.6 of Chapter 11. In principle,
the Langevin–Hastings algorithm allows joint updates of all the parameters
of the model; in the present Gaussian situation, one would update location
and dispersion parameters in a single pass.
13.6
Linear Models with Thick-Tailed
Distributions
13.6.1
Motivation
It is generally accepted that the normal distribution is sensitive to depar-
tures from the assumptions, because of its “thin” tails. Outlier observations
can have a marked impact on inferences, so many alternative, “robust”,

13.6 Linear Models with Thick-Tailed Distributions
589
methods have been developed. For instance, see Hampel et al. (1986). Re-
views and applications of robust procedures for parametric linear models
are, for example, in Rogers and Tukey (1972), Zellner (1976), and Lange
and Sinsheimer (1993). One of the possibilities that have been suggested
consists of replacing the normal process with a thicker-tailed distribution,
such as the Student-t, either in its univariate or multivariate forms. The
use of mixed models with t distributions in quantitative genetics was pio-
neered by Strand´en (1996) and Strand´en and Gianola (1999), and some of
the ideas were extended by Rosa (1998) to a wider family of distributions.
In this section, we motivate the problem and, subsequently, present Gibbs
sampling implementations for some variants of the theme. The problem
was already encountered in Chapter 6 in the context of a linear regression
model with t distributed errors, and will be dealt with again in the chapter
on analysis of longitudinal trajectories.
Consider a linear regression model under the “usual” assumptions, assign
a ﬂat prior to the regression vector β, and assume that the residual variance

σ2
is known. Under these conditions, the joint posterior density of the
regression coeﬃcients is
p

β|y,σ2
∝exp

−1
2σ2 (y −Xβ)′ (y −Xβ)

,
and the modal vector (equal to the mean vector in this setting) is identical
to the maximum likelihood estimator
5β
=

X′X
σ2
−1 X′y
σ2
=
 n

i=1
xix′
i
σ2
−1  n

i=1
xiyi
σ2

,
(13.64)
where x′
i is the ith row of X. Although this point estimator does not depend
on σ2 (because the dispersion parameter cancels out), it is instructive to
note that, implicitly, each observation is weighted equally by the reciprocal
of the variance (or, equivalently, all observations receive a weight equal to
1).
Abandon the normality assumption for the residuals and suppose that
these are independently and identically distributed as t

0, σ2, ν

, where
σ2 is now the scale parameter and ν is the degrees of freedom parame-
ter, with both assumed known. Then it follows that the observations are
independently distributed as
yi|β, σ2, ν ∼t1

x′
iβ, σ2, ν

,
(13.65)
deﬁning a univariate-t distribution. If a ﬂat prior is adopted for the regres-
sion vector, the posterior density is then (see Chapter 1 for the form of the

590
13. Gaussian and Thick-Tailed Linear Models
t-density)
p

β|y, σ2, ν

∝
n
-
i=1

1 + (yi −x′
iβ)2
σ2ν
−1+ν
2
.
Here the modal vector is also identical to the ML estimator, because of
the ﬂat prior. To ﬁnd the mode, we proceed to take derivatives of the log-
posterior density (which is equal to the likelihood function, apart from an
additive constant K), yielding
∂
∂β log

p

β|y, σ2, ν

= −

1 + ν
2

n

i=1
∂
∂β log

1 + (yi −x′
iβ)2
σ2ν

+ K
=

1 + ν
ν

n

i=1
xi (yi −x′
iβ)
σ2 + (yi−x′
iβ)
2
ν
.
Setting this gradient to 0, it follows that the posterior mode must satisfy
the system
n

i=1
xix′
i
σ2 + (yi−x′
iβ)
2
ν
β =
n

i=1
xiyi
σ2 + (yi−x′
iβ)
2
ν
,
(13.66)
which is not explicit in β. However, one can construct a functional iteration
by assigning a starting value to the regression coeﬃcients, and updating
iterates (t denotes round number) as
β[t+1] =


n

i=1
xix′
i
σ2 + (yi−x′
iβ[t])
2
ν


−1 

n

i=1
xiyi
σ2 + (yi−x′
iβ[t])
2
ν

.
(13.67)
This can be written in matrix form by putting
D−1
β
= Diag


1
σ2 + (yi−x′
iβ)
2
ν

,
and noting that
n

i=1
xix′
i
σ2 + (yi−x′
iβ)
2
ν
= X′D−1
β X
and
n

i=1
xiyi
σ2 + (yi−x′
iβ)
2
ν
= X′D−1
β y.
Hence (13.67) becomes
β[t+1] =

X′ 
D−1
β
[t]
X
−1
X′ 
D−1
β
[t]
y
(13.68)

13.6 Linear Models with Thick-Tailed Distributions
591
which deﬁnes an iteratively reweighted least-squares algorithm, showing
a clear analogy between the linear regression models with normal and t-
distributed residuals. Note from (13.67) that the implicit weight received
by datum i is
1
σ2 + (yi−x′
iβ)
2
ν
.
(13.69)
If the error distribution approaches normality (ν →∞), the weight is 1/σ2,
as in the standard regression model. If, on the other hand, the distribution
has increasingly thicker tails (ν →0), the observation is downweighted fur-
ther and further. At a ﬁxed value of the degrees of freedom parameter, the
weight is inversely proportional to (yi −x′
iβ)2, that is, the more an obser-
vation deviates from its expected value (given β), the smaller the weight it
receives in the analysis, and the less it perturbs inference. This is the reason
why the t-distribution is considered as being more robust than the normal:
observations that are in discrepancy with the predictive structure are atten-
uated, thus reducing the impact on inferences. This phenomenon, clearly,
does not take place in the regression model with normally distributed er-
rors. A comprehensive discussion of the eﬀect of outliers is in Barnett and
Lewis (1995).
An alternative to the univariate process (13.65) is to adopt a multivariate-
t error distribution of order n for the residuals, that is, assume
tn

0, Iσ2, ν

,
where Iσ2 is the scale matrix. Here the residuals are uncorrelated although
not independent (recall that in a multivariate-t distribution with a diagonal
scale matrix, the joint density cannot be obtained by multiplying the corre-
sponding marginal densities, all of which are univariate-t). The regression
model is then based on the data generating scheme
y|β, σ2, ν ∼tn

Xβ, σ2, ν

.
Note that the data constitute a sample of size 1 from a multivariate-t
distribution of order n. Assuming that σ2 and ν are both known, and that
β has been assigned a ﬂat prior, the posterior density takes the form
p

β|y, σ2, ν

∝

1 + (y −Xβ)′ (y −Xβ)
σ2ν
−n+ν
2
.
A search for the mode gives
5β = (X′X)−1 X′y,
as meeting the ﬁrst-order condition, which is the posterior mode as if the
errors had been assigned a multivariate normal distribution. The reason for

592
13. Gaussian and Thick-Tailed Linear Models
this is that all observations here receive the same implicit weight
1
σ2 + 1
ν
n
i=1
(yi −x′
iβ)2 ,
so that no “robustness” in inference is gained by adopting this multivariate
error distribution, other than an inﬂation of the posterior standard devi-
ations of individual regression coeﬃcients. Further, Zellner (1976) states
that when β, σ2, and ν are all unknown, the joint posterior (using ﬂat pri-
ors for β, σ2, and ν) does not have a maximum, indicating that the joint
posterior is improper. On the other hand, Liu and Rubin (1995) describe
modiﬁcations of the EM algorithm for the situation where the degrees of
freedom are unknown (these can be estimated when there is replication of
samples from the same multivariate-t process) and obtain reasonable re-
sults in their examples. At any rate, posterior modes (or ML estimates in
this case) of β and of σ2 exist for any ﬁxed value of the degrees of free-
dom parameter, even when a single sample is drawn from the multivariate-t
distribution (Zellner, 1976; McLachlan and Krishnan, 1997). As implied by
Zellner (1976), the scale and degrees of freedom parameters are confounded
when a single sample is drawn, even if the data vector contains n observa-
tions. If, in a Bayesian context, a proper prior is assigned to the degrees
of freedom parameter, the net eﬀect is to spread the uncertainty across
all parameters of the model, but it is diﬃcult to assess how each of the
marginal distributions would be aﬀected, since the analytical approach is
intractable.
Now consider the linear model of (13.1), but under the assumptions
yi|β, a, σ2
e, νe
∼
t1

x′
iβ + z′
ia, σ2
e, νe

,
(13.70)
a|A, σ2
a, νa
∼
tq

0, A σ2
a, νa

,
(13.71)
where z′
i is the ith row of Z, A σ2
a is the scale matrix of the q variate t
distribution given above, and νa is the corresponding degrees of freedom
parameter. This assumption preserves the property of the multivariate nor-
mal distribution usually employed for additive genetic eﬀects: all marginal
and conditional distributions are univariate- or multivariate-t, and any lin-
ear combination of breeding values is t as well. For example, if the joint
distribution of the additive genetic eﬀects of the mother, father, and of the
segregation residual is trivariate-t, then the breeding value of the progeny
is also t. On the other hand, if the breeding values of the father and mother
are independently distributed as univariate-t and the segregation residual
is also an independent univariate-t variable, the additive genetic value of
the oﬀspring is not t. Now suppose that the scales and degrees of freedom
are known and, as before, that a ﬂat prior is assigned to the location vector

13.6 Linear Models with Thick-Tailed Distributions
593
β. The logarithm of the joint posterior density is
p

β, a|σ2
a, νa, σ2
e, νe, y

= K −

1 + νe
2

n

i=1
log

1 +

yi −x′
iβ −z′
ia
2
σ2eνe

−

q + νa
2

log

1 + a′A−1a
σ2aνa

.
(13.72)
The gradients of this density are
∂p

β, a|σ2
a, νa, σ2
e, νe, y

∂β
=

1 + νe
νe

n

i=1
xi

yi −x′
iβ −z′
ia

σ2e + (yi−x′
iβ−z′
ia)
2
νe
,
∂p

β, a|σ2
a, νa, σ2
e, νe, y

∂a
=

1 + νe
νe

n

i=1
zi

yi −x′
iβ −z′
ia

σ2e + (yi−x′
iβ−z′
ia)
2
νe
−

q + νa
νa

A−1a

σ2a + a′A−1a
νa
.
Let
σ2
ei = σ2
eνe +

yi −x′
iβ −z′
ia
2
νe + 1
,
i = 1, 2, . . . , n,
and note that this “pseudo-variance” can be viewed as a weighted average of
σ2
e (known parameter) and of

yi −x′
iβ −z′
ia
2 ; if νe →∞, then σ2
ei →σ2
e
(normality). Similarly, let
σ2
a =
νaσ2
a + q a′A−1a
q
νa + q
,
which is a weighted average between σ2
a and [a′A−1a]/q, and observe that
if νa →∞, σ2
a →σ2
a. Setting all derivatives to 0 simultaneously and rear-
ranging leads to the iterative system

X′D−1
βuX
X′D−1
βuZ
Z′D−1
βuX
Z′D−1
βuZ + A−1
σ2
a
[t+1] 
β
a
[t]
=
 X′D−1
βuy
Z′D−1
βuy
[t]
, (13.73)
where
D−1
βu = Diag

 1
σ2
ei

and σ2
a change iteratively. This is a set of iteratively reweighted mixed
model equations, where observations that are far away from their condi-
tional expectations (x′
iβ + z′
ia) are downweighted, and the more so when
the degrees of freedom of the residual distribution are small. Similarly, the

594
13. Gaussian and Thick-Tailed Linear Models
pseudo-variance σ2
a is modiﬁed iteratively, as the values of a change from
round to round. In summary, this illustrates, at least when the degrees of
freedom are ﬁxed, that the univariate-t distribution for the residuals has
the eﬀect of attenuating observations that are away, in some sense, from
the predictive structure of the model, whereas the multivariate-t assump-
tion for the additive genetic eﬀects has the eﬀect of modifying the impact
of the scale parameter σ2
a in the light of what the data have to say about
breeding values.
A word of caution about modal estimates is in order here. At least for
the regression model and the multivariate-t residual distribution, Liu and
Rubin (1995) point out that the likelihood function (or posterior distri-
bution under ﬂat priors) can be multimodal when the degrees of freedom
parameter is small or unknown. In this case, the point estimates may be
of little interest by themselves, even though they may be global or local
maxima. McLachlan and Krishnan (1997) gave an example where the data
vector was y′ = [−20, 1, 2, 3], and ﬁtted a univariate-t distribution with
scale parameter equal to 1 and the degrees of freedom set to 0.05. For this
situation, they found that the likelihood of the unknown mean µ had four
local maxima:
µ1 = −19.993, µ2 = 1.086, µ3 = 1.997, µ4 = 2.906,
and three local minima:
µ5 = −14.516, µ6 = 1.373, µ7 = 2.647.
The plot of the log-likelihood (or posterior density under a ﬂat prior, apart
from an additive constant) revealed that the likelihood fell abruptly in the
neighborhood of the global maximum µ3 = 1.997, so there would be little
posterior probability mass in the neighboring region. This reinforces the
point that often there is no substitute for the entire posterior distribution.
Strand´en and Gianola (1998), in a simulation study, evaluated the uni-
variate t residual distribution for coping with the eﬀects of an unknown
preferential treatment of some animals in livestock breeding. Alternatively,
a multivariate-t residual distribution was used, where residuals were clus-
tered by herd; here the assumption was that the residuals were uncorrelated
but not independent. They used a Bayesian model (with Gibbs sampling),
where the residual distribution was univariate-t, and treated the degrees of
freedom as an unknown, discrete parameter; the usual multivariate normal
distribution was assigned to the breeding values. In the simulation, they
compared the mean squared error of predicted breeding values (using pos-
terior means), in situations with or without preferential treatment. In the
absence of preferential treatment, the t-models were as good as the Gaus-
sian ones. When such treatment was present, the univariate-t model was
clearly the best, and the posterior distribution of the degrees of freedom
pointed away from the Gaussian assumption. The authors pointed out that

13.6 Linear Models with Thick-Tailed Distributions
595
it was encouraging that a symmetric error distribution improved upon the
Gaussian one, even under a single-tailed form of preferential treatment. A
robust asymmetric distribution, such as in Fernandez and Steel (1998), may
do even better, but perhaps at the expense of conceptual and computational
simplicity. Their Bayesian implementation, with some slight modiﬁcations,
is discussed subsequently.
13.6.2
A Student-t Mixed Eﬀects Model
Return to a model with the assumptions as in (13.70) and (13.71) but
assume now that the degrees of freedom and the scale parameters are un-
known. As seen in Chapter 1 and Chapter 6, the t distributions can be
generated by mixing a normal distribution over gamma processes with ap-
propriate parameters. The two assumptions can be replaced by the “aug-
mented” hierarchy
yi|β, a, σ2
e, wi ∼N

x′
iβ + z′
ia,σ2
e
wi

,
i = 1, 2, ..., n,
(13.74)
wi|νe ∼Ga
νe
2 , νe
2

,
i = 1, 2, ..., n,
(13.75)
a|A, σ2
a, wa ∼N

0, A σ2
a
wa

,
(13.76)
wa|νa ∼Ga
νa
2 , νa
2

.
(13.77)
Assume now that β and the two scale parameters are assigned independent,
proper, uniform distributions and that the prior densities of the degrees of
freedom are p (νe) and p (νa) ; also let the two degrees of freedom have
independent prior distributions. Then the joint posterior density for the
augmented hierarchy has the form
p

β, a,σ2
e, w,σ2
a, wa, νe, νa|y

∝
n
-
i=1

p

yi|β, a,σ2
e, wi

p (wi|νe)

×p

a|A, σ2
a, wa

p (wa|νa) p (νe) p (νa) ,
(13.78)
where w = {wi} is the vector of residual weights. The conditional poste-
rior distributions needed for running a MCMC scheme are presented next,
making use of results derived in Chapter 6, in connection with the linear
regression model with residuals distributed as t. In what follows the usual
notation “ELSE” is employed to denote the data vector y and all param-
eters that are treated as known in the appropriate conditional posterior
distribution.

596
13. Gaussian and Thick-Tailed Linear Models
Residual and Genetic “Weights”
Note in (13.78) that the residual weights wi are conditionally independent
of each other, with the individual densities being
p (wi|ELSE) ∝p

yi|β, a, σ2
e, wi

p (wi|νe)
∝

σ2
e
wi
−1
2
w
νe
2 −1
i
exp
#
−wi
2

yi −x′
iβ −z′
ia
2 + νeσ2
e
σ2e
$
∝w
νe+1
2
−1
i
exp

−wiSi
2

,
i = 1, 2, ..., n.
(13.79)
where
Si =

yi −x′
iβ −z′
ia
2 + νeσ2
e
σ2e
.
This indicates that the conditional posterior distribution of each wi is the
gamma distribution
wi|ELSE ∼Ga

νe + 1
2
, Si
2

,
i = 1, 2, ..., n.
(13.80)
Similarly,
p (wa|ELSE) ∝p

a|A, σ2
a, wa

p (wa|νa)
∝

 σ2
a
wa
−q
2
w
νa
2 −1
a
exp

−wa
2

a′A−1a + νaσ2
a
σ2a

∝w
νa+q
2
−1
a
exp

−wa
2

a′A−1a + νaσ2
a
σ2a

,
so its conditional distribution is also gamma
wa|ELSE ∼Ga

νa + q
2
, a′A−1a + νaσ2
a
2σ2a

.
(13.81)
Location Eﬀects
From the joint density, it follows that
p (β, a|ELSE) ∝
n
-
i=1
p

yi|β, a, σ2
e, wi

p

a|A, σ2
a, wa

∝exp

−1
2σ2e

(y −Xβ −Za)′ W (y −Xβ −Za) + waσ2
e
σ2a
a′A−1a
%
,
where W = {wi} is an n×n matrix. This form was encountered in Chapter
6 and in Section 13.2 of this chapter. The conditional posterior distribution
is multivariate normal with parameters
θ|ELSE ∼N

5θ, C−1σ2
e

,
(13.82)

13.6 Linear Models with Thick-Tailed Distributions
597
where
5θ
=

X′WX
X′WZ
Z′WX
Z′WZ + A−1 waσ2
e
σ2
a
−1 
X′Wy
Z′Wy

,
C−1
=

X′WX
X′WZ
Z′WX
Z′WZ + A−1 waσ2
e
σ2
a
−1
.
Techniques for drawing the location eﬀects either in piecewise or in block-
wise manners, or in a single pass, have been discussed earlier in this chapter,
with the only novelty being the appearance of the wi and wa weights.
Scale Parameters
The conditional posterior density of the scale parameter σ2
e, making refer-
ence to (13.78), is
p

σ2
e|ELSE

∝
n
-
i=1

p

yi|β, a,σ2
e, wi

∝

σ2
e
−n
2 exp

−1
2σ2e
(y −Xβ −Za)′ W (y −Xβ −Za)

∝

σ2
e
−n−2
2
+1 exp

−1
2σ2e
(y −Xβ −Za)′ W (y −Xβ −Za)

.
It follows that the conditional posterior distribution is the scaled inverted
chi-square process
σ2
e|ELSE ∼(y −Xβ −Za)′ W (y −Xβ −Za) χ−2
n−2.
(13.83)
Similarly,
p

σ2
a|ELSE

∝p

a|A, σ2
a, wa

∝

σ2
a
−q−2
2
+1 exp

−wa
2σ2a
a′A−1a

,
which indicates that
σ2
a|ELSE ∼waa′A−1aχ−2
q−2.
(13.84)
Degrees of Freedom
The conditional posterior distribution of the degrees of freedom parame-
ters can be deduced from (13.78). One arrives immediately at the result
that νe and νa have conditionally independent posterior distributions, with
densities
p (νe|ELSE) ∝
 n
-
i=1
p (wi|νe)

p (νe) ,

598
13. Gaussian and Thick-Tailed Linear Models
and
p (νa|ELSE) ∝p (wa|νa) p (νa) .
It is not possible to go further without being speciﬁc about the form of the
prior distributions. Here, we will consider two alternative settings.
In the ﬁrst one, the degrees of freedom (positive parameters) are allowed
to take integer values only over a ﬁnite set of values having equal prior
probability (Albert and Chib, 1993; Geweke, 1993; Strand´en, 1996). Let
these sets contain values fj = 1, 2, . . . , de and qk = 1, 2, . . . , da, respectively.
The prior distributions are then
p (νe) = 1
de
,
νe ∈{fj = 1, 2, ..., de} ,
and
p (νe) = 1
da
,
νa ∈{qk = 1, 2, ..., da} .
Recalling that the prior distributions of the weights are in Gamma form,
it follows that
p (νe|ELSE) ∝


 νe
2
(
νe
2 )
Γ
 νe
2



n
n
-
i=1

w
νe
2 −1
i
exp

−νewi
2

.
This is a discrete distribution, and samples can be drawn by extracting
degrees of freedom values with probabilities
Pr (νe = fj|ELSE) = Ce



fj
2

 fj
2

Γ

fj
2



n
n
-
i=1

w
fj
2 −1
i
exp

−fjwi
2

,
(13.85)
where Ce is equal to


de

j=1










fj
2

 fj
2

Γ

fj
2



n
n
-
i=1

w
fj
2 −1
i
exp

−fjwi
2










−1
.
Likewise, and following a similar type of algebra, the conditional posterior
distribution of the degrees of freedom of the multivariate-t distribution of
the additive genetic eﬀects can be found to be:
Pr (νa = qk|ELSE) = Ca
 qk
2
(
qk
2 )
Γ
 qk
2
 w
qk
2 −1
a
exp

−qkwa
2

(13.86)

13.6 Linear Models with Thick-Tailed Distributions
599
where
Ca =



da

k=1


 qk
2
(
qk
2 )
Γ
 qk
2
 w
qk
2 −1
a
exp

−qkwa
2






−1
.
Hence, new states for the degrees of freedom of the genetic distribution
can be drawn by sampling with probabilities (13.86). The Gibbs sampling
scheme is completed by eﬀecting draws from distributions (13.80)–(13.86)
in any suitable order.
If the degrees of freedom are treated as continuous, the conditional pos-
terior densities are not in any recognizable form. Here, one may consider
embedding a Metropolis–Hastings step in the MCMC scheme, as in Geweke
(1993) and Rodriguez-Zas (1998). For example, Geweke (1993) uses an ex-
ponential distribution for these parameters, so that the conditional poste-
rior density of the residual degrees of freedom has the form
p (νe|ELSE) ∝


 νe
2
(
νe
2 )
Γ
 νe
2



n
n
-
i=1

w
νe
2 −1
i
exp

−νewi
2

exp (−ωeνe)
∝


 νe
2
(
νe
2 )
Γ
 νe
2



n
exp

−νe (2ωe + nw)
2

n
-
i=1
w
νe
2 −1
i
,
(13.87)
where ωe is the parameter of the prior exponential distribution and w is the
average of the weights wi at any given iterate. For example, if ωe = 0.15,
this corresponds to a prior mean and variance of 6.6 and 44.4, respectively,
for the residual degrees of freedom. The exponential prior can be assessed
such that both small and large values of the degrees of freedom receive
“high” prior probability, making the speciﬁcation vague enough. Rodriguez-
Zas (1998) used a normal proposal distribution for Metropolis–Hastings
with parameters based on the current values of the weights in the course
of iteration. Similarly, the conditional posterior density of the degrees of
freedom of the genetic distribution would be
p (νa|ELSE) ∝
 νa
2
(
νa
2 )
Γ
 νa
2
 w
νa
2 −1
a
exp

−νawa
2

exp (−ωaνa)
∝
 νa
2
(
νa
2 )
Γ
 νa
2

exp

−νa (2ωa + wa)
2

w
νa
2 −1
a
,
(13.88)
where ωa is the parameter of the prior exponential distribution.
Making an analogy with the single sample multivariate-t model of Zellner
(1976), we conjecture that there is no information in the likelihood available
to separate νa from σ2
a. As noted, using proper priors for both parameters
solves the identiﬁability problem and spreads the uncertainty throughout
the model, which is realistic. On the other hand, Strand´en and Gianola

600
13. Gaussian and Thick-Tailed Linear Models
(1999) suggest ﬁtting a series of models with alternative values for νa and
then assessing the impact on inferences. The diﬀerent models can be con-
trasted using some of the Bayesian model comparison tools discussed earlier
in the book. On the other hand, if the t-distribution has replicate samples,
such as in a sire model where the transmitting abilities are distributed in-
dependently, the two parameters are not confounded. Hence, in practice,
one can cluster individuals into families, assume these are independent, and
ﬁnd the most probable value of the degrees of freedom. Then, conditionally
on such modal value, one proceeds with the implementation given above,
ignoring the uncertainty about its error.
13.6.3
Model with Clustered Random Eﬀects
The setting here is one where observations are clustered in some natu-
ral manner, and where cluster eﬀects are independent and identically dis-
tributed as univariate-t, with unknown scale and degrees of freedom pa-
rameters. For example, individuals may be clustered in nuclear or half-sib
families, or in randomly created inbred lines; alternatively, observations
could consist of repeated measures on individuals, in which case the sub-
jects constitute the clustering criterion.
The assumptions to be made here are
yi|β, a, σ2
e, wi
∼
N

x′
iβ + z′
ia,σ2
e
wi

,
i = 1, 2, ..., n,
wi|νe
∼
Ga
νe
2 , νe
2

,
i = 1, 2, ..., n,
for the observations, thus deﬁning a univariate-t distribution upon integra-
tion of the joint distribution over the weights, and
ai|σ2
a, wai
∼
N

0, σ2
a
wai

,
i = 1, 2, ..., q,
wai|νa
∼
Ga
νa
2 , νa
2

,
i = 1, 2, ..., q.
This is equivalent to stating that the cluster eﬀects are independent and
identically distributed as univariate-t, a priori, with null mean, scale pa-
rameter σ2
a, and degrees of freedom νa. Here there is “replication” of the
second-stage distribution, so both σ2
a and νa are estimable in the maximum
likelihood sense.
The joint posterior density of all unknowns is
p

β, a, σ2
e, w,σ2
a, wa, νe, νa|y

∝
n
-
i=1

p

yi|β, a, σ2
e, wi

p (wi|νe)

×
q
-
i=1

p

ai|σ2
a, wai

p (wai|νa)

p (νe) p (νa) ,

13.6 Linear Models with Thick-Tailed Distributions
601
where wa = {wai} is a q × 1 vector. The conditional posterior distribution
of the residual weights and of σ2
e remain as in (13.79) and (13.83 ), respec-
tively. Further, the conditional posterior distribution of the cluster weights
and variance takes the form
wai|ELSE ∼Ga

νa + 1
2
, a2
i + νaσ2
a
2σ2a

,
i = 1, 2, . . . , q.
(13.89)
The scale parameter σ2
a has as conditional posterior distribution
σ2
a|ELSE ∼
 q

i=1
waia2
i

χ−2
q−2,
(13.90)
noting that the random eﬀects enter attenuated by the weights wai, relative
to their counterpart in a purely Gaussian model. For example, the scale
parameter of this scaled inverted chi-square distribution would be q
i=1 a2
i
in the latter.
The conditional posterior density of the location eﬀects is
p (β, a|ELSE) ∝
n
-
i=1
p

yi|β, a,σ2
e, wi

q
-
i=1
p

ai|σ2
a, wai

∝exp

−1
2σ2e
(y −Xβ −Za)′ W (y −Xβ −Za)

exp

−1
2σ2a
a′Waa

,
where Wa is a q × q diagonal matrix with typical element equal to wai
(i = 1, 2, ..., q) . Using well established results, manipulation of the above
density leads directly to the normal distribution
θw|ELSE ∼N

5θw, C−1
w σ2
e

,
(13.91)
where
5θw =

X′WX
X′WZ
Z′WX
Z′WZ + Wa
σ2
e
σ2
a
−1 
X′Wy
Z′Wy

,
and
C−1
w =

X′WX
X′WZ
Z′WX
Z′WZ + Wa
σ2
e
σ2
a
−1
.
Finally, the conditional posterior densities of the degrees of freedom (con-
tinuous case with exponential prior distributions) are as in (13.87) for the
residual distribution and
p (νa|ELSE) ∝


 νa
2
(
νa
2 )
Γ
 νa
2



q
exp

−νa (2ωa + qwa)
2

q
-
i=1
w
νa
2 −1
ai
. (13.92)
This completes the speciﬁcation of an MCMC scheme for a linear model
where all residuals and cluster eﬀects are distributed as univariate-t with
appropriate parameters.

602
13. Gaussian and Thick-Tailed Linear Models
13.7
Parameterizations and the Gibbs Sampler
Roberts and Sahu (1997) noted that high correlations among the elements
of the parameter vector in the posterior distribution can lead to poor
convergence of the single-site Gibbs sampler. An illustration of the con-
sequences of a large posterior correlation of parameters was discussed in
Chapter 12 in Example 12.2. The eﬀect of parameterization on the behav-
ior of the Markov chain is also discussed in Gelfand et al. (1995) and in
Gelfand et al. (1996). These authors argued that “hierarchically centered”
parameterizations for linear and nonlinear models reduce the extent of pos-
terior intercorrelations and lead to faster mixing and convergence. Here we
will give a brief discussion of the problem, by adapting the presentation in
Gelfand et al. (1996) to a quantitative genetics setting.
Consider the additive genetic model
yi = µ + ai + ei,
where there is a single observation made in each animal, i = 1, 2, ..., n;
assume that animals are genetically unrelated. As usual, take
yi|µ, ai ∼N

µ + ai, σ2
e

as the data generating process and
ai|σ2
a ∼N

0, σ2
a

as a prior for the genetic eﬀects. Suppose the two variance components are
known, and assign the process µ ∼N

µ0, σ2
µ

as a prior distribution for
µ, where hyperparameters are taken as known as well. Using standard re-
sults employed several times in this book, the posterior variance–covariance
matrix of µ and of the additive genetic eﬀects is
V ar










µ
a1
a2
.
.
an


""""""""""""
µ0, σ2
µ, σ2
a, σ2
e, y








=


n + σ2
e
σ2µ
1
1
.
1
1
1
1 + σ2
e
σ2
a
0
.
0
0
1
0
1 + σ2
e
σ2
a
.
0
0
.
.
.
.
.
.
1
0
.
.
1 + σ2
e
σ2
a
0
1
0
.
.
0
1 + σ2
e
σ2
a


−1
σ2
e.

13.7 Parameterizations and the Gibbs Sampler
603
The inverse of this matrix can be found using results for partitioned matri-
ces (e.g., Searle, 1982). Gelfand et al. (1996) arrive at the following repre-
sentations (the conditioning on the known parameters or hyperparameters
is suppressed in the notation)
Corr (µ, ai|y)
=
−

1 + nσ2
e
σ2a
+ σ2
e
σ2µ
+ σ4
e
σ2µ
−1
2
,
Corr (ai, aj|y)
=

1 + nσ2
e
σ2a
+ σ2
e
σ2µ
+ σ4
e
σ2µ
−1
.
As σ2
e tends to inﬁnity, the correlations go to 0, but tend to −1 and
1, respectively if either σ2
a or σ2
µ become larger and larger (vague prior
knowledge about the random eﬀects). From an animal breeding point of
view, suppose that a ﬂat prior is assigned to µ (σ2
µ →∞) and recall that
σ2
e/σ2
a = (1 −h2)/h2, where h2 is heritability. Then, when h2 →0, so that
the variability is dominated by environmental eﬀects,
Corr (µ, ai|y) = −

1 + n1 −h2
h2
−1
2
→0
and
Corr (ai, aj|y) =

1 + n1 −h2
h2
−1
→0.
On the other hand, when h2 →1, a situation in which one deﬁnitely needs
the random eﬀects model, the absolute value of the correlations tend to 1.
This implies that the Gibbs sampler will mix slower for highly heritable
traits, at least with the standard parameterization given above.
Gelfand et al. (1996) also discuss “hierarchical centering”. Here, they
deﬁne ηi = µ + ai, so that the resulting hierarchical model is yi|ηi ∼
N

ηi, σ2
e

, with ηi|µ, σ2
a ∼N

µ, σ2
a

, and µ ∼N

µ0, σ2
µ

. These two
models are probabilistically equivalent, that is, give the same prior predic-
tive distribution or marginal distribution of the observations. The authors
encounter
Corr (µ, ηi|y)
=
−

1 + nσ2
a
σ2e
+ σ2
a
σ2µ
+ σ4
a
σ2µ
−1
2
,
Corr

ηi, ηj|y

=

1 + nσ2
a
σ2e
+ σ2
a
σ2µ
+ σ4
a
σ2µ
−1
.

604
13. Gaussian and Thick-Tailed Linear Models
Here the correlations do not go to 0 if σ2
e →∞. Returning again to the
animal breeding setting, with a uniform improper prior for µ, one gets
Corr (µ, ηi|y)
=
−

1 + n
h2
1 −h2
−1
2
,
Corr

ηi, ηj|y

=

1 + n
h2
1 −h2
−1
.
When heritability tends to 0, the correlations tend to 1, but when the
genetic variance becomes relatively more and more important, the correla-
tions go to 0, indicating that the hierarchical parameterization should be
preferred. In the standard parameterization, the data inform directly on
ηi = µ + ai, so if there is vague prior knowledge about ai (large value of
the additive genetic variance) the data cannot separate µ from ai; thus, the
large negative correlation between these unknowns. Gelfand et al. (1996)
note that, in practice, the variance components are unknown, so it is nec-
essary to consider the joint posterior distribution of location eﬀects and
dispersion parameters; however, they recommend hierarchical centering as
a default procedure. A detailed study of the problem, including longitudinal
data settings, is in Gelfand et al. (1995). They concluded that hierarchical
centering will usually improve convergence of sampling-based procedures
for Bayesian analysis.
Other types of parameterizations and strategies to improve mixing and
convergence were studied by Hills and Smith (1992, 1993); Liu (1994) and
by Liu et al. (1994).

14
Threshold Models
for Categorical Responses
14.1
Introduction
Discrete response variables are ubiquitous in genetics. In particular, cate-
gorical responses arise when the outcome is an assignment into one of sev-
eral mutually exclusive and exhaustive classes. Typical examples include
congenital malformations, leg weakness traits in pigs, presence or absence
of intra-mammary infection in cows, subjective scores describing diﬃculties
at birth in cattle, X-ray readings of hip-dysplasia in dogs, and litter-size in
sheep. When there are two categories of response, the trait is referred to as
binary or “all or none” (Dempster and Lerner, 1950). With more than two
categories, a distinction must be made as to whether the classes are either
unordered or ordered in some manner. Unordered categories can arise when
the outcome is a choice; for example, electing an item in a menu or voting
for a certain candidate. In this chapter, however, the focus will be on the
ordered categories. This is so, because in biological systems, response cate-
gories can be ordered almost invariably along some hypothetical gradient.
For example, it is possible to think about a fecundity gradient in sheep,
from least proliﬁc to most proliﬁc. Here, the litter size observed at birth
would be related somehow to this conceptual gradient.
Quantitative geneticists have used the so called threshold model to relate
a hypothetical continuous scale to an outward phenotype (the observed cat-
egory of response). The underlying variate is often called “the latent vari-
able” or, at least in genetics of disease, “liability”, after Falconer (1965).
The model postulates that the continuous response is rendered discrete via

606
14. Threshold Models for Categorical Responses
some ﬁxed thresholds or boundaries delimiting categories (Wright, 1934;
Robertson and Lerner, 1949; Dempster and Lerner, 1950; Falconer, 1965,
1967). For example, if there are two categories of response, the observa-
tion would be in the second class, e.g., “disease”, if the liability exceeds
the threshold. The origins of the threshold model can be traced back to
Pearson (1900), Wright (1934), Bliss (1935), and Dempster and Lerner
(1950). Recent methodological contributions, made primarily from a statis-
tical genetic perspective, include Gianola (1982), Harville and Mee (1984),
Gianola and Foulley (1983), Foulley et al. (1987) and Foulley and Manfredi
(1991), among others. Curnow (1972) and Curnow and Smith (1975) sug-
gested an alternative concept where, instead of having abrupt thresholds,
there is a “risk function”. However, at least as these authors formulate the
model, their development is mathematically equivalent to one with abrupt
thresholds.
The threshold model for analysis of binary traits was encountered in
Chapter 4. There, it was pointed out that whenever the model requires
speciﬁcation of random eﬀects, the likelihood function (or marginal pos-
terior distribution) does not have a closed form. In this case, standard
likelihood-based analyses have been conducted using Gaussian quadrature
approximations in models with independent random eﬀects, as in Anderson
and Aitkin (1985) or, when the random eﬀects are correlated, with the
Monte Carlo EM algorithm, as in McCulloch (1994). Analyses based on
approximations have been developed by Gilmour et al. (1985) and Lee and
Nelder (1996) have suggested what are called “hierarchical likelihoods”.
Basically, these methods can be viewed as approximations to REML and
BLUP in the context of generalized linear mixed models. Many of the ar-
guments on which these methods rest are of an asymptotic nature, and the
ﬁnite sample properties of the procedures are unknown.
In this chapter, an MCMC Bayesian implementation of two models in-
volving ordered categorical traits is described. In all cases, it is assumed
that the underlying or latent variable has a (conditional) Gaussian distribu-
tion. The object of inference may include, for example, the additive genetic
values of individuals (to rank candidates for selection in genetic improve-
ment programs), the probability distribution by category of response for
some individuals or experimental conditions or interest, or the genetic vari-
ance of the trait in question. The ﬁrst model discussed extends the analysis
of a binary trait presented in Chapter 4 to one with an arbitrary number of
ordered response categories. Following Albert and Chib (1993), it is shown
that a Gibbs sampler, used with data augmentation, leads to fully poste-
rior distributions that are easy to sample from. The development is based
on Sorensen et al. (1995), Heringstad et al. (2001) and Kadarmideen et al.
(2002). The second model described can be used when the outcome is an
ordered categorical response and an observation on a Gaussian trait, follow-
ing ideas in Jensen (1994), Sorensen (1996) and in Wang et al. (1997). See
Foulley et al. (1983) for an approximate Bayesian analysis of this model.

14.2 Analysis of a Single Polychotomous Trait
607
14.2
Analysis of a Single Polychotomous Trait
14.2.1
Sampling Model
Let all underlying latent variables or liabilities be represented by the vector
l = {li} (i = 1, 2, . . . , n), such that for the ith individual or data point it is
postulated that
li = x′
iβ + z′
ia + ei.
(14.1)
Here β are some location eﬀects, a is a q × 1 vector of additive genetic
values (perhaps of order larger than n), and ei ∼N

0, σ2
e

is a random
residual. As usual, x′
i and z′
i are incidence row vectors. It will be assumed
that, given the location parameters β and a, the elements of the vector l
are conditionally independent and distributed as

l|β, a, σ2
e

∼N

Xβ + Za, Iσ2
e

.
(14.2)
Since the liability variate is unobservable, the parameterization σ2
e = 1 will
be adopted here (i.e., Cox and Snell, 1989), in order to achieve identiﬁabil-
ity in the likelihood. It must be noted that when inferences are based on
posterior distributions with proper priors assigned to all parameters, this is
not technically required (Bernardo and Smith, 1994). However, this setting
is used here nonetheless since it is standard in threshold model analysis.
Other parameterizations of the threshold model are discussed in Sorensen
et al. (1995).
Let y = {yi} (i = 1, 2, . . . , n), denote the vector of observed categorical
data. Here, each yi represents an assignment into one of c mutually exclu-
sive and exhaustive categories of response arrived at more or less arbitrarily.
Often, the assignment is subjective, as in analysis of conformation scores.
These classes result from the hypothetical existence of c + 1 thresholds in
the latent scale, such that tmin < t1 < t2 < · · · < tc−1 < tmax. For exam-
ple, if the realized value of liability is between t1 and t2 , the assignment
is in the second category of response. Set the two extreme thresholds to
t0 = tmin, tc = tmax so that the remaining c−1 thresholds can take any value
between tmin and tmax, subject to the preceding order requirement. How-
ever, one of the thresholds must be ﬁxed, so as to center the distribution; a
typical assignment is t1 = 0. Then the conditional probability that yi falls
in category j (j = 1, 2, . . . , c), given β, a, and t = (tmin, t1, . . . , tc−1, tmax)′
is given by
Pr (yi = j|β, a, t) = Pr (tj−1 < li < tj|β, a, t)
= Φ

tj −x′
iβ −z′
ia

−Φ

tj−1 −x′
iβ −z′
ia

= p (yi|β, a, t) .
(14.3)

608
14. Threshold Models for Categorical Responses
The data are conditionally independent, given β, a, and t. Therefore the
sampling model can be written as
p (y|β, a, t) =
n
-
i=1
c

j=1
I (yi = j) p (yi|β, a, t)
=
n
-
i=1
c

j=1
I (yi = j)

Φ

tj −x′
iβ −z′
ia

−Φ

tj−1 −x′
iβ −z′
ia

,
(14.4)
where I (yi = j) is an indicator function taking the value 1 if the response
falls in category j and 0 otherwise.
14.2.2
Prior Distribution and Joint Posterior Density
Adopting the usual hierarchical model building strategy, prior distributions
must be assigned to β, a, and t. It will be assumed, as usual, that the prior
distribution of a depends on an unknown dispersion parameter σ2
a.
The
density of the joint prior distribution adopted has the form
p

β, a, t, σ2
a

= p (β) p

a|σ2
a

p

σ2
a

p (t) .
Hence, the joint posterior density is:
p

β, a, t, σ2
a|y

∝p (y|β, a, t) p (β) p

a|σ2
a

p

σ2
a

p (t)
= p (β) p

a|σ2
a

p

σ2
a

p (t)
×
n
-
i=1
c

j=1
I (yi = j)

Φ

tj −x′
iβ −z′
ia

−Φ

tj−1 −x′
iβ −z′
ia

.
The fully conditional posterior distributions of parameters β, a, t, and
σ2
a must be derived from the above expression. Irrespective of the form of
the joint prior distribution, these conditional processes are not in standard
form, because the parameters appear implicitly inside of the normal inte-
grals. Therefore special strategies must be used for implementing a Gibbs
sampler; see, for example, Moreno et al. (1997).
An algorithmically simpler approach consists of augmenting the joint
posterior distribution with the unobserved liabilities l. If the latent variables
are modelled hierarchically as in a Gaussian linear model for observed data,
this approach yields fully conditional posterior distributions that have a
standard form and which are easy to sample from. Augmenting the joint
posterior with l, the resulting density takes the form
p

β, a, l, t, σ2
a|y

∝p

y|β, a, l, t, σ2
a

p

β, a, l, t,σ2
a

= p (y|l, t) p

β, a, l, t, σ2
a

= p (y|l, t) p (l|β, a) p

β, a, t, σ2
a

= p (y|l, t)
 n
-
i=1
p (li|β, a)

p

β, a, t, σ2
a

.
(14.5)

14.2 Analysis of a Single Polychotomous Trait
609
The second and third lines of the expression above follow because: 1) the
distribution of the polychotomous observations, given the liabilities, de-
pends only on the thresholds, and 2) given β and a, the liabilities are
conditionally independent, as indicated in (14.2). All conditional posterior
distributions can be derived from (14.5), and this will be discussed later
on.
Consider the ith element in the ﬁrst term on the right hand side of (14.5)
Pr (yi = j|li, tj−1, tj) =

1,
if tj−1 < li ≤tj,
0,
otherwise.
(14.6)
That is, the probability that a given data point falls in a given category,
given the value of liability and thresholds, is completely speciﬁed. This
means that p (y|l, t) is a degenerate distribution. Following the notation in
Albert and Chib (1993), p (y|l, t) can be written as
p (y|l, t) =
n
-
i=1
 c

i=1
I (tj−1 < li ≤tj) I (yi = j)

.
(14.7)
The prior distribution of the vector β must be speciﬁed judiciously. It is
well-established that data structures with a small number of observations
(or in extreme cases, when all observations fall into a particular category)
per element or level of β, can lead to poor inferences. Misztal et al. (1989)
have referred to this as the extreme category problem (ECP). The problem
is related to the fact that when all observations for a location parameter
are in one of the extreme categories (e.g., in the binary situation, when
all observations are either 0 or 1), the ML estimate of the corresponding
parameter is not ﬁnite. This is clear in a probit model with a single location
parameter: if all observations are 0′s, the ML estimate of the probability
of response is 0. Hence, the corresponding ML estimate of the location
parameter in the liability scale is −∞. In the context of a hierarchical
structure, this problem propagates to other tiers of the model. For example,
when there are ECP instances for at least some elements of β and when
a uniform prior distribution is assigned to this vector, Bayesian MCMC
inferences about σ2
a can be severely distorted (Hoeschele and Tier, 1995;
Moreno et al., 1997). It is not obvious how this problem can be solved
satisfactorily, although some ad hoc approaches have been suggested in the
literature.
Following Kadarmideen et al. (2002), partition the vector β as
β =

β′
h, β′
r
′ ,
with the corresponding partition for the incidence matrix being
X = [Xh, Xr] .

610
14. Threshold Models for Categorical Responses
Here βh (H × 1) contains elements of β known to have observations with
ECPs (such as small herds of cattle in which mastitis is screened), and
βr contains elements of β with well-structured data. Then, the distorting
eﬀects of the ECP on inferences can be tempered somewhat by assuming
that the vector βh follows, a priori, a normal distribution with a nonnull
mean. A simple possibility is to pose
βh|β, σ2
βh ∼N

1β, Ihσ2
βh

.
(14.8)
Here 1 is a vector of ones, β is a scalar common to all elements of βh,
and σ2
βh is an unknown dispersion parameter. For βr one can assume a
vague normal distribution with zero mean and large, known, variance. For
instance, assuming that all scalar elements of βr are mutually independent,
one can adopt
βr ∼N

0, Ir106
.
(14.9)
Assuming prior independence between βh and βr, the prior distribution of
β has density
p (β) = p

βh|β, σ2
βh

p (βr) .
(14.10)
The scalar β can be assumed to follow the uniform distribution
β|βmin, βmax ∼Un (βmin, βmax)
(14.11)
where βmin and βmax are chosen appropriately. The prior distribution for
σ2
βh can be a scaled inverted chi-square process with known parameters νβ
and Sβ,with density
p

σ2
βh|νβh, Sβh

∝

σ2
βh
−
 νβh
2
+1

exp

−νβhSβh
2σ2
βh

.
(14.12)
The prior for the thresholds arises naturally from the assumptions of
the model. This model postulates that the thresholds are ordered, so it
is sensible to assume that these are distributed as order statistics from a
uniform distribution, in the interval [tmin, tmax]. Also, recall that t1 = 0, to
give an origin to the underlying distribution; hence, there are c−2 unknown
thresholds. Therefore, the joint prior density of t is(Mood et al., 1974):
p (t) = (c −2)!

1
tmin −tmax
c−2
I (t ∈T) ,
(14.13)
where T = {(t1 = 0, t2, . . . , tc−1) |tmin < t1 < t2 < · · · < tc−1 < tmax}.
If, as stated earlier, the location vector a consists of additive genetic
eﬀects (meaning that genetic variation is due to additive and independent
contributions from a large number of loci with small gene substitution
eﬀects), a sensible prior is
a|A, σ2
a ∼N

0, Aσ2
a

,
(14.14)

14.2 Analysis of a Single Polychotomous Trait
611
where σ2
a is the additive genetic variance and A is the usual additive rela-
tionship matrix. In turn, the additive genetic variance can be conveniently
assumed to be distributed, a priori, as a scaled inverted chi-square random
variable with density
p

σ2
a|νa, Sa

∝

σ2
a
−(
νa
2 +1) exp

−νaSa
2σ2a

,
(14.15)
and νa, Sa are known hyperparameters.
The ﬁnal assumption is that the joint prior density of all unknown pa-
rameters, including the liabilities, can be factorized as
p

l, β, β, a, t, σ2
a, σ2
βh

= p (l|β, a) p

βh|β, σ2
βh

p (βr) p

a|σ2
a

p (t) p

σ2
a

p

σ2
βh

,
where the dependency on the hyperparameters is suppressed in the nota-
tion. In view of this and of (14.5), the joint posterior density can be written
as
p

l, β, β, a, t, σ2
a, σ2
βh|y

∝p (y|l, t) p (t)
×
 n
-
i=1
p (li|β, a)

p

βh|β, σ2
βh

p (βr) p

a|σ2
a

p

σ2
a

p

σ2
βh

.
(14.16)
14.2.3
Fully Conditional Posterior Distributions
Liabilities
Notationally, the fully conditional posterior distribution of parameter x
will be represented as p (x|ELSE), where ELSE refers to data y and to
the values of all the parameters that x depends on. Consider ﬁrst the fully
conditional posterior distribution of liability li. In order to obtain this,
one must extract the terms involving li from the joint posterior (14.16).
Since the liabilities are conditionally independent, it follows that, given the
parameters and the liabilities, the polychotomous observations are inde-
pendent as well. This yields
p (li|ELSE) ∝p (yi = j|li, t) p (li|β, a) .
(14.17)
Given the liabilities and the thresholds, the categorical outcome is not
stochastic, since its value is known with certainty. Hence, p (yi = j|li, t) is
a constant and gets absorbed in the Bayes formula. Therefore,
p (li|ELSE) ∝
 c

i=1
I (tj−1 < li ≤tj) I (yi = j) |β, a

,

612
14. Threshold Models for Categorical Responses
where I (tj−1 < li ≤tj) I (yi = j) indicates that liability falls in the interval
tj−1 < li ≤tj if yi = j. Since liability is Gaussian, it follows that this is
the density of a truncated normal distribution, with density
p (li|ELSE) =
φ

x′
iβ + z′
ia,1

Φ

tj −x′
iβ −z′
ia

−Φ

tj−1 −x′
iβ −z′
ia
.
(14.18)
Thresholds
The density of the fully conditional posterior distribution of the ith thresh-
old ti, is
p (ti|ELSE) ∝p (y|l, t) p (t)
∝
N
-
j=1
[I (ti−1 < lj < ti) I (yj = i) + I (ti < lj < ti+1) I (yj = i + 1)] .
(14.19)
The preceding is the collection of all terms in the joint posterior density
where ti appears. For example, consider threshold t2. This will appear
either in connection with liabilities corresponding to responses in either
the second category (where the threshold is an upper bound) or in the
third class (where the threshold is a lower bound). Seen as a function of ti,
(14.19) shows formally that ti lies in an interval whose limits are as follows:
the upper limit must be smaller than or equal to the smallest value of l for
which yj = i + 1. The lower limit is given by the maximum value of l for
which yj = i. The prior condition (t ∈T) is fulﬁlled automatically. Within
these boundaries, the conditional posterior distribution of threshold ti is
the uniform process
p (ti|ELSE) =
1
min (l|y = i + 1) −max (l|y = i),
(14.20)
where min (l|y = i + 1) indicates the minimum value of the liabilities within
observations in category i+1; similarly, max (l|y = i) denotes the maximum
value of liabilities for observations in category i.
A comment about implementation is in order here. The interval
min (l|y = i + 1) −max (l|y = i)
is typically very narrow, and varies little between successive iterates of the
Gibbs sampler. This leads to strong autocorrelations between samples and
slows convergence. Cowles (1996) and Nandram and Chen (1996) propose
alternative algorithms for ameliorating this diﬃculty.
Additive Genetic Variance
The density of the fully conditional posterior distribution of σ2
a is
p

σ2
a|ELSE

∝p

a|σ2
a

p

σ2
a

.

14.2 Analysis of a Single Polychotomous Trait
613
This is identical to the expression for the density of the conditional poste-
rior distribution of σ2
a in a single-trait additive genetic model presented in
Section 13.2. Therefore,
σ2
a|ELSE∼

a′A−1a + νaSa

χ−2
νa+q.
(14.21)
Dispersion Parameter σ2
βh
Recall that σ2
βh is a dispersion parameter describing variability between
levels of eﬀects containing (possibly) ECPs. The density of the fully condi-
tional posterior distribution of σ2
βh is
p

σ2
βh|ELSE

∝p

βh|β, σ2
βh

p

σ2
βh

.
This is clearly in the same form as the density of the conditional distribution
of the additive genetic variance, the diﬀerence being that the prior mean
of each of the elements of βh is β, instead of 0. After making an oﬀset
for the non-null mean, some algebra leads to the scaled inverted chi-square
distribution
σ2
βh|ELSE ∼

(βh −1β)′ (βh −1β) + νβSβ

χ−2
νβh+H,
(14.22)
where H is the number of elements in βh.
Location Eﬀects
We consider ﬁrst the conditional distribution of [β, a] and, subsequently,
that of the scalar parameter β. As usual, the fully conditional posterior dis-
tribution of [β, a] is obtained from (14.16). Extracting the terms containing
β, and a one obtains
p

β, a|σ2
βh, σ2
a, β, l, y

∝
 n
-
i=1
p (li|β, a)

p

βh|β, σ2
βh

p (βr) p

a|σ2
a

∝p

β, a|σ2
βh, σ2
a, β, l

.
This follows because, given the liabilities, the categorical responses y do not
bring any additional information about the location eﬀects. The preceding
expression has a form similar to (13.8), except that l here, replaces y. This
is precisely the density of the joint posterior distribution of the location
eﬀects in a Gaussian hierarchical model, which was discussed extensively
in Chapter 6. Using the result in Example 1.18 of Chapter 1, the fully
conditional posterior distributions follow rather directly. First

βh|βr, a, σ2
a, σ2
βh, β, l, y

∼N

5βh,

X′
hXh +
1
σ2
βh
Ih
−1
,
(14.23)

614
14. Threshold Models for Categorical Responses
where
5βh =

X′
hXh +
1
σ2
βh
Ih
−1 
X′
h (l −Xrβr −Za) + 1 β
σ2
βh
.

Further,

βr|βh, a, σ2
a, σ2
βh, β, l, y

∼N

5βr,

X′
rXr + 10−6Ir
−1
,
(14.24)
where
5βr =

X′
rXr +
1
106 Ir
−1
X′
r (l −Xhβh −Za) .
Likewise, for the additive genetic eﬀects,

a|βh, βr, σ2
a, σ2
βh, β, l, y

∼N

5a,

Z′Z + 1
σ2a
A−1
−1
(14.25)
where
5a =

Z′Z + 1
σ2a
A−1
−1
Z′ (l −Xrβr −Xhβh) .
Using these expressions, a single site updating Gibbs sampler can be de-
veloped as for the Gaussian hierarchical model. Alternatively (and perhaps
more eﬃciently from a computational point of view), a joint updating algo-
rithm can be chosen along the lines described in Section 13.5 of the previous
chapter.
Finally, the density of the fully conditional posterior distribution of the
scalar β is:
p

β|β, a,σ2
βh, σ2
a, t, l, y

∝p

βh|β, σ2
βh

∝exp

−
1
2σ2
βh
(βh −1β)′ (βh −1β)

.
Viewed as a function of β, this is the density of the normal distribution

β|β, a, σ2
βh, σ2
a, t, l, y

∼N

βh,
σ2
βh
H

,
(14.26)
where
βh = 1
H
H

i=1
βhi
and βhi is the ith element of βh, drawn from (14.23).

14.3 Analysis of a Categorical and a Gaussian Trait
615
14.2.4
The Gibbs Sampler
To summarize, the Gibbs sampler consists of iterating through the following
loop:
1. Read through the data ﬁle and sample the liabilities l from the trun-
cated normal distribution with density (14.18).
2. Sample the thresholds from the uniform distribution (14.20).
3. Sample the additive genetic variance from the scaled inverted chi-
square process (14.21).
4. Sample σ2
βh from the scaled inverted chi-square distribution (14.22).
5. Build mixed model equations and their right-hand sides using l as
“data”.
6. Sample the location parameters from the normal distributions (14.23),
(14.24) and 14.25).
7. Sample β from the normal distribution (14.26)
8. Return to Step 1 or terminate when chain length is adequate to meet
convergence diagnostics.
14.3
Joint Analysis of an Ordered Categorical
and a Normally Distributed Trait
The results presented in the previous section are extended for a joint analy-
sis of a model for one categorical and one normally distributed trait. Such a
model could be relevant to study the genetic associations between growth
rate and leg weakness in pigs, for example, since the last trait is scored
categorically. The setting is as in Foulley et al. (1983), who introduced
the model and suggested an approximate Bayesian analysis. The approach
presented here can accommodate a general pattern of missing data. For
the purpose of presentation, a simple additive genetic model is postulated
for each of the two traits. Bayesian MCMC related work can be found in
Jensen (1994), Sorensen (1996), and in Wang et al. (1997). A more gen-
eral model that encompasses several categorical and Gaussian traits was
described by Van Tassell et al. (1998). Korsgaard et al. (2002) proposed
a model for the joint analysis of categorical, censored and Gaussian traits
using the Gibbs sampler.

616
14. Threshold Models for Categorical Responses
14.3.1
Sampling Model
Suppose there are n individuals, each of which is potentially measured for
each of the two traits. However, it is typical that there will be at least
some individuals on which the measurement is available for one of the
traits only. Subscript 1 will refer to the continuous trait, and subscript 2 to
the categorical trait. Denote by y1o and by y2o the vectors of the observed
data for the continuous and categorical trait, respectively, and by y1m and
by y2m, the vectors of the missing data for the continuous and categorical
trait, respectively. Let y′
1 = (y′
1o, y′
1m) and y′
2 = (y′
2o, y′
2m) be vectors of
dimension n each. As in Section 13.4, it is assumed that data are missing
at random. Also, as before, let l (of order n × 1), represent the unobserved
liabilities associated with the categorical trait, which can be partitioned in
an obvious notation as l′ = (l′
o, l′
m).
The approach followed here is to augment the posterior distribution with
(y′
1m, l′) , that is, with the missing data for the continuous trait and all
liabilities. It is assumed that the vector of complete continuous data, which
is deﬁned here as (y′
1, l′) = (y′
1o, y′
1m, l′), is normally distributed, given
vectors of location parameters β =

β′
1, β′
2
′ and a = (a′
1, a′
2)′ , with the
latter being the additive genetic values for the two traits. The order of a1
and a2 is q × 1 each. If the complete continuous data are sorted by trait
and by individual within trait, with the resulting vector labeled as v, the
conditional distribution of the complete continuous data given the location
parameters has the form
v|β, a, Re ∼N


X1β1 + Z1a1
X2β2 + Z2a2

, R

,
(14.27)
where the X′s and Z′s are incidence matrices of appropriate order. In
(14.27), R = Re ⊗In, Re is the 2 × 2 variance–covariance matrix
Re =

σ2
e1
σe1,2
σe1,2
σ2
e2

,
(14.28)
and In is an n × n identity matrix. As in Section 13.4, the data can be
augmented with residuals as in Wang et al. (1997), in which case the ap-
propriate rows of incidence matrices X and Z in (14.27) have all elements
equal to zero.
Following the results in Example 1.17 of Chapter 1, the density associated
with (14.27) can be written as
p (v|β, a, Re) ∝|Re|−n
2 exp

−1
2tr

R−1
e Se

.
(14.29)
Here
Se =

e′
1e1
e′
1e2
e′
2e1
e′
2e2


14.3 Analysis of a Categorical and a Gaussian Trait
617
is a matrix of sums of squares and products involving the residuals
e1 = y1 −X1β1−Z1a1,
e2 = l −X2β2−Z2a2.
14.3.2
Prior Distribution and Joint Posterior Density
Based on the assumptions of the inﬁnitesimal model , the prior distribution
[a1, a2|A, G0], is taken to be the multivariate normal process

a1
a2
"""" A, G0 ∼N


0
0

, G0 ⊗A

,
(14.30)
where
G0 =

σ2
a1
σa12
σa12
σ2
a2

is the additive genetic (co)variance matrix between the two traits, and A
is the q ×q additive relationship matrix between members of the genealogy
(recall that, typically, q > n) .
The treatment of the vector β requires extending the developments for
dealing with potential ECPs for the polychotomous trait to a bivariate
situation. We adopt the partition
β =


β1h
β1r
β2h
β2r

,
where β2h are location eﬀects on liabilities whose levels have (possibly)
ECPs; the vector β1h contains the eﬀects of these levels on the Gaussian
trait. Subsequently, it is assumed that the prior distribution of β is such
that its density factorizes as
p (β) = p (β1h, β2h) p (β1r) p (β2r) ,
where
p (β1r) ∝constant
if β1r,min < β1r< β1r,max, and
β2r ∼N

0, I2r106
.
Further,
 β1h
β2h
"""" β, Bh ∼N


0
1β

, Bh ⊗I

,
(14.31)

618
14. Threshold Models for Categorical Responses
where
Bh =

σ2
β1,h
σβ12,h
σβ12,h
σ2
β2,h

.
Here σ2
β1,h and σ2
β2,h are variance components and σβ12,h is the covariance
between the β1h and β2h eﬀects on the two traits. As in the previous
section, the scalar parameter β is assigned the uniform prior distribution
β|βmin, βmax ∼Un (β|βmin, βmax) ,
(14.32)
where βmin and βmax are appropriately chosen hyperparameters.
It is assumed that the matrices Re, G0, and Bh follow independent
scaled-inverted Wishart distributions, a priori. The respective densities are
p (Re|υe, Ve) ∝|Re|−1
2 (υe+3) exp

−1
2 tr

R−1
e V−1
e

,
(14.33)
p (G0|υ0, V0) ∝|G0|−1
2 (υ0+3) exp

−1
2 tr

G−1
0 V−1
0

,
(14.34)
and
p (Bh|υh, Vh) ∝|Bh|−1
2 (υh+3) exp

−1
2 tr

B−1
h V−1
h

,
where υi and Vi (i = e, 0, h), are the usual parameters of the scaled in-
verted Wishart distributions. An important point: although the liabilities
are (conditionally) Gaussian, the fact that the responses are categorical
imposes some conditions on the form of the inverse Wishart distribution
with density as in (14.33). We shall return to this issue later on.
The unknown thresholds in t delimiting the c categories of response, are
assumed to be distributed a priori as ordered statistics from a uniform
distribution in the interval [tmin, tmax], as in (14.13).
The parameter vector is augmented with the missing data for the con-
tinuous trait (y1m) and with the unobserved liabilities l. The parameters
of the augmented model are represented as (Ω, y1m, l) ,where
Ω= (β, a, G0, Re, β, Bh, t) .
Before embarking on the derivation of the fully conditional posterior
distributions, we focus on the conditional distribution [y2o|lo, Ω] of the
observed categorical responses, given their liabilities and Ω. Recall that,
given the liabilities li and the thresholds, the categorical responses y2o
are known with certainty. This means that given (Ω, y1m, l) , that is, the
parameters of the augmented model, the observations y1o (y1m) and y2o
are conditionally independent. Ignoring hyperparameters in the notation,
the joint posterior density of all uncertain variables in the augmented model

14.3 Analysis of a Categorical and a Gaussian Trait
619
is
p (Ω, y1m, l|y1o, y2o) ∝p (y1o, y2o|Ω, y1m, l) p (Ω, y1m, l)
∝p (y2o|y1o, Ω, y1m, l) p (y1o|Ω, y1m, l) p (Ω, y1m, l)
∝p (y2o|y1, Ω, l) p (y1, l|Ω) p (Ω)
∝p (y1, l|Ω) p (y2o|l, Ω) p (Ω) .
(14.35)
The last line follows from the conditional independence of y1 and y2o,
given l and t. The ﬁrst term on the right-hand side term of (14.35) is the
density of the sampling model for the complete continuous data, as given
in (14.29). The second term is not stochastic (given l and t one knows the
categories of response with certainty), so it gets absorbed as a constant in
Bayes theorem. The third term is the density of the joint prior distribution
of the parameters, which is assumed to factorize as
p (Ω) ∝p (β1h, β2h|β, Bh) p (β1r) p (β2r) p (a|G0) p (G0) p (Re) p (Bh) p (t) .
(14.36)
Although the conditioning on Ωwill be kept to simplify notation, note
that in (14.35)
p (y1, l|Ω) = p (y1, l|β, a, Re)
and
p (y2o|l, Ω) = p (y2o|l, t) .
Consider now the prior distribution of the variance–covariance matrix
of the sampling model. If Re is assigned an inverted Wishart distribution,
then σ2
e2 (the residual variance of the liability) should be stochastic, so it
cannot be ﬁxed at some value. Treating σ2
e2 as a random variable requires
parameterizing the model such that two thresholds, instead of only one,
are given arbitrary values. Again, these must satisfy tmin < t1 < t2 < · · · <
tc−1 < tmax (Sorensen et al., 1995). Typical choices are t1 = 0 and t2 = 1.
However, this requires that the data fall into three or more categories of re-
sponse. If the data are binary, this parameterization is not possible. When
this is the case, a prior density p

Re|σ2
e2 = 1

in the form of a scaled in-
verted Wishart can be speciﬁed. It turns out that the fully conditional pos-
terior density p

Re|ELSE, σ2
e2 = 1, data

is also scaled inverted Wishart.
A simple way of drawing samples from this distribution is based on the
properties of the inverted Wishart distribution described in Subsubsection
1.4.6 of Chapter 1. The algorithm was presented by Korsgaard et al. (1999),
and is summarized at the end of this section.
14.3.3
Fully Conditional Posterior Distributions
As before the fully conditional posterior distribution of parameter x say,
is written as p (x|ELSE), where now y = (y1o, y2o). We start by deriving
the fully conditional posterior distribution of the missing data (y1m, l).

620
14. Threshold Models for Categorical Responses
Case
Observed data
Missing data
Generate
1
y1o,i, y2o,i
none
lo,i
2
y1o,i
trait 2
lm,i
3
y2o,i
trait 1
y1m,i, lo,i
TABLE 14.1. Possible patterns of observed and missing data.
Missing Continuous Data and Liabilities
From the sampling model in (14.27), it follows that a missing continuous
record for individual i, say, is sampled from
y1m,i|ELSE ∼N (E (y1m,i|ELSE) , V ar (y1m,i|ELSE)) .
Here,
E (y1m,i|ELSE) = x′
1,iβ1 + z′
1,ia1 + σe1,2
σ2
e2
(li −x′
2iβ2 −z′
2ia2)
(14.37)
and
V ar (y1m,i|ELSE) = σ2
e1

1 −(σe1,2)2
σ2
e1σ2
e2

,
(14.38)
where σ2
e2 = 1. Thus, the conditional distribution of a missing Gaussian
observation, given the liabilities, the observed data, and all parameters does
not depend on the observed data. In these expressions, x′
1i (x′
2i) and z′
1i
(z′
2i) are rows of matrices X1 (X2) and Z1 (Z2) associated with individual
i. In (14.37), if the joint posterior is augmented with the residuals (instead
of with the missing observations), elements of x′
1i and of z′
1i are all equal
to zero.
The next step involves drawing samples of the underlying vector l, not-
ing that all liabilities are conditionally independent. The possible patterns
of missing data for the ith record (i = 1, 2, . . . , n) are shown in Table 14.1.
In the table, subscripts 1o, i (2o, i) associated with y, represent the ob-
served continuous (categorical) record of the ith individual, and subscripts
1m, i (2m, i) represent the missing continuous (categorical) record of the
ith individual.
If the pattern of missing records is as in Case (1), both records on the
individual are available, so the fully conditional posterior distribution of
lo,i must be derived here. From the joint posterior density presented in
(14.35), and exploiting the conditional independence assumptions, the re-
quired conditional density is
p (lo,i|ELSE) ∝p (y1o,i, lo,i|Ω) p (y2o,i|lo,i, Ω)
∝p (lo,i|y1o,i, Ω)


c

j=1
I (tj−1 < lo,i ≤tj) I (y2o,i = j)

.
(14.39)

14.3 Analysis of a Categorical and a Gaussian Trait
621
From (14.27), density (14.39) is recognized as that of a truncated condi-
tional normal distribution, with truncation points at tj−1 and tj. The mean
of the untruncated distribution is
E (lo,i|y1o,i, Ω) = x′
2,iβ2 + z′
2,ia2 + σe1,2
σ2
e1

y1o,i −x′
1,iβ1 −z′
1,ia1

(14.40)
and the variance is
V ar (lo,i|y1o,i, Ω) = σ2
e2

1 −(σe1,2)2
σ2
e1σ2
e2

,
(14.41)
where σ2
e2 = 1.
If the pattern of missing data is as in Case (2), the observation for the
categorical trait is missing and lm,i must be generated. With y2o,i absent,
it follows from (14.35) that the density of the fully conditional posterior
distribution of lm,i is proportional to p (y1, l|Ω). Therefore, recalling the
conditional independence structure,
p (lm,i|ELSE)
∝
p (y1o,i, lm,i|Ω)
∝
p (lm,i|y1o,i, Ω) .
(14.42)
From (14.35), this is a conditional normal distribution with mean and vari-
ance given by (14.40) and (14.41), respectively. Thus,
lm,i|ELSE ∼N [E (lo,i|y1o,i, Ω) , V ar (lo,i|y1o,i, Ω)] .
(14.43)
Finally, if the pattern of missing data is as in Case (3), both y1m,i and
lo,i must be sampled. From (14.35) we can write
p (y1m,i, lo,i|ELSE) ∝p (y1m,i, lo,i|Ω) p (y2o,i|lo,i, Ω)
= p (y1m,i, lo,i|Ω)
c

j=1
I (tj−1 < lo,i ≤tj) I (y2o,i = j)
= p (y1m,i|lo,i, Ω) p (lo,i|Ω)
c

j=1
I (tj−1 < lo,i ≤tj) I (y2o,i = j) .
(14.44)
A simple way of obtaining samples from the distribution with density
(14.44) is ﬁrst to sample a realized value l∗
o,i from the normal distribu-
tion [lo,i|Ω] truncated at tj−1 and at tj, and second, to sample y1m,i from
the conditional normal distribution

y1m,i|l∗
o,i, Ω

. Again, conditional inde-
pendence holds, so the process can be eﬀected piecewise, observation by
observation.
Location Eﬀects
We derive now the fully conditional posterior distribution of θ′ =

β′, a′
and, subsequently, that of the scalar β. From (14.35), and noting that
p (y2o|l, Ω) is not a function of θ, we get

622
14. Threshold Models for Categorical Responses
p (θ|ELSE) ∝p (y1, l|Ω) p (Ω)
= p (y1, l, θ,β, G0, Re, Bh, t)
∝p (θ|y1, l,β,G0, Re, Bh) .
(14.45)
This is the density of a bivariate Gaussian hierarchical model in which
the continuous responses and the liabilities (the complete continuous data)
are the observations. Hence, the conditional posterior distribution of the
location eﬀects θ is a multivariate normal process, with its mean vector and
variance–covariance matrix calculated as seen in Chapter 13. The general
expression for the fully conditionals is very similar to equations (13.50),
(13.51), and (13.52), with a slight modiﬁcation in the interpretation of
some terms, to accommodate the diﬀerent model scenarios.
Consider the fully conditional posterior distributions of β and a. Let
G−1
0
=

g11
g12
g21
g22

,
R−1
e
=

r11
r12
r21
r22

,
and
B−1
h
=

b11
b12
b21
b22

.
Then
[β|ELSE] ∼N

5β, 5Vβ

,
(14.46)
where the mean vector has the following four partitions:
5β =


5β1h
5β2h
5β1r
5β2r

.
An explicit representation of the mean vector is arrived at by writing the
incidence matrix of all elements of β as
X =
 X1h
0
X1r
0
0
X2h
0
X2r

.
With this notation
5β =

X′ 
R−1
e
⊗In

X + P−1−1 
X′ 
R−1
e
⊗In

(v −Za) + m

,
where
Za =
 Z1
0
0
Z2
  a1
a2

,

14.3 Analysis of a Categorical and a Gaussian Trait
623
P−1 =


b11I
b12I
0
0
b21I
b22I
0
0
0
0
0
0
0
0
0
10−6I

,
and
m = P−1


0
Iβ
0
0

=


b12βI
b22βI
0
0

.
The variance–covariance matrix of the conditional posterior distribution in
(14.46) is
5Vβ =

X′ 
R−1
e
⊗In

X + P−1−1 .
The fully conditional posterior distribution of a is
[a|ELSE] ∼N

5a, 5Va

,
(14.47)
where
5a =

Z′ 
R−1
e
⊗In

Z + G−1
0
⊗A−1−1 Z′ 
R−1
e
⊗In

(v −Xβ) ,
and
5Va =

Z′ 
R−1
e
⊗In

Z + G−1
0
⊗A−1−1 .
The location eﬀect remaining to be sampled is β. It follows from the joint
posterior density (14.35) and from the form of the prior in (14.36) that
p (β|ELSE) ∝p (β1h, β2h|β, Bh)
∝p

β1h|σ2
β1,h

p (β2h|β1h, β, Bh)
∝p (β2h|β1h, β, Bh) ,
which is multivariate normal, since [β1h, β2h|β, Bh] is multivariate normal.
Further, (14.31) indicates that the pairs

β1h,k, β2h,k

and

β1h,k′, β2h,k′

are mutually independent, a priori, where k = 1, 2, ..., H, denotes a level of
the factor possibly associated with ECP problems. Hence
p (β|ELSE) ∝
H
-
k=1
p

β2h,k|β1h,k, β, Bh

∝
H
-
k=1
exp

−

β2h,k −µβ2.1,k
2
2τ 2
β2.1

,
where
µβ2.1,k = β +
σβ12,h
σ2
β1,h
β1h,k,

624
14. Threshold Models for Categorical Responses
and
τ 2
β2.1 = σ2
β2,h


1 −

σβ12,h
2
σ2
β1,hσ2
β2,h


.
Now, deﬁning the oﬀset β∗
2h,k = β2h,k −
σβ12,h
σ2
β1,h β1h,k, the density of the
conditional posterior distribution of β can be written as
p (β|ELSE) ∝
H
-
k=1
exp

−

β∗
2h,k −β
2
2τ 2
β2.1

.
Viewed as a function of β, it follows that this is the density of a normal
distribution with mean β =
1
H
H

k=1
β∗
2h,k and variance
1
H τ 2
β2.1, where H is
the number of levels of the factor with ECPs. In short,
β|ELSE ∼N





H

k=1
β∗
2h,k
H
,
σ2
β2,h
H


1 −

σβ12,h
2
σ2
β1,hσ2
β2,h







.
(14.48)
Dispersion Parameters
The fully conditional posterior distributions of the covariance matrices are
derived by extracting the relevant terms from (14.35). In view of the form of
the prior in (14.36), all dispersion matrices are conditionally independent,
given all other parameters. Thus,
p (G0|ELSE) ∝p (Ω)
∝p (a|G0) p (G0)
(14.49)
which is identical to (13.55). Therefore, the distribution is as in (13.56)
G0|ELSE ∼IW2

V−1
a
+ Sa
−1 , υa + q

.
(14.50)
Likewise,
p (Bh|ELSE) ∝p (β1h, β2h|β, Bh) p (Bh) .
(14.51)
This is the density of the inverse Wishart process
Bh|ELSE ∼IW2

V−1
h
+ Sh
−1 , υh + H

,
(14.52)
where
Sh =

β′
1hβ1h
β′
1h (β2h −1β)
(β2h −1β)′ β1h
(β2h −1β)′ (β2h −1β)

.

14.3 Analysis of a Categorical and a Gaussian Trait
625
Similarly, for the residual covariance matrix,
p (Re|ELSE) ∝p (y1, l|Ω) p (Ω)
∝p (y1, l|β, a, Re) p (Re) .
This is identical to (13.53). The resulting fully conditional distribution is:
Re|ELSE∼IW2

Se + V−1
e
−1 , υe + n

(14.53)
with the term Se appropriately deﬁned.
Thresholds
The fully conditional posterior distribution of the jth unknown threshold
is obtained as follows. From (14.35)
p (tj|ELSE) ∝p (y2o|l, Ω) p (Ω)
∝
n2o
-
i=1
[I (tj−1 < lo,i < tj) I (y2o,i = j) + I (tj < lo,i < tj+1) I (y2o,i = j + 1)] ,
(14.54)
where n2o is the number of observed categorical records. This expression is
identical to (14.19).
14.3.4
The Gibbs Sampler
To summarize, a Gibbs sampler can be run as follows:
1. Read in the data ﬁle and generate missing data (and liabilities) from
(14.39), (14.43) or (14.44), depending on the pattern of missing data.
2. Build the mixed model equations.
3. Sample θ from (14.45), with the distributions given explicitly in
(14.46) and (14.47). Recall that the samples can be drawn either
blockwise or piecewise, in which case the expression must be modi-
ﬁed slightly.
4. Sample the scalar β from (14.48).
5. Sample the covariance matrices from (14.50), (14.52) and (14.53).
6. Sample the thresholds from (14.54).
7. Go to Step 1 or exit if chain is long enough.
Like in the Gaussian multiple-trait case, note that the coeﬃcient matrix
of the mixed model equations has to be recreated every iterate with the
strategy described above.

626
14. Threshold Models for Categorical Responses
14.3.5
Implementation with Binary Traits
As mentioned above, when there are only two categories of response the
residual covariance matrix can be speciﬁed as
Re =

σ2
e1
σe1,2
σe1,2
1

.
From a Bayesian perspective, this is a random matrix with two stochas-
tic elements

σ2
e1, σe1,2

instead of three. This is so because the residual
variance in the liability scale is set equal to 1, since the parameter is not
identiﬁable from the likelihood. In this situation, rather than adopting an
inverse Wishart prior for Re, one can assign a conditional inverse Wishart
prior (an inverse Wishart distribution, conditional on σ2
e2 = 1). The result-
ing fully conditional posterior distribution

Re|ELSE, σ2
e2 = 1, data

(14.55)
turns out to have the form of a conditional inverse Wishart distribution
also (conditional on σ2
e2 = 1). Korsgaard et al. (1999) have shown how to
draw samples from such a distribution. They described a simple algorithm,
where a more general formulation than the one described here can be found.
Based on the properties of the Wishart distribution presented in Subsection
1.4.6 of Chapter 1, the Gibbs sampler can proceed as follows. Implement
the algorithm in the manner described in the previous section, with the
exception of the draw involving (14.53), which is replaced by a draw from
(14.55). In order to obtain a realized value from the latter,
• Sample X1 from (1.112).
• Sample X2 from (1.113).
• Construct T11 from (1.109), T12 from (1.110) and set T22 = X−1
3
= 1
Then T11, T12 and T22 are the elements of the draw from
Re|ELSE, σ2
e2 = 1, data.
A ﬁnal comment is in order. The threshold model is perhaps appealing
in quantitative genetics because all the theory available for additive in-
heritance carries on to the liability scale. The reader must be aware that
there are alternative methods of analysis of categorical responses; see, for
example, Fahrmeir and Tutz (2001) or Agresti (1989, 1990, 1996).

15
Bayesian Analysis of
Longitudinal Data
15.1
Introduction
Suppose individuals are sampled from a set of populations, with the latter
deﬁned in a statistical, rather than demographic, sense. In at least some of
the individuals, the trajectory of a trait is measured over a period of time,
collecting, thus, a time series of observations. Examples of such longitudinal
trajectories are: milk yield of a dairy cow at several points in the course of
lactation, body weight or feed intake measured repeatedly during some test
period in which animals grow, proliﬁcacy (litter size produced) of a sow in
the course of her lifetime, wool growth assessed at diﬀerent stages of the
development of a ewe, presence or absence of clinical mastitis in a dairy cow
in each bi-weekly period from calving until the end of lactation, the height
of a tree as it grows, etc. The trait monitored may be either “continuous”
(e.g., milk yield or body weight), or a count (e.g., lambs per litter over
a series of litters), or binary (presence or absence of a disease at a given
time). Issues of interest may include inferring the expected trajectory of
the trait within an individual or assessing sources of variation, genetic and
nongenetic, among the trajectory patterns of groups of individuals. This
type of problem is fairly old in the study of animal systems. For example,
lactation curves and growth functions have been the subject of research for
decades in milk- and meat-producing species, respectively. Animals diﬀer
in their rate of growth or adult body weight, and it is known that there is
genetic variation for these features, both between and within breeds, that

628
15. Bayesian Analysis of Longitudinal Data
can be exploited in animal breeding programs. The question posed is: How
can this variation be assessed adequately?
In the animal and veterinary sciences, there has been renewed interest in
the analysis of longitudinal records of performance. Perhaps this is a con-
sequence of more intensive recording systems (for instance, in dairy cattle
production it is possible to monitor instantaneous milk ﬂow) and of better
statistical methods for the analysis of longitudinal mixed eﬀects models.
In particular, linear random regression models (Laird and Ware, 1982) or
similar approaches have been applied in animal breeding, where there is
a large body of literature in connection with the analysis of “test-day”
yields in dairy cattle (e.g., Kirkpatrick et al., 1994; Jamrozik and Scha-
eﬀer, 1997; Wiggans and Goddard, 1997). Similar applications have been
made in meat-producing species. For example, an assessment of growth in
beef cows from 19 to 119 months of age is in Meyer (1999).
In this chapter, the analysis of longitudinal data will be dealt with in a
parametric Bayesian framework, to illustrate the ﬂexibility and attractive-
ness of the paradigm. First, a description is given of hierarchical or multi-
stage models for describing longitudinal observations, assuming Gaussian
processes throughout. Second, methods for an approximate Bayesian anal-
ysis of these models are described. These methods can be construed as
generalizations of the usual “tandem” employed for analysis of mixed ef-
fects linear models in animal breeding, consisting of BLUP (of random
eﬀects) plus likelihood-based procedures for parameter inference. However,
at least in theory, a Bayesian hierarchical probability model can yield exact
ﬁnite sample inferences about all unknowns. Hence, a subsequent section
discusses an implementation based on MCMC procedures. We also discuss
some extensions, including alternative structures for the residual dispersion
of the process.
15.2
Hierarchical or Multistage Models
Envisage a setting where, in a randomly drawn sample, each individual is
measured longitudinally at several times. For example, suppose that male
and female rabbits from several breeds are weighed at several phases of their
development, from near birth to the adult stage. An objective might be to
study growth patterns of the two sexes in each of the breeds, while taking
into account interindividual variability. Typically, there will be variation in
the number of measurements per individual, leading at least to longitudinal
unbalancedness. In individuals with sparse information, the individual tra-
jectories would probably be estimated imprecisely, unless information from
relatives is abundant or prior information about the expected trajectory is
sharp.

15.2 Hierarchical or Multistage Models
629
A hierarchical or multistage model consists of a series of nested functional
speciﬁcations, together with the associated distributional assumptions. An
important paper introducing the basic ideas is that of Lindley and Smith
(1972). In the context of longitudinal data, at the ﬁrst stage of the model,
a mathematical function is used to describe the expected trajectory of
individuals, and a stochastic residual having some distribution reﬂects the
departure of the observations from such a trajectory. At the second stage, a
submodel is used to describe the interindividual variation of parameters of
the ﬁrst-stage speciﬁcation. A second-stage residual is included to reﬂect
the inability of the submodel to explain completely the variation of the
parameters. Additional stages can be imposed in a Bayesian context to
describe uncertainty about all other unknown parameters. We shall now
proceed to describe each of these stages systematically.
15.2.1
First Stage
The trajectory (body weights of the same individual, for example) will be
described with the parametric model
yi = fi (θi, ti) + εi,
i = 1, 2, . . . , M,
(15.1)
where yi = {yij} (i = 1, 2, . . . , M, j = 1, 2, . . . , ni) is an ni × 1 vector of
records on the trajectory of individual i; fi (θi, ti) is its expected trajectory
(e.g., expected growth curve) given a vector of animal-speciﬁc parameters
θi of order r × 1, and ti is an ni × 1 vector of known times of measure-
ment. In (15.1), the ni × 1 residual vector εi represents the inability of the
function fi (θi, ti) of reproducing the observed body weights yi exactly. An
observation on individual i at time j is then
yij = fij (θi, tij) + εij,
(15.2)
so the parameters θi dictate the form of the expected trajectory of indi-
vidual i. For example, when describing animal growth, use is often made of
what is called a Gompertz growth function (e.g., Blasco and Varona, 1999).
Here r = 3; one of the parameters represents adult or asymptotic weight
(i.e., body weight as time goes to inﬁnity), the second parameter is related
to growth rate, and the third parameter bears an interpretation in terms
of the “initial conditions” of growth. Often, animals that are measured at
a given time are clustered in diﬀerent contemporary groups. For example,
for some dairy cows the yield may be recorded in February, while for other
cows it may be recorded in December. In this situation, the model can be
written in a slightly more general form as
yijk = Gk + fij (θi, tij) + εijk,
where Gk is an eﬀect peculiar to all measurements taken on individuals
belonging to group k (k = 1, 2, ..., K) , e.g., month–year at which milk yield

630
15. Bayesian Analysis of Longitudinal Data
at time tij is measured on cow i. Since the group eﬀect enters linearly in
the model, it can be dealt with in a straightforward manner. For example,
in the context of a Gibbs sampler, after Gk is drawn, one would form the
oﬀset yijk −Gk, with the conditional model thus being again in the form
of speciﬁcation (15.2). Hence, the simpler model suﬃces for descriptive
purposes without great loss of generality.
The relationship between observed body weights and parameters may
be linear or nonlinear, the latter being the case in the Gompertz function.
In a linear speciﬁcation, the derivatives of the model with respect to the
parameters do not depend on θi. This can be stated as
∂fij (θi, tij)
∂θi
= hij,
where hij is an r × 1 vector of constants not involving θi. On the other
hand, in a nonlinear model,
∂fij (θi, tij)
∂θi
= hij (θi) ,
indicating that the vector hij (θi) involves the parameters, although some
may enter linearly into the model.
Example 15.1
Quadratic trajectory
Suppose a longitudinal process can be described with the ﬁrst-stage model
yij = ai + bitij + cit2
ij + εij,
i = 1, 2, . . . , M, j = 1, 2, . . . , ni.
Here,
θi =


ai
bi
ci

, and ti =


ti1
ti2
...
tini

.
In matrix form, the observations made on individual i can be written as


yi1
yi2
...
yini

=


1
ti1
t2
i1
1
ti2
t2
i2
...
...
...
1
tini
t2
ini




ai
bi
ci

+


εi1
εi2
...
εini

,
so
fi (θi, ti) =


1
ti1
t2
i1
1
ti2
t2
i2
...
...
...
1
tini
t2
ini




ai
bi
ci

= Hiθi,

15.2 Hierarchical or Multistage Models
631
where Hi is an incidence matrix. Then
∂f ′
i (θi, ti)
∂θi
= H′
i =


1
1
· · ·
1
ti1
ti2
· · ·
tini
t2
i1
t2
i2
· · ·
t2
ini


=

hi1
hi2
· · ·
hini

.
Since the matrix of derivatives (or incidence matrix) does not involve any
of the parameters, the model is linear.
■
Example 15.2
Growth curve
Typically, animals grow at an increasing rate from birth to puberty, and
at a decreasing rate thereafter, until a mature or asymptotic weight is
reached. The resulting average growth curve (assuming a large group of
animals treated under similar conditions) is usually S-shaped. Suppose the
trajectory of the body weight of an individual from birth onward can be
described by the mathematical model
yij = Ai [1 −Bi exp (−Kitij)]−1 + εij,
where Ai, Bi, and Ki are parameters indicating some aspect of growth.
For example, for positive Ki, when tij →∞, E (yij) →Ai, which is inter-
pretable as adult body weight. Likewise, as tij →0, E (yij) →Ai/(1−Bi),
interpretable as birth weight. Hence, 1/(1 −Bi) is the fraction of adult
body weight attained at birth and −Bi/(1−Bi) represents the fraction yet
to be attained during the growth process. Then, by deﬁnition, Bi must be
a negative parameter and related to degree of maturity at birth (as Bi →0
the animal is more mature at birth). The parameter Ki is a rate. Here, as
in Example 15.1, r = 3 and the derivatives of interest are
hij (θi) =


∂fij (θi, tij)
∂Ai
∂fij (θi, tij)
∂Bi
∂fij (θi, tij)
∂Ki


=


[1 −Bi exp (−Kitij)]−1
Ai exp (−Kitij) [1 −Bi exp (−Kitij)]−2
−AiBitij exp (−Kitij) [1 −Bi exp (−Kitij)]−2

.
Clearly the model is not linear, although the degree of nonlinearity varies
from parameter to parameter. For example, the partial gradient of the
model with respect to Ai involves only Bi and Ki, while the other two
partial gradients involve all three parameters.
■

632
15. Bayesian Analysis of Longitudinal Data
Example 15.3
Inverse third-order polynomial
A third-order inverse polynomial has the functional form
yij =
1
βi0 + βi1tij + βi2t2
ij + βi3t3
ij + εij
,
where θ′
i = [βi0, βi1, βi2, βi3] are parameters peculiar to individual i and
εij is a residual having a null expectation. The ﬁrst derivatives of the model
with respect to the parameters can be written as
∂fij (θi, tij)
∂βk
=
−xijk

βi0 + βi1tij + βi2t2
ij + βi3t3
ij + εij
2 ,
xijk = tk
ij,
k = 0, 1, 2, 3.
The model, thus, is not linear in the parameters. Note, however, that if a
reciprocal transformation of the observations is made, the model becomes
linear since
zij = 1
yij
= βi0 + βi1tij + βi2t2
ij + βi3t3
ij + εij,
and the derivatives now do not involve any of the parameters. It is impor-
tant to observe, however, that if the original model had the error entering
as
yij =
1
βi0 + βi1tij + βi2t2
ij + βi3t3
ij
+ εij,
the model would be nonlinear, even after making a reciprocal transforma-
tion. The two models for yij are not the same, as distinct assumptions are
made about the errors. In the ﬁrst model, the errors are additive to the
expectation of the reciprocal (zij) of the random variable of interest. In the
second model, the errors are additive to the reciprocal of the expectation
of the reciprocal of yij.
■
Returning to the ﬁrst-stage model, the entire vector of records can be
represented as
y = f (θ, t) + ε,
(15.3)
where θ is the Mr × 1 vector of parameters of all individuals, t contains
times of measurement, and ε is the M
i=1 ni × 1 vector of residuals. Com-
monly, it is assumed that the ﬁrst-stage residuals are mutually independent
between individuals, but some dependence within trajectories may exist.
Hereinafter, given the parameters, the observations taken in diﬀerent in-
dividuals will be assumed to be conditionally independent of each other.
Possible dependencies, such as those resulting from genetic or environmen-
tal relatedness between individuals, can be introduced in the next stage of
the model. Assuming normality of the residuals (sometimes, a thick-tailed

15.2 Hierarchical or Multistage Models
633
distribution, such as Student-t, may be a more sensible speciﬁcation), the
density of the ﬁrst-stage distribution is expressible as
yi|θi, γ ∼N [fi (θi, ti) , Ri (γ)] ,
i = 1, 2, . . . , M,
(15.4)
with yi being independent of yj, for all such pairs, conditionally on the
parameters and on γ. In (15.4), Ri (γ) is an ni × ni ﬁrst-stage variance–
covariance matrix, which depends on γ, a vector of dispersion parame-
ters. For example, if residuals are independently and identically distributed
within individuals, then Ri (γ) = Iniγ, where γ is the variance about the
expected trajectory, so γ would be a scalar parameter here.
The form of the matrix Ri (γ) depends on the dispersion assumptions
made. Some possible alternatives to the preceding speciﬁcation are dis-
cussed below.
(1) Residuals may be independently distributed, but heteroscedastic across
individuals. Then
Ri (γ) = Iniγi,
i = 1, 2, . . . , M.
Here γ′ = [γ1, γ2, . . . , γM] and γi is the variance about the trajectory of
individual i.
(2) The residuals may be heteroscedastic across times of measurement.
In this situation, there would be a ﬁrst-stage residual variance component
for each of the times at which the trajectory is evaluated (if this is done
at ﬁxed times, as in experimental settings, for example, every three weeks
when measuring body weights in children).
(3) Perhaps the residuals are neither homoscedastic nor independently
distributed. For example, there may be a ﬁrst-order autoregressive process
with heterogeneous variance across the times at which measurements are
taken, such that
Cov

εit, εi(t+k)

= ρkγt,
where ρ is a ﬁrst-stage residual correlation and γt is the residual variance at
time t (t = 1, 2, . . . , T) . If the variance were homogeneous, this would make
observations taken adjacently in time to be more correlated than those
farther apart. Another speciﬁcation may be a Markov-type process, where
observations are dependent only if the measurement times are contiguous,
but not otherwise.
(4) A structural model may be entertained for the residual variance. For
example, Blasco and Varona (1999) used a Gompertz function to describe
body weight growth in rabbits, and proposed modeling the trajectory of the
residual standard deviation over time using the Gompertz function as well.
The parameters of this function were assumed to be homogeneous across
individuals. An even more ambitious model would consist of building up
a hierarchical model for the trajectory of the standard deviation, where
parameters vary according to some explanatory variables.

634
15. Bayesian Analysis of Longitudinal Data
The density of the conditional distribution in (15.4), over all individuals,
is then
p (y|θ, γ) ∝
M
-
i=1
|Ri (γ)|−1
2 exp

−1
2ε′
iR−1
i
(γ) εi
%
,
(15.5)
where εi = yi−fi (θi, ti) , from (15.1). If individual parameters are inferred
via maximum likelihood from this ﬁrst stage model, unstable estimates may
be obtained, specially for subjects having few observations. If heterogeneity
between individuals is accounted for somehow using a submodel, perhaps
the total variation can be described in terms of fewer parameters.
15.2.2
Second Stage
The second stage of the model is a statement of how individual-speciﬁc
parameters are thought to vary according to explanatory factors, some of
these perhaps representing genetic sources of variation. In brief, as stated
earlier, the ﬁrst stage of the model delineates the expected trajectory that
longitudinal observations take within a given subject, while the second
stage accounts for cross-sectional (between-subject) heterogeneity of pa-
rameters of the trajectory.
In order to facilitate implementation, it is convenient to assume that the
second stage of the model is linear in the eﬀects of the explanatory variables.
However, at least in theory, there is no reason to preclude a nonlinear
speciﬁcation, particularly if this is dictated by mechanistic considerations.
Hereinafter, it will be assumed that the trajectory parameters are described
suitably by the linear model
θi = Xiβ + ui + ei,
i = 1, 2, . . . , M.
(15.6)
Above, the vector β represents the eﬀects of p explanatory variables con-
tained in the r×p matrix Xi (without loss of generality, this matrix will be
assumed to have full-column rank), ui are subject-speciﬁc eﬀects on each
of the r parameters, and ei is a vector of second-stage residuals. Similar to
the errors in the ﬁrst stage, these residuals represent discrepancies between
the second-stage explanatory structure Xiβ + ui and the “true values” θi.
In animal breeding applications, for example, the vector ui may be addi-
tive genetic eﬀects on trajectory parameters, and these may or may not be
identiﬁable separately from the residual vector ei, depending on the genetic
relationship structure. For example, suppose the θ′s represent parameters
of a three-coeﬃcient lactation curve for each of 100 cows, and that such
cows are progeny of a set of 20 sires, each having 5 daughters. In this case,
one may wish to employ a double subscript in the notation and let θij
be the coeﬃcients for daughter j of sire i. Here ui might be a 3 × 1 vec-
tor of “sire eﬀects”, common to all progeny of sire i (i = 1, 2, . . . , 20) and

15.2 Hierarchical or Multistage Models
635
the second-stage residual vector would be eij, representing the discrepancy
θij −Xijβ−ui peculiar to cow ij. Parameters of the distributions of ui and
eij are identiﬁable, as is the case in a standard analysis of variance, even
if the θij are not observable.
The second-stage distributional assumptions pertain to the uncertainty
induced by the presence of ei in model (15.6), given β and ui. It is often
convenient to postulate that
θi|β, ui, Σe ∼N (Xiβ + ui, Σe) ,
(15.7)
implying that
ei|Σe ∼N (0, Σe) ,
where the second stage variance–covariance matrix has the form
Σe =


σ2
1
σ12
· · ·
σ1r
σ21
σ2
2
· · ·
σ2r
...
...
...
...
σr1
σr2
· · ·
σ2
r

.
(15.8)
Here the diagonal elements are the variances of the second-stage residuals
and the oﬀ-diagonals are corresponding covariances; for example, σ2
r is the
second-stage variance of parameter r and σr−1,r is the second-stage covari-
ance between parameters r −1 and r. In some instances, one may wish to
assign a thick-tailed or robust distribution to the residuals, e.g., an r-variate
t distribution. In this situation, one would write ei|νe, Σe ∼tr (0, Σe, νe) to
denote a t distribution of dimension r, having a null mean vector, variance–
covariance Σe, and degrees of freedom νe. It must be noted that in a
multivariate-t distribution, Σe =
νe
νe−2Se, where Se is a scale matrix, so
νe > 2 is a necessary condition for the existence of the variance–covariance
matrix (Zellner, 1971). An implementation based on the t distribution will
be discussed later.
Often, it is assumed that second-stage residuals are mutually indepen-
dent across individuals. Then the joint density of all parameters at the
second stage can be expressed as
p (θ1, θ2, . . . , θM|β, u1, u2, . . . , uM, Σe) =
M
-
i=1
p (θi|β, ui, Σe) .
(15.9)

636
15. Bayesian Analysis of Longitudinal Data
Put θ =

θ′
1, θ′
2, . . . , θ′
M
′ and u = [u′
1, u′
2, . . . , u′
M]′ . Under the normality
assumption made in (15.7), the preceding takes the form
p (θ|β, u, Σe) ∝|Σe|−M
2 exp

−1
2
M

i=1
e′
iΣ−1
e ei

∝|Σe|−M
2 exp

−1
2 tr
M

i=1
e′
iΣ−1
e ei

∝|Σe|−M
2 exp

−1
2 tr

Σ−1
e B

,
(15.10)
where ei = θi −Xiβ −ui, as before, and B = M
i=1 eie′
i is an r × r matrix.
The diagonal elements of this matrix contain the sum of squared deviations
of the appropriate parameter from their second-stage conditional expecta-
tions; the oﬀ-diagonals are sums of products of such parameter deviations.
It must be noted that in many models, motivated by mechanistic consider-
ations about growth and lactation, the values of the trajectory parameters
must be deﬁned within a restricted range. For example, it is clear that adult
body weight or total milk yield produced cannot be negative. Such restric-
tions are easy to incorporate in the model via an appropriate deﬁnition of
the parameter space. This issue will be discussed further later.
It is useful to note that if all parameters are concatenated vertically, the
second stage structure in (15.6) can be represented as


θ1
θ1
...
θM

=


X1
X2
...
XM

β +


u1
u2
...
uM

+


e1
e2
...
eM


or, more compactly, as
θMr×1 = XMr×pβp×1 + uMr×1 + eMr×1.
This indicates that the second-stage distribution of all parameters of all
individuals is
θ|β, u, Σe ∼N (Xβ + u, I ⊗Σe) .
(15.11)
An alternative formulation can be obtained by arranging individuals within
parameters; here X must be redeﬁned accordingly, and the covariance ma-
trix of the process would then be Σe ⊗I. The choice between the two
alternative orders is entirely a matter of computational convenience.
Example 15.4
Two-stage model for a quadratic trajectory
Suppose that feed intake measurements are taken serially in each of a num-
ber of descendants of a random sample of boars to be evaluated in a progeny

15.2 Hierarchical or Multistage Models
637
test. Consider the second-order quadratic model of Example 15.1, and as-
sume it provides a reasonable description of the trajectory of feed intake
over time. Let yijk be measurement k taken in oﬀspring j of boar i. The
ﬁrst-stage model can be written as
yijk = aij + bijtijk + cijt2
ijk + εijk,
i = 1, 2, . . . , M, j = 1, 2, . . . , oi, k = 1, 2, . . . , nijk,
yijk|aij, bij, cij, σ2
ε ∼NIID(aij + bijtijk + cijt2
ijk, σ2
ε),
where aij, bij, and cij are coeﬃcients peculiar to oﬀspring j of boar i. As
usual, εijk is a discrepancy from the expected trajectory of individual ij
when measured at time k. If the sample of boars is homogeneous in every
possible respect, it might be sensible to ﬁt the second stage model


aij
bij
cij

=


1
0
0
0
1
0
0
0
1




a0
b0
c0

+


ai
bi
ci

+


eaij
ebij
ecij

,
where a0, b0, and c0 are regression parameters common to all observations;
ai, bi, and ci are deviations common to all progeny of boar i, and the e′s are
the second-stage residuals, peculiar to oﬀspring j of boar i. Here, Xij = I3,
∀i, j,
ui =


ai
bi
ci

,
and
Σe =


σ2
ea
σeab
σeac
σeab
σ2
eb
σebc
σeac
σebc
σ2
ec


is the variance–covariance matrix between progeny-speciﬁc regression pa-
rameters, conditionally on ai, bi, and ci.
It is instructive to write the ﬁrst-stage model in terms of the second-stage
structure, to obtain
yijk =

a0 + b0tijk + c0t2
ijk

+

ai + bitijk + cit2
ijk

+

eaij + ebijtijk + ecijt2
ijk

+ εijk.
The function in parentheses, (·) , can be construed as a “population re-
gression”, common to all individuals measured; the second function, [·] ,
is a deviation from the population regression shared by all descendants of
boar i; and the function {·} is a deviation speciﬁc to oﬀspring j of boar i.
Conditionally on the parameters of the second-stage model, and assuming

638
15. Bayesian Analysis of Longitudinal Data
normality throughout, it follows that
yijk|a0, b0, c0, ai, bi, ci, tijk, v (tijk) ∼
N

(a0 + ai) + (b0 + bi) tijk + (c0 + ci) t2
ijk, v (tijk)

,
where
v (tijk) =
 1
tijk
t2
ijk



σ2
ea
σeab
σeac
σeab
σ2
eb
σebc
σeac
σebc
σ2
ec




1
tijk
t2
ijk

+ σ2
ε
is a “variance function”. Likewise, and conditionally on the second-stage
parameters, the covariance between observations taken at times k and k′
on individual ij is
Cov (yijk, yijk′|second-stage parameters) = t′
k


σ2
ea
σeab
σeac
σeab
σ2
eb
σebc
σeac
σebc
σ2
ec

tk′,
where t′
k =
 1
tijk
t2
ijk

and t′
k′ =
 1
tijk′
t2
ijk′

. The corre-
sponding “correlation function” is then
Corr (yijk, yijk′|second stage parameters) =
t′
kΣetk′
>
(t′
kΣetk) (t′
k′Σetk′).
Suppose, further, that the boar-speciﬁc parameters are assigned the distri-
bution


ai
bi
ci


""""""
Σs ∼N




0
0
0

, Σs

,
where
Σs =


σ2
sa
σsab
σsac
σsab
σ2
sb
σsbc
σsac
σsbc
σ2
sc


is the covariance matrix between boar-speciﬁc deviations from the overall
population regression. Then, the additional assumption that boar-speciﬁc
and progeny-speciﬁc deviations are independent yields
yijk|a0, b0, c0, tijk, Σe, Σs ∼N

a0 + b0tijk + c0t2
ijk, t′
k (Σs + Σe) tk

.
The distribution given above has a mean and variance describing how the
trait and its variability change in time, averaged over all individuals in the
population.
■
A special case is when the ﬁrst and second stages of the model are both
linear in the parameters. If the ﬁrst stage is linear, (15.1) can be written
as
yi = Tiθi + εi,
i = 1, 2, ..., M,
(15.12)

15.2 Hierarchical or Multistage Models
639
for some known matrix Ti. Employing (15.6) in (15.12) gives the represen-
tation
yi = Ti (Xiβ + ui + ei) + εi
= (TiXi) β + Tiui + Tiei + εi
= X∗
i β + Tiui + Tiei + εi,
(15.13)
where X∗
i = TiXi. Under the usual normality assumptions, this induces
the conditional distributions
yi|β, ui, ei, γ ∼N [X∗
i β + Tiui + Tiei, Ri (γ)] ,
(15.14)
and
yi|β, ui, Σe, γ ∼N [X∗
i β + Tiui, TiΣeT′
i + Ri (γ)] .
(15.15)
It follows from (15.13)-(15.15) that a two-stage linear hierarchy for lon-
gitudinal data is a special case of the general mixed eﬀects linear model.
Further deconditioning can be obtained by introducing additional tiers in
the hierarchical structure.
The form of (15.12) implies that in a linear hierarchical model, once a
functional form is adopted for the ﬁrst stage, then all second-stage elements
contribute to the overall model in a similar form; for example, the gradient
of the observations with respect to either Xiβ, ui, or ei is always T′
i. It is
possible to give a Bayesian implementation in a more general and ﬂexible
manner, but this is notationally awkward (specially if a nonlinear speciﬁ-
cation is chosen for the ﬁrst stage). Hence, the hierarchical representation
is kept, with the understanding that all subsequent developments apply to
most models, at least conceptually.
15.2.3
Third Stage
In a Bayesian model, as pointed out in the chapters on Bayesian inference,
prior distributions must be assigned to all unknown quantities in the sta-
tistical system posited. Thus, priors must be adopted for β, u, Σe, and
γ.
Let the vector u represent additive genetic eﬀects on the trajectory pa-
rameters. In this case, a classical (and convenient) assumption made in
quantitative genetics is that
u|G0 ∼N (0, A ⊗G0) ,
(15.16)
where it is implied that parameters are ordered within individuals. Above,
A is the additive genetic relationship matrix between the M individuals,
and G0 is an r × r additive genetic variance–covariance matrix between

640
15. Bayesian Analysis of Longitudinal Data
parameters, that is
G0 =


σ2
u1
σu12
. . .
σu1r
σu21
σ2
u2
. . .
σu2r
...
...
...
...
σur1
σur2
. . .
σ2
ur

,
with the understanding that σuij = σuji. If G0 is unknown, a prior dis-
tribution must be elicited for this matrix as well. For a linear model as in
(15.15) the preceding prior implies that, given β, Σe, G0, and γ, the prior
predictive distribution is
yi|β, Σe, G0, γ ∼N [X∗
i β, Ti (G0 + Σe) T′
i + Ri (γ)] .
It will be assumed further that
β|α, Γ ∼N (α, Γ)
(15.17)
where α, and Γ are known hyperparameters. The joint prior density of all
unknowns can be taken to be equal to
p (β, u, G0, Σe, γ|α, Γ) = p (β|α, Γ) p (u|G0) p (G0) p (Σe) p (γ) . (15.18)
The preceding implies, a priori, that β, Σe, and γ are mutually indepen-
dent of each other and of u and G0, with the only dependence assumed a
priori being that of the distribution of u on G0. As seen before, after data
are combined with the prior via formal use of Bayes theorem, parameters
become interdependent, even if the data set contains just a few observa-
tions. Vague priors can be adopted for the dispersion components, with the
corresponding densities being
p (G0) ∝constant,
|G0| > 0,
(15.19)
p (Σe) ∝constant,
|Σe| > 0,
(15.20)
and
p (γ) ∝constant,
γ ∈ℜγ,
(15.21)
where ℜγ is the allowable parameter space of the dispersion vector γ. For
example, if this vector contains a single residual variance component, its
parameter space would be the positive part of the real line, R+.
The preceding prior distributions may be bounded, based on either prior
knowledge of parameter values or on theoretical considerations. It must be
emphasized that an advantage of the Bayesian approach resides in the pos-
sibility of incorporating external stochastic information into the analysis.
If such information exists, and if one wishes to use it, the prior densities
should be modiﬁed accordingly. For instance, one may adopt informative

15.2 Hierarchical or Multistage Models
641
priors for the dispersion parameters, for example, inverted Wishart distri-
butions for the covariance matrices and scaled inverted chi-square distribu-
tions for variance components. If the priors are conjugate, a Markov chain
Monte Carlo implementation does not become much more diﬃcult than
one based on bounded uniform priors.
It is often convenient, especially in the case of nonlinear models, to aug-
ment the prior distribution with the trajectory parameters θ, leading to
the joint prior
p (θ, β, u, G0, Σe, γ|α, Γ) = p (θ|β, u, G0, Σe, γ, α, Γ)
p (β, u, G0, Σe, γ|α, Γ) .
(15.22)
Now, because the parameters of diﬀerent individuals, conditionally on β
and u, are independently distributed, with joint distribution as in (15.9),
use of this and of the prior densities (15.18)–(15.21) in (15.22) leads to the
following form for the augmented joint prior density
p (θ, β, u, G0, Σe, γ|α, Γ) ∝
M
-
i=1
p (θi|β, ui, Σe) p (β|α, Γ) p (u|G0) .
(15.23)
The joint posterior distribution has support for any value of β in Rp and for
any value of ui in Rr. However, as noted earlier, if trajectory parameters
take values only within a restricted range, it may not always be reasonable
assigning an r-variate normal distribution as prior for the θi coeﬃcients.
Nevertheless, the normality assumption may hold well for a transformation
of such parameters. For example, if a hypothetical curve for somatic cell
count in milk (a measure of udder health in dairy cattle) has four param-
eters representing, for example, initial conditions, growth rate, change in
growth rate, and level near the end of lactation, it may be sensible to adopt
a parameterization in a log-scale, with the normal prior assigned to the en-
suing parameterization. At any rate, this seldom causes serious problems,
provided that the longitudinal series is “long enough” for most individuals
(so the prior has a mild eﬀect on inferences), and that the genetic rela-
tionship structure is suﬃciently dense, so that information between related
individuals can be exchanged. Alternative parameterizations do not com-
plicate the Bayesian analysis conceptually, but can make implementation
more involved.
15.2.4
Joint Posterior Distribution
From Bayes theorem, the joint posterior density of all unknowns is formed
by combining the density of the sampling model in (15.5) with the joint

642
15. Bayesian Analysis of Longitudinal Data
prior (15.23), yielding
p (θ, β, u, G0, Σe, γ|y1, y2, . . . , yM, α, Γ)
∝
# M
-
i=1
|Ri (γ)|−1
2 exp

−1
2ε′
iR−1
i
(γ) εi

p (θi|β, ui, Σe)
$
×p (β|α, Γ) p (u|G0) .
(15.24)
As a side issue, it is instructive to consider the density of the distribution
of the observations unconditionally on β and u, and this can be written
explicitly when the trajectory is linear in the parameters. From (15.15), it
follows that the resulting prior predictive distribution is:
yi|Σe, γ, G0, α, Γ ∼N [X∗
i α, X∗
i ΓX∗′
i + Ti (G0 + Σe) T′
i + Ri (γ)] .
(15.25)
This distribution, given Σe, γ, G0, α, and Γ can be interpreted as the
probability distribution of the data before observations are collected.
15.3
Two-Step Approximate Bayesian Analysis
Bayesian inference always relies on applying the probability calculus to
some target distribution, this being the joint posterior of all unknowns
in (15.24). Probability statements are obtained from appropriate sets of
marginal, joint, or conditional distributions, depending in the type of in-
ference sought. However, in the case of the probability model with density
given by (15.24), it is impossible to arrive at the marginal distributions of
interest because the required integrals cannot be evaluated in closed form;
furthermore, numerical quadrature seldom works well beyond a few dimen-
sions. An alternative consists in extracting samples from the joint poste-
rior, with the appropriate coordinate of the sample being a draw from the
corresponding marginal distribution. From such samples, features of the
posterior distribution of interest (e.g., mean, median, variance, quantiles
or densities) can be estimated. A MCMC analysis can be used for this
purpose, and this will be discussed later. Often, however, an approximate,
simpler, analysis can lead to satisfactory inferences, at least for some fea-
tures of the posterior distribution. For example, in animal breeding it is
customary to carry out the following two-step analysis for mixed eﬀects
linear models: ﬁrst, estimate dispersion parameters by some method, such
as REML. Second, conditionally on the estimates of dispersion parameters,
ﬁnd point estimates and (sometimes) a measure of uncertainty for β and
for ui (i = 1, 2, . . . , M). A similar two-step analysis is described for the
hierarchical model under discussion.

15.3 Two-Step Approximate Bayesian Analysis
643
15.3.1
Estimating β, u, and e when Variances are Known
Suppose there is no uncertainty about the dispersion parameters, that is,
G0, Σe, and γ are known without error. Recall from (15.3) that the ﬁrst-
stage residual vector can be expressed as
ε = y −f (θ, t) = y −f [Xβ + u + e, t] ,
so that, for individual i,
εi = yi −fi (θi, ti) = yi −fi (Xiβ + ui + ei, ti) .
The joint posterior density of β, u, and e, given G0, Σe, and γ, can be
written (without making use of augmentation with the θ parameters) as
p (β, u, e|G0, Σe, γ, y, α, Γ) ∝
M
-
i=1
exp

−1
2ε′
iR−1
i
(γ) εi

p (β|α, Γ) p (u|G0) p (e|Σe)
∝exp
#
−1
2
 M

i=1
ε′
iR−1
i
(γ) εi + (β −α)′ Γ−1 (β −α)
$
× exp
#
−1
2

u′ 
A−1⊗G−1
0

u +
M

i=1
e′
iΣ−1
e ei
$
.
(15.26)
An approximate Bayesian analysis, conditionally on the dispersion com-
ponents, consists of approximating the joint posterior distribution with
density in (15.26) by a Gaussian process having mean vector equal to the
joint mode, and a variance–covariance matrix given by the inverse of the
corresponding negative Hessian matrix. If the ﬁrst-stage model is linear,
the approximation is exact, as veriﬁed subsequently. In general, the modal
vector needs to be calculated with an iterative algorithm (converging in one
step for a linear trajectory). Considering that second derivatives are needed
for completing inferences, the Newton–Raphson or scoring algorithms are
natural candidates here.
Let l (β, u) be the logarithm of the joint posterior density (15.26). Apart
from an additive constant
l (β, u, e) = −1
2
 M

i=1
ε′
iR−1
i
(γ) εi + (β −α)′ Γ−1 (β −α)

−1
2

u′ 
A−1 ⊗G−1
0

u +
M

i=1
e′
iΣ−1
e ei

= −1
2

ε′R−1 (γ) ε + (β −α)′ Γ−1 (β −α)

−1
2

u′ 
A−1 ⊗G−1
0

u + e′ 
I ⊗Σ−1
e

e

,
(15.27)

644
15. Bayesian Analysis of Longitudinal Data
with ε′ = [ε′
1, ε′
2, . . . , ε′
M], R−1 (γ) = R−1
1
(γ) ⊕R−1
2
(γ) ⊕· · · ⊕R−1
M (γ),
and ⊕is the direct-sum operator, denoting that R is a block-diagonal
matrix with individual blocks being Ri (γ) .
First Derivatives
Expressions for the ﬁrst and second derivatives are facilitated by employing
the chain rule of calculus. Observing matrix comformability, one can write
∂ε′R−1 (γ) ε
∂β

p×1
=
∂θ′
∂β
 ∂f ′ (θ, t)
∂θ
 ∂ε′R−1 (γ) ε
∂f (θ, t)

.
(15.28)
Similarly,
∂ε′R−1 (γ) ε
∂u

q×1
=
∂θ′
∂u
 ∂f ′ (θ, t)
∂θ
 ∂ε′R−1 (γ) ε
∂f (θ, t)

,
(15.29)
and
∂ε′R−1 (γ) ε
∂e

rM×1
=
∂θ′
∂e
 ∂f ′ (θ, t)
∂θ
 ∂ε′R−1 (γ) ε
∂f (θ, t)

.
(15.30)
In (15.29) note that if one includes in u the additive genetic values of
related individuals that lack longitudinal information, then q > rM . This
augmentation is a routine practice in animal breeding, since it permits
inferring the genetic worth of candidates for selection indirectly, via the
additive genetic relationship matrix A. Put
q = rM + rM = r

M + M

,
where M is the number of individuals without measurements. It is conve-
nient to write the second-stage model as
θ = Xβ + Zu + e,
where
ZrM×r(M+M) =

IrM×rM, 0rM×rM

.
The order of the vectors is dropped in the notation hereinafter. Now
∂θ′
∂β = ∂(Xβ + Zu + e)′
∂β
= X′,
(15.31)
∂θ′
∂u = ∂(Xβ + Zu + e)′
∂u
=
 I
0

,
(15.32)
∂θ′
∂e = ∂(Xβ + Zu + e)′
∂e
= I,
(15.33)
∂ε′R−1 (γ) ε
∂f (θ, t)

= −2R−1 (γ) [y −f (θ, t)] ,
(15.34)

15.3 Two-Step Approximate Bayesian Analysis
645
and deﬁne
H′ (θ) =
∂f ′ (θ, t)
∂θ

=
∂[f ′
1 (θ1, t) f ′
2 (θ2, t) . . . f ′
M (θM, t)]
∂θ

=


H′
1 (θ1)
0
· · ·
0
0
H′
2 (θ2)
· · ·
0
...
...
...
...
0
0
· · ·
H′
M (θM)

,
(15.35)
where θ = [θ′
1, θ′
2, . . . , θM]′. Above
H′
´ı (θi) = ∂f ′
i (θi, t)
∂θi
is an r×ni matrix, being independent of θi only when the ﬁrst-stage model
is linear in the parameters. Hence
H′ (θ) = H′
1 (θ) ⊕H′
2 (θ) ⊕· · · ⊕H′
M (θ) .
Applying (15.31)–(15.35) in (15.28), (15.29) and (15.30):
∂ε′R−1 (γ) ε
∂β

= −2X′H′ (θ) R−1 (γ) [y −f (θ, t)] ,
∂ε′R−1 (γ) ε
∂u

= −2

I
0

H′ (θ) R−1 (γ) [y −f (θ, t)]
= −2

H′ (θ) R−1 (γ) [y −f (θ, t)]
0

,
and
∂ε′R−1 (γ) ε
∂e

= −2H′ (θ) R−1 (γ) [y −f (θ, t)] .
The gradient vector of the logarithm of the conditional posterior density of
β, u, and e in (15.27) is, using the preceding expressions,
∂L (β, u, e)
∂β
= X′H′ (θ) R−1 (γ) ε −Γ−1 (β −α) ,
(15.36)
∂L (β, u, e)
∂u
=
 H′ (θ) R−1 (γ) ε
0

−

A−1 ⊗G−1
0

u,
(15.37)
∂L (β, u, e)
∂e
= H′ (θ) R−1 (γ) ε −

I ⊗Σ−1
e

e,
(15.38)
with ε = y −f (θ, t) .

646
15. Bayesian Analysis of Longitudinal Data
Second Derivatives
The Newton–Raphson or scoring algorithms, and the Gaussian approxi-
mation to the posterior distribution, require second derivatives. Hence, an
additional diﬀerentiation is needed. For simplicity of notation let, here-
inafter,
R−1 (γ) = R−1.
Now, note that
∂2L (β, u, e)
∂β ∂β′
= ∂

X′H′ (θ) R−1ε −Γ−1 (β −α)

∂β′
= ∂

X′H′ (θ) R−1ε

∂β′
−Γ−1
= X′
∂H′ (θ)
∂β′

R−1ε −X′H′ (θ) R−1 ∂f (θ, t)
∂β′
−Γ−1.
The expression [∂H′ (θ) /∂β′] is an informal representation, as the deriva-
tives of a matrix with respect to a vector require a “three-dimensional
matrix” (technically, one needs to arrange elements of H′ (θ) into a vector,
and then take derivatives). Ignoring this, note that if expectations of the
second derivatives are taken with respect to the ﬁrst-stage distribution,
given in (15.4), one has
Ey|θ [ε = y −f (θ, t)] = 0.
Hence, considerable simpliﬁcation is obtained, the result being
Ey|θ
∂2L (β, u, e)
∂β ∂β′

= −

X′H′ (θ) R−1 ∂f (θ, t)
∂β′
+ Γ−1

= −

X′H′ (θ) R−1 ∂f (θ, t)
∂θ′
∂θ
∂β′ + Γ−1

= −

X′H′ (θ) R−1H (θ) X + Γ−1
.
(15.39)
Likewise,
Ey|θ
∂2L (β, u, e)
∂u ∂u′

= −

Z′H′ (θ) R−1H (θ) Z + A−1 ⊗G−1
0

= −

I
0
 
H′ (θ) R−1H (θ)
 
I
0

+ A−1 ⊗G−1
0
%
.
(15.40)

15.3 Two-Step Approximate Bayesian Analysis
647
Now the additive genetic relationship matrix A can be partitioned in a
form consistent with that of the vector u, such that
A−1 ⊗G−1
0
=

AMM
AMM
AMM
AMM
−1
⊗G−1
0
=

AMM ⊗G−1
0
AMM ⊗G−1
0
AMM ⊗G−1
0
AMM ⊗G−1
0

=

GMM
GMM
GMM
GMM

.
Using this in (15.40)
Ey|θ
∂2L (β, u, e)
∂u∂u′

= −

H′ (θ) R−1H (θ) + GMM
GMM
GMM
GMM

.
(15.41)
Further,
Ey|θ
∂2L (β, u, e)
∂e∂e′

= −

H′ (θ) R−1H (θ) + I ⊗Σ−1
e

.
(15.42)
Using similar algebra, the second-order mixed partial derivatives are
Ey|θ
∂2L (β, u, e)
∂β∂u′

= −X′H′ (θ) R−1H (θ) Z
= −

X′H′ (θ) R−1H (θ)
0

,
(15.43)
Ey|θ
∂2L (β, u, e)
∂β∂e′

= −

X′H′ (θ) R−1H (θ)

,
(15.44)
and
Ey|θ
∂2L (β, u, e)
∂u∂e′

= −

Z′H′ (θ) R−1H (θ)

= −

I
0

H′ (θ) R−1H (θ)
= −

H′ (θ) R−1H (θ)
0

.
(15.45)
Gaussian Approximation to the Conditional Posterior Distribution
Using results given in the chapter on approximate methods for inference,
the conditional posterior distribution
[β, u, e|G0, Σe, γ, y, α, Γ]

648
15. Bayesian Analysis of Longitudinal Data
can be approximated as
β, u, e|G0, Σe, γ, y, α, Γ ∼N(c, C−1
c ),
(15.46)
where
c =Arg max
β,u,ep (β, u, e|G0, Σe, γ, y, α, Γ)
(15.47)
is the modal vector, and Cc is the negative of the matrix of second deriva-
tives of l (β, u, e) with respect to β, u, e, evaluated at the modal value c.
Collecting the submatrices of second derivatives in (15.39), and (15.41)–
(15.45), the symmetric negative Hessian matrix can be expressed as
C =


X′WX + Γ−1
X′W
0
X′W
.
W + GMM
GMM
W
.
.
GMM
0
.
.
.
W + I ⊗Σ−1
e

,
(15.48)
where
W = W (θ, γ) = H′ (θ) R−1H (θ) ,
noting that this is a symmetric matrix and with order rM × rM.
The Scoring Algorithm
Computation of the modal vector c using the scoring algorithm proceeds
with the iteration
c[t+1] = c[t] + C−1
[t] g[t],
(15.49)
where [t] denotes the iterate number and g[t] is the gradient vector at
iteration [t] . An alternative representation is
C[t]c[t+1] = C[t]c[t] + g[t].
(15.50)
Making use of (15.36)–(15.38), and putting ε[t] = y −f

θ[t], t

,
g[t] =


X′H′ 
θ[t]
R−1ε[t] −Γ−1 
β[t] −α

H′ 
θ[t]
R−1ε[t] −GMMu[t]
M −GMMu[t]
M
−GMMu[t]
M −GMMu[t]
M
H′ 
θ[t]
R−1ε[t] −

I ⊗Σ−1
e

e[t]


.
Note that
C[t]c[t] =


X′W[t] 
Xβ[t] + u[t]
M + e[t]
+ Γ−1β[t]
W[t] 
Xβ[t] + u[t]
M + e[t]
+ GMMu[t]
M + GMMu[t]
M
GMMu[t]
M + GMMu[t]
M
W[t] 
Xβ[t] + u[t]
M + e[t]
+

I ⊗Σ−1
e

e[t]


.

15.3 Two-Step Approximate Bayesian Analysis
649
Since
θ[t] = Xβ[t] + u[t]
M + e[t],
adding the two preceding vectors gives
C[t]c[t] + g[t] =


X′W[t]θ[t] + X′H′ 
θ[t]
R−1ε[t] + Γ−1α
W[t]θ[t] + H′ 
θ[t]
R−1ε[t]
0
W[t]θ[t] + H′ 
θ[t]
R−1ε[t]


.
(15.51)
In (15.51), observe that
W[t]θ
[t] + H′ 
θ[t]
R−1ε[t]
= H′ 
θ[t]
R−1H

θ[t]
θ[t] + H′ 
θ[t]
R−1ε[t]
= H′ 
θ[t]
R−1 9
H

θ[t]
θ[t] + ε[t]:
= H′ 
θ[t]
R−15y[t],
where
5y[t] = H

θ[t]
θ[t] + ε[t]
(15.52)
is a “pseudo-data” vector that changes values iteratively. Collecting (15.48),
(15.51), and (15.52) to form the scoring algorithm, the iteration can be
represented as


β[t+1]
u[t+1]
M
u[t+1]
M
e[t+1]

=

C[t]−1


X′d[t] + Γ−1α
H′d[t]
0
d[t]

,
(15.53)
where
d[t] = H′ 
θ[t]
R−15y[t].
This is in the form of an iteratively reweighted system of linear mixed
model equations for a Gaussian process. The coeﬃcient matrix and the
vector of the right-hand sides change from iterate to iterate. If the scoring
algorithm converges to a global maximum (this being extremely diﬃcult or
impossible to check), we denote the maximum a posteriori estimates of β,
u, and e as 5β (G0, Σe, γ) , 5u (G0, Σe, γ) , and 5e (G0, Σe, γ) , respectively,
to emphasize the dependence on the dispersion parameters.

650
15. Bayesian Analysis of Longitudinal Data
15.3.2
Estimating G0, Σe and γ from Their
Marginal Posterior Distribution
In the preceding, methods for inferring unknowns from the conditional
posterior distribution
[β, u, e|G0, Σe, γ, y, α, Γ]
were outlined, using Gaussian approximation (15.46). This requires knowl-
edge of the nuisance parameters G0, Σe and γ, a situation seldom encoun-
tered in practice. In approximate Bayesian analysis (e.g., Box and Tiao,
1973; Gianola and Fernando, 1986), the usual approach consists of ﬁnding
the maximizers of the marginal posterior distribution [G0, Σe, γ|y, α, Γ] of
the dispersion components, say
5G0, 5Σe, 5γ = Arg
max
G0,Σe,γ p (G0, Σe, γ|y, α, Γ) .
(15.54)
If uniform priors are adopted for G0, Σe, and γ, as in the developments pre-
sented here, 5G0, 5Σe and 5γ are usually known as “marginal ML estimates”.
Subsequently, these estimates can be used to obtain inferences based on
the distribution

β, u, e|G0 = 5G0, Σe = 5Σe, γ = 5γ, y, α, Γ

.
It must be emphasized that such an analysis does not take into account
the error of estimation of the nuisance parameters. The main diﬃculty, spe-
cially for a nonlinear ﬁrst-stage model, is that p (G0, Σe, γ|y, α, Γ) cannot
be arrived at in closed form, and the marginal ML estimates cannot be writ-
ten explicitly. However, these estimates can be approximated as described
below.
Consider again the ﬁrst-stage model in (15.3):
y = f (θ, t) + ε
and expand its structure f (θ, t) about provisional estimates
5β
=
5β (G0, Σe, γ) ,
5u
=
5u (G0, Σe, γ) ,
and
5e = 5e (G0, Σe, γ) ,
obtained as in the preceding section for some sensible starting values of the
dispersion parameters. Putting η =

β′, u′, e′′, a ﬁrst order Taylor series
approximation about 5η yields
y ≈f

X5β + Z5u + 5e, t

+
∂f (θ, t)
∂θ′
  ∂θ
∂η′
%
η=η
(η −5η) + ε

15.3 Two-Step Approximate Bayesian Analysis
651
= f

X5β + Z5u + 5e, t

+ H

5θ
 
X
Z
I



β −5β
u −5u
e −5e

+ ε
= f

5θ, t

−H

5θ
 
X5β + Z5u + 5e

+ H

5θ

(Xβ + Zu + e) + ε. (15.55)
In the preceding, now let
H

5θ

X
=
5X,
H

5θ

Z
=
5Z,
H

5θ

=
5H,
and
5ε = y −f

5θ, t

.
Recalling that the pseudo-data vector is
5y = 5X5β + 5Z5u + 5H5e + 5ε,
then, (15.55) can be rearranged as
5y ≈5Xβ + 5Zu + 5He + ε,
(15.56)
so the expansion leads to a linear “pseudo-model” for “pseudo-data” from
which an update for the dispersion parameters G0, Σe, and γ can be ob-
tained by some standard method suitable for linear models, such as those
based on maximizing likelihoods. With this update, a new point estimate
of θ can be computed with the scoring algorithm (15.53).
Under the linear pseudo-model, one can adopt the Bayesian hierarchy
5y|β, u, 5X, 5Z, γ ≈N

5Xβ + 5Zu + 5He, R (γ)

(15.57)
with priors as in (15.16)–(15.21). If ﬂat, improper priors are adopted for
β and for the dispersion parameters, any available algorithm for REML
estimation of dispersion parameters can be used to obtain the revised val-
ues for G0, Σe, and γ. This is because with such ﬂat priors, the REML
estimates are the modal values of the distribution [G0, Σe, γ|5y, α, Γ] in a
Gaussian linear model (Harville, 1974). Such algorithms do not consider
the situation where it is assumed that β has the informative distribution
in (15.17), so this would work only if p(β) is taken to be proportional
to a constant. In this case, one proceeds to iterate between the REML
algorithm applied to 5y, to obtain new values of G0, Σe, γ, the scoring
algorithm (15.53) to obtain new values 5β (G0, Σe, γ) , 5u (G0, Σe, γ) , and
5e (G0, Σe, γ) , and the Taylor series expansion leading to the linear pseudo-
model for 5y as in (15.56) and (15.57). The cycle is repeated until G0, Σe, γ

652
15. Bayesian Analysis of Longitudinal Data
stabilize, these being the modal values sought. Finally, inferences about β,
u, e are obtained from the Gaussian approximation (15.46). In particular,
since
θ = Xβ + Zu + e =

X
Z
I



β
u
e

= K


β
u
e

,
so θ is a linear combination of β, u, e, then approximately,
θ|G0, Σe, γ, y, α, Γ ∼N(Kc, KC−1
c K′),
(15.58)
with the understanding that if an REML algorithm is employed, then there
is the implicit assumption that Γ−1 →0, to make the prior distribution of
β ﬂat (and improper).
An alternative approach to locating the modal values of the dispersion
parameters of G0, Σe, γ would be using a Laplacian approximation, but
this is not discussed here.
15.3.3
Special Case: Linear First Stage
If the trajectory is linear in the parameters, as in (15.12), the entire vector
of observations can be written as
y = Tθ + ε
(15.59)
for T = T1 ⊕T2 ⊕· · · ⊕TM, where Ti is a known matrix, and
θ = Xβ + Zu + e,
as before. For this linear model, the matrix of derivatives of the conditional
expectation vector with respect to θ is
H′ (θ) = ∂

θ′T′
∂θ
= T′
which is independent of the parameters. Also, note that the pseudo-data
vector 5y becomes
5y[t]
=
H

θ[t]
θ[t] +

y −f

θ[t], t

=
Tθ[t] +

y −Tθ[t]
= y,
so it is identical to the vector of observations y. In this situation, as seen
in Chapter 6, the posterior distribution
[β, u, e|G0, Σe, γ, y, α, Γ]

15.4 Computation via Markov Chain Monte Carlo
653
is exactly Gaussian, with mean vector m = M−1r, and posterior variance–
covariance matrix M−1, where
M =


X′QX + Γ−1
X′Q
0
X′Q
.
Q + GMM
GMM
Q
.
.
GMM
0
.
.
.
Q + I ⊗Σ−1
e

,
(15.60)
for Q = T′R−1T, and
r =


X′T′R−1y + Γ−1α
T′R−1y
0
T′R−1y

.
(15.61)
If a diﬀuse prior for β is adopted, such that Γ−1 →0, the β component
of the mean vector tends toward the best linear unbiased estimator of
β, whereas the u and e components tend to the BLUP of u and of e,
respectively. Further, in the approximate ﬁrst-stage distribution in (15.57),
one has that
5Xβ + 5Zu + 5He = TXβ + TZu + Te = Tθ.
Thus:
5y|β, u, e, 5X, 5Z, γ ≈N

5Xβ + 5Zu + 5He, R

≡y|β, u, e, X, Z, T, γ ∼N [Tθ, R]
(15.62)
which is exactly the ﬁrst-stage distribution. Therefore, as Γ−1 →0, the
mode of the posterior distribution [G0, Σe, γ|y, α, Γ] tends to the REML
estimates, provided the prior for u is as in (15.16).
15.4
Computation via Markov Chain Monte Carlo
The approximate Bayesian method described in the preceding section is,
undoubtedly, computationally involved, specially for nonlinear trajectories.
However, it is statistically and algorithmically equivalent to what might
be termed a “quasi-BLUP coupled with REML” analysis of a nonlinear
mixed eﬀects model. Actually, the latter represents the state of the art from
a likelihood-frequentist perspective (e.g., Wolﬁnger, 1993; Wolﬁnger and
Lin, 1997), and relies on asymptotic arguments as well. The approximate
Bayesian approach, however, has the additional ﬂexibility conferred by the
possibility of introducing prior information, as noted earlier. It cannot be
overemphasized, however, that in the approximate analysis:

654
15. Bayesian Analysis of Longitudinal Data
(1) the estimates of the dispersion parameters G0, Σe, γ are from a joint
mode;
(2) inferences obtained for β, u, e or θ are conditional on such modal
values, and
(3) an asymptotic approximation to a conditional posterior distribution, as
in (15.46), is used.
An alternative, and probably simpler from a computational point of view,
is to carry out a fully Bayesian analysis by sampling from marginal poste-
rior distributions of interest. This must be contrasted at the onset with the
conditioning arguments employed in the approximate method. These reveal
that uncertainty about nuisance parameters that one should integrate out
is not taken into account. Although procedures for sampling from the pos-
terior probably require more computer time, these are easier to program.
In addition, an entire distribution can be estimated, as opposed to just its
location and (perhaps) dispersion.
An implementation, based possibly on a combination of several sampling
techniques, is discussed in this section since there is seldom a unique, opti-
mal approach for drawing samples from posterior distributions. The start-
ing point is to consider constructing a Gibbs sampler, that is, attempting
to form (and identify) all possible fully conditional posterior distributions,
and then looping through all such conditionals by iterative updating of the
conditioning unknowns. As noted in Chapter 11, a complete pass or scan
through all fully conditional distributions deﬁnes an iteration of the Gibbs
sampler, at least in its best-known form. The scan can be done in a ﬁxed or
randomized order, or in an “up and down” direction, that is, when the order
of updating in the iteration is exactly the reverse of that in the preceding
one. Also, some “sites” (conditional distributions) may not be visited at
all in a given iteration, although all sites must be updated, eventually, and
visited an inﬁnite number of times, theoretically. For example, in a statis-
tical model with ﬁve unknowns, a scan pattern of 1, 2, 3, 1, 2, 3, 1, 2, 3, 4,
5 may be repeated indeﬁnitely (Neal, personal communication). One might
do this if variables 4 and 5 are thought to be almost independent of the
others, and mix faster, so that it is better to spend more time working on
1, 2, 3. For example, a suitable reparameterization of the trajectory param-
eters or of the location vector β, may enhance orthogonality, so one can
then spend more time updating the dispersion parameters and the u and e
vectors. In genetic applications, these two vectors may be highly colinear,
and may not be identiﬁable distinctly from each other when the additive
genetic relationship matrix (A) is nearly an identity matrix and G0 and Σe
are unknown. On the other hand, when dispersion parameters are known,
u and e have distinct conditional posterior distributions, although strong
posterior inter-correlations still may exist. The implementation described
here operates as follows: if the fully conditional distribution of a scalar or
vector can be identiﬁed, the corresponding sample is drawn directly, as in

15.4 Computation via Markov Chain Monte Carlo
655
standard Gibbs sampling. Otherwise, a Metropolis–Hastings, acceptance–
rejection or importance sampling (with resampling) step can be adopted.
15.4.1
Fully Conditional Posterior Distributions
The starting point for identifying the needed conditional posterior distri-
butions is the augmented joint posterior density (15.24). This is restated
here to facilitate references to the expression in subsequent developments.
The joint density is
p (θ, β, u, G0, Σe, γ|y1, y2, . . . , yM, α, Γ)
∝
# M
-
i=1
|Ri (γ)|−1
2 exp

−1
2ε′
iR−1
i
(γ) εi

p (θi|β, ui, Σe)
$
p (β|α, Γ) p (u|G0) .
(15.63)
The conditional posterior densities can be deduced by retaining the part
of (15.63) that is a function of the pertinent unknown, and treating the re-
maining portion as ﬁxed, becoming, thus, a part of the integration constant
of the conditional distribution sought. Such distributions will be examined
systematically in what follows.
Trajectory Parameters
From (15.63):
p (θ|β, u, G0, Σe, γ, y1, y2, . . . , yM, α, Γ)
∝
# M
-
i=1
exp

−1
2ε′
iR−1
i
(γ) εi

p (θi|β, ui, Σe)
$
.
(15.64)
This indicates that, given all other parameters, the trajectory coeﬃcients θi
of diﬀerent individuals are mutually independent of each other and, hence,
can be sampled independently. Thus
p (θi|β, u, G0, Σe, γ, y1, y2, . . . , yM, α, Γ) = p (θi|β, ui, Σe, γ, yi)
∝exp

−1
2 [yi −fi (θi, t)]′ R−1
i
(γ) [yi −fi (θi, t)]
%
× exp

−1
2 [θi −Xiβ −ui]′ Σ−1
e
[θi −Xiβ −ui]
%
(15.65)
for i = 1, 2, ..., M. The sampling method to be used depends on whether or
not the ﬁrst-stage model is linear in the trajectory parameters or not.

656
15. Bayesian Analysis of Longitudinal Data
Linear First-Stage: Gibbs Sampling
In the case of a linear ﬁrst-stage model
fi (θi, t) = Tiθi,
i = 1, 2, . . . , M,
for some known matrix Ti. Standard results for Bayesian linear models with
known dispersion parameters give as conditional posterior distribution,
θi|β, ui, Σe, γ, yi ∼N

θi, Vi

,
i = 1, 2, . . . , M,
(15.66)
where
θi =

T′
iR−1
i
(γ) Ti + Σ−1
e
−1 
T′
iR−1
i
(γ) yi + Σ−1
e
(Xiβ + ui)

,
(15.67)
and
Vi =

T′
iR−1
i
(γ) Ti + Σ−1
e
−1 .
(15.68)
Hence, collecting samples from the conditional posterior of the trajectory
parameters is straightforward, as the draws involve sampling from M in-
dependent r-variate normal distributions at each iteration of the MCMC
process. Most often, r is small, e.g., r = 3 in a Gompertz growth function.
Nonlinear First Stage: Metropolis–Hastings
When the trajectory is not linear in θi, the kernel of the distribution (15.65)
does not have a recognizable form, so direct drawing is not feasible. A ﬁrst
possibility here consists of setting up a Metropolis–Hastings scheme. The
main diﬃculty is the ﬁne-tuning of a proposal distribution that does not
result either in too frequent rejection or in a large acceptance rate. In the
ﬁrst situation, the sampler does not visit eﬀectively the parameter space;
in the second one, the algorithm moves very slowly, so that the ensuing
eﬀective sample size is small. Additional details on Metropolis–Hastings
computations are in the chapter on MCMC procedures.
A reasonable candidate-generating distribution might be a multivariate
normal process such as that in (15.66), but using the pseudo-data (instead
of the observed data vector) resulting from the Taylor series expansion
5yi = 6
Xi5β + 5Zi5u + 5H5ei + 5εi
evaluated at the current values of the unknowns in the course of iteration.
A computationally simpler (although probably less eﬀective) proposal dis-
tribution could be a normal process with ﬁxed mean vector and variance–
covariance matrix. For example, the proposal could be centered at the max-
imum likelihood estimates of individual θi parameters, i = 1, 2, ..., M. The
covariance matrix could be taken to be equal to the inverse of Fisher’s
expected information (given θi). This may work well if trajectories are
“long enough”, but it may be unsatisfactory for individuals having sparse
longitudinal information.

15.4 Computation via Markov Chain Monte Carlo
657
Nonlinear First Stage: Acceptance–Rejection
Another option in the nonlinear situation is using a rejection scheme (Ripley,
1987), also known as “acceptance and rejection”. Here the basic idea is to
cover the conditional posterior density of interest by an envelope, this being
the product of some sampling density S (θi) times a positive constant Qi,
such that density of the conditional posterior is smaller or equal than the
envelope at all values of θi. In our context, the required condition is, from
(15.65), that C (θi) ≤1, where
C (θi) = exp

−1
2

ε′
i (θi) R−1
i
(γ) εi (θi) + e′
i (θi) Σ−1
e ei (θi)

S (θi) Qi
(15.69)
i = 1, 2, ..., M. The ﬁrst- and second-stage residuals are written as
εi (θi) = yi −fi (θi, t) ,
and
ei (θi) = θi −Xiβ −ui,
respectively, to emphasize the dependence on θi; the integration constant
of the conditional posterior is unimportant because it can be absorbed in
Qi. Now take
S (θi) ∝exp

−1
2e′
i (θi) Σ−1
e ei (θi)

,
(15.70)
that is, the sampling density is the conditional (given all other parameters)
prior of θi. In short, S (θi) is the density of the normal process
θi|β, ui, Σe ∼N (Xiβ + ui, Σe) .
(15.71)
Further, let ,θi be the conditional ML estimator of θi obtained from the
ﬁrst-stage of the model, assuming Ri (γ) is known (here, γ would be ﬁxed
at the current value of the MCMC scheme). Then, set
Qi = exp

−1
2ε′
i

,θi

R−1
i
(γ) εi

,θi

.
It follows that in (15.69):
C (θi) =
exp

−1
2ε′
i (θi) R−1
i
(γ) εi (θi)

exp

−1
2ε′
i

,θi

R−1
i
(γ) εi

,θi
 ≤1,
(15.72)
for all i. This is so because the conditional likelihood, being proportional
to the numerator, is maximized when evaluated at ,θi, so the denominator
must be at least as large as the numerator for any value of the trajectory
parameters, with this being true for each individual. The sampling scheme
is conducted as follows:

658
15. Bayesian Analysis of Longitudinal Data
(a) draw the parameters from the conditional prior (15.71);
(b) evaluate (15.72), and
(c) extract a random deviate from an uniform U (0, 1) process.
If this deviate is smaller than C (θi) , the value sampled from the condi-
tional prior is accepted as belonging to the conditional posterior distribu-
tion having density as in (15.65); otherwise, the sample value is rejected, so
the process must be repeated until acceptance. A potential problem with
this scheme is that if the values drawn from the conditional prior have a
small likelihood, then C (θi) is always very small, causing a high rate of
rejection.
The sampling scheme is dynamic, in the sense that the parameters of
(15.71) change in the course of iteration. A more general treatment of
adaptive rejection/sampling schemes is in Gilks and Wild (1992). The basic
idea is that when a point is rejected, the envelope is updated, to correspond
more closely to the target density. This reduces the chances of rejecting
subsequent proposals, thus decreasing the number of functions that need
to be evaluated.
Nonlinear First Stage: Importance Sampling
A third possibility for nonlinear ﬁrst-stage models consists of employing an
importance sampling/resampling scheme (e.g., Tanner, 1996). This topic
was discussed in Chapter 12 in conjunction with a sensitivity analysis of
the Bayesian model. Here it is described in a little more detail in the context
of the longitudinal data problem. We consider ﬁrst how the method can
be used to compute a posterior expectation, and then see how the drawn
samples can be resampled, to arrive at draws from the posterior distribution
of interest.
Let g(θ) be the density of some posterior distribution of interest, and
suppose one wishes to compute the posterior expectation of the parameter
vector θ (or of h (θ), a function of θ); that is:
Eg(θ)
=

θg(θ)dθ
=

θg(θ)dθ

g(θ)dθ .
Often, this computation is not feasible, because of analytical diﬃculties.
Suppose there is some distribution having the same support as the poste-
rior of interest, and which is easy to sample from. This will be called the
importance distribution, and let its density be I (θ). For example, if the
sampling space of θ is ℜr, perhaps a multivariate normal or multivariate−t
distribution
of order r could be used. The posterior expectation of θ is
derived from (12.30):
Eg(θ) =

θw(θ)I(θ)dθ
EI [w(θ)]
(15.73)

15.4 Computation via Markov Chain Monte Carlo
659
where
w(θ) = g(θ)
I(θ)
(15.74)
and
EI [w(θ)] =

w(θ)I(θ)dθ =

g(θ)dθ.
Deﬁning the random variable
z(θ) =
w(θ)
EI [w(θ)]θ,
(15.75)
one can write
Eg(θ) =

z(θ)I(θ)dθ =EI [z(θ)] .
Now suppose that m independent samples are drawn from the distribution
with density I(θ), also called the “importance sampling” function, and let
such draws be θ[k] (k = 1, 2, . . . , m). Then a simulation consistent estimator
of the posterior expectation Eg(θ) or, equivalently, of EI [z(θ)] , is given
by (12.31)
5EI [z(θ)] =
m

k=1
w(θ[k])
m

k=1
w(θ[k])
θ[k].
(15.76)
In this expression, the denominator in (15.73) has been replaced by its
consistent estimator
5EI [w(θ)] = 1
m
m

k=1
w(θ[k]).
(15.77)
The “importance” weights are
w(θ[k])
m

k=1
w(θ[k])
=
g(θ[k])
I(θ[k])
m

k=1
g(θ[k])
I(θ[k])
=
cp(y|θ[k])p(θ[k])
I(θ[k])
m

k=1
cp(y|θ[k])p(θ[k])
I(θ[k])
=
p(y|θ[k])p(θ[k])
I(θ[k])
m

k=1
p(y|θ[k])p(θ[k])
I(θ[k])
,
(15.78)
where p (θ) is the prior density, p (y|θ) is the density of the sampling model,
and c is the integration constant of the posterior density. It follows that

660
15. Bayesian Analysis of Longitudinal Data
knowledge of the constant of integration is not needed to carry out the
importance sampling process, as it cancels out in the numerator and de-
nominator. Note, incidentally, that for the “new” weight
w∗(θ) = p (y|θ) p (θ)
I(θ)
,
EI [w∗(θ)]
=
 p (y|θ) p (θ)
I(θ)
I(θ)dθ
=

p (y|θ) p (θ) dθ = c−1.
Hence, a simulation consistent estimator of the integration constant (or of
its reciprocal) is given by
5c =
m
m

k=1
p(y|θ[k])p(θ[k])
I(θ[k])
.
As an important tuning issue, observe that an importance sampling den-
sity having thinner tails than the unnormalized posterior may cause some
weights to “blow up”, with the consequence that a few values dominate
others in the weighted average. This is a reason why a multivariate-t dis-
tribution, perhaps with six to eight degrees of freedom may be a better
importance function than the multivariate normal. For example, in the
context of drawing the nonlinear parameters of the individual trajecto-
ries, one could use the following r-variate t distribution, constructed from
(15.66):
θi|β, ui, Σe, γ, yi ∼tr

θi, Vi
v
v −2, v

with v degrees of freedom, and where Vi
v
v−2 is the variance–covariance
matrix of the t process.
Note in (15.76) that when the weights are constant
5EI [z(θi)] =
m

k=1
w(θ[k]
i )
m

k=1
w(θ[k]
i )
θ[k]
i
= 1
m
m

k=1
θ[k]
i ,
which is the posterior expectation calculated directly from the importance
sampling distribution. This implies that as the coeﬃcient of variation of the
weights goes to 0, the importance distribution “gets closer” to the posterior
distribution. This follows from (15.74), that is, if g(θi)/I(θi) is a constant
that does not depend on θi, it cancels out in the expression, with the result
that the importance distribution is identical to the posterior distribution
of interest.

15.4 Computation via Markov Chain Monte Carlo
661
The preceding indicates how a posterior expectation can be computed,
but does not give guidance on how a sample is to be drawn from the
conditional posterior distribution with density (15.65), so that the MCMC
procedure can continue. Now (15.73) implies that the random variable θi
with posterior density (15.65) has the same distribution as θi with density
w(θ)I(θ)
EI [w(θ)],
which clearly integrates to 1. This observation is the basis of the importance
sampling/resampling algorithm of Rubin (1988):
(1) Draw θ[k]
i , (k = 1, 2, ..., m) , from the importance distribution.
(2) Calculate the weights
w∗(θ[k]
i ) =
p

y|θ[k]
i

p

θ[k]
i

I(θ[k]
i )
,
and the relative weights
q[k] =
w∗(θ[k]
i )
m

k=1
w∗(θ[k]
i )
.
(3) Draw θ∗
i from a discrete distribution with Pr

θ∗
i = θ[k]
i

= q[k], so the
sample space of θ∗
i is the set
9
θ[1]
i , θ[2]
i , ..., θ[m]
i
:
.
As m →∞, then θ∗
i is a draw from the target posterior distribution. In
our context, this requires drawing a large number of importance samples
from the conditional posterior distribution of each of the trajectory param-
eters, and then resampling one value at random, with probability q[k]. This
value would be retained to continue the MCMC algorithm.
Second-Stage Location Eﬀects
Return to the joint posterior density in (15.63) and consider the part that
varies with β and u only. The conditional posterior density of β and u is
then
p (β, u|θ, G0, Σe, γ, y1, y2, . . . , yM, α, Γ)
∝
 M
-
i=1
p (θi|β, ui, Σe)

p (β|α, Γ) p (u|G0)
∝p (θ|β, u, Σe) p (β|α, Γ) p (u|G0) .
(15.79)

662
15. Bayesian Analysis of Longitudinal Data
This is precisely the posterior distribution of “ﬁxed” and “random” eﬀects
in a multivariate Gaussian mixed eﬀects linear model with known disper-
sion parameters, but with θi in lieu of the observations taken in individual
i. Here θi can be viewed as r attributes measured simultaneously on in-
dividual i. As developed in previous chapters, the conditional posterior
distribution is Gaussian, with mean vector
 ←→
β
←→
u

=

X′ 
I ⊗Σ−1
e

X + Γ−1
X′ 
I ⊗Σ−1
e

Z
Z′ 
I ⊗Σ−1
e

X
Z′ 
I ⊗Σ−1
e

Z + A−1 ⊗G−1
0
−1
×

X′ 
I ⊗Σ−1
e

θ + Γ−1α
Z′ 
I ⊗Σ−1
e

θ

,
(15.80)
and variance–covariance matrix

X′ 
I ⊗Σ−1
e

X + Γ−1
X′ 
I ⊗Σ−1
e

Z
Z′ 
I ⊗Σ−1
e

X
Z′ 
I ⊗Σ−1
e

Z + A−1 ⊗G−1
0
−1
.
(15.81)
It has been seen already that the draws from a Gaussian posterior dis-
tribution can be eﬀected either in a piecewise, blockwise, or multivariate
manner, with the only consequence of the method chosen being on the
mixing rate of the MCMC scheme. Naturally, if the order of θ allows one
to do so, samples can be obtained by standard methods for drawing from
multivariate Gaussian distribution.
First-Stage Dispersion Parameters
From (15.63), the conditional posterior density of the ﬁrst-stage dispersion
parameter vector γ is
p (γ|θ, β, u, G0, Σe, y1, y2, . . . , yM, α, Γ)
∝
M
-
i=1
|Ri (γ)|−1
2 exp

−1
2ε′
iR−1
i
(γ) εi

.
(15.82)
The form of this distribution depends on the speciﬁcation of the covariance
matrix Ri (γ) in (15.4). For example, if the ﬁrst-stage residuals are assumed
to be independently distributed and homoscedastic, then Ri (γ) = Iniγ.
Using this in (15.82) gives, as conditional posterior,
p (γ|θ, β, u, G0, Σe, y1, y2, ..., yM, α, Γ) ∝
M
-
i=1
γ−ni
2 exp

−1
2γ ε′
iεi

∝γ−N
2 exp


−1
2γ
M

i=1
ni

j=1
[yij −fij (θi, tij)]2


,
(15.83)

15.4 Computation via Markov Chain Monte Carlo
663
where N = M
i=1 ni is the total number of observations taken. This density
is that of the scaled inverted chi-square random variable
γ|θ, β, u, G0, Σe, y1, y2, ..., yM, α, Γ
∼



M

i=1
ni

j=1
[yij −fij (θi, tij)]2


χ−2
N−2,
γ ∈ℜ+
γ
(15.84)
which may be truncated if limits are placed on the parameter space ℜ+
γ .
On the other hand, if the residuals are independent but heteroscedas-
tic across individuals, such that Ri (γ) = Iniγi for i = 1, 2, ..., M, so
γ = [γ1, γ2, ..., γM]′ , then the ﬁrst-stage dispersion parameters are condi-
tionally independent of each other, with each having the conditional pos-
terior distribution
γi|θ, β, u, G0, Σe, y1, y2, ..., yM, α, Γ
∼



ni

j=1
[yij −fij (θi, tij)]2


χ−2
ni−2,
γi ∈ℜγi.
(15.85)
While in the preceding cases the conditional posterior distributions can
be identiﬁed and are easy to sample from, this is not so when there is a
more complex structure in the residual variance–covariance matrix. For ex-
ample, with auto-regressive processes, Markov-type dependencies, or with
a structural model for Ri (γ) , the corresponding distributions cannot be
recognized. Hence, a Metropolis–Hastings step, for example, must be in-
corporated.
Second-Stage Dispersion Parameters
The conditional posterior density of the second-stage residual variance-
covariance matrix can be deduced directly from (15.63) yielding
p (Σe|θ, β, u, G0, γ, y1, y2, . . . , yM, α, Γ) ∝
M
-
i=1
p (θi|β, ui, Σe)
∝|Σe|−M
2 exp

−1
2tr

Σ−1
e B

,
(15.86)
where, as before, B = M
i=1 eie′
i is an r × r matrix and ei = θi −Xiβ −ui.
The preceding density is the kernel of a scaled inverted Wishart distribution
of order r, scale matrix B, and “degrees of freedom” parameter equal to
M −r −1. If the parameter space of Σe is subject to restrictions beyond
|Σe| > 0, then the distribution is a truncated scaled inverted Wishart. As
seen in a previous chapter, it is relatively easy to sample from standard or
truncated scaled inverted Wishart distributions.

664
15. Bayesian Analysis of Longitudinal Data
Reorder now the additive genetic eﬀects by nesting individuals within
parameters, and let the ensuing vector be u∗, such that u∗
1 contains additive
genetic eﬀects for parameter 1, and so on. Thus, the conditional posterior
density of the second-stage additive genetic variance–covariance matrix G0
is
p (G0|γ, θ, β, u, Σe, y1, y2, ..., yM, α, Γ) ∝p (u|G0) = p (u∗| G0)
∝|G0 ⊗A|−1
2 exp

−1
2u∗′ (G0 ⊗A)−1 u∗

∝|G0|−q
2 exp

−1
2u∗′ 
G
−1
0
⊗A
−1
u∗

∝|G0|−q
2 exp

−1
2 tr

G
−1
0 U∗
,
(15.87)
where, as before, q is the order of the additive genetic relationship matrix
A, and U∗is the r × r symmetric matrix
U∗=


u∗′
1 A
−1u∗
1
u∗′
1 A
−1u∗
2
· · ·
u∗′
1 A
−1u∗
r
u∗′
2 A
−1
1 u∗
1
u∗′
2 A
−1u∗
2
· · ·
u∗′
1 A
−1u∗
r
...
...
...
...
u∗′
r A
−1u∗
1
u∗′
r A
−1u∗
2
· · ·
u∗′
r A
−1u∗
r


.
Density (15.87) is the kernel of a scaled inverted Wishart distribution of
order r, with scale matrix U∗and q −r −1 degrees of freedom.
This completes the description of all conditional posterior distributions
needed to implement the MCMC procedure. Once a chain of appropriate
length is run, and convergence to the equilibrium distribution seems to have
been attained, the analysis of the output proceeds in a standard manner.
Output analysis is discussed in Chapter 12.
15.5
Analysis with Thick-Tailed Distributions
It is known that the normal distribution is sensitive to departures from
assumptions, so one may wish to consider models based on “robust” distri-
butions, and one of these is the t process, either in its univariate or multi-
variate form (e.g., Rogers and Tukey, 1972; Zellner, 1976; Lange and Sin-
sheimer, 1993; Strand´en and Gianola, 1999). Regression and cross-sectional
models with t distributed errors have been discussed in Chapter 13, Section
13.6.
In the treatment of longitudinal data given so far, normality has been as-
sumed for the residuals of the ﬁrst two stages of the models. In what follows,
the assumption of normality of residuals at the ﬁrst stage will be replaced

15.5 Analysis with Thick-Tailed Distributions
665
by one of i.i.d. errors having a univariate-t distribution with unknown de-
grees of freedom. At the second stage, a multivariate-t distribution will be
employed for the second-stage residuals, instead of an r-variate normal dis-
tribution. For the sake of simplicity, it will be assumed that individuals are
genetically unrelated to each other. While this assumption is not tenable
in animal breeding, it is used frequently in biostatistics (Laird and Ware,
1982).
15.5.1
First- and Second-Stage Models
Recall that a t distribution, either univariate or multivariate, arises from
mixing a normal distribution over a gamma (equivalently, over an inverted
gamma or scale inverted chi-squared) process. This can be used to advan-
tage in an MCMC implementation by augmenting the joint posterior with
some unobservable “weights” following the appropriate gamma distribu-
tions.
The ﬁrst-stage model will be as in (15.2), but amended as
yij
=
fij (θi, tij) +
εij
√wij
=
fij (θi, tij) + ε∗
ij,
(15.88)
where εij ∼N (0, γ) and wij ∼Ga
 νε
2 , νε
2

are independently distributed
random variables. As seen before, the distribution of the “new” residual
ε∗
ij can be shown to be t (0, γ, νε), where γ is the scale parameter of the
t distribution and νε are the degrees of freedom, which will be treated as
unknown. The new residuals will be assumed to be mutually independent,
both within and between individuals. Note that, given wij, then
yij|θi, tij, γ, wij ∼N

fij (θi, tij) , γ
wij

.
Likewise, the second-stage model will be taken to be
θi = Xiβ +
ei
√wi
= Xiβ + e∗
i ,
i = 1, 2, . . . , M,
(15.89)
where ei ∼N (0, Σe) and wi ∼Ga
 νe
2 , νe
2

are independently distributed,
for all i, as well as between individuals. Under this assumption, the dis-
tribution of the “new” residual is the r-dimensional process tr (0, Σe, νe) ,
where Σe is the scale matrix and νε are the degrees of freedom, which will
also be treated as unknown. The trajectory parameters will be assumed
to be independent across individuals, a priori. As stated above, a genetic
eﬀect is not included in the speciﬁcation of the second-stage model. Here,
the second-stage residual reﬂects both genetic and nongenetic sources of

666
15. Bayesian Analysis of Longitudinal Data
variation (and covariation) between parameters, plus any error in the spec-
iﬁcation of the model. Note that, given wi, then
θi|β, Σe, wi ∼N

Xiβ, Σe
wi

.
The prior distributions for β, Σe, and γ will be as in (15.17), (15.20), and
(15.21), and the prior distributions of the two degrees of freedom parame-
ters will be assumed to be independent a priori, and uniform within some
bounded interval of the positive part of the real line. After augmenting the
prior with the trajectory parameters (as before) and with all weights wij
and wi, the joint prior density of all unknowns can be written as
p (θ, β, γ, Σe, wε, we, νε, νe|α, Γ) = p (θ|β, Σe, we) p (β|α, Γ) p (γ, Σe)
×p (wε|νε) p (we|νe) p (νε) p (νe)
∝p (θ|β, Σe, we) p (β|α, Γ) p (wε|νε) p (we|νe) ,
(15.90)
where wε = {wij} is an M
i=1 ni × 1 vector and we = {wi} is an M × 1
vector of second-stage “weights”. Furthermore, in view of the independence
assumption for parameters and weights,
p (θ, β, γ, Σe, wε, we, νε, νe|α, Γ)
∝
 M
-
i=1
p (θi|β, Σe, wi) p (wi|νe)

p (β|α, Γ)
M
-
i=1
ni
-
j=1
p (wij|νε) .
(15.91)
The joint posterior density is expressible as
p (θ, β,γ, Σe, wε, we, νε, νe|y1, y2, ..., yM, α, Γ)
∝


M
-
i=1
ni
-
j=1
p (yij|θi, γ, wij) p (wij|νε)


 M
-
i=1
p (θi|β, Σe, wi) p (wi|νe)

p (β|α, Γ) .
(15.92)
15.5.2
Fully Conditional Posterior Distributions
Trajectory Parameters
As in the purely normal hierarchical model, all trajectory parameters are
conditionally independent from each other. Using the
[parameter|ELSE]

15.5 Analysis with Thick-Tailed Distributions
667
notation to represent a fully conditional posterior distribution, one has that
p (θi|ELSE) ∝exp

−1
2γ [yi −fi (θi, t)]′
i Wεi [yi −fi (θi, t)]
%
× exp

−wi
2 (θi −Xiβ)′ Σ−1
e
(θi −Xiβ)

,
(15.93)
where Wεi = Diag {wi1, wi2, . . . , wini} is an ni × ni diagonal matrix, i =
1, 2, . . . , M.
If the model is nonlinear in the parameters, the distribution is not recog-
nizable, and the samples must be drawn by, for example, the Metropolis–
Hastings algorithm. On the other hand, if the model is linear, with
fi (θi, t) = Tiθi,
then the conditional posterior distribution is normal, so one can use Gibbs
sampling since draws can be eﬀected easily. Employing standard results for
combining quadratic forms, one can arrive at
θi|ELSE ∼N

θi, Vi

,
i = 1, 2, . . . , M.
(15.94)
The parameters of this normal distribution are
θi =

T′
i
Wεi
γ
Ti + wiΣ−1
e
−1 
T′
i
Wεi
γ
yi + wiΣ−1
e Xiβ

,
and
Vi =

T′
i
Wεi
γ
Ti + wiΣ−1
e
−1
.
Second-Stage Location Eﬀects
From the joint posterior density (15.92) one arrives at
p (β|ELSE) ∝
 M
-
i=1
p (θi|β, Σe, wi)

p (β|α, Γ)
∝exp
#
−1
2
 M

i=1
(θi −Xiβ)′ wiΣ−1
e
(θi −Xiβ) + (β −α)′ Γ−1 (β −α)
$
∝exp

−1
2

(θ −Xβ)′ 
We ⊗Σ−1
e

(θ −Xβ) + (β −α)′ Γ−1 (β −α)
%
,
(15.95)
where We = Diag (wi) is an M × M matrix of second stage weights. Since
the two intervening densities are in Gaussian form, this implies that the
conditional posterior distribution of β is normal, with mean vector

X′ 
We ⊗Σ−1
e

X + Γ−1−1 
X′ 
We ⊗Σ−1
e

θ + Γ−1α

,
(15.96)

668
15. Bayesian Analysis of Longitudinal Data
and covariance matrix

X′ 
We ⊗Σ−1
e

X + Γ−1−1 .
(15.97)
Hence, Gibbs sampling is straightforward.
Scale Parameter of the First Stage Distribution
Retaining, in the joint posterior density (15.92), only the terms that depend
on γ leads to
p (γ|ELSE) ∝
M
-
i=1
ni
-
j=1
p (yij|θi, γ, wij)
∝γ−N
2 exp


−1
2γ
M

i=1
ni

j=1
wij [yij −fij (θi, tij)]2


.
(15.98)
This is the density of the distribution
γ|ELSE ∼



M

i=1
ni

j=1
wij [yij −fij (θi, tij)]2


χ−2
N−2,
γ ∈ℜ+
γ ,
which is straightforward to sample from in the context of Gibbs sampling.
Scale Parameter of the Second-Stage Distribution
Similarly,
p (Σe|ELSE) ∝
M
-
i=1
9
|Σe|−1
2 exp

−wi
2 (θi −Xiβ)′ Σ−1
e
(θi −Xiβ)
:
∝|Σe|−M
2 exp

−1
2
M

i=1
w′
i (θi −Xiβ) Σ−1
e
(θi −Xiβ)

∝|Σe|−M
2 exp

−1
2tr

Σ−1
e Be

,
(15.99)
where
Be =
M

i=1
wi (θi −Xiβ) (θi −Xiβ)′ .
It can be readily ascertained that (15.99) is the kernel of a scaled inverted
Wishart distribution of order r, scale matrix Be, and “degrees of freedom”
parameter equal to M−r−1. If the parameter space of the scaled matrix Σe
is subject to restrictions beyond |Σe| > 0, then the resulting distribution
is truncated scaled-inverted Wishart.

15.5 Analysis with Thick-Tailed Distributions
669
Weight Parameters
The preceding conditional posterior distributions indicate that, so far, the
sampling process is as in the purely normal case, with the only novelty
being the appearance of the weights, which need to be sampled from the
corresponding conditional distributions. Consider the joint posterior den-
sity as a function of the ﬁrst-stage weights wij, yielding
p (wij|ELSE) ∝p (yij|θi, γ, wij) p (wij|νε)
∝w
1
2
ij exp
#
−wij [yij −fij (θi, tij)]2
2γ
$ 
w
νε
2 −1
ij
exp

−νεwij
2

for i = 1, 2, . . . , M and j = 1, 2, , . . . , ni. The expression in brackets is the
contribution from the gamma prior distribution of the ﬁrst-stage weights.
Rearrangement leads to
p (wij|ELSE) ∝w
νε+1
2
−1
ij
exp

−wijSij
2

,
(15.100)
where
Sij = [yij −fij (θi, tij)]2 + νεγ
γ
.
Hence, the conditional posterior distribution of each wij weight is the
gamma process
wij|ELSE ∼Ga

νε + 1
2
, Sij
2

,
i = 1, 2, . . . , M, j = 1, 2, . . . , ni.
(15.101)
Thus, the samples can be drawn without diﬃculty.
Similarly, the density of the conditional posterior distribution of the
second-stage weights can be put, after some algebra, as
p (wi|ELSE) ∝w
νe+1
2
−1
i
exp

−wiSi
2
%
,
where
Si = (θi −Xiβ)′ Σ−1
e
(θi −Xiβ) + νe.
Thus
wi|ELSE ∼Ga

νe + 1
2
, Si
2

,
i = 1, 2, . . . , M.
(15.102)
Degrees of Freedom
Inspection of the joint posterior density of the degrees of freedom parame-
ters reveals that, given all other parameters, the degrees of freedom of the

670
15. Bayesian Analysis of Longitudinal Data
ﬁrst- and second-stage distributions are mutually independent. For the ﬁrst
stage distribution, one has
p (νε|ELSE) ∝
M
-
i=1
ni
-
j=1
p (wij|νε)
∝


 νε
2
(
νε
2 )
Γ
 νε
2



N M
-
i=1
ni
-
j=1

w
νε
2 −1
ij
exp

−νεwij
2

.
(15.103)
For the second-stage degrees of freedom parameters, the resulting condi-
tional posterior distribution is
p (νe|ELSE) ∝
M
-
i=1
p (wi|νe)
∝


 νe
2
(
νe
2 )
Γ
 νe
2



M M
-
i=1

w
νe
2 −1
ij
exp

−νewi
2

.
(15.104)
None of the two distributions has a recognizable form. Hence, either a
Metropolis–Hastings, rejection, or importance sampling with resampling
step needs to be tailored, to draw the degrees of freedom. This is probably
the most diﬃcult part of the implementation, since it is not easy to arrive
at suitable proposal distributions or rejection envelopes (e.g., Strand´en,
1996). An alternative might be to set the degrees of freedom parameters
to some ﬁxed values, and then vary these, to study sensitivity of infer-
ences (Rodriguez-Zas, 1998; Rosa et al., 2001). In brief, the distributions
with densities as in (15.93), (15.95),(15.98), (15.99), and (15.101)–(15.104),
complete the speciﬁcation of a possible MCMC sampler for a longitudinal
data model with two tiers of robustness.
In conclusion, an MCMC analysis is particularly attractive for linear and
nonlinear ﬁrst-stage models for longitudinal data, relative to the two-step
approximate Bayesian analysis, where a number of (sometimes dubious)
approximations must be employed. In the linear case, the MCMC compu-
tations are equivalent to those needed to undertake a Bayesian mixed eﬀects
linear model analysis. In the nonlinear situations, computations are more
involved. When robust distributions are used to describe the uncertainty
about the ﬁrst- and second-stage models, additional diﬃculties arise, due
to the need to tune proposal distributions for sampling, e.g., the degrees of
freedom in the case of the t distributions.

16
Introduction to Segregation and
Quantitative Trait Loci Analysis
16.1
Introduction
The genetic model assumed so far is based on a very large number of
independent loci with each locus contributing additively with an inﬁnites-
imally small eﬀect to the additive genetic value of an individual. This has
been termed the inﬁnitesimal model, as noted earlier in this book. Like all
models, this is an intellectual abstraction. Box (1976) pointed out that all
models are wrong but that some are useful, and this is certainly the case of
the inﬁnitesimal model. It has been shown, however, to be remarkably ef-
fective for predicting expected response to artiﬁcial selection programmes,
for predicting breeding values of candidates for selection, for estimating
genetic variances and, interestingly enough, for interpreting results from
selection experiments (Martinez et al., 2000).
In many traits, in contrast, part of the genetic variance can be attributed
to one or more major genes segregating in the population. Many reasons
can be advanced for studying the number, location, mode of action and
magnitude of such gene eﬀects. The so called mixed inheritance model poses
that the genetic variance is partly due to many loci of inﬁnitesimally small
eﬀect, and partly due to the presence of a ﬁnite number of loci of relatively
large eﬀect. Prior to the availability of molecular markers, a method known
as complex segregation analysis was one of the most important tools for
detection of major genes. Inﬂuential papers were Elston and Stewart (1971),
Morton and MacLean (1974) and Lange and Elston (1975). This approach
was the basis for the successful mapping of many Mendelian genes in the

672
16. Segregation and Quantitative Trait Loci Analysis
1980s. Shortcomings that have been pointed out include the limited power
of the method for ﬁnding major genes and a lack of robustness, leading
to false detection (Go et al., 1978). Both problems are much alleviated
if information on genetic markers is incorporated into the analysis. The
explosion of molecular polymorphisms in the last twenty years or so has
stimulated the development and successful application of many methods for
major gene or, more generally, for quantitative trait loci (QTL) detection.
This chapter provides an introduction to some models that can be used
for inferring the presence of one or more major genes. The chapter is or-
ganized as follows. The ﬁrst section introduces the topic of segregation
analysis, and a Bayesian implementation is presented. The second section
discusses models for QTL detection. First, a model postulating a single
QTL is presented and both likelihood and Bayesian inferences are illus-
trated. Second, models with an unknown number of QTL are introduced.
The chapter ends with an application of reversible jump MCMC for making
inferences about the number of QTL segregating in a population.
16.2
Segregation Analysis Models
The simplest mixed inheritance model postulates that there is a single
major locus. Applications of the mixed inheritance model using MCMC can
be found, for example, in Guo and Thompson (1994), Janss et al. (1995),
and Lund and Jensen (1999). A useful recent review including many topics
in pedigree analysis, can be found in Thompson (2001).
16.2.1
Notation and Model
Assume a major locus with two alleles, A1 and A2, with respective gene
frequencies (1 −q) and q, and that the base (founding) population from
which base individuals were conceptually sampled was in Hardy–Weinberg
and linkage equilibrium. The genotypes at the major locus are A1A1, A1A2,
and A2A2. Due to Hardy–Weinberg equilibrium, alleles combine indepen-
dently to form these genotypes with frequencies (1 −q)2, 2q (1 −q), and q2,
respectively. No distinction is made between maternally and paternally in-
herited alleles. Deﬁne a vector m = (m1, m2, m3)′, whose elements describe
the eﬀects that genotypes A1A1, A1A2, and A2A2 have on the phenotypic
scale.
The genealogy is assumed to consist of nq individuals. For each individual
in the pedigree, deﬁne an unknown random variable wi (i = 1, 2, . . . , nq),
taking the values (1, 0, 0), (0, 1, 0) or (0, 0, 1) associated with genotypes
A1A1, A1A2, or A2A2, respectively. Also let
Pr (wi = k|q) ,
k = 1, 2, 3,

16.2 Segregation Analysis Models
673
Genotype of father:
wfi = 2
Genotype of mother:
wmi = 1
wmi = 2
wmi = 3
wi = 1
1/2
1/4
0
wi = 2
1/2
1/2
1/2
wi = 3
0
1/4
1/2
TABLE 16.1. Probability of oﬀspring genotypes given parental genotypes, when
the father is heterozygote at the major locus.
be the probability that wi takes values (1, 0, 0), (0, 1, 0), or (0, 0, 1), respec-
tively.
In the pedigree, there are individuals with unidentiﬁed fathers and moth-
ers. These are deﬁned as founders. On the other hand, the nonfounders are
individuals with both parents identiﬁed. When an individual has only one
parent identiﬁed, a phantom parent is created. This leads to simpler nota-
tion and simpler expressions later on.
Let W be a matrix of order nq × 3 such that its ith row is w′
i; thus, W
denotes the conﬁguration of the underlying genotypes at the major locus.
The p.m.f. of the genotypic conﬁguration is
p (W|q) =
-
founders i
p (wi|q)
-
nonfounders j
p (wj|wmj, wfj) ,
(16.1)
where subscript m (f) stands for the mother (father) of j. The product
decomposition of the ﬁrst term on the right-hand side arises because geno-
types of founders are assumed to be a function of gene frequencies, and
the model here postulates independence of the two alleles at the locus. The
model of genetic transmission gives rise to the product decomposition of the
second term on the right-hand side. This postulates that oﬀspring geno-
types are conditionally independent, given parental genotypes. Example
1.16 from Chapter 1 illustrates (16.1) for a pedigree with loops.
The ﬁrst term on the right-hand of (16.1) can take one of the following
forms:
Pr [wi = (1, 0, 0) |q] = (1 −q)2 ,
Pr [wi = (0, 1, 0) |q] = 2q (1 −q) ,
and
Pr [wi = (0, 0, 1) |q] = q2.
The second term on the right-hand side of (16.1) is a model for Mendelian
segregation. To illustrate, Table 16.1 shows the probabilities of oﬀspring
genotypes, given the genotypes of both parents, for one of the three possi-
ble paternal genotypes (the heterozygote) and the three possible maternal
genotypes. The notation has been simpliﬁed as follows: wi = (1, 0, 0) be-
comes wi = 1; wi = (0, 1, 0) becomes wi = 2, and wi = (0, 0, 1) becomes
wi = 3.

674
16. Segregation and Quantitative Trait Loci Analysis
In models with more than one locus, the p.m.f. p (wj|wmj, wfj) is a
function of the recombination fraction between the loci involved, provided
these loci are linked.
The model for the data assumes the following linear additive structure:
y = Xβ + Za + ZWm + e.
(16.2)
Note that matrix W is not observed. The other elements of the model have
been deﬁned in Section 13.2 of Chapter 13. The (conditional) sampling
distribution of the data is assumed to be Gaussian, with the form
y|β, a, W, m,σ2
e ∼N

Xβ + Za + ZWm, Iσ2
e

,
where σ2
e is the residual variance. That is, the elements of this vector are
assumed to be conditionally independent, given the parameters; the as-
sociated densities are referred to as penetrances in the linkage analysis
literature.
The prior distribution of the additive genetic values is
a|A, σ2
a ∼N

0, Aσ2
a

,
where A and σ2
a are the known relationship matrix and the unknown ad-
ditive genetic variance in the conceptual base population from which base
individuals were sampled, respectively. Parameters β, σ2
a, and σ2
e are as-
signed priors of the form in (13.4) and (13.5). The vector m is assigned a
proper uniform prior in R3, of the same form as in (13.4). Finally, a beta
distribution with known parameters e and f, Be (e, f), is assigned a priori
to describe previous knowledge of the gene frequency; thus,
p (q|e, f) ∝qe−1 (1 −q)f−1 .
(16.3)
After augmentation with W, the joint prior density admits the form
p

β, a, W, m, q, σ2
e, σ2
a

∝p (W|q) p (q) p

a|σ2
a

p

σ2
a

p

σ2
e

.
Given the model, the joint posterior distribution of the parameters (we
leave implicit the conditioning on A, the known incidence matrices and
hyperparameters) is given by
p

β, a, W, m, q, σ2
e, σ2
a|y

∝p

y|β, a, W, m, σ2
e

p

β, a, W, m, q, σ2
e, σ2
a

∝p

y|β, a, W, m, σ2
e

p (W|q) p (q) p

a|σ2
a

p

σ2
a

p

σ2
e

.
(16.4)
A classical full likelihood approach involves the joint maximization of
L (Ω|y) ∝

W

p

y|β, a, W, m,σ2
e

p

a|σ2
a

da

p (W|q) ,

16.2 Segregation Analysis Models
675
over Ω=

β′, m′, q, σ2
e, σ2
a
′. This shows that the likelihood is the expected
value of the conditional likelihood given W, with the distribution [W|q]
acting as the mixing process. The sum is taken over all possible genotypic
conﬁgurations of the pedigree, and the dimension of the integral is of the
order of the number of elements in a. In the Gaussian model the integration
can be performed analytically, but the summation over all possible genotype
conﬁgurations, in the case of complex pedigrees, is an insurmountable task.
In contrast, the MCMC Bayesian model can be implemented in a relatively
straightforward manner, as shown below.
16.2.2
Fully Conditional Posterior Distributions
The ﬁrst step of the Gibbs sampling algorithm is to ﬁnd a starting value
for the genotypic conﬁguration W. A simple way of achieving this is to
sample genes/genotypes from the prior distribution of founder individuals,
and then sample the nonfounder genotypes from the conditional probabil-
ity distribution of the nonfounder, given the genotype of its parents. This
is known as “gene dropping” (MacCluer et al., 1986) and is essentially a
Monte Carlo implementation of (16.1). Since the data contain information
on the major gene eﬀects, Guo and Thompson (1994) propose what they
call a “posterior gene dropping” approach: the major gene is dropped down
from the top of the pedigree conditionally on the current values of parame-
ters and the data on each individual. This method works well in the case of
the model under consideration with a continuous penetrance function in the
absence of typed genotypic information. However, with other penetrance
functions and data structures, as in pedigree studies where the data consist
of genotypes of some members of a pedigree (usually the younger ones) and
the objects of inference are the genotypes of the remaining members, the
approach can be exceedingly ineﬃcient. This is so, because the availabil-
ity of partial genotypic information imposes rigid compatibility constraints:
every proposed conﬁguration must be checked for consistency with the data
at hand, and rejection rates can be in the neighborhood of 100%. With this
type of data structure, a more eﬃcient approach can be found in Lange and
Goradia (1987) and in Lange (1997). Sheehan (2000) provides a good dis-
cussion about this and other issues involved in the application of MCMC
to genetic analyses on complex pedigrees.
The joint posterior distribution (16.4) has the same form as the joint
posterior (13.6). Therefore, allowing for the extra terms involving W and
m, the fully conditional posterior distribution of θ′ =

β′, m′, a′
is iden-
tical to (13.9) or (13.11). For example, a simple manipulation of (13.11)
shows that the fully conditional posterior distribution of m is
m|β, a, W, σ2
e, y ∼N

5m, (W′Z′ZW)−1 σ2
e

,

676
16. Segregation and Quantitative Trait Loci Analysis
where
5m = (W′Z′ZW)−1 W′Z′ (y −Xβ −Za) .
The term W′Z′ZW is a diagonal matrix, with elements equal to the num-
ber of A1A1 genotypes, A1A2 genotypes, and A2A2 genotypes, among those
individuals with records. Having sampled from the fully conditional poste-
rior distribution of m, one may wish to store contrasts like
k′ =

1
0
−1
−0.5
1
−0.5

.
The ﬁrst line in k′ represents the diﬀerence between homozygotes at the
major locus and the second represents the degree of dominance.
Likewise, the fully conditional posterior distributions of σ2
e and σ2
a have
the same forms as in (13.14) and (13.16). Thus
σ2
a|a, y ∼

a′A−1a + νaSa

χ−2
nq+νa
(16.5)
and
σ2
e|β, m, a, W, y
∼

(y −Xβ −Za −ZWm)′ (y −Xβ −Za −ZWm) + νeSe

χ−2
n+νe.
(16.6)
The preceding assumes independent scale inverted chi-square prior distri-
butions for σ2
a and σ2
e.
We derive now the fully conditional posterior distribution of wi. Follow-
ing Guo and Thompson (1994), deﬁne {wij} as the genotypes of the mates
of individual i, {wijl} as the genotypes of the oﬀspring of individuals i and
j, wmi as the genotype of the mother of i and, ﬁnally, wfi as the genotype
of the father of i. From the joint posterior (16.4) we can write
p

wi|β, a, W−i, m, σ2
e, y

∝p

y|β, a, W, m, σ2
e

p (W)
∝p

yi|β, a, wi, m, σ2
e

p (wi|W−i) .
(16.7)
The terms that include wi in p (wi|W−i) are
p (wi|W−i) ∝p (wi| {wij} , {wijl} , wmi, wfi)
∝p (wi, {wij} , {wijl} , wmi, wfi)
=
ni
-
j=1
p (wijl|wi, wij, wmi, wfi) p (wi, wij, wmi, wfi)
∝
ni
-
j=1
p (wijl|wi, wij) p (wi|wmi, wfi) ,
(16.8)

16.2 Segregation Analysis Models
677
where ni are the number of mates of i and nij (below) are the number of
oﬀspring that i has with mate j. Substituting in (16.7) yields
p

wi|β, a, W−i, m, σ2
e, y

∝p

yi|β, a, wi, m, σ2
e

×
ni
-
j=1
nij
-
l=1
p (wijl|wi, wij) p (wi|wmi, wfi) .
(16.9)
This is a discrete distribution of unknown analytical form, but which is easy
to sample from. Expression (16.9) is evaluated for the three possible values
that wi can take and, after normalization, wi is accepted with probability
equal to the appropriate normalized value. This is done by drawing from a
uniform distribution Un (0, 1).
If individual i has no phenotypic record, the fully conditional distribu-
tion is proportional to p (wi|W−i). If individual i is a founder, the term
p (wi|wmi, wfi) in (16.9) is replaced by p (wi|q), which is a function of gene
frequency, as indicated in (16.1).
Finally, the fully conditional posterior distribution of the gene frequency
is obtained as follows. From (16.4),
p (q|W, y)
∝
p (W|q) p (q)
∝
p (q)
-
founders i
p (wi|q) .
(16.10)
Seen as a function of q, the second term has the form
-
founders i
p (wi|q) ∝(1 −q)nA1 qnA2,
where nA1 and nA2 are the number of A1 and A2 alleles among founder
individuals. Given the prior density Be (q|e, f) in (16.3), (16.10) becomes
p (q|W, y) ∝(1 −q)(nA1+f−1) q(nA2+e−1),
which is the kernel of a beta distribution with parameters nA1 + f and
nA2 + e. That is,
q|W, y ∼Be (nA1 + f, nA2 + e) .
(16.11)
If a uniform prior Un (0, 1) is assumed for q, instead of (16.3), the fully
conditional posterior distribution for q has density
q|W, y ∼Be (q|nA1, nA2) .
16.2.3
Some Implementation Issues
It was discussed in Chapter 10 that a ﬁnite-state space discrete Markov
chain converges to a unique stationary distribution, provided it is irre-
ducible and aperiodic. The sampling scheme for the major locus genotype

678
16. Segregation and Quantitative Trait Loci Analysis
deﬁnes a discrete chain. Starting from a particular “legal” genotypic con-
ﬁguration, one wishes to know whether it is possible to visit any other legal
conﬁguration by updating individual genotypes one at a time.
In the case of a diallelic locus with alleles A1 and A2, Sheehan and
Thomas (1993) show that irreducibility of the Markov chain is guaranteed,
provided that, if for all y satisfying
p (y|A1A1) p (y|A2A2) > 0,
(16.12)
it also holds that
p (y|A1A2) > 0.
(16.13)
In other words, irreducibility is established unless the genetic model allows
for a phenotype compatible with both homozygous genotypes, to be incom-
patible with the heterozygote state. This is the only situation in which the
Gibbs sampler may deﬁne a reducible Markov chain for a diallelic system.
To illustrate, consider the following example taken from Sheehan (2000).
Individuals are classiﬁed into “aﬀected” or “normal”, depending on whether
they are homozygotes or heterozygotes, respectively. Data are available on
a mother-daughter pair, where both are “aﬀected”. If the starting legal
conﬁguration assigns genotype A1A1 to the mother, then the updating
scheme based on the genotypic distribution of the daughter conditional
on the mother’s genotype assigns probability 1 to the event “genotype of
daughter is A1A1”. The daughter can never change to the other homozygote
genotype, given that the mother has genotype A1A1.
With more than two alleles at a single locus, irreducibility is no longer en-
sured by imposing a simple condition such as that deﬁned by (16.12) and
(16.13). Reducibility depends on the data and method of sampling. For
example, consider the following human ABO blood group example, taken
from Sheehan and Thomas (1993). The data (y) consist of the genotypes of
two oﬀspring, which are A1B and OO, respectively. This implies that the
genotype of the parents must be (A1O, BO) or (BO, A1O). Consider an
updating scheme consisting of drawing from the conditional posterior dis-
tribution of a parent, given the genotype of the other parent and the data.
For example, once the father’s genotype (f) has been assigned (A1O, say),
the maternal genotype (m) is forced to be the complementary type (BO)
with probability 1. Given the data and the sampling mechanism, this up-
dating scheme creates a Markov chain with two noncommunicating states.
The way around this toy problem is to sample the parental genotypes
jointly, in one block, from the joint distribution
Pr (f = A1O, m = BO|y) = 1
2,
Pr (f = BO, m = A1O|y) = 1
2.

16.3 QTL Models
679
These examples emphasize that care must be taken about inferences de-
rived from a single site updating Gibbs scheme, in multiallelic systems.
In fact, there is at present no method that can guarantee irreducibility in
large complex pedigrees. More importantly, even if the chain is irreducible,
mixing may be slow; consequently, a limited range of the support of the
posterior distribution may be visited by the Monte Carlo draws. This will
result in poor inferences. Slow mixing of the chain aﬀects convergence of the
time-average of draws to the expectation of the function under the equilib-
rium distribution, even if the chain starts in the equilibrium distribution.
Various methods for dealing with reducibility and slow mixing have been
proposed in recent years, and many of these are reviewed by Sheehan (2000)
and by Thompson (2001). Several of the approaches are based on a vari-
ety of joint-updating schemes. For example, Jensen et al. (1995) update
genotypes of blocks of individuals jointly at several loci. The method was
extended by Lund and Jensen (1999) to cope with the mixed inheritance
model. Janss et al. (1995) also propose a blocking scheme, whereby a sire
with all its ﬁnal oﬀspring are updated in one pass. Other forms of joint up-
dating were discussed by Heath (1997), Thompson and Heath (2000) and
Sheehan et al. (2002). In general, the joint updating sampling strategies
are a great improvement over single-site methods.
16.3
Models for the Detection of
Quantitative Trait Loci
In the last decade, and due to the availability of information on molecular
markers, there has been much interest in detecting chromosomal regions
responsible for some of the variation observed for quantitative traits. In
particular, an extensive literature on methods for detecting QTL using
marked regions has accumulated. The objective here is to provide an intro-
duction to statistical aspects of the subject and to illustrate how likelihood
or MCMC-based methods can be applied for drawing inferences concern-
ing eﬀects and positions of such QTL. The presentation is restricted to the
analysis of a backcross design involving inbred lines; full marker information
in a single chromosome is assumed.
First, a model with a single QTL ﬂanked by two marker loci is introduced.
This is then extended to models with an arbitrary number of QTL using
information on a large number of genetic markers. In this ﬁnal section,
ways in which models can be compared using their posterior probabilities
are outlined.

680
16. Segregation and Quantitative Trait Loci Analysis
16.3.1
Models with a Single QTL
In the single QTL model, the marker at the ﬁrst locus has alleles M and
m, and at the second locus, the marker alleles are N and n. At each locus,
markers are assumed to be codominant. Interest focuses on whether there is
evidence for the presence of a QTL, with unknown alleles Q and q, placed
between the two (known) markers. That is, it is assumed here that the
locus order is MQN. (The frequency of the QTL alleles does not feature
in this section; therefore the symbol q in this section is always associated
with the QTL allele, and not with allele frequency).
There may also be genetic variation contributed by genes of small eﬀect.
Due to the nature of the experimental design, this variation cannot be
estimated and is part of the residual variance.
The recombination fraction between locus M and the QTL will be de-
noted by rm, and the recombination fraction between the QTL and locus N
by rn. It is assumed that recombination in the M-Q interval is independent
of recombination in the Q-N interval. Therefore the probability of a double
recombinant is rmrn. Another simplifying assumption is that the recombi-
nation fraction between the two markers, r, is known, and that it is less
than 0.5. A recombination occurs, if the number of crossovers between the
loci involved is odd. Recombination between M and N, which is the prob-
ability of an odd number of crossovers, arises as follows. A recombination
occurs between M and Q and not between Q and N, or a recombination
occurs between Q and N and not between M and Q. With no interference,
the probability of recombination is the sum of the probabilities of these two
mutually exclusive events. Thus,
r = rm (1 −rn) + (1 −rm) rn = rm + rn −2rmrn
(e.g., Ott, 1999), which implies that
rm = (r −rn)/ (1 −2rn) ,
0 < rm < r, 0 < rn < r.
(16.14)
Thus, with r known, there is only one recombination fraction to be esti-
mated, because rm can be written as a function of rn or vice-versa.
It is often convenient to parameterize the model in terms of genetic map
distances rather than in terms of recombination fractions. The genetic map
distance between two loci is deﬁned as the expected number of crossovers
occurring on a given chromosome (in a gamete) between the loci (Ott,
1999). Genetic map distances are expressed in centimorgans (cM). The
advantage of genetic map distances is that these are additive, whereas re-
combination fractions are not. Parameterization with genetic map distances
requires mapping functions; there are several such functions (see Ott (1999)
for an overview). Here the one proposed by Haldane (1919) is used, which
assumes that recombination between any two markers is independent of
that occurring at other marker intervals (i.e., no chiasma interference is as-
sumed). Using this mapping function, the distance λ (measured in units of

16.3 QTL Models
681
Morgans, a positive quantity) and the recombination fraction r are related
by the equation
r = f (λ) = 1
2 [1 −exp (−2λ)] ,
(16.15)
with inverse function
λ = f −1 (r) = −1
2 ln (1 −2r) .
(16.16)
Thus, if the genetic map distance between loci i and i + 1 is |λi+1 −λi|,
the recombination fraction is
r = f (λi+1 −λi) = 1
2 [1 −exp (−2 |λi+1 −λi|)] .
Data from a backcross design are assumed to be generated as follows.
Consider two completely inbred lines, one with genotype MQN/MQN and
the other with genotype mqn/mqn. These lines are crossed to produce F1
individuals (generation 1), all having genotype MQN/mqn. Notationally,
the haplotype to the left of the slanted line in MQN/mqn represents the
paternal gamete, and the one on the right (mqn) represents the maternal
gamete. The inbred lines are typically chosen on the basis of some pheno-
typic attribute, as opposed to being randomly sampled.
The F1 individuals are crossed back to individuals from one of the inbred
lines, those carrying genotype MQN/MQN say, to produce the backcross
generation (generation 2). The marker genotype for the ith individual of
generation 2 is denoted Mi, which can take values G1 = MN/MN, G2 =
Mn/MN, G3 = mN/MN, and G4 = mn/MN. The QTL genotype of
individual i from generation 2, Qi, is a random variable which can take
values Qi = QQ or Qi = Qq.
In a backcross design, individuals are genetically uncorrelated. To see
this, let ai and aj denote additive genetic values of individuals i and j
randomly sampled from generation 2. These additive genetic values arise
due to the presence of many genes each of small eﬀect acting additively
on the genotype. If the original lines are completely inbred, there is no
variation among the paternal additive genetic values (denoted as af) nor
among the maternal additive genetic values (am) of individuals i and j.
Then,
Cov (ai, aj)
= E [Cov (ai, aj|af, am)] + Cov [E (ai|af, am) , E (aj|af, am)]
= 0 + Cov
1
2 (af + am) , 1
2 (af + am)

= 1
4V ar (af + am) = 0,
i ̸= j.
The term E [Cov (ai, aj|af, am)] is zero because, given the additive genetic
values of the parents, additive genetic values in the oﬀspring are uncorre-
lated. Absence of genetic variation in the parents implies absence of genetic

682
16. Segregation and Quantitative Trait Loci Analysis
covariation in the oﬀspring, despite there being genetic variation among the
latter.
Conditionally on the unknown QTL genotype, it is assumed that the
phenotypic record of the ith individual is normal of the form
yi|Qi = QQ ∼N

µ1, σ2
,
(16.17)
yi|Qi = Qq ∼N

µ2, σ2
,
(16.18)
where µ1 is the mean phenotype of individuals carrying QTL genotype
QQ, and µ2 is the mean phenotype of individuals carrying QTL genotype
Qq. The variance term σ2 typically contains a contribution from polygenic
eﬀects at many other loci.
Likelihood Inference
Writing the Likelihood
The parameters of interest are θ′ =

µ1, µ2, rm, σ2
. The data consist of
records on a particular trait, represented by the vector y of length nobs and
by information on the marker genotypes from generation 2. The likelihood
is proportional to the density of the data, given marker information, which
for record i is denoted by p (yi|θ, Mi). The contribution to the likelihood
from the record of individual i can be written as
L (θ|yi, Mi)
∝p (yi|QQ, Mi) Pr (QQ|Mi, rm) + p (yi|Qq, Mi) Pr (Qq|Mi, rm)
= p (yi|QQ) Pr (QQ|Mi, rm) + p (yi|Qq) Pr (Qq|Mi, rm) ,
(16.19)
which is a mixture of normal distributions. (Notationally, terms of the form
p (yi|Qi = QQ, Mi) or Pr (Qi = QQ|Mi, rm) say, are written here as
p (yi|QQ, Mi) ,
or
Pr (QQ|Mi, rm) ,
respectively). The equality in the second line of (16.19) arises because,
given the QTL genotype, the marker genotype does not contribute with
additional information to the probability of observing phenotype yi. In
view of the fact that records are independently distributed, and denoting
the complete marker information by the vector M, the likelihood is given
by
L (θ|y, M) ∝
n
-
i=1
L (θ|yi, Mi) .
(16.20)
In (16.19), the terms Pr (QQ|Mi) and Pr (Qq|Mi) are the conditional
probabilities of observing a particular QTL genotype given marker infor-
mation. As shown below, these are functions of the recombination fractions.

16.3 QTL Models
683
Genotype
Pr (Genotype)
Marker
QTL
MqN/MQN
rmrn/2
MN/MN
Qq
mQn/MQN
rmrn/2
mn/MN
QQ
Mqn/MQN
rm (1 −rn) /2
Mn/MN
Qq
mQN/MQN
rm (1 −rn) /2
mN/MN
QQ
MQn/MQN
rn (1 −rm) /2
Mn/MN
QQ
mqN/MQN
rn (1 −rm) /2
mN/MN
Qq
MQN/MQN
(1 −rm) (1 −rn) /2
MN/MN
QQ
mqn/MQN
(1 −rm) (1 −rn) /2
mn/MN
Qq
TABLE 16.2. Distribution of genotypes in the backcross design.
The genotypic distribution among generation 2 individuals, together with
the observed marker information and the associated putative QTL geno-
types, are shown in Table 16.2. It is emphasized that Haldane’s mapping
function, which assumes no interference, is assumed for these calculations.
The marker genotype is observed in a backcross oﬀspring, but the geno-
type (ﬁrst column) is unknown, because the QTL genotype is not observed.
Given that marker genotypes and phase of parents are known, the prob-
ability of a given genotype (Column 2) in the oﬀspring can be readily
calculated.
To illustrate, consider the term Pr (QQ|Mi = G3, rm) where, for indi-
vidual i, the observed marker genotype is G3 = mN/MN, say. This is
computed as
Pr (QQ|Mi = G3, rm) = Pr (QQ, Mi = G3|rm)/ Pr (Mi = G3|rm) .
The term Pr (QQ, Mi = G3|rm) is equal to 1
2rm (1 −rn), associated with
genotype mQN/MQN in the 4th row in the body of Table 16.2. The de-
nominator
Pr (Mi = G3|rm) = Pr (QQ, Mi = G3|rm) + Pr (Qq, Mi = G3|rm)
is equal to the sum of the genotype probabilities in rows 4 and 6 in the
body of Table 16.2,
1
2rm (1 −rn) + 1
2 (1 −rm) rn.
Therefore,
Pr (QQ|Mi = G3, rm)
=
rm (1 −rn)
rm (1 −rn) + (1 −rm) rn
=
rm (1 −rn)
r
.
Likewise,
Pr (Qq|Mi = G3, rm) = (1 −rm) rn
r
.

684
16. Segregation and Quantitative Trait Loci Analysis
Marker
Pr (QQ|Mi, rm)
Pr (Qq|Mi, rm)
MN/MN
(1 −rm) (1 −rn)/ (1 −r)
rmrn/ (1 −r)
Mn/MN
(1 −rm) rn/ r
rm (1 −rn)/ r
mN/MN
rm (1 −rn)/ r
(1 −rm) rn/ r
mn/MN
rmrn/ (1 −r)
(1 −rm) (1 −rn)/ (1 −r)
TABLE 16.3. Conditional probabilities of QTL genotypes given marker genotypes
in the backcross generation.
Table 16.3 shows the terms Pr (QQ|Mi, rm) and Pr (Qq|Mi, rm) for all
possible values of Mi.
If individual i has observed marker information Mi = mN/MN, from
(16.19), its contribution to the overall likelihood is
L (θ|yi, Mi) ∝p (yi|QQ) rm (1 −rn) + p (yi|Qq) (1 −rm) rn.
(16.21)
The recombination fractions satisfy (16.14).
Hypotheses Tests
The test for the presence of a QTL between marker loci M and N, versus
absence of a QTL segregating, consists of computing the likelihood ratio.
This observed likelihood ratio is (Knott and Haley, 1992)
LR =
L

5µ1, 5µ2, 6
σ2, 5rm|y, M

L

,µ, E
σ2|y, M

,
(16.22)
where 5µ1, 5µ2, 6
σ2, 5rm are the values obtained by joint maximization of
L (θ|y, M) ,
and ,µ, E
σ2 are the values obtained by maximizing the likelihood restricted
by the null hypothesis (no QTL segregating).
Let T (Y) be equal to (16.22), but with the important diﬀerence that
T (Y) is a function of the random variable Y rather than of the observed
data y, which is a realized value of this random variable. A test of the null
hypothesis may require computing the so-called p-value
Pr [T (Y) ≥LR] =

I [T (y) ≥LR] p

y|µ, σ2, M

dy.
(16.23)
In a classical test of hypothesis, the null hypothesis is rejected if (16.23)
is smaller than or equal to Pr [Type I error]. Because µ and σ2 are usually
not known, (16.23) cannot be computed exactly. In this situation one often
appeals to asymptotic results. Under regularity conditions
Pr [2 ln T (Y) ≥2 ln LR] = Pr

χ2
2 ≥2 ln LR

.

16.3 QTL Models
685
That is, asymptotically, 2 ln T (Y) has a chi-square distribution, with two
degrees of freedom in this case.
An alternative to asymptotic theory is to obtain a Monte Carlo approx-
imation to (16.23). Since µ and σ2 are not known, these are replaced by
their maximum likelihood estimates ,µ, E
σ2. The required probability is now
approximated by
Pr [T (Y) ≥LR] =

I [T (y) ≥LR] p

y|,µ, E
σ2, M

dy.
(16.24)
This integration can be approximated drawing data vectors yi (i = 1, . . . , N)
from p

yi|,µ, E
σ2, M

and computing
Pr [T (Y) ≥LR] ≈1
N

i
I (T (yi) ≥LR) ,
(16.25)
where N represents the number of samples drawn. Other suggested proce-
dures are based on permutation tests (Doerge and Churchill, 1996).
Rather than maximizing (16.22), a proﬁle likelihood is often computed,
whereby log10 of the ratios of the form in (16.22) are calculated over a
grid of values of rm. The term 2 log10 is known as the LOD score (see Ott,
1999 for a detailed discussion). The maximum LOD score indicates the grid
value of rm closest to the maximum likelihood estimate of rm. A smooth
curve is ﬁtted to the set of LOD score values, and a measure of uncertainty,
in conceptual repeated sampling, is obtained by a 2 (LOD) interval. This
interval is the set of values of rm at which the LOD is not smaller than
its maximum value minus two. The LOD score can be multiplied by a
factor 2 ln (10) = 4.605 for it to have the convenient property of being
asymptotically distributed as a chi-square random variable.
We end this section with a word of caution. Setting up the correct test
of hypothesis in a conventional likelihood scenario is a contentious issue. A
test often entertained is based on
L

5µ1, 5µ2, 6
σ2, 5rm|y, M

L

,µ1, ,µ2F
, σ2, rm = 0.5|y, M
.
The null hypothesis assumes the absence of a QTL between the region
ﬂanked by the markers, but allows for the fact that a QTL may be present
elsewhere in the genome. This form of the likelihood ratio places the pa-
rameter rm ∈[0, 0.5] at the boundary of the parameter space and, as a
consequence, the necessary regularity conditions associated with classical
asymptotic theory are not satisﬁed. Other possible tests are discussed in
Knott and Haley (1992).

686
16. Segregation and Quantitative Trait Loci Analysis
Bayesian Inference
The analysis in the previous section is now performed from a Bayesian
perspective. Except for the additional presence of prior distributions, the
model here is similar to the one in the previous section. As before, it is
assumed that the genetic distance between the two markers is known.
Let Q represent the unknown QTL genotype of all individuals and let
Qi represent the unknown QTL genotype of individual i. The parameters
of the Bayesian model are (Q, rm) and θ′ =

µ1, µ2, σ2
.
The prior distributions of the parameters are assumed to be as follows.
First, the elements of θ are taken to be a priori independently distributed.
As prior for µi a normal distribution is invoked, with zero mean and vari-
ance 10, say, to allow for large QTL eﬀects; thus µi ∼N

0, σ2
0 = 10

,
(i = 1, 2). The prior for the residual variance is assumed to be a scaled
inverted chi-square distribution with known parameters ν and S; that is,
σ2 ∼νSχ−2
ν . It is also assumed that (Q, rm) and θ are a priori indepen-
dently distributed.
Prior information about the recombination fraction could be incorpo-
rated as follows. Consider the beta distributed random variable with den-
sity
η ∼Be (η|a, b) ,
(0 < η < 1) ,
where a and b are known parameters deﬁning the shape of the distribution.
The recombination fraction rm must take positive probability in the set
]0, r[ (see (16.14)). Now let rm = rη. The inverse transformation is equal
to η = r−1rm, the Jacobian of the transformation is r−1, and therefore the
probability density of rm has the form
p (rm|a, b) =

C

r−1rm
a−1 
1 −r−1rm
b−1 r−1,
0 < rm < r,
0,
otherwise,
(16.26)
where C is a constant that does not depend on rm.
The posterior distribution is given by
p (Q, rm, θ|y, M) ∝p (Q, rm) p (θ) p (y,M|Q, rm, θ)
= p (Q, rm) p (θ) Pr (M|Q, rm, θ) p (y|M, Q, rm, θ)
∝p (rm) p (µ1) p (µ2) p

σ2
Pr (Q|rm, M) p (y|Q, θ) ,
(16.27)
where
p (y|Q, θ) =
n
-
i=1
p (yi|Qi, θ) ,
(16.28)
and
Pr (Q|rm, M) =
n
-
i=1
Pr (Qi|rm, Mi) .
(16.29)

16.3 QTL Models
687
Expressions (16.28) and (16.29) imply that the QTL genotype can be drawn
for each individual at a time. This conditional independence property is a
consequence of the experimental design. In the backcross design, both the
phase and the QTL genotype of parents are known. Therefore, the parental
origin of an oﬀspring’s gamete can be unambiguously assigned. This is not
the case with data from outbred populations.
The joint posterior is deﬁned within the range of values of the parame-
ters. Marginalization of (16.27) with respect to Q, assuming uniform prior
distributions for rm and θ, retrieves a posterior distribution which has the
same form as likelihood (16.20) derived from the mixture (16.19).
Fully Conditional Posterior Distributions
Extracting the terms containing µ1 from (16.27) yields
p (µ1|., data) ∝p (µ1)
n
-
i=1
p

yi|Qi, µ1, σ2I(Qi=QQ)
∝exp

−µ2
1
2σ2
0

exp

−
n
i=1 I (Qi = QQ) (yi −µ1)2
2σ2

,
(16.30)
where I(Qi = QQ) is the indicator function that takes the value one
when the individual is QQ, and zero otherwise. Let 5µ1 = [n
i=1 I(Qi =
QQ)yi]/nQQ be the mean of the records on individuals whose QTL geno-
type is QQ, where nQQ is the number of such individuals. Then (16.30)
can be expressed as
p (µ1|., data)
∝exp

−σ2µ2
1 + σ2
0
n
i=1 I (Qi = QQ) [(yi −5µ1) + (5µ1 −µ1)]2
2σ2σ2
0

∝exp

−σ2µ2
1 + σ2
0nQQ (5µ1 −µ1)2
2σ2σ2
0

.
(16.31)
Making use of the identity (Box and Tiao, 1973, page 74):
A (z −a)2 + B (z −b)2 = (A + B) (z −c)2 +
AB
A + B (a −b)2
and letting c = (Aa + Bb)/(A + B), A = σ2, B = σ2
0nQQ, z = µ1, a = 0,
and b = 5µ1 allows expressing (16.31) as
p (µ1|., data) ∝exp

−

σ2 + nQQσ2
0

(µ1 −c)2
2σ2σ2
0

,
where c = nQQσ2
05µ1/(σ2 + nQQσ2
0). Therefore,
µ1|., data ∼N

 nQQσ2
05µ1
σ2 + nQQσ2
0
,
σ2σ2
0
σ2 + nQQσ2
0

.
(16.32)

688
16. Segregation and Quantitative Trait Loci Analysis
By symmetry considerations,
µ2|., data ∼N

 nQqσ2
05µ2
σ2 + nQqσ2
0
,
σ2σ2
0
σ2 + nQqσ2
0

,
(16.33)
where nQq is the number of individuals with QTL genotype Qq, and 5µ2 =
n
i=1 I(Qi = Qq)yi/nQq.
To derive p

σ2|., data

the terms containing σ2 are extracted from the
joint posterior (16.27). This yields
p

σ2|., data

∝p

σ2
n
-
i=1

p

yi|Qi, µ1, σ2I(Qi=QQ)
× p

yi|Qi, µ2, σ2I(Qi=Qq)
∝p

σ2
n
-
i=1
#

σ2−1
2 exp

−I (Qi = QQ) (yi −µ1)2
2σ2

× exp

−I (Qi = Qq) (yi −µ2)2
2σ2
$
.
Let
VQQ =
n
i=1
I (Qi = QQ) (yi −5µ1)2
nQQ
and
VQq =
n
i=1
I (Qi = Qq) (yi −5µ2)2
nQq
.
Then p

σ2|., data

can be written as
p

σ2|., data

∝

σ2−( ν+n
2
+1)
× exp


−
nQQ

VQQ + (µ1 −5µ1)2
+ nQq

VQq + (µ2 −5µ2)2
+ νS
2σ2



=

σ2−( ν
2 +1) exp

−,ν ,S
2σ2

,
where ,ν = ν + n and
,S =
9
nQQ

VQQ + (µ1 −5µ1)2
+ nQq

VQq + (µ2 −5µ2)2
+ νS
:G
,ν.

16.3 QTL Models
689
This is recognized as the density of a scaled inverted chi-square distribution
with parameters ,S and ,ν
σ2|., data ∼,ν ,Sχ−2
ν .
(16.34)
From the joint posterior (16.27) the fully conditional posterior distribu-
tion of the QTL genotypes is proportional to
Pr (Q|., data) ∝Pr (Q|rm, M) p (y|Q, θ)
=
n
-
i=1
Pr (Qi|rm, Mi) p (yi|Qi, θ) ,
(16.35)
which implies that sampling can proceed separately for each individual. For
individual i
Pr (Qi = QQ|., data) = Pr (Qi = QQ|rm, Mi) p (yi|Qi = QQ, θ)

ω
Pr (Qi = ω|rm, Mi) p (yi|Qi = ω, θ) , (16.36)
where ω denotes QQ or Qq. Similarly,
Pr (Qi = Qq|., data) = Pr (Qi = Qq|rm, Mi) p (yi|Qi = Qq, θ)

ω
Pr (Qi = ω|rm, Mi) p (yi|Qi = ω, θ).
(16.37)
Finally, the fully conditional posterior distribution of the recombination
fraction rm is proportional to
p (rm|., data) ∝p (rm)
n
-
i=1
Pr (Qi|rm, Mi) .
(16.38)
In (16.38), p (rm) is given by (16.26). Expression (16.38) does not have a
standard form; therefore a univariate Metropolis-Hastings algorithm can be
used for drawing samples rm. Let r∗
m denote a candidate value generated by
the candidate generating density u (r∗
m|rm). Then the proposal is accepted
with probability α (r∗
m, rm) given by
α (r∗
m, rm) =
#
min

p(r∗
m|.,data)u(rm|r∗
m)
p(rm|.,data)u(r∗
m|rm), 1

, if p (rm|., data) > 0,
1, otherwise.
(16.39)
The candidate generating density could be a uniform distribution on the
interval (rm −d, rm + d), where d is chosen such that the acceptance rate is
in the range 20% to 50% (Chib and Greenberg, 1995). If a uniform density is
chosen, Un (r∗
m|rm −d, rm + d) = Un (rm|r∗
m −d, r∗
m + d) = 1/2d. Then,
only the ratio p (r∗
m|., data) /p (rm|., data) needs to be computed in (16.39).
This is known as the Metropolis algorithm (Metropolis et al., 1953).
Implementation of the MCMC approach described above generates Mon-
te Carlo samples from the joint posterior distribution (16.27). Speciﬁc pa-
rameters of the model can be inferred from their marginal posterior distri-
bution.

690
16. Segregation and Quantitative Trait Loci Analysis
Model Selection
A Bayesian counterpart of (16.22) is the Bayes factor that was discussed
in Chapter 8. One may wish to compare the model that assumes one QTL
is segregating, labeled M1, versus a model that assumes that no QTL is
segregating, labeled M0. Under M0, the posterior distribution is
p

µ, σ2
e|y, M0

∝p

µ, σ2
e|M0

p

y|µ, σ2
e, M0

.
The Bayes factor of model M1 relative to model M0 requires computation
of
B10 = p (y|M1)
p (y|M0)
=

Q

p (y|Q, θ, M1) Pr (Q|rm, M, M1) p (rm, θ|M1) dθ

p (y|µ, σ2e, M0) p (µ, σ2e|M0) dµdσ2e
.
As discussed in Chapter 8, B10 can be computed using standard MCMC
output with the approach suggested by Newton and Raftery (1994). A
Monte Carlo estimate of B10 is given by
5B10 =
N

j=1
p−1 
y|µ(j), σ2(j)
e
, M0

N

j=1
p−1

y|Q(j), θ(j), M1
 ,
where N is the length of the Monte Carlo chain, µ(j), σ2(j)
e
is the jth draw
from

µ, σ2
e|y,M0

, and Q(j), θ(j) is the jth draw from [Q, θ|y, M1].
Another approach to Bayesian model choice is based on a Monte Carlo
estimate of the posterior probability of the model; this is discussed at the
end of the next section.
As a ﬁnal word of warning, we remind the reader that tests involving
speciﬁc values of continuous parameters (r = 0.5, say), must build on a
prior speciﬁcation which assigns probability mass to that speciﬁc value.
Otherwise, by deﬁnition, the posterior probability that the continuous pa-
rameter takes the speciﬁed value is 0. A point null hypothesis cannot be
tested under a continuous prior distribution. For example, the prior (16.26)
cannot be used for testing a speciﬁc value of the recombination fraction.
16.3.2
Models with an Arbitrary Number of QTL
In this section the model is extended to include an arbitrary number of
QTL. A model indicator M is introduced, which can take values 1, 2, . . . , I,
where I is an integer. Inference is restricted to the Bayesian approach only.
Suppose that (1, 2, . . . , K) ordered markers and phenotypic observations yi,

16.3 QTL Models
691
(i = 1, . . . , nobs) are available from a backcross line. The marker genotype
information for the K loci of individual i is denoted Mi = {Mij}j=1,...,K,
where Mij is the information on the jth marker. The known positions of
the K markers are collected in the vector D = {Dl}l=1,...,K, where Dl is
the genetic map distance between markers 1 and l and D1 = 0.
Suppose that m QTL are present at locations λ = (λ1, . . . , λm), where
D1 < λi < DK. More than one QTL may be present in the region deﬁned
by two ﬂanking markers. Let the matrix
Q = {Qij}i=1,...,nobs,j=1,...,m
represent a genotype conﬁguration where Qij is the QTL genotype at loca-
tion λj for individual i. Let the ith row of Q, Qi = {Qij}j=1,...,m, represent
the QTL genotypes for individual i, and let Qj = {Qij}i=1,...,nobs repre-
sent the jth column of Q with QTL genotypes for the nobs individuals at
location λj.
In the backcross design, conditionally on marker information M and D
and on number and QTL locations, the QTL genotypes of individuals are
independent, so
Pr (Q|m, M, λ, D) =
nobs
-
i=1
Pr (Qi|m, Mi, λ, D) .
(16.40)
Following Sillanp¨a¨a and Arjas (1998), the generic term “object” will
be used for any marker or QTL in the linkage group. Let Gj
i,L and Gj
i,R
represent the genotypes of two ﬂanking objects (marker or QTL) in in-
dividual i, located, respectively, to the left and right of the jth QTL in
individual i. These ﬂanking objects represent a subset of the parameters
one conditions on in the conditional probability distribution of the QTL
genotypes of individual i. In the case of the present backcross design, the
conditional probability distribution of the QTL genotypes of individual i,
Pr (Qi|m, Mi, λ, D), given the QTL locations and the genotypes and loca-
tions of other objects (markers or QTL), can be written as
Pr (Qi|m, Mi, λ, D) =
m
-
j=1
Pr

Qij|Gj
i,L, Gj
i,R, λj−1, λj, λj+1

,
(16.41)
where λj−1 is the map distance between Gj
i,L and D1, λj is the map distance
between QTL genotype Qij and D1, and λj+1 is the map distance between
Gj
i,R and D1. In order to generate the correct joint prior distribution from
(16.41), it is important to choose the objects judiciously. If the object is a
marker, it is chosen among the complete set of markers in the chromosome;
if it is a QTL, it is chosen among the set of QTL whose index is lower than
the one being considered (Sillanp¨a¨a and Arjas, 1998).

692
16. Segregation and Quantitative Trait Loci Analysis
It is also possible to specify the conditional distribution of a particular
QTL genotype across the nobs individuals, given the genotypic conﬁgu-
ration for the remaining QTL genotypes, the QTL locations λ, and the
marker information M. For the present design,
Pr

Qj|Q1, . . . , Qj−1, M, λ, D

=
nobs
-
i=1
Pr

Qij|Gj
i,L, Gj
i,R, λj−1, λj, λj+1

,
(16.42)
which leads to the alternative form for Pr (Q|m, M, λ, D)
Pr (Q|m, M, λ, D) =
m
-
j=1
Pr

Qj|Q1, . . . , Qj−1, M, λ, D

=
m
-
j=1
nobs
-
i=1
Pr

Qij|Gj
i,L, Gj
i,R, λj−1, λj, λj+1

.
(16.43)
In the backcross design, at each of the m loci only two genotypes are pos-
sible, with eﬀects represented by real parameters µj1 or µj2, (j = 1, . . . , m).
The data y = {yi} are assumed to be a realized value from Y, where
Y|m, µ,Q, σ2 ∼N

Xµ, Iσ2
.
(16.44)
In (16.44), µ =

µji

j=1,...,m,i=1,2, X is an incidence matrix associating
QTL eﬀects to the data, I is the identity matrix, and σ2 > 0 is a resid-
ual variance which may include a polygenic contribution from other loci
aﬀecting the trait.
It will be assumed that the prior distribution of the parameters can
be factorized as follows (ignoring the dependence on hyperparameters to
simplify notation)
p

µ, σ2, λ, Q, m|M, D

= Pr (m) p

µ, σ2, λ, Q|m, M, D

= Pr (m) p (µ|m) p

σ2|m

Pr (Q|m, M, λ, D) p (λ|m, D) .
(16.45)
The posterior distribution is given by
p

µ, σ2, λ, Q, m|y, M, D

∝Pr (m) p

µ, σ2, λ, Q|m, M, D

×p

y|m, µ,Q, σ2
.
(16.46)
The prior distribution of the parameters of the model could be speciﬁed
as follows. Given M = m, the locations λ1, . . . , λm are assumed to be in-
dependent and uniformly distributed in the interval ∆= (D1, DK). For m,
the number of QTL, a Poisson distribution with mean α can be posited.
The elements of µ can be assumed to be a priori independently and nor-
mally distributed with mean zero and variance σ2
0. Finally, the residual
variance can be assumed to follow a scaled inverted chi-square distribution
with known parameters ν and S: σ2 ∼νSχ−2
ν .

16.3 QTL Models
693
The posterior distribution (16.46) does not have a ﬁxed dimension be-
cause m varies according to the unknown number of QTL. Reversible jump
MCMC provides a ﬂexible method for drawing samples from such a pos-
terior distribution. The algorithm consists of moves that lead to a change
of dimension and thereby a change of model, increasing or decreasing the
number of QTL, and of updates within models. Updates within a model
are very similar to those discussed in connection with the model assuming
one QTL and two ﬂanking markers and are brieﬂy dealt with ﬁrst.
Fully Conditional Posterior Distributions with Fixed Number of QTL
Updating µ
From (16.45) and (16.46) the fully conditional posterior density of µ is
p (µ|., data) ∝p (µ|m) p

y|m, µ,Q, σ2
∝exp

−µ′µ
2σ2
0

exp

−(y −Xµ)′ (y −Xµ)
2σ2

.
(16.47)
The quadratic term, viewed as a function of µ, can be shown to be propor-
tional to
(y −Xµ)′ (y −Xµ) ∝(µ −5µ)′ X′X (µ −5µ) ,
where
5µ = (X′X)−1 X
′y.
Substituting in (16.47) and combining quadratic forms as indicated in Box
and Tiao (1973), page 418, it can be shown that
p (µ|., data) ∝exp

−(µ −c)′ 
Iσ2 + X′Xσ2
0

(µ −c)
2σ2σ2
0

,
where
c =

Iσ2 + X′Xσ2
0
−1 σ2
0X′X5µ.
Therefore,
µ|., data ∼N

c,

Iσ2 + X′Xσ2
0
−1 σ2
0σ2
.
(16.48)
Elements of µ have the same form as (16.32) and (16.33).
Updating σ2
The fully conditional posterior distribution of σ2 is given by
p

σ2|., data

∝p

σ2
p

y|m, µ,Q, σ2
,
which is in the form of a scaled inverted chi-square distribution
σ2|., data ∼,v ,Sχ−2
v
(16.49)
with ,v = ν + n and ,S =

(y −Xµ)′ (y −Xµ) + νS

/,v .

694
16. Segregation and Quantitative Trait Loci Analysis
Updating Q
The conditional independence of (16.43) can be exploited to update Q,
sampling each genotype separately, one individual at a time. The fully
conditional posterior distribution is
Pr (Q|., data)
∝
m
-
j=1
nobs
-
i=1
Pr

Qij|Gj
i,L, Gj
i,R, λj−1, λj, λj+1

p

yij|m, µj1, µj2

(16.50)
and drawing from it can be performed after appropriate scaling. The form
of (16.50) resembles (16.36) and (16.37).
Updating λ
Elements of λ can be updated one at a time using a Metropolis-Hastings
step. The fully conditional posterior distribution is
p (λj|., data) ∝p (λj|m, D)
m
-
j=1
Pr

Qij|Gj
i,L, Gj
i,R, λj−1, λj, λj+1

.
Following a similar strategy used in (16.39), the proposal λ∗
j could be
generated from a uniform distribution with density Un

λ∗
j|a, b

where
a = max (λj−1, λj −d), b = min (λj+1, λj + d), and d is a tuning parame-
ter. This proposal is accepted with probability
α

λ∗
j, λj

=



min

p(λ∗
j |.,data)Un(λj|a,b)
p(λj|.,data)Un(λ∗
j |a,b), 1

, if p (λj|., data) > 0,
1, otherwise.
The proposal distribution maintains ordering of the loci.
Updates Leading to Changes of the Model
This section derives the acceptance probability for a reversible jump MCMC
algorithm for moves involving a change in the number of QTL. The ap-
proach builds on that in Section 11.7 of Chapter 11, where the acceptance
probability was derived and illustrated for models with continuous param-
eters. The discrete nature of the QTL genotypes calls for some minor mod-
iﬁcations. The material is largely taken from Waagepetersen and Sorensen
(2001). Applications of reversible jump in QTL studies can be found in
Heath (1997), Uimari and Hoeschele (1997), Stephens and Fisch (1998),
Sillanp¨a¨a and Arjas (1998, 1999), George et al. (2000), Lee and Thomas
(2000), and Yi and Xu (2000). In this last part, following the notation used
in Chapter 11, no distinction is made among vectors, matrices, and scalars,
unless otherwise stated.

16.3 QTL Models
695
Suppose that the current state of the Markov chain has M = m QTL.
With probability pm,m it is proposed to update parameters within the cur-
rent model, with probability pm,m−1 it is proposed to decrease the number
of QTL by one, and with probability pm,m+1 it is proposed to increase the
number of QTL by one (if it is proposed to decrease the number of QTL
the chain remains at the current state if m = 0).
Given m, deﬁne
z =

µ11, µ12, λ1, Q1, . . . , µm1, µm2, λm, Qm
.
In dimension changing moves, the residual variance σ2 is not updated; it is
updated in moves within models. Therefore to economize on notation, σ2
is omitted from the formulas in the remaining of this section. Let (m, z)
represent the state of the Markov chain, where z = (z1, . . . , zm) is a vector
of QTL conﬁgurations zj =

µj1, µj2, λj, Qj
(j = 1, . . . , m). Thus, each
QTL conﬁguration zj consists of the QTL eﬀects, together with the asso-
ciated QTL location and nobs genotypes. A QTL conﬁguration belongs in
the space Cconf = R2 × ∆× {0, 1}nobs, where 0 and 1 are labels for geno-
types Qq and QQ. The vector z of the m QTL conﬁgurations belongs in
the space Cm = Cm
conf.
Removal of a QTL
Suppose there are m ≥1 QTL conﬁgurations and that this number is
proposed to be reduced by 1 with probability pm,m−1. The current state of
the Markov chain is Xn = (m, z) where z = (z1, . . . , zm). A move reducing
the number of QTL may be accomplished by deterministically removing the
last (in terms of the position in the vector z, and not in terms of the physical
position of the locus on the chromosome) QTL conﬁguration in z. As in
Section 11.7.2 of Chapter 11, denote the proposal Yn+1 =

Y ind
n+1, Y par
n+1

,
where Y ind
n+1 = m −1 and Y par
n+1 = g1m,m−1 (z1, . . . , zm−1) = (z1, . . . , zm−1),
since g1m,m−1 is the identity mapping. Suppose that Am is a subset of Cm
and that Bm−1 is a subset of Cm−1. The left-hand side of the reversibility
condition (11.67) is
P

Mn = m, Zn ∈Am, Y ind
n+1 = m −1, Y par
n+1 ∈Bm−1
and Yn+1 accepted)
=

∆m

R2m

Q∈{0,1}mnobs
Qa
m,m−1 (z, Bm−1) I (z ∈Am)
×f (m, z|y) dλ dµ,
(16.51)
where
f (m, z|y) ∝Pr (M = m|y) p (µ, λ, Q|m, y)

696
16. Segregation and Quantitative Trait Loci Analysis
and
Qa
m,m−1 (z, Bm−1)
= P

Y ind
n+1 = m −1, Y par
n+1 ∈Bm−1, Yn+1 accepted|Xn = (m, z)

. (16.52)
Since the proposal Yn+1 is generated deterministically, in analogy with
(11.77),
Qa
m,m−1 (z, Bm−1)
=
pm,m−1I ((z1, . . . , zm−1) ∈Bm−1)
×am,m−1 (z, (z1, . . . , zm−1)) .
(16.53)
Substituting in (16.51) yields
pm,m−1

∆m

R2m

Q∈{0,1}mnobs
f (m, z|y)
×I (z ∈Am, (z1, . . . , zm−1) ∈Bm−1) am,m−1 (z, (z1, . . . , zm−1)) dλ dµ.
(16.54)
In these expressions,
Q =

Q1, . . . , Qm
,
λ = (λ1, . . . , λm) ,
and
µ = (µ11, µ12, . . . , µm1, µm2) .
Further, dλ and dµ are shorthand for
dλ1, dλ2, . . . , dλm
and for
dµ11, dµ12, . . . , dµm1, dµm2,
respectively.
Addition of a QTL
Suppose now that there are m −1 QTL and that this number is to be
increased by one with probability pm−1,m. The current state of the Markov
chain is Xn = (m −1, z′) = (m −1, z1, . . . , zm−1). The right-hand side of
the reversibility condition (11.67) is
P

Mn = m −1, Zn ∈Bm−1, Y ind
n+1 = m, Y par
n+1 ∈Am
and Yn+1 accepted)
=

∆m−1

R2(m−1)

Q′∈{0,1}(m−1)nobs
Qa
m−1,m (z′, Am) I (z′ ∈Bm−1)
×f (m −1, z′|y) dλ′ dµ′
(16.55)

16.3 QTL Models
697
where
f (m −1, z′|y) ∝Pr (M = m −1|y) p

µ′, λ′,Q′|m −1, y

,
and
Qa
m−1,m (z′, Am)
= P

Y ind
n+1 = m, Y par
n+1 ∈Am and Yn+1 accepted|Xn = (m −1, z′)

.
(16.56)
In this move, the proposal
Y par
n+1
=
g1m−1,m (z1, . . . , zm−1, (µm1, µm2, λm, Qm))
=
(z1, . . . , zm−1, (µm1, µm2, λm, Qm))
is generated stochastically: one must draw
zm = (µm1, µm2, λm, Qm)
from the proposal density qm−1,m (z′, zm). The elements in zm are placed
immediately after zm−1.
A simple approach to generate zm could be to draw each QTL eﬀect
from µm,i ∼N

0, τ 2
where τ 2 is properly tuned, the location λm from a
uniform distribution between D1 and DK, and the vector of QTL genotypes
from the conditional probability
Pr

Qm|Q′, λm, λ′, D, M, m

=
nobs
-
i=1
Pr

Qim|Gm
i,L, Gm
i,R, λm, λ′
,
where Gm
i,L

Gm
i,R

is the ﬂanking object to the left (right) of λm in individual
i. Therefore, the proposal density is
qm−1,m (z′, zm) = f

µm1|0, τ 2
f

µm2|0, τ 2
1
DK −D1
× Pr

Qm|Q′, λm, λ′, D, M, m

.
(16.57)
In the expressions above,
Q′ =

Q1, . . . , Qm−1
,
λ′ = (λ1, . . . , λm−1) ,
and
µ′ =

µ11, µ12, . . . , µ(m−1)1, µ(m−1)2

.
Due to the stochastic nature of the proposal, (16.56) can be written as
Qa
m−1,m (z′, Am) = pm−1,m

∆

R2

Qm∈{0,1}nobs
I ((z′, zm) ∈Am)
×am−1,m (z′, (z′, zm)) qm−1,m (z′, zm) dλm dµm1 dµm2.
(16.58)

698
16. Segregation and Quantitative Trait Loci Analysis
Substituting in (16.55) leads to the following form for the right hand side
of the reversibility condition (11.67):
pm−1,m

∆m

R2m

Q∈{0,1}mnobs
I ((z′, zm) ∈Am, z′ ∈Bm−1)
×f (m −1, z′|y) am−1,m (z′, (z′, zm))
×qm−1,m (z′, zm) dλm dµm1 dµm2 dλ′ dµ′.
(16.59)
In the expressions above, dλ′ and dµ′ are shorthand for
dλ1, dλ2, . . . , dλm−1
and for
dµ11, dµ12, . . . , dµ(m−1)1, dµ(m−1)2,
respectively.
Derivation of the Acceptance Probability
The dimensions of (16.54) and (16.59) are equal. In terms of the dimension
matching expression (11.60), here, nm = m, nm,m−1 = 0, nm−1 = m −
(2 + 1 + nobs), and nm−1,m = 2 + 1 + nobs, so that
nm + nm,m−1 = m = nm−1 + nm−1,m.
For reversibility to hold, (16.54) must equal (16.59). The equality is satisﬁed
if
pm,m−1f (m, z|y) am,m−1 (z, (z1, . . . , zm−1))
=
pm−1,mf (m −1, z′|y) am−1,m (z′, (z′, zm)) qm−1,m (z′, zm) ,
which leads to the following expression for the acceptance probability:
am,m−1 (z, z′)
= min

1, pm−1,mf (m −1, z′|y) qm−1,m (z′, zm)
pm,m−1f (m, z|y)

.
(16.60)
The acceptance probability (16.60) holds when the dimension changing
moves are based on the strategy of deleting deterministically the last (in
terms of the position in the vector z) QTL and appending the new QTL
in the last position in z. The diligent reader may wish to conﬁrm that the
same acceptance probability is arrived at when the QTL to be deleted is
randomly chosen with probability 1/m among the m existing positions in
z, and the QTL to be added is inserted randomly with probability 1/m
among the m available positions in z.

16.3 QTL Models
699
Model Selection
The above reversible jump algorithm generates samples from the posterior
distribution p (θi|M = i, y) Pr (M = i|y), where θi are parameters of model
i. Models can be compared by means of their posterior probabilities. These
can be estimated from
6
Pr (M = i|y) = 1
N
N

j=1
I (mj = i)
where mj is the jth Monte Carlo sample of a variable that takes a particular
value for each model and N is the number of samples.

This page intentionally left blank

References
Abramowitz, M. and I. A. Stegun (1972). Handbook of Mathematical Func-
tions. Dover Publications.
Agresti, A. (1989). A survey of models for repeated ordered categorical
response data. Statistics in Medicine 8, 1209–1224.
Agresti, A. (1990). Categorical Data Analysis. Wiley.
Agresti, A. (1996). An Introduction to Categorical Data Analysis. Wiley.
Aitken, A. C. (1934).
A note on selection from a multivariate normal
population. Proceedings of the Edinburgh Mathematical Society 4, 106–
110.
Akaike, H. (1973). Information theory as an extension of the maximum
likelihood principle. In B. N. Petrov and F. Csaki (Eds.), Second In-
ternational Symposium on Information Theory, pp. 267–281. Akademiai
Kiado, Budapest.
Albert, J. H. and S. Chib (1993). Bayesian analysis of binary and polychoto-
mous response data. Journal of the American Statistical Association 88,
669–679.
Albert, J. H. and S. Chib (1995). Bayesian residual analysis for binary
response regression models. Biometrika 82, 747–759.
Anderson, D. A. and M. Aitkin (1985). Variance component models with
binary response: Interviewer variability. Journal of the Royal Statistical
Society Series B 47, 203–210.

702
References
Anderson, T. W. (1984). An Introduction to Multivariate Statistical Anal-
ysis. Wiley.
Applebaum, D. (1996). Probability and Information - An Integrated Ap-
proach. Cambridge University Press.
Baldi, P. and S. Brunak (1998). Bioinformatics: The Machine Learning
Approach. MIT Press.
Barndorﬀ-Nielsen, O. E. (1983). On a formula for a distribution of the
maximum likelihood estimator. Biometrika 70, 343–365.
Barndorﬀ-Nielsen, O. E. (1986). Inference on full or partial parameters
based on the standardized log likelihood ratio. Biometrika 73, 307–322.
Barndorﬀ-Nielsen, O. E. (1991).
Likelihood theory.
In D. V. Hinkley,
N. Reid, and E. J. Snell (Eds.), Statistical Theory and Modelling, Chap-
ter 10, pp. 232–264. Chapman and Hall.
Barndorﬀ-Nielsen, O. E. and D. R. Cox (1994). Inference and Asymptotics.
Chapman and Hall.
Barnett, V. (1999). Comparative Statistical Inference. Wiley.
Barnett, V. and T. Lewis (1995). Outliers in Statistical Data. Wiley.
Bates, D. and D. G. Watts (1988). Nonlinear Regression Analysis and its
Applications. Wiley.
Bayarri, M. J. (1981).
Inferencia Bayesiana sobre el coeﬁciente de cor-
relaci´on de una poblaci´on normal bivariante. Trabajos de Estad´ıstica 32,
18–31.
Bayes, T. (1763). An essay towards solving a problem in the doctrine of
chances. Philosophical Transactions of the Royal Society of London 53,
370–418.
Becker, W. A. (1984). Manual of Quantitative Genetics. Academic Enter-
prises.
Berger, J. O. (1985). Statistical Decision Theory and Bayesian Analysis.
Springer–Verlag.
Berger, J. O. and J. M. Bernardo (1992). On the development of reference
priors. In J. M. Bernardo, J. O. Berger, A. P. Dawid, and A. F. M. Smith
(Eds.), Bayesian Statistics 4, pp. 35–60. Oxford University Press.
Berger, J. O. and L. R. Pericchi (1996). The intrinsic Bayes Factor for
model selection and prediction. Journal of the American Statistical As-
sociation 91, 109–122.

References
703
Bernardo, J. M. (1979).
Reference posterior distributions for Bayesian
inference (with discussion). Journal of the Royal Statistical Society Series
B 41, 113–147.
Bernardo, J. M. (2001). Bayesian statistics. Submitted manuscript.
Bernardo, J. M. and A. F. M. Smith (1994). Bayesian Theory. Wiley.
Besag, J. (1974). Spatial interaction and the statistical analysis of lattice
systems (with discussion). Journal of the Royal Statistical Society Series
B 36, 192–326.
Besag, J. (1994). Contribution to the discussion paper by Grenander and
Miller. Journal of the Royal Statistical Society Series B 56, 591–592.
Bibby, J. and H. Toutenburg (1977). Prediction and Improved Estimation
in Linear Models. Wiley.
Blasco, A. (2001). The Bayesian controversy in animal breeding. Journal
of Animal Science 79, 2023–2046.
Blasco, A. and L. Varona (1999). Ajuste y comparaci´on de curvas de crec-
imiento. ITEA 95A, 131–142.
Bliss, C. I. (1935). The calculation of the dosage-mortality curve. Annals
of Applied Biology 22, 134–167.
Box, G. E. P. (1976).
Science and statistics.
Journal of the American
Statistical Association 71, 791–799.
Box, G. E. P. (1980). Sampling and Bayes’ inference in scientiﬁc modelling
and robustness (with discussion). Journal of the Royal Statistical Society
Series A 143, 383–430.
Box, G. E. P. and M. E. Muller (1958). A note on the generation of random
normal deviates. Annals of Mathematical Statistics 29, 610–611.
Box, G. E. P. and G. C. Tiao (1973). Bayesian Inference in Statistical
Analysis. Wiley.
Brooks, S. P. and A. Gelman (1998).
General methods for monitoring
convergence of iterative simulations. Journal of Computer Graphics and
Statistics 7, 434–455.
Brooks, S. P., P. Giudici, and G. O. Roberts (2001). Eﬃcient construction
of reversible jump MCMC proposal distributions. Submitted manuscript.
Brooks, S. P. and G. O. Roberts (1998). Diagnosing convergence of Markov
chain Monte Carlo algorithms. Statistics and Computing 8, 319–335.

704
References
Brown, L. D., T. T. Cai, and A. DasGupta (2001). Interval estimation for
a binomial proportion. Statistical Science 16, 101–133.
Bulmer, M. G. (1971). The eﬀect of selection on genetic variability. Amer-
ican Naturalist 105, 201–211.
Bulmer, M. G. (1979). Principles of Statistics. Dover Publications.
Bulmer, M. G. (1980). The Mathematical Theory of Quantitative Genetics.
Oxford University Press.
Bunke, O. (1975). Minimax linear, ridge and shrunken estimators for linear
parameters. Mathematische Operationsforschung und Statistik 6, 697–
701.
Carlin, B. P. and T. A. Louis (1996). Bayes and Empirical Bayes Methods
for Data Analysis. Chapman and Hall.
Casella, G. and R. L. Berger (1990). Statistical Inference. Brooks–Cole.
Casella, G. and E. I. George (1992). Explaining the Gibbs sampler. The
American Statistician 46, 167–170.
Casella, G., M. Lavine, and C. P. Robert (2001). Explaining the perfect
sampler. The American Statistician 55, 299–305.
Casella, G. and C. P. Robert (1996).
Rao-Blackwellisation of sampling
schemes. Biometrika 83, 81–94.
Chen, M. H., Q. M. Shao, and J. G. Ibrahim (2000). Monte Carlo Methods
in Bayesian Computation. Springer–Verlag.
Chib, S. (1995). Marginal likelihood from the Gibbs output. Journal of the
American Statistical Association 90, 1313–1321.
Chib, S. and E. Greenberg (1995). Understanding the Metropolis-Hastings
algorithm. The American Statistician 49, 327–335.
Chib, S. and I. Jeliazkov (2001). Marginal likelihood from the Metropolis-
Hastings output.
Journal of the American Statistical Association 96,
270–281.
Cohen, M. D. (1986). Pseudo-random number generators. In S. Kotz, N. L.
Johnson, and C. B. Read (Eds.), Encyclopedia of Statistics, Vol. 7, pp.
327–333. Wiley.
Collet, D. (1994). Modelling Survival Data in Medical Research. Chapman
and Hall.
Congdon, P. (2001). Bayesian Statistical Modelling. Wiley.

References
705
Cowles, M. K. (1996). Accelerating Monte Carlo Markov chain convergence
for cumulative-link generalized linear models.
Statistics and Comput-
ing 6, 101–111.
Cowles, M. K. and B. P. Carlin (1996). Markov chain Monte Carlo con-
vergence diagnostics: A comparative review. Journal of the American
Statistical Association 91, 883–904.
Cox, D. R. (1961). Tests of separate families of hypotheses. Proceedings of
the 4th Berkeley Symposium 1, 105–123.
Cox, D. R. (1962). Further results on tests of separate families of hypothe-
ses. Journal of the Royal Statistical Society Series B 24, 406–424.
Cox, D. R. and D. V. Hinkley (1974). Theoretical Statistics. Chapman and
Hall.
Cox, D. R. and H. D. Miller (1965). The Theory of Stochastic Processes.
Chapman and Hall.
Cox, D. R. and N. Reid (1987). Parameter orthogonality and approximate
conditional inference (with discussion). Journal of the Royal Statistical
Society Series B 49, 1–39.
Cox, D. R. and E. J. Snell (1989). Analysis of Binary Data. Chapman and
Hall.
Crow, J. F. and M. Kimura (1970). An Introduction to Population Genetics
Theory. Harper and Row.
Curnow, R. N. (1961).
The estimation of repeatability and heritability
from records subject to culling. Biometrics 17, 553–566.
Curnow, R. N. (1972).
The multifactorial model for the inheritance of
liability to disease and its implications for relatives at risk. Biometrics 28,
931–946.
Curnow, R. N. and C. Smith (1975).
Multifactorial models for familial
diseases in man. Journal of the Royal Statistical Society Series A 138,
131–169.
Dahlquist, ˚A, B. and ˚A. Bj¨orck (1974). Numerical Methods. Prentice -Hall.
De Finetti, B. (1975a). Theory of Probability, Vol. 1. Wiley.
De Finetti, B. (1975b). Theory of Probability, Vol. 2. Wiley.
Dempster, A. P. (1974). The direct use of likelihood for signiﬁcance testing.
In O. E. Barndorﬀ-Nielsen, P. Blæsild, and G. Schou (Eds.), Proceedings
of the Conference on the Foundational Questions in Statistical Inference,
pp. 335–352. Department of Theoretical Statistics, University of Aarhus.

706
References
Dempster, A. P. (1997). The direct use of likelihood for signiﬁcance testing.
Statistics and Computing 7, 247–252.
Dempster, A. P., N. M. Laird, and D. B. Rubin (1977). Maximum likelihood
from incomplete data via de EM algorithm (with discussion). Journal of
the Royal Statistical Society Series B 39, 1–38.
Dempster, E. R. and I. M. Lerner (1950). Heritability of threshold charac-
ters. Genetics 35, 212–236.
Dennis, J. E. and R. B. Schnabel (1983). Numerical Methods for Uncon-
strained Optimization and Nonlinear Equations. Prentice–Hall.
Devroye, L. (1986). Non-Uniform Random Variate Generation. Springer–
Verlag.
Dickey, J. M. (1968).
Three multidimensional integral identities with
Bayesian applications. Annals of Statistics 39, 1615–1627.
Doerge, R. W. and G. A. Churchill (1996). Permutation tests for multiple
loci aﬀecting a quantitative character. Genetics 142, 285–294.
Draper, N. R. and H. Smith (1981). Applied Regression Analysis. Wiley.
Ducrocq, V., R. L. Quaas, E. Pollak, and G. Casella (1988). Length of
productive life of dairy cows. 2. Variance component estimation and sire
evaluation. Journal of Dairy Science 71, 3071–3079.
Durbin, R., S. R. Eddy, A. Krogh, and G. J. Mitchison (1998). Biological
Sequence Analysis: Probabilistic Models of Proteins and Nucleic Acids.
Cambridge University Press.
Earman, J. (1992). Bayes or Bust. The MIT Press.
Edwards, A. W. F. (1974). The history of likelihood. International Statis-
tical Review 42, 9–15.
Edwards, A. W. F. (1992). Likelihood. The John Hopkins University Press.
Efron, B. (1993). Bayes and likelihood calculations from conﬁdence inter-
vals. Biometrika 80, 3–26.
Efron, B. (1998). R. A. Fisher in the 21st century. Statistical Science 13,
95–122.
Efron, B. and D. V. Hinkley (1978). Assessing the accuracy of the maxi-
mum likelihood estimator: Observed versus expected Fisher information.
Biometrika 65, 457–482.
Elston, R. C. and J. Stewart (1971).
A general model for the genetic
analysis of pedigree data. Human Heredity 21, 523–542.

References
707
Fahrmeir, L. and G. Tutz (2001). Multivariate Statistical Modelling Based
on Generalized Linear Models. Springer–Verlag.
Falconer, D. S. (1965).
The inheritance of liability to certain diseases,
estimated from the incidence among relatives. Annals of Human Genet-
ics 29, 51–76.
Falconer, D. S. (1967). The inheritance of liability to diseases with variable
age of onset, with particular reference to diabetes mellitus. Annals of
Human Genetics 31, 1–20.
Falconer, D. S. and T. F. C. Mackay (1996). Introduction to Quantitative
Genetics. Longman.
Famula, T. R. (1981). Exponential stayability model with censoring and
covariates. Journal of Dairy Science 64, 538–545.
Fan, J., H. Hung, and W. Wong (2000). Geometric understanding of likeli-
hood ratio statistics. Journal of the American Statistical Association 95,
836–841.
Feller, W. (1970). An Introduction to Probability Theory and its Applica-
tions, Vol. 1. Wiley.
Feng, Z. D. and C. E. McCulloch (1996). Using bootstrap likelihood ratios
in ﬁnite mixture models. Journal of the Royal Statistical Society Series
B 58, 609–617.
Fernandez, C. and M. F. J. Steel (1998). On Bayesian modelling of fat
tails and skewness. Journal of the American Statistical Association 93,
359–371.
Fisher, R. A. (1918). The correlation between relatives on the supposition
of Mendelian inheritance. Transactions of the Royal Society of Edin-
burgh 52, 399–433.
Fisher, R. A. (1920). A mathematical examination of determining accuracy
of an observation by the mean error, and by the mean square error.
Monthly Notices of the Royal Astronomical Society 80, 758–770.
Fisher, R. A. (1922). On the mathematical foundations of theoretical statis-
tics. Philosophical Transactions of the Royal Society of London Series
A 222, 309–368.
Fisher, R. A. (1925). Theory of statistical information. Proceedings of the
Cambridge Philosophical Society 22, 700–725.
Fishman, G. S. (1973). Concepts and Methods in Discrete Event Digital
Simulation. Wiley.

708
References
Flury, B. and A. Zoppe (2000). Exercises in EM. The American Statisti-
cian 54, 207–209.
Foulley, J. L., D. Gianola, and R. Thompson (1983). Prediction of genetic
merit from data on categorical and quantitative variates with an appli-
cation to calving diﬃculty, birth weight and pelvic opening. Genetics,
Selection, Evolution 25, 407–424.
Foulley, J. L., S. Im, D. Gianola, and I. Hoeschele (1987). Empirical Bayes
estimation of parameters for n polygenic binary traits. Genetics, Selec-
tion, Evolution 19, 197–224.
Foulley, J. L. and E. Manfredi (1991).
Approches statistiques de
l’´evaluation g´en´etiques des reproducteurs pour des caract`eres binaires
`a seuils. Genetics, Selection, Evolution 23, 309–338.
Fox, C. (1987). An Introduction to Calculus of Variations. Dover.
Galton, F. (1885). Regression towards mediocrity in hereditary stature.
Journal of the Anthropological Institute 15, 246–263.
Garc´ıa-Cort´es, L. A. and D. Sorensen (1996). On a multivariate implemen-
tation of the Gibbs sampler. Genetics, Selection, Evolution 28, 121–126.
Geisser, S. (1993). Predictive Inference: An Introduction. Chapman and
Hall.
Geisser, S. and W. F. Eddy (1979). A predictive approach to model selec-
tion. Journal of the American Statistical Association 74, 153–160.
Gelfand, A. E. (1996). Model determination using sampling-based methods.
In W. R. Gilks, S. Richardson, and D. J. Spiegelhalter (Eds.), Markov
Chain Monte Carlo in Practice, pp. 145–161. Chapman and Hall.
Gelfand, A. E. and D. Dey (1994). Bayesian model choice: Asymptotics
and exact calculations. Journal of the Royal Statistical Society Series
B 56, 501–514.
Gelfand, A. E., D. K. Dey, and H. Chang (1992). Model determination using
predictive distributions with implementation via sampling-based meth-
ods. In J. M. Bernardo, J. O. Berger, A. P. Dawid, and A. F. M. Smith
(Eds.), Bayesian Statistics 4, pp. 147–167. Oxford University Press.
Gelfand, A. E., S. E. Hills, A. Racine-Poon, and A. F. M. Smith (1990).
Illustration of Bayesian inference in normal data models using Gibbs
sampling. Journal of the American Statistical Association 85, 972–985.
Gelfand, A. E., S. K. Sahu, and B. P. Carlin (1995). Eﬃcient parameteri-
zation for normal linear mixed models. Biometrika 82, 479–488.

References
709
Gelfand, A. E., S. K. Sahu, and B. P. Carlin (1996). Eﬃcient parameter-
izations for generalized linear mixed models. In J. M. Bernardo, J. O.
Berger, A. P. Dawid, and A. F. M. Smith (Eds.), Bayesian Statistics 5,
pp. 165–180. Oxford University Press.
Gelfand, A. E. and A. F. M. Smith (1990). Sampling based approaches
to calculating marginal densities. Journal of the American Statistical
Association 85, 398–409.
Gelman, A., J. B. Carlin, H. S. Stern, and D. B. Rubin (1995). Bayesian
Data Analysis. Chapman and Hall.
Gelman, A., X. L. Meng, and H. Stern (1996). Posterior predictive as-
sessment of model ﬁtness via realized discrepancies (with discussion).
Statistica Sinica 6, 733–807.
Gelman, A. and D. B. Rubin (1992). Inference from iterative simulation
using multiple sequences. Statistical Science 7, 457–511.
Geman, S. and D. Geman (1984). Stochastic relaxation, Gibbs distribu-
tions, and the Bayesian restoration of images. IEEE Transactions on
Pattern Analysis and Machine Intelligence 6, 721–741.
George, A. W., K. L. Mengersen, and G. P. Davis (2000). Localization of a
quantitative trait locus via a Bayesian approach. Biometrics 56, 40–51.
Geweke, J. (1989). Bayesian inference in econometric models using Monte
Carlo integration. Econometrica 57, 1317–1339.
Geweke, J. (1993). Bayesian treatment of the independent Student-t linear
model. Journal of Applied Econometrics 8, S19–S40.
Geyer, C. J. (1992). Practical Markov chain Monte Carlo. Statistical Sci-
ence 7, 473–511.
Gianola, D. (1982). Theory and analysis of threshold characters. Journal
of Animal Science 54, 1079–1096.
Gianola, D. and R. L. Fernando (1986). Bayesian methods in animal breed-
ing theory. Journal of Animal Science 63, 217–244.
Gianola, D., R. L. Fernando, S. Im, and J. L. Foulley (1989). Likelihood es-
timation of quantitative genetic parameters when selection occurs: Mod-
els and problems. Genome 31, 768–777.
Gianola, D. and J. L. Foulley (1983). Sire evaluation for ordered categorical
data with a threshold model. Genetics, Selection, Evolution 15, 201–223.

710
References
Gianola, D., S. Im, and F. W. Macedo (1990). A framework for prediction
of breeding values. In D. Gianola and K. Hammond (Eds.), Statistical
Methods for Genetic Improvement of Livestock, pp. 210–238. Springer–
Verlag.
Gilks, W. R. and G. O. Roberts (1996). Strategies for improving MCMC.
In W. R. Gilks, S. Richardson, and D. J. Spiegelhalter (Eds.), Markov
Chain Monte Carlo in Practice, pp. 89–114. Chapman and Hall.
Gilks, W. R. and P. Wild (1992). Adaptive rejection sampling for Gibbs
sampling. Applied Statistics 41, 336–348.
Gilmour, A. R., R. D. Anderson, and A. L. Rae (1985). The analysis of
binomial data by a generalized linear mixed model. Biometrika 72, 593–
599.
Go, R. C. P., R. C. Elston, and E. B. Kaplan (1978). Eﬃciency and ro-
bustness of pedigree segregation analysis. American Journal of Human
Genetics 30, 28–37.
Good, I. J. (1952). Rational decisions. Journal of the Royal Statistical
Society Series B 14, 107–114.
Good, I. J. (1958). Signiﬁcance tests in parallel and in series. Journal of
the American Statistical Association 53, 799–813.
Goodman, L. A. and H. O. Hartley (1958). The precision of unbiased ratio-
type estimators.
Journal of the American Statistical Association 53,
491–508.
Green, P. (1995).
Reversible jump MCMC computation and Bayesian
model determination. Biometrika 82, 711–732.
Grimmet, G. R. and D. R. Stirzaker (1992). Probability and Random Pro-
cesses. Clarendon Press.
Gross, A. J. and V. A. Clark (1975). Survival Distributions: Reliability
Applications in the Biomedical Sciences. Wiley.
Grossman, S. I. and J. E. Turner (1974). Mathematics for the Biological
Sciences. Macmillan.
Guo, S. W. and E. A. Thompson (1994). Monte Carlo estimation of mixed
models for large complex pedigrees. Biometrics 50, 417–432.
Hacking, I. (1965). Logic of Statistical Inference. Cambridge University
Press.
Hager, W. H. (1988). Applied Numerical Linear Algebra. Prentice–Hall.

References
711
Haldane, J. B. S. (1919). The combination of linkage values and the calcula-
tion of distances between the loci of linked factors. Journal of Genetics 8,
229–309.
Haldane, J. B. S. (1948). The precision of observed values of small frequen-
cies. Biometrika 35, 297–303.
Hammersley, J. M. and D. C. Handscomb (1964). Monte Carlo Methods.
Wiley.
Hampel, F. R., E. M. Ronchetti, P. J. Rousseeuw, and W. A. Stahel (1986).
Robust Statistics. Wiley.
Han, C. and B. P. Carlin (2001).
Markov chain Monte Carlo methods
for computing Bayes Factors: A comparative review.
Journal of the
American Statistical Association 96, 1122–1132.
Harville, D. A. (1974). Bayesian inference of variance components using
only error contrasts. Biometrika 61, 383–385.
Harville, D. A. (1977). Maximum likelihood approaches to variance com-
ponent estimation and to related problems. Journal of the American
Statistical Association 72, 320–340.
Harville, D. A. and R. W. Mee (1984).
A mixed model procedure for
analyzing ordered categorical data. Biometrics 40, 393–408.
Hastings, W. K. (1970).
Monte Carlo sampling methods using Markov
chains and their application. Biometrika 57, 97–109.
Hazel, L. N. (1943). The genetic basis for constructing selection indices.
Genetics 28, 476–490.
Heath, S. C. (1997). Markov chain Monte Carlo segregation and linkage
analysis for oligogenic models. American Journal of Human Genetics 61,
748–760.
Heisenberg, W. (1958).
The representation of nature in contemporary
physics. Daedalus 87, 95–108.
Henderson, C. R. (1953). Estimation of variance and covariance compo-
nents. Biometrics 9, 226–252.
Henderson, C. R. (1963). Selection index and expected selection advance.
In W. D. Hanson and H. F. Robinson (Eds.), Statistical Genetics and
Plant Breeding, pp. 141–163. National Academy of Sciences, National
Research Council Publication No. 982, Washington, D C.

712
References
Henderson, C. R. (1973). Sire evaluation and genetic trends. In Proceedings
of the Animal Breeding and Genetics Symposium in Honor of Dr. J. L.
Lush, pp. 10–41. American Society of Animal Science.
Henderson, C. R. (1975). Best linear unbiased estimation and prediction
under a selection model. Biometrics 31, 423–447.
Henderson, C. R. (1984). Applications of Linear Models in Animal Breed-
ing. University of Guelph.
Henderson, C. R., O. Kempthorne, S. R. Searle, and C. N. Von Krosigk
(1959).
Estimation of environmental and genetic trends from records
subject to culling. Biometrics 15, 192–218.
Henderson, M. and M. C. Meyer (2001). Exploring the conﬁdence interval
for a binomial parameter in a ﬁrst course in statistical computing. The
American Statistician 55, 337–344.
Heringstad, B., R. Rekaya, D. Gianola, G. Klemetsdal, and K. A. Weigel
(2001). Bayesian analysis of liability of clinical mastitis in Norwegian
cattle with a threshold model: Eﬀects of data sampling and model spec-
iﬁcation. Journal of Dairy Science 84, 2337–2346.
Hills, S. E. and A. F. M. Smith (1992). Parameterization issues in Bayesian
inference (with discussion). In J. M. Bernardo, J. O. Berger, A. P. Dawid,
and A. F. M. Smith (Eds.), Bayesian Statistics 4, pp. 227–246. Oxford
University Press.
Hills, S. E. and A. F. M. Smith (1993).
Diagnostic plots for improved
parameterization in Bayesian inference. Biometrika 80, 61–74.
Hobert, J. P. and G. Casella (1996).
The eﬀect of improper priors on
Gibbs sampling in hierarchical linear models. Journal of the American
Statistical Association 91, 1461–1473.
Hoel, P. G., S. C. Port, and C. J. Stone (1971). Introduction to Probability
Theory. Houghton Miﬄin.
Hoerl, A. E. and R. W. Kennard (1970). Ridge regression. Technomet-
rics 12, 55–67; 69–82.
Hoeschele, I. and B. Tier (1995). Estimation of variance components of
threshold characters by marginal posterior modes and means via Gibbs
sampling. Genetics, Selection, Evolution 27, 519–540.
Hoeting, J. A., D. Madigan, A. E. Raftery, and C. T. Volinsky (1999).
Bayesian model averaging: A tutorial. Statistical Science 14, 382–417.
Hogg, R. V. and A. T. Craig (1995). Introduction to Mathematical Statis-
tics. Prentice Hall.

References
713
Howson, C. and P. Urbach (1989).
Scientiﬁc Reasoning: The Bayesian
Approach. Open Court, La Salle.
Jamrozik, J. and L. R. Schaeﬀer (1997). Estimates of genetic parameters for
a test-day model with random regressions for production of ﬁrst lactation
Holsteins. Journal of Dairy Science 80, 762–770.
Janss, L. L. G., R. Thompson, and J. A. M. Van Arendonk (1995). Appli-
cation of Gibbs sampling for inference in a mixed major gene-polygenic
inheritance model in animal populations. Theoretical and Applied Ge-
netics 91, 1137–1147.
Jaynes, E. T. (1957).
On the rationale of maximum-entropy methods.
Physics Review 106, 620–630.
Jaynes, E. T. (1994).
Probability Theory: The Logic of Science.
http://omega.albany.edu:8008/JaynesBook.
Jeﬀreys, H. (1961). Theory of Probability. Clarendon Press.
Jensen, C. S., U. Kjærulﬀ, and A. Kong (1995). Blocking Gibbs sampling in
very large probabilistic expert systems. International Journal of Human
Computer Studies 42, 647–666.
Jensen, J. (1994). Bayesian analysis of bivariate mixed models with one
continuous and one binary trait using the Gibbs sampler. In Proceedings
of the 5th World Congress of Genetics Applied to Livestock Production,
Vol. 18, pp. 333–336. University of Guelph.
Jensen, J., C. S. Wang, D. Sorensen, and D. Gianola (1994).
Bayesian
inference on variance and covariance components for traits inﬂuenced
by maternal and direct genetic eﬀects using the Gibbs sampler. Acta
Agricultura Scandinavica 44, 193–201.
Johnson, N. L. and S. Kotz (1969). Distributions in Statistics: Discrete
Distributions. Wiley.
Johnson, N. L. and S. Kotz (1970a). Distributions in Statistics: Continuous
Univariate Distributions, Vol. 1. Wiley.
Johnson, N. L. and S. Kotz (1970b). Distributions in Statistics: Continuous
Univariate Distributions, Vol. 2. Wiley.
Johnson, N. L. and S. Kotz (1972). Distributions in Statistics: Continuous
Multivariate Distributions. John Wiley.
Kackar, R. N. and D. A. Harville (1981). Ubiasedness of two-stage estima-
tion and prediction procedures for mixed linear models. Communications
in Statistics Series A: Theory and Methods 10, 1249–1261.

714
References
Kadarmideen, H. N., R. Rekaya, and D. Gianola (2002). Genetic parame-
ters for clinical mastitis in Holstein Freisians in the United Kingdom: A
Bayesian analysis. Animal Science. In press.
Kalbﬂeisch, J. D. and D. A. Sprott (1970). Application of likelihood meth-
ods to models involving large numbers of parameters (with discussion).
Journal of the Royal Statistical Society Series B 32, 175–208.
Kalbﬂeisch, J. D. and D. A. Sprott (1973). Marginal and conditional like-
lihoods. Sankya A 35, 311–328.
Kaplan, W. (1993). Advanced Calculus. Addison and Wesley.
Karlin, S. and H. M. Taylor (1975). A First Course in Stochastic Processes.
Academic Press.
Kass, E. R. and A. E. Raftery (1995). Bayes factors. Journal of the Amer-
ican Statistical Association 90, 773–795.
Kass, R. E. (1995). A reference Bayesian test for nested hypotheses and its
relationship to the Schwarz criterion. Journal of the American Statistical
Association 90, 928–934.
Kass, R. E., B. P. Carlin, A. Gelman, and R. M. Neal (1998). Markov
chain Monte Carlo in practice: A roundtable discussion. The American
Statistician 52, 93–100.
Keller, E. F. (2000). The Century of the Gene. Harvard University Press.
King, G. (1989). Unifying Political Methodology. The Likelihood Theory of
Statistical Inference. Cambridge University Press.
Kirkpatrick, M., W. G. Hill, and R. Thompson (1994).
Estimating the
covariance structure of traits during growth and ageing, illustrated with
lactation in dairy cattle. Genetical Research 64, 57–69.
Kleinbaum, D. G. (1996). Survival Analysis. Springer–Verlag.
Knott, S. A. and C. S. Haley (1992). Aspects of maximum likelihood meth-
ods for the mapping of quantitative trait loci in line crosses. Genetical
Research 60, 139–151.
Koerkhuis, A. N. M. and R. Thompson (1997). Models to estimate maternal
eﬀects for juvenile body weights in broiler chickens. Genetics, Selection,
Evolution 29, 225–249.
Korsgaard, I. R., A. H. Andersen, and D. Sorensen (1999). A useful repa-
rameterisation to obtain samples from conditional inverse Wishart dis-
tributions. Genetics, Selection, Evolution 31, 177–181.

References
715
Korsgaard, I. R., M. S. Lund, D. Sorensen, D. Gianola, P. Madsen, and
J. Jensen (2002). Multivariate Bayesian analysis of Gaussian, right cen-
sored Gaussian, ordered categorical and binary traits using Gibbs sam-
pling. Genetics, Selection, Evolution. In press.
Kullback, S. (1968). Information Theory and Statistics. Wiley.
Laird, N. M. and J. H. Ware (1982). Random-eﬀects models for longitudinal
data. Biometrics 38, 963–974.
Lange, K. (1995). A Quasi-Newton acceleration of the EM algorithm. Jour-
nal of the Royal Statistical Society Series B 44, 226–233.
Lange, K. (1997). Mathematical and Statistical Methods for Genetic Anal-
ysis. Springer–Verlag.
Lange, K. and R. C. Elston (1975).
Extensions to pedigree analysis. I.
Likelihood calculations for simple and complex pedigrees. Human Hered-
ity 25, 95–105.
Lange, K. and T. M. Goradia (1987). An algorithm for automatic genotype
elimination. American Journal of Human Genetics 40, 250–256.
Lange, K. and J. S. Sinsheimer (1993). Normal/independent distributions
and their applications in robust regression. Journal of Computer Graph-
ics and Statistics 2, 175–198.
Lee, J. K. and D. C. Thomas (2000). Performance of Markov chain Monte
Carlo approaches for mapping genes in oligogenic models with unknown
number of loci. American Journal of Human Genetics 67, 1232–1250.
Lee, P. M. (1989). Bayesian Statistics: An Introduction. Edward Arnold.
Lee, Y. and J. A. Nelder (1996). Hierarchical generalized linear models
(with discussion). Journal of the Royal Statistical Society Series B 58,
619–678.
Lehmann, E. L. (1999).
Elements of Large-Sample Theory.
Springer–
Verlag.
Lehmann, E. L. and G. Casella (1998).
Theory of Point Estimation.
Springer–Verlag.
Leonard, T. and J. S. Hsu (1999). Bayesian Methods. Cambridge University
Press.
Lindley, D. V. (1956). On a measure of information provided by an exper-
iment. Annals of Mathematical Statistics 27, 986–1005.
Lindley, D. V. (1957). A statistical paradox. Biometrika 44, 187–192.

716
References
Lindley, D. V. and A. F. M. Smith (1972). Bayesian estimates for the linear
model. Journal of the Royal Statistical Society Series B 34, 1–41.
Little, R. J. A. and D. B. Rubin (1987). Statistical Analysis with Missing
Data. Wiley.
Liu, C. and D. B. Rubin (1995). ML estimation of the t distribution using
EM and its extensions, ECM, and ECME. Statistica Sinica 5, 19–39.
Liu, J. S. (1994). The collapsed Gibbs sampler in Bayesian computations
with applications to a gene-regulation problem. Journal of the American
Statistical Association 89, 958–966.
Liu, J. S. (2001). Monte Carlo Strategies in Scientiﬁc Computing. Springer–
Verlag.
Liu, J. S., H. W. Wong, and A. Kong (1994). Covariance structure of the
Gibbs sampler with applications to the comparisons of estimators and
augmentation schemes. Biometrika 81, 27–40.
Lo, Y., N. R. Mendell, and D. B. Rubin (2001). Testing the number of
components in a normal mixture. Biometrika 88, 767–778.
Louis, T. A. (1982). Finding the observed information matrix when using
the EM algorithm. Journal of the Royal Statistical Society Series B 44,
226–233.
Lund, M. S. and C. S. Jensen (1999). Blocking Gibbs sampling in the mixed
inheritance model using graph theory. Genetics, Selection, Evolution 31,
3–24.
Lynch, M. and B. Walsh (1998). Genetics and Analysis of Quantitative
Traits. Sinauer Associates.
MacCluer, J. W., J. L. Vandeburg, B. Read, and O. A. Ryder (1986).
Pedigree analysis by computer simulation. Zoo Biology 5, 147–160.
Madigan, D. and A. E. Raftery (1994). Model selection and accounting for
model uncertainty in graphical models using Occam’s window. Journal
of the American Statistical Association 89, 1535–1546.
Mal´ecot, G. (1947). Annotated translation by D. Gianola of: Les criteres
statistiques et la subjectivite de la connaisance scientiﬁque (Statistical
methods and the subjective basis of scientiﬁc knowledge), by G. Mal´ecot,
(1947), Annales de l’Universite de Lyon, X, 43-74. Genetics, Selection,
Evolution 31, 269–298.
Mal´ecot, G. (1969). The Mathematics of Heredity. W. H. Freeman. Origi-
nally published in 1948 by Masson et Cie.

References
717
Mardia, K. V., J. T. Kent, and J. M. Bibby (1979). Multivariate Analysis.
Academic Press.
Marsaglia, G. and A. Zaman (1993). The Kiss Generator. Technical Report,
Department of Statistics, University of Florida.
Martinez, V., L. B¨unger, and W. G. Hill (2000). Analysis of response to 20
generations of selection for body composition in mice: Fit to inﬁnitesimal
model. Genetics, Selection, Evolution 32, 3–21.
McCullagh, P. and J. A. Nelder (1989). Generalized Linear Models. Chap-
man and Hall.
McCulloch, C. E. (1994). Maximum likelihood variance components estima-
tion for binary data. Journal of the American Statistical Association 89,
330–335.
McCulloch, R. E. and P. E. Rossi (1991). A Bayesian approach to testing
the arbitrage pricing theory. Journal of Econometrics 49, 141–168.
McLachlan, G. J. and T. Krishnan (1997). The EM Algorithm and Exten-
sions. Wiley.
Meeker, W. Q. and L. A. Escobar (1995). Teaching about approximate con-
ﬁdence regions based on maximum likelihood estimation. The American
Statistician 49, 48–53.
Meilijson, I. (1989). A fast improvement to the EM algorithm on its own
terms. Journal of the Royal Statistical Society Series B 51, 127–138.
Meng, X. and D. B. Rubin (1991). Using EM to obtain asymptotic variance-
covariance matrices: The SEM algorithm. Journal of the American Sta-
tistical Association 86, 899–909.
Mengersen, K. L., C. P. Robert, and C. Guihenneuc-Jouyaux (1999).
MCMC convergence diagnostics: A reviewww (with discussion).
In
J. M. Bernardo, J. O. Berger, A. P. Dawid, and A. F. M. Smith (Eds.),
Bayesian Statistics 6, pp. 415–440. Oxford University Press.
Metropolis, N., A. W. Rosenbluth, M. N. Rosenbluth, A. H. Teller, and
E. Teller (1953). Equations of state calculations by fast computing ma-
chines. Journal of Chemical Physics 21, 1087–1092.
Meyer, K. (1999). Estimates of genetic and phenotypic covariance functions
for post-weaning growth and mature weight of beef cows. Journal of
Animal Breeding and Genetics 116, 181–205.
Meyn, S. P. and R. L. Tweedie (1993).
Markov Chains and Stochastic
Stability. Springer–Verlag.

718
References
Milliken, G. A. and D. E. Johnson (1992). Analysis of Messy Data, Vol. I:
Designed Experiments. Chapman and Hall.
Misztal, I., D. Gianola, and J. L. Foulley (1989). Computing aspects of
nonlinear methods of sire evaluation for categorical data.
Journal of
Dairy Science 72, 1557–1568.
Mood, A. M., F. A. Graybill, and D. C. Boes (1974). Introduction to the
Theory of Statistics. McGraw-Hill.
Moreno, C., D. Sorensen, L. A. Garc´ıa-Cort´es, L. Varona, and J. Altarriba
(1997). On biased inferences about variance components in the binary
threshold model. Genetics, Selection, Evolution 29, 145–160.
Morton, N. E. and C. J. MacLean (1974). Analysis of family resemblance.
III. Complex segregation of quantitative traits.
American Journal of
Human Genetics 26, 489–503.
Nandram, B. and M. H. Chen (1996). Reparameterizing the generalized lin-
ear model to accelerate Gibbs sampler convergence. Journal of Statistical
Computation and Simulation 54, 129–144.
Nelder, J. A. and R. W. M. Wedderburn (1972). Generalized linear models.
Journal of the Royal Statistical Society Series A 135, 370–384.
Newton, M. A. and A. E. Raftery (1994). Approximate Bayesian inference
by the weighted likelihood bootstrap (with discussion). Journal of the
Royal Statistical Society Series B 56, 1–48.
Neyman, J. and E. S. Pearson (1928). On the use and interpretation of
certain test criteria for purposes of statistical inference. Parts I and II.
Biometrika 20A, 175–240; 263–294.
Norris, J. R. (1997). Markov Chains. Cambridge University Press.
Oakes, D. (1999). Direct calculation of the information matrix via the EM
algorithm. Journal of the Royal Statistical Society Series B 61, 479–482.
Odell, P. L. and A. H. Feiveson (1966). A numerical procedure to gen-
erate a sample covariance matrix. Journal of the American Statistical
Association 61, 198–203.
O’Hagan, A. (1994).
Kendall’s Advanced Theory of Statistics, Vol. 2B:
Bayesian Inference. Edward Arnold.
Ott, J. (1999). Analysis of Human Genetic Linkage. John Hopkins Uni-
versity Press.
Patterson, H. D. and R. Thompson (1971). Recovery of inter-block infor-
mation when block sizes are unequal. Biometrika 58, 545–554.

References
719
Pauler, D. K., J. C. Wakeﬁeld, and R. E. Kass (1999). Bayes factors and
approximations for variance component models. Journal of the American
Statistical Association 94, 1242–1253.
Pawitan, Y. (2000).
A reminder of the fallibility of the Wald statistic:
Likelihood explanation. The American Statistician 54, 54–56.
Pearson, K. (1900). Mathematical contributions to the theory of evolution.
VIII. On the inheritance of characters not capable of exact quantitative
measurement. Philosophical Transactions of the Royal Society of London
Series A 195, 79–121.
Pearson, K. (1903). Mathematical contributions to the theory of evolution.
XI. On the inﬂuence of natural selection on the variability and correlation
of organs. Philosophical Transactions of the Royal Society of London
Series A 200, 1–66.
Peskun, P. H. (1973).
Optimum Monte Carlo sampling using Markov
chains. Biometrika 60, 607–612.
Popper, K. R. (1972). The Logic of Scientiﬁc Discovery. Hutchinson.
Popper, K. R. (1982). Quantum Theory and the Schism in Physics. Rout-
ledge.
Priestley, M. B. (1981).
Spectral Analysis and Time Series.
Academic
Press.
Propp, J. G. and D. B. Wilson (1996). Exact sampling with coupled Markov
chains and applications to statistical mechanics. Random Structures and
Algorithms 9, 223–252.
Raftery, A. E., D. Madigan, and J. A. Hoeting (1997). Model selection and
accounting for model uncertainty in linear regression models. Journal of
the American Statistical Association 92, 179–191.
Raj, D. (1968). Sampling Theory. McGraw-Hill.
Rao, C. R. (1947). Large sample tests of statistical hypotheses concerning
several parameters with applications to problems of estimation. Proceed-
ings of the Cambridge Philosophical Society 44, 50–57.
Rao, C. R. (1973). Linear Statistical Inference and its Applications. Wiley.
Reid, N. (1995). The roles of conditioning in inference. Statistical Sci-
ence 10, 138–199.
Reid, N. (2000). Likelihood. Journal of the American Statistical Associa-
tion 95, 1335–1340.

720
References
Richardson, S. and P. Green (1997).
On Bayesian analysis of mixtures
with an unknown number of components (with discussion). Journal of
the Royal Statistical Society Series B 59, 731–792.
Ripley, B. (1987). Stochastic Simulation. Wiley.
Robert, C. P. (1994). The Bayesian Choice. Springer–Verlag.
Robert, C. P. (1998). Discretization and MCMC Convergence Assessment.
Lecture Notes in Statistics, Vol. 135. Springer–Verlag.
Robert, C. P. and G. Casella (1999).
Monte Carlo Statistical Methods.
Springer–Verlag.
Roberts, G. O., A. Gelman, and W. R. Gilks (1997). Weak convergence
and optimal scaling of random walk Metropolis algorithms. Annals of
Applied Probability 7, 110–120.
Roberts, G. O. and S. K. Sahu (1997).
Updating schemes, correlation
structure, blocking and parameterization for the Gibbs sampler. Journal
of the Royal Statistical Society Series B 59, 291–317.
Robertson, A. (1977). The eﬀect of selection on the estimation of genetic
parameters. Journal of Animal Breeding and Genetics 94, 131–135.
Robertson, A. and I. M. Lerner (1949).
The heritability of all-or-none
traits: Viability of poultry. Genetics 34, 395–411.
Rodriguez-Zas, S. L. (1998). Bayesian Analysis of Somatic Cell Score Lac-
tation Patterns in Holstein Cows Using Nonlinear Mixed Eﬀects Models.
Ph. D. thesis, University of Wisconsin-Madison.
Roﬀ, D. E. (1997).
Evolutionary Quantitative Genetics.
Chapman and
Hall.
Rogers, W. H. and J. W. Tukey (1972). Understanding some long-tailed
distributions. Statistica Neerlandica 26, 211–226.
Rosa, G. J. M. (1998). Analise Bayesiana de Models Lineares Mistos Ro-
bustos Via Amostrador de Gibbs. Ph. D. thesis, Escola Superior de Agri-
cultura Luis de Queiroz, Piracicaba, Sao Paulo, Brazil.
Rosa, G. J. M., D. Gianola, and J. I. Urioste (2001). Assessing relationships
between genetic evaluations using robust regression with an application
to Holsteins in Uruguay.
Acta Agricultura Scandinavica Series A 51,
21–34.
Ross, S. M. (1997). Simulation. Academic Press.
Royall, R. (1997). Statistical Evidence. Chapman and Hall.

References
721
Rubin, D. B. (1976). Inference and missing data. Biometrika 63, 581–592.
Rubin, D. B. (1987a). Multiple Imputation for Nonresponse in Surveys.
Wiley.
Rubin, D. B. (1987b).
A noniterative sampling/importance resampling
alternative to the data augmentation algorithm for creating a few im-
putations when fractions of missing information are modest: The SIR
algorithm. Discussion of Tanner and Wong. Journal of the American
Statistical Association 82, 543–546.
Rubin, D. B. (1988). Using the SIR algorithm to simulate posterior dis-
tributions (with discussion). In J. M. Bernardo, M. H. DeGroot, D. V.
Lindley, and A. F. M. Smith (Eds.), Bayesian Statistics 3, pp. 395–402.
Oxford University Press.
Savage, L. J. (1972). The Foundations of Statistics. Wiley.
Schafer, J. L. (2000). Analysis of Incomplete Multivariate Data. Chapman
and Hall/CRC Press.
Schwarz, G. (1978).
Estimating the dimension of a model.
Annals of
Statistics 6, 461–464.
Scott, D. W. (1992). Multivariate Density Estimation. Wiley.
Searle, S. R. (1971). Linear Models. Wiley.
Searle, S. R. (1982). Matrix Algebra Useful for Statistics. Wiley.
Searle, S. R., G. Casella, and C. E. McCulloch (1992). Variance Compo-
nents. Wiley.
Severini, T. A. (1998). Lilelihood functions for inference in the presence of
a nuisance parameter. Biometrika 85, 507–522.
Severini, T. A. (2000). Likelihood Methods in Statistics. Oxford University
Press.
Shannon, C. E. (1948). A mathematical theory of communication. Bell
System Technical Journal 27, 623–656.
Sheehan, N. A. (2000). On the application of Markov chain Monte Carlo
methods to genetic analyses on complex pedigrees. International Statis-
tical Review 68, 83–110.
Sheehan, N. A., B. Guldbrandtsen, M. S. Lund, and D. Sorensen (2002).
Bayesian McMC mapping of quantitative trait loci in a half-sib design: A
graphical model perspective. International Statistical Review. In press.

722
References
Sheehan, N. A. and A. Thomas (1993). On the irreducibility of a Markov
chain deﬁned on a space of genotype conﬁgurations by a sampling
scheme. Biometrics 49, 163–175.
Sillanp¨a¨a, M. J. and E. Arjas (1998). Bayesian mapping of multiple quan-
titative trait loci from incomplete inbred line cross data. Genetics 148,
1373–1388.
Sillanp¨a¨a, M. J. and E. Arjas (1999). Bayesian mapping of multiple quan-
titative trait loci from incomplete outbred oﬀspring data. Genetics 151,
1605–1619.
Silverman, B. (1992). Density Estimation. Chapman and Hall.
Sivia, D. S. (1996). Data Analysis. A Bayesian Tutorial. Oxford University
Press.
Smith, A. F. M. and A. E. Gelfand (1992). Bayesian statistics without tears:
A sampling–resampling perspective. The American Statistician 46, 84–
88.
Smith, C. A. B. (1959). Some comments on the statistical methods used in
linkage investigations. American Journal of Human Genetics 11, 289–
304.
Smith, H. F. (1936). A discriminant function for plant selection. Annals
of Eugenics 7, 240–250.
Sorensen, D. (1996).
Gibbs Sampling in Quantitative Genetics.
Danish
Institute of Agricultural Sciences; Internal Report 82, 192 pp.
Sorensen, D., S. Andersen, D. Gianola, and I. R. Korsgaard (1995).
Bayesian inference in threshold models using Gibbs sampling. Genet-
ics, Selection, Evolution 27, 229–249.
Sorensen, D., R. L. Fernando, and D. Gianola (2001). Inferring the tra-
jectory of genetic variance in the course of artiﬁcial selection. Genetical
Research 77, 83–94.
Sorensen, D., A. Vernersen, and S. Andersen (2000). Bayesian analysis of
response to selection: A case study using litter size in Danish Yorkshire
pigs. Genetics 156, 283–295.
Sorensen, D., C. S. Wang, J. Jensen, and D. Gianola (1994). Bayesian anal-
ysis of genetic change due to selection using Gibbs sampling. Genetics,
Selection, Evolution 26, 333–360.
Spiegelhalter, D. J., N. G. Best, B. P. Carlin, and A. van der Linde (2002).
Bayesian measures of model complexity and ﬁt (with discussion). Journal
of the Royal Statistical Society Series B. In press.

References
723
Stein, S. K. (1977). Calculus and Analytic Geometry. McGraw-Hill.
Stephens, D. A. and R. D. Fisch (1998). Bayesian analysis of a quantita-
tive trait locus data using reversible jump Markov chain Monte Carlo.
Biometrics 54, 1334–1347.
Stramer, O. and R. L. Tweedie (1998).
Langevin-type models II: Self-
targetting candidates for MCMC algorithms. Methodology and Comput-
ing in Applied Probability 1, 307–328.
Strand´en, I. (1996).
Robust Mixed Eﬀects Linear Models with t-
Distributions and Applications to Dairy Cattle Breeding. Ph. D. thesis,
University of Wisconsin-Madison.
Strand´en, I. and D. Gianola (1998). Atenuating eﬀects of preferential treat-
ment with Student-t mixed linear models: A simulation study. Genetics,
Selection, Evolution 30, 565–583.
Strand´en, I. and D. Gianola (1999).
Mixed eﬀects linear models with
t-distributions for quantitative genetic analysis: A Bayesian approach.
Genetics, Selection, Evolution 31, 25–42.
Stuart, A. and J. K. Ord (1987). Kendall’s Advanced Theory of Statistics.
Distribution Theory. Edward Arnold.
Stuart, A. and J. K. Ord (1991). Kendall’s Advanced Theory of Statistics.
Classical Inference and Relationship. Edward Arnold.
Swendsen, R. and J. Wang (1987).
Non-universal critical dynamics in
Monte Carlo simulations. Physical Review Letters 58, 86–88.
Tanner, M. A. (1996). Tools for Statistical Inference. Springer–Verlag.
Tanner, M. A. and W. Wong (1987).
The calculation of posterior dis-
tributions by data augmentation. Journal of the American Statistical
Association 82, 528–550.
Thompson, E. A. (2001). Monte Carlo methods on genetic structures. In
O. E. Barndorﬀ-Nielsen, D. R. Cox, and C. Kl¨uppelberg (Eds.), Complex
Stochastic Systems, pp. 175–218. Chapman and Hall.
Thompson, E. A. and S. C. Heath (2000). Estimation of conditional mul-
tilocus gene identity among relatives. In F. Seiller-Moiseiwitsch (Ed.),
Statistics in Molecular Biology and Genetics: Selected Proceedings of a
1997 Joint AMS-IMS-SIAM Summer Conference on Statistics in Molec-
ular Biology, pp. 95–113. Institute of Mathematical Statistics, Hayward,
CA.: IMS Lecture Note-Monograph Series, Volume 33.

724
References
Thompson, R. (1973). The estimation of variance and covariance compo-
nents with an application when records are subject to culling. Biomet-
rics 29, 527–550.
Thompson, R. (1976). Estimation of quantitative genetic parameters. In
E. Pollak, O. Kempthorne, and T. B. Bailey (Eds.), Proceedings of the
International Conference on Quantitative Genetics, pp. 639–657. Iowa
State University.
Thompson, R. (1980). Maximum likelihood estimation of variance compo-
nents. Mathematische Operationsforschung und Statistik 11, 545–561.
Tierney, L. (1994).
Markov chains for exploring posterior distributions
(with discussion). Annals of Statistics 22, 1701–1786.
Tierney, L. and J. B. Kadane (1989). Accurate approximations for posterior
moments and marginal densities.
Journal of the American Statistical
Association 81, 82–86.
Toutenburg, H. (1982). Prior Information in Linear Models. Wiley.
Uimari, P. and I. Hoeschele (1997). Mapping linked quantitative trait loci
using Bayesian analysis and Markov chain Monte Carlo algorithms. Ge-
netics 146, 735–743.
Van Tassell, C. P. and L. D. Van Vleck (1996). Multiple-trait Gibbs sampler
for animal models: Flexible programs for Bayesian and likelihood-based
(co)variance component inferences. Journal of Animal Science 74, 2586–
2597.
Van Tassell, C. P., L. D. Van Vleck, and K. E. Gregory (1998). Bayesian
analysis of twinning and ovulation rates using a multiple-trait threshold
model and Gibbs sampling. Journal of Animal Science 76, 2048–2061.
Van Vleck, L. D. (1993). Selection Index and Introduction to Mixed Model
Methods. CRC Press.
Vuong, Q. H. (1989). Likelihood ratio tests for model selection and non-
nested hypotheses. Econometrica 57, 307–333.
Waagepetersen, R. and D. Sorensen (2001). A tutorial on reversible jump
MCMC with a view towards applications in QTL-mapping. International
Statistical Review 69, 49–61.
Wang, C. S., D. Gianola, D. Sorensen, J. Jensen, A. Christensen, and
J. J. Rutledge (1994). Response to selection in Danish Landrace pigs: A
Bayesian analysis. Theoretical and Applied Genetics 88, 220–230.

References
725
Wang, C. S., R. L. Quaas, and E. J. Pollak (1997). Bayesian analysis of
calving ease scores and birth weights. Genetics, Selection, Evolution 29,
117–143.
Wei, G. C. G. and M. A. Tanner (1990). A Monte Carlo implementation
of the EM algorithm and the poor man’s data augmentation algorithm.
Journal of the American Statistical Association 85, 699–704.
Weinstock, R. (1974). Calculus of Variations with Applications to Physics
and Engineering. Dover.
Weir, B. S. (1996). Genetic Data Analysis II. Sinauer Associates.
Wiggans, G. R. and M. E. Goddard (1997). A computationally feasible
test-day model for genetic evaluation of yield traits in the United States.
Journal of Dairy Science 80, 1795–1800.
Wilks, S. S. (1938). The large-sample distribution of the likelihood ratio for
testing composite hypotheses. The Annals of Mathematical Statistics 9,
60–62.
Willham, R. L. (1963). The covariance between relatives for characters
composed of components contributed by related individuals.
Biomet-
rics 19, 18–27.
Williamson, R. E., R. H. Crowell, and H. F. Trotter (1972). Calculus of
Vector Functions. Prentice–Hall.
Wilton, J. W., D. A. Evans, and L. D. Van Vleck (1968). Selection indices
for quadratic models of total merit. Biometrics 24, 937–949.
Wolﬁnger, R. (1993). Laplace’s approximation for nonlinear mixed models.
Biometrika 80, 791–795.
Wolﬁnger, R. and X. Lin (1997). Two Taylor-series approximation methods
for nonlinear mixed models. Computational Statistics and Data Analy-
sis 25, 465–490.
Wright, S. (1934). An analysis of variability in number of digits in an inbred
strain of guinea pigs. Genetics 19, 506–536.
Wright, S. (1968). Evolution and the Genetics of Populations. Genetic and
Biometric Foundations. University of Chicago.
Yi, N. and S. Xu (2000). Bayesian mapping of quantitative trait loci under
the identity-by-descent-based variance component model. Genetics 156,
411–422.
Zellner, A. (1971). An Introduction to Bayesian Inference in Econometrics.
Wiley.

726
References
Zellner, A. (1976). Bayesian and non-Bayesian analysis of the regression
model with multivariate Student-t error terms. Journal of the American
Statistical Association 71, 400–405.
Zellner, A. and R. Highﬁeld (1988). Calculation of maximum entropy distri-
butions and approximation of marginal posterior distributions. Journal
of Econometrics 37, 195–210.

List of Citations
Abramowitz and Stegun (1972),
86, 368
Agresti (1989), 626
Agresti (1990), 626
Agresti (1996), 626
Aitken (1934), 62
Akaike (1973), 421
Albert and Chib (1993), 435, 598,
606, 609
Albert and Chib (1995), 292, 435
Anderson and Aitkin (1985), 606
Anderson (1984), 43, 51, 56
Applebaum (1996), 334, 335, 338,
340, 342, 370, 371
Baldi and Brunak (1998), 338
Barndorﬀ-Nielsen and Cox (1994),
166, 181
Barndorﬀ-Nielsen (1983), 133
Barndorﬀ-Nielsen (1986), 181
Barndorﬀ-Nielsen (1991), 181
Barnett and Lewis (1995), 434,
591
Barnett (1999), 219
Bates and Watts (1988), 292
Bayarri (1981), 81, 104
Bayes (1763), 81
Becker (1984), 89
Berger and Bernardo (1992), 337,
356, 397
Berger and Pericchi (1996), 422,
423
Berger (1985), 406
Bernardo and Smith (1994), 4, 104,
120, 214, 219, 289, 330,
331, 333, 342, 356, 364,
379, 382, 393, 395, 397,
405, 406, 411, 412, 564,
607
Bernardo (1979), 337, 356, 379,
385, 397, 400
Bernardo (2001), 394, 395
Besag (1974), 511
Besag (1994), 517
Bibby and Toutenburg (1977), 301
Blasco and Varona (1999), 629,
633
Blasco (2001), 119, 121
Bliss (1935), 606
727

728
List of Citations
Box and Muller (1958), 105
Box and Tiao (1973), 17, 85, 104,
214, 227, 237, 244, 262,
278, 289, 337, 356, 360,
362, 366, 514, 545, 650,
687, 693
Box (1976), 671
Box (1980), 218
Brooks and Gelman (1998), 550
Brooks and Roberts (1998), 547
Brooks et al. (2001), 532
Brown et al. (2001), 13
Bulmer (1971), 44
Bulmer (1979), 14, 44, 46
Bulmer (1980), 26, 62, 65
Bunke (1975), 301
Carlin and Louis (1996), 543
Casella and Berger (1990), 13, 33,
138, 145, 148, 152
Casella and George (1992), 488
Casella and Robert (1996), 552
Casella et al. (2001), 556
Chen et al. (2000), 551, 554, 585
Chib and Greenberg (1995), 503,
504, 689
Chib and Jeliazkov (2001), 428
Chib (1995), 427, 428
Cohen (1986), 20
Collet (1994), 439
Congdon (2001), 421, 427, 437
Cowles and Carlin (1996), 547, 550
Cowles (1996), 612
Cox and Hinkley (1974), 157, 166
Cox and Miller (1965), 477, 479,
494
Cox and Reid (1987), 181
Cox and Snell (1989), 178, 188,
204, 607
Cox (1961), 174, 529
Cox (1962), 174, 529
Crow and Kimura (1970), 166, 175
Curnow and Smith (1975), 606
Curnow (1961), 62
Curnow (1972), 606
Dahlquist and Bj¨orck (1974), 162–
164
Dempster and Lerner (1950), 89,
90, 92, 202, 605, 606
Dempster et al. (1977), 444, 584
Dempster (1974), 429
Dempster (1997), 429
Dennis and Schnabel (1983), 162
Devroye (1986), 39, 66
De Finetti (1975a), 4
De Finetti (1975b), 29
Dickey (1968), 278
Doerge and Churchill (1996), 685
Draper and Smith (1981), 292, 434
Ducrocq et al. (1988), 80
Durbin et al. (1998), 338
Earman (1992), 219
Edwards (1974), 119
Edwards (1992), 26, 119, 167, 181
Efron and Hinkley (1978), 131
Efron (1993), 181
Efron (1998), 133
Elston and Stewart (1971), 671
Fahrmeir and Tutz (2001), 626
Falconer and Mackay (1996), 287,
344
Falconer (1965), 202, 605, 606
Falconer (1967), 606
Famula (1981), 24
Fan et al. (2000), 174
Feller (1970), 477
Feng and McCulloch (1996), 147
Fernandez and Steel (1998), 595
Fisher (1918), 44, 102
Fisher (1920), 142
Fisher (1922), 119, 122, 142
Fisher (1925), 346
Fishman (1973), 19, 21
Flury and Zoppe (2000), 447
Foulley and Manfredi (1991), 606
Foulley et al. (1983), 606, 615
Foulley et al. (1987), 606
Fox (1987), 375, 377
Galton (1885), 287

List of Citations
729
Garc´ıa-Cort´es and Sorensen (1996),
585
Geisser and Eddy (1979), 438
Geisser (1993), 438
Gelfand and Dey (1994), 429, 438
Gelfand and Smith (1990), 85, 540,
556
Gelfand et al. (1990), 547, 552
Gelfand et al. (1992), 436, 437
Gelfand et al. (1995), 602, 604
Gelfand et al. (1996), 602–604
Gelfand (1996), 436, 437
Gelman and Rubin (1992), 540,
548–550
Gelman et al. (1995), 10, 40, 57,
61, 214, 220, 577
Gelman et al. (1996), 437, 438
Geman and Geman (1984), 497
George et al. (2000), 694
Geweke (1989), 556
Geweke (1993), 598, 599
Geyer (1992), 540, 554, 555
Gianola and Fernando (1986), 52,
66, 313, 650
Gianola and Foulley (1983), 83,
606
Gianola et al. (1989), 26, 62
Gianola et al. (1990), 313
Gianola (1982), 202, 606
Gilks and Roberts (1996), 547
Gilks and Wild (1992), 658
Gilmour et al. (1985), 606
Go et al. (1978), 672
Goodman and Hartley (1958), 155
Good (1952), 441
Good (1958), 401
Green (1995), 497, 518
Grimmet and Stirzaker (1992), 482
Gross and Clark (1975), 439
Grossman and Turner (1974), 478
Guo and Thompson (1994), 672,
675, 676
Hacking (1965), 167
Hager (1988), 162
Haldane (1919), 680
Haldane (1948), 359, 360
Hammersley and Handscomb (1964),
556
Hampel et al. (1986), 589
Han and Carlin (2001), 428
Harville and Mee (1984), 606
Harville (1974), 472, 651
Harville (1977), 266
Hastings (1970), 497, 555
Hazel (1943), 576
Heath (1997), 679, 694
Heisenberg (1958), 219
Henderson and Meyer (2001), 13
Henderson et al. (1959), 62, 318
Henderson (1953), 26, 313
Henderson (1963), 26, 51, 125
Henderson (1973), 26, 50, 51, 125,
212, 279, 282, 318
Henderson (1975), 26, 62, 64
Henderson (1984), 102
Heringstad et al. (2001), 606
Hills and Smith (1992), 584, 604
Hills and Smith (1993), 604
Hobert and Casella (1996), 545,
566
Hoel et al. (1971), 79
Hoerl and Kennard (1970), 300
Hoeschele and Tier (1995), 609
Hoeting et al. (1999), 439, 441,
442
Hogg and Craig (1995), 3
Howson and Urbach (1989), 219
Jamrozik and Schaeﬀer (1997), 628
Janss et al. (1995), 672, 679
Jaynes (1957), 341
Jaynes (1994), 354, 367
Jeﬀreys (1961), 289, 356, 360, 361,
367, 401
Jensen et al. (1994), 571
Jensen et al. (1995), 679
Jensen (1994), 606, 615
Johnson and Kotz (1969), 4
Johnson and Kotz (1970a), 4
Johnson and Kotz (1970b), 4, 83
Johnson and Kotz (1972), 4

730
List of Citations
Kackar and Harville (1981), 212
Kadarmideen et al. (2002), 606,
609
Kalbﬂeisch and Sprott (1970), 181
Kalbﬂeisch and Sprott (1973), 181
Kaplan (1993), 96, 97, 376
Karlin and Taylor (1975), 477, 492
Kass and Raftery (1995), 401, 402,
420, 421, 425, 427
Kass et al. (1998), 540
Kass (1995), 421
Keller (2000), 4
King (1989), 119
Kirkpatrick et al. (1994), 628
Kleinbaum (1996), 80
Knott and Haley (1992), 684, 685
Koerkhuis and Thompson (1997),
571
Korsgaard et al. (1999), 57, 619,
626
Korsgaard et al. (2002), 615
Kullback (1968), 346, 347, 349,
351
Laird and Ware (1982), 628, 665
Lange and Elston (1975), 671
Lange and Goradia (1987), 675
Lange and Sinsheimer (1993), 589,
664
Lange (1995), 453
Lange (1997), 675
Lee and Nelder (1996), 606
Lee and Thomas (2000), 694
Lee (1989), 214
Lehmann and Casella (1998), 144–
148, 152, 171
Lehmann (1999), 46, 93, 131, 148,
180, 181
Leonard and Hsu (1999), 214, 356,
364, 420, 421
Lindley and Smith (1972), 52, 85,
266, 313, 629
Lindley (1956), 350
Lindley (1957), 409
Little and Rubin (1987), 473, 577
Liu and Rubin (1995), 592, 594
Liu et al. (1994), 510, 552, 554,
584, 604
Liu (1994), 584, 585, 604
Liu (2001), 477
Lo et al. (2001), 174
Louis (1982), 449, 453
Lund and Jensen (1999), 672, 679
Lynch and Walsh (1998), 56
MacCluer et al. (1986), 675
Madigan and Raftery (1994), 439,
442
Mal´ecot (1947), 120, 214
Mal´ecot (1969), 44, 46
Mardia et al. (1979), 43, 56
Marsaglia and Zaman (1993), 20
Martinez et al. (2000), 671
McCullagh and Nelder (1989), 77,
123, 181–183, 188
McCulloch and Rossi (1991), 425
McCulloch (1994), 606
McLachlan and Krishnan (1997),
444, 449, 452, 473, 592,
594
Meeker and Escobar (1995), 178,
180
Meilijson (1989), 453
Meng and Rubin (1991), 453, 454,
456
Mengersen et al. (1999), 547
Metropolis et al. (1953), 497, 504,
689
Meyer (1999), 628
Meyn and Tweedie (1993), 477,
498, 501
Milliken and Johnson (1992), 270
Misztal et al. (1989), 609
Mood et al. (1974), 3, 157, 610
Moreno et al. (1997), 608, 609
Morton and MacLean (1974), 671
Nandram and Chen (1996), 612
Nelder and Wedderburn (1972),
77, 123
Newton and Raftery (1994), 426,
690
Neyman and Pearson (1928), 166

List of Citations
731
Norris (1997), 477
O’Hagan (1994), 214, 217, 219, 253,
264, 298, 354, 356, 364,
411, 414–416, 420–424, 526,
541
Oakes (1999), 453, 457, 458, 465
Odell and Feiveson (1966), 59
Ott (1999), 680, 685
Patterson and Thompson (1971),
26, 128, 183, 186
Pauler et al. (1999), 418
Pawitan (2000), 180
Pearson (1900), 606
Pearson (1903), 26, 62, 64
Peskun (1973), 507, 508, 523
Popper (1972), 219
Popper (1982), 219
Priestley (1981), 555
Propp and Wilson (1996), 556
Raftery et al. (1997), 439, 442
Raj (1968), 155
Rao (1947), 180
Rao (1973), 26, 43, 87, 115, 145,
200
Reid (1995), 181
Reid (2000), 181
Richardson and Green (1997), 518
Ripley (1987), 19, 554, 657
Robert and Casella (1999), 477,
498, 547, 551
Roberts and Sahu (1997), 584, 585,
602
Roberts et al. (1997), 504
Robertson and Lerner (1949), 90,
606
Robertson (1977), 62
Robert (1994), 20
Robert (1998), 547
Rodriguez-Zas (1998), 599, 670
Roﬀ(1997), 56
Rogers and Tukey (1972), 589, 664
Rosa et al. (2001), 670
Rosa (1998), 589
Ross (1997), 19
Royall (1997), 167, 168
Rubin (1976), 576
Rubin (1987a), 577
Rubin (1987b), 556
Rubin (1988), 661
Savage (1972), 218
Schafer (2000), 577
Schwarz (1978), 420, 421
Scott (1992), 553
Searle et al. (1992), 26, 51, 125,
184, 234, 279, 343
Searle (1971), 32, 39, 43, 53, 67,
85, 92, 115, 147, 169, 183,
254, 284, 313, 318, 564
Searle (1982), 51, 53, 183, 603
Severini (1998), 181
Severini (2000), 166, 181
Shannon (1948), 337, 341, 346
Sheehan and Thomas (1993), 678
Sheehan et al. (2002), 679
Sheehan (2000), 675, 678, 679
Sillanp¨a¨a and Arjas (1998), 691,
694
Sillanp¨a¨a and Arjas (1999), 694
Silverman (1992), 553
Sivia (1996), 367, 370, 371
Smith and Gelfand (1992), 556
Smith (1936), 576
Smith (1959), 408
Sorensen et al. (1994), 26
Sorensen et al. (1995), 548, 555,
606, 607, 619
Sorensen et al. (2000), 438
Sorensen et al. (2001), 66
Sorensen (1996), 606, 615
Spiegelhalter et al. (2002), 429–
431
Stein (1977), 132
Stephens and Fisch (1998), 694
Stramer and Tweedie (1998), 517
Strand´en and Gianola (1998), 594
Strand´en and Gianola (1999), 589,
599, 664
Strand´en (1996), 589, 598, 670
Stuart and Ord (1987), 53

732
List of Citations
Stuart and Ord (1991), 39, 139,
166, 167
Swendsen and Wang (1987), 532
Tanner and Wong (1987), 241, 293,
473, 532, 556
Tanner (1996), 28, 454, 658
Thompson and Heath (2000), 679
Thompson (1973), 62
Thompson (1976), 62
Thompson (1980), 266
Thompson (2001), 672, 679
Tierney and Kadane (1989), 420
Tierney (1994), 497, 498, 501
Toutenburg (1982), 301
Uimari and Hoeschele (1997), 694
Van Tassell and Van Vleck (1996),
581
Van Tassell et al. (1998), 615
Van Vleck (1993), 571
Vuong (1989), 174
Waagepetersen and Sorensen (2001),
497, 518, 694
Wang et al. (1994), 24
Wang et al. (1997), 581, 606, 615,
616
Wei and Tanner (1990), 447, 462
Weinstock (1974), 375, 377
Weir (1996), 175, 195
Wiggans and Goddard (1997), 628
Wilks (1938), 166
Willham (1963), 208, 570
Williamson et al. (1972), 97
Wilton et al. (1968), 283
Wolﬁnger and Lin (1997), 653
Wolﬁnger (1993), 653
Wright (1934), 90, 202, 606
Wright (1968), 23, 69
Yi and Xu (2000), 694
Zellner and Highﬁeld (1988), 370
Zellner (1971), 214, 263, 296, 337,
356, 360, 361, 403, 635
Zellner (1976), 589, 592, 599, 664

Subject Index
Acceptance-rejection sampling, 657
Additive genetic covariance ma-
trix, 572
Additive genetic model
Bayesian view, 313
clustered random eﬀects, 600
conditional posterior distribu-
tions, 51
marginal posterior density of
additive genetic value, 231
maternal eﬀects, 570
multivariate, 576
multivariate (blocked) Gibbs
sampling, 584
posterior probability distribu-
tions, 259
quadratic selection index, 283
repeated measurements, 226
robust analysis, 592, 595
univariate, 564
updating additive genetic ef-
fects, 253
Additive genetic relationship ma-
trix, 564
Aitken’s integral, 32, 41
Akaike information criterion (AIC),
421
Aperiodicity, 483
Autocorrelation between MCMC
samples, 543
Backcross design, 681
Bayes factor, 221, 400
approximations, 418
computation, 424
decision theoretic view, 403
inﬂuence of the prior, 412
intrinsic Bayes factor, 422
partial Bayes factor, 422
Bayes theorem
continuous case, 224
discrete case, 216
Bayesian asymptotic theory
continuous parameters, 331
discrete parameters, 330
Bayesian information criterion (BIC),
420
Bayesian learning, 216, 222, 249,
546
Bayesian model average, 439

734
Subject Index
Predictive ability, 441
Behrens-Fisher problem, 170
Best linear unbiased predictor, 318,
565
Best predictor, 68, 281
Beta function, 84
Binary and Gaussian responses
joint analysis, 626
Box-Muller transformation, see Sim-
ulation of random vari-
ables
Burn-in period, 540
Calculus of variations, 375
Euler-Lagrange condition, 377
Categorical and Gaussian responses
joint analysis, 615
Categorical traits, 605
analysis of a single polychoto-
mous response variable,
607
residual analysis, 435
Cauchy-Schwarz inequality, 139
Central limit theorem, 44
Chapman-Kolmogorov equations,
480
Cholesky factorization, 59, 112
Clustered random eﬀects, 600
Complete data, 444
Complex segregation analysis, 671
Composition, 28, 437
Conditional distribution, 30
Conditional multivariate normal
distribution, 51
Conditional posterior distribution,
228, 229, 235, 240
Conﬁdence regions, 177, 179, 180
Conjugacy, 298
Conjugate prior, 297
Constant of integration, 16, 32
Continuity correction, 13
Convergence diagnostics, 547
Convergence in distribution, 46,
93
Convergence in probability, 93, 146
Countably inﬁnite, 11
Covariance between relatives, 72
genetic marker information,
74
Cram´er-Rao lower bound, 138
Credibility sets, 262
Cross-validation, 436
Cumulative distribution function,
6, 13, 14
Data augmentation, 241, 293, 532,
576, 608, 616, 665
Degree of belief, 57
Delta method, 93
Detailed balance equation, 487
Deviance, 171, 430
Deviance information criterion, 431
Discrete traits, see Categorical traits
Distribution
Bernoulli distribution, 7
beta distribution, 21
beta-binomial distribution, 68,
71
binomial distribution, 9, 78
normal approximation, 12
Poisson approximation, 10
Cauchy distribution, 28
chi-square distribution, 24, 53
Dirichlet distribution, 40
exponential distribution, 24
gamma distribution, 24
inverse chi-square, 84
inverse gamma, 57
inverse Wishart distribution,
57
logistic distribution, 83, 204
moment generating function,
83
lognormal distribution, 80
mixture distribution, 28
multinomial distribution, 37,
98, 190, 458, 488
multivariate normal distribu-
tion, 41

Subject Index
735
multivariate uniform distribu-
tion, 40
multivariate-t distribution, 60
negative binomial distribution,
405
normal distribution, 25
independence, 42
linear functions, 44
Poisson distribution, 11
scaled inverse chi-square dis-
tribution, 85, 565
sech-squared distribution, 83
singular normal distribution,
43
Student-t distribution, 28
mixture interpretation, 28
uniform distribution, 18
Wishart distribution, 55
Distribution of a ratio, 100
Distributions with constrained sam-
ple space, 62
Eﬀective chain length, 548, 555
Eﬀective number of parameters,
430
EM algorithm, 443
exponential families, 451
maximum likelihood, 466
Monte Carlo EM, 447, 462
rate of convergence, 449
restricted maximum likelihood,
472
standard errors, 452
supplemented EM algorithm
(SEM), 452
Entropy, 334
entropy of a distribution
continuous distributions, 341
discrete distributions, 337
entropy of joint and condi-
tional distributions, 340
relative entropy, 354
Shannon-Jaynes entropy, 371
Equilibrium distribution, see Sta-
tionary distribution
Ergodic average, 551
Ergodicity, 484
Estimability, 147
Exchangeability, see Random vari-
able
Expected information, see Infor-
mation
Expected posterior loss, 403
Expected value, 8
Exponential families, see EM al-
gorithm
Extreme category problem, 609
FF algorithm, 525
First-order autoregressive process,
633
Fisher’s information, see Informa-
tion
Fisher’s scoring algorithm, 162
Founder and nonfounder individ-
uals, 673
Fully conditional posterior distri-
bution, 509, 510
Gamma function, 17, 22
Gamma integral, 17
Gaussian linear model
joint modes, 273
marginal modes, 277
Gene-dropping, 675
Gibbs sampling, 509
blocked Gibbs sampling, 584
systematic-scan, single-site, 491
Goodness of ﬁt, 429
Growth curve, 631
Hammersley-Cliﬀord theorem, 511
Heteroscedastic residuals, 633
Hierarchical models, 628
High credibility sets, 262
Highest posterior density interval,
262
Homoscedastic residuals, 633
Homoscedastic variance, 42
Hyperparameters, 235

736
Subject Index
Hypothesis test, 166, 271, 401
composite vs composite hy-
potheses, 411
deviance information criterion,
431
loss function, 404
nested models, 166, 173, 414
point null hypothesis, 407
simple vs composite hypothe-
ses, 406
two simple hypotheses, 404
Identiﬁability, 147, 543
Implementation of MCMC, 540
Importance sampling, 424, 556, 658
Improper distribution, 35
Improper posterior distribution, 269
Imputation of missing records, 580
Inadmissibility, 186
Incomplete data, 444
Independence, 9, 29
mutual independence, 29
pairwise independence, 29
Indicator function, 7, 15
Information, 127
entropy, 334
expected information, 131, 138
expected information matrix,
135
Fisher’s information, 128, 132,
139, 351
observed information, 131
Information about a parameter,
346
Information as curvature, 132
Information per observation, 199
Information provided by an ex-
periment, 350
Invariant distribution, see Station-
ary distribution
Inverse probability, 216
Irreducibility, 483
segregation analysis models,
677
Iterated expectations, 61, 67
Jacobian, 79, 96
Jeﬀreys’ Priors
many parameters, 364
single parameter, 360
Jensen’s inequality, 145
Joint cumulative distribution func-
tion, 30
Joint modes, 265, 273
Joint posterior distribution, 235
Joint probability density function,
30
Joint updating schemes, 679
Kernel of the distribution, 16
Kullback’s Information Measure,
346
divergence between hypothe-
ses, 347
Kullback-Leibler discrepancy, 331
relative entropy, 353
Kullback-Leibler distance, 429, 448
Lagrange multiplier test, 179
Langevin-Hastings algorithm, 517
Laplace integration, 419
Least-squares, 127
Liability, 203, 605
Likelihood
integrated likelihood, 128
marginal likelihood, 128, 182
MCMC computation, 424
proﬁle likelihood, 186
restricted likelihood, 128, 186
Likelihood function, 121
Likelihood ratio test, 166
asymptotic distribution, 171
Monte Carlo likelihood ratio
test, 685
power, 174
Lindley’s paradox, 409
Linear model
Bayes factor, 413
Linear regression, 136, 287
multivariate-t error distribu-
tion, 591

Subject Index
737
univariate-t error distribution,
589
Linear transformations, see Trans-
formations
Linkage, 407
Location parameters
marginal distribution, 323
Log-likelihood, 122
Logistic regression, 202
Logistic transformation, see Trans-
formations
Logit transform, 194
Longitudinal data, 627
analysis with thick-tailed dis-
tributions, 664
computation via MCMC, 653
Gaussian approximation, 647
scoring algorithm, 648
two-step approximate Bayesian
analysis, 642
Loss function, 186, 262, 264, 281,
404
Major genes, 671
Map distance, 680
Mapping function, 680
Marginal distribution
continuous random variables,
29, 32
discrete random variables, 31
Marginal distribution of data, 232
additive genetic model, 226
Bayes factor, 415
Marginal maximum likelihood, 650
Marginal modes, 266
Marginal posterior distribution, 235
Marginal probability density, 29
Markov chain Monte Carlo, 497
Markov chains, 477
convergence to stationarity, 492
Jordan decomposition, 494
limiting behavior, 492
long-term behavior, 481
stage of a Markov chain, 478
state of a Markov chain, 478
time homogeneous, 479
Markov property, 479
Maternal eﬀects additive genetic
model, 570
Maximum entropy prior distribu-
tions, 367
Gibbs distribution, 370
Maximum likelihood, 119
Maximum likelihood estimator, 122
asymptotic properties
multiparameter models, 152
single-parameter models, 143
conﬁdence regions, 177
consistency, 146
eﬃciency, 151
functional invariance, 153, 157,
159
regularity conditions, 147
residual variance, 54
Mean squared error, 185
Method of composition, see Com-
position
Metropolis algorithm, 504, 689
Metropolis-Hastings algorithm, 502
acceptance probability, 502,
503
joint updating, 504
proposal distribution, 502
random walk proposal, 517
single-site updating, 507
Missing information principle, 448
Mixed inheritance model, 671
Mixed linear model, 313
EM algorithm, 466, 472
maximum likelihood inferences,
466
restricted maximum likelihood
inferences, 472
Mixture distribution, 71
Model ﬁt
posterior distribution of resid-
uals, see Residuals
Models with thick-tailed distribu-
tions, 588
Moment generating function

738
Subject Index
multivariate, 43
univariate, 26
Moments of a distribution, 26
Monte Carlo variance, 553
initial positive sequence esti-
mator, 555
method of batching, 555
Multimodal posterior distribution,
594
Multiple comparisons, 270
Multistage model, 628
Multivariate distribution, 29
Multivariate-t distribution, 314, 324
Newton-Raphson, 162
Normalized distribution, 62
Nuisance parameters, 35, 125, 152,
166, 181–183, 186, 208
Objective Bayesian analysis, 288
Orthogonal parameterization, 307
p-value, 401, 684
Bayesian p-value, 438
Parameter, 8
Parameter space, 120
Penetrance, 674
Perfect sampling, 556
Polar coordinates, 17
Posterior correlation between pa-
rameters, 238, 285, 546,
584
hierarchical centering, 543, 602
Posterior credibility sets
additive genetic model, 262
Posterior distribution, 235
discrepancy with prior distri-
bution, 353
Gaussian approximation, 419
Posterior loss, see Expected pos-
terior loss
Posterior median
additive genetic model, 262
Posterior mode, 264, 418
Posterior odds ratio, 401
Posterior probability, 258
Posterior probability distribution,
213, 216, 224
Posterior probability of a hypoth-
esis, 403
Posterior probability of linkage,
see Linkage
Posterior quantiles, 262
Prediction error variance, 282
Predictive ability of model, 433
Predictive distribution, 292, 306
posterior predictive distribu-
tion, 293, 307, 437
prior predictive distribution,
292, 306, 402
Predictive log-score, 441
Principle of insuﬃcient reason, 356
Prior odds ratio, 401
Prior probability distribution, 213,
215, 218, 223
conjugate prior, 297
eﬀect on posterior inferences,
328
heritability, 106, 109, 357
improper uniform prior, 224,
288, 357, 565
maximum entropy priors, 367
mutation rate, 359
reference priors, 379
uniform prior, 356
vague information, 356
Probability density function, 14
lack of uniqueness, 14
Probability function, 5
Probability mass, 6
Probability mass function, see Prob-
ability function
Probit model, 204
QTL analysis, 679
arbitrary number of QTL, 690
Bayesian inference, 686
Bayes factors and model se-
lection, 690

Subject Index
739
fully conditional posterior
distributions, 687
likelihood inference, 682
hypotheses tests, 684
likelihood ratio test, 684
Monte Carlo likelihood ra-
tio test, 685
proﬁle likelihood, 685
reversible jump MCMC, 694
single QTL model, 680
Quadratic genetic merit, 283
Quasi-BLUP approach, 653
Random quantity, see Random vari-
able
Random variable
continuous random variable,
13
discrete random variable, 5
distribution, 5
exchangeable random variables,
29
Rao-Blackwell estimator, 552
Rao-Blackwellization, 280
Reference analysis
multiparameter models, 396
single nuisance parameter, 389
single parameter, 379
Reference prior distributions, 379
Regression curve, 36
Residual analysis, 434
Residual covariance matrix, 578
Residuals
posterior distribution, 292, 306
Restricted maximum likelihood, see
Mixed linear model, 651
Reversibility, 487, 501, 520
Reversible jump MCMC, 517
acceptance probability, 522
addition of a QTL, 696
deterministic proposals, 523
dimension matching condition,
520
FF proposals, 525
model selection, 699
proposal distribution, 519
QTL analysis, 694
removal of a QTL, 695
Ridge regression, 300
Robust analysis
additive genetic model, 592,
595
clustered random eﬀects, 600
linear regression, 240, 241, 589
longitudinal data, 664
Robust methods, 589
Sample space
continuous sample space, 13
discrete sample space, 5
Schwarz BIC, 421
Score, 123, 132, 134, 159
Score test, 179
Scoring algorithm, 162
longitudinal analysis, 648
Segregation analysis, 672
Selection by truncation, 62, see
Truncation selection
Sensitivity analysis, 556
Simulation of random variables
binomial random variable, 10
Box-Muller transformation, 105
Dirichlet random variables, 40
discrete random variables, 18
inverse transform method, 19
multinomial distribution, 39
multivariate normal samples,
112
t-distributed random variable,
28
truncated distributions, 66
Wishart and inverse Wishart
distribution, 59
Stationary distribution, 481, 500
Statistical information, 334
Student-t mixed eﬀects model, 595
Student-t model
linear regression, 240, 241
longitudinal data, 664
Suﬃciency, 142

740
Subject Index
jointly suﬃcient statistics, 143
Support, 6
t-model, see Student-t model
Taylor approximations, 89, 114
Threshold model, 90, 202, 605
Transformation invariance
reference prior, 385
Transformations
bivariate normal distribution,
113
continuous random variables,
79, 95
discrete random variables, 78,
97
linear transformations, 111
logistic transformation, 82
many-to-one, 87
multivariate transformations,
95
univariate transformations, 78
Transition kernel, 499, 501
Transition probability, 479
Truncated normal distribution, 612
Kullback-Leibler distance, 355
Truncation selection, 64
Unbiased estimator, 138
Uncertainty, 8
Uniform Prior, see Prior proba-
bility distribution
Univariate continuous distributions,
13
Univariate discrete distributions,
4
Univariate-t distribution, 665
Variance components
marginal distribution, 322
Wald’s test, 179

