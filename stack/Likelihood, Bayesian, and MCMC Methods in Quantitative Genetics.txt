Likelihood, Bayesian,
and MCMC Methods
in Quantitative Genetics
Daniel Sorensen
Daniel Gianola
Springer

Statistics for Biology and Health
Series Editors
K. Dietz, M. Gail, K. Krickeberg, J. Samet, A. Tsiatis

Daniel Sorensen
Daniel Gianola
Likelihood, Bayesian,
and MCMC Methods
in Quantitative Genetics

Daniel Sorensen
Daniel Gianola
Department of Animal Breeding
Department of Animal Science
and Genetics
Department of Dairy Science
Danish Institute of Agricultural Sciences
Department of Biostatistics and Medical
DK-8830 Tjele
Informatics
Denmark
University of Wisconsin-Madison
sorensen@inet.uni2.dk
Madison, WI 53706
USA
gianola@calshp.cals.wisc.edu
Series Editors
K. Dietz
M. Gail
K. Krickeberg
Institut fu¬®r Medizinische Biometrie
National Cancer Institute
Le Chatelet
Universita¬®t Tu¬®bingen
Rockville, MD 20892
F-63270 Manglieu
Westbahnhofstrasse 55
USA
FRANCE
D-72070 Tu¬®bingen
GERMANY
J. Samet
A. Tsiatis
School of Public Health
Department of Statistics
Department of Epidemiology
North Carolina State University
Johns Hopkins University
Raleigh, NC 27695
615 Wolfe Street
USA
Baltimore, MD 21205-2103
USA
Library of Congress Cataloging-in-Publication Data
Sorensen, Daniel.
Likelihood, Bayesian and MCMC methods in quantitative genetics / Daniel Sorensen,
Daniel Gianola.
p. cm. ‚Äî (Statistics for biology and health)
Includes bibliographical references and index.
ISBN 0-387-95440-6 (alk. paper)
1. Genetics‚ÄîStatistical methods.
2. Monte Carlo method.
3. Markov processes.
4. Bayesian statistical decision theory.
I. Gianola, Daniel, 1947‚Äì
II. Title.
III. Series.
QH438.4.S73 S675 2002
575.6‚Ä≤07‚Ä≤27‚Äîdc21
2002019555
ISBN 0-387-954406
Printed on acid-free paper.
Ôõô2002 Springer-Verlag New York, Inc.
All rights reserved. This work may not be translated or copied in whole or in part without the
written permission of the publisher (Springer-Verlag New York, Inc., 175 Fifth Avenue, New York,
NY 10010, USA), except for brief excerpts in connection with reviews or scholarly analysis. Use
in connection with any form of information storage and retrieval, electronic adaptation, computer
software, or by similar or dissimilar methodology now known or hereafter developed is forbidden.
The use in this publication of trade names, trademarks, service marks and similar terms, even if
they are not identified as such, is not to be taken as an expression of opinion as to whether or not
they are subject to proprietary rights.
Printed in the United States of America.
9 8 7 6 5 4 3 2 1
SPIN 10866246
www.springer-ny.com
Springer-Verlag
New York Berlin Heidelberg
A member of BertelsmannSpringer Science+Business Media GmbH

Preface
Statistical genetics results from the merger of genetics and statistics into a
coherent quantitative theory for predicting and interpreting genetic data.
Based on this theory, statistical geneticists developed techniques that made
notable contributions to animal and plant breeding practices in the second
half of the last century as well as to advances in human genetics.
There has been an enormous research impetus in statistical genetics over
the last 10 years. Arguably, this was stimulated by major breakthroughs
in molecular genetics, by the advent of automatic data-recording devices,
and by the possibility of applying computer-intensive statistical methods
to large bodies of data with relative ease. Data from molecular biology
and biosensors are characterized by their massive volume. Often, intricate
distributions need to be invoked for appropriate modeling. Data-reduction
techniques are needed for accounting for the involved nature of these data
and for extracting meaningful information from the observations. Statisti-
cal genetics plays a major role in this process through the development,
implementation, and validation of probability models for inference. Many
of these models can be daunting and, often, cannot be Ô¨Åtted via standard
methods. Fortunately, advances in computing power and computer-based
inference methods are making the task increasingly feasible, especially in
connection with likelihood and Bayesian inference.
Two important breakthroughs in computational statistics have been the
bootstrap and Markov chain Monte Carlo (MCMC) methods. In this book
we focus on the latter. MCMC was introduced into the statistical literature
in the late 1980s and early 1990s, and incorporation and adaptation of the
methods to the needs of quantitative genetic analysis was relatively rapid,

vi
particularly in animal breeding. Also, MCMC is having a major impact in
applied statistics (especially from a Bayesian perspective), opening the way
for posing models with an enormous amount of Ô¨Çexibility. With MCMC,
it is possible to arrive at better descriptions of the perceived underlying
structures of the data at hand, free from the strictures of standard methods
of statistical analysis.
The objective of this book is to present the main ideas underlying like-
lihood and Bayesian inference and MCMC methods in a manner that is
accessible to numerate biologists, giving step-by-step derivations and fully
worked-out examples. Most of these examples are from quantitative genet-
ics and, although not exclusively, we focus on normal or generalized linear
models.
Most students and researchers in agriculture, biology, and medicine lack
the background needed for understanding the foundations of modern bio-
metrical techniques. This book has been written with this particular read-
ership in mind. A number of excellent books describing MCMC methods
have become available in recent years. However, the main ideas are pre-
sented typically in a technically demanding style, as these books have been
written by and addressed to statisticians. The statistician often has the
mathematical background needed to ‚ÄúÔ¨Åll in the blanks‚Äù. What is tedious
detail to a statistician, so that it can be omitted from a derivation, can
cause considerable consternation to a reader with a diÔ¨Äerent background.
In particular, biologists need a careful motivation of each model from a
subject matter perspective, plus a detailed treatment of all the algebraic
steps needed to carry out the analysis. Cavalier statements such as ‚Äúit fol-
lows immediately‚Äù, or ‚Äúit is easy to show‚Äù, are encountered frequently in
the statistical literature and cause much frustration to biological scientists,
even to numerate ones. For this reason, we oÔ¨Äer considerably more detail in
the developments than what may be warranted for a more mathematically
apt audience. We do not apologize for this, and hope that this approach
will be viewed sympathetically by the scientiÔ¨Åc community to which we
belong. Nevertheless, some mathematical and statistical prerequisites are
needed in order to be able to extract maximum beneÔ¨Åt from the mate-
rial presented in this book. These include a beginning course in diÔ¨Äerential
and integral calculus, an exposure to elementary linear algebra (preferably
with a statistical bent), an understanding of probability theory and of the
concepts of statistical inference, and a solid grounding in the applications
of mixed eÔ¨Äects linear models. Most students of quantitative genetics and
animal breeding acquire this preparation during the Ô¨Årst two years of their
graduate education, so we do not feel that the requirements are especially
stringent. Some applied statisticians reading this book may be caught by
the quantitative genetics jargon. However, we attempt to relate biological
to statistical parameters and we trust that the meaning will become clear
from the context.

vii
The book is organized into four parts. Part I (Chapters 1 and 2) presents
a review of probability and distribution theory. Random variables and their
distributions are introduced and illustrated. This is followed by a discus-
sion on functions of random variables. Applied and theoretical statisticians
can skip this part of the book safely, although they may Ô¨Ånd some of the
examples interesting.
The Ô¨Årst part lays the background needed for introducing methods of
inference, which is the subject of the seven chapters in Part II. Chapters
3 and 4 cover the classical theory of likelihood inference. Properties of
the maximum likelihood estimator and tests of hypotheses based on the
Neyman‚ÄìPearson theory are discussed. An eÔ¨Äort has been made to derive,
in considerable detail, many of the important asymptotic results and sev-
eral examples are given. The problems encountered in likelihood inference
under the presence of nuisance parameters are discussed and illustrated.
Chapter 4 ends with a presentation of models for which the likelihood does
not have a closed form. Bayesian inference is the subject of chapters 5-8
in Part II . Chapter 5 provides the essential ingredients of the Bayesian
approach. This is followed by a chapter covering in fair detail the analysis
of the linear model. Chapter 7 discusses the role of the prior distribution
in Bayesian analysis. After a short tour of Bayesian asymptotics, the con-
cepts of statistical information and entropy are introduced. This is followed
by a presentation of Bayesian analysis using prior distributions conveying
vague prior knowledge, perhaps the most contentious topic of the Bayesian
paradigm. The chapter ends with an overview of a technically diÔ¨Écult topic
called reference analysis. Chapter 8 deals brieÔ¨Çy with hypothesis testing
from a Bayesian perspective. Chapter 9, the Ô¨Ånal one of this second part,
provides an introduction to the expectation‚Äìmaximization (EM) algorithm,
a topic which has had far-reaching inÔ¨Çuences in the statistical genetics lit-
erature. This algorithm is extremely versatile, and is so inextricable from
the statistical structure of a likelihood or Bayesian problem that we opted
to include it in this part of the book.
The Ô¨Årst two parts of the book described above provide the basis for
positing probability models. The implementation and validation of models
via MCMC requires some insight on the subtleties on which this technique is
based. This is presented in Part III, whose intent is to explain this remark-
able computational tool, within the constraints imposed by the authors‚Äô
limited mathematics. After an introduction to discrete Markov chains in
Chapter 10, the MCMC procedures are discussed in a detailed manner in
Chapter 11. An inquisitive reader should be able to follow the derivation of
the acceptance probability of various versions of the celebrated Metropolis‚Äì
Hastings algorithm, including reversible jump. An overview of methods for
analyzing MCMC output is the subject of Chapter 12.
Part IV gives a presentation of some of the models that are being used
in quantitative genetics at present. The treatment is mostly Bayesian and
the models are implemented via MCMC. The classical Gaussian mixed

viii
model for single- and multiple-trait analyses is described in Chapter 13.
Extensions are given for robust analyses using t distributions. The Bayesian
MCMC implementation of this robust analysis requires minor changes in
a code previously developed for analyzing Gaussian models, illustrating
the remarkable versatility of the MCMC techniques. Chapter 14 discusses
analyses involving ordered categorical traits based on the threshold model
of Sewall Wright. This chapter also includes a Bayesian MCMC descrip-
tion of a model for joint analysis of categorical and Gaussian responses.
Chapter 15 deals with models for the analysis of longitudinal data, and
the book concludes with Chapter 16, which introduces segregation analysis
and models for the detection of quantitative trait loci.
Although this book can be used as a text, it cannot claim such status
fully. A textbook requires carefully chosen exercises, and probably a more
linear development than the one presented here. Hence, these elements will
need to be provided by the instructor, should this book be considered for
classroom use. We have decided not to discuss software issues, although
some reasonably powerful public domain programs are already available.
The picture in this area is changing too rapidly, and we felt that many of
our views or recommendations in this respect would probably be rendered
obsolete at the time of publication.
The book evolved from cooperation between the two authors with col-
leagues from Denmark and Wisconsin leading to a series of papers in which
the Ô¨Årst applications in animal breeding of Bayesian hierarchical models
computed via MCMC methods were reported. Subsequently, we were in-
vited to teach or coteach courses in Likelihood and Bayesian MCMC analy-
sis at Ames (USA), Armidale (Australia), Buenos Aires (Argentina), Edin-
burgh (Scotland), Guelph (Canada), Jokioinen (Finland), Li`ege (Belgium),
Lleida (Spain), Madison (USA), Madrid (Spain), Milan (Italy), Montecillo
(Mexico), Piracicaba (Brazil), Ribeirao Preto (Brazil), Toulouse (France),
Uppsala (Sweden), Valencia (Spain), and Vi¬∏cosa (Brazil). While in the
course of these teaching experiences, we thought it would be useful to amal-
gamate some of our ideas in book form. What we hope you will read is the
result of several iterations, starting from a monograph written by Daniel
Sorensen and entitled ‚ÄúGibbs Sampling in Quantitative Genetics‚Äù. This was
published Ô¨Årst in 1996 as Internal Report No. 82 by the Danish Institute
of Agricultural Sciences (DIAS).
Colleagues, friends, and loved ones have contributed in a variety of
ways toward the making of this book. Carlos Becerril, Agust¬¥ƒ±n Blasco,
Rohan Fernando, Bernt Guldbrandtsen (who also made endless contribu-
tions with LaTeX related problems), Larry SchaeÔ¨Äer, and Bruce Walsh
worked through a large part of the manuscript. SpeciÔ¨Åc chapters were read
by Anders Holst Andersen, Jos¬¥e Miguel Bernardo, Yu-mei Chang, Miguel
P¬¥erez Enciso, Davorka Gulisija, Shyh-Forng Guo, Mark Henryon, Bjorg
Heringstad, Just Jensen, Inge Riis Korsgaard, Mogens Sand√∏ Lund, Nuala

ix
Sheehan, Mikko Sillanp¬®a¬®a, Miguel Angel Toro, and Rasmus Waagepetersen.
We acknowledge their valuable suggestions and corrections. However, we
are solely responsible for the mistakes that evaded scrutiny, as no book
is entirely free of errors. Some of the mistakes Ô¨Ånd a place in the book by
what one may mercifully call random accidents. Other mistakes may reÔ¨Çect
incomplete knowledge of the topic on our side. We would be grateful if we
could be made aware of these errors.
We wish to thank colleagues at the Department of Animal Breeding and
Genetics, DIAS, and at the Departments of Animal Sciences and of Dairy
Science of the University of Wisconsin-Madison for providing an intellectu-
ally stimulating and socially pleasant atmosphere. We are in special debt
to Bernt Bech Andersen for much support and encouragement, and for
providing a rare commodity: intellectual space.
We acknowledge John Kimmel from Springer-Verlag for encouragement
and patience. Tony Orrantia, also from Springer-Verlag, is thanked for his
sharp professional editing.
DG wishes to thank Arthur B. Chapman for his inÔ¨Çuential mentoring and
for his views on the ethics of science, the late Charles R. Henderson for his
pioneering work in linear models in animal breeding, and my colleagues and
friends Jean-Louis Foulley, Rohan Fernando, and Sotan Im, from whom I
learned much. DG had to Ô¨Åt the book into a rather hectic schedule of
lecturing and research, both at home and overseas. This took much time
away from Graciela, Magdalena, and Daniel Santiago, but they always gave
me love, support and encouragement. I also wish to thank Gorgias and
Alondra (my parents), the late Tatu, Morocha, and H¬¥ector, and Chiquita,
Mumu, Arturo, and Cecilia for their love.
This book was written ‚Äúat work‚Äù, at home, in airports and in hotels,
on week-ends and on vacation. Irrespective of place, DS received consistent
support from Maiken, Jon, and Elsebeth. They accepted that I was unavail-
able, and put up with moments of frustration (often in good spirit) when
things did not work out. I was inÔ¨Çuenced by and am in debt to my early
teachers in Reading and Edinburgh, especially Robert Curnow, Bill Hill,
and Alan Robertson. Brian Kennedy introduced me to mixed linear model
theory while I was a post-doc in Davis, California, and later in Guelph. I
have learned much from him. To my parents I owe unveiling for me at an
early age, that part of life that thrives on the top of trees, in worlds of
reason and poetry, where it Ô¨Ånds its space and achieves its splendor.

This page intentionally left blank

Contents
Preface
v
I
Review of Probability and Distribution Theory
1
1
Probability and Random Variables
3
1.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . .
3
1.2
Univariate Discrete Distributions . . . . . . . . . . . . . . .
4
1.2.1
The Bernoulli and Binomial Distributions . . . . . .
7
1.2.2
The Poisson Distribution
. . . . . . . . . . . . . . .
10
1.2.3
Binomial Distribution: Normal Approximation
. . .
12
1.3
Univariate Continuous Distributions . . . . . . . . . . . . .
13
1.3.1
The Uniform, Beta, Gamma, Normal,
and Student-t Distributions . . . . . . . . . . . . . .
18
1.4
Multivariate Probability Distributions . . . . . . . . . . . .
29
1.4.1
The Multinomial Distribution . . . . . . . . . . . . .
37
1.4.2
The Dirichlet Distribution . . . . . . . . . . . . . . .
40
1.4.3
The d-Dimensional Uniform Distribution
. . . . . .
40
1.4.4
The Multivariate Normal Distribution . . . . . . . .
41
1.4.5
The Chi-square Distribution . . . . . . . . . . . . . .
53
1.4.6
The Wishart and Inverse Wishart Distributions . . .
55
1.4.7
The Multivariate-t Distribution . . . . . . . . . . . .
60
1.5
Distributions with Constrained Sample Space . . . . . . . .
62
1.6
Iterated Expectations
. . . . . . . . . . . . . . . . . . . . .
67

xii
Contents
2
Functions of Random Variables
77
2.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . .
77
2.2
Functions of a Single Random Variable . . . . . . . . . . . .
78
2.2.1
Discrete Random Variables . . . . . . . . . . . . . .
78
2.2.2
Continuous Random Variables
. . . . . . . . . . . .
79
2.2.3
Approximating the Mean and Variance
. . . . . . .
89
2.2.4
Delta Method . . . . . . . . . . . . . . . . . . . . . .
93
2.3
Functions of Several Random Variables . . . . . . . . . . . .
95
2.3.1
Linear Transformations
. . . . . . . . . . . . . . . .
111
2.3.2
Approximating the Mean and Covariance Matrix . .
114
II
Methods of Inference
117
3
An Introduction to Likelihood Inference
119
3.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . .
119
3.2
The Likelihood Function . . . . . . . . . . . . . . . . . . . .
120
3.3
The Maximum Likelihood Estimator . . . . . . . . . . . . .
122
3.4
Likelihood Inference in a Gaussian Model
. . . . . . . . . .
125
3.5
Fisher‚Äôs Information Measure . . . . . . . . . . . . . . . . .
128
3.5.1
Single Parameter Case . . . . . . . . . . . . . . . . .
128
3.5.2
Alternative Representation of Information . . . . . .
131
3.5.3
Mean and Variance of the Score Function . . . . . .
134
3.5.4
Multiparameter Case . . . . . . . . . . . . . . . . . .
135
3.5.5
Cram¬¥er‚ÄìRao Lower Bound
. . . . . . . . . . . . . .
138
3.6
SuÔ¨Éciency . . . . . . . . . . . . . . . . . . . . . . . . . . . .
142
3.7
Asymptotic Properties: Single Parameter Models . . . . . .
143
3.7.1
Probability of the Data Given the Parameter . . . .
144
3.7.2
Consistency . . . . . . . . . . . . . . . . . . . . . . .
146
3.7.3
Asymptotic Normality and EÔ¨Éciency . . . . . . . . .
147
3.8
Asymptotic Properties: Multiparameter Models . . . . . . .
152
3.9
Functional Invariance
. . . . . . . . . . . . . . . . . . . . .
153
3.9.1
Illustration of Functional Invariance
. . . . . . . . .
153
3.9.2
Invariance in a Single Parameter Model . . . . . . .
157
3.9.3
Invariance in a Multiparameter Model . . . . . . . .
159
4
Further Topics in Likelihood Inference
161
4.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . .
161
4.2
Computation of Maximum Likelihood Estimates
. . . . . .
162
4.3
Evaluation of Hypotheses
. . . . . . . . . . . . . . . . . . .
166
4.3.1
Likelihood Ratio Tests . . . . . . . . . . . . . . . . .
166
4.3.2
ConÔ¨Ådence Regions . . . . . . . . . . . . . . . . . . .
177
4.3.3
Wald‚Äôs Test . . . . . . . . . . . . . . . . . . . . . . .
179
4.3.4
Score Test . . . . . . . . . . . . . . . . . . . . . . . .
179
4.4
Nuisance Parameters . . . . . . . . . . . . . . . . . . . . . .
181

Contents
xiii
4.4.1
Loss of EÔ¨Éciency Due to Nuisance Parameters
. . .
182
4.4.2
Marginal Likelihoods . . . . . . . . . . . . . . . . . .
182
4.4.3
ProÔ¨Åle Likelihoods . . . . . . . . . . . . . . . . . . .
186
4.5
Analysis of a Multinomial Distribution . . . . . . . . . . . .
190
4.5.1
Amount of Information per Observation . . . . . . .
199
4.6
Analysis of Linear Logistic Models . . . . . . . . . . . . . .
202
4.6.1
The Logistic Distribution . . . . . . . . . . . . . . .
204
4.6.2
Likelihood Function under Bernoulli Sampling
. . .
204
4.6.3
Mixed EÔ¨Äects Linear Logistic Model . . . . . . . . .
208
5
An Introduction to Bayesian Inference
211
5.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . .
211
5.2
Bayes Theorem: Discrete Case . . . . . . . . . . . . . . . . .
214
5.3
Bayes Theorem: Continuous Case . . . . . . . . . . . . . . .
223
5.4
Posterior Distributions . . . . . . . . . . . . . . . . . . . . .
235
5.5
Bayesian Updating . . . . . . . . . . . . . . . . . . . . . . .
249
5.6
Features of Posterior Distributions . . . . . . . . . . . . . .
257
5.6.1
Posterior Probabilities . . . . . . . . . . . . . . . . .
258
5.6.2
Posterior Quantiles . . . . . . . . . . . . . . . . . . .
262
5.6.3
Posterior Modes
. . . . . . . . . . . . . . . . . . . .
264
5.6.4
Posterior Mean Vector and Covariance Matrix
. . .
280
6
Bayesian Analysis of Linear Models
287
6.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . .
287
6.2
The Linear Regression Model . . . . . . . . . . . . . . . . .
287
6.2.1
Inference under Uniform Improper Priors
. . . . . .
288
6.2.2
Inference under Conjugate Priors . . . . . . . . . . .
297
6.2.3
Orthogonal Parameterization of the Model
. . . . .
307
6.3
The Mixed Linear Model . . . . . . . . . . . . . . . . . . . .
313
6.3.1
Bayesian View of the Mixed EÔ¨Äects Model . . . . . .
313
6.3.2
Joint and Conditional Posterior Distributions . . . .
317
6.3.3
Marginal Distribution of Variance Components . . .
322
6.3.4
Marginal Distribution of Location Parameters . . . .
323
7
The Prior Distribution and Bayesian Analysis
327
7.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . .
327
7.2
An Illustration of the EÔ¨Äect of Priors on Inferences . . . . .
328
7.3
A Rapid Tour of Bayesian Asymptotics
. . . . . . . . . . .
330
7.3.1
Discrete Parameter . . . . . . . . . . . . . . . . . . .
330
7.3.2
Continuous Parameter . . . . . . . . . . . . . . . . .
331
7.4
Statistical Information and Entropy
. . . . . . . . . . . . .
334
7.4.1
Information . . . . . . . . . . . . . . . . . . . . . . .
334
7.4.2
Entropy of a Discrete Distribution . . . . . . . . . .
337
7.4.3
Entropy of a Joint and Conditional Distribution
. .
340
7.4.4
Entropy of a Continuous Distribution
. . . . . . . .
341

xiv
Contents
7.4.5
Information about a Parameter . . . . . . . . . . . .
346
7.4.6
Fisher‚Äôs Information Revisited
. . . . . . . . . . . .
351
7.4.7
Prior and Posterior Discrepancy
. . . . . . . . . . .
353
7.5
Priors Conveying Little Information
. . . . . . . . . . . . .
356
7.5.1
The Uniform Prior . . . . . . . . . . . . . . . . . . .
356
7.5.2
Other Vague Priors . . . . . . . . . . . . . . . . . . .
358
7.5.3
Maximum Entropy Prior Distributions . . . . . . . .
367
7.5.4
Reference Prior Distributions . . . . . . . . . . . . .
379
8
Bayesian Assessment of Hypotheses and Models
399
8.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . .
399
8.2
Bayes Factors . . . . . . . . . . . . . . . . . . . . . . . . . .
400
8.2.1
DeÔ¨Ånition . . . . . . . . . . . . . . . . . . . . . . . .
400
8.2.2
Interpretation . . . . . . . . . . . . . . . . . . . . . .
402
8.2.3
The Bayes Factor and Hypothesis Testing . . . . . .
403
8.2.4
InÔ¨Çuence of the Prior Distribution . . . . . . . . . .
412
8.2.5
Nested Models
. . . . . . . . . . . . . . . . . . . . .
414
8.2.6
Approximations to the Bayes Factor . . . . . . . . .
418
8.2.7
Partial and Intrinsic Bayes Factors . . . . . . . . . .
422
8.3
Estimating the Marginal Likelihood
. . . . . . . . . . . . .
424
8.4
Goodness of Fit and Model Complexity
. . . . . . . . . . .
429
8.5
Goodness of Fit and Predictive Ability of a Model
. . . . .
433
8.5.1
Analysis of Residuals . . . . . . . . . . . . . . . . . .
434
8.5.2
Predictive Ability and Predictive Cross-Validation .
436
8.6
Bayesian Model Averaging . . . . . . . . . . . . . . . . . . .
439
8.6.1
General . . . . . . . . . . . . . . . . . . . . . . . . .
439
8.6.2
DeÔ¨Ånitions
. . . . . . . . . . . . . . . . . . . . . . .
440
8.6.3
Predictive Ability of BMA . . . . . . . . . . . . . . .
441
9
Approximate Inference Via the EM Algorithm
443
9.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . .
443
9.2
Complete and Incomplete Data . . . . . . . . . . . . . . . .
444
9.3
The EM Algorithm . . . . . . . . . . . . . . . . . . . . . . .
445
9.3.1
Form of the Algorithm . . . . . . . . . . . . . . . . .
445
9.3.2
Derivation . . . . . . . . . . . . . . . . . . . . . . . .
445
9.4
Monotonic Increase of ln p (Œ∏|y) . . . . . . . . . . . . . . . .
447
9.5
The Missing Information Principle . . . . . . . . . . . . . .
448
9.5.1
Complete, Observed and Missing Information . . . .
448
9.5.2
Rate of Convergence of the EM Algorithm . . . . . .
449
9.6
EM Theory for Exponential Families . . . . . . . . . . . . .
451
9.7
Standard Errors and Posterior Standard Deviations . . . . .
452
9.7.1
The Method of Louis . . . . . . . . . . . . . . . . . .
453
9.7.2
Supplemented EM Algorithm (SEM) . . . . . . . . .
454
9.7.3
The Method of Oakes
. . . . . . . . . . . . . . . . .
457
9.8
Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . .
458

Contents
xv
III
Markov Chain Monte Carlo Methods
475
10 An Overview of Discrete Markov Chains
477
10.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . .
477
10.2 DeÔ¨Ånitions . . . . . . . . . . . . . . . . . . . . . . . . . . . .
478
10.3 State of the System after n-Steps . . . . . . . . . . . . . . .
479
10.4 Long-Term Behavior of the Markov Chain . . . . . . . . . .
481
10.5 Stationary Distribution
. . . . . . . . . . . . . . . . . . . .
481
10.6 Aperiodicity and Irreducibility
. . . . . . . . . . . . . . . .
483
10.7 Reversible Markov Chains . . . . . . . . . . . . . . . . . . .
487
10.8 Limiting Behavior
. . . . . . . . . . . . . . . . . . . . . . .
492
11 Markov Chain Monte Carlo
497
11.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . .
497
11.2 Preliminaries
. . . . . . . . . . . . . . . . . . . . . . . . . .
498
11.2.1 Notation . . . . . . . . . . . . . . . . . . . . . . . . .
498
11.2.2 Transition Kernels . . . . . . . . . . . . . . . . . . .
499
11.2.3 Varying Dimensionality
. . . . . . . . . . . . . . . .
499
11.3 An Overview of Markov Chain Monte Carlo . . . . . . . . .
500
11.4 The Metropolis‚ÄìHastings Algorithm
. . . . . . . . . . . . .
502
11.4.1 An Informal Derivation
. . . . . . . . . . . . . . . .
502
11.4.2 A More Formal Derivation . . . . . . . . . . . . . . .
504
11.5 The Gibbs Sampler . . . . . . . . . . . . . . . . . . . . . . .
509
11.5.1 Fully Conditional Posterior Distributions
. . . . . .
510
11.5.2 The Gibbs Sampling Algorithm . . . . . . . . . . . .
510
11.6 Langevin‚ÄìHastings Algorithm . . . . . . . . . . . . . . . . .
517
11.7 Reversible Jump MCMC . . . . . . . . . . . . . . . . . . . .
517
11.7.1 The Invariant Distribution . . . . . . . . . . . . . . .
518
11.7.2 Generating the Proposal . . . . . . . . . . . . . . . .
519
11.7.3 Specifying the Reversibility Condition . . . . . . . .
520
11.7.4 Derivation of the Acceptance Probability
. . . . . .
522
11.7.5 Deterministic Proposals . . . . . . . . . . . . . . . .
523
11.7.6 Generating Proposals via the Identity Mapping . . .
525
11.8 Data Augmentation
. . . . . . . . . . . . . . . . . . . . . .
532
12 Implementation and Analysis of MCMC Samples
539
12.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . .
539
12.2 A Single Long Chain or Several Short Chains?
. . . . . . .
540
12.3 Convergence Issues . . . . . . . . . . . . . . . . . . . . . . .
541
12.3.1 EÔ¨Äect of Posterior Correlation on Convergence
. . .
541
12.3.2 Monitoring Convergence . . . . . . . . . . . . . . . .
547
12.4 Inferences from the MCMC Output . . . . . . . . . . . . . .
550
12.4.1 Estimators of Posterior Quantities . . . . . . . . . .
550
12.4.2 Monte Carlo Variance . . . . . . . . . . . . . . . . .
553
12.5 Sensitivity Analysis . . . . . . . . . . . . . . . . . . . . . . .
556

xvi
Contents
IV
Applications in Quantitative Genetics
561
13 Gaussian and Thick-Tailed Linear Models
563
13.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . .
563
13.2 The Univariate Linear Additive Genetic Model . . . . . . .
564
13.2.1 A Gibbs Sampling Algorithm . . . . . . . . . . . . .
566
13.3 Additive Genetic Model with Maternal EÔ¨Äects
. . . . . . .
570
13.3.1 Fully Conditional Posterior Distributions
. . . . . .
575
13.4 The Multivariate Linear Additive Genetic Model . . . . . .
576
13.4.1 Fully Conditional Posterior Distributions
. . . . . .
580
13.5 A Blocked Gibbs Sampler for Gaussian Linear Models . . .
584
13.6 Linear Models with Thick-Tailed Distributions
. . . . . . .
588
13.6.1 Motivation
. . . . . . . . . . . . . . . . . . . . . . .
588
13.6.2 A Student-t Mixed EÔ¨Äects Model . . . . . . . . . . .
595
13.6.3 Model with Clustered Random EÔ¨Äects . . . . . . . .
600
13.7 Parameterizations and the Gibbs Sampler . . . . . . . . . .
602
14 Threshold Models for Categorical Responses
605
14.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . .
605
14.2 Analysis of a Single Polychotomous Trait
. . . . . . . . . .
607
14.2.1 Sampling Model
. . . . . . . . . . . . . . . . . . . .
607
14.2.2 Prior Distribution and Joint Posterior Density
. . .
608
14.2.3 Fully Conditional Posterior Distributions
. . . . . .
611
14.2.4 The Gibbs Sampler . . . . . . . . . . . . . . . . . . .
615
14.3 Analysis of a Categorical and a Gaussian Trait
. . . . . . .
615
14.3.1 Sampling Model
. . . . . . . . . . . . . . . . . . . .
616
14.3.2 Prior Distribution and Joint Posterior Density
. . .
617
14.3.3 Fully Conditional Posterior Distributions
. . . . . .
619
14.3.4 The Gibbs Sampler . . . . . . . . . . . . . . . . . . .
625
14.3.5 Implementation with Binary Traits . . . . . . . . . .
626
15 Bayesian Analysis of Longitudinal Data
627
15.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . .
627
15.2 Hierarchical or Multistage Models
. . . . . . . . . . . . . .
628
15.2.1 First Stage
. . . . . . . . . . . . . . . . . . . . . . .
629
15.2.2 Second Stage . . . . . . . . . . . . . . . . . . . . . .
634
15.2.3 Third Stage . . . . . . . . . . . . . . . . . . . . . . .
639
15.2.4 Joint Posterior Distribution . . . . . . . . . . . . . .
641
15.3 Two-Step Approximate Bayesian Analysis . . . . . . . . . .
642
15.3.1 Estimating Location Parameters
. . . . . . . . . . .
643
15.3.2 Estimating Dispersion Parameters
. . . . . . . . . .
650
15.3.3 Special Case: Linear First Stage
. . . . . . . . . . .
652
15.4 Computation via Markov Chain Monte Carlo . . . . . . . .
653
15.4.1 Fully Conditional Posterior Distributions
. . . . . .
655
15.5 Analysis with Thick-Tailed Distributions . . . . . . . . . . .
664

Contents
xvii
15.5.1 First- and Second-Stage Models . . . . . . . . . . . .
665
15.5.2 Fully Conditional Posterior Distributions
. . . . . .
666
16 Segregation and Quantitative Trait Loci Analysis
671
16.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . .
671
16.2 Segregation Analysis Models . . . . . . . . . . . . . . . . . .
672
16.2.1 Notation and Model . . . . . . . . . . . . . . . . . .
672
16.2.2 Fully Conditional Posterior Distributions
. . . . . .
675
16.2.3 Some Implementation Issues . . . . . . . . . . . . . .
677
16.3 QTL Models
. . . . . . . . . . . . . . . . . . . . . . . . . .
679
16.3.1 Models with a Single QTL . . . . . . . . . . . . . . .
680
16.3.2 Models with an Arbitrary Number of QTL
. . . . .
690
References
701
List of Citations
727
Subject Index
733

This page intentionally left blank

Part I
Review of Probability and
Distribution Theory
1

This page intentionally left blank

1
Uncertainty, Random Variables, and
Probability Distributions
1.1
Introduction
Suppose there is a data set consisting of observations on body weight taken
on beef animals and that there are questions of scientiÔ¨Åc or practical impor-
tance to be answered from these data. The questions, for example, might
be:
(1) Which are potential factors to be included in a statistical model for
analysis of the records?
(2) What is the amount of additive genetic variance in the reference beef
cattle population?
Answers to these questions are to be obtained from a Ô¨Ånite amount of
information contained in the data at hand. Because information is Ô¨Ånite,
there will be uncertainty associated with the outcome of the analyses. In
this book, we are concerned with learning about the state of nature from
experimental or observational data, and with measuring the degree of un-
certainty at diÔ¨Äerent stages of the learning process using probability mod-
els. This uncertainty is expressed via probabilities; arguably, probability
can be viewed as the mathematics of uncertainty.
This chapter reviews probability models encountered often in quantita-
tive genetic analysis, and that are used subsequently in various parts of
this book. The treatment is descriptive and informal. It is assumed that
the reader will have had an introductory course in probability theory or
in mathematical statistics at the level of, for example, Hogg and Craig
(1995), or Mood et al. (1974). The concept of a random variable is deÔ¨Åned

4
1. Probability and Random Variables
and this is followed by an overview of univariate discrete and continuous
distributions. Multivariate distributions are presented in the last section of
the chapter, with considerable emphasis on the multivariate normal distri-
bution, because of the important role it plays in statistical genetic analysis.
For additional information, readers should consult treatises such as Johnson
and Kotz (1969, 1970a,b, 1972).
1.2
Univariate Discrete Distributions
We start by providing a succinct and intuitive introduction to the concepts
of random variable and probability which is adequate for our purposes.
Informally, a locus is a location in the genome where a gene resides. The
word gene has become a rather elusive concept over recent years (see Keller,
2000, for a discussion). Here, we refer to the gene as the Mendelian ‚Äúfactor‚Äù
transmitted from parent to oÔ¨Äspring. Consider a locus at which there are
two possible variants or alleles: A1 and A2. Envisage the random experiment
consisting of generating genotypes from alleles A1 and A2. There are four
possible outcomes for this experiment, represented by the four possible
genotypes: A1A1, A1A2, A2A1, and A2A2. Any particular outcome cannot
be known in advance; thus the use of the word ‚Äúrandom‚Äù. The set of all
possible outcomes of the experiment constitutes its sample space, denoted
by ‚Ñ¶. ‚ÄúPossible‚Äù means ‚Äúconceptually possible‚Äù and often compromises
between some perception of physical reality and mathematical convenience.
In this simple case, the sample space consists of the four possible genotypes;
that is, ‚Ñ¶= {œâ1, œâ2, œâ3, œâ4}. Each possible outcome œâ is a sample point in
‚Ñ¶. Here, œâ1 = A1A1, œâ2 = A1A2, œâ3 = A2A1, and œâ4 = A2A2.
Typically one is interested not so much on the experiment and on the
totality of its outcomes, but on some consequences of the experiment. For
example, imagine that individuals carrying allele A2 suÔ¨Äer from a measur-
able disease. Let X (œâ) be the function of the sample point œâ, deÔ¨Åned as
follows: X (œâ) = 0 if œâ = œâ1, X (œâ) = 1 if œâ = œâ2, œâ3 or œâ4, where 0 and
1 indicate absence or presence of disease, respectively.
The same gene could have an additive eÔ¨Äect on another trait, such that
allele A2 confers an increase in some measurement. A diÔ¨Äerent function
Y (œâ) of œâ could represent this situation deÔ¨Åned as follows: Y (œâ) = 0 if œâ =
œâ1, Y (œâ) = 1/2 if œâ = œâ2, œâ3, Y (œâ) = 1 if œâ = œâ4. These two functions
map the sample space to the real line R. That is, the functions X (œâ) and
Y (œâ), with domain ‚Ñ¶, make some real number correspond to each outcome
of the experiment. The functions X (œâ) and Y (œâ) are known as random
variables. Some authors prefer the term random quantities (i.e., De Finetti,
1975a; Bernardo and Smith, 1994), because the function in question is not
a variable, but a deterministic mapping from ‚Ñ¶to R. However, in this book,
the usual term, random variable, will be adopted.

1.2 Univariate Discrete Distributions
5
When attention focuses at the level of the random variable, this allows
in some sense, to ignore the (often abstract) sample space ‚Ñ¶. One can
deÔ¨Åne a new (concrete) sample space S in R, to be the range of possible
values that the random variable can take, so that no further mapping is
required. In this case the random variable becomes the identity mapping,
and can be thought of as the variable that describes the result of the random
experiment. In the case of X (œâ), the sample space is S = {0, 1}, whereas
in the case of Y (œâ) the sample space is S =

0, 1
2, 1

. The random variable
X (œâ) will be written simply as X.
To distinguish between a random variable and its realized values, upper
case letters are often used for the former whereas lower case letters are
employed for the latter. This notation becomes awkward when bold face
letters denote matrices (upper case) or vectors (lower case), so this con-
vention is not followed consistently in this book. However, it will be clear
from the context whether one is discussing a random variable or its realized
values.
The sample spaces of the random variables X and Y consist of a count-
able number of values. Such spaces are called discrete and X and Y are
discrete random variables.
The random variable X (whether discrete or continuous) in turn induces
a probability PX on S. This is a function that to each event (subset) G ‚äÜS,
assigns a number in the closed interval [0, 1], deÔ¨Åned by
PX(G) = Pr({œâ ‚àà‚Ñ¶: X(œâ) ‚ààG}) = Pr (X ‚ààG) .
(1.1)
The probability PX is called the distribution of X, which for a discrete
random variable, is completely determined by specifying Pr (X = x) for all
x.
The mathematical model sketched above is quite divorced from the intu-
itive understanding(s) of the concept of probability. The interpretation of
probability of the event G usually adopted in this book, is that it measures
the subjective degree of plausibility or belief in G, conditional on infor-
mation currently available to the subject. An alternative interpretation
of probability is as the long run frequency of occurrence of G with inde-
pendent replications of the random experiment carried out under identical
conditions.
By deÔ¨Ånition, a probability is a number between 0 and 1; both 0 and 1
are included as valid numbers. A probability cannot be negative or larger
than 1. Thus,
0 ‚â§Pr (X = x) ‚â§1, for all x ‚ààS.
Suppose that the sample space S = {x1, x2, . . .} is countable. The prob-
ability function (or probability mass function, abbreviated p.m.f.) of a dis-
crete random variable X is deÔ¨Åned by
p (x) =

Pr (X = x) , if x ‚ààS,
0, otherwise.

6
1. Probability and Random Variables
In distribution theory, the values that X can take are often denoted mass
points, and p (x) is the probability mass associated with the mass point x.
The distribution (1.1) reduces to the p.m.f. when the subset G becomes
a mass point at X = x. The support is the set of values of x for which
the probability function is non-zero. The probability function gives a full
description of the randomness or uncertainty associated with X. It can be
used to calculate the probability that X takes certain values, or that it falls
in a given region.
To illustrate, suppose that an individual is the progeny from crossing
randomly two lines, and that each line consists of genotypes A1A2 only.
Let the random variable X now represent the number of A2 alleles in the
progeny from this cross. From Mendel‚Äôs laws of inheritance, the probability
distribution of X is
Genotype
A2A2
A2A1
A1A1
x
2
1
0
Pr (X = x)
1
4
1
2
1
4
The probability that the number of A2 alleles in a randomly drawn in-
dividual is 2, say, is
Pr({œâ ‚àà‚Ñ¶: X(œâ) = 2}) = 1
4,
abbreviated hereinafter as
Pr (X = 2) = p (2) = 1
4.
(1.2)
In the above example, we have
Pr (X = 0) = 1
4; Pr (X = 1) = 1
2; Pr (X = 2) = 1
4,
for each of the three values that X can take. Similarly,
Pr (X = 1 or X = 2) = 1
2 + 1
4 = 3
4.
To every random variable X there is an associated function called the
cumulative distribution function (c.d.f.) of X. It is denoted by F (x) and
is deÔ¨Åned by
F (x) = Pr (X ‚â§x) , for all x.
(1.3)
The c.d.f. is also a special case of (1.1), when G is the half-open interval
(‚àí‚àû, x]. The notation emphasizes that the c.d.f. is a function of x. For the
example above, for x = 1,
F (1) = p (0) + p (1) = 1
4 + 1
2 = 3
4.
(1.4)

1.2 Univariate Discrete Distributions
7
Since F (¬∑) is deÔ¨Åned for all values of x, not just those in S = {0, 1, 2}, one
can compute for x = 1.5 and for x = 1.99 say, F (1.5) = 1
4 + 1
2 = 3
4 and
F (1.99) = 1
4 + 1
2 = 3
4, respectively. However, for x = 2, F (2) = 1. Thus,
F (x) is a step-function. In this example, the entire c.d.f. is
F (x)
=
0, for x < 0,
F (x)
=
1
4, for 0 ‚â§x < 1,
F (x)
=
3
4, for 1 ‚â§x < 2,
F (x)
=
1, for x ‚â•2.
In general, one writes
F (x) =

t‚â§x
p (t) ,
(1.5)
where t is a ‚Äúdummy‚Äù variable as used in calculus. Another notation that
will be used in this book is
F (x) =

t
I (t ‚â§x) p (t) ,
(1.6)
where I (t ‚â§x) is the indicator function that takes the value 1 if the argu-
ment is satisÔ¨Åed, in this case, t ‚â§x, and zero otherwise.
The distribution function F (x) has the following properties:
‚Ä¢ If x1 ‚â§x2, then F (x1) ‚â§F (x2) .
‚Ä¢ Pr (x1 < X ‚â§x2) = Pr (X ‚â§x2) ‚àíPr (X ‚â§x1) = F (x2) ‚àíF (x1).
‚Ä¢ limx‚Üí‚àûF (x) = 1 and limx‚Üí‚àí‚àûF (x) = 0.
Example 1.1
Point mass
Consider a point mass at i (such that the total probability mass is concen-
trated on i). Then,
F (x) =

0,
x < i,
1,
x ‚â•i.
This property that F is only 0 or 1 is a characteristic of point masses.
‚ñ†
In the following section, some of the most widely used discrete distribu-
tions in genetics are discussed. These are the Bernoulli, binomial and the
Poisson distributions.
1.2.1
The Bernoulli and Binomial Distributions
A random variable X that has a Bernoulli probability distribution can take
either 1 or 0 as possible values (more generally, it can have two modalities

8
1. Probability and Random Variables
only) with probabilities Œ∏ and 1 ‚àíŒ∏, respectively. The quantity Œ∏ is called
a parameter of the distribution of X and, in practice, this may be known
or unknown. The p.m.f. of X is
Pr (X = x|Œ∏) = Br (x|Œ∏)

Œ∏x (1 ‚àíŒ∏)1‚àíx ,
for x = 0, 1,
0,
otherwise,
(1.7)
where 0 ‚â§Œ∏ ‚â§1 and Br is an abbreviation for Bernoulli.
In general, the notation Br (Œ∏), say, will be used to specify a distri-
bution, in this case the Bernoulli distribution with parameter Œ∏, whereas
Br (x|Œ∏) refers to the corresponding p.m.f. of X. The notation for the p.m.f.
Pr (X = x|Œ∏) is used to stress the dependence on the parameter Œ∏.
Often a random variable X having a Bernoulli p.m.f. is referred to as say-
ing that X has a Bernoulli distribution. This phraseology will be adopted
in this book for other random variables (discrete or continuous) having a
named p.m.f. or probability density function (abbreviated p.d.f. and deÔ¨Åned
in Section 1.3).
Suppose the frequency of allele A in a population is 0.34 (the gene fre-
quency is given by the number of A alleles found, divided by twice the
number of individuals in a diploid organism). Then one may postulate that
a randomly drawn allele is a Bernoulli random variable with parameter
Œ∏ = 0.34. If Œ∏ is known, one can specify the probability distribution of X
exactly. In this case, there will be uncertainty about the values that X can
take, but not about the value of Œ∏. If Œ∏ is unknown, there are two (inti-
mately related) consequences. First, the Bernoulli random variables in the
sequence X1, X2, . . . are no longer independent (the concept of indepen-
dence is discussed below and in Section 1.4). Second, there will be an extra
source of uncertainty associated with X, and this will require introducing
an additional probability distribution for Œ∏ in order to calculate the proba-
bility distribution of X correctly (accounting for the uncertainty about Œ∏).
This is a central problem in statistical inference, as will be seen later in this
book. Typically, (1.7) cannot be speciÔ¨Åed free of error about the parameter,
because Œ∏ must be inferred from previous considerations, or from a Ô¨Ånite
set of data at hand. Clearly, it is somewhat disturbing to use an estimated
value of Œ∏ and then proceed as if (1.7) were the ‚Äútrue‚Äù distribution. In this
book, procedures for taking such ‚Äúerrors‚Äù into account will be described.
The mean of the Bernoulli distribution is obtained as follows:
E (X|Œ∏) = 0 √ó Pr (X = 0|Œ∏) + 1 √ó Pr (X = 1|Œ∏) = Œ∏,
(1.8)
where E (¬∑) denotes expected or average value. Similarly,
E

X2|Œ∏

= 02 √ó Pr (X = 0|Œ∏) + 12 √ó Pr (X = 1|Œ∏) = Œ∏.
Then, by deÔ¨Ånition of the variance of a random variable, abbreviated as
V ar hereinafter,
V ar (X|Œ∏) = E

X2|Œ∏

‚àíE2 (X|Œ∏) = Œ∏ ‚àíŒ∏2 = Œ∏ (1 ‚àíŒ∏) .
(1.9)

1.2 Univariate Discrete Distributions
9
A random variable X has a binomial distribution (the term process will
be used interchangeably) if its p.m.f. has the form
Pr (X = x|Œ∏, n) = Bi (x|Œ∏, n) =
Ô£±
Ô£≤
Ô£≥

n
x

Œ∏x (1 ‚àíŒ∏)n‚àíx ,
x = 0, 1, ..., n,
0,
otherwise,
(1.10)
where 0 ‚â§Œ∏ ‚â§1 and n ‚â•1. Here the parameters of the process are Œ∏ and
n, and inferences about X are conditional on these parameters. In a se-
quence of n identical and independent Bernoulli trials, each with ‚Äúsuccess‚Äù
probability Œ∏, deÔ¨Åne the random variables Y1, Y2, . . . , Yn as follows:
Yi =

1,
with probability Œ∏,
0,
with probability 1 ‚àíŒ∏.
Then the random variable X = n
i=1 Yi has the binomial distribution
Bi (Œ∏, n). The mean and variance of X are readily seen to be equal to nŒ∏
and to nŒ∏ (1 ‚àíŒ∏), respectively.
The binomial distribution thus stems from consideration of n mutually
independent Bernoulli random variables all having the same parameter Œ∏.
Mutual independence means that knowledge of the value of some subset
of the n Bernoulli random variables does not alter the state of uncertainty
about the remaining ones. For example, in the two-variable case, if two
coins are tossed by two diÔ¨Äerent individuals, having observed the outcome
of one of the tosses will not modify the state of uncertainty about the
second toss.
Example 1.2
Number of females in a sibship of size 3
Let the random variable X denote the total number of females observed in
three randomly chosen births, so n = 3 and the value of X ranges from 0
to 3. Assume that Œ∏, the probability of a female birth, is known and equal
to Œ∏ = 0.49. Then
Pr (X = 0|Œ∏, n)
=
Bi (0|0.49, 3) = 0.1327,
Pr (X = 1|Œ∏, n)
=
Bi (1|0.49, 3) = 0.3823,
Pr (X = 2|Œ∏, n)
=
Bi (2|0.49, 3) = 0.3674,
Pr (X = 3|Œ∏, n)
=
Bi (3|0.49, 3) = 0.1176.
The events are mutually exclusive and exhaustive because, for example, if
X = 0, then all other values of the random variable are excluded. Hence,
the probability that X takes at least one of these four values is equal to 1.
Further, the probability that X is equal to 0 or 2 is
Pr (X = 0 or X = 2|Œ∏, n) = Pr (X = 0|Œ∏, n) + Pr (X = 2|Œ∏, n)
= 0.1327 + 0.3674 = 0.5001.

10
1. Probability and Random Variables
When events are disjoint (mutually exclusive), the probability that either
one or another event of interest occurs is given by the sum of their elemen-
tary probabilities, as in the above case.
‚ñ†
Example 1.3
Simulating a binomial random variable
A simple way of simulating a binomial random variable X ‚àºBi (Œ∏, n) is
to generate n independent standard uniforms and to set X equal to the
number of uniform variates that are less than or equal to Œ∏ (Gelman et al.,
1995).
‚ñ†
1.2.2
The Poisson Distribution
When Œ∏ is small and n is large, an approximation to Bi (x|Œ∏, n) is obtained
as follows. Let nŒ∏ = Œª be a new parameter. Expanding the binomial coef-
Ô¨Åcient in (1.10) one obtains, for X = k,
Bi (k|Œ∏, n) = n (n ‚àí1) ... (n ‚àík + 1)
k!
Œ∏k (1 ‚àíŒ∏)n‚àík
= n
n
n ‚àí1
n
¬∑ ¬∑ ¬∑ n ‚àík + 1
n
√ó Œªk
k!

1 ‚àíŒª
n
n 
1 ‚àíŒª
n
‚àík
.
(1.11)
Suppose that n ‚Üí‚àûand Œ∏ ‚Üí0, such that nŒ∏ = Œª remains constant.
Taking limits, when n ‚Üí‚àû,
lim
n‚Üí‚àûBi (k|Œ∏, n) = lim
n‚Üí‚àû

n ‚àí1
n
¬∑ ¬∑ ¬∑ n ‚àík + 1
n

1 ‚àíŒª
n
‚àík
√óŒªk
k! lim
n‚Üí‚àû

1 ‚àíŒª
n
n
= Œªk
k! lim
n‚Üí‚àû

1 ‚àíŒª
n
n
.
(1.12)
Recall the binomial expansion
(a + b)n =
n

x=0

n
x

bxan‚àíx.
Put a = 1 and b = ‚àíŒª/n. Then

1 ‚àíŒª
n
n
=
n

x=0

n
x
 
‚àíŒª
n
x
=
n

x=0
n
n
(n ‚àí1)
n
¬∑ ¬∑ ¬∑ (n ‚àíx + 1)
n
(‚àíŒª)x
x!
,

1.2 Univariate Discrete Distributions
11
so
lim
n‚Üí‚àû

1 ‚àíŒª
n
n
=
‚àû

x=0
(‚àíŒª)x
x!
.
(1.13)
A Taylor series expansion of exp (V ) about 0 yields
exp (V ) = 1 + V + V 2
2! + V 3
3! + ¬∑ ¬∑ ¬∑
so if the series has an inÔ¨Ånite number of terms, this is expressible as
exp (V ) =
‚àû

x=0
V x
x! .
Making use of this in (1.13), with V = ‚àíŒª, leads directly to
lim
n‚Üí‚àû

1 ‚àíŒª
n
n
= exp (‚àíŒª) .
Finally, employing this in (1.12) gives the result
lim
n‚Üí‚àûBi (k|Œ∏, n) = Œªk exp (‚àíŒª)
k!
.
(1.14)
This gives an approximation to the binomial probabilities for situations
where n is ‚Äúvery large‚Äù and the probability Œ∏ is ‚Äúvery small‚Äù. This may
be useful, for example, for calculating the probability of Ô¨Ånding a certain
number of mutants in a population when the mutation rate is low.
In fact, approximation (1.14) plays an important role in distribution the-
ory in statistics: a random variable X is said to have a Poisson distribution
with parameter Œª (this being strictly positive, by deÔ¨Ånition) if its proba-
bility function is
Pr (X = x|Œª) = Po (x|Œª) =

Œªx exp (‚àíŒª) /x!,
for x = 0, 1, 2, . . . , ‚àû,
0,
otherwise,
(1.15)
where Po is an abbreviation for Poisson. Here, X is a discrete random
variable, and its sample space is countably inÔ¨Ånite. This distribution could
feature in models for quantitative trait loci (QTL) detection, and may
be suitable, for example, for describing the occurrence of family sizes in
animals with potentially large sibships, such as insects or Ô¨Åsh. It has been
used in animal breeding in the analysis of litter size in pigs and sheep.
Example 1.4
Exposure of mice to carcinogens
In order to illustrate the relationship between the binomial and the Poisson
distributions, suppose there is a 2% chance that a mouse exposed to a
certain carcinogen will develop cancer. What is the probability that two

12
1. Probability and Random Variables
or more mice will develop cancer within a group of 60 experimental mice
exposed to the causative agent? Let the random variable X describe the
number of mice that develop cancer, and suppose that the sampling model
Bi (x|0.02, 60) is tenable. Using (1.10), the probability that two or more
mice will have a carcinoma is computed to be
Pr (X ‚â•2|Œ∏, n) = 1 ‚àíPr (X ‚â§1|Œ∏, n)
= 1 ‚àíPr (X = 0|Œ∏, n) ‚àíPr (X = 1|Œ∏, n)
= 0.3381.
However, given that Œ∏ = 0.02 and n = 60 seem (in a rather arbitrary
manner) reasonably small and large, respectively, the above probability is
approximated using the Poisson distribution Po (1.2), with its parameter
arrived at as Œª = 0.02 √ó 60 = 1.2. The probability that k mice will develop
cancer is given by
Pr (X = k|Œª = 1.2) = e‚àí1.2 (1.2)k
k!
.
Therefore,
Pr (X ‚â•2|Œª = 1.2) = 1 ‚àíPr (X ‚â§1|Œª = 1.2)
= 1 ‚àíPr (X = 0|Œª = 1.2) ‚àíPr (X = 1|Œª = 1.2)
= 1 ‚àí1 + 1.2
e1.2
= 0.3374,
so the approximation is satisfactory, at least in this case.
‚ñ†
1.2.3
Binomial Distribution: Normal Approximation
Another useful approximation to the binomial distribution is based on the
central limit theorem, to be discussed later in this chapter. Let X be a
binomial variable, deÔ¨Åned as X = n
i=1 Ui, where the Ui‚Äôs are independent
and identically distributed (for short, i.i.d.) Br (Œ∏) random variables. Then,
as given in (1.8) and in (1.9), E (Ui) = Œ∏ and V ar (Ui) = Œ∏ (1 ‚àíŒ∏). There-
fore, the algebra of expectations yields E (X) = n
i=1 E (Ui) = nŒ∏ and
V ar (X) = n
i=1 V ar (Ui) = nŒ∏ (1 ‚àíŒ∏). The formula for the variance is
a consequence of the independence assumption made about the Bernoulli
variates, so the variance of a sum is the sum of the variances. Now, the
random variable
Z =
X ‚àínŒ∏
[nŒ∏ (1 ‚àíŒ∏)]1/2
(1.16)
has expectation and variance equal to 0 and 1, respectively. By virtue of
the central limit theorem, for large n, Z is approximately distributed as an
N (0, 1) random variable, which stands for a standard, normally distributed

1.3 Univariate Continuous Distributions
13
random variable (it is assumed that the reader is somewhat familiar with
the normal or Gaussian distribution, but it will be discussed subsequently).
If X ‚àºBi (Œ∏, n), given Ô¨Åxed numbers x1 and x2, x1 < x2, as n ‚Üí‚àû, it
follows, from the approximation above, that
Pr (x1 < X ‚â§x2) = Pr

x1 ‚àínŒ∏
[nŒ∏ (1 ‚àíŒ∏)]1/2 <
X ‚àínŒ∏
[nŒ∏ (1 ‚àíŒ∏)]1/2 ‚â§
x2 ‚àínŒ∏
[nŒ∏ (1 ‚àíŒ∏)]1/2

= Pr (z1 < Z ‚â§z2) = Pr (Z ‚â§z2) ‚àíPr (Z ‚â§z1)
‚âàŒ¶ (z2) ‚àíŒ¶ (z1) ,
(1.17)
where zi = (xi ‚àínŒ∏)/ [nŒ∏ (1 ‚àíŒ∏)]1/2, and Œ¶ (¬∑) is the c.d.f. of a standard
normal random variable.
For the mouse example, the probability that two or more mice will de-
velop cancer can be approximated as
Pr (X ‚â•2)
=
Pr

X ‚àínŒ∏
[nŒ∏ (1 ‚àíŒ∏)]1/2 ‚â•
2 ‚àínŒ∏
[nŒ∏ (1 ‚àíŒ∏)]1/2

=
Pr (Z ‚â•0.7377)
=
1 ‚àíPr (Z < 0.7377) ‚âà0.230.
Here the normal approximation is not adequate. In general, this approxi-
mation does not work well, unless min [nŒ∏, n (1 ‚àíŒ∏)] ‚â•5, and the behavior
is more erratic when Œ∏ is near the boundaries. In the example, nŒ∏ is equal
to 60 (0.02) = 1.2. A better approximation is obtained using the continu-
ity correction (e.g., Casella and Berger, 1990). Instead of approximating
Pr (X ‚â•2), one approximates Pr (X ‚â•2 ‚àí0.5); this yields 0.391, which is
a little closer to the exact result 0.338 obtained before.
For a recent discussion about the complexities associated with this ap-
proximation in the context of calculation of the conÔ¨Ådence interval for a
proportion, the reader is referred to Brown et al. (2001) and to Henderson
and Meyer (2001).
1.3
Univariate Continuous Distributions
The variable Z, deÔ¨Åned in (1.16) is an example of a continuous random
variable, that is, one taking any of an inÔ¨Ånite number of values. In the
case of Z these values exist in the real line, R; the sample space is here
continuous. More precisely, a random variable is (absolutely) continuous if
its c.d.f. F (x), deÔ¨Åned in (1.18), is continuous at every real number x.
The probability that the continuous random variable will assume any
particular value is zero because the possible values that X can take are not
countable. A typical example of a random variable that can be regarded

14
1. Probability and Random Variables
as continuous is the body weight of an animal. In practice, however, mea-
surements are taken on a discrete scale only, with the degree of discreteness
depending on the resolution of the instrument employed for recording. How-
ever, much simplicity can be gained by treating body weight as if it were
continuous.
In the discrete case, as given in (1.5), the distribution function is deÔ¨Åned
in terms of a sum. On the other hand, the c.d.f. of a continuous random
variable X on the real line is given by the integral
F (x) = Pr (X ‚â§x) =
x

‚àí‚àû
p (t) dt,
for ‚àí‚àû< x < ‚àû,
(1.18)
where p (¬∑) is the p.d.f. of this random variable. If dt is an inÔ¨Ånitesimally
small increment in the variable, so that the density is continuous and fairly
constant between t and t + dt, the probability that the random variable
takes a value in the interval between t and t + dt is nearly p (t) dt (Bulmer,
1979). If p (t) > p (u) then values of X are more likely to be seen close to t
than values of X close to u.
As a little technical aside, the p.d.f. is not uniquely deÔ¨Åned. This is
because one can always change the value of a function at a Ô¨Ånite number of
points, without changing the integral of the function over the interval. This
lack of uniqueness does not pose problems for probability calculations but
can lead to subtle complications in certain statistical applications. However,
if the distribution function is diÔ¨Äerentiable then a natural choice for p (x)
is
d
dxF (x) = p (x) .
(1.19)
Any probability statement about the random variable X can be deduced
from either F (x) or p (x); the p.d.f. encapsulates all the necessary infor-
mation needed to make statements of uncertainty about X. This does not
mean that X is characterized by its c.d.f.. Two diÔ¨Äerent random variables
can have the same c.d.f. or p.d.f.. As a trivial example, consider the Gaus-
sian random variable X with mean zero and variance 1. Then the random
variable ‚àíX is also Gaussian with the same mean and variance.
Properties of the p.d.f. are:
‚Ä¢ p (x) ‚©æ0 for all x.
‚Ä¢
 ‚àû
‚àí‚àûp (x) dx = 1.
(Any positive function p satisfying
 ‚àû
‚àí‚àûp (x) dx = 1 may be termed a
p.d.f.).
‚Ä¢ for x ‚àà(a, b), Pr (a < X < b) =
 b
a p (x) dx = F (b) ‚àíF (a) .

1.3 Univariate Continuous Distributions
15
(Because of the lack of uniqueness mentioned above, it is possible to
Ô¨Ånd another density function p‚àó, such that
 b
a p (x) dx =
 b
a p‚àó(x) dx,
for all a and b but for which p (x) and p‚àó(x) diÔ¨Äer at a Ô¨Ånite number
of points).
By virtue of the continuity of X,
Pr (a < X < b) = Pr (a < X ‚â§b)
= Pr (a ‚â§X < b) = Pr (a ‚â§X ‚â§b) ,
(1.20)
so < and ‚â§can be used indiscriminately in this case.
‚Ä¢ The probability that X takes a particular value is,
Pr (X = x) = lim
Œµ‚Üí0
a+Œµ

a‚àíŒµ
p (x) dx = 0.
‚Ä¢ As noted above, the p.d.f. describes the probability that X takes a
value in a neighborhood of a point x. Thus if ‚àÜis small and p is
continuous,
Pr (x ‚â§X ‚â§x + ‚àÜ) =
x+‚àÜ

x
p (x) dx ‚àº= ‚àÜp (x) .
(1.21)
Consider the continuous random variable X that has density p (x) with
support on the real line, R. Let A be some interval in R. Notationally, we
say that A ‚äÜR and, using the concept of the indicator function introduced
in (1.6), the probability that the random variable X belongs in A can be
written
Pr (X ‚ààA)
=

A
p (x) dx
=

I (x ‚ààA) p (x) dx
=
E [I (x ‚ààA)] .
(1.22)
In this book we shall often encounter integrals of the form

g (x) p (x) dx.
This is the expectation of g with respect to a probability on R. It will be
referred to as ‚Äúintegrating g over the distribution of X‚Äù. We may also use,
loosely, ‚Äúintegrating g, or taking the expectation of g, with respect to p‚Äù.

16
1. Probability and Random Variables
Often it suÔ¨Éces to identify the p.d.f. of a random variable X only up to
proportionality; this is particularly convenient in the context of a Bayesian
analysis, as will be seen repeatedly in this book. A function f (x), such
that p (x) ‚àùf (x), is called the kernel of p (x). That is, if it is known that
p (x) = Cf (x), the constant C can be determined using the fact that a
p.d.f. integrates to 1. Thus, for a random variable X taking any value on
the real line
 ‚àû
‚àí‚àû
p (x) dx = C
 ‚àû
‚àí‚àû
f (x) dx = 1
so
C =
1
 ‚àû
‚àí‚àûf (x) dx.
Equivalently,
p (x) =
f (x)
 ‚àû
‚àí‚àûf (x) dx.
(1.23)
If the random variable takes values within the interval (lower, upper), say,
the integration limits above must be changed accordingly.
Example 1.5
Kernel and integration constant of a normal distribution
Suppose X has a normal distribution with mean ¬µ and variance œÉ2. Then
its density function is
p

x|¬µ, œÉ2
=

2œÄœÉ2‚àí1
2 exp

‚àí(x ‚àí¬µ)2
2œÉ2

.
The kernel is
f (x) = exp

‚àí(x ‚àí¬µ)2
2œÉ2

and the constant of integration is
C =
1
 ‚àû
‚àí‚àûexp

‚àí(x‚àí¬µ)2
2œÉ2

dx
=

2œÄœÉ2‚àí1
2 .
(1.24)
To prove (1.24), denote the integral in the denominator as I. Using the
substitution u = (x ‚àí¬µ)/œÉ, since dx = œÉ du, I can be written
I = œÉ
 ‚àû
‚àí‚àû
exp

‚àíu2
2

du.
Now write the square of I as a double integral
I2
=
œÉ2
 ‚àû
‚àí‚àû
exp

‚àíu2
2

du
 ‚àû
‚àí‚àû
exp

‚àív2
2

dv
=
œÉ2
 ‚àû
‚àí‚àû
 ‚àû
‚àí‚àû
exp

‚àí

u2 + v2
2

du dv

1.3 Univariate Continuous Distributions
17
and make a transformation to polar coordinates
u
=
r cos (Œ∏) ,
v
=
r sin (Œ∏) .
If the angle Œ∏ is expressed in radians, then it can take values between 0
and 2œÄ. The variable r takes values between 0 and ‚àû. From calculus, the
absolute value of the two-dimensional Jacobian of the transformation is the
absolute value of the determinant
det
‚àÇ(u, v)
‚àÇ(r, Œ∏)

=
det

‚àÇu/‚àÇr
‚àÇu/‚àÇŒ∏
‚àÇv/‚àÇr
‚àÇv/‚àÇŒ∏

=
det

cos (Œ∏)
‚àír sin (Œ∏)
sin (Œ∏)
r cos (Œ∏)

=
r

cos2 (Œ∏) + sin2 (Œ∏)

= r.
Using this, together with the result
u2 + v2 = r2 
cos2 (Œ∏) + sin2 (Œ∏)

= r2
one obtains
I2
=
œÉ2
 ‚àû
0
 2œÄ
0
r exp

‚àír2
2

dŒ∏dr
=
œÉ2
 ‚àû
0
r exp

‚àír2
2

dr
 2œÄ
0
dŒ∏
=
2œÄœÉ2
 ‚àû
0
r exp

‚àír2
2

dr.
The integral in the preceding expression can be evaluated from the family
of gamma integrals (Box and Tiao, 1973). For a > 0 and p > 0:
 ‚àû
0
rp‚àí1 exp

‚àíar2
dr = 1
2a‚àíp/2Œì
p
2

,
(1.25)
where Œì (¬∑) is the gamma function (which will be discussed later). Setting
p = 2 and a = 1
2, it follows that
 ‚àû
0
r exp

‚àír2
2

dr = 1
2 √ó 2 √ó Œì (1) = 1,
because Œì (1) = 0! = 1. Therefore, I2 = 2œÄœÉ2, and, Ô¨Ånally,
C =
1
 ‚àû
0
exp

‚àí(x‚àí¬µ)2
2œÉ2

dx
= 1
I =
1
‚àö
2œÄœÉ2
yielding the desired result.
‚ñ†

18
1. Probability and Random Variables
1.3.1
The Uniform, Beta, Gamma, Normal,
and Student-t Distributions
Uniform Distribution
A random variable X has a uniform distribution over the interval [a, b] (in
view of (1.20) we shall allow ourselves to be not quite consistent throughout
this book when deÔ¨Åning the support of distributions of continuous random
variables whose sample space is truncated) if its p.d.f. is
p (x|a, b) = Un (x|a, b) =

1
b‚àía,
for a ‚â§x ‚â§b,
0,
otherwise.
(1.26)
The c.d.f. is
F (x|a, b) =
Ô£±
Ô£≤
Ô£≥
0,
if x < a,
 x
a Un (x|a, b) dx =
1
b‚àía
 x
a dx = x‚àía
b‚àía ,
for a ‚â§x ‚â§b,
1,
for x > b.
The average value of X is found by multiplying the value x times its ‚Äúin-
Ô¨Ånitesimal probability‚Äù p (x) dx, and then summing over all possible such
values. This corresponds to the integral
E (X|a, b) =
b

a
x
1
b ‚àíadx = a + b
2
.
(1.27)
Likewise the average value of X2 is
E

X2|a, b

=
b

a
x2
1
b ‚àíadx = b3 ‚àía3
3 (b ‚àía) = (a + b)2 ‚àíab
3
.
Then the variance can be found to be, after algebra,
V ar (X|a, b)
=
E

X2|a, b

‚àíE2 (X|a, b)
=
(a + b)2 ‚àíab
3
‚àí

a + b
2
2
= (b ‚àía)2
12
.
(1.28)
Example 1.6
Simulation of discrete random variables
Suppose one wishes to generate a discrete random variable with p.m.f.
Pr (X = xi) = pi,
i = 1, 2, 3, 4,
with 4
i=1 pi = 1. To accomplish this, generate a random number U from
a uniform distribution Un (0, 1) and set
X =
Ô£±
Ô£¥
Ô£¥
Ô£≤
Ô£¥
Ô£¥
Ô£≥
x1,
if U < p1,
x2,
if p1 ‚â§U < p1 + p2,
x3,
if p1 + p2 ‚â§U < p1 + p2 + p3,
x4,
if p1 + p2 + p3 ‚â§U < 1.

1.3 Univariate Continuous Distributions
19
To conÔ¨Årm that X has the desired distribution, Ô¨Årst note that p (u) = 1.
Then
Pr (X = x3)
=
Pr (p1 + p2 ‚â§U < p1 + p2 + p3)
=
Pr (U ‚â§p1 + p2 + p3) ‚àíPr (U ‚â§p1 + p2)
=
 p1+p2+p3
0
du ‚àí
 p1+p2
0
du = p3.
This method of generating the random variable is known as the inverse
transform method. For continuous random variables, the general algorithm
is as follows. If one wishes to generate a random variable that has c.d.f. F,
then:
(a) Ô¨Årst generate U ‚àºUn (0, 1); and
(b) then set X = F ‚àí1
X (U);
and X has the desired c.d.f. FX (¬∑). To see this, note that
Pr (X ‚â§x)
=
Pr

F ‚àí1
X (U) ‚â§x

=
Pr (U ‚â§FX (x))
=
 FX(x)
0
du = FX (x) .
The equality in the second line follows because FX (x) is a monotone
function.
‚ñ†
Simulating Uniform Random Numbers
The uniform distribution plays an important role in the generation of pseu-
dorandom numbers with a computer. Ripley (1987) and Ross (1997) discuss
how uniform random ‚Äúdeviates‚Äù can be produced in an electronic com-
puter; Fishman (1973) presents a readable discussion. A summary of the
basic ideas follows.
The most common algorithm for generating ‚Äúrandom‚Äù numbers actually
produces a nonrandom sequence of values. Each number depends on the
previous one, which implies that all numbers are determined by the initial
value in the sequence. However, if a generator is designed appropriately (in
some sense which is beyond the scope of this book), the numbers appear
as ‚Äúindependent‚Äù draws from a uniform distribution. A popular method is
known as the ‚Äúlinear congruential generator‚Äù. It has the form
Zi ‚â°aZi‚àí1 + c mod (m) ,
(1.29)
where the ‚Äúseed‚Äù Z0 is an integer 0 ‚â§Z0 < m, the multiplier a is an
integer 1 < a < m, c is an integer increment (0 ‚â§c < m), and modulus m
is an integer 0 < m. The modulus notation means that Zi is the remainder
obtained when aZi‚àí1 +c is divided by m. This remainder can be expressed

20
1. Probability and Random Variables
as
Zi = aZi‚àí1 + c ‚àí
aZi‚àí1 + c
m

m,
where
aZi‚àí1 + c
m

= largest integer in the quantity,
note that aZi‚àí1 + c is integer. For example, if the quantity within brackets
is 12
5 , the largest integer is 2; if the quantity is 1
6, the largest integer is 0.
The arithmetic guarantees that each element in the sequence {Zi} is an
integer in the interval [0, m ‚àí1] . Then, when {Zi} displays ‚ÄúsuÔ¨Éciently
random‚Äù behavior, a pseudorandom sequence in the unit interval can be
obtained as
Xi = Zi
m .
Since Zi < m, the sequence can contain at most m distinct numbers and,
as soon as a number is repeated, the entire sequence repeats itself. Thus,
m should be as large as possible to ensure a large quantity of distinct
numbers. The parameters a, c and Z0 must be selected so that as many
as possible of the m numbers occur in a cycle. It can be shown (Cohen,
1986) that the sequence {Zi} is periodic: there exists a P ‚â§m such that
Zi = Zi+P for all i ‚â•0. A large P (therefore, a large m) is desirable
for achieving apparent randomness. A generator is said to have full period
when P = m; however, a full period does not guarantee that a generator
is adequate. For example, old generators, such as RANDU (formerly used
in the IBM ScientiÔ¨Åc Subroutine Package; Cohen, 1986) had a = 65, 539,
c = 0 and m = 231; it had a maximal period of 229, and yet failed some
tests. On the other hand, the full-period LCG generator (Cohen, 1986) has
a = 69, 069, c = 1 and m = P = 232 and has failed tests as well. Marsaglia
and Zaman (1993) describe ‚ÄúKiss‚Äù, a generator that has displayed excellent
performance from a statistical point of view (Robert, 1994).
The dependency between Zi and Z0 can be deduced as follows. Consider
(1.29) and write
Z1
‚â°
aZ0 + c mod (m) ,
Z2
‚â°
aZ1 + c mod (m) ,
Z3
‚â°
aZ2 + c mod (m) .
Expressing Z3 in terms of Z2, and this in terms of Z1, one obtains the
identity
Z3 ‚â°a3Z0 + (a2 + a + 1)c mod (m)
and, more generally
Zn ‚â°anZ0 + (an‚àí1 + an‚àí2 + ... + a + 1)c mod (m) .

1.3 Univariate Continuous Distributions
21
Note that
(an‚àí1 + an‚àí2 + ... + a + 1)(a ‚àí1) = an ‚àí1.
Hence
Zn ‚â°anZ0 + an ‚àí1
(a ‚àí1)c mod (m)
(1.30)
and, consequently,
Xn ‚â°anX0 +
an ‚àí1
(a ‚àí1)mc mod (m)
where X0 = Z0/m.
Example 1.7
A hypothetical random number generator
In order to illustrate, an example from Fishman (1973) will be modiÔ¨Åed
slightly. Consider a hypothetical generator with a = 3, c = 0, Z0 = 4, and
m = 7. The pseudo-random uniform deviates, given in the table below, can
be computed with (1.29) or with (1.30):
i
Zi = aZi‚àí1 + c

aZi‚àí1+c
m

m
Xi = Zi
m
0
4
4/7 = 0.5714
1
3 √ó 4 ‚àí
 3√ó4
7
!
√ó 7 = 12 ‚àí1 √ó 7 = 5
5/7 = 0.7143
2
3 √ó 5 ‚àí
 3√ó5
7
!
√ó 7 = 15 ‚àí2 √ó 7 = 1
1/7 = 0.1429
3
3 √ó 5 ‚àí
 3√ó5
7
!
√ó 7 = 15 ‚àí2 √ó 7 = 1
3/7 = 0.4286
4
3 √ó 3 ‚àí
 3√ó3
7
!
√ó 7 = 9 ‚àí1 √ó 7 = 2
2/7 = 0.2857
5
3 √ó 2 ‚àí
 3√ó2
7
!
√ó 7 = 6 ‚àí0 √ó 7 = 6
6/7 = 0.8571
6
3 √ó 6 ‚àí
 3√ó6
7
!
√ó 7 = 18 ‚àí2 √ó 7 = 4
4/7 = 0.5714
The sequence repeats itself after six steps. If the parameters stay as before
and m = 8, a string of 4s is obtained, illustrating a very poor generator of
pseudo-random numbers.
‚ñ†
Beta Distribution
The beta distribution can be used as a model for random variables that
take values between zero and one, such as probabilities, a gene frequency, or
the coeÔ¨Écient of heritability. A random variable X has a beta distribution
if its p.d.f. has the form
p (x|a, b) = Be (x|a, b) =

Cxa‚àí1 (1 ‚àíx)b‚àí1 ,
for x ‚àà[0, 1] ,
0,
otherwise,
(1.31)

22
1. Probability and Random Variables
where a > 0 and b > 0 are parameters of this distribution. The value of
the integration constant is C = Œì (a + b) /Œì (a) Œì (b), where
Œì (Œ±) =
‚àû

0
xŒ±‚àí1e‚àíx dx,
Œ± > 0,
(1.32)
and Œì (¬∑) is the gamma function, as seen in (1.25). Note that, if Œ± = 1,
Œì (1) =
‚àû

0
e‚àíx dx = ‚àíe‚àíx""‚àû
0 = 1.
Suppose Œ± > 1. The integral in (1.32) can be evaluated by parts. Recall
the basic calculus result

u dv = uv ‚àí

v du.
Put u = xŒ±‚àí1 and dv = e‚àíx; then du = (Œ± ‚àí1) xŒ±‚àí2 and v = ‚àíe‚àíx. The
integral sought is then
Œì (Œ±) =
‚àû

0
xŒ±‚àí1e‚àíx dx = ‚àíxŒ±‚àí1e‚àíx""‚àû
0 +
‚àû

0
(Œ± ‚àí1) xŒ±‚àí2e‚àíx dx
= (Œ± ‚àí1)
‚àû

0
xŒ±‚àí2e‚àíx dx = (Œ± ‚àí1) Œì (Œ± ‚àí1)
(1.33)
with the preceding following from the deÔ¨Ånition of the gamma function in
(1.32). Thus, provided Œ± is integer and greater than one,
Œì (Œ±) = (Œ± ‚àí1) (Œ± ‚àí2) . . . 3 √ó 2 √ó 1 √ó Œì (1) = (Œ± ‚àí1)!
(1.34)
Consider now the beta density in (1.31), and write it explicitly as
p (x|a, b) = Œì (a + b)
Œì (a) Œì (b)xa‚àí1 (1 ‚àíx)b‚àí1 .
(1.35)
The expected value and variance of the beta distribution can be deduced
by using the fact that the constant of integration is such that
C‚àí1 =
1

0
xa‚àí1 (1 ‚àíx)b‚àí1 dx = Œì (a) Œì (b)
Œì (a + b) .
(1.36)

1.3 Univariate Continuous Distributions
23
The mean value of a beta distributed random variable is then
E (X|a, b) = C
 1
0
x

xa‚àí1 (1 ‚àíx)b‚àí1
dx
= C
 1
0
xa+1‚àí1 (1 ‚àíx)b‚àí1 dx
= C Œì (a + 1) Œì (b)
Œì (a + b + 1)
= Œì (a + b)
Œì (a) Œì (b)
Œì (a + 1) Œì (b)
Œì (a + b + 1)
=
Œì (a + b) aŒì (a)
Œì (a) (a + b) Œì (a + b) =
a
(a + b).
(1.37)
Using a similar development, it can be established that
V ar (X|a, b)
=
E

X2|a, b

‚àíE2 (X|a, b)
=
ab
(a + b)2 (a + b + 1)
.
(1.38)
Example 1.8
Variation in gene frequencies
Consider the following, patterned after Wright (1968). Suppose n alleles are
sampled from some population, and that the outcome of each draw is either
A or a. In the absence of correlation between outcomes, the probability
distribution of X, the number of A alleles observed in the sample of size
n, can be calculated using the binomial distribution (1.10); the probability
Œ∏ corresponds to the frequency of allele A in the population. Wright refers
to this set of n draws as ‚Äúclusters‚Äù. If this sampling scheme were to be
repeated a large number of times, an excess of clusters consisting largely or
entirely of A or of a would indicate correlation between outcomes resulting,
for example, from family aggregation. The distribution of correlated draws
can be derived by assuming that Œ∏ varies according to some distribution.
Because Œ∏ varies between 0 and 1, the beta distribution is convenient here.
The parameterization of Wright is adopted subsequently. Let
c = a + b
and
Œ∏ =
a
a + b
be the mean of the distribution. The beta density (1.35) can then be written
as
p

Œ∏|Œ∏, c

=
Œì (c)
Œì

Œ∏c

Œì

c

1 ‚àíŒ∏
Œ∏cŒ∏‚àí1 (1 ‚àíŒ∏)c(1‚àíŒ∏)‚àí1 .
(1.39)

24
1. Probability and Random Variables
If gene frequencies vary at random in the clusters according to this beta
distribution, the between-cluster variance of gene frequencies is given by
V ar

Œ∏|Œ∏, c

= Œ∏

1 ‚àíŒ∏

1 + c
.
This variability can accommodate ‚Äúextra-binomial‚Äù variation and account
for the possible correlations between alleles that have been drawn. This
example will be elaborated further later.
‚ñ†
Gamma, Exponential, and Chi-square Distributions
The gamma distribution arises in quantitative genetic studies of variance
components and of heritability through Bayesian methods. For example,
Wang et al. (1994) analyzed a selection experiment for increased proliÔ¨Å-
cacy in pigs in which the initial state of uncertainty about genetic and
environmental variance components was represented by ‚Äúinverted‚Äù gamma
distributions (these are random variables whose reciprocals are distributed
according to gamma distributions). A random variable X has a gamma
distribution Ga (a, b) if its p.d.f. has the form
p (x|a, b) = Ga (x|a, b) = Cxa‚àí1 exp [‚àíbx] ,
x > 0,
(1.40)
where C = ba/Œì (a); the ‚Äúshape‚Äù parameter a and the ‚Äúinverse scale‚Äù
parameter b are positive. A special case of interest arises when a = 1; then
X is said to have an exponential distribution with parameter b and density
p (x|b) = b exp (‚àíbx) ,
x ‚â•0.
(1.41)
The exponential distribution has been employed for modeling survival or
length of productive life in livestock (Famula, 1981).
Another special case of interest is when a = v/2 and b = 1/2; then X is
said to possess a central chi-square distribution with positive parameter v
(often known as degrees of freedom) and density
p (x|v) = Cx(v/2)‚àí1 exp (‚àíx/2) ,
for v > 0 and x > 0,
(1.42)
where the integration constant is
C = (1/2)v/2
Œì (v/2) .
(1.43)
The chi-square process will be revisited in a section where quadratic forms
on normal variables are discussed.

1.3 Univariate Continuous Distributions
25
The kth (k = 1, 2, ...) moment from the origin of the gamma distribution
is, by deÔ¨Ånition,
E

Xk|a, b

=
‚àû

0
xk ba
Œì (a)xa‚àí1 exp (‚àíbx) dx
=
ba
Œì (a)
‚àû

0
xk+a‚àí1 exp (‚àíbx) dx.
Let X = Y 2, so dx = 2y dy. Then
E

Xk|a, b

= 2ba
Œì (a)
‚àû

0
y2(k+a)‚àí1 exp

‚àíby2
dy.
Using the gamma integral in (1.25) with p = 2(k + a),
E

Xk|a, b

=
2ba
Œì (a)
 ‚àû
0
y
2(k+a)‚àí1 exp

‚àíby2
dy
=
ba
Œì (a)b‚àí2(k+a)
2
Œì

2(k + a)
2

.
(1.44)
The mean of the distribution follows, by putting k = 1,
E (X|a, b) = Œì (a + 1)
bŒì (a)
= a
b
(1.45)
because Œì (a + 1) = aŒì (a) , as seen in (1.33). Similarly,
E

X2|a, b

= Œì (a + 2)
b2Œì (a)
= a (a + 1)
b2
.
The variance of the distribution is thus,
V ar (X|a, b) = a
b2 .
(1.46)
The coeÔ¨Écient of variation (ratio between the standard deviation and the
mean of the distribution) is equal to 1/‚àöa, so it depends on only one of
the parameters.
The Normal and Student-t Distributions
The normal distribution, without doubt, is the most important random pro-
cess in statistical genetics. It has played a central role in model development
(e.g., the inÔ¨Ånitesimal model of inheritance; for a thorough discussion, see

26
1. Probability and Random Variables
Bulmer, 1980); selection theory (Pearson, 1903; Gianola et al., 1989), esti-
mation of dispersion parameters (Henderson, 1953; Patterson and Thomp-
son, 1971; Searle et al., 1992); prediction of genetic values (Henderson, 1963,
1973, 1975), inference about response to selection (Sorensen et al., 1994),
and evaluation of hypotheses (Edwards, 1992; Rao, 1973). The density of
a normally distributed random variable X with mean ¬µ and variance œÉ2,
i.e., N

x|¬µ, œÉ2
, has been presented already, and is
p

x|¬µ, œÉ2
=
1
œÉ
‚àö
2œÄ exp

‚àí(x ‚àí¬µ)2
2œÉ2

,
for ‚àí‚àû< x < ‚àû,
(1.47)
with ‚àí‚àû< ¬µ < ‚àûand œÉ > 0 being the parameters and their corre-
sponding spaces. Since p (¬µ + Œµ) = p (¬µ ‚àíŒµ), it is seen that the distri-
bution is symmetric about ¬µ, so the mean and median are equal, with
E

X|¬µ, œÉ2
= ¬µ. The parameter œÉ is the standard deviation of X. The
density has a maximum at x = ¬µ (so the mean is equal to the mode and to
the median) and, at this value, p (¬µ) =

œÉ
‚àö
2œÄ
‚àí1. Furthermore, the den-
sity has inÔ¨Çection points (where the curve changes from concave to convex)
at ¬µ ¬± œÉ.
In order to derive the mean and variance, use is made of what is called
the moment generating function. Let X be a random variable having some
distribution, and consider a Taylor series expansion of exp (tX) about X =
0, where t is a dummy variable. Then
exp (tX)
=
1 + {t exp (tX)}X=0 X +

t2 exp (tX)

X=0
X2
2! + ¬∑ ¬∑ ¬∑
¬∑ ¬∑ ¬∑ +

tk exp (tX)

X=0
Xk
k! +
=
1 + tX + t2X2
2!
+ ¬∑ ¬∑ ¬∑ + tkXk
k!
+ ¬∑ ¬∑ ¬∑
The moment generating function is the average value of exp (tX):
E [exp (tX)] = 1 + tE (X) + t2E

X2
2!
+ ¬∑ ¬∑ ¬∑ + tkE

Xk
k!
+ ¬∑ ¬∑ ¬∑
(1.48)
which is a linear combination of all moments of the distribution. DiÔ¨Äerenti-
ation of (1.48) with respect to t, and setting t = 0, yields E(X). Likewise,
diÔ¨Äerentiation of (1.48) twice with respect to the dummy variable, and set-
ting t = 0, gives E

X2
; in general, k diÔ¨Äerentiations (followed by putting
t = 0) will produce E

Xk
. With this technique, moments can be found
for most distributions, provided the moment generating function exists.

1.3 Univariate Continuous Distributions
27
Example 1.9
Finding the mean and variance of a normal process
For the normal distribution, the moment generating function is
M (t) =
‚àû

‚àí‚àû
exp (tx)
1
œÉ
‚àö
2œÄ exp

‚àí(x ‚àí¬µ)2
2œÉ2

dx.
(1.49)
The exponents in the integral can be combined, by adding and subtracting
terms and by completing a square, as follows:
tx ‚àí(x ‚àí¬µ)2
2œÉ2
= tx ‚àí

 x2
2œÉ2 + ¬µ2
2œÉ2 ‚àí2¬µx
2œÉ2

+

¬µt + œÉ2t2
2

‚àí

¬µt + œÉ2t2
2

=

¬µt + œÉ2t2
2

+ 2œÉ2tx
2œÉ2
‚àí

 x2
2œÉ2 + ¬µ2
2œÉ2 ‚àí2¬µx
2œÉ2

‚àí

2œÉ2¬µt
2œÉ2
+ œÉ4t2
2œÉ2

=

¬µt + œÉ2t2
2

‚àí1
2œÉ2

x2 + ¬µ2 + œÉ4t2 ‚àí2¬µx ‚àí2œÉ2tx ‚àí2œÉ2¬µt

=

¬µt + œÉ2t2
2

‚àí

x ‚àí

¬µ + œÉ2t
2
2œÉ2
.
Employing this in (1.49), one obtains
M (t)
=
exp

¬µt + œÉ2t2
2

œÉ
‚àö
2œÄ
‚àû

‚àí‚àû
exp
#
‚àí

x ‚àí

¬µ + œÉ2t
2
2œÉ2
$
dx
=
exp

¬µt + œÉ2t2
2

(1.50)
as the integral evaluates to œÉ
‚àö
2œÄ, the reciprocal of the integration constant
of the normal distribution. The moments of the distribution can now be
obtained by successive diÔ¨Äerentiation of M(t) with respect to t, and then
evaluating the result at t = 0. For example, the Ô¨Årst moment is
E (X)
=
M ‚Ä≤ (0) =
 d
dt exp

¬µt + œÉ2t2
2

t=0
=

¬µ + œÉ2t

exp

¬µt + œÉ2t2
2

t=0
= ¬µ,

28
1. Probability and Random Variables
yielding the mean of the distribution. Similarly, the second moment is
E

X2
=
M ‚Ä≤‚Ä≤ (0) =

œÉ2 +

¬µ + œÉ2t
2
exp

¬µt + œÉ2t2
2
%
t=0
=
œÉ2 + ¬µ2.
It follows that the variance of the distribution is E

X2
‚àíE2 (X) = œÉ2,
as anticipated.
‚ñ†
The normal and Student-t distributions are intimately related to each
other. If a random variable X has a t

¬µ, œÉ2, ŒΩ

distribution, its density
function is
p

x|¬µ, œÉ2, ŒΩ

= Œì [(ŒΩ + 1) /2]
Œì (ŒΩ/2) ‚àöŒΩœÄœÉ

1 + (x ‚àí¬µ)2
ŒΩœÉ2
‚àí(ŒΩ+1)/2
.
(1.51)
It can be seen that this distribution is symmetric about ¬µ, its mean. Two
additional positive parameters here are: the degrees of freedom ŒΩ and the
scale of the distribution œÉ. The variance of a t process is
V ar

X|¬µ, œÉ2, ŒΩ

=
ŒΩ
(ŒΩ ‚àí2)œÉ2.
Thus, when ŒΩ is 2 or less the variance is inÔ¨Ånite. When the degrees of
freedom parameter goes to inÔ¨Ånity, the process tends toward a normal
N

¬µ, œÉ2
one; when ŒΩ = 1 the distribution is called Cauchy (whose mean
and higher moments do not exist).
Derivation of density (1.51) requires consideration of the distribution of
pairs of random variables. As shown in more detail later on in this chapter,
if given ¬µ and S2
i , Xi is normally distributed
Xi|¬µ, S2
i ‚àºN

¬µ, S2
i

,
(1.52)
and if S2
i follows a scaled inverted chi-square distribution with parameters
ŒΩ and œÉ2:
S2
i |ŒΩ, œÉ2 ‚àºŒΩœÉ2œá‚àí2
ŒΩ ,
(1.53)
then the density
p

xi|¬µ, œÉ2, ŒΩ

=

p

xi|¬µ, S2
i

p

S2
i |ŒΩ, œÉ2
dS2
i
(1.54)
is that of the t

¬µ, œÉ2, ŒΩ

distribution (1.51). The t distribution can therefore
be interpreted as a mixture of normal distributions with a common mean
and a variance that varies according to a scaled inverted chi-square.
This way of viewing the t distribution leads to a straightforward method
for drawing Monte Carlo samples. First draw S2‚àófrom (1.53); second, draw
x‚àó
i from

Xi|¬µ, S‚àó2
. Then x‚àó
i is a draw from (1.54) and repeating this
procedure generates an i.i.d. sample from (1.54). This is known as the
method of composition (Tanner, 1996).

1.4 Multivariate Probability Distributions
29
1.4
Multivariate Probability Distributions
Results presented for a single random variable generalize in a fairly direct
way to multivariate situations, that is, to settings in which it is desired
to assign probabilities to events involving several random variables jointly.
For example, consider the joint distribution of genotypes at two loci for a
quantitative trait. A question of interest might be if genotype Bb, say, at
locus B appears more frequently in association with genotype aa at locus
A than with either genotype AA or Aa. Here one can let X be a random
variable denoting genotype at locus A, and Y denote the genotype at locus
B. The joint distribution of X and Y is called a bivariate distribution.
Whenever the random process involves two or more random variables, one
speaks of multivariate probability distributions.
We start this section by introducing the concept of independence. Let
(X1, X2, . . . , Xn) be a random vector whose elements Xi (i = 1, 2, . . . , n)
are one-dimensional random variables. Then
X1, X2, . . . , Xn
are called mutually independent if, for every (x1, x2, . . . , xn),
p (x1, x2, . . . , xn) = p (x1) p (x2) . . . p (xn) .
(1.55)
The term p (xi) is a marginal probability density. It is obtained by inte-
grating (adding if the random variable is discrete) over the distribution of
the remaining (n ‚àí1) random variables
p (xi) =

p (x1, . . . , xi‚àí1, xi, xi+1, . . . , xn) dx1 . . . dxi‚àí1 dxi+1 . . . dxn,
(1.56)
where the integral is understood to be of dimension (n ‚àí1). Mutual inde-
pendence implies pairwise independence; however, pairwise independence
does not imply mutual independence.
A generalization of the concept of i.i.d. random variables is that of ex-
changeable random variables, an idea due to De Finetti (1975b). The ran-
dom variables X1, X2, . . . , Xn are exchangeable, if any permutation of any
subset of size k of the random variables, for any k ‚â§n, has the same dis-
tribution. That is, for exchangeable random variables, any permutation of
the indexes will lead to the same joint probability density. In practice, in a
modeling context, the idea of exchangeability is associated with ignorance
or symmetry: the less one knows about a problem, the more conÔ¨Ådently one
can make claims of exchangeability. For example, in rolling a die on which
no information is available, one is prepared to assign equal probabilities
to all six outcomes. Then the probability of obtaining a 1 and a 5, in two
independent throws, should be the same as that of obtaining any other two
possible outcomes. Note that random variables that are not independent

30
1. Probability and Random Variables
can be exchangeable. For example, if X1, X2, . . . , Xn are i.i.d. as Bernoulli
Br (Œ∏), then, given n
i=1 Xi = t, X1, X2, . . . , Xn are exchangeable, but not
independent.
For simplicity, the remainder of this section motivates developments us-
ing a bivariate situation. Analogously to the univariate case, the joint c.d.f.
of two random variables X and Y is deÔ¨Åned to be
F (x, y) = Pr (X ‚â§x, Y ‚â§y) ,
(1.57)
where Pr (X ‚â§x, Y ‚â§y) means ‚Äúprobability that X takes a value smaller
than or equal to x and Y takes a value smaller than or equal to y‚Äù. If
the two random variables are discrete, their joint probability distribution
is given by
Pr (X = x, Y = y) = p (x, y) .
(1.58)
When the two random variables are continuous, a density function must
be introduced, as in the univariate case, because the probability that X = x
and Y = y is 0. The joint p.d.f. of random variables (X, Y ), that take value
in R2, is, by deÔ¨Ånition,
p (x, y) = ‚àÇ2F (x, y)
‚àÇx‚àÇy
(1.59)
provided the distribution function F (¬∑) is diÔ¨Äerentiable (in this case at
least twice). The joint p.d.f. p (x, y) is deÔ¨Åned for all (x, y) in R2. The joint
probability that (X, Y ) belong to a subset A of R2 is given by the two-fold
integral
Pr [(X, Y ) ‚ààA]
=
 
A
p (x, y) dx dy
=
 
I [(x, y) ‚ààA] p (x, y) dx dy.
For example, if A = (a1, b1) √ó (a2, b2), then the integral above is
Pr [(X, Y ) ‚ààA]
=
 
I [x ‚àà(a1, b1) , y ‚àà(a2, b2)] p (x, y) dx dy
=
 b1
a1
 b2
a2
p (x, y) dx dy.
Conditional distributions play an important role when making inferences
about nonobservable quantities given information on observable quantities.
The conditional probability distribution of X given Y = y is the function
of X given by
p (x|y) = p (x, y)
p (y) ,
p (y) > 0.
(1.60)

1.4 Multivariate Probability Distributions
31
In the case p (y) = 0, the convention is that p (x|y) = p (x). Expression
(1.60) implies that
p (x, y) = p (x|y) p (y)
regardless of whether p (y) > 0. In the case of continuous random variables,
notice that (1.60) integrates to 1:

p (x|y) dx
=

p (x, y) dx
p (y)
=
p (y)
p (y) = 1.
Because p (x|y) is a function of X at the Ô¨Åxed value Y = y, one can also
write
p (x|y) ‚àùp (x, y)
(1.61)
as the denominator does not depend on x. This result holds both for dis-
crete and continuous random variables, and highlights the fact that a joint
distribution must be the starting point in a multivariate analysis. Again,
when X and Y are statistically independent, p(x, y) = p(x)p(y) and
p (x|y) = p (x) p (y)
p (y)
= p (x) .
(1.62)
In the case of n mutually independent random variables X1, X2, . . . , Xn,
the conditional distribution of any subset of the coordinates, given the value
of the rest, is the same as the marginal distribution of the subset.
In the discrete two-dimensional case, the marginal distribution of X is
given by
Pr (X = x) = p (x) =

y
p (x, y) =

y
p (x|y) p (y) .
(1.63)
Above, the sum is over all possible values of Y , each weighted by its
marginal probability Pr (Y = y). For instance, if one were interested in
Ô¨Ånding the marginal distribution of genotypes at locus A from the joint
distribution of genotypes at the two loci, application of (1.63) yields
Pr(X = x) = p(x|BB)p(BB) + p(x|Bb)p(Bb) + p(x|bb)p(bb)
where x = AA, Aa, or aa. This gives the total probability that X = x
(Aa, say) as the sum of the probabilities of the three ways in which this
can occur, i.e., when X = Aa jointly with Y being either BB, Bb, or bb.
Further, the conditional probability that Y = BB given that X = Aa can

32
1. Probability and Random Variables
be written, making use of (1.60), as
Pr(Y = BB|X = Aa)
=
p(Aa|BB)p(BB)
p(Aa|BB)p(BB) + p(Aa|Bb)p(Bb) + p(Aa|bb)p(bb)
=
p(BB|Aa)p(Aa)
p(Aa|BB)p(BB) + p(Aa|Bb)p(Bb) + p(Aa|bb)p(bb).
In the continuous case, the counterpart of (1.63) is the two-dimensional
analogue of (1.56), and is obtained by integrating over the distribution of
Y :
p (x) =

p (x, y) dy =

p (x|y) p (y) dy.
(1.64)
Results (1.60) and (1.62) are fairly general, and apply to either scalar or
vector variates. The random variables can be either discrete or continuous,
or one can be discrete while the other continuous. For example, suppose Y
is the genotype at a molecularly marked locus (discrete random variable)
and X is the breeding value at a QTL for growth rate in pigs (continuous).
If the observation that Y = y alters our knowledge about the distribution
of breeding values, then one can exploit this stochastic dependence in a
marker-assisted selection program for faster growth. Otherwise, if knowl-
edge of the marked locus does not contribute information about growth
rate, we would be in the situation depicted by (1.62).
Often, a joint density can be identiÔ¨Åed only up to proportionality. As in
the univariate case, one can write p(x, y) = Cf(x, y), where C is a constant
(i.e., it does not depend on X and Y ), and f(x, y) is known as the kernel
of the joint density. Hence,
p (x, y) =
f (x, y)
 
f (x, y) dx dy .
(1.65)
It follows that
C‚àí1 =
 
f (x, y) dx dy.
(1.66)
Before reviewing standard multivariate distributions, a number of exam-
ples are discussed to illustrate some of the ideas presented so far.
Example 1.10
Aitken‚Äôs integral and the bivariate normal distribution
A useful result in multivariate normal theory is Aitken‚Äôs integral (e.g.,
Searle, 1971). Here assume that the random variables X and Y possess
what is called a bivariate normal distribution, to be discussed in more
detail below. Such a process can be represented as

X
Y

‚àºN


¬µX
¬µY

,

œÉ2
X
œÉXY
œÉXY
œÉ2
Y

,

1.4 Multivariate Probability Distributions
33
where ¬µX = E (X), ¬µY = E (Y ) , and
V =

œÉ2
X
œÉXY
œÉXY
œÉ2
Y

is the 2√ó2 positive-deÔ¨Ånite variance‚Äìcovariance matrix of the distribution.
The joint density of X and Y is
p(x, y|¬µX, ¬µY , V) =
1
2œÄ |V|
1
2
√ó exp

‚àí1
2

X ‚àí¬µX,
Y ‚àí¬µY
‚Ä≤ V‚àí1

X ‚àí¬µX
Y ‚àí¬µY

.
(1.67)
The kernel of the density is the expression exp [¬∑], and Aitken‚Äôs integral is
 
exp

‚àí1
2

X ‚àí¬µX,
Y ‚àí¬µY
‚Ä≤ V‚àí1

X ‚àí¬µX
Y ‚àí¬µY

dx dy
=
2œÄ |V|
1
2
which follows because p(x, y|¬µX, ¬µY , V) integrates to 1. Then C‚àí1 = 2œÄ |V|
1
2 ,
and the bivariate normal density can be represented as
p(x, y|¬µX, ¬µY , V) =
exp

‚àí1
2

X ‚àí¬µX,
Y ‚àí¬µY
‚Ä≤ V‚àí1

X ‚àí¬µX
Y ‚àí¬µY

 
exp

‚àí1
2

X ‚àí¬µX,
Y ‚àí¬µY
‚Ä≤ V‚àí1

X ‚àí¬µX
Y ‚àí¬µY

dx dy
.
‚ñ†
Example 1.11
A discrete bivariate distribution
This example is elaborated after Casella and Berger (1990). Mastitis is
a serious disease of the mammary gland in milk producing animals, and
it has important economic consequences in dairy farming. Suppose that
in a breed of sheep, the disease appears in three mutually exclusive and
exhaustive modalities: absent, subclinical, or clinical. Let Y be a random
variable denoting the disease status. Also, let X be another random variable
taking two values only, X = 1 or X = 2, representing ewes that are born as
singles or twins, respectively. From knowledge of the population, it is known
that the bivariate random vector (X, Y ) has a joint probability distribution
given by:
Y = 1
Y = 2
Y = 3
p (x)
X = 1
1/10
2/10
2/10
5/10
X = 2
1/10
1/10
3/10
5/10
p (y)
2/10
3/10
5/10

34
1. Probability and Random Variables
The entries in this table give the joint probabilities of the six possible
events. The element in the Ô¨Årst row and column represents
Pr (X = 1, Y = 1) = 1/10.
The last column gives the marginal probability distribution of X, and the
last row presents the marginal probability distribution of Y . For example,
the marginal probability of an animal born as a single (X = 1) is obtained
as
Pr (X = 1) = Pr (X = 1, Y = 1) + Pr (X = 1, Y = 2)
+ Pr (X = 1, Y = 3) = 5
10.
This can also be obtained from
Pr (X = 1) =
i=3

i=1
Pr (X = 1|Y = yi) Pr (Y = yi)
=

1
2
2
10

+

2
3
3
10

+

2
5
5
10

= 5
10.
The random variables X and Y are not independent because
p (x, y) Ã∏= p (x) p (y) .
For example,
p (1, 3) = 1
5 Ã∏= 1
2
1
2 = Pr (X = 1) Pr (Y = 3) .
Note that there are some (X, Y ) values for which their joint probability is
equal to the product of their marginals. For example,
p (1, 1) = 1
10 = 1
2
1
5 = Pr (X = 1) Pr (Y = 1) .
However, this does not ensure independence, as seen from the case of p(1, 3).
All values must be checked.
‚ñ†
Example 1.12
Two independent continuous random variables
Suppose the following function is a suitable density arising in the descrip-
tion of uncertainty about the pair of continuous random variables X and
Y
p (x, y) =

6xy2,
0 < x < 1, 0 < y < 1,
0,
otherwise.
The joint sample space can be viewed as a square of unit length. We check
Ô¨Årst whether this function is suitable as a joint p.d.f., by integrating it over

1.4 Multivariate Probability Distributions
35
the entire sample space
‚àû

‚àí‚àû
‚àû

‚àí‚àû
p (x, y) dx dy
=
1

0
1

0
6xy2dx dy
=
1

0
3y2dy = y3""1
0
=
1.
It is not essential that the function integrates to 1; the only requirement
is that the integral must be Ô¨Ånite. For example, if it integrates to K, say,
then the joint p.d.f. would be 6xy2/K. When a density function does not
integrate to a Ô¨Ånite value, the corresponding distribution is said to be im-
proper. Improper distributions arise in certain forms of Bayesian analysis,
as will be seen later on. Employing (1.64), the marginal density of Y is
obtained by integrating the joint density p(x, y) over the distribution of X
p (y) =
1

0
p (x, y) dx =
1

0
6xy2 dx = 6y2 x2
2
""""
1
0
= 3y2.
It can be veriÔ¨Åed that the resulting density integrates to 1. We say that X
has been ‚Äúintegrated out‚Äù of the joint density; the resulting marginal p.d.f.
of Y is, therefore, not a function of X. This is important in a Bayesian
context, where ‚Äúnuisance parameters‚Äù are eliminated by integration. For
example, if X were a nuisance parameter in a Bayesian model, the marginal
density of the parameter of interest (Y in this case) would not depend on
X. Likewise, the marginal density of X is
p (x) =
1

0
p (x, y) dy =
1

0
6xy2 dy = 2x,
and this integrates to 1 as well. The density function of the conditional
distribution of Y given X is, using (1.60),
p (y|x) = p (x, y)
p (x)
= 6xy2
2x
= 3y2, for 0 < y < 1.
Similarly, the conditional density of X given Y is
p (x|y) = p (x, y)
p (y)
= 6xy2
3y2 = 2x.
Hence, the conditional distribution [Y |X] does not depend on X, and the
conditional distribution [X|Y ] does not depend on Y (in general, the nota-
tion [A|B, C] is employed to refer to the conditional distribution of random

36
1. Probability and Random Variables
variable A given random variables B and C). This is because X and Y are
independent random variables: their joint density is obtained by multipli-
cation of the marginal densities
p (x, y) = p (x) p (y) = 2x3y2 = 6xy2.
Because of independence, p (y|x) = p (y) and p (x|y) = p (x), as already
seen.
We can calculate the regression curve, or regression function, of Y on X,
which is denoted by E (Y |X = x). This function, using the assumptions
made in the preceding example, is
E (Y |X = x) =
1

0
yp (y|x) dy =
1

0
y3y2 dy = 3
4.
(1.68)
This is a constant because Y is independent of X, so E(Y |x) = E(Y ) here.
The uncertainty about the distribution of Y , once we know that X = x,
can be assessed through the variance of the distribution [Y |X], that is, by
V ar (Y |X = x). By deÔ¨Ånition
V ar (Y |X = x) = E

Y 2|x

‚àíE2 (Y |x) .
For the example above, having computed E(Y |x) = E(Y ) = 3/4, we need
now
E

Y 2|x

=
1

0
y2 p (y|x) dy =
1

0
y2 p (y) dy = E

Y 2
= 3
5
so
V ar (Y |X = x) = 3/5 ‚àí[3/4]2 = 3/80,
which is equal to the variance of the distribution of Y in this case. As
expected (because of the independence noted), the variance does not de-
pend on X. Here, it is said that the dispersion is homoscedastic or constant
throughout the regression line.
‚ñ†
Example 1.13
A continuous bivariate distribution
Two random variables have as joint p.d.f.
p (x, y) =

x + y,
0 ‚â§x ‚â§1, 0 ‚â§y ‚â§1,
0,
otherwise.
The marginal densities of X and Y are
p (x)
=
1

0
p (x, y) dy = x + 1
2,
p (y)
=
y + 1
2.

1.4 Multivariate Probability Distributions
37
The probability that X is between 1/2 and 1 can be found to be equal to
1

1
2
(x + 1/ 2) dx = 5/8.
The probability that X is between 1/2 and 1 and that Y is between 0 and
1/2 is
1

1
2
1
2

0
(x + y) dx dy = 1/4.
The variables X and Y are not independent, as p (x, y) Ã∏= p (x) p (y). The
conditional density of Y given x is
p (y|x) = p (y, x)
p (x)
= y + x
x + 1
2
.
The regression function of Y on X is
E (Y |X = x) =
1

0
yp (y|x) dy =
1

0
y (y + x)
x + 1
2
dy =
3x + 2
3 (1 + 2x),
which is a decreasing, nonlinear function of X. The variance of the condi-
tional distribution [Y |X] can be found to be equal to
V ar (Y |X = x) =

1 + 6x + 6x2

18 (1 + 2x)2 ,
0 ‚â§x ‚â§1.
Here the conditional distribution is not homoscedastic. For example, for
x = 0 and x = 1, the conditional variance is equal to 1/18 and 13/162,
respectively.
‚ñ†
1.4.1
The Multinomial Distribution
The binomial distribution is generalized to the multinomial distribution as
follows. Let C1, C2, . . . , Ck represent k mutually exclusive and exhaustive
classes or outcomes. Imagine an experiment consisting of n independent
trials and, in each trial, only one of these k distinct outcomes is possible.
The probability of occurrence of outcome i is pi (i = 1, 2, . . . , k) on every
trial. Let Xi be a random variable corresponding to the number (counts)
of times that the ith outcome occurred in the n trials. When k = 2, this
is a binomial experiment and X1 counts the number of ‚Äúsuccesses‚Äù and
X2 = n ‚àíX1 counts the number of ‚Äúfailures‚Äù in the n independent trials.

38
1. Probability and Random Variables
The p.m.f. of the random vector (X1, X2, . . . , Xk) is
Pr (X1 = x1, X2 = x2, . . . , Xk = xk|X1 + X2 + ¬∑ ¬∑ ¬∑ + Xk = n)
=
n!
x1! x2! . . . xk!px1
1 px2
2 . . . pxk
k ,
(1.69)
with k
i=1 xi = n, k
i=1 pi = 1. Therefore an alternative way of writing
(1.69) is
Pr (X1 = x1, X2 = x2, . . . , Xk = xk|X1 + X2 + ¬∑ ¬∑ ¬∑ + Xk = n)
= n!px1
1 px2
2 . . . (1 ‚àíp1 ‚àí¬∑ ¬∑ ¬∑ ‚àípk‚àí1)(n‚àíx1‚àí¬∑¬∑¬∑‚àíxk‚àí1)
x1!x2! . . . (n ‚àíx1 ‚àí¬∑ ¬∑ ¬∑ ‚àíxk‚àí1)!
.
Observe that the k random variables (X1, X2, . . . , Xk) are not independent;
any k ‚àí1 of them determines the kth. The mean and (co)variance of this
distribution are
E (Xi) = npi,
V ar (Xi) = npi (1 ‚àípi) ,
Cov (Xi, Xj) = ‚àínpipj,
i Ã∏= j.
The n trials can be divided into two classes: Xi counts belonging to out-
come i, and the remaining events corresponding to ‚Äúnon-i‚Äù. The marginal
distribution of the random variable Xi is binomial with p.m.f.
Bi (xi|pi, n) .
(1.70)
Further, given Xi, the vector X‚àíi = (X1, . . . , Xi‚àí1, Xi+1, . . . , Xk) is
multinomially distributed
X‚àíi|Xi ‚àºMu (q1, . . . , qi‚àí1, qi+1, . . . , qk, n ‚àíXi) ,
(1.71)
where qj = pj/(1 ‚àípi).
Let
X‚àíi,‚àíj = (X1, . . . , Xi‚àí1, Xi+1, . . . , Xj‚àí1, Xj+1, . . . , Xk) .
Then the p.m.f. of the distribution [X‚àíi,‚àíj|Xi, Xj] is,
Mu (x‚àíi,‚àíj|r1, . . . , ri‚àí1, ri+1, . . . , rj‚àí1, rj+1, . . . , rk, n ‚àíXi ‚àíXj) (1.72)
where rs = ps/ (1 ‚àípi ‚àípj).
As in the binomial case, a generalization of (1.16) exists. As n ‚Üí‚àû, the
vector of observed responses X will tend to a multivariate normal distri-
bution (to be discussed below) with mean vector
E (X) = {npi} ,
i = 1, . . . , k,
(1.73)

1.4 Multivariate Probability Distributions
39
and covariance matrix
V =n
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞
p1 (1 ‚àíp1)
‚àíp1p2
¬∑ ¬∑ ¬∑
‚àíp1pk
‚àíp2p1
p2 (1 ‚àíp2)
¬∑ ¬∑ ¬∑
‚àíp2pk
...
...
...
...
‚àípkp1
‚àípkp2
¬∑ ¬∑ ¬∑
pk (1 ‚àípk)
Ô£π
Ô£∫Ô£∫Ô£∫Ô£ª.
(1.74)
Summing the elements of any row of (1.74), the ith, say, yields
npi (1 ‚àíp1 ‚àíp2 ‚àí¬∑ ¬∑ ¬∑ ‚àípk) = 0.
Hence V has rank k ‚àí1. A generalized inverse of V can readily be shown
(Stuart and Ord, 1991) to be equal to
V‚àí= 1
n
 ,V‚àí1
0
0
0

where ,V‚àí1 is a (k ‚àí1) √ó (k ‚àí1) matrix with diagonal elements (1/pi) +
(1/pk) (i = 1, 2, . . . , k ‚àí1) and oÔ¨Ä-diagonals 1/pk. Therefore the exponent
of the limiting singular multivariate normal distribution has the following
form:
(x‚àínp)‚Ä≤ V‚àí(x‚àínp) ,
(1.75)
where p‚Ä≤ = (p1, . . . , pk). For large n, the random variable deÔ¨Åned in (1.75)
has a central chi-square distribution with tr (V‚àíV) = (k ‚àí1) degrees of
freedom (Searle, 1971). This gives the basis for the usual test of goodness
of Ô¨Åt.
Example 1.14
Generating samples from the multinomial distribution
A general procedure for simulating multivariate random variables is based
on the composition or conditional distribution method (Devroye, 1986).
Let
X = (X1, . . . , Xd)
denote a d-dimensional random vector with p.d.f. f (x). Then,
f (x) = f (x1) f (x2|x1) f (x3|x1, x2) . . . f (xd|x1, . . . , xd‚àí1) .
If the marginal distributions and all the conditional distributions are known,
this method allows reducing a multivariate generation problem to d uni-
variate generations.
As an example, consider generating a multinomial random variable
(X1, X2, X3, X4) ‚àºMu (p1, p2, p3, p4, n)
with 4
i=1 pi = 1 and 4
i=1 Xi = n. Using (1.70), (1.71), and (1.72), one
proceeds as follows. Generate
X1 ‚àºBi (p1, n) ,

40
1. Probability and Random Variables
X2|X1 = x1 ‚àºBi

p2
1 ‚àíp1
, n ‚àíx1

,
X3|X1 = x1, X2 = x2 ‚àºBi

p3
1 ‚àíp1 ‚àíp2
, n ‚àíx1 ‚àíx2

.
Set
X4 = n ‚àíx1 ‚àíx2 ‚àíx3.
The vector (x1, x2, x3, x4) is a realized value from X. If at any time in the
simulation n = 0, use the convention that a Bi (p, 0) random variable is
identically zero (Gelman et al., 1995).
‚ñ†
1.4.2
The Dirichlet Distribution
The Dirichlet distribution is a multivariate generalization of the beta distri-
bution. For example, as discussed in Chapter 11, Example 11.7, the Dirich-
let is a natural model for the distribution of gene frequencies at a locus
with more than two alleles. The random vector
X = (X1, X2, . . . , Xk) ,
X1, X2, . . . , Xk ‚â•0, k
i=1 Xi = 1, follows the Dirichlet distribution of
dimension k, with parameters
Œ± = (Œ±1, Œ±2, . . . , Œ±k) ,
Œ±j > 0,
if its probability density Dik (x|Œ±) is
p (x|Œ±) = Œì (Œ±1 + Œ±2 + ¬∑ ¬∑ ¬∑ + Œ±k)
Œì (Œ±1) Œì (Œ±2) . . . Œì (Œ±k)
k
-
i=1
xŒ±i‚àí1
i
.
(1.76)
A simple and eÔ¨Écient way to sample from (1.76) is Ô¨Årst to draw k inde-
pendent gamma random variables with scale parameter Œ±i and unit scale:
Ga (yi|Œ±i, 1). Then form the ratios
xi =
yi
k
j=1 yi
,
i = 1, . . . , k.
The vector (x1, x2, . . . , xk) is a realized value from (1.76).
1.4.3
The d-Dimensional Uniform Distribution
A d √ó 1 random vector x is uniformly distributed on [0, 1]d if
p (x) =

1,
if x ‚àà[0, 1]d ,
0,
otherwise.
From (1.56), each scalar element of x is distributed as [0, 1]. Often, mul-
tidimensional uniform distributions are assigned as prior distributions to
some of the parameters of a Bayesian model.

1.4 Multivariate Probability Distributions
41
1.4.4
The Multivariate Normal Distribution
In this subsection, the multivariate normal distribution is introduced, fol-
lowed by a presentation of the marginal and conditional distributions in-
duced by this process, of its moment generating function, of the distribution
of linear combinations of normal variates, and by a simple derivation of the
central limit theorem. The subsection concludes with examples that illus-
trate applications of the multivariate normal distribution in quantitative
genetics.
Density, Mean Vector, and Variance‚ÄìCovariance Matrix
Let y denote a random vector of dimension n (the notational distinction
between a random variable and its realized value is omitted here). This
vector is said to have an n-dimensional multivariate normal distribution if
its p.d.f. is
p (y|m, V) = |2œÄV|‚àí1/2 exp

‚àí1
2 (y ‚àím)‚Ä≤ V‚àí1 (y ‚àím)

(1.77)
where
m =E(y|m, V) =

yp (y|m, V) dy
(1.78)
is the mean vector, and
V
=
E[(y ‚àím)(y ‚àím)‚Ä≤] =

(y ‚àím)(y ‚àím)‚Ä≤p (y|m, V) dy
=

yy‚Ä≤p (y|m, V) dy ‚àímm‚Ä≤
(1.79)
is the variance‚Äìcovariance matrix of the distribution, assumed to be non-
singular. All integrals are n-dimensional, and taken over Rn, the entire
n-dimensional space. The notation dy is used for dy1dy2 . . . dyn. From the
density in (1.77) it follows that

exp

‚àí1
2 (y ‚àím)‚Ä≤ V‚àí1 (y ‚àím)

dy = |2œÄV|1/2 .
This is a generalization of Aitken‚Äôs integral seen in Example 1.10
Marginal and Conditional Distributions
Partition the vector of random variables as y = [y‚Ä≤
1, y‚Ä≤
2]‚Ä≤, with the corre-
sponding partitions of m and V being
m
=
[m‚Ä≤
1, m‚Ä≤
2]‚Ä≤
V
=

V11
V12
V21
V22

.

42
1. Probability and Random Variables
For any arbitrary partition, it can be shown that all marginal distributions
are normal. For example, the marginal density of y1 is
p (y1|m1, V11) =
‚àû

‚àí‚àû
p (y1, y2|m, V) dy2
= (2œÄ)‚àín1
2 |V11|
1
2 exp

‚àí1
2 (y1‚àím‚Ä≤
1)‚Ä≤ V‚àí1
11 (y1‚àím1)

,
(1.80)
where n1 is the order of y1. The conditional distribution
[y1|y2, m, V]
is normal as well, with density
p (y1|y2, m, V) = (2œÄ)‚àín1
2 |V ar (y1|y2)|‚àí1
2
√ó exp

‚àí1
2 [y1 ‚àíE (y1|y2)]‚Ä≤ V ar‚àí1 (y1|y2) [y1 ‚àíE (y1|y2)]
%
.
(1.81)
This holds for any partition of y, irrespective of whether the components
are scalars or vectors. The mean vector and covariance matrix of this con-
ditional distribution are
E (y1|y2, m, V) = m1 + V12V‚àí1
22 (y2‚àím2)
(1.82)
and
V ar (y1|y2, m, V) = V11 ‚àíV12V‚àí1
22 V21,
(1.83)
respectively. The variance‚Äìcovariance matrix does not depend on y2, so this
conditional distribution is homoscedastic. This is an important feature of
the multivariate normal distribution. Another important fact to be noticed
is that marginal and conditional normality is arrived at assuming bivariate
normality as point of departure. However, marginal normality does not
imply joint normality.
A suÔ¨Écient condition for independence in the multivariate normal dis-
tribution is that the variance‚Äìcovariance matrix is diagonal. For example,
suppose that
V = IœÉ2,
where œÉ2 is a positive scalar, and that E (yi) = m (i = 1, 2, ..., n). Then
(y ‚àím)‚Ä≤ V‚àí1 (y ‚àím) = 1
œÉ2
n

i=1
(yi ‚àím)2
and
|2œÄV|‚àí1
2 =
""2œÄIœÉ2""‚àí1
2 =

2œÄœÉ2‚àín
2 |I| =

2œÄœÉ2‚àín
2 .

1.4 Multivariate Probability Distributions
43
Using this in (1.77) yields
p (y|m, V) =
1
(2œÄœÉ2)
n
2 exp
Ô£Æ
Ô£ØÔ£ØÔ£∞‚àí
n
i=1
(yi ‚àím)2
2œÉ2
Ô£π
Ô£∫Ô£∫Ô£ª=
n
-
i=1
p

yi|m, œÉ2
which is the product of the densities of n i.i.d. normal variates, each with
mean m and variance œÉ2.
If the matrix V is singular, then y is said to have a singular or degenerate
normal distribution. If the rank of V is k < n, the singular density can be
written as
(2œÄ)‚àík/2
(Œª1Œª2 . . . Œªk) 1/2 exp

‚àí1
2 (y ‚àím)‚Ä≤ V‚àí(y ‚àím)

,
where V‚àíis a generalized inverse of V and Œª1, Œª2, . . . , Œªk are the nonzero
eigenvalues of V. Details can be found, for example, in Searle (1971), Rao
(1973), Anderson (1984), and Mardia et al. (1979).
Moment Generating Function
Let y ‚àºN (m, V) . The formula for the univariate case in (1.49) extends to
the multivariate normal situation, after similar algebra (Searle, 1971), to
M (t) = E [exp (t‚Ä≤y)] =
1
|2œÄV|1/2
√ó
‚àû

‚àí‚àû
exp [t‚Ä≤y] exp

‚àí1
2 (y ‚àím)‚Ä≤ V‚àí1 (y ‚àím)

dy
= exp

t‚Ä≤m+t‚Ä≤Vt
2

,
(1.84)
where t is a dummy vector of order n. DiÔ¨Äerentiation of the moment gen-
erating function with respect to t yields
‚àÇM (t)
‚àÇt
= exp

t‚Ä≤m+t‚Ä≤Vt
2

(m + Vt)
and putting t = 0 gives directly E (y) = m. An additional diÔ¨Äerentiation
gives
‚àÇ2M (t)
‚àÇt ‚àÇt‚Ä≤
= exp

t‚Ä≤m + t‚Ä≤Vt
2
 
V+ (m + Vt) (m + Vt)‚Ä≤
which, for t = 0, leads to E (yy‚Ä≤) = V + mm‚Ä≤. The covariance matrix is
then E (yy‚Ä≤) ‚àíE (y) E (y‚Ä≤) = V.

44
1. Probability and Random Variables
Linear Functions of Normally Distributed Random Variables
Another important property of the multivariate Gaussian distribution is
that a linear transformation of a normally distributed vector is normal
as well. Let x = Œ± + Œ≤‚Ä≤y be a random variable resulting from a linear
combination of the multivariate normal vector y, where Œ± is a known scalar
and Œ≤ is a vector, also known. The moment generating function of x is then
E

exp

Œ± + Œ≤‚Ä≤y

t

= exp (Œ±t) E

exp

tŒ≤‚Ä≤y

= exp (Œ±t) M (t‚àó) ,
where t‚àó= tŒ≤. Making use of (1.84) in the preceding expression gives
E

exp

Œ± + Œ≤‚Ä≤y

t

=
exp (Œ±t) exp

t‚àó‚Ä≤m + t‚àó‚Ä≤Vt‚àó
2

=
exp

t

Œ± + Œ≤‚Ä≤m

+ t2 
Œ≤‚Ä≤VŒ≤

2

.
This is the moment generating function of a normal random variable with
mean Œ± + Œ≤‚Ä≤m and variance Œ≤‚Ä≤VŒ≤. The property is important in quanti-
tative genetics under additive inheritance: here, the additive genetic value
of an oÔ¨Äspring is equal to the average value of the parents, plus a residual.
If all these terms follow a multivariate normal distribution, it follows that
the additive value in the progeny is normally distributed as well.
Central Limit Theorem
As stated, the normal distribution has played a central role in statistics
and quantitative genetics. An example is the so-called inÔ¨Ånitesimal model
(Fisher, 1918; Bulmer, 1971). Here, y in (1.77) represents a vector of ad-
ditive genetic values and V is a function of additive genetic relationships
between subjects, or of twice the coeÔ¨Écients of coancestry (Mal¬¥ecot, 1969).
This matrix of coancestries enters when quantifying the process of genetic
drift, in estimation of genetic variance‚Äìcovariance components, and in in-
ferences about additive genetic values and functions thereof in animal and
plant breeding.
The inÔ¨Ånitesimal model posits that the additive genetic value for a quan-
titative trait is the result of the sum of values at each of an inÔ¨Ånite number
loci. If the population in question is in joint equilibrium at all loci as a
result of random mating without selection over many generations, the con-
tributions from the diÔ¨Äerent loci will be statistically independent from each
other. This will be true even under the presence of linkage, since linkage
slows down the approach to equilibrium but does not change the equilib-
rium ultimately attained. The celebrated central limit theorem leads to the
result that the additive genetic value follows a normal distribution, approx-
imately, irrespective of the distribution of eÔ¨Äects at individual loci. Here,
borrowing from Bulmer (1979), it is shown that this is so.

1.4 Multivariate Probability Distributions
45
Let Y = n
i=1 Yi be the additive genetic value of an individual, and let
Yi be the value at locus i. The mean and variance at locus i are ¬µi and œÉ2
i ,
respectively, so that E (Y ) = n
i=1 ¬µi = ¬µ and V ar (Y ) = n
i=1 œÉ2
i = œÉ2,
say, as the eÔ¨Äects at diÔ¨Äerent loci are mutually independent of each other.
Consider the moment generating function of the standardized variate Z =
(Y ‚àí¬µ) /œÉ:
MZ (t) = E [exp (tZ)] = E
#
exp
 n

i=1
tZi
$
,
where Zi = (Yi ‚àí¬µi) /œÉ and 
i Zi = Z. Further,
MZ (t) =

exp
 n

i=1
tZi

p (z1, z2, ..., zn) dz
=
n
-
i=1
E

exp
t (Yi ‚àí¬µi)
œÉ
%
=
n
-
i=1
Mi

 t
œÉ

,
(1.85)
where Mi (t) is the moment generating function of (Yi ‚àí¬µi) . The preceding
follows because the n random variables Yi are mutually independent. Using
(1.48), one can write
Mi

 t
œÉ

= E
t (Yi ‚àí¬µi)
œÉ

= 1 + tE (Yi ‚àí¬µi)
œÉ
+ t2E (Yi ‚àí¬µi)2
2œÉ2
+ ¬∑ ¬∑ ¬∑
¬∑ ¬∑ ¬∑ + tkE (Yi ‚àí¬µi)k
k!œÉk
+ ¬∑ ¬∑ ¬∑
‚âà1 + t2œÉ2
i
2œÉ2 + ¬∑ ¬∑ ¬∑ .
(1.86)
This is so, because for large n, third- and higher-order moments (from the
mean) for small individual loci eÔ¨Äects are small, relative to œÉ3, œÉ4, etc., so
the corresponding terms can be ignored. Then, employing (1.86) in (1.85)
gives
log [MZ (t)] =
n

i=1
log

Mi

 t
œÉ

‚âà
n

i=1
log

1 + t2œÉ2
i
2œÉ2

.
(1.87)
Now consider an expansion about 0 of the function
log (1 + x) = log (1) + x ‚àíx2 + 2x3 + ¬∑ ¬∑ ¬∑ ‚âàx.
The approximation results from the fact that for values of x near 0, higher-
order terms can be neglected. Using this in (1.87) gives
MZ (t) ‚âàexp
n

i=1
t2œÉ2
i
2œÉ2 = exp

t2
2

(1.88)

46
1. Probability and Random Variables
which is the moment generating function of a normally distributed variable
with mean 0 and variance 1; this can be veriÔ¨Åed by inspection of (1.50).
Thus, approximately, Z ‚àºN (0, 1) and, since Y = ¬µ + ZœÉ is a linear
combination of Z, it follows that Y ‚àº

¬µ, œÉ2
is approximately normal as
well. (A little more formally, it is the c.d.f. of Z that converges to the c.d.f.
of the N (0, 1), as n ‚Üí‚àû. This is known as convergence in distribution).
As pointed out by Bulmer (1979), the central limit theorem explains
why many observed distributions ‚Äúlook‚Äù normal. To the extent that ran-
dom variation is the result of a large number of independent factors acting
additively, each making a small contribution to the total variation, the
resulting distribution should be close to a normal one.
There are extensions of the central limit theorem for variables that are
independent but not identically distributed, and for dependent random
variables. The latter is particularly relevant for the study of time series and
Markov processes. A good starting point is the book of Lehmann (1999).
Example 1.15
A tetravariate normal distribution
Consider two genetically related individuals and suppose that a measure-
ment, e.g., stature, is taken on each of them. Assume that the inÔ¨Ånitesi-
mal additive genetic model described above operates. The expected genetic
value of a child, given the genetic values of the parents, is equal to the av-
erage of the genetic values of the father and mother. Also, assume that
the population from which the two individuals are drawn randomly, is ho-
mogeneous in every possible respect. Let the model for the measurements
be (we relax the distinction between random variables and their realized
values unless the setting requires maintaining it)
y1
=
a1 + e1,
y2
=
a2 + e2.
Further, suppose that E (ai) = E (ei) = 0 (i = 1, 2), so that E (yi) = 0,
and that Cov (ai, ei) = 0, implying that V ar (yi) = œÉ2
a + œÉ2
e, where œÉ2
a
is the additive genetic variance and œÉ2
e is the residual or environmental
variance. Also, let Cov (e1, e2) = 0; thus, Cov (y1, y2) = Cov (a1, a2) =
r12œÉ2
a where r12 is the additive genetic relationship between individuals 1
and 2. The additive relationship is equal to twice the probability that a
randomly chosen allele drawn from a locus from individual 1 is identical
by descent (i.e., it is a biochemical copy from a common ancestor) to a
randomly chosen allele taken from the same locus of individual 2 (Mal¬¥ecot,
1969). For example, the average additive genetic relationship between two
full-sibs is equal to 1/2, whereas that between an individual and itself, in
the absence of inbreeding, is equal to 1. Under multivariate normality, if
pairs of such individuals are drawn at random from the population, the
joint distribution of measurements and of additive genetic values can be

1.4 Multivariate Probability Distributions
47
represented as
Ô£Æ
Ô£ØÔ£ØÔ£∞
y1
y2
a1
a2
Ô£π
Ô£∫Ô£∫Ô£ª‚àºN
Ô£´
Ô£¨
Ô£¨
Ô£≠
Ô£Æ
Ô£ØÔ£ØÔ£∞
0
0
0
0
Ô£π
Ô£∫Ô£∫Ô£ª,
Ô£Æ
Ô£ØÔ£ØÔ£∞
œÉ2
a + œÉ2
e
r12œÉ2
a
œÉ2
a
r12œÉ2
a
r12œÉ2
a
œÉ2
a + œÉ2
e
r12œÉ2
a
œÉ2
a
œÉ2
a
r12œÉ2
a
œÉ2
a
r12œÉ2
a
r12œÉ2
a
œÉ2
a
r12œÉ2
a
œÉ2
a
Ô£π
Ô£∫Ô£∫Ô£ª
Ô£∂
Ô£∑
Ô£∑
Ô£∏.
In what follows, use is made of results (1.77) through (1.83). First, the
mean and variance of the conditional distribution [y1|a1] is derived, with
the dependence on the parameters suppressed in the notation. Because of
the assumption of joint normality, this distribution must be normal as well.
The mean and variance are given by
E (y1|a1)
=
0 + œÉ2
a

œÉ2
a
‚àí1 (a1 ‚àí0) = a1,
V ar (y1|a1)
=
œÉ2
a + œÉ2
e ‚àíœÉ2
a

œÉ2
a
‚àí1 œÉ2
a = œÉ2
e,
and these are calculated with (1.82) and (1.83), respectively. The expected
value and variance of this conditional distribution can also be deduced
directly from the model, and also hold in the absence of normality, provided
that the covariance between the additive genetic value and the residual
deviation of the same individual is null. This can be veriÔ¨Åed simply by
Ô¨Åxing the additive genetic value, and then taking the expectation and the
variance under this assumption.
Consider now the conditional distribution [y1|a1, a2]. Intuitively, it is clear
that this must be the same as [y1|a1] because, given the additive genetic
value of individual 1, knowledge of the genetic value of individual 2 should
not provide any additional information about the stature of individual 1.
Having Ô¨Åxed a1, the only remaining term in the model is the residual e1,
and this is independent of a2. Anyhow, the distribution [y1|a1, a2] must be
normal, and application of (1.82) and (1.83) yields
E (y1|a1, a2) =

œÉ2
a
r12œÉ2
a
 
œÉ2
a
r12œÉ2
a
r12œÉ2
a
œÉ2
a
‚àí1 
a1
a2

= a1
and
V ar (y1|a1, a2) = œÉ2
a + œÉ2
e
‚àí
 œÉ2
a
r12œÉ2
a
 
œÉ2
a
r12œÉ2
a
r12œÉ2
a
œÉ2
a
‚àí1 
œÉ2
a
rœÉ2
a

= œÉ2
e.
Because the mean and variance are suÔ¨Écient to identify fully the desired
normal distribution, it follows that [y1|a1, a2] = [y1|a1] , as expected. Fur-
ther, given the genotypic values, the observations are conditionally inde-
pendent. Thus, the joint density of the two measurements, given the two
genetic values, can be written as
p (y1, y2|a1, a2)
=
p (y1|a1, a2) p (y2|a1, a2)
=
p (y1|a1) p (y2|a2) .

48
1. Probability and Random Variables
Exploiting situations of conditional independence is a key issue in the
Bayesian analysis of hierarchical models. This is discussed in Chapter 6.
We turn attention now to the distribution [y1|a2]. Again, this conditional
probability distribution is normal, with mean
E (y1|a2) = 0 + r12œÉ2
a

œÉ2
a
‚àí1 a2 = r12a2
and variance:
V ar (y1|a2)
=
œÉ2
a + œÉ2
e ‚àír12œÉ2
a

œÉ2
a
‚àí1 r12œÉ2
a
=
œÉ2
a

1 ‚àír2
12

+ œÉ2
e.
The variance of this distribution is smaller than that of the marginal dis-
tribution of y1, but is larger than the variance of [y1|a1] ; this is because r12
is larger than 0, although it cannot exceed 1. Conversely, the distribution
[a2|y1] has mean and variance
E (a2|y1) =
r12œÉ2
a
œÉ2a + œÉ2e
y1,
V ar (a2|y1) = œÉ2
a

1 ‚àír2
12
œÉ2
a
œÉ2a + œÉ2e

= œÉ2
a

1 ‚àír2
12h2
,
where h2 = œÉ2
a
4 
œÉ2
a + œÉ2
e

is called ‚Äúheritability‚Äù in quantitative genetics.
This parameter measures the fraction by which measurable diÔ¨Äerences be-
tween parents for a given trait are expected to be recovered in the next
generation, under the supposition of additive Mendelian inheritance. The
preceding mean and variance would be the parameters of the normal dis-
tribution used to infer the unobservable genetic value of individual 2 using
a measurement (y1) on related individual 1. Letting r12 = 1, one obtains
the formulas needed to make inferences about the genetic value of animal
1 using his measurement or phenotypic value (remember that in genetics,
‚Äúphenotype‚Äù means whatever can be observed in an individual, e.g., a blood
group, or the testosterone level of a bull at a speciÔ¨Åed age).
Another conditional distribution of interest might be [y1|y2]. This pro-
cess can be useful in a situation where, for example, one wishes to make
probability statements about the phenotype of individual 1 based on a mea-
surement obtained on relative 2. This distribution is normal with mean
E (y1|y2)
=
0 + r12œÉ2
a

œÉ2
a + œÉ2
e
‚àí1 y2
=
r12h2y2
and variance
V ar (y1|y2)
=
œÉ2
a + œÉ2
e ‚àír12œÉ2
a

œÉ2
a + œÉ2
e
‚àí1 r12œÉ2
a
=

œÉ2
a + œÉ2
e
 
1 ‚àír2
12h4
.

1.4 Multivariate Probability Distributions
49
It is seen that even for individuals that are as closely related as full-
sibs (r12 = 1/2), and for heritability values as large as 1/2, knowledge
of y2 produces a reduction in variance of only 1/16, so not much knowledge
about y1 is gained in this situation.
‚ñ†
Example 1.16
Decomposing the joint distribution of additive genetic
values
Consider the following genealogy (also called pedigree in animal breeding):
Individual
Father
Mother
1
‚àí
‚àí
2
‚àí
‚àí
3
1
2
4
1
2
5
1
3
6
4
3
7
5
6
Let ai (i = 1, 2, . . . , 7) be the additive genetic value of individual i. Assume
a model of gene transmission that allows writing the regression of the addi-
tive genetic value of a child (ao) on the additive genetic values of its father
(af) and of its mother (am) as
ao = 1
2af + 1
2am + eo,
where eo, often known as the Mendelian sampling term, is distributed in-
dependently of similar terms in any other ancestors. The joint distribution
of the additive genetic values in the pedigree can be factorized as follows:
p (a1, a2, . . . , a7) = p (a7|a1, a2 . . . , a6) p (a1, a2 . . . , a6)
= p (a7|a5, a6) p (a1, a2 . . . , a6) .
The equality in the second line follows, because under the Mendelian inher-
itance model, given the additive genetic values of its parents, the additive
genetic value a7 is conditionally independent of the additive genetic values
of non-descendants of 7. Similarly p (a1, a2 . . . , a6) can be factorized as
p (a1, a2 . . . , a6) = p (a6|a1, a2, . . . , a5) p (a1, a2, . . . , a5)
= p (a6|a3, a4) p (a1, a2, . . . , a5) .
Continuing in this way with the remaining terms, we obtain Ô¨Ånally
p (a1, a2, . . . , a7) = p (a7|a5, a6) p (a6|a3, a4) p (a5|a1, a3)
p (a4|a1, a2) p (a3|a1, a2) p (a1) p (a2) .
This is an important decomposition that will be encountered again in (16.1)
from Chapter 16 in the context of segregation analysis, where genetic values

50
1. Probability and Random Variables
are modeled as discrete random variables. Incidentally, note that oÔ¨Äspring
are conditionally independent, given their parents, and therefore
p (a5, a6|a1, a3, a4) = p (a5|a1, a3) p (a6|a3, a4) .
However, given a1, a3, a4, and a7 (the child of 5 and 6), then a5 and a6 are
no longer conditionally independent; they are correlated negatively.
‚ñ†
Example 1.17
A multivariate normal sampling model
Imagine there is a data set consisting of two measurements (traits) taken
on each of n subjects. For each trait, a model having the following form is
adopted:
yj = XjŒ≤j + Zjaj + ej,
j = 1, 2,
(1.89)
where Œ≤j and aj are, formally, location vectors containing the eÔ¨Äects of
factors aÔ¨Äecting variation of the responses, and Xj and Zj are known in-
cidence matrices relating these parameters to the data vectors yj, each of
order n. The term ej is a residual representing random variation about
XjŒ≤j+Zjaj. For example, (1.89) could represent a mixed eÔ¨Äects model for
quantitative genetic analysis (Henderson, 1973) in which case Œ≤1 (Œ≤2) and
a1 (a2) would be Ô¨Åxed and random eÔ¨Äects, respectively, on trait 1 (trait 2),
respectively. The records on traits 1 and 2 for individual i can be put in a
two-dimensional vector y‚àó
i = [yi1, yi2]‚Ä≤ , (i = 1, 2, ..., n). These n vectors are
assumed to be conditionally independent, given the location vectors, and
assumed to follow a bivariate normal distribution. The joint density of all
data y‚àó= [y‚àó‚Ä≤
1 , y‚àó‚Ä≤
2 , ..., y‚àó‚Ä≤
n ]‚Ä≤ is then expressible as
p (y‚àó|Œ≤, a, R0)
‚àù|R0|‚àín
2 exp

‚àí1
2
n

i=1
(yi1 ‚àími1, yi2 ‚àími2) R‚àí1
0

yi1 ‚àími1
yi2 ‚àími2

,
(1.90)
where mi1 = x‚Ä≤
i1Œ≤1 + z‚Ä≤
i1a1, mi2 = x‚Ä≤
i2Œ≤2 + z‚Ä≤
i2a2, and x‚Ä≤
i1, x‚Ä≤
i2, z‚Ä≤
i1, z‚Ä≤
i2 are
the ith rows of the incidence matrices X1, X2, Z1, Z2, respectively. Here
the dispersion structure will be taken to be
V ar

yi1|Œ≤1, a1
yi2|Œ≤2, a2

=
 r11
r12
r21
r22

= R0 ‚àÄi,
(1.91)
so R0 = {rij} is the variance‚Äìcovariance matrix between the two traits
measured on the same individual. Let
Se =
Ô£Æ
Ô£ØÔ£ØÔ£∞
n
i=1
(yi1 ‚àími1)2
n
i=1
(yi1 ‚àími1) (yi2 ‚àími2)
n
i=1
(yi1 ‚àími1) (yi2 ‚àími2)
n
i=1
(yi2 ‚àími2)2
Ô£π
Ô£∫Ô£∫Ô£ª(1.92)

1.4 Multivariate Probability Distributions
51
be a matrix of sums of squares and cross-products of residuals. Then
n

i=1
[yi1 ‚àími1, yi2 ‚àími2] R‚àí1
0

yi1 ‚àími1
yi2 ‚àími2

=
tr
# n

i=1
[yi1 ‚àími1, yi2 ‚àími2] R‚àí1
0

yi1 ‚àími1
yi2 ‚àími2
$
=
tr

R‚àí1
0 Se

,
where tr (¬∑) means ‚Äútrace‚Äù (sum of diagonal elements) of a matrix (Searle,
1982). Matrices can be commuted cyclically (preserving comformability)
under the tr operator. With this notation, the joint probability density of
all data is
p (y‚àó|Œ≤, a, R0) = (2œÄ)‚àín
2 |R0|‚àín
2 exp

‚àí1
2 tr

R‚àí1
0 Se

,
(1.93)
which is a useful representation in multivariate analysis (Anderson, 1984).‚ñ†
Example 1.18
Computing conditional multivariate normal distributions
Conditional distributions are important in prediction of genetic values us-
ing mixed eÔ¨Äects models (Henderson, 1963, 1973; Searle et al., 1992). These
distributions also play a key role in a Gibbs sampling-based Bayesian anal-
ysis. The algorithm will be discussed in detail later, and at this point it
suÔ¨Éces to state that its implementation requires constructing all possible
conditional distributions from a joint distribution of interest. The example
here will illustrate how some conditional distributions that arise in connec-
tion with a mixed eÔ¨Äects linear model can be computed under Gaussian
assumptions.
Consider the linear model
y = XŒ≤ + Za + e,
(1.94)
where the random vectors a and e have a joint multivariate normal distri-
bution with null mean vector and covariance matrix
V ar

a
e

=

AœÉ2
a
0
0
IœÉ2
e

.
Above, X and Z are known incidence matrices, A is a known matrix (e.g.,
of additive relationships between individuals), and œÉ2
a and œÉ2
e are variance
components. Then E (y|Œ≤) = XŒ≤ and V ar

y|Œ≤,œÉ2
a, œÉ2
e

= ZAZ‚Ä≤œÉ2
a + IœÉ2
e.
Let
Œ∏‚Ä≤
=

Œ≤‚Ä≤, a‚Ä≤‚Ä≤ ,
W = [X, Z] ,
Œ£
=

0
0
0
A‚àí1k

and C = W‚Ä≤W + Œ£,

52
1. Probability and Random Variables
where k = œÉ2
e/œÉ2
a. The linear set of equations
C 5Œ∏ = W‚Ä≤y = r
has solution 5Œ∏ = C‚àí1r, assuming the inverse of the coeÔ¨Écient matrix C
exists. This system is called Henderson‚Äôs mixed model equations. It will
be shown later on, that in a certain Bayesian setting (Lindley and Smith,
1972; Gianola and Fernando, 1986) the posterior distribution of Œ∏ when œÉ2
a
and œÉ2
e are known, is multivariate normal with parameters
Œ∏|œÉ2
a, œÉ2
e, y ‚àºN

5Œ∏, C‚àí1œÉ2
e

.
(1.95)
Now, partition Œ∏ =

Œ∏‚Ä≤
1, Œ∏‚Ä≤
2
‚Ä≤ arbitrarily, so Œ∏1 can be a scalar or a vector.
What then is the distribution

Œ∏1|Œ∏2, œÉ2
a, œÉ2
e, y

? From multivariate nor-
mal theory, because the posterior distribution

Œ∏|œÉ2
a, œÉ2
e, y

is normal with
parameters (1.95), it follows that the desired conditional posterior distribu-
tion must be normal as well. A useful way of arriving at the parameters of
this conditional distribution is presented here. Given the above partition,
one can write the joint posterior distribution of Œ∏ as

Œ∏1
Œ∏2
"""" œÉ2
a, œÉ2
e, y

‚àºN

5Œ∏1
5Œ∏2

,

C11
C12
C21
C22

œÉ2
e

,
(1.96)
where

C11
C12
C21
C22
‚àí1
=

C11
C12
C21
C22

= C‚àí1.
Now deÔ¨Åne r‚Ä≤ = [r‚Ä≤
1, r‚Ä≤
2], such that the partition is consistent with that of
Œ∏. Using (1.82), the expected value of the distribution

Œ∏1|Œ∏2, œÉ2
a, œÉ2
e, y

is
E

Œ∏1|Œ∏2, œÉ2
a, œÉ2
e, y

= 5Œ∏1 + C12 
C22‚àí1 
Œ∏2 ‚àí5Œ∏2

.
(1.97)
From the mixed model equations, after inverting C, one has
5Œ∏1
=
C11r1 + C12r2,
5Œ∏2
=
C21r1 + C22r2.
Employing these expressions for 5Œ∏1 and 5Œ∏2 in (1.97) above, we get:
E

Œ∏1|Œ∏2, œÉ2
a, œÉ2
e, y

= C11r1 + C12r2 + C12 
C22‚àí1
√ó

Œ∏2 ‚àíC21r1‚àíC22r2

=

C11 ‚àíC12 
C22‚àí1 C21
r1
+C12 
C22‚àí1 Œ∏2
= C‚àí1
11

r1 + C11C12 
C22‚àí1 Œ∏2

= C‚àí1
11 (r1 ‚àíC12Œ∏2) .
(1.98)

1.4 Multivariate Probability Distributions
53
In the above derivation, use is made of the standard matrix algebra result
for partitioned inverses (e.g., Searle, 1982):
C11 ‚àíC12 
C22‚àí1 C21 = C‚àí1
11
and
C11C12 
C22‚àí1 = ‚àíC12.
The variance of the conditional posterior distribution

Œ∏1|Œ∏2, œÉ2
a, œÉ2
e, y

is
V ar

Œ∏1|Œ∏2, œÉ2
a, œÉ2
e, y

=

C11 ‚àíC12 
C22‚àí1 C21
œÉ2
e
=
C‚àí1
11 œÉ2
e.
(1.99)
Therefore, one can write
Œ∏1|Œ∏2, œÉ2
a, œÉ2
e, y ‚àºN

C‚àí1
11 (r1‚àíC12Œ∏2) , C‚àí1
11 œÉ2
e

.
(1.100)
Results (1.98) and (1.99) are useful in the implementation of a Gibbs sam-
pler in a hierarchical or mixed eÔ¨Äects linear model. Even if Œ∏ has a large
number of elements and C is, therefore, a very large matrix (diÔ¨Écult to
invert by brute force methods), the mean and variance of

Œ∏1|Œ∏2, œÉ2
a, œÉ2
e, y

involve the inverse of a matrix having order equal to the number of elements
in Œ∏1. For example, if Œ∏1 is a scalar, only the reciprocal of the appropriate
scalar element is needed.
‚ñ†
1.4.5
Quadratic Forms on Normal Variables:
the Chi-square Distribution
Let the random vector y have the distribution
y ‚àºN (m, V)
and consider the random variable y‚Ä≤Qy. This is called a quadratic form on
vector y; the matrix of constants Q can be taken to be symmetric, without
loss of generality. Then, provided that QV is idempotent one has that
y‚Ä≤Qy ‚àºœá2

rank (Q) , 1
2m‚Ä≤Qm

.
(1.101)
(Matrix A is idempotent if AA = A. If A is idempotent, rank (A) = tr A).
Expression (1.101) means that the quadratic form y‚Ä≤Qy has a noncentral
chi-square distribution with integer degrees of freedom equal to rank(Q) ,
and where 1
2m‚Ä≤Qm is the noncentrality parameter (see Searle, 1971). Other
authors (e.g., Stuart and Ord, 1987) deÔ¨Åne the noncentrality parameter as
m‚Ä≤Qm. If the non-centrality parameter is null, with a suÔ¨Écient condition
for this being m = 0, then y‚Ä≤Qy is said to have a central chi-square distri-
bution.

54
1. Probability and Random Variables
The mean value of the distribution (1.101) is
E (y‚Ä≤Qy) = m‚Ä≤Qm + tr (QV)
(1.102)
and the variance can be shown to be equal to
V ar (y‚Ä≤Qy) = 4m‚Ä≤QVQm + 2tr (QV)2 .
(1.103)
In the special case when m = 0, then E (y‚Ä≤Qy) = tr (QV) and
V ar (y‚Ä≤Qy) = 2 tr (QV)2 = 2 tr (QV) .
Typically,
rank (Q) < rank (V) ,
so
tr (QV) = rank (QV) = rank (Q) ,
this resulting from the idempotency of QV. Then, for m = 0,
E (y‚Ä≤Qy) = rank (Q)
and
V ar (y‚Ä≤Qy) = 2 rank (Q) ,
so the mean and variance of the distribution are given by the number of
degrees of freedom, and by twice the degrees of freedom, respectively.
Example 1.19
Distribution of estimates of the variance of a normal
distribution
In Chapter 3, it will be shown that for the sampling model
y ‚àºN

XŒ≤, IœÉ2
e

,
the maximum likelihood (ML) estimator of the variance œÉ2
e is given by
6
œÉ2e
=

y ‚àíX5Œ≤
‚Ä≤ 
y ‚àíX5Œ≤

n
=
y‚Ä≤Qy
n
,
where 5Œ≤ = (X‚Ä≤X)‚àí1 X‚Ä≤y is the ordinary least-squares estimator of Œ≤,
Q =[I ‚àíX (X‚Ä≤X)‚àí1 X‚Ä≤] is an idempotent n √ó n matrix, and n is the order
of y. Here it is assumed that the matrix X has full column rank equal to
p, the order of Œ≤. Now
6
œÉ2e
=
œÉ2
e
n
y‚Ä≤Qy
œÉ2e
= œÉ2
e
n y‚Ä≤Q

 I
œÉ2e

Qy
=
œÉ2
e
n y‚àó‚Ä≤

 I
œÉ2e

y‚àó.

1.4 Multivariate Probability Distributions
55
Hence, 6
œÉ2e is a multiple of the quadratic form y‚àó‚Ä≤ 
IœÉ‚àí2
e

y‚àó, where y‚àó= Qy.
It will be veriÔ¨Åed that this new quadratic form has a chi-square distribution.
First note that y‚àóis normal by virtue of being a linear combination of y,
with
E (y‚àó) = QE (y) = QXŒ≤ = XŒ≤ ‚àíX (X‚Ä≤X)‚àí1 X‚Ä≤XŒ≤ = 0
and
V ar (y‚àó) = QœÉ2
e.
Further

 I
œÉ2e

V ar (y‚àó) =

 I
œÉ2e

QœÉ2
e = Q
is idempotent. Hence,
y‚àó‚Ä≤

 I
œÉ2e

y‚àó= y‚Ä≤Qy
œÉ2e
has a central chi-square distribution (since E (y‚àó) = 0) with degrees of
freedom equal to
rank

 Q
œÉ2e

=
rank (Q) = tr (Q) = tr

I ‚àíX (X‚Ä≤X)‚àí1 X‚Ä≤
=
n ‚àítr

X (X‚Ä≤X)‚àí1 X‚Ä≤
=

n ‚àítr (X‚Ä≤X)‚àí1 X‚Ä≤X

= n ‚àíp.
It follows that
6
œÉ2e
=
œÉ2
e
n y‚àó‚Ä≤

 I
œÉ2e

y‚àó
=
œÉ2
e
n œá2
n‚àíp,
so the ML estimator of œÉ2
e is distributed as a scaled chi-square random
variable.
‚ñ†
1.4.6
The Wishart and Inverse Wishart Distributions
Multivariate analysis is important in quantitative and evolutionary genet-
ics. For example, in plant and animal breeding, selection for multiple at-
tributes, e.g., yield and quality of wheat or growth rate and feed eÔ¨Éciency
in beef cattle, is the rule rather than the exception. This requires knowl-
edge of genetic variances and covariances between traits. Correlation and
covariance matrices are also of interest in evolution because, for example,
a genetic variance‚Äìcovariance matrix contains information about possible

56
1. Probability and Random Variables
bottlenecks under natural selection (Lynch and Walsh, 1998), or about
the evolutionary trajectory of several traits (RoÔ¨Ä, 1997). The Wishart and
inverse Wishart distributions appear in connection with inference about
covariance matrices involving attributes that follow a multivariate normal
distribution, and play an important role in multivariate analysis (Mardia
et al., 1979; Anderson, 1984). Deriving these distributions is technically
involved, so only a few features are sketched here.
Suppose there is data on p traits or attributes, each expressed as a de-
viation from their respective expectations, on each of n individuals. The
p-dimensional vector of traits is assumed to follow the p-variate Np (0, Œ£)
distribution, where Œ£ is a positive-deÔ¨Ånite variance‚Äìcovariance matrix be-
tween attributes. Let Y(n√óp) be a data matrix containing in row i, say, the
p measurements taken on individual i. Often Y(n√óp) can be assumed to
represent a collection of n independent draws from this normal sampling
model. Form now M = Y‚Ä≤Y, a random p √ó p matrix of sums of squares
and crossproducts. Given the normality assumption, the symmetric matrix
M is said to have a Wishart distribution of order p with scale matrix Œ£
and degrees of freedom parameter equal to n. We write M ‚àºWp (Œ£, n)
to denote this random process. The Wishart distribution is a matrix gen-
eralization of the chi-squared distribution, as we shall see later, and it
involves p (p + 1) /2 random quantities: the p distinct sums of squares and
the p (p ‚àí1) /2 sums of products.
The p.d.f. of the Wishart distribution is
p (M|Œ£, n) =
|M|(n‚àíp‚àí1)/2 exp

‚àí1
2 tr

Œ£‚àí1M

2np/2œÄp(p‚àí1)/4 |Œ£|n/2
p7
i=1
Œì
 1
2 (n + 1 ‚àíi)

‚àù|M|
(n‚àíp‚àí1)/2
exp

‚àí1
2 tr

Œ£‚àí1M

(1.104)
with the sample space being such that |M| > 0. A special situation is when
p = 1, in which case M and Œ£ are scalars; here M = n
i=1 Y 2
i is a sum
of squares, and Œ£ = œÉ2 is the variance of the normal distribution. In this
situation, the Wishart density reduces to the univariate p.d.f.
p

M|œÉ2, n

‚àùM (n‚àí2)/2 exp

‚àíM
2œÉ2

.
(1.105)
This is the kernel of a gamma density with parameters n/2 and (2œÉ2)‚àí1. It
is also the density of the distribution of œÉ2œá2
(n), a scaled chi-square random
variable, with scale parameter œÉ2 and n degrees of freedom. Return to the
general case, and put M = {Mij} and Œ£ = {œÉij}. It can be shown that the
expected (matrix) value of the random matrix M with p.d.f. given in (1.104)
is E (M|Œ£,n) = nŒ£ so, for any element of the matrix E (Mij) = nœÉij.

1.4 Multivariate Probability Distributions
57
The Inverse Wishart Distribution
A related distribution is that of the inverse of a matrix of sums of squares
and products of n randomly drawn vectors from Np (0, Œ£). The distribu-
tion of T = M‚àí1, for |Œ£| > 0 and n ‚â•p, is called the inverse Wishart
distribution; it is symbolized as T ‚àºW ‚àí1
p
(Œ£, n) or as T ‚àºIWp (Œ£, n).
The density of an inverse Wishart matrix is
p (T|Œ£, n)
=
|T|‚àí(n+p+1)/2 exp

‚àí1
2 tr

Œ£‚àí1T‚àí1
2np/2œÄp(p‚àí1)/4 |Œ£|n/2
p7
i=1
Œì
 1
2 (n + 1 ‚àíi)

‚àù
|T|
‚àí(n+p+1)/2
exp

‚àí1
2 tr

Œ£‚àí1T‚àí1
.
(1.106)
The expected value of T is
E (T|Œ£, n) =
Œ£‚àí1
(n ‚àíp ‚àí1),
(1.107)
provided n ‚â•p + 2. An important special case is when T is a scalar (p =
1). For example, in a Bayesian context one may consider using the scalar
version of (1.106) to describe uncertainty about the variance œÉ2. Here,
T = œÉ2, and Œ£‚àí1 = S is now the scale parameter. Then (1.106) reduces to:
p

œÉ2|S, n

‚àù

œÉ2‚àí( n
2 +1) exp

‚àíS
2œÉ2

.
(1.108)
This is the kernel of the density of a scaled inverted chi-square distribution
with parameters S and n (also, of an inverted gamma process with param-
eters n/2 and S/2). An alternative representation is that of, e.g., Gelman
et al. (1995), who put S = nS‚àó. In a sense, S can be interpreted, Bayesianly
speaking, as a prior sum of squares, whereas S‚àówould play the role of a
value of œÉ2 that, a priori, is viewed as very likely (actually, the most likely
one when n is very large). In a Bayesian setting, n can be interpreted as a
‚Äúdegree of belief‚Äù in S‚àó.
More generally, if the random matrix is now T = Œ£, the variance‚Äìcova-
riance matrix of a normal distribution, then (1.106) will describe uncer-
tainty about it. The scale parameter would be interpretable as a matrix of
‚Äúprior sums of squares and products‚Äù. If one has a prior opinion about the
value of an unknown variance‚Äìcovariance matrix Œ£, and this is Œ£‚àó, say, the
value of the scale matrix (which we denote now as S‚àí1 to avoid confusion
with the covariance matrix that one wishes to infer, this being Œ£) can be
assessed from (1.107) as S‚àí1 = (n ‚àíp ‚àí1) Œ£‚àó, given a value of n.
Properties of Wishart and Inverse Wishart Distributions
Some properties of Wishart and inverse Wishart distributions are summa-
rized below. The material is taken from Korsgaard et al. (1999), where

58
1. Probability and Random Variables
the results presented here are given in a more general setting. Let M be
a 2 √ó 2 symmetric, positive-deÔ¨Ånite random matrix having the Wishart
distribution
M|V, n ‚àºW2 (V,n) ,
where V is the scale matrix. Let T = M‚àí1 so that
T|V, n ‚àºIW2 (V, n) .
The symmetric matrices M, V, T have elements
M =

M11
M12
M21
M22

,
V =

V11
V12
V21
V22

,
and
T =

T11
T12
T21
T22

,
respectively. Using properties of partitioned matrices one can write
M11
=

T11 ‚àíT 2
12T ‚àí1
22
‚àí1 ,
M12
=
‚àíT12
4
T11T22 ‚àíT 2
12

,
M22
=

T22 ‚àíT 2
12T ‚àí1
11
‚àí1 .
DeÔ¨Åne
X1
=
M11,
X2
=
M ‚àí1
11 M12,
X3
=
M22 ‚àíM 2
12M ‚àí1
11 .
From these equalities it can be deduced that the following one-to-one rela-
tionships exist between the Xs and the Ts:
T11
=
X‚àí1
1
+ X2
2X‚àí1
3 ,
(1.109)
T12
=
‚àíX2X‚àí1
3 ,
(1.110)
T22
=
X‚àí1
3 .
(1.111)
Then the following properties can be shown to hold:
X1 ‚àºW1 (V11, n) ,
(1.112)
(X2|X1 = x1) ‚àºN

V ‚àí1
11 V12, x‚àí1
1 V22.1

,
(1.113)
X3 ‚àºW1 (V22.1, n ‚àí1) ,
(1.114)
p (x1, x2|x3) = p (x1, x2) ,
(1.115)

1.4 Multivariate Probability Distributions
59
where V22.1 = V22 ‚àíV 2
12V ‚àí1
11 . This means that X1 and X3 have univariate
Wishart (or gamma) distributions with appropriate parameters, that the
conditional distribution [X2|X1 = x1] is normal, and the joint distribution
[X1, X2] is independent of X3. All these distributions are easy to sample
from. Thus, if one wishes to draw a random matrix from an W2 (V, n)
distribution, the sampling procedure consists of:
(a) draw x from the three preceding distributions;
(b) compute T‚àófrom x. This is a realized value from T|V, n ‚àºIW2 (V, n);
(c) Ô¨Ånally, invert T‚àóto obtain a draw from W2 (V, n).
In a joint analysis of Gaussian and discrete data (the latter employing
a probit model) with Bayesian methods, the following problem is often
encountered. Draws of 2 √ó 2 covariance matrices are to be obtained from
a certain posterior distribution, subject to the restriction that one of the
variances is equal to 1 (this is the residual variance in the probit scale).
In Chapter 14 we show how to exploit the properties described above, in
order to perform a Markov chain Monte Carlo (MCMC) based Bayesian
analysis of Gaussian and binary distributed traits.
Simulation of an Inverse Wishart Distribution
An eÔ¨Écient way of simulating a p-dimensional Wishart distribution with
n degrees of freedom and scale matrix S, Wp (S, n), is described by Odell
and Feiveson (1966). The algorithm is as follows:
‚Ä¢ Compute the Cholesky factorization of S = L‚Ä≤L, such that L‚Ä≤ is lower
triangular.
‚Ä¢ Construct a lower triangular matrix
T = {tij} , i = 1, 2, . . . , p, j = 1, 2, . . . , p,
with tii =
8
œá2
n+1‚àíi, tij ‚àºN (0, 1), if i > j, and tij = 0 if i < j.
‚Ä¢ Compute the product L‚Ä≤TT‚Ä≤L. This matrix is distributed as Wp (S, n).
‚Ä¢ The matrix

L‚Ä≤TT‚Ä≤L
‚àí1 is distributed as IWp (S, n).
As discussed later in this book, a Bayesian analysis requires posing a prior
distribution for all the parameters of the model. If a covariance matrix C
of dimension p√óp, say, is assigned a priori the distribution IWp (V, n) , one
way of choosing the scale matrix V is from consideration of the expected
value of C:
E (C|V, n) =
V‚àí1
n ‚àíp ‚àí1.
Then set V‚àí1 = (n ‚àíp ‚àí1) ,E (C|V, n), where ,E (C|V, n) is some ‚Äúrea-
sonable‚Äù value chosen on the basis of prior information.

60
1. Probability and Random Variables
1.4.7
The Multivariate-t Distribution
Density of the Distribution
Suppose a random vector has the conditional multivariate normal distribu-
tion with p.d.f.
y|¬µ, Œ£, w ‚àºN

y|¬µ, Œ£
w

,
(1.116)
where w, in turn, is a scalar random variable following a
Ga
ŒΩ
2, ŒΩ
2

process; here ŒΩ > 0 is a parameter. The density of the joint distribution of
y and w, using (1.40) and (1.77), is then
p (y, w|¬µ, Œ£, ŒΩ) = p (y|¬µ, Œ£, w) p (w|ŒΩ)
=
""""2œÄ

Œ£
w
""""
‚àí1
2
exp

‚àí1
2 (y ‚àí¬µ)‚Ä≤

Œ£
w
‚àí1
(y ‚àí¬µ)

√ó (ŒΩ/2)
ŒΩ
2
Œì (ŒΩ/2)w
ŒΩ
2 ‚àí1 exp

‚àíŒΩw
2

.
(1.117)
The marginal density of y is obtained by integrating the joint density above
with respect to w, yielding
p (y|¬µ, Œ£, ŒΩ) = |2œÄŒ£|‚àí1
2 (ŒΩ/2)
ŒΩ
2
Œì (ŒΩ/2)
√ó
‚àû

0
w
n+ŒΩ
2
‚àí1 exp

‚àíw(y ‚àí¬µ)‚Ä≤ Œ£‚àí1 (y ‚àí¬µ) + ŒΩ
2

dw.
(1.118)
Reference to (1.40) indicates that the integrand is the kernel of the density
Ga

w|n + ŒΩ
2
, (y ‚àí¬µ)‚Ä≤ Œ£‚àí1 (y ‚àí¬µ) + ŒΩ
2

.
Hence, the integral in (1.118) is equal to the reciprocal of the integration
constant of the corresponding distribution, that is
‚àû

0
w
n+ŒΩ
2
‚àí1 exp

‚àíw(y ‚àí¬µ)‚Ä≤ Œ£‚àí1 (y ‚àí¬µ) + ŒΩ
2

dw
=
Œì
 n+ŒΩ
2


(y‚àí¬µ)‚Ä≤Œ£‚àí1(y‚àí¬µ)+ŒΩ
2
 n+ŒΩ
2 .
(1.119)

1.4 Multivariate Probability Distributions
61
Employing (1.119) in (1.118), and rearranging
p (y|¬µ, Œ£, ŒΩ) = (ŒΩ)
ŒΩ
2 Œì
 n+ŒΩ
2

Œì
 ŒΩ
2

|œÄŒ£|
1
2

(y ‚àí¬µ)‚Ä≤ Œ£‚àí1 (y ‚àí¬µ) + ŒΩ
‚àín+ŒΩ
2
=
Œì
 n+ŒΩ
2

Œì
 ŒΩ
2

|ŒΩœÄŒ£|
1
2

1 + (y ‚àí¬µ)‚Ä≤ Œ£‚àí1 (y ‚àí¬µ)
ŒΩ
‚àín+ŒΩ
2
.
(1.120)
This is the density of an n-dimensional multivariate-t distribution with
mean vector ¬µ, scale matrix Œ£, and degrees of freedom parameter ŒΩ. Note
that ŒΩ can take any value in the positive part of the real line and does not
need to be an integer. When n = 1, a univariate-t distribution results, and
the density has already been given in (1.51).
It is interesting to observe that the t distribution results by averaging
an inÔ¨Ånite number of normal processes with a randomly varying covari-
ance matrix Œ£w‚àí1 over a gamma (or inverse gamma, or scaled inverse
chi-square) distribution. For this reason, it is often stated in the litera-
ture that the t distribution is a mixture of an inÔ¨Ånite number of Gaussian
processes (Gelman et al., 1995).
The mean vector and covariance matrix of the multivariate-t distribution
can be deduced from (1.116) using iterated expectations (see below), as this
leads to
E (y|¬µ, Œ£, ŒΩ) = Ew (y|¬µ, Œ£, w) = Ew (¬µ) = ¬µ
and to
V ar (y|¬µ, Œ£, ŒΩ) = Ew [V ar (y|¬µ, Œ£, w)] + V arw [E (y|¬µ, Œ£, w)]
= Ew [V ar (y|¬µ, Œ£, w)] = Ew

Œ£
w

= Œ£Ew

 1
w

.
It is shown in Chapter 2 that the average value of the reciprocal of a gamma
random variable is given by
Ew

 1
w

=
a
b ‚àí1 =
ŒΩ
2
ŒΩ
2 ‚àí1 =
ŒΩ
ŒΩ ‚àí2
in this case. Thus
V ar (y|¬µ, Œ£, ŒΩ) =
ŒΩ
ŒΩ ‚àí2Œ£.
Marginal and Conditional Distributions
A similar development can be adopted to show that all marginal and con-
ditional distributions deriving from a multivariate-t distribution are uni-
variate or multivariate-t as well. This is so since for any arbitrary partition
of y in (1.116), say,

y1
y2
"""" ¬µ, Œ£, w ‚àºN


¬µ1
¬µ2

,
 Œ£11
Œ£12
Œ£21
Œ£22
 1
w

,

62
1. Probability and Random Variables
all marginal and conditional distributions are normal, with appropriate
mean vector and covariance matrix. Integration over the Ga (ŒΩ/2, ŒΩ/2)
distribution leads to the desired result directly. For example, the condi-
tional distribution of y1 given y2 is an n1-dimensional (the order of y1)
multivariate-t with mean vector
E (y1|y2, ¬µ, Œ£, ŒΩ) = ¬µ1 + Œ£12 (Œ£22)‚àí1 (y2 ‚àí¬µ2) ,
covariance matrix
V ar (y1|y2, ¬µ, Œ£, ŒΩ) =
ŒΩ
ŒΩ ‚àí2

Œ£11 ‚àíŒ£12 (Œ£22)‚àí1 Œ£21

,
and degrees of freedom ŒΩ. A similar reasoning leads to the result that any
linear combination of a vector that has a multivariate-t distribution must
be multivariate- (or univariate-) t distributed also.
1.5
Distributions with Constrained Sample Space
In genetics, situations where the sampling space of random variables or
where the space of unknowns is restricted, are not uncommon. For example,
a Poisson sampling model might be sensible for describing litter size in pigs.
However, litters of size 0 may not be reported in practice! In this situation,
one may consider adopting a Poisson distribution with probabilities ‚Äúnor-
malized‚Äù such that the event x = 0 is not observable. Likewise, many ‚Äúselec-
tion‚Äù models have been proposed in the context of the multivariate normal
distribution and these often involve a restriction of the sampling space of
the variables entering into the problem. Perhaps the best known one is selec-
tion by truncation (Pearson, 1903; Aitken, 1934). Henderson et al. (1959),
Curnow (1961), Thompson (1973), Henderson (1975), Thompson (1976),
Robertson (1977), Bulmer (1980), and Gianola et al. (1989), among others,
have discussed the eÔ¨Äects of selection of multivariate normal variates in
a genetic context, from the point of view of either parameter estimation
(Ô¨Åxed eÔ¨Äects; variance components) or of prediction of breeding values.
Let X be a discrete random variable with p.m.f. p (x) and let a and b be
constants lying within the support of the domain of p. Then, the doubly
truncated p.m.f. of X, given that a < X ‚â§b, is
Pr (X = x|a < X ‚â§b) = Pr (X = x, a < X ‚â§b)
Pr (a < X ‚â§b)
=
Pr (X = x)
Pr (a < X ‚â§b),
for a < X ‚â§b.
Therefore,
Pr (X = x|a < X ‚â§b) =
#
0,
if x ‚â§a or x > b,
p(x)
F (b)‚àíF (a),
if a < x ‚â§b,

1.5 Distributions with Constrained Sample Space
63
where F(¬∑) is the distribution function. In particular, given a truncation
point t, the p.m.f. of X, given that X > t (i.e., truncated below), is
Pr (X = x|X > t) =
#
0,
if x ‚â§t,
p(x)
1‚àíF (t),
if x > t.
(1.121)
Example 1.20
Sibship size
As an example of a discrete truncated distribution, let s denote the number
of children in a nuclear family. Assuming that a male or a female birth are
equally likely (i.e., Œ∏ = 1/2), the probability that there will be exactly x
girls in a family of size s, assuming a binomial sampling model, is
Pr (X = x|Œ∏, s) =

s
x
 
1
2
x 
1
2
s‚àíx
=

s
x
 
1
2
s
,
x = 0, 1, . . . , s.
Suppose that there is interest in the distribution of the number of girls
in nuclear families of size s that have at least one girl. The corresponding
probability, using (1.121), is
Pr (x|X > 0, Œ∏, s) =
Pr (X = x|Œ∏, s)
1 ‚àíPr (X = 0|Œ∏, s)
=

s
x
  1
2
s
1 ‚àí
 1
2
s
,
x = 1, 2, . . . , s.
For example, for s = 4, the unconstrained and truncated probability dis-
tributions are
x
0
1
2
3
4
p (x|Œ∏, s)
0.0625
0.2500
0.3750
0.2500
0.0625
p (x|X > 0, Œ∏, s)
0
0.2667
0.4000
0.2667
0.0667
The two distributions do not diÔ¨Äer by much because Pr(X = 0|Œ∏, s) takes
a low value in the untruncated binomial distribution.
‚ñ†
Let a random vector z have a multivariate distribution with density
p (z|Œ∏), where Œ∏ is the parameter vector. If the original sampling space of
z, Rz is constrained such that this vector is observed only in a more limited
space, Rc
z, and this happens with probability

p (z|Œ∏) I (z ‚ààRc
z) dz = P
where I (¬∑) is an indicator function, then the density of the constrained
distribution of z is p (z|Œ∏) /P, which can be seen to integrate to 1 over

64
1. Probability and Random Variables
the space Rc
z. This setting has been used extensively by Pearson (1903)
and Henderson (1975), among others, in genetic applications. A special
case is the truncation selection model of quantitative genetics, which is
described now for a bivariate situation. Let two random variables X and Y
have a bivariate distribution with joint p.d.f. p(x, y). Assume that selection
operates on X such that this random variable is observed only if X ‚â•t,
where t is a known truncation point, or threshold of selection, whereas
the sampling space of Y remains unrestricted. The density of the joint
distribution of X and Y (after selection), using the result given above, is
p (x, y|X ‚â•t) =
p (x, y)
‚àû

‚àí‚àû
‚àû

t
p (x, y) dx dy
=
p (x, y)
‚àû

t
p (x) dx
= p (x, y)
P
,
(1.122)
where P is now the proportion selected. The conditional density of Y given
X (after selection) is, by deÔ¨Ånition
p (y|x, X ‚â•t) = p (y, x|X ‚â•t)
p (x|X ‚â•t)
= p (y, x) /P
p (x) /P
= p (y|x) .
(1.123)
It follows that this conditional distribution is the same as in the absence of
selection. This result is not unexpected because, intuitively, the conditional
distribution [Y |X] is deÔ¨Åned for any X, so whatever happens with the
sample space of X is irrelevant. An important corollary is that the form of
E(Y |X), the regression function, is unaÔ¨Äected by selection operating on X,
this being true for any distribution. However, the marginal distribution of Y
is altered by selection, unless X and Y are independent. This is illustrated
below.
Example 1.21
Parental selection
Suppose there are two unrelated parents and one oÔ¨Äspring; let their additive
genetic values be a1, a2 and a3 respectively. Under multivariate normality
the conditional distribution of the additive genetic value of a3 given a1 and
a2 is normal with mean E (a3|a1, a2) = (a1 + a2) /2 and variance œÉ2
a/2;
this variance can be found using formula (1.83), taking into account that
the covariance between the additive genetic values of a parent and of an
oÔ¨Äspring is œÉ2
a/2. This conditional distribution holds true for any pair of
parents, selected or unselected, so the expected value of the additive genetic
value of an oÔ¨Äspring, conditionally on parental breeding values, is always
equal to the average of the additive genetic values of parents. However, the
unconditional mean and variance of a3 are aÔ¨Äected by the selection process.
Letting Es denote expected value under selection, it follows, by virtue of
the theorem of double expectation (to be discussed below), that
Es (a3) = Es [E (a3|a1, a2)] = Es
a1 + a2
2

Ã∏= E
a1 + a2
2

,

1.5 Distributions with Constrained Sample Space
65
so the mean of the additive genetic values in the oÔ¨Äspring, following selec-
tion, in general, will not be equal to the mean breeding value of parents
selected at random, unless selection is of a stabilizing form (Bulmer, 1980).
Likewise, the variance of breeding values in progeny of selected parents,
V ars, is
V ars (a3)
=
Es [V ars (a3|a1, a2)] + V ars [Es (a3|a1, a2)]
=
œÉ2
a
2 + V ars
a1 + a2
2

=
1
2

œÉ2
a + œÉ2
as

,
where œÉ2
as is the variance of the breeding values within selected parents.
Most often, the latter will not be equal to the variance in the absence of
selection. It can be lower or larger depending on the form of selection.
‚ñ†
In a Bayesian analysis, inferences about a parameter vector Œ∏ are made
from the conditional distribution [Œ∏|y], called posterior distribution in this
speciÔ¨Åc context. Parameters are treated in the Bayesian approach as ran-
dom variables (the randomness arises from the state of uncertainty about
their values) and the analysis is made conditionally on the observed data,
y. Suppose y0 and y1 are vectors of data of a selection experiment col-
lected at generations 0 and 1, respectively. Assume that individuals with
data y1 are the oÔ¨Äspring of selected parents, and that selection was based
on the available phenotypic records, y0. Further, suppose that selection is
by truncation, as before, and that the truncation point t is known. Had
there been no selection, the density of the posterior distribution (using all
data) is
p (Œ∏|y0, y1) = p (Œ∏, y0, y1)
p (y0, y1) .
(1.124)
However, if the joint distribution [y0, y1] is modiÔ¨Åed by selection such that
only individuals whose phenotypic records exceed t (this is informally de-
noted as y0 > t) are used as parents, the posterior distribution must be
written as
p (Œ∏|y0, y1, y0 > t) = p (Œ∏, y0, y1|y0 > t)
p (y0, y1|y0 > t)
= p (Œ∏, y0, y1)
P
P
p (y0, y1)
= p (Œ∏|y0, y1) ,
(1.125)
where
P =
 ‚àû
t
 ‚àû
‚àí‚àû
p (y0, y1) dy1 dy0
is the probability of selection. The important point is that when all data
on which selection operates is included in the analysis, inferences about

66
1. Probability and Random Variables
Œ∏ can be made from the posterior density p (Œ∏|y0, y1), as if selection had
not taken place. Including all data (records from parents, nonparents and
oÔ¨Äspring) in the analysis is necessary, to justify using the sampling model
[y0, y1|Œ∏] as if selection had not occurred. This sampling model is needed for
constructing the posterior density (1.125). Gianola and Fernando (1986),
and more recently Sorensen et al. (2001), discuss this result in a more
general setting.
Simulation of Univariate Truncated Distributions
An eÔ¨Écient algorithm for sampling from truncated distributions can be
found in Devroye (1986) and is as follows. Let Y be a random variable from
a normal distribution, truncated between a (lower bound) and b (upper
bound). To sample from the truncated normal TN(a,b)

¬µ, œÉ2
, where ¬µ
and œÉ2 are the mean and variance before truncation:
‚Ä¢ Simulate U from a uniform distribution Un (0, 1).
‚Ä¢ The truncated normal is
Y = ¬µ + œÉŒ¶‚àí1 [p1 + U (p2 ‚àíp1)] ,
where Œ¶‚àí1 (¬∑) is the inverse c.d.f. of the normal distribution,
p1 = Œ¶ [(a ‚àí¬µ) /œÉ]
and
p2 = Œ¶ [(b ‚àí¬µ) /œÉ] .
The method can be generalized to any univariate distribution truncated
in the interval [a, b]. If the c.d.f. of the untruncated variate is F, then a
draw from the truncated distribution is
y = F ‚àí1 {F (a) + U [F (b) ‚àíF (a)]} .
(1.126)
The proof that (1.126) is a value from the desired truncated distribution
in the interval [a, b] is as follows
Pr (Y ‚â§y) = Pr

F ‚àí1 {F (a) + U [F (b) ‚àíF (a)]} ‚â§y

= Pr [F (a) + U [F (b) ‚àíF (a)] ‚â§F (y)]
= Pr

U ‚â§F (y) ‚àíF (a)
F (b) ‚àíF (a)

=

F (y)‚àíF (a)
F (b)‚àíF (a)
0
du
= F (y) ‚àíF (a)
F (b) ‚àíF (a) .

1.6 Iterated Expectations
67
1.6
Iterated Expectations
Let x and y be two random vectors with joint p.d.f. p(x, y) and let p(x|y)
denote the conditional p.d.f. of x given y (the distinction between a random
variable and its realized value is dropped in this section). Then
E (x) = Ey [E (x|y)] .
(1.127)
This also holds for an arbitrary function of x, f (x), in which case x is
replaced above by f(x). The proof of (1.127) is as follows:
E (x)
=

xp (x) dx =
 
xp (x, y) dx dy
=
 
xp (x|y) dx

p (y) dy
=

[E (x|y)] p (y) dy = Ey [E (x|y)] ,
where p(y) is the marginal density of y. Thus the mean of x can be obtained
by averaging conditional (given y) means over the marginal distribution of
y. This result is at the root of Monte Carlo methods that use conditional
distributions. For example, one can estimate E(x) by either direct drawings
from the distribution of x or, alternatively, by drawing samples from the
distribution of y and computing E(x|y) for each sample. Then weighted
averages of these conditional expectations are calculated, where the weights
are assigned according to the density that the values of y take in their dis-
tribution. As shown below, since V ar (x) ‚â•V ary [E (x|y)], inferences using
the conditional mean rather than direct drawings from the distribution of
x are usually more precise.
A similar result holds for the covariance
Cov (x, y‚Ä≤) = Ez [Cov (x, y‚Ä≤|z)] + Covz [E (x|z) , E (y‚Ä≤|z)] ,
(1.128)
where Cov(¬∑) now indicates a covariance matrix. Using the above result,
observe that by deÔ¨Ånition of a covariance matrix (Searle, 1971),
Ez [Cov (x, y‚Ä≤|z)] = Ez {E (xy‚Ä≤|z) ‚àí[E (x|z)] [E (y‚Ä≤|z)]}
= Ez [E (xy‚Ä≤|z)] ‚àíEz {[E (x|z)] [E (y‚Ä≤|z)]}
= E (xy‚Ä≤) ‚àí[E (x) E (y‚Ä≤)] ‚àíEz {[E (x|z)] [E (y‚Ä≤|z)]}
+ Ez [E (x|z)] Ez [E (y‚Ä≤|z)]
= Cov (x, y‚Ä≤) ‚àíCovz [E (x|y) , E (y‚Ä≤|z)]
and this leads to the desired result directly. The expression for the variance
is obtained immediately, setting x = y,
V ar (x) =Ey [V ar (x|y)] + V ary [E (x|y)] .
(1.129)

68
1. Probability and Random Variables
Example 1.22
Predicting a random variable using the conditional mean
Let Y be a random variable to be predicted using some function u (X) in-
volving the known random variable X = x. For any function u (X), consider
E[(Y ‚àíu (X))2] equal to
E{[Y ‚àíu (X)]2} =
 
[y ‚àíu (x)]2 p (x, y) dxdy.
(1.130)
Expression (1.130) is minimized when u (x) = E (Y |X = x). The proof is
as follows. Let E (Y |X = x) = w (x) and write (1.130) as
E{[Y ‚àíu (X)]2} = E
9
[(Y ‚àíw (X)) + (w (X) ‚àíu (X))]2:
= E
9
[Y ‚àíw (X)]2:
+ E
9
[w (X) ‚àíu (X)]2:
+2E {[Y ‚àíw (X)] [w (X) ‚àíu (X)]} .
The expectation involving the cross-product term can be written as
E {[Y ‚àíw (X)] [w (X) ‚àíu (X)]}
= EX {EY [(Y ‚àíw (X)) (w (X) ‚àíu (X)) |X]}
= EX {(w (X) ‚àíu (X)) EY [(Y ‚àíw (X)) |X]} .
However
EY [(Y ‚àíw (X)) |X] = E (Y |X) ‚àíw (X) = 0.
Then it follows that
E{[Y ‚àíu (X)]2} = E
9
[Y ‚àíw (X)]2:
+ E
9
[w (X) ‚àíu (X)]2:
.
The Ô¨Årst term in the right-hand side does not involve u (X), and
E
9
[w (X) ‚àíu (X)]2:
‚â•0
with equality if u (X) ‚â°w (X) = E (Y |x). DeÔ¨Åning the ‚Äúbest predictor‚Äù
as that for which E[(Y ‚àíu (X))2] is a minimum, then the best choice for
u (x) is
u (x) = E (Y |X = x) .
‚ñ†
Example 1.23
Variation in gene frequencies: the beta‚Äìbinomial distri-
bution
Return to Example 1.8, where clusters of n alleles are drawn at random,
and where Œ∏ = Pr (allele A) . Suppose now that Œ∏ varies between clusters
according to a beta distribution, with density
p

Œ∏|Œ∏, c

=
Œì (c)
Œì

cŒ∏

Œì

c

1 ‚àíŒ∏
Œ∏cŒ∏‚àí1 (1 ‚àíŒ∏)c(1‚àíŒ∏)‚àí1 .

1.6 Iterated Expectations
69
Recall that, in the parameterization of Wright (1968), c = a + b and
Œ∏ =
a
a + b.
The two alternative parameterizations will be used interchangeably here.
Let X be the number of A alleles in a cluster of size n and suppose that,
given Œ∏, its distribution is binomial. Then, X = n
i=1 Xi, where, given Œ∏,
Xi is a Bernoulli variable with success probability Œ∏. Hence
E (X|Œ∏) =
n

i=1
E (Xi) = nŒ∏
and
V ar (X|Œ∏) =
n

i=1
V ar (Xi) = nŒ∏ (1 ‚àíŒ∏) ,
since the Bernoulli variables are assumed to be conditionally independent.
Now, using (1.127), the mean value of X over clusters is
E (X) = E [E (X|Œ∏)] = E (nŒ∏) = nŒ∏ =
na
a + b.
(1.131)
Employing (1.129),
V ar (X)
=
E [V ar (X|Œ∏)] + V ar [E (X|Œ∏)]
=
E [nŒ∏ (1 ‚àíŒ∏)] + V ar (nŒ∏)
=
n

E (Œ∏) ‚àíE2 (Œ∏) + n V ar (Œ∏)

.
Using the mean and variance of the beta distribution, given in (1.37) and
(1.38), respectively, one obtains after algebra,
V ar (X)
=
nab (a + b + n)
(a + b)2 (a + b + 1)
=
nŒ∏

1 ‚àíŒ∏
 
c + n
c + 1

.
(1.132)
If the cluster has size n = 1, the mean of the marginal distribution of X is
Œ∏, and the variance is Œ∏

1 ‚àíŒ∏

.
Next, we derive the correlation between alleles within a cluster. Note that
for a cluster of size n
V ar (X)
=
V ar
 n

i=1
Xi

= n V ar (Xi) + n (n ‚àí1) Cov (Xi, Xj)
=
n V ar (Xi) [1 + (n ‚àí1) œÅ]
=
nŒ∏

1 ‚àíŒ∏

[1 + (n ‚àí1) œÅ] ,

70
1. Probability and Random Variables
where œÅ is the correlation between alleles; if œÅ = 0, there is no within-cluster
aggregation and sampling is purely binomial. The correlation between al-
leles is
œÅ =
1
n ‚àí1

V ar (X)
nŒ∏

1 ‚àíŒ∏
 ‚àí1

.
The marginal distribution of X is called beta‚Äìbinomial, which is discrete.
Its form can be derived by noting that if X|Œ∏ is binomial, and Œ∏ follows a
beta process, then the marginal distribution of X is given by
Pr

X = x|n, c, Œ∏

=
1

0

n
x

Œ∏x (1 ‚àíŒ∏)n‚àíx
√ó
Œì (c)
Œì

cŒ∏

Œì

c

1 ‚àíŒ∏
Œ∏cŒ∏‚àí1 (1 ‚àíŒ∏)c(1‚àíŒ∏)‚àí1 dŒ∏
=

n
x

Œì (c)
Œì

cŒ∏

Œì

c

1 ‚àíŒ∏

√ó
1

0
Œ∏x+cŒ∏‚àí1 (1 ‚àíŒ∏)n‚àíx+c(1‚àíŒ∏)‚àí1 dŒ∏
=

n
x

Œì (c)
Œì

cŒ∏

Œì

c

1 ‚àíŒ∏

√óŒì

x + cŒ∏

Œì

n ‚àíx + c

1 ‚àíŒ∏

Œì (n + c)
.
(1.133)
The last expression results from use of the integral in (1.36). Below is
another example of the beta‚Äìbinomial process.
‚ñ†
Example 1.24
Deconditioning a binomial distribution having a random
parameter
As discussed before, the sum of n independent random variables Xi, each
following a Bernoulli probability distribution Br (Œ∏), is a random variable
that has a binomial distribution. Assume now that the probability of suc-
cess, Œ∏, is unknown, and that this random variable is assigned a beta dis-
tribution Be (a, b). Thus, conditionally on Œ∏
Pr (Xi = xi|Œ∏) =

Br (xi|Œ∏) = Œ∏xi (1 ‚àíŒ∏)1‚àíxi ,
for xi = 0, 1,
0,
for other values of xi.
(1.134)
The density of the beta distribution for Œ∏ is
p (Œ∏|a, b) = Be (Œ∏|a, b) =

CŒ∏a‚àí1 (1 ‚àíŒ∏)b‚àí1 ,
for 0 ‚â§Œ∏i ‚â§1,
0,
for other values of Œ∏,
(1.135)

1.6 Iterated Expectations
71
where a and b are the parameters of the beta distribution. Recall that the
mean and variance of the beta distribution are given, respectively, by
E (Œ∏|a, b)
=
a
a + b,
V ar (Œ∏|a, b)
=
ab

(a + b)2 (a + b + 1)
.
Consider the random variable Y = n
i=1 Xi, the total number of ‚Äúsuc-
cesses‚Äù in n trials. Given Œ∏, the random variable Y is Bi (Œ∏, n), and its
marginal distribution is in the form of a beta‚Äìbinomial, which is generated
by the mixture
Bb (y|a, b, n) =
 1
0
Bi (y|n, Œ∏) Be (Œ∏|a, b) dŒ∏,
(1.136)
as in the previous example. Any of the moments of [Y |a, b, n] can be ob-
tained from (1.136). In animal breeding, the beta‚Äìbinomial model could
arise in the following context. Suppose that in a given cluster (a cow in
a given herd, say), a cow is artiÔ¨Åcially inseminated and pregnancy is reg-
istered. Pregnancy can be modeled as a Bernoulli random variable, with
unknown probability equal to Œ∏ associated with the particular cluster. Here
we obtain the mean and variance of Y , the rate of calving of the cow in n
trials, unconditionally on Œ∏, using iterated expectations (in a more realis-
tic set up, there would be several clusters, each with its own probability of
pregnancy). The mean of the marginal distribution of Y is obtained directly
as follows:
E (Y )
=
E
 n

i=1
Xi

=

i
EŒ∏ [E (Xi|Œ∏)]
=

i
EŒ∏ (Œ∏)
=
na
a + b.
Because, given Œ∏, the Xi are independent, the conditional variance is
V ar (Y |Œ∏) = V ar
 n

i=1
(Xi|Œ∏)

=
n

i=1
V ar (Xi|Œ∏) = nŒ∏ (1 ‚àíŒ∏) .

72
1. Probability and Random Variables
Recalling (1.129), the marginal variance of Y is
V ar (Y ) = EŒ∏ [V ar (Y |Œ∏)] + V arŒ∏ [E (Y |Œ∏)] .
Since E (Y |Œ∏) = nŒ∏, and Œ∏ has a beta distribution,
V arŒ∏ [E (Y |Œ∏)] = V ar (nŒ∏) =
n2ab
(a + b)2 (a + b + 1)
.
Also, V ar (Y |Œ∏) = nŒ∏ (1 ‚àíŒ∏). Then
EŒ∏ [V ar (Y |Œ∏)] = EŒ∏ [nŒ∏ (1 ‚àíŒ∏)]
= n
1

0
Œ∏ (1 ‚àíŒ∏) p (Œ∏|a, b) dŒ∏
= Cn
1

0
Œ∏ (1 ‚àíŒ∏) Œ∏a‚àí1 (1 ‚àíŒ∏)b‚àí1 dŒ∏
=
nab
(a + b) (a + b + 1).
Thus
V ar (Y ) =
nab
(a + b)2
a + b + n
a + b + 1
= nE (Œ∏|a, b) [1 ‚àíE (Œ∏|a, b)] a + b + n
a + b + 1 .
The variance of the beta‚Äìbinomial with mean probability a/ (a + b) is
greater by a factor (a + b + n) / (a + b + 1) than the binomial with the
same probability. When n = 1, there is no information available to dis-
tinguish between the beta and binomial variation, and both models have
equal variances.
‚ñ†
Example 1.25
Genetic markers and the covariance between half-sibs
Suppose a male is drawn at random from a population in equilibrium (i.e.,
where mating is at random, so gene and genotypic frequencies are constant
from generation to generation, and there is no inbreeding or assortative
mating). Let the additive genetic variance of a trait of interest at an auto-
somal additive locus be Vg. Since allelic eÔ¨Äects would be independently and
identically distributed in this population, Vg is equal to twice the variance
between either paternal or maternal allelic eÔ¨Äects. Assume that this male
is randomly mated to an unknown female, also sampled at random, and
that two half-sibs are born from this mating. Let x and y designate the
values of the alleles (haplotypes) that the two half-sibs received from their

1.6 Iterated Expectations
73
father and assume that E (x) = E (y) = 0. The allelic variance is
V ar (x) = V ar (y) = 1
2Vg.
The additive genetic covariance between half-sibs, Cov (x, y), can be de-
rived as follows. Let z be a Bernoulli random variable taking the value 1 if
the paternally derived alleles from both individuals are identical by descent
(IBD), and 0 otherwise. Using (1.128),
Cov (x, y)
=
Ez [Cov (x, y|z)] + Covz [E (x|z) , E (y|z)]
=
Ez [Cov (x, y|z)] .
This is so, because E (x|z) = E (x) = 0, a constant, with the same holding
for y. In other words, the mean value of an allelic eÔ¨Äect is not altered by
knowledge of identity by descent. Taking expectations with respect to the
distribution of z yields
Cov (x, y) = Ez [Cov (x, y|z)]
= Cov (x, y|z = 0) Pr (z = 0) + Cov (x, y|z = 1) Pr (z = 1) .
(1.137)
The term Cov (x, y|z = 0) is null. This is because, if the two alleles are not
IBD, the allelic eÔ¨Äects are independently distributed; thus
Cov (x, y|z = 0) = E (xy|z = 0) = E (x) E (y) = 0.
Further
Cov (x, y|z = 1) = V ar (x) = 1
2Vg,
(1.138)
because if the two alleles are identical by descent, x ‚â°y. In order to
calculate the probability of IBD, suppose the father has genotype A1A2,
where subscripts 1 and 2 are labels for chromosomes 1 and 2, respectively.
Thus, in principle, alleles at position A1A2 can be identical in state. We
refer to the event ‚Äúdrawing allele Ai (i = 1, 2) from the Ô¨Årst half-sib with
value x‚Äù as ‚ÄúAi in x‚Äù. Similarly, the event, ‚Äúdrawing allele Ai from the other
half-sib with value y‚Äù is written as ‚ÄúAi in y‚Äù. The required probability is
Pr (z = 1) = œâ11 Pr (A1 in x ‚à©A1 in y)
+ œâ22 Pr (A2 in x ‚à©A2 in y)
+ œâ12 [Pr (A1 in x ‚à©A2 in y) + Pr (A2 in x ‚à©A1 in y)]
= œâ11
1
2
1
2 + œâ22
1
2
1
2 + 2œâ12
1
2
1
2,
(1.139)
where œâij = Pr (Ai ‚â°Aj|Ai in x ‚à©Aj in y), i, j = 1, 2, which we write
Pr (Ai ‚â°Aj) for short. Above, the four terms of the form
Pr (Ai in x ‚à©Aj in y) ,
i = 1, 2, j = 1, 2,

74
1. Probability and Random Variables
are equal to
Pr (Ai in x ‚à©Aj in y)
=
Pr (Ai in x) Pr (Aj in y)
=
1
2
1
2 = 1
4,
since there is a probability of 1/2 of drawing one of the two alleles from
each individual and the two drawings are independent. Expression (1.139)
results from the fact that the probability involves four mutually exclusive
and exhaustive events. Two such events pertain to the situation where the
same allele is picked on each of the individuals, and here
Pr (A1 ‚â°A1) = Pr (A2 ‚â°A2) = 1.
The other two events involve diÔ¨Äerent alleles, but it must be noted that
Pr (A1 ‚â°A2) may not always be zero, as these alleles might be copies of
the same allele from a common ancestor. Since we assume there is no in-
breeding, Pr (A1 ‚â°A2) = 0, so
Pr (z = 1) = 1
2
1
2 + 1
2
1
2 = 1
2.
(1.140)
Using (1.138) and (1.140) in (1.137), the half-sib covariance is
Cov (x, y) = 1
4Vg.
(1.141)
Imagine now that the sire is known to be heterozygote for a genetic marker
linked to the locus aÔ¨Äecting the trait in question. Let the recombination
fraction between the marker and the locus be r, and deÔ¨Åne a new random
variable M, taking the value 1 if the marker alleles are the same in both
half-sibs, and 0 otherwise. The additive genetic covariance between the two
half-sibs, conditionally on their marker information, is expressible as
Cov (x, y|M) = Ez [Cov (x, y|z, M)]
+ Covz [E (x|z, M) , E (y|z, M)]
= Ez [Cov (x, y|z, M)] ,
since the covariance between the conditional means is zero, following the
same argument as before. Now take expectations with respect to the dis-
tribution of z. When the half-sibs receive the same marker allele from the
sire (M = 1), one has
Ez [Cov (x, y|z, M = 1)]
= Cov (x, y|z = 0, M = 1) Pr (z = 0|M = 1)
+ Cov (x, y|z = 1, M = 1) Pr (z = 1|M = 1)
= Cov (x, y|z = 1, M = 1) Pr (z = 1|M = 1) .
(1.142)

1.6 Iterated Expectations
75
The conditional probability in the bottom line of (1.142) is
Pr (z = 1|M = 1) = Pr (z = 1, M = 1)
Pr (M = 1)
.
(1.143)
The numerator in (1.143) is the probability of drawing independently two
gametes from the sire that have the same marker allele and the same allele
at the locus in question. These gametes are either nonrecombinant, with
probability 1
2 (1 ‚àír)2, or recombinant, with the corresponding probability
being 1
2r2. Therefore the numerator of (1.143) is equal to
1
2 (1 ‚àír)2 + 1
2r2.
Since marker alleles in the two half-sibs are equal to each other one-half of
the time, Pr (M = 1) = 1
2. Therefore
Pr (z = 1|M = 1) = (1 ‚àír)2 + r2.
The conditional covariance between the half-sibs, given that they inherited
the same marker allele from their sire, is
Cov (x, y|M = 1)
=
Cov (x, y|z = 1, M = 1)

(1 ‚àír)2 + r2
=
(1 ‚àír)2 + r2
2
Vg.
(1.144)
This is because, if the two alleles are identical by descent,
Cov (x, y|z = 1, M = 1) = Vg
2
in (1.142). Similar arguments lead to the following expression for the condi-
tional covariance between the half-sibs, given that they inherited diÔ¨Äerent
marker alleles from their sire
Cov (x, y|M = 0) = r (1 ‚àír)
2
Vg.
(1.145)
As r ‚Üí0, (1.144) and (1.145) tend to V g/2 and to 0, respectively. On the
other hand, when the marker provides less and less information about the
locus in question (i.e., when r ‚Üí1/2), both (1.144) and (1.145) tend to
V g/4, as in (1.141), as it should.
‚ñ†
In this chapter, we have discussed and illustrated the most important
univariate and multivariate distributions encountered in statistical genetics.
This is extended in Chapter 2, which discusses random processes arising
from functions of random variables.

This page intentionally left blank

2
Uncertainty about Functions
of Random Variables
2.1
Introduction
It is seldom the case that the initial parameterization of a statistical model
for genetic analysis will lead directly to inferences about all parameters of
interest. One may also wish to learn about functions of the parameters of
the model. An illustration is the use of a linear mixed model parameter-
ized in terms of variance components. The investigator may wish to make
inferences about variance ratios or functions thereof, such as intraclass cor-
relations or heritabilities. As another example, consider data from a trial
in which experimental mice are subjected to varying doses of a carcino-
genic agent. The response variable is whether a tumor has developed or
not at the end of the trial. Because of the binary nature of the response,
a Bernoulli sampling model may be adopted, and a linear structure (with
dose as an explanatory variable) may be imposed to the log of the ratio
between the probability of developing a tumor at a given dose and that
of the complementary event. This is a ‚Äúgeneralized linear model‚Äù with a
logit link function (Nelder and Wedderburn, 1972; McCullagh and Nelder,
1989). Then, for mouse j at dose k, one could have
log
pjk
1 ‚àípjk
= Œ≤0 + Œ≤1xk,
(2.1)
where pjk is the probability of response, xk is the dose or a function thereof
(such as the logarithm of the dose), and Œ≤0 and Œ≤1 are parameters of the
dose-response process. It may be that the latter are not the parameters
of primary interest, for example, one may wish to Ô¨Ånd the dose at which

78
2. Functions of Random Variables
the probability of developing a tumor is 1
2 (this is usually called the LD-50
dose, with LD standing for ‚Äúlethal dose‚Äù). At this probability, it is seen
that
xLD-50 = ‚àíŒ≤0
Œ≤1
.
(2.2)
In a model where the parameters are regarded as random and, therefore,
having a joint distribution, it would follow that the LD-50 is also a random
variable, by virtue of being a function of randomly varying quantities. Even
if the parameters are not ‚Äúrandom‚Äù, in the usual sense of being drawn
randomly from a conceptual population of values (as in a random eÔ¨Äects
model), one could perhaps argue from a Bayesian perspective, as follows.
Since the LD-50 is unknown, it is an uncertain quantity and, therefore,
there is randomness which is naturally measured by probability. Here the
LD-50 would be viewed as random irrespective of whether or not the Œ≤‚Äôs are
drawn randomly from a population! Disregarding the issue of the origin of
the randomness, model (2.1) would require invoking a bivariate distribution
for Œ≤0 and Œ≤1, whereas the LD-50 in (2.2) involves a scalar distribution
resulting from a nonlinear function of Œ≤0 and Œ≤1.
In this chapter, we review some concepts of distribution theory needed
for functions of random variables, and illustrate the theory with examples.
Single random variables are considered in the Ô¨Årst section. Subsequently,
functions of sets of random variables are discussed.
2.2
Functions of a Single Random Variable
2.2.1
Discrete Random Variables
If X is a discrete random variable having some p.m.f. p (x), then any func-
tion of X, say Y = f (X), is also a random variable. If the inverse trans-
formation from Y to X is denoted by f ‚àí1 (Y ) = X, then the p.m.f. of the
random variable Y is
pY (y) = pX

f ‚àí1 (y)

.
(2.3)
Example 2.1
A situation involving the binomial distribution
Suppose that X has a binomial distribution, Bi (p, n). Consider the random
variable Y = f (X) = n‚àíX. The inverse transformation is f ‚àí1 (Y ) = X =
n ‚àíY . The probability function of Y is then
pY (y) = pX

f ‚àí1 (y)

= pX (n ‚àíy)
=

n
n ‚àíy

pn‚àíy (1 ‚àíp)n‚àí(n‚àíy)
=

n
y

(1 ‚àíp)y pn‚àíy.

2.2 Functions of a Single Random Variable
79
Therefore, Y has also a binomial distribution, but now with parameters
(1 ‚àíp) and n. The sample space of Y is the same as that of X, that is,
Y = 0, 1, ..., n.
‚ñ†
2.2.2
Continuous Random Variables
Let X be a continuous random variable having sample space A (denoted
as X ‚ààA) and p.d.f. p (x) (so p (x) = 0 for x /‚ààA). Let Y be a function
of X, Y = f (X). Then, if f(¬∑) is a monotone function and the inverse
transformation is X = f ‚àí1 (Y ), the p.d.f. of Y is given by
pY (y) = pX

f ‚àí1 (y)
 """"
d
dy f ‚àí1 (y)
""""
= pX

f ‚àí1 (y)

|J (y)| ,
y ‚ààf (A) ,
(2.4)
and pY (y) = 0 for y /‚ààf (A). In (2.4), |J (y)| is the absolute value of the
Jacobian of the transformation as a function of y. This ensures that the
density is positive throughout (the derivative of the inverse function with
respect to y may be negative at some values).
Interpretation of expression (2.4) can be facilitated recalling (1.21) which
discloses that a p.d.f. has units: probability by unit of measurement of the
random variable. A change in these units leads naturally to a change in the
density function.
An equivalent, perhaps more suggestive way of writing (2.4) is
pY (y) = pX (x)
""""
dx
dy
"""" ,
y ‚ààf (A) , x = f ‚àí1 (y) .
(2.5)
Result (2.4) is not intuitively obvious, and its proof is as follows (e.g.
Hoel et al., 1971). Let F and G denote the respective c.d.f.s of X and Y .
Suppose Ô¨Årst that Y = f (X) is strictly increasing, i.e., f (x1) < f (x2) if
x1 < x2, with x1 ‚ààA and x2 ‚ààA. Then f ‚àí1 is strictly increasing on f (A) ,
for y ‚ààf (A). One can write
G (y) = Pr (Y ‚â§y)
= Pr (f (X) ‚â§y)
= Pr

X ‚â§f ‚àí1 (y)

= F

f ‚àí1 (y)

.
Using the chain rule of diÔ¨Äerentiation yields
d
dy G (y) = d
dy F

f ‚àí1 (y)

=
d
df ‚àí1 (y)F

f ‚àí1 (y)
 df ‚àí1 (y)
dy
= p

f ‚àí1 (y)
 df ‚àí1 (y)
dy
.

80
2. Functions of Random Variables
Now
df ‚àí1 (y)
dy
=
""""
df ‚àí1 (y)
dy
""""
since f ‚àí1 is strictly increasing, so this yields (2.4). Suppose next that f is
strictly decreasing on A. Then f ‚àí1 is strictly decreasing on f (A), and for
y ‚ààf (A) the following holds:
G (y) = Pr (Y ‚â§y)
= Pr (f (X) ‚â§y)
= Pr

X ‚â•f ‚àí1 (y)

= 1 ‚àíF

f ‚àí1 (y)

.
Thus
d
dy G (y) = ‚àíd
dy F

f ‚àí1 (y)

=
d
df ‚àí1 (y)F

f ‚àí1 (y)
 
‚àídf ‚àí1 (y)
dy

= p

f ‚àí1 (y)
 
‚àídf ‚àí1 (y)
dy

.
Here
‚àídf ‚àí1 (y)
dy
=
""""
df ‚àí1 (y)
dy
""""
because f ‚àí1 is strictly decreasing, thus yielding (2.4) again. Therefore, in
either case, we see that the density of Y is given by (2.4).
Example 2.2
The lognormal distribution
A variable whose logarithm is normally distributed is said to have a lognor-
mal distribution. Let X ‚àºN

m, œÉ2
, for ‚àí‚àû< x < ‚àû, and suppose one
seeks the distribution of the transformed random variable Y = f (X) =
exp (X). The inverse transformation is X = f ‚àí1 (Y ) = ln (Y ). The p.d.f.
of Y is then, using (2.4),
p (y) = p

f ‚àí1 (y)
 """"
d
dy f ‚àí1 (y)
""""
=
1
y
‚àö
2œÄœÉ2 exp

‚àí1
2œÉ2 (ln y ‚àím)2

,
where 1/y enters from the Jacobian of the transformation. Then, Y =
exp (X) is said to have a lognormal distribution, with density as given
above. This distribution arises, for example, in quantitative genetic analysis
of the productive life of breeding animals (Ducrocq et al., 1988) and in
survival analysis (Kleinbaum, 1996).
‚ñ†

2.2 Functions of a Single Random Variable
81
Example 2.3
Distribution of the inverse of a lognormal random variable
Consider now the transformation Z = 1/Y where Y is lognormal, with
density as given above. The absolute value of the Jacobian of the transfor-
mation is z‚àí2 and, employing this in conjunction with the density above
gives
p (z) =
1
z‚àí1‚àö
2œÄœÉ2 exp

‚àí1
2œÉ2 (‚àíln z ‚àím)2

z‚àí2
=
1
z
‚àö
2œÄœÉ2 exp

‚àí1
2œÉ2 (ln z + m)2

.
This implies that the reciprocal of a lognormal random variable is lognormal
as well.
‚ñ†
Example 2.4
Transforming a uniform random variable
Let X be a random variable having a uniform distribution in the interval
[0, 1], so its p.d.f. is
p (x|0, 1) =

1,
if 0 ‚â§x ‚â§1,
0,
otherwise.
The uniform distribution was used by Bayes (1763) in an attempt to rep-
resent prior ignorance about the value of a parameter within a certain
range. This ‚Äúprinciple of insuÔ¨Écient reason‚Äù has been used extensively in
quantitative genetics and in other Ô¨Åelds. The uniform distribution assigns
equal probabilities to all possible ranges of equal length within which the
random variable can fall. Thus, it is intuitively appealing to use the uni-
form process to represent (in terms of probability) lack of prior knowledge.
Often though, the uniform distribution is not a good choice for conveying
vague prior knowledge. For example, one may think that a uniform distri-
bution between ‚àí1 and 1 can be used to represent prior ignorance about
a coeÔ¨Écient of correlation. However, Bayarri (1981) showed that the prior
distribution that should be used for such purpose is not the uniform.
Suppose that one is interested in the variable Y = f (X) = ‚àílog X. The
inverse transformation is X = f ‚àí1 (Y ) = exp (‚àíY ). The interval [0, 1], con-
stituting the sample space of X, maps onto the interval [0, ‚àû) as the sample
space for Y . The absolute value of the Jacobian of the transformation is
then
""""
d
dy f ‚àí1 (‚àíy)
"""" =
""""
d
dy exp (‚àíy)
"""" = exp (‚àíy) .
The p.d.f. of Y is, therefore,
p (y) = p

f ‚àí1 (y)

exp (‚àíy) = exp (‚àíy) ,
for y > 0.
(2.6)
This is the density of an exponentially distributed random variable with
mean and variance equal to 1. More generally, as noted in Chapter 1, the

82
2. Functions of Random Variables
density of an exponential distribution with parameter Œ∑ is
p (y|Œ∑) = Œ∑ exp (‚àíŒ∑y) ,
for y > 0
and the mean and variance can be shown to be Œ∑‚àí1 and Œ∑‚àí2, respectively.
From the developments leading to (2.6), it follows that if one wishes to
simulate random variables from an exponential distribution with parameter
Œ∑ the draws can be computed as
y = ‚àíln (x)
Œ∑
,
where x is a draw from Un (0, 1) .
‚ñ†
Example 2.5
From the beta to the logistic distribution
Assume that X follows the beta distribution, Be (a, b), and recall that its
support is the set of values of X in the closed interval [0, 1]. Applying the
logistic transformation yields
Y = f (X) = ln

X
1 ‚àíX

,
where Y is often referred to as a logit. Note that Y is deÔ¨Åned in the space
(‚àí‚àû, ‚àû). The inverse transformation is
f ‚àí1 (Y ) =
exp (Y )
1 + exp (Y ).
The Jacobian of the transformation, in this case being positive for all values
of y, is
d
dy

f ‚àí1 (y)

=
exp (y)
[1 + exp (y)]2
so the p.d.f. of Y is
p (y) = C

exp (y)
1 + exp (y)
a‚àí1 
1
1 + exp (y)
b‚àí1
exp (y)
[1 + exp (y)]2
= C
#
[exp (y)]a
[1 + exp (y)]a+b
$
,
‚àí‚àû< y < ‚àû,
(2.7)
where the constant is:
C = Œì (a + b)
Œì (a) Œì (b)
as given in (1.36) of the previous chapter. For appropriate values of the
parameters a and b, the density (2.7) can be shown to approximate well
that of a normally distributed random variable. This is why linear models
are often employed for describing the variation of logistically transformed

2.2 Functions of a Single Random Variable
83
variables taking values between 0 and 1 (such as probabilities) before trans-
formation. For example, Gianola and Foulley (1983) described methods for
genetic analysis of logits of probabilities, using Bayesian ideas.
‚ñ†
Example 2.6
The standard logistic distribution
Consider the random variable Y from a logistic distribution with p.d.f.
p (y) =
exp (y)
[1 + exp (y)]2 .
This distribution has mean 0 and variance œÄ2/3. There may be interest in
Ô¨Ånding the distribution of the linear combination f (Y ) = Z = Œ± + Œ≤Y ,
where Œ± and Œ≤ are constants. Here the inverse transformation is
f ‚àí1 (Z) = Œ≤‚àí1 (Z ‚àíŒ±) ,
and the Jacobian of the transformation is Œ≤‚àí1. The density of interest is
then
p (z) =
exp

z‚àíŒ±
Œ≤

Œ≤

1 + exp

z‚àíŒ±
Œ≤
2 .
The density characterizes the sech-squared distribution (Johnson and Kotz,
1970b), that has mean and variance
E (Z) = Œ±
and
V ar (Z) = (Œ≤œÄ)2
3
,
respectively. If, in the transformation, one takes Œ± = 0 and Œ≤ =
‚àö
3/œÄ, the
density becomes
p (z) =
œÄ exp

œÄz
‚àö
3

‚àö
3

1 + exp

œÄz
‚àö
3
2
(2.8)
this being the density of a standard logistic distribution, with mean 0 and
variance 1.
‚ñ†
Example 2.7
Moment generating function of the logistic distribution
The moment generating function (see Chapter 1) of a random variable
following a logistic distribution is
E [exp (tX)] =
‚àû

‚àí‚àû
exp (tx)
exp (x)
[1 + exp (x)]2 dx.
(2.9)

84
2. Functions of Random Variables
Change variables to
Y =
exp (x)
[1 + exp (x)],
so
1 ‚àíY =
1
[1 + exp (x)],
and note that the sampling space of Y goes from 0 to 1. The inverse trans-
formation is
X = ln

Y
1 ‚àíY

with Jacobian
dX
dY =
1
Y (1 ‚àíY ).
Observe now that
exp (tX) = exp

t ln

Y
1 ‚àíY

= Y t (1 ‚àíY )‚àít .
Using the preceding in (2.9) yields the moment generating function
E [exp (tX)] =
1

0
y1+t‚àí1 (1 ‚àíy)1‚àít‚àí1 dy = B (1 + t, 1 ‚àít)
= Œì (1 + t) Œì (1 ‚àít)
Œì (2)
= Œì (1 + t) Œì (1 ‚àít) ,
(2.10)
where B (¬∑) is called the beta function and Œì (¬∑) is the gamma function,
seen in Chapter 1.
‚ñ†
Example 2.8
The inverse chi-square distribution from a gamma process
As seen in Chapter 1, a gamma random variable has as p.d.f.
p (x|a, b) = Ga (x|a, b) =

Œì (a) b‚àía‚àí1 xa‚àí1 exp (‚àíbx) ,
x > 0,
(2.11)
where a and b are strictly positive parameters. If one sets a = ŒΩ/2 and
b = 1/2, where ŒΩ is an integer, the above p.d.f. becomes that of a chi-square
random variable. The parameter ŒΩ is known as the degrees of freedom of
the distribution. The p.d.f. of a chi-square random variable is
p (x|ŒΩ) =

Œì
ŒΩ
2

2ŒΩ/2‚àí1
x
ŒΩ
2 ‚àí1 exp

‚àí1
2x

,
x > 0.
(2.12)
Let Y = 1/X be an ‚Äúinverse chi-square‚Äù random variable. Noting that
d
dy f ‚àí1 (y) = ‚àíy‚àí2,

2.2 Functions of a Single Random Variable
85
then the p.d.f. of the inverse chi-square distribution is
p (y|ŒΩ) =

Œì
ŒΩ
2

2ŒΩ/2‚àí1
y‚àí( ŒΩ
2 ‚àí1) exp

‚àí1
2y

y‚àí2
=

Œì
ŒΩ
2

2ŒΩ/2‚àí1
y‚àí( ŒΩ
2 +1) exp

‚àí1
2y

,
y > 0.
(2.13)
‚ñ†
Example 2.9
The scaled inverse chi-square distribution
A by-product of the inverse chi-square distribution plays an important
role in variance component problems. Assume that X is a random variable
following an inverse chi-square distribution. Let S be a positive nonrandom
quantity called the scale parameter, and consider the transformation Y =
f (X) = SX, with inverse X = f ‚àí1 (Y ) = Y/S. Noting that the Jacobian
is
d
dy f ‚àí1 (y) = 1
S ,
then the p.d.f. of Y , using (2.4) and (2.13), is
p (y|ŒΩ, S) =

Œì
ŒΩ
2

2ŒΩ/2‚àí1
S( ŒΩ
2)y‚àí( ŒΩ
2 +1) exp

‚àíS
2y

‚àùy‚àí( ŒΩ
2 +1) exp

‚àíS
2y

,
y > 0, ŒΩ > 0, S > 0.
(2.14)
This is the density of the scaled inverse chi-square distribution. An alter-
native parameterization (employed often) is obtained by deÔ¨Åning the scale
parameter as S = ŒΩS‚àó. For example, in certain Bayesian variance compo-
nent problems (Lindley and Smith, 1972; Box and Tiao, 1973; Gelfand and
Smith, 1990), a scaled inverse chi-square distribution is used to represent
prior uncertainty about a variance component. Here the value of S‚àómay
be interpreted as a statement about the mean or mode of this prior distri-
bution of the variance component, and ŒΩ as a degree of belief in such value.
The term ‚Äúdegree of belief‚Äù is appealing because ŒΩ is a strictly positive,
continuous parameter, whereas ‚Äúdegrees of freedom‚Äù is normally employed
in connection with linear models to refer to integer quantities pertaining to
the rank of certain matrices (Searle, 1971). It will be shown later that when
ŒΩ tends to inÔ¨Ånity, the mean of the scaled inverse chi-square distribution
tends toward S‚àó.
The scaled inverse chi-square is a special case of the inverse gamma distri-
bution, whose density is
p (x|a, b) = Cx‚àí(a+1) exp (‚àíb/x) ,
x > 0, a, b > 0,
(2.15)
where C = ba/Œì (a). Note that (2.14) can be retrieved from (2.15), by
setting a = ŒΩ/2 and b = S/2.

86
2. Functions of Random Variables
Suppose one seeks the mean of the distribution with density kernel (2.14).
First, the propriety (integrability to a Ô¨Ånite value) of the distribution will
be assessed. Consider
 ‚àû
0
Cx‚àí( ŒΩ
2 +1) exp

‚àíS
2x

dx.
(2.16)
To evaluate this integral, make the change of variable y = 1/x, so dx =
‚àíy‚àí2 dy. Then (2.16) is expressible as
C
 ‚àû
0
y( ŒΩ
2 +1) exp

‚àíS
2 y

y‚àí2 dy = C
 ‚àû
0
y( ŒΩ
2 ‚àí1) exp

‚àíS
2 y

dy.
Recalling the result from gamma integrals employed earlier in the book (for
details, see Abramowitz and Stegun, 1972), for Œ± > 0 and Œª > 0,
 ‚àû
0
zŒ±‚àí1 exp [‚àíŒªz] dz = Œì (Œ±)
ŒªŒ± .
Making use of this in the expression above yields
C
 ‚àû
0
y( ŒΩ
2 ‚àí1) exp

‚àíS
2 y

dy = C Œì
 ŒΩ
2

 S
2
 ŒΩ
2
since
C =

Œì
 ŒΩ
2

 S
2
 ŒΩ
2
‚àí1
.
Hence, the distribution is proper provided S and ŒΩ are both positive. Com-
putation of the expected value of the distribution requires evaluation of
E (X|ŒΩ, S) = C
 ‚àû
0
x x‚àí( ŒΩ
2 +1) exp

‚àíS
2 x‚àí1

dx
= C
 ‚àû
0
y( ŒΩ
2) exp

‚àíS
2 y

y‚àí2dy
= C
 ‚àû
0
y( ŒΩ
2 ‚àí1‚àí1) exp

‚àíS
2 y

dy.
Making use of the gamma integral again:
E (X|ŒΩ, S) = C Œì
 ŒΩ
2 ‚àí1

 S
2
 ŒΩ
2 ‚àí1 ,
ŒΩ
2 ‚àí1 > 0,
with the preceding condition required for the gamma integral to exist.
Finally, recalling that Œì (ŒΩ) = (ŒΩ ‚àí1) Œì (ŒΩ ‚àí1), this expression reduces to
E (X|ŒΩ, S) =
S
ŒΩ ‚àí2,
ŒΩ > 2.
(2.17)

2.2 Functions of a Single Random Variable
87
If ŒΩ ‚â§2, the expected value is not Ô¨Ånite. A similar development leads to
the result that if ŒΩ ‚â§4, the variance of the distribution is not deÔ¨Åned.
However, the scaled inverse chi-square distribution is still proper, provided
that the degree of belief parameter is positive. Note that for S = ŒΩS‚àó, the
expectation in (2.17) becomes
E (X|ŒΩ, S‚àó) = ŒΩS‚àó
ŒΩ ‚àí2,
ŒΩ > 2.
Hence, E (X|ŒΩ, S‚àó) ‚ÜíS‚àóas ŒΩ ‚Üí‚àû. It can also be veriÔ¨Åed that the mode
of the scaled inverted chi-square distribution is
Mode (X|ŒΩ, S‚àó) = ŒΩS‚àó
ŒΩ + 2.
‚ñ†
Many-to-one Transformations
There are situations in which the transformation is not one-to-one. For
example consider Y = X2, where X is a random variable with known
distribution. Here the transformation from X to Y is clearly not one-to-
one, because both X and ‚àíX produce the same value of Y . This section
discusses how to cope with these many-to-one transformations.
To formalize the argument, let X be a continuous random variable with
p.d.f. pX (x) and let Y deÔ¨Åne the many-to-one transformation Y = f (X).
If A denotes the space where p (x) > 0, and B is the space where g (y) > 0,
then there exist points in B that correspond to more than one point in
A. However, if A can be partitioned into k sets A1, A2, . . . , Ak, such that
fi deÔ¨Ånes a one-to-one transformation of each Ai onto Bi (the Bi can be
overlapping), then the p.d.f. of Y is (i.e., Rao, 1973)
g (y) =
k

i=1
Ii (y ‚ààBi) pX

f ‚àí1
i
(y)

|Ji (y)| ,
(2.18)
where the indicator function Ii is 1 if y ‚ààBi and 0 otherwise, and Ji (y) =
df ‚àí1
i
(y) /dy is the Jacobian of the transformation in partition i. That
is, within each region i we work with (2.4), and then add all parts i =
1, 2, . . . , k.
Example 2.10
The square of a normal random variable: the chi-squared
distribution
Let X ‚àºN (0, 1), with density
pX (x) =
1
‚àö
2œÄ exp

‚àíx2
2

,
‚àí‚àû< x < ‚àû.

88
2. Functions of Random Variables
Let Y = f (X) = X2. The transformation f is not one-to-one over the
given domain. However, one can partition the domain into disjoint regions
within which the transformation is one-to-one. These two regions in the
space of X are A1 = (‚àí‚àû, 0) and A2 = (0, ‚àû). The corresponding regions
in the space of Y are B1 = (0, ‚àû) and B2 = (0, ‚àû), which in this case are
completely overlapping. In (A1, B1), f ‚àí1
1
(Y ) = ‚àíY 1/2, and in (A2, B2),
f ‚àí1
2
(Y ) = Y 1/2. The absolute value of the Jacobian of the transformation
in both partitions is 1
2Y ‚àí1/2. The density of Y , applying (2.18), is
g (y) = 1
2y‚àí1/2 
pX

‚àíy1/2
+ pX

y1/2
= (1/2)1/2
‚àöœÄ
y‚àí1/2 exp

‚àíy2
2

= (1/2)1/2
Œì (1/2) y‚àí1/2 exp

‚àíy2
2

,
0 < y < ‚àû,
where the last equality arises because Œì (1/2) = ‚àöœÄ. Here the indicator
function Ii is not needed because the regions in the space of Y are com-
pletely overlapping. From (1.42), g (y) is the p.d.f. of a random variable
from a chi-squared distribution with one degree of freedom.
‚ñ†
Example 2.11
Many-to-one transformation with partially overlapping
regions
Let X have p.d.f.
pX (x) = C exp(x),
‚àí1 ‚â§x ‚â§2,
where C is the integration constant. Consider again the many-to-one trans-
formation Y = f (X) = X2. DeÔ¨Åne the regions in the space of X, A1 =
(‚àí1, 0) and A2 = (0, 2), with corresponding regions in the space of Y ,
B1 = (0, 1) and B2 = (0, 4), which are partially overlapping. In (A1, B1),
f ‚àí1
1
(Y ) = ‚àíY 1/2, and in (A2, B2), f ‚àí1
2
(Y ) = Y 1/2, as in the previous
example. The absolute value of the Jacobian of the transformation in both
partitions is again equal to Y ‚àí1/2/2. The density of Y , applying (2.18), is
g (y) = 1
2y‚àí1/2 
pX

‚àíy1/2
I1 (y ‚ààB1) + pX

y1/2
I2 (y ‚ààB2)

= 1
2y‚àí1/2 [C exp (‚àí‚àöy) I1 (y ‚ààB1)] + [C exp (‚àöy) I2 (y ‚ààB2)] .
This can also be expressed as
g (y) =
Ô£±
Ô£≤
Ô£≥
1
2y‚àí1/2 
C exp

‚àí‚àöy

+

C exp
‚àöy

,
0 < y ‚â§1,
1
2y‚àí1/2C exp
‚àöy

,
1 < y ‚â§4,
which shows the discontinuity at y = 1 more transparently.
‚ñ†

2.2 Functions of a Single Random Variable
89
2.2.3
Approximating the Mean and Variance
Sometimes it is extremely diÔ¨Écult to arrive at the distribution of functions
of random variables by analytical means. Often, one may just wish to have
a rough idea of a distribution by using an approximation to its mean and
variance. In this subsection and the following one, two widely employed
and useful methods for approximating the mean and variance of the dis-
tribution of a function of a random variable are presented, and examples
are given to illustrate. The approximation in this subsection is arguably
based on a mathematical, rather than statistical argument, and has been
used extensively in quantitative genetics, specially for obtaining standard
errors of estimates of heritabilities and genetic correlations (e.g., Becker,
1984; Dempster and Lerner, 1950), as these involve nonlinear functions of
estimates of variance and covariance components. As discussed later in this
book, more powerful computer-based weaponry is presently available.
Let X be a random variable and let Y = f(X) be a function of X that
is diÔ¨Äerentiable at least twice. Expanding f(X) in a Taylor series about
X = E [X] = ¬µ gives
Y = f (X) ‚àº= f [¬µ] +
d
dX f (X)
""""
X=¬µ
(X ‚àí¬µ)
+ 1
2
d2
(dX)2 f (X)
"""""
X=¬µ
(X ‚àí¬µ)2 .
(2.19)
Taking expectations, one obtains, as a second-order approximation to E(Y )
E [Y ] ‚àº= f [¬µ] + 1
2
d2
(dX)2 f (X)
"""""
X=¬µ
V ar (X) .
(2.20)
Now, taking variances over approximation (2.19) and retaining only second
order terms
V ar (Y ) ‚àº=

d
dX f (X)
""""
X=¬µ
2
V ar (X) .
Sometimes, only the linear term is retained in (2.20), and one uses as a
rough guide
Y ‚àº
Ô£±
Ô£≤
Ô£≥f (¬µ) ,

d
dX f (X)
""""
X=¬µ
2
V ar (X)
Ô£º
Ô£Ω
Ô£æ.
(2.21)
Example 2.12
Mean and variance of a transformed, beta-distributed
random variable
In Example 2.5, it was noted that a logistic transformation of a beta ran-
dom variable gives an approximately normally distributed process. Thus,

90
2. Functions of Random Variables
if X ‚àºBe (a, b), then the variable
Y = f (X) = ln [X/ (1 ‚àíX)]
is approximately distributed as N [E (Y ) , V ar (Y )]. The mean and variance
of the beta distribution are
E (X|a, b) =
a
a + b,
V ar (X|a, b) =
ab

(a + b)2 (a + b + 1)
.
(2.22)
The mean of Y , using (2.21), is
E (Y |a, b) ‚àº= ln

E (X)
1 ‚àíE (X)

= ln
a
b

,
and using (2.20), is
E (Y |a, b) ‚àº= ln
a
b

+ (a ‚àíb) (a + b)
2ab (a + b + 1).
From (2.21) the resulting variance is
V ar (Y |a, b) ‚àº=
(a + b)2
ab (a + b + 1).
‚ñ†
Example 2.13
Genotypes in Gaussian and discrete scales: the threshold
model
Dempster and Lerner (1950) discussed the quantitative genetic analysis of a
binary character (Yo) following the ideas of Wright (1934) and of Robertson
and Lerner (1949). These authors assumed that the expression of the trait
(Yo = 0 = attribute absent, Yo = 1 = attribute present) is related to an
underlying, unobservable normal process, and that gene substitutions take
place at this level. Let
Y = ¬µ + G + E
be the Gaussian variable, where ¬µ is the mean of Y and G and E are
random terms representing the genetic and environmental eÔ¨Äects on Y ,
respectively. Suppose that G ‚àºN(0, VG), E ‚àºN(0, VE) have indepen-
dent distributions. Hence, the marginal distribution of the latent variable
is Y ‚àºN(¬µ, VG+VE). Dempster and Lerner (1950) deÔ¨Åned ‚Äúgenotype in the
observable scale‚Äù as the conditional probability of observing the attribute,
given the genotype in the latent scale, that is
Pr (Yo = 1|G) = Pr(Y > t|G),

2.2 Functions of a Single Random Variable
91
where t is a threshold (assume, subsequently, that t = 0). In other words,
the attribute is observed if the value of the latent variable exceeds the
threshold. The ‚Äúoutward‚Äù genotype can be expressed as
G0 = Pr (Yo = 1|G) = Pr(Y ‚àí¬µ ‚àíG > t ‚àí¬µ ‚àíG|G)
= 1 ‚àíPr

Z ‚â§‚àí¬µ ‚àíG
‚àöVE

= Œ¶

¬µ + G
‚àöVE

,
where Z ‚àºN(0, 1). A Ô¨Årst-order approximation of the outward genotype
about 0, the mean of the distribution of G, is
G0 ‚àº= Œ¶

¬µ
‚àöVE

+ œÜ

¬µ
‚àöVE

G
‚àöVE
.
The genetic variance in the outward scale is, approximately,
V ar (G0) = V ar

Œ¶

¬µ + G
‚àöVE

‚àº= œÜ2

¬µ
‚àöVE
 VG
VE
.
The heritability (ratio between genetic and total variance) in the underlying
scale is
h2 =
VG
VG + VE
so ‚Äúheritability in the outward scale‚Äù is approximately:
h2
o = V ar (G0)
V ar (Yo)
‚àº=
œÜ2

¬µ
‚àöVE

VG/VE
V ar (Yo)
.
Now
V ar (Yo) = E

Y 2
o

‚àíE2 (Yo)
= 02 √ó Pr(Y < t) + 12 Pr (Y ‚â•t) ‚àí[Pr (Y ‚â•t)]2
= Pr (Y ‚â•t) [1 ‚àíPr (Y ‚â•t)]
= Œ¶

‚àí¬µ
‚àöVG + VE
 
1 ‚àíŒ¶

‚àí¬µ
‚àöVG + VE

=

1 ‚àíŒ¶

¬µ
‚àöVG + VE

Œ¶

¬µ
‚àöVG + VE

,
so
h2
o =
œÜ2

¬µ
‚àöVE

VG/VE

1 ‚àíŒ¶

¬µ
‚àöVG + VE

Œ¶

¬µ
‚àöVG + VE
.

92
2. Functions of Random Variables
Since the latent variable cannot be observed, one can take the residual
standard deviation in the underlying scale to be equal to 1, so all terms are
expressed in units of ‚àöVE. In this scale
h2
o =
œÜ2 (¬µ) h2/

1 ‚àíh2

1 ‚àíŒ¶

¬µ
‚àöVG + 1

Œ¶

¬µ
‚àöVG + 1
.
Alternatively, one could take VG + VE = 1, so VG = h2, heritability in the
latent scale. Here
h2
o =
œÜ2

¬µ
‚àö
1 ‚àíh2

h2/

1 ‚àíh2
[1 ‚àíŒ¶ (¬µ)] Œ¶ (¬µ)
.
The two expressions for h2
o do not coincide with what was given by Dempster
and Lerner (1950). The reason is that these authors used a diÔ¨Äerent linear
approximation to the genotype in the discrete scale.
Consider now a second-order approximation for the genotype in the out-
ward scale. Here
G0 = Œ¶

¬µ + G
‚àöVE

‚àº= Œ¶

¬µ
‚àöVE

+ œÜ

¬µ
‚àöVE

G
‚àöVE
‚àí¬µ
VE
œÜ

¬µ
‚àöVE

G2.
The genetic variance in the outward scale in this case is
V ar (G0) ‚àº= œÜ2

¬µ
‚àöVE
 VG
VE
+
 ¬µ
VE
œÜ

¬µ
‚àöVE
2
V ar(G2)
+ 2œÜ2

¬µ
‚àöVE

¬µ
(VE)
3
2 Cov(G, G2).
Using the moment generating function of the normal distribution or, di-
rectly, results for the variance of quadratic forms on normal variates (and
for the covariance between a linear and a quadratic form, (Searle, 1971)),
it can be established that
V ar

G2
= 2 (VG)2
and
Cov

G, G2
= 0.
Hence, the heritability in the outward scale resulting from the second-order
approximation is
h2
o =
#
œÜ2

¬µ
‚àöVE

+ 2

¬µœÜ

¬µ
‚àöVE
2
(VG/VE)
$
(VG/VE)

1 ‚àíŒ¶

¬µ
‚àöVG + VE

Œ¶

¬µ
‚àöVG + VE

.

2.2 Functions of a Single Random Variable
93
The threshold model is revisited in Chapters 4 and 14.
‚ñ†
2.2.4
Delta Method
The approach based on the Taylor series described above yields approxi-
mate formulas for means and variances of functions of random variables.
However, nothing is said here about the distributional properties of the
derived statistics. A related large-sample based technique, that is more for-
mally anchored statistically, known as the delta method, does this. Borrow-
ing from Lehmann (1999), let Tn be a random variable where the subscript
expresses its dependence on sample size n. As n increases towards inÔ¨Ånity,
suppose that the sequence of c.d.f.s of ‚àön (Tn ‚àí¬µ) converges to the c.d.f.
of a normal variable with mean 0 and variance œÉ2. This limiting behavior
is known as convergence in distribution, denoted here by
‚àön (Tn ‚àí¬µ)
D
‚ÜíN

0, œÉ2
.
(2.23)
The delta method provides the following limiting distribution for a function
of Tn, f (Tn):
‚àön [f (Tn) ‚àíf (¬µ)]
D
‚ÜíN

0, œÉ2 [f ‚Ä≤ (¬µ)]2
,
(2.24)
where f ‚Ä≤ (¬µ) denotes the Ô¨Årst derivative of f (Tn) evaluated at ¬µ. The proof
of this result is based on a Taylor expansion of f (Tn) around f (¬µ):
f (Tn) = f (¬µ) + (Tn ‚àí¬µ) f ‚Ä≤ (¬µ) + op (Tn ‚àí¬µ) .
(2.25)
The notation op (Tn ‚àí¬µ) denotes a random variable of smaller order than
Tn ‚àí¬µ for large n, in the sense that, for Ô¨Åxed œµ > 0,
Pr (op (Tn ‚àí¬µ)/ (Tn ‚àí¬µ) ‚â§œµ) ‚Üí1
as n ‚Üí‚àû. Therefore this last term converges in probability to 0 as n
increases toward inÔ¨Ånity. (If Xn converges in probability to X, then for
œµ > 0,
lim
n‚Üí‚àûPr (|Xn ‚àíX| ‚â•œµ) = 0
and hence, for large n, Xn ‚âàX). Subtracting f (¬µ) from both sides and
multiplying by ‚àön yields
‚àön [f (Tn) ‚àíf (¬µ)] = ‚àön (Tn ‚àí¬µ) f ‚Ä≤ (¬µ) + ‚àönop (Tn ‚àí¬µ) .
As n ‚Üí‚àû, the second term in the right-hand side vanishes; therefore, the
left-hand side has the same limiting distribution as
‚àön (Tn ‚àí¬µ) f ‚Ä≤ (¬µ) .

94
2. Functions of Random Variables
Since Tn ‚àí¬µ is approximately normal with variance œÉ2/n, then f (Tn) ‚àí
f (¬µ) is approximately normal with variance œÉ2 [f ‚Ä≤ (¬µ)]2 /n and result (2.24)
follows.
When the term f ‚Ä≤ (¬µ) = 0, in (2.25), it is natural to carry the expansion
to a higher-order term, provided that the second derivative exists and that
it is not 0:
f (Tn) = f (¬µ) + 1
2 (Tn ‚àí¬µ)2 f
‚Ä≤‚Ä≤ (¬µ) + op (Tn ‚àí¬µ)2 ,
where f
‚Ä≤‚Ä≤ (¬µ) is the second derivative of f (Tn) evaluated at ¬µ. The last
term tends to 0 for large n; therefore we can write, loosely,
n [f (Tn) ‚àíf (¬µ)] = 1
2f
‚Ä≤‚Ä≤ (¬µ) n (Tn ‚àí¬µ)2 .
(2.26)
Now from (2.23) it follows that
n (Tn ‚àí¬µ)2
œÉ2
‚Üíœá2
1.
Therefore, using (2.26),
n [f (Tn) ‚àíf (¬µ)] ‚Üí1
2œÉ2f
‚Ä≤‚Ä≤ (¬µ) œá2
1.
(2.27)
Example 2.14
Bernoulli random variables
Let Xi (i = 1, 2, . . .) be independent Bernoulli random variables with pa-
rameter Œ∏ and let Tn = n‚àí1 n
i=1 Xi. By the central limit theorem,
‚àön (Tn ‚àíŒ∏) ‚ÜíN [0, Œ∏ (1 ‚àíŒ∏)]
because E (Tn) = Œ∏ and V ar (Tn) = Œ∏ (1 ‚àíŒ∏) /n. Imagine that one is inter-
ested in the large sample behavior of the statistic f (Tn) = Tn (1 ‚àíTn) as
an estimate of f (Œ∏) = Œ∏ (1 ‚àíŒ∏). From (2.24), since f ‚Ä≤ (Œ∏) = 1 ‚àí2Œ∏,
‚àön [Tn (1 ‚àíTn) ‚àíŒ∏ (1 ‚àíŒ∏)] ‚ÜíN

0, Œ∏ (1 ‚àíŒ∏) (1 ‚àí2Œ∏)2
for Œ∏ Ã∏= 1/ 2. When Œ∏ = 1/ 2, f ‚Ä≤ (1/ 2) = 0. Then, using (2.27), for Œ∏ = 1/ 2,
since f (1/ 2) = 1/ 4 and f ‚Ä≤‚Ä≤ (1/ 2) = ‚àí2,
n

Tn (1 ‚àíTn) ‚àí1
4

‚Üí1
2
1
4 (‚àí2) œá2
1 = ‚àí1
4œá2
1
or, equivalently,
4n
1
4 ‚àíTn (1 ‚àíTn)

‚Üíœá2
1.
‚ñ†

2.3 Functions of Several Random Variables
95
The delta method generalizes straightforwardly to functions of random
vectors. Let Tn = (Tn1, Tn2, . . . , Tnp)‚Ä≤ be asymptotically multivariate nor-
mal, with mean Œ∏ = (Œ∏1, Œ∏2, . . . , Œ∏p)‚Ä≤ and covariance matrix Œ£/n. Suppose
the function f (t1, t2, . . . , tp) has nonzero diÔ¨Äerential ‚àÜ= (‚àÜ1, ‚àÜ2, . . . , ‚àÜp)‚Ä≤
at Œ∏, where
‚àÜi = ‚àÇf
‚àÇti
""""
t=Œ∏
.
Then,
‚àön [f (Tn) ‚àíf (Œ∏)]
D
‚ÜíN

0, ‚àÜ‚Ä≤Œ£‚àÜ

.
(2.28)
2.3
Transformations Involving Several Discrete
or Continuous Random Variables
Let X = (X1, X2, . . . , Xn)‚Ä≤ be a random vector with p.m.f. or p.d.f. equal to
pX (x), and let Y = f (X) be a one-to-one transformation. Let the sample
space of X, denoted as S ‚äÜRn, be such that
Pr [(X1, X2, . . . , Xn) ‚ààS] = 1,
or, equivalently, that the integral of pX (x) over S is equal to 1. DeÔ¨Åne
T ‚äÜRn to be the image of S under the transformation, that is, as the
values of X1, X2, . . . , Xn vary over S, the values of Y1, Y2, . . . , Yn vary over
T. Corresponding to each value of Y1, Y2, . . . , Yn in the set T there is a
unique value of X1, X2, . . . , Xn in the set S, and vice-versa. This ensures
that the inverse transformation exists and this is denoted by X =f ‚àí1 (Y).
The elements of Y = f (X) are
Y1 = f1 (X1, X2, . . . , Xn) ,
Y2 = f2 (X1, X2, . . . , Xn) ,
...
Yn = fn (X1, X2, . . . , Xn) ,
whereas X = f ‚àí1 (Y) has elements
X1 = f ‚àí1
1
(Y1, Y2, . . . , Yn) ,
X2 = f ‚àí1
2
(Y1, Y2, . . . , Yn) ,
...
Xn = f ‚àí1
n
(Y1, Y2, . . . , Yn) .
Now let S‚Ä≤ be a subset of S and let T ‚Ä≤ denote the mapping of S‚Ä≤ under the
transformation. The events (X1, X2, . . . , Xn) ‚ààS‚Ä≤ and (Y1, Y2, . . . , Yn) ‚ààT ‚Ä≤

96
2. Functions of Random Variables
are said to be equivalent. Hence,
Pr [(Y1, Y2, . . . , Yn) ‚ààT ‚Ä≤] = Pr [(X1, X2, . . . , Xn) ‚ààS‚Ä≤]
(2.29)
which, in the continuous case, is equal to

pX (x1, x2, . . . , xn) dx1 dx2 . . . dxn,
where the integral is multidimensional and taken over S‚Ä≤. In order to Ô¨Ånd
the p.d.f. of Y, a change of variable of integration must be eÔ¨Äected in (2.29),
such that x1 = f ‚àí1
1
(y) , x2 = f ‚àí1
2
(y) , . . . , xn = f ‚àí1
n
(y). In calculus books
(e.g., Kaplan, 1993) it is shown that this change of variables results in the
following expression:
Pr [(Y1, Y2, . . . , Yn) ‚ààT ‚Ä≤]
=

pX

f ‚àí1
1
(y) , f ‚àí1
2
(y) , . . . , f ‚àí1
n
(y)

|J (y)| dy1dy2 . . . dyn,
(2.30)
where |J (y)| is the absolute value of the Jacobian of the transformation.
This implies that the p.d.f. of the vector Y is (writing from now onwards
J (y) = J)
pY (y) =

pX

f ‚àí1
1
(y) , f ‚àí1
2
(y) , . . . , f‚àí1
n
(y)

|J| ,
y ‚ààT,
0,
otherwise.
(2.31)
In the multivariate situation, the Jacobian is the determinant of a matrix
of Ô¨Årst derivatives and is given by
J = det
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
‚àÇf ‚àí1
1
(y)
‚àÇy1
. . .
‚àÇf ‚àí1
1
(y)
‚àÇyn
...
...
...
‚àÇf ‚àí1
n
(y)
‚àÇy1
. . .
‚àÇf ‚àí1
n
(y)
‚àÇyn
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
,
(2.32)
where ‚àÇf ‚àí1
j
(y) /‚àÇyk is the Ô¨Årst partial derivative of the jth element of
f ‚àí1 (Y) with respect to the kth element of Y. The Jacobian is also denoted
J = det
‚àÇ(x1, x2, . . . , xn)
‚àÇ(y1, y2, . . . , yn)

.
(2.33)
It may be that there is only one function of interest, for example, Y1 =
f1 (X1, . . . , Xn). By deÔ¨Åning appropriate additional arbitrary functions f2,
. . . , fn, such that the transformation is one-to-one, then the joint p.d.f. of
Y can be obtained by the method above. The p.d.f. of Y1 can be derived
subsequently by integrating pY (y) over the space spanned by Y2, Y3 . . . , Yn.

2.3 Functions of Several Random Variables
97
For discrete random variables, the joint probability of Y = y, deÔ¨Åned
within the sample space T, is given directly by
Pr (Y1 = y1, Y2 = y2, . . . , Yn = yn)
= p (X1 = x1, X2 = x2, . . . , Xn = xn) ,
(2.34)
where x1 = f ‚àí1
1
(y1, y2, . . . , yn) , . . . , xn = f ‚àí1
n
(y1, y2 . . . , yn).
If Y = f (X) is a many-to-one diÔ¨Äerentiable transformation, the density
of Y can be obtained by applying (2.31) to each solution of the inverse
transformation separately, and then summing the transformed densities
for each solution. This is exactly in the same spirit as in (2.18).
The Derivative and the Jacobian as a Local MagniÔ¨Åcation of a Projection
In this section a heuristic motivation of expression (2.31) is provided. A
more rigorous treatment can be found in standard books on multivariate
calculus (e.g., Kaplan, 1993; Williamson et al., 1972).
Consider the projection of a point x of a line onto the point g (x) of
another line. The magniÔ¨Åcation at x, when x is projected onto g (x), is
the absolute value of the derivative of g at x. For example, if g (x) = 2x,
then the magniÔ¨Åcation at x = u is |g‚Ä≤ (u)| = 2. There is a magniÔ¨Åcation
by a factor of 2 at all points, irrespective of the value of x. This means
that a distance R = x1 ‚àíx on the original line is projected onto a distance
S = g (x1) ‚àíg (x), and that the magniÔ¨Åcation of the distance at x is given
by the absolute value of the derivative of g evaluated at x:
"""" lim
x1‚Üíx
g (x1) ‚àíg (x)
x1 ‚àíx
"""" = |g‚Ä≤ (x)| .
(2.35)
As x1 ‚Üíx, the length S is, approximately,
length of S = |g‚Ä≤ (x)| length of R
or, equivalently,
dS = |g‚Ä≤ (x)| dR.
(2.36)
This argument extends to the multivariate case as follows, and two vari-
ables are used to illustrate. Let R be a region in the uv plane and let S be
a region in the xy plane. Consider a mapping given by the functions
x = f1 (u, v) ,
y = f2 (u, v) ,
and let S be the image of R under this mapping. As R approaches zero,
area of S
area of R ‚Üí|J|

98
2. Functions of Random Variables
or, equivalently,
area of S = |J| area of R,
(2.37)
where |J| is the absolute value of the Jacobian of the transformation. The
Jacobian is equal to the determinant
J = det
‚àÇ(f1, f2)
‚àÇ(u, v)

which is a function of (u, v). The absolute value of the Jacobian is a measure
of the local magniÔ¨Åcation at the point (u, v). In the bivariate case, if area
of S is approximated by dx dy and area of R is approximated by du dv,
(2.37) can be written as
dx dy = |J| du dv.
(2.38)
Then,

S
h (x, y) dx dy =

R
h (f1 (u, v) , f2 (u, v)) |J| du dv.
(2.39)
In a trivariate case, the absolute value of the Jacobian would represent the
magniÔ¨Åcation of a volume, and so on.
Example 2.15
Transforming a multinomial distribution
This example illustrates a multivariate transformation in the discrete case
and, also, how a conditional distribution can be derived from the joint and
marginal probability distributions of the transformed variables. As seen in
Chapter 1, the multinomial distribution applies to a sampling model where
n independent draws are made from the same population. The outcome of
each draw is a realization into one of C mutually exclusive and exhaustive
classes, or categories of response. The categories can be ordered or un-
ordered. For example, in beef cattle breeding the degree of ease of calving
is often scored into four classes (C = 4), for example, ‚Äúno diÔ¨Éculty‚Äù, ‚Äúsome
assistance is needed‚Äù, ‚Äúmechanical pull‚Äù, or ‚Äúcaesarean section required‚Äù,
so the categories are ordered. On the other hand, unordered categories ap-
pear, for example, in genetic analyses of leg deformities in chickens. Here
the possible classes cannot be ordered in a meaningful way.
Let ni be the number of observations falling into the ith class, and let
pi be the probability that an individual observation falls in the ith class,
for i = 1, 2, 3. Then n = n1 + n2 + n3, and the joint probability function of
(n1, n2) is (the dependence on parameters p1, p2, and n is omitted)
p (n1, n2) =
n!
n1! n2! (n ‚àín1 ‚àín2)!pn1
1 pn2
2 (1 ‚àíp1 ‚àíp2)n‚àín1‚àín2 .
(2.40)
Suppose that one needs to Ô¨Ånd the conditional probability distribution of
n1, given n1 + n2. That is, the conditional probability function is
p (n1|n1 + n2) = p (n1, n1 + n2)
p (n1 + n2)
.
(2.41)

2.3 Functions of Several Random Variables
99
To simplify the notation, let (n1, n2) = (X, Y ) and (n1, n1 + n2) = (U, V ) ,
and omit the parameters (p1, p2) as arguments of p (¬∑). The probability
function of (n1, n2) = (X, Y ) is then expressible as
p (X, Y ) =
n!
X! Y ! (n ‚àíX ‚àíY )!pX
1 pY
2 (1 ‚àíp1 ‚àíp2)n‚àíX‚àíY .
(2.42)
To derive the numerator in (2.41), that is, p (n1, n1+n2) = p (U, V ), note
that the transformation (X, Y ) ‚Üí(U, V ) can be written as

U
V

= f (X, Y ) =

1
0
1
1
 
X
Y

=

X
X + Y

(2.43)
with inverse transformation

X
Y

= f ‚àí1 (U, V ) =

1
0
‚àí1
1
 
U
V

=

U
V ‚àíU

.
Therefore, from (2.34),
pU,V (U, V ) =
n!
U! (V ‚àíU)! (n ‚àíV )!pU
1 pV ‚àíU
2
(1 ‚àíp1 ‚àíp2)n‚àíV .
(2.44)
To obtain p (n1|n1 + n2), (2.44) must be divided by p (V ). Now the random
variable V follows the binomial distribution
V ‚àºBi (p1 + p2, n) .
This is so because the three classes can be regrouped into two ‚Äúwider‚Äù cat-
egories, one where the counts (n1 + n2) are observed to fall, and the other
involving the third original category with counts n3. In view of the inde-
pendence of the draws, it follows that (n1 + n2) is binomially distributed.
Dividing pU,V (U, V ) in (2.44) by the marginal probability function pV (V )
we obtain
p (n1|n1 + n2) = p (U|V )
= p (U, V )
p (V )
=
V !
U! (V ‚àíU)!
pU
1 pV ‚àíU
2
(p1 + p2)V
= (n1 + n2)!
n1! n2!
pn1
1 pn2
2
(p1 + p2)n1+n2
= (n1 + n2)!
n1! n2!

p1
p1 + p2
n1 
p2
p1 + p2
n2
.
This implies that
[n1|n1 + n2] ‚àºBi

p1
p1 + p2
, n1 + n2

.

100
2. Functions of Random Variables
Hence, the conditional distribution [n1|n1 + n2] has mean,
E (n1|n1 + n2) = (n1 + n2)
p1
p1 + p2
and variance
V ar (n1|n1 + n2) = (n1 + n2)
p1
p1 + p2
p2
p1 + p2
.
‚ñ†
Example 2.16
Distribution of the ratio between two independent ran-
dom variables
Suppose that two random variables are i.i.d., with densities equal to p (xi) =
2xi, for i = 1, 2, with the sample space being the set of all points contained
in the interval (0, 1). Their joint p.d.f. is then
p (x1, x2) =

4x1x2,
for 0 < x1 < 1 and 0 < x2 < 1,
0,
otherwise.
(2.45)
We wish to Ô¨Ånd the p.d.f. of the ratio Y1 = X1/X2. This is a situation
where an auxiliary variable is needed in order to make the transformation
one-to-one. To Ô¨Ånd the p.d.f. of the ratio, the auxiliary variable is integrated
out from the joint density of all transformed variables. Let the auxiliary
variable be Y2 = X2. Then
Y1 = f1 (X1, X2) = X1/X2,
Y2 = f2 (X1, X2) = f2 (X2) = X2.
The inverse transformation is
X1 = f ‚àí1
1
(Y1, Y2) = Y1Y2,
X2 = f ‚àí1
2
(Y1, Y2) = f ‚àí1
2
(Y2) = Y2.
In view of the support of p (x1, x2), to Ô¨Ånd the sample space of the joint
distribution [Y1, Y2] , observe that
0 < y1 < ‚àû,
0 < y2 < 1.
Now, from
0 < y1y2 < 1,
the following relationship also holds
0 < y2 < 1
y1
.

2.3 Functions of Several Random Variables
101
The Jacobian of the transformation is
J = det
Ô£Æ
Ô£ØÔ£ØÔ£∞
‚àÇf ‚àí1
1
(y1, y2)
‚àÇy1
‚àÇf ‚àí1
1
(y1, y2)
‚àÇy2
‚àÇf ‚àí1
2
(y1, y2)
‚àÇy1
‚àÇf ‚àí1
2
(y1, y2)
‚àÇy2
Ô£π
Ô£∫Ô£∫Ô£ª
= det
Ô£Æ
Ô£ØÔ£ØÔ£∞
‚àÇ(y1y2)
‚àÇy1
‚àÇ(y1y2)
‚àÇy2
‚àÇy2
‚àÇy1
‚àÇy2
‚àÇy2
Ô£π
Ô£∫Ô£∫Ô£ª
= det

y2
y1
0
1

= y2.
The p.d.f. of the vector Y = [Y1, Y2]‚Ä≤ is obtained as follows: in the density
p (x1, x2), replace x1 by y1y2, x2 by y2, and then multiply the result by the
absolute value of J. This leads to
p (y1, y2) =

4y1y3
2,
0 < y1 < ‚àû, 0 < y2 < 1, 0 < y1y2 < 1,
0,
otherwise.
(2.46)
We now check whether or not this is a proper p.d.f.: the integral of p (y1, y2)
over the sampling space induced by the transformation must be Ô¨Ånite. We
then have
 
p (y1, y2) dy1 dy2
=
 1
y2=0
 1
y1=0
p (y1, y2) dy1dy2 +
 1/y1
y2=0
 ‚àû
y1=1
p (y1, y2) dy1 dy2
=
 1
y1=0
4y1

y4
2
4
""""
1
0

dy1 +
 ‚àû
y1=1
4y1

y4
2
4
""""
1
y1
0

dy1
= 1
2 + 1
2 = 1.
Thus, propriety is established. The marginal p.d.f. of Y1 = X1/X2 is ob-
tained by integrating the joint density with respect to Y2, yielding
p (y1) =
Ô£±
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£≤
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£≥
 1
y2=0
p (y1, y2) dy2 = 4y1

y4
2
4
""""
1
0

= y1, 0 < y1 < 1,
 1
y2=0
p (y1, y2) dy2 = 4y1

y4
2
4
""""
1
y1
0

= 1
y3
1
, 1 < y1 < ‚àû.
Consider now the calculation of
Pr

X1 < 1
2, X2 < 0.7

=
 0.7
x2=0

1
2
x1=0
4x1x2 dx1 dx2
= 0.1225.

102
2. Functions of Random Variables
In view of the relationship between (X1, X2) and (Y1, Y2) this joint proba-
bility can be written in terms of (Y1, Y2) as
Pr

Y1Y2 < 1
2, Y2 < 0.7

= Pr

Y1 <
1
2Y2
, Y2 < 0.7

=
 0.7
y2=0

1
2y2
y1=0
4y1y3
2 dy1 dy2
= 0.1225,
corroborating (2.29).
‚ñ†
Example 2.17
Parameterization of a variance components model
Consider a Gaussian linear mixed eÔ¨Äects model for animal breeding data,
and let this model have two sets of random eÔ¨Äects with variances œÉ2
a and
œÉ2
e, respectively, both unknown. For example, œÉ2
a could be the additive
genetic variance, and œÉ2
e the environmental variance for a certain trait.
Suppose that in a Bayesian setting (where unknown parameters are treated
as random variables), the two variance components (both strictly positive)
are assigned a proper joint prior density equal to
p(œÉ2
a, œÉ2
e) = p(œÉ2
a)p(œÉ2
e),
œÉ2
a ‚â•0, œÉ2
e > 0,
so that there is independence between the two components, a priori. Sup-
pose, further, that there is a family structure, so that observations can be
clustered into families of half-sibs, such that the observations within a clus-
ter are equicorrelated, whereas those in diÔ¨Äerent clusters are independent.
Let the variance between half-sib clusters be œÉ2
s and the variance within
clusters be œÉ2
w. Conceivably, one may wish to parameterize the model in
terms of random eÔ¨Äects having variances œÉ2
s and œÉ2
w. From a classical per-
spective, the two models are said to be equivalent (Henderson, 1984) if the
same likelihood (see Chapter 3 for a formal deÔ¨Ånition of the concept) is
conferred by the data to values of the same parameter under each of the
models. For this equivalence to hold, a relationship is needed to establish a
link between the two sets of parameters, and this comes from genetic theory.
Under additive inheritance (Fisher, 1918) in a randomly mated population
in linkage equilibrium, one has:
1) œÉ2
s = 1
4œÉ2
a, and
2) œÉ2
w = 3
4œÉ2
a + œÉ2
e.
This is a one-to-one linear transformation expressible in matrix notation
as

œÉ2
s
œÉ2
w

=

1
4
0
3
4
1
 Ô£Æ
Ô£ØÔ£∞
œÉ2
a
œÉ2
e
Ô£π
Ô£∫Ô£ª.
Since heritability h2 = œÉ2
a/(œÉ2
a +œÉ2
e) necessarily takes values between 0 and
1, it follows that œÉ2
s/œÉ2
w must take a value between 0 and 1/3. In order to

2.3 Functions of Several Random Variables
103
arrive at the probabilistic beliefs in the parameterization in terms of half-
sib clusters, one must eÔ¨Äect a change of variables in the prior distribution.
It turns out that the joint prior density of œÉ2
s and œÉ2
w is:
p

œÉ2
s, œÉ2
w

= 4p

4œÉ2
s

p

œÉ2
w ‚àí3œÉ2
s

,
œÉ2
w > 0, 0 ‚â§œÉ2
s ‚â§œÉ2
w
3 .
(2.47)
This shows that œÉ2
s and œÉ2
w are not independent a priori. It follows that a
Bayesian that takes œÉ2
a and œÉ2
e as independent a priori, has diÔ¨Äerent prior
beliefs than one that assigns independent prior distributions to œÉ2
s and œÉ2
w.
Even if the likelihood in the two models confers the same strength to values
of the same parameter, the two models would not be equivalent, at least
in the Bayesian sense, unless the joint prior density in the second param-
eterization has the form given in (2.47). This illustrates that constructing
models that are probabilistically consistent (or, in a Bayesian context, that
reÔ¨Çect beliefs in a coherent manner) require careful consideration not only
of the statistics, but of the subject matter of the problem as well.
‚ñ†
Example 2.18
Conditioning on a function of random variables
Suppose there are two continuously distributed random vectors x and y
having joint p.d.f. p (x, y). Here the distinction between a random variable
and its realized value is omitted. Let z = f (y) be a vector valued function
of y, and let y = f ‚àí1 (z) be the inverse transformation. We wish to Ô¨Ånd
the p.d.f. of the conditional distribution of x given z. Assume that each of
the elements of any of the vectors can take any value in the real line. The
joint density of x and z is
pXZ (x, z) = pXY

x, f ‚àí1 (z)

|J|
where the Jacobian is
J = det
Ô£Æ
Ô£ØÔ£ØÔ£∞
‚àÇx
‚àÇx‚Ä≤
‚àÇx
‚àÇz‚Ä≤
‚àÇf ‚àí1 (z)
‚àÇx‚Ä≤
‚àÇf ‚àí1 (z)
‚àÇz‚Ä≤
Ô£π
Ô£∫Ô£∫Ô£ª= det
Ô£Æ
Ô£∞
I
0
0
‚àÇf ‚àí1 (z)
‚àÇz‚Ä≤
Ô£π
Ô£ª
= det
‚àÇf ‚àí1 (z)
‚àÇz‚Ä≤

.
Therefore
pXZ (x, z) = p

x, f ‚àí1 (z)
 """"det
‚àÇf ‚àí1 (z)
‚àÇz‚Ä≤
"""" .
Recall that
pZ (z) = pY

f ‚àí1 (z)
 """"det
‚àÇf ‚àí1 (z)
‚àÇz‚Ä≤
"""" .

104
2. Functions of Random Variables
To obtain p (x|z), use is made of the fact that
pX|Z (x|z) = p (x, z)
p (z)
=
pXY

x, f ‚àí1 (z)
 """det

‚àÇf ‚àí1(z)
‚àÇz‚Ä≤
"""
pY (f ‚àí1 (z))
"""det

‚àÇf ‚àí1(z)
‚àÇz‚Ä≤
"""
= pX|Y

x|f ‚àí1 (z)

.
(2.48)
This is an important result: it shows that if z is a one-to-one transformation
of y, the conditional p.d.f. (or distribution) of x, given z, is the same as the
conditional p.d.f. of x, given y. Arguing intuitively, this implies that the
same inferences about parameters of this conditional distribution are drawn
irrespective of whether one bases inferences on x|y or on x|z. Suppose, in a
Bayesian context, that x is unknown and that y is an observed data vector.
Often, all information about x will be contained in a vector z having a lower
dimension than y (loosely speaking, one can refer to this as a principle of
suÔ¨Éciency, but see Chapter 3); in this case, the posterior distribution of x
based on z will lead to the same inferences about x than the corresponding
posterior distribution based on y, but with a considerable reduction in
dimensionality. For example, in Bayesian inferences about the coeÔ¨Écient
of correlation of a bivariate normal distribution from which n pairs have
been sampled at random (so the data y are in a vector of order 2n√ó1), the
posterior distribution of the correlation parameter can be shown to depend
on the data only through its ML estimator, which is a scalar variable (Box
and Tiao, 1973; Bayarri, 1981; Bernardo and Smith, 1994).
To illustrate (2.48), suppose that phenotypic values for a certain trait y
are recorded on members of nuclear families consisting of an oÔ¨Äspring (y0),
a father (yf), and a mother (ym). Assume that their joint p.d.f. is the
trivariate normal process
Ô£´
Ô£≠
y0
yf
ym
Ô£∂
Ô£∏‚àºN
Ô£´
Ô£¨
Ô£¨
Ô£≠
Ô£Æ
Ô£ØÔ£ØÔ£∞
¬µ
¬µ
¬µ
Ô£π
Ô£∫Ô£∫Ô£ª,
Ô£Æ
Ô£ØÔ£ØÔ£∞
œÉ2
œÉ2
a/2
œÉ2
a/2
œÉ2
a/2
œÉ2
0
œÉ2
a/2
0
œÉ2
Ô£π
Ô£∫Ô£∫Ô£ª
Ô£∂
Ô£∑
Ô£∑
Ô£∏,
where œÉ2 is the variance of the phenotypic values, and œÉ2
a is the additive
genetic variance for this trait in the population from which individuals are
sampled. The independence between parental records reÔ¨Çects the often-
made assumption that the two parents are not genetically related or mated
assortatively. The distribution [y0|yf, ym] is also normal, with expected
value,
E (y0|yf, ym) = ¬µ + 1
2h2 (yf ‚àí¬µ) + 1
2h2 (ym ‚àí¬µ)

2.3 Functions of Several Random Variables
105
and variance
V ar (y0|yf, ym) = œÉ2

1 ‚àíh4
2

,
where h2 = œÉ2
a/œÉ2. Consider now the random variables
ÀÜaf = h2 (yf ‚àí¬µ)
and
ÀÜam = h2 (ym ‚àí¬µ) .
These are the means of the conditional distribution of the father‚Äôs and
mother‚Äôs additive genetic values, respectively, given their phenotypic values.
The inverse transformations are yf = h‚àí2ÀÜaf + ¬µ and ym = h‚àí2ÀÜam + ¬µ,
respectively. Suppose that one wishes to derive the distribution [y0|ÀÜaf, ÀÜam].
According to (2.48), this distribution must be the same as [y0|yf, ym], where
we replace, in the latter, yf by h‚àí2ÀÜaf + ¬µ and ym by h‚àí2ÀÜam + ¬µ. That is,
[y0|ÀÜaf, ÀÜam] is normal, with expected value:
E (y0|ÀÜaf, ÀÜam) = ¬µ + 1
2ÀÜaf + 1
2ÀÜam
and variance as before.
‚ñ†
Example 2.19
The Box‚ÄìMuller transformation
The following technique was devised for generating standard normal ran-
dom variables (Box and Muller, 1958). Let U1 and U2 be independent ran-
dom variables having the same uniform distribution Un (0, 1) and construct
the transformation
X = f1 (U1, U2) = (‚àí2 ln U1)
1
2 cos (2œÄU2) ,
Y = f2 (U1, U2) = (‚àí2 ln U1)
1
2 sin (2œÄU2) .
It will be shown that X and Y are i.i.d. N (0, 1). The proof below makes
use of the following trigonometric relationships:
sin2 (x) + cos2 (x) = 1,
tan (x) = sin (x)
cos (x),
tan‚àí1 (tan x) = x,
d
dx tan‚àí1 (u) =
1
1 + u2
du
dx.
Using these relationships, the inverse transformations are obtained as
U1 = f ‚àí1
1
(X, Y ) = exp

‚àí1
2

X2 + Y 2
,
U2 = f ‚àí1
2
(X, Y ) = (2œÄ)‚àí1 tan‚àí1

 Y
X

.

106
2. Functions of Random Variables
Then, the joint p.d.f. of (X, Y ) is
pX,Y (x, y) = pU1U2

f ‚àí1
1
(X, Y ) , f ‚àí1
2
(X, Y )

|J| = |J| .
(2.49)
The Jacobian is given by
J = det
Ô£Æ
Ô£ØÔ£ØÔ£∞
‚àÇf ‚àí1
1
(X, Y )
‚àÇX
‚àÇf ‚àí1
1
(X, Y )
‚àÇY
‚àÇf ‚àí1
2
(X, Y )
‚àÇX
‚àÇf ‚àí1
2
(X, Y )
‚àÇY
Ô£π
Ô£∫Ô£∫Ô£ª
= det
Ô£Æ
Ô£ØÔ£ØÔ£∞
‚àíexp

‚àí1
2

X2 + Y 2
X
‚àíexp

‚àí1
2

X2 + Y 2
Y
‚àíY

2œÄ

1 + Y 2
X2

X2‚àí1

2œÄ

1 + Y 2
X2

X
‚àí1
Ô£π
Ô£∫Ô£∫Ô£ª
= ‚àí(2œÄ)‚àí1 exp

‚àí1
2

X2 + Y 2
.
(2.50)
Using the absolute value of this in (2.49) gives
pX,Y (x, y) =
‚àö
2œÄ
‚àí1
exp

‚àíX2
2
 ‚àö
2œÄ
‚àí1
exp

‚àíY 2
2

which can be recognized as the p.d.f. of two independent standard normal
variables.
‚ñ†
Example 2.20
Implied distribution of beliefs about heritability
Suppose a quantitative geneticist wishes to undertake a Bayesian analysis of
the heritability

h2
of a certain trait. Let there be two sources of variance
in the population, genetic and environmental, and let the values of the cor-
responding variance components be unknown. As pointed out earlier (and
see Chapter 5), the Bayesian approach requires the speciÔ¨Åcation of an un-
certainty distribution, the prior distribution, which is supposed to describe
beliefs about heritability before the data are observed. Let the positive ran-
dom variables Y and X denote the genetic and environmental components
of variance, respectively. Suppose that this geneticist uses proper uniform
distributions to represent vague prior knowledge about components of vari-
ance. In addition, the geneticist assumes that prior beliefs about X are
independent of those about Y , and takes as prior densities
p (x) = 1
a,
for 0 < x < a,
p (y) = 1
b ,
for 0 < y < b,
where a and b are the maximum values that X and Y , respectively, are
allowed to take. These upper bounds are perhaps established on the basis

2.3 Functions of Several Random Variables
107
of some knowledge of the population in question, or on mechanistic con-
siderations. Since X and Y are assumed to be independent, the joint prior
density of X and Y is simply the product of the two densities above. The
problem is to derive the induced prior p.d.f. of the heritability, denoted
here W and deÔ¨Åned as the ratio
W =
Y
Y + X .
It will be shown that uniform prior distributions for each of the two variance
components lead to discontinuous and perhaps sharp prior distributions of
W, depending on the values of a and b adopted. In order to make a one-
to-one transformation, deÔ¨Åne the auxiliary random variable U = Y . Thus,
the transformation can be written as
W = f1 (X, Y ) =
Y
X + Y ,
U = f2 (X, Y ) = f2 (Y ) = Y,
and the inverse transformation is
X = f ‚àí1
1
(U, W) = U (1 ‚àíW)
W
,
Y = f ‚àí1
2
(U, W) = f ‚àí1
2
(U) = U.
Then the joint p.d.f. of U and W is
p (u, w) = p

f ‚àí1
1
(u, w) , f ‚àí1
2
(u)

|J| ,
where
J = det
Ô£Æ
Ô£ØÔ£ØÔ£∞
‚àÇf ‚àí1
1
(u, w)
‚àÇu
‚àÇf ‚àí1
1
(u, w)
‚àÇw
‚àÇf ‚àí1
2
(u)
‚àÇu
‚àÇf ‚àí1
2
(u)
‚àÇw
Ô£π
Ô£∫Ô£∫Ô£ª
= det
Ô£Æ
Ô£∞
1‚àíw
w
‚àíu
w2
1
0
Ô£π
Ô£ª= u
w2 .
Therefore,
p (u, w) =
u
abw2 .
(2.51)
The next step involves Ô¨Ånding the support of this joint p.d.f. The relation-
ship U = Y implies
(i) 0 < u < b
and the relationship W = Y/ (Y + X) implies
(ii) 0 < w < 1.

108
2. Functions of Random Variables
Further, the fact that X = U (1 ‚àíW) /W implies
(iiia) 0 < u (1 ‚àíw)
w
< a,
(iiib) 0 < u (1 ‚àíw) < wa,
(iiic) 0 < u <
wa
1 ‚àíw.
From (i) and (iiic) the following inequalities must be satisÔ¨Åed:
(u < b)
and

u <
wa
1 ‚àíw

.
Since a and b are given, and u must be smaller than b, one must determine
the values of w such that wa/ (1 ‚àíw) is smaller than b. We have
wa
1 ‚àíw < b =‚áíwa < b (1 ‚àíw) =‚áíw <
b
b + a.
From all these relationships, the sample space of the joint distribution
[U, W], with density in (2.51), is
0 < w < 1,
0 < u <
wa
1 ‚àíw,
for 0 < w <
b
b + a,
0 < u < b,
for
b
b + a < w < 1.
Hence, p (u, w) is discontinuous at b/ (b + a). Verifying that p (u, w) in
(2.51) is a proper p.d.f. requires integrating the density over the range
of all possible values of U and W:
 
p (u, w) du dw
= 1
ab
b
a+b

w=0
wa
1‚àíw

u=0
u
w2 du dw + 1
ab
1

w=
b
a+b
b

u=0
u
w2 du dw
= 1
ab
ab
2 + ab
2

= 1,
so propriety is established. The marginal density of heritability (W) is
p (w) =
Ô£±
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£≤
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£≥
wa
1‚àíw

u=0
u
abw2 du,
for 0 < w <
b
a + b,
b

u=0
u
abw2 du,
for
b
a + b ‚â§w < 1.

2.3 Functions of Several Random Variables
109
0.2
0.4
0.6
0.8
1.0
0.5
1.0
1.5
2.0
2.5
a=1.0
b=0.25
a=1.0
b=0.5
a=1.0
b=1.0
a=1.0
b=4.0
a=1.0
b=2.0
h
2
p(h
2)
FIGURE 2.1. Prior distributions of heritability

h2 = W

implied by assuming
uniform prior distributions of the variance components, for diÔ¨Äerent bounds a
and b.
This yields
p (w) =
Ô£±
Ô£≤
Ô£≥
a
2b(1‚àíw)2 ,
for 0 < w <
b
a+b,
b
2aw2 ,
for
b
a+b ‚â§w < 1.
(2.52)
Figure 2.1 shows the implied prior distribution of heritability (W), given
the assumed prior distributions for the variance components, for diÔ¨Äerent
values of a and b. Five diÔ¨Äerent prior densities are depicted. It can be
seen that independent, proper, uniform distributions, for each of the two
variance components, induce a spiked prior density for heritability, with a
degree of sharpness or asymmetry that depends on the bounds a and b. ‚ñ†
Example 2.21
Revisited implied distribution of beliefs about heritability
Suppose now that the additive genetic (Y ) and the environmental (X) com-
ponents of variance follow independent inverse gamma (or scaled inverse
chi-square) distributions, a priori. It follows immediately that their recip-
rocals G = 1/Y and T = 1/X possess independent gamma distributions.
Note that heritability can be written as
W =
Y
X + Y =
T
T + G.
Once again, the geneticist wishes to derive the implied prior distribution
of W. The prior gamma densities of T and G are
p (t) =
bat
t
Œì (at)tat‚àí1 exp (‚àíbtt) ,
t, bt, at > 0,
p (g) =
bag
g
Œì (ag)gag‚àí1 exp (‚àíbgg) ,
g, bg, ag > 0,

110
2. Functions of Random Variables
where bt, at, bg, and ag are parameters of the appropriate distributions. In
view of the independence between T and G, their joint density is
p (t, g) = bat
t bag
g tat‚àí1gag‚àí1
Œì (at) Œì (ag)
exp (‚àíbtt ‚àíbgg) .
In order to arrive at the marginal distribution of W, make the one-to-one
transformation
W = f1 (T, G) =
T
T + G,
0 < W < 1,
U = f2 (T, G) = T + G,
U > 0,
where U is an auxiliary random variable. The inverse transformation is
T = f ‚àí1
1
(U, W) = UW,
G = f ‚àí1
2
(U, W) = U (1 ‚àíW) .
The joint p.d.f. of U and W is
pUW (u, w) = pT G

f ‚àí1
1
(U, W) , f ‚àí1
2
(U, W)

|J| ,
where
J = det
Ô£Æ
Ô£ØÔ£ØÔ£∞
‚àÇf ‚àí1
1
(u, w)
‚àÇu
‚àÇf ‚àí1
1
(u, w)
‚àÇw
‚àÇf ‚àí1
2
(u, w)
‚àÇu
‚àÇf ‚àí1
2
(u, w)
‚àÇw
Ô£π
Ô£∫Ô£∫Ô£ª
= det

w
u
1 ‚àíw
‚àíu

= ‚àíu.
Therefore
pUW (u, w) =
bat
t bag
g
Œì (ae) Œì (ag)u (uw)at‚àí1 [u (1 ‚àíw)]
ag‚àí1
√ó exp [‚àíbtuw ‚àíbgu (1 ‚àíw)] .
To arrive at the desired marginal distribution, one must integrate the pre-
ceding joint density with respect to U:
p (w) =
 ‚àû
0
pUW (u, w) du
=
bat
t bag
g
Œì (at) Œì (ag)wat‚àí1 (1 ‚àíw)
ag‚àí1
√ó
 ‚àû
0
uat+ag‚àí1 exp [‚àí(bg + btw ‚àíbgw) u] du.

2.3 Functions of Several Random Variables
111
Now the integrand is the kernel of the density of a gamma distribution, so
 ‚àû
0
uat+ag‚àí1 exp [‚àí(bg + btw ‚àíbgw) u] du
=

(bg + btw ‚àíbgw)at+ag
Œì (at + ag)
‚àí1
.
Hence, the prior density of heritability is
p (w) = Œì (at + ag) bat
t bag
g
Œì (at) Œì (ag)
wat‚àí1 (1 ‚àíw)
ag‚àí1
(bg + btw ‚àíbgw)‚àíat‚àíag .
(2.53)
The corresponding distribution does not have an easily recognizable form.
Consider now the special case where bg = bt; then (2.53) reduces to
p (w) = Œì (at + ag) wat‚àí1 (1 ‚àíw)ag‚àí1
Œì (at) Œì (ag)
.
(2.54)
This is the density of a beta distribution with parameters at, ag. In the hy-
pothetical Bayesian analysis of variance components, the parameters of the
gamma distribution assigned to the reciprocal of the variance components,
would be equal to those of the inverse gamma (or scaled inverse chi-square)
process assigned to the variance components. A typical speciÔ¨Åcation would
be at = ŒΩt/2, ag = ŒΩg/2, bt = ŒΩtSt/2, and bg = ŒΩgSg/2. Here, ŒΩ and S are
parameters of the scaled inverse chi-square distributions associated with
the variance components. The implied prior density of heritability would
be the beta form given by (2.54), provided that ŒΩtSt = ŒΩgSg; otherwise,
the implied prior density of heritability would be as in (2.53).
‚ñ†
2.3.1
Linear Transformations
A special case is when the transformation is linear, as in (2.43). Let x be a
random vector possessing p.d.f. p (x), and let y = f (x) = Ax be a one-to-
one linear transformation, so that the matrix of constants A is nonsingular.
The inverse transformation is, therefore
x = f ‚àí1 (y) = A‚àí1y.
From (2.31), the p.d.f. of Y is given by
pY (y) = pX

f ‚àí1 (y)

|J|
= pX

f ‚àí1 (y)
 """"det

‚àÇf ‚àí1 (y)
‚àÇy‚Ä≤
"""" .
Now, the matrix of derivatives is
‚àÇf ‚àí1 (y)
‚àÇy‚Ä≤
= ‚àÇA‚àí1y
‚àÇy‚Ä≤
= A‚àí1.

112
2. Functions of Random Variables
Hence
pY (y) = pX

f ‚àí1 (y)
 ""det

A‚àí1""
= pX

f ‚àí1 (y)
 """"
1
det (A)
"""" .
(2.55)
Example 2.22
Samples from the multivariate normal distribution
Imagine one wishes to obtain realizations from y ‚àºN(m, V). The starting
point consists of drawing a vector of independent standard normal deviates,
x, from a N(0, I) distribution having the same order (n, say) as y. Then
write x ‚àºN(0, I), so
p (x) =
1
(2œÄ)
n
2 exp

‚àíx‚Ä≤x
2

.
Because V is a nonsingular variance‚Äìcovariance matrix and, therefore, pos-
itive deÔ¨Ånite, it can be expressed as V = L‚Ä≤L, where L‚Ä≤ is a nonsingular,
lower triangular matrix; this is called the Cholesky decomposition. Then
V‚àí1 = L‚àí1 (L‚Ä≤)‚àí1 and, further, the following relationships can be estab-
lished
""V‚àí1"" =
""L‚àí1""2 ‚áí
""L‚àí1"" = |V|‚àí1
2 .
Now, the linear transformation
y = f(x) = m + L‚Ä≤x
has the desired distribution, by being a linear combination of the normal
vector x. To verify this, note that the inverse transformation is
x = f ‚àí1 (y) = (L‚Ä≤)‚àí1 (y ‚àím)
with the absolute value of the Jacobian of the transformation being
""""det

‚àÇf ‚àí1 (y)
‚àÇy‚Ä≤
"""" =
"""det (L‚Ä≤)‚àí1""" =
"""det (V)‚àí1
2
""" .
In the usual notation employed for the Gaussian model, we write
det (V)‚àí1
2 = |V|‚àí1
2 .
Then, applying (2.55), and recalling that the initial distribution is that of
n independent standard normal variables, gives, as p.d.f. of Y,
p (y) = (2œÄ)‚àín
2 |V|‚àí1
2 exp

‚àí1
2 (y ‚àím)‚Ä≤ L‚àí1 (L‚Ä≤)‚àí1 (y ‚àím)

which proves the result.
‚ñ†

2.3 Functions of Several Random Variables
113
Example 2.23
Bivariate normal distribution with null means
Consider the centered (null means) bivariate normal density function:
p (x, y) = C exp

‚àí
1
2 (1 ‚àíœÅ2)

 x2
œÉ2
X
‚àí2œÅxy
œÉXœÉY
+ y2
œÉ2
Y

,
(2.56)
where œÉ2
X = V ar(X), œÉ2
Y = V ar(Y ), œÅ is the coeÔ¨Écient of correlation
between X and Y, and C is the integration constant. Hereinafter, we will
retain only the kernel of this density. If œÅ = 0, the joint density can be
factorized as
p (x, y) = p (x) p (y) ,
where
p (x) ‚àùexp

‚àíx2
2œÉ2
X

,
p (y) ‚àùexp

‚àíy2
2œÉ2
Y

.
This veriÔ¨Åes that a null value of the correlation is a suÔ¨Écient condition for
independence in the bivariate normal distribution (as discussed in Chapter
1, a suÔ¨Écient condition for mutual independence in the multivariate distri-
bution is that the variance‚Äìcovariance matrix has a diagonal form).
Returning to the general case of nonnull correlation, now let U and V be
random variables arising through the linear transformation of X and Y

U
V

=
Ô£Æ
Ô£∞
X
œÉx ‚àíY
œÉy
X
œÉx + Y
œÉy
Ô£π
Ô£ª
=
 œÉ‚àí1
x
‚àíœÉ‚àí1
y
œÉ‚àí1
x
œÉ‚àí1
y
 
X
Y

= A

X
Y

,
where A is the coeÔ¨Écient matrix preceding the [X, Y ]‚Ä≤ vector. Hence, U is
a contrast between the standardized X and Y variables, whereas V is their
sum. Since U and V are linear combinations of normal random variables,
they must also follow a joint bivariate normal distribution, and are also
normal at the margin. It can be veriÔ¨Åed that the covariance between U and
V is equal to 0, so these two variates are independent. We proceed to verify
this formally. The inverse transformation is
 X
Y

=
 œÉ‚àí1
x
‚àíœÉ‚àí1
y
œÉ‚àí1
x
œÉ‚àí1
y
‚àí1  U
V

= 1
2

œÉx
œÉx
‚àíœÉy
œÉy
  U
V

.
Therefore,
X = f ‚àí1
1
(U, V ) = œÉx (U + V )
2
,
Y = f ‚àí1
2
(U, V ) = œÉy (V ‚àíU)
2
.

114
2. Functions of Random Variables
The absolute value of the Jacobian of the transformation is
|J| =
""""
1
det A
"""" = œÉxœÉy
2
.
Using (2.55) the joint p.d.f. of U and V is
p (u, v) = p

f ‚àí1
1
(u, v) , f ‚àí1
2
(u, v)
 œÉxœÉy
2
‚àùexp
#
‚àí
1
2 (1 ‚àíœÅ2)

u2 + v2 + 2uv

4
‚àí2œÅ

v2 ‚àíu2
4
+

u2 + v2 ‚àí2uv

4
$
‚àùexp

‚àí
1
4 (1 ‚àíœÅ2)

u2 + v2 ‚àíœÅ

v2 ‚àíu2
which can be factorized as
p (u, v) = C exp

‚àí
1
4 (1 ‚àíœÅ2)u2 (1 + œÅ)

√ó exp

‚àí
1
4 (1 ‚àíœÅ2)v2 (1 ‚àíœÅ)

.
Hence,
p (u, v) ‚àùp (u) p (v)
which shows that U and V are independent.
‚ñ†
2.3.2
Approximating the Mean and Covariance Matrix
Let x be a random vector having mean m and covariance matrix V, and
suppose one is interested in a scalar valued function, Y = f(x), of the
vector x. Assume that this function admits at least up to second-order
partial diÔ¨Äerentiability with respect to x. Put
 ‚àÇ
‚àÇx‚Ä≤ , f (x)
""""
x=m

= b‚Ä≤
and

‚àÇ2
‚àÇx‚àÇx‚Ä≤ f (x)
""""
x=m

= B.
Expanding f(x) in a second-order Taylor series about m gives
f (x) ‚àº= f (m) + b‚Ä≤ (x ‚àím)
+1
2 (x ‚àím)‚Ä≤ B (x ‚àím) .
(2.57)

2.3 Functions of Several Random Variables
115
Taking expectations and variances, one obtains the following approxima-
tions to E(Y ) and V ar (Y ):
(1) First order:
E (Y ) = E [f (x)] ‚àº= f [m] ,
(2.58)
V ar (Y ) = V ar [f (x)] ‚àº= b‚Ä≤Vb.
(2.59)
(2) Second order:
E (Y ) = E [f (x)] ‚àº= f (m) + 1
2 tr [BV] ,
(2.60)
V ar (Y ) = V ar [f (x)] ‚àº= b‚Ä≤Vb
+1
4 V ar [tr (SxB)] + Cov

b‚Ä≤ (x ‚àím) , (x ‚àím)‚Ä≤ B (x ‚àím)

,
(2.61)
where
Sx = (x ‚àím) (x ‚àím)‚Ä≤
is a matrix. The feasibility of evaluating the variance of the quadratic form
in (2.61) depends on the distribution of the vector x (Searle, 1971; Rao,
1973). For example, if its distribution is normal, the covariance term van-
ishes because the third moments from the mean are null.
Example 2.24
Approximate mean and variance of a ratio
Let x‚Ä≤ = [x1, x2], E (x)‚Ä≤ = m‚Ä≤ = [m1, m2], and
V ar (x) = V =

œÉ2
1
œÉ12
œÉ12
œÉ2
2

.
Consider the function
Y = f (x) = X1
X2
.
Then, from (2.58), the linear approximation gives as mean
E (Y ) = E

X1
X2

‚àº= m1
m2
.
The vector of Ô¨Årst derivatives is
b = ‚àÇf (x)
‚àÇx
""""
x=m
=
Ô£Æ
Ô£∞
‚àÇY
‚àÇX1
‚àÇY
‚àÇX2
Ô£π
Ô£ª
X1=m1
X2=m2
=
Ô£Æ
Ô£∞
1
m2
‚àím1
m2
2
Ô£π
Ô£ª.

116
2. Functions of Random Variables
Then, from (2.59)

‚àÇ
‚àÇX‚Ä≤ f (X)
""""
x=m

V

‚àÇ
‚àÇXf (X)
""""
X=m

=

1
m2
‚àím1
m2
2
 
œÉ2
1
œÉ12
œÉ12
œÉ2
2
 Ô£Æ
Ô£∞
1
m2
‚àím1
m2
2
Ô£π
Ô£ª
and, Ô¨Ånally,
V ar

X1
X2

‚àº=

m1
m2
2 
 œÉ2
1
m2
1
+ œÉ2
2
m2
2
‚àí2 œÉ12
m1m2

=

m1
m2
2 
C2
1 + C2
2 ‚àí2œÅC1C2

,
where œÅ is the coeÔ¨Écient of correlation and C1 and C2 are the corresponding
coeÔ¨Écients of variation.
‚ñ†
At this point, we have completed a review of the basic elements of dis-
tribution theory needed by geneticists to begin the study of parametric
methods of inference. In the next chapters, a discussion of two important
methods for drawing inferences is presented. Chapters 3 and 4 give a de-
scription of the principles of likelihood-based inference, whereas Chapters
5, 6, 7, and 8 present an overview of the Bayesian approach to statistical
analysis.

Part II
Methods of Inference
117

This page intentionally left blank

3
An Introduction to
Likelihood Inference
3.1
Introduction
A well-established problem in statistics, especially in what today is called
the classical approach, is the estimation of a single parameter or an en-
semble of parameters from a set of observations. Data are used to make
statements about a statistical model proposed to describe aspects of the
state of nature. This model is characterized in terms of parameters that
have a ‚Äútrue‚Äù value. The description of uncertainty, typically, is in terms
of a distribution that assigns probabilities or densities to the values that
all random variables in the model, including the observations, can take. In
general, the values of the parameters of this distribution are unknown and
must be inferred from observable quantities or data. In genetics, the observ-
ables can consist of phenotypic measurements for quantitative or discrete
traits, and/or information on molecular polymorphisms.
In this chapter, an important method of statistical inference called es-
timation by maximum likelihood (ML) is discussed. There is an extensive
literature on this method, originally due to Fisher (1922) and now Ô¨Årmly
entrenched in statistics. A historical account can be found in Edwards
(1974). A readable introduction is given by King (1989), and a discussion
of related concepts, from the point of view of animal breeding, is in Blasco
(2001). Edwards (1992), one of the strong adherents to the use of likelihood
inference in statistics, starts his book as follows:
‚ÄúLikelihood is the central concept in statistical inference.
Not only does it lead to inferential techniques in its own right,

120
3. An Introduction to Likelihood Inference
but it is as fundamental to the repeated-sampling theories of
estimation advanced by the ‚Äúclassical‚Äù statisticians as it is to
the probabilistic reasoning advanced by the Bayesian. It is the
key to an understanding of Ô¨Åducial probability, and the con-
cept without which no serious discussion of the philosophy of
statistical inference can begin.‚Äù
Although likelihood is indeed a central concept in statistical inference,
it is not free from controversy. In fact, all methods of inference are the
subject of some form of controversy. Mal¬¥ecot (1947) gives an interesting
critique of approaches based on likelihood, and Bernardo and Smith (1994)
discuss it from a Bayesian perspective, another approach to inference in
which parameters are viewed as random variables, with the randomness
stemming from subjective uncertainty. It will be shown in Chapter 5 how
likelihood enters into the Bayesian paradigm.
This chapter is organized as follows. The likelihood function and the
ML estimator are presented, including a measure of the information about
the parameters contained in the data. First-order asymptotic theory of
likelihood inference is covered subsequently in some detail. This gives the
basis for the appeal of the method, at least from a frequentist point of view.
The chapter ends with a discussion of the functional invariance of the ML
estimator.
3.2
The Likelihood Function
Assume that the observed data y (scalar, vector, or matrix) is the outcome
of a stochastic model (i.e., a random process), that can be characterized
in terms of a p.d.f. p (y|Œ∏) indexed by a parameter(s) Œ∏ taking values in
the interior of a parameter space ‚Ñ¶. (Hereinafter, unless it is clear from
the context, we use boldface for the parameters, to allow for the possibility
that Œ∏ may be a vector). For example, in a bivariate normal distribution,
the parameter space of the correlation coeÔ¨Écient includes all real numbers
from ‚àí1 to 1. This gives an illustration of a bounded parameter space. On
the other hand, in a regression model with p coeÔ¨Écients, each of which can
take any value in the real line, the parameter space is the p-dimensional
hypervolume Rp. There are situations where the parameter space may be
constrained, due to restrictions imposed by the model, or due to mechanis-
tic considerations. It was seen previously that, in a random eÔ¨Äects model
where the total variation is partitioned into between and within half-sib
family components, purely additive genetic action imposes the constraint
that the variance within families must be at least three times as large
as the variance between families. Without this constraint, the statistical
model would not be consistent with the biological system it is supposed
to describe. Hence, taking these constraints into account is important in

3.2 The Likelihood Function
121
likelihood-based inference. This is because one is interested in Ô¨Ånding the
maximum values the parameters can take inside their allowable space, given
the data, so constrained maximization techniques must be employed.
Given the probability model and the parameter Œ∏, the joint probability
density (or distribution, if the random variable under study is discrete) of
the observations, p (y|Œ∏), is a function of y. This describes the plausibility
of the diÔ¨Äerent values y can take in its sampling space, at a given value
of Œ∏. The likelihood function or, just likelihood, is based on an ‚Äúinversion‚Äù
of the preceding concept. By deÔ¨Ånition, the likelihood is any function of
Œ∏ that is proportional to p (y|Œ∏) ; it is denoted as L(Œ∏|y) or L(Œ∏). Thus,
the likelihood is a mathematical function of the parameter for Ô¨Åxed data,
whereas the p.d.f. is viewed as varying with y at Ô¨Åxed values of Œ∏. There-
fore, the likelihood is not a probability density or a probability function, so
the diÔ¨Äerent values Œ∏ takes in the likelihood cannot be interpreted in the
usual probabilistic sense. Further, because the true value of a parameter is
Ô¨Åxed, one cannot apply the probability calculus to a likelihood function,
at least in principle. Blasco (2001) pointed out that Fisher proposed the
likelihood function as a rational measure of degree of belief but without
sharing the properties of probability. The adherents of the likelihood ap-
proach to inference view the entire likelihood as a complete description of
the information about Œ∏ contained in the data, given the model.
Now, by deÔ¨Ånition
L(Œ∏|y) = k(y)p (y|Œ∏) ‚àùp (y|Œ∏) ,
(3.1)
where k(y) is a function that does not depend on Œ∏, but may depend on
the data. For Œ∏ = Œ∏‚àó, the value L(Œ∏‚àó|y) is called the likelihood of Œ∏‚àó. It
is apparent that a likelihood, by construction, must be positive, because
any density (or probability) is positive for any Œ∏ in the allowable space.
While a probability takes values between 0 and 1, a likelihood evaluated
at a given point has no speciÔ¨Åc meaning. On the other hand, it is mean-
ingful to compare the ratio of likelihoods from the same data. To be more
speciÔ¨Åc, consider a one-to-one transformation f (y) = z. For example, the
transformation could represent diÔ¨Äerent scales of measurement associated
with y and z, centimeters and meters respectively, say. Then the likelihood
function based on z is
L(Œ∏|z) = L(Œ∏|y)
""""
‚àÇy
‚àÇz
"""" .
Then for two possible values of the parameter, Œ∏ = Œ∏‚àóand Œ∏ = Œ∏‚àó‚àó, the
likelihood ratio is the relevant quantity to study
L(Œ∏‚àó|y)
L(Œ∏‚àó‚àó|y),

122
3. An Introduction to Likelihood Inference
as opposed to the diÔ¨Äerence
[L(Œ∏‚àó|y) ‚àíL(Œ∏‚àó‚àó|y)]
""""
‚àÇy
‚àÇz
"""" .
The latter is arbitrarily aÔ¨Äected by the particular transformation used, in
the above example, by the arbitrary scale of measurement.
The ratio of the likelihoods can be interpreted as a measure of support
brought up by the data set for one value of Œ∏ relative to the other. It
must be emphasized that likelihood values obtained from diÔ¨Äerent data
sets cannot be compared.
3.3
The Maximum Likelihood Estimator
Suppose for the moment that the random variable Y is discrete taking
values y with probabilities depending on a parameter Œ∏:
Pr (Y = y|Œ∏) = p (y|Œ∏) .
(3.2)
We said that the likelihood of Œ∏, L (Œ∏|y), is proportional to (3.2), and the
value of Œ∏ that maximizes the likelihood L (Œ∏|y) is the ML estimate of Œ∏; it
is denoted by 5Œ∏. The ML estimate 5Œ∏ can be viewed as the most likely value
of Œ∏ given the data, in the following sense. If Œ∏1 and Œ∏2 are two possible
values for Œ∏, and if Pr (Y = y|Œ∏1) > Pr (Y = y|Œ∏2), then the probability of
observing what was actually observed, i.e., Y = y, is greater when the true
value of Œ∏ is Œ∏1. Therefore it is more likely that Œ∏ = Œ∏1 than Œ∏ = Œ∏2. The
ML estimate provides the best explanation for observing the data point
y, under the probability model (3.2). This does not mean that 5Œ∏ is the
value of Œ∏ that maximizes the probability of observing the data y. This
role is played by the true value of the parameter Œ∏, in the sense discussed
in Subsection 3.7.1.
The above interpretation can be adapted to the case where Y is continu-
ously distributed, provided we view p (y|Œ∏) as the probability that Y takes
values in a small set containing y.
Often, rather than working with the likelihood it is more convenient to
work with the logarithm of the likelihood function. This log-likelihood is
denoted l(Œ∏|y). The maximizer of the likelihood is also the maximizer of
the log-likelihood. The value of the ML estimate of Œ∏ must be inside the
parameter space and must be a global maximum, in the sense that any
other value of the parameter would produce a smaller likelihood.
Fisher (1922) suggested that it would be meaningful to rescale all likeli-
hood values relative to the maximum value it can take for a speciÔ¨Åc data
set. For example, if the maximizer of L(Œ∏|y) is 5Œ∏, one could rescale values
as
r(Œ∏) = L(Œ∏|y)
L(5Œ∏|y)
,
(3.3)

3.3 The Maximum Likelihood Estimator
123
so that rescaled values are between 0 and 1.
Beyond providing a means of viewing uncertainty in terms of a relative
fall in likelihood, it turns out that the maximizer of the likelihood func-
tion plays an important role in statistical inference. If 5Œ∏ is used as a point
estimator of Œ∏, conceptual repeated sampling over the distribution of y
generates a distribution of 5Œ∏ values, one corresponding to each realization
of y. In fact, the sampling distribution of 5Œ∏ has interesting coverage proba-
bilities in relation to the true value of Œ∏. This distribution will be discussed
in a later section.
If the p.d.f. is everywhere continuous and there are no corner solutions,
the ML estimator, if it exists, can be found by diÔ¨Äerentiating the log-
likelihood with respect to Œ∏, setting all partial derivatives equal to zero,
and solving the resulting equations for Œ∏. It is a joint maximization with
respect to all elements in Œ∏ that must be achieved. If this parameter has
p elements, then there are p simultaneous equations to be solved, one for
each of its components. The vector of Ô¨Årst-order partial derivatives of the
log-likelihood with respect to each of the elements of Œ∏ is called the gradient
or score, and is often denoted as S (Œ∏|y) or as l‚Ä≤ (Œ∏|y).
There are some potential diÔ¨Éculties inherent to inferences based on like-
lihood. First, there is the issue of constructing the likelihood. For example,
consider generalized mixed eÔ¨Äects linear models (Nelder and Wedderburn,
1972; McCullagh and Nelder, 1989). These are models that can include
several sets of random eÔ¨Äects (u) and where the response variable (y) may
not be normal. The starting point in building such models is the speciÔ¨Å-
cation of the marginal distribution of u, followed by an assumption about
the form of the conditional distribution of y given u. This produces the
joint distribution [y, u|Œ∏] . In order to form the likelihood as in (3.1), one
must obtain the marginal p.d.f. of the data. This requires evaluating the
integral:
L(Œ∏|y) ‚àùp (y|Œ∏) =

p (y|u, Œ∏) p (u|Œ∏) du,
(3.4)
where p (y|u, Œ∏) is the density of the conditional distribution of the data
given the random eÔ¨Äects, and p (u|Œ∏) is the marginal density of the random
eÔ¨Äects. The integration in (3.4) seldom leads to a likelihood function that is
expressible in closed form. This creates diÔ¨Éculties in evaluating likelihoods
and in Ô¨Ånding the ML estimator of Œ∏.
A second issue is that it is often not straightforward to locate the global
maximum of a likelihood, especially in high-dimensional models (those hav-
ing many parameters). This is particularly troublesome in animal breeding
because of the potentially large number of parameters a model can have. For
example, consider a 17-variate analysis of type traits in dairy cattle, such
as scores on body condition and feet and legs, and suppose that the same
Gaussian linear model with two random factors is entertained for each trait.
These factors could be the breeding value of a cow plus a random residual.

124
3. An Introduction to Likelihood Inference
Here there are two covariance matrices, each having 153 potentially distinct
elements to be estimated, unless these matrices are structured as a function
of a smaller number of parameters. If, additionally, the number of location
parameters or Ô¨Åxed eÔ¨Äects is f, the order of Œ∏ would be f + 306. Typi-
cally, the order of f is in the thousands when Ô¨Åeld records are employed in
the analysis. In this situation, encountered often in practice, it would be
extremely diÔ¨Écult to verify that a global maximum has been found. The
zeros of the Ô¨Årst derivatives only locate extreme points in the interior of
the domain of a function. If extrema occur on the boundaries or corners,
the Ô¨Årst derivative may not be zero at that point. First derivatives can also
be null at local minima or maxima, or at inÔ¨Çection points. To ensure that
a maximum (local or global) has been found, the second derivative of the
log-likelihood with respect to the parameter must be negative, and this is
relatively easy to verify in a single parameter model. If Œ∏ is a vector, the
conditions for a maximum are:
(i) The vector of partial derivatives of l(Œ∏|y) with respect to Œ∏ must be
null.
(ii) The symmetric matrix of mixed second-order partial derivatives of
l(Œ∏|y) with respect to all parameters:
‚àÇ2l(Œ∏|y)
‚àÇŒ∏ ‚àÇŒ∏‚Ä≤
(3.5)
must be negative deÔ¨Ånite. A symmetric matrix is negative deÔ¨Ånite if all its
eigenvalues are negative. In the dairy cattle example given above, f + 306
eigenvalues would need to be computed to check this. Further, even if all
eigenroots of (3.5) are negative, there is no assurance that the stationary
point would be a global maximum.
The form of a likelihood function depends on the sampling model hypoth-
esized for the observations. Maximum likelihood is a parametric method, so
the distributional assumptions made are central. However, there is a certain
automatism in the calculations required: Ô¨Ånd Ô¨Årst and second derivatives;
if iterative methods are employed, iterate until convergence; verify that
a stationary point has been reached and attempt to ascertain that it is
a supremum. However, the chores of Ô¨Ånding the ML estimator of Œ∏, in a
model where normality is assumed, diÔ¨Äer from those in a model where, say,
Student-t distributions with unknown degrees of freedom are used.
Another important issue is the potential susceptibility of the ML esti-
mator of Œ∏ to small changes in the data. A slightly diÔ¨Äerent sample of
observations can produce very diÔ¨Äerent estimates when the likelihood is
Ô¨Çat in the neighborhood of the maximum. In theory, it is advisable to ex-
plore the likelihood as much as possible. However this can be complicated,
if not impossible, in multidimensional models. In the dairy cattle breeding
example, how would one represent a likelihood in f + 306 dimensions?
One of the main diÔ¨Éculties of ML is encountered in multiparameter
situations. The problem is caused by the existence of nuisance parame-

3.4 Likelihood Inference in a Gaussian Model
125
ters. For example, suppose that the multivariate normal sampling model
y ‚àºN (XŒ≤, V) is proposed, where Œ≤ is an unknown location vector to
be estimated, and V is a variance‚Äìcovariance matrix, also unknown. This
matrix is needed for a correct speciÔ¨Åcation of the model but it may not be
of primary interest. The ML procedure, at least in its original form, treats
all parameters identically, and does not account well for the possible loss
of information incurred in estimating V. An approach for dealing with this
shortcoming, at least in some models, will be discussed later.
A related problem is that of inferences about random eÔ¨Äects in linear
models (Henderson, 1963, 1973; Searle et al., 1992). In the notation em-
ployed in connection with (3.4), suppose that u is a vector of genetic eÔ¨Äects,
and that one wishes to make statements about the conditional distribution
[u|y, Œ∏], with unknown Œ∏ (e.g., Œ∏ may include Œ≤ and V). Intuitively, one
may wish to use the approximation [u|y, Œ∏ =5Œ∏], where 5Œ∏ is the ML estimate
of Œ∏. However, this does not take into account the fact that there is an error
of estimation associated with 5Œ∏, and it is not obvious why one should use
5Œ∏ as opposed to any other point estimate of Œ∏. Arguably, likelihood infer-
ence was developed for assessing the plausibility of values of Œ∏, and not for
making statements about unobservable random variables from conditional
distributions.
3.4
Likelihood Inference in a Gaussian Model
Suppose a single data point y = 10 is drawn from a normal distribution
with unknown mean m and known variance œÉ2 = 3. The likelihood resulting
from this single observation follows from the corresponding normal p.d.f.
L

m|œÉ2 = 3, y = 10

=
1
‚àö
6œÄ exp

‚àí(10 ‚àím)2
6

.
(3.6)
The function k(y) in (3.1) is 1/
‚àö
6œÄ, and the only part that matters (from
a likelihood inference viewpoint) is the exponential function in (3.6). This
is interpreted as a function of m for a given value of œÉ2 and of the observed
data point y = 10. A plot of the likelihood function using k(y) = 1/
‚àö
6œÄ is
in Figure 3.1.
The likelihood is symmetric about m = 10, its maximizer, as shown in
Figure 3.1. Likelihoods are not probability functions or density functions
and do not necessarily yield a Ô¨Ånite value (e.g., 1) when integrated with
respect to Œ∏. In this example, however, if (3.6) is integrated with respect
to m (which would be meaningless in a likelihood setting because m is not
stochastic), the value of the integral is precisely equal to one. At any rate,
the main issues in likelihood inference are the shape of the function and
the relative heights.

126
3. An Introduction to Likelihood Inference
5
10
15
20 m
L
FIGURE 3.1. Likelihood of m based on a sample of size 1.
Suppose now that four additional independent random samples are drawn
from the same distribution and that the new data set consists of the 5 √ó 1
vector
y‚Ä≤ = [y1 = 10, y2 = 8, y3 = 12, y4 = 9, y5 = 11] .
The likelihood is now built from the joint p.d.f. of the observations. This,
by virtue of the independence assumptions, is the product of Ô¨Åve normal
densities each with mean m and variance œÉ2 = 3. For this example, the
likelihood of m is
L

m|œÉ2 = 3, y

=
1
‚àö
6œÄ
5 exp

‚àí(10 ‚àím)2 + ¬∑ ¬∑ ¬∑ + (11 ‚àím)2
6

.
(3.7)
This can be written as
L

m|œÉ2 = 3, y

=
‚àö
6œÄ
‚àí5
exp

‚àí(10 ‚àí10)2 + ¬∑ ¬∑ ¬∑ + (11 ‚àí10)2
6

√ó exp

‚àí5 (10 ‚àím)2
6

‚àùk (y) exp

‚àí5 (10 ‚àím)2
6

,
where k(y) =
‚àö
6œÄ
‚àí5 exp
9
‚àí[(10 ‚àí10)2 + ¬∑ ¬∑ ¬∑ + (11 ‚àí10)2]/6
:
. The prod-
uct of the two exp(¬∑) functions arises because the term in the exponent in
(3.7) can be written as
5

i=1
(yi ‚àím)2 =
5

i=1
(yi ‚àíy)2 + 5 (y ‚àím)2 ,
(3.8)
where
y =
n
i=1
yi
5
.

3.4 Likelihood Inference in a Gaussian Model
127
5
10
15
20 m
L
FIGURE 3.2. Likelihood of m based on a sample of size 5.
The relevant part is that containing m. It follows that
L

m|œÉ2 = 3, y

‚àùexp

‚àí5 (y ‚àím)2
6

.
(3.9)
The relative values of the likelihood are the same irrespective of whether
(3.7) or (3.9) are used. The plot of the likelihood is shown in Figure 3.2.
Note that the likelihood is much sharper than that presented in Figure
3.1. The curvature of the likelihood at its maximum is larger with the larger
sample. This is because the likelihood of m based on a sample of size 5 has
more information about m than likelihood (3.6), a concept to be deÔ¨Åned
formally later in this chapter. In both data sets, the maximum value of the
likelihood is obtained when m = 10, so the ML estimate of this parameter
is 5m = 10 in the two situations. In practice, this would rarely happen,
but we have chosen the situation deliberately to compare the sharpness of
two likelihood functions having the same maximum. For example, if the
observed value in the Ô¨Årst data set had been y = 7.6, this would have been
the corresponding ML estimate of m.
With a sample of n independent observations, it follows from (3.8) that
the log-likelihood can be expressed as
l

m|œÉ2, y

= constant ‚àí
1
2œÉ2
 n

i=1
(yi ‚àíy)2 + n (y ‚àím)2

.
(3.10)
Maximizing (3.10) with respect to m is equivalent to minimizing the sum
of squared deviations n
i=1 (yi ‚àím)2 , so there is a relationship with the
least-squares criterion in this case. Taking the derivative of (3.10) with
respect to m gives the linear form in m :
dl

m|œÉ2, y

dm
= n (y ‚àím)
œÉ2
.
(3.11)

128
3. An Introduction to Likelihood Inference
Setting (3.11) to zero and solving for m gives, as ML of m,
5m =
n
i=1
yi
n
= y.
(3.12)
Consequently, the ML estimator of the expected value of a normal distribu-
tion with known variance is the arithmetic mean of the observations (the
estimator would have been the same if the variance had been unknown,
as shown later). It is also the least-squares estimator, but this coincides
with the ML estimator of m only when normality is assumed, as in this
example. DiÔ¨Äerentiating (3.11) with respect to m gives ‚àín/ œÉ2. Because
this derivative is negative, it follows that 5m is a maximum.
Suppose, temporarily, that m is a random variable in the real line, and
that œÉ2 is an unknown parameter. Now integrate (3.7) with respect to m
to obtain, for a sample of size n,

L

m|œÉ2, y

dm
=
1
(2œÄœÉ2) (n‚àí1)/2 ‚àön
exp

‚àí1
2œÉ2
n

i=1
(yi ‚àíy)2

.
(3.13)
which, given the data, is a function of œÉ2 only. (The limits of integration are
¬±‚àû, but this is generally omitted unless the context dictates otherwise).
Expression (3.13) is called an integrated or marginal likelihood, and has
the same form as the so-called restricted likelihood, proposed by Patterson
and Thompson (1971). In fact, anticipating a subject to be discussed later
in this book, when œÉ2 is an unknown parameter, maximization of (3.13)
with respect to œÉ2 yields
6
œÉ2 =
n
i=1
(yi ‚àíy)2
n ‚àí1
,
which is the restricted maximum likelihood estimator (REML) of œÉ2.
3.5
Fisher‚Äôs Information Measure
3.5.1
Single Parameter Case
Suppose the random vector y has a distribution indexed by a single pa-
rameter Œ∏, and let the corresponding density function be p (y|Œ∏). Fisher‚Äôs
information (also known as Fisher‚Äôs expected information, or expected in-
formation) about Œ∏, represented as I(Œ∏), is deÔ¨Åned to be
I(Œ∏) = Ey

 dl
dŒ∏
2
,
(3.14)

3.5 Fisher‚Äôs Information Measure
129
where the notation emphasizes that expectations are taken with respect to
the marginal distribution of y. Hereinafter, l = l (Œ∏) = l (Œ∏|y) will be used
indistinctly for the log-likelihood of Œ∏. This measure of information must
be positive because it involves an average of positive random variables, the
square of the Ô¨Årst derivatives of the log-likelihood with respect to Œ∏.
If the elements of y are drawn independently of each other, the log-
likelihood is expressible as
l = constant + ln p (y|Œ∏) = constant +
n

i=1
ln p(yi|Œ∏).
(3.15)
Hence, the amount of information in a sample of size n can be written as
I(Œ∏) =
n

i=1
Ii(Œ∏),
(3.16)
where
Ii(Œ∏) = E

d ln p(yi|Œ∏)
dŒ∏
2
and the expectation is taken over the marginal distribution of yi. If the ob-
servations have independent and identical distributions, the result in (3.16)
indicates that the amount of information in a sample of size n is exactly
n times larger than that contained in a single draw from the distribution.
This is because Ii(Œ∏) is constant from observation to observation in this
case.
Information accrues additively, irrespective of whether the observations
are distributed independently or not. In the general case, it can be veriÔ¨Åed
that, for a sample of size n,
I(Œ∏) = I1(Œ∏) + I2.1(Œ∏) + ... + Ii.i‚àí1,i‚àí2,...,1(Œ∏) + ... + In‚àí1.n‚àí2,...,1(Œ∏)
+ In.n‚àí1,n‚àí2,...,1(Œ∏),
(3.17)
where Ii.i‚àí1,i‚àí2,...,1(Œ∏) is the information contributed by a sample from the
distribution with density p(yi|yi‚àí1, yi‚àí2, ..., y2, y1, Œ∏). If the random vari-
ables are independent, the amount of information about Œ∏ is larger than
otherwise.
Example 3.1
Information in a sample of size two
Consider a sample of size 2 consisting of draws from independently dis-
tributed random variables X and Y with p.d.f. p (x|Œ∏) and g (y|Œ∏). From

130
3. An Introduction to Likelihood Inference
(3.14), the information about Œ∏ in the sample is
I (Œ∏) = E
 d
dŒ∏ ln p (x|Œ∏) + d
dŒ∏ ln g (y|Œ∏)
2
= E
 d
dŒ∏ ln p (x|Œ∏)
2
+ E
 d
dŒ∏ ln g (y|Œ∏)
2
+2 E
 d
dŒ∏ ln p (x|Œ∏)

E
 d
dŒ∏ ln g (y|Œ∏)

.
(3.18)
The last line follows because X and Y are independent, and therefore, the
expectation of the product is equal to the product of the expectations. As
will be shown below, the terms in the third line are equal to zero. Letting
IX (Œ∏) = E[ d
dŒ∏ ln p (x|Œ∏)]2, (3.18) shows that the information in the sample
is given by the sum of the information contributed by each sample point
I (Œ∏) = IX (Œ∏) + IY (Œ∏) .
‚ñ†
Example 3.2
Information under independent sampling from a normal
distribution
In the normal model with likelihood (3.10), the Ô¨Årst derivative is given in
(3.11). Upon squaring it, the term (y ‚àím)2 is encountered and this has
expectation œÉ2/n. Hence
I(Œ∏) = n2
œÉ4
œÉ2
n = n
œÉ2 ,
so the information is proportional to sample size.
‚ñ†
Example 3.3
Information with correlated draws from a normal distri-
bution
Two lactation milk yields are available from the same cow. Their distribu-
tion is identical, having the same unknown mean ¬µ and known variance œÉ2,
but there is a known correlation œÅ between records, induced by factors that
are common to all lactations of a cow. Here a bivariate normal sampling
model may be appropriate, in which case the joint density of the two yields
is
p(y|¬µ, œÉ2, œÅ) =
1
2œÄ
>
(1 ‚àíœÅ2) œÉ4 exp

‚àí
1
2 (1 ‚àíœÅ2) œÉ2 Q
%
,
where
Q =

(y1 ‚àí¬µ)2 + (y2 ‚àí¬µ)2 ‚àí2œÅ (y1 ‚àí¬µ) (y2 ‚àí¬µ)

.
With ¬µ being the only unknown parameter, the log-likelihood, apart from
a constant is
l

¬µ|œÉ2, œÅ, y

= ‚àí(y1 ‚àí¬µ)2 + (y2 ‚àí¬µ)2 ‚àí2œÅ (y1 ‚àí¬µ) (y2 ‚àí¬µ)
2 (1 ‚àíœÅ2) œÉ2
.

3.5 Fisher‚Äôs Information Measure
131
After taking derivatives with respect to ¬µ, squaring the result and taking
expectations, the information about ¬µ can be shown to be equal to
I(¬µ) =
2
(1 + œÅ)œÉ2
and this is smaller than the information under independence in Example
3.2 by a factor (1 + œÅ)‚àí1. When the correlation is perfect, the information
is equal to that obtained from a single sample drawn from N(¬µ, œÉ2).
‚ñ†
The deÔ¨Ånition of information given in (3.14) involves conceptual repeated
sampling, as the expectation operator indicates averaging with respect to
the distribution [y|Œ∏] or, equivalently, over all possible values that the ran-
dom vector y can take at a Ô¨Åxed, albeit unknown, value of Œ∏. This suggests
that a more precise term is expected information, to make a distinction with
the observed information resulting from a single realization of the random
process. Hence, the observed information is (dl/dŒ∏)2. The observed infor-
mation is a function both of y and Œ∏, whereas the expected information is
a function of Œ∏ only.
The observed information represents the curvature (see Example 3.4 be-
low) of the observed log-likelihood for the given data y, whereas the ex-
pected information is an average curvature over realizations of y. In this
sense, the observed information is to be preferred (Efron and Hinkley, 1978).
Typically, because the true value of the parameter is unknown, expected
and observed information are approximated by replacing Œ∏ with the max-
imum likelihood estimate. The observed information evaluated at 5Œ∏ repre-
sents the curvature at 5Œ∏; a large curvature is associated with a strong peak,
intuitively indicating less uncertainty about Œ∏. In Example 3.2 the expected
information was found to be n/œÉ2, whereas the observed information is the
square of (3.11). Note that the expected information does not depend on
the unknown m, whereas the observed information involves the two pa-
rameters of the model

m, œÉ2
plus the sample average y, which is the
ML estimator in this case. Often, the expected information is algebraically
simpler than the observed information.
A word of warning is particularly apposite here. Although one speaks of
information about a parameter contained in a data point, the concept is
mainly justiÔ¨Åed by the fact that, in many cases, under regularity conditions
to be speciÔ¨Åed below, the inverse of the information is the smallest asymp-
totic variance obtainable by an estimator. In other words, the concept of
information has an asymptotic justiÔ¨Åcation and is meaningful provided reg-
ularity conditions are satisÔ¨Åed (Lehmann, 1999).
3.5.2
Alternative Representation of Information
Recall that l = ln p(y|Œ∏), and let l‚Ä≤ and l‚Ä≤‚Ä≤ denote the Ô¨Årst and second
derivatives of the log-likelihood with respect to Œ∏. Here it is shown that

132
3. An Introduction to Likelihood Inference
Fisher‚Äôs expected information can also be expressed as
I (Œ∏) = ‚àíEy (l‚Ä≤‚Ä≤)
= ‚àí

l‚Ä≤‚Ä≤p(y|Œ∏) dy
(3.19)
and, thus,
I (Œ∏) = Ey (l‚Ä≤)2
= Ey (‚àíl‚Ä≤‚Ä≤) .
Since p(y|Œ∏) is a density function, it follows that

p(y|Œ∏) dy = 1.
Therefore,
d
dŒ∏

p(y|Œ∏) dy = 0.
If the derivative can be taken inside the integral sign, it follows that
0 =

d
dŒ∏p(y|Œ∏) dy
=

l‚Ä≤p(y|Œ∏) dy
= Ey (l‚Ä≤) .
(3.20)
Thus, the expected value of the score is equal to zero. If a second diÔ¨Äeren-
tiation also can be taken under the integral sign, then we have
0 =

d
dŒ∏l‚Ä≤p(y|Œ∏) dy
=

l‚Ä≤‚Ä≤p(y|Œ∏) dy+

(l‚Ä≤)2 p(y|Œ∏) dy
leading directly to (3.19).
Example 3.4
Curvature as a measure of information
Another way of visualizing information derives from the deÔ¨Ånition of cur-
vature at a given point of the log-likelihood function l (Œ∏). The curvature
at the point Œ∏ is (Stein, 1977)
c (Œ∏) =
l‚Ä≤‚Ä≤ (Œ∏)

1 + l‚Ä≤ (Œ∏)2 3
2 .

3.5 Fisher‚Äôs Information Measure
133
Since l‚Ä≤ 
5Œ∏

= 0, the curvature at the maximum of the function is given by
the second derivative
c

5Œ∏

= l‚Ä≤‚Ä≤ 
5Œ∏

.
When the curvature is large, the sample of data points clearly toward the
value 5Œ∏. On the other hand, if the curvature is small, there is ambiguity since
a range of values of Œ∏ leads to almost the same value of the likelihood.
‚ñ†
Example 3.5
A quadratic approximation to the log-likelihood
Consider a second order Taylor series expansion of l (Œ∏) = ln L (Œ∏|y) around
5Œ∏
l (Œ∏) ‚âàl

5Œ∏

+ l‚Ä≤ 
5Œ∏
 
Œ∏ ‚àí5Œ∏

+ 1
2l‚Ä≤‚Ä≤ 
5Œ∏
 
Œ∏ ‚àí5Œ∏
2
= l

5Œ∏

‚àí1
2J

5Œ∏
 
Œ∏ ‚àí5Œ∏
2
,
where J

5Œ∏

= ‚àíl‚Ä≤‚Ä≤ 
5Œ∏

is the observed information. Therefore,
L (Œ∏|y)
L

5Œ∏|y
 ‚âàexp

‚àí1
2J

5Œ∏
 
Œ∏ ‚àí5Œ∏
2
.
(3.21)
It is important to note that in (3.21), Œ∏ is a Ô¨Åxed parameter, and 5Œ∏ varies
in conceptual replications.
In Subsection 3.7.3, it will be shown that under regularity conditions, a
slight variant of the following asymptotic approximation can be established
5Œ∏ ‚àºN

Œ∏, J

5Œ∏
‚àí1
,
which means that, approximately,
p

5Œ∏

‚âà(2œÄ)‚àí1
2
"""J

5Œ∏
"""
1
2 exp

‚àí1
2J

5Œ∏
 
Œ∏ ‚àí5Œ∏
2
.
(3.22)
Again, this is an approximation to the sampling density of 5Œ∏, with Œ∏ Ô¨Åxed.
Using (3.21) in (3.22), we obtain
p

5Œ∏

‚âà(2œÄ)‚àí1
2
"""J

5Œ∏
"""
1
2 L (Œ∏|y)
L

5Œ∏|y
,
(3.23)
which is a more accurate approximation than (3.22). A slightly modiÔ¨Åed
version of this formula due to BarndorÔ¨Ä-Nielsen (1983) is so precise that
Efron (1998) refers to it as the ‚Äúmagic formula‚Äù.
‚ñ†

134
3. An Introduction to Likelihood Inference
3.5.3
Mean and Variance of the Score Function
The mean and variance of the score function are needed to establish some
properties of the ML estimator, as discussed later. The score function in a
single parameter model is S (Œ∏|y) = l‚Ä≤. As seen in connection with (3.11),
the score is a function of the data, so it must be a random variable. Usually,
the distribution of the score is unknown, although in (3.11) it possesses a
normal distribution, by virtue of being a linear function of y, a normally
distributed vector. In (3.20), it was shown that the score has zero expec-
tation. Here we show that the variance of the score is equal to Fisher‚Äôs
expected information. Thus
l‚Ä≤ = S (Œ∏|y) ‚àº[0, I (Œ∏)] .
(3.24)
The variance of the score, by deÔ¨Ånition, is
V ar(l‚Ä≤) = E (l‚Ä≤)2 ‚àíE2 (l‚Ä≤) = E (l‚Ä≤)2 = I (Œ∏)
(3.25)
which follows from the deÔ¨Ånition of information as given in (3.14) and
(3.19). Note, from the derivation in (3.20), that it is assumed that the
derivative can be moved inside of the integral sign.
Example 3.6
Mean and variance of the score function in the normal
distribution
When sampling n observations independently from a normal distribution
with mean ¬µ and variance œÉ2, the score, as given in (3.11), is
dl

¬µ|œÉ2, y

d¬µ
= n (y ‚àí¬µ)
œÉ2
.
Consequently the score is a function of the data, as stated before. For any
particular sample, it can be positive or negative, depending on the value
of y ‚àí¬µ. However, over an inÔ¨Ånite number of samples drawn from the
distribution of y, its average value is
Ey
n (y ‚àí¬µ)
œÉ2

= n
œÉ2 Ey [(y ‚àí¬µ)] = 0
and it has variance
V ar
n (y ‚àí¬µ)
œÉ2

= n
œÉ2 .
This is precisely the information about ¬µ, as found in Example 3.2. This
completes the veriÔ¨Åcation that the distribution of the score has parameters
as in (3.24). In addition, this distribution is normal in the present example,
as pointed out previously.
‚ñ†

3.5 Fisher‚Äôs Information Measure
135
3.5.4
Multiparameter Case
The results given above generalize to a vector of parameters Œ∏ in a straight-
forward manner. The expected information matrix is deÔ¨Åned to be:
I (Œ∏) = Ey

 ‚àÇl
‚àÇŒ∏
 
 ‚àÇl
‚àÇŒ∏
‚Ä≤
= ‚àíEy

‚àÇ2l
‚àÇŒ∏ ‚àÇŒ∏‚Ä≤

,
(3.26)
where l = ln (Œ∏|y) is now a function of a vector-valued parameter, ‚àÇl/‚àÇŒ∏
is a vector of Ô¨Årst partial derivatives of the log-likelihood with respect to
each of the elements of Œ∏, and ‚àÇ2l/‚àÇŒ∏ ‚àÇŒ∏‚Ä≤ is the matrix (whose dimension
is equal to the number of elements in Œ∏) of second derivatives of the log-
likelihood with respect to the parameters. The equivalent of (3.24) is that
now there is a score vector S (Œ∏|y) having the multivariate distribution
S (Œ∏|y) ‚àº[0, I (Œ∏)] .
(3.27)
The concept of information is more subtle in the multiparameter than in
the single parameter case, unless the information matrix is diagonal. In
general, in a multiparameter model, the ith diagonal element of I (Œ∏) cannot
be interpreted literally as information about the ith element of Œ∏. Often,
reference needs to be made to the entire information matrix.
To illustrate, consider the sampling model y ‚àºN (XŒ≤, V) where Œ≤ is
unknown, X is a known matrix of explanatory variables and V is a non-
singular variance‚Äìcovariance matrix, assumed known. The likelihood func-
tion is expressible as
L (Œ≤|V, y) ‚àùexp

‚àí1
2 (y ‚àíXŒ≤)‚Ä≤ V‚àí1 (y ‚àíXŒ≤)

.
(3.28)
The score vector is
S (Œ≤|y) = ‚àÇ

‚àí1
2 (y ‚àíXŒ≤)‚Ä≤ V‚àí1 (y ‚àíXŒ≤)

‚àÇŒ≤
= X‚Ä≤V‚àí1 (y ‚àíXŒ≤) .
(3.29)
The observed information matrix is equal to the expected information ma-
trix in this case, because the second derivatives do not involve y. Here
I (Œ≤) = E

‚àí
‚àÇ2l
‚àÇŒ≤ ‚àÇŒ≤‚Ä≤

= E

‚àí‚àÇS (Œ≤|y)
‚àÇŒ≤‚Ä≤

= E

X‚Ä≤V‚àí1X

= X‚Ä≤V‚àí1X.
(3.30)
From (3.26) and (3.29), the information matrix can also be calculated as
I (Œ≤) = E

 ‚àÇl
‚àÇŒ≤
 
 ‚àÇl
‚àÇŒ≤
‚Ä≤
= E

X‚Ä≤V‚àí1 (y ‚àíXŒ≤) (y ‚àíXŒ≤)‚Ä≤ V‚àí1X

= X‚Ä≤V‚àí1E

(y ‚àíXŒ≤) (y ‚àíXŒ≤)‚Ä≤
V‚àí1X
= X‚Ä≤V‚àí1VV‚àí1X = X‚Ä≤V‚àí1X.

136
3. An Introduction to Likelihood Inference
The preceding result follows because the variance‚Äìcovariance matrix of y,
by deÔ¨Ånition, is V = E

(y ‚àíXŒ≤) (y ‚àíXŒ≤)‚Ä≤
. Also, note from (3.29) that
E [S (Œ≤|y)] = X‚Ä≤V‚àí1E (y ‚àíXŒ≤) = 0
and that
V ar [S (Œ≤|y)] = X‚Ä≤V‚àí1 [V ar (y ‚àíXŒ≤)] V‚àí1X‚Ä≤
= X‚Ä≤V‚àí1 V ar (y) V‚àí1X‚Ä≤ = X‚Ä≤V‚àí1X.
This veriÔ¨Åes that the score has a distribution with mean 0 and a variance‚Äì
covariance matrix equal to the expected information. Here, this distribution
is multivariate normal, with dimension equal to the number of elements in
Œ≤. This is because the score vector in (3.29) is a linear transformation of
the data vector y.
Example 3.7
Linear regression
In a simple linear regression model, it is postulated that the observations are
linked to an intercept Œ≤0 and to a slope parameter Œ≤1 via the relationship
yi = Œ≤0 + Œ≤1xi + ei,
where xi (i = 1, 2, ..., n) are known values of an explanatory variable and
ei ‚àºN

0, œÉ2
is a residual. The distributions of the n residuals are as-
sumed to be mutually independent. The parameter vector is [Œ≤0, Œ≤1, œÉ2].
The likelihood function can be written as
L

Œ≤0, Œ≤1, œÉ2|y

‚àù

œÉ2‚àín
2 exp

‚àí1
2œÉ2
n

i=1
(yi ‚àíŒ≤0 ‚àíŒ≤1xi)2

and the corresponding log-likelihood, apart from an additive constant, is
l

Œ≤0, Œ≤1, œÉ2|y

= ‚àín
2 ln

œÉ2
‚àí
1
2œÉ2
n

i=1
(yi ‚àíŒ≤0 ‚àíŒ≤1xi)2 .
The score vector is given by
Ô£Æ
Ô£ØÔ£ØÔ£∞
‚àÇl
‚àÇŒ≤0
‚àÇl
‚àÇŒ≤1
‚àÇl
‚àÇœÉ2
Ô£π
Ô£∫Ô£∫Ô£ª=
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
1
œÉ2
n

i=1
(yi ‚àíŒ≤0 ‚àíŒ≤1xi)
1
œÉ2
n

i=1
xi (yi ‚àíŒ≤0 ‚àíŒ≤1xi)
‚àín
2œÉ2 +
1
2œÉ4
n

i=1
(yi ‚àíŒ≤0 ‚àíŒ≤1xi)2
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
.
Setting the score vector to zero and solving simultaneously for the unknown
parameters gives explicit solutions to the ML equations. The ML estimators

3.5 Fisher‚Äôs Information Measure
137
are
5œÉ2 =
n
i=1
(yi ‚àí5Œ≤0 ‚àí5Œ≤1xi)2
n
,
where 5Œ≤0 and 5Œ≤1 are the solutions to the matrix equation
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞
n
n

i=1
xi
n

i=1
xi
n

i=1
x2
i
Ô£π
Ô£∫Ô£∫Ô£∫Ô£ª

5Œ≤0
5Œ≤1

=
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞
n

i=1
yi
n

i=1
xiyi
Ô£π
Ô£∫Ô£∫Ô£∫Ô£ª.
Explicitly,
5Œ≤0 = y ‚àí5Œ≤1x
and
5Œ≤1 =
n
i=1
xiyi ‚àí1
n
n
i=1
xi
n
i=1
yi
n
i=1
x2
i ‚àí1
n

 n
i=1
xi
2 .
The Ô¨Årst two columns of the 3 √ó 3 matrix of negative second derivatives of
the log-likelihood with respect to the parameters, or observed information
matrix, are
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
œÉ‚àí2n
œÉ‚àí2
n

i=1
xi
œÉ‚àí2
n

i=1
xi
œÉ‚àí2
n

i=1
x2
i
œÉ‚àí4
n

i=1
(yi ‚àíŒ≤0 ‚àíŒ≤1xi)
œÉ‚àí4
n

i=1
xi (yi ‚àíŒ≤0 ‚àíŒ≤1xi)
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
and the last column
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
œÉ‚àí4
n

i=1
(yi ‚àíŒ≤0 ‚àíŒ≤1xi)
œÉ‚àí4
n

i=1
xi (yi ‚àíŒ≤0 ‚àíŒ≤1xi)
‚àí

2œÉ4‚àí1 n + œÉ‚àí6
n

i=1
(yi ‚àíŒ≤0 ‚àíŒ≤1xi)2
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
.
It is easy to verify that the expected value of each of the elements of the
score vector is null. For example,
E

 ‚àÇl
‚àÇŒ≤1

= 1
œÉ2
n

i=1
xiE (yi ‚àíŒ≤0 ‚àíŒ≤1xi) = 0

138
3. An Introduction to Likelihood Inference
and
E

 ‚àÇl
‚àÇœÉ2

= ‚àín
2œÉ2 +
1
2œÉ4
n

i=1
E (yi ‚àíŒ≤0 ‚àíŒ≤1xi)2
= ‚àín
2œÉ2 +
1
2œÉ4 nœÉ2 = 0.
Further, the expected information matrix is given by
I (Œ∏) = œÉ‚àí2
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
n
n

i=1
xi
0
n

i=1
xi
n

i=1
x2
i
0
0
0

2œÉ2‚àí1 n
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
.
Note that the elements (1, 2) and (2, 1) of this matrix are not null. This
illustrates that in a multiparameter model it is often more sensible to speak
about joint information on a set of parameters, rather than about informa-
tion on individual parameters themselves. Also, observe that the elements
(1, 3) , (3, 1) , (2, 3) , and (3, 2) are null. This has a connection with the
distribution of the ML estimator of Œ∏, as discussed later.
‚ñ†
3.5.5
Cram¬¥er‚ÄìRao Lower Bound
The concept of expected information can be used to determine a lower
bound for the variance of an estimator of the scalar parameter Œ∏. Let 5Œ∏ =
T (y) be a function of a sample y drawn from the distribution [y|Œ∏] ; this
function, usually called a statistic, is employed for estimating Œ∏. Also, let
E

5Œ∏

= m (Œ∏) ,
where m (Œ∏) is a function of Œ∏; for example, m can be the identity operator.
If m (Œ∏) = Œ∏, the estimator is said to be unbiased for Œ∏. The Cram¬¥er‚ÄìRao
inequality (e.g., Casella and Berger, 1990) states that
V ar

5Œ∏

‚â•[m‚Ä≤ (Œ∏)]2
I (Œ∏)
,
(3.31)
where m‚Ä≤ (Œ∏) = dm(Œ∏)/dŒ∏, assuming m (Œ∏) is diÔ¨Äerentiable. If the variance
of 5Œ∏ attains the right-hand side of (3.31), then the estimator is said to have
minimum variance.
A particular case is when 5Œ∏ is unbiased, that is, when E(5Œ∏) = Œ∏. Here, the
derivative of m (Œ∏) = Œ∏ with respect to Œ∏ is equal to 1. Then the Cram¬¥er‚Äì
Rao lower bound for the variance of an unbiased estimator is
V ar

5Œ∏

‚â•[I (Œ∏)]‚àí1 .
(3.32)

3.5 Fisher‚Äôs Information Measure
139
This states that an unbiased estimator cannot have a variance that is lower
than the inverse of Fisher‚Äôs information measure. If
V ar

5Œ∏

= [I (Œ∏)]‚àí1 ,
then 5Œ∏ is said to be a minimum variance unbiased estimator.
In general, as in (3.31), the lower bound depends on the expectation of
the estimator and on the distribution of the observations, because I (Œ∏)
depends on p (y|Œ∏) . In order to prove (3.31), use is made of the Cauchy‚Äì
Schwarz inequality (Stuart and Ord, 1991), as given below.
Cauchy‚ÄìSchwarz Inequality
Let u and v be any two random variables and let c be any constant. Then
(u ‚àícv)2 is positive or null, and so is its expectation. Hence
E (u ‚àícv)2 = E

u2
+ c2E

v2
‚àí2cE (uv) ‚â•0.
In this expression, arbitrarily choose
c = E (uv)
E (v2)
to obtain
E

u2
+ E2 (uv)
E (v2) ‚àí2E2 (uv)
E (v2) ‚â•0.
This leads directly to the Cauchy‚ÄìSchwarz inequality
E2 (uv) ‚â§E

u2
E

v2
.
(3.33)
If the random variables are expressed as deviations from their expectations,
the above implies that
Cov2 (u, v) ‚â§V ar (u) V ar (v) .
(3.34)
In addition, if these deviations are measured in standard deviation units of
the corresponding variate, (3.34) implies that
Corr2 (u, v) ‚â§1,
(3.35)
where Corr(¬∑) denotes the coeÔ¨Écient of correlation.
‚ñ†

140
3. An Introduction to Likelihood Inference
Now let u = 5Œ∏, and v = l‚Ä≤, the score function. It has been established
already that E (l‚Ä≤) = 0 and V ar (l‚Ä≤) = I (Œ∏) . Hence
Cov

5Œ∏, l‚Ä≤
=
 
5Œ∏ d ln p (y|Œ∏)
dŒ∏

p (y|Œ∏) dy
=

5Œ∏ dp (y|Œ∏)
dŒ∏
dy
= d
dŒ∏

5Œ∏ p (y|Œ∏) dy
= d
dŒ∏E

5Œ∏

= d
dŒ∏m (Œ∏) = m‚Ä≤ (Œ∏) .
(3.36)
Applying the Cauchy‚ÄìSchwarz inequality, as in (3.34),
Cov2 
5Œ∏, l‚Ä≤
= [m‚Ä≤ (Œ∏)]2 ‚â§V ar

5Œ∏

I (Œ∏) .
Rearrangement of this expression leads directly to the Cram¬¥er‚ÄìRao lower
bound given in (3.31). Note that the proof requires interchange of inte-
gration and diÔ¨Äerentiation, as seen in connection with (3.36). There are
situations in which it is not possible to do this; typically, when the limits
of integration depend on the parameter. An example is provided by the
uniform distribution
X ‚àºUn (0, Œ∏) =

1
Œ∏,
for 0 ‚â§x ‚â§Œ∏,
0,
otherwise.
Integration with respect to p (x|Œ∏) is over the range 0 ‚â§x ‚â§Œ∏, which
includes Œ∏.
Example 3.8
Cram¬¥er‚ÄìRao lower bound in the linear regression model
Return to the linear regression Example 3.7, and consider Ô¨Ånding the
Cram¬¥er‚ÄìRao lower bound for an estimator of the variance. It was seen that
I

œÉ2
= n/2œÉ4. Hence, any unbiased estimator of the variance (5v, say)
must be such that
V ar (5v) ‚â•2œÉ4
n .
The ML estimator of œÉ2 for this example can be written as
6
œÉ2 =
n
i=1

yi ‚àí5Œ≤0 ‚àí5Œ≤1xi
2
n
=
n
i=1

yi ‚àí5Œ≤0 ‚àí5Œ≤1xi
2
œÉ2
œÉ2
n
= œá2
n‚àí2
œÉ2
n ,

3.5 Fisher‚Äôs Information Measure
141
because the residual sum of squares
n

i=1

yi ‚àí5Œ≤0 ‚àí5Œ≤1xi
2
‚àºœá2
n‚àí2œÉ2
has a scaled chi-square distribution with n ‚àí2 degrees of freedom. The
expectation of the estimator is
E

5œÉ2
= œÉ2 n ‚àí2
n
,
so the estimator has a downward bias. It can readily be veriÔ¨Åed that the
variance of the ML estimator of œÉ2 satisÔ¨Åes the Cram¬¥er‚ÄìRao lower bound
given by (3.31) and equal to 2œÉ4 (n ‚àí2) /n2. An unbiased estimator is
,œÉ2 = 5œÉ2
n
n ‚àí2 = œá2
n‚àí2
œÉ2
n ‚àí2,
with variance
V ar

,œÉ2
= 2œÉ4
n ‚àí2.
Hence, this unbiased estimator does not attain the Cram¬¥er‚ÄìRao lower
bound for 5v given above.
‚ñ†
This example suggests that it would be useful to Ô¨Ånd a condition under
which an unbiased estimator 5Œ∏ reaches the lower bound. Suppose that the
score can be written as a linear function of 5Œ∏ as follows:
l‚Ä≤ = a (Œ∏)

5Œ∏ ‚àíŒ∏

,
(3.37)
where a (Œ∏) is some constant that does not involve the observations. Then,
Cov

5Œ∏, l‚Ä≤
=

5Œ∏ a (Œ∏)

5Œ∏ ‚àíŒ∏

p (y|Œ∏) dy
= a (Œ∏) V ar

5Œ∏

,
and
V ar (l‚Ä≤) = I (Œ∏) = a2 (Œ∏) V ar

5Œ∏

.
The Cram¬¥er‚ÄìRao inequality in (3.31), in view of (3.37), can be written as
Cov2 
5Œ∏, l‚Ä≤
= a2 (Œ∏) V ar2 
5Œ∏

‚â§V ar

5Œ∏

I (Œ∏)
= V ar

5Œ∏

a2 (Œ∏) V ar

5Œ∏

.
The inequality becomes an equality, so the Cram¬¥er‚ÄìRao lower bound is
attained automatically for an unbiased estimator 5Œ∏ provided the score can

142
3. An Introduction to Likelihood Inference
be written as in (3.37). For example, in a normal sampling process with
unknown mean and known variance, the score is, as given in (3.11),
dl

¬µ|œÉ2, y

d¬µ
= n (y ‚àí¬µ)
œÉ2
.
Because this has the required form, with Œ∏ = ¬µ, a (Œ∏) = n/œÉ2 and 5Œ∏ = y, it
follows that the sample mean is the minimum variance unbiased estimator
of ¬µ.
In a multiparameter situation, the condition for an unbiased estimator
to attain the lower bound is written as
l‚Ä≤ = A (Œ∏)

5Œ∏ ‚àíŒ∏

,
(3.38)
where l is the score vector and A (Œ∏) is a matrix that may be a function
of the parameters, but not of the data. For example, under the sampling
model y ‚àºN (XŒ≤, V) , with known variance‚Äìcovariance matrix, it was seen
in (3.29) that
l‚Ä≤ = S (Œ≤|y) = X‚Ä≤V‚àí1 (y ‚àíXŒ≤) ,
and if

X‚Ä≤V‚àí1X
‚àí1 exists, the score can be written as
S (Œ≤|y) = X‚Ä≤V‚àí1X

X‚Ä≤V‚àí1X
‚àí1 X‚Ä≤V‚àí1y ‚àíŒ≤

,
which is in the form of (3.38), with Œ∏ = Œ≤, 5Œ∏ =

X‚Ä≤V‚àí1X
‚àí1 X‚Ä≤V‚àí1y,
and A (Œ∏) =

X‚Ä≤V‚àí1X

. Hence,

X‚Ä≤V‚àí1X
‚àí1 X‚Ä≤V‚àí1y is the minimum
variance unbiased estimator of Œ≤.
3.6
SuÔ¨Éciency
The concept of suÔ¨Éciency was developed by Fisher in the early 1920s
(Fisher, 1920, 1922). Let y represent data from the sampling model p (y|Œ∏).
An estimator T (y) of a parameter Œ∏ is said to be suÔ¨Écient if the condi-
tional distribution of the data y, given T (y) , is independent of Œ∏. This
implies that T (y) contains as much information about Œ∏ as the data it-
self. Obviously, when T (y) = y, then T (y) is suÔ¨Écient. But this is not
very useful because the idea of suÔ¨Éciency is to reduce dimensionality of y
without losing information about Œ∏.
The deÔ¨Ånition of suÔ¨Éciency is model dependent. That is, if T (y) is suf-
Ô¨Åcient for Œ∏ under a certain probability model, it may not be so under
another probability model.
Given a model, ML estimators are suÔ¨Écient. To see this, assume that the
likelihood can be factorized into a term that depends on Œ∏ and a second

3.7 Asymptotic Properties: Single Parameter Models
143
term that does not
L (Œ∏|y) ‚àùp (y|Œ∏)
= g (T (y) |Œ∏) h (y) ,
(3.39)
where the equality must hold for all y and Œ∏. The solution of the equation
dL (Œ∏|y)
dŒ∏
= 0
is the same as that of equation
dg (T (y) |Œ∏)
dŒ∏
= 0.
Therefore, the ML estimator must be a function of the suÔ¨Écient statistic
T (y), if the latter exists. Similarly, if
T1 (y) , T2 (y) , . . . , Tr (y)
are jointly suÔ¨Écient statistics in a model with parameters
Œ∏ = [Œ∏1, Œ∏2, . . . , Œ∏r]‚Ä≤ ,
then the ML estimators of these parameters are functions only of T1 (y) ,
T2 (y) , . . . , Tr (y) and are, thus, jointly suÔ¨Écient themselves.
SuÔ¨Écient statistics are not unique. If a statistic T is suÔ¨Écient for a pa-
rameter Œ∏, then a one-to-one transformation function of T is also suÔ¨Écient.
Example 3.9
A sample from a normal distribution with known variance
The log-likelihood based on a sample from a normal distribution with un-
known mean and known variance can be put, from (3.10), as
l

¬µ|œÉ2, y

= constant ‚àín (y ‚àí¬µ)2
2œÉ2
‚àí

i (yi ‚àíy)2
2œÉ2
.
Clearly, the Ô¨Årst term is in the form g (T (y) |¬µ), and the second term is
independent of ¬µ. In this example, the suÔ¨Écient statistic for ¬µ is T (y) = y,
which is the ML estimator of this parameter.
‚ñ†
3.7
Asymptotic Properties:
Single Parameter Models
An important reason why ML estimation is often advocated in the statis-
tical literature is because the method possesses some properties that are
deemed desirable, in some sense. In general, these properties can be shown

144
3. An Introduction to Likelihood Inference
to hold only when, given n independent draws from the same distribution,
sample size increases beyond all bounds, i.e., when n ‚Üí‚àû. Such properties,
termed asymptotic, are consistency, eÔ¨Éciency and asymptotic normality. A
mathematically rigorous discussion of these properties is beyond the scope
of this book. However, because application of ML estimation typically re-
quires recourse to asymptotic properties, some results based on the classi-
cal, Ô¨Årst-order asymptotic theory are presented, to enhance understanding
of this method of estimation.
3.7.1
Probability of the Data Given the
True Value of the Parameter
Suppose n observations are drawn independently from the distribution
[y|Œ∏0], where Œ∏0 is the true value of the parameter Œ∏, at some point of
the parameter space. The joint density of the observations under this dis-
tribution is then
p (y|Œ∏0) =
n
-
i=1
p (yi|Œ∏0) .
Likewise, let
p (y|Œ∏) =
n
-
i=1
p (yi|Œ∏)
be the joint density under any other value of the parameter. Then, under
certain conditions described in, e.g., Lehmann and Casella (1998),
lim
n‚Üí‚àûPr
# n
-
i=1
p (yi|Œ∏0) >
n
-
i=1
p (yi|Œ∏)
$
= 1.
(3.40)
This result can be interpreted in the following manner: as the sample gets
larger, the density (or probability if the random variable is discrete) of the
observations at Œ∏0 exceeds that at any other value of Œ∏ with high probability.
In order to prove the above, consider the statement
n7
i=1
p (yi|Œ∏)
n7
i=1
p (yi|Œ∏0)
< 1.
This is equivalent to
ln
 p (y|Œ∏)
p (y|Œ∏0)

=
n

i=1
ln
 p (yi|Œ∏)
p (yi|Œ∏0)

< 0
and to
1
n
n

i=1
ln
 p (yi|Œ∏)
p (yi|Œ∏0)

< 0.
(3.41)

3.7 Asymptotic Properties: Single Parameter Models
145
Then, by the law of large numbers,
lim
n‚Üí‚àû
#
1
n
n

i=1
ln
 p (yi|Œ∏)
p (yi|Œ∏0)
$
= E

ln
 p (y|Œ∏)
p (y|Œ∏0)
%
.
In an inÔ¨Ånitely large sample the condition in (3.41) would become
E

ln
 p (y|Œ∏)
p (y|Œ∏0)
%
< 0.
(3.42)
By Jensen‚Äôs inequality (see below):
E

ln
 p (y|Œ∏)
p (y|Œ∏0)
%
< ln E
 p (y|Œ∏)
p (y|Œ∏0)

.
(3.43)
Now
E
 p (y|Œ∏)
p (y|Œ∏0)

=

p (y|Œ∏)
p (y|Œ∏0)p (y|Œ∏0) dy
=

p (y|Œ∏) dy = 1.
so
ln E
 p (y|Œ∏)
p (y|Œ∏0)

= 0.
(3.44)
In view of (3.43), with the right-hand side as in (3.44), inequality (3.42)
follows, thus proving the statement in (3.40). As pointed out by Lehmann
and Casella (1998), (3.40) suggests that if in large samples the ML esti-
mator of Œ∏ were close to Œ∏0, it would constitute a reasonable estimator,
generating the observed data with near maximum probability.
Jensen‚Äôs Inequality
Jensen‚Äôs inequality (Rao, 1973; Casella and Berger, 1990) states that if X
is a random variable, and g(X) is a concave function (‚Äúholds water‚Äù), then
g [E(X)] ‚â§E [g(X)] .
(3.45)
If g(X) is convex (‚Äúspills water‚Äù)
E [g(X)] ‚â§g [E(X)] .
(3.46)
A function is said to be convex if its second derivative with respect to
the variable is negative or null throughout. For example, ln (X) is convex.
Under convexity, the function lies below all its tangent lines (Casella and
Berger, 1990). Hence, it must be true that
g(X) ‚â§l(X) = a + bX,

146
3. An Introduction to Likelihood Inference
where l(X) is a line tangent to g(X) at X. Taking expectations of the
inequality
E [g(X)] ‚â§E [l(X)] = l [E (X)] = g [E (X)] ,
thus proving (3.46). The Ô¨Årst equality arises because the expectation of a
linear function is equal to the linear function of the expectation, and the
second, because l is the tangent at E (X).
A similar argument can be employed for a concave upward function
(‚Äúholds water‚Äù); in this case the function lies above the tangent lines.
‚ñ†
3.7.2
Consistency
Suppose that 5Œ∏n is an estimator of the parameter Œ∏ based on random vari-
ables Y1, Y2, . . . , Yn. If, as n increases, the sampling distribution of 5Œ∏n be-
comes more and more concentrated around Œ∏, then 5Œ∏n is said to be consis-
tent. More formally, the sequence of estimators {5Œ∏n} is consistent if {5Œ∏n}
converges in probability to the constant Œ∏. That is,
Pr
"""5Œ∏n ‚àíŒ∏
""" < œµ

‚Üí1
as n ‚Üí‚àû
for each œµ > 0 and each Œ∏. Although convergence refers to a sequence of
estimators, one writes informally that 5Œ∏n is a consistent estimator of Œ∏.
The consistency property of the ML estimator (sketched below) states
that, as n ‚Üí‚àû, the solution to the ML equation l‚Ä≤ (Œ∏) = 0 has a root
5Œ∏n = f(y) tending to the true value Œ∏0 with probability 1. For this to
hold, the likelihood must be diÔ¨Äerentiable with respect to Œ∏ ‚àà‚Ñ¶, and
the observations must be i.i.d.. Following Lehmann and Casella (1998),
where more technical detail is given, suppose that a is small enough so
that (Œ∏0 ‚àía, Œ∏0 + a) ‚àà‚Ñ¶, and consider the event
Sn = {[p (y|Œ∏0) > p (y|Œ∏0 ‚àía)] ‚à©[p (y|Œ∏0) > p (y|Œ∏0 + a)]}
= {[l (Œ∏0) > l (Œ∏0 ‚àía)] ‚à©[l (Œ∏0) > l (Œ∏0 + a)]} .
By virtue of (3.40), it must be true that, as n ‚Üí‚àû, then Pr (Sn) ‚Üí1. This
implies that within the interval considered there exists a value Œ∏0 ‚àía <
5Œ∏n < Œ∏0 + a at which l (Œ∏) has a local maximum. Hence, the Ô¨Årst-order
condition l‚Ä≤ 
5Œ∏n

= 0 would be satisÔ¨Åed. It follows that for any a > 0
suÔ¨Éciently small, it must be true that
Pr
"""5Œ∏n ‚àíŒ∏0
""" < a

‚Üí1.
(3.47)
An alternative motivation follows from (3.40). Note that
lim
n‚Üí‚àûPr
#
ln
n
-
i=1
p (yi|Œ∏0) > ln
n
-
i=1
p (yi|Œ∏)
$
= 1

3.7 Asymptotic Properties: Single Parameter Models
147
can be restated as
lim
n‚Üí‚àûPr {l (Œ∏0|y) > l (Œ∏|y)} = 1.
However, by deÔ¨Ånition of the ML estimator, it must be true that
l

5Œ∏n|y

‚âßl (Œ∏0|y) .
Together the two preceding expressions imply that as n ‚Üí‚àû, l

5Œ∏n|y

must take the value l (Œ∏0|y) . This means that
lim
n‚Üí‚àûPr

5Œ∏n = Œ∏0

= 1,
which shows the consistency of 5Œ∏n.
Feng and McCulloch (1996) have extended these results by proving con-
sistency of the ML estimator when the true parameter value is on the
boundary of the parameter space.
3.7.3
Asymptotic Normality and EÔ¨Éciency
Lehmann and Casella (1998) give a formal statement of the asymptotic
properties of the ML estimator 5Œ∏n of the parameter Œ∏ ‚àà‚Ñ¶, where ‚Ñ¶is the
parameter space. These properties are attained subject to the following
regularity conditions:
1. The parameter space ‚Ñ¶is an open interval (not necessarily Ô¨Ånite).
This guarantees that Œ∏ lies inside ‚Ñ¶and not on the boundaries. Unless
this condition is satisÔ¨Åed, a generally valid Taylor expansion of (5Œ∏‚àíŒ∏)
is not possible. If ‚Ñ¶is a closed interval (such as 0 ‚â§h2 ‚â§1), the
theory holds for values of the parameters that do not include the
boundary.
2. The support of the p.d.f. of the data does not depend on Œ∏; that is,
the set
A = {y : p (y|Œ∏) > 0}
is independent of Œ∏. The data are Y = (Y1, . . . , Yn) and the Yi are
i.i.d. with p.d.f. p (y|Œ∏) or with p.m.f. Pr (Y = y|Œ∏) = p (y|Œ∏).
3. The distributions of the observations are distinct; that is,
F (y|Œ∏1) = F (y|Œ∏2)
implies Œ∏1 = Œ∏2. In other words, the parameter must be identiÔ¨Åable.
In linear models, this condition is typically referred to as estimability
(Searle, 1971).

148
3. An Introduction to Likelihood Inference
4. The density p (y|Œ∏) is three times diÔ¨Äerentiable with respect to Œ∏ and
the third derivative is continuous in Œ∏.
5. The integral

p (y|Œ∏) dy can be diÔ¨Äerentiated three times under the
integral sign. This condition implies E [l‚Ä≤ (Œ∏|y)] = 0 and that
E [‚àíl‚Ä≤‚Ä≤ (Œ∏|y)] = E [l‚Ä≤ (Œ∏|y)]2 = I (Œ∏) .
6. The Fisher information satisÔ¨Åes 0 < I (Œ∏) < ‚àû.
7. There exists a function M (y) (whose expectation is Ô¨Ånite) such that
third derivatives are bounded as follows:
"""""
d3
(dŒ∏)3 [ln p (y|Œ∏)]
""""" ‚â§M (y)
for all y in A and for Œ∏ near Œ∏0, where Œ∏0 is the true value of the
parameter.
Under the above conditions, Lehmann and Casella (1998) prove that 5Œ∏n
is asymptotically (n ‚Üí‚àû) normal
‚àön

5Œ∏n ‚àíŒ∏0
 D
‚ÜíN

0,
1
I1 (Œ∏0)

(3.48)
where I1 (Œ∏0) = I (Œ∏0) /n is the amount of information about Œ∏ contained
in a sample of size 1 and I (Œ∏0) is the information about Œ∏ contained in
the data. The notation ‚Äú
D
‚Üí‚Äù means convergence in distribution; it was
encountered in Chapter 2, Section 2.2.4. See Casella and Berger (1990)
and Lehmann (1999) for a careful development of this concept. As pointed
out in Chapter 2, (3.48) means that the sequence of random variables
‚àön

5Œ∏n ‚àíŒ∏0

, as n ‚Üí‚àû, has a sequence of cumulative distribution func-
tions (c.d.f.) that converges to the c.d.f. of an N

0,
1
I1(Œ∏0)

random variable.
Expression (3.48) is often interpreted as:
5Œ∏n ‚àºN (Œ∏0, I (Œ∏0)) .
(3.49)
The basic elements of the proof are elaborated below.
Let Y1, Y2, ..., Yn be i.i.d. random variables from a distribution having
true parameter value Œ∏0. The maximized log-likelihood is then l

5Œ∏n

, and
expanding the corresponding score function about Œ∏0 yields
l‚Ä≤ 
5Œ∏n

‚âàl‚Ä≤ (Œ∏0) + l‚Ä≤‚Ä≤ (Œ∏0)

5Œ∏n ‚àíŒ∏0

+ 1
2l‚Ä≤‚Ä≤‚Ä≤ (Œ∏0)

5Œ∏n ‚àíŒ∏0
2
.
(3.50)

3.7 Asymptotic Properties: Single Parameter Models
149
The Ô¨Årst, second, and third derivatives are functions of both Œ∏0 and y.
Assuming that 5Œ∏n is the maximizer of l (Œ∏) , then l‚Ä≤ 
5Œ∏n

= 0. Expression
(3.50) can be rearranged, after multiplying both sides by ‚àön, as
‚àön

5Œ∏n ‚àíŒ∏0

=
‚àön l‚Ä≤ (Œ∏0)
‚àíl‚Ä≤‚Ä≤ (Œ∏0) ‚àí1
2l‚Ä≤‚Ä≤‚Ä≤ (Œ∏0)

5Œ∏n ‚àíŒ∏0
.
Dividing the numerator and denominator by n yields
‚àön

5Œ∏n ‚àíŒ∏0

=
n‚àí1/2 l‚Ä≤ (Œ∏0)
‚àín‚àí1l‚Ä≤‚Ä≤ (Œ∏0) ‚àí(2n)‚àí1 l‚Ä≤‚Ä≤‚Ä≤ (Œ∏0)

5Œ∏n ‚àíŒ∏0
.
(3.51)
Now we examine the limiting behavior of the terms in (3.51), as n ‚Üí‚àû,
considering the three terms in the numerator and denominator of (3.51)
separately.
(1) First, note that
1
‚àönl‚Ä≤ (Œ∏0) = ‚àön 1
n
 d
dŒ∏ [ln p (y|Œ∏)]
%
Œ∏=Œ∏0
= ‚àön 1
n
#
d
dŒ∏
n

i=1
ln [p (yi|Œ∏)]
$
Œ∏=Œ∏0
= ‚àön 1
n
n

i=1
p‚Ä≤ (yi|Œ∏0)
p (yi|Œ∏0) .
(3.52)
By being expressible as a sum of independent random variables, this func-
tion should have an asymptotically normal distribution (central limit the-
orem). Its expectation is
E
 1
‚àönl‚Ä≤ (Œ∏0)

= ‚àön 1
n
n

i=1
E
p‚Ä≤ (yi|Œ∏0)
p (yi|Œ∏0)

= 0
(3.53)
this being so because
E
p‚Ä≤ (yi|Œ∏0)
p (yi|Œ∏0)

=

p‚Ä≤ (yi|Œ∏0) dyi = d
dŒ∏

p (yi|Œ∏0) dyi = 0
under the condition that diÔ¨Äerentiation and integration are interchange-
able, as noted earlier. Also, from (3.25),
V ar
 1
‚àönl‚Ä≤ (Œ∏0)

= 1
nV ar [l‚Ä≤ (Œ∏0)]
= 1
nI (Œ∏0) = nI1 (Œ∏0)
n
= I1 (Œ∏0) .
(3.54)

150
3. An Introduction to Likelihood Inference
From (3.53), (3.54), and the central limit theorem, it follows that
1
‚àönl‚Ä≤ (Œ∏0)
D
‚ÜíN [0, I1 (Œ∏0)] .
(3.55)
(2) The second term of interest in (3.51) is
‚àí1
nl‚Ä≤‚Ä≤ (Œ∏0) = ‚àí1
n
n

i=1
 d
dŒ∏
p‚Ä≤ (yi|Œ∏)
p (yi|Œ∏)
%
Œ∏=Œ∏0
= 1
n
n

i=1
[p‚Ä≤ (yi|Œ∏0)]2 ‚àíp (yi|Œ∏0) p‚Ä≤‚Ä≤ (yi|Œ∏0)
p2 (yi|Œ∏0)
= 1
n
n

i=1
[p‚Ä≤ (yi|Œ∏0)]2
p2 (yi|Œ∏0) ‚àí1
n
n

i=1
p‚Ä≤‚Ä≤ (yi|Œ∏0)
p (yi|Œ∏0) .
(3.56)
As n ‚Üí‚àû,
1
n
n

i=1
[p‚Ä≤ (yi|Œ∏0)]2
p2 (yi|Œ∏0)
‚ÜíE
#
[p‚Ä≤ (yi|Œ∏0)]2
p2 (yi|Œ∏0)
$
= E
 d
dŒ∏ ln p (yi|Œ∏)
2
Œ∏=Œ∏0
= I1 (Œ∏0)
and
1
n
n

i=1
p‚Ä≤‚Ä≤ (yi|Œ∏0)
p (yi|Œ∏0) ‚ÜíE
p‚Ä≤‚Ä≤ (yi|Œ∏0)
p (yi|Œ∏0)

=

p‚Ä≤‚Ä≤ (yi|Œ∏0) dyi
=
d2
dŒ∏

p (yi|Œ∏) dyi

Œ∏=Œ∏0
= 0.
Hence, in probability, we have that (3.56)
‚àí1
nl‚Ä≤‚Ä≤ (Œ∏0) ‚ÜíI1 (Œ∏) .
(3.57)
(3) The third term in (3.51) is
‚àí1
2nl‚Ä≤‚Ä≤‚Ä≤ (Œ∏0)

5Œ∏n ‚àíŒ∏0

.
Note that
1
nl‚Ä≤‚Ä≤‚Ä≤ (Œ∏0) = 1
n
n

i=1
#
d3
(dŒ∏)3 [ln p (yi|Œ∏)]
$
Œ∏=Œ∏0
.
Based on the regularity condition 7, suppose that the third derivatives are
bounded as follows:
"""""
d3
(dŒ∏)3 [ln p (y|Œ∏)]
""""" ‚â§M (y)

3.7 Asymptotic Properties: Single Parameter Models
151
for all y in A and for Œ∏ near Œ∏0. Then
""""
1
nl‚Ä≤‚Ä≤‚Ä≤ (Œ∏0)
"""" ‚â§1
n
n

i=1
"""""
#
d3
(dŒ∏)3 [ln p (yi|Œ∏)]
$
Œ∏=Œ∏0
"""""
‚â§1
n
n

i=1
M (yi) .
As n ‚Üí‚àû,
""n‚àí1l‚Ä≤‚Ä≤‚Ä≤ (Œ∏0)
"" must be smaller than or equal to E [M (y)] , which
in condition 7 above, is assumed to be Ô¨Ånite. Hence, as sample size goes to
inÔ¨Ånity, the term
‚àí1
2nl‚Ä≤‚Ä≤‚Ä≤ (Œ∏0)

5Œ∏n ‚àíŒ∏0

‚Üí0,
(3.58)
this being so because 5Œ∏n ‚ÜíŒ∏0 and the third derivatives are bounded in the
preceding sense.
Considering (3.55), collecting (3.57) and (3.58), then the expression of
interest in (3.51), that is,
‚àön

5Œ∏n ‚àíŒ∏0

=
n‚àí1/2l‚Ä≤ (Œ∏0)
‚àín‚àí1l‚Ä≤‚Ä≤ (Œ∏0) ‚àí(2n)‚àí1 l‚Ä≤‚Ä≤‚Ä≤ (Œ∏0)

5Œ∏n ‚àíŒ∏0

behaves in the limit as
n‚àí1/2l‚Ä≤ (Œ∏0)
I1 (Œ∏0)
‚àºN

0,
1
I1 (Œ∏0)

.
(3.59)
This indicates that if
‚àön

5Œ∏n ‚àíŒ∏0

‚àºN

0,
1
I1 (Œ∏0)

(3.60)
then, as n ‚Üí‚àû, the ML estimator can be written as the random variable
5Œ∏n = Œ∏0 +
1
>
nI1 (Œ∏0)
N (0, 1)) ,
(3.61)
where nI1 (Œ∏0) = I (Œ∏0). Hence, as n ‚Üí‚àû:
(1) E

5Œ∏n

= Œ∏0, so the ML estimator is asymptotically unbiased.
(2) V ar

5Œ∏n

= [nI1 (Œ∏0)]‚àí1 = [I (Œ∏0)]‚àí1, so it reaches the Cram¬¥er-Rao
lower bound, i.e., it has minimum variance among asymptotically unbiased
estimators of Œ∏. When this limit is reached, the estimator is said to be
eÔ¨Écient.
(3) Informally stated, as in (3.49), it is said that the ML estimator has the
asymptotic distribution 5Œ∏n ‚àΩN

Œ∏0, I‚àí1 (Œ∏0)

. This facilitates inferences,
although these are valid strictly for a sample having inÔ¨Ånite size.
(4) As seen earlier, 5Œ∏n is consistent, reaching the true value Œ∏0 when n ‚Üí‚àû.

152
3. An Introduction to Likelihood Inference
3.8
Asymptotic Properties:
Multiparameter Models
Consider now the situation where the distribution of the observations is in-
dexed by parameters Œ∏. This vector can have distinct components, some of
primary interest and others incidental, often referred to as nuisance param-
eters. The asymptotic properties of the ML estimator extend rather nat-
urally, and constitute multidimensional counterparts of (3.60) and (3.61).
Establishing these properties is technically more involved than in the single
parameter case (Lehmann and Casella, 1998), so only an informal account
will be given here. Similar to (3.50), the score vector l‚Ä≤ 
5Œ∏n

evaluated at
5Œ∏n, the ML estimator based on n i.i.d. samples, can be expanded about the
true parameter value Œ∏0 as
l‚Ä≤ 
5Œ∏n

‚âàl‚Ä≤ (Œ∏0) +
‚àÇ2l (Œ∏)
‚àÇŒ∏ ‚àÇŒ∏‚Ä≤

Œ∏=Œ∏0

5Œ∏n ‚àíŒ∏0

.
Derivatives higher than second-order are ignored in the above expansion.
The score vector must be null when evaluated at a stationary point, so
rearranging and multiplying both sides by ‚àön yields
‚àön

5Œ∏n ‚àíŒ∏0

‚âà

‚àí1
n
‚àÇ2l (Œ∏)
‚àÇŒ∏ ‚àÇŒ∏‚Ä≤
‚àí1
Œ∏=Œ∏0
1
‚àönl‚Ä≤ (Œ∏0) .
(3.62)
The random vector:
1
‚àönl‚Ä≤ (Œ∏0) =
1
‚àön
n

i=1
l‚Ä≤
i (Œ∏0)
D
‚ÜíN [0, I1 (Œ∏0)]
(3.63)
by the central limit theorem, because it involves the sum of many i.i.d.
random score vectors l‚Ä≤
i (Œ∏0) . Likewise, as n ‚Üí‚àû, the random matrix

‚àí1
n
‚àÇ2l (Œ∏)
‚àÇŒ∏ ‚àÇŒ∏‚Ä≤
‚àí1
Œ∏=Œ∏0
=
#
1
n
n

i=1

‚àí‚àÇ2li (Œ∏)
‚àÇŒ∏ ‚àÇŒ∏‚Ä≤
$‚àí1
Œ∏=Œ∏0
‚ÜíI‚àí1
1
(Œ∏0) .
Hence, because n‚àí1/2l‚Ä≤ (Œ∏0) converges in distribution to an N [0, I1 (Œ∏)]
process and

‚àí1
n
‚àÇ2l (Œ∏)
‚àÇŒ∏ ‚àÇŒ∏‚Ä≤
‚àí1
Œ∏=Œ∏0
converges to the constant I‚àí1
1
(Œ∏0) , the multivariate version of Slutsky‚Äôs
theorem (Casella and Berger, 1990) yields

‚àí1
n
‚àÇ2l (Œ∏)
‚àÇŒ∏ ‚àÇŒ∏‚Ä≤
‚àí1
Œ∏=Œ∏0
1
‚àönl‚Ä≤ (Œ∏0)
D
‚ÜíI‚àí1
1
(Œ∏0) N [0, I1 (Œ∏)] .
(3.64)

3.9 Functional Invariance
153
Using this in (3.62), it follows that
‚àön

5Œ∏n ‚àíŒ∏0
 D
‚ÜíN

0, I‚àí1
1
(Œ∏0)

.
(3.65)
Hence, in large samples, the ML estimator can be written (informally) as
5Œ∏n ‚àºN

Œ∏0, I‚àí1 (Œ∏0)

,
(3.66)
so the ML estimator is asymptotically unbiased, eÔ¨Écient, and multivariate
normal. Distribution (3.66) gives the basis for the solution of many infer-
ential problems in genetics via large sample theory. Note, however, that
the asymptotic distribution depends on the unknown Œ∏0. In practice, one
proceeds by using the approximate distribution 5Œ∏n ‚àºN[5Œ∏n, I‚àí1(5Œ∏n)]. For
a large sample, this may be accurate enough, in view of the consistency
property of 5Œ∏n. It should be clear, however, that this approximation does
not take into account the error associated with the estimation of Œ∏.
3.9
Functional Invariance of Maximum
Likelihood Estimators
The property of functional invariance states that if 5Œ∏ is the ML estimator
of Œ∏, then the ML estimator of the function f (Œ∏) is f(5Œ∏). The property is
motivated using the linear regression model of Example 3.7, and a more
formal presentation is given subsequently.
3.9.1
Illustration of Functional Invariance
In Example 3.7 the ML estimators of Œ≤0 and Œ≤1 were found to be 5Œ≤0 and
5Œ≤1, respectively. If the function Œ≤0/Œ≤1 exists, then its ML estimator is:
5Œ≤0
5Œ≤1
=

y ‚àí5Œ≤1x
 
n
i=1
x2
i ‚àí1
n

 n
i=1
xi
2
n
i=1
xiyi ‚àí1
n
n
i=1
xi
n
i=1
yi
.
We proceed to verify that this is indeed the case. The log-likelihood, as-
suming independent sampling from a normal distribution, apart from an
additive constant, is
l

Œ≤0, Œ≤1, œÉ2|y

= ‚àín
2 ln

œÉ2
‚àí
1
2œÉ2
n

i=1
(yi ‚àíŒ≤0 ‚àíŒ≤1xi)2 .

154
3. An Introduction to Likelihood Inference
DeÔ¨Åne Œ∑ = Œ≤0/Œ≤1, supposing the slope coeÔ¨Écient cannot be null. The
sampling scheme remains unchanged, but the model is formulated now in
terms of the one-to-one reparameterization

Œ≤0
Œ≤1

‚Üí

Œ∑
Œ≤1

.
This means that it is possible to go from one parameterization to another
in a unique manner. For example, a reparameterization to Œ≤0 and Œ≤2
1 would
not be one-to-one because Œ≤1 and ‚àíŒ≤1 yield the same value of Œ≤2
1. The log-
likelihoods under the alternative parameterizations are related according
to
l

Œ≤0, Œ≤1, œÉ2|y

= l

Œ∑, Œ≤1, œÉ2|y

= ‚àín
2 ln

œÉ2
‚àí
1
2œÉ2
n

i=1
(yi ‚àíŒ∑Œ≤1 ‚àíŒ≤1xi)2 .
The score vector under the new parameterization is
Ô£Æ
Ô£ØÔ£ØÔ£∞
‚àÇl/‚àÇŒ∑
‚àÇl/‚àÇŒ≤1
‚àÇl/‚àÇœÉ2
Ô£π
Ô£∫Ô£∫Ô£ª=
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
Œ≤1
œÉ2
n

i=1
(yi ‚àíŒ∑Œ≤1 ‚àíŒ≤1xi)
1
œÉ2
n

i=1
(yi ‚àíŒ∑Œ≤1 ‚àíŒ≤1xi) (Œ∑ + xi)
‚àín
2œÉ2 +
1
2œÉ4
n

i=1
(yi ‚àíŒ∑Œ≤1 ‚àíŒ≤1xi)2
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
.
Setting its three elements simultaneously to zero, and solving for the un-
known parameters gives the ML estimators. From the Ô¨Årst equation, one
obtains directly
5Œ∑ =

y ‚àí5Œ≤1x

5Œ≤1
.
The second equation leads to
5Œ∑
n

i=1

yi ‚àíy ‚àí5Œ≤1 (xi ‚àíx)

+
n

i=1

xiyi ‚àí5Œ∑5Œ≤1xi ‚àí5Œ≤1x2
i

=
n

i=1

xiyi ‚àí

y ‚àí5Œ≤1x

xi ‚àí5Œ≤1x2
i

= 0,
which, when solved for 5Œ≤1, gives as solution
5Œ≤1 =
n
i=1
xiyi ‚àí1
n
n
i=1
xi
n
i=1
yi
n

i=1
x2
i ‚àí1
n

 n
i=1
xi
2 .

3.9 Functional Invariance
155
This is identical to the ML estimator found under the initial parameter-
ization, as shown in Example 3.7. The third equation leads to the ML
estimator of œÉ2
5œÉ2 = 1
n
n
i=1

yi ‚àí5Œ∑5Œ≤1 ‚àí5Œ≤1xi
2
= 1
n
n
i=1

yi ‚àí5Œ≤0 ‚àí5Œ≤1xi
2
as found before. Finally, from the reparameterized model, one can estimate
Œ≤0 as 5Œ≤0 = 5Œ∑5Œ≤1.
It can be veriÔ¨Åed that the ML estimators of Œ≤0 and Œ≤1 are unbiased. For
example,
E

5Œ≤1

= E
Ô£´
Ô£¨
Ô£¨
Ô£¨
Ô£≠
n
i=1
xiyi ‚àí1
n
n
i=1
xi
n
i=1
yi
n
i=1
x2
i ‚àí1
n

 n
i=1
xi
2
Ô£∂
Ô£∑
Ô£∑
Ô£∑
Ô£∏
=
E

 n
i=1
xiyi ‚àí1
n
n
i=1
xi
n
i=1
yi

n
i=1
x2
i ‚àí1
n

 n
i=1
xi
2
=
Œ≤0
n
i=1
xi + Œ≤1
n
i=1
x2
i ‚àí1
n
n
i=1
xi

nŒ≤0 + Œ≤1
n
i=1
xi

n
i=1
x2
i ‚àí1
n

 n
i=1
xi
2
= Œ≤1,
and the same is true for Œ≤0. This should not be construed as an indication
that ML estimators are always unbiased; it was seen in Example 3.8 that
the ML estimator of œÉ2 has a downward bias. It is not obvious how to
obtain an unbiased estimator of Œ∑ = Œ≤0/Œ≤1, but the functional invariance
property of the ML estimator leads directly to ML estimation of this ratio.
The estimator is biased, as shown below.
Example 3.10
Bias of a ratio of estimators
Suppose we wish to evaluate, in the linear regression model,
E
 5Œ≤0
5Œ≤1

.
Following Goodman and Hartley (1958) and Raj (1968), one can write
Cov
 5Œ≤0
5Œ≤1
, 5Œ≤1

= E

5Œ≤0

‚àíE
 5Œ≤0
5Œ≤1

E

5Œ≤1

= Œ≤0 ‚àíE
 5Œ≤0
5Œ≤1

Œ≤1.

156
3. An Introduction to Likelihood Inference
Rearranging,
E
 5Œ≤0
5Œ≤1

= Œ≤0
Œ≤1
‚àí
Cov
 Œ≤0
Œ≤1 , 5Œ≤1

Œ≤1
,
which shows that unless Cov

5Œ≤0/5Œ≤1, 5Œ≤1

is null, the ML estimator of Œ≤0/Œ≤1
is biased. This bias, E

5Œ≤0/5Œ≤1

‚àíŒ≤0/Œ≤1, expressed in units of standard
deviation, is
Bias

5Œ≤0/5Œ≤1

?
V ar

5Œ≤0/5Œ≤1
 = ‚àí
Corr

5Œ≤0/5Œ≤1, 5Œ≤1
 ?
V ar

5Œ≤1

Œ≤1
= ‚àíCorr

5Œ≤0/5Œ≤1, 5Œ≤1

CV

5Œ≤1

,
where CV (¬∑) denotes coeÔ¨Écient of variation. Hence
"""Bias

5Œ≤0/5Œ≤1
"""
?
V ar

5Œ≤0/5Œ≤1
 ‚â§CV

5Œ≤1

which gives an upper bound for the absolute value of the bias in units
of standard deviation. From the form of the ML estimator of Œ≤1 given in
Example 3.7, it can be deduced that
V ar

5Œ≤1

=
œÉ2
n
i=1
x2
i ‚àí1
n

 n
i=1
xi
2 .
Thus,
CV

5Œ≤1

= œÉ
Œ≤
@
A
A
A
B
1
n
i=1
x2
i ‚àí1
n

 n
i=1
xi
2 .
This indicates that the absolute standardized bias of the ratio estimator
can be reduced by increasing the dispersion of the values of the explanatory
variable x, as measured by
n

i=1
x2
i ‚àí1
n

 n
i=1
xi
2
.
‚ñ†

3.9 Functional Invariance
157
3.9.2
Invariance in a Single Parameter Model
In the preceding section, it was seen that for a one-to-one reparameteriza-
tion, the same results are obtained irrespective of whether the likelihood
function is maximized with respect to Œ∏ or Œ∑ = f (Œ∏). In the presentation
below it is assumed that the transformation is one-to-one, in the sense that
Œ∏ = f ‚àí1 (Œ∑). However, the principle of invariance can be extended to hold
for any transformation (Mood et al., 1974; Cox and Hinkley, 1974).
The explanation of the principle of invariance given here is based on
the fact that two alternative parameterizations must generate the same
probability distribution of y. Hence
p (y|Œ∏, Œ∏ ‚àà‚Ñ¶) = p

y|f ‚àí1 (Œ∑) ,
Œ∑ ‚àà‚Ñ¶‚àó
= p (y|Œ∑, Œ∑ ‚àà‚Ñ¶‚àó) = p [y|f (Œ∏) ,
Œ∏ ‚àà‚Ñ¶] ,
(3.67)
where ‚Ñ¶‚àóis the parameter space of Œ∑. A similar relationship must then
hold for the log-likelihoods
l (Œ∏|y) = l

f ‚àí1 (Œ∑) |y

= l (Œ∑|y) = l [f (Œ∏) |y] .
(3.68)
Then, if 5Œ∑ is the maximizer of l (Œ∑|y) , it must be true that 5Œ∑ = f

5Œ∏

,
because l (Œ∏|y) and l (Œ∑|y) have the same maximum and the relationship is
one-to-one. Under the new parameterization, the score can be written as
dl (Œ∑|y)
dŒ∑
= dl (Œ∏|y)
dŒ∑
= dl (Œ∏|y)
dŒ∏
dŒ∏
dŒ∑ .
(3.69)
Setting this equation to zero leads to 5Œ∑, the ML estimator of Œ∑. In order
to evaluate the observed information about Œ∑ contained in the sample,
diÔ¨Äerentiation of (3.69) with respect to Œ∑ yields
d2l (Œ∑|y)
(dŒ∑)2
= d
dŒ∑
dl (Œ∏|y)
dŒ∏
dŒ∏
dŒ∑

=
 d
dŒ∑
dl (Œ∏|y)
dŒ∏
% dŒ∏
dŒ∑ +
dl (Œ∏|y)
dŒ∏
 d2Œ∏
(dŒ∑)2
=
#
d2l (Œ∏|y)
(dŒ∏)2

dŒ∏
dŒ∑
$
dŒ∏
dŒ∑ +
dl (Œ∏|y)
dŒ∏
 d2Œ∏
(dŒ∑)2
=

d2l (Œ∏|y)
(dŒ∏)2

dŒ∏
dŒ∑
2
+ dl (Œ∏|y)
dŒ∏
d2Œ∏
(dŒ∑)2

.
(3.70)

158
3. An Introduction to Likelihood Inference
The expected information about the new parameter Œ∑ contained in the
sample is then
I (Œ∑) = E

d2l (Œ∏|y)
(dŒ∏)2
 
dŒ∏
dŒ∑
2
+ E
dl (Œ∏|y)
dŒ∏
 d2Œ∏
(dŒ∑)2
= E

d2l (Œ∏|y)
(dŒ∏)2
 
dŒ∏
dŒ∑
2
= I (Œ∏)

dŒ∏
dŒ∑
2
= I

f ‚àí1 (Œ∑)
 df ‚àí1 (Œ∑)
dŒ∑
2
.
(3.71)
This being so because E [dl (Œ∏|y) /dŒ∏] = 0, as seen in (3.20). Now, using
the general result
dx
dy =
1
dy/dx,
the (asymptotic) variance of the ML estimator of the transformed param-
eter is
V ar (Œ∑) = [I (Œ∑)]‚àí1
= [I (Œ∏)]‚àí1
dŒ∑
dŒ∏
2
= V ar (Œ∏)
dŒ∑
dŒ∏
2
(3.72)
which would be evaluated at Œ∏ = 5Œ∏. Expression (3.72) can give erroneous
results when f is not monotone, as shown in the following example.
Example 3.11
A binomially distributed random variable
For a binomially distributed random variable X (number of ‚Äúsuccesses‚Äù in
n trials), the likelihood is
L (Œ∏|x, n) ‚àùŒ∏x (1 ‚àíŒ∏)n‚àíx
and the log-likelihood, ignoring an additive constant, is
l (Œ∏|x, n) = x ln Œ∏ + (n ‚àíx) ln (1 ‚àíŒ∏) .
Setting the Ô¨Årst diÔ¨Äerential of the log-likelihood with respect to Œ∏ equal to
zero and solving for Œ∏ yields the closed-form ML estimator
5Œ∏ = x
n.
Since, by deÔ¨Ånition, X results from the sum of n independent Bernoulli
trials, each with variance Œ∏ (1 ‚àíŒ∏), the variance of 5Œ∏ is
V ar

5Œ∏

= Œ∏ (1 ‚àíŒ∏)
n
.

3.9 Functional Invariance
159
Imagine that interest focuses on the odds ratio Œ∑ = f (Œ∏) = Œ∏/(1‚àíŒ∏). Since
dŒ∑
dŒ∏ =
1
(1 ‚àíŒ∏)2 ,
using (3.72), we obtain
V ar (5Œ∑) =
5Œ∏
n

1 ‚àí5Œ∏
3 .
In this case, the transformation f is one-to-one. Suppose instead that there
is interest in the quantity œâ = g (Œ∏) = Œ∏ (1 ‚àíŒ∏). Now,
dœâ
dŒ∏ = 1 ‚àí2Œ∏.
Then (3.72) yields, for the asymptotic variance of the ML of œâ,
V ar (5œâ) =
5Œ∏

1 ‚àí5Œ∏

n

1 ‚àí25Œ∏
2
,
which underestimates the variance rather drastically if 5Œ∏ = 1/2. The source
of this problem is that g is not one-to-one. A way around this problem was
discussed in Section 2.2.4 of Chapter 2.
‚ñ†
3.9.3
Invariance in a Multiparameter Model
Let the distribution of the random vector y be indexed by a parameter
Œ∏ having more than one element. Consider the one-to-one transformation
Œ∑ = f (Œ∏) such that the inverse function Œ∏ = f ‚àí1 (Œ∑) exists, and suppose
that the likelihood is diÔ¨Äerentiable with respect to Œ∑ at least twice. The
relationship between likelihoods given in (3.68) carries directly to the mul-
tiparameter situation. The score vector in the reparameterized model, after
equation (3.69), is
‚àÇl (Œ∑|y)
‚àÇŒ∑
= ‚àÇŒ∏‚Ä≤
‚àÇŒ∑
‚àÇl (Œ∏|y)
‚àÇŒ∏
,
(3.73)
where ‚àÇŒ∏‚Ä≤/‚àÇŒ∑ = {‚àÇŒ∏j/‚àÇŒ∑i} is a matrix of order p √ó p, p is the number of
elements in Œ∏, and subscripts i and j refer to row and column, respectively.
Thus, the jth row of ‚àÇŒ∏‚Ä≤/‚àÇŒ∑ looks as follows:
‚àÇŒ∏1
‚àÇŒ∑j
, ‚àÇŒ∏2
‚àÇŒ∑j
, . . . , ‚àÇŒ∏p
‚àÇŒ∑j

.
The expected information matrix, following (3.71), is
I (Œ∑) = ‚àÇŒ∏‚Ä≤
‚àÇŒ∑ I (Œ∏) ‚àÇŒ∏
‚àÇŒ∑‚Ä≤ = ‚àÇ

f ‚àí1 (Œ∑)
‚Ä≤
‚àÇŒ∑
I

f ‚àí1 (Œ∑)
 ‚àÇ

f ‚àí1 (Œ∑)

‚àÇŒ∑‚Ä≤
,
(3.74)

160
3. An Introduction to Likelihood Inference
and the expression equivalent to (3.72) is
V ar (5Œ∑) = ‚àÇŒ∑
‚àÇŒ∏‚Ä≤
""""
Œ∏=Œ∏
V ar

5Œ∏
 ‚àÇŒ∑‚Ä≤
‚àÇŒ∏
""""
Œ∏=Œ∏
.
(3.75)
Example 3.12
A reparameterized linear regression model
The matrix in (3.74) is illustrated using the linear regression model. The
original parameterization was in terms of Œ∏‚Ä≤ = [Œ≤0, Œ≤1, œÉ2]. The new pa-
rameterization consists of the vector
Œ∑ =
Ô£Æ
Ô£∞
Œ∑1
Œ∑2
Œ∑3
Ô£π
Ô£ª=
Ô£Æ
Ô£∞
Œ≤0/Œ≤1
Œ≤1
œÉ2
Ô£π
Ô£ª
with inverse
Œ∏ = f ‚àí1 (Œ∑) =
Ô£Æ
Ô£∞
Œ∑1Œ∑2
Œ∑2
Œ∑3
Ô£π
Ô£ª.
The 3 √ó 3 matrix deÔ¨Åned after (3.73) would be
‚àÇŒ∏‚Ä≤
‚àÇŒ∑ =
Ô£Æ
Ô£∞
‚àÇŒ∏1/‚àÇŒ∑1
‚àÇŒ∏2/‚àÇŒ∑1
‚àÇŒ∏3/‚àÇŒ∑1
‚àÇŒ∏1/‚àÇŒ∑2
‚àÇŒ∏2/‚àÇŒ∑2
‚àÇŒ∏3/‚àÇŒ∑2
‚àÇŒ∏1/‚àÇŒ∑3
‚àÇŒ∏2/‚àÇŒ∑3
‚àÇŒ∏3/‚àÇŒ∑3
Ô£π
Ô£ª=
Ô£Æ
Ô£∞
Œ∑2
0
0
Œ∑1
1
0
0
0
1
Ô£π
Ô£ª.
(3.76)
DiÔ¨Äerentiating the log-likelihood twice under the new parameterization,
multiplying by ‚àí1, and taking expectations, yields the information matrix
I (Œ∑) = 1
œÉ2
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
nŒ∑2
2
Œ∑2

nŒ∑1 +
n

i=1
xi

0
Œ∑2

nŒ∑1 +
n

i=1
xi

n

i=1
(Œ∑1 + xi)2
0
0
0
n
2Œ∑3
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
.
It can be veriÔ¨Åed that the same result is obtained from the matrix product
given in (3.74), employing the matrix (3.76).
‚ñ†

4
Further Topics in Likelihood Inference
4.1
Introduction
This chapter continues the discussion on likelihood inference. First, two
commonly used numerical methods for obtaining ML estimates when the
likelihood equations do not have a closed form or are diÔ¨Écult to solve are
introduced. (A third method, the Expectation‚ÄìMaximization algorithm,
often referred to as the EM-algorithm, is discussed in Chapter 9). This is
followed by a discussion of the traditional test of hypotheses entrenched in
the Neyman‚ÄìPearson theory. The classical Ô¨Årst-order asymptotic distribu-
tion of the likelihood ratio is derived assuming that the standard regularity
conditions are satisÔ¨Åed, and examples of likelihood ratio tests are given.
The presence of so-called nuisance parameters has been one of the major
challenges facing the likelihood paradigm. How does one make inferences
about the parameters of interest in the presence of nuisance parameters
without overstating precision? Many diÔ¨Äerent approaches have been sug-
gested for dealing with this problem and some of these are brieÔ¨Çy discussed
and illustrated here. The chapter ends with examples involving the multi-
nomial model and the analysis of ordered categorically distributed data.

162
4. Further Topics in Likelihood Inference
4.2
Computation of Maximum
Likelihood Estimates
As noted earlier, the ML equations (the score vector)
l‚Ä≤ (Œ∏) = 0
may not have an explicit solution, so numerical methods must be used
to solve them. Maximization of a likelihood function (or minimization of
any induced objective function) can be viewed as a nonlinear optimization
problem. A plethora of algorithms can be used for this purpose. Treatises in
numerical analysis, such as Dahlquist and Bj¬®orck (1974), Dennis and Schn-
abel (1983), and Hager (1988) can be consulted. Here, a sketch is presented
of two widely used algorithms employed in connection with likelihood in-
ference: the Newton‚ÄìRaphson method and Fisher‚Äôs scoring procedure.
The Newton‚ÄìRaphson Procedure
Consider expanding the score vector about a trial value Œ∏[t], where t =
0, 1, 2, ... denotes an iterate number. A linear approximation to the score
gives
l‚Ä≤ (Œ∏) ‚âàl‚Ä≤ 
Œ∏[t]
+
‚àÇ2l (Œ∏)
‚àÇŒ∏ ‚àÇŒ∏‚Ä≤

Œ∏=Œ∏[t]

Œ∏ ‚àíŒ∏[t]
.
In the vicinity of a stationary point, it must be true that l‚Ä≤ (Œ∏) ‚âà0. Using
this in the preceding expression, and solving for Œ∏ at each step t, gives the
sequence of updated values
Œ∏[t+1] = Œ∏[t] +

‚àí‚àÇ2l (Œ∏)
‚àÇŒ∏ ‚àÇŒ∏‚Ä≤
‚àí1
Œ∏=Œ∏[t] l‚Ä≤ 
Œ∏[t]
,
t = 0, 1, 2, ... .
(4.1)
The diÔ¨Äerence Œ∏[t+1] ‚àíŒ∏[t] is called a correction. The iterative process is
continued until the correction is null, corresponding to the situation where
l‚Ä≤(Œ∏[t]) = 0. This procedure is known as the Newton‚ÄìRaphson algorithm;
Fisher‚Äôs scoring method uses I (Œ∏) instead of the observed information ma-
trix ‚àí‚àÇ2l (Œ∏) /‚àÇŒ∏ ‚àÇŒ∏‚Ä≤. Typically, the method of scoring requires fewer cal-
culations, because many expressions vanish or simplify in the process of
taking expectations. However, it may converge at a slower rate. The two
methods may not converge at all and, even if they do, there is no assurance
that a global maximum would be located. This is not surprising, as the
methods search for stationarity without reference to the possible existence
of multiple maxima. This is a potential problem in models having many
parameters, and it is expected to occur more frequently when sample sizes
are small, as the likelihood may have several ‚Äúpeaks and valleys‚Äù.

4.2 Computation of Maximum Likelihood Estimates
163
Quadratic Convergence Property
The Newton‚ÄìRaphson procedure has the property of converging quadrat-
ically to a stationary point. Dahlquist and Bj¬®orck (1974) provide a proof
for the single parameter case, and this is presented in more detail here.
Let the root of l‚Ä≤ (Œ∏) = 0 be 5Œ∏. Hence, l‚Ä≤‚Ä≤(5Œ∏) cannot be a null matrix and
l‚Ä≤‚Ä≤ (Œ∏) cannot be null for all Œ∏ near (in some sense) 5Œ∏. Let the error of the
trial values at iterates t and t + 1 be œµ[t] = Œ∏[t] ‚àí5Œ∏ and œµ[t+1] = Œ∏[t+1] ‚àí5Œ∏,
respectively. A second-order Taylor series expansion of the score evaluated
at 5Œ∏ (which must be equal to zero, because 5Œ∏ is a root), about iterate value
Œ∏[t], gives
l‚Ä≤ 
5Œ∏

‚âàl‚Ä≤ 
Œ∏[t]
+ l‚Ä≤‚Ä≤ 
Œ∏[t] 
5Œ∏ ‚àíŒ∏[t]
+ 1
2l‚Ä≤‚Ä≤‚Ä≤ 
Œ∏[t] 
5Œ∏ ‚àíŒ∏[t]2
.
Because l‚Ä≤‚Ä≤ 
Œ∏[t]
is not null, one can write
5Œ∏ ‚àíŒ∏[t] +
l‚Ä≤ 
Œ∏[t]
l‚Ä≤‚Ä≤

Œ∏[t] =
‚àí1
2l‚Ä≤‚Ä≤‚Ä≤ 
Œ∏[t] 
5Œ∏ ‚àíŒ∏[t]2
l‚Ä≤‚Ä≤

Œ∏[t]
.
(4.2)
Now, because of the form of the Newton‚ÄìRaphson iteration in (4.1), ex-
pression (4.2) is equivalent to
5Œ∏ ‚àíŒ∏[t+1] =
‚àí1
2l‚Ä≤‚Ä≤‚Ä≤ 
Œ∏[t] 
5Œ∏ ‚àíŒ∏[t]2
l‚Ä≤‚Ä≤

Œ∏[t]
.
Writing the above in terms of the errors about the root, one obtains
œµ[t+1] =
l‚Ä≤‚Ä≤‚Ä≤ 
Œ∏[t]
2l‚Ä≤‚Ä≤

Œ∏[t]

œµ[t]2
.
(4.3)
This indicates that the error at iterate t is proportional to the square of the
error at the preceding iteration, so the method is said to be quadratically
convergent. Now
lim
Œ∏[t]‚ÜíŒ∏

œµ[t+1]

œµ[t]2

= lim
Œ∏[t]‚ÜíŒ∏
Ô£Æ
Ô£∞
l‚Ä≤‚Ä≤‚Ä≤ 
Œ∏[t]
2l‚Ä≤‚Ä≤

Œ∏[t]
Ô£π
Ô£ª=
l‚Ä≤‚Ä≤‚Ä≤ 
5Œ∏

2l‚Ä≤‚Ä≤

5Œ∏
.
(4.4)
Also,
lim
Œ∏[t]‚ÜíŒ∏
"""""
œµ[t+1]

œµ[t]2
"""""

= lim
Œ∏[t]‚ÜíŒ∏
""œµ[t+1]""

œµ[t]2

= 1
2
"""l‚Ä≤‚Ä≤‚Ä≤ 
5Œ∏
"""
"""l‚Ä≤‚Ä≤

5Œ∏
"""
= C

164
4. Further Topics in Likelihood Inference
and C, called the ‚Äúasymptotic error constant‚Äù, will be nonnull whenever
l‚Ä≤‚Ä≤‚Ä≤ 
5Œ∏

Ã∏= 0. For a sample of size n, the second derivative of the log-
likelihood, with respect to Œ∏, is equal to
l‚Ä≤‚Ä≤ (Œ∏) =
n

i=1
d2
(dŒ∏)2 ln p (yi|Œ∏)
and, for large n, this converges in probability to ‚àíI (Œ∏) = ‚àínI1 (Œ∏) . Hence,
C goes to zero as the sample size increases, provided the third derivatives
are bounded; recall that this was an assumption made when proving con-
sistency of the ML estimator. This suggests a relatively faster convergence
rate at larger values of n, given that the algorithm converges, as assumed
here.
The Newton‚ÄìRaphson algorithm always converges to a root of the ML
equation provided that the starting value Œ∏[0] is suÔ¨Éciently close to such a
root (Dahlquist and Bj¬®orck, 1974). Using (4.3),
5Œ∏ ‚àíŒ∏[1] =

5Œ∏ ‚àíŒ∏[0]
Ô£Æ
Ô£∞

5Œ∏ ‚àíŒ∏[0] l‚Ä≤‚Ä≤‚Ä≤ 
Œ∏[0]
2l‚Ä≤‚Ä≤

Œ∏[0]
Ô£π
Ô£ª.
Thus, if
""""""

5Œ∏ ‚àíŒ∏[0] l‚Ä≤‚Ä≤‚Ä≤ 
Œ∏[0]
2l‚Ä≤‚Ä≤

Œ∏[0]
""""""
< 1
the next approximation is closer to the root than the starting value. Let
1
2
""""
l‚Ä≤‚Ä≤‚Ä≤ (Œ∏)
l‚Ä≤‚Ä≤ (Œ∏)
"""" ‚â§m
for all Œ∏ ‚ààI
with m > 0 and where I is a region near 5Œ∏, and suppose that
"""

5Œ∏ ‚àíŒ∏[0]
m
""" =
"""œµ[0]m
""" < 1.
Then, from (4.3),
"""œµ[t+1]""" =
""""""
l‚Ä≤‚Ä≤‚Ä≤ 
Œ∏[t]
2l‚Ä≤‚Ä≤

Œ∏[t]
""""""

œµ[t]2
‚â§m

œµ[t]2
so
"""mœµ[t+1]""" ‚â§

mœµ[t]2
,

4.2 Computation of Maximum Likelihood Estimates
165
and
"""œµ[t+1]""" ‚â§1
m

mœµ[t]2
= 1
m
"""mœµ[t]"""
2
‚â§1
m

mœµ[t‚àí1]22
= 1
m
"""mœµ[t‚àí1]"""
22
‚â§1
m

mœµ[t‚àí2]23
= 1
m
"""mœµ[t‚àí2]"""
23
‚â§1
m

mœµ[0]2t+1
.
(4.5)
Therefore, as t ‚Üí‚àû, then
""œµ[t+1]"" ‚Üí0, and the algorithm converges toward
5Œ∏. Similarly, let Œ∏[0] = 5Œ∏+œµ[0], Œ∏[1] = 5Œ∏+œµ[1], . . . , Œ∏[t] = 5Œ∏+œµ[t] be the sequence
of iterates produced by the Newton‚ÄìRaphson algorithm. If Œ∏[0] belongs to
the region I, this is equivalent to the statement

5Œ∏ ‚àí
"""œµ[0]""" ,5Œ∏ +
"""œµ[0]"""

‚ààI.
(4.6)
The next iterate, Œ∏[1], must fall in the set
I1 =

5Œ∏ ‚àí
"""œµ[1]""" ,5Œ∏ +
"""œµ[1]"""

.
(4.7)
Now, by (4.5),
5Œ∏ ‚àí
"""œµ[1]""" ‚â•5Œ∏ ‚àí1
m

mœµ[0]2
= 5Œ∏ ‚àím
"""œµ[0]"""
2
= 5Œ∏ ‚àí
"""mœµ[0]"""
"""œµ[0]""" .
By assumption,
""mœµ[0]"" < 1, so it must be true that
5Œ∏ ‚àí
"""œµ[1]""" ‚â•5Œ∏ ‚àí
"""œµ[0]""" .
(4.8)
Similarly,
5Œ∏ +
"""œµ[1]""" ‚â§5Œ∏ + 1
m

mœµ[0]2
= 5Œ∏ +
"""mœµ[0]"""
"""œµ[0]"""
so
5Œ∏ +
"""œµ[1]""" ‚â§5Œ∏ +
"""œµ[0]""" .
(4.9)
From (4.8) and (4.9)
5Œ∏ ‚àí
"""œµ[0]""" ‚â§5Œ∏ ‚àí
"""œµ[1]""" < 5Œ∏ +
"""œµ[1]""" ‚â§5Œ∏ +
"""œµ[0]""" .
Then the region of values I1 of the Ô¨Årst iterate is such that I1 ‚äÇI. Using
this argument repeatedly leads to
It ‚äÇIt‚àí1 ‚äÇIt‚àí2 ‚äÇ¬∑ ¬∑ ¬∑ ‚äÇI2 ‚äÇI1 ‚äÇI.
(4.10)
This indicates that all iterates stay within the initial region, and that as
t ‚Üí‚àû, I‚àûshould contain a single value, 5Œ∏, proving convergence of the
iterative scheme (provided iteration starts in the vicinity of the root).

166
4. Further Topics in Likelihood Inference
4.3
Evaluation of Hypotheses
A frequently encountered problem is the need for evaluating or testing a hy-
pothesis about the state of a biological system. In genetics, for example, in
a large randomly mated population, in the absence of mutation, migration,
or selection, genotypic frequencies are expected to remain constant gener-
ation after generation. This is called the Hardy‚ÄìWeinberg equilibrium law
(i.e., Crow and Kimura, 1970) and it may be of interest to test if this hy-
pothesis holds, given a body of data. A discussion of how such tests can
be constructed from a frequentist point of view based on a ML analysis
is presented below. The presentation is introductory, and only classical,
Ô¨Årst-order asymptotic results are discussed. This Ô¨Åeld has been undergoing
rapid developments. The reader is referred to books such as BarndorÔ¨Ä-
Nielsen and Cox (1994) and Severini (2000) for an account of the modern
approach to likelihood inference.
4.3.1
Likelihood Ratio Tests
The use of ratios between likelihoods obtained under diÔ¨Äerent models or hy-
potheses is widespread in genetics. The theoretical basis of tests constructed
from likelihood ratios was developed by Neyman and Pearson (1928). The
asymptotic distribution of twice the logarithm of a maximum likelihood
ratio statistic was derived by Wilks (1938). Cox and Hinkley (1974) and
Stuart and Ord (1991) give a heuristic account of this classical theory, and
this is followed closely here. In order to pose the problem in a suÔ¨Éciently
general framework, partition the parameter vector Œ∏ ‚ààŒò as
Œ∏ =

Œ∏1
Œ∏2

,
where Œ∏1 is an r√ó1 vector of parameters involved in the hypothesis, having
at least one element, and Œ∏2 is an s√ó1 vector of supplementary or nuisance
parameters, with zero or more components. Let p = r+s. Some hypotheses
to be contrasted can be formulated as
H0 : Œ∏1 = Œ∏10
versus
H1 : Œ∏1 Ã∏= Œ∏10,
where Œ∏10 is the value of the parameter under the null hypothesis. It is
important here to note the nesting structure of Œ∏: in one of the hypotheses
or models, elements of Œ∏ take a Ô¨Åxed value, but are free to vary in the
other.
The likelihood ratio test is based on the fact that the likelihood max-
imized under H1 must be at least as large as that under H0. This is so

4.3 Evaluation of Hypotheses
167
because there is always the possibility that there may be a higher likeli-
hood at values of the parameters violating the restrictions imposed by the
null hypothesis. Let
5Œ∏ =

5Œ∏1
5Œ∏2

be the unrestricted ML estimator, and let
,Œ∏ =
 Œ∏10
,Œ∏2

be the estimator under H0. In general, ,Œ∏2, which is the ML estimator of
Œ∏2, for Ô¨Åxed Œ∏1 = Œ∏10, will not coincide with 5Œ∏2. The ratio of maximized
likelihoods is
LR =
L

Œ∏10, ,Œ∏2|y

L

5Œ∏1, 5Œ∏2|y
 ,
0 ‚â§LR ‚â§1.
(4.11)
Values of LR close to 1 suggest plausibility of H0, because the restriction
imposed by the hypothesis does not lower the likelihood in an appreciable
manner. Since LR is a function of y, it is a random variable having some
sampling distribution. Now, because large values of LR suggest that H0
holds, one would reject the null hypothesis whenever the LR is below a
critical threshold t. For a test with rejection rate (under H0) Œ±, the value
of the threshold is given by the solution to the integral equation (Stuart
and Ord, 1991)
 tŒ±
0
p (LR) dLR = Œ±,
(4.12)
where p (LR) is the density of the distribution of the likelihood ratio un-
der H0. In general, this distribution is unrecognizable and must be ap-
proximated. However, there are instances in which this distribution can be
identiÔ¨Åed. Examples of the two situations follow.
Before embarking on these examples, we mention brieÔ¨Çy an alternative,
‚Äúpure likelihoodist‚Äù, approach to inference. Arguments in favor of this ap-
proach can be found in Edwards (1992) and in Royall (1997). Rather than
maximizing the likelihood and studying the distribution of the ML esti-
mator in conceptual replications, adherents to this school draw inferences
from the likelihood or from the likelihood ratio only, with the data Ô¨Åxed.
The justiÔ¨Åcation is to be found in what Hacking (1965) termed the law of
likelihood:
‚ÄúIf one hypothesis, H1, implies that a random variable X
takes the value x with probability f1 (x), while another hypoth-
esis, H2, implies that the probability is f2 (x), then the observa-
tion X = x is evidence supporting H1 over H2 if f1 (x) > f2 (x),

168
4. Further Topics in Likelihood Inference
and the likelihood ratio f1 (x) /f2 (x) measures the strength of
that evidence.‚Äù
Likelihood ratios close to 1 represent weak evidence, and extreme ratios
represent strong evidence. The problem is Ô¨Ånding a benchmark value k that
would give support to H1 over H2 analogous to the traditional p values
equal to 0.05 and 0.01. Values of k = 8 (representing ‚Äúfairly strong‚Äù) and
k = 32 (representing ‚Äústrong‚Äù) have been proposed and Royall (1997) gives
a rationale for these choices. We will not pursue this subject further. Instead
the reader is referred to the works mentioned above, where arguments in
favor of this approach can be found.
Example 4.1
Likelihood ratio in a Gaussian linear model
Suppose that a vector of observations is drawn from the multivariate dis-
tribution
y ‚àºN

XŒ≤, VœÉ2
,
where Œ≤ (p √ó 1) and œÉ2 are unknown parameters and V is a known ma-
trix. A hypothesis of interest may be H0: Œ≤ = Œ±, so œÉ2 is the incidental
parameter here. The likelihood under H1 is
L

Œ≤,œÉ2|y

= (2œÄ)‚àíN
2 ""VœÉ2""‚àí1
2 exp

‚àí1
2œÉ2 (y ‚àíXŒ≤)‚Ä≤ V‚àí1 (y ‚àíXŒ≤)

,
where N is the order of y. In the absence of any restriction, the ML esti-
mators can be found to be
5Œ≤ =

X‚Ä≤V‚àí1X
‚àí1 X‚Ä≤V‚àí1y
and
6
œÉ2 = 1
N

y ‚àíX5Œ≤
‚Ä≤
V‚àí1 
y ‚àíX5Œ≤

The likelihood under H0 is
L

œÉ2|Œ≤ = Œ±, y

= (2œÄ)‚àíN
2 ""VœÉ2""‚àí1
2 exp

‚àí1
2œÉ2 (y ‚àíXŒ±)‚Ä≤ V‚àí1 (y ‚àíXŒ±)

and the corresponding ML estimator of œÉ2 is
55œÉ
2 = 1
N (y ‚àíXŒ±)‚Ä≤ V‚àí1 (y ‚àíXŒ±) .
Note that
(y ‚àíXŒ±)‚Ä≤ V‚àí1 (y ‚àíXŒ±)
=

y ‚àíX5Œ≤ + X

5Œ≤ ‚àíŒ±
‚Ä≤
V‚àí1 
y ‚àíX5Œ≤ + X

5Œ≤ ‚àíŒ±

=

y ‚àíX5Œ≤
‚Ä≤
V‚àí1 
y ‚àíX5Œ≤

+

5Œ≤ ‚àíŒ±
‚Ä≤ 
X‚Ä≤V‚àí1X
 
5Œ≤ ‚àíŒ±

= N6
œÉ2 +

5Œ≤ ‚àíŒ±
‚Ä≤ 
X‚Ä≤V‚àí1X
 
5Œ≤ ‚àíŒ±

.

4.3 Evaluation of Hypotheses
169
Hence
55œÉ
2 = 6
œÉ2 +

5Œ≤ ‚àíŒ±
‚Ä≤ 
X‚Ä≤V‚àí1X
 
5Œ≤ ‚àíŒ±

N
.
Using (4.11) the ratio of maximized likelihoods is then
LR =
L

55œÉ
2|Œ≤ = Œ±, y

L

5Œ≤,6
œÉ2|y

=
Ô£Æ
Ô£ØÔ£∞1 +

5Œ≤ ‚àíŒ±
‚Ä≤ 
X‚Ä≤V‚àí1X
 
5Œ≤ ‚àíŒ±

N6
œÉ2
Ô£π
Ô£∫Ô£ª
‚àíN
2
.
Now, under the null hypothesis (Searle, 1971),

5Œ≤ ‚àíŒ±
‚Ä≤ 
X‚Ä≤V‚àí1X
 
5Œ≤ ‚àíŒ±

œÉ2
‚àºœá2
p
and, further, this random variable can be shown to be distributed indepen-
dently of
N6
œÉ2
œÉ2
‚àºœá2
N‚àíp.
Using these results, the likelihood ratio is expressible as
LR =

1 +
œÉ2œá2
p
œÉ2œá2
N‚àíp
‚àíN
2
=
Ô£Æ
Ô£∞1 +
p
œá2
p
p
(N ‚àíp)
œá2
N‚àíp
N‚àíp
Ô£π
Ô£ª
‚àíN
2
.
In addition (Searle, 1971),
Fp,N‚àíp =
œá2
p/p
œá2
N‚àíp/(N ‚àíp)
deÔ¨Ånes an F-distributed random variable with p and n ‚àíp degrees of free-
dom. Hence
LR =

1 +
p
(N ‚àíp)Fp,N‚àíp
‚àíN
2
.
Thus, we see that the LR decreases monotonically as F increases, so
Pr (LR ‚â¶tŒ±) = Pr (F ‚âßFŒ±,p,N‚àíp) ,
where FŒ±,p,N‚àíp is a critical value deÔ¨Åned by
 FŒ±,p,N‚àíp
0
p (Fp,N‚àíp) dF = 1 ‚àíŒ±.
Here the distribution of the LR statistic is known exactly.
‚ñ†

170
4. Further Topics in Likelihood Inference
Example 4.2
The Behrens‚ÄìFisher problem
Suppose samples are drawn from two populations having distinct variances
œÉ2
1 and œÉ2
2. The sampling model is

y1
y2

‚àºN


X1Œ≤1
X2Œ≤2

,

I1œÉ2
1
0
0
I2œÉ2
2

.
A hypothesis of interest may be: H0: Œ≤1 = Œ≤2 = Œ≤. Under H1 : Œ≤1 Ã∏= Œ≤2
the likelihood is
L

Œ≤1, Œ≤2, œÉ2
1, œÉ2
2|y

‚àù
2
-
i=1

œÉ2
i
‚àíNi
2 exp

‚àí1
2œÉ2
i
(yi‚àíXiŒ≤i)‚Ä≤ (yi‚àíXiŒ≤i)

,
where Ni is the order of the data vector yi. The maximizers of the unre-
stricted likelihood are
5Œ≤i = (X‚Ä≤
iXi)‚àí1 Xi
‚Ä≤yi,
i = 1, 2,
6
œÉ2 = 1
Ni

yi ‚àíXi5Œ≤
‚Ä≤ 
yi ‚àíXi5Œ≤

,
i = 1, 2,
and the maximized likelihood is
L

5Œ≤1, 5Œ≤2, 6
œÉ2
1, 6
œÉ2
2|y

‚àù
2
-
i=1
6
œÉ2
i
‚àíNi
2
exp

‚àíNi
2

.
Under H0:
L

Œ≤, œÉ2
1, œÉ2
2|y

‚àù
2
-
i=1

œÉ2
i
‚àíNi
2 exp

‚àí1
2œÉ2
i
(yi ‚àíXiŒ≤)‚Ä≤ (yi ‚àíXiŒ≤)

.
After diÔ¨Äerentiation of the log-likelihood with respect to the parameters,
setting the resulting equations to zero gives the system
55œÉ
2
i = 1
Ni

yi ‚àíXi55Œ≤
‚Ä≤ 
yi ‚àíXi55Œ≤

,
i = 1, 2,
55Œ≤ =
 2

i=1
X‚Ä≤
iXi
55œÉ
2
i
‚àí1  2

i=1
Xi‚Ä≤yi
55œÉ
2
i

which is not explicit in 55Œ≤, so it must be solved iteratively. The likelihood
ratio is
LR =
2
-
i=1
55œÉ
2
i
6
œÉ2
‚àíNi
2
=
2
-
i=1
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞

yi ‚àíXi55Œ≤
‚Ä≤ 
yi ‚àíXi55Œ≤


yi ‚àíXi5Œ≤i
‚Ä≤ 
yi ‚àíXi5Œ≤i

Ô£π
Ô£∫Ô£∫Ô£∫Ô£ª
‚àíNi
2
.
Because 55Œ≤ cannot be written explicitly, the distribution of the LR is diÔ¨Écult
or impossible to arrive at without using approximations.
‚ñ†

4.3 Evaluation of Hypotheses
171
Approximating the Distribution of the Likelihood Ratio
The derivation of the asymptotic distribution of the likelihood ratio test
presented below is based on Ô¨Årst-order asymptotic results where Taylor
expansions play a central role. Here it is important to respect the conditions
for the diÔ¨Äerentiability of functions and the rate of convergence of the
terms that are ignored relative to the rate of convergence of those that are
kept. The reader is referred to Lehmann and Casella (1998) for a careful
treatment of this subject.
In (4.11) the likelihood ratio was deÔ¨Åned as
LR =
L

Œ∏10, ,Œ∏2|y

L

5Œ∏1, 5Œ∏2|y
 .
(4.13)
Minus twice the log-likelihood ratio (sometimes called the deviance) is equal
to
‚àí2 ln LR = ‚àí2

l

Œ∏10, ,Œ∏2|y

‚àíl

5Œ∏1, 5Œ∏2|y

= 2

l

5Œ∏1, 5Œ∏2|y

‚àíl

Œ∏10, ,Œ∏2|y

.
(4.14)
In these expressions, ,Œ∏2 is the ML estimator of Œ∏2 under H0 and

5Œ∏1, 5Œ∏2

are the ML estimators of Œ∏ under H1.
The asymptotic properties of (4.14) are derived as follows. First, ex-
pand the log-likelihood of Œ∏, l (Œ∏1, Œ∏2|y), in a Taylor series about the ML
estimates

5Œ∏1, 5Œ∏2

. Since Ô¨Årst derivatives evaluated at the ML estimates

5Œ∏1, 5Œ∏2

are zero, this yields
l (Œ∏1, Œ∏2|y) ‚âàl

5Œ∏1, 5Œ∏2|y

+1
2

Œ∏1 ‚àí5Œ∏1
‚Ä≤ ‚àÇ2l (Œ∏1, Œ∏2|y)
‚àÇŒ∏1 ‚àÇŒ∏‚Ä≤
1
""""
Œ∏=Œ∏

Œ∏1 ‚àí5Œ∏1

+1
2

Œ∏2 ‚àí5Œ∏2
‚Ä≤ ‚àÇ2l (Œ∏1, Œ∏2|y)
‚àÇŒ∏2 ‚àÇŒ∏‚Ä≤
2
""""
Œ∏=Œ∏

Œ∏2 ‚àí5Œ∏2

+

Œ∏1 ‚àí5Œ∏1
‚Ä≤ ‚àÇ2l (Œ∏1, Œ∏2|y)
‚àÇŒ∏1 ‚àÇŒ∏‚Ä≤
2
""""
Œ∏=Œ∏

Œ∏2 ‚àí5Œ∏2

.
(4.15)
DeÔ¨Åne
‚àí‚àÇ2l (Œ∏i, Œ∏j|y)
‚àÇŒ∏i ‚àÇŒ∏‚Ä≤
j
"""""
Œ∏=Œ∏
= Jij

5Œ∏

,
(4.16)
which is the ijth block of the observed information matrix J

5Œ∏

. (This
should not be confused with a similar symbol used to deÔ¨Åne the Jacobian

172
4. Further Topics in Likelihood Inference
in (2.4) and (2.30)). We will need
Ô£Æ
Ô£∞J11 
5Œ∏

J12 
5Œ∏

J21 
5Œ∏

J22 
5Œ∏

Ô£π
Ô£ª=
Ô£Æ
Ô£∞J11

5Œ∏

J12

5Œ∏

J21

5Œ∏

J22

5Œ∏

Ô£π
Ô£ª
‚àí1
=

J

5Œ∏
‚àí1
,
which is an expression for the asymptotic covariance matrix of 5Œ∏. Substi-
tuting (4.16) in (4.15) gives
l (Œ∏1, Œ∏2|y) ‚âàl

5Œ∏1, 5Œ∏2|y

‚àí1
2

5Œ∏1 ‚àíŒ∏1
‚Ä≤
J11

5Œ∏
 
5Œ∏1 ‚àíŒ∏1

‚àí1
2

5Œ∏2 ‚àíŒ∏2
‚Ä≤
J22

5Œ∏
 
5Œ∏2 ‚àíŒ∏2

‚àí

5Œ∏1 ‚àíŒ∏1
‚Ä≤
J12

5Œ∏
 
5Œ∏2 ‚àíŒ∏2

.
(4.17)
Under H1, (4.17) evaluated at 5Œ∏ is equal to l

5Œ∏1, 5Œ∏2|y

.
The log-likelihood (4.17) under H0, which is the log-likelihood for Œ∏2,
with Œ∏1 Ô¨Åxed at Œ∏10, is equal to
l (Œ∏10, Œ∏2|y) ‚âàl

5Œ∏1, 5Œ∏2|y

‚àí1
2

5Œ∏1 ‚àíŒ∏10
‚Ä≤
J11

5Œ∏
 
5Œ∏1 ‚àíŒ∏10

‚àí1
2

5Œ∏2 ‚àíŒ∏2
‚Ä≤
J22

5Œ∏
 
5Œ∏2 ‚àíŒ∏2

‚àí

5Œ∏1 ‚àíŒ∏10
‚Ä≤
J12

5Œ∏
 
5Œ∏2 ‚àíŒ∏2

.
(4.18)
It is easy to obtain ,Œ∏2, the ML estimator of Œ∏2, given Œ∏1 = Œ∏10, from (4.18).
Taking partial derivatives with respect to Œ∏2, setting these equal to zero
and solving for Œ∏2 yields
,Œ∏2 = 5Œ∏2 +

J22

5Œ∏
‚àí1
J21

5Œ∏
 
5Œ∏1 ‚àíŒ∏10

,
(4.19)
whose asymptotic covariance matrix is equal to
V ar

,Œ∏2

= J22 
5Œ∏

‚àíJ21 
5Œ∏
 
J11 
5Œ∏
‚àí1
J12 
5Œ∏

=

J22

5Œ∏
‚àí1
.
After some algebra, the log-likelihood (4.18), evaluated at Œ∏2 = ,Œ∏2, can be
shown to be equal to
l

Œ∏10, ,Œ∏2|y

‚âàl

5Œ∏1, 5Œ∏2|y

‚àí1
2

5Œ∏1 ‚àíŒ∏10
‚Ä≤ 
J11

5Œ∏

‚àíJ12

5Œ∏
 
J22

5Œ∏
‚àí1
J21

5Œ∏

√ó

5Œ∏1 ‚àíŒ∏10

,
(4.20)

4.3 Evaluation of Hypotheses
173
which is not a function of Œ∏2. Recalling that
J11

5Œ∏

‚àíJ12

5Œ∏
 
J22

5Œ∏
‚àí1
J21

5Œ∏

=

J11 
5Œ∏
‚àí1
,
(4.20) can be written
l

Œ∏10, ,Œ∏2|y

‚âàl

5Œ∏1, 5Œ∏2|y

‚àí1
2

5Œ∏1 ‚àíŒ∏10
‚Ä≤ 
J11 
5Œ∏
‚àí1 
5Œ∏1 ‚àíŒ∏10

.
(4.21)
It follows that ‚àí2 ln LR has the form
2

l

5Œ∏1, 5Œ∏2|y

‚àíl

Œ∏10, ,Œ∏2|y

‚âà

5Œ∏1 ‚àíŒ∏10
‚Ä≤ 
J11 
5Œ∏
‚àí1 
5Œ∏1 ‚àíŒ∏10

= ‚àön

5Œ∏1 ‚àíŒ∏10
‚Ä≤ 
nJ11 
5Œ∏
‚àí1 ‚àön

5Œ∏1 ‚àíŒ∏10

.
(4.22)
As n ‚Üí‚àû, ‚àön

5Œ∏1 ‚àíŒ∏10

converges to N

0, I11 (Œ∏)

and nJ11 
5Œ∏

to
I11 (Œ∏), the covariance matrix of the limiting marginal distribution of

5Œ∏1 ‚àíŒ∏10

.
Therefore, as n ‚Üí‚àû, ‚àí2 ln LR converges to a chi-square distribution, with
noncentrality parameter equal to zero under H0, and with r = dim (Œ∏1)
degrees of freedom
‚àí2 ln (LR) |H0 ‚àºœá2
r.
(4.23)
Note that this limiting chi-square distribution does not involve the nuisance
parameter Œ∏2.
The test criterion decreases monotonically as the LR increases. Thus, if
œá2
r exceeds a certain critical value, this corresponds to a signiÔ¨Åcant lowering
of the likelihood under H0 and, thus, to rejection. If H1 holds
‚àí2 ln (LR) |H1 ‚àºœá2
r,Œª
(4.24)
where Œª = [Œ∏1 ‚àíŒ∏10]‚Ä≤ 
J11 
5Œ∏
‚àí1
[Œ∏1 ‚àíŒ∏10] is a noncentrality parameter
(this is clearly null when Œ∏1 = Œ∏10). When s > 0 (the number of nuisance
parameters) the computation of LR requires two maximizations: one under
H0 and another under H1.
The developments in this section make it clear that a comparison be-
tween two models by means of the likelihood ratio test assumes a common
parameter Œ∏ that is allowed to take speciÔ¨Åc values under either model.
The test does not make sense otherwise. There must be a nested structure
whereby the reduced model is embedded under the unrestricted model. If
this requirement is not satisÔ¨Åed, the asymptotic theory described does not

174
4. Further Topics in Likelihood Inference
hold. A modiÔ¨Åcation of the likelihood ratio criterion for dealing with tests
involving nonnested models has been proposed by Cox (1961, 1962); more
recent and important generalizations can be found in Vuong (1989) and Lo
et al. (2001).
The asymptotic results presented above assume asymptotic normality
of the ML estimator. Recently, Fan et al. (2000) provided a proof of the
asymptotic chi-square distribution of the log-likelihood ratio statistic, for
cases where the ML estimator is not asymptotically normal.
As a Ô¨Ånal warning, with many nuisance parameters, notably when their
number is large relative to the number of observations, use of the above
theory to discriminate between models can give misleading results.
Power of the Likelihood Ratio Test
When designing experiments, it is important to assess the power of the test.
This is the probability of rejecting H0, given that H1 is true or, equivalently,
of accepting that the parameter value is Œ∏1, instead of Œ∏10. For example, a
genetic experiment may be carried out to evaluate linkage between a genetic
marker and a QTL. A design question could be: How many individuals
need to be scored such that the hypothesis that the recombination rate
is, say, equal to 10% (Œ∏10) , is rejected with probability P? Under H0, the
noncentrality parameter is 0, and the probability of rejection for a test of
size Œ± would be computed using (4.12) and (4.23) as
Pr (rejecting H0|H0) =
 ‚àû
tŒ±
p

œá2
1

dœá2
1 = Œ±,
(4.25)
where tŒ± is a critical value of a central chi-square distribution on one degree
of freedom. If H1 holds, the power would be computed as
Pr (rejecting H0|H1) =
 ‚àû
tŒ±
p

œá2
1,Œª

dœá2
1,Œª.
(4.26)
For the linkage experiment considered here, the noncentrality parameter
would be Œª = (Œ∏ ‚àí0.10)2 I (Œ∏) and the calculation can be carried out for
any desired Œ∏.
Example 4.3
Likelihood ratio test of Hardy‚ÄìWeinberg proportions
Consider a segregating locus with two alleles: B and b. Suppose there are
three observable phenotypes corresponding to individuals with genotypes
BB, Bb, and bb. A random sample is drawn from a population. The data
consist of the vector y‚Ä≤ = [yBB, yBb, ybb], where yBB, yBb, and ybb are the
observed number of individuals with genotype BB, Bb, and bb, respectively,
in a sample of size n = yBB + yBb + ybb. Let the unknown probabilities of
drawing a BB, Bb, or bb individual be Œ∏BB, Œ∏Bb, and Œ∏bb, respectively. If
the sample size is Ô¨Åxed by design, and individuals are drawn independently
and with replacement, it may be reasonable to compute the probability of

4.3 Evaluation of Hypotheses
175
observing y using the multinomial distribution. This, of course, would ig-
nore knowledge about family aggregation that may exist in the population.
For example, if some parents are both bb, their progeny must be bb, nec-
essarily, so the random sampling model discussed here would not take this
into account. Under multinomial sampling, the probability of observing y
is
p (yBB, yBb, ybb) =
y!
yBB! yBb! ybb! (Œ∏BB)yBB (Œ∏Bb)yBb (Œ∏bb)ybb
so the likelihood is
L (Œ∏BB, Œ∏Bb, Œ∏bb) ‚àù(Œ∏BB)yBB (Œ∏Bb)yBb (Œ∏bb)ybb .
The model imposes the parametric restriction Œ∏BB +Œ∏Bb +Œ∏bb = 1 so there
are only two ‚Äúfree‚Äù parameters governing the distribution. Also, ybb =
n ‚àíyBB ‚àíyBb, say. The likelihood is then reexpressible as
L (Œ∏BB, Œ∏Bb) ‚àùŒ∏yBB
BB Œ∏yBb
Bb (1 ‚àíŒ∏BB ‚àíŒ∏Bb)n‚àíyBB‚àíyBb .
DiÔ¨Äerentiation of the log-likelihood with respect to the two Œ∏‚Ä≤s, and setting
the derivatives to zero, gives the relationships
5Œ∏BB =
yBB
n ‚àíyBB ‚àíyBb

1 ‚àí5Œ∏BB ‚àí5Œ∏Bb

,
5Œ∏Bb =
yBb
n ‚àíyBB ‚àíyBb

1 ‚àí5Œ∏BB ‚àí5Œ∏Bb

.
Summing these two equations yields
5Œ∏BB + 5Œ∏Bb =
yBB + yBb
n ‚àíyBB ‚àíyBb

1 ‚àí5Œ∏BB ‚àí5Œ∏Bb

.
Because of the parametric relationship (probabilities of all possible disjoint
events sum to one), the invariance property of the ML estimates yields
5Œ∏bb = 1 ‚àí5Œ∏BB ‚àí5Œ∏Bb. Using this above
1 ‚àí5Œ∏bb =
yBB + yBb
n ‚àíyBB ‚àíyBb
5Œ∏bb,
from which: 5Œ∏bb = ybb/n. Similarly, it can be established that 5Œ∏BB = yBB/n
and 5Œ∏Bb = yBb/n. In the absence of restrictions (other than those imposed
by probability theory) on the values of the parameters, i.e., under H1, the
maximized likelihood is
L

5Œ∏BB,5Œ∏Bb,5Œ∏bb

‚àù
yBB
n
yBB yBb
n
yBb ybb
n
ybb .
A genetic hypothesis (denoted as H0 here) is that the population is in
Hardy‚ÄìWeinberg equilibrium (Crow and Kimura, 1970; Weir, 1996). Under

176
4. Further Topics in Likelihood Inference
H0, the genotypic distribution is expected to obey the parametric relation-
ship
Œ∏BB =

Œ∏BB + 1
2Œ∏Bb
2 = Œ∏2
B,
Œ∏Bb = 2Œ∏B (1 ‚àíŒ∏B) ,
Œ∏bb = (1 ‚àíŒ∏B)2 ,
where Œ∏B = Œ∏BB + Œ∏Bb/2 is called the frequency of allele B in the popula-
tion. This is the total probability of drawing an allele B at this locus, that
is, the sum of:
a) the probability of drawing an individual with genotype BB (Œ∏BB) times
the probability of obtaining a B from BB, which is a certain event; plus
b) the probability of drawing a Bb (Œ∏Bb) , times the probability of obtaining
B from a heterozygote
 1
2

.
Hence, under Hardy‚ÄìWeinberg equilibrium the probability distribution of
the observations is governed by a single parameter, Œ∏B. The likelihood un-
der H0 is
L (Œ∏B) ‚àù

Œ∏2
B
yBB [Œ∏B (1 ‚àíŒ∏B)]yBb 
(1 ‚àíŒ∏B)2ybb .
DiÔ¨Äerentiation of the log-likelihood with respect to Œ∏B and setting to zero
gives an explicit solution that can be expressed as a function of the ML
estimators under H1:
55Œ∏B = 2yBB + yBb
2y
= 5Œ∏BB + 1
2
5Œ∏Bb.
Because of the parametric relationships, under H0 the ML estimators of
the probabilities of genotypes in the population (or genotypic frequencies,
in the population genetics literature) are:
55Œ∏BB = 55Œ∏
2
B =

5Œ∏BB + 1
25Œ∏Bb
2
,
55Œ∏Bb = 2

5Œ∏BB + 1
25Œ∏Bb
 
5Œ∏bb + 1
25Œ∏Bb

,
55Œ∏bb =

5Œ∏bb + 1
25Œ∏Bb
2
.

4.3 Evaluation of Hypotheses
177
Using (4.23), the statistic for the likelihood ratio test of the hypothesis that
the population is in Hardy‚ÄìWeinberg equilibrium, is
‚àí2 ln (LR) = ‚àí2 ln
Ô£±
Ô£¥
Ô£≤
Ô£¥
Ô£≥
Ô£Æ
Ô£ØÔ£∞

5Œ∏BB + 1
25Œ∏Bb
2
5Œ∏BB
Ô£π
Ô£∫Ô£ª
yBB
√ó
Ô£Æ
Ô£∞
2

5Œ∏BB + 1
25Œ∏Bb
 
5Œ∏bb + 1
25Œ∏Bb

5Œ∏Bb
Ô£π
Ô£ª
yBb Ô£Æ
Ô£ØÔ£∞

5Œ∏bb + 1
25Œ∏Bb
2
5Œ∏bb
Ô£π
Ô£∫Ô£ª
yBb Ô£º
Ô£¥
Ô£¥
Ô£Ω
Ô£¥
Ô£¥
Ô£æ
.
Asymptotically, this has a central œá2
1 distribution. In the general setting
described in Section 4.3, the parameter vector Œ∏ has r + s identiÔ¨Åable pa-
rameters under H1; here r+s = 2 instead of 3 because there is a redundant
parameter. Under H0 there is a single estimable parameter. This provides
the basis for the single degree of freedom of the distribution of the LR
statistic.
It is possible to move from the unrestricted to the restricted model by
setting a single function of parameters to zero. For example, letting
d = Œ∏Bb ‚àí2

Œ∏BB + 1
2Œ∏Bb
 
Œ∏bb + 1
2Œ∏Bb

it follows that a test of the Hardy‚ÄìWeinberg equilibrium is equivalent to
a test of the hypothesis H0: d = 0. Hence, the models under the two hy-
potheses diÔ¨Äer by a single parameter which, upon setting to 0, can produce
the null model as a ‚Äúnested‚Äù version of the unrestricted model.
‚ñ†
4.3.2
ConÔ¨Ådence Regions
The asymptotic distribution of ML estimators enables one to obtain inter-
val inferences about the ‚Äútrue‚Äù value of the parameters. These inferences
are expressed in terms of conÔ¨Ådence regions. There is a close relationship
between the techniques needed here and those described for the evaluation
of hypotheses. Hence, construction of conÔ¨Ådence regions is dealt with only
brieÔ¨Çy.
Before we do so, a comment about the interpretation of conÔ¨Ådence inter-
vals is in order. ConÔ¨Ådence intervals are deÔ¨Åned in terms of the distribution
of the random variable y, the data, and as such the conÔ¨Ådence interval is
also a random variable. Given a realization of y, the conÔ¨Ådence interval
will either contain the true value of the parameter or not. If the conÔ¨Ådence
region is 1 ‚àíŒ± (see below), then under repeated sampling, 100 (1 ‚àíŒ±) %
of the intervals will contain the true value of the parameter. Thus the con-
Ô¨Ådence interval is not a probability statement about the parameter but
about the random interval.

178
4. Further Topics in Likelihood Inference
Given that a model or a hypothesis is true, the score has, asymptotically,
a normal distribution with null mean and covariance matrix I (Œ∏) . Hence,
a conÔ¨Ådence region of size 1‚àíŒ± can be constructed from the property that
Pr

l‚Ä≤ (Œ∏)‚Ä≤ I‚àí1 (Œ∏) l‚Ä≤ (Œ∏) ‚â§tŒ±

= 1 ‚àíŒ±,
(4.27)
where tŒ± is the critical value of a œá2
p random variable. Values of Œ∏ outside
the region are viewed as being unlikely, but not in a probabilistic sense,
because values of the parameters cannot be assigned probabilities. Note
that if the statement involves a single parameter, the conÔ¨Ådence region can
be formed from a standard normal distribution, this being so because the
random variable
[l‚Ä≤ (Œ∏)]2
I (Œ∏)
‚àºœá2
1
so
l‚Ä≤ (Œ∏)
>
I (Œ∏)
‚àºN (0, 1) .
Another way of constructing conÔ¨Ådence regions is based on the asymp-
totic distribution of the ML estimator, as given in (3.66) of the previous
chapter. The conÔ¨Ådence region stems from the fact that, asymptotically,
the distribution under Œ∏ = Œ∏0 is

5Œ∏ ‚àíŒ∏0
‚Ä≤
I (Œ∏0)

5Œ∏ ‚àíŒ∏0

‚àºœá2
p.
(4.28)
In the single parameter case, the region is deÔ¨Åned by the appropriate Œ±/2
percentiles of a N (0, 1) distribution. This is so because
5Œ∏ ‚àíŒ∏0
>
I‚àí1 (Œ∏0)
=

5Œ∏ ‚àíŒ∏0
 >
I (Œ∏0) ‚àºN (0, 1) .
Similarly, a conÔ¨Ådence region can be developed from the asymptotic dis-
tribution of the likelihood ratio as given in (4.23). A conÔ¨Ådence region
here is a set of values of Œ∏ in the neighborhood of the maximum (Cox
and Snell, 1989). With a single parameter, the asymptotic distribution
of the likelihood ratio statistic ‚àí2 ln (LR) is œá2
1 under H0. Noting that
a œá2
1 random variable can be generated by squaring a standard normal
deviate, an equivalent form of generating a conÔ¨Ådence region is to use
>
‚àí2 ln (LR) ‚àºN (0, 1) if the ML estimator is larger than the null value
Œ∏0, and ‚àí
>
‚àí2 ln (LR) ‚àºN (0, 1) otherwise. This is because if x and ‚àíx
are realized values from a N (0, 1) process, these two generate the same
realized value from a œá2
1 distribution, in a two-to-one mapping.
In passing, we mention that the ‚Äúpure likelihoodist‚Äù computes conÔ¨Ådence
regions based on quantiles derived from the likelihood ratio directly, with-
out invoking its distribution over replications of the data. A tutorial on the
topic can be found in Meeker and Escobar (1995).
This section Ô¨Ånishes with a brief description of two tests that are also
based on classical, Ô¨Årst-order asymptotic likelihood theory.

4.3 Evaluation of Hypotheses
179
4.3.3
Wald‚Äôs Test
As stated in the previous chapter, in large samples, the distribution of the
ML estimator can be written (informally) as 5Œ∏ ‚àºN

Œ∏0, I‚àí1 (Œ∏0)

, where
Œ∏0 is the true value of the parameter Œ∏. Then, asymptotically, the quadratic
form
ŒªW =

5Œ∏1 ‚àíŒ∏10
‚Ä≤ 
I11 
5Œ∏
‚àí1 
5Œ∏1 ‚àíŒ∏10

‚àºœá2
r,
(4.29)
where 5Œ∏1 is the ML estimator of Œ∏1 under H1, and œá2
r is a central chi-square
random variable with r degrees of freedom (the number of elements in Œ∏1).
Result (4.29) is an immediate application of (1.101). If ŒªW is ‚Äútoo large‚Äù,
the hypothesis is rejected. Approximate conÔ¨Ådence regions can be readily
constructed from (4.29).
In (4.29), I11 
5Œ∏

is the top left element of the inverse of Fisher‚Äôs expected
information matrix. That is
I

5Œ∏

= ‚àíE

‚àÇ2l
‚àÇŒ∏ ‚àÇŒ∏‚Ä≤

Œ∏=Œ∏
=
Ô£Æ
Ô£∞I11

5Œ∏

I12

5Œ∏

I21

5Œ∏

I22

5Œ∏

Ô£π
Ô£ª,
(4.30)
and
Ô£Æ
Ô£∞I11

5Œ∏

I12

5Œ∏

I21

5Œ∏

I22

5Œ∏

Ô£π
Ô£ª
‚àí1
=
Ô£Æ
Ô£∞I11 
5Œ∏

I12 
5Œ∏

I21 
5Œ∏

I22 
5Œ∏

Ô£π
Ô£ª.
Therefore,

I11 
5Œ∏
‚àí1
= I11

5Œ∏

‚àíI12

5Œ∏
 
I22

5Œ∏
‚àí1
I21

5Œ∏

.
Rather than using the inverse of Fisher‚Äôs information, other estimators
of I11 (Œ∏), which are consistent under H0, can be used. Thus, an alterna-
tive form of Wald‚Äôs statistic uses the inverse of the observed information,
J11 
5Œ∏

, in (4.29), which retrieves (4.22). This makes it clear that the Wald
statistic is based on a quadratic approximation to ‚àí2 ln LR.
Due to the relationship between a chi-squared variable and a normal
variable, for scalar Œ∏, the 100 (1 ‚àíŒ±) % conÔ¨Ådence interval based on the
Wald statistic is given by the well known formula
5Œ∏ ¬± zŒ±/2I‚àí1/2 
5Œ∏

.
4.3.4
Score Test
The score test (or Lagrange multiplier test, as called by econometricians) is
also based on a quadratic approximation to the log-likelihood ratio. It was

180
4. Further Topics in Likelihood Inference
proposed by Rao (1947) and uses the asymptotic properties of the score
(3.63). The score statistic is
l‚Ä≤
Œ∏1

Œ∏10, ,Œ∏2
‚Ä≤
I11 
Œ∏10, ,Œ∏2

l‚Ä≤
Œ∏1

Œ∏10, ,Œ∏2

,
(4.31)
which, in view of (3.63) and of (1.101), follows a œá2
r distribution. The null
hypothesis is rejected for large values of (4.31). In (4.31),
l‚Ä≤
Œ∏1

Œ∏10, ,Œ∏2

= ‚àÇl (Œ∏1, Œ∏2|y)
‚àÇŒ∏1
""""Œ∏1=Œ∏10
Œ∏2=Œ∏2
.
Contrary to the likelihood ratio test and the Wald test, (4.31) requires only
one maximization (to compute the ML estimate under H0, ,Œ∏2). Approxi-
mate conÔ¨Ådence regions can also be constructed from (4.31).
The three tests described here (likelihood ratio, Wald‚Äôs, and score test)
are asymptotically equivalent and are all Ô¨Årst-order approximations. Which
test to use depends on the situation (Lehmann, 1999), although Meeker and
Escobar (1995) and Pawitan (2000) argue in favor of the likelihood ratio.
There are two reasons. First, the Wald and score statistics are convenient
only if the log-likelihood is well-approximated by a quadratic function.
Second, the Wald statistic has an important disadvantage relative to the
likelihood ratio test: it is not transformation invariant. However, asymp-
totically, when the likelihood is a quadratic function of the parameter, the
tests are equivalent. To illustrate this in the case of a single parameter,
write the scaled log-likelihood (i.e., the logarithm of (3.21)) as
l (Œ∏) = k

Œ∏ ‚àí5Œ∏
2
.
Therefore,
dl (Œ∏)
dŒ∏
= 2k

Œ∏ ‚àí5Œ∏

and
d2l (Œ∏)
d2Œ∏
= ‚àíI (Œ∏) = 2k.
Assume that under the null hypothesis, H0 : Œ∏ = Œ∏0 and the alternative
hypothesis is H1 : Œ∏ Ã∏= Œ∏0. Then the test based on the likelihood ratio is
2

l

5Œ∏

‚àíl (Œ∏0)

= 2

k

5Œ∏ ‚àí5Œ∏
2
‚àík

Œ∏0 ‚àí5Œ∏
2
= ‚àí2k

Œ∏0 ‚àí5Œ∏
2
.
The Wald test is obtained from (4.29):

5Œ∏ ‚àíŒ∏0
2
I‚àí1 (Œ∏)Œ∏=Œ∏ = ‚àí2k

Œ∏0 ‚àí5Œ∏
2
.

4.4 Nuisance Parameters
181
The score test is obtained from (4.31):
[dl (Œ∏0) /dŒ∏]2
I (Œ∏0)
=

2k

Œ∏0 ‚àí5Œ∏
2
‚àí2k
= ‚àí2k

Œ∏0 ‚àí5Œ∏
2
.
4.4
Nuisance Parameters
Recall that a statistical model typically includes parameters of primary in-
ferential interest (denoted as Œ∏1 here), plus additional parameters (Œ∏2) that
are necessary to index completely the distributions of all random variables
entering into a probability model. The additional parameters are called
nuisance parameters. As pointed out by Edwards (1992), one would wish
to make statements about the values of Œ∏1 without reference to the values
of Œ∏2. If Œ∏2 were known, there would be no diÔ¨Éculty, as one would write the
likelihood of Œ∏1, with Œ∏2 replaced by its true value. In the absence of such
knowledge, a possibility would be to infer Œ∏1 at each of a series of possible
values of Œ∏2. This is unsatisfactory, because it does not give guidance about
the plausibility of each of the values of the nuisance parameters. A more
appealing option is to estimate Œ∏1 and Œ∏2 jointly, as if both were of pri-
mary interest, using the machinery for analysis of likelihoods developed so
far. Unfortunately, when making inferences about Œ∏1, this modus operandi
does not take into account the fact that part of the information contained
in the data must be used to estimate Œ∏2. For example, in a nonlinear re-
gression model, the vector of parameters of the expectation function may
be of primary interest, with the residual variance playing the role of a nui-
sance parameter. Another example is that of a model with two means and
two variances, where the inferential interest centers on, say, œÉ2/¬µ1, with
the remaining parameters acting as nuisances. In any case, it is not ob-
vious how to deal with nuisance parameters in likelihood-based inference
and many solutions have been proposed. This area has been undergoing
rapid development (KalbÔ¨Çeisch and Sprott, 1970, 1973; BarndorÔ¨Ä-Nielsen,
1986, 1991; Cox and Reid, 1987; McCullagh and Nelder, 1989; Efron, 1993;
BarndorÔ¨Ä-Nielsen and Cox, 1994; Severini, 1998). Useful reviews can be
found in Reid (1995) and Reid (2000). The recent book of Severini (2000)
is a good starting point to study modern likelihood methods. Here we only
discuss the use of marginal and proÔ¨Åle likelihoods.
The subject is introduced below with an example that illustrates the loss
of eÔ¨Éciency in the estimation of parameters of interest due to the presence
of nuisance parameters. The material is taken from Lehmann (1999).

182
4. Further Topics in Likelihood Inference
4.4.1
Loss of EÔ¨Éciency Due to Nuisance Parameters
Consider a model depending on p parameters Œ∏ with elements
Œ∏1, . . . , Œ∏p.
Let I (Œ∏) denote the information matrix, with typical element Iij (Œ∏) and
with inverse [I (Œ∏)]‚àí1 with typical element Iij (Œ∏). Assuming that the nec-
essary regularity conditions hold, then from (3.65),
‚àön

5Œ∏1 ‚àíŒ∏1

, . . . , ‚àön

5Œ∏k ‚àíŒ∏p

has a joint multivariate distribution with mean (0, 0, . . . , 0)‚Ä≤ and covariance
matrix [I (Œ∏)]‚àí1. In particular,
‚àön

5Œ∏j ‚àíŒ∏j

‚ÜíN

0, Ijj (Œ∏)

,
where Ijj (Œ∏) is the element in row j and column j of [I (Œ∏)]‚àí1. On the
other hand, if Œ∏1, . . . , Œ∏j‚àí1, Œ∏j+1, . . . , Œ∏p are known, from (3.55),
‚àön

5Œ∏j ‚àíŒ∏j

‚ÜíN

0, [Ijj (Œ∏)]‚àí1
.
The loss in eÔ¨Éciency due to the presence of nuisance parameters can be
studied via the relationship between Ijj (Œ∏) and [Ijj (Œ∏)]‚àí1. Consider the
case p = 2 and j = 1. The inverse of the 2 √ó 2 matrix I (Œ∏) is
[I (Œ∏)]‚àí1 =

I22 (Œ∏) /‚àÜ
‚àíI12 (Œ∏) /‚àÜ
‚àíI12 (Œ∏) /‚àÜ
I11 (Œ∏) /‚àÜ

,
where ‚àÜ= I11 (Œ∏) I22 (Œ∏) ‚àí[I12 (Œ∏)]2. Since I (Œ∏) is positive deÔ¨Ånite, ‚àÜis
positive. This implies that I11 (Œ∏) I22 (Œ∏) ‚â•‚àÜwhich is equivalent to
I11 (Œ∏) = I22 (Œ∏)
‚àÜ
‚â•[I11 (Œ∏)]‚àí1 ,
(4.32)
with equality holding when I12 (Œ∏) = 0. The conclusion is that, even in an
asymptotic scenario, unless the estimators are asymptotically independent,
the asymptotic variance of the estimator of the parameter of interest is
larger in models containing unknown nuisance parameters.
4.4.2
Marginal Likelihoods
The marginal likelihood approach is based on the construction of a likeli-
hood for the parameters of interest from a ‚Äúsuitably chosen subset of the
data vector‚Äù (McCullagh and Nelder, 1989). It is desirable to choose this
subset to be as large as possible, to minimize any loss of information. As

4.4 Nuisance Parameters
183
indicated by McCullagh and Nelder (1989), the method does not always
work satisfactorily because general rules do not seem to exist. Even in cases
where it appears to work acceptably (e.g., elimination of location parame-
ters in a Gaussian linear model), it is diÔ¨Écult to evaluate whether or not
there is a loss of information in the process of the eliminating parameters.
This is illustrated below.
Consider the sampling model studied in Example 4.1, and suppose the
parameter of primary interest is Œ∏1 = œÉ2 whereas the nuisance parameter
is the location vector Œ∏2 = Œ≤. A likelihood that does not involve Œ≤ can be
constructed from a vector of Ô¨Åtted residuals, also called ‚Äúerror‚Äù contrasts,
as suggested by Patterson and Thompson (1971). Let
w = y ‚àíX

X‚Ä≤V‚àí1X
‚àí1 X‚Ä≤V‚àí1y
=

I ‚àíX

X‚Ä≤V‚àí1X
‚àí1 X‚Ä≤V‚àí1
y
= My
(4.33)
for M =[I ‚àíX

X‚Ä≤V‚àí1X
‚àí1 X‚Ä≤V‚àí1] of order N√óN. If y is normal, w must
be normal by virtue of it being a linear combination of normal variables.
Further
E (w) = XŒ≤ ‚àíX

X‚Ä≤V‚àí1X
‚àí1 X‚Ä≤V‚àí1XŒ≤ = 0
(4.34)
and:
V ar (w) = MVM‚Ä≤œÉ2.
(4.35)
Observe that
rank [V ar (w)] ‚â§rank [M]
and that M is an idempotent matrix. Using properties of idempotent ma-
trices plus cyclical commutation under the trace operator (Searle, 1982)
rank [M] = tr [M] = tr

I ‚àíX

X‚Ä≤V‚àí1X
‚àí1 X‚Ä≤V‚àí1
= N ‚àítr

X‚Ä≤V‚àí1X
‚àí1 X‚Ä≤V‚àí1X

= N ‚àíp,
where p is the order of Œ≤. Hence, V ar (w) has deÔ¨Åcient rank and
w ‚àºSN

0, MVM‚Ä≤œÉ2
,
(4.36)
where SN denotes a singular normal distribution having a covariance ma-
trix of rank N ‚àíp (Searle, 1971). This means that p of the elements of
w are redundant and can be expressed as a linear function of the N ‚àíp
linearly independent combinations. Put
w =

wL
wR

=
 ML
MR

y =
 MLy
MRy

,

184
4. Further Topics in Likelihood Inference
where wL (wR) stands for a linearly independent (redundant) part of w,
and ML (MR) denotes the corresponding partition of M.
Consider now the distribution of wL. From (4.34) and (4.35), this distri-
bution wL must be normal with a nonsingular covariance matrix
wL ‚àºN

0, MLVM‚Ä≤
LœÉ2
.
(4.37)
This distribution depends on œÉ2 only, so it is ‚Äúfree‚Äù of the nuisance param-
eter Œ≤. Hence, a likelihood function devoid of Œ≤ can be constructed based
on wL. This marginal likelihood can be written as
L

œÉ2
‚àù
""MLVM‚Ä≤
LœÉ2""‚àí1
2 exp

‚àí1
2œÉ2 w‚Ä≤
L (MLVML)‚àí1 wL
%
‚àù

œÉ2‚àíN‚àíp
2
exp

‚àí1
2œÉ2 y‚Ä≤M‚Ä≤
L

MLVM‚Ä≤
L
‚àí1 MLy
%
.
(4.38)
A result in linear algebra (Searle et al., 1992) states that if MLX = 0 and
V is positive deÔ¨Ånite, two conditions met here, then
M‚Ä≤
L

MLVM‚Ä≤
L
‚àí1 ML = V‚àí1M
and this holds for any ML having full row rank. Using the preceding in
(4.38), the marginal log-likelihood function is
l

œÉ2
= ‚àíN ‚àíp
2
ln

œÉ2
‚àí
1
2œÉ2 y‚Ä≤V‚àí1My
= ‚àíN ‚àíp
2
ln

œÉ2
‚àí
1
2œÉ2

y‚Ä≤V‚àí1y‚àí5Œ≤
‚Ä≤X‚Ä≤V‚àí1y

= ‚àíN ‚àíp
2
ln

œÉ2
‚àí
1
2œÉ2

y ‚àíX5Œ≤
‚Ä≤
V‚àí1 
y ‚àíX5Œ≤

,
(4.39)
where 5Œ≤ =

X‚Ä≤V‚àí1X
‚àí1 X‚Ä≤V‚àí1y. Maximization of (4.39) with respect to
œÉ2 gives, as the marginal ML estimator,
,œÉ2 =

y ‚àíX5Œ≤
‚Ä≤
V‚àí1 
y ‚àíX5Œ≤

N ‚àíp
=
Sy
N ‚àíp.
(4.40)
Writing
,œÉ2 = Sy
œÉ2
œÉ2
N ‚àíp,
and noting that Sy/œÉ2 ‚àºœá2
N‚àíp, it follows that the marginal ML estimator
has a scaled chi-square distribution, with mean and variance
E

,œÉ2
=
œÉ2
N ‚àíp (N ‚àíp) = œÉ2,
(4.41)
V ar

,œÉ2
=

œÉ2
N ‚àíp
2
2 (N ‚àíp) =
2œÉ4
N ‚àíp.
(4.42)

4.4 Nuisance Parameters
185
This should be contrasted with the ML estimator:
6
œÉ2 = Sy
N
(4.43)
that also has a scaled chi-square distribution with mean and variance
E

6
œÉ2

= œÉ2 N ‚àíp
N
,
(4.44)
V ar

6
œÉ2

= 2œÉ4
N .
(4.45)
The marginal ML estimator ,œÉ2 is unbiased, whereas 6
œÉ2 has a downward
bias. This bias can be severe if p is large relative to N. On the other hand,
the ML estimator is more precise (lower variance) than the estimator based
on a marginal likelihood, suggesting that some information is lost in the
process of eliminating the nuisance parameter Œ≤. This can be checked by
computing the information about œÉ2 contained in the marginal likelihood
(4.38). DiÔ¨Äerentiating the marginal log-likelihood twice with respect to œÉ2,
and multiplying by ‚àí1, gives the observed information
‚àíN ‚àíp
2œÉ4
+ Sy
œÉ6 .
The expected information is
IM

œÉ2
= ‚àíN ‚àíp
2œÉ4
+ (N ‚àíp) œÉ2
œÉ6
= N ‚àíp
2œÉ4 .
(4.46)
Using a similar procedure, the information from the full likelihood can be
found to be equal to
I

œÉ2
= N
2œÉ4 .
(4.47)
A comparison between (4.46) and (4.47) indicates that there is a loss of
information in the process of eliminating the nuisance parameter, at least
in the situation considered here. The loss of information can be serious in
a model where p/N is large. The corresponding asymptotic distributions
are then ,œÉ2 ‚àºN

0, 2œÉ4/(N ‚àíp)

and 6
œÉ2 ‚àºN

0, 2œÉ4/N

. This fact would
seem to favor the ML estimator. However, for very large N (relative to p)
the two distributions are expected to diÔ¨Äer by little.
These asymptotic distributions do not give guidance on how to choose
between the estimators when samples are Ô¨Ånite. Here, it was shown that
the two ML estimators have distributions that are multiples of chi-square
random variables but with diÔ¨Äerent means and variances. Consider a com-
parison based on the mean squared error criterion. The mean squared error

186
4. Further Topics in Likelihood Inference
of an estimator 5Œ∏ is:
E

5Œ∏ ‚àíŒ∏
2
= E

5Œ∏ ‚àíE

5Œ∏

+ E

5Œ∏

‚àíŒ∏
2
= E

5Œ∏ ‚àíE

5Œ∏
2
+

E

5Œ∏

‚àíŒ∏
2
= V ar

5Œ∏

+ Bias2 
5Œ∏

,
(4.48)
where Bias

5Œ∏

= E

5Œ∏

‚àíŒ∏ gives the expected deviation of the average
of the realized value of the estimators from the true parameter value in
a process of conceptual repeated sampling. The marginal ML estimator
has null bias (see equation 4.41) so its mean squared error is equal to
its variance, as given in (4.42). Using expressions (4.44) and (4.45), the
corresponding mean squared error of the ML estimator is
E

6
œÉ2 ‚àíœÉ2
=
2œÉ4
N ‚àíp

1 ‚àík + k2 N ‚àíp
2

,
where k = p/N. From this, conditions can be found under which one of the
two estimators is ‚Äúbetter‚Äù than the other in terms of mean squared error.
For example, if p = 1, that is, if the model has a single location parameter,
the ML estimator has a smaller mean squared error than the estimator
based on marginal likelihood throughout the parameter space. For this
particular model and loss function (mean squared error), the marginal ML
estimator is said to be inadmissible, because it is known that a better
estimator exists for all values of œÉ2. Unfortunately, these calculations are
not possible in more complicated models, for example, Gaussian mixed
eÔ¨Äects models with unknown variance components. In general, the choice
of the likelihood to be maximized is not obvious.
The idea of using a subset of the data (error contrasts) to make inferences
about variance components in a linear model was suggested by Patterson
and Thompson (1971). These authors used the term restricted likelihood
instead of marginal likelihood, and called the resulting estimates REML for
short. It is unclear how this idea can be generalized to other parameters
of a linear or nonlinear model. The Bayesian approach, on the other hand,
provides a completely general form of elimination of nuisance parameters.
This will be discussed in the following chapter.
4.4.3
ProÔ¨Åle Likelihoods
When a model has nuisance parameters, it is possible to deÔ¨Åne a likelihood
that can be used almost invariably, but not without pitfalls. It is known
as a proÔ¨Åle likelihood. In order to introduce the concept, consider a model
with parameters (Œ∏1, Œ∏2), where Œ∏2 is regarded as a vector of nuisance

4.4 Nuisance Parameters
187
parameters. Observe that ML estimation equations must satisfy
‚àÇl (Œ∏1, Œ∏2|y)
‚àÇŒ∏1
= 0,
‚àÇl (Œ∏1, Œ∏2|y)
‚àÇŒ∏2
= 0.
The Ô¨Årst equation deÔ¨Ånes the ML estimator of Œ∏1 at a Ô¨Åxed value of the
nuisance parameter, and vice-versa for the second equation. Let 5Œ∏2|1 be the
partial ML estimator of the nuisance parameter obtained from the second
equation, that is, with Œ∏1 Ô¨Åxed. This ‚Äúpartial‚Äù estimator depends only on
the data and Œ∏1. The proÔ¨Åle log-likelihood of Œ∏1 is obtained by replacing
Œ∏2 by 5Œ∏2|1 in the likelihood function, and is deÔ¨Åned as
lP

Œ∏1, 5Œ∏2|1|y

.
(4.49)
Note that (4.49) has the same form as the numerator of the likelihood ratio
(4.11), which can also be viewed as a proÔ¨Åle likelihood ratio. Expression
(4.49) is a function of Œ∏1 only, and its maximizer must be such that
‚àÇlP

Œ∏1, 5Œ∏2|1|y

‚àÇŒ∏1
= 0.
It follows that the maximizer of the proÔ¨Åle likelihood must be identical to
the ML estimator of Œ∏1 obtained from l (Œ∏1, Œ∏2|y) , this being so because
the Ô¨Årst of the two ML estimating equations given above is satisÔ¨Åed by
5Œ∏2|1.
In Subsection 4.3.1, the nuisance parameter Œ∏2 was estimated condition-
ally on a given value of Œ∏1. This conditional estimator, labelled ,Œ∏2, was
replaced in l (Œ∏1, Œ∏2|y) and the latter evaluated at Œ∏1 = Œ∏10. This led to
expression (4.21) for l

Œ∏10, ,Œ∏2|y

. If instead, Œ∏1 is left free to vary, one
obtains
lP

Œ∏1, ,Œ∏2|y

‚âàconstant ‚àí1
2

5Œ∏1 ‚àíŒ∏1
‚Ä≤ 
J11 
5Œ∏
‚àí1 
5Œ∏1 ‚àíŒ∏1

, (4.50)
which is an asymptotic Ô¨Årst-order approximation to the proÔ¨Åle log-likelihood.
Then the standard results that led to (3.66) hold here also, and we can write
(informally),
5Œ∏1 ‚àºN

Œ∏1, I11 
5Œ∏

.
(4.51)
In view of (4.32), this shows that the asymptotic variance in the presence
of nuisance parameters is larger than when these are absent or assumed
known, and the proÔ¨Åle likelihood accounts for this extra uncertainty.
However, a word of caution is necessary. Although it would seem that
the ML machinery can be applied in a straightforward manner using a

188
4. Further Topics in Likelihood Inference
proÔ¨Åle likelihood, this is not always the case. The proÔ¨Åle likelihood is not
proportional to the density function of a random variable. In large sam-
ples, replacing Œ∏2 by its ML estimate has relatively little consequences for
inferences involving Œ∏1. However, if the dimension of Œ∏2 is large relative
to sample size, a situation in which it would be diÔ¨Écult to argue asymp-
totically, the proÔ¨Åle log-likelihood can be misleading when interpreted as a
log-likelihood function (McCullagh and Nelder, 1989; Cox and Snell, 1989).
Example 4.4
ProÔ¨Åle likelihoods in a linear model
Let the joint distribution of the observations be
y ‚àºN

X1Œ≤1 + X2Œ≤2, VœÉ2
where Œ≤1, Œ≤2, and œÉ2 are the unknown parameters and V is a known
matrix. The likelihood function is
L

Œ≤1, Œ≤2,œÉ2|y

= (2œÄ)‚àíN
2 ""VœÉ2""‚àí1
2
√ó exp

‚àí1
2œÉ2 (y ‚àíX1Œ≤1 ‚àíX2Œ≤2)‚Ä≤ V‚àí1 (y ‚àíX1Œ≤1 ‚àíX2Œ≤2)

.
It will be shown here how diÔ¨Äerent proÔ¨Åle likelihoods are constructed.
(a) Suppose that the nuisance parameter is œÉ2 and that inferences are
sought about the location vectors Œ≤1 and Œ≤2. The partial ML estimator of
œÉ2 is
‚àí‚Üí
œÉ 2=(y ‚àíX1Œ≤1 ‚àíX2Œ≤2)‚Ä≤ V‚àí1 (y ‚àíX1Œ≤1 ‚àíX2Œ≤2)
N
.
Replacing this in the likelihood gives the proÔ¨Åle likelihood of Œ≤1 and Œ≤2:
LP (Œ≤1, Œ≤2|y) ‚àù
‚àí‚Üí
œÉ 2‚àíN
2 exp

‚àíN
2

‚àù
‚àí‚Üí
œÉ 2‚àíN
2
and this does not depend on the nuisance parameter œÉ2. The corresponding
proÔ¨Åle log-likelihood, ignoring the constant, is
lP (Œ≤1, Œ≤2|y) = ‚àíN
2 ln

(y ‚àíX1Œ≤1 ‚àíX2Œ≤2)‚Ä≤ V‚àí1 (y ‚àíX1Œ≤1 ‚àíX2Œ≤2)

.
Setting the Ô¨Årst derivatives with respect to the unknowns to zero and re-
arranging gives as solution

5Œ≤1
5Œ≤2

=
 X‚Ä≤
1V‚àí1X1
X‚Ä≤
1V‚àí1X2
X‚Ä≤
2V‚àí1X1
X‚Ä≤
2V‚àí1X2
‚àí1  X‚Ä≤
1V‚àí1y
X‚Ä≤
2V‚àí1y

,
retrieving the ML estimator, as expected. In this case the proÔ¨Åle likelihood
has the same asymptotic properties as the usual likelihood, this being so
because ‚àí‚Üí
œÉ 2 is a consistent estimator of œÉ2. This can be veriÔ¨Åed by noting

4.4 Nuisance Parameters
189
that V can be decomposed as V = LL‚Ä≤ with nonsingular L. Hence, for
y‚àó= (L)‚àí1 y, then
y‚àó‚àºN

(L)‚àí1 (X1Œ≤1 + X2Œ≤2) , IœÉ2
,
so the transformed observations are independent. Putting (L)‚àí1 Xi = X‚àó
i ,
and letting x‚àó‚Ä≤
ij denote the jth row of X‚àó
i , it turns out that the partial ML
estimator can be written as
‚àí‚Üí
œÉ 2 = 1
N
N

j=1

y‚àó
j ‚àíx‚àó‚Ä≤
1jŒ≤1 ‚àíx‚àó‚Ä≤
2jŒ≤2
2 .
It follows that as N ‚Üí‚àû, then
‚àí‚Üí
œÉ 2‚ÜíE

y‚àó
j ‚àíx‚àó‚Ä≤
1jŒ≤1 ‚àíx‚àó‚Ä≤
2jŒ≤2
2
= œÉ2,
because of the law of large numbers.
(b) Let Œ≤1 now be of primary interest, with Œ≤2 and œÉ2 acting as nuisance
parameters. The partial ML estimators of Œ≤2 and œÉ2 are
‚Üê‚àí
Œ≤ 2 =

X‚Ä≤
2V‚àí1X2
‚àí1 X‚Ä≤
2V‚àí1 (y ‚àíX1Œ≤1)
and
‚Üê‚àí
œÉ 2 =

y ‚àíX1Œ≤1 ‚àíX2
‚Üê‚àí
Œ≤ 2
‚Ä≤
V‚àí1 
y ‚àíX1Œ≤1 ‚àíX2
‚Üê‚àí
Œ≤ 2

N
.
Note that ‚Üê‚àí
Œ≤ 2 ‚àºN[Œ≤2,

X‚Ä≤
2V‚àí1X2
‚àí1 œÉ2] is the ML estimator of Œ≤2 applied
to the ‚Äúcorrected‚Äù data y ‚àíX1Œ≤1, so it must converge in probability to Œ≤2.
Hence, ‚Üê‚àí
œÉ 2 must be consistent as well. The proÔ¨Åle likelihood for this case
is
LP (Œ≤1|, y) ‚àù
‚Üê‚àí
œÉ 2‚àíN
2
√ó exp

‚àí
1
2‚Üê‚àí
œÉ 2

y ‚àíX1Œ≤1 ‚àíX2
‚Üê‚àí
Œ≤ 2
‚Ä≤
V‚àí1 
y ‚àíX1Œ≤1 ‚àíX2
‚Üê‚àí
Œ≤ 2

.
(c) The parameter of interest now is œÉ2. The partial ML estimators of
the nuisance parameters are 5Œ≤1 and 5Œ≤2, as with full ML. This is so because
solving the full ML equations for Œ≤1 and Œ≤2 does not require knowledge of
œÉ2. The proÔ¨Åle likelihood is then
LP

œÉ2|y

‚àù

œÉ2‚àíN
2
√ó exp

‚àí1
2œÉ2

y ‚àíX15Œ≤1 ‚àíX25Œ≤2
‚Ä≤
V‚àí1 
y ‚àíX15Œ≤1 ‚àíX25Œ≤2

.

190
4. Further Topics in Likelihood Inference
It can be shown readily that the maximizer of this proÔ¨Åle likelihood is
identical to the ML estimator of œÉ2, as it should be. The identity between
the ML and the partial ML estimators of the nuisance parameters Œ≤1 and
Œ≤2 indicates that these must be consistent, so the proÔ¨Åle likelihood can be
used as a full likelihood to obtain inferences about œÉ2. For example, the
information measure based on the proÔ¨Åle likelihood is
IP

œÉ2
= E

‚àí‚àÇ2 ln LP

œÉ2|y

(‚àÇœÉ2)2

= ‚àíN
2œÉ4 +
E

y ‚àíX15Œ≤1 ‚àíX25Œ≤2
‚Ä≤
V‚àí1 
y ‚àíX15Œ≤1 ‚àíX25Œ≤2

œÉ6
= ‚àíN
2œÉ4 + (N ‚àíp) œÉ2
œÉ6
= N
2œÉ4

1 ‚àí2p
N

.
The expectation taken above can be evaluated using (4.41). Note that this
measure indicates that account is taken of the information lost in elim-
inating the nuisance parameters. Observe also that there is information,
provided that p < N/2. It is of interest to compare the information in the
proÔ¨Åle likelihood with the information resulting from the marginal likeli-
hood, as given in (4.46) and reexpressible as
IM

œÉ2
= N
2œÉ4

1 ‚àíp
N

.
If one accepts the marginal likelihood as the ‚Äúcorrect‚Äù way of accounting
for nuisance parameters, this example illustrates that proÔ¨Åle likelihoods do
not always do so in the same manner.
‚ñ†
4.5
Analysis of a Multinomial Distribution
The multinomial sampling process was applied previously in connection
with the evaluation of the hypothesis that a population is in Hardy‚ÄìWein-
berg equilibrium; see Example 4.3. Inferences about the parameters of a
multinomial distribution will be dealt with here in a more general manner.
The objective is to illustrate the application of ML methods to problems
other than those arising in the classical linear model.
The data are counts ni (i = 1, 2, ..., T) in each of T mutually exclusive
and exhaustive classes. The draws are made independently of each other
from the same distribution, the only restriction being that the total num-
ber of draws, n = T
i=1 ni, is Ô¨Åxed by sampling. The unknown probability
that an observation falls in class i is denoted as Œ∏i. These unknown pa-
rameters are subject to the natural restriction that T
i=1 Œ∏i = 1, because
the probability that an observation falls in at least one class must be equal

4.5 Analysis of a Multinomial Distribution
191
to 1, and this observation cannot fall in two or more classes. As seen in
Chapter 1, under multinomial sampling the joint probability function of
the observations is
Pr (n1, n2, ..., nT |Œ∏1, Œ∏2, ..., Œ∏T ) = n!
T
-
i=1
Œ∏ni
i
ni! .
Given n, there are only T ‚àí1 independent n‚Ä≤
is so this is a multivariate
distribution of dimension T ‚àí1, having T ‚àí1 free parameters Œ∏i. When
T = 2, the multinomial distribution yields the binomial distribution as a
particular case. In order to implement the ML machinery, the means and
variances of the n‚Ä≤
is, as well as their covariances, are needed. These are:
E (ni) = nŒ∏i,
V ar (ni) = nŒ∏i (1 ‚àíŒ∏i) ,
and
Cov (ni, nj) = ‚àínŒ∏iŒ∏j.
The likelihood function is
L (Œ∏1, Œ∏2, ..., Œ∏T |n1, n2, ..., nT ) ‚àù
T
-
i=1
Œ∏ni
i .
(4.52)
Because there are only T ‚àí1 free parameters and independent counts,
Œ∏T = 1 ‚àíT ‚àí1
i=1 Œ∏i and nT = n ‚àíT ‚àí1
i=1 ni. Employing this in the likeli-
hood function and taking logs, the log-likelihood is, apart from an additive
constant
l (Œ∏1, Œ∏2, ..., Œ∏T ‚àí1|n1, n2, ..., nT )
=
T ‚àí1

i=1
ni ln (Œ∏i) +

n ‚àí
T ‚àí1

i=1
ni

ln

1 ‚àí
T ‚àí1

i=1
Œ∏i

.
(4.53)
DiÔ¨Äerentiating with respect to Œ∏i gives the score
‚àÇl (Œ∏1, Œ∏2, ..., Œ∏T ‚àí1|n1, n2, ..., nT )
‚àÇŒ∏i
= ni
Œ∏i
‚àí

n ‚àíT ‚àí1
i=1 ni


1 ‚àíT ‚àí1
i=1 Œ∏i

= ni
Œ∏i
‚àínT
Œ∏T
,
i = 1, 2, ..., T ‚àí1.
(4.54)
Setting this to 0 gives
5Œ∏i = ni
nT
5Œ∏T ,
i = 1, 2, ..., T ‚àí1.

192
4. Further Topics in Likelihood Inference
Because the ML estimates must be constrained to reside in the interior of
the parameter space, it must be that T
i=1 5Œ∏i = 1. Summing the above over
the T ML estimates of probabilities gives
5Œ∏T = nT
n .
Hence,
5Œ∏i = ni
n ,
i = 1, 2, ..., T,
(4.55)
so the proportions of observations falling in class i gives the ML estimates of
the corresponding probability directly. It can be veriÔ¨Åed that the estimator
is unbiased because
E

5Œ∏i

= E
ni
n

= nŒ∏i
n
= Œ∏i.
DiÔ¨Äerentiating (4.54) again with respect to Œ∏i (taking into account the fact
that the last parameter is a function of the T ‚àí1 probabilities for the
preceding categories):
‚àÇ2l (Œ∏1, Œ∏2, ..., Œ∏T ‚àí1|n1, n2, ..., nT )
(‚àÇŒ∏i)2
= ‚àíni
Œ∏2
i
‚àí
nT

1 ‚àí
T ‚àí1

i=1
Œ∏i
2 ,
i = 1, 2, ..., T ‚àí1.
and
‚àÇ2l (Œ∏1, Œ∏2, ..., Œ∏T ‚àí1|n1, n2, ..., nT )
‚àÇŒ∏i ‚àÇŒ∏j
= ‚àí
nT

1 ‚àí
T ‚àí1

i=1
Œ∏i
2 ,
i, j = 1, 2, ..., T ‚àí1, i Ã∏= j.
The second derivatives multiplied by ‚àí1 give the elements of the informa-
tion matrix about the free parameters of the model. Taking expectations
yields Fisher‚Äôs information matrix, with elements
I (i, i) = n

 1
Œ∏i
+ 1
Œ∏T

,
i = 1, 2, ..., T ‚àí1,
(4.56)
and:
I (i, j) = n
Œ∏T
,
i, j = 1, 2, ..., T ‚àí1, i Ã∏= j.
(4.57)
The information is proportional to sample size. The inverse of the infor-
mation matrix gives the asymptotic variance covariance matrix of the ML
estimates. This inverse can be shown to have elements
v (i, i) = 1
nŒ∏i (1 ‚àíŒ∏i) ,
i = 1, 2, ..., T ‚àí1,
(4.58)
v (i, j) = ‚àí1
nŒ∏iŒ∏j,
i, j = 1, 2, ..., T ‚àí1, i Ã∏= j.
(4.59)

4.5 Analysis of a Multinomial Distribution
193
These coincide with the exact variances and covariances, as given earlier.
Hence, the ML estimator attains the Cram¬¥er‚ÄìRao lower bound, and is a
minimum variance unbiased estimator in this case. The correlation between
parameter estimates is
Corr

5Œ∏i,5Œ∏j

= ‚àí
C
Œ∏iŒ∏j
(1 ‚àíŒ∏i) (1 ‚àíŒ∏j).
When T = 2, the multinomial model reduces to binomial sampling. The
ML estimator of the probability of response in the Ô¨Årst class, say, is the
proportion of observations falling in this category. The sampling variance
of the estimate is
V ar

5Œ∏

= Œ∏ (1 ‚àíŒ∏)
n
.
In all cases, because the Œ∏‚Ä≤s are not known, parameter values must be
replaced by the ML estimates to obtain an approximation to the asymptotic
distribution. The goodness of this approximation is expected to improve as
n increases.
Example 4.5
Analysis of a trichotomy
Suppose the data consist of n = 100 observations collected at random
from a homogeneous populations of experimental chickens. Each bird is
scored for the presence or absence of leg deformities and, within deformed
individuals, there are two modalities. Hence, T = 3. Suppose the outcome
of the experiment is n1 = 20, n2 = 35, and n3 = 45. The objective is to
obtain ML estimates of the prevalence of each of the three modalities, and
of a non-linear function of the associated probabilities. Here we work with
Œ∏1 and Œ∏2 as free parameters, because Œ∏3 is redundant. From (4.53) the
log-likelihood, apart from a constant, is
l (Œ∏1, Œ∏2|n1, n2, n3) = 20 ln (Œ∏1) + 35 ln (Œ∏2) + 45 ln (1 ‚àíŒ∏1 ‚àíŒ∏2) .
Using (4.55) the ML estimates are 5Œ∏1 = .20, 5Œ∏2 = .35, and 5Œ∏3 = 1 ‚àí5Œ∏1‚àí
5Œ∏2 = 1 ‚àí.20 ‚àí.35 = .45. From (4.58), the asymptotic standard deviation
of the estimate (equal to the standard deviation of the exact sampling
distribution in this case) of 5Œ∏1 can be estimated as
?
(.20) (.80)
100
= 0.04.
Based on the ML estimator, a conÔ¨Ådence region based on two standard
deviations is .20 ¬± .08. The interval (0.12 ‚àí0.28) indicates that inferences
about the true value of Œ∏1 are not very sharp in a sample of this size. How
sharp a conÔ¨Ådence region should be depends on the problem in question.
A similar calculation can be carried out for the probabilities of falling into

194
4. Further Topics in Likelihood Inference
any of the other two classes.
Suppose now that inferences are sought using the logit transform
wi = ln
Œ∏i
1 ‚àíŒ∏i
with inverse
Œ∏i =
exp (wi)
1 + exp (wi).
The logit is interpretable as a log-odds ratio, that is, the ratio between the
probabilities of ‚Äúresponse‚Äù and of ‚Äúnonresponse‚Äù measured on a logarithmic
scale. The logit is positive when the probability of ‚Äúresponse‚Äù is larger than
the probability of the complementary event, and negative otherwise. While
the parameter space of Œ∏i is bounded between 0 and 1, that for the logit is
‚àí‚àû< wi < ‚àû. The ML estimator of the logit is
5wi = ln
5Œ∏i
1 ‚àí5Œ∏i
.
Here,
5w1 = ln .20
.80 = ‚àí1.3863,
5w2 = ln .35
.65 = ‚àí0.6190,
5w3 = ln .45
.55 = ‚àí0.2007.
The variance of the asymptotic distribution of 5wi can be deduced from
the variance‚Äìcovariance matrix of the asymptotic distribution of the ML
estimates of the probabilities. Because this is a multi-parameter problem,
one needs to form the information matrix about the logits. Using (3.74),
and working with two ‚Äúfree‚Äù logits only
I


w1
w2

=
Ô£Æ
Ô£ØÔ£ØÔ£∞
dŒ∏1
dw1
dŒ∏2
dw1
dŒ∏1
dw2
dŒ∏2
dw2
Ô£π
Ô£∫Ô£∫Ô£ªI


Œ∏1
Œ∏2

Ô£Æ
Ô£ØÔ£ØÔ£∞
dŒ∏1
dw1
dŒ∏1
dw2
dŒ∏2
dw1
dŒ∏2
dw2
Ô£π
Ô£∫Ô£∫Ô£ª
=
Ô£Æ
Ô£ØÔ£∞
dŒ∏1
dw1
0
0
dŒ∏2
dw2
Ô£π
Ô£∫Ô£ªI

 Œ∏1
Œ∏2

Ô£Æ
Ô£ØÔ£∞
dŒ∏1
dw1
0
0
dŒ∏2
dw2
Ô£π
Ô£∫Ô£ª.
The asymptotic variance of the ML estimates of the logits is then:
V ar
 5w1
5w2

=
Ô£Æ
Ô£ØÔ£∞
dŒ∏1
dw1
0
0
dŒ∏2
dw2
Ô£π
Ô£∫Ô£ª
‚àí1
I‚àí1


Œ∏1
Œ∏2

Ô£Æ
Ô£ØÔ£∞
dŒ∏1
dw1
0
0
dŒ∏2
dw2
Ô£π
Ô£∫Ô£ª
‚àí1
.

4.5 Analysis of a Multinomial Distribution
195
The derivatives are:
dŒ∏i
dwi
=
exp (wi)
1 + exp (wi)

1 ‚àí
exp (wi)
1 + exp (wi)

= Œ∏i (1 ‚àíŒ∏i) ,
so, using the asymptotic covariance matrix of the ML estimates of the
probabilities, with elements as in expressions (4.58) and (4.59),
V ar

5w1
5w2

= 1
n
Ô£Æ
Ô£ØÔ£ØÔ£∞
1
Œ∏1 (1 ‚àíŒ∏1)
‚àí
1
(1 ‚àíŒ∏1) (1 ‚àíŒ∏2)
‚àí
1
(1 ‚àíŒ∏1) (1 ‚àíŒ∏2)
1
Œ∏2 (1 ‚àíŒ∏2)
Ô£π
Ô£∫Ô£∫Ô£ª.
The asymptotic correlation between the ML estimates of the logits is then:
Corr ( 5w1, 5w2) = ‚àí
C
Œ∏1Œ∏2
(1 ‚àíŒ∏1) (1 ‚àíŒ∏2).
In this example, the estimated variance‚Äìcovariance matrix of the ML esti-
mates of the logits is
D
V ar

5w1
5w2

=
1
100
Ô£Æ
Ô£∞
1
.16
‚àí1
.07
‚àí1
.07
1
.2275
Ô£π
Ô£ª
=

.0625
‚àí.1429
‚àí.1429
.0440

.
‚ñ†
Example 4.6
Estimation of allele frequencies with inbreeding
This example is adapted from Weir (1996). From population genetics the-
ory, it is known that if a population is in Hardy‚ÄìWeinberg equilibrium,
and inbreeding is practiced, this causes a reduction in the frequency of
heterozygotes and a corresponding increase in homozygosity. Consider a
locus with segregating alleles A and a, so that three genotypes are pos-
sible: AA, Aa, and aa. As seen previously, if Œ∏ is the frequency of allele
A, with Hardy‚ÄìWeinberg equilibrium, the frequency of the three diÔ¨Äerent
genotypes is Pr (AA) = Œ∏2, Pr (Aa) = 2Œ∏ (1 ‚àíŒ∏), and Pr (aa) = (1 ‚àíŒ∏)2 .
With inbreeding, the genotypic distribution changes to
Pr (AA) = Œ∏2 + Œ∏ (1 ‚àíŒ∏) f,
Pr (Aa) = 2Œ∏ (1 ‚àíŒ∏) (1 ‚àíf) ,
Pr (aa) = (1 ‚àíŒ∏)2 + Œ∏ (1 ‚àíŒ∏) f,
where f is the inbreeding coeÔ¨Écient or, equivalently, the fractional reduc-
tion in heterozygosity. Suppose that nAA, nAa, and naa individuals having

196
4. Further Topics in Likelihood Inference
genotypes AA, Aa, and aa, respectively, are observed, and that the total
number of observations is Ô¨Åxed by the sampling scheme. The unknown pa-
rameters are the allele frequency and the coeÔ¨Écient of inbreeding. If the
individuals are sampled at random from the same conceptual population, a
multinomial sampling model would be reasonable. The likelihood function
can be expressed as
L (Œ∏, f) ‚àùŒ∏nAA+nAa [Œ∏ + (1 ‚àíŒ∏) f]nAA [(1 ‚àíf)]nAa
√ó [(1 ‚àíŒ∏) + Œ∏f]naa (1 ‚àíŒ∏)nAa+naa
where 1 ‚àíŒ∏ is the allelic frequency of a. The score vector has two ele-
ments, ‚àÇl/‚àÇŒ∏ and ‚àÇl/‚àÇf where, as usual, l is the log-likelihood function.
The elements are
‚àÇl
‚àÇŒ∏ = nAA + nAa
Œ∏
+ nAA (1 ‚àíf)
Œ∏ + (1 ‚àíŒ∏) f ‚àínaa (1 ‚àíf)
(1 ‚àíŒ∏) + Œ∏f
‚àínAa + naa
(1 ‚àíŒ∏)
and
‚àÇl
‚àÇf = nAA (1 ‚àíŒ∏)
Œ∏ + (1 ‚àíŒ∏) f ‚àí
nAa
(1 ‚àíf) +
naaŒ∏
(1 ‚àíŒ∏) + Œ∏f .
Setting the gradient to zero leads to a nonlinear system in Œ∏ and f that
does not have an explicit solution. Hence, second derivatives are needed not
only to complete the ML analysis, but to construct an iterative procedure
for obtaining the estimates. The second derivatives are
‚àÇ2l
(‚àÇŒ∏)2 = ‚àínAA + nAa
Œ∏2
‚àí
nAA (1 ‚àíf)2
[Œ∏ + (1 ‚àíŒ∏) f]2 ‚àí
naa (1 ‚àíf)2
[(1 ‚àíŒ∏) + Œ∏f]2
‚àínAa + naa
(1 ‚àíŒ∏)2 ,
‚àÇ2l
(‚àÇf)2 = ‚àínAA (1 ‚àíŒ∏)2
[Œ∏ + (1 ‚àíŒ∏) f]2 ‚àí
nAa
(1 ‚àíf)2 ‚àí
naaŒ∏2
[(1 ‚àíŒ∏) + Œ∏f]2 ,
and
‚àÇ2l
‚àÇŒ∏‚àÇf = ‚àí
nAA
Œ∏ + (1 ‚àíŒ∏) f ‚àínAA (1 ‚àíŒ∏) (1 ‚àíf)
[Œ∏ + (1 ‚àíŒ∏) f]2
+
naa
(1 ‚àíŒ∏) + Œ∏f
+ naa (1 ‚àíf) Œ∏
[(1 ‚àíŒ∏) + Œ∏f]2
= ‚àí
nAA
[Œ∏ + (1 ‚àíŒ∏) f]2 +
naa
[(1 ‚àíŒ∏) + Œ∏f]2 .

4.5 Analysis of a Multinomial Distribution
197
Genotype
Phenotype
Observed counts
Frequency
AA
A
nA
p2
A
AO
A
2pApO
AB
AB
nAB
2pApB
BB
B
nB
p2
B
BO
B
2pBpO
OO
O
nO
p2
O
TABLE 4.1. Frequency of genotypes and phenotypes of ABO blood group data.
From the Ô¨Årst and second derivatives, the Newton‚ÄìRaphson algorithm can
be formed as presented in (4.1). The expected second derivatives are ob-
tained by replacing nAA, nAa, and naa by their expectations. For example
E (nAa) = 2Œ∏ (1 ‚àíŒ∏) (1 ‚àíf) (nAA + nAa + naa) .
To illustrate, suppose that nAA = 100, nAa = 200, and naa = 200. The ML
estimates are 5Œ∏ = .40 and 5f = .1667, after round-oÔ¨Ä. From the Newton‚Äì
Raphson algorithm, the observed information matrix can be estimated as
5Io (Œ∏, f) =
Ô£Æ
Ô£ØÔ£ØÔ£∞
‚àí‚àÇ2l
(‚àÇŒ∏)2
‚àí‚àÇ2l
‚àÇŒ∏‚àÇf
‚àí‚àÇ2l
‚àÇŒ∏‚àÇf
‚àí‚àÇ2l
(‚àÇf)2
Ô£π
Ô£∫Ô£∫Ô£ª
Œ∏=Œ∏
f= 
f
.
This can be used in lieu of the expected information matrix to estimate the
asymptotic variance‚Äìcovariance matrix of the estimates. In this example
5I‚àí1
o
(Œ∏, f) =
1
1000

.28001
.02777
.02777
1.98688

.
The asymptotic correlation between ML estimates is approximately .04.‚ñ†
Example 4.7
ABO blood groups
Consider the following blood group data in Table 4.1. With three alleles,
A, B, and O there are six genotypes but only four phenotypic classes can
be observed. The expected frequency of each genotype in the last column
is derived assuming Hardy‚ÄìWeinberg equilibrium. The problem at hand is
to infer pA, pB and pO, the frequency of alleles A, B, and O, respectively,
subject to the constraint pA + pB + pO = 1.
The observed data is y‚Ä≤ = (nA, nAB, nB, nO). The log-likelihood is given
by:
l (pA, pB|y) ‚àùnA ln [pA (2 ‚àípA ‚àí2pB)] + nAB ln [2pApB]
+ nB ln [pB (2 ‚àípB ‚àí2pA)] + 2nO ln [(1 ‚àípA ‚àípB)] .
(4.60)

198
4. Further Topics in Likelihood Inference
DiÔ¨Äerentiating with respect to pA and pB yields the nonlinear system of
equations
‚àÇl (pA, pB|y)
‚àÇpA
= nAB
pA
+ nA (2 ‚àí2pA ‚àí2pB)
pA (2 ‚àípA ‚àí2pB) ‚àí
2nB
2 ‚àí2pA ‚àípB
‚àí
2nO
1 ‚àípA ‚àípB
,
(4.61)
‚àÇl (pA, pB|y)
‚àÇpB
= nAB
pB
+ nB (2 ‚àí2pA ‚àí2pB)
pB (2 ‚àí2pA ‚àípB) ‚àí
2nA
2 ‚àípA ‚àí2pB
‚àí
2nO
1 ‚àípA ‚àípB
.
(4.62)
A solution can be obtained using Newton‚ÄìRaphson. This requires the fol-
lowing second derivatives
‚àÇ2l (pA, pB|y)
(‚àÇpA)2
= nA (2 ‚àí2pA ‚àí2pB)
pA (pA + 2pB ‚àí2)2 +
2nA
pA (pA + 2pB ‚àí2)
‚àínA (2pA + 2pB ‚àí2)
p2
A (pA + 2pB ‚àí2) ‚àínAB
p2
A
‚àí
2nO
(pA + pB ‚àí1)2
‚àí
4nB
(2pA + pB ‚àí2)2 ,
‚àÇ2l (pA, pB|y)
(‚àÇpB)2
= nB (2 ‚àí2pA ‚àí2pB)
pB (2pA + pB ‚àí2)2 +
2nB
pB (2pA + pB ‚àí2)
‚àínB (2pA + 2pB ‚àí2)
p2
B (2pA + pB ‚àí2) ‚àínAB
p2
B
‚àí
2nO
(pA + pB ‚àí1)2
‚àí
4nA
(pA + 2pB ‚àí2)2 ,
‚àÇ2l (pA, pB|y)
‚àÇpA‚àÇpB
= ‚àí
2nA
(2 ‚àípA ‚àí2pB)2 ‚àí
2nB
(2 ‚àí2pA ‚àípB)2
‚àí
2nO
(1 ‚àípA ‚àípB)2 .
Suppose the data are nA = 725, nAB = 72, nB = 258, nO = 1073. Using
these expressions in (4.1) yields, at convergence, the ML estimates: 5pA =
0.2091 and 5pB = 0.0808. The observed information matrix evaluated at the
ML estimates is
I (5pA, 5pB|y) =

23, 215.5
5, 031.14
5, 031.14
56, 009.4

,

4.5 Analysis of a Multinomial Distribution
199
resulting in an estimate of the asymptotic covariance matrix equal to:
V ar (5pA, 5pB|y) = [I (5pA, 5pB|y)]‚àí1 = 10‚àí6

43.930
‚àí3.946
‚àí3.946
18.209

.
‚ñ†
4.5.1
Amount of Information per Observation
An important problem in experimental genetics is the evaluation of designs
for estimation of genetic parameters. In this context, a relevant question is
the measurement of the average amount of information about a parameter
per experimental unit included in the trial. It will be shown here how to
assess this when the entity of interest is yet another parameter aÔ¨Äecting
the probabilities that govern the multinomial distribution. Subsequently,
two examples are given where the target parameter is the recombination
fraction between loci.
Under multinomial sampling, the likelihood is given in (4.52). Suppose
now that the Œ∏‚Ä≤s depend on some scalar parameter Œ±. To emphasize this
dependency, the probabilities are denoted as Œ∏i (Œ±) . The likelihood is then
viewed as a function of Œ±, and the score function is
dl (Œ±)
dŒ±
=
T

i=1
ni
Œ∏i (Œ±)
dŒ∏i (Œ±)
dŒ±
.
(4.63)
The amount of information about Œ± contained in the sample is denoted as:
In (Œ±) = E

‚àíd2l (Œ±)
(dŒ±)2

= ‚àíE
# T

i=1

‚àí
ni
Œ∏2
i (Œ±)

dŒ∏i (Œ±)
dŒ±
2
+
ni
Œ∏i (Œ±)
d2Œ∏i (Œ±)
(dŒ±)2
$
=
# T

i=1

1
Œ∏i (Œ±)

dŒ∏i (Œ±)
dŒ±
2
‚àíd2Œ∏i (Œ±)
(dŒ±)2
$
E

ni
Œ∏i (Œ±)

.
(4.64)
Now, E [ni/Œ∏i (Œ±)] = nŒ∏i (Œ±) /Œ∏i (Œ±) = n. Using this in the preceding ex-
pression, distributing the sum, and noting that
T

i=1
d2Œ∏i (Œ±)
(dŒ±)2
=
d2
(dŒ±)2
T

i=1
Œ∏i (Œ±) =
d2
(dŒ±)2 1 = 0,
one arrives at
In (Œ±) = n
T

i=1
1
Œ∏i (Œ±)

dŒ∏i (Œ±)
dŒ±
2
.
(4.65)

200
4. Further Topics in Likelihood Inference
Hence, the expected amount of information per observation is
I1 (Œ±) =
T

i=1
1
Œ∏i (Œ±)

dŒ∏i (Œ±)
dŒ±
2
.
(4.66)
Example 4.8
Estimating the recombination rate between two loci from
matings involving coupling heterozygotes
This example is patterned after Rao (1973). Consider two loci, each with
two alleles. Let the alleles be A and a at the Ô¨Årst locus, and B and b at the
second locus. Suppose that individuals that are heterozygous at both loci
are available, and that such heterozygotes originate from a cross between
AABB and aabb parents. In this case, individuals are called coupling het-
erozygotes, and their genotype is indicated as AB/ab. This means that they
came from AB and ab gametes only. Coupling heterozygotes are crossed
inter se, and the objective is to estimate the probability of recombination
(or recombination rate) between the two loci, denoted here as Œ±. The dis-
tribution of gametes produced by coupling heterozygotes can be deduced
by considering that recombinant gametes arise with probability Œ±, whereas
the complementary event (lack of recombination) has probability 1 ‚àíŒ±.
Then the gametic distribution is
Pr (AB) = Pr (ab) = 1 ‚àíŒ±
2
, Pr (Ab) = Pr (aB) = Œ±
2 .
The random union of these gametes produces 16 possible genotypes, i.e.,
AABB, ..., aabb, nine of which are distinguishable at the genotypic level,
assuming that maternal or paternal origin of the chromosome cannot be
traced. For example, AB/Ab cannot be distinguished from Ab/AB. The fo-
cus of inference is the parameter Œ± and to evaluate the eÔ¨Éciency of this and
of another design in terms of expected information per observation. The
data are genotypic counts scored in the progeny from the appropriate mat-
ings. Table 4.2 provides the distribution of genotypes in matings between
coupling heterozygotes, as well as the expected amount of information per
observation (for each genotype) calculated with formula (4.66).
Using (4.66), the expected amount of information per observation is ob-
tained by summing elements in the third column of Table 4.2, yielding
I1 (Œ±) = 4 + 8
 1
2 ‚àíŒ±
2
Œ± (1 ‚àíŒ±) +
(4Œ± ‚àí2)2
(4Œ±2 ‚àí4Œ± + 2).
For example, if the loci are in diÔ¨Äerent chromosomes, Œ± = 1/2 and I1 (Œ±) =
4. As the loci become more closely linked, this measure of information
increases and tends to inÔ¨Ånity as Œ± ‚Üí0. From Table 4.2, the log-likelihood

4.5 Analysis of a Multinomial Distribution
201
Genotype
Œ∏i (Œ±)
I[i]
1 (Œ±)
Counts
AABB
1
4 (1 ‚àíŒ±)2
1
n1
AaBb
 1
2 ‚àíŒ± + Œ±2
(4Œ± ‚àí2)2 4
4Œ±2 ‚àí4Œ± + 2

n2
AABb
1
2Œ± (1 ‚àíŒ±)
2
 1
2 ‚àíŒ±
2 4
Œ± ‚àíŒ±2
n3
AaBB
1
2Œ± (1 ‚àíŒ±)
2
 1
2 ‚àíŒ±
2 4
Œ± ‚àíŒ±2
n4
aabb
1
4 (1 ‚àíŒ±)2
1
n5
Aabb
1
2Œ± (1 ‚àíŒ±)
2
 1
2 ‚àíŒ±
2 4
Œ± ‚àíŒ±2
n6
aaBb
1
2Œ± (1 ‚àíŒ±)
2
 1
2 ‚àíŒ±
2 4
Œ± ‚àíŒ±2
n7
AAbb
1
4Œ±2
1
n8
aaBB
1
4Œ±2
1
n9
TABLE 4.2. Probability distribution of genotypes in progeny from crosses be-
tween coupling heterozygotes and contribution of each genotype to expected in-
formation per observation (third column).
of the parameter Œ± can be deduced to be
l (Œ±) = (n1 + n5) ln

(1 ‚àíŒ±)2
4

+ (n3 + n4 + n6 + n7) ln
Œ± (1 ‚àíŒ±)
2

+ (n8 + n9) ln

Œ±2
4

+ n2
#
(1 ‚àíŒ±)2
2
+ Œ±2
2
$
.
Given data, the log-likelihood can be maximized numerically. Suppose that,
out of n = 65 descendants from matings involving coupling heterozy-
gote parents, the genotypic count recovered is such that n1 + n5 = 30,
n3 + n4 + n6 + n7 = 8, n8 + n9 = 9, and n2 = 18. The ML estimate
of the recombination rate is, in this case, 5Œ± = .2205. Using second deriva-
tives, an approximation to the asymptotic standard error based on observed
information is
8
D
V ar (5Œ±) = .0412. The expected information per observa-
tion is estimated as I1 (5Œ±) = 8.58743. Thus, with n = 65, (4.65) gives
I65 (5Œ±) = (65) (8.58743) = 558.1830, as an estimate of the expected infor-
mation about the recombination rate parameter. The corresponding esti-
mated asymptotic standard error is
‚àö
558.1830‚àí1 = .0423. There is good
agreement between the standard errors based on observed and expected
information.
‚ñ†
Example 4.9
Estimating the recombination rate between two loci from
a backcross
An alternative genetic design for estimating Œ± consists of crossing the cou-
pling heterozygotes AB/ab to ab/ab (i.e., one of the parental genotypes).
This is called a backcross. The nonrecombinant types appearing in the

202
4. Further Topics in Likelihood Inference
 0.2 
 0.4 
 0.6 
 0.8 
1.0
 0.1 
 0.2 
 0.3 
 0.4 
 0.5 
Œ± 
I1(Œ±)/I1(Œ±)
B
FIGURE 4.1. Plot of IB
1 (Œ±) /I1 (Œ±) as a function of recombination Œ±.
progeny with a total probability of 1 ‚àíŒ±, are AB/ab and ab/ab. The re-
combinant types (with probability Œ±) are Ab/ab and aB/ab. Using (4.66),
the expected information per observation obtained from this design is
IB
1 (Œ±) =
T

i=1
1
Œ∏i (Œ±)

dŒ∏i (Œ±)
dŒ±
2
= 2
1
(1‚àíŒ±)
2

‚àí1
2
2
+ 2 1
Œ±
2

1
2
2
=
1
Œ± (1 ‚àíŒ±),
where the superscript B denotes the backcross design. If the loci are in
diÔ¨Äerent chromosomes IB
1 (Œ±) = 4 and, when Œ± goes to zero, then IB
1 (Œ±) ‚Üí
‚àû, as in the preceding design. A plot of IB
1 (Œ±) /I1 (Œ±) as a function of Œ±
is given in Figure 4.1. It is seen that a design involving matings between
coupling heterozygotes is always more eÔ¨Écient, although the designs diÔ¨Äer
little for values of Œ± near 1/2.
‚ñ†
4.6
Analysis of Linear Logistic Models
Response variables that are categorical in expression often arise in genetic
analysis. For example, Wright (1934) analyzed the inheritance of the num-
ber of digits in guinea pigs and proposed relating the expression of this
variable to an underlying normal process. It would be at this level where
gene substitutions operate. This model, known as the threshold model, was
elaborated further in quantitative genetics by Dempster and Lerner (1950),
Falconer (1965), and Gianola (1982), among others. Here, the special case
of binary response variables is considered. Attention is given to the de-
velopment of ML machinery appropriate for some situations of interest in
quantitative genetic applications.
The point of departure of a likelihood analysis is the probability mass
function of the data, given the parameters, which is developed below. Sup-

4.6 Analysis of Linear Logistic Models
203
pose there is an underlying or latent unobservable variable, l, often called
liability, and that the categories of response (two such categories assumed
here) result from the value of l relative to a Ô¨Åxed threshold t. (The same
letter l, is used for liability and for log-likelihood, although the latter is
indexed by the parameters of the model. The common use of l should
therefore not lead to ambiguities). Let the dichotomy be ‚Äúsurvival‚Äù versus
‚Äúdeath‚Äù, say. If l < t then the individual survives and the binary variable
takes the value Y = 1. If l ‚â•t the individual dies and Y = 0. Denote the
liability associated with datum i as li, and suppose that the underlying
variate is related to an unknown parameter vector Œ≤ (of order p √ó 1) in
terms of the linear model
li = x‚Ä≤
iŒ≤ + ei,
i = 1, 2, ..., N,
(4.67)
where x‚Ä≤
i is the ith row of the known, nonstochastic N √ó p matrix of ex-
planatory variables X and ei is a random residual with p.d.f. p (ei). Assume
that the residuals are independent and identically distributed. The proba-
bility of survival of individual i (which is the p.m.f. of the random variable
Yi) is
Pr (Yi = 1|Œ≤) = Pr (li < t|Œ≤) = Pr (li ‚àíx‚Ä≤
iŒ≤ < t ‚àíx‚Ä≤
iŒ≤|Œ≤)
= Pr (ei < t ‚àíx‚Ä≤
iŒ≤|Œ≤) =
 t‚àíx‚Ä≤
iŒ≤
‚àí‚àû
p (ei) dei
= 1 ‚àí
 ‚àû
t‚àíx‚Ä≤
iŒ≤
p (ei) dei = 1 ‚àí
 x‚Ä≤
iŒ≤‚àít
‚àí‚àû
p (ei) dei.
(4.68)
The last equality above requires that ei is symmetrically distributed around
0. The liabilities cannot be observed, and a convenient origin is to set the
value of the threshold to 0. Hence, the scale is one of deviations from the
threshold. This constraint makes the likelihood model identiÔ¨Åable and the
Hessian becomes negative deÔ¨Ånite.
It is interesting to note that, although in the underlying scale li changes
with xi at a constant rate, this is not so at the level of the probabilities.
This is veriÔ¨Åed by noting that
‚àÇli
‚àÇxi
= Œ≤,
whereas from expression (4.68)
‚àÇPr (y = 1|Œ≤)
‚àÇxi
=
‚àÇ
‚àÇxi

1 ‚àí
 x‚Ä≤
iŒ≤
‚àí‚àû
p (ei) dei

= ‚àí
‚àÇ
‚àÇx‚Ä≤
iŒ≤
 x‚Ä≤
iŒ≤
‚àí‚àû
p (ei) dei

‚àÇx‚Ä≤
iŒ≤
‚àÇxi
= ‚àíp (x‚Ä≤
iŒ≤) Œ≤.
(4.69)

204
4. Further Topics in Likelihood Inference
The change is not constant and depends on the value of the explanatory
vector xi.
4.6.1
The Logistic Distribution
In the analysis of binary responses (Cox and Snell, 1989), two distributions
are often assigned to the residuals. A natural candidate is the normal dis-
tribution, as in linear models. However, because the underlying variable
cannot be observed, the unit of measurement is set to be equal to the stan-
dard deviation, so a N (0, 1) residual distribution is adopted. This leads
to the probit model, and parameter estimates must be interpreted as de-
viations from the threshold in units of standard deviation. Alternatively,
a logistic distribution can be adopted, because it leads to somewhat sim-
pler algebraic expressions. The parameters are also in standard deviation
units. It should be understood, however, that if mechanistic considerations
dictate a normal distribution, this should be preferred.
Consider a random variable Z having, as density function,
p(z) =
exp (z)
[1 + exp (z)]2 ,
‚àí‚àû< z < ‚àû.
(4.70)
Then Z has a logistic distribution with E (Z) = 0 and V ar (Z) = œÄ2/3.
Hence, for a constant k,
Pr (Z < k) =
 k
‚àí‚àû
p (z) dz =
 k
‚àí‚àû
exp (z)
[1 + exp (z)]2 dz
=
exp (k)
[1 + exp (k)].
(4.71)
If the residual distribution in (4.67) is logistic, the probability of survival
in (4.68) is
Pr (Y = 1|Œ≤) = 1 ‚àí
 x‚Ä≤
iŒ≤
‚àí‚àû
p (ei) dei = 1 ‚àí
exp (x‚Ä≤
iŒ≤)
[1 + exp (x‚Ä≤
iŒ≤)]
= [1 + exp (x‚Ä≤
iŒ≤)]‚àí1 = pi (Œ≤) ,
(4.72)
and the probability of death is
Pr (Y = 0|Œ≤) =
exp (x‚Ä≤
iŒ≤)
[1 + exp (x‚Ä≤
iŒ≤)] = 1 ‚àípi (Œ≤) .
(4.73)
4.6.2
Likelihood Function under Bernoulli Sampling
The data consist of binary responses on N subjects and inferential interest
is on Œ≤, the location vector of the underlying distribution. Each of the (0, 1)

4.6 Analysis of Linear Logistic Models
205
outcomes can be viewed as a Bernoulli trial with probability
Pr (Yi = yi|Œ≤) =

1
1 + exp (x‚Ä≤
iŒ≤)
yi 
exp (x‚Ä≤
iŒ≤)
1 + exp (x‚Ä≤
iŒ≤)
1‚àíyi
,
(4.74)
with Yi = 1 if the individual survives, and Yi = 0 otherwise. If, given Œ≤,
the N responses are mutually independent, the probability of observing the
data y is
p (y|Œ≤) =
N
-
i=1

1
1 + exp (x‚Ä≤
iŒ≤)
yi 
exp (x‚Ä≤
iŒ≤)
1 + exp (x‚Ä≤
iŒ≤)
1‚àíyi
.
(4.75)
This is the likelihood function when viewed as a function of Œ≤. The resulting
log-likelihood can be written as
l (Œ≤|y) =
N

i=1
{(1 ‚àíyi) x‚Ä≤
iŒ≤ ‚àíln [1 + exp (x‚Ä≤
iŒ≤)]} .
(4.76)
The score vector is:
l‚Ä≤ (Œ≤|y) =
N

i=1
‚àÇ
‚àÇŒ≤ {(1 ‚àíyi) x‚Ä≤
iŒ≤ ‚àíln [1 + exp (x‚Ä≤
iŒ≤)]}
=
N

i=1
(1 ‚àíyi) xi ‚àí
exp (x‚Ä≤
iŒ≤)
1 + exp (x‚Ä≤
iŒ≤)xi
=
N

i=1
{1 ‚àíyi ‚àí[1 ‚àípi (Œ≤)]} xi
= ‚àí
N

i=1
[yi ‚àípi (Œ≤)] xi.
(4.77)
Now let the N √ó 1 vector of probabilities of survival for the N individuals
be p (Œ≤) , and observe that
N

i=1
[yi ‚àípi (Œ≤)] xi = {x1 [y1 ‚àíp1 (Œ≤)] , ..., xN [yN ‚àípN (Œ≤)]}
= X‚Ä≤ [y ‚àíp (Œ≤)] .
The vector y ‚àíp (Œ≤) consists of deviations of the observations from their
expectations, that is, residuals in the discrete scale. Using this representa-
tion in (4.77) it can be seen that the Ô¨Årst-order condition for a maximum
is satisÔ¨Åed if
X‚Ä≤p

5Œ≤

= X‚Ä≤y
(4.78)

206
4. Further Topics in Likelihood Inference
where p

5Œ≤

is the vector of probabilities of survival for the N individuals
evaluated at the ML estimator 5Œ≤, if this exists. The estimating equations
(4.78) are not explicit in 5Œ≤ and must be solved iteratively. The Newton‚Äì
Raphson algorithm requires second derivatives, and an additional diÔ¨Äeren-
tiation of the log-likelihood with respect to the parameters gives
l‚Ä≤‚Ä≤ (Œ≤|y) = ‚àÇ2l (Œ≤|y)
‚àÇŒ≤ ‚àÇŒ≤‚Ä≤
=
‚àÇ
‚àÇŒ≤‚Ä≤
#
‚àí
N

i=1
[yi ‚àípi (Œ≤)] xi
$
=
N

i=1
xi
‚àÇ
‚àÇŒ≤‚Ä≤ pi (Œ≤) .
(4.79)
Now,
‚àÇ
‚àÇŒ≤‚Ä≤ pi (Œ≤) =
‚àÇ
‚àÇŒ≤‚Ä≤ [1 + exp (x‚Ä≤
iŒ≤)]‚àí1
= ‚àí[1 + exp (x‚Ä≤
iŒ≤)]‚àí2 exp (x‚Ä≤
iŒ≤) x‚Ä≤
i
= ‚àípi (Œ≤) [1 ‚àípi (Œ≤)] x‚Ä≤
i.
Using this in (4.79)
l‚Ä≤‚Ä≤ (Œ≤|y) = ‚àí
N

i=1
xipi (Œ≤) [1 ‚àípi (Œ≤)] x‚Ä≤
i = ‚àíX‚Ä≤D (Œ≤) X,
(4.80)
where D (Œ≤) = {pi (Œ≤) [1 ‚àípi (Œ≤)]} is an N √ó N diagonal matrix. Because
the second derivatives do not depend on the observations, the expected
information is equal to the observed information in this case. Hence, the
Newton‚ÄìRaphson and the scoring algorithms are identical. From (4.1) the
iteration can be represented as

X‚Ä≤D

Œ≤[t]
X

Œ≤[t+1] =

X‚Ä≤D

Œ≤[t]
X

Œ≤[t] + X‚Ä≤v

Œ≤[t]
,
(4.81)
where the vector v

Œ≤[t]
= p

Œ≤[t]
‚àíy. Now let
y‚àó
Œ≤[t]
= XŒ≤[t] + D‚àí1 
Œ≤[t]
v

Œ≤[t]
be a pseudo-data vector evaluated at iteration [t]. Then the system (4.81)
can be written as

X‚Ä≤D

Œ≤[t]
X

Œ≤[t+1] = X‚Ä≤D

Œ≤[t]
y‚àó
Œ≤[t]
.
(4.82)
This is an iterative reweighted least-squares system where the matrix of
weights
D

Œ≤[t]
=
9
pi

Œ≤[t] 
1 ‚àípi

Œ≤[t]:

4.6 Analysis of Linear Logistic Models
207
Calf
Birth weight
Score
Calf
Birth weight
Score
1
40
1
7
47
0
2
40
1
8
47
0
3
40
0
9
47
1
4
43
0
10
50
0
5
43
1
11
50
0
6
43
0
12
50
0
TABLE 4.3. Hypothetical data on birth weight and calving scores taken on 12
calves.
is the reciprocal of the variance of the logit
ln
pi (Œ≤)
1 ‚àípi (Œ≤)
evaluated at Œ≤ = Œ≤[t]; this was shown in Example 4.5. The Newton‚ÄìRaphson
algorithm is iterated until the change in successive rounds is negligible. If
convergence is to a global maximum 5Œ≤, then this is the ML estimate. The
asymptotic variance covariance matrix is estimated as
D
V ar

5Œ≤

=

X‚Ä≤D

5Œ≤

X
‚àí1
.
(4.83)
Example 4.10
Birth weight and calving diÔ¨Éculty
Suppose that each of 12 genetically unrelated cows of the same breed gives
birth to a calf. These are weighed at birth, and a score is assigned to
indicate if there were birth diÔ¨Éculties. The scoring system is: 1 if calving
is normal and 0 otherwise. The objective of the analysis is to assess if there
is a relationship between birth weight and birth diÔ¨Éculty. A logit model is
used where the underlying variate is expressed as
li = Œ± + Œ≤xi+ei,
i = 1, 2, ..., 12,
and where xi is the birth weight in kilograms. The hypothetical data are
in Table 4.3.
Using (4.76) the log-likelihood is
l (Œ±, Œ≤|y) ‚àù(Œ± + Œ≤ 40) ‚àí3 ln [1 + exp (Œ± + Œ≤ 40)]
+ 2 (Œ± + Œ≤ 43) ‚àí3 ln [1 + exp (Œ± + Œ≤ 43)]
+ 2 (Œ± + Œ≤ 47) ‚àí3 ln [1 + exp (Œ± + Œ≤ 47)]
+ 3 (Œ± + Œ≤ 50) ‚àí3 ln [1 + exp (Œ± + Œ≤ 50)] .
The Newton‚ÄìRaphson algorithm gives 5Œ± = ‚àí12.81 and 5Œ≤ = 0.305. The
estimated asymptotic variance‚Äìcovariance matrix is
V ar
 5Œ±
5Œ≤

=

82.476
‚àí1.881
‚àí1.881
.043

.

208
4. Further Topics in Likelihood Inference
The asymptotic standard error of 5Œ≤ is
‚àö
.043 = .2074. A conÔ¨Ådence region
of size 95% is approximately (‚àí.101, .711) . The interval includes the value
0, so this data cannot refute the hypothesis that birth weight does not aÔ¨Äect
the probability of a diÔ¨Écult calving. Because Œ± is a nuisance parameter, and
it is not obvious how a marginal likelihood can be constructed for Œ≤, the
proÔ¨Åle likelihood L

Œ≤, 55Œ± (Œ≤)

is calculated where 55Œ± (Œ≤) is the partial ML
estimator at a Ô¨Åxed value of Œ≤. Numerically, this is done by making a grid
of values of Œ≤, Ô¨Ånding the partial ML estimator of Œ± corresponding to each
value of Œ≤, and then computing the value of the resulting log-likelihood.
Values of 55Œ± (Œ≤) and of the proÔ¨Åle log-likelihood at selected values of Œ≤ are
shown in Table 4.4. The value Œ≤ = .305, the ML estimate, is the maximizer
of the proÔ¨Åle likelihood, as discussed in Section 4.4.3.
‚ñ†
4.6.3
Mixed EÔ¨Äects Linear Logistic Model
Assume that the underlying variable is modelled now as:
li = x‚Ä≤
iŒ≤ + z‚Ä≤
ia+ei,
i = 1, 2, ..., N,
(4.84)
where a ‚àº[0, G (œÜ)] is a vector of random eÔ¨Äects having some distribu-
tion (typically, multivariate normality is assumed in quantitative genetics)
with covariance matrix G (œÜ) . In turn, this covariance matrix may de-
pend on unknown parameters œÜ, which may be variance and covariance
components, for example, for traits that are subject to maternal genetic
inÔ¨Çuences (Willham, 1963). The vector z‚Ä≤
i is a row incidence vector that
plays the same role as x‚Ä≤
i. The residual ei has a logistic distribution, as
before. The probability of survival for the ith individual, given Œ≤ and a,
after setting the threshold to 0, is now
Pr (Yi = 1|Œ≤, a) = Pr (li < t|Œ≤, a) = 1 ‚àí
 x‚Ä≤
iŒ≤+z‚Ä≤
ia
‚àí‚àû
p (ei) dei
=

1 + exp

x‚Ä≤
iŒ≤ + z‚Ä≤
ia
‚àí1 = pi (Œ≤, a) ,
(4.85)
and the conditional probability of death is Pr (Yi = 0|Œ≤, a) = 1 ‚àípi (Œ≤, a) .
Under Bernoulli sampling the conditional probability of observing the data
Œ≤
55Œ± (Œ≤)
L

Œ≤, 55Œ± (Œ≤)

.100
‚àí3.78275
‚àí6.82957
.200
‚àí8.21124
‚àí6.38209
.300
‚àí12.5950
‚àí6.24025
.305
‚àí12.8135
‚àí6.23996
.400
‚àí16.9390
‚àí6.33550
.500
‚àí21.2498
‚àí6.60396
TABLE 4.4. ProÔ¨Åle likelihood for Œ≤.

4.6 Analysis of Linear Logistic Models
209
obtained is then:
p (y|Œ≤, a) =
N
-
i=1
[pi (Œ≤, a)]yi [1 ‚àípi (Œ≤, a)]1‚àíyi .
In order to form the likelihood function, the marginal probability distribu-
tion of the observations is needed. Thus, the joint distribution [y, a|Œ≤, œÜ]
must be integrated over a to obtain
p (y|Œ≤, œÜ) =

N
-
i=1
[pi (Œ≤, a)]yi [1 ‚àípi (Œ≤, a)]1‚àíyi p (a|œÜ) da,
(4.86)
where p (a|œÜ) is the density of the joint distribution of the random eÔ¨Äects.
This integral cannot be expressed in closed form, and must be evaluated by
numerical procedures, such as Gaussian quadrature. As shown below, this
is feasible only when the random eÔ¨Äects are independent (which is seldom
the case in genetic applications), because then the problem reduces to one
of evaluating unidimensional integrals. An alternative is to use Monte Carlo
integration procedures, such as the Gibbs sampler. This will be discussed
in subsequent chapters.
Let the model for liability be
lij = x‚Ä≤
ijŒ≤+ai+eij,
i = 1, 2, ..., S, j = 1, 2, ..., ni.
Thus, there are S random eÔ¨Äects and ni observations are associated with
random eÔ¨Äect ai. Suppose the random eÔ¨Äects are i.i.d. with distribution
ai ‚àºN (0, œÜ) . Then the joint density of the vector of random eÔ¨Äects is
p (a|œÜ) =
S
-
i=1
p (ai|œÜ) .
Using this, (4.86) can be written as
p (y|Œ≤, œÜ) =
S
-
i=1

ni
-
j=1
[pij (Œ≤,ai)]yij [1 ‚àípij (Œ≤,ai)]1‚àíyij p (ai|œÜ) dai
(4.87)
indicating that in (4.87), S single dimension integrals need to be evaluated,
instead of a multivariate integral of order S, as pointed out in connection
with (4.86). However, the integral is not explicit. This illustrates that there
are computational challenges in connection with ML estimation. The same
is true of Bayesian methods, which are introduced in Chapter 5.

This page intentionally left blank

5
An Introduction to
Bayesian Inference
5.1
Introduction
The potential appeal of Bayesian techniques for statistical analysis of ge-
netic data will be motivated using examples from the Ô¨Åeld of animal breed-
ing. Here, two types of data are encountered most often. First, there are
observations obtained from animal production and disease recording pro-
grams; for example, birth and weaning weights in beef cattle breeding
schemes and udder disease data in dairy cattle breeding. These are called
‚ÄúÔ¨Åeld‚Äù records, which are collected directly on the farms where animals
are located. Second, there are data from genetic selection experiments con-
ducted under fairly controlled conditions. For example, there may be lines
of mice selected for increased ovulation rate, and lines in which there is
random selection, serving as ‚Äúcontrols‚Äù. Field records are usually available
in massive amounts, whereas experimental information is often limited.
Suppose milk yield Ô¨Åeld records have been taken on a sample of cows,
and that one wishes to infer the amount of additive genetic variance in
the population, perhaps using a mixed eÔ¨Äects linear model. Often, use may
be made of a large part of an entire national data base. In this situa-
tion, the corresponding full, marginal, or proÔ¨Åle likelihood functions are
expected to be sharp, leading to fairly precise estimates of additive genetic
variance. Naturally, this is highly dependent on the model adopted for anal-
ysis. That is, if an elaborate multivariate structure with a large number of
nuisance parameters is Ô¨Åtted, it is not necessarily true that all proÔ¨Åle or
marginal likelihoods will be sharp. However, in many instances, the amount

212
5. An Introduction to Bayesian Inference
of information is so large that genetic parameters are well-estimated, with
asymptotic theory working handsomely. Here, it is reasonable to expect
that inferences drawn from alternative statistical methods will seldom lead
to qualitatively diÔ¨Äerent conclusions.
Consider now inferences drawn using data from designed experiments,
where estimates tend to be far from precise. A common outcome is that
experimental results are ambiguous, and probably consistent with several
alternative explanations of the state of nature. This is because the scarce
amount of resources available dictates a small eÔ¨Äective population size of
the experimental lines. In this situation, there can be much uncertainty
about the parameters to be inferred remaining after analysis. Here, one
would need to adopt methods capable of conveying accurately the limited
precision of the estimates obtained. Arguably, inference assuming samples
of an inÔ¨Ånite size (asymptotic theory) should not be expected to be satis-
factory.
How would many quantitative geneticists address the following question:
‚ÄúHow much genetic change has taken place in the course of
selection?‚Äù
Suppose that the assumption of joint normality of additive genetic ef-
fects and of phenotypic values is tenable. First, they would probably at-
tempt to estimate components of variance (or covariance) in the base pop-
ulation using either a full or a marginal likelihood; in the latter case, this
leads to REML estimates of the parameters. Second, conditionally on these
estimates, they would proceed to obtain best linear unbiased predictions
(BLUP) of the additive genetic eÔ¨Äects (treated as random) (Henderson,
1973). Theoretically, BLUP is the linear combination of the observations
that minimizes prediction error variance in the class of linear functions
whose expected value is equal to the expected value of the predictand (the
genetic eÔ¨Äects). BLUP can be derived only if the dispersion parameters
(variance and covariance components) are known, at least to proportion-
ality. However, when the latter parameters are estimated from the data at
hand (by REML, say), the resulting empirical BLUP is no longer linear or
best, and remains unbiased only under certain conditions, assuming ran-
dom sampling and absence of selection or of assortative mating (Kackar and
Harville, 1981). Quantitative geneticists often ignore this problem, and pro-
ceed to predict genetic means (these being random over conceptual repeated
sampling) for diÔ¨Äerent generations or cohorts using the empirical BLUP.
From the estimated means, measures of genetic change can be obtained.
It must be noted that if dispersion parameters are known and random
sampling holds, the estimated means have a jointly Gaussian distribution,
with known mean vector and variance‚Äìcovariance matrix. However, one
could argue that if genetic variances were known, there would not be any
need for conducting the experiment! Further, if the REML estimates are

5.1 Introduction
213
used in lieu of the true values, the BLUP analysis ignores their error of
estimation. Here, one could invoke asymptotic theory and hope that the
data are informative enough, such that reality can be approximated well
using limit arguments. The resulting ‚ÄúBLUP‚Äù of a generation mean has an
unknown distribution, so how does one construct tests of the hypothesis
‚Äúselection is eÔ¨Äective‚Äù in such a situation? Also, complications caused by
nonrandom sampling mechanisms are encountered, as parents of a subse-
quent generation are not chosen at random. Unless selection is ignorable,
in some sense, inferences are liable to be distorted. In short, this is an ex-
ample of a situation where the animal breeding paradigms for parameter
estimation (maximum likelihood, asymptotic theory) and for prediction of
random variables (BLUP), used together, are incapable of providing a com-
pletely satisfactory answer to one of the most important questions that can
be posed in applied quantitative genetics. Should one use the conditional
distribution of the random eÔ¨Äects, given the data, ignoring selection and
the error of estimation of the parameters for inferring genetic change?
If one wishes to know what to expect, at least in the frequentist sense
of hypothetical repeated sampling, the only answer would seem to reside
in simulating all conceivable selection schemes and designs, for all possible
values of the parameters. Clearly, this is not feasible. Simulations would
need to be sensibly restricted to experimental settings and to parameter
values that are likely to reÔ¨Çect reality. This implies that at least something
must be known about the state of nature, before experimentation. However,
there is always uncertainty, ranging from mild to large.
An alternative is to adopt the Bayesian point of view. Under this setting,
all unknown quantities in a statistical system are treated as random vari-
ables, reÔ¨Çecting (typically) subjective uncertainty measured by a probabil-
ity distribution. The unknowns may include parameters (e.g., heritability or
the inbreeding coeÔ¨Écient), random eÔ¨Äects (e.g., the additive genetic value
of an artiÔ¨Åcial insemination bull), data that are yet to be observed (e.g.,
the mean of the oÔ¨Äspring of a pair of parents that will be measured under
certain conditions), the sampling distribution adopted for the data gener-
ating process (e.g., given the parameters, the observations may have either
a Gaussian or a multivariate-t distribution), or the entire model itself, en-
gulÔ¨Ång all assumptions made. Here, there may be a number of competing
probability models, of varying dimension. In the Bayesian approach, the
idea is to combine what is known about the statistical ensemble before the
data are observed (this knowledge is represented in terms of a prior proba-
bility distribution) with the information coming from the data, to obtain a
posterior distribution, from which inferences are made using the standard
probability calculus techniques presented in Chapters 1 and 2.
The inferences to be drawn depend on the question posed. Sometimes,
one may seek a marginal posterior distribution, whereas in other instances,
joint or conditional posterior distributions of subsets of variables may be
targets in the analysis. Since any unknown quantity, irrespective of whether

214
5. An Introduction to Bayesian Inference
it is a model, a parameter, or a future data point, is treated symmetrically,
the answer is always found in the same manner, that is, by arriving at
the corresponding posterior distribution via probability theory. The results
of a Bayesian analysis can be presented by displaying the entire posterior
distribution (or density function), or just some posterior summaries, such
as the mean, median, variance, or percentiles. The results are interpreted
probabilistically. For example, if one wishes to infer the mean (¬µ) of a
distribution, one would say ‚Äúthe posterior probability that ¬µ is between a
and b is so much‚Äù. This illustrates how diÔ¨Äerent the Bayesian construct is
from the frequentist-likelihood paradigms.
An overview of the basic elements of the Bayesian approach to inference
will be presented in this chapter. The treatment begins with a description
of Bayes theorem and of its consequences. Subsequently, joint, marginal,
and conditional posterior distributions are introduced, including a presen-
tation of the Bayesian manner of handling nuisance parameters. For the
sake of clarity, the Bayesian probability models considered to illustrate de-
velopments emphasize linear speciÔ¨Åcations for Gaussian observations; lin-
ear models are well-known by quantitative geneticists. The presentation
is introductory, and a much deeper coverage can be found, for example,
in Zellner (1971), Box and Tiao (1973), Lee (1989), Bernardo and Smith
(1994), O‚ÄôHagan (1994), Gelman et al. (1995), and Leonard and Hsu (1999).
Additional topics in Bayesian analysis are discussed in Chapters 6, 7, and
8.
5.2
Bayes Theorem: Discrete Case
Suppose a scientist has M disjoint hypotheses (H1, H2, . . . , HM) about
some mechanism, these being mutually exclusive and, at least temporar-
ily, exhaustive. The latter is an important consideration because at any
point in time, one cannot formulate all possible hypotheses. Rather, the
set H1, H2, . . . , HM constitutes the collection of all hypotheses that can
be formulated by this scientist now, in the light of existing knowledge
(Mal¬¥ecot, 1947). Additional, competing, hypotheses would surely emerge,
as more knowledge is acquired. Obviously, the ‚Äútrue‚Äù hypothesis cannot be
observed, but there may be some inclination by the scientist toward ac-
cepting one in the set as being more likely than the others. In other words,
there may be more certainty that one such hypothesis is true, relative to
the alternatives. In Bayesian analysis, this uncertainty is expressed in terms
of probabilities. In such a context, it is reasonable to speak of a random
variable H taking one of M mutually exclusive and exhaustive states.

5.2 Bayes Theorem: Discrete Case
215
Let p(Hi) be the prior probability assigned by this scientist to the event
‚Äúhypothesis Hi is true‚Äù, with
0 ‚â§p(Hi) ‚â§1,
i = 1, 2, ..., M,
p(Hi ‚à©Hj) = 0,
i Ã∏= j,
M

i=1
p(Hi) = 1.
Then p(Hi) (i = 1, 2, . . . , M) gives the prior probability distribution of
the competing hypotheses. The prior distribution may be elicited either on
subjective grounds, on mechanistic considerations, on evidence available so
far, or using a combination of these three approaches to assess beliefs.
The Bayesian approach provides a description of how existing knowl-
edge is modiÔ¨Åed by experience. Now let there be N observable eÔ¨Äects
E1, E2, . . . , EN. Given that hypothesis Hi holds, one expects to observe
eÔ¨Äects with conditional probabilities
0 ‚â§p(Ej|Hi) ‚â§1,
i = 1, 2, . . . , M, j = 1, 2, . . . , N,
p(Ej ‚à©Ej‚Ä≤|Hi) = 0,
j Ã∏= j‚Ä≤,
N

j=1
p(Ej|Hi) = 1.
Thus, p(Ej|Hi) gives the conditional probability of the eÔ¨Äects observed
under hypothesis Hi, with these eÔ¨Äects being disjoint, that is, Ej and
Ej‚Ä≤ cannot be observed simultaneously. For example, under a one-locus
Mendelian model (the hypothesis), an individual cannot be homozygote
and heterozygote at the same time. This conditional distribution represents
the probabilities of eÔ¨Äects to be observed if experimentation proceeded un-
der the conditions imposed by the hypothesis. Again, under Mendelian
inheritance, random mating, no migration or mutation and a large pop-
ulation, one would expect the probabilities of observing AA, Aa, and aa
individuals to be those resulting from the Hardy‚ÄìWeinberg equilibrium.
Then let E be a random variable taking one of the states Ej (j =
1, 2, . . . , N), and let H and E have the joint distribution
p(H = Hi, E = Ej) = p(Ej|Hi)p(Hi).
The conditional probability that hypothesis Hi holds, given that eÔ¨Äects Ej
are observed, is then
p(Hi|Ej) = p(H = Hi, E = Ej)
p(Ej)
= p(Ej|Hi)p(Hi)
p(Ej)
,
(5.1)

216
5. An Introduction to Bayesian Inference
where p(Ej) is the marginal or total probability of observing eÔ¨Äect Ej, that
is, the probability of observing Ej over all possible hypotheses
p(Ej) =
M

i=1
p(Ej|Hi)p(Hi) = EH [p(Ej|Hi)] ,
(5.2)
where EH (¬∑) indicates an expectation taken with respect to the prior dis-
tribution of the hypotheses. Using (5.2) in (5.1)
p(Hi|Ej)
=
p(Ej|Hi)p(Hi)
EH [p(Ej|Hi)]
(5.3)
‚àù
p(Ej|Hi)p(Hi).
(5.4)
This standard result of conditional probability is also known as Bayes the-
orem when applied to the speciÔ¨Åc problem of inferring ‚Äúcauses‚Äù from ‚Äúef-
fects‚Äù, and it is also called ‚Äúinverse probability‚Äù. It states that the proba-
bility of a cause or hypothesis Hi, given evidence Ej, is proportional to the
product of the prior probability assigned to the hypothesis, p(Hi), times the
conditional probability of observing the eÔ¨Äect Ej under hypothesis Hi. The
distribution in (5.3) or (5.4) is called the posterior probability distribution,
with the denominator in (5.3) acting as normalizing constant.
Expression (5.4) illustrates a concept called ‚ÄúBayesian learning‚Äù. This
is the process by which a prior opinion (with the associated uncertainty
stated by the prior distribution) is modiÔ¨Åed by evidence E (generated with
uncertainty under a sampling model characterized by a distribution with
probabilities p(Ej|Hi)), to become a posterior opinion, this having a distri-
bution with probabilities p(Hi|Ej). Suppose now that additional evidence
E‚Ä≤
j‚Ä≤ accrues. Using Bayes theorem in (5.3) or (5.4):
p(Hi|E‚Ä≤
j‚Ä≤, Ej)
=
p(E‚Ä≤
j‚Ä≤, Ej|Hi)p(Hi)
EH[p(E‚Ä≤
j‚Ä≤, Ej|Hi)]
‚àù
p(E‚Ä≤
j‚Ä≤, Ej|Hi)p(Hi)
‚àù
p(E‚Ä≤
j‚Ä≤|Ej, Hi)p(Ej|Hi)p(Hi)
‚àù
p(E‚Ä≤
j‚Ä≤|Ej, Hi)p(Hi|Ej).
(5.5)
The preceding indicates that the posterior distribution after evidence Ej
conveys the prior opinion before E‚Ä≤
j‚Ä≤ is observed. It also describes how
opinions are revised sequentially or, equivalently, how knowledge is modiÔ¨Åed
by evidence. If, given Hi, E‚Ä≤
j‚Ä≤ is conditionally independent of Ej, then
p(E‚Ä≤
j‚Ä≤|Ej, Hi)p(Ej|Hi) = p(E‚Ä≤
j‚Ä≤|Hi)p(Ej|Hi).
More generally, for S pieces of evidence, assuming conditional indepen-
dence,
p(Hi|ES
jS, ES‚àí1
jS‚àí1, ..., E2
j2, E1
j1) ‚àù
S
-
k=1
p(Ek
jk|Hi)p(Hi),
(5.6)

5.2 Bayes Theorem: Discrete Case
217
where Ek
jk denotes the evidence in datum k, with j = 1, 2, ..., N indicating
the diÔ¨Äerent values that E can take at any step k of the process of accumu-
lating information. Letting E be the entire evidence, (5.6) can be written
as
p(Hi|E) ‚àùexp
# S

k=1
log

p(Ek
jk|Hi)

+ log p(Hi)
$
‚àùexp

SL + log p(Hi)

‚àùexp

SL

1 + log p(Hi)
S L
%
,
(5.7)
where
L = 1
S
S

k=1
log

p(Ek
jk|Hi)

is the average log-probability of observing Ek
jk under hypothesis Hi. Now,
letting S ‚Üí‚àû, and provided that p(Hi) > 0, it can be seen that the expo-
nent in expression (5.7) tends toward SL, indicating that the contribution
of the prior to the posterior is of order 1/S. This implies that the evidence
tends to overwhelm the prior as more and more information accumulates
so, for large S,
p(Hi|E) ‚àù
S
-
k=1
p(Ek
jk|Hi).
(5.8)
Following O‚ÄôHagan (1994), note from (5.1) that evidence E will increase
the probability of a hypothesis H only if
p(E|H) > p(E),
where subscripts are ignored, for simplicity. Now, from (5.2) and denoting
as H the event ‚ÄúH not true‚Äù, with p(H) being the corresponding probabil-
ity, one can write
p(E)
=
p(E|H)p(H) + p(E|H)p(H)
=
p(E|H)

1 ‚àíp(H)

+ p(E|H)p(H).
Rearranging
p(E|H) ‚àíp(E) =

p(E|H) ‚àíp(E|H)

p(H).
This indicates that evidence E will increase the probability of a hypothesis
if and only if p(E|H) > p(E|H), that is, if the chances of observing E
are larger under H than under any of the competing hypothesis. If this
is the case, it is said that E confers a higher likelihood to H than to H;
thus, p(E|H) is called the likelihood of H. This is exactly the concept

218
5. An Introduction to Bayesian Inference
of likelihood function discussed in Chapter 3, that is, the probability (or
density) of the observations viewed as a function of the parameters (the
hypotheses play the role of the parameter values in this discussion). Thus,
the maximum likelihood estimator is the function of the data conferring
the highest likelihood to a particular value of the parameter.
The controversy in statistics about the use of Bayes theorem in science
centers on that the prior distribution is often based on subjective, if not
arbitrary (or convenient), elicitation. In response to this criticism, Savage
(1972) wrote:
‚ÄúIt has been countered, I believe, that if experience system-
atically leads people with opinions originally diÔ¨Äerent to hold a
common opinion, then that common opinion, and it only, is the
proper subject of scientiÔ¨Åc probability theory. There are two
inaccuracies in this argument. In the Ô¨Årst place, the conclusion
of the personalistic view is not that evidence brings holders of
diÔ¨Äerent opinions to the same opinions, but rather to similar
opinions. In the second place, it is typically true of any obser-
vational program, however extensive but prescribed in advance,
that there exist pairs of opinions, neither of which can be called
extreme in any precisely deÔ¨Åned sense, but which cannot be
expected, either by their holders or any other person, to be
brought into close agreement after the observational program.‚Äù
Furthermore, Box (1980) argued as follows:
‚ÄúIn the past, the need for probabilities expressing prior be-
liefs has often been thought of, not as a necessity for all scientiÔ¨Åc
inference, but rather as a feature peculiar to Bayesian inference.
This seems to come from the curious idea that an outright as-
sumption does not count as a prior belief...I believe that it is
impossible logically to distinguish between model assumptions
and the prior distribution of the parameters.‚Äù
The probability distribution of the hypotheses cannot be construed as
a frequency distribution, that is, as a random process generated as if hy-
potheses were drawn at random, with replacement, from an urn. If this were
the case, one could refute or corroborate the prior distribution by draw-
ing a huge number of independent samples from the said urn. Technically,
however, there would always be ambiguity, unless the number of samples
is inÔ¨Ånite! Instead, the prior probabilities in Bayesian inference must be
interpreted as relative degrees of belief about the state of nature, before
any experimentation or observation takes place.
The debate about the alternative methods of inference has a long his-
tory with eloquent arguments from both camps; we do not feel much can
be added here. A balanced comparative overview of methods of inference

5.2 Bayes Theorem: Discrete Case
219
is presented by Barnett (1999). A partisan, in-depth presentation of the
Bayesian approach is given in Bernardo and Smith (1994) and O‚ÄôHagan
(1994). A philosophical discussion of the concept(s) of probability can be
found in Popper (1972, 1982), who holds a strong, anti-inductive and anti-
subjective position. Indeed, Popper (1982) is concerned with the inÔ¨Çuence
that subjective probability has had on quantum mechanics. Popper cites
Heisenberg (1958), who writes:
‚ÄúThe conception of objective reality ... has thus evaporated
... into the transparent clarity of a mathematics that represents
no longer the behavior of particles but rather our knowledge of
this behavior‚Äù.
For a detailed and focused philosophical critique of the Bayesian ap-
proach to inference, the reader can consult the books of Howson and Urbach
(1989) and Earman (1992).
When a prior distribution is ‚Äúuniversally agreed upon‚Äù, or when it is
based on mechanistic considerations, then Bayes theorem is accepted as a
basis for inference without reservation. Two examples of the latter situation
follow.
Example 5.1
Incidence of a rare disease
Consider a rare disease whose frequency in the population is 1 in 5, 000
(i.e., 0.0002). A test for detecting the disease is available and it has a false
positive rate of 0.05 and a false negative rate of 0.01. A person is taken
at random from the population and the test gives a positive result. What
is the probability that the person is diseased? Let Œ∏ represent a random
variable taking the value 1 if the person is diseased and 0 otherwise. Let
Y be a random variable that takes the value 1 if the test is positive, and
0 if the test is negative. The data are here represented by the single value
Y = 1. The prior probability (before the test result is available) that a
randomly chosen individual is diseased is
Pr (Œ∏ = 1) = 0.0002.
Based on the false positive and false negative rates, we can write
Pr (Y = 1|Œ∏ = 0)
=
0.05,
Pr (Y = 0|Œ∏ = 0) = 0.95,
Pr (Y = 0|Œ∏ = 1)
=
0.01,
Pr (Y = 1|Œ∏ = 1) = 0.99.
Applying Bayes theorem, the posterior probability that the individual is
diseased (after having observed Y = 1) is given by
Pr (Œ∏ = 1|Y = 1)
=
Pr (Œ∏ = 1) Pr (Y = 1|Œ∏ = 1)
Pr (Œ∏ = 1) Pr (Y = 1|Œ∏ = 1) + Pr (Œ∏ = 0) Pr (Y = 1|Œ∏ = 0)
=
(0.0002) (0.99)
(0.0002) (0.99) + (0.9998) (0.05) ‚âà0.0039.

220
5. An Introduction to Bayesian Inference
Thus, a positive test, has raised the probability that the individual has
the disease from 0.0002 (the a priori probability before the test result is
available) to 0.0039 (the posterior probability after observing Y = 1).
‚ñ†
Example 5.2
Inheritance of hemophilia
The following is adapted from Gelman et al. (1995). Hemophilia is a ge-
netic disease in humans. The locus responsible for its expression is located
in the sex chromosomes (these are denoted as XX in women, and XY in
men). The condition is observed in women only in double recessive individ-
uals (aa), and in men that are carriers of the a allele in the X-chromosome.
Suppose there is a nonhemophiliac woman whose father and mother are not
aÔ¨Äected by the disease, but her brother is known to be hemophiliac. This
implies that her nonhemophiliac mother must be heterozygote, a carrier of
a. What is the probability that the propositus woman is also a carrier of
the gene? Let Œ∏ be a random variable taking one of two mutually exclusive
and exhaustive values (playing the role of the hypotheses in the preceding
section). Either Œ∏ = 1 if the woman is a carrier, or Œ∏ = 0 otherwise. Since
it is known that the mother of the woman must be a carrier (this consti-
tutes part of the system within which probabilities are assigned), the prior
distribution of Œ∏ is
Pr (Œ∏ = 1) = Pr (Œ∏ = 0) = 1
2.
In the absence of additional information, it is not possible to make a very
sharp probability assignment. Suppose now that the woman has two sons,
none of which is aÔ¨Äected. Let Yi be a binary random variable taking the
value 0 if son i is not aÔ¨Äected, or 1 if he has the disease; thus, the values
of Y1 and Y2 constitute the evidence E. Given that Œ∏ = 1, the probability
of the observed data is
Pr (Y1 = 0, Y2 = 0|Œ∏ = 1)
=
Pr (Y1 = 0|Œ∏ = 1) Pr (Y2 = 0|Œ∏ = 1) =

1
2
2
= 1
4.
This follows because:
a) the observations are assumed to be independent, conditionally on Œ∏ and
b) if the woman is a carrier (Œ∏ = 1), there is a 50% probability that she
will not transmit the allele.
On the other hand, if she is not a carrier (Œ∏ = 0):
Pr (Y1 = 0, Y2 = 0|Œ∏ = 0)
=
Pr (Y1 = 0|Œ∏ = 0) Pr (Y2 = 0|Œ∏ = 0) = 1 √ó 1 = 1,
this being so because it is impossible for a son to have the disease unless the
mother is a carrier (ignoring mutation). Hence, the data confer four times

5.2 Bayes Theorem: Discrete Case
221
more likelihood to the hypothesis that the mother is not a carrier. Using
the information that none of the sons is diseased, the posterior distribution
of Œ∏ is then
Pr (Œ∏ = 1|Y1 = 0, Y2 = 0) = Pr (Œ∏ = 1) Pr (Y1 = 0, Y2 = 0|Œ∏ = 1)
Pr (Y1 = 0, Y2 = 0)
=
Pr (Œ∏ = 1) Pr (Y1 = 0, Y2 = 0|Œ∏ = 1)
1
i=0
Pr (Œ∏ = i) Pr (Y1 = 0, Y2 = 0|Œ∏ = i)
=
1
2
1
4
1
21 + 1
2
1
4
= 1
5
and
Pr (Œ∏ = 0|Y1 = 0, Y2 = 0) = 1 ‚àí1
5 = 4
5.
A sharper probability assignment can be made now, and the combination
of prior information with the evidence suggests that the mother is probably
not a carrier. The latter possibility cannot be ruled out, however, as there
is a 20% probability that the mother is heterozygote. The posterior odds
in favor of the hypothesis that the mother is not a carrier is given by the
ratio
Pr (Œ∏ = 0|Y1 = 0, Y2 = 0)
Pr (Œ∏ = 1|Y1 = 0, Y2 = 0) = Pr (Y1 = 0, Y2 = 0|Œ∏ = 0)
Pr (Y1 = 0, Y2 = 0|Œ∏ = 1)
Pr (Œ∏ = 0)
Pr (Œ∏ = 1)
= 1
1
4
1
2
1
2
= 4,
where the ratio
Pr (Œ∏ = 0)
Pr (Œ∏ = 1) = 1
is called the prior odds in favor of the hypothesis. Further,
B01 = Pr (Y1 = 0, Y2 = 0|Œ∏ = 0)
Pr (Y1 = 0, Y2 = 0|Œ∏ = 1) = 4
is called the Bayes factor, that is, the factor by which the prior odds about
the hypotheses are modiÔ¨Åed by the evidence and converted into posterior
odds (a more thorough discussion of Bayes factors will be presented in
Chapter 8). In this example, the odds in favor of the hypothesis that Œ∏ = 0
relative to Œ∏ = 1 increase by a factor of 4 after observing two sons that are
not aÔ¨Äected by the disease. Suppose that the woman suspected of being a
carrier has n children. The posterior distribution of Œ∏ can be represented
as
Pr (Œ∏ = i|y) =
Pr (Œ∏ = i) Pr (y|Œ∏ = i)
Pr (Œ∏ = i) Pr (y|Œ∏ = i) + Pr (Œ∏ Ã∏= i) Pr (y|Œ∏ Ã∏= i),
i = 0, 1,

222
5. An Introduction to Bayesian Inference
where y = [Y1, Y2, ..., Yn]‚Ä≤. Partition the data as y = [y‚Ä≤
A, y‚Ä≤
B]‚Ä≤ , with yA be-
ing the records on presence or absence of the disease for the Ô¨Årst m progeny,
and with yB containing data on the last n ‚àím children. The posterior dis-
tribution is
Pr (Œ∏ = i|y) =
Pr (Œ∏ = i) p (yA|Œ∏ = i) p (yB|yA, Œ∏ = i)
1
i=0
Pr (Œ∏ = i) p (yA|Œ∏ = i) p (yB|yA, Œ∏ = i)
.
Dividing the numerator and denominator by the marginal probability of
observing yA, that is, by p (yA) gives
Pr (Œ∏ = i|y) =
Pr (Œ∏ = i) p (yA|Œ∏ = i)
p (yA)
p (yB|yA, Œ∏ = i)
1
i=0
Pr (Œ∏ = i) p (yA|Œ∏ = i)
p (yA)
p (yB|yA, Œ∏ = i)
.
Note, however, that
Pr (Œ∏ = i) p (yA|Œ∏ = i)
p (yA)
= Pr (Œ∏ = i|yA)
is the posterior probability after observing yA, which acts as a prior before
observing yB. Then, it follows that
Pr (Œ∏ = i|y) =
Pr (Œ∏ = i|yA) p (yB|yA, Œ∏ = i)
1
i=0
Pr (Œ∏ = i|yA) p (yB|yA, Œ∏ = i)
illustrating the ‚Äúmemory‚Äù property of Bayes theorem. If the observations
are conditionally independent, as assumed in this example, then
p (yB|yA, Œ∏ = i) = p (yB|Œ∏ = i) .
Suppose now that the woman has a third, unaÔ¨Äected, son. The prior dis-
tribution now assigns probabilities of 4
5 and 1
5 to the events ‚Äúnot being a
carrier‚Äù and ‚Äúcarrying the allele‚Äù, respectively. The posterior probability
of the woman being a carrier, after observing a third, unaÔ¨Äected child, is
Pr (Œ∏ = 1|Y1 = 0, Y2 = 0, Y3 = 0)
=
1
5 Pr (Y3 = 0|Œ∏ = 1)
1
5 Pr (Y3 = 0|Œ∏ = 1) + 4
5 Pr (Y3 = 0|Œ∏ = 0)
=
1
5
1
2
1
5
1
2 + 4
51 = 1
9.

5.3 Bayes Theorem: Continuous Case
223
The same result is obtained starting from the prior before observing any
children
Pr (Œ∏ = 1|Y1 = 0, Y2 = 0, Y3 = 0) =
1
2.
 1
2
3
1
2.
 1
2
3 + 1
2. (1)3
= 1
9.
‚ñ†
5.3
Bayes Theorem: Continuous Case
In a somewhat narrower setting, consider now the situation where the role
of the evidence E is played by a vector of observations y, with the hypoth-
esis H replaced by a vector of unknowns Œ∏. The latter will be generally re-
ferred to as the ‚Äúparameter‚Äù vector, although Œ∏ can include random eÔ¨Äects
(in the usual frequentist sense), missing data, censored observations, etc.
It will be assumed that Œ∏ and y are continuous valued, and that a certain
probability model M posits the joint distribution [Œ∏, y|M] . For example,
M may postulate that this distribution is jointly Gaussian, whereas model
M ‚Ä≤ supposes a multivariate-t distribution. Alternatively, M and M ‚Ä≤ could
represent two alternative explanatory structures in a regression model. At
this point it will be assumed that there is complete certainty about model
M holding, although this may not be so, in which case one encounters the
important problem of Bayesian model selection. We will return to this later
on but now, with the understanding that developments are conditional on
model M, the dependency on the model will be abandoned in the notation.
The joint density of Œ∏ and y can be written as
h (Œ∏, y) = g (Œ∏) f (y|Œ∏) = m(y)p (Œ∏|y) ,
(5.9)
where
‚Ä¢ g (Œ∏) is the marginal density of Œ∏:
g (Œ∏) =

h (Œ∏, y) dy =

p (Œ∏|y) m(y) dy =Ey [p (Œ∏|y)] ,
The corresponding distribution describes the plausibility of values
that Œ∏ takes in a parameter space Œò, unconditionally on the ob-
servations y. This is the density of the prior distribution of Œ∏, which
provides a summary of nonsample information about the parameters.
Values of Œ∏ outside of Œò have null density. For example, a reason-
able Bayesian model would assign null density to values of a genetic
correlation outside of the [‚àí1, 1] boundaries, or to negative values of

224
5. An Introduction to Bayesian Inference
a variance component. In the preceding, as well as in all subsequent
developments, it will be assumed that the required integrals exist,
unless stated otherwise.
‚Ä¢ f (y|Œ∏) is the density of values that y takes at a given, albeit unknown,
value of Œ∏. It represents the likelihood that evidence y confers to Œ∏;
the part of f (y|Œ∏) that varies with Œ∏ is called the likelihood function
or, simply, the likelihood of Œ∏. The set of possible values that y can
take is given by ‚Ñúy, the sampling space of y. The integration above
is implicitly over this sampling space. For example, if the data vector
contains some truncated random variables, the sampling space would
be given by the boundaries within which these variables are allowed
to vary.
‚Ä¢ m(y) is the density of the marginal distribution of the observations.
In the Bayesian context, this does not depend on Œ∏, since
m (y) =

h (Œ∏, y) dŒ∏ =

f (y|Œ∏) g (Œ∏) dŒ∏ = EŒ∏ [f (y|Œ∏)] ,
where the integration is over the sample space of Œ∏, Œò. This implies
that m (y) is the average, taken over the prior distribution of Œ∏, of
all possible likelihood values that the evidence y would confer to Œ∏.
‚Ä¢ p (Œ∏|y) is the density of the posterior distribution of Œ∏, [Œ∏|y] , provid-
ing a summary of the information about Œ∏ contained in both y and
in the prior distribution [Œ∏] . From (5.9), the posterior density can be
written as
p (Œ∏|y) = g (Œ∏) f (y|Œ∏)
m(y)
‚àùg (Œ∏) f (y|Œ∏)
(5.10)
as one is interested in variation with respect to Œ∏ only. Further, let
L (Œ∏|y) be the part of f (y|Œ∏) varying with Œ∏, or likelihood function.
Thus, an alternative representation of the posterior density is
p (Œ∏|y) ‚àùg (Œ∏) L (Œ∏|y) ,
(5.11)
or
p (Œ∏|y) =
g (Œ∏) L (Œ∏|y)

g (Œ∏) L (Œ∏|y) dŒ∏ .
(5.12)
A special case of Bayesian analysis is encountered when the prior
density g (Œ∏) is uniform over the entire parameter space; in other
words, g (Œ∏) is proportional to a constant. This is called a ‚ÄúÔ¨Çat prior‚Äù.
The constant cancels in the numerator and denominator of (5.12), and
the posterior becomes
p (Œ∏|y) =
L (Œ∏|y)

L (Œ∏|y) dŒ∏ ‚àùL (Œ∏|y) .

5.3 Bayes Theorem: Continuous Case
225
Hence, the posterior is proportional solely to the likelihood, and it
exists only if the integral of the likelihood over Œ∏ is Ô¨Ånite. Otherwise,
the posterior density is said to be improper. Note that if the required
integral converges, the integration constant of the posterior is the
reciprocal of the integrated likelihood. Also, if the posterior distri-
bution exists, the mode of the posterior density is identical to the
ML estimator. This suggests that this estimator is a feature of any
(existing) posterior distribution constructed from an initial position
where the observer is indiÔ¨Äerent to any of the possible values of Œ∏,
when this is represented by the uniform distribution.
Example 5.3
Inferring additive genetic eÔ¨Äects
Let an observation for a quantitative trait be y. Suppose that a reasonable
model for representing a phenotypic value is
y = ¬µ + a + e,
where ¬µ is a known constant, a is the additive genetic eÔ¨Äect of the individ-
ual, and e is an environmental deviation. Let a ‚àºN(0, va) and e ‚àºN(0, ve)
be independently distributed, where va and ve are the additive genetic
and environmental variances, respectively, both being assumed known here.
Thus, y must also be normal. It follows that the conditional distribution of
y given a is the normal process
[y|¬µ, a, va, ve] ‚àºN (¬µ + a, ve) .
Suppose the problem is inferring a from y. Here, Œ∏ = a. The prior density
of a is normal, with the parameters given above. The posterior distribution
of interest is then:
[a|y, ¬µ, va, ve]
which is identical to the conditional distribution of a given y in a frequentist
sense. If a and y are jointly normal, as is the case here, it follows directly
that the posterior distribution must be normal as well, with mean
E (a|y, ¬µ, va, ve)
=
E (a) + Cov(a, y) V ar‚àí1 (y) (y ‚àí¬µ)
=
va
va + ve
(y ‚àí¬µ) = h2 (y ‚àí¬µ) ,
where h2 is the heritability of the trait. The posterior variance is
V ar (a|y, ¬µ, va, ve) = va ‚àí
v2
a
va + ve
= va

1 ‚àíh2
so there will always be some uncertainty about the genetic value, given the
phenotypic value, unless the genotype has complete penetrance, that is,

226
5. An Introduction to Bayesian Inference
when there is no environmental variance. In view of the mean and variance
of the posterior distribution, the corresponding density is
p (a|y, ¬µ, va, ve) =
1
>
2œÄva (1 ‚àíh2)
exp
#
‚àí

a ‚àíh2 (y ‚àí¬µ)
2
2va (1 ‚àíh2)
$
.
Note that the prior opinion has been modiÔ¨Åed by the evidence provided by
observation of the phenotypic value. For example, in the prior distribution
the modal value of a is 0, whereas it is h2 (y ‚àí¬µ) , a posteriori. Further, in
the prior distribution, the probability that a > 0 is 1/2. In the posterior
distribution, this value is
Pr (a > 0|y, ¬µ, va, ve) = 1 ‚àíPr (a ‚â§0|y, ¬µ, va, ve)
= 1 ‚àíŒ¶

‚àíh2 (y ‚àí¬µ)
>
va (1 ‚àíh2)

= Œ¶

h2 (y ‚àí¬µ)
>
va (1 ‚àíh2)

.
The marginal density of the observation is
p (y|¬µ, va, ve) =
1
>
2œÄ (va + ve)
exp

‚àí(y ‚àí¬µ)2
2 (va + ve)

.
This follows because the model states that the phenotypic value is a linear
combination of two normally distributed random variables. The expression
is identical to the marginal density of the observations in a frequentist
setting, because our Bayesian model does not postulate uncertainty about
¬µ, va and ve. Otherwise, the marginal density given above would need to
be deconditioned over the prior distribution of ¬µ, va, and ve.
‚ñ†
Example 5.4
Inferring additive genetic eÔ¨Äects from repeated measures
Let the setting be as in the preceding example. Suppose now that n in-
dependent measurements are taken on the individual, and that ¬µ is an
unknown quantity, with prior distribution ¬µ ‚àºN (¬µ0, v0) , where the hy-
perparameters (parameters of the prior distribution) are known. The model
for the ith measure is then
yi = ¬µ + a + ei.
(5.13)

5.3 Bayes Theorem: Continuous Case
227
If ¬µ and a are assumed to be independent, a priori, the joint posterior
density is
p (¬µ, a|y1, y2, ..., yn, ¬µ0, v0, va, ve)
‚àù
n
-
i=1
p(yi|¬µ, a, ve)p (a|va) p (¬µ|¬µ0, v0)
‚àù
n
-
i=1
exp

‚àí(yi ‚àí¬µ ‚àía)2
2ve

exp

‚àía2
2va

exp

‚àí(¬µ ‚àí¬µ0)2
2v0

‚àùexp
Ô£Æ
Ô£ØÔ£ØÔ£∞‚àí
n
i=1
(yi ‚àí¬µ ‚àía)2
2ve
‚àía2
2va
Ô£π
Ô£∫Ô£∫Ô£ªexp

‚àí(¬µ ‚àí¬µ0)2
2v0

.
(5.14)
Now
n
i=1
(yi ‚àí¬µ ‚àía)2
ve
+ a2
va
=
n
i=1
[a + ¬µ ‚àíyi]2
ve
+ a2
va
=
n
i=1
[a ‚àí(y ‚àí¬µ) ‚àí(yi ‚àíy)]2
ve
+ a2
va
=
n [a ‚àí(y ‚àí¬µ)]2 +
n
i=1
(yi ‚àíy)2
ve
+ a2
va
,
where y is the average of the n records; recall that n
i=1 (yi ‚àíy) = 0. Using
this in the joint posterior (5.14):
p (¬µ, a|y, ¬µ0, v0, va, ve)
‚àùexp

‚àí1
2
#
n [a ‚àí(y ‚àí¬µ)]2
ve
+ a2
va
$
exp

‚àí(¬µ ‚àí¬µ0)2
2v0

(5.15)
as expressions not involving ¬µ or a get absorbed in the integration constant.
Now, there is an identity for combining quadratic forms (Box and Tiao,
1973) stating that
M(z ‚àím)2 + B(z ‚àíb)2 = (M + B)(z ‚àíc)2 +
MB
M + B (m ‚àíb)2
(5.16)
with
c = (M + B)‚àí1 (Mm + Bb) .
(5.17)

228
5. An Introduction to Bayesian Inference
Now put
m
=
y ‚àí¬µ,
b
=
0,
M
=
n
ve
,
B
=
1
va
,
z
=
a.
Hence, by analogy,
n [a ‚àí(y ‚àí¬µ)]2
ve
+ a2
va
=

 n
ve
+ 1
va
 #
a ‚àí

 n
ve
+ 1
va
‚àí1  n
ve
(y ‚àí¬µ)
$2
+
n
ve
1
va
n
ve +
1
va
(y ‚àí¬µ)2.
Employing this decomposition, the joint posterior density in (5.15) is ex-
pressible as
p (¬µ, a|y, ¬µ0, v0, va, ve)
‚àùexp
Ô£Æ
Ô£∞‚àí1
2

 n
ve
+ 1
va
 #
a ‚àí

 n
ve
+ 1
va
‚àí1  n
ve
(y ‚àí¬µ)
$2Ô£π
Ô£ª
√ó exp

‚àín(y ‚àí¬µ)2
2 (nva + ve)

exp

‚àí(¬µ ‚àí¬µ0)2
2v0

.
(5.18)
From this, one can proceed to derive a series of distributions of interest, as
given below.
Conditional (given ¬µ) posterior density of the additive genetic eÔ¨Äect. This
is obtained by retaining the part of the joint posterior that varies with a:
p (a|¬µ, y, ¬µ0, v0, va, ve)
‚àùexp
Ô£Æ
Ô£∞‚àí1
2

 n
ve
+ 1
va
 #
a ‚àí

 n
ve
+ 1
va
‚àí1  n
ve
(y ‚àí¬µ)
$2Ô£π
Ô£ª
(5.19)
This is clearly the density of a normal distribution with mean
E (a|¬µ, y, ¬µ0, v0, va, ve)
=

 n
ve
+ 1
va
‚àí1  n
ve
(y ‚àí¬µ)

=
va
va + ve
n
(y ‚àí¬µ)
=
n
n + 1‚àíh2
h2
(y ‚àí¬µ)

5.3 Bayes Theorem: Continuous Case
229
and variance
V ar (a|¬µ, y, ¬µ0, v0, va, ve) =

 n
ve
+ 1
va
‚àí1
= ve

n + 1 ‚àíh2
h2
‚àí1
.
Note that this posterior distribution depends on the data only through y.
Conditional (given a) posterior density of ¬µ. This is arrived at in a
similar manner, that is, by retaining in (5.14) only the terms varying with
¬µ. One obtains
p (¬µ|a, y, ¬µ0, v0, va, ve) ‚àùexp
Ô£Æ
Ô£ØÔ£ØÔ£∞‚àí
n
i=1
(yi ‚àí¬µ ‚àía)2
2ve
Ô£π
Ô£∫Ô£∫Ô£ªexp

‚àí(¬µ ‚àí¬µ0)2
2v0

.
Here, using some of the previous results,
n
i=1
(yi ‚àí¬µ ‚àía)2
ve
+ (¬µ ‚àí¬µ0)2
v0
=
n [¬µ ‚àí(y ‚àía)]2 +
n
i=1
(yi ‚àíy)2
ve
+ (¬µ ‚àí¬µ0)2
v0
.
There are now two quadratic forms on ¬µ that can be combined, employing
(5.16) and (5.17), as
n [¬µ ‚àí(y ‚àía)]2
ve
+ (¬µ ‚àí¬µ0)2
v0
=

 n
ve
+ 1
v0
 #
¬µ ‚àí

 n
ve
+ 1
v0
‚àí1  n
ve
(y ‚àía) + ¬µ0
v0
$2
+
n
ve . 1
v0
n
ve + 1
v0
(y ‚àí¬µ0 ‚àía)2 .
(5.20)
Using this in the conditional density, and retaining only the terms that
vary with ¬µ
p (¬µ|a, y, ¬µ0, v0, va, ve)
‚àùexp
Ô£Æ
Ô£∞‚àí1
2

 n
ve
+ 1
v0
 #
¬µ ‚àí

 n
ve
+ 1
v0
‚àí1  n
ve
(y ‚àía) + ¬µ0
v0
$2Ô£π
Ô£ª.
(5.21)

230
5. An Introduction to Bayesian Inference
It follows that the preceding density is that of a normal process with pa-
rameters
E (¬µ|a, y, ¬µ0, v0, va, ve) =

 n
ve
+ 1
v0
‚àí1  n
ve
(y ‚àía) + ¬µ0
v0

= ¬µ0 +
n
n + ve
v0
(y ‚àí¬µ0 ‚àía)
and
V ar (¬µ|a, y, ¬µ0, v0, va, ve) =

 n
ve
+ 1
v0
‚àí1
= ve

n + ve
v0
‚àí1
.
Marginal posterior density of ¬µ. This is found by integrating the joint
density (5.18) over a:
p (¬µ|y, ¬µ0, v0, va, ve)
=

p (¬µ, a|y, ¬µ0, v0, va, ve) da
‚àùexp
#
‚àí1
2

n
(nva + ve)(y ‚àí¬µ)2 + (¬µ ‚àí¬µ0)2
v0
$
√ó

exp
Ô£Æ
Ô£∞‚àí1
2

 n
ve
+ 1
va
 #
a ‚àí

 n
ve
+ 1
va
‚àí1  n
ve
(y ‚àí¬µ)
$2Ô£π
Ô£ªda.
The integral is over a normal kernel, and evaluates to
C
2œÄ

 n
ve
+ 1
va
‚àí1
.
Noting that the expression does not involve ¬µ, it follows that
p (¬µ|y, ¬µ0, v0, va, ve) ‚àùexp
#
‚àí1
2

n
(nva + ve)(y ‚àí¬µ)2 + (¬µ ‚àí¬µ0)2
v0
$
.
Using (5.16) and (5.17), the two quadratic forms on ¬µ can be combined as
n
(nva + ve)(¬µ ‚àíy)2 + (¬µ ‚àí¬µ0)2
v0
= 1
V¬µ
(¬µ ‚àí¬µ)2 +
1

v0 + va + ve
n
 (y ‚àí¬µ0)2
where
¬µ = ¬µ0 +
v0

v0 + va + ve
n
 (y ‚àí¬µ0) ,
V¬µ = v0

1 ‚àí
v0
v0 + va + ve
n

.

5.3 Bayes Theorem: Continuous Case
231
Thus
p (¬µ|y, ¬µ0, v0, va, ve) ‚àùexp

‚àí1
2V ‚àí1
¬µ
(¬µ ‚àí¬µ)2

√ó exp

‚àí1
2

v0 + va + ve
n
‚àí1
(y ‚àí¬µ0)2

.
The second term does not depend on ¬µ, so
p (¬µ|y, ¬µ0, v0, va, ve) ‚àùexp

‚àí1
2V ‚àí1
¬µ
(¬µ ‚àí¬µ)2

.
(5.22)
Thus the marginal posterior distribution is ¬µ ‚àºN (¬µ, V¬µ) . It is seen, again,
that this posterior distribution depends on the data through y.
Marginal posterior density of a. Note that
p (¬µ|a, y, ¬µ0, v0, va, ve) = p (¬µ, a|y, ¬µ0, v0, va, ve)
p (a|y, ¬µ0, v0, va, ve) ,
so
p (a|y, ¬µ0, v0, va, ve) = p (¬µ, a|y, ¬µ0, v0, va, ve)
p (¬µ|a, y, ¬µ0, v0, va, ve)
(5.23)
with the densities in the numerator and denominator given in (5.15) and
(5.21), respectively. Alternatively, an instructive representation can be ob-
tained by rewriting the joint density in (5.15). Employing (5.20), this can
be put as
p (¬µ, a|y, ¬µ0, v0, va, ve)
‚àùexp

‚àí1
2
#
n
ve . 1
v0
n
ve + 1
v0
(y ‚àí¬µ0 ‚àía)2 + a2
va
$
√ó exp
Ô£Æ
Ô£∞‚àí1
2

 n
ve
+ 1
v0
 #
¬µ ‚àí

 n
ve
+ 1
v0
‚àí1  n
ve
(y ‚àía) + ¬µ0
v0
$2Ô£π
Ô£ª.
Integrating over ¬µ, to obtain the marginal posterior density of a, yields
p (a|y, ¬µ0, v0, va, ve)
‚àùexp

‚àí1
2
#
n
ve . 1
v0
n
ve + 1
v0
(y ‚àí¬µ0 ‚àía)2 + a2
va
$
√ó

exp
Ô£Æ
Ô£∞‚àí1
2

 n
ve
+ 1
v0
 #
¬µ ‚àí

 n
ve
+ 1
v0
‚àí1  n
ve
(y ‚àía) + ¬µ0
v0
$2
d¬µ
Ô£π
Ô£ª.
(5.24)

232
5. An Introduction to Bayesian Inference
The integral involves a normal kernel and evaluates to
C
2œÄ

 n
ve
+ 1
v0
‚àí1
which, by not being a function of a, gets absorbed in the integration con-
stant. Further, using (5.16) and (5.17),
n
ve . 1
v0
n
ve + 1
v0
(y ‚àí¬µ0 ‚àía)2 + a2
va
=
n
nv0 + ve
[a ‚àí(y ‚àí¬µ0)]2 + a2
va
= 1
Va
(a ‚àía)2 +
1

v0 + va + ve
n
 (y ‚àí¬µ0)2 ,
(5.25)
where
a =
va
v0 + va + ve
n
(y ‚àí¬µ0) ,
Va = va

1 ‚àí
va
v0 + va + ve
n

.
Employing (5.25) in (5.24), and retaining only the term in a,
p (a|y, ¬µ0, v0, va, ve) ‚àùexp

‚àí1
2V ‚àí1
a
(a ‚àía)2

.
(5.26)
In conclusion, the marginal posterior density of the additive genetic eÔ¨Äect is
normal with mean a and variance Va; it depends on the data only through
y.
Marginal distribution of the data. Finding the marginal density of the
observations, i.e., the denominator of Bayes theorem, is also of interest. As
we shall see later, the corresponding distribution plays an important role
in model assessment. First, observe that the joint density of y, ¬µ, and a,
given the hyperparameters, is
p(y, ¬µ, a|¬µ0, v0, va, ve) =
n
-
i=1
p(yi|¬µ, a, ve)p (a|va) p (¬µ|¬µ0, v0)
(5.27)
with all kernels in a normal form. Using results developed previously, the
forms inside of the exponents can be combined as
n
i=1
(yi ‚àí¬µ ‚àía)2
ve
+ a2
va
+ (¬µ ‚àí¬µ0)2
v0

5.3 Bayes Theorem: Continuous Case
233
=
n [a ‚àí(y ‚àí¬µ)]2 +
n
i=1
(yi ‚àíy)2
ve
+ a2
va
+ (¬µ ‚àí¬µ0)2
v0
=
n
i=1
(yi ‚àíy)2
ve
+ (¬µ ‚àí¬µ0)2
v0
+ n [a ‚àí(y ‚àí¬µ)]2
ve
+ a2
va
=
n
i=1
(yi ‚àíy)2
ve
+ (¬µ ‚àí¬µ0)2
v0
+

 n
ve
+ 1
va
 #
a ‚àí

 n
ve
+ 1
va
‚àí1  n
ve
(y ‚àí¬µ)
$2
+
n
ve
1
va
n
ve +
1
va
(y ‚àí¬µ)2.
Using the preceding, the marginal density of the observations is
p(y|¬µ0, v0, va, ve)
=
 
n
-
i=1
p(yi|¬µ, a, ve)p (a|va) p (¬µ|¬µ0, v0) da d¬µ
‚àùexp
Ô£Æ
Ô£ØÔ£ØÔ£∞‚àí
n
i=1
(yi ‚àíy)2
2ve
Ô£π
Ô£∫Ô£∫Ô£ª

exp

‚àín(y ‚àí¬µ)2
2 (nva + ve)

exp

‚àí(¬µ ‚àí¬µ0)2
2v0

d¬µ
√ó

exp
Ô£Æ
Ô£∞‚àí1
2

 n
ve
+ 1
va
 #
a ‚àí

 n
ve
+ 1
va
‚àí1  n
ve
(y ‚àí¬µ)
$2Ô£π
Ô£ªda.
(5.28)
The last integral evaluates to
C
2œÄ

 n
ve
+ 1
va
‚àí1
and since it does not involve y, it gets absorbed in the integration constant
of (5.28). Thus
p(y|¬µ0, v0, va, ve) ‚àùexp
Ô£Æ
Ô£ØÔ£ØÔ£∞‚àí
n
i=1
(yi ‚àíy)2
2ve
Ô£π
Ô£∫Ô£∫Ô£ª
√ó

exp

‚àín(y ‚àí¬µ)2
2 (nva + ve)

exp

‚àí(¬µ ‚àí¬µ0)2
2v0

d¬µ.
(5.29)

234
5. An Introduction to Bayesian Inference
It was seen before that
n(¬µ ‚àíy)2
(nva + ve) + (¬µ ‚àí¬µ0)2
v0
= n(y ‚àí¬µ)2
(nva + ve) + (¬µ ‚àí¬µ0)2
v0
= (¬µ ‚àí¬µ)2
V¬µ
+
(y ‚àí¬µ0)2

v0 + va + ve
n
.
Making use of this in (5.29), one gets, after integrating over ¬µ,
p(y|¬µ0, v0, va, ve) ‚àùexp
Ô£±
Ô£¥
Ô£¥
Ô£≤
Ô£¥
Ô£¥
Ô£≥
‚àí1
2
Ô£Æ
Ô£ØÔ£ØÔ£∞
n
i=1
(yi ‚àíy)2
ve
+
(y ‚àí¬µ0)2

v0 + va + ve
n

Ô£π
Ô£∫Ô£∫Ô£ª
Ô£º
Ô£¥
Ô£¥
Ô£Ω
Ô£¥
Ô£¥
Ô£æ
.
(5.30)
This is the density of an n-dimensional distribution, that depends on the
data through y, and through the sum of squared deviations of the observa-
tions from the mean. It will be shown that this is the kernel of the density
of the n-variate normal distribution
y|¬µ0, v0, va, ve ‚àºN(1¬µ0, V)
(5.31)
where 1 denotes a vector of 1‚Ä≤s of order n, and
V = (v0 + va) J + veI
is the variance‚Äìcovariance matrix of the process, where J is an n√ón matrix
of 1‚Ä≤s. Now, the inverse of V is given (Searle et al., 1992) by
V‚àí1 = 1
ve

I ‚àí
v0 + va
ve + n (v0 + va)J

.
Hence, the kernel of the density of the multivariate normal distribution
[y|¬µ0, v0, va, ve] can be put as
p (y|¬µ0, v0, va, ve) ‚àùexp

‚àí
1
2 (y ‚àí1¬µ0)‚Ä≤ 1
ve

I‚àí
v0 + va
ve + n (v0 + va)J
%
√ó (y ‚àí1¬µ0)] .
Now,
(y ‚àí1¬µ0)‚Ä≤ 1
ve

I ‚àí
v0 + va
ve + n (v0 + va)J

(y ‚àí1¬µ0)
= [y ‚àí1y ‚àí1 (¬µ0 ‚àíy)]‚Ä≤ 1
ve

I ‚àí
v0 + va
ve + n (v0 + va)J

√ó [y ‚àí1y ‚àí1 (¬µ0 ‚àíy)]

5.4 Posterior Distributions
235
= 1
ve
 n

i=1
(yi ‚àíy)2 + n (y ‚àí¬µ0)2 ‚àí(v0 + va) n2 (y ‚àí¬µ0)2
ve + n (v0 + va)

=
n
i=1
(yi ‚àíy)2
ve
+ n
ve
(y ‚àí¬µ0)2

1 ‚àí
n (v0 + va)
ve + n (v0 + va)

=
n
i=1
(yi ‚àíy)2
ve
+
(y ‚àí¬µ0)2
(v0 + va) + ve
n
.
Thus
p (y|¬µ0, v0, va, ve) ‚àùexp
Ô£±
Ô£¥
Ô£¥
Ô£≤
Ô£¥
Ô£¥
Ô£≥
‚àí1
2
Ô£Æ
Ô£ØÔ£ØÔ£∞
n
i=1
(yi ‚àíy)2
ve
+
(y ‚àí¬µ0)2

v0 + va + ve
n

Ô£π
Ô£∫Ô£∫Ô£ª
Ô£º
Ô£¥
Ô£¥
Ô£Ω
Ô£¥
Ô£¥
Ô£æ
and this is precisely (5.30). Hence, the marginal distribution of the ob-
servations is the normal process given in (5.31). This distribution is often
referred to as the prior predictive distribution of the observation, that is,
the stochastic process describing the probabilities with which data occur,
before any observation is made. Its density depends entirely on the pa-
rameters of the prior distributions, commonly called ‚Äúhyperparameters‚Äù.
This result could have been anticipated by viewing (5.13) as a random ef-
fects model, where the two independently distributed random eÔ¨Äects have
distributions ¬µ ‚àºN (¬µ0, v0) and a ‚àºN (0, va) . Since, in vector notation,
y = 1 (¬µ + a) + e
is a linear combination of normal variates, where e ‚àºN (0, Ive) , it follows
immediately that y (given ¬µ0) must be normal, with mean vector
E (y |¬µ0 ) = 1¬µ0
and variance‚Äìcovariance matrix
E (y |v0, va, ve ) = 11‚Ä≤ (v0 + va) + Ive
= J (v0 + va) + Ive.
‚ñ†
5.4
Posterior Distributions
Consider Bayes theorem in any of the forms given in (5.10) to (5.12), and
partition the vector of all quantities subject to uncertainty as Œ∏ =

Œ∏‚Ä≤
1, Œ∏‚Ä≤
2
‚Ä≤ ,

236
5. An Introduction to Bayesian Inference
where Œ∏1 and Œ∏2 represent distinct, unknown features of the probability
model. For example, in a linear model, Œ∏1 may be the location vector and
Œ∏2 the dispersion components, i.e., the variance and covariance parameters.
The joint posterior density of all unknowns is
p (Œ∏1, Œ∏2|y)
=
L (Œ∏1, Œ∏2|y) g (Œ∏1, Œ∏2)
 
L (Œ∏1, Œ∏2|y) g (Œ∏1, Œ∏2) dŒ∏1 dŒ∏2
‚àù
L (Œ∏1, Œ∏2|y) g (Œ∏1, Œ∏2) ,
(5.32)
where L (Œ∏1, Œ∏2|y) is the likelihood function (joint density or distribution
of the observations, viewed as a function of the unknowns), and g (Œ∏1, Œ∏2)
is the joint prior density. The latter is typically a multivariate density
function, and elicitation may be facilitated by writing
g (Œ∏1, Œ∏2) = g (Œ∏1|Œ∏2) g (Œ∏2) = g (Œ∏2|Œ∏1) g (Œ∏1) .
Here g (Œ∏i) is the marginal prior density of Œ∏i, and g (Œ∏j|Œ∏i) for i Ã∏= j is
the conditional prior density of Œ∏j given Œ∏i. Hence, one can assign prior
probabilities to Œ∏1, and then to Œ∏2 at each of the values of Œ∏1, or to Œ∏2,
and then to Œ∏1, given Œ∏2. Irrespective of the form and order of elicitation,
one must end up with the same joint process; otherwise, there would be in-
coherence in the probabilistic ensemble. Often, it happens (sometimes for
mathematical convenience) that the two sets of parameters are assigned
independent prior distributions. However, it is uncommon that parameters
remain mutually independent, a posteriori. For this to occur, the likelihood
must factorize into independent pieces, as well, with each of the portions
corresponding to each of the sets of parameters. It will be seen later that pa-
rameters that are independent a priori can become dependent a posteriori,
even when a single data point is observed.
The marginal posterior densities of each parameter (or set of parameters)
are, by deÔ¨Ånition
p (Œ∏1|y) =

p (Œ∏1, Œ∏2|y) dŒ∏2
(5.33)
and
p (Œ∏2|y) =

p (Œ∏1, Œ∏2|y) dŒ∏1.
(5.34)
It may be necessary to carry out the marginalization to further levels in a
Bayesian analysis. For example, suppose that Œ∏1 =

Œ∏‚Ä≤
1A, Œ∏‚Ä≤
1B
‚Ä≤ where Œ∏1A
is a vector of additive genetic eÔ¨Äects, say, and Œ∏1B includes some other
location parameters. Then, if one wishes to assign posterior probabilities
(inference) to additive genetic eÔ¨Äects, the marginal posterior density to be
used is
p (Œ∏1A|y)
=
 
p (Œ∏1, Œ∏2|y) dŒ∏1B dŒ∏2
=

p (Œ∏1|y) dŒ∏1B.
(5.35)

5.4 Posterior Distributions
237
In any of the situations described above, the parameters that are inte-
grated out of the joint posterior density are referred to as nuisance pa-
rameters. Technically, these are components of the statistical model that
need to be considered, because the probabilistic structure adopted requires
it, but that are not of primary inferential interest. Suppose that Œ∏1 is of
primary interest, in which case the distribution to be used for inference is
the process [Œ∏1|y] . The corresponding marginal density can be rewritten
as
p (Œ∏1|y)
=

p (Œ∏1, Œ∏2|y) dŒ∏2
=

p (Œ∏1|Œ∏2, y) p (Œ∏2|y) dŒ∏2
(5.36)
=
EŒ∏2|y [p (Œ∏1|Œ∏2, y)] ,
(5.37)
where p(Œ∏1|Œ∏2, y) is the density of the conditional posterior distribution of
Œ∏1 given Œ∏2. Representations (5.36) and (5.37) indicate that the marginal
density of the primary parameter Œ∏1 is the weighted average of an inÔ¨Å-
nite number of conditional densities p(Œ∏1|Œ∏2, y), where the weighting or
mixing function is the marginal posterior density of the nuisance parame-
ter, p(Œ∏2|y). The conditional posterior distribution [Œ∏1|Œ∏2, y] describes the
uncertainty of inferences about Œ∏1 that can be drawn when the nuisance
parameter Œ∏2 is known, whereas the marginal distribution [Œ∏2|y] gives the
relative plausibility of diÔ¨Äerent values of the nuisance parameter, in the light
of any prior information and of other assumptions built into the model, and
of the evidence provided by the data (Box and Tiao, 1973).
The conditional posterior distributions can be identiÔ¨Åed (at least con-
ceptually) from the joint posterior distribution, with the latter following
directly from the assumptions, once a prior and a data generating process
are postulated. Note that the conditional posterior density of a parameter
can be expressed as
p (Œ∏1|Œ∏2, y) = p (Œ∏1, Œ∏2|y)
p (Œ∏2|y)
.
Since, in this distribution, one is interested in variation with respect to Œ∏1
only, the denominator enters merely as part of the integration constant.
Thus, one can write
p (Œ∏1|Œ∏2, y) ‚àùp (Œ∏1, Œ∏2|y)
‚àùL (Œ∏1, Œ∏2|y) p (Œ∏1, Œ∏2)
‚àùL (Œ∏1, Œ∏2|y) p (Œ∏1|Œ∏2)
‚àùL (Œ∏1|Œ∏2, y) p (Œ∏1|Œ∏2) .
(5.38)
Above, L (Œ∏1|Œ∏2, y) is the likelihood function with Œ∏2 treated as a known
constant, rather than as a feature subject to uncertainty. The preceding de-
velopment implies that a conditional posterior distribution can (often) be

238
5. An Introduction to Bayesian Inference
identiÔ¨Åed by inspection of the joint posterior density and by retaining only
the parts that vary with the parameter(s) of interest, treating the remain-
ing parts as known. This method can be useful for identifying conditional
posterior distributions in the context of MCMC methods, as discussed in
a subsequent chapter.
Example 5.5
Posterior dependence between parameters
Suppose that a sample of size n is obtained by drawing independently
from the same normal distribution N

¬µ, œÉ2
, where the mean and variance
are both unknown. Assume that the parameters are taken as following
independent distributions, with prior densities,
p (¬µ|a, b) =
1
b ‚àía,
p

œÉ2|c, d

=
1
d ‚àíc,
where a, b, c, d are bounds on parameter values that have been elicited some-
how. The joint posterior density is
p

¬µ, œÉ2|y, a, b, c, d

‚àù
n
-
i=1

œÉ2‚àí1
2 exp

‚àí(yi ‚àí¬µ)2
2œÉ2

1
(b ‚àía) (d ‚àíc)
‚àù

œÉ2‚àín
2 exp

‚àí1
2œÉ2
n

i=1
(yi ‚àí¬µ)2

‚àù

œÉ2‚àín
2 exp
Ô£Æ
Ô£ØÔ£ØÔ£∞‚àí
n
i=1
(yi ‚àíy)2 + n (¬µ ‚àíy)2
2œÉ2
Ô£π
Ô£∫Ô£∫Ô£ª
(5.39)
this being nonnull for a < ¬µ < b and c < œÉ2 < d. The marginal posterior
density of œÉ2 is obtained by integrating over ¬µ:
p

œÉ2|y, a, b, c, d

‚àù

œÉ2‚àín
2 exp

‚àí1
2œÉ2
n

i=1
(yi ‚àíy)2

√ó
b

a
‚àö
2œÄœÉ2/n
‚àö
2œÄœÉ2/n exp

‚àín(¬µ‚àíy)2
2œÉ2

d¬µ
‚àù

œÉ2‚àín‚àí1
2
exp

‚àí1
2œÉ2
n

i=1
(yi ‚àíy)2
 
Œ¶

b‚àíy
œÉ/‚àön

‚àíŒ¶

a‚àíy
œÉ/‚àön

.
(5.40)
The diÔ¨Äerence between the integrals is equal to 1 if ¬µ is allowed to take
values anywhere in the real line (i.e., if a = ‚àí‚àûand b = ‚àû). The marginal

5.4 Posterior Distributions
239
posterior density of ¬µ is found by integrating (5.39) with respect to œÉ2:
p (¬µ|y, a, b, c, d) ‚àù
d

c

œÉ2‚àín
2 exp

‚àí1
2œÉ2
n

i=1
(yi ‚àí¬µ)2

dœÉ2
‚àù
d

c

œÉ2‚àí( n‚àí2
2
+1) exp

‚àíS¬µ
œÉ2

dœÉ2,
where
S¬µ = 1
2
n

i=1
(yi ‚àí¬µ)2 .
The integral above cannot be written in closed form. However, if one takes
the positive part of the real line as parameter space for œÉ2, that is, c = 0
and d = ‚àû, use of properties of the inverse gamma (or scaled inverse chi-
square) distribution seen in Chapter 1 yields
p (¬µ|y, a, b)
‚àù
‚àû

0

œÉ2‚àí( n‚àí2
2
+1) exp

‚àíS¬µ
œÉ2

dœÉ2 = Œì
 n‚àí2
2

S
n‚àí2
2
¬µ
‚àù
S
‚àín‚àí2
2
¬µ
‚àù
 n

i=1
(yi ‚àíy)2 + n (¬µ ‚àíy)2
‚àín‚àí2
2
.
In this density, only variation with respect to ¬µ is of concern. Hence, one
can factor out the sum of squared deviations of the observations from the
sample mean, to obtain
p (¬µ|y, a, b)
‚àù
Ô£Æ
Ô£ØÔ£ØÔ£∞1 +
n (¬µ ‚àíy)2
n
i=1
(yi ‚àíy)2
Ô£π
Ô£∫Ô£∫Ô£ª
‚àín‚àí3+1
2
‚àù

1 + (¬µ ‚àíy)2
(n ‚àí3) s2
n
‚àín‚àí3+1
2
,
(5.41)
where
5s2 =
1
n ‚àí3
n

i=1
(yi ‚àíy)2 .
If ¬µ were allowed to take values anywhere in the real line, (5.41) gives
the kernel of a t-distribution with n ‚àí3 degrees of freedom, mean y, and
variance equal to
5s2 (n ‚àí3)
n (n ‚àí5) =
n
i=1
(yi ‚àíy)2
n (n ‚àí5)
.

240
5. An Introduction to Bayesian Inference
This distribution is proper if n > 3, and the variance is Ô¨Ånite if n > 5.
However, since in the example ¬µ takes density only in the [a, b] interval, it
turns out that the marginal posterior distribution is a truncated t-process
with density
p (¬µ|y, a, b) =

1 +
(¬µ‚àíy)2
(n‚àí3) s2
n
‚àín‚àí3+1
2
b

a

1 + (¬µ ‚àíy)2
(n ‚àí3) s2
n
‚àín‚àí3+1
2
d¬µ
.
(5.42)
Finally, note that the product of (5.42) and (5.40) does not yield (5.39),
even if a = ‚àí‚àû, b = ‚àû, c = 0, and d = ‚àû. Hence ¬µ and œÉ2 are not
independent in the posterior distribution, even if they are so, a priori.
‚ñ†
Example 5.6
Conditional posterior distribution
Revisit Example 5.5, and consider Ô¨Ånding the two induced conditional
posterior distributions. From representation (5.39) of the joint posterior
density, one can deduce the density of

¬µ|y, a, b, c, d, œÉ2
, treating œÉ2 as a
constant. This yields
p

¬µ|y, a, b, c, d, œÉ2
‚àùexp

‚àín (¬µ ‚àíy)2
2œÉ2

I (a < ¬µ < b)
which is the density of a normal distribution truncated between a and b. The
mean and variance of the untruncated distribution are y and œÉ2/n, respec-
tively. Similarly, the density of the conditional distribution

œÉ2|y, a, b, c, d, ¬µ

is arrived at by regarding ¬µ as a constant in the joint density, to obtain
p

œÉ2|y, a, b, c, d, ¬µ

‚àù

œÉ2‚àí( n‚àí2
2
+1) exp

‚àíS¬µ
œÉ2

I

c < œÉ2 < d

.
This is in an inverse gamma form (truncated between c and d), and the
parameters of the distribution in the absence of truncation are (n ‚àí2)/2
and S¬µ.
‚ñ†
Example 5.7
Posterior distributions in a linear regression model with t
distributed errors
Suppose a response y is related to a predictor variable x according to the
linear relationship
yi = Œ≤0 + Œ≤1xi + œµi,
i = 1, 2, ..., n,
(5.43)
where the residuals are i.i.d. as
t

0, œÉ2, ŒΩ

,

5.4 Posterior Distributions
241
where œÉ2 is the scale parameter of the t-distribution and ŒΩ are the degrees
of freedom, with the latter assumed known. Recall from Chapter 1 that an
equivalent representation of (5.43) is given by
yi = Œ≤0 + Œ≤1xi +
ei
‚àöwi
,
(5.44)
where ei ‚àºN

0, œÉ2
and wi ‚àºGa
 ŒΩ
2, ŒΩ
2

are independently distributed
random variables, i = 1, 2, ..., n. If model (5.44) is deconditioned with re-
spect to the gamma random variable, then (5.43) results. Let the parameter
vector include the unknown parameters of the model, that is, the regression
coeÔ¨Écients and œÉ2, plus all the gamma weights wi. This is known as ‚Äúdata
augmentation‚Äù (Tanner and Wong, 1987) and is discussed in Chapter 11.
The augmented parameter vector is
Œ∏ =

Œ≤0, Œ≤1, œÉ2, w1, w2, ..., wn
‚Ä≤
and adopt as joint prior density
p

Œ≤0, Œ≤1, œÉ2, w1, w2, ..., wn|ŒΩ

= p (Œ≤0) p (Œ≤1) p

œÉ2
n
-
i=1
p (wi|ŒΩ) .
(5.45)
The joint posterior density is
p

Œ≤0, Œ≤1, œÉ2, w1, w2, ..., wn|y, ŒΩ

‚àù
n
-
i=1

p

yi|Œ≤0, Œ≤1, œÉ2, wi

p (wi|ŒΩ)

√óp (Œ≤0) p (Œ≤1) p

œÉ2
.
(5.46)
Explicitly, this takes the form
p

Œ≤0, Œ≤1, œÉ2, w1, w2, ..., wn|y, ŒΩ

‚àù
n
-
i=1

œÉ2
wi
‚àí1
2
w
ŒΩ
2 ‚àí1
i
exp
#
‚àíwi
2

(yi ‚àíŒ≤0 ‚àíŒ≤1xi)2 + ŒΩœÉ2
œÉ2
$
√óp (Œ≤0) p (Œ≤1) p

œÉ2
.
(5.47)
Conditional posterior distribution of wi given all other parameters. Note
in (5.47) that, given all other parameters, the w‚Ä≤s are mutually independent.
The kernel of the conditional posterior density of wi is found by collecting
terms that depend on this random variable. Thus, for i = 1, 2, ..., n,
p

wi|Œ≤0, Œ≤1, œÉ2, y, ŒΩ

‚àùw
ŒΩ+1
2
‚àí1
i
exp

‚àíwiSi
2

,
where
Si = (yi ‚àíŒ≤0 ‚àíŒ≤1xi)2 + ŒΩœÉ2
œÉ2
.

242
5. An Introduction to Bayesian Inference
This implies that the conditional posterior distribution of wi is the gamma
process
wi|Œ≤0, Œ≤1, œÉ2, y, ŒΩ ‚àºGa

ŒΩ + 1
2
, ŒΩ + (yi ‚àíŒ≤0 ‚àíŒ≤1xi)2 /œÉ2
2

.
(5.48)
The observations enter only through data point i.
Conditional posterior distribution of Œ≤0 and Œ≤1 given all other parame-
ters. From the joint density (5.47):
p

Œ≤0, Œ≤1|œÉ2, w, y, ŒΩ

‚àù
n
-
i=1
exp

‚àíwi
2œÉ2 (yi ‚àíŒ≤0 ‚àíŒ≤1xi)2
p (Œ≤0) p (Œ≤1) .
(5.49)
It is not possible to be more speciÔ¨Åc about the form of the distribution
unless the priors are stated explicitly. For example, suppose that, a priori,
Œ≤0 ‚àºN

Œ±0, œÉ2
Œ≤0

and Œ≤1 ‚àºN

Œ±1, œÉ2
Œ≤1

. Then,
p

Œ≤0, Œ≤1|œÉ2, w, y, ŒΩ, Œ±0, œÉ2
Œ≤0, Œ±1, œÉ2
Œ≤1

‚àùexp
#
‚àí1
2œÉ2
 n

i=1
wi (yi ‚àíŒ≤0 ‚àíŒ≤1xi)2
+ œÉ2
œÉ2
Œ≤0
(Œ≤0 ‚àíŒ±0)2 + œÉ2
œÉ2
Œ≤1
(Œ≤1 ‚àíŒ±1)2
$
.
(5.50)
Now
n

i=1
wi (yi ‚àíŒ≤0 ‚àíŒ≤1xi)2 = (y ‚àíXŒ≤)‚Ä≤ W (y ‚àíXŒ≤) ,
(5.51)
where
Œ≤ =

Œ≤0
Œ≤1

and
W
=
Diag (wi) ,
X =
 1
x 
n√ó2 ,
1
=
{1} ,
x = [x1, x2, ..., xn]‚Ä≤ .
DeÔ¨Åne the following function of the data (and of the w‚Ä≤s):
5Œ≤ =

5Œ≤0
5Œ≤1

= (X‚Ä≤WX)‚àí1 X‚Ä≤Wy.
(5.52)

5.4 Posterior Distributions
243
Then
(y ‚àíXŒ≤)‚Ä≤ W (y ‚àíXŒ≤)
=

y ‚àíX5Œ≤ ‚àíXŒ≤ + X5Œ≤
‚Ä≤
W

y ‚àíX5Œ≤ ‚àíXŒ≤ + X5Œ≤

=

y ‚àíX5Œ≤ ‚àíX

Œ≤‚àí5Œ≤
‚Ä≤
W

y ‚àíX5Œ≤ ‚àíX

Œ≤ ‚àí5Œ≤

=

y ‚àíX5Œ≤
‚Ä≤
W

y ‚àíX5Œ≤

+

Œ≤ ‚àí5Œ≤
‚Ä≤
X‚Ä≤WX

Œ≤ ‚àí5Œ≤

(5.53)
because the cross-product term vanishes, as a consequence of the deÔ¨Ånition
of 5Œ≤:
2

Œ≤ ‚àí5Œ≤

X‚Ä≤W

y ‚àíX5Œ≤

= 2

Œ≤ ‚àí5Œ≤
 
X‚Ä≤Wy ‚àíX‚Ä≤WX5Œ≤

= 2

Œ≤ ‚àí5Œ≤

(X‚Ä≤Wy ‚àíX‚Ä≤Wy) = 0.
Employing (5.53) in (5.51), the posterior density in (5.50) becomes
p

Œ≤0, Œ≤1|œÉ2, w, y, ŒΩ, Œ±0, œÉ2
Œ≤0, Œ±1, œÉ2
Œ≤1

‚àùexp

‚àí1
2œÉ2

y ‚àíX5Œ≤
‚Ä≤
W

y ‚àíX5Œ≤

+

Œ≤ ‚àí5Œ≤
‚Ä≤
X‚Ä≤WX

Œ≤ ‚àí5Œ≤

+ œÉ2 (Œ≤0 ‚àíŒ±0)2
œÉ2
Œ≤0
+ œÉ2 (Œ≤1 ‚àíŒ±1)2
œÉ2
Œ≤1
$
‚àùexp

‚àí1
2œÉ2

Œ≤ ‚àí5Œ≤
‚Ä≤
X‚Ä≤WX

Œ≤ ‚àí5Œ≤

+ œÉ2 (Œ≤0 ‚àíŒ±0)2
œÉ2
Œ≤0
+ œÉ2 (Œ≤1 ‚àíŒ±1)2
œÉ2
Œ≤1
$
,
(5.54)
upon retaining only the terms that vary with Œ≤. DeÔ¨Åning
Œ± =
 Œ±0
Œ±1

,
Œõ =
Ô£Æ
Ô£∞
œÉ2
œÉ2
Œ≤0
0
0
œÉ2
œÉ2
Œ≤1
Ô£π
Ô£ª=

Œª0
0
0
Œª1

the conditional posterior that concerns us is expressible as
p

Œ≤0, Œ≤1|œÉ2, w, y, ŒΩ, Œ±0, œÉ2
Œ≤0, Œ±1, œÉ2
Œ≤1

‚àù
exp

‚àí1
2œÉ2

Œ≤ ‚àí5Œ≤
‚Ä≤
X‚Ä≤WX

Œ≤ ‚àí5Œ≤

+ (Œ≤ ‚àíŒ±)‚Ä≤ Œõ (Œ≤ ‚àíŒ±)

.
(5.55)

244
5. An Introduction to Bayesian Inference
The two quadratic forms on Œ≤ can be combined using an extension of
formulas (5.16) and (5.17), as given in Box and Tiao (1973),
(z ‚àím)‚Ä≤M(z ‚àím) + (z ‚àíb)‚Ä≤B(z ‚àíb)
=
(z ‚àíc)‚Ä≤(M + B)(z ‚àíc)
+ (m ‚àíb)‚Ä≤ M (M + B)‚àí1 B (m ‚àíb)
(5.56)
with
c = (M + B)‚àí1 (Mm + Bb) .
(5.57)
Employing this in the context of (5.55):

Œ≤ ‚àí5Œ≤
‚Ä≤
X‚Ä≤WX

Œ≤ ‚àí5Œ≤

+ (Œ≤ ‚àíŒ±)‚Ä≤ Œõ (Œ≤ ‚àíŒ±)
=

Œ≤ ‚àíŒ≤
‚Ä≤ (X‚Ä≤WX + Œõ)

Œ≤ ‚àíŒ≤

+(5Œ≤‚àíŒ±)‚Ä≤X‚Ä≤WX (X‚Ä≤WX + Œõ)‚àí1 Œõ(5Œ≤‚àíŒ±)
(5.58)
with
Œ≤
=
(X‚Ä≤WX + Œõ)‚àí1 
X‚Ä≤WX5Œ≤ + ŒõŒ±

(5.59)
=
(X‚Ä≤WX + Œõ)‚àí1 (X‚Ä≤Wy + ŒõŒ±) .
(5.60)
Using (5.58) in (5.55) and retaining only the part that varies with Œ≤:
p

Œ≤0, Œ≤1|œÉ2, w, y, ŒΩ, Œ±0, œÉ2
Œ≤0, Œ±1, œÉ2
Œ≤1

‚àùexp

‚àí1
2œÉ2

Œ≤ ‚àíŒ≤
‚Ä≤ (X‚Ä≤WX + Œõ)

Œ≤ ‚àíŒ≤

.
(5.61)
Thus, the posterior distribution of the regression coeÔ¨Écients, given all other
parameters, is normal, with mean vector as in (5.59) or (5.60) and variance‚Äì
covariance matrix
V ar

Œ≤0, Œ≤1|œÉ2, w, y, ŒΩ, Œ±0, œÉ2
Œ≤0, Œ±1, œÉ2
Œ≤1

= (X‚Ä≤WX + Œõ)‚àí1 œÉ2.
(5.62)
Note that the mean of this posterior distribution is a matrix-weighted av-
erage of 5Œ≤ and Œ±, where the weights are X‚Ä≤WX and Œõ, respectively.
Conditional posterior distribution of Œ≤1 given Œ≤0 and all other param-
eters. If Œ≤0 is known, it can be treated as an oÔ¨Äset in the model, that is,
one can write a ‚Äúnew‚Äù response variable
ri = yi ‚àíŒ≤0 = Œ≤1xi + œµi.
Put r = {ri} . Using this in the joint posterior density, and treating Œ≤0 as
a constant, yields
p

Œ≤1|Œ≤0, œÉ2, w, y, ŒΩ, Œ±0, œÉ2
Œ≤0, Œ±1, œÉ2
Œ≤1

‚àù
exp
#
‚àí1
2œÉ2
 n

i=1
wi (ri ‚àíŒ≤1xi)2 + Œª1 (Œ≤1 ‚àíŒ±1)2
$
.

5.4 Posterior Distributions
245
Using similar algebra as in (5.51)to (5.60), the conditional density can be
represented in the Gaussian form
p

Œ≤1|Œ≤0, œÉ2, w, y, ŒΩ, Œ±0, œÉ2
Œ≤0, Œ±1, œÉ2
Œ≤1

‚àùexp
#
‚àí1
2œÉ2
 n

i=1
wi (ri ‚àíŒ≤1xi)2 + Œª1 (Œ≤1 ‚àíŒ±1)2
$
‚àùexp

‚àí1
2œÉ2

Œ≤1 ‚àíŒ≤1.0
‚Ä≤ (x‚Ä≤Wx+Œª1)

Œ≤1 ‚àíŒ≤1.0

,
(5.63)
where
Œ≤1.0
=
(x‚Ä≤Wx + Œª1)‚àí1 (x‚Ä≤Wr + Œª1Œ±1)
=
n
i=1
wixi (yi ‚àíŒ≤0) + Œª1Œ±1
n
i=1
wix2
i + Œª1
and the variance of the process is
V ar

Œ≤1|Œ≤0, œÉ2, w, y, ŒΩ, Œ±0, œÉ2
Œ≤0, Œ±1, œÉ2
Œ≤1

= (x‚Ä≤Wx + Œª1)‚àí1 œÉ2
=
œÉ2
n
i=1
wix2
i + Œª1
.
Conditional posterior distribution of Œ≤0 given Œ≤1 and all other param-
eters. The development is similar, except that the oÔ¨Äset is now Œ≤1xi, to
form the ‚Äúnew‚Äù response
ti = yi ‚àíŒ≤1xi = Œ≤0 + œµi
with t = {yi ‚àíŒ≤1xi} . The resulting density is
p

Œ≤0|Œ≤1, œÉ2, w, y, ŒΩ, Œ±0, œÉ2
Œ≤0, Œ±1, œÉ2
Œ≤1

‚àùexp

‚àí1
2œÉ2

Œ≤0 ‚àíŒ≤0.1
‚Ä≤ (1‚Ä≤W1+Œª0)

Œ≤0 ‚àíŒ≤0.1

,
(5.64)
where
Œ≤0.1
=
(1‚Ä≤W1 + Œª0)‚àí1 (1‚Ä≤Wt + Œª0Œ±0)
=
n
i=1
wi (yi ‚àíŒ≤1xi) + Œª0Œ±0
n
i=1
wi + Œª0

246
5. An Introduction to Bayesian Inference
and
V ar

Œ≤0|Œ≤1, œÉ2, w, y, ŒΩ, Œ±0, œÉ2
Œ≤0, Œ±1, œÉ2
Œ≤1

=
œÉ2
n
i=1
wi + Œª0
.
Conditional posterior distribution of œÉ2 given all other parameters. Re-
taining terms in œÉ2 in the joint density of all parameters given in (5.47),
one obtains
p

œÉ2|Œ≤0, Œ≤1, w1, w2, ..., wn, y, ŒΩ

‚àù

œÉ2‚àín
2 exp

‚àí1
2œÉ2
n

i=1
wi (yi ‚àíŒ≤0 ‚àíŒ≤1xi)2

p

œÉ2
.
(5.65)
It is not possible to go further unless an explicit statement is made about
the prior distribution of œÉ2. For example, suppose that the prior distribu-
tion is lognormal, that is, the logarithm of œÉ2 follows a Gaussian distribu-
tion with mean 0 and variance œâ. Then, as seen in Chapter 2, the prior
density of œÉ2 takes the form
p

œÉ2|œâ

‚àù

œÉ2‚àí1 exp

‚àí

log œÉ22
2œâ

.
The conditional posterior density of œÉ2 would be
p

œÉ2|Œ≤0, Œ≤1, w, y, ŒΩ

‚àù

œÉ2‚àín+2
2
exp
#
‚àí1
2œÉ2
 n

i=1
wi (yi ‚àíŒ≤0 ‚àíŒ≤1xi)2 + œÉ2 
log œÉ22
œâ
$
,
which is not in a recognizable form. In this situation, further analytical
treatment is not feasible. On the other hand, suppose that elicitation yields
a scaled inverse chi-square distribution with parameters Q and R as a
reasonable prior. The corresponding density (see Chapter 1) is
p

œÉ2|Q, R

‚àù

œÉ2‚àí( Q
2 +1) exp

‚àíQR
2œÉ2

.
(5.66)
Upon using (5.66) in (5.65), the conditional posterior density of œÉ2 turns
out to be
p

œÉ2|Œ≤0, Œ≤1, w1, w2, ..., wn, y, ŒΩ

‚àù

œÉ2‚àí( n+Q
2
+1) exp

‚àíQ‚àóR‚àó
2œÉ2
%
. (5.67)
This is the density of a scaled inverse chi-square process with parameters
Q‚àó= n + Q and
R‚àó= ns2 + QR
n + Q
,

5.4 Posterior Distributions
247
with
s2 =
n
i=1
wi (yi ‚àíŒ≤0 ‚àíŒ≤1xi)2
n
.
Note that R‚àóis a weighted average between R (‚Äúa prior value of the vari-
ance‚Äù) and s2 (‚Äúa variance provided by the data, recalling that the w‚Ä≤s
are treated as observed in this conditional distribution). This is another
example of the data-prior compromise that arises in Bayesian analysis.
Conditional posterior distribution of Œ≤0 and Œ≤1 given w. Suppose the
prior density of œÉ2 is scaled inverse chi-squared, as in (5.66). Then, using
this in (5.47) and integrating over œÉ2 while keeping w Ô¨Åxed, gives
p (Œ≤0, Œ≤1|w, y, Q, R, ŒΩ) ‚àùp (Œ≤0) p (Œ≤1)
‚àû

0

œÉ2‚àín
2
√ó
n
-
i=1
exp

‚àíwi
2
(yi ‚àíŒ≤0 ‚àíŒ≤1xi)2
œÉ2

√ó

œÉ2‚àí( Q
2 +1) exp

‚àíQR
2œÉ2

dœÉ2
‚àùp (Œ≤0) p (Œ≤1)
‚àû

0

œÉ2‚àí( n+Q
2
+1)
√ó exp
Ô£±
Ô£¥
Ô£¥
Ô£≤
Ô£¥
Ô£¥
Ô£≥
‚àí
n
i=1
wi (yi ‚àíŒ≤0 ‚àíŒ≤1xi)2 + QR
2œÉ2
Ô£º
Ô£¥
Ô£¥
Ô£Ω
Ô£¥
Ô£¥
Ô£æ
dœÉ2
‚àùp (Œ≤0) p (Œ≤1)
‚àû

0

œÉ2‚àí( n+Q
2
+1)
√ó exp

‚àí(y ‚àíXŒ≤)‚Ä≤ W (y ‚àíXŒ≤) + QR
2œÉ2
%
dœÉ2
(5.68)
after making use of (5.51). The integrand is in a scaled inverse gamma
form, so the expression becomes
p (Œ≤0, Œ≤1|w, y, Q, R, ŒΩ)
‚àùp (Œ≤0) p (Œ≤1)

(y ‚àíXŒ≤)‚Ä≤ W (y ‚àíXŒ≤) + QR
‚àí( n+Q
2 )
.
Recall the decomposition in (5.53), that is,
(y ‚àíXŒ≤)‚Ä≤ W (y ‚àíXŒ≤)
=

y ‚àíX5Œ≤
‚Ä≤
W

y ‚àíX5Œ≤

+

Œ≤ ‚àí5Œ≤
‚Ä≤
X‚Ä≤WX

Œ≤ ‚àí5Œ≤

.

248
5. An Introduction to Bayesian Inference
Using this in the preceding density, rearranging and keeping only terms in
Œ≤ gives
p (Œ≤0, Œ≤1|w, y, Q, R, ŒΩ)
‚àùp (Œ≤0) p (Œ≤1)
Ô£Æ
Ô£ØÔ£∞1 +

Œ≤ ‚àí5Œ≤
‚Ä≤
X‚Ä≤WX

Œ≤ ‚àí5Œ≤


y ‚àíX5Œ≤
‚Ä≤
W

y ‚àíX5Œ≤

+ QR
Ô£π
Ô£∫Ô£ª
‚àí( n‚àí2+Q+2
2
)
‚àùp (Œ≤0) p (Œ≤1)
Ô£Æ
Ô£ØÔ£∞1 +

Œ≤ ‚àí5Œ≤
‚Ä≤
X‚Ä≤WX

Œ≤ ‚àí5Œ≤

(n ‚àí2 + Q) k2
Ô£π
Ô£∫Ô£ª
‚àí( n‚àí2+Q+2
2
)
,
(5.69)
where
k2 =

y ‚àíX5Œ≤
‚Ä≤
W

y ‚àíX5Œ≤

+ QR
(n ‚àí2 + Q)
.
The expression in brackets in (5.69) is the kernel of the density of a bivariate
t distribution having mean vector 5Œ≤, scale parameter matrix
(X‚Ä≤WX)‚àí1 k2
and n ‚àí2 + Q degrees of freedom. However, the form of the distribution
[Œ≤0, Œ≤1|w, y, Q, R, ŒΩ] will depend on the form of the priors adopted for the
regression coeÔ¨Écients. For example, if these priors are uniform, the poste-
rior distribution is a truncated bivariate-t deÔ¨Åned inside the corresponding
boundaries.
Joint posterior density of Œ≤0, Œ≤1, and œÉ2, unconditionally to w. This is
obtained by integrating over w. Note that the joint density (5.47) factorizes
into n independent parts that can be integrated separately, to obtain
p

Œ≤0, Œ≤1, œÉ2|y, ŒΩ

‚àù
n
-
i=1
‚àû

0

œÉ2
wi
‚àí1
2
√ów
ŒΩ
2 ‚àí1
i
exp
#
‚àíwi
2

(yi ‚àíŒ≤0 ‚àíŒ≤1xi)2 + ŒΩœÉ2
œÉ2
$
dwip (Œ≤0) p (Œ≤1) p

œÉ2
‚àùp (Œ≤0) p (Œ≤1) p

œÉ2 
œÉ2‚àín
2
n
-
i=1
‚àû

0
w
ŒΩ+1
2
‚àí1
i
exp

‚àíwiSi
2
%
dwi
recalling that
Si = (yi ‚àíŒ≤0 ‚àíŒ≤1xi)2 + ŒΩœÉ2
œÉ2
.

5.5 Bayesian Updating
249
The integrand in the preceding expression is the kernel of a gamma density
with parameters (ŒΩ + 1) /2 and Si/2, so that one obtains
p

Œ≤0, Œ≤1, œÉ2|y, ŒΩ

‚àùp (Œ≤0) p (Œ≤1) p

œÉ2 
œÉ2‚àín
2
√ó
n
-
i=1
Œì

ŒΩ + 1
2
 
Si
2
‚àíŒΩ+1
2
‚àùp (Œ≤0) p (Œ≤1) p

œÉ2 
œÉ2‚àín
2
√ó
n
-
i=1

(yi ‚àíŒ≤0 ‚àíŒ≤1xi)2 + ŒΩœÉ2
œÉ2
‚àíŒΩ+1
2
‚àùp (Œ≤0) p (Œ≤1) p

œÉ2
√ó
n
-
i=1

œÉ2‚àí1
2

1 + (yi ‚àíŒ≤0 ‚àíŒ≤1xi)2
ŒΩœÉ2
‚àíŒΩ+1
2
.
(5.70)
This is precisely the joint posterior distribution of all parameters for the
linear regression model in (5.43), assuming known degrees of freedom. Ad-
ditional marginalization is not possible by analytical means, but one can
estimate lower-dimensional posterior distributions of interest using Monte
Carlo methods, as discussed later in the book.
‚ñ†
5.5
Bayesian Updating
The concept of Bayesian learning in a discrete setting was discussed in
a previous section; see (5.5). This will be revisited for continuous-valued
parameters and observations. Suppose that data accrue sequentially as
y1, y2, ..., yK, and that the problem is to infer a parameter vector Œ∏. The
posterior density of Œ∏ is
p (Œ∏|y1, y2, ..., yK) = g (Œ∏) p (y1, y2, . . . , yK|Œ∏)
m (y1, y2, . . . , yK)
= g (Œ∏) p (y1|Œ∏) p (y2|y1, Œ∏) ...p (yK|y1, . . . , yK‚àí1, Œ∏)
m (y1) m (y2|y1) . . . m (yK|y1, . . . , yK‚àí1)
‚àùg (Œ∏) p (y1|Œ∏) p (y2|y1, Œ∏) . . . p (yK|y1, . . . , yK‚àí1, Œ∏) .
(5.71)
The posterior distribution of any function h(Œ∏) is arrived at by making a
one-to-one transformation of the parameter vector such that one of the new
variables is h (or set of variables, if h is vector-valued), and then integrating
over the remaining ‚Äúdummy‚Äù variables. From (5.71) it follows that
p (Œ∏|y1, y2, . . . , yK) ‚àùp (Œ∏|y1) p (y2|y1, Œ∏) . . . p (yK|y1, y2, . . . , yK‚àí1, Œ∏)
‚àùp (Œ∏|y1, y2) . . . p (yK|y1, y2, . . . , yK‚àí1, Œ∏)
‚àùp (Œ∏|y1, y2, . . . , yK‚àí1) p (yK|y1, y2, . . . , yK‚àí1, Œ∏) .
(5.72)

250
5. An Introduction to Bayesian Inference
Note that the posterior distribution at stage i of learning acts as a prior for
stage i + 1, as already noted in the discrete case. This implies that a single
Bayesian analysis carried out at the end of the process will lead to the
same inferences about Œ∏ as one carried out sequentially. If data accruing at
diÔ¨Äerent stages are conditionally independent, (5.71) becomes
p (Œ∏|y1, y2, . . . , yK) ‚àùg (Œ∏) p (y1|Œ∏) p (y2|Œ∏) . . . p (yK|Œ∏)
‚àùg (Œ∏)
K
-
i=1
p (yi|Œ∏) .
(5.73)
Example 5.8
Progeny test of dairy bulls
Suppose that S unrelated dairy bulls are mated to unrelated cows, leading
to ni daughters per bull. Suppose, for simplicity, that the milk production
of such daughters is measured under the same environmental conditions. A
linear model for the production of daughter j of bull i could be
yij = si + eij,
where, in the dairy cattle breeding lexicon, si is known as the ‚Äútransmitting
ability‚Äù of bull i, and eij ‚àºN

0, œÉ2
e

is a residual, assumed to be indepen-
dently distributed of any si, and of any other residual. Let the average
production of daughters of bull i be yi. The model for such an average is
yi =
ni

j=1
yij
ni
= si + ei,
where ei ‚àºN

0, œÉ2
e/ni

is the average of individual residuals. We seek
to infer si, given information on the average production of the daughters.
Suppose that, a priori, each of the transmitting abilities is assigned the
distribution si ‚àºN

¬µ, œÉ2
s

, and that these are independent among bulls;
assume that ¬µ, œÉ2
s, and œÉ2
e are known. The posterior distribution of all
transmitting abilities, based on averages is
p

s1, s2, . . . , sS|y, y2, . . . , yS, ¬µ, œÉ2
s, œÉ2
e

‚àù
S
-
i=1
exp

‚àíni
2œÉ2e
(yi ‚àísi)2
 S
-
i=1
exp

‚àí1
2œÉ2s
(si ‚àí¬µ)2

=
S
-
i=1
exp

‚àí1
2œÉ2e

ni (si ‚àíyi)2 + œÉ2
e
œÉ2s
(si ‚àí¬µ)2
%
.
(5.74)
Hence, the transmitting abilities of all bulls are also independent a poste-
riori; this would not be so if bulls were genetically related, in which case a
multivariate prior would need to be elicited (a situation to be encountered

5.5 Bayesian Updating
251
in the following example). Now, the two quadratic forms in the transmitting
ability in (5.74) can be combined employing (5.16) as
ni (si ‚àíyi)2 + œÉ2
e
œÉ2s
(si ‚àí¬µ)2 =

ni + œÉ2
e
œÉ2s

(si ‚àísi)2 +
ni
œÉ2
e
œÉ2
s
ni + œÉ2
e
œÉ2
s
(yi ‚àí¬µ)2 ,
(5.75)
where:
si =

ni + œÉ2
e
œÉ2s
‚àí1 
niyi + œÉ2
e
œÉ2s
¬µ

=
niyi
ni + œÉ2
e
œÉ2
s
+
Ô£´
Ô£≠1 ‚àí
ni
ni + œÉ2
e
œÉ2
s
Ô£∂
Ô£∏¬µ
= ¬µ +
ni
ni + œÉ2
e
œÉ2
s
(yi ‚àí¬µ) .
(5.76)
Using (5.75) in (5.74) and retaining only the portion that varies with si
gives as posterior density of the transmitting ability of bull i:
p

si|yi, ¬µ, œÉ2
s, œÉ2
e

‚àùexp
Ô£Æ
Ô£∞‚àí

ni + œÉ2
e
œÉ2
s

2œÉ2e
(si ‚àísi)2
Ô£π
Ô£ª.
(5.77)
Hence, the posterior distribution is normal, with mean as in (5.76) and
variance
V ar

si|yi, ¬µ, œÉ2
s, œÉ2
e

=
œÉ2
e
ni + œÉ2
e
œÉ2
s
.
(5.78)
Suppose now that, for each sire, the data arrive sequentially in two ‚Äúcrops‚Äù
of daughters of sizes ni1 and ni2, respectively. We proceed to verify that if
the posterior distribution after crop 1 is used as prior for crop 2, one obtains
the posterior distribution with density as in (5.77). From the preceding
developments, it follows immediately that the posterior density after crop
1 is
p

si|yi1, ¬µ, œÉ2
s, œÉ2
e

‚àùexp

‚àí1
2œÉ2e

ni1 + œÉ2
e
œÉ2s

(si ‚àísi1)2
%
,
(5.79)
where yi1 is the average production of daughters in crop 1, and si1 is the
mean of the posterior distribution after crop 1. Using this as prior for crop
2, the posterior distribution, after observing that the average production
of the ni2 cows is yi2, is
p

si|yi1, yi2, ¬µ, œÉ2
s, œÉ2
e

‚àùexp
Ô£Æ
Ô£∞‚àíni2 (yi2 ‚àísi)2
2œÉ2e
‚àí

ni1 + œÉ2
e
œÉ2
s

(si ‚àísi1)2
2œÉ2e
Ô£π
Ô£ª.
(5.80)

252
5. An Introduction to Bayesian Inference
Combining the two quadratic forms as before
ni2 (si ‚àíyi2)2 +

ni1 + œÉ2
e
œÉ2s

(si ‚àísi1)2
=

ni1 + ni2 + œÉ2
e
œÉ2s
 
si ‚àísi
2 +
ni2

ni1 + œÉ2
e
œÉ2
s

ni1 + ni2 + œÉ2
e
œÉ2
s
(yi2 ‚àísi1)2
where
si =
ni2yi2 +

ni1 + œÉ2
e
œÉ2
s

si1
ni1 + ni2 + œÉ2
e
œÉ2
s
=
ni2yi2 +

ni1 + œÉ2
e
œÉ2
s
 
¬µ +
ni1
ni1+
œÉ2e
œÉ2s
(yi1 ‚àí¬µ)

ni1 + ni2 + œÉ2
e
œÉ2
s
=
ni1yi1 + ni2yi2 + œÉ2
e
œÉ2
s ¬µ
ni1 + ni2 + œÉ2
e
œÉ2
s
= ni1yi1 + ni2yi2
ni1 + ni2 + œÉ2
e
œÉ2
s
+
Ô£Æ
Ô£∞1 ‚àí
ni1 + ni2
ni1 + ni2 + œÉ2
e
œÉ2
s
Ô£π
Ô£ª¬µ
= ¬µ +
ni
ni + œÉ2
e
œÉ2
s
(yi ‚àí¬µ) = si,
which is identical to (5.76). Use of this in (5.80), after retaining only the
part that involves si, and noting that ni1 + ni2 = ni, gives
p

si|yi1, yi2, ¬µ, œÉ2
s, œÉ2
e

‚àùexp
Ô£Æ
Ô£∞‚àí

ni + œÉ2
e
œÉ2
s

(si ‚àísi)2
2œÉ2e
Ô£π
Ô£ª.
Thus, the posterior density is identical to (5.77), illustrating that Bayes
theorem has ‚Äúmemory‚Äù, and that inferences can be updated sequentially.
As a side point, note that there can be two alternative scenarios in this
hypothetical scheme. In scenario A, say, inferences are done at the end, and
all one needs for constructing the posterior (given the prior information)
is ni and yi, without knowledge of the averages of each of the two crops of
daughters. In the sequential updating setting (scenario B), one needs yi1,
yi2, ni1, and ni2. Hence, scenario B requires knowing the progeny group
sizes and the averages at each crop, i.e., more information about the data
collection process is needed. At any rate, the Bayesian analysis leads to
the same inferences. This is related to what are called ‚Äústopping rules‚Äù in

5.5 Bayesian Updating
253
sequential experimental design (O‚ÄôHagan, 1994). Suppose an experiment
is designed such that it terminates after collecting n observations. Then,
the Bayesian analysis infers the parameters using the n observations, and
inferences are the same irrespective of whether the experiment has been
designed sequentially or not. On the other hand, classical methods give
diÔ¨Äerent inferences for the same data collected either in a sequential or
nonsequential manner. For example, if a hypothesis is tested, the sequential
experiment gives a lower degree of signiÔ¨Åcance than the nonsequential one
(O‚ÄôHagan, 1994).
‚ñ†
Example 5.9
Updating additive genetic eÔ¨Äects
The setting is similar to that of the preceding example. Suppose that at
stage 1 (2), measurements y1 (y2) are taken on n1 (n2) diÔ¨Äerent individuals
(so that an individual measured at any stage is not recorded at the other
stage), and that the objective is to infer their additive genetic eÔ¨Äects a1
(a2) . Suppose the following linear model holds

y1
y2

=

11¬µ1
12¬µ2

+

a1
a2

+

e1
e2

,
(5.81)
where ¬µ1 and ¬µ2 are known location parameters common to records col-
lected in stages 1 and 2, respectively, and

e1
e2
"""" œÉ2
e ‚àºN


0
0

,

I1
0
0
I2

œÉ2
e

is a vector of independently distributed residual eÔ¨Äects, where œÉ2
e is the
(known) residual variance; Ii is an identity matrix of order ni√óni, (i = 1, 2).
The conditional distribution of the observations, given the additive genetic
eÔ¨Äects, is
p

y1, y2|¬µ1, ¬µ2, a1, a2, œÉ2
e

‚àùexp
Ô£Æ
Ô£ØÔ£ØÔ£∞‚àí
2
i=1
(yi ‚àí1i¬µi ‚àíai)‚Ä≤ (yi ‚àí1i¬µi ‚àíai)
2œÉ2e
Ô£π
Ô£∫Ô£∫Ô£ª.
(5.82)
In the classical inÔ¨Ånitesimal model of inheritance, the additive genetic ef-
fects are assumed to follow the multivariate normal distribution (acting as
a prior in the Bayesian sense):

a1
a2
"""" œÉ2
a ‚àºN


0
0

,

A11
A12
A21
A22

œÉ2
a

(5.83)
where œÉ2
a is the additive genetic variance in the population (also assumed
known), and
A =

A11
A12
A21
A22


254
5. An Introduction to Bayesian Inference
is the matrix of additive genetic relationships between individuals, or twice
the matrix of coeÔ¨Écients of coancestry. This matrix is assumed to have
full rank, that is, clones or identical twins are not encountered. For ex-
ample, A12 contains the additive genetic relationships between individuals
measured at stages 1 and 2. Recall that

A11 ‚àíA12A‚àí1
22 A21

œÉ2
a = A1.2œÉ2
a
(5.84)
is the covariance matrix of the conditional distribution of a1 given a2, and
that

A22 ‚àíA21A‚àí1
11 A12

œÉ2
a = A2.1œÉ2
a
(5.85)
is the covariance matrix of the conditional distribution of a2 given a1.
Further, A21A‚àí1
11 is the multivariate regression of additive genetic eÔ¨Äects
of individuals measured in stage 2 on additive genetic eÔ¨Äects of individuals
measured in stage 1. In fact, one can write
a2 = E (a2|a1) + œµ
= E (a2) + A21A‚àí1
11 [a1 ‚àíE (a1)] + œµ
= A21A‚àí1
11 a1 + œµ,
(5.86)
where œµ is a residual distributed independently of a1, and having the dis-
tribution
œµ ‚àºN

0, A2.1œÉ2
a

.
(5.87)
Further, using properties of inverses of partitioned matrices (Searle, 1971),
let
A‚àí1 =

A11
A12
A21
A22

,
where
A11 =

A11 ‚àíA12A‚àí1
22 A21
‚àí1 = A‚àí1
1.2,
A12 = ‚àíA11A12A‚àí1
22 ,
A21 = ‚àíA‚àí1
22 A21A11,
and
A22 = A‚àí1
22 + A‚àí1
22 A21A11A‚àí1
22 .
Suppose that data collection at stage 2 has been completed, and that we
proceed to infer the breeding values of all individuals. The posterior density
of the additive genetic eÔ¨Äects, in view of (5.82) and (5.83), can be written
as
p

a1, a2|y1, y2, ¬µ1, ¬µ2, œÉ2
e, œÉ2
a

‚àùexp

‚àí1
2œÉ2e

(a ‚àíw)‚Ä≤ (a ‚àíw) + œÉ2
e
œÉ2a
a‚Ä≤A‚àí1a
%
,
(5.88)

5.5 Bayesian Updating
255
where
w =

y1 ‚àí11¬µ1
y2 ‚àí12¬µ2

=

w1
w2

.
Combining the two quadratic forms in (5.88), by means of (5.56) to (5.57),
(a ‚àíw)‚Ä≤ (a ‚àíw) + œÉ2
e
œÉ2a
a‚Ä≤A‚àí1a
= (a ‚àí5a)‚Ä≤

I + A‚àí1 œÉ2
e
œÉ2a

(a ‚àí5a) + w‚Ä≤

I + A‚àí1 œÉ2
e
œÉ2a
‚àí1
A‚àí1 œÉ2
e
œÉ2a
w,
where
5a =

I + A‚àí1 œÉ2
e
œÉ2a
‚àí1
w =

I + A‚àí1 œÉ2
e
œÉ2a
‚àí1 
y1 ‚àí11¬µ1
y2 ‚àí12¬µ2

.
(5.89)
Using this in (5.88) and retaining only the part that varies with a gives as
posterior density of the additive genetic eÔ¨Äects,
p

a1, a2|y1, y2, ¬µ1, ¬µ2, œÉ2
e, œÉ2
a

‚àùexp

‚àí1
2œÉ2e

(a‚àí5a)‚Ä≤

I + A‚àí1 œÉ2
e
œÉ2a

(a‚àí5a)
%
.
(5.90)
Thus, the joint posterior of all additive genetic eÔ¨Äects is normal with mean
vector 5a and variance‚Äìcovariance matrix
V ar

a1, a2|y1, y2, ¬µ1, ¬µ2, œÉ2
e, œÉ2
a

=

I + A‚àí1 œÉ2
e
œÉ2a
‚àí1
œÉ2
e.
(5.91)
Using (5.89) to (5.91), the density of the ‚ÄúÔ¨Årst-stage‚Äù distribution

a1|y1, ¬µ1, œÉ2
e, œÉ2
a

is immediately found to be
p

a1|y1, ¬µ1, œÉ2
e, œÉ2
a

‚àùexp

‚àí1
2œÉ2e

(a1 ‚àí,a1)‚Ä≤

I1 + A‚àí1
11
œÉ2
e
œÉ2a

(a1 ‚àí,a1)
%
.
(5.92)
The posterior mean at stage 1 is then
,a1 =

I1 + A‚àí1
11
œÉ2
e
œÉ2a
‚àí1
(y1 ‚àí11¬µ1) =

I1 + A‚àí1
11
œÉ2
e
œÉ2a
‚àí1
w1
(5.93)
and the posterior covariance is
V ar

a1|y1, ¬µ1, œÉ2
e, œÉ2
a

=

I1 + A‚àí1
11
œÉ2
e
œÉ2a
‚àí1
œÉ2
e = ,C1.
(5.94)

256
5. An Introduction to Bayesian Inference
What can be said about all additive genetic eÔ¨Äects at stage 1? The joint
posterior at stage 1 is
p

a1, a2|y1, ¬µ1, œÉ2
e, œÉ2
a

‚àù

p

y1|¬µ1, œÉ2
e

p

a1|œÉ2
a

p

a2|a1, œÉ2
a

.
Noting that the expression in brackets is the posterior after stage 1, one
can write
p

a1, a2|y1, ¬µ1, œÉ2
e, œÉ2
a

‚àùp

a2|a1, œÉ2
a

p

a1|y1, ¬µ1, œÉ2
e, œÉ2
a

and this is the density of a normal process because the two intervening
densities are in normal forms. Hence, the marginal distribution of a2 at
stage 1 is normal as well, with marginal density
p

a2|y1, ¬µ1, œÉ2
e, œÉ2
a

=

p

a2|a1, œÉ2
a

p

a1|y1, ¬µ1, œÉ2
e, œÉ2
a

da1.
The representation above indicates that the mean of the posterior distri-
bution of a2 at stage 1 can be found to be, making use of (5.86),
,a2 = E

a2|y1, ¬µ1, œÉ2
e, œÉ2
a

= Ea1|y1 [E (a2|a1)]
= Ea1|y1

A21A‚àí1
11 a1

= A21A‚àí1
11 ,a1
= A21A‚àí1
11

I1 + A‚àí1
11
œÉ2
e
œÉ2a
‚àí1
w1.
(5.95)
Note that this has the form of E (a2|a1) , but with a1 replaced by its pos-
terior expectation. Likewise,
,C2 = V ar

a2|y1, ¬µ1, œÉ2
e, œÉ2
a

= Ea1|y1 [V ar (a2|a1)] + V ara1|y1 [E (a2|a1)]
= Ea1|y1

A22œÉ2
a ‚àíA21A‚àí1
11 A12œÉ2
a

+ V ara1|y1

A21A‚àí1
11 a1

= A22œÉ2
a ‚àíA21A‚àí1
11 A12œÉ2
a + A21A‚àí1
11 ,C1A‚àí1
11 A12.
(5.96)
The Ô¨Årst term represents the variance of a2 before observing anything; the
second term is the reduction in variance that would be obtained if a1 were
known, and the third term is a penalty that results from having to infer
a1 from y1. Thus, the prior distribution for a2 to be used at stage 2 is
a normal process with mean vector (5.95) and covariance matrix (5.96).
Finally, at stage 2, the posterior density of a2 is
p

a2|y1, y2, ¬µ1, ¬µ2, œÉ2
e, œÉ2
a

‚àùp

y2|¬µ2, a2, œÉ2
e

p

a2|,a2, ,C2

‚àùexp

‚àí(y2 ‚àí12¬µ2 ‚àía2)‚Ä≤ (y2 ‚àí12¬µ2 ‚àía2)
2œÉ2e
‚àí(a2 ‚àí,a2)‚Ä≤ ,C‚àí1
2 œÉ2
e (a2 ‚àí,a2)
2œÉ2e

.
(5.97)

5.6 Features of Posterior Distributions
257
We know that the density is in a normal form, so the quadratics on a2 can be
combined in the usual manner, to arrive at the mean vector and covariance
matrix of the distribution. Alternatively, noting that a normal distribution
is unimodal (so the mean is identical to the mode), the posterior mean at
stage 2 can be found by maximizing the logarithm of (5.97). Let
F (a2) = ‚àí
(y2 ‚àí12¬µ2 ‚àía2)‚Ä≤ (y2 ‚àí12¬µ2 ‚àía2)
2œÉ2e
+(a2 ‚àí,a2)‚Ä≤ ,C‚àí1
2 œÉ2
e (a2 ‚àí,a2)
2œÉ2e

so
‚àÇF (a2)
‚àÇa2
= (‚àí1) ‚àí2 (y2 ‚àí12¬µ2 ‚àía2) + 2,C‚àí1
2 œÉ2
e (a2 ‚àí,a2)
2œÉ2e
.
Setting to 0 and solving for a2 yields
5a2 =

I2 + ,C‚àí1
2 œÉ2
e
‚àí1 
y2 ‚àí12¬µ2 + ,C‚àí1
2 œÉ2
e,a2

(5.98)
as mean of the posterior distribution of a2, after stages 1 and 2. This is a
matrix weighted average of ,a2 and of y2 ‚àí12¬µ2. The variance‚Äìcovariance
matrix of the distribution is given by
V ar

a2|y1, y2, ¬µ1, ¬µ2, œÉ2
e, œÉ2
a

=

I2 + ,C‚àí1
2 œÉ2
e
‚àí1
œÉ2
e.
(5.99)
It can be veriÔ¨Åed that this is equal to the inverse of minus the matrix
of second derivatives of F (a2) with respect to a2. One can also verify
that (5.98) is identical to the a2-component of the solution to (5.89). This
requires very tedious algebra, so it is not shown here.
‚ñ†
5.6
Features of Posterior Distributions
The marginal posterior distribution gives an exact, complete description of
the state of knowledge about an unknown, after having observed the data.
This unknown can be a parameter, a hypothesis, a model, or a yet to be
observed data point, and can be unidimensional or multidimensional, de-
pending on the inferential objectives. In principle, for reporting purposes,
one could present the posterior distribution, and then make a commented
tour of it, highlighting zones of relatively high density or probability, point-
ing out the existence of any multimodality, and indicating areas where the
true value of the parameter may be located. The posterior holds for sam-
ples of any size, so this gives a complete solution to the problem of Ô¨Ånite
sample size inference. In a Bayesian report, however, due to space consider-
ations, all posterior distributions of interest cannot be presented. Instead,

258
5. An Introduction to Bayesian Inference
these are typically replaced by selected posterior summaries. Clearly, when
condensing all the information in the posterior distribution into a couple of
posterior summaries, some information about the form of the posterior is
lost. There are exceptions; for example, if the posterior distribution is nor-
mal, then a report of the mean and variance suÔ¨Éces for characterizing the
posterior process in full. Here we present some of the most widely used pos-
terior summaries, and discuss their justiÔ¨Åcation from a decision-theoretic
point of view.
5.6.1
Posterior Probabilities
A natural summary is provided by a set of probabilities that the true
parameter falls in some regions of interest. If Œò is the parameter space,
the probability that Œ∏ falls in some region ‚Ñúof Œò is
Pr (Œ∏ ‚àà‚Ñú|y) =

‚Ñú
p (Œ∏|y) dŒ∏.
(5.100)
Often, interest centers on just some of the elements of Œ∏, say Œ∏1, with the
remaining elements (Œ∏2) acting as nuisance parameters. In this case, the
required probability is
Pr (Œ∏1 ‚àà‚Ñú1|y) =

‚Ñú1

Œò2
p (Œ∏2, Œ∏1|y) dŒ∏
=

‚Ñú1
p (Œ∏1|y) dŒ∏1,
(5.101)
where Œò2 is the parameter space of Œ∏2. An alternative expression is
Pr (Œ∏1 ‚àà‚Ñú1|y) =

‚Ñú1

Œò2
p (Œ∏2, Œ∏1|y) dŒ∏
=

‚Ñú1

Œò2
p (Œ∏1|Œ∏2, y) p (Œ∏2|y) dŒ∏.
(5.102)
Reversing the order of integration one can write
Pr (Œ∏1 ‚àà‚Ñú1|y) =

Œò2

‚Ñú1
p (Œ∏1|Œ∏2, y) dŒ∏1

p (Œ∏2|y) dŒ∏2.
(5.103)
The term in brackets is the conditional probability that Œ∏1 ‚àà‚Ñú1, given Œ∏2
and y. Hence, the marginal probability can be expressed as
Pr (Œ∏1 ‚àà‚Ñú1|y) = EŒ∏2|y [Pr (Œ∏1 ‚àà‚Ñú1|Œ∏2, y)] .
(5.104)
It follows that the posterior probability that Œ∏1 ‚àà‚Ñú1 is the weighted av-
erage of the corresponding probabilities at each possible value of the nui-
sance parameter Œ∏2, with the weight function being the marginal posterior

5.6 Features of Posterior Distributions
259
density p (Œ∏2|y) . Expression (5.104) can be useful in connection with the
estimation of probabilities by Monte Carlo methods. BrieÔ¨Çy, suppose that
m samples are drawn from the posterior distribution of the nuisance pa-
rameter [Œ∏2|y] , and that these samples are Œ∏(1)
2 , Œ∏(1)
2 , ..., Œ∏(m)
2
. A consistent
estimator of the posterior probability that Œ∏1 ‚àà‚Ñú1 is given by
6
Pr (Œ∏1 ‚àà‚Ñú1|y) = 1
m
m

i=1
Pr

Œ∏1 ‚àà‚Ñú1|Œ∏(i)
2 , y

.
(5.105)
This sort of calculation can be useful when analytical integration over Œ∏2
is not feasible or very diÔ¨Écult. However, requirements include:
(a) it must be relatively easy to sample from [Œ∏2|y] , and
(b) Pr

Œ∏1 ‚àà‚Ñú1|Œ∏(i)
2 , y

must be available in closed form, so that this con-
ditional probability can be evaluated at each draw Œ∏(i)
2 .
An alternative consistent estimator of the integral (5.101), which is sim-
pler to compute, is
6
Pr (Œ∏1 ‚àà‚Ñú1|y) = 1
m
m

i=1
I

Œ∏(i)
1
‚àà‚Ñú1

,
where I (¬∑) is the indicator function and Œ∏(i)
1
are Monte Carlo draws from
[Œ∏1|y]. Here one must be able to draw samples from [Œ∏1|y].
Example 5.10
Posterior probability that the breeding value of an indi-
vidual exceeds a certain threshold
A sample of n unrelated individuals is drawn from a population of ovines
raised to produce cashmere. The problem is to compute the probability
that the breeding value (additive genetic eÔ¨Äect) of a particular individual
is larger or smaller than a certain quantity. This is similar to the setting
in Example 5.3. Suppose that a tenable model (although very naive in
practice) for describing cashmere Ô¨Åber diameter measured in individual i
is
yi = ai + ei,
where ai is the breeding value of i for Ô¨Åber diameter and ei is an environ-
mental eÔ¨Äect. It is known that ai has the normal distribution N

0, œÉ2
a

,
where œÉ2
a is unknown, and that it is statistically independent of ei ‚àº
N

0, œÉ2
e

, where œÉ2
e is also unknown. Assume that the ‚Äútotal‚Äù variance,
œÉ2
a+œÉ2
e, is known without error, and has been estimated from the variability
between individual measurements in a large collection of individuals. This
being the case, one can rescale the observations by dividing by
>
œÉ2a + œÉ2e,
to obtain
y‚àó
i = a‚àó
i + e‚àó
i
=
ai
>
œÉ2a + œÉ2e
+
ei
>
œÉ2a + œÉ2e
.

260
5. An Introduction to Bayesian Inference
Thus, a‚àó
i ‚àºN

0, h2
and e‚àó
i ‚àºN

0, 1 ‚àíh2
, where
h2 =
œÉ2
a
œÉ2a + œÉ2e
is the unknown heritability of cashmere Ô¨Åber diameter, a parameter ranging
between 0 and 1. Hence, there is a single dispersion parameter, h2. The joint
posterior density of all unknowns is then
p

a‚àó
1, a‚àó
2, . . . , a‚àó
n, h2|y‚àó
‚àùp

h2
n
-
i=1
1
>
2œÄ (1 ‚àíh2)
exp

‚àí(y‚àó
i ‚àía‚àó
i )2
2 (1 ‚àíh2)

,
(5.106)
where p

h2
is the prior density of heritability; let the prior distribution of
this parameter be uniform between 0 and 1. Combining the two quadratics
in a‚àó
i :
(a‚àó
i ‚àíy‚àó
i )2
(1 ‚àíh2) + a‚àó2
i
h2 =

1
1 ‚àíh2 + 1
h2

(a‚àó
i ‚àía‚àó
i )2 +

1
1‚àíh2 +
1
h2
‚àí1
(1 ‚àíh2) h2
y‚àó2
i
=
1
(1 ‚àíh2) h2 (a‚àó
i ‚àía‚àó
i )2 + y‚àó2
i ,
(5.107)
where
a‚àó
i = h2y‚àó2
i .
Using (5.107) in (5.106), the joint posterior is then
p

a‚àó
1, a‚àó
2, . . . , a‚àó
n, h2|y‚àó
‚àù

1
(1 ‚àíh2) h2
 n
2
n
-
i=1
exp

‚àí1
2
(a‚àó
i ‚àía‚àó
i )2
(1 ‚àíh2) h2

√ó exp

‚àí1
2
n

i=1
y‚àó2
i

p

h2
.
(5.108)
This indicates that, given h2, breeding values have independent normal
distributions with means a‚àó
i and common variance

1 ‚àíh2
h2. Integrat-
ing now over the n breeding values gives, as marginal posterior density of
heritability,
p

h2|y‚àó
‚àù

1 ‚àíh2
h2‚àín
2 p

h2
n
-
i=1
‚àû

‚àí‚àû
exp

‚àí(a‚àó
i ‚àía‚àó
i )2
2 (1 ‚àíh2) h2

da‚àó
i
‚àù

1 ‚àíh2
h2‚àín
2 p

h2 >
(1 ‚àíh2) h2
n
‚àùp

h2
.
(5.109)
Hence, the Bayesian analysis yields the prior as posterior distribution of
heritability. This is because the data do not contribute information about

5.6 Features of Posterior Distributions
261
the partition of the variance into an additive genetic and an environmental
component, even if the total or phenotypic variance is known. The situation
would be diÔ¨Äerent if some of the individuals were genetically related, but
this is beyond the scope of the example. It follows that the posterior distri-
bution of the nuisance parameter (heritability in this case) is precisely the
prior distribution of h2. Now, to infer features of the posterior distribution
of the breeding value of individual i while taking into account uncertainty
about the nuisance parameter h2, one must average over its posterior dis-
tribution. Recall that our objective is to calculate the posterior probability
that the breeding value is larger than a certain value (0, say). Using (5.103),
and taking into account that the posterior (or prior, in this case) density
of heritability is uniform
Pr (a‚àó
i > 0|y‚àó) =
1

0
Ô£Æ
Ô£∞
‚àû

0
p

a‚àó
i |h2, y‚àó
da‚àó
i
Ô£π
Ô£ªp

h2
dh2
=
1

0

1 ‚àíPr

a‚àó
i ‚â§0|h2, y‚àó
p

h2
dh2
= 1 ‚àí
1

0
Pr

a‚àó
i ‚àía‚àó
i
>
(1 ‚àíh2) h2 ‚â§
‚àía‚àó
i
>
(1 ‚àíh2) h2 |h2, y‚àó

p

h2
dh2
= 1 ‚àí
1

0
Œ¶

‚àía‚àó
i
>
(1 ‚àíh2) h2

p

h2
dh2.
(5.110)
The integral in (5.110) cannot be expressed in closed form, since the in-
tegrand is an integral itself. However, it can be evaluated by Monte Carlo
methods, as follows:
(1) Draw m independent values of h2 from a U (0, 1) distribution; let each
draw be h2(j).
(2) Compute, for each draw, a‚àó(j)
i
= h2(j)y‚àó2
i .
(3) For each draw, calculate
Œ¶
Ô£´
Ô£≠
‚àía‚àó(j)
i
8
1 ‚àíh2(j)
h2(j)
Ô£∂
Ô£∏.
(4) Form the consistent estimator of the desired probability
6
Pr (a‚àó
i > 0|y‚àó) = 1 ‚àí1
m
m

j=1
Œ¶
Ô£´
Ô£≠
‚àía‚àó(j)
i
8
1 ‚àíh2(j)
h2(j)
Ô£∂
Ô£∏.
(5.111)
‚ñ†

262
5. An Introduction to Bayesian Inference
5.6.2
Posterior Quantiles
Consider the scalar posterior distribution [Œ∏|y] deÔ¨Åned in Œ∏L ‚â§Œ∏ ‚â§Œ∏U,
where Œ∏L and Œ∏U are the minimum and maximum values, respectively,
that Œ∏ can take. The Œ±-quantile of the posterior distribution is the value q
satisfying the equation
Pr (Œ∏ ‚â§q|y) = Œ±.
A quantile commonly used to characterize location of the posterior distri-
bution is the posterior median, m, such that
Pr (Œ∏ ‚â§q|y) = Pr (Œ∏ > q|y) = 1
2
or, in the continuous case,
m

‚àí‚àû
p (Œ∏|y) dŒ∏ =
‚àû

m
p (Œ∏|y) dŒ∏.
Quantiles arise in the construction of high-credibility sets, that is, sets
of Œ∏ values that contain the true parametric value at high probability. For
example, a credibility set of size 1‚àíŒ± is given by all possible values between
q1 and q2 such that
Pr (q1 ‚â§Œ∏ ‚â§q2|y) = 1 ‚àíŒ±
(5.112)
Pr (Œ∏ ‚â§q1|y) = Œ±
2 ,
Pr (Œ∏ > q2|y) = Œ±
2 .
If Œ± = 0.05, say, then q1 is the 2.5% quantile of the posterior distribution
and q2 is the 97.5% quantile. In principle, there can be many, perhaps inÔ¨Å-
nite, credibility regions of size 1 ‚àíŒ±. However, if the probability statement
(5.112) is such that all values within the interval are required to have higher
posterior density than values outside it, then (q1, q2) is called a 1‚àíŒ± highest
posterior density interval, or HPD for short (Box and Tiao, 1973). If such
a region exists, then an HPD region of size 1 ‚àíŒ± deÔ¨Ånes the boundaries
unambiguously.
Using the median of a posterior distribution as a ‚Äúpoint estimate‚Äù of Œ∏ has
a justiÔ¨Åcation on decision-theoretic grounds. To illustrate, let the parameter
vector be Œ∏ =

Œ∏1, Œ∏‚Ä≤
2
‚Ä≤ , where Œ∏1 varies over the real line, and Œ∏2 ‚àà‚ÑúŒ∏2 are
nuisance parameters. DeÔ¨Åne L

5Œ∏1, Œ∏1

to be a loss function, with minimum
value 0 when L

5Œ∏1 = Œ∏1, Œ∏1

. Here, 5Œ∏1 is a function involving the data and
possibly the hyperparameters. Now let the loss function have the form
L

5Œ∏1, Œ∏1

= a
"""5Œ∏1 ‚àíŒ∏1
"""
(5.113)

5.6 Features of Posterior Distributions
263
where a is a scalar constant. This implies that the loss is proportional to
the absolute ‚Äúerror of estimation‚Äù. The expected posterior loss is
E

L

5Œ∏1, Œ∏1

|y

=
‚àû

‚àí‚àû

‚ÑúŒ∏2
a
"""5Œ∏1 ‚àíŒ∏1
""" p (Œ∏1, Œ∏2|y) dŒ∏1dŒ∏2
=
‚àû

‚àí‚àû
a
"""5Œ∏1 ‚àíŒ∏1
"""
Ô£Æ
Ô£ØÔ£∞

‚ÑúŒ∏2
p (Œ∏1, Œ∏2|y) dŒ∏2
Ô£π
Ô£∫Ô£ªdŒ∏1
=
‚àû

‚àí‚àû
a
"""5Œ∏1 ‚àíŒ∏1
""" p (Œ∏1|y) dŒ∏1
(5.114)
since integration of the joint density over Œ∏2 gives the marginal posterior
of Œ∏1. Now
"""5Œ∏1 ‚àíŒ∏1
""" =
#
5Œ∏1 ‚àíŒ∏1,
for Œ∏1 ‚â§5Œ∏1,
Œ∏1 ‚àí5Œ∏1,
for 5Œ∏1 < Œ∏1.
Hence, following Zellner (1971),
E

L

5Œ∏1, Œ∏1

|y

= a
Ô£Æ
Ô£ØÔ£∞
Œ∏1

‚àí‚àû

5Œ∏1 ‚àíŒ∏1

p (Œ∏1|y) dŒ∏1 +
‚àû

Œ∏1

Œ∏1 ‚àí5Œ∏1

p (Œ∏1|y) dŒ∏1
Ô£π
Ô£∫Ô£ª
‚àù
Ô£±
Ô£¥
Ô£≤
Ô£¥
Ô£≥
5Œ∏1 Pr

Œ∏1 ‚â§5Œ∏1|y

‚àí
Œ∏1

‚àí‚àû
Œ∏1p (Œ∏1|y) dŒ∏1
+
‚àû

Œ∏1
Œ∏1p (Œ∏1|y) dŒ∏1 ‚àí5Œ∏1

1 ‚àíPr

Œ∏1 ‚â§5Œ∏1|y

Ô£º
Ô£¥
Ô£Ω
Ô£¥
Ô£æ
.
(5.115)
We seek now the 5Œ∏1 minimizing the expected posterior loss. DiÔ¨Äerentiating
(5.115) with respect to 5Œ∏1, recalling that
‚àÇ
 Œ∏1
‚àí‚àû
Œ∏1p (Œ∏1|y) dŒ∏1

‚àÇ5Œ∏1
= 5Œ∏1p

5Œ∏1|y

and setting the derivative to 0, yields, after some terms cancel out
‚àÇE

L

5Œ∏1, Œ∏1

|y

‚àÇ5Œ∏1
= 2 Pr

Œ∏1 ‚â§5Œ∏1|y

‚àí1 = 0.

264
5. An Introduction to Bayesian Inference
This gives the equation
Pr

Œ∏1 ‚â§5Œ∏1|y

= 1
2
which is satisÔ¨Åed by taking 5Œ∏1 to be the median of the posterior distri-
bution. Hence, the median is optimum in the sense of minimizing the ex-
pected absolute ‚Äúerror of estimation‚Äù. The median is functionally invariant
under one-to-one transformation. This implies that if 5Œ∏1 is the median of
the posterior distribution of Œ∏, then g

5Œ∏1

is the median of the posterior
distribution of g (Œ∏) . Hence, g

5Œ∏1

minimizes the expected posterior loss
E
9
L

g

5Œ∏1

, g (Œ∏1)

|y
:
=
‚àû

‚àí‚àû
a
"""g

5Œ∏1

‚àíg (Œ∏1)
""" p (Œ∏1|y) dŒ∏1.
5.6.3
Posterior Modes
The modal vector of the posterior distribution is deÔ¨Åned as
,Œ∏ = Arg max
Œ∏
[p (Œ∏|y)] = Arg max
Œ∏
[c L (Œ∏|y) g (Œ∏)]
= Arg max
Œ∏
{log [L (Œ∏|y)] + log [g (Œ∏)]} ,
(5.116)
that is, as the value of the parameter having highest density (or probability
in discrete situations). As noted in Section 5.3, if the prior distribution of Œ∏
is uniform, then the posterior mode is identical to the maximum likelihood
estimator; this is an incidental, and not a fundamental issue in Bayesian
analysis. Since the posterior mode is interpretable as ‚Äúthe most likely value
of the parameter‚Äù, it is a natural candidate as a purveyor of information
about the location of the posterior distribution.
The mode has a decision-theoretic justiÔ¨Åcation as a point estimator of a
parameter, at least in the single parameter situation. Suppose that the loss
function has the following form (O‚ÄôHagan, 1994):
L

5Œ∏1, Œ∏1

Ô£±
Ô£¥
Ô£≤
Ô£¥
Ô£≥
0,
if
"""5Œ∏1 ‚àíŒ∏1
""" ‚â§b,
1,
if
"""5Œ∏1 ‚àíŒ∏1
""" > b,

5.6 Features of Posterior Distributions
265
for some constant b. The expected posterior loss is then
E

L

5Œ∏1, Œ∏1

|y

= 0 √ó Pr
"""5Œ∏1 ‚àíŒ∏1
""" ‚â§b|y

+ 1 √ó Pr
"""5Œ∏1 ‚àíŒ∏1
""" > b|y

= Pr
"""5Œ∏1 ‚àíŒ∏1
""" > b|y

= Pr

5Œ∏1 ‚àíŒ∏1 > b|y

+ Pr

Œ∏1 ‚àí5Œ∏1 > b|y

=
Œ∏1‚àíb

‚àí‚àû
p (Œ∏1|y) dŒ∏1 +
‚àû

Œ∏1+b
p (Œ∏1|y) dŒ∏1.
(5.117)
Now, take derivatives of (5.117) with respect to 5Œ∏1 to locate a minimum
‚àÇE

L

5Œ∏1, Œ∏1

|y

‚àÇ5Œ∏1
= p

5Œ∏1 ‚àíb|y

‚àíp

5Œ∏1 + b|y

.
(5.118)
Setting to zero gives, as Ô¨Årst-order condition,
p

5Œ∏1 ‚àíb|y

= p

5Œ∏1 + b|y

.
If 5Œ∏1 is a mode of the posterior distribution, it must be true that p

5Œ∏1|y

‚â•
p

5Œ∏1 ‚àíb|y

and p

5Œ∏1|y

‚â•p

5Œ∏1 + b|y

. As b ‚Üí0 the condition is sat-
isÔ¨Åed only by the mode. In the limit, the ‚Äúoptimal‚Äù 5Œ∏1 is given by the
posterior mode.
In a multiparameter situation, the modal vector has as many elements as
there are parameters in the model. Here it is important to make a distinc-
tion between the components of the modal vector of the joint distribution,
and the modes of the marginal distribution of one or of a set of parame-
ters of interest, after some nuisance parameters have been integrated out.
Most often, the marginal models diÔ¨Äer from the corresponding component
of the joint modal vector. Exceptions occur, such as when the posterior is
multivariate normal or multivariate-t. In these cases, if one maximizes the
joint distribution, each of the components of the modal vector is identical
to the mode of the marginal distribution of the corresponding parameter.
With the usual notation, the mode of a joint posterior distribution is
deÔ¨Åned by the statement
 ,,Œ∏1
,,Œ∏2

= Arg max
Œ∏1,Œ∏2
{log (c) + log [L (Œ∏1, Œ∏2|y)] + log [g (Œ∏1, Œ∏2)]} .
The notation ,Œ∏1, ,Œ∏2 will be employed here to denote the marginal modes.
In computing joint modes, it is useful to note that
log [p (Œ∏1, Œ∏2|y)] = log [p (Œ∏1|y)] + log [p (Œ∏2|Œ∏1, y)]
= log [p (Œ∏2|y)] + log [p (Œ∏1|Œ∏2, y)] .

266
5. An Introduction to Bayesian Inference
In order to locate a maximum, the following system of equations must be
solved, simultaneously,
log [p (Œ∏1, Œ∏2|y)]
‚àÇŒ∏1
= ‚àÇlog [p (Œ∏1|Œ∏2, y)]
‚àÇŒ∏1
= 0
(5.119)
and
log [p (Œ∏1, Œ∏2|y)]
‚àÇŒ∏2
= ‚àÇlog [p (Œ∏2|Œ∏1, y)]
‚àÇŒ∏2
= 0.
(5.120)
Typically, this will deÔ¨Åne an iterative algorithm (Lindley and Smith, 1972).
Note that, in some sense, the two equations do not incorporate measures of
uncertainty about nuisance parameters provided by the marginal densities
of the parameters not involved in the diÔ¨Äerentiation. For example, in (5.119)
one constructs a system of equations as if Œ∏2 were known. This suggests
that if a joint mode is used as a point estimator, there may be an implicit
overstatement of precision in the analysis. At least in some variance com-
ponent problems (e.g., Thompson, 1980; Harville, 1977) it has been found
that a component of a joint mode can lead to estimators of variance com-
ponents that are identically equal to zero when used with vague priors.
With multiparameter problems, the joint mode does not always take into
account fully the uncertainty about other parameters in the model that
may be acting as nuisances.
Example 5.11
Joint and marginal modes in comparisons between treat-
ment and control populations
Independent samples of sizes n1, n2, n3 are drawn at random from the
three normal populations N

¬µ1, œÉ2
, N

¬µ2, œÉ2
, and N

¬µ3, œÉ2
, respec-
tively, where the variance is common, but unknown. One of the populations
or ‚Äútreatments‚Äù is a control of some sort. An attribute is measured in each
of the items sampled, and the objective is to infer the diÔ¨Äerences between
means ¬µ1 ‚àí¬µ2 and, possibly, ¬µ1 ‚àí¬µ3, marginally or jointly with ¬µ1 ‚àí¬µ2.
Here œÉ2 legitimately represents a nuisance parameter in all cases, so it is
important to take into account uncertainty about dispersion in the analysis.
Also, note that there are three location parameters, but the desired infer-
ences involve either the marginal distribution of a linear combination of
two means (¬µ1 ‚àí¬µ2) , or a bivariate posterior distribution (that of ¬µ1 ‚àí¬µ2
and ¬µ1 ‚àí¬µ3). In these cases, although marginalization proceeds to diÔ¨Äerent
degrees, the Bayesian approach has a single (and simple) solution: trans-
form the variables as needed, and integrate nuisance parameters to Ô¨Ånd
the target distribution. In many instances, this cannot be done analyti-
cally, but sampling methods are available, as discussed later in this book.
Fortunately, there is an analytical solution in our setting, which is devel-
oped below, step-by-step. The likelihood function of the four parameters

5.6 Features of Posterior Distributions
267
entering into the model is
p

y|¬µ1, ¬µ2, ¬µ3, œÉ2
‚àù
3
-
i=1
ni
-
j=1

œÉ2‚àíni
2 exp

‚àí1
2œÉ2 (yij ‚àí¬µi)2

‚àù

œÉ2‚àín1+n2+n3
2
exp
Ô£Æ
Ô£∞‚àí1
2œÉ2

i

j
(yij ‚àí¬µi)2
Ô£π
Ô£ª
‚àù

œÉ2‚àín1+n2+n3
2
exp
#
‚àí1
2œÉ2

i
Ô£Æ
Ô£∞
j
(yij ‚àíyi)2
+ ni (¬µi ‚àíyi)2:
,
(5.121)
where yij is observation j in treatment i, and yi is the average value of all
observations in the treatment. The maximum likelihood estimators of the
parameters can be readily found to be yi for ¬µi (i = 1, 2, 3), and
5œÉ2 =

i

j
(yij ‚àíyi)2
n1 + n2 + n3
for œÉ2. The Ô¨Ånite sample distributions of the maximum likelihood estima-
tors are
yi
‚àº
N

¬µi, œÉ2
ni

,
i = 1, 2, 3,
5œÉ2
‚àº
œÉ2
n1 + n2 + n3
œá2
n1+n2+n3‚àí3.
Put n = n1 + n2 + n3. Recall that the asymptotic distribution of 5œÉ2 is nor-
mal with mean œÉ2 and variance 2œÉ4/n, which diÔ¨Äers from the Ô¨Ånite sample
distribution given above. On the other hand, the exact and asymptotic dis-
tributions of the means are identical. We now give a Bayesian structure
to the model, and adopt a bounded uniform prior between ¬µmin and ¬µmax
for each of the three location parameters, and a scaled inverted chi-square
distribution with parameters ŒΩ (degree of belief) and œÑ 2 (‚Äúprior value of
the variance‚Äù). The joint posterior density of all parameters, omitting hy-
perparameters in the notation, can be written as
p

¬µ1, ¬µ2, ¬µ3, œÉ2|y

‚àùp

y|¬µ1, ¬µ2, ¬µ3, œÉ2
p (¬µ1, ¬µ2, ¬µ3) p

œÉ2|ŒΩ, œÑ

‚àù

œÉ2‚àín+ŒΩ+2
2
exp
#
‚àí1
2œÉ2

n5œÉ2 + ŒΩœÑ 2 +

i
ni (¬µi ‚àíyi)2
$
(5.122)

268
5. An Introduction to Bayesian Inference
deÔ¨Åned within the boundaries given above. The mode of the joint posterior
distribution is arrived at by diÔ¨Äerentiating the logarithm of (5.122) with
respect to all parameters. The maximizers can be veriÔ¨Åed to be
,,¬µi = yi,
for i = 1, 2, 3 and ,,œÉ
2 = n5œÉ2 + ŒΩœÑ 2
n + ŒΩ + 2 .
(5.123)
The marginal posterior density of œÉ2 is obtained by integrating (5.122) over
the ¬µ‚Ä≤s. Note that the resulting expression can be written as
p

œÉ2|y

‚àù

œÉ2‚àín+ŒΩ+2
2
exp

‚àí1
2œÉ2

n5œÉ2 + ŒΩœÑ 2
√ó
3
-
i=1
¬µmax

¬µmin
exp

‚àíni (¬µi ‚àíyi)2
2œÉ2

d¬µi
‚àù

œÉ2‚àí(n1‚àí1+n2‚àí1+n3‚àí1)+ŒΩ+2
2
exp

‚àí1
2œÉ2

n5œÉ2 + ŒΩœÑ 2
√ó
3
-
i=1

Œ¶

¬µmax ‚àíyi
œÉ2/ni

‚àíŒ¶

¬µmin ‚àíyi
œÉ2/ni

.
(5.124)
This density is not explicit in œÉ2, and Ô¨Ånding the maximizer requires tai-
loring an iterative algorithm. Suppose that the upper and lower boundaries
of the prior distributions of the means approach minus and plus inÔ¨Ånity,
respectively. In the limit, this would give a very vague (in fact, improper)
uniform prior distribution. In this case, the diÔ¨Äerence between the normal
c.d.f.‚Äôs given above goes to 1 for each of the three populations, and the
marginal posterior density of œÉ2 tends to
p

œÉ2|y

‚àù

œÉ2‚àí(n1‚àí1+n2‚àí1+n3‚àí1)+ŒΩ+2
2
exp

‚àí1
2œÉ2

n5œÉ2 + ŒΩœÑ 2
. (5.125)
This is a scaled inverted chi-square density with degree of belief parameter
ŒΩ‚àó= n + ŒΩ ‚àí3,
and scale parameter
œÑ ‚àó2 = n5œÉ2 + ŒΩœÑ 2
ŒΩ‚àó
.
The mode of this marginal distribution is
,œÉ2 = ŒΩ‚àóœÑ ‚àó2
ŒΩ‚àó+ 2.
(5.126)
Comparison of (5.126) with ,,œÉ
2 in (5.123), assuming the same vague priors
for the means as in the last situation, illustrates that the mode of a marginal

5.6 Features of Posterior Distributions
269
distribution can diÔ¨Äer from the corresponding modal component of a joint
distribution. As a side issue, note that the process of integrating out the
three unknown means from the posterior distribution results in a ‚Äúloss of
three degrees of freedom‚Äù, reÔ¨Çecting the straightforward, automatic book-
keeping of information that the probability calculus makes in the Bayesian
context. These three degrees of freedom are not accounted for in the joint
maximization leading to the modal component given in (5.123). Assign
now a scale inverted chi-square process prior to œÉ2 but, instead, take ŒΩ =
0 as a ‚Äúprior degree of belief‚Äù value. In this scenario, the prior density
degenerates to œÉ‚àí2, which is improper, as the integral between 0 and ‚àûis
not Ô¨Ånite. However, the marginal posterior density is still proper provided
that at least two observations are collected in at least one of the three
populations. The improper prior œÉ‚àí2 appears often in Bayesian analysis. In
general, we recommend exercising utmost caution when improper priors are
assigned to parameters, because the posterior distribution may turn out to
be improper, and this is not always straightforward to check. On the other
hand, proper priors ensure that the posterior process will be a proper one.
Next, we proceed to Ô¨Ånd the marginal posterior distribution of the three
means, after integrating œÉ2 out of the joint density of all parameters, given
in (5.122). When this density is viewed as a function of œÉ2, it can be readily
seen that it is in an inverse gamma form, whose integration constant we
know (see Chapter 1). Then the desired integral is given by the reciprocal
of the integration constant
p (¬µ1, ¬µ2, ¬µ3|y) ‚àù

n5œÉ2 + ŒΩœÑ 2 +

i
ni (¬µi ‚àíyi)2
‚àín+ŒΩ
2
.
Eliminating terms that do not depend on the means, and rearranging the
exponent, yields,
p (¬µ1, ¬µ2, ¬µ3|y) ‚àù
Ô£Æ
Ô£ØÔ£∞1 +

i
ni (¬µi ‚àíyi)2
n5œÉ2 + ŒΩœÑ 2
Ô£π
Ô£∫Ô£ª
‚àín‚àí3+ŒΩ+3
2
.
(5.127)
Now write

i
ni (¬µi ‚àíyi)2
= [(¬µ1 ‚àíy1) , (¬µ2 ‚àíy2) , (¬µ3 ‚àíy3)] N
Ô£Æ
Ô£∞
¬µ1 ‚àíy1
¬µ2 ‚àíy2
¬µ3 ‚àíy3
Ô£π
Ô£ª
= (¬µ ‚àíy)‚Ä≤ N (¬µ ‚àíy)

270
5. An Introduction to Bayesian Inference
where
N =
Ô£Æ
Ô£∞
n1
0
0
0
n2
0
0
0
n3
Ô£π
Ô£ª,
and let:
c2 = n5œÉ2 + ŒΩœÑ 2
n ‚àí3 + ŒΩ .
Then (5.127) can be put as
p (¬µ1, ¬µ2, ¬µ3|y) ‚àù

1 + (¬µ ‚àíy)‚Ä≤ N (¬µ ‚àíy)
(n ‚àí3 + ŒΩ) c2
‚àín‚àí3+ŒΩ+3
2
.
(5.128)
This is the density of a trivariate-t distribution with mean vector y, degrees
of freedom (n ‚àí3 + ŒΩ), and variance‚Äìcovariance matrix
V ar (¬µ1, ¬µ2, ¬µ3|y) = c2N‚àí1 (n ‚àí3 + ŒΩ)
(n ‚àí5 + ŒΩ).
(5.129)
This distribution is unimodal and symmetric, and the mode is given by y.
Hence, the mode obtained after marginalizing with respect to œÉ2 is iden-
tical to the ¬µ‚àícomponent of the mode of the joint posterior distribution

¬µ1, ¬µ2, ¬µ3, œÉ2|y

. Further, since the process is multivariate-t, it follows
that all the marginal distributions are univariate t with mean vector yi,
(n ‚àí3 + ŒΩ) degrees of freedom and variance:
V ar (¬µi|y) = c2 (n ‚àí3 + ŒΩ)
ni (n ‚àí5 + ŒΩ).
It is important, however, to note that the true means ¬µi are not mutually
independent, even though they are not correlated (diagonal N) a posteriori;
this is because the joint density (multivariate-t) cannot be written as a
product of the resulting marginal densities, which are all univariate-t. It
also follows that the distribution of any linear contrast ¬µi‚àí¬µi‚Ä≤ is univariate-
t, with mean yi ‚àíyi‚Ä≤, degrees of freedom equal to (n ‚àí3 + ŒΩ) and posterior
variance
V ar (¬µi ‚àí¬µi‚Ä≤|y) = c2 (n ‚àí3 + ŒΩ)
(n ‚àí5 + ŒΩ)

 1
ni
+ 1
ni‚Ä≤

.
Another consequence is that the joint distribution of, for example, pairs of
diÔ¨Äerences between population means is also bivariate-t, with parameters
derived directly from the joint distribution having density (5.128).
‚ñ†
Example 5.12
Digression on multiple comparisons
Consider now the problem of carrying out multiple comparisons between
means (Milliken and Johnson, 1992) in the setting of Example 5.11. From
a frequentist point of view, the chance of Ô¨Ånding a diÔ¨Äerence that appears

5.6 Features of Posterior Distributions
271
to be ‚ÄúsigniÔ¨Åcant‚Äù increases with the number of comparisons made, even
if such diÔ¨Äerences are truly null. For example, these authors note that if
an experiment involves 100 ‚Äúindependent‚Äù tests at a signiÔ¨Åcance level of
Œ± = 0.05, one should expect (over repeated sampling) (0.05) 100 = 5 such
tests to be signiÔ¨Åcant, just by chance. In order to adjust for the number
of comparisons made, diÔ¨Äerent error rates must be computed. For exam-
ple, the ‚Äúexperimentwise error‚Äù rate is the probability of making at least
one error in an experiment in which there are no real diÔ¨Äerences between
means. A battery of statistical tests has been developed for multiple com-
parisons, where the experimentwise error rate is Ô¨Åxed at some level, e.g.,
5%, although it is far from clear when one test ought to be preferred over
another. Further, some of these tests have not been extended to accommo-
date unequal sample sizes or the presence of covariates in the model. From
a Bayesian point of view, all this is a nonissue. First, ‚Äúhypothesis testing‚Äù
is approached in a completely diÔ¨Äerent manner in Bayesian statistics, as
it will be seen later. Second, the joint posterior distribution keeps track
of any existing dependencies between parameters. Third, the probability
calculus applied to the joint posterior distribution adjusts probability vol-
umes automatically, that is, uncertainty about whatever is regarded as a
nuisance in a multiple comparison is accounted for via integration. Joint
inferences are then done from a distribution of appropriate dimension, and
probability statements are either marginal or joint, depending on the objec-
tive of the analysis. Suppose that one is interested in inferring all possible
diÔ¨Äerences between means, and that if one of such diÔ¨Äerences does not
exceed a certain threshold t1‚àíŒ±, e.g., the 1 ‚àíŒ± quantile of the posterior
distribution of the diÔ¨Äerence, then the ‚Äúhypothesis‚Äù that the diÔ¨Äerence is
null is accepted. With the setting as in the preceding example, all possible
diÔ¨Äerences between pairs of means can be written in matrix notation as,
Ô£Æ
Ô£∞
¬µ12
¬µ13
¬µ23
Ô£π
Ô£ª=
Ô£Æ
Ô£∞
1
‚àí1
0
1
0
‚àí1
0
1
‚àí1
Ô£π
Ô£ª
Ô£Æ
Ô£∞
¬µ1
¬µ2
¬µ3
Ô£π
Ô£ª.
The third comparison is redundant because it can be obtained from the
diÔ¨Äerence between the second and third comparisons. Hence, it suÔ¨Éces to
work with the full-rank subset of comparisons
‚Üê‚Üí
¬µ =
 ¬µ12
¬µ13

=
 1
‚àí1
0
1
0
‚àí1
 Ô£Æ
Ô£∞
¬µ1
¬µ2
¬µ3
Ô£π
Ô£ª= L¬µ.
Since the joint posterior distribution of ¬µ1, ¬µ2, and ¬µ3 is trivariate-t, it
follows that the posterior distribution of ‚Üê‚Üí
¬µ is bivariate-t (by virtue of
being a linear combination), and that the marginal distributions of ¬µ12 and
¬µ13 are univariate-t. All these distributions have parameters that can be
deduced from results in the preceding example. For instance, the posterior

272
5. An Introduction to Bayesian Inference
covariance between ¬µ12 and ¬µ13 is
Cov (¬µ12, ¬µ13|y)
= V ar (¬µ1|y) ‚àíCov (¬µ1, ¬µ3|y) ‚àíCov (¬µ1, ¬µ2|y) + Cov (¬µ2, ¬µ3|y)
= V ar (¬µ1|y)
since all posterior covariances between means are null, as N is a diagonal
matrix. The posterior probability that ¬µ12 > t1‚àíŒ± is
p12 =
‚àû

t1‚àíŒ±
p (¬µ12|y) d¬µ12
=
‚àû

t1‚àíŒ±
Ô£±
Ô£¥
Ô£≤
Ô£¥
Ô£≥
1 +
[¬µ12 ‚àí(y1 ‚àíy2)]2 
1
n1 +
1
n2
‚àí1
(n ‚àí3 + ŒΩ) c2
Ô£º
Ô£¥
Ô£Ω
Ô£¥
Ô£æ
‚àín‚àí3+ŒΩ+1
2
d¬µ12
‚àû

‚àí‚àû
Ô£±
Ô£¥
Ô£≤
Ô£¥
Ô£≥
1 +
[¬µ12 ‚àí(y1 ‚àíy2)]2 
1
n1 +
1
n2
‚àí1
(n ‚àí3 + ŒΩ) c2
Ô£º
Ô£¥
Ô£Ω
Ô£¥
Ô£æ
‚àín‚àí3+ŒΩ+1
2
d¬µ12
which can be computed readily from tabled values of the standardized
t distribution or from some available computer routine. Using a similar
procedure one can calculate p13, the posterior probability that ¬µ13 > t1‚àíŒ±.
Next, we consider the joint distribution of ¬µ12 and ¬µ13. This is a bivariate-t
process with density
p (¬µ12, ¬µ13|y) ‚àù

1 +
‚Üê‚Üí
¬µ ‚àíLy
‚Ä≤ LNL‚Ä≤ ‚Üê‚Üí
¬µ ‚àíLy

(n ‚àí3 + ŒΩ) c2
‚àín‚àí3+ŒΩ+2
2
.
The posterior probability that both ¬µ12 and ¬µ13 exceed the threshold t1‚àíŒ±
is given by
p12,13 =
‚àû

t1‚àíŒ±
‚àû

t1‚àíŒ±

1 + (‚Üê
‚Üí
¬µ ‚àíLy)
‚Ä≤LNL‚Ä≤(‚Üê
‚Üí
¬µ ‚àíLy)
(n‚àí3+ŒΩ)c2
‚àín‚àí3+ŒΩ+2
2
d¬µ12 d¬µ13
‚àû

‚àí‚àû
‚àû

‚àí‚àû

1 + (‚Üê
‚Üí
¬µ ‚àíLy)
‚Ä≤LNL‚Ä≤(‚Üê
‚Üí
¬µ ‚àíLy)
(n‚àí3+ŒΩ)c2
‚àín‚àí3+ŒΩ+2
2
d¬µ12 d¬µ13
.
Then the probability that at least one of the comparisons will exceed the
threshold, posterior to the data, is
Pr (¬µ12 > t1‚àíŒ± ‚à™¬µ13 > t1‚àíŒ±) = p12 + p13 ‚àíp12,13
(5.130)
with p12 calculated as before, p13 computed with a similar expression, and
p12,13 from the expression preceding (5.130).

5.6 Features of Posterior Distributions
273
There is an important diÔ¨Äerence with the frequentist approach. In the
Bayesian analysis, the calculations involve posterior probabilities of events.
In the classical methodology of multiple comparisons, the probabilities in-
volve sampling distributions of some estimates, with calculations conducted
as if the null hypothesis were true. In the Bayesian approach, there is no
such thing as a ‚Äúnull or alternative‚Äù hypothesis, except in some metaphoric
sense. The calculations indicated above would be carried out for two models
representing diÔ¨Äerent states of nature. Subsequently, the posterior odds ra-
tio, as in Example 5.2, would quantify the strength of the evidence in favor
of one of the two models. It is seen that the Bayesian probability calculus
enables posing and solving the problem in a conceptually straightforward
manner.
‚ñ†
Example 5.13
Joint modes in a Gaussian linear model
Let the linear model be
y = XŒ≤ + Zu + e,
(5.131)
where Œ≤, u, and e follow the independent distributions:
Œ≤|Œ≤0, œÉ2
Œ≤
‚àº
N

1Œ≤0, IbœÉ2
Œ≤

,
u|u0, œÉ2
u
‚àº
N

1u0, ItœÉ2
u

,
e
‚àº
N

0, InœÉ2
e

,
with the identity matrices having orders b, t, and n as indicated. The prior
distribution of Œ≤ postulates that all elements of this vector are i.i.d., with
a common mean and variance. A similar assumption is made about the
elements of u. Above, Œ≤0 and u0 are unknown scalar parameters and œÉ2
Œ≤, œÉ2
u,
and œÉ2
e are unknown variance components. We shall adopt the independent
prior densities
p (Œ≤0)
=
1
Œ≤0,max ‚àíŒ≤0,min
,
p (u0)
=
1
u0,max ‚àíu0,min
,
where (min, max) refer to (upper, lower) boundaries for the appropriate
parameters. The variance components are assigned independent scaled in-
verted chi-square distributions, with densities
p

œÉ2
Œ≤|ŒΩŒ≤, s2
Œ≤

‚àù

œÉ2
Œ≤
‚àí
 ŒΩŒ≤ +2
2

exp

‚àí
ŒΩŒ≤s2
Œ≤
2œÉ2
Œ≤

,
p

œÉ2
u|ŒΩu, s2
u

‚àù

œÉ2
u
‚àí(
ŒΩu+2
2
) exp

‚àíŒΩus2
u
2œÉ2u

,
p

œÉ2
e|ŒΩe, s2
e

‚àù

œÉ2
e
‚àí(
ŒΩe+2
2 ) exp

‚àíŒΩes2
e
2œÉ2e

,

274
5. An Introduction to Bayesian Inference
where ŒΩŒ≤, ŒΩu, and ŒΩe are known ‚Äúdegree of belief‚Äù parameters, and s2
Œ≤, s2
u,
and s2
e are some prior values of the dispersion components. An alternative
representation of model (5.131) is
y = XŒ≤ + Zu + e
= (X1) Œ≤0 + XŒæŒ≤ + (Z1) u0 + ZŒæu + e
= mŒ≤0 + nu0 + XŒæŒ≤ + ZŒæu + e,
(5.132)
where m = X1 and n = Z1 are incidence vectors of appropriate order and
the new variables have distributions
ŒæŒ≤|œÉ2
Œ≤
‚àº
N

0, IbœÉ2
Œ≤

,
Œæu|œÉ2
u
‚àº
N

0, ItœÉ2
u

.
Hence, the entire parameter vector is then
Œ∏ =

Œ≤0, u0, Œæ‚Ä≤
Œ≤, Œæ‚Ä≤
u, œÉ2
Œ≤, œÉ2
u, œÉ2
e
‚Ä≤ .
Suppose we wish to Ô¨Ånd the joint mode of the posterior distribution of Œ∏,
having density
p

Œ∏|Œ≤0,max, Œ≤0,min, u0,max, u0,min, ŒΩŒ≤, ŒΩu, ŒΩe, s2
Œ≤, s2
u, s2
e, y

‚àùp

y|Œ≤0, u0, ŒæŒ≤, Œæu, œÉ2
e

p

Œ≤0|Œ≤0,max, Œ≤0,min

p (u0|u0,max, u0,min)
√óp

ŒæŒ≤|œÉ2
Œ≤

p

Œæu|œÉ2
u

√óp

œÉ2
Œ≤|ŒΩŒ≤, s2
Œ≤

p

œÉ2
u|ŒΩu, s2
u

p

œÉ2
e|ŒΩe, s2
e

.
(5.133)
In order to Ô¨Ånd a stationary point, the following Ô¨Årst derivatives are needed,
letting L (Œ∏|y) be the logarithm of the joint posterior density.
Gradient for Œ≤0:
‚àÇL (Œ∏|y)
‚àÇŒ≤0
=
‚àÇ
‚àÇŒ≤0

log

p

y|Œ≤0, u0, ŒæŒ≤, Œæu, œÉ2
e

+ ‚àÇ
‚àÇŒ≤0

log

p

Œ≤0|Œ≤0,max, Œ≤0,min

= ‚àí1
2œÉ2e
‚àÇ
‚àÇŒ≤0
(e‚Ä≤e)
= m‚Ä≤ 
y ‚àímŒ≤0 ‚àínu0 ‚àíXŒæŒ≤ ‚àíZŒæu

œÉ2e
where
e = y ‚àímŒ≤0 ‚àínu0 ‚àíXŒæŒ≤ ‚àíZŒæu.

5.6 Features of Posterior Distributions
275
Gradient for u0:
‚àÇL (Œ∏|y)
‚àÇu0
=
‚àÇ
‚àÇu0

log

p

y|Œ≤0, u0, ŒæŒ≤, Œæu, œÉ2
e

+ ‚àÇ
‚àÇu0
{log [p (u0|u0,max, u0,min)]}
= ‚àí1
2œÉ2e
‚àÇ
‚àÇu0
(e‚Ä≤e)
= n‚Ä≤ 
y ‚àímŒ≤0 ‚àínu0 ‚àíXŒæŒ≤ ‚àíZŒæu

œÉ2e
Gradient for ŒæŒ≤:
‚àÇL (Œ∏|y)
‚àÇŒæŒ≤
=
‚àÇ
‚àÇŒæŒ≤

log

p

y|Œ≤0, u0, ŒæŒ≤, Œæu, œÉ2
e

+
‚àÇ
‚àÇŒæŒ≤

log

p

ŒæŒ≤|œÉ2
Œ≤

= ‚àí1
2œÉ2e
‚àÇ
‚àÇŒæŒ≤
(e‚Ä≤e) ‚àí
‚àÇ
‚àÇŒæŒ≤

Œæ‚Ä≤
Œ≤ŒæŒ≤
2œÉ2
Œ≤

= X‚Ä≤ 
y ‚àímŒ≤0 ‚àínu0 ‚àíXŒæŒ≤ ‚àíZŒæu

œÉ2e
‚àíŒæŒ≤
œÉ2
Œ≤
.
Gradient for Œæu:
‚àÇL (Œ∏|y)
‚àÇŒæu
=
‚àÇ
‚àÇŒæu

log

p

y|Œ≤0, u0, ŒæŒ≤, Œæu, œÉ2
e

+ ‚àÇ
‚àÇŒæu

log p

Œæu|œÉ2
u

=
‚àí1
2œÉ2e
‚àÇ
‚àÇŒæu
(e‚Ä≤e) ‚àí
‚àÇ
‚àÇŒæu

Œæ‚Ä≤
uŒæu
2œÉ2u

=
Z‚Ä≤ 
y ‚àímŒ≤0 ‚àínu0 ‚àíXŒæŒ≤ ‚àíZŒæu

œÉ2e
‚àíŒæu
œÉ2u
.

276
5. An Introduction to Bayesian Inference
Gradient for œÉ2
e:
‚àÇL (Œ∏|y)
‚àÇœÉ2e
=
‚àÇ
‚àÇœÉ2e

log

p

y|Œ≤0, u0, ŒæŒ≤, Œæu, œÉ2
e

+ ‚àÇ
‚àÇœÉ2e

log

p

œÉ2
e|ŒΩe, s2
e

= ‚àín + ŒΩe + 2
2œÉ2e
+ e‚Ä≤e + ŒΩes2
e
2œÉ4e
.
Gradient for œÉ2
Œ≤:
‚àÇL (Œ∏|y)
‚àÇœÉ2
Œ≤
=
‚àÇ
‚àÇœÉ2
Œ≤

log

p

ŒæŒ≤|œÉ2
Œ≤

+ log

p

œÉ2
Œ≤|ŒΩŒ≤, s2
Œ≤

= ‚àíb + ŒΩŒ≤ + 2
2œÉ2
Œ≤
+
Œæ‚Ä≤
Œ≤ŒæŒ≤ + ŒΩŒ≤s2
Œ≤
2œÉ4
Œ≤
.
Gradient for œÉ2
u:
‚àÇL (Œ∏|y)
‚àÇœÉ2u
=
‚àÇ
‚àÇœÉ2u

log

p

Œæu|œÉ2
u

+ log

p

œÉ2
u|ŒΩu, s2
u

= ‚àít + ŒΩu + 2
2œÉ2u
+ Œæ‚Ä≤
uŒæu + ŒΩus2
u
2œÉ4u
.
Setting all Ô¨Årst derivatives simultaneously to 0 gives a system of equa-
tions that is not explicit in the solutions. A rearrangement of the system
gives, after algebra, the following functional iteration (the superscript i in
parentheses indicates round number):
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
m‚Ä≤m
m‚Ä≤n
m‚Ä≤X
m‚Ä≤Z
n‚Ä≤m
n‚Ä≤n
n‚Ä≤X
n‚Ä≤Z
X‚Ä≤m
X‚Ä≤n
X‚Ä≤X + I

œÉ2
e
œÉ2
Œ≤
(i)
X‚Ä≤X
Z‚Ä≤m
Z‚Ä≤n
Z‚Ä≤X
Z‚Ä≤Z+

œÉ2
e
œÉ2
u
(i)
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
Ô£Æ
Ô£ØÔ£ØÔ£∞
Œ≤0
u0
ŒæŒ≤
Œæu
Ô£π
Ô£∫Ô£∫Ô£ª
(i)
=
Ô£Æ
Ô£ØÔ£ØÔ£∞
m‚Ä≤y
n‚Ä≤y
X‚Ä≤y
Z‚Ä≤y
Ô£π
Ô£∫Ô£∫Ô£ª
(5.134)
œÉ2(i+1)
e
= (e‚Ä≤e)(i) + ŒΩes2
e
n + ŒΩe + 2
,
œÉ2(i+1)
Œ≤
=

Œæ‚Ä≤
Œ≤ŒæŒ≤
(i) + ŒΩŒ≤s2
Œ≤
b + ŒΩŒ≤ + 2
,
and:
œÉ2(i+1)
u
=

Œæ‚Ä≤
uŒæu
(i) + ŒΩus2
u
t + ŒΩu + 2
.

5.6 Features of Posterior Distributions
277
The functional iteration starts by specifying starting values for œÉ2
e, œÉ2
Œ≤, and
œÉ2
u, and then solving (5.134), to obtain values for Œ≤0, u0, ŒæŒ≤, and Œæu. The
variance components are then updated with the three expressions below
(5.134). If the iteration converges, it will produce a mode of the joint pos-
terior distribution of all parameters. The iteration must be tailored such
that values of Œ≤0, u0 stay within the boundaries stated in the probability
model.
‚ñ†
Example 5.14
Marginal modes in a Gaussian linear model
The setting is as in Example 5.13, but consider now Ô¨Ånding the modal
vector of the lower-dimensional distribution

Œ≤0, u0, ŒæŒ≤, Œæu|Œ≤0,max, Œ≤0,min, u0,max, u0,min, ŒΩŒ≤, ŒΩu, ŒΩe, s2
Œ≤, s2
u, s2
e, y

.
(5.135)
The integral of the joint posterior density (5.133) with respect to all three
variance components is needed to obtain the density of the desired lower-
dimensional distribution. Omitting the dependency on hyperparameters in
the notation, this integral is
p

Œ≤0, u0, ŒæŒ≤, Œæu|y

‚àùp (Œ≤0) p (u0)
‚àû

0
p

y|Œ≤0, u0, ŒæŒ≤, Œæu, œÉ2
e

p

œÉ2
e

dœÉ2
e
√ó
‚àû

0
p

ŒæŒ≤|œÉ2
Œ≤

p

œÉ2
Œ≤

dœÉ2
Œ≤
‚àû

0
p

Œæu|œÉ2
u

p

œÉ2
u

dœÉ2
u.
(5.136)
Each of the integrals is in a scaled inverted chi-squared form, and can
be evaluated explicitly. We now evaluate each of the integrals in (5.136).
Retaining only terms that vary with Œ≤0, u0, ŒæŒ≤, and Œæu, we Ô¨Årst get
‚àû

0
p

y|Œ≤0, u0, ŒæŒ≤, Œæu, œÉ2
e

p

œÉ2
e

dœÉ2
e
‚àù
‚àû

0

œÉ2
e
‚àí(
n+ŒΩe+2
2
) exp

‚àíe‚Ä≤e + ŒΩes2
e
2œÉ2e

dœÉ2
e
‚àù

e‚Ä≤e + ŒΩes2
e
‚àín+ŒΩe
2
‚àù

1 + e‚Ä≤e
ŒΩes2e
‚àín+ŒΩe
2
,
(5.137)
recalling that
e =

y ‚àímŒ≤0 ‚àínu0 ‚àíXŒæŒ≤ ‚àíZŒæu

.

278
5. An Introduction to Bayesian Inference
Similarly,
‚àû

0
p

ŒæŒ≤|œÉ2
Œ≤

p

œÉ2
Œ≤

dœÉ2
Œ≤
‚àù
‚àû

0

œÉ2
Œ≤
‚àí
 b+ŒΩŒ≤ +2
2

exp

‚àí
Œæ‚Ä≤
Œ≤ŒæŒ≤ + ŒΩŒ≤s2
Œ≤
2œÉ2
Œ≤

dœÉ2
Œ≤
‚àù

1 + Œæ‚Ä≤
Œ≤ŒæŒ≤
ŒΩŒ≤s2
Œ≤
‚àí
b+ŒΩŒ≤
2
.
(5.138)
Further, using a similar algebra,
‚àû

0
p

Œæu|œÉ2
u

p

œÉ2
u

dœÉ2
u ‚àù

1 + Œæ‚Ä≤
uŒæu
ŒΩus2u
‚àít+ŒΩu
2
.
(5.139)
Each of the expressions in (5.137)-(5.139) is the kernel of a multivariate-t
distribution. Now, collecting the integrals and using these in (5.136) we
obtain, as posterior density, after suitable normalization,
p

Œ≤0, u0, ŒæŒ≤, Œæu|y

‚àù
37
i=1

1 + Œª‚Ä≤
iŒªi
ci
‚àídi
2
37
i=1
‚àû

‚àí‚àû

1 + Œª‚Ä≤
iŒªi
ci
‚àídi
2
dŒªi
,
(5.140)
where
Œª1
=
y ‚àímŒ≤0 ‚àínu0 ‚àíXŒæŒ≤ ‚àíZŒæu,
c1 = ŒΩes2
e,
d1 = n + ŒΩe,
Œª2
=
ŒæŒ≤,
c2 = ŒΩŒ≤s2
Œ≤,
d2 = b + ŒΩŒ≤,
Œª3
=
Œæu,
c3 = ŒΩus2
u,
d3 = t + ŒΩu.
If the Œª‚Ä≤
is were the random variables of interest, the distribution with den-
sity (5.140) would be a truncated (in the intervals Œ≤0,max ‚àíŒ≤0,min and
u0,max‚àíu0,min) poly-t or product multivariate-t distribution (Box and Tiao,
1973); its properties are presented in Dickey (1968). For example, this dis-
tribution is known to be asymmetric and multimodal. However, note that
Œª1 involves all four random terms of interest, so the process of interest
is not poly-t. Consider now Ô¨Ånding the mode of the marginal distribution
of concern. DiÔ¨Äerentiation of the logarithm of (5.140), L

Œ≤0, u0, ŒæŒ≤, Œæu|y

,
with respect to each of the four terms, yields,
‚àÇL

Œ≤0, u0, ŒæŒ≤, Œæu|y

‚àÇŒ≤0
=
d1m‚Ä≤e

c1 + Œª‚Ä≤
1Œª1
,

5.6 Features of Posterior Distributions
279
‚àÇL

Œ≤0, u0, ŒæŒ≤, Œæu|y

‚àÇu0
=
d1n‚Ä≤e

c1 + Œª‚Ä≤
1Œª1
,
‚àÇL

Œ≤0, u0, ŒæŒ≤, Œæu|y

‚àÇŒæŒ≤
=
d1X‚Ä≤e

c1 + Œª‚Ä≤
1Œª1
 ‚àí
d2ŒæŒ≤

c2 + Œª‚Ä≤
2Œª2
,
and
‚àÇL

Œ≤0, u0, ŒæŒ≤, Œæu|y

‚àÇŒæu
=
d1Z‚Ä≤e

c1 + Œª‚Ä≤
1Œª1
 ‚àí
d3Œæu

c3 + Œª‚Ä≤
3Œª3
.
Setting all diÔ¨Äerentials simultaneously to zero gives
m‚Ä≤mŒ≤0 + m‚Ä≤nu0 + m‚Ä≤XŒæŒ≤ + m‚Ä≤ZŒæu = m‚Ä≤y,
n‚Ä≤mŒ≤0 + n‚Ä≤nu0 + n‚Ä≤XŒæŒ≤ + n‚Ä≤ZŒæu = n‚Ä≤y,
X‚Ä≤mŒ≤0 + X‚Ä≤nu0 +

X‚Ä≤X + Iw1
w2

ŒæŒ≤ + X‚Ä≤ZŒæu = X‚Ä≤y,
Z‚Ä≤mŒ≤0 + Z‚Ä≤nu0 + Z‚Ä≤XŒæŒ≤ +

Z‚Ä≤Z + Iw1
w3

Œæu = Z‚Ä≤y,
where
w1 = c1 + Œª‚Ä≤
1Œª1
d1
= e‚Ä≤e + ŒΩes2
e
n + ŒΩe
,
w2 = c2 + Œª‚Ä≤
2Œª2
d2
=
Œæ‚Ä≤
Œ≤ŒæŒ≤ + ŒΩŒ≤s2
Œ≤
b + ŒΩŒ≤
,
and
w3 = c3 + Œª‚Ä≤
3Œª3
d3
= Œæ‚Ä≤
uŒæu + ŒΩus2
u
t + ŒΩu
,
can be construed as ‚Äúestimates‚Äù of variance components in a Gaussian
linear mixed eÔ¨Äects model (Henderson, 1973; Searle et al., 1992). Clearly,
the equations that need to be solved simultaneously are not explicit in the
solutions. As in Example 5.13, a functional iteration can be constructed,
which can be expressed in matrix form as
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£∞
m‚Ä≤m
m‚Ä≤n
m‚Ä≤X
m‚Ä≤Z
n‚Ä≤m
n‚Ä≤n
n‚Ä≤X
n‚Ä≤Z
X‚Ä≤m
X‚Ä≤n
X‚Ä≤X + I w(i)
1
w(i)
2
X‚Ä≤X
Z‚Ä≤m
Z‚Ä≤n
Z‚Ä≤X
Z‚Ä≤Z + I w(i)
1
w(i)
3
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
Ô£Æ
Ô£ØÔ£ØÔ£∞
Œ≤0
u0
ŒæŒ≤
Œæu
Ô£π
Ô£∫Ô£∫Ô£ª
(i+1)
=
Ô£Æ
Ô£ØÔ£ØÔ£∞
m‚Ä≤y
n‚Ä≤y
X‚Ä≤y
Z‚Ä≤y
Ô£π
Ô£∫Ô£∫Ô£ª,
(5.141)
where the w‚Ä≤s depend on the unknowns, and change values from iterate to
iterate. The algorithm starts by assigning starting values to the pseudo-
variances (perhaps the means of the prior distributions of the variance
components), and then calculating the Ô¨Årst set of w‚Ä≤s. These are used to
obtain revised values for the location eÔ¨Äects, and so on. The properties

280
5. An Introduction to Bayesian Inference
of the algorithm are unknown. If it converges, it will locate one of the
possibly many stationary points of the poly‚àít distributions. Note that
(5.141) are structurally similar to (5.134). However, the algorithm does
not involve estimating equations for the variance components, as these
have been integrated out of the joint posterior distribution. While it is
extremely diÔ¨Écult to show that the modal vector of the joint distribution of
all parameters diÔ¨Äers from the mode of the lower-dimensional distribution
with density (5.140), this example illustrates at least that the modes have
diÔ¨Äerent forms in the two cases.
‚ñ†
5.6.4
Posterior Mean Vector and Covariance Matrix
Mean Vector
The mean (mean vector) and variance (covariance matrix) of posterior
distributions have been identiÔ¨Åed in several of the highly stylized examples
discussed before. It is convenient, however, to recall the pervasive presence
of unknown nuisance parameters, so the following notation is helpful in this
respect. The mean of the posterior distribution of a vector Œ∏1, when the
statistical model posits the presence of a nuisance parameter Œ∏2, can be
expressed as
E (Œ∏1|y) = EŒ∏2|y [E (Œ∏1|Œ∏2, y)] .
(5.142)
The inner expectation gives the posterior mean value of the parameter of in-
terest at speciÔ¨Åed values of the nuisance parameters, as if these were known.
The outer expectation averages over the marginal posterior distribution of
the nuisances, thus incorporating whatever uncertainty exists about their
values. Representation (5.142) is also useful in connection with sampling
methods. As seen in Chapter 1, the process of Rao‚ÄìBlackwellization en-
ables one to obtain a more precise Monte Carlo estimator of E (Œ∏1|y) by
drawing m samples from the posterior distribution of the nuisance param-
eter (whenever this is feasible), and then estimating the desired posterior
mean as
5E (Œ∏1|y) = 1
m
m

i=1
E

Œ∏1|Œ∏(i)
2 , y

,
where Œ∏(1)
2 , Œ∏(2)
2 , . . . , Œ∏(m)
2
are draws from [Œ∏2|y] . The preceding estimator
is at least as precise as
55E (Œ∏1|y) = 1
m
m

i=1
Œ∏(i)
1 ,
where Œ∏1 is a direct draw from the posterior distribution [Œ∏1|y] .

5.6 Features of Posterior Distributions
281
Optimality of the Mean Vector
Use of the mean as an ‚Äúestimator‚Äù of the parameter has a decision-theoretic
justiÔ¨Åcation. Suppose the loss function is quadratic, with the form
L

5Œ∏1, Œ∏1

=

5Œ∏1 ‚àíŒ∏1
‚Ä≤
Q

5Œ∏1 ‚àíŒ∏1

,
where Q is a known symmetric, positive-deÔ¨Ånite matrix. The ‚Äúerror‚Äù of
estimation is 5Œ∏1 ‚àíŒ∏1, and the form adopted above for L

5Œ∏1, Œ∏1

indi-
cates that the penalty accruing from estimating the parameter with error
is proportional to the square of such error. Using well-known formulas for
expected values of quadratic forms, the expected posterior loss is
EŒ∏1|y

L

5Œ∏1, Œ∏1

=

5Œ∏1 ‚àíE (Œ∏1|y)
‚Ä≤
Q

5Œ∏1 ‚àíE (Œ∏1|y)

+tr [Q V ar (Œ∏1|y)] .
(5.143)
The second term does not involve 5Œ∏1, and the Ô¨Årst term is nonnegative.
Hence, the expected loss is minimized by taking as ‚Äúoptimal estimator‚Äù:
5Œ∏1 = E (Œ∏1|y)
(5.144)
which is the posterior mean.
Relationship Between the Posterior Mean and the ‚ÄúBest‚Äù Predictor
Consider adopting a frequentist point of view, that is, let [Œ∏1, y|Œ∏2] be a
joint distribution having a long-run frequency interpretation, where Œ∏2 is
a known parameter (this is a very strong assumption; in practice, such a
parameter is unknown, most often). In this setting, Œ∏1 is an unobservable
random variable, or ‚Äúrandom eÔ¨Äect‚Äù in the usual frequentist sense. Then,
using a logic similar to the preceding one, we will show that the frequentist
expected loss
EŒ∏1,y|Œ∏2

L

5Œ∏1, Œ∏1

= EŒ∏1,y|Œ∏2

5Œ∏1 ‚àíŒ∏1
‚Ä≤
Q

5Œ∏1 ‚àíŒ∏1

(5.145)
is minimized by 5Œ∏1 = E (Œ∏1|y, Œ∏2), among all possible predictors. In (5.145),
expectations are taken with respect to the joint distribution [Œ∏1, y|Œ∏2] , with
the nuisance parameter Œ∏2 treated as a known constant. Using the theorem

282
5. An Introduction to Bayesian Inference
of double expectation
EŒ∏1,y|Œ∏2

L

5Œ∏1, Œ∏1

= Ey|Œ∏2
9
EŒ∏1|y,Œ∏2

L

5Œ∏1, Œ∏1
:
= Ey|Œ∏2

5Œ∏1 ‚àíE (Œ∏1|y, Œ∏2)
‚Ä≤
Q

5Œ∏1 ‚àíE (Œ∏1|y, Œ∏2)

+ [Q V ar (Œ∏1|y, Œ∏2)]}
= Ey|Œ∏2

5Œ∏1 ‚àíE (Œ∏1|y, Œ∏2)
‚Ä≤
Q

5Œ∏1 ‚àíE (Œ∏1|y, Œ∏2)

+Ey|Œ∏2tr [Q V ar (Œ∏1|y, Œ∏2)] .
(5.146)
The second term in (5.146) does not involve 5Œ∏1; thus, minimizing expected
(frequentist) quadratic loss is achieved by minimizing the Ô¨Årst term. Now,
the latter is a weighted average, where the weight function is the density
of the distribution [y|Œ∏2] . It turns out that, if one minimizes,

5Œ∏1 ‚àíE (Œ∏1|y, Œ∏2)
‚Ä≤
Q

5Œ∏1 ‚àíE (Œ∏1|y, Œ∏2)

(5.147)
for each realization of y, this will also minimize
Ey|Œ∏2

5Œ∏1 ‚àíE (Œ∏1|y, Œ∏2)
‚Ä≤
Q

5Œ∏1 ‚àíE (Œ∏1|y, Œ∏2)
%
and, hence, (5.146). Using the same argument as in the preceding sec-
tion, it follows that the minimizer for each y is the conditional expectation
E (Œ∏1|y, Œ∏2) which, superÔ¨Åcially, ‚Äúlooks similar‚Äù to our Bayesian E (Œ∏1|y) .
The similarity does not stand scrutiny, however, as E (Œ∏1|y) incorporates
the uncertainty about the nuisance parameter Œ∏2, this is clearly not the
case for 5Œ∏1 = E (Œ∏1|y, Œ∏2) .
The ‚Äúbest predictor‚Äù is, thus, the conditional mean of the unobservable
random eÔ¨Äects given the observations, but assuming known parameters of
the joint distribution of the data and of the random eÔ¨Äects (Henderson,
1973). This statistic was introduced in Example 1.22 of Chapter 1. In ad-
dition to minimizing the expected squared error of prediction (5.145), the
conditional mean has some frequentist properties of interest. For example,
it is ‚Äúunbiased‚Äù, in some sense. This can be veriÔ¨Åed by taking the expec-
tations of the conditional mean over the distribution of the observations
Ey|Œ∏2

5Œ∏1

= Ey|Œ∏2 [E (Œ∏1|y, Œ∏2)] = E (Œ∏1) ,
(5.148)
recalling that the parameter Œ∏2 is not a random variable in the frequentist
sense, so the dependence on it can be omitted in the notation. This is the
deÔ¨Ånition of unbiasedness in the frequentist context of prediction of random
eÔ¨Äects. Now, since the conditional mean is an unbiased predictor, it follows
that it minimizes prediction error variance as well. This can be seen simply

5.6 Features of Posterior Distributions
283
by putting Q = I in (5.145), and by considering a scalar element of Œ∏1;
then, the expected loss is the prediction error variance. A third property is
Cov

5Œ∏1, Œ∏‚Ä≤
1

= V ar

5Œ∏1

.
(5.149)
This results from the fact that
Cov

5Œ∏1, Œ∏‚Ä≤
1

= Ey,Œ∏1|Œ∏2

5Œ∏1Œ∏‚Ä≤
1

‚àíEy,Œ∏1|Œ∏2

5Œ∏1

E

Œ∏‚Ä≤
1

= Ey|Œ∏2

EŒ∏1|y,Œ∏2

5Œ∏1Œ∏‚Ä≤
1

‚àíE

5Œ∏1

E

5Œ∏
‚Ä≤
1

= Ey|Œ∏2

5Œ∏1EŒ∏1|y,Œ∏2

Œ∏‚Ä≤
1

‚àíE

5Œ∏1

E

5Œ∏
‚Ä≤
1

= E

5Œ∏15Œ∏
‚Ä≤
1

‚àíE

5Œ∏1

E

5Œ∏
‚Ä≤
1

= V ar

5Œ∏1

.
An immediate consequence is that the prediction error 5Œ∏1 ‚àíŒ∏1 and the
predictor 5Œ∏1 have a null covariance. Additional consequences of (5.149), for
any scalar element of Œ∏1, say Œ∏1i, are that
Corr

5Œ∏1i, Œ∏1i

=
@
A
A
BV ar

5Œ∏1i

V ar (Œ∏1i) ,
and
V ar

5Œ∏1i ‚àíŒ∏1i

= V ar (Œ∏1i)

1 ‚àíCorr2 
5Œ∏1i, Œ∏1i

.
In general,
V ar

5Œ∏1 ‚àíŒ∏1

= V ar (Œ∏1) ‚àíV ar

5Œ∏1

.
(5.150)
Example 5.15
Best prediction of quadratic genetic merit
Suppose we are in the setting of Example 5.3, and that the function
T = k0a + k1a2
is to be predicted. As before, a is the additive genetic value of a certain
farm animal for some trait, and k0 and k1 are known constants. This func-
tion is what is called ‚Äúquadratic merit‚Äù in animal breeding (Wilton et al.,
1968), and it postulates that the breeding worth of a potential parent is
proportional to the square of its additive genetic value. The expectation of
T is
E (T)
=
k0E (a) + k1E2 (a) + k1va
=
k1va
since it is assumed that E (a) = 0. As in Example 5.3 suppose that ¬µ, va,
and ve are known. The frequentist conditional distribution of a, given the

284
5. An Introduction to Bayesian Inference
phenotypic value y, coincides in this case with the Bayesian conditional
posterior distribution of a. As shown in Example 5.3, this distribution is
a|y, ¬µ, va, ve ‚àºN

h2 (y ‚àí¬µ) , va

1 ‚àíh2
,
where h2 = va/ (va + ve). From results in the preceding section, the best
predictor of T is
5T
=
E (T|y, ¬µ, va, ve) = k0E (a|y, ¬µ, va, ve) + k1E

a2|y, ¬µ, va, ve

=
k0h2 (y ‚àí¬µ) + k1

h2 (y ‚àí¬µ)
2 + k1va

1 ‚àíh2
,
and this can be evaluated readily. It is easy to verify that the predictor is
unbiased, as it has the same expectation as the predictand. Taking expec-
tations over the distribution of y:
EyE (T|y, ¬µ, va, ve) = k0E (a) + k1E

E

a2|y, ¬µ, va, ve

= k0E (a) + k1E

a2
= k1va.
Using (5.150), the prediction error variance is
V ar

5T ‚àíT

= V ar (T) ‚àíV ar

5T

.
In the preceding expression
V ar (T) = V ar

k0a + k1a2
= k2
0va + k2
1 V ar(a2) + 2k0k1 Cov(a, a2)
= k2
0va + 2k2
1v2
a
with this being so, because if a ‚àºN (0, va) , then V ar

a2
= 2v2
a and
Cov(a, a2) = 0 (Searle, 1971). Further, letting
5a = E (a|y, ¬µ, va, ve) ,
and using similar arguments, it follows that
V ar

5T

= V ar
9
k0h2 (y ‚àí¬µ) + k1

h2 (y ‚àí¬µ)
2 + k1va

1 ‚àíh2:
= V ar

k05a + k15a2
= k2
0 V ar (5a) + 2k2
1 V ar2 (5a)
with
V ar (5a) = h4 (va + ve) .
Collecting terms,
V ar

5T ‚àíT

= k2
0 [va ‚àíV ar (5a)] + 2k2
1

v2
a ‚àíV ar2 (5a)

= [va ‚àíV ar (5a)]

k2
0 + 2k2
1 [va + V ar (5a)]

.
‚ñ†

5.6 Features of Posterior Distributions
285
Dispersion Matrix of the Posterior Distribution
The posterior variance‚Äìcovariance matrix between parameters Œ∏1 and Œ∏2 is
Cov (Œ∏1, Œ∏2|y) =
  
Œ∏1 ‚àíŒ∏1
 
Œ∏2 ‚àíŒ∏2
‚Ä≤ p (Œ∏1, Œ∏2|y) dŒ∏1dŒ∏2
=
 
Œ∏1Œ∏‚Ä≤
2p (Œ∏1, Œ∏2|y) dŒ∏1dŒ∏2 ‚àíŒ∏1Œ∏2,
(5.151)
where
Œ∏i = E (Œ∏i|y) ,
i = 1, 2.
The posterior correlation between any pair of parameters can be deduced
readily from (5.151). There are situations in which a high degree of pos-
terior intercorrelation between parameters can be modiÔ¨Åed drastically by
some suitable reparameterization. This can have an impact in the numerical
behavior of Markov chain Monte Carlo (MCMC) algorithms for sampling
from posterior distributions. We shall return to this in the MCMC part of
this book.
If the probability model includes a third vector of nuisance parameters,
say Œ∏3, a sometimes useful representation of the variance‚Äìcovariance ma-
trix, of the joint posterior distribution of Œ∏1 and Œ∏2 is
Cov (Œ∏1, Œ∏2|y) = EŒ∏3|y [Cov (Œ∏1, Œ∏2|Œ∏3, y)]
+CovŒ∏3|y [E (Œ∏1|Œ∏3, y) , E (Œ∏2|Œ∏3, y)] .
(5.152)
The basic principles of Bayesian analysis have been covered at this point
using relatively simple settings. In the following chapter, a detailed discus-
sion of the linear regression model and of the mixed linear model will be
presented from a Bayesian perspective.

This page intentionally left blank

6
Bayesian Analysis of Linear Models
6.1
Introduction
A review of the basic tenets of Bayesian inference was given in the pre-
ceding chapter. Here the treatment is extended by presenting the Bayesian
analysis of some standard linear models used in quantitative genetics, and
appropriate analytical solutions are derived whenever these exist. First, the
standard linear regression model is discussed from a Bayesian perspective.
Subsequently, the Bayesian mixed eÔ¨Äects model under Gaussian assump-
tions is contrasted with its frequentist counterpart. The chapter Ô¨Ånishes
with a presentation of several marginal and conditional distributions of in-
terest. The developments presented give a necessary background for a fully
Bayesian analysis of linear models via Markov chain Monte Carlo methods,
a topic to be discussed subsequently in this book.
6.2
The Linear Regression Model
Arguably, linear regression analysis is one of the most widely used statistical
methods and its use in genetics probably dates back to Galton (1885).
For example, one of the simplest methods for estimating heritability is
based on regressing the mean value for a quantitative trait measured on
several oÔ¨Äspring from a pair of parents on the midparental mean (Falconer
and Mackay, 1996). While regression models have been discussed earlier in
this book, a more general tour of the Bayesian analysis of such models is

288
6. Bayesian Analysis of Linear Models
presented in this section. Most of the needed notation has been presented
before, so we concentrate on essentials only.
A Gaussian linear regression model, making a distinction between two
distinct sets of coeÔ¨Écients Œ≤1, (p1 √ó 1) and Œ≤2, (p2 √ó 1) , is
y = X1Œ≤1 + X2Œ≤2 + e
=

X1
X2
 
Œ≤1
Œ≤2

+ e = XŒ≤ + e,
(6.1)
where the error term is e ‚àºN

0, IœÉ2
and œÉ2 is an unknown dispersion
parameter. The likelihood function is then
L

Œ≤1, Œ≤2, œÉ2|y

‚àù

œÉ2‚àín
2 exp

‚àí(y ‚àíXŒ≤)‚Ä≤ (y ‚àíXŒ≤)
2œÉ2

.
(6.2)
It has been seen before that
(y ‚àíX1Œ≤1 ‚àíX2Œ≤2)‚Ä≤ (y ‚àíX1Œ≤1 ‚àíX2Œ≤2) = Se + SŒ≤,
where
Se
=

y ‚àíX15Œ≤1 ‚àíX25Œ≤2
‚Ä≤ 
y ‚àíX15Œ≤1 ‚àíX25Œ≤2

,
SŒ≤
=
 
Œ≤1 ‚àí5Œ≤1
‚Ä≤

Œ≤2 ‚àí5Œ≤2
‚Ä≤ 
C

Œ≤1 ‚àí5Œ≤1
Œ≤2 ‚àí5Œ≤2

,

5Œ≤1
5Œ≤2

= C‚àí1

X‚Ä≤
1y
X‚Ä≤
2y

,
and
C =

X‚Ä≤
1X1
X‚Ä≤
1X2
X‚Ä≤
2X1
X‚Ä≤
2X2

.
This decomposition leads rather directly to the joint, marginal and con-
ditional posterior distributions of interest. Results from Bayesian analyses
under two diÔ¨Äerent prior speciÔ¨Åcations will be presented separately.
6.2.1
Inference under Uniform Improper Priors
Joint Posterior Density
Suppose a uniform distribution is adopted as joint prior for all elements of
the parameter vector Œ∏ =

Œ≤‚Ä≤
1, Œ≤‚Ä≤
2, œÉ2‚Ä≤. Unless such a uniform prior does
not have Ô¨Ånite boundaries, it is improper. At Ô¨Årst sight improper priors
do not seem to make sense in the light of probability theory, at least as
presented so far. However, improper priors have played a role in an area
usually referred to as ‚Äúobjective Bayesian analysis‚Äù, an approach founded

6.2 The Linear Regression Model
289
by JeÔ¨Äreys (1961). For some parameters and models (Box and Tiao, 1973;
Bernardo and Smith, 1994), an improper uniform distribution can be shown
to introduce as little information as possible, in some sense, beyond that
contained in the data.
A speciÔ¨Åcation based on improper uniform priors will be developed in
this section. Here, Œ≤1 and Œ≤2 are allowed to take any values in the p1- and
p2-dimensional spaces ‚Ñúp1 and ‚Ñúp2 respectively, while œÉ2 falls between 0
and ‚àû. Hence, the joint posterior density is strictly proportional to the
likelihood function. After normalization (assuming the integrals exist), the
joint density can be written as
p

Œ≤1, Œ≤2, œÉ2|y

=

œÉ2‚àín
2 exp

‚àíSe+SŒ≤
2œÉ2


‚Ñúp1

‚Ñúp2
‚àû

0
(œÉ2)‚àín
2 exp

‚àíSe+SŒ≤
2œÉ2

dŒ≤1dŒ≤2 dœÉ2
.
(6.3)
The limits of integration are omitted in the notation hereinafter.
Conditional Posterior Distributions of the Regression CoeÔ¨Écients
These can be found directly from the joint posterior density by retaining
the part that varies with Œ≤1 and Œ≤2. One gets
p

Œ≤1, Œ≤2|œÉ2, y

‚àùexp

‚àíSŒ≤
2œÉ2

.
It follows that the joint posterior distribution of the regression coeÔ¨Écients,
given œÉ2, is

Œ≤1
Œ≤2
"""" œÉ2, y ‚àùN

5Œ≤1
5Œ≤2

,

X‚Ä≤
1X1
X‚Ä≤
1X2
X‚Ä≤
2X1
X‚Ä≤
2X2
‚àí1
œÉ2

.
(6.4)
Distributions of individual elements of Œ≤ are, thus, normal, with mean given
directly by the corresponding component of the mean vector, and with
variance equal to the appropriate element of the inverse matrix above,
times œÉ2. Posterior distributions of linear combinations of the regression
coeÔ¨Écients are normal as well, given œÉ2.
If, in addition to œÉ2, one treats either Œ≤1 or Œ≤2 as known, the resulting
conditional posterior distributions are
Œ≤1|Œ≤2, œÉ2, y ‚àùN

,Œ≤1, (X‚Ä≤
1X1)‚àí1 œÉ2
(6.5)
and
Œ≤2|Œ≤1, œÉ2, y ‚àùN

,Œ≤2, (X‚Ä≤
2X2)‚àí1 œÉ2
,
(6.6)
where
,Œ≤i = (X‚Ä≤
iXi)‚àí1 X‚Ä≤
i

y ‚àíXjŒ≤j

,
i = 1, 2, i Ã∏= j.

290
6. Bayesian Analysis of Linear Models
The conditional posterior distribution of an individual regression coeÔ¨É-
cient, Œ≤k, given œÉ2 and all other regression coeÔ¨Écients

Œ≤‚àík

is also normal,
with mean
,Œ≤k = x‚Ä≤
k

y ‚àíX‚àíkŒ≤‚àík

x‚Ä≤
kxk
,
(6.7)
where xk is the kth column of X, and X‚àík is X without column xk. The
variance is
V ar

Œ≤k|Œ≤‚àík, œÉ2, y

=
œÉ2
x‚Ä≤
kxk
.
(6.8)
Conditional Posterior Distribution of œÉ2
If the joint density is viewed now only as a function of œÉ2, with the regres-
sion parameters treated as constants, one gets
p

œÉ2|Œ≤1, Œ≤2, y

‚àù

œÉ2‚àín
2 exp

‚àíSe + SŒ≤
2œÉ2

.
(6.9)
This is the kernel of a scaled inverse chi-square process with degree of
belief parameter equal to n ‚àí2 (this ‚Äúloss‚Äù of two degrees of freedom is
a curious consequence of the uniform prior adopted), and scale parameter
(Se + SŒ≤) / (n ‚àí2) . It has mean and variance equal to
E

œÉ2|Œ≤1, Œ≤2, y

= (Se + SŒ≤)
n ‚àí4
and to
V ar

œÉ2|Œ≤1, Œ≤2, y

=
2 (Se + SŒ≤)2
(n ‚àí4)2 (n ‚àí6)
.
For the conditional posterior distribution of œÉ2 to exist, it is necessary that
n > 2. The mean of the distribution exists if n > 4, and the variance of the
process is deÔ¨Åned only if n > 6. Thus, given certain constraints on sample
size, the conditional posterior distribution is proper even if the prior is not
so.
Marginal Distributions of the Regression CoeÔ¨Écients
Integration of the joint density over œÉ2 gives
p (Œ≤1, Œ≤2|y) ‚àù
 
œÉ2‚àí( n‚àí2
2
+1) exp

‚àíSe + SŒ≤
2œÉ2

dœÉ2
‚àù(Se + SŒ≤)‚àí( n‚àí2
2 ) ‚àù

1 +
SŒ≤
(n ‚àí2 ‚àíp1 ‚àíp2)
Se
(n‚àí2‚àíp1‚àíp2)
‚àík
,
(6.10)

6.2 The Linear Regression Model
291
where k = (n ‚àí2 ‚àíp1 ‚àíp2 + p1 + p2) /2. This is the kernel of a p1 + p2-
dimensional multivariate-t distribution, with mean 5Œ≤ = [5Œ≤
‚Ä≤
1, 5Œ≤
‚Ä≤
2]‚Ä≤, degrees
of freedom n ‚àí2 ‚àíp1 ‚àíp2, and variance‚Äìcovariance matrix
V ar (Œ≤1, Œ≤2|y) =
Se
(n ‚àíp1 ‚àíp2 ‚àí4)

X‚Ä≤
1X1
X‚Ä≤
1X2
X‚Ä≤
2X1
X‚Ä≤
2X2
‚àí1
.
The distribution is proper only if n > p1 + p2 + 2.
Since the joint distribution of the regression coeÔ¨Écients is multivariate t,
it follows that any marginal and conditional distributions of the regression
coeÔ¨Écients (after integrating œÉ2 out) are either univariate or multivariate-t
(see Chapter 1). The same is true for the posterior distribution of any linear
combination of the coeÔ¨Écients.
Marginal Distribution of œÉ2
Integrating the joint density with respect to the regression coeÔ¨Écients gives
p

œÉ2|y

‚àù

œÉ2‚àín
2 exp

‚àíSe
2œÉ2
  
exp

‚àíSŒ≤
2œÉ2

dŒ≤1dŒ≤2.
The integrand is the kernel of the density of the multivariate normal dis-
tribution (6.4), so the integral evaluates to
(2œÄ)
p1+p2
2
""C‚àí1œÉ2""
1
2 .
Using this in the previous expression and retaining only those terms varying
with œÉ2, one gets
p

œÉ2|y

‚àù

œÉ2‚àí(
n‚àíp1‚àíp2‚àí2
2
+1) exp

‚àíSe
2œÉ2

.
(6.11)
This indicates that the marginal posterior distribution of the variance is a
scaled inverted chi-square process with degree of belief parameter n ‚àíp1 ‚àí
p2 ‚àí2, mean equal to
E

œÉ2|y

=
Se
n ‚àíp1 ‚àíp2 ‚àí4,
(6.12)
and variance
V ar

œÉ2|y

=
2S2
e
(n ‚àíp1 ‚àíp2 ‚àí4)2 (n ‚àíp1 ‚àíp2 ‚àí6)
.
(6.13)
The distribution is proper provided that n > p1 + p2 + 2 = rank (X) + 2.

292
6. Bayesian Analysis of Linear Models
Posterior Distribution of the Residuals
The residuals contain information about the quality of Ô¨Åt of the model
(Draper and Smith, 1981; Bates and Watts, 1988). Hence, an analysis of
the residuals is often indicated as a starting point for exploring the ad-
equacy of the assumptions, for example, of the functional form adopted.
From a Bayesian perspective, this is equivalent to evaluating the poste-
rior distribution of the residuals. If the residual for an observation is not
centered near zero, this can be construed as evidence that the model is
probably inconsistent with such observation (Albert and Chib, 1995). Let
the residual for datum i be
ei = yi ‚àíx‚Ä≤
iŒ≤,
where x‚Ä≤
i is now the ith row of X. Since yi is Ô¨Åxed in Bayesian analysis,
then this is a linear combination of the random variable x‚Ä≤
iŒ≤. The latter,
a posteriori, is distributed as univariate-t on n ‚àí2 ‚àíp1 ‚àíp2 degrees of
freedom, with mean x‚Ä≤
i5Œ≤, and variance
V ar (x‚Ä≤
iŒ≤|y) =
Sex‚Ä≤
iC‚àí1xi
(n ‚àíp1 ‚àíp2 ‚àí4).
(6.14)
It follows that the posterior distribution of ei is univariate-t, also on n ‚àí
2 ‚àíp1 ‚àíp2 degrees of freedom, with mean:
E (ei|y) = yi ‚àíx‚Ä≤
i5Œ≤
(6.15)
and variance equal to (6.14). The topic of model Ô¨Åt based on analysis of
residuals is taken up again in Chapter 8.
Predictive Distributions
In Bayesian analysis, a distinction needs to be made between two predictive
distributions that play diÔ¨Äerent roles. The Ô¨Årst one is the prior predictive
distribution, which assigns densities or probabilities to the data points be-
fore these are observed. If a Bayesian model postulates that the data are
generated according to the process [y|Œ∏] , and if Œ∏ has a prior density in-
dexed by hyperparameter H, then the prior predictive distribution is given
by the mixture
p (y|H)
=

p (y|Œ∏) p (Œ∏|H) dŒ∏
=
EŒ∏ [p (y|Œ∏)] ,
(6.16)
with the prior acting as mixing distribution. Hence, the prior predictive
distribution results from averaging out the sampling model [y|Œ∏] over all
possible values that the parameter vector can take, with the relative weights

6.2 The Linear Regression Model
293
being the prior density at each value of Œ∏. In other words, the prior predic-
tive distribution gives the total probability of observing the actual data,
unconditionally on parameter values, but given H.
The prior predictive distribution does not exist unless the prior is proper;
otherwise, the integral in (6.16) would not be deÔ¨Åned. Hence, there is no
prior predictive distribution for the Bayesian regression model with an un-
bounded Ô¨Çat prior. When it exists, as will be the case with the second
set of priors discussed later on, the predictive distribution provides a basis
for model comparison. We shall return to this later but, for the moment,
consider the following statement:
‚ÄúOne ought to be inclined towards choosing a model over
a competing one if the former predicts (before data collection)
the observations that will occur with a higher probability than
the latter.‚Äù
On the other hand, the posterior predictive process is the distribution or
density of future observations, given past data, and unconditionally with
respect to parameter values. Thus, a natural application of this distribu-
tion is in forecasting problems. The distribution will exist whenever the
posterior distribution of the parameters is proper. In the context of the
linear regression model, suppose that one wishes to forecast a vector of
future observations, yf, of order nf √ó 1, under the assumption that these
will be generated according to model (6.1). Since yf is unobservable, it can
be included as an unknown in Bayes theorem, with this being a particu-
lar case of the technique called ‚Äúdata augmentation‚Äù (Tanner and Wong,
1987), discussed in Chapter 11. The joint posterior density of all unknowns
is:
p

yf, Œ≤1, Œ≤2, œÉ2|y

= p

yf|Œ≤1, Œ≤2, œÉ2, y

p

Œ≤1, Œ≤2, œÉ2|y

= p

yf|Œ≤1, Œ≤2, œÉ2
p

Œ≤1, Œ≤2, œÉ2|y

,
because, given the parameters, the future and current observations are
mutually independent. The distribution

yf|Œ≤1, Œ≤2, œÉ2
is as postulated by
the sampling model, that is,
yf|Œ≤1, Œ≤2, œÉ2 ‚àºN

XfŒ≤, IfœÉ2
,
where Xf and If have suitable dimensions. The posterior predictive density
is then
p (yf|y) =
  
p

yf|Œ≤1, Œ≤2, œÉ2
p

Œ≤1, Œ≤2, œÉ2|y

dŒ≤1dŒ≤2 dœÉ2.
(6.17)
Thus, this distribution is a mixture of the sampling model for the future
observations using the joint posterior distribution of the parameters (based

294
6. Bayesian Analysis of Linear Models
on past observations) as a mixing process. Now note that the sampling
model for future observations implies that
yf = XfŒ≤ + ef,
where the future errors have the distribution ef ‚àºN

0, IfœÉ2
. The mean
of the posterior predictive distribution is then
E (yf|y) = E [E (yf|Œ≤)] = E (XfŒ≤) = Xf 5Œ≤,
(6.18)
where the outer expectation is taken with respect to the posterior distribu-
tion. The variance‚Äìcovariance matrix of the predictive distribution, using a
similar conditioning and deconditioning (outer expectations and variances
taken over the posterior distribution of the parameters), is:
V ar (yf|y) = V ar

E

yf|Œ≤,œÉ2
+ E

V ar

yf|Œ≤,œÉ2
= Xf V ar (Œ≤|y) X‚Ä≤
f + IfE

œÉ2|y

=

XfC‚àí1X‚Ä≤
f + If

Se
(n ‚àíp1 ‚àíp2 ‚àí4).
(6.19)
In order to specify completely the predictive distribution, return to (6.17)
and rewrite it as
p (yf|y) =
  
p

yf|Œ≤1, Œ≤2, œÉ2
√óp

Œ≤1, Œ≤2|œÉ2, y

p

œÉ2|y

dŒ≤1dŒ≤2 dœÉ2.
(6.20)
The Ô¨Årst two densities under the integral sign are in normal forms. Now the
quadratics in the exponents involve the regression coeÔ¨Écients and can be
put as follows (we shall not distinguish between the two sets of regressions
here)
(yf ‚àíXfŒ≤)‚Ä≤ (yf ‚àíXfŒ≤) +

Œ≤ ‚àí5Œ≤
‚Ä≤
C

Œ≤ ‚àí5Œ≤

=

yf ‚àíXf 5Œ≤
‚Ä≤ 
yf ‚àíXf 5Œ≤f

+

Œ≤ ‚àí5Œ≤f
‚Ä≤
X‚Ä≤
fXf

Œ≤ ‚àí5Œ≤f

+

Œ≤ ‚àí5Œ≤
‚Ä≤
C

Œ≤ ‚àí5Œ≤

= Sef +

Œ≤ ‚àí5Œ≤f
‚Ä≤
X‚Ä≤
fXf

Œ≤ ‚àí5Œ≤f

+

Œ≤ ‚àí5Œ≤
‚Ä≤
C

Œ≤ ‚àí5Œ≤

,
(6.21)
where
5Œ≤f =

X‚Ä≤
fXf
‚àí1 X‚Ä≤
fyf
and
Sef =

yf ‚àíXf 5Œ≤
‚Ä≤ 
yf ‚àíXf 5Œ≤

.

6.2 The Linear Regression Model
295
The Ô¨Årst term in (6.21) does not involve the regression coeÔ¨Écients. The
second and third terms can be combined, using (5.56) and (5.57), as

Œ≤ ‚àí5Œ≤f
‚Ä≤
X‚Ä≤
fXf

Œ≤ ‚àí5Œ≤f

+

Œ≤ ‚àí5Œ≤
‚Ä≤
C

Œ≤ ‚àí5Œ≤

=

Œ≤ ‚àí55Œ≤f
‚Ä≤ 
X‚Ä≤
fXf + X‚Ä≤X
 
Œ≤ ‚àí55Œ≤f

+

5Œ≤f ‚àí5Œ≤
‚Ä≤
X‚Ä≤
fXf

X‚Ä≤
fXf + X‚Ä≤X
‚àí1 X‚Ä≤X

5Œ≤f ‚àí5Œ≤

.
(6.22)
Here,
55Œ≤f
=

X‚Ä≤
fXf + X‚Ä≤X
‚àí1 
X‚Ä≤
fXf 5Œ≤f + X‚Ä≤X5Œ≤

=

X‚Ä≤
fXf + X‚Ä≤X
‚àí1 
X‚Ä≤
fyf + X‚Ä≤y

.
Let
X‚Ä≤
fXf + X‚Ä≤X = C+.
Using (6.22) in (6.21) and, further, employing the ensuing result in density
(6.20), one gets
p (yf|y) ‚àù
 
œÉ2‚àí
 nf +p1+p2
2

exp

‚àí
Sef +(Œ≤f ‚àíŒ≤)
‚Ä≤X‚Ä≤
f Xf C‚àí1
+ X‚Ä≤X(Œ≤f ‚àíŒ≤)
2œÉ2

√ó

exp
Ô£Æ
Ô£∞‚àí

Œ≤‚àíŒ≤f
‚Ä≤
C+

Œ≤‚àíŒ≤f

2œÉ2
Ô£π
Ô£ªdŒ≤
Ô£º
Ô£Ω
Ô£æp

œÉ2|y

dœÉ2.
The last integral involves a Gaussian kernel and is equal to
(2œÄ)
p1+p2
2
""C‚àí1
+ œÉ2""
1
2 =

2œÄœÉ2 p1+p2
2
""C‚àí1
+
""
1
2 .
Making use of this, the predictive density can be written as
p (yf|y)
‚àù
 
œÉ2‚àí
nf
2 exp
Ô£Æ
Ô£ØÔ£∞‚àí
Sef +

5Œ≤f ‚àí5Œ≤
‚Ä≤
X‚Ä≤
fXfC‚àí1
+ X‚Ä≤X

5Œ≤f ‚àí5Œ≤

2œÉ2
Ô£π
Ô£∫Ô£ª
√óp

œÉ2|y

dœÉ2.
(6.23)
Writing the marginal density of œÉ2 explicitly, as shown in (6.11), and letting
Q+ = X‚Ä≤
fXfC‚àí1
+ X‚Ä≤X,

296
6. Bayesian Analysis of Linear Models
the predictive density is expressible as
p (yf|y) ‚àù
 
œÉ2‚àí
 nf +n‚àíp1‚àíp2‚àí2
2
+1

√ó exp
Ô£Æ
Ô£ØÔ£∞‚àí
Sef +

5Œ≤f ‚àí5Œ≤
‚Ä≤
Q+

5Œ≤f ‚àí5Œ≤

+ Se
2œÉ2
Ô£π
Ô£∫Ô£ªdœÉ2.
The integrand is the kernel of a scaled inverse chi-square (or inverted
gamma) density, and this type of integral has been encountered a num-
ber of times before. Upon integrating over œÉ2, one obtains
p (yf|y) ‚àù

Sef +

5Œ≤f ‚àí5Œ≤
‚Ä≤
Q+

5Œ≤f ‚àí5Œ≤

+ Se
‚àí
 nf +n‚àíp1‚àíp2‚àí2
2

.
(6.24)
After lengthy matrix manipulations (see, e.g., Zellner, 1971), the posterior
predictive density can be put in the form
p (yf|y) ‚àù
Ô£Æ
Ô£ØÔ£∞1 +

yf ‚àíXf 5Œ≤
‚Ä≤
Pf

yf ‚àíXf 5Œ≤

Se
Ô£π
Ô£∫Ô£ª
‚àí
 nf +Œ∑
2

,
(6.25)
where Œ∑ = n ‚àíp1 ‚àíp2 ‚àí2, and
Pf = I ‚àíXf

X‚Ä≤
fXf + X‚Ä≤X
‚àí1 Xf.
This indicates that the distribution is a multivariate-t process of order nf,
having mean vector Xf 5Œ≤, Œ∑ degrees of freedom, and variance‚Äìcovariance
matrix
Se
n ‚àíp1 ‚àíp2 ‚àí4

I ‚àíXf

X‚Ä≤
fXf + X‚Ä≤X
‚àí1 Xf
‚àí1
.
(6.26)
In order to show that (6.19) and (6.26) are equal, use can be made of the
matrix identity

A + BGB‚Ä≤‚àí1 = A‚àí1 ‚àíA‚àí1B

B‚Ä≤A‚àí1B + G‚àí1‚àí1 B‚Ä≤A‚àí1
(6.27)
provided that the inverses involved exist. Then

XfC‚àí1X‚Ä≤
f + If
‚àí1 = If ‚àíXf

X‚Ä≤
fXf + X‚Ä≤X
‚àí1 X‚Ä≤
f,
recalling that C = X‚Ä≤X. This implies that, in (6.26),

I ‚àíXf

X‚Ä≤
fXf + X‚Ä≤X
‚àí1 Xf
‚àí1
= XfC‚àí1X‚Ä≤
f + If,
which is the matrix expression entering into (6.19). There is a much simpler
way of arriving at (6.25), which will be presented in the following section
on inference under conjugate priors.

6.2 The Linear Regression Model
297
6.2.2
Inference under Conjugate Priors
Suppose now that a scaled inverse chi-square prior is adopted for œÉ2 and
that a multivariate normal prior is assigned to Œ≤, as follows (again, let H
be a set of hyperparameters):
p

Œ≤, œÉ2|H

= p

Œ≤|œÉ2, HŒ≤

p

œÉ2|HœÉ2
.
Here HœÉ2 indexes the scaled inverse chi-square process, and HŒ≤ is the set
of hyperparameters of the multivariate normal distribution for Œ≤, given œÉ2.
Write
p

œÉ2|HœÉ2 = ŒΩ‚àó, s‚àó2
‚àù

œÉ2‚àí( ŒΩ‚àó
2 +1) exp

‚àíŒΩ‚àós‚àó2
2œÉ2

,
and
p

Œ≤|œÉ2, HŒ≤ = mŒ≤, VŒ≤

‚àù
""VŒ≤œÉ2""‚àí1
2 exp

‚àí

Œ≤ ‚àímŒ≤
‚Ä≤ V‚àí1
Œ≤

Œ≤ ‚àímŒ≤

2œÉ2

,
where mŒ≤ is the prior mean and VŒ≤œÉ2 is the covariance matrix of the
conditional (given œÉ2) prior distribution. The joint prior density is then
p

Œ≤, œÉ2|H

‚àù

œÉ2‚àí
 ŒΩ‚àó+p1+p2
2
+1

√ó exp

‚àí

Œ≤ ‚àímŒ≤
‚Ä≤ V‚àí1
Œ≤

Œ≤ ‚àímŒ≤

+ ŒΩ‚àós‚àó2
2œÉ2

.
(6.28)
Using the likelihood function in (6.2), in conjunction with the prior given
above, yields as joint posterior density of all unknown parameters
p

Œ≤, œÉ2|y, H

‚àù

œÉ2‚àí
 n+ŒΩ‚àó+p1+p2
2
+1

√ó exp

‚àí
SŒ≤ +

Œ≤ ‚àímŒ≤
‚Ä≤ V‚àí1
Œ≤

Œ≤ ‚àímŒ≤

+ Se + ŒΩ‚àós‚àó2
2œÉ2

.
(6.29)
Now the quadratic forms in Œ≤ can be combined, using (5.56) and (5.57), as
SŒ≤ +

Œ≤ ‚àímŒ≤
‚Ä≤ V‚àí1
Œ≤

Œ≤ ‚àímŒ≤

=

Œ≤ ‚àí5Œ≤
‚Ä≤
C

Œ≤ ‚àí5Œ≤

+

Œ≤ ‚àímŒ≤
‚Ä≤ V‚àí1
Œ≤

Œ≤ ‚àímŒ≤

=

Œ≤ ‚àíŒ≤
‚Ä≤ 
C + V‚àí1
Œ≤
‚àí1 
Œ≤ ‚àíŒ≤

+ S5
Œ≤,
where
Œ≤
=

C + V‚àí1
Œ≤
‚àí1 
C5Œ≤ + V‚àí1
Œ≤ mŒ≤

=

C + V‚àí1
Œ≤
‚àí1 
X‚Ä≤y + V‚àí1
Œ≤ mŒ≤

,
(6.30)

298
6. Bayesian Analysis of Linear Models
and
S5
Œ≤ =

5Œ≤‚àímŒ≤
‚Ä≤
C

C + V‚àí1
Œ≤

V‚àí1
Œ≤

5Œ≤‚àímŒ≤

.
Employing this in (6.29), the joint posterior density is expressible as
p

Œ≤,œÉ2|y, H

‚àù

œÉ2‚àí
 n+ŒΩ‚àó+p1+p2
2
+1

√ó exp
Ô£Æ
Ô£∞‚àí

Œ≤ ‚àíŒ≤
‚Ä≤ 
C + V‚àí1
Œ≤
 
Œ≤ ‚àíŒ≤

+ S5
Œ≤ + Se + ŒΩ‚àós‚àó2
2œÉ2
Ô£π
Ô£ª.
(6.31)
Note that this is in the same mathematical form as the joint prior (6.28).
This property is called conjugacy, meaning that the process remains in the
same family of distributions, a posteriori, so only the parameters need to
be updated, as the posterior has the same form as the prior. This implies
that the marginal posterior distribution of œÉ2 must be scaled inverted chi-
square, and that the conditional posterior distribution of Œ≤, given œÉ2, must
be multivariate normal. We now proceed to verify that this is so.
Conditional Posterior Distribution of the Regression CoeÔ¨Écients
This can be found via the usual procedure of examining the joint posterior
density, Ô¨Åxing some parameters, and then letting those whose distribution
is sought, vary. If (6.31) is viewed as a function of Œ≤, it is clear that the
conditional posterior distribution of the regression coeÔ¨Écients, given œÉ2, is
the multivariate normal process
Œ≤|œÉ2, y, H ‚àºN

Œ≤,

X‚Ä≤X + V‚àí1
Œ≤
‚àí1
œÉ2

.
(6.32)
Hence, it is veriÔ¨Åed that conditional posterior has the same form as the
conditional prior distribution, except that the mean vector and covariance
matrix have been modiÔ¨Åed as a consequence of learning from the data. Since
the stochastic process is multivariate normal, all posterior distributions of
either individual or sets of regression coeÔ¨Écients are normal as well, given
œÉ2. The means and (co)variances of conditional posterior distributions of
regression coeÔ¨Écients, given some other such coeÔ¨Écients, can be arrived at
using the formulas given in Chapter 1.
It is instructive writing (6.30) as
Œ≤ =

C + V‚àí1
Œ≤
‚àí1
V‚àí1
Œ≤ mŒ≤ +

C + V‚àí1
Œ≤
‚àí1
C5Œ≤.
Following O‚ÄôHagan (1994), now let
W
=

C + V‚àí1
Œ≤
‚àí1
C,
I ‚àíW
=

C + V‚àí1
Œ≤
‚àí1
V‚àí1
Œ≤ ,

6.2 The Linear Regression Model
299
so the posterior mean becomes
Œ≤
=
(I ‚àíW) mŒ≤ + W5Œ≤
=
mŒ≤ + W

5Œ≤ ‚àímŒ≤

.
This representation indicates that the posterior mean is a matrix-weighted
average of the prior mean mŒ≤ and of the ML estimator 5Œ≤, with the latter
receiving more weight as W increases, in some sense. When the information
in the data is substantial relative to the prior information, so that C is much
larger than V‚àí1
Œ≤ , then
W =

C + V‚àí1
Œ≤
‚àí1
C ‚ÜíI.
The implication is that the prior mean is overwhelmed by the ML estimator,
as the former receives an eÔ¨Äective weight of 0. On the other hand, when
‚Äúprior precision‚Äù (the inverse of the covariance matrix) is large relative to
the information contributed by the data, then W ‚Üí0, as VŒ≤ has ‚Äúsmall‚Äù
elements. In this case, the posterior mean would be expected to be very
close to the prior mean, indicating a mild modiÔ¨Åcation of prior opinions
about the value of Œ≤, after having observed the data.
Similarly, using (6.27), the posterior covariance matrix can be written as

C + V‚àí1
Œ≤
‚àí1
œÉ2 =

VŒ≤ ‚àíVŒ≤

VŒ≤ + C‚àí1‚àí1 VŒ≤

œÉ2.
The representation on the right-hand side illustrates that the posterior
variances are always smaller than or equal to the prior variances, at least
when the prior and posterior distributions are normal. When V‚àí1
Œ≤
‚Üí0,
so prior information becomes increasingly diÔ¨Äuse (large elements of V‚àí1
Œ≤ ),
the posterior covariance tends to C‚àí1œÉ2, which is in the same form as the
variance covariance matrix of the ML estimator; the same occurs when
the information in the data is very large relative to that contributed by
the prior distribution. Hence, we see that either when prior information
becomes relatively weaker and weaker, or when the relative contribution
of the prior to knowledge about the regression coeÔ¨Écients is much smaller
than that made by the data, then the conditional posterior distribution
tends to
Œ≤|œÉ2, y ‚àºN

5Œ≤, (X‚Ä≤X)‚àí1 œÉ2
.
(6.33)
Hence, in the limit, our conditional posterior distribution is centered at the
ML estimator, and the posterior covariance matrix is identical in form to
the asymptotic covariance matrix of the ML estimator. This is a particular
case of a more general result on asymptotic approximations to posterior
distributions under regularity conditions and will be discussed in Chapter
7. A technical point must be highlighted here: recall that Ô¨Ånding the asymp-
totic covariance matrix of the ML estimator requires taking expectations

300
6. Bayesian Analysis of Linear Models
over conceptual repeated sampling. However, Fisher‚Äôs information matrix
(see Chapter 3) does not play the same role in Bayesian asymptotics; this
is because the paradigm does not involve repeated sampling over the joint
distribution [Œ∏, y] . It will be seen that in Bayesian asymptotics, the ob-
served information plays a role similar to that of expected information in
ML estimation. In the speciÔ¨Åc case of the linear regression model discussed
here, the matrix of negative second derivatives of the log-posterior with
respect to Œ≤ does not involve the observations. Thus, the observed infor-
mation is a constant and, therefore, is equal to its expected value taken
over the distribution of the data. However, this would retrieve the form of
the asymptotic variance‚Äìcovariance matrix of the ML estimator only when
C overwhelms V‚àí1
Œ≤ .
Example 6.1
Ridge regression from Bayesian and frequentist points of
view
Suppose the conditional prior of the regressions has the form

Œ≤1
Œ≤2
"""" œÉ2, HŒ≤ ‚àºN

m1
m2

,

I
œÉ2
Œ≤1
œÉ2
0
0
I
œÉ2
Œ≤2
œÉ2

œÉ2

,
so the two sets of coeÔ¨Écients are independent, a priori. Then the mean of
the conditional posterior distribution of the regression coeÔ¨Écients, using
(6.30) and (6.32), is

Œ≤1
Œ≤2

=
Ô£Æ
Ô£∞
X‚Ä≤
1X1 + I œÉ2
œÉ2
Œ≤1
X‚Ä≤
1X2
X‚Ä≤
2X1
X‚Ä≤
2X2 + I œÉ2
œÉ2
Œ≤2
Ô£π
Ô£ª
‚àí1 Ô£Æ
Ô£∞
X‚Ä≤
1y + m1 œÉ2
œÉ2
Œ≤1
X‚Ä≤
1y + m2 œÉ2
œÉ2
Œ≤2
Ô£π
Ô£ª.
When there is a single set of regression coeÔ¨Écients and when the prior
mean is a null vector, this reduces to
Œ≤ = (X‚Ä≤X + Ik)‚àí1 X‚Ä≤y,
where
k = œÉ2
œÉ2
Œ≤
is a positive scalar. In the regression literature, the linear function of the
data Œ≤ is known as the ‚Äúridge regression estimator‚Äù, after Hoerl and Ken-
nard (1970). Seemingly, it did not evolve from a Bayesian argument. Now,
using the notation where the posterior mean is expressed as a weighted
average of the ML estimator and of the prior mean (null in this case), one
can write symbolically
Œ≤
=
0 + W

5Œ≤ ‚àí0

=
(X‚Ä≤X + Ik)‚àí1 X‚Ä≤X5Œ≤.

6.2 The Linear Regression Model
301
Hence, the ridge regression ‚Äúshrinks‚Äù the ML estimator toward zero, with a
strength that depends on the value of k, the ratio between the two ‚Äúvariance
components‚Äù. From a Bayesian perspective, if œÉ2
Œ≤ ‚Üí‚àû, indicating a vague
prior opinion about the value of the regression coeÔ¨Écients, then there is
little shrinkage, as k is near 0. Here the ridge estimator approaches the
ML estimator. Thus, if X‚Ä≤X is nearly singular, as in models with extreme
collinearity, the posterior distribution is also nearly singular when k = 0.
In this case, there would be extremely large posterior variances, as the
diagonal elements of
(X‚Ä≤X + Ik)‚àí1 ‚Üí‚àû.
On the other hand, when œÉ2
Œ≤ ‚Üí0, that is, in a situation where prior infor-
mation is very precise, then the ridge regression estimator tends toward the
prior mean, which is null. Hence, by incorporating prior information, i.e.,
by increasing the value of k, a nearly singular distribution becomes better
conditioned. However, the ‚Äúimprovement‚Äù in condition does not depend
on the data; rather, it is a consequence of bringing external information
into the picture. Although this has a natural interpretation in a Bayesian
context, the arguments are less transparent from a frequentist perspective.
For non-Bayesian descriptions of the ridge estimator, see Bunke (1975),
Bibby and Toutenburg (1977), and Toutenburg (1982). In particular, and
from a frequentist point of view, the ridge regression estimator has a Gaus-
sian distribution, since it is a linear combination of Gaussian observations.
Its mean value, taking expectations over the sampling model, is
Ey|Œ≤,œÉ2
Ô£Æ
Ô£∞

X‚Ä≤X + I œÉ2
œÉ2
Œ≤
‚àí1
X‚Ä≤X5Œ≤
Ô£π
Ô£ª=

X‚Ä≤X + I œÉ2
œÉ2
Œ≤
‚àí1
X‚Ä≤XŒ≤
=

X‚Ä≤X + I œÉ2
œÉ2
Œ≤
‚àí1 
X‚Ä≤X + I œÉ2
œÉ2
Œ≤
‚àíI œÉ2
œÉ2
Œ≤

Œ≤
= Œ≤ ‚àí

X‚Ä≤X + I œÉ2
œÉ2
Œ≤
‚àí1
œÉ2
œÉ2
Œ≤
Œ≤.
Thus, the ridge regression estimator is biased for Œ≤, with the bias vector
being
‚àí

X‚Ä≤X + I œÉ2
œÉ2
Œ≤
‚àí1
œÉ2
œÉ2
Œ≤
Œ≤,

302
6. Bayesian Analysis of Linear Models
which goes to 0 as œÉ2
Œ≤ goes to inÔ¨Ånity. The variance‚Äìcovariance matrix of
its sampling distribution is
V ary|Œ≤,œÉ2
Ô£Æ
Ô£∞

X‚Ä≤X + I œÉ2
œÉ2
Œ≤
‚àí1
X‚Ä≤X5Œ≤
Ô£π
Ô£ª
=

X‚Ä≤X + I œÉ2
œÉ2
Œ≤
‚àí1
X‚Ä≤X

X‚Ä≤X + I œÉ2
œÉ2
Œ≤
‚àí1
œÉ2.
What can be gained by using the biased estimator, instead of the ML
statistic? The answer resides in the possibility of attaining a smaller mean
squared error of estimation. The mean squared error matrix, that is, the
sum of the covariance matrix plus the product of the bias vector times is
transpose, is
M (Œ≤) = (X‚Ä≤X + Ik)‚àí1 
Œ≤Œ≤‚Ä≤k2 + X‚Ä≤XœÉ2
(X‚Ä≤X + Ik)‚àí1 .
A ‚Äúglobal‚Äù measure of the squared error of estimation is given by the sum
of the diagonal elements of the mean squared error matrix
tr [M (Œ≤)]
=
tr

(X‚Ä≤X + Ik)‚àí1 Œ≤Œ≤‚Ä≤k2 (X‚Ä≤X + Ik)‚àí1
+ tr

(X‚Ä≤X + Ik)‚àí1 X‚Ä≤XœÉ2 (X‚Ä≤X + Ik)‚àí1
=
k2Œ≤‚Ä≤ (X‚Ä≤X + Ik)‚àí2 Œ≤ + œÉ2 tr

X‚Ä≤X (X‚Ä≤X + Ik)‚àí2
,
after appropriate cyclical commutation of matrices under the trace opera-
tor. When k = 0 (no shrinkage at all), then
tr [M (Œ≤)] = œÉ2 tr

(X‚Ä≤X)‚àí1
,
as one would expect, as then the global mean squared error is the sum of the
sampling variances of the ML estimates of individual regression coeÔ¨Écients.
On the other hand, when k ‚Üí‚àû, that is, when there is strong shrinkage
toward 0, then tr [M (Œ≤)] goes to Œ≤‚Ä≤Œ≤ .
In order to illustrate, consider the case of a single parameter speciÔ¨Åcation,
for example, a model with a regression line going through the origin. Here
the model is
yi = Œ≤xi + ei.
The ridge regression estimator takes the simple form
Œ≤ =
n
i=1
xiyi
n
i=1
x2
i + k
.

6.2 The Linear Regression Model
303
Its expectation and sampling variance are
Ey|Œ≤,œÉ2

Œ≤

=
Œ≤
1 +
k
n

i=1
x2
i
,
and
V ary|Œ≤,œÉ2

Œ≤

=
œÉ2
n
i=1
x2
i
Ô£´
Ô£≠1 +
k
n

i=1
x2
i
Ô£∂
Ô£∏
2 ,
respectively. Clearly, the ridge estimator (or posterior mean of Œ≤ for the
prior under discussion) is less variable than the ML estimator. The mean
squared error, after rearrangement, is
M (Œ≤) =
Œ≤2k2 + œÉ2

 n
i=1
x2
i


 n
i=1
x2
i
2
Ô£´
Ô£≠1 +
k
n

i=1
x2
i
Ô£∂
Ô£∏
2 .
Viewed as a function of k, at Ô¨Åxed Œ≤, the mean squared error approaches
œÉ2/
n
i=1 x2
i

when k approaches 0, and Œ≤2 when k approaches ‚àû. A
question of interest is whether there is a value of k making M (Œ≤) minimum.
Taking derivatives of the logarithm of M (Œ≤) with respect to k and then
solving for the ‚Äúoptimum‚Äù value yields
k (Œ≤) = œÉ2
Œ≤2 .
However, Œ≤ is unknown, so the optimum k must be estimated. An intuitively
appealing procedure is given by the following iterative algorithm. Set the
functional iteration
Œ≤[i+1] =
n
i=1
xiyi
n
i=1
x2
i +
œÉ2

Œ≤[i]2
.
Start with Œ≤[0] equal to the ML estimator, and then iterate until values sta-
bilize. In practice, œÉ2 must be estimated as well, and natural candidates are
the ML or the REML estimators of the variance for this regression model.
The frequentist properties of this ‚Äúempirical minimum mean squared error
estimator‚Äù are diÔ¨Écult to evaluate analytically.
‚ñ†

304
6. Bayesian Analysis of Linear Models
Conditional Posterior Distribution of œÉ2
If one regards the joint density (6.29) as a function of œÉ2, with the regression
coeÔ¨Écients Ô¨Åxed, then it is clear that the conditional posterior distribution
of the variance is a scaled inverse chi-square process with degree of belief
parameter equal to n + ŒΩ‚àó+ p1 + p2, and with mean value and variance
given by
E

œÉ2|Œ≤, y,H

=
(y ‚àíXŒ≤)‚Ä≤ (y ‚àíXŒ≤) +

Œ≤ ‚àímŒ≤
‚Ä≤ V‚àí1
Œ≤

Œ≤ ‚àímŒ≤

+ ŒΩ‚àós‚àó2
n + ŒΩ‚àó+ p1 + p2 ‚àí2
,
and
V ar

œÉ2|Œ≤, y,H

=
2

(y ‚àíXŒ≤)‚Ä≤ (y ‚àíXŒ≤) +

Œ≤ ‚àímŒ≤
‚Ä≤ V‚àí1
Œ≤

Œ≤ ‚àímŒ≤

+ ŒΩ‚àós‚àó22
(n + ŒΩ‚àó+ p1 + p2 ‚àí2)2 (n + ŒΩ‚àó+ p1 + p2 ‚àí4)
,
respectively.
Marginal Distribution of the Regression CoeÔ¨Écients
Using properties of the inverse gamma distribution, the joint density in
(6.31) can be integrated over œÉ2 to obtain the following explicit form as
marginal density of the regression coeÔ¨Écients
p (Œ≤|y, H) ‚àù
 
œÉ2‚àí
 n+ŒΩ‚àó+p1+p2
2
+1

√ó exp
Ô£Æ
Ô£∞‚àí

Œ≤ ‚àíŒ≤
‚Ä≤ 
C + V‚àí1
Œ≤
 
Œ≤ ‚àíŒ≤

+ S5
Œ≤ + Se + ŒΩ‚àós‚àó2
2œÉ2
Ô£π
Ô£ªdœÉ2
‚àù

(Œ≤‚àíŒ≤)
‚Ä≤(C+V‚àí1
Œ≤ )(Œ≤‚àíŒ≤)+S
Œ≤+Se+ŒΩ‚àós‚àó2
2
‚àín+ŒΩ‚àó+p1+p2
2
‚àù
Ô£Æ
Ô£∞1 +

Œ≤ ‚àíŒ≤
‚Ä≤ 
C + V‚àí1
Œ≤
 
Œ≤ ‚àíŒ≤

(n + ŒΩ‚àó)

S
Œ≤+Se+ŒΩ‚àós‚àó2
n + ŒΩ‚àó

Ô£π
Ô£ª
‚àíp1+p2+n+ŒΩ‚àó
2
.
(6.34)
This is the kernel of a multivariate-t density function of order p1 + p2. The
corresponding distribution has mean vector Œ≤, covariance matrix
V ar (Œ≤|y, H) = S5
Œ≤ + Se + ŒΩ‚àós‚àó2
n + ŒΩ‚àó‚àí2

C + V‚àí1
Œ≤
‚àí1
,
and n+ŒΩ‚àódegrees of freedom. Thus, all marginal distributions of individual
or of subsets of regression coeÔ¨Écients are either univariate or multivariate-t.

6.2 The Linear Regression Model
305
The same holds for any linear combination or any conditional distribution
of the regression coeÔ¨Écients, given some other coeÔ¨Écients.
As a side point, note that at Ô¨Årst sight there does not seem to be a ‚Äúloss of
degrees of freedom‚Äù, relative to n (sample size), in the process of accounting
for uncertainty about the variance. In fact, however, there is a ‚Äúhidden loss‚Äù
of p1 + p2 degrees of freedom, which is canceled by the contribution made
by the conditional prior of the regression coeÔ¨Écients, which involves œÉ2; see
(6.28). If the prior for Œ≤ had not involved œÉ2, with VŒ≤ then being the prior
variance‚Äìcovariance matrix (assumed known), then the degrees of freedom
of the marginal posterior distribution of the regression coeÔ¨Écients would
have been n+ŒΩ‚àó‚àíp1 ‚àíp2. If, in addition, the ‚Äúdegree of belief‚Äù parameter
of the prior distribution for œÉ2 had been taken to be ŒΩ‚àó= 0, the degrees
of freedom would have been n ‚àíp1 ‚àíp2, that is, the number of degrees of
freedom arising in a standard classical analysis of linear regression. Note,
however, that the Bayesian assignment ŒΩ‚àó= 0 produces the improper prior
distribution
p

œÉ2
‚àù1
œÉ2 ,
which is some sort of ‚Äúnoninformative‚Äù prior. In this case, the posterior
distribution of the regression coeÔ¨Écients would be proper if n‚àíp1‚àíp2 > 0.
Marginal Distribution of œÉ2
Integrating the joint density (6.31) with respect to the regressions, to obtain
the marginal density of the variance, gives:
p

œÉ2|y, H

‚àù

œÉ2‚àí
 n+ŒΩ‚àó+p1+p2
2
+1

exp

‚àíS5
Œ≤ + Se + ŒΩ‚àós‚àó2
2œÉ2

√ó
 
exp
Ô£Æ
Ô£∞‚àí

Œ≤ ‚àíŒ≤
‚Ä≤ 
C + V‚àí1
Œ≤
 
Œ≤ ‚àíŒ≤

2œÉ2
Ô£π
Ô£ªdŒ≤
‚àù

œÉ2‚àí

n+ŒΩ‚àó
2
+1

exp

‚àíS5
Œ≤ + Se + ŒΩ‚àós‚àó2
2œÉ2

.
(6.35)
Hence, the posterior process is a scaled inverted chi-square distribution,
with mean
E

œÉ2|y, H

= S5
Œ≤ + Se + ŒΩ‚àós‚àó2
n + ŒΩ‚àó‚àí2
,
mode equal to
Mode

œÉ2|y, H

= S5
Œ≤ + Se + ŒΩ‚àós‚àó2
n + ŒΩ‚àó+ 2
,
and variance
V ar

œÉ2|y, H

=
2

S5
Œ≤ + Se + ŒΩ‚àós‚àó22
(n + ŒΩ‚àó‚àí2)2 (n + ŒΩ‚àó‚àí4)
.

306
6. Bayesian Analysis of Linear Models
Again, it is veriÔ¨Åed that the posterior distribution of œÉ2 has the same form
as the prior process, because of the conjugacy property mentioned earlier.
Bayesian learning updates the parameter ŒΩ‚àóin the prior distribution to
n + ŒΩ‚àó, posterior to the data. Similarly, the parameter s‚àó2 becomes
S5
Œ≤ + Se + ŒΩ‚àós‚àó2
n + ŒΩ‚àó
in the posterior distribution.
Posterior Distribution of the Residuals
Using a similar argument to that employed when inference under improper
uniform priors was discussed, the posterior distribution of a residual is also
univariate-t, with degrees of freedom n + ŒΩ‚àó, mean value equal to
E (ei|y, H) = yi ‚àíx‚Ä≤
iŒ≤,
and variance
V ar (ei|y, H) = S5
Œ≤ + Se + ŒΩ‚àós‚àó2
n + ŒΩ‚àó‚àí2
x‚Ä≤
i

C + V‚àí1
Œ≤
‚àí1
xi.
Predictive Distributions
In the regression model with proper priors, the two predictive distribu-
tions exist. The prior predictive distribution can be arrived at directly by
taking expectations of the model over the joint prior distribution of the pa-
rameters. First, we take expectations, given œÉ2, and then decondition with
respect to the variance. In order to do this, note that the observations are
a linear combination of the regression coeÔ¨Écients (which, given œÉ2, have a
multivariate normal prior distribution) and of the errors, with the latter
being normal as well. Hence, it follows that, conditionally on œÉ2, the prior
predictive distribution is the Gaussian process
y|mŒ≤, VŒ≤, œÉ2 ‚àºN

XmŒ≤, (XVŒ≤X‚Ä≤ + I) œÉ2
.
Since the prior distribution of œÉ2 is scaled inverted chi-square, decondition-
ing the normal distribution above requires evaluating the integral
p

y|mŒ≤, VŒ≤, s2‚àó, ŒΩ‚àó
‚àù
 
œÉ2‚àín+ŒΩ‚àó
2
+1
√ó exp

‚àí(y ‚àíXmŒ≤) (XVŒ≤X‚Ä≤ + I)‚àí1 (y ‚àíXmŒ≤) + ŒΩ‚àós‚àó2
2œÉ2

dœÉ2
‚àù

(y ‚àíXmŒ≤) (XVŒ≤X‚Ä≤ + I)‚àí1 (y ‚àíXmŒ≤) + ŒΩ‚àós‚àó2‚àín+ŒΩ‚àó
2
‚àù

1 + (y ‚àíXmŒ≤) (XVŒ≤X‚Ä≤ + I)‚àí1 (y ‚àíXmŒ≤)
ŒΩ‚àós‚àó2
‚àín+ŒΩ‚àó
2
,

6.2 The Linear Regression Model
307
after making use of the results given in Chapter 1, as seen several times
before. Hence, the prior predictive distribution is the multivariate-t process
on ŒΩ‚àódegrees of freedom
y|H ‚àºtn

XmŒ≤, ŒΩ‚àó, (XVŒ≤X‚Ä≤ + I) s‚àó2ŒΩ‚àó
ŒΩ‚àó‚àí2

.
(6.36)
The third term in the argument of the distribution is the variance‚Äìcovariance
matrix. Thus, (6.36) gives the probability distribution of the data before
observation takes place. If a model assigns low probability to the obser-
vations that actually occur, this can be taken as evidence against such a
model, so some revision would be needed. This issue will be elaborated
further in the discussion about model comparisons.
The posterior predictive distribution applies to future observations, as
generated by the model
yf = XfŒ≤ + ef.
The argument employed for the prior predictive distribution can also be
used here. Conditionally on œÉ2, the model for the future observations is
a linear combination of normals, with Œ≤ following the normal conditional
posterior distribution given in (6.32). The future errors have the usual
normal distribution, which is independent of the posterior distribution of Œ≤,
since the latter depends on past errors only. Then, given œÉ2, the predictive
distribution of future observations is the normal process
yf|y, œÉ,2 H ‚àºN

XfŒ≤, Xf

X‚Ä≤X + V‚àí1
Œ≤
‚àí1
X‚Ä≤
fœÉ2 + IfœÉ2

.
Next, we must decondition, but this time integrating over the marginal
posterior distribution of œÉ2, with the corresponding density given in (6.35).
Using similar algebra to that employed for the prior predictive distribution,
one arrives at
yf|y, H ‚àºt

XfŒ≤, n + ŒΩ‚àó,
Xf(X‚Ä≤X+V‚àí1
Œ≤ )
‚àí1X‚Ä≤
f(S
Œ≤+Se+ŒΩ‚àós‚àó2)
n+ŒΩ‚àó‚àí2

,
(6.37)
with this distribution being an nf-dimensional one. The predictions made
by the model can then be contrasted with future observations in some test
of predictive ability.
6.2.3
Orthogonal Parameterization of the Model
Consider again the linear model
y = XŒ≤ + e
= X1Œ≤1 + X2Œ≤2 + e,
(6.38)

308
6. Bayesian Analysis of Linear Models
where the residuals follow the usual normal process, and suppose the Œ≤‚Ä≤s
have been assigned some prior distribution. It may be that there is a strong
intercorrelation between the ML estimates of Œ≤1 and Œ≤2. In Bayesian analy-
sis, this would probably be reÔ¨Çected in a high degree of correlation between
parameters in the posterior distribution, unless sharp independent priors
are adopted for each of Œ≤1 and Œ≤2. A more comprehensive discussion of
the role of the prior on inferences will be presented in the next chapter.
For the time being, it suÔ¨Éces to recall (5.8) where it was seen that as data
contribute more and more information, the inÔ¨Çuence of the prior vanishes
asymptotically. At any rate, a strong posterior inter-correlation tends to
hamper interpretation and to impair numerical behavior, as well as the per-
formance of MCMC algorithms (these techniques, however, would probably
have doubtful value in this simple linear model, since an analytical solution
exists).
An alternative is to entertain an alternative parameterization such that
the new model reproduces the same location and dispersion structure as
(6.38). Ideally, it is desirable to render the new parameters independent
from each other, either in the ML analysis (in which case uncorrelated ML
estimates are obtained), or a posteriori, in the Bayesian setting. Under the
usual normality assumption, with i.i.d. residuals, the ML estimator of Œ≤ is

5Œ≤1
5Œ≤2

=

X‚Ä≤
1X1
X‚Ä≤
1X2
X‚Ä≤
2X1
X‚Ä≤
2X2
‚àí1 
X‚Ä≤
1y
X‚Ä≤
2y

.
(6.39)
After eliminating 5Œ≤1 from the estimating equations, the ML estimator of
Œ≤2 can be written as
5Œ≤2 = (X‚Ä≤
2M1X2)‚àí1 X‚Ä≤
2M1y,
where
M1 =

I ‚àíX1 (X‚Ä≤
1X1)‚àí1 X‚Ä≤
1

.
Recall that this matrix is idempotent. The Ô¨Åtted residuals are given by
y ‚àíX5Œ≤ = y ‚àíX (X‚Ä≤X)‚àí1 X‚Ä≤y
=

I ‚àíX (X‚Ä≤X)‚àí1 X‚Ä≤
y
=

I ‚àíX (X‚Ä≤X)‚àí1 X‚Ä≤
(XŒ≤ + e)
=

I ‚àíX (X‚Ä≤X)‚àí1 X‚Ä≤
e.
Hence, the distribution of the Ô¨Åtted residuals is independent of the Œ≤ pa-
rameters. The independence of the distribution of the residuals with respect
to Œ≤ is a consequence of the fact that

I ‚àíX (X‚Ä≤X)‚àí1 X‚Ä≤
X = 0,

6.2 The Linear Regression Model
309
and it is said that

I ‚àíX (X‚Ä≤X)‚àí1 X‚Ä≤
and X are orthogonal to each other.
In the context of (6.38), it follows that
M1X1 =

I ‚àíX1 (X‚Ä≤
1X1)‚àí1 X‚Ä≤
X1 = 0
and, consequently, that
X‚Ä≤
2M1X1 = X‚Ä≤
2

I ‚àíX1 (X‚Ä≤
1X1)‚àí1 X‚Ä≤
1

X1 = 0.
Hence, X‚Ä≤
2M1 is orthogonal to X1. Then, for
W
=
[W1, W2]
=
[X1, M1X2],
one has that
W‚Ä≤W =
Ô£Æ
Ô£∞W‚Ä≤
1W1
W‚Ä≤
1W2
W‚Ä≤
2W1
W‚Ä≤
2W2
Ô£π
Ô£ª=
Ô£Æ
Ô£∞X‚Ä≤
1X1
0
0
X‚Ä≤
2M1X2
Ô£π
Ô£ª.
Now, if instead of (6.38), one Ô¨Åts the model
y = W1Œ±1 + W2Œ±2 + e,
(6.40)
it turns out the ML estimator of the new regression coeÔ¨Écients is
 5Œ±1
5Œ±2

=
Ô£Æ
Ô£∞W‚Ä≤
1W1
0
0
W‚Ä≤
2W2
Ô£π
Ô£ª
‚àí1 Ô£Æ
Ô£∞W‚Ä≤
1y
W‚Ä≤
2y
Ô£π
Ô£ª
=
Ô£Æ
Ô£∞(W‚Ä≤
1W1)‚àí1 W‚Ä≤
1y
(W‚Ä≤
2W2)‚àí1 W‚Ä≤
2y
Ô£π
Ô£ª
=
Ô£Æ
Ô£∞
(X‚Ä≤
1X1)‚àí1 X‚Ä≤
1y
(X‚Ä≤
2M1X2)‚àí1 X‚Ä≤
2M1y
Ô£π
Ô£ª.
(6.41)
The Ô¨Årst term of the vector (6.41) gives the ML estimator of Œ≤1 in a model
ignoring Œ≤2, whereas the second term gives the ML estimator of Œ≤2 in the
full model (6.38). Taking expectations of the ML estimators of the ‚Äúnew‚Äù
parameters, with respect to the original model (6.38),
E (5Œ±1)
=
E

(W‚Ä≤
1W1)‚àí1 W‚Ä≤
1y

= (X‚Ä≤
1X1)‚àí1 X‚Ä≤
1E (y)
=
(X‚Ä≤
1X1)‚àí1 X‚Ä≤
1 (X1Œ≤1 + X2Œ≤2)
=
(X‚Ä≤
1X1)‚àí1 X‚Ä≤
1X1Œ≤1 + (X‚Ä≤
1X1)‚àí1 X‚Ä≤
1X2Œ≤2
=
Œ≤1 + (X‚Ä≤
1X1)‚àí1 X‚Ä≤
1X2Œ≤2.

310
6. Bayesian Analysis of Linear Models
This reveals the parametric relationship
Œ±1 = Œ≤1 + (X‚Ä≤
1X1)‚àí1 X‚Ä≤
1X2Œ≤2.
(6.42)
Further,
E (5Œ±2) = E

(W‚Ä≤
2W2)‚àí1 W‚Ä≤
2y

= (X‚Ä≤
2M1X2)‚àí1 X‚Ä≤
2M1E (y)
= (X‚Ä≤
2M1X2)‚àí1 X‚Ä≤
2M1X1Œ≤1 + (X‚Ä≤
2M1X2)‚àí1 X‚Ä≤
2M1X2Œ≤2 = Œ≤2.
so that
Œ±2 = Œ≤2.
(6.43)
Hence, models (6.38) and (6.40) have the same expectation, since
W1Œ±1 + W2Œ±2 = X1Œ≤1 + X1 (X‚Ä≤
1X1)‚àí1 X‚Ä≤
1X2Œ≤2 + M1X2Œ≤2
= X1Œ≤1 + X1 (X‚Ä≤
1X1)‚àí1 X‚Ä≤
1X2Œ≤2 + X2Œ≤2
‚àíX1 (X‚Ä≤
1X1)‚àí1 X‚Ä≤
1X2Œ≤2
= X1Œ≤1 + X2Œ≤2.
(6.44)
Thus, the two models generate the same probability distribution of the
observations, and this can be represented by the statement
y|Œ±1, Œ±1, œÉ2 ‚àºN

W1Œ±1 + W2Œ±2, IœÉ2
‚â°y|Œ≤1, Œ≤2, œÉ2 ‚àºN

X1Œ≤1 + X2Œ≤2, IœÉ2
.
Note that for model (6.40):
V ar

 5Œ±1
5Œ±2

=

(X‚Ä≤
1X1)‚àí1 œÉ2
0
0
(X‚Ä≤
2M1X2)‚àí1 œÉ2

,
whereas in model (6.38) it is fairly direct to show that:
V ar

5Œ≤1
5Œ≤2

=

(X‚Ä≤
1M1X1)‚àí1
Q12
Q‚Ä≤
12
(X‚Ä≤
2M2X2)‚àí1

œÉ2,
where M2 = I ‚àíX2 (X‚Ä≤
2X2)‚àí1 X‚Ä≤
2 and
Q12 = (X‚Ä≤
1M1X1)‚àí1 X‚Ä≤
1M1M2X2 (X‚Ä≤
2M2X2)‚àí1 .
Consider now a Bayesian implementation. Model (6.38) requires assign-
ing a prior distribution to the Œ≤-coeÔ¨Écients, whereas a prior must be as-
signed to the Œ±‚Ä≤s in the alternative model (6.40). These two priors must
be probabilistically consistent, that is, uncertainty statements about the
Œ±‚Ä≤s must translate into equivalent statements on the Œ≤‚Ä≤s, and vice-versa.

6.2 The Linear Regression Model
311
Suppose that one works with (6.40), and that the prior distribution of the
Œ±-coeÔ¨Écients is the normal process

Œ±1
Œ±2

‚àºN


a1
a2

,

V1
0
0
V2

œÉ2,
where the hyperparameters are known. Further, œÉ2 is assumed to follow
a scaled inverted chi-square distribution with parameters ŒΩ and s2. Using
(6.34), the density of the posterior distribution of the Œ±-coeÔ¨Écients is
p (Œ±|y, H) ‚àù
Ô£Æ
Ô£∞1 + (Œ± ‚àíŒ±)‚Ä≤ 
W‚Ä≤W + V‚àí1
(Œ± ‚àíŒ±)
(n + œÖ)

S
Œ±+Se+ŒΩs2
n + œÖ

Ô£π
Ô£ª
‚àíp1+p2+n+ŒΩ
2
, (6.45)
where

W‚Ä≤W + V‚àí1
=

W‚Ä≤
1W1 + V‚àí1
1
0
0
W‚Ä≤
2W2 + V‚àí1
2

,
Œ± =
 Œ±1
Œ±2

=
 
W‚Ä≤
1W1 + V‚àí1
1
‚àí1 
W‚Ä≤
1y + V‚àí1
1 a1


W‚Ä≤
2W2 + V‚àí1
2
‚àí1 
W‚Ä≤
2y + V‚àí1
2 a2


,
and
S5
Œ± =
2

i=1
(5Œ±i ‚àíai)‚Ä≤ W‚Ä≤
iWi

W‚Ä≤
iWi + V‚àí1
i
‚àí1 V‚àí1
i
(5Œ±i ‚àíai) ,
with
Se = y‚Ä≤y ‚àí
2

i=1
5Œ±‚Ä≤
iW‚Ä≤
iy.
The two sets of regression coeÔ¨Écients are also uncorrelated, a posteriori,
because the variance‚Äìcovariance matrix of the multivariate-t distribution
with density (6.45) is diagonal. However, Œ±1 and Œ±2 are not stochasti-
cally independent, even with independent priors and with an orthogonal
parameterization. This is because the process of deconditioning with re-
spect to œÉ2, renders the Œ±‚Ä≤s mutually dependent. Recall that the density of
a multivariate-t distribution with a diagonal covariance matrix cannot be
written as the product of the intervening marginal densities.
Since, presumably, inferential interest centers on the original regression
coeÔ¨Écients, the joint posterior density of Œ≤1 and Œ≤2 can be obtained by

312
6. Bayesian Analysis of Linear Models
eÔ¨Äecting the change of variables implicit in (6.42) and (6.43). In matrix
notation

Œ≤1
Œ≤2

=

I
‚àí(X‚Ä≤
1X1)‚àí1 X‚Ä≤
1X2
0
I
 
Œ±1
Œ±2

= TŒ±,
with inverse
Œ±1
Œ±2

=

I
(X‚Ä≤
1X1)‚àí1 X‚Ä≤
1X2
0
I
 
Œ≤1
Œ≤2

= T‚àí1Œ≤.
The determinant of the Jacobian matrix is
|J| = det

T‚àí1
= 1
since the determinant of a triangular matrix is equal to the product of its
diagonal elements, in this case all being equal to 1. Then the joint p.d.f. of
Œ≤1 and Œ≤2, using the inverse transformation in conjunction with (6.45), is
p (Œ≤|y, H) ‚àù
Ô£Æ
Ô£∞1 +

Œ≤ ‚àíŒ≤
‚Ä≤ T‚Ä≤‚àí1 
W‚Ä≤W + V‚àí1
T‚àí1 
Œ≤ ‚àíŒ≤

(n + œÖ)

S
Œ±+Se+ŒΩs2
n + œÖ

Ô£π
Ô£ª
‚àíp+n+ŒΩ
2
‚àù
Ô£±
Ô£¥
Ô£≤
Ô£¥
Ô£≥
1 +

Œ≤ ‚àíŒ≤
‚Ä≤ 
T

W‚Ä≤W + V‚àí1‚àí1 T‚Ä≤‚àí1 
Œ≤ ‚àíŒ≤

(n + œÖ)

S
Œ±+Se+ŒΩs2
n+œÖ

Ô£º
Ô£¥
Ô£Ω
Ô£¥
Ô£æ
‚àíp+n+ŒΩ
2
,
(6.46)
where p = p1 + p2, and
Œ≤ = TŒ±.
Hence, the posterior distribution of the original regression coeÔ¨Écients is
also multivariate‚àít, with mean vector Œ≤, variance‚Äìcovariance matrix:
V ar (Œ≤|y, H) = S5
Œ± + Se + ŒΩs2
n + œÖ ‚àí2
T

W‚Ä≤W + V‚àí1‚àí1 T‚Ä≤
and n + œÖ degrees of freedom.
It cannot be overemphasized that if one works with the parameterization
on the Œ≤‚Ä≤s, and that if independent priors are assigned to the two sets
of regressions, the resulting multivariate-t distribution would be diÔ¨Äerent
from that with density (6.46). This is so because, then, the induced prior
for the Œ±‚Ä≤s implies that the two sets of regressions are correlated, with prior
covariance matrix
Cov (Œ±1, Œ±‚Ä≤
2) = Cov

Œ≤1 + (X‚Ä≤
1X1)‚àí1 X‚Ä≤
1X2Œ≤2, Œ≤‚Ä≤
2

= Cov

Œ≤1, Œ≤‚Ä≤
2

+ (X‚Ä≤
1X1)‚àí1 X‚Ä≤
1X2 V ar (Œ≤2) .
Thus, if the Œ≤‚Ä≤s are taken as independently distributed a priori, this would
not be the case for the Œ±‚Ä≤s. Hence, one would not obtain the same proba-
bility statements as those resulting from a model where the Œ±‚Ä≤s are inde-
pendent, a priori.

6.3 The Mixed Linear Model
313
6.3
The Mixed Linear Model
The mixed linear model, or (co)variance components model (e.g., Henderson,
1953; Searle, 1971), is one that includes Ô¨Åxed and random eÔ¨Äects entering
linearly into the conditional (given the random eÔ¨Äects) expectation of the
observations. Typically, as seen several times before, the random eÔ¨Äects and
the model residuals are assigned Gaussian distributions which depend on
components of variance or covariance. These dispersion parameters may be
regarded as known (e.g., as in the case of best linear unbiased prediction)
or unknown. The distinction between Ô¨Åxed and random eÔ¨Äects does not
arise naturally in the Bayesian framework. However, many frequentist and
likelihood-based results can be obtained as special (and limiting) cases of
the Bayesian linear model. In this section, a linear model with two vari-
ance components will be discussed in detail from a Bayesian perspective.
Models with more than two components of variance, or multivariate linear
models where several vectors of location parameters enter into the condi-
tional expectation structure, require a somewhat more involved analytical
treatment. However, Bayesian implementations are possible through Monte
Carlo methods, as discussed in subsequent chapters. Here, use will be made
of results presented in Lindley and Smith (1972), Gianola and Fernando
(1986), and Gianola et al. (1990), as well as developments given in the
present and in the preceding chapters.
6.3.1
Bayesian View of the Mixed EÔ¨Äects Model
Consider the linear model
y = XŒ≤ + Zu + e,
where Œ≤, (p √ó 1), and u, (q √ó 1), are location vectors related to observa-
tions y, (n √ó 1), through the nonstochastic matrices X and Z, respectively.
Further, let
e|œÉ2
e ‚àºN

0, IœÉ2
e

,
be a vector of independently distributed residuals. Hence, given the residual
variance, the sampling model is
y|Œ≤, u, œÉ2
e ‚àºN

XŒ≤ + Zu, IœÉ2
e

.
(6.47)
Suppose that elicitation yields the prior distribution
Œ≤|œÉ2
Œ≤ ‚àºN

0, BœÉ2
Œ≤

,
where B is a known, nonsingular matrix and œÉ2
Œ≤ is a hyperparameter. In
the Bayesian setting, thus, the ‚ÄúÔ¨Åxed‚Äù (in the frequentist view) vector Œ≤ is
random, since it is assigned a distribution, this being a normal one in this
case. Further, take
u|AœÉ2
u ‚àºN

0, AœÉ2
u

,

314
6. Bayesian Analysis of Linear Models
where, in a quantitative genetics context, A would be a matrix of addi-
tive relationships (also assumed to be nonsingular) whenever u is a vector
of additive genetic eÔ¨Äects and œÉ2
u is the additive genetic variance. In the
classical mixed model, the preceding distribution, rather than viewed as
a Bayesian prior, is interpreted as one resulting from a long-run process
of sampling vectors from some conceptual population, with Ô¨Åxed A and
œÉ2
u. Such a sampling process generates a distribution with null mean and
covariance matrix AœÉ2
u. It follows that the location structure, XŒ≤ + Zu,
has only one random component (u) in a frequentist setting, whereas it has
two (Œ≤ and u) in the Bayesian probability model.
Assume that the prior distributions elicited for the variance compo-
nents are independent scaled inverted chi-square processes with parameters

s2
e, ŒΩe

and

s2
u, ŒΩu

for œÉ2
e and œÉ2
u, respectively. The consequences of these
Bayesian assumptions follow:
‚Ä¢ Unconditionally to the residual variance, the sampling model, given
Œ≤ and u, is multivariate-t, with density
p

y|Œ≤, u, s2
e, ŒΩe

‚àù

1 + (y ‚àíXŒ≤ ‚àíZu)‚Ä≤ (y ‚àíXŒ≤ ‚àíZu)
ŒΩes2e
‚àín+ŒΩe
2
.
(6.48)
‚Ä¢ The marginal distribution of the additive eÔ¨Äects, unconditionally to
the additive genetic variance, is a multivariate-t process having den-
sity
p

u|s2
u, ŒΩu

‚àù

1 + u‚Ä≤A‚àí1u
ŒΩus2u
‚àíq+ŒΩu
2
.
(6.49)
‚Ä¢ In the frequentist mixed eÔ¨Äects linear model, the marginal distribu-
tion of the observations is indexed by the ‚ÄúÔ¨Åxed‚Äù parameter Œ≤ and
by the variance components œÉ2
u and œÉ2
e, and this is
y|Œ≤, œÉ2
u,œÉ2
e ‚àºN

XŒ≤, ZAZ‚Ä≤œÉ2
u + IœÉ2
e

.
(6.50)
In the Bayesian model, on the other hand, the marginal distribution
of the observations is obtained by integrating over all unknown pa-
rameters

Œ∏ = Œ≤, u, œÉ2
u, œÉ2
e

entering in the model. If the joint prior
of all parameters has the form
p

Œ≤, u, œÉ2
u, œÉ2
e|œÉ2
Œ≤, s2
u, ŒΩu, s2
e, ŒΩe

= p

Œ≤|œÉ2
Œ≤

p

u|œÉ2
u

p

œÉ2
u|s2
u, ŒΩu

p

œÉ2
e|s2
e, ŒΩe

,

6.3 The Mixed Linear Model
315
then the marginal density of the observations can be written as
p

y|œÉ2
Œ≤, s2
u, ŒΩu, s2
e, ŒΩe

=
   
p

y|Œ≤, u, œÉ2
e

p

Œ∏|œÉ2
Œ≤, s2
u, ŒΩu, s2
e, ŒΩe

dŒ∏
=
  
p

y|Œ≤, u, œÉ2
e

p

œÉ2
e|s2
e, ŒΩe

dœÉ2
e

√ó

p

u|œÉ2
u

p

œÉ2
u|s2
u, ŒΩu

dœÉ2
u

p

Œ≤|œÉ2
Œ≤

dŒ≤ du
(6.51)
with the integrals in brackets evaluating to (6.48) and (6.49), re-
spectively. Hence, the marginal density of the observations, or prior
predictive distribution, cannot be written in closed form when the
variance components are unknown.
‚Ä¢ It is possible to go further than in (6.51) when the dispersion pa-
rameters are known. In the Bayesian model, one has to contemplate
the variability (uncertainty) of the Œ≤ values introduced by the prior
distribution. In this case, the marginal density of the observations
would be
p

y|œÉ2
Œ≤, œÉ2
u, œÉ2
e

=
 
p

y|Œ≤, u, œÉ2
e

p

Œ≤|œÉ2
Œ≤

p

u|œÉ2
u

dŒ≤ du.
(6.52)
Since the three densities in the integrand are normal, the exponents
can be combined as
1
œÉ2e

(y ‚àíXŒ≤ ‚àíZu)‚Ä≤ (y ‚àíXŒ≤ ‚àíZu) + Œ≤‚Ä≤B‚àí1Œ≤ œÉ2
e
œÉ2
Œ≤
+u‚Ä≤A‚àí1u œÉ2
e
œÉ2u

= 1
œÉ2e

(y ‚àíWŒ±)‚Ä≤ (y ‚àíWŒ±) + Œ±‚Ä≤Œ£‚àí1Œ±

,
(6.53)
where Œ± =

Œ≤‚Ä≤, u‚Ä≤‚Ä≤, W =
 X
Z 
, and
Œ£‚àí1 =
Ô£Æ
Ô£∞B‚àí1 œÉ2
e
œÉ2
Œ≤
0
0
A‚àí1 œÉ2
e
œÉ2
u
Ô£π
Ô£ª.
Now let
5Œ± =

W‚Ä≤W + Œ£‚àí1‚àí1 W‚Ä≤y.
(6.54)

316
6. Bayesian Analysis of Linear Models
Hence,
(y ‚àíWŒ±)‚Ä≤ (y ‚àíWŒ±) + Œ±‚Ä≤Œ£‚àí1Œ±
=
y‚Ä≤y ‚àí2Œ±‚Ä≤W‚Ä≤y + Œ±‚Ä≤ 
W‚Ä≤W + Œ£‚àí1
Œ±
=
y‚Ä≤y‚àí2Œ±‚Ä≤ 
W‚Ä≤W + Œ£‚àí1 5Œ± + Œ±‚Ä≤ 
W‚Ä≤W + Œ£‚àí1
Œ±
+5Œ±‚Ä≤ 
W‚Ä≤W + Œ£‚àí1 5Œ± ‚àí5Œ±‚Ä≤ 
W‚Ä≤W + Œ£‚àí1 5Œ±
where the quadratic in 5Œ± is added and subtracted in order to complete
a ‚Äúsquare‚Äù. One can then write
y‚Ä≤y‚àí5Œ±‚Ä≤ 
W‚Ä≤W + Œ£‚àí1 5Œ±+
(Œ±‚àí5Œ±)‚Ä≤ 
W‚Ä≤W + Œ£‚àí1
(Œ±‚àí5Œ±)
= y‚Ä≤y‚àí5Œ±‚Ä≤W‚Ä≤y + (Œ±‚àí5Œ±)‚Ä≤ 
W‚Ä≤W + Œ£‚àí1
(Œ±‚àí5Œ±) .
(6.55)
Using this in (6.53), it turns out that the integrand in (6.52) is ex-
pressible as
p

y|œÉ2
Œ≤, œÉ2
u, œÉ2
e

‚àù

exp

‚àí1
2œÉ2e

y‚Ä≤y ‚àí5Œ±‚Ä≤W‚Ä≤y
+ (Œ± ‚àí5Œ±)‚Ä≤ 
W‚Ä≤W + Œ£‚àí1
(Œ± ‚àí5Œ±)

dŒ±
‚àùexp

‚àíy‚Ä≤y ‚àí5Œ±‚Ä≤W‚Ä≤y
2œÉ2e
 
exp

‚àí
(Œ± ‚àíŒ±)‚Ä≤(W‚Ä≤W+Œ£‚àí1)(Œ± ‚àíŒ±)
2œÉ2
e

dŒ±.
(6.56)
The integral in (6.56) involves the kernel of a p+q-dimensional Gaus-
sian distribution, and it evaluates to
(2œÄ)
p+q
2
"""

W‚Ä≤W + Œ£‚àí1‚àí1 œÉ2
e
"""
1
2 .
Since this does not involve the data, it gets absorbed in the integration
constant, yielding, as marginal density of the data,
p

y|œÉ2
Œ≤, œÉ2
u, œÉ2
e

=
exp

‚àíy‚Ä≤y ‚àí5Œ±‚Ä≤W‚Ä≤y
2œÉ2e


exp

‚àíy‚Ä≤y ‚àí5Œ±‚Ä≤W‚Ä≤y
2œÉ2e

dy
.
(6.57)
This can be shown to be the density of the normal process
y|œÉ2
Œ≤, œÉ2
u, œÉ2
e ‚àºN

0, XBX‚Ä≤œÉ2
Œ≤ + ZAZ‚Ä≤œÉ2
u + IœÉ2
e

.
A comparison of this with (6.50) reveals that the Bayesian linear
model cannot be construed as its frequentist counterpart. In the lat-
ter, the marginal distribution of the observations requires knowledge

6.3 The Mixed Linear Model
317
of the Ô¨Åxed eÔ¨Äects and of the variances in order to compute probabil-
ities. In the Bayesian model, the ‚ÄúÔ¨Åxed‚Äù eÔ¨Äects have been integrated
out (with respect to the prior), and the marginal process of the obser-
vations also depends on the known hyperparameter œÉ2
Œ≤. The Bayesian
model diÔ¨Äers in other important respects, as illustrated in the follow-
ing sections. For example, Ô¨Ånding the marginal distribution of the
observations (unconditionally to all parameters), or prior predictive
distribution requires additional integration with respect to œÉ2
u and
œÉ2
e.
6.3.2
Joint and Conditional Posterior Distributions
Under the assumptions made, the joint posterior density of all parameters
is
p

Œ≤, u, œÉ2
u, œÉ2
e|y, œÉ2
Œ≤, s2
u, ŒΩu, s2
e, ŒΩe

‚àùp

y|Œ≤, u, œÉ2
e

p

Œ≤|œÉ2
Œ≤

p

u|œÉ2
u

√óp

œÉ2
u|s2
u, ŒΩu

p

œÉ2
e|s2
e, ŒΩe

.
(6.58)
All fully conditional posterior distributions can be identiÔ¨Åed directly from
the joint posterior density, by viewing the latter as a function of the param-
eter(s) of interest, while keeping all other parameters and data y (‚ÄúELSE‚Äù)
Ô¨Åxed. All such densities can be written in closed form, as shown below.
‚Ä¢ The conditional posterior density of the additive genetic variance, œÉ2
u,
given all other parameters, is
p

œÉ2
u|œÉ2
Œ≤, s2
u, ŒΩu, s2
e, ŒΩe, ELSE

‚àùp

u|œÉ2
u

p

œÉ2
u|s2
u, ŒΩu

‚àù

œÉ2
u
‚àíq+ŒΩu+2
2
exp

‚àíu‚Ä≤A‚àí1u + ŒΩus2
u
2œÉ2u

.
(6.59)
This is the kernel of a scaled inverted chi-square distribution with
degree of belief parameter ŒΩ‚Ä≤
u = q + ŒΩu, and scale parameter
s‚Ä≤2
u = u‚Ä≤A‚àí1u + ŒΩus2
u
q + ŒΩu
.
Similarly, the fully conditional posterior density of œÉ2
e is
p

œÉ2
e|œÉ2
Œ≤, s2
u, ŒΩu, s2
e, ŒΩe, ELSE

‚àùp

y|Œ≤, u, œÉ2
e

p

œÉ2
e|s2
e, ŒΩe

‚àù

œÉ2
e
‚àín+ŒΩe+2
2
exp

‚àí(y ‚àíXŒ≤ ‚àíZu)‚Ä≤ (y ‚àíXŒ≤ ‚àíZu) +ŒΩes2
e
2œÉ2e

.
(6.60)
Hence, the conditional posterior distribution of the residual variance
is scaled inverted chi-square, with parameters ŒΩ‚Ä≤
e = n + ŒΩe, and
s‚Ä≤2
e = (y ‚àíXŒ≤ ‚àíZu)‚Ä≤ (y ‚àíXŒ≤ ‚àíZu) + ŒΩes2
e
n + ŒΩe
.

318
6. Bayesian Analysis of Linear Models
It is easy to verify that, given all other parameters, the two variance
components are conditionally independent.
‚Ä¢ The density of the conditional distribution of the ‚ÄúÔ¨Åxed‚Äù and ‚Äúran-
dom‚Äù eÔ¨Äects, given the variance components, is
p

Œ≤, u|œÉ2
Œ≤, s2
u, ŒΩu, s2
e, ŒΩe, ELSE

‚àùp

y|Œ≤, u, œÉ2
e

p

Œ≤|œÉ2
Œ≤

p

u|œÉ2
u

.
Use can be made here of (6.53) and (6.55), to arrive at
p

Œ≤, u|œÉ2
Œ≤, s2
u, ŒΩu, s2
e, ŒΩe, ELSE

‚àùexp

‚àí1
2œÉ2e

y‚Ä≤y ‚àí5Œ±‚Ä≤W‚Ä≤y + (Œ± ‚àí5Œ±)‚Ä≤ 
W‚Ä≤W + Œ£‚àí1
(Œ± ‚àí5Œ±)
%
‚àùexp

‚àí(Œ± ‚àí5Œ±)‚Ä≤ 
W‚Ä≤W + Œ£‚àí1
(Œ± ‚àí5Œ±)
2œÉ2e

,
(6.61)
since the term that does not depend on Œ± is absorbed in the integra-
tion constant. It follows that the corresponding conditional posterior
distribution is the multivariate normal process, as discussed in Chap-
ter 1, Example 1.18,
Œ≤, u|y, œÉ2
Œ≤, s2
u, ŒΩu, s2
e, ŒΩe, œÉ2
u, œÉ2
e ‚àºN

5Œ±,

W‚Ä≤W + Œ£‚àí1‚àí1 œÉ2
e

.
(6.62)
Note that since œÉ2
u and œÉ2
e are assumed to be known, the distribution
does not depend on parameters s2
u, ŒΩu, s2
e and ŒΩe. However, these are
left in the notation for the sake of symmetry in the presentation. We
now write the mean of this distribution explicitly as
Ô£Æ
Ô£∞X‚Ä≤X + B‚àí1 œÉ2
e
œÉ2
Œ≤
X‚Ä≤Z
Z‚Ä≤X
Z‚Ä≤Z + A‚àí1 œÉ2
e
œÉ2
u
Ô£π
Ô£ª
 5Œ≤
5u

=

X‚Ä≤y
Z‚Ä≤y

.
If œÉ2
Œ≤ ‚Üí‚àû, which represents in some sense vague prior knowledge
about Œ≤, the above system of equations becomes what is known as
Henderson‚Äôs mixed model equations (Henderson et al., 1959; Hender-
son, 1973; Searle, 1971). In this situation, it can be shown that 5Œ≤
above is the ML estimator of the Ô¨Åxed vector in a Gaussian mixed
linear model with known variance components. Further, 5u is the best
linear unbiased predictor of u, or BLUP (this holds under a mixed
linear model even if the joint distribution of the random eÔ¨Äects and
of the observations is not Gaussian, but the dispersion parameters
must be known). Hence, both the ML estimator and the BLUP arise
in the context of special cases of a more general Bayesian setting. If
there is prior information about the ‚ÄúÔ¨Åxed‚Äù eÔ¨Äects, œÉ2
Œ≤ would be Ô¨Ånite
and, hence, the Bayesian model would eÔ¨Äect some shrinkage toward

6.3 The Mixed Linear Model
319
the prior mean (in this case assumed to be null) of the Œ≤ vector. In
fact, let ,Œ± be the solution to the system
W‚Ä≤W,Œ± = W‚Ä≤y.
The vector ,Œ± can be interpreted as the ML estimator of Œ± in a model
in which both Œ≤ and u are treated as Ô¨Åxed (or as posterior mean in a
model where both œÉ2
Œ≤ and œÉ2
u go to inÔ¨Ånity). Often, ,Œ± is not deÔ¨Åned
uniquely, as W‚Ä≤W may have deÔ¨Åcient rank. On the other hand, 5Œ±
is unique, provided that either X has full-column rank, or that œÉ2
Œ≤ is
Ô¨Ånite. Then, the posterior mean vector
5Œ± =

W‚Ä≤W + Œ£‚àí1‚àí1 W‚Ä≤y
=

W‚Ä≤W + Œ£‚àí1‚àí1 
W‚Ä≤W,Œ± + Œ£‚àí10

can be viewed as a matrix weighted average of ,Œ± (with this vector
being a function of the data only) and of the mean of the prior dis-
tribution of Œ±, which is 0. The matrix weights are W‚Ä≤W (a measure
of the precision of inferences contributed by the data) and Œ£‚àí1(a
measure of prior precision), respectively.
‚Ä¢ Result (6.62) implies that the marginal distributions of Œ≤ and u, given
the variance components, are also multivariate normal, having mean
vectors 5Œ≤ and 5u, respectively. We will derive the form of these two
densities systematically. First, note that the joint posterior density
of Œ≤ and u, given the dispersion parameters, can be written as
p

Œ≤, u|y, œÉ2
Œ≤, œÉ2
u, œÉ2
e

‚àùexp

‚àí1
2œÉ2e

(y ‚àíXŒ≤ ‚àíZu)‚Ä≤ (y ‚àíXŒ≤ ‚àíZu)
+ Œ≤‚Ä≤B‚àí1Œ≤ œÉ2
e
œÉ2
Œ≤
+ u‚Ä≤A‚àí1u œÉ2
e
œÉ2u
$
.
(6.63)
Now put w (Œ≤) = y ‚àíXŒ≤ and write, employing a decomposition
similar to that in (6.55),
(y ‚àíXŒ≤ ‚àíZu)‚Ä≤ (y ‚àíXŒ≤ ‚àíZu) + u‚Ä≤A‚àí1u œÉ2
e
œÉ2u
= [w (Œ≤) ‚àíZu]‚Ä≤ [w (Œ≤) ‚àíZu] + u‚Ä≤A‚àí1u œÉ2
e
œÉ2u
= w‚Ä≤ (Œ≤) w (Œ≤) ‚àí55u (Œ≤)‚Ä≤ Z‚Ä≤w (Œ≤)
+

55u (Œ≤) ‚àíu
‚Ä≤ 
Z‚Ä≤Z + A‚àí1 œÉ2
e
œÉ2u
 
55u (Œ≤) ‚àíu

,
where
55u (Œ≤) =

Z‚Ä≤Z + A‚àí1 œÉ2
e
œÉ2u
‚àí1
Z‚Ä≤w (Œ≤) .

320
6. Bayesian Analysis of Linear Models
Making use of this in the joint density (6.63) and integrating with
respect to u, to obtain the marginal posterior density of Œ≤, given the
variance components, yields
p

Œ≤|y, œÉ2
Œ≤, œÉ2
u, œÉ2
e

‚àùexp

‚àí1
2œÉ2e

w‚Ä≤ (Œ≤) w (Œ≤) ‚àí55u (Œ≤)‚Ä≤ Z‚Ä≤w (Œ≤) + Œ≤‚Ä≤B‚àí1Œ≤ œÉ2
e
œÉ2
Œ≤
%

exp

‚àí1
2œÉ2e

55u (Œ≤) ‚àíu
‚Ä≤ 
Z‚Ä≤Z + A‚àí1 œÉ2
e
œÉ2u
 
55u (Œ≤) ‚àíu
%
du
‚àùexp

‚àí1
2œÉ2e

w‚Ä≤ (Œ≤) w (Œ≤) ‚àí55u (Œ≤)‚Ä≤ Z‚Ä≤w (Œ≤) + Œ≤‚Ä≤B‚àí1Œ≤ œÉ2
e
œÉ2
Œ≤
%
√ó (2œÄ)
q
2
""""

Z‚Ä≤Z + A‚àí1 œÉ2
e
œÉ2
u
‚àí1
œÉ2
e
""""
1
2
‚àùexp

‚àí1
2œÉ2e

w‚Ä≤ (Œ≤) w (Œ≤) ‚àí55u (Œ≤)‚Ä≤ Z‚Ä≤w (Œ≤) + Œ≤‚Ä≤B‚àí1Œ≤ œÉ2
e
œÉ2
Œ≤
%
.
(6.64)
Now
w‚Ä≤ (Œ≤) w (Œ≤) ‚àí55u (Œ≤)‚Ä≤ Z‚Ä≤w (Œ≤) + Œ≤‚Ä≤B‚àí1Œ≤ œÉ2
e
œÉ2
Œ≤
= (y ‚àíXŒ≤)‚Ä≤ (y ‚àíXŒ≤)
‚àí(y ‚àíXŒ≤)‚Ä≤ Z

Z‚Ä≤Z + A‚àí1 œÉ2
e
œÉ2u
‚àí1
Z‚Ä≤ (y ‚àíXŒ≤) + Œ≤‚Ä≤B‚àí1Œ≤ œÉ2
e
œÉ2
Œ≤
= (y ‚àíXŒ≤)‚Ä≤

I ‚àíZ

Z‚Ä≤Z + A‚àí1 œÉ2
e
œÉ2
u
‚àí1
Z‚Ä≤

(y ‚àíXŒ≤)
+ Œ≤‚Ä≤B‚àí1Œ≤ œÉ2
e
œÉ2
Œ≤ .
(6.65)
Using matrix identity (6.27), it can be shown that
I ‚àíZ

Z‚Ä≤Z + A‚àí1 œÉ2
e
œÉ2
u
‚àí1
Z‚Ä≤ =

ZAZ‚Ä≤ œÉ2
u
œÉ2
e + I
‚àí1
= V‚àí1.
Making use of this in (6.65), the marginal density (6.64) can be writ-
ten as
p

Œ≤|y, œÉ2
Œ≤, œÉ2
u, œÉ2
e

‚àù
9
‚àí1
2œÉ2
e

(y ‚àíXŒ≤)‚Ä≤ V‚àí1 (y ‚àíXŒ≤) + Œ≤‚Ä≤B‚àí1Œ≤ œÉ2
e
œÉ2
Œ≤
:
.
Note that, as œÉ2
Œ≤ ‚Üí‚àû, the kernel of the posterior density tends
toward the likelihood of Œ≤ in a Gaussian linear mixed model with

6.3 The Mixed Linear Model
321
known variance components and covariance matrix as given in (6.50).
The ML estimator of Œ≤ is
,Œ≤ =

X‚Ä≤V‚àí1X
‚àí1 X‚Ä≤V‚àí1y,
and using the standard decomposition, one obtains
p

Œ≤|y, œÉ2
Œ≤, œÉ2
u, œÉ2
e

‚àùexp

‚àí1
2œÉ2
e

y ‚àíX,Œ≤
‚Ä≤
V‚àí1 
y ‚àíX,Œ≤

+

Œ≤ ‚àí,Œ≤
‚Ä≤
X‚Ä≤V‚àí1X

Œ≤ ‚àí,Œ≤

+ Œ≤‚Ä≤B‚àí1Œ≤ œÉ2
e
œÉ2
Œ≤
%
‚àùexp

‚àí1
2œÉ2
e

Œ≤ ‚àí,Œ≤
‚Ä≤
X‚Ä≤V‚àí1X

Œ≤ ‚àí,Œ≤

+ Œ≤‚Ä≤B‚àí1Œ≤ œÉ2
e
œÉ2
Œ≤
%
. (6.66)
The quadratic forms in Œ≤ can now be combined as

Œ≤ ‚àí,Œ≤
‚Ä≤
X‚Ä≤V‚àí1X

Œ≤ ‚àí,Œ≤

+ Œ≤‚Ä≤B‚àí1Œ≤ œÉ2
e
œÉ2
Œ≤
=

Œ≤ ‚àí5Œ≤
‚Ä≤ 
X‚Ä≤V‚àí1X + B‚àí1 œÉ2
e
œÉ2
Œ≤
 
Œ≤ ‚àí5Œ≤

+ ,Œ≤
‚Ä≤X‚Ä≤V‚àí1X

X‚Ä≤V‚àí1X + B‚àí1 œÉ2
e
œÉ2
Œ≤
‚àí1
B‚àí1 œÉ2
e
œÉ2
Œ≤
,Œ≤,
(6.67)
where
5Œ≤ =

X‚Ä≤V‚àí1X + B‚àí1 œÉ2
e
œÉ2
Œ≤
‚àí1
X‚Ä≤V‚àí1y.
After some matrix manipulations, it is possible to show that this is
precisely the Œ≤-component of 5Œ± in (6.62). Using (6.67) in (6.66) and
retaining the part that varies with Œ≤ yields, as density of the marginal
distribution of Œ≤ (conditionally on the variance components),
p

Œ≤|y, œÉ2
Œ≤, œÉ2
u, œÉ2
e

‚àùexp

‚àí1
2œÉ2e

Œ≤ ‚àí5Œ≤
‚Ä≤ 
X‚Ä≤V‚àí1X + B‚àí1 œÉ2
e
œÉ2
Œ≤
 
Œ≤ ‚àí5Œ≤
%
.
Thus, the marginal posterior density of Œ≤ when the dispersion pa-
rameters are known, is the normal process
Œ≤|y, œÉ2
Œ≤, œÉ2
u, œÉ2
e ‚àºN

5Œ≤,

X‚Ä≤V‚àí1X + B‚àí1 œÉ2
e
œÉ2
Œ≤
‚àí1
œÉ2
e

.
(6.68)
Again, note that the mean of the distribution is a matrix weighted
average of the ML estimator of Œ≤ and of the mean of the prior distri-
bution of this vector, assumed to be null in this case.

322
6. Bayesian Analysis of Linear Models
‚Ä¢ Using similar algebra, it can be found that the marginal posterior
density of u, given the variance components, is
u|y, œÉ2
Œ≤, œÉ2
u, œÉ2
e ‚àºN

5u,

Z‚Ä≤T‚àí1Z + A‚àí1 œÉ2
e
œÉ2
u
‚àí1
œÉ2
e

,
(6.69)
with
5u =

Z‚Ä≤T‚àí1Z + A‚àí1 œÉ2
e
œÉ2
u
‚àí1
Z‚Ä≤T‚àí1y,
and
T = XBX‚Ä≤ œÉ2
Œ≤
œÉ2
e + I.
The mean vector is equal to the u-component of 5Œ± in (6.62).
‚Ä¢ Since the joint distribution of Œ≤ and u is jointly Gaussian, with pa-
rameters as in (6.62), it follows that the processes

Œ≤|y, œÉ2
Œ≤, s2
u, ŒΩu, s2
e, ŒΩe, œÉ2
u, œÉ2
e, u

,

u|y, œÉ2
Œ≤, s2
u, ŒΩu, s2
e, ŒΩe, œÉ2
u, œÉ2
e, Œ≤

,
are normal as well. Making use of results in Chapter 1, one can arrive
at
Œ≤|y, œÉ2
Œ≤, s2
u, ŒΩu, s2
e, ŒΩe, œÉ2
u, œÉ2
e, u
‚àºN

5Œ≤ (u) ,

X‚Ä≤X + B‚àí1 œÉ2
e
œÉ2
Œ≤
‚àí1
œÉ2
e

,
(6.70)
where
5Œ≤ (u) =

X‚Ä≤X + B‚àí1 œÉ2
e
œÉ2
Œ≤
‚àí1
X‚Ä≤ (y ‚àíZu) .
Also
u|y, œÉ2
Œ≤, s2
u, ŒΩu, s2
e, ŒΩe, œÉ2
u, œÉ2
e, Œ≤
‚àºN

5u (Œ≤) ,

Z‚Ä≤Z + A‚àí1 œÉ2
e
œÉ2
u
‚àí1
œÉ2
e

,
(6.71)
where
5u (Œ≤) =

Z‚Ä≤Z + A‚àí1 œÉ2
e
œÉ2
u
‚àí1
Z‚Ä≤ (y ‚àíXŒ≤) .
6.3.3
Marginal Distribution of Variance Components
The location parameters can be integrated out analytically of the joint
posterior density. First, write the density of the joint posterior distribution

6.3 The Mixed Linear Model
323
of all parameters as
p

Œ≤, u,œÉ2
u,œÉ2
e|y, œÉ2
Œ≤, s2
u, ŒΩu, s2
e, ŒΩe

‚àùp

œÉ2
u|s2
u, ŒΩu

p

œÉ2
e|s2
e, ŒΩe
 
œÉ2
e
‚àín
2 
œÉ2
u
‚àíq
2
√ó exp
9
‚àí1
2œÉ2
e

y‚Ä≤y ‚àí5Œ±‚Ä≤
cW‚Ä≤y + (Œ± ‚àí5Œ±c)‚Ä≤ 
W‚Ä≤W + Œ£‚àí1
(Œ± ‚àí5Œ±c)
:
‚àùp

œÉ2
u|s2
u, ŒΩu

p

œÉ2
e|s2
e, ŒΩe
 
œÉ2
e
‚àín
2 
œÉ2
u
‚àíq
2 exp

‚àíy‚Ä≤y ‚àí5Œ±‚Ä≤
cW‚Ä≤y
2œÉ2e

√ó exp

‚àí(Œ± ‚àí5Œ±c)‚Ä≤ 
W‚Ä≤W + Œ£‚àí1
(Œ± ‚àí5Œ±c)
2œÉ2e

,
where 5Œ±c is 5Œ± of (6.62), with the subscript placed to emphasize the de-
pendence of this vector on the unknown variance components œÉ2
u and œÉ2
e.
The location parameters now appear in the kernel of a multivariate normal
density and can be integrated out by analytical means. After integration,
one gets
p

œÉ2
u, œÉ2
e|y, œÉ2
Œ≤, s2
u, ŒΩu, s2
e, ŒΩe

‚àùp

œÉ2
u|s2
u, ŒΩu

p

œÉ2
e|s2
e, ŒΩe
 
œÉ2
e
‚àín
2 
œÉ2
u
‚àíq
2 exp

‚àíy‚Ä≤y ‚àí5Œ±‚Ä≤
cW‚Ä≤y
2œÉ2e

√ó
"""

W‚Ä≤W + Œ£‚àí1‚àí1 œÉ2
e
"""
1
2 .
The joint density of the two variance components can be written explicitly
as
p

œÉ2
u, œÉ2
e|y, œÉ2
Œ≤, s2
u, ŒΩu, s2
e, ŒΩe

‚àù

œÉ2
u
‚àíq+ŒΩu+2
2

œÉ2
e
‚àín‚àíp‚àíq+ŒΩe+2
2
√ó exp
Ô£´
Ô£≠‚àí
y‚Ä≤y ‚àí5Œ±‚Ä≤
cW‚Ä≤y + ŒΩes2
e + ŒΩus2
u
œÉ2
e
œÉ2
u
2œÉ2e
Ô£∂
Ô£∏""
W‚Ä≤W + Œ£‚àí1""‚àí1
2 .
(6.72)
It is not possible to go further in the level of marginalization. This is because
the joint distribution involves ratios between variance components, both in
the exponential expression and in the matrix Œ£, as part of the determinant.
Note that the two variance components are not independent a posteriori
(even if they are so, a priori).
6.3.4
Marginal Distribution of Location Parameters
The integral of the joint posterior density (6.58), with respect to the un-
known variance components, so as to obtain the unconditional posterior

324
6. Bayesian Analysis of Linear Models
density of the location parameters, can be represented as
p

Œ≤, u|y, œÉ2
Œ≤, s2
u, ŒΩu, s2
e, ŒΩe

‚àù

p

y|Œ≤, u, œÉ2
e

p

œÉ2
e|s2
e, ŒΩe

dœÉ2
e

√ó

p

u|œÉ2
u

p

œÉ2
u|s2
u, ŒΩu

dœÉ2
u

p

Œ≤|œÉ2
Œ≤

.
Each of the two integrals is proportional to the kernel of a multivariate-t
density of appropriate order, so one can write:
p

Œ≤, u|y, œÉ2
Œ≤, s2
u, ŒΩu, s2
e, ŒΩe

‚àù

1 + (y ‚àíXŒ≤ ‚àíZu)‚Ä≤ (y ‚àíXŒ≤ ‚àíZu)
ŒΩes2e
‚àín+ŒΩe
2
√ó

1 + u‚Ä≤A‚àí1u
ŒΩus2u
‚àíq+ŒΩu
2
p

Œ≤|œÉ2
Œ≤

.
(6.73)
This density does not have a recognizable form. The Ô¨Årst two expressions
are multivariate-t, and their product would deÔ¨Åne a poly-t density, if it
were not for the presence of p

Œ≤|œÉ2
Œ≤

.
Suppose now that œÉ2
Œ≤ goes to inÔ¨Ånity, so that, in the limit, one is in a
situation of vague prior knowledge about this location parameter. The prior
density of Œ≤ becomes Ô¨Çatter and Ô¨Çatter and, in the limit, it is proportional
to a constant. However, this uniform process is not proper, as the integral
over Œ≤ is not Ô¨Ånite. In this case, the joint distribution of Œ≤ and u is in
a poly-t form, and marginalization can be carried out one step further.
Before integration of the variance components, the joint posterior density
takes the form
p

Œ≤, u, œÉ2
e, œÉ2
u|y, s2
u, ŒΩu, s2
e, ŒΩe

‚àùp

y|Œ≤, u, œÉ2
e

p

œÉ2
e|s2
e, ŒΩe

p

u|œÉ2
u

p

œÉ2
u|s2
u, ŒΩu

,
since now the prior density of Œ≤ is Ô¨Çat. One can write
(y ‚àíXŒ≤ ‚àíZu)‚Ä≤ (y ‚àíXŒ≤ ‚àíZu) = [w (u) ‚àíXŒ≤]‚Ä≤ [w (u) ‚àíXŒ≤]
=

w (u) ‚àíX,Œ≤
‚Ä≤ 
w (u) ‚àíX,Œ≤

+

Œ≤ ‚àí,Œ≤
‚Ä≤
X‚Ä≤X

Œ≤ ‚àí,Œ≤

,
where
,Œ≤
=
(X‚Ä≤X)‚àí1 X‚Ä≤w (u)
=
(X‚Ä≤X)‚àí1 X‚Ä≤ (y ‚àíZu)

6.3 The Mixed Linear Model
325
is the ‚Äúregression‚Äù of w (u) = y‚àíZu on X. Using this in the joint posterior
density yields
p

Œ≤, u, œÉ2
e, œÉ2
u|y, s2
u, ŒΩu, s2
e, ŒΩe

‚àù

œÉ2
e
‚àín+ŒΩe+2
2
exp
Ô£±
Ô£¥
Ô£¥
Ô£≤
Ô£¥
Ô£¥
Ô£≥
‚àí

w (u) ‚àíX,Œ≤
‚Ä≤ 
w (u) ‚àíX,Œ≤

+ ŒΩes2
e
2œÉ2e
Ô£º
Ô£¥
Ô£¥
Ô£Ω
Ô£¥
Ô£¥
Ô£æ
√ó exp
Ô£Æ
Ô£ØÔ£∞‚àí

Œ≤ ‚àí,Œ≤
‚Ä≤
X‚Ä≤X

Œ≤ ‚àí,Œ≤

2œÉ2e
Ô£π
Ô£∫Ô£ª

œÉ2
u
‚àíq+ŒΩu+2
2
exp

‚àíu‚Ä≤A‚àí1u + ŒΩus2
u
2œÉ2u

.
This expression can be integrated analytically with respect to Œ≤, to obtain
p

u,œÉ2
e, œÉ2
u|y, s2
u, ŒΩu, s2
e, ŒΩe

‚àù

œÉ2
e
‚àín+ŒΩe+2
2
exp
Ô£Æ
Ô£ØÔ£∞‚àí

w (u) ‚àíX,Œ≤
‚Ä≤ 
w (u) ‚àíX,Œ≤

+ ŒΩes2
e
2œÉ2e
Ô£π
Ô£∫Ô£ª
√ó
"""(X‚Ä≤X)‚àí1 œÉ2
e
"""
1
2 
œÉ2
u
‚àíq+ŒΩu+2
2
exp

‚àíu‚Ä≤A‚àí1u + ŒΩus2
u
2œÉ2u

‚àù

œÉ2
e
‚àín‚àíp+ŒΩe+2
2
exp
Ô£Æ
Ô£ØÔ£∞‚àí

w (u) ‚àíX,Œ≤
‚Ä≤ 
w (u) ‚àíX,Œ≤

+ ŒΩes2
e
2œÉ2e
Ô£π
Ô£∫Ô£ª
√ó

œÉ2
u
‚àíq+ŒΩu+2
2
exp

‚àíu‚Ä≤A‚àí1u + ŒΩus2
u
2œÉ2u

.
(6.74)
Note now that the two variance components appear in kernels of scaled
inverted chi-square densities and that, given u, their distributions are in-
dependent. Further, the dispersion parameters can be integrated out ana-
lytically, obtaining
p

u|y, œÉ2
Œ≤, s2
u, ŒΩu, s2
e, ŒΩe

‚àù

w (u) ‚àíX,Œ≤
‚Ä≤ 
w (u) ‚àíX,Œ≤

+ ŒΩes2
e
‚àín‚àíp+ŒΩe
2
√ó

u‚Ä≤A‚àí1u + ŒΩus2
u
‚àíq+ŒΩu
2
‚àù
Ô£Æ
Ô£ØÔ£∞1 +

w (u) ‚àíX,Œ≤
‚Ä≤ 
w (u) ‚àíX,Œ≤

ŒΩes2e
Ô£π
Ô£∫Ô£ª
‚àín‚àíp+ŒΩe
2
√ó

1 + u‚Ä≤A‚àí1u
ŒΩus2u
‚àíq+ŒΩu
2
,

326
6. Bayesian Analysis of Linear Models
as the hyperparameters are constant, and can be factored out of the ex-
pression. Finally, note that
w (u) ‚àíX,Œ≤
=
y ‚àíZu ‚àíX (X‚Ä≤X)‚àí1 X‚Ä≤ (y ‚àíZu)
=
M (y ‚àíZu) ,
where M = I ‚àíX (X‚Ä≤X)‚àí1 X‚Ä≤. Using this in the marginal posterior density
of u above gives
p

u|y, œÉ2
Œ≤, s2
u, ŒΩu, s2
e, ŒΩe

‚àù

1 + (y ‚àíZu)‚Ä≤ M (y ‚àíZu)
ŒΩes2e
‚àín‚àíp+ŒΩe
2
√ó

1 + u‚Ä≤A‚àí1u
ŒΩus2u
‚àíq+ŒΩu
2
.
(6.75)
Hence, the marginal distribution of u is not in any easily recognizable form
and further marginalization, e.g., with respect to a subvector of this location
parameter, is not feasible by analytical means. However, it is possible to
draw samples from the marginal distribution of each element of u by means
of MCMC methods to be discussed in later chapters.
In summary, most of the analytical results available for a Bayesian analy-
sis of linear models under Gaussian assumptions have been presented here.
When the model contains more than one unknown variance component, it
is not possible to arrive at the fully marginal distributions of individual pa-
rameters. The analytical treatment is even more involved when the model
is nonlinear in the location parameters, or when the response variables
are not Gaussian. In the next two chapters, additional topics in Bayesian
analysis will be presented, such as the role of the prior distribution and
Bayesian tools for model comparison.

7
The Prior Distribution and
Bayesian Analysis
7.1
Introduction
In the preceding two chapters, some of the basic machinery for the develop-
ment of Bayesian probability models, with emphasis on linear speciÔ¨Åcations,
was presented. It was seen that the inferences drawn depend on the forms
of the likelihood function and of the prior distribution. A natural question
is: What is the impact of the prior on inferences? Clearly, a similar query
could be raised about the eÔ¨Äect of the likelihood function. Alternatively,
one can pose the question: How much information is contributed by the
data (or by the prior) about the quantities of interest? In this chapter, we
begin with an example that illustrates the eÔ¨Äect of the prior distribution on
inferences. Subsequently, some measures of statistical information are pre-
sented and used to quantify what is encoded in the likelihood function, and
in the prior and posterior processes. Another section describes how prior
distributions, contributing ‚Äúlittle‚Äù information relative to that contributed
by data, can be constructed. This section includes a discussion of JeÔ¨Äreys‚Äô
prior, of the maximum entropy principle, and of what is called reference
analysis. A step-by-step derivation of the associated reference prior, includ-
ing the situation in which nuisance parameters are present in the model, is
given at the end.

328
7. The Prior Distribution and Bayesian Analysis
n
ML estimate
Posterior mode
Posterior mean
5
0.2
0.2
0.286
10
0.2
0.2
0.250
20
0.2
0.2
0.227
40
0.2
0.2
0.214
TABLE 7.1. EÔ¨Äect of sample size (n) on the mean and mode of the posterior
distribution of the gene frequency: uniform prior.
7.2
An Illustration of the EÔ¨Äect
of Priors on Inferences
Suppose that one wishes to infer the frequency of a certain allele (Œ∏) in
some homogeneous population. Further, assume that all that can be stated
a priori is that the frequency is contained in the interval [0, 1] . A random
sample of n genes is drawn from the population and x copies of the allele
of interest are observed in the sample. The ML estimator of Œ∏ is x/n, and
its sample variance is Œ∏ (1 ‚àíŒ∏) /n (this can be estimated empirically by
replacing the unknown parameter by its ML estimate).
A reasonable Bayesian probability model consists of a uniform prior dis-
tribution in the said interval, plus a binomial sampling
model, with x
successes out of n independent trials. The prior distribution is centered at
1/2, and the posterior density of the gene frequency is
p (Œ∏|x, n) ‚àùŒ∏x (1 ‚àíŒ∏)n‚àíx .
Hence, the posterior process is the Be (x + 1, n ‚àíx + 1) distribution, with
its mode being equal to the ML estimator, and the posterior mean being
(x + 1) / (n + 2). Table 7.1 gives a sequence of posterior distributions at
increasing sample sizes; in each of these distributions the ratio of the num-
ber of successes to sample size is kept constant at 0.2, which is the ML
estimate of the gene frequency. Note that the posterior mean gets closer
to the ML estimate as n increases; in the limit, the posterior mean tends
toward x/n, suggesting that the information from the sample overwhelms
the prior, asymptotically.
Assume now that the prior distribution is the Be (11, 11) process. The
posterior density is now
p (Œ∏|x, n) ‚àùŒ∏x+11‚àí1 (1 ‚àíŒ∏)n+11‚àíx‚àí1 ,
so the corresponding distribution is Be (x + 11, n + 11 ‚àíx) , having mean
(x + 11) / (n + 22), and mode (x + 10) / (n + 20). Note that as n ‚Üí‚àû,
both the mean and mode go towards x/n. Table 7.2 gives a sequence of
posterior distributions similar to that displayed in Table 7.1. Again, both
the posterior mean and mode move toward the ML estimator as sample size
increases, but the inÔ¨Çuence of the beta prior assigned here is more marked

7.2 An Illustration of the EÔ¨Äect of Priors on Inferences
329
n
ML estimate
Posterior mode
Posterior mean
5
0.2
0.440
0.444
10
0.2
0.400
0.406
20
0.2
0.350
0.357
50
0.2
0.286
0.292
TABLE 7.2. EÔ¨Äect of sample size (n) on the mean and mode of the posterior
distribution of the gene frequency: beta prior.
than that of the uniform distribution in the preceding case. The reason for
this is that the Be (11, 11) distribution is fairly sharp and assigns small
prior probability to values of Œ∏ smaller than 1
4. If n = 1000 and x = 200,
thus keeping the ML estimator at 1
5, the posterior mean and mode would be
both approximately equal to 0.207, verifying the ‚Äúasymptotic domination‚Äù
of the prior by the likelihood function.
The dissipation of the inÔ¨Çuence of the prior as sample size increases was
already seen in a discrete setting, when Bayes theorem was introduced.
The result can be coined in a more general form as follows. Suppose that
n independent draws are made from the same distribution [yi|Œ∏] , and let
the prior density be p (Œ∏|H) , where H could be a set of hyperparameters.
The posterior density of Œ∏ is then
p (Œ∏|y1, y2, . . . , yn, H) ‚àùp (Œ∏|H)
n
-
i=1
p (yi|Œ∏)
‚àùexp
# n

i=1

log [p (yi|Œ∏)] + log [p (Œ∏|H)]
n
$
.
Then, as n increases,
log [p (yi|Œ∏)] + log [p (Œ∏|H)]
n
‚Üílog [p (yi|Œ∏)] ,
and
p (Œ∏|y1, y2, ..., yn, H) ‚Üí
exp
 n
i=1
log [p (yi|Œ∏)]
%

exp
 n
i=1
log [p (yi|Œ∏)]
%
dŒ∏
,
which is the normalized likelihood, assuming the integral exists. Hence, the
contribution of the prior to the posterior becomes less and less important as
the sample size grows. This can be expressed by saying that, given enough
data, the prior is expected to have a small inÔ¨Çuence on inferences about Œ∏.
This is examined in more detail in the following section.

330
7. The Prior Distribution and Bayesian Analysis
7.3
A Rapid Tour of Bayesian Asymptotics
Intuitively, the beliefs about a parameter Œ∏, reÔ¨Çected in its posterior distri-
bution, should become more concentrated about the true parameter value
Œ∏0 as the amount of information increases (loosely speaking, as the num-
ber of observations n ‚Üí‚àû). In this section, a summary of some rele-
vant asymptotic results is presented, following Bernardo and Smith (1994)
closely. The arguments and results parallel those for asymptotic theory in
ML estimation, as given in Chapters 3 and 4. Hence, only the essentials
are presented.
7.3.1
Discrete Parameter
Suppose Œ∏ takes one of several mutually exclusive and exhaustive states, so
its prior distribution is discrete. The posterior distribution is
p (Œ∏i|y) =
p (y|Œ∏i) p (Œ∏i)

i
p (y|Œ∏i) p (Œ∏i),
where the sum is taken over all possible states of the parameter. Dividing
both numerator and denominator by the likelihood conferred by the data
to the true parameter value Œ∏0, one gets
p (Œ∏i|y) =
p(y|Œ∏i)
p(y|Œ∏0)p (Œ∏i)

i
p(y|Œ∏i)
p(y|Œ∏0)p (Œ∏i)
.
Assuming the observations are independent, given the parameter, the pos-
terior is expressible as
p (Œ∏i|y) =
exp

log p(y|Œ∏i)
p(y|Œ∏0) + log p (Œ∏i)


i
exp

log p(y|Œ∏i)
p(y|Œ∏0) + log p (Œ∏i)

=
exp

n
j=1
log p(yj|Œ∏i)
p(yj|Œ∏0) + log p (Œ∏i)


i
exp

n
j=1
log p(yj|Œ∏i)
p(yj|Œ∏0) + log p (Œ∏i)
.
Now, as n ‚Üí‚àû,
lim
n‚Üí‚àû
1
n
n

j=1
log p (yj|Œ∏i)
p (yj|Œ∏0)
=

log p (yj|Œ∏i)
p (yj|Œ∏0)p (yj|Œ∏0) dy
=
‚àí

log p (yj|Œ∏0)
p (yj|Œ∏i) p (yj|Œ∏0) dy.

7.3 A Rapid Tour of Bayesian Asymptotics
331
The integral immediately above is called the Kullback‚ÄìLeibler distance or
the discrepancy between two distributions, which is shown in Section 7.4.7
to be 0 when Œ∏i = Œ∏0 and positive otherwise. Hence, as n ‚Üí‚àû,
1
n
n

j=1
log p (yj|Œ∏i)
p (yj|Œ∏0) ‚Üí‚àí

log p (yj|Œ∏0)
p (yj|Œ∏i) p (xj|Œ∏0) dy
=

0,
for Œ∏i = Œ∏0,
‚àí‚àû,
otherwise.
Hence
lim
n‚Üí‚àûp (Œ∏i|y) = lim
n‚Üí‚àû
Ô£±
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£≤
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£≥
exp

n
j=1
log p(yj|Œ∏i)
p(yj|Œ∏0) + log p (Œ∏i)


i
exp

n
j=1
log p(yj|Œ∏i)
p(yj|Œ∏0) + log p (Œ∏i)

Ô£º
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£Ω
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£æ
= 0,
for all Œ∏i Ã∏= Œ∏0,
and is equal to 1 for Œ∏i = Œ∏0. This indicates that, as sample size grows,
the posterior distribution becomes more and more concentrated around Œ∏0
and that, in the limit, all probability mass is placed on the true value. It
can be shown that if one fails to assign positive prior probability to the
true state of the parameter, the limiting posterior concentrates around the
parameter value producing the smallest Kullback‚ÄìLeibler discrepancy with
the true model (Bernardo and Smith, 1994).
7.3.2
Continuous Parameter
The posterior density of a parameter vector can be represented as
p (Œ∏|y) ‚àùp (y|Œ∏) p (Œ∏) ‚àùexp [log p (y|Œ∏) + log p (Œ∏)] .
Let ,Œ∏ be the prior mode and let 5Œ∏n be the ML estimator based on a sam-
ple of size n. Taylor series expansions of the log-prior density and of the
likelihood function yield (recall that the gradient is null when evaluated at
the corresponding modal value):
log p (Œ∏) ‚âàlog p

,Œ∏

‚àí1
2

Œ∏ ‚àí,Œ∏
‚Ä≤ ,H

Œ∏ ‚àí,Œ∏

and
log p (y|Œ∏) ‚âàlog p

5Œ∏n

‚àí1
2

Œ∏ ‚àí5Œ∏n
‚Ä≤ 5H

Œ∏ ‚àí5Œ∏n

,
where
,H = ‚àí‚àÇ2 log p (Œ∏)
‚àÇŒ∏ ‚àÇŒ∏‚Ä≤
""""
Œ∏=Œ∏

332
7. The Prior Distribution and Bayesian Analysis
and
5H = ‚àí‚àÇ2 log p (y|Œ∏)
‚àÇŒ∏ ‚àÇŒ∏‚Ä≤
""""
Œ∏=Œ∏n
is the observed information matrix. These two matrices of second deriva-
tives, called Hessians, can be interpreted as the ‚Äúprecision‚Äù matrices of
the prior distribution and of the sampling model, respectively. Under the
usual regularity conditions, the remainder of the approximation to the log-
likelihood is small in large samples, since the likelihood is expected to be
sharp and concentrated near the true value of the parameter vector. Fur-
ther, recall that in large samples the ML estimate 5Œ∏n is expected to be
close to its true value Œ∏0. Using the Taylor series approximations to the
prior and the likelihood, the posterior density is, roughly,
p (Œ∏|y) ‚àùexp
9
log

p

5Œ∏n

p

,Œ∏

‚àí1
2

Œ∏ ‚àí5Œ∏n
‚Ä≤ 5H

Œ∏ ‚àí5Œ∏n

+

Œ∏ ‚àí,Œ∏
‚Ä≤ ,H

Œ∏ ‚àí,Œ∏
%
.
After retaining only the terms that involve Œ∏ one gets
p (Œ∏|y) ‚àùexp

‚àí1
2

Œ∏ ‚àí5Œ∏n
‚Ä≤ 5H

Œ∏ ‚àí5Œ∏n

+

Œ∏ ‚àí,Œ∏
‚Ä≤ ,H

Œ∏ ‚àí,Œ∏
%
.
(7.1)
The two quadratic forms on the parameter vector can be combined, via the
formulas used repeatedly in the preceding chapter, to obtain

Œ∏ ‚àí5Œ∏n
‚Ä≤ 5H

Œ∏ ‚àí5Œ∏n

+

Œ∏ ‚àí,Œ∏
‚Ä≤ ,H

Œ∏ ‚àí,Œ∏

=

Œ∏ ‚àí55Œ∏n
‚Ä≤ 
5H + ,H
 
Œ∏ ‚àí55Œ∏n

+

5Œ∏n ‚àí,Œ∏
‚Ä≤ 5H

5H + ,H
‚àí1 ,H

5Œ∏n ‚àí,Œ∏

,
(7.2)
with
55Œ∏n =

5H + ,H
‚àí1 
5H5Œ∏n + ,H,Œ∏

.
(7.3)
Employing (7.2) in (7.1) and keeping only the terms involving Œ∏ gives
p (Œ∏|y) ‚àùexp
#
‚àí1
2

Œ∏ ‚àí55Œ∏n
‚Ä≤ 
5H + ,H
 
Œ∏ ‚àí55Œ∏n
$
.
(7.4)
Hence, under regularity conditions, the posterior distribution is asymptot-
ically normal with mean 55Œ∏n and covariance matrix

5H + ,H
‚àí1
. We write
Œ∏|y ‚àºN

55Œ∏n,

5H + ,H
‚àí1
.
(7.5)

7.3 A Rapid Tour of Bayesian Asymptotics
333
Note that the approximation to the posterior mean is a matrix weighted
average of the prior mode and of the ML estimator, with the weights being
the corresponding precision matrices. The matrix sum 5H + ,H is a measure
of posterior precision and its inverse reÔ¨Çects the posterior variances and
covariances among elements of Œ∏.
Another approximation can be obtained as follows. For large n, the pre-
cision matrix 5H will tend to be much larger than the prior precision matrix
,H. Then, roughly, 5H + ,H ‚âà5H and 55Œ∏n ‚âà5Œ∏n. Since the prior precision ma-
trix is ‚Äúdominated‚Äù by the Hessian corresponding to the sampling model,
the ML estimator receives a much larger weight, so the mean of the ap-
proximate posterior distribution will be very close to the ML estimator.
Hence, for large n,
Œ∏|y ‚àºN

5Œ∏n, 5H‚àí1
.
(7.6)
For conditionally i.i.d. observations,
‚àí‚àÇ2 log p (y|Œ∏)
‚àÇŒ∏ ‚àÇŒ∏‚Ä≤
= ‚àí
n

i=1
‚àÇ2 log p (yi|Œ∏)
‚àÇŒ∏ ‚àÇŒ∏‚Ä≤
= n

1
n
n

i=1
‚àí‚àÇ2 log p (yi|Œ∏)
‚àÇŒ∏ ‚àÇŒ∏‚Ä≤

.
By the weak law of large numbers, the term in square brackets, which is
equal to 1
n 5H, converges in probability (symbolized ‚Äú
p‚Üí‚Äù) to its expecta-
tion:
1
n
n

i=1
‚àí‚àÇ2 log p (yi|Œ∏)
‚àÇŒ∏‚àÇŒ∏‚Ä≤
p‚ÜíE

‚àí‚àÇ2 log p (y|Œ∏)
‚àÇŒ∏‚àÇŒ∏‚Ä≤

= I1 (Œ∏) ,
which is Fisher‚Äôs information measure for a sample of size 1. Hence,
5H
p‚ÜínI1 (Œ∏) = I (Œ∏) .
That is, the observed information matrix converges in probability to Fisher‚Äôs
information matrix. Then, an alternative approximation to the posterior
distribution is
Œ∏|y ‚àºN

5Œ∏n, [nI1 (Œ∏)]‚àí1
.
It is important to stress that the heuristics presented above build on the
existence of regularity conditions. It is deÔ¨Ånitely not always the case that
the posterior distribution approaches normality as sample size increases.
We refer the reader to Bernardo and Smith (1994), and references therein,
for a discussion of this delicate subject.
Example 7.1
Asymptotic distribution of location parameters in a normal
model
Suppose that n observations are taken from some strain of Ô¨Åsh and that
the sampling model for some attribute is
xi|Œ∏1, œÉ2 ‚àºN

Œ∏1, œÉ2
.

334
7. The Prior Distribution and Bayesian Analysis
The residual dispersion is known. The location parameter has an uncer-
tainty distribution according to the process Œ∏1 ‚àºN (0, Œª1) , where the vari-
ance is also known. Another random sample of n observations is drawn
from some other strain according to the model
yi|Œ∏2, œÑ 2 ‚àºN

Œ∏2, œÑ 2
,
with Œ∏2 ‚àºN (0, Œª2) ; again, Œª2 is known. The joint posterior density of Œ∏1
and Œ∏2 is
p

Œ∏1, Œ∏2|œÉ,2 Œª1, œÑ 2, Œª2, x, y

‚àù
 n
-
i=1
p

xi|Œ∏1, œÉ2
p (Œ∏1|Œª1)
  n
-
i=1
p

yi|Œ∏2, œÑ 2
p (Œ∏2|Œª2)

.
It can be seen that Œ∏1 and Œ∏2 are independently distributed, a posteriori,
for any sample size. Since the prior and the posterior are normal, it follows
that the posterior distribution of each Œ∏ is normal as well. For example, the
posterior distribution of Œ∏1 has mean
E

Œ∏1|Œª1, œÉ2, x

=

n + œÉ2
Œª1
‚àí1
nx
=

1 + œÉ2
nŒª1
‚àí1
x
and variance
V ar

Œ∏1|Œª1, œÉ2, x

=

n + œÉ2
Œª1
‚àí1
œÉ2.
As n ‚Üí‚àû, the mean and variance tend to x and œÉ2/n, respectively. The
asymptotic joint posterior distribution of Œ∏1 and Œ∏2 can then be written as
Œ∏1, Œ∏2|œÉ2, œÑ 2, x, y ‚àºN


x
y

,

œÉ2
0
0
œÑ 2

n‚àí1

.
The same result is arrived at by employing (7.6) since x and y are the
ML estimators of Œ∏1 and Œ∏2, respectively. In this example the observed and
expected information matrices are equal because the variances are assumed
to be known.
‚ñ†
7.4
Statistical Information and Entropy
7.4.1
Information
A readable introduction to the concept of information theory in probability
is in Applebaum (1996), and some of the ideas are adapted to our context

7.4 Statistical Information and Entropy
335
hereinafter. Consider the following two statements:
(1) In two randomly mated, randomly selected lines of Ô¨Ånite size derived
from the same population, mean values and gene frequencies diÔ¨Äer after 50
generations.
(2) A new mutant is found at generation 2 with frequency equal to 15%.
Arguably, the second statement conveys more information than the Ô¨Årst,
because it involves an event having a very low prior probability. The Ô¨Årst
statement should not evoke surprise, as the corresponding event is expected
in the light of well-established theory. Let E be an event and let p (E) be
its probability. Then using the above line of argument, the information
contained in the observed event, I (E) , should be a decreasing function of
its probability.
There are three conditions that an information measure must meet:
(1) It must be positive.
(2) The information from observing two events jointly must be at least as
large as that from the observation of any of the single elementary events.
For example, suppose there are two unlinked loci in a population in Hardy‚Äì
Weinberg equilibrium and that the probability of observing an AA genotype
is p (AA) , while that of observing Bb is p (Bb) . As the two events are
independent, one has that
p (AA ‚à©Bb) = p (AA) p (Bb) .
Since p (AA ‚à©Bb) ‚â§p (AA) and p (AA ‚à©Bb) ‚â§p (Bb) , it follows that the
information content I (AA ‚à©Bb) ‚â•I (AA) and, similarly, I (AA ‚à©Bb) ‚â•
I (Bb) .
(3) For independent events E1 and E2 (Applebaum, 1996):
I (E1 ‚à©E2) = I (E1) + I (E2) .
To illustrate, suppose a machine reads: ‚Äúgenotype at Ô¨Årst locus is AA‚Äù,
so the information is I (AA) , while another machine yields ‚Äúgenotype at
second locus is Bb‚Äù, with the information being I (Bb) . Hence, the infor-
mation is I (AA) + I (Bb) . On the other hand, if the machine reads both
genotypes simultaneously, we would not have information over and above
I (AA) + I (Bb) .
From information theory, the function satisfying the three conditions
given above must have the form
I (E) = ‚àíK loga [p (E)] ,
(7.7)
where K and a are positive constants. Since 0 ‚â§p (E) ‚â§1, it follows
that this information measure is positive, as K is positive, thus meeting
the Ô¨Årst condition. If the event is certain, I (E) = 0, and no information
is gained from knowing that the event took place (this would be known
beforehand). On the other hand, if the event is impossible, p (E) = 0, the

336
7. The Prior Distribution and Bayesian Analysis
information measure is not Ô¨Ånite; this is viewed as reasonable, indicating
the impossibility of obtaining information from events that do not occur.
Second, for independent events
I (E1 ‚à©E2)
=
‚àíK loga [p (E1 ‚à©E2)]
=
‚àíK loga [p (E1)] ‚àíK loga [p (E2)]
=
I (E1) + I (E2) ,
satisfying the second and third conditions. Standard choices for the con-
stants are K = 1 and a = 2, and the units in which information is measured
are called ‚Äúbits‚Äù. For example, suppose one crosses genotypes Aa and aa;
the oÔ¨Äspring can be either Aa or aa, with equal probability. Hence, the
information resulting from observing one of the two alternatives is:
I (Aa) = I (aa) = ‚àílog2

2‚àí1
= 1 bit.
Example 7.2
Cross between double heterozygotes
Suppose that the cross AaBb √ó AaBb is made, with the two loci unlinked,
as before. We calculate the information accruing from observation of each
of the following events:
1.
progeny is aa;
2.
progeny is Bb;
3.
an oÔ¨Äspring aabb is observed;
4.
the oÔ¨Äspring is Aabb; and
5.
the outcome of the cross is AaBb.
Using Mendel‚Äôs rules:
1.
p (aa) = p (bb) = 1
4, so I (aa) = I (bb) = ‚àílog2

2‚àí2
= 2 bits.
2.
p (Bb) = 1
2, so I (Bb) = ‚àílog2

2‚àí1
= 1 bit.
3.
p (aabb) =
1
16, so I (aabb) = ‚àílog2

2‚àí4
= 4 bits. Note that
I (aabb) = I (aa) + I (bb) since the two events are independent.
4.
p (Aabb) = 1
2
1
4 = 1
8 and I (Aabb) = ‚àílog2

2‚àí3
= 3 bits.
5.
p (AaBb) = 1
4 and I (AaBb) = ‚àílog2

2‚àí2
= 2 bits.
‚ñ†
In the continuous case, heuristically, if one replaces ‚Äúevent‚Äù by ‚Äúobserved
data y‚Äù, the information measure becomes
I (y)
=
‚àílog2 [p (y|Œ∏)]
=
‚àí
log p (y|Œ∏)
log (2)

=
0.69315 ‚àílog p (y|Œ∏) ,

7.4 Statistical Information and Entropy
337
where p (y|Œ∏) is the density function indexed by parameter Œ∏. Since 0.69315
is a constant, it can be dropped, as it is convenient to work with natural
logarithms (that is, any calculation of information should be increased by
0.69315 to have the ‚Äúcorrect‚Äù number of bits). Further, for n data points
drawn independently from the same distribution, the information would be
I (y) = 0.69315 ‚àí
n

i=1
log p (yi|Œ∏) .
(7.8)
For example, if yi ‚àºNIID

¬µ, œÉ2
, the information in a sample of size n
would be
I (y) = 0.69315 + n
2 log

2œÄœÉ2
+ 1
2
n

i=1

yi ‚àí¬µ
œÉ
2
,
so information is related to the ‚Äúdeviance‚Äù n
i=1
 yi‚àí¬µ
œÉ
2 .
7.4.2
Entropy of a Discrete Distribution
Since the information content of an event depends on its probability (or
density), it is technically more sensible to think in terms of information
from the distribution of a random variable. It is important to keep this
in mind, because in Bayesian analysis sometimes one wishes to Ô¨Ånd and
use a prior distribution conveying ‚Äúas little information as possible‚Äù (Box
and Tiao, 1973; Zellner, 1971; Bernardo, 1979; Berger and Bernardo, 1992).
The formal development requires Ô¨Ånding a distribution that minimizes some
information measure.
Consider one discrete random variable X having K mutually exclusive
and exhaustive states and probability distribution pi (i = 1, 2, . . . , K) (in
this section, Pr (X = xi) = p (xi), the usual notation for a p.m.f. is replaced
by pi). Since X is random, one does not know beforehand how much in-
formation will be contained in a yet-to-occur observation. Hence, I (X) is
random as well, but a property of the distribution is the mean information
H (p1, p2, ..., pK)
=
E [I (X)] = E {‚àílog [Pr (X = xi)]}
=
‚àí
K

i=1
pi log (pi) .
(7.9)
This is called the entropy of a distribution, with its name due to the fact
that this functional form appears in thermodynamics. In physics, entropy is
used to refer to the degree of randomness or disorder in processes. Shannon
(1948) coined the term information entropy to describe the tendency of
communication to become more and more distorted by noise. For example,
if some material is photocopied over again and in this process becomes illeg-
ible, the information is continually degraded. Thus, a distribution conveying

338
7. The Prior Distribution and Bayesian Analysis
minimum information, given some constraints that we wish this distribu-
tion to reÔ¨Çect (e.g., the probabilities must add up to 1), can be viewed as a
maximum entropy distribution. The role of entropy in Bayesian analysis in
connection with the elicitation of priors, is discussed later in this chapter.
The entropy function is not deÔ¨Åned when pi = 0, and the term pi log (pi)
is taken to be null in such a case (Applebaum, 1996). Since entropy involves
an average of numbers that must be at least 0, it follows that H (¬∑) ‚â•
0, with the null value corresponding to the situation where there is no
uncertainty whatsoever (so no information is gained). It follows, then, that
a situation of ‚Äúmaximum entropy‚Äù is one where ‚Äúa lot‚Äù of information is to
be gained from observation. Entropy is a measure of the prior uncertainty
of a random experiment associated with the probability distribution in
question or, alternatively, of the information gained when the outcome is
observed (Baldi and Brunak, 1998). If one is told about the outcome of
an event, the entropy is reduced from H (¬∑) to 0, so this measures the
gain in information. In some sense the concept is counterintuitive (Durbin
et al., 1998) because the more randomness, the higher the entropy and
the information. The concept becomes clearer when one thinks in terms
of a reduction of entropy after some information is received. Hence, what
matters is the diÔ¨Äerence in entropy before and after observing data; this
diÔ¨Äerence can be interpreted as the informational content provided by a
set of observations. For example, one may wish to compute the diÔ¨Äerence
in entropy between the prior and posterior distributions.
Example 7.3
Entropy of a random DNA
This is from Durbin et al. (1998): if each base (A, C, G, T) occurs equiprob-
ably within a DNA sequence, the probability of a random base is 1
4. The
entropy per base is then
‚àí
4

i=1
1
4 log2

1
4

= 2 bits.
Durbin et al. (1998) state that this can be interpreted as the number of
binary questions (with yes or no responses) needed to discover the out-
come. For example, the Ô¨Årst question would be: Is the basis a purine or a
pyrimidine?. If the answer is ‚Äúpurine‚Äù, the choice must be between A or G.
The second question is: Which is the speciÔ¨Åc base? The more uncertainty
there is, the more questions are needed to discover the outcome.
‚ñ†
Example 7.4
Entropy of a conserved position
Suppose that a DNA sequence is expected to be random so that, before
observation, the entropy is 2 bits, as in the preceding example of Durbin
et al. (1998). It is observed that in a given position the frequency of A is
0.7, whereas that of G is 0.3. Using (7.9), the entropy after observation is

7.4 Statistical Information and Entropy
339
then
‚àí
 7
10 log2
7
10 + 3
10 log2
3
10

= 0.88 bits.
The information content of the position is given by the diÔ¨Äerence in entropy
before and after observation: 2 ‚àí0.88 = 1.12 bits. The more conserved the
position, the higher its information content is.
‚ñ†
Example 7.5
Sampling of genes
Let allele A have frequency p in some population, so that all other possible
alleles appear with probability 1 ‚àíp. Suppose that n alleles are drawn at
random and that x are of the A form. The process is binomial and the
entropy of the distribution of the random variable X is
H (p) = ‚àí
n

x=0

log n! px (1 ‚àíp)n‚àíx
x! (n ‚àíx)!

n! px (1 ‚àíp)n‚àíx
x! (n ‚àíx)!
.
If n = 1, this reduces to the entropy of a Bernoulli distribution
H (p) = ‚àíp log (p) ‚àí(1 ‚àíp) log (1 ‚àíp) .
Taking derivatives with respect to p, to Ô¨Ånd the maximum value of the
entropy for this distribution,
dH (p)
dp
= ‚àílog (p) + log (1 ‚àíp) .
Setting to 0 and solving gives p = 1
2 as the gene frequency giving maximum
entropy, with the maximized entropy being equal to 1 bit when expressed
in a log2 base. Hence the gene frequency distribution producing maximum
entropy is that corresponding to the situation where allele A has the same
frequency as that of all the other alleles combined (but without making a
distinction between these).
Now consider the situation where there are three alleles with frequencies
p1, p2, and p3 = 1‚àíp1 ‚àíp2. The entropy of the gene frequency distribution
is now
H (p1, p2) = ‚àíp1 log (p1) ‚àíp2 log (p2) ‚àí(1 ‚àíp1 ‚àíp2) log (1 ‚àíp1 ‚àíp2) .
To Ô¨Ånd the maximum entropy distribution we calculate the gradients
dH (p1, p2)
dpi
= ‚àílog (pi) + log (1 ‚àíp1 ‚àíp2) ,
i = 1, 2.
Setting these derivatives to 0, one arrives at p1 = p2 = 1
3. Again, the max-
imum entropy distribution is one where the three alleles are equally likely.

340
7. The Prior Distribution and Bayesian Analysis
In general, for K states, the entropy in (7.9) has the following gradient,
after introducing the constraint that K
i=1 pi = 1,
dH (p1, p2, ..., pK‚àí1)
dpi
= ‚àílog (pi) + log (1 ‚àíp1 ‚àíp2 ‚àí¬∑ ¬∑ ¬∑ ‚àípK‚àí1) .
After setting to 0, one obtains the solution pi = 1/K for all alleles. Hence,
the maximum entropy distribution is uniform and the maximized entropy
is equal to
H

 1
K , 1
K , ..., 1
K

= ‚àí
K

i=1
1
K log

 1
K

= log (K) .
‚ñ†
The preceding example illustrates that entropy is a measure of uncer-
tainty. The entropy is null when there is complete certainty about the al-
lele to be sampled, and it is maximum when one cannot make an informed
choice about which of the allelic states is more likely. Further, for a num-
ber of states K < M, log (K) < log (M) , which implies that the entropy
(uncertainty) grows monotonically with the number of choices that can be
made.
7.4.3
Entropy of a Joint and Conditional Distribution
Suppose that the pair of random variables (X, Y ) has a joint distribution
with joint probabilities pxy, (x = 1, 2, . . . , m, y = 1, 2, . . . , n) . The entropy
of the joint distribution (Applebaum, 1996) is
H (p11, p12, . . . , pmn) = ‚àí
m

x=1
n

y=1
pxy log (pxy)
= ‚àí
m

x=1
n

y=1
pxpy|x log

pxpy|x

.
(7.10)
Further
H (p11, p12, . . . , pmn) = ‚àí
m

x=1
n

y=1
pxpy|x

log (px) + log

py|x

= ‚àí
m

x=1
px log (px)
n

y=1
py|x ‚àí
m

x=1
 n

y=1
py|x log

py|x


px
= H (px) +
m

x=1
H

py|x

px = H (px) + H

py|x

,
(7.11)

7.4 Statistical Information and Entropy
341
where px is the vector of probabilities of the marginal distribution of X and
py|x is the vector of probabilities of the conditional distribution of Y given
X. This indicates that the entropy of the joint distribution is the sum of two
components: the entropy of the distribution of X and the average H

py|x

of the conditional entropies H

py|x

, taken with respect to the distribution
of X. The second term provides a measure of the uncertainty about Y
knowing that X has been realized, but without being able to state what its
value is; on the other hand, H

py|x

measures the uncertainty when one
knows the value taken by X. Note that if X and Y are independent
H (p11, p12, . . . , pmn) = ‚àí
m

x=1
n

y=1
pxpy [log (px) + log (py)]
= ‚àí
m

x=1
px log (px)
n

y=1
py ‚àí
n

y=1
py log (py)
m

x=1
px
= H (px) + H (py) ,
(7.12)
where py is the vector of probabilities of the distribution of Y.
7.4.4
Entropy of a Continuous Distribution
The entropy of a continuous distribution (Shannon, 1948; Jaynes, 1957) is
deÔ¨Åned to be
H [p (y|Œ∏)] = ‚àí

¬∑ ¬∑ ¬∑

log [p (y|Œ∏)] p (y|Œ∏) dy,
(7.13)
where p (y|Œ∏) is the density function of the random vector y, indexed by
a parameter Œ∏. Note that entropy is not invariant under transformation.
Suppose the random variable is scalar, and that one considers the one-to-
one change of variables z = f (y), where z increases monotonically with y.
Then
p (z|Œ∏) = p

f ‚àí1 (z) |Œ∏
 df ‚àí1 (z)
dz
.
The entropy of the distribution of z becomes
H [p (z|Œ∏)] = ‚àí

log

p

f ‚àí1 (z) |Œ∏
 df ‚àí1 (z)
dz
%
p

f ‚àí1 (z) |Œ∏
 df ‚àí1 (z)
dz
dz
= ‚àí

log

p

f ‚àí1 (z) |Œ∏

p

f ‚àí1 (z) |Œ∏
 df ‚àí1 (z)
dz
dz
‚àí

log
df ‚àí1 (z)
dz

p

f ‚àí1 (z) |Œ∏
 df ‚àí1 (z)
dz
dz
= ‚àí

{log [p (y|Œ∏)]} p (y|Œ∏) dy ‚àí

log
df ‚àí1 (z)
dz

p (z|Œ∏) dz

342
7. The Prior Distribution and Bayesian Analysis
= H [p (y|Œ∏)] ‚àíEz

log
df ‚àí1 (z)
dz
%
.
This indicates that the information content is not invariant even under a
one-to-one, monotonic, transformation, putting in question the usefulness
of entropy as a measure of uncertainty in the continuous case (Bernardo and
Smith, 1994). In Section 7.4.7 it is shown that a transformation invariant
measure is provided by the relative entropy.
Example 7.6
Entropy of a uniform distribution
Suppose Y is a scalar variable distributed uniformly in the interval (a, b) ,
so its density is 1/ (b ‚àía). Using (7.13) yields
H [p (y|a, b)]
=
‚àí
b

a

log

1
b ‚àía

1
b ‚àíady
=
log (b ‚àía) .
When dealing with discrete random variables, it was pointed out earlier
that entropy is at least null. This does not always carry to the continuous
case (Applebaum, 1996). For example, note that if the diÔ¨Äerence between
the bounds b ‚àía is < 1, then the entropy would be negative.
‚ñ†
Example 7.7
Entropy of a normal distribution
Let now Y ‚àºN

¬µ, œÉ2
, so
H

p

y|¬µ, œÉ2
= ‚àí
 #
log

1
‚àö
2œÄœÉ2 exp

‚àí(y ‚àí¬µ)2
2œÉ2
$
√ó
1
‚àö
2œÄœÉ2 exp

‚àí(y ‚àí¬µ)2
2œÉ2

dy
= ‚àílog

1
‚àö
2œÄœÉ2
 
1
‚àö
2œÄœÉ2 exp

‚àí(y ‚àí¬µ)2
2œÉ2

dy
+
1
2œÉ2

(y ‚àí¬µ)2
1
‚àö
2œÄœÉ2 exp

‚àí(y ‚àí¬µ)2
2œÉ2

dy
= 1
2

1 + log

2œÄœÉ2
.
Note that entropy increases with œÉ2. Consider now the standardized multi-
variate normal distribution y ‚àºNn (0, R) , where R is a correlation matrix

7.4 Statistical Information and Entropy
343
having all oÔ¨Ä-diagonal elements equal to œÅ. Then
H [p (y|0, R)] = ‚àí
 #
log

1
(2œÄ)
n
2 |R|
1
2 exp ‚àí

y‚Ä≤R‚àí1y
2
$
√ó
1
(2œÄ)
n
2 |R|
1
2 exp ‚àí

y‚Ä≤R‚àí1y
2

dy
= 1
2 [log (2œÄ)n + log |R|] + E

y‚Ä≤R‚àí1y
2

= 1
2

log (2œÄ)n + log |R| + trR‚àí1V ar (y)

= 1
2 [log (2œÄ)n + log |R|+n] .
Now, using results in Searle et al. (1992),
|R| = (1 ‚àíœÅ) [1 + (n ‚àí1) œÅ] ,
and employing this in the preceding yields
H [p (y|0, R)] = 1
2 {log (2œÄ)n + log (1 ‚àíœÅ) + log [1 + (n ‚àí1) œÅ] + n} .
When œÅ = 0, the entropy is equal to
H [p (y|0, R)] = n
2 [1 + log (2œÄ)] ,
which is n times larger than the entropy of a univariate standard normal
distribution.
‚ñ†
Example 7.8
Entropy in a truncated normal model
Consider calculating the entropies of the prior, likelihood, and posterior
distributions in a model where the mean of a normal distribution, ¬µ, is
to be inferred; the variance is known. Suppose that n observations are
collected and that these are i.i.d. as N

¬µ, œÉ2
. The p.d.f. of an observation
is
p

yj|¬µ, œÉ2
=
1
‚àö
2œÄœÉ2 exp

‚àí1
2œÉ2 (yj ‚àí¬µ)2

,
where j = 1, 2, . . . , n identiÔ¨Åes the data point. Assume that the prior dis-
tribution of ¬µ is uniform between boundaries a and b. As in Example 7.6,
the entropy of the prior distribution with density p0 is
H (p0) = ‚àí

log (po) po d¬µ = log (b ‚àía) .
Thus, the prior entropy increases with the distance between boundaries a
and b. This means that the prior uncertainty about the values of ¬µ increases
as the boundaries are further apart.

344
7. The Prior Distribution and Bayesian Analysis
The entropy of the likelihood for a sample of size n is
H

p

y|¬µ, œÉ2
= ‚àí

¬∑ ¬∑ ¬∑

log

p

y|¬µ, œÉ2
p

y|¬µ, œÉ2
dy
= ‚àí

¬∑ ¬∑ ¬∑

log
#

2œÄœÉ2‚àín
2 exp

‚àí1
2œÉ2
n

i=1
(yi ‚àí¬µ)2
$
p

y|¬µ, œÉ2
dy
= n
2

1 + log

2œÄœÉ2
.
Hence, the entropy of the likelihood increases as sample size increases. This
is because the joint density becomes smaller and smaller, so one gets more
‚Äúsurprised‚Äù, i.e., more information accrues, as more data are observed.
The density of the posterior distribution of ¬µ is
p

¬µ|œÉ2, a, b, y

‚àùexp

‚àín (¬µ ‚àíy)2
2œÉ2

I (a < ¬µ < b) ,
where y is the mean of all observations and I (¬∑) is an indicator variable
denoting the region where the parameter is allowed to take density. Hence,
the posterior distribution is a truncated normal process between a and b.
In the absence of truncation
¬µ|a, b, y ‚àºN

y, œÉ2/n

.
Now the normalized posterior density is
p

¬µ|œÉ2, a, b, y

=
exp

‚àín(¬µ‚àíy)2
2œÉ2

b
a
exp

‚àín(¬µ‚àíy)2
2œÉ2

d¬µ
=
exp

‚àín(¬µ‚àíy)2
2œÉ2

>
2œÄœÉ2/n

Œ¶

b‚àíy
œÉ/‚àön

‚àíŒ¶

a‚àíy
œÉ/‚àön
,
(7.14)
with the integration constant being
cn =
>
2œÄœÉ2/n

Œ¶

 b ‚àíy
œÉ/‚àön

‚àíŒ¶

 a ‚àíy
œÉ/‚àön
%‚àí1
.
(7.15)
If b = ‚àû, so that the posterior takes nonnull density only between a and
‚àû, standard results from truncation selection in quantitative genetics (e.g.,
Falconer and Mackay, 1996) give
E

¬µ|œÉ2, a, b, y

= y +
œÜ

a‚àíy
œÉ/‚àön

1 ‚àíŒ¶

a‚àíy
œÉ/‚àön
 œÉ
‚àön = Œ∑,

7.4 Statistical Information and Entropy
345
and
V ar

¬µ|œÉ2, a, b, y

= œÉ2
n
Ô£±
Ô£≤
Ô£≥1 ‚àí
œÜ

a‚àíy
œÉ/‚àön

1 ‚àíŒ¶

a‚àíy
œÉ/‚àön

Ô£Æ
Ô£∞
œÜ

a‚àíy
œÉ/‚àön

1 ‚àíŒ¶

a‚àíy
œÉ/‚àön
 ‚àía ‚àíy
œÉ/‚àön
Ô£π
Ô£ª
Ô£º
Ô£Ω
Ô£æ= Œ≥,
where œÜ (¬∑) and Œ¶ (¬∑) are the standard normal density and distribution
functions, respectively.
The entropy of the truncated normal posterior distribution is then
H

p

¬µ|œÉ2, a, b, y

= ‚àí
b

a
log

p

¬µ|œÉ2, a, b, y

p

¬µ|œÉ2, a, b, y

d¬µ
=
n
2œÉ2
b

a
(¬µ ‚àíy)2 p

¬µ|œÉ2, a, b, y

d¬µ
+ log
>
2œÄœÉ2/n

Œ¶

 b ‚àíy
œÉ/‚àön

‚àíŒ¶

 a ‚àíy
œÉ/‚àön
%
.
The expectation to be computed can be written as
E¬µ|œÉ2,a,b,y (¬µ ‚àíy)2 =
b

a
(¬µ ‚àíy)2 p

¬µ|œÉ2, a, b, y

d¬µ
=
b

a
[(¬µ ‚àíŒ∑) + (Œ∑ ‚àíy)]2 p

¬µ|œÉ2, a, b, y

d¬µ
=
b

a
(¬µ ‚àíŒ∑)2 p

¬µ|œÉ2, a, b, y

d¬µ + (Œ∑ ‚àíy)2 = Œ≥ + (Œ∑ ‚àíy)2 ,
(7.16)
with the last term resulting because the expected value of ¬µ ‚àíŒ∑ is 0 under
the posterior distribution. Recall that Œ∑ and Œ≥ are the mean and variance
of the posterior distribution, respectively. Now, using this in the last ex-
pression for entropy given above, it turns out that
H

p

¬µ|œÉ2, a, b, y

=
n

Œ≥ + (Œ∑ ‚àíy)2
2œÉ2
+ log
>
2œÄœÉ2/n + log

Œ¶

 b ‚àíy
œÉ/‚àön

‚àíŒ¶

 a ‚àíy
œÉ/‚àön

.
The entropy of the posterior density goes to 0 as sample size goes to inÔ¨Ånity.
Algebraically, the Ô¨Årst and second terms go to ‚àûand ‚àí‚àûas n ‚Üí‚àû, so

346
7. The Prior Distribution and Bayesian Analysis
they cancel out. As n ‚Üí‚àû, the third term goes to 0 because the posterior
distribution becomes a point mass in the limit, with all density assigned
to the ‚Äútrue‚Äù value of ¬µ. In the limit, there is no longer ‚Äúsurprise‚Äù, as the
true value of ¬µ is known with probability equal to 1.
‚ñ†
7.4.5
Information about a Parameter
The concept of amount of information about a parameter provided by a
sample of observations was discussed earlier in the book in connection
with likelihood inference. However, the treatment presented was heuristic,
without making use of information-theoretic arguments. Important contri-
butions to the use of information theory in statistics are Fisher (1925),
Shannon (1948), and Kullback (1968). Here an elementary introduction
to the subject is provided, following closely some of the developments in
Kullback (1968).
Kullback‚Äôs Information Measure
Suppose that an observation y is made and that one wishes to evaluate two
competing models or hypotheses Hi (i = 1, 2), each having prior probability
p (Hi) . For example, one of the hypotheses could be that the observation
belongs to some distribution. The posterior probability of hypothesis 1
being true is
p (H1|y) =
p (y|H1) p (H1)
p (y|H1) p (H1) + p (y|H2) p (H2),
where p (y|Hi) is the density of the observation under hypothesis Hi and
p (Hi) is the prior probability of Hi being true. The logarithm of the ratio
of posterior probabilities, or posterior log-odds ratio, is then
log p (H1|y)
p (H2|y) = log p (H1)
p (H2) + log p (y|H1)
p (y|H2),
where the ratio of densities is often known as the Bayes factor in favor of
hypothesis 1 relative to hypothesis 2. Rearranging the preceding expression
log p (y|H1)
p (y|H2) = log p (H1|y)
p (H2|y) ‚àílog p (H1)
p (H2).
(7.17)
The diÔ¨Äerence between posterior and prior log-odds ratios was interpreted
by Kullback (1968) as the information contained in y for discrimination in
favor of H1 against H2. The diÔ¨Äerence can be either negative or positive.

7.4 Statistical Information and Entropy
347
The expected information for discrimination per observation from H1 is
I (1 : 2) =

log p (y|H1)
p (y|H2)p (y|H1) dy
=
 
log p (H1|y)
p (H2|y) ‚àílog p (H1)
p (H2)

p (y|H1) dy
=
 
log p (H1|y)
p (H2|y)

p (y|H1) dy ‚àílog p (H1)
p (H2).
(7.18)
This is the diÔ¨Äerence between the mean value (taken with respect to the
distribution [y|H1]) of the logarithm of the posterior odds ratio and of the
prior log-odds ratio. Similarly, the expected information per observation
for discrimination in favor of H2 is
I (2 : 1) =

log p (y|H2)
p (y|H1)p (y|H2) dy.
(7.19)
Kullback (1968) deÔ¨Ånes the ‚Äúdivergence‚Äù between hypotheses (or distribu-
tions) as
J (1 : 2) = I (1 : 2) + I (2 : 1)
=
 
log p (H1|y)
p (H2|y)

p (y|H1) dy ‚àílog p (H1)
p (H2)+
 
log p (H2|y)
p (H1|y)

p (y|H2) dy ‚àílog p (H2)
p (H1)
=
 
log p (H1|y)
p (H2|y)

p (y|H1) dy +
 
log p (H2|y)
p (H1|y)

p (y|H2) dy.
(7.20)
This measure has most of the properties of a distance (Kullback, 1968).
All preceding deÔ¨Ånitions generalize naturally to the situation where the
observation is a vector, instead of a scalar.
Example 7.9
Correlation versus independence under normality assump-
tions
Suppose there are two random variables having distributions X ‚àºN

0, œÉ2
X

and Y ‚àºN

0, œÉ2
Y

. Let H1 be the hypothesis that the variables have a
joint normal distribution with correlation œÅ, whereas H2 will pose that their
distributions are independent. Now let p (x, y) be the bivariate normal den-
sity and let g (x) and h (y) be the corresponding marginal densities. Then,
using the Ô¨Årst representation leading to (7.18),
I (1 : 2) =
 
log
 p (x, y)
g (x) h (y)

p (x, y) dx dy
= EH1 {log [p (x, y)]} ‚àíEH1 {log [g (x)]} ‚àíEH1 {log [h (y)]} .
(7.21)

348
7. The Prior Distribution and Bayesian Analysis
Now the form of the bivariate normal density gives
log [p (x, y)] = ‚àílog (2œÄœÉXœÉY ) ‚àí1
2 log

1 ‚àíœÅ2
‚àí
1
2 (1 ‚àíœÅ2)

 x2
œÉ2
X
+ y2
œÉ2
Y
‚àí2œÅ
xy
œÉXœÉY

.
Taking expectations under H1:
EH1 {log [p (x, y)]}
= ‚àílog (2œÄœÉXœÉY ) ‚àí1
2 log

1 ‚àíœÅ2
‚àí
1
2 (1 ‚àíœÅ2)

2 ‚àí2œÅ2
= ‚àílog (2œÄœÉXœÉY ) ‚àí1
2 log

1 ‚àíœÅ2
‚àí1.
(7.22)
Likewise,
EH1 {log [g (x)]}
=
EH1

‚àílog

œÉX
‚àö
2œÄ

‚àíx2
2œÉ2
X

=
‚àílog

œÉX
‚àö
2œÄ

‚àí1
2,
(7.23)
and
EH1 {log [h (y)]} = ‚àílog

œÉY
‚àö
2œÄ

‚àí1
2.
(7.24)
Collecting (7.22) to (7.24) in (7.21)
I (1 : 2) = ‚àí1
2 log

1 ‚àíœÅ2
.
(7.25)
Hence, the expected information per observation for discrimination is a
function of the correlation coeÔ¨Écient. Its value ranges from 0 (when the
correlation is equal to 0) to ‚àûwhen its absolute value is 1.
Similarly,
I (2 : 1) =
 
log
g (x) h (y)
p (x, y)

g (x) h (y) dx dy
= EH2 {log [g (x)]} + EH2 {log [h (y)]} ‚àíEH2 {log [p (x, y)]} .
(7.26)
Now
EH2 {log [g (x)]} = ‚àílog

œÉX
‚àö
2œÄ

‚àí1
2,
EH2 {log [h (y)]} = ‚àílog

œÉY
‚àö
2œÄ

‚àí1
2,

7.4 Statistical Information and Entropy
349
and
EH2 {log [p (x, y)]} = ‚àílog (2œÄœÉXœÉY ) ‚àí1
2 log

1 ‚àíœÅ2
‚àí
1
2 (1 ‚àíœÅ2)EH2

 x2
œÉ2
X
+ y2
œÉ2
Y
‚àí2œÅ
xy
œÉXœÉY

= ‚àílog (2œÄœÉXœÉy) ‚àí1
2 log

1 ‚àíœÅ2
‚àí
1
(1 ‚àíœÅ2).
Hence,
I (2 : 1) = 1
2 log

1 ‚àíœÅ2
+
1
(1 ‚àíœÅ2) ‚àí1.
(7.27)
The mean information per observation for discrimination in favor of the
independence hypothesis ranges from 0, when the correlation is null, to ‚àû
when œÅ = 1. The divergence measure in (7.20) is, thus,
J (1 : 2)
=
I (1 : 2) + I (2 : 1)
=
œÅ2
(1 ‚àíœÅ2)
(7.28)
which ranges from 0 to ‚àûas well.
‚ñ†
Example 7.10
Comparing hypotheses
Following Kullback (1968), suppose that H2 is a set of mutually exclusive
and exhaustive hypotheses, one of which must be true, and that hypothesis
H1 is a member of such a set. We wish to calculate the information in y in
favor of H1. As stated, the problem implies necessarily that
p (H2) = p (H2|y) = 1,
this being so because the event ‚ÄúH2 is true‚Äù is certain, a priori, so it must
be certain a posteriori as well. Using this in (7.17) gives
log p (y|H1)
p (y|H2) = log p (H1|y) ‚àílog p (H1) .
If the observation y ‚Äúproves‚Äù that H1 is true, i.e., that p (H1| y) = 1, then
the information in y about H1 is
log p (y|H1)
p (y|H2) = ‚àílog p (H1) .
This implies that if the prior probability of H1 is small, the information
resulting from its veriÔ¨Åcation is large. On the other hand, if the prior prob-
ability is large, the information is small. This is reasonable on the intuitive
grounds that much is learned if an implausible proposition is found to be
true, as noted in Subsection 7.4.1.

350
7. The Prior Distribution and Bayesian Analysis
If all possible hypotheses are H1, H2, . . . , Hn, the information in y about
Hi would be ‚àílog p (Hi) . The mean value (taken over the prior distribution
of the hypotheses) is
H = ‚àí
n

i=1
[log p (Hi)] p (Hi) ,
which is the entropy of the distribution of the hypotheses.
‚ñ†
Example 7.11
Information provided by an experiment
Let Œ∏ be a parameter vector having some prior distribution with density
h (Œ∏) . Take g (y|Œ∏) to be the density of the data vector y under the sampling
model posed, and take g (y) as the marginal density of the observations,
that is, the average of the density of the sampling model over the prior
distribution of Œ∏; let h (Œ∏|y) be the resulting posterior density. Put
H1 = Œ∏ and y have a joint distribution with density f (Œ∏, y) ,
in which case the data have something to say about the parameters, and
H2 = Œ∏ and y are independent.
Using (7.21), the expected information per observation for discrimination
in favor of H1 is
I (1 : 2) =
 
log
 f (Œ∏, y)
h (Œ∏) g (y)

f (Œ∏, y) dŒ∏ dy
=
 
log
h (Œ∏|y)
h (Œ∏)

h (Œ∏|y) dŒ∏
%
g (y)dy.
(7.29)
This measure was termed ‚Äúinformation about a parameter provided by an
experiment‚Äù by Lindley (1956). Further,
I (1 : 2) =
 
[log h (Œ∏|y) ‚àílog h (Œ∏)] h (Œ∏|y) dŒ∏
%
g (y)dy
=
 
[log h (Œ∏|y)] h (Œ∏|y) dŒ∏
%
g (y)dy
‚àí
 
[log h (Œ∏)] h (Œ∏|y) g (y)dŒ∏dy
=
 
[log h (Œ∏|y)] h (Œ∏|y) dŒ∏
%
g (y)dy
‚àí

[log h (Œ∏)] h (Œ∏)

g (y|Œ∏)dydŒ∏.
(7.30)

7.4 Statistical Information and Entropy
351
In the preceding expression, note that

g (y|Œ∏)dy = 1. Further, recall that
‚àí

log h (Œ∏) h (Œ∏) dŒ∏ = H (Œ∏)
is the prior entropy, and that
‚àí

log h (Œ∏|y) h (Œ∏|y) dŒ∏ = H (Œ∏|y)
is the posterior entropy. Using these deÔ¨Ånitions in (7.30), the information
about Œ∏ in the experiment can be written as
I (1 : 2) = H (Œ∏) ‚àíEy [H (Œ∏|y)] ,
(7.31)
where the second term is the average of the posterior entropy over all pos-
sible values that the data can take, should the experiment be repeated
an inÔ¨Ånite number of times. It follows that the information in an experi-
ment can be interpreted as the decrease in entropy stemming from having
observed data.
‚ñ†
7.4.6
Fisher‚Äôs Information Revisited
It will be shown here that Fisher‚Äôs information measure (see Chapter 3) can
be derived employing the concepts of mean information for discrimination
and of divergence proposed by Kullback (1968). Although only the case
of a single parameter will be discussed, the developments extend to the
multiparameter situation in a straightforward manner.
Suppose that Œ∏ and Œ∏+‚àÜŒ∏ are neighboring points in the parameter space.
Now consider the mean information measure given in the Ô¨Årst line of (7.18),
and put
I (Œ∏ : Œ∏ + ‚àÜŒ∏) =
 
log
p (y|Œ∏)
p (y|Œ∏ + ‚àÜŒ∏)

p (y|Œ∏) dy
(7.32)
and
I (Œ∏ + ‚àÜŒ∏ : Œ∏) =
 
log p (y|Œ∏ + ‚àÜŒ∏)
p (y|Œ∏)

p (y|Œ∏ + ‚àÜŒ∏) dy.
(7.33)
The divergence, as deÔ¨Åned in (7.20), is
J (Œ∏ : Œ∏ + ‚àÜŒ∏) = I (Œ∏ : Œ∏ + ‚àÜŒ∏) + I (Œ∏ + ‚àÜŒ∏ : Œ∏)
=
 
log
p (y|Œ∏)
p (y|Œ∏ + ‚àÜŒ∏)

[p (y|Œ∏) ‚àíp (y|Œ∏ + ‚àÜŒ∏)] dy.
(7.34)
Recall that Fisher‚Äôs measure of information about Œ∏ (here, we shall use
the notation Inf (Œ∏) , to avoid confusion with the notation for mean dis-

352
7. The Prior Distribution and Bayesian Analysis
crimination) is
Inf (Œ∏) = Ey|Œ∏
d log p (y|Œ∏)
dŒ∏
2
=
 
1
p (y|Œ∏)
dp (y|Œ∏)
dŒ∏
2
p (y|Œ∏) dy.
(7.35)
Using a Taylor series, now expand the logarithm of p (y|Œ∏ + ‚àÜŒ∏) about Œ∏
as follows:
log p (y|Œ∏ + ‚àÜŒ∏) ‚âàlog p (y|Œ∏) + d log p (y|Œ∏)
dŒ∏
‚àÜŒ∏
+1
2
d2 log p (y|Œ∏)
(dŒ∏)2
(‚àÜŒ∏)2 + 1
6
d3 log p (y|Œ∏)
(dŒ∏)3
(‚àÜŒ∏)3 .
Hence
log p (y|Œ∏) ‚àílog p (y|Œ∏ + ‚àÜŒ∏)
‚âà‚àí

d log p (y|Œ∏)
dŒ∏
‚àÜŒ∏ + d2 log p (y|Œ∏)
2 (dŒ∏)2
(‚àÜŒ∏)2 + d3 log p (y|Œ∏)
6 (dŒ∏)3
(‚àÜŒ∏)3

.
(7.36)
Using this, (7.32) can now be written as
I (Œ∏ : Œ∏ + ‚àÜŒ∏) =
 
log
p (y|Œ∏)
p (y|Œ∏ + ‚àÜŒ∏)

p (y|Œ∏) dy
‚âà‚àí‚àÜŒ∏
 d log p (y|Œ∏)
dŒ∏
p (y|Œ∏) dy ‚àí(‚àÜŒ∏)2
2
 d2 log p (y|Œ∏)
(dŒ∏)2
p (y|Œ∏) dy
‚àí(‚àÜŒ∏)3
6
 d3 log p (y|Œ∏)
(dŒ∏)3
p (y|Œ∏) dy.
Under regularity conditions, as seen in Chapter 3, the Ô¨Årst term in the pre-
ceding expression vanishes because the expected value of the score of the
log-likelihood is 0. The quadratic term involves the expected value of the
second derivatives of the log-likelihood, or the negative of Fisher‚Äôs informa-
tion measure. Further, the cubic term can be neglected if the expansion is
up to second-order (provided that the expected value of the third deriva-
tives is bounded). Then, up to second-order,
I (Œ∏ : Œ∏ + ‚àÜŒ∏)
‚âà
‚àí(‚àÜŒ∏)2
2
 d2 log p (y|Œ∏)
(dŒ∏)2
p (y|Œ∏) dy
=
(‚àÜŒ∏)2 Inf (Œ∏)
2
.
(7.37)

7.4 Statistical Information and Entropy
353
Hence, the mean information for discrimination in favor of Œ∏ is propor-
tional to Fisher‚Äôs information measure and to the diÔ¨Äerence between the
neighboring values of the parameter.
Now consider the divergence in (7.34)
J (Œ∏ : Œ∏ + ‚àÜŒ∏) =
 
log
p (y|Œ∏)
p (y|Œ∏ + ‚àÜŒ∏)

[p (y|Œ∏) ‚àíp (y|Œ∏ + ‚àÜŒ∏)] dy
=
 
log p (y|Œ∏ + ‚àÜŒ∏)
p (y|Œ∏)
 p (y|Œ∏ + ‚àÜŒ∏) ‚àíp (y|Œ∏)
p (y|Œ∏)

p (y|Œ∏) dy.
(7.38)
Now
log p (y|Œ∏ + ‚àÜŒ∏)
p (y|Œ∏)
= log

1 + p (y|Œ∏ + ‚àÜŒ∏) ‚àíp (y|Œ∏)
p (y|Œ∏)

‚âàp (y|Œ∏ + ‚àÜŒ∏) ‚àíp (y|Œ∏)
p (y|Œ∏)
,
with this result following from an expansion of log (1 + x) about x = 0.
Here the role of x is played by the relative diÔ¨Äerence between densities
at the neighboring points, which is near zero, by assumption. Using the
preceding result in (7.38)
J (Œ∏ : Œ∏ + ‚àÜŒ∏) ‚âà
 p (y|Œ∏ + ‚àÜŒ∏) ‚àíp (y|Œ∏)
p (y|Œ∏)
2
p (y|Œ∏) dy.
(7.39)
An additional approximation results from noting that, by deÔ¨Ånition,
dp (y|Œ∏)
dŒ∏
= p (y|Œ∏ + ‚àÜŒ∏) ‚àíp (y|Œ∏)
‚àÜŒ∏
,
as ‚àÜŒ∏ ‚Üí0. Making use of this in (7.39)
J (Œ∏ : Œ∏ + ‚àÜŒ∏) ‚âà
 
1
p (y|Œ∏)
dp (y|Œ∏)
dŒ∏
‚àÜŒ∏
2
p (y|Œ∏) dy
= (‚àÜŒ∏)2
 d log p (y|Œ∏)
dŒ∏
2
p (y|Œ∏) dy = (‚àÜŒ∏)2 Inf (Œ∏) .
(7.40)
Therefore the discrepancy is proportional to the square of the diÔ¨Äerence
between the neighboring points and to Fisher‚Äôs information measure.
7.4.7
Prior and Posterior Discrepancy
It is instructive to evaluate how much information is gained (equivalently,
how much the posterior diÔ¨Äers from the prior) as the process of Bayesian
learning proceeds. A measure of ‚Äúdistance‚Äù between the prior and posterior
distributions is given by the Kullback‚ÄìLeibler discrepancy between the two

354
7. The Prior Distribution and Bayesian Analysis
corresponding densities. Letting p0 (Œ∏) and p1 (Œ∏) be the prior and posterior
densities, respectively, the discrepancy is (O‚ÄôHagan, 1994)
D (p0, p1)
=

¬∑ ¬∑ ¬∑
 
log p1
p0

p1 dŒ∏
=
E

log p1
p0

(7.41)
=
‚àíE

log p0
p1

,
(7.42)
with the expectation taken over the posterior distribution. Note that the
Kullback‚ÄìLeibler distance can be viewed as a relative entropy. This has
two advantages over absolute entropy. First, as shown below, this relative
entropy is always at least zero, contrary to the plain entropy of a continuous
distribution; see Example 7.6 for an illustration of this problem. Second,
relative entropy is invariant under transformation (Jaynes, 1994). Note that
a change of variables from Œ∏ to Œª (it suÔ¨Éces to consider the scalar situation
to see that this holds true), with p‚àó
1 and p‚àó
0 representing the new densities,
gives
D (p‚àó
0, p‚àó
1)
=
‚àí

log

p1 dŒ∏
dŒª
p0 dŒ∏
dŒª

p1
dŒ∏
dŒªdŒª
=
D (p0, p1) ,
since the diÔ¨Äerentials ‚Äúcancel out‚Äù.
As noted above, the discrepancy is greater than or equal to 0, being null
only when the data do not contribute any information about the parameter,
as p1 = p0 in this case. In order to show that D (¬∑) is at least 0, recall
Jensen‚Äôs inequality (Subsubsection 3.7.1 in Chapter 3), stating that for a
convex function g,
g [E(X)] ‚â§E [g(X)] .
In our context, for g being the logarithmic function, this yields
log E

p1
p0

‚â§E

log p1
p0

.
Also,
log E

p0
p1

‚â§E

log p0
p1

.
Now
log E

p0
p1

= log

¬∑ ¬∑ ¬∑
 p0
p1
p1dŒ∏

= log

¬∑ ¬∑ ¬∑

p0 dŒ∏

= log (1) = 0,

7.4 Statistical Information and Entropy
355
provided the prior is proper, so it integrates to 1. Hence, the Kullback‚Äì
Leibler discrepancy is null or positive.
When the observations are conditionally independent and sample size is
n, the Kullback‚ÄìLeibler distance can be expressed as:
D (p0, p1) =

¬∑ ¬∑ ¬∑
 
log p1
p0

p1 dŒ∏
=

¬∑ ¬∑ ¬∑

Ô£Æ
Ô£ØÔ£ØÔ£∞log
cng (Œ∏)
n7
i=1
p (yi|Œ∏)
g (Œ∏)
Ô£π
Ô£∫Ô£∫Ô£ªp1 dŒ∏
= log (cn) +

¬∑ ¬∑ ¬∑
 
log
n
-
i=1
p (yi|Œ∏)

p1 dŒ∏
= log (cn) +
n

i=1
E [log p (yi|Œ∏)] ,
(7.43)
where p0 = g (Œ∏) is the prior density and cn is the integration constant of
the posterior density based on n observations. Recall that the expectation
in (7.43) is taken over the posterior distribution.
Example 7.12
Kullback‚ÄìLeibler distance in a truncated normal model
Consider again the truncated normal model in Example 7.8. Then, from
(7.43),
D (p0, p1) = log (cn) ‚àín log

2œÄœÉ2
2
‚àí
1
2œÉ2
n

i=1
E (yi ‚àí¬µ)2 ,
(7.44)
with the expectation taken over the posterior distribution of ¬µ. From (7.16),
in the truncated normal model, one has
E (yi ‚àí¬µ)2 = Œ≥ + (Œ∑ ‚àíyi)2 .
Using this in (7.44), the Kullback‚ÄìLeibler discrepancy is
D (p0, p1) = log (cn) ‚àín log

2œÄœÉ2
2
‚àí
nŒ≥ +
n
i=1
(Œ∑ ‚àíyi)2
2œÉ2
= log (cn) ‚àín log

2œÄœÉ2
2
‚àí
n

Œ≥ + (Œ∑ ‚àíy)2
+
n
i=1
(yi ‚àíy)2
2œÉ2
.
Making use of the integration constant given in (7.15), and with b = ‚àû, the
distance between the prior and the posterior distributions can be expressed

356
7. The Prior Distribution and Bayesian Analysis
as
D (p0, p1) = ‚àí1
2 log

2œÄœÉ2/n

‚àílog

1 ‚àíŒ¶

 a ‚àíy
œÉ/‚àön

‚àí1
2n log

2œÄœÉ2
‚àí
1
2œÉ2
#
n

Œ≥ + (Œ∑ ‚àíy)2
+
n

i=1
(yi ‚àíy)2
$
.
‚ñ†
7.5
Priors Conveying Little Information
One of the criticisms often made of Bayesian inference is the potential ef-
fect that a possibly subjective, arbitrary or misguided prior can have on
inferences. Hence, eÔ¨Äorts have been directed at arriving at ‚Äúobjective pri-
ors‚Äù, by this meaning prior distributions that say ‚Äúlittle‚Äù relative to the
contributions made by the data (JeÔ¨Äreys, 1961; Box and Tiao, 1973; Zell-
ner, 1971). It has been seen already that the eÔ¨Äect of the prior dissipates
as sample size increases, so the problem is essentially one aÔ¨Äecting Ô¨Ånite
sample inferences. Further, when the observations in the sample are corre-
lated or when the model involves many parameters, it is not always clear
how large the sample should be for any inÔ¨Çuences of the prior to be over-
whelmed by the data. The problem of Ô¨Ånding objective or noninformative
priors is an extremely diÔ¨Écult one, and consensus seems to be lacking be-
tween researchers that have worked in this area. Here we will present a
short review of some of the approaches that have been suggested. For addi-
tional detail, see Bernardo (1979), Berger and Bernardo (1992), Bernardo
and Smith (1994), O‚ÄôHagan (1994), and Leonard and Hsu (1999).
7.5.1
The Uniform Prior
The most widely used (and abused) ‚Äúnoninformative‚Äù prior is that based
on the Bayes-Laplace ‚Äúprinciple of insuÔ¨Écient reason‚Äù. This states that,
in the absence of evidence to the contrary, all possibilities should have the
same prior probability (e.g., Bernardo and Smith, 1994). For example, if Œ∏
takes one of K possible values, the noninformative prior indicated by this
principle is the uniform distribution
 1
K , 1
K , . . . , 1
K
%
.
As noted in Example 7.5 this is also a maximum entropy distribution when
all that is known is that there are K mutually exclusive and exhaustive
states.

7.5 Priors Conveying Little Information
357
In the continuous case the counterpart is the continuous uniform dis-
tribution, but this leads to inconsistencies. Suppose that Œ∏ is assigned a
uniform prior distribution to convey lack of knowledge about the values
of this parameter. Then, the density of the distribution of a parameter
resulting from a monotone transformation Œª = f (Œ∏) is
p (Œª)
=
p

f ‚àí1 (Œª)
 """"
f ‚àí1 (Œª)
dŒª
""""
‚àù
""""
f ‚àí1 (Œª)
dŒª
"""" .
If the transformation is linear, the Jacobian is a constant, so it follows that
the density of Œª is uniform as well. On the other hand, if the transformation
is nonlinear, the density varies with Œª. This implies that if one claims
ignorance with respect to Œ∏, the same cannot be said about Œª. This is a
severe inconsistency.
Example 7.13
The improper uniform prior as a limiting case of the nor-
mal distribution
Let the prior be Œ∏ ‚àºN

¬µŒ∏, œÉ2
Œ∏

. If œÉ2
Œ∏ is very small, this implies that the
values are concentrated around ¬µŒ∏ or, equivalently, sharp prior knowledge.
On the other hand, if there is large prior uncertainty, as measured by a
large variance, the distribution becomes dispersed. As œÉ2
Œ∏ increases, the
normal distribution gets Ô¨Çatter and Ô¨Çatter and, in the limit, it degenerates
to a uniform distribution between ‚àí‚àûand ‚àû. This is an improper dis-
tribution, as the integral of the density is not Ô¨Ånite. However, a posterior
distribution can be proper even if the prior is improper. For example, take
yi ‚àºN

¬µ, œÉ2
, (i = 1, 2, . . . , n) to be a sample of i.i.d. random variables
with known variance. Adopting as prior
p (¬µ) ‚àùconstant,
generates an improper distribution, unless Ô¨Ånite boundaries are assigned
to the values of ¬µ. However, it is easy to verify that the posterior density
is always proper, since
p (¬µ|y1, y2, . . . , yn) ‚àùexp

‚àín
2œÉ2 (¬µ ‚àíy)2
,
integrates to
>
2œÄœÉ2/n.
‚ñ†
Example 7.14
A uniform prior distribution for heritability
Consider the following example, in the same spirit as Examples 2.20 and
2.21 of Chapter 2. Suppose that in a linear model the residual variance
is known. Heritability, h2, is deÔ¨Åned as usual, but observations have been
rescaled (by dividing by the residual standard deviation) so that one can

358
7. The Prior Distribution and Bayesian Analysis
write
h2 =
œÉ2
a
œÉ2a + œÉ2e
=
œÉ2‚àó
a
œÉ2‚àó
a + 1,
where œÉ2‚àó
a = œÉ2
a/œÉ2
e. Suppose that all we know, a priori, is that h2 is be-
tween 0 and 1. A uniform prior is assigned to this parameter, following
the principle of insuÔ¨Écient reason. Hence, the prior probability that h2 is
smaller than or equal to 1
4 is 1
4, and so is the prior probability that it is
larger than or equal to 3
4.
What is the implied prior distribution of the ratio of variances œÉ2‚àó
a ?
Note that œÉ2‚àó
a = h2/

1 ‚àíh2
, so that the parameter space is (0, ‚àû). After
calculating the Jacobian of the transformation, the density of the prior
distribution is
p

œÉ2‚àó
a

=
1
(œÉ2‚àó
a + 1)2 .
Now, the statement ‚Äúheritability is smaller than or equal to 1
5‚Äù is equivalent
to the statement that œÉ2‚àó
a
is smaller than or equal to
1
4. The resulting
probability is
Pr

œÉ2‚àó
a ‚â§1
4

=
1
4

0
1
(œÉ2‚àó
a + 1)2 dœÉ2‚àó
a
=
‚àí1
œÉ2‚àó
a + 1
""""
1
4
0
= 1
5.
Also, the probability of the statement ‚Äúheritability is smaller than or equal
to 2
5‚Äù is equivalent to
Pr

œÉ2‚àó
a ‚â§2
3

=
‚àí1
œÉ2‚àó
a + 1
""""
‚àû
2
3
= 2
5.
Hence, while the prior distribution of heritability assigns equal probability
to intervals of equal length, this is not so for the induced prior distribution
of œÉ2‚àó
a . This implies that prior indiÔ¨Äerence about heritability (as reÔ¨Çected
by the uniform prior) does not translate into prior indiÔ¨Äerence about the
scaled additive genetic variance.
‚ñ†
7.5.2
Other Vague Priors
Although the uniform distribution is probably the most widely employed
‚Äúvague‚Äù prior, other distributions that supposedly convey vague prior knowl-
edge have been suggested. For example, imagine one seeks to infer the
probability of success in a binomial distribution (Œ∏) and that a Be (Œ∏|a, b)

7.5 Priors Conveying Little Information
359
distribution is used as prior for Œ∏. If x is the number of successes after n
independent Bernoulli trials, the posterior density of Œ∏ is
p (Œ∏|n, x, a, b) ‚àùŒ∏x (1 ‚àíŒ∏)n‚àíx Œ∏a‚àí1 (1 ‚àíŒ∏)b‚àí1 .
Since a + b can be interpreted as ‚Äúthe size of a prior sample‚Äù, one can take
a = b = 0, yielding an improper prior distribution. However, one obtains
as posterior density
p (Œ∏|n, x, a = 0, b = 0) ‚àùŒ∏x‚àí1 (1 ‚àíŒ∏)n‚àíx‚àí1 ,
which is a Be (Œ∏|x, n ‚àíx) density function. For this distribution to be
proper, the two parameters must be positive. Hence, if either x = 0 or
x = n, the posterior distribution is improper. For example, if Œ∏ is small,
it is not unlikely that the posterior distribution turns out to be improper
unless n is large. The preceding illustrates that an improper prior can lead
to an improper posterior, and that caution must be exercised when using
this form of prior assignment.
Example 7.15
Haldane‚Äôs analysis of mutation rate
Haldane (1948) studied the problem of inferring mutation rates in a popula-
tion. He noted that since a mutation is a rare event, the sampling distribu-
tion of the relative frequencies (number of mutants/number of individuals
scored) is skewed. Haldane considered using a prior distribution for the
mutation rate Œ∏. If p (Œ∏|H) is some prior density (where H denotes hyper-
parameters) and x mutants are observed out of n individuals, the typical
Bernoulli sampling model produces as posterior density of Œ∏,
p (Œ∏|n, x, H) =
Œ∏x (1 ‚àíŒ∏)n‚àíx p (Œ∏|H)
1
0
Œ∏x (1 ‚àíŒ∏)n‚àíx p (Œ∏|H) dŒ∏
.
Observe that the uniform prior corresponds to a Be (1, 1) distribution, in
which case the posterior is always proper. Haldane noted that if the uniform
prior is adopted for Œ∏, the posterior density is in the form
Be (Œ∏|x + 1, n ‚àíx + 1) ,
corresponding to a Beta distribution, with mean (x + 1) / (n + 2). If the
posterior mean is used as a point estimator, the statistic is biased in the
frequentist sense, a fact found objectionable by Haldane. He argued that
assuming a uniform prior does not make sense because Œ∏ should be greater
than 10‚àí20 and lower than 10‚àí3; in some particular cases, Œ∏ would be as
likely to be between 10‚àí6 and 10‚àí7 as between 10‚àí6 and 10‚àí5. He suggested
the prior
p (Œ∏) ‚àù
1
Œ∏ (1 ‚àíŒ∏),

360
7. The Prior Distribution and Bayesian Analysis
which is the improper Be (0, 0) distribution. The posterior distribution of
the mutation rate is then Be (x, n ‚àíx), with mean x/n, satisfying Hal-
dane‚Äôs requirement of unbiasedness. As noted above, this posterior can
be improper, but Haldane (without elaboration) cautioned that his prior
would ‚Äúwork‚Äù only if x > 0 (at least one mutant is observed) and if x < n
(which would be almost certain, unless n is extremely small and by sheer
accident all individuals scored are mutant). He stated that for small x/‚àön
the posterior distribution would be well-approximated by a gamma process
with density
p (Œ∏|n, x) ‚àùŒ∏x‚àí1 exp (‚àínŒ∏) ,
which also has mean x/n. Haldane (1948) showed then that a cube root
transformation of Œ∏ would have a nearly normal distribution. Note that
Haldane‚Äôs ‚Äúvague‚Äù prior for Œ∏ implies an uniform prior for the logit trans-
formation. Let
Œª = log
Œ∏
1 ‚àíŒ∏,
with inverse transformation
Œ∏ =
exp (Œª)
1 + exp (Œª)
and Jacobian
dŒ∏
dŒª =
exp (Œª)
[1 + exp (Œª)]2 .
Observe that the logit can take any value in the real line. Then the induced
prior density of Œª is
p (Œª) ‚àù

exp (Œª)
1 + exp (Œª)
‚àí1 
1
1 + exp (Œª)
‚àí1
exp (Œª)
[1 + exp (Œª)]2 = 1.
This uniform distribution is improper because the integral of the density
over the entire parameter space is not Ô¨Ånite.
‚ñ†
A Single Parameter
Although JeÔ¨Äreys (1961) proposed a class of improper priors for represent-
ing ignorance, it is not entirely transparent why these should be considered
‚Äúnoninformative‚Äù in some precise sense. Zellner (1971) and Box and Tiao
(1973) provide some heuristic justiÔ¨Åcations, based on the two rules sug-
gested by JeÔ¨Äreys:
(1) If a parameter Œ∏ can take any value in a Ô¨Ånite range or between ‚àí‚àû
and ‚àû, it should be taken as distributed uniformly, a priori.
(2) If it takes values between 0 and ‚àû, then its logarithm should have a
uniform distribution.

7.5 Priors Conveying Little Information
361
The justiÔ¨Åcation for the Ô¨Årst rule (Zellner, 1971) is that since the range
covers the entire real line, the probability that Œ∏ takes any value drawn
from a uniform distribution is
Pr (‚àí‚àû< Œ∏ < ‚àû) =
‚àû

‚àí‚àû
dŒ∏ = ‚àû,
and JeÔ¨Äreys interprets this as the probability of the certain event (1 be-
comes ‚àû!). Then the probability that it falls in any interval (a, b) is 0,
and similarly for another interval (c, d) . Since the ratio of probabilities is
indeterminate, JeÔ¨Äreys argues that this constitutes a formal representation
of ignorance, as one cannot favor an interval when choosing over any pair
of Ô¨Ånite intervals. If the uniform prior is bounded (a typical device used by
quantitative geneticists to avoid improper posteriors), this implies that one
knows something about the range of values and the ratios between prob-
abilities are then determinate. However, if the boundaries are stretched,
the ratio of probabilities becomes indeterminate in the limit, thus satisfy-
ing JeÔ¨Äreys‚Äô requirements. Concerning the second rule, note that if log Œ∏ is
taken as uniformly distributed, the prior density of Œ∏ should be proportional
to 1/Œ∏. In this setting (Zellner, 1971), one has
‚àû

0
1
Œ∏dŒ∏ = ‚àû,
a

0
1
Œ∏dŒ∏ = ‚àû,
‚àû

a
1
Œ∏dŒ∏ = ‚àû.
It ‚àûrepresents certainty, then the ratio between the last two integrals is
indeterminate, leading to a representation of ignorance: one cannot pick
the interval in which Œ∏ is more likely to reside.
An important step toward the development of noninformative priors was
the introduction of invariance requirements by JeÔ¨Äreys (1961). In a nutshell,
the central point is to recognize that ignorance about Œ∏ implies ignorance
about the monotone transformation Œª = f (Œ∏) . Hence, if the prior density
of Œ∏ is p (Œ∏) , the requirement is that the probability contents must be
preserved, that is, p (Œ∏) dŒ∏ = p (Œª) dŒª, which leads directly to the usual
formula for the density of a transformed random variable. JeÔ¨Äreys‚Äô idea
was to use, as prior density of Œ∏, the square root of Fisher‚Äôs information
measure
>
I (Œ∏) =
C
E
dl (Œ∏|y)
dŒ∏
2
=
@
A
A
B‚àíE

d2l (Œ∏|y)
(dŒ∏)2

,
(7.45)

362
7. The Prior Distribution and Bayesian Analysis
where l (Œ∏|y) is the log-likelihood function. Making a change of variables,
consider the prior density of Œª induced by JeÔ¨Äreys‚Äô rule
p (Œª) ‚àù
C
E
dl (Œ∏|y)
dŒ∏
2 dŒ∏
dŒª
‚àù
C
E
dl (f ‚àí1 (Œª) |y)
dŒ∏
dŒ∏
dŒª
2
‚àù
C
E
dl (f ‚àí1 (Œª) |y)
dŒª
2
=
>
I (Œª),
recalling that the likelihood is invariant under a change in parameteriza-
tion. One can transform back to Œ∏, and retrieve
>
I (Œ∏) as prior distribution.
Hence, JeÔ¨Äreys‚Äô prior is invariant under transformation. The posterior den-
sity of Œ∏ has the form
p (Œ∏|y)
‚àù
L (Œ∏|y)
>
I (Œ∏)
‚àù
L (Œ∏|y)
C
E
dl (Œ∏|y)
dŒ∏
2
,
(7.46)
where L (Œ∏|y) is the likelihood. The posterior density of Œª is then
p (Œª|y)
‚àù
L [Œª|y]
C
E
dl (f ‚àí1 (Œª) |y)
dŒ∏
2 dŒ∏
dŒª
‚àù
L (Œª|y)
>
I (Œª).
(7.47)
One can now go back and forth between parameterizations, and preserve
the probability contents. Some examples of JeÔ¨Äreys‚Äô rule are presented
subsequently.
At Ô¨Årst sight, JeÔ¨Äreys‚Äô prior seems to depend on the data, since it involves
the likelihood function. Actually, it does not; rather, it depends on the form
of the ‚Äúexperiment‚Äù. Note that in the process of taking expectations, the
dependence on the observed data is removed. However, the prior depends on
how the data are to be collected. From an orthodox Bayesian point of view,
prior information should not be aÔ¨Äected by the form of the experiment or
by how much data is to be collected, but this is not the case with JeÔ¨Äreys‚Äô
prior. Box and Tiao (1973) argue that a noninformative prior does not
necessarily represent a prior opinion and that ‚Äúknowing little‚Äù is meaningful
only relative to a speciÔ¨Åc experiment. Thus, the form of a noninformative
prior would depend on the experiment, and two diÔ¨Äerent experiments would
lead to diÔ¨Äerent noninformative priors.
Example 7.16
Normal distribution with unknown mean
Let there be n samples from the same normal distribution with unknown

7.5 Priors Conveying Little Information
363
mean but known variance. The log-likelihood is
l

¬µ|œÉ2, y

= constant ‚àín (y ‚àí¬µ)2
2œÉ2
.
The square of the score with respect to ¬µ is

dl

¬µ|œÉ2, y

d¬µ
2
=
n (y ‚àí¬µ)
œÉ2
2
,
and its expectation is
I (¬µ) = n
œÉ2 .
JeÔ¨Äreys‚Äô prior is the square root of this expression. Since it does not involve
¬µ, it follows that the noninformative prior is Ô¨Çat and improper.
‚ñ†
Example 7.17
Normal distribution with unknown variance
The setting is as in the previous example, but now we seek JeÔ¨Äreys‚Äô prior
for the variance. The log-likelihood is now
l

œÉ2|¬µ, y

= constant ‚àín
2 log œÉ2 ‚àí
n
i=1
(yi ‚àí¬µ)2
2œÉ2
.
DiÔ¨Äerentiating twice with respect to œÉ2 and multiplying by ‚àí1 to obtain
the observed Fisher‚Äôs information gives
‚àín
2œÉ4 +
n
i=1
(yi ‚àí¬µ)2
œÉ6
.
Taking expectations and then the square root yields as JeÔ¨Äreys‚Äô prior
p

œÉ2
‚àù
? n
2œÉ4 ‚àù1
œÉ2 ,
which is in conformity with the second rule discussed above. Since JeÔ¨Äreys‚Äô
prior is invariant under reparameterization one obtains for the standard
deviation,
p (œÉ) ‚àù1
œÉ2
dœÉ2
dœÉ ‚àù1
œÉ ,
whereas for the logarithm of the standard deviation, the prior is
p [log (œÉ)]
‚àù
1
œÉ
d

explog(œÉ)
d log (œÉ)
‚àù
1
œÉ explog(œÉ) ‚àù1,
which is the improper uniform prior.
‚ñ†

364
7. The Prior Distribution and Bayesian Analysis
Example 7.18
Exponential distribution
As in Leonard and Hsu (1999), draw a random sample of size n from an
exponential distribution and take its parameter Œ∏ to be unknown. The
likelihood function is
L (Œ∏|y) = Œ∏n exp

‚àíŒ∏
n

i=1
yi

.
The second derivative of the log-likelihood with respect to Œ∏ is ‚àín/œÉ2. Since
this does not depend on the observations, JeÔ¨Äreys‚Äô prior is
p (Œ∏) ‚àù
? n
Œ∏2 ‚àù1
Œ∏.
The posterior distribution is then
p (Œ∏|y) ‚àùŒ∏n‚àí1 exp

‚àíŒ∏
n

i=1
yi

,
which is a Ga (n, n
i=1 yi) process.
‚ñ†
Many Parameters
Consider now a multi-parameter situation. JeÔ¨Äreys‚Äô rule generalizes to:
p (Œ∏) ‚àù
>
|I (Œ∏)|,
(7.48)
where I (Œ∏) is Fisher‚Äôs information matrix about the p √ó 1 parameter vec-
tor Œ∏. The multiparameter rule has often been criticized in the Bayesian
literature because of ‚Äúinconsistencies‚Äù (e.g., O‚ÄôHagan, 1994), or due to ‚Äúin-
tuitively unappealing implications‚Äù (Bernardo and Smith, 1994). The lat-
ter objection stems from the fact that when the rule is applied to certain
problems, it does not yield results that are equivalent to their frequentist
counterparts, or because no account is taken of degrees of freedom lost. We
give an example where the rule leads to an objectionable result.
Example 7.19
JeÔ¨Äreys‚Äô rule in a regression model
Consider the usual linear regression model under normality assumptions.
The unknown parameters are the regression coeÔ¨Écients Œ≤ (p √ó 1) and the
residual variance œÉ2. As shown in Chapter 3, the expected information
matrix is
I

Œ≤, œÉ2
=
Ô£Æ
Ô£∞
X‚Ä≤X
œÉ2
0
0
n
2œÉ4
Ô£π
Ô£ª.

7.5 Priors Conveying Little Information
365
According to JeÔ¨Äreys‚Äô rule (7.48), the joint prior density of the parameters
is
p

Œ≤, œÉ2
‚àù
""""""
Ô£Æ
Ô£∞
X‚Ä≤X
œÉ2
0
0
n
2œÉ4
Ô£π
Ô£ª
""""""
1
2
‚àù

œÉ2‚àíp+2
2 .
(7.49)
When Œ≤ contains a sole component (a mean, ¬µ, say), the prior reduces
to œÉ‚àí3. We now further develop this simpler situation and examine the
marginal posterior distribution of œÉ2. For p = 1, the joint posterior based
on a sample of size n is
p

¬µ, œÉ2|y

‚àù

œÉ2‚àín
2 exp
Ô£Æ
Ô£ØÔ£ØÔ£∞
n
i=1
(yi ‚àí¬µ)2
2œÉ2
Ô£π
Ô£∫Ô£∫Ô£ª

œÉ2‚àí3
2
‚àù

œÉ2‚àín+3
2
exp
Ô£Æ
Ô£ØÔ£ØÔ£∞
n
i=1
(yi ‚àíy)2
2œÉ2
Ô£π
Ô£∫Ô£∫Ô£ªexp

‚àín (y ‚àí¬µ)2
2œÉ2

.
(7.50)
Integration over the mean, ¬µ, leads to the following marginal density of œÉ2
(after rearrangement):
p

œÉ2|y

‚àù

œÉ2‚àí( n
2 +1) exp
Ô£Æ
Ô£ØÔ£ØÔ£∞‚àí
n
i=1
(yi ‚àíy)2
2œÉ2
Ô£π
Ô£∫Ô£∫Ô£ª.
(7.51)
Change now variables to
œá2 =
n
i=1
(yi ‚àíy)2
œÉ2
.
In a frequentist setting, this is a central chi-square random variable on n‚àí1
degrees of freedom. From a Bayesian point of view, we need to consider its
posterior distribution. Noting that the absolute value of the Jacobian of
the transformation is n
i=1 (yi ‚àíy)2 œá‚àí4, the posterior density of œá2 is
p

œá2|y

‚àù

œá2 n
2 ‚àí1 exp

‚àíœá2
2

,
indicating a chi-square distribution on n degrees of freedom. Since ‚Äúevery-
body knows‚Äù that estimating the mean consumes one degree of freedom,
it becomes apparent that JeÔ¨Äreys‚Äô multiparameter prior does not take into
account the loss of information incurred.

366
7. The Prior Distribution and Bayesian Analysis
On the other hand, suppose one employs JeÔ¨Äreys‚Äô rule for each of ¬µ and
œÉ2 separately (assuming that the other parameter is known in each case),
and takes as joint prior
p

¬µ, œÉ2
‚àùp

¬µ|œÉ2
p

œÉ2|¬µ

‚àù1
œÉ2 .
Replacing

œÉ2‚àí3
2 in (7.50) by

œÉ2‚àí1 and integrating over ¬µ yields an
expression that diÔ¨Äers from (7.51) in the exponent of the Ô¨Årst term, which
is instead ‚àí[(n + 1)/2]. After transforming as before, the posterior density
of œá2 is now
p

œá2|y

‚àù

œá2 n‚àí1
2
‚àí1 exp

‚àíœá2
2

.
Here, one arrives at the ‚Äúcorrect‚Äù posterior distribution of chi-square, i.e.,
one on n ‚àí1 degrees of freedom. There is no formal justiÔ¨Åcation for such
prior, but ‚Äúit works‚Äù, at least in the sense of coinciding with the frequen-
tist treatment, which was one of JeÔ¨Äreys‚Äô objectives. Box and Tiao (1973)
recommend exercising ‚Äúspecial care‚Äù when choosing noninformative priors
for location and scale parameters simultaneously.
The problems of the multiparameter rule become even more serious when
the model is even more richly parameterized. Suppose now that Œ≤ contains
two means (¬µ1 and ¬µ2). Using (7.49), the joint prior p

¬µ1, ¬µ2, œÉ2
turns
out to be proportional to œÉ‚àí4. The joint posterior density can be written
as
p

¬µ1, ¬µ2, œÉ2|y

‚àù

œÉ2‚àín1+n2+4
2
exp
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞
2
i=1
ni

j=1
(yij ‚àíyi)2
2œÉ2
Ô£π
Ô£∫Ô£∫Ô£∫Ô£ª
√ó
2
-
i=1
exp

‚àíni (yi ‚àí¬µi)2
2œÉ2

,
where ni is the number of observations associated with mean i; the rest
of the notation is clear from the context. Integrating over the two means
yields as marginal posterior density of œÉ2,
p

œÉ2|y

‚àù

œÉ2‚àín1+n2+2
2
exp
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞
2
i=1
ni

j=1
(yij ‚àíyi)2
2œÉ2
Ô£π
Ô£∫Ô£∫Ô£∫Ô£ª.
Now put
œá2 =
2
i=1
ni

j=1
(yij ‚àíyi)2
œÉ2
.

7.5 Priors Conveying Little Information
367
This variable, in a frequentist setting, should have a chi-squared distribu-
tion on n1 + n2 ‚àí2 degrees of freedom. Changing variables, the posterior
density of œá2 becomes
p

œá2|y

‚àù

œá2 n1+n2
2
‚àí1 exp

‚àíœá2
2

.
Again, the multiparameter rule does not take into account the fact that
several means appear as nuisance parameters: the degrees of freedom of
the chi-square distribution are left intact at n = n1 + n2. JeÔ¨Äreys (1961),
in the context of testing location parameters, writes: ‚ÄúThe index in the
corresponding t distribution would always be (n + 1) /2 however many true
values were estimated. This is unacceptable‚Äù. We agree.
‚ñ†
7.5.3
Maximum Entropy Prior Distributions
Using the principle of maximum entropy as a means of allocating prob-
abilities in a prior distribution was suggested by Jaynes, a physicist. A
comprehensive account of the idea is in his unÔ¨Ånished book (Jaynes, 1994).
Suppose one wishes to assign a prior distribution to some unknown quan-
tity. Naturally, this prior should take into account whatever information
is available, but not more. For example, knowledge of average values (or
of other aspects of the distribution) will give a reason for favoring some
possibilities over others but, beyond this, the distribution should be as un-
committed as possible. Further, no possibilities should be ruled out, unless
dictated by prior knowledge. The information available deÔ¨Ånes constraints
that Ô¨Åx some properties of the prior distribution, but not all of them. Jaynes
formulates the problem as follows:
‚ÄúTo cast it into mathematical form, the aim of avoiding un-
warranted conclusions leads us to ask whether there is some rea-
sonable numerical measure of how uniform a probability distri-
bution is, which the robot could maximize subject to constraints
which represent its available information.‚Äù
Jaynes used Shannon‚Äôs (1948) theorem, which states that the only mea-
sure of the uncertainty represented by a probability distribution is entropy.
Then he argued that a distribution maximizing entropy, subject to the con-
straints imposed by the information available, would represent the ‚Äúmost
honest‚Äù description of what is known about a set of propositions. He noted
that the only source of arbitrariness is the base of the logarithms employed
in entropy. However, since this operates as a multiplicative constant in the
expression for entropy, it has no eÔ¨Äect on the values of the probabilities
that maximize H; see, e.g., the form of entropy in (7.9). A derivation of
the principle, as in Jaynes (1994) and in Sivia (1996), follows.

368
7. The Prior Distribution and Bayesian Analysis
Discrete Case
Let I represent the information to be used for assigning probabilities
{p1, p2, ..., pK}
to K diÔ¨Äerent possibilities. Suppose there are n >> K small ‚Äúquanta‚Äù of
probability (so that n is very large) to distribute in any way one sees Ô¨Åt. If
the quanta are tossed completely at random (reÔ¨Çecting lack of information
in the process of allocating quanta), so that each of the options has an
equal probability 1/K of getting a quanta, the allocation would be viewed
as a fair one. At the end of the experiment, it is observed that option i has
received ni quanta, and so on, so that the experiment has generated the
probability assignment
pi = ni
n ;
i = 1, 2, . . . , K.
The probability that this particular allocation of quanta will be observed
is given by the multinomial distribution
W =
n!
n1! n2!... nK!

 1
K
n1 
 1
K
n2
...

 1
K
nK
=
n!
n1! n2!... nK!K
‚àí
K

i=1
nK =
n!
n1! n2!... nK!K‚àín,
(7.52)
where n = K
i=1 ni. The experiment is repeated over and over, and the
probability assignment is examined to see if it is consistent with the in-
formation I; if not, the assignment is rejected. What is the most likely
probability distribution resulting from the experiment? In order to Ô¨Ånd it,
use will be made of Stirling‚Äôs approximation to factorials (e.g., Abramowitz
and Stegun, 1972) for large n:
log n! ‚âàn log n ‚àín.
Employing this in (7.52), one can write
1
n log W = 1
n

n log n ‚àín ‚àí
K

i=1
(ni log ni ‚àíni) ‚àín log K

= log n ‚àí
K

i=1
ni
n log ni

‚àílog K.

7.5 Priors Conveying Little Information
369
Since pi = ni/n,
1
n log W = log n ‚àí
K

i=1
(pi log npi) ‚àílog K
= constant ‚àí
K

i=1
(pi log pi)
= constant + H (p1, p2, ..., pK) .
(7.53)
Maximizing W with respect to pi is equivalent to maximizing (1/n) log W,
and the preceding expression indicates that this is achieved when the en-
tropy H (p1, p2, ..., pK) is extremized with respect to the probabilities, sub-
ject to any constraints imposed by the available information I.
Now consider Ô¨Ånding an explicit representation of the maximum entropy
prior distribution of a discrete random variable Œ∏ with K mutually exclusive
and exhaustive states Œ∏i. The information available is that the sum of the
probabilities must be equal to 1 and assume, further, that the mean of the
prior distribution to be speciÔ¨Åed, i.e., Œ∏ = K
i=1 Œ∏ipi, is taken to be known.
This knowledge must be incorporated in the maximization problem as a
Lagrangian condition. Hence, the objective function to be extremized is:
H ‚àó(p1, p2, . . . , pK, Œª0, Œª1)
= ‚àí
K

i=1
(pi log pi) + Œª0
 K

i=1
pi ‚àí1

+ Œª1
 K

i=1
Œ∏ipi ‚àíŒ∏

,
(7.54)
where Œª0 and Œª1 are Lagrange multipliers ensuring that the two information
constraints are observed. The multipliers ensure propriety and knowledge of
the mean of the prior distribution, respectively. DiÔ¨Äerentiation with respect
to the K + 2 unknowns yields
‚àÇH ‚àó(p1, p2, ..., pK, Œª0, Œª1)
‚àÇpi
= ‚àílog pi ‚àí1 + Œª0 + Œª1Œ∏i,
i = 1, 2, ..., K,
‚àÇH ‚àó(p1, p2, ..., pK, Œª0, Œª1)
‚àÇŒª0
=
K

i=1
pi ‚àí1,
‚àÇH ‚àó(p1, p2, ..., pK, Œª0, Œª1)
‚àÇŒª1
=
K

i=1
Œ∏ipi ‚àíŒ∏.
The diÔ¨Äerentials are set to 0 simultaneously and solved for the unknowns.
The pi equation yields
pi = exp (Œª1Œ∏i) exp (Œª0 ‚àí1) .

370
7. The Prior Distribution and Bayesian Analysis
Since the Ô¨Årst Lagrangian condition dictates that the sum of the probabil-
ities must add up to 1, the preceding must be equal to
pi =
exp (Œª1Œ∏i) exp (Œª0 ‚àí1)
K

i=1
exp (Œª1Œ∏i) exp (Œª0 ‚àí1)
=
exp (Œª1Œ∏i)
K

i=1
exp (Œª1Œ∏i)
,
i = 1, 2, ..., K.
(7.55)
This is called the Gibbs distribution, and the denominator is known as the
partition function (Applebaum, 1996). Expression (7.55) gives the maxi-
mum entropy prior distribution of a univariate discrete random variable
with known mean. If the mean of the distribution is left unspeciÔ¨Åed, this
is equivalent to removing the second Lagrangian condition in (7.54), or
setting Œª1 to 0. Doing this in expression (7.55), one obtains pi = 1/K,
(i = 1, 2, ..., K) as maximum entropy distribution. In general, as additional
side conditions are introduced (as part of the prior information I), the sys-
tem of equations that needs to be solved is non-linear. Zellner and HighÔ¨Åeld
(1988) discuss one of the possible numerical solutions.
Continuous Case via Discretization
When Œ∏ is continuous, the technical arguments are more involved. Further,
it must be recalled that entropy is not well deÔ¨Åned in such a setting (for
an illustration, see Example 7.6). The problem now consists of Ô¨Ånding the
prior density or distribution that maximizes the entropy
H [p (Œ∏)] = ‚àí

(log Œ∏) p (Œ∏) dŒ∏,
subject to the constraints imposed by the available information I. The in-
tegral above is a functional, that is, a function that depends on another
function, this being the density p (Œ∏) . Hence, one must Ô¨Ånd the function
that extremizes the integral. First, we present a solution based on dis-
cretizing the prior distribution, and a formal calculus of variations solution
is given later on.
First, return to the derivation of the principle of maximum entropy given
in (7.52) and (7.53), but assume now that the chance of a quanta falling into
a speciÔ¨Åc one of the K options is mi, instead of being equal for all options
(Sivia, 1996). Then the experiment generates the probability assignment
W ‚àó=
n!
n1! n2!... nK! (m1)n1 (m2)n2 ... (mk)nK ,
which is the multinomial distribution. Next, as before, put
1
n log W ‚àó= 1
n

log n! ‚àí
K

i=1
log ni! +
K

i=1
ni log mi

,

7.5 Priors Conveying Little Information
371
and make use of Stirling‚Äôs approximation, and of pi = ni/n, to arrive at
1
n log W ‚àó= log n ‚àí
K

i=1
(pi log npi) +
K

i=1
pi log mi
= ‚àí
K

i=1

pi log pi
mi

.
(7.56)
This is a generalization of (7.53), where allowance is made for the options
in the random experiment to have unequal probability. Note that (7.56) is a
relative entropy, which has the advantage (in the continuous case) of being
invariant under a change of variables, as noted earlier. Now the continuous
counterpart of (7.56) is
H ‚àó(Œ∏) = ‚àí
 
log p (Œ∏)
m (Œ∏)

p (Œ∏) dŒ∏.
(7.57)
This is in the same mathematical form as Kullback‚Äôs expected information
for discrimination, and is also known as the Shannon-Jaynes entropy (Sivia,
1996). Here, m (Œ∏) plays the role of a ‚Äúreference‚Äù distribution, often taken
to be the uniform one. In this case, (7.57) reduces to the standard entropy.
Now consider a discretization approach to Ô¨Ånding the maximum relative
entropy distribution. Suppose that Œ∏ takes appreciable density in the inter-
val (a, b) . Following Applebaum (1996), one can deÔ¨Åne a partition of (a, b)
such that:
a = Œ∏0 < Œ∏1 < ¬∑ ¬∑ ¬∑ < Œ∏K‚àí1 < Œ∏K = b.
DeÔ¨Åne the event Œæj ‚àà(Œ∏j‚àí1, Œ∏j) for 1 ‚â§j ‚â§K, and the discrete random
variable Œ∏‚àówith K mutually exclusive and exhaustive states and probability
distribution
Pr

Œ∏‚àó= Œæj

=
Œ∏j

Œ∏j‚àí1
p (Œ∏) dŒ∏
= F (Œ∏j) ‚àíF (Œ∏j‚àí1) = pj,
(7.58)
where F (¬∑) is the c.d.f.. The relative entropy of the discretized distribution
is then
H (Œ∏‚àó) = ‚àí
K

j=1
pj log pj
mj
.

372
7. The Prior Distribution and Bayesian Analysis
Proceed now to maximize H (Œ∏‚àó) with respect to pj, subject to the La-
grangian condition that
K

j=1
pj = 1. Then
‚àÇH (Œ∏‚àó)
‚àÇpj
=
‚àí1 ‚àílog pj
mj
‚àí
‚àÇ
‚àÇpj
Œª
 K

i=1
pi ‚àí1

=
‚àí1 ‚àílog pj
mj
‚àíŒª,
with the derivative with respect to the multiplier Œª leading directly to the
condition that the sum of the probabilities must be equal to 1. Setting the
preceding diÔ¨Äerential to 0 produces
pj = mj exp [‚àí(1 + Œª)] .
(7.59)
Summing now over the K states, and assuming that the discretized ‚Äúrefer-
ence‚Äù distribution is proper, gives exp [‚àí(1 + Œª)] = 1, so that Œª = ‚àí1. It
follows that pj = mj. Further, the distribution with probabilities mj is the
one representing complete randomness of the experiment, since the proba-
bility ‚Äúquanta‚Äù are allocated completely at random, with mj = 1/n for all
j when the options are equally likely. Now, the deÔ¨Ånition of the derivative
implies that
lim
‚àÜŒ∏‚Üí0 pj = lim
‚àÜŒ∏‚Üí0
F (Œ∏j) ‚àíF (Œ∏j‚àí1)
Œ∏j ‚àíŒ∏j‚àí1
= F ‚Ä≤ (Œ∏j) = p (Œ∏) .
Hence, in the limit, it follows that the density of the maximum entropy
distribution p (Œ∏) is the uniform density distribution m (Œ∏), since all the
options are of equal size and inÔ¨Ånitesimally small. Before presenting the
variational argument, two examples are given, using the discretization pro-
cedure given above. Note that in the continuous case the maximum relative
entropy distribution is not invariant with respect to the reference distribu-
tion chosen.
Example 7.20
Maximum entropy prior distribution when the mean is
known
Suppose the mean of the prior distribution is given. The setting then is as
in (7.54). However, the objective function to be optimized now involves en-
tropy relative to the reference uniform distribution. The objective function
takes the form:
H ‚àó(p1, p2, . . . , pK, Œª0, Œª1)
= ‚àí
K

i=1

pi log pi
mi

+ Œª0
 K

i=1
pi ‚àí1

+ Œª1
 K

i=1
Œ∏ipi ‚àíŒ∏

.

7.5 Priors Conveying Little Information
373
Setting the derivatives to 0 leads to
pi = mi exp (Œª1Œ∏i) exp (Œª0 ‚àí1) .
The continuous counterpart gives, as density of the maximum relative en-
tropy distribution,
p (Œ∏) = m (Œ∏) exp (Œª1Œ∏) exp (Œª0 ‚àí1) .
If m (Œ∏) (the reference density) is uniform, it follows that
p (Œ∏) ‚àùexp (‚àíŒª‚àó
1Œ∏) ,
where Œª‚àó
1 = ‚àíŒª1. If Œ∏ is a strictly positive parameter, it follows that the
maximum entropy distribution relative to a uniform measure is exponential.
Since the mean of an exponential distribution is Œ∏ = 1/Œª‚àó
1, the Œª‚àó
1 parameter
can be assessed readily once the prior mean is speciÔ¨Åed.
‚ñ†
Example 7.21
Maximum entropy prior distribution when the mean, vari-
ance, and higher-order moments are given
Knowledge of the mean and variance of the prior distribution imposes three
constraints in the optimization procedure. The Ô¨Årst constraint ensures pro-
priety of the distribution, and the other two give the conditions stating
knowledge of the Ô¨Årst and second moments. Using the discretization pro-
cedure, the objective function to be maximized is
H‚àó(p1, p2, ..., pK, Œª0, Œª1, Œª2) = ‚àí
K

i=1

pi log pi
mi

+ Œª0
 K

i=1
pi ‚àí1

+ Œª1
 K

i=1
Œ∏ipi ‚àíŒ∏

+ Œª2
 K

i=1
Œ∏2
i pi ‚àíŒ∏2

,
where Œ∏2 = K
i=1 Œ∏2
i pi and, therefore, œÉ2 = Œ∏2 ‚àíŒ∏
2 is the variance of the
prior distribution. DiÔ¨Äerentiation with respect to pi, and setting to 0, gives
log pi
mi
= 1 ‚àíŒª0 ‚àíŒª1Œ∏i ‚àíŒª2Œ∏2
i .
The continuous counterpart, after solving for the maximum relative entropy
density p (Œ∏), is
p (Œ∏) ‚àùm (Œ∏) exp

1 ‚àíŒª0 ‚àíŒª1Œ∏ ‚àíŒª2Œ∏2
.
(7.60)
Note that by restating the Lagrangian condition
Œª0
 K

i=1
pi ‚àí1


374
7. The Prior Distribution and Bayesian Analysis
as
Œª0

1 ‚àí
K

i=1
pi

,
and so on, one can put, without loss of generality,
p (Œ∏) ‚àùm (Œ∏) exp

1 + Œª0 + Œª1Œ∏ + Œª2Œ∏2
.
(7.61)
If the reference density m (Œ∏) is taken to be uniform (and proper, so that
the maximum relative entropy density is guaranteed to be proper as well)
one obtains
p (Œ∏) ‚àùexp

Œª1Œ∏ + Œª2Œ∏2
,
and the maximum entropy density is then
p (Œ∏) =
exp

Œª1Œ∏ + Œª2Œ∏2

exp

Œª1Œ∏ + Œª2Œ∏2
dŒ∏
=
exp

1 + Œª0 + Œª1Œ∏ + Œª2Œ∏2

exp

1 + Œª0 + Œª1Œ∏ + Œª2Œ∏2
dŒ∏.
(7.62)
If the Ô¨Årst M moments of the prior distribution are speciÔ¨Åed, the preceding
generalizes to
p (Œ∏) =
exp

1 +
M

i=0
ŒªiŒ∏i


exp

1 +
M

i=0
ŒªiŒ∏i

dŒ∏
.
(7.63)
‚ñ†
Example 7.22
The special case of known mean and variance
Return now to a setting of known mean and variance of the prior distri-
bution and consider the Kullback‚ÄìLeibler discrepancy in (7.41). Let g (Œ∏)
denote any other density with mean Œ∏ and variance œÉ2 and let p (Œ∏) be the
maximum relative entropy distribution sought. Since the discrepancy is at
least null
D [g (Œ∏) , p (Œ∏)] =
 
log p (Œ∏)
g (Œ∏)

p (Œ∏) dŒ∏ ‚â•0,
which implies that
H [p (Œ∏)] ‚â§‚àí

{log [g (Œ∏)]} p (Œ∏) dŒ∏,
(7.64)

7.5 Priors Conveying Little Information
375
with the equality holding if and only if p (Œ∏) = g (Œ∏) (Applebaum, 1996).
Now take g (Œ∏) to be the density of the normal distribution N

Œ∏, œÉ2
. Then
‚àí

{log [g (Œ∏)]} p (Œ∏) dŒ∏ = ‚àí
 #
log

1
‚àö
2œÄœÉ2 exp

‚àí

Œ∏ ‚àíŒ∏
2
2œÉ2
$
p (Œ∏) dŒ∏
= ‚àílog

1
‚àö
2œÄœÉ2

+
1
2œÉ2
 
Œ∏ ‚àíŒ∏
2 p (Œ∏) dŒ∏.
Note that the integral in the preceding expression is œÉ2, the variance of the
prior distribution, by deÔ¨Ånition. Then, using (7.64),
H [p (Œ∏)] ‚â§‚àí

{log [g (Œ∏)]} p (Œ∏) dŒ∏ = 1
2

1 + log

2œÄœÉ2
.
It follows that the entropy of the maximum entropy distribution cannot
exceed

1 + log

2œÄœÉ2
/2. Now, from Example 7.7, this quantity is pre-
cisely the entropy of a normal distribution. Hence, the maximum entropy
prior distribution when the mean and variance are given must be a normal
distribution with mean Œ∏ and variance œÉ2.
‚ñ†
Continuous Case via Variational Arguments
As noted, the search for a maximum relative entropy distribution involves
Ô¨Ånding the function p (Œ∏) that minimizes the integral
Int [p (Œ∏)] =
 
log p (Œ∏)
m (Œ∏)

p (Œ∏) dŒ∏.
(7.65)
This is called a ‚Äúvariational‚Äù problem and its solution requires employing
the advanced techniques of the calculus of variations (e.g., Weinstock, 1974;
Fox, 1987). Since few biologists are exposed to the basic ideas (referred to
as ‚Äústandard‚Äù in some statistical texts) we provide a cursory introduction.
Subsequently, a speciÔ¨Åc result is applied to the problem of Ô¨Ånding a contin-
uous maximum entropy distribution. We follow Weinstock (1974) closely.
In general, consider the integral
Int [y (x)] =
x2

x1
f [x, y (x) , y‚Ä≤ (x)] dx,
(7.66)
where y (x) is a twice diÔ¨Äerentiable function and y‚Ä≤ (x) is its Ô¨Årst derivative
with respect to x. We seek the function y (x) rendering the above integral
minimum (or maximum). The problem resides in Ô¨Ånding such a function or,
equivalently, in arriving at conditions that the function must obey. First,
the function must satisfy the boundary conditions y (x1) = y1 and y (x2) =
y2, with x1, x2, y1, and y2 given. Now, when going from the point (x1, y1)

376
7. The Prior Distribution and Bayesian Analysis
to the point (x2, y2), one can draw a number of curves, each relating the
dependent variable to x, and with each curve deÔ¨Åning a speciÔ¨Åc integration
path. Next, deÔ¨Åne the ‚Äúfamily‚Äù of functions
Y (x) = y (x) + ŒµŒ∑ (x) ,
(7.67)
where Œµ is a parameter of the family and Œ∑ (x) is diÔ¨Äerentiable with Œ∑ (x1) =
Œ∑ (x2) = 0. This ensures that Y (x1) = y (x1) = y1 and Y (x2) = y (x2) =
y2, so that all members of the family possess the required ends. Also, note
that the form of (7.67) indicates that no matter what Œ∑ (x) is chosen, the
minimizing function y (x) will be a member of the family for Œµ = 0. Now,
return to (7.66), and replace y and y‚Ä≤ by Y and Y ‚Ä≤, respectively. Then,
write
Int (Œµ) =
x2

x1
f [x, Y (x) , Y ‚Ä≤ (x)] dx,
(7.68)
where, for a given Œ∑ (x) , the integral is a function of Œµ. Observe from (7.67)
that
Y ‚Ä≤ (x) = y‚Ä≤ (x) + ŒµŒ∑‚Ä≤ (x) ,
so, in conjunction with (7.67), it becomes clear that setting Œµ = 0 is equiv-
alent to replacing Y and Y ‚Ä≤ by y and y‚Ä≤, respectively. Hence (7.68) is
minimum with respect to Œµ at the value Œµ = 0. The problem of minimizing
(7.68) then reduces to one of standard calculus with respect to Œµ, except
that we know in advance that the minimizing value is Œµ = 0! Hence, it must
be true that
d Int (Œµ)
dŒµ
""""
Œµ=0
= 0.
Now, since the limits of integration do not involve Œµ:
d Int (Œµ)
dŒµ
=
x2

x1
d
dŒµf [x, Y (x) , Y ‚Ä≤ (x)] dx.
(7.69)
Employing standard results for the derivative of a function of several vari-
ables (Kaplan, 1993):
d
dŒµf [x, Y (x) , Y ‚Ä≤ (x)]
=
‚àÇf
‚àÇY
‚àÇY
‚àÇŒµ + ‚àÇf
‚àÇY ‚Ä≤
‚àÇY ‚Ä≤
‚àÇŒµ
=
‚àÇf
‚àÇY Œ∑ (x) + ‚àÇf
‚àÇY ‚Ä≤ Œ∑‚Ä≤ (x) ,
the integral in (7.69) becomes
d Int (Œµ)
dŒµ
=
x2

x1
 ‚àÇf
‚àÇY Œ∑ (x) + ‚àÇf
‚àÇY ‚Ä≤ Œ∑‚Ä≤ (x)

dx.

7.5 Priors Conveying Little Information
377
Further, since setting Œµ = 0 is equivalent to replacing (Y, Y ‚Ä≤) by (y, y‚Ä≤), use
of the preceding in (7.69) gives
d Int (Œµ)
dŒµ
""""
Œµ=0
=
x2

x1
‚àÇf
‚àÇy Œ∑ (x) dx +
x2

x1
‚àÇf
‚àÇy‚Ä≤ Œ∑‚Ä≤ (x) dx = 0.
(7.70)
Integrating the second term of (7.70) by parts gives
d Int (Œµ)
dŒµ
""""
Œµ=0
=
x2

x1
‚àÇf
‚àÇy Œ∑ (x) dx + ‚àÇf
‚àÇy‚Ä≤ Œ∑ (x)
""""
x2
x1
‚àí
x2

x1
 d
dx

 ‚àÇf
‚àÇy‚Ä≤

Œ∑ (x) dx = 0.
Now recall that at the boundary points, Œ∑ (x1) = Œ∑ (x2) = 0, so the second
term in the preceding three-term expression vanishes. Rearranging,
d Int (Œµ)
dŒµ
""""
Œµ=0
=
x2

x1
‚àÇf
‚àÇy ‚àíd
dx

 ‚àÇf
‚àÇy‚Ä≤

Œ∑ (x) dx = 0.
(7.71)
This condition must hold true for all Œ∑ (x) , in view of the requirements
imposed above. Hence, a condition that the function must obey in order to
extremize the integral (7.66) is that
‚àÇf [x, y (x) , y‚Ä≤ (x)]
‚àÇy
‚àí
 d
dx
‚àÇf [x, y (x) , y‚Ä≤ (x)]
‚àÇy‚Ä≤
%
= 0.
(7.72)
This diÔ¨Äerential equation is called the Eulerian or Euler‚ÄìLagrange condi-
tion (Weinstock, 1974; Fox, 1987). Solving the equation for y (x) provides
the function that minimizes the integral, provided that a minimum exists.
Return now to the problem of Ô¨Ånding the prior density p (Œ∏) that mini-
mizes the integral (7.65) subject to the constraints imposed by the informa-
tion available. As seen before, the constraints may result from specifying
moments of the prior distribution or some features thereof; for example,
one of the constraints is that the prior must be proper. Suppose there are
M constraints having the form
E [qi (Œ∏)] = mi,
i = 0, 1, . . . , M ‚àí1,
where q (¬∑) denotes some function of Œ∏. For instance, if one speciÔ¨Åes the Ô¨Årst
and second moments of the prior distribution, the constraints would be

p (Œ∏) dŒ∏
=
1 = m0,

Œ∏p (Œ∏) dŒ∏
=
E (Œ∏) = m1,

Œ∏2p (Œ∏) dŒ∏
=
E

Œ∏2
= m2.

378
7. The Prior Distribution and Bayesian Analysis
In general, we seek to minimize the objective function
I [p (Œ∏)] =
 
log p (Œ∏)
m (Œ∏)

p (Œ∏) dŒ∏ +
M‚àí1

i=0
Œªi

qi (Œ∏) p (Œ∏) dŒ∏ ‚àími

, (7.73)
with respect to p (¬∑) ; as usual, the Œª‚Ä≤s are Lagrange multipliers. We now
apply the variational argument leading to the derivative (7.70) and, for sim-
plicity, set m (Œ∏) = 1; that is, the reference density is taken to be uniform.
Then, write:
pŒµ (Œ∏)
=
p (Œ∏) + ŒµŒ∑ (Œ∏) ,
p‚Ä≤
Œµ (Œ∏)
=
d
dŒµ [p (Œ∏) + ŒµŒ∑ (Œ∏)] = Œ∑ (Œ∏) .
Using this in (7.73), with m (Œ∏) = 1 gives:
I [pŒµ (Œ∏)] =

{log [p (Œ∏) + ŒµŒ∑ (Œ∏)]} [p (Œ∏) + ŒµŒ∑ (Œ∏)] dŒ∏+
M‚àí1

i=0
Œªi

qi (Œ∏) [p (Œ∏) + ŒµŒ∑ (Œ∏)] dŒ∏ ‚àími

.
Hence:
dI [p (Œ∏) + ŒµŒ∑ (Œ∏)]
dŒµ
=

{log [p (Œ∏) + ŒµŒ∑ (Œ∏)]} Œ∑ (Œ∏) dŒ∏ +

Œ∑ (Œ∏) dŒ∏
+
M‚àí1

i=0
Œªi

qi (Œ∏) Œ∑ (Œ∏) dŒ∏.
Rearranging and setting the derivative to 0 as required by the variational
argument, yields:
dI [p (Œ∏) + ŒµŒ∑ (Œ∏)]
dŒµ
""""
Œµ=0
=
 #
log [p (Œ∏) + ŒµŒ∑ (Œ∏)] +
M‚àí1

i=0
Œªiqi (Œ∏)
$
Œ∑ (Œ∏) dŒ∏
"""""
Œµ=0
= 0.
Since this must be true for all integration paths, the extremizing density
satisÔ¨Åes the equation:
log [p (Œ∏)] +
M‚àí1

i=0
Œªiqi (Œ∏) = 0.
Solving for the maximum entropy density gives
p (Œ∏) = exp

‚àí
M‚àí1

i=0
Œªiqi (Œ∏)

.

7.5 Priors Conveying Little Information
379
After normalization, and noting that the sign of the Lagrange multipliers
is unimportant (one can write Œª‚àó
i = ‚àíŒªi), this becomes
p(Œ∏) =
exp
M‚àí1

i=0
Œª‚àó
i qi(Œ∏)


exp
M‚àí1

i=0
Œª‚àó
i qi(Œ∏)

dŒ∏
.
(7.74)
This is precisely in the form of (7.63), with the latter being a particular case
of (7.74) when the qi (Œ∏) functions are the moments of the prior distribution.
It is important to note that in all cases the maximum entropy distribu-
tion, must be deÔ¨Åned relative to a reference distribution m (Œ∏) ; otherwise,
the concept of entropy does not carry to the continuous case. Here, a uni-
form reference distribution has been adopted arbitrarily. This illustrates
that the concept of maximum entropy (in the continuous case) does not
help to answer completely the question of how a non-informative prior
should be constructed (Bernardo, 1979; Bernardo and Smith, 1994). On
the one hand, information is introduced (and perhaps legitimately so) via
the constraints, these stating features of the prior that are known. On the
other hand, the reference measure must be a representation of ignorance it-
self, so the problem relays to one of how to construct a truly noninformative
(in some sense) reference distribution.
7.5.4
Reference Prior Distributions
Reference analysis (Bernardo, 1979) can be viewed as a way of rendering
inferences as ‚Äúobjective‚Äù as possible, in the sense that the prior should
have a minimal eÔ¨Äect, relative to the data, on posterior inference. As dis-
cussed below, the notion of ‚Äúminimal eÔ¨Äect‚Äù is deÔ¨Åned in a precise manner.
Statisticians often use the term reference priors rather than reference anal-
ysis. The theory is technically involved, and the area is still the subject of
much research. Hence, only a sketch of the main ideas is presented following
Bernardo and Smith (1994) closely.
Single Parameter Model
From (7.29), the amount of information about a parameter that an exper-
iment is expected to provide, can be written as
I [e, h (Œ∏)]
=
 
log
h (Œ∏|y)
h (Œ∏)

h (Œ∏|y) dŒ∏
%
g (y)dy
=
E

log
h (Œ∏|y)
h (Œ∏)

h (Œ∏|y) dŒ∏
%
,
where the expectation is taken with respect to the marginal (in the Bayesian
sense) distribution of the observations, but conditionally on the experimen-

380
7. The Prior Distribution and Bayesian Analysis
tal design adopted, as usual. The notation I [e, h (Œ∏)] makes explicit that
this measure of information is actually a functional of the prior density
h (Œ∏) ; e denotes a speciÔ¨Åc experiment. Note that the information measure
is the expectation of the Kullback‚ÄìLeibler distance between the posterior
and prior distributions taken over all data that can result from this ex-
periment, given a certain probability model. Hence, this expectation must
be nonnegative. Also, it can be veriÔ¨Åed readily that I [e, h (Œ∏)] is invariant
under one-to-one transformations.
Now suppose that the experiment is replicated K times, yielding the
hypothetical data vector
y‚àó
K =

y‚Ä≤
1
y‚Ä≤
2
.
.
.
y‚Ä≤
K
‚Ä≤ .
The sampling model would then be
p (y‚àó
K|Œ∏) =
K
-
i=1
p (yi|Œ∏) .
The expected information from such an experiment is
I [e (K) , h (Œ∏)] =
 
log
h (Œ∏|y‚àó
K)
h (Œ∏)

h (Œ∏|y‚àó
K) dŒ∏
%
g (y‚àó
K)dy‚àó
K.
(7.75)
If the experiment could be replicated an inÔ¨Ånite number of times, one would
be in a situation of perfect or complete information about the parameter.
Hence, the quantity
I [e (‚àû) , h (Œ∏)] = lim
K‚Üí‚àûI [e (K) , h (Œ∏)]
measures, in some sense, the missing information about Œ∏ expressed as a
function of the prior density h (Œ∏) . As more information is contained in
the prior, less is expected to be gained from exhaustive data. On the other
hand, if the prior contains little information, more would be expected to
be gained from valuable experimentation. A ‚Äúnoninformative‚Äù prior would
then be that maximizing the missing information.
The reference prior, denoted as œÄ (Œ∏), is deÔ¨Åned formally to be the prior
that maximizes the missing information functional given above. If œÄK (Œ∏)
denotes the prior density that maximizes I [e (K) , h (Œ∏)] for a certain amount
of replication K, then œÄ (Œ∏) is the limiting value as K ‚Üí‚àûof the sequence
of priors {œÄK (Œ∏) , K = 1, 2, . . .} that ensues as replication increases. Asso-
ciated with each œÄK (Œ∏) there is the corresponding posterior
œÄK (Œ∏|y) ‚àùp (y|Œ∏) œÄK (Œ∏) ,
(7.76)
whose limit, as K ‚Üí‚àû, is deÔ¨Åned as the reference posterior distribution
œÄ (Œ∏|y) = lim
K‚Üí‚àûœÄK (Œ∏|y) .

7.5 Priors Conveying Little Information
381
The reference prior is deÔ¨Åned as any positive function œÄ (Œ∏), such that
œÄ (Œ∏|y) ‚àùp (y|Œ∏) œÄ (Œ∏) .
The deÔ¨Ånition implies that the reference posterior distribution depends only
on the asymptotic behavior of the model since the amount of replication is
allowed to go to inÔ¨Ånity.
The technical details of the procedure for obtaining the reference prior,
that culminates in expressions (7.82) and (7.83), are given below. Consider
(7.75) and rewrite it as
I [e (K) , h (Œ∏)] =
 
log
h (Œ∏|y‚àó
K)
h (Œ∏)

p (y‚àó
K|Œ∏) dy‚àó
K
%
h (Œ∏) dŒ∏.
(7.77)
Since p(y‚àó
K|Œ∏) integrates to 1, the inner integral can be rearranged as

[log h (Œ∏|y‚àó
K)] p (y‚àó
K|Œ∏) dy‚àó
K ‚àílog h (Œ∏) .
Thus
I [e (K) , h (Œ∏)]
=

log

exp

[log h (Œ∏|y‚àó
K)] p (y‚àó
K|Œ∏) dy‚àó
K

h (Œ∏)

h (Œ∏) dŒ∏.
DeÔ¨Åne
fK (Œ∏) = exp

[log h (Œ∏|y‚àó
K)] p (y‚àó
K|Œ∏) dy‚àó
K
%
,
(7.78)
and note that fK (Œ∏) depends implicitly on h (Œ∏) through the posterior
density h (Œ∏|y‚àó
K) . Then one has
I [e (K) , h (Œ∏)] =

log
fK (Œ∏)
h (Œ∏)

h (Œ∏) dŒ∏.
Imposing the constraint that the prior density integrates to 1, the prior
œÄK (Œ∏) that maximizes I [e (K) , h (Œ∏)] must be an extremal of the functional
F {h (Œ∏)} =

log
fK (Œ∏)
h (Œ∏)

h (Œ∏) dŒ∏ + Œª

h (Œ∏) dŒ∏ ‚àí1

,
(7.79)
where Œª is a Lagrange multiplier.
DeÔ¨Åne
hŒµ (Œ∏) = h (Œ∏) + ŒµŒ∑ (Œ∏) ,
so
h‚Ä≤
Œµ (Œ∏) = d
dŒµ [h (Œ∏) + ŒµŒ∑ (Œ∏)] = Œ∑ (Œ∏) .

382
7. The Prior Distribution and Bayesian Analysis
Now we make use of the variational argument employed in connection with
maximum entropy priors and consider the function
F {hŒµ (Œ∏)} =

log
fŒµK (Œ∏)
hŒµ (Œ∏)

hŒµ (Œ∏) dŒ∏ + Œª

hŒµ (Œ∏) dŒ∏ ‚àí1

.
Hence
dF [h (Œ∏) + ŒµŒ∑ (Œ∏)]
dŒµ
""""
Œµ=0
=
  d
dŒµ log
fŒµK (Œ∏)
hŒµ (Œ∏)

hŒµ (Œ∏)
%
dŒ∏ + Œª
  d
dŒµhŒµ (Œ∏)
%
dŒ∏ = 0.
Further,
dF [h (Œ∏) + ŒµŒ∑ (Œ∏)]
dŒµ
""""
Œµ=0
=

h‚Ä≤
Œµ (Œ∏) log
fŒµK (Œ∏)
hŒµ (Œ∏)

dŒ∏
+

hŒµ (Œ∏) hŒµ (Œ∏)
fŒµK (Œ∏)
f ‚Ä≤
ŒµK (Œ∏) hŒµ (Œ∏) ‚àífŒµK (Œ∏) h‚Ä≤
Œµ (Œ∏)
h2Œµ (Œ∏)

dŒ∏
+Œª

h‚Ä≤
Œµ (Œ∏) dŒ∏
=

Œ∑ (Œ∏) log
fŒµK (Œ∏)
hŒµ (Œ∏)

dŒ∏ +
 f ‚Ä≤
ŒµK (Œ∏) hŒµ (Œ∏)
fŒµK (Œ∏)
dŒ∏
‚àí

Œ∑ (Œ∏) dŒ∏ + Œª

Œ∑ (Œ∏) dŒ∏ = 0.
Hence, as in Bernardo and Smith (1994)
dF [h (Œ∏) + ŒµŒ∑ (Œ∏)]
dŒµ
""""
Œµ=0
=

{[log fK (Œ∏)] Œ∑(Œ∏) + h (Œ∏)
fK (Œ∏)f ‚Ä≤
K (Œ∏)
‚àí[log h (Œ∏) + 1] Œ∑(Œ∏) + ŒªŒ∑ (Œ∏)}dŒ∏ = 0,
(7.80)
where f ‚Ä≤
K (Œ∏) is the derivative of fŒµK (Œ∏) with respect to Œµ evaluated at
Œµ = 0.
Given the form of fK (Œ∏) given in (7.78),
f ‚Ä≤
K (Œ∏) = d
dŒµfŒµK (Œ∏)
""""
Œµ=0
= d
dŒµ exp

[log hŒµ (Œ∏|y‚àó
K)] p (y‚àó
K|Œ∏) dy‚àó
K
%""""
Œµ=0
.

7.5 Priors Conveying Little Information
383
Recalling that the posterior is proportional to the product of the prior and
of the sampling model, the preceding can be expressed as
f ‚Ä≤
K (Œ∏) = d
dŒµ exp
 
log
p(y‚àó
K|Œ∏)[h(Œ∏) + ŒµŒ∑(Œ∏)]

p(y‚àó
K|Œ∏)[h(Œ∏) + ŒµŒ∑(Œ∏)]dŒ∏

p (y‚àó
K|Œ∏) dy‚àó
K
%""""
Œµ=0
.
Carrying out the diÔ¨Äerentiation
f ‚Ä≤
K (Œ∏) = fK (Œ∏)
 d
dŒµ

log (p (y‚àó
K|Œ∏) [h (Œ∏) + ŒµŒ∑ (Œ∏)]) p (y‚àó
K|Œ∏) dy‚àó
K
%""""
Œµ=0
‚àífK (Œ∏)
 d
dŒµ
 
log


p (y‚àó
K|Œ∏) [h (Œ∏) + ŒµŒ∑ (Œ∏)] dŒ∏

p (y‚àó
K|Œ∏) dy‚àó
K
%""""
Œµ=0
.
Proceeding with the algebra,
f ‚Ä≤
K (Œ∏) = fK (Œ∏)

d
dŒµ (log [h (Œ∏) + ŒµŒ∑ (Œ∏)]) p (y‚àó
K|Œ∏) dy‚àó
K
%""""
Œµ=0
‚àífK (Œ∏)
√ó

d
dŒµ

log


p (y‚àó
K|Œ∏) [h (Œ∏) + ŒµŒ∑ (Œ∏)] dŒ∏

p (y‚àó
K|Œ∏) dy‚àó
K
%""""
Œµ=0
.
Further,
f ‚Ä≤
K (Œ∏) = fK (Œ∏)

Œ∑ (Œ∏)
h (Œ∏) + ŒµŒ∑ (Œ∏)

p (y‚àó
K|Œ∏) dy‚àó
K
%""""
Œµ=0
‚àífK (Œ∏)


p(y‚àó
K|Œ∏)Œ∑(Œ∏)dŒ∏

p(y‚àó
K|Œ∏)[h(Œ∏)+ŒµŒ∑(Œ∏)]dŒ∏p (y‚àó
K|Œ∏) dy‚àó
K
%""""
Œµ=0
.
Evaluating appropriate terms at Œµ = 0, noting that

p (y‚àó
K|Œ∏) dy‚àó
K = 1,
and that

p (y‚àó
K|Œ∏) h (Œ∏) dy‚àó
K = p(y‚àó
K), gives
f ‚Ä≤
K (Œ∏) = fK (Œ∏) Œ∑ (Œ∏)
h (Œ∏) ‚àífK (Œ∏)


p(y‚àó
K|Œ∏)Œ∑(Œ∏)dŒ∏
p(y‚àó
K)
p (y‚àó
K|Œ∏) dy‚àó
K
%
.
(7.81)
Note now that hŒµ (Œ∏) = h (Œ∏) + ŒµŒ∑ (Œ∏) implies that

hŒµ (Œ∏) dŒ∏ =

h (Œ∏) dŒ∏ + Œµ

Œ∑ (Œ∏) dŒ∏.
Hence,

Œ∑ (Œ∏) dŒ∏ = 0 is a necessary condition for h (¬∑) being a p.d.f.. This
follows because, then,

hŒµ (Œ∏) dŒ∏ =

h (Œ∏) dŒ∏ = 1.

384
7. The Prior Distribution and Bayesian Analysis
Now, if h (¬∑) is a proper density function, the posterior is also a proper
density. Therefore,
1
=
 p (y‚àó
K|Œ∏) hŒµ (Œ∏) dŒ∏
p(y‚àó
K)
=
 p (y‚àó
K|Œ∏) h (Œ∏) dŒ∏
p(y‚àó
K)
+ Œµ
 p (y‚àó
K|Œ∏) Œ∑ (Œ∏) dŒ∏
p(y‚àó
K)
=
1 + Œµ
 p (y‚àó
K|Œ∏) Œ∑ (Œ∏) dŒ∏
p(y‚àó
K)
= 1.
Hence

p (y‚àó
K|Œ∏) Œ∑ (Œ∏) dŒ∏ = 0. Using this condition in (7.81), we get
f ‚Ä≤
K (Œ∏) = fK (Œ∏) Œ∑ (Œ∏)
h (Œ∏).
Employing this in (7.80), the condition that the extremal must satisfy is

{log [fK (Œ∏)] + 1 ‚àílog h (Œ∏) ‚àí1 + Œª} Œ∑(Œ∏) dŒ∏
=

{log [fK (Œ∏)] ‚àílog h (Œ∏) + Œª} Œ∑(Œ∏)dŒ∏ = 0.
Since this must hold for all Œ∑ (Œ∏) , the extremizing density can be found by
solving
log [fK (Œ∏)] ‚àílog h (Œ∏) + Œª = 0.
Upon retaining only the terms that vary with Œ∏:
h (Œ∏)
‚àù
exp {log [fK (Œ∏)] + Œª}
‚àù
fK (Œ∏) .
(7.82)
Finally, in view of the form of fK (Œ∏) given in (7.78),
h (Œ∏) ‚àùexp

[log h (Œ∏|y‚àó
K)] p (y‚àó
K|Œ∏) dy‚àó
K
%
.
(7.83)
This prior h (Œ∏), which maximized I [e (K) , h (Œ∏)] for each K, was denoted
before œÄK (Œ∏). Expression (7.83) gives an implicit solution because fK (Œ∏)
depends on the prior through the posterior density h (Œ∏|y‚àó
K) . However, as
K ‚Üí‚àûthe posterior density h (Œ∏|y‚àó
K) will approach an asymptotic form
with density h‚àó(Œ∏|y‚àó
K) say, that does not depend on the prior at all. Such
an approximation is given, for example, by the asymptotic process (7.6),
either with I (Œ∏) or 5H as precision matrix. Let the asymptotic posterior
have density h‚àó(Œ∏|y‚àó
K) . Then, the sequence of positive functions
h‚àó
K (Œ∏) ‚àùexp

[log h‚àó(Œ∏|y‚àó
K)] p (y‚àó
K|Œ∏) dy‚àó
K
%
,
K = 1, 2, . . . ,
(7.84)

7.5 Priors Conveying Little Information
385
derived from such an asymptotic posterior, will induce a sequence of pos-
terior distributions
œÄK (Œ∏|y) ‚àùp (y|Œ∏) h‚àó
K (Œ∏) ,
K = 1, 2, . . . ,
as in (7.76). Note that p (y|Œ∏) is the density of the actual observations
resulting from the experiment. Then, as in (7.76), the reference posterior
distribution of Œ∏, œÄ (Œ∏|y), is deÔ¨Åned as the limiting distribution resulting
from this K-fold replicated ‚Äúconceptual experiment‚Äù
œÄ (Œ∏|y) = lim
K‚Üí‚àûœÄK (Œ∏|y) ,
(7.85)
where the limit is understood in the information-entropy sense
lim
K‚Üí‚àû

log
œÄK (Œ∏|y)
œÄ (Œ∏|y)

œÄK (Œ∏|y) dŒ∏ = 0.
The reference prior is a function retrieving the reference posterior œÄ (Œ∏|y)
by formal use of Bayes theorem, i.e., a positive function œÄ (Œ∏), such that,
for all y,
œÄ (Œ∏|y) =
p (y|Œ∏) œÄ (Œ∏)

p (y|Œ∏) œÄ (Œ∏) dŒ∏.
The limiting posterior distribution (7.85) is the same as the one that
could be obtained from the sequence of priors œÄK (Œ∏) maximizing the ex-
pected information I[e (K) , h (Œ∏)], as obtained from (7.76).
Although the construction guarantees that œÄK (Œ∏) is proper for each K,
this is not so for œÄ (Œ∏) , the limiting reference prior, as will be shown in some
examples. Hence, only the reference posterior distribution is amenable to
probabilistic interpretation. Also, observe that each of the terms in the se-
quence is the expected value of the logarithm of the density of the asymp-
totic approximation of the posterior taken with respect to the sampling
distribution of the observations generated in the appropriate K-fold repli-
cated experiment. This deÔ¨Ånes an algorithm for arriving at the form of the
reference function h‚àó
K (Œ∏) , with this leading to the reference prior when
K ‚Üí‚àû. Reference priors are, thus, limiting forms.
Invariance under Transformation
The invariance under one-to-one transformations is shown in Bernardo
(1979). Let Œæ = g (Œ∏) be such a transformation. The reference prior of Œæ
should have the form
œÄ (Œæ) ‚àùœÄ

g‚àí1 (Œæ)
 """"
dg‚àí1 (Œæ)
dŒæ
"""" ‚àùœÄ

g‚àí1 (Œæ)

|JŒæ| ,

386
7. The Prior Distribution and Bayesian Analysis
where |JŒæ| is the absolute value of the Jacobian of the transformation. The
sequence of functions approaching the reference prior, using (7.84), is
h‚àó
K (Œæ)
‚àù
exp

[log h‚àó(Œæ|y‚àó
K)] p (y‚àó
K|Œæ) dy‚àó
K
%
‚àù
exp

[log h‚àó(Œ∏|y‚àó
K) |JŒæ|] p (y‚àó
K|Œ∏) dy‚àó
K
%
‚àù
exp (log |JŒæ|) exp

[log h‚àó(Œ∏|y‚àó
K)] p (y‚àó
K|Œ∏) dy‚àó
K
%
‚àù
exp

[log h‚àó(Œ∏|y‚àó
K)] p (y‚àó
K|Œ∏) dy‚àó
K
%
|JŒæ|
‚àù
h‚àó
K (Œ∏) |JŒæ| .
Hence, the reference functions follow the usual rules for change of variables,
that is, the reference function for Œæ is proportional to the product of the
reference function for Œ∏ times the absolute value of the Jacobian of the
transformation.
Reference Prior under Consistency and Asymptotic Normality
It is now shown that the reference prior has an explicit form when a consis-
tent estimator of Œ∏ exists, and when the asymptotic posterior distribution
of Œ∏, given the hypothetical data y‚àó
K from a K-fold replicated experiment,
is normal. As mentioned at the beginning of this chapter, important regu-
larity conditions must be satisÔ¨Åed to justify these asymptotic assumptions.
Consider an experiment based on n observations. As before, let y‚àó
K be a
hypothetical data vector of order kn √ó 1 resulting from a K-fold replicate
of the said experiment. Also, let 5Œ∏kn be a suÔ¨Écient statistic or estimator
(a function of y‚àó
K) such that, with complete certainty,
lim
K‚Üí‚àû
5Œ∏kn = Œ∏.
Since 5Œ∏kn is suÔ¨Écient for the parameter, the posterior density of Œ∏, given
the data, is the same as the posterior density, given 5Œ∏kn. This is so because
the likelihood function can be written as the product of a function of the
data only (which gets absorbed in the integration constant), times another
part involving both Œ∏ and 5Œ∏kn. Next, let the asymptotic approximation to
the posterior density be
h‚àó(Œ∏|y‚àó
K) = h‚àó
Œ∏|5Œ∏kn

.
The counterpart of (7.84) can be written as
h‚àó
K (Œ∏) ‚àùexp
 
log h‚àó
Œ∏|5Œ∏kn

p

5Œ∏kn|Œ∏

d5Œ∏kn
%
,

7.5 Priors Conveying Little Information
387
where p

5Œ∏kn|Œ∏

is the density of the sampling distribution of the suÔ¨Écient
statistic or consistent estimator. Now evaluate the asymptotic approxima-
tion at 5Œ∏kn = Œ∏, and denote this as:
h‚àó
Œ∏|5Œ∏kn
"""Œ∏kn=Œ∏ .
This should be very ‚Äúclose‚Äù to h‚àó
Œ∏|5Œ∏kn

(in the Kullback‚ÄìLeibler sense).
Hence, one can write
h‚àó
K (Œ∏)
‚àù
exp
 
log h‚àó
Œ∏|5Œ∏kn
"""Œ∏kn=Œ∏

p

5Œ∏kn|Œ∏

d5Œ∏kn
%
‚àù
exp

log h‚àó
Œ∏|5Œ∏kn
"""Œ∏kn=Œ∏
 
p

5Œ∏kn|Œ∏

d5Œ∏kn
%
‚àù
h‚àó
Œ∏|5Œ∏kn
"""Œ∏kn=Œ∏ .
(7.86)
This implies that h‚àó
K (Œ∏) is proportional to the density of any asymptotic
approximation to the posterior in which the consistent estimator 5Œ∏kn is
replaced by the unknown parameter Œ∏.
Suppose now that the asymptotic posterior distribution of Œ∏ is normal;
as seen in Section 7.3, the assumption is valid under regularity conditions.
Hence, for a K-fold replicated experiment, write
Œ∏|5Œ∏kn ‚àºN

5Œ∏kn,

knI1

5Œ∏kn
‚àí1
,
where I1

5Œ∏kn

is Fisher‚Äôs information measure for a single observation
evaluated at 5Œ∏kn. Then, by virtue of (7.86), the reference function must be
h‚àó
K (Œ∏)
‚àù
N

Œ∏,

knI1

5Œ∏kn
‚àí1""""Œ∏kn=Œ∏
‚àù
1

knI1

5Œ∏kn
‚àí1
2 exp

‚àí
knI1(Œ∏kn)
2

Œ∏ ‚àí5Œ∏kn
2
"""""""
Œ∏kn=Œ∏
‚àù
>
I1 (Œ∏).
(7.87)
The limit of this sequence, which is the reference prior œÄ (Œ∏), is also
>
I1 (Œ∏),
the square root of Fisher‚Äôs information measure for a single observation.
Hence,
œÄ (Œ∏) ‚àù
>
I1 (Œ∏).
It follows that in the special case of a single continuous parameter and when
the posterior distribution is asymptotically normal, the reference prior al-
gorithm yields JeÔ¨Äreys‚Äô invariance prior (see Section 7.5.2).

388
7. The Prior Distribution and Bayesian Analysis
Example 7.23
Reference prior for the mean of a normal distribution
The experiment consists of n samples from a normal distribution with
unknown mean ¬µ and known variance œÉ2. Fisher‚Äôs information measure
for a single observation is
Ey|¬µ,œÉ2

‚àí
d2
(d¬µ)2

log
1
‚àö
2œÄœÉ2 exp

‚àí1
2œÉ2 (y ‚àí¬µ)2
%
= 1
œÉ2 .
Hence, the reference prior œÄ (¬µ) is a constant, since the information measure
does not involve the parameter of interest. Note that the reference prior is
improper. Although, by construction, the reference functions obtained by
maximizing the information measure I [e (K) , h (Œ∏)] are proper for each K,
as shown in (7.79), the reference prior obtained by taking limits may be
improper.
‚ñ†
Example 7.24
Reference prior for the probability of success in the bino-
mial distribution
The experiment consists of n Bernoulli trials with success probability Œ∏.
The distribution of a single observation in a Bernoulli trial is
p (y|Œ∏) = Œ∏y (1 ‚àíŒ∏)1‚àíy ,
where y takes the value 1 with probability Œ∏ or the value 0 with probability
1 ‚àíŒ∏. The information measure from a single draw is
Ey|Œ∏
#
‚àíd2
(dŒ∏)2 [y log Œ∏ + (1 ‚àíy) log (1 ‚àíŒ∏)]
$
= Ey|Œ∏

y
Œ∏2 + (1 ‚àíy)
(1 ‚àíŒ∏)2

=
1
Œ∏ (1 ‚àíŒ∏).
The reference prior is then
œÄ (Œ∏) ‚àù
C
1
Œ∏ (1 ‚àíŒ∏),
which is a Be
 1
2, 1
2

distribution. The reference prior is proper in this case,
and the reference posterior distribution is
œÄ (Œ∏|y)
‚àù
Œ∏y (1 ‚àíŒ∏)n‚àíy Œ∏‚àí1
2 (1 ‚àíŒ∏)‚àí1
2
‚àù
Œ∏y+ 1
2 ‚àí1 (1 ‚àíŒ∏)n‚àíy+ 1
2 ‚àí1 ,
where y is the number of successes observed out of n Bernoulli trials.
The posterior is then a Be

y + 1
2, n ‚àíy + 1
2

distribution, which is always
proper (even if all trials are all successes or all failures). If all observations

7.5 Priors Conveying Little Information
389
are failures, the mean of the posterior distribution is equal to 1/(2n + 2).
For example, if n = 1000 Ô¨Çies are screened in search of a speciÔ¨Åc mutant
and all are found to be normal, the posterior mean estimate of the muta-
tion rate is 1/2002. The Bayesian estimate admits the possibility that the
population is liable to at least some mutation. On the other hand, the ML
estimator of the mutation rate would be 0 in this case.
‚ñ†
Presence of a Single Nuisance Parameter
Suppose that the model has two unknown parameters, so Œ∏ = (Œ∏1, Œ∏2)‚Ä≤ .
Parameter Œ∏1 is of primary inferential interest and Œ∏2 acts as a nuisance
parameter. This will be called an ordered parameterization, denoted as
(Œ∏1, Œ∏2) . The problem is to develop a reference prior for Œ∏1. As usual, the
joint prior density can be written as
h (Œ∏) = h (Œ∏1) h (Œ∏2|Œ∏1) ,
where h (Œ∏2|Œ∏1) is the density of the conditional distribution of the nuisance
parameter, given Œ∏1. Correspondingly, the density of the Œ∏1-reference prior
distribution is expressible as
œÄŒ∏1 (Œ∏1, Œ∏2) = œÄŒ∏1 (Œ∏1) œÄŒ∏1 (Œ∏2|Œ∏1) .
It is important to note that the reference prior may depend on the order
of the parameterization. That is, the Œ∏1-reference prior distribution will be
diÔ¨Äerent, in general, from the Œ∏2-reference prior distribution with density
œÄŒ∏2 (Œ∏2, Œ∏1) = œÄŒ∏2 (Œ∏2) œÄŒ∏2 (Œ∏1|Œ∏2) .
This is perplexing at Ô¨Årst sight, but it can be explained on the grounds
that the information-theoretic measures that are maximized involve loga-
rithmic divergences between distributions, and these depend on the speciÔ¨Åc
distributions intervening.
Note that if the conditional reference prior density œÄ (Œ∏2|Œ∏1) were known,
it could be used to integrate the nuisance parameter Œ∏2 out of the likelihood
to obtain the one-parameter model
p (y|Œ∏1) =

p (y|Œ∏1, Œ∏2) œÄ (Œ∏2|Œ∏1) dŒ∏2,
from which œÄ (Œ∏1) can be deduced as before. Then, the reference posterior
for Œ∏1 would be
p (Œ∏1|y) ‚àùp (y|Œ∏1) œÄ (Œ∏1) .
We now describe the algorithm for obtaining the marginal reference pos-
terior density of Œ∏1. There are essentially two steps. First, the conditional
reference prior for the nuisance parameter, œÄ (Œ∏2|Œ∏1) (dropping the sub-
scripts indexing œÄ from now on), can be arrived at by applying the algo-
rithm presented before for the single parameter situation. This is because,

390
7. The Prior Distribution and Bayesian Analysis
given Œ∏1, p (y|Œ∏1, Œ∏2) only depends on the nuisance parameter Œ∏2. The ref-
erence function is now
h‚àó
K (Œ∏2|Œ∏1) ‚àùexp

[log h‚àó(Œ∏2|Œ∏1, y‚àó
K)] p(y‚àó
K|Œ∏)dy‚àó
K
%
,
K = 1, 2, . . . ,
where h‚àó(Œ∏2|Œ∏1, y‚àó
K) is the density of any asymptotic approximation to the
conditional posterior distribution of Œ∏2, given Œ∏1, that does not depend on
the prior. The conditional reference prior, œÄ (Œ∏2|Œ∏1) is obtained as the limit,
as K ‚Üí‚àû, of the sequence of reference functions, h‚àó
K (Œ∏2|Œ∏1), as before.
The Ô¨Årst step of the algorithm is completed, using œÄ (Œ∏2|Œ∏1) to integrate out
the nuisance parameter, thus obtaining the integrated likelihood p (y‚àó
K|Œ∏1):
p (y‚àó
K|Œ∏1) =

p (y‚àó
K|Œ∏1, Œ∏2) œÄ (Œ∏2|Œ∏1) dŒ∏2.
This step assumes that œÄ (Œ∏2|Œ∏1) is a proper density. Recall, however, that
reference analysis can lead to improper reference priors. If this is the case,
the algorithm will not work. We return to this point brieÔ¨Çy at the end of
this section.
In the second step, the algorithm is again applied using as reference
function
h‚àó‚àó
K (Œ∏1) ‚àùexp

[log h‚àó‚àó(Œ∏1|y‚àó
K)] p (y‚àó
K|Œ∏1) dy‚àó
K
%
,
K = 1, 2, . . . ,
(7.88)
where h‚àó‚àó(Œ∏1|y‚àó
K) is any asymptotic approximation to the posterior distri-
bution of the parameter of interest, but constructed using the integrated
likelihood p (y‚àó
K|Œ∏1). The marginal reference prior œÄ (Œ∏1) is obtained as the
limit, as K ‚Üí‚àû, of the sequence of reference functions h‚àó‚àó
K (Œ∏1). Finally,
the reference posterior of Œ∏1 is obtained as
p (Œ∏1|y) ‚àùp (y|Œ∏1) œÄ (Œ∏1) ,
which completes the algorithm.
We shall now consider the regular case where joint posterior asymptotic
normality can be established. It is also assumed that the conditional refer-
ence prior is proper. The asymptotic approximation to the joint posterior
distribution of Œ∏ = (Œ∏1, Œ∏2)‚Ä≤ is written as
 Œ∏1
Œ∏2

|5Œ∏kn ‚àºN

5Œ∏1kn
5Œ∏2kn

,

IŒ∏1Œ∏1
IŒ∏1Œ∏2
IŒ∏1Œ∏2
IŒ∏2Œ∏2
‚àí1"""""
Œ∏=Œ∏

.
Recall that IŒ∏2Œ∏2 is the information about Œ∏2 in a model, either without Œ∏1
or when assuming that this parameter is known. Also, let

IŒ∏1Œ∏1
IŒ∏1Œ∏2
IŒ∏1Œ∏2
IŒ∏2Œ∏2
‚àí1
=

IŒ∏1Œ∏1
IŒ∏1Œ∏2
IŒ∏1Œ∏2
IŒ∏2Œ∏2

,

7.5 Priors Conveying Little Information
391
and note than an asymptotic approximation to the marginal posterior dis-
tribution of Œ∏1 is given by
Œ∏1|5Œ∏kn ‚àºN

5Œ∏1kn, IŒ∏1Œ∏1""
Œ∏=Œ∏

.
(7.89)
With independent samples, the information matrix is nk times the infor-
mation from a single observation, so IŒ∏1Œ∏1 is equal to IŒ∏1Œ∏1
1
/nk, where IŒ∏1Œ∏1
1
is the appropriate part of the inverse of Fisher‚Äôs information matrix for a
single observation. Further, IŒ∏1Œ∏1
1
is typically a function of both Œ∏1 and Œ∏2;
hence, it is instructive to write IŒ∏1Œ∏1
1
as IŒ∏1Œ∏1
1
(Œ∏1, Œ∏2).
Employing the argument in (7.87), the reference function for arriving at
the conditional reference prior of the nuisance parameter is
h‚àó
K (Œ∏2|Œ∏1) ‚àù
8
I1(Œ∏2Œ∏2).
(7.90)
Hence, the density of the conditional reference prior for the nuisance pa-
rameter is
œÄ (Œ∏2|Œ∏1) ‚àù
8
I1(Œ∏2Œ∏2).
Since we assume that this is proper, the integrated likelihood is
p (y‚àó
K|Œ∏1)
=

p (y‚àó
K|Œ∏1, Œ∏2) œÄ (Œ∏2|Œ∏1) dŒ∏2
‚àù

p (y‚àó
K|Œ∏1, Œ∏2)
8
I1(Œ∏2Œ∏2) dŒ∏2.
Consequently, the reference function needed to arrive at the reference prior
for the parameter of interest Œ∏1 in (7.88) has the form
h‚àó‚àó
K (Œ∏1)
‚àùexp

[log h‚àó‚àó(Œ∏1|y‚àó
K)]

p (y‚àó
K|Œ∏1, Œ∏2)
8
I1(Œ∏2Œ∏2)dŒ∏2

dy‚àó
K
%
. (7.91)
Rearranging the integral expression
h‚àó‚àó
K (Œ∏1)
‚àùexp
 8
I1(Œ∏2Œ∏2)

[log h‚àó‚àó(Œ∏1|y‚àó
K)] p (y‚àó
K|Œ∏1, Œ∏2) dy‚àó
K
%
dŒ∏2
%
‚àùexp
 8
I1(Œ∏2Œ∏2)Ey‚àó
K|Œ∏1,Œ∏2 [log h‚àó‚àó(Œ∏1|y‚àó
K)] dŒ∏2
%
,
(7.92)
which is a function of Œ∏1 only since Œ∏2 is integrated out. Now, using the
asymptotic approximation (7.89) to the marginal posterior of Œ∏1, and taking
expectations over the distribution of the observations arising in the K-
fold replicated experiment, for large K (recall that 5Œ∏1kn is asymptotically

392
7. The Prior Distribution and Bayesian Analysis
unbiased)
Ey‚àó
K|Œ∏1,Œ∏2 [log h‚àó‚àó(Œ∏1|y‚àó
K)]
= Ey‚àó
K|Œ∏1,Œ∏2
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£∞
log
1
C
2œÄ

IŒ∏1Œ∏1
1
(Œ∏1,Œ∏2)
nk
 ‚àí
nk

Œ∏1 ‚àí5Œ∏1kn
2
2

IŒ∏1Œ∏1
1
(Œ∏1, Œ∏2)

Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
= ‚àílog
@
A
A
B2œÄ

IŒ∏1Œ∏1
1
(Œ∏1, Œ∏2)
nk

‚àí1
2 ‚àùlog

IŒ∏1Œ∏1
1
(Œ∏1, Œ∏2)
‚àí1
2 .
Using this in (7.92):
h‚àó‚àó
K (Œ∏1) ‚àùexp
 8
I1(Œ∏2Œ∏2) log

IŒ∏1Œ∏1
1
(Œ∏1, Œ∏2)
‚àí1
2 dŒ∏2
%
,
(7.93)
and recall that œÄ (Œ∏1) ‚àùh‚àó‚àó
K (Œ∏1) for K ‚Üí‚àû. Since (7.93) involves infor-
mation measures (or their inverses) for a single observation, this gives the
reference prior directly. Hence, if the conditional reference prior œÄ (Œ∏2|Œ∏1) is
proper, the reference prior of the primary parameter is obtained as follows:
(1) Compute Fisher‚Äôs information measure about the nuisance parameter
from a single observation (acting as if the primary parameter were known
or absent from the model).
(2) Compute Fisher‚Äôs information matrix for a single observation for the
full, two-parameter model, and invert it (this may depend on both Œ∏1 and
Œ∏2).
Then proceed to evaluate the integral and the exponential function in
(7.93).
Suppose now that >I1(Œ∏2Œ∏2) factorizes as
8
I1(Œ∏2Œ∏2) ‚àùfŒ∏2 (Œ∏1) gŒ∏2 (Œ∏2) .
This implies that the conditional reference prior of the nuisance parameter
is
œÄ (Œ∏2|Œ∏1) = agŒ∏2 (Œ∏2) ,
(7.94)
where a‚àí1 =

gŒ∏2 (Œ∏2) dŒ∏2. Also suppose that the inverse of Fisher‚Äôs infor-
mation measure (the Œ∏1 part) factorizes as

IŒ∏1Œ∏1
1
(Œ∏1, Œ∏2)
‚àí1
2 ‚àùfŒ∏1 (Œ∏1) gŒ∏1 (Œ∏2) .

7.5 Priors Conveying Little Information
393
Then, if the space of the nuisance parameter does not depend on Œ∏1, appli-
cation of these conditions in (7.93) gives
h‚àó‚àó
K (Œ∏1) ‚àùexp

agŒ∏2 (Œ∏2) log [fŒ∏1 (Œ∏1) gŒ∏1 (Œ∏2)] dŒ∏2
%
‚àùexp

log [fŒ∏1 (Œ∏1)]

agŒ∏2 (Œ∏2) dŒ∏2 +

log [gŒ∏1 (Œ∏2)] agŒ∏2 (Œ∏2) dŒ∏2
%
‚àùexp {log [fŒ∏1 (Œ∏1)]} ‚àùfŒ∏1 (Œ∏1) ,
(7.95)
since

agŒ∏2 (Œ∏2) dŒ∏2 = 1. Combining (7.94) and (7.95) yields as Œ∏1-reference
prior,
œÄŒ∏1 (Œ∏1, Œ∏2) ‚àùfŒ∏1 (Œ∏1) gŒ∏2 (Œ∏2) .
(7.96)
This result holds irrespective of whether the conditional reference prior is
proper or not.
If the conditional reference prior of the nuisance parameter is not proper,
the technical arguments are more involved. Bernardo and Smith (1994)
indicate that the entire parameter space of Œ∏2, say Œò2, must be broken into
increasing sequences Œò2i, possibly dependent on Œ∏1. For each sequence, one
obtains a normalized conditional reference prior such that
œÄi (Œ∏2|Œ∏1) =
œÄ (Œ∏2|Œ∏1)

Œò2i
œÄ (Œ∏2|Œ∏1) dŒ∏2
,
i = 1, 2, . . . .
An integrated likelihood is obtained for each i, and a marginal reference
prior œÄi (Œ∏1) is identiÔ¨Åed using the procedures described above, to arrive
at the joint prior {œÄi (Œ∏1) œÄi (Œ∏2|Œ∏1)} . The limit of this sequence yields the
desired reference prior. The strategy requires identifying suitable ‚Äúcuts‚Äù of
the parameter space.
Example 7.25
Reference prior for the mean of a normal distribution:
unknown standard deviation
Let n samples be drawn from a normal distribution with mean and stan-
dard deviation ¬µ and œÉ, respectively, with both parameters unknown. First
we shall derive the reference posterior distribution for the ordered param-
eterization (¬µ, œÉ) in which œÉ acts as a nuisance parameter. Second, the
reference analysis is carried out for the ordered parameterization (œÉ, ¬µ) .
Fisher‚Äôs information measure for a single observation is formed from the
expected negative second derivatives
Ey|¬µ,œÉ
#
‚àí
‚àÇ2
(‚àÇ¬µ)2 log

1
œÉ
‚àö
2œÄ exp

‚àí1
2œÉ2 (y ‚àí¬µ)2
%$
= 1
œÉ2 ,
Ey|¬µ,œÉ

‚àí‚àÇ2
‚àÇ¬µ‚àÇœÉ log

1
œÉ
‚àö
2œÄ exp

‚àí1
2œÉ2 (y ‚àí¬µ)2
%%
= 0,

394
7. The Prior Distribution and Bayesian Analysis
Ey|¬µ,œÉ
#
‚àí
‚àÇ2
(‚àÇœÉ)2 log

1
œÉ
‚àö
2œÄ exp

‚àí1
2œÉ2 (y ‚àí¬µ)2
%$
= 2
œÉ2 .
Thus, Fisher‚Äôs information matrix for n = 1, and arranging it consistently
with the ordered parameterization (¬µ, œÉ) , is

I1(¬µ¬µ)
I1(¬µœÉ)
I1(¬µœÉ)
I1(œÉœÉ)

=
Ô£Æ
Ô£ØÔ£∞
1
œÉ2
0
0
2
œÉ2
Ô£π
Ô£∫Ô£ª.
The inverse is

I¬µ¬µ
1
I¬µœÉ
1
I¬µœÉ
1
IœÉœÉ
1

=
Ô£Æ
Ô£∞
œÉ2
0
0
œÉ2
2
Ô£π
Ô£ª.
The conditional reference prior of œÉ, given ¬µ, using (7.90), is
œÄ (œÉ|¬µ) ‚àù
8
I1(œÉœÉ) ‚àù
?
2
œÉ2 ‚àù1
œÉ .
Since the conditional reference prior is not proper, one encounters the tech-
nical diÔ¨Éculty mentioned earlier. However, we proceed to check whether
the conditions leading to (7.96) hold. Following Bernardo (2001), note that
8
I1(œÉœÉ) =
‚àö
2œÉ‚àí1
factorizes as
8
I1(œÉœÉ) = fœÉ (¬µ) gœÉ (œÉ) ,
where fœÉ (¬µ) =
‚àö
2 and gœÉ (œÉ) = œÉ‚àí1. Also, [I¬µ¬µ
1 ]‚àí1
2 = œÉ‚àí1 factorizes as
[I¬µ¬µ
1 ]‚àí1
2 ‚àùf¬µ (¬µ) g¬µ (œÉ) ,
where f¬µ (¬µ) = 1 and g¬µ (œÉ) = œÉ‚àí1. Thus (7.96) leads to the ¬µ‚àíreference
prior
œÄ¬µ (¬µ, œÉ) = œÄ (œÉ|¬µ) œÄ (¬µ) ‚àùf¬µ (¬µ) gœÉ (œÉ) = 1 √ó œÉ‚àí1 = œÉ‚àí1.
Hence, the reference prior for the mean is the improper uniform prior and
the joint ¬µ-reference prior is the reciprocal of the standard deviation. The
reference posterior distribution of ¬µ has density
œÄ¬µ (¬µ|y)
‚àù

p (y|¬µ, œÉ) œÄ¬µ (¬µ, œÉ) dœÉ
‚àù

(œÉ)‚àín‚àí1 exp
#
‚àí1
2œÉ
n

i=1
(yi ‚àí¬µ)2
$
dœÉ.

7.5 Priors Conveying Little Information
395
Carrying out the integration, as seen earlier in the book, leads to a univariate-
t process with mean y, scale parameter
n
i=1
(yi ‚àíy)2
n ‚àí1
,
and n ‚àí1 degrees of freedom as reference posterior distribution.
‚ñ†
Example 7.26
Reference prior for the standard deviation of a normal
distribution: unknown mean
The setting is as in Example 7.25 but we now consider the situation where
¬µ acts as nuisance parameter. Here, >I1(¬µ¬µ) = 1/œÉ factorizes as f¬µ (¬µ) =
1, g¬µ (œÉ) = œÉ‚àí1, and [IœÉœÉ
1 ]‚àí1
2 =
‚àö
2/œÉ factorizes as fœÉ (¬µ) =
‚àö
2 times
gœÉ (œÉ) = œÉ‚àí1. This leads to the œÉ-reference prior
œÄœÉ (¬µ, œÉ) ‚àùf¬µ (¬µ) gœÉ (œÉ) = 1 √ó œÉ‚àí1 = œÉ‚àí1,
which is identical to the joint ¬µ-reference prior in this case. The reference
posterior density is then
œÄœÉ (œÉ|y) ‚àù

(œÉ)‚àín‚àí1 exp
#
‚àí1
2œÉ
n

i=1
(yi ‚àí¬µ)2
$
d¬µ.
This integration leads to a scaled inverted chi-square density with parame-
ters (n ‚àí1) n
i=1 (yi ‚àíy)2 and n ‚àí1. Equivalently, the reference posterior
distribution for inferring œÉ is the process
Ga
Ô£´
Ô£¨
Ô£¨
Ô£≠
n ‚àí1
2
,
(n ‚àí1)
n
i=1
(yi ‚àíy)2
2
Ô£∂
Ô£∑
Ô£∑
Ô£∏.
‚ñ†
Example 7.27
Reference prior for a standardized mean
Following Bernardo and Smith (1994) and Bernardo (2001), consider in-
ferring œÜ = ¬µ/œÉ, a standardized mean or reciprocal of the coeÔ¨Écient of
variation of a normal distribution, with the standard deviation œÉ acting as
a nuisance parameter. The sampling model is as in the two preceding exam-
ples, but parameterized in terms of œÜ. In order to form Fisher‚Äôs information
measure for a single observation, the required derivatives are
Ey|œÜ,œÉ
#
‚àí
‚àÇ2
(‚àÇœÜ)2 log

1
œÉ
‚àö
2œÄ exp

‚àí1
2œÉ2 (y ‚àíœÜœÉ)2
%$
= 1,
Ey|œÜ,œÉ

‚àí‚àÇ2
‚àÇœÜ‚àÇœÉ log

1
œÉ
‚àö
2œÄ exp

‚àí1
2œÉ2 (y ‚àíœÜœÉ)2
%%
= œÜ
œÉ ,

396
7. The Prior Distribution and Bayesian Analysis
Ey|œÜ,œÉ
#
‚àí
‚àÇ2
(‚àÇœÉ)2 log

1
œÉ
‚àö
2œÄ exp

‚àí1
2œÉ2 (y ‚àíœÜœÉ)2
%$
= 2 + œÜ2
œÉ2
.
The information matrix is, thus,

I1(œÜœÜ)
I1(œÜœÉ)
I1(œÜœÉ)
I1(œÉœÉ)

=
Ô£Æ
Ô£ØÔ£∞
1
œÜ
œÉ
œÜ
œÉ
2 + œÜ2
œÉ2
Ô£π
Ô£∫Ô£ª,
with inverse

IœÜœÜ
1
IœÜœÉ
1
IœÜœÉ
1
IœÉœÉ
1

=
Ô£Æ
Ô£ØÔ£∞
1 + œÜ2
2
‚àíœÜœÉ
2
‚àíœÜœÉ
2
œÉ2
2
Ô£π
Ô£∫Ô£ª.
If œÉ is the nuisance parameter, then
8
I1(œÉœÉ) ‚àù
8
2 + œÜ2œÉ‚àí1,
factorizes into fœÉ (œÜ) =
>
2 + œÜ2 and gœÉ (œÉ) = œÉ‚àí1. Further

IœÜœÜ
1
‚àí1
2 ‚àù

1 + œÜ2
2
‚àí1
2
factorizes into fœÜ (œÜ) =

1 + œÜ2
2
‚àí1
2 and gœÜ (œÉ) = 1. The œÜ‚àíreference prior
is then given by
œÄœÜ (œÜ, œÉ) ‚àùfœÜ (œÜ) gœÉ (œÉ) ‚àù

1 + œÜ2
2
‚àí1
2
œÉ‚àí1.
‚ñ†
Multiparameter Models
In a multiparameter situation, the reference posterior distribution is rela-
tive to an ordered parameterization of inferential interest. For example, if
the ordered parameter vector is:
Œ∏ =
 Œ∏1
Œ∏2
.
.
.
Œ∏m
‚Ä≤ ,
so that Œ∏m is of primary inferential interest, the corresponding reference
prior can be represented as
œÄŒ∏m (Œ∏) = œÄŒ∏m (Œ∏m|Œ∏1, Œ∏2, ..., Œ∏m‚àí1) œÄŒ∏m (Œ∏m‚àí1|Œ∏1, Œ∏2, ..., Œ∏m‚àí2) √ó ¬∑ ¬∑ ¬∑
¬∑ ¬∑ ¬∑ √ó œÄŒ∏m (Œ∏2|Œ∏1) œÄŒ∏m (Œ∏1) .

7.5 Priors Conveying Little Information
397
An algorithm for deriving the reference prior under asymptotic normality
(an extension of the procedure described for the two parameter situation)
is presented in Berger and Bernardo (1992) and in Bernardo and Smith
(1994). The method is quite involved algebraically and is not presented
here.
As a Ô¨Ånal comment, the reader should be aware that there is no consen-
sus among Bayesians on this topic. For example, Lindley, in the discussion
of Bernardo (1979), remarks that ‚Äú... the distributions derived by this pro-
cedure violate the likelihood principle, and therefore violate requirements
of coherence ....‚Äù Lindley is referring to the foundational inconsistency of
the method which is based on integrating over the sample space of the data.
Further, McCulloch concludes his discussion of Berger and Bernardo (1992)
with the following words: ‚ÄúWe all use ‚Äònoninformative‚Äô priors and this work
is probably the most important current work in the area. I found the pa-
pers very, um ... er ... ah ..., interesting. If the authors obtain impossible
solutions it is because they are working on an impossible problem.‚Äù

This page intentionally left blank

8
Bayesian Assessment of
Hypotheses and Models
8.1
Introduction
The three preceding chapters gave an overview of how Bayesian probability
models are constructed. Once a prior distribution is elicited and the form
of the likelihood function is agreed upon, Bayesian analysis is conceptually
straightforward: nuisance parameters are eliminated via integration and
parameters of interest are inferred from their marginal posterior distribu-
tions. Further, yet-to-be-observed random variables can be predicted from
the corresponding predictive distributions. In all cases, probability is the
sole measure of uncertainty at each and everyone of the stages of Bayesian
learning.
Inferences are expected to be satisfactory if the entire probability model
(the prior, the likelihood, and all assumptions made) is a ‚Äúgood one‚Äù, in
some sense. In practice, however, agreement about the model to be used
is more the exception than the rule, unless there is some well-established
theory or mechanism underlying the problem. For example, a researcher
may be uncertain about which hypothesis or theory holds. Further, al-
most always, there are alternative choices about the distributional form
to be adopted or about the explanatory variables that should enter into
a regression equation, say. Hence, it is important to take into account un-
certainties about the model-building process. This is perfectly feasible in
Bayesian analysis and new concepts do not need to be introduced in this
respect. If there is a set of competing models in a certain class, each of the
models in the set can be viewed as a diÔ¨Äerent state of a random variable.

400
8. Bayesian Assessment of Hypotheses and Models
The prior distribution of this variable (the model) is updated using the
information contained in the data, to arrive at the posterior distribution of
the possible states of the model. Then inferences are drawn, either from the
most probable model, a posteriori, or from the entire posterior distribution
of the models, in a technique called Bayesian model averaging.
In this chapter, several concepts and techniques for the Bayesian eval-
uation of hypotheses and models are presented. Some of the approaches
described are well founded theoretically; others are of a more exploratory
nature. The next section deÔ¨Ånes the posterior probability of a model and
an intimately related concept: the Bayes factor. Subsequently, the issue of
‚Äútesting hypotheses‚Äù is presented from a Bayesian perspective. Approxi-
mations to the Bayes factor and extensions to the concept are suggested.
A third section presents some methods for calculating the Bayes factor,
including Monte Carlo procedures, since it is seldom the case that one can
arrive at the desired quantities by analytical methods. The fourth and Ô¨Åfth
sections present techniques for evaluating goodness of Ô¨Åt and the predictive
ability of a model. The Ô¨Ånal section provides an introduction to Bayesian
model averaging, with emphasis on highlighting its theoretical appeal from
the point of view of predicting future observations.
8.2
Bayes Factors
8.2.1
DeÔ¨Ånition
Suppose there are several competing theories, hypotheses, or models about
some aspect of a biological system. For example, consider diÔ¨Äerent theories
explaining how a population evolves. These theories are mutually exclusive
and exhaustive (at least temporarily). The investigator assigns prior proba-
bility p (Hi) , (i = 1, 2, ..., K) to hypothesis, or theory i, with 
i p (Hi) = 1.
There is no limit to K and nesting requirements are not involved. After
observing data y, the posterior probability of hypothesis i is
p (Hi|y) =
p (Hi) p (y|Hi)
K

i=1
p (Hi) p (y|Hi)
,
i = 1, 2, . . . , K,
(8.1)
where p (y|Hi) is the probability of the data under hypothesis i. If all
hypotheses are equally likely a priori, which is the maximum entropy or
reference prior in the discrete case (Bernardo, 1979), then
p (Hi|y) =
p (y|Hi)
K

i=1
p (y|Hi)
.

8.2 Bayes Factors
401
The posterior odds ratio of hypothesis i relative to hypothesis j takes the
form
p (Hi|y)
p (Hj|y) = p (Hi)
p (Hj)
p (y|Hi)
p (y|Hj).
(8.2)
It follows that the posterior odds ratio is the product of the prior odds
ratio and of the ratio between the marginal probabilities of observing the
data under each of the hypotheses. The Bayes factor is deÔ¨Åned to be
Bij = p (y|Hi)
p (y|Hj) =
p(Hi|y)
p(Hj|y)
p(Hi)
p(Hj)
= posterior odds ratio
prior odds ratio
.
(8.3)
According to Kass and Raftery (1995) this terminology is apparently due
to Good (1958). A Bij > 1 means that Hi is more plausible than Hj in the
light of y. While the priors are not visible in the ratio p (y|Hi) /p (y|Hj),
because algebraically they cancel out, this does not mean that Bij in general
is not aÔ¨Äected by prior speciÔ¨Åcations. This point is discussed below.
It is instructive to contrast this approach with the one employed in stan-
dard statistical analysis. In classical hypothesis testing, a null hypothesis
H0 : Œ∏ ‚ààŒ∏0 and an alternative hypothesis H1 : Œ∏ ‚ààŒ∏1 are speciÔ¨Åed. The
choice between these hypotheses is driven by the distribution under H0 of
a test statistic that is a function of the data (it could be the likelihood
ratio), T (y) , and by the so-called p-value. This is deÔ¨Åned as
Pr [T (y) at least as extreme as the value observed|Œ∏, H0] .
(8.4)
Then H0 is accepted (or rejected, in which case H1 is accepted) if the
p-value is large (small) enough, or one may just quote the p-value and
leave things there. Notice that (8.4) represents the probability of obtaining
results larger than the one actually obtained; that is, (8.4) is concerned
with events that might have occurred, but have not. Thus, the famous
quotation from JeÔ¨Äreys (1961):
‚ÄúWhat the use of p implies, therefore, is that a hypothesis
which may be true may be rejected because it has not predicted
observable results which have not occurred. ... On the face of
it the fact that such results have not occurred might more rea-
sonably be taken as evidence for the law, not against it.‚Äù
Often (and incorrectly), the p-value is interpreted as the probability that
H0 holds true. The interpretation in terms of probability of hypotheses,
p [H0|T (y) = t (y)], which is the Bayesian formulation of the problem, is
conceptually more straightforward than the one associated with (8.4). De-
spite its conceptual clarity, the Bayesian approach is not free from prob-
lems. Perhaps not surprisingly, these arise especially in cases when prior
information is supposed to convey vague knowledge.

402
8. Bayesian Assessment of Hypotheses and Models
8.2.2
Interpretation
The appeal of the Bayes factor as formulated in (8.3), is that it provides
a measure of whether the data have increased or decreased the odds of
Hi relative to Hj. This, however, does not mean that in general, the Bayes
factor is driven by the data only. It is only when both Hi and Hj are simple
hypotheses, that the prior inÔ¨Çuence vanishes and the Bayes factor takes the
form of a likelihood ratio. In general, however, the Bayes factor depends
on prior input, a point to which we will return.
Kass and Raftery (1995) give guidelines for interpreting the evidence
against some ‚Äúnull hypothesis‚Äù, H0. For example, they suggest that a Bayes
factor larger than 100 should be construed as ‚Äúdecisive evidence‚Äù against
the null. Note that a Bayes factor under 1 means that there is evidence in
support of H0. When working in a logarithmic scale, 2 log Bij, for example,
the values are often easier to interpret by those who are familiar with
likelihood ratio tests. It should be made clear from the onset that the
Bayes factor cannot be viewed as a statistic having an asymptotic chi-
square distribution under the null hypothesis. Again, Bij is the quantity
by which prior odds ratios are increased (or decreased) to become posterior
odd ratios.
There are many diÔ¨Äerences between the Bayes factor and the usual like-
lihood ratio statistic. First, the intervening p (y|Hi) is not the classical
likelihood, in general. Recall that the Bayesian marginal probability (or
density) of the data is arrived at by integrating the joint density of the
parameters and of the observations over all values that the parameters can
take in their allowable space. For example, if hypothesis or model Hi has
parameters Œ∏i, then for continuous data and continuous valued parameter
vector
p (y|Hi)
=

p (y|Œ∏i, Hi) p (Œ∏i|Hi) dŒ∏i
=
EŒ∏i|Hi [p (y|Œ∏i, Hi)] .
(8.5)
The marginal density is, therefore, the expected value of all possible likeli-
hoods, where the expectation is taken with respect to the prior distribution
of the parameters. In likelihood inference, no such integration takes place
unless the ‚Äúparameters‚Äù are random variables having a frequentist interpre-
tation. Since, in turn, these random variables have distributions indexed by
parameters, the classical likelihood always depends on some Ô¨Åxed, unknown
parameters. In the Bayesian approach, on the other hand, any dependence
of the marginal distribution of the data is with respect to any hyperparam-
eters the prior distribution may have, and with respect to the form of the
model. In fact, p (y|Hi) is the prior predictive distribution and gives the
density or probability of the data calculated before observation, uncondi-
tionally with respect to parameter values.

8.2 Bayes Factors
403
A second important diÔ¨Äerence is that the Bayes factor is not explicitly
related to any critical value deÔ¨Åning a rejection region of a certain size.
For example, the usual p-values in classical hypothesis testing cannot be
interpreted as the probabilities that either the null or the alternative hy-
potheses are ‚Äútrue‚Äù. The p-value arises from the distribution of the test
statistic (under the null hypothesis) in conceptual replications of the ex-
periment. In contrast, the Bayes factor and the prior odds contribute di-
rectly to forming the posterior probabilities of the hypotheses. In order to
illustrate, suppose that two models are equally probable, a priori. Then a
Bayes factor B01 = 19, would indicate that the null hypothesis or model is
19 times more probable than its alternative, and that the posterior proba-
bility that the null model is true is 0.95. On the other hand, in a likelihood
ratio test, a value of the test statistic generating a p-value of 0.95 as de-
Ô¨Åned by (8.4) cannot be construed as evidence that the null hypothesis has
a 95% chance of being true.
8.2.3
The Bayes Factor and Hypothesis Testing
Decision-Theoretic View
In Bayesian analysis, ‚Äúhypothesis testing‚Äù is viewed primarily as a decision
problem (e.g., Zellner, 1971). Suppose there are two hypotheses or models:
H0 (null) and H1 (alternative). If one chooses H0 when H1 is ‚Äútrue‚Äù, then
a loss L10 is incurred. Similarly, when H1 is adopted when the null holds,
the loss is L01. Otherwise, there are no losses.
The posterior expectation of the decision ‚Äúaccept the null hypothesis‚Äù is
E (loss|accept H0, y)
=
0 √ó p (H0|y) + L10 p (H1|y)
=
L10 p (H1|y) .
Likewise, the expected posterior loss of the decision ‚Äúaccept the alternative‚Äù
is
E (loss|reject H0, y)
=
0 √ó p (H1|y) + L01 p (H0|y)
=
L01 p (H0|y) .
Naturally, if the expected posterior loss of accepting H0 is larger than that
of rejecting it, one would decide to reject the null hypothesis. Then the
decision rule is
if E (loss|reject H0, y) < E (loss|accept H0, y ) ‚Üíreject H0.
The preceding is equivalent to
L01 p (H0|y) < L10 p (H1|y) ,

404
8. Bayesian Assessment of Hypotheses and Models
or, in terms of (8.3),
B10 = p (y|H1)
p (y|H0) > L01 p (H0)
L10 p (H1).
(8.6)
This indicates that the null hypothesis is to be rejected if the Bayes factor
(ratio of marginal likelihoods under the two hypotheses or models) for the
alternative, relative to the null, exceeds the ratio of the prior expected
losses. Note that L01 p (H0) is the expected prior loss of rejecting the null
when this is true; L10 p (H1) is the expected prior loss that results when H1
is true and one accepts H0. Then the ratio of prior to posterior expected
losses
L01 p (H0)
L10 p (H1)
plays the role of the ‚Äúcritical‚Äù value in classical hypothesis testing. If one
views the Bayes factor as the ‚Äútest statistic‚Äù, the critical value is higher
when one expects to loose more from rejecting the null than from accepting
it. In other words, the larger the prior expected loss from rejecting the null
(when this hypothesis is true) relative to the prior expected loss of accepting
it (when H1 holds), the larger the weight of the evidence should be in favor
of the alternative, as measured by the Bayes factor.
If the losses are such that L01 = L10, it follows from (8.6) that the
decision rule is simply
B10 = p (y|H1)
p (y|H0) > p (H0)
p (H1).
This implies that if the two models or hypotheses are equiprobable a priori,
then the alternative should be chosen over the null whenever the Bayes
factor exceeds 1. Similarly, a ‚Äúcritical value‚Äù of 10 should be adopted if it
is believed a priori that the null hypothesis is 10 times more likely than
the alternative. In all cases, it must be noted that the ‚Äúaccept‚Äù or ‚Äúreject‚Äù
framework depends nontrivially on the form of the loss function, and that
adopting L01 = L10 may not be realistic in many cases.
The deÔ¨Ånition in the form of (8.6) highlights the importance, in Bayesian
testing, of deÔ¨Åning non-zero a priori probabilities. This is so even though
the Bayes factor can be calculated without specifying p (H0) and p (H1).
If H0 or H1 are a priori impossible, the observations will not modify this
information.
Bayesian Comparisons
Contrasting Two Simple Hypotheses
The deÔ¨Ånition of the Bayes factor in (8.3) as a ratio of marginal densities
does not make explicit the inÔ¨Çuence of the prior distributions. With one

8.2 Bayes Factors
405
exception, Bayes factors are aÔ¨Äected by prior speciÔ¨Åcations. The excep-
tion occurs when the comparison involves two simple hypotheses. In this
case, under H0, a particular value Œ∏0 is assigned to the parameter vector,
whereas under H1, another value Œ∏ = Œ∏1 is posited. There is no uncer-
tainty about the value of the parameter under any of the two competing
hypotheses. Then one can express the discrete prior probability of hypoth-
esis i as p (Hi) = Pr (Œ∏ = Œ∏i), and the conditional p.d.f. for y given Hi as
p (y|Hi) = p (y|Œ∏ = Œ∏i). The Bayes factor for the alternative against the
null is then
B10 = posterior odds
prior odds
= p (y|Œ∏ = Œ∏1)
p (y|Œ∏ = Œ∏0).
(8.7)
In this particular situation, the Bayes factor is the odds for H1 relative
to H0 given by the data only. Expression (8.7) is a ratio of standard like-
lihoods, where the values of the parameters are completely speciÔ¨Åed. In
general, however, B10 depends on prior input. When a hypothesis is not
simple, in order to arrive at the form equivalent to (8.7), one must compute
the expectation of the likelihood of Œ∏i with respect to the prior distribu-
tion. For continuously distributed values of the vector Œ∏i and prior density
p (Œ∏i|Hi), one writes
p (y|Hi) =

p (y|Œ∏i, Hi) p (Œ∏i|Hi) dŒ∏i.
In contrast to the classical likelihood ratio frequentist test, the Bayes
factor does not impose nesting restrictions concerning the form of the
likelihood functions, as illustrated in the following example adapted from
Bernardo and Smith (1994).
Example 8.1
Two fully speciÔ¨Åed models: Poisson versus negative bino-
mial process
Two completely speciÔ¨Åed models are proposed for counts. A sample of size
n with values y1, y2, . . . , yn is drawn independently from some population.
Model P states that the distribution of the observations is Poisson with
parameter Œ∏P . Then the likelihood under this model is
p (y|Œ∏ = Œ∏P ) =
n
-
i=1
Œ∏yi
P exp (‚àíŒ∏P )
yi!

=
Œ∏ny
P
exp (nŒ∏P )
n7
i=1
yi!
.
(8.8)
Model N proposes a negative binomial distribution with parameter Œ∏P .
The corresponding likelihood is
p (y|Œ∏ = Œ∏N) =
n
-
i=1
[Œ∏N (1 ‚àíŒ∏N)yi] = Œ∏n
N (1 ‚àíŒ∏N)ny .
(8.9)

406
8. Bayesian Assessment of Hypotheses and Models
The Bayes factor for Model N relative to Model P is then
BNP =

1 ‚àíŒ∏N
Œ∏P
ny
Œ∏n
N

exp (nŒ∏P )
n7
i=1
yi!
‚àí1 ,
and its logarithm can be expressed as
log BNP = n

y log

1 ‚àíŒ∏N
Œ∏P

+ log (Œ∏N)

+ nŒ∏P +
n

i=1
log (yi!) .
‚ñ†
Simple Versus Composite Hypotheses
A second type of comparison is one where one of the models (Model 0 =
M0) postulates a given value of the parameter, whereas the other model
(Model 1 = M1) allows the unknown parameter to take freely any of its
values in the allowable parameter space. This is called a simple versus
composite test (Bernardo and Smith, 1994), and the Bayes factor in this
case takes the form
B10 = p (y|M1)
p (y|M0) =

p (y|Œ∏,M1) p (Œ∏|M1) dŒ∏
p (y|Œ∏ = Œ∏0, M0)
,
where p (Œ∏|M1) is the density of the prior distribution of the parameter
vector under the assumptions of Model 1.
There is an interesting relationship between the posterior probability
that Œ∏ = Œ∏0 and the Bayes factor (Berger, 1985). Denote the prior proba-
bility of models 0 and 1 as p (M0) and p (M1), respectively, with p (M0) +
p (M1) = 1. The term p (M0) can also be interpreted as the prior probability
that Œ∏ = Œ∏0. Then, the posterior probability that Œ∏ = Œ∏0 is
Pr (Œ∏ = Œ∏0|y) = p (M0) p (y|Œ∏ = Œ∏0, M0)
p (y)
.
The constant term in the denominator is given by
p (y) = p (y|Œ∏ = Œ∏0, M0) Pr (Œ∏ = Œ∏0|M0) p (M0)
+

p (y|Œ∏,M1) p (Œ∏|M1) p (M1) dŒ∏
= p (M0) p (y|Œ∏ = Œ∏0, M0) + p (M1)

p (y|Œ∏,M1) p (Œ∏|M1) dŒ∏,

8.2 Bayes Factors
407
Genotypes
AB/ab
Ab/ab
aB/ab
ab/ab
Phenotype
AB
Ab
aB
ab
Frequency
1
2 (1 ‚àír)
1
2r
1
2r
1
2 (1 ‚àír)
Observed
a
b
c
d
TABLE 8.1. Genotypic distribution in oÔ¨Äspring from a backcross design.
with the equality arising in the last line because Pr (Œ∏ = Œ∏0|M0) = 1. Sub-
stituting above yields
Pr (Œ∏ = Œ∏0|y) =

1 + p (M1)
p (M0)B10
‚àí1
.
It is important to mention that in evaluating a point null hypothesis
Œ∏ = Œ∏0, say, Œ∏0 must be assigned a positive probability a priori. The point
null hypothesis cannot be tested invoking a continuous prior distribution,
since any such prior will give Œ∏0 prior (and therefore posterior) probability
of zero.
In contrast to the traditional likelihood ratio, the test of a parameter
value on the boundary of the parameter space using the Bayes factor does
not in principle create diÔ¨Éculties. This being so because asymptotic distri-
butions and series expansions do not come into play. Such a test is illus-
trated in the example below.
Example 8.2
A point null hypothesis: assessing linkage between two loci
The problem consists of inferring the probability of recombination r be-
tween two autosomal loci A and B, each with two alleles. The parameter r
is deÔ¨Åned in the closed interval

0, 1
2

, with the upper value corresponding
to the situation where there is no linkage. We wish to derive the poste-
rior distribution of the recombination fraction between the two loci, and to
contrast the two models that follow. The null model (M0) postulates that
segregation is independent (that is, r = 1
2), whereas the alternative model
(M1) claims that the loci are linked (that is, r < 1
2).
Let the alleles at the corresponding loci be A, a, B, and b, where A
and B are dominant alleles. Suppose that a line consisting of coupling
heterozygote individuals AB/ab is crossed to homozygotes ab/ab. Hence
four oÔ¨Äspring classes can be observed: AB/ab, Ab/ab, aB/ab, and ab/ab.
Let n = a + b + c + d be the total number of oÔ¨Äspring observed. The four
possible genotypes resulting from this cross, their phenotypes, the expected
frequencies and the observed numbers are shown in Table 8.1. The expected
relative frequencies follow from the fact that if the probability of observing a
recombinant is r, the individual can be either Ab/ab or aB/ab, with the two
classes being equally likely. A similar reasoning applies to the observation
of a non-recombinant type.

408
8. Bayesian Assessment of Hypotheses and Models
Suppose that the species under consideration has 22 pairs of autosomal
chromosomes. In the absence of prior information about loci A and B, it
may be reasonable to assume that the probability that these are located
on the same chromosome (and therefore, linked, so that r < 1
2) is
1
22. This
is so because the probability that 2 randomly picked alleles are in a given
chromosome is
 1
22
2, and there are 22 chromosomes in which this can
occur. Hence, a priori, p (M1) =
1
22 and p (M0) = 21
22; the two models are
viewed as mutually exclusive and exhaustive.
Next, we must arrive at some reasonable prior distribution for r under
M1. Here, a development by Smith (1959) is followed. First, note that the
recombination fraction takes the value r =
1
2 with prior probability 21
22.
Further, assume a uniform distribution for r otherwise (provided one can
view the values 0 < r < 1
2 as ‚Äúequally likely‚Äù). Then the density of this
uniform distribution, p (r|M1) must be such that
Pr

r < 1
2

=
1
2

0
p (u|M1) du = 1
22.
Solving for the desired uniform density gives
p (r|M1) = 1
11.
Therefore the prior is the uniform process p (r|M1) =
1
11 for 0 < r < 1
2,
and the point mass 21
22 at r = 1
2. That is, Pr

r = 1
2

= p (M0) = 21
22. Note
that given M0, Pr

r = 1
2|M0

= 1.
Given the data y = (a, b, c, d)‚Ä≤ in Table 8.1, the conditional distribution
of the observations under linkage has the multinomial form
p (y|r)
=
n!
a!b!c!d!
1
2 (1 ‚àír)
a 
1
2r
b 
1
2r
c 1
2 (1 ‚àír)
d
‚àù

1
2
n
(1 ‚àír)a+d rb+c.
Under no linkage
p

y|r = 1
2

=
n!
a!b!c!d!

1
4
a+b+c+d
‚àù

1
4
n
.
Therefore the posterior odds ratio is given by
p (M1|y)
p (M0|y) = p (M1)
p (M0)
 1
2
0 p (r|M1) p (y|r, M1) dr
p

y|r = 1
2, M0

= 1
21
1
11
 1
2
n  1
2
0 rb+c (1 ‚àír)a+d dr
 1
4
n
,

8.2 Bayes Factors
409
where
B10 =
 1
2
0 p (r|M1) p (y|r, M1) dr
p

y|r = 1
2, M0

.
The posterior probability of linkage is
Pr

r < 1
2|y,M1

=
1
2

0
p (r|y, M1) dr,
where
p (r|y, M1) = p (r|M1) p (y|r, M1)
p (y)
,
(8.10)
whereas the posterior probability of no linkage is
Pr

r = 1
2|y, M0

= 1 ‚àíPr

r < 1
2|y, M1

.
The denominator in (8.10) is equal to
p (y) = p (M0) p

y|r = 1
2, M0

+p (M1)

1
2
0
p (r|M1) p (y|r, M1) dr.
The integrals in these expressions can easily be evaluated numerically.
‚ñ†
Example 8.3
Lindley‚Äôs paradox
This problem was brought to light initially by Lindley (1957). The data
sampling involves n independent draws from N

¬µ, œÉ2
, with œÉ2 known and
¬µ to be inferred. Model 0 corresponds to the simple or sharp hypothesis
that ¬µ = ¬µ0. Model 1 takes œÉ2 as known and ¬µ as unknown, with its
prior distribution being N

¬µ1, œÉ2
1

; the hyperparameters are assumed to
be known. This corresponds to a classical setting in which Model 0 is the
null hypothesis ¬µ = ¬µ0, and Model 1 is the alternative that the parameter
can take any value other than ¬µ = ¬µ0.
The marginal density of the data under Model 0 is
p0

y|¬µ0, œÉ2
=

1
‚àö
2œÄœÉ2
n
exp

‚àí1
2œÉ2
n

i=1
(yi ‚àí¬µ0)2

=

1
‚àö
2œÄœÉ2
n
exp

‚àí1
2œÉ2
n

i=1
(yi ‚àíy)2

exp

‚àín
2œÉ2 (y ‚àí¬µ0)2
.
(8.11)

410
8. Bayesian Assessment of Hypotheses and Models
The marginal density under Model 1 can be written as
p1

y|¬µ1, œÉ2
1, œÉ2
=

p

y|¬µ, œÉ2
p

¬µ|¬µ1, œÉ2
1

d¬µ
=

1
‚àö
2œÄœÉ2
n
exp

‚àí1
2œÉ2
n

i=1
(yi ‚àíy)2

1
>
2œÄœÉ2
1

exp

‚àín
2œÉ2 (y ‚àí¬µ)2
exp

‚àí(¬µ ‚àí¬µ1)2
2œÉ2
1

d¬µ.
(8.12)
The Bayes factor for Model 0 relative to Model 1 is given by the ratio
between (8.11) and (8.12)
B01 =
exp

‚àín
2œÉ2 (y ‚àí¬µ0)2
1
‚àö
2œÄœÉ2
1

exp
9
‚àí1
2

n
œÉ2 (¬µ ‚àíy)2 + (¬µ‚àí¬µ1)2
œÉ2
1
:
d¬µ
.
(8.13)
Now the two quadratic forms on ¬µ in the integrand can be combined in the
usual manner, leading to
n
œÉ2 (¬µ ‚àíy)2 + (¬µ ‚àí¬µ1)2
œÉ2
1
=

 n
œÉ2 + 1
œÉ2
1

(¬µ ‚àí5¬µ)2
+ n
œÉ2

 n
œÉ2 + 1
œÉ2
1
‚àí1 1
œÉ2
1
(y ‚àí¬µ1)2 .
Above
5¬µ =

 n
œÉ2 + 1
œÉ2
1
‚àí1 
ny
œÉ2 + ¬µ1
œÉ2
1

.
Carrying out the integration, the Bayes factor becomes, after some algebra,
B01 =
@
A
A
B
œÉ2
1

n
œÉ2 +
1
œÉ2
1
‚àí1
exp

‚àín
2œÉ2 (y ‚àí¬µ0)2
exp

‚àín
2œÉ2

n
œÉ2 +
1
œÉ2
1
‚àí1
1
œÉ2
1 (y ‚àí¬µ1)2
.
(8.14)
Now examine what happens when the prior information becomes more and
more diÔ¨Äuse, that is, eventually œÉ2
1 is so large that 1/œÉ2
1 is near 0. The
Bayes factor is, approximately,
B01 ‚âà
?
œÉ2
1œÉ2
n
exp

‚àín
2œÉ2 (y ‚àí¬µ0)2
.
For any Ô¨Åxed value of y, the Bayes factor goes to ‚àûwhen œÉ2
1 ‚Üí‚àû, which
implies that p (Model 0|y) ‚Üí1. This means that no matter what the value
of y is, the null hypothesis would tend to be favored, even for values of

8.2 Bayes Factors
411
"""(y ‚àí¬µ0) /
>
œÉ2/n
""" that are large enough to cause rejection of the null at
any, arbitrary, ‚ÄúsigniÔ¨Åcance‚Äù level in classical testing. This result, known
as ‚ÄúLindley‚Äôs paradox‚Äù, illustrates that a comparison of models in which
one of the hypothesis is ‚Äúsharp‚Äù (simple), strongly depends on the form
of the prior distribution. In particular, when the distribution is improper,
the Bayes factor leads to acceptance of the null. O‚ÄôHagan (1994) concludes
that improper priors cannot be used when comparing models. However,
the problem is not avoided entirely by adopting vague uniform priors over
some large but Ô¨Ånite range. This will be discussed later.
‚ñ†
Comparing Two Composite Hypotheses
Third, the comparison may be a ‚Äúcomposite versus composite‚Äù, that is, one
where the two models allow their respective parameters to take any values
in the corresponding spaces. Here the Bayes factor is
B10 = p (y|M1)
p (y|M0) =

p (y|Œ∏1,M1) p1 (Œ∏1|M1) dŒ∏

p (y|Œ∏0, M0) p0 (Œ∏0|M0) dŒ∏ .
(8.15)
In general, as is the case with likelihood ratios, all constants appearing
in p (y|Œ∏i,Mi) must be included when computing B10.
Example 8.4
Marginal distributions and the Bayes factor in Poisson
and negative binomial models
The setting is as in Example 8.1, but the parameter values are allowed to
take any values in their spaces. Following Bernardo and Smith (1994), take
as prior distribution for the Poisson parameter
Œ∏P ‚àºGa (aP , bP ) ,
and for the parameter of the negative binomial model adopt as prior the
Beta distribution
Œ∏N ‚àºBe (aN, bN) .
The a‚Ä≤s and b‚Ä≤s are known hyperparameters. The marginal distribution of
the data under the Poisson model, using (8.8) as likelihood function (sup-
pressing the dependence on hyperparameters in the notation), is obtained
as
p (y|P) =
 Œ∏ny
P exp (‚àínŒ∏P )
n7
i=1
yi!
baP
P
Œì (aP )Œ∏aP ‚àí1
P
exp (‚àíbP Œ∏P ) dŒ∏P
=
baP
P
Œì (aP )
n7
i=1
yi!

Œ∏ny+aP ‚àí1
P
exp [‚àí(n + bP ) Œ∏P ] dŒ∏P .
(8.16)
The integrand is the kernel of the density of the
Œ∏P ‚àºGa (ny + aP , n + bP )

412
8. Bayesian Assessment of Hypotheses and Models
distribution. Hence, the marginal of interest is
p (y|P) =
Œì (aP + ny) baP
P
Œì (aP ) (n + bP )aP +ny
n7
i=1
yi!
.
(8.17)
Similarly, using (8.9), the marginal distribution of the data under the neg-
ative binomial model takes the form
p (y|N) =

Œ∏n
N (1 ‚àíŒ∏N)ny Œì (aN + bN)
Œì (aN) Œì (bN)Œ∏aN‚àí1
N
(1 ‚àíŒ∏N)bN‚àí1 dŒ∏N
= Œì (aN + bN)
Œì (aN) Œì (bN)

Œ∏aN+n‚àí1
N
(1 ‚àíŒ∏N)bN+ny‚àí1 dŒ∏N.
The integrand is the kernel of a beta density, so the integral can be evalu-
ated analytically, yielding
p (y|N) = Œì (aN + bN)
Œì (aN) Œì (bN)
Œì (aN + n) Œì (bN + ny)
Œì (aN + n + bN + ny) .
(8.18)
The Bayes factor in favor of the N model relative to the P model is given by
the ratio between (8.18) and (8.17). Note that the two marginal densities
and the Bayes factor depend only on the data and on the hyperparameters,
contrary to the ratio of likelihoods. This is because all unknown parame-
ters are integrated out in the process of Ô¨Ånding the marginals. It cannot be
overemphasized that all integration constants must be kept when calculat-
ing the Bayes factors. In classical likelihood ratio tests, on the other hand,
only those parts of the density functions that depend on the parameters
are kept.
‚ñ†
8.2.4
InÔ¨Çuence of the Prior Distribution
From its deÔ¨Ånition, and from Example 8.4, it should be apparent that the
Bayes factor depends on the prior distributions adopted for the competing
models. The exception is when two simple hypotheses are at play. For the
Poisson versus Negative Binomial setting discussed above, Bernardo and
Smith (1994) give numerical examples illustrating that minor changes in
the values of the hyperparameters produce changes in the direction of the
Bayes factors. This dependence is illustrated with a few examples in what
follows. Before we do so, note that one can write

p (y|Mi) dy
=
 
p (y|Œ∏i, Mi) p (Œ∏i|Mi) dŒ∏idy
=

p (Œ∏i|Mi)

p (y|Œ∏i, Mi) dy

dŒ∏i
=

p (Œ∏i|Mi) dŒ∏i,

8.2 Bayes Factors
413
where the last equality follows because

p (y|Œ∏i, Mi) dy =1. The message
here is that when p (Œ∏i|Mi) is improper, so is p (y|Mi). In this case, the
Bayes factor is not well deÔ¨Åned. This is discussed further in Subsection
8.2.5 below.
Example 8.5
InÔ¨Çuence of the bounds of a uniform prior
Let the sampling model be yi|¬µ ‚àºN (¬µ, 1) and let the prior distribution
adopted for ¬µ under Model 1 be uniform over [‚àíL, L] . Model 2 postulates
the same sampling model but the bounds are [‚àíŒ±L, Œ±L] , where Œ± is a
known, positive, real number. Suppose n independent samples are drawn,
so that the marginal density of the data under Model 2 is
p (y|Œ±, L) =
Œ±L

‚àíŒ±L

1
‚àö
2œÄ
n
exp

‚àí1
2
n

i=1
(yi ‚àí¬µ)2

1
2Œ±Ld¬µ
=
1
2Œ±L

1
‚àö
2œÄ
n
exp

‚àí1
2
n

i=1
(yi ‚àíy)2

Œ±L

‚àíŒ±L
exp

‚àín
2 (¬µ ‚àíy)2
d¬µ.
The integrand is in a normal form and can be evaluated readily, yielding
p (y|Œ±, L) =
1
2Œ±L

1
‚àö
2œÄ
n
exp

‚àí1
2
n

i=1
(yi ‚àíy)2

√ó
Ô£Æ
Ô£∞Œ¶
Ô£´
Ô£≠Œ±L ‚àíy
8
1
n
Ô£∂
Ô£∏‚àíŒ¶
Ô£´
Ô£≠‚àíŒ±L ‚àíy
8
1
n
Ô£∂
Ô£∏
Ô£π
Ô£ª
?
2œÄ 1
n.
The Bayes factor for Model 2 relative to Model 1 is
B21 = p (y|Œ±, L)
p (y|1, L) =
Œ¶

Œ±L‚àíy
‚àö
1
n

‚àíŒ¶

‚àíŒ±L‚àíy
‚àö
1
n

Œ±

Œ¶

L‚àíy
‚àö
1
n

‚àíŒ¶

‚àíL‚àíy
‚àö
1
n
.
This clearly shows that the Bayes factor is sensitive with respect to the
value of Œ±. This is relevant in conjunction with the problem outlined in
Example 8.3: the diÔ¨Éculties caused by improper priors in model selection
via the Bayes factors are not solved satisfactorily by adopting, for example,
bounded uniform priors. The Bayes factor depends very strongly on the
width of the interval used.
‚ñ†
Example 8.6
The Bayes factor for a simple linear model
Consider the linear model
y = Œ≤ + e,

414
8. Bayesian Assessment of Hypotheses and Models
where the variance of the residual distribution, œÉ2, is known, so it can
be set equal to 1 without loss of generality. Model 1 posits as prior dis-
tribution Œ≤ ‚àºN

0, IœÉ2
1

, whereas for Model 2 the prior distribution is
Œ≤ ‚àºN

0, IœÉ2
2

. Since the sampling model and the priors are both normal, it
follows that the marginal distributions of the data are normal as well. The
means and variances of these distributions are arrived at directly by taking
expectations of the sampling model with respect to the appropriate prior.
One gets y|œÉ2, œÉ2
1 ‚àºN

0, I

œÉ2
1 + œÉ2
and y|œÉ2, œÉ2
2 ‚àºN

0, I

œÉ2
2 + œÉ2
for Models 1 and 2, respectively. The Bayes factor for Model 1 relative to
Model 2 is
B12
=
n7
i=1
1
	
2œÄ(œÉ2
1+1) exp

‚àí
y2
i
2(œÉ2
1+1)

n7
i=1
1
	
2œÄ(œÉ2
2+1) exp

‚àí
y2
i
2(œÉ2
2+1)

=

œÉ2
2 + 1
œÉ2
1 + 1
 n
2 exp

y‚Ä≤y
2(œÉ2
2+1)

exp

y‚Ä≤y
2(œÉ2
1+1)
.
Taking logarithms and multiplying by 2, to arrive at the same scale as the
likelihood ratio statistic, yields
2 log (B12) = n log

œÉ2
2 + 1
œÉ2
1 + 1

+ y‚Ä≤y

œÉ2
1 ‚àíœÉ2
2
(œÉ2
1 + 1) (œÉ2
2 + 1)

.
The Ô¨Årst term will contribute toward favoring Model 1 whenever œÉ2
2 is larger
than œÉ2
1, whereas the opposite occurs in the second term.
‚ñ†
8.2.5
Nested Models
As seen in Chapter 3, a nested model is one that can be viewed as a special
case of a more general, larger model, and is typically obtained by Ô¨Åxing or
‚Äúzeroing in‚Äù some parameters in the latter. Following O‚ÄôHagan (1994), let
the bigger model have parameters (Œ∏, œÜ) and denote it Model 1 whereas in
the nested model Ô¨Åx œÜ = œÜ0, with this value being usually 0. This is Model
0.
Let the prior probability of the larger model (often called the ‚Äúalterna-
tive‚Äù one) be œÄ1, and let the prior density of its parameters be p1 (Œ∏, œÜ) .
The prior probability of the nested model is œÄ0 = 1 ‚àíœÄ1, which can be
interpreted as the prior probability that œÜ = œÜ0. This is somewhat per-
plexing at Ô¨Årst sight, since the probability that a continuous parameter
takes a given value is 0. However, the fact that consideration is given to
the nested model as a plausible model implies that one is assigning some
probability to the special situation that œÜ = œÜ0 holds. In the nested model,

8.2 Bayes Factors
415
the prior density of the ‚Äúfree parameters‚Äù is p0 (Œ∏) = p (Œ∏|œÜ = œÜ0) , that is,
the density of the conditional distribution of the theta parameter, given
that œÜ = œÜ0. Now, for the larger model, write
p1 (Œ∏, œÜ) = p1 (Œ∏|œÜ) p1 (œÜ) ,
where p1 (Œ∏|œÜ) is the density of the conditional distribution of Œ∏, given œÜ. In
practice, it is reasonable to assume that the conditional density of Œ∏, given
œÜ, is continuous at œÜ = œÜ0 (O‚ÄôHagan, 1994).
In order to obtain the marginal density of the data under Model 0 one
must integrate the joint density of the observations, and of the free param-
eters (given œÜ = œÜ0) with respect to the latter, to obtain
p (y|Model 0)
=

p (y|Œ∏, œÜ = œÜ0) p (Œ∏|œÜ = œÜ0) dŒ∏
=
p (y|œÜ = œÜ0) .
(8.19)
For the larger model
p (y|Model 1)
=
 
p (y|Œ∏, œÜ) p (Œ∏|œÜ) dŒ∏

p1 (œÜ) dœÜ
=

p (y|œÜ) p1 (œÜ) dœÜ = EœÜ [p (y|œÜ)] .
(8.20)
The expectation above is an average of the sampling model marginal densi-
ties (after integrating out Œ∏) taken over all values of œÜ (other than œÜ0) and
with plausibility as conveyed by the prior density p1 (œÜ) under the larger
model. The posterior probability of the null model is then
p (Model 0|y) =
p (y|Model 0) œÄ0
p (y|Model 0) œÄ0 + p (y|Model 1) (1 ‚àíœÄ0)
=
p (y|œÜ = œÜ0) œÄ0
p (y|œÜ = œÜ0) œÄ0 +

p (y|œÜ) p1 (œÜ) dœÜ (1 ‚àíœÄ0),
(8.21)
and p (Model 1|y) = 1 ‚àíp (Model 0|y) .
Consider now the case where there is a single parameter, so that Model
0 poses œÜ = œÜ0 and Model 1 corresponds to the ‚Äúalternative‚Äù hypothesis
œÜ Ã∏= œÜ0 (the problem then consists of one of evaluating the ‚Äúsharp‚Äù null
hypothesis œÜ = œÜ0). Then (8.21) holds as well, with the only diÔ¨Äerence
being that the marginal distributions of the data are calculated directly as
p (y|Model 0) = p (y|œÜ = œÜ0) ,
and
p (y|Model 1) =

p (y|œÜ) p1 (œÜ) dœÜ.
(8.22)

416
8. Bayesian Assessment of Hypotheses and Models
It is instructive to study the consequences of using a vague prior distri-
bution on the Bayes factor. Following O‚ÄôHagan (1994), suppose that œÜ is a
scalar parameter on (‚àí‚àû, ‚àû). Vague prior knowledge is expressed as the
limit of a uniform distribution
p1 (œÜ) = (2c)‚àí1 , for ‚àíc ‚â§œÜ ‚â§c,
by letting c ‚Üí‚àû, in which case, p1 (œÜ) ‚Üí0. Then (8.22) is

p (y|œÜ) p1 (œÜ) dœÜ = (2c)‚àí1
 c
‚àíc
p (y|œÜ) dœÜ.
Often, p (y|œÜ) will tend to zero as œÜ tends to inÔ¨Ånity, such that the limit
of the integral above is Ô¨Ånite. Then as c ‚Üí‚àû, (8.22) tends to zero and
the Bayes factor B01 tends to inÔ¨Ånity. Thus, using a prior with very large
spread on œÜ in an attempt to describe vague prior knowledge, forces the
Bayes factor to favor Model 0.
Example 8.7
Normal model: known versus unknown variance
The setting will be the usual N

¬µ, œÉ2
for each of n independent observa-
tions. In the larger model, both the mean and variance are taken as un-
known. In the nested model, the variance is assumed to be known, such that
œÉ2 = œÉ2
0. As in O‚ÄôHagan (1994), it will be assumed that the conditional prior
distribution of the mean is the normal process ¬µ|¬µ1, wœÉ2 ‚àºN

¬µ1, wœÉ2
,
where w is a known scalar. This implies that the variance of the prior dis-
tribution is proportional to that of the sampling model. Further, it will be
assumed that the prior distribution of œÉ2 is a scaled inverted chi-square
distribution with parameters ŒΩ and S2.
Under the null or nested model (known variance), the prior distribution
is then ¬µ|¬µ1, wœÉ2
0 ‚àºN

¬µ1, wœÉ2
0

, and the marginal distribution of the data,
following (8.19) and making use of (8.12), is
p (y|Model 0) =

p(y|¬µ, œÉ2
0)p

¬µ|¬µ1, wœÉ2
0

d¬µ
=

1
>
2œÄœÉ2
0
n
exp

‚àí1
2œÉ2
0
n

i=1
(yi ‚àíy)2

√ó
1
>
2œÄwœÉ2
0

exp

‚àín
2œÉ2
0
(y ‚àí¬µ)2

exp

‚àí(¬µ ‚àí¬µ1)2
2wœÉ2
0

d¬µ.

8.2 Bayes Factors
417
Combining the two quadratics in ¬µ gives
p (y|Model 0) =

1
>
2œÄœÉ2
0
n
exp

‚àí1
2œÉ2
0
n

i=1
(yi ‚àíy)2

1
>
2œÄwœÉ2
0
exp

‚àín
2œÉ2
0

 n
œÉ2
0
+
1
wœÉ2
0
‚àí1
1
wœÉ2
0
(y ‚àí¬µ1)2


exp

‚àí1
2

 n
œÉ2
0
+
1
wœÉ2
0

(¬µ ‚àí5¬µ)2

d¬µ,
where 5¬µ has the same form as in Example 8.3. After the integration is
carried out, one gets
p (y|Model 0) =

1
>
2œÄœÉ2
0
n
exp

‚àí1
2œÉ2
0
n

i=1
(yi ‚àíy)2

1
>
2œÄwœÉ2
0
exp

‚àí1
2
n
œÉ2
0

 n
œÉ2
0
+
1
wœÉ2
0
‚àí1
1
wœÉ2
0
(y ‚àí¬µ1)2
 C
2œÄœÉ2
0

n + 1
w
‚àí1
=

1
>
2œÄœÉ2
0
n
1
‚àönw + 1 exp

‚àíQy
2œÉ2
0

= p

y|œÉ2=œÉ2
0

,
(8.23)
where
Qy =
n

i=1
(yi ‚àíy)2 + n

n + 1
w
‚àí1 1
w (y ‚àí¬µ1)2 .
In order to obtain the marginal density of the data under Model 1, use
is made of (8.20) and of (8.23), although noting that œÉ2 is now a free pa-
rameter. Then, recalling that the prior distribution of œÉ2 is scaled inverted
chi-square
p (y|Model 1) =

p

y|œÉ2
p

œÉ2|ŒΩ, S2
dœÉ2
=
 
2œÄœÉ2‚àín
2
(nw + 1)
1
2 exp

‚àíQy
2œÉ2
 
ŒΩS2
2
 ŒΩ
2
Œì
 ŒΩ
2


œÉ2‚àí( ŒΩ+2
2 ) exp

‚àíŒΩS2
2œÉ2

dœÉ2
=
(2œÄ)‚àín
2
(nw + 1)
1
2

ŒΩS2
2
 ŒΩ
2
Œì
 ŒΩ
2

 
œÉ2‚àí( n+ŒΩ+2
2
) exp

‚àíŒΩS2 + Qy
2œÉ2

dœÉ2
=
(2œÄ)‚àín
2
(nw + 1)
1
2

ŒΩS2
2
 ŒΩ
2 Œì
 n+ŒΩ
2

Œì
 ŒΩ
2


ŒΩS2 + Qy
2
‚àí( n+ŒΩ
2 )
.
(8.24)
In order to arrive at the last result, use is made of the gamma integrals
(see Chapter 1). The Bayes factor in favor of Model 1 relative to Model 0

418
8. Bayesian Assessment of Hypotheses and Models
is given by the ratio between (8.24) and (8.23) yielding
B10 =

ŒΩS2
2
 ŒΩ
2 Œì
 n+ŒΩ
2

(œÉ2
0)‚àín
2 exp

‚àíQy
2œÉ2
0

Œì
 ŒΩ
2


ŒΩS2 + Qy
2
‚àí( n+ŒΩ
2 )
.
‚ñ†
8.2.6
Approximations to the Bayes Factor
There is extensive literature describing various approximate criteria for
Bayesian model selection. Some have been motivated by the desire for sup-
pressing the dependence of the Ô¨Ånal results on the prior. Ease of computa-
tion has also been an important consideration, especially in the pre-MCMC
era. Some of these methods are still a useful part of the toolkit for com-
paring models. This section introduces widely used approximations to the
Bayes factor based on asymptotic arguments. The latter are based on reg-
ularity conditions which fail when the parameter lies on a boundary of its
parameter space (Pauler et al., 1999), a restriction not encountered with
the Bayes factor.
The marginal density of the data under Model i, say, is
p (y|Mi) =

p (y|Œ∏i, Mi) p (Œ∏i|Mi) dŒ∏i,
(8.25)
where Œ∏i is the pi √ó 1 vector of parameters under this model. In what
follows, it will be assumed that the dimension of the parameter vector
does not increase with the number of observations or that, if this occurs,
it does so in a manner that, for n being the number of observations, pi/n
goes to 0 as n ‚Üí‚àû. This is important for asymptotic theory to hold. In
the context of quantitative genetic applications, there are models in which
the number of parameters, e.g., the additive genetic eÔ¨Äects, increases as the
number of observations increases. For such models the approximations hold
provided that these eÔ¨Äects are Ô¨Årst integrated out, in which case p (y|Œ∏i, Mi)
would be an integrated likelihood. For example, suppose a Gaussian linear
model has f location parameters, n additive genetic eÔ¨Äects (one for each
individual), and two variance components. Then analytical integration of
the additive eÔ¨Äects (over their prior distribution) would need to be eÔ¨Äected
before proceeding. On the other hand, if the model is one of repeated
measures taken on subjects or clusters (such as a family of half-sibs), it is
reasonable to defend the assumption that pi/n goes to 0 asymptotically.
Using the Posterior Mode
As in Chapter 7, expand the logarithm of the integrand in (8.25) around
the posterior mode, ,Œ∏i, using a second-order Taylor series expansion, to

8.2 Bayes Factors
419
obtain (recall that the gradient vanishes at the maximum value)
log [p (y|Œ∏i, Mi) p (Œ∏i|Mi)]
‚âàlog

p

y|,Œ∏i, Mi

p

,Œ∏i|Mi

‚àí1
2

Œ∏i ‚àí,Œ∏i
‚Ä≤ 
HŒ∏i
 
Œ∏i ‚àí,Œ∏i

,
(8.26)
where HŒ∏i is the corresponding negative Hessian matrix. Then, using this
in (8.25),
p (y|Mi) =

exp {log [p (y|Œ∏i, Mi) p (Œ∏i|Mi)]} dŒ∏i
‚âàexp
9
log

p

y|,Œ∏i, Mi

p

,Œ∏i|Mi
:
√ó

exp

‚àí1
2

Œ∏i ‚àí,Œ∏i
‚Ä≤ 
HŒ∏i
 
Œ∏i ‚àí,Œ∏i

dŒ∏i.
The integral is in a Gaussian form (this approach to integration is called
Laplace‚Äôs method for integrals), so it can be evaluated readily. Hence
p (y|Mi) ‚âàp

y|,Œ∏i, Mi

p

,Œ∏i|Mi

(2œÄ)
pi
2
"""H‚àí1
Œ∏i
"""
1
2 ,
(8.27)
where H‚àí1
Œ∏i is the variance‚Äìcovariance matrix of the Gaussian approxima-
tion to the posterior distribution. Further
log [p (y|Mi)] ‚âàlog

p

y|,Œ∏i, Mi

+ log

p

,Œ∏i|Mi

+ pi
2 log (2œÄ) + 1
2 log
"""H‚àí1
Œ∏i
"""

.
(8.28)
Twice the logarithm of the Bayes factor for Model i relative to Model j,
to express the ‚Äúevidence brought up by the data‚Äù in support of Model i
relative to j in the same scale as likelihood ratio tests, is then
2 log (Bij) ‚âà2 log
Ô£Æ
Ô£∞
p

y|,Œ∏i, Mi

p

y|,Œ∏j, Mj

Ô£π
Ô£ª+ 2 log
p

,Œ∏i|Mi

p

,Œ∏j|Mj

+ (pi ‚àípj) log (2œÄ) + log
Ô£´
Ô£≠
"""H‚àí1
Œ∏i
"""
"""H‚àí1
Œ∏j
"""
Ô£∂
Ô£∏.
(8.29)
Note that the criterion depends on the log-likelihood ratios (evaluated at
the posterior modes), on the log-prior ratios (also evaluated at the modes),
on the diÔ¨Äerence between the dimensions of the two competing models, and
on a Hessian adjustment.

420
8. Bayesian Assessment of Hypotheses and Models
Using the Maximum Likelihood Estimator
A variant to approximation (8.26) is when the expansion of the logarithm
of the product of the prior density and of the conditional distribution of
the observations (given the parameters) is about the maximum likelihood
estimator 5Œ∏i, instead of the mode of the posterior distribution (Tierney and
Kadane, 1989; O‚ÄôHagan, 1994; Kass and Raftery, 1995). Here one obtains
in (8.27),
p (y|Mi) ‚âàp

y|5Œ∏i, Mi

p

5Œ∏i|Mi

(2œÄ)
pi
2
"""H‚àí1
Œ∏i
"""
1
2 ,
(8.30)
where HŒ∏ is the observed information matrix evaluated at the maximum
likelihood estimator. In particular, if the observations are i.i.d. one has
HŒ∏ = nH1,Œ∏, where H1,Œ∏ is the observed information matrix calculated
from a single observation. Then
p (y|Mi) ‚âàp

y|5Œ∏i, Mi

p

5Œ∏i|Mi

(2œÄ)
pi
2 (n)‚àípi
2
"""H‚àí1
1,Œ∏i
"""
1
2 .
(8.31)
The approximation to twice the logarithm of the Bayes factor becomes
2 log (Bij) ‚âà2 log
Ô£Æ
Ô£∞
p

y|5Œ∏i, Mi

p

y|5Œ∏j, Mj

Ô£π
Ô£ª+ 2 log
p

5Œ∏i|Mi

p

5Œ∏j|Mj

‚àí(pi ‚àípj) log n
2œÄ + log
"""H‚àí1
1,Œ∏i
"""
"""H‚àí1
1,Œ∏j
"""
.
(8.32)
It is important to note that even though the asymptotic approximation to
the posterior distribution (using the maximum likelihood estimator) does
not depend on the prior, the resulting approximation to the Bayes factor
does depend on the ratio of priors evaluated at the corresponding maximum
likelihood estimators. If the term on the logarithm of the prior densities
is excluded, the resulting expression is called the Bayesian information
criterion (or BIC) (Schwarz, 1978; Kass and Raftery, 1995; Leonard and
Hsu, 1999).
Suppose that the prior conveys some sort of ‚Äúminimal‚Äù information rep-
resented by the distribution Œ∏i|Mi ‚àºN

5Œ∏i, H‚àí1
1,Œ∏

. This is a unit infor-
mation prior centered at the maximum likelihood estimator and having a
precision (inverse of the covariance matrix) equivalent to that brought up

8.2 Bayes Factors
421
by a sample of size n = 1. Using this in (8.31):
p (y|Mi) ‚âàp

y|5Œ∏i, Mi

(2œÄ)‚àípi
2
"""H‚àí1
1,Œ∏
"""
‚àí1
2
√ó exp

‚àí1
2

5Œ∏ ‚àí5Œ∏
‚Ä≤ 
H1,Œ∏1,Œ∏
 
5Œ∏ ‚àí5Œ∏

(2œÄ)
pi
2 (n)‚àípi
2
"""H‚àí1
1,Œ∏
"""
1
2
= p

y|5Œ∏i, Mi

(n)‚àípi
2 .
(8.33)
Hence
2 log (Bij) ‚âà2 log
Ô£Æ
Ô£∞
p

y|5Œ∏i, Mi

p

y|5Œ∏j, Mj

Ô£π
Ô£ª‚àí(pi ‚àípj) log n.
(8.34)
This is Schwarz (1978) BIC in its most commonly presented form (Kass
and Raftery, 1995; O‚ÄôHagan, 1994). Some authors (Leonard and Hsu, 1999;
Congdon, 2001) use the term BIC to refer just to the approximated marginal
densities, e.g., the logarithm of (8.33). At any rate, note that (8.34) is twice
the maximized log-likelihood ratio, plus an adjustment that penalizes the
model with more parameters. If n = 1, there is no penalty. However, the
term (pi ‚àípj) becomes more important as a sample size increases. When
pi > pj, (8.34) is smaller than twice the log-likelihood ratio, so the adjust-
ment favors parsimony. In contrast, classical testing based on the traditional
likelihood ratio tends to favor the more complex models. Contrary to the
traditional likelihood ratio, BIC is well deÔ¨Åned for nonnested models.
Denoting S the right hand side of (8.34) divided by 2, as sample size
n ‚Üí‚àû, this quantity satisÔ¨Åes:
S ‚àílog Bij
log Bij
‚Üí0,
so it is consistent in this sense (Kass and Raftery, 1995). Recent extensions
of BIC can be found in Kass (1995).
A related criterion is AIC (or the Akaike‚Äôs information criterion) (Akaike,
1973), where the penalty is 2 (pi ‚àípj) . The argument underlying the AIC is
that if two models favor the data equally well, then the more parsimonious
one should be favored. The BIC produces an even more drastic penalty,
which increases with sample size, as noted.
The diÔ¨Äerences between the likelihood ratio criterion, the BIC, and the
AIC are discussed by O‚ÄôHagan (1994) in the context of a nested model. The
larger model has parameters (Œ∏, œÜ) and dimension p2, whereas the ‚Äúsmaller
or null‚Äù model has a parameter vector Œ∏ with p1 elements and p2 ‚àíp1
Ô¨Åxed components œÜ = œÜ0. For a large sample size, the log-likelihood ratio
may favor the larger model, yet the penalty, (p2 ‚àíp1) log n, may be severe
enough so that the Bayes factor may end up favoring the null model.

422
8. Bayesian Assessment of Hypotheses and Models
It is instructive to examine the behavior of the approximation to the
Bayes factor under repeated sampling from the appropriate model. Con-
sider the BIC as given in (8.32), and take its expected value under the
null model, with only the likelihood ratio viewed as a random variable.
Recalling that the expected value of twice the log-likelihood ratio statistic
under the null hypothesis is equal to the diÔ¨Äerence in dimension between
the competing models, or (p2 ‚àíp1) , one gets
E [2 log (B21)] ‚âà2 log
p

5Œ∏2|M2

p

5Œ∏1|M1
 ‚àí(p2 ‚àíp1)

log n
2œÄ ‚àí1

+ constant.
Hence, as n ‚Üí‚àû, the expected value of the log of the Bayes factor in favor
of the larger model goes to ‚àí‚àû. This implies that the posterior probability
of the larger model goes to 0 when the null model is true, regardless of the
prior odds ratios as conveyed by p

5Œ∏2|M2

/p

5Œ∏1|M1

. Conversely, when
the larger model is true, the expected value of twice the log-likelihood ratio
statistic is approximately equal to nQ (œÜ, œÜ0) ,where Q (¬∑) is a quadratic
form (O‚ÄôHagan, 1994). This is a consequence of the asymptotically normal
distribution of the maximum likelihood estimator (see Chapters 3 and 4).
Then, under the larger model,
E [2 log (Bij)] ‚âànQ (œÜ, œÜ0)+2 log
p

5Œ∏i|Mi

p

5Œ∏j|Mj
 ‚àí(p2 ‚àíp1) log n
2œÄ +constant.
As n ‚Üí‚àû, the logarithm of the Bayes factor in favor of the larger model
goes to ‚àû, since n grows faster than log n. Consequently, the posterior
probability of the larger model goes to 1, no matter what the prior odds
are. Strictly from a classical point of view, and no matter how large n is,
the null model will be rejected with probability equal to the signiÔ¨Åcance
level even when the model is true. Hence, more stringent signiÔ¨Åcance levels
should be adopted in classical hypothesis testing when sample sizes are
large. Classical theory does not give a procedure for modifying the type-
1 error as a function of sample size, and the probability of this error is
prescribed arbitrarily. As noted by O‚ÄôHagan (1994), the Bayesian approach
gives an automatic procedure in which in a single formula, such as (8.32),
the evidence from the data, the prior odds, the model dimensionality, and
the sample size are combined automatically.
8.2.7
Partial and Intrinsic Bayes Factors
The Bayes factor is only deÔ¨Åned up to arbitrary constants when prior dis-
tributions are improper (i.e., Berger and Pericchi, 1996), as was illustrated
at the end of Subsection 8.2.5. Further, when the priors are proper, the

8.2 Bayes Factors
423
Bayes factor depends on the form of the chosen prior distribution, as seen
in connection with (8.32). This dependence does not decrease as sample
size increases, contrary to the case of estimation of parameters from poste-
rior distributions. In estimation problems and under regularity conditions,
one can obtain an asymptotic approximation centered at the maximum
likelihood estimator that does not involve the prior.
Berger and Pericchi (1996) suggested what are called intrinsic Bayes
factors, in an attempt to circumvent the dependence on the prior, and to
allow for the use of improper prior distributions, such as those based on
JeÔ¨Äreys‚Äô rule. Here, a brief overview of one of the several proposed types
of Bayes factors (the arithmetic intrinsic Bayes factor) is presented.
Let the data vector of order n be partitioned as
y =

y‚Ä≤
(1), y‚Ä≤
(2), . . . , y‚Ä≤
(L)
‚Ä≤
,
where y(l), (l = 1, 2, ..., L) denotes what is called the minimal training sam-
ple. This is the minimal number of observations needed for the posterior
distribution to be proper. For example, if the minimal size of the training
sample is m, there would be Cn
m diÔ¨Äerent possible training samples. The
posterior distribution based on the minimal training sample has density
p

Œ∏i|y(l), Mi

. Further, put
y =

y‚Ä≤
(l), y‚Ä≤
(‚àíl)
‚Ä≤
,
where y(‚àíl) is the data vector with y(l) removed. Then the predictive den-
sity of y(‚àíl) under model i, conditionally on the data of the training sample
y(l), is
p

y(‚àíl)|y(l), Mi

=

p

y(‚àíl)|Œ∏i, y(l), Mi

p

Œ∏i|y(l), Mi

dŒ∏i.
The Bayes factor for model j relative to model i, conditionally on y(l), or
partial Bayes factor (O‚ÄôHagan, 1994) is
Bji

y(l)

= p

y(‚àíl)|y(l), Mj

p

y(‚àíl)|y(l), Mi
 .
(8.35)
Clearly, the partial Bayes factor depends on the choice of the training sam-
ple y(l). To eliminate this dependence, Berger and Pericchi (1996) propose
averaging Bji

y(l)

over all Cn
m = K training samples. This yields the
arithmetic intrinsic Bayes factor, deÔ¨Åned formally as
BAI
ji = 1
K
K

l=1
p

y(‚àíl)|y(l), Mj

p

y(‚àíl)|y(l), Mi
 .
(8.36)

424
8. Bayesian Assessment of Hypotheses and Models
This expression can be computed for any pair of models, irrespective of
whether these are nested or not. Although the procedure is appealing, some
diÔ¨Éculties arise. First, for most realistic hierarchical models it is not pos-
sible to determine in advance what the minimum sample size should be in
order for the posterior to be proper. Second, and especially in animal breed-
ing, the data sets are very large so, at best, just a few minimal training
samples could be processed in practice.
There have been several other attempts to circumvent the need to us-
ing proper priors and to restrict the dependence on the prior. These are
reviewed in O‚ÄôHagan (1994).
8.3
Estimating the Marginal Likelihood
from Monte Carlo Samples
Except in highly stylized models, the integration indicated in (8.25) is not
feasible by analytical means. An alternative is to use Monte Carlo methods.
Here we shall consider the method of importance sampling, which will
be encountered again in Chapters 12 and 15, where more details on the
technique are given. Suppose samples of Œ∏i, the parameter vector under
Model i, can be obtained from some known distribution that is relatively
easy to sample from. This distribution, having the same support as the prior
or posterior, is called the importance sampling distribution, and its density
will be denoted as g (Œ∏i) . Then since

p (Œ∏i|Mi) dŒ∏i = 1, the marginal
density of the data under Model i is expressible as
p (y|Mi)
=

p (y|Œ∏i, Mi) p (Œ∏i|Mi) dŒ∏i

p (Œ∏i|Mi) dŒ∏i
=

p (y|Œ∏i, Mi) p(Œ∏i|Mi)
g(Œ∏i) g (Œ∏i) dŒ∏i
 p(Œ∏i|Mi)
g(Œ∏i) g (Œ∏i) dŒ∏i
.
(8.37)
Various Monte Carlo sampling schemes can be derived from (8.37), depend-
ing on the importance sampling function adopted. Suppose m samples can
be obtained from the distribution with density g (Œ∏i) ; let the samples be
Œ∏[j]
i , (j = 1, 2, . . . , m) . Then note that the denominator of (8.37) can be
written as
 p (Œ∏i|Mi)
g (Œ∏i)
g (Œ∏i) dŒ∏i = lim
m‚Üí‚àû
Ô£Æ
Ô£∞1
m
m

j=1
p

Œ∏[j]
i |Mi

g

Œ∏[j]
i

Ô£π
Ô£ª,

8.3 Estimating the Marginal Likelihood
425
where p

Œ∏[j]
i |Mi

is the prior density under Model i evaluated at sampled
value j. Likewise, the numerator can be written as

p (y|Œ∏i, Mi) p (Œ∏i|Mi)
g (Œ∏i)
g (Œ∏i) dŒ∏i
= lim
m‚Üí‚àû
Ô£Æ
Ô£∞1
m
m

j=1
p

y|Œ∏[j]
i , Mi
 p

Œ∏[j]
i |Mi

g

Œ∏[j]
i

Ô£π
Ô£ª,
where p

y|Œ∏[j]
i , Mi

is the density of the sampling model evaluated at the
jth sample obtained from the importance distribution. Hence for large m,
and putting w[j]
i
= p

Œ∏[j]
i |Mi

/g

Œ∏[j]
i

, a consistent estimator of (8.37)
is given by the ratio
5p (y|Mi) =
m

j=1
w[j]
i p

y|Œ∏[j]
i , Mi

m

j=1
w[j]
i
,
(8.38)
which is a weighted average of the density of the sampling distribution
evaluated at the corresponding sampled values of the parameter vector
under the appropriate model.
Sampling from the Prior
If the importance distribution is the prior, each of the weights w[j]
i
are equal
to 1, and the Monte Carlo estimator (8.38) of the marginal density at the
observed value of y becomes
5p (y|Mi) = 1
m
m

j=1
p

y|Œ∏[j]
i , Mi

,
(8.39)
where the Œ∏[j]
i
are draws from the prior distribution. The procedure is very
simple because the joint prior distribution of the parameters is often simple
to sample from. However, the estimator is imprecise because, typically, the
Œ∏[j]
i
drawn from the prior are conferred little likelihood by the data. There
will be just a few draws that will have appreciable likelihood and these will
‚Äúdominate‚Äù the average (Kass and Raftery, 1995). Numerical studies can
be found in McCulloch and Rossi (1991).

426
8. Bayesian Assessment of Hypotheses and Models
Sampling from the Posterior
If the importance distribution is the posterior, then
wi =
p (Œ∏i|Mi)
p (Œ∏i|y, Mi)
=
p (Œ∏i|Mi)
p(y|Œ∏i,Mi)p(Œ∏i|Mi)
p(y|Mi)
=
p (y|Mi)
p (y|Œ∏i, Mi).
Using this in (8.38):
5p (y|Mi) =
m

j=1
p(y|Mi)
p

y|Œ∏[j]
i ,Mi
p

y|Œ∏[j]
i , Mi

m

j=1
p(y|Mi)
p

y|Œ∏[j]
i ,Mi

=
m
m

j=1
1
p

y|Œ∏[j]
i ,Mi

=
Ô£Æ
Ô£∞1
m
m

j=1
1
p

y|Œ∏[j]
i , Mi

Ô£π
Ô£ª
‚àí1
.
(8.40)
This estimator, the harmonic mean of the likelihood values, was derived
by Newton and Raftery (1994), but arguing directly from Bayes theorem.
Observe that a rearrangement of the theorem leads to
p (Œ∏i|Mi)
p (y|Mi) = p (Œ∏i|y, Mi)
p (y|Œ∏i, Mi).
Then, integrating both sides with respect to Œ∏i, yields
1
p (y|Mi)

p (Œ∏i|Mi) dŒ∏i =

1
p (y|Œ∏i,Mi)p (Œ∏i|y, Mi) dŒ∏i.
Since the prior must be proper for the marginal density of the data to be
deÔ¨Åned, the integral on the left is equal to 1 leading directly to
p (y|Mi) =
1
EŒ∏i|y,Mi [p‚àí1 (y|Œ∏i, Mi)].
(8.41)
The Monte Carlo estimator of the reciprocal of the posterior expectation
of the reciprocal of the likelihood values is precisely (8.40). An advantage
of the harmonic mean estimator is that one does not need to know the
form of the posterior distribution. The Markov chain Monte Carlo meth-
ods presented in the next part of the book enable one to draw samples
from complex, unknown, distributions. The disadvantage, however, is its

8.3 Estimating the Marginal Likelihood
427
numerical instability. The form of (8.40) reveals that values of Œ∏i with very
small likelihood can have a strong impact on the estimator. An alternative
is to form some robust estimator of the harmonic mean (Congdon, 2001)
such as a trimmed average. Kass and Raftery (1995) state that, in spite of
the lack of stability, the estimator is accurate enough for interpretation on
a logarithmic scale.
Caution must be exercised in the actual computation of (8.40), to avoid
numerical over- or under-Ô¨Çows. A possible strategy could be as follows. Let
v
=
1
m
m

j=1
p‚àí1 
y|Œ∏[j], Mi

=
1
m
n

j=1
S[j]
i ,
where S[j]
i
= p‚àí1 
y|Œ∏[j], Mi

, and store log S[j]
i
in a Ô¨Åle for each sampled
value. Then, since
exp (x) = exp (x ‚àíc + c) = exp (x ‚àíc) exp c,
one can write v in the form
v = 1
m
m

j=1
exp

log S[j]
i
‚àíc

exp c,
where c is the largest value of log S[j]
i . Taking logarithms yields
log v = log
Ô£Æ
Ô£∞1
m
m

j=1
exp

log S[j]
i
‚àíc

Ô£π
Ô£ª+ c.
Hence
log [5p (y|Mi)] = ‚àílog v.
Chib‚Äôs Method
Most often, the marginal posterior distributions cannot be identiÔ¨Åed. How-
ever, there are many models where the conditional posterior distributions
can be arrived at from inspection of the joint posterior densities. Advan-
tage of this is taken in a Markov chain-based method called the Gibbs
sampler, which will be introduced in Chapter 11. Chib (1995) outlined a
procedure for estimating the marginal density of the data under a given
model when the fully conditional posterior distributions can be identiÔ¨Åed.
These distributions are deÔ¨Åned in Section 11.5.1 of Chapter 11. We will

428
8. Bayesian Assessment of Hypotheses and Models
suppress the dependency on the model in the notation, for simplicity. Sup-
pose the parameter vector is partitioned as Œ∏ =

Œ∏‚Ä≤
1, Œ∏‚Ä≤
2
‚Ä≤ . The logarithm
of the marginal density of the data can be expressed as
log p (y) = log [p (y|Œ∏1, Œ∏2)] + log [p (Œ∏1, Œ∏2)] ‚àílog [p (Œ∏1, Œ∏2|y)]
= log [p (y|Œ∏1, Œ∏2)] + log [p (Œ∏1, Œ∏2)] ‚àílog [p (Œ∏2|Œ∏1, y)] ‚àílog [p (Œ∏1|y)] .
Suppose now that samples of Œ∏1, Œ∏2 have been drawn from the posterior
distribution using the Gibbs sampler. Inspection of a large number of sam-
ples permits us to calculate, e.g., the posterior mean, mode, or median for
each of the elements of the parameter vector, such that one can form, say,
the vector of posterior medians ,Œ∏ =

,Œ∏
‚Ä≤
1, ,Œ∏
‚Ä≤
2
‚Ä≤
. An estimate of the marginal
density of the data can be obtained as
log 5p (y) = log

p

y|,Œ∏1, ,Œ∏2

+ log

p

,Œ∏1, ,Œ∏2

‚àílog

p

,Œ∏2|,Œ∏1, y

‚àílog

p

,Œ∏1|y

.
(8.42)
If the conditional density log [p (Œ∏2|Œ∏1, y)] is known, the third term can
be evaluated readily. The diÔ¨Éculty resides in the fact that the marginal
posterior density may not be known. However, recall that
p (Œ∏1|y) = EŒ∏2|y [p (Œ∏1|Œ∏2, y)] .
Hence
p

,Œ∏1|y

= EŒ∏2|y

p

,Œ∏1|Œ∏2, y

,
and an estimate of the marginal posterior density can be obtained as
5p

,Œ∏1|y

= 1
m
m

j=1
p

,Œ∏1|Œ∏[j]
2 , y

,
where Œ∏[j]
2 , (j = 1, 2, . . . , m) are samples from the marginal posterior distri-
bution of Œ∏2 obtained with the Gibbs sampler. Then, using this in (8.42),
the estimated marginal density of the data is arrived at as
log 5p (y) = log

p

y|,Œ∏1, ,Œ∏2

+ log

p

,Œ∏1, ,Œ∏2

‚àílog

p

,Œ∏2|,Œ∏1, y

‚àílog
Ô£Æ
Ô£∞1
m
m

j=1
p

,Œ∏1|Œ∏[j]
2 , y

Ô£π
Ô£ª.
(8.43)
The procedure is then repeated for each of the models in order to calculate
the Bayes factor. However, the fully conditional posterior distribution of
one parameter given the other must be identiÔ¨Åable in each of the models.
The method can be extended from two to several parameter blocks (Chib,
1995; Han and Carlin, 2001). Additional reÔ¨Ånements of the procedure are
in Chib and Jeliazkov (2001).

8.4 Goodness of Fit and Model Complexity
429
8.4
Goodness of Fit and Model Complexity
In general, as a model becomes increasingly more complex, i.e., by increas-
ing the number of parameters, its Ô¨Åt gets better. For example, it is well
known that if one Ô¨Åts n regression coeÔ¨Écients to a data set consisting of
n points, the Ô¨Åt is perfect. As seen earlier, the AIC and BIC introduce
penalties against more highly parameterized models. A slightly diÔ¨Äerent
approach was suggested by Spiegelhalter et al. (2002), and it is based on
calculating the expected posterior deviance, i.e., a measure of Ô¨Åt.
Consider a model with parameter vector [Œ∏1, Œ∏2] . For example, in a mixed
linear model Œ∏1, (p1 √ó 1) may be a vector of ‚ÄúÔ¨Åxed‚Äù eÔ¨Äects such as breed
or sex of animal and variance parameters, while Œ∏2 may be a vector of ran-
dom eÔ¨Äects or missing data. Hence, in some sense, the dimension of Œ∏1 can
be viewed as Ô¨Åxed, as the dimension of the data vector increases, whereas
the order of Œ∏2 may perhaps increase with n. Clearly, neither the AIC nor
the BIC can be used in models where the parameters outnumber the ob-
servations (Gelfand and Dey, 1994) unless some parameters are integrated
out. In what follows it will be assumed that Œ∏2 can be integrated some-
how, possibly by analytical means, and that one arrives at the integrated
likelihood
p (y|Œ∏1) =

p (y|Œ∏1, Œ∏2) p (Œ∏2|Œ∏1) dŒ∏2.
Here p (Œ∏2|Œ∏1) is the density of the conditional distribution of the nuisance
parameters, given the primary model parameters. The dependency on the
model will be suppressed in the notation. Now rearrange the components
of Bayes theorem as
p (Œ∏1|y)
p (Œ∏1)
= p (y|Œ∏1)
p (y) .
Taking expectations of the logarithm of both sides with respect to the
posterior distribution of Œ∏1, one gets

log
p (Œ∏1|y)
p (Œ∏1)

p (Œ∏1|y) dŒ∏1 =

log
p (y|Œ∏1)
p (y)

p (Œ∏1|y) dŒ∏1.
The left-hand side is the Kullback‚ÄìLeibler distance between the posterior
and prior distributions, so it is at least null. Following Dempster (1974,
1997), Spiegelhalter et al. (2002) suggest to view (given the model) p (y)
as a standardizing term and to set p (y) = 1. This can be construed as a
‚Äúperfect predictor‚Äù, giving probability 1 to each of the observations in the
data set (prior to observation). Hence, the larger the logarithm of p (y|Œ∏1)
(the log-likelihood), the closer the model is to ‚Äúperfect prediction‚Äù. Setting
p (y) = 1 and multiplying the right-hand side of the above expression by

430
8. Bayesian Assessment of Hypotheses and Models
‚àí2, deÔ¨Åne
D
=
‚àí2

[log p (y|Œ∏1)] p (Œ∏1|y) dŒ∏1
=
EŒ∏1|y [‚àí2 log p (y|Œ∏1)]
=
EŒ∏1|y [D (Œ∏1)] ,
(8.44)
where D (Œ∏1) = ‚àí2 log p (y|Œ∏1) is called the deviance (a function of the
unknown parameter), and D is its expected value taken over the posterior
distribution. Note that when the deviance is evaluated at the maximum
likelihood estimator, one obtains the numerator (or denominator) of the
usual likelihood ratio statistic. Thus, one averages out the deviance crite-
rion over values whose plausibilities are dictated by the posterior distribu-
tion. The expected deviance is interpreted as a posterior summary of the
Ô¨Åt of the model. In general, D will need to be computed using Monte Carlo
procedures for sampling from the posterior distribution: samples from the
posterior are obtained, and then one averages the log-likelihoods evaluated
at each of the draws.
Concerning model complexity (degree of parameterization), Spiegelhalter
et al. (2002) suggest using the ‚ÄúeÔ¨Äective number of parameters‚Äù
pD = D ‚àíD

Œ∏1

,
(8.45)
where D

Œ∏1

is the deviance evaluated at the posterior mean of the primary
parameter vector. In order to motivate this concept, expand the deviance
around the posterior mean Œ∏1, to obtain
D (Œ∏1) ‚âà‚àí2 log p

y|Œ∏1

‚àí2
‚àÇlog p (y|Œ∏1)
‚àÇŒ∏1
‚Ä≤
Œ∏1=Œ∏1

Œ∏1 ‚àíŒ∏1

‚àí

Œ∏1 ‚àíŒ∏1
‚Ä≤ ‚àÇ2 log p (y|Œ∏1)
‚àÇŒ∏1‚àÇŒ∏‚Ä≤
1
 
Œ∏1 ‚àíŒ∏1

.
(8.46)
Taking the expectation of (8.46), with respect to the posterior distribution
of the parameter vector, gives the expected deviance
D ‚âà‚àí2 log p

y|Œ∏1

+ tr

‚àí‚àÇ2 log p (y|Œ∏1)
‚àÇŒ∏1‚àÇŒ∏‚Ä≤
1

Œ∏1=Œ∏1
V ar (Œ∏1|y)
= D

Œ∏1

+ tr
#
‚àí‚àÇ2 log p (y|Œ∏1)
‚àÇŒ∏1‚àÇŒ∏‚Ä≤
1

Œ∏1=Œ∏1
V ar (Œ∏1|y)
$
= D

Œ∏1

+ tr

[I (Œ∏1)]Œ∏1=Œ∏1 V ar (Œ∏1|y)

,
(8.47)
where I (Œ∏1) is the observed information matrix and V ar (Œ∏1|y) is the
variance‚Äìcovariance matrix of the posterior distribution. The trace adjust-
ment in (8.47) is called the ‚ÄúeÔ¨Äective number of parameters‚Äù and is denoted

8.4 Goodness of Fit and Model Complexity
431
pD, following (8.45). Recall from Chapter 7, that an asymptotic approxi-
mation to the posterior distribution is given by a normal process having a
covariance matrix that is equal to the inverse of the sum of the observed
information matrix (evaluated at some mode), plus the negative Hessian of
the log-prior density (evaluated at some mode); the latter will be denoted as
P (Œ∏1)Œ∏1=Œ∏1 when evaluated at the posterior mean. Hence, approximately,
pD ‚âàtr

[I (Œ∏1)]Œ∏1=Œ∏1 V ar (Œ∏1|y)

‚âàtr
9
[I (Œ∏1)]Œ∏1=Œ∏1

[I (Œ∏1)]Œ∏1=Œ∏1 + P (Œ∏1)Œ∏1=Œ∏1
‚àí1:
.
(8.48)
Thus, the eÔ¨Äective number of parameters can be interpreted as the informa-
tion about Œ∏1 contained in the likelihood relative to the total information
in both the likelihood and the prior. Some additional algebra yields
pD ‚âàtr

[I (Œ∏1)]Œ∏1=Œ∏1 + P (Œ∏1)Œ∏1=Œ∏1 ‚àíP (Œ∏1)Œ∏1=Œ∏1

√ó

[I (Œ∏1)]Œ∏1=Œ∏1 + P (Œ∏1)Œ∏1=Œ∏1
‚àí1:
= p1 ‚àítr

P (Œ∏1)Œ∏1=Œ∏1

[I (Œ∏1)]Œ∏1=Œ∏1 + P (Œ∏1)Œ∏1=Œ∏1
‚àí1
.
(8.49)
This representation leads to the interpretation that the eÔ¨Äective number of
parameters is equal to the number of parameters in Œ∏1, minus an adjust-
ment measuring the amount of information in the prior relative to the total
information contained in the asymptotic approximation to the posterior.
Further, Spiegelhalter et al. (2002) suggested combining the measure of
Ô¨Åt given by D (the posterior expectation of the deviance) in (8.44) with
the eÔ¨Äective number of parameters in (8.48) or (8.49), into a deviance
information criterion (DIC). This is deÔ¨Åned as
DIC
=
D + pD
=
D

Œ∏1

+ 2pD,
(8.50)
with the last expression resulting from (8.45). Models having a smaller
DIC should be favored, as this indicates a better Ô¨Åt and a lower degree of
model complexity. The authors emphasize that they consider DIC to be a
preliminary device for screening alternative models.
Example 8.8
Deviance information criterion in the mixed linear model
Consider a hierarchical model with structure
y = WŒ∏ + e,
where y|Œ∏, R ‚àºN (WŒ∏, R) . This model has been discussed several times,
especially in Chapter 6. In animal breeding Œ∏ =

Œ≤‚Ä≤, u‚Ä≤‚Ä≤ is typically a vector
of ‚ÄúÔ¨Åxed‚Äù and ‚Äúrandom‚Äù eÔ¨Äects, and the corresponding known incidence
matrix is then W = [X, Z]. Suppose that the dimension of Œ≤ (pŒ≤) does not

432
8. Bayesian Assessment of Hypotheses and Models
increase with the number of observations, and that the vector u (having
order pu) contains the eÔ¨Äects of clusters, e.g., half-sib families. Hence, one
can conceptually let the number of observations per cluster go to inÔ¨Ån-
ity (or, equivalently, think that the number of observations increases more
rapidly than the number of clusters). Under these conditions, one can em-
ploy the asymptotic approximations discussed earlier. The second level of
the hierarchy poses
Œ∏|¬µŒ≤, ¬µu, VŒ≤, œÉ2
Œ≤, Gu ‚àºN

 ¬µŒ≤
¬µu

,
 VŒ≤œÉ2
Œ≤
0
0
Gu

.
The dispersion parameters R, VŒ≤, œÉ2
Œ≤, Gu, and the location vectors ¬µŒ≤ and
¬µu are assumed known. As mentioned in Chapter 1, Example 1.18, and
shown in Chapter 6, the posterior distribution of Œ∏ is normal, with mean
vector
Œ∏ =

Œ≤
u

=

X‚Ä≤R‚àí1X +
V‚àí1
Œ≤
œÉ2
Œ≤
X‚Ä≤R‚àí1Z
Z‚Ä≤R‚àí1X
Z‚Ä≤R‚àí1Z + G‚àí1
u
‚àí1
√ó

X‚Ä≤R‚àí1y +
V‚àí1
Œ≤
œÉ2
Œ≤ ¬µŒ≤
Z‚Ä≤R‚àí1y + G‚àí1
u ¬µu

,
and variance‚Äìcovariance matrix
C‚àí1 =

X‚Ä≤R‚àí1X +
1
œÉ2
Œ≤ V‚àí1
Œ≤
X‚Ä≤R‚àí1Z
Z‚Ä≤R‚àí1X
Z‚Ä≤R‚àí1Z + G‚àí1
u
‚àí1
.
The deviance is
D (Œ∏)
=
‚àí2 log p (y|Œ∏, R)
=
N log (2œÄ) + log |R| + (y ‚àíWŒ∏)‚Ä≤ R‚àí1 (y ‚àíWŒ∏) .
Then,
D

Œ∏

= N log (2œÄ) + log |R| +

y ‚àíWŒ∏
‚Ä≤ R‚àí1 
y ‚àíWŒ∏

,
and the expected deviance becomes
D = N log (2œÄ) + log |R| +

y ‚àíWŒ∏
‚Ä≤ R‚àí1 
y ‚àíWŒ∏

+tr

R‚àí1WC‚àí1W‚Ä≤
.
Employing (8.45), the eÔ¨Äective number of parameters is
pD
=
D ‚àíD

Œ∏

=
tr

C‚àí1W‚Ä≤R‚àí1W

.

8.5 Goodness of Fit and Predictive Ability of a Model
433
For example, let R = IœÉ2
e and Gu = IœÉ2
u, which results in a variance com-
ponent model. Further, let œÉ2
Œ≤ ‚Üí‚àû, to make prior information about Œ≤
vague. Here
C‚àí1
=

X‚Ä≤X
X‚Ä≤Z
Z‚Ä≤X
Z‚Ä≤Z + œÉ2
e
œÉ2
u I
‚àí1
œÉ2
e
=

CŒ≤Œ≤
CŒ≤u
CuŒ≤
Cuu

œÉ2
e,
and
C‚àí1W‚Ä≤R‚àí1W =

X‚Ä≤X
X‚Ä≤Z
Z‚Ä≤X
Z‚Ä≤Z+ œÉ2
e
œÉ2
u I
‚àí1 
X‚Ä≤X
X‚Ä≤Z
Z‚Ä≤X
Z‚Ä≤Z

= IpŒ≤+pu ‚àí

CŒ≤Œ≤
CŒ≤u
CuŒ≤
Cuu
 
0
0
0
œÉ2
e
œÉ2
u Ipu

.
Hence
pD
=
tr

C‚àí1W‚Ä≤R‚àí1W

=
tr

IpŒ≤+pu

‚àítr
#
CŒ≤Œ≤
CŒ≤u
CuŒ≤
Cuu
 
0
0
0
œÉ2
e
œÉ2
u Ipu
$
=
pŒ≤ + pu ‚àíœÉ2
e
œÉ2u
tr

0
CŒ≤u
0
Cuu

=
pŒ≤ + pu ‚àíœÉ2
e
œÉ2u
tr [Cuu] .
Note that the prior information about the u vector results in that the
eÔ¨Äective number of parameters is smaller than the dimension of Œ∏.
‚ñ†
8.5
Goodness of Fit and Predictive
Ability of a Model
The posterior probability of a model and the Bayes factors can be viewed
as global measures of model relative plausibility. However, one often needs
to go further than that. For example, a model can be the most plausible
within a set of competing models and, yet, either be unable to predict the
data at hand well or to give reasonable predictions of future observations.
Here we will provide just a sketch of some of the procedures that can be
used for gauging the quality of Ô¨Åt and predictive performance of a model.

434
8. Bayesian Assessment of Hypotheses and Models
8.5.1
Analysis of Residuals
A comprehensive account of techniques for examination of residuals is given
by Barnett and Lewis (1995). In order to illustrate some of the basic ideas,
consider, for example, a linear regression analysis. One of the most widely
used techniques for assessing Ô¨Åt is to carry out a residual analysis (e.g.,
Draper and Smith, 1981). In the context of classical regression, one calcu-
lates the predicted value of an observation, 5y, and forms the Studentized
Ô¨Åtted residual
y ‚àí5y
8
5œÉ2
e
,
where 5œÉ2
e is typically the unbiased estimator of the residual variance. If the
absolute value of the Studentized residual exceeds a certain critical value of
the t or normal distributions, then the observation is viewed as suspicious
and regarded as a potential outlier. This may be construed as an indication
that the model does not Ô¨Åt well.
The Bayesian counterpart of this classical regression analysis consists of
examining the posterior distribution of the unobserved standardized quan-
tity
ri = yi ‚àíx‚Ä≤
iŒ≤
>
œÉ2e
,
where the row vector x‚Ä≤
i contains known explanatory variables linking the
unknown regression vector Œ≤ to yi. Using the standard normality assump-
tions with independent and identically distributed errors, the distribution
of ri under the sampling model is ri ‚àºN (0, 1) , provided œÉ2
e is known. If Œ≤
has the prior distribution Œ≤|Œ±, VŒ≤ ‚àºN (Œ±, VŒ≤) , where the hyperparame-
ters are also known, one obtains as prior (or predictive) distribution of the
residual above, given œÉ2
e,
ri|Œ±, œÉ2
e, VŒ≤ ‚àºN

yi ‚àíx‚Ä≤
iŒ±
>
œÉ2e
, x‚Ä≤
iVŒ≤xi
œÉ2e

.
The unconditional (with respect to œÉ2
e) prior distribution of the standard-
ized residual will depend on the prior adopted for œÉ2
e. Then one could carry
out an analysis of the residuals prior to proceeding with Bayesian learning
about the parameters. More commonly, however, the residual analysis will
be undertaken based on the joint posterior distribution of Œ≤ and œÉ2
e. As
seen in Chapter 6, given œÉ2
e, the posterior distribution of Œ≤ is the normal
process
Œ≤|Œ±, VŒ≤, œÉ2
e, y ‚àºN

,Œ≤,

X‚Ä≤X
œÉ2e
+ V‚àí1
Œ≤
‚àí1
,
where
,Œ≤ =

X‚Ä≤X
œÉ2e
+ V‚àí1
Œ≤
‚àí1 
X‚Ä≤y
œÉ2e
+ V‚àí1
Œ≤ Œ±

.

8.5 Goodness of Fit and Predictive Ability of a Model
435
Further, given œÉ2
e, the posterior distribution of the Studentized residual will
have the form
ri|Œ±, VŒ≤, œÉ2
e, y ‚àºN
Ô£Æ
Ô£ØÔ£∞yi ‚àíx‚Ä≤
i,Œ≤
>
œÉ2e
,
x‚Ä≤
i

X‚Ä≤X
œÉ2
e + V‚àí1
Œ≤
‚àí1
xi
œÉ2e
Ô£π
Ô£∫Ô£ª.
The unconditional (with respect to œÉ2
e) posterior distribution will depend
on the form of the marginal posterior distribution of the residual variance,
and its density is obtained as
p

ri|Œ±, VŒ≤, œÉ2
e, y

=

p

ri|Œ±, VŒ≤, œÉ2
e, y

p

œÉ2
e|y

dœÉ2
e,
where p

œÉ2
e|y

is the marginal posterior density of the residual variance.
Unless standard conjugate priors are adopted, the marginal posterior dis-
tribution of the Studentized residual cannot be arrived at in closed form.
In such a situation, one can adopt the sampling techniques described in the
third part of the book and obtain draws from the posterior distribution of
the standardized residual. This is done simply by drawing from the pos-
terior distribution of the model parameters. Then, for observation i, one
forms samples
r[j]
i
= yi ‚àíx‚Ä≤
iŒ≤[j]
8
œÉ2[j]
e
,
j = 1, 2, . . . , m,
where Œ≤[j] and œÉ2[j]
e
are samples from the joint posterior distribution of the
regression vector and of the residual variance. Thus, one obtains an entire
distribution for each Studentized residual, which can be used to decide
whether or not the observation is in reasonable agreement with what the
model predicts. If the value 0 appears at high density in the posterior
distribution, this can be construed as an indication that the observation is
in conformity with the model.
This simple idea extends naturally to other models in which residuals
are well deÔ¨Åned. For example, for binary (0, 1) responses analyzed with a
probit model, Albert and Chib (1993, 1995) deÔ¨Åne the Bayesian residual
ri = yi ‚àíŒ¶ (x‚Ä≤
iŒ≤), which is real valued on the interval [yi ‚àí1, yi]. If samples
are taken from the posterior distribution of Œ≤, one can form corresponding
draws from the posterior distribution of each residual. Since Œ¶ (x‚Ä≤
iŒ≤) takes
values between 0 and 1, an observation yi = 0 will be outlying if the pos-
terior distribution of ri is concentrated towards the endpoint ‚àí1, and an
observation yi = 1 is suspect if the posterior of ri is concentrated towards
the value 1. A value of 0 appearing at high density in the posterior dis-
tribution of the residuals can be interpreted as an indication of reasonable
Ô¨Åt. Albert and Chib (1995) propose an alternative residual deÔ¨Åned at the
level of a latent variable called the liability (see Chapter 14 for a deÔ¨Ånition
of this concept). The reader is referred to their paper for details.

436
8. Bayesian Assessment of Hypotheses and Models
8.5.2
Predictive Ability and Predictive Cross-Validation
Predictive ability and goodness of Ô¨Åt are distinct features of a model. A
certain model may explain and predict adequately the observations used
for model building. However, it may yield poor predictions of future obser-
vations or of data points that are outside the range represented in the data
employed for model building. A number of techniques is available for gaug-
ing the predictive ability of a Bayesian model. Even though some attention
is paid to foundational issues, the approaches here are often eclectic and
explorative. They constitute an important set of tools for understanding
the predictive ability of a model.
Cross-validation methods involve constructing the posterior distribution
of the parameters but leaving some observations out. Then the predictive
distributions of the observations that have been removed are derived to
examine whether or not the actual data points fall in regions of reasonably
high density. Partition the data as y‚Ä≤ =

yout, y‚Ä≤
‚àíout

, where yout is the
observation to be removed, and y‚àíout is the vector of the remaining obser-
vations. The density of the posterior predictive distribution can be written
as
p (yout|y‚àíout, M) =

p (yout|Œ∏, y‚àíout, M) p (Œ∏|y‚àíout, M) dŒ∏,
(8.51)
where p (Œ∏|y‚àíout, M) is the density of the posterior distribution built from
y‚àíout and model M. In hierarchical modeling, one typically writes the
sampling distribution of the data such that conditional independence can
be exploited. Thus, given the parameters, yout is independent of y‚àíout, and
one can write (suppressing the notation denoting model M)
p (yout|y‚àíout) =

p (yout|Œ∏) p (Œ∏|y‚àíout) dŒ∏.
(8.52)
Since, in general, the form of the posterior density is unknown or analyti-
cally intractable, the predictive density will be calculated via Monte Carlo
methods (Gelfand et al., 1992; Gelfand, 1996). For example, if m draws
from the posterior distribution can be made via MCMC procedures, the
form of (8.52) suggests the estimator
5p (yout|y‚àíout) = 1
m
m

j=1
p

yout|Œ∏[j]
,
where Œ∏[j] is a draw from [Œ∏|y‚àíout] . The mean and variance of the predic-
tive distribution can also be computed by Monte Carlo procedures. Since
the expected value of the sampling model can almost always be deduced
readily, e.g., in regression E (yout|Œ∏) = x‚Ä≤
outŒ≤, the mean of the predictive
distribution can be estimated as
5E (yout|y‚àíout) = 1
m
m

j=1
E

yout|Œ∏[j]
.
(8.53)

8.5 Goodness of Fit and Predictive Ability of a Model
437
Similarly, a Monte Carlo estimate of the variance of the predictive distri-
bution can be obtained as
D
V ar (yout|y‚àíout) = 5E[Œ∏|y‚àíout] [V ar (yout|Œ∏)] + D
V ar [E (yout|Œ∏)] .
(8.54)
This can be illustrated with a regression model, although in this situation
there is an analytical solution under the standard assumptions. For exam-
ple, if the regression model postulates yout|Œ≤, œÉ2
e ‚àºN

x‚Ä≤
outŒ≤, œÉ2
e

, then
5E[Œ∏|y‚àíout] [V ar (yout|Œ∏)] = 1
m
m

j=1
œÉ2[j]
e
,
and
D
V ar[Œ∏|y‚àíout] [E (yout|Œ∏)] = D
V ar [x‚Ä≤
outŒ≤]
= 1
m
m

j=1

x‚Ä≤
outŒ≤[j]2
‚àí
Ô£´
Ô£≠1
m
m

j=1
x‚Ä≤
outŒ≤[j]
Ô£∂
Ô£∏
2
.
Subsequently, the following composite statistic can be used to evaluate the
overall predictive ability of the model (Congdon, 2001):
D2 =
n

out=1
Ô£Æ
Ô£∞yout ‚àí5E (yout|y‚àíout)
8
D
V ar (yout|y‚àíout)
Ô£π
Ô£ª
2
.
(8.55)
Models having a smaller value of D2 would be viewed as having a better pre-
dictive ability. Clearly, if n is very large, the computations may be taxing,
since n posterior and predictive distributions need to be computed. Other
statistics are described in Gelfand et al. (1992) and in Gelfand (1996).
A related idea has been advocated by Gelman et al. (1996). Rather than
working with the leave-one-out method in (8.52), they propose generating
data ,y from the posterior predictive distribution with density
p (,y|y,M) =

p (,y|Œ∏,M) p (Œ∏|y, M) dŒ∏.
(8.56)
One then wishes to study whether the simulated value ,y agrees with the
observed data y. Systematic diÔ¨Äerences between the simulations and the
observed data indicate potential failure of model M. Various criteria or test
quantities can be used to carry out the comparisons. Examples of these are
given in Gelfand (1996). The choice of test quantities should be driven by
the aspect of the model whose Ô¨Åt is in question and/or by the purpose with
which the model will be used. The method of composition (introduced in
Chapter 1), can be used to obtain draws from (8.56), and can be described
as follows:

438
8. Bayesian Assessment of Hypotheses and Models
1. Draw Œ∏ from the posterior distribution p (Œ∏|y, M). Ways of achieving
this are discussed later in this book.
2. Draw ,y from the sampling distribution p (,y|Œ∏,M). One has now a
single realization from the joint distribution p (,y, Œ∏|M).
3. Repeat steps 1 and 2 many times.
The set of ,y‚Ä≤s drawn using this algorithm constitutes samples from (8.56).
Letting h (y) be a particular test quantity, for example, the average of the
top 10 observations, one can then study whether h (y) falls in a region of
high posterior probability in the distribution [h (,y) |y,M]. This can be re-
peated for all the models under investigation. Gelman et al. (1996) propose
the calculation of Bayesian p-values, pB, for given test quantities h (y, Œ∏).
The notation emphasizes that, in contrast with classical p-values, the test
quantity can depend on both data and parameters. Then,
pB
=
p [h (,y, Œ∏) > h (y, Œ∏) |y]
=
 
I [h (,y, Œ∏) > h (y, Œ∏)] p (,y|Œ∏) p (Œ∏|y) dŒ∏d,y,
(8.57)
gives the probability that the simulated data ,y is more extreme than the
observed data y, averaged over the distribution [Œ∏|y]. A possible test quan-
tity could be
h (y, Œ∏) =
n

i=1
[yi ‚àíE (Yi|Œ∏)]2
V ar (Yi|Œ∏)
and
h (,y, Œ∏) =
n

i=1

,yi ‚àíE

,Yi|Œ∏
2
V ar

,Yi|Œ∏

.
These are then used for computing (8.57). A cross-validation approach can
also be implemented using this idea. An application of these techniques in
animal breeding is in Sorensen et al. (2000).
Another way of assessing global predictive ability of a set of models was
proposed by Geisser and Eddy (1979) and by Geisser (1993) via the con-
ditional predictive ordinate (CPO). The logarithm of the CPO for Model
i is
log [CPOModel i] =
n

out=1
log [p (yout|y‚àíout, Model i)] .
Gelfand and Dey (1994) describe techniques for calculating the CPO that
avoid carrying out the n implementations of the sampling procedure de-
scribed above. Chapter 12, especially Section 12.4, discusses Monte Carlo
implementation of these quantities in more detail.

8.6 Bayesian Model Averaging
439
8.6
Bayesian Model Averaging
8.6.1
General
Consider a survival analysis of sheep or of dairy cows. The information
available may consist of covariates such as herd or Ô¨Çock, sire, year-season
of birth, molecular markers, and last known survival status, since censoring
is pervasive. The objective of the analysis may be to assess the eÔ¨Äects of
explanatory variables, or to predict the survival time of the future progeny
of some of the sires. Hence, one searches for some reasonable survival model
(e.g., Gross and Clark, 1975; Collet, 1994) and Ô¨Ånds that a proportional
hazards model M1 Ô¨Åts well and that it gives sensible parameter estimates.
Then one proceeds to make predictions. However, another proportional
hazards model M2 also Ô¨Åts well, but it gives diÔ¨Äerent estimates and pre-
dictions. Which model should be used at the end?
Now imagine a standard regression analysis in which 15 predictor vari-
ables are available, and suppose that some ‚Äúbest‚Äù model must be sought.
Even if second-order and cross-product terms are ignored, there would be
215 diÔ¨Äerent models. For example, suppose that the variables are Y, X1, X2.
Then, using the standard notation, there are the following four possible
models
model 1
:
Y = Œ≤0 + e,
model 2
:
Y = Œ≤0 + Œ≤1X1 + e,
model 3
:
Y = Œ≤0 + Œ≤2X2 + e,
model 4
:
Y = Œ≤0 + Œ≤1X1 + Œ≤2X2 + e.
These models may diÔ¨Äer little in relative plausibility. Again, which model
ought to be used for predictions?
A third example is that of choosing between genetic models to infer
parameters, and to predict the genetic merit of future progeny. One spec-
iÔ¨Åcation may be the classical inÔ¨Ånitesimal model. A second speciÔ¨Åcation
may be a model with a Ô¨Ånite number of loci. If so, how many? A third
model may pose polygenic variation, plus the eÔ¨Äects of some marked QTL.
The preceding three examples illustrate that the problem of model choice
is pervasive. Typically, models are chosen in some ad-hoc manner, and
inferences are based on the model eventually chosen, as if there were no
uncertainty about it. In Bayesian analysis, however, it is possible to view
the model as an item subject to uncertainty. Then the ‚Äúmodel random
variable‚Äù is treated as a nuisance, and the posterior distribution of the
‚Äúmodel random variable‚Äù is used to obtain inferences that automatically
take into account the relative plausibility of the models under consideration.
This is called Bayesian model averaging, or BMA for short. We will outline
the basic ideas, and refer the reader to Madigan and Raftery (1994), Raftery
et al. (1997), and Hoeting et al. (1999) for additional details. These authors

440
8. Bayesian Assessment of Hypotheses and Models
argue as follows: since part of the evidence must be used in the process
of model selection, ignoring the uncertainty about the model leads to an
overstatement of precision in the analysis. In turn, this can lead to declaring
‚Äúfalse positives‚Äù, and the analysis lacks robustness unless, by chance, one
stumbles into the ‚Äúright‚Äù model. It will be shown at the end of this section
that BMA can be used to enhance the predictive ability of an analysis.
8.6.2
DeÔ¨Ånitions
Let
‚àÜ
=
parameter or future data point,
y
=
data,
M
=
{M1, M2, . . . , MK} set of models,
p (Mi)
=
prior probability of model i,
p (Mi|y)
=
posterior probability of model i.
The ‚Äúusual‚Äù Bayesian approach gives, as posterior distribution (or density)
of ‚àÜ,
p (‚àÜ|y, Mi) = p (y|‚àÜ, Mi) p (‚àÜ|Mi)
p (y|Mi)
,
and the notation indicates clearly that inferences are conditional on Mi, as
if the model were known to be true for sure. In BMA, on the other hand,
the idea is to average out over the posterior distribution of the models,
leading to
p (‚àÜ|y) = p (‚àÜand M1|y) + ¬∑ ¬∑ ¬∑ + p (‚àÜand MK|y)
=
K

i=1
p (‚àÜand Mi|y) =
K

i=1
p (‚àÜ|y, Mi) p (Mi|y) .
(8.58)
The preceding expression reveals that, in BMA, the model is treated as a
nuisance parameter. Hence, the nuisance is eliminated in the usual manner,
by integration or by summing. Then the inferences about a parameter can
be viewed as a weighted average of the inferences that would be drawn if
each of the models were true, using the posterior probability of the model
as a mixing distribution.
In BMA, the posterior expectation and variance are calculated in the
usual manner. For example, let the posterior mean of ‚àÜunder model k be
E (‚àÜ|Mk, y) =

‚àÜp (‚àÜ|Mk, y) d‚àÜ= 5‚àÜk
Then, unconditionally with respect to the model, one obtains
E (‚àÜ|y) = EM|y [E (‚àÜ|Mk, y)] =
K

k=1
5‚àÜk p (Mk|y) .
(8.59)

8.6 Bayesian Model Averaging
441
Similarly, one can use the variance decomposition
V ar (‚àÜ|y) = EM|y [V ar (‚àÜ|M, y)] + V arM|y [E (‚àÜ|M, y)] ,
leading to
V ar (‚àÜ|y) =
K

k=1
V ar (‚àÜ|Mk, y) p (Mk|y) +
K

k=1

5‚àÜk
2
p (Mk|y)
‚àí
 K

k=1
5‚àÜk p (Mk|y)
2
.
(8.60)
The idea is straightforward, and it makes eminent sense, at least from
a Bayesian perspective. The diÔ¨Éculty resides in that there can be many
models, as in a regression equation, where there may be at least 2p (for p
being the number of covariates) models, and even more when interactions
are included. Hoeting et al. (1999) discusses some of the methods that
have been used for reducing the number of terms to be included in the
sums appearing in (8.59) and (8.60).
8.6.3
Predictive Ability of BMA
Suppose one partitions the data as
y = [y‚Ä≤
Build, y‚Ä≤
Pred]‚Ä≤ ,
where yBuild is the data used for model building, and yPred includes the
data points to be predicted, as in predictive cross-validation. Good (1952)
introduced the predictive logscore (PLS) which, for Model k, is
PLSk = ‚àí

y‚ààyPred
log p (y|Mk, yBuild)
= ‚àí

y‚ààyPred
log

p (y|Œ∏k, Mk, yBuild) p (Œ∏k|Mk, yBuild) dŒ∏k,
(8.61)
where Œ∏k is the parameter vector under Model k. It it desirable to have a
model with as small a PLS as possible. Under BMA
PLSBMA = ‚àí

y‚ààyPred
log
 K

k=1
p (y|Mk, yBuild) p (Mk|yBuild)

.
(8.62)
Suppose that the model and the data to be predicted are unknown, which
is the usual situation. Now consider the diÔ¨Äerence
PLSBMA ‚àíPLSk = ‚àí

y‚ààyPred
log
K

k=1
p (y|Mk, yBuild) p (Mk|yBuild)
p (y|Mk, yBuild)
.

442
8. Bayesian Assessment of Hypotheses and Models
Next, take expectations of this diÔ¨Äerence with respect to the predictive
distribution under BMA (that is, averaging over all possible models). This
distribution has density
p (yPred|yBuild) =
K

k=1
p (yPred|Mk, yBuild) p (Mk|yBuild)].
Thus,
EyPred|yBuild (PLSBMA ‚àíPLSk)
= ‚àí

y‚ààyPred
E
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
log
K

k=1
p (y|Mk, yBuild) p (Mk|yBuild)
p (y|Mk, yBuild)
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
.
The expected value in the right hand side, taken over the distribution
[yPred|yBuild], is the Kullback‚ÄìLeibler discrepancy between the predictive
distributions of datum y under BMA and under Model k. Since the dis-
crepancy is at least 0, it follows that the right-hand side is at most null.
Hence
EyPred|yBuild (PLSBMA) ‚â§EyPred|yBuild (PLSk) ,
as in Madigan and Raftery (1994). This implies that under model uncer-
tainty, the predictive performance of BMA (at least in the PLS sense) is
expected to be better than that obtained under a single model, even if the
latter is the most probable one. Raftery et al. (1997) and Hoeting et al.
(1999) present several study cases supporting this theoretical result.
Typically, BMA leads to posterior distributions that are more spread
than those under a single model. This illustrates that inferences based on a
single model may give an unrealistic statement of precision; this may lead
to false positive results.
The reader is now equipped with the foundations on which Bayesian
inference rests. As stated before and especially for complex models, it is
seldom the case that exact methods of inference can be used. Fortunately,
methods for sampling from posterior distributions are available, and these
are discussed in Part III of this book.

9
Approximate Inference
Via the EM Algorithm
9.1
Introduction
The classical paradigm of maximum likelihood estimation is based on Ô¨Ånd-
ing the supremum of the likelihood function (if it exists), and on attaching
a measure of uncertainty via Fisher‚Äôs information measure, which has an
asymptotic justiÔ¨Åcation. An overview of the classical Ô¨Årst-order asymptotic
ML theory was presented in Chapters 3 and 4. The Bayesian counterpart of
this large sample theory consists of using an asymptotic approximation to
the posterior distribution. The most commonly used approximation relies
on computing the posterior mode and the observed information matrix.
Computation of maximum likelihood estimates with the Newton‚ÄìRaphson
or scoring algorithms was dealt with in Chapter 4, but little has been said
so far of how the calculations should proceed in the approximate Bayesian
analysis.
In this chapter, an introductory account is given of one of the most versa-
tile iterative algorithms for computing maximum likelihood and posterior
modes: the expectation‚Äìmaximization, or EM algorithm. This algorithm
is conceptually simple, at least in its basic form, and brings considerable
insight into the statistical structure of a maximum likelihood or posterior
mode problem, contrary to Newton‚ÄìRaphson or scoring, which are based
primarily on numerical considerations. The chapter begins with a deÔ¨Ånition
of the concepts of complete and incomplete data. The subsequent section
presents a derivation of the algorithm in its basic form. Additional sections
of the chapter discuss properties of the algorithm, the special form it takes

444
9. Approximate Inference Via the EM Algorithm
when applied to exponential families, and extensions that have been sug-
gested for recovering measures of uncertainty. The chapter concludes with
a set of examples. Many developments and extensions have become avail-
able since its introduction by Dempster et al. (1977). Several of these can
be found in the comprehensive book of McLachlan and Krishnan (1997).
9.2
Complete and Incomplete Data
The EM algorithm was given its name in a celebrated paper by Dempster
et al. (1977). As mentioned above, this is an iterative method for Ô¨Ånding ML
estimates or posterior modes in what are called incomplete-data problems.
The inÔ¨Çuence of the EM algorithm has been far reaching, not only as a
computational tool but as a way of solving diÔ¨Écult statistical problems. A
main reason for this impact is because it is easy to implement. The basic
idea behind the method is to transform an incomplete- into a complete-
data problem for which the required maximization is computationally more
tractable. Also, the algorithm is numerically stable: each iteration increases
the likelihood or posterior density and convergence is nearly always to a
local maximum.
The concept of missing data is fairly broad. It includes, for example,
missing data in an unbalanced layout, but it extends to observations from
truncated distributions, censored data, and latent variables. In these cases,
one can view the complete data x as consisting of the vectors (y, z), where
y is the observed data or incomplete data, and z is the missing data. More
generally, many statistical problems which at Ô¨Årst glance do not appear to
involve missing data can be reformulated into missing-data problems, by
judicious augmentation of the data set, with unobserved values. As such,
one can view the observations at hand and the parameters of the posed
model as data: part of these data is observed (the records) and another
part is missing (the parameters). Mixed eÔ¨Äects and hierarchical models,
and models with latent variables, such as the threshold model, are typi-
cally amenable to an EM formulation. An example is an additive genetic
model where inference may focus on Œ∏ =

Œ≤‚Ä≤, œÉ2
a, œÉ2
e
‚Ä≤, where Œ≤ is a vector
of location parameters and

œÉ2
a, œÉ2
e

are variance components. Here, one
may augment the observed data y, with the missing data a, the unob-
served vector of additive genetic values. As shown later, this simpliÔ¨Åes the
computations involved in Ô¨Ånding the ML estimates of Œ∏, or the maximum
of p

Œ≤, œÉ2
a, œÉ2
e|y

, or mode of the posterior distribution

Œ≤, œÉ2
a, œÉ2
e|y

. On
the other hand, if one wishes to Ô¨Ånd the mode of the distribution with den-
sity p

œÉ2
a, œÉ2
e|y

, an EM strategy is to consider (Œ≤, a) as the missing data.
Here, if improper priors are adopted for the variance components and for
the location vector Œ≤, the mode is identical to the REML estimates of

œÉ2
a, œÉ2
e

. Another example is a regression model with t-distributed errors;

9.3 The EM Algorithm
445
this can be formulated as a standard weighted least-squares problem where
the missing data are related to the ‚Äúweights‚Äù.
9.3
The EM Algorithm
9.3.1
Form of the Algorithm
Suppose that the objective is to draw inferences about the d √ó 1 vector
Œ∏ ‚àà‚Ñ¶using the mode of [Œ∏|y] as point estimator. We will use p (Œ∏|y) to
denote a posterior density (or a likelihood function if Ô¨Çat priors are adopted
for the parameters) where, as usual, y is the vector of observed data. Let z
represent a vector of missing data, such as missing records or unobserved
‚Äúparameters‚Äù of the model, and let its conditional, p.d.f. be p (z|y, Œ∏). The
marginal posterior density of Œ∏ is
p (Œ∏|y) =

p (Œ∏, z|y) dz,
where p (Œ∏, z|y) is the joint posterior density of Œ∏ and z. The integration
above typically leads to an expression which makes p (Œ∏|y) diÔ¨Écult to max-
imize, even though maximization of p (Œ∏, z|y) ‚àùp (Œ∏|z, y) with respect to
Œ∏ may be trivial if z were observed. The EM algorithm formalizes an old
idea for dealing with missing-data problems. Starting with a guessed value
for the parameter Œ∏, carry out the following iteration:
‚Ä¢ Replace the missing data z by their expectation given the guessed
value of the parameters and the observed data. Let this conditional
expectation be ,z.
‚Ä¢ Maximize p (Œ∏, z|y) with respect to Œ∏ replacing the missing data z by
their expected values. This is equivalent to maximizing p (Œ∏|,z, y).
‚Ä¢ Reestimate the missing values z using their conditional expectation
based on the updated Œ∏.
‚Ä¢ Reestimate Œ∏ and continue until convergence is reached.
9.3.2
Derivation
Consider the identity
p (Œ∏|y) = p (Œ∏, z|y)
p (z|y, Œ∏).
Taking logarithms on both sides leads to
ln p (Œ∏|y) = ln p (Œ∏, z|y) ‚àíln p (z|y, Œ∏) ,
(9.1)

446
9. Approximate Inference Via the EM Algorithm
where the Ô¨Årst term on the right-hand side is known as the complete-
data log-likelihood (more generally, as complete-data log-posterior). Now
take expectations of both sides with respect to

z|Œ∏[t], y

, where Œ∏[t] is the
current guess of Œ∏. The left-hand side of (9.1) does not depend on z, so
averaging over z, providing the integrals exist, gives
ln p (Œ∏|y) =

ln p (Œ∏, z|y) p

z|Œ∏[t], y

dz‚àí

ln p (z|y, Œ∏) p

z|Œ∏[t], y

dz.
(9.2)
The Ô¨Årst term on the right-hand side of (9.2) is a function of Œ∏ for Ô¨Åxed
y and Ô¨Åxed Œ∏[t], and it is denoted as Q

Œ∏|Œ∏[t]
in the EM literature. The
second term is denoted H

Œ∏|Œ∏[t]
. Thus,
ln p (Œ∏|y) = Q

Œ∏|Œ∏[t]
‚àíH

Œ∏|Œ∏[t]
.
(9.3)
The EM algorithm involves working with the Ô¨Årst term only, Q

Œ∏|Œ∏[t]
,
disregarding H

Œ∏|Œ∏[t]
. The two steps are:
1. E-step: calculation of Q

Œ∏|Œ∏[t]
, that is, the expectation of the com-
plete data log-likelihood (log-posterior) with respect to the condi-
tional distribution of the missing data, given the observed data and
the current guess for Œ∏.
2. M-step: maximization of Q

Œ∏|Œ∏[t]
with respect to Œ∏, solving for Œ∏,
and setting the result equal to Œ∏[t+1], the new value of the parameter.
Thus, if Œ∏[t+1] maximizes Q

Œ∏|Œ∏[t]
, the M-step is such that
Q

Œ∏[t+1]|Œ∏[t]
‚â•Q

Œ∏|Œ∏[t]
,
for all Œ∏ ‚àà‚Ñ¶,
(9.4)
which implies that Œ∏[t+1] is a solution to the equation
‚àÇQ

Œ∏|Œ∏[t]
‚àÇŒ∏
= 0.
(9.5)
The two steps are repeated iteratively until convergence is reached. It is
shown below that this iterative sequence leads to a monotonic increase of
ln p (Œ∏|y). That is,
ln p

Œ∏[t+1]|y

‚©æln p

Œ∏[t]|y

.
(9.6)
Since the marginal posterior density increases in each step, the EM algo-
rithm, with few exceptions, converges to a local mode.

9.4 Monotonic Increase of ln p (Œ∏|y)
447
In many important applications, the E-step involves replacing the miss-
ing data by their conditional expectations. In some cases, computation
of the E-step as formally dictated by step 1 above, can more easily dis-
close pathologies rendering the EM algorithm inapplicable. For instance,
see Flury and Zoppe (2000) for a case where the EM is not applicable
because the log-likelihood function takes the value zero in a subset of the
parameter space, so the relevant conditional expectation is not deÔ¨Åned.
In some models the calculation of Q

Œ∏|Œ∏[t]
in the E-step may be dif-
Ô¨Åcult. Wei and Tanner (1990) propose a Monte Carlo approach for overcom-
ing this diÔ¨Éculty. This consists of simulating z1, z2, . . . , zm from p

z|Œ∏[t], y

and then forming the simulation consistent estimator
5Q

Œ∏|Œ∏[t]
‚âà1
m
m

i=1
ln p (Œ∏, zi|y) .
9.4
Monotonic Increase of ln p (Œ∏|y)
with Each EM Iteration
Consider a sequence of iterates Œ∏[0], Œ∏[1], . . . , Œ∏[t+1]. The diÔ¨Äerence in value
of ln p (Œ∏|y) in successive iterates is obtained from (9.3) as
ln p

Œ∏[t+1]|y

‚àíln p

Œ∏[t]|y

= Q

Œ∏[t+1]|Œ∏[t]
‚àíQ

Œ∏[t]|Œ∏[t]
‚àí

H

Œ∏[t+1]|Œ∏[t]
‚àíH

Œ∏[t]|Œ∏[t]
.
(9.7)
The diÔ¨Äerence between Q (¬∑) functions on the right-hand side is nonnegative
due to (9.4). Therefore (9.6) holds if the diÔ¨Äerence in H (¬∑) above is non-
positive; that is, if
H

Œ∏[t+1]|Œ∏[t]
‚àíH

Œ∏[t]|Œ∏[t]
‚â§0.
(9.8)
Now for any Œ∏,
H

Œ∏|Œ∏[t]
‚àíH

Œ∏[t]|Œ∏[t]
=

ln p (z|y, Œ∏) p

z|Œ∏[t], y

dz
‚àí

ln p

z|y, Œ∏[t]
p

z|Œ∏[t], y

dz
=

ln
Ô£Æ
Ô£∞p (z|y, Œ∏)
p

z|y, Œ∏[t]
Ô£π
Ô£ªp

z|Œ∏[t], y

dz
= ‚àí

ln
Ô£Æ
Ô£∞
p

z|y, Œ∏[t]
p (z|y, Œ∏)
Ô£π
Ô£ªp

z|Œ∏[t], y

dz.
(9.9)

448
9. Approximate Inference Via the EM Algorithm
The integral is the Kullback‚ÄìLeibler distance between the distributions
with densities p

z|y, Œ∏[t]
and p (z|y, Œ∏) , which is at least 0. Hence, this
integral preceded by a negative sign is at most 0. Thus, H

Œ∏|Œ∏[t]
‚â§
H

Œ∏[t]|Œ∏[t]
. This establishes (9.8) and, hence, inequality (9.6). In well
behaved problems the sequence of EM iterates converges to a stationary
point which is a global maximum, in which case EM yields the unique
posterior mode or ML estimate of Œ∏, the maximizer of ln p (Œ∏|y).
Since
H

Œ∏|Œ∏[t]
‚â§H

Œ∏[t]|Œ∏[t]
for all Œ∏, this implies that H

Œ∏|Œ∏[t]
has a maximum at Œ∏ = Œ∏[t]. Hence,
‚àÇH

Œ∏|Œ∏[t]
‚àÇŒ∏
""""""
Œ∏=Œ∏[t]
=
‚àÇH

Œ∏[t]|Œ∏[t]
‚àÇŒ∏
= 0.
(9.10)
Therefore from (9.3),
‚àÇln p (Œ∏|y)
‚àÇŒ∏
""""
Œ∏=Œ∏[t] =
‚àÇQ

Œ∏|Œ∏[t]
‚àÇŒ∏
""""""
Œ∏=Œ∏[t]
.
(9.11)
9.5
The Missing Information Principle
9.5.1
Complete, Observed and Missing Information
Recall the identity
ln p (Œ∏|y) = ln p (Œ∏, z|y) ‚àíln p (z|y, Œ∏) .
DiÔ¨Äerentiating twice with respect to Œ∏ and multiplying by ‚àí1 gives
‚àí‚àÇ2 ln p (Œ∏|y)
‚àÇŒ∏ ‚àÇŒ∏‚Ä≤
= ‚àí‚àÇ2 ln p (Œ∏, z|y)
‚àÇŒ∏ ‚àÇŒ∏‚Ä≤
+ ‚àÇ2 ln p (z|y, Œ∏)
‚àÇŒ∏ ‚àÇŒ∏‚Ä≤
.
(9.12)
Note that the left-hand side is not a function of z. Now taking expectations
of both sides over the distribution [z|y, Œ∏] gives
‚àí‚àÇ2 ln p (Œ∏|y)
‚àÇŒ∏ ‚àÇŒ∏‚Ä≤
= ‚àí
 ‚àÇ2 ln p (Œ∏, z|y)
‚àÇŒ∏ ‚àÇŒ∏‚Ä≤
p (z|y, Œ∏) dz
+
 ‚àÇ2 ln p (z|y, Œ∏)
‚àÇŒ∏ ‚àÇŒ∏‚Ä≤
p (z|y, Œ∏) dz.
(9.13)

9.5 The Missing Information Principle
449
Recalling the deÔ¨Ånition of the Q and H functions given in (9.2), and pro-
vided that the integral and diÔ¨Äerential operations are interchangeable, one
arrives at
‚àí‚àÇ2 ln p (Œ∏|y)
‚àÇŒ∏ ‚àÇŒ∏‚Ä≤
= ‚àí‚àÇ2Q (Œ∏|Œ∏)
‚àÇŒ∏ ‚àÇŒ∏‚Ä≤
‚àí

‚àí‚àÇ2H (Œ∏|Œ∏)
‚àÇŒ∏ ‚àÇŒ∏‚Ä≤

.
(9.14)
If one calls the Ô¨Årst and second terms on the right-hand side as ‚Äúcomplete‚Äù
and ‚Äúmissing‚Äù information, respectively, (9.14) has the following interpre-
tation (Louis, 1982):
Observed information = complete information ‚àímissing information.
This can be represented as
I (Œ∏|y) = Ic (Œ∏|y) ‚àíIm (Œ∏|y) .
(9.15)
The rate of convergence of the EM algorithm is related to these matrix-
valued quantities: the larger the proportion of missing information (relative
to the complete information), the slower the rate of convergence. This can
be represented in the following manner (a formal justiÔ¨Åcation for the ex-
pression is given later on):
Œ∏[t+1] ‚àíŒ∏‚àó= [Ic (Œ∏‚àó|y)]‚àí1 Im (Œ∏‚àó|y)

Œ∏[t] ‚àíŒ∏‚àó
.
(9.16)
The preceding indicates that as the EM iteration proceeds, the distance
between the iterates and Œ∏‚àó(a stationary point of the likelihood func-
tion or posterior density) is a function of the rate of convergence matrix
[Ic (Œ∏‚àó|y)]‚àí1 Im (Œ∏‚àó|y). In short, the larger the proportion of missing in-
formation, the slower the algorithm proceeds towards Œ∏‚àó, and the form of
(9.16) suggests a linear approach towards Œ∏‚àó. Note from (9.15) that the
rate of convergence matrix can be written also as
[Ic (Œ∏‚àó|y)]‚àí1 Im (Œ∏‚àó|y) = Id ‚àí[Ic (Œ∏‚àó|y)]‚àí1 I (Œ∏‚àó|y) ,
(9.17)
where Id is the identity matrix of dimension d, the number of elements in
Œ∏.
9.5.2
Rate of Convergence of the EM Algorithm
A formal derivation of (9.16) is presented here. Following McLachlan and
Krishnan (1997), consider a Taylor series expansion of the score vector
‚àÇln p (Œ∏|y)
‚àÇŒ∏
about the point Œ∏ = Œ∏[t]. This yields
‚àÇln p (Œ∏|y)
‚àÇŒ∏
‚âà
‚àÇln p

Œ∏[t]|y

‚àÇŒ∏
‚àíI

Œ∏[t]|y
 
Œ∏ ‚àíŒ∏[t]
.

450
9. Approximate Inference Via the EM Algorithm
Setting Œ∏ = Œ∏‚àó, the term on the left-hand side vanishes and one obtains
after rearrangement
Œ∏‚àó‚âàŒ∏[t] +

I

Œ∏[t]|y
‚àí1 ‚àÇln p

Œ∏[t]|y

‚àÇŒ∏
.
(9.18)
Now expand ‚àÇQ

Œ∏|Œ∏[t]
/‚àÇŒ∏
"""
Œ∏=Œ∏[t+1] in a linear Taylor series about Œ∏ = Œ∏[t]:
‚àÇQ

Œ∏[t+1]|Œ∏[t]
‚àÇŒ∏
‚âà
‚àÇQ

Œ∏[t]|Œ∏[t]
‚àÇŒ∏
+
‚àÇ2Q

Œ∏[t]|Œ∏[t]
‚àÇŒ∏‚àÇŒ∏‚Ä≤

Œ∏[t+1] ‚àíŒ∏[t]
.
From (9.5), the term on the left-hand side is equal to zero. Making use of
this in the preceding expression and employing the notation in (9.15), this
can be written as
‚àÇQ

Œ∏[t]|Œ∏[t]
‚àÇŒ∏
‚âàIc

Œ∏[t]|y
 
Œ∏[t+1] ‚àíŒ∏[t]
which, from (9.11), is
‚àÇln p

Œ∏[t]|y

‚àÇŒ∏
‚âàIc

Œ∏[t]|y
 
Œ∏[t+1] ‚àíŒ∏[t]
.
(9.19)
Substituting approximation (9.19) into (9.18) yields
Œ∏‚àó‚àíŒ∏[t]
‚âà

I

Œ∏[t]|y
‚àí1
Ic

Œ∏[t]|y
 
Œ∏[t+1] ‚àíŒ∏[t]
=

I

Œ∏[t]|y
‚àí1
Ic

Œ∏[t]|y
 
Œ∏[t+1] ‚àíŒ∏‚àó+ Œ∏‚àó‚àíŒ∏[t]
.
Therefore,

Id ‚àí

I

Œ∏[t]|y
‚àí1
Ic

Œ∏[t]|y
% 
Œ∏‚àó‚àíŒ∏[t]
‚âà

I

Œ∏[t]|y
‚àí1
Ic

Œ∏[t]|y
 
Œ∏[t+1] ‚àíŒ∏‚àó
.
Premultiplying both sides by

Ic

Œ∏[t]|y
‚àí1
I

Œ∏[t]|y

yields
Œ∏[t+1] ‚àíŒ∏‚àó‚âà

Id ‚àí

Ic

Œ∏[t]|y
‚àí1
I

Œ∏[t]|y
% 
Œ∏[t] ‚àíŒ∏‚àó
.
In view of (9.17), this is expressible as
Œ∏[t+1] ‚àíŒ∏‚àó‚âà

Ic

Œ∏[t]|y
‚àí1
Im

Œ∏[t]|y
 
Œ∏[t] ‚àíŒ∏‚àó
,
and for Œ∏[t] close to Œ∏‚àóthis leads to the desired result (9.16) directly.

9.6 EM Theory for Exponential Families
451
9.6
EM Theory for Exponential Families
The EM algorithm has a simpler form when the complete data x = (y‚Ä≤, z‚Ä≤)‚Ä≤
have a distribution from the regular exponential family. The density can
be written in its canonical form as
p (x|Œ∏) = b (x) exp

Œ∏‚Ä≤t (x)

a (Œ∏)
.
(9.20)
In this expression, the vector Œ∏ is the natural or canonical parameter in-
dexing the distribution, b (x) is a function of the complete data alone, a (Œ∏)
is a function of the vector Œ∏ alone, and t (x) is the d√ó1 vector of complete-
data suÔ¨Écient statistics. Many common distributions can be put in the
form (9.20), which is characterized by a number of nice statistical proper-
ties. In an exponential family, the statistic t (x) carries all the information
about Œ∏ contained in the data x; therefore, inferences about Œ∏ can be based
solely on t (x). In a Bayesian context this means that the posterior distri-
bution of Œ∏ given x is identical to the posterior distribution [Œ∏|y] . The ML
or posterior mode equations (the score) take a particularly simple form. To
see this, take logarithms on both sides of (9.20):
ln p (x|Œ∏) = ln b (x) + Œ∏‚Ä≤t (x) ‚àíln a (Œ∏) .
(9.21)
In the context of ML estimation, the score is
‚àÇln p (x|Œ∏)
‚àÇŒ∏
= t (x) ‚àí‚àÇln a (Œ∏)
‚àÇŒ∏
.
(9.22)
Taking expectations of both sides over the sampling model for the complete
data with density p (x|Œ∏), and recalling from ML theory that
E
‚àÇln p (x|Œ∏)
‚àÇŒ∏

= 0
leads to
1
a (Œ∏)
‚àÇa (Œ∏)
‚àÇŒ∏
=

t (x) p (x|Œ∏) dx
=
E [t (x|Œ∏)] .
(9.23)
This is the expected value of the vector of suÔ¨Écient statistics, given Œ∏.
Now from (9.22), and in the context of likelihood-based inference, the ML
estimator of Œ∏ is the solution to the equations
‚àÇln p (x|Œ∏)
‚àÇŒ∏
= t (x) ‚àíE [t (x|Œ∏)] = 0,
or, equivalently,
t (x) = E [t (x|Œ∏)] .
(9.24)

452
9. Approximate Inference Via the EM Algorithm
If equations (9.24) can be solved for Œ∏, then the solution is unique due to
the convexity property of the log-likelihood for regular exponential fami-
lies. When the equations are not solvable, the maximizer of Œ∏ lies on the
boundary of the parameter space (McLachlan and Krishnan, 1997).
The regular exponential family of distributions also leads to a simple rep-
resentation of the complete-data information matrix. Note from (9.21), that
the second derivatives of the complete-data log-likelihood do not depend
on x, and that
‚àí‚àÇ2 ln p (x|Œ∏)
‚àÇŒ∏ ‚àÇŒ∏‚Ä≤
= ‚àÇ2 ln a (Œ∏)
‚àÇŒ∏ ‚àÇŒ∏‚Ä≤
= Ic (Œ∏) ,
(9.25)
where Ic (Œ∏) is Fisher‚Äôs expected information matrix.
Return now to the computation of EM to obtain the ML estimator of Œ∏
when the complete data can be written in the form of (9.20). The E-step
is given by
Q

Œ∏|Œ∏[t]
=

ln p (y, z|Œ∏) p

z|Œ∏[t], y

dz
=

ln b (x) p

z|Œ∏[t], y

dz + Œ∏‚Ä≤

t (x) p

z|Œ∏[t], y

dz ‚àíln a (Œ∏) .
(9.26)
The M-step consists of diÔ¨Äerentiating this expression with respect to Œ∏.
Since the Ô¨Årst term does not depend on Œ∏,
‚àÇQ

Œ∏|Œ∏[t]
‚àÇŒ∏
=

t (x) p

z|Œ∏[t], y

dz‚àí
1
a (Œ∏)
‚àÇa (Œ∏)
‚àÇŒ∏
= E

t (x) |Œ∏[t], y

‚àíE [t (x|Œ∏)] ,
(9.27)
where the Ô¨Årst term on the right-hand side is the conditional expected value
of the vector of suÔ¨Écient statistics, given the observed data and the current
value of Œ∏. It follows from (9.27) that in the M-step, Œ∏[t+1] is chosen by
solving the equations
E

t (x) |Œ∏[t], y

= E [t (x|Œ∏)] ,
(9.28)
so the form of the algorithm is quite simple.
9.7
Standard Errors and Posterior
Standard Deviations
One of the early criticisms of the EM approach was that, unlike the Newton‚Äì
Raphson and related methods, it does not automatically produce an esti-
mate of the asymptotic covariance matrix of the ML estimators of Œ∏ or some

9.7 Standard Errors and Posterior Standard Deviations
453
indication of the uncertainty in the posterior distribution of a parameter
of interest. A number of ways of computing estimates of the asymptotic
covariance matrix of the ML estimates have been suggested over the last
few years. Important contributions include those of Louis (1982), Meilijson
(1989), Meng and Rubin (1991), Lange (1995), and Oakes (1999). All meth-
ods make use of asymptotic theory. Three of these approaches will be de-
scribed here.
9.7.1
The Method of Louis
Louis (1982) showed that
‚àÇ2 ln p (Œ∏|y)
‚àÇŒ∏ ‚àÇŒ∏‚Ä≤
= Ez|Œ∏,y
‚àÇ2 ln p (Œ∏, z|y)
‚àÇŒ∏ ‚àÇŒ∏‚Ä≤

+ V arz|Œ∏,y
‚àÇln p (Œ∏, z|y)
‚àÇŒ∏

,
(9.29)
which, on multiplication by ‚àí1, yields (9.15). Thus,
Ic (Œ∏|y) = Ez|Œ∏,y

‚àí‚àÇ2 ln p (Œ∏, z|y)
‚àÇŒ∏‚àÇŒ∏‚Ä≤

(9.30)
and
Im (Œ∏|y) = V arz|Œ∏,y
‚àÇln p (Œ∏, z|y)
‚àÇŒ∏

.
(9.31)
To prove (9.29), Ô¨Årst note that
‚àÇln p (Œ∏|y)
‚àÇŒ∏
=
1
p (Œ∏|y)
‚àÇ
‚àÇŒ∏

p (Œ∏, z|y) dz
=
1
p (Œ∏|y)
 ‚àÇln p (Œ∏, z|y)
‚àÇŒ∏
p (Œ∏, z|y) dz
=
 ‚àÇln p (Œ∏, z|y)
‚àÇŒ∏
p (z|Œ∏, y) dz
= Ez|Œ∏,y
‚àÇln p (Œ∏, z|y)
‚àÇŒ∏

.
(9.32)
Now, take derivatives with respect to Œ∏ again
‚àÇ
‚àÇŒ∏
‚àÇln p (Œ∏|y)
‚àÇŒ∏‚Ä≤

= ‚àÇ
‚àÇŒ∏
 ‚àÇln p (Œ∏, z|y)
‚àÇŒ∏
p (z|Œ∏, y) dz

=
 ‚àÇ2 ln p (Œ∏, z|y)
‚àÇŒ∏ ‚àÇŒ∏‚Ä≤
p (z|Œ∏, y) dz+
 ‚àÇln p (Œ∏, z|y)
‚àÇŒ∏
‚àÇp (z|Œ∏, y)
‚àÇŒ∏‚Ä≤
dz
= Ez|Œ∏,y
‚àÇ2 ln p (Œ∏, z|y)
‚àÇŒ∏‚àÇŒ∏‚Ä≤

+
 ‚àÇln p (Œ∏, z|y)
‚àÇŒ∏
‚àÇln p (z|Œ∏, y)
‚àÇŒ∏‚Ä≤
p (z|Œ∏, y) dz.
(9.33)

454
9. Approximate Inference Via the EM Algorithm
The second term in (9.33) can be manipulated as follows
 ‚àÇln p (Œ∏, z|y)
‚àÇŒ∏
‚àÇln p (z|Œ∏, y)
‚àÇŒ∏‚Ä≤
p (z|Œ∏, y) dz
=
 ‚àÇln p (Œ∏, z|y)
‚àÇŒ∏
‚àÇln p (Œ∏, z|y)
‚àÇŒ∏‚Ä≤
‚àí‚àÇln p (Œ∏|y)
‚àÇŒ∏‚Ä≤

p (z|Œ∏, y) dz
=
 ‚àÇln p (Œ∏, z|y)
‚àÇŒ∏
‚àÇln p (Œ∏, z|y)
‚àÇŒ∏‚Ä≤
p (z|Œ∏, y) dz
‚àí
 ‚àÇln p (Œ∏, z|y)
‚àÇŒ∏
‚àÇln p (Œ∏|y)
‚àÇŒ∏‚Ä≤
p (z|Œ∏, y) dz
= Ez|Œ∏,y
‚àÇln p (Œ∏, z|y)
‚àÇŒ∏
‚àÇln p (Œ∏, z|y)
‚àÇŒ∏‚Ä≤

‚àí
 ‚àÇln p (Œ∏, z|y)
‚àÇŒ∏
‚àÇln p (Œ∏|y)
‚àÇŒ∏‚Ä≤
p (z|Œ∏, y) dz.
In view of (9.32), the last two lines of the preceding expression can be
written as
Ez|Œ∏,y
‚àÇln p (Œ∏, z|y)
‚àÇŒ∏
‚àÇln p (Œ∏, z|y)
‚àÇŒ∏‚Ä≤

‚àí
 ‚àÇln p (Œ∏, z|y)
‚àÇŒ∏
Ez|Œ∏,y
‚àÇln p (Œ∏, z|y)
‚àÇŒ∏‚Ä≤

p (z|Œ∏, y) dz
= Ez|Œ∏,y
‚àÇln p (Œ∏, z|y)
‚àÇŒ∏
‚àÇln p (Œ∏, z|y)
‚àÇŒ∏‚Ä≤

‚àíEz|Œ∏,y
‚àÇln p (Œ∏, z|y)
‚àÇŒ∏

Ez|Œ∏,y
‚àÇln p (Œ∏, z|y)
‚àÇŒ∏‚Ä≤

= V arz|Œ∏,y
‚àÇln p (Œ∏, z|y)
‚àÇŒ∏

.
Hence, (9.33) becomes
‚àÇ2 ln p (Œ∏|y)
‚àÇŒ∏‚àÇŒ∏‚Ä≤
= Ez|Œ∏,y
‚àÇ2 ln p (Œ∏, z|y)
‚àÇŒ∏‚àÇŒ∏‚Ä≤

+ V arz|Œ∏,y
‚àÇln p (Œ∏, z|y)
‚àÇŒ∏

,
thus establishing (9.29), and the matrix of second derivatives from which
a measure of uncertainty can be derived. Tanner (1996) suggests a Monte
Carlo approximation to (9.29) that can be used when the integrals over
p (z|Œ∏, y) are diÔ¨Écult to obtain analytically.
9.7.2
Supplemented EM Algorithm (SEM)
This method was proposed by Meng and Rubin (1991) who showed that
the asymptotic covariance matrix of the ML of Œ∏ (evaluated at 5Œ∏, the ML

9.7 Standard Errors and Posterior Standard Deviations
455
estimator) is equal to

I

5Œ∏|y
‚àí1
=

Ic

5Œ∏|y
‚àí1
+

Id ‚àí‚àÜ

5Œ∏|y
‚àí1
‚àÜ

5Œ∏|y
 
Ic

5Œ∏|y
‚àí1
.
(9.34)
Here, the term ‚àÜ

5Œ∏|y

=

Ic

5Œ∏|y
‚àí1
Im

5Œ∏|y

is the rate of conver-
gence matrix and, as before, Id is an identity matrix of dimension d √ó d.
The Ô¨Årst term on the right-hand side of (9.34) is the asymptotic covariance
matrix based on the complete-data log-likelihood and averaged over the
distribution

z|5Œ∏, y


Ic

5Œ∏|y
‚àí1
=

E

‚àí‚àÇ2 ln p (Œ∏, z|y)
‚àÇŒ∏ ‚àÇŒ∏‚Ä≤
"""" Œ∏, y
""""
Œ∏=Œ∏
‚àí1
=

‚àí
‚àÇ2
‚àÇŒ∏ ‚àÇŒ∏‚Ä≤

ln p (Œ∏, z|y) p

z|5Œ∏, y

dz
""""
Œ∏=Œ∏
‚àí1
=
Ô£Æ
Ô£∞‚àí
‚àÇ2Q

Œ∏|5Œ∏

‚àÇŒ∏ ‚àÇŒ∏‚Ä≤
""""""
Œ∏=Œ∏
Ô£π
Ô£ª
‚àí1
.
Often, this can be computed analytically, and it is simple to calculate when
the density of the complete-data distribution is in the exponential family
form (9.20), as indicated in (9.25).
The derivation of (9.34) is as follows. From (9.15),
I

5Œ∏|y

=
Ic

5Œ∏|y
 
Id ‚àí

Ic

5Œ∏|y
‚àí1
Im

5Œ∏|y

=
Ic

5Œ∏|y
 
Id ‚àí‚àÜ

5Œ∏|y

.
Inverting this expression establishes (9.34)

I

5Œ∏|y
‚àí1
=

Id ‚àí‚àÜ

5Œ∏|y
‚àí1 
Ic

5Œ∏|y
‚àí1
=

Id +

Id ‚àí‚àÜ

5Œ∏|y
‚àí1
‚àÜ

5Œ∏|y
% 
Ic

5Œ∏|y
‚àí1
=

Ic

5Œ∏|y
‚àí1
+

Id ‚àí‚àÜ

5Œ∏|y
‚àí1
‚àÜ

5Œ∏|y
 
Ic

5Œ∏|y
‚àí1
.
The second line arises from the matrix algebra result

A ‚àíBD‚àí1C
‚àí1 = A‚àí1 + A‚àí1B

D ‚àíCA‚àí1B
‚àí1 CA‚àí1,
after setting A = B = D = Id and C = ‚àÜ

5Œ∏|y

.

456
9. Approximate Inference Via the EM Algorithm
In order to compute the rate of convergence matrix, ‚àÜ

5Œ∏|y

, Meng and
Rubin (1991) suggest the following approach:
‚Ä¢ Run the EM algorithm until convergence, and Ô¨Ånd
5Œ∏ =

5Œ∏1,5Œ∏2, . . . ,5Œ∏d
‚Ä≤
,
the maximizer of ln p (Œ∏|y).
‚Ä¢ Choose a starting point for Œ∏ diÔ¨Äerent from 5Œ∏ in all components. A
possible starting point is the one used for the original EM calculation.
Run the EM algorithm through t iterations.
1. INPUT: Œ∏[t] and 5Œ∏. Run the usual E- and M-steps to obtain
Œ∏[t+1]. Repeat the following steps (a) and (b) for i = 1, 2, . . . , d,
to obtain a matrix R[t] with element r[t]
ij deÔ¨Åned below.
(a) Construct
Œ∏[t] (i) =

5Œ∏1,5Œ∏2, . . . ,5Œ∏i‚àí1, Œ∏[t]
i ,5Œ∏i+1, . . . ,5Œ∏d

.
(b) Use Œ∏[t] (i) as the input value for one EM step to obtain
Œ∏[t+1] (i) with elements Œ∏[t+1]
j
(i), (j = 1, 2, . . . , d). The ith
row of R[t] is obtained as
r[t]
ij =
Œ∏[t+1]
j
(i) ‚àí5Œ∏j
Œ∏[t]
i ‚àí5Œ∏i
,
j = 1, 2, . . . , d.
(9.35)
2. OUTPUT: Œ∏[t+1] and
9
r[t]
ij ,
(i, j = 1, 2, . . . , d)
:
. Set t = t + 1
and GO TO 1.
When the value of an element rij no longer changes between successive
iterates, this represents a numerical estimate of the corresponding element
in ‚àÜ

5Œ∏|y

. It is possible that diÔ¨Äerent values of t may be required for
diÔ¨Äerent rij components. Meng and Rubin (1991) discuss many relevant
implementation issues. The reader is referred to their paper for details. The
numerical estimate of ‚àÜ

5Œ∏|y

and the expression derived analytically for
Ic

5Œ∏|y

are then used in expression (9.34) to obtain the desired asymptotic
covariance matrix of 5Œ∏.

9.7 Standard Errors and Posterior Standard Deviations
457
9.7.3
The Method of Oakes
Oakes (1999) provided an expression for the observed information matrix
based on the second derivatives of Q

Œ∏|Œ∏[t]
, the conditional expectation
of the complete-data log-likelihood given the observed data and a Ô¨Åxed
value of Œ∏ = Œ∏[t]. Oakes‚Äô formula is
I (Œ∏|y)
=
‚àí‚àÇ2 ln p (Œ∏|y)
‚àÇŒ∏ ‚àÇŒ∏‚Ä≤
=
‚àí
Ô£Æ
Ô£∞
Ô£´
Ô£≠
‚àÇ2Q

Œ∏|Œ∏[t]
‚àÇŒ∏ ‚àÇŒ∏‚Ä≤
+
‚àÇ2Q

Œ∏|Œ∏[t]
‚àÇŒ∏ ‚àÇŒ∏‚Ä≤[t]
Ô£∂
Ô£∏
""""""
Œ∏=Œ∏[t]
Ô£π
Ô£ª.
(9.36)
The second term on the right-hand side corresponds to the information
from the missing data, and is equal to Im (Œ∏|y) evaluated at Œ∏ = Œ∏[t].
In order to derive (9.36), results (3.20) and (3.26) from Chapter 3 will
be used. These results are valid for any Œ∏, and therefore

‚àÇ
‚àÇŒ∏[t] ln p

z|y, Œ∏[t]
p

z|y, Œ∏[t]
dz
= Ez|y,Œ∏[t]

‚àÇ
‚àÇŒ∏[t] ln p

z|y, Œ∏[t]
= 0,
(9.37)
and
Ez|y,Œ∏[t]

‚àÇ
‚àÇŒ∏[t] ln p

z|y, Œ∏[t] 
‚àÇ
‚àÇŒ∏[t] ln p

z|y, Œ∏[t]‚Ä≤
= ‚àíEz|y,Œ∏[t]

‚àÇ2
‚àÇŒ∏[t]‚àÇŒ∏‚Ä≤[t] ln p

z|y, Œ∏[t]
.
(9.38)
In the development that follows, both Œ∏ and Œ∏[t] are regarded as argu-
ments of Q. To prove (9.36), Ô¨Årst diÔ¨Äerentiate (9.3) with respect to Œ∏; this
gives
‚àÇln p (Œ∏|y)
‚àÇŒ∏
=
‚àÇQ

Œ∏|Œ∏[t]
‚àÇŒ∏
‚àí
 ‚àÇln p (z|y, Œ∏)
‚àÇŒ∏
p

z|y, Œ∏[t]
dz.
(9.39)
Setting Œ∏ = Œ∏[t], the second term vanishes because of (9.37), and the score
for the observed data becomes
‚àÇln p (Œ∏|y)
‚àÇŒ∏
=
‚àÇQ

Œ∏|Œ∏[t]
‚àÇŒ∏
""""""
Œ∏=Œ∏[t]
.
(9.40)
Take now partial derivatives of (9.39) with respect to Œ∏ and Œ∏[t]. First
note that the left hand side of (9.39) is not a function of Œ∏[t], and that one

458
9. Approximate Inference Via the EM Algorithm
may write
‚àÇln p (z|y, Œ∏)
‚àÇŒ∏
Ô£´
Ô£≠
‚àÇp

z|y, Œ∏[t]
‚àÇŒ∏[t]
Ô£∂
Ô£∏
‚Ä≤
= ‚àÇln p (z|y, Œ∏)
‚àÇŒ∏
Ô£´
Ô£≠
‚àÇln p

z|y, Œ∏[t]
‚àÇŒ∏[t]
Ô£∂
Ô£∏
‚Ä≤
p

z|y, Œ∏[t]
.
Then diÔ¨Äerentiating (9.39 Ô¨Årst with respect to Œ∏ yields
‚àÇ2 ln p (Œ∏|y)
‚àÇŒ∏ ‚àÇŒ∏‚Ä≤
=
‚àÇ2Q

Œ∏|Œ∏[t]
‚àÇŒ∏ ‚àÇŒ∏‚Ä≤
‚àí
 ‚àÇ2 ln p (z|y, Œ∏)
‚àÇŒ∏ ‚àÇŒ∏‚Ä≤
p

z|y, Œ∏[t]
dz
=
‚àÇ2Q

Œ∏|Œ∏[t]
‚àÇŒ∏ ‚àÇŒ∏‚Ä≤
‚àíEz|y,Œ∏[t]
‚àÇ2 ln p (z|y, Œ∏)
‚àÇŒ∏ ‚àÇŒ∏‚Ä≤

,
and, second, with respect to Œ∏[t], yields
0 =
‚àÇ2Q

Œ∏|Œ∏[t]
‚àÇŒ∏‚àÇŒ∏‚Ä≤[t]
‚àí
 ‚àÇln p (z|y, Œ∏)
‚àÇŒ∏
Ô£´
Ô£≠
‚àÇp

z|y, Œ∏[t]
‚àÇŒ∏[t]
Ô£∂
Ô£∏
‚Ä≤
dz
=
‚àÇ2Q

Œ∏|Œ∏[t]
‚àÇŒ∏‚àÇŒ∏‚Ä≤[t]
‚àíEz|y,Œ∏[t]

 ‚àÇ
‚àÇŒ∏ ln p

z|y, Œ∏[t] 
‚àÇ
‚àÇŒ∏[t] ln p

z|y, Œ∏[t]‚Ä≤
.
Substituting Œ∏ = Œ∏[t], adding, using identity (9.38), and multiplying both
sides by ‚àí1, retrieves (9.36).
As Oakes (1999) points out, (9.36) together with (9.40), implies that
the function Q

Œ∏|Œ∏[t]
can be used to perform standard Newton‚ÄìRaphson
maximization of the observed data likelihood.
9.8
Examples
Illustrations of the EM-related computations are presented Ô¨Årst with two
examples involving discrete data and a multinomial sampling model. The
third and fourth examples deal with inferences about variance components
in a Gaussian hierarchical model.
Example 9.1
A multinomial example
Example 4.8 from Chapter 4 illustrated estimation of the recombination

9.8 Examples
459
Class
Genotypic class
Frequency
1
AA/BB
Œ±2 /4
2
Aa/Bb
(Œ±2 + (1 ‚àíŒ±)2) /2
3
AA/Bb
Œ± (1 ‚àíŒ±) /2
4
Aa/BB
Œ± (1 ‚àíŒ±) /2
5
aa/bb
Œ±2 /4
6
Aa/bb
Œ± (1 ‚àíŒ±) /2
7
aa/Bb
Œ± (1 ‚àíŒ±) /2
8
AA/bb
(1 ‚àíŒ±)2 /4
9
aa/BB
(1 ‚àíŒ±)2 /4
TABLE 9.1. Genotypic classes and frequencies from a mating between repulsion
heterozygotes.
fraction Œ± between loci A and B using coupling heterozygotes. Here, Œ± is
estimated with data from repulsion heterozygotes. The gametes produced
are AB, ab, Ab, and aB with respective frequencies Œ±/2, Œ±/2, (1 ‚àíŒ±) /2,
and (1 ‚àíŒ±) /2. Random union of these gametes produces the genotypic
classes shown in Table 9.1.
Assuming complete dominance at both loci, the phenotypic classes which
are distinguishable (the observed data) are shown in Table 9.2, together
with the number of observations for each phenotype and their expected
frequencies. For example, the frequency of phenotype AB is obtained sum-
ming genotypic classes 1, 2, 3, and 4.
Maximum Likelihood Estimation
Denoting Œ∏ = Œ±2 and n = (n1, n2, n3, n4)‚Ä≤, the observed data likelihood is
in the form of a multinomial distribution
p (Œ∏|n) ‚àù

1
2 + 1
4Œ∏
n1 
1
4 ‚àí1
4Œ∏
n3 
1
4 ‚àí1
4Œ∏
n4 
1
4Œ∏
n2
and the observed data log-likelihood, excluding an additive constant, is
ln p (Œ∏|n) = n1 ln (2 + Œ∏) + (n3 + n4) ln (1 ‚àíŒ∏) + n2 ln Œ∏.
The score is equal to
‚àÇln p (Œ∏|n)
‚àÇŒ∏
=
n1
2 + Œ∏ ‚àín3 + n4
1 ‚àíŒ∏
+ n2
Œ∏ .
Phenotype
Frequency
Number observed
AB
1/2 + Œ±2/4
n1
ab
Œ±2/4
n2
aB
1/4 ‚àíŒ±2/4
n3
Ab
1/4 ‚àíŒ±2/4
n4
TABLE 9.2. Distribution of phenotypic classes with complete dominance.

460
9. Approximate Inference Via the EM Algorithm
The observed information is
‚àí‚àÇ2 ln p (Œ∏|n)
‚àÇŒ∏2
=
n1
(2 + Œ∏)2 + n3 + n4
(1 ‚àíŒ∏)2 + n2
Œ∏2 .
(9.41)
Upon setting the score to 0, one obtains a quadratic equation in Œ∏ with
one positive root, which is the ML estimator of Œ∏. For example, for n =
(20, 12, 120, 117)‚Ä≤, one obtains 5Œ∏ = 0.050056 and 5Œ± =
>
5Œ∏ = 0.22. The
observed information evaluated at 5Œ∏ is
I

5Œ∏|n

= 5056.6997.
(9.42)
The estimate of the asymptotic variance of 5Œ∏ based on the observed infor-
mation is:

V ar

5Œ∏

=

‚àí‚àÇ2 ln p (Œ∏|n)
‚àÇŒ∏2
""""
Œ∏=Œ∏
‚àí1
= 0.000198,
(9.43)
and the estimate of the asymptotic variance of 5Œ± is

V ar (5Œ±) =

V ar

5Œ∏
 
dŒ±
dŒ∏
2"""""
Œ∏=Œ∏

= 0.000986.
Note that the transformation Œ∏ = Œ±2 is not one-to-one; therefore the ex-
pression above for

V ar (5Œ±) can be misleading. However, it can be veriÔ¨Åed
that in this case, parameterizing the likelihood in terms of Œ± rather than
in terms of Œ∏, yields and estimate of the asymptotic V ar (5Œ±) based on the
inverse of the observed information equal to 0.000988.
Computation via the EM Algorithm
The ML estimator of Œ∏ is obtained now using the EM algorithm. The choice
of missing data must be based on the form of the resulting complete-data
likelihood. Suppose that we split the Ô¨Årst of the four multinomial cells n1,
with associated frequency

1/2 + Œ±2/4

into two parts, n11 and n12, with
associated frequencies 1/2 and Œ±2/4, respectively. Thus, n1 = n11+n12 and
the missing data is z = (n11, n12)‚Ä≤. The complete-data likelihood is now
p (Œ∏, z|n) ‚àù

1
2
n11 
1
4Œ∏
n12 
1
4 ‚àí1
4Œ∏
n3 
1
4 ‚àí1
4Œ∏
n4 
1
4Œ∏
n2
.
The complete-data log-likelihood, excluding an additive constant, is
ln p (Œ∏, z|n) = (n12 + n2) ln Œ∏ + (n3 + n4) ln (1 ‚àíŒ∏) ,
which is the log-likelihood of the binomial probability model
Bi (n12 + n2|Œ∏, n12 + n2 + n3 + n4) .

9.8 Examples
461
The score is
‚àÇln p (Œ∏, z|n)
‚àÇŒ∏
= n12 + n2
Œ∏
‚àín3 + n4
1 ‚àíŒ∏ ,
which is linear in Œ∏. If n12 were observed, the ML estimator of Œ∏ is obtained
by setting the score equal to 0. The explicit solution is
5Œ∏ =
n12 + n2
n12 + n2 + n3 + n4
.
(9.44)
Of course, 5Œ∏ cannot be obtained from (9.44) because n12 is not observed.
Instead, n12 is replaced in (9.44) by its conditional expectation, given the
observed data n and Œ∏[t]. The E-step of the EM algorithm consists of eval-
uating
Q

Œ∏|Œ∏[t]
= E
9
[(n12 + n2) ln Œ∏ + (n3 + n4) ln (1 ‚àíŒ∏)]| n, Œ∏[t]:
=

E

n12|n, Œ∏[t]
+ n2

ln Œ∏ + (n3 + n4) ln (1 ‚àíŒ∏) .
(9.45)
The equality in the second line arises because, conditionally on n, the only
random variable in (9.45) is n12. The M-step consists of choosing Œ∏[t+1] as
the solution to the equation
‚àÇQ

Œ∏|Œ∏[t]
‚àÇŒ∏
= 0.
This yields
Œ∏[t+1] =
E

n12|n, Œ∏[t]
+ n2
E

n12|n, Œ∏[t]
+ n2 + n3 + n4
,
(9.46)
which has the same form as (9.44), with n12 replaced by its conditional
expectation, E

n12|n, Œ∏[t]
. The EM scheme calculates E

n12|n, Œ∏[t]
and
(9.46) in an iterative fashion, until convergence is reached.
In order to obtain E

n12|n, Œ∏[t]
= E

n12|n11 + n12, Œ∏[t]
, recall from
Example 2.15 in Chapter 2 that
E

n12|n11 + n12, Œ∏[t]
=
(n11 + n12) Œ∏[t]/4
1
2 + Œ∏[t]
4
=
n1
Œ∏[t]
2 + Œ∏[t] ,
because
n12|Œ∏, n1 ‚àºBi

Œ∏/4
1
2 + Œ∏
4
, n1

.
(9.47)

462
9. Approximate Inference Via the EM Algorithm
Iteration
Œ∏[t]
r[t]
1
0.063241
0.03598
2
0.050530
0.03623
3
0.050073
0.03624
4
0.050056
0.03624
...
...
¬∑
10
0.050056
¬∑
TABLE 9.3.
Using Œ∏[0] = 0.5 as the starting value, the results of the iterative EM se-
quence are shown in Table 9.3. After a few iterations the algorithm con-
verges to 5Œ∏ = 0.050056.
A Monte Carlo EM algorithm along the lines suggested by Wei and
Tanner (1990) can be implemented by replacing E

n12|n, Œ∏[t]
in (9.46)
with
n12 = 1
m
m

i=1
zi
where z1, z2, . . . , zm are draws from (9.47).
Rate of Convergence and Standard Errors
The third column of Table 9.3 shows the evolution of the rate of convergence
given by (9.35), which for this one-dimensional example is
r[t] = Œ∏[t+1] ‚àí5Œ∏
Œ∏[t] ‚àí5Œ∏
.
The rate of convergence of the EM iteration is 0.03624.
Now we calculate Ic

5Œ∏|n

and Im

5Œ∏|n

.
Ic

5Œ∏|n

= ‚àí
‚àÇ2Q

Œ∏|5Œ∏

(‚àÇŒ∏)2
""""""
Œ∏=Œ∏
.
DiÔ¨Äerentiating (9.45) twice with respect to Œ∏ gives
‚àí
‚àÇ2Q

Œ∏|5Œ∏

(‚àÇŒ∏)2
=
E

n12|n,5Œ∏

+ n2
Œ∏2
+ n3 + n4
(1 ‚àíŒ∏)2
=
n1
Œ∏
2+Œ∏ + n2
Œ∏2
+ n3 + n4
(1 ‚àíŒ∏)2 .
(9.48)

9.8 Examples
463
Evaluated at Œ∏ = 5Œ∏, (9.48) is equal to
Ic

5Œ∏|n

= 5246.84.
(9.49)
To calculate Im

5Œ∏|n

, use is made of (9.31). This produces
V ar
 ‚àÇ
‚àÇŒ∏ (n12 + n2) ln Œ∏ + (n3 + n4) ln (1 ‚àíŒ∏)
"""" n, Œ∏

= V ar
 ‚àÇ
‚àÇŒ∏ (n12 ln Œ∏)| n, Œ∏

= V ar
 n12
Œ∏
""" n, Œ∏

=
2n1
Œ∏ (2 + Œ∏)2 ,
where the last line follows from the variance of the binomial distribution
(9.47). When evaluated at Œ∏ = 5Œ∏, the expression above is equal to
Im

5Œ∏|n

= 190.14.
(9.50)
From (9.42), (9.49), and (9.50), (9.15) can be veriÔ¨Åed:
5056.70 = 5246.84 ‚àí190.14.
Second, the rate of convergence ‚àÜ

5Œ∏|n

is

Ic

5Œ∏|n
‚àí1
Im

5Œ∏|n

= 190.14
5246.84 = 0.036238,
as obtained in the third column of Table 9.3.
The asymptotic variance based on (9.34) is given by

I

5Œ∏|n
‚àí1
= (5246.84)‚àí1 +
(1 ‚àí0.03624)‚àí1 (0.03624) (5246.84)‚àí1
= 0.0001978,
in agreement with (9.43).
In order to obtain the asymptotic variance of 5Œ∏ based on (9.36), Ô¨Årst
compute
‚àÇ2Q

Œ∏|Œ∏[t]
‚àÇŒ∏‚àÇŒ∏[t]
=
2n1
Œ∏

2 + Œ∏[t]2 ,
(9.51)
and, from (9.48),
‚àÇ2Q

Œ∏|Œ∏[t]
(‚àÇŒ∏)2
= ‚àí
n1
Œ∏
2+Œ∏ + n2
Œ∏2
‚àín3 + n4
(1 ‚àíŒ∏)2 .
(9.52)

464
9. Approximate Inference Via the EM Algorithm
When Œ∏[t] and Œ∏ are evaluated at 5Œ∏, the sum of these two expressions times
‚àí1 is equal to:
‚àí
Ô£Æ
Ô£∞
‚àÇ2Q

Œ∏|Œ∏[t]
(‚àÇŒ∏)2
+
‚àÇ2Q

Œ∏|Œ∏[t]
‚àÇŒ∏‚àÇŒ∏[t]
""""""
Œ∏[t],Œ∏=Œ∏
Ô£π
Ô£ª
=
‚àí(‚àí5246.84 + 190.14)
=
5056.70,
which agrees with (9.42) and (9.41).
‚ñ†
Example 9.2
Blood groups
In Example 4.7 from Chapter 4, the ML estimates of the frequency of blood
group alleles were computed via Newton‚ÄìRaphson. Here the EM algorithm
is used instead. Let n = (nA, nAB, nB, nO)‚Ä≤ be the observed data, with
nA = 725, nAB = 72, nB = 258, nO = 1073. It is sensible to treat the
unobserved counts nAO, nAA, nBB and nBO as missing data. The resulting
complete-data vector is
nc = (nAA, nAO, nAB, nBB, nBO, nO)‚Ä≤ .
The complete-data log-likelihood excluding an additive constant is
ln f (pA, pB|nc) = 2nAA ln (pA) + nAO ln (2pApO) + nAB ln (2pApB)
+ 2nBB ln (pB) + nBO ln (2pBpO) + 2nO ln (pO) ,
where pO = (1 ‚àípA ‚àípB). The E-step consists of computing the expected
value of the complete-data log-likelihood, conditionally on the observed
counts n and on the value of the parameters at iteration t,

p[t]
A , p[t]
B

.
Explicitly, this is
Q

pA,pB|p[t]
A , p[t]
B

= E [{2nAA ln (pA) + nAO ln (2pApO) + nAB ln (2pApB)
+ 2nBB ln (pB) + nBO ln (2pBpO) + 2nO ln (pO)} |p[t]
A , p[t]
B , n

= 2,nAA ln (pA) + ,nAO ln (2pApO) + nAB ln (2pApB) + 2,nBB ln (pB)
+ ,nBO ln (2pBpO) + 2nO ln (pO) ,
(9.53)
where
,nAA
=
E

nAA|p[t]
A , p[t]
B , n

,
,nAO
=
E

nAO|p[t]
A , p[t]
B , n

,
,nBB
=
E

nBB|p[t]
A , p[t]
B , n

,
,nBO
=
E

nBO|p[t]
A , p[t]
B , n

.

9.8 Examples
465
The M-step consists of maximizing (9.53) with respect to pA and pB. This
yields the following closed-form solution for pA and pB at a round (t + 1):
p[t+1]
A
=
2,nAA + nAB + ,nAO
2 (nA + nAB + nB + nO),
(9.54)
p[t+1]
B
=
2,nBB + nAB + ,nBO
2 (nA + nAB + nB + nO).
(9.55)
The unobserved counts at iteration t are imputed via their expected values,
given n and

p[t]
A , p[t]
B

. The unobserved counts are distributed binomially
(see Example 2.15 in Chapter 2) as follows:
nAA ‚àºBi

p2
A
p2
A + 2pA (1 ‚àípA ‚àípB), nA

,
nAO ‚àºBi

2pA (1 ‚àípA ‚àípB)
p2
A + 2pA (1 ‚àípA ‚àípB), nA

,
nBB ‚àºBi

p2
B
p2
B + 2pB (1 ‚àípA ‚àípB), nB

,
and
nBO ‚àºBi

2pB (1 ‚àípA ‚àípB)
p2
B + 2pB (1 ‚àípA ‚àípB), nB

.
Hence, expectations can be computed immediately. For example,
,nAA = nA
p2[t]
A
p2[t]
A
+ 2p[t]
A

1 ‚àíp[t]
A ‚àíp[t]
B
,
and similarly for the other components of the missing data. Using some
starting values for the gene frequencies, the missing counts ,nij are imputed
and the next round of gene frequency values are computed from (9.54) and
(9.55). In the case of the present example, starting with p[0]
A = p[0]
B = 0.2
leads to 5pA = 0.209130654 and to 5pB = 0.080801008 (with 5p0 = 1‚àí5pA‚àí5pB)
after 9 EM iterations.
To obtain an estimate of the asymptotic variance we apply the method of
Oakes (1999). This requires obtaining the 2√ó2 matrix of second derivatives
evaluated at pA, p[t]
A = 5pA and at pB, p[t]
B = 5pB, which yields
Ô£Æ
Ô£ØÔ£∞
‚àÇ2Q

pA,pB|p[t]
A ,p[t]
B

‚àÇpA‚àÇpA
‚àÇ2Q

pA,pB|p[t]
A ,p[t]
B

‚àÇpA‚àÇpB
‚àÇ2Q

pA,pB|p[t]
A ,p[t]
B

‚àÇpB‚àÇpA
‚àÇ2Q

pA,pB|p[t]
A ,p[t]
B

‚àÇpB‚àÇpB
Ô£π
Ô£∫Ô£ª
"""""""pA,p[t]
A =pA
pB,p[t]
B =pB
= ‚àí

26, 349.8
5993.29
5993.29
58667.1

(9.56)

466
9. Approximate Inference Via the EM Algorithm
and
Ô£Æ
Ô£ØÔ£ØÔ£∞
‚àÇ2Q

pA,pB|p[t]
A ,p[t]
B

‚àÇpA‚àÇp[t]
A
‚àÇ2Q

pA,pB|p[t]
A ,p[t]
B

‚àÇpA‚àÇp[t]
B
‚àÇ2Q

pA,pB|p[t]
A ,p[t]
B

‚àÇpB‚àÇp[t]
A
‚àÇ2Q

pA,pB|p[t]
A ,p[t]
B

‚àÇpB‚àÇp[t]
B
Ô£π
Ô£∫Ô£∫Ô£ª
""""""""pA,p[t]
A =pA
pB,p[t]
B =pB
=

3134.28
962.148
962.148
2657.74

.
(9.57)
Using (9.36), the estimate of the observed information matrix is:
‚àí

‚àí26, 349.8
‚àí5, 993.29
‚àí5, 993.29
‚àí58, 667.1

+

3, 134.28
962.148
962.148
2, 657.74
%
=

23, 215.5
5, 031.14
5, 031.14
56, 009.4

,
where the second term in the Ô¨Årst line corresponds to the negative of the
missing information. The estimate of the asymptotic variance of (5pA, 5pB)‚Ä≤
is:
V ar (5pA, 5pB|n) = 10‚àí6

43.930
‚àí3.946
‚àí3.946
18.209

,
in agreement with the estimate obtained in Example 4.7 based on Newton‚Äì
Raphson.
‚ñ†
Example 9.3
Maximum likelihood estimation in the mixed linear model
In this example the iterative EM equations for the ML estimation of Ô¨Åxed
eÔ¨Äects and of variance components in a univariate Gaussian mixed linear
model with 2 variance components are derived. The general EM algorithm,
with its distinct E- and M-steps, and the form of EM discussed in Section
9.6, are illustrated. Both approaches, of course, yield identical results.
The model considered here was introduced in Example 1.18 in Chapter
1. The data y (vector of dimension n √ó 1) are assumed to be a realization
from
y|Œ≤, a, œÉ2
e ‚àºN

XŒ≤ + Za, IœÉ2
e

,
and the unobserved vector of the additive genetic values (q √ó1) is assumed
to follow the multivariate normal distribution
a|AœÉ2
a ‚àºN

0, AœÉ2
a

.
The vector of Ô¨Åxed eÔ¨Äects Œ≤ has order p √ó 1; X and Z are known incidence
matrices, and the unknown variance components are the scalars œÉ2
a and
œÉ2
e. The matrix A is known; it describes the covariance structure of a
and depends on the additive genetic relationships among individuals in the

9.8 Examples
467
pedigree. Here the focus of inference is Œ∏ =

Œ≤‚Ä≤, œÉ2
a, œÉ2
e
‚Ä≤. The observed data
likelihood is
L (Œ∏|y) =

p

y|Œ≤, a, œÉ2
e

p

a|AœÉ2
a

da
‚àù|V|‚àí1
2 exp

‚àí1
2 (y ‚àíXŒ≤)‚Ä≤ V‚àí1 (y ‚àíXŒ≤)

,
(9.58)
where V = ZAZ‚Ä≤œÉ2
a + IœÉ2
e is the unconditional variance‚Äìcovariance matrix
of the observed data y. Rather than working with (9.58), the ML estimate
of Œ∏ is obtained using the EM algorithm.
General EM Algorithm
Regarding the random eÔ¨Äects a as the missing data, the complete-data
likelihood is
L (Œ∏, a|y) =
""IœÉ2
e
""‚àí1
2 exp

‚àí1
2œÉ2e
(y ‚àíXŒ≤ ‚àíZa)‚Ä≤ (y ‚àíXŒ≤ ‚àíZa)

√ó
""AœÉ2
a
""‚àí1
2 exp

‚àí1
2œÉ2a
a‚Ä≤A‚àí1a

,
(9.59)
and the corresponding log-likelihood is
ln p (Œ∏, a|y) = constant ‚àín
2 ln œÉ2
e ‚àíq
2 ln œÉ2
a ‚àí
1
2œÉ2a
a‚Ä≤A‚àí1a
‚àí1
2œÉ2e
(y ‚àíXŒ≤ ‚àíZa)‚Ä≤ (y ‚àíXŒ≤ ‚àíZa) .
(9.60)
The E-step is
Q

Œ∏|Œ∏[t]
=

ln p (Œ∏, a|y) p

a|Œ∏[t], y

da
= ‚àín
2 ln œÉ2
e ‚àíq
2 ln œÉ2
a ‚àí
1
2œÉ2a
Ea|Œ∏[t],y

a‚Ä≤A‚àí1a

‚àí1
2œÉ2e
Ea|Œ∏[t],y (y ‚àíXŒ≤ ‚àíZa)‚Ä≤ (y ‚àíXŒ≤ ‚àíZa) .
Let:
Ea|Œ∏[t],y

a|Œ∏[t], y

= ,a[t],
and
V ara|Œ∏[t],y

a|Œ∏[t], y

= ,V[t]
a .
Then using results for expectation of quadratic forms (Searle, 1971),
Q

Œ∏|Œ∏[t]
= ‚àín
2 ln œÉ2
e ‚àíq
2 ln œÉ2
a ‚àí
1
2œÉ2a

,a[t]‚Ä≤A‚àí1,a[t] + tr

A‚àí1 ,V[t]
a

‚àí1
2œÉ2e

y ‚àíXŒ≤ ‚àíZ,a[t]‚Ä≤ 
y ‚àíXŒ≤ ‚àíZ,a[t]
+ tr

Z‚Ä≤Z ,V[t]
a

.

468
9. Approximate Inference Via the EM Algorithm
The M-step consists of setting the following equations equal to zero:
‚àÇQ

Œ∏|Œ∏[t]
‚àÇŒ≤
= 1
œÉ2e
X‚Ä≤ 
y ‚àíXŒ≤ ‚àíZ,a[t]
,
‚àÇQ

Œ∏|Œ∏[t]
‚àÇœÉ2a
= ‚àíq
2œÉ2a
+
1
2 (œÉ2a)2

,a[t]‚Ä≤A‚àí1,a[t] + tr

A‚àí1 ,V[t]
a

,
and
‚àÇQ

Œ∏|Œ∏[t]
‚àÇœÉ2e
= ‚àín
2œÉ2e
+
1
2 (œÉ2e)2

y ‚àíXŒ≤ ‚àíZ,a[t]‚Ä≤ 
y ‚àíXŒ≤ ‚àíZ,a[t]
+ tr

Z‚Ä≤Z ,V[t]
a

.
Solving for Œ∏ one obtains the iterative system
Œ≤[t+1] = (X‚Ä≤X)‚àí1 X‚Ä≤ 
y ‚àíZ,a[t]
,
(9.61)
œÉ2[t+1]
a
= 1
q

,a[t]‚Ä≤A‚àí1,a[t] + tr

A‚àí1 ,V[t]
a

,
(9.62)
and
œÉ2[t+1]
e
= 1
n

y ‚àíXŒ≤[t+1]‚àíZ,a[t]‚Ä≤ 
y ‚àíXŒ≤[t+1]‚àíZ,a[t]
+tr

Z‚Ä≤Z ,V[t]
a

.
(9.63)
More explicit expressions for ,a[t] and ,V[t]
a are given in a section at the end
of this example.
Exponential Family Version of EM
The complete-data likelihood (9.59) can be put in the form (9.20) up to
proportionality, by making use of the following:
x = [y‚Ä≤, a‚Ä≤]‚Ä≤ ,
b (x) = 1,
Œ∏
=
(Œ∏1, Œ∏2, Œ∏3)‚Ä≤
=
 1
œÉ2a
, 1
œÉ2e
, Œ≤
œÉ2e
‚Ä≤
,

9.8 Examples
469
a (Œ∏)
=

œÉ2
a
 q
2 
œÉ2
e
 n
2 exp
 1
2œÉ2e
Œ≤‚Ä≤X‚Ä≤XŒ≤

=
Œ∏
‚àíq
2
1
Œ∏
‚àín
2
2
exp
Œ∏‚Ä≤
3X‚Ä≤XŒ∏3
2Œ∏2

,
(9.64a)
t (x) =
Ô£Æ
Ô£∞
‚àí1
2a‚Ä≤A‚àí1a
‚àí1
2 (y ‚àíZa)‚Ä≤ (y ‚àíZa)
X‚Ä≤ (y ‚àíZa)
Ô£π
Ô£ª.
The implementation of the EM algorithm consists of solving for Œ∏[t+1] the
system of equations deÔ¨Åned by (9.28). The right-hand side of (9.28) is the
unconditional expectation of the vector of suÔ¨Écient statistics t (x), which
is equal to (9.23). From (9.64a),
ln a (Œ∏) = ‚àíq
2 ln Œ∏1 ‚àín
2 ln Œ∏2 +
1
2Œ∏2
Œ∏‚Ä≤
3X‚Ä≤XŒ∏3.
The required unconditional expectations are given by the following partial
derivatives
‚àÇln a (Œ∏)
‚àÇŒ∏1
= ‚àíq
2Œ∏1
,
‚àÇln a (Œ∏)
‚àÇŒ∏2
= ‚àín
2Œ∏2
‚àí
1
2Œ∏2
2
Œ∏‚Ä≤
3X‚Ä≤XŒ∏3,
and
‚àÇln a (Œ∏)
‚àÇŒ∏3
= 1
Œ∏2
X‚Ä≤XŒ∏3.
The conditional expectation of t (x) (given y and Œ∏[t]) has the components
E
a|Œ∏[t],y

‚àí1
2a‚Ä≤A‚àí1a|Œ∏[t], y

= ‚àí1
2

,a[t]‚Ä≤A‚àí1,a[t] + tr

A‚àí1 ,V[t]
a

,
E
a|Œ∏[t],y

‚àí1
2 (y ‚àíZa)‚Ä≤ (y ‚àíZa) |Œ∏[t], y

=
‚àí1
2

y ‚àíZ,a[t]‚Ä≤ 
y ‚àíZ,a[t]
+ tr

Z‚Ä≤Z ,V[t]
a

,
E
a|Œ∏[t],y

X‚Ä≤ (y ‚àíZa) |Œ∏[t], y

= X‚Ä≤ 
y ‚àíZ,a[t]
.
Writing the equations ‚àÇln a (Œ∏)/ ‚àÇŒ∏ in terms of

Œ≤, œÉ2
a, œÉ2
e

, and equating
to the conditional expectations, yields
œÉ2[t+1]
a
= 1
q

,a[t]‚Ä≤A‚àí1,a[t] + tr

A‚àí1 ,V[t]
a

,
(9.65)

470
9. Approximate Inference Via the EM Algorithm
nœÉ2[t+1]
e
+ Œ≤[t+1]‚Ä≤X‚Ä≤XŒ≤[t+1]
=

y ‚àíZ,a[t]‚Ä≤ 
y ‚àíZ,a[t]
+ tr

Z‚Ä≤Z ,V[t]
a

,
(9.66)
and
X‚Ä≤XŒ≤[t+1] = X‚Ä≤ 
y ‚àíZ,a[t]
.
(9.67)
From (9.67),
Œ≤[t+1] = (X‚Ä≤X)‚àí1 X‚Ä≤ 
y ‚àíZ,a[t]
.
(9.68)
Substituting (9.68) in (9.66), and using

y ‚àíZ,a[t]‚Ä≤ 
I ‚àíX (X‚Ä≤X)‚àí1 X‚Ä≤ 
y ‚àíZ,a[t]
=

y ‚àíXŒ≤[t+1] ‚àíZ,a[t]‚Ä≤ 
y ‚àíXŒ≤[t+1] ‚àíZ,a[t]
,
one obtains
œÉ2[t+1]
e
= 1
n

y ‚àíXŒ≤[t+1] ‚àíZ,a[t]‚Ä≤ 
y ‚àíXŒ≤[t+1] ‚àíZ,a[t]
+ 1
ntr

Z‚Ä≤Z ,V[t]
a

.
The iterative system arrived at is identical to that deÔ¨Åned by (9.61), (9.62)
and (9.63).
Algebraic Notes
The starting point for the computation of the mean vector and variance‚Äì
covariance matrix of

a|Œ∏[t], y

is the joint distribution

a, y|Œ∏[t]
:
y
a
"""" Œ∏[t] ‚àºN


XŒ≤
0

,

V
ZAœÉ2
a
AZ‚Ä≤œÉ2
a
AœÉ2
a

,
where V = ZAZ‚Ä≤œÉ2
a +IœÉ2
e. From properties of the multivariate normal dis-
tribution it follows that
a|Œ∏[t], y ‚àºN

E

a|Œ∏[t], y

,V ar

a|Œ∏[t], y

,
where
E

a|Œ∏[t], y

= AZ‚Ä≤V‚àí1[t] 
y ‚àíXŒ≤[t]
œÉ2[t]
a .
Substituting in this expression
V‚àí1[t] =
1
œÉ2[t]
e
I ‚àí
1
œÉ2[t]
e
Z

Z‚Ä≤Z + A‚àí1k[t]‚àí1
Z‚Ä≤,

9.8 Examples
471
gives
E

a|Œ∏[t], y

= AZ‚Ä≤
 1
œÉ2[t]
e
I ‚àí
1
œÉ2[t]
e
Z

Z‚Ä≤Z + A‚àí1k[t]‚àí1
Z‚Ä≤
 
y ‚àíXŒ≤[t]
œÉ2[t]
a
=

A 1
k[t] ‚àí1
k[t] AZ‚Ä≤Z

Z‚Ä≤Z + A‚àí1k[t]‚àí1
Z‚Ä≤ 
y ‚àíXŒ≤[t]
=

Z‚Ä≤Z + A‚àí1k[t]‚àí1
Z‚Ä≤ 
y ‚àíXŒ≤[t]
,
(9.69)
where k[t] = œÉ2[t]
e
/œÉ2[t]
a . The last line follows because
A 1
k[t] ‚àí1
k[t] AZ‚Ä≤Z

Z‚Ä≤Z + A‚àí1k[t]‚àí1
=

Z‚Ä≤Z + A‚àí1k[t]‚àí1
.
This can be veriÔ¨Åed by post-multiplying the left-hand side by the inverse
of the right-hand side, which retrieves I.
Now we show that
V ar

a|Œ∏[t], y

=

Z‚Ä≤Z + A‚àí1k[t]‚àí1
œÉ2[t]
e
.
(9.70)
Again, using properties of the multivariate normal distribution,
V ar

a|Œ∏[t], y

= AœÉ2[t]
a
‚àíAZ‚Ä≤œÉ2[t]
a V‚àí1[t]ZAœÉ2[t]
a
= AœÉ2[t]
a
‚àí

A 1
k[t] ‚àí1
k[t] AZ‚Ä≤Z

Z‚Ä≤Z + A‚àí1k[t]‚àí1
Z‚Ä≤ZAœÉ2[t]
a
= AœÉ2[t]
a
‚àí

Z‚Ä≤Z + A‚àí1k[t]‚àí1
Z‚Ä≤ZAœÉ2[t]
a
=

Z‚Ä≤Z + A‚àí1k[t]‚àí1
œÉ2[t]
e
.
The last line can be veriÔ¨Åed by premultiplying the third line by

Z‚Ä≤Z + A‚àí1k[t]
,
which gives IœÉ2[t]
e
.
An alternative derivation from a Bayesian perspective (which implies
assigning a Ô¨Çat, improper prior to the location vector Œ≤) is to use as point
of departure
Œ≤
a
"""" œÉ2[t]
a , œÉ2[t]
e
, y ‚àºN

5Œ≤
[t]
5a[t]

,

C11
C12
C21
C22
[t]
œÉ2[t]
e

.
(9.71)
Using similar algebra as in Example 1.18 from Chapter 1, one can show
that the mean vector of

a|Œ≤,œÉ2[t]
a , œÉ2[t]
e
, y

is equal to

Z‚Ä≤Z + A‚àí1k[t]‚àí1
Z‚Ä≤ (y ‚àíXŒ≤) ,

472
9. Approximate Inference Via the EM Algorithm
and its covariance matrix is equal to (9.70).
‚ñ†
Example 9.4
Restricted maximum likelihood estimation in the mixed lin-
ear model
The model is as in the preceding example, but now the focus of inference is
Œ∏ =

œÉ2
a, œÉ2
e

, with Œ≤ viewed as a nuisance parameter. A Bayesian perspec-
tive will be adopted, and the mode of the posterior distribution with density
p

œÉ2
a, œÉ2
e|y

is chosen as point estimator. Assigning improper uniform prior
distributions to each of

œÉ2
a, œÉ2
e

and to Œ≤, then
p

œÉ2
a, œÉ2
e|y

‚àù

p

y|Œ≤, a, œÉ2
e

p

a|A, œÉ2
a

da dŒ≤.
In this setting the mode of the posterior distribution of the variance compo-
nents is identical to the REML estimator, (Harville, 1974). Joint maximiza-
tion of this expression is diÔ¨Écult. However, it is relatively easy to struc-
ture an EM algorithm, where the missing data are now z =

Œ≤‚Ä≤, a‚Ä≤‚Ä≤. The
complete-data posterior distribution p

œÉ2
a, œÉ2
e, z|y

is identical to (9.60)
and the E-step is now
Q

Œ∏|Œ∏[t]
=

ln p (Œ∏, Œ≤, a|y) p

Œ≤, a|Œ∏[t], y

da dŒ≤
= ‚àín
2 ln œÉ2
e ‚àíq
2 ln œÉ2
a ‚àí
1
2œÉ2a
EŒ≤,a|Œ∏[t],y

a‚Ä≤A‚àí1a

‚àí1
2œÉ2e
EŒ≤,a|Œ∏[t],y (y ‚àíXŒ≤ ‚àíZa)‚Ä≤ (y ‚àíXŒ≤ ‚àíZa) .
(9.72)
This, on using (9.71), takes the form
Q

Œ∏|Œ∏[t]
= ‚àín
2 ln œÉ2
e ‚àíq
2 ln œÉ2
a
‚àí1
2œÉ2a

5a[t]‚Ä≤A‚àí15a + tr

A‚àí1C22[t]
œÉ2[t]
e

‚àí1
2œÉ2e

5e[t]‚Ä≤5e[t] + tr

[X, Z] C‚àí1[t] [X, Z]‚Ä≤
œÉ2[t]
e

,
where 5e[t] =

y ‚àíX5Œ≤
[t]‚àíZ5a[t]

and
C‚àí1 =
 C11
C12
C21
C22

.
Since the missing data are now z =

Œ≤‚Ä≤, a‚Ä≤‚Ä≤, expectations in (9.72) are
taken with respect to

Œ≤, a|Œ∏[t], y

. The M-step is
‚àÇQ

Œ∏|Œ∏[t]
‚àÇœÉ2a
= ‚àíq
2œÉ2a
+
1
2 (œÉ2a)2

5a[t]‚Ä≤A‚àí15a + tr

A‚àí1C22[t]
œÉ2[t]
e

,

9.8 Examples
473
and
‚àÇQ

Œ∏|Œ∏[t]
‚àÇœÉ2e
= ‚àín
2œÉ2e
+
1
2 (œÉ2e)2

5e[t]‚Ä≤5e[t] + tr

[X, Z] C‚àí1[t] [X, Z]‚Ä≤
œÉ2[t]
e

.
Setting to zero yields the iterative system
œÉ2[t+1]
a
= 5a[t]‚Ä≤A‚àí15a + tr

A‚àí1C22[t]
œÉ2[t]
e
q
,
(9.73)
œÉ2[t+1]
e
= 5e[t]‚Ä≤5e[t] + tr

[X, Z] C‚àí1[t] [X, Z]‚Ä≤
œÉ2[t]
e
n
.
(9.74)
Contrary to ML estimation, REML estimation or inference via the posterior
mode of

œÉ2
a, œÉ2
e|y

requires inverting the entire coeÔ¨Écient matrix C.
‚ñ†
The preceding examples illustrate the versatility of the EM algorithm.
Viewed more generally, the algorithm can be interpreted as a data aug-
mentation technique (Tanner and Wong, 1987), with the imputations made
using conditional expectations. Additional examples of varying degrees of
complexity can be found in Little and Rubin (1987) and in McLachlan and
Krishnan (1997).

This page intentionally left blank

Part III
Markov Chain Monte
Carlo Methods
475

This page intentionally left blank

10
An Overview of Discrete
Markov Chains
10.1
Introduction
The theory of Markov chains governs the behavior of the Markov chain
Monte Carlo (MCMC) methods that are discussed in Chapter 11 and on-
wards. The purpose of this chapter is to present an overview of some ele-
ments of this theory so that the reader can obtain a feeling for the mecha-
nisms underlying MCMC. Feller (1970), Cox and Miller (1965), Karlin and
Taylor (1975), Meyn and Tweedie (1993), Norris (1997), or Robert and
Casella (1999) should be consulted for a more detailed and formal treat-
ment of the subject. A recent contribution to the literature, that discusses
the Ô¨Åner theoretical and practical issues in a clear style with a view towards
MCMC applications, is Liu (2001). For the sake of simplicity, this chapter
considers only chains with Ô¨Ånite state spaces.
The aspects of Markov chain theory discussed here are the following
ones. First, the fundamental ingredients of a Markov chain are introduced.
These consist of the initial probability distribution of the states of the
chain and the matrix of transition probabilities. The two together govern
the evolution of the Markov chain. One wishes to know if, after a number of
transitions, the Markov chain converges to some equilibrium distribution,
independently of the initial probability distribution, and if this equilibrium
distribution is unique. Two important properties for discrete, Ô¨Ånite state
Markov chains, establish the existence of a unique equilibrium distribution:
aperiodicity and irreducibility. Another important property especially from
the point of view of MCMC is that of reversibility. As shown in Chapter

478
10. An Overview of Discrete Markov Chains
11, transition probability matrices or transition kernels (in the case of con-
tinuous Markov chains) can be constructed more or less easily using the
condition of reversibility as point of departure. The transition kernels con-
structed in this way, guarantee that aperiodic and irreducible chains have a
unique equilibrium distribution. Convergence to this distribution, however,
takes place only asymptotically. Further, the rate of convergence is an im-
portant aspect of the behavior of MCMC, a subject which is discussed and
illustrated in a closing section of this chapter. Other examples illustrating
convergence can be found in Chapter 12.
10.2
DeÔ¨Ånitions
Following Grossman and Turner (1974), suppose that a mouse is placed in a
box divided into three intercommunicating compartments labelled 1, 2, and
3. Technically the three compartments are the possible states of the system
at any time, and the set S = {1, 2, 3} is called the state space. One can de-
Ô¨Åne the random variable ‚Äúpresence of the mouse in a given compartment‚Äù.
This random variable can take one of three possible values. A movement
from compartment i to j (which includes the case where i = j, indicating
no movement) is referred to as a transition of the system from the ith to
the jth state. This transition will occur with some conditional probabil-
ity p (i, j) , called the transition probability of moving from compartment
i to j. This probability is conditional on the mouse being in compartment
i. Consider a sequence of random variables deÔ¨Åning the presence of the
mouse in the compartments over a period of time. If the probability that
the mouse moves to a given compartment depends on which compartment
it Ô¨Ånds itself immediately before the move, and not on which compartments
the mouse had visited in the past, then the sequence of random variables
deÔ¨Ånes a Markov chain. A Markov chain deals with the study of the possible
transitions between the states of a system via probabilistic methods. An
important goal is to characterize the probability distribution after n tran-
sitions. This is the distribution of the proportion of times that the mouse
is expected to be in each compartment after n transitions. It is of interest
to know whether for large n, this distribution stabilizes, irrespective of the
initial distribution. A formal deÔ¨Ånition of a Markov chain follows.
A Ô¨Ånite state, discrete Markov chain is a sequence of random vari-
ables Xn, (n = 0, 1, 2, . . . , ) where the Xn take values in the Ô¨Ånite set
S = {0, 1, . . . , N ‚àí1} and are called the states of the Markov chain. The
subscripts n can be interpreted as stages or time periods. If Xn = i, the
process is said to be in state i at time n. A Markov chain must satisfy the

10.3 State of the System after n-Steps
479
following Markov property
Pr (Xn = j|Xn‚àí1 = i, Xn‚àí2 = k, . . . , X0 = m)
= Pr (Xn = j|Xn‚àí1 = i) = p (i, j) ,
i, j, k, . . . , m ‚ààS.
(10.1)
This property may be interpreted as stating that, for a Markov chain,
the conditional distribution at time n, given all the past states X0 =
m, . . . , Xn‚àí1 = i, only depends on the immediately preceding state, Xn‚àí1 =
i. A sequence of independent random variables is a trivial example of a
Markov chain.
The evolution of the chain is described by its transition probability (10.1).
The element p (i, j) represents the probability that the chain at time n is
in state j, given that at time n ‚àí1 it was in state i. This is a conditional
probability, where j is stochastic and i is Ô¨Åxed. In the notation of distribu-
tion theory, this would normally be written p (j|i). However, the standard
Markov chain notation is adhered to here and in the next chapter (for
example, as in Cox and Miller, 1965). The transition probabilities can be
arranged in a matrix P = {p (i, j)}, where i is a row suÔ¨Éx and j a column
suÔ¨Éx. This is the N √óN transition probability matrix of the Markov chain.
Only transition probability matrices that are independent of time (n) are
considered here. These are known as time homogeneous chains: the prob-
ability of a transition from a given state to another depends on the two
states and not on time.
The (i + 1)th row of P is the probability distribution of the values of Xn
under the condition that Xn‚àí1 = i. Every entry of P satisÔ¨Åes p (i, j) ‚â•0,
and every row of P satisÔ¨Åes 
j p (i, j) = 1.
10.3
State of the System after n-Steps
As stated above, the matrix P describes the probability of transitions of the
chain in one time period. Consider a chain with state space S = {0, 1, 2}.
The matrix of transition probabilities, of order 3 √ó 3 takes the form
P =
Ô£Æ
Ô£∞
p (0, 0)
p (0, 1)
p (0, 2)
p (1, 0)
p (1, 1)
p (1, 2)
p (2, 0)
p (2, 1)
p (2, 2)
Ô£π
Ô£ª.

480
10. An Overview of Discrete Markov Chains
The probability that at stage (or time) 2 + m the random variable will be
in state 2, given that it was in stage 1 at time m can be written as
p(2) (1, 2) = Pr (X2+m = 2|Xm = 1)
=
2

j=0
Pr (X2+m = 2, X1+m = j|Xm = 1)
=
2

j=0
Pr (X1+m = j|Xm = 1) Pr (X2+m = 2|X1+m = j)
= p (1, 0) p (0, 2) + p (1, 1) p (1, 2) + p (1, 2) p (2, 2) .
(10.2)
The equality in the third line follows from the Markov property. Equation
(10.2) makes it explicit that to compute the probability of moving from
state 1 to 2 in two transitions, requires summation over the probabilities
of going through all possible intermediate states before the system reaches
state 2. The last line in (10.2) can be recognized as the product of row 2 of
P (associated with state 1) and column 3 of P (associated with state 2).
The preceding argument can be generalized to arrive at two important
results. The Ô¨Årst one concerns the probability of moving from i to j in n
transitions. This is given by
p(n) (i, j) = Pr (Xn = j|X0 = i) = Pr (Xn+k = j|Xk = i) .
(10.3)
The element p(n) (i, j) is the (i + 1, j + 1) entry of Pn. The second result
is described by the Chapman‚ÄìKolmogorov equations
p(m+n) (i, j)
=
Pr (Xm+n = j|X0 = i)
=
N‚àí1

k=0
Pr (Xm+n = j, Xm = k|X0 = i)
=
N‚àí1

k=0
Pr (Xm+n = j|Xm = k) Pr (Xm = k|X0 = i)
=
N‚àí1

k=0
p(m) (i, k) p(n) (k, j) .
(10.4)
The matrix analogue to (10.4) is
Pm+n = PmPn.
This result relates long-term behavior to short-term behavior and describes
how Xn depends on the starting value X0.

10.4 Long-Term Behavior of the Markov Chain
481
10.4
Long-Term Behavior of the Markov Chain
Let œÄ‚Ä≤(n) be the N-dimensional row vector denoting the probability distri-
bution of Xn. The ith component of œÄ(n) is:
œÄ(n) (i) = Pr (Xn = i) ,
i ‚ààS.
Clearly, when n = 0, œÄ(0) deÔ¨Ånes the initial probability distribution of the
chain. Now,
Pr (Xn = j) =
N‚àí1

i=0
Pr (Xn = j|Xn‚àí1 = i) Pr (Xn‚àí1 = i)
=
N‚àí1

i=0
p (i, j) Pr (Xn‚àí1 = i) , j = 0, 1, . . . , N ‚àí1.
(10.5)
The left-hand side is the (j + 1)th element of œÄ(n), and the right-hand
side is the product of the row vector œÄ‚Ä≤(n‚àí1) with column (j + 1) of the
transition matrix P. Therefore a matrix generalization of (10.5) is given by
œÄ‚Ä≤(n) = œÄ‚Ä≤(n‚àí1)P.
(10.6)
Since œÄ‚Ä≤(1) = œÄ‚Ä≤(0)P, œÄ‚Ä≤(2) = œÄ‚Ä≤(1)P = œÄ‚Ä≤(0)PP = œÄ‚Ä≤(0)P2, and so on, it
follows that the probability distribution of the chain at time n is given by
œÄ‚Ä≤(n) = œÄ‚Ä≤(0)Pn.
(10.7)
Thus, we reach the important conclusion that the random evolution of
the Markov chain is completely speciÔ¨Åed in terms of the distribution of the
initial state and the transition probability matrix P. This means that given
the initial probability distribution and the transition probability matrix, it
is possible to describe the behavior of the process at any speciÔ¨Åed time
period n.
10.5
Stationary Distribution
A question of fundamental importance is whether the chain converges to
a limiting distribution, independent of any legal starting distribution. As
shown below, this requires that Pn converges to some invariant matrix,
and that, at the limit, it has identical rows. Suppose that œÄ is a limiting
probability vector such that, from (10.7),
œÄ‚Ä≤ = lim
n‚Üí‚àûœÄ‚Ä≤(0)Pn.

482
10. An Overview of Discrete Markov Chains
Then,
œÄ‚Ä≤
=
lim
n‚Üí‚àûœÄ‚Ä≤(0)Pn+1
=

lim
n‚Üí‚àûœÄ‚Ä≤(0)Pn
P
=
œÄ‚Ä≤P.
(10.8)
The distribution œÄ is said to be a stationary distribution (also known as the
invariant or equilibrium distribution) if it satisÔ¨Åes (10.8). An interpretation
of (10.8) is that if a chain has reached a stage where œÄ is the stationary
distribution, it retains it in subsequent moves. Expression (10.8) can also
be written as the system of equations
œÄ (j) =
N‚àí1

i=0
œÄ (i) p (i, j) .
(10.9)
For the case of Ô¨Ånite state Markov chains under consideration here, station-
ary distributions always exist (Grimmet and Stirzaker, 1992). The issue in
general is convergence and uniqueness. This problem is taken up at the end
of this chapter.
Example 10.1
A three-state space Markov chain
Consider a Markov chain consisting of three states, 0, 1, 2, with the following
transition probability matrix:
P
=
Ô£Æ
Ô£∞
p (0, 0)
p (0, 1)
p (0, 2)
p (1, 0)
p (1, 1)
p (1, 2)
p (2, 0)
p (2, 1)
p (2, 2)
Ô£π
Ô£ª
=
Ô£Æ
Ô£ØÔ£∞
1
2
1
2
0
1
4
1
2
1
4
0
1
3
2
3
Ô£π
Ô£∫Ô£ª.
Row 2 say, with elements p (1, 0) = 1
4, p (1, 1) = 1
2, and p (1, 2) = 1
4 rep-
resents the probability distribution of the values of Xn+1 given Xn = 1.
Element p (1, 2) say, is the probability that the system moves from state 1
to state 2 in one transition. It can be veriÔ¨Åed that
P4 =
Ô£Æ
Ô£∞
0.2760
0.4653
0.2587
0.2326
0.4485
0.3189
0.1725
0.4251
0.4024
Ô£π
Ô£ª
and p4 (1, 2) = 0.3189 is the probability of moving from state 1 to state 2
in four transitions. If the starting probability distribution of the chain is
œÄ‚Ä≤(0) =
 0
1
0 
,

10.6 Aperiodicity and Irreducibility
483
then we have, to four decimal places,
œÄ‚Ä≤(1)
=

0.2500
0.5000
0.2500

,
œÄ‚Ä≤(2)
=

0.2500
0.4583
0.2917

,
œÄ‚Ä≤(4)
=

0.2396
0.4514
0.3090

,
œÄ‚Ä≤(5)
=

0.2326
0.4485
0.3189

.
Eventually the system converges to the stationary distribution
œÄ‚Ä≤ =

0.2222
0.4444
0.3333

,
(10.10)
and, for large n,
Pn =
Ô£Æ
Ô£∞
œÄ‚Ä≤
œÄ‚Ä≤
œÄ‚Ä≤
Ô£π
Ô£ª.
The same stationary distribution (10.10) is arrived at, regardless of the
starting value of œÄ(0). In fact, for this example, the distribution (10.10)
is unique. This distribution can be derived from (10.8) or (10.9). Let the
stationary distribution be
œÄ‚Ä≤ =

œÄ (0)
œÄ (1)
œÄ (2)

,
with œÄ (2) = 1 ‚àíœÄ (1) ‚àíœÄ (0). From (10.8), the system of equations to be
solved for œÄ (0) and œÄ (1) is
œÄ (0)
2
+ œÄ (1)
4
= œÄ (0) ,
œÄ (0)
2
+ œÄ (1)
2
+ 1 ‚àíœÄ (1) ‚àíœÄ (0)
3
= œÄ (1) .
The unique solution is œÄ (0) = 2/9, œÄ (1) = 4/9, and œÄ (2) = 1 ‚àíœÄ (1) ‚àí
œÄ (0) = 3/9.
‚ñ†
10.6
Aperiodicity and Irreducibility
In the example above the Markov chain converges to a stationary distri-
bution, and it was stated that this distribution is unique. For Ô¨Ånite state
Markov chains, convergence and uniqueness require the chain to be aperi-
odic and irreducible.
Consider a three state Markov chain and that the only possible transi-
tions are 1 ‚Üí2, 2 ‚Üí3, and 3 ‚Üí1. That is, the states repeat themselves
every 3 movements of the chain. If the process starts at state 2, say, this
state will be revisited in a periodic fashion at times 3, 6, 9, .... The greatest

484
10. An Overview of Discrete Markov Chains
common divisor of these integers is 3; it is then said that this state has
period equal to 3.
Formally, the period of state j is deÔ¨Åned as the greatest common divisor
of all integers n ‚â•1 for which p(n) (j, j) > 0. If the period of state j of
the chain is d say, it means that p(n) (j, j) = 0 whenever n is not divisible
by d, and d is the largest integer with this property. An aperiodic state
has period 1, or alternatively, a state j is aperiodic if p(n) (j, j) > 0 for all
suÔ¨Éciently large n. A suÔ¨Écient condition for a state to have period 1 is
Pr (Xn = j|X0 = j) and Pr (Xn+1 = j|X0 = j) > 0
(10.11)
for some n ‚â•0 and some state j = 0, 1, . . . , N ‚àí1. Clearly, aperiodicity
holds when p (j, j) = Pr (Xn = j|Xn‚àí1 = j) > 0 for all j.
An important property of aperiodicity is that if states j and i commu-
nicate and state j is aperiodic, then state i is also aperiodic. A chain is
aperiodic if all states have period 1. Now, if all states of a Markov chain
communicate, such that every state is reachable from every other state in
a Ô¨Ånite number of transitions, the Markov chain is called irreducible. All
states of an irreducible chain have the same period.
More formally, irreducibility means that for every pair of states (i, j),
p(k) (i, j) = Pr (Xn+k = j|Xn = i) > 0 for some k ‚â•0 (the value of k may
be diÔ¨Äerent for diÔ¨Äerent i, j; more formally we should write k (i, j)).
A chain that is irreducible with period d has a transition probability ma-
trix with d eigenvalues with absolute value 1. This property has important
implications for convergence of the chain, as is illustrated in the example
below.
Finite state Markov chains that are aperiodic and irreducible have the
property that, for some n ‚â•0, Pn has all entries positive. Such a Markov
chain is called ergodic. In the Ô¨Ånal section of this chapter, it is shown that
an ergodic Markov chain converges to a unique stationary distribution œÄ,
for all legal initial probability distributions. This stationary distribution
is the unique solution to (10.8). Four cases of Markov chains that are not
ergodic are presented in the following example. The transition probability
matrices of these Markov chains are such that, for large n, Pn has non-
positive entries.
Example 10.2
Four Markov chains that are not ergodic
Consider a Markov chain with the following transition probability matrix
P =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£∞
0
1
0
0
0
0.5
0
0.5
0
0
0
0.5
0
0.5
0
0
0
0.5
0
0.5
0
0
0
1
0
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
.

10.6 Aperiodicity and Irreducibility
485
Taking powers of this matrix discloses quickly that Pn looks diÔ¨Äerent de-
pending on whether n is even or odd. For large n, if n is even,
Pn ‚âÉ
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£∞
0.25
0
0.50
0
0.25
0
0.50
0
0.50
0
0.25
0
0.50
0
0.25
0
0.50
0
0.50
0
0.25
0
0.50
0
0.25
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
,
and if n is odd,
Pn ‚âÉ
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£∞
0
0.50
0
0.50
0
0.25
0
0.50
0
0.25
0
0.50
0
0.50
0
0.25
0
0.50
0
0.25
0
0.50
0
0.50
0
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
.
The Markov chain deÔ¨Åned by this transition probability is irreducible (for
some n, p(n) (i, j) > 0 for all i, j) and periodic with period d = 2 (start-
ing in state 1 say, pn (1, 1) > 0 at times n = 2, 4, 6, 8, . . .; the greatest
common divisor of the values that n can take is 2). The unique stationary
distribution for this chain (the only solution to (10.8)) can be shown to be
œÄ
=
lim
n‚Üí‚àû
1
2

œÄ(n) + œÄ(n+1)
=

0.125
0.25
0.25
0.25
0.125
‚Ä≤ .
Even though a unique stationary distribution exists, the chain does not
converge to it. The equilibrium probability at state i, œÄ (i), rather than
representing the limit of p(n) (j, i), represents the average amount of time
that is spent in state i.
One can verify that the eigenvalues of P are ‚àí1, 0, 1, ‚àí1/
‚àö
2, 1/
‚àö
2; that
is, there are d = 2 eigenvalues with absolute value 1. We return to this
point at the end of the chapter.
As a second case, consider the chain with the transition probability ma-
trix
P =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£∞
1
0
0
0
0
0.5
0
0.5
0
0
0
0.5
0
0.5
0
0
0
0.5
0
0.5
0
0
0
0
1
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
.
(10.12)

486
10. An Overview of Discrete Markov Chains
Rather than a unique stationary distribution, the system has Ô¨Åve stationary
distributions. Thus, for large n,
Pn ‚âÉ
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£∞
1
0
0
0
0
0.75
0
0
0
0.25
0.50
0
0
0
0.50
0.25
0
0
0
0.75
0
0
0
0
1
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
.
The rows of Pn represent the Ô¨Åve stationary distributions, and each of
these satisfy (10.8).
As a third case, consider the following reducible chain deÔ¨Åned by the
transition probability
P =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
1
2
1
2
0
0
0
1
6
5
6
0
0
0
0
0
3
4
1
4
0
0
0
3
24
16
24
5
24
0
0
0
1
6
5
6
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
.
For large n
Pn ‚âÉ
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£∞
0.25
0.75
0
0
0
0.25
0.75
0
0
0
0
0
0.182
0.364
0.455
0
0
0.182
0.364
0.455
0
0
0.182
0.364
0.455
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
.
The chain splits into two noncommunicating subchains; each of these con-
verges to an equilibrium distribution, but one cannot move from state
spaces {0, 1} to state spaces {2, 3, 4}.
The fourth case illustrates an irreducible (all the states of the chain
communicate), periodic chain, with period d = 3,
P =
Ô£Æ
Ô£∞
0
1
0
0
0
1
1
0
0
Ô£π
Ô£ª.
The only possible transitions are from state 1, to state 2, to state 3, to
state 1, and so on. That is, the chain returns to a given state at times
n = 3, 6, 9, . . .; the greatest common divisor of these numbers is 3. The
stationary distribution obtained solving (10.8) is
œÄ =
1
3, 1
3, 1
3
‚Ä≤
.
(10.13)

10.7 Reversible Markov Chains
487
However, the chain does not converge to (10.13). One can verify that P
has d = 3 eigenvalues with absolute value 1. The consequences of this will
become apparent in the last section of this chapter.
‚ñ†
10.7
Reversible Markov Chains
Consider an ergodic Markov chain with state space S that converges to
an invariant distribution œÄ. Let x ‚ààS denote the current state of the
system, and let y ‚ààS denote the state at the next step. Let p (x, y) be the
probability of a transition from x to y and let p (y, x) denote the probability
of a transition in the opposite direction. A Markov chain is said to be
reversible if it satisÔ¨Åes the condition:
œÄ (x) p (x, y) = œÄ (y) p (y, x) ,
for all x, y ‚ààS,
(10.14)
which is known as the detailed balance equation. An ergodic chain in equi-
librium, satisfying (10.14) has œÄ as its unique stationary distribution. This
can be conÔ¨Årmed by summing both sides of (10.14) over y to yield the
equilibrium condition (10.9):
œÄ (x) =

y‚ààS
œÄ (y) p (y, x) .
The left-hand side of (10.14) can be expressed as
œÄ (x) p (x, y) = Pr (Xn = x) Pr (Xn+1 = y|Xn = x)
= Pr (Xn = x, Xn+1 = y) ,
for all x, y ‚ààS.
(10.15)
This makes explicit that detailed balance is a statement involving a joint
probability. Therefore, the reversibility condition can also be written as
Pr (Xn = x, Xn+1 = y) = Pr (Xn = y, Xn+1 = x) ,
for all x, y ‚ààS.
(10.16)
The reversibility condition plays an important role in the construction
of MCMC algorithms, because it is often easy to generate a Markov chain
having the desired stationary distribution using (10.14) as a point of de-
parture. This is discussed in Chapter 11, where the role of reversibility in
deriving appropriate transition kernels is highlighted. Proving that œÄ is the
unique stationary distribution from nonreversible chains can be diÔ¨Écult.
Below is an example where this is not the case.
Example 10.3
An irreducible nonreversible Markov chain
Consider the three-state chain on S = {0, 1, 2} with transition probability
matrix
P =
Ô£Æ
Ô£∞
0.7
0.2
0.1
0.1
0.7
0.2
0.2
0.1
0.7
Ô£π
Ô£ª.

488
10. An Overview of Discrete Markov Chains
It is easy to verify that for large n,
Pn =
Ô£Æ
Ô£ØÔ£∞
1
3
1
3
1
3
1
3
1
3
1
3
1
3
1
3
1
3
Ô£π
Ô£∫Ô£ª.
Therefore, the unique invariant distribution is
œÄ =

1
3
1
3
1
3
‚Ä≤ .
However, the chain is not reversible. For example,
œÄ (0) p (0, 1) = 1
3 (0.2)
Ã∏= œÄ (1) p (1, 0) = 1
3 (0.1) .
‚ñ†
Example 10.4
A multinomial distribution
Consider the 2 √ó 2 table studied by Casella and George (1992):
Pr (X = 0, Y = 0) = fx,y (0, 0) = p1,
Pr (X = 1, Y = 0) = fx,y (1, 0) = p2,
Pr (X = 0, Y = 1) = fx,y (0, 1) = p3,
Pr (X = 1, Y = 1) = fx,y (1, 1) = p4,
with all pi ‚â•0 and 4
i=1 pi = 1. The marginal distributions of X and Y are
Bernoulli, with success probabilities (p2 + p4) and (p3 + p4), respectively,
fx (0) = p1 + p3,
fx (1) = p2 + p4,
and
fy (0) = p1 + p2,
fy (1) = p3 + p4.
The conditional distributions are easily derived. These are
Px|y
=

fx|y (0|0)
fx|y (1|0)
fx|y (0|1)
fx|y (1|1)

=
Ô£Æ
Ô£∞
p1
p1+p2
p2
p1+p2
p3
p3+p4
p4
p3+p4
Ô£π
Ô£ª

10.7 Reversible Markov Chains
489
and
Py|x
=

fy|x (0|0)
fy|x (1|0)
fy|x (0|1)
fy|x (1|1)

=
Ô£Æ
Ô£∞
p1
p1+p3
p3
p1+p3
p2
p2+p4
p4
p2+p4
Ô£π
Ô£ª.
A number of Markov chains can be generated from this model. For example,
the product Px|yPy|x generates the transition probability matrix Px|x:
Px|x = Px|yPy|x
with elements
Pr (Xn+1 = i|Xn = j) =
1

k=0
Pr (Xn+1 = i|Yn+1 = k)
√ó Pr (Yn+1 = k|Xn = j) .
(10.17)
For some start value of the marginal distribution of X, this transition
probability matrix generates the sequence Xn, (n = 1, 2, . . . , ) which is a
Markov chain. From (10.17) one can readily obtain Px|x whose elements
are:
Pr (X1 = 0|X0 = 0) =
p1
p1 + p2
p1
p1 + p3
+
p3
p3 + p4
p3
p1 + p3
,
Pr (X1 = 1|X0 = 0) =
p2
p1 + p2
p1
p1 + p3
+
p4
p3 + p4
p3
p1 + p3
,
Pr (X1 = 0|X0 = 1) =
p1
p1 + p2
p2
p2 + p4
+
p3
p3 + p4
p4
p2 + p4
,
Pr (X1 = 1|X0 = 1) =
p2
p1 + p2
p2
p2 + p4
+
p4
p3 + p4
p4
p2 + p4
.
It can be conÔ¨Årmed that
Pr (X1 = 0|X0 = 0) + Pr (X1 = 1|X0 = 0) = 1
and that
Pr (X1 = 0|X0 = 1) + Pr (X1 = 1|X0 = 1) = 1.
Since Px|x is a probability matrix and the elements of Pn
x|x are all positive,
the Markov chain has a unique stationary distribution that satisÔ¨Åes œÄ‚Ä≤ =
œÄ‚Ä≤Px|x. Simple calculations yield
(p1 + p3) Pr (X1 = 0|X0 = 0) + (p2 + p4) Pr (X1 = 0|X0 = 1)
= (p1 + p3)

490
10. An Overview of Discrete Markov Chains
and
(p1 + p3) Pr (X1 = 1|X0 = 0) + (p2 + p4) Pr (X1 = 1|X0 = 1)
= (p2 + p4) ,
conÔ¨Årming that œÄ =

p1 + p3
p2 + p4
‚Ä≤ is the stationary distribution of
the Markov chain.
The above Markov chain satisÔ¨Åes the reversibility condition (10.16).
Thus,
Pr (Xn = 1, Xn‚àí1 = 0) = Pr (Xn = 1|Xn‚àí1 = 0) Pr (Xn‚àí1 = 0)
= Pr (Xn = 0, Xn‚àí1 = 1)
=
p1p2
p1 + p2
+
p3p4
p3 + p4
.
From the same multinomial model, another Markov chain can be gener-
ated by the 4√ó4 transition probability matrix Pxnyn|xn‚àí1yn‚àí1 with elements
deÔ¨Åned as follows:
Pr (Xn = k, Yn = l|Xn‚àí1 = i, Yn‚àí1 = j)
= Pr (Xn = k|Xn‚àí1 = i, Yn‚àí1 = j)
√ó Pr (Yn = l|Xn = k, Xn‚àí1 = i, Yn‚àí1 = j) ,
(i, j) (k, l) ‚ààS.
(10.18)
Instead of using (10.18), consider generating a Markov chain using the
following transition probability:
Pr (Xn = k|Yn‚àí1 = j) Pr (Yn = l|Xn = k) ,
(i, j) (k, l) ‚ààS.
(10.19)
This creates the Markov chain (Xn, Yn) , (n = 1, 2, . . .) whose state space
is S = {0, 1}2 and which consists of correlated Bernoulli random variables.
Notice that the chain is formed by a sequence of conditional distributions:
Ô¨Årst Xn is updated from its conditional distribution, given Yn‚àí1, and this
is followed by updating Yn from its conditional distribution, given the value
of Xn from the previous step.
It is easy to verify that this Markov chain has a unique stationary dis-
tribution given by œÄ‚Ä≤ = (p1, p2, p3, p4), that is the only solution to (10.8).
Thus, the chain formed by the sequence of conditional distributions has
the joint distribution œÄ as its unique stationary distribution. This is an
important observation that will be elaborated further in the next chapter.

10.7 Reversible Markov Chains
491
The Markov chain deÔ¨Åned by (10.19) does not satisfy the reversibility
condition. To verify this, consider, for example,
Pr (Xn = 1, Yn = 1, Xn‚àí1 = 0, Yn‚àí1 = 0)
= Pr (Xn = 1, Yn = 1|Xn‚àí1 = 0, Yn‚àí1 = 0)
√ó Pr (Xn‚àí1 = 0, Yn‚àí1 = 0)
= Pr (Xn = 1|Yn‚àí1 = 0) Pr (Yn = 1|Xn = 1)
√ó Pr (Xn‚àí1 = 0, Yn‚àí1 = 0)
= p1

p2
p1 + p2
p4
p2 + p4

.
(10.20)
On the other hand,
Pr (Xn = 0, Yn = 0, Xn‚àí1 = 1, Yn‚àí1 = 1)
= Pr (Xn = 0, Yn = 0|Xn‚àí1 = 1, Yn‚àí1 = 1)
√ó Pr (Xn‚àí1 = 1, Yn‚àí1 = 1)
= Pr (Xn = 0|Yn‚àí1 = 1) Pr (Yn = 0|Xn = 0)
√ó Pr (Xn‚àí1 = 1, Yn‚àí1 = 1)
= p4

p1
p1 + p3
p3
p3 + p4

.
(10.21)
Since in general, (10.20) Ã∏= (10.21), the ergodic Markov chain deÔ¨Åned by
(10.19) is not reversible.
The mechanism deÔ¨Åned by the transition probability (10.19) updates the
variables (X, Y ) one at a time in a systematic manner. While (10.19) does
not generate a reversible Markov chain, notice that each individual update
represents a transition probability that deÔ¨Ånes a reversible Markov chain.
For example
Pr (Xn = 0, Yn‚àí1 = 1)
=
Pr (Xn = 0|Yn‚àí1 = 1) Pr (Yn‚àí1 = 1)
=
p2
p1 + p2
(p1 + p2) = p2,
which is equal to the time reversed transition
Pr (Xn‚àí1 = 1, Yn = 0)
=
Pr (Yn = 0|Xn‚àí1 = 1) Pr (Xn‚àí1 = 1)
=
p2
p2 + p4
(p2 + p4) = p2,
and this holds for all possible values of (X, Y ). As discussed in the next
chapter, the Markov chain deÔ¨Åned by (10.19) is an example of the system-
atic scan, single-site Gibbs sampler.
‚ñ†

492
10. An Overview of Discrete Markov Chains
10.8
Limiting Behavior of Discrete, Finite
State-Spaces, Ergodic Markov Chains
In Section 10.6 it was stated that ergodic Markov chains converge to a
unique equilibrium distribution œÄ that satisÔ¨Åes
œÄ‚Ä≤ = œÄ‚Ä≤P.
(10.22)
This implies that œÄ‚Ä≤ is a left eigenvector with eigenvalue 1. (From linear
algebra, recall that if œÄ‚Ä≤ is a left eigenvector of P, if it satisÔ¨Åes œÄ‚Ä≤P =ŒªœÄ‚Ä≤,
where Œª is the associated eigenvalue. Setting Œª = 1 retrieves (10.22), so
that an equilibrium distribution must be a left eigenvector with eigenvalue
1. On the other hand, a right eigenvector satisÔ¨Åes PœÄ =ŒªœÄ). Since the
evolution of the Markov chain is governed by its transition probability,
insight into the limiting behavior of the Markov chain amounts to studying
the properties of Pn as n ‚Üí‚àû, which is the topic of this Ô¨Ånal section.
Below, use is made of the Perron-Frobenius theorem from linear algebra
(Karlin and Taylor, 1975), which implies that ergodic matrices have one
simple eigenvalue equal to 1, and all other eigenvalues have absolute value
less than 1. It follows that œÄ in (10.22) is the unique stationary distribu-
tion of the Markov chain. However, the proof of convergence and rate of
convergence of the ergodic Markov chain to œÄ requires a little more work.
First suppose that matrix P, of dimension m√óm, has distinct eigenvalues
Œªi, i = 1, 2, . . . , m. Let C = (c1, c2, . . . , cm) be the matrix whose columns
ci represent the corresponding m linearly independent right eigenvectors.
Then it is a standard result of matrix theory (Karlin and Taylor, 1975)
that
P = CDC‚àí1,
(10.23)
where the rows of C‚àí1 are the left eigenvectors of P and D is a diagonal
matrix with diagonal elements consisting of the eigenvalues Œªi. That is,
D = diag {Œª1, Œª2, . . . , Œªm} .
The square of P is given by
P2
=
CDC‚àí1CDC‚àí1
=
CD2C‚àí1,
and, in general,
Pn = CDnC‚àí1.
(10.24)
Consider (10.23). Since D is diagonal it can be written in the form:
D = Œª1I1 + Œª2I2 + ¬∑ ¬∑ ¬∑ + ŒªmIm,

10.8 Limiting Behavior
493
where Ii is a square matrix with all elements equal to zero except for the
term on the diagonal corresponding to the intersection of the ith row and
ith column, which is equal to 1. Substituting in (10.23) yields
P
=
Œª1CI1C‚àí1 + Œª2CI2C‚àí1 + ¬∑ ¬∑ ¬∑ + ŒªmCImC‚àí1
=
Œª1Q1 + Œª2Q2 + ¬∑ ¬∑ ¬∑ + ŒªmQm,
(10.25)
where Qi = CIiC‚àí1 and has the following properties:
(a)
QiQi = CIiC‚àí1CIiC‚àí1 = Qi,
i = 1, 2, . . . , m;
(b)
QiQj = CIiC‚àí1CIjC‚àí1 = CIiIjC‚àí1 = 0,
i Ã∏= j.
Using these properties, it follows that (10.24) can be written as
Pn = Œªn
1Q1 + Œªn
2Q2 + ¬∑ ¬∑ ¬∑ + Œªn
mQm.
(10.26)
Without loss of generality, assume that the Ô¨Årst term on the right-hand side
corresponds to the largest eigenvalue in absolute terms. At this point we
assume that P is aperiodic and irreducible and therefore ergodic. Here we
invoke the Perron-Frobenius theorem which allows to set in (10.26), Œª1 = 1
and |Œªi| < 1, i > 1. Therefore, for large n, Pn converges to Q1 which is
unique; that is,
lim
n‚Üí‚àûPn = Q1,
since all terms in (10.26), with the exception of the Ô¨Årst one, tend to 0 as
n tends to inÔ¨Ånity. The rate of convergence to Q1 depends on the value of
Œªi with the largest modulus, other than Œª1 = 1.
We now show that Q1 = 1œÄ‚Ä≤, where 1 is a column vector of 1‚Ä≤s of
dimension m. A little manipulation yields
Q1
=
CI1C‚àí1
=
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞
c11
0
. . .
0
c21
0
. . .
0
...
0
...
0
cm1
0
. . .
0
Ô£π
Ô£∫Ô£∫Ô£∫Ô£ª
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞
c11
c12
. . .
c1m
c21
c22
. . .
c2m
...
...
...
...
cm1
cm2
. . .
cmm
Ô£π
Ô£∫Ô£∫Ô£∫Ô£ª, (10.27)
where ci1 (i = 1, 2, . . . , m) is the ith element of c1 (the right eigenvector
associated with the eigenvalue equal to 1) and cji is the ith element of the
jth row of matrix C‚àí1. The Ô¨Årst row of C‚àí1 is the left eigenvector with
eigenvalue equal to 1. In connection with (10.22), it was argued that this
Ô¨Årst row of C‚àí1 is equal to the stationary distribution œÄ. Now, expanding
(10.27) yields
Q1 =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞
c11( c11
c12
. . .
c1m )
c21( c11
c12
. . .
c1m )
...
cm1( c11
c12
. . .
c1m )
Ô£π
Ô£∫Ô£∫Ô£∫Ô£ª.

494
10. An Overview of Discrete Markov Chains
Since the elements of the rows of Q1 deÔ¨Åne the stationary distribution,
they must add to 1. It follows that
c11 = c21 = ¬∑ ¬∑ ¬∑ = cm1 = w1, say.
Hence, Q1 has identical rows ,œÄ‚Ä≤ say, where ,œÄ is some equilibrium distribu-
tion equal to
,œÄ‚Ä≤ = w1

c11
c12
. . .
c1m 
= w1œÄ‚Ä≤.
We know that the equilibrium distribution is the left eigenvector with eigen-
value 1, with m
j=1 c1j = 1; therefore w1 = 1 and ,œÄ = œÄ is the unique
equilibrium distribution.
In the development above it was assumed that P could be written in the
form (10.23), where D is diagonal. When this is not possible, one can Ô¨Ånd
a matrix B via a Jordan decomposition that has the form
B =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞
1
0
. . .
0
0
...
M
0
Ô£π
Ô£∫Ô£∫Ô£∫Ô£ª
such that Pn= CBnC‚àí1 where Mn ‚Üí0 (Cox and Miller, 1965). Then, in
the same way as in the previous case,
lim
n‚Üí‚àûPn = lim
n‚Üí‚àûCBnC‚àí1 = C
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞
1
0
. . .
0
0
...
0
0
Ô£π
Ô£∫Ô£∫Ô£∫Ô£ªC‚àí1 = 1œÄ‚Ä≤.
Example 10.5
A three-state ergodic Markov chain
Consider the stochastic, ergodic matrix in Example 10.1. The matrix has
three distinct eigenvalues
Œª1 = 1,
Œª2 = 1
12

4 ‚àí
‚àö
10

,
Œª3 = 1
12

4 +
‚àö
10

,
and the corresponding right eigenvectors are
c‚Ä≤
1
=
[1, 1, 1] ,
c‚Ä≤
2
=
1
2

1 +
‚àö
10

, 1
4

‚àí4 ‚àí
‚àö
10

, 1

,
c‚Ä≤
3
=
1
2

1 ‚àí
‚àö
10

, 1
4

‚àí4 +
‚àö
10

, 1

.
Forming the diagonal matrix D = diag {Œª1, Œª2, Œª3} with the three eigen-
values, and forming matrix C with columns corresponding to the three

10.8 Limiting Behavior
495
eigenvectors, it can be veriÔ¨Åed readily that, up to four decimal places,
P =
Ô£Æ
Ô£ØÔ£∞
1
2
1
2
0
1
4
1
2
1
4
0
1
3
2
3
Ô£π
Ô£∫Ô£ª= CDC‚àí1
=
Ô£Æ
Ô£∞
1
2.0811
‚àí1.0811
1
‚àí1.7906
‚àí0.2094
1
1
1
Ô£π
Ô£ªD
Ô£Æ
Ô£∞
0.2222
0.4444
0.3333
0.1699
‚àí0.2925
0.1225
‚àí0.3922
‚àí0.1519
0.5442
Ô£π
Ô£ª.
Further, expression (10.26) is of the following form:
Pn
=
1n
Ô£Æ
Ô£∞
0.2222
0.4444
0.3333
0.2222
0.4444
0.3333
0.2222
0.4444
0.3333
Ô£π
Ô£ª
+

 1
12

4 ‚àí
‚àö
10
n
Q2 +

 1
12

4 +
‚àö
10
n
Q3,
which shows that the two transient terms in the second line tend rapidly
to zero as n increases.
‚ñ†
Example 10.6
A periodic, irreducible Markov chain
Consider the chain introduced in Example 10.2 with transition probability
matrix
P =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£∞
0
1
0
0
0
1
2
0
1
2
0
0
0
1
2
0
1
2
0
0
0
1
2
0
1
2
0
0
0
1
0
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
.
This matrix has period d = 2. The Ô¨Åve distinct eigenvalues are
Œª1 = ‚àí1,
Œª2 = 0,
Œª3 = 1,
Œª4 = ‚àí1
‚àö
2,
Œª5 =
1
‚àö
2.
The matrix C whose columns are the corresponding right eigenvectors is
C =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£∞
1
1
1
‚àí1
‚àí1
‚àí1
0
1
1
‚àö
2
‚àí1
‚àö
2
1
‚àí1
1
0
0
‚àí1
0
1
‚àí1
‚àö
2
1
‚àö
2
1
1
1
1
1
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
.
Forming the diagonal matrix D whose diagonal elements are the Ô¨Åve eigen-
values Œª1, . . . , Œª5, one can verify result (10.23).

496
10. An Overview of Discrete Markov Chains
For this example, (10.26) takes the following form:
Pn = (‚àí1)n
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£∞
0.125
‚àí0.25
0.25
‚àí0.25
0.125
‚àí0.125
0.25
‚àí0.25
0.25
‚àí0.125
0.125
‚àí0.25
0.25
‚àí0.25
0.125
‚àí0.125
0.25
‚àí0.25
0.25
‚àí0.125
0.125
‚àí0.25
0.25
‚àí0.25
0.125
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
+ 1n
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£∞
0.125
0.25
0.25
0.25
0.125
0.125
0.25
0.25
0.25
0.125
0.125
0.25
0.25
0.25
0.125
0.125
0.25
0.25
0.25
0.125
0.125
0.25
0.25
0.25
0.125
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
+

‚àí1
‚àö
2
n
Q4 +

 1
‚àö
2
n
Q5,
which shows that, for large n, the eigenvectors associated with the eigenval-
ues with absolute value less than 1 become irrelevant. Also, as illustrated
before, the value of Pn varies depending on whether n is even or odd. The
unique stationary distribution for this Markov chain exists, but the chain
fails to converge to it.
‚ñ†
The next chapter discusses MCMC methods. Here, one creates a Markov
chain using random number generators. This particular chain is constructed
in such a way that, eventually, the numbers drawn can be interpreted as
samples from the stationary distribution œÄ, which is often a posterior distri-
bution. The key to the right construction process is the choice of transition
kernel which governs the behavior of the system. Many transition kernels
can be used for a particular problem, and, typically, they are all derived
by imposing the condition of reversibility. Then provided that the Markov
chain is ergodic, and only then, a reversible system has œÄ as its unique sta-
tionary distribution. Convergence to this distribution however, takes place
asymptotically. Many factors, such as the choice of transition kernel, the
degree of correlation of the parameters of the model in their posterior distri-
butions, and the structure of the data available, can interfere with a smooth
trip towards the Ô¨Ånal goal. This general area is discussed an illustrated in
the chapters ahead.

11
Markov Chain Monte Carlo
11.1
Introduction
Markov chain Monte Carlo (MCMC) has become a very important compu-
tational tool in Bayesian statistics, since it allows inferences to be drawn
from complex posterior distributions where analytical or numerical inte-
gration techniques cannot be applied. The idea underlying these methods
is to generate a Markov chain via iterative Monte Carlo simulation that
has, at least in an asymptotic sense, the desired posterior distribution as
its equilibrium or stationary distribution. An important paper in this area
is Tierney (1994).
The classic papers of Metropolis et al. (1953) and of Hastings (1970) gave
rise to a very general MCMC method, namely the Metropolis‚ÄìHastings al-
gorithm, of which the Gibbs sampler, which was introduced in statistics
and given its name by Geman and Geman (1984), is a special case. In
Hastings (1970), the algorithm is used for the simulation of discrete equi-
librium distributions on a space of Ô¨Åxed dimension. In a statistical setting,
Ô¨Åxed dimensionality implies that the number of parameters of the model is
a known value. The reversible jump MCMC algorithm introduced in Green
(1995) is a generalization of Metropolis‚ÄìHastings; it has been applied for
simulation of equilibrium distributions on spaces of varying dimension. The
number of parameters of a model is inferred via its marginal posterior dis-
tribution.
In this chapter, the focus is on these MCMC algorithms. Much of the
material is drawn from the tutorial paper of Waagepetersen and Sorensen

498
11. Markov Chain Monte Carlo
(2001). Markov chains with continuous state spaces are considered here;
that is, the random variables are assumed to be distributed continuously.
Many results for discrete state spaces discussed in Chapter 10 hold also, but
some technical modiÔ¨Åcations are needed for Markov chains with continuous
state spaces. For example, convergence to a unique stationary distribution
requires the continuous Markov chain to be irreducible and aperiodic, as for
discrete chains. In addition, a continuous Markov chains must be ‚ÄúHarris
recurrent‚Äù. This is a measure theoretical technicality required to avoid the
possibility of starting points from which convergence is not assured. This
property shall not be discussed here, and the reader is referred to Meyn
and Tweedie (1993), to Tierney (1994), and to Robert and Casella (1999).
This chapter is organized as follows. The following section introduces
notation and terminology, since the continuous state space situation re-
quires some additional details. Subsequently, an overview of MCMC is pre-
sented, including a description of the Metropolis‚ÄìHastings, Gibbs sampling,
Langevin-Hastings and reversible jump methods. The chapter ends with a
short description of the data augmentation strategy, a technique that may
lead to simpler computational expressions in the process of Ô¨Åtting a prob-
ability model via MCMC.
11.2
Preliminaries
11.2.1
Notation
The notation in this chapter is the standard one found in the literature
on MCMC, and it deviates somewhat from that used in the rest of this
book. Here no notational distinction is made between scalar and vector
random variables, but this should not lead to ambiguities. In this chapter,
the probability of the event E is denoted by P (E) rather than by Pr (E).
Suppose that Z = (Z1, . . . , Zd) is a real random vector of dimension
d ‚â•1. Resuming the notation introduced in (1.1), we shall say that Z has
density f on Rd if the probability that Z belongs to a subset A of Rd is
given by the d-fold integral
P(Z ‚ààA) =
 ‚àû
‚àí‚àû
 ‚àû
‚àí‚àû
¬∑ ¬∑ ¬∑
 ‚àû
‚àí‚àû
I((z1, . . . , zd) ‚ààA)f(z1, . . . , zd) dz1 ¬∑ ¬∑ ¬∑ dzd,
where the indicator function I((z1, . . . , zd) ‚ààA) is one if (z1, . . . , zd) is in A,
and zero otherwise. If, for example, A = [a1, b1] √ó [a2, b2], then the integral
is
 
I(x ‚àà[a1, b1], y ‚àà[a2, b2])f(x, y) dx dy =
 b1
a1
 b2
a2
f(x, y) dx dy.

11.2 Preliminaries
499
Integrals will usually be written in abbreviated form
P(Z ‚ààA) =

I(z ‚ààA)f(z) dz =

A
f(z) dz
whenever the dimension of the integral is clear from the context.
11.2.2
Transition Kernels
In the case of Markov chains with continuous state spaces, it does not
make sense to write p (i, j) = Pr (Xn = j|Xn‚àí1 = i), since this probability
is always null. Further, the fact that X is continuously distributed precludes
construction of transition probability matrices and transition kernels are
used instead. Assume that X is a p-dimensional stochastic vector with
density f on Rp and assume that A is a subset in Rp. Then the transition
kernel P(¬∑, ¬∑) speciÔ¨Åes the conditional probability that Xn+1 falls in A given
that Xn = x and will be written as
P (x, A) = P (Xn+1 ‚ààA|Xn = x) .
(11.1)
This notation (departing from what has been used before in the book)
is required for reasons mentioned below.
11.2.3
Varying Dimensionality
Let Y be a stochastic vector in Rq. In general, the joint probability that
X ‚ààA and Y ‚ààB will be written as
P (X ‚ààA, Y ‚ààB) =

A
P (x, B) f (x) dx
(11.2)
for all subsets A ‚äÜRp and B ‚äÜRq. If Y has density in Rq, then (X, Y )
has joint density h on Rp+q. In this case, (11.2) is the well-known identity
P (X ‚ààA, Y ‚ààB) =

A

B
h (x, y) dx dy.
(11.3)
However, as discussed in the sections describing single-site Metropolis‚Äì
Hastings updates and reversible jump samplers, there are situations when
the q-dimensional stochastic vector Y has density on Rq‚Ä≤, q‚Ä≤ < q. In this
case, (X, Y ) does not have density h on Rp+q and, consequently, the joint
probability P (X ‚ààA, Y ‚ààB) cannot be calculated from (11.3) and one
needs to refer to (11.2). As an example of the latter, consider computing
for the two-dimensional vector Y = (Y1, Y2)‚Ä≤ and the scalar variable X:
P (Y ‚ààB|X = x)
(11.4)

500
11. Markov Chain Monte Carlo
Suppose that given X = x, Y is deÔ¨Åned by the deterministic mapping g
(Y1, Y2) = g (x, U) = (x + U, x ‚àíU) ,
where U is a stochastic one-dimensional random variable with density q
on R. Given x, Y does not have density on R2 since it can only take
values on the line y1 = 2x ‚àíy2; that is, once y2 is known, the value of
y1 is determined completely. Then (11.4) must be computed from the one-
dimensional integral
P (Y ‚ààB|X = x) = P (x, B) =

I ((x + U, x ‚àíU) ‚ààB) q (u) du
and P (X ‚ààA, Y ‚ààB), A ‚äÜR, B ‚äÜR2, is computed from (11.2):
P (X ‚ààA, Y ‚ààB) =

A

I ((x + U, x ‚àíU) ‚ààB) q (u) f (x) du dx.
If B = [a1, b1] √ó [a2, b2], this becomes
P (X ‚ààA, Y ‚ààB)
=

A

I (x + U ‚àà[a1, b1] , x ‚àíU ‚àà[a2, b2]) q (u) f (x) du dx.
On the other hand, if [Y1, Y2, X] had density h on R3, one could use the
standard formula
P (X ‚ààA, Y ‚ààB) =

A
 b1
a1
 b2
a2
h (x, y1, y2) dx dy1 dy2.
11.3
An Overview of Markov Chain Monte Carlo
The MCMC algorithms to be described below are devices for constructing
a Markov chain that has the desired equilibrium distribution as its limit-
ing invariant distribution. In the previous chapter on Markov chains, the
transition probability matrix P was known, and one wished to character-
ize the equilibrium distribution. The nature of the problem with MCMC
methods is the opposite one: the equilibrium distribution is known (usually
up to proportionality), but the transition kernel is unknown. How does one
construct a transition kernel that generates a Markov chain with a desired
stationary distribution?
Let X denote a real stochastic vector of unknown parameters or unob-
servable variables associated with some model, and assume it has a dis-
tribution with density œÄ on Rd. The density œÄ could represent a posterior
density. This density has typically a complex form, such that the necessary

11.3 An Overview of Markov Chain Monte Carlo
501
expectations with respect to œÄ cannot be evaluated analytically or by using
standard techniques for numerical integration. In particular, œÄ may only
be known up to an unknown normalizing constant. Direct simulation of œÄ
may be diÔ¨Écult, but as shown below, it is usually quite easy to construct
a Markov chain whose invariant distribution has density given by œÄ. The
Markov chain X1, X2, . . . is speciÔ¨Åed in terms of the distribution for the
initial state X1 and the transition kernel P(¬∑, ¬∑) which speciÔ¨Åes the condi-
tional distribution of Xi+1 given the previous state Xi. If the value of the
current state is Xi = x, then the probability that Xi+1 is in a set A ‚äÜRd is
given by (11.1). If the generated Markov chain is irreducible with invariant
distribution œÄ, it can be used for Monte Carlo estimation of various ex-
pectations E (h (X)) with respect to œÄ (Tierney, 1994; Meyn and Tweedie,
1993). That is, for any function h on Rd with Ô¨Ånite expectation E (h (X)),
E (h (X)) =

h (x) œÄ (x) dx = lim
N‚Üí‚àû
1
N
N

i=1
h (Xi) .
(11.5)
Thus, E (h (X)) can be approximated by
E (h (X)) ‚âà
N

i=1
h (Xi)
N
,
(11.6)
the sample average, for some large N. Here h could be the indicator function
of a set A ‚äÜRd, so that E (h (X)) equals the probability
P (X ‚ààA) = E (I (X ‚ààA)) ,
which is approximated by N
i=1 I (Xi ‚ààA) /N . (Convergence of the sample
average to E (h (X)) for all starting values, also requires the assumption
of Harris recurrence). Result (11.5) does not rest upon the condition of
aperiodicity, even though this is required for convergence of the Markov
chain to œÄ.
As discussed in the previous chapter, the density œÄ is invariant for the
Markov chain if the transition kernel P(¬∑, ¬∑) of the Markov chain preserves
œÄ, i.e., if Xi ‚àºœÄ implies Xi+1 ‚àºœÄ or, in terms of P(¬∑, ¬∑) and œÄ, if

Rd P (x, B) œÄ (x) dx =

B
œÄ (x) dx.
(11.7)
Both terms of this expression are equal to the marginal probability that
X ‚ààB, provided that œÄ is the invariant density. In order to verify that œÄ
is the invariant density using (11.7) is a diÔ¨Écult task, since this involves
integration with respect to œÄ. The infeasibility of doing this was the reason
for using MCMC in the Ô¨Årst place. However, choosing a transition kernel
which imposes the stronger condition of reversibility with respect to œÄ is

502
11. Markov Chain Monte Carlo
suÔ¨Écient to guarantee that œÄ is invariant for the Markov chain. In the
previous chapter it was indicated that reversibility holds if (Xi, Xi+1) has
the same distribution as the time-reversed subchain (Xi+1, Xi) whenever
Xi has density œÄ, i.e., if
P (Xi+1 ‚ààA, Xi ‚ààB) =

B
P (x, A) œÄ (x) dx
= P (Xi+1 ‚ààB, Xi ‚ààA) =

A
P (x, B) œÄ (x) dx
(11.8)
for subsets A, B ‚äÜRd. Taking A = Rd, the integral in the Ô¨Årst line be-
comes

B P

x, Rd
œÄ (x) dx =

B œÄ (x) dx, since P

x, Rd
= 1. Therefore
the reversibility condition (11.8) implies (11.7). The Metropolis‚ÄìHastings
algorithm or, more generally, the reversible jump MCMC, oÔ¨Äers a practical
recipe for constructing a reversible Markov chain with the desired invariant
distribution by an appropriate choice of a transition kernel.
11.4
The Metropolis‚ÄìHastings Algorithm
11.4.1
An Informal Derivation
An informal and intuitive derivation of the Metropolis‚ÄìHastings algorithm
will be given Ô¨Årst. This is followed by a more formal approach which eases
the way into reversible jump MCMC.
Since direct sampling from the target distribution may not be possible,
the Metropolis‚ÄìHastings algorithm starts by generating candidate draws
from a so-called proposal distribution. These draws are then ‚Äúcorrected‚Äù
so that they behave, asymptotically, as random observations from the de-
sired equilibrium or target distribution. The Markov chain produced by
the Metropolis‚ÄìHastings algorithm at each stage is thus constructed in two
steps: a proposal step and an acceptance step. These two steps are asso-
ciated with the proposal distribution and with the acceptance probability
which are the two ingredients of the Metropolis‚ÄìHastings transition kernel.
Assume that at stage n the state of the chain is Xn = x. The random vari-
able X may be a scalar or a vector. The next state of the chain is chosen
by Ô¨Årst sampling a candidate point Yn+1 = y from a proposal distribution
with p.d.f. q (x, ¬∑). The density q (x, ¬∑) may (or may not) depend on the cur-
rent point x. If it does, q (x, y) is a conditional p.d.f. of y, given x, which
in other parts of this text has been referred to as p (y|x). For example,
q (x, ¬∑) may be a multivariate normal distribution with mean x and Ô¨Åxed
covariance matrix. The candidate point y is then accepted with probability
a (x, y) to be derived below. If y is accepted, then the next state becomes
Xn+1 = y. If the candidate point is rejected, the chain does not move, i.e.,
Xn+1 = x. The algorithm is extremely simple:

11.4 The Metropolis‚ÄìHastings Algorithm
503
INITIALIZE X0
DO i = 1, n
SAMPLE Y FROM q (x, ¬∑)
SAMPLE U FROM Un (0, 1)
IF (U ‚â§a (X, Y )) SET Xi = y
OTHERWISE SET Xi = x
ENDDO
We turn to the informal derivation of the acceptance probability a (x, y),
drawing from the tutorial of Chib and Greenberg (1995). For a joint up-
dating Metropolis‚ÄìHastings algorithm in equilibrium, the random vector
(Xn, Yn+1), consisting of the current Markov chain state and the proposal,
has joint density g, given by
g (x, y) = q (x, y) œÄ (x) ,
(11.9)
where œÄ is the equilibrium density and q is the proposal density of Yn+1,
given that Xn has value x. If q satisÔ¨Åes the reversibility condition such that
q (x, y) œÄ (x) = q (y, x) œÄ (y)
(11.10)
for all (x, y) , then the proposal density is the correct transition kernel of
the Metropolis‚ÄìHastings chain. Most likely it will be the case that, for some
(x, y),
q (x, y) œÄ (x) > q (y, x) œÄ (y) ,
(11.11)
say. In order to achieve equality and thus ensuring reversibility, one can
introduce a probability a (x, y) < 1 on the left-hand side, such that tran-
sitions from x to y (x Ã∏= y) are made according to q (x, y) a (x, y). Setting
a (y, x) = 1 on the right-hand side yields
q (x, y) œÄ (x) a (x, y)
=
q (y, x) œÄ (y) a (y, x)
=
q (y, x) œÄ (y)
from which the acceptance probability becomes
a (x, y) = œÄ (y) q (y, x)
œÄ (x) q (x, y).
(11.12)
If inequality (11.11) is reversed, a probability a (y, x) is introduced appro-
priately and derived as above, after setting a (x, y) = 1. The probabilities
a guarantee that detailed balance is satisÔ¨Åed. These arguments imply that
the probability of acceptance must be
a (x, y) = min

1, œÄ (y) q (y, x)
œÄ (x) q (x, y)

,
œÄ (x) q (x, y) > 0.
(11.13)
Notice that if (11.10) holds, a (x, y) = 1 and the candidate draw y is
accepted. This is equivalent to sampling the candidate point from the equi-
librium distribution.

504
11. Markov Chain Monte Carlo
A key observation is that the posterior distribution œÄ must be known up
to proportionality only, since the normalizing constant cancels in the ratio
œÄ (y) /œÄ(x). Finally, it is clear from (11.13) that when symmetrical proposal
densities are considered with q (y, x) = q (x, y), the acceptance probability
reduces to
a (x, y) = min

1, œÄ (y)
œÄ (x)

,
œÄ (x) > 0,
which is the case originally considered by Metropolis et al. (1953).
A few comments about implementation are in order. The acceptance
probability is clearly deÔ¨Åned provided œÄ (x) q (x, y) > 0. If the chain starts
with a value x0 such that œÄ (x0) > 0, then œÄ (xn) > 0 for every n, since val-
ues of the proposal Y for which œÄ (y) = 0 lead to an acceptance probability
equal to zero, and are therefore rejected. The success of the method depends
on striking some ‚Äúright‚Äù rate of acceptance. Parameterization and choice of
the proposal density play a fundamental role here. Acceptance ratios in the
neighborhood of 1 imply very similar values between previous and proposed
states and the chain will move very slowly (unless, of course, the proposal
distribution is the equilibrium distribution, leading to an acceptance ratio
of 1!). On the other hand, if the proposed displacement is too large and
falls where the posterior has no support, this will lead to a high rejection
rate. Here, the chain will remain in the same state for many iterations.
No general optimization rules are available. However, Roberts et al. (1997)
obtained optimal acceptance rates of 23.4% for high-dimensional problems
under quite general conditions. Guidance on the proper choice of proposal
densities can be found, for example, in Chib and Greenberg (1995).
11.4.2
A More Formal Derivation
A more formal derivation of the acceptance probability (11.13) is given
here; this will be useful in understanding reversible jump MCMC later
on. A distinction is made between two implementations of the Metropolis‚Äì
Hastings algorithm. These are the simultaneous and the single-site updat-
ing schemes. In the former scheme all the random variables of the model are
updated jointly, whereas in the latter, random variables are updated one at
a time. In the single-site updating strategy, and in common with reversible
jump MCMC, the proposal distribution and the target distribution have
densities on spaces of diÔ¨Äerent dimension. The informal derivation of the
previous section was based on the simultaneous updating algorithm.
Metropolis‚ÄìHastings Simultaneous Updating Algorithm
As before, let Xn denote the nthe state of a Metropolis‚ÄìHastings chain
X1, X2, . . . and let Yn+1 denote the proposal for the next state of the chain.
The random vector (Xn, Yn+1) has joint density g on R2d given by (11.9),
where œÄ is the d-dimensional target density and q (x, ¬∑) is the d‚àídimensional

11.4 The Metropolis‚ÄìHastings Algorithm
505
proposal density of Yn+1, given that X has the value x ‚ààRd. In this sec-
tion the acceptance probability of the simultaneous updating Metropolis‚Äì
Hastings algorithm is derived subject to the reversibility condition
P (Xn ‚ààA, Xn+1 ‚ààB) = P (Xn ‚ààB, Xn+1 ‚ààA)
(11.14)
for all A, B ‚äÜRd. The left-hand side of (11.14) can be written as
P (Xn ‚ààA, Xn+1 ‚ààB) =

A
P (Xn+1 ‚ààB|Xn = x) œÄ (x) dx.
(11.15)
For any B ‚äÜRd deÔ¨Åne the proposal distribution as
Q (x, B) = P (Yn+1 ‚ààB|Xn = x) =

I (y ‚ààB) q (x, y) dy
(11.16)
which is the conditional probability that Yn+1 belongs in a set B, given
that Xn = x. Also deÔ¨Åne
Qa (x, B)
=
P (Yn+1 ‚ààB and Yn+1 is accepted|Xn = x)
=

I (y ‚ààB) q (x, y) a (x, y) dy
(11.17)
as the conditional probability that Yn+1 belongs in a set B and Yn+1 is
accepted, given that Xn = x and deÔ¨Åne further
s (x) = P (Yn+1 is rejected|Xn = x)
(11.18)
as the conditional probability of rejecting the proposal, given that Xn = x.
Then the transition kernel P (Xn+1 ‚ààB|Xn = x) can be written as
P (Xn+1 ‚ààB|Xn = x) = Qa (x, B) + s (x) I (x ‚ààB) .
(11.19)
This is by virtue of the law of total probability, since there are two ways
in which Xn+1 ‚ààB. One is generating a proposal Yn+1 that belongs in B
and that this candidate is accepted; the other one is rejecting the proposal,
so that the new state Xn+1 = Xn = x, and that x ‚ààB. Hence (11.15)
becomes equal to
P (Xn ‚ààA, Xn+1 ‚ààB)
=

A
Qa (x, B) œÄ (x) dx +

A
s (x) I (x ‚ààB) œÄ (x) dx.
(11.20)
By virtue of symmetry, the right-hand side of (11.14) is given by
P (Xn ‚ààB, Xn+1 ‚ààA)
=

B
Qa (x‚Ä≤, A) œÄ (x‚Ä≤) dx‚Ä≤ +

B
s (x‚Ä≤) I (x‚Ä≤ ‚ààA) œÄ (x‚Ä≤) dx‚Ä≤.
(11.21)

506
11. Markov Chain Monte Carlo
The second term on the right-hand side of (11.20) can be written as

A
s (x) I (x ‚ààB) œÄ (x) dx =

s (x) I (x ‚ààB ‚à©A) œÄ (x) dx
(11.22)
and the second term on the right-hand side of (11.21) can similarly be
written as

B
s (x‚Ä≤) I (x‚Ä≤ ‚ààA) œÄ (x‚Ä≤) dx‚Ä≤ =

s (x‚Ä≤) I (x‚Ä≤ ‚ààB ‚à©A) œÄ (x‚Ä≤) dx‚Ä≤. (11.23)
Clearly, (11.22) = (11.23) (notice that x and x‚Ä≤ are dummy variables of
integration), and therefore the reversibility condition is satisÔ¨Åed if

A
Qa (x, B) œÄ (x) dx =

B
Qa (x‚Ä≤, A) œÄ (x‚Ä≤) dx‚Ä≤.
(11.24)
Using deÔ¨Ånition (11.17) for Qa (x, B), the left-hand side of (11.24) can be
written more explicitly as

A
Qa (x, B) œÄ (x) dx =

A

I (y ‚ààB) q (x, y) a (x, y) œÄ (x) dx dy
=
 
I (x ‚ààA, y ‚ààB) q (x, y) a (x, y) œÄ (x) dx dy,
(11.25)
and, similarly, for the right-hand side of (11.24):

B
Qa (x‚Ä≤, A) œÄ (x‚Ä≤) dx‚Ä≤
=
 
I (x‚Ä≤ ‚ààB, y‚Ä≤ ‚ààA) q (x‚Ä≤, y‚Ä≤) a (x‚Ä≤, y‚Ä≤) œÄ (x‚Ä≤) dx‚Ä≤ dy‚Ä≤.
(11.26)
In order to write (11.25) and (11.26) explicitly as functions of the same
variables, one can make the change of variables (y = x‚Ä≤) and (x = y‚Ä≤) ‚àísee
the Note below. Since the Jacobian of the transformation is 1, the right-
hand side of (11.26) now becomes
 
I (y ‚ààB, x ‚ààA) q (y, x) a (y, x) œÄ (y) dy dx.
Substituting in (11.24) yields the following expression for the reversibility
condition:
 
I (x ‚ààA, y ‚ààB) q (x, y) a (x, y) œÄ (x) dx dy
=
 
I (y ‚ààB, x ‚ààA) q (y, x) a (y, x) œÄ (y) dy dx.
Equality is satisÔ¨Åed if
q (x, y) a (x, y) œÄ (x) = q (y, x) a (y, x) œÄ (y) .

11.4 The Metropolis‚ÄìHastings Algorithm
507
Choosing the acceptance probability as large as possible (i.e., setting a (y, x)
equal to 1) subject to detailed balance, as suggested by Peskun (1973) yields
a (x, y) = min

1, q (y, x) œÄ (y)
q (x, y) œÄ (x)

.
(11.27)
Note In moving from Xn = x to Xn+1 = x‚Ä≤, a proposal with realized
value y is generated from q (x, ¬∑). If the proposal is accepted, x‚Ä≤ = y. In
the opposite move, from Xn = x‚Ä≤ to Xn+1 = x, the proposal with realized
value y‚Ä≤ is generated from q (x‚Ä≤, ¬∑). If the proposal is accepted, x = y‚Ä≤.
Metropolis‚ÄìHastings Single-Site Updating Algorithm
In the single-site updating algorithm, only one component of Xn ‚ààRd is
updated at a time. Then, given that Xn = x, Yn+1 equals x except at
the ithe component, where xi is replaced by a random variable Zi gener-
ated from a one-dimensional proposal density qi (x, ¬∑) that may or may not
depend on x or on a subset of x. Since
Yn+1 ‚ààB ‚áî(x1, . . . , xi‚àí1, Zi, xi+1, . . . , xd) ‚ààB,
then the probability that Yn+1 belongs in B ‚äÜRd, given Xn = x, is given
by the proposal distribution
Q (x, B) = P (Yn+1 ‚ààB|Xn = x)
=

I ((x1, . . . , xi‚àí1, zi, xi+1, . . . , xd) ‚ààB) qi (x, zi) dzi
(11.28)
which is a one-dimensional integral. Notice that target density œÄ lives on
Rd, while the proposal density qi (x, ¬∑) lives on R.
Consider the move from a realized value equal to
x = (x1, . . . , xi‚àí1, xi, xi+1, . . . , xd)
and to a realized value equal to
x‚Ä≤ = (x1, . . . , xi‚àí1, zi, xi+1, . . . , xd) .
Note that in the notation used here, contrary to the case in the previous
section, x‚Ä≤ diÔ¨Äers from x in one element only. The probability that Xn+1
belongs in B ‚äÜRd, given Xn = x, is given by
P (Xn+1 ‚ààB|Xn = x) = Qa (x, B) + s (x) I (x ‚ààB) ,
(11.29)
where Qa (x, B) = P (Yn+1 ‚ààB and Yn+1 is accepted|Xn = x) is here equal
to
Qa (x, B) =

I (x‚Ä≤ ‚ààB) a (x, x‚Ä≤) qi (x, zi) dzi.
(11.30)

508
11. Markov Chain Monte Carlo
The second term in (11.29) is s (x) = P (Yn+1 is rejected|Xn = x).
The left-hand side of the reversibility equation (11.14) can be written as
follows:
P (Xn ‚ààA, Xn+1 ‚ààB)
=

A
Qa (x, B) œÄ (x) dx +

A
s (x) I (x ‚ààB) œÄ (x) dx.
Similarly, for the move in the opposite direction, the right-hand side of
(11.14) is given by
P (Xn ‚ààB, Xn+1 ‚ààA)
=

B
Qa (x‚Ä≤, A) œÄ (x‚Ä≤) dx‚Ä≤ +

B
s (x‚Ä≤) I (x‚Ä≤ ‚ààA) œÄ (x‚Ä≤) dx‚Ä≤,
where Xn has the realized value x‚Ä≤. As in the previous section, the second
terms in the right-hand side of these expressions can be shown to be equal.
Therefore, reversibility is satisÔ¨Åed if

A
Qa (x, B) œÄ (x) dx =

B
Qa (x‚Ä≤, A) œÄ (x‚Ä≤) dx‚Ä≤,
(11.31)
where
Qa (x‚Ä≤, A) = P (Yn+1 ‚ààA and Yn+1 is accepted|Xn = x‚Ä≤)
=

I (x ‚ààA) qi (x‚Ä≤, xi) a (x‚Ä≤, x) dxi.
(11.32)
Substituting (11.30) in the left-hand side of (11.31) yields

A

I (x‚Ä≤ ‚ààB) qi (x, zi) a (x, x‚Ä≤) œÄ (x) dzi dx
=

Rd

R
I (x‚Ä≤ ‚ààB, x ‚ààA) qi (x, zi) a (x, x‚Ä≤) œÄ (x) dzi dx.
(11.33)
Similarly, substituting (11.32) in the right-hand side of (11.31) yields:

Rd

R
I (x ‚ààA, x‚Ä≤ ‚ààB) qi (x‚Ä≤, xi) a (x‚Ä≤, x) œÄ (x‚Ä≤) dx‚Ä≤ dxi.
(11.34)
A condition for equality of (11.33) and (11.34) is that
qi (x, zi) a (x, x‚Ä≤) œÄ (x) = qi (x‚Ä≤, xi) a (x‚Ä≤, x) œÄ (x‚Ä≤) .
As in (11.27), using the criterion due to Peskun (1973) yields the acceptance
probability
a (x, x‚Ä≤) = min

1, qi (x‚Ä≤, xi) œÄ (x‚Ä≤)
qi (x, zi) œÄ (x)

.
(11.35)
The arguments above also hold when the updating variable, rather than
being a scalar, is a vector and a subset of x.

11.5 The Gibbs Sampler
509
11.5
The Gibbs Sampler
The Gibbs sampler is a very popular MCMC algorithm because of its com-
putational simplicity. As shown below, it is a special case of the Metropolis‚Äì
Hastings algorithm. In order to see this, in the move from x to x‚Ä≤, let the
proposal be generated from
qi (x, zi) = œÄ (zi|x‚àíi) ,
(11.36)
where x‚àíi is equal to x with its ith component deleted, that is, x‚àíi =
(x1, . . . , xi‚àí1, xi+1, . . . , xd). Similarly, in the opposite move, let the proposal
be a draw from
qi (x‚Ä≤, xi) = œÄ (xi|x‚àíi) .
(11.37)
In a Bayesian context, the right-hand sides of (11.36) and (11.37) are known
as the fully conditional posterior distributions. Since
œÄ (zi|x‚àíi) = œÄ (x‚Ä≤) /œÄ (x‚àíi)
and
œÄ (xi|x‚àíi) = œÄ (x) /œÄ (x‚àíi) ,
substituting in (11.35) yields
qi (x‚Ä≤, xi) œÄ (x‚Ä≤)
qi (x, zi) œÄ (x) = œÄ (x) œÄ (x‚Ä≤) œÄ (x‚àíi)
œÄ (x) œÄ (x‚Ä≤) œÄ (x‚àíi) = 1.
Therefore, a Metropolis‚ÄìHastings proposal generated from the appropri-
ate fully conditional distribution (a conditional posterior distribution in
the Bayesian setting), is accepted always. This scheme is known as Gibbs
sampling.
The transition kernel of the Gibbs sampler for the updating of all ele-
ments of x involves the product
œÄ (z1|x2, x3, . . . , xd) œÄ (z2|z1, x3, . . . , xd) . . . œÄ (zd|z1, z2, . . . , zd‚àí1) .
The transition kernel of the Gibbs sampler preserves œÄ. To see this and for
notational simplicity, let d = 3. Then, in terms of (11.7):
P (x, B) =

I (z1, z2, z3 ‚ààB) œÄ (z1|x2, x3) œÄ (z2|z1, x3)
√óœÄ (z3|z1, z2) dz1 dz2 dz3,
where it is clear from the context that the integral is three-dimensional.
Substituting in the left-hand side of (11.7) and integrating over the distri-
bution of x1, x2, and x3 yields

P (x, B) œÄ (x1, x2, x3) dx1 dx2 dx3
=

I (z1, z2, z3 ‚ààB) œÄ (z1, z2, z3) dz1 dz2 dz3
= P (X1, X2, X3 ‚ààB) .

510
11. Markov Chain Monte Carlo
Equation (11.7) is also satisÔ¨Åed if the transition kernel is deÔ¨Åned with
respect to only one of the elements of x. To verify this, and in terms of
this three-dimensional example, the transition kernel for the updating of
the Ô¨Årst element of x is now
P (x, B) =

I (z1, x2, x3 ‚ààB) œÄ (z1|x2, x3) dz1
and the left-hand side of equation (11.7) is

P (x, B) œÄ (x) dx
=
 
I (z1, x2, x3 ‚ààB) œÄ (z1|x2, x3) œÄ (x1, x2, x3) dz1 dx1 dx2 dx3.
Integrating over the distribution of x1 yields

I (z1, x2, x3 ‚ààB) œÄ (z1, x2, x3) dz1 dx2 dx3
= P (X1, X2, X3 ‚ààB) .
11.5.1
Fully Conditional Posterior Distributions
Consider the vector of parameters (Œ∏1, Œ∏2, . . . , Œ∏p) whose posterior distribu-
tion is proportional to p (Œ∏1, . . . , Œ∏i‚àí1, Œ∏i, Œ∏i+1, . . . , Œ∏p|y). Let
Œ∏‚àíi = (Œ∏1, . . . , Œ∏i‚àí1, Œ∏i+1, . . . , Œ∏p)
be the vector of dimension (p ‚àír), p > r, r ‚â•1, which is equal to Œ∏ with
its ith component, Œ∏i, deleted, and where r is the number of elements in
Œ∏i. The fully conditional posterior distribution of Œ∏i is
p (Œ∏i|Œ∏‚àíi, y) =
p (Œ∏1, . . . , Œ∏i‚àí1, Œ∏i, Œ∏i+1, . . . , Œ∏p|y)

p (Œ∏1, . . . , Œ∏i‚àí1, Œ∏i, Œ∏i+1, . . . , Œ∏p|y) dŒ∏i
‚àùp (Œ∏1, . . . , Œ∏i‚àí1, Œ∏i, Œ∏i+1, . . . , Œ∏p|y) .
(11.38)
In many applications, r = 1 and parameters are updated one at a time. In
general, single-site updating leads to moves along each coordinate, whereas
updating several components in a block allows for more general moves.
Joint updating, which incorporates information on the correlation structure
among the components in the joint conditional posterior distribution, can
result in faster convergence when correlations are strong (Liu et al., 1994).
11.5.2
The Gibbs Sampling Algorithm
Consider the vector of parameters of a model (Œ∏1, Œ∏2, . . . , Œ∏p), with posterior
density p (Œ∏1, Œ∏2, . . . , Œ∏p|y), known up to proportionality. Assume that the

11.5 The Gibbs Sampler
511
user supplies ‚Äúlegal‚Äù starting values

Œ∏(0)
1 , Œ∏(0)
2 , . . . , Œ∏(0)
p

,
in the sense that p

Œ∏(0)
1 , Œ∏(0)
2 , . . . , Œ∏(0)
p |y

> 0. The implementation of the
Gibbs sampler consists of iterating through the loop:
draw Œ∏(1)
1
from p

Œ∏1|Œ∏(0)
2 , . . . , Œ∏(0)
p , y

,
draw Œ∏(1)
2
from p

Œ∏2|Œ∏(1)
1 , Œ∏(0)
3 , . . . , Œ∏(0)
p , y

,
draw Œ∏(1)
3
from p

Œ∏3|Œ∏(1)
1 , Œ∏(1)
2 , Œ∏(0)
4 , . . . , Œ∏(0)
p , y

,
...
draw Œ∏(1)
p
from p

Œ∏p|Œ∏(1)
1 , . . . , Œ∏(1)
p‚àí1, y

,
draw Œ∏(2)
1
from p

Œ∏1|Œ∏(1)
2 , . . . , Œ∏(1)
p , y

,
...
and so on.
After an initial period during which samples are dependent on the starting
value (burn in period), the draws Œ∏(i)
1 , Œ∏(i)
2 , . . . , Œ∏(i)
p , for suÔ¨Éciently large i,
are regarded as samples from the normalized posterior distribution with
density
p (Œ∏1, Œ∏2, . . . , Œ∏p|y)/

p (Œ∏1, Œ∏2, . . . , Œ∏p|y) dŒ∏1 . . . dŒ∏p.
The coordinate Œ∏(i)
j
is regarded as a draw from its marginal posterior dis-
tribution with density
p (Œ∏j|y)/

p (Œ∏j|y) dŒ∏j.
The Fully Conditional Distributions Determine the Joint Distribution
The Gibbs sampler produces draws from a joint distribution by sampling
successively from all fully conditional posterior distributions. This implies
that the form of the fully conditional distributions determines uniquely the
form of the joint distribution. The main idea is sketched below for the two-
dimensional case. The general case is known as the Hammersley‚ÄìCliÔ¨Äord
in the spatial statistics literature (Besag, 1974).
Consider the identity
p (x, y) = p (y|x) p (x) = p (x|y) p (y) .
(11.39)

512
11. Markov Chain Monte Carlo
From (11.39), it follows that
p (y) = p (y|x)
p (x|y)p (x) ‚àùp (y|x)
p (x|y).
(11.40)
The normalized marginal density is
p (y) =
p (y|x)/ p (x|y)

p (y|x)/ p (x|y) dy .
Substituting in (11.39) yields
p (x, y) =
p (y|x)

p (y|x)/ p (x|y) dy .
This shows that p (x, y) can be expressed in terms of the conditional distri-
butions (even though it may not be possible to write the joint distribution
explicitly). This result is based on the implicit assumption that the joint
distribution [X, Y ] exists.
Example 11.1
A single observation from a bivariate normal distribution
with known covariance matrix
As a trivial example, consider data that consist of a single observation
y = (y1, y2) from a bivariate normal distribution with unknown mean
Œ∏ = (Œ∏1, Œ∏2)
and known covariance matrix
V =

1
œÅ
œÅ
1

.
Thus,
y|Œ∏, V ‚àºN (Œ∏, V) .
One wishes to obtain draws from p (Œ∏|y, V). Let the prior distribution for Œ∏
be proportional to a constant, independent of Œ∏. Then the posterior density
of Œ∏ is
p (Œ∏|y, V)
‚àù
p (Œ∏) p (y|Œ∏, V)
‚àù
p (y|Œ∏, V) ,
(11.41)
a bivariate normal distribution. The stochastic element in this distribution
is the vector Œ∏, for Ô¨Åxed data y and covariance matrix V. When regarded
as a function of Œ∏, (11.41) is the density of a normal distribution with mean
y and variance V. That is
Œ∏|y, V ‚àºN2 (y, V) .
(11.42)

11.5 The Gibbs Sampler
513
Implementation of the Gibbs sampler in a single-site updating scheme re-
quires drawing successively from
Œ∏1|Œ∏2, y, V ‚àºN

y1 + œÅ (Œ∏2 ‚àíy2) , 1 ‚àíœÅ2
(11.43)
and from
Œ∏2|Œ∏1, y, V ‚àºN

y2 + œÅ (Œ∏1 ‚àíy1) , 1 ‚àíœÅ2
.
(11.44)
After a number of rounds of iteration (the burn-in period) the system con-
verges to the stationary distribution (11.42). At this point, the samples
obtained from (11.43) are Monte Carlo draws from p (Œ∏1|y, V) and those
from (11.44) are Monte Carlo draws from p (Œ∏2|y, V), the densities of the
respective marginal posterior distributions. The samples (Œ∏1, Œ∏2) are corre-
lated draws from p (Œ∏|y, V). This correlation among the samples slows down
convergence and increases the Monte Carlo sampling error of estimates of
features of the posterior distribution. On the other hand, joint updating
involves sampling from (11.42); in this example, the system converges in
one round. In the joint updating implementation, the pairs (Œ∏1, Œ∏2) are
independent draws from p (Œ∏|y, V). This is equivalent to direct sampling
from the target distribution of interest. Obviously, one would not employ
MCMC in such a situation.
‚ñ†
Example 11.2
A hierarchical Bayesian model
Consider the two-parameter Bayesian model
yi|¬µ, œÉ2 ‚àºN

¬µ, œÉ2
,
i = 1, . . . , n, ‚àí‚àû< ¬µ < ‚àû, œÉ2 > 0,
where ¬µ and œÉ2 are the unknown mean and variance, respectively. These
are assumed to be a priori independent, with prior distributions equal to
¬µ ‚àºN (0, 1)
and
œÉ2|S, v ‚àºvSœá‚àí2
v .
That is, the mean is assigned a normal (0, 1) prior, and the variance a
scaled inverted chi-square distribution with (assumed known) parameters
S and v. Under conditional independence, the likelihood is
p

y|¬µ, œÉ2
=
n
-
i=1

2œÄœÉ2‚àí1
2 exp

‚àí(yi ‚àí¬µ)2
2œÉ2

=

2œÄœÉ2‚àín
2 exp
Ô£Æ
Ô£ØÔ£ØÔ£∞‚àí
n
i=1
(yi ‚àí¬µ)2
2œÉ2
Ô£π
Ô£∫Ô£∫Ô£ª,

514
11. Markov Chain Monte Carlo
and the joint posterior density is given by
p

¬µ, œÉ2|y

‚àùp (¬µ) p

œÉ2
p

y|¬µ, œÉ2
‚àùexp

‚àí¬µ2
2
 
œÉ2‚àí( v
2 +1) exp

‚àívS
2œÉ2
 
œÉ2‚àín
2
√ó exp
Ô£Æ
Ô£ØÔ£ØÔ£∞‚àí
n
i=1
(yi ‚àí¬µ)2
2œÉ2
Ô£π
Ô£∫Ô£∫Ô£ª.
(11.45)
Implementing the Gibbs sampler requires knowledge of the fully conditional
posterior distributions with densities p

¬µ|œÉ2, y

and p

œÉ2|¬µ, y

‚àíomitting
the conditioning on hyperparameters S and v.
The derivation of the fully conditional posterior distribution of ¬µ requires
extracting terms which are function of ¬µ from the joint posterior density
(11.45). This leads to
p

¬µ|œÉ2, y

‚àùexp

‚àí¬µ2
2

exp
Ô£Æ
Ô£ØÔ£ØÔ£∞‚àí
n
i=1
(yi ‚àí¬µ)2
2œÉ2
Ô£π
Ô£∫Ô£∫Ô£ª
= exp

‚àí¬µ2
2

exp
Ô£Æ
Ô£ØÔ£ØÔ£∞‚àí
n
i=1
[(yi ‚àí5¬µ) + (5¬µ ‚àí¬µ)]2
2œÉ2
Ô£π
Ô£∫Ô£∫Ô£ª,
(11.46)
where 5¬µ = n
i=1 yi/n. Expanding the square in the second term and noting
that

i
(yi ‚àí5¬µ) (5¬µ ‚àí¬µ) = (5¬µ ‚àí¬µ)

i
(yi ‚àí5¬µ) = 0,
expression (11.46) reduces to
p

¬µ|œÉ2, y

‚àùexp

‚àíœÉ2¬µ2 + n (¬µ ‚àí5¬µ)2
2œÉ2

.
(11.47)
Using the identity (i.e., Box and Tiao, 1973, page 74):
A (z ‚àía)2 + B (z ‚àíb)2 = (A + B) (z ‚àíc)2 +
AB
A + B (a ‚àíb)2
with c = (Aa + Bb) /(A+B), and associating œÉ2 with A, n with B, ¬µ with
z, 0 with a and 5¬µ with b, then (11.47) can be rewritten as
p

¬µ|œÉ2, y

‚àùexp

‚àí

œÉ2 + n

(¬µ ‚àím)2
2œÉ2

,
(11.48)

11.5 The Gibbs Sampler
515
where m = n5¬µ/

œÉ2 + n

. By inspection, (11.48) is recognized as the ker-
nel of the density of a normal distribution with mean m and variance
œÉ2/

œÉ2 + n

. Therefore,
¬µ|œÉ2, y ‚àºN

n5¬µ
(œÉ2 + n),
œÉ2
(œÉ2 + n)

.
(11.49)
In order to derive p

œÉ2|¬µ, y

, terms including œÉ2 are extracted from the
joint posterior density (11.45); this leads to
p

œÉ2|¬µ, y

‚àù

œÉ2‚àí( v
2 +1) exp

‚àívS
2œÉ2
 
œÉ2‚àín
2 exp
Ô£Æ
Ô£ØÔ£ØÔ£∞‚àí
n
i=1
(yi ‚àí¬µ)2
2œÉ2
Ô£π
Ô£∫Ô£∫Ô£ª
=

œÉ2‚àí( v
2 +1) exp

‚àí,v ,S
2œÉ2

,
where ,S = (vS + n
i=1 (yi ‚àí¬µ)2)/,v, and ,v = v + n. This is the kernel of
the density of a scaled inverted chi-square distribution with parameters ,v
and ,S. In brief,
œÉ2|¬µ, y ‚àº,v ,Sœá‚àí2
v .
(11.50)
Generation of a sample from (11.50) requires drawing a chi-square devi-
ate with ,v degrees of freedom, inverting this value, and multiplying it by

vS + n
i=1 (yi ‚àí¬µ)2
. One cycle of the Gibbs sampling algorithm consists
of drawing from (11.49), computing the quantities

vS + n
i=1 (yi ‚àí¬µ)2
using the realized value in place of ¬µ, and Ô¨Ånally drawing from (11.50). At
convergence, ¬µ(i) and œÉ2(i) are elements of the ith sample from the marginal
distributions [¬µ|y] and

œÉ2|y

, respectively.
‚ñ†
Example 11.3
A bivariate normal model with unknown covariance matrix
Let yi = (yi1, yi2)‚Ä≤ , (i = 1, . . . , n), represent n independent samples from
the bivariate normal distribution with mean vector ¬µ = (¬µ1, ¬µ2) and vari-
ance deÔ¨Åned by the 2 √ó 2 matrix V. That is,
p (y|¬µ, V) = |2œÄV|‚àín
2 exp

‚àí1
2
n

i=1
(yi ‚àí¬µ)‚Ä≤ V‚àí1 (yi ‚àí¬µ)

= (2œÄ)‚àín |V|‚àín
2 exp

‚àí1
2tr

V‚àí1S

,
(11.51)
where
S =
Ô£Æ
Ô£∞

i
(yi1 ‚àí¬µ1)2

i
(yi1 ‚àí¬µ1) (yi2 ‚àí¬µ2)

i
(yi1 ‚àí¬µ1) (yi2 ‚àí¬µ2)

i
(yi2 ‚àí¬µ2)2
Ô£π
Ô£ª.

516
11. Markov Chain Monte Carlo
Take a two-dimensional uniform distribution as prior for ¬µ, and let the
prior for V be the scaled inverted Wishart distribution with density
p (V|V0, v) ‚àù|V|‚àí1
2 (v+3) exp

‚àí1
2tr

V‚àí1V‚àí1
0

,
where V0 and v are hyperparameters assumed known. The joint posterior
density of the parameters of this model (suppressing the dependence on V0
and v in the notation) is
p (¬µ, V|y) ‚àùp (y|¬µ, V) p (V)
= (2œÄ)‚àín |V|‚àín
2 exp

‚àí1
2tr

V‚àí1S

|V|‚àí1
2 (v+3)
√ó exp

‚àí1
2tr

V‚àí1V‚àí1
0

.
(11.52)
Extracting the terms in ¬µ from (11.52) yields as conditional posterior den-
sity
p (¬µ|V, y) ‚àùexp

‚àí1
2
n

i=1
(yi ‚àí¬µ)‚Ä≤ V‚àí1 (yi ‚àí¬µ)

= exp

‚àí1
2
n

i=1
[(yi ‚àíy) + (y ‚àí¬µ)]‚Ä≤ V‚àí1 [(yi ‚àíy) + (y ‚àí¬µ)]

‚àùexp

‚àí1
2
n

i=1
(y ‚àí¬µ)‚Ä≤ V‚àí1 (y ‚àí¬µ)

= exp

‚àí1
2n (y ‚àí¬µ)‚Ä≤ V‚àí1 (y ‚àí¬µ)

,
where y‚Ä≤ = (y1, y2) =

n‚àí1 n
i=1 yi1, n‚àí1 n
i=1 yi2

. Therefore, from in-
spection, the fully conditional posterior distribution of the vector ¬µ is
¬µ|V, y ‚àºN

y, n‚àí1V

.
(11.53)
The fully conditional distribution of the covariance matrix is derived from
(11.52) as well. The terms in V are
p (V|¬µ, y) ‚àù|V|‚àín
2 exp

‚àí1
2tr

V‚àí1S

|V|‚àí1
2 (v+3)
√ó exp

‚àí1
2tr

V‚àí1V‚àí1
0

= |V|‚àí1
2 (v+3+n) exp

‚àí1
2tr

V‚àí1 
S + V‚àí1
0

,

11.6 Langevin‚ÄìHastings Algorithm
517
which is the kernel of a scaled inverted Wishart distribution with parame-
ters v + n and

S + V‚àí1
0
‚àí1. Thus,
V|¬µ, y ‚àºIW

S + V‚àí1
0
‚àí1 , v + n

.
(11.54)
‚ñ†
11.6
Langevin‚ÄìHastings Algorithm
A general algorithm that, in principle, allows joint updates for the complete
parameter vector of a model (œï, say, of length r), is based on the Langevin‚Äì
Hastings algorithm (Besag, 1994). The idea is to generate a proposal vector
œï(t+1) at cycle t + 1, from a candidate generating density deÔ¨Åned by the
normal process
N

œï(t)+Œ≥
2
‚àÇ
‚àÇœï(t) ln p

œï(t)|y

, IŒ≥

,
(11.55)
where Œ≥ is a scalar tuning parameter, I is the r √ó r identity matrix and
ln p (œï|y) is the log-posterior density, usually known up to proportional-
ity. Note that the vector œï includes all the parameters of the model. For
example in a Gaussian process, this includes location and dispersion pa-
rameters. The proposal is then accepted using the Metropolis‚ÄìHastings
acceptance ratio (11.27). The use of the gradient of the log-target density
in the proposal distribution can lead to much better convergence properties
than, for example, the simple random walk Metropolis‚ÄìHastings proposal
kernel N

œï(t), IŒ≥

. The gradient in (11.55) is supposed to improve moves
toward the mode of p (œï|y). Further developments and improvements of
this approach were presented by Stramer and Tweedie (1998).
11.7
Reversible Jump MCMC
One motivation for reversible jump MCMC is to provide a more general
recipe than Metropolis‚ÄìHastings to simulate from posterior distributions
on spaces of varying dimension; these arise naturally when the number of
parameters of the object of inference is not Ô¨Åxed. An example is inferences
concerning the number of quantitative trait loci (QTL) aÔ¨Äecting a trait,
using genetic markers. This number can be treated as a random variable.
The Markov chain is allowed to jump across states of diÔ¨Äerent dimension,
and each state is characterized by a particular number of QTL. The distri-
bution of the proportion of times that the chain has spent across the various
states is a Monte Carlo estimate of the marginal posterior distribution of

518
11. Markov Chain Monte Carlo
the number of QTL aÔ¨Äecting the trait. This is illustrated in Chapter 16.
Another classical example is the number of densities appearing in a mixture
distribution, a problem studied by Richardson and Green (1997).
Reversible jump though, is not limited to simulation of invariant dis-
tributions that have densities on spaces of diÔ¨Äerent dimensions. The algo-
rithm can also be used when the models posed have the same number of
parameters. An example of this situation is given at the end of this chap-
ter involving a gamma and a lognormal model. Of course, if the number
of competing models is small, other approaches may be computationally
more eÔ¨Écient than reversible jump.
The reversible jump algorithm was introduced by Green (1995), who il-
lustrates possible applications of reversible jump with several examples.
The paper is rather technical and a reader unfamiliar with measure theory
will Ô¨Ånd it diÔ¨Écult to fully grasp the details. Waagepetersen and Sorensen
(2001) present a simple, self-contained derivation of reversible jump in a
tutorial style and avoiding measure theoretical details. This section is based
on the latter paper. The derivation of the acceptance probability to moves
between spaces of possibly diÔ¨Äerent dimension presented here, follows es-
sentially the same steps as for the Metropolis‚ÄìHastings acceptance proba-
bility: using the reversibility condition as the point of departure, Ô¨Årst, the
transition kernel is expressed in terms of a proposal distribution and an ac-
ceptance probability. Second, a change of variable is performed that allows
both sides of the detailed balance equation to be expressed in terms of the
same parameters. The change of variable is made possible by a monotone
transformation; in the case of the reversible jump, this requires that the
dimension matching condition is fulÔ¨Ålled, as explained below. Identifying
the conditions for equality between the probability of opposite moves and,
thus, for detailed balance to hold, leads to the Ô¨Ånal step in the derivation
of the acceptance probability.
Below, a step-by-step derivation of the reversible jump acceptance prob-
ability is presented. The notation is a little involved since there is a need
to account for the fact that parameters change as the Markov chain jumps
from one model to the next. Two examples are presented at the end of
this chapter and a further application of reversible jump can be found in
Chapter 16 on QTL analysis. In one of the examples, two models with a
diÔ¨Äerent number of parameters are considered. In the other, the number
of parameters is the same in both models, illustrating the generality of the
algorithm for model choice.
11.7.1
The Invariant Distribution
Here œÄ denotes the joint probability distribution of (M, Z), where M ‚àà
{1, 2, . . . , I} is a ‚Äúmodel indicator‚Äù and Z is a real stochastic vector, pos-
sibly of varying dimension (I represents either a Ô¨Ånite integer or ‚àû). The
models can be diÔ¨Äerent because of their parametric form but the number

11.7 Reversible Jump MCMC
519
of parameters do not need to diÔ¨Äer. The vector Z takes values in the set
C deÔ¨Åned as the union C = ‚à™I
m=1Cm of spaces Cm = Rnm, (nm > 1).
Given M = m, Z can only take values in Cm, so that œÄ is speciÔ¨Åed by
pm = P (M = m) and densities f (¬∑|M = m) on Cm, (m = 1, 2, . . .). Thus,
for Am ‚äÜCm, the joint probability distribution of (M, Z) is
P (M = m, Z ‚ààAm)
=
P (M = m) P (Z ‚ààAm|M = m)
=
pm

Am
f (z|M = m) dz.
(11.56)
The density f (¬∑|M = m) is denoted fm hereinafter.
If a number of competing models are posed, then pm may represent the
posterior probability of model m, and given M = m, fm is the poste-
rior density of the nm-dimensional vector Z of parameters associated with
model m. In this case
pmfm = c‚àí1,pmh (z|m) l (y|m, z) ,
(11.57)
where ,pm is the prior probability of model m, h (z|m) is the prior density
of Z given M = m, l (y|m, z) is the likelihood of the data y given (M, Z) =
(m, z), and c is the overall (typically unknown) normalizing constant
c =
I

m=1
,pm

Cm
h (z|m) l (y|m, z) dz.
(11.58)
11.7.2
Generating the Proposal
In the joint updating Metropolis‚ÄìHastings algorithm, the candidate point
Yn+1 ‚ààRd for the new state Xn+1 ‚ààRd, is generated from the d-dimen-
sional proposal density q (x, ¬∑). In the single-site updating, the d-dimen-
sional candidate point
Yn+1 = (x1, x2, . . . , xi‚àí1, Zi, xi+1, . . . , xd)
is generated by drawing the random variable Zi from the one-dimensional
proposal density qi (x, ¬∑). With some abuse of notation, Yn+1 above can be
written
Yn+1 = g (x1, x2, . . . , xi‚àí1, Zi, xi+1, . . . , xd) ,
where the function g is the identity mapping.
In the context of reversible jump, each state Xi of the chain contains
two components, i.e., Xi = (Mi, Zi), where Mi is the model indicator and
where Zi is a stochastic vector in CMi. Suppose that (m, z) is the value of
the current state Xn of the Markov chain and a move to the value (m‚Ä≤, z‚Ä≤)
is considered for the next state Xn+1. A proposal Yn+1 =

Y ind
n+1, Y par
n+1

for
Xn+1 is generated as described below, where superscripts ind and par are

520
11. Markov Chain Monte Carlo
labels for the proposal of the model indicator Mn+1 and for the vector Zn+1,
respectively. With user-deÔ¨Åned probability pmm‚Ä≤,
I
m‚Ä≤=1 pmm‚Ä≤ = 1

, the
proposal Y ind
n+1 is set equal to m‚Ä≤, and given Y ind
n+1 = m‚Ä≤, the proposal Y par
n+1
is generated in Cm‚Ä≤. A very general mechanism is to construct the proposal
Y par
n+1 by applying a deterministic mapping g1mm‚Ä≤ to the previous value z
and to a random component U. This mechanism can be formulated as
Y par
n+1 = g1mm‚Ä≤ (z, U) ,
(11.59)
where U is a random vector which has density qmm‚Ä≤ (z, ¬∑) on Rnmm‚Ä≤. The
proposal Yn+1 is Ô¨Ånally accepted with an acceptance probability
amm‚Ä≤ 
z, Y par
n+1

,
which is derived below.
When considering a move from a state (m, z) to
(m‚Ä≤, z‚Ä≤) = (m‚Ä≤, g1mm‚Ä≤ (z, u)) ,
and a move in the opposite direction from (m‚Ä≤, z‚Ä≤) to
(m, z) = (m, g1m‚Ä≤m (z‚Ä≤, u‚Ä≤)) ,
the vectors (z, u) and (z‚Ä≤, u‚Ä≤) must be of equal dimension. That is, the
dimension matching condition
nm + nmm‚Ä≤ = nm‚Ä≤ + nm‚Ä≤m
(11.60)
needs to be fulÔ¨Ålled. Further, it will be assumed that there exist functions
g2mm‚Ä≤ and g2m‚Ä≤m such that the mapping gmm‚Ä≤, given by
(z‚Ä≤, u‚Ä≤) = gmm‚Ä≤ (z, u) = (g1mm‚Ä≤ (z, u) , g2mm‚Ä≤ (z, u)) ,
(11.61)
is one-to-one with
(z, u) = g‚àí1
mm‚Ä≤ (z‚Ä≤, u‚Ä≤)
= gm‚Ä≤m (z‚Ä≤, u‚Ä≤) = (g1m‚Ä≤m (z‚Ä≤, u‚Ä≤) , g2m‚Ä≤m (z‚Ä≤, u‚Ä≤))
(11.62)
and that gmm‚Ä≤ is diÔ¨Äerentiable. The transformations (11.61) and (11.62) are
possible because the mapping gmm‚Ä≤ is one-to-one with gm‚Ä≤m; a necessary
condition for the existence of the one-to-one mapping is that the dimension
matching (11.60) holds.
11.7.3
Specifying the Reversibility Condition
Assuming Xn = (Mn, Zn) ‚àºœÄ, the condition of reversibility is
P (Mn = m, Zn ‚ààAm, Mn+1 = m‚Ä≤, Zn+1 ‚ààBm‚Ä≤)
= P (Mn = m‚Ä≤, Zn ‚ààBm‚Ä≤, Mn+1 = m, Zn+1 ‚ààAm)
(11.63)

11.7 Reversible Jump MCMC
521
for all m, m‚Ä≤ ‚àà{1, 2, . . . , I}, and all subsets Am and Bm‚Ä≤ of Cm and Cm‚Ä≤,
respectively. In analogy with (11.15), the left-hand side of (11.63) is
P (Mn = m, Zn ‚ààAm, Mn+1 = m‚Ä≤, Zn+1 ‚ààBm‚Ä≤)
=

Am
P (Mn+1 = m‚Ä≤, Zn+1 ‚ààBm‚Ä≤|Xn = (m, z)) pmfm (z) dz,
(11.64)
where P (Mn+1 = m‚Ä≤, Zn+1 ‚ààBm‚Ä≤|Xn = (m, z)) is the transition kernel. As
in (11.17), let
Qa
mm‚Ä≤ (z, Bm‚Ä≤)
= P

Y ind
n+1 = m‚Ä≤, Y par
n+1 ‚ààBm‚Ä≤ and Yn+1 is accepted|Xn = (m, z)

be the joint probability of generating the proposal Yn+1 with Y ind
n+1 = m‚Ä≤
and Y par
n+1 in Bm‚Ä≤ and accepting the proposal, given that the current state
of the Markov chain is Xn = (m, z). Also, as in (11.18), let
sm (z) = P (Yn+1 is rejected|Xn = (m, z))
be the probability of rejecting the proposal. Then the transition kernel can
be written as
P (Mn+1 = m‚Ä≤, Zn+1 ‚ààBm‚Ä≤|Xn = (m, z))
=
Qa
mm‚Ä≤ (z, Bm‚Ä≤) + sm (z) I (m = m‚Ä≤, z ‚ààBm‚Ä≤) .
Substituting in (11.64), the left-hand side of (11.63) equals
pm

Am
Qa
mm‚Ä≤ (z, Bm‚Ä≤) fm (z) dz
+ pm

Am
sm (z) I (m = m‚Ä≤, z ‚ààBm‚Ä≤) fm (z) dz
= pm

Am
Qa
mm‚Ä≤ (z, Bm‚Ä≤) fm (z) dz
+ pm

sm (z) I (m = m‚Ä≤, z ‚ààAm ‚à©Bm‚Ä≤) fm (z) dz,
(11.65)
where
pm

Am
Qa
mm‚Ä≤ (z, Bm‚Ä≤) fm (z) dz
= P

Mn = m, Zn ‚ààAm, Y ind
n+1 = m‚Ä≤, Y par
n+1 ‚ààBm‚Ä≤, Yn+1 is accepted

.
By symmetry, the right-hand side of (11.63) equals
pm‚Ä≤

Bm‚Ä≤
Qa
m‚Ä≤m (z‚Ä≤, Am) fm‚Ä≤ (z‚Ä≤) dz‚Ä≤
+pm‚Ä≤

sm‚Ä≤ (z‚Ä≤) I (m = m‚Ä≤, z‚Ä≤ ‚ààBm‚Ä≤ ‚à©Am) fm‚Ä≤ (z‚Ä≤) dz‚Ä≤.
(11.66)

522
11. Markov Chain Monte Carlo
The second terms in (11.65) and (11.66) are equal both in the case when
m Ã∏= m‚Ä≤ (in which case they are zero, because the indicator function takes
the value zero), and when m = m‚Ä≤ (in which case the move is within
the same model, and both expressions are identical). Therefore a suÔ¨Écient
condition for reversibility to hold is, for all m and m‚Ä≤,
pm

Am
Qa
mm‚Ä≤ (z, Bm‚Ä≤) fm (z) dz
= pm‚Ä≤

Bm‚Ä≤
Qa
m‚Ä≤m (z‚Ä≤, Am) fm‚Ä≤ (z‚Ä≤) dz‚Ä≤.
(11.67)
11.7.4
Derivation of the Acceptance Probability
Equation (11.67) is now written more explicitly. Since,
(a) Y ind
n+1 is set equal to m‚Ä≤ with probability pmm‚Ä≤;
(b) Y par
n+1 is generated in Cm‚Ä≤ and belongs in Bm‚Ä≤ implies Y par
n+1 ‚ààBm‚Ä≤ ‚áî
g1mm‚Ä≤ (z, U) = z‚Ä≤ ‚ààBm‚Ä≤;
(c) Yn+1 is accepted with probability amm‚Ä≤ (z, g1mm‚Ä≤ (z, U)) = amm‚Ä≤ (z, z‚Ä≤),
and U ‚àºqmm‚Ä≤ (z, ¬∑).
It follows that
Qa
mm‚Ä≤ (z, Bm‚Ä≤) = pmm‚Ä≤

I (z‚Ä≤ ‚ààBm‚Ä≤) amm‚Ä≤ (z, z‚Ä≤) qmm‚Ä≤ (z, u) du. (11.68)
The left-hand side of (11.67) is therefore
pm

Am
Qa
mm‚Ä≤ (z, Bm‚Ä≤) fm (z) dz
= pm

Am

I (z‚Ä≤ ‚ààBm‚Ä≤) pmm‚Ä≤amm‚Ä≤ (z, z‚Ä≤) qmm‚Ä≤ (z, u) fm (z) dz du
= pm
 
I (z ‚ààAm, z‚Ä≤ ‚ààBm‚Ä≤) pmm‚Ä≤
√óamm‚Ä≤ (z, z‚Ä≤) qmm‚Ä≤ (z, u) fm (z) dz du,
(11.69)
and, by symmetry, the right-hand side is
pm‚Ä≤

Bm‚Ä≤
Qa
m‚Ä≤m (z‚Ä≤, Am) fm‚Ä≤ (z‚Ä≤) dz‚Ä≤
= pm‚Ä≤
 
I (z‚Ä≤ ‚ààBm‚Ä≤, z ‚ààAm) pm‚Ä≤m
√óam‚Ä≤m (z‚Ä≤, z) qm‚Ä≤m (z‚Ä≤, u‚Ä≤) fm‚Ä≤ (z‚Ä≤) dz‚Ä≤ du‚Ä≤.
(11.70)
To study the conditions that satisfy reversibility and therefore equality of
(11.69) and (11.70), both equations will now be expressed as functions of the
same variables. This is possible due to the dimension matching assumption

11.7 Reversible Jump MCMC
523
and relationships (11.61) and (11.62). Using the fact that (see equations
(2.36) and (2.38) of Chapter 2)
dz‚Ä≤du‚Ä≤ = |det (g‚Ä≤
mm‚Ä≤ (z, u))| dz du,
(11.71)
where
g‚Ä≤
mm‚Ä≤ (z, u) = ‚àÇgmm‚Ä≤ (z, u)
‚àÇ(z, u)
=
Ô£Æ
Ô£ØÔ£∞
‚àÇg1mm‚Ä≤(z,u)
‚àÇdz
‚àÇg2mm‚Ä≤(z,u)
‚àÇz
‚àÇg1mm‚Ä≤(z,u)
‚àÇdu
‚àÇg2mm‚Ä≤(z,u)
‚àÇu
Ô£π
Ô£∫Ô£ª,
equation (11.70) can be written as
P (Mn = m‚Ä≤, Zn ‚ààBm‚Ä≤, Mn+1 = m, Zn+1 ‚ààAm)
=
 
I (z‚Ä≤ ‚ààBm‚Ä≤, z ‚ààAm) pm‚Ä≤mam‚Ä≤m (z‚Ä≤, z)
√óqm‚Ä≤m (z‚Ä≤, u‚Ä≤) pm‚Ä≤fm‚Ä≤ (z‚Ä≤) |det (g‚Ä≤
mm‚Ä≤ (z, u))| dz du.
(11.72)
By inspection, it is clear that equality between (11.69) and (11.72) is sat-
isÔ¨Åed if
pmm‚Ä≤amm‚Ä≤ (z, z‚Ä≤) qmm‚Ä≤ (z, u) pmfm (z)
= pm‚Ä≤mam‚Ä≤m (z‚Ä≤, z) qm‚Ä≤m (z‚Ä≤, u‚Ä≤) pm‚Ä≤fm‚Ä≤ (z‚Ä≤) |det (g‚Ä≤
mm‚Ä≤ (z, u))| .
Choosing the acceptance probability as large as possible, subject to the
detailed balance condition as suggested by Peskun (1973), yields
amm‚Ä≤ (z, z‚Ä≤)
= min

1, pm‚Ä≤mqm‚Ä≤m (z‚Ä≤, u‚Ä≤) pm‚Ä≤fm‚Ä≤ (z‚Ä≤)
pmm‚Ä≤qmm‚Ä≤ (z, u) pmfm (z)
""""det

‚àÇgmm‚Ä≤ (z, u)
‚àÇ(z, u)
""""

(11.73)
whenever pmm‚Ä≤qmm‚Ä≤ (z, u) pmfm (z) > 0 and where
(z‚Ä≤, u‚Ä≤) = gmm‚Ä≤ (z, u) .
In practice, pmm‚Ä≤qmm‚Ä≤ (z, u) pmfm (z) = 0 only happens if the Markov chain
is initialized in a state (m, z) for which pmfm (z) = 0. The acceptance
probability for a move from z‚Ä≤ to z is given by the inverse of (11.73).
11.7.5
Deterministic Proposals
Sometimes it may be simpler to apply deterministic proposals for a move
from Cm to Cm‚Ä≤, i.e., to let Yn+1 = g1mm‚Ä≤ (z), and still use a stochastic
proposal for the move in the opposite direction. In this case, the dimension
matching condition equals
nm = nm‚Ä≤ + nm‚Ä≤m
(11.74)

524
11. Markov Chain Monte Carlo
since nmm‚Ä≤ = 0. Equations (11.61) and (11.62) become
(z‚Ä≤, u‚Ä≤) = gmm‚Ä≤ (z) = (g1mm‚Ä≤ (z) , g2mm‚Ä≤ (z))
(11.75)
and
(z) = g‚àí1
mm‚Ä≤ (z‚Ä≤, u‚Ä≤) = gm‚Ä≤m (z‚Ä≤, u‚Ä≤) = g1m‚Ä≤m (z‚Ä≤, u‚Ä≤) .
(11.76)
That is, the change from state z to state z‚Ä≤, deÔ¨Åned by (11.75), does not
involve the generation of a stochastic variable U; the move is deterministic.
The move in the opposite direction, deÔ¨Åned by (11.76), requires U ‚Ä≤; this
move is stochastic. The reversibility condition has the same form as in
(11.67), but (11.68) is now given by
Qa
mm‚Ä≤ (z, Bm‚Ä≤)
=
pmm‚Ä≤I (g1mm‚Ä≤ (z) ‚ààBm‚Ä≤) amm‚Ä≤ (z, g1mm‚Ä≤ (z))
=
pmm‚Ä≤I (z‚Ä≤ ‚ààBm‚Ä≤) amm‚Ä≤ (z, z‚Ä≤) .
(11.77)
Substituting (11.77) in the left-hand side of (11.67):
pm

Am
Qa
mm‚Ä≤ (z, Bm‚Ä≤) fm (z) dz
=

I (z ‚ààAm, z‚Ä≤ ‚ààBm‚Ä≤) pmm‚Ä≤
√óamm‚Ä≤ (z, z‚Ä≤) pmfm (z) dz.
(11.78)
With a stochastic proposal for the opposite move, the right-hand side of
(11.67) is unchanged and is given by (11.70). The equivalent to (11.71) is
now
dz‚Ä≤ du‚Ä≤ = |det (g‚Ä≤
mm‚Ä≤ (z))| dz,
(11.79)
where
g‚Ä≤
mm‚Ä≤ (z) = ‚àÇgmm‚Ä≤ (z)
‚àÇz
.
Substituting (11.79) in (11.70), and using (11.78), yields the following ex-
pression for the detailed balance equation

I (z ‚ààAm, z‚Ä≤ ‚ààBm‚Ä≤) pmm‚Ä≤amm‚Ä≤ (z, z‚Ä≤) pmfm (z) dz
=

I (z‚Ä≤ ‚ààBm‚Ä≤, z ‚ààAm) pm‚Ä≤mam‚Ä≤m (z‚Ä≤, z)
√óqm‚Ä≤m (z‚Ä≤, u‚Ä≤) pm‚Ä≤fm‚Ä≤ (z‚Ä≤) |det (g‚Ä≤
mm‚Ä≤ (z))| dz,
(11.80)
where u‚Ä≤ = g2mm‚Ä≤ (z), a function of z. Using the same approach as before,
the acceptance probability is given now by
a (z, z‚Ä≤) = min

1, pm‚Ä≤mqm‚Ä≤m (z‚Ä≤, u‚Ä≤) pm‚Ä≤fm‚Ä≤ (z‚Ä≤)
pmm‚Ä≤pmfm (z)
|det (g‚Ä≤
mm‚Ä≤ (z))|

.
(11.81)

11.7 Reversible Jump MCMC
525
11.7.6
Generating Proposals via the Identity Mapping
In the development presented so far, the proposal in the move from (m, z) to
(m‚Ä≤, z‚Ä≤) is generated via the deterministic mapping (11.16). An alternative
to this approach is to let g1mm‚Ä≤ (z, U) be the identity mapping and to set
U = Z‚Ä≤, which results in
Y par
n+1 = Z‚Ä≤.
The random variable Z‚Ä≤ is generated from the density qmm‚Ä≤ (z, ¬∑) on Rnm‚Ä≤,
which may depend on the current value z. The expression equivalent to
(11.68) is now
Qa
mm‚Ä≤ (z, Bm‚Ä≤) = pmm‚Ä≤

I (z‚Ä≤ ‚ààBm‚Ä≤) amm‚Ä≤ (z, z‚Ä≤) qmm‚Ä≤ (z, z‚Ä≤) dz‚Ä≤. (11.82)
Then it is easy to show that under this strategy, the acceptance probability
is given by
amm‚Ä≤ (z, z‚Ä≤) = min

1, pm‚Ä≤mqm‚Ä≤m (z‚Ä≤, z) pm‚Ä≤fm‚Ä≤ (z‚Ä≤)
pmm‚Ä≤qmm‚Ä≤ (z, z‚Ä≤) pmfm (z)

,
(11.83)
which does not include a Jacobian term because of the use of the identity
mapping.
This strategy (which we label the FF strategy) was suggested by S.
Fern¬¥andez and R. Fernando (Rohan Fernando (2001), personal commu-
nication). The form of (11.83) is similar to (11.13); however, in (11.83),
there is an extra term pm‚Ä≤m/ pmm‚Ä≤ and, further, qmm‚Ä≤ and qm‚Ä≤m are densi-
ties on Rnm‚Ä≤ and on Rnm, respectively. In (11.13), q (x, y) and q (y, x) are
both densities on Rd where d is the dimension of Y and X.
Example 11.4
Comparing diÔ¨Äerences between two treatments
The reversible jump algorithm is showed in detail with a trivial example.
Consider a model (M = 1) where the data are assumed to be an i.i.d.
realization from
yi|M = 1, t, œÉ2 ‚àºN

t, œÉ2
,
i = 1, . . . , N,
(11.84)
or from the alternative model (M = 2)
yij|M = 2, ti, œÉ2 ‚àºN

ti, œÉ2
,
i = 1, 2, j = 1, . . . , n,
(11.85)
where N = 2n. In (11.84), t is an overall mean and œÉ2 is the variance of
the distribution

yi|M = 1, t, œÉ2
. The sampling model (11.85) postulates
instead that there are two ‚Äútreatments‚Äù t1 and t2, and that observations
have variance œÉ2. To complete the Bayesian structure, prior distributions
Pr (M = 1) p

t, œÉ2|M = 1

and
Pr (M = 2) p

t1, t2, œÉ2|M = 2


526
11. Markov Chain Monte Carlo
are assigned to the parameters of both sampling models. The problem
consists of discriminating between these two models. This is done here using
reversible jump MCMC, despite the fact that by an appropriate choice of
prior distributions, closed forms for the relevant posterior distributions and
for the Bayes factor for these models are available (see e.g., O‚ÄôHagan, 1994).
It is convenient to introduce the stochastic variable T:
T =

t,
M = 1,
(t1, t2) ,
M = 2.
The posterior distribution, which has the form in (11.56), can then be
written
p

M = i, T, œÉ2|y

‚àùPr (M = i) p

T, œÉ2|M = i

p

y|M = i, T, œÉ2
.
(11.86)
First, reversible jump is implemented using stochastic proposals in both
directions. Assume that the current state of the Markov chain is (m = 1, z)
and a move to (m‚Ä≤ = 2, z‚Ä≤), where z =

t, œÉ2
and z‚Ä≤ =

t1, t2, œÉ2
, is
proposed with probability pmm‚Ä≤. This probability is chosen by the user,
subject to p11 + p12 = 1. With stochastic proposals in both directions, the
dimension-matching condition (11.60) is satisÔ¨Åed if two stochastic variables
(u = v1, v2) are generated in the move from m to m‚Ä≤, and one stochastic
variable (u‚Ä≤ = v) is generated in the move from m‚Ä≤ to m. In this case,
nm = 2 (associated with t, œÉ2), nmm‚Ä≤ = 2 (associated with v1, v2), nm‚Ä≤ = 3
(associated with t1, t2, œÉ2), and nm‚Ä≤m = 1 (associated with v). The mapping
is
(z‚Ä≤, u‚Ä≤)
=

t1, t2, œÉ2, v

=
gmm‚Ä≤ (z, u)
=
(g1mm‚Ä≤ (z, u) , g2mm‚Ä≤ (z, u)) .
A reasonable choice could be
Ô£Æ
Ô£ØÔ£ØÔ£∞
t1
t2
œÉ2
v
Ô£π
Ô£∫Ô£∫Ô£ª=
Ô£Æ
Ô£ØÔ£ØÔ£∞
1
0
1
0
1
0
0
1
0
1
0
0
0
0
1
2
1
2
Ô£π
Ô£∫Ô£∫Ô£ª
Ô£Æ
Ô£ØÔ£ØÔ£∞
t
œÉ2
v1
v2
Ô£π
Ô£∫Ô£∫Ô£ª.
That is, g1mm‚Ä≤ (z, u) =

t1, t2, œÉ2
=

t + v1, t + v2, œÉ2
and
g2mm‚Ä≤ (z, u) = v = 1
2 (v1 + v2) .
The absolute value of the Jacobian of the transformation in the move from
m to m‚Ä≤ is
""""det
‚àÇgmm‚Ä≤ (z, u)
‚àÇ(z, u)
"""" =
"""""det

‚àÇ

t + v1, t + v2, œÉ2, 1
2 (v1 + v2)

‚àÇ(t, œÉ2, v1, v2)
""""" = 1.

11.7 Reversible Jump MCMC
527
If v is generated from qv (z, ¬∑) and v1, v2 from qv1v2 (z‚Ä≤, ¬∑) , the expression
for the acceptance probability (11.73) is
amm‚Ä≤ (z, z‚Ä≤)
= min

1,
pm‚Ä≤mqv (z, v) pm‚Ä≤fm‚Ä≤
pmm‚Ä≤qv1v2 (z‚Ä≤, v1, v2) pmfm

,
where, in terms of (11.73), the posterior distribution of

M, T, œÉ2
is
pm‚Ä≤fm‚Ä≤ = c‚àí1 Pr (M = 2) p

t1, t2, œÉ2|M = 2

p

y|M = 2, t1, t2, œÉ2
(11.87)
and
pmfm = c‚àí1 Pr (M = 1) p

t, œÉ2|M = 1

p

y|M = 1, t, œÉ2
.
(11.88)
Notice that the constant c‚àí1 cancels in the acceptance ratio.
Second, reversible jump is implemented using a stochastic proposal in one
of the moves and a deterministic proposal in the other. As before, consider
the move from m = 1 to m‚Ä≤ = 2. Now the dimension-matching condition is
nm + nmm‚Ä≤ = nm‚Ä≤
because nm‚Ä≤m = 0. For the present example, nm = 2 (associated with
t, œÉ2), nmm‚Ä≤ = 1 (associated with u), and nm‚Ä≤ = 3 (associated with t1, t2,
œÉ2). The move from m to m‚Ä≤ is based on a stochastic proposal, since it
requires the generation of the random variable U from q (z, ¬∑). The mapping
is
z‚Ä≤
=

t1, t2, œÉ2
=
gmm‚Ä≤ (z, u)
=
g1mm‚Ä≤ (z, u) .
A reasonable choice is
Ô£Æ
Ô£∞
t1
t2
œÉ2
Ô£π
Ô£ª=
Ô£Æ
Ô£∞
1
1
0
1
‚àí1
0
0
0
1
Ô£π
Ô£ª
Ô£Æ
Ô£∞
t
u
œÉ2
Ô£π
Ô£ª.
(11.89)
That is, g1mm‚Ä≤ (z, u) =

t1, t2, œÉ2
=

t + u, t ‚àíu, œÉ2
. The absolute value
of the Jacobian of the transformation is
""""det

‚àÇgmm‚Ä≤ (z, u)
‚àÇ(z, u)
"""" =
"""""det

‚àÇ

t + u, t ‚àíu, œÉ2
‚àÇ(t, œÉ2, u)
""""" = 2
and the acceptance probability for the jump from m to m‚Ä≤ is
amm‚Ä≤ (z, z‚Ä≤) = min

1,
pm‚Ä≤mpm‚Ä≤fm‚Ä≤
pmm‚Ä≤q (z, u) pmfm
2

,

528
11. Markov Chain Monte Carlo
where pm‚Ä≤fm‚Ä≤ is equal to (11.87) and pmfm is equal to (11.88).
The move from m‚Ä≤ = 3 to m = 2 is deterministic. Inverting (11.89) yields
Ô£Æ
Ô£∞
t
u
œÉ2
Ô£π
Ô£ª=
Ô£Æ
Ô£∞
1
2
1
2
0
1
2
‚àí1
2
0
0
0
1
Ô£π
Ô£ª
Ô£Æ
Ô£∞
t1
t2
œÉ2
Ô£π
Ô£ª.
That is,
(z, u)
=

t, œÉ2, u

=
gm‚Ä≤m (z)
=
(g1m‚Ä≤m (z) , g2m‚Ä≤m (z)) ,
where
g1m‚Ä≤m (z) =

t, œÉ2
=

1
2 (t1 + t2) , œÉ2

and
g2m‚Ä≤m (z) = u = 1
2 (t1 ‚àít2) .
The absolute value of the Jacobian of the transformation is
""""det

‚àÇgm‚Ä≤m (z)
‚àÇz
"""" =
"""""det

‚àÇ
 1
2 (t1 + t2) , œÉ2, 1
2 (t1 ‚àít2)

‚àÇ(t1, t2, œÉ2)
""""" = 1
2.
The acceptance probability for the jump from m‚Ä≤ to m is given by
am‚Ä≤m (z‚Ä≤, z) = min

1, pmm‚Ä≤q (z, u) pmfm
pm‚Ä≤mpm‚Ä≤fm‚Ä≤
1
2

,
which is equal to the inverse of amm‚Ä≤ (z, z‚Ä≤), as expected.
Finally, the problem is approached via the FF strategy. Consider the same
move from m = 1 to m‚Ä≤ = 2 with now, z = t and the update including
z‚Ä≤ = (t1, t2) only, as œÉ2 is common to Models 1 and 2. Then the acceptance
probability is simply
amm‚Ä≤ (z, z‚Ä≤) = min

1,
pm‚Ä≤mqm‚Ä≤m (t) pm‚Ä≤fm‚Ä≤
pmm‚Ä≤qmm‚Ä≤ (t1, t2) pmfm

,
where qm‚Ä≤m (¬∑) is a density on R (e.g., a normal density with mean and
variance (t1 + t2) /2 and œÉ2, respectively) and qmm‚Ä≤ is a density on R2 (e.g.,
a bivariate normal density with mean (t, t) and well-tuned covariance).
Extension to an unknown number of covariates (treatments) is obvious,
and requires incorporating a prior distribution to this number. In this case,
reversible jump oÔ¨Äers a recipe for computing the posterior probability for
the number of covariates in the regression model. That is, the number of
covariates is treated as a random variable, to be inferred from the data at
hand.
‚ñ†

11.7 Reversible Jump MCMC
529
Example 11.5
Choosing between a gamma and a lognormal model
In this example, reversible jump is applied to obtain a MCMC-based pos-
terior probability of two models with the same number of parameters, the
gamma and the lognormal model. Such a comparison cannot be performed
via the traditional Neyman‚ÄìPearson maximum likelihood ratio test de-
scribed in Chapter 4. This is so because these models do not generate the
required nested structure. In order to perform the test within the frequen-
tist paradigm a modiÔ¨Åcation of the Neyman‚ÄìPearson maximum likelihood
ratio test is required (Cox, 1961, 1962).
A gamma distributed random variable has p.d.f.
g1 (y|Œ±, Œ≤) =
1
Œì (Œ±) Œ≤Œ± yŒ±‚àí1 exp [‚àíy /Œ≤ ] ,
0 < y < ‚àû, Œ± > 0, Œ≤ > 0.
The Ô¨Årst two moments of this distribution are
E (y)
=
Œ±Œ≤,
E

y2
=
Œ≤2Œ± (Œ± + 1) .
A lognormally distributed random variable has p.d.f.
g2

y|¬µ, œÉ2
=
1
y
‚àö
2œÄœÉ exp

‚àí1
2œÉ2 (ln y ‚àí¬µ)2

,
0
<
y < ‚àû, ‚àí‚àû< ¬µ < ‚àû, œÉ > 0.
The Ô¨Årst two moments of this distribution are
E (y)
=
exp

¬µ + œÉ2
2

,
E

y2
=
exp

2¬µ + 2œÉ2
.
Consider Model 1 as the gamma model, and Model 2 as the lognormal
model. DeÔ¨Åne the stochastic indicator M ‚àà{1, 2} for Models 1 and 2
respectively.
Let the posterior probability associated with Model 1 be
f1(Œ±, Œ≤, M = 1|y) ‚àùg1(y|Œ±, Œ≤, M = 1)h1(Œ±, Œ≤|M = 1) Pr (M = 1) ,
(11.90)
where g1 is the gamma density, h1 is a prior for the gamma density param-
eters Œ± and Œ≤, and Pr (M = 1) is the a priori probability of Model 1. Also,
let the posterior probability associated with Model 2 be
f2(¬µ, œÉ2, M = 2|y) ‚àùg2(y|¬µ, œÉ2, M = 2)h2(¬µ, œÉ2|M = 2) Pr (M = 2) ,
(11.91)
where g2 is the lognormal density, h2 is a prior for the lognormal density
parameters ¬µ and œÉ2, and Pr (M = 2) is the a priori probability of Model

530
11. Markov Chain Monte Carlo
2. Suppose that the current state of the Markov chain is (m, z), m = 1,
z = (Œ±, Œ≤), and that a move is to be made to the lognormal model, i.e.,
to a state (m‚Ä≤, z‚Ä≤), m‚Ä≤ = 2, z‚Ä≤ =

¬µ, œÉ2
. One way to propose values for
the parameters ¬µ and œÉ2 might be to equate the Ô¨Årst- and second-order
moments under the current gamma model and the proposed lognormal
model and, subsequently, add/multiply some noise, U. More precisely, solve
exp(Àú¬µ+ ÀúœÉ2/2) = Œ±Œ≤ and exp(2Àú¬µ+2ÀúœÉ2) = Œ≤2Œ±(Œ±+1) with respect to Àú¬µ and
ÀúœÉ2, and let the proposals be ¬µ = Àú¬µ+U1 and œÉ2 = ÀúœÉ2U2, where U = (U1, U2)
is generated from qmm‚Ä≤. In this case, we have
¬µ
=
ln

Œ±Œ≤
>
1 + 1/Œ±

+ U1,
œÉ2
=
ln (1 + 1/Œ±) U2,
(11.92)
U ‚Ä≤
1
=
U1,
U ‚Ä≤
2
=
U2.
That is,
(z, u) = (Œ±, Œ≤, u1, u2)
and
(z‚Ä≤, U ‚Ä≤) =

¬µ, œÉ2, U ‚Ä≤
1, U ‚Ä≤
2

= gmm‚Ä≤(Œ±, Œ≤, U1, U2),
= g1mm‚Ä≤ (Œ±, Œ≤, U1, U2) , g2mm‚Ä≤ (Œ±, Œ≤, U1, U2) ,
=

log(Œ±Œ≤/
>
1 + 1/Œ±) + U1, log(1 + 1/Œ±)U2

, (U1, U2) ,
where
g1mm‚Ä≤ (Œ±, Œ≤, U1, U2) =

¬µ, œÉ2
=

log(Œ±Œ≤/
>
1 + 1/Œ±) + U1, log(1 + 1/Œ±)U2

,
g2mm‚Ä≤ (Œ±, Œ≤, U1, U2) = (U1, U2) .
This move requires generating the stochastic vector U = (U1, U2) from
qmm‚Ä≤.
In the opposite move, from (m‚Ä≤, z‚Ä≤) to (m, z), solving (11.92) for Œ±, Œ≤, U1,
and U2 yields
Œ±
=
1/(exp(œÉ2/U ‚Ä≤
2) ‚àí1,
Œ≤
=
exp(¬µ ‚àíU ‚Ä≤
1 + œÉ2/(2U ‚Ä≤
2))(exp(œÉ2/U ‚Ä≤
2) ‚àí1),
U1
=
U ‚Ä≤
1,
U2
=
U ‚Ä≤
2.

11.7 Reversible Jump MCMC
531
That is,
(z, U) = (Œ±, Œ≤, U1, U2)
= gm‚Ä≤m(¬µ, œÉ2, U ‚Ä≤
1, U ‚Ä≤
2)
= g1m‚Ä≤m

¬µ, œÉ2, U ‚Ä≤
1, U ‚Ä≤
2

, g2m‚Ä≤m

¬µ, œÉ2, U ‚Ä≤
1, U ‚Ä≤
2

=

1/(exp(œÉ2/U ‚Ä≤
2) ‚àí1), exp(¬µ ‚àíU ‚Ä≤
1 + œÉ2/(2U ‚Ä≤
2))(exp(œÉ2/U ‚Ä≤
2) ‚àí1)

,
(U ‚Ä≤
1, U ‚Ä≤
2) ,
where U ‚Ä≤
1 and U ‚Ä≤
2 are generated from qm‚Ä≤m and
g1m‚Ä≤m

¬µ, œÉ2, U ‚Ä≤
1, U ‚Ä≤
2

=

1/(exp(œÉ2/U ‚Ä≤
2) ‚àí1), exp(¬µ ‚àíU ‚Ä≤
1 + œÉ2/(2U ‚Ä≤
2))(exp(œÉ2/U ‚Ä≤
2) ‚àí1)

,
g2m‚Ä≤m

¬µ, œÉ2, U ‚Ä≤
1, U ‚Ä≤
2

= (U ‚Ä≤
1, U ‚Ä≤
2) .
The moves in both directions use stochastic proposals qmm‚Ä≤ and qm‚Ä≤m.
The absolute value of the Jacobian of the transformation in the move from
(m, z) to (m‚Ä≤, z‚Ä≤), is
""""
‚àÇgmm‚Ä≤ (z, u)
‚àÇ(z, u)
"""" =
""""""
‚àÇ

log(Œ±Œ≤/
>
1 + 1/Œ±) + u1, log(1 + 1/Œ±)u2, u1, u2

‚àÇ(Œ±, Œ≤, u1, u2)
""""""
= u2 [Œ±Œ≤ (Œ± + 1)]‚àí1 .
Finally, the acceptance probability for the move from (m, z) to (m‚Ä≤, z‚Ä≤) is
a (z, z‚Ä≤) = min

1, f2(¬µ, œÉ2, M = 2|y)pm‚Ä≤mqm‚Ä≤m
f1(Œ±, Œ≤, M = 1|y)pmm‚Ä≤qmm‚Ä≤ u2 [Œ±Œ≤ (Œ± + 1)]‚àí1

,
where the posterior distributions f1 and f2 are given by (11.90) and (11.91),
respectively.
The move from the gamma to the lognormal model and the reverse move,
were chosen here to be stochastic. In this example where the number of
parameters is the same in both models, one could have chosen deterministic
moves in both directions. In this case, the acceptance probability of moving
from z to z‚Ä≤ would be given by
a (z, z‚Ä≤) = min

1, pm‚Ä≤mpm‚Ä≤fm‚Ä≤ (z‚Ä≤)
pmm‚Ä≤pmfm (z) |det (g‚Ä≤
mm‚Ä≤ (z))|

.
(11.93)
In the example above, the Jacobian is equal to
""""
‚àÇgmm‚Ä≤ (z)
‚àÇz
""""
=
""""""
‚àÇ

log(Œ±Œ≤/
>
1 + 1/Œ±), log(1 + 1/Œ±)

‚àÇ(Œ±, Œ≤)
""""""
=
[Œ±Œ≤ (Œ± + 1)]‚àí1 ,

532
11. Markov Chain Monte Carlo
and the acceptance probability of moving from the gamma to the lognormal
model is
a (z, z‚Ä≤) = min

1, f2(¬µ, œÉ2, M = 2|y)pm‚Ä≤m
f1(Œ±, Œ≤, M = 1|y)pmm‚Ä≤ [Œ±Œ≤ (Œ± + 1)]‚àí1

.
In terms of the rate of convergence and degree of autocorrelation of the
Markov chain, it is diÔ¨Écult a priori to determine which of the two ap-
proaches is to be preferred.
This example illustrates that the Jacobian is not an inherent component of
dimension-changing MCMC. The Jacobian arises due to the deterministic
transformation used in the proposal mechanism, and the change of variable
used when equating (11.69) and (11.70).
‚ñ†
As a concluding remark on the topic, we wish to draw attention to poten-
tial diÔ¨Éculties in successful implementation of the algorithm in highly di-
mensional problems. The competing models under consideration may have
diÔ¨Äerent sets of parameters and the reversible jump machinery simply pro-
vides no guidance to generate eÔ¨Äective jump proposals. A satisfactory rate
of transdimensional jumping may require very delicate tuning. A discussion
on this topic can be found in Brooks et al. (2001).
11.8
Data Augmentation
We conclude this chapter with a topic that is particularly relevant in the
implementation of MCMC. Imagine that there is interest in obtaining the
posterior distribution of a parameter Œ∏. Due to analytical intractability, one
chooses to approximate p (Œ∏|y) using MCMC. Often, the fully conditional
posterior distributions p (Œ∏i|Œ∏‚àíi, y) do not have a standard form and the
MCMC algorithm can be diÔ¨Écult to implement. The idea of data augmen-
tation is to augment with the so-called latent data or missing data œï, in
order to exploit the simplicity of the resulting conditional posterior distri-
butions p (Œ∏i|Œ∏‚àíi, œï, y). This is in the same spirit as in the EM algorithm:
by increasing the dimensionality of the problem, possibly at the expense
of extra computing time, although this is not always the case (Swendsen
and Wang, 1987), the problem is simpliÔ¨Åed algorithmically. Notice that the
focus of inference is
p (Œ∏|y) =

p (Œ∏|œï, y) p (œï|y) dœï,
and this marginalization is carried out via MCMC. A key paper is Tanner
and Wong (1987).
Example 11.6
Inference from truncated data
Consider a data set (the observed data) consisting of No independent obser-

11.8 Data Augmentation
533
vations from a truncated normal distribution, where the truncation point
T is known. Out of a total of NT o original observations, each one of the No
is kept because its value is larger than T. It is also known that there are
Nm = NT o ‚àíNo missing observations and the only information available
on these is that they are i.i.d., that sampling was at random, and that each
one is smaller than or equal to T.
The NT o original observations are assumed to be independently and nor-
mally distributed, with mean ¬µ and variance œÉ2. The observed data are
denoted by the vector y of length No; the missing data by the vector z of
length Nm. The objective of inference is to characterize ¬µ and œÉ2 with the
data available.
The p.d.f. of y is given by the product of truncated normal distributions.
The contribution to the likelihood from each element of y is
L

¬µ, œÉ2|yi > T

‚àù
p

yi|¬µ, œÉ2
 ‚àû
T p (yi|¬µ, œÉ2) dyi
=
p

yi|¬µ, œÉ2

1 ‚àíŒ¶

T ‚àí¬µ
œÉ
,
i = 1, . . . , No,
(11.94)
where p

¬∑|¬µ, œÉ2
is the p.d.f. of the normal distribution and Œ¶ (¬∑) is the
cumulative density function of the standard normal distribution.
The contribution to the likelihood from each element of z, zj (j = 1, . . . , Nm) ,
is
L

¬µ, œÉ2|zj ‚â§T

‚àùP

zj ‚â§T|¬µ, œÉ2
=
 T
‚àí‚àû
p

zj|¬µ, œÉ2
dzj
= Œ¶

T ‚àí¬µ
œÉ

.
(11.95)
By virtue of independence, the likelihood is
L

¬µ, œÉ2|y

‚àù
No
7
i=1
p

yi|¬µ, œÉ2

1 ‚àíŒ¶

T ‚àí¬µ
œÉ
No

Œ¶

T ‚àí¬µ
œÉ
Nm
.
(11.96)
Assuming independent uniform prior distributions for ¬µ and œÉ2, the joint
posterior distribution p

¬µ, œÉ2|y

is proportional to (11.96). Implementation
of the Gibbs sampler requires drawing samples from p

¬µ|œÉ2, y

and from
p

œÉ2|¬µ, y

. It is clear from (11.96) that these fully conditional posterior
distributions do not reduce to standard form. To facilitate the problem
algorithmically one can augment with the missing data z. The complete

534
11. Markov Chain Monte Carlo
data (observed data + missing data) are denoted by x‚Ä≤ = (z‚Ä≤, y‚Ä≤). Thus,
the complete data vector x has i.i.d. elements each with distributional form
xi ‚àºN

¬µ, œÉ2
.
The observed data can be envisaged as being generated in the following
manner:
yi|¬µ, œÉ2, yi > T ‚àºN

¬µ, œÉ2
I (xi > T)
(11.97)
and the missing data
zi|¬µ, œÉ2, zi ‚â§T ‚àºN

¬µ, œÉ2
I (xi ‚â§T) ,
(11.98)
where I (x ‚ààA) is the indicator function, which takes the value 1 if x is
contained in the set A, and zero otherwise.
The density of the complete data is
p

x|¬µ, œÉ2
‚àù
NT o
-
i=1

p

xi|¬µ, œÉ2
I (xi ‚â§T) + p

xi|¬µ, œÉ2
I (xi > T)

.
(11.99)
The augmented posterior of the parameters takes the form
p

¬µ, œÉ2, z|y

‚àùp

¬µ, œÉ2, z

p

y|¬µ, œÉ2, z

= p

¬µ, œÉ2
p

y, z|¬µ, œÉ2
,
which, assuming independent uniform prior distributions for ¬µ and œÉ2, is
proportional to (11.99). The Gibbs sampler run under the augmentation
scheme involves drawing from p

z|¬µ, œÉ2, y

, from p

œÉ2|z, ¬µ, y

, and from
p

¬µ|z,œÉ2, y

.
To derive p

z|¬µ, œÉ2, y

, one retains in (11.99) those terms that include z.
Therefore, from (11.98) and (11.99),
p

z|¬µ, œÉ2, y

‚àù
Nm
-
j=1
p

xj|¬µ, œÉ2
I (xj ‚â§T) .
(11.100)
This has the form of a left-truncated normal distribution, with mean ¬µ and
variance œÉ2, where the truncation point is T.
The fully conditional posterior distribution of œÉ2 is
p

œÉ2|z, ¬µ, y

‚àù
NT o
-
i=1
p

xi|¬µ, œÉ2
‚àù

œÉ2‚àíNT o
2
exp
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞‚àí
NT o

i=1
(xi ‚àí¬µ)2
2œÉ2
Ô£π
Ô£∫Ô£∫Ô£∫Ô£ª.

11.8 Data Augmentation
535
This is the kernel of a scaled inverted chi-square distribution, with scale
parameter NT o
i=1 (xi ‚àí¬µ)2 and NT o ‚àí2 degrees of freedom. Therefore,
œÉ2|z, ¬µ, y ‚àº
NT o

i=1
(xi ‚àí¬µ)2

œá‚àí2
NT o‚àí2.
(11.101)
Finally, the fully conditional posterior distribution of ¬µ is
p

¬µ|œÉ2z, y

‚àù
NT o
-
i=1
p

xi|¬µ, œÉ2
.
This is proportional to exp

‚àí
NT o
i=1 (xi‚àí¬µ)2
2œÉ2

. Adding and subtracting x =
NT o
i=1 xi/NT o in the squared term yields
exp
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞
NT o

i=1
[(xi ‚àíx) + (x ‚àí¬µ)]2
2œÉ2
Ô£π
Ô£∫Ô£∫Ô£∫Ô£ª.
Retaining only terms in ¬µ one obtains
¬µ|œÉ2z, y ‚àºN

x, œÉ2
NT o

.
(11.102)
The Gibbs sampling algorithm consists of drawing repeatedly from (11.100),
from (11.101), and Ô¨Ånally from (11.102).
‚ñ†
Example 11.7
ABO blood groups
Gene frequencies of the ABO blood group data were inferred using maxi-
mum likelihood implemented via Newton-Raphson in Example 4.7 of Chap-
ter 4 and implemented via the EM algorithm in Example 9.2 of Chapter 9.
Here a Bayesian MCMC approach is implemented via data augmentation,
using the data as in Example 4.7.
Assume a Dirichlet prior with parameters (Œ±A, Œ±B, Œ±O) for the gene fre-
quencies. On the basis of the data in Table 4.1 the joint posterior distribu-
tion f (pA, pB, pO|n) is proportional to

p2
A + 2pApO
nA [2pApB]nAB 
p2
B + 2pBpO
nB 
p2
O
nO
[pA]Œ±A‚àí1 [pB]Œ±B‚àí1 [pO]Œ±O‚àí1 ,
where n = (nA, nAB, nB, nO)‚Ä≤ is the observed data. It is not possible to
extract standard fully conditional posterior distributions for pA, pB, and
pO from this joint posterior. Augmenting with the missing counts
nm = (nAO, nAA, nBB, nBO)

536
11. Markov Chain Monte Carlo
yields the following augmented posterior distribution
f (nm, pA, pB, pO|n) ‚àùf (nm, pA, pB, pO) f (n|nm, pA, pB, pO)
= f (n, nm|pA, pB, pO) f (pA, pB, pO) .
(11.103)
The augmented posterior (11.103) has the form
[pA]2nAA [2pApO]nAO [2pApB]nAB [pB]2nBB [2pBpO]nBO
[pO]2nO [pA]Œ±A‚àí1 [pB]Œ±B‚àí1 [pO]Œ±O‚àí1 ,
(11.104)
which is proportional to
[pA]2nAA [pA]nAO [pA]nAB [pA]Œ±A‚àí1
[pB]2nBB [pB]nAB [pB]nBO [pB]Œ±B‚àí1
[pO]2nO [pO]nAO [pO]nBO [pO]Œ±O‚àí1
= [pA]2nAA+nAO+nAB+Œ±A‚àí1 [pB]2nBB+nAB+nBO+Œ±B‚àí1
[pO]2nO+nAO+nBO+Œ±O‚àí1 .
From this expression, the joint conditional posterior distribution
[pA, pB, pO|nm, n]
is immediately recognized as Dirichlet, with parameters a = 2nAA +nAO +
nAB + Œ±A, b = 2nBB + nAB + nBO + Œ±B, and c = 2nO + nAO + nBO + Œ±O;
that is,
pA, pB, pO|nm, n ‚àºDi (a, b, c) .
(11.105)
To derive the fully conditional posterior distribution of nAA, Ô¨Årst write
nAO = nA ‚àínAA and extract the terms in nAA from (11.104). This yields
nAA|pA, pO, nA ‚àºBi

p2
A
p2
A + 2pApO
, nA

,
(11.106)
with nAO = nA ‚àínAA. Similarly,
nBB|pB, pO, nB ‚àºBi

p2
B
p2
B + 2pBpO
, nB

,
(11.107)
with nBO = nB ‚àínBB.
The Gibbs sampling algorithm deÔ¨Åned by (11.105), (11.106), and (11.107),
was run using a chain length equal to 3500. After discarding the Ô¨Årst
500 samples, the mean and standard deviation of the marginal posterior
distributions were estimated from the remaining 3000 draws. Choosing
Œ±A = Œ±B = Œ±0 = 2, the Monte Carlo estimate of the posterior means
are for pA = 0.20925 and for pB = 0.08102. The corresponding poste-
rior standard deviations are 0.0066312 and 0.0043504 (for p0 the posterior

11.8 Data Augmentation
537
standard deviation is 0.0073635). Choosing Œ±A = Œ±B = Œ±0 = 1 (leading
to a uniform prior for the gene frequencies) yields estimates of posterior
means for pA = 0.20915 and for pB = 0.08084; the corresponding posterior
standard deviations are 0.0066315 and 0.0043533 (the posterior standard
deviation of p0 is 0.0073300).
‚ñ†

This page intentionally left blank

12
Implementation and Analysis of
MCMC Samples
12.1
Introduction
The typical output of a Bayesian MCMC analysis consists of correlated
samples from the joint posterior distribution of all parameters of a single
model or of a number of models. Using these samples, the analyst may be
interested in estimating various features of the posterior distribution. These
could include quantiles or moments of marginal posterior distributions of
the parameters or of functions thereof.
Three partly related questions that could be posed in the analysis of
MCMC output are:
‚Ä¢ Can the simulated or sampled values be considered to be draws from
the posterior distribution of interest?
‚Ä¢ Are estimates of features of the posterior distribution precise enough?
‚Ä¢ Have the empirical averages computed from the Monte Carlo output
converged to their expectation under the equilibrium distribution?
Other pertinent problems involve the possible impropriety of posterior
distributions (a potential danger when improper priors are employed), and
issues related to the sensitivity of an analysis using the MCMC samples.
These points are discussed in this chapter and partly in Subsection 16.2.3
of Chapter 16. First, we discuss the relative advantages and disadvantages
of conducting the MCMC analysis using one or several independent chains.
Subsequently, the eÔ¨Äects of inter-correlation between parameters on the

540
12. Implementation and Analysis of MCMC Samples
behavior of the MCMC are illustrated, and some techniques for diagnosing
convergence are presented. Another section gives an overview of estimators
of features of the posterior distribution and of their Monte Carlo precision.
The chapter concludes with a discussion of sensitivity assessment.
12.2
A Single Long Chain or Several Short Chains?
In the early 1990s when MCMC methods entered into the statistical arena,
considerable discussion centered on the best ways of running the algorithms.
DiÔ¨Äerent implementation strategies aÔ¨Äect the serial correlations between
successive samples of the same or diÔ¨Äerent parameters within a chain. These
correlations, as discussed below, inÔ¨Çuence the rate of convergence to the
stationary distribution and the sampling error of estimates of features of
this distribution. In short, the speciÔ¨Åc implementation adopted can have a
profound impact on the eÔ¨Éciency of the computations.
A strategy that was advocated in the early literature (Gelfand and Smith,
1990) but that has fallen in disuse thereafter, consists of running several
short independent chains, k say, and on saving the last sample (the mth)
from each of the chains. This is known as the multiple-chain or short-
chain method. Here, mk samples are generated but only k are kept for
the post-MCMC analysis. The method is extremely ineÔ¨Écient because
(100 (m ‚àí1) /m) % of the samples are discarded and the value of m is at
least in the dozens. In addition, the length of the chains was often judged
to be insuÔ¨Écient to guarantee convergence of each of the runs to the target
distribution.
Current recommendations about implementation strategies in the lit-
erature range from running either several long chains (Gelman and Ru-
bin, 1992) or a single, very long one (Geyer, 1992). Supporters of the Ô¨Årst
approach argue that a comparison of results from several seemingly con-
verged chains might reveal genuine diÔ¨Äerences, if the chains have not yet
approached stationarity. Those in favor of the single, long-chain implemen-
tation, believe that this method has better chances of producing samples
which properly represent the complete support of the target distribution.
In practice, one almost always uses more than a single long run, and con-
vergence is assessed graphically or via more formal tests, as discussed later.
To avoid possible inÔ¨Çuences of the starting values, the initial samples are
often discarded. This is usually referred to as the burn-in period. After
burn-in, unless correlations between adjacent samples are extremely high,
or if storage is a problem, all samples are kept for later processing. A very
useful reference where many implementation problems are discussed is the
tutorial of Kass et al. (1998).

12.3 Convergence Issues
541
12.3
Convergence Issues
It was seen in Chapters 10 and 11 that an ergodic Markov chain generated
via iterative Monte Carlo converges to its stationary distribution asymp-
totically. This means that the number of iterations of the chain must ap-
proach inÔ¨Ånity! In practice, however, one runs a chain which is long enough
in some sense, so that the values obtained can be regarded as approximate
draws from the posterior distribution of interest. The diÔ¨Éculty resides in
determining how long the chain must be. Unfortunately, there is no simple
answer to this question. Clearly, if the iterations have not proceeded long
enough, the draws may be unrepresentative of the whole support of the
target distribution and this will probably result in poor inferences.
12.3.1
EÔ¨Äect of Posterior Correlation on Convergence
A strong intercorrelation between parameters in the posterior distribution
hampers the behavior of the MCMC scheme. We start this section with a
couple of stylized examples. The Ô¨Årst one illustrates how a high posterior
correlation between parameters can slow down the motion of the chain
towards its equilibrium distribution. The second presents a model in which
the two parameters of a model are not identiÔ¨Åable from the likelihood
function. If improper priors are used, this leads to a situation where even
though the conditional distributions are well deÔ¨Åned, the joint posterior
does not exist. In this case, a Gibbs sampler will lead to absurd results,
even though the ‚Äúnumbers‚Äù may appear sensible. When proper priors are
adopted, it is shown that the analysis produces Bayesian learning, but
the inÔ¨Çuence of the prior does not dissipate asymptotically. Further, the
sampler will move very slowly in the parameter space, because of the high
intercorrelation between the poorly identiÔ¨Åed parameters.
Example 12.1
A 2 √ó 2 table
Consider Example 10.4 from Chapter 10, and following O‚ÄôHagan (1994) let
p1 = p4 = p
2.
p3 = p2 = 1 ‚àíp
2
.
It can be veriÔ¨Åed that
Cov (X, Y ) = E (XY ) ‚àíE (X) E (Y )
= 1
4 (2p ‚àí1) ,
and that
V ar (X) = V ar (Y ) = 1
4.

542
12. Implementation and Analysis of MCMC Samples
Therefore, the correlation between X and Y is
œÅ = 2p ‚àí1.
The transition probability matrix Px|x is
Px|x =

1 ‚àí2p (1 ‚àíp)
2p (1 ‚àíp)
2p (1 ‚àíp)
1 ‚àí2p (1 ‚àíp)

.
The matrix Px|x is ergodic provided that p > 0 and the unique solution to
equation (10.8)
œÄ‚Ä≤Px|x = œÄ‚Ä≤,
is
œÄ‚Ä≤ =

1
2
1
2

.
Therefore œÄ is the unique stationary distribution of the Markov chain. The
matrix Px|x has two eigenvalues, Œª1 = 1 and Œª2 = œÅ2. The corresponding
eigenvectors are
c‚Ä≤
1 =

1
1

,
c‚Ä≤
2 =

‚àí1
1

.
Then the rate of convergence of the Markov chain can be studied using
(10.26),
Pn
x|x = Œªn
1Q1 + Œªn
2Q2,
n = 1, 2, . . . ,
which in the present example is equal to
Pn
x|x = 1n
Ô£Æ
Ô£∞
1
2
1
2
1
2
1
2
Ô£π
Ô£ª+

œÅ2n
Ô£Æ
Ô£∞
1
2
‚àí1
2
‚àí1
2
1
2
Ô£π
Ô£ª
=
Ô£Æ
Ô£∞
1
2

1 + œÅ2n
1
2

1 ‚àíœÅ2n
1
2

1 ‚àíœÅ2n
1
2

1 + œÅ2n
Ô£π
Ô£ª.
(12.1)
Recall expression (10.7) from Chapter 10,
œÄ‚Ä≤(n) = œÄ‚Ä≤(0)Pn
x|x,
where œÄ(0) = (p(0), 1 ‚àíp(0))‚Ä≤ represents the distribution of the initial state
of the Markov chain. It is easy to verify from (12.1) that after n transitions
p(n) = 1
2

1 ‚àíœÅ2n 
1 ‚àí2p(0)
.
Thus, for high values of œÅ, convergence toward the equilibrium distribution
is very slow. To illustrate with an extreme case, setting œÅ = 0.9998 and

12.3 Convergence Issues
543
p(0) = 0.1 leads, after one transition, to p(1) = 0.10016 and, after n = 1000
transitions, to p(1000) = 0.23188, still a long way from the equilibrium value
of 1/2. Another way of illustrating the same phenomenon is to set n = 1000
in (12.1), which gives
P1000
x|x =

0.835
0.165
0.165
0.835

.
This is far from the equilibrium value
Ô£Æ
Ô£∞
1
2
1
2
1
2
1
2
Ô£π
Ô£ª.
‚ñ†
Although correlations as large as 0.9998 are not that common, moderate
to high correlations between parameters of the model can be encountered
frequently. This induces slow mixing of the chain: successive transitions
are strongly correlated and convergence can be very slow. Even if the chain
converges to the equilibrium distribution, with poor mixing, using the time-
average over the chain under the equilibrium distribution, i.e., estimator
(11.6), will result in poor inferences.
In highly parameterized models, relatively small correlations can result
in a similar behavior of the chain, because of intercorrelations between
parameters. From a practical point of view, autocorrelation between suc-
cessive samples produces long sequences where little change is detected,
misleadingly suggesting that the chain has converged. Another consequence
of within-sequence correlation is that it leads to less precise inferences than
those obtained from the same number of independent samples. Two strate-
gies are often suggested for ameliorating slow mixing in MCMC imple-
mentations: reparameterization of the model, and sampling parameters in
blocks, rather than sampling each parameter individually. However, the two
strategies often lead to added computational complexity.
Example 12.2
IdentiÔ¨Åability and impropriety of posterior distributions
This example is based on an exercise in Chapter 5 of Carlin and Louis
(1996). Suppose that data yi, (i = 1, 2, . . . , n), are independent realizations
from a normal distribution with known variance
yi|Œ∏1, Œ∏2 ‚àºN (Œ∏1 + Œ∏2, 1) .
(12.2)
Since the variance is known, observations have been rescaled to have a
standard deviation equal to 1. The likelihood of Œ∏ = (Œ∏1, Œ∏2)‚Ä≤ can be written

544
12. Implementation and Analysis of MCMC Samples
as
p (y|Œ∏) ‚àùexp

‚àín
2 (Œ∏1 + Œ∏2 ‚àíy)2
‚àùexp

‚àí1
2 (Œ∏ ‚àí¬µ)‚Ä≤ N (Œ∏ ‚àí¬µ)

,
(12.3)
where y = 1
n
 yi, ¬µ‚Ä≤=
y
2, y
2

and
N =

n
n
n
n

.
Because the matrix N has rank equal to 1, (12.3) is the kernel of a singular
bivariate normal distribution, if viewed as a function of Œ∏. It is clearly
impossible to obtain ML estimates of Œ∏1 and Œ∏2 from (12.3); on the other
hand, the ML estimator of Œ∏1 + Œ∏2 is y. If independent, improper uniform
prior distributions are adopted for each of the parameters, the density of
the joint posterior distribution of Œ∏1, Œ∏2 is
p (Œ∏1, Œ∏2|y) ‚àùexp

‚àín
2 (Œ∏1 + Œ∏2 ‚àíy)2
.
(12.4)
It can be veriÔ¨Åed readily that the densities of the conditional posterior
distributions are
Œ∏1|Œ∏2, y ‚àºN

y ‚àíŒ∏2, 1
n

(12.5)
and
Œ∏2|Œ∏1, y ‚àºN

y ‚àíŒ∏1, 1
n

.
(12.6)
The marginal posterior density of Œ∏1, say, is
p (Œ∏1|y) = p (Œ∏1, Œ∏2|y)
p (Œ∏2|Œ∏1, y)
=
c12 exp

‚àín
2 (Œ∏1 + Œ∏2 ‚àíy)2
c2|1 exp

‚àín
2 (Œ∏1 + Œ∏2 ‚àíy)2
= c12
c2|1
,
(12.7)
where c12 and c2|1 are the constants of integration associated with (12.4)
and (12.6), respectively, assuming c12 is Ô¨Ånite. Hence, this marginal poste-
rior distribution does not depend on Œ∏1. Clearly, the integral of (12.7) over
the real line (the sample space of Œ∏1) does not converge, indicating that the
marginal posterior distribution is improper.
In this setting, the parameters Œ∏1 and Œ∏2 in (12.4) are unidentiÔ¨Åable from

12.3 Convergence Issues
545
each other: the posterior distribution carries information on their sum,
Œ∏1 + Œ∏2, but not on each one of them separately. Since the fully conditional
posterior distributions (12.5) and (12.6) are proper, a Gibbs sampling im-
plementation of the model is possible, yielding valid inferences about fea-
tures of the distribution [Œ∏1 + Œ∏2|y]. However, one could naively use the
samples from the chains generated from (12.5) and (12.6), to ‚Äúinfer‚Äù fea-
tures of the marginal posterior distribution of Œ∏1 or of Œ∏2. This would lead
to meaningless results, since these marginal distributions do not exist. This
illustrates the pitfall of using improper priors in hierarchical models. Ex-
cept in highly stylized models (such as in this example) it is very diÔ¨Écult to
assess impropriety analytically (e.g., Hobert and Casella, 1996). All condi-
tional posterior distributions may exist even when the joint distribution is
not deÔ¨Åned. Yet, the output analysis may produce seemingly ‚Äúreasonable‚Äù
results!
Now consider assigning independent normal distributions, a priori, to
each of Œ∏1 and Œ∏2. Take
Œ∏i ‚àºN

ai, b2
i

,
i = 1, 2,
such that the joint prior density is
p (Œ∏1, Œ∏2) ‚àùexp

‚àí1
2 (Œ∏ ‚àía)‚Ä≤ B (Œ∏ ‚àía)

,
(12.8)
where Œ∏ = (Œ∏1, Œ∏2)‚Ä≤, a = (a1, a2)‚Ä≤, and
B =
Ô£Æ
Ô£ØÔ£∞
1
b2
1
0
0
1
b2
2
Ô£π
Ô£∫Ô£ª.
With proper prior distributions, the problem of the lack of identiÔ¨Åability of
the parameters in the posterior distribution disappears. Now using (12.8)
and (12.3) the joint posterior density is
p (Œ∏|y) ‚àùexp

‚àí1
2

(Œ∏ ‚àía)‚Ä≤ B (Œ∏ ‚àía) + (Œ∏ ‚àí¬µ)‚Ä≤ N (Œ∏ ‚àí¬µ)
%
.
(12.9)
Combining these two quadratic forms using results in Box and Tiao (1973),
page 418, and keeping only the terms which are functions of Œ∏, leads to
p (Œ∏|y) ‚àùexp

‚àí1
2

Œ∏ ‚àíŒ∏
‚Ä≤ (B + N)

Œ∏ ‚àíŒ∏

.
(12.10)

546
12. Implementation and Analysis of MCMC Samples
This is the kernel of the density of a bivariate normal distribution with
mean vector Œ∏ and variance‚Äìcovariance matrix (B + N)‚àí1, where
Œ∏ = (B + N)‚àí1 (Ba + N¬µ)
=
Ô£Æ
Ô£ØÔ£∞
n + 1
b2
1
n
n
n + 1
b2
2
Ô£π
Ô£∫Ô£ª
‚àí1 Ô£Æ
Ô£ØÔ£∞
a1
b2
1
+ ny
a2
b2
2
+ ny
Ô£π
Ô£∫Ô£ª.
(12.11)
After some algebra, the posterior mean vector (12.11) can be written as:
 Œ∏1
Œ∏2

=

a1 + k1 (y ‚àía1 ‚àía2)
a2 + k2 (y ‚àía1 ‚àía2)

,
(12.12)
where
k1 =
b2
1
b2
1 + b2
2 + 1
n
,
and
k2 =
b2
2
b2
1 + b2
2 + 1
n
.
The posterior variance‚Äìcovariance matrix is
V ar (Œ∏|y)
=
Ô£Æ
Ô£ØÔ£∞
n + 1
b2
1
n
n
n + 1
b2
2
Ô£π
Ô£∫Ô£ª
‚àí1
=

b2
1 (1 ‚àík1)
‚àíb2
1k2
‚àíb2
1k2
b2
2 (1 ‚àík2)

.
(12.13)
There are a number of important conclusions that can be drawn from this
exercise. First, note that the marginal posterior distribution of Œ∏i is now
proper, and that it diÔ¨Äers from the prior distribution. In this sense, there
is Bayesian learning via the data. From (12.12), it is apparent that the
inÔ¨Çuence of the data on the posterior mean depends on the values of the
prior variances, via the ‚Äúregression‚Äù ki = b2
i /

b2
1 + b2
2 + 1
n

. Second, (12.13)
indicates that the variance of the marginal posterior distribution is smaller
than the prior variance. Third, (12.12) and (12.13) illustrate that as the
number of observations n ‚Üí‚àû, the inÔ¨Çuence of the prior does not vanish
asymptotically. The posterior variance does not tend to zero, and infer-
ences always depend on the relative values of the prior variances b2
1/b2
2. For
example (for large n), if b2
1 >> b2
2, the posterior mean of Œ∏1 will tend to
y ‚àía2, whereas the posterior mean of Œ∏2 will be close to a2, its prior mean.
Here, Bayesian learning occurs for Œ∏1, but not for Œ∏2. Finally, from (12.13),
the posterior correlation between Œ∏1 and Œ∏2 is given by:
Corr (Œ∏1, Œ∏2|y) = ‚àí
b1b2
8 1
n + b2
1
  1
n + b2
2
,
(12.14)

12.3 Convergence Issues
547
which is very close to ‚àí1 for moderately large n. From the point of view of
implementing a Gibbs sampler, this has important implications. While the
proper prior distributions of Œ∏1 and Œ∏2 lead to proper marginal posterior
distributions, the very high posterior correlation between Œ∏1 and Œ∏2 will
generate a strong serial correlation between samples of a Gibbs chain. This
will have sizable eÔ¨Äects on convergence, and will retard the movement of
the Gibbs sampler over the support of the posterior distribution. In such a
situation, the quality of posterior inferences would be impaired seriously. A
broad discussion on strategies for improving MCMC can be found in Gilks
and Roberts (1996).
‚ñ†
MCMC opens the opportunity for Ô¨Åtting complex hierarchical models to
data, and these models perhaps describe better the biological system un-
der study. However, the richness and Ô¨Çexibility are accompanied by caveats.
For example, it may be dangerous to entertain models that are not well un-
derstood analytically. In highly complex models, there is always the pitfall
that parameters may be unidentiÔ¨Åed or very weakly identiÔ¨Åed. In contrast
with the preceding example, it may not always be possible to detect lack of
identiÔ¨Åability. Therefore, it is important to learn as much as possible about
the model, and to experiment with it step by step before launching a full
MCMC-based analysis.
12.3.2
Monitoring Convergence
A large literature on convergence diagnostics has developed during the last
decade. Useful reviews can be found in Cowles and Carlin (1996), Brooks
and Roberts (1998), Robert (1998), Robert and Casella (1999), Mengersen
et al. (1999), and references herein. In this section some of the commonly
used convergence diagnostics are described, and the reader is referred to
the above reviews for a description of other methods.
Graphical Procedures
Gelfand et al. (1990) suggested informal convergence checks based on graph-
ical techniques. They run a number of independent chains with variable
starting values and perform the analysis for each of the parameters of in-
terest. Plots of histograms are overlaid for increasing chain lengths, until
the graphs become visually indistinguishable among chains. Stability of
the results is assessed by increasing chain length. Another simple graphical
tool for studying convergence and mixing behavior of the chain is what is
called the ‚Äútrace plot‚Äù. Samples of the parameter of interest, from repli-
cated chains started from overdispersed values, are plotted as a function
of iterate number. Wavy patterns typically indicate strong autocorrelations
within chains, while a zigzag suggest that the parameter moves more freely.
In highly dimensional problems, however, it is not feasible to examine the

548
12. Implementation and Analysis of MCMC Samples
time series plots of all the parameters. In this case, a selective choice is
often made, such that either the focus of inference or a high-level parame-
ter in a hierarchy (e.g., variance components in a generalized linear model)
are followed, since these tend to mix more slowly. Here, it is important to
be aware that parameters converge at diÔ¨Äerent rates. Further, a slow con-
vergence of nuisance parameters may aÔ¨Äect convergence of the parameters
of interest adversely. Also, an assessment of convergence of the marginal
distributions does not provide an exhaustive diagnostic of convergence of
the joint process.
Auto-Correlograms
Examination of lag-t autocorrelations is an easy way of monitoring the
mixing behavior of the Markov chain. For instance, one can plot the au-
tocorrelations as a function of the lag (this is called a correlogram), and
detect the lag at which the correlation ‚Äúdies out‚Äù. As indicated below,
these autocorrelations are also useful to estimate the eÔ¨Äective chain length
(Sorensen et al., 1995).
Between and within-chain Variability of Sample Values
A popular quantitative convergence diagnostic was suggested by Gelman
and Rubin (1992). The method involves running m independent chains,
each of length 2n. Each of the chains starts from a diÔ¨Äerent starting point
from a distribution that is overdispersed with respect to the target distri-
bution. This is an important design feature, because it can make lack of
convergence apparent. As discussed in Gelman and Rubin (1992), satisfying
this requirement may involve considerable initial eÔ¨Äort toward eliciting a
rough estimate of the variance of the marginal posterior distribution of the
scalar of interest. This knowledge is important for generating an overdis-
persed starting sequence. Starting from points far apart will also ensure
that the complete support of the target distribution will be visited. The
Ô¨Årst n iterations of each chain are discarded and the last n are retained.
Each feature of interest from the target distribution is monitored sepa-
rately. The following step consists of carrying out an analysis of variance:
approximate convergence is diagnosed when the variability between chains
is not larger than that within chains. The rationale of the method is that
before convergence is reached, due to the initial over-dispersion, the vari-
ance between chains should be relatively large. On the other hand, variation
within chains would be relatively small because in the intermediate stages
of the iteration the support of the target distribution would be incompletely
represented in the simulations.
Suppose that for chain i (i = 1, 2, . . . , m), simulated values
Œ∏ij,
j = 1, 2, . . . , n

12.3 Convergence Issues
549
are available from the scalar posterior distribution [Œ∏|y]. Hence, there are
m chains each of length n. The simulated values can be organized as a one-
way layout, with m classes and n observations per class. The between-chain
mean square B and the within-chain mean square W are
B =
n
m

i=1

Œ∏i. ‚àíŒ∏..
2
m ‚àí1
,
and
W =
m

i=1
S2
i
m
.
Here
Œ∏i. =
n
j=1
Œ∏ij
n
,
Œ∏.. =
m

i=1
Œ∏i.
m
are the within-chain sample average and the mean of the chain averages,
respectively, and
S2
i =
n
j=1

Œ∏ij ‚àíŒ∏i.
2
n ‚àí1
is the estimated variance of sampled values in chain i. Let
¬µ =

Œ∏p (Œ∏|y) dŒ∏
and
œÉ2 =

(Œ∏ ‚àí¬µ)2 p (Œ∏|y) dŒ∏,
be the mean and variance of the target posterior distribution, respectively.
If the simulated values are drawn from [Œ∏|y], it can be veriÔ¨Åed readily that
the following expectations hold:
E (B) = œÉ2,
(12.15)
and
E

S2
i

= E (W) = œÉ2.
(12.16)
Gelman and Rubin (1992) suggest the estimator of the posterior variance
6
œÉ2 = n ‚àí1
n
W + 1
nB,
(12.17)
which is clearly unbiased for œÉ2, provided that all draws are from the target
distribution. On the other hand, if there is at least some initial overdisper-
sion, the estimator will have an upward bias because the averages Œ∏i. would

550
12. Implementation and Analysis of MCMC Samples
vary more than if drawn from the same distribution. This suggests that if
the initial draws are not overdispersed enough, 6
œÉ2 can be too low, falsely di-
agnosing convergence. Gelman and Rubin (1992) suggest that convergence
of any scalar quantity of interest can be monitored by the quantity
5R =
C
6
œÉ2
W =
C
1 + 1
n

 B
W ‚àí1

(12.18)
which is expected to be larger than 1, and declines to 1 as n ‚Üí‚àû. There-
fore, convergence can be evaluated by examining the proximity of 5R to 1.
Gelman and Rubin (1992) mention that values of 5R around 1.2 may be
satisfactory for most problems. In some cases, however, a higher level of
precision may be required. Although normality is not required for unbi-
asedness of 6
œÉ2, Gelman and Rubin (1992) state that the method works
better if the posterior distribution is nearly normal.
A number of shortcomings of the method have been raised. First, con-
structing a distribution for sampling starting values may be diÔ¨Écult, es-
pecially in models in which multimodality is expected. Second, discarding
the Ô¨Årst n samples is computationally wasteful. Third, the method relies
to some extent on approximate normality of the target distribution, and
this may not always hold, specially in Ô¨Ånite sample situations. Fourth,
the procedure will not work if the chains get ‚Äútrapped‚Äù within the same
subregion of the parameter space. Finally, the convergence criterion is uni-
dimensional; hence, it gives an inadequate evaluation of convergence to
the joint distribution. Brooks and Gelman (1998) have developed a multi-
parameter version of this approach. These criticisms must be seen in the
light of the fact that none of the many methods proposed can be relied
upon unilaterally. In a review of convergence diagnosis methods, Cowles
and Carlin (1996) concluded that all procedures can fail to detect the sort
of convergence failure that they were designed to identify. Therefore, the
general recommendation is to use a combination of approaches as diagnos-
tic tools (including graphical methods) and to learn as much as possible
from the target distribution before embarking in a MCMC algorithm. As
a minimum, ensuring propriety of the posterior distribution is essential.
12.4
Inferences from the MCMC Output
12.4.1
Estimators of Posterior Quantities
The MCMC output is typically used to estimate features of the posterior
distribution, such as posterior means and medians, or the posterior vari-
ance. While the issue of convergence of the Markov chain to the target
distribution is of fundamental importance, many authors place emphasis

12.4 Inferences from the MCMC Output
551
on the properties of estimators of features of the posterior distribution.
For example, it is of interest to establish whether or not the diÔ¨Äerence be-
tween estimates of a posterior mean obtained from two or more independent
chains, can be explained by Monte Carlo sampling error. In this section,
two commonly used estimators of features of posterior distributions will
be presented. Other estimators are described in Robert and Casella (1999)
and in Chen et al. (2000), where a more formal treatment of the subject
can be found.
Ergodic Averages
Consider a single chain consisting of correlated samples Œ∏(i) (i = 1, 2, . . . , n)
from the target distribution [Œ∏|y]. As presented in the previous chapter
(expression (11.5)), for some function h (Œ∏), the ergodic theorem states
that the ergodic average of the function h (Œ∏), given by
1
n
n

i=1
h

Œ∏(i)
(12.19)
is a consistent estimator of

h (Œ∏) p (Œ∏|y) dŒ∏, provided that this integral
converges. That is,
1
n
n

i=1
h

Œ∏(i)
‚Üí

h (Œ∏) p (Œ∏|y) dŒ∏
(12.20)
as n ‚Üí‚àû, with probability 1, if

h (Œ∏) p (Œ∏|y) dŒ∏ < ‚àû. As mentioned
above, the rate of convergence may be seriously aÔ¨Äected by slow mixing
of the chain. At any rate, (12.19) is a consistent estimator of the expected
value of h (Œ∏) with respect to the invariant distribution [Œ∏|y], despite any
existing autocorrelation between the simulated values Œ∏(i) of the Markov
chain. For example, expression (12.19) is an estimator of:
‚Ä¢ the posterior mean if h (Œ∏) = Œ∏;
‚Ä¢ the posterior variance if h (Œ∏) = [Œ∏ ‚àíE (Œ∏|y)]2. The estimator of the
posterior variance is
1
n
n

i=1

Œ∏(i)2 ‚àí

5E (Œ∏|y)
2
,
where 5E (Œ∏|y) = 1
n

i Œ∏(i);
‚Ä¢ the posterior probability that Œ∏ ‚ààA, if h (Œ∏) = I (Œ∏ ‚ààA), such that
Pr (Œ∏ ‚ààA|y) =

I (Œ∏ ‚ààA) p (Œ∏|y) dŒ∏.

552
12. Implementation and Analysis of MCMC Samples
Here, the estimator is n
i=1 I

Œ∏(i) ‚ààA

/n. As a special case, the
cumulative distribution function is estimated as
5F (t) = 1
n
n

i=1
I

Œ∏(i) < t

.
Therefore, the estimator of the posterior probability that t1 < Œ∏ < t2
is
6
Pr (t1 < Œ∏ < t2|y) = 1
n
 n

i=1
I

t1 < Œ∏(i) < t2

;
‚Ä¢ the posterior predictive density:
p (z|y) =

p (z|Œ∏, y) p (Œ∏|y) dŒ∏,
where, usually, the form of the problem is such that p (z|Œ∏, y) =
p (z|Œ∏) . In this setting, h (Œ∏) = p (z|Œ∏) , and the estimator of the pre-
dictive density is n
i=1 p

z|Œ∏(i)
/n.
Rao‚ÄìBlackwell Estimator
Another estimator that has been proposed in the literature is known as
the Rao-Blackwell estimator (Gelfand et al., 1990; Liu et al., 1994; Casella
and Robert, 1996), which derives its name from the Rao‚ÄìBlackwell theorem.
This theorem states that conditioning an unbiased estimator on a suÔ¨Écient
statistic will result in a uniformly better unbiased estimator.
Let the parameter vector of a model consist of two scalars, that is, Œ∏ =
(Œ∏1, Œ∏2)‚Ä≤, and suppose that interest focuses on the mean of the marginal
posterior distribution of the function h (Œ∏1), or E [h (Œ∏1) |y]. As mentioned
above, the ergodic average estimator is
1
n
n

i=1
h

Œ∏(i)
1

,
(12.21)
where Œ∏(i)
1
is a sample from [Œ∏1|y]. The Rao‚ÄìBlackwell estimator is obtained
using results discussed in Chapter 1, Section 1.6. Recall that
E [h (Œ∏1) |y] =
 
h (Œ∏1) p (Œ∏1|Œ∏2, y) dŒ∏1

p (Œ∏2|y) dŒ∏2
=

EŒ∏1|Œ∏2,y [h (Œ∏1) |Œ∏2, y] p (Œ∏2|y) dŒ∏2
= EŒ∏2|y

EŒ∏1|Œ∏2,y [h (Œ∏1) |Œ∏2, y]

.

12.4 Inferences from the MCMC Output
553
The Rao‚ÄìBlackwell estimator has the form
5E [h (Œ∏1) |y] ‚âà1
n
n

i=1
EŒ∏1|Œ∏2,y

h (Œ∏1) |Œ∏(i)
2 , y

,
(12.22)
where Œ∏(i)
2 is a draw from the marginal posterior distribution [Œ∏2|y]. Hence,
the estimator is an ergodic average of conditional means, and one must be
able to write these in closed form, to be able to form (12.22). Recall from
(1.129) that the variance of h (Œ∏1|y) in (12.21) can be written as
V ar [h (Œ∏1|y)]
= V ar {E [h (Œ∏1) |Œ∏2, y]} + E {V ar [h (Œ∏1) |Œ∏2, y]} .
Therefore, V ar {E [h (Œ∏1) |Œ∏2, y]} ‚â§V ar [h (Œ∏1|y)] indicating that (12.22)
can improve upon (12.21) in terms of variance. While making optimal use
of the available data is a praiseworthy endeavor, the improvement of (12.22)
over (12.21) is often limited in chains that have been run long enough. This
improvement comes at the cost of having to know the closed form of the
expected value of the conditional posterior distribution [Œ∏1|Œ∏2, y] , and at a
loss of the simplicity with which (12.21) is calculated.
Density Estimation
Another way of estimating features of the posterior distribution of a pa-
rameter of interest is to obtain a smooth estimate of the posterior density
using the chain output, and then computing moments from this density
using numerical integration. This approach seems to be in disuse in output
analysis, in favor of the simple practice of approximating the density by
histograms, and of estimating features from posterior distributions using
(12.19) and (12.22), for example. The reader is referred to classical texts on
density estimation by Silverman (1992) and by Scott (1992) for a detailed
description of this approach.
12.4.2
Monte Carlo Variance
DeÔ¨Ånition
Here it is assumed that draws from the stationary distribution [Œ∏|y] are
available. Because only a Ô¨Ånite number of these draws can be obtained,
there is always sampling uncertainty associated with an estimator of fea-
tures of the target distribution, such as (12.19). This sampling variance
is known as the Monte Carlo (MC) variance of estimators of posterior
quantities. In principle, it can be made as small as desired, by taking a
suÔ¨Éciently large number of samples. This MC variance can be estimated
by running several independent chains, and then calculating the empirical,
between-chain variance of the estimates obtained for each chain. Since this

554
12. Implementation and Analysis of MCMC Samples
is often computationally expensive, one resorts to theoretical estimators of
MC variance. These estimators account for the autocorrelation among the
samples taken from the target distribution. Useful references are Ripley
(1987), Geyer (1992), and Chen et al. (2000). Here, two commonly used
estimators are described.
Consider estimating the cumulative distribution function
F (t|y) = Pr (Œ∏ < t|y)
from the MCMC output Œ∏(1), Œ∏(2), . . . , Œ∏(n). The estimator is
5F (t|y) = 1
n
n

i=1
I

Œ∏(i) < t

.
Now I

Œ∏(i) < t

has a Bernoulli distribution with success probability F (t|y).
Thus, if the draws Œ∏(1), Œ∏(2), . . . , Œ∏(n) were independent,
n

i=1
I

Œ∏(i) < t

would have a binomial distribution with parameters (F (t|y) , n). It follows
that with independent draws, the estimator of the Monte Carlo variance of
5F (t|y) would be equal to
D
V ar

5F (t|y)

= 1
n
5F (t|y)

1 ‚àí5F (t|y)

.
(12.23)
However, (12.23) may give a very distorted picture of the true Monte Carlo
variance, depending on the pattern of the autocorrelation among the sam-
ples from the target distribution. As shown by Liu et al. (1994), this pattern
can be complex since, in a reversible chain, even-lag autocorrelations are
non-negative, while odd-lag auto-covariances need not be positive.
Geyer‚Äôs Estimator
Geyer (1992) proposed an estimator of the Monte Carlo variance of the
estimator of the mean of h (Œ∏) based on time-series theory. The estimates
produced are larger than or equal to the true Monte Carlo variance. First,
from (12.19), deÔ¨Åne the estimator of the mean of h (Œ∏) as
5¬µ = 1
n
n

i=1
h

Œ∏(i)
.
(12.24)
Let the lag-t autocovariance of the stationary Markov chain h

Œ∏(i)
be
Œ≥ (t) = Cov

h

Œ∏(i)
, h

Œ∏(i+t)
,
i = 1, 2, . . . , n.

12.4 Inferences from the MCMC Output
555
An estimator of Œ≥ (t) is
5Œ≥ (t) = 1
n
n‚àít

i=1
9
h

Œ∏(i)
‚àí5¬µ
 
h

Œ∏(i+t)
‚àí5¬µ
:
.
(12.25)
Priestley (1981) mentions that it has been asserted that in general, this
biased estimator with divisor n has smaller mean square error than the
unbiased estimator with divisor n ‚àít. One of the estimators of the Monte
Carlo variance of 5¬µ proposed by Geyer (1992), which he calls the initial
positive sequence estimator, uses (12.25) as input and is equal to
D
V ar (5¬µ) = 1
n

5Œ≥ (0) + 2
i=2Œ¥+1

i=1
5Œ≥ (i)

,
(12.26)
where Œ¥ is chosen such that it is the largest integer satisfying
5Œ≥

2Œ¥‚Ä≤
+ 5Œ≥

2Œ¥‚Ä≤ + 1

> 0,
Œ¥‚Ä≤ = 0, 1, . . . , Œ¥.
If the samples are independent,
D
V ar (5¬µ) = 1
n5Œ≥ (0) .
An idea of the eÔ¨Äect of the autocorrelation on the amount of informa-
tion contained in the chain for inferring features of the target distribution,
can be obtained by computing an ‚ÄúeÔ¨Äective chain size‚Äù. Denote this as Œ®
(Sorensen et al., 1995), where
Œ® =
5Œ≥ (0)
D
V ar (5¬µ)
.
When the chain consists of independent draws from the target distribution,
Œ® = n, and the eÔ¨Äective and nominal sizes of the chain are equal.
Batching
A popular method of estimating Monte Carlo variances that is easy to
implement, is known as ‚Äúbatching‚Äù (Hastings, 1970). It is based on the
idea that if individual draws are correlated, grouping successive draws into b
batches or groups of size m each, and computing the raw averages, will lead
to b batch means that are less strongly inter-correlated than the original
draws. This can be so provided that m is chosen appropriately. Further, the
larger the autocorrelation among samples, the larger m must be. Suppose
that a chain of total length n is divided into b batches each of size m. Let
the average of the ith batch be
xi = 1
m
m

j=1
h

Œ∏(j)
,
i = 1, 2, . . . , b.

556
12. Implementation and Analysis of MCMC Samples
Here, h

Œ∏(j)
is some feature of the posterior distribution evaluated at the
sampled value Œ∏(j). The batch estimator of the variance of (12.24), assuming
that m is large enough so that the x‚Ä≤
is are uncorrelated, is equal to
D
V arb (5¬µ) =
b
i=1
(xi ‚àí5¬µ)2
b (b ‚àí1)
.
(12.27)
An estimate of the batch-eÔ¨Äective chain size can be obtained as
Œ®b =
n
i=1

h

Œ∏(i)
‚àí5¬µ
2
(n ‚àí1) D
V arb (5¬µ)
.
When the samples are independent, m = 1, xi = h

Œ∏(i)
for all i, and
Œ®b = n.
If the autocorrelation among the samples of the chain is very high (>
0.95), estimator (12.26) seems to be preferred over (12.27).
MCMC algorithms converge to the target distribution asymptotically
and the samples are typically correlated. A new and exciting approach,
termed perfect sampling proposed by Propp and Wilson (1996), avoids
problems of convergence and of serial correlations, since it generates inde-
pendent draws from the target distribution. This is an area where research
is just beginning; it is not clear at the moment whether the technique can
applied in settings involving high dimensional distributions without nice
symmetry properties. A tutorial can be found in Casella et al. (2001).
12.5
Sensitivity Analysis
An important part of a Bayesian analysis is the study of how robust are
inferences to modeling assumptions, including prior and likelihood speci-
Ô¨Åcations, or presence of outliers. Smith and Gelfand (1992) describe how
to address this question using importance sampling, a technique that was
described by Hammersley and Handscomb (1964). Geweke (1989) showed
how importance sampling can be applied in Bayesian analyses. This tech-
nique was already encountered in Chapter 8, in connection with estimation
of the marginal likelihood from the Monte Carlo samples, and is discussed
again in Chapter 15. Other relevant literature on the subject can be found
in Tanner and Wong (1987), Rubin (1987b), and Gelfand and Smith (1990).
The starting point of a Bayesian analysis is the posterior density
p1 (Œ∏|y) = c1p1 (Œ∏) p (y|Œ∏) ,
where c1 is the typically unknown normalizing constant, p1 (Œ∏) is the density
of some prior distribution assigned to the parameter, and p (y|Œ∏) is the

12.5 Sensitivity Analysis
557
likelihood. The expectation of a function h (Œ∏) with respect to the posterior
distribution with density p1 (Œ∏|y) is
E1 [h (Œ∏)] =

h (Œ∏) p1 (Œ∏|y) dŒ∏.
Suppose that n draws Œ∏(i), (i = 1, 2, . . . , n) are available from this posterior
distribution. Based on (12.19), E1 [h (Œ∏)] can be estimated as
5E1 [h (Œ∏)] = 1
n
n

i=1
h

Œ∏(i)
.
(12.28)
Now, one may be interested in inferences about h (Œ∏) , conditionally on
the same data, but using a diÔ¨Äerent set of modeling assumptions. These
could involve perturbations either of the likelihood (this could take a new
functional form, or perhaps part of the data could be omitted) or of the
prior distribution. For example, suppose that one wishes to study the con-
sequences of changing the prior speciÔ¨Åcation, such that the new posterior
becomes
p2 (Œ∏|y) = c2p2 (Œ∏) p (y|Œ∏) .
(12.29)
Here, p2 (Œ∏) is the density of the ‚Äúnew‚Äù prior distribution, and c2 is the
corresponding integration constant. Using the draws Œ∏(i) generated under
the distribution with density p1 (Œ∏|y), inferences about h (Œ∏) under the new
posterior with density p2 (Œ∏|y) can be obtained without having to run the
MCMC procedure again. This is done by using p1 (Œ∏|y) as importance sam-
pling density. Thus, expectations under p2 (Œ∏|y) can be obtained as follows
E2 [h (Œ∏)] =

h (Œ∏) p2(Œ∏|y)
p1(Œ∏|y)p1 (Œ∏|y) dŒ∏
 p2(Œ∏|y)
p1(Œ∏|y)p1 (Œ∏|y) dŒ∏
=

h (Œ∏) c2p2(Œ∏)p(y|Œ∏)
c1p1(Œ∏)p(y|Œ∏)p1 (Œ∏|y) dŒ∏
 c2p2(Œ∏)p(y|Œ∏)
c1p1(Œ∏)p(y|Œ∏)p1 (Œ∏|y) dŒ∏
=

h (Œ∏) w (Œ∏) p1 (Œ∏|y) dŒ∏

w (Œ∏) p1 (Œ∏|y) dŒ∏
,
(12.30)
where
w (Œ∏) = p2 (Œ∏)
p1 (Œ∏).
Note that in the second line of (12.30) the ratio of constants of integra-
tion and the likelihood cancel out in the numerator and denominator. A
consistent estimator of (12.30) based on (12.19) is
5E2 [h (Œ∏)] =
n
i=1 h

Œ∏(i)
w

Œ∏(i)
n
i=1 w

Œ∏(i)
,
(12.31)

558
12. Implementation and Analysis of MCMC Samples
where Œ∏(i), (i = 1, 2, . . . , n) are the draws from the distribution with density
p1 (Œ∏|y). The weight function wi is equal to
w

Œ∏(i)
=
p2

Œ∏(i)
p1

Œ∏(i).
If wi = 1 for all i, p2 (Œ∏|y) = p1 (Œ∏|y) and (12.30) is equal to (12.28).
Moments and quantiles under the new posterior distribution can be ob-
tained along the same lines, using the draws from the original posterior
distribution. For instance
D
V ar2 [h (Œ∏)] = 5E

h2 (Œ∏)

‚àí

5E [h (Œ∏)]
2
=
n
i=1
h2 
Œ∏(i)
w

Œ∏(i)
n
i=1
w

Œ∏(i)
‚àí

5E [h (Œ∏)]
2
(12.32)
and
6
Pr2 [h (Œ∏) < t] =
n
i=1
I

h

Œ∏(i)
< t

w

Œ∏(i)
n
i=1
w

Œ∏(i)
,
(12.33)
where subscript 2 indicates that inferences are being drawn from the pos-
terior distribution with density p2 [Œ∏|y].
Often, it can be computationally advantageous to Ô¨Åt a particular model
elicited under a certain prior or likelihood speciÔ¨Åcation. However, the ana-
lyst may have in mind an alternative model which is less tractable compu-
tationally. The approach described above provides a powerful tool for doing
this in a rather straightforward manner. This is illustrated in the following
example.
Example 12.3
Inferences from two beta distributions
Suppose n independent draws are made from a Bernoulli distribution with
unknown probability of success Œ∏. Let x denote the number of successes
and y the number of failures. The likelihood is
p (x|Œ∏, n) ‚àùŒ∏x (1 ‚àíŒ∏)y .
(12.34)
The experimenter wishes to perform the Bayesian analysis under two dif-
ferent sets of prior assumptions. The Ô¨Årst model assumes a uniform prior
distribution for Œ∏, Un (0, 1):
p1 (Œ∏) = 1,
0 ‚â§Œ∏ ‚â§1.
(12.35)

12.5 Sensitivity Analysis
559
Mean √ó 10
Variance √ó 102
Probability
S
x
y
Exact
IS
Exact
IS
Exact
IS
4
4
1
6.364
6.356
1.9284
1.9757
0.1742
0.1710
100
4
1
6.364
6.361
1.9284
1.9322
0.1742
0.1728
1000
4
1
6.364
6.365
1.9284
1.9286
0.1742
0.1743
4
12
3
7.143
7.134
0.9276
0.9515
0.3155
0.3004
100
12
3
7.143
7.141
0.9276
0.9283
0.3155
0.3124
1000
12
3
7.143
7.143
0.9276
0.9277
0.3155
0.3157
TABLE 12.1. Comparison betwen exact results and estimates based on impor-
tance sampling (IS). S: number of samples in thousands; x: number of successes;
y: number of failures; Probability: posterior probability that the binomial param-
eter takes a value between 0.75 and 0.85.
Under this prior, the posterior density is proportional to (12.34)
p1 (Œ∏|x, n) ‚àùŒ∏x (1 ‚àíŒ∏)y ,
(12.36)
which is recognized as the density of a beta distributed random variable
with parameters x + 1, y + 1, that is Be (Œ∏|x + 1, y + 1). The second model
assumes the same likelihood, but the prior distribution for Œ∏ is beta, with
parameters a and b. The posterior density is now
p2 (Œ∏|x, n) ‚àùŒ∏a+x‚àí1 (1 ‚àíŒ∏)b+y‚àí1 ,
(12.37)
which is the density Be (Œ∏|a + x, b + y). In this example, the form of the
posterior distribution is known under either prior, so it is straightforward
to draw inferences from (12.36) or from (12.37). To illustrate, independent
samples will be drawn from (12.36), and then importance sampling will be
used to obtain inferences based on (12.37), using the draws from (12.36).
Further, the Monte Carlo-based estimates will then be compared with ex-
act results.
The results for Œ∏ = 0.8, obtained with n = x + y = 5 or 15, are shown in
Table 12.1, for three importance sampling sample sizes. The focus of infer-
ence is on the posterior mean and variance, and on the probability that the
value of Œ∏ lies between 0.75 and 0.85. In the model that provides the basis
of inference, p2 (Œ∏|x, n), the parameters of the Beta prior are a = b = 3.
The results in the table illustrate that the estimator is consistent: as the
number of samples increases from 4000 to 1 million, the estimates based
on (12.31), (12.32), and (12.33) converge to the true values.
When the probability to be estimated is small, a larger number of im-
portance samples must be drawn to achieve the same level of precision.
For example, the true probability that Œ∏ lies between 0.3 and 0.4, based
on p2 (Œ∏|x, n) , is 15.68 √ó 10‚àí4. Estimates obtained with sample sizes of
four thousand, one hundred thousand and one million were 12.10 √ó 10‚àí4,
14.66 √ó 10‚àí4, and 15.75 √ó 10‚àí4, respectively.
‚ñ†

560
12. Implementation and Analysis of MCMC Samples
While in this example the importance sampling approach performs sat-
isfactorily, in higher-dimensional problems the relative weights
w

Œ∏(i)
n
i=1
w

Œ∏(i)
may be concentrated on a small number of samples. As a consequence, the
Monte Carlo sampling error associated with estimates of posterior features
is likely to be large. A larger eÔ¨Äective sample size is required in order to
mitigate this drawback.

Part IV
Applications in
Quantitative Genetics
561

This page intentionally left blank

13
Gaussian and Thick-Tailed
Linear Models
13.1
Introduction
The fourth part of this book illustrates applications of MCMC methods in
genetic analyses, in a Bayesian context. The treatment, in parts, is rather
schematic, as the objective is to present the mechanics of MCMC sam-
pling in diÔ¨Äerent modeling scenarios. Attention is restricted to models that
appear quite often in quantitative genetics, e.g., linear speciÔ¨Åcations (uni-
variate and multivariate), binary and ordered polychotomous responses,
longitudinal trajectories, segregation analysis, and the study of QTL.
We start in this chapter with a class of models that is probably the
most common in animal breeding applications, the Gaussian model. Here
the data and other random components are assumed to follow a multi-
variate normal distribution and, further, location parameters and data are
linearly related. The model is discussed in several settings, including situ-
ations where one (univariate) or several (multivariate) response variables
are measured, and where traits may be inÔ¨Çuenced by maternal eÔ¨Äects.
Also, procedures for robust (in some sense) analysis of linear models are
discussed. The reader should be aware that in several of the applications
discussed below, other approaches may be computationally more eÔ¨Écient
than the MCMC algorithms presented here. Further, an MCMC algorithm
can be tailored in many diÔ¨Äerent ways, and it is not claimed that the im-
plementations discussed are, necessarily, the best ones. The Ô¨Ånal section
gives a brief discussion of the impact of the alternative parameterizations
of a linear model on the behavior of Gibbs sampling algorithms.

564
13. Gaussian and Thick-Tailed Linear Models
13.2
The Univariate Linear Additive
Genetic Model
This model was introduced in Example 1.18 of Chapter 1. Genetic aspects
of the model were described brieÔ¨Çy in Subsection 1.4.4 of the same chapter.
For analytical details, see Chapter 6.
A phenotypic record for a given trait is modeled as a linear combination
of eÔ¨Äects of some explanatory variables. It is assumed that the distribution
of data y (vector of order n) for this trait, given some parameters Œ≤, a,
and œÉ2
e, is the multivariate normal process
y|Œ≤, a, œÉ2
e ‚àºN

XŒ≤ + Za, IœÉ2
e

.
(13.1)
Here Œ≤ is a vector of ‚ÄúÔ¨Åxed‚Äù (in a frequentist sense) eÔ¨Äects of order p,
a is the vector of additive genetic values of order q, X and Z are known
incidence matrices associating Œ≤ and a with y, I is an identity matrix
of order n √ó n, and œÉ2
e is the variance of this conditional distribution,
often referred to as the residual variance of the model. Genotypic values
of individuals in a pedigree result from the sum of a very large number of
independent contributions from many independently segregating loci, each
with a small eÔ¨Äect. The number of individuals in the pedigree (q) is often
larger than the number of phenotypic records (n), which implies that Z has
q ‚àín null columns. The genetic model justiÔ¨Åes invoking the central limit
theorem, which allows us to write
a|A, œÉ2
a ‚àºN

0, AœÉ2
a

.
(13.2)
Above, A is the additive genetic relationship matrix (of dimension q √ó q)
and œÉ2
a is the additive genetic variance in some conceptual or ‚Äúbase‚Äù pop-
ulation. From a classical point of view, the parameters of the distribution
(13.2) result from a hypothetical conceptual repeated sampling process in
which vectors of additive genetic values of order q √ó 1 are drawn at ran-
dom, while maintaining the pedigree constant (i.e., with A Ô¨Åxed in every
repetition of such sampling). Hence, a is called a ‚Äúrandom‚Äù eÔ¨Äect. From
a Bayesian perspective, on the other hand, (13.2) represents the uncer-
tainty distribution about genetic eÔ¨Äects before data are observed or, in
other words, it is the prior distribution of such eÔ¨Äects. A large value of
œÉ2
a implies large uncertainty. The parameters which are the focus of in-
ference are Œ≤, a, œÉ2
a, and œÉ2
e and, possibly, functions thereof, such as the
coeÔ¨Écient of heritability œÉ2
a/(œÉ2
a + œÉ2
e). From a frequentist point of view,
Œ≤ must be deÔ¨Åned uniquely (X must have full-column rank); otherwise,
there is an identiÔ¨Åcation problem. Hence, inferences would need to center
on linearly estimable functions of Œ≤ (Searle, 1971). Hereinafter, only the
Bayesian approach is considered, where the identiÔ¨Åcation problem in the-
ory disappears whenever a proper distribution is assigned to Œ≤ (Bernardo
and Smith, 1994).

13.2 The Univariate Linear Additive Genetic Model
565
To carry out a Bayesian analysis, prior distributions must be assigned to
each of Œ≤, a, œÉ2
a and œÉ2
e. A distribution which approximates the notion of
vague prior knowledge about Œ≤ is the Ô¨Çat prior
p (Œ≤) ‚àùconstant.
(13.3)
This is an improper prior distribution, which can be made proper by as-
signing upper and lower limits to each of the elements of Œ≤. In this case,
the posterior distribution will then be deÔ¨Åned within these assigned limits.
In a Bayesian model with known variance components, use of priors (13.2)
and (13.3) yields normal marginal posterior distributions for both Œ≤ and
a, with mean values equal to the ML estimator of Œ≤ and to the BLUP of
a, respectively (see Chapter 6).
Two common prior speciÔ¨Åcations for the variance components are either
proper uniform distributions or scaled inverted chi-square distributions.
The corresponding densities have the forms
p

œÉ2
i

=
1
œÉ2
i max
,
0 < œÉ2
i < œÉ2
i max, i = a, e,
(13.4)
and
p

œÉ2
i |ŒΩi, S2
i

‚àù

œÉ2
i
‚àí(
ŒΩi
2 +1) exp

‚àíŒΩiS2
i
2œÉ2
i

,
i = a, e.
(13.5)
In (13.4), œÉ2
i max is the maximum value that œÉ2
i is allowed to take, according
to mechanistic considerations or prior knowledge about the trait. In (13.5),
ŒΩi and S2
i are parameters of the corresponding scaled inverted chi-square
distribution. Here it is assumed that these hyperparameters (like œÉ2
i max)
are known; otherwise, these can be assigned prior distributions, in a hi-
erarchical manner. The prior speciÔ¨Åed by (13.5) reduces to an improper
uniform distribution by taking ŒΩi = ‚àí2 and S2
i = 0.
Assuming that Œ≤,

a, œÉ2
a

, and œÉ2
e are independent a priori , the joint
posterior density of all unknown quantities is proportional to
p

Œ≤, a, œÉ2
a, œÉ2
e|y

‚àùp (Œ≤) p

a|œÉ2
a

p

œÉ2
a

p

œÉ2
e

p

y|Œ≤, a, œÉ2
e

.
(13.6)
In the notation, conditioning on hyperparameters is omitted. Using (13.1),
(13.2), (13.3) and, for instance, (13.5), the joint posterior density (13.6) is
given by
p

Œ≤, a, œÉ2
a, œÉ2
e|y

‚àù

œÉ2
e
‚àí(
n+ŒΩe
2
+1) 
œÉ2
a
‚àí(
q+ŒΩa
2
+1)
√ó exp

‚àí(y ‚àíXŒ≤ ‚àíZa)‚Ä≤ (y ‚àíXŒ≤ ‚àíZa) + ŒΩeS2
e
2œÉ2e

exp

‚àía‚Ä≤A‚àí1a + ŒΩaS2
a
2œÉ2a

.
(13.7)
The joint posterior density of a model that assumes instead improper uni-
form prior distributions for the variance components, is obtained by setting

566
13. Gaussian and Thick-Tailed Linear Models
ŒΩi = ‚àí2 and S2
i = 0 in (13.7). A word of caution is in order here: when
improper priors are assigned to

Œ≤, œÉ2
a, œÉ2
e

, the posterior distribution may
not always be proper (Hobert and Casella, 1996).
13.2.1
A Gibbs Sampling Algorithm
The single-site, systematic scan Gibbs sampling algorithm described be-
low is based on the fully conditional posterior distributions of each scalar
parameter. As usual, these are deduced from the joint posterior (13.7).
However, for the location parameters a and Œ≤, the derivation is simpler if
one appeals to the more general results for mixed linear models given in
Chapter 6; see also Example 1.18 in Chapter 1. First, note that the fully
conditional posterior density of a and Œ≤ is proportional to
p

Œ≤, a|œÉ2
a, œÉ2
e, y

‚àùp (Œ≤) p

a|œÉ2
a

p

y|Œ≤, a,œÉ2
e

.
(13.8)
Rather than manipulating this expression, we proceed as follows. As in
Example 1.18 of Chapter 1, let
XŒ≤ + Za = WŒ∏,
and
Œ£ =

0
0
0
A‚àí1k

,
where W =

X
Z

, Œ∏ =

Œ≤‚Ä≤, a‚Ä≤‚Ä≤, and k = œÉ2
e/œÉ2
a. Then, using results
presented in Chapter 6, the conditional posterior distribution of Œ∏ is
Œ∏|œÉ2
a, œÉ2
e, y ‚àºN

5Œ∏, C‚àí1œÉ2
e

,
(13.9)
where C = W‚Ä≤W + Œ£, and the posterior mean 5Œ∏ is the solution to the
linear system:
C5Œ∏ = W‚Ä≤y = r.
(13.10)
One way of deriving the fully conditional posterior distribution of the ith
element of Œ∏ is as follows. Let Œ∏‚àíi be Œ∏, except for the ith element (Œ∏i) ,
which is removed from the entire location vector. The results that follow
hold irrespective of whether or not Œ∏i is a scalar or a vector. Based on
Example 1.18, one obtains
Œ∏i|Œ∏‚àíi, œÉ2
a, œÉ2
e, y ‚àºN

,Œ∏i, C‚àí1
i,i œÉ2
e

(13.11)
where ,Œ∏i satisÔ¨Åes
Ci,i,Œ∏i = (ri ‚àíCi,‚àíiŒ∏‚àíi) .
(13.12)
For example, when Œ∏i is a scalar, Ci,i is the diagonal element of the coef-
Ô¨Åcient matrix of the mixed model equations (13.10), ri is the ith element

13.2 The Univariate Linear Additive Genetic Model
567
of the right-hand side vector in (13.10) associated with Œ∏i, and Ci,‚àíi is a
row vector obtained by deleting element i from the ith row of C. Notice
that drawing samples from the distribution

Œ∏i|Œ∏‚àíi, œÉ2
a, œÉ2
e, y

, as required
in Gibbs sampling, does not require inversion of matrices if Œ∏i is a scalar.
Expression (13.11) is quite general and applies, with appropriate minor
changes, to all fully conditional densities of location parameters in Gaus-
sian linear models.
In order to derive the fully conditional posterior distribution of the vari-
ance components, one retains only those terms in (13.7) that involve the rel-
evant variance component. Note that, given Œ∏ =

Œ≤‚Ä≤, a‚Ä≤‚Ä≤, the two variance
components are conditionally independent. The fully conditional posterior
distribution of œÉ2
a is given by
p

œÉ2
a|Œ≤, a, œÉ2
e, y

‚àù

œÉ2
a
‚àí(
q+ŒΩa
2
+1) exp

‚àía‚Ä≤A‚àí1a + ŒΩaS2
a
2œÉ2a

=

œÉ2
a
‚àí(
ŒΩa
2 +1) exp

‚àí,ŒΩa ,S2
a
2œÉ2a

,
(13.13)
where
,S2
a =

a‚Ä≤A‚àí1a + ŒΩaS2
a

/,ŒΩa
and ,ŒΩa = q + ŒΩa. By inspection, it follows that (13.13) is in the form of
a scaled inverted chi-square density with parameters ,ŒΩa and ,S2
a. In short,
one can then write
œÉ2
a|Œ≤, a, œÉ2
e, y ‚àº,ŒΩa ,S2
aœá‚àí2
ŒΩa .
(13.14)
To sample from (13.14), a draw is made from a chi-square distribution
with ,ŒΩa = q + ŒΩa degrees of freedom, and the reciprocal of this number
is multiplied by ,ŒΩa ,S2
a =

a‚Ä≤A‚àí1a + ŒΩaS2
a

. The resulting quantity is a
realization from the scaled inverted chi-square process (13.14).
Similarly, collecting those terms from (13.7) that involve œÉ2
e only yields
p

œÉ2
e|Œ≤, a, œÉ2
a, y

‚àù

œÉ2
e
‚àí(
n+ŒΩe
2
+1)
√ó exp

‚àí(y ‚àíXŒ≤ ‚àíZa)‚Ä≤ (y ‚àíXŒ≤ ‚àíZa) + ŒΩeS2
e
2œÉ2e

=

œÉ2
e
‚àí(
ŒΩe
2 +1) exp

‚àí,ŒΩe ,S2
e
2œÉ2e

,
(13.15)
where ,ŒΩe = n + ŒΩe and
,S2
e =

(y ‚àíXŒ≤ ‚àíZa)‚Ä≤ (y ‚àíXŒ≤ ‚àíZa) + ŒΩeS2
e

/,ŒΩe.
By inspection, (13.15) is proportional to the density of the following scaled
inverted chi-square distribution with parameters ,ŒΩe and ,S2
e
œÉ2
e|Œ≤, a, œÉ2
a, y ‚àº,ŒΩe ,S2
eœá‚àí2
ŒΩe .
(13.16)

568
13. Gaussian and Thick-Tailed Linear Models
ID
Father
Mother
Sex
Record
1
‚àí
‚àí
1
‚àí
2
‚àí
‚àí
2
‚àí
3
1
‚àí
1
y3
4
1
2
2
y4
5
3
4
1
y5
6
1
4
2
y6
7
5
6
1
‚àí
TABLE 13.1. A hypothetical example.
The implementation of the Gibbs sampler consists of drawing succes-
sively from (13.11) for each location parameter (or block of parameters
whenever Œ∏i is vector valued), and from the distributions (13.14) and
(13.16). The process is repeated as needed to satisfy convergence require-
ments, and to attain a reasonably small Monte Carlo error.
Example 13.1
An additive genetic model
Consider the data in Table 13.1. There are seven individuals, but only those
numbered as 3, 4, 5, and 6 have a phenotypic record. Hence, the pedigree
information available involves a total of q = 7 individuals, and n = 4. Note
that subjects 4 to 7 have known mothers and fathers, whereas the parents
of subjects 1 and 2 are unknown; for individual 3, only the father is known.
Suppose the sex of the individual is the only source of heterogeneity, other
than the subjects themselves. Hence, the vector Œ≤ contains the eÔ¨Äects of
sex [S1, S2]‚Ä≤ on the trait, and the vector of additive genetic values of the
seven individuals is a = [a1, a2, . . . , a7]‚Ä≤. The variance components will be
assigned uniform distributions a priori, as in (13.4), to convey (naively) a
state of vague prior knowledge about their values. Arbitrary starting values
adopted for the Gibbs sampler are: Œ≤(0)= 0, a(0)= 0, œÉ2(0)
a
= 5, œÉ2(0)
e
= 5,
therefore, the implied starting value for the ratio of variance component is
k(0) = 1. A convenient algorithm for drawing samples from (13.11) is based
on the mixed model equations. Note that C is a 9 √ó 9 matrix, the order
resulting from p = 2 and q = 7. The Ô¨Årst seven columns of the coeÔ¨Écient
matrix of the mixed model equations are
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
2.00
0.00
0.00
0.00
1.00
0.00
1.00
0.00
2.00
0.00
0.00
0.00
1.00
0.00
0.00
0.00
2.33k
0.50k
‚àí0.66k
‚àí0.50k
0.00k
0.00
0.00
0.50k
1.50k
0.00k
‚àí1.00k
0.00k
1.00
0.00
‚àí0.66k
0.00k
1 + 2.83k
0.50k
‚àí1.00k
0.00
1.00
‚àí0.50k
‚àí1.00k
0.50k
1 + 3k
‚àí1.00k
1.00
0.00
0.00k
0.00k
‚àí1.00k
‚àí1.00k
1 + 2.62k
0.00
1.00
‚àí1.00k
0.00k
0.00k
‚àí1.00k
0.62k
0.00
0.00
0.00k
0.00k
0.00k
0.00k
‚àí1.23k
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
,

13.2 The Univariate Linear Additive Genetic Model
569
and the last two columns are
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
0.00
0.00
1.00
0.00
‚àí1.00k
0.00k
0.00k
0.00k
0.00k
0.00k
‚àí1.00k
0.00k
0.62k
‚àí1.23k
1 + 2.62k
‚àí1.23k
‚àí1.23k
2.46k
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
.
The location parameters are
Œ∏ = [S1, S2, a1, . . . , a7]‚Ä≤ .
The single-site, systematic Gibbs sampler draws the parameters at each
iteration in the order in which they appear in Œ∏ above. At iteration 1, the
Ô¨Årst draw from the fully conditional distribution, with density
p

S1|S2, a1, . . . , a7, œÉ2
e, y

,
is obtained as:
S(1)
1 |S(0)
2 , a(0)
1 , . . . , a(0)
7 , œÉ2(0)
e
, œÉ2(0)
a
, y ‚àºN

5S(1)
1 , œÉ2(0)
e
2

,
where, in view of the starting values adopted, 5S(1)
1
= (y3 + y5) /2. Next,
draw S(1)
2
from
S(1)
2 |S(1)
1 , a(0)
1 , . . . , a(0)
7 , œÉ2(0)
e
, œÉ2(0)
a
, y ‚àºN

5S(1)
2 , œÉ2(0)
e
2

,
where 5S(1)
2
= (y4 + y6) /2. Subsequently, draw a(1)
1
from
a(1)
1 |S(1)
1 , S(1)
2 , a(0)
2 , . . . , a(0)
7 , œÉ2(0)
e
, œÉ2(0)
a
, y ‚àºN

5a(1)
1 ,
œÉ2(0)
e
2.33k(0)

,
where 5a(1)
1
= 0/2.33k(0) = 0. The process is continued systematically for
the genetic eÔ¨Äects of individuals 2, 3, 4, 5, 6, and 7. For example, additive
genetic value 6 in iteration 1, a(1)
6 , is sampled from
a(1)
6 |S(1)
1 , S(1)
2 , a(1)
1 , . . . , a(1)
5 , a(0)
7 , œÉ2(0)
e
, œÉ2(0)
a
, y ‚àºN

5a(1)
6 ,
œÉ2(0)
e
1 + 2.62k(0)

,

570
13. Gaussian and Thick-Tailed Linear Models
where
5a(1)
6
=
y6 ‚àíS(1)
2
‚àík(0) 
‚àí1.00a(1)
1
‚àí1.00a(1)
4
+ 0.62a(1)
5
‚àí1.23a(1)
7


1 + 2.62k(0)
.
Finally, for additive genetic value 7, sample a(1)
7
from
a(1)
7 |S(1)
1 , S(1)
2 , a(1)
1 , . . . , a(1)
6 , œÉ2(0)
e
, œÉ2(0)
a
, y ‚àºN

5a(1)
7 ,
œÉ2(0)
e
1 + 2.46k(0)

,
where
5a(1)
7
=

0 ‚àík(0) 
‚àí1.23a(1)
5
‚àí1.23a(1)
6

2.46k(0)
.
Having sampled all location parameters, one proceeds to drawing the two
variance components, to complete the Ô¨Årst iteration of the sampler. First,
the following sums of squares are computed:
SS(1)
e
=

y ‚àíXŒ≤(1) ‚àíZa(1)‚Ä≤ 
y ‚àíXŒ≤(1) ‚àíZa(1)
,
and
SS(1)
a
= a‚Ä≤(1)A‚àí1a(1).
The Ô¨Årst-round sample for the additive genetic variance, œÉ2(1)
a
, is extracted
from
œÉ2
a|Œ≤, a, y ‚àºSS(1)
a œá‚àí2
7‚àí2,
and the residual variance œÉ2(1)
e
from
œÉ2
e|Œ≤, a, y ‚àºSS(1)
e œá‚àí2
4‚àí2.
Algorithmically, the second round of iteration starts by updating the ratio
of variance components k(1) = œÉ2(1)
e
/œÉ2(1)
a
, followed by updating the coeÔ¨É-
cient matrix of the mixed model equations. The iteration then proceeds as
sketched above.
‚ñ†
13.3
Additive Genetic Model with
Maternal EÔ¨Äects
This model was proposed by Willham (1963), and has been used widely
in animal breeding applications. Here, it is assumed that an oÔ¨Äspring‚Äôs
attribute (or phenotypic record) is aÔ¨Äected by the ‚Äúusual‚Äù genetic and
environmental eÔ¨Äects, plus a contribution from its mother‚Äôs phenotype. The
latter may include both genetic and nongenetic components of maternal

13.3 Additive Genetic Model with Maternal EÔ¨Äects
571
origin, but it will be assumed here that the maternal eÔ¨Äect is only genetic
in nature, although it acts as an environmental inÔ¨Çuence on the oÔ¨Äspring‚Äôs
record. The maternal inÔ¨Çuence is transmitted in a Mendelian manner, so
both males and females carry genes for maternal eÔ¨Äects. Naturally, genetic
diÔ¨Äerences for maternal eÔ¨Äects can be assessed only when females produce
oÔ¨Äspring.
A typical example of a trait aÔ¨Äected by maternal eÔ¨Äects is body weight
at weaning in cattle or sheep. At weaning time, variation in body weight
can be partly attributed to the calf‚Äôs genes (direct genetic eÔ¨Äects), by the
pre- and post-natal environment provided by the dam of the animal (e.g.,
amount of milk available for suckling), and to environmental eÔ¨Äects stem-
ming from sources other than the mother‚Äôs inÔ¨Çuence. Again, although the
maternal eÔ¨Äects are environmental vis-a-vis the measurement made in the
oÔ¨Äspring, part of the variation in milk yield between dams is assumed to be
additive genetic. The calf or lamb receives a sample of 50% of its mother‚Äôs
and father‚Äôs autosomal genes for milk yield. If the calf is female, and if it
becomes a mother, these genes will inÔ¨Çuence her oÔ¨Äspring‚Äôs weaning weight.
With this model, the dispersion parameters of interest are:
(1) the additive genetic variance of ‚Äúdirect‚Äù eÔ¨Äects aÔ¨Äecting the trait e.g.,
calf‚Äôs weaning weight;
(2) the additive genetic variance for maternal eÔ¨Äects;
(3) a possible additive genetic covariance between direct and maternal ef-
fects; and
(4) the residual variance.
As usual, interest may also focus on the posterior distribution of location
parameters or functions thereof, and on genetic parameters such as the
heritabilities of direct and maternal eÔ¨Äects, and the genetic correlation
between direct and maternal eÔ¨Äects. For applications to livestock breeding
and for variations of the model, see Van Vleck (1993); an extension of
Willham‚Äôs speciÔ¨Åcation can be found in Koerkhuis and Thompson (1997).
A Bayesian analysis of a maternal eÔ¨Äects model via the Gibbs sampler was
described by Jensen et al. (1994).
The model used here to illustrate the algorithm is as follows
y = XŒ≤ + Zmm + Zaa + e.
(13.17)
The notation is as for model (13.1), with the only novelty being the in-
troduction of m, a vector of order q of maternal additive genetic values;
a is now the vector of order q of direct additive genetic values, and Zm
and Za are known incidence matrices associating the data to m and to a,
respectively. The model is written in a manner that allows each individual
in a pedigree to have both direct and maternal genetic eÔ¨Äects, and this
is the reason for which the order of the two vectors is q √ó 1; this will be
illustrated below

572
13. Gaussian and Thick-Tailed Linear Models
ID
Father
Mother
Sex
Record
1‚àó
‚àí
‚àí
2
‚àí
2
‚àí
‚àí
1
‚àí
3
‚àí
‚àí
2
‚àí
4
2
1‚àó
1
y4
5
2
3
2
y5
6
4
5
1
y6
7
2
5
2
y7
8
6
7
1
‚àí
TABLE 13.2. Hypothetical data structure from a maternal eÔ¨Äects model.
Example 13.2
Incidence matrices in a maternal eÔ¨Äects model
To illustrate the structure of matrices Zm and Za, we revert to the example
in Table 13.1. Notice that individual 3 does not have a known mother. It
simpliÔ¨Åes matters algorithmically, if a ‚Äúphantom‚Äù mother is created. After
creating the ‚Äúphantom‚Äù mother (1‚àó) of individual 3 (which now becomes
individual 4, as all individuals are renumbered), the data looks as in Table
13.2.
Based on the data in Table 13.2, the structure of the matrix Zm, of order
4 √ó 8 and which links a data point to its mother, is
Ô£Æ
Ô£ØÔ£ØÔ£∞
1
0
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
1
0
0
0
Ô£π
Ô£∫Ô£∫Ô£ª,
and note that the columns are nonnull only for those individuals that are
mothers and that have a measured progeny (i.e., 1‚àó, 3, 5). Likewise, the
structure of the matrix Za, also of order 4 √ó 8, is
Ô£Æ
Ô£ØÔ£ØÔ£∞
0
0
0
1
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
1
0
Ô£π
Ô£∫Ô£∫Ô£ª.
Here, the nonnull columns pertain to the individuals with measurements
(4 to 7) .
‚ñ†
Proceeding with the deÔ¨Ånition of the parameters of the model, let G0
be a 2 √ó 2 variance‚Äìcovariance matrix whose diagonal elements are œÉ2
m,
the maternal additive genetic variance, and œÉ2
a, the direct additive genetic
variance; œÉa,m, the additive genetic covariance between direct and mater-
nal eÔ¨Äects, is in the oÔ¨Ä-diagonal position. The genetic correlation between
maternal and direct additive genetic eÔ¨Äects is, then
ra,m = œÉa,m
œÉaœÉm
.

13.3 Additive Genetic Model with Maternal EÔ¨Äects
573
The inverse of the matrix G0 is
G‚àí1
0
=

œÉ2
m
œÉa,m
œÉa,m
œÉ2
a
‚àí1
=

gm,m
gm,a
gm,a
ga,a

.
The covariance structure associated with the entire vector of genetic eÔ¨Äects
[m‚Ä≤, a‚Ä≤]‚Ä≤ is
G =V ar

m
a

= G0 ‚äóA,
(13.18)
where the symbol ‚äóstands, as usual, for ‚Äúdirect product‚Äù, and A is the
additive genetic relationship matrix.
According to the model postulated, the variance of y (in the frequentist
sense, where both Œ≤ and G0 are Ô¨Åxed parameters) is given by
V ar (y) = V ar (XŒ≤ + Zmm + Zaa + e)
=

Zm
Za

V ar

m
a
 
Z‚Ä≤
m
Z‚Ä≤
a

+ V ar (e)
=

Zm
Za

[G0 ‚äóA]

Z‚Ä≤
m
Z‚Ä≤
a

+ IœÉ2
e
= ZmAZ‚Ä≤
mœÉ2
m + ZaAZ‚Ä≤
aœÉ2
a + ZaAZ‚Ä≤
mœÉa,m + ZmAZ‚Ä≤
aœÉa,m+IœÉ2
e.
The scalar version of this expression is as follows. Let ya and yb represent
records of individuals a and b, respectively. Let the mothers of a and b be
c and d, respectively. Then, if Aij denotes the additive genetic relationship
between i and j:
Cov (ya, yb) = AabœÉ2
a + AcdœÉ2
m + (Aad + Abc) œÉa,m
with the extra term œÉ2
e when a = b.
Return now to the Bayesian implementation via Gibbs sampling. As in
(13.1) it is assumed that, conditionally on all location eÔ¨Äects Œ≤, m, a, and
on the residual variance œÉ2
e, the data are a realization from the normal
process
y|Œ≤, m, a, œÉ2
e ‚àºN

XŒ≤ + Zmm + Zaa, IœÉ2
e

.
(13.19)
The joint prior distribution of m and a, under the assumptions of the
inÔ¨Ånitesimal model, is
 m
a
"""" A, G0 ‚àºN


0
0

, G0 ‚äóA

.
(13.20)
On deÔ¨Åning g = [m‚Ä≤, a‚Ä≤]‚Ä≤, one can write the corresponding density as
p (g|A, G0) = |2œÄG|‚àí1
2 exp

‚àí1
2g‚Ä≤ 
G‚àí1
0
‚äóA‚àí1
g

‚àù|G0|‚àíq
2 exp

‚àí1
2g‚Ä≤ 
G‚àí1
0
‚äóA‚àí1
g

.
(13.21)

574
13. Gaussian and Thick-Tailed Linear Models
This expression is now written in the form of Example 1.17 in Chapter 1,
since this makes it easier to derive the fully conditional posterior distribu-
tion of G0. DeÔ¨Åne
Sg =

m‚Ä≤A‚àí1m
m‚Ä≤A‚àí1a
a‚Ä≤A‚àí1m
a‚Ä≤A‚àí1a

.
Then
g‚Ä≤ 
G‚àí1
0
‚äóA‚àí1
g =

m‚Ä≤
a‚Ä≤  
gm,mA‚àí1
gm,aA‚àí1
gm,aA‚àí1
ga,aA‚àí1
 
m
a

= tr

G‚àí1
0 Sg

.
Therefore (13.21) can be expressed as
p (g|A, G0) ‚àù|G0|‚àíq
2 exp

‚àí1
2 tr

G‚àí1
0 Sg

.
(13.22)
The vector Œ≤ is assigned again the uniform prior distribution, with den-
sity
p (Œ≤) ‚àùconstant.
The residual variance is assumed to follow, a priori, a scaled inverted chi-
square distribution with density
p

œÉ2
e|ŒΩe, S2
e

‚àù

œÉ2
e
‚àí(
ŒΩe
2 +1) exp

‚àíŒΩeS2
e
2œÉ2e

,
(13.23)
where ŒΩe and S2
e are hyperparameters, assumed known. Finally, the covari-
ance matrix G0 is assumed to follow, a priori, a scaled, two-dimensional,
inverse Wishart distribution having density function
p (G0|V, œÖ) ‚àù|G0|‚àí1
2 (œÖ+k+1) exp

‚àí1
2tr

G‚àí1
0 V‚àí1
,
(13.24)
where k, the dimension of G0, is equal to 2 here. The density (13.24) is
symbolized as IW (V, œÖ) , and it reduces to a two-dimensional improper
uniform distribution by setting œÖ = ‚àí(k + 1) and V = 0.
The joint posterior density of all parameters is given by
p

Œ≤, m, a, G0, œÉ2
e|y

‚àùp

y|Œ≤, m, a, œÉ2
e

p

Œ≤, m, a, G0, œÉ2
e

‚àùp

y|Œ≤, m, a, œÉ2
e

p (m, a|G0) p (G0|V, œÖ) p

œÉ2
e|ŒΩe, S2
e

,
(13.25)
with the second line of (13.25) arising in view of the prior assumed for
Œ≤, and because œÉ2
e and (m, a, G0) are assumed to be independent, a pri-
ori. This joint density (distribution) is the basis for deriving all the fully
conditional posterior densities (distributions) needed for implementing the
Gibbs sampler.

13.3 Additive Genetic Model with Maternal EÔ¨Äects
575
13.3.1
Fully Conditional Posterior Distributions
As in the previous section, in order to derive the fully conditional posterior
distributions of Œ≤, m, and a, let
XŒ≤ + Zmm + Zaa = WŒ∏,
(13.26)
where
W =

X
Zm
Za

,
and put
C = W‚Ä≤W + Œ£,
with
Œ£ =
Ô£Æ
Ô£∞
0
0
0
0
A‚àí1km,m
A‚àí1km,a
0
A‚àí1km,a
A‚àí1ka,a
Ô£π
Ô£ª.
(13.27)
In (13.27), kmm = œÉ2
egm,m, km,a = œÉ2
egm,a, and ka,a = œÉ2
ega,a. Then, as in
(13.11), the fully conditional posterior distribution of Œ∏i is
Œ∏i|Œ∏‚àíi, G0, œÉ2
e, y ‚àºN

5Œ∏i, C‚àí1
i,i œÉ2
e

(13.28)
where 5Œ∏i satisÔ¨Åes
Ci,i5Œ∏i = ri ‚àíCi,‚àíiŒ∏‚àíi.
(13.29)
To derive the fully conditional posterior distribution of the residual vari-
ance note, from (13.25), that the only terms that include œÉ2
e are the Ô¨Årst
and the last ones in the joint density. Therefore,
p

œÉ2
e|Œ≤, m, a, G0, y

‚àùp

y|Œ≤, m, a, œÉ2
e

p

œÉ2
e

‚àù

œÉ2
e
‚àín
2 exp

‚àí(y ‚àíWŒ∏)‚Ä≤ (y ‚àíWŒ∏)
2œÉ2e
 
œÉ2
e
‚àí(
ŒΩe
2 +1) exp

‚àíŒΩeS2
e
2œÉ2e

‚àù

œÉ2
e
‚àí(
ŒΩe
2 +1) exp

‚àí,ŒΩe ,S2
e
2œÉ2e

,
(13.30)
where ,ŒΩe = ŒΩe + n and
,S2
e = (y ‚àíWŒ∏)‚Ä≤ (y ‚àíWŒ∏) + ŒΩeS2
e
,ŒΩe
.
Expression (13.30) is proportional to a scaled inverted chi-square density,
with ,ŒΩe degrees of freedom and scale parameter ,S2
e. Thus
œÉ2
e|Œ≤, m, a, G0, y ‚àº,ŒΩe ,S2
eœá‚àí2
ŒΩe .
(13.31)

576
13. Gaussian and Thick-Tailed Linear Models
Finally, the fully conditional posterior distribution of G0 is obtained by
retaining those terms in (13.25) that involve G0:
p

G0|Œ≤, m, a, œÉ2
e, y

‚àùp (m, a|G0) p (G0|V, œÖ)
‚àù|G0|‚àíq
2 exp

‚àí1
2tr

G‚àí1
0 Sg

|G0|‚àí1
2 (œÖ+k+1) exp

‚àí1
2tr

G‚àí1
0 V‚àí1
.
Collecting terms yields
p

G0|Œ≤, m, a, œÉ2
e, y

‚àù|G0|‚àí1
2 (œÖ+q+k+1) exp

‚àí1
2tr

G‚àí1
0

Sg + V‚àí1%
(13.32)
which can be recognized as the kernel of a 2 √ó 2 scaled inverted Wishart
distribution (k = 2), with degrees of freedom equal to v+q and scale matrix
Sg + V‚àí1. Hence, we write
G0|Œ≤, m, a, œÉ2
e, y ‚àºIW2

Sg + V‚àí1‚àí1 , v + q

.
(13.33)
The Gibbs sampler proceeds by sampling from distribution (13.28), either
elementwise or blockwise, and from (13.31) and (13.33).
13.4
The Multivariate Linear Additive
Genetic Model
This section describes a Gibbs sampler for making Bayesian inferences
based on a multiple-trait mixed linear model. This is a model that applies
to a situation where several response variables are measured simultane-
ously on an individual. For example, Smith (1936) considered the problem
of selecting among varieties of wheat diÔ¨Äering in yield and quality traits;
Hazel (1943) applied some of the ideas to pig breeding schemes, where body
weights and scores had been collected in each of the animals. The ‚Äúselec-
tion index‚Äù procedures suggested by these authors depend on knowledge of
genetic and environmental correlations between traits, and these must be
inferred using some multivariate model.
The developments presented here are circumscribed to a two-trait case,
but extension to more response variables is straightforward. An arbitrary
pattern of missing data will be assumed, and use is made of the ideas of data
augmentation to ‚ÄúÔ¨Åll-in‚Äù the missing observations. This simpliÔ¨Åes the Gibbs
sampler, because the fully conditional posterior distributions of residual
covariance matrices follow standard inverse Wishart distributions, at least
for certain forms of the prior distribution. An important assumption is
that the missing data are missing at random, in the sense deÔ¨Åned by Rubin
(1976). This means that the pattern of missing data may depend on the
observed data, but not on the missing data. In this case, the missing data

13.4 The Multivariate Linear Additive Genetic Model
577
mechanism can be ignored in the formulation of the probabilistic model,
simplifying matters considerably. If, on the other hand, the data are not
missing at random, it is necessary to model the missing data mechanism,
which requires making extra assumptions, and additional parameters enter
into the model. See Rubin (1987a), Little and Rubin (1987), Gelman et al.
(1995), and Schafer (2000) for a comprehensive discussion of the issues.
Consider two traits denoted by Y and Z, and let yo and zo denote the
vectors of observed data for traits Y and Z, respectively. The individuals are
hypothetically from a polytocous (litter bearing) species, so that there may
be eÔ¨Äects common to individuals raised in the same litter. Ideally, each of
the individuals would have measurements for both traits, but this is seldom
the case, at least with animal breeding data, where some individuals will
have records for both Y and Z, whereas others will have measurements for
either Y or Z only. Suppose that the number of individuals having at least
one record is n, that is, n = nY Z + nY + nZ, where nY Z is the number
of individuals having records for each of the two traits, and nY (nZ) is
the number of subjects with records only for trait Y (Z). If there were
no missing records, the vector of ‚Äúcomplete‚Äù data would have dimension
2n√ó1. Let ym and zm be the vectors of missing data, and let the complete
data vectors then be y‚Ä≤ = [y‚Ä≤
o, y‚Ä≤
m] and z‚Ä≤ = [z‚Ä≤
o, z‚Ä≤
m]. The following model
will be assumed

y
z

=

Xy
0
0
Xz
  Œ≤y
Œ≤z

+

Wy
0
0
Wz
 
ly
lz

+

Zy
0
0
Zz
 
ay
az

+

ey
ez

,
(13.34)
where Œ≤y (Œ≤z) is a vector of ‚ÄúÔ¨Åxed eÔ¨Äects‚Äù aÔ¨Äecting trait Y (Z), ly (lz) is
a vector of litter eÔ¨Äects of order s (s), ay (az) is the vector of order q (q)
of additive genetic values for trait Y (Z) and ey (ez) is a vector of residual
eÔ¨Äects of order n (n) for trait Y (Z). As stated, the litter eÔ¨Äect parameters
account for the common environment aÔ¨Äecting individuals that are raised
together (often, but not always, contemporaneous full-sibs). Alternatively,
they could represent, for example, ‚Äúpen‚Äù or ‚Äúcage‚Äù eÔ¨Äects in laboratory
animals; if such eÔ¨Äects do not exist, they can be removed from the model.
Matrices X, W, and Z, with subscripts y and z, are known incidence arrays
relating location eÔ¨Äects for each trait to the data. This is a model that could
be used, for example, to analyze data on daily gain and feed intake in pigs.
Put now Œ≤ =

Œ≤‚Ä≤
y, Œ≤‚Ä≤
z
‚Ä≤ , l =

l‚Ä≤
y, l‚Ä≤
z
‚Ä≤ , a =

a‚Ä≤
y, a‚Ä≤
z
‚Ä≤, with appropriate
partitions for matrices X, W and Z, such that
X =

Xy
0
0
Xz

, W =

Wy
0
0
Wz

, Z =

Zy
0
0
Zz

.
The conditional distribution of the complete data for each individual, given
the parameters, is assumed to be bivariate normal. For all individuals the

578
13. Gaussian and Thick-Tailed Linear Models
distribution can be written as
v|Œ≤, l, a, Re ‚àºN (XŒ≤ + Wl + Za, R) ,
(13.35)
where v contains y and z. We shall assume that records have been sorted
by individual and trait within individual, so that v is a sequence of (Y, Z)
values for each individual. Pairs of records from diÔ¨Äerent individuals are
assumed to be conditionally independent, given the parameters, but a cor-
relation between residuals of the same individual is allowed. Hence, the
sorting is such that the residual variance‚Äìcovariance matrix can be written
as R = In‚äóRe, a block diagonal matrix with n submatrices of residual co-
variances Re (of order 2 √ó 2) and, as usual, In is the n √ó n identity matrix.
The residual dispersion matrix is
Re =
 œÉ2
e,y
œÉe,yz
œÉe,yz
œÉ2
e,z

,
where œÉ2
e,y is the residual variance for trait Y, œÉ2
e,z is the residual vari-
ance for trait Z, and œÉe,yz is the residual covariance. The residual correla-
tion œÉe,yz/ (œÉe,yœÉe,z) is a measure of the association between traits due to
sources other than genetic and litter (or pen) eÔ¨Äects.
Prior distributions are speciÔ¨Åed now, starting with the location eÔ¨Äects.
For the vector Œ≤, a proper uniform distribution is assigned, with density
p (Œ≤) ‚àùconstant,
(13.36)
with possible boundaries, Œ≤min, Œ≤max, to ensure propriety of the joint pos-
terior distribution. The vector of litter eÔ¨Äects is assumed to have a prior
uncertainty distribution well-reÔ¨Çected by the multivariate normal process
l|Rl ‚àºN (0, Rl ‚äóIs) ,
(13.37)
where Is is the s √ó s identity matrix, and Rl is the covariance matrix
Rl =
 œÉ2
l,y
œÉl,yz
œÉl,yz
œÉ2
l,z

.
Above, œÉ2
l,y

œÉ2
l,z

is the variance between litter eÔ¨Äects for trait Y (Z), and
œÉl,yz is a covariance component. The vector of additive genetic values is
assumed to follow, a priori, the multivariate normal distribution
a|G0, A ‚àºN (0, G0 ‚äóA) ,
(13.38)
where A is the additive genetic relationship matrix of order q √ó q, and
G0 =
 œÉ2
a,y
œÉa,yz
œÉa,yz
œÉ2
a,z


13.4 The Multivariate Linear Additive Genetic Model
579
is a 2√ó2 matrix, whose elements are the additive genetic (co)variance com-
ponents. The genetic correlation between traits, œÉa,yz/ (œÉa,yœÉa,z) measures
the strength of the linear association between additive eÔ¨Äects for traits Y
and Z.
Two-dimensional scaled inverted Wishart distributions are assigned as
prior processes for each of the Re, Rl, and G0 covariance matrices, with
the respective densities being
p (Re|œÖe, Ve) ‚àù|Re|‚àí1
2 (œÖe+k+1) exp

‚àí1
2tr

R‚àí1
e V‚àí1
e

,
(13.39)
p (Rl|œÖl, Vl) ‚àù|Rl|‚àí1
2 (œÖl+k+1) exp

‚àí1
2tr

R‚àí1
l
V‚àí1
l

,
(13.40)
and
p (G0|œÖa, Va) ‚àù|G0|‚àí1
2 (œÖa+k+1) exp

‚àí1
2tr

G‚àí1
0 V‚àí1
a

,
(13.41)
where k = 2 in our hypothetical bivariate model. In these expressions,
œÖi and Vi (i = e, l, a) are hyperparameters of the distributions, which are
assumed known. As mentioned in connection with (13.24), these inverse
Wishart distributions reduce to improper uniform distributions, if œÖi =
‚àí(k + 1) and Vi = 0.
The joint posterior density of all parameters (after augmentation with
the missing records), allowing for dependence of the distribution of the
litter and additive eÔ¨Äects on the corresponding covariance matrices, but
assuming prior independence otherwise, is given by
p (ym, zm, Œ≤, l, a, Re, Rl, G0|yo, zo)
‚àùp (ym, zm, Œ≤, l, a, Re, Rl, G0) p (yo, zo|ym, zm, Œ≤, l, a, Re)
= p (ym, yo, zm, zo|Œ≤, l, a, Re) p (l|Rl) p (Rl) p (a|G0) p (G0) p (Re) .
(13.42)
This density is deÔ¨Åned within the bounds speciÔ¨Åed in connection with the
prior (13.36). Note that the Ô¨Årst term in (13.42) is the conditional density
of the complete data, deÔ¨Åned in (13.35). The fully conditional posterior
distributions are derived from (13.42) by proceeding in the usual manner,
that is, Ô¨Åxing the appropriate conditioning variables in the joint density.
Here scaled inverse Wishart distributions were chosen as prior speciÔ¨Åca-
tions for the covariance matrices. This makes implementation of the Gibbs
sampler straightforward. Clearly, other prior speciÔ¨Åcations could be chosen,
such as assigning independent prior distributions to the 3 elements of the
covariance matrices, and using a parameterization in terms of correlations.

580
13. Gaussian and Thick-Tailed Linear Models
13.4.1
Fully Conditional Posterior Distributions
Imputation of Missing Records
Since the missing records are treated as unknowns in the probability model
having density (13.42), their values must be imputed via Gibbs sampling
by eÔ¨Äecting draws from their fully conditional posterior distributions. First,
note from the joint posterior density that
p (zm|ym, Œ≤, l, a, Re, Rl, G0, yo, zo) ‚àùp (ym, yo, zm, zo|Œ≤, l, a, Re)
‚àùp (zm|Œ≤, l, a, Re, yo, zo)
‚àùp (zm|Œ≤, l, a, Re, yo) .
(13.43)
The preceding follows because:
(1) Missingness is at random, so the distribution of missing records depends
on the observed data, and not on the missing observations.
(2) Given Œ≤, l, a, Re, the missing observations for trait Z do not depend
on the observed records for this trait, as these have been measured on
other individuals, and observations from diÔ¨Äerent subjects are conditionally
independent.
Furthermore, because of such conditional independence,
p (zm|Œ≤, l, a, Re, yo) =
-
i‚ààMZ
p (zm,i|Œ≤, l, a, Re, yo,i) ,
(13.44)
where zm,i is the missing record, and yo,i is the observed record for subject
i in the set MZ of individuals with missing data for trait Z, comprising nY
members. Similarly
p (ym|zm, Œ≤, l, a, Re, Rl, G0, yo, zo) ‚àù
-
i‚ààMY
p (ym,i|Œ≤, l, a, Re, zo,i) ,
(13.45)
where now ym,i is the missing record and zo,i is the observed record for sub-
ject i in the set MY of individuals with missing data for trait Y, comprising
nZ subjects.
Consider an individual i with a record for trait Y and no record for Z.
Using (13.35), and the results from multivariate normal theory, the fully
conditional posterior distribution of zm,i is
zm,i|yo,i, Œ≤, l, a, Re ‚àºN

5zm,i, Vzm,i

,
(13.46)
where
5zm,i = E (zm,i|yo,i, Œ≤, l, a, Re)
= x‚Ä≤
mz,iŒ≤z + w‚Ä≤
mz,ilz+z‚Ä≤
mz,iaz + œÉe,yz
œÉ2e,y

yo,i ‚àíx‚Ä≤
oy,iŒ≤y ‚àíw‚Ä≤
oy,ily ‚àíz‚Ä≤
oy,iay

(13.47)

13.4 The Multivariate Linear Additive Genetic Model
581
and
Vzm,i = œÉ2
e,z

1 ‚àí
œÉ2
e,yz
œÉ2e,yœÉ2e,z

.
(13.48)
In these expressions, x‚Ä≤
mz,i is the row of Xz associating Œ≤z to zm,i, w‚Ä≤
mz,i is
the row of Wz associating lz to zm,i, and z‚Ä≤
mz,i is the row of Zz associating
az to zm,i. If, instead, individual i has an observation for trait Z and lacks a
record for Y , the fully conditional posterior distribution for ym,i is derived
in a similar manner. Thus, the draws for the missing records for Z can be
done from (13.46), or from its counterpart when Y is the missing trait.
In this computing strategy, generation of the missing data requires knowl-
edge of elements of the incidence matrices, that is, of the way that location
eÔ¨Äects enter into the missing record. Often, for some missing records, this
information may not be available. An alternative strategy, which is com-
putationally simpler, and which avoids knowledge of incidence matrices
altogether, is to generate ‚Äúmissing residuals‚Äù instead of missing observa-
tions, as suggested by Van Tassell and Van Vleck (1996) and Wang et al.
(1997). In this setting, the joint posterior distribution is augmented with
the ‚Äúmissing residuals‚Äù in lieu of the missing records. The missing residuals
are sampled from a normal distribution with mean
5emz,i = œÉe,yz
œÉ2e,y

yo ‚àíx‚Ä≤
oy,iŒ≤y ‚àíw‚Ä≤
oy,ily ‚àíz‚Ä≤
oy,iay

.
The variance of the fully conditional posterior distribution of the missing
residual is as in (13.48). Similar expressions apply to the ‚Äúmissing residuals‚Äù
for trait Y.
Location EÔ¨Äects
The derivation of the fully conditional posterior distributions of the location
parameters Œ≤, l and a is similar to the one leading to (13.9), with a slight
modiÔ¨Åcation needed to accommodate the multiple-trait case. Let
Œ∏ =

Œ≤‚Ä≤, l‚Ä≤, a‚Ä≤‚Ä≤ ,
M =

Xy
0y
Wy
0y
Zy
0y
0z
Xz
0z
Wz
0z
Zz

,
R = Re ‚äóIn,
noting that this implies the sorting of individuals within trait,
‚Ñ¶=
Ô£Æ
Ô£∞
0
0
0
0
R‚àí1
l
‚äóIs
0
0
0
G‚àí1
0
‚äóA‚àí1
Ô£π
Ô£ª,
and
C = M‚Ä≤R‚àí1M + ‚Ñ¶.

582
13. Gaussian and Thick-Tailed Linear Models
The multiple-trait mixed model equations are
C5Œ∏ = t
(13.49)
where the right-hand side vector t is equal to
t
=
M‚Ä≤R‚àí1v
=
M‚Ä≤R‚àí1

y
z

.
Then the fully conditional posterior distribution of Œ∏ is
Œ∏|Re, Rl, G0, y, z ‚àºN

5Œ∏, C‚àí1
,
(13.50)
and the multiple-trait version of (13.11) is, for any sub-vector Œ∏i of Œ∏,
Œ∏i|Œ∏‚àíi, Re, Rl, G0, y, z ‚àºN

,Œ∏i, C‚àí1
i,i

,
(13.51)
where ,Œ∏i satisÔ¨Åes
Ci,i,Œ∏i = ti ‚àíCi,‚àíiŒ∏‚àíi.
(13.52)
As before, when the Gibbs sampler is implemented in a scalar mode (draw-
ing from the fully conditional posterior distributions one element of Œ∏ at a
time), Œ∏i and Ci,i above are scalars and Ci,‚àíi is a row vector. On the other
hand, when the entire location vector Œ∏ is sampled (as discussed later on),
the oÔ¨Äset Ci,‚àíiŒ∏‚àíi is null, and (13.51) reproduces (13.50).
Dispersion Matrices
The fully conditional posterior distributions of the dispersion matrices are
derived next. From (13.42):
p (Re|Œ≤, l, a, Rl, G0, y, z) ‚àùp (y, z|Œ≤, l, a, Re) p (Re) .
(13.53)
DeÔ¨Åne
Se =
 e‚Ä≤
yey
e‚Ä≤
yez
e‚Ä≤
yez
e‚Ä≤
zez

,
where
ey = y ‚àíXyŒ≤y ‚àíWyly ‚àíZyay,
and
ez = z ‚àíXzŒ≤z ‚àíWzlz ‚àíZzaz.
The density of the conditional distribution of the complete data, given Œ≤,
l, a, Re, can be expressed as
p (y, z|Œ≤, l, a, Re) ‚àù|R|‚àí1
2 exp

‚àí1
2tr

R‚àí1
e Se

= |Re|‚àín
2 exp

‚àí1
2tr

R‚àí1
e Se

.

13.4 The Multivariate Linear Additive Genetic Model
583
Combining this with prior (13.39) yields
p (Re|Œ≤, l, a, Rl, G0, y, z) ‚àù|Re|‚àí1
2 (œÖe+n+k+1)
√ó exp

‚àí1
2 tr

R‚àí1
e

Se + V‚àí1
e
%
.
This is the kernel of a scaled inverted Wishart distribution of order k = 2
(the number of traits in this case), with (œÖe + n) degrees of freedom and
scale matrix

Se + V‚àí1
e
‚àí1. Hence, the Gibbs sampler obtains updates for
the residual covariance matrix from
Re|Œ≤, l, a, Rl, G0, y, z ‚àºIW2

Se + V‚àí1
e
‚àí1 , œÖe + n

.
(13.54)
Similarly for Rl, from the joint posterior (13.42) one can deduce that
p (Rl|Œ≤, l, a, Re, G0, y, z) ‚àùp (l|Rl) p (Rl) .
DeÔ¨Åne
Sl =
 l‚Ä≤
yly
l‚Ä≤
ylz
l‚Ä≤
ylz
l‚Ä≤
zlz

and express the density p (l|Rl) as
p (l|Rl) ‚àù|R|‚àís
2 exp

‚àí1
2 tr

R‚àí1
l
Sl

.
Combining this with prior (13.40) yields
p (Rl|Œ≤, l, a, Re, G0, y, z) ‚àù|Rl|‚àí1
2 (œÖl+s+k+1)
√ó exp

‚àí1
2 tr

R‚àí1
l

Sl + V‚àí1
l
%
.
Therefore
Rl|Œ≤, l, a, Re, G0, y, z ‚àºIW2

Sl + V‚àí1
l
‚àí1 , œÖl + s

,
so the samples are also obtained from an inverted Wishart distribution.
Finally, for the genetic covariance matrix, from (13.42) one obtains
p (G0|Œ≤, l, a, Re, Rl, y, z) ‚àùp (a|G0) p (G0) .
(13.55)
The term p (a|G0) is
p (a|G0) ‚àù|G0|‚àíq
2 exp

‚àí1
2

ay
az
‚Ä≤ [G0 ‚äóA]‚àí1
 ay
az
%
.
On deÔ¨Åning
Sa =
 a‚Ä≤
yA‚àí1ay
a‚Ä≤
yA‚àí1az
a‚Ä≤
zA‚àí1ay
a‚Ä≤
zA‚àí1az

,

584
13. Gaussian and Thick-Tailed Linear Models
the density p (a|G0) can be expressed as
p (a|G0) ‚àù|G0|‚àíq
2 exp

‚àí1
2 tr

G‚àí1
0 Sa

.
Therefore, combining this with prior (13.41) yields
p (G0|Œ≤, l, a, Re, Rl, y, z) ‚àù|G0|‚àí1
2 (œÖa+q+k+1)
√ó exp

‚àí1
2tr

G‚àí1
0

V‚àí1
a
+ Sa
%
,
which is recognized as the kernel of the scaled inverse Wishart distribution
G0|Œ≤, l, a, Re, Rl, y, z ‚àºIW2

V‚àí1
a
+ Sa
‚àí1 , œÖa + q

.
(13.56)
This process completes the speciÔ¨Åcation of all conditional posterior distri-
butions needed for running a Gibbs sampler. The order of visitation of the
distributions is dictated primarily by computational convenience. In prin-
ciple, one can follow an order similar to that described for the univariate
additive genetic model in Section 13.2, keeping in mind that imputations
for the missing records must be eÔ¨Äected before draws are made for the
location eÔ¨Äects.
13.5
A Blocked Gibbs Sampler for
Gaussian Linear Models
In the Gibbs sampler, as stated earlier, the location eÔ¨Äects can be drawn
either element-by-element or in blockwise manners. In the elementwise or
scalar version of the sampler, each location parameter is drawn successively.
A consequence of scalar sampling is that convergence may be very slow,
especially in models where the parameters in the posterior distribution are
highly intercorrelated (Hills and Smith, 1992; Roberts and Sahu, 1997).
Also, use of data augmentation in situations where there is a large fraction
of missing observations (in the broad sense of including random eÔ¨Äects or
unobserved latent variables, such as in threshold models ), can also slow
down convergence, for reasons similar to those hampering the expectation-
maximization algorithm (Dempster et al., 1977; Liu et al., 1994).
Liu (1994), Liu et al. (1994), and Roberts and Sahu (1997) compared
rates of convergence of various blocking strategies in Gibbs sampling. Liu
(1994) and Liu et al. (1994) considered a three-dimensional posterior dis-
tribution with parameters a, b, c, say, and contrasted elementwise sampling
with a blockwise sampling algorithm, called a group sampler. In the lat-
ter, two parameters (a, b) were blocked, so that the draws were made from
[a, b|c, data] and from [c|a, b, data]. In our context, a and b could correspond

13.5 A Blocked Gibbs Sampler for Gaussian Linear Models
585
to Ô¨Åxed and random eÔ¨Äects, respectively, whereas c could correspond to
dispersion parameters. A collapsed sampler was considered as well, where
draws were made from [a, b|data] (so integration of parameter c is required
here) and from [c|a, b, data] . Liu (1994) found that the collapsed sampler
works better than the blocked sampler, with the latter performing bet-
ter than the elementwise implementation. The collapsed sampler has the
potential shortcoming that integration may not always be possible in non-
stylized models; for further discussion, see Chen et al. (2000). In the study
of Roberts and Sahu (1997), where the joint posterior distribution was mul-
tivariate normal, it was found that when all partial correlations between
parameters were positive, the blocked Gibbs sampler had a faster rate of
convergence than the piecewise algorithm. This suggests that grouping pos-
itively correlated parameters can be advantageous (e.g., members of a fam-
ily in a genetic context), although there are instances in which blocking
can make things worse.
Hence, the eÔ¨Äectiveness of a sampler may be enhanced by drawing pa-
rameters in blocks. For example, in the case of the linear additive genetic
model discussed in Section 13.2, rather than drawing each element of the
vector Œ∏ at a time, one may draw the whole vector Œ∏ in one pass. Clearly,
if the dispersion parameters were known, this is identical to sampling di-
rectly from the target posterior distribution of the location parameter with-
out creating a Markov chain. However, since the (co)variance parameters
are typically unknown, this would be equivalent to the group sampler de-
scribed above, with c being the location eÔ¨Äects, say, and a and b playing
the role of the variance components, as stated. Here a strategy presented
by Garc¬¥ƒ±a-Cort¬¥es and Sorensen (1996) is described, which makes feasible
sampling the entire Œ∏ directly for some large Gaussian linear models. For
simplicity, the model is restricted to a univariate (single-trait) setting with
a sole random eÔ¨Äect other than the residual. Extension to models with
several variance components and to multivariate (multiple-trait) settings is
relatively straightforward.
Consider the same model assumptions as in Section 13.2, and adopt
proper prior distributions for a (13.2), for Œ≤ ((13.3), with upper and lower
bounds), and for the two variance components œÉ2
i (i = a, e) as in (13.4).
Then the fully conditional posterior distributions, needed for implementing
the blocked or grouped Gibbs sampler, are
Œ∏|œÉ2
a, œÉ2
e, y ‚àºN

5Œ∏, C‚àí1œÉ2
e

,
(13.57)
and
œÉ2
i |Œ∏, y ‚àº,ŒΩi ,Siœá‚àí2
ŒΩi ,
i = a, e,
(13.58)

586
13. Gaussian and Thick-Tailed Linear Models
where ,ŒΩi and ,Si are deÔ¨Åned in connection with (13.14) and (13.16). Recall
that the coeÔ¨Écient matrix of the mixed model equation is
C =
 X‚Ä≤X
X‚Ä≤Z
Z‚Ä≤X
Z‚Ä≤Z + A‚àí1k

,
where k is the ratio between the residual and the genetic components of
variance. The vector of the right-hand sides is W‚Ä≤y, where W =

X
Z

.
It is computationally diÔ¨Écult to draw Œ∏ from (13.57) in a single pass when
p+q is very large; the usual calculations for extracting a multivariate normal
vector involve performing the Cholesky decomposition of the covariance
matrix, etc., and these are involved and must be repeated iteration after
iteration. The Garc¬¥ƒ±a-Cort¬¥es and Sorensen procedure, instead, makes use
of the fact that 5Œ∏ = C‚àí1W‚Ä≤y can be calculated rapidly using iterative
methods for solving linear systems of equations.
DeÔ¨Åne the following random vector of order (p + q) √ó 1:
Œ∏‚àó= 5Œ∏ +

¬µ
a

‚àíC‚àí1W‚Ä≤z,
(13.59)
where:
‚Ä¢ The vector 5Œ∏ is the mean of the conditional posterior distribution

Œ∏|œÉ2
a, œÉ2
e, y

.
‚Ä¢ ¬µ is a p √ó 1 vector of ‚Äúpseudo-Ô¨Åxed‚Äù eÔ¨Äects (we will see later that
the value of ¬µ is immaterial, so each of its elements can be set con-
veniently equal to 0).
‚Ä¢ As before, a|A, œÉ2
a ‚àºN

0, A œÉ2
a

is a vector of random genetic eÔ¨Äects
and A is the usual, non-stochastic, additive relationship matrix.
‚Ä¢ The random vector z is a vector of pseudo-observations generated
according to the process

z|¬µ, a, œÉ2
e

‚àºN

X¬µ + Za, IœÉ2
e

.
(13.60)
Since the additive genetic eÔ¨Äects are normally distributed, and the con-
ditional distribution (13.60) is normal, it follows that the process
[a, z|¬µ, A, œÉ2
a, œÉ2
e]
is jointly normal, with parameters

a
z
"""" ¬µ, œÉ2
a, œÉ2
e ‚àºN


0
X¬µ

,

AœÉ2
a
AZ‚Ä≤œÉ2
a
ZAœÉ2
a
ZAZ‚Ä≤œÉ2
a + IœÉ2
e

.
(13.61)

13.5 A Blocked Gibbs Sampler for Gaussian Linear Models
587
First, write Œ∏‚àóin (13.59) as
Œ∏‚àó= 5Œ∏ + C‚àí1

C

¬µ
a

‚àíW‚Ä≤z
%
= 5Œ∏ + C‚àí1

X‚Ä≤X¬µ + X‚Ä≤Za ‚àíX‚Ä≤ (X¬µ + Za + e)
Z‚Ä≤X¬µ +

Z‚Ä≤Z + A‚àí1k

a ‚àíZ‚Ä≤ (X¬µ + Za + e)

= 5Œ∏ + C‚àí1

‚àíX‚Ä≤e
A‚àí1ka ‚àíZ‚Ä≤e

,
(13.62)
and observe that this is a linear combination of normal vectors, so its
distribution (given the observed data) must be normal as well. Second, note
that this vector does not depend at all on ¬µ so there is no loss of generality
in assuming that ¬µ= 0. Taking the expectation of Œ∏‚àóover [a, z|¬µ, A, œÉ2
a, œÉ2
e]
one gets at once that
E (Œ∏‚àó) = 5Œ∏,
so the mean of the distribution of Œ∏‚àóis identical to the mean of the condi-
tional posterior distribution of Œ∏. Third, taking variances and covariances
of representation (13.62) over

a, z|¬µ, A, œÉ2
a, œÉ2
e

yields, since both 5Œ∏ and
C‚àí1 are Ô¨Åxed,
V ar (Œ∏‚àó) = C‚àí1V ar

‚àíX‚Ä≤e
A‚àí1ka ‚àíZ‚Ä≤e

C‚àí1
= C‚àí1
 X‚Ä≤X
X‚Ä≤Z
Z‚Ä≤X
Z‚Ä≤Z + A‚àí1k

C‚àí1œÉ2
e = C‚àí1œÉ2
e,
which is identical to the variance‚Äìcovariance matrix of the target condi-
tional posterior distribution. Hence, the distribution of Œ∏‚àóis precisely that
of the posterior process (13.57). The three results together imply that sam-
ples from the conditional posterior distribution of the location eÔ¨Äects can
be obtained by generating Œ∏‚àódraws using (13.59). It is convenient to rear-
range Œ∏‚àó(after setting ¬µ = 0, in view of considerations above) as
Œ∏‚àó=

0
a

+ 5Œ∏ ‚àíC‚àí1W‚Ä≤z
=

0
a

+ C‚àí1W‚Ä≤ (y ‚àíz)
=
 0
a

+
 X‚Ä≤X
X‚Ä≤Z
Z‚Ä≤X
Z‚Ä≤Z + A‚àí1Œ±
‚àí1 
X‚Ä≤ (y ‚àíz)
Z‚Ä≤ (y ‚àíz)

.
(13.63)
To summarize, the algorithm for carrying out the fully blocked imple-
mentation of the Gibbs sampler is
1. Provide starting values for œÉ2
a and œÉ2
e.

588
13. Gaussian and Thick-Tailed Linear Models
2. Generate a‚àófrom N

0, AœÉ2
a

.
3. Generate z‚àófrom N

Za‚àó, IœÉ2
e

.
4. Calculate y ‚àíz‚àó.
5. Compute
Œ∏ =

Œ≤
a

=

0
a‚àó

+
 X‚Ä≤X
X‚Ä≤Z
Z‚Ä≤X
Z‚Ä≤Z + G‚àí1
‚àí1 
X‚Ä≤ (y ‚àíz‚àó)
Z‚Ä≤ (y ‚àíz‚àó)

,
where

Œ≤‚Ä≤, a‚Ä≤‚Ä≤ is a draw from

Œ∏|œÉ2
a, œÉ2
e, y

.
6. Compute ,Si, (i = a, e).
7. Sample variance components from (13.58), and update the coeÔ¨Écient
matrix of the mixed model equations.
8. Return to step 2 and continue with this loop, until the end of the
chain.
Many diÔ¨Äerent iterative algorithms that do not require inversion of C
are available for solving the linear system
 X‚Ä≤X
X‚Ä≤Z
Z‚Ä≤X
Z‚Ä≤Z + A‚àí1k

s =

X‚Ä≤ (y ‚àíz‚àó)
Z‚Ä≤ (y ‚àíz‚àó)

.
The choice of method to apply depends on the dimension of C and on
whether or not one wishes to pay attention to memory requirements or to
computing time.
Another useful approach for sampling parameters in blocks, which is not
restricted to Gaussian models, is based on the Langevin‚ÄìHastings algo-
rithm; this was brieÔ¨Çy described in Section 11.6 of Chapter 11. In principle,
the Langevin‚ÄìHastings algorithm allows joint updates of all the parameters
of the model; in the present Gaussian situation, one would update location
and dispersion parameters in a single pass.
13.6
Linear Models with Thick-Tailed
Distributions
13.6.1
Motivation
It is generally accepted that the normal distribution is sensitive to depar-
tures from the assumptions, because of its ‚Äúthin‚Äù tails. Outlier observations
can have a marked impact on inferences, so many alternative, ‚Äúrobust‚Äù,

13.6 Linear Models with Thick-Tailed Distributions
589
methods have been developed. For instance, see Hampel et al. (1986). Re-
views and applications of robust procedures for parametric linear models
are, for example, in Rogers and Tukey (1972), Zellner (1976), and Lange
and Sinsheimer (1993). One of the possibilities that have been suggested
consists of replacing the normal process with a thicker-tailed distribution,
such as the Student-t, either in its univariate or multivariate forms. The
use of mixed models with t distributions in quantitative genetics was pio-
neered by Strand¬¥en (1996) and Strand¬¥en and Gianola (1999), and some of
the ideas were extended by Rosa (1998) to a wider family of distributions.
In this section, we motivate the problem and, subsequently, present Gibbs
sampling implementations for some variants of the theme. The problem
was already encountered in Chapter 6 in the context of a linear regression
model with t distributed errors, and will be dealt with again in the chapter
on analysis of longitudinal trajectories.
Consider a linear regression model under the ‚Äúusual‚Äù assumptions, assign
a Ô¨Çat prior to the regression vector Œ≤, and assume that the residual variance

œÉ2
is known. Under these conditions, the joint posterior density of the
regression coeÔ¨Écients is
p

Œ≤|y,œÉ2
‚àùexp

‚àí1
2œÉ2 (y ‚àíXŒ≤)‚Ä≤ (y ‚àíXŒ≤)

,
and the modal vector (equal to the mean vector in this setting) is identical
to the maximum likelihood estimator
5Œ≤
=

X‚Ä≤X
œÉ2
‚àí1 X‚Ä≤y
œÉ2
=
 n

i=1
xix‚Ä≤
i
œÉ2
‚àí1  n

i=1
xiyi
œÉ2

,
(13.64)
where x‚Ä≤
i is the ith row of X. Although this point estimator does not depend
on œÉ2 (because the dispersion parameter cancels out), it is instructive to
note that, implicitly, each observation is weighted equally by the reciprocal
of the variance (or, equivalently, all observations receive a weight equal to
1).
Abandon the normality assumption for the residuals and suppose that
these are independently and identically distributed as t

0, œÉ2, ŒΩ

, where
œÉ2 is now the scale parameter and ŒΩ is the degrees of freedom parame-
ter, with both assumed known. Then it follows that the observations are
independently distributed as
yi|Œ≤, œÉ2, ŒΩ ‚àºt1

x‚Ä≤
iŒ≤, œÉ2, ŒΩ

,
(13.65)
deÔ¨Åning a univariate-t distribution. If a Ô¨Çat prior is adopted for the regres-
sion vector, the posterior density is then (see Chapter 1 for the form of the

590
13. Gaussian and Thick-Tailed Linear Models
t-density)
p

Œ≤|y, œÉ2, ŒΩ

‚àù
n
-
i=1

1 + (yi ‚àíx‚Ä≤
iŒ≤)2
œÉ2ŒΩ
‚àí1+ŒΩ
2
.
Here the modal vector is also identical to the ML estimator, because of
the Ô¨Çat prior. To Ô¨Ånd the mode, we proceed to take derivatives of the log-
posterior density (which is equal to the likelihood function, apart from an
additive constant K), yielding
‚àÇ
‚àÇŒ≤ log

p

Œ≤|y, œÉ2, ŒΩ

= ‚àí

1 + ŒΩ
2

n

i=1
‚àÇ
‚àÇŒ≤ log

1 + (yi ‚àíx‚Ä≤
iŒ≤)2
œÉ2ŒΩ

+ K
=

1 + ŒΩ
ŒΩ

n

i=1
xi (yi ‚àíx‚Ä≤
iŒ≤)
œÉ2 + (yi‚àíx‚Ä≤
iŒ≤)
2
ŒΩ
.
Setting this gradient to 0, it follows that the posterior mode must satisfy
the system
n

i=1
xix‚Ä≤
i
œÉ2 + (yi‚àíx‚Ä≤
iŒ≤)
2
ŒΩ
Œ≤ =
n

i=1
xiyi
œÉ2 + (yi‚àíx‚Ä≤
iŒ≤)
2
ŒΩ
,
(13.66)
which is not explicit in Œ≤. However, one can construct a functional iteration
by assigning a starting value to the regression coeÔ¨Écients, and updating
iterates (t denotes round number) as
Œ≤[t+1] =
Ô£Æ
Ô£∞
n

i=1
xix‚Ä≤
i
œÉ2 + (yi‚àíx‚Ä≤
iŒ≤[t])
2
ŒΩ
Ô£π
Ô£ª
‚àí1 Ô£Æ
Ô£∞
n

i=1
xiyi
œÉ2 + (yi‚àíx‚Ä≤
iŒ≤[t])
2
ŒΩ
Ô£π
Ô£ª.
(13.67)
This can be written in matrix form by putting
D‚àí1
Œ≤
= Diag
Ô£Æ
Ô£∞
1
œÉ2 + (yi‚àíx‚Ä≤
iŒ≤)
2
ŒΩ
Ô£π
Ô£ª,
and noting that
n

i=1
xix‚Ä≤
i
œÉ2 + (yi‚àíx‚Ä≤
iŒ≤)
2
ŒΩ
= X‚Ä≤D‚àí1
Œ≤ X
and
n

i=1
xiyi
œÉ2 + (yi‚àíx‚Ä≤
iŒ≤)
2
ŒΩ
= X‚Ä≤D‚àí1
Œ≤ y.
Hence (13.67) becomes
Œ≤[t+1] =

X‚Ä≤ 
D‚àí1
Œ≤
[t]
X
‚àí1
X‚Ä≤ 
D‚àí1
Œ≤
[t]
y
(13.68)

13.6 Linear Models with Thick-Tailed Distributions
591
which deÔ¨Ånes an iteratively reweighted least-squares algorithm, showing
a clear analogy between the linear regression models with normal and t-
distributed residuals. Note from (13.67) that the implicit weight received
by datum i is
1
œÉ2 + (yi‚àíx‚Ä≤
iŒ≤)
2
ŒΩ
.
(13.69)
If the error distribution approaches normality (ŒΩ ‚Üí‚àû), the weight is 1/œÉ2,
as in the standard regression model. If, on the other hand, the distribution
has increasingly thicker tails (ŒΩ ‚Üí0), the observation is downweighted fur-
ther and further. At a Ô¨Åxed value of the degrees of freedom parameter, the
weight is inversely proportional to (yi ‚àíx‚Ä≤
iŒ≤)2, that is, the more an obser-
vation deviates from its expected value (given Œ≤), the smaller the weight it
receives in the analysis, and the less it perturbs inference. This is the reason
why the t-distribution is considered as being more robust than the normal:
observations that are in discrepancy with the predictive structure are atten-
uated, thus reducing the impact on inferences. This phenomenon, clearly,
does not take place in the regression model with normally distributed er-
rors. A comprehensive discussion of the eÔ¨Äect of outliers is in Barnett and
Lewis (1995).
An alternative to the univariate process (13.65) is to adopt a multivariate-
t error distribution of order n for the residuals, that is, assume
tn

0, IœÉ2, ŒΩ

,
where IœÉ2 is the scale matrix. Here the residuals are uncorrelated although
not independent (recall that in a multivariate-t distribution with a diagonal
scale matrix, the joint density cannot be obtained by multiplying the corre-
sponding marginal densities, all of which are univariate-t). The regression
model is then based on the data generating scheme
y|Œ≤, œÉ2, ŒΩ ‚àºtn

XŒ≤, œÉ2, ŒΩ

.
Note that the data constitute a sample of size 1 from a multivariate-t
distribution of order n. Assuming that œÉ2 and ŒΩ are both known, and that
Œ≤ has been assigned a Ô¨Çat prior, the posterior density takes the form
p

Œ≤|y, œÉ2, ŒΩ

‚àù

1 + (y ‚àíXŒ≤)‚Ä≤ (y ‚àíXŒ≤)
œÉ2ŒΩ
‚àín+ŒΩ
2
.
A search for the mode gives
5Œ≤ = (X‚Ä≤X)‚àí1 X‚Ä≤y,
as meeting the Ô¨Årst-order condition, which is the posterior mode as if the
errors had been assigned a multivariate normal distribution. The reason for

592
13. Gaussian and Thick-Tailed Linear Models
this is that all observations here receive the same implicit weight
1
œÉ2 + 1
ŒΩ
n
i=1
(yi ‚àíx‚Ä≤
iŒ≤)2 ,
so that no ‚Äúrobustness‚Äù in inference is gained by adopting this multivariate
error distribution, other than an inÔ¨Çation of the posterior standard devi-
ations of individual regression coeÔ¨Écients. Further, Zellner (1976) states
that when Œ≤, œÉ2, and ŒΩ are all unknown, the joint posterior (using Ô¨Çat pri-
ors for Œ≤, œÉ2, and ŒΩ) does not have a maximum, indicating that the joint
posterior is improper. On the other hand, Liu and Rubin (1995) describe
modiÔ¨Åcations of the EM algorithm for the situation where the degrees of
freedom are unknown (these can be estimated when there is replication of
samples from the same multivariate-t process) and obtain reasonable re-
sults in their examples. At any rate, posterior modes (or ML estimates in
this case) of Œ≤ and of œÉ2 exist for any Ô¨Åxed value of the degrees of free-
dom parameter, even when a single sample is drawn from the multivariate-t
distribution (Zellner, 1976; McLachlan and Krishnan, 1997). As implied by
Zellner (1976), the scale and degrees of freedom parameters are confounded
when a single sample is drawn, even if the data vector contains n observa-
tions. If, in a Bayesian context, a proper prior is assigned to the degrees
of freedom parameter, the net eÔ¨Äect is to spread the uncertainty across
all parameters of the model, but it is diÔ¨Écult to assess how each of the
marginal distributions would be aÔ¨Äected, since the analytical approach is
intractable.
Now consider the linear model of (13.1), but under the assumptions
yi|Œ≤, a, œÉ2
e, ŒΩe
‚àº
t1

x‚Ä≤
iŒ≤ + z‚Ä≤
ia, œÉ2
e, ŒΩe

,
(13.70)
a|A, œÉ2
a, ŒΩa
‚àº
tq

0, A œÉ2
a, ŒΩa

,
(13.71)
where z‚Ä≤
i is the ith row of Z, A œÉ2
a is the scale matrix of the q variate t
distribution given above, and ŒΩa is the corresponding degrees of freedom
parameter. This assumption preserves the property of the multivariate nor-
mal distribution usually employed for additive genetic eÔ¨Äects: all marginal
and conditional distributions are univariate- or multivariate-t, and any lin-
ear combination of breeding values is t as well. For example, if the joint
distribution of the additive genetic eÔ¨Äects of the mother, father, and of the
segregation residual is trivariate-t, then the breeding value of the progeny
is also t. On the other hand, if the breeding values of the father and mother
are independently distributed as univariate-t and the segregation residual
is also an independent univariate-t variable, the additive genetic value of
the oÔ¨Äspring is not t. Now suppose that the scales and degrees of freedom
are known and, as before, that a Ô¨Çat prior is assigned to the location vector

13.6 Linear Models with Thick-Tailed Distributions
593
Œ≤. The logarithm of the joint posterior density is
p

Œ≤, a|œÉ2
a, ŒΩa, œÉ2
e, ŒΩe, y

= K ‚àí

1 + ŒΩe
2

n

i=1
log

1 +

yi ‚àíx‚Ä≤
iŒ≤ ‚àíz‚Ä≤
ia
2
œÉ2eŒΩe

‚àí

q + ŒΩa
2

log

1 + a‚Ä≤A‚àí1a
œÉ2aŒΩa

.
(13.72)
The gradients of this density are
‚àÇp

Œ≤, a|œÉ2
a, ŒΩa, œÉ2
e, ŒΩe, y

‚àÇŒ≤
=

1 + ŒΩe
ŒΩe

n

i=1
xi

yi ‚àíx‚Ä≤
iŒ≤ ‚àíz‚Ä≤
ia

œÉ2e + (yi‚àíx‚Ä≤
iŒ≤‚àíz‚Ä≤
ia)
2
ŒΩe
,
‚àÇp

Œ≤, a|œÉ2
a, ŒΩa, œÉ2
e, ŒΩe, y

‚àÇa
=

1 + ŒΩe
ŒΩe

n

i=1
zi

yi ‚àíx‚Ä≤
iŒ≤ ‚àíz‚Ä≤
ia

œÉ2e + (yi‚àíx‚Ä≤
iŒ≤‚àíz‚Ä≤
ia)
2
ŒΩe
‚àí

q + ŒΩa
ŒΩa

A‚àí1a

œÉ2a + a‚Ä≤A‚àí1a
ŒΩa
.
Let
œÉ2
ei = œÉ2
eŒΩe +

yi ‚àíx‚Ä≤
iŒ≤ ‚àíz‚Ä≤
ia
2
ŒΩe + 1
,
i = 1, 2, . . . , n,
and note that this ‚Äúpseudo-variance‚Äù can be viewed as a weighted average of
œÉ2
e (known parameter) and of

yi ‚àíx‚Ä≤
iŒ≤ ‚àíz‚Ä≤
ia
2 ; if ŒΩe ‚Üí‚àû, then œÉ2
ei ‚ÜíœÉ2
e
(normality). Similarly, let
œÉ2
a =
ŒΩaœÉ2
a + q a‚Ä≤A‚àí1a
q
ŒΩa + q
,
which is a weighted average between œÉ2
a and [a‚Ä≤A‚àí1a]/q, and observe that
if ŒΩa ‚Üí‚àû, œÉ2
a ‚ÜíœÉ2
a. Setting all derivatives to 0 simultaneously and rear-
ranging leads to the iterative system

X‚Ä≤D‚àí1
Œ≤uX
X‚Ä≤D‚àí1
Œ≤uZ
Z‚Ä≤D‚àí1
Œ≤uX
Z‚Ä≤D‚àí1
Œ≤uZ + A‚àí1
œÉ2
a
[t+1] 
Œ≤
a
[t]
=
 X‚Ä≤D‚àí1
Œ≤uy
Z‚Ä≤D‚àí1
Œ≤uy
[t]
, (13.73)
where
D‚àí1
Œ≤u = Diag

 1
œÉ2
ei

and œÉ2
a change iteratively. This is a set of iteratively reweighted mixed
model equations, where observations that are far away from their condi-
tional expectations (x‚Ä≤
iŒ≤ + z‚Ä≤
ia) are downweighted, and the more so when
the degrees of freedom of the residual distribution are small. Similarly, the

594
13. Gaussian and Thick-Tailed Linear Models
pseudo-variance œÉ2
a is modiÔ¨Åed iteratively, as the values of a change from
round to round. In summary, this illustrates, at least when the degrees of
freedom are Ô¨Åxed, that the univariate-t distribution for the residuals has
the eÔ¨Äect of attenuating observations that are away, in some sense, from
the predictive structure of the model, whereas the multivariate-t assump-
tion for the additive genetic eÔ¨Äects has the eÔ¨Äect of modifying the impact
of the scale parameter œÉ2
a in the light of what the data have to say about
breeding values.
A word of caution about modal estimates is in order here. At least for
the regression model and the multivariate-t residual distribution, Liu and
Rubin (1995) point out that the likelihood function (or posterior distri-
bution under Ô¨Çat priors) can be multimodal when the degrees of freedom
parameter is small or unknown. In this case, the point estimates may be
of little interest by themselves, even though they may be global or local
maxima. McLachlan and Krishnan (1997) gave an example where the data
vector was y‚Ä≤ = [‚àí20, 1, 2, 3], and Ô¨Åtted a univariate-t distribution with
scale parameter equal to 1 and the degrees of freedom set to 0.05. For this
situation, they found that the likelihood of the unknown mean ¬µ had four
local maxima:
¬µ1 = ‚àí19.993, ¬µ2 = 1.086, ¬µ3 = 1.997, ¬µ4 = 2.906,
and three local minima:
¬µ5 = ‚àí14.516, ¬µ6 = 1.373, ¬µ7 = 2.647.
The plot of the log-likelihood (or posterior density under a Ô¨Çat prior, apart
from an additive constant) revealed that the likelihood fell abruptly in the
neighborhood of the global maximum ¬µ3 = 1.997, so there would be little
posterior probability mass in the neighboring region. This reinforces the
point that often there is no substitute for the entire posterior distribution.
Strand¬¥en and Gianola (1998), in a simulation study, evaluated the uni-
variate t residual distribution for coping with the eÔ¨Äects of an unknown
preferential treatment of some animals in livestock breeding. Alternatively,
a multivariate-t residual distribution was used, where residuals were clus-
tered by herd; here the assumption was that the residuals were uncorrelated
but not independent. They used a Bayesian model (with Gibbs sampling),
where the residual distribution was univariate-t, and treated the degrees of
freedom as an unknown, discrete parameter; the usual multivariate normal
distribution was assigned to the breeding values. In the simulation, they
compared the mean squared error of predicted breeding values (using pos-
terior means), in situations with or without preferential treatment. In the
absence of preferential treatment, the t-models were as good as the Gaus-
sian ones. When such treatment was present, the univariate-t model was
clearly the best, and the posterior distribution of the degrees of freedom
pointed away from the Gaussian assumption. The authors pointed out that

13.6 Linear Models with Thick-Tailed Distributions
595
it was encouraging that a symmetric error distribution improved upon the
Gaussian one, even under a single-tailed form of preferential treatment. A
robust asymmetric distribution, such as in Fernandez and Steel (1998), may
do even better, but perhaps at the expense of conceptual and computational
simplicity. Their Bayesian implementation, with some slight modiÔ¨Åcations,
is discussed subsequently.
13.6.2
A Student-t Mixed EÔ¨Äects Model
Return to a model with the assumptions as in (13.70) and (13.71) but
assume now that the degrees of freedom and the scale parameters are un-
known. As seen in Chapter 1 and Chapter 6, the t distributions can be
generated by mixing a normal distribution over gamma processes with ap-
propriate parameters. The two assumptions can be replaced by the ‚Äúaug-
mented‚Äù hierarchy
yi|Œ≤, a, œÉ2
e, wi ‚àºN

x‚Ä≤
iŒ≤ + z‚Ä≤
ia,œÉ2
e
wi

,
i = 1, 2, ..., n,
(13.74)
wi|ŒΩe ‚àºGa
ŒΩe
2 , ŒΩe
2

,
i = 1, 2, ..., n,
(13.75)
a|A, œÉ2
a, wa ‚àºN

0, A œÉ2
a
wa

,
(13.76)
wa|ŒΩa ‚àºGa
ŒΩa
2 , ŒΩa
2

.
(13.77)
Assume now that Œ≤ and the two scale parameters are assigned independent,
proper, uniform distributions and that the prior densities of the degrees of
freedom are p (ŒΩe) and p (ŒΩa) ; also let the two degrees of freedom have
independent prior distributions. Then the joint posterior density for the
augmented hierarchy has the form
p

Œ≤, a,œÉ2
e, w,œÉ2
a, wa, ŒΩe, ŒΩa|y

‚àù
n
-
i=1

p

yi|Œ≤, a,œÉ2
e, wi

p (wi|ŒΩe)

√óp

a|A, œÉ2
a, wa

p (wa|ŒΩa) p (ŒΩe) p (ŒΩa) ,
(13.78)
where w = {wi} is the vector of residual weights. The conditional poste-
rior distributions needed for running a MCMC scheme are presented next,
making use of results derived in Chapter 6, in connection with the linear
regression model with residuals distributed as t. In what follows the usual
notation ‚ÄúELSE‚Äù is employed to denote the data vector y and all param-
eters that are treated as known in the appropriate conditional posterior
distribution.

596
13. Gaussian and Thick-Tailed Linear Models
Residual and Genetic ‚ÄúWeights‚Äù
Note in (13.78) that the residual weights wi are conditionally independent
of each other, with the individual densities being
p (wi|ELSE) ‚àùp

yi|Œ≤, a, œÉ2
e, wi

p (wi|ŒΩe)
‚àù

œÉ2
e
wi
‚àí1
2
w
ŒΩe
2 ‚àí1
i
exp
#
‚àíwi
2

yi ‚àíx‚Ä≤
iŒ≤ ‚àíz‚Ä≤
ia
2 + ŒΩeœÉ2
e
œÉ2e
$
‚àùw
ŒΩe+1
2
‚àí1
i
exp

‚àíwiSi
2

,
i = 1, 2, ..., n.
(13.79)
where
Si =

yi ‚àíx‚Ä≤
iŒ≤ ‚àíz‚Ä≤
ia
2 + ŒΩeœÉ2
e
œÉ2e
.
This indicates that the conditional posterior distribution of each wi is the
gamma distribution
wi|ELSE ‚àºGa

ŒΩe + 1
2
, Si
2

,
i = 1, 2, ..., n.
(13.80)
Similarly,
p (wa|ELSE) ‚àùp

a|A, œÉ2
a, wa

p (wa|ŒΩa)
‚àù

 œÉ2
a
wa
‚àíq
2
w
ŒΩa
2 ‚àí1
a
exp

‚àíwa
2

a‚Ä≤A‚àí1a + ŒΩaœÉ2
a
œÉ2a

‚àùw
ŒΩa+q
2
‚àí1
a
exp

‚àíwa
2

a‚Ä≤A‚àí1a + ŒΩaœÉ2
a
œÉ2a

,
so its conditional distribution is also gamma
wa|ELSE ‚àºGa

ŒΩa + q
2
, a‚Ä≤A‚àí1a + ŒΩaœÉ2
a
2œÉ2a

.
(13.81)
Location EÔ¨Äects
From the joint density, it follows that
p (Œ≤, a|ELSE) ‚àù
n
-
i=1
p

yi|Œ≤, a, œÉ2
e, wi

p

a|A, œÉ2
a, wa

‚àùexp

‚àí1
2œÉ2e

(y ‚àíXŒ≤ ‚àíZa)‚Ä≤ W (y ‚àíXŒ≤ ‚àíZa) + waœÉ2
e
œÉ2a
a‚Ä≤A‚àí1a
%
,
where W = {wi} is an n√ón matrix. This form was encountered in Chapter
6 and in Section 13.2 of this chapter. The conditional posterior distribution
is multivariate normal with parameters
Œ∏|ELSE ‚àºN

5Œ∏, C‚àí1œÉ2
e

,
(13.82)

13.6 Linear Models with Thick-Tailed Distributions
597
where
5Œ∏
=

X‚Ä≤WX
X‚Ä≤WZ
Z‚Ä≤WX
Z‚Ä≤WZ + A‚àí1 waœÉ2
e
œÉ2
a
‚àí1 
X‚Ä≤Wy
Z‚Ä≤Wy

,
C‚àí1
=

X‚Ä≤WX
X‚Ä≤WZ
Z‚Ä≤WX
Z‚Ä≤WZ + A‚àí1 waœÉ2
e
œÉ2
a
‚àí1
.
Techniques for drawing the location eÔ¨Äects either in piecewise or in block-
wise manners, or in a single pass, have been discussed earlier in this chapter,
with the only novelty being the appearance of the wi and wa weights.
Scale Parameters
The conditional posterior density of the scale parameter œÉ2
e, making refer-
ence to (13.78), is
p

œÉ2
e|ELSE

‚àù
n
-
i=1

p

yi|Œ≤, a,œÉ2
e, wi

‚àù

œÉ2
e
‚àín
2 exp

‚àí1
2œÉ2e
(y ‚àíXŒ≤ ‚àíZa)‚Ä≤ W (y ‚àíXŒ≤ ‚àíZa)

‚àù

œÉ2
e
‚àín‚àí2
2
+1 exp

‚àí1
2œÉ2e
(y ‚àíXŒ≤ ‚àíZa)‚Ä≤ W (y ‚àíXŒ≤ ‚àíZa)

.
It follows that the conditional posterior distribution is the scaled inverted
chi-square process
œÉ2
e|ELSE ‚àº(y ‚àíXŒ≤ ‚àíZa)‚Ä≤ W (y ‚àíXŒ≤ ‚àíZa) œá‚àí2
n‚àí2.
(13.83)
Similarly,
p

œÉ2
a|ELSE

‚àùp

a|A, œÉ2
a, wa

‚àù

œÉ2
a
‚àíq‚àí2
2
+1 exp

‚àíwa
2œÉ2a
a‚Ä≤A‚àí1a

,
which indicates that
œÉ2
a|ELSE ‚àºwaa‚Ä≤A‚àí1aœá‚àí2
q‚àí2.
(13.84)
Degrees of Freedom
The conditional posterior distribution of the degrees of freedom parame-
ters can be deduced from (13.78). One arrives immediately at the result
that ŒΩe and ŒΩa have conditionally independent posterior distributions, with
densities
p (ŒΩe|ELSE) ‚àù
 n
-
i=1
p (wi|ŒΩe)

p (ŒΩe) ,

598
13. Gaussian and Thick-Tailed Linear Models
and
p (ŒΩa|ELSE) ‚àùp (wa|ŒΩa) p (ŒΩa) .
It is not possible to go further without being speciÔ¨Åc about the form of the
prior distributions. Here, we will consider two alternative settings.
In the Ô¨Årst one, the degrees of freedom (positive parameters) are allowed
to take integer values only over a Ô¨Ånite set of values having equal prior
probability (Albert and Chib, 1993; Geweke, 1993; Strand¬¥en, 1996). Let
these sets contain values fj = 1, 2, . . . , de and qk = 1, 2, . . . , da, respectively.
The prior distributions are then
p (ŒΩe) = 1
de
,
ŒΩe ‚àà{fj = 1, 2, ..., de} ,
and
p (ŒΩe) = 1
da
,
ŒΩa ‚àà{qk = 1, 2, ..., da} .
Recalling that the prior distributions of the weights are in Gamma form,
it follows that
p (ŒΩe|ELSE) ‚àù
Ô£Æ
Ô£∞
 ŒΩe
2
(
ŒΩe
2 )
Œì
 ŒΩe
2

Ô£π
Ô£ª
n
n
-
i=1

w
ŒΩe
2 ‚àí1
i
exp

‚àíŒΩewi
2

.
This is a discrete distribution, and samples can be drawn by extracting
degrees of freedom values with probabilities
Pr (ŒΩe = fj|ELSE) = Ce
Ô£Æ
Ô£ØÔ£ØÔ£∞

fj
2

 fj
2

Œì

fj
2

Ô£π
Ô£∫Ô£∫Ô£ª
n
n
-
i=1

w
fj
2 ‚àí1
i
exp

‚àífjwi
2

,
(13.85)
where Ce is equal to
Ô£Æ
Ô£ØÔ£ØÔ£∞
de

j=1
Ô£±
Ô£¥
Ô£¥
Ô£≤
Ô£¥
Ô£¥
Ô£≥
Ô£Æ
Ô£ØÔ£ØÔ£∞

fj
2

 fj
2

Œì

fj
2

Ô£π
Ô£∫Ô£∫Ô£ª
n
n
-
i=1

w
fj
2 ‚àí1
i
exp

‚àífjwi
2

Ô£º
Ô£¥
Ô£¥
Ô£Ω
Ô£¥
Ô£¥
Ô£æ
Ô£π
Ô£∫Ô£∫Ô£ª
‚àí1
.
Likewise, and following a similar type of algebra, the conditional posterior
distribution of the degrees of freedom of the multivariate-t distribution of
the additive genetic eÔ¨Äects can be found to be:
Pr (ŒΩa = qk|ELSE) = Ca
 qk
2
(
qk
2 )
Œì
 qk
2
 w
qk
2 ‚àí1
a
exp

‚àíqkwa
2

(13.86)

13.6 Linear Models with Thick-Tailed Distributions
599
where
Ca =
Ô£±
Ô£≤
Ô£≥
da

k=1
Ô£Æ
Ô£∞
 qk
2
(
qk
2 )
Œì
 qk
2
 w
qk
2 ‚àí1
a
exp

‚àíqkwa
2

Ô£π
Ô£ª
Ô£º
Ô£Ω
Ô£æ
‚àí1
.
Hence, new states for the degrees of freedom of the genetic distribution
can be drawn by sampling with probabilities (13.86). The Gibbs sampling
scheme is completed by eÔ¨Äecting draws from distributions (13.80)‚Äì(13.86)
in any suitable order.
If the degrees of freedom are treated as continuous, the conditional pos-
terior densities are not in any recognizable form. Here, one may consider
embedding a Metropolis‚ÄìHastings step in the MCMC scheme, as in Geweke
(1993) and Rodriguez-Zas (1998). For example, Geweke (1993) uses an ex-
ponential distribution for these parameters, so that the conditional poste-
rior density of the residual degrees of freedom has the form
p (ŒΩe|ELSE) ‚àù
Ô£Æ
Ô£∞
 ŒΩe
2
(
ŒΩe
2 )
Œì
 ŒΩe
2

Ô£π
Ô£ª
n
n
-
i=1

w
ŒΩe
2 ‚àí1
i
exp

‚àíŒΩewi
2

exp (‚àíœâeŒΩe)
‚àù
Ô£Æ
Ô£∞
 ŒΩe
2
(
ŒΩe
2 )
Œì
 ŒΩe
2

Ô£π
Ô£ª
n
exp

‚àíŒΩe (2œâe + nw)
2

n
-
i=1
w
ŒΩe
2 ‚àí1
i
,
(13.87)
where œâe is the parameter of the prior exponential distribution and w is the
average of the weights wi at any given iterate. For example, if œâe = 0.15,
this corresponds to a prior mean and variance of 6.6 and 44.4, respectively,
for the residual degrees of freedom. The exponential prior can be assessed
such that both small and large values of the degrees of freedom receive
‚Äúhigh‚Äù prior probability, making the speciÔ¨Åcation vague enough. Rodriguez-
Zas (1998) used a normal proposal distribution for Metropolis‚ÄìHastings
with parameters based on the current values of the weights in the course
of iteration. Similarly, the conditional posterior density of the degrees of
freedom of the genetic distribution would be
p (ŒΩa|ELSE) ‚àù
 ŒΩa
2
(
ŒΩa
2 )
Œì
 ŒΩa
2
 w
ŒΩa
2 ‚àí1
a
exp

‚àíŒΩawa
2

exp (‚àíœâaŒΩa)
‚àù
 ŒΩa
2
(
ŒΩa
2 )
Œì
 ŒΩa
2

exp

‚àíŒΩa (2œâa + wa)
2

w
ŒΩa
2 ‚àí1
a
,
(13.88)
where œâa is the parameter of the prior exponential distribution.
Making an analogy with the single sample multivariate-t model of Zellner
(1976), we conjecture that there is no information in the likelihood available
to separate ŒΩa from œÉ2
a. As noted, using proper priors for both parameters
solves the identiÔ¨Åability problem and spreads the uncertainty throughout
the model, which is realistic. On the other hand, Strand¬¥en and Gianola

600
13. Gaussian and Thick-Tailed Linear Models
(1999) suggest Ô¨Åtting a series of models with alternative values for ŒΩa and
then assessing the impact on inferences. The diÔ¨Äerent models can be con-
trasted using some of the Bayesian model comparison tools discussed earlier
in the book. On the other hand, if the t-distribution has replicate samples,
such as in a sire model where the transmitting abilities are distributed in-
dependently, the two parameters are not confounded. Hence, in practice,
one can cluster individuals into families, assume these are independent, and
Ô¨Ånd the most probable value of the degrees of freedom. Then, conditionally
on such modal value, one proceeds with the implementation given above,
ignoring the uncertainty about its error.
13.6.3
Model with Clustered Random EÔ¨Äects
The setting here is one where observations are clustered in some natu-
ral manner, and where cluster eÔ¨Äects are independent and identically dis-
tributed as univariate-t, with unknown scale and degrees of freedom pa-
rameters. For example, individuals may be clustered in nuclear or half-sib
families, or in randomly created inbred lines; alternatively, observations
could consist of repeated measures on individuals, in which case the sub-
jects constitute the clustering criterion.
The assumptions to be made here are
yi|Œ≤, a, œÉ2
e, wi
‚àº
N

x‚Ä≤
iŒ≤ + z‚Ä≤
ia,œÉ2
e
wi

,
i = 1, 2, ..., n,
wi|ŒΩe
‚àº
Ga
ŒΩe
2 , ŒΩe
2

,
i = 1, 2, ..., n,
for the observations, thus deÔ¨Åning a univariate-t distribution upon integra-
tion of the joint distribution over the weights, and
ai|œÉ2
a, wai
‚àº
N

0, œÉ2
a
wai

,
i = 1, 2, ..., q,
wai|ŒΩa
‚àº
Ga
ŒΩa
2 , ŒΩa
2

,
i = 1, 2, ..., q.
This is equivalent to stating that the cluster eÔ¨Äects are independent and
identically distributed as univariate-t, a priori, with null mean, scale pa-
rameter œÉ2
a, and degrees of freedom ŒΩa. Here there is ‚Äúreplication‚Äù of the
second-stage distribution, so both œÉ2
a and ŒΩa are estimable in the maximum
likelihood sense.
The joint posterior density of all unknowns is
p

Œ≤, a, œÉ2
e, w,œÉ2
a, wa, ŒΩe, ŒΩa|y

‚àù
n
-
i=1

p

yi|Œ≤, a, œÉ2
e, wi

p (wi|ŒΩe)

√ó
q
-
i=1

p

ai|œÉ2
a, wai

p (wai|ŒΩa)

p (ŒΩe) p (ŒΩa) ,

13.6 Linear Models with Thick-Tailed Distributions
601
where wa = {wai} is a q √ó 1 vector. The conditional posterior distribution
of the residual weights and of œÉ2
e remain as in (13.79) and (13.83 ), respec-
tively. Further, the conditional posterior distribution of the cluster weights
and variance takes the form
wai|ELSE ‚àºGa

ŒΩa + 1
2
, a2
i + ŒΩaœÉ2
a
2œÉ2a

,
i = 1, 2, . . . , q.
(13.89)
The scale parameter œÉ2
a has as conditional posterior distribution
œÉ2
a|ELSE ‚àº
 q

i=1
waia2
i

œá‚àí2
q‚àí2,
(13.90)
noting that the random eÔ¨Äects enter attenuated by the weights wai, relative
to their counterpart in a purely Gaussian model. For example, the scale
parameter of this scaled inverted chi-square distribution would be q
i=1 a2
i
in the latter.
The conditional posterior density of the location eÔ¨Äects is
p (Œ≤, a|ELSE) ‚àù
n
-
i=1
p

yi|Œ≤, a,œÉ2
e, wi

q
-
i=1
p

ai|œÉ2
a, wai

‚àùexp

‚àí1
2œÉ2e
(y ‚àíXŒ≤ ‚àíZa)‚Ä≤ W (y ‚àíXŒ≤ ‚àíZa)

exp

‚àí1
2œÉ2a
a‚Ä≤Waa

,
where Wa is a q √ó q diagonal matrix with typical element equal to wai
(i = 1, 2, ..., q) . Using well established results, manipulation of the above
density leads directly to the normal distribution
Œ∏w|ELSE ‚àºN

5Œ∏w, C‚àí1
w œÉ2
e

,
(13.91)
where
5Œ∏w =

X‚Ä≤WX
X‚Ä≤WZ
Z‚Ä≤WX
Z‚Ä≤WZ + Wa
œÉ2
e
œÉ2
a
‚àí1 
X‚Ä≤Wy
Z‚Ä≤Wy

,
and
C‚àí1
w =

X‚Ä≤WX
X‚Ä≤WZ
Z‚Ä≤WX
Z‚Ä≤WZ + Wa
œÉ2
e
œÉ2
a
‚àí1
.
Finally, the conditional posterior densities of the degrees of freedom (con-
tinuous case with exponential prior distributions) are as in (13.87) for the
residual distribution and
p (ŒΩa|ELSE) ‚àù
Ô£Æ
Ô£∞
 ŒΩa
2
(
ŒΩa
2 )
Œì
 ŒΩa
2

Ô£π
Ô£ª
q
exp

‚àíŒΩa (2œâa + qwa)
2

q
-
i=1
w
ŒΩa
2 ‚àí1
ai
. (13.92)
This completes the speciÔ¨Åcation of an MCMC scheme for a linear model
where all residuals and cluster eÔ¨Äects are distributed as univariate-t with
appropriate parameters.

602
13. Gaussian and Thick-Tailed Linear Models
13.7
Parameterizations and the Gibbs Sampler
Roberts and Sahu (1997) noted that high correlations among the elements
of the parameter vector in the posterior distribution can lead to poor
convergence of the single-site Gibbs sampler. An illustration of the con-
sequences of a large posterior correlation of parameters was discussed in
Chapter 12 in Example 12.2. The eÔ¨Äect of parameterization on the behav-
ior of the Markov chain is also discussed in Gelfand et al. (1995) and in
Gelfand et al. (1996). These authors argued that ‚Äúhierarchically centered‚Äù
parameterizations for linear and nonlinear models reduce the extent of pos-
terior intercorrelations and lead to faster mixing and convergence. Here we
will give a brief discussion of the problem, by adapting the presentation in
Gelfand et al. (1996) to a quantitative genetics setting.
Consider the additive genetic model
yi = ¬µ + ai + ei,
where there is a single observation made in each animal, i = 1, 2, ..., n;
assume that animals are genetically unrelated. As usual, take
yi|¬µ, ai ‚àºN

¬µ + ai, œÉ2
e

as the data generating process and
ai|œÉ2
a ‚àºN

0, œÉ2
a

as a prior for the genetic eÔ¨Äects. Suppose the two variance components are
known, and assign the process ¬µ ‚àºN

¬µ0, œÉ2
¬µ

as a prior distribution for
¬µ, where hyperparameters are taken as known as well. Using standard re-
sults employed several times in this book, the posterior variance‚Äìcovariance
matrix of ¬µ and of the additive genetic eÔ¨Äects is
V ar
Ô£´
Ô£¨
Ô£¨
Ô£¨
Ô£¨
Ô£¨
Ô£¨
Ô£≠
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
¬µ
a1
a2
.
.
an
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
""""""""""""
¬µ0, œÉ2
¬µ, œÉ2
a, œÉ2
e, y
Ô£∂
Ô£∑
Ô£∑
Ô£∑
Ô£∑
Ô£∑
Ô£∑
Ô£∏
=
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
n + œÉ2
e
œÉ2¬µ
1
1
.
1
1
1
1 + œÉ2
e
œÉ2
a
0
.
0
0
1
0
1 + œÉ2
e
œÉ2
a
.
0
0
.
.
.
.
.
.
1
0
.
.
1 + œÉ2
e
œÉ2
a
0
1
0
.
.
0
1 + œÉ2
e
œÉ2
a
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
‚àí1
œÉ2
e.

13.7 Parameterizations and the Gibbs Sampler
603
The inverse of this matrix can be found using results for partitioned matri-
ces (e.g., Searle, 1982). Gelfand et al. (1996) arrive at the following repre-
sentations (the conditioning on the known parameters or hyperparameters
is suppressed in the notation)
Corr (¬µ, ai|y)
=
‚àí

1 + nœÉ2
e
œÉ2a
+ œÉ2
e
œÉ2¬µ
+ œÉ4
e
œÉ2¬µ
‚àí1
2
,
Corr (ai, aj|y)
=

1 + nœÉ2
e
œÉ2a
+ œÉ2
e
œÉ2¬µ
+ œÉ4
e
œÉ2¬µ
‚àí1
.
As œÉ2
e tends to inÔ¨Ånity, the correlations go to 0, but tend to ‚àí1 and
1, respectively if either œÉ2
a or œÉ2
¬µ become larger and larger (vague prior
knowledge about the random eÔ¨Äects). From an animal breeding point of
view, suppose that a Ô¨Çat prior is assigned to ¬µ (œÉ2
¬µ ‚Üí‚àû) and recall that
œÉ2
e/œÉ2
a = (1 ‚àíh2)/h2, where h2 is heritability. Then, when h2 ‚Üí0, so that
the variability is dominated by environmental eÔ¨Äects,
Corr (¬µ, ai|y) = ‚àí

1 + n1 ‚àíh2
h2
‚àí1
2
‚Üí0
and
Corr (ai, aj|y) =

1 + n1 ‚àíh2
h2
‚àí1
‚Üí0.
On the other hand, when h2 ‚Üí1, a situation in which one deÔ¨Ånitely needs
the random eÔ¨Äects model, the absolute value of the correlations tend to 1.
This implies that the Gibbs sampler will mix slower for highly heritable
traits, at least with the standard parameterization given above.
Gelfand et al. (1996) also discuss ‚Äúhierarchical centering‚Äù. Here, they
deÔ¨Åne Œ∑i = ¬µ + ai, so that the resulting hierarchical model is yi|Œ∑i ‚àº
N

Œ∑i, œÉ2
e

, with Œ∑i|¬µ, œÉ2
a ‚àºN

¬µ, œÉ2
a

, and ¬µ ‚àºN

¬µ0, œÉ2
¬µ

. These two
models are probabilistically equivalent, that is, give the same prior predic-
tive distribution or marginal distribution of the observations. The authors
encounter
Corr (¬µ, Œ∑i|y)
=
‚àí

1 + nœÉ2
a
œÉ2e
+ œÉ2
a
œÉ2¬µ
+ œÉ4
a
œÉ2¬µ
‚àí1
2
,
Corr

Œ∑i, Œ∑j|y

=

1 + nœÉ2
a
œÉ2e
+ œÉ2
a
œÉ2¬µ
+ œÉ4
a
œÉ2¬µ
‚àí1
.

604
13. Gaussian and Thick-Tailed Linear Models
Here the correlations do not go to 0 if œÉ2
e ‚Üí‚àû. Returning again to the
animal breeding setting, with a uniform improper prior for ¬µ, one gets
Corr (¬µ, Œ∑i|y)
=
‚àí

1 + n
h2
1 ‚àíh2
‚àí1
2
,
Corr

Œ∑i, Œ∑j|y

=

1 + n
h2
1 ‚àíh2
‚àí1
.
When heritability tends to 0, the correlations tend to 1, but when the
genetic variance becomes relatively more and more important, the correla-
tions go to 0, indicating that the hierarchical parameterization should be
preferred. In the standard parameterization, the data inform directly on
Œ∑i = ¬µ + ai, so if there is vague prior knowledge about ai (large value of
the additive genetic variance) the data cannot separate ¬µ from ai; thus, the
large negative correlation between these unknowns. Gelfand et al. (1996)
note that, in practice, the variance components are unknown, so it is nec-
essary to consider the joint posterior distribution of location eÔ¨Äects and
dispersion parameters; however, they recommend hierarchical centering as
a default procedure. A detailed study of the problem, including longitudinal
data settings, is in Gelfand et al. (1995). They concluded that hierarchical
centering will usually improve convergence of sampling-based procedures
for Bayesian analysis.
Other types of parameterizations and strategies to improve mixing and
convergence were studied by Hills and Smith (1992, 1993); Liu (1994) and
by Liu et al. (1994).

14
Threshold Models
for Categorical Responses
14.1
Introduction
Discrete response variables are ubiquitous in genetics. In particular, cate-
gorical responses arise when the outcome is an assignment into one of sev-
eral mutually exclusive and exhaustive classes. Typical examples include
congenital malformations, leg weakness traits in pigs, presence or absence
of intra-mammary infection in cows, subjective scores describing diÔ¨Éculties
at birth in cattle, X-ray readings of hip-dysplasia in dogs, and litter-size in
sheep. When there are two categories of response, the trait is referred to as
binary or ‚Äúall or none‚Äù (Dempster and Lerner, 1950). With more than two
categories, a distinction must be made as to whether the classes are either
unordered or ordered in some manner. Unordered categories can arise when
the outcome is a choice; for example, electing an item in a menu or voting
for a certain candidate. In this chapter, however, the focus will be on the
ordered categories. This is so, because in biological systems, response cate-
gories can be ordered almost invariably along some hypothetical gradient.
For example, it is possible to think about a fecundity gradient in sheep,
from least proliÔ¨Åc to most proliÔ¨Åc. Here, the litter size observed at birth
would be related somehow to this conceptual gradient.
Quantitative geneticists have used the so called threshold model to relate
a hypothetical continuous scale to an outward phenotype (the observed cat-
egory of response). The underlying variate is often called ‚Äúthe latent vari-
able‚Äù or, at least in genetics of disease, ‚Äúliability‚Äù, after Falconer (1965).
The model postulates that the continuous response is rendered discrete via

606
14. Threshold Models for Categorical Responses
some Ô¨Åxed thresholds or boundaries delimiting categories (Wright, 1934;
Robertson and Lerner, 1949; Dempster and Lerner, 1950; Falconer, 1965,
1967). For example, if there are two categories of response, the observa-
tion would be in the second class, e.g., ‚Äúdisease‚Äù, if the liability exceeds
the threshold. The origins of the threshold model can be traced back to
Pearson (1900), Wright (1934), Bliss (1935), and Dempster and Lerner
(1950). Recent methodological contributions, made primarily from a statis-
tical genetic perspective, include Gianola (1982), Harville and Mee (1984),
Gianola and Foulley (1983), Foulley et al. (1987) and Foulley and Manfredi
(1991), among others. Curnow (1972) and Curnow and Smith (1975) sug-
gested an alternative concept where, instead of having abrupt thresholds,
there is a ‚Äúrisk function‚Äù. However, at least as these authors formulate the
model, their development is mathematically equivalent to one with abrupt
thresholds.
The threshold model for analysis of binary traits was encountered in
Chapter 4. There, it was pointed out that whenever the model requires
speciÔ¨Åcation of random eÔ¨Äects, the likelihood function (or marginal pos-
terior distribution) does not have a closed form. In this case, standard
likelihood-based analyses have been conducted using Gaussian quadrature
approximations in models with independent random eÔ¨Äects, as in Anderson
and Aitkin (1985) or, when the random eÔ¨Äects are correlated, with the
Monte Carlo EM algorithm, as in McCulloch (1994). Analyses based on
approximations have been developed by Gilmour et al. (1985) and Lee and
Nelder (1996) have suggested what are called ‚Äúhierarchical likelihoods‚Äù.
Basically, these methods can be viewed as approximations to REML and
BLUP in the context of generalized linear mixed models. Many of the ar-
guments on which these methods rest are of an asymptotic nature, and the
Ô¨Ånite sample properties of the procedures are unknown.
In this chapter, an MCMC Bayesian implementation of two models in-
volving ordered categorical traits is described. In all cases, it is assumed
that the underlying or latent variable has a (conditional) Gaussian distribu-
tion. The object of inference may include, for example, the additive genetic
values of individuals (to rank candidates for selection in genetic improve-
ment programs), the probability distribution by category of response for
some individuals or experimental conditions or interest, or the genetic vari-
ance of the trait in question. The Ô¨Årst model discussed extends the analysis
of a binary trait presented in Chapter 4 to one with an arbitrary number of
ordered response categories. Following Albert and Chib (1993), it is shown
that a Gibbs sampler, used with data augmentation, leads to fully poste-
rior distributions that are easy to sample from. The development is based
on Sorensen et al. (1995), Heringstad et al. (2001) and Kadarmideen et al.
(2002). The second model described can be used when the outcome is an
ordered categorical response and an observation on a Gaussian trait, follow-
ing ideas in Jensen (1994), Sorensen (1996) and in Wang et al. (1997). See
Foulley et al. (1983) for an approximate Bayesian analysis of this model.

14.2 Analysis of a Single Polychotomous Trait
607
14.2
Analysis of a Single Polychotomous Trait
14.2.1
Sampling Model
Let all underlying latent variables or liabilities be represented by the vector
l = {li} (i = 1, 2, . . . , n), such that for the ith individual or data point it is
postulated that
li = x‚Ä≤
iŒ≤ + z‚Ä≤
ia + ei.
(14.1)
Here Œ≤ are some location eÔ¨Äects, a is a q √ó 1 vector of additive genetic
values (perhaps of order larger than n), and ei ‚àºN

0, œÉ2
e

is a random
residual. As usual, x‚Ä≤
i and z‚Ä≤
i are incidence row vectors. It will be assumed
that, given the location parameters Œ≤ and a, the elements of the vector l
are conditionally independent and distributed as

l|Œ≤, a, œÉ2
e

‚àºN

XŒ≤ + Za, IœÉ2
e

.
(14.2)
Since the liability variate is unobservable, the parameterization œÉ2
e = 1 will
be adopted here (i.e., Cox and Snell, 1989), in order to achieve identiÔ¨Åabil-
ity in the likelihood. It must be noted that when inferences are based on
posterior distributions with proper priors assigned to all parameters, this is
not technically required (Bernardo and Smith, 1994). However, this setting
is used here nonetheless since it is standard in threshold model analysis.
Other parameterizations of the threshold model are discussed in Sorensen
et al. (1995).
Let y = {yi} (i = 1, 2, . . . , n), denote the vector of observed categorical
data. Here, each yi represents an assignment into one of c mutually exclu-
sive and exhaustive categories of response arrived at more or less arbitrarily.
Often, the assignment is subjective, as in analysis of conformation scores.
These classes result from the hypothetical existence of c + 1 thresholds in
the latent scale, such that tmin < t1 < t2 < ¬∑ ¬∑ ¬∑ < tc‚àí1 < tmax. For exam-
ple, if the realized value of liability is between t1 and t2 , the assignment
is in the second category of response. Set the two extreme thresholds to
t0 = tmin, tc = tmax so that the remaining c‚àí1 thresholds can take any value
between tmin and tmax, subject to the preceding order requirement. How-
ever, one of the thresholds must be Ô¨Åxed, so as to center the distribution; a
typical assignment is t1 = 0. Then the conditional probability that yi falls
in category j (j = 1, 2, . . . , c), given Œ≤, a, and t = (tmin, t1, . . . , tc‚àí1, tmax)‚Ä≤
is given by
Pr (yi = j|Œ≤, a, t) = Pr (tj‚àí1 < li < tj|Œ≤, a, t)
= Œ¶

tj ‚àíx‚Ä≤
iŒ≤ ‚àíz‚Ä≤
ia

‚àíŒ¶

tj‚àí1 ‚àíx‚Ä≤
iŒ≤ ‚àíz‚Ä≤
ia

= p (yi|Œ≤, a, t) .
(14.3)

608
14. Threshold Models for Categorical Responses
The data are conditionally independent, given Œ≤, a, and t. Therefore the
sampling model can be written as
p (y|Œ≤, a, t) =
n
-
i=1
c

j=1
I (yi = j) p (yi|Œ≤, a, t)
=
n
-
i=1
c

j=1
I (yi = j)

Œ¶

tj ‚àíx‚Ä≤
iŒ≤ ‚àíz‚Ä≤
ia

‚àíŒ¶

tj‚àí1 ‚àíx‚Ä≤
iŒ≤ ‚àíz‚Ä≤
ia

,
(14.4)
where I (yi = j) is an indicator function taking the value 1 if the response
falls in category j and 0 otherwise.
14.2.2
Prior Distribution and Joint Posterior Density
Adopting the usual hierarchical model building strategy, prior distributions
must be assigned to Œ≤, a, and t. It will be assumed, as usual, that the prior
distribution of a depends on an unknown dispersion parameter œÉ2
a.
The
density of the joint prior distribution adopted has the form
p

Œ≤, a, t, œÉ2
a

= p (Œ≤) p

a|œÉ2
a

p

œÉ2
a

p (t) .
Hence, the joint posterior density is:
p

Œ≤, a, t, œÉ2
a|y

‚àùp (y|Œ≤, a, t) p (Œ≤) p

a|œÉ2
a

p

œÉ2
a

p (t)
= p (Œ≤) p

a|œÉ2
a

p

œÉ2
a

p (t)
√ó
n
-
i=1
c

j=1
I (yi = j)

Œ¶

tj ‚àíx‚Ä≤
iŒ≤ ‚àíz‚Ä≤
ia

‚àíŒ¶

tj‚àí1 ‚àíx‚Ä≤
iŒ≤ ‚àíz‚Ä≤
ia

.
The fully conditional posterior distributions of parameters Œ≤, a, t, and
œÉ2
a must be derived from the above expression. Irrespective of the form of
the joint prior distribution, these conditional processes are not in standard
form, because the parameters appear implicitly inside of the normal inte-
grals. Therefore special strategies must be used for implementing a Gibbs
sampler; see, for example, Moreno et al. (1997).
An algorithmically simpler approach consists of augmenting the joint
posterior distribution with the unobserved liabilities l. If the latent variables
are modelled hierarchically as in a Gaussian linear model for observed data,
this approach yields fully conditional posterior distributions that have a
standard form and which are easy to sample from. Augmenting the joint
posterior with l, the resulting density takes the form
p

Œ≤, a, l, t, œÉ2
a|y

‚àùp

y|Œ≤, a, l, t, œÉ2
a

p

Œ≤, a, l, t,œÉ2
a

= p (y|l, t) p

Œ≤, a, l, t, œÉ2
a

= p (y|l, t) p (l|Œ≤, a) p

Œ≤, a, t, œÉ2
a

= p (y|l, t)
 n
-
i=1
p (li|Œ≤, a)

p

Œ≤, a, t, œÉ2
a

.
(14.5)

14.2 Analysis of a Single Polychotomous Trait
609
The second and third lines of the expression above follow because: 1) the
distribution of the polychotomous observations, given the liabilities, de-
pends only on the thresholds, and 2) given Œ≤ and a, the liabilities are
conditionally independent, as indicated in (14.2). All conditional posterior
distributions can be derived from (14.5), and this will be discussed later
on.
Consider the ith element in the Ô¨Årst term on the right hand side of (14.5)
Pr (yi = j|li, tj‚àí1, tj) =

1,
if tj‚àí1 < li ‚â§tj,
0,
otherwise.
(14.6)
That is, the probability that a given data point falls in a given category,
given the value of liability and thresholds, is completely speciÔ¨Åed. This
means that p (y|l, t) is a degenerate distribution. Following the notation in
Albert and Chib (1993), p (y|l, t) can be written as
p (y|l, t) =
n
-
i=1
 c

i=1
I (tj‚àí1 < li ‚â§tj) I (yi = j)

.
(14.7)
The prior distribution of the vector Œ≤ must be speciÔ¨Åed judiciously. It is
well-established that data structures with a small number of observations
(or in extreme cases, when all observations fall into a particular category)
per element or level of Œ≤, can lead to poor inferences. Misztal et al. (1989)
have referred to this as the extreme category problem (ECP). The problem
is related to the fact that when all observations for a location parameter
are in one of the extreme categories (e.g., in the binary situation, when
all observations are either 0 or 1), the ML estimate of the corresponding
parameter is not Ô¨Ånite. This is clear in a probit model with a single location
parameter: if all observations are 0‚Ä≤s, the ML estimate of the probability
of response is 0. Hence, the corresponding ML estimate of the location
parameter in the liability scale is ‚àí‚àû. In the context of a hierarchical
structure, this problem propagates to other tiers of the model. For example,
when there are ECP instances for at least some elements of Œ≤ and when
a uniform prior distribution is assigned to this vector, Bayesian MCMC
inferences about œÉ2
a can be severely distorted (Hoeschele and Tier, 1995;
Moreno et al., 1997). It is not obvious how this problem can be solved
satisfactorily, although some ad hoc approaches have been suggested in the
literature.
Following Kadarmideen et al. (2002), partition the vector Œ≤ as
Œ≤ =

Œ≤‚Ä≤
h, Œ≤‚Ä≤
r
‚Ä≤ ,
with the corresponding partition for the incidence matrix being
X = [Xh, Xr] .

610
14. Threshold Models for Categorical Responses
Here Œ≤h (H √ó 1) contains elements of Œ≤ known to have observations with
ECPs (such as small herds of cattle in which mastitis is screened), and
Œ≤r contains elements of Œ≤ with well-structured data. Then, the distorting
eÔ¨Äects of the ECP on inferences can be tempered somewhat by assuming
that the vector Œ≤h follows, a priori, a normal distribution with a nonnull
mean. A simple possibility is to pose
Œ≤h|Œ≤, œÉ2
Œ≤h ‚àºN

1Œ≤, IhœÉ2
Œ≤h

.
(14.8)
Here 1 is a vector of ones, Œ≤ is a scalar common to all elements of Œ≤h,
and œÉ2
Œ≤h is an unknown dispersion parameter. For Œ≤r one can assume a
vague normal distribution with zero mean and large, known, variance. For
instance, assuming that all scalar elements of Œ≤r are mutually independent,
one can adopt
Œ≤r ‚àºN

0, Ir106
.
(14.9)
Assuming prior independence between Œ≤h and Œ≤r, the prior distribution of
Œ≤ has density
p (Œ≤) = p

Œ≤h|Œ≤, œÉ2
Œ≤h

p (Œ≤r) .
(14.10)
The scalar Œ≤ can be assumed to follow the uniform distribution
Œ≤|Œ≤min, Œ≤max ‚àºUn (Œ≤min, Œ≤max)
(14.11)
where Œ≤min and Œ≤max are chosen appropriately. The prior distribution for
œÉ2
Œ≤h can be a scaled inverted chi-square process with known parameters ŒΩŒ≤
and SŒ≤,with density
p

œÉ2
Œ≤h|ŒΩŒ≤h, SŒ≤h

‚àù

œÉ2
Œ≤h
‚àí
 ŒΩŒ≤h
2
+1

exp

‚àíŒΩŒ≤hSŒ≤h
2œÉ2
Œ≤h

.
(14.12)
The prior for the thresholds arises naturally from the assumptions of
the model. This model postulates that the thresholds are ordered, so it
is sensible to assume that these are distributed as order statistics from a
uniform distribution, in the interval [tmin, tmax]. Also, recall that t1 = 0, to
give an origin to the underlying distribution; hence, there are c‚àí2 unknown
thresholds. Therefore, the joint prior density of t is(Mood et al., 1974):
p (t) = (c ‚àí2)!

1
tmin ‚àítmax
c‚àí2
I (t ‚ààT) ,
(14.13)
where T = {(t1 = 0, t2, . . . , tc‚àí1) |tmin < t1 < t2 < ¬∑ ¬∑ ¬∑ < tc‚àí1 < tmax}.
If, as stated earlier, the location vector a consists of additive genetic
eÔ¨Äects (meaning that genetic variation is due to additive and independent
contributions from a large number of loci with small gene substitution
eÔ¨Äects), a sensible prior is
a|A, œÉ2
a ‚àºN

0, AœÉ2
a

,
(14.14)

14.2 Analysis of a Single Polychotomous Trait
611
where œÉ2
a is the additive genetic variance and A is the usual additive rela-
tionship matrix. In turn, the additive genetic variance can be conveniently
assumed to be distributed, a priori, as a scaled inverted chi-square random
variable with density
p

œÉ2
a|ŒΩa, Sa

‚àù

œÉ2
a
‚àí(
ŒΩa
2 +1) exp

‚àíŒΩaSa
2œÉ2a

,
(14.15)
and ŒΩa, Sa are known hyperparameters.
The Ô¨Ånal assumption is that the joint prior density of all unknown pa-
rameters, including the liabilities, can be factorized as
p

l, Œ≤, Œ≤, a, t, œÉ2
a, œÉ2
Œ≤h

= p (l|Œ≤, a) p

Œ≤h|Œ≤, œÉ2
Œ≤h

p (Œ≤r) p

a|œÉ2
a

p (t) p

œÉ2
a

p

œÉ2
Œ≤h

,
where the dependency on the hyperparameters is suppressed in the nota-
tion. In view of this and of (14.5), the joint posterior density can be written
as
p

l, Œ≤, Œ≤, a, t, œÉ2
a, œÉ2
Œ≤h|y

‚àùp (y|l, t) p (t)
√ó
 n
-
i=1
p (li|Œ≤, a)

p

Œ≤h|Œ≤, œÉ2
Œ≤h

p (Œ≤r) p

a|œÉ2
a

p

œÉ2
a

p

œÉ2
Œ≤h

.
(14.16)
14.2.3
Fully Conditional Posterior Distributions
Liabilities
Notationally, the fully conditional posterior distribution of parameter x
will be represented as p (x|ELSE), where ELSE refers to data y and to
the values of all the parameters that x depends on. Consider Ô¨Årst the fully
conditional posterior distribution of liability li. In order to obtain this,
one must extract the terms involving li from the joint posterior (14.16).
Since the liabilities are conditionally independent, it follows that, given the
parameters and the liabilities, the polychotomous observations are inde-
pendent as well. This yields
p (li|ELSE) ‚àùp (yi = j|li, t) p (li|Œ≤, a) .
(14.17)
Given the liabilities and the thresholds, the categorical outcome is not
stochastic, since its value is known with certainty. Hence, p (yi = j|li, t) is
a constant and gets absorbed in the Bayes formula. Therefore,
p (li|ELSE) ‚àù
 c

i=1
I (tj‚àí1 < li ‚â§tj) I (yi = j) |Œ≤, a

,

612
14. Threshold Models for Categorical Responses
where I (tj‚àí1 < li ‚â§tj) I (yi = j) indicates that liability falls in the interval
tj‚àí1 < li ‚â§tj if yi = j. Since liability is Gaussian, it follows that this is
the density of a truncated normal distribution, with density
p (li|ELSE) =
œÜ

x‚Ä≤
iŒ≤ + z‚Ä≤
ia,1

Œ¶

tj ‚àíx‚Ä≤
iŒ≤ ‚àíz‚Ä≤
ia

‚àíŒ¶

tj‚àí1 ‚àíx‚Ä≤
iŒ≤ ‚àíz‚Ä≤
ia
.
(14.18)
Thresholds
The density of the fully conditional posterior distribution of the ith thresh-
old ti, is
p (ti|ELSE) ‚àùp (y|l, t) p (t)
‚àù
N
-
j=1
[I (ti‚àí1 < lj < ti) I (yj = i) + I (ti < lj < ti+1) I (yj = i + 1)] .
(14.19)
The preceding is the collection of all terms in the joint posterior density
where ti appears. For example, consider threshold t2. This will appear
either in connection with liabilities corresponding to responses in either
the second category (where the threshold is an upper bound) or in the
third class (where the threshold is a lower bound). Seen as a function of ti,
(14.19) shows formally that ti lies in an interval whose limits are as follows:
the upper limit must be smaller than or equal to the smallest value of l for
which yj = i + 1. The lower limit is given by the maximum value of l for
which yj = i. The prior condition (t ‚ààT) is fulÔ¨Ålled automatically. Within
these boundaries, the conditional posterior distribution of threshold ti is
the uniform process
p (ti|ELSE) =
1
min (l|y = i + 1) ‚àímax (l|y = i),
(14.20)
where min (l|y = i + 1) indicates the minimum value of the liabilities within
observations in category i+1; similarly, max (l|y = i) denotes the maximum
value of liabilities for observations in category i.
A comment about implementation is in order here. The interval
min (l|y = i + 1) ‚àímax (l|y = i)
is typically very narrow, and varies little between successive iterates of the
Gibbs sampler. This leads to strong autocorrelations between samples and
slows convergence. Cowles (1996) and Nandram and Chen (1996) propose
alternative algorithms for ameliorating this diÔ¨Éculty.
Additive Genetic Variance
The density of the fully conditional posterior distribution of œÉ2
a is
p

œÉ2
a|ELSE

‚àùp

a|œÉ2
a

p

œÉ2
a

.

14.2 Analysis of a Single Polychotomous Trait
613
This is identical to the expression for the density of the conditional poste-
rior distribution of œÉ2
a in a single-trait additive genetic model presented in
Section 13.2. Therefore,
œÉ2
a|ELSE‚àº

a‚Ä≤A‚àí1a + ŒΩaSa

œá‚àí2
ŒΩa+q.
(14.21)
Dispersion Parameter œÉ2
Œ≤h
Recall that œÉ2
Œ≤h is a dispersion parameter describing variability between
levels of eÔ¨Äects containing (possibly) ECPs. The density of the fully condi-
tional posterior distribution of œÉ2
Œ≤h is
p

œÉ2
Œ≤h|ELSE

‚àùp

Œ≤h|Œ≤, œÉ2
Œ≤h

p

œÉ2
Œ≤h

.
This is clearly in the same form as the density of the conditional distribution
of the additive genetic variance, the diÔ¨Äerence being that the prior mean
of each of the elements of Œ≤h is Œ≤, instead of 0. After making an oÔ¨Äset
for the non-null mean, some algebra leads to the scaled inverted chi-square
distribution
œÉ2
Œ≤h|ELSE ‚àº

(Œ≤h ‚àí1Œ≤)‚Ä≤ (Œ≤h ‚àí1Œ≤) + ŒΩŒ≤SŒ≤

œá‚àí2
ŒΩŒ≤h+H,
(14.22)
where H is the number of elements in Œ≤h.
Location EÔ¨Äects
We consider Ô¨Årst the conditional distribution of [Œ≤, a] and, subsequently,
that of the scalar parameter Œ≤. As usual, the fully conditional posterior dis-
tribution of [Œ≤, a] is obtained from (14.16). Extracting the terms containing
Œ≤, and a one obtains
p

Œ≤, a|œÉ2
Œ≤h, œÉ2
a, Œ≤, l, y

‚àù
 n
-
i=1
p (li|Œ≤, a)

p

Œ≤h|Œ≤, œÉ2
Œ≤h

p (Œ≤r) p

a|œÉ2
a

‚àùp

Œ≤, a|œÉ2
Œ≤h, œÉ2
a, Œ≤, l

.
This follows because, given the liabilities, the categorical responses y do not
bring any additional information about the location eÔ¨Äects. The preceding
expression has a form similar to (13.8), except that l here, replaces y. This
is precisely the density of the joint posterior distribution of the location
eÔ¨Äects in a Gaussian hierarchical model, which was discussed extensively
in Chapter 6. Using the result in Example 1.18 of Chapter 1, the fully
conditional posterior distributions follow rather directly. First

Œ≤h|Œ≤r, a, œÉ2
a, œÉ2
Œ≤h, Œ≤, l, y

‚àºN
Ô£´
Ô£≠5Œ≤h,

X‚Ä≤
hXh +
1
œÉ2
Œ≤h
Ih
‚àí1Ô£∂
Ô£∏,
(14.23)

614
14. Threshold Models for Categorical Responses
where
5Œ≤h =

X‚Ä≤
hXh +
1
œÉ2
Œ≤h
Ih
‚àí1 
X‚Ä≤
h (l ‚àíXrŒ≤r ‚àíZa) + 1 Œ≤
œÉ2
Œ≤h
.

Further,

Œ≤r|Œ≤h, a, œÉ2
a, œÉ2
Œ≤h, Œ≤, l, y

‚àºN

5Œ≤r,

X‚Ä≤
rXr + 10‚àí6Ir
‚àí1
,
(14.24)
where
5Œ≤r =

X‚Ä≤
rXr +
1
106 Ir
‚àí1
X‚Ä≤
r (l ‚àíXhŒ≤h ‚àíZa) .
Likewise, for the additive genetic eÔ¨Äects,

a|Œ≤h, Œ≤r, œÉ2
a, œÉ2
Œ≤h, Œ≤, l, y

‚àºN

5a,

Z‚Ä≤Z + 1
œÉ2a
A‚àí1
‚àí1
(14.25)
where
5a =

Z‚Ä≤Z + 1
œÉ2a
A‚àí1
‚àí1
Z‚Ä≤ (l ‚àíXrŒ≤r ‚àíXhŒ≤h) .
Using these expressions, a single site updating Gibbs sampler can be de-
veloped as for the Gaussian hierarchical model. Alternatively (and perhaps
more eÔ¨Éciently from a computational point of view), a joint updating algo-
rithm can be chosen along the lines described in Section 13.5 of the previous
chapter.
Finally, the density of the fully conditional posterior distribution of the
scalar Œ≤ is:
p

Œ≤|Œ≤, a,œÉ2
Œ≤h, œÉ2
a, t, l, y

‚àùp

Œ≤h|Œ≤, œÉ2
Œ≤h

‚àùexp

‚àí
1
2œÉ2
Œ≤h
(Œ≤h ‚àí1Œ≤)‚Ä≤ (Œ≤h ‚àí1Œ≤)

.
Viewed as a function of Œ≤, this is the density of the normal distribution

Œ≤|Œ≤, a, œÉ2
Œ≤h, œÉ2
a, t, l, y

‚àºN

Œ≤h,
œÉ2
Œ≤h
H

,
(14.26)
where
Œ≤h = 1
H
H

i=1
Œ≤hi
and Œ≤hi is the ith element of Œ≤h, drawn from (14.23).

14.3 Analysis of a Categorical and a Gaussian Trait
615
14.2.4
The Gibbs Sampler
To summarize, the Gibbs sampler consists of iterating through the following
loop:
1. Read through the data Ô¨Åle and sample the liabilities l from the trun-
cated normal distribution with density (14.18).
2. Sample the thresholds from the uniform distribution (14.20).
3. Sample the additive genetic variance from the scaled inverted chi-
square process (14.21).
4. Sample œÉ2
Œ≤h from the scaled inverted chi-square distribution (14.22).
5. Build mixed model equations and their right-hand sides using l as
‚Äúdata‚Äù.
6. Sample the location parameters from the normal distributions (14.23),
(14.24) and 14.25).
7. Sample Œ≤ from the normal distribution (14.26)
8. Return to Step 1 or terminate when chain length is adequate to meet
convergence diagnostics.
14.3
Joint Analysis of an Ordered Categorical
and a Normally Distributed Trait
The results presented in the previous section are extended for a joint analy-
sis of a model for one categorical and one normally distributed trait. Such a
model could be relevant to study the genetic associations between growth
rate and leg weakness in pigs, for example, since the last trait is scored
categorically. The setting is as in Foulley et al. (1983), who introduced
the model and suggested an approximate Bayesian analysis. The approach
presented here can accommodate a general pattern of missing data. For
the purpose of presentation, a simple additive genetic model is postulated
for each of the two traits. Bayesian MCMC related work can be found in
Jensen (1994), Sorensen (1996), and in Wang et al. (1997). A more gen-
eral model that encompasses several categorical and Gaussian traits was
described by Van Tassell et al. (1998). Korsgaard et al. (2002) proposed
a model for the joint analysis of categorical, censored and Gaussian traits
using the Gibbs sampler.

616
14. Threshold Models for Categorical Responses
14.3.1
Sampling Model
Suppose there are n individuals, each of which is potentially measured for
each of the two traits. However, it is typical that there will be at least
some individuals on which the measurement is available for one of the
traits only. Subscript 1 will refer to the continuous trait, and subscript 2 to
the categorical trait. Denote by y1o and by y2o the vectors of the observed
data for the continuous and categorical trait, respectively, and by y1m and
by y2m, the vectors of the missing data for the continuous and categorical
trait, respectively. Let y‚Ä≤
1 = (y‚Ä≤
1o, y‚Ä≤
1m) and y‚Ä≤
2 = (y‚Ä≤
2o, y‚Ä≤
2m) be vectors of
dimension n each. As in Section 13.4, it is assumed that data are missing
at random. Also, as before, let l (of order n √ó 1), represent the unobserved
liabilities associated with the categorical trait, which can be partitioned in
an obvious notation as l‚Ä≤ = (l‚Ä≤
o, l‚Ä≤
m).
The approach followed here is to augment the posterior distribution with
(y‚Ä≤
1m, l‚Ä≤) , that is, with the missing data for the continuous trait and all
liabilities. It is assumed that the vector of complete continuous data, which
is deÔ¨Åned here as (y‚Ä≤
1, l‚Ä≤) = (y‚Ä≤
1o, y‚Ä≤
1m, l‚Ä≤), is normally distributed, given
vectors of location parameters Œ≤ =

Œ≤‚Ä≤
1, Œ≤‚Ä≤
2
‚Ä≤ and a = (a‚Ä≤
1, a‚Ä≤
2)‚Ä≤ , with the
latter being the additive genetic values for the two traits. The order of a1
and a2 is q √ó 1 each. If the complete continuous data are sorted by trait
and by individual within trait, with the resulting vector labeled as v, the
conditional distribution of the complete continuous data given the location
parameters has the form
v|Œ≤, a, Re ‚àºN


X1Œ≤1 + Z1a1
X2Œ≤2 + Z2a2

, R

,
(14.27)
where the X‚Ä≤s and Z‚Ä≤s are incidence matrices of appropriate order. In
(14.27), R = Re ‚äóIn, Re is the 2 √ó 2 variance‚Äìcovariance matrix
Re =

œÉ2
e1
œÉe1,2
œÉe1,2
œÉ2
e2

,
(14.28)
and In is an n √ó n identity matrix. As in Section 13.4, the data can be
augmented with residuals as in Wang et al. (1997), in which case the ap-
propriate rows of incidence matrices X and Z in (14.27) have all elements
equal to zero.
Following the results in Example 1.17 of Chapter 1, the density associated
with (14.27) can be written as
p (v|Œ≤, a, Re) ‚àù|Re|‚àín
2 exp

‚àí1
2tr

R‚àí1
e Se

.
(14.29)
Here
Se =

e‚Ä≤
1e1
e‚Ä≤
1e2
e‚Ä≤
2e1
e‚Ä≤
2e2


14.3 Analysis of a Categorical and a Gaussian Trait
617
is a matrix of sums of squares and products involving the residuals
e1 = y1 ‚àíX1Œ≤1‚àíZ1a1,
e2 = l ‚àíX2Œ≤2‚àíZ2a2.
14.3.2
Prior Distribution and Joint Posterior Density
Based on the assumptions of the inÔ¨Ånitesimal model , the prior distribution
[a1, a2|A, G0], is taken to be the multivariate normal process

a1
a2
"""" A, G0 ‚àºN


0
0

, G0 ‚äóA

,
(14.30)
where
G0 =

œÉ2
a1
œÉa12
œÉa12
œÉ2
a2

is the additive genetic (co)variance matrix between the two traits, and A
is the q √óq additive relationship matrix between members of the genealogy
(recall that, typically, q > n) .
The treatment of the vector Œ≤ requires extending the developments for
dealing with potential ECPs for the polychotomous trait to a bivariate
situation. We adopt the partition
Œ≤ =
Ô£Æ
Ô£ØÔ£ØÔ£∞
Œ≤1h
Œ≤1r
Œ≤2h
Œ≤2r
Ô£π
Ô£∫Ô£∫Ô£ª,
where Œ≤2h are location eÔ¨Äects on liabilities whose levels have (possibly)
ECPs; the vector Œ≤1h contains the eÔ¨Äects of these levels on the Gaussian
trait. Subsequently, it is assumed that the prior distribution of Œ≤ is such
that its density factorizes as
p (Œ≤) = p (Œ≤1h, Œ≤2h) p (Œ≤1r) p (Œ≤2r) ,
where
p (Œ≤1r) ‚àùconstant
if Œ≤1r,min < Œ≤1r< Œ≤1r,max, and
Œ≤2r ‚àºN

0, I2r106
.
Further,
 Œ≤1h
Œ≤2h
"""" Œ≤, Bh ‚àºN


0
1Œ≤

, Bh ‚äóI

,
(14.31)

618
14. Threshold Models for Categorical Responses
where
Bh =

œÉ2
Œ≤1,h
œÉŒ≤12,h
œÉŒ≤12,h
œÉ2
Œ≤2,h

.
Here œÉ2
Œ≤1,h and œÉ2
Œ≤2,h are variance components and œÉŒ≤12,h is the covariance
between the Œ≤1h and Œ≤2h eÔ¨Äects on the two traits. As in the previous
section, the scalar parameter Œ≤ is assigned the uniform prior distribution
Œ≤|Œ≤min, Œ≤max ‚àºUn (Œ≤|Œ≤min, Œ≤max) ,
(14.32)
where Œ≤min and Œ≤max are appropriately chosen hyperparameters.
It is assumed that the matrices Re, G0, and Bh follow independent
scaled-inverted Wishart distributions, a priori. The respective densities are
p (Re|œÖe, Ve) ‚àù|Re|‚àí1
2 (œÖe+3) exp

‚àí1
2 tr

R‚àí1
e V‚àí1
e

,
(14.33)
p (G0|œÖ0, V0) ‚àù|G0|‚àí1
2 (œÖ0+3) exp

‚àí1
2 tr

G‚àí1
0 V‚àí1
0

,
(14.34)
and
p (Bh|œÖh, Vh) ‚àù|Bh|‚àí1
2 (œÖh+3) exp

‚àí1
2 tr

B‚àí1
h V‚àí1
h

,
where œÖi and Vi (i = e, 0, h), are the usual parameters of the scaled in-
verted Wishart distributions. An important point: although the liabilities
are (conditionally) Gaussian, the fact that the responses are categorical
imposes some conditions on the form of the inverse Wishart distribution
with density as in (14.33). We shall return to this issue later on.
The unknown thresholds in t delimiting the c categories of response, are
assumed to be distributed a priori as ordered statistics from a uniform
distribution in the interval [tmin, tmax], as in (14.13).
The parameter vector is augmented with the missing data for the con-
tinuous trait (y1m) and with the unobserved liabilities l. The parameters
of the augmented model are represented as (‚Ñ¶, y1m, l) ,where
‚Ñ¶= (Œ≤, a, G0, Re, Œ≤, Bh, t) .
Before embarking on the derivation of the fully conditional posterior
distributions, we focus on the conditional distribution [y2o|lo, ‚Ñ¶] of the
observed categorical responses, given their liabilities and ‚Ñ¶. Recall that,
given the liabilities li and the thresholds, the categorical responses y2o
are known with certainty. This means that given (‚Ñ¶, y1m, l) , that is, the
parameters of the augmented model, the observations y1o (y1m) and y2o
are conditionally independent. Ignoring hyperparameters in the notation,
the joint posterior density of all uncertain variables in the augmented model

14.3 Analysis of a Categorical and a Gaussian Trait
619
is
p (‚Ñ¶, y1m, l|y1o, y2o) ‚àùp (y1o, y2o|‚Ñ¶, y1m, l) p (‚Ñ¶, y1m, l)
‚àùp (y2o|y1o, ‚Ñ¶, y1m, l) p (y1o|‚Ñ¶, y1m, l) p (‚Ñ¶, y1m, l)
‚àùp (y2o|y1, ‚Ñ¶, l) p (y1, l|‚Ñ¶) p (‚Ñ¶)
‚àùp (y1, l|‚Ñ¶) p (y2o|l, ‚Ñ¶) p (‚Ñ¶) .
(14.35)
The last line follows from the conditional independence of y1 and y2o,
given l and t. The Ô¨Årst term on the right-hand side term of (14.35) is the
density of the sampling model for the complete continuous data, as given
in (14.29). The second term is not stochastic (given l and t one knows the
categories of response with certainty), so it gets absorbed as a constant in
Bayes theorem. The third term is the density of the joint prior distribution
of the parameters, which is assumed to factorize as
p (‚Ñ¶) ‚àùp (Œ≤1h, Œ≤2h|Œ≤, Bh) p (Œ≤1r) p (Œ≤2r) p (a|G0) p (G0) p (Re) p (Bh) p (t) .
(14.36)
Although the conditioning on ‚Ñ¶will be kept to simplify notation, note
that in (14.35)
p (y1, l|‚Ñ¶) = p (y1, l|Œ≤, a, Re)
and
p (y2o|l, ‚Ñ¶) = p (y2o|l, t) .
Consider now the prior distribution of the variance‚Äìcovariance matrix
of the sampling model. If Re is assigned an inverted Wishart distribution,
then œÉ2
e2 (the residual variance of the liability) should be stochastic, so it
cannot be Ô¨Åxed at some value. Treating œÉ2
e2 as a random variable requires
parameterizing the model such that two thresholds, instead of only one,
are given arbitrary values. Again, these must satisfy tmin < t1 < t2 < ¬∑ ¬∑ ¬∑ <
tc‚àí1 < tmax (Sorensen et al., 1995). Typical choices are t1 = 0 and t2 = 1.
However, this requires that the data fall into three or more categories of re-
sponse. If the data are binary, this parameterization is not possible. When
this is the case, a prior density p

Re|œÉ2
e2 = 1

in the form of a scaled in-
verted Wishart can be speciÔ¨Åed. It turns out that the fully conditional pos-
terior density p

Re|ELSE, œÉ2
e2 = 1, data

is also scaled inverted Wishart.
A simple way of drawing samples from this distribution is based on the
properties of the inverted Wishart distribution described in Subsubsection
1.4.6 of Chapter 1. The algorithm was presented by Korsgaard et al. (1999),
and is summarized at the end of this section.
14.3.3
Fully Conditional Posterior Distributions
As before the fully conditional posterior distribution of parameter x say,
is written as p (x|ELSE), where now y = (y1o, y2o). We start by deriving
the fully conditional posterior distribution of the missing data (y1m, l).

620
14. Threshold Models for Categorical Responses
Case
Observed data
Missing data
Generate
1
y1o,i, y2o,i
none
lo,i
2
y1o,i
trait 2
lm,i
3
y2o,i
trait 1
y1m,i, lo,i
TABLE 14.1. Possible patterns of observed and missing data.
Missing Continuous Data and Liabilities
From the sampling model in (14.27), it follows that a missing continuous
record for individual i, say, is sampled from
y1m,i|ELSE ‚àºN (E (y1m,i|ELSE) , V ar (y1m,i|ELSE)) .
Here,
E (y1m,i|ELSE) = x‚Ä≤
1,iŒ≤1 + z‚Ä≤
1,ia1 + œÉe1,2
œÉ2
e2
(li ‚àíx‚Ä≤
2iŒ≤2 ‚àíz‚Ä≤
2ia2)
(14.37)
and
V ar (y1m,i|ELSE) = œÉ2
e1

1 ‚àí(œÉe1,2)2
œÉ2
e1œÉ2
e2

,
(14.38)
where œÉ2
e2 = 1. Thus, the conditional distribution of a missing Gaussian
observation, given the liabilities, the observed data, and all parameters does
not depend on the observed data. In these expressions, x‚Ä≤
1i (x‚Ä≤
2i) and z‚Ä≤
1i
(z‚Ä≤
2i) are rows of matrices X1 (X2) and Z1 (Z2) associated with individual
i. In (14.37), if the joint posterior is augmented with the residuals (instead
of with the missing observations), elements of x‚Ä≤
1i and of z‚Ä≤
1i are all equal
to zero.
The next step involves drawing samples of the underlying vector l, not-
ing that all liabilities are conditionally independent. The possible patterns
of missing data for the ith record (i = 1, 2, . . . , n) are shown in Table 14.1.
In the table, subscripts 1o, i (2o, i) associated with y, represent the ob-
served continuous (categorical) record of the ith individual, and subscripts
1m, i (2m, i) represent the missing continuous (categorical) record of the
ith individual.
If the pattern of missing records is as in Case (1), both records on the
individual are available, so the fully conditional posterior distribution of
lo,i must be derived here. From the joint posterior density presented in
(14.35), and exploiting the conditional independence assumptions, the re-
quired conditional density is
p (lo,i|ELSE) ‚àùp (y1o,i, lo,i|‚Ñ¶) p (y2o,i|lo,i, ‚Ñ¶)
‚àùp (lo,i|y1o,i, ‚Ñ¶)
Ô£Æ
Ô£∞
c

j=1
I (tj‚àí1 < lo,i ‚â§tj) I (y2o,i = j)
Ô£π
Ô£ª.
(14.39)

14.3 Analysis of a Categorical and a Gaussian Trait
621
From (14.27), density (14.39) is recognized as that of a truncated condi-
tional normal distribution, with truncation points at tj‚àí1 and tj. The mean
of the untruncated distribution is
E (lo,i|y1o,i, ‚Ñ¶) = x‚Ä≤
2,iŒ≤2 + z‚Ä≤
2,ia2 + œÉe1,2
œÉ2
e1

y1o,i ‚àíx‚Ä≤
1,iŒ≤1 ‚àíz‚Ä≤
1,ia1

(14.40)
and the variance is
V ar (lo,i|y1o,i, ‚Ñ¶) = œÉ2
e2

1 ‚àí(œÉe1,2)2
œÉ2
e1œÉ2
e2

,
(14.41)
where œÉ2
e2 = 1.
If the pattern of missing data is as in Case (2), the observation for the
categorical trait is missing and lm,i must be generated. With y2o,i absent,
it follows from (14.35) that the density of the fully conditional posterior
distribution of lm,i is proportional to p (y1, l|‚Ñ¶). Therefore, recalling the
conditional independence structure,
p (lm,i|ELSE)
‚àù
p (y1o,i, lm,i|‚Ñ¶)
‚àù
p (lm,i|y1o,i, ‚Ñ¶) .
(14.42)
From (14.35), this is a conditional normal distribution with mean and vari-
ance given by (14.40) and (14.41), respectively. Thus,
lm,i|ELSE ‚àºN [E (lo,i|y1o,i, ‚Ñ¶) , V ar (lo,i|y1o,i, ‚Ñ¶)] .
(14.43)
Finally, if the pattern of missing data is as in Case (3), both y1m,i and
lo,i must be sampled. From (14.35) we can write
p (y1m,i, lo,i|ELSE) ‚àùp (y1m,i, lo,i|‚Ñ¶) p (y2o,i|lo,i, ‚Ñ¶)
= p (y1m,i, lo,i|‚Ñ¶)
c

j=1
I (tj‚àí1 < lo,i ‚â§tj) I (y2o,i = j)
= p (y1m,i|lo,i, ‚Ñ¶) p (lo,i|‚Ñ¶)
c

j=1
I (tj‚àí1 < lo,i ‚â§tj) I (y2o,i = j) .
(14.44)
A simple way of obtaining samples from the distribution with density
(14.44) is Ô¨Årst to sample a realized value l‚àó
o,i from the normal distribu-
tion [lo,i|‚Ñ¶] truncated at tj‚àí1 and at tj, and second, to sample y1m,i from
the conditional normal distribution

y1m,i|l‚àó
o,i, ‚Ñ¶

. Again, conditional inde-
pendence holds, so the process can be eÔ¨Äected piecewise, observation by
observation.
Location EÔ¨Äects
We derive now the fully conditional posterior distribution of Œ∏‚Ä≤ =

Œ≤‚Ä≤, a‚Ä≤
and, subsequently, that of the scalar Œ≤. From (14.35), and noting that
p (y2o|l, ‚Ñ¶) is not a function of Œ∏, we get

622
14. Threshold Models for Categorical Responses
p (Œ∏|ELSE) ‚àùp (y1, l|‚Ñ¶) p (‚Ñ¶)
= p (y1, l, Œ∏,Œ≤, G0, Re, Bh, t)
‚àùp (Œ∏|y1, l,Œ≤,G0, Re, Bh) .
(14.45)
This is the density of a bivariate Gaussian hierarchical model in which
the continuous responses and the liabilities (the complete continuous data)
are the observations. Hence, the conditional posterior distribution of the
location eÔ¨Äects Œ∏ is a multivariate normal process, with its mean vector and
variance‚Äìcovariance matrix calculated as seen in Chapter 13. The general
expression for the fully conditionals is very similar to equations (13.50),
(13.51), and (13.52), with a slight modiÔ¨Åcation in the interpretation of
some terms, to accommodate the diÔ¨Äerent model scenarios.
Consider the fully conditional posterior distributions of Œ≤ and a. Let
G‚àí1
0
=

g11
g12
g21
g22

,
R‚àí1
e
=

r11
r12
r21
r22

,
and
B‚àí1
h
=

b11
b12
b21
b22

.
Then
[Œ≤|ELSE] ‚àºN

5Œ≤, 5VŒ≤

,
(14.46)
where the mean vector has the following four partitions:
5Œ≤ =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞
5Œ≤1h
5Œ≤2h
5Œ≤1r
5Œ≤2r
Ô£π
Ô£∫Ô£∫Ô£∫Ô£ª.
An explicit representation of the mean vector is arrived at by writing the
incidence matrix of all elements of Œ≤ as
X =
 X1h
0
X1r
0
0
X2h
0
X2r

.
With this notation
5Œ≤ =

X‚Ä≤ 
R‚àí1
e
‚äóIn

X + P‚àí1‚àí1 
X‚Ä≤ 
R‚àí1
e
‚äóIn

(v ‚àíZa) + m

,
where
Za =
 Z1
0
0
Z2
  a1
a2

,

14.3 Analysis of a Categorical and a Gaussian Trait
623
P‚àí1 =
Ô£Æ
Ô£ØÔ£ØÔ£∞
b11I
b12I
0
0
b21I
b22I
0
0
0
0
0
0
0
0
0
10‚àí6I
Ô£π
Ô£∫Ô£∫Ô£ª,
and
m = P‚àí1
Ô£Æ
Ô£ØÔ£ØÔ£∞
0
IŒ≤
0
0
Ô£π
Ô£∫Ô£∫Ô£ª=
Ô£Æ
Ô£ØÔ£ØÔ£∞
b12Œ≤I
b22Œ≤I
0
0
Ô£π
Ô£∫Ô£∫Ô£ª.
The variance‚Äìcovariance matrix of the conditional posterior distribution in
(14.46) is
5VŒ≤ =

X‚Ä≤ 
R‚àí1
e
‚äóIn

X + P‚àí1‚àí1 .
The fully conditional posterior distribution of a is
[a|ELSE] ‚àºN

5a, 5Va

,
(14.47)
where
5a =

Z‚Ä≤ 
R‚àí1
e
‚äóIn

Z + G‚àí1
0
‚äóA‚àí1‚àí1 Z‚Ä≤ 
R‚àí1
e
‚äóIn

(v ‚àíXŒ≤) ,
and
5Va =

Z‚Ä≤ 
R‚àí1
e
‚äóIn

Z + G‚àí1
0
‚äóA‚àí1‚àí1 .
The location eÔ¨Äect remaining to be sampled is Œ≤. It follows from the joint
posterior density (14.35) and from the form of the prior in (14.36) that
p (Œ≤|ELSE) ‚àùp (Œ≤1h, Œ≤2h|Œ≤, Bh)
‚àùp

Œ≤1h|œÉ2
Œ≤1,h

p (Œ≤2h|Œ≤1h, Œ≤, Bh)
‚àùp (Œ≤2h|Œ≤1h, Œ≤, Bh) ,
which is multivariate normal, since [Œ≤1h, Œ≤2h|Œ≤, Bh] is multivariate normal.
Further, (14.31) indicates that the pairs

Œ≤1h,k, Œ≤2h,k

and

Œ≤1h,k‚Ä≤, Œ≤2h,k‚Ä≤

are mutually independent, a priori, where k = 1, 2, ..., H, denotes a level of
the factor possibly associated with ECP problems. Hence
p (Œ≤|ELSE) ‚àù
H
-
k=1
p

Œ≤2h,k|Œ≤1h,k, Œ≤, Bh

‚àù
H
-
k=1
exp
Ô£Æ
Ô£ØÔ£∞‚àí

Œ≤2h,k ‚àí¬µŒ≤2.1,k
2
2œÑ 2
Œ≤2.1
Ô£π
Ô£∫Ô£ª,
where
¬µŒ≤2.1,k = Œ≤ +
œÉŒ≤12,h
œÉ2
Œ≤1,h
Œ≤1h,k,

624
14. Threshold Models for Categorical Responses
and
œÑ 2
Œ≤2.1 = œÉ2
Œ≤2,h
Ô£´
Ô£¨
Ô£≠1 ‚àí

œÉŒ≤12,h
2
œÉ2
Œ≤1,hœÉ2
Œ≤2,h
Ô£∂
Ô£∑
Ô£∏.
Now, deÔ¨Åning the oÔ¨Äset Œ≤‚àó
2h,k = Œ≤2h,k ‚àí
œÉŒ≤12,h
œÉ2
Œ≤1,h Œ≤1h,k, the density of the
conditional posterior distribution of Œ≤ can be written as
p (Œ≤|ELSE) ‚àù
H
-
k=1
exp

‚àí

Œ≤‚àó
2h,k ‚àíŒ≤
2
2œÑ 2
Œ≤2.1

.
Viewed as a function of Œ≤, it follows that this is the density of a normal
distribution with mean Œ≤ =
1
H
H

k=1
Œ≤‚àó
2h,k and variance
1
H œÑ 2
Œ≤2.1, where H is
the number of levels of the factor with ECPs. In short,
Œ≤|ELSE ‚àºN
Ô£´
Ô£¨
Ô£¨
Ô£¨
Ô£≠
H

k=1
Œ≤‚àó
2h,k
H
,
œÉ2
Œ≤2,h
H
Ô£´
Ô£¨
Ô£≠1 ‚àí

œÉŒ≤12,h
2
œÉ2
Œ≤1,hœÉ2
Œ≤2,h
Ô£∂
Ô£∑
Ô£∏
Ô£∂
Ô£∑
Ô£∑
Ô£∑
Ô£∏.
(14.48)
Dispersion Parameters
The fully conditional posterior distributions of the covariance matrices are
derived by extracting the relevant terms from (14.35). In view of the form of
the prior in (14.36), all dispersion matrices are conditionally independent,
given all other parameters. Thus,
p (G0|ELSE) ‚àùp (‚Ñ¶)
‚àùp (a|G0) p (G0)
(14.49)
which is identical to (13.55). Therefore, the distribution is as in (13.56)
G0|ELSE ‚àºIW2

V‚àí1
a
+ Sa
‚àí1 , œÖa + q

.
(14.50)
Likewise,
p (Bh|ELSE) ‚àùp (Œ≤1h, Œ≤2h|Œ≤, Bh) p (Bh) .
(14.51)
This is the density of the inverse Wishart process
Bh|ELSE ‚àºIW2

V‚àí1
h
+ Sh
‚àí1 , œÖh + H

,
(14.52)
where
Sh =

Œ≤‚Ä≤
1hŒ≤1h
Œ≤‚Ä≤
1h (Œ≤2h ‚àí1Œ≤)
(Œ≤2h ‚àí1Œ≤)‚Ä≤ Œ≤1h
(Œ≤2h ‚àí1Œ≤)‚Ä≤ (Œ≤2h ‚àí1Œ≤)

.

14.3 Analysis of a Categorical and a Gaussian Trait
625
Similarly, for the residual covariance matrix,
p (Re|ELSE) ‚àùp (y1, l|‚Ñ¶) p (‚Ñ¶)
‚àùp (y1, l|Œ≤, a, Re) p (Re) .
This is identical to (13.53). The resulting fully conditional distribution is:
Re|ELSE‚àºIW2

Se + V‚àí1
e
‚àí1 , œÖe + n

(14.53)
with the term Se appropriately deÔ¨Åned.
Thresholds
The fully conditional posterior distribution of the jth unknown threshold
is obtained as follows. From (14.35)
p (tj|ELSE) ‚àùp (y2o|l, ‚Ñ¶) p (‚Ñ¶)
‚àù
n2o
-
i=1
[I (tj‚àí1 < lo,i < tj) I (y2o,i = j) + I (tj < lo,i < tj+1) I (y2o,i = j + 1)] ,
(14.54)
where n2o is the number of observed categorical records. This expression is
identical to (14.19).
14.3.4
The Gibbs Sampler
To summarize, a Gibbs sampler can be run as follows:
1. Read in the data Ô¨Åle and generate missing data (and liabilities) from
(14.39), (14.43) or (14.44), depending on the pattern of missing data.
2. Build the mixed model equations.
3. Sample Œ∏ from (14.45), with the distributions given explicitly in
(14.46) and (14.47). Recall that the samples can be drawn either
blockwise or piecewise, in which case the expression must be modi-
Ô¨Åed slightly.
4. Sample the scalar Œ≤ from (14.48).
5. Sample the covariance matrices from (14.50), (14.52) and (14.53).
6. Sample the thresholds from (14.54).
7. Go to Step 1 or exit if chain is long enough.
Like in the Gaussian multiple-trait case, note that the coeÔ¨Écient matrix
of the mixed model equations has to be recreated every iterate with the
strategy described above.

626
14. Threshold Models for Categorical Responses
14.3.5
Implementation with Binary Traits
As mentioned above, when there are only two categories of response the
residual covariance matrix can be speciÔ¨Åed as
Re =

œÉ2
e1
œÉe1,2
œÉe1,2
1

.
From a Bayesian perspective, this is a random matrix with two stochas-
tic elements

œÉ2
e1, œÉe1,2

instead of three. This is so because the residual
variance in the liability scale is set equal to 1, since the parameter is not
identiÔ¨Åable from the likelihood. In this situation, rather than adopting an
inverse Wishart prior for Re, one can assign a conditional inverse Wishart
prior (an inverse Wishart distribution, conditional on œÉ2
e2 = 1). The result-
ing fully conditional posterior distribution

Re|ELSE, œÉ2
e2 = 1, data

(14.55)
turns out to have the form of a conditional inverse Wishart distribution
also (conditional on œÉ2
e2 = 1). Korsgaard et al. (1999) have shown how to
draw samples from such a distribution. They described a simple algorithm,
where a more general formulation than the one described here can be found.
Based on the properties of the Wishart distribution presented in Subsection
1.4.6 of Chapter 1, the Gibbs sampler can proceed as follows. Implement
the algorithm in the manner described in the previous section, with the
exception of the draw involving (14.53), which is replaced by a draw from
(14.55). In order to obtain a realized value from the latter,
‚Ä¢ Sample X1 from (1.112).
‚Ä¢ Sample X2 from (1.113).
‚Ä¢ Construct T11 from (1.109), T12 from (1.110) and set T22 = X‚àí1
3
= 1
Then T11, T12 and T22 are the elements of the draw from
Re|ELSE, œÉ2
e2 = 1, data.
A Ô¨Ånal comment is in order. The threshold model is perhaps appealing
in quantitative genetics because all the theory available for additive in-
heritance carries on to the liability scale. The reader must be aware that
there are alternative methods of analysis of categorical responses; see, for
example, Fahrmeir and Tutz (2001) or Agresti (1989, 1990, 1996).

15
Bayesian Analysis of
Longitudinal Data
15.1
Introduction
Suppose individuals are sampled from a set of populations, with the latter
deÔ¨Åned in a statistical, rather than demographic, sense. In at least some of
the individuals, the trajectory of a trait is measured over a period of time,
collecting, thus, a time series of observations. Examples of such longitudinal
trajectories are: milk yield of a dairy cow at several points in the course of
lactation, body weight or feed intake measured repeatedly during some test
period in which animals grow, proliÔ¨Åcacy (litter size produced) of a sow in
the course of her lifetime, wool growth assessed at diÔ¨Äerent stages of the
development of a ewe, presence or absence of clinical mastitis in a dairy cow
in each bi-weekly period from calving until the end of lactation, the height
of a tree as it grows, etc. The trait monitored may be either ‚Äúcontinuous‚Äù
(e.g., milk yield or body weight), or a count (e.g., lambs per litter over
a series of litters), or binary (presence or absence of a disease at a given
time). Issues of interest may include inferring the expected trajectory of
the trait within an individual or assessing sources of variation, genetic and
nongenetic, among the trajectory patterns of groups of individuals. This
type of problem is fairly old in the study of animal systems. For example,
lactation curves and growth functions have been the subject of research for
decades in milk- and meat-producing species, respectively. Animals diÔ¨Äer
in their rate of growth or adult body weight, and it is known that there is
genetic variation for these features, both between and within breeds, that

628
15. Bayesian Analysis of Longitudinal Data
can be exploited in animal breeding programs. The question posed is: How
can this variation be assessed adequately?
In the animal and veterinary sciences, there has been renewed interest in
the analysis of longitudinal records of performance. Perhaps this is a con-
sequence of more intensive recording systems (for instance, in dairy cattle
production it is possible to monitor instantaneous milk Ô¨Çow) and of better
statistical methods for the analysis of longitudinal mixed eÔ¨Äects models.
In particular, linear random regression models (Laird and Ware, 1982) or
similar approaches have been applied in animal breeding, where there is
a large body of literature in connection with the analysis of ‚Äútest-day‚Äù
yields in dairy cattle (e.g., Kirkpatrick et al., 1994; Jamrozik and Scha-
eÔ¨Äer, 1997; Wiggans and Goddard, 1997). Similar applications have been
made in meat-producing species. For example, an assessment of growth in
beef cows from 19 to 119 months of age is in Meyer (1999).
In this chapter, the analysis of longitudinal data will be dealt with in a
parametric Bayesian framework, to illustrate the Ô¨Çexibility and attractive-
ness of the paradigm. First, a description is given of hierarchical or multi-
stage models for describing longitudinal observations, assuming Gaussian
processes throughout. Second, methods for an approximate Bayesian anal-
ysis of these models are described. These methods can be construed as
generalizations of the usual ‚Äútandem‚Äù employed for analysis of mixed ef-
fects linear models in animal breeding, consisting of BLUP (of random
eÔ¨Äects) plus likelihood-based procedures for parameter inference. However,
at least in theory, a Bayesian hierarchical probability model can yield exact
Ô¨Ånite sample inferences about all unknowns. Hence, a subsequent section
discusses an implementation based on MCMC procedures. We also discuss
some extensions, including alternative structures for the residual dispersion
of the process.
15.2
Hierarchical or Multistage Models
Envisage a setting where, in a randomly drawn sample, each individual is
measured longitudinally at several times. For example, suppose that male
and female rabbits from several breeds are weighed at several phases of their
development, from near birth to the adult stage. An objective might be to
study growth patterns of the two sexes in each of the breeds, while taking
into account interindividual variability. Typically, there will be variation in
the number of measurements per individual, leading at least to longitudinal
unbalancedness. In individuals with sparse information, the individual tra-
jectories would probably be estimated imprecisely, unless information from
relatives is abundant or prior information about the expected trajectory is
sharp.

15.2 Hierarchical or Multistage Models
629
A hierarchical or multistage model consists of a series of nested functional
speciÔ¨Åcations, together with the associated distributional assumptions. An
important paper introducing the basic ideas is that of Lindley and Smith
(1972). In the context of longitudinal data, at the Ô¨Årst stage of the model,
a mathematical function is used to describe the expected trajectory of
individuals, and a stochastic residual having some distribution reÔ¨Çects the
departure of the observations from such a trajectory. At the second stage, a
submodel is used to describe the interindividual variation of parameters of
the Ô¨Årst-stage speciÔ¨Åcation. A second-stage residual is included to reÔ¨Çect
the inability of the submodel to explain completely the variation of the
parameters. Additional stages can be imposed in a Bayesian context to
describe uncertainty about all other unknown parameters. We shall now
proceed to describe each of these stages systematically.
15.2.1
First Stage
The trajectory (body weights of the same individual, for example) will be
described with the parametric model
yi = fi (Œ∏i, ti) + Œµi,
i = 1, 2, . . . , M,
(15.1)
where yi = {yij} (i = 1, 2, . . . , M, j = 1, 2, . . . , ni) is an ni √ó 1 vector of
records on the trajectory of individual i; fi (Œ∏i, ti) is its expected trajectory
(e.g., expected growth curve) given a vector of animal-speciÔ¨Åc parameters
Œ∏i of order r √ó 1, and ti is an ni √ó 1 vector of known times of measure-
ment. In (15.1), the ni √ó 1 residual vector Œµi represents the inability of the
function fi (Œ∏i, ti) of reproducing the observed body weights yi exactly. An
observation on individual i at time j is then
yij = fij (Œ∏i, tij) + Œµij,
(15.2)
so the parameters Œ∏i dictate the form of the expected trajectory of indi-
vidual i. For example, when describing animal growth, use is often made of
what is called a Gompertz growth function (e.g., Blasco and Varona, 1999).
Here r = 3; one of the parameters represents adult or asymptotic weight
(i.e., body weight as time goes to inÔ¨Ånity), the second parameter is related
to growth rate, and the third parameter bears an interpretation in terms
of the ‚Äúinitial conditions‚Äù of growth. Often, animals that are measured at
a given time are clustered in diÔ¨Äerent contemporary groups. For example,
for some dairy cows the yield may be recorded in February, while for other
cows it may be recorded in December. In this situation, the model can be
written in a slightly more general form as
yijk = Gk + fij (Œ∏i, tij) + Œµijk,
where Gk is an eÔ¨Äect peculiar to all measurements taken on individuals
belonging to group k (k = 1, 2, ..., K) , e.g., month‚Äìyear at which milk yield

630
15. Bayesian Analysis of Longitudinal Data
at time tij is measured on cow i. Since the group eÔ¨Äect enters linearly in
the model, it can be dealt with in a straightforward manner. For example,
in the context of a Gibbs sampler, after Gk is drawn, one would form the
oÔ¨Äset yijk ‚àíGk, with the conditional model thus being again in the form
of speciÔ¨Åcation (15.2). Hence, the simpler model suÔ¨Éces for descriptive
purposes without great loss of generality.
The relationship between observed body weights and parameters may
be linear or nonlinear, the latter being the case in the Gompertz function.
In a linear speciÔ¨Åcation, the derivatives of the model with respect to the
parameters do not depend on Œ∏i. This can be stated as
‚àÇfij (Œ∏i, tij)
‚àÇŒ∏i
= hij,
where hij is an r √ó 1 vector of constants not involving Œ∏i. On the other
hand, in a nonlinear model,
‚àÇfij (Œ∏i, tij)
‚àÇŒ∏i
= hij (Œ∏i) ,
indicating that the vector hij (Œ∏i) involves the parameters, although some
may enter linearly into the model.
Example 15.1
Quadratic trajectory
Suppose a longitudinal process can be described with the Ô¨Årst-stage model
yij = ai + bitij + cit2
ij + Œµij,
i = 1, 2, . . . , M, j = 1, 2, . . . , ni.
Here,
Œ∏i =
Ô£Æ
Ô£∞
ai
bi
ci
Ô£π
Ô£ª, and ti =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞
ti1
ti2
...
tini
Ô£π
Ô£∫Ô£∫Ô£∫Ô£ª.
In matrix form, the observations made on individual i can be written as
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞
yi1
yi2
...
yini
Ô£π
Ô£∫Ô£∫Ô£∫Ô£ª=
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞
1
ti1
t2
i1
1
ti2
t2
i2
...
...
...
1
tini
t2
ini
Ô£π
Ô£∫Ô£∫Ô£∫Ô£ª
Ô£Æ
Ô£∞
ai
bi
ci
Ô£π
Ô£ª+
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞
Œµi1
Œµi2
...
Œµini
Ô£π
Ô£∫Ô£∫Ô£∫Ô£ª,
so
fi (Œ∏i, ti) =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞
1
ti1
t2
i1
1
ti2
t2
i2
...
...
...
1
tini
t2
ini
Ô£π
Ô£∫Ô£∫Ô£∫Ô£ª
Ô£Æ
Ô£∞
ai
bi
ci
Ô£π
Ô£ª= HiŒ∏i,

15.2 Hierarchical or Multistage Models
631
where Hi is an incidence matrix. Then
‚àÇf ‚Ä≤
i (Œ∏i, ti)
‚àÇŒ∏i
= H‚Ä≤
i =
Ô£Æ
Ô£∞
1
1
¬∑ ¬∑ ¬∑
1
ti1
ti2
¬∑ ¬∑ ¬∑
tini
t2
i1
t2
i2
¬∑ ¬∑ ¬∑
t2
ini
Ô£π
Ô£ª
=

hi1
hi2
¬∑ ¬∑ ¬∑
hini

.
Since the matrix of derivatives (or incidence matrix) does not involve any
of the parameters, the model is linear.
‚ñ†
Example 15.2
Growth curve
Typically, animals grow at an increasing rate from birth to puberty, and
at a decreasing rate thereafter, until a mature or asymptotic weight is
reached. The resulting average growth curve (assuming a large group of
animals treated under similar conditions) is usually S-shaped. Suppose the
trajectory of the body weight of an individual from birth onward can be
described by the mathematical model
yij = Ai [1 ‚àíBi exp (‚àíKitij)]‚àí1 + Œµij,
where Ai, Bi, and Ki are parameters indicating some aspect of growth.
For example, for positive Ki, when tij ‚Üí‚àû, E (yij) ‚ÜíAi, which is inter-
pretable as adult body weight. Likewise, as tij ‚Üí0, E (yij) ‚ÜíAi/(1‚àíBi),
interpretable as birth weight. Hence, 1/(1 ‚àíBi) is the fraction of adult
body weight attained at birth and ‚àíBi/(1‚àíBi) represents the fraction yet
to be attained during the growth process. Then, by deÔ¨Ånition, Bi must be
a negative parameter and related to degree of maturity at birth (as Bi ‚Üí0
the animal is more mature at birth). The parameter Ki is a rate. Here, as
in Example 15.1, r = 3 and the derivatives of interest are
hij (Œ∏i) =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
‚àÇfij (Œ∏i, tij)
‚àÇAi
‚àÇfij (Œ∏i, tij)
‚àÇBi
‚àÇfij (Œ∏i, tij)
‚àÇKi
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
=
Ô£Æ
Ô£∞
[1 ‚àíBi exp (‚àíKitij)]‚àí1
Ai exp (‚àíKitij) [1 ‚àíBi exp (‚àíKitij)]‚àí2
‚àíAiBitij exp (‚àíKitij) [1 ‚àíBi exp (‚àíKitij)]‚àí2
Ô£π
Ô£ª.
Clearly the model is not linear, although the degree of nonlinearity varies
from parameter to parameter. For example, the partial gradient of the
model with respect to Ai involves only Bi and Ki, while the other two
partial gradients involve all three parameters.
‚ñ†

632
15. Bayesian Analysis of Longitudinal Data
Example 15.3
Inverse third-order polynomial
A third-order inverse polynomial has the functional form
yij =
1
Œ≤i0 + Œ≤i1tij + Œ≤i2t2
ij + Œ≤i3t3
ij + Œµij
,
where Œ∏‚Ä≤
i = [Œ≤i0, Œ≤i1, Œ≤i2, Œ≤i3] are parameters peculiar to individual i and
Œµij is a residual having a null expectation. The Ô¨Årst derivatives of the model
with respect to the parameters can be written as
‚àÇfij (Œ∏i, tij)
‚àÇŒ≤k
=
‚àíxijk

Œ≤i0 + Œ≤i1tij + Œ≤i2t2
ij + Œ≤i3t3
ij + Œµij
2 ,
xijk = tk
ij,
k = 0, 1, 2, 3.
The model, thus, is not linear in the parameters. Note, however, that if a
reciprocal transformation of the observations is made, the model becomes
linear since
zij = 1
yij
= Œ≤i0 + Œ≤i1tij + Œ≤i2t2
ij + Œ≤i3t3
ij + Œµij,
and the derivatives now do not involve any of the parameters. It is impor-
tant to observe, however, that if the original model had the error entering
as
yij =
1
Œ≤i0 + Œ≤i1tij + Œ≤i2t2
ij + Œ≤i3t3
ij
+ Œµij,
the model would be nonlinear, even after making a reciprocal transforma-
tion. The two models for yij are not the same, as distinct assumptions are
made about the errors. In the Ô¨Årst model, the errors are additive to the
expectation of the reciprocal (zij) of the random variable of interest. In the
second model, the errors are additive to the reciprocal of the expectation
of the reciprocal of yij.
‚ñ†
Returning to the Ô¨Årst-stage model, the entire vector of records can be
represented as
y = f (Œ∏, t) + Œµ,
(15.3)
where Œ∏ is the Mr √ó 1 vector of parameters of all individuals, t contains
times of measurement, and Œµ is the M
i=1 ni √ó 1 vector of residuals. Com-
monly, it is assumed that the Ô¨Årst-stage residuals are mutually independent
between individuals, but some dependence within trajectories may exist.
Hereinafter, given the parameters, the observations taken in diÔ¨Äerent in-
dividuals will be assumed to be conditionally independent of each other.
Possible dependencies, such as those resulting from genetic or environmen-
tal relatedness between individuals, can be introduced in the next stage of
the model. Assuming normality of the residuals (sometimes, a thick-tailed

15.2 Hierarchical or Multistage Models
633
distribution, such as Student-t, may be a more sensible speciÔ¨Åcation), the
density of the Ô¨Årst-stage distribution is expressible as
yi|Œ∏i, Œ≥ ‚àºN [fi (Œ∏i, ti) , Ri (Œ≥)] ,
i = 1, 2, . . . , M,
(15.4)
with yi being independent of yj, for all such pairs, conditionally on the
parameters and on Œ≥. In (15.4), Ri (Œ≥) is an ni √ó ni Ô¨Årst-stage variance‚Äì
covariance matrix, which depends on Œ≥, a vector of dispersion parame-
ters. For example, if residuals are independently and identically distributed
within individuals, then Ri (Œ≥) = IniŒ≥, where Œ≥ is the variance about the
expected trajectory, so Œ≥ would be a scalar parameter here.
The form of the matrix Ri (Œ≥) depends on the dispersion assumptions
made. Some possible alternatives to the preceding speciÔ¨Åcation are dis-
cussed below.
(1) Residuals may be independently distributed, but heteroscedastic across
individuals. Then
Ri (Œ≥) = IniŒ≥i,
i = 1, 2, . . . , M.
Here Œ≥‚Ä≤ = [Œ≥1, Œ≥2, . . . , Œ≥M] and Œ≥i is the variance about the trajectory of
individual i.
(2) The residuals may be heteroscedastic across times of measurement.
In this situation, there would be a Ô¨Årst-stage residual variance component
for each of the times at which the trajectory is evaluated (if this is done
at Ô¨Åxed times, as in experimental settings, for example, every three weeks
when measuring body weights in children).
(3) Perhaps the residuals are neither homoscedastic nor independently
distributed. For example, there may be a Ô¨Årst-order autoregressive process
with heterogeneous variance across the times at which measurements are
taken, such that
Cov

Œµit, Œµi(t+k)

= œÅkŒ≥t,
where œÅ is a Ô¨Årst-stage residual correlation and Œ≥t is the residual variance at
time t (t = 1, 2, . . . , T) . If the variance were homogeneous, this would make
observations taken adjacently in time to be more correlated than those
farther apart. Another speciÔ¨Åcation may be a Markov-type process, where
observations are dependent only if the measurement times are contiguous,
but not otherwise.
(4) A structural model may be entertained for the residual variance. For
example, Blasco and Varona (1999) used a Gompertz function to describe
body weight growth in rabbits, and proposed modeling the trajectory of the
residual standard deviation over time using the Gompertz function as well.
The parameters of this function were assumed to be homogeneous across
individuals. An even more ambitious model would consist of building up
a hierarchical model for the trajectory of the standard deviation, where
parameters vary according to some explanatory variables.

634
15. Bayesian Analysis of Longitudinal Data
The density of the conditional distribution in (15.4), over all individuals,
is then
p (y|Œ∏, Œ≥) ‚àù
M
-
i=1
|Ri (Œ≥)|‚àí1
2 exp

‚àí1
2Œµ‚Ä≤
iR‚àí1
i
(Œ≥) Œµi
%
,
(15.5)
where Œµi = yi‚àífi (Œ∏i, ti) , from (15.1). If individual parameters are inferred
via maximum likelihood from this Ô¨Årst stage model, unstable estimates may
be obtained, specially for subjects having few observations. If heterogeneity
between individuals is accounted for somehow using a submodel, perhaps
the total variation can be described in terms of fewer parameters.
15.2.2
Second Stage
The second stage of the model is a statement of how individual-speciÔ¨Åc
parameters are thought to vary according to explanatory factors, some of
these perhaps representing genetic sources of variation. In brief, as stated
earlier, the Ô¨Årst stage of the model delineates the expected trajectory that
longitudinal observations take within a given subject, while the second
stage accounts for cross-sectional (between-subject) heterogeneity of pa-
rameters of the trajectory.
In order to facilitate implementation, it is convenient to assume that the
second stage of the model is linear in the eÔ¨Äects of the explanatory variables.
However, at least in theory, there is no reason to preclude a nonlinear
speciÔ¨Åcation, particularly if this is dictated by mechanistic considerations.
Hereinafter, it will be assumed that the trajectory parameters are described
suitably by the linear model
Œ∏i = XiŒ≤ + ui + ei,
i = 1, 2, . . . , M.
(15.6)
Above, the vector Œ≤ represents the eÔ¨Äects of p explanatory variables con-
tained in the r√óp matrix Xi (without loss of generality, this matrix will be
assumed to have full-column rank), ui are subject-speciÔ¨Åc eÔ¨Äects on each
of the r parameters, and ei is a vector of second-stage residuals. Similar to
the errors in the Ô¨Årst stage, these residuals represent discrepancies between
the second-stage explanatory structure XiŒ≤ + ui and the ‚Äútrue values‚Äù Œ∏i.
In animal breeding applications, for example, the vector ui may be addi-
tive genetic eÔ¨Äects on trajectory parameters, and these may or may not be
identiÔ¨Åable separately from the residual vector ei, depending on the genetic
relationship structure. For example, suppose the Œ∏‚Ä≤s represent parameters
of a three-coeÔ¨Écient lactation curve for each of 100 cows, and that such
cows are progeny of a set of 20 sires, each having 5 daughters. In this case,
one may wish to employ a double subscript in the notation and let Œ∏ij
be the coeÔ¨Écients for daughter j of sire i. Here ui might be a 3 √ó 1 vec-
tor of ‚Äúsire eÔ¨Äects‚Äù, common to all progeny of sire i (i = 1, 2, . . . , 20) and

15.2 Hierarchical or Multistage Models
635
the second-stage residual vector would be eij, representing the discrepancy
Œ∏ij ‚àíXijŒ≤‚àíui peculiar to cow ij. Parameters of the distributions of ui and
eij are identiÔ¨Åable, as is the case in a standard analysis of variance, even
if the Œ∏ij are not observable.
The second-stage distributional assumptions pertain to the uncertainty
induced by the presence of ei in model (15.6), given Œ≤ and ui. It is often
convenient to postulate that
Œ∏i|Œ≤, ui, Œ£e ‚àºN (XiŒ≤ + ui, Œ£e) ,
(15.7)
implying that
ei|Œ£e ‚àºN (0, Œ£e) ,
where the second stage variance‚Äìcovariance matrix has the form
Œ£e =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞
œÉ2
1
œÉ12
¬∑ ¬∑ ¬∑
œÉ1r
œÉ21
œÉ2
2
¬∑ ¬∑ ¬∑
œÉ2r
...
...
...
...
œÉr1
œÉr2
¬∑ ¬∑ ¬∑
œÉ2
r
Ô£π
Ô£∫Ô£∫Ô£∫Ô£ª.
(15.8)
Here the diagonal elements are the variances of the second-stage residuals
and the oÔ¨Ä-diagonals are corresponding covariances; for example, œÉ2
r is the
second-stage variance of parameter r and œÉr‚àí1,r is the second-stage covari-
ance between parameters r ‚àí1 and r. In some instances, one may wish to
assign a thick-tailed or robust distribution to the residuals, e.g., an r-variate
t distribution. In this situation, one would write ei|ŒΩe, Œ£e ‚àºtr (0, Œ£e, ŒΩe) to
denote a t distribution of dimension r, having a null mean vector, variance‚Äì
covariance Œ£e, and degrees of freedom ŒΩe. It must be noted that in a
multivariate-t distribution, Œ£e =
ŒΩe
ŒΩe‚àí2Se, where Se is a scale matrix, so
ŒΩe > 2 is a necessary condition for the existence of the variance‚Äìcovariance
matrix (Zellner, 1971). An implementation based on the t distribution will
be discussed later.
Often, it is assumed that second-stage residuals are mutually indepen-
dent across individuals. Then the joint density of all parameters at the
second stage can be expressed as
p (Œ∏1, Œ∏2, . . . , Œ∏M|Œ≤, u1, u2, . . . , uM, Œ£e) =
M
-
i=1
p (Œ∏i|Œ≤, ui, Œ£e) .
(15.9)

636
15. Bayesian Analysis of Longitudinal Data
Put Œ∏ =

Œ∏‚Ä≤
1, Œ∏‚Ä≤
2, . . . , Œ∏‚Ä≤
M
‚Ä≤ and u = [u‚Ä≤
1, u‚Ä≤
2, . . . , u‚Ä≤
M]‚Ä≤ . Under the normality
assumption made in (15.7), the preceding takes the form
p (Œ∏|Œ≤, u, Œ£e) ‚àù|Œ£e|‚àíM
2 exp

‚àí1
2
M

i=1
e‚Ä≤
iŒ£‚àí1
e ei

‚àù|Œ£e|‚àíM
2 exp

‚àí1
2 tr
M

i=1
e‚Ä≤
iŒ£‚àí1
e ei

‚àù|Œ£e|‚àíM
2 exp

‚àí1
2 tr

Œ£‚àí1
e B

,
(15.10)
where ei = Œ∏i ‚àíXiŒ≤ ‚àíui, as before, and B = M
i=1 eie‚Ä≤
i is an r √ó r matrix.
The diagonal elements of this matrix contain the sum of squared deviations
of the appropriate parameter from their second-stage conditional expecta-
tions; the oÔ¨Ä-diagonals are sums of products of such parameter deviations.
It must be noted that in many models, motivated by mechanistic consider-
ations about growth and lactation, the values of the trajectory parameters
must be deÔ¨Åned within a restricted range. For example, it is clear that adult
body weight or total milk yield produced cannot be negative. Such restric-
tions are easy to incorporate in the model via an appropriate deÔ¨Ånition of
the parameter space. This issue will be discussed further later.
It is useful to note that if all parameters are concatenated vertically, the
second stage structure in (15.6) can be represented as
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞
Œ∏1
Œ∏1
...
Œ∏M
Ô£π
Ô£∫Ô£∫Ô£∫Ô£ª=
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞
X1
X2
...
XM
Ô£π
Ô£∫Ô£∫Ô£∫Ô£ªŒ≤ +
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞
u1
u2
...
uM
Ô£π
Ô£∫Ô£∫Ô£∫Ô£ª+
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞
e1
e2
...
eM
Ô£π
Ô£∫Ô£∫Ô£∫Ô£ª
or, more compactly, as
Œ∏Mr√ó1 = XMr√ópŒ≤p√ó1 + uMr√ó1 + eMr√ó1.
This indicates that the second-stage distribution of all parameters of all
individuals is
Œ∏|Œ≤, u, Œ£e ‚àºN (XŒ≤ + u, I ‚äóŒ£e) .
(15.11)
An alternative formulation can be obtained by arranging individuals within
parameters; here X must be redeÔ¨Åned accordingly, and the covariance ma-
trix of the process would then be Œ£e ‚äóI. The choice between the two
alternative orders is entirely a matter of computational convenience.
Example 15.4
Two-stage model for a quadratic trajectory
Suppose that feed intake measurements are taken serially in each of a num-
ber of descendants of a random sample of boars to be evaluated in a progeny

15.2 Hierarchical or Multistage Models
637
test. Consider the second-order quadratic model of Example 15.1, and as-
sume it provides a reasonable description of the trajectory of feed intake
over time. Let yijk be measurement k taken in oÔ¨Äspring j of boar i. The
Ô¨Årst-stage model can be written as
yijk = aij + bijtijk + cijt2
ijk + Œµijk,
i = 1, 2, . . . , M, j = 1, 2, . . . , oi, k = 1, 2, . . . , nijk,
yijk|aij, bij, cij, œÉ2
Œµ ‚àºNIID(aij + bijtijk + cijt2
ijk, œÉ2
Œµ),
where aij, bij, and cij are coeÔ¨Écients peculiar to oÔ¨Äspring j of boar i. As
usual, Œµijk is a discrepancy from the expected trajectory of individual ij
when measured at time k. If the sample of boars is homogeneous in every
possible respect, it might be sensible to Ô¨Åt the second stage model
Ô£Æ
Ô£∞
aij
bij
cij
Ô£π
Ô£ª=
Ô£Æ
Ô£∞
1
0
0
0
1
0
0
0
1
Ô£π
Ô£ª
Ô£Æ
Ô£∞
a0
b0
c0
Ô£π
Ô£ª+
Ô£Æ
Ô£∞
ai
bi
ci
Ô£π
Ô£ª+
Ô£Æ
Ô£∞
eaij
ebij
ecij
Ô£π
Ô£ª,
where a0, b0, and c0 are regression parameters common to all observations;
ai, bi, and ci are deviations common to all progeny of boar i, and the e‚Ä≤s are
the second-stage residuals, peculiar to oÔ¨Äspring j of boar i. Here, Xij = I3,
‚àÄi, j,
ui =
Ô£Æ
Ô£∞
ai
bi
ci
Ô£π
Ô£ª,
and
Œ£e =
Ô£Æ
Ô£∞
œÉ2
ea
œÉeab
œÉeac
œÉeab
œÉ2
eb
œÉebc
œÉeac
œÉebc
œÉ2
ec
Ô£π
Ô£ª
is the variance‚Äìcovariance matrix between progeny-speciÔ¨Åc regression pa-
rameters, conditionally on ai, bi, and ci.
It is instructive to write the Ô¨Årst-stage model in terms of the second-stage
structure, to obtain
yijk =

a0 + b0tijk + c0t2
ijk

+

ai + bitijk + cit2
ijk

+

eaij + ebijtijk + ecijt2
ijk

+ Œµijk.
The function in parentheses, (¬∑) , can be construed as a ‚Äúpopulation re-
gression‚Äù, common to all individuals measured; the second function, [¬∑] ,
is a deviation from the population regression shared by all descendants of
boar i; and the function {¬∑} is a deviation speciÔ¨Åc to oÔ¨Äspring j of boar i.
Conditionally on the parameters of the second-stage model, and assuming

638
15. Bayesian Analysis of Longitudinal Data
normality throughout, it follows that
yijk|a0, b0, c0, ai, bi, ci, tijk, v (tijk) ‚àº
N

(a0 + ai) + (b0 + bi) tijk + (c0 + ci) t2
ijk, v (tijk)

,
where
v (tijk) =
 1
tijk
t2
ijk

Ô£Æ
Ô£∞
œÉ2
ea
œÉeab
œÉeac
œÉeab
œÉ2
eb
œÉebc
œÉeac
œÉebc
œÉ2
ec
Ô£π
Ô£ª
Ô£Æ
Ô£∞
1
tijk
t2
ijk
Ô£π
Ô£ª+ œÉ2
Œµ
is a ‚Äúvariance function‚Äù. Likewise, and conditionally on the second-stage
parameters, the covariance between observations taken at times k and k‚Ä≤
on individual ij is
Cov (yijk, yijk‚Ä≤|second-stage parameters) = t‚Ä≤
k
Ô£Æ
Ô£∞
œÉ2
ea
œÉeab
œÉeac
œÉeab
œÉ2
eb
œÉebc
œÉeac
œÉebc
œÉ2
ec
Ô£π
Ô£ªtk‚Ä≤,
where t‚Ä≤
k =
 1
tijk
t2
ijk

and t‚Ä≤
k‚Ä≤ =
 1
tijk‚Ä≤
t2
ijk‚Ä≤

. The corre-
sponding ‚Äúcorrelation function‚Äù is then
Corr (yijk, yijk‚Ä≤|second stage parameters) =
t‚Ä≤
kŒ£etk‚Ä≤
>
(t‚Ä≤
kŒ£etk) (t‚Ä≤
k‚Ä≤Œ£etk‚Ä≤).
Suppose, further, that the boar-speciÔ¨Åc parameters are assigned the distri-
bution
Ô£Æ
Ô£∞
ai
bi
ci
Ô£π
Ô£ª
""""""
Œ£s ‚àºN
Ô£´
Ô£≠
Ô£Æ
Ô£∞
0
0
0
Ô£π
Ô£ª, Œ£s
Ô£∂
Ô£∏,
where
Œ£s =
Ô£Æ
Ô£∞
œÉ2
sa
œÉsab
œÉsac
œÉsab
œÉ2
sb
œÉsbc
œÉsac
œÉsbc
œÉ2
sc
Ô£π
Ô£ª
is the covariance matrix between boar-speciÔ¨Åc deviations from the overall
population regression. Then, the additional assumption that boar-speciÔ¨Åc
and progeny-speciÔ¨Åc deviations are independent yields
yijk|a0, b0, c0, tijk, Œ£e, Œ£s ‚àºN

a0 + b0tijk + c0t2
ijk, t‚Ä≤
k (Œ£s + Œ£e) tk

.
The distribution given above has a mean and variance describing how the
trait and its variability change in time, averaged over all individuals in the
population.
‚ñ†
A special case is when the Ô¨Årst and second stages of the model are both
linear in the parameters. If the Ô¨Årst stage is linear, (15.1) can be written
as
yi = TiŒ∏i + Œµi,
i = 1, 2, ..., M,
(15.12)

15.2 Hierarchical or Multistage Models
639
for some known matrix Ti. Employing (15.6) in (15.12) gives the represen-
tation
yi = Ti (XiŒ≤ + ui + ei) + Œµi
= (TiXi) Œ≤ + Tiui + Tiei + Œµi
= X‚àó
i Œ≤ + Tiui + Tiei + Œµi,
(15.13)
where X‚àó
i = TiXi. Under the usual normality assumptions, this induces
the conditional distributions
yi|Œ≤, ui, ei, Œ≥ ‚àºN [X‚àó
i Œ≤ + Tiui + Tiei, Ri (Œ≥)] ,
(15.14)
and
yi|Œ≤, ui, Œ£e, Œ≥ ‚àºN [X‚àó
i Œ≤ + Tiui, TiŒ£eT‚Ä≤
i + Ri (Œ≥)] .
(15.15)
It follows from (15.13)-(15.15) that a two-stage linear hierarchy for lon-
gitudinal data is a special case of the general mixed eÔ¨Äects linear model.
Further deconditioning can be obtained by introducing additional tiers in
the hierarchical structure.
The form of (15.12) implies that in a linear hierarchical model, once a
functional form is adopted for the Ô¨Årst stage, then all second-stage elements
contribute to the overall model in a similar form; for example, the gradient
of the observations with respect to either XiŒ≤, ui, or ei is always T‚Ä≤
i. It is
possible to give a Bayesian implementation in a more general and Ô¨Çexible
manner, but this is notationally awkward (specially if a nonlinear speciÔ¨Å-
cation is chosen for the Ô¨Årst stage). Hence, the hierarchical representation
is kept, with the understanding that all subsequent developments apply to
most models, at least conceptually.
15.2.3
Third Stage
In a Bayesian model, as pointed out in the chapters on Bayesian inference,
prior distributions must be assigned to all unknown quantities in the sta-
tistical system posited. Thus, priors must be adopted for Œ≤, u, Œ£e, and
Œ≥.
Let the vector u represent additive genetic eÔ¨Äects on the trajectory pa-
rameters. In this case, a classical (and convenient) assumption made in
quantitative genetics is that
u|G0 ‚àºN (0, A ‚äóG0) ,
(15.16)
where it is implied that parameters are ordered within individuals. Above,
A is the additive genetic relationship matrix between the M individuals,
and G0 is an r √ó r additive genetic variance‚Äìcovariance matrix between

640
15. Bayesian Analysis of Longitudinal Data
parameters, that is
G0 =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞
œÉ2
u1
œÉu12
. . .
œÉu1r
œÉu21
œÉ2
u2
. . .
œÉu2r
...
...
...
...
œÉur1
œÉur2
. . .
œÉ2
ur
Ô£π
Ô£∫Ô£∫Ô£∫Ô£ª,
with the understanding that œÉuij = œÉuji. If G0 is unknown, a prior dis-
tribution must be elicited for this matrix as well. For a linear model as in
(15.15) the preceding prior implies that, given Œ≤, Œ£e, G0, and Œ≥, the prior
predictive distribution is
yi|Œ≤, Œ£e, G0, Œ≥ ‚àºN [X‚àó
i Œ≤, Ti (G0 + Œ£e) T‚Ä≤
i + Ri (Œ≥)] .
It will be assumed further that
Œ≤|Œ±, Œì ‚àºN (Œ±, Œì)
(15.17)
where Œ±, and Œì are known hyperparameters. The joint prior density of all
unknowns can be taken to be equal to
p (Œ≤, u, G0, Œ£e, Œ≥|Œ±, Œì) = p (Œ≤|Œ±, Œì) p (u|G0) p (G0) p (Œ£e) p (Œ≥) . (15.18)
The preceding implies, a priori, that Œ≤, Œ£e, and Œ≥ are mutually indepen-
dent of each other and of u and G0, with the only dependence assumed a
priori being that of the distribution of u on G0. As seen before, after data
are combined with the prior via formal use of Bayes theorem, parameters
become interdependent, even if the data set contains just a few observa-
tions. Vague priors can be adopted for the dispersion components, with the
corresponding densities being
p (G0) ‚àùconstant,
|G0| > 0,
(15.19)
p (Œ£e) ‚àùconstant,
|Œ£e| > 0,
(15.20)
and
p (Œ≥) ‚àùconstant,
Œ≥ ‚àà‚ÑúŒ≥,
(15.21)
where ‚ÑúŒ≥ is the allowable parameter space of the dispersion vector Œ≥. For
example, if this vector contains a single residual variance component, its
parameter space would be the positive part of the real line, R+.
The preceding prior distributions may be bounded, based on either prior
knowledge of parameter values or on theoretical considerations. It must be
emphasized that an advantage of the Bayesian approach resides in the pos-
sibility of incorporating external stochastic information into the analysis.
If such information exists, and if one wishes to use it, the prior densities
should be modiÔ¨Åed accordingly. For instance, one may adopt informative

15.2 Hierarchical or Multistage Models
641
priors for the dispersion parameters, for example, inverted Wishart distri-
butions for the covariance matrices and scaled inverted chi-square distribu-
tions for variance components. If the priors are conjugate, a Markov chain
Monte Carlo implementation does not become much more diÔ¨Écult than
one based on bounded uniform priors.
It is often convenient, especially in the case of nonlinear models, to aug-
ment the prior distribution with the trajectory parameters Œ∏, leading to
the joint prior
p (Œ∏, Œ≤, u, G0, Œ£e, Œ≥|Œ±, Œì) = p (Œ∏|Œ≤, u, G0, Œ£e, Œ≥, Œ±, Œì)
p (Œ≤, u, G0, Œ£e, Œ≥|Œ±, Œì) .
(15.22)
Now, because the parameters of diÔ¨Äerent individuals, conditionally on Œ≤
and u, are independently distributed, with joint distribution as in (15.9),
use of this and of the prior densities (15.18)‚Äì(15.21) in (15.22) leads to the
following form for the augmented joint prior density
p (Œ∏, Œ≤, u, G0, Œ£e, Œ≥|Œ±, Œì) ‚àù
M
-
i=1
p (Œ∏i|Œ≤, ui, Œ£e) p (Œ≤|Œ±, Œì) p (u|G0) .
(15.23)
The joint posterior distribution has support for any value of Œ≤ in Rp and for
any value of ui in Rr. However, as noted earlier, if trajectory parameters
take values only within a restricted range, it may not always be reasonable
assigning an r-variate normal distribution as prior for the Œ∏i coeÔ¨Écients.
Nevertheless, the normality assumption may hold well for a transformation
of such parameters. For example, if a hypothetical curve for somatic cell
count in milk (a measure of udder health in dairy cattle) has four param-
eters representing, for example, initial conditions, growth rate, change in
growth rate, and level near the end of lactation, it may be sensible to adopt
a parameterization in a log-scale, with the normal prior assigned to the en-
suing parameterization. At any rate, this seldom causes serious problems,
provided that the longitudinal series is ‚Äúlong enough‚Äù for most individuals
(so the prior has a mild eÔ¨Äect on inferences), and that the genetic rela-
tionship structure is suÔ¨Éciently dense, so that information between related
individuals can be exchanged. Alternative parameterizations do not com-
plicate the Bayesian analysis conceptually, but can make implementation
more involved.
15.2.4
Joint Posterior Distribution
From Bayes theorem, the joint posterior density of all unknowns is formed
by combining the density of the sampling model in (15.5) with the joint

642
15. Bayesian Analysis of Longitudinal Data
prior (15.23), yielding
p (Œ∏, Œ≤, u, G0, Œ£e, Œ≥|y1, y2, . . . , yM, Œ±, Œì)
‚àù
# M
-
i=1
|Ri (Œ≥)|‚àí1
2 exp

‚àí1
2Œµ‚Ä≤
iR‚àí1
i
(Œ≥) Œµi

p (Œ∏i|Œ≤, ui, Œ£e)
$
√óp (Œ≤|Œ±, Œì) p (u|G0) .
(15.24)
As a side issue, it is instructive to consider the density of the distribution
of the observations unconditionally on Œ≤ and u, and this can be written
explicitly when the trajectory is linear in the parameters. From (15.15), it
follows that the resulting prior predictive distribution is:
yi|Œ£e, Œ≥, G0, Œ±, Œì ‚àºN [X‚àó
i Œ±, X‚àó
i ŒìX‚àó‚Ä≤
i + Ti (G0 + Œ£e) T‚Ä≤
i + Ri (Œ≥)] .
(15.25)
This distribution, given Œ£e, Œ≥, G0, Œ±, and Œì can be interpreted as the
probability distribution of the data before observations are collected.
15.3
Two-Step Approximate Bayesian Analysis
Bayesian inference always relies on applying the probability calculus to
some target distribution, this being the joint posterior of all unknowns
in (15.24). Probability statements are obtained from appropriate sets of
marginal, joint, or conditional distributions, depending in the type of in-
ference sought. However, in the case of the probability model with density
given by (15.24), it is impossible to arrive at the marginal distributions of
interest because the required integrals cannot be evaluated in closed form;
furthermore, numerical quadrature seldom works well beyond a few dimen-
sions. An alternative consists in extracting samples from the joint poste-
rior, with the appropriate coordinate of the sample being a draw from the
corresponding marginal distribution. From such samples, features of the
posterior distribution of interest (e.g., mean, median, variance, quantiles
or densities) can be estimated. A MCMC analysis can be used for this
purpose, and this will be discussed later. Often, however, an approximate,
simpler, analysis can lead to satisfactory inferences, at least for some fea-
tures of the posterior distribution. For example, in animal breeding it is
customary to carry out the following two-step analysis for mixed eÔ¨Äects
linear models: Ô¨Årst, estimate dispersion parameters by some method, such
as REML. Second, conditionally on the estimates of dispersion parameters,
Ô¨Ånd point estimates and (sometimes) a measure of uncertainty for Œ≤ and
for ui (i = 1, 2, . . . , M). A similar two-step analysis is described for the
hierarchical model under discussion.

15.3 Two-Step Approximate Bayesian Analysis
643
15.3.1
Estimating Œ≤, u, and e when Variances are Known
Suppose there is no uncertainty about the dispersion parameters, that is,
G0, Œ£e, and Œ≥ are known without error. Recall from (15.3) that the Ô¨Årst-
stage residual vector can be expressed as
Œµ = y ‚àíf (Œ∏, t) = y ‚àíf [XŒ≤ + u + e, t] ,
so that, for individual i,
Œµi = yi ‚àífi (Œ∏i, ti) = yi ‚àífi (XiŒ≤ + ui + ei, ti) .
The joint posterior density of Œ≤, u, and e, given G0, Œ£e, and Œ≥, can be
written (without making use of augmentation with the Œ∏ parameters) as
p (Œ≤, u, e|G0, Œ£e, Œ≥, y, Œ±, Œì) ‚àù
M
-
i=1
exp

‚àí1
2Œµ‚Ä≤
iR‚àí1
i
(Œ≥) Œµi

p (Œ≤|Œ±, Œì) p (u|G0) p (e|Œ£e)
‚àùexp
#
‚àí1
2
 M

i=1
Œµ‚Ä≤
iR‚àí1
i
(Œ≥) Œµi + (Œ≤ ‚àíŒ±)‚Ä≤ Œì‚àí1 (Œ≤ ‚àíŒ±)
$
√ó exp
#
‚àí1
2

u‚Ä≤ 
A‚àí1‚äóG‚àí1
0

u +
M

i=1
e‚Ä≤
iŒ£‚àí1
e ei
$
.
(15.26)
An approximate Bayesian analysis, conditionally on the dispersion com-
ponents, consists of approximating the joint posterior distribution with
density in (15.26) by a Gaussian process having mean vector equal to the
joint mode, and a variance‚Äìcovariance matrix given by the inverse of the
corresponding negative Hessian matrix. If the Ô¨Årst-stage model is linear,
the approximation is exact, as veriÔ¨Åed subsequently. In general, the modal
vector needs to be calculated with an iterative algorithm (converging in one
step for a linear trajectory). Considering that second derivatives are needed
for completing inferences, the Newton‚ÄìRaphson or scoring algorithms are
natural candidates here.
Let l (Œ≤, u) be the logarithm of the joint posterior density (15.26). Apart
from an additive constant
l (Œ≤, u, e) = ‚àí1
2
 M

i=1
Œµ‚Ä≤
iR‚àí1
i
(Œ≥) Œµi + (Œ≤ ‚àíŒ±)‚Ä≤ Œì‚àí1 (Œ≤ ‚àíŒ±)

‚àí1
2

u‚Ä≤ 
A‚àí1 ‚äóG‚àí1
0

u +
M

i=1
e‚Ä≤
iŒ£‚àí1
e ei

= ‚àí1
2

Œµ‚Ä≤R‚àí1 (Œ≥) Œµ + (Œ≤ ‚àíŒ±)‚Ä≤ Œì‚àí1 (Œ≤ ‚àíŒ±)

‚àí1
2

u‚Ä≤ 
A‚àí1 ‚äóG‚àí1
0

u + e‚Ä≤ 
I ‚äóŒ£‚àí1
e

e

,
(15.27)

644
15. Bayesian Analysis of Longitudinal Data
with Œµ‚Ä≤ = [Œµ‚Ä≤
1, Œµ‚Ä≤
2, . . . , Œµ‚Ä≤
M], R‚àí1 (Œ≥) = R‚àí1
1
(Œ≥) ‚äïR‚àí1
2
(Œ≥) ‚äï¬∑ ¬∑ ¬∑ ‚äïR‚àí1
M (Œ≥),
and ‚äïis the direct-sum operator, denoting that R is a block-diagonal
matrix with individual blocks being Ri (Œ≥) .
First Derivatives
Expressions for the Ô¨Årst and second derivatives are facilitated by employing
the chain rule of calculus. Observing matrix comformability, one can write
‚àÇŒµ‚Ä≤R‚àí1 (Œ≥) Œµ
‚àÇŒ≤

p√ó1
=
‚àÇŒ∏‚Ä≤
‚àÇŒ≤
 ‚àÇf ‚Ä≤ (Œ∏, t)
‚àÇŒ∏
 ‚àÇŒµ‚Ä≤R‚àí1 (Œ≥) Œµ
‚àÇf (Œ∏, t)

.
(15.28)
Similarly,
‚àÇŒµ‚Ä≤R‚àí1 (Œ≥) Œµ
‚àÇu

q√ó1
=
‚àÇŒ∏‚Ä≤
‚àÇu
 ‚àÇf ‚Ä≤ (Œ∏, t)
‚àÇŒ∏
 ‚àÇŒµ‚Ä≤R‚àí1 (Œ≥) Œµ
‚àÇf (Œ∏, t)

,
(15.29)
and
‚àÇŒµ‚Ä≤R‚àí1 (Œ≥) Œµ
‚àÇe

rM√ó1
=
‚àÇŒ∏‚Ä≤
‚àÇe
 ‚àÇf ‚Ä≤ (Œ∏, t)
‚àÇŒ∏
 ‚àÇŒµ‚Ä≤R‚àí1 (Œ≥) Œµ
‚àÇf (Œ∏, t)

.
(15.30)
In (15.29) note that if one includes in u the additive genetic values of
related individuals that lack longitudinal information, then q > rM . This
augmentation is a routine practice in animal breeding, since it permits
inferring the genetic worth of candidates for selection indirectly, via the
additive genetic relationship matrix A. Put
q = rM + rM = r

M + M

,
where M is the number of individuals without measurements. It is conve-
nient to write the second-stage model as
Œ∏ = XŒ≤ + Zu + e,
where
ZrM√ór(M+M) =

IrM√órM, 0rM√órM

.
The order of the vectors is dropped in the notation hereinafter. Now
‚àÇŒ∏‚Ä≤
‚àÇŒ≤ = ‚àÇ(XŒ≤ + Zu + e)‚Ä≤
‚àÇŒ≤
= X‚Ä≤,
(15.31)
‚àÇŒ∏‚Ä≤
‚àÇu = ‚àÇ(XŒ≤ + Zu + e)‚Ä≤
‚àÇu
=
 I
0

,
(15.32)
‚àÇŒ∏‚Ä≤
‚àÇe = ‚àÇ(XŒ≤ + Zu + e)‚Ä≤
‚àÇe
= I,
(15.33)
‚àÇŒµ‚Ä≤R‚àí1 (Œ≥) Œµ
‚àÇf (Œ∏, t)

= ‚àí2R‚àí1 (Œ≥) [y ‚àíf (Œ∏, t)] ,
(15.34)

15.3 Two-Step Approximate Bayesian Analysis
645
and deÔ¨Åne
H‚Ä≤ (Œ∏) =
‚àÇf ‚Ä≤ (Œ∏, t)
‚àÇŒ∏

=
‚àÇ[f ‚Ä≤
1 (Œ∏1, t) f ‚Ä≤
2 (Œ∏2, t) . . . f ‚Ä≤
M (Œ∏M, t)]
‚àÇŒ∏

=
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞
H‚Ä≤
1 (Œ∏1)
0
¬∑ ¬∑ ¬∑
0
0
H‚Ä≤
2 (Œ∏2)
¬∑ ¬∑ ¬∑
0
...
...
...
...
0
0
¬∑ ¬∑ ¬∑
H‚Ä≤
M (Œ∏M)
Ô£π
Ô£∫Ô£∫Ô£∫Ô£ª,
(15.35)
where Œ∏ = [Œ∏‚Ä≤
1, Œ∏‚Ä≤
2, . . . , Œ∏M]‚Ä≤. Above
H‚Ä≤
¬¥ƒ± (Œ∏i) = ‚àÇf ‚Ä≤
i (Œ∏i, t)
‚àÇŒ∏i
is an r√óni matrix, being independent of Œ∏i only when the Ô¨Årst-stage model
is linear in the parameters. Hence
H‚Ä≤ (Œ∏) = H‚Ä≤
1 (Œ∏) ‚äïH‚Ä≤
2 (Œ∏) ‚äï¬∑ ¬∑ ¬∑ ‚äïH‚Ä≤
M (Œ∏) .
Applying (15.31)‚Äì(15.35) in (15.28), (15.29) and (15.30):
‚àÇŒµ‚Ä≤R‚àí1 (Œ≥) Œµ
‚àÇŒ≤

= ‚àí2X‚Ä≤H‚Ä≤ (Œ∏) R‚àí1 (Œ≥) [y ‚àíf (Œ∏, t)] ,
‚àÇŒµ‚Ä≤R‚àí1 (Œ≥) Œµ
‚àÇu

= ‚àí2

I
0

H‚Ä≤ (Œ∏) R‚àí1 (Œ≥) [y ‚àíf (Œ∏, t)]
= ‚àí2

H‚Ä≤ (Œ∏) R‚àí1 (Œ≥) [y ‚àíf (Œ∏, t)]
0

,
and
‚àÇŒµ‚Ä≤R‚àí1 (Œ≥) Œµ
‚àÇe

= ‚àí2H‚Ä≤ (Œ∏) R‚àí1 (Œ≥) [y ‚àíf (Œ∏, t)] .
The gradient vector of the logarithm of the conditional posterior density of
Œ≤, u, and e in (15.27) is, using the preceding expressions,
‚àÇL (Œ≤, u, e)
‚àÇŒ≤
= X‚Ä≤H‚Ä≤ (Œ∏) R‚àí1 (Œ≥) Œµ ‚àíŒì‚àí1 (Œ≤ ‚àíŒ±) ,
(15.36)
‚àÇL (Œ≤, u, e)
‚àÇu
=
 H‚Ä≤ (Œ∏) R‚àí1 (Œ≥) Œµ
0

‚àí

A‚àí1 ‚äóG‚àí1
0

u,
(15.37)
‚àÇL (Œ≤, u, e)
‚àÇe
= H‚Ä≤ (Œ∏) R‚àí1 (Œ≥) Œµ ‚àí

I ‚äóŒ£‚àí1
e

e,
(15.38)
with Œµ = y ‚àíf (Œ∏, t) .

646
15. Bayesian Analysis of Longitudinal Data
Second Derivatives
The Newton‚ÄìRaphson or scoring algorithms, and the Gaussian approxi-
mation to the posterior distribution, require second derivatives. Hence, an
additional diÔ¨Äerentiation is needed. For simplicity of notation let, here-
inafter,
R‚àí1 (Œ≥) = R‚àí1.
Now, note that
‚àÇ2L (Œ≤, u, e)
‚àÇŒ≤ ‚àÇŒ≤‚Ä≤
= ‚àÇ

X‚Ä≤H‚Ä≤ (Œ∏) R‚àí1Œµ ‚àíŒì‚àí1 (Œ≤ ‚àíŒ±)

‚àÇŒ≤‚Ä≤
= ‚àÇ

X‚Ä≤H‚Ä≤ (Œ∏) R‚àí1Œµ

‚àÇŒ≤‚Ä≤
‚àíŒì‚àí1
= X‚Ä≤
‚àÇH‚Ä≤ (Œ∏)
‚àÇŒ≤‚Ä≤

R‚àí1Œµ ‚àíX‚Ä≤H‚Ä≤ (Œ∏) R‚àí1 ‚àÇf (Œ∏, t)
‚àÇŒ≤‚Ä≤
‚àíŒì‚àí1.
The expression [‚àÇH‚Ä≤ (Œ∏) /‚àÇŒ≤‚Ä≤] is an informal representation, as the deriva-
tives of a matrix with respect to a vector require a ‚Äúthree-dimensional
matrix‚Äù (technically, one needs to arrange elements of H‚Ä≤ (Œ∏) into a vector,
and then take derivatives). Ignoring this, note that if expectations of the
second derivatives are taken with respect to the Ô¨Årst-stage distribution,
given in (15.4), one has
Ey|Œ∏ [Œµ = y ‚àíf (Œ∏, t)] = 0.
Hence, considerable simpliÔ¨Åcation is obtained, the result being
Ey|Œ∏
‚àÇ2L (Œ≤, u, e)
‚àÇŒ≤ ‚àÇŒ≤‚Ä≤

= ‚àí

X‚Ä≤H‚Ä≤ (Œ∏) R‚àí1 ‚àÇf (Œ∏, t)
‚àÇŒ≤‚Ä≤
+ Œì‚àí1

= ‚àí

X‚Ä≤H‚Ä≤ (Œ∏) R‚àí1 ‚àÇf (Œ∏, t)
‚àÇŒ∏‚Ä≤
‚àÇŒ∏
‚àÇŒ≤‚Ä≤ + Œì‚àí1

= ‚àí

X‚Ä≤H‚Ä≤ (Œ∏) R‚àí1H (Œ∏) X + Œì‚àí1
.
(15.39)
Likewise,
Ey|Œ∏
‚àÇ2L (Œ≤, u, e)
‚àÇu ‚àÇu‚Ä≤

= ‚àí

Z‚Ä≤H‚Ä≤ (Œ∏) R‚àí1H (Œ∏) Z + A‚àí1 ‚äóG‚àí1
0

= ‚àí

I
0
 
H‚Ä≤ (Œ∏) R‚àí1H (Œ∏)
 
I
0

+ A‚àí1 ‚äóG‚àí1
0
%
.
(15.40)

15.3 Two-Step Approximate Bayesian Analysis
647
Now the additive genetic relationship matrix A can be partitioned in a
form consistent with that of the vector u, such that
A‚àí1 ‚äóG‚àí1
0
=

AMM
AMM
AMM
AMM
‚àí1
‚äóG‚àí1
0
=

AMM ‚äóG‚àí1
0
AMM ‚äóG‚àí1
0
AMM ‚äóG‚àí1
0
AMM ‚äóG‚àí1
0

=

GMM
GMM
GMM
GMM

.
Using this in (15.40)
Ey|Œ∏
‚àÇ2L (Œ≤, u, e)
‚àÇu‚àÇu‚Ä≤

= ‚àí

H‚Ä≤ (Œ∏) R‚àí1H (Œ∏) + GMM
GMM
GMM
GMM

.
(15.41)
Further,
Ey|Œ∏
‚àÇ2L (Œ≤, u, e)
‚àÇe‚àÇe‚Ä≤

= ‚àí

H‚Ä≤ (Œ∏) R‚àí1H (Œ∏) + I ‚äóŒ£‚àí1
e

.
(15.42)
Using similar algebra, the second-order mixed partial derivatives are
Ey|Œ∏
‚àÇ2L (Œ≤, u, e)
‚àÇŒ≤‚àÇu‚Ä≤

= ‚àíX‚Ä≤H‚Ä≤ (Œ∏) R‚àí1H (Œ∏) Z
= ‚àí

X‚Ä≤H‚Ä≤ (Œ∏) R‚àí1H (Œ∏)
0

,
(15.43)
Ey|Œ∏
‚àÇ2L (Œ≤, u, e)
‚àÇŒ≤‚àÇe‚Ä≤

= ‚àí

X‚Ä≤H‚Ä≤ (Œ∏) R‚àí1H (Œ∏)

,
(15.44)
and
Ey|Œ∏
‚àÇ2L (Œ≤, u, e)
‚àÇu‚àÇe‚Ä≤

= ‚àí

Z‚Ä≤H‚Ä≤ (Œ∏) R‚àí1H (Œ∏)

= ‚àí

I
0

H‚Ä≤ (Œ∏) R‚àí1H (Œ∏)
= ‚àí

H‚Ä≤ (Œ∏) R‚àí1H (Œ∏)
0

.
(15.45)
Gaussian Approximation to the Conditional Posterior Distribution
Using results given in the chapter on approximate methods for inference,
the conditional posterior distribution
[Œ≤, u, e|G0, Œ£e, Œ≥, y, Œ±, Œì]

648
15. Bayesian Analysis of Longitudinal Data
can be approximated as
Œ≤, u, e|G0, Œ£e, Œ≥, y, Œ±, Œì ‚àºN(c, C‚àí1
c ),
(15.46)
where
c =Arg max
Œ≤,u,ep (Œ≤, u, e|G0, Œ£e, Œ≥, y, Œ±, Œì)
(15.47)
is the modal vector, and Cc is the negative of the matrix of second deriva-
tives of l (Œ≤, u, e) with respect to Œ≤, u, e, evaluated at the modal value c.
Collecting the submatrices of second derivatives in (15.39), and (15.41)‚Äì
(15.45), the symmetric negative Hessian matrix can be expressed as
C =
Ô£Æ
Ô£ØÔ£ØÔ£∞
X‚Ä≤WX + Œì‚àí1
X‚Ä≤W
0
X‚Ä≤W
.
W + GMM
GMM
W
.
.
GMM
0
.
.
.
W + I ‚äóŒ£‚àí1
e
Ô£π
Ô£∫Ô£∫Ô£ª,
(15.48)
where
W = W (Œ∏, Œ≥) = H‚Ä≤ (Œ∏) R‚àí1H (Œ∏) ,
noting that this is a symmetric matrix and with order rM √ó rM.
The Scoring Algorithm
Computation of the modal vector c using the scoring algorithm proceeds
with the iteration
c[t+1] = c[t] + C‚àí1
[t] g[t],
(15.49)
where [t] denotes the iterate number and g[t] is the gradient vector at
iteration [t] . An alternative representation is
C[t]c[t+1] = C[t]c[t] + g[t].
(15.50)
Making use of (15.36)‚Äì(15.38), and putting Œµ[t] = y ‚àíf

Œ∏[t], t

,
g[t] =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
X‚Ä≤H‚Ä≤ 
Œ∏[t]
R‚àí1Œµ[t] ‚àíŒì‚àí1 
Œ≤[t] ‚àíŒ±

H‚Ä≤ 
Œ∏[t]
R‚àí1Œµ[t] ‚àíGMMu[t]
M ‚àíGMMu[t]
M
‚àíGMMu[t]
M ‚àíGMMu[t]
M
H‚Ä≤ 
Œ∏[t]
R‚àí1Œµ[t] ‚àí

I ‚äóŒ£‚àí1
e

e[t]
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
.
Note that
C[t]c[t] =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
X‚Ä≤W[t] 
XŒ≤[t] + u[t]
M + e[t]
+ Œì‚àí1Œ≤[t]
W[t] 
XŒ≤[t] + u[t]
M + e[t]
+ GMMu[t]
M + GMMu[t]
M
GMMu[t]
M + GMMu[t]
M
W[t] 
XŒ≤[t] + u[t]
M + e[t]
+

I ‚äóŒ£‚àí1
e

e[t]
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
.

15.3 Two-Step Approximate Bayesian Analysis
649
Since
Œ∏[t] = XŒ≤[t] + u[t]
M + e[t],
adding the two preceding vectors gives
C[t]c[t] + g[t] =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
X‚Ä≤W[t]Œ∏[t] + X‚Ä≤H‚Ä≤ 
Œ∏[t]
R‚àí1Œµ[t] + Œì‚àí1Œ±
W[t]Œ∏[t] + H‚Ä≤ 
Œ∏[t]
R‚àí1Œµ[t]
0
W[t]Œ∏[t] + H‚Ä≤ 
Œ∏[t]
R‚àí1Œµ[t]
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
.
(15.51)
In (15.51), observe that
W[t]Œ∏
[t] + H‚Ä≤ 
Œ∏[t]
R‚àí1Œµ[t]
= H‚Ä≤ 
Œ∏[t]
R‚àí1H

Œ∏[t]
Œ∏[t] + H‚Ä≤ 
Œ∏[t]
R‚àí1Œµ[t]
= H‚Ä≤ 
Œ∏[t]
R‚àí1 9
H

Œ∏[t]
Œ∏[t] + Œµ[t]:
= H‚Ä≤ 
Œ∏[t]
R‚àí15y[t],
where
5y[t] = H

Œ∏[t]
Œ∏[t] + Œµ[t]
(15.52)
is a ‚Äúpseudo-data‚Äù vector that changes values iteratively. Collecting (15.48),
(15.51), and (15.52) to form the scoring algorithm, the iteration can be
represented as
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞
Œ≤[t+1]
u[t+1]
M
u[t+1]
M
e[t+1]
Ô£π
Ô£∫Ô£∫Ô£∫Ô£ª=

C[t]‚àí1
Ô£Æ
Ô£ØÔ£ØÔ£∞
X‚Ä≤d[t] + Œì‚àí1Œ±
H‚Ä≤d[t]
0
d[t]
Ô£π
Ô£∫Ô£∫Ô£ª,
(15.53)
where
d[t] = H‚Ä≤ 
Œ∏[t]
R‚àí15y[t].
This is in the form of an iteratively reweighted system of linear mixed
model equations for a Gaussian process. The coeÔ¨Écient matrix and the
vector of the right-hand sides change from iterate to iterate. If the scoring
algorithm converges to a global maximum (this being extremely diÔ¨Écult or
impossible to check), we denote the maximum a posteriori estimates of Œ≤,
u, and e as 5Œ≤ (G0, Œ£e, Œ≥) , 5u (G0, Œ£e, Œ≥) , and 5e (G0, Œ£e, Œ≥) , respectively,
to emphasize the dependence on the dispersion parameters.

650
15. Bayesian Analysis of Longitudinal Data
15.3.2
Estimating G0, Œ£e and Œ≥ from Their
Marginal Posterior Distribution
In the preceding, methods for inferring unknowns from the conditional
posterior distribution
[Œ≤, u, e|G0, Œ£e, Œ≥, y, Œ±, Œì]
were outlined, using Gaussian approximation (15.46). This requires knowl-
edge of the nuisance parameters G0, Œ£e and Œ≥, a situation seldom encoun-
tered in practice. In approximate Bayesian analysis (e.g., Box and Tiao,
1973; Gianola and Fernando, 1986), the usual approach consists of Ô¨Ånding
the maximizers of the marginal posterior distribution [G0, Œ£e, Œ≥|y, Œ±, Œì] of
the dispersion components, say
5G0, 5Œ£e, 5Œ≥ = Arg
max
G0,Œ£e,Œ≥ p (G0, Œ£e, Œ≥|y, Œ±, Œì) .
(15.54)
If uniform priors are adopted for G0, Œ£e, and Œ≥, as in the developments pre-
sented here, 5G0, 5Œ£e and 5Œ≥ are usually known as ‚Äúmarginal ML estimates‚Äù.
Subsequently, these estimates can be used to obtain inferences based on
the distribution

Œ≤, u, e|G0 = 5G0, Œ£e = 5Œ£e, Œ≥ = 5Œ≥, y, Œ±, Œì

.
It must be emphasized that such an analysis does not take into account
the error of estimation of the nuisance parameters. The main diÔ¨Éculty, spe-
cially for a nonlinear Ô¨Årst-stage model, is that p (G0, Œ£e, Œ≥|y, Œ±, Œì) cannot
be arrived at in closed form, and the marginal ML estimates cannot be writ-
ten explicitly. However, these estimates can be approximated as described
below.
Consider again the Ô¨Årst-stage model in (15.3):
y = f (Œ∏, t) + Œµ
and expand its structure f (Œ∏, t) about provisional estimates
5Œ≤
=
5Œ≤ (G0, Œ£e, Œ≥) ,
5u
=
5u (G0, Œ£e, Œ≥) ,
and
5e = 5e (G0, Œ£e, Œ≥) ,
obtained as in the preceding section for some sensible starting values of the
dispersion parameters. Putting Œ∑ =

Œ≤‚Ä≤, u‚Ä≤, e‚Ä≤‚Ä≤, a Ô¨Årst order Taylor series
approximation about 5Œ∑ yields
y ‚âàf

X5Œ≤ + Z5u + 5e, t

+
‚àÇf (Œ∏, t)
‚àÇŒ∏‚Ä≤
  ‚àÇŒ∏
‚àÇŒ∑‚Ä≤
%
Œ∑=Œ∑
(Œ∑ ‚àí5Œ∑) + Œµ

15.3 Two-Step Approximate Bayesian Analysis
651
= f

X5Œ≤ + Z5u + 5e, t

+ H

5Œ∏
 
X
Z
I

Ô£Æ
Ô£∞
Œ≤ ‚àí5Œ≤
u ‚àí5u
e ‚àí5e
Ô£π
Ô£ª+ Œµ
= f

5Œ∏, t

‚àíH

5Œ∏
 
X5Œ≤ + Z5u + 5e

+ H

5Œ∏

(XŒ≤ + Zu + e) + Œµ. (15.55)
In the preceding, now let
H

5Œ∏

X
=
5X,
H

5Œ∏

Z
=
5Z,
H

5Œ∏

=
5H,
and
5Œµ = y ‚àíf

5Œ∏, t

.
Recalling that the pseudo-data vector is
5y = 5X5Œ≤ + 5Z5u + 5H5e + 5Œµ,
then, (15.55) can be rearranged as
5y ‚âà5XŒ≤ + 5Zu + 5He + Œµ,
(15.56)
so the expansion leads to a linear ‚Äúpseudo-model‚Äù for ‚Äúpseudo-data‚Äù from
which an update for the dispersion parameters G0, Œ£e, and Œ≥ can be ob-
tained by some standard method suitable for linear models, such as those
based on maximizing likelihoods. With this update, a new point estimate
of Œ∏ can be computed with the scoring algorithm (15.53).
Under the linear pseudo-model, one can adopt the Bayesian hierarchy
5y|Œ≤, u, 5X, 5Z, Œ≥ ‚âàN

5XŒ≤ + 5Zu + 5He, R (Œ≥)

(15.57)
with priors as in (15.16)‚Äì(15.21). If Ô¨Çat, improper priors are adopted for
Œ≤ and for the dispersion parameters, any available algorithm for REML
estimation of dispersion parameters can be used to obtain the revised val-
ues for G0, Œ£e, and Œ≥. This is because with such Ô¨Çat priors, the REML
estimates are the modal values of the distribution [G0, Œ£e, Œ≥|5y, Œ±, Œì] in a
Gaussian linear model (Harville, 1974). Such algorithms do not consider
the situation where it is assumed that Œ≤ has the informative distribution
in (15.17), so this would work only if p(Œ≤) is taken to be proportional
to a constant. In this case, one proceeds to iterate between the REML
algorithm applied to 5y, to obtain new values of G0, Œ£e, Œ≥, the scoring
algorithm (15.53) to obtain new values 5Œ≤ (G0, Œ£e, Œ≥) , 5u (G0, Œ£e, Œ≥) , and
5e (G0, Œ£e, Œ≥) , and the Taylor series expansion leading to the linear pseudo-
model for 5y as in (15.56) and (15.57). The cycle is repeated until G0, Œ£e, Œ≥

652
15. Bayesian Analysis of Longitudinal Data
stabilize, these being the modal values sought. Finally, inferences about Œ≤,
u, e are obtained from the Gaussian approximation (15.46). In particular,
since
Œ∏ = XŒ≤ + Zu + e =

X
Z
I

Ô£Æ
Ô£∞
Œ≤
u
e
Ô£π
Ô£ª= K
Ô£Æ
Ô£∞
Œ≤
u
e
Ô£π
Ô£ª,
so Œ∏ is a linear combination of Œ≤, u, e, then approximately,
Œ∏|G0, Œ£e, Œ≥, y, Œ±, Œì ‚àºN(Kc, KC‚àí1
c K‚Ä≤),
(15.58)
with the understanding that if an REML algorithm is employed, then there
is the implicit assumption that Œì‚àí1 ‚Üí0, to make the prior distribution of
Œ≤ Ô¨Çat (and improper).
An alternative approach to locating the modal values of the dispersion
parameters of G0, Œ£e, Œ≥ would be using a Laplacian approximation, but
this is not discussed here.
15.3.3
Special Case: Linear First Stage
If the trajectory is linear in the parameters, as in (15.12), the entire vector
of observations can be written as
y = TŒ∏ + Œµ
(15.59)
for T = T1 ‚äïT2 ‚äï¬∑ ¬∑ ¬∑ ‚äïTM, where Ti is a known matrix, and
Œ∏ = XŒ≤ + Zu + e,
as before. For this linear model, the matrix of derivatives of the conditional
expectation vector with respect to Œ∏ is
H‚Ä≤ (Œ∏) = ‚àÇ

Œ∏‚Ä≤T‚Ä≤
‚àÇŒ∏
= T‚Ä≤
which is independent of the parameters. Also, note that the pseudo-data
vector 5y becomes
5y[t]
=
H

Œ∏[t]
Œ∏[t] +

y ‚àíf

Œ∏[t], t

=
TŒ∏[t] +

y ‚àíTŒ∏[t]
= y,
so it is identical to the vector of observations y. In this situation, as seen
in Chapter 6, the posterior distribution
[Œ≤, u, e|G0, Œ£e, Œ≥, y, Œ±, Œì]

15.4 Computation via Markov Chain Monte Carlo
653
is exactly Gaussian, with mean vector m = M‚àí1r, and posterior variance‚Äì
covariance matrix M‚àí1, where
M =
Ô£Æ
Ô£ØÔ£ØÔ£∞
X‚Ä≤QX + Œì‚àí1
X‚Ä≤Q
0
X‚Ä≤Q
.
Q + GMM
GMM
Q
.
.
GMM
0
.
.
.
Q + I ‚äóŒ£‚àí1
e
Ô£π
Ô£∫Ô£∫Ô£ª,
(15.60)
for Q = T‚Ä≤R‚àí1T, and
r =
Ô£Æ
Ô£ØÔ£ØÔ£∞
X‚Ä≤T‚Ä≤R‚àí1y + Œì‚àí1Œ±
T‚Ä≤R‚àí1y
0
T‚Ä≤R‚àí1y
Ô£π
Ô£∫Ô£∫Ô£ª.
(15.61)
If a diÔ¨Äuse prior for Œ≤ is adopted, such that Œì‚àí1 ‚Üí0, the Œ≤ component
of the mean vector tends toward the best linear unbiased estimator of
Œ≤, whereas the u and e components tend to the BLUP of u and of e,
respectively. Further, in the approximate Ô¨Årst-stage distribution in (15.57),
one has that
5XŒ≤ + 5Zu + 5He = TXŒ≤ + TZu + Te = TŒ∏.
Thus:
5y|Œ≤, u, e, 5X, 5Z, Œ≥ ‚âàN

5XŒ≤ + 5Zu + 5He, R

‚â°y|Œ≤, u, e, X, Z, T, Œ≥ ‚àºN [TŒ∏, R]
(15.62)
which is exactly the Ô¨Årst-stage distribution. Therefore, as Œì‚àí1 ‚Üí0, the
mode of the posterior distribution [G0, Œ£e, Œ≥|y, Œ±, Œì] tends to the REML
estimates, provided the prior for u is as in (15.16).
15.4
Computation via Markov Chain Monte Carlo
The approximate Bayesian method described in the preceding section is,
undoubtedly, computationally involved, specially for nonlinear trajectories.
However, it is statistically and algorithmically equivalent to what might
be termed a ‚Äúquasi-BLUP coupled with REML‚Äù analysis of a nonlinear
mixed eÔ¨Äects model. Actually, the latter represents the state of the art from
a likelihood-frequentist perspective (e.g., WolÔ¨Ånger, 1993; WolÔ¨Ånger and
Lin, 1997), and relies on asymptotic arguments as well. The approximate
Bayesian approach, however, has the additional Ô¨Çexibility conferred by the
possibility of introducing prior information, as noted earlier. It cannot be
overemphasized, however, that in the approximate analysis:

654
15. Bayesian Analysis of Longitudinal Data
(1) the estimates of the dispersion parameters G0, Œ£e, Œ≥ are from a joint
mode;
(2) inferences obtained for Œ≤, u, e or Œ∏ are conditional on such modal
values, and
(3) an asymptotic approximation to a conditional posterior distribution, as
in (15.46), is used.
An alternative, and probably simpler from a computational point of view,
is to carry out a fully Bayesian analysis by sampling from marginal poste-
rior distributions of interest. This must be contrasted at the onset with the
conditioning arguments employed in the approximate method. These reveal
that uncertainty about nuisance parameters that one should integrate out
is not taken into account. Although procedures for sampling from the pos-
terior probably require more computer time, these are easier to program.
In addition, an entire distribution can be estimated, as opposed to just its
location and (perhaps) dispersion.
An implementation, based possibly on a combination of several sampling
techniques, is discussed in this section since there is seldom a unique, opti-
mal approach for drawing samples from posterior distributions. The start-
ing point is to consider constructing a Gibbs sampler, that is, attempting
to form (and identify) all possible fully conditional posterior distributions,
and then looping through all such conditionals by iterative updating of the
conditioning unknowns. As noted in Chapter 11, a complete pass or scan
through all fully conditional distributions deÔ¨Ånes an iteration of the Gibbs
sampler, at least in its best-known form. The scan can be done in a Ô¨Åxed or
randomized order, or in an ‚Äúup and down‚Äù direction, that is, when the order
of updating in the iteration is exactly the reverse of that in the preceding
one. Also, some ‚Äúsites‚Äù (conditional distributions) may not be visited at
all in a given iteration, although all sites must be updated, eventually, and
visited an inÔ¨Ånite number of times, theoretically. For example, in a statis-
tical model with Ô¨Åve unknowns, a scan pattern of 1, 2, 3, 1, 2, 3, 1, 2, 3, 4,
5 may be repeated indeÔ¨Ånitely (Neal, personal communication). One might
do this if variables 4 and 5 are thought to be almost independent of the
others, and mix faster, so that it is better to spend more time working on
1, 2, 3. For example, a suitable reparameterization of the trajectory param-
eters or of the location vector Œ≤, may enhance orthogonality, so one can
then spend more time updating the dispersion parameters and the u and e
vectors. In genetic applications, these two vectors may be highly colinear,
and may not be identiÔ¨Åable distinctly from each other when the additive
genetic relationship matrix (A) is nearly an identity matrix and G0 and Œ£e
are unknown. On the other hand, when dispersion parameters are known,
u and e have distinct conditional posterior distributions, although strong
posterior inter-correlations still may exist. The implementation described
here operates as follows: if the fully conditional distribution of a scalar or
vector can be identiÔ¨Åed, the corresponding sample is drawn directly, as in

15.4 Computation via Markov Chain Monte Carlo
655
standard Gibbs sampling. Otherwise, a Metropolis‚ÄìHastings, acceptance‚Äì
rejection or importance sampling (with resampling) step can be adopted.
15.4.1
Fully Conditional Posterior Distributions
The starting point for identifying the needed conditional posterior distri-
butions is the augmented joint posterior density (15.24). This is restated
here to facilitate references to the expression in subsequent developments.
The joint density is
p (Œ∏, Œ≤, u, G0, Œ£e, Œ≥|y1, y2, . . . , yM, Œ±, Œì)
‚àù
# M
-
i=1
|Ri (Œ≥)|‚àí1
2 exp

‚àí1
2Œµ‚Ä≤
iR‚àí1
i
(Œ≥) Œµi

p (Œ∏i|Œ≤, ui, Œ£e)
$
p (Œ≤|Œ±, Œì) p (u|G0) .
(15.63)
The conditional posterior densities can be deduced by retaining the part
of (15.63) that is a function of the pertinent unknown, and treating the re-
maining portion as Ô¨Åxed, becoming, thus, a part of the integration constant
of the conditional distribution sought. Such distributions will be examined
systematically in what follows.
Trajectory Parameters
From (15.63):
p (Œ∏|Œ≤, u, G0, Œ£e, Œ≥, y1, y2, . . . , yM, Œ±, Œì)
‚àù
# M
-
i=1
exp

‚àí1
2Œµ‚Ä≤
iR‚àí1
i
(Œ≥) Œµi

p (Œ∏i|Œ≤, ui, Œ£e)
$
.
(15.64)
This indicates that, given all other parameters, the trajectory coeÔ¨Écients Œ∏i
of diÔ¨Äerent individuals are mutually independent of each other and, hence,
can be sampled independently. Thus
p (Œ∏i|Œ≤, u, G0, Œ£e, Œ≥, y1, y2, . . . , yM, Œ±, Œì) = p (Œ∏i|Œ≤, ui, Œ£e, Œ≥, yi)
‚àùexp

‚àí1
2 [yi ‚àífi (Œ∏i, t)]‚Ä≤ R‚àí1
i
(Œ≥) [yi ‚àífi (Œ∏i, t)]
%
√ó exp

‚àí1
2 [Œ∏i ‚àíXiŒ≤ ‚àíui]‚Ä≤ Œ£‚àí1
e
[Œ∏i ‚àíXiŒ≤ ‚àíui]
%
(15.65)
for i = 1, 2, ..., M. The sampling method to be used depends on whether or
not the Ô¨Årst-stage model is linear in the trajectory parameters or not.

656
15. Bayesian Analysis of Longitudinal Data
Linear First-Stage: Gibbs Sampling
In the case of a linear Ô¨Årst-stage model
fi (Œ∏i, t) = TiŒ∏i,
i = 1, 2, . . . , M,
for some known matrix Ti. Standard results for Bayesian linear models with
known dispersion parameters give as conditional posterior distribution,
Œ∏i|Œ≤, ui, Œ£e, Œ≥, yi ‚àºN

Œ∏i, Vi

,
i = 1, 2, . . . , M,
(15.66)
where
Œ∏i =

T‚Ä≤
iR‚àí1
i
(Œ≥) Ti + Œ£‚àí1
e
‚àí1 
T‚Ä≤
iR‚àí1
i
(Œ≥) yi + Œ£‚àí1
e
(XiŒ≤ + ui)

,
(15.67)
and
Vi =

T‚Ä≤
iR‚àí1
i
(Œ≥) Ti + Œ£‚àí1
e
‚àí1 .
(15.68)
Hence, collecting samples from the conditional posterior of the trajectory
parameters is straightforward, as the draws involve sampling from M in-
dependent r-variate normal distributions at each iteration of the MCMC
process. Most often, r is small, e.g., r = 3 in a Gompertz growth function.
Nonlinear First Stage: Metropolis‚ÄìHastings
When the trajectory is not linear in Œ∏i, the kernel of the distribution (15.65)
does not have a recognizable form, so direct drawing is not feasible. A Ô¨Årst
possibility here consists of setting up a Metropolis‚ÄìHastings scheme. The
main diÔ¨Éculty is the Ô¨Åne-tuning of a proposal distribution that does not
result either in too frequent rejection or in a large acceptance rate. In the
Ô¨Årst situation, the sampler does not visit eÔ¨Äectively the parameter space;
in the second one, the algorithm moves very slowly, so that the ensuing
eÔ¨Äective sample size is small. Additional details on Metropolis‚ÄìHastings
computations are in the chapter on MCMC procedures.
A reasonable candidate-generating distribution might be a multivariate
normal process such as that in (15.66), but using the pseudo-data (instead
of the observed data vector) resulting from the Taylor series expansion
5yi = 6
Xi5Œ≤ + 5Zi5u + 5H5ei + 5Œµi
evaluated at the current values of the unknowns in the course of iteration.
A computationally simpler (although probably less eÔ¨Äective) proposal dis-
tribution could be a normal process with Ô¨Åxed mean vector and variance‚Äì
covariance matrix. For example, the proposal could be centered at the max-
imum likelihood estimates of individual Œ∏i parameters, i = 1, 2, ..., M. The
covariance matrix could be taken to be equal to the inverse of Fisher‚Äôs
expected information (given Œ∏i). This may work well if trajectories are
‚Äúlong enough‚Äù, but it may be unsatisfactory for individuals having sparse
longitudinal information.

15.4 Computation via Markov Chain Monte Carlo
657
Nonlinear First Stage: Acceptance‚ÄìRejection
Another option in the nonlinear situation is using a rejection scheme (Ripley,
1987), also known as ‚Äúacceptance and rejection‚Äù. Here the basic idea is to
cover the conditional posterior density of interest by an envelope, this being
the product of some sampling density S (Œ∏i) times a positive constant Qi,
such that density of the conditional posterior is smaller or equal than the
envelope at all values of Œ∏i. In our context, the required condition is, from
(15.65), that C (Œ∏i) ‚â§1, where
C (Œ∏i) = exp

‚àí1
2

Œµ‚Ä≤
i (Œ∏i) R‚àí1
i
(Œ≥) Œµi (Œ∏i) + e‚Ä≤
i (Œ∏i) Œ£‚àí1
e ei (Œ∏i)

S (Œ∏i) Qi
(15.69)
i = 1, 2, ..., M. The Ô¨Årst- and second-stage residuals are written as
Œµi (Œ∏i) = yi ‚àífi (Œ∏i, t) ,
and
ei (Œ∏i) = Œ∏i ‚àíXiŒ≤ ‚àíui,
respectively, to emphasize the dependence on Œ∏i; the integration constant
of the conditional posterior is unimportant because it can be absorbed in
Qi. Now take
S (Œ∏i) ‚àùexp

‚àí1
2e‚Ä≤
i (Œ∏i) Œ£‚àí1
e ei (Œ∏i)

,
(15.70)
that is, the sampling density is the conditional (given all other parameters)
prior of Œ∏i. In short, S (Œ∏i) is the density of the normal process
Œ∏i|Œ≤, ui, Œ£e ‚àºN (XiŒ≤ + ui, Œ£e) .
(15.71)
Further, let ,Œ∏i be the conditional ML estimator of Œ∏i obtained from the
Ô¨Årst-stage of the model, assuming Ri (Œ≥) is known (here, Œ≥ would be Ô¨Åxed
at the current value of the MCMC scheme). Then, set
Qi = exp

‚àí1
2Œµ‚Ä≤
i

,Œ∏i

R‚àí1
i
(Œ≥) Œµi

,Œ∏i

.
It follows that in (15.69):
C (Œ∏i) =
exp

‚àí1
2Œµ‚Ä≤
i (Œ∏i) R‚àí1
i
(Œ≥) Œµi (Œ∏i)

exp

‚àí1
2Œµ‚Ä≤
i

,Œ∏i

R‚àí1
i
(Œ≥) Œµi

,Œ∏i
 ‚â§1,
(15.72)
for all i. This is so because the conditional likelihood, being proportional
to the numerator, is maximized when evaluated at ,Œ∏i, so the denominator
must be at least as large as the numerator for any value of the trajectory
parameters, with this being true for each individual. The sampling scheme
is conducted as follows:

658
15. Bayesian Analysis of Longitudinal Data
(a) draw the parameters from the conditional prior (15.71);
(b) evaluate (15.72), and
(c) extract a random deviate from an uniform U (0, 1) process.
If this deviate is smaller than C (Œ∏i) , the value sampled from the condi-
tional prior is accepted as belonging to the conditional posterior distribu-
tion having density as in (15.65); otherwise, the sample value is rejected, so
the process must be repeated until acceptance. A potential problem with
this scheme is that if the values drawn from the conditional prior have a
small likelihood, then C (Œ∏i) is always very small, causing a high rate of
rejection.
The sampling scheme is dynamic, in the sense that the parameters of
(15.71) change in the course of iteration. A more general treatment of
adaptive rejection/sampling schemes is in Gilks and Wild (1992). The basic
idea is that when a point is rejected, the envelope is updated, to correspond
more closely to the target density. This reduces the chances of rejecting
subsequent proposals, thus decreasing the number of functions that need
to be evaluated.
Nonlinear First Stage: Importance Sampling
A third possibility for nonlinear Ô¨Årst-stage models consists of employing an
importance sampling/resampling scheme (e.g., Tanner, 1996). This topic
was discussed in Chapter 12 in conjunction with a sensitivity analysis of
the Bayesian model. Here it is described in a little more detail in the context
of the longitudinal data problem. We consider Ô¨Årst how the method can
be used to compute a posterior expectation, and then see how the drawn
samples can be resampled, to arrive at draws from the posterior distribution
of interest.
Let g(Œ∏) be the density of some posterior distribution of interest, and
suppose one wishes to compute the posterior expectation of the parameter
vector Œ∏ (or of h (Œ∏), a function of Œ∏); that is:
Eg(Œ∏)
=

Œ∏g(Œ∏)dŒ∏
=

Œ∏g(Œ∏)dŒ∏

g(Œ∏)dŒ∏ .
Often, this computation is not feasible, because of analytical diÔ¨Éculties.
Suppose there is some distribution having the same support as the poste-
rior of interest, and which is easy to sample from. This will be called the
importance distribution, and let its density be I (Œ∏). For example, if the
sampling space of Œ∏ is ‚Ñúr, perhaps a multivariate normal or multivariate‚àít
distribution
of order r could be used. The posterior expectation of Œ∏ is
derived from (12.30):
Eg(Œ∏) =

Œ∏w(Œ∏)I(Œ∏)dŒ∏
EI [w(Œ∏)]
(15.73)

15.4 Computation via Markov Chain Monte Carlo
659
where
w(Œ∏) = g(Œ∏)
I(Œ∏)
(15.74)
and
EI [w(Œ∏)] =

w(Œ∏)I(Œ∏)dŒ∏ =

g(Œ∏)dŒ∏.
DeÔ¨Åning the random variable
z(Œ∏) =
w(Œ∏)
EI [w(Œ∏)]Œ∏,
(15.75)
one can write
Eg(Œ∏) =

z(Œ∏)I(Œ∏)dŒ∏ =EI [z(Œ∏)] .
Now suppose that m independent samples are drawn from the distribution
with density I(Œ∏), also called the ‚Äúimportance sampling‚Äù function, and let
such draws be Œ∏[k] (k = 1, 2, . . . , m). Then a simulation consistent estimator
of the posterior expectation Eg(Œ∏) or, equivalently, of EI [z(Œ∏)] , is given
by (12.31)
5EI [z(Œ∏)] =
m

k=1
w(Œ∏[k])
m

k=1
w(Œ∏[k])
Œ∏[k].
(15.76)
In this expression, the denominator in (15.73) has been replaced by its
consistent estimator
5EI [w(Œ∏)] = 1
m
m

k=1
w(Œ∏[k]).
(15.77)
The ‚Äúimportance‚Äù weights are
w(Œ∏[k])
m

k=1
w(Œ∏[k])
=
g(Œ∏[k])
I(Œ∏[k])
m

k=1
g(Œ∏[k])
I(Œ∏[k])
=
cp(y|Œ∏[k])p(Œ∏[k])
I(Œ∏[k])
m

k=1
cp(y|Œ∏[k])p(Œ∏[k])
I(Œ∏[k])
=
p(y|Œ∏[k])p(Œ∏[k])
I(Œ∏[k])
m

k=1
p(y|Œ∏[k])p(Œ∏[k])
I(Œ∏[k])
,
(15.78)
where p (Œ∏) is the prior density, p (y|Œ∏) is the density of the sampling model,
and c is the integration constant of the posterior density. It follows that

660
15. Bayesian Analysis of Longitudinal Data
knowledge of the constant of integration is not needed to carry out the
importance sampling process, as it cancels out in the numerator and de-
nominator. Note, incidentally, that for the ‚Äúnew‚Äù weight
w‚àó(Œ∏) = p (y|Œ∏) p (Œ∏)
I(Œ∏)
,
EI [w‚àó(Œ∏)]
=
 p (y|Œ∏) p (Œ∏)
I(Œ∏)
I(Œ∏)dŒ∏
=

p (y|Œ∏) p (Œ∏) dŒ∏ = c‚àí1.
Hence, a simulation consistent estimator of the integration constant (or of
its reciprocal) is given by
5c =
m
m

k=1
p(y|Œ∏[k])p(Œ∏[k])
I(Œ∏[k])
.
As an important tuning issue, observe that an importance sampling den-
sity having thinner tails than the unnormalized posterior may cause some
weights to ‚Äúblow up‚Äù, with the consequence that a few values dominate
others in the weighted average. This is a reason why a multivariate-t dis-
tribution, perhaps with six to eight degrees of freedom may be a better
importance function than the multivariate normal. For example, in the
context of drawing the nonlinear parameters of the individual trajecto-
ries, one could use the following r-variate t distribution, constructed from
(15.66):
Œ∏i|Œ≤, ui, Œ£e, Œ≥, yi ‚àºtr

Œ∏i, Vi
v
v ‚àí2, v

with v degrees of freedom, and where Vi
v
v‚àí2 is the variance‚Äìcovariance
matrix of the t process.
Note in (15.76) that when the weights are constant
5EI [z(Œ∏i)] =
m

k=1
w(Œ∏[k]
i )
m

k=1
w(Œ∏[k]
i )
Œ∏[k]
i
= 1
m
m

k=1
Œ∏[k]
i ,
which is the posterior expectation calculated directly from the importance
sampling distribution. This implies that as the coeÔ¨Écient of variation of the
weights goes to 0, the importance distribution ‚Äúgets closer‚Äù to the posterior
distribution. This follows from (15.74), that is, if g(Œ∏i)/I(Œ∏i) is a constant
that does not depend on Œ∏i, it cancels out in the expression, with the result
that the importance distribution is identical to the posterior distribution
of interest.

15.4 Computation via Markov Chain Monte Carlo
661
The preceding indicates how a posterior expectation can be computed,
but does not give guidance on how a sample is to be drawn from the
conditional posterior distribution with density (15.65), so that the MCMC
procedure can continue. Now (15.73) implies that the random variable Œ∏i
with posterior density (15.65) has the same distribution as Œ∏i with density
w(Œ∏)I(Œ∏)
EI [w(Œ∏)],
which clearly integrates to 1. This observation is the basis of the importance
sampling/resampling algorithm of Rubin (1988):
(1) Draw Œ∏[k]
i , (k = 1, 2, ..., m) , from the importance distribution.
(2) Calculate the weights
w‚àó(Œ∏[k]
i ) =
p

y|Œ∏[k]
i

p

Œ∏[k]
i

I(Œ∏[k]
i )
,
and the relative weights
q[k] =
w‚àó(Œ∏[k]
i )
m

k=1
w‚àó(Œ∏[k]
i )
.
(3) Draw Œ∏‚àó
i from a discrete distribution with Pr

Œ∏‚àó
i = Œ∏[k]
i

= q[k], so the
sample space of Œ∏‚àó
i is the set
9
Œ∏[1]
i , Œ∏[2]
i , ..., Œ∏[m]
i
:
.
As m ‚Üí‚àû, then Œ∏‚àó
i is a draw from the target posterior distribution. In
our context, this requires drawing a large number of importance samples
from the conditional posterior distribution of each of the trajectory param-
eters, and then resampling one value at random, with probability q[k]. This
value would be retained to continue the MCMC algorithm.
Second-Stage Location EÔ¨Äects
Return to the joint posterior density in (15.63) and consider the part that
varies with Œ≤ and u only. The conditional posterior density of Œ≤ and u is
then
p (Œ≤, u|Œ∏, G0, Œ£e, Œ≥, y1, y2, . . . , yM, Œ±, Œì)
‚àù
 M
-
i=1
p (Œ∏i|Œ≤, ui, Œ£e)

p (Œ≤|Œ±, Œì) p (u|G0)
‚àùp (Œ∏|Œ≤, u, Œ£e) p (Œ≤|Œ±, Œì) p (u|G0) .
(15.79)

662
15. Bayesian Analysis of Longitudinal Data
This is precisely the posterior distribution of ‚ÄúÔ¨Åxed‚Äù and ‚Äúrandom‚Äù eÔ¨Äects
in a multivariate Gaussian mixed eÔ¨Äects linear model with known disper-
sion parameters, but with Œ∏i in lieu of the observations taken in individual
i. Here Œ∏i can be viewed as r attributes measured simultaneously on in-
dividual i. As developed in previous chapters, the conditional posterior
distribution is Gaussian, with mean vector
 ‚Üê‚Üí
Œ≤
‚Üê‚Üí
u

=

X‚Ä≤ 
I ‚äóŒ£‚àí1
e

X + Œì‚àí1
X‚Ä≤ 
I ‚äóŒ£‚àí1
e

Z
Z‚Ä≤ 
I ‚äóŒ£‚àí1
e

X
Z‚Ä≤ 
I ‚äóŒ£‚àí1
e

Z + A‚àí1 ‚äóG‚àí1
0
‚àí1
√ó

X‚Ä≤ 
I ‚äóŒ£‚àí1
e

Œ∏ + Œì‚àí1Œ±
Z‚Ä≤ 
I ‚äóŒ£‚àí1
e

Œ∏

,
(15.80)
and variance‚Äìcovariance matrix

X‚Ä≤ 
I ‚äóŒ£‚àí1
e

X + Œì‚àí1
X‚Ä≤ 
I ‚äóŒ£‚àí1
e

Z
Z‚Ä≤ 
I ‚äóŒ£‚àí1
e

X
Z‚Ä≤ 
I ‚äóŒ£‚àí1
e

Z + A‚àí1 ‚äóG‚àí1
0
‚àí1
.
(15.81)
It has been seen already that the draws from a Gaussian posterior dis-
tribution can be eÔ¨Äected either in a piecewise, blockwise, or multivariate
manner, with the only consequence of the method chosen being on the
mixing rate of the MCMC scheme. Naturally, if the order of Œ∏ allows one
to do so, samples can be obtained by standard methods for drawing from
multivariate Gaussian distribution.
First-Stage Dispersion Parameters
From (15.63), the conditional posterior density of the Ô¨Årst-stage dispersion
parameter vector Œ≥ is
p (Œ≥|Œ∏, Œ≤, u, G0, Œ£e, y1, y2, . . . , yM, Œ±, Œì)
‚àù
M
-
i=1
|Ri (Œ≥)|‚àí1
2 exp

‚àí1
2Œµ‚Ä≤
iR‚àí1
i
(Œ≥) Œµi

.
(15.82)
The form of this distribution depends on the speciÔ¨Åcation of the covariance
matrix Ri (Œ≥) in (15.4). For example, if the Ô¨Årst-stage residuals are assumed
to be independently distributed and homoscedastic, then Ri (Œ≥) = IniŒ≥.
Using this in (15.82) gives, as conditional posterior,
p (Œ≥|Œ∏, Œ≤, u, G0, Œ£e, y1, y2, ..., yM, Œ±, Œì) ‚àù
M
-
i=1
Œ≥‚àíni
2 exp

‚àí1
2Œ≥ Œµ‚Ä≤
iŒµi

‚àùŒ≥‚àíN
2 exp
Ô£±
Ô£≤
Ô£≥‚àí1
2Œ≥
M

i=1
ni

j=1
[yij ‚àífij (Œ∏i, tij)]2
Ô£º
Ô£Ω
Ô£æ,
(15.83)

15.4 Computation via Markov Chain Monte Carlo
663
where N = M
i=1 ni is the total number of observations taken. This density
is that of the scaled inverted chi-square random variable
Œ≥|Œ∏, Œ≤, u, G0, Œ£e, y1, y2, ..., yM, Œ±, Œì
‚àº
Ô£±
Ô£≤
Ô£≥
M

i=1
ni

j=1
[yij ‚àífij (Œ∏i, tij)]2
Ô£º
Ô£Ω
Ô£æœá‚àí2
N‚àí2,
Œ≥ ‚àà‚Ñú+
Œ≥
(15.84)
which may be truncated if limits are placed on the parameter space ‚Ñú+
Œ≥ .
On the other hand, if the residuals are independent but heteroscedas-
tic across individuals, such that Ri (Œ≥) = IniŒ≥i for i = 1, 2, ..., M, so
Œ≥ = [Œ≥1, Œ≥2, ..., Œ≥M]‚Ä≤ , then the Ô¨Årst-stage dispersion parameters are condi-
tionally independent of each other, with each having the conditional pos-
terior distribution
Œ≥i|Œ∏, Œ≤, u, G0, Œ£e, y1, y2, ..., yM, Œ±, Œì
‚àº
Ô£±
Ô£≤
Ô£≥
ni

j=1
[yij ‚àífij (Œ∏i, tij)]2
Ô£º
Ô£Ω
Ô£æœá‚àí2
ni‚àí2,
Œ≥i ‚àà‚ÑúŒ≥i.
(15.85)
While in the preceding cases the conditional posterior distributions can
be identiÔ¨Åed and are easy to sample from, this is not so when there is a
more complex structure in the residual variance‚Äìcovariance matrix. For ex-
ample, with auto-regressive processes, Markov-type dependencies, or with
a structural model for Ri (Œ≥) , the corresponding distributions cannot be
recognized. Hence, a Metropolis‚ÄìHastings step, for example, must be in-
corporated.
Second-Stage Dispersion Parameters
The conditional posterior density of the second-stage residual variance-
covariance matrix can be deduced directly from (15.63) yielding
p (Œ£e|Œ∏, Œ≤, u, G0, Œ≥, y1, y2, . . . , yM, Œ±, Œì) ‚àù
M
-
i=1
p (Œ∏i|Œ≤, ui, Œ£e)
‚àù|Œ£e|‚àíM
2 exp

‚àí1
2tr

Œ£‚àí1
e B

,
(15.86)
where, as before, B = M
i=1 eie‚Ä≤
i is an r √ó r matrix and ei = Œ∏i ‚àíXiŒ≤ ‚àíui.
The preceding density is the kernel of a scaled inverted Wishart distribution
of order r, scale matrix B, and ‚Äúdegrees of freedom‚Äù parameter equal to
M ‚àír ‚àí1. If the parameter space of Œ£e is subject to restrictions beyond
|Œ£e| > 0, then the distribution is a truncated scaled inverted Wishart. As
seen in a previous chapter, it is relatively easy to sample from standard or
truncated scaled inverted Wishart distributions.

664
15. Bayesian Analysis of Longitudinal Data
Reorder now the additive genetic eÔ¨Äects by nesting individuals within
parameters, and let the ensuing vector be u‚àó, such that u‚àó
1 contains additive
genetic eÔ¨Äects for parameter 1, and so on. Thus, the conditional posterior
density of the second-stage additive genetic variance‚Äìcovariance matrix G0
is
p (G0|Œ≥, Œ∏, Œ≤, u, Œ£e, y1, y2, ..., yM, Œ±, Œì) ‚àùp (u|G0) = p (u‚àó| G0)
‚àù|G0 ‚äóA|‚àí1
2 exp

‚àí1
2u‚àó‚Ä≤ (G0 ‚äóA)‚àí1 u‚àó

‚àù|G0|‚àíq
2 exp

‚àí1
2u‚àó‚Ä≤ 
G
‚àí1
0
‚äóA
‚àí1
u‚àó

‚àù|G0|‚àíq
2 exp

‚àí1
2 tr

G
‚àí1
0 U‚àó
,
(15.87)
where, as before, q is the order of the additive genetic relationship matrix
A, and U‚àóis the r √ó r symmetric matrix
U‚àó=
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£∞
u‚àó‚Ä≤
1 A
‚àí1u‚àó
1
u‚àó‚Ä≤
1 A
‚àí1u‚àó
2
¬∑ ¬∑ ¬∑
u‚àó‚Ä≤
1 A
‚àí1u‚àó
r
u‚àó‚Ä≤
2 A
‚àí1
1 u‚àó
1
u‚àó‚Ä≤
2 A
‚àí1u‚àó
2
¬∑ ¬∑ ¬∑
u‚àó‚Ä≤
1 A
‚àí1u‚àó
r
...
...
...
...
u‚àó‚Ä≤
r A
‚àí1u‚àó
1
u‚àó‚Ä≤
r A
‚àí1u‚àó
2
¬∑ ¬∑ ¬∑
u‚àó‚Ä≤
r A
‚àí1u‚àó
r
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
.
Density (15.87) is the kernel of a scaled inverted Wishart distribution of
order r, with scale matrix U‚àóand q ‚àír ‚àí1 degrees of freedom.
This completes the description of all conditional posterior distributions
needed to implement the MCMC procedure. Once a chain of appropriate
length is run, and convergence to the equilibrium distribution seems to have
been attained, the analysis of the output proceeds in a standard manner.
Output analysis is discussed in Chapter 12.
15.5
Analysis with Thick-Tailed Distributions
It is known that the normal distribution is sensitive to departures from
assumptions, so one may wish to consider models based on ‚Äúrobust‚Äù distri-
butions, and one of these is the t process, either in its univariate or multi-
variate form (e.g., Rogers and Tukey, 1972; Zellner, 1976; Lange and Sin-
sheimer, 1993; Strand¬¥en and Gianola, 1999). Regression and cross-sectional
models with t distributed errors have been discussed in Chapter 13, Section
13.6.
In the treatment of longitudinal data given so far, normality has been as-
sumed for the residuals of the Ô¨Årst two stages of the models. In what follows,
the assumption of normality of residuals at the Ô¨Årst stage will be replaced

15.5 Analysis with Thick-Tailed Distributions
665
by one of i.i.d. errors having a univariate-t distribution with unknown de-
grees of freedom. At the second stage, a multivariate-t distribution will be
employed for the second-stage residuals, instead of an r-variate normal dis-
tribution. For the sake of simplicity, it will be assumed that individuals are
genetically unrelated to each other. While this assumption is not tenable
in animal breeding, it is used frequently in biostatistics (Laird and Ware,
1982).
15.5.1
First- and Second-Stage Models
Recall that a t distribution, either univariate or multivariate, arises from
mixing a normal distribution over a gamma (equivalently, over an inverted
gamma or scale inverted chi-squared) process. This can be used to advan-
tage in an MCMC implementation by augmenting the joint posterior with
some unobservable ‚Äúweights‚Äù following the appropriate gamma distribu-
tions.
The Ô¨Årst-stage model will be as in (15.2), but amended as
yij
=
fij (Œ∏i, tij) +
Œµij
‚àöwij
=
fij (Œ∏i, tij) + Œµ‚àó
ij,
(15.88)
where Œµij ‚àºN (0, Œ≥) and wij ‚àºGa
 ŒΩŒµ
2 , ŒΩŒµ
2

are independently distributed
random variables. As seen before, the distribution of the ‚Äúnew‚Äù residual
Œµ‚àó
ij can be shown to be t (0, Œ≥, ŒΩŒµ), where Œ≥ is the scale parameter of the
t distribution and ŒΩŒµ are the degrees of freedom, which will be treated as
unknown. The new residuals will be assumed to be mutually independent,
both within and between individuals. Note that, given wij, then
yij|Œ∏i, tij, Œ≥, wij ‚àºN

fij (Œ∏i, tij) , Œ≥
wij

.
Likewise, the second-stage model will be taken to be
Œ∏i = XiŒ≤ +
ei
‚àöwi
= XiŒ≤ + e‚àó
i ,
i = 1, 2, . . . , M,
(15.89)
where ei ‚àºN (0, Œ£e) and wi ‚àºGa
 ŒΩe
2 , ŒΩe
2

are independently distributed,
for all i, as well as between individuals. Under this assumption, the dis-
tribution of the ‚Äúnew‚Äù residual is the r-dimensional process tr (0, Œ£e, ŒΩe) ,
where Œ£e is the scale matrix and ŒΩŒµ are the degrees of freedom, which will
also be treated as unknown. The trajectory parameters will be assumed
to be independent across individuals, a priori. As stated above, a genetic
eÔ¨Äect is not included in the speciÔ¨Åcation of the second-stage model. Here,
the second-stage residual reÔ¨Çects both genetic and nongenetic sources of

666
15. Bayesian Analysis of Longitudinal Data
variation (and covariation) between parameters, plus any error in the spec-
iÔ¨Åcation of the model. Note that, given wi, then
Œ∏i|Œ≤, Œ£e, wi ‚àºN

XiŒ≤, Œ£e
wi

.
The prior distributions for Œ≤, Œ£e, and Œ≥ will be as in (15.17), (15.20), and
(15.21), and the prior distributions of the two degrees of freedom parame-
ters will be assumed to be independent a priori, and uniform within some
bounded interval of the positive part of the real line. After augmenting the
prior with the trajectory parameters (as before) and with all weights wij
and wi, the joint prior density of all unknowns can be written as
p (Œ∏, Œ≤, Œ≥, Œ£e, wŒµ, we, ŒΩŒµ, ŒΩe|Œ±, Œì) = p (Œ∏|Œ≤, Œ£e, we) p (Œ≤|Œ±, Œì) p (Œ≥, Œ£e)
√óp (wŒµ|ŒΩŒµ) p (we|ŒΩe) p (ŒΩŒµ) p (ŒΩe)
‚àùp (Œ∏|Œ≤, Œ£e, we) p (Œ≤|Œ±, Œì) p (wŒµ|ŒΩŒµ) p (we|ŒΩe) ,
(15.90)
where wŒµ = {wij} is an M
i=1 ni √ó 1 vector and we = {wi} is an M √ó 1
vector of second-stage ‚Äúweights‚Äù. Furthermore, in view of the independence
assumption for parameters and weights,
p (Œ∏, Œ≤, Œ≥, Œ£e, wŒµ, we, ŒΩŒµ, ŒΩe|Œ±, Œì)
‚àù
 M
-
i=1
p (Œ∏i|Œ≤, Œ£e, wi) p (wi|ŒΩe)

p (Œ≤|Œ±, Œì)
M
-
i=1
ni
-
j=1
p (wij|ŒΩŒµ) .
(15.91)
The joint posterior density is expressible as
p (Œ∏, Œ≤,Œ≥, Œ£e, wŒµ, we, ŒΩŒµ, ŒΩe|y1, y2, ..., yM, Œ±, Œì)
‚àù
Ô£Æ
Ô£∞
M
-
i=1
ni
-
j=1
p (yij|Œ∏i, Œ≥, wij) p (wij|ŒΩŒµ)
Ô£π
Ô£ª
 M
-
i=1
p (Œ∏i|Œ≤, Œ£e, wi) p (wi|ŒΩe)

p (Œ≤|Œ±, Œì) .
(15.92)
15.5.2
Fully Conditional Posterior Distributions
Trajectory Parameters
As in the purely normal hierarchical model, all trajectory parameters are
conditionally independent from each other. Using the
[parameter|ELSE]

15.5 Analysis with Thick-Tailed Distributions
667
notation to represent a fully conditional posterior distribution, one has that
p (Œ∏i|ELSE) ‚àùexp

‚àí1
2Œ≥ [yi ‚àífi (Œ∏i, t)]‚Ä≤
i WŒµi [yi ‚àífi (Œ∏i, t)]
%
√ó exp

‚àíwi
2 (Œ∏i ‚àíXiŒ≤)‚Ä≤ Œ£‚àí1
e
(Œ∏i ‚àíXiŒ≤)

,
(15.93)
where WŒµi = Diag {wi1, wi2, . . . , wini} is an ni √ó ni diagonal matrix, i =
1, 2, . . . , M.
If the model is nonlinear in the parameters, the distribution is not recog-
nizable, and the samples must be drawn by, for example, the Metropolis‚Äì
Hastings algorithm. On the other hand, if the model is linear, with
fi (Œ∏i, t) = TiŒ∏i,
then the conditional posterior distribution is normal, so one can use Gibbs
sampling since draws can be eÔ¨Äected easily. Employing standard results for
combining quadratic forms, one can arrive at
Œ∏i|ELSE ‚àºN

Œ∏i, Vi

,
i = 1, 2, . . . , M.
(15.94)
The parameters of this normal distribution are
Œ∏i =

T‚Ä≤
i
WŒµi
Œ≥
Ti + wiŒ£‚àí1
e
‚àí1 
T‚Ä≤
i
WŒµi
Œ≥
yi + wiŒ£‚àí1
e XiŒ≤

,
and
Vi =

T‚Ä≤
i
WŒµi
Œ≥
Ti + wiŒ£‚àí1
e
‚àí1
.
Second-Stage Location EÔ¨Äects
From the joint posterior density (15.92) one arrives at
p (Œ≤|ELSE) ‚àù
 M
-
i=1
p (Œ∏i|Œ≤, Œ£e, wi)

p (Œ≤|Œ±, Œì)
‚àùexp
#
‚àí1
2
 M

i=1
(Œ∏i ‚àíXiŒ≤)‚Ä≤ wiŒ£‚àí1
e
(Œ∏i ‚àíXiŒ≤) + (Œ≤ ‚àíŒ±)‚Ä≤ Œì‚àí1 (Œ≤ ‚àíŒ±)
$
‚àùexp

‚àí1
2

(Œ∏ ‚àíXŒ≤)‚Ä≤ 
We ‚äóŒ£‚àí1
e

(Œ∏ ‚àíXŒ≤) + (Œ≤ ‚àíŒ±)‚Ä≤ Œì‚àí1 (Œ≤ ‚àíŒ±)
%
,
(15.95)
where We = Diag (wi) is an M √ó M matrix of second stage weights. Since
the two intervening densities are in Gaussian form, this implies that the
conditional posterior distribution of Œ≤ is normal, with mean vector

X‚Ä≤ 
We ‚äóŒ£‚àí1
e

X + Œì‚àí1‚àí1 
X‚Ä≤ 
We ‚äóŒ£‚àí1
e

Œ∏ + Œì‚àí1Œ±

,
(15.96)

668
15. Bayesian Analysis of Longitudinal Data
and covariance matrix

X‚Ä≤ 
We ‚äóŒ£‚àí1
e

X + Œì‚àí1‚àí1 .
(15.97)
Hence, Gibbs sampling is straightforward.
Scale Parameter of the First Stage Distribution
Retaining, in the joint posterior density (15.92), only the terms that depend
on Œ≥ leads to
p (Œ≥|ELSE) ‚àù
M
-
i=1
ni
-
j=1
p (yij|Œ∏i, Œ≥, wij)
‚àùŒ≥‚àíN
2 exp
Ô£±
Ô£≤
Ô£≥‚àí1
2Œ≥
M

i=1
ni

j=1
wij [yij ‚àífij (Œ∏i, tij)]2
Ô£º
Ô£Ω
Ô£æ.
(15.98)
This is the density of the distribution
Œ≥|ELSE ‚àº
Ô£±
Ô£≤
Ô£≥
M

i=1
ni

j=1
wij [yij ‚àífij (Œ∏i, tij)]2
Ô£º
Ô£Ω
Ô£æœá‚àí2
N‚àí2,
Œ≥ ‚àà‚Ñú+
Œ≥ ,
which is straightforward to sample from in the context of Gibbs sampling.
Scale Parameter of the Second-Stage Distribution
Similarly,
p (Œ£e|ELSE) ‚àù
M
-
i=1
9
|Œ£e|‚àí1
2 exp

‚àíwi
2 (Œ∏i ‚àíXiŒ≤)‚Ä≤ Œ£‚àí1
e
(Œ∏i ‚àíXiŒ≤)
:
‚àù|Œ£e|‚àíM
2 exp

‚àí1
2
M

i=1
w‚Ä≤
i (Œ∏i ‚àíXiŒ≤) Œ£‚àí1
e
(Œ∏i ‚àíXiŒ≤)

‚àù|Œ£e|‚àíM
2 exp

‚àí1
2tr

Œ£‚àí1
e Be

,
(15.99)
where
Be =
M

i=1
wi (Œ∏i ‚àíXiŒ≤) (Œ∏i ‚àíXiŒ≤)‚Ä≤ .
It can be readily ascertained that (15.99) is the kernel of a scaled inverted
Wishart distribution of order r, scale matrix Be, and ‚Äúdegrees of freedom‚Äù
parameter equal to M‚àír‚àí1. If the parameter space of the scaled matrix Œ£e
is subject to restrictions beyond |Œ£e| > 0, then the resulting distribution
is truncated scaled-inverted Wishart.

15.5 Analysis with Thick-Tailed Distributions
669
Weight Parameters
The preceding conditional posterior distributions indicate that, so far, the
sampling process is as in the purely normal case, with the only novelty
being the appearance of the weights, which need to be sampled from the
corresponding conditional distributions. Consider the joint posterior den-
sity as a function of the Ô¨Årst-stage weights wij, yielding
p (wij|ELSE) ‚àùp (yij|Œ∏i, Œ≥, wij) p (wij|ŒΩŒµ)
‚àùw
1
2
ij exp
#
‚àíwij [yij ‚àífij (Œ∏i, tij)]2
2Œ≥
$ 
w
ŒΩŒµ
2 ‚àí1
ij
exp

‚àíŒΩŒµwij
2

for i = 1, 2, . . . , M and j = 1, 2, , . . . , ni. The expression in brackets is the
contribution from the gamma prior distribution of the Ô¨Årst-stage weights.
Rearrangement leads to
p (wij|ELSE) ‚àùw
ŒΩŒµ+1
2
‚àí1
ij
exp

‚àíwijSij
2

,
(15.100)
where
Sij = [yij ‚àífij (Œ∏i, tij)]2 + ŒΩŒµŒ≥
Œ≥
.
Hence, the conditional posterior distribution of each wij weight is the
gamma process
wij|ELSE ‚àºGa

ŒΩŒµ + 1
2
, Sij
2

,
i = 1, 2, . . . , M, j = 1, 2, . . . , ni.
(15.101)
Thus, the samples can be drawn without diÔ¨Éculty.
Similarly, the density of the conditional posterior distribution of the
second-stage weights can be put, after some algebra, as
p (wi|ELSE) ‚àùw
ŒΩe+1
2
‚àí1
i
exp

‚àíwiSi
2
%
,
where
Si = (Œ∏i ‚àíXiŒ≤)‚Ä≤ Œ£‚àí1
e
(Œ∏i ‚àíXiŒ≤) + ŒΩe.
Thus
wi|ELSE ‚àºGa

ŒΩe + 1
2
, Si
2

,
i = 1, 2, . . . , M.
(15.102)
Degrees of Freedom
Inspection of the joint posterior density of the degrees of freedom parame-
ters reveals that, given all other parameters, the degrees of freedom of the

670
15. Bayesian Analysis of Longitudinal Data
Ô¨Årst- and second-stage distributions are mutually independent. For the Ô¨Årst
stage distribution, one has
p (ŒΩŒµ|ELSE) ‚àù
M
-
i=1
ni
-
j=1
p (wij|ŒΩŒµ)
‚àù
Ô£Æ
Ô£∞
 ŒΩŒµ
2
(
ŒΩŒµ
2 )
Œì
 ŒΩŒµ
2

Ô£π
Ô£ª
N M
-
i=1
ni
-
j=1

w
ŒΩŒµ
2 ‚àí1
ij
exp

‚àíŒΩŒµwij
2

.
(15.103)
For the second-stage degrees of freedom parameters, the resulting condi-
tional posterior distribution is
p (ŒΩe|ELSE) ‚àù
M
-
i=1
p (wi|ŒΩe)
‚àù
Ô£Æ
Ô£∞
 ŒΩe
2
(
ŒΩe
2 )
Œì
 ŒΩe
2

Ô£π
Ô£ª
M M
-
i=1

w
ŒΩe
2 ‚àí1
ij
exp

‚àíŒΩewi
2

.
(15.104)
None of the two distributions has a recognizable form. Hence, either a
Metropolis‚ÄìHastings, rejection, or importance sampling with resampling
step needs to be tailored, to draw the degrees of freedom. This is probably
the most diÔ¨Écult part of the implementation, since it is not easy to arrive
at suitable proposal distributions or rejection envelopes (e.g., Strand¬¥en,
1996). An alternative might be to set the degrees of freedom parameters
to some Ô¨Åxed values, and then vary these, to study sensitivity of infer-
ences (Rodriguez-Zas, 1998; Rosa et al., 2001). In brief, the distributions
with densities as in (15.93), (15.95),(15.98), (15.99), and (15.101)‚Äì(15.104),
complete the speciÔ¨Åcation of a possible MCMC sampler for a longitudinal
data model with two tiers of robustness.
In conclusion, an MCMC analysis is particularly attractive for linear and
nonlinear Ô¨Årst-stage models for longitudinal data, relative to the two-step
approximate Bayesian analysis, where a number of (sometimes dubious)
approximations must be employed. In the linear case, the MCMC compu-
tations are equivalent to those needed to undertake a Bayesian mixed eÔ¨Äects
linear model analysis. In the nonlinear situations, computations are more
involved. When robust distributions are used to describe the uncertainty
about the Ô¨Årst- and second-stage models, additional diÔ¨Éculties arise, due
to the need to tune proposal distributions for sampling, e.g., the degrees of
freedom in the case of the t distributions.

16
Introduction to Segregation and
Quantitative Trait Loci Analysis
16.1
Introduction
The genetic model assumed so far is based on a very large number of
independent loci with each locus contributing additively with an inÔ¨Ånites-
imally small eÔ¨Äect to the additive genetic value of an individual. This has
been termed the inÔ¨Ånitesimal model, as noted earlier in this book. Like all
models, this is an intellectual abstraction. Box (1976) pointed out that all
models are wrong but that some are useful, and this is certainly the case of
the inÔ¨Ånitesimal model. It has been shown, however, to be remarkably ef-
fective for predicting expected response to artiÔ¨Åcial selection programmes,
for predicting breeding values of candidates for selection, for estimating
genetic variances and, interestingly enough, for interpreting results from
selection experiments (Martinez et al., 2000).
In many traits, in contrast, part of the genetic variance can be attributed
to one or more major genes segregating in the population. Many reasons
can be advanced for studying the number, location, mode of action and
magnitude of such gene eÔ¨Äects. The so called mixed inheritance model poses
that the genetic variance is partly due to many loci of inÔ¨Ånitesimally small
eÔ¨Äect, and partly due to the presence of a Ô¨Ånite number of loci of relatively
large eÔ¨Äect. Prior to the availability of molecular markers, a method known
as complex segregation analysis was one of the most important tools for
detection of major genes. InÔ¨Çuential papers were Elston and Stewart (1971),
Morton and MacLean (1974) and Lange and Elston (1975). This approach
was the basis for the successful mapping of many Mendelian genes in the

672
16. Segregation and Quantitative Trait Loci Analysis
1980s. Shortcomings that have been pointed out include the limited power
of the method for Ô¨Ånding major genes and a lack of robustness, leading
to false detection (Go et al., 1978). Both problems are much alleviated
if information on genetic markers is incorporated into the analysis. The
explosion of molecular polymorphisms in the last twenty years or so has
stimulated the development and successful application of many methods for
major gene or, more generally, for quantitative trait loci (QTL) detection.
This chapter provides an introduction to some models that can be used
for inferring the presence of one or more major genes. The chapter is or-
ganized as follows. The Ô¨Årst section introduces the topic of segregation
analysis, and a Bayesian implementation is presented. The second section
discusses models for QTL detection. First, a model postulating a single
QTL is presented and both likelihood and Bayesian inferences are illus-
trated. Second, models with an unknown number of QTL are introduced.
The chapter ends with an application of reversible jump MCMC for making
inferences about the number of QTL segregating in a population.
16.2
Segregation Analysis Models
The simplest mixed inheritance model postulates that there is a single
major locus. Applications of the mixed inheritance model using MCMC can
be found, for example, in Guo and Thompson (1994), Janss et al. (1995),
and Lund and Jensen (1999). A useful recent review including many topics
in pedigree analysis, can be found in Thompson (2001).
16.2.1
Notation and Model
Assume a major locus with two alleles, A1 and A2, with respective gene
frequencies (1 ‚àíq) and q, and that the base (founding) population from
which base individuals were conceptually sampled was in Hardy‚ÄìWeinberg
and linkage equilibrium. The genotypes at the major locus are A1A1, A1A2,
and A2A2. Due to Hardy‚ÄìWeinberg equilibrium, alleles combine indepen-
dently to form these genotypes with frequencies (1 ‚àíq)2, 2q (1 ‚àíq), and q2,
respectively. No distinction is made between maternally and paternally in-
herited alleles. DeÔ¨Åne a vector m = (m1, m2, m3)‚Ä≤, whose elements describe
the eÔ¨Äects that genotypes A1A1, A1A2, and A2A2 have on the phenotypic
scale.
The genealogy is assumed to consist of nq individuals. For each individual
in the pedigree, deÔ¨Åne an unknown random variable wi (i = 1, 2, . . . , nq),
taking the values (1, 0, 0), (0, 1, 0) or (0, 0, 1) associated with genotypes
A1A1, A1A2, or A2A2, respectively. Also let
Pr (wi = k|q) ,
k = 1, 2, 3,

16.2 Segregation Analysis Models
673
Genotype of father:
wfi = 2
Genotype of mother:
wmi = 1
wmi = 2
wmi = 3
wi = 1
1/2
1/4
0
wi = 2
1/2
1/2
1/2
wi = 3
0
1/4
1/2
TABLE 16.1. Probability of oÔ¨Äspring genotypes given parental genotypes, when
the father is heterozygote at the major locus.
be the probability that wi takes values (1, 0, 0), (0, 1, 0), or (0, 0, 1), respec-
tively.
In the pedigree, there are individuals with unidentiÔ¨Åed fathers and moth-
ers. These are deÔ¨Åned as founders. On the other hand, the nonfounders are
individuals with both parents identiÔ¨Åed. When an individual has only one
parent identiÔ¨Åed, a phantom parent is created. This leads to simpler nota-
tion and simpler expressions later on.
Let W be a matrix of order nq √ó 3 such that its ith row is w‚Ä≤
i; thus, W
denotes the conÔ¨Åguration of the underlying genotypes at the major locus.
The p.m.f. of the genotypic conÔ¨Åguration is
p (W|q) =
-
founders i
p (wi|q)
-
nonfounders j
p (wj|wmj, wfj) ,
(16.1)
where subscript m (f) stands for the mother (father) of j. The product
decomposition of the Ô¨Årst term on the right-hand side arises because geno-
types of founders are assumed to be a function of gene frequencies, and
the model here postulates independence of the two alleles at the locus. The
model of genetic transmission gives rise to the product decomposition of the
second term on the right-hand side. This postulates that oÔ¨Äspring geno-
types are conditionally independent, given parental genotypes. Example
1.16 from Chapter 1 illustrates (16.1) for a pedigree with loops.
The Ô¨Årst term on the right-hand of (16.1) can take one of the following
forms:
Pr [wi = (1, 0, 0) |q] = (1 ‚àíq)2 ,
Pr [wi = (0, 1, 0) |q] = 2q (1 ‚àíq) ,
and
Pr [wi = (0, 0, 1) |q] = q2.
The second term on the right-hand side of (16.1) is a model for Mendelian
segregation. To illustrate, Table 16.1 shows the probabilities of oÔ¨Äspring
genotypes, given the genotypes of both parents, for one of the three possi-
ble paternal genotypes (the heterozygote) and the three possible maternal
genotypes. The notation has been simpliÔ¨Åed as follows: wi = (1, 0, 0) be-
comes wi = 1; wi = (0, 1, 0) becomes wi = 2, and wi = (0, 0, 1) becomes
wi = 3.

674
16. Segregation and Quantitative Trait Loci Analysis
In models with more than one locus, the p.m.f. p (wj|wmj, wfj) is a
function of the recombination fraction between the loci involved, provided
these loci are linked.
The model for the data assumes the following linear additive structure:
y = XŒ≤ + Za + ZWm + e.
(16.2)
Note that matrix W is not observed. The other elements of the model have
been deÔ¨Åned in Section 13.2 of Chapter 13. The (conditional) sampling
distribution of the data is assumed to be Gaussian, with the form
y|Œ≤, a, W, m,œÉ2
e ‚àºN

XŒ≤ + Za + ZWm, IœÉ2
e

,
where œÉ2
e is the residual variance. That is, the elements of this vector are
assumed to be conditionally independent, given the parameters; the as-
sociated densities are referred to as penetrances in the linkage analysis
literature.
The prior distribution of the additive genetic values is
a|A, œÉ2
a ‚àºN

0, AœÉ2
a

,
where A and œÉ2
a are the known relationship matrix and the unknown ad-
ditive genetic variance in the conceptual base population from which base
individuals were sampled, respectively. Parameters Œ≤, œÉ2
a, and œÉ2
e are as-
signed priors of the form in (13.4) and (13.5). The vector m is assigned a
proper uniform prior in R3, of the same form as in (13.4). Finally, a beta
distribution with known parameters e and f, Be (e, f), is assigned a priori
to describe previous knowledge of the gene frequency; thus,
p (q|e, f) ‚àùqe‚àí1 (1 ‚àíq)f‚àí1 .
(16.3)
After augmentation with W, the joint prior density admits the form
p

Œ≤, a, W, m, q, œÉ2
e, œÉ2
a

‚àùp (W|q) p (q) p

a|œÉ2
a

p

œÉ2
a

p

œÉ2
e

.
Given the model, the joint posterior distribution of the parameters (we
leave implicit the conditioning on A, the known incidence matrices and
hyperparameters) is given by
p

Œ≤, a, W, m, q, œÉ2
e, œÉ2
a|y

‚àùp

y|Œ≤, a, W, m, œÉ2
e

p

Œ≤, a, W, m, q, œÉ2
e, œÉ2
a

‚àùp

y|Œ≤, a, W, m, œÉ2
e

p (W|q) p (q) p

a|œÉ2
a

p

œÉ2
a

p

œÉ2
e

.
(16.4)
A classical full likelihood approach involves the joint maximization of
L (‚Ñ¶|y) ‚àù

W

p

y|Œ≤, a, W, m,œÉ2
e

p

a|œÉ2
a

da

p (W|q) ,

16.2 Segregation Analysis Models
675
over ‚Ñ¶=

Œ≤‚Ä≤, m‚Ä≤, q, œÉ2
e, œÉ2
a
‚Ä≤. This shows that the likelihood is the expected
value of the conditional likelihood given W, with the distribution [W|q]
acting as the mixing process. The sum is taken over all possible genotypic
conÔ¨Ågurations of the pedigree, and the dimension of the integral is of the
order of the number of elements in a. In the Gaussian model the integration
can be performed analytically, but the summation over all possible genotype
conÔ¨Ågurations, in the case of complex pedigrees, is an insurmountable task.
In contrast, the MCMC Bayesian model can be implemented in a relatively
straightforward manner, as shown below.
16.2.2
Fully Conditional Posterior Distributions
The Ô¨Årst step of the Gibbs sampling algorithm is to Ô¨Ånd a starting value
for the genotypic conÔ¨Åguration W. A simple way of achieving this is to
sample genes/genotypes from the prior distribution of founder individuals,
and then sample the nonfounder genotypes from the conditional probabil-
ity distribution of the nonfounder, given the genotype of its parents. This
is known as ‚Äúgene dropping‚Äù (MacCluer et al., 1986) and is essentially a
Monte Carlo implementation of (16.1). Since the data contain information
on the major gene eÔ¨Äects, Guo and Thompson (1994) propose what they
call a ‚Äúposterior gene dropping‚Äù approach: the major gene is dropped down
from the top of the pedigree conditionally on the current values of parame-
ters and the data on each individual. This method works well in the case of
the model under consideration with a continuous penetrance function in the
absence of typed genotypic information. However, with other penetrance
functions and data structures, as in pedigree studies where the data consist
of genotypes of some members of a pedigree (usually the younger ones) and
the objects of inference are the genotypes of the remaining members, the
approach can be exceedingly ineÔ¨Écient. This is so, because the availabil-
ity of partial genotypic information imposes rigid compatibility constraints:
every proposed conÔ¨Åguration must be checked for consistency with the data
at hand, and rejection rates can be in the neighborhood of 100%. With this
type of data structure, a more eÔ¨Écient approach can be found in Lange and
Goradia (1987) and in Lange (1997). Sheehan (2000) provides a good dis-
cussion about this and other issues involved in the application of MCMC
to genetic analyses on complex pedigrees.
The joint posterior distribution (16.4) has the same form as the joint
posterior (13.6). Therefore, allowing for the extra terms involving W and
m, the fully conditional posterior distribution of Œ∏‚Ä≤ =

Œ≤‚Ä≤, m‚Ä≤, a‚Ä≤
is iden-
tical to (13.9) or (13.11). For example, a simple manipulation of (13.11)
shows that the fully conditional posterior distribution of m is
m|Œ≤, a, W, œÉ2
e, y ‚àºN

5m, (W‚Ä≤Z‚Ä≤ZW)‚àí1 œÉ2
e

,

676
16. Segregation and Quantitative Trait Loci Analysis
where
5m = (W‚Ä≤Z‚Ä≤ZW)‚àí1 W‚Ä≤Z‚Ä≤ (y ‚àíXŒ≤ ‚àíZa) .
The term W‚Ä≤Z‚Ä≤ZW is a diagonal matrix, with elements equal to the num-
ber of A1A1 genotypes, A1A2 genotypes, and A2A2 genotypes, among those
individuals with records. Having sampled from the fully conditional poste-
rior distribution of m, one may wish to store contrasts like
k‚Ä≤ =

1
0
‚àí1
‚àí0.5
1
‚àí0.5

.
The Ô¨Årst line in k‚Ä≤ represents the diÔ¨Äerence between homozygotes at the
major locus and the second represents the degree of dominance.
Likewise, the fully conditional posterior distributions of œÉ2
e and œÉ2
a have
the same forms as in (13.14) and (13.16). Thus
œÉ2
a|a, y ‚àº

a‚Ä≤A‚àí1a + ŒΩaSa

œá‚àí2
nq+ŒΩa
(16.5)
and
œÉ2
e|Œ≤, m, a, W, y
‚àº

(y ‚àíXŒ≤ ‚àíZa ‚àíZWm)‚Ä≤ (y ‚àíXŒ≤ ‚àíZa ‚àíZWm) + ŒΩeSe

œá‚àí2
n+ŒΩe.
(16.6)
The preceding assumes independent scale inverted chi-square prior distri-
butions for œÉ2
a and œÉ2
e.
We derive now the fully conditional posterior distribution of wi. Follow-
ing Guo and Thompson (1994), deÔ¨Åne {wij} as the genotypes of the mates
of individual i, {wijl} as the genotypes of the oÔ¨Äspring of individuals i and
j, wmi as the genotype of the mother of i and, Ô¨Ånally, wfi as the genotype
of the father of i. From the joint posterior (16.4) we can write
p

wi|Œ≤, a, W‚àíi, m, œÉ2
e, y

‚àùp

y|Œ≤, a, W, m, œÉ2
e

p (W)
‚àùp

yi|Œ≤, a, wi, m, œÉ2
e

p (wi|W‚àíi) .
(16.7)
The terms that include wi in p (wi|W‚àíi) are
p (wi|W‚àíi) ‚àùp (wi| {wij} , {wijl} , wmi, wfi)
‚àùp (wi, {wij} , {wijl} , wmi, wfi)
=
ni
-
j=1
p (wijl|wi, wij, wmi, wfi) p (wi, wij, wmi, wfi)
‚àù
ni
-
j=1
p (wijl|wi, wij) p (wi|wmi, wfi) ,
(16.8)

16.2 Segregation Analysis Models
677
where ni are the number of mates of i and nij (below) are the number of
oÔ¨Äspring that i has with mate j. Substituting in (16.7) yields
p

wi|Œ≤, a, W‚àíi, m, œÉ2
e, y

‚àùp

yi|Œ≤, a, wi, m, œÉ2
e

√ó
ni
-
j=1
nij
-
l=1
p (wijl|wi, wij) p (wi|wmi, wfi) .
(16.9)
This is a discrete distribution of unknown analytical form, but which is easy
to sample from. Expression (16.9) is evaluated for the three possible values
that wi can take and, after normalization, wi is accepted with probability
equal to the appropriate normalized value. This is done by drawing from a
uniform distribution Un (0, 1).
If individual i has no phenotypic record, the fully conditional distribu-
tion is proportional to p (wi|W‚àíi). If individual i is a founder, the term
p (wi|wmi, wfi) in (16.9) is replaced by p (wi|q), which is a function of gene
frequency, as indicated in (16.1).
Finally, the fully conditional posterior distribution of the gene frequency
is obtained as follows. From (16.4),
p (q|W, y)
‚àù
p (W|q) p (q)
‚àù
p (q)
-
founders i
p (wi|q) .
(16.10)
Seen as a function of q, the second term has the form
-
founders i
p (wi|q) ‚àù(1 ‚àíq)nA1 qnA2,
where nA1 and nA2 are the number of A1 and A2 alleles among founder
individuals. Given the prior density Be (q|e, f) in (16.3), (16.10) becomes
p (q|W, y) ‚àù(1 ‚àíq)(nA1+f‚àí1) q(nA2+e‚àí1),
which is the kernel of a beta distribution with parameters nA1 + f and
nA2 + e. That is,
q|W, y ‚àºBe (nA1 + f, nA2 + e) .
(16.11)
If a uniform prior Un (0, 1) is assumed for q, instead of (16.3), the fully
conditional posterior distribution for q has density
q|W, y ‚àºBe (q|nA1, nA2) .
16.2.3
Some Implementation Issues
It was discussed in Chapter 10 that a Ô¨Ånite-state space discrete Markov
chain converges to a unique stationary distribution, provided it is irre-
ducible and aperiodic. The sampling scheme for the major locus genotype

678
16. Segregation and Quantitative Trait Loci Analysis
deÔ¨Ånes a discrete chain. Starting from a particular ‚Äúlegal‚Äù genotypic con-
Ô¨Åguration, one wishes to know whether it is possible to visit any other legal
conÔ¨Åguration by updating individual genotypes one at a time.
In the case of a diallelic locus with alleles A1 and A2, Sheehan and
Thomas (1993) show that irreducibility of the Markov chain is guaranteed,
provided that, if for all y satisfying
p (y|A1A1) p (y|A2A2) > 0,
(16.12)
it also holds that
p (y|A1A2) > 0.
(16.13)
In other words, irreducibility is established unless the genetic model allows
for a phenotype compatible with both homozygous genotypes, to be incom-
patible with the heterozygote state. This is the only situation in which the
Gibbs sampler may deÔ¨Åne a reducible Markov chain for a diallelic system.
To illustrate, consider the following example taken from Sheehan (2000).
Individuals are classiÔ¨Åed into ‚ÄúaÔ¨Äected‚Äù or ‚Äúnormal‚Äù, depending on whether
they are homozygotes or heterozygotes, respectively. Data are available on
a mother-daughter pair, where both are ‚ÄúaÔ¨Äected‚Äù. If the starting legal
conÔ¨Åguration assigns genotype A1A1 to the mother, then the updating
scheme based on the genotypic distribution of the daughter conditional
on the mother‚Äôs genotype assigns probability 1 to the event ‚Äúgenotype of
daughter is A1A1‚Äù. The daughter can never change to the other homozygote
genotype, given that the mother has genotype A1A1.
With more than two alleles at a single locus, irreducibility is no longer en-
sured by imposing a simple condition such as that deÔ¨Åned by (16.12) and
(16.13). Reducibility depends on the data and method of sampling. For
example, consider the following human ABO blood group example, taken
from Sheehan and Thomas (1993). The data (y) consist of the genotypes of
two oÔ¨Äspring, which are A1B and OO, respectively. This implies that the
genotype of the parents must be (A1O, BO) or (BO, A1O). Consider an
updating scheme consisting of drawing from the conditional posterior dis-
tribution of a parent, given the genotype of the other parent and the data.
For example, once the father‚Äôs genotype (f) has been assigned (A1O, say),
the maternal genotype (m) is forced to be the complementary type (BO)
with probability 1. Given the data and the sampling mechanism, this up-
dating scheme creates a Markov chain with two noncommunicating states.
The way around this toy problem is to sample the parental genotypes
jointly, in one block, from the joint distribution
Pr (f = A1O, m = BO|y) = 1
2,
Pr (f = BO, m = A1O|y) = 1
2.

16.3 QTL Models
679
These examples emphasize that care must be taken about inferences de-
rived from a single site updating Gibbs scheme, in multiallelic systems.
In fact, there is at present no method that can guarantee irreducibility in
large complex pedigrees. More importantly, even if the chain is irreducible,
mixing may be slow; consequently, a limited range of the support of the
posterior distribution may be visited by the Monte Carlo draws. This will
result in poor inferences. Slow mixing of the chain aÔ¨Äects convergence of the
time-average of draws to the expectation of the function under the equilib-
rium distribution, even if the chain starts in the equilibrium distribution.
Various methods for dealing with reducibility and slow mixing have been
proposed in recent years, and many of these are reviewed by Sheehan (2000)
and by Thompson (2001). Several of the approaches are based on a vari-
ety of joint-updating schemes. For example, Jensen et al. (1995) update
genotypes of blocks of individuals jointly at several loci. The method was
extended by Lund and Jensen (1999) to cope with the mixed inheritance
model. Janss et al. (1995) also propose a blocking scheme, whereby a sire
with all its Ô¨Ånal oÔ¨Äspring are updated in one pass. Other forms of joint up-
dating were discussed by Heath (1997), Thompson and Heath (2000) and
Sheehan et al. (2002). In general, the joint updating sampling strategies
are a great improvement over single-site methods.
16.3
Models for the Detection of
Quantitative Trait Loci
In the last decade, and due to the availability of information on molecular
markers, there has been much interest in detecting chromosomal regions
responsible for some of the variation observed for quantitative traits. In
particular, an extensive literature on methods for detecting QTL using
marked regions has accumulated. The objective here is to provide an intro-
duction to statistical aspects of the subject and to illustrate how likelihood
or MCMC-based methods can be applied for drawing inferences concern-
ing eÔ¨Äects and positions of such QTL. The presentation is restricted to the
analysis of a backcross design involving inbred lines; full marker information
in a single chromosome is assumed.
First, a model with a single QTL Ô¨Çanked by two marker loci is introduced.
This is then extended to models with an arbitrary number of QTL using
information on a large number of genetic markers. In this Ô¨Ånal section,
ways in which models can be compared using their posterior probabilities
are outlined.

680
16. Segregation and Quantitative Trait Loci Analysis
16.3.1
Models with a Single QTL
In the single QTL model, the marker at the Ô¨Årst locus has alleles M and
m, and at the second locus, the marker alleles are N and n. At each locus,
markers are assumed to be codominant. Interest focuses on whether there is
evidence for the presence of a QTL, with unknown alleles Q and q, placed
between the two (known) markers. That is, it is assumed here that the
locus order is MQN. (The frequency of the QTL alleles does not feature
in this section; therefore the symbol q in this section is always associated
with the QTL allele, and not with allele frequency).
There may also be genetic variation contributed by genes of small eÔ¨Äect.
Due to the nature of the experimental design, this variation cannot be
estimated and is part of the residual variance.
The recombination fraction between locus M and the QTL will be de-
noted by rm, and the recombination fraction between the QTL and locus N
by rn. It is assumed that recombination in the M-Q interval is independent
of recombination in the Q-N interval. Therefore the probability of a double
recombinant is rmrn. Another simplifying assumption is that the recombi-
nation fraction between the two markers, r, is known, and that it is less
than 0.5. A recombination occurs, if the number of crossovers between the
loci involved is odd. Recombination between M and N, which is the prob-
ability of an odd number of crossovers, arises as follows. A recombination
occurs between M and Q and not between Q and N, or a recombination
occurs between Q and N and not between M and Q. With no interference,
the probability of recombination is the sum of the probabilities of these two
mutually exclusive events. Thus,
r = rm (1 ‚àírn) + (1 ‚àírm) rn = rm + rn ‚àí2rmrn
(e.g., Ott, 1999), which implies that
rm = (r ‚àírn)/ (1 ‚àí2rn) ,
0 < rm < r, 0 < rn < r.
(16.14)
Thus, with r known, there is only one recombination fraction to be esti-
mated, because rm can be written as a function of rn or vice-versa.
It is often convenient to parameterize the model in terms of genetic map
distances rather than in terms of recombination fractions. The genetic map
distance between two loci is deÔ¨Åned as the expected number of crossovers
occurring on a given chromosome (in a gamete) between the loci (Ott,
1999). Genetic map distances are expressed in centimorgans (cM). The
advantage of genetic map distances is that these are additive, whereas re-
combination fractions are not. Parameterization with genetic map distances
requires mapping functions; there are several such functions (see Ott (1999)
for an overview). Here the one proposed by Haldane (1919) is used, which
assumes that recombination between any two markers is independent of
that occurring at other marker intervals (i.e., no chiasma interference is as-
sumed). Using this mapping function, the distance Œª (measured in units of

16.3 QTL Models
681
Morgans, a positive quantity) and the recombination fraction r are related
by the equation
r = f (Œª) = 1
2 [1 ‚àíexp (‚àí2Œª)] ,
(16.15)
with inverse function
Œª = f ‚àí1 (r) = ‚àí1
2 ln (1 ‚àí2r) .
(16.16)
Thus, if the genetic map distance between loci i and i + 1 is |Œªi+1 ‚àíŒªi|,
the recombination fraction is
r = f (Œªi+1 ‚àíŒªi) = 1
2 [1 ‚àíexp (‚àí2 |Œªi+1 ‚àíŒªi|)] .
Data from a backcross design are assumed to be generated as follows.
Consider two completely inbred lines, one with genotype MQN/MQN and
the other with genotype mqn/mqn. These lines are crossed to produce F1
individuals (generation 1), all having genotype MQN/mqn. Notationally,
the haplotype to the left of the slanted line in MQN/mqn represents the
paternal gamete, and the one on the right (mqn) represents the maternal
gamete. The inbred lines are typically chosen on the basis of some pheno-
typic attribute, as opposed to being randomly sampled.
The F1 individuals are crossed back to individuals from one of the inbred
lines, those carrying genotype MQN/MQN say, to produce the backcross
generation (generation 2). The marker genotype for the ith individual of
generation 2 is denoted Mi, which can take values G1 = MN/MN, G2 =
Mn/MN, G3 = mN/MN, and G4 = mn/MN. The QTL genotype of
individual i from generation 2, Qi, is a random variable which can take
values Qi = QQ or Qi = Qq.
In a backcross design, individuals are genetically uncorrelated. To see
this, let ai and aj denote additive genetic values of individuals i and j
randomly sampled from generation 2. These additive genetic values arise
due to the presence of many genes each of small eÔ¨Äect acting additively
on the genotype. If the original lines are completely inbred, there is no
variation among the paternal additive genetic values (denoted as af) nor
among the maternal additive genetic values (am) of individuals i and j.
Then,
Cov (ai, aj)
= E [Cov (ai, aj|af, am)] + Cov [E (ai|af, am) , E (aj|af, am)]
= 0 + Cov
1
2 (af + am) , 1
2 (af + am)

= 1
4V ar (af + am) = 0,
i Ã∏= j.
The term E [Cov (ai, aj|af, am)] is zero because, given the additive genetic
values of the parents, additive genetic values in the oÔ¨Äspring are uncorre-
lated. Absence of genetic variation in the parents implies absence of genetic

682
16. Segregation and Quantitative Trait Loci Analysis
covariation in the oÔ¨Äspring, despite there being genetic variation among the
latter.
Conditionally on the unknown QTL genotype, it is assumed that the
phenotypic record of the ith individual is normal of the form
yi|Qi = QQ ‚àºN

¬µ1, œÉ2
,
(16.17)
yi|Qi = Qq ‚àºN

¬µ2, œÉ2
,
(16.18)
where ¬µ1 is the mean phenotype of individuals carrying QTL genotype
QQ, and ¬µ2 is the mean phenotype of individuals carrying QTL genotype
Qq. The variance term œÉ2 typically contains a contribution from polygenic
eÔ¨Äects at many other loci.
Likelihood Inference
Writing the Likelihood
The parameters of interest are Œ∏‚Ä≤ =

¬µ1, ¬µ2, rm, œÉ2
. The data consist of
records on a particular trait, represented by the vector y of length nobs and
by information on the marker genotypes from generation 2. The likelihood
is proportional to the density of the data, given marker information, which
for record i is denoted by p (yi|Œ∏, Mi). The contribution to the likelihood
from the record of individual i can be written as
L (Œ∏|yi, Mi)
‚àùp (yi|QQ, Mi) Pr (QQ|Mi, rm) + p (yi|Qq, Mi) Pr (Qq|Mi, rm)
= p (yi|QQ) Pr (QQ|Mi, rm) + p (yi|Qq) Pr (Qq|Mi, rm) ,
(16.19)
which is a mixture of normal distributions. (Notationally, terms of the form
p (yi|Qi = QQ, Mi) or Pr (Qi = QQ|Mi, rm) say, are written here as
p (yi|QQ, Mi) ,
or
Pr (QQ|Mi, rm) ,
respectively). The equality in the second line of (16.19) arises because,
given the QTL genotype, the marker genotype does not contribute with
additional information to the probability of observing phenotype yi. In
view of the fact that records are independently distributed, and denoting
the complete marker information by the vector M, the likelihood is given
by
L (Œ∏|y, M) ‚àù
n
-
i=1
L (Œ∏|yi, Mi) .
(16.20)
In (16.19), the terms Pr (QQ|Mi) and Pr (Qq|Mi) are the conditional
probabilities of observing a particular QTL genotype given marker infor-
mation. As shown below, these are functions of the recombination fractions.

16.3 QTL Models
683
Genotype
Pr (Genotype)
Marker
QTL
MqN/MQN
rmrn/2
MN/MN
Qq
mQn/MQN
rmrn/2
mn/MN
QQ
Mqn/MQN
rm (1 ‚àírn) /2
Mn/MN
Qq
mQN/MQN
rm (1 ‚àírn) /2
mN/MN
QQ
MQn/MQN
rn (1 ‚àírm) /2
Mn/MN
QQ
mqN/MQN
rn (1 ‚àírm) /2
mN/MN
Qq
MQN/MQN
(1 ‚àírm) (1 ‚àírn) /2
MN/MN
QQ
mqn/MQN
(1 ‚àírm) (1 ‚àírn) /2
mn/MN
Qq
TABLE 16.2. Distribution of genotypes in the backcross design.
The genotypic distribution among generation 2 individuals, together with
the observed marker information and the associated putative QTL geno-
types, are shown in Table 16.2. It is emphasized that Haldane‚Äôs mapping
function, which assumes no interference, is assumed for these calculations.
The marker genotype is observed in a backcross oÔ¨Äspring, but the geno-
type (Ô¨Årst column) is unknown, because the QTL genotype is not observed.
Given that marker genotypes and phase of parents are known, the prob-
ability of a given genotype (Column 2) in the oÔ¨Äspring can be readily
calculated.
To illustrate, consider the term Pr (QQ|Mi = G3, rm) where, for indi-
vidual i, the observed marker genotype is G3 = mN/MN, say. This is
computed as
Pr (QQ|Mi = G3, rm) = Pr (QQ, Mi = G3|rm)/ Pr (Mi = G3|rm) .
The term Pr (QQ, Mi = G3|rm) is equal to 1
2rm (1 ‚àírn), associated with
genotype mQN/MQN in the 4th row in the body of Table 16.2. The de-
nominator
Pr (Mi = G3|rm) = Pr (QQ, Mi = G3|rm) + Pr (Qq, Mi = G3|rm)
is equal to the sum of the genotype probabilities in rows 4 and 6 in the
body of Table 16.2,
1
2rm (1 ‚àírn) + 1
2 (1 ‚àírm) rn.
Therefore,
Pr (QQ|Mi = G3, rm)
=
rm (1 ‚àírn)
rm (1 ‚àírn) + (1 ‚àírm) rn
=
rm (1 ‚àírn)
r
.
Likewise,
Pr (Qq|Mi = G3, rm) = (1 ‚àírm) rn
r
.

684
16. Segregation and Quantitative Trait Loci Analysis
Marker
Pr (QQ|Mi, rm)
Pr (Qq|Mi, rm)
MN/MN
(1 ‚àírm) (1 ‚àírn)/ (1 ‚àír)
rmrn/ (1 ‚àír)
Mn/MN
(1 ‚àírm) rn/ r
rm (1 ‚àírn)/ r
mN/MN
rm (1 ‚àírn)/ r
(1 ‚àírm) rn/ r
mn/MN
rmrn/ (1 ‚àír)
(1 ‚àírm) (1 ‚àírn)/ (1 ‚àír)
TABLE 16.3. Conditional probabilities of QTL genotypes given marker genotypes
in the backcross generation.
Table 16.3 shows the terms Pr (QQ|Mi, rm) and Pr (Qq|Mi, rm) for all
possible values of Mi.
If individual i has observed marker information Mi = mN/MN, from
(16.19), its contribution to the overall likelihood is
L (Œ∏|yi, Mi) ‚àùp (yi|QQ) rm (1 ‚àírn) + p (yi|Qq) (1 ‚àírm) rn.
(16.21)
The recombination fractions satisfy (16.14).
Hypotheses Tests
The test for the presence of a QTL between marker loci M and N, versus
absence of a QTL segregating, consists of computing the likelihood ratio.
This observed likelihood ratio is (Knott and Haley, 1992)
LR =
L

5¬µ1, 5¬µ2, 6
œÉ2, 5rm|y, M

L

,¬µ, E
œÉ2|y, M

,
(16.22)
where 5¬µ1, 5¬µ2, 6
œÉ2, 5rm are the values obtained by joint maximization of
L (Œ∏|y, M) ,
and ,¬µ, E
œÉ2 are the values obtained by maximizing the likelihood restricted
by the null hypothesis (no QTL segregating).
Let T (Y) be equal to (16.22), but with the important diÔ¨Äerence that
T (Y) is a function of the random variable Y rather than of the observed
data y, which is a realized value of this random variable. A test of the null
hypothesis may require computing the so-called p-value
Pr [T (Y) ‚â•LR] =

I [T (y) ‚â•LR] p

y|¬µ, œÉ2, M

dy.
(16.23)
In a classical test of hypothesis, the null hypothesis is rejected if (16.23)
is smaller than or equal to Pr [Type I error]. Because ¬µ and œÉ2 are usually
not known, (16.23) cannot be computed exactly. In this situation one often
appeals to asymptotic results. Under regularity conditions
Pr [2 ln T (Y) ‚â•2 ln LR] = Pr

œá2
2 ‚â•2 ln LR

.

16.3 QTL Models
685
That is, asymptotically, 2 ln T (Y) has a chi-square distribution, with two
degrees of freedom in this case.
An alternative to asymptotic theory is to obtain a Monte Carlo approx-
imation to (16.23). Since ¬µ and œÉ2 are not known, these are replaced by
their maximum likelihood estimates ,¬µ, E
œÉ2. The required probability is now
approximated by
Pr [T (Y) ‚â•LR] =

I [T (y) ‚â•LR] p

y|,¬µ, E
œÉ2, M

dy.
(16.24)
This integration can be approximated drawing data vectors yi (i = 1, . . . , N)
from p

yi|,¬µ, E
œÉ2, M

and computing
Pr [T (Y) ‚â•LR] ‚âà1
N

i
I (T (yi) ‚â•LR) ,
(16.25)
where N represents the number of samples drawn. Other suggested proce-
dures are based on permutation tests (Doerge and Churchill, 1996).
Rather than maximizing (16.22), a proÔ¨Åle likelihood is often computed,
whereby log10 of the ratios of the form in (16.22) are calculated over a
grid of values of rm. The term 2 log10 is known as the LOD score (see Ott,
1999 for a detailed discussion). The maximum LOD score indicates the grid
value of rm closest to the maximum likelihood estimate of rm. A smooth
curve is Ô¨Åtted to the set of LOD score values, and a measure of uncertainty,
in conceptual repeated sampling, is obtained by a 2 (LOD) interval. This
interval is the set of values of rm at which the LOD is not smaller than
its maximum value minus two. The LOD score can be multiplied by a
factor 2 ln (10) = 4.605 for it to have the convenient property of being
asymptotically distributed as a chi-square random variable.
We end this section with a word of caution. Setting up the correct test
of hypothesis in a conventional likelihood scenario is a contentious issue. A
test often entertained is based on
L

5¬µ1, 5¬µ2, 6
œÉ2, 5rm|y, M

L

,¬µ1, ,¬µ2F
, œÉ2, rm = 0.5|y, M
.
The null hypothesis assumes the absence of a QTL between the region
Ô¨Çanked by the markers, but allows for the fact that a QTL may be present
elsewhere in the genome. This form of the likelihood ratio places the pa-
rameter rm ‚àà[0, 0.5] at the boundary of the parameter space and, as a
consequence, the necessary regularity conditions associated with classical
asymptotic theory are not satisÔ¨Åed. Other possible tests are discussed in
Knott and Haley (1992).

686
16. Segregation and Quantitative Trait Loci Analysis
Bayesian Inference
The analysis in the previous section is now performed from a Bayesian
perspective. Except for the additional presence of prior distributions, the
model here is similar to the one in the previous section. As before, it is
assumed that the genetic distance between the two markers is known.
Let Q represent the unknown QTL genotype of all individuals and let
Qi represent the unknown QTL genotype of individual i. The parameters
of the Bayesian model are (Q, rm) and Œ∏‚Ä≤ =

¬µ1, ¬µ2, œÉ2
.
The prior distributions of the parameters are assumed to be as follows.
First, the elements of Œ∏ are taken to be a priori independently distributed.
As prior for ¬µi a normal distribution is invoked, with zero mean and vari-
ance 10, say, to allow for large QTL eÔ¨Äects; thus ¬µi ‚àºN

0, œÉ2
0 = 10

,
(i = 1, 2). The prior for the residual variance is assumed to be a scaled
inverted chi-square distribution with known parameters ŒΩ and S; that is,
œÉ2 ‚àºŒΩSœá‚àí2
ŒΩ . It is also assumed that (Q, rm) and Œ∏ are a priori indepen-
dently distributed.
Prior information about the recombination fraction could be incorpo-
rated as follows. Consider the beta distributed random variable with den-
sity
Œ∑ ‚àºBe (Œ∑|a, b) ,
(0 < Œ∑ < 1) ,
where a and b are known parameters deÔ¨Åning the shape of the distribution.
The recombination fraction rm must take positive probability in the set
]0, r[ (see (16.14)). Now let rm = rŒ∑. The inverse transformation is equal
to Œ∑ = r‚àí1rm, the Jacobian of the transformation is r‚àí1, and therefore the
probability density of rm has the form
p (rm|a, b) =

C

r‚àí1rm
a‚àí1 
1 ‚àír‚àí1rm
b‚àí1 r‚àí1,
0 < rm < r,
0,
otherwise,
(16.26)
where C is a constant that does not depend on rm.
The posterior distribution is given by
p (Q, rm, Œ∏|y, M) ‚àùp (Q, rm) p (Œ∏) p (y,M|Q, rm, Œ∏)
= p (Q, rm) p (Œ∏) Pr (M|Q, rm, Œ∏) p (y|M, Q, rm, Œ∏)
‚àùp (rm) p (¬µ1) p (¬µ2) p

œÉ2
Pr (Q|rm, M) p (y|Q, Œ∏) ,
(16.27)
where
p (y|Q, Œ∏) =
n
-
i=1
p (yi|Qi, Œ∏) ,
(16.28)
and
Pr (Q|rm, M) =
n
-
i=1
Pr (Qi|rm, Mi) .
(16.29)

16.3 QTL Models
687
Expressions (16.28) and (16.29) imply that the QTL genotype can be drawn
for each individual at a time. This conditional independence property is a
consequence of the experimental design. In the backcross design, both the
phase and the QTL genotype of parents are known. Therefore, the parental
origin of an oÔ¨Äspring‚Äôs gamete can be unambiguously assigned. This is not
the case with data from outbred populations.
The joint posterior is deÔ¨Åned within the range of values of the parame-
ters. Marginalization of (16.27) with respect to Q, assuming uniform prior
distributions for rm and Œ∏, retrieves a posterior distribution which has the
same form as likelihood (16.20) derived from the mixture (16.19).
Fully Conditional Posterior Distributions
Extracting the terms containing ¬µ1 from (16.27) yields
p (¬µ1|., data) ‚àùp (¬µ1)
n
-
i=1
p

yi|Qi, ¬µ1, œÉ2I(Qi=QQ)
‚àùexp

‚àí¬µ2
1
2œÉ2
0

exp

‚àí
n
i=1 I (Qi = QQ) (yi ‚àí¬µ1)2
2œÉ2

,
(16.30)
where I(Qi = QQ) is the indicator function that takes the value one
when the individual is QQ, and zero otherwise. Let 5¬µ1 = [n
i=1 I(Qi =
QQ)yi]/nQQ be the mean of the records on individuals whose QTL geno-
type is QQ, where nQQ is the number of such individuals. Then (16.30)
can be expressed as
p (¬µ1|., data)
‚àùexp

‚àíœÉ2¬µ2
1 + œÉ2
0
n
i=1 I (Qi = QQ) [(yi ‚àí5¬µ1) + (5¬µ1 ‚àí¬µ1)]2
2œÉ2œÉ2
0

‚àùexp

‚àíœÉ2¬µ2
1 + œÉ2
0nQQ (5¬µ1 ‚àí¬µ1)2
2œÉ2œÉ2
0

.
(16.31)
Making use of the identity (Box and Tiao, 1973, page 74):
A (z ‚àía)2 + B (z ‚àíb)2 = (A + B) (z ‚àíc)2 +
AB
A + B (a ‚àíb)2
and letting c = (Aa + Bb)/(A + B), A = œÉ2, B = œÉ2
0nQQ, z = ¬µ1, a = 0,
and b = 5¬µ1 allows expressing (16.31) as
p (¬µ1|., data) ‚àùexp

‚àí

œÉ2 + nQQœÉ2
0

(¬µ1 ‚àíc)2
2œÉ2œÉ2
0

,
where c = nQQœÉ2
05¬µ1/(œÉ2 + nQQœÉ2
0). Therefore,
¬µ1|., data ‚àºN

 nQQœÉ2
05¬µ1
œÉ2 + nQQœÉ2
0
,
œÉ2œÉ2
0
œÉ2 + nQQœÉ2
0

.
(16.32)

688
16. Segregation and Quantitative Trait Loci Analysis
By symmetry considerations,
¬µ2|., data ‚àºN

 nQqœÉ2
05¬µ2
œÉ2 + nQqœÉ2
0
,
œÉ2œÉ2
0
œÉ2 + nQqœÉ2
0

,
(16.33)
where nQq is the number of individuals with QTL genotype Qq, and 5¬µ2 =
n
i=1 I(Qi = Qq)yi/nQq.
To derive p

œÉ2|., data

the terms containing œÉ2 are extracted from the
joint posterior (16.27). This yields
p

œÉ2|., data

‚àùp

œÉ2
n
-
i=1

p

yi|Qi, ¬µ1, œÉ2I(Qi=QQ)
√ó p

yi|Qi, ¬µ2, œÉ2I(Qi=Qq)
‚àùp

œÉ2
n
-
i=1
#

œÉ2‚àí1
2 exp

‚àíI (Qi = QQ) (yi ‚àí¬µ1)2
2œÉ2

√ó exp

‚àíI (Qi = Qq) (yi ‚àí¬µ2)2
2œÉ2
$
.
Let
VQQ =
n
i=1
I (Qi = QQ) (yi ‚àí5¬µ1)2
nQQ
and
VQq =
n
i=1
I (Qi = Qq) (yi ‚àí5¬µ2)2
nQq
.
Then p

œÉ2|., data

can be written as
p

œÉ2|., data

‚àù

œÉ2‚àí( ŒΩ+n
2
+1)
√ó exp
Ô£±
Ô£≤
Ô£≥‚àí
nQQ

VQQ + (¬µ1 ‚àí5¬µ1)2
+ nQq

VQq + (¬µ2 ‚àí5¬µ2)2
+ ŒΩS
2œÉ2
Ô£º
Ô£Ω
Ô£æ
=

œÉ2‚àí( ŒΩ
2 +1) exp

‚àí,ŒΩ ,S
2œÉ2

,
where ,ŒΩ = ŒΩ + n and
,S =
9
nQQ

VQQ + (¬µ1 ‚àí5¬µ1)2
+ nQq

VQq + (¬µ2 ‚àí5¬µ2)2
+ ŒΩS
:G
,ŒΩ.

16.3 QTL Models
689
This is recognized as the density of a scaled inverted chi-square distribution
with parameters ,S and ,ŒΩ
œÉ2|., data ‚àº,ŒΩ ,Sœá‚àí2
ŒΩ .
(16.34)
From the joint posterior (16.27) the fully conditional posterior distribu-
tion of the QTL genotypes is proportional to
Pr (Q|., data) ‚àùPr (Q|rm, M) p (y|Q, Œ∏)
=
n
-
i=1
Pr (Qi|rm, Mi) p (yi|Qi, Œ∏) ,
(16.35)
which implies that sampling can proceed separately for each individual. For
individual i
Pr (Qi = QQ|., data) = Pr (Qi = QQ|rm, Mi) p (yi|Qi = QQ, Œ∏)

œâ
Pr (Qi = œâ|rm, Mi) p (yi|Qi = œâ, Œ∏) , (16.36)
where œâ denotes QQ or Qq. Similarly,
Pr (Qi = Qq|., data) = Pr (Qi = Qq|rm, Mi) p (yi|Qi = Qq, Œ∏)

œâ
Pr (Qi = œâ|rm, Mi) p (yi|Qi = œâ, Œ∏).
(16.37)
Finally, the fully conditional posterior distribution of the recombination
fraction rm is proportional to
p (rm|., data) ‚àùp (rm)
n
-
i=1
Pr (Qi|rm, Mi) .
(16.38)
In (16.38), p (rm) is given by (16.26). Expression (16.38) does not have a
standard form; therefore a univariate Metropolis-Hastings algorithm can be
used for drawing samples rm. Let r‚àó
m denote a candidate value generated by
the candidate generating density u (r‚àó
m|rm). Then the proposal is accepted
with probability Œ± (r‚àó
m, rm) given by
Œ± (r‚àó
m, rm) =
#
min

p(r‚àó
m|.,data)u(rm|r‚àó
m)
p(rm|.,data)u(r‚àó
m|rm), 1

, if p (rm|., data) > 0,
1, otherwise.
(16.39)
The candidate generating density could be a uniform distribution on the
interval (rm ‚àíd, rm + d), where d is chosen such that the acceptance rate is
in the range 20% to 50% (Chib and Greenberg, 1995). If a uniform density is
chosen, Un (r‚àó
m|rm ‚àíd, rm + d) = Un (rm|r‚àó
m ‚àíd, r‚àó
m + d) = 1/2d. Then,
only the ratio p (r‚àó
m|., data) /p (rm|., data) needs to be computed in (16.39).
This is known as the Metropolis algorithm (Metropolis et al., 1953).
Implementation of the MCMC approach described above generates Mon-
te Carlo samples from the joint posterior distribution (16.27). SpeciÔ¨Åc pa-
rameters of the model can be inferred from their marginal posterior distri-
bution.

690
16. Segregation and Quantitative Trait Loci Analysis
Model Selection
A Bayesian counterpart of (16.22) is the Bayes factor that was discussed
in Chapter 8. One may wish to compare the model that assumes one QTL
is segregating, labeled M1, versus a model that assumes that no QTL is
segregating, labeled M0. Under M0, the posterior distribution is
p

¬µ, œÉ2
e|y, M0

‚àùp

¬µ, œÉ2
e|M0

p

y|¬µ, œÉ2
e, M0

.
The Bayes factor of model M1 relative to model M0 requires computation
of
B10 = p (y|M1)
p (y|M0)
=

Q

p (y|Q, Œ∏, M1) Pr (Q|rm, M, M1) p (rm, Œ∏|M1) dŒ∏

p (y|¬µ, œÉ2e, M0) p (¬µ, œÉ2e|M0) d¬µdœÉ2e
.
As discussed in Chapter 8, B10 can be computed using standard MCMC
output with the approach suggested by Newton and Raftery (1994). A
Monte Carlo estimate of B10 is given by
5B10 =
N

j=1
p‚àí1 
y|¬µ(j), œÉ2(j)
e
, M0

N

j=1
p‚àí1

y|Q(j), Œ∏(j), M1
 ,
where N is the length of the Monte Carlo chain, ¬µ(j), œÉ2(j)
e
is the jth draw
from

¬µ, œÉ2
e|y,M0

, and Q(j), Œ∏(j) is the jth draw from [Q, Œ∏|y, M1].
Another approach to Bayesian model choice is based on a Monte Carlo
estimate of the posterior probability of the model; this is discussed at the
end of the next section.
As a Ô¨Ånal word of warning, we remind the reader that tests involving
speciÔ¨Åc values of continuous parameters (r = 0.5, say), must build on a
prior speciÔ¨Åcation which assigns probability mass to that speciÔ¨Åc value.
Otherwise, by deÔ¨Ånition, the posterior probability that the continuous pa-
rameter takes the speciÔ¨Åed value is 0. A point null hypothesis cannot be
tested under a continuous prior distribution. For example, the prior (16.26)
cannot be used for testing a speciÔ¨Åc value of the recombination fraction.
16.3.2
Models with an Arbitrary Number of QTL
In this section the model is extended to include an arbitrary number of
QTL. A model indicator M is introduced, which can take values 1, 2, . . . , I,
where I is an integer. Inference is restricted to the Bayesian approach only.
Suppose that (1, 2, . . . , K) ordered markers and phenotypic observations yi,

16.3 QTL Models
691
(i = 1, . . . , nobs) are available from a backcross line. The marker genotype
information for the K loci of individual i is denoted Mi = {Mij}j=1,...,K,
where Mij is the information on the jth marker. The known positions of
the K markers are collected in the vector D = {Dl}l=1,...,K, where Dl is
the genetic map distance between markers 1 and l and D1 = 0.
Suppose that m QTL are present at locations Œª = (Œª1, . . . , Œªm), where
D1 < Œªi < DK. More than one QTL may be present in the region deÔ¨Åned
by two Ô¨Çanking markers. Let the matrix
Q = {Qij}i=1,...,nobs,j=1,...,m
represent a genotype conÔ¨Åguration where Qij is the QTL genotype at loca-
tion Œªj for individual i. Let the ith row of Q, Qi = {Qij}j=1,...,m, represent
the QTL genotypes for individual i, and let Qj = {Qij}i=1,...,nobs repre-
sent the jth column of Q with QTL genotypes for the nobs individuals at
location Œªj.
In the backcross design, conditionally on marker information M and D
and on number and QTL locations, the QTL genotypes of individuals are
independent, so
Pr (Q|m, M, Œª, D) =
nobs
-
i=1
Pr (Qi|m, Mi, Œª, D) .
(16.40)
Following Sillanp¬®a¬®a and Arjas (1998), the generic term ‚Äúobject‚Äù will
be used for any marker or QTL in the linkage group. Let Gj
i,L and Gj
i,R
represent the genotypes of two Ô¨Çanking objects (marker or QTL) in in-
dividual i, located, respectively, to the left and right of the jth QTL in
individual i. These Ô¨Çanking objects represent a subset of the parameters
one conditions on in the conditional probability distribution of the QTL
genotypes of individual i. In the case of the present backcross design, the
conditional probability distribution of the QTL genotypes of individual i,
Pr (Qi|m, Mi, Œª, D), given the QTL locations and the genotypes and loca-
tions of other objects (markers or QTL), can be written as
Pr (Qi|m, Mi, Œª, D) =
m
-
j=1
Pr

Qij|Gj
i,L, Gj
i,R, Œªj‚àí1, Œªj, Œªj+1

,
(16.41)
where Œªj‚àí1 is the map distance between Gj
i,L and D1, Œªj is the map distance
between QTL genotype Qij and D1, and Œªj+1 is the map distance between
Gj
i,R and D1. In order to generate the correct joint prior distribution from
(16.41), it is important to choose the objects judiciously. If the object is a
marker, it is chosen among the complete set of markers in the chromosome;
if it is a QTL, it is chosen among the set of QTL whose index is lower than
the one being considered (Sillanp¬®a¬®a and Arjas, 1998).

692
16. Segregation and Quantitative Trait Loci Analysis
It is also possible to specify the conditional distribution of a particular
QTL genotype across the nobs individuals, given the genotypic conÔ¨Ågu-
ration for the remaining QTL genotypes, the QTL locations Œª, and the
marker information M. For the present design,
Pr

Qj|Q1, . . . , Qj‚àí1, M, Œª, D

=
nobs
-
i=1
Pr

Qij|Gj
i,L, Gj
i,R, Œªj‚àí1, Œªj, Œªj+1

,
(16.42)
which leads to the alternative form for Pr (Q|m, M, Œª, D)
Pr (Q|m, M, Œª, D) =
m
-
j=1
Pr

Qj|Q1, . . . , Qj‚àí1, M, Œª, D

=
m
-
j=1
nobs
-
i=1
Pr

Qij|Gj
i,L, Gj
i,R, Œªj‚àí1, Œªj, Œªj+1

.
(16.43)
In the backcross design, at each of the m loci only two genotypes are pos-
sible, with eÔ¨Äects represented by real parameters ¬µj1 or ¬µj2, (j = 1, . . . , m).
The data y = {yi} are assumed to be a realized value from Y, where
Y|m, ¬µ,Q, œÉ2 ‚àºN

X¬µ, IœÉ2
.
(16.44)
In (16.44), ¬µ =

¬µji

j=1,...,m,i=1,2, X is an incidence matrix associating
QTL eÔ¨Äects to the data, I is the identity matrix, and œÉ2 > 0 is a resid-
ual variance which may include a polygenic contribution from other loci
aÔ¨Äecting the trait.
It will be assumed that the prior distribution of the parameters can
be factorized as follows (ignoring the dependence on hyperparameters to
simplify notation)
p

¬µ, œÉ2, Œª, Q, m|M, D

= Pr (m) p

¬µ, œÉ2, Œª, Q|m, M, D

= Pr (m) p (¬µ|m) p

œÉ2|m

Pr (Q|m, M, Œª, D) p (Œª|m, D) .
(16.45)
The posterior distribution is given by
p

¬µ, œÉ2, Œª, Q, m|y, M, D

‚àùPr (m) p

¬µ, œÉ2, Œª, Q|m, M, D

√óp

y|m, ¬µ,Q, œÉ2
.
(16.46)
The prior distribution of the parameters of the model could be speciÔ¨Åed
as follows. Given M = m, the locations Œª1, . . . , Œªm are assumed to be in-
dependent and uniformly distributed in the interval ‚àÜ= (D1, DK). For m,
the number of QTL, a Poisson distribution with mean Œ± can be posited.
The elements of ¬µ can be assumed to be a priori independently and nor-
mally distributed with mean zero and variance œÉ2
0. Finally, the residual
variance can be assumed to follow a scaled inverted chi-square distribution
with known parameters ŒΩ and S: œÉ2 ‚àºŒΩSœá‚àí2
ŒΩ .

16.3 QTL Models
693
The posterior distribution (16.46) does not have a Ô¨Åxed dimension be-
cause m varies according to the unknown number of QTL. Reversible jump
MCMC provides a Ô¨Çexible method for drawing samples from such a pos-
terior distribution. The algorithm consists of moves that lead to a change
of dimension and thereby a change of model, increasing or decreasing the
number of QTL, and of updates within models. Updates within a model
are very similar to those discussed in connection with the model assuming
one QTL and two Ô¨Çanking markers and are brieÔ¨Çy dealt with Ô¨Årst.
Fully Conditional Posterior Distributions with Fixed Number of QTL
Updating ¬µ
From (16.45) and (16.46) the fully conditional posterior density of ¬µ is
p (¬µ|., data) ‚àùp (¬µ|m) p

y|m, ¬µ,Q, œÉ2
‚àùexp

‚àí¬µ‚Ä≤¬µ
2œÉ2
0

exp

‚àí(y ‚àíX¬µ)‚Ä≤ (y ‚àíX¬µ)
2œÉ2

.
(16.47)
The quadratic term, viewed as a function of ¬µ, can be shown to be propor-
tional to
(y ‚àíX¬µ)‚Ä≤ (y ‚àíX¬µ) ‚àù(¬µ ‚àí5¬µ)‚Ä≤ X‚Ä≤X (¬µ ‚àí5¬µ) ,
where
5¬µ = (X‚Ä≤X)‚àí1 X
‚Ä≤y.
Substituting in (16.47) and combining quadratic forms as indicated in Box
and Tiao (1973), page 418, it can be shown that
p (¬µ|., data) ‚àùexp

‚àí(¬µ ‚àíc)‚Ä≤ 
IœÉ2 + X‚Ä≤XœÉ2
0

(¬µ ‚àíc)
2œÉ2œÉ2
0

,
where
c =

IœÉ2 + X‚Ä≤XœÉ2
0
‚àí1 œÉ2
0X‚Ä≤X5¬µ.
Therefore,
¬µ|., data ‚àºN

c,

IœÉ2 + X‚Ä≤XœÉ2
0
‚àí1 œÉ2
0œÉ2
.
(16.48)
Elements of ¬µ have the same form as (16.32) and (16.33).
Updating œÉ2
The fully conditional posterior distribution of œÉ2 is given by
p

œÉ2|., data

‚àùp

œÉ2
p

y|m, ¬µ,Q, œÉ2
,
which is in the form of a scaled inverted chi-square distribution
œÉ2|., data ‚àº,v ,Sœá‚àí2
v
(16.49)
with ,v = ŒΩ + n and ,S =

(y ‚àíX¬µ)‚Ä≤ (y ‚àíX¬µ) + ŒΩS

/,v .

694
16. Segregation and Quantitative Trait Loci Analysis
Updating Q
The conditional independence of (16.43) can be exploited to update Q,
sampling each genotype separately, one individual at a time. The fully
conditional posterior distribution is
Pr (Q|., data)
‚àù
m
-
j=1
nobs
-
i=1
Pr

Qij|Gj
i,L, Gj
i,R, Œªj‚àí1, Œªj, Œªj+1

p

yij|m, ¬µj1, ¬µj2

(16.50)
and drawing from it can be performed after appropriate scaling. The form
of (16.50) resembles (16.36) and (16.37).
Updating Œª
Elements of Œª can be updated one at a time using a Metropolis-Hastings
step. The fully conditional posterior distribution is
p (Œªj|., data) ‚àùp (Œªj|m, D)
m
-
j=1
Pr

Qij|Gj
i,L, Gj
i,R, Œªj‚àí1, Œªj, Œªj+1

.
Following a similar strategy used in (16.39), the proposal Œª‚àó
j could be
generated from a uniform distribution with density Un

Œª‚àó
j|a, b

where
a = max (Œªj‚àí1, Œªj ‚àíd), b = min (Œªj+1, Œªj + d), and d is a tuning parame-
ter. This proposal is accepted with probability
Œ±

Œª‚àó
j, Œªj

=
Ô£±
Ô£≤
Ô£≥
min

p(Œª‚àó
j |.,data)Un(Œªj|a,b)
p(Œªj|.,data)Un(Œª‚àó
j |a,b), 1

, if p (Œªj|., data) > 0,
1, otherwise.
The proposal distribution maintains ordering of the loci.
Updates Leading to Changes of the Model
This section derives the acceptance probability for a reversible jump MCMC
algorithm for moves involving a change in the number of QTL. The ap-
proach builds on that in Section 11.7 of Chapter 11, where the acceptance
probability was derived and illustrated for models with continuous param-
eters. The discrete nature of the QTL genotypes calls for some minor mod-
iÔ¨Åcations. The material is largely taken from Waagepetersen and Sorensen
(2001). Applications of reversible jump in QTL studies can be found in
Heath (1997), Uimari and Hoeschele (1997), Stephens and Fisch (1998),
Sillanp¬®a¬®a and Arjas (1998, 1999), George et al. (2000), Lee and Thomas
(2000), and Yi and Xu (2000). In this last part, following the notation used
in Chapter 11, no distinction is made among vectors, matrices, and scalars,
unless otherwise stated.

16.3 QTL Models
695
Suppose that the current state of the Markov chain has M = m QTL.
With probability pm,m it is proposed to update parameters within the cur-
rent model, with probability pm,m‚àí1 it is proposed to decrease the number
of QTL by one, and with probability pm,m+1 it is proposed to increase the
number of QTL by one (if it is proposed to decrease the number of QTL
the chain remains at the current state if m = 0).
Given m, deÔ¨Åne
z =

¬µ11, ¬µ12, Œª1, Q1, . . . , ¬µm1, ¬µm2, Œªm, Qm
.
In dimension changing moves, the residual variance œÉ2 is not updated; it is
updated in moves within models. Therefore to economize on notation, œÉ2
is omitted from the formulas in the remaining of this section. Let (m, z)
represent the state of the Markov chain, where z = (z1, . . . , zm) is a vector
of QTL conÔ¨Ågurations zj =

¬µj1, ¬µj2, Œªj, Qj
(j = 1, . . . , m). Thus, each
QTL conÔ¨Åguration zj consists of the QTL eÔ¨Äects, together with the asso-
ciated QTL location and nobs genotypes. A QTL conÔ¨Åguration belongs in
the space Cconf = R2 √ó ‚àÜ√ó {0, 1}nobs, where 0 and 1 are labels for geno-
types Qq and QQ. The vector z of the m QTL conÔ¨Ågurations belongs in
the space Cm = Cm
conf.
Removal of a QTL
Suppose there are m ‚â•1 QTL conÔ¨Ågurations and that this number is
proposed to be reduced by 1 with probability pm,m‚àí1. The current state of
the Markov chain is Xn = (m, z) where z = (z1, . . . , zm). A move reducing
the number of QTL may be accomplished by deterministically removing the
last (in terms of the position in the vector z, and not in terms of the physical
position of the locus on the chromosome) QTL conÔ¨Åguration in z. As in
Section 11.7.2 of Chapter 11, denote the proposal Yn+1 =

Y ind
n+1, Y par
n+1

,
where Y ind
n+1 = m ‚àí1 and Y par
n+1 = g1m,m‚àí1 (z1, . . . , zm‚àí1) = (z1, . . . , zm‚àí1),
since g1m,m‚àí1 is the identity mapping. Suppose that Am is a subset of Cm
and that Bm‚àí1 is a subset of Cm‚àí1. The left-hand side of the reversibility
condition (11.67) is
P

Mn = m, Zn ‚ààAm, Y ind
n+1 = m ‚àí1, Y par
n+1 ‚ààBm‚àí1
and Yn+1 accepted)
=

‚àÜm

R2m

Q‚àà{0,1}mnobs
Qa
m,m‚àí1 (z, Bm‚àí1) I (z ‚ààAm)
√óf (m, z|y) dŒª d¬µ,
(16.51)
where
f (m, z|y) ‚àùPr (M = m|y) p (¬µ, Œª, Q|m, y)

696
16. Segregation and Quantitative Trait Loci Analysis
and
Qa
m,m‚àí1 (z, Bm‚àí1)
= P

Y ind
n+1 = m ‚àí1, Y par
n+1 ‚ààBm‚àí1, Yn+1 accepted|Xn = (m, z)

. (16.52)
Since the proposal Yn+1 is generated deterministically, in analogy with
(11.77),
Qa
m,m‚àí1 (z, Bm‚àí1)
=
pm,m‚àí1I ((z1, . . . , zm‚àí1) ‚ààBm‚àí1)
√óam,m‚àí1 (z, (z1, . . . , zm‚àí1)) .
(16.53)
Substituting in (16.51) yields
pm,m‚àí1

‚àÜm

R2m

Q‚àà{0,1}mnobs
f (m, z|y)
√óI (z ‚ààAm, (z1, . . . , zm‚àí1) ‚ààBm‚àí1) am,m‚àí1 (z, (z1, . . . , zm‚àí1)) dŒª d¬µ.
(16.54)
In these expressions,
Q =

Q1, . . . , Qm
,
Œª = (Œª1, . . . , Œªm) ,
and
¬µ = (¬µ11, ¬µ12, . . . , ¬µm1, ¬µm2) .
Further, dŒª and d¬µ are shorthand for
dŒª1, dŒª2, . . . , dŒªm
and for
d¬µ11, d¬µ12, . . . , d¬µm1, d¬µm2,
respectively.
Addition of a QTL
Suppose now that there are m ‚àí1 QTL and that this number is to be
increased by one with probability pm‚àí1,m. The current state of the Markov
chain is Xn = (m ‚àí1, z‚Ä≤) = (m ‚àí1, z1, . . . , zm‚àí1). The right-hand side of
the reversibility condition (11.67) is
P

Mn = m ‚àí1, Zn ‚ààBm‚àí1, Y ind
n+1 = m, Y par
n+1 ‚ààAm
and Yn+1 accepted)
=

‚àÜm‚àí1

R2(m‚àí1)

Q‚Ä≤‚àà{0,1}(m‚àí1)nobs
Qa
m‚àí1,m (z‚Ä≤, Am) I (z‚Ä≤ ‚ààBm‚àí1)
√óf (m ‚àí1, z‚Ä≤|y) dŒª‚Ä≤ d¬µ‚Ä≤
(16.55)

16.3 QTL Models
697
where
f (m ‚àí1, z‚Ä≤|y) ‚àùPr (M = m ‚àí1|y) p

¬µ‚Ä≤, Œª‚Ä≤,Q‚Ä≤|m ‚àí1, y

,
and
Qa
m‚àí1,m (z‚Ä≤, Am)
= P

Y ind
n+1 = m, Y par
n+1 ‚ààAm and Yn+1 accepted|Xn = (m ‚àí1, z‚Ä≤)

.
(16.56)
In this move, the proposal
Y par
n+1
=
g1m‚àí1,m (z1, . . . , zm‚àí1, (¬µm1, ¬µm2, Œªm, Qm))
=
(z1, . . . , zm‚àí1, (¬µm1, ¬µm2, Œªm, Qm))
is generated stochastically: one must draw
zm = (¬µm1, ¬µm2, Œªm, Qm)
from the proposal density qm‚àí1,m (z‚Ä≤, zm). The elements in zm are placed
immediately after zm‚àí1.
A simple approach to generate zm could be to draw each QTL eÔ¨Äect
from ¬µm,i ‚àºN

0, œÑ 2
where œÑ 2 is properly tuned, the location Œªm from a
uniform distribution between D1 and DK, and the vector of QTL genotypes
from the conditional probability
Pr

Qm|Q‚Ä≤, Œªm, Œª‚Ä≤, D, M, m

=
nobs
-
i=1
Pr

Qim|Gm
i,L, Gm
i,R, Œªm, Œª‚Ä≤
,
where Gm
i,L

Gm
i,R

is the Ô¨Çanking object to the left (right) of Œªm in individual
i. Therefore, the proposal density is
qm‚àí1,m (z‚Ä≤, zm) = f

¬µm1|0, œÑ 2
f

¬µm2|0, œÑ 2
1
DK ‚àíD1
√ó Pr

Qm|Q‚Ä≤, Œªm, Œª‚Ä≤, D, M, m

.
(16.57)
In the expressions above,
Q‚Ä≤ =

Q1, . . . , Qm‚àí1
,
Œª‚Ä≤ = (Œª1, . . . , Œªm‚àí1) ,
and
¬µ‚Ä≤ =

¬µ11, ¬µ12, . . . , ¬µ(m‚àí1)1, ¬µ(m‚àí1)2

.
Due to the stochastic nature of the proposal, (16.56) can be written as
Qa
m‚àí1,m (z‚Ä≤, Am) = pm‚àí1,m

‚àÜ

R2

Qm‚àà{0,1}nobs
I ((z‚Ä≤, zm) ‚ààAm)
√óam‚àí1,m (z‚Ä≤, (z‚Ä≤, zm)) qm‚àí1,m (z‚Ä≤, zm) dŒªm d¬µm1 d¬µm2.
(16.58)

698
16. Segregation and Quantitative Trait Loci Analysis
Substituting in (16.55) leads to the following form for the right hand side
of the reversibility condition (11.67):
pm‚àí1,m

‚àÜm

R2m

Q‚àà{0,1}mnobs
I ((z‚Ä≤, zm) ‚ààAm, z‚Ä≤ ‚ààBm‚àí1)
√óf (m ‚àí1, z‚Ä≤|y) am‚àí1,m (z‚Ä≤, (z‚Ä≤, zm))
√óqm‚àí1,m (z‚Ä≤, zm) dŒªm d¬µm1 d¬µm2 dŒª‚Ä≤ d¬µ‚Ä≤.
(16.59)
In the expressions above, dŒª‚Ä≤ and d¬µ‚Ä≤ are shorthand for
dŒª1, dŒª2, . . . , dŒªm‚àí1
and for
d¬µ11, d¬µ12, . . . , d¬µ(m‚àí1)1, d¬µ(m‚àí1)2,
respectively.
Derivation of the Acceptance Probability
The dimensions of (16.54) and (16.59) are equal. In terms of the dimension
matching expression (11.60), here, nm = m, nm,m‚àí1 = 0, nm‚àí1 = m ‚àí
(2 + 1 + nobs), and nm‚àí1,m = 2 + 1 + nobs, so that
nm + nm,m‚àí1 = m = nm‚àí1 + nm‚àí1,m.
For reversibility to hold, (16.54) must equal (16.59). The equality is satisÔ¨Åed
if
pm,m‚àí1f (m, z|y) am,m‚àí1 (z, (z1, . . . , zm‚àí1))
=
pm‚àí1,mf (m ‚àí1, z‚Ä≤|y) am‚àí1,m (z‚Ä≤, (z‚Ä≤, zm)) qm‚àí1,m (z‚Ä≤, zm) ,
which leads to the following expression for the acceptance probability:
am,m‚àí1 (z, z‚Ä≤)
= min

1, pm‚àí1,mf (m ‚àí1, z‚Ä≤|y) qm‚àí1,m (z‚Ä≤, zm)
pm,m‚àí1f (m, z|y)

.
(16.60)
The acceptance probability (16.60) holds when the dimension changing
moves are based on the strategy of deleting deterministically the last (in
terms of the position in the vector z) QTL and appending the new QTL
in the last position in z. The diligent reader may wish to conÔ¨Årm that the
same acceptance probability is arrived at when the QTL to be deleted is
randomly chosen with probability 1/m among the m existing positions in
z, and the QTL to be added is inserted randomly with probability 1/m
among the m available positions in z.

16.3 QTL Models
699
Model Selection
The above reversible jump algorithm generates samples from the posterior
distribution p (Œ∏i|M = i, y) Pr (M = i|y), where Œ∏i are parameters of model
i. Models can be compared by means of their posterior probabilities. These
can be estimated from
6
Pr (M = i|y) = 1
N
N

j=1
I (mj = i)
where mj is the jth Monte Carlo sample of a variable that takes a particular
value for each model and N is the number of samples.

This page intentionally left blank

References
Abramowitz, M. and I. A. Stegun (1972). Handbook of Mathematical Func-
tions. Dover Publications.
Agresti, A. (1989). A survey of models for repeated ordered categorical
response data. Statistics in Medicine 8, 1209‚Äì1224.
Agresti, A. (1990). Categorical Data Analysis. Wiley.
Agresti, A. (1996). An Introduction to Categorical Data Analysis. Wiley.
Aitken, A. C. (1934).
A note on selection from a multivariate normal
population. Proceedings of the Edinburgh Mathematical Society 4, 106‚Äì
110.
Akaike, H. (1973). Information theory as an extension of the maximum
likelihood principle. In B. N. Petrov and F. Csaki (Eds.), Second In-
ternational Symposium on Information Theory, pp. 267‚Äì281. Akademiai
Kiado, Budapest.
Albert, J. H. and S. Chib (1993). Bayesian analysis of binary and polychoto-
mous response data. Journal of the American Statistical Association 88,
669‚Äì679.
Albert, J. H. and S. Chib (1995). Bayesian residual analysis for binary
response regression models. Biometrika 82, 747‚Äì759.
Anderson, D. A. and M. Aitkin (1985). Variance component models with
binary response: Interviewer variability. Journal of the Royal Statistical
Society Series B 47, 203‚Äì210.

702
References
Anderson, T. W. (1984). An Introduction to Multivariate Statistical Anal-
ysis. Wiley.
Applebaum, D. (1996). Probability and Information - An Integrated Ap-
proach. Cambridge University Press.
Baldi, P. and S. Brunak (1998). Bioinformatics: The Machine Learning
Approach. MIT Press.
BarndorÔ¨Ä-Nielsen, O. E. (1983). On a formula for a distribution of the
maximum likelihood estimator. Biometrika 70, 343‚Äì365.
BarndorÔ¨Ä-Nielsen, O. E. (1986). Inference on full or partial parameters
based on the standardized log likelihood ratio. Biometrika 73, 307‚Äì322.
BarndorÔ¨Ä-Nielsen, O. E. (1991).
Likelihood theory.
In D. V. Hinkley,
N. Reid, and E. J. Snell (Eds.), Statistical Theory and Modelling, Chap-
ter 10, pp. 232‚Äì264. Chapman and Hall.
BarndorÔ¨Ä-Nielsen, O. E. and D. R. Cox (1994). Inference and Asymptotics.
Chapman and Hall.
Barnett, V. (1999). Comparative Statistical Inference. Wiley.
Barnett, V. and T. Lewis (1995). Outliers in Statistical Data. Wiley.
Bates, D. and D. G. Watts (1988). Nonlinear Regression Analysis and its
Applications. Wiley.
Bayarri, M. J. (1981).
Inferencia Bayesiana sobre el coeÔ¨Åciente de cor-
relaci¬¥on de una poblaci¬¥on normal bivariante. Trabajos de Estad¬¥ƒ±stica 32,
18‚Äì31.
Bayes, T. (1763). An essay towards solving a problem in the doctrine of
chances. Philosophical Transactions of the Royal Society of London 53,
370‚Äì418.
Becker, W. A. (1984). Manual of Quantitative Genetics. Academic Enter-
prises.
Berger, J. O. (1985). Statistical Decision Theory and Bayesian Analysis.
Springer‚ÄìVerlag.
Berger, J. O. and J. M. Bernardo (1992). On the development of reference
priors. In J. M. Bernardo, J. O. Berger, A. P. Dawid, and A. F. M. Smith
(Eds.), Bayesian Statistics 4, pp. 35‚Äì60. Oxford University Press.
Berger, J. O. and L. R. Pericchi (1996). The intrinsic Bayes Factor for
model selection and prediction. Journal of the American Statistical As-
sociation 91, 109‚Äì122.

References
703
Bernardo, J. M. (1979).
Reference posterior distributions for Bayesian
inference (with discussion). Journal of the Royal Statistical Society Series
B 41, 113‚Äì147.
Bernardo, J. M. (2001). Bayesian statistics. Submitted manuscript.
Bernardo, J. M. and A. F. M. Smith (1994). Bayesian Theory. Wiley.
Besag, J. (1974). Spatial interaction and the statistical analysis of lattice
systems (with discussion). Journal of the Royal Statistical Society Series
B 36, 192‚Äì326.
Besag, J. (1994). Contribution to the discussion paper by Grenander and
Miller. Journal of the Royal Statistical Society Series B 56, 591‚Äì592.
Bibby, J. and H. Toutenburg (1977). Prediction and Improved Estimation
in Linear Models. Wiley.
Blasco, A. (2001). The Bayesian controversy in animal breeding. Journal
of Animal Science 79, 2023‚Äì2046.
Blasco, A. and L. Varona (1999). Ajuste y comparaci¬¥on de curvas de crec-
imiento. ITEA 95A, 131‚Äì142.
Bliss, C. I. (1935). The calculation of the dosage-mortality curve. Annals
of Applied Biology 22, 134‚Äì167.
Box, G. E. P. (1976).
Science and statistics.
Journal of the American
Statistical Association 71, 791‚Äì799.
Box, G. E. P. (1980). Sampling and Bayes‚Äô inference in scientiÔ¨Åc modelling
and robustness (with discussion). Journal of the Royal Statistical Society
Series A 143, 383‚Äì430.
Box, G. E. P. and M. E. Muller (1958). A note on the generation of random
normal deviates. Annals of Mathematical Statistics 29, 610‚Äì611.
Box, G. E. P. and G. C. Tiao (1973). Bayesian Inference in Statistical
Analysis. Wiley.
Brooks, S. P. and A. Gelman (1998).
General methods for monitoring
convergence of iterative simulations. Journal of Computer Graphics and
Statistics 7, 434‚Äì455.
Brooks, S. P., P. Giudici, and G. O. Roberts (2001). EÔ¨Écient construction
of reversible jump MCMC proposal distributions. Submitted manuscript.
Brooks, S. P. and G. O. Roberts (1998). Diagnosing convergence of Markov
chain Monte Carlo algorithms. Statistics and Computing 8, 319‚Äì335.

704
References
Brown, L. D., T. T. Cai, and A. DasGupta (2001). Interval estimation for
a binomial proportion. Statistical Science 16, 101‚Äì133.
Bulmer, M. G. (1971). The eÔ¨Äect of selection on genetic variability. Amer-
ican Naturalist 105, 201‚Äì211.
Bulmer, M. G. (1979). Principles of Statistics. Dover Publications.
Bulmer, M. G. (1980). The Mathematical Theory of Quantitative Genetics.
Oxford University Press.
Bunke, O. (1975). Minimax linear, ridge and shrunken estimators for linear
parameters. Mathematische Operationsforschung und Statistik 6, 697‚Äì
701.
Carlin, B. P. and T. A. Louis (1996). Bayes and Empirical Bayes Methods
for Data Analysis. Chapman and Hall.
Casella, G. and R. L. Berger (1990). Statistical Inference. Brooks‚ÄìCole.
Casella, G. and E. I. George (1992). Explaining the Gibbs sampler. The
American Statistician 46, 167‚Äì170.
Casella, G., M. Lavine, and C. P. Robert (2001). Explaining the perfect
sampler. The American Statistician 55, 299‚Äì305.
Casella, G. and C. P. Robert (1996).
Rao-Blackwellisation of sampling
schemes. Biometrika 83, 81‚Äì94.
Chen, M. H., Q. M. Shao, and J. G. Ibrahim (2000). Monte Carlo Methods
in Bayesian Computation. Springer‚ÄìVerlag.
Chib, S. (1995). Marginal likelihood from the Gibbs output. Journal of the
American Statistical Association 90, 1313‚Äì1321.
Chib, S. and E. Greenberg (1995). Understanding the Metropolis-Hastings
algorithm. The American Statistician 49, 327‚Äì335.
Chib, S. and I. Jeliazkov (2001). Marginal likelihood from the Metropolis-
Hastings output.
Journal of the American Statistical Association 96,
270‚Äì281.
Cohen, M. D. (1986). Pseudo-random number generators. In S. Kotz, N. L.
Johnson, and C. B. Read (Eds.), Encyclopedia of Statistics, Vol. 7, pp.
327‚Äì333. Wiley.
Collet, D. (1994). Modelling Survival Data in Medical Research. Chapman
and Hall.
Congdon, P. (2001). Bayesian Statistical Modelling. Wiley.

References
705
Cowles, M. K. (1996). Accelerating Monte Carlo Markov chain convergence
for cumulative-link generalized linear models.
Statistics and Comput-
ing 6, 101‚Äì111.
Cowles, M. K. and B. P. Carlin (1996). Markov chain Monte Carlo con-
vergence diagnostics: A comparative review. Journal of the American
Statistical Association 91, 883‚Äì904.
Cox, D. R. (1961). Tests of separate families of hypotheses. Proceedings of
the 4th Berkeley Symposium 1, 105‚Äì123.
Cox, D. R. (1962). Further results on tests of separate families of hypothe-
ses. Journal of the Royal Statistical Society Series B 24, 406‚Äì424.
Cox, D. R. and D. V. Hinkley (1974). Theoretical Statistics. Chapman and
Hall.
Cox, D. R. and H. D. Miller (1965). The Theory of Stochastic Processes.
Chapman and Hall.
Cox, D. R. and N. Reid (1987). Parameter orthogonality and approximate
conditional inference (with discussion). Journal of the Royal Statistical
Society Series B 49, 1‚Äì39.
Cox, D. R. and E. J. Snell (1989). Analysis of Binary Data. Chapman and
Hall.
Crow, J. F. and M. Kimura (1970). An Introduction to Population Genetics
Theory. Harper and Row.
Curnow, R. N. (1961).
The estimation of repeatability and heritability
from records subject to culling. Biometrics 17, 553‚Äì566.
Curnow, R. N. (1972).
The multifactorial model for the inheritance of
liability to disease and its implications for relatives at risk. Biometrics 28,
931‚Äì946.
Curnow, R. N. and C. Smith (1975).
Multifactorial models for familial
diseases in man. Journal of the Royal Statistical Society Series A 138,
131‚Äì169.
Dahlquist, ÀöA, B. and ÀöA. Bj¬®orck (1974). Numerical Methods. Prentice -Hall.
De Finetti, B. (1975a). Theory of Probability, Vol. 1. Wiley.
De Finetti, B. (1975b). Theory of Probability, Vol. 2. Wiley.
Dempster, A. P. (1974). The direct use of likelihood for signiÔ¨Åcance testing.
In O. E. BarndorÔ¨Ä-Nielsen, P. Bl√¶sild, and G. Schou (Eds.), Proceedings
of the Conference on the Foundational Questions in Statistical Inference,
pp. 335‚Äì352. Department of Theoretical Statistics, University of Aarhus.

706
References
Dempster, A. P. (1997). The direct use of likelihood for signiÔ¨Åcance testing.
Statistics and Computing 7, 247‚Äì252.
Dempster, A. P., N. M. Laird, and D. B. Rubin (1977). Maximum likelihood
from incomplete data via de EM algorithm (with discussion). Journal of
the Royal Statistical Society Series B 39, 1‚Äì38.
Dempster, E. R. and I. M. Lerner (1950). Heritability of threshold charac-
ters. Genetics 35, 212‚Äì236.
Dennis, J. E. and R. B. Schnabel (1983). Numerical Methods for Uncon-
strained Optimization and Nonlinear Equations. Prentice‚ÄìHall.
Devroye, L. (1986). Non-Uniform Random Variate Generation. Springer‚Äì
Verlag.
Dickey, J. M. (1968).
Three multidimensional integral identities with
Bayesian applications. Annals of Statistics 39, 1615‚Äì1627.
Doerge, R. W. and G. A. Churchill (1996). Permutation tests for multiple
loci aÔ¨Äecting a quantitative character. Genetics 142, 285‚Äì294.
Draper, N. R. and H. Smith (1981). Applied Regression Analysis. Wiley.
Ducrocq, V., R. L. Quaas, E. Pollak, and G. Casella (1988). Length of
productive life of dairy cows. 2. Variance component estimation and sire
evaluation. Journal of Dairy Science 71, 3071‚Äì3079.
Durbin, R., S. R. Eddy, A. Krogh, and G. J. Mitchison (1998). Biological
Sequence Analysis: Probabilistic Models of Proteins and Nucleic Acids.
Cambridge University Press.
Earman, J. (1992). Bayes or Bust. The MIT Press.
Edwards, A. W. F. (1974). The history of likelihood. International Statis-
tical Review 42, 9‚Äì15.
Edwards, A. W. F. (1992). Likelihood. The John Hopkins University Press.
Efron, B. (1993). Bayes and likelihood calculations from conÔ¨Ådence inter-
vals. Biometrika 80, 3‚Äì26.
Efron, B. (1998). R. A. Fisher in the 21st century. Statistical Science 13,
95‚Äì122.
Efron, B. and D. V. Hinkley (1978). Assessing the accuracy of the maxi-
mum likelihood estimator: Observed versus expected Fisher information.
Biometrika 65, 457‚Äì482.
Elston, R. C. and J. Stewart (1971).
A general model for the genetic
analysis of pedigree data. Human Heredity 21, 523‚Äì542.

References
707
Fahrmeir, L. and G. Tutz (2001). Multivariate Statistical Modelling Based
on Generalized Linear Models. Springer‚ÄìVerlag.
Falconer, D. S. (1965).
The inheritance of liability to certain diseases,
estimated from the incidence among relatives. Annals of Human Genet-
ics 29, 51‚Äì76.
Falconer, D. S. (1967). The inheritance of liability to diseases with variable
age of onset, with particular reference to diabetes mellitus. Annals of
Human Genetics 31, 1‚Äì20.
Falconer, D. S. and T. F. C. Mackay (1996). Introduction to Quantitative
Genetics. Longman.
Famula, T. R. (1981). Exponential stayability model with censoring and
covariates. Journal of Dairy Science 64, 538‚Äì545.
Fan, J., H. Hung, and W. Wong (2000). Geometric understanding of likeli-
hood ratio statistics. Journal of the American Statistical Association 95,
836‚Äì841.
Feller, W. (1970). An Introduction to Probability Theory and its Applica-
tions, Vol. 1. Wiley.
Feng, Z. D. and C. E. McCulloch (1996). Using bootstrap likelihood ratios
in Ô¨Ånite mixture models. Journal of the Royal Statistical Society Series
B 58, 609‚Äì617.
Fernandez, C. and M. F. J. Steel (1998). On Bayesian modelling of fat
tails and skewness. Journal of the American Statistical Association 93,
359‚Äì371.
Fisher, R. A. (1918). The correlation between relatives on the supposition
of Mendelian inheritance. Transactions of the Royal Society of Edin-
burgh 52, 399‚Äì433.
Fisher, R. A. (1920). A mathematical examination of determining accuracy
of an observation by the mean error, and by the mean square error.
Monthly Notices of the Royal Astronomical Society 80, 758‚Äì770.
Fisher, R. A. (1922). On the mathematical foundations of theoretical statis-
tics. Philosophical Transactions of the Royal Society of London Series
A 222, 309‚Äì368.
Fisher, R. A. (1925). Theory of statistical information. Proceedings of the
Cambridge Philosophical Society 22, 700‚Äì725.
Fishman, G. S. (1973). Concepts and Methods in Discrete Event Digital
Simulation. Wiley.

708
References
Flury, B. and A. Zoppe (2000). Exercises in EM. The American Statisti-
cian 54, 207‚Äì209.
Foulley, J. L., D. Gianola, and R. Thompson (1983). Prediction of genetic
merit from data on categorical and quantitative variates with an appli-
cation to calving diÔ¨Éculty, birth weight and pelvic opening. Genetics,
Selection, Evolution 25, 407‚Äì424.
Foulley, J. L., S. Im, D. Gianola, and I. Hoeschele (1987). Empirical Bayes
estimation of parameters for n polygenic binary traits. Genetics, Selec-
tion, Evolution 19, 197‚Äì224.
Foulley, J. L. and E. Manfredi (1991).
Approches statistiques de
l‚Äô¬¥evaluation g¬¥en¬¥etiques des reproducteurs pour des caract`eres binaires
`a seuils. Genetics, Selection, Evolution 23, 309‚Äì338.
Fox, C. (1987). An Introduction to Calculus of Variations. Dover.
Galton, F. (1885). Regression towards mediocrity in hereditary stature.
Journal of the Anthropological Institute 15, 246‚Äì263.
Garc¬¥ƒ±a-Cort¬¥es, L. A. and D. Sorensen (1996). On a multivariate implemen-
tation of the Gibbs sampler. Genetics, Selection, Evolution 28, 121‚Äì126.
Geisser, S. (1993). Predictive Inference: An Introduction. Chapman and
Hall.
Geisser, S. and W. F. Eddy (1979). A predictive approach to model selec-
tion. Journal of the American Statistical Association 74, 153‚Äì160.
Gelfand, A. E. (1996). Model determination using sampling-based methods.
In W. R. Gilks, S. Richardson, and D. J. Spiegelhalter (Eds.), Markov
Chain Monte Carlo in Practice, pp. 145‚Äì161. Chapman and Hall.
Gelfand, A. E. and D. Dey (1994). Bayesian model choice: Asymptotics
and exact calculations. Journal of the Royal Statistical Society Series
B 56, 501‚Äì514.
Gelfand, A. E., D. K. Dey, and H. Chang (1992). Model determination using
predictive distributions with implementation via sampling-based meth-
ods. In J. M. Bernardo, J. O. Berger, A. P. Dawid, and A. F. M. Smith
(Eds.), Bayesian Statistics 4, pp. 147‚Äì167. Oxford University Press.
Gelfand, A. E., S. E. Hills, A. Racine-Poon, and A. F. M. Smith (1990).
Illustration of Bayesian inference in normal data models using Gibbs
sampling. Journal of the American Statistical Association 85, 972‚Äì985.
Gelfand, A. E., S. K. Sahu, and B. P. Carlin (1995). EÔ¨Écient parameteri-
zation for normal linear mixed models. Biometrika 82, 479‚Äì488.

References
709
Gelfand, A. E., S. K. Sahu, and B. P. Carlin (1996). EÔ¨Écient parameter-
izations for generalized linear mixed models. In J. M. Bernardo, J. O.
Berger, A. P. Dawid, and A. F. M. Smith (Eds.), Bayesian Statistics 5,
pp. 165‚Äì180. Oxford University Press.
Gelfand, A. E. and A. F. M. Smith (1990). Sampling based approaches
to calculating marginal densities. Journal of the American Statistical
Association 85, 398‚Äì409.
Gelman, A., J. B. Carlin, H. S. Stern, and D. B. Rubin (1995). Bayesian
Data Analysis. Chapman and Hall.
Gelman, A., X. L. Meng, and H. Stern (1996). Posterior predictive as-
sessment of model Ô¨Åtness via realized discrepancies (with discussion).
Statistica Sinica 6, 733‚Äì807.
Gelman, A. and D. B. Rubin (1992). Inference from iterative simulation
using multiple sequences. Statistical Science 7, 457‚Äì511.
Geman, S. and D. Geman (1984). Stochastic relaxation, Gibbs distribu-
tions, and the Bayesian restoration of images. IEEE Transactions on
Pattern Analysis and Machine Intelligence 6, 721‚Äì741.
George, A. W., K. L. Mengersen, and G. P. Davis (2000). Localization of a
quantitative trait locus via a Bayesian approach. Biometrics 56, 40‚Äì51.
Geweke, J. (1989). Bayesian inference in econometric models using Monte
Carlo integration. Econometrica 57, 1317‚Äì1339.
Geweke, J. (1993). Bayesian treatment of the independent Student-t linear
model. Journal of Applied Econometrics 8, S19‚ÄìS40.
Geyer, C. J. (1992). Practical Markov chain Monte Carlo. Statistical Sci-
ence 7, 473‚Äì511.
Gianola, D. (1982). Theory and analysis of threshold characters. Journal
of Animal Science 54, 1079‚Äì1096.
Gianola, D. and R. L. Fernando (1986). Bayesian methods in animal breed-
ing theory. Journal of Animal Science 63, 217‚Äì244.
Gianola, D., R. L. Fernando, S. Im, and J. L. Foulley (1989). Likelihood es-
timation of quantitative genetic parameters when selection occurs: Mod-
els and problems. Genome 31, 768‚Äì777.
Gianola, D. and J. L. Foulley (1983). Sire evaluation for ordered categorical
data with a threshold model. Genetics, Selection, Evolution 15, 201‚Äì223.

710
References
Gianola, D., S. Im, and F. W. Macedo (1990). A framework for prediction
of breeding values. In D. Gianola and K. Hammond (Eds.), Statistical
Methods for Genetic Improvement of Livestock, pp. 210‚Äì238. Springer‚Äì
Verlag.
Gilks, W. R. and G. O. Roberts (1996). Strategies for improving MCMC.
In W. R. Gilks, S. Richardson, and D. J. Spiegelhalter (Eds.), Markov
Chain Monte Carlo in Practice, pp. 89‚Äì114. Chapman and Hall.
Gilks, W. R. and P. Wild (1992). Adaptive rejection sampling for Gibbs
sampling. Applied Statistics 41, 336‚Äì348.
Gilmour, A. R., R. D. Anderson, and A. L. Rae (1985). The analysis of
binomial data by a generalized linear mixed model. Biometrika 72, 593‚Äì
599.
Go, R. C. P., R. C. Elston, and E. B. Kaplan (1978). EÔ¨Éciency and ro-
bustness of pedigree segregation analysis. American Journal of Human
Genetics 30, 28‚Äì37.
Good, I. J. (1952). Rational decisions. Journal of the Royal Statistical
Society Series B 14, 107‚Äì114.
Good, I. J. (1958). SigniÔ¨Åcance tests in parallel and in series. Journal of
the American Statistical Association 53, 799‚Äì813.
Goodman, L. A. and H. O. Hartley (1958). The precision of unbiased ratio-
type estimators.
Journal of the American Statistical Association 53,
491‚Äì508.
Green, P. (1995).
Reversible jump MCMC computation and Bayesian
model determination. Biometrika 82, 711‚Äì732.
Grimmet, G. R. and D. R. Stirzaker (1992). Probability and Random Pro-
cesses. Clarendon Press.
Gross, A. J. and V. A. Clark (1975). Survival Distributions: Reliability
Applications in the Biomedical Sciences. Wiley.
Grossman, S. I. and J. E. Turner (1974). Mathematics for the Biological
Sciences. Macmillan.
Guo, S. W. and E. A. Thompson (1994). Monte Carlo estimation of mixed
models for large complex pedigrees. Biometrics 50, 417‚Äì432.
Hacking, I. (1965). Logic of Statistical Inference. Cambridge University
Press.
Hager, W. H. (1988). Applied Numerical Linear Algebra. Prentice‚ÄìHall.

References
711
Haldane, J. B. S. (1919). The combination of linkage values and the calcula-
tion of distances between the loci of linked factors. Journal of Genetics 8,
229‚Äì309.
Haldane, J. B. S. (1948). The precision of observed values of small frequen-
cies. Biometrika 35, 297‚Äì303.
Hammersley, J. M. and D. C. Handscomb (1964). Monte Carlo Methods.
Wiley.
Hampel, F. R., E. M. Ronchetti, P. J. Rousseeuw, and W. A. Stahel (1986).
Robust Statistics. Wiley.
Han, C. and B. P. Carlin (2001).
Markov chain Monte Carlo methods
for computing Bayes Factors: A comparative review.
Journal of the
American Statistical Association 96, 1122‚Äì1132.
Harville, D. A. (1974). Bayesian inference of variance components using
only error contrasts. Biometrika 61, 383‚Äì385.
Harville, D. A. (1977). Maximum likelihood approaches to variance com-
ponent estimation and to related problems. Journal of the American
Statistical Association 72, 320‚Äì340.
Harville, D. A. and R. W. Mee (1984).
A mixed model procedure for
analyzing ordered categorical data. Biometrics 40, 393‚Äì408.
Hastings, W. K. (1970).
Monte Carlo sampling methods using Markov
chains and their application. Biometrika 57, 97‚Äì109.
Hazel, L. N. (1943). The genetic basis for constructing selection indices.
Genetics 28, 476‚Äì490.
Heath, S. C. (1997). Markov chain Monte Carlo segregation and linkage
analysis for oligogenic models. American Journal of Human Genetics 61,
748‚Äì760.
Heisenberg, W. (1958).
The representation of nature in contemporary
physics. Daedalus 87, 95‚Äì108.
Henderson, C. R. (1953). Estimation of variance and covariance compo-
nents. Biometrics 9, 226‚Äì252.
Henderson, C. R. (1963). Selection index and expected selection advance.
In W. D. Hanson and H. F. Robinson (Eds.), Statistical Genetics and
Plant Breeding, pp. 141‚Äì163. National Academy of Sciences, National
Research Council Publication No. 982, Washington, D C.

712
References
Henderson, C. R. (1973). Sire evaluation and genetic trends. In Proceedings
of the Animal Breeding and Genetics Symposium in Honor of Dr. J. L.
Lush, pp. 10‚Äì41. American Society of Animal Science.
Henderson, C. R. (1975). Best linear unbiased estimation and prediction
under a selection model. Biometrics 31, 423‚Äì447.
Henderson, C. R. (1984). Applications of Linear Models in Animal Breed-
ing. University of Guelph.
Henderson, C. R., O. Kempthorne, S. R. Searle, and C. N. Von Krosigk
(1959).
Estimation of environmental and genetic trends from records
subject to culling. Biometrics 15, 192‚Äì218.
Henderson, M. and M. C. Meyer (2001). Exploring the conÔ¨Ådence interval
for a binomial parameter in a Ô¨Årst course in statistical computing. The
American Statistician 55, 337‚Äì344.
Heringstad, B., R. Rekaya, D. Gianola, G. Klemetsdal, and K. A. Weigel
(2001). Bayesian analysis of liability of clinical mastitis in Norwegian
cattle with a threshold model: EÔ¨Äects of data sampling and model spec-
iÔ¨Åcation. Journal of Dairy Science 84, 2337‚Äì2346.
Hills, S. E. and A. F. M. Smith (1992). Parameterization issues in Bayesian
inference (with discussion). In J. M. Bernardo, J. O. Berger, A. P. Dawid,
and A. F. M. Smith (Eds.), Bayesian Statistics 4, pp. 227‚Äì246. Oxford
University Press.
Hills, S. E. and A. F. M. Smith (1993).
Diagnostic plots for improved
parameterization in Bayesian inference. Biometrika 80, 61‚Äì74.
Hobert, J. P. and G. Casella (1996).
The eÔ¨Äect of improper priors on
Gibbs sampling in hierarchical linear models. Journal of the American
Statistical Association 91, 1461‚Äì1473.
Hoel, P. G., S. C. Port, and C. J. Stone (1971). Introduction to Probability
Theory. Houghton MiÔ¨Ñin.
Hoerl, A. E. and R. W. Kennard (1970). Ridge regression. Technomet-
rics 12, 55‚Äì67; 69‚Äì82.
Hoeschele, I. and B. Tier (1995). Estimation of variance components of
threshold characters by marginal posterior modes and means via Gibbs
sampling. Genetics, Selection, Evolution 27, 519‚Äì540.
Hoeting, J. A., D. Madigan, A. E. Raftery, and C. T. Volinsky (1999).
Bayesian model averaging: A tutorial. Statistical Science 14, 382‚Äì417.
Hogg, R. V. and A. T. Craig (1995). Introduction to Mathematical Statis-
tics. Prentice Hall.

References
713
Howson, C. and P. Urbach (1989).
ScientiÔ¨Åc Reasoning: The Bayesian
Approach. Open Court, La Salle.
Jamrozik, J. and L. R. SchaeÔ¨Äer (1997). Estimates of genetic parameters for
a test-day model with random regressions for production of Ô¨Årst lactation
Holsteins. Journal of Dairy Science 80, 762‚Äì770.
Janss, L. L. G., R. Thompson, and J. A. M. Van Arendonk (1995). Appli-
cation of Gibbs sampling for inference in a mixed major gene-polygenic
inheritance model in animal populations. Theoretical and Applied Ge-
netics 91, 1137‚Äì1147.
Jaynes, E. T. (1957).
On the rationale of maximum-entropy methods.
Physics Review 106, 620‚Äì630.
Jaynes, E. T. (1994).
Probability Theory: The Logic of Science.
http://omega.albany.edu:8008/JaynesBook.
JeÔ¨Äreys, H. (1961). Theory of Probability. Clarendon Press.
Jensen, C. S., U. Kj√¶rulÔ¨Ä, and A. Kong (1995). Blocking Gibbs sampling in
very large probabilistic expert systems. International Journal of Human
Computer Studies 42, 647‚Äì666.
Jensen, J. (1994). Bayesian analysis of bivariate mixed models with one
continuous and one binary trait using the Gibbs sampler. In Proceedings
of the 5th World Congress of Genetics Applied to Livestock Production,
Vol. 18, pp. 333‚Äì336. University of Guelph.
Jensen, J., C. S. Wang, D. Sorensen, and D. Gianola (1994).
Bayesian
inference on variance and covariance components for traits inÔ¨Çuenced
by maternal and direct genetic eÔ¨Äects using the Gibbs sampler. Acta
Agricultura Scandinavica 44, 193‚Äì201.
Johnson, N. L. and S. Kotz (1969). Distributions in Statistics: Discrete
Distributions. Wiley.
Johnson, N. L. and S. Kotz (1970a). Distributions in Statistics: Continuous
Univariate Distributions, Vol. 1. Wiley.
Johnson, N. L. and S. Kotz (1970b). Distributions in Statistics: Continuous
Univariate Distributions, Vol. 2. Wiley.
Johnson, N. L. and S. Kotz (1972). Distributions in Statistics: Continuous
Multivariate Distributions. John Wiley.
Kackar, R. N. and D. A. Harville (1981). Ubiasedness of two-stage estima-
tion and prediction procedures for mixed linear models. Communications
in Statistics Series A: Theory and Methods 10, 1249‚Äì1261.

714
References
Kadarmideen, H. N., R. Rekaya, and D. Gianola (2002). Genetic parame-
ters for clinical mastitis in Holstein Freisians in the United Kingdom: A
Bayesian analysis. Animal Science. In press.
KalbÔ¨Çeisch, J. D. and D. A. Sprott (1970). Application of likelihood meth-
ods to models involving large numbers of parameters (with discussion).
Journal of the Royal Statistical Society Series B 32, 175‚Äì208.
KalbÔ¨Çeisch, J. D. and D. A. Sprott (1973). Marginal and conditional like-
lihoods. Sankya A 35, 311‚Äì328.
Kaplan, W. (1993). Advanced Calculus. Addison and Wesley.
Karlin, S. and H. M. Taylor (1975). A First Course in Stochastic Processes.
Academic Press.
Kass, E. R. and A. E. Raftery (1995). Bayes factors. Journal of the Amer-
ican Statistical Association 90, 773‚Äì795.
Kass, R. E. (1995). A reference Bayesian test for nested hypotheses and its
relationship to the Schwarz criterion. Journal of the American Statistical
Association 90, 928‚Äì934.
Kass, R. E., B. P. Carlin, A. Gelman, and R. M. Neal (1998). Markov
chain Monte Carlo in practice: A roundtable discussion. The American
Statistician 52, 93‚Äì100.
Keller, E. F. (2000). The Century of the Gene. Harvard University Press.
King, G. (1989). Unifying Political Methodology. The Likelihood Theory of
Statistical Inference. Cambridge University Press.
Kirkpatrick, M., W. G. Hill, and R. Thompson (1994).
Estimating the
covariance structure of traits during growth and ageing, illustrated with
lactation in dairy cattle. Genetical Research 64, 57‚Äì69.
Kleinbaum, D. G. (1996). Survival Analysis. Springer‚ÄìVerlag.
Knott, S. A. and C. S. Haley (1992). Aspects of maximum likelihood meth-
ods for the mapping of quantitative trait loci in line crosses. Genetical
Research 60, 139‚Äì151.
Koerkhuis, A. N. M. and R. Thompson (1997). Models to estimate maternal
eÔ¨Äects for juvenile body weights in broiler chickens. Genetics, Selection,
Evolution 29, 225‚Äì249.
Korsgaard, I. R., A. H. Andersen, and D. Sorensen (1999). A useful repa-
rameterisation to obtain samples from conditional inverse Wishart dis-
tributions. Genetics, Selection, Evolution 31, 177‚Äì181.

References
715
Korsgaard, I. R., M. S. Lund, D. Sorensen, D. Gianola, P. Madsen, and
J. Jensen (2002). Multivariate Bayesian analysis of Gaussian, right cen-
sored Gaussian, ordered categorical and binary traits using Gibbs sam-
pling. Genetics, Selection, Evolution. In press.
Kullback, S. (1968). Information Theory and Statistics. Wiley.
Laird, N. M. and J. H. Ware (1982). Random-eÔ¨Äects models for longitudinal
data. Biometrics 38, 963‚Äì974.
Lange, K. (1995). A Quasi-Newton acceleration of the EM algorithm. Jour-
nal of the Royal Statistical Society Series B 44, 226‚Äì233.
Lange, K. (1997). Mathematical and Statistical Methods for Genetic Anal-
ysis. Springer‚ÄìVerlag.
Lange, K. and R. C. Elston (1975).
Extensions to pedigree analysis. I.
Likelihood calculations for simple and complex pedigrees. Human Hered-
ity 25, 95‚Äì105.
Lange, K. and T. M. Goradia (1987). An algorithm for automatic genotype
elimination. American Journal of Human Genetics 40, 250‚Äì256.
Lange, K. and J. S. Sinsheimer (1993). Normal/independent distributions
and their applications in robust regression. Journal of Computer Graph-
ics and Statistics 2, 175‚Äì198.
Lee, J. K. and D. C. Thomas (2000). Performance of Markov chain Monte
Carlo approaches for mapping genes in oligogenic models with unknown
number of loci. American Journal of Human Genetics 67, 1232‚Äì1250.
Lee, P. M. (1989). Bayesian Statistics: An Introduction. Edward Arnold.
Lee, Y. and J. A. Nelder (1996). Hierarchical generalized linear models
(with discussion). Journal of the Royal Statistical Society Series B 58,
619‚Äì678.
Lehmann, E. L. (1999).
Elements of Large-Sample Theory.
Springer‚Äì
Verlag.
Lehmann, E. L. and G. Casella (1998).
Theory of Point Estimation.
Springer‚ÄìVerlag.
Leonard, T. and J. S. Hsu (1999). Bayesian Methods. Cambridge University
Press.
Lindley, D. V. (1956). On a measure of information provided by an exper-
iment. Annals of Mathematical Statistics 27, 986‚Äì1005.
Lindley, D. V. (1957). A statistical paradox. Biometrika 44, 187‚Äì192.

716
References
Lindley, D. V. and A. F. M. Smith (1972). Bayesian estimates for the linear
model. Journal of the Royal Statistical Society Series B 34, 1‚Äì41.
Little, R. J. A. and D. B. Rubin (1987). Statistical Analysis with Missing
Data. Wiley.
Liu, C. and D. B. Rubin (1995). ML estimation of the t distribution using
EM and its extensions, ECM, and ECME. Statistica Sinica 5, 19‚Äì39.
Liu, J. S. (1994). The collapsed Gibbs sampler in Bayesian computations
with applications to a gene-regulation problem. Journal of the American
Statistical Association 89, 958‚Äì966.
Liu, J. S. (2001). Monte Carlo Strategies in ScientiÔ¨Åc Computing. Springer‚Äì
Verlag.
Liu, J. S., H. W. Wong, and A. Kong (1994). Covariance structure of the
Gibbs sampler with applications to the comparisons of estimators and
augmentation schemes. Biometrika 81, 27‚Äì40.
Lo, Y., N. R. Mendell, and D. B. Rubin (2001). Testing the number of
components in a normal mixture. Biometrika 88, 767‚Äì778.
Louis, T. A. (1982). Finding the observed information matrix when using
the EM algorithm. Journal of the Royal Statistical Society Series B 44,
226‚Äì233.
Lund, M. S. and C. S. Jensen (1999). Blocking Gibbs sampling in the mixed
inheritance model using graph theory. Genetics, Selection, Evolution 31,
3‚Äì24.
Lynch, M. and B. Walsh (1998). Genetics and Analysis of Quantitative
Traits. Sinauer Associates.
MacCluer, J. W., J. L. Vandeburg, B. Read, and O. A. Ryder (1986).
Pedigree analysis by computer simulation. Zoo Biology 5, 147‚Äì160.
Madigan, D. and A. E. Raftery (1994). Model selection and accounting for
model uncertainty in graphical models using Occam‚Äôs window. Journal
of the American Statistical Association 89, 1535‚Äì1546.
Mal¬¥ecot, G. (1947). Annotated translation by D. Gianola of: Les criteres
statistiques et la subjectivite de la connaisance scientiÔ¨Åque (Statistical
methods and the subjective basis of scientiÔ¨Åc knowledge), by G. Mal¬¥ecot,
(1947), Annales de l‚ÄôUniversite de Lyon, X, 43-74. Genetics, Selection,
Evolution 31, 269‚Äì298.
Mal¬¥ecot, G. (1969). The Mathematics of Heredity. W. H. Freeman. Origi-
nally published in 1948 by Masson et Cie.

References
717
Mardia, K. V., J. T. Kent, and J. M. Bibby (1979). Multivariate Analysis.
Academic Press.
Marsaglia, G. and A. Zaman (1993). The Kiss Generator. Technical Report,
Department of Statistics, University of Florida.
Martinez, V., L. B¬®unger, and W. G. Hill (2000). Analysis of response to 20
generations of selection for body composition in mice: Fit to inÔ¨Ånitesimal
model. Genetics, Selection, Evolution 32, 3‚Äì21.
McCullagh, P. and J. A. Nelder (1989). Generalized Linear Models. Chap-
man and Hall.
McCulloch, C. E. (1994). Maximum likelihood variance components estima-
tion for binary data. Journal of the American Statistical Association 89,
330‚Äì335.
McCulloch, R. E. and P. E. Rossi (1991). A Bayesian approach to testing
the arbitrage pricing theory. Journal of Econometrics 49, 141‚Äì168.
McLachlan, G. J. and T. Krishnan (1997). The EM Algorithm and Exten-
sions. Wiley.
Meeker, W. Q. and L. A. Escobar (1995). Teaching about approximate con-
Ô¨Ådence regions based on maximum likelihood estimation. The American
Statistician 49, 48‚Äì53.
Meilijson, I. (1989). A fast improvement to the EM algorithm on its own
terms. Journal of the Royal Statistical Society Series B 51, 127‚Äì138.
Meng, X. and D. B. Rubin (1991). Using EM to obtain asymptotic variance-
covariance matrices: The SEM algorithm. Journal of the American Sta-
tistical Association 86, 899‚Äì909.
Mengersen, K. L., C. P. Robert, and C. Guihenneuc-Jouyaux (1999).
MCMC convergence diagnostics: A reviewww (with discussion).
In
J. M. Bernardo, J. O. Berger, A. P. Dawid, and A. F. M. Smith (Eds.),
Bayesian Statistics 6, pp. 415‚Äì440. Oxford University Press.
Metropolis, N., A. W. Rosenbluth, M. N. Rosenbluth, A. H. Teller, and
E. Teller (1953). Equations of state calculations by fast computing ma-
chines. Journal of Chemical Physics 21, 1087‚Äì1092.
Meyer, K. (1999). Estimates of genetic and phenotypic covariance functions
for post-weaning growth and mature weight of beef cows. Journal of
Animal Breeding and Genetics 116, 181‚Äì205.
Meyn, S. P. and R. L. Tweedie (1993).
Markov Chains and Stochastic
Stability. Springer‚ÄìVerlag.

718
References
Milliken, G. A. and D. E. Johnson (1992). Analysis of Messy Data, Vol. I:
Designed Experiments. Chapman and Hall.
Misztal, I., D. Gianola, and J. L. Foulley (1989). Computing aspects of
nonlinear methods of sire evaluation for categorical data.
Journal of
Dairy Science 72, 1557‚Äì1568.
Mood, A. M., F. A. Graybill, and D. C. Boes (1974). Introduction to the
Theory of Statistics. McGraw-Hill.
Moreno, C., D. Sorensen, L. A. Garc¬¥ƒ±a-Cort¬¥es, L. Varona, and J. Altarriba
(1997). On biased inferences about variance components in the binary
threshold model. Genetics, Selection, Evolution 29, 145‚Äì160.
Morton, N. E. and C. J. MacLean (1974). Analysis of family resemblance.
III. Complex segregation of quantitative traits.
American Journal of
Human Genetics 26, 489‚Äì503.
Nandram, B. and M. H. Chen (1996). Reparameterizing the generalized lin-
ear model to accelerate Gibbs sampler convergence. Journal of Statistical
Computation and Simulation 54, 129‚Äì144.
Nelder, J. A. and R. W. M. Wedderburn (1972). Generalized linear models.
Journal of the Royal Statistical Society Series A 135, 370‚Äì384.
Newton, M. A. and A. E. Raftery (1994). Approximate Bayesian inference
by the weighted likelihood bootstrap (with discussion). Journal of the
Royal Statistical Society Series B 56, 1‚Äì48.
Neyman, J. and E. S. Pearson (1928). On the use and interpretation of
certain test criteria for purposes of statistical inference. Parts I and II.
Biometrika 20A, 175‚Äì240; 263‚Äì294.
Norris, J. R. (1997). Markov Chains. Cambridge University Press.
Oakes, D. (1999). Direct calculation of the information matrix via the EM
algorithm. Journal of the Royal Statistical Society Series B 61, 479‚Äì482.
Odell, P. L. and A. H. Feiveson (1966). A numerical procedure to gen-
erate a sample covariance matrix. Journal of the American Statistical
Association 61, 198‚Äì203.
O‚ÄôHagan, A. (1994).
Kendall‚Äôs Advanced Theory of Statistics, Vol. 2B:
Bayesian Inference. Edward Arnold.
Ott, J. (1999). Analysis of Human Genetic Linkage. John Hopkins Uni-
versity Press.
Patterson, H. D. and R. Thompson (1971). Recovery of inter-block infor-
mation when block sizes are unequal. Biometrika 58, 545‚Äì554.

References
719
Pauler, D. K., J. C. WakeÔ¨Åeld, and R. E. Kass (1999). Bayes factors and
approximations for variance component models. Journal of the American
Statistical Association 94, 1242‚Äì1253.
Pawitan, Y. (2000).
A reminder of the fallibility of the Wald statistic:
Likelihood explanation. The American Statistician 54, 54‚Äì56.
Pearson, K. (1900). Mathematical contributions to the theory of evolution.
VIII. On the inheritance of characters not capable of exact quantitative
measurement. Philosophical Transactions of the Royal Society of London
Series A 195, 79‚Äì121.
Pearson, K. (1903). Mathematical contributions to the theory of evolution.
XI. On the inÔ¨Çuence of natural selection on the variability and correlation
of organs. Philosophical Transactions of the Royal Society of London
Series A 200, 1‚Äì66.
Peskun, P. H. (1973).
Optimum Monte Carlo sampling using Markov
chains. Biometrika 60, 607‚Äì612.
Popper, K. R. (1972). The Logic of ScientiÔ¨Åc Discovery. Hutchinson.
Popper, K. R. (1982). Quantum Theory and the Schism in Physics. Rout-
ledge.
Priestley, M. B. (1981).
Spectral Analysis and Time Series.
Academic
Press.
Propp, J. G. and D. B. Wilson (1996). Exact sampling with coupled Markov
chains and applications to statistical mechanics. Random Structures and
Algorithms 9, 223‚Äì252.
Raftery, A. E., D. Madigan, and J. A. Hoeting (1997). Model selection and
accounting for model uncertainty in linear regression models. Journal of
the American Statistical Association 92, 179‚Äì191.
Raj, D. (1968). Sampling Theory. McGraw-Hill.
Rao, C. R. (1947). Large sample tests of statistical hypotheses concerning
several parameters with applications to problems of estimation. Proceed-
ings of the Cambridge Philosophical Society 44, 50‚Äì57.
Rao, C. R. (1973). Linear Statistical Inference and its Applications. Wiley.
Reid, N. (1995). The roles of conditioning in inference. Statistical Sci-
ence 10, 138‚Äì199.
Reid, N. (2000). Likelihood. Journal of the American Statistical Associa-
tion 95, 1335‚Äì1340.

720
References
Richardson, S. and P. Green (1997).
On Bayesian analysis of mixtures
with an unknown number of components (with discussion). Journal of
the Royal Statistical Society Series B 59, 731‚Äì792.
Ripley, B. (1987). Stochastic Simulation. Wiley.
Robert, C. P. (1994). The Bayesian Choice. Springer‚ÄìVerlag.
Robert, C. P. (1998). Discretization and MCMC Convergence Assessment.
Lecture Notes in Statistics, Vol. 135. Springer‚ÄìVerlag.
Robert, C. P. and G. Casella (1999).
Monte Carlo Statistical Methods.
Springer‚ÄìVerlag.
Roberts, G. O., A. Gelman, and W. R. Gilks (1997). Weak convergence
and optimal scaling of random walk Metropolis algorithms. Annals of
Applied Probability 7, 110‚Äì120.
Roberts, G. O. and S. K. Sahu (1997).
Updating schemes, correlation
structure, blocking and parameterization for the Gibbs sampler. Journal
of the Royal Statistical Society Series B 59, 291‚Äì317.
Robertson, A. (1977). The eÔ¨Äect of selection on the estimation of genetic
parameters. Journal of Animal Breeding and Genetics 94, 131‚Äì135.
Robertson, A. and I. M. Lerner (1949).
The heritability of all-or-none
traits: Viability of poultry. Genetics 34, 395‚Äì411.
Rodriguez-Zas, S. L. (1998). Bayesian Analysis of Somatic Cell Score Lac-
tation Patterns in Holstein Cows Using Nonlinear Mixed EÔ¨Äects Models.
Ph. D. thesis, University of Wisconsin-Madison.
RoÔ¨Ä, D. E. (1997).
Evolutionary Quantitative Genetics.
Chapman and
Hall.
Rogers, W. H. and J. W. Tukey (1972). Understanding some long-tailed
distributions. Statistica Neerlandica 26, 211‚Äì226.
Rosa, G. J. M. (1998). Analise Bayesiana de Models Lineares Mistos Ro-
bustos Via Amostrador de Gibbs. Ph. D. thesis, Escola Superior de Agri-
cultura Luis de Queiroz, Piracicaba, Sao Paulo, Brazil.
Rosa, G. J. M., D. Gianola, and J. I. Urioste (2001). Assessing relationships
between genetic evaluations using robust regression with an application
to Holsteins in Uruguay.
Acta Agricultura Scandinavica Series A 51,
21‚Äì34.
Ross, S. M. (1997). Simulation. Academic Press.
Royall, R. (1997). Statistical Evidence. Chapman and Hall.

References
721
Rubin, D. B. (1976). Inference and missing data. Biometrika 63, 581‚Äì592.
Rubin, D. B. (1987a). Multiple Imputation for Nonresponse in Surveys.
Wiley.
Rubin, D. B. (1987b).
A noniterative sampling/importance resampling
alternative to the data augmentation algorithm for creating a few im-
putations when fractions of missing information are modest: The SIR
algorithm. Discussion of Tanner and Wong. Journal of the American
Statistical Association 82, 543‚Äì546.
Rubin, D. B. (1988). Using the SIR algorithm to simulate posterior dis-
tributions (with discussion). In J. M. Bernardo, M. H. DeGroot, D. V.
Lindley, and A. F. M. Smith (Eds.), Bayesian Statistics 3, pp. 395‚Äì402.
Oxford University Press.
Savage, L. J. (1972). The Foundations of Statistics. Wiley.
Schafer, J. L. (2000). Analysis of Incomplete Multivariate Data. Chapman
and Hall/CRC Press.
Schwarz, G. (1978).
Estimating the dimension of a model.
Annals of
Statistics 6, 461‚Äì464.
Scott, D. W. (1992). Multivariate Density Estimation. Wiley.
Searle, S. R. (1971). Linear Models. Wiley.
Searle, S. R. (1982). Matrix Algebra Useful for Statistics. Wiley.
Searle, S. R., G. Casella, and C. E. McCulloch (1992). Variance Compo-
nents. Wiley.
Severini, T. A. (1998). Lilelihood functions for inference in the presence of
a nuisance parameter. Biometrika 85, 507‚Äì522.
Severini, T. A. (2000). Likelihood Methods in Statistics. Oxford University
Press.
Shannon, C. E. (1948). A mathematical theory of communication. Bell
System Technical Journal 27, 623‚Äì656.
Sheehan, N. A. (2000). On the application of Markov chain Monte Carlo
methods to genetic analyses on complex pedigrees. International Statis-
tical Review 68, 83‚Äì110.
Sheehan, N. A., B. Guldbrandtsen, M. S. Lund, and D. Sorensen (2002).
Bayesian McMC mapping of quantitative trait loci in a half-sib design: A
graphical model perspective. International Statistical Review. In press.

722
References
Sheehan, N. A. and A. Thomas (1993). On the irreducibility of a Markov
chain deÔ¨Åned on a space of genotype conÔ¨Ågurations by a sampling
scheme. Biometrics 49, 163‚Äì175.
Sillanp¬®a¬®a, M. J. and E. Arjas (1998). Bayesian mapping of multiple quan-
titative trait loci from incomplete inbred line cross data. Genetics 148,
1373‚Äì1388.
Sillanp¬®a¬®a, M. J. and E. Arjas (1999). Bayesian mapping of multiple quan-
titative trait loci from incomplete outbred oÔ¨Äspring data. Genetics 151,
1605‚Äì1619.
Silverman, B. (1992). Density Estimation. Chapman and Hall.
Sivia, D. S. (1996). Data Analysis. A Bayesian Tutorial. Oxford University
Press.
Smith, A. F. M. and A. E. Gelfand (1992). Bayesian statistics without tears:
A sampling‚Äìresampling perspective. The American Statistician 46, 84‚Äì
88.
Smith, C. A. B. (1959). Some comments on the statistical methods used in
linkage investigations. American Journal of Human Genetics 11, 289‚Äì
304.
Smith, H. F. (1936). A discriminant function for plant selection. Annals
of Eugenics 7, 240‚Äì250.
Sorensen, D. (1996).
Gibbs Sampling in Quantitative Genetics.
Danish
Institute of Agricultural Sciences; Internal Report 82, 192 pp.
Sorensen, D., S. Andersen, D. Gianola, and I. R. Korsgaard (1995).
Bayesian inference in threshold models using Gibbs sampling. Genet-
ics, Selection, Evolution 27, 229‚Äì249.
Sorensen, D., R. L. Fernando, and D. Gianola (2001). Inferring the tra-
jectory of genetic variance in the course of artiÔ¨Åcial selection. Genetical
Research 77, 83‚Äì94.
Sorensen, D., A. Vernersen, and S. Andersen (2000). Bayesian analysis of
response to selection: A case study using litter size in Danish Yorkshire
pigs. Genetics 156, 283‚Äì295.
Sorensen, D., C. S. Wang, J. Jensen, and D. Gianola (1994). Bayesian anal-
ysis of genetic change due to selection using Gibbs sampling. Genetics,
Selection, Evolution 26, 333‚Äì360.
Spiegelhalter, D. J., N. G. Best, B. P. Carlin, and A. van der Linde (2002).
Bayesian measures of model complexity and Ô¨Åt (with discussion). Journal
of the Royal Statistical Society Series B. In press.

References
723
Stein, S. K. (1977). Calculus and Analytic Geometry. McGraw-Hill.
Stephens, D. A. and R. D. Fisch (1998). Bayesian analysis of a quantita-
tive trait locus data using reversible jump Markov chain Monte Carlo.
Biometrics 54, 1334‚Äì1347.
Stramer, O. and R. L. Tweedie (1998).
Langevin-type models II: Self-
targetting candidates for MCMC algorithms. Methodology and Comput-
ing in Applied Probability 1, 307‚Äì328.
Strand¬¥en, I. (1996).
Robust Mixed EÔ¨Äects Linear Models with t-
Distributions and Applications to Dairy Cattle Breeding. Ph. D. thesis,
University of Wisconsin-Madison.
Strand¬¥en, I. and D. Gianola (1998). Atenuating eÔ¨Äects of preferential treat-
ment with Student-t mixed linear models: A simulation study. Genetics,
Selection, Evolution 30, 565‚Äì583.
Strand¬¥en, I. and D. Gianola (1999).
Mixed eÔ¨Äects linear models with
t-distributions for quantitative genetic analysis: A Bayesian approach.
Genetics, Selection, Evolution 31, 25‚Äì42.
Stuart, A. and J. K. Ord (1987). Kendall‚Äôs Advanced Theory of Statistics.
Distribution Theory. Edward Arnold.
Stuart, A. and J. K. Ord (1991). Kendall‚Äôs Advanced Theory of Statistics.
Classical Inference and Relationship. Edward Arnold.
Swendsen, R. and J. Wang (1987).
Non-universal critical dynamics in
Monte Carlo simulations. Physical Review Letters 58, 86‚Äì88.
Tanner, M. A. (1996). Tools for Statistical Inference. Springer‚ÄìVerlag.
Tanner, M. A. and W. Wong (1987).
The calculation of posterior dis-
tributions by data augmentation. Journal of the American Statistical
Association 82, 528‚Äì550.
Thompson, E. A. (2001). Monte Carlo methods on genetic structures. In
O. E. BarndorÔ¨Ä-Nielsen, D. R. Cox, and C. Kl¬®uppelberg (Eds.), Complex
Stochastic Systems, pp. 175‚Äì218. Chapman and Hall.
Thompson, E. A. and S. C. Heath (2000). Estimation of conditional mul-
tilocus gene identity among relatives. In F. Seiller-Moiseiwitsch (Ed.),
Statistics in Molecular Biology and Genetics: Selected Proceedings of a
1997 Joint AMS-IMS-SIAM Summer Conference on Statistics in Molec-
ular Biology, pp. 95‚Äì113. Institute of Mathematical Statistics, Hayward,
CA.: IMS Lecture Note-Monograph Series, Volume 33.

724
References
Thompson, R. (1973). The estimation of variance and covariance compo-
nents with an application when records are subject to culling. Biomet-
rics 29, 527‚Äì550.
Thompson, R. (1976). Estimation of quantitative genetic parameters. In
E. Pollak, O. Kempthorne, and T. B. Bailey (Eds.), Proceedings of the
International Conference on Quantitative Genetics, pp. 639‚Äì657. Iowa
State University.
Thompson, R. (1980). Maximum likelihood estimation of variance compo-
nents. Mathematische Operationsforschung und Statistik 11, 545‚Äì561.
Tierney, L. (1994).
Markov chains for exploring posterior distributions
(with discussion). Annals of Statistics 22, 1701‚Äì1786.
Tierney, L. and J. B. Kadane (1989). Accurate approximations for posterior
moments and marginal densities.
Journal of the American Statistical
Association 81, 82‚Äì86.
Toutenburg, H. (1982). Prior Information in Linear Models. Wiley.
Uimari, P. and I. Hoeschele (1997). Mapping linked quantitative trait loci
using Bayesian analysis and Markov chain Monte Carlo algorithms. Ge-
netics 146, 735‚Äì743.
Van Tassell, C. P. and L. D. Van Vleck (1996). Multiple-trait Gibbs sampler
for animal models: Flexible programs for Bayesian and likelihood-based
(co)variance component inferences. Journal of Animal Science 74, 2586‚Äì
2597.
Van Tassell, C. P., L. D. Van Vleck, and K. E. Gregory (1998). Bayesian
analysis of twinning and ovulation rates using a multiple-trait threshold
model and Gibbs sampling. Journal of Animal Science 76, 2048‚Äì2061.
Van Vleck, L. D. (1993). Selection Index and Introduction to Mixed Model
Methods. CRC Press.
Vuong, Q. H. (1989). Likelihood ratio tests for model selection and non-
nested hypotheses. Econometrica 57, 307‚Äì333.
Waagepetersen, R. and D. Sorensen (2001). A tutorial on reversible jump
MCMC with a view towards applications in QTL-mapping. International
Statistical Review 69, 49‚Äì61.
Wang, C. S., D. Gianola, D. Sorensen, J. Jensen, A. Christensen, and
J. J. Rutledge (1994). Response to selection in Danish Landrace pigs: A
Bayesian analysis. Theoretical and Applied Genetics 88, 220‚Äì230.

References
725
Wang, C. S., R. L. Quaas, and E. J. Pollak (1997). Bayesian analysis of
calving ease scores and birth weights. Genetics, Selection, Evolution 29,
117‚Äì143.
Wei, G. C. G. and M. A. Tanner (1990). A Monte Carlo implementation
of the EM algorithm and the poor man‚Äôs data augmentation algorithm.
Journal of the American Statistical Association 85, 699‚Äì704.
Weinstock, R. (1974). Calculus of Variations with Applications to Physics
and Engineering. Dover.
Weir, B. S. (1996). Genetic Data Analysis II. Sinauer Associates.
Wiggans, G. R. and M. E. Goddard (1997). A computationally feasible
test-day model for genetic evaluation of yield traits in the United States.
Journal of Dairy Science 80, 1795‚Äì1800.
Wilks, S. S. (1938). The large-sample distribution of the likelihood ratio for
testing composite hypotheses. The Annals of Mathematical Statistics 9,
60‚Äì62.
Willham, R. L. (1963). The covariance between relatives for characters
composed of components contributed by related individuals.
Biomet-
rics 19, 18‚Äì27.
Williamson, R. E., R. H. Crowell, and H. F. Trotter (1972). Calculus of
Vector Functions. Prentice‚ÄìHall.
Wilton, J. W., D. A. Evans, and L. D. Van Vleck (1968). Selection indices
for quadratic models of total merit. Biometrics 24, 937‚Äì949.
WolÔ¨Ånger, R. (1993). Laplace‚Äôs approximation for nonlinear mixed models.
Biometrika 80, 791‚Äì795.
WolÔ¨Ånger, R. and X. Lin (1997). Two Taylor-series approximation methods
for nonlinear mixed models. Computational Statistics and Data Analy-
sis 25, 465‚Äì490.
Wright, S. (1934). An analysis of variability in number of digits in an inbred
strain of guinea pigs. Genetics 19, 506‚Äì536.
Wright, S. (1968). Evolution and the Genetics of Populations. Genetic and
Biometric Foundations. University of Chicago.
Yi, N. and S. Xu (2000). Bayesian mapping of quantitative trait loci under
the identity-by-descent-based variance component model. Genetics 156,
411‚Äì422.
Zellner, A. (1971). An Introduction to Bayesian Inference in Econometrics.
Wiley.

726
References
Zellner, A. (1976). Bayesian and non-Bayesian analysis of the regression
model with multivariate Student-t error terms. Journal of the American
Statistical Association 71, 400‚Äì405.
Zellner, A. and R. HighÔ¨Åeld (1988). Calculation of maximum entropy distri-
butions and approximation of marginal posterior distributions. Journal
of Econometrics 37, 195‚Äì210.

List of Citations
Abramowitz and Stegun (1972),
86, 368
Agresti (1989), 626
Agresti (1990), 626
Agresti (1996), 626
Aitken (1934), 62
Akaike (1973), 421
Albert and Chib (1993), 435, 598,
606, 609
Albert and Chib (1995), 292, 435
Anderson and Aitkin (1985), 606
Anderson (1984), 43, 51, 56
Applebaum (1996), 334, 335, 338,
340, 342, 370, 371
Baldi and Brunak (1998), 338
BarndorÔ¨Ä-Nielsen and Cox (1994),
166, 181
BarndorÔ¨Ä-Nielsen (1983), 133
BarndorÔ¨Ä-Nielsen (1986), 181
BarndorÔ¨Ä-Nielsen (1991), 181
Barnett and Lewis (1995), 434,
591
Barnett (1999), 219
Bates and Watts (1988), 292
Bayarri (1981), 81, 104
Bayes (1763), 81
Becker (1984), 89
Berger and Bernardo (1992), 337,
356, 397
Berger and Pericchi (1996), 422,
423
Berger (1985), 406
Bernardo and Smith (1994), 4, 104,
120, 214, 219, 289, 330,
331, 333, 342, 356, 364,
379, 382, 393, 395, 397,
405, 406, 411, 412, 564,
607
Bernardo (1979), 337, 356, 379,
385, 397, 400
Bernardo (2001), 394, 395
Besag (1974), 511
Besag (1994), 517
Bibby and Toutenburg (1977), 301
Blasco and Varona (1999), 629,
633
Blasco (2001), 119, 121
Bliss (1935), 606
727

728
List of Citations
Box and Muller (1958), 105
Box and Tiao (1973), 17, 85, 104,
214, 227, 237, 244, 262,
278, 289, 337, 356, 360,
362, 366, 514, 545, 650,
687, 693
Box (1976), 671
Box (1980), 218
Brooks and Gelman (1998), 550
Brooks and Roberts (1998), 547
Brooks et al. (2001), 532
Brown et al. (2001), 13
Bulmer (1971), 44
Bulmer (1979), 14, 44, 46
Bulmer (1980), 26, 62, 65
Bunke (1975), 301
Carlin and Louis (1996), 543
Casella and Berger (1990), 13, 33,
138, 145, 148, 152
Casella and George (1992), 488
Casella and Robert (1996), 552
Casella et al. (2001), 556
Chen et al. (2000), 551, 554, 585
Chib and Greenberg (1995), 503,
504, 689
Chib and Jeliazkov (2001), 428
Chib (1995), 427, 428
Cohen (1986), 20
Collet (1994), 439
Congdon (2001), 421, 427, 437
Cowles and Carlin (1996), 547, 550
Cowles (1996), 612
Cox and Hinkley (1974), 157, 166
Cox and Miller (1965), 477, 479,
494
Cox and Reid (1987), 181
Cox and Snell (1989), 178, 188,
204, 607
Cox (1961), 174, 529
Cox (1962), 174, 529
Crow and Kimura (1970), 166, 175
Curnow and Smith (1975), 606
Curnow (1961), 62
Curnow (1972), 606
Dahlquist and Bj¬®orck (1974), 162‚Äì
164
Dempster and Lerner (1950), 89,
90, 92, 202, 605, 606
Dempster et al. (1977), 444, 584
Dempster (1974), 429
Dempster (1997), 429
Dennis and Schnabel (1983), 162
Devroye (1986), 39, 66
De Finetti (1975a), 4
De Finetti (1975b), 29
Dickey (1968), 278
Doerge and Churchill (1996), 685
Draper and Smith (1981), 292, 434
Ducrocq et al. (1988), 80
Durbin et al. (1998), 338
Earman (1992), 219
Edwards (1974), 119
Edwards (1992), 26, 119, 167, 181
Efron and Hinkley (1978), 131
Efron (1993), 181
Efron (1998), 133
Elston and Stewart (1971), 671
Fahrmeir and Tutz (2001), 626
Falconer and Mackay (1996), 287,
344
Falconer (1965), 202, 605, 606
Falconer (1967), 606
Famula (1981), 24
Fan et al. (2000), 174
Feller (1970), 477
Feng and McCulloch (1996), 147
Fernandez and Steel (1998), 595
Fisher (1918), 44, 102
Fisher (1920), 142
Fisher (1922), 119, 122, 142
Fisher (1925), 346
Fishman (1973), 19, 21
Flury and Zoppe (2000), 447
Foulley and Manfredi (1991), 606
Foulley et al. (1983), 606, 615
Foulley et al. (1987), 606
Fox (1987), 375, 377
Galton (1885), 287

List of Citations
729
Garc¬¥ƒ±a-Cort¬¥es and Sorensen (1996),
585
Geisser and Eddy (1979), 438
Geisser (1993), 438
Gelfand and Dey (1994), 429, 438
Gelfand and Smith (1990), 85, 540,
556
Gelfand et al. (1990), 547, 552
Gelfand et al. (1992), 436, 437
Gelfand et al. (1995), 602, 604
Gelfand et al. (1996), 602‚Äì604
Gelfand (1996), 436, 437
Gelman and Rubin (1992), 540,
548‚Äì550
Gelman et al. (1995), 10, 40, 57,
61, 214, 220, 577
Gelman et al. (1996), 437, 438
Geman and Geman (1984), 497
George et al. (2000), 694
Geweke (1989), 556
Geweke (1993), 598, 599
Geyer (1992), 540, 554, 555
Gianola and Fernando (1986), 52,
66, 313, 650
Gianola and Foulley (1983), 83,
606
Gianola et al. (1989), 26, 62
Gianola et al. (1990), 313
Gianola (1982), 202, 606
Gilks and Roberts (1996), 547
Gilks and Wild (1992), 658
Gilmour et al. (1985), 606
Go et al. (1978), 672
Goodman and Hartley (1958), 155
Good (1952), 441
Good (1958), 401
Green (1995), 497, 518
Grimmet and Stirzaker (1992), 482
Gross and Clark (1975), 439
Grossman and Turner (1974), 478
Guo and Thompson (1994), 672,
675, 676
Hacking (1965), 167
Hager (1988), 162
Haldane (1919), 680
Haldane (1948), 359, 360
Hammersley and Handscomb (1964),
556
Hampel et al. (1986), 589
Han and Carlin (2001), 428
Harville and Mee (1984), 606
Harville (1974), 472, 651
Harville (1977), 266
Hastings (1970), 497, 555
Hazel (1943), 576
Heath (1997), 679, 694
Heisenberg (1958), 219
Henderson and Meyer (2001), 13
Henderson et al. (1959), 62, 318
Henderson (1953), 26, 313
Henderson (1963), 26, 51, 125
Henderson (1973), 26, 50, 51, 125,
212, 279, 282, 318
Henderson (1975), 26, 62, 64
Henderson (1984), 102
Heringstad et al. (2001), 606
Hills and Smith (1992), 584, 604
Hills and Smith (1993), 604
Hobert and Casella (1996), 545,
566
Hoel et al. (1971), 79
Hoerl and Kennard (1970), 300
Hoeschele and Tier (1995), 609
Hoeting et al. (1999), 439, 441,
442
Hogg and Craig (1995), 3
Howson and Urbach (1989), 219
Jamrozik and SchaeÔ¨Äer (1997), 628
Janss et al. (1995), 672, 679
Jaynes (1957), 341
Jaynes (1994), 354, 367
JeÔ¨Äreys (1961), 289, 356, 360, 361,
367, 401
Jensen et al. (1994), 571
Jensen et al. (1995), 679
Jensen (1994), 606, 615
Johnson and Kotz (1969), 4
Johnson and Kotz (1970a), 4
Johnson and Kotz (1970b), 4, 83
Johnson and Kotz (1972), 4

730
List of Citations
Kackar and Harville (1981), 212
Kadarmideen et al. (2002), 606,
609
KalbÔ¨Çeisch and Sprott (1970), 181
KalbÔ¨Çeisch and Sprott (1973), 181
Kaplan (1993), 96, 97, 376
Karlin and Taylor (1975), 477, 492
Kass and Raftery (1995), 401, 402,
420, 421, 425, 427
Kass et al. (1998), 540
Kass (1995), 421
Keller (2000), 4
King (1989), 119
Kirkpatrick et al. (1994), 628
Kleinbaum (1996), 80
Knott and Haley (1992), 684, 685
Koerkhuis and Thompson (1997),
571
Korsgaard et al. (1999), 57, 619,
626
Korsgaard et al. (2002), 615
Kullback (1968), 346, 347, 349,
351
Laird and Ware (1982), 628, 665
Lange and Elston (1975), 671
Lange and Goradia (1987), 675
Lange and Sinsheimer (1993), 589,
664
Lange (1995), 453
Lange (1997), 675
Lee and Nelder (1996), 606
Lee and Thomas (2000), 694
Lee (1989), 214
Lehmann and Casella (1998), 144‚Äì
148, 152, 171
Lehmann (1999), 46, 93, 131, 148,
180, 181
Leonard and Hsu (1999), 214, 356,
364, 420, 421
Lindley and Smith (1972), 52, 85,
266, 313, 629
Lindley (1956), 350
Lindley (1957), 409
Little and Rubin (1987), 473, 577
Liu and Rubin (1995), 592, 594
Liu et al. (1994), 510, 552, 554,
584, 604
Liu (1994), 584, 585, 604
Liu (2001), 477
Lo et al. (2001), 174
Louis (1982), 449, 453
Lund and Jensen (1999), 672, 679
Lynch and Walsh (1998), 56
MacCluer et al. (1986), 675
Madigan and Raftery (1994), 439,
442
Mal¬¥ecot (1947), 120, 214
Mal¬¥ecot (1969), 44, 46
Mardia et al. (1979), 43, 56
Marsaglia and Zaman (1993), 20
Martinez et al. (2000), 671
McCullagh and Nelder (1989), 77,
123, 181‚Äì183, 188
McCulloch and Rossi (1991), 425
McCulloch (1994), 606
McLachlan and Krishnan (1997),
444, 449, 452, 473, 592,
594
Meeker and Escobar (1995), 178,
180
Meilijson (1989), 453
Meng and Rubin (1991), 453, 454,
456
Mengersen et al. (1999), 547
Metropolis et al. (1953), 497, 504,
689
Meyer (1999), 628
Meyn and Tweedie (1993), 477,
498, 501
Milliken and Johnson (1992), 270
Misztal et al. (1989), 609
Mood et al. (1974), 3, 157, 610
Moreno et al. (1997), 608, 609
Morton and MacLean (1974), 671
Nandram and Chen (1996), 612
Nelder and Wedderburn (1972),
77, 123
Newton and Raftery (1994), 426,
690
Neyman and Pearson (1928), 166

List of Citations
731
Norris (1997), 477
O‚ÄôHagan (1994), 214, 217, 219, 253,
264, 298, 354, 356, 364,
411, 414‚Äì416, 420‚Äì424, 526,
541
Oakes (1999), 453, 457, 458, 465
Odell and Feiveson (1966), 59
Ott (1999), 680, 685
Patterson and Thompson (1971),
26, 128, 183, 186
Pauler et al. (1999), 418
Pawitan (2000), 180
Pearson (1900), 606
Pearson (1903), 26, 62, 64
Peskun (1973), 507, 508, 523
Popper (1972), 219
Popper (1982), 219
Priestley (1981), 555
Propp and Wilson (1996), 556
Raftery et al. (1997), 439, 442
Raj (1968), 155
Rao (1947), 180
Rao (1973), 26, 43, 87, 115, 145,
200
Reid (1995), 181
Reid (2000), 181
Richardson and Green (1997), 518
Ripley (1987), 19, 554, 657
Robert and Casella (1999), 477,
498, 547, 551
Roberts and Sahu (1997), 584, 585,
602
Roberts et al. (1997), 504
Robertson and Lerner (1949), 90,
606
Robertson (1977), 62
Robert (1994), 20
Robert (1998), 547
Rodriguez-Zas (1998), 599, 670
RoÔ¨Ä(1997), 56
Rogers and Tukey (1972), 589, 664
Rosa et al. (2001), 670
Rosa (1998), 589
Ross (1997), 19
Royall (1997), 167, 168
Rubin (1976), 576
Rubin (1987a), 577
Rubin (1987b), 556
Rubin (1988), 661
Savage (1972), 218
Schafer (2000), 577
Schwarz (1978), 420, 421
Scott (1992), 553
Searle et al. (1992), 26, 51, 125,
184, 234, 279, 343
Searle (1971), 32, 39, 43, 53, 67,
85, 92, 115, 147, 169, 183,
254, 284, 313, 318, 564
Searle (1982), 51, 53, 183, 603
Severini (1998), 181
Severini (2000), 166, 181
Shannon (1948), 337, 341, 346
Sheehan and Thomas (1993), 678
Sheehan et al. (2002), 679
Sheehan (2000), 675, 678, 679
Sillanp¬®a¬®a and Arjas (1998), 691,
694
Sillanp¬®a¬®a and Arjas (1999), 694
Silverman (1992), 553
Sivia (1996), 367, 370, 371
Smith and Gelfand (1992), 556
Smith (1936), 576
Smith (1959), 408
Sorensen et al. (1994), 26
Sorensen et al. (1995), 548, 555,
606, 607, 619
Sorensen et al. (2000), 438
Sorensen et al. (2001), 66
Sorensen (1996), 606, 615
Spiegelhalter et al. (2002), 429‚Äì
431
Stein (1977), 132
Stephens and Fisch (1998), 694
Stramer and Tweedie (1998), 517
Strand¬¥en and Gianola (1998), 594
Strand¬¥en and Gianola (1999), 589,
599, 664
Strand¬¥en (1996), 589, 598, 670
Stuart and Ord (1987), 53

732
List of Citations
Stuart and Ord (1991), 39, 139,
166, 167
Swendsen and Wang (1987), 532
Tanner and Wong (1987), 241, 293,
473, 532, 556
Tanner (1996), 28, 454, 658
Thompson and Heath (2000), 679
Thompson (1973), 62
Thompson (1976), 62
Thompson (1980), 266
Thompson (2001), 672, 679
Tierney and Kadane (1989), 420
Tierney (1994), 497, 498, 501
Toutenburg (1982), 301
Uimari and Hoeschele (1997), 694
Van Tassell and Van Vleck (1996),
581
Van Tassell et al. (1998), 615
Van Vleck (1993), 571
Vuong (1989), 174
Waagepetersen and Sorensen (2001),
497, 518, 694
Wang et al. (1994), 24
Wang et al. (1997), 581, 606, 615,
616
Wei and Tanner (1990), 447, 462
Weinstock (1974), 375, 377
Weir (1996), 175, 195
Wiggans and Goddard (1997), 628
Wilks (1938), 166
Willham (1963), 208, 570
Williamson et al. (1972), 97
Wilton et al. (1968), 283
WolÔ¨Ånger and Lin (1997), 653
WolÔ¨Ånger (1993), 653
Wright (1934), 90, 202, 606
Wright (1968), 23, 69
Yi and Xu (2000), 694
Zellner and HighÔ¨Åeld (1988), 370
Zellner (1971), 214, 263, 296, 337,
356, 360, 361, 403, 635
Zellner (1976), 589, 592, 599, 664

Subject Index
Acceptance-rejection sampling, 657
Additive genetic covariance ma-
trix, 572
Additive genetic model
Bayesian view, 313
clustered random eÔ¨Äects, 600
conditional posterior distribu-
tions, 51
marginal posterior density of
additive genetic value, 231
maternal eÔ¨Äects, 570
multivariate, 576
multivariate (blocked) Gibbs
sampling, 584
posterior probability distribu-
tions, 259
quadratic selection index, 283
repeated measurements, 226
robust analysis, 592, 595
univariate, 564
updating additive genetic ef-
fects, 253
Additive genetic relationship ma-
trix, 564
Aitken‚Äôs integral, 32, 41
Akaike information criterion (AIC),
421
Aperiodicity, 483
Autocorrelation between MCMC
samples, 543
Backcross design, 681
Bayes factor, 221, 400
approximations, 418
computation, 424
decision theoretic view, 403
inÔ¨Çuence of the prior, 412
intrinsic Bayes factor, 422
partial Bayes factor, 422
Bayes theorem
continuous case, 224
discrete case, 216
Bayesian asymptotic theory
continuous parameters, 331
discrete parameters, 330
Bayesian information criterion (BIC),
420
Bayesian learning, 216, 222, 249,
546
Bayesian model average, 439

734
Subject Index
Predictive ability, 441
Behrens-Fisher problem, 170
Best linear unbiased predictor, 318,
565
Best predictor, 68, 281
Beta function, 84
Binary and Gaussian responses
joint analysis, 626
Box-Muller transformation, see Sim-
ulation of random vari-
ables
Burn-in period, 540
Calculus of variations, 375
Euler-Lagrange condition, 377
Categorical and Gaussian responses
joint analysis, 615
Categorical traits, 605
analysis of a single polychoto-
mous response variable,
607
residual analysis, 435
Cauchy-Schwarz inequality, 139
Central limit theorem, 44
Chapman-Kolmogorov equations,
480
Cholesky factorization, 59, 112
Clustered random eÔ¨Äects, 600
Complete data, 444
Complex segregation analysis, 671
Composition, 28, 437
Conditional distribution, 30
Conditional multivariate normal
distribution, 51
Conditional posterior distribution,
228, 229, 235, 240
ConÔ¨Ådence regions, 177, 179, 180
Conjugacy, 298
Conjugate prior, 297
Constant of integration, 16, 32
Continuity correction, 13
Convergence diagnostics, 547
Convergence in distribution, 46,
93
Convergence in probability, 93, 146
Countably inÔ¨Ånite, 11
Covariance between relatives, 72
genetic marker information,
74
Cram¬¥er-Rao lower bound, 138
Credibility sets, 262
Cross-validation, 436
Cumulative distribution function,
6, 13, 14
Data augmentation, 241, 293, 532,
576, 608, 616, 665
Degree of belief, 57
Delta method, 93
Detailed balance equation, 487
Deviance, 171, 430
Deviance information criterion, 431
Discrete traits, see Categorical traits
Distribution
Bernoulli distribution, 7
beta distribution, 21
beta-binomial distribution, 68,
71
binomial distribution, 9, 78
normal approximation, 12
Poisson approximation, 10
Cauchy distribution, 28
chi-square distribution, 24, 53
Dirichlet distribution, 40
exponential distribution, 24
gamma distribution, 24
inverse chi-square, 84
inverse gamma, 57
inverse Wishart distribution,
57
logistic distribution, 83, 204
moment generating function,
83
lognormal distribution, 80
mixture distribution, 28
multinomial distribution, 37,
98, 190, 458, 488
multivariate normal distribu-
tion, 41

Subject Index
735
multivariate uniform distribu-
tion, 40
multivariate-t distribution, 60
negative binomial distribution,
405
normal distribution, 25
independence, 42
linear functions, 44
Poisson distribution, 11
scaled inverse chi-square dis-
tribution, 85, 565
sech-squared distribution, 83
singular normal distribution,
43
Student-t distribution, 28
mixture interpretation, 28
uniform distribution, 18
Wishart distribution, 55
Distribution of a ratio, 100
Distributions with constrained sam-
ple space, 62
EÔ¨Äective chain length, 548, 555
EÔ¨Äective number of parameters,
430
EM algorithm, 443
exponential families, 451
maximum likelihood, 466
Monte Carlo EM, 447, 462
rate of convergence, 449
restricted maximum likelihood,
472
standard errors, 452
supplemented EM algorithm
(SEM), 452
Entropy, 334
entropy of a distribution
continuous distributions, 341
discrete distributions, 337
entropy of joint and condi-
tional distributions, 340
relative entropy, 354
Shannon-Jaynes entropy, 371
Equilibrium distribution, see Sta-
tionary distribution
Ergodic average, 551
Ergodicity, 484
Estimability, 147
Exchangeability, see Random vari-
able
Expected information, see Infor-
mation
Expected posterior loss, 403
Expected value, 8
Exponential families, see EM al-
gorithm
Extreme category problem, 609
FF algorithm, 525
First-order autoregressive process,
633
Fisher‚Äôs information, see Informa-
tion
Fisher‚Äôs scoring algorithm, 162
Founder and nonfounder individ-
uals, 673
Fully conditional posterior distri-
bution, 509, 510
Gamma function, 17, 22
Gamma integral, 17
Gaussian linear model
joint modes, 273
marginal modes, 277
Gene-dropping, 675
Gibbs sampling, 509
blocked Gibbs sampling, 584
systematic-scan, single-site, 491
Goodness of Ô¨Åt, 429
Growth curve, 631
Hammersley-CliÔ¨Äord theorem, 511
Heteroscedastic residuals, 633
Hierarchical models, 628
High credibility sets, 262
Highest posterior density interval,
262
Homoscedastic residuals, 633
Homoscedastic variance, 42
Hyperparameters, 235

736
Subject Index
Hypothesis test, 166, 271, 401
composite vs composite hy-
potheses, 411
deviance information criterion,
431
loss function, 404
nested models, 166, 173, 414
point null hypothesis, 407
simple vs composite hypothe-
ses, 406
two simple hypotheses, 404
IdentiÔ¨Åability, 147, 543
Implementation of MCMC, 540
Importance sampling, 424, 556, 658
Improper distribution, 35
Improper posterior distribution, 269
Imputation of missing records, 580
Inadmissibility, 186
Incomplete data, 444
Independence, 9, 29
mutual independence, 29
pairwise independence, 29
Indicator function, 7, 15
Information, 127
entropy, 334
expected information, 131, 138
expected information matrix,
135
Fisher‚Äôs information, 128, 132,
139, 351
observed information, 131
Information about a parameter,
346
Information as curvature, 132
Information per observation, 199
Information provided by an ex-
periment, 350
Invariant distribution, see Station-
ary distribution
Inverse probability, 216
Irreducibility, 483
segregation analysis models,
677
Iterated expectations, 61, 67
Jacobian, 79, 96
JeÔ¨Äreys‚Äô Priors
many parameters, 364
single parameter, 360
Jensen‚Äôs inequality, 145
Joint cumulative distribution func-
tion, 30
Joint modes, 265, 273
Joint posterior distribution, 235
Joint probability density function,
30
Joint updating schemes, 679
Kernel of the distribution, 16
Kullback‚Äôs Information Measure,
346
divergence between hypothe-
ses, 347
Kullback-Leibler discrepancy, 331
relative entropy, 353
Kullback-Leibler distance, 429, 448
Lagrange multiplier test, 179
Langevin-Hastings algorithm, 517
Laplace integration, 419
Least-squares, 127
Liability, 203, 605
Likelihood
integrated likelihood, 128
marginal likelihood, 128, 182
MCMC computation, 424
proÔ¨Åle likelihood, 186
restricted likelihood, 128, 186
Likelihood function, 121
Likelihood ratio test, 166
asymptotic distribution, 171
Monte Carlo likelihood ratio
test, 685
power, 174
Lindley‚Äôs paradox, 409
Linear model
Bayes factor, 413
Linear regression, 136, 287
multivariate-t error distribu-
tion, 591

Subject Index
737
univariate-t error distribution,
589
Linear transformations, see Trans-
formations
Linkage, 407
Location parameters
marginal distribution, 323
Log-likelihood, 122
Logistic regression, 202
Logistic transformation, see Trans-
formations
Logit transform, 194
Longitudinal data, 627
analysis with thick-tailed dis-
tributions, 664
computation via MCMC, 653
Gaussian approximation, 647
scoring algorithm, 648
two-step approximate Bayesian
analysis, 642
Loss function, 186, 262, 264, 281,
404
Major genes, 671
Map distance, 680
Mapping function, 680
Marginal distribution
continuous random variables,
29, 32
discrete random variables, 31
Marginal distribution of data, 232
additive genetic model, 226
Bayes factor, 415
Marginal maximum likelihood, 650
Marginal modes, 266
Marginal posterior distribution, 235
Marginal probability density, 29
Markov chain Monte Carlo, 497
Markov chains, 477
convergence to stationarity, 492
Jordan decomposition, 494
limiting behavior, 492
long-term behavior, 481
stage of a Markov chain, 478
state of a Markov chain, 478
time homogeneous, 479
Markov property, 479
Maternal eÔ¨Äects additive genetic
model, 570
Maximum entropy prior distribu-
tions, 367
Gibbs distribution, 370
Maximum likelihood, 119
Maximum likelihood estimator, 122
asymptotic properties
multiparameter models, 152
single-parameter models, 143
conÔ¨Ådence regions, 177
consistency, 146
eÔ¨Éciency, 151
functional invariance, 153, 157,
159
regularity conditions, 147
residual variance, 54
Mean squared error, 185
Method of composition, see Com-
position
Metropolis algorithm, 504, 689
Metropolis-Hastings algorithm, 502
acceptance probability, 502,
503
joint updating, 504
proposal distribution, 502
random walk proposal, 517
single-site updating, 507
Missing information principle, 448
Mixed inheritance model, 671
Mixed linear model, 313
EM algorithm, 466, 472
maximum likelihood inferences,
466
restricted maximum likelihood
inferences, 472
Mixture distribution, 71
Model Ô¨Åt
posterior distribution of resid-
uals, see Residuals
Models with thick-tailed distribu-
tions, 588
Moment generating function

738
Subject Index
multivariate, 43
univariate, 26
Moments of a distribution, 26
Monte Carlo variance, 553
initial positive sequence esti-
mator, 555
method of batching, 555
Multimodal posterior distribution,
594
Multiple comparisons, 270
Multistage model, 628
Multivariate distribution, 29
Multivariate-t distribution, 314, 324
Newton-Raphson, 162
Normalized distribution, 62
Nuisance parameters, 35, 125, 152,
166, 181‚Äì183, 186, 208
Objective Bayesian analysis, 288
Orthogonal parameterization, 307
p-value, 401, 684
Bayesian p-value, 438
Parameter, 8
Parameter space, 120
Penetrance, 674
Perfect sampling, 556
Polar coordinates, 17
Posterior correlation between pa-
rameters, 238, 285, 546,
584
hierarchical centering, 543, 602
Posterior credibility sets
additive genetic model, 262
Posterior distribution, 235
discrepancy with prior distri-
bution, 353
Gaussian approximation, 419
Posterior loss, see Expected pos-
terior loss
Posterior median
additive genetic model, 262
Posterior mode, 264, 418
Posterior odds ratio, 401
Posterior probability, 258
Posterior probability distribution,
213, 216, 224
Posterior probability of a hypoth-
esis, 403
Posterior probability of linkage,
see Linkage
Posterior quantiles, 262
Prediction error variance, 282
Predictive ability of model, 433
Predictive distribution, 292, 306
posterior predictive distribu-
tion, 293, 307, 437
prior predictive distribution,
292, 306, 402
Predictive log-score, 441
Principle of insuÔ¨Écient reason, 356
Prior odds ratio, 401
Prior probability distribution, 213,
215, 218, 223
conjugate prior, 297
eÔ¨Äect on posterior inferences,
328
heritability, 106, 109, 357
improper uniform prior, 224,
288, 357, 565
maximum entropy priors, 367
mutation rate, 359
reference priors, 379
uniform prior, 356
vague information, 356
Probability density function, 14
lack of uniqueness, 14
Probability function, 5
Probability mass, 6
Probability mass function, see Prob-
ability function
Probit model, 204
QTL analysis, 679
arbitrary number of QTL, 690
Bayesian inference, 686
Bayes factors and model se-
lection, 690

Subject Index
739
fully conditional posterior
distributions, 687
likelihood inference, 682
hypotheses tests, 684
likelihood ratio test, 684
Monte Carlo likelihood ra-
tio test, 685
proÔ¨Åle likelihood, 685
reversible jump MCMC, 694
single QTL model, 680
Quadratic genetic merit, 283
Quasi-BLUP approach, 653
Random quantity, see Random vari-
able
Random variable
continuous random variable,
13
discrete random variable, 5
distribution, 5
exchangeable random variables,
29
Rao-Blackwell estimator, 552
Rao-Blackwellization, 280
Reference analysis
multiparameter models, 396
single nuisance parameter, 389
single parameter, 379
Reference prior distributions, 379
Regression curve, 36
Residual analysis, 434
Residual covariance matrix, 578
Residuals
posterior distribution, 292, 306
Restricted maximum likelihood, see
Mixed linear model, 651
Reversibility, 487, 501, 520
Reversible jump MCMC, 517
acceptance probability, 522
addition of a QTL, 696
deterministic proposals, 523
dimension matching condition,
520
FF proposals, 525
model selection, 699
proposal distribution, 519
QTL analysis, 694
removal of a QTL, 695
Ridge regression, 300
Robust analysis
additive genetic model, 592,
595
clustered random eÔ¨Äects, 600
linear regression, 240, 241, 589
longitudinal data, 664
Robust methods, 589
Sample space
continuous sample space, 13
discrete sample space, 5
Schwarz BIC, 421
Score, 123, 132, 134, 159
Score test, 179
Scoring algorithm, 162
longitudinal analysis, 648
Segregation analysis, 672
Selection by truncation, 62, see
Truncation selection
Sensitivity analysis, 556
Simulation of random variables
binomial random variable, 10
Box-Muller transformation, 105
Dirichlet random variables, 40
discrete random variables, 18
inverse transform method, 19
multinomial distribution, 39
multivariate normal samples,
112
t-distributed random variable,
28
truncated distributions, 66
Wishart and inverse Wishart
distribution, 59
Stationary distribution, 481, 500
Statistical information, 334
Student-t mixed eÔ¨Äects model, 595
Student-t model
linear regression, 240, 241
longitudinal data, 664
SuÔ¨Éciency, 142

740
Subject Index
jointly suÔ¨Écient statistics, 143
Support, 6
t-model, see Student-t model
Taylor approximations, 89, 114
Threshold model, 90, 202, 605
Transformation invariance
reference prior, 385
Transformations
bivariate normal distribution,
113
continuous random variables,
79, 95
discrete random variables, 78,
97
linear transformations, 111
logistic transformation, 82
many-to-one, 87
multivariate transformations,
95
univariate transformations, 78
Transition kernel, 499, 501
Transition probability, 479
Truncated normal distribution, 612
Kullback-Leibler distance, 355
Truncation selection, 64
Unbiased estimator, 138
Uncertainty, 8
Uniform Prior, see Prior proba-
bility distribution
Univariate continuous distributions,
13
Univariate discrete distributions,
4
Univariate-t distribution, 665
Variance components
marginal distribution, 322
Wald‚Äôs test, 179

