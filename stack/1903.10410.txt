arXiv:1903.10410v4  [cs.NE]  22 Sep 2020
A Conceptual Bio-Inspired Framework for the
Evolution of Artiﬁcial General Intelligence
Sidney Pontes-Filho∗,† and Stefano Nichele∗,‡
∗Department of Computer Science, Oslo Metropolitan University, Oslo, Norway
†Department of Computer Science, Norwegian University of Science and Technology, Trondheim, Norway
‡Department of Holistic Systems, Simula Metropolitan, Oslo, Norway
Email: sidneyp@oslomet.no, stenic@oslomet.no
Abstract—In this work, a conceptual bio-inspired parallel and
distributed learning framework for the emergence of general
intelligence is proposed, where agents evolve through environ-
mental rewards and learn throughout their lifetime without
supervision, i.e., self-learning through embodiment. The chosen
control mechanism for agents is a biologically plausible neuron
model based on spiking neural networks. Network topologies
become more complex through evolution, i.e., the topology is
not ﬁxed, while the synaptic weights of the networks cannot
be inherited, i.e., newborn brains are not trained and have
no innate knowledge of the environment. What is subject to
the evolutionary process is the network topology, the type of
neurons, and the type of learning. This process ensures that
controllers that are passed through the generations have the
intrinsic ability to learn and adapt during their lifetime in
mutable environments. We envision that the described approach
may lead to the emergence of the simplest form of artiﬁcial
general intelligence.
Index Terms—Bio-inspired AI, Self-learning, Embodiment,
Conceptual framework, Artiﬁcial General Intelligence
I. INTRODUCTION
The brain is a truly remarkable computing machine that
continuously adapts through sensory inputs. Rewards and
penalties are encoded and learned throughout the evolution
of organisms living in an environment (our world) that con-
tinuously provides unlabeled and mutable data. The supervi-
sion in the brain is a product of such evolutionary process.
A real-world environment does not provide labeled data or
predeﬁned ﬁtness functions for organisms and their brains as in
supervised and reinforcement learning in Artiﬁcial Intelligence
(AI) systems. However, organisms selected by natural and
sexual selection [1] know or are able to learn which sensory
inputs or input sequences may affect positively or negatively
their survival and reproduction. One of the key components
which ensures that a species will reproduce is the lifetime
of the organisms. Pleasure, joy, and desire (or other positive
inputs) may increase the lifetime of an organism and act as
rewards. Pain, fear, and disgust may decrease the lifetime and
act as penalties. All those feelings and emotions are results
of the evolutionary pressure for increasing the life expectancy
and succeeding in generating offspring [2]. One example is
the desire and disgust that arise for some smells. The desire
may come from the smell of nutritive food which increases
life expectancy, while the disgust may come from spoiled
food which may cause food poisoning, and therefore causing
a lifetime reduction. Evolution by natural selection made it
possible for living beings to be “interpreters” of sensory inputs
by being attracted to rewards and repulsed by penalties, even
though their ﬁrst ancestors did not know what was beneﬁcial
or harmful in the surrounding environment.
Artiﬁcial General Intelligence (AGI) or strong AI has been
pursued for many years by researchers in many ﬁelds, with
the goal of reproducing human-level intelligence in machines,
e.g., the ability of generalization and self-adaptation. So far,
the AI scientiﬁc community has achieved outstanding results
for speciﬁc tasks, i.e., weak or narrow AI. Such narrow AI
implementations require highly specialized high performance
computing systems for the training process. In this work,
we propose to tackle the quest for general intelligence in
its simplest form through evolution. It is therefore essential
to develop a mutable environment that mimics what the ﬁrst
living beings with the simplest nervous systems faced. We
deﬁne the simplest form of artiﬁcial general intelligence as
the ability of an organism to continuously self-learn and
adapt in a continuously changing environment of increasing
complexity. Our deﬁnition of self-learning covers the meaning
of self-supervised learning [3] and self-reinforcement learning
[4]. Therefore, a self-learning agent is capable of interpreting
the reactions of the surrounding environment affected by the
agent’s actions, then the sensory inputs of the agent are utilized
to supervise or reinforce itself. This occurs because the sensory
inputs contain cues for the agent to learn on its own. For
example, the pain after touching a candle’s ﬂame (as the
authors experienced in their childhood).
In this work, we propose the Neuroevolution of Artiﬁcial
General Intelligence (NAGI) framework. NAGI is a bio-
inspired framework which uses plausible models of biological
neurons, i.e., spiking neurons [5], in an evolved network
structure that controls a sensory-motor system in a mutable
environment. Evolution affects the connection structure of
neurons, their neurotransmitters (excitatory and inhibitory),
and their local bio-inspired learning algorithms. The inclusion
of such learning algorithms under evolutionary control is an
important factor to generate long-term associative memory
neural networks which may have cells with different plasticity
rules [6]. Moreover, the genotype of the NAGI’s agents does
not contain the synaptic strength (weights) of the connections
to avoid any innate knowledge about the environment. How-
1

ever, controllers that are selected for reproduction are those
rewarded for their ability of self-learning and adaptation to
new environments, i.e., an artiﬁcial newborn brain of an agent
is an “empty box” with the innate ability to learn how to
survive in different environments during its lifetime. That is
somewhat similar to how humans have a specialized brain part
for learning quickly any language when they are born [2].
The remainder of this paper is organized as follows.
It provides background knowledge for the proposed NAGI
framework, and then describes related works. Afterward, it
contains a detailed explanation of the conceptual framework.
Finally, we conclude this work by discussing the relevance of
our approach for current AGI research, and we elaborate on
possible future works which may include such a novel bio-
inspired parallel and distributed learning method.
II. BACKGROUND
The proposed NAGI framework brings together several key
approaches in artiﬁcial intelligence, artiﬁcial life, and evolu-
tionary robotics [7], brieﬂy reviewed in this section. A Spiking
Neural Network (SNN) is a type of artiﬁcial neural network
that consists of biologically plausible neuron models [5]. Such
neurons communicate with spikes or binary values in time
series. SNNs incorporate the concept of time by intrinsically
modeling the membrane potential within each neuron. Neurons
spike when the membrane potential reaches a certain threshold.
When the signals propagate as neurotransmitters to the neigh-
boring neurons, their membrane potentials are therefore af-
fected, increasing or decreasing. While SNNs are able to learn
through unsupervised methods, i.e., Hebbian learning [8] and
Spike-Timing-Dependent Plasticity (STDP) [9], spike trains
are not differentiable and cannot be trained efﬁciently through
gradient descent. NeuroEvolution of Augmenting Topologies
(NEAT) [10] is a method that uses a Genetic Algorithm (GA)
[11] to grow the topology of a simple neural network and
adjust the weights of the connections to optimize a target
ﬁtness function, while allowing to keep diversity (speciation)
in the population and to maintain compatible gene crossover
with historical marking. The neuroplasticity used to adapt
the weights in the proposed NAGI framework includes the
Hebbian learning rule as STDP. In particular, the weight
adaptation of STDP happens when the neuron produces a spike
or action potential through the axon (i.e., output connection).
Such an event allows the modiﬁcation of the synaptic strength
of the dendrites (i.e., input connections) that caused or did not
cause that spike.
Funes and Pollack [12] describe the body/brain interaction
(sensors and actuators vs. controller) as “chicken and egg”
problem; the course of natural evolution shows a history of
body, nervous system, and environment all evolving simultane-
ously in cooperation with, and in response to, each other [13].
Embodied evolution [14] is an evolutionary learning method
for training agents through embodiment, i.e., embodied agents
learning in an environment. Thus, in nature, general intelli-
gence is a result of evolved self-learning through embodiment.
III. RELATED WORK
The idea of neuroevolution with adaptive synapses is not
new. Stanley et al. [15] present NEAT with adaptive synapses
using Hebbian local learning rules, with the goal of training
neural networks for controlling agents in an environment.
The authors verify the difference in performance with and
without adaptation on the dangerous food foraging domain.
The differences between their approach and ours are that
their environment is static throughout the agent lifetime and
they have inheritance of synaptic strength. Their results show
that both networks with and without adaptive synapses reach
the maximum ﬁtness on that domain, and therefore both
present “adaptation”. An extended version of the previous
method is Adaptive Hypercube-based NEAT (HyperNEAT)
[16]. Adaptive HyperNEAT includes indirect encoding of the
network topology as large geometric patterns.
A recent work that uses NEAT and no weight inheritance
is Weight Agnostic Neural Networks (WANN), introduced by
Gaier and Ha [17]. WANN is tested successfully with different
supervision and reinforcement learning tasks whose weights
are randomly initialized. Such promising results demonstrate
that the network topology is as important as the connection
weights in an artiﬁcial neural network. This is one of the
motivations for the development of the NAGI framework.
WANN and NEAT with adaptive synapses have been shown
to be successful methods. However, such methods miss an
important component for self-learning, which is a mutable
environment as proposed in this work.
A recent review of neuroevolution can be found in [18]
and it shows how competitive NEAT and its extensions are
in comparison to deep neural networks trained with gradient-
based methods for reinforcement learning tasks. Neuroevolu-
tion provides several extensions, which include indirect encod-
ing to allow scalability, novelty search to promote diversity,
meta-learning for training a network to learn how to learn,
and the combination with deep learning for searching deep
neural network architectures. Furthermore, its authors envisage
that neuroevolution will be a key factor to reach AGI through
meta-learning and open-ended evolution. However, in NEAT
the neural weights are inherited, so there is no explicit target
for general intelligence and adaptation.
A framework for the neuroevolution of SNNs and topology
growth with genetic algorithms is proposed by Schaffer [19],
with the goal of pattern generation and sequence detection.
Eskandari et al. [20] propose a similar framework for artiﬁcial
creature control, where the evolutionary process modiﬁes and
inherits the network topology and the SNN weights to perform
a given task.
A method which tries to produce general intelligence incre-
mentally is PathNet [21], where deep neural network paths are
selected through evolution to perform the forward propagation
and weight adjustment. Such evolving selection allows the
network to learn new tasks faster by re-using frozen (previ-
ously learned) paths without catastrophic forgetting. Another
framework that tries to produce low-level general intelligence
2

is described by Voss [22]. It is a functional proof-of-concept
prototype, owned by the company Adaptive A.I. Inc., which
can interact with virtual and real world through sensors and
actuators. Its controller, which has conceptual general intelli-
gence capabilities, consists of a memory to save all data and
to store the proprietary cognitive algorithms.
Multi-agent environments have also been considered a valu-
able stepping-stone towards AGI because the behavior of
agents must adapt to cooperate and compete among them. One
of the ﬁrst examples of such multi-agent environment is the
PolyWorld ecological simulator, introduced by Yaeger [23].
PolyWorld is a simulated environment of randomly generated
food where evolving artiﬁcial organisms controlled by neural
networks with Hebbian learning live. Organisms are able to
eat, mate, ﬁght, move, change the ﬁeld of view, and utilize
body brightness as a form of emergent communication. Their
emergent behaviors are to some extent similar to the ones
found in nature. Another recent multi-agent environment is
presented by Lowe et al. [24]. However, in such an environ-
ment, the adaptation occurs in the actor-critic methods of their
reinforcement learning framework. Such method outperforms
traditional reinforcement learning approaches on competitive
and cooperative multi-agent environments. Another reinforce-
ment learning method which exploits multi-agent environ-
ments is introduced by Jaderberg et al. [25]. In their work,
they use the environment of Quake III Arena Capture the
Flag, a 3D ﬁrst-person multiplayer game. Their method in
this game exceeds human-level performance, therefore the
artiﬁcial agents are able to cooperate and compete among
them and even with human players. One work that provides an
open-source competitive multi-agent environment for research
purposes is Neural MMO [26]. Here, the agents are players
which need to survive and prosper in an environment similar
to the ones used in Massively Multiplayer Online Role-Playing
Games (MMORPGs).
One important aspect of natural evolution is the ability to
endlessly produce diverse solutions of increasing complexity,
i.e., open-ended evolution (OOE). In contrast, OOE is difﬁcult
to achieve in artiﬁcial systems. A conceptual framework for the
implementation of OOE in evolutionary systems is presented
by Taylor [27]. Embodiment plays a key role in OOE in
the context of the agent and its morphology, as discussed
by Bongard [28]. For an articulated summary and discussion
of OOE see [29]. In [30] the authors argue that open-ended
evolution is a grand challenge for artiﬁcial general intelligence,
and artiﬁcial life methods are promising starting points for the
inclusion of OOE in AI systems.
IV. FRAMEWORK CONCEPT
The main concept of the proposed NAGI framework is to
mimic as close as possible the evolution of general intelligence
in biological organisms, starting from the simplest form. To
do that, we propose a minimalistic model with the following
components. An agent is equipped with a randomly-initialized
minimal spiking neural network. The agent is placed in a
mutable environment in order to be able to generalize (learn
to learn), instead of merely learning to solve the speciﬁc
environment. Agents are more likely to survive if they perform
correct actions. Agents have access to the environment through
sensory inputs. The environment also provides intrinsic re-
wards and penalties. New agents inherit the topologies of
the controllers from the previous generation (untrained), with
the possibility of complexiﬁcation (e.g., new neurons and
synapses can appear through genetic operators). Training or
learning happens throughout a generation. The goal of the
untrained inherited controllers is to possess a topology that
supports the ability to learn new environments. Neural learning
occurs by utilizing environmental information (sensory input
and environmental rewards/penalties) and neuroplasticity (e.g.,
Hebbian learning through spike-timing-dependent plasticity).
The expected result is an unsupervised evolving system that
learns without explicit training in a self-learning manner
through embodiment. In this section, the components of the
conceptual framework are described in details.
A. Data representation
The data that ﬂows to and from the spiking neural networks
that control the agents are encoded as ﬁring rate, i.e., the
number of spikes per second. The ﬁring rate has minimum and
maximum values, and is represented for simpliﬁcation as real
number between 0 and 1 (i.e., range [0.0, 1.0]). The stimulus
to the neural networks can be Poisson-distributed spikes which
have irregular interspike intervals, as observed in the human
cortex [31]. That representation can be used for encoding input
from binary environments (e.g., binary numbers 0 and 1 or
Boolean values False and T rue), or multi-value environments
(e.g., represented as grayscale from black to white), and allows
for representation of minimum and maximum activation values
of sensors and actuators.
B. Self-Learning through Embodiment
A new agent learns through the reactions of an environment
via embodiment (i.e., by having a “body” that affects an
environment while sensing it). As such, the input of the neural
network controller includes reward and penalty information
for the learning process, such as the collision sensor in
an autonomous robot whose activation represents a penalty,
and a reward otherwise. This feedback information is the
key factor for achieving self-learning. The concept of self-
adaptation is closely connected with embodied cognition, a
core property of living beings [32]. In contrast, supervised
learning and reinforcement learning use the error of the neural
network output to globally adjust the network model through
methods of iterative error reduction, such as gradient descent.
In embodied learning, the input itself is used to adjust the
agent’s controller. Such sensory input contains the reactions
of the environment to the actions of the agent.
In the proposed framework, the local learning rules of the
spiking neural network controller are responsible to correct the
global behavior of the network according to agent experiences.
This learning approach is, therefore, a result of self-learning
through embodiment. The framework overview is depicted
3

in Fig. 1. Note that self-learning through embodiment only
works with agents in reactive environments (environments
that affect the agents and are affected by them), such as any
sensory-motor system deployed in the real-world. Non-reactive
environments, on the other hand, do not react to any action of
the agent, like any image classiﬁer or object detector which
only gives environmental information, thus there is no mutual
interaction between an agent and a non-reactive environment.
Therefore, we propose to create a virtual reactive environment
for such cases. Virtual Embodied Learning (VEL) is the
proposed method for such cases when no reward and penalty
feedback is available through the sensory input. VEL adds
reward and penalty inputs to a given sensory-motor system as
illustrated in Fig. 1. It can also include the internal states of
the agents, such as hunger and health. In addition, VEL can
substitute supervised and reinforcement learning by using the
loss of the model as penalty input and the opposite of the loss
as reward input.
C. Mutable environment
To truly exploit and assess the self-learning capabilities and
the generalization of the evolving spiking neural network,
a mutable environment is proposed. The evolutionary goal
of agents is to survive the changes in their environment. In
the real-world, living organisms inherit modiﬁcations to their
body and/or behavior through the generations. For example,
a species may evolve a camouﬂage, such as the stick insects
[33], and another one may evolve the appearance of a poi-
sonous or venomous animal, such as the false coral snakes
[34]. The proposed mutable environment is a simple metaphor
of such examples.
Fig. 2a shows mutable environments that every agent in
the population faces during its lifetime. Each agent has one
sensor which provides one bit of information (i.e., black or
white) and can perform two actions (i.e., eat or avoid). In
each generation, the agents are presented with environmental
data from several environments. Each sample is presented for
a given period to allow the agents’ controllers to learn. In
the ﬁrst environment, the correct action eat is associated with
the white color while the action avoid is associated with
black. Once the environmental data has been consumed by
the agent, there is an abrupt change in the interpretation of
the environment (black and white are ﬂipped) and the agents
are presented with the environmental data again. Agents that
perform well in many environments within each generation
are more likely to go through the next generation.
Fig. 2b presents more complex mutable environments where
agents have two sensors and non-binary environmental values
can be received. As shown in the ﬁgure, different environments
are procedurally generated and presented in each generation,
where abrupt changes in the labeling of correct and wrong ac-
tions have happened. The set of actions may also be expanded
to more than two, with different effects on agents’ lifetime and
their ﬁtness scores.
D. Neuroplasticity
Each neuron in the evolved spiking neural network may
have a different plasticity rule. The different types of learning
rules are subject to evolutionary control. Examples of learning
rules include asymmetric Hebbian, symmetric Hebbian, asym-
metric anti-Hebbian, symmetric anti-Hebbian [9]. Together
with all the Hebbian learning rules encoded in the genome,
there will be the effectiveness of the potentiation and the
depression of the synapse strengths, i.e., how strong the
learning rules are going to be for reducing or increasing the
weight of the synapses. Moreover, other types of learning rules
discovered in neuroscience may be added together or in paral-
lel to those, such as non-Hebbian learning, neuromodulation,
and synapse fatigue [35]–[37]. The neuroplasticity will also
be regulated by a maximum total value of synaptic strength
that a neuron can have for its dendrites. In case this value is
reached, the increase in the weight of a synapse will cause a
decrease of the others in the same neuron. This type of weight
normalization is reported in [38], [39] for biological neurons.
E. Neuroevolution
The population of genomes (spiking neural network con-
trollers) for the agents is evolved through a modiﬁcation of
NEAT [10]. The genotypes of NEAT describe the topology
and weights of the synapses, while our proposed method does
not evolve the weights while includes in the genotype the
type of neurotransmitter and neuroplasticity [9]. The weights
of the spiking neural networks are randomly initialized in
every generation because the agents should not have innate
knowledge of the environment [2]. Therefore, the proposed
framework focuses on the self-learning capabilities of the
agents. Their lifetime will be longer when agents perform
correct actions and shorter when they perform wrong actions.
The lifetime of agents is used as ﬁtness score to deﬁne the
best performing neural networks.
Algorithm 1 explains how an agent’s genome is evaluated
while it is in a mutable environment during its lifetime. The
ﬁtness score for the agent is equal to the time the agent is alive
until its death. Each agent has a maximum life expectancy.
Such life expectancy is reduced faster when an agent receives
a penalty and it is reduced slower when the agent receives
a reward. Both penalties and rewards reduce the lifetime of
agents, as one agent is not to live for an inﬁnite amount of
time if it always performs the correct action.
The neuroevolution process allows the growth of neural net-
work topologies and therefore the population is initialized with
minimal networks that complexify over time. Nevertheless,
there may be a penalty on lifetime to avoid the generation of
big networks which may have neuron groups that specialize
for each different environment. Therefore it allows the network
to learn how to forget the previous environment, and then
be able to adapt to the new one [40]. Another reason to
apply this penalty for the size of the network is that more
neurons require more energy to maintain them. This reduction
of lifetime caused by the number of neurons can be regulated
4

Self-learning
controller
Agent
Sensors
Actuators
Data
Reward
Penalty
Action
Correct action?
Yes
No
Environment
Internal states
Fig. 1. Illustration of virtual embodied learning or self-learning through embodiment in a non-reactive environment. In the case of a reactive environment,
rewards and penalties are embedded within the environmental data.
(a)
(b)
Fig. 2. Samples of mutable environments that can be presented to the agents through their evolution. Agents can execute two actions (eat or avoid). Within
each generation, after the agents of a generation have seen all samples of an environment, a new one is presented. (a) 1D environment where the agent has
one binary sensor. (b) The agent has two non-binary sensors (i.e, the axes).
by a parameter, therefore choosing it is of high importance to
the ﬁtness and lifetime of the agent.
V. DISCUSSION AND CONCLUSION
While current AI methods such as deep learning and rein-
forcement learning (and their combinations) have proven to
be successful in solving a multitude of challenging tasks, e.g.,
defeating humans in the real-time strategy game Starcraft II
[41], there is a lot of debate around the limitation of current
methods for breakthroughs in Artiﬁcial General Intelligence.
One key difference between AI and AGI is the learning
ability. Most of AI methods (supervised, unsupervised, and
reinforcement learning) are explicitly trained, while AGI needs
some intrinsic ability to self-learn.
One of the open questions for AGI research is: how can
artiﬁcial agents be able to acquire the general skill of learning,
in order to continuously adapt throughout their lifetime?
In biological systems, we infer that self-learning is a result
of rewards and penalties which are embedded in the sensory
data living beings receive from the environment (unlabeled
5

Algorithm 1 Agent’s genome evaluation using mutable environment and virtual embodied learning
1: procedure EVALUATE(genome)
2:
agent ←new Agent(genome)
⊲Agent is initialized with untrained neural network
3:
lifetime ←0
4:
while agent is alive do
5:
if dataset is empty then
6:
dataset ←getNextDatasetForCurrentGeneration()
⊲Next temporary environment of the current
generation
7:
data, label ←getRandomSampleAndDeleteFrom(dataset)
8:
reward ←False
⊲Initialization of reward
9:
penalty ←False
⊲Initialization of penalty
10:
while (agent is learningSample) ∧(agent is alive) do
⊲Agent learns the presented sample with neuroplasticity
for a period of time
11:
lifetime ←lifetime + 1
12:
action ←agent(data, reward, penalty)
13:
reward ←action = label
14:
penalty ←action ̸= label
15:
agent.healthReduction(reward, penalty)
⊲Penalty reduces the agent’s health faster than reward, then
accelerating its death
16:
return lifetime
and mutable data). Their ability to learn through this form of
self-learning through embodiment is a result of evolution.
One of the goals of the proposed NAGI framework is an
AGI system that allows the adaptation and general learning
skills through the three main levels of self-organization in
living systems [42]:
• Phylogeny, which includes evolution of genetic represen-
tations;
• Ontogeny, which takes care of the morphogenetic process
(growth) from a single cell to a multicellular machine, by
following the genotype instructions;
• Epigenesis, which allows the emergence of a learning sys-
tem through an indirect encoding between genotype and
phenotype, and the phenotype is subject to modiﬁcations
(learning) throughout the lifetime while interacting with
the environment.
We, therefore, envision the proposed spiking neural net-
work model will include developmental and morphogenetic
processes [43] in future extensions of the framework.
Another envisioned stepping-stone to AGI is the extension
of the framework to artiﬁcial life multi-agent systems. Multi-
agent environments will allow the emergence of more ad-
vanced strategies of adaptation and learning based on collabo-
ration and competition. In addition, the framework may beneﬁt
from extending the environment itself into an evolving agent,
which can also allow for increased complexity and open-ended
evolution.
Finally, we expect that future implementations of the NAGI
framework and its extensions will be deployed/embodied into
real robot agents equipped with physical sensors.
In conclusion, this work proposes a novel general frame-
work for the neuroevolution of artiﬁcial general intelligence
(NAGI) in its simplest form, which can be extended to more
complex tasks and environments. In NAGI, the general intelli-
gence, i.e., learning to learn to adapt to different environments,
is a result of self-learning through embodiment. Therefore,
the learning process is not a result of explicit training with
supervision or reinforcement learning, as there is no loss
function used to adjust the neural network weights. The
proposed neural network model is a bio-inspired model based
on spiking neural networks. Their learning is based on spike-
timing-dependent plasticity which uses only input data for
learning. As such, penalties and rewards are embedded within
the environmental data sensed by the agents.
This work describes the details of the NAGI conceptual
framework as a novel paradigm for self-learning. Therefore,
the experimental results are not included in this contribution.
However, our current experimental results are promising, and
part of a separate contribution.
The NAGI conceptual framework proposes a computational
system which may allow the simplest form of general intelli-
gence observed in nature to emerge. Self-learning through em-
bodiment shifts the way machines currently learn by changing
the paradigm of supervised and reinforcement learning. Our
efforts are also to reduce the gap between biological neural
networks computation and artiﬁcial intelligence implemen-
tations, allowing for a biologically-inspired neural network
model that suits the paradigm in artiﬁcial life of massively
parallel, distributed, and local interactions.
ACKNOWLEDGEMENTS
We thank Gustavo Moreno e Mello, Kristine Heiney, Anis
Yazidi, and Benedikt Vogler for thoughtful comments and
discussions. This work was supported by Norwegian Research
Council SOCRATES project (grant number 270961).
6

REFERENCES
[1] S. J. Arnold and M. J. Wade, “On the measurement of natural and sexual
selection: theory,” Evolution, vol. 38, no. 4, pp. 709–719, 1984.
[2] A. Zador, “A critique of pure learning: What artiﬁcial neural networks
can learn from animal brains,” bioRxiv preprint bioRxiv:582643, 2019.
[3] P. Sermanet, C. Lynch, Y. Chebotar, J. Hsu, E. Jang, S. Schaal, S. Levine,
and G. Brain, “Time-contrastive networks: Self-supervised learning
from video,” in 2018 IEEE International Conference on Robotics and
Automation (ICRA).
IEEE, 2018, pp. 1134–1141.
[4] B. Hilleli and R. El-Yaniv, “Toward deep reinforcement learning without
a simulator: An autonomous steering example,” in Thirty-Second AAAI
Conference on Artiﬁcial Intelligence, 2018.
[5] E. M. Izhikevich, “Simple model of spiking neurons,” IEEE Transactions
on neural networks, vol. 14, no. 6, pp. 1569–1572, 2003.
[6] B. F. Grewe, J. Gr¨undemann, L. J. Kitch, J. A. Lecoq, J. G. Parker, J. D.
Marshall, M. C. Larkin, P. E. Jercog, F. Grenier, J. Z. Li et al., “Neural
ensemble dynamics underlying a long-term associative memory,” Nature,
vol. 543, no. 7647, p. 670, 2017.
[7] S. Doncieux, N. Bredeche, J.-B. Mouret, and A. E. G. Eiben, “Evolu-
tionary robotics: what, why, and where to,” Frontiers in Robotics and
AI, vol. 2, p. 4, 2015.
[8] D. O. Hebb, The organization of behavior: A neuropsychological theory.
New York: Wiley, Jun. 1949.
[9] Y. Li, Y. Zhong, J. Zhang, L. Xu, Q. Wang, H. Sun, H. Tong, X. Cheng,
and X. Miao, “Activity-dependent synaptic plasticity of a chalcogenide
electronic synapse for neuromorphic systems,” Scientiﬁc reports, vol. 4,
p. 4906, 2014.
[10] K. O. Stanley and R. Miikkulainen, “Evolving neural networks through
augmenting topologies,” Evolutionary computation, vol. 10, no. 2, pp.
99–127, 2002.
[11] J. H. Holland, “Genetic algorithms,” Scientiﬁc american, vol. 267, no. 1,
pp. 66–73, 1992.
[12] P. Funes and J. Pollack, “Evolutionary body building: Adaptive physical
designs for robots,” Artiﬁcial Life, vol. 4, no. 4, pp. 337–357, 1998.
[13] C. Mautner and R. K. Belew, “Evolving robot morphology and control,”
Artiﬁcial Life and Robotics, vol. 4, no. 3, pp. 130–136, 2000.
[14] R. A. Watson, S. G. Ficiei, and J. B. Pollack, “Embodied evolution:
embodying an evolutionary algorithm in a population of robots,” in
Proceedings of the 1999 Congress on Evolutionary Computation-CEC99
(Cat. No. 99TH8406), vol. 1, July 1999, pp. 335–342 Vol. 1.
[15] K. O. Stanley, B. D. Bryant, and R. Miikkulainen, “Evolving adaptive
neural networks with and without adaptive synapses,” in The 2003
Congress on Evolutionary Computation, 2003. CEC’03., vol. 4.
IEEE,
2003, pp. 2557–2564.
[16] S. Risi and K. O. Stanley, “Indirectly encoding neural plasticity as a
pattern of local rules,” in International Conference on Simulation of
Adaptive Behavior.
Springer, 2010, pp. 533–543.
[17] A. Gaier and D. Ha, “Weight agnostic neural networks,” in Advances in
Neural Information Processing Systems, 2019, pp. 5365–5379.
[18] K.
O.
Stanley,
J.
Clune,
J.
Lehman,
and
R.
Miikkulainen,
“Designing neural networks through neuroevolution,” Nature Machine
Intelligence, vol. 1, no. 1, pp. 24–35, 2019. [Online]. Available:
https://doi.org/10.1038/s42256-018-0006-z
[19] J. D. Schaffer, “Evolving spiking neural networks: A novel growth algo-
rithm corrects the teacher,” in 2015 IEEE Symposium on Computational
Intelligence for Security and Defense Applications (CISDA), May 2015,
pp. 1–8.
[20] E. Eskandari, A. Ahmadi, S. Gomar, M. Ahmadi, and M. Saif, “Evolving
spiking neural networks of artiﬁcial creatures using genetic algorithm,”
in Neural Networks (IJCNN), 2016 International Joint Conference on.
IEEE, 2016, pp. 411–418.
[21] C. Fernando, D. Banarse, C. Blundell, Y. Zwols, D. Ha, A. A. Rusu,
A. Pritzel, and D. Wierstra, “Pathnet: Evolution channels gradient
descent in super neural networks,” arXiv preprint arXiv:1701.08734,
2017.
[22] P. Voss, “Essentials of general intelligence: The direct path to artiﬁcial
general intelligence,” in Artiﬁcial general intelligence.
Springer, 2007,
pp. 131–157.
[23] L. Yaeger, “Computational genetics, physiology, metabolism, neural
systems, learning, vision, and behavior or poly world: Life in a new
context,” in Santa Fe Institute Studies in the Sciences of Complexity,
vol. 17.
Addison-Wesley Publishing CO, 1994, pp. 263–263.
[24] R. Lowe, Y. Wu, A. Tamar, J. Harb, O. P. Abbeel, and I. Mordatch,
“Multi-agent actor-critic for mixed cooperative-competitive environ-
ments,” in Advances in Neural Information Processing Systems, 2017,
pp. 6379–6390.
[25] M. Jaderberg, W. M. Czarnecki, I. Dunning, L. Marris, G. Lever, A. G.
Castaneda, C. Beattie, N. C. Rabinowitz, A. S. Morcos, A. Ruderman
et al., “Human-level performance in ﬁrst-person multiplayer games
with population-based deep reinforcement learning,” arXiv preprint
arXiv:1807.01281, 2018.
[26] J. Suarez, Y. Du, P. Isola, and I. Mordatch, “Neural mmo: A massively
multiagent game environment for training and evaluating intelligent
agents,” arXiv preprint arXiv:1903.00784, 2019.
[27] T. Taylor, “Evolutionary innovations and where to ﬁnd them: Routes to
open-ended evolution in natural and artiﬁcial systems,” Artiﬁcial Life,
vol. 25, no. forthcoming, 2019.
[28] J. Bongard, N. Cheney, Z. Mahoor, and J. Powers, “The role of
embodiment in open-ended evolution,” OOE3: The Third Workshop on
Open-Ended Evolution, 2018.
[29] T. Taylor, M. Bedau, A. Channon, D. Ackley, W. Banzhaf, G. Beslon,
E. Dolson, T. Froese, S. Hickinbotham, T. Ikegami et al., “Open-ended
evolution: perspectives from the oee workshop in york,” Artiﬁcial life,
vol. 22, no. 3, pp. 408–423, 2016.
[30] K. O. Stanley, J. Lehman, and L. Soros, “Open-endedness: The last
grand challenge youve never heard of,” O’Reilly, 2017.
[31] D. Heeger, “Poisson model of spike generation,” Handout, University of
Standford, vol. 5, pp. 1–13, 2000.
[32] L. Smith and M. Gasser, “The development of embodied cognition: Six
lessons from babies,” Artiﬁcial life, vol. 11, no. 1-2, pp. 13–29, 2005.
[33] S. Lev-Yadun, A. Dafni, M. A. Flaishman, M. Inbar, I. Izhaki, G. Katzir,
and G. Ne’eman, “Plant coloration undermines herbivorous insect cam-
ouﬂage,” BioEssays, vol. 26, no. 10, pp. 1126–1130, 2004.
[34] T. M. Davidson and J. Eisner, “United states coral snakes,” Wilderness
& Environmental Medicine, vol. 7, no. 1, pp. 38–45, 1996.
[35] H. K. Kato, A. M. Watabe, and T. Manabe, “Non-hebbian synaptic
plasticity induced by repetitive postsynaptic action potentials,” Journal
of Neuroscience, vol. 29, no. 36, pp. 11 153–11 160, 2009.
[36] J. P. Johansen, L. Diaz-Mataix, H. Hamanaka, T. Ozawa, E. Ycu,
J. Koivumaa, A. Kumar, M. Hou, K. Deisseroth, E. S. Boyden et al.,
“Hebbian and neuromodulatory mechanisms interact to trigger asso-
ciative memory formation,” Proceedings of the National Academy of
Sciences, vol. 111, no. 51, pp. E5584–E5592, 2014.
[37] T. Abrahamsson, B. Gustafsson, and E. Hanse, “Synaptic fatigue at the
naive perforant path–dentate granule cell synapse in the rat,” The Journal
of physiology, vol. 569, no. 3, pp. 737–750, 2005.
[38] S. Royer and D. Par´e, “Conservation of total synaptic weight through
balanced synaptic depression and potentiation,” Nature, vol. 422, no.
6931, p. 518, 2003.
[39] S. El-Boustani, J. P. K. Ip, V. Breton-Provencher, H. Okuno, H. Bito, and
M. Sur, “Locally coordinated synaptic plasticity shapes cell-wide plas-
ticity of visual cortex neurons in vivo,” bioRxiv preprint bioRxiv:249706,
2018.
[40] A. S. Benjamin, Successful remembering and successful forgetting: A
festschrift in honor of Robert A. Bjork.
Psychology Press, 2011.
[41] O. Vinyals, I. Babuschkin, J. Chung, M. Mathieu, M. Jaderberg,
W. M. Czarnecki, A. Dudzik, A. Huang, P. Georgiev, R. Powell,
T. Ewalds, D. Horgan, M. Kroiss, I. Danihelka, J. Agapiou, J. Oh,
V. Dalibard, D. Choi, L. Sifre, Y. Sulsky, S. Vezhnevets, J. Mol-
loy, T. Cai, D. Budden, T. Paine, C. Gulcehre, Z. Wang, T. Pfaff,
T. Pohlen, Y. Wu, D. Yogatama, J. Cohen, K. McKinney, O. Smith,
T. Schaul, T. Lillicrap, C. Apps, K. Kavukcuoglu, D. Hassabis,
and D. Silver, “AlphaStar: Mastering the Real-Time Strategy Game
StarCraft II,” https://deepmind.com/blog/alphastar-mastering-real-time-
strategy-game-starcraft-ii/, 2019.
[42] M. Sipper, E. Sanchez, D. Mange, M. Tomassini, A. P´erez-Uribe,
and A. Stauffer, “A phylogenetic, ontogenetic, and epigenetic view
of bio-inspired hardware systems,” IEEE Transactions on Evolutionary
Computation, vol. 1, no. 1, pp. 83–97, 1997.
[43] R. Doursat, H. Sayama, and O. Michel, Morphogenetic engineering:
toward programmable complex systems.
Springer, 2012.
7

