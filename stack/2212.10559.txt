Why Can GPT Learn In-Context?
Language Models Secretly Perform Gradient Descent as Meta-Optimizers
Damai Dai‚Ä†‚àó, Yutao Sun‚à•‚àó, Li Dong‚Ä°, Yaru Hao‚Ä°, Zhifang Sui‚Ä†, Furu Wei‚Ä°
‚Ä† Peking University
‚à•Tsinghua University
‚Ä° Microsoft Research
https://github.com/microsoft/LMOps
Abstract
Large pretrained language models have shown
surprising In-Context Learning (ICL) ability.
With a few demonstration input-label pairs,
they can predict the label for an unseen input
without additional parameter updates. Despite
the great success in performance, the work-
ing mechanism of ICL still remains an open
problem. In order to better understand how
ICL works, this paper explains language mod-
els as meta-optimizers and understands ICL
as a kind of implicit Ô¨Ånetuning.
Theoreti-
cally, we Ô¨Ågure out that the Transformer at-
tention has a dual form of gradient descent
based optimization. On top of it, we under-
stand ICL as follows: GPT Ô¨Årst produces meta-
gradients according to the demonstration ex-
amples, and then these meta-gradients are ap-
plied to the original GPT to build an ICL
model. Experimentally, we comprehensively
compare the behavior of ICL and explicit Ô¨Åne-
tuning based on real tasks to provide empiri-
cal evidence that supports our understanding.
The results prove that ICL behaves similarly
to explicit Ô¨Ånetuning at the prediction level,
the representation level, and the attention be-
havior level. Further, inspired by our under-
standing of meta-optimization, we design a
momentum-based attention by analogy with
the momentum-based gradient descent algo-
rithm. Its consistently better performance over
vanilla attention supports our understanding
again from another aspect, and more impor-
tantly, it shows the potential to utilize our un-
derstanding for future model designing.
1
Introduction
In recent years, large pretrained language models,
especially in Transformer-based architectures (e.g.,
GPT; Brown et al. 2020), have shown strong emer-
gent In-Context Learning (ICL) ability. Different
from Ô¨Ånetuning which needs additional parame-
ter updates, ICL just needs several demonstration
‚àóContribution during internship at Microsoft Research.
(Sentence, ?)
‚Ä¶ 
Demonstration Examples
Query Example
Feed-Forward Network
Self-Attention
(Sentence1, Answer1) (Sentence2, Answer2)
Meta-Gradients
Answer
‚Ä¶ 
GPT
In-Context Learning
Finetuning
GPT
(Sentence1, Answer1) 
GPT
(Sentence2, Answer2) 
Back-Propagation
Forward 
Computation
ùö´ùö´ùëæùëæùêÖùêÖùêÖùêÖ
ùö´ùö´ùëæùëæùêàùêàùêàùêàùêàùêà
Gradients
Dual 
View
Figure 1: According to the demonstration examples,
GPT produces meta-gradients for In-Context Learn-
ing (ICL) through forward computation. ICL works by
applying these meta-gradients to the model through at-
tention. The meta-optimization process of ICL shares
a dual view with Ô¨Ånetuning that explicitly updates the
model parameters with back-propagated gradients.
examples prepended before the original input, and
then the model can predict the label for even unseen
inputs. On numerous downstream tasks, a large
GPT model can achieve a quite great performance,
which even exceeds some smaller models with su-
pervised Ô¨Ånetuning. However, although ICL has
achieved great success in performance, the work-
ing mechanism of it is still an open problem to be
investigated.
In this paper, we explain ICL as a process of
meta-optimization and attempt to build connections
between GPT-based ICL and Ô¨Ånetuning. Concen-
trating on the attention modules, we Ô¨Ågure out that
the Transformer attention has a dual form of gra-
dient descent based optimization. On top of it, we
arXiv:2212.10559v2  [cs.CL]  21 Dec 2022

propose a novel perspective to explain ICL: (1) a
pretrained GPT serves as a meta-optimizer; (2) it
produces meta-gradients according to the demon-
stration examples through forward computation;
(3) the meta-gradients are applied to the original
language model through attention to build an ICL
model. As illustrated in Figure 1, ICL and explicit
Ô¨Ånetuning share a dual view of gradient descent
based optimization. The only difference is that
ICL produces meta-gradients through forward com-
putation, while Ô¨Ånetuning computes gradients by
back-propagation. Therefore, it is reasonable to un-
derstand ICL as some kind of implicit Ô¨Ånetuning.
In order to provide empirical evidence to sup-
port our understanding, we conduct comprehensive
experiments based on real tasks. On six classiÔ¨Å-
cation tasks, we compare the model predictions,
attention outputs, and attention scores of pretrained
GPT models in the ICL and Ô¨Ånetuning settings.
As expected, the behavior of ICL is highly sim-
ilar to explicit Ô¨Ånetuning at all of the prediction
level, the representation level, and the attention be-
havior level. These results are strong evidence to
prove the reasonability of our understanding that
ICL performs implicit Ô¨Ånetuning.
Further, we attempt to take advantage of our
understanding of meta-optimization for model de-
signing. To be speciÔ¨Åc, we design a momentum-
based attention, which regards the attention val-
ues as meta-gradients and applies the momentum
mechanism to them. Experiments on both lan-
guage modeling and in-context learning show that
our momentum-based attention consistently outper-
forms vanilla attention, which supports our under-
standing of meta-optimization again from another
aspect. We note that beyond this preliminary ap-
plication, our understanding of meta-optimization
may have more potential to be used to aid in model
designing, which is worth investigating in the fu-
ture.
Our contributions are summarized as follows:
‚Ä¢ We Ô¨Ågure out a dual form between Trans-
former attention and gradient descent based
optimization, and explain language models as
meta-optimizers.
‚Ä¢ We build connections between ICL and ex-
plicit Ô¨Ånetuning and propose to understand
ICL as a kind of implicit Ô¨Ånetuning.
‚Ä¢ We provide several lines of empirical evidence
to prove that ICL and explicit Ô¨Ånetuning be-
have similarly at multiple levels.
‚Ä¢ We design a momentum-based attention that
achieves consistent performance improve-
ments, which shows the potential of our under-
standing of meta-optimization to aid in future
model designing.
2
Background
2.1
In-Context Learning with GPT
In this paper, we focus on ICL for classiÔ¨Åca-
tion tasks using GPT (Brown et al., 2020).
A
GPT model is stacked with L identical Trans-
former (Vaswani et al., 2017) decoder layers where
each layer consists of an attention module and a
feed-forward network. For a classiÔ¨Åcation task,
given a query input text x and a candidate an-
swer set Y = {y1, y2, . . . , ym}, we need to pre-
dict a label ÀÜy conditional on n demonstration
examples C = {(x‚Ä≤
1, y‚Ä≤
1), (x‚Ä≤
2, y‚Ä≤
2), . . . , (x‚Ä≤
n, y‚Ä≤
n)},
where (x‚Ä≤
i, y‚Ä≤
i) is an input-label pair different from
the query one. Formally, given a GPT model M,
we Ô¨Årst compute the probability of each answer yj:
PM(yj | C, x).
(1)
Since the label space is restricted for classiÔ¨Åca-
tion, we predict the Ô¨Ånal answer ÀÜy by selecting
the answer with the highest probability from the
candidate answer set Y :
ÀÜy = yarg maxj PM(yj|C,x).
(2)
In practice, we usually use a pre-deÔ¨Åned template
to format the demonstrations and prepend them
before the query input. Let T (¬∑) be the function
that formats an example, e.g.:
T (x, y) = Sentence: x. Sentiment: y.
(3)
The contextual model input I is organized like
T (x‚Ä≤
1, y‚Ä≤
1) T (x‚Ä≤
2, y‚Ä≤
2) ... T (x‚Ä≤
n, y‚Ä≤
n) T (x, _). (4)
Feeding this contextual input into M, the probabil-
ity of an answer yj is computed as
lj = M(I) ¬∑ eyj,
(5)
PM(yj | C, x) = softmax(lj),
(6)
where M(I) denotes the output hidden state at the
last token position; eyj denotes the word embed-
ding of yj; and lj is the logit corresponding to the
j-th answer.

2.2
Dual Form Between Gradient Descent
Based Optimization and Attention
The idea in this paper to explain language models
as meta-optimizers is inspired by Aizerman et al.
(1964); Irie et al. (2022). They present that linear
layers optimized by gradient descent have a dual
form of linear attention. Let W0, ‚àÜW ‚ààRdout√ódin
be the initialized parameter matrix and the update
matrix, respectively, and x ‚ààRdin be the input rep-
resentation. A linear layer optimized by gradient
descent can be formulated as
F(x) = (W0 + ‚àÜW) x.
(7)
In the back-propagation algorithm, ‚àÜW is com-
puted by accumulating the outer products of the
historic input representations x‚Ä≤T
i
‚ààRdin and the
gradients ei ‚ààRdout of their corresponding outputs:
‚àÜW =
X
i
ei ‚äóx‚Ä≤T
i .
(8)
Combing Equation (7) and Equation (8), we can de-
rive the dual form between gradient descent based
optimization and linear attention:
F(x) = (W0 + ‚àÜW) x
=W0x + ‚àÜWx
=W0x +
X
i

ei ‚äóx‚Ä≤T
i

x
=W0x +
X
i
ei

x‚Ä≤T
i x

=W0x + LinearAttn
 E, X‚Ä≤, x

,
(9)
where LinearAttn(V, K, q) denotes the linear at-
tention operation, where we regard the historic out-
put gradients E as values, the historic inputs X‚Ä≤ as
keys, and the current input x as the query.
3
In-Context Learning (ICL) Performs
Implicit Finetuning
We Ô¨Årst qualitatively analyze the Transformer atten-
tion under a relaxed linear attention form to Ô¨Ågure
out a dual form between it and gradient descent
based optimization. Then, we compare ICL with
explicit Ô¨Ånetuning and build connections between
these two optimization forms. Based on these theo-
retical Ô¨Åndings, we propose to understand ICL as a
kind of implicit Ô¨Ånetuning.
3.1
Transformer Attention as
Meta-Optimization
Let x ‚ààRd be the input representation of a query
token t, and q = WQx ‚ààRd‚Ä≤ be the attention
query vector. In the ICL setting, the attention result
of a head is formulated as
FICL(q) = Attn(V, K, q)
=WV [X‚Ä≤; X] softmax
 
(WK[X‚Ä≤; X])T q
‚àö
d
!
, (10)
where WQ, WK, WV ‚ààRd‚Ä≤√ód are the projection
matrices for computing the attention queries, keys,
and values, respectively;
‚àö
d denotes the scaling
factor; X denotes the input representations of query
tokens before t; X‚Ä≤ denotes the input represen-
tations of the demonstration tokens; and [X‚Ä≤; X]
denotes the matrix concatenation. For ease of qual-
itative analysis, we approximate the standard atten-
tion to a relaxed linear attention by removing the
softmax operation and the scaling factor:
FICL(q) = Attn(V, K, q)
‚âàWV [X‚Ä≤; X]
 WK[X‚Ä≤; X]
T q
= WV X (WKX)T q + WV X‚Ä≤  WKX‚Ä≤T q
= e
FICL(q).
(11)
We deÔ¨Åne WZSL = WV X (WKX)T as the ini-
tialized parameters to be updated since WZSLq is
the attention result in the Zero-Shot Learning (ZSL)
setting, where no demonstrations are given. Fol-
lowing the reverse direction of Equation (9), we
derive a dual form of the Transformer attention:
e
FICL(q) = WZSLq + WV X‚Ä≤  WKX‚Ä≤T q
=WZSLq + LinearAttn
 WV X‚Ä≤, WKX‚Ä≤, q

=WZSLq +
X
i
WV x‚Ä≤
i
 WKx‚Ä≤
i
T q

=WZSLq +
X
i

WV x‚Ä≤
i ‚äó
 WKx‚Ä≤
i
T 
q
=WZSLq + ‚àÜWICLq
= (WZSL + ‚àÜWICL) q.
(12)
As shown in the above equations, the attention to
the demonstration tokens is equivalent to parameter
updates ‚àÜWICL that take effect on WZSL. In addi-
tion, By analogy with Equation (9), we can regard
WV X‚Ä≤ as some meta-gradients, which are used to
compute the update matrix ‚àÜWICL.
In summary, we explain ICL as a process of
meta-optimization: (1) a Transformer-based pre-
trained language model serves as a meta-optimizer;
(2) it produces meta-gradients according to the
demonstration examples through forward computa-
tion; (3) through attention, the meta-gradients are
applied to the original language model to build an
ICL model.

3.2
Comparing ICL with Finetuning
In order to compare the meta-optimization of ICL
with explicit optimization, we design a speciÔ¨Åc Ô¨Åne-
tuning setting as a baseline for comparison. Con-
sidering that ICL directly takes effect on only the
attention keys and values, our Ô¨Ånetuning setting
also updates only the parameters for the key and
value projection. Also in the relaxed linear atten-
tion form, the attention result of a Ô¨Ånetuned head
is formulated as
e
FFT(q) = (WV + ‚àÜWV )XXT (WK + ‚àÜWK)T q
= (WZSL + ‚àÜWFT) q,
(13)
where ‚àÜWK and ‚àÜWV denote the parameter up-
dates to WK and WV , respectively, which are ac-
quired by back-propagation from some training
objectives; and ‚àÜWFT is the updates to WZSL in-
troduced by Ô¨Ånetuning.
For a more fair comparison with ICL, we further
restrict the Ô¨Ånetuning setting as follows: (1) we
specify the training examples as the demonstration
examples for ICL; (2) we train each example for
only one step in the same order as demonstrated for
ICL; (3) we format each training example with the
same template used for ICL T (x‚Ä≤
i, y‚Ä≤
i) and use the
causal language modeling objective for Ô¨Ånetuning.
Comparing ICL and this Ô¨Ånetuning setting, we
Ô¨Ånd that ICL has many properties in common with
Ô¨Ånetuning. We organize these common properties
from the following four aspects.
Both Perform Gradient Descent
Comparing
Equation (12) and Equation (13), we Ô¨Ånd that both
ICL and Ô¨Ånetuning introduce updates (‚àÜWICL v.s.
‚àÜWFT) to WZSL, which can both be regarded as
gradient descent. The only difference is that ICL
produces meta-gradients by forward computation
while Ô¨Ånetuning acquires real gradients by back-
propagation.
Same
Training
Information
The
meta-
gradients of ICL are produced according to
the demonstration examples.
The gradients of
Ô¨Ånetuning are also derived from the same training
examples. That is to say, ICL and Ô¨Ånetuning share
the same source of training information.
Same Causal Order of Training Examples
ICL and our Ô¨Ånetuning setting share the same
causal order of training examples.
ICL uses
decoder-only Transformers so the subsequent to-
kens in the demonstrations will not affect the pre-
ceding ones. For our Ô¨Ånetuning setting, we use the
Dataset
# Valid. Examples
# Labels
CB
56
3
SST2
872
2
SST5
1101
5
Subj
2000
2
MR
1066
2
AGNews
7600
4
Table 1: Statistics of six classiÔ¨Åcation datasets.
same order of training examples and train only one
epoch, so we can also guarantee that the subsequent
examples have no effect on the preceding ones.
Both Aim at Attention
Compared with zero-
shot learning, the direct effect of ICL and our Ô¨Åne-
tuning are both restricted to the computation of
attention keys and values. For ICL, the model pa-
rameters are unchanged and it encodes demonstra-
tion information into additional keys and values
to change the attention behavior. For Ô¨Ånetuning,
due to our restriction, the training information can
be introduced to only the projection matrices for
attention keys and values as well.
Considering all of these common properties be-
tween ICL and Ô¨Ånetuning, we think it is reasonable
to understand ICL as a kind of implicit Ô¨Ånetuning.
In the rest of this paper, we compare ICL and Ô¨Åne-
tuning empirically from multiple aspects to provide
quantitative results to support this understanding.
4
Experiments
4.1
Tasks and Datasets
We compare ICL and Ô¨Ånetuning based on six
datasets spanning three classiÔ¨Åcation tasks. SST-
2 (Socher et al., 2013), SST-5 (Socher et al., 2013),
MR (Pang and Lee, 2005) and Subj (Pang and Lee,
2004) are four datasets for sentiment classiÔ¨Åcation;
AGNews (Zhang et al., 2015) is a topic classiÔ¨Å-
cation dataset; and CB (de Marneffe et al., 2019)
is used for natural language inference. The statis-
tics of the validation examples and label types are
summarized in Table 1.
4.2
Experimental Settings
In our experiments, we use two GPT-like pretrained
language models with 1.3B and 2.7B model param-
eters, respectively, which are released by fairseq1.
In the rest of this paper, we call them GPT 1.3B and
1https://github.com/facebookresearch/fairseq

GPT 2.7B for short. All experiments are conducted
on NVIDIA V100 GPUs with 32 GB memory.
For each task, we use the same template to for-
mat examples for Zero-Shot Learning (ZSL), ICL,
and Ô¨Ånetuning. Details of the templates used for
each task are provided in Appendix A. The answer
prediction processes for ZSL and Ô¨Ånetuning are the
same with ICL as described in Section 2.1, except
that they do not have demonstration examples.
For ICL, we Ô¨Åx the number of demonstration
examples to 32 and tune the random seed for each
task to Ô¨Ånd a set of demonstration examples that
achieves the best validation performance. For Ô¨Åne-
tuning, we use the same demonstration examples
for ICL as the training examples and use SGD as
the optimizer. For a fair comparison, we Ô¨Åne-tune
the model for only one epoch and the training exam-
ples are provided in the same order as demonstrated
for ICL. We tune the learning rate for Ô¨Ånetuning
and select the one that achieves the best validation
performance. Details of the search range and se-
lected value for the random seeds and learning rates
are shown in Appendix B.
4.3
Metrics
We design three metrics to measure the similarity
between ICL and Ô¨Ånetuning at three different levels:
the prediction level, the representation level, and
the attention behavior level.
Recall to Finetuning Predictions (Rec2FTP)
At the prediction level, this metric measures ICL
can cover how much behavior of Ô¨Ånetuning. We
Ô¨Årst count NFT, the number of query examples that
Ô¨Ånetuning can predict correctly but ZSL cannot.
Then, among these examples, we count Nboth, the
number that ICL can also predict correctly. Finally,
we compute the Rec2FTP score as Nboth
NFT . A higher
Rec2FTP score suggests that ICL covers more be-
havior of Ô¨Ånetuning at the prediction level.
Similarity
of
Attention
Output
Updates
(SimAOU)
This metric measures the similarity
between the effects that ICL and Ô¨Ånetuning have
on ZSL at the representation level. For a query
example, let h(l)
X denote the output representation
of the last token at the l-th attention layer in the X
setting. The updates of ICL and Ô¨Ånetuning com-
pared with ZSL are h(l)
ICL ‚àíh(l)
ZSL and h(l)
FT ‚àíh(l)
ZSL,
respectively.
We compute the cosine between
these two updates to get the SimAOU score at the
l-th layer. A higher SimAOU score means ICL
Model
Task
ZSL
FT
ICL
GPT 1.3B
CB
37.5
57.1
57.1
SST2
70.5
73.5
92.7
SST5
39.3
42.0
45.0
Subj
72.6
78.8
90.0
MR
65.9
68.2
89.0
AGNews
46.3
53.7
79.2
GPT 2.7B
CB
42.9
60.7
55.4
SST2
71.4
91.2
95.0
SST5
35.9
46.6
46.5
Subj
75.2
85.6
90.3
MR
60.9
78.8
91.3
AGNews
39.8
66.6
80.3
Table 2: Validation accuracy in the ZSL, Ô¨Ånetuning,
and ICL settings on six classiÔ¨Åcation datasets.
is more inclined to update the attention output
representation in the same direction as Ô¨Ånetuning.
Similarity of Attention Map (SimAM)
This
metric measures the similarity of the attention be-
havior of ICL and Ô¨Ånetuning. For a query example,
let m(l,h)
X
denote the attention weights before soft-
max of the last token at the h-th attention head in
the l-th attention layer in the X setting. For ICL,
we omit the attention to the demonstration tokens
and only monitor the attention weights to the query
input. We compute the cosine between m(l,h)
ICL and
m(l,h)
FT
and then average the similarity across the
attention heads to get the SimAM score at each
layer. A higher SimAM score indicates that the
attention weights that ICL and Ô¨Ånetuning pay to
the query tokens are more similar.
4.4
Results
Accuracy
We Ô¨Årst show the validation accuracy
in the ZSL, ICL, and Ô¨Ånetuning settings on six
classiÔ¨Åcation datasets in Table 2. Compared with
ZSL, ICL and Ô¨Ånetuning both achieve considerable
improvements, which means the optimizations they
make are both helpful to these downstream tasks.
In addition, we Ô¨Ånd that ICL is better at few-shot
scenarios than Ô¨Ånetuning.
Rec2FTP
We show the Rec2FTP scores for two
GPT models on six datasets in Table 3. As shown
in the table, on average, ICL can correctly predict
87.64% of the examples that Ô¨Ånetuning can cor-
rect from ZSL. These results indicate that at the
prediction level, ICL can cover most of the correct

Model
Task
Rec2FTP
SimAOU
Random SimAOU
SimAM
ZSL SimAM
GPT 1.3B
CB
91.67
0.189
0.004
0.386
0.152
SST2
86.32
0.128
0.003
0.608
0.555
SST5
70.16
0.173
0.004
0.430
0.391
Subj
84.39
0.070
0.004
0.504
0.378
MR
92.14
0.188
0.003
0.513
0.398
AGNews
85.41
0.155
0.003
0.536
0.152
GPT 2.7B
CB
100.00
0.184
-0.001
0.362
0.228
SST2
93.87
0.113
0.003
0.687
0.687
SST5
74.32
0.142
0.001
0.411
0.380
Subj
90.46
0.100
0.004
0.375
0.346
MR
95.44
0.120
0.001
0.346
0.314
AGNews
87.48
0.210
-0.003
0.305
0.172
Table 3: Rec2FTP, SimAOU, and SimAM scores on six classiÔ¨Åcation datasets. The demonstrated SimAOU and
SimAM scores are averaged across examples and layers. For comparison, we also show two baseline metrics for
SimAOU and SimAM, respectively. On all of these datasets, ICL tends to perform similar behavior to Ô¨Ånetuning
at the prediction, representation, and attention behavior levels.
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
Layer
0.5
0.0
0.5
SimAOU
GPT 1.3B
1
2
3
4
5
6
7
8
9
10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32
Layer
0.5
0.0
0.5
SimAOU
GPT 2.7B
Figure 2: Statistics of the SimAOU scores at different layers. The yellow lines denote medians.
behavior of Ô¨Ånetuning.
SimAOU
We present the SimAOU scores aver-
aged across examples and layers for two GPT mod-
els on six datasets in Table 3. For comparison, we
also provide a baseline metric (Random SimAOU)
that computes the similarity between ICL updates
and randomly generated updates. From the table,
we Ô¨Ånd that ICL updates are much more similar to
Ô¨Ånetuning updates than to random updates, which
means at the representation level, ICL tends to
change the attention results in the same direction
as Ô¨Ånetuning changes.
SimAM
Table 3 also demonstrates the SimAM
scores averaged across examples and layers for two
GPT models on six datasets. As a baseline metric
for SimAM, ZSL SimAM computes the similarity
between ICL attention weights and ZSL attention
weights. Comparing these two metrics, we also
observe that compared with ZSL, ICL is more in-
clined to generate attention weights similar to those
of Ô¨Ånetuning. Again, at the attention behavior level,
we prove that ICL behaves similarly to Ô¨Ånetuning.
5
Discussions
5.1
Similarity at Different Layers
In order to investigate the similarity between ICL
and Ô¨Ånetuning more thoroughly, we look into the
SimAOU and SimAM scores at different layers.
We randomly sample 50 validation examples from
each dataset and draw box plots for SimAOU and

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
Layer
0.5
0.0
0.5
1.0
SimAM
GPT 1.3B
1
2
3
4
5
6
7
8
9
10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32
Layer
0.5
0.0
0.5
1.0
SimAM
GPT 2.7B
Figure 3: Statistics of the SimAM scores at different layers. The yellow lines denote medians.
SimAM in Figure 2 and Figure 3, respectively.
From the Ô¨Ågures, we Ô¨Ånd that both SimAOU and
SimAM are Ô¨Çuctuated at lower layers, and tend
to be steadily larger at higher layers. This phe-
nomenon suggests that the meta-optimization made
by ICL has forward accumulated effects, so with
more accumulation, ICL will behave more simi-
larly to Ô¨Ånetuning at higher layers.
5.2
Mapping Between Optimization
Algorithm and Transformer Architecture
Gradient Descent
(GD)
GD with
Momentum
Attention
Momentum
Attention
We have Ô¨Ågured out the dual form between
Transformer attention and gradient descent based
optimization. Inspired by this dual view, we inves-
tigate whether we can utilize momentum (Polyak,
1964; Sutskever et al., 2013), a widely used tech-
nique for improving optimization algorithms, to
improve Transformer attention.
Method
As stated in Section 3.1, the attention
values serve as some kind of meta-gradients. By
analogy with momentum SGD that averages gradi-
ents among timestamps, we try to apply Exponen-
tial Moving Average (EMA) (Hunter, 1986) to the
attention values to build momentum-based atten-
tion:
MoAttn(V, K, qt) = Attn(V, K, qt) + EMA(V )
= V softmax(KT qt
‚àö
d
) +
t‚àí1
X
i=1
Œ∑t‚àíivi,
where Œ∑ is a hyper-parameter, and vi is the i-th
attention value vector. We assume that introducing
momentum into attention will help capture long
dependency and thus lead to faster convergence
and better performance.
Experiments on Language Modeling
First, we
evaluate the effect of momentum-based attention
on language modeling. We train two GPT models
with 350M parameters from scratch, where one is
the vanilla Transformer, and another applies mo-
mentum to attention. We evaluate the perplexity
of these two models on the training set and three
validation sets with input lengths of 256, 512, and
1024, respectively. The results are shown in Ta-
ble 4. We Ô¨Ånd that on all of the validation sets,
applying momentum to attention introduces a con-
sistent perplexity improvement compared with the
vanilla Transformer.
Experiments on In-Context Learning
We also
evaluate the in-context learning ability of the above
language models to verify the effectiveness of the
momentum-based attention on downstream tasks.
We consider six datasets for sentiment analysis
(SST5 (Socher et al., 2013), IMDB (Maas et al.,
2011), MR (Pang and Lee, 2005)), natural language
inference (CB (de Marneffe et al., 2019)), and
multi-choice selection (ARC-E (Clark et al., 2018),
PIQA (Bisk et al., 2020)). For all these datasets,
we use 32 examples as demonstrations. As shown
in Table 5, compared with the vanilla Transformer,
using momentum-based attention achieves consis-
tently higher accuracy in all the tasks.
The performance improvements on both lan-
guage modeling and in-context learning prove that

Model
Train1024
Valid256
Valid512
Valid1024
Transformer
17.61
19.50
16.87
15.14
TransformerMoAttn
17.55
19.37
16.73
15.02
Table 4: Perplexity on the training set and validation sets with different input lengths for language modeling.
Applying momentum to attention introduces a consistent perplexity improvement compared with the vanilla Trans-
former.
Model
SST5
IMDB
MR
CB
ARC-E
PIQA
Average
Transformer
25.3
64.0
61.2
43.9
48.2
68.7
51.9
TransformerMoAttn
27.4
70.3
64.8
46.8
50.0
69.0
54.7
Table 5: Accuracy on six in-context learning tasks. Introducing momentum into attention improves the accuracy
of the vanilla Transformer by 2.8 on average.
introducing momentum into attention is an effec-
tive strategy, which supports our understanding of
meta-optimization from another aspect.
6
Conclusion
In this paper, we aim to explain the working mech-
anism of GPT-based ICL. Theoretically, we Ô¨Ågure
out a dual form of ICL and propose to understand
ICL as a process of meta-optimization. Further,
we build connections between ICL and a speciÔ¨Åc
Ô¨Ånetuning setting and Ô¨Ånd that it is reasonable to
regard ICL as a kind of implicit Ô¨Ånetuning. Empir-
ically, in order to support our understanding that
ICL performs implicit Ô¨Ånetuning, we comprehen-
sively compare the behavior of ICL and Ô¨Ånetuning
based on real tasks. The results prove that ICL
behaves similarly to explicit Ô¨Ånetuning at the pre-
diction level, the representation level, and the at-
tention behavior level. Further, inspired by our
understanding of meta-optimization, we design a
momentum-based attention that achieves consistent
performance improvements. We believe that our
understanding will have more potential to aid in
ICL application and model designing in the future.
References
Mark A Aizerman, Emmanuil M Braverman, and Lev I
Rozonoer. 1964. Theoretical foundation of potential
functions method in pattern recognition. Avtomatika
i Telemekhanika, 25(6):917‚Äì936.
Ekin Aky√ºrek, Dale Schuurmans, Jacob Andreas,
Tengyu Ma, and Denny Zhou. 2022. What learning
algorithm is in-context learning? investigations with
linear models. CoRR, abs/2211.15661.
Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jian-
feng Gao, and Yejin Choi. 2020. Piqa: Reasoning
about physical commonsense in natural language. In
Thirty-Fourth AAAI Conference on ArtiÔ¨Åcial Intelli-
gence.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah,
Jared
D
Kaplan,
Prafulla
Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry,
Amanda Askell, Sandhini Agarwal, Ariel Herbert-
Voss, Gretchen Krueger, Tom Henighan, Rewon
Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu,
Clemens Winter, Chris Hesse, Mark Chen, Eric
Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess,
Jack Clark, Christopher Berner, Sam McCandlish,
Alec Radford, Ilya Sutskever, and Dario Amodei.
2020. Language models are few-shot learners. In
Advances in Neural Information Processing Systems,
volume 33, pages 1877‚Äì1901. Curran Associates,
Inc.
Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot,
Ashish Sabharwal, Carissa Schoenick, and Oyvind
Tafjord. 2018.
Think you have solved question
answering?
try arc, the ai2 reasoning challenge.
arXiv:1803.05457v1.
Marie-Catherine de Marneffe, Mandy Simons, and
Judith Tonhauser. 2019.
The CommitmentBank:
Investigating projection in naturally occurring dis-
course.
Proceedings of Sinn und Bedeutung,
23(2):107‚Äì124.
Shivam Garg, Dimitris Tsipras, Percy Liang, and Gre-
gory Valiant. 2022. What can transformers learn in-
context? A case study of simple function classes.
CoRR, abs/2208.01066.
J Stuart Hunter. 1986.
The exponentially weighted
moving average.
Journal of quality technology,
18(4):203‚Äì210.
Kazuki Irie, R√≥bert Csord√°s, and J√ºrgen Schmidhuber.
2022. The dual form of neural networks revisited:
Connecting test time predictions to training patterns

via spotlights of attention.
In International Con-
ference on Machine Learning, ICML 2022, volume
162 of Proceedings of Machine Learning Research,
pages 9639‚Äì9659. PMLR.
Andrew L. Maas, Raymond E. Daly, Peter T. Pham,
Dan Huang, Andrew Y. Ng, and Christopher Potts.
2011. Learning word vectors for sentiment analy-
sis. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 142‚Äì150, Port-
land, Oregon, USA. Association for Computational
Linguistics.
Bo Pang and Lillian Lee. 2004.
A sentimental edu-
cation: Sentiment analysis using subjectivity sum-
marization based on minimum cuts.
In Proceed-
ings of the 42nd Annual Meeting of the Association
for Computational Linguistics (ACL-04), pages 271‚Äì
278, Barcelona, Spain.
Bo Pang and Lillian Lee. 2005. Seeing stars: Exploit-
ing class relationships for sentiment categorization
with respect to rating scales. In Proceedings of the
ACL.
Boris T Polyak. 1964. Some methods of speeding up
the convergence of iteration methods.
Ussr com-
putational mathematics and mathematical physics,
4(5):1‚Äì17.
Richard Socher, Alex Perelygin, Jean Wu, Jason
Chuang, Christopher D. Manning, Andrew Ng, and
Christopher Potts. 2013.
Recursive deep models
for semantic compositionality over a sentiment tree-
bank.
In Proceedings of the 2013 Conference on
Empirical Methods in Natural Language Processing,
pages 1631‚Äì1642, Seattle, Washington, USA. Asso-
ciation for Computational Linguistics.
Ilya Sutskever, James Martens, George Dahl, and Ge-
offrey Hinton. 2013. On the importance of initial-
ization and momentum in deep learning. In Interna-
tional conference on machine learning, pages 1139‚Äì
1147. PMLR.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, ≈Åukasz
Kaiser, and Illia Polosukhin. 2017.
Attention is
all you need.
In Advances in Neural Information
Processing Systems, pages 5998‚Äì6008. Curran As-
sociates, Inc.
Xiang Zhang, Junbo Jake Zhao, and Yann LeCun. 2015.
Character-level convolutional networks for text clas-
siÔ¨Åcation. In Advances in neural information pro-
cessing systems.
A
Templates for In-Context Learning
We demonstrate the templates used to format exam-
ples and the candidate answer sets for six classiÔ¨Å-
cation datasets used in our experiments in Table 6.
B
Hyper-parameters
We perform grid search to Ô¨Ånd the best random seed
for ICL and the best learning rate for Ô¨Ånetuning.
The search range for all the datasets is the same.
For random seeds, we search in {1, 2, 3, 4, 5, 6, 7}.
For learning rates, the search base values are
{1, 2, 3, 4, 5, 6, 7, 8, 9} and we scale them to 0.1,
0.01, and 0.001 times, i.e., we have 9 √ó 3 = 27
values to search.
In Table 7, we present the details of the selected
random seeds and learning rates for two GPT mod-
els on six classiÔ¨Åcation datasets.

Dataset
Template
Candidate Answer Set
CB
{Premise}
{ True, False, Neither }
Question: {Hypothesis} True, False, or Nei-
ther?
Answer: {Label}
SST-2
Sentence: {Sentence}
{ Negative, Positive }
Label: {Label}
SST-5
Sentence: {Sentence}
{ terrible, bad, neutral, good, great }
Label: {Label}
Subj
Input: {Sentence}
{ objective, subjective }
Type: {Label}
MR
Review: {Sentence}
{ Negative, Positive }
Sentiment: {Label}
AGNews
Classify the news articles into the categories
of World, Sports, Business, and Technology.
{ World, Sports, Business, Technology }
News: {Sentence}
Type: {Label}
Table 6: Formatting templates and candidate answer sets for six classiÔ¨Åcation datasets.
Hyper-parameter
Dataset
GPT 1.3B
GPT 2.7B
Random Seed
CB
3
3
SST2
2
7
SST5
5
5
Subj
4
4
MR
5
1
AGNews
3
3
Learning Rate
CB
0.100
0.090
SST2
0.020
0.007
SST5
0.006
0.003
Subj
0.003
0.002
MR
0.010
0.001
AGNews
0.200
0.060
Table 7: Selected random seeds and learning rates for two GPT models on six classiÔ¨Åcation datasets.

